Timesheet Assistant: Mining and Reporting Developer
Effort∗
Renuka Sindhgatta, Nanjangud C.
Narendra, Bikram Sengupta, Karthik
Visweswariah
IBM Research India; Bangalore, India
{renuka.sr,narendra,bsengupt,v-
karthik}@in.ibm.comArthur G. Ryman
IBM Rational
Toronto, Canada
ryman@ca.ibm.com
ABSTRACT
Timesheets are an important instrument used to track time
spent by team members in a software project on the tasks as-signed to them. In a typical project, developers ﬁll timesheets
manually on a periodic basis. This is often tedious, time
consuming and error prone. Over or under reporting of
time spent on tasks causes errors in billing developmentcosts to customers and wrong estimation baselines for fu-
ture work, which can have serious business consequences.
In order to assist developers in ﬁlling their timesheets ac-curately, we present a tool called Timesheet Assistant (TA)
that non-intrusively mines developer activities and uses sta-
tistical analysis on historical data to estimate the actualeﬀort the developer may have spent on individual assigned
tasks. TA further helps the developer or project manager
by presenting the details of the activities along with eﬀort
data so that the eﬀort may be seen in the context of the
actual work performed. We report on an empirical study of
TA in a software maintenance project at IBM that providespreliminary validation of its feasibility and usefulness. Someof the limitations of the TA approach and possible ways to
address those are also discussed.
Categories and Subject Descriptors
D.2 [Software Engineering ]: Design - methodologies ;D e -
sign Tools and Techniques - computer-aided software engi-
neering
General Terms
Design, Experimentation, Human Factors, Veriﬁcation
Keywords
Timesheet, Development Activity, Mining, Estimation
∗Thanks to Saurabh Sinha and Subhajit Datta for their feed-
back.
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies arenot made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.ASE’10, September 20–24, 2010, Antwerp, Belgium
Copyright 2010 ACM 978-1-4503-0116-9/10/09 ...$10.00.1. INTRODUCTION
One of the key features of a software development or main-
tenance contract is that usually customers provide payment
based on the time spent by software developers in executing
that contract. This necessitates the use of timesheets, which
are records of eﬀort expended. These timesheets are manu-
ally ﬁlled in by developers, and then veriﬁed by the softwarevendor and the customer. Appropriate payment is made
to the software vendor after the veriﬁcation. In addition,
timesheets are also used within the software organization totrack eﬀort and developer productivity.
While several products that facilitate manual timesheet
reporting exist [8, 1, 2, 3], how to accurately estimate theactual eﬀort expended on software development tasks con-
tinues to remain a thorny issue. Manually recording time
in timesheets is a tedious and error-prone process. A num-
ber of reasons may lead to incorrect timesheet data beingrecorded; for example:
•Developers often work on multiple development tasksin parallel, or they interleave these tasks with other
activities (e.g., code reviews, learning activities etc).
Later, it becomes diﬃcult to separate out time thatwas spent on these tasks and activities individually.
•The developer may not have expended the full eﬀortfor which he/she is expected to be billed, and to coverthat up, the developer may over-report the eﬀort spent
on certain tasks.
•If the developer truthfully reports an eﬀort that is sig-
niﬁcantly diﬀerent from the one originally estimated
(often by the team lead) then he/she may have to ex-
plain why a certain task took more/less time. It hasbeen our experience that this is a situation many de-velopers tend to avoid.
•Also, it is diﬃcult for project managers to thoroughlyanalyze and validate timesheet data, given that a man-ager would usually not know the full development con-
text, and manually retrieving and checking the neces-
sary information from the project repositories for eachtimesheet entry is not practical.
Errors in timesheets that arise due to such challenges can
be damaging in many ways. Under-reporting of eﬀort results
in lower revenue for the software vendor and also unrealistic
expectations from the customer for future work, which couldseriously impact the quality of software delivery. On the
265
other hand, over-reporting of eﬀort – if detected by the cus-
tomer – results in damaging the vendor’s reputation, leadingto potential loss of business. Timesheet data is also used to
reﬁne estimation baselines in organizations following a pro-
cess improvement model such as Capability Maturity Model
Integration (CMMI)
1. A wrong baseline resulting from poor
quality timesheet data can lead to severe estimation prob-
lems and delivery issues for the software organization in the
long run.
To address these challenges, we present Timesheet As-
sistant (TA), a tool that mines developer activities to (i)estimate the time (i.e., eﬀort) a developer may have actu-
ally spent on assigned tasks, and (ii) generate an activity
report that reviewers may drill down to diﬀerent levels of
detail to understand the characteristics of work carried out.Unlike some existing approaches for timesheet automation(e.g., [5]) that involve intrusive (and therefore unpopular)
methods such as recording keyboard/mouse clicks and mon-
itoring the applications that the developer has accessed, TA
is non-intrusive and does not require any change to the en-
vironment a developer is working in. One of the key require-
ments of TA is the availability of information on the ﬁles
modiﬁed for a given task . In most modern development envi-
ronments such as IBM
R/circlecopyrtRational Team ConcertTM(RTC)2
or Visual Studio Team System (VSTS)3, this information is
available as a part of tasks or work items assigned to the
developer. In Section 2, we explain the role of such envi-
ronments in our overall solution approach.
TA analyzes information in the following manner. First, it
extracts all the tasks a developer had worked on in the given
period of time. Second, for each task, TA mines the ﬁles that
were changed and computes a set of metrics that help explainthe overall development eﬀort and context, for example, the
size of the change, the expertise of the developer on the
ﬁles that had to be changed etc. Third, it uses some ofthe computed metrics and statistical techniques based on
historical data (in this paper we demonstrate usage of linear
regression) to estimate the time that may have been taken
to complete the task. Finally, it reports all the relevant
information along with the tasks in the timesheet to help
reviewers drill-down and validate the data, and periodicallyre-calibrates the estimation model based on new eﬀort valuessubmitted. Section 3 presents the overall architecture of TA
and explains the functioning of its diﬀerent components.
Our evaluation of TA in a maintenance project at IBM has
provided preliminary validation of its usefulness. For exam-ple, we have found that it is possible to develop reasonably
accurate estimation models at the individual work item leveland use that to suggest timesheet entries based on analysis
of development work undertaken. We call this “estimation
in the small”, as opposed to the traditional project-level“es-timation in the large”, exempliﬁed by estimation methods
such as COCOMO [9] or SLIM [7]. Also, user feedback from
the case study suggests that summarizing and linking devel-opment activities with timesheet reports signiﬁcantly easethe task of reviewing the reports or justifying the entries
to project managers or customers. These results are en-
couraging and have helped spark a discussion with one ofour business partners on possible productization of TA con-
1http://www.sei.cmu.edu/cmmi/
2http://www-01.ibm.com/software/awdtools/rtc/
3http://msdn.microsoft.com/en-
us/teamsystem/default.aspxcepts, even as we carry out larger-scale validation. At the
same time, we have also identiﬁed some of the threats tothe validity of our current approach (and possible ways toaddress those) and realized that there are certain eﬀort de-
terminants that are inherently diﬃcult to capture fully, par-
ticularly using a non-intrusive approach such as ours. Theresults and lessons learnt from our empirical study of TA
are detailed in Sections 4 and 5, while the rest of the pa-
per discusses related work and presents our conclusions anddirections for future work.
We will conclude this section with a summary of the main
contributions of our work:
1. We consider the challenges associated with timesheet
reporting and validation that are of great practicalsigniﬁcance to the application services industry, butwhich have been little studied by the software engi-
neering research community.
2. We present a novel framework called Timesheet Assis-
tant (TA) that provides automated and non-intrusivesupport for addressing these challenges through a com-
bination of software repository mining, statistical anal-ysis and in-context reporting of development activities
in timesheet reports.
3. Our solution approach departs in two signiﬁcant ways
from traditional estimation literature: we perform es-timation “in the small” at the level of individual tasks
and we estimate post-facto, based on actual develop-ment work carried out.
4. We demonstrate the feasibility and usefulness of TA
on a real-life case study from a maintenance project atIBM and discuss the lessons learnt.
2. TOWARDS TIMESHEET AUTOMATION
While timesheets have been used in the software indus-
try for decades, we believe that a number of recent trends
now make it possible to mine project repositories and im-
prove and automate the way timesheets are reported. Tobegin with, in the last few years, Integrated Development
Environments (IDEs) have evolved from being developer fo-
cused tools providing features to compile, debug and deploy
software programs to collaborative environments supporting
project planning, work assignment, discussions, source code
management, build and test management, project trackingand reporting. In such development environments, each task
– whether planning, development, testing, or defect ﬁx – is
modeled as a work item that is expected to deliver a de-velopment plan, design, feature enhancement, or a code ﬁx,
as the case may be. A work item carries a set of basic at-
tributes that are useful for tracking it e.g., name, unique
identiﬁer, description, creator, owner, creation date, closuredate, estimated eﬀort, eﬀort spent and so on. Custom at-
tributes can also be deﬁned as needed. Moreover, links may
be established between a work item and associated softwaredevelopment artifacts (code, test cases, designs, plans, etc.)
stored in a conﬁguration management system through the
deﬁnition of one or more change sets . A change set is a
collection of ﬁles grouped together by the developer in a
manner that is meaningful for the project. For example,
all GUI-related ﬁle changes can be grouped together into asingle change set and checked in against one or more work
266items. This rich contextual information linked to work items
makes it possible to track development activities undertakento implement the items, and is core to the Timesheet Assis-
tant approach.
With IDEs becoming increasingly popular and data-rich,
and tool vendors focusing on end-to-end integration across
the software development lifecycle (SDLC) tool stack, we
are also seeing the emergence of data warehouses to help
archive large volumes of SDLC data eﬃciently and supportfast querying and retrieval of information. Data from SDLC
tools can be extracted, transformed and loaded into these
warehouses, and then business intelligence techniques canbe applied on the data to get deeper insight into the health
of the project and obtain various kinds of reports for in-
formed decision-making. Along similar lines, in the case of
Timesheet Assistant, our goal is to run statistical analysistechniques on data and metrics extracted from the devel-
opment environment to estimate the actual eﬀort that may
have been spent on assigned tasks, and then generate in-sightful reports on the development activities undertaken to
help reviewers validate timesheet entries
4. The increasing
adoption of data warehouses and BI techniques in main-
stream software development [16], along with modern IDEs
described above, thus provide the necessary ingredients to
develop an automated solution for timesheets. Speciﬁcally,Timesheet Assistant (TA) assumes Rational Team Concert
as the development environment, Rational Insight
5as the
data warehouse and reporting engine and GNU Octave [4]
for statistical analysis and predictive modeling of eﬀort.
3. TA: TIMESHEET ASSISTANT
We begin this section by describing our approach towards
estimation in the small , i.e., how we model factors that may
determine or help explain eﬀort spent on individual devel-
opment tasks in timesheets. Following this, we will present
an overview of the Timesheet Assistant architecture and ex-plain the functioning of its diﬀerent components.
3.1 Estimation in the Small
A number of parametric software estimation models [9,
7] have evolved over the last several decades to accurately
predict the overall cost, schedule and quality of a software
product to be developed. These models typically embody
estimation in the large ; they apply across the software de-
velopment lifecycle, are governed by a set of gross eﬀort indi-
cators whose values themselves need to be estimated (often
subjectively) at the start of a project, and are then used for
budgeting, project planning and control, tradeoﬀ and riskanalyses, etc. Given that these models have been built, re-
ﬁned and calibrated through a large number of completed
software projects over several years, they provide an excel-lent starting point for us to study well-accepted factors that
impact eﬀort (in the large), and then consider which of those
factors may also be relevant for estimation in the small, and
how they may need to be re-interpreted.
For our study, we used the well-known COCOMO II Post
Architecture model [9] that estimates eﬀort as a functionof the size of the software project (in terms of thousands ofsource lines of code, or function points), adjusted by 22 vari-
ables (scale factors or eﬀort multipliers), values for which are
4http://jazz.net/library/content/articles/insight/performance-
management.pdf
5http://www-01.ibm.com/software/rational/products/insight/given by selecting a rating from a well-deﬁned scale (ranging
from Very Low to Extra High). The choice of COCOMO IIwas motivated by its widespread acceptance in the softwarecommunity. While size of source code produced (or modi-
ﬁed) will naturally inﬂuence the eﬀort needed even for in-
dividual development tasks tracked through timesheets, weperformed an analysis of the 22 identiﬁed variables to deter-
mine their applicability for task-level estimation. We kept
two things in mind while doing this. First, we wanted to ﬁxthe initial context of TA at the level of individual projects;
in other words, we were interested in factors that can cause
eﬀort variations across tasks within the same project envi-ronment, rather than factors that are project-wide and likely
to impact all tasks more or less uniformly. Second, since our
goal is to automate timesheet reporting and validation tothe extent possible, we wished to focus on factors whosevalues can potentially be mined from the development en-
vironment with relative ease/accuracy, without having to
depend on subjective assessment by team members who ﬁllup timesheets, or burdening them with additional reporting
overhead.
With these objectives in mind, we ﬁrst identiﬁed the set
of COCOMO eﬀort factors that would normally character-ize the project as a whole, and whose eﬀect on individual
development tasks we will assume to be uniform. These in-clude for example, Development Flexibility (degree of con-
formance to requirements and interface standards), Ana-
lyst Capability (capability of personnel working on require-ments), Team Cohesion (willingness of parties to work to-gether), Personnel Continuity (annual personnel turnover),
Process Maturity (e.g., CMM level), Use of Software Tools
(degree of sophistication in tools used for the project), Plat-form Volatility (e.g., frequency of changes in compilers, as-
semblers), etc. One may argue, of course, that even some of
these factors may aﬀect one work item more than anotherwithin a project; for example, willingness to work together
may vary from one group of team members to another, or
some requirements analysts may be much more capable thanothers in the project. But we have decided to treat these fac-
tors as having uniform inﬂuence for now, given that it would
anyway be diﬃcult to objectively measure their inﬂuence onindividual development tasks carried out by team members.Next we studied COCOMO factors that may be relevant for
individual work items once they have been re-interpreted for
estimation in the small. We discuss these below.
3.1.1 Reliability
The COCOMO factor Required Software Reliability (more
eﬀort would need to be expended to avoid software failures
that carry higher risks) may vary from one component (and
its work items) to another within a project. Another factor,Data Base Size (eﬀort required to generate test data) is con-
sidered as a measure of the testing eﬀort required and is also
related to reliability. In our estimation model for timesheet
tasks, we have included a factor called Required Reliabilitythat will be measured in terms of the number of test cases
that will have to be written (for new features) or executed
(for defect ﬁxes) before closing the work item. We have as-sumed that components that are deemed risky will be tested
more comprehensively, hence we have used number of test
cases as an indicator of a component’s required reliability.
2673.1.2 Complexity
The Product Complexity factor in COCOMO is factored
into our model as task complexity in multiple ways. First, we
recommend building separate models for diﬀerent types of
work items such as new feature implementations, enhance-
ments or defects, since the dimensions of complexity varyacross these types. For example, for the same change interms of number of lines of code, a defect could take longer
time than an original feature development [13]. On the other
hand, the eﬀort spent on developing a new feature may beinﬂuenced by the number of libraries that are being used (as
the functionality of each of them needs to be understood),
while in a defect ﬁxing activity it may not be as impor-tant since the developer may already be aware of the library
functionalities. This approach may be further extended to
separate out server side work from client side work if needed,since the former would typically involve more complex logic.
Second, for any work item, the complexity of its associated
development activity is measured in terms of the numberof distinct ﬁles changed, for each such ﬁle the number ofmethods changed and for each such method, (i) the well-
known cyclomatic complexity (that measures the number
of independent paths through a program unit) and (ii) thefan-out, i.e., number of external methods/functions being
called. The number of ﬁles/methods changed helps us track
whether the change was localized or relatively distributed (inthe latter case it may be considered more complex). Note
that not all of these measures will necessarily impact the
eﬀort spent on each work item in a statistically signiﬁcant
way; however, we still track these since they also help in
summarizing the overall development context to timesheet
reviewers.
Third, a work item may manipulate ﬁles of diﬀerent types.
The core functionality of the system may be implemented
in some major programming language, but there will also
be accompanying miscellaneous minor ﬁles such as conﬁgu-ration and build scripts, properties ﬁles, xml/html ﬁles, etc.
Making a change to a minor ﬁle such as properties ﬁle will
be in general, far less complex than an equal-sized change ina Java ﬁle. Classifying changes by identifying the types of
ﬁles changed is therefore an important factor in sizing work.
For example, we classify ﬁles as“major”and“minor”for keydevelopment ﬁles and miscellaneous ﬁles respectively. How-
ever, for “minor” ﬁles, due to their simpler structure, we
currently do not calculate any complexity measure.
3.1.3 Expertise
Team capability related factors in COCOMO such as Pro-
grammer Capability (capability of programmers as a team),
Applications/Platform Experience (level of applications or
platform experience of the project team), Language and
Tool Experience (of team), etc., are re-considered within
our timesheet model in terms of the expertise of the devel-oper performing the task. This expertise has two dimen-
sions. The ﬁrst one, which corresponds roughly to experi-
ence, may be used to diﬀerentiate how two developers withdiﬀerent levels of application/language/platform familiarity
may perform the same activity. To handle this, we can clus-
ter developers by experience level, which may be determinedby indicators such as the time spent on the project, the over-
all development experience in years, etc.; the intuition being
that higher the experience, less will be eﬀort required for a
given work item. The second dimension of expertise, whichdeals with task-level familiarity, may be mined from histori-
cal development data and will be used to adjust the eﬀort adeveloper spends on a work item based on his/her familiaritywith the ﬁles that need to be updated. This low-level no-
tion of expertise is particularly important for estimation in
the small, since familiarity with ﬁles can signiﬁcantly bringdown the eﬀort that would be needed to update them to
incorporate a new feature/ﬁx a defect.
We measure the work item level familiarity of a developer
for a work item as the weighted average of the familiarityon each ﬁle changed by the developer, where the weight as-
signed to a ﬁle is the proportion of the total developmentsize for the work item that corresponds to the ﬁle. For ex-
ample, if a developer D has changed ﬁles F1 and F2 for
a work item W, with 20 lines changed in F1 and 30 lineschanged in F2, then D’s familiarity index for W would bea weighted average of her familiarity on each ﬁle ( ﬀ), i.e.,
0.4∗ff for F 1+0 .6∗ff for F 2. Developer expertise
computed in this way lies between 0 and 1, with a highernumber indicating higher expertise.
The ﬁle familiarity ﬀof developer D with a ﬁle F is com-
puted by ﬁrst considering the ratio of the number of times
F has been updated by developer D, compared to the maxi-
mum number of times it has been updated by any developer
throughout the evolution history of the ﬁle. This providesa relative measure of expertise compared to an expert on
that ﬁle. The ratio is used to further classify developer ﬁle
familiarity into 3 buckets (High, Medium, Low) based onthreshold values that may be set by the user. Finally, eachbucket is given normalized scores (we used 1.0, 0.6 and 0.3),
for High, Medium and Low familiarity, respectively. For a
user updating a ﬁle for the ﬁrst time, familiarity is set closeto 0.
We realize that modeling expertise is in itself an interest-
ing topic of research [15], and hence we also experimentedwith a slightly diﬀerent measure of ﬁle familiarity, in terms
of the proportion of total code in the ﬁle that has been up-
dated by the developer, compared to the updates made tothe ﬁle by other developers throughout the evolution history
of the ﬁle. However, when we computed both the measures
and cross-checked with a team of developers who were part
of the TA case study, we realized that equating ﬁle exper-tise with proportional code updated, may result in grossly
inaccurate familiarity levels since it ignores familiarity that
a developer gains of code submitted by others in course ofmaking his/her own updates. On the other hand, according
to the ﬁrst measure, familiarity increases each time a de-
veloper updates a ﬁle, since he/she gets an opportunity toreview its source code, and this model resulted in familiarity
values that to a large extent, mirrored the level of familiar-
ity the developers themselves reported. Hence we have usedthe ﬁrst approach in our model. Details of our interviews
with developers on the topic of ﬁle familiarity could not be
included here due to lack of space.
Table 1 summarizes the eﬀort factors and associated work
item attributes and metrics that we are tracking in our task
level model for timesheets. This information will be pre-
sented to reviewers of timesheets to help them quickly get anunderstanding of the development work carried out. They
will also be used in designing statistical eﬀort models for
development activities.
268Figure 1: Timesheet Assistant Architecture
3.2 Timesheet Assistant Architecture
We will now outline the architecture of TA, comprising
four main components, as shown in Figure 1:
•Activity Tracker: Extracts all attributes and metrics
of work items that provide context of the developmentactivity and are indicative of the eﬀort spent on it.
Steps (1), (2) and (3) in Figure 1 indicate the extrac-
tion and storage of this data.
•Eﬀort Calculator: Uses statistical analysis techniques
to build an eﬀort model based on eﬀort predictors ex-tracted by Activity Tracker. Subsequently, it uses theeﬀort model to compute the eﬀort possibly expended
by the developer on a work item (steps (4) and (5) of
Figure 1).
•Re-Calibrator: As more work item data is captured,the regression model may need to be reﬁned. Re-Calibrator re-computes the regression coeﬃcients whichare further used by Eﬀort Calculator.
•Timesheet Visualizer: The data mined by the ActivityTracker and eﬀort computed by Eﬀort Calculator can
be viewed using Timesheet Visualizer represented by
step (6) in Figure 1.
The following sections describe the components in detail.
3.2.1 Activity Tracker
The key components of Activity Tracker are Work Item
Data Extractor, Code Parser, and Metrics Analyzer. Work
Item Data Extractor uses the Rational Team Concert Client
API6to extract work item attributes such as type of work
item, creator of the work item, owner, status, estimated ef-
fort and change sets associated with the work item. For
each ﬁle in the change set, before and after versions of theﬁle are extracted, the changes made are identiﬁed using the
NetBeans (http://netbeans.org/) diﬀ utility for Windows.
Code Parser parses the ﬁle and changes for method decla-
rations, method invocations, decision statements which are
6https://jazz.net/wiki/bin/view/Main/RtcSdk20used by Metrics Analyzer to compute the metrics listed in
Table 1. These metrics are stored in the Data Store. Met-rics Analyzer also uses historical work item data availablein the Data Store to compute the expertise of the developer
on changed ﬁles and further for the work item as a whole
To allow projects the ﬂexibility of deﬁning their own met-
rics that can be conﬁgured and extracted by Activity Tracker,we deﬁne an extension point for adding additional metrics
– by extending an AbstractMetricProvider of Metrics An-alyzer component. A metric can be deﬁned for diﬀerent
levels of granularity – work item, ﬁle and changes. Activity
Tracker extracts metrics computed by all extension pointsand generates and stores them in the Data Store for each
work item as name/value pairs of metrics.
3.2.2 Effort Calculator
The work item and metrics information mined and com-
puted by the Activity Tracker from historical tasks and eﬀort
reported for these tasks are used by the Eﬀort Calculator,
which applies statistical analysis techniques to predict eﬀortfor subsequent tasks. In our implementation so far, we have
used linear regression for eﬀort prediction. A user can cre-
ate a model template for a project using TA. A model tem-
plate allows the user to deﬁne inputs for the linear regressionmodel, such as the time period, software size, expertise, com-
plexity metrics, etc., to be considered and any other custom
predictors deﬁned in Activity Tracker. The model templatecan be instantiated resulting in extraction of relevant pre-
dictors and using Ordinary Least Squares (OLS) regression
to compute the model. If the predictor distributions are not
normal, they are linearized by taking logarithms. The re-
gression coeﬃcients and the resulting eﬀort curve are stored
in the Data Store. The user can select, discard or reﬁne themodel template inputs based on a measure of goodness of
ﬁt (described later in Section 4). Once a model template is
selected suitable for prediction, for any new work item, theeﬀort is computed based on the model contained in the tem-plate, i.e., using its regression coeﬃcients and predictors. As
stated earlier in this paper, we have used GNU Octave for
linear regression modeling.
3.2.3 Re-Calibrator
As a project advances, the inﬂuence of factors on the ef-
fort for a work item, changes. Familiarity of technology,
stability of features through a development cycle, etc., may
cause lesser eﬀort to be expended for the same change as
compared to the eﬀort during the initial stages of the cycle.
On the other hand, in long-running projects, code decaycan lead to increase in change eﬀort over time [13]. Either
way, it becomes necessary to adapt the model to changes
in the project environment. Re-calibrator essentially is thecreation of new Eﬀort Calculator model templates. Hence,
during the project life cycle, a new model template can be
instantiated containing the relevant work item information,metrics and predictors.
3.2.4 Timesheet Visualizer
The information mined by Activity Tracker and eﬀort pre-
dicted by Eﬀort Calculator is used to visualize the timesheet
via Timesheet Visualizer. As shown in Figure 2, for a de-
veloper, the work items and the original estimated eﬀort for
each work item are listed. Eﬀort predicted by the EﬀortCalculator is also presented. The Details view provides a
269Table 1: Eﬀort Factors and Related Attributes/Metrics
Eﬀort Factor Metric/Attribute Description
Size Lines of Code (LOC) Number of non-commented lines of code updated (added, changed)
Required Reliability Number of Test Cases Number of test cases that have to be written/executed for a work item
Complexity Task Type Type of task e.g., feature development, enhancement, defect ﬁx etc
Number of Files/Methods changed Number of distinct ﬁles/methods that were updated
File Types Change distribution across diﬀerent ﬁle types (e.g., core logic, properties, xml etc)
Cyclomatic Complexity Number of linearly-independent paths through a program unit
Fan-out Number of other functions being called from a given program unit
Expertise Experience Level Developer’s overall experience (e.g., in years), experience in project etc
Task Familiarity Developer’s familiarity with the ﬁles modiﬁed by the task
summary of the ﬁles and the changes made which can be
further drilled down for details of changes made to each ﬁle.
Size, Complexity, Expertise, etc., metrics are provided for
each ﬁle that can help a developer or a project manageridentify why a speciﬁc development activity took more/less
time. The Visualizer can highlight metric values that cross
a user-deﬁned threshold or range. As Timesheet Visual-
izer runs on Rational Insight, its reporting component can
be used to create customized reports of the information ex-
tracted by Activity Tracker.
4. EVALUATING TA
In this section we present the results of evaluating TA.
The case study was a software development project at IBM
during its maintenance cycle, hence we obtain only defect
work items for our evaluation. Evaluation of TA on otherwork item types – original feature development, enhance-ments, etc., is important and will be taken up in our next
phase of experiments, as when those types of work items
become available to us. Nevertheless, we consider this casestudy signiﬁcant, since maintenance activities involving de-
fect ﬁxes and minor enhancements constitute a signiﬁcant
share of the application services domain where timesheetsare routinely used.
4.1 Goals and Method
The primary goals of our case study were to (i) determine
how well the data extracted using TA could be used for es-
timating the actual eﬀort at the work item level, and (ii)
obtain feedback from the developers and their project man-
agers on our tool and approach in general, and how it could
be improved in the future.
The project team that we chose was responsible for main-
taining a web-based product, and the team used Rational
Team Concert as their development environment. Each taskwas modeled as a work item, estimated time was deﬁnedand the task was assigned to a team member. The team
member would work on the assigned task, and check-in the
relevant development artifacts. Work item data collected for1.5 months was used as a training set to predict eﬀort for
an additional 1 month validation period.
We chose linear regression modeling as our statistical tech-
nique, since it is one of the most widely used methods for
eﬀort estimation, and is simple to use and experiment with.
For each work item, we computed the metrics set shown inTable 1 with two exceptions: Experience Level of the de-
veloper and Number of Test Cases. The former was not
considered since all the 10 developers who were part of themaintenance activity had been with the team for more thana year, and had helped develop the product they were thenmaintaining. There did not seem to be any signiﬁcant dif-
ferences between their platform/application experience lev-
els although we did not collect information on their overalldevelopment experience. We could not collect data on the
number of test cases that were executed for each defect item,
since the testing activity was not formally recorded againstthe defect items in the repository. Correlation among the
factors was then determined using Pearson coeﬃcient, and
only relatively uncorrelated variables were selected [12] asindependent variables for running the regression analysis.These variables were: number of lines of code updated
(DELTALOC), average cyclomatic complexity of changed
methods (DELTACC), and (1-EXP), where EXP refers tothe task familiarity of the developer, computed using the
method outlined in Section 3.1. (The variable (1-EXP)
measures the lackof expertise which should positively cor-
relate with eﬀort). To compute EXP values, ﬁle change
history was extracted for a period of 6 months before the
start of the case study.
The metrics calculated for determining the eﬃcacy of the
regression analysis are the well-known R
2–c o e ﬃ c i e n to f
determination – and Magnitude of Relative Error (MRE).
R2is deﬁned as (1 −(actual effort −predicted effort )2
(actual effort −mean )2 ), whereas
MRE is deﬁned as the absolute value of
(predicted effort −actual effort )∗100
(actual effort ). While the latter provides
an indication of the typical ﬁt error present in the model,
the former measures the goodness of ﬁt of the regression
model, and may be interpreted as the proportion of responsevariation explained by the regressors in the model.
To ensure trustworthy learning data during the case study,
a speciﬁc process recommendation was implemented by theproject managers that ensured accurate reporting of eﬀort
data. This was done by checking each work item in the
training data set, and interviewing the developer responsible
for that workitem, before the work item was closed in theRational Team Concert tool.
4.2 Estimation Results
Using TA, we were able to extract a total of 100 work
items as training data over the previously mentioned 1.5
month period. While ﬁxing defects, the project team pri-
marily developed code in “major” programming languages
such as Java or Javascript, with insigniﬁcant number of“mi-
nor” ﬁles updated – hence we did not consider the latter forour regression analysis. In order to ensure accuracy of this
historical data, we veriﬁed the reported eﬀort for each work
item with the respective developer and his/her project man-ager.
270Figure 2: Timesheet Visualizer
We ran the regression analysis on the input combinations
(DELTALOC), (DELTALOC, 1-EXP) and (DELTALOC,
DELTACC, 1-EXP). Table 2 lists results of our regressionanalysis for these three combinations. Table 2 also depicts
regression analysis results with and without outliers, which
needs explanation. When we analyzed the distribution of
MRE values, we realized that a small share of work items
contributed a very signiﬁcant share of the estimation er-
ror. Discussions with the project team revealed that someoutliers will always exist; for example, a task may require
a developer to learn a new framework/library which takes
signiﬁcantly more time than the actual coding eﬀort andthis will be included in the overall time spent. For the
input combination (DELTALOC, 1-EXP), a small subset
of work items – 10 out of a total of 100 – contributed toabout 80% of the overall ﬁt error measured in terms ofsquared residuals, where the squared residual is calculated
as (actual effort −predicted effort )
2. Project managers
in the team told us that they tend to focus more on stream-
lining the development of the vast majority of work items
hence we decided to rerun the regression analysis by remov-
ing such outliers (as has also been recommended elsewheree.g., [18]) after which we got much more predictable results
as shown in Table 2.
It is clear from the regression results that the size of the
development eﬀort, DELTALOC, was a dominating variable
in terms of its impact on eﬀort, with around 54% of the
eﬀort variation attributable to it (after outlier removal), asshown by the R
2value. While the MRE value may appear
high in % terms, it is mainly a result of the fact that many
defect work items take only a few hours to ﬁx. When we in-
cluded (1-EXP) into the formulation, the impact was smallin terms of its coeﬃcient but signiﬁcant in terms of improv-
ing the regression results. Since most of the developers were
working on defect items associated with development work
they had carried out earlier, expertise was high in a largenumber of cases, and consequently not much of a diﬀerentia-tor in general. However, we discovered that for certain workitems where expertise was rated low by our model, the highdevelopment eﬀort required (relative to the size of work) was
estimated much better, leading to the improved results.
Finally, we included the complexity factor DELTACC in
our experiments. Interestingly, this led to a slight deterio-ration in the regression results for (DELTALOC, 1-EXP), al-
though it was better than those obtained for only DELTALOC.There could be a number of explanations for this, including
the possibility that the metric cyclomatic complexity of up-
dated methods that we used is too generic and does notaccurately capture the actual dimensions of task complex-
ity in a project. We will experiment with other complexity
metrics going forward. However, our hypothesis, based oninterviews with developers on the topic of ﬁle familiarity, isthat in software development, code complexity and familiar-
ity are closely related; as developers become familiar with
a ﬁle, its perceived complexity comes down (even thoughabsolute complexity measures remain the same). When a
developer is ﬁxing defects in a ﬁle he/she is familiar with,
its actual complexity measure is no longer an eﬀort determi-
nant of signiﬁcance; in fact, including it (along with exper-
tise) to estimate eﬀort may even add noise in some data sets,
as seems to be the case in our experiments. On the otherhand, for new feature development that leads to creation of
signiﬁcant new code that a developer has no prior familiar-
ity with, metrics such as cyclomatic complexity, number oflibraries used, etc., may become quite relevant and help inpost-facto explanation of eﬀort spent. We will test this hy-
pothesis in our next round of experiments with TA where
we will have access to an IBM project where signiﬁcant new
features will be incorporated in a product.
To validate the model, we collected data for an additional
49 work items for the 1-month validation period. The av-
erage MRE we obtained was 63% with outliers. Since the
271Table 2: Results of Linear Regression on Training Data
Eﬀort Equation With Outliers Without Outliers
Eﬀort = 1.09 DELTALOC0.41R2= 0.26 and Average MRE = 72.8% R2= 0.54 and Average MRE = 60.2%
Eﬀort = 1.2 DELTALOC0.42(1−EXP)0.06R2= 0.34 and Average MRE = 68.5% R2= 0.62 and Average MRE = 54.5%
Eﬀort = 1.18 DELTACC0.01DELTALOC0.42(1−EXP)0.06R2= 0.34 and Average MRE = 69.6% R2= 0.57 and Average MRE = 55.3%
Figure 3: Frequency Distribution of Squared Resid-
uals for Training Data using (DELTALOC, 1-EXP)
Figure 4: % Deviation of Actual vs. Predicted Eﬀortfor Validation data using (DELTALOC, 1-EXP)
purpose of validation is to determine the estimation accu-
racy of the generated regression model, we used MRE values
to remove the top 10% of outliers. After this removal, the
average MRE value dropped to 37.1%, which is even better
than average MRE obtained in the training set. The rele-vant box plots of MRE values – with and without outliers –
are displayed in Figure 4.
Overall, the regression analysis results are promising, par-
ticularly in the absence of testing related information whichwas a clear gap, since every work item went through a testing
phase, with some work items requiring many more test cases
than others. At the same time, there are several ways in
which our estimation model can be further improved, some
of which are discussed in Section 5.
4.3 Qualitative Feedback
We conducted semi-structured interviews [19] with the ten
developers and the four project managers from the project
team which participated in this case study. Each interview
consisted of two parts – speciﬁc objective-type questions
with pre-speciﬁed answer categories, and requests for qual-itative feedback on TA. Figure 5 summarizes the responses
against pre-speciﬁed answer categories (details of the inter-
views are not presented here due to lack of space).
A few key ﬁndings can be summarized from the interviews:
Figure 5: Developer Feedback on Timesheet Assis-
tant
•It is interesting to observe that the developers inter-
viewed seldom update original estimated eﬀort for tasks,
even though the actual eﬀort diﬀers frequently (Fig-ure 5). One of the managers also conﬁrmed this and
pointed out how it stands in the way of improved es-
timation for future tasks.
•Given this, both developers and managers felt that bylinking development activities to timesheets, TA will
help justfy actual eﬀort spent and promote greatertransparency. Managers also felt that a tool like TA
can help improve their own productivity by cutting
down on project tracking eﬀort, while the eﬀort valueestimated will at least provide some benchmark againstwhich they can validate developer reported eﬀort.
•Two developers who did not ﬁnd TA useful said thatthey were not sure how accurate the task-level eﬀort
prediction model would be in practice, so whether their
reporting burden would be actually reduced. Indeed,there are limitations in our current approach that we
discuss in Section 5, along with how some of them may
be addressed.
•Some of the developers suggested that TA should be
able to calculate & “pop up” the estimated eﬀort as
soon as a developer closed the work item, since thiswill act as a useful reference while recording eﬀort.
Currently, this calculation is done oﬄine and sent di-
rectly to the timesheet report.
•Interestingly, while we internally used the metrics ofR
2and MRE for measuring the goodness of estimation,
project managers suggested that we report these val-
ues through TA as they may be helpful in identifying
projects that show low predictability, so that appro-priate process improvements can be initiated to drive
272down variation and make application services delivery
more predictable.
•Finally, it was also suggested that we integrate TA
with requirements management and design tools so
that the approach may be broadened to support timesheet
assistance for these activities.
5. DISCUSSION
In this section, we discuss some of the limitations of the
TA approach and possible ways to address the same.
One of the threats to the validity of our approach comes
from the focus on development size (lines of code) as the keyeﬀort determinant, and its possible impact on developer be-havior. Expected size is central to all estimation approaches,
but since we are doing post-facto analysis there is the dan-
ger that some redundant changes are deliberately made toincrease development size and claim or justify higher eﬀort
than should have been the case. Currently, we do not per-
form any semantic analysis of code as part of our overallapproach, and rely instead on process level monitoring, e.g.,code reviews, to discourage such practices. Since we link all
changes made to the timesheet report, it becomes easier for
reviewers to check the changes made and detect if something
is amiss. Also, sometimes a trivial change made for valid
reasons (e.g., renaming a widely used variable) can lead to
many lines of code change, hence, at least some lightweightsemantic analysis needs to be built into TA to ensure such
changes do not lead to gross over-estimates of eﬀort.
A related issue is if such a tool will discourage develop-
ers from writing optimized code (which may also take moreeﬀort). In fact, developers may have diﬀerent coding styles
with more experienced developers producing higher quality(with less defects) and more eﬃcient code than their juniorcolleagues. Clustering developers by their experience lev-
els and building models for each cluster may be one way to
address this.
Also, cloning of code can signiﬁcantly boost productiv-
ity. This could be a signiﬁcant factor for eﬀort prediction of
enhancements, especially when there is user interface codeinvolved where reuse is very common. There is a rich lit-
erature on code clone detection [17] and we are currently
reviewing how such techniques can be incorporated into ouroverall solution design.
There are at least two aspects of ﬁle familiarity which
the expertise model discussed in Section 3.1 does not cover.First, we need to factor in decay in ﬁle familiarity of a de-
veloper, when no updates have been by him/her over a sig-niﬁcant period of time; this decay will be more pronouncedwhen other updates have been made in the meantime. Sec-ond, we only consider ﬁle familiarity obtained by making
code updates. What we found out during our case study is
that developers also become familiar by simply reading codesubmitted by team members if they are pertinent to their
own work, or through formal code reviews. In a nonintrusive
approach like ours, it is diﬃcult to accurately track this as-pect of familiarity; possible solutions may include a process
recommendation in IDEs such as Rational Team Concert
where we could associate reviewers to each work item orchange set. This could be a factor in identifying familiarity
of a developer with a given ﬁle.
Apart from the actual act of writing code for an assigned
task, developers can spend a signiﬁcant amount of time ondiscussions with team-members to get a better understand-
ing of the problem, discuss solutions, etc. Modern IDEsprovide collaborative features to hold such discussions in
the context of work items, so persisted discussions can pro-
vide at least some indication of whether a task required sig-
niﬁcant brainstorming or not. However, while distributedteams are often heavy users of these collaborative features,
collocated teams (like the one in our case study) often prefer
face-to-face discussions, which a tool like TA will be unableto track.
Reusability is another aspect not considered within the
current TA approach. It is well-understood that there will
be a cost (higher eﬀort) associated with writing a generic
and reusable framework or library. Code complexity metrics
indicating reusability (such as fan-in) cannot be measuredwhen the code is written as these metrics evolve over a pe-riod of time. Hence to identify a task that requires reusable
components to be written, we may need to trace back to its
associated design space and analyze class or collaborationdiagrams to measure the reusability for the components and
allocate additional time if needed.
6. RELATED WORK
Time Tracking Solutions A number of commercial
solutions (e.g., Actitime [1], Baralga [2], Tasktop [8]), help
oﬃce workers manually record their eﬀorts for various ac-
tivities, and also link that information with other corpo-rate tools such as ERP/CRM and other project management
tools. However, unlike TA, none of these solutions provide a
means to automatically extract the actual quantum of work
performed by a software developer, and estimate the eﬀort
thereof via a statistical technique. In comparison, oDesk [5]
actually measures the eﬀort taken per task by monitoringkeystrokes on a computer. A similar solution is also pro-vided by the Eclipse environment, which can keep track of
the times when ﬁles are opened, edited and closed. However,
the intrusive nature of these approaches can limit their ac-
ceptance in practice.
Mining Change Data: In general, our TA solution is
inspired by the rapidly emerging area of Development Intel-
ligence [16], which is the application of Business Intelligence
ideas to software development projects. Prominent among
these ideas is how to eﬀectively mine developer activitiesand change information from software repositories. In [14],
the authors present a taxonomy of approaches for mining
source code repositories for extracting developer activitiesand change data, such as extracting via CVS annotations,
data mining, heuristics and diﬀerencing. In [22], Zimmer-
mann describes a CVS plugin called APFEL that collectsand stores ﬁne-grained changes from version archives in adatabase. By searching for speciﬁc tokens such as method
declarations, variable usages, exceptions, method calls and
throw/catch statements, APFEL determines changes to ﬁlesin terms of these tokens. On similar lines, in [23], the authors
present a technique called “annotation graph” that identi-
ﬁes line changes in a ﬁle across several versions. The ci-tation [11] presents an algorithm that compares the sets of
lines added and deleted in a change set, using a combination
of information retrieval techniques. We will investigate suchapproaches for code diﬀerencing in TA.
Software Eﬀort Estimation: Several parametric soft-
ware estimation models have been proposed over the years(e.g., PROBE [6], SLIM [7], COCOMO [9]) and empirically
273calibrated using actual project data, with multiple regres-
sion approach being one of the commonly used techniques.These models are useful for “estimation in the large”, while
for estimating eﬀort for timesheet tasks, we had to design
models at a much lower-level granularity. In addition, our
approach has the beneﬁt of analyzing actual developmentwork to estimate factors such as complexity and developer
expertise. Along similar lines, [13] presents a multivariate
regression model for estimating eﬀort for a modiﬁcation re-quest based on the following factors: nature of work (e.g.,
defect or new feature development), size of change, the de-
veloper making the change, and the date the change wasinitiated (to account for a code decay factor). While diﬀer-
ent developers were found to expend varying levels of eﬀort
for comparable work, the reasons behind this were not in-
vestigated, and in particular, the impact of ﬁle familiarity inﬁxing defects has not been considered. Moreover, our focus
is on providing an end-to-end automation framework that
applies repository mining, statistical analysis and data sum-marization techniques to address some practical challenges
in timesheet reporting and validation; this also diﬀerentiates
it from other research eﬀorts where the primary motivationhas been oﬄine analysis of historical data to build explana-
tory models. On a diﬀerent note, [21] presents an approach
for calculating defect ﬁxing eﬀort, by extracting the eﬀortfor the “nearest neighbors” (based on a similarity threshold
using a text similarity engine) of the defect in question. It
would be interesting to apply text similarity (e.g., defect
work item descriptions) to see if our TA estimation modelmay be improved.
7. CONCLUSIONS AND FUTURE WORK
When we set out to develop Timesheet Assistant, our aim
was to determine the usefulness of our proposed“estimation
in the small” approach, in support of the ﬁlling and review-
ing of timesheets. We feel that the case study reported in
our paper does point towards the general feasibility of thisapproach, especially for maintenance projects. The eﬀort
ﬁt of the training set, as well as the validation results, are
deﬁnitely encouraging, and demonstrate the usefulness of ex-pertise, along with development size, as key eﬀort indicators.
The qualitative feedback received from our semi-structured
interviews is also quite positive, with the idea of TA beingwelcomed not only by project managers, but also by devel-
opers. We believe that this is due to the non-intrusive and
overall helpful nature of TA. At the same time, we discussed
some limitations of our current approach and ways in which
the eﬀort model may be enriched with additional parameters
from the development environment.
Our future work will be along the following directions.
First, we are initiating a case study of TA in a project that
involves signiﬁcant development of new features; it would
be interesting to ﬁnd out to what extent our current modelwould need to be tailored to account for eﬀort in new de-
velopment tasks. Second, continuous re-calibration of our
prediction model has not yet been tested, for which we willbe investigating techniques such as those reported in [20].
While outliers are currently eliminated in our model, using
them to create and maintain separate regression models foroutlier work items (cf. [18]) could also be useful. Finally, so
far we have used linear regression for eﬀort prediction since
we found that the simplest to use. Perhaps other techniquessuch as Bayesian analysis [12] or decision tree analysis [10]would provide better results, and we would like to try themout.
8. REFERENCES
[1] Actitime,. http://www.actitime.com.
[2] Baralga,. http://baralga.origo.ethz.ch.[3] FreeTime,. http://www.zoo2.com.au.
[4] GNU Octave,. http://www.gnu.org/software/octave/.
[5] oDesk,. http://www.odesk.com.[6] PROxy Based Estimation (PROBE) for Structured
Query Language (SQL),.
http://www.sei.cmu.edu/reports/06tn017.pdf.
[7] QSM,.
http://www.qsm.com/tools/slim-estimate/index.html.
[8] Tasktop,. http://www.tasktop.com.
[9] B. W. Boehm. Software Cost Estimation with
COCOMO II . Prentice-Hall, Inc., 2000.
[10] L. Breiman, J. Friedman, C. J. Stone, and R. Olshen.
Classiﬁcation and Regression Trees . Chapman and
Hall/CRC, 1 edition, 1984.
[11] G. Canfora, L. Cerulo, and M. D. Penta. Tracking
your changes: A language-independent approach.
IEEE Software , 26(1):50–57, 2009.
[12] S. Chulani. Bayesian analysis of software cost and
quality models. In ICSM , pages 565–, 2001.
[13] T. L. Graves and A. Mockus. Inferring change eﬀort
from conﬁguration management databases. In IEEE
METRICS , pages 267–, 1998.
[14] H. H. Kagdi, M. L. Collard, and J. I. Maletic. Towards
a taxonomy of approaches for mining of source code
repositories. In MSR, 2005.
[15] A. Mockus and J. D. Herbsleb. Expertise browser: a
quantitative approach to identifying expertise. In
ICSE, pages 503–512, 2002.
[16] B. Rotibi. Development Intelligence: Business
Intelligence for Software-Development.http://www.borland.com/resources/en/pdf/solutions/lqm-ovum-developmental-intellligence.pdf.
[17] C. K. Roy, J. R. Cordy, and R. Koschke. Comparison
and evaluation of code clone detection techniques andtools: A qualitative approach. Sci. Comput. Program. ,
74(7):470–495, 2009.
[18] Y.-S. Seo, K.-A. Yoon, and D.-H. Bae. An empirical
analysis of software eﬀort estimation with outlierelimination. In PROMISE ’08 , pages 25–32, New
York, NY, USA, 2008.
[19] T. R. L. B. C. Taylor. Qualitative Communication
Research Methods . SAGE, Inc., 2002.
[20] A. Trendowicz, J. Heidrich, J. M ¨unch, Y. Ishigai,
K. Yokoyama, and N. Kikuchi. Development of ahybrid cost estimation model in an iterative manner.InICSE, pages 331–340, 2006.
[21] C. Weiß, R. Premraj, T. Zimmermann, and A. Zeller.
How long will it take to ﬁx this bug? In MSR, page 1,
2007.
[22] T. Zimmermann. Fine-grained processing of cvs
archives with apfel. In ETX, pages 16–20, 2006.
[23] T. Zimmermann, S. Kim, A. Zeller, and E. J. W. Jr.
Mining version archives for co-changed lines. In MSR,
pages 72–75, 2006.
274