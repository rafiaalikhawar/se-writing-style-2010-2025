Understanding Performance Stairs: Elucidating Heuristics
Bryan Marker, Don Batory, and Robert van de Geijn
Department of Computer Science
The University of Texas at Austin
bamarker@cs.utexas.edu ,batory@cs.utexas.edu,rvdg@cs.utexas.edu
ABSTRACT
How do experts navigate the huge space of implementations for a
given speciﬁcation to ﬁnd an efﬁcient choice with minimal search-
ing? Answer: They use “heuristics” – rules of thumb that are more
street wisdom than scientiﬁc fact. We provide a scientiﬁc justiﬁca-
tion for Dense Linear Algebra (DLA) heuristics by showing that only
a few decisions (out of many possible) are critical to performance;
once these decisions are made, the die is cast and only relatively mi-
nor performance improvements are possible. The (implementation
performance) space of DLA is stair-stepped. Each stair is a set of
implementations with very similar performance and (surprisingly)
share key design decision(s). High-performance stairs align with
heuristics that prescribe certain decisions in a particular context.
Stairs also tell us how to tailor the search engine of a DLA code
generator to reduce the time it needs to ﬁnd implementations that
are as good or better than those crafted by experts.
Categories and Subject Descriptors
D.1.2 [ Automatic Programming ]; D.1.3 [ Concurrent Program-
ming ]; G.4 [ Mathematical Software ]: Efﬁciency; D.2.2 [ Software
Engineering ]: Design Tools and Techniques—Computer-aided soft-
ware engineering
General Terms
Design, Performance.
Keywords
program generation; dense linear algebra; high-performance soft-
ware; distributed-memory computing; model driven engineering
1. INTRODUCTION
Writing high-performance software requires considerable skill.
In immature domains, programmers not only have to know what to
code, but how to code; they shoulder the burden of both application
design and attaining high performance.
In mature domains, standard application programming interfaces
(APIs) are used: programmers code what they want in terms of
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full citation
on the ﬁrst page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior speciﬁc permission
and/or a fee. Request permissions from permissions@acm.org.
ASE’14, September 15-19, 2014, Vasteras, Sweden.
Copyright is held by the owner/author(s). Publication rights licensed to ACM.
ACM 978-1-4503-3013-8/14/09 ...$15.00.
http://dx.doi.org/10.1145/2642937.2642975.these APIs, while experts bear the burden of how to implement
APIs so that user applications run efﬁciently. This separation of con-
cerns has generally worked well: it is the basis of modern software
engineering tools called middleware.
The task of an expert is non-trivial. There may be hundreds or
thousands of possible ways to implement each API operation. An
expert must (at least intuitively) be familiar with them all. Yet,
experts explore only a few possible ways, using insights to navigate
myriad prospects to ﬁnd good, if not optimal, implementations
quickly. Such insights are heuristics; they are intuitive guides that
experts learn over time. Why heuristics work is more street wisdom
than scientiﬁc fact.
We found ourselves in a unique position to relate heuristics to
scientiﬁc insight. Our research automates the development of Dense
Linear Algebra (DLA) libraries – libraries that are deﬁned by stan-
dard APIs and that are coded by experts per hardware architecture.
DLA experts shoulder the burden of writing high-performance li-
brary routines; they must know how to code efﬁcient algorithms,
how to customize algorithms for target architectures, and how to
navigate huge implementation spaces quickly to ﬁnd the highest-
performing algorithms. We have automated the exploration of these
spaces (by generating all implementations using a methodical pro-
cess) and we evaluate the efﬁciency of each implementation via
cost estimation.1This is how we ﬁnd the best-performing algo-
rithm that experts would intuitively select [17, 18, 19]. In all tests,
generated code is the same or better than experts’ hand-produced
implementations.
We discovered that the space of (implementation performance)
is not smooth but stair-stepped; each “stair” represents a set of
implementations that perform approximately the same and – surpris-
ingly – that share design decisions. That is, a few design decisions
(out of many possible) are critical to performance. Once these de-
cisions have been made, the die is cast and only relatively minor
performance improvements are subsequently possible: the important
decisions relate to algorithm selection and parallelization while the
less important ones optimize data communication. Of course, some
“stairs” overlap, where the performance difference between key de-
cisions is minimal, and secondary decisions rise to signiﬁcance.
Heuristics align with combinations of decisions that eliminate
outright poor-performing designs. To exploit this observation re-
quires design knowledge to be modularized by design decisions, so
that good decisions (or decision combinations) can be distinguished
from bad decisions (decision combinations). We modularize such
decisions in DLA by transformations, not code modules.
1Runtimes can be quite long and require thousands of cores on expensive
machines, so empirical timing data is not viable to judge implementation
efﬁciency. Estimates must be used.
301
In this paper, we explain how tools can be built based on these
ideas and show how to exploit the staired structure of an (implemen-
tationsperformance) space to reduce the number of implementa-
tions to explore. For complicated algorithms with intractably large
search spaces, this reduces search time to less than an hour and the
ﬁnal code that it produces is as good as or better than that produced
by an expert.
The main contributions of this paper are:
(1)Our explanation of how an expert effectively codes a high-
performance algorithm without spending months exploring
options.
(2)We use two classes of transformations – reﬁnements and op-
timizations – and now further classify reﬁnements in two
ways, “variant reﬁnements” and “rephrasing reﬁnements” (ex-
plained below) so we can identify which DLA design deci-
sions are important. We identiﬁed 54 variant reﬁnements and
53 rephrasing reﬁnements.
(3)Our evidence of how stairs of an (implementation perfor-
mance) space can be exploited to effectively reduce the search
for efﬁcient implementations.
We conjecture that our ﬁndings are representative of other domains,
where our analysis and approach could be applied. Admittedly this
will take more time to prove (or disprove). This paper presents a
valuable data point in this quest: the domain of DLA.
We begin with a brief overview of how we generate the space of
implementations for a given operation in the domain. Our approach
is called Design by Transformation (DxT) – more details are given
in [9, 17, 18, 19, 27].
2. DESIGN BY TRANSFORMATION
Consider the universe of dataﬂow applications. Each application
is represented by a directed, acyclic graph (DAG). Every node or
boxdenotes an operation. Operation inputs and outputs are incom-
ing and outgoing edges, respectively. Operations can be either an
interface or a primitive. An interface has no implementation details,
other than a name, inputs, outputs, preconditions and postconditions
[11]. A primitive additionally has a given code implementation (eg
an API function call).
An expert starts with a DAG that speciﬁes a desired algorithm
independent of implementation or hardware-architecture speciﬁcs.
Usually, this DAG contains only interfaces. We encode design
knowledge of how to implement interfaces using reﬁnements. A
reﬁnement is a transformation that replaces an interface with a graph
(with lower-level interfaces or primitives) implementing that inter-
face. It might use architecture-speciﬁc operations or algorithms. For
example, a reﬁnement can encode how to parallelize an operation.
Reﬁnements are applied repeatedly until the graph contains only
primitives. This represents an implementation of the starting graph,
but not necessarily the most efﬁcient implementation. Optimizations
can yield higher performance. An optimization is a transformation
that replaces a subgraph with another subgraph that implements
the same functionality but in a different way. An expert generally
applies a sequence of optimizations to improve performance.
DxTer is our tool that automatically applies reﬁnements and opti-
mizations to a user-speciﬁed graph. It relies on a knowledge base
of transformations that we have mined from re-engineered DLA
code. DxTer applies all transformations that it can to form a space
of all possible implementations of the starting graph. It then rank
orders these implementations using cost estimates. Each primitive
(which represents a function call) has a cost estimate based on the
data (matrix) sizes input to the node and the target hardware (eg an
estimate of cost to move data between cores).For distributed-memory DLA, ﬁrst-order cost estimates are suf-
ﬁcient [17, 18, 19] to enable an expert to judge trade offs between
the cost of communicating data over a network and increasing par-
allelism that is enabled by that communication. Just as an expert
estimates efﬁciency when manually coding, DxTer does so automati-
cally by summing the estimated runtime of all nodes on a graph [19].
Then, the graph with the lowest estimated cost is converted to code
and output by DxTer.
3. BIG PICTURE
Classical development of an algorithm starts with a speciﬁcation
S, and through a series of design decisions – which we represent as
reﬁnements and optimizations – an algorithm a1is produced. The
derivation of a1is represented as a path from Stoa1(Figure 1(a)).
In general, a large space of algorithms may exist for S. This space
is encoded as a decision orderivation tree [3]; a tiny tree of three
derivations is shown in Figure 1(a). All of the derived algorithms
{a1;a17;a21} implement S, but do so differently and have different
performance. It is the task of a domain expert to intuitively navigate
this tree to ﬁnd the best performing algorithm for a given situation.
ܽଶଵܽଵ଻ ܽଵܽଵ଻
s
ܽଵܽଵ଻ܽଶଵs
ܽଵܽଵ଻ܽଶଵ(a) s
ܽଵܽଵ଻ܽଶଵ(b) s
ܽଵ(a)
ܽଶଵs (b) slower
performance
faster
performance
Figure 1: Tree of Algorithm Derivations from a Spec S.
As derivation trees go, Figure 1(a) is as typical as it is misleading.
Its arrows (transformations) suggest that all are of equal importance.
This is not the case – certainly not for DLA. Some arrows (design de-
cisions) make a bigdifference in the ultimate runtime performance
of an algorithm; most arrows merely tweak performance up or down.
Figure 1(b) more realistically emphasizes the importance of deci-
sions with respect to performance. The longer the arrow, the more
signiﬁcant is its impact on performance. The higher the algorithm’s
placement in Figure 1(b), the slower it executes (lower is better).
It is the responsibility of a domain expert to know intuitively the
arrows/decisions that are performance-signiﬁcant from those that
are not. This knowledge is rarely written down; it is an art mas-
tered by domain experts. As we demonstrate throughout this paper,
in DLA choices of how to parallelize operations (ie reﬁnements)
make a substantially greater impact on performance than subsequent
optimizations.
Note: The trees of Figure 1 are speciﬁc for a particular
context such as matrix size or architecture. Change the
context, algorithm a21may become the fastest. By no
means do the arrows of derivation trees have a ﬁxed
length or direction – this extra dimension of “context”
(egwhat are the inputs to the operations whose algo-
rithms are to be optimized) signiﬁcantly complicates
the job of an expert and any generative tool.
Suppose we could plot algorithms vsruntime on a graph. Points
along the x-axis are algorithms that are derived from spec S; the
y-axis indicates algorithm runtime where lower (faster) is better.
By sorting algorithms on the x-axis from poorest-performing to
best-performing, one would imagine a graph like Figure 2(a), which
has a smooth ﬂowing curve; the best algorithms are at the far right.
302(a)runtime
algorithm
(b)runtime
algorithm
(c)algorithmruntime
(d)algorithmruntime
distinct
stairs/
design
decisionsFigure 2: Shapes of Performance Space.
But a plot of actual (algorithm,runtime) pairs yields something
quite different: it is a discrete set of stairs as in Figure 2(b), where
each stair represents variations of a set of algorithms that share a
major design decision (eg a long arrow in Figure 1(b)). And like a
fractal [16], a single stair internally is a set of closely performing
stairs as in Figure 2(c), recursively, representing the progressively
smaller effects of less-important design decisions. Distinct stairs can
overlap as in Figure 2(d), where fundamentally similar reﬁnements
compete for the best performance. This is the shape of a performance
space that experts intuitively navigate – they use their intuition
about the relationship between design decisions and the stair(s)
whose algorithms they explore, even though they may be completely
unaware of the stair nature of their space.
As we are among the ﬁrst to be able to generate a space of al-
gorithms that implement a speciﬁcation (at least in DLA), we can
now “see” the entire performance space of which experts could only
glimpse a few points at a time. This enables us to connect speciﬁc
reﬁnements to overall algorithm performance.
The following sections document concrete instances of these ideas
in the domain of distributed-memory DLA.
4. DLA AND ELEMENTAL BACKGROUND
Elemental [24, 25] is a distributed-memory DLA library with
functionality similar to ScaLAPACK [6]. It includes implementa-
tions of common high-level functionalities such as matrix factoriza-
tion, eigensolvers, and solvers for systems of equations. Elemental
also includes an API of primitives that are used to implement these
high-level functionalities.
In prior work [17, 18, 19], we demonstrated how design knowl-
edge for Elemental is encoded in DxT. We start with interfaces that
represent sequential and architecture-agnostic DLA operations. We
encode Elemental-speciﬁc transformations that parallelize and opti-
mize algorithms. DxTer explores the implementation space for each
input graph (a “spec” in the vernacular of Section 3) to generate a
graph that references only primitives, which map to library calls.Elemental views the pprocesses in a computer cluster as a 2D
process grid (p=rc) ofrrows and ccolumns.2There is a
default way to distribute matrix data on the process grid. This is a
2D cyclic distribution, which maps matrix element (i;j)to process
(i%r;j%c). There are ten other distributions of interest; each has
its own notation (eg the default denoted by [MC;MR]) [24]. Elemental
uses a single-program, multiple-data (SPMD) programming model.
All processes run the same program, but have different portions
of the data on which they compute. The various data distributions
enable this.
DLA algorithms are largely loop-based. In each iteration, input
and output matrices are partitioned into submatrices. Loop-body
operations, or update statements, perform computation using the
submatrices and overwrite submatrices.
Loop updates are parallelized by redistributing data3from the
default [MC;MR]to other distributions in a way that enables computa-
tion to be performed in parallel across the pprocesses. The result is
then redistributed back to the default distribution as needed, possibly
with a reduction such as a summation of partial results.
Communication or data redistribution is expensive, but it enables
parallelism. An expert balances the overhead of communicating
data with improved parallelism. With larger matrices, the amount
of computation, generally O(M3)where Mis the matrix size, is much
greater than with smaller matrices. Communication, an O(M2)cost,
is less of a concern with large matrices, but it must still be chosen
carefully. Depending on matrix sizes, an expert chooses among dif-
ferent parallelization schemes which incur different communication
costs and provide different amounts of parallelism. For example,
an expert might choose redundant computation with small matrices
because the communication cost is reduced and more parallelism is
not needed for a small amount of computation. The heuristics we
study in this paper balance these details for particular operations.
Let’s take a look at some of the deep knowledge that experts
rely on when manually developing distributed-memory DLA al-
gorithms.4We start with Cholesky factorization, a comparatively
simple algorithm, and later examine the more complicated symmet-
ric positive deﬁnite (SPD) inversion and even more complicated
operations two-sided Trsm andtwo-sided Trmm.
Note: We realize that few readers are familiar with
DLA. We try to balance our discussions so that both
DLA experts and DLA novices can appreciate our re-
sults. Novices should notfocus on DLA details but
rather on named rewrites and the performance that us-
ing them brings.
Note: Although we (and our tools) derive DLA algo-
rithms by transformation, it is unclear whether experts
think in terms of transformations. The disconnect be-
tween algorithm derivation by machine and algorithm
invention by humans is an interesting subject we are
exploring and whose results are beyond the scope of
this paper.
2randcare parameters tuned at runtime based on how many processes are
available for execution. Their choice does not affect which ﬁnal implementa-
tion code is used.
3Redistribution is implemented with collective communication found in the
Message Passing Interface (MPI) [29]. The cost of collective communication
is estimated by the lower-bounds given in [7].
4The knowledge we present largely came from backwards engineering
existing code in terms of reﬁnements and optimizations [19, 18, 17].
303Algorithm: A:=CHOL_BLK(A)
Partition A!ATL ?
ABL ABR
where ATLis00
while m(ATL)<m(A)do
Determine block size b
RepartitionATL ?
ABL ABR
! A00 ? ?
A10 A11 ?
A20 A21A22!
where A11isbb
Variant 1 Variant 2 Variant 3
A10:=A10TRIL(A00) T(TrsmRLT)
A11:=A11 TRIL(A10AT
10) (SyrkLN)
A11:=CHOL(A11) ( Chol)A11:=A11 TRIL(A10AT
10) (SyrkLN)
A11:=CHOL(A11) ( Chol)
A21:=A21 A20AT
10 (GemmNT)
A21:=A21TRIL(A11) T(TrsmRLT)A11:=CHOL(A11) ( Chol)
A21:=A21TRIL(A11) T(TrsmRLT)
A22:=A22 TRIL(A21AT
21) (SyrkLN)
Continue withATL ATR
ABL ABR
  A00 ? ?
A10A11 ?
A20A21 A22!
endwhile
Figure 3: Three algorithms for computing the Cholesky factorization. m(A)stands for the number of rows of Aand TRIL(A)indicates
the lower triangular part of A. The ‘?’ symbol denotes entries that are not referenced. The Partition, Repartition, and Continue with
operations effectively partitioning the matrix Ainto submatrices, the boundaries of which change with each iteration of the loop.
These details are unimportant as the loop body is our focus.
5. CHOLESKY FACTORIZATION
5.1 Parallelizing Cholesky
Cholesky factorization is an algorithm to compute the Cholesky
factor Lfor a symmetric, positive-deﬁnite (SPD) matrix Asuch that
A=LLT. Here, Ais overwritten by Lto reduce memory use. There
are ﬁve reﬁnements of Cholesky used in Elemental and in this paper.
Three map a Cholesky operation on a large matrix Ato a series of
Cholesky operations on smaller submatrices. These reﬁnements are
called (for historical reasons) variants [30]. The fourth and ﬁfth
reﬁnements are different: one is a primitive that directly implements
Cholesky; the other rephrases a Cholesky operation in terms of
another (Cholesky) operation that uses a different (matrix) data
distribution. We call this latter a rephrasing reﬁnement. Figure 4
sketches the basic Cholesky rewrite rules that we use in this paper.
Chol′ܣܣ Variant1′ܣܣ 
Variant2′ܣܣ 
Rephrasing′ܣܣ Variant3′ܣܣ 
A A’ A A’Primitive′ܣܣ 
Figure 4: Cholesky Rewrites Used in Elemental.
Now, let’s look closer. Figure 3 shows the three variant reﬁne-
ments for Cholesky. Each algorithm computes the same factor, but
does so with different computation (updates) in the loop. In paren-
theses, we show the names given to each individual update. A DLA
expert will recognize them. For everyone else, they are simply DxT
interface nodes.
Each update is a commonly-encountered DLA operation, so it
is represented by an interface in DxT. The updates in Variant 3,
for example, are Chol ,TrsmRLT , and SyrkLN , from top to bottom.SyrkLN can have multiple parallelized implementations, but only
one is considered in this case because of the structure of the operands
(we will not detail this low-level domain knowledge). Chol has
only one parallelization scheme in Elemental (one reﬁnement to
parallelize the loop body).
Notice there is recursion in the Cholesky variant reﬁnements
of Figure 3 – all have a Chol operation in the loop body. This
is expected, as mentioned above, since variants map a Cholesky
operation on a large matrix to a Cholesky operation on smaller (sub)-
matrices. These “smaller” Cholesky operations can be implemented
as a primitive or by a reﬁnement that rephrases the operation using
another Cholesky operation that expects a different data distribution
(ieparallelizes it for distributed memory). Figure 5 shows one
such reﬁnement that rephrases Chol intoLChol , an operation that is
later transformed with an implementation that calls a Chol function
provided by an external library. In a Elemental, one does not use
a variant implementation for this “smaller” Cholesky operation
because the result would be an even-smaller Cholesky with matrix
sizes that are too small – communication overhead would be too
great.5One uses a variant implementation and then parallelizes the
loop-body operations with rephrasing reﬁnements.
Chol′ܣܣ Variant1′ܣܣ 
Variant2′ܣܣ 
nonVariant′ܣܣ Variant3′ܣܣ 
A A’ A A’
Figure 5: A Rephrasing of Chol Operation in Terms of LChol.
Note: Each iteration of a variant’s loop completes be-
fore the next iteration starts, so there is no parallelism
between iterations. Within each iteration, parallelism is
attained by distributing the loop-body operations’ com-
putation across the process grid. The abstractions used
5Recursive applications of variant reﬁnements doarise in other DLA
libraries (e.g., BLIS [31]), but notin Elemental.
304in Elemental (shown in our rephrasing transformations)
hide this parallelism somewhat. Each box is executed
on all processes in parallel and different data distribu-
tion choices determine which processes work on which
portion of the overall computation.
LTrsmRLTX[MC,MR]→[π,*][MC,MR]→ [*,*][π,*]→[MC,MR]TrsmRLTX'X'TrsmRLTLXL
Figure 6: TrsmRLT reﬁnements with P2fV C;VR;MC;MR;g.
In contrast to the single rephrasing options for SyrkLN andChol ,
TrsmRLT has ﬁve reﬁnements. Figure 6 shows them in a templatized
form. The templatization parameter Pis instantiated such that [P;]
represents an Elemental distribution. Boxes labeled A!Btransform
data in distribution Ato distribution B.LTrsmRLT is a primitive that
performs a local computation in parallel across processes (since that
box is executed on all processes).
The choice of Pleads to different amounts of communication and
parallelism. An expert explores these options based partly on the
problem size and knows (from experience looking at cost estimates
repeatedly) that P=VCorVRis best for large problem sizes6since
they lead to parallelism where each process performs a different
portion of the computation. P=MCorMRresults in a process row or
column performing redundant computation, with different process
rows or columns computing different portions in parallel. Due to the
amount of computation, these are good choices for a moderate but
not large amount of data. A choice of P=results in all processes
computing the same result, so it is best when there is a small amount
of data.
We call this the Trsm heuristic: choose P=VCorVRfor large
problem sizes, P=MCorMRfor medium problem sizes, and P=
for small problem sizes. We will see the correlation of this heuristic
to performance stairs in the next section.
Note: We name and summarize two heuristics in this
paper. There are similar heuristics that apply to each in-
terface of the widely-used level-3 Basic Linear Algebra
Subprograms (BLAS3) API [8, 18]. All of these heuris-
tics are similar to the two we name here, all form stairs
like those we describe below, and all work with the
generalized heuristic we present in Section 6.2. Trsm ,
for example, has another heuristic very similar to the
one we show that is used for a different ﬂavor of the
operation (B :=TRIL(L) TBorTrsmLLT).
Besides parallelizing computations, different values of Penable
different optimizations to be subsequently applied. We now illustrate
a simple (and rather typical) optimization that could be applied
depending on the Pvalue chosen for the Trsm reﬁnement. An expert
explores alternate ways to implement one redistribution as a series
of others or to remove redundant redistributions as in the templatized
optimization of Figure 7. More optimizations and reﬁnements are
given in [17, 18, 19].
For large problem sizes, the Trsm heuristic tells us to choose VC
orVRto reﬁne TrsmRLT . The choice between these two depends on
what optimization can be subsequently applied given how data is
already distributed.
6The idea of what is “large” or “small” is rough and depends on the number
of processes, so we do not provide speciﬁc numbers here.
AΩ→∑Ω→∑BCAΩ→∑BCFigure 7: Templatized optimization to remove a redundant
W!Sredistribution.
Note: We do not detail optimizations further because
they are unimportant to understand the structure of the
DLA (implementation performance) space. Opti-
mizations can incrementally improve performance, but
reﬁnements make more of an impact.
5.2 Cholesky Stairs
We use k-means clustering, explained below, on DxTer’s explic-
itly enumerated search space to reveal the performance stairs of
Cholesky algorithms. We identify stairs with known heuristics that
limit the implementations that an expert would explore.
5.2.1 Variant 3
Figure 8 shows the performance of all Variant 3 Cholesky im-
plementations ordered by performance7and numbered along the
horizontal axis. The vertical axis predicts performance in cycles, so
lower is better.8Notice that all graphs use a logarithmic scale on the
vertical (runtime) axis: there is a factor of 33 difference from the
worst algorithm (far left) to the most efﬁcient algorithm (far right).
Stair1Stair2Stair3
Figure 8: Predicted performance of the Cholesky Variant 3 im-
plementations, clustered with k=3.
k-means clustering partitions data (implementation costs here)
intokclusters to minimize åk
i=1åxj2Cikxj µikwhere Ciis the
ithcluster, xjis the jthpiece of data in Ci, and µiis the mean
of the data in Ci. We used 3-means clustering in Figure 8. The
implementation clusters (henceforth “stairs”) that arise are linked to
the three option groups for reﬁning TrsmRLT.
DxTer keeps track of the transformations used to generate a given
implementation, so we can look at which transformations are com-
mon to all implementations within a stair. For Stair 1, all imple-
mentations use the TrsmRLT reﬁnement with P=. The Trsm
heuristic tells us this reﬁnement is the worst for large problem sizes –
which Figure 8 clearly indicates. There are four implementations in
this stair with varying performance due to optimizations that were
applied.
7DxTer implements and optimizes an algorithm given a speciﬁc problem;
we use 80,000 here, which is large.
8Predicting the number of cycles does not necessarily reﬂect the actual
number of cycles at runtime, but rather is an estimate to enable ordering.
305This is an important feature of distributed-memory DLA: reﬁne-
ment choices impact performance greatly while optimizations simply
tweak performance after reﬁnement decisions have been made.
123456789101010.351010.361010.371010.381010.391010.4Cost Estimate (Cycles)
Implementation Number∏=VR∏=VCWorst implementation
Best implementation
Figure 9: Close-up of Stair 3 from Figure 8.
Similarly, Stair 2 contains implementations that use P=MCorMR,
and Stair 3 contains implementations with P=VCorVR. Figure 9
shows these “micro-stairs” within Stair 3, where P=VRleads to
two implementations and P=VCleads to eight. Again, there is
lower-order cost variation within stairs due to optimizations that are
subsequently applied.
5.2.2 Variant 2
Cholesky Variant 2 also has a TrsmRLT interface that is again best
reﬁned (for large problem sizes) using P=VCorVR, as prescribed
by the Trsm heuristic. The third update ( A21:=A21 A20AT
10) is a
GemmNT operation that can be reﬁned (parallelized) in one of three
ways, shown in Figure 10.
DGemm NTBACC'[MC,MR]→[*,MR]LGemmNT
DGemm NTBACC'[MC,MR]→[MC,*][MC,MR]→[MR,*]LGemmNTTemp[MC, *]SumScatterDGemmNTBACC'DGemm NTBACC'LGemmNTTemp[*, MC]SumScatter[MC,MR]→[*, MR](a)(b)
(c)
Figure 10: Reﬁnements of GemmNT.
Experts use the Gemm heuristic: Whichever input matrix is largest
(A,B, orCin Figure 10) should be kept stationary, meaning it stays
in the default distribution. This limits their options to only one
reﬁnement.
For the third update of Cholesky Variant 2, A20is much larger
than the other operands, so an expert employs the Gemm heuristic
to keep A20stationary. The reﬁnement of Figure 10 (c) does this;
the others redistribute A20.
Figure 11 shows the predicted performance of all Variant 2 im-
plementations, clustered with k=6. Stairs 4-6 all use the GemmNT
reﬁnement that keeps the largest matrix ( A20) stationary (as pre-
scribed by the Gemm heuristic). Stairs 1-3 have implementations
that use one of the other two reﬁnements that violate the Gemm
heuristic. Further, within Stairs 1-3 and Stairs 4-6, the difference
among the three stairs in each is due to which TrsmRLT reﬁnement
is used (where an expert would employ the Trsm heuristic).
Stair
1Stair
2Stair
3
Stair
   5
Stair   6Stair
4Figure 11: Predicted performance of the Cholesky Variant 2
implementations, clustered with k=6.
This is a great example of how two independent heuristics “com-
pose”. The GemmNT reﬁnement tells an expert to limit his consider-
ation to only Stairs 4-6 and then the TrsmRLT reﬁnement tells him
to limit consideration to Stair 6. These two orthogonal decisions
reduce the search of an implementation space considerably – to
one-tenth of the entire space.
The Cholesky update (the second update of Variant 2) has only
one reﬁnement. The ﬁrst update ( A11:=A11 TRIL(A10AT
10), called
SyrkLN , has four reﬁnements. Because this operation does not
account for much computation (ie smaller orders of magnitude than
theGemmNT andTrsmRLT updates), the choice between these four
makes little impact on performance. This means the four choices do
not form four clearly visible stairs within the six stairs of Figure 11,
but rather look as if they all belong to a single stair.
5.3 Bringing All Variants Together
Figure 12 shows predicted (implementation performance) space
for all three Cholesky variants (294 implementations total). Notice
that the worst performing implementation runs for an estimated
13,842-times as many cycles as the best.
All Variant 1 implementations are in Stairs 1-3 (the worst per-
forming). Variant 1 is known to perform badly because none of the
ﬁveTrsmRLT reﬁnement parallelize well with a large Linput, which
is what Variant 1 uses.
The remaining stairs (4-10) come from the union of Variant 2 and
3 implementations (ie the overlaying of Figures 8 and 11).
Stair1 - 3Stair4 - 7Stair8Stair10Stair9
Figure 12: Predicted performance of all Cholesky implementa-
tions, clustered with k=10.
306Stair 10 contains 50 implementations which are a union of Variant
3 implementations of Stair 3 in Figure 8 and Variant 2 implementa-
tions of Stair 6 in Figure 11. In other words, a decision between im-
plementing Variant 2 or Variant 3 is not immediately obvious.There
is no heuristic to tell an expert quickly that Variant 2 or 3 is better;
he must explore both. This graph shows why: performance of the
best implementations of each variant are similar.
6. SPD INVERSION
6.1 Combinatorial Explosion
While Cholesky factorization is a prototypical DLA operation,
it is relatively simple compared to most operations supported by
Elemental. For example, SPD Inversion [5] takes an SPD matrix
A, computes 1) its Cholesky factor L, 2)L 1, and then 3) LTL(Ais
overwritten with the result of each of these three operations). Each
operation can be implemented independently. For example, one can
use any implementation of Cholesky from the previous section for
the ﬁrst operation of SPD Inversion. However, an expert can do
better. By choosing the right algorithmic variant for each operation,
an expert can fuse loops [5, 20]. This enables further optimization
on the fused loop body, shown in Figure 13 with interface names
in parenthesis. DxTer fuses loops automatically as an optimization
transformation when it is legal to do so [20].
A11:=Chol( A11) ( Chol)
A01:=A01A 1
11 (TrsmRLT)
A00:=A00+A01AT
01(SyrkLN)
A12:=A T
11A12 (TrsmLLT)
A02:=A02 A01A12(GemmNN)
A22:=A22 AT
12A12(SyrkLT)
A01:=A01A T
11 (TrsmRLT)
A12:= A 1
11A12 (TrsmLLN)
A11:=A 1
11 (TriInv)
A11:=A11AT
11 (Trtrmm)
Figure 13: SPD inversion loop body.
There are two TrsmRLT updates and two TrsmLLT updates, which
have similar reﬁnement options and stairs; the Trsm heuristic applies
toTrsmLLT , too. For these operations alone, there are 54=625re-
ﬁnement combinations from the Pinstantiation options. Then, there
are reﬁnements for the other loop-body operations and a combina-
torial increase in the search space when optimizations are applied.
Further, there are multiple algorithmic variants explored for each of
the three operations and combinations of partially- and non-merged
implementations.
In short, the result is a implementation space that is far too large
for an expert developer to explore completely. Similarly, it is too
large for DxTer to form explicitly, so we had to develop a way to
limit it as an expert might do. For example, we know from the Trsm
heuristic that only two options ( P=VCandVR) are reasonable for
each of the four Trsm operations, so an expert would only consider
24=16of the 625 reﬁnement combinations for Trsm alone. We
want DxTer to exploit the stairs of the performance space to limit
the space similarly.
6.2 A Search Heuristic of Our Own
As we saw above, reﬁnements lead to a particular stair of im-
plementations while optimizations only tweak performance within
the stair. We want DxTer to limit consideration to a few stairs with
high performance and then explore optimized versions within those
stairs.There are two basic and disjoint ﬂavors of distributed-memory
DLA reﬁnements of interest here. The ﬁrst chooses an algorithmic
variant (eg one of the three for Cholesky). We call those variant
reﬁnements. These are the initial decisions on how to partition the
input matrices and how to structure the loop-body computation. We
have 54 of these reﬁnements encoded in DxTer.9
After a variant reﬁnement is chosen, the loop-body updates must
be implemented for distributed-memory hardware. The second
type of reﬁnement encodes these choices of how to implement
loop-body updates. We call those rephrasing reﬁnements. These
are architecture-speciﬁc (distributed-memory) ways to implement
a loop-body computation by distributing data and computing in
parallel. We have 53 of these reﬁnements encoded in DxTer. We now
explain how we characterize reﬁnements as one of these two types
and use a heuristic to signiﬁcantly limit the number of rephrasing
reﬁnement options that need to be explored.
As we saw with Cholesky factorization in Section 5.3, we cannot
always determine a priori which variant reﬁnements are bad or good
because one has to apply some rephrasing reﬁnements to judge
an implementations’ performance. For example, an expert cannot
quickly glance at Cholesky Variants 2 and 3 and say one will be
better; one has to explore implementation details for each. There
are exceptions, with one mentioned in the next section, where a
variant requires a large amount of extra computation. Such always
bad reﬁnements are simply not encoded in DxTer.
For rephrasing reﬁnements on the other hand, experienced de-
velopers limit their consideration of parallelization schemes. The
Gemm and Trsm heuristics, for example, do so based on what we
quantiﬁed when looking at the implementation space with Cholesky:
cost estimates point to good and bad sets of rephrasing reﬁnements.
We can leverage this to create a heuristic to limit the number of
rephrasing reﬁnements DxTer explores.
When DxTer ﬁnds multiple rephrasing reﬁnements that apply to
a particular interface, it estimates the cost of each right-hand side
(RHS) graph that would be used as an implementation.10It then
ranks all RHS reﬁnements and applies (explores) only the nbest
options. This is a local decision of which nare best, not considering
optimizations that can be applied subsequently to the RHS when part
of the surrounding implementation graph. We henceforth call this
thelocally n-best (L nB)heuristic. It is a generalization of the Gemm
andTrsm heuristics, where one rank-orders rephrasing reﬁnements,
respectively, and explores only the best n.11Doing so limits the
search to particular stairs in the implementation space. This is how
we exploit the stair structure of the domain in DxTer to make the
search space tractable for complex operations like SPD Inversion.
6.3 L nB Heuristic Results
We evaluated the L nB heuristic in two ways:
1.Does the implementation space become tractable for compli-
cated operations like SPD Inversion and does it get smaller
and faster to search for simpler operations like Cholesky?
2.Is the output generated by DxTer still the same or better than
what an expert would code by hand?
9For perspective, we have 703 optimizations, most of which come from
instantiating a template optimization on various data distributions.
10This requires input matrix sizes. As interfaces can be in loops, this analysis
is done for each time the code is run across all iterations of the loop. All of
the costs are summed.
11Setting n=1,n-best reduces to a Greedy heuristic, discussed later in
Section 7.3.
307For all test operations we have studied [17, 18, 19], the L nB
heuristic succeeds with respect to both metrics (we re-ran previous
studies using L nB).
For Cholesky, with a heuristic value of n=2the search time goes
from over two seconds to under one second. The search space is
reduced from 294 implementations to 62. The output code is exactly
the same as when the heuristic is not used.
The L 2B reﬁnements obey the Trsm heuristic, so only P=VC
andVRare explored. For Gemm , this heuristic limits DxTer to explore
what is prescribed by the Gemm heuristic and one additional reﬁne-
ment (since the Gemm heuristic only prescribes the single locally
best reﬁnement). Therefore, DxTer does not generate implementa-
tions that come from bad Trsm reﬁnements, for example, and only
some of the implementations in the stairs with bad Gemm reﬁnements
(from the three options, it omits the worst Gemm reﬁnement and
includes the second-worst).
Figure 14 shows the implementations generated. The L 2B heuris-
tic limits the search space to three stairs that are a subset of the
stairs in Figure 12 (Stairs 3, 7, and 10) and with the expected fewer
implementations.
0 10 20 30 40 50 60 70101010111012101310141015Cost Estimate (Cycles)
Implementation Number
Figure 14: Cholesky implementations with only locally 2-best
reﬁnements explored.
Without this heuristic, the implementation space for SPD Inver-
sion is simply too large to generate. After one day of running, DxTer
generated over 50 million implementations and was still forming
more. With n=3and the L nB search heuristic activated, 3,257,506
implementations are generated in 496 seconds, and the output is the
same as developed by hand (ie the same Elemental users run in their
applications).
With n=2, only 107,986 implementations are generated in 181
seconds, and, again, the same code is output as best. The heuristic
limited the implementation search space to make it tractable and
quickly formed. For example, this limits the options for the four
Trsm interfaces to 24combinations one would consider in manual
development instead of the full 54.
Figure 15 shows the implementations generated with n=2. Even
with fewer rephrasing reﬁnement options, stairs are still formed, but
they are smaller and there are many fewer of them. Almost all variety
of these implementations comes from different combinations of
optimizations. The L 2B heuristic limits consideration to a few stairs,
where many similarly-performing implementations are generated
with similar parallelization and different optimizations.
Indeed, this heuristic exploits DLA performance stairs to limit the
number of generated implementations considerably. This enables
DxTer to generate the same implementations as a person does man-
ually, but much faster (for SPD Inversion, DxTer code generation
took minutes instead of hours of manual development and testing).
0 2 4 6 8 10 12
x 104101010111012101310141015Cost Estimate (Cycles)
Implementation NumberFigure 15: Cost of SPD Inversion implementations generated
with locally 2-best heuristic.
7. TWO-SIDED PROBLEMS
Other DLA operations also lead to intractably large search spaces.
We now look at two more complex operations to see that the the
L2B heuristic is again necessary and effective. Two-sided Trsm and
two-sided Trmm are similar operations used as preprocessors for the
generalized eigenvalue problem [25]. Each has four or ﬁve variants
to explore.12The loop body of one variant is shown in Figure 16 to
convey its complexity. Our L nB heuristic is again vital to making
the search space tractable.
A10:=L 1
11A10 (TrsmLLN)
A20:=A20 L21A10 (GemmNN)
A11:=L 1
11A11L T
11 (Two-sided Trsm)
Y21:=L21A11 (SymmRLN)
A21:=A21L T
11 (TrsmRLT)
A21:=W21=A21 1
2Y21 (Axpy)
A22:=A22 (L21AT
21+A21LT
21) (Syr2kLN)
A21:=A21 1
2Y21 (Axpy)
Figure 16: Variant 4 of two-sided Trsm.
7.1 Limiting the Locally n-Best Heuristic
In [17], we demonstrated how searching just two of the 4-5 vari-
ants of each two-sided operation leads to a massive search space.
We presented ways to reduce the search space by restructuring some
transformations. We also showed how limiting the ways one inter-
face (called Axpy ) is reﬁned reduces the search space by 100-fold
or more13. We now quickly review this Axpy heuristic and explain
how it compliments our L nB heuristic. The Axpy heuristic leads us
to a guideline for the interfaces to which the L nB heuristic does not
apply well (one could think of this as a meta-heuristic to tell when
the L nB heuristic applies and does not).
Axpy is aO(M2)operation on O(M2)data. This means the amount
of computation performed is roughly the same as the amount of
communication required (ie O(M2)data is redistributed and used for
computation). In such cases, optimizations are as important as the
rephrasing reﬁnement choice for interfaces. Choosing a locally-
suboptimal parallelization of Axpy can be best when a subsequent
12One variant of two-sided Trmm’s ﬁve is omitted because it has an ex-
traO(M3)computation, which means it is always bad no matter the target
hardware architecture
13The previously presented heuristics are important, but insufﬁcient for ex-
ploring all variants of two-sided operations. The L nB heuristic presented
here in conjunction with previous heuristics is sufﬁcient.
308optimization removes the O(M2)redistribution cost. The heuristic
presented in [17] caters to this case, so Axpy reﬁnements are limited
to those that can be followed by an optimization to remove redistri-
bution. We expect a generalization of this for other interfaces with
similar reﬁnement versus optimization cost impact.
For such interfaces, we do not use the L nB heuristic to limit
reﬁnement options because locally-bad choices can be globally best.
Therefore, the L nB heuristic’s local view of the graph is ineffective
when the best choice depends on available optimizations. Interface
reﬁnements are marked in DxTer to specify if the L nB heuristic is
used to limit reﬁnement options that are explored.
Optimizations affect the cost of communication by removing re-
distribution operations (eg as in Figure 7). The RHS of rephrasing
reﬁnements consists of a computation component (from computa-
tion boxes) and a communication component (and the total cost
is the sum of these components). When the computation compo-
nent’s cost is the same order of magnitude as the communication
component’s cost, then optimizations can make locally-suboptimal
reﬁnements globally best – the L nB heuristic does not work. When
the computation component is larger by an order of magnitude,
lower-order optimizations are less important in the overall design
than achieving good parallelism, local choices are sufﬁcient. This
is demonstrated by the stairs in previous sections, where steps are
deﬁned by rephrasing choices and optimizations affect where an
implementation is in the step. This provides an understanding, using
cost estimates, of expert heuristics when exploring Axpy and other
interfaces with similar cost structures.
There are many similar cases to this in distributed-memory DLA
(where computation and communication cost have the same order of
magnitude). The L nB heuristic does not apply in such cases, but it
is still applicable to most interfaces we have studied in the past [17,
18, 19], where generally computation is O(M3)and communication
isO(M2).
7.2 L nB Heuristic Results
Without L nB, DxTer runs out of 96GB of memory after a day of
execution in generating two-sided Trmm code. Once again, we need
to limit the search space to only the best implementations. We do so
with the L nB heuristic to omit the worst stairs from consideration.
With the L 3B heuristic, DxTer generates 37,200 implementations
after 715 seconds. With n=2, DxTer generates 1,982 implementa-
tions after 47 seconds. See Figure 17.
0 1000 2000 3000 4000 5000 6000 7000 8000 9000101010111012101310141015Cost Estimate (Cycles)
Implementation Number
Figure 17: Cost of two-sided Trsm implementations generated
with locally 2-best heuristic.
In both cases, the output DxTer generates for each operation is
the same (about 40 lines of code). In fact, as reported in [17, 19],the output is slightly better than what was manually developed, but
the time to generate code is signiﬁcantly reduced with the heuristic.
The Elemental expert implemented these operations before cre-
ating an optimization to improve the way data is redistributed. He
forgot to re-optimize these operations. Since the optimization was
added to DxTer, it is automatically applied to any generated code,
including the two operations.
In such cases, it is even more important to speedup implementa-
tion space generation. When an expert thinks of a new optimization,
he can add it to and task DxTer with reimplementing large amounts
of library code automatically. It would consider where the new
optimization applies to existing code. This is much easier and less
error prone than doing so manually.
If implementation space generation takes a long time for each
operation, regenerating a library of code would be painfully slow.
As our heuristic reduces search time for each operation, library
generation time is also reduced.
7.3 Why Not Fully Greedy?
One might ask what happens when L nB has n=1. This would
be a purely greedy search, where only the locally-best option is
explored. The answer is that DxTer generates worse ﬁnal implemen-
tations. With n=2, the best stair for each of the operations we have
studied includes implementations with each of the L 2B rephrasing
reﬁnements. Sometimes the locally-best option is globally subop-
timal because optimizations make a difference between which of
the two options is best within a stair – their computation is roughly
the same so performance difference comes from communication,
which is optimized differently. In other words: when a stair is
formed by two (or more) different reﬁnements, optimizations make
the difference within the stair, so more than one reﬁnement option
must be included .
This is similar to Axpy , discussed in Section 7.1. In both cases, op-
timizations make the difference in which of the similarly-performing
reﬁnements is best. For all operations we have studied, n=2is
the lowest value that results in the same implementations. For
distributed-memory DLA reﬁnements, this value is the size of the
smallest selection of reﬁnements for each interface such that opti-
mizations make the different of which choice is best. For this group
(iethe locally 2-best), the RHS costs are the same or similar because
theO(M3)computation is parallelized to the same degrees. Then, like
with Axpy , optimizations on communication are the differentiating
design decisions.
When using DxTer with another domain, we expect the L nB
heuristic to be useful when there are stairs in the implementation
space. We expect the value of nto be determined by the size of the
smallest set of locally-chosen reﬁnements for which optimizations
differentiate the globally-best solution, but demonstrating this is
future work.
8. RELATED WORK
The grandfather (origin) of DxT and DxTer can be traced to rule-
based query optimization (RBQO) [15] in the late 1980s. DxT and
DxTer generalize RBQO to the domain of DLA. Not surprisingly,
DxT is closely related to many projects, as detailed in [17, 19]. We
discuss below those that are related with respect to DxTer’s search
and the locally n-best heuristic.
Model Driven Engineering (MDE) is a basis for our work. Plat-
form Independent Models (PIMs), like our interface-only graphs,
represent functionality that is transformed (reﬁned) into architecture-
speciﬁc (primitive-only) graphs or Platform Speciﬁc Models (PSMs)
[10, 13]. Optimizing transformations, not particularly prominent in
MDE, are essential in DxTer to squeeze out the last bit of perfor-
309mance. Optimizations also complicate the implementation search
process. Without optimizations, a purely greedy search of reﬁne-
ments would be sufﬁcient because locally suboptimal choices are
always globally suboptimal as well.
It is common for software generation projects [2, 4, 26, 32] in
scientiﬁc computing to apply rewrite rules similar to our transfor-
mations to search a space of high-performance implementations.
Because of the nature of their application domain, cost estimates are
not accurate enough to judge an implementation’s expected perfor-
mance; the code must be compiled and run on sample data.14As a
result, these projects often use machine learning techniques to limit
the search space based on empirical data. Such techniques could be
used in conjunction with our L nB heuristic to limit the search space
more if it every becomes too large with future algorithms. DxT and
these other software generation projects borrow many search ideas
from artiﬁcial intelligence techniques [14] and will likely incorpo-
rate or apply more existing algorithms as needed with new domains
in the future.
The DxTer search process is closely related to path planning,
where one plans a path through some space for which there is a cost
associated with portions of the path. With DxT, we search design
choices – path segments – that have positive and negative weights.
Our goal is to ﬁnd a path that ends in a primitive-only implementa-
tion that performs well (ie the path is low-cost). The work of [22, 23]
shows how subspaces of decisions / paths can be ignored, or pruned,
to limit consideration. In DxTer, simpliﬁers are optimizations that
are always applied because they reduce an implementation’s cost
and it is never worth exploring implementations that do not have
the transformation applied because they always perform worse [17].
This is an example of a dominance relationship [22]. Loosely, this
means the subspace of implementations that result from applying
the simpliﬁers dominate – or always have lower cost than – the
subspace of implementations without the simpliﬁer applied. Opti-
mizations are manually identiﬁed as simpliﬁers in DxTer. The L nB
heuristic is meant to identify dominating rephrasing reﬁnements in
DLA automatically and only explore those.
The idea of engineering heuristics based on only a few decisions
of signiﬁcance is not new [21, 33]. Similarly, for artiﬁcial intelli-
gence problems, heuristics are widely used [1]. Here, we used a
program generation system to discover the important decisions in
DLA and then utilize them for search space reduction. As heuristics
are important in other domains, we believe our approach can be
applied elsewhere.
Lastly, recent work on feature-based program generation [12]
may reveal a counterpart to our work. Namely, how a few decisions
of features (instead of rephrasing and optimization choices) predict
the performance of a program conﬁguration accurately. In [28], a
heuristic is presented to limit the space of conﬁgurations that must be
sampled for performance, limiting to the most important decisions.
They assume limited interactions between choices and mainly pair-
wise, lower-order interactions. For DLA this can sometimes apply,
as we have developed the L nB heuristic to apply to such cases, but
the Axpy heuristic shows that it is not always the case. In both
our work and theirs, the unit of modularity is a transformation.
DxT transformations manipulate algorithms, while feature-based
program generation uses larger transformations that manipulate
layers of systems.
14For our code, empirical test are not feasible because runtime is too large and
the machines are ﬁnancially expensive to use. Further, distributed-memory
DLA cost estimates are accurate enough to judge implementations well.9. CONCLUSION
Explicitly enumerating the (implementation performance) space
for DLA operations allows us to give scientiﬁc meaning and justiﬁ-
cation to the heuristics that DLA experts have used for years. These
heuristics have made experts effective in navigating a large number
of design decisions to produce highly-efﬁcient code without taking
days to analyze options.
The Trsm and Gemm heuristics (and others that we do not detail
here but are similar) now make sense when analyzing the structure
of the (implementation performance) space by looking at the
stairs and the design choices that yield them. This view helps us
understand heuristics that experts have used and explain or motivate
them with cost estimates. They are no longer rough design “rules of
thumb” passed down by generations of expert developers.
Further, we have shown how performance stairs enabled us to de-
velop the locally n-best (L nB) heuristic that reduces the time it takes
to generate a space of implementations or makes it tractable for com-
plicated algorithms without sacriﬁcing the performance of generated
code. Basically, we have abstracted many of the heuristics DLA
developers use to exploit the implementation space’s stair structure
(though we only detail two here). Instead of taking days to generate
complicated code (or not completing given system resources), it
takes minutes. This is an important step to make automated code
generation a useful design tool for expert developers. In fact, au-
tomatically generated code improved on hand-developed code for
two-sided Trmm (and other operations), so some generated code is
now used in the Elemental library.
We are in a rare position to enumerate the implementation space
of a speciﬁcation and analyze the performance of its members. As
automatic code generation becomes more popular (and necessary
with more complicated hardware and software), such a beneﬁt must
be further exploited. We expect other domains to have similar
structure to that of DLA and to similarly beneﬁt from heuristics
based on the stair stepping structure. We believe this is especially
true in scientiﬁc computing domains, which have often taken design
cues from DLA in the past. We will study other domains as part of
our future work.
Finally, while we have not studied the pedagogical value of iden-
tifying stair steps, we see potential. When teaching how to engineer
distributed-memory DLA software to a novice, stairs demonstrate
that reﬁnements must be well understood to make the best choices
and then optimizations are a less important lesson. This gives us a
partial order of lessons based on their importance on performance.
Also, when examining a student’s implementation, we can explain
why his/her choices lead to a lower performing solution than the best
choices. We can identify to which stair the implementation belongs
and, therefore, which set of reﬁnements were chosen incorrectly,
leading him/her to a suboptimal stair. We can explain “you should
have chosen reﬁnement ref1 instead of ref2 because it leads to
better performance in this algorithm.” Exploiting the pedagogical
beneﬁts to identifying stairs and being able to enumerate the search
space (or important parts of it with the L nB heuristic) is future work.
Acknowledgements. We gratefully acknowledge support for this
work by NSF grants CCF-0724979, CCF-0917167, and ACI-1148125.
Marker held fellowships from Sandia National Laboratories and the
NSF (grant DGE-1110007).
Any opinions, ﬁndings and conclusions or recommendations ex-
pressed in this material are those of the author(s) and do not neces-
sarily reﬂect the views of the National Science Foundation (NSF).
31010. REFERENCES
[1] S. Amarel. Program synthesis as a theory formation task:
Problem representations and solution methods. In Machine
Learning: An Artiﬁcial Intelligence Approach: Volume II,
pages 499–569. Kaufmann, Los Altos, CA, 1986.
[2] A. Auer, G. Baumgartner, D. Bernholdt, A. Bibireata,
V . Choppella, D. C. an ˜d X. Gao, R. Harrison,
S. Krishnamoorthy, S. Krishnan, S. Lam, Q. Lu, M. Nooijen,
R. P. añd J. Ramanujam, P. Sadayappan, and A. Sibiryakov.
Automatic code generation for many-body electronic structure
methods: The Tensor Contractio n Engine. Molecular Physics,
2005.
[3] I. D. Baxter. Design Maintenance Systems. CACM, April
1992.
[4]G. Belter, E. Jessup, I. Karlin, and J. G. Siek. Automating the
generation of composed linear algebra kernels. In SC, 2009.
[5] P. Bientinesi et al. Families of algorithms related to the
inversion of a symmetric positive deﬁnite matrix. ACM Trans.
Math. Softw., 35(1):1–22, 2008.
[6]L. S. Blackford, J. Choi, A. Cleary, E. D’Azevedo, J. Demmel,
I. Dhillon, J. Dongarra, S. Hammarling, G. Henry, A. Petitet,
K. Stanley, D. Walker, and R. C. Whaley. ScaLAPACK Users’
Guide. SIAM, 1997.
[7] E. Chan, M. Heimlich, A. Purkayastha, and R. van de Geijn.
Collective communication: theory, practice, and experience:
Research articles. Concurrency and Computation: Practice &
Experience, 19(13):1749–1783, Sept. 2007.
[8] J. J. Dongarra, J. Du Croz, S. Hammarling, and I. Duff. A set
of level 3 basic linear algebra subprograms. ACM Trans. Math.
Softw., 16(1):1–17, Mar. 1990.
[9] J. Feigenspan, D. S. Batory, and T. L. Riché. Is the derivation
of a model easier to understand than the model itself? In
ICPC, pages 47–52, 2012.
[10] D. S. Frankel. Model Driven Architecture: Applying MDA to
Enterprise Computing. John Wiley & Sons, Inc., 2003.
[11] R. C. Gonçalves, D. Batory, and J. Sobral. ReFlO: An
interactive tool for pipe-and-ﬁlter domain speciﬁcation and
program generation. submitted, 2013.
[12] J. Guo, K. Czarnecki, S. Apel, N. Siegmund, and
A. Wasowski. Variability-aware performance prediction: A
statistical learning approach. In Proceedings of IEEE/ACM
International Conference on Automated Software Engineering
(ASE), Nov 2013.
[13] A. Kleppe, J. Warmer, and W. Bast. MDA Explained: The
Model-Driven Architecture. Addison-Wesley, Boston, MA,
2003.
[14] R. E. Korf. Artiﬁcial intelligence search algorithms. In In
Algorithms and Theory of Computation Handbook. CRC
Press, 1996.
[15] G. M. Lohman. Grammar-like functional rules for
representing query optimization alternatives. In ACM
SIGMOD, 1988.
[16] B. R. Mandelbrot. The Fractal Geometry of Nature.
Macmillian, Philadelphia, PA, USA, 1983.
[17] B. Marker, D. Batory, and R. van de Geijn. A case study in
mechanically deriving dense linear algebra code.
International Journal of High Performance Computing
Applications, 27(4):439–452, 2013.
[18] B. Marker, D. Batory, and R. A. van de Geijn. Code
generation and optimization of distributed-memory dense
linear algebra kernels. In ICCS, 2013.[19] B. Marker, J. Poulson, D. Batory, and R. A. van de Geijn.
Designing linear algebra algorithms by transformation:
Mechanizing the expert developer. In VECPAR, 2012.
[20] T. Meng Low, B. Marker, and R. van de Geijn. FLAME
Working Note #64. Theory and practice of fusing loops when
optimizing parallel dense linear algebra operations. Technical
Report TR-12-18, The University of Texas at Austin,
Department of Computer Sciences, 2012.
[21] T. Menzies, D. Owen, and J. Richardson. The strangest thing
about software. Computer, 40(1):54–60, January 2007.
[22] S. Nedunuri, D. R. Smith, and W. R. Cook. Synthesis of
greedy algorithms using dominance relations. In NASA
Formal Methods, pages 97–108, 2010.
[23] S. Nedunuri, D. R. Smith, and W. R. Cook. Theory and
techniques for synthesizing efﬁcient breadth-ﬁrst search
algorithms. In FM, pages 308–325, 2012.
[24] J. Poulson, B. Marker, R. A. van de Geijn, J. R. Hammond,
and N. A. Romero. Elemental: A new framework for
distributed memory dense matrix computations. ACM Trans.
Math. Softw., 39(2):13:1–13:24, 2013.
[25] J. Poulson, R. van de Geijn, and J. Bennighof. (Parallel)
algorithms for reducing the generalized hermitian-deﬁnite
eigenvalue problem. ACM Trans. on Math. Softw., 2012.
submitted.
[26] M. Püschel, J. M. F. Moura, J. Johnson, D. Padua, M. Veloso,
B. Singer, J. Xiong, F. Franchetti, A. Gacic, Y . V oronenko,
K. Chen, R. W. Johnson, and N. Rizzolo. SPIRAL: Code
generation for DSP transforms. Proceedings of the IEEE,
special issue on “Program Generation, Optimization, and
Adaptation”, 93(2):232– 275, 2005.
[27] T. Riché, R. Goncalves, B. Marker, and D. Batory. Pushouts in
Software Architecture Design. In GPCE, 2012.
[28] N. Siegmund, S. S. Kolesnikov, C. Kästner, S. Apel, D. Batory,
M. Rosenmüller, and G. Saake. Predicting performance via
automated feature-interaction detection. In Proceedings of the
34th International Conference on Software Engineering, ICSE
’12, pages 167–177, Piscataway, NJ, USA, 2012. IEEE Press.
[29] M. Snir, S. W. Otto, S. Huss-Lederman, D. W. Walker, and
J. Dongarra. MPI: The Complete Reference. The MIT Press,
1996.
[30] R. A. van de Geijn and E. S. Quintana-Ortí. The Science of
Programming Matrix Computations. www.lulu.com, 2008.
[31] F. Van Zee and R. van de Geijn. BLIS: A framework for rapid
instantiation of blas functionality. ACM Trans. Math. Softw.
accepted.
[32] R. C. Whaley and J. J. Dongarra. Automatically tuned linear
algebra software. In Proceedings of SC’98, 1998.
[33] R. Williams, C. P. Gomes, and B. Selman. Backdoors to
typical case complexity. In Proceedings of the 18th
International Joint Conference on Artiﬁcial Intelligence,
IJCAI’03, pages 1173–1178, San Francisco, CA, USA, 2003.
Morgan Kaufmann Publishers Inc.
311