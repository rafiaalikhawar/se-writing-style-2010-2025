Sample Size vs. Bias in Defect Prediction
Foyzur Rahman
Univ. of California, Davis, CA
USA
mfrahman@ucdavis.eduDaryl Posnett
Univ. of California, Davis, CA
USA
dpposnett@ucdavis.edu
Israel Herraiz
Universidad Politécnica de
Madrid, Spain
israel.herraiz@upm.esPremkumar Devanbu
Univ. of California, Davis, CA
USA
ptdevanbu@ucdavis.edu
ABSTRACT
Most empirical disciplines promote the reuse and sharing
of datasets, as it leads to greater possibility of replication .
While this is increasingly the case in Empirical Software En-
gineering, some of the most popular bug-ﬁx datasets are now
known to be biased. This raises two signiﬁcant concerns: ﬁrst,
that sample bias may lead to underperforming prediction
models, and second, that the external validity of the studies
based on biased datasets may be suspect. This issue has
raised considerable consternation in the ESE literature in
recent years. However, there is a confounding factor of these
datasets that has not been examined carefully: size. Biased
datasets are sampling only someof the data that could be
sampled, and doing so in a biased fashion; but biased sam-
ples could be smaller, or larger. Smaller data sets in general
provide less reliable bases for estimating models, and thus
could lead to inferior model performance. In this setting, we
ask the question, what aﬀects performance more, bias, or
size?We conduct a detailed, large-scale meta-analysis, using
simulated datasets sampled with bias from a high-quality
dataset which is relatively free of bias. Our results suggest
that size always matters just as much bias direction, and
in factmuch more than bias direction when considering
information-retrieval measures such as aucandF-score .
This indicates that at least for prediction models, even when
dealing with sampling bias, simply ﬁnding larger samples
can sometimes be suﬃcient. Our analysis also exposes the
complexity of the bias issue, and raises further issues to be
explored in the future.
Categories and Subject Descriptors
K.6.3 [Software Management ]: Software maintenance
General Terms
Management
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
ESEC/FSE ’13, August 18–26, 2013, Saint Petersburg, Russia
Copyright 13 ACM 978-1-4503-2237-9/13/08 ...$15.00.Keywords
Bias, size, defect prediction
1. INTRODUCTION
Detailed data on bugs are clearly crucial to empirical
studies of software quality. Such data is generally collected
inbug-ﬁx datasets . Several such datasets have been publicy
released into repositories such as Promise1. These datasets
include both detail on bugs reported by users and developers
and the source code location where the bugs were ﬁxed. The
ﬁx location is provided by a linkto commits in the version
control system. These links identify the source code ﬁles
involved in a bug report, as well as other details, such as
the developer who committed the ﬁx, the date and time,
and the lines changed in the corresponding ﬁles. This is a
rich source of historical data for building software quality
prediction models that may yield improved understanding of
the factors that aﬀect software quality.
These links, between bugs and commits, are recovered
through a post facto examination of the commit logs in ver-
sion control, and/or the comments and meta-data associated
to the bug report; both of these are manually entered by
code contributors . Thus, the ability to recover these links
depend on the tagging practices of the contributors to the
software project under study. These practices vary across
each project; this heterogeneity makes the consistent, reli-
able recovery of all of the links between bugs and commits
problematic. Indeed, many datasets widely used within the
research community are missing some links and have been
found to be incomplete and biased [2, 3].
Training defect prediction models using biased and/or in-
complete datasets is problematic. Wu et al.[21] compared
the accuracy of a prediction model trained using three diﬀer-
ent datasets: a biased dataset obtained using heuristics, a
putatively unbiased dataset obtained manually and a puta-
tively biased dataset obtained using an automated method.
The model trained with the biased dataset did not perform
as well as the models trained with unbiased datasets.
This is a disappointing result, suggesting that, despite all
the hard work invested in creating them, one should abandon
datasets that may be biased. In this paper, we therefore take
a contrarian view on bias. Available datasets could possibly
be biased, but still be large in size. Can such datasets still
be useful in some prediction settings? If it should turn out
1http://promisedata.googlecode.comthat obtaining larger training datasets (of bugs with linked
ﬁxing commits) is more important than the diﬃcult task
of obtaining entirely unbiased datasets, then this would be
good news: large datasets could be used to train prediction
models, with some conﬁdence that such prediction models
would still perform usefully.
In this research, our goal is to do a controlled study of the
relative eﬀects of bias and sample size on defect prediction .
In order to do this, we begin with a very high-quality dataset,
wherein linkage rates are very high (median of 80%, as com-
pared to the more typical 50% [ 4] for other datasets). We
then artiﬁcially sample sub-datasets with varying levels of
size and bias, by selectively sampling linked defects based o n
biases that are known or suspected to exist. Using these ar-
tiﬁcially biased sub-datasets, we examine the relative eﬀec ts
of bias and sample size on defect prediction models. We use
ameta-modeling approach, constructing meta-models that
explain the performance of prediction models . Our research
makes the following contributions.
•We examine the eﬀects of bias on defect prediction, by
artiﬁcially sampling with bias from a dataset known to
be very high quality.
•We study the relative eﬀects of diﬀerent types of bias
on defect prediction, using meta-models to analyze the
prediction models.
•We use the meta-models to study the relative eﬀects
of bias and sample size on defect prediction.
•Finally we introduce and examine the eﬀects of pollu-
tionwhere data sets that fail to link defects that were
actually ﬁxed introduce false negatives , e.g. ﬁles that
are really defective but are not marked as such.
2. RELATED WORK AND THEORY
Defect prediction models use supervised methods to learn
the association of diﬀerent predictors and the defect prone-
ness of entities. A labeled training dataset labels each enti ty
as defective or not. Existing research identiﬁes defective
entitiespost facto , based on the locations where defect repair
occurs. This requires clear identiﬁcation of a defect-ﬁxing
activity.
Past research has used developers’ comments associated
with the source code changes to determine whether a change
is defect-ﬁxing. Ideally, a responsible developer always iden-
tiﬁes defect ﬁxes in the change log. Keywords such as“bug”,
“ﬁxed”etc. in the change log message could be detected by
a tool to identify defect-ﬁxing changes [ 14]. One can also
identify numerical bug-ids mentioned in the change log and
match those ids with defect database such as Bugzilla to
locate defect-ﬁxing changes [ 7,8]. However, developers do
not always annotate a change with proper description; this
impairs the automated identiﬁcation of defect-ﬁxing changes .
Birdet al.[4] found that automated process only identiﬁed
less than 50% of the defect ﬁxing changes in most cases.
Bachmann et al.[2] noted that missing links , the defect-
ﬁxing changes that automated process fails to recover, may
impact the prediction performance of supervised learners. .
BiasMissing links can confuse and hinder a supervised
learner. Links may be systematically missing, based on the
properties of a defect-ﬁxing change such as, e.g., severity
of the defect or the experience of the ﬁxer. The resultingbiased dataset may trick the supervised learner. For example,
suppose only experienced developers are annotating their
changes. Automated tools will only identify defect-ﬁxing
changes made by experienced developers; we will have over
representation of the entities ﬁxed by experienced developers.
In earlier work [ 4] we introduced the notion of bias, and
have provided formal probabilistic deﬁnitions of bias. For
our purposes, here, it is suﬃcient to informally deﬁne bias
as the situation where the linked bug sample distributions of
the co-variates of interest are systematically diﬀerent fr om
the distributions of the same co-variates among the entire
population of ﬁxed bugs . Consequently, the linked sample is
not truly representative of the population. So for example,
the distribution of developer experience among the linked,
ﬁxed bugs would diﬀer from the distribution of developer
experience among allof the ﬁxed bugs.
Diﬀerent properties of defect-ﬁxing changes may introduce
diﬀerent types of bias. Thus, as discussed above, developer
experience may introduce Experience bias. Similarly, sever-
ityofthedefectsmayintroduce Severity bias. Bird et al.[4]
found that defect-ﬁxing changes of less severe defects are
more likely to be linked. However, given a dataset, we may
not know the source of bias, e.g., whether it is Experience
orSeverity . If diﬀerent sources of Biashave diﬀerent levels
of inﬂuence on prediction performance, Biaswould be more
damaging due to the uncertainty over the source of bias.
We therefore study whether diﬀerent sources of Biashave
diﬀerent impacts on prediction performance. We note here
that Kim et al.[10] have studied the inﬂuence of noiseon
bug prediction. They also propose an algorithm to ﬁnd noisy
instances in bug datasets, so they can be removed to avoid
potential problems. Our concern is more with systematic
bias, rather than noise.
Research Question 1: Do diﬀerent sources of bias
have varying impact on prediction performance?
Pollution The second side-eﬀect of missing links, which to
our knowledge has not been addressed, is false negatives . An
unlinked defect-ﬁxing change fails to identify the defective
ﬁles associated with that change. This also eﬀectively labe ls
ﬁles that are actually defective as defect free! These false-
negatives constitute a form of pollution that might aﬀect
supervised learners. We informally deﬁne pollution asan
erroneous condition of bug-ﬁx datasets where ﬁles that are,
in fact, defective are incorrectly labeled as defect-free .
Besides these two consequences, missing links, inherently
reduce the number of defect-ﬁxing changes available. We use
the term Sizeto represent the total number of links available.
Increasing Sizemay increase the performance of the learner
and may also ameliorate the impact of BiasandPollution .
Sizeis also a more tractable problem; larger datasets may
be easier to obtain than data sets that are both unbiased
and unpolluted.
While existing research studies Biasand the presence of
missing links, so far, none have compared the eﬀects of Size,
Pollution andBias, to determine the relative eﬀects of
each on the performance of prediction models; this motivates
our core research question.Table 1: Studied Projects and Release Information
Project Description Releases Avg Avg Link
Files SLOC Rate
CXF Services Framework July 2007–April 2012, 6 releases 403 8.33 358846 .67 0.77
Camel Enterprise Integration Framework January 2008–Marc h 2012, 8 releases 4600 .38 241668 .12 0.84
Derby Relational Database February 2006–October 2011, 7 re leases 2497 .29 530633 .00 0.85
Felix OSGi R4 Implementation August 2007–November 2011, 9 r eleases 2740 .56 249886 .22 0.85
HBase Distributed Scalable Data Store Jun 2007–May 2012, 8 r eleases 934 .75 187953 .38 0.85
HadoopC Common libraries for Hadoop Jun 2007–September 200 9, 6 releases 1047 .17 142257 .33 0.79
Hive Data Warehouse System for Hadoop October 2008–May 2012 , 7 releases 966 .29 152079 .86 0.75
Lucene Text Search Engine Library October 2005–November 20 09, 7 releases 990 .86 122527 .00 0.85
OpenEJB Enterprise Java Beans August 2007–April 2012, 7 rel eases 2895 .43 225018 .43 0.86
OpenJPA Java Persistence Framework January 2007–February 2012, 8 releases 3181 .50 321033 .50 0.92
Qpid Enterprise Messaging system November 2008–May 2012, 7 releases 1724 .00 198311 .86 0.73
Wicket Web Application Framework November 2008–May 2012, 5 releases 2295 .20 152565 .40 0.77
Table 2: Process Metrics
Short Name Description
COMM Commit Count
ADEV Active Dev Count
DDEV Distinct Dev Count
ADD Normalized Lines Added
DEL Normalized Lines Deleted
OWN Owner’s Contributed Lines
MINOR Minor Contributor Count
NADEV Neighbor’s Active Dev Count
NDDEV Neighbor’s Distinct Dev Count
NCOMM Neighbor’s Commit Count
OEXP Owner’s Experience
EXP All Committer’s Experience
Research Question 2: Considering Bias,Pollution ,
andSize, which aspect of missing links aﬀects prediction
models the most?
False Positives Finally, it is also possible that the bug links
containfalse positives ,viz.., edits that are not really bug ﬁxes,
but accidentally get included into bug-ﬁxing commit. This
issue is outside the scope of our current research, and we hope
to address this in the future. However, since developers may
just accidentally include non-bug-ﬁxing changes in a bug-
ﬁxing commit, our belief is that this type of data pollution i s
more likely to be characteristically noisyrather than biased.
Arguably, the approach introduced by Kim et al.[10] for
dealing with noise should be eﬀective in dealing with false
positives.
3. EXPERIMENTAL METHODOLOGY
Projects Studied Table 1 shows the 12 open source projects
studied in this paper. In addition to release information
and project size, the table also lists the median percentage
of defects for which we could identify the ﬁxing-commits
(link rates). All are Apache Software Foundations projects,
written in Java; however, they come from a very diverse
range of domains. For each project we downloaded the gitrepository2and extracted the full commit history. We also
usedgit blame on every ﬁle at every release to identify
the contributors’ information with more detail. We carefully
ignored whitespace changes and code movement during our
blameprocess to identify the correct provenance of each
line.
All the projects shown in Table 1 use jira3issue tracking
system. We mined jirato extract all the data associated to
each bug report. Moreover, thanks to the linking and tagging
practices of the ASF projects, as well as to the features of
jirabug tracking system, we were able to retrieve the related
commits which ﬁxed each bug. We only considered Jira en-
tries clearly identiﬁed as defects; we ignored feature changes,
refactorings, etc. We then locate those ﬁxing commits in
gitto extract commit information, such as changed lines
and commit author. Any ﬁles modiﬁed in these bug-ﬁxing
commits are considered as defective post-facto .
Jira Data All of our projects have very high linking rates ,
with a median rate of approximately 80%. Even at these
high link rates, our data indicates a slight bias; e.g., severe
defects are linked at a median rate of about 80% and less
severe defects at about 75%. Nevertheless, these linking rates
are much higher than the 50% rates reported in prior papers
on bias [ 2,4]. From these very highly linked bug-ﬁx data
sets, we deliberately choose samples with higher and and
lower bias, to study the eﬀects of bias.
Predicting Defects Following common research practice,
we study prediction at the ﬁle level. We use Logistic Regres-
sion from the WEKA toolkit to compute a probability that
a ﬁle will be defective or not in a subsequent release4. The
models are trained in a prediction setting, viz., we train the
model on k-th release, we test the model in k+1-th release,
using process attributes with a binary response indicating
whether a ﬁle is defective. We describe our process metrics
in detail below.
Since we are interested in obtaining the best prediction
model possible, we want to use as many variables as we can to
capture as much variation as possible. Multicollinearity, viz.,
strong correlation of two or more predictor variables, can be
an issue with models with many variables. Multicollinearity
is typically mitigated through the use of either a manual
or automated stepwise procedure where a discrete subset
2http://git.apache.org
3https://issues.apache.org/jira/
4http://www.cs.waikato.ac.nz/ml/weka/of the variables are selected for the model. Because we are
evaluating the use of biased samples we build a very large
number of prediction models; consequently, it is impractical
to individually determine a particular set of predictors to
use in each prediction model manually, and an automated
stepwise procedure would dramatically increase our runtime.
As an alternative, we use ridge regression [12].
Ridge regression introduces (coeﬃcient) bias, reducing vari-
ance in the coeﬃcients, while improving the stability of the
model. The technical details are beyond the scope of this pa-
per5. In the best case, the variance reduction is signiﬁcantly
larger in magnitude than the introduced bias. Because we are
building prediction models, however, this is not a signiﬁcant
concern as we are not interpreting the (prediction) regression
coeﬃcients. Since we are introducing (sampling) bias into
our training sets, which may disturb the relationship between
the training set and test set distributions, ridge regression
provides insurance that we can be reasonably certain that
the (coeﬃcient) bias introduced by multicollinearity is not
impacting the quality of our prediction models in the face of
(sample) bias introduced by our experimental setup.
Our choice to use Logistic regression is motivated by its
popularity in empirical software engineering research. More-
over, researchers have found that the choice of metrics, rather
than classiﬁcation methods, is the primary driver of predic-
tion performance [1].
Predictor Metrics Table 2 shows the process metrics we
use in the prediction models.
•COMM measures the number of commits made to a
ﬁle during a release.
•ADEVis the number of developers who made changes
to that ﬁle during a release.
•DDEVis the number of distinct developers contribut-
ing to this ﬁle up to this release.
•ADDandDELare the normalized (by the total number
of added and deleted lines) added and deleted lines in
the ﬁle during a release.
•OWNis the percentage of the lines authored by the
highest contributor of a ﬁle.
•MINOR is the number of contributors who authored
less than 5% of the code in that ﬁle.
•OEXPis the experience of the highest contributor of
that ﬁle using the percent of lines he authored in the
project at a given point in time.
•EXPis the geometric mean of the experiences of all
the developers.
All of these metrics have been widely used in prior re-
search literature [ 1,5,15,18]. In addition, we use some
“neighborhood” metrics inspired by BugCache [ 11]. Bug-
Cache uses co-commit history to identify ﬁles related to a
buggy ﬁle which may also be buggy. This suggests the use
of “co-commit neighbor” based process metrics. For these
metrics we ﬁrst ﬁnd the list of ﬁles co-committed with a given
ﬁle, weighted by the frequency of co-commit in a particular
release; we then average the above metrics over this list.
5See [12] for details.•We use the weighted average of a metric of the“commit
neighbors”of a ﬁle: NADEV ,NDDEV , andNCOMM
are just the derived measures of ADEV,DDVEV and
COMM.
All of these metrics are individually positively correlated
with the existence of defects, and are thus reasonable can-
didates for inclusion in a prediction model; we rely on the
aforementioned ridge estimator to handle any issues of mul-
ticollinearity. Software engineering data is highly skewed;
following Menzies et al.[13], we log transform all variables
to stabilize variance and improve prediction quality.
Bias-inﬂuence metrics Our goal here, as stated above, is
to try to understand the relative eﬀects of size and diﬀerent
types of bias on prediction performance . Bias arises because
programmers link only somedefect-ﬁxing commits to defect
reports, and fail to link others. Such bias can derive from the
properties of the defects, the defect ﬁxer or the ﬁles ﬁxed.
Birdet al.[4] observed that developers are less likely to
link severe defects than less-severe defects, and experienced
defect-ﬁxers will more often link their ﬁx than inexperienced
developers.
Experienceandseveritycanthusbeviewedas bias-inﬂuencing
properties . We deﬁne bias inﬂuence metrics as measures of
such bias-inﬂuencing properties. In this paper we consider 5
diﬀerent bias-inﬂuencing properties, each with an associated
bias inﬂuence metric ( BI Metric ), that have been discussed
in prior literature [2, 4, 17].
•Experience Defect-ﬁxer experience (measured as the
percent of all commits to date that were made by the
developer who ﬁxed that bug).
•Severity Severity of the ﬁxed defects (an ordinal
measure).
•Proximity Proximity of the defect-ﬁxing commit to
the release deadline (days fromr defect resolution until
the proximate future release).
•Latency The amount of time it took to ﬁx a defect
(days from reporting date to resolution date).
•Cardinality Size of commits (total of number of ﬁles
committed in the defect-ﬁxing commit).
Creating Biased Sub-datasets We use all of the above
BI Metrics to create biased samples. For generality, we
consider both directions of bias, for all sources of bias. Thus,
instead of assuming that experienced developers would be
more likely to link their ﬁxing commits, or that the commits
that ﬁx severe defects are less likely to be linked, we study
both directions: how is prediction performance aﬀected when
experienced developers are morelikely to link their commits,
as well as when they are lesslikely to link their commits;
likewise we also consider the cases of severe defects being
morelikely to be linked, andless likely to be linked.
For each bias-inﬂuencing property, we partition our set
of ﬁxing commits using the median of the corresponding
BI Metric . This gives us two sets: Lowercontaining the
ﬁxing commits with BI Metric <medianBI Metric , and
Higher containing the ﬁxing commits with BI Metric >
medianBI Metric . We add the remaining ﬁxing commits
(withBI Metric exactly equal to the median BI Metric )
to either LowerorHigher, whichever is smaller. This fairlycoarse split is done to keep the experimental combinatorics
under control; even with this simpliﬁcation, we already have
millions of sub-samples available for training.
Based on this split, we choose our biased subsamples.
Our approach is to select subsamples with varying levels of
linking probability from LowerandHigher. So for each
ofLowerandHigher, we vary the probability of linking
p∈ {0.0,0.1,0.2,...1.0}. Thus, when p= 0.2, and we are
considering Lower, we assume that 20% of the ﬁxing com-
mits from Lowerare linked. These 11 discrete probabilities
can be used to select 11 diﬀerent samples for each of Lower
andHigher, designated as {Lower p}and{Higher p}, (for
each value of p) randomly chosen from LowerandHigher.
During the sampling all the unselected ﬁxing-commits are
considered as missing links.
We then ﬁnd the unions of all possible cartesian pairs of
{Lower p}∪{Higher p}(121 pairs) to study diﬀerent ranges
of bias. We discard the base set {Lower 0.0}∪{Higher 0.0}
as we need at least one defect-ﬁxing commit to train a model.
This approach allows us to study the bi-directional bias
impact (when Loweris less likely to link than Higher, as
well as when Loweris more likely) of each source of missing
links. In order to compare the relative eﬀects of size and
bias, for each biased set of links Bl={Lower p∪Higher p}
we alsouniformly randomly sample an unbiased set of links
Ulfrom the entire pool of ﬁxing commits of the samesize
|Bl|.
Pollution Eﬀects Besides possibly creating biased samples,
missing links may pollutethe data with false negatives: We
thereforealsoexaminetheimpactof Pollution , bysampling
sub-datasets both with and without pollution. We create
unpolluted sub-datasets by discarding any false negatives
from the sampled sub-dataset, i.e., any ﬁles known to be
defective, but not selected based on the biased sampling
procedure, are discarded from the training set. To study the
impact of Pollution , we created polluted sub-datasets by
labeling defect-ﬁxing commits as non-defective when they are
not chosen by the biased sampling procedure. We can then
compare the impact of Pollution against the performance
of an unpolluted dataset.
Finally, after sampling the defect-ﬁxing commits with the
four possible combinations (biased and polluted, biased and
unpolluted, unbiased and polluted, unbiased and unpollute d)
we derive the set of defective ﬁles based on our sampled
defect-ﬁxing commits. Any ﬁle that appears in one of the
sampled ﬁxing commits are considered as defective, otherwise
defect-free. We can then build models on these newly labeled
(coming from diﬀerent combinations of bias and pollution)
dataset. We replicate the entire process 100 times, to reduce
the risk of random variation, and average the performance
measures over all runs.
Summary of sampling procedure Figure 1 illustrates how
we sample to create sub-datasets. The sampling process is
adjustable; it can be tuned to select sub-datasets based on
diﬀerent settings described above; in addition, it can sample
with or without pollution. Finally, the procedure can also
sample without any bias, to create sub-datasets of varying
sizes. Given all of the combinations of bias & pollution, all
of the product-release pairs, and our 100-fold replication,
this sampling procedure eventually creates about 17 million
sub-datasets. These sub-datasets are used to train prediction
models. We then evaluate the eﬀect of bias, pollution, and
size on prediction model performance.
Figure 1: Pictorial summary of of the experimental proce-
dure. We begin with a highly linked dataset (1) and selective ly
sample (2) under controls for bias, pollution and size (3) to
create a cloud (4) of sub-datasets, with and without polluti on,
and bias, as well as an entirely unbiased sub-datasets. Thes e
sub-datasets are used to estimate prediction models (5); th e
results of these prediction models are then subjected to mul ti-
ple regression meta-modeling (6) to tease apart the eﬀects o f
pollution, size, and bias (7)
Evaluation Our evaluation is a two step process. First, we a
build a model for each training release, using the sub-dataset,
and evaluate the model on the corresponding test release.
It should be noted that the test release data is used in full,
without sampling.
There are several diﬀerent approaches to evaluating predic-
tion models. We discuss them in general terms; details have
been discussed in several prior publications. Precision/Reca ll
and F-score are the traditional performance measures. Preci-
sion and recall have a natural trade-oﬀ, and one can choose
a high-recall/low-precision combination, or the reverse. The
precise value is based on a particular threshold of predicted
probability of defects. These measures are also dependent
on the proportion of defects; thus if most ﬁles are defective,
even random choice will give relatively high precision.
There are also several threshold-independent, non-para-
metric methods. First, is the area under the ROC curve,
auc, which evaluates performance independent of threshold
or defect occurrence rate. However, aucdoesn’t directly
consider the cost of methods such as inspection. Arisholm
et al.[1] suggest that using threshold-dependent measure
such asPrecision ,Recall andF-measure may not be
suitable for software engineering data, since inspection cost
dependsondefect density. Theyrecommendtheuseof aucec
(area under the cost -eﬀectiveness curve), and speciﬁcally
aucecat 10% ( aucec 10) and 20% ( aucec 20) source lines of
code (SLOC) to compare model performance. It should be
noted however, aucecis just one model of cost; e.g.it does
NOT consider the cost of false negatives. If false negatives
matter,aucin fact might be a better measure.
Forourpurposes, weconsiderthreshold-dependent, threshold -
independent, and aucecbased measures. Speciﬁcally, we
useF-measure at 0.5 cutoﬀ ( F50) to give the reader some
idea about the model performance when evaluated in tradi-
tional cutoﬀ based settings. Following Arisholm et al., we
stress that such a threshold-based measure may give an erro-neous impression about the true performance of the model
due to the class imbalance (only a small portion of our ﬁles
are reported as defect prone) that is common in bug-ﬁx
datasets. So, we also measure performance using auc, as
well asaucec 10andaucec 20. We replicate the entire pro-
cess of random sampling and model building 100 times and
then average over these runs to obtain the average model
performance for each {Lower p×Higher p}andPollution
combination ( Pollution turned oﬀ and on).
Meta-modeling Our goal is to use the models trained over
the millions of samples of varying bias & size is to determine
the relative eﬀects of size and bias on performance. We do
this by using a meta-model to model the performance of
the resulting millions of prediction models; this meta-model
essentially gauges the degree to which Bias,Pollution and
Sizeinﬂuence the performance of the millions of prediction
models. Thus, the meta-response of this meta-modeling step
istheperformanceofthepredictionofpredictionmodels. F50,
auc,aucec 10andaucec 20of the learned prediction models
are all used as (meta) responses (measures of performance)
in our meta-analysis, while Bias,Pollution andSizeare
the (meta) predictor variables. We build one meta-model for
each training-test release pair . So, for each training release,
we build an ensemble of samples of varying Bias,Pollution ,
Size, as described above. We then train models for each
sample, testing on the next release to ﬁnd auc,F50etc. We
use the prediction performance of these models as a response
to our meta-model.
We validated our meta-models using standard OLS diag-
nostic techniques. All of our models exhibited reasonably
highR2. Forauc,aucec 10andaucec 10we observed a
medianR2of around 0 .6, while for the F-measure models,
we observed an R2over 0.7. We checked for inﬂuential points
and excessive heteroscedasticy through visual examination
of the regression diagnostic plots. For inﬂuential points we
looked for excessive separation or high Cook’s Distance [ 6].
We also used a metric deﬁned by Lindeman, Merenda, and
Gold, (LMG) to measure the impact of the variables [ 9]. As
described in detail in the next subsection, LMG provides a
robust way to measure the relative importance of predictor
variables on a response. LMG permutes the sequential sum
of squares to yield an order insensitive degree of variance
explained for each of the variables. The use of LMG provides
resilience against the impact of multi-collinearity. This pro -
cess is quite compute-intensive: building prediction model s
for the 17 million sub-datasets, over 85 releases, and the
meta modeling phase, consume about 180 hours of time on
12-Xeon (3.0 GHz) core, 96 GB workstation.
We identify the percentage of variance explained by each
of these variables using the R package Relaimpo [9]; The
Relaimpo package computes the aforementioned “LMG”
statistic which we use to measure the impact of the meta
variables on the performance of the prediction models.
LMG measure of inﬂuence InanOLSmultipleregression
setting the Relaimpo package computes a statistic called
LMG for each predictor. LMG is a measure of the inﬂuence
that a predictor has on a response in a dataset. Using the
LMG statistic, we can compare the eﬀects of Size,Bias, and
Pollution on the performance of prediction models. We
now present our reasons for using LMG.
Ordinary Least Squares ( OLS) linear regression models
can be used to evaluate the relationship between a set of
predictors x1,...,xpand a response y. We write the modelform of the relationsip as
y=β0+β1x1+...+βpxp+ǫ
where each βirepresents an estimated coeﬃcient for the
relationship between xiandy. Ifβiis positive and signiﬁcant
then we can infer that a βiunit increase in xinduces a one
unit increase in y.
We were primarily interested in the magnitude of impact
on the variance, rather than the direction of the inﬂuence.
Bias may improve or retard a particular model performance
measure; we would be interested in either, equally. Conse-
quently, we are interested in the impact that each regressor
has on the variance of the model response.
Typically one measures the variance in the response of an
OLS model using the coeﬃcient of determination, denoted by
R2.R2is interpreted as the percentage of variance explained
in the response by the predictors, and is computed as the
proportion of model sum of squares over the total sum of
squares (SS)
R2=Model SS
Total SS=/summationtextn
i=1(ˆy−¯y)2
/summationtextn
i=1(yi−¯y)2
whereyiis the given value of response in the ithsample,¯yis
the sample mean, and ˆyis the predicted response from the
OLS model, from the values of predictors in the ithsample.
Decomposing the sum of squares will allow us to compute
the impact of each regressor on the variance of the response.
We can imagine a simple method for decomposing the
sum of squares as follows: We add each predictor of interest
in turn to the model and compute the additional variance
explained by each new predictor. This will yield a particular
decomposition of the sum of squares. A decomposition deter-
mined this way is referred to as a sequential sums of squares .
No matter which order we choose to enter variables, every
sequential sum of squares of the same set of predictors will
yield the same totalSS. Practically, the explained variance
is typically computed by performing an anovaanalysis.
Unfortunately, this procedure is sensitive to the order in
which the variables are added to the model; Although we will
alwaysobtainthesametotalSS,thesequentialdecompositio n
may not be consistent across each variable. To see why,
consider two predictor variables x1andx2that are not
fully independent, viz., each variable has both an indepenent
component and a shared component. Then some part of
x1can be explained by x2, and the intersection will impact
the response ysimiarly for either variable. If we initially
compute the sum of squares for the model containing only
x1,x1will account for all of the variance that is explained
both by its independent component, and the component
shared with x2. When we add x2to the model, only its
independent component adds additional variance explained
as the variance owing to their collinearity is accounted for in
x1. Reversing the order will attribute the joint variance now
tox2instead of x1. This order-dependence makes this simple
analysis unsuitable for decomposing the variance explained
in our setting.
LMG is an order-independent method that calculates the
decomposition of variance for all possible variable orderings
and then computes the mean impact of each variable on the
response. LMG is thus able to correct for order-dependency
and provides a uniform measure of the impact of each predic-
tor on the explained response. We calculate the raw LMG,0.50.60.70.80.91.0
●
●●
●●●
●
●●●
●
●●●
●
●●
●●
●
E C S L P
Sources of BiasMedian: AUC
(a) Median auc0.00.20.40.60.8
E C S L P
Sources of BiasMedian: F_50
(b) Median F500.010.020.030.040.05●
●●
●
E C S L P
Sources of BiasMedian: AUCEC_10
(c) Median aucec 100.050.100.15
●
●●
●
E C S L P
Sources of BiasMedian: AUCEC_20
(d) Median aucec 20
Figure 2: Median of Performance for diﬀerent bias sources. E for Experience ; C forCardinality ; S forSeverity ; L for
Latency ; P forProximity
which calculates each variable’s inﬂuence as a proportion of
the total variance in the response.
Summary: To recap, by reference of Figure 1: the generated
sub-data sets in Figure 1. (labeled 4) (controlled for size,
bias, pollution etc) are used to build prediction models. The
performance of these prediction models is evaluated. We
now have a collection of data, one for each prediction model,
which capture the Size,Bias, andPollution of the data
that went into that model, and the measured performance.
Thus, for each model, we have one element of a meta-dataset.
We now use ordinary least squares meta-modeling (step6
in ﬁgure) on this dataset, and use LMG to tease apart the
eﬀects of Size,Bias, andPollution .
4. RESULTS
We begin by investigating the impact of diﬀerent sources
of bias on prediction performance.
RQ 1: Do diﬀerent sources of bias have diﬀerent impacts
on prediction performance?
The raw data on the performance of the various models
varies quite a bit due to diﬀerent reasons: bias type, bias
degree (H/L), release cycle, as well as sample size. We deal
with the relative eﬀects size and bias subsequently. For now,
to focus on the eﬀects of bias type, rather than bias degree
or release cycle, we focus on summary statistics (median and
variance per release , while bias degree varies).
For each release, and for each type of bias, as we allow
the bias degree to vary, both the corresponding sample size
and the degree of pollution will vary; so we expect to see
some variation in the performance of models trained on
each sample. So within each type of bias, we calculate the
median andvariance of performance per release as bias
degree varies. These two statistics give us a sense of the
range of performance that can be observed for each bias type,
as its bias degree varies. The bias types in our case could
be one of Experience ,Cardinality ,Severity ,Latency
andProximity . Since the goal here is to check whether
diﬀerent sources of bias have diﬀerent eﬀects (and thus if any
of these sources of bias are a greater threat than any other)
we consider only biased, andpolluted sub-datasets, infected
with bias of each type.
Figure 2 plots the median of prediction performance for
diﬀerent sources of bias. The ﬁgure shows hardly any dif-
ference in the distribution of median performance acrossdiﬀerent sources of bias. Our ﬁndings suggest that none of
the observed variations are statistically signiﬁcant. A non-
parametric Kruskal-Wallis test also failed to reject the null
hypothesis that the distributions are the same.
0.0000.0050.0100.0150.0200.025
●
●●
●●
●
● ●
●●●
●●
●
●●
●●●
●●
●
●●
●●●
●●
●●●●
●
●●
E C S L P
Sources of BiasVariance: AUC
(a)aucvariance0.000.010.020.030.04
●●●●
●●●●
●
●●●
E C S L P
Sources of BiasVariance: F_50
(b)F50variance
Figure 3: Variance of Performance for diﬀerent bias sources.
E forExperience ; C forCardinality ; S forSeverity ;
L forLatency ; P forProximity
In addition to analyzing the diﬀerence in median perfor-
mance, we examined the stability of prediction performance
using the variance of the performance measures per release .
Figure 3a and ﬁgure 3b presents the variance of aucandF50
for diﬀerent sources of bias across releases. For brevity we
do not present aucec 10andaucec 20as they are closely re-
semble the aucplot (ﬁgure 3a). The ﬁgures suggest that the
threshold-dependent performance measure F50is more sensi-
tive to diﬀerent sources of bias than the threshold-invariant
measures such as aucandaucec. A Kruskal-Wallis test
also failed to conﬁrm any signiﬁcant diﬀerence of variances
between diﬀerent sources of bias. We observe similar ﬁndings
foraucec 10andaucec 20. It is clear that there is consider-
able variation in the median of the performance within each
bias type. We attribute this variation in training sample sizes
across releases; some releases have more defects, and thus
more training data, than others. We return to the subject of
sample sizes again below.
Bias Eﬀect These two ﬁndings together have an important
implication. In general, across releases , performance as mea-
sured by auc,aucec 10andaucec 20varies (ﬁg 2); these
models, however, perform quite well, the median of aucis
about 0.9 and the median of F50is around 0.4. The median of
aucec 10is around 0.015, a ﬁgure that is 3 times 0.005, whichcorresponds to inspecting random lines6, and the aucec 20
median ﬁgure is around 0.05, or about 2.5 times the random
rate of 0.02. However, as we examine variation in perfor-
mance,within a release , for models trained with sub-datasets
with varying BiasandPollution , arising from diﬀerent
sources of bias inﬂuence, the variation in performance of
prediction is very limited (Figure 3) for the non-parametric
measures auc, as well as for aucec 10andaucec 20(not
shown)7. Furthermore, the eﬀect on performance, and the
variance in performance across diﬀerent sources of bias, is
also similar.
Diﬀerent sources of Biashave very similar eﬀects on
performance; furthermore, the eﬀect of varying rates
ofBiasis also minimal on non-parametric measures of
performance, for all sources of Bias.
This is an unexpected result; data that is selectively miss-
ing for diﬀerent reasons might be expected to aﬀect the
performance diﬀerently, and diﬀerent rates of bias might
also be expected to aﬀect performance. Nevertheless, we
still can see that each source of missing links induce some,
within-bias-source, variance in prediction performance. This
variance can be attributed to varying SizeandBiasand the
Pollution of the sample space with false negatives. We
therefore study the impact of Size,BiasandPollution
separately.
RQ 2: Considering Bias,Pollution , andSize, which
aspect of missing links aﬀects prediction models the most?
When studying the relative eﬀects of size and bias, we
found a high degree of collinearity between size and bias in
our meta-models. Multi-collinearity, and the resulting diﬃ-
culties in interpreting models with potentially high variance
inﬂation factors, casts doubt on the stability and validity of
such models. Therefore, we chose to focus on a simpliﬁed,
categorial measure of bias: bias polarity . Bias polarity ad-
mits two categories of possible, BiaslandBiasm.Biasl,
negative bias polarity, represents all the samples where we
haveLower p>Higher p, whereLower pandHigher prep-
resent the probability of linking from LowerandHigher
respectively. In case of Experience bias, this would indicate
that inexperienced developers are more likely to link than
experienced developers. Likewise Biasm, positive polarity
represents all the samples where Lower p<Higher p. The
samples are coded so that Biasl(resp.,Biasm) take the
value 1 if the sample has negative (resp., positive) bias po-
larity. They both take the value 0 if the sample has neutral
bias,viz.,Lower p=Higher p.
Arguably, for the BI Metrics we consider, polarity is
a reasonable categorical abstraction. Concern about bias
can be stated in categorical terms such as Are more/less
experienced developers more/less/equally likely to link b ugs?,
6If we inspect lines randomly, over many samples, we expect
to discover bugs at the same rate we inspect, viz.10% of the
defects for inspecting 10% of the lines; thus the area under
the resulting diagonal line would be 0.1 times 0.1 times 0.5
7The parametric measure F50does show a much more sub-
stantial eﬀect, but this is due to 50% threshold parameter
not always being ideal; we discuss this laterorDoes linking rate for ﬁxes that include more changed ﬁles
lower? or higher? or the same? , orare more/less severe bugs
less/more/equally likely to be linked than less severe bugs .
Ourabstractionshedslightonwhethersuchcategoricalbiase s
aﬀect prediction performance.
Our ensemble of samples includes both biased and unbi-
ased samples. Our set of unbiased samples will include both
uniformly randomly picked unbiased samples, as well as sam-
ples where Lower p=Higher p. We also add Pollution
as a treatment, with two levels, polluted and unpolluted, as
described above. Those are the predictors; the (continuous)
response variables are auc,aucec 10,aucec 20andF50as
response.
To ﬁnd the impact of diﬀerent variables we use LMG as
described in Section 3. Figure 4 shows the LMG impact of
Bias,SizeandPollution variables on diﬀerent measures
of prediction performance. BiaslandBiasmare marked as
L and M respectively on the x-axis. P and S represent Pol-
lutionandSizerespectively. Since there is one prediction
model for each of the training releases, we get a range of
LMG values bias, size, and pollution; these are shown as box
plots labeled below with L, M, P and S as just explained.
The ﬁgure depicts a surprising trend: forauc,Sizematters
much more than both BiaslandBiasm. We ran two sample
pairedWilcox test (with alternative hypothesis set to“ Size
has a larger LMG impact than either BiaslorBiasm”) on
each pair , and corrected the p-values for multiple hypothesis
test using Benjamini-Hochberg correction. For all sources of
missing links, we observed that the impact of Sizeis statis-
tically signiﬁcantly more than the impact of either Biaslor
Biasmwith very low p-values p <0.001.
When considering auc,Sizeis much more important
thanBiaspolarity. Focusing eﬀort on collecting more
samples may mitigate much of the impact of Biaspo-
larity.
Foraucec 10andaucec 20, however, the impact of Size
doesn’t dominate that of Bias. A one-sided Wilcox test
with alternative hypothesis “ Sizehaslessimpact than ei-
therBiaslorBiasm”, failed to establish any statistical
signiﬁcance of BiasagainstSizeafter Benjamini Hochberg
correction; all of the p-values were greater than 0 .40. In
fact, testing the alternative hypothesis“ Sizehaslargerim-
pact than either BiaslorBiasm”, we observed that Size
has a larger impact than both BiaslandBiasmforLa-
tency(p <0.001). This result holds for both aucec 10and
aucec 20.Sizealso dominates both Biasl(p= 0.004) and
Biasm(p= 0.001) for aucec 20when considering Proxim-
ity, andBiasmforaucec 20(p= 0.014) when considering
Cardinality , andBiasmfor both aucec 10(p= 0.048) and
aucec 20(p= 0.001) when considering Severity . This sug-
gests that Sizeis as important as Biaseven when evaluating
aucec.
With respect to F50, we again observed a dominance of
SizeoverBias. In all of our comparisons we found a sta-
tistically signiﬁcantly higher impact ( p <0.001) of Size,
compared to either BiaslorBiasm. In fact, it is clear from
the ﬁgure that, for F50,Size, moreso than auc, dominates
bothBiaslandBiasm.Experience Cardinality Latency Proximity Severity
0.00.20.40.60.8
●●
●●●●
●●●
●
●●
●
●
●●
●●●
●●
●
●●●
●
●●
●
●●●
●●
●●●
●●●
●●
●●
●●
●●
●
●●●
●
●
●●●
●
●
●●●
●●
●●●
●●
●
●
●●
●
●
●●●●●
●
●●
●●
●●●
●●●●
●●●
●●
●
L M P S L M P S L M P S L M P S L M P S
VariablesLMG
(a) Impact on aucExperience Cardinality Latency Proximity Severity
0.00.20.40.60.8
●●
●●
●●●●
●●
●
●●
●●●
●●●●
●●●
●●
●●
●●
●●
●
●●●
●●●
●●●●
●
●
●●
●●
●●●
●
●●●
●●
●
●●●●
●●
●
●●
●
●
●●
●
●●
L M P S L M P S L M P S L M P S L M P S
VariablesLMG
(b) Impact on F50
Experience Cardinality Latency Proximity Severity
0.00.20.40.60.8
●
●●
●●●
●●
●
●●●
●
●●●●
●●●●
●
●
●●
●●●
●●
●●
●●
●●
●●
●
●
●●●
●●
●●
L M P S L M P S L M P S L M P S L M P S
VariablesLMG
(c) Impact on aucec 10Experience Cardinality Latency Proximity Severity
0.00.20.40.60.8
●●
●
●●
●●
●●●●●
●●
●
●
●●
●●●
●●●●
●●●
●●
●●●●●
●●
●●●
●●●
●●●
●●
●
●●●●●
●
●●
●●●
L M P S L M P S L M P S L M P S L M P S
VariablesLMG
(d) Impact on aucec 20
Figure 4: Impact of diﬀerent aspects of missing links. L stands for bia s polarity to low values ( Biasl); M bias polarity to higher
values (Biasm); P stands for pollution; all of those are categorical varia bles. S stands for size, the sole numerical variable
For the IR performance measures aucandF50, the eﬀect
ofSizestrongly dominates that of Bias. For the aucec
performance measure, Sizehas as much of an inﬂuence
asBiaspolarity.
We have discussed the impact of SizeandBias, however,
missing links would inevitably introduce Pollution . As
stated above, we introduce Pollution as an additional
treatment in our meta-model. Figure 4 shows, in addition
toBiasandSize, the impact of Pollution on performance.
The ﬁgure shows that Pollution matters much more for
the threshold-dependent measure F50, than the threshold-
invariant measures aucandaucec. We compared the impact
ofPollution withSize,BiaslandBiasmusing apaired
Wilcox test and corrected the p-values using Benjamini-
Hochberg correction. Our ﬁndings suggest that for auc,
aucec 10andaucec 20, the impact of Pollution is typically
neither signiﬁcantly greater nor smaller than the impact of
BiaslorBiasm. In fact Pollution showed a signiﬁcantly
smaller impact ( p= 0.016) for onlyone case: against Biasl
foraucandCardinality . In all cases, for aucandaucec,
Pollution showed statistically signiﬁcantly less impact than
Size(forauc,p <0.001, and for aucecthe largest observed
p <0.027).
However, a similar comparison for the threshold-dependent
measure F50tells a diﬀerent story. In this case, Pollution
was dominant over both BiaslandBiasm, and was only out-
performed by Size. ApairedWilcox test, using a one-sided
alternative that“ Pollution has greater impact than either
BiaslorBiasm”, shows a statistical diﬀerence between the
variables with a p-values ( p <0.001). A pairedWilcox test
for the alternative hypothesis that“ Pollution has smallerimpact than Size”, however, conﬁrms the dominant role of
Size(p <0.001).
ForF50performance, Pollution plays a more damaging
role than Biaspolarity. Sizestill has the most impact.
5. DISCUSSION
While previous studies have considered bias and its impli-
cations, ours is the ﬁrst detailed comparison of the eﬀects of
Size,BiasandPollution . We now discuss the implications.
Positive & Negative Bias Note in Figure 4 that the po-
larity of Biasindicates varying impact on the prediction
performance. In case of Experience , (top left of the ﬁgure)
withaucas performance measure, linking by more experi-
enced developers (the box plot labeled Biasm) has a stronger
eﬀect on performance. (a statistical test conﬁrms this). But
in the case of Cardinality , we see the opposite eﬀect on
auc.
We also examined the eﬀect of combining positive and
negative bias. In general, we found that combining bias into
a“directionless biased”treatment tended to lower the eﬀect o f
bias (p <0.001 for all pairwise comparisons of directionless
Biaswith directional Bias). In practice, however, it is
not always easy to determine whether a given dataset has
positive or negative bias. Our study suggests that direction
does matter, and some eﬀort, perhaps based on sampling,
might provide useful information.
From Figure 4 we also notice that each Biasmay have
an impact on the prediction performance. E.g., in case of
Experience andaucas performance measure, linking by
more experienced developers may be more important. A two-
samplepairedWilcox test shows that the impact of Biaslontheaucis signiﬁcantly diﬀerent than the impact of Biasm
forCardinality (p <0.001) and Experience (p= 0.013).
Foraucec, the direction of Biasmattered only for Severity
(p= 0.034 foraucec 10andp= 0.025 foraucec 20). ForF50,
direction mattered only for Cardinality (p= 0.016) and
Latency (p <0.001 ).
Pollution eﬀect on F50From Figure 3, it is clear that the
F50performance measure has a much greater variance than
auc.F50also has a lower variance than the aucecmeasures.
It is also clear from Figure 4 that F50is quite strongly
aﬀected by pollution, when compared to other prediction
performance measurements.
F50assumes a 50% threshold on the predicted probability
from the underlying logistic regression prediction models. We
hypothesized that performance based on this ﬁxed threshold
is highly sensitive to the precise training set. We expect that
diﬀerent sizes and biases in the training set will result in vari-
able performance. To investigate this further, we measured
theF50score on the full datasets for each release. A two-
tailed, two-sample Kolmogorov-Smirnov test showed that F50
over the unsampled data and F50over the sampled data are
indeed diﬀerent, i.e., sampling aﬀects the F50performance.
To reduce the sample-dependent eﬀect on F50, we tried an
alternative approach: for each training data set, we chose
the threshold that maximized F-measure performance on
the training set. This is a feasible approach, since no test set
information is used. We call this F-measure valueFtrmax.
We calculated Ftrmaxfor both sampled sub-datasets and
the full dataset. A statistical test failed to reject the null
hypothesis that the distribution of the Ftrmaxscores in the
sub-datasets was diﬀerent from the Ftrmaxscores in the
full dataset. This provides a plausible explanation for the
instability in F50.
Implications Overall, our meta-analysis shows that Size
inﬂuencespredictionperformanceatleastasmuchas Biaspo-
larity and Pollution . When considering inspection-oriented
applications, all factors matter. The aucecresults suggest
that one should not only try to maximize the size of the
training data, but also to the extent possible, strive to obtai n
unbiased and unpolluted samples. aucec, however is not the
last word. For example, it doesn’t account very well for the
cost of false negatives. When these matter, aucmight be a
more suitable measure. In this case, our data suggests that
Sizehas amuch stronger inﬂuence on performance; therefore
eﬀorts to obtain larger training sets might be a more eﬀective
way to improve performance. Since, in general, it is diﬃcult
to determine the nature and extent of Biaspolarity, this
ﬁnding is a reassuring and positive result: if better aucis
a goal, and if there is concern about the polarity of bias, a
larger training sample is likely to help.
Finally, we note that our study focused on Biaspolarity ,
rather than the degree of bias. Our results indicate very
strongly that bias polarity matters far less than size; if large
sample sizes are available, our results suggest bias polarity
per seis not a major concern that it is very likely that trained
prediction models would perform reasonably well, at least
from from an aucorF50perspective. Indeed, the eﬀect of
bias isso strongly confounded by the eﬀect of size that we
found it diﬃcult to tease apart the eﬀect by linear modeling,
and this remains for future work.
6. THREATS TO V ALIDITYChoice of Metrics Our prediction models use popular pro-
cess metrics. We did not use any code metrics; generally
speaking, process metrics outperform code metrics [ 1,16,19,
20]. Our models performed well: the median aucwas around
0.9, and median F50was about 0 .5 for the full dataset, on
par to what is reported in the existing research [1, 22].
Using Logistic regression We use LR to predict the defect-
proneness of ﬁles. LR is very widely used [ 1,15,18,22];
Moreover, prior research indicates that the choice of proper
metrics matters more than the learning techniques used, and
that LR has good performance [1].
Data Quality Instead of relying on automated extraction
of links from commit logs, we rely on a high ﬁdelity linking
process available in jira. All of our projects have very high
linking rates, with a median linking rate about 80%. This
is well above the typical linking rate of under 50% reported
in prior research [ 4] using traditional heuristic based linking
technique [ 7,8]. The Jira data has link bias for Severity (
80% median link rate for more severe vs. 75% for less severe)
andLatency (80% vs 78%); it is unbiased for Proximity .
Nevertheless, the high linking rate allows us to subsample
and introduce both positive and negative bias, and pollution ,
to study the impact on prediction performance.
7. CONCLUSION
Several publications in the last 3-4 years have highlighted
the presence of signiﬁcant bias in bug-ﬁx datasets. This
has led to widespread concerns, reported in several papers,
that biased datasets would lead to under-performing, even
misleading, prediction models of limited practical value. We
investigated this issue in depth using simulated sampling
with bias from high-quality dataset, and found clear evidence
that a) the type of bias have limited impact in on prediction
results, and b) the eﬀect of bias is strongly confounded by
size, and c) Bias polarity has a relatively small eﬀect eﬀect
when compared to size for the aucandF50measures, and is
comparable to size for the aucecmeasures. Indeed, we found
that the eﬀect bias is diﬃcult to tease apart from size, and
this remains a challenge for future work. Thus the bias-eﬀect
results reported earlier [ 4] may indeed be due to diminished
sample size in the biased samples. Our work does strongly
suggest, however, that even ifthere are concerns about bias
polarity in a potential training sample of defect repairs, such
a sample could still be used to train a prediction model, as
long as it is large; the resulting prediction performance is
likely to be boosted more by the size of the sample than it is
hindered by any bias polarity that may exist.
8. ACKNOWLEDGMENTS
This research was conducted while Israel Herraiz was visit-
ing UC Davis, thanks to the funding provided by the Ministry
of Education of Spain (“Jos´ e Castillejo”grant no. JC2011-
0093). Prem Devanbu, Foyzur Rahman and Daryl Posnett
were supported by NSF, Award Number: 0964703. Daryl
Posnett was also supported by the Air Force Oﬃce of Scien-
tiﬁc Research, award FA955-11-1-0246.9. REFERENCES
[1]E. Arisholm, L. C. Briand, and E. B. Johannessen. A
systematic and comprehensive investigation of methods
to build and evaluate fault prediction models. JSS,
83(1):2–17, 2010.
[2]A. Bachmann, C. Bird, F. Rahman, P. Devanbu, and
A. Bernstein. The Missing Links : Bugs and Bug-ﬁx
Commits Categories and Subject Descriptors. In Pro-
ceedings of the European Software Engineering Confer-
ence and the ACM SIGSOFT Symposium on the Foun-
dations of Software Engineering (ESEC/FSE 2010) , vol-
ume 2 of FSE ’10, pages 97–106. ACM, 2010.
[3]C. Bird, A. Bachmann, E. Aune, and J. Duﬀy. Fair and
balanced?: bias in bug-ﬁx datasets. In Proceedings of
the the 7th joint meeting of the European Software Engi-
neering Conference and the ACM SIGSOFT symposium
on The Foundations of Software Engineering , 2009.
[4]C. Bird, A. Bachmann, E. Aune, J. Duﬀy, A. Bernstein,
V. Filkov, and P. Devanbu. Fair and balanced?: bias
in bug-ﬁx datasets. In Proceedings of the the 7th FSE ,
pages 121–130. ACM, 2009.
[5]C. Bird, N. Nagappan, B. Murphy, H. Gall, and P. T.
Devanbu. Don’t touch my code!: examining the eﬀects
of ownership on software quality. In T. Gyim´ othy and
A. Zeller, editors, SIGSOFT FSE , pages 4–14. ACM,
2011.
[6]J. Cohen. Applied multiple regression/correlation anal-
ysis for the behavioral sciences . Lawrence Erlbaum,
2003.
[7]D. Cubrani´ c and G. C. Murph. Hipikat: recommend-
ing pertinent software development artifacts. In Proc.
Int’l Conf. Software Engineering (ICSE) , pages 408–418,
Portland, Oregon, 2003. IEEE Computer Society Press.
[8]M. Fischer, M. Pinzger, and H. Gall. Populating a
release history database from version control and bug
tracking systems. In Proceedings of the International
Conference on Software Maintenance , pages 23–32, Los
Alamitos CA, September 2003. IEEE Press.
[9]U. Gr¨omping. Relative importance for linear regres-
sion in r: the package relaimpo. Journal of Statistical
Software , 17(1):1–27, 2006.
[10]S. Kim, H. Zhang, R. Wu, and L. Gong. Dealing with
noise in defect prediction. In Proceedings of the 33rd In-
ternational Conference on Software Engineering , pages
481–490. ACM, 2011.[11]S.Kim, T.Zimmermann, E.Whitehead Jr, and A.Zeller.
Predicting faults from cached history. In Proceedings of
the 29th ICSE , pages 489–498. IEEE Computer Society,
2007.
[12]S. Le Cessie and J. Van Houwelingen. Ridge estimators
in logistic regression. Applied statistics , pages 191–201,
1992.
[13]T. Menzies, J. Greenwald, and A. Frank. Data mining
static code attributes to learn defect predictors. IEEE
TSE, 33(1):2–13, 2007.
[14]A. Mockus and L. G. Votta. Identifying reasons for
software changes using historic databases. In ICSM ’00 ,
page 120, Washington, DC, USA, 2000. IEEE Computer
Society.
[15]A. Mockus and D. M. Weiss. Predicting risk of software
changes. Bell Labs Technical Journal , 5(2):169–180,
2000.
[16]R. Moser, W. Pedrycz, and G. Succi. A comparative
analysis of the eﬃciency of change metrics and static
code attributes for defect prediction. In W. Sch ¨afer,
M. B. Dwyer, and V. Gruhn, editors, ICSE, pages 181–
190. ACM, 2008.
[17]T. H. Nguyen, B. Adams, and A. E. Hassan. A case
study of bias in bug-ﬁx datasets. In Proceedings of
WCRE, pages 259–268, 2010.
[18]D. Posnett, V. Filkov, and P. Devanbu. Ecological
inference in empirical software engineering. In ASE’2011 ,
pages 362–371. IEEE, 2011.
[19]F. Rahman and P. Devanbu. How, and why, process
metrics are better. http://www.cs.ucdavis.edu/
research/tech-reports/2011/CSE-2012-33.pdf ,
2012.
[20]F. Rahman, D. Posnett, and P. Devanbu. Recalling the
“imprecision”of cross-project defect prediction. In the
20th ACM SIGSOFT FSE , pages –. ACM, 2012.
[21]R. Wu, H. Zhang, S. Kim, and S. C. Cheung. Re-
Link : Recovering Links between Bugs and Changes.
InProceedings of the European Software Engineering
Conference and the ACM SIGSOFT Symposium on the
Foundations of Software Engineering (ESEC/FSE 2011) ,
2011.
[22]T. Zimmermann, R. Premraj, and A. Zeller. Predicting
defects for eclipse. In Proceedings of the Third Inter-
national Workshop on Predictor Models in Software
Engineering , PROMISE ’07, pages 9–, Washington, DC,
USA, 2007. IEEE Computer Society.