Memory Indexing: Canonicalizing Addresses Across
Executions
William N. Sumner Xiangyu Zhang
Department of Computer Science, Purdue University
{wsumner,xyzhang}@cs.purdue.edu
ABSTRACT
Fine-grained program execution comparison examines dif-
ferent executions generated by diﬀerent program versions,
diﬀerent inputs, or by perturbations. It has a wide range of
applications in debugging, regression testing, program com-prehension, and security. Meaningful comparison demands
that executions are aligned before they are compared, other-
wise the resulting diﬀerences do not reﬂect semantic diﬀer-
ences. Prior work has focused on aligning executions along
the control ﬂow dimension. In this paper, we observe that
thememorydimensionisalsocriticalandproposeanovelso-lution to align memory locations across diﬀerent executions.
We introduce a canonical representation for memory loca-
tions and pointer values called memory indexing. Alignedmemory locations across runs share the same index. We for-
mally deﬁne the semantics of memory indexing and present
a cost-eﬀective design. We also show that memory indexing
overcomes an important challenge in automated debuggingby enabling robust state replacement across runs.
Categories and Subject Descriptors
D.2.5 [Software Engineering ]: Testing and Debugging
General Terms
Algorithms, Measurement, Reliability
1. INTRODUCTION
Comparing executions is a fundamental challenge in dy-
namic program analysis with a wide range of applications.
For instance, comparing executions of two versions of a pro-
gram with the same input can be used to isolate regression
faults [10], and analyze the impact of code changes [17]. It
helps identify implementation diﬀerences between the twoversions, which can be exploited by attackers. Comparing
program state at diﬀerent points within executions can also
be used to normalize and cluster execution traces, simplify-ing analyses that use those traces as input [6]. Execution
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies arenot made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.FSE-18, November 7–11, 2010, Santa Fe, New Mexico, USA.
Copyright 2010 ACM 978-1-60558-791-2/10/11 ...$10.00.comparison also provides unique advantages in program de-
obfuscation[7]anddebuggingcompileroptimizations, where
aggressivetransformationsmakestaticcomparisonlesseﬀec-
tive. Two executions from the same concurrent program canbe generated with schedule perturbations to conﬁrm harm-
ful data races [21], and real deadlocks [15]. In computing
cause transitions for failures [22, 19], a failing execution and
a passing execution are compared to isolate instructions orprogram states relevant to the failure.
Recently, a technique called structural indexing [21] was
proposed to align the dynamic control ﬂow of executions atthe granularity of instruction execution such that compari-
son can be carried out at aligned places. This substantially
improves accuracy in race detection [21], deadlock detec-
tion [15], and computing cause transitions for failures [19].
However, structural indexing only solves one dimension of
the problem – control ﬂow. The other unsolved dimension,orthogonal to control ﬂow, is memory. In the presence of
program diﬀerences, input diﬀerences, or non-determinism,
corresponding heap memory regions are allocated in diﬀer-ent places across runs. Therefore, although executions are
aligned along control ﬂow paths, if memory regions are not
properly aligned, comparing values is hardly meaningful.
Existing techniques rely on sub-optimal solutions [22, 17,
19], such as identifying memory using symbolic names. In
particular, to compare memory states of two executions, ref-
erence graphs [1] are ﬁrst constructedin which globaland lo-cal variables are roots, and memory regions, especially heap
regions, are connected by reference edges. Roots align by
their symbolic names; other memory regions align by theirreference paths, which consist of variable and ﬁeld names.
We call it symbolic alignment . However, symbolic alignment
is problematic in the presence of aliasing. Detailed discus-
sion can be found in Section 2.
In this paper, we propose a technique called memory in-
dexing(MI). The central idea is to canonicalize memory ad-
dresses such that each memory location is associated with acanonical value called its memory index . Memory locations
across multiple executions align according to their indices.Pointers are compared by comparing the indices of their val-ues. Memoryindicesaremaintainedalonganexecutionsuch
that they can be directly accessible or computable.
Overall, we make the following contributions.
•We formally present the memory indexing problem.
We identify key properties of valid solutions.
•We discuss two semantics for memory indexing. Theﬁrst one is an online semantics that computes indiceson the ﬂy during execution and handles pointer arith-
217metic. The second is a lazy semantics that computes
indices on demand. It has lower cost and is more suit-able for languages without pointer arithmetic.
•We introduce a practical design that uses a tree to
allow multiple indices to share their common parts.Optimizationsremoveredundanttreeconstructionand
maintenance.
•We illustratehowmemoryindexingfacilitatescomput-
ing cause transitions for failures. Novel memory com-
parison and substitution primitives resolve limitations
of existing solutions. They allow robust mutation ofa passing run to a failing run by copying state across
runs.
•We evaluate the proposed MI scheme. It causes a 41%
slow down and 213% space overhead on average. The
results of two client studies show that MI is able to
canonicalize address traces across runs, and it scalescause transition computation to programs with com-plex heap structures.
2. MOTIV ATION
Execution comparison not only requires alignment on the
control ﬂow dimension, but also on the memory dimension.
Aligning and comparing memory snapshots across runs is
thus a key challenge. Existing techniques do not provide
satisfactory solutions to the following challenges.
Figure 1: Pointer Comparison. Linked lists rep-
resent the snapshots of diﬀerent executions. Eachnode has two ﬁelds: valandnext.
Support for Pointer Comparison. Many applications
require the ability to compare pointers across runs. For ex-
ample, regression debugging [10] and computing cause tran-
sitions [22] rely on contrasting variable values in a passing
run and a failing run to identify faulty values. For pointer
related failures, it is critical to identify when a pointer con-
tains a faulty value. However, due to semantic diﬀerencesor non-determinism, even pointers that point to the same(heap) data structure can have diﬀerent values across runs,
and hence they are not directly comparable. Most exist-
ing techniques do not support pointer comparison. Instead,
non-pointer ﬁeld values, such as p→valandp→next→val
in Figure 1, are compared following their symbolic referencepaths. For the case in Figure 1 (b), such comparison yieldsthe right result. That is, only p→valhas diﬀerent val-
ues across executions. Whereas in case (a), the conclusionis thatp→valandp→next→valhave diﬀerent values,
implying the deﬁnitions to these ﬁelds are faulty in a de-
bugging application, which is not true. A more appropriate
conclusion is that onlythe pointer phas diﬀerent values, all
other diﬀerences are manifestations of the pointer diﬀerence.Figure 2: Destructive State Mutation. (a) Snapshot
in run one. (b) In run two. (c) Mutating (a) to (b).
Destructive State Mutation. Applications such as com-
puting cause transitions [22] compare memory snapshots
from a passing run and a failing run. A variable having dif-ferent values in the two respective runs is called a diﬀerence .
In order to reason about the causal relevance of diﬀerences
with the failure, values of diﬀerence subsets are copied fromthe failing run to the passing run to see if the failure is even-
tually triggered in the mutated passing run. However, using
symbolic alignment causes a destructive mutation problem
in the presence of aliasing. In particular, a memory location
may have multiple reference paths. It may be classiﬁed as
a diﬀerence when it is compared under one path but not
along another path. Mutation along one path destroys the
semantic constraints along other paths and often leads to
undesirable eﬀects. Consider the example in Figure 2. Twosnapshots are shown in (a) and (b) with ppointing to diﬀer-
ent locations in each. With symbolic alignment, the root p
is aligned ﬁrst, followed by nodes along paths from p.A sa
result, the ﬁrst node in (a) is aligned with the second nodein (b), the second node in (a) with the third node in (b),
and so on. Comparing the non-pointer ﬁelds of the aligned
nodes yields the following reference paths denoting diﬀer-ences:p→val,p→next→valandp→next→next→val.
They are ﬁelds in (b) having values diﬀerent than those in(a). Suppose we try to mutate (a) to (b) by copying val-ues from (b) to (a) following the paths of diﬀerences. The
resulting state is shown in (c). Observe that t’s value is
undesirably destroyed.
Figure 3: Lost Mutation.
Lost Mutation. If multiple diﬀerences alias, they may re-
sult inlost mutation when they are applied together. Specif-
ically, the diﬀerences applied earlier may be overwritten un-
desirably by diﬀerences applied later. This may lead to in-correct conclusions about the relevance of diﬀerences. Con-
sider the example in Figure 3. Assume the failure is that
(p→val)+(t→val) has the wrong value. Pointer tpoints
to the wrong place, and the node pointed to by phas the
wrong value. These together cause the failure. Symbolic
alignment and non-pointer value comparison identiﬁes twodiﬀerences denoted by their reference paths: p→valand
t→val. However, as pandtalias in (a), when the dif-
ferences are applied to (a) in the order of pdiﬀerence ﬁrst
and then tdiﬀerence, the rightmost leaf ﬁrst has the value
2182 and then 1. The mutated state does not lead to the ex-
pected failure. Hence, we mistakenly conclude that the twodiﬀerences are irrelevant to the failure.
3. PROBLEM STATEMENT AND OVERVIEW
To overcome the aforementioned problems and provide ro-
bust support for memory comparison and mutation, we pro-
pose a novel technique called memory indexing . The basic
challenge is to associate each memory location with a canon-
ical value such that locations across runs are aligned by their
canonical values; pointers can be compared by their canonical
values. Such values are also called memory indices because
they essentially provide an indexing structure for memory.
Figure 4: Overview of Memory Indexing.
The idea is illustrated by Figure 4, which revisits the ex-
ample in Figure 3. Focus on the parts inside the boxes for
now. Each node is associated with a canonical value (in-
dex) circled at a corner. Nodes are aligned by their indices.
Hence, we can see the root nodes align as they have the same
indexα. The node with index δon the right does not align
with any node on the left. Besides its concrete value, pointer
palso has a canonical value πin both runs. Pointer thas
πon the left but δon the right. With memory indexing,
the diﬀerences of the two states can be correctly identiﬁed:
pointerthas a diﬀerent pointer value and the nodes pointed
to byphave diﬀerent ﬁeld values. When mutation occurs,
tis set to the location with index δ, which is not present in
the passing run and thus entails allocation. p’s ﬁeld value is
changed to 2. Such mutation properly induces the failure.
A valid memory indexing scheme should have the follow-
ing property: at any execution point, each live memory loca-
tion must have a unique index . We call this the uniqueness
property. If this property is not satisﬁed, multiple locations
may share the same index or one location may have multi-
ple indices, which makes proper alignment across runs im-
possible. Symbolic alignment does not always satisfy thisproperty and is thus not a good indexing scheme.
A good indexing scheme should have the following addi-
tional feature: locations across runs that semantically corre-
spond to each other should share the same index .W ec a l lt h i s
thealignment feature. Using the concrete address of a mem-
ory location is an indexing scheme satisfying the uniqueness
property, but it does not deliver good alignment.
Figure 5: Graph Matching may be Undesirable.
Inadequacy of Graph Matching. Finding the most ap-
propriate memory alignment concerns program semanticsand is thus, in general, not a concretely knowable problem.
As pointed out in [22], one possible approximate solution inthe general case is to represent the memory snapshots undercomparison as reference graphs and formulate the alignment
problem as a graph matching problem [2]; the goal of which
is to produce a match with the minimal number of graph dif-ferences. However, this solution is too expensive (NP com-
plexity) to be practical [22]. More important, we observe
that it fails to deliver desirable alignment in many cases be-cause it does not capture semantic diﬀerences. Consider the
example in Figure 5. The failure in (b) occurs because the
value ﬁeld passed to the node constructor is incremented byone. With a graph matching algorithm, to minimize graph
diﬀerences, the second node in (a) aligns with the ﬁrst node
in (b), the third node in (a) aligns with the the second nodein (b), and so on. As highlighted in the ﬁgure, the graphdiﬀerences, namely the graph operations needed to mutate
(a) to (b), are: add (b)’s tail to (a); add the edge to the
added node; remove the head in (a). However, such diﬀer-ences imply that the shape of the linked list is faulty, which
is not true. The most appropriate alignment matches the
corresponding nodes in the lists, resulting in four ﬁeld valuediﬀerences that precisely reﬂect the semantic diﬀerences.
Our Indexing Scheme. We propose to use the execu-
tion point where the allocation of a memory region occurs as
the index of the region. We leverage the observation that se-
mantic equivalence between executions often manifests itselfas control ﬂow equivalence. Hence, semantically equivalentmemory regions are often allocated at corresponding execu-tion points. Figure 4 presents an overview. The two lines in
the middle represent the control ﬂows. The memory indices
are essentially canonical ﬂow representations of the alloca-
tion points. For instance, the root nodes share the memory
indexα, indicating they are allocated at the same point α.
In contrast, index δ’s presence in only the failing run means
that the allocation does not occur in the passing run. Ourindexing scheme satisﬁes the uniqueness property and pro-
vides high quality alignment in practice.
4. SEMANTICS
In this section, we present two semantics for memory in-
dexing. The ﬁrst is for low level languages such as C. It
supports pointer arithmetic by updating indices on the ﬂy.
This is called the online semantics . The other semantics
computes indices on demand and does not need interpreta-
tion of pointer arithmetic. We call this the lazy semantics .
Inour semantics, each memory locationis canonicallyrep-
resented by a pair ( region,oﬀset), withregionas the canon-
icalized representation of its containing allocated region and
oﬀsetas its oﬀset inside the region. The canonical represen-
tation of a region is generated when the region is allocatedand serves as a birthmark of the region during its lifetime.We provide a function MI() that maps a concrete address
to its index. We also maintain a function PV() that maps
a pointer variable to the index of the value stored in the
pointer. In the lazy semantics, PV( p) is lazily computed
from MI( p), whereas in the online semantics PV( p) is up-
dated on the ﬂy through pointer manipulations on pointerp.H e n c e ,P V ( p) may be diﬀerent than MI( p) in the online
semantics. As we will later show, separating PV from MIallows us to precisely handle pointer arithmetic, which isdesirable for certain applications.
219Rule Event Instrumentation
(5) Prog. starts for each global variable g:
MI(& g)= (nil, global_offset (g))
(6) Enter proc. X for each local variable lvofX:
MI(& lv)= (CS, local_offset (lv))
(7) pc:p=malloc(s) for i=0 to s-1:
MI(p+i)= ([SI, pc],i)
PV(p)= MI( p)
(8) p=&v PV(p)= MI(& v)
(9) p=q PV(p)= PV( q)
(10) p=q±offset PV(p)=(PV( q).ﬁrst,
PV(q).second ±offset)
Figure 6: Online semantics. A memory index MI( a)
represents the memory index of an address a,w h i c h
is a pair comprising a region identiﬁer and an oﬀ-
set. CS represents the current call stack. pcrepre-
sents the program counter. SI represents the cur-rent structural index. PV( p) represents the memory
index of the address value stored in p.
4.1 Online Semantics
In this subsection, we discuss the online semantics.
Indexing Global Memory. We consider global memory
locations as part of a global region . Hence, the memory in-
dex of a global location is its oﬀset in the global region (Rule5 of Figure 6). In our terminology, & gdenotes the concrete
address of a variable g. If executions from diﬀerent program
versions are considered, e.g. in comparing regressing execu-
tions, symbolic names of variables are used instead of theiroﬀsets. It is easy to see the uniqueness property is satisﬁed.
Indexing Stack Memory. We consider stack memory to
be allocated upon function entry. The allocated region is
the stack frame of the function. Hence, we use a stack frame
identiﬁerandthe stackframeoﬀset of a locationto represent
its index. Recursive calls allow multiple instances of the
same function to exist in the call stack at an execution point
so that we have to use the call path of a stack frame as its id.
Such stack indices trivially satisfy the uniqueness propertyand providemeaningfulalignment. This is presentedin Rule
6ofFigure6. Someprogramsperformdynamicallocationon
the stack, which makes stack variables have varying oﬀsets.We identify such variables through static analysis and useour own IDs to replace the oﬀsets.
4.1.1 Indexing Heap Memory.
The essence of our technique is to create a birthmark of a
memory location as its canonical representation. The birth-
marks of heap locations are more tricky. Using the program
counter (PC) of the allocation point is not suﬃcient because
multiple live heap regions may be allocated at the same PC.
Thecallingcontextof theallocationpointis notsuﬃcientei-ther. For example, the code in Figure. 7 (a) creates a linked
list in the loop on lines 1 and 2. All allocations occur in the
same context (statement 2 inside F()). Adding an instance
count does not help either because diﬀerent executions may
take diﬀerent paths so that the same count does not imply
correspondence. Inthispaper, weutilize structural indexing .
Background: Structural Indexing [21] is a technique
that provides a canonical representation for execution points
within control ﬂow such that points across runs can be
aligned. Readers familiar with structural indexing can skip
to the ending 2symbol.
Conceptually, an execution is indexed by a tree represent-
ing its nesting execution structure. A leaf node denotes anexecution point, and internal nodes represent control struc-
turessuchasbranches, loopbodies, andmethodinvocations.
Consider the program in Figure 7 (a). The index trees for
the two runs are shown in Figure 7 (b) and (c). The two
control ﬂow traces are displayed horizontally from left to
right. Individual trace points are also the leaves. Considerthe (b) tree. The root represents the whole execution, which
consists of six statement executions presented as the leaf
children of the root, namely statements 1, 3, 4, 5, 10, and11. Since the forstatement has substructure, an internal
node 1
T, pronounced as “the true branch of statement 1”,
is created to represent the loop body. The remainder is
constructed similarly. Traces are aligned by the index trees
in a top-down fashion. First, the two roots align. Then the
six leaf children align, dictated by the aligned parents. Note
that their alignment is independent of the branch outcomes of1a n d5. Internal nodes may or may not align, depending on
their labels (i.e., the branch outcomes). If they do, recursive
alignment is performed. The two trees in Figure 7 (b) and(c) align except for the subtree rooted at 5
T.
The structural index of an execution point is the path lead-
ing from the root to the leaf node representing the point .
For instance, the index of the shaded 7 in the left tree is
[F,5T,7]. Deciding the presence of an execution point in
other runs is equivalent to deciding the presence of its index
in the corresponding trees. An important property is that
each dynamic point in an execution has a unique index .
Rule Event Instrumentation
(1) invoke function FSI.push( Fc)
at call cite c
(2) Exit proc. F SI.pop()
(3) Predicate ptakes SI.push( pB)
branch B
(4) Statement s while ( sis the immediate post-
dominator of SI.top()) SI.pop()
*SI is the structural index represented as a stack.
Figure 8: Semantics for Structural Indexing.
Structural indices are computed using control dependence
analysis [8]. Figure 8 deﬁnes the semantics of structural
indexing. Each internal node in an index tree representsan execution region delimited by a function entry and its
exit or by a branch point (including loop predicates) and
its immediate postdominator. Regions are either disjoint ornesting, analogous to function invocations. Hence, a stack
similar to a call stack, named the structural indexing stack
(SI), is used to maintain the structural nesting relation. An
entry is pushed upon function entries (Rule 1) or predicateexecutions (Rule 3). The top entry is popped upon the exit
of a function (Rule 2) or when the immediate postdominator
of the predicate on top is encountered (Rule 4). The SI stackalways contains the structural index of the current execution
point. More details and examples can be found in [21]. 2
To index heap memory, we use the structural index of the
allocation point as the id of an allocated region to composethe memory index. The uniqueness property of structural
indexing ensures the uniqueness of heap indices. The align-ment feature of the memory indexing scheme also originates
from the fact that structural indexing identiﬁes equivalent
allocation points across executions. In particular, heap in-dices are set when a region is allocated (Rule 7 in Figure 6).
A heap index consists of the current SI and the allocation
sitepc. Besidessettingthememoryindices, therulealsosets
the canonical value of the pointer variable, i.e. PV( p), to the
220Figure 7: Example for heap indexing. The code constructs a linked list of three nodes with values of 0, 1
and 3. Initially, the three pointers h,p,a n drall point to the head of list. There is a regression bug at
line 4 in computing the predicate. As a result, the failing run takes the false branch, making ppoint to its
second node. Pointer pfurther advances to the third node at line 10. In contrast, the passing run takes the
true branch, eventually resulting in both pandrpointing to its third node. The failure is observably wrong
output. The memory snapshots are before the failure at statement 11.
memory index of the head of the region. Such a canonical
value will be used in pointer manipulation. For example,in the second iteration of the loop in Figure 7, after theallocation in statement 2, PV( h)=([F,1
T,1T,2], 0).
Memorylocationsacrossmultiplerunsarealignedbytheir
indices. By this criterion, in Figure 7, the head of the list
in (b) does not have alignment while the remaining three
nodes align with the list in (c).
A key feature of memory indexing is pointer value com-
parison across runs. Besides a concrete memory address,
a pointer variable is also associated with a canonical value.
Canonical pointer values are updated on the ﬂy in the onlinesemantics, asspeciﬁedbyRules8-10. Forbrevity, weassume
a simple syntax for pointer operations. In particular, if the
address of a variable vis retrieved and assigned to a pointer,
the canonical value of the pointer is the memory index of v’s
address (Rule 8). If a pointer variable is copied to another
variable, the canonical value gets copied too (Rule 9). For
pointer arithmetic expressions p=q±offset,v a r i a b l e p’s
canonical value is computed by copying the region identiﬁer
ofqand adding rto the oﬀset of q(Rule 10). For brevity,
our semantics assumes type information has been processed
so that oﬀset variables are identiﬁed at the unit of bytes.
4.2 Lazy Semantics
For high level languages in which pointer arithmetic is not
permitted, or when client applications do not require con-
sidering the eﬀects of pointer arithmetic, we can derive PV
values on demand and hence allow a more eﬃcient imple-
mentation. The semantics is called the lazy semantics .T h e
observation is that canonical values of pointers can be lazily
inferred from their concrete values. That is, given a pointer
p,P V (p)=MI(p). Recall that in the online semantics, PV is
computed by interpreting pointer arithmetic (Rule 10) and
hence PV( p) is not necessarily equivalent to MI( p).
Rule Event Instrumentation
(11) pc:p=malloc(s) MI( p)=([SI, pc], 0)
(12) Query the index t=a
of heap address a while(MI( t)≡nil)t=t-1
return(MI( t).ﬁrst, a-t)
Figure 9: Lazy Semantics.
The new rules are presented in Figure 9. On the ﬂy com-
putation is only needed upon heap allocation (Rule 11): thecurrent SI is assigned to the region base address, but not
to the other cells in the region. When the MI value of aheap address is queried (Rule 12), the algorithm scans back-wards from the given address to ﬁnd the ﬁrst address with
a non-empty index. The memory index of the given address
consists of the region denoted by the non-empty index andthe oﬀset inside the region. For large heap regions, we set
the MI for a number of addresses at set intervals besides the
base address such that the linear scan can quickly encountera non-empty MI. No on-the-ﬂy computation is needed for
global and stack memory. The MI values of global and stack
addresses can similarly be computed on demand.
In languages with pointer arithmetic, the lazy semantics
does not instrument pointer operations or track the origi-
nal regions of pointers. The looser coupling with programsemantics may lead to undesirable imprecision in certain ap-plications. Details of when this may occur can be found in
our extended technical report [20].
5. DESIGN AND OPTIMIZATIONS
Rule Event Instrumentation
(15) pc:p=malloc(s) l=new Leaf( pc,p,s)
Tree Insert(SI,l)
MI(p)=(l,0 )
(16) free(p) Tree Remove (MI( p).ﬁrst)
Figure 10: Tree based Indexing in Lazy Semantics.
Thesemanticsintheprevioussectionareconceptual. They
model an index as a sequence of symbols (the region) and an
integer (the oﬀset). This is too expensive to operate with inpractice. In our design, we explicitly maintain an index tree
for heap memory and represent a heap region as a reference
to some leaf in the tree. The full index of a heap location can
be acquired by traversing bottom-up from the leaf. Rules
15 and 16 show the tree based instrumentation for the lazy
semantics. Upon heap allocation (Rule 15), a leaf node rep-resenting the allocation is created and inserted into the tree
by calling Tree
Insert(). The function ﬁrst checks if the cur-
rent SI is part of the tree. If not, it adds the SI to the tree
before it inserts the leaf node. At the end, the instrumenta-
tion sets the MI of the region base address to the leaf. Upon
deallocation (Rule 16), TreeRemove() is called with the leaf
node corresponding to the to-be-freed region. Recursive tree
221elimination is performed, meaning that removing a leaf node
may lead to removing its ancestors if they have no children.Shaded subtrees in Figure 7 are example heap index trees.
Dotted edges link leaf nodes to memory regions. We have
the following optimizations to make our design practical.
OPT-1: Removing Redundant Instrumentation. We
have two observations that help remove redundant instru-
mentation. The ﬁrst one is that we only need a partial
structuraltreetoindexheapallocations. Hencewecan avoid
instrumentation that maintains irrelevant structural indices.
If a function does not allocate heap memory, it is not nec-
essary to compute structural indices inside that functions.More formally, a function or a predicate branch is relevant
to heap allocation if and only if a heap allocation can di-
rectly or transitively occur in its body. Irrelevant functions
and predicates are not instrumented with pushes and pops.
The second observation is that we do not need to instru-
ment all relevant functions or predicate branches. Morespeciﬁcally, given a relevant function other than main(pred-
icate)n, if all index paths from any of n’ sp a r e n t st oah e a p
allocation inside n’s body have to go through n,w ed o n ’ t
need to instrument n.W ec a l l nadominant function (pred-
icate). Intuitively, we don’t need to instrument if we can
infer the presence of non an index path given the allocation
site and the parent node. We have developed static anal-
yses to identify relevant but not dominant functions and
predicates. They are analyses on call graphs and control
dependence graphs. Details are elided. Note that such opti-
mizations are not applicable to general structural indexingbecause they leverage heap allocation information.
OPT-2: Handling Loop Nodes. From the semantics,
loops require pushing multiple entries of the same predicate,
which can be optimized as follows. As in [21, 15], we add
a counter ﬁeld to the stack entry; then upon encountering
a loop predicate, it is ﬁrst checked if the top entry is thesame predicate. If so, the counter is incremented instead
of pushing. When the current stack is materialized to the
tree due to heap allocation, a new node is inserted to thetree if there is not an existing node with the same counter
value. The optimization does not aﬀect the uniqueness and
alignment properties of indexing.
The space consumption is dominated by the tree, whose
size depends on its shape and the number of live heap re-
gions. A pessimistic bound is O( maximum tree depth ×
maximum live heap regions ). In theory, the tree depth is un-
bounded because it is tied with loop counts and the depth ofrecursion. In practice, because we are only interested in the
partial tree for allocations and we optimize loop predicatesusing counters, tree depth is often well bounded such that
the space overhead is feasible (see Section 7).
6. ROBUST MEMORY COMPARISON AND
REPLACEMENT
Cause transition computation [5, 22] produces a causal
explanation for a software failure. The technique takes two
executions: one failing and the other passing that closely re-
sembles the failing. The passing run can be generated by se-
lecting an input similar to the failing input. The overall ideais to compare memory snapshots of the two runs at selected
executionpoints. Areferencegraph[1] isconstructedtorep-
resent a snapshot. Memory comparison is reduced to graphcomparison driven by symbolic reference paths. Causalitytesting is conducted to isolate a minimal subset of graph dif-
ferences relevant to the failure. More speciﬁcally, subsets ofgraph diﬀerences are enumerated through the delta debug-ging algorithm. A subset is considered relevant if replacing
the program state speciﬁed by the subset in the passing runwith the corresponding values in the failing run produces thefailure. The minimal subsets computed at the selected ex-
ecution points are chained together to form an explanation.In [19], the technique is improved by automatically aligningtwo execution traces using structural indexing before com-
parison. However, in both [5] and [19], memory comparison
and replacement is driven by symbolic paths, and hence hasthe issues mentioned in Section 2.
Consider the example in Figure 7. Using symbolic align-
ment and comparing only non-pointer values, if only heapmemory is considered, the set of diﬀerences (failing - pass-ing) Δ= {p→val,r→val,r→next→val,r→next→
next→val,h→next→val,h→next→next→val}.
None of the subsets, including the Δ set itself, can inducethe same failure. For instance, applying subset {p→val,
r→val}does not work due to the lost mutation prob-
lem. As a result, the delta debugging algorithm terminateswithout ﬁnding the minimal failure inducing subset. Since
aliasing is very common in general programs, we need to
perform robust memory comparison and replacement.
With MI, we are able to develop two robust primitives:
comparison of memory snapshots Mem
Comp() and appli-
cation of a memory diﬀerence DiﬀApply(), i.e., copy a value
from one memory snapshot to the other (across runs).
For the comparison primitive, snapshots are ﬁrst aligned
bytheirindicesandthencomparisonisconductedonaligned
locations. Memory locations with non-pointer types arecompared by their concrete values. Locations with pointer
types are compared by their canonical values. Diﬀerences
are presented as a set of indices, denoting that the corre-sponding locations are diﬀerent.
Consider the two snapshots in Figure 7. Global variables
C,h,p,a n drare aligned. Since Cis of boolean type, its
values are compared and classiﬁed as diﬀerences. In con-
trast, canonical value comparison is conducted for pointer
variables h,pandr. It is easy to see that they are diﬀerent.
Heap memory is compared by the index trees. The regionpointedtoby hin (b) is identiﬁed as the only tree diﬀerence.
Hence, if we compute the diﬀerence set (passing - failing),the result is {(nil, oﬀset( C)), (nil, oﬀset( h)), (nil, oﬀset( p)),
(nil, oﬀset( r)), ([ F,5
T,7], *)}. The symbol ‘*’ in the last in-
dex signiﬁes that the entire region is diﬀerent. It is much
smaller than the symbolic results.
The second primitive is the application of a unit diﬀer-
ence1represented as an index, from which the corresponding
concrete memory location in both snapshots can be identi-ﬁed. The value is copied from the source snapshot to the
target snapshot. If the value is a pointer, we cannot simply
copy the concrete address. Instead, we identify the properconcrete address in the target snapshot following the canon-ical value of the pointer. If the region is not present in the
target snapshot, it is ﬁrst allocated.
Function Diﬀ
Apply() in Algorithm 1 describes how to
apply a heap unit diﬀerence. In the algorithm, the source
and target heaps are indexed by trees rooted at T/primeandT,
respectively. Variable δrepresents the unit diﬀerence. Lines
1A unitdiﬀerenceis a diﬀerenceregardinga speciﬁcmemory
location instead of a region.
222Algorithm 1 Apply a heap diﬀerence.
Description : Copy the value in location δfromT/primetoT. Leaf
node is of the type ( pc,base,size).
1:DiﬀApply (T,T/prime,δ):
2: letδbe (path ,oﬀset )
3: let(-,base/prime, -) be the leaf node in T/primealongpath
4: let(-,base, -) be the leaf node in Talongpath
5:a←base +oﬀset
6:a/prime←base/prime+oﬀset
7: if(*(a/prime)T/primeis NOT a pointer) then:
8: ∗(a)T←∗ (a/prime)T/prime
9: else
10: letPV(a/prime)T/primebe (p/prime,f/prime)
11: if(Tdoes not have path p/prime)then:
12: RegionCopy (T,T/prime,p/prime)
13: let(-,b, -) be the leaf node in Tfollowing p/prime
14: ∗(a)T←b+f/prime
Description: copy region path/primeinT/primetoT.
1:RegionCopy (T,T/prime,path/prime):
2: let(-,base/prime,size/prime) be the leaf in T/primealongpath/prime
3:r←allocate( size/prime) in the run denoted by T
4: insertpath/primetoT
5: set the leaf node following path/primeinTto (-,r,size/prime)
6: for(i=0 tosize/prime−1)do:
7: DiﬀApply (T,T/prime,(path/prime,i))
3 and4 identifytheheapregiondenotedby δinT/primeandT.I n
lines 5 and 6, the concrete addresses are computed. At line
7, the algorithm tests if the computed address is a pointer
(the superscript speciﬁes where the dereference occurs). If
not, the algorithm copies the value (line 8). If so, it testsif the region pointed-to is present in T(line 11). If not, it
copies the region (line 12). Finally at line 14, the concrete
address stored to the pointer is set to a location in the region(inT) aligned with the source region (in T
/prime).
Function RegionCopy()copies a region denoted by the
parameter path/primefromT/primetoT. It ﬁrst locates the region in
T/prime(line 2) and allocates a region of the same size in T(line
3). The path/primeis inserted to Tand a leaf node is created
to represent the allocated region (lines 4-5). This avoids
allocating the same region again. Finally, individual ﬁelds
are copied from T/primetoTby calling DiﬀApply() (lines 6-7).
Note, it may transitively copy more regions from T/primetoT.
Applying stack and global diﬀerences is similarly deﬁned.
Example. Consider the example in Figure 7. Assume we
want to apply the diﬀerences of pandrto the passing run.
Observe that ppoints to the third node in the failing run
and PV( p)fail=( [F,1T,1T,1T,2], 0). During the pdiﬀerence
application, following the path, the concrete address of the
fourth node in the passing run is identiﬁed and assigned
top. Similarly, after applying the rdiﬀerence, rholds the
concrete address of the second node in the passing run. Note
that, by applying these two diﬀerences, the same failure can
be produced. Applying other diﬀerences, such as C,a tt h i s
point (before statement 11) has no impact on the failure.
The minimal failure inducing diﬀerence subset including p
andris emitted as one cause transition.
The same memory comparison and diﬀerence minimiza-
tion is further performed at aligned instructions 10 and 5; it
stops at 4 as no state diﬀerence is identiﬁed. The chain ofcausetransitionsis: Chastheincorrectvaluefalseat5, thenTable 1: Instrumentation and allocation.
program instmt. instmt. # of alloc avg. alloc tree
func branch stat/ dyn. size dep.
164.gzip 11 (12%) 17 (1.8%) 5 / 436k 28 KB 130
175.vpr 100 (37%) 202 (7.5%) 3 / 107k 481 B 59
176.gcc 1282 (57%) 17774 (24.9%) 236/10.2m 5K B 700
181.mcf 5 (19%) 6( 2 % ) 4/ 3 33 MB 8
186.crafty 8 (7.3%) 158 (2.9%) 12 / 37 23 KB 6
197.parser 2 (0.6%) 41(1.3%) 1/ 1 31 MB 292
254.gap 596 (70%) 4109 (19%) 2/ 2 100 MB 10
255.vortex 672 (73%) 3400 (19%) 9 / 258k 399 B 365
256.bzip2 8 (11%) 7 (1.1%) 10 / 36 16 MB 51
300.twolf 59 (31%) 218 (3.5%) 3 / 574k 31 B 28
pandrpoint to the wrong places at 10 and 11, and ﬁnally
the failure. These transitions compose a failure explanation.
Next, we deﬁne the composability property and show that
it holds for the proposed primitive.
Definition 1.A scheme for memory diﬀerencing and
replacement is composable iﬀ given a set of unit diﬀerences
Δ={δ1,δ2, ...,δn}and the universal set Uof all diﬀer-
ences, after applying the diﬀerences in ΔfromT/primetoT,t h e
diﬀerences between T/primeand the mutated TisU−Δ.
Composability is very important for cause transition com-
putation, it ensures that the delta debugging algorithm isable to make progress, because it mandates that the eﬀectof applying a set of diﬀerences must subsume the eﬀect ofapplying a subset of the diﬀerences [22]. If a replacement
scheme is not composable, applying the universal set of dif-
ferences may even fail to convert TtoT
/prime. The symbolic
path based scheme is not necessarily composable. As shown
in Figure 7, applying the two diﬀerences of p→valand
r→valfrom the failing run to the passing run results in a
state in which p→valstill manifests itself as a diﬀerence.
Property 1.The proposed MI based memory diﬀerenc-
ing and replacement primitive is composable.
The proof is elided due to space limits. However from
Algorithm 1, we observe that for non-pointers, the primitive
faithfully copies values; hence the property is trivially true.
For pointers, the primitive either allocates a region when itis not present in the index tree or simply assigns the address
if the region is present. Such behavior does not lead to
additional diﬀerences that were not present in the original
diﬀerence set or mask any other existing diﬀerences.
7. EV ALUATION
The implementation consists of both semantics and two
client studies. It is based on the CIL infrastructure and has
3500 lines OCaml, 3500 lines C and 3000 lines Python.
7.1 Efﬁciency
The ﬁrst experiment focuses on cost. The evaluation is
on SPECint 2000 benchmarks. We excluded 252.eon and
253.perlbmk because they were not compatible with CIL.
All experiments were executed on an Intel Core 2 2.1GHz
machine with 2 GB RAM and running Ubuntu 9.04.
Table 1 shows the instrumentation needed and character-
istics of allocations. All executions are acquired on reference
inputs. The second column shows the number of instru-
mented functions (after optimizations) and their percentageover all functions. The third column shows the same data
223for predicates. The fourth column shows the numbers of
static allocation sites and dynamic allocations. The ﬁfthcolumn shows the average size of each allocation. The last
column shows the maximum depth of the memory index
tree. We observe that some programs make a lot of alloca-
tions with various sizes ( gccandtwolf)and some make very
few but large allocations ( mcfandbzip2). They have diﬀer-
ent impacts on the performance. Programs parserand gap
allocate a memory pool at the beginning and then rely on
their own memory management systems. Our current sys-
temdoesnottraceintomemorypoolmanagement. Weleave
it for future work. Observe, the maximum tree depth is nothigh with respect to the structural complexity of programs.
Recall that we collapse consecutive instances of a loop pred-
icate, so the depths are largely decoupled from loop counts.
The overhead can be seen in Figure 11, in which Fullrep-
resents implementation without removing redundant instru-
mentation; Partremoving redundant instrumentation; Flow
t h eo n l i n es e m a n t i c s ;a n d Lazythe lazy semantics. The
ﬁgure presents the performance overhead for a number ofcombinations. In practice, Part+Lazy is desirable for most
applications, as illustrated by later client studies. Space
represents the space overhead for Part+Lazy . All data is
normalized against original runs without instrumentation.
We observe ﬁrst that Full+Lazy has substantially more
runtime overhead than Part+Lazy .Part+Flow is slightly
more expensive than Part+Lazy due to instrumentation on
pointeroperations. Theoverheadof Part+Lazy islow(41%).
Next, observe that in half the benchmarks, there is little
space overhead. This is because the number of allocations
and the tree depth are relatively low regarding the size of
each allocation. In contrast, 300.twolf had the most over-
head. It performs a large number of very small allocations,
<32 bytes on average, so on average maintaining the index
for each allocation is more costly
2. Nonetheless, the average
space overhead is 213% (111% without twolf). The conclu-
sion is that the cost of MI is feasible for many applications.
7.2 Trace Canonicalization
Trace canonicalization is the alignment of control ﬂow
and memory accesses across traces from two executions. It
plays a part in debugging and regression analyses [17, 10,
7], among others. With MI, an important question can be
answered, given two address entries in two respective traces,
should they be considered diﬀerences? Note, two accesses at
the corresponding points in the two traces do not mean that
they operate on the same data; the accessed addresses being
diﬀerentdoesnotmeantheydonotsemanticallycorrespond.
The study is on three common, open source programs,
make,gawk,a n d dot. We reported the number of address
diﬀerences before and after MI canonicalization. We turnedoﬀ all memory layout randomization. To avoid comparing
trace entries that do not correspond, we used structural in-
dexing to establish which memory accesses occurred at thesame point in both traces and only compare those accesses.
The traces were generated from the programs’ provided
test suites or, in the case of dot, the provided examples
in the documentation. Each full trace was compared withtraces generated by a ﬁxed percentage of the input, i.e., re-
moving part of the inputs. The results are shown in Fig. 12.
For each percentage of input similarity, we present the per-centage of matched stack and heap memory accesses before
2In our implementation, we use 22 bytes for each tree node.and after canonicalization. For MI, these are ‘ MI locals ’
and ‘ MI allocs ’ respectively, while for the addresses with-
out canonicalization, they are ‘ Addr allocs ’a n d‘ Addr lo-
cals’. We furthermore present the percentage of control
ﬂow correspondence (‘ Control Flow ’).
Observe ﬁrst that MI provides a substantially higher level
of heap access correspondence (50% more for makeand 50-
60% more for gawk, and 15-30% more for dot). Less beneﬁt
was observed in dotbecause dot’s dynamic allocations are
largely on ﬁxed buﬀers that do not change according to in-
puts. MI was able to ﬁnd more corresponding addresses for
local variables too. Observe that stack local allocation andvariable sized objects on the stack make it more diﬃcult to
ﬁnd correspondences without MI (e.g. the makecase).
The control ﬂow similarity increases as the input similar-
ity increases. Note that the address correspondence without
canonicalization stays roughly the same or even decreases
as the control ﬂow similarity increases (like in dot). The
decrease happened because greater control ﬂow similarity
allows more (diﬀerent) addresses to be compared. In con-
trast, the correspondence found by MI is roughly consistent.
7.3 Cause Transition Computation
This experiment evaluates the impact of MI on computing
cause transitions. The algorithm in [19] was implemented
as a platform on which we tested two versions of the mem-
ory comparison and replacement primitive: one is symbolicpath based, used in [5, 19]; the other is the new MI-based.
The study is on several real bugs in open source programs,
including gcc,make,a n d gawk, that have non-trivial heap
behavior and aliasing. The failing runs are generated ac-
cording to the bug reports. The passing runs are acquired
from the correct inputs in the reports if provided; previ-ous non-regressing versions; or using an automated patch-ing technique [23]. Note that acquiring passing runs is an
orthogonal problem out of our scope. Other patching tech-
niques such as [12] can also be used.
Results are summarized in Table 2. The Program column
contains the buggy programs. Bug IDpresents the bug id,
through which one can identify the report online, or thepublication date on the mailing list. Bugdescribes each bug.
Passing Run shows the sources of the passing runs: inputs
provided in reports ( correct input ), non-regressing ver-
sions ( non-regressing ), and dynamic patching ( predicate
switch). The maximum number of diﬀerences (present in
failing and absent in passing) found using symbolic diﬀer-encing is presented in Sym Diffs , and the maximum when
using MI is in MI Diffs .I nSym Diffs , we report one mem-
ory cell only once although it may be a diﬀerence alongmultiple paths. Of further interest is the number of diﬀer-ences with aliases (in column Aliases), or multiple symbolic
paths. They can cause issues as discussed in Section 2. Col-
umn Issuepresents the exhibited problems when using the
symbolic path based primitive. We also present the number
of transitionsand the average number of diﬀerences included
in each transition in Trans/Diffs , along with time required
(in seconds) in Timewhen using the MI version.
Observe that in every case, the number of symbolic dif-
ferences is substantially, 2-50 times, larger than the numberof diﬀerences when using MI, because the proper memory
correspondence cannot be found. Furthermore, the Aliases
column shows that substantial aliasing is common, creating
a lot of diﬃculties for the symbolic method. As seen in the
224Figure 11: Normalized runtime and space overheads of memory indexing with and without optimizations.
Figure 12: Percent of corresponding memory accesses w. and w/o MI.Figure 13: Heap accesses and control
ﬂow trace similarity after state muta-
tion in the execution of make.
Issuecolumn, in most cases, symbolic path based computa-
tion would not terminate within 12 hours. The main reason
is that lost mutation and destructive mutation caused (see
Section 2) by aliasing prevent the relevant diﬀerence subset
from being computed, so the algorithm ends up searching
subsets of the already signiﬁcant diﬀerence sets. For ex-
ample, in the ﬁrst gcccase, it may be possible that all the
enumeratedsubsetsof the 8365diﬀerences need to be tested.
Ingawk, the algorithm simply terminated early, unable to
produce relevant transitions for the failure.
A Case Study on Detailed Comparison. We performed a
separate test focusing on makebug 18435 from Table 2. We
selected 10 sample points at 10% intervals along the part
of the passing run that is beyond the ﬁrst divergence of the
two runs. At each sample point, we compared the memorysnapshotsacrossthetworunsandthenmutatedthememory
in the passing to that in the failing by applying the univer-
sal set of diﬀerences, alternatively using the symbolic pathbased primitive [5, 19] and the MI based primitive. Then
we collected the trace after the mutation and compared it to
that of the failing run. Trace comparison is conducted usingcontrol ﬂow canonicalization and MI based memory canon-icalization. According to the discussion in Section 6, the
traces should be identical if the primitives are composable.
The results are shownin Figure13. ‘ Heap (MI) ’and‘ Con-
trol Flow (MI) ’ represent the similarities of heap access
and control ﬂow traces using the MI based primitive, and
‘Heap (Sym) ’a n d‘ Control Flow (Sym) ’ represent those us-
ing the symbolic primitive. Observe, the access similarity
when using MI is consistently near 100%, and the control
ﬂow similarity is consistently above 90% until the end. Thismeans the mutation is mostly successful in turning the pass-
ing run to the failing run. The similarity is not 100% be-
cause we currently do not model external state such as ﬁleIDs, process IDs, etc. Thus, such states are not eligiblefor meaningful comparison and replacement. In contrast,
when the symbolic primitive is used, the execution quicklydivergesfromtheexpectedcontrolﬂow, havingnear0%sim-
ilarity, and it has near 0% similarity for accessing the heap.In fact, the mutated run often quickly crashes due to de-
structive mutation (Section 2). This supports that the MI
primitive is composable, but the symbolic primitive is not.
A more in-depth consideration of memory indexing and con-
crete examples of computing cause transitions can be found
in our tech report [20].
In summary, MI allows the strength of cause transition
computation to be fully realized, reﬂected by our success of
scaling to programs like gccwith full automation. Note,
although a gcccase was presented in [22]. It was conducted
with human intervention. The later automated system [5]works well for small programs without much aliasing.
8. RELATED WORK
Trace normalization [6] divides traces into segments. Seg-
ments with the same starting and ending state are consid-
ered equivalent. Client applications using such traces only
need to look at a consistent representative segment fromeach equivalence class. The outcome is reduced workload
and increased precision. Memory indexing is complemen-
tary in that it provides a robust way of comparing program
state across executions and hence helps identify equivalenttrace segments.
There has been recent work on comparing executions for
debugging regression faults [10], analyzing impact of codechanges [17], and ﬁnding matching statements across pro-
gram versions [7]. These techniques are able to construct
a symbolic mapping of variables across program versions
through proﬁling, such as pointer xin version one being
renamed to yin version two. The constructed mapping is
static. In comparison, we focus on comparing dynamic (ad-
dress)valuesofcorrespondingvariables, answeringquestions
like “does xpoint to the corresponding address in the two
executions”. Furthermore, trace canonicalization facilitated
by MI would improve the precision of these analyses.
225Table 2: Cause Transition Computation for Failures.
Program Bug ID Bug Passing Sym. Diﬀs MI Diﬀs Aliases Issue Trans/Diﬀs Time (s)
gcc 2.95.2 529-Wshadow warns on functions predicate switch 8365 233 8105 >12h 8/1 4559
gcc 2.95.2 776Large array size causes abort predicate switch 10101 230 9027 >12h 2/1 379
gcc 2.95.2 2771-O1 breaks strength-reduce provided input 11095 28410254 >12h 4/1 1797
make 3.81 16958 .PHONY targets are unrecognized non-regressing 2699 184 33 >12h 9/1 740
make 3.81 18435 Parentheses break make targets provided input 3301 336 356 >12h 29/2 645
make 3.81 19133 ./ prevents self remake provided input 3728 550 187 >12h 5/2 235
make 3.80 112Rules cannot handle colons provided input 3309 645 217 >12h 11/6 685
gawk 3.1.5 1/20/06 Deallocate bad pointer provided input 630 22 509early term. 8/1 56
Many debugging techniques [14, 16, 3, 13, 4, 11] compute
fault candidates by looking at a large number of executions,
both passing and failing. In these techniques, execution pro-
ﬁlesarecollectedandanalyzedstatistically. Somedebuggingtechniques compare a simple proﬁle of a failure with a small
number of correct runs (usually one) [9]. They use control
ﬂow paths and code coverage. MI is complementary to these
techniques by providing a way to canonicalize proﬁles before
they are analyzed to achieve better precision, especially for
pointer related bugs. We have demonstrated in this paperthat MI is able to drive cause transition computation thatis highly sensitive to memory alignment.
Compared to the recent advances on generating causal
explanations of failures [13, 4], The proposed robust, ﬁne-
grainedmemorydiﬀerencingandsubstitutionprimitivesmake
itfeasibletoextractsuccinctandin-depthinformationabout
failures. For instance, it is relatively easier for us to reasonabout whether a value at a given execution point is rele-
vant to a failure. Furthermore, MI improves cause transition
computation [5, 19] by allowing alignment along the mem-ory dimension, which substantially improves robustness in
the presence of aliasing.
Abstractions for memory regions used in static analysis
also have the notion that small, bounded chains of the con-
trol dependence of a region’s creation provide a notion of
identity for that region[18]. In contrast, we extend this into
the dynamic domain, eﬃciently capturing precise identitiesonline instead of just over-approximations.
In [15], execution indices are used to locate locks across
executions of Java programs, but the approach is not as
generalized or optimized as memory indexing.
9. CONCLUSIONS
We present a novel challenge in dynamic program analy-
sis: aligning memory locations across executions. We pro-
poseasolutioncalledmemoryindexing(MI),whichprovides
a canonical representation for memory addresses such thatmemory locations across runs can be aligned by their in-dices. Pointer values can be compared across runs by their
indices. The index of a memory region is derived from the
canonical control ﬂow representation of its dynamic alloca-tion site such that control ﬂow correspondence is projected
to memory correspondence. Enabled by MI, we also pro-
pose a novel memory substitution primitive that allows ro-bustly copying states across runs. We evaluate the eﬃciency
of two memory indexing semantics. Our results show that
the technique has 41% runtime overhead and 213% space
overhead on average. We evaluate eﬀectiveness through two
client studies: one is trace canonicalization and the other is
cause transition computation on failures. The studies showthat MI reduces address trace diﬀerences by 15-60%. It alsoscales cause transition computation to programs with com-
plex heap structures.
10. ACKNOWLEDGEMENTS
We would like to thank the anonymous reviewers for their
insightful comments. This research is supported in part
by the National Science Foundation (NSF) under grants
0917007 and 0845870. Any opinions, ﬁndings, and conclu-sions or recommendations in this paper are those of the au-thors and do not necessarily reﬂect the views of NSF.
11. REFERENCES[1] H. Boehm. Space eﬃcient conservative garbage collection. In
PLDI’93 .
[2] C. Bron and J. Kerbosch. Algorithm 457: ﬁnding all cliques of
an undirected graph. CACM, 16(9), 1973.
[3] Y. Brun and M. Ernst. Finding latent code errors via machine
learning over program executions. In ICSE’04 .
[4] T. Chilimbi, B. Liblit, K. Mehra, A. Nori, and K. Vaswani.
Holmes: Eﬀective statistical debugging via eﬃcient path
proﬁling. ICSE’09 .
[5] H. Cleve and A. Zeller. Locating causes of program failures. In
ICSE’05 .
[6] M. Diep, S. Elbaum, and M. Dwyer. Trace normalization. In
ISSRE’08 .
[7] M. Feng and R. Gupta. Detecting virus mutations via dynamic
matching. In ICSM’09 .
[8] J. Ferrante, K. Ottenstein, and J. Warren. The program
dependence graph and its use in optimization. ACM Trans.
Program. Lang. Syst. , 9(3):319–349, 1987.
[9] L. Guo, A. Roychoudhury, and T. Wang. Accurately choosing
execution runs for software fault localization. In CC’06.
[10] K. Hoﬀman, P. Eugster, and S. Jagannathan. Semantics-aware
trace analysis. In PLDI’09 .
[11] H.Y. Hsu, J. Jones, and A. Orso. Rapid: Identifying bug
signatures to support debugging activities. In ASE’08 .
[12] D. Jeﬀrey, N. Gupta, and R. Gupta. Fault localization using
value replacement. In ISSTA’08 .
[13] L. Jiang and Z. Su. Context-aware statistical debugging: from
bug predictors to faulty control ﬂow paths. In ASE ’07 .
[14] J. Jones and M.J. Harrold. Empirical evaluation of the
tarantula automatic fault-localization technique. In ASE’05 .
[15] P. Joshi, C. Park, K. Sen, and M. Naik. A randomized dynamic
program analysis technique for detecting real deadlocks. InPLDI’09 .
[16] B. Liblit, A. Aiken, A. Zheng, and M. Jordan. Bug isolation via
remote program sampling. In PLDI’03 .
[17] M.K. Ramanathan, A. Grama, and S. Jagannathan. Sieve: A
tool for automatically detecting variations across program
versions. In ASE’06 .
[18] O. Shivers. Control-Flow Analysis of Higher-Order
Languages . Ph.d. thesis, Carnegie Mellon University, 1991.
[19] W.N. Sumner and X. Zhang. Algorithms for automatically
computing the causal paths of failures. In FASE, 2009.
[20] W.N. Sumner and X. Zhang. Memory indexing and its use in
automated debugging. Technical report, Purdue University,2010.
[21] B. Xin, N. Sumner, and X. Zhang. Eﬃcient program execution
indexing. In PLDI’08 .
[22] A. Zeller. Isolating cause-eﬀect chains from computer programs.
InFSE’02 .
[23] X. Zhang, N. Gupta, and R. Gupta. Locating faults through
automated predicate switching. In ICSE’06 .
226