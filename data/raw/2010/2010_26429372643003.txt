Evaluation of String Constraint Solvers in the Context of
Symbolic Execution
Scott Kausler
Boise State University
Department of Computer Science
Boise ID, 83725
scottkausler@u.boisestate.eduElena Sherman
Boise State University
Department of Computer Science
Boise ID, 83725
elenasherman@boisestate.edu
ABSTRACT
Symbolic execution tools query constraint solvers for tasks
such as determining the feasibility of program paths. There-
fore, the eectiveness of such tools depends on their con-
straint solvers.
Most modern constraint solvers for primitive types are ef-
cient and accurate. However, research on constraint solvers
for complex types, such as strings, is less converged.
In this paper, we introduce two new solver adequacy cri-
teria, modeling cost and accuracy, to help the user identify
an adequate solver. Using these metrics and performance
criterion, we evaluate four distinct string constraint solvers
in the context of symbolic execution. Our results show that,
depending on the needs of the user and composition of the
program, one solver might be more appropriate than an-
other. Yet, none of the solvers exhibit the best results for all
programs. Hence, if resources permit, the user will benet
the most from executing all solvers in parallel and enabling
communication between solvers.
Categories and Subject Descriptors
D.2.5 [ Software Engineering ]: Testing and Debugging|
Symbolic execution
Keywords
Constraint Solver Analysis; String Constraint Solving; Sym-
bolic Execution
1. INTRODUCTION
Symbolic execution (SE) [20, 8] is a path sensitive pro-
gram analysis that traverses program paths using depth-
rst search and interprets program execution on symbolic
input values instead of concrete values. When interpret-
ing a program, SE generates a set of constraints on the
program's symbolic inputs. The conjunction of those con-
straints, which is called a path condition (PC), is used by
SE to avoid traversals of infeasible paths. If the PC is un-
satisable, then the path is infeasible and feasible otherwise.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proï¬t or commercial advantage and that copies bear this notice and the full cita-
tion on the ï¬rst page. Copyrights for components of this work owned by others than
ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or re-
publish, to post on servers or to redistribute to lists, requires prior speciï¬c permission
and/or a fee. Request permissions from permissions@acm.org.
ASEâ€™14, September 15-19, 2014, Vasteras, Sweden.
Copyright 2014 ACM 978-1-4503-3013-8/14/09 ...$15.00.
http://dx.doi.org/10.1145/2642937.2643003.Commonly, SE tools outsource PC evaluations to constraint
solvers, making solvers one of SE's crucial components. In
fact, through advances in constraint solvers, SE gained its
wide-spread applicability in software verication, e.g., test-
case generation [29] and property verication [9].
SE tools heavily rely on the availability of adequate solvers,
i.e., solvers that can reason about constraints of the required
data types in a way that is best suited for a particular appli-
cation of SE. With so many available and emerging solvers,
the users of SE tools are faced with the problem of identify-
ing the adequate one, especially if the solvers were originally
developed for dierent problems.
For example, while analyzing available string constraint
solvers to use in SE of Java programs, a user might no-
tice that some of them are used in a dierent analysis but
for the same programming language, e.g., JSA [7], in the
same analysis but for a dierent language, e.g., Z3-str [38],
in a dierent analysis and for a dierent language, e.g.,
STRANGER [34], and even in dierent areas such as model-
driven software engineering, e.g., ECLiPSe-str [4, 5]. Cur-
rently, the only information available for identifying an ade-
quate solver is solvers' performances. In this paper, we show
that, in addition to performance , solvers vary in two factors
that are important to a user: modeling cost and the accuracy
with which a solver evaluates PCs. We dene these two new
adequacy criteria and use them with performance to eval-
uate four diverse string constraint solvers in the context of
SE, i.e., by traversing program paths.
Modeling cost is associated with the observation that
string solvers lack in one-to-one support for string operations
and predicates, i.e., string methods, of the target program-
ming language. If a solver was originally designed to process
string constraints in another language, e.g., PHP instead of
Java, then it would not have interfaces to support some Java
string methods. Or, if the solver developers chose to imple-
ment a set of basic operations for the underlying symbolic
string representation then the user must use the interface
for the basic operations to implement more complex string
methods. For example, a solver that uses automata to rep-
resent symbolic strings might implement basic regular lan-
guage operations, e.g., concatenation, union, etc. In either
of those situations, the user must model the string meth-
ods of its target language by using the interface provided by
the solver, i.e., by extending the solver. We found that the
modeling cost varies substantially among the solvers.
In addition, we learned that some of Java's string meth-
ods inherently cannot be modeled precisely by solvers. The
level of precision varies from the complete inability to model
259
a string method to an approximation of the method with
some other method, e.g., the startsWith (\a") predicate can
be over-approximated with the contains(\a") predicate. In
some cases such approximations are implicit, i.e., obscured
from users. Moreover, the user might unknowingly ex-
tend a solver so that the modeled method exhibits an over-
approximating behavior.
All of the above scenarios of imprecise modeling may re-
sult in the loss of accuracy in the result obtained from
the extended solver, e.g., a SAT result of a PC where
startWith is substituted with contains should be inter-
preted as SAT over, i.e., an over-approximating SAT. This
silent over-approximating behavior might cause the user's
application to behave dierently, e.g., it might produce false
positives, which are interpreted as actual property viola-
tions. Therefore, the user should be able to detect when a
solver returns approximating results. In this paper, we in-
troducea a set of accuracy measurements that empirically
provide insights into the solvers' relative accuracies, i.e.,
without an oracle.
In this work, we extended four publicly available string
constraint solvers: JSA, STRANGER, Z3-str and ECLiPSe-
str, and evaluated them on 174 string PCs dynamically col-
lected from eight open-source string processing applications.
We use our proposed metrics to assess the performance,
modeling cost and accuracy of those solvers. We found that
in the context of our experiments STRANGER achieves the
highest ranking, followed respectively by JSA, Z3-str, and
ECLiPSe-str.
However, none of the solvers outperformed all others
in all three categories. For example, we found that the
automata-based string constraint solvers, which are JSA
and STRANGER and their extensions, perform well and
are capable of representing more string methods than the
other solvers, but there are other string methods that the
automata-based solvers could not model as precisely. Thus,
Z3-str has the highest ranking in modeling cost, but the
lowest one in performance. Moreover, solvers received dif-
ferent rankings for each analyzed program, which suggests
that solvers should be selected based on the program pro-
le. This prole includes the number and the type of string
methods used in the program and the ability of a solver to
precisely model them. Overall, our empirical studies demon-
strate that each of the solvers ranks higher than others in
some evaluation criterion for some program. This nding
suggests that if resources permit then the execution of mul-
tiple solvers with diverse algorithmic techniques is the best
alternative.
Even though the set of string solvers and string PCs that
we analyzed is by no means exhaustive, we perceive the in-
troduction of new decisive factors in selecting a constraint
solver for complex symbolic types and insights on varied re-
sults of the solvers as the main contributions of this paper.
As SE extends to more advanced data types, such as sym-
bolic collections or symbolic heaps, the chances of precisely
modeling their methods decreases. Therefore, one can ex-
pect the same type of questions about accuracy in the evalu-
ation of complex data type PCs. In particular, users should
think about how their implementation should deal with ap-
proximating behavior of solvers for complex symbolic types.
The rest of our paper is organized as follows. In the next
section, we explain how string constraint solvers work and
discuss related work on those solvers. Then, we give a mo-tivating example. In Section 4, we present two new criteria
for string solver evaluation. After that, we present our eval-
uation approach and examine the results.
2. BACKGROUND AND RELATED WORK
To illustrate how SE generates string constraints, consider
the code snippet in Figure 1. On the entrance to method
m(String s1, String s2) , SE assigns symbolic values S1
andS2 to the rst and the second arguments, respectively.
When SE reads in the assignment statement with the sub-
string string operation, SE updates the value of the s1 vari-
able to S1:substring (2), which means that the new symbolic
value for s1 contains all strings that start at index 2 of the
original symbolic string S1 and extend to its end. To explore
the true branch of the conditional statement, SE generates
the following constraint:
(S1:substring (2)):equals (S2) (1)
This constraint restricts the possible values of s1 to those
whose substrings starting at index 2 are the same as the val-
ues of s2. To determine whether such values for S1 and S2
exist, i.e., whether the constraint is satisable, SE queries a
string constraint solver. However, there are several solvers
to choose from; we have found about 13 constraint solvers
in literature. While investigating these solvers, we applied
several selection criteria besides the necessary one { public
availability at the time of our experiments. Our rst cri-
terion was the ability of a solver to model a majority of
methods in java.lang.String ,java.lang.StringBuffer ,
and java.lang.StringBuilder classes. At a minimum, it
must handle concatenation and substring operations in ad-
dition to the equality predicate. The second criterion was
eciency { we only selected those solvers that were able to
evaluate the majority our PCs within ve seconds.
For the past decade, researchers developed a variety of
automata-based solvers targeted for a particular type of pro-
gram analysis. The programming language research group
at Aarhus University pioneered this research area by devel-
oping the Java String Analyzer tool (JSA) [7], which per-
forms data-ow analysis on string expressions in Java pro-
grams with automata as the abstract domain of string vari-
ables. JSA relies on the nite state automata package [24]
developed by the same group to implement string transfer
functions and merge operations. The automata implementa-
tion uses the Unicode alphabet for state transitions. Several
tools have used JSA, the automata package, or both to im-
plement their analyses by extending JSA for modeling string
operations in their target language.1For example, an ex-
perimental SE tool Haderach [30] extends JSA to represent
symbolic strings instead of an array of characters, as other
SE tools that don't support symbolic strings as a separate
type do. While the authors of Haderach omit the discussion
of precision or modeling cost, the developers of Java String
Testing (JST) [12], an automated test case generation tool
based on SE that also extends JSA's automata, do mention
that they rened and added models of string operations be-
cause some are not implemented and others lack in precision.
At the time of our studies, JST was not publicly available.
An alternative implementation of an automata-based
string solver was used in STRANGER [36, 37, 34, 35], a
1Since it is common to refer to the automata package used
in JSA as JSA, we will do the same here.
260m( String s1 , String s2 ) f
s1 = s1 . substr ing ( 2 ) ;
i f( s1 . equals ( s2 ) ) f
. . .
g
g
Figure 1: Code snippet example.S1 2
substring(int)targetargS2
equals(Object)arg
target
Figure 2: Data-ow graph.Solversubstring equals Time
LOC Precise LOC Precise (s)
EJSA2 no8 no0.0001
EJSA 25 maybe 0.0001
ESTRANGER 2 maybe 9 no 0.0041
EZ3-str 9 maybe 2 maybe 0.0217
EECLiPSe-str 3 no 3 maybe 0.0014
Table 1: Variations in modeling cost, accuracy and performance.
data-ow analysis for PHP programs where, as in JSA, au-
tomata are used to represent possible values of string ex-
pressions. STRANGER uses the MONA [2] automata pack-
age, where transitions between states are represented as a
multi-terminal binary decision diagram. Even though the
authors of STRANGER do mention JSA in their related
work section, they fail to compare these two distinct imple-
mentations of automata. For brevity, from this point on we
use STRANGER to refer to the constraint solver used in
this data-ow analysis. Both JSA and STRANGER satis-
ed our selection criteria, and we extend them for use in the
context of SE of Java programs.
Several other decision procedures, such as DPRLE [15], its
unnamed successor [16], and Rex [33], use dierent custom
automata implementations. Since those solvers focus on a
decidable fragment of string theory, they support a limited
number of string methods and thus do not satisfy our rst
selection criterion.
An alternative representation of a symbolic string is a -
nite linked structure, which imposes a bounded length on
symbolic values. For example, HAMPI [19] translates a
string into an ordered list of bits, i.e., bit-vectors, and uses
the operations available in bit-vector theory to express string
constraints. The same approach to symbolic string rep-
resentation is implemented in the string constraint solver
Kaluza, which extends HAMPI but is used in the SE based
test generation framework Kudzu [28]. A solver introduced
by Bjrner et al. [3] also represents symbolic strings as bit-
vectors. On the other hand, a bounded string solver [4, 5]
based on the ECLiPSe framework and used in model driven
engineering represents a symbolic string as a list of con-
crete characters and symbolic variables. This solver uses
constraint logic programming and constraint handling rules
to express string constraints. In this paper, we refer to
the aforementioned solver as ECLiPSe-str. PASS [22] is
a string constraint solver that uses a parameterized array
based model to represent symbolic strings. In PASS, string
constraints are expressed as quantied formulas, for which
the authors developed a custom quantier elimination al-
gorithm used in SMT solving. All of these bounded string
solvers are used in dierent types of analyses and for dier-
ent programming/modeling languages. Among these only
ECLiPSe-str satised our selection criteria. Unfortunately,
HAMPI timed out on all queries even on small string lengths
and PASS was not publicly available at that time.
To circumvent the length limitation imposed by tradi-
tional bit-vector solvers, Z3-str [38] employs SMT solving
to model symbolic strings of arbitrary length. Z3-str ex-
tends the Z3 SMT solver [10]. It denes the string sort in
Z3 and uses the theory of interpreted functions and axioms
to express string operations and predicates. This string con-
straint solver has satised all our selection criteria and was
used in our study.The work on comparison of dierent solvers is limited
to mainly performance-based evaluations. Hooimeijer et
al. [16] compared their unnamed string constraint solver to
DPRLE and HAMPI on a set of regular expressions and lim-
ited operations on string sets. Another paper by Hooimeijer
et al. [14] evaluated the performance of dierent automata-
based constraint solvers on basic automata operations. Re-
delinghuys et al. [27] compared the performance of their
custom implementations of bit-vector constraint solvers and
their custom extension of JSA in the context of symbolic
Java PathFinder [26]. The authors of axiom-based string
solver Z3-str [38] compared their tool with bit-vector based
Kaluza [28]. The comparison is done both in terms of perfor-
mance and correctness, although the authors do not dene
the latter. Choi et al. make a comparison with JSA based
on performance and precision in [6]. In this case, the authors
use the generality of regular expressions to determine which
solver is more precise. Finally, the authors of [22] compared
PASS with an automata approach and an approach similar
to a bit-vector one on performance and the number of solved
cases.
To summarize, for evaluation PCs of Java programs in the
context of SE we have selected the following four solvers:
JSA, STRANGER, ECLiPSe-str and Z3-str. We extended
each of these solvers and nameed to our extensions EJSA,
ESTRANGER, EZ3-str, and EECLiPSe-str respectively.
Since we are impartial to the string solvers, we used our
best eort to extend them, including communications with
their developers. However, the inapplicability of other string
solvers to our problem indicates their focus on special sets
of problems. Although each solver has contributed to note-
worthy research, the specialization of these solvers makes it
dicult to apply them to other problems.
3. MOTIVATING EXAMPLE
To demonstrate diversity in modeling cost, accuracy and
performance of the four string solvers, consider Table 1,
which shows how each of the extended solvers evaluates in
each category for the constraint in Formula 1. The rst
column contains dierent extensions of string solvers. Col-
umn two shows the number of lines of codes that it took
for us to extend the solver to model the substring(int)
method, while column three tells whether the model might
be precise or not. Columns four and ve provide a similar
description for the equals(Object) method. The last col-
umn contains the average time in seconds that the string
solvers required to process the two methods. The average is
calculated after querying the solvers ten times. We label the
precision of method implementation with \maybe" or \no".
Forequals(Object) , we considered precision of models of
both the predicate and its negation. When we cannot de-
termine that the model of a method is certainly imprecise,
261we label it with \maybe", but when we are certain that the
model is not precise, we label it with \no".
In this example, we have two versions of JSA extensions
for the substring(int) method. The rst, EJSA, uses
the built-in method, which only requires a couple of lines
of code to invoke. JSA's native modeling of this method
over-approximates the result by allowing the resulting au-
tomaton to represent any postx of the original automaton.
For example, if S1 in Figure 1 is the concrete string \foo",
the symbolic string after the native modeling would repre-
sent concrete strings \foo", \oo", \o", and \", which is an
over-approximation of the correct substring \o". In order to
improve accuracy, we reimplemented the substring meth-
ods in EJSA using the algorithm developed for JST string
solver [12].
Since we cannot obtain the proof of correctness of this
algorithm, we mark it as \maybe" precise. Note that it re-
quired a substantial amount of eort to implement a more
accurate model of the substring methods, and modeling
cost involves the lines of code in addition to other eorts,
such as understanding the theory of automata.
Unlike JSA, STRANGER already provides an interface
forsubstring with no obvious approximations. Therefore,
to achieve the same level of accuracy, we spend less eort
to model this string operation. However, neither automata-
based solver could model the equals predicate and its nega-
tion without introducing over-approximation.
For EZ3-str, we found no obvious approximation for ei-
ther of the two methods. Z3-str comes with a direct inter-
face for the substring operation, equals predicate, and a
negation quantier. Therefore, the substring operation in
Figure 1 can be modeled using Z3-str's built in operation in-
terface along with Z3's symbolic integer type. Furthermore,
theequals predicate is modeled using Z3-str's equals inter-
face, and the predicate's negation is modeled by applying
the negation operator to the original predicate.
In EECLiPSe-str, we were unable to model the sub-
string(int) method without introducing clear over-
approximations. ECLiPSe-str cannot model it precisely be-
cause its substring method must return a string of at least
length one. Since the empty string is a feasible result, a
sound model must over-approximate the method by disjoin-
ing the result with the empty string.
The \Time" column clearly demonstrates the variations
in the performance of all string solver extensions. EZ3-str
takes the most time to process the constraint but at the
same time models the string methods with better precision.
EJSA denitely displays the best performance while main-
taining the same precision as ESTRANGER. EECLiPSe-str
comes in second in the performance category, with a level of
precision that is incomparable to that of the automata-based
solvers.
This example illustrates the coupling between modeling
cost and precision, i.e., a higher modeling cost results in a
higher precision, which results in a more accurate solver, and
the coupling between precision and performance, i.e., the
most precise solver took the longest time to execute. Also,
the example shows that solvers with the same level of pre-
cision do not necessarily exhibit the same performance, i.e.,
EJSA and ESTRANGER have similar precision but the for-
mer outperforms the latter. Moreover, there are situations
when the solvers' precisions are not comparable. Hence, the
performance cannot be judged adequately in such circum-stances. In this example, we illustrate the dierences and
by no means make conclusions on these four solvers. In the
following section, we describe in detail what metrics we used
to perform the comparisons of the four string solvers.
4. CRITERIA FOR SOLVER EVALUATION
As we previously discussed, the majority of the compar-
isons between constraint solvers is based on performance.
This factor is important for constraint solver developers,
since solver competitions [31] mainly focus on performance.
Even though performance is important for users, other fac-
tors can also play critical roles. We speculate that two ad-
ditional factors, modeling cost and accuracy, should be de-
cisive in selecting an adequate solver. We explain why they
are important and how they can be measured below.
4.1 Modeling Cost
We dene modeling as the adaptation of a constraint
solver to the context of the user's problem. Essentially, mod-
eling is the translation of the language of the problem to the
language of the solver. Since dierent solvers have been de-
veloped to solve specic problems, we expect that the eort
required by a user to model a dierent problem, i.e., mod-
eling cost, should vary by solver.
In order to model a problem, the user studies the solver's
interface. Sometimes there is a direct match between a
string method and the solver's interface, e.g., for the equals
method and Z3-str's interface as shown in Table 1. In other
cases, the user must invoke several calls to the solver's inter-
face to model the method, e.g., modeling the equals method
by EJSA as exemplied in Table 1. More complicated
extensions also require an understanding of relevant data-
structures. For example, to extend EJSA the user should
be familiar with automata theory. In addition, even when a
solver claims to support a particular string method, it might
happen that the method is not supported adequately in the
context of the problem. For example, Z3-str supports a re-
place method, but its support is limited and non-applicable
in the context of SE of Java programs.
Obviously, more eort results in higher precision. Thus,
our extra eort in implementing the substring operation
in EJSA resulted in a more precise model of the operation
compared to JSA's native one. Lack in eort might result
in both poor modeling and inappropriate use of the solver.
In our study, we used the lines of codes required to extend
a solver to model a method as the measure of modeling cost.
A more objective measure would be the amount of time we
invested to extend each solver, but we believe lines of code
adequately describes our eort.
4.2 Accuracy
When a solver can precisely model all string methods,
then such solver is both sound and complete, i.e., it never
reports that a satisable constraint is unsatisable and that
an unsatisable constraint is satisable. If a solver is sound
but incomplete, we say that it over-approximates, i.e., it
might return SAT for UNSAT, and if a solver is complete but
unsound, then we say it under-approximates, i.e., it might
return UNSAT for SAT.
The source of imprecise modeling can be internal to the
solver, e.g., as in EJSA's modeling of substring . It can
also be the result of the solver's inability to precisely model
a string method, as we see in EECLiPSe-str's model of sub-
262string(int) . We have also encountered an inability to rep-
resent the full set of characters used in Java. Thus, Z3-str
has limited support of ASCII characters, so we had to map
unsupported Java characters to those that Z3-str can sup-
port. Also, STRANGER cannot support at least two UTF-
16 characters, one with the decimal value of 0, i.e., a null
character, and another with the decimal value of 56319.
Since neither the developers of the solvers prove that their
implementations are precise, nor we prove the precision of
our extensions, we evaluate the accuracy of the solvers em-
pirically. Ideally, we would do this by comparing the solver's
set of solutions for a constraint to the accurate set of solu-
tions. However, since the solution oracle cannot be obtained,
we propose three conservative measurements that can be
used to conjecture about solvers' accuracy in the context
of SE. We have yet to reason about the accuracy of opera-
tions, so these measurements are based on the accuracy of
predicates.
In our measurements of accuracy, we must assume that
our extended solvers are sound. This assumption is justi-
ed by two reasons. First, the authors of string constraint
solvers often claim their solvers are sound without providing
a formal proof. Second, we empirically check for unsound
results in our analysis using concrete valued gathered in our
analysis.
Measure 1: Disjoint Branching Points
Conceptually, when a branching point is encountered, op-
posing predicates should partition the set of values repre-
sented by a symbolic value occurring before the branching
point, as each value may only follow one branch in concrete
execution. This is shown in Figure 3a, where the \Original"
set represents the values before a branching point and the
\True"and\False"sets disjointly represent the values in each
branch. The rst measure of accuracy deals with whether a
solver can partition this domain of symbolic strings. When
a solver can partition the domain, we refer to it a disjoint
branching point. Otherwise, we call it a non-disjoint branch-
ing point, as is shown in Figure 3b. In Figure 3b, the grey
portion of the \True" set represents over-approximated val-
ues present in both sets. We say a solver  1is more accurate
than a solver  2if the set of disjoint branching points pro-
duced by  1is a proper superset of  2's disjoint branching
point set, i.e.,  2disj1.
A disjoint branching point does not imply the symbolic
values involved are free from over-approximation. Instead,
they indicate the current branch has been modeled accu-
rately by the solver.
Measure 2: Unsatisable Branching Points
The second measure of accuracy is a special case of disjoint
branching points based on the number of unsatisable con-
straints that a solver can detect. If one solver  2evaluates
a constraint as SAT and another  1as UNSAT, then we
say that the latter is more accurate than the former, i.e.,
2unsat 1.
We say this because we know an UNSAT result comes
from an actual unsatisable PC, since our solvers are sound.
The ability of a solver to detect unsatisable PCs is crucial
for SE, since it prevents SE from exploring infeasible paths.
We refer to branching points where a solver can detect that
one of its outcomes is unsatisable as unsatisable. One such
branching point is shown in Figure 3c, where the \True" set
is the empty set and the \False" set equals the \Original" set.Original
False True
(a)Disjoint.Original
False True
(b)Not disjoint.
Original
False True
(c)Unsatisable.ffoogOriginal
ffoogFalse True
(d)Singleton unsatisable.
Figure 3: Conceptually illustrates measures of accuracy.
Measure 3: Non-singleton Unsatisable Branching Points
In order to assess the complexity of the PCs for unsatis-
able branching points, we check if all symbolic strings in-
volved in a branching point represent only a single concrete
string value. If this is not the case, then evaluation of the
constraint is non-trivial and branching point is marked as
anon-singleton branching point and singleton otherwise.
A singleton unsatisable branching point is shown in Fig-
ure 3d, where the \True" set again represents the empty set
and the \False" and \Original" sets both contain only the
string foo. If a solver  1can detect an unsatisable PC at a
non-singleton branching point, then  1has higher accuracy
than a solver  2that can detect an unsatisable PC at a
singleton branching point, i.e.,  2unsat 1.
In the next section, we describe how we conducted our
study to evaluate the four string solvers using metrics intro-
duced in this section.
5. EVALUATION
We evaluate the string constraint solvers on a set of
benchmarks comprised of actual string PCs obtained from
real-world Java programs. We limited our analysis to
one language and leave evaluation of multiple languages
to future work. Instead of employing a SE tool such as
SPF [26], we collect PCs by means of dynamic symbolic ex-
ecution(DSE) [29]. DSE combines concrete execution with
SE, i.e., it performs SE along a single concrete execution
path. By using DSE instead of SE, we can record PCs of
long execution paths with less eort for diverse set of pro-
grams, e.g., no need to model programs' libraries. Moreover,
additional information gathered during a DSE run, such as
concrete values of string variables at certain program points
and actual branch outcomes, assisted us with the measure-
ment of solvers' accuracies.
5.1 Collecting PCs
We implemented our custom DSE using a program in-
strumentation technique built on top of the instrumentation
features of the Java optimization framework Soot [32]. We
opted to create our own DSE over using an existing tool for
more complete capturing of strings data-ows. Our collec-
tion tool instruments the subject program with additional
method calls that enable inter-procedural collection of string
PCs. In particular, the instrumentation monitors the targets
and arguments of string methods. When an argument of a
method is not a string type, we use the argument's concrete
value for that program execution run. We exclude libraries
263Name (Abr.) Cl. Description Tr. Op. Pred. TOs. %Cov.
Jericho HTML Parser (JHP) 119 Library for HTML parsing. 25 232.1 31.8 1.7 30/16
jXML2SQL (JXM) 23 Converts XML les to SQL or
HTML.24 150.4 56.3 0 49/42
MathParser Java (MPJ) 48 Solves inputted math expres-
sions.20 279.2 377.3 9.2 87/72
MathQuiz Game (MQG) 1 GUI based math study tool. 28 103.6 68 0 98/93
Natural CLI (NCL) 49 Allows command line input
based on a natural language.19 11.4 158 77.9 42/52
Beasties (BEA) 6 Command line combat game. 30 466.4 110.1 0 98/99
HtmlCleaner (HCL) 57 Converts dirty HTML to well-
formed XML.16 2509.8 429.2 25.1 46/30
iText (ITE) 573 A Java PDF Library. 12 22707.8 656.1 0 4/2
Table 2: Program artifacts and constraint descriptions.
from monitoring and instead, we assign a new symbolic value
to each string value returned from a library.
Instead of using constraint solvers during the DSE run,
we record PCs in a le then parse the le to instantiate
each PC and pass it to solvers. During constraint collection,
we represent each PC in a data-ow graph similar to one
presented in Figure 2 for the code snippet on its left.
In each data-ow graph, the source vertices represent ei-
ther symbolic string values or concrete values of some types
used in a program execution. Thus, in Figure 2 the sources
represent symbolic values S1 and S2 of string variables s1
ands2 respectively, as well as a concrete integer value 2
that is the argument in the substring(int) method. Sink
vertices represent either methods that take a string as an
argument, e.g., a print statement, or string predicates. In
Figure 2, equals(Object) is one such sink vertex. The
rest of the vertices represent string operations, i.e., meth-
ods that return a string, or modify the calling string, as
is the case with the mutable java.lang.StringBuilder or
java.lang.StringBuffer classes. In our example, this is
the vertex substring(int) . Edges represent the ow of
data, i.e., they represent how string values are used by meth-
ods. The edges are labeled either\target", i.e., the target ob-
ject of the method, or\arg", i.e., an argument to the method.
When a method takes more than one argument, we record
additional meta-data for the proper argument order. In Fig-
ure 2, the substring(int) vertex has two incoming edges:
one from S1 labeled \target" and one from 2 labeled \arg".
Thus, a substring(int) operation was performed on string
symbolic value S1 with an argument that takes the concrete
value 2.
To mimic the construction of a PC by SE, we record the
time stamp for each node. Hence, traversing the graph ac-
cording to the time stamp reconstructs the original structure
of the constraint. In addition, we perform a set of simplica-
tions to our graph, e.g., by removing nodes that do not con-
tribute to a constraint. At the end of the collecting phase,
the resulting graph is recorded in a le.
5.2 Processing PCs
The processing of a PC starts by reading the graph le and
instantiating the graphical representation of the PC. Then,
the graph is traversed in a chronological order, i.e., by visit-
ing vertices with the oldest time stamps rst. When a pred-
icate method's vertex is encountered, e.g., equals(Object) ,
the new constraint is generated and added to the PC. Then,
the extended PC is translated into the language of each
solver, and we query the solver for satisability. To accom-plish this task, we used Java interfaces for the two automata-
based solvers, a le in a special input format for Z3-str,
and a custom query for ECLiPSe-str. As the PC is aug-
mented by additional predicates, EZ3-str and EECLiPSe-str
re-evaluate the whole PC, while the automata-based solvers
incrementally evaluate it. In addition we apply all length
constraints in all solver extensions. For bit-vector based
EECLiPSe-str we set bounds of 10K on its string lengths.
We used a timeout of ve seconds in our experiments,
although EJSA and ESTRANGER never timed out. If a
solver was not able to decide a constraint within the allotted
time frame, then we safely over-approximate the constraint
by assuming that the PC is satisable and setting symbolic
values to the set of all strings. In addition, when a solver
cannot model a string operation, e.g., toLowerCase() , the
result of the operation is also safely over-approximated with
a symbolic value representing the set of all strings.
As we mentioned, we put our best eort to create an un-
biased extension of each solver [17]. Furthermore, we aimed
to model each string method uniformly across the solvers.
5.3 Acquiring Data for Solversâ€™ Evaluation
5.3.1 Modeling Cost
For modeling cost, we report the lines of code that we
used to extend each solver for each method. For each string
predicate, we report the total lines of code for implementing
the predicate and its negation.
5.3.2 Performance
We measure performance by timing the evaluations of two
PCs for each branching point, i.e., one PC for the taken
branch and another for the not taken branch. We chose to
record the time for both PCs because both PCs are used in
our measures of accuracy, e.g., to detect disjoint branching
points. To make a fair comparison of performance between
incremental and non-incremental solvers, we kept a total
running time for each branching point and added the time
required to process each additional constraint for the incre-
mental solvers.
We also attempted to avoid including startup time in our
performance evaluation. For EJSA, we ran the underlying
solver with several string operations before evaluation to
avoid loading times. Z3-str is run in a standalone program,
so we opted to use its self-reported time, which doesn't in-
clude startup time. This might allow EZ3-str to underre-
port its time, as the tool is likely bias in its estimations,
but this does not appear to aect any conclusions on per-
264formance. STRANGER and ECLiPSe-str appear to be un-
aected by startup time, as STRANGER is implemented
in C and passes messages via JNA2while ECLiPSe-str is
initiated in its own thread.
At the end of processing, we report the performance of
each solver as the average of each branching point's perfor-
mance. When a solver times out on a taken branch, we use
the timeout value as the solver's processing time.
5.3.3 Accuracy
We evaluate extra constraints, in addition to PCs gath-
ered in DSE, and add tracking mechanisms to record the
measures of accuracy proposed in the previous section. To
detect disjoint branching points, we conjunct the PC of one
outcome of the branching point with the PC of another out-
come of the branching point and send this constraint to the
solver. If the solver returns UNSAT, then we mark the
branching point as disjoint. In the case of the automata-
based solvers, the disjoint branching point is identied when
the intersection of automata of opposite outcomes results in
the automaton that recognizes the empty language ;. At
the end of PC processing we report all disjoint branching
points per solver.
To detect unsatisable branching points, we generate the
PC with the negated predicate, i.e., for the not taken branch
outcome in concrete execution, for each predicate node in the
graph. If the constraint solver returns UNSAT, i.e., the PC
evaluates to false, then we mark that branching point as un-
satisable. In the end, we report the unsatisable branching
points for each solver.
In order to determine singleton branching points, i.e., ones
whose PCs contain only concrete values, we record concrete
values of symbolic string variables at each branching point
during the collection phase. Then, we create a query that
forces the solver to nd a solution other than the recorded
concrete value. Such constraints can be created by conjoin-
ing the original satisable PC with the disequalities of sym-
bolic values and their concrete values. If the solver returns
UNSAT, then such branching point is marked as a singleton.
At the end of the execution, the sets of singleton branching
points are reported. Ideally, all solvers will report that all
singleton branching points are also unsatisable branching
points, since it is a trivial case.
5.4 Artifacts
Table 2 gives an overview of the program artifacts used
to generate the benchmarks, i.e., the PCs. The rst col-
umn describes the name of the artifact and its abbreviation,
which we use to refer to the program. The second column
shows the number of classes in each program. Column three
provides a brief description of the program. Column \Tr."
displays the number of program executions, or traces, from
which we collected PCs. The \Op." column lists the average
number of string operations per trace from all PCs, while
\Pred." does the same for string predicates.
We opted to collect our own inter-procedural PCs instead
of using available PCs for several reasons. Thus, several
existing PCs are not completely representative of a full pro-
gram path because they are limited, e.g, in the length of
input strings or constraints in the PC [28] and taken from
non Java programs. Also, existing PCs may represent paths
2https://github.com/twall/jnathat are not likely to be traversed, whereas our PCs repre-
sent paths from the test cases developed by the authors of
the artifacts.
We disregard all branching points where a timeout occurs
in the negated path from the one taken in DSE, as in this
case we are unsure that a SAT result is correct. Thus, the
\TOs" column lists the average number of branching points
excluded in each trace of that artifact due to timeouts.
Finally, the \%Cov." column lists the statement and
branch coverage obtained by all traces of the artifact, in the
format %statement/%branch. Our objective for the traces
was to collect diverse string constraints, not to achieve high
coverages. Furthermore, we only targeted a small part of
large artifacts such as for program ITE. All the program
artifacts are open-source and available from SourceForge [1]
online code repository. We selected those programs because
of their extensive use of strings.
We used our DSE tool to instrument the programs and
executed them using the test suite supplied with the pro-
grams or generated our own using the category partition
method [25]. In the next section, we present the results that
our experiments produced for these 174 program traces.
All experiments were run on a machine with a 2.3 GHz
Intel i7 processor running Mac OS X Version 10.8.5 with
8 GB of memory. We used Soot version 2.5.0 to instru-
ment our programs. We extended the solvers using Java
version 1.6.0 65. ECLiPSe-str was run using the ECLiPSe
constraint logic programming environment version 6.1. We
used the rst release of Z3-str and JSA version 2.1-1. Fi-
nally, we obtained an unnamed release of the STRANGER
SML library on December 3, 2013, by contacting the devel-
opers3.
6. RESULTS
We ranked our solvers in several categories for each pro-
gram trace, and we present the aggregated results in Table 3.
This table separates the categories of comparisons into mod-
eling cost, performance, and accuracy.
We label the categories \No Model" for the count of meth-
ods a solver could not model,\Lines"for average lines of code
required to model, \Timeouts" for the number of timeouts
in negated branches, \Time" for the average time, \Disjoint"
for the percentage of disjoint branching points, \UNSAT"for
the percentage of unsatisable branching points, and \Non-
trivial" for the percentage of non-singleton, i.e., non-trivial,
unsatisable branching points. Within each category, the
\S" column denotes the statistic for a solver in each cate-
gory, e.g., the count of unmodeled methods, while the \R"
column denotes the rank of each solver in that category. The
nal column describes the overall rank of each solver. The
results presented below discuss data from this table, as well
as rankings for each individual program.
6.1 Modeling Cost
Even though Java's string types, i.e., java.lang.String ,
java.lang.StringBuffer , and java.lang.StringBuilder ,
contain over 165 methods, among which 12 are predicates,
we only present the modeling cost of those string meth-
ods that appeared in the programs' PCs, while excluding
those that have no eect on symbolic expressions, such
3STRANGER is now available at https://github.com/vlab-
cs-ucsb/Stranger.
265Modeling Cost Performance Accuracy
No Model Lines Timeouts Time Disjoint UNSAT Non-trivial
Solver S R S R S R S R S R S R S R Overall
EJSA 8 1 19.1 4 0 1 .00147 277 2 22 1 2.1 2 2
ESTRANGER 8 1 11.9 3 0 1 .00141 179 1 22 1 2.2 1 1
EZ3-str 12 2 5 1 439 2 .28174 470 3 12 2 1.3 3 3
EECLiPSe-str 12 2 5.8 23275 3 .22492 361 4 11 3 .1 4 4
Table 3: Overall ranking of our solvers in several categories.
append(-)
charAt(
int)
length
()
toLowerC
ase()
equals
(Object)indexOf(-)
trim()
equalsI
gnoreCase(String)
startsWith
(String)
substring(int,
int)
substring(int)
setLength(
int)
replace
(-,-)
delete(int
,int)
replaceAl
l(String,String)
endsWith(S
tring)
contains(CharS
equence)
isEmpty()
split(
String)
toUpperC
ase()
append(char[],int,int)
compareToI
gnoreCase(String)
lastIndexO
f(String)
copyV al
ueOf(char[],int,int)11121>30Lines
of CodeEJSA
ESTRANGER
EZ3-str
EECLiPSe-str
Figure 4: Displays the modeling cost for each method and each extended solver.
as the toString() method. The total number of meth-
ods used in the traces are 33, from which only 24 have
unique names. We represent all overloaded methods with
the same names as a method with the wildcard character \-"
in methods' signatures, e.g., replace(-,-) denotes both re-
place(char,char) and replace(CharSequence, CharSe-
quence) . The modeling cost of those methods is the sum
of lines of code of all aggregated methods. Figure 4 displays
the encountered string methods on its x-axis. The order of
these methods is based on the on the frequency with which
they appear in all program traces. Thus, append(-) meth-
ods were encountered more often than any other method.
The modeling cost for each string constraint is shown on
the y-axis of the bar graph in Figure 4. If a solver cannot
model a method, the corresponding bar is absent. For ex-
ample, none of the solvers can model charAt(int) , and only
EJSA and ESTRANGER can model toLowerCase() . The
graph shows that, out of 24 aggregated methods, eight of
them could not be modeled by any solver4.
The higher number of lines of code that is needed
by EJSA to model equalsIgnoreCase(String) ,sub-
string(int,int) ,substring(int) , and delete(int,int) ,
as well as trim() and equalsIgnoreCase(String) by ES-
TRANGER, reect our custom implementation of the sub-
string algorithm to increase the modeling precision. Even
though EJSA and ESTRANGER required more lines of code
to model methods, we were able to model more methods due
to their available interfaces. So, it is no surprise that many
analysis tools [30, 12] adapt JSA in their constraint solvers.
This implies that users benet from an extensive interface
that can manipulate the solvers' underlying representations.
The modeling cost and the importance of accurate mod-
eling might vary by program, since a program might use a
subset of string operations. By using data from Figure 4 and
4We did not model operations such as charAt(int) be-
cause a comparison to a character is required to generate
the corresponding constraint. This can be handled by hy-
brid solvers, which also operate on symbolic integers. For
example, in the context of mixed constraints JST can model
that method [12].recording each method present in each program, we calcu-
lated that the average modeling costs for two programs could
vary signicantly depending on the solver. Thus, EZ3-str
had the best modeling cost for programs JXM, MQG, NCL,
BEA, HCL, and ITE, but EECLiPSe-str had the best mod-
eling cost for programs JHP and MPJ. Overall, we see in
the \Lines" columns of Table 3 that EZ3-str performed the
best in terms of modeling cost, but couldn't model as many
methods as EJSA or ESTRANGER, as shown in the \No
Model" columns.
6.2 Performance
Figure 5a displays the performance of the solvers. The y
axis of the graph depicts time in seconds while the x axis
contains all traces grouped by the program. Dashed vertical
lines indicate the boundaries of program traces. The data
shows that the performance not only varies among solvers
for the same program, e.g., the data for BEA program, but
that no single constraint solver outperforms all other solvers
for all programs. We also see that the performance of the
solvers depends on the program and the PCs within it. For
example, EECLiPSe-str has the second best performance for
program JXM but the worst performance for NCL.
In addition, the data shows that when a solver has the
worst or the best performance for a single program trace,
it tends to stay the same for the rest of the program traces
for one program. Therefore, by sampling PCs of a program
the user can identify solvers that are more likely to perform
well for that program. We conjecture that this is due to the
string method composition of PCs.
EZ3-str has the worst performance overall, since for each
program it either demonstrates the worst performance or it
is the close second to worst. EECLiPSE-str has the most
variability in its performance. For some programs, such as
NCL, it has the worst performance due to timeouts, while for
other programs, such as JXM, its performance is comparable
with automata-based solvers.
For each program trace, one of the automata-based solvers
always exhibits the best performance, except for a handful of
26610 410 2100
JHP JXM MPJ MQG NCL BEA HCL ITE
(a)Y axis displays the average time per branching point in seconds.
0 %50 %100 %
JH
P JX
M M
PJ M
QG N
CL B
EA H
CL I
TE
(b)The proportion of all disjoint branching points.
0 %50 %100 %
JH
P JX
M M
PJ M
QG N
CL B
EA H
CL I
TE
(c)The proportion of unsatisable branching points.
0 %50 %100 %
JH
P JX
M M
PJ M
QG N
CL B
EA H
CL I
TEEJSA
ESTRANGER
EZ3-str
EECLiPSe-str
Total
(d)The proportion of unsatisable branching points that are not singleton.
Figure 5: Records accuracy in our test suite.
traces for program HCL, where EECLiPSe-str exhibits the
best performance. Overall, we see in the \Time" columns of
Table 3 that ESTRANGER has the best aggregated perfor-
mance results, although EJSA is a close second.
6.3 Accuracy
The set of graphs in Figure 5, excluding Figure 5a, depicts
the accuracy data derived from the metrics of accuracy in-
troduced in Section 4. As for the performance graph, the
x axis displays the program traces grouped by program. In
these graphs, the order of traces is the same as in the perfor-
mance graph. The unit of the y axis is the percentage of the
type of branching points, e.g., disjoint branching points, to
the total number of branching points analyzed in the trace.
The rst and the least measure of accuracy that we use is
the number of disjoint branching points. Figure 5b's graph
shows shows this metric. Interestingly, EZ3-str's result is
far from 100% disjoint branching points, as solvers based
on SMT such as Z3-str should always partition the domains
of a predicate and its negation when it doesn't time out.
However, several of these non-disjoint branching points were
introduced in our models of Java predicates, i.e., they occur
when Z3-str does not have a direct mapping and we must
provide an imprecise model. Furthermore, recent work has
revealed that Z3-str is not always complete [23].
Figure 5b shows some variation in the results. For ex-
ample, ESTRANGER or EJSA record the most of these
branching points for most traces of programs JHP, JXM,
MQG, BEA, and ITE, while EZ3-str or EECLiPSe-str often
report the most disjoint branching points for traces of MPJ,
NCL, and HCL. We see in the \Disjoint" columns of Table 3that in general ESTRANGER is best at detecting disjoint
branching points.
The graph in Figure 5c shows the percentage of unsatis-
able branching points that each solver is able to identify with
the solid line representing the total number of unsatisable
branching points found by all solvers. The data indicates
that no solver always reports the most unsatisable branch-
ing points, although in the \UNSAT" columns of Table 3 we
see that EJSA and ESTRANGER usually report the most.
We believe this is because the other solvers were forced to
over-approximate the results of operations such as toLow-
erCase() . Thus, all solvers could gain from including an
interface to model these methods.
Since we want to examine how eective string solvers are
in identifying unsatisable branching points, we want to re-
move all trivial cases when a PC contains no symbolic values.
Earlier we identied the branching points whose PCs contain
only concrete values as singleton branching points. Thus, in
order to identify non-trivial cases of detecting unsatisable
branching points we removed all singleton branching points
from the unsatisable branching points.
The result of this operation is displayed in Figure 5d. The
data implies that a majority of the unsatisable branching
points were detected due to non-symbolic constraints. We
believe this is purely due to the structure of the constraints,
and perhaps it is a common characteristic of string con-
straints. Hence, solvers might benet from implementing an
additional layer for evaluating concrete constraints, reliev-
ing users of the need to implement those checks as part of
their analysis tool.
Program NCL clearly has the most non-singleton unsat-
isable branching points, and we also see poor performance
267in EECLiPSe-str and EZ3-str for NCL. This result supports
our conjecture that these branching points are more dicult
to solve.
Figure 5d shows that EJSA and ESTRANGER were usu-
ally able to detect the most unsatisable branching points
for non-trivial PCs, mainly in the traces of NCL and BEA
programs. The overall results on these branching points
in the \Non-trivial" columns of Table 3 shows that ES-
TRANGER reports the most of these branching points while
EJSA is a close second.
6.4 Discussion
When we ranked the solvers in the categories shown in
Table 3 for each artifact, we found that EJSA had the best
ranking for 5 programs, ESTRANGER for 6, EZ3-str for 0,
and ECLiPSe-str for 1, as some solvers had the same ranking
for the same programs. Based on that result and the \Over-
all" column of Table 3, we conclude that ESTRANGER dis-
played the best overall results. Thus, we recommend that if
the user must choose one solver for SE tasks with no knowl-
edge of the Java programs to be analyzed, then he or she
should select STRANGER.
However, examinations of the structure of a program
through a light-weight static analysis may provide addi-
tional information for selecting an adequate solver. In our
experiments we encountered several cases when the com-
position of a PC aected evaluation criteria. For exam-
ple, we have noticed that non automata solvers EZ3-str and
EECLiPSe-str have the best accuracy ranking only for pro-
gram MPJ. When examining the PCs of MPJ we observed
that MPJ contains several instances of equals(Object) ,
which automata-based solver cannot handle very well, and
no instances of toUpperCase() ortoLowerCase() , which
non-automata solvers fail to model. If PCs contain string
methods that are modeled uniformly by all solvers, like
in BEA program, then their accuracies are the same and,
hence, the user might consider other factors for selection.
This observation also explains the correlation of the vari-
ation in solvers' accuracy levels with the number of string
methods in program PCs. For example, Figure 5b shows the
most variation in programs JHP, NCL, HCL and ITE, which
have 15, 11, 18 and 17 aggregated methods correspondently.
Thus, in some PCs of a program, methods that are modeled
more precisely by one solver are present, and in other PCs
methods modeled better by another solver are present.
In some cases, merely matching the set of methods in a PC
to a solver that models them more precise might be inade-
quate and a more advance analysis of PCs should be consid-
ered. For example, even though EZ3-str and ECLiPSe-str
have the best model of startsWith(String) method, de-
pending on the type of the argument passed to the method,
EJSA's less precise model of this method produces better
accuracy results. This is the case of JHP program. We
have noticed that for some JHP PCs EZ3-str/ECLiPSe-
str have better accuracy, e.g., in the number of disjoint
branches, while on other PCs they don't. We found that
when the argument passed to startsWith(String) was con-
crete EJSA had more disjoint branches, while when the ar-
gument was symbolic EZ3-str/ECLiPSe-str determines more
disjoint branches.
Perhaps, even more complex interaction of string methods
in a PC might aect the accuracy of a solver. For exam-
ple, in NCL program EECLiPSe-str has the worst accuracydue to timeouts in processing startsWith(String) that in-
volved more complex string method sequences than JHP.
Finding such complex interactions of string methods in a
PC is a challenging but rewarding task and would require
ner accuracy measurements that, for example, use string
model counting, or account for imprecisions in the modeling
of string operations.
As for the performance criterion, we observed a strong
positive correlation ( r=:94) between EZ3-str's performance
and the average number of predicates in each query. We con-
jecture that this is due Z3-str not being incremental, since it
cannot eciently solve PCs containing multiple predicates.
Thus, we recommend the user not use Z3-str to solve PCs
containing several predicates, and we hope future versions
of Z3-str and ECLiPSe-str support incremental solving.
7. CONCLUSION AND FUTURE WORK
Advances in constraint solvers and successful applications
of SE in software verication prompted researchers to extend
SE-based techniques to incorporate, in addition to primitive
numerical data types such as integers, advanced data types.
As a result, the research in SE has been targeting constraints
over complex data types such as symbolic strings [27, 12, 28,
3], symbolic collections [21], and symbolic heaps [18, 11, 13].
When selecting a solver for advanced data types, one needs
to be aware that solvers might vary greatly in the level of
support they provide for the theories of their data type.
Our exploratory study demonstrated this for the theory of
strings. As the number of operations and predicates in-
creases, the theory of a particular data type can become
undecidable, e.g., the theory of strings is in general unde-
cidable [3], as, presumably, are a majority of theories of
advanced data types. To deal with undecidability, solvers
either over-approximate or under-approximate the problem,
usually without informing a user about it. In this work, we
have introduced measurements of accuracy for string con-
straint solvers in the context of symbolic execution, which
can be used for other solvers of advanced data types.
Our evaluations of four string constraints solvers revealed
that STRANGER solver has the highest rank with JSA as
a close second, even though none of them were originally
designed for SE; moreover, STRANGER was even originally
designed to support a dierent programming language.
Yet, the overall performance of the solvers vary both be-
tween programs and between PCs of the same program.
We conjuncture that, if resources permit, all solvers should
be executed in parallel and exchange information between
themselves. For example, an automata-based solver might
create a regular expression of accepting strings and a bit-
vector solver might create constraints from that regular ex-
pression. Furthermore, passing learned information would
allow for advanced accuracy comparisons. For example, we
would be able to determine if the set of values represented
by a symbolic value in EJSA is a subset of the set of values
represented by a symbolic value in EZ3-str.
Another direction of future work is to identify and mea-
sure the accuracy of string operations and how it aects the
overall accuracy of a solver. For example, a disjoint branch-
ing point does not guarantee accurate values. Instead, pre-
vious over-approximation could aect the overall accuracy
of the solver. We plan to track dependencies from potential
sources of such approximation and present the results in our
subsequent work.
2688. REFERENCES
[1] Sourceforge. http://sourceforge.net/ .
[2] M. Biehl, N. Klarlund, and T. Rauhe. Algorithms for
guided tree automata. In D. Raymond, D. Wood, and
S. Yu, editors, Automata Implementation, volume
1260 of Lecture Notes in Computer Science, pages
6{25. Springer Berlin Heidelberg, 1997.
[3] N. Bjrner, N. Tillmann, and A. Voronkov. Path
feasibility analysis for string-manipulating programs.
InProceedings of the 15th International Conference on
Tools and Algorithms for the Construction and
Analysis of Systems,ETAPS 2009, , TACAS '09, pages
307{321, Berlin, Heidelberg, 2009. Springer-Verlag.
[4] F. B uttner and J. Cabot. Lightweight string reasoning
for ocl. In Modelling Foundations and Applications ,
pages 244{258. Springer, 2012.
[5] F. B uttner and J. Cabot. Lightweight string reasoning
in model nding. Software & Systems Modeling, pages
1{15, 2013.
[6] T.-H. Choi, O. Lee, H. Kim, and K.-G. Doh. A
practical string analyzer by the widening approach. In
Programming Languages and Systems, pages 374{388.
Springer, 2006.
[7] A. S. Christensen, A. Mller, and M. I. Schwartzbach.
Precise analysis of string expressions. In Proceedings of
the 10th International Conference on Static Analysis ,
SAS'03, pages 1{18, Berlin, Heidelberg, 2003.
Springer-Verlag.
[8] L. A. Clarke. A system to generate test data and
symbolically execute programs. IEEE Trans. Softw.
Eng., 2(3):215{222, May 1976.
[9] A. Coen-Porisini, G. Denaro, C. Ghezzi, and M. Pezz e.
Using symbolic execution for verifying safety-critical
systems. In Proceedings of the 8th European Software
Engineering Conference Held Jointly with 9th ACM
SIGSOFT International Symposium on Foundations
of Software Engineering, ESEC/FSE-9, pages 142{151,
New York, NY, USA, 2001. ACM.
[10] L. De Moura and N. Bjrner. Z3: An ecient SMT
solver. Tools and Algorithms for the Construction and
Analysis of Systems , pages 337{340, 2008.
[11] I. Dillig, T. Dillig, A. Aiken, and M. Sagiv. Precise
and compact modular procedure summaries for heap
manipulating programs. In PLDI , pages 567{577,
2011.
[12] I. Ghosh, N. Shaei, G. Li, and W.-F. Chiang. Jst: an
automatic test generation tool for industrial java
applications with strings. In Proceedings of the 2013
International Conference on Software Engineering ,
pages 992{1001. IEEE Press, 2013.
[13] B. Hillery, E. Mercer, N. Rungta, and S. Person.
Towards a lazier symbolic pathnder. In Proceedings
of the Java Pathnder Workshop , 2013.
[14] P. Hooimeijer and M. Veanes. An evaluation of
automata algorithms for string analysis. In
Proceedings of the 12th International Conference on
Verication, Model Checking, and Abstract
Interpretation , VMCAI'11, pages 248{262, Berlin,
Heidelberg, 2011. Springer-Verlag.
[15] P. Hooimeijer and W. Weimer. A decision procedure
for subset constraints over regular languages. In
Proceedings of the 2009 ACM SIGPLAN Conferenceon Programming Language Design and
Implementation, PLDI '09, pages 188{198, New York,
NY, USA, 2009. ACM.
[16] P. Hooimeijer and W. Weimer. Solving string
constraints lazily. In Proceedings of the IEEE/ACM
International Conference on Automated Software
Engineering, ASE '10, pages 377{386, New York, NY,
USA, 2010. ACM.
[17] S. Kausler. Evaluation of string constraint solvers
using dynamic symbolic execution. Master's thesis,
Boise State University, USA, 2014.
[18] S. Khurshid, C. S. P as areanu, and W. Visser.
Generalized symbolic execution for model checking
and testing. In Proceedings of the 9th International
Conference on Tools and Algorithms for the
Construction and Analysis of Systems , TACAS'03,
pages 553{568, Berlin, Heidelberg, 2003.
Springer-Verlag.
[19] A. Kiezun, V. Ganesh, S. Artzi, P. J. Guo,
P. Hooimeijer, and M. D. Ernst. Hampi: A solver for
word equations over strings, regular expressions, and
context-free grammars. ACM Trans. Softw. Eng.
Methodol. , 21(4):25:1{25:28, Feb. 2013.
[20] J. C. King. Symbolic execution and program testing.
Communications of the ACM , 19(7):385{394, 1976.
[21] D. Kr oning, P. R ummer, and G. Weissenbacher. A
proposal for a theory of nite sets, lists, and maps for
the smt-lib standard. In Informal proceedings, 7th
International Workshop on Satisability Modulo
Theories at CADE 22 , 2009.
[22] G. Li and I. Ghosh. Pass: String solving with
parameterized array and interval automaton. In
Hardware and Software: Verication and Testing ,
pages 15{31. Springer, 2013.
[23] T. Liang, A. Reynolds, C. Tinelli, C. Barrett, and
M. Deters. A dpll (t) theory solver for a theory of
strings and regular expressions*. Technical report,
Technical report, Department of Computer Science,
The University of Iowa, 2014.(Available at
http://cvc4. cs. nyu. edu/papers/CAV2014-strings/).
[24] A. Mller. dk.brics.automaton.
http://www.brics.dk/automaton .
[25] T. J. Ostrand and M. J. Balcer. The category-partition
method for specifying and generating fuctional tests.
Communications of the ACM , 31(6):676{686, 1988.
[26] C. S. P as areanu and N. Rungta. Symbolic pathnder:
Symbolic execution of java bytecode. In Proceedings of
the IEEE/ACM International Conference on
Automated Software Engineering, ASE '10, pages
179{180, New York, NY, USA, 2010. ACM.
[27] G. Redelinghuys, W. Visser, and J. Geldenhuys.
Symbolic execution of programs with strings. In
Proceedings of the South African Institute for
Computer Scientists and Information Technologists
Conference, pages 139{148. ACM, 2012.
[28] P. Saxena, D. Akhawe, S. Hanna, F. Mao,
S. McCamant, and D. Song. A symbolic execution
framework for javascript. In Security and Privacy
(SP), 2010 IEEE Symposium on , pages 513{528.
IEEE, 2010.
269[29] K. Sen, D. Marinov, and G. Agha. Cute: a concolic
unit testing engine for c. In Proc. ESEC/FSE , pages
263{272, 2005.
[30] D. Shannon, S. Hajra, A. Lee, D. Zhan, and
S. Khurshid. Abstracting symbolic execution with
string analysis. In Testing: Academic and Industrial
Conference Practice and Research
Techniques-MUTATION, 2007.
TAICPART-MUTATION 2007 , pages 13{22. IEEE,
2007.
[31] A. Stump, T. Weber, and D. Cok. SMTEVAL
2013:progress report. http://sat2013.cs.helsinki.
fi/slides/SMTEVAL2013.pdf .
[32] R. Vall ee-Rai, P. Co, E. Gagnon, L. Hendren, P. Lam,
and V. Sundaresan. Soot: A java bytecode
optimization framework. In CASCON First Decade
High Impact Papers , pages 214{224. IBM Corp., 2010.
[33] M. Veanes, P. De Halleux, and N. Tillmann. Rex:
Symbolic regular expression explorer. In Software
Testing, Verication and Validation (ICST), 2010
Third International Conference on , pages 498{507.
IEEE, 2010.[34] F. Yu, M. Alkhalaf, and T. Bultan. Stranger: An
automata-based string analysis tool for php. In Tools
and Algorithms for the Construction and Analysis of
Systems , pages 154{157. Springer, 2010.
[35] F. Yu, M. Alkhalaf, T. Bultan, and O. H. Ibarra.
Automata-based symbolic string analysis for
vulnerability detection. Formal Methods in System
Design, pages 1{27, 2013.
[36] F. Yu, T. Bultan, M. Cova, and O. H. Ibarra.
Symbolic string verication: An automata-based
approach. In Model Checking Software , pages 306{324.
Springer, 2008.
[37] F. Yu, T. Bultan, and O. H. Ibarra. Symbolic string
verication: Combining string analysis and size
analysis. In Tools and Algorithms for the Construction
and Analysis of Systems , pages 322{336. Springer,
2009.
[38] Y. Zheng, X. Zhang, and V. Ganesh. Z3-str: a
z3-based string solver for web application analysis. In
Proceedings of the 2013 9th Joint Meeting on
Foundations of Software Engineering , pages 114{124.
ACM, 2013.
270