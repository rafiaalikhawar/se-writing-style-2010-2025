Programs, Tests, and Oracles:
The Foundations of Testing Revisited∗
Matt Staats, Michael W. Whalen, and Mats P .E. Heimdahl
Department of Computer Science and Eng.
University of Minnesota
[staats,whalen,heimdahl]@cs.umn.edu
ABSTRACT
In previous decades, researchers have explored the formal foun-
dations of program testing. By exploring the foundations of test-
ing largely separate from any speciﬁc method of testing, these re-
searchers provided a general discussion of the testing process, in-
cluding the goals, the underlying problems, and the limitations of
testing. Unfortunately, a common, rigorous foundation has not
been widely adopted in empirical software testing research, making
it difﬁcult to generalize and compare empirical research.
We continue this foundational work, providing a framework in-
tended to serve as a guide for future discussions and empirical stud-
ies concerning software testing. Speciﬁcally, we extend Gourlay’s
functional description of testing with the notion of a test oracle, an
aspect of testing largely overlooked in previous foundational work
and only lightly explored in general. We argue additional work
exploring the interrelationship between programs, tests, and ora-
cles should be performed, and use our extension to clarify concepts
presented in previous work, present new concepts related to test
oracles, and demonstrate that oracle selection must be considered
when discussing the efﬁcacy of a testing process.
Categories and Subject Descriptors
D.2.5 [ Software Engineering ]: Testing and Debugging
General Terms
Theory
Keywords
Theory of testing, testing formalism
∗This work has been partially supported by NASA Ames Research
Center Cooperative Agreement NNA06CB21A, NASA IV&V Fa-
cility Contract NNG-05CB16C, and NSF grants CCF-0916583 and
CNS-0931931.
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
ICSE ’11, May 21–28, 2011, Waikiki, Honolulu, HI, USA
Copyright 2011 ACM 978-1-4503-0445-0/11/05 ...$10.00.1. INTRODUCTION
For several decades, testing has been an important research topic
and a standard part of software development practice. In the 1980s
and early 1990s, several authors explored the theory of testing in
an attempt to put testing research on a solid formal foundation and
provide coherent frameworks for discussion. Notable contributions
include explorations of: the problem of test set selection [16, 17];
the problem of constructing test data adequacy criteria [26, 38, 44];
the need to compare test data adequacy criteria [12, 17, 41]; the
use of input partitioning when constructing test data adequacy cri-
teria [12, 40]; and the use of test hypotheses when selecting test
sets [4, 5, 14]. This body of work largely discusses the founda-
tion of testing—the goals, underlying problems, and limitations of
testing—separate from any speciﬁc method of testing.
While these early contributions are valuable and helped shape
the direction of testing research as well as our understanding of
testing practice, this body of work has unfortunately not established
itself as a foundation for continued testing research; new testing
approaches are typically informally described and generally poorly
evaluated. This lack of a formal foundation and rigorous evaluation
of proposed new approaches has been a persistent problem in the
research community [6].
Consider as examples the generally well conducted and highly
inﬂuential studies of Rothermel et al. [31], and Wong et al. [42].
Although these studies provide insight into test suite reduction, cru-
cial aspects of the experimental setup such as what test oracle was
used and the nature and structure of the programs under test are
omitted or left implicit, leaving the reader to infer these proper-
ties of the artifacts. This in turn makes it difﬁcult to interpret the
conﬂicting conclusions reached in the studies. We believe these
problems can be traced to the lack of a common foundation for
empirical testing research, making it difﬁcult—if not impossible—
to conduct, for example, meta-analysis synthesizing the empirical
results from several independent investigations.
In this work, we attempt to remedy this situation and provide
a common framework for empirical testing research by revisiting
work on the formal foundations of testing, highlighting and clar-
ifying issues with the existing work. We have identiﬁed two is-
sues with the existing formalizations that we believe should be ad-
dressed. First, the existing formalizations overlook certain factors
inﬂuencing testing—notably test oracles—leading to implicit as-
sumptions about testing that may not be true in practice. These
assumptions make it straightforward to prove properties about dif-
ferent aspects of testing, for example, properties about test cover-
age criteria, but such assumptions lead to research results that may
be misleading or may not be generally applicable in real testing
projects. In addition, as mentioned above, such implicit assump-
tions makes comparisons of research efforts difﬁcult.Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
ICSE ’11, May 21–28, 2011, Waikiki, Honolulu, HI, USA
Copyright 2011 ACM 978-1-4503-0445-0/11/05 ...$10.00
391P
S
T OSyntactic structure may guide 
test selectionSemantic
s dete
rmine
s propagation 
of errors fo
r each test
Combination of O and T determines efficacy of testing  process
Tests suggest variables worth observingTests designed to di
stinguish i
ncorrect P 
from SS may guide test selectionP attempts to 
implement S
O approximates S
Observability of P limits information available to OFigure 1: Example relationships between testing factors.
Second, most foundational research has focused on narrow as-
pects of the testing problem, for example, criteria for selecting tests
or techniques to generate tests from programs. We believe a holistic
view of the testing problem is essential to move the ﬁeld forward
and yield practically useful results in testing research, both in terms
of theoretical and empirical results. In particular, the interrelation-
shipbetween the programs under test, the test set selected, and the
oracle used is not well understood. For example, the effects of pro-
gram structure on the efﬁcacy of structural coverage criteria and the
effect of oracle selection on fault ﬁnding have not been adequately
explored in current literature.
The implications of this extend to all testing processes. We have
in our empirical work observed how failing to consider all factors
in testing can impact the efﬁcacy of the testing process, speciﬁ-
cally: how the the structure of the program under test affects the
efﬁcacy of the MC/DC structural coverage criterion [28], and how
the percentage of program state considered by an oracle and the
size of a randomly generated test set jointly inﬂuence fault ﬁnding
ability [32].
In this paper, we begin to address these issues by ﬁrst providing a
formal foundation of testing acknowledging the role of test oracles
in the testing process. We then use this framework to explore the
interrelationship between the artifacts involved in software testing.
In particular, we consider how one common implicit assumption—
the existence of a test oracle—affects the results of previous work
on the theory of testing. We ﬁnd this implicit assumption is key to
existing results, demonstrating, for example, that comparisons be-
tween test coverage criteria must be made with respect to a constant
test oracle; we clarify existing work, demonstrating, for example,
that the spectrum of mutation testing techniques in fact represents
one technique deﬁned in terms of the adequacy of both a test set and
an oracle; and ﬁnally we discuss new concepts, deﬁning, for exam-
ple, desirable properties of test oracles such as sound andcomplete .
Based on our formalization and observations, we identify a collec-
tion of research challenges we see as important to further the fron-
tiers in software testing.
2. A HOLISTIC VIEW OF TESTING
Early work on the testing process focused on test selection, lead-
ing to the deﬁnition of test data adequacy criteria by Goodenough
and Gerhart [16] as well as others [17, 40]. Subsequent work
in testing has reinforced this test-selection-centric view of testing,
with one author even observing that the problem of testing is to
“ﬁnd a graph and cover it” [3].
Unsurprisingly, a large quantity of work explores issues such aswhat test coverage criteria to use and how to generate tests satisfy-
ing those criteria.
Nevertheless, some authors have acknowledged and explored the
inﬂuence of other artifacts on testing research. In Figure 1, we il-
lustrate the interrelationships between 4 testing artifacts commonly
discussed in the literature: speciﬁcations, programs, tests, and ora-
cles. In this context, speciﬁcations Srepresent the abstract, perfect
notion of correctness; the only arbiter of correctness representing
the true requirements of the software—not the possibly ﬂawed re-
quirements as captured in a document or formal speciﬁcation.
Some of these relationships are straightforward and intuitive. For
example, the relationship between the speciﬁcation and the other
artifacts is obvious—the program is derived from, and intended to
implement, the speciﬁcation (for instance, through requirements
capture, architecture, design, and coding, or through the creation
of a formal model and code generation), tests are in a similar way
derived from the speciﬁcation and are intended to demonstrate exe-
cutions where the program violates the the speciﬁcation, and, sim-
ilarly, the oracle is based on the speciﬁcation and is intended to
determine, for each test, if the program has violated the speciﬁca-
tion. The interrelationships between programs, tests, and oracles
are less obvious, however, and warrant further discussion.
It is easy to see that all three artifacts are intertwined in the test-
ing process. For example, consider a stateful embedded program
with a sequence of simple decisions1. In testing such a program,
it is likely that longer test cases will be beneﬁcial, allowing cor-
rupted state to propagate to outputs. It is also likely that a more
powerful oracle considering internal state information will be ben-
eﬁcial since it may be able to detect corrupted internal states early.
In using longer test cases, however, we reduce the beneﬁts of using
an oracle observing internal state, as corrupted state information is
more likely propagate to the output. Thus, the characteristics of the
program suggest two methods of improving fault ﬁnding, but the
use of one method diminishes the value of the other. Such issues
involve the largely unexplored interrelationship between programs,
tests, and oracles.
While programs, tests and oracles are intertwined, to manage the
complexity of testing research we tend to think of the artifacts in
terms of pairs. Most testing research has been concerned with the
relationship between programs Pand testsT. Examples include
the development of structural coverage criteria, where the adequacy
of a test suite is based on how well it covers syntactic aspects of the
program structure [46] and the creation of automated techniques
to generate tests providing the desired coverage [29, 13] . Work
1A decision in this context is any boolean expression containing a
boolean operator.392relating other aspects of programs and tests is less common, and
includes work by Rajan et al. on how the structure of the program
affects the efﬁcacy of a test suite providing structural code cover-
age [28], and work by Voas et al. and others demonstrating how the
likelihood of errors propagating to the output can be used to guide
test selection [1, 10, 33].
In comparison, the interrelationships between oracles Oand both
the programs Por the testsThave received relatively little atten-
tion. Concerning the interrelationship between the programs Pand
oraclesO, Voas et al. explored how testability information can be
used to identify program locations where faults can hide, thus in-
dicating locations where assertions may be placed to improve the
quality of the oracle [35]. Concerning the interrelationship between
testsTand oraclesO, there have been a handful of studies inves-
tigating how the oracle and test set used jointly affect fault ﬁnd-
ing, including work by Memon et al. in the domain of GUI test-
ing [24], work by Briand et al. comparing state-based invariants
and input/expected value oracles [8], and our own work exploring
the joint affect of oracle data and test suite size [32].
The practical implications of this holistic view of testing are
threefold. First, we must develop new theories, techniques, and
tools for ﬁnding effective combinations of program characteristics,
tests, and oracles. In the next section, we propose an extension to
Gourlay’s testing framework aimed at addressing the lack of test-
ing theory related to oracles. We provide a formal treatment of
oracles, oracle selection, and oracle properties similar to that pro-
vided for tests and test selection. Note that this formal framework
is intended to provide a conceptual framework that can be used as
a foundation for future work in empirical software testing research,
not as a framework intended to prove obscure (and often unrealis-
tic) properties of the testing process.
Second, empirical research should more accurately reﬂect the
holistic nature of testing. At a minimum, testing research should
make explicit what is changed (e.g., the coverage criterion) and
what is kept constant (e.g., the structure of the program and the or-
acle). Authors should then clearly state why their experimental pa-
rameters were chosen and argue why they are reasonable. Unfortu-
nately, this has not generally been the case in the literature, leaving
the reader to guess what types of programs and oracles the research
generalizes to. Other researchers have also expressed concerns re-
garding these issues, for example, Briand touches upon some of
these issues in his discussion of validity issues in empirical testing
research [7].
Finally, we must study the interrelationships between artifacts
in greater detail; in particular, we believe greater study on the ef-
fect of test oracles is warranted. Example questions concerning the
relationship between oracles and programs include: “how do we
create suitable oracles for stateful programs where errors may take
signiﬁcant time to propagate to outputs?” . Example questions con-
cerning the relationship between oracles and tests include: ”given
test coverage criteria sensitive to program structure, which internal
variables should the oracle observe to improve fault ﬁnding abil-
ity [28]?” , and more generally, ”given ﬁnite testing resources, for
this a program Pwhat combination of tests Tand oracleOshould
I use to achieve high levels of fault ﬁnding?” —maybe improving
the test oracle by inserting more assertions in the code may be a
more cost effective solution than producing more tests.
3. FUNCTIONAL MODEL OF TESTING
Beginning with Goodenough and Gerhart’s seminal work [16], a
signiﬁcant portion of the research in the theory of testing has used
a functional model for testing, a convention we follow here. We
deﬁne our functional model of testing based on Gourlay’s frame-work [17], extending and modifying it for our discussion. We have
selected Gourlay’s framework as a basis for two reasons. First, a
signiﬁcant quantity of relevant theoretical work is based on this for-
malization; by extending his framework, we can easily reexamine
this previous work. Second, the framework is easy to understand
and mostly matches our intuitive sense of the testing process.
In Gourlay’s approach, a testing system is deﬁned as a collection
(P,S,T,corr,ok )where:
•Sis a set of speciﬁcations
•Pis a set of programs
•Tis a set of tests
•corr⊆P×S
•ok⊆T×P×S
As in our discussion in the previous section, each speciﬁcation
s∈Srepresents an abstract, perfect notion of correctness.
The predicate corr is deﬁned such that for p∈P,s∈S,
corr (p,s)impliespis correct with respect to s. Of course, the
value ofcorr (p,s)is generally not known; this predicate is thus
theoretical and used to explore how testing relates to correctness.
The predicate okis deﬁned such that for p∈P,s∈S,t∈
T ok (t,p,s )implies that pis judged as correct with respect to
speciﬁcation sfor testt. Furthermore, okis deﬁned such that
∀p∈P,∀s∈S,∀t∈T corr (p,s)⇒ok(t,p,s ), i.e., ifpis
correct with respect to sthenokis true for all tests. The predicate
okapproximately corresponds to what is now called a test oracle
or simply oracle .
While intuitively appealing, there are problems with this frame-
work. First, each testing system has only one possible oracle ( ok).
Just as there exist many possible tests and programs, however, there
exist many possible oracles for determining if test executions are
successful [30]. Selecting an oracle is the problem of oracle selec-
tion, and we cannot easily discuss or even formulate this problem
using Gourlay’s framework.
Second, the notion of correctness and how it relates to test or-
acles is—in our opinion—too coarse. If for p∈Pands∈S,
ifcorr (p,s)then we know that ∀t∈T, ok (t,p,s ). However,
there are no requirements on oracles in terms of their effectiveness
in ﬁnding faults. For example, the oracle that universally returns
true for all programs and speciﬁcations satisﬁes this relationship.
Furthermore, it will often (always?) be the case that the program p
does not satisfy the speciﬁcation s, i.e.,¬corr (p,s), in which case
the framework places no constraints on ok.
Both the inability to discuss oracle selection, and the loosely
speciﬁed relationship between program correctness and oracle be-
havior create difﬁculties and ambiguities when discussing the ef-
fectiveness of test selection techniques and test oracles. We there-
fore make two major changes to Gourlay’s deﬁnition of a testing
system. First, we remove the predicate ok, replacing it with the set
Oof test oracles. We state that an oracle o∈Ois a predicate:
o⊆T×P
An oracle determines, for a given program and test if the test passes.
Second, we add a predicate deﬁning correctness with respect to a
testt∈T. This predicate is2:
corr t⊆T×P×S
The predicate corr t(t,p,s )holds if and only if the speciﬁcation s
holds for program pwhen running test t. Obviously,
∀p∈P,∀s∈S, corr (p,s)⇒ ∀t∈T corr t(t,p,s )
2Note that the tsubscript incorr tis used to differentiate the pred-
icate from Gourlay’s corr predicate. It does not relate to any spe-
ciﬁc testt393In summation, we deﬁne a testing system to be a collection
(P,S,T,O,corr,corr t)where:
•Sis a set of speciﬁcations
•Pis a set of programs
•Tis a set of tests
•Ois a set of oracles
•corr⊆P×S
•corr t⊆T×P×S
To keep things general, we make no attempt to deﬁne what ex-
actly constitutes a test, oracle, speciﬁcation, or program. We state
that a test (sometimes called test data) is a sequence of inputs ac-
cepted by some program. As in Gourlay’s framework, we consider
a speciﬁcation s∈Sto be the true (idealized) speciﬁcation of
the desired functionality of program P, possibly including inter-
nal state behavior. As mentioned earlier, it is quite likely that the
stated software requirements or formal speciﬁcations used in the
development of a program differ from s. Finally, we note that the
predicates are partially deﬁned: not all tests can be executed on all
programs, and not all oracles can be used to determine if a test tis
successful when run against a program p.
These modiﬁcations to the framework allows us to have a more
realistic discussion of the testing problem and explore the interre-
lationships between programs, tests, and oracles.
3.1 Test Oracles
A test oracle determines if the result of executing a program p
using a testtis correct. There are many methods of creating an or-
acle, including manually specifying expected outputs for each test,
monitoring user-deﬁned assertions during test execution, and veri-
fying if the outputs match those produced by some reference imple-
mentation, for example, an executable model. A uniform method
of describing the numerous types of oracles is outside the scope of
this work.
Nevertheless, we can deﬁne general oracle properties. We be-
gin by deﬁning oracle properties related to correctness of the pro-
gram being testing, borrowing terms commonly used in software
veriﬁcation. An oracle is complete with respect to program pand
speciﬁcation sfor a test case tif:
corr t(t,p,s ) =⇒o(t,p)
Complete oracles relate to correctness as we intuitively expect: if
the result of running toverpis correct with respect to s, the oracleo
will state the test passes. Most oracles discussed in testing research
and used in practice are designed to be complete, though like all
software engineering artifacts, oracles are imperfect and may con-
tain ﬂaws. For example, a common problem is an oracle that is too
precise. The oracle may have been deﬁned to expect an output of
11
3but the program generates 1.3334. In the application domain,
this accuracy in the computation is perfectly ﬁne and the program
is thus correct, but the oracle will reject the test. Nevertheless, in
order to discuss the efﬁcacy of the testing process, researchers of-
ten assume the oracle used is complete. Note that Gourlay’s ok
predicate is by deﬁnition complete if corr (p,s).
An oracle is sound with respect to pandsfor a test case tif:
o(t,p) =⇒corr t(t,p,s )
Sound oracles represent the conventional wisdom in testing that
“testing may be imperfect, but at least we know that the program
is correct for the tests we have run. ” For this statement to hold,
we must use a sound oracle. Unfortunately, in practice, oracles are
rarely sound. For example, an oracle might only observe a sub-
set of the program outputs (and/or the program’s persistent state)and would naturally miss any faults manifested in the variables (or
state) not observed by the oracle. For this reason, we do not assume
sound oracles in our discussion.
We say that an oracle is perfect with respect to p,s, andtif it is
both sound andcomplete . We can now generalize the deﬁnitions of
complete ,sound andperfect to test suites and again to the entire set
of tests. For example, an oracle is perfect forpandsif:
∀t,o(t,p)⇔corr t(t,p,s ).
As mentioned above, oracles need not be sound nor complete;
oracles may both fail to detect faults and may detect faults that do
not exist. Heuristic oracles may be neither sound nor complete,
and may be used in domains like image processing where precisely
deﬁning correctness is difﬁcult or time consuming [23]. Relating
this type of oracles to correctness would require probabilistic argu-
ments and is beyond the scope of this work. Weyuker informally
discusses these oracle characteristics in [37], noting several practi-
cal challenges concerning test oracle construction; we are unaware
of any formulation of these oracle properties however.
In this paper, we will often consider oracles which base correct-
ness partly on the internal state of the program. Such oracles may
be constructed if the speciﬁcation deﬁnes behaviors internal to the
program (e.g., state invariants or class invariants). In some situa-
tions, these oracles will detect faults that do not propagate to the
output (at least not immediately). We use Avizienis and Laprie’s
terminology [2], in which a fault is deﬁned as a system state where
a design error manifests. Thus, detecting a fault is not synonymous
with detecting a failure .
3.1.1 Oracle Data
As noted above, the problem of constructing an oracle, while in
principle simply involved partitioning each (t,p)tuple into True
andFalse outcomes, is quite complex in practice. Many factors in
oracle selection are dependent on the method underlying the con-
struction of the oracle; these factors are outside the scope of our
framework.
One factor present in all oracles, however, is the portion of the
program state considered by the oracle, which we term the oracle
data. For example, a commonly used oracle in the testing literature
is one which determines correctness for test tby comparing the
outputs produced by the program to outputs speciﬁed in the oracle.
The oracle data for this oracle is the set of outputs. If the oracle
were instead to consider the value of every internal variable as well
as the output, the oracle data would be the set of program variables
(internal as well as outputs). For simplicity, in this paper we limit
our discussion to oracles where the oracle data is a set of variables.
In our discussions, we will use as examples oracles that operate
by comparing values produced by the program for some test against
expected values for said tests. We will refer to such oracles as
input/expected value oracles . When presenting several oracles for
the same system, these oracles will typically differ in their oracle
data.
Highly related to our discussions in this section, Richardson et
al. discuss the oracle problem —the need for testers to provide a
test oracle for the testing process [30]. This work has served as the
standard for oracle terminology; the authors deﬁne the oracle infor-
mation and the oracle procedure . The oracle information speciﬁes
the correct behavior, and the oracle procedure veriﬁes the test exe-
cution with respect to the oracle information.
We consider the issues of oracle information and oracle proce-
dure to be speciﬁc to the method of oracle construction. Deﬁning
precisely what constitutes oracle information and what constitutes
oracle procedure is difﬁcult, and we therefore make no attempt to394incorporate them into our framework. We instead opt to use deﬁni-
tions that are useful in discussing all oracles (such as complete and
oracle data ).
3.2 Test and Oracle Adequacy
Gourlay deﬁnes a test methodMas a function:
M:P×S→T
Thus, a test method takes a program and a speciﬁcation and gen-
erates a test. Gourlay appears to also consider test methods
M:P×S→2T;
that is, test methods producing sets of tests. Nevertheless, the use of
test coverage criteria (also called test data adequacy criteria [38]
ortest selection criteria [16]) where one or more (or none) test
sets are acceptable for a given program and speciﬁcation is much
more common in both the testing literature and in practice. We thus
adopt it here, using the predicate deﬁnition originally presented by
Weiss [36]:
TC⊆P×S×2T.
We are exploring how test oracles inﬂuence the testing process.
We therefore propose an analogous concept for oracles, termed an
oracle adequacy criterion . An oracle adequacy criterion OCis a
predicate:
OC⊆P×S×O.
This predicate reﬂects how oracle selection is usually done in prac-
tice: a single oracle is used to evaluate the result of every test. Most
testing approaches used in practice or described in the testing liter-
ature can be described using TCandOC. However, it is possible to
deﬁne adequacy of the testing process in terms of both the test set
and the test oracle used, i.e., deﬁne adequacy as a pairing of a test
set and an oracle. We deﬁne a complete adequacy criterion as the
following predicate:
TOC⊆P×S×2T×O
For example, a stateful program responsible for mode switching
in an avionics systems may be best combined with a test suite pro-
viding MCDC coverage and an oracle observing a majority of the
internal state variables in addition to all outputs. In Section 5, we
will explore an existing example of a complete adequacy criterion.
4. ORACLE COMPARISONS
By extending Gourlay’s framework with a set Oof oracles, we
have introduced the problem of oracle selection: the problem of
selecting an oracle ofrom a set of possible oracles. Just as with
the problem of test selection, we desire some method of estimating
the relative usefulness of oracles. Unfortunately, we are unaware of
any comparison relations speciﬁc to oracles (though mutation test-
ing represents a method of comparing combinations of test inputs
and test oracles; see Section 5.2). To facilitate such comparisons,
we present several possible oracle comparison relations, based on
the test coverage criteria comparison relations explored by several
authors [17, 41] and discussed in this paper in Section 5.1.
Our oracle comparisons, like test coverage criteria comparisons,
are based on the ability of the oracles to detect faults. Recall that
an oracle is complete with respect to pandsif for all tests t,
corr t(t,p,s ) =⇒o(t,p)
in other words, when a fault is detected by o, the fault is real, i.e.,
it represents an error in the program. As most oracles discussedin testing research are designed to be complete, the oracle compar-
isons we present assume complete oracles. Comparisons between
non-complete oracles would require a different approach account-
ing for oracles signaling faults when none have occurred.
4.1 Power Comparison
Our ﬁrst relation is based on Gourlay’s deﬁnition of power [17].
We state an oracle o1has a power greater than or equal to oracle
o2with respect to a test set TS(writteno1≥TSo2) for program p
and speciﬁcation sif:
∀t∈TS,o 1(t,p)⇒o2(t,p)
In other words, if o1fails to detect a fault for some test, then so
doeso2. Ifo1≥TSo2, it is possible that o1ando2are equally
powerful, i.e.,
∀t∈TS,o 1(t,p)⇔o2(t,p).
We may wish to state that some oracle o1is strictly better than
an oracleo2. We state that o1is more powerful than o2for test set
TS(o1>TSo2) if:
∀t∈TS,o 1(t,p)⇒o2(t,p)∧
∃t/prime∈TS,¬o1(t/prime,p)∧o2(t/prime,p)
In other words, o1≥TSo2and for some test t∈TS,o1detects a
fault whereo2fails to detect a fault.
0: var: x : int = 0
1: var: y : int = 0
2: if input() = true:
3: x = 1
4: else:
5: y = 1
Figure 2: Oracle Comparison Example Program
Note that power is relative to a ﬁxed test set TS. Given different
test sets, the relative power of oracles may vary. Consider the sam-
ple program pin Figure 2. Consider two oracles, oxandoy, with
both oracles being simple input/output oracles, and with oxhaving
oracle dataxandoyhaving oracle data y. Consider two test sets
TtandTf, each with exactly one test, such that Ttsetsinput ()to
true andTfsetsinput ()to false. Assume both lines 3 and 5 are
incorrect (e.g., wrong constant is assigned). Then:
ox>Ttoy
oy>Tfox
For some pairs of of oracles, it may be the case that:
∀TS⊆2T,o1≥TSo2
In other words, o1has power greater than or equal to o2for all
possible test sets TS. In such a case we state that oracle o1has
power universally greater than or equal to oracle o2(writteno1≥
o2). For example, consider an oracle oadeﬁned in terms of a set
of assertions A, where ¬oa(t,p)indicates that test tviolates an
assertiona∈A. LetA/primebe an additional set of assertions, and
let oracleoa2be an oracle deﬁned in terms of a set of assertions
A∪A/prime. As the set of assertions used by oa2is a superset of the
set of assertions used by oa, for any test set TS,oa2≥TSoa
and thusoa2≥oa. A similar situation occurs when an oracle o1
is observing a superset of the oracle data observed by an oracle
o2, for example, o2observes the outputs from the program and o1
observes additional internal state information. Given that both o1
ando2are complete, o1≥o2.3954.2 Probabilistic Comparison
The power relation is a fairly restrictive relation between oracles:
ifo1≥TSo2, then not only does o1detect more faults, it must de-
tect every fault detected by o2. While this relationship will often
hold for oracles constructed using the same basic principle (e.g.,
sets of assertions), we desire a method of comparing the effective-
ness of alloracles, i.e., a total comparison relation.
Weyuker et al. recognized this problem with respect to test cov-
erage criteria and have deﬁned a more useful probabilistic com-
parison between test criteria called PROBBETTER [41]. (We will
hereafter refer to PROBBETTER asPB.) A criterion C1isPBthan
C2with respect to program pand speciﬁcation sif a randomly se-
lected test set satisfying C1is probabilistically more likely to detect
a failure inpthan a randomly selected test set satisfying C2(writ-
ten asC1PBC 2). A ‘randomly selected test set’ refers to a test set
drawn from the set of all possible test sets satisfying a criterion C.
As most criteria are monotonic, the number of test sets satisfying C
is often very large or inﬁnite [38]. Consequently, it can be difﬁcult
toprove thatC1isPBthanC2; nevertheless, empirical studies of
test coverage criteria effectiveness can be used to approximate this
relationship (indeed, this is arguably one of the primary contribu-
tions of such studies), thus, rendering this criterion comparison and
other similar probabilistic comparisons useful.
We base our total oracle comparison on the Weyuker et al. PB
relation. We state an oracle o1isPBthan oracleo2with respect to
a test setTS
o1PBTSo2
for program pif for a randomly selected test t∈TS,o1is more
likely to detect a fault than o2. We stateo1is universally PBthan
o2ifo1PBTo2, whereTis the entire set of tests that can be
run againstp. (This is conceptually different from the deﬁnition of
universally greater power outlined above.)
We can show power is a strictly stronger relation than PBwhen
applied to oracles, i.e., for test set TSand program p,
o1>TSo2⇒o1PBTSo2.
Assume we have oracle o1ando2such that for test set TSand
programp,o1>TSo2. For any test t∈TS, one of the following
is true:
o1(t,p)∧o2(t,p) (1)
¬o1(t,p)∧ ¬o2(t,p) (2)
¬o1(t,p)∧o2(t,p) (3)
In other words, for all t∈TSit must be true that (1) neither
oracle detects a fault, (2) both oracles detect a fault or (3) o1detects
a fault while o2does not. Based on the deﬁnition of oracle power,
it cannot be the case that o2detects a fault if o1does not detect a
fault. Clearly, given an randomly selected t∈TS,o1is at least as
likely to detect a fault as o2. Furthermore, we know there exists at
least onet∈TSsuch that ¬o1(t,p)∧o2(t,p)(note the use >TS)
and thus for at least one t∈TS,o1is detects a fault that o2does
not. Therefore o1PBTSo2.
4.3 Oracle Metrics
Arguably, one of the core contributions of testing research is
evaluating how testing approaches relate to one another. Unsurpris-
ingly, then, a number of metrics have been proposed for discussing
the set of programs Pand the set of tests T, including software
testability [33], various test coverage criteria, and the test coverage
criteria comparison relations of power, PROBBETTER , subsumes,
etc. [41].However, we are unaware of any metrics speciﬁc to test oracles.
In this section, we have proposed two basic oracle comparison met-
rics, and have shown that the more restrictive (but non-total) com-
parison, power , implies the less powerful, but total comparison,
PB. These metrics allow us to compare oracles, and highlight a po-
tential (albeit in retrospect rather obvious) avenue for research into
oracles—analytically and empirically comparing different oracles,
as is commonly done for test coverage criteria. Future work in or-
acles may yield more metrics not explicitly based on fault ﬁnding
ability. In the remainder of the paper, we will explore how our ex-
tended framework explicitly considering test oracles inﬂuences the
testing process.
5. APPLICATIONS TO PREVIOUS WORK
In this section, we revisit some earlier inﬂuential work in test-
ing, exploring how explicitly considering a test oracle affects the
results. We also explore how existing research can be used to dis-
cuss problems related to test oracles.
5.1 Comparing Coverage Criteria
A signiﬁcant portion of the theoretical and empirical testing re-
search is concerned with methods of comparing coverage criteria.
While several methods have been proposed, they implicitly assume
the presence of an oracle. This can lead to conclusions relying on
key assumptions that are either unstated or minimally discussed.
If we instead consider that the oracle may vary, we can arrive at
conclusions that are different from published results in subtle, but
important ways.
5.1.1 Power Comparison
We ﬁrst illustrate why oracles are relevant using the power rela-
tion, ﬁrst proposed by Gourlay for test methods and subsequently
adjusted by Weiss [36] for use with test coverage criteria. Weiss
states a criterion C1is at least as powerful as C2, written asC1≥
C2, if for any program pand speciﬁcation s, if all test sets satisfy-
ingC2exposes an error in pthen so do all test sets satisfying C1.
Note here that the deﬁnition requires alltest sets satisfying the cri-
terion reveal the fault—an unlikely occurrence in practice. Weiss’s
discussion completely omits the notion of an oracle; we assume a
constant complete oracle ois used.
We restate the deﬁnition of the power of a test coverage criterion
using our framework. A criterion C1is at least as powerful as a
criterionC2with respect to a complete oracle o(writtenC1≥oC2)
if:
∀p∈P,s∈S,T 1∈C1,T2∈C2:
(∃t2∈T2¬o(p,t2)⇒ ∃t1∈T1¬o(p,t1))
In other words, if all test sets satisfying C2are guaranteed to ﬁnd
a fault forpwhen using oracle o, then so are all test sets satisfying
C1. This formulation makes the role of the oracle explicit—the
relative power of a test coverage criterion is deﬁned with respect to
a constant oracle.
It is easy to show that the oracle is relevant in the power relation.
Consider the statement ( ST) and branch ( BR) coverage criteria.
As subsumption between criteria implies power [41], we know that
BR≥ST. Generally speaking, this is a vacuous relationship,
as neither coverage is guaranteed to ﬁnd faults for most programs
p. Nevertheless, assume there exists a program pwhich has some
faultfrevealed by every test set satisfying statement coverage (and
thus every test set satisfying branch coverage), but does not always
propagate to the output (e.g., an incorrect constant is used).
Letooutbe an input/expected value oracle considering only the
outputs, and let oallbe an input/expected value oracle consider-396ing both the outputs and the internal variables. If a test set TBR
satisfying branch coverage is paired with oout, and a test set TST
satisfying statement coverage is paired with oall, thenTST,oallis
guaranteed to detect f, butTBR,ooutis not. Thus, the power re-
lation for test coverage criteria requires the same oracle to be used
with both coverage criteria.
5.1.2 Probabilistic Comparison
The power relation between test coverage criteria is known to
be vacuous for most criteria and is thus of limited value [41]. We
extend the Weyuker et al. PBrelation [41] to consider oracles ex-
plicitly using the same notation and terminology used for power
above.
0: var: temp : int = 0
1: var: out : int = 0
2: forever:
3: out = temp
4: temp = input() *2
Figure 3: PROBBETTER Example Program
To demonstrate the effect of oracles on PB, consider the follow-
ing example. Let pbe the program in Figure 3, and let the spec-
iﬁcationsstate that outat iterationishould be equal to the input
at iterationi−1, or 0 ifi= 0. Assume the number of possible
inputs is bounded at 100, with a range of -49 to 50. Let TL1alland
TL2sinbe coverage criteria such that for a test set TS,TL1allis
satisﬁed ifTScontains every test of length 1 and no other tests,
andTL2sinis satisﬁed if TScontains exactly one test of length
2. (We deﬁne length as the number of loop iterations.) Let oout
be an oracle with oracle data out, and letoallbe an oracle with
oracle data outandtemp. Both oracles are input/expected value or-
acles, signaling a fault when the value of a variable is incorrect. out
is considered incorrect when sis violated and temp is considered
incorrect when the value is not equal to the prior input.
pcontains a fault, as line 4 should not double the input. Con-
sequently, the value of outat iterationiis only equal to the input
at iterationi−1if the input at iteration i−1was 0. We make
two observations about detecting this fault. First, to detect the fault
we must use an input other than 0, as psatisﬁessfor 0. Second,
the fault requires at least least two inputs to reach the output, and
thus outwill be correct for all tests of length one. Consequently,
no test set satisfying TL1allwill detect the fault using oracle oout,
while all test sets satisfying TL1allwill detect the fault using oall.
Furthermore, most test sets satisfying TL2sinwill detect the fault
using either oracle; when using oout, only tests in which the ﬁrst
input is 0 will fail to detect the fault, while when using oallonly
the test in which both inputs are 0 will fail to detect the fault.
LetProb (C,O )be the probability of detecting a fault when us-
ing oracleOand a randomly selected test set satisfying criteria C.
We can state:
Prob (TL1all,oout) = 0.0
Prob (TL1all,oall) = 1.0
Prob (TL2sin,oout) = 0.99
Prob (TL2sin,oall) = 0.9999
Thus, for program pand speciﬁcation s:
TL2sinPBooutTL1all
TL1allPBoallTL2sin5.1.3 Implications
These results highlight the relationship between oracles, tests,
and programs on the efﬁcacy of the testing process. The relation-
ship between oracles and tests can easily be seen from these results.
Both the power andPBrelation were deﬁned to compare the efﬁ-
cacy of the test coverage criteria (with respect to a program and
speciﬁcation in the case of PB). However, it is clear that oracles
cannot be ignored when discussing test selection; to do so may
yield misleading or incorrect conclusions.
While less obvious, these results also highlight the relationship
between oracles and programs. The construction of the program in
Figure 3 is such that the error on line 4 produces incorrect internal
state for 99% of the inputs, but cannot affect the output unless a
test length of at least 2 is used. If instead the error had occurred
on line 3 (i.e., outwas doubled and temp was not), the program
would be semantically equivalent in terms of input/output behav-
ior, but PBwould be unaffected by the oracle used. That such a
subtle change can completely negate the beneﬁt of using a more
powerful oracle indicates that as with test selection and oracles, we
cannot ignore program characteristics when discussing oracle se-
lection. We further explore the relationship between programs and
test oracles later in this section.
5.2 Mutation Testing
Mutation testing is a test selection method based on selecting a
set of tests to detect small (usually syntactic) changes in the pro-
gram [11]. Brieﬂy, to select a set of tests satisfying mutation cover-
age for a program p, we ﬁrst produce a set of mutants Mthat differ
frompin small ways (e.g., change arithmetic operators, swap vari-
able names, etc.). We then select a set of tests Tsuch that each
semantically different mutant m∈Mis distinguished from p.
Several types of mutation testing have been proposed. In strong
mutation testing [11], we must ﬁnd a set of tests Tsuch that ∀m∈
M,∃t∈T, p (t)/negationslash=m(t), i.e., the output of each faulty pro-
grammdiffers from p’s output for some test t. In weak mutation
testing [21], we need only ﬁnd a set of tests Tsuch that for each
m∈M, the internal state of the pandmdiffers for some test t.
In [43], Woodward and Haywood note that mutation testing exists
on a spectrum, with strong and weak mutation on opposite ends of
the spectrum.
This spectrum of approaches is primarily differentiated by the
method used to determine if a mutant has been detected. Recall
that in Section 3.2 we deﬁned a complete adequacy criterion to
be an adequacy criterion deﬁned in terms of both the test set and
the oracle used. If we view the method used to distinguish the
mutantsMfrom the program pas an oracle, we can reformulate
the spectrum of mutation testing approaches as a single, complete
adequacy criterion.
For the set of mutants M, mutation adequacy Mut Mis satisﬁed
for program p, speciﬁcation s, test setTS, and oracleoif:
Mut M(p×s×TS×o)⇒ ∀m∈M,∃t∈TS:¬o(t,m)
In other words, for each mutant m∈M, there exists a test tsuch
that the oracle osignals a fault.
This formulation of mutation testing differs slightly from the
usual approaches to mutation testing, as the oracle is part of the
actual testing process, whereas generally the method used to dis-
tinguishMfrompis only used to select tests. Nevertheless, this
formulation captures the core of mutation testing—constructing a
testing process that is guaranteed to detect a set of pre-speciﬁed
faults—without focusing on how the faults are detected. A very
strong oracle can be used with a small number of simple tests; con-
versely, a weak output-only oracle can be used with a tests that397ensure the mutant faults propagate to the output.
The relationship between the program being tested, the tests se-
lected, and the oracle used is clear in this formulation of mutation
testing. From the program p, a set of mutants Mare generated.
Using the set of mutants, an oracle oand a set of tests TSare se-
lected such that each mutant is detected. If we change one testing
factor, the other factors must also change accordingly to satisfy the
criterion—a different program yields different mutants, thus requir-
ing different tests and/or a different oracle; a weaker oracle may
require more or different tests; simpler tests may require a more
powerful oracle. We believe the close relationship between fac-
tors in mutation testing to be worth considering—mutation testing
is based on detecting faults, and detecting faults is the goal of any
testing process. Insights related to mutation testing seem likely to
hold in many testing processes.
5.3 Testability
The testability of a software system, as deﬁned by Voas et al.,
is the probability that the system will fail if faulty [34]. Generally,
methods of computing software testability estimate the probability
of a fault in a speciﬁc program location (e.g., a statement) propa-
gating to the output, usually with respect to an input distribution
or speciﬁc input. By computing the testability of each program
location, it is argued, we can focus testing resources on program
locations that have low probabilities of propagating errors. As a
representative technique, we explore only work led by Voas related
to the PIE (Propagation, Infection, and Execution) method [33, 34,
35].
Voas et al. deﬁne several testability metrics [33]. Consider the
propagation estimate metric, denoted ψl,a. The propagation es-
timate is the estimated probability that a perturbed value of aat
locationlwill affect the output. In practice, measuring testability
is about estimating failure probabilities, and ψl,ais therefore used
to estimate the probability that a, if incorrect, will cause the pro-
gram to fail. Consequently, ψl,aonly makes sense if we assume the
presence of an oracle deﬁned in terms of the outputs. If we instead
consider that the oracle may not be deﬁned in terms of the outputs,
the propagation estimate above becomes less informative—we are
not interested in the probability of a fault propagating to anyout-
put, we are interested in the probability of a fault being detectable
by the oracle used.
To account for the oracle, we redeﬁne propagation estimate with
respect to an oracle o, denoting it ψl,a,o. This redeﬁned propaga-
tion estimate is the estimated probability that a perturbed value of a
at locationlwill affect a variable in the oracle data of o. This metric
thus estimates the probability that a fault at awill be detectable by
oracleo. We can show that given an arbitrary o∈O,ψl,a,o is not
necessarily equal to ψl,a. Suppose we have a program p, a set of
testst, and three oracles, ooandovandoa. Letoobe an oracle with
oracle data containing every output and no other variables (i.e., the
oracle assumed by ψl,a), letovbe an oracle considering the single
internal variable v, and letoabe an oracle considering the single
variablea. Let 1.0> ψ l,a,o o>0.0and letabe some variable
deﬁned after the last assignment of v(thusacannot propagate to
v). Therefore:
1.0> ψ l,a,o o>0.0
ψl,a,o v= 0.0
ψl,a,o a= 1.0
ψl,a,o a> ψ l,a,o o>ψ l,a,o v
ψl,a,o a> ψ l,a>ψ l,a,o v
We can see that in order to accurately use testability information toguide software testing, we must account for the oracle used. If we
do not, we may select tests likely to propagate errors to variables in
which we are not interested, or we may direct resources to increase
testing of parts of the program unlikely to propagate errors to the
output, ignoring the fact that these parts of the program may already
be covered by the oracle data.
5.3.1 Effect on Oracle Selection
Software testability is often proposed as a method of directing
test selection; by determining which parts of the program are and
are not likely to hide faults, we can select tests proportionally.
However, software testability can also be used to guide oracle se-
lection.
Consider the previous example, assume the variable ais unlikely
to propagate to the output. If we wish to improve fault ﬁnding, we
can select tests aimed at improving the probability of apropagat-
ing to the output, orwe can use a stronger oracle with oracle data
containing a variable to which an error in ais likely to propagate.
This leads to the observation that variables with low propagation
estimates represent opportunities for increasing the oracle power .
It then naturally follows that if all variables in the program have
high propagation estimates, increasing the oracle data is unlikely
to signiﬁcantly improve the oracle power . Note here that the former
observation has been alluded to by Voas and Miller, who proposed
using testability to guide the creation of assertions [35].
5.3.2 Implications
Like mutation testing, testability metrics highlight the close in-
terrelationship between programs, test sets, and oracles. Certain
faults may be difﬁcult to uncover in a program pthrough testing.
By computing the testability of p, we can determine where these
faults are likely to hide, and then direct testing resources—both in
terms of tests and oracles—to ﬁnding them. Voas suggests adding
tests to better exercise parts of the code likely to hide faults [33],
thus using testability information to improve the testing process.
As noted above (and by Voas [35]) we can also use testability in-
formation to select better oracles. Clearly, doing both may be un-
necessary; if we use testability information to select a better oracle
and thus increase the testability, we may no longer need additional
tests. Similarly, given a large number of tests compensating for
a low propagation estimate, selecting a better oracle may provide
little improvement.
6. ADDITIONAL RELATED WORK
We have discussed several related works throughout this paper.
In this section, we discuss additional related works, with a focus
on work in the theory of testing. To the best of our knowledge, our
work is the only foundational work focused on how test oracles in-
ﬂuence the testing process, though Weyuker has an interesting dis-
cussion on practical difﬁculties encountered in testing practice [37].
Goodenough and Gerhart’s seminal work outlines a theory of
testing based on test data selection [16]. Weyuker et al. [40], Budd
and Angluin [9], and Gourlay [17] subsequently highlight problems
in this theory; nevertheless, Goodenough and Gerhart’s ideal of a
testing criterion capable of ﬁnding all faults in a program captures
the general goal of test selection and subsequent theories of pro-
gram testing generally focus on methods of test selection. Gourlay
presents a mathematical framework for testing [17], and uses it to
re-interpret previous work by Goodenough and Gerhart [16], How-
den [20], Gellar [15], and Weyuker [40].
There exists a large body of formal and semi-formal work on
the speciﬁc problem of test selection criteria. Weyuker et al. pro-
poses a set of axioms for test data adequacy criteria that charac-398terize “good” test data adequacy criteria [38]. This idea is further
discussed by Zhu and Hall [44, 45], by Parrish and Zweben [26,
27], and by Weyuker again [39]. Formal analysis of methods of
comparing test selection criteria has been performed by many au-
thors, including Weyuker, Weiss and Hamlet [41], and by Frankl
and Weyuker (speciﬁcally for partition testing methods) [12]. Hi-
erons illustrates how the presence of test hypotheses and fault do-
mains inﬂuence comparison of test selection criteria [19].
Several theories of program testing based on test selection exist.
Morell introduces a theory of fault-based testing in which the goal
of testing is to show the absence of a certain set of faults [25]. Ham-
let presents an outline of a theory of software dependability arguing
that the foundations of software testing should be statistical in na-
ture [18]. Zhu and He propose a theory of behavior observation for
testing concurrent software systems [47].
Bernot, Gaudel and other authors have developed a theory of
testing based on formal speciﬁcations [14, 4, 5]. In this work, they
deﬁne a testing context as a triple (H,T,O )whereHis a set of
testing hypotheses (i.e., assumptions) about the program and spec-
iﬁcation,Tis a set of tests, and Ois a test oracle. This work uses
algebraic speciﬁcations as the basis of testing; tests are created
based on the axioms of the algebra that deﬁne the program inter-
face. Gaudel notes that it is not sufﬁcient to provide a statement
of correctness (speciﬁcation) and program, as it is often the case
that certain parts of the program necessary to ascertain correctness
are not observable, e.g., “opaque” type equality and internal com-
ponent state [14]. The oracle deﬁnes the often imperfect mapping
between the speciﬁcation of correctness and what it is possible to
observe about the outcome of a test. The authors also discuss the
relationship between testing hypotheses and test selection criteria,
and the need for test oracles. Note that this body of work is in terms
of algebraic sorts and is not applicable to most real-world testing
situations; furthermore, the formalization is quite terse.
7. CONCLUDING REMARKS
In this paper, we have aimed to provide a foundation for software
testing research—in particular empirical testing research—that is
better suited for the task than previous attempts. In particular, we
include the notion of test oracles in the framework and point out the
crucial interrelationship between tests, programs, and oracles. To
accomplish our goals, we extended Gourlay’s well known frame-
work to account for oracles, allowing us to discuss the problem of
oracle selection and explore oracle properties. We then continued
to reexamine previous work in testing, demonstrating the effect of
explicitly considering oracles. It is worth reiterating that our goal is
not yet another formalization of software testing; our goal is to pro-
vide solid foundation for the future empirical exploration of soft-
ware testing. Given our results, we make two recommendations
related to future testing research directions.
First, in both theoretical and empirical testing research we must
acknowledge all factors inﬂuencing the efﬁcacy of a testing ap-
proach; we must state and defend our all relevant assumptions . We
are often interested in exploring only one or two aspects of the test-
ing process. Nevertheless, we must be aware that our results are
inﬂuenced by other factors, for example, the choice of oracle. In
some cases, simply making our assumptions explicit is sufﬁcient.
This was the case in the deﬁnition of the PROBBETTER relation
for test coverage criteria for example—the results were sound, but
only with respect to a speciﬁc oracle.
In other cases, we must argue that our assumptions are reason-
able. This is particularly important in empirical testing research
since it is labor intensive and time consuming and, thus, is gener-
ally only capable of thoroughly exploring one factor at the time,holding other factors constant. Authors must explicitly chose con-
stant factors, argue why they were chosen and discuss the level of
generalizablity resulting from these choices. For example, in [10]
Chen et al. explore the effect of fault exposure estimates on the ef-
ﬁcacy of the testing process. This is a exceptionally well conducted
study; nevertheless, the results are dependent on the tests used (cat-
egory partitioning, with manually generated tests), the oracle used
(presumably output-only), and the characteristics of the programs
studied (the popular Siemens programs [22]). By placing a greater
emphasis on these factors, the ability of other researchers to in-
terpret and apply the results could be greatly improved—ignoring
or leaving assumptions unstated or implicit makes interpreting and
comparing empirical studies impossible and hinders progress in the
ﬁeld.
This leads to our next recommendation; greater emphasis should
be placed how combinations of factors inﬂuence the testing pro-
cess. Some work in this area, discussed previously, has been done.
Nevertheless, we strongly believe that more work exploring how
combinations of factors related to the program under test, the tests
themselves, and the oracle used inﬂuence the testing process is nec-
essary to better understand and make improvements in the efﬁcacy
of the testing process. For example, questions such as how to mate
a coverage criterion with a suitable oracle and what coverage crite-
rion suits a particular program structure have not yet been system-
atically addressed.
8. ACKNOWLEDGMENTS
The authors would like to thank the anonymous ICSE referees
and Elaine Weyuker for their comments on this work.
9. REFERENCES
[1]Z. Al-Khanjari, M. Woodward, and H. Ramadhan. Critical
analysis of the pie testability technique. Software Quality
Journal , 10(4):331–354, 2002.
[2]A. Avizienis, J.-C. Laprie, B. Randell, and C. Landwehr.
Basic Concepts and Taxonomy of Dependable and Secure
Computing. IEEE Trans. Dependable and Secure
Computing , 1(1):11–33, 2004.
[3]B. Beizer. Software Testing Techniques, 2nd Edition . Van
Nostrand Reinhold, New York, 1990.
[4]G. Bernot. Testing against formal speciﬁcations: A
theoretical view. In TAPSOFT’91: Colloquium on Trees in
Algebra and Programming (CAAP’91) , page 99. Springer,
1991.
[5]G. Bernot, M. Gaudel, B. Marre, and U. Liens. Software
testing based on formal speciﬁcations: a theory and a tool.
Software Engineering Journal , 6(6):387–405, 1991.
[6]A. Bertolino. Software testing research: Achievements,
challenges, dreams. In L. Briand and A. Wolf, editors, Future
of Software Engineering 2007 . IEEE-CS Press, 2007.
[7]L. Briand. A critical analysis of empirical research in
software testing. In Empirical Software Engineering and
Measurement, 2007. ESEM 2007. First Int’l Symposium on ,
pages 1–8, 2007.
[8]L. Briand, M. DiPenta, and Y. Labiche. Assessing and
improving state-based class testing: A series of experiments.
IEEE Trans. on Software Engineering , 30 (11), 2004.
[9]T. Budd and D. Angluin. Two notions of correctness and
their relation to testing. Acta Informatica , 18(1):31–45, 1982.
[10] W. Chen, R. Untch, G. Rothermel, S. Elbaum, and
J. Von Ronne. Can fault-exposure-potential estimates399improve the fault detection abilities of test suites? Software
Testing Veriﬁcation and Reliability , 12(4):197–218, 2002.
[11] R. DeMillo, R. Lipton, and F. Sayward. Hints on test data
selection: Help for the practicing programmer. IEEE
computer , 11(4):34–41, 1978.
[12] P. Frankl and E. Weyuker. A formal analysis of the
fault-detecting ability of testing methods. In IEEE Trans. on
Software Engineering , 1993.
[13] A. Gargantini and C. Heitmeyer. Using model checking to
generate tests from requirements speciﬁcations. Software
Engineering Notes , 24(6):146–162, November 1999.
[14] M. Gaudel. Testing can be formal, too. Lecture Notes in
Computer Science , 915:82–96, 1995.
[15] M. Geller. Test data as an aid in proving program
correctness. In Proc. of the 3rd ACM SIGACT-SIGPLAN
Symp. on Principles on Programming Languages , pages
209–218. ACM New York, NY, USA, 1976.
[16] J. B. Goodenough and S. L. Gerhart. Toward a theory of
testing: Data selection criteria. In R. T. Yeh, editor, Current
trends in programming methodology . Prentice Hall, 1979.
[17] J. Gourlay. A mathematical framework for the investigation
of testing. IEEE Trans. on Software Engineering , pages
686–709, 1983.
[18] D. Hamlet. Foundations of software testing: dependability
theory. ACM SIGSOFT Software Engineering Notes ,
19(5):128–139, 1994.
[19] R. Hierons. Comparing test sets and criteria in the presence
of test hypotheses and fault domains. ACM Transactions on
Software Engineering and Methodology (TOSEM) ,
11(4):448, 2002.
[20] W. Howden. Reliability of the path analysis testing strategy.
IEEE Transactions on Software Engineering , 2(3), 1976.
[21] W. Howden. Weak mutation testing and completeness of test
sets. IEEE Trans. on Software Engineering , pages 371–379,
1982.
[22] M. Hutchins, H. Foster, T. Goradia, and T. Ostrand.
Experiments of the effectiveness of dataﬂow-and
controlﬂow-based test adequacy criteria. In Proc. of the 16th
Int’l Conference on Software Engineering , pages 191–200.
IEEE Computer Society Press Los Alamitos, CA, USA,
1994.
[23] J. Mayer and R. Guderlei. Test oracles using statistical
methods. In Proc. of the First Int’l Workshop on Software
Quality , pages 179–189. Citeseer, 2004.
[24] A. Memon, I. Banerjee, and A. Nagarajan. What test oracle
should I use for effective GUI testing? Automated Software
Engineering, 2003. Proc. 18th IEEE Int’l Conf. on , pages
164–173, 2003.
[25] L. Morell. A theory of fault-based testing. IEEE Transactions
on Software Engineering , 16(8):844–857, 1990.
[26] A. Parrish and S. Zweben. Analysis and reﬁnement of
software test data adequacy properties. IEEE Trans. on
Software Engineering , 17(6):565–581, 1991.
[27] A. Parrish and S. Zweben. Clarifying some fundamental
concepts in software testing. IEEE Trans. on Software
Engineering , 19(7):742–746, 1993.
[28] A. Rajan, M. Whalen, and M. Heimdahl. The effect of
program and model structure on MC/DC test adequacy
coverage. In Proc. of the 30th Int’l Conference on Software
engineering , pages 161–170. ACM New York, NY, USA,
2008.[29] S. Rayadurgam and M. P. Heimdahl. Coverage based
test-case generation using model checkers. In Proc. of the 8th
IEEE Int’l. Conf. and Workshop on the Engineering of
Computer Based Systems , pages 83–91. IEEE Computer
Society, April 2001.
[30] D. J. Richardson, S. L. Aha, and T. O’Malley.
Speciﬁcation-based test oracles for reactive systems. In Proc.
of the 14th Int’l Conference on Software Engineering , pages
105–118. Springer, May 1992.
[31] G. Rothermel, M. J. Harrold, J. Ostrin, and C. Hong. An
empirical study of the effects of minimization on the fault
detection capabilities of test suites. In Proceedings of the
International Conference on Software Maintenance , pages
34–43, November 1998.
[32] M. Staats, M. Whalen, and M. Heimdahl. Better testing
through oracle selection (NIER track). In Proc. of the Int’l
Conf. on Software Engineering 2011 , 2011.
[33] J. Voas. Dynamic testing complexity metric. Software
Quality Journal , 1(2):101–114, 1992.
[34] J. Voas. PIE: A dynamic failure-based technique. IEEE
Trans. on Software Engineering , 18(8):717–727, 1992.
[35] J. Voas and K. Miller. Putting assertions in their place. In
Software Reliability Engineering, 1994., 5th Int’l Symposium
on, pages 152–157, 1994.
[36] S. Weiss. Comparing test data adequacy criteria. ACM
SIGSOFT Software Engineering Notes , 14(6):42–49, 1989.
[37] E. Weyuker. On testing non-testable programs. The
Computer Journal , 25(4):465, 1982.
[38] E. Weyuker. Axiomatizing software test data adequacy. IEEE
Trans. on Software Engineering , 12(12):1128–1138, 1986.
[39] E. Weyuker. The evaluation of program-based software test
data adequacy criteria. Communications of the ACM ,
31(6):668–675, 1988.
[40] E. Weyuker and T. Ostrand. Theories of program testing and
the application of revealing subdomains. IEEE Trans. on
Software Engineering , pages 236–246, 1980.
[41] E. Weyuker, S. Weiss, and D. Hamlet. Comparison of
program testing strategies. In Proc. of the Symposium on
Testing, Analysis, and Veriﬁcation , page 10. ACM, 1991.
[42] W. Wong, J. Horgan, S. London, and A. Mathur. Effect of
test set minimization on fault detection effectiveness. In
Proc. of the 17th Int’l Conf. on Software Engineering , pages
41–50. ACM, 1995.
[43] M. Woodward and K. Halewood. From weak to strong, dead
or alive? an analysis of some mutation testing issues. In
Software Testing, Veriﬁcation, and Analysis, 1988., Proc. of
the 2nd Workshop on , pages 152–158, 1988.
[44] H. Zhu. Axiomatic assessment of control ﬂow-based
software test adequacy criteria. Software Engineering
Journal , 10(5):194–204, 1995.
[45] H. Zhu and P. Hall. Test data adequacy measurement.
Software Engineering Journal , 8(1):21–29, 1993.
[46] H. Zhu, P. Hall, and J. R. May. Software unit test coverage
and adequacy. ACM Computing Surveys , 29(4):366–427,
December 1997.
[47] H. Zhu and X. He. A theory of behaviour observation in
software testing. Technical report, 1999.400