SimRT: An Automated Framework to Support
Regression Testing for Data Races
Tingting Yu, Witawas Srisa-an, and Gregg Rothermel
Department of Computer Science and Engineering
University of Nebraska-Lincoln, USA
{tyu,witty,grother}@cse.unl.edu
ABSTRACT
Concurrent programs are prone to various classes of difÔ¨Åcult-to-
detect faults, of which data races are particularly prevalent. Prior
work has attempted to increase the cost-effectiveness of approaches
for testing for data races by employing race detection techniques,
but to date, no work has considered cost-effective approaches for
re-testing for races as programs evolve. In this paper we present
SIMRT, an automated regression testing framework for use in de-
tecting races introduced by code modiÔ¨Åcations. S IMRT employs a
regression test selection technique, focused on sets of program ele-
ments related to race detection, to reduce the number of test cases
that must be run on a changed program to detect races that occur
due to code modiÔ¨Åcations, and it employs a test case prioritiza-
tion technique to improve the rate at which such races are detected.
Our empirical study of S IMRT reveals that it is more efÔ¨Åcient and
effective for revealing races than other approaches, and that its con-
stituent test selection and prioritization components each contribute
to its performance.
Categories and Subject Descriptors
D.2.5 [ Software Engineering ]: Testing and Debugging‚Äî Testing
tools, Tracing ; D.4.1 [Operating Systems]: Process Management
General Terms
Reliability, Experimentation
Keywords
Testing, Concurrency, Processes, Data Races, Kernels
1. INTRODUCTION
The advent of multicore processors has greatly increased the
prevalence of concurrent software. However, failures due to con-
currency faults still occur proliÔ¨Åcally in deployed concurrent sys-
tems [56, 58, 59]. There are several classes of concurrency faults,
including data races, atomicity violations, and deadlock. Among
these, data races occur particularly frequently. Research has shown,
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proÔ¨Åt or commercial advantage and that copies
bear this notice and the full citation on the Ô¨Årst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciÔ¨Åc
permission and/or a fee.
ICSE ‚Äô14, May 31 - June 7, 2014, Hyderabad, India
Copyright 14 ACM 978-1-4503-2756-5/14/05 ...$15.00.for example, that data races remain a primary cause of concurrency
faults in the Windows kernel [10]. Searches of bug repositories for
widely used software systems such as the Linux OS and Apache
Tomcat also reveal frequent occurrences of data races [22, 41].
In practice, software testing is the primary approach used by en-
gineers to detect concurrency faults [27]. In software testing, the
most typical approach for verifying test results involves checking
system outputs following test execution. However, data races do
not always produce failures that are visible in program outputs;
therefore, techniques that inspect outputs to detect failures can be
ineffective at detecting data races.
To address this problem, two general approaches have been used.
First, developers can apply dynamic analysis techniques to detect
potential races instead of waiting for the effect of races to propagate
to output [2, 40]. Unfortunately, many reported potential races are
false positives that cannot result in actual races. Furthermore, these
techniques monitor every shared memory access and synchroniza-
tion operation so they incur signiÔ¨Åcant runtime overhead. A second
group of techniques attempt to distinguish real races from potential
races [41, 42]. These techniques explore multiple thread schedul-
ings to determine whether a potential race can lead to a real race.
To do this, the techniques may require a program under test to be
executed several times under each test input in order to explore dif-
ferent thread interleavings.
When used with large test suites, the foregoing approaches can
render the testing process unduly expensive, due to the high over-
head of potential race detection and the costs of multiple program
executions in race veriÔ¨Åcation. Recent work [4] reports that each
of the two approaches can introduce a 10x-100x slowdown for each
test run. Such overhead increases as test suite size increases.
To address this problem, Yu et al. [54] propose M APLE , which
reduces the number of potential races that must be veriÔ¨Åed. Instead
of verifying every detected potential race with a test input, M APLE
avoids re-verifying potential races that have previously been ver-
iÔ¨Åed. This technique, however, still incurs a 10x-100x slowdown
because it does not reduce the cost of detecting potential races.
While the challenges for testing concurrent programs for data
races are extensive, an additional challenge arises as these pro-
grams evolve. As programs evolve they must be regression tested,
to assess whether changes have adversely affected system behav-
ior, and whether new code behaves as intended. Regression test-
ing can be expensive, and to render it more cost-effective, various
approaches have been suggested, including regression test selec-
tion and test case prioritization. However, most research on regres-
sion testing to date has focused on sequential software, and to our
knowledge, none has considered the issues involved in regression
testing for concurrency faults such as data races.
One approach for re-validating concurrent systems would be toPermission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proÔ¨Åt or commercial advantage and that copies bear this notice and the full citation
on the Ô¨Årst page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior speciÔ¨Åc permission and/or a
fee. Request permissions from Permissions@acm.org.
ICSE‚Äô14 , May 31 ‚Äì June 7, 2014, Hyderabad, India
Copyright 2014 ACM 978-1-4503-2756-5/14/05...$15.00
http://dx.doi.org/10.1145/2568225.2568294
48
apply dynamic race detection techniques to new versions of sys-
tems following the application of traditional regression testing tech-
niques [8, 36]. However, traditional regression testing techniques
may not be cost-effective in this context. For example, empirical
studies have shown that in certain cases, traditional regression test
selection techniques can select inordinately large numbers of test
cases, and yield no beneÔ¨Åt in reducing the overhead associated with
race detection [55].
To address this problem, in this work we propose S IMRT, an au-
tomated regression testing framework for use in detecting races that
are induced in concurrent programs by code modiÔ¨Åcations. S IMRT
identiÔ¨Åes variables that can be accessed by multiple threads in a
modiÔ¨Åed program, and that are impacted by modiÔ¨Åcations. S IMRT
then employs a regression test selection technique to select the test
cases from the program‚Äôs regression test suite that exercise these
shared variables in a manner that involves more than one thread. It
is these test cases that are speciÔ¨Åcally relevant to detecting races.
While regression test selection targeting shared variables helps
SIMRT reduce the cost of regression testing, the testing process
is still unnecessarily burdened by the cost of dynamic race detec-
tion approaches, because different inputs tend to repeatedly execute
the same memory locations and interleavings. Deng et al. [4] ob-
serve that up to 88% of test inputs always execute the same shared
memory locations and interleavings and thus do not increase race
detection rate as test cases execute. Thus, S IMRT next applies a
greedy test case prioritization algorithm to schedule the test cases
selected in its prior phase in an order that detects races faster.
To assess S IMRT, we conducted an empirical study in which we
applied the approach to modiÔ¨Åed versions of nine concurrent source
code objects, all of which contain data races that are the result of
code modiÔ¨Åcations. We assessed both the regression test selection
and test case prioritization components of our approach by compar-
ing S IMRT to traditional and baseline regression test selection and
test case prioritization techniques. Our results show that S IMRT
is more efÔ¨Åcient than a common baseline regression test selection
technique, and substantially more efÔ¨Åcient than the default tech-
nique of executing all test cases, while retaining the effectiveness
of those techniques. Our results also show that the prioritization
of test cases by S IMRT substantially increases the rate at which
races are detected with respect to a common baseline prioritization
technique, and a random ordering of test cases.
2. BACKGROUND
2.1 Regression Testing
LetPbe a program, let P0be a modiÔ¨Åed version of P, and
letTbe a test suite for P. Regression testing is concerned with
validating P0. To facilitate this, engineers often begin by reusing
T, but reusing all of T(theretest-all approach ) can be inordinately
expensive. Thus, a wide variety of approaches have been developed
for rendering reuse more cost-effective via regression test selection
and test case prioritization ([52] provides a recent survey).
Regression test selection (RTS) techniques select, from test suite
T, a subset T0that contains test cases that are important to re-
run. In our own prior work [36], we presented the RTS technique
DEJAVU . DEJAVU performs simultaneous depth-Ô¨Årst traversals on
control Ô¨Çow graphs (CFGs) for procedures in PandP0to Ô¨Ånd dan-
gerous edges that lead to code that has changed. Execution traces
of test cases (bit vectors indicating whether basic blocks were cov-
ered) on Pare then used to select test cases that traversed danger-
ous edges in P. We have shown [35] that when certain conditions
are met, D EJAVU issafe; i.e., it cannot omit test cases which, if ex-
ecuted on P0, would reveal faults in P0due to code modiÔ¨Åcations.Test case prioritization (TCP) techniques reorder the test cases
inTsuch that testing objectives can be met more quickly, and
one potential objective involves revealing faults. A wide range of
TCP techniques have been proposed and studied, and one technique
that has proven successful is the ‚Äúadditional-block-coverage‚Äù tech-
nique [37]. Given the results of executing tests and gathering trace
information, this technique prioritizes test cases in terms of the
numbers of new (not-yet-covered) basic blocks they cover, by iter-
atively selecting the test case that covers the most not-yet-covered
blocks until all blocks are covered, then repeating this process until
all blocks have been covered.
Because TCP techniques do not themselves discard test cases,
they can avoid the drawbacks that can occur when regression test
selection cannot achieve safety. Alternatively, in cases where dis-
carding test cases is acceptable, test case prioritization can be used
in conjunction with regression test selection to prioritize the test
cases in the selected test suite. Further, test case prioritization can
increase the likelihood that, if regression testing activities are un-
expectedly terminated, testing time will have been spent more ben-
eÔ¨Åcially than if test cases were not prioritized.
A key insight behind the techniques just described is that certain
testing-related tasks (such as gathering code coverage data) can be
performed in the ‚Äúpreliminary period‚Äù of testing, before changes to
a new version are complete. The information derived from these
tasks can then be used during the ‚Äúcritical period‚Äù of testing after
changes are complete and when time is more limited.
2.2 Race Detection and VeriÔ¨Åcation
A data race occurs when two threads can simultaneously access
a shared variable, with at least one access being a write [40]. Many
static [9, 48] and dynamic [11, 29] analysis techniques have been
developed to detect data races. In this work, we consider dynamic
race detection techniques.
Algorithms to dynamically detect data races can be classiÔ¨Åed as
lock-set [40, 60], vector-clock [2, 11], and hybrid approaches [29,
42]. These algorithms can report false positives, because they can-
not guarantee that a race can really occur under speciÔ¨Åc thread
interleavings. We refer to the races reported by these algorithms
aspotential races . Techniques have also been proposed to deter-
mine whether potential races identiÔ¨Åed by a detector can actually
occur. We refer to races that have been veriÔ¨Åed in this way as real
races . However, even race veriÔ¨Åers cannot differentiate harmful
races from benign races; they report any pair of unsynchronized
accesses (at least one of which is a write) as a race.
RACEFUZZER [41] is a race detection tool that combines dy-
namic race detection and race veriÔ¨Åcation. R ACEFUZZER com-
putes pairs of program instructions that could potentially race dur-
ing concurrent execution. It then randomly schedules the program
under test based on the computed instruction pairs to allow real
racing events to be placed temporally next to each other.
Employing random schedules for race veriÔ¨Åcation is precise and-
cost effective, but can be incomplete [41, 58]. Random scheduling
does not guarantee that veriÔ¨Åcation can distinguish all real races
from potential races; rather, it guarantees that if it can cause a race
to occur, this potential race will become a real race. On the other
hand, complete replay techniques such as PENELOPE [45] may
incur much higher overhead due to context switches.
2.3 Testing for Races
Software testing requires test inputs. Given a set of test inputs,
the race detection process for a multi-threaded program involves
two steps. First, for each test input, the program is executed by
a dynamic race detection tool that identiÔ¨Åes potential races (race491p u b l i c boolean c o n t a i n s V a l u e ( O b j e c t v a l u e ) {
2 E n t r y t a b [ ] = t a b l e ; / / b l o c k ID : 1
3 i f( v a l u e == n u l l ) {
4 f o r (i n t i = t a b . l e n g t h ; i   > 0 ; ) / / b l o c k ID : 2
5 . . .
6 }
7 e l s e {
8 . . .
9 return t ru e ;/ / b l o c k ID : 4
10 }
11 return f a l s e ;
12 }
13
14
15
16
17
18
19
20
21p u b l i c void c l e a r ( ) {
22 . . .
23 synchronized (t h i s ) {
24 E n t r y t a b [ ] = t a b l e ; / / b l o c k ID : 3
25 f o r (i n t i =0; i < t a b . l e n g t h ; i ++)
26 t a b [ i ] = n u l l ;/ / b l o c k ID : 4
27 }
28 / / b l o c k ID : 5
29 . . .
30 }1p u b l i c boolean c o n t a i n s V a l u e ( O b j e c t v a l u e ) {
2 E n t r y t a b [ ] = t a b l e ;
3 i f( v a l u e == n u l l ) {
4 f o r (i n t i =0; i < t a b . l e n g t h ; i ++) / / change
5 . . .
6 }
7 e l s e {
8 . . .
9 return c o n t a i n s N u l l ( ) ; / / change
10 }
11 return f a l s e ;
12 }
13
14 p r i v a t e boolean c o n t a i n s N u l l ( ) {
15 E n t r y t a b [ ] = t a b l e ;
16 f o r (i n t i =0; i < t a b . l e n g t h ; i ++)
17 f o r ( E n t r y e= t a b [ i ] ; e != n u l l ; e=e . n e x t )
18 . . .
19 }
20
21 p u b l i c void c l e a r ( ) {
22 . . .
23 synchronized (t h i s ) {
24 E n t r y t a b [ ] = t a b l e ;
25 f o r (i n t i =0; i < t a b . l e n g t h ; i ++)
26 t a b [ i ] = n u l l ;
27 }
28 modCount ++; / / change
29 . . .
30 }
Figure 1: Original HM program P(left) and a modiÔ¨Åed version of the HM program P0(right)
detection). Second, for any input tthat produces a set of potential
races PRaceSet in the Ô¨Årst step, and for each pr2PRaceSet ,
the program is executed n(1nN) times to verify prunder
t, where Nis deÔ¨Åned by users in terms of a testing budget (race
veriÔ¨Åcation). We deÔ¨Åne N= 1 for S IMRT to simulate a resource-
constrained testing environment.
Our S IMRT approach uses R ACEFUZZER to test for races by
following the foregoing two steps. However, in the race veriÔ¨Åca-
tion phase, R ACEFUZZER veriÔ¨Åes every potential race reported in
the race detection phase for the given input regardless of whether
this race has already been veriÔ¨Åed by previous inputs. This can be
expensive when given large test suites, particularly when each test
case reports redundant potential races in the race detection phase.
To alleviate this problem, we adjusted R ACEFUZZER so that, once
a potential racing pair is conÔ¨Årmed to be a real race, it is not ver-
iÔ¨Åed again under another input. SpeciÔ¨Åcally, for each test input,
SIMRT Ô¨Årst invokes R ACEFUZZER and reports potential races. If
there exist any potential races that have not been conÔ¨Årmed as real
races, S IMRT invokes R ACEFUZZER to verify each of them, under
the same test input, before proceeding to the next test input. As
such, the actual number of test runs for an entire test suite Tis:
TR=jTj+jPRaceSetjP
i=1NTi
Here,jTjis the total number of original tests, jPRaceSetjis the
number of potential races, and NTiis the number of tests required
to verify the ithpair in PRaceSet .
3. APPROACH
Figure 1 contains an example that we use to illustrate our ap-
proach. The Ô¨Ågure provides code snippets from two versions of the
JDK‚Äôs HashMap utility (slightly modiÔ¨Åed, and referred to here as
HM). The variables table ,tab[i] ,next andmodCount are
identiÔ¨Åed as shared variables by thread escape analysis (see Sec-
tion 3.2). A test driver for this code instantiates an hmobject from
classHM. The two parameters in the constructor of HMspecify the
initial capacity and load factor. Each of the test cases for the code
involves two components, each executing in its own thread. We
deÔ¨Åne four test case components:
case0: hm.clear()
case1: hm.containsValue(new HM(100, 10.0f));
case2: hm.containsValue(new HM(0, 100.0f));
case3: hm.containsValue(null)A test case tciis denoted by ( m,n) (0m; n3), where mand
ndenote two of the foregoing cases (i.e., case mandcase n).
Suppose there are six test cases generated for version PofHM:
tc1= (0;1),tc2= (0;2),tc3= (0;0),tc4= (0;3),tc5=
(1;2), andtc6= (3;3). In version P0ofHM, there are two changes
that cause three races to occur. The Ô¨Årst change involves addition
of a new method containsNull() and a call to it from line 9.
This change causes a read-write race involving tab[i] at line 17
andtab[i] at line 26 when executing tc1ortc2(i.e., concur-
rently executing containsNull() andclear() ). The second
change involves addition of a new line of code (line 28) that reads
and writes a class variable modCount . Two data races occur in this
case, involving modCount when tc3is executed; one is a write-
write race, and the other is a read-write race.
3.1 Overview of SimRT
Figure 2 provides an overview of S IMRT. S IMRT contains Ô¨Åve
components: SVLocator ,ImpAnalyzer ,Matcher ,Selector ,Ranker .
LetPbe a program, let P0be a modiÔ¨Åed version of P, letC0be the
set of changes made to Pto produce P0(a set of program locations
inP0), let B0
SVdenote a block in a method in P0that contains
a shared variable SV, and let BSVbe a block in a method in P
that corresponds to B0
SV. SIMRT Ô¨Årst computes a list of shared
variables L0
SVinP0using SVLocator . Next, ImpAnalyzer updates
L0
SVby mapping B0
SVtoBSV, and identifying the shared vari-
ables in L0
SVthat are potentially impacted by one of the changes
C0inP0. Next, Matcher iterates over each SVinL0
SV, and selects
theSVs that can be matched into pairs. The output of Matcher
is a list of impacted shared variable pairs ( I0
SV P ) that are cover-
age targets for RTS and TCP techniques. Next, Selector selects
T02Tfor use in regression testing P0. Finally, Ranker prioritizes
P P‚Äô 
ImpAnalyzer	 ¬†I‚ÄôSVP L‚ÄôSV T HP  
selector	 ¬†
T‚Äô  T‚Äôp L‚ÄôSV (updated)   
SVLocator	 ¬†
matcher	 ¬†
ranker	 ¬†
B‚ÄôSV BSV 
 Tp 
Figure 2: Overview of SimRT50the test cases in T0(orT), producing T0
P(orTP). Both Selector
andRanker utilize coverage history information provided as HP.
We describe each of these components in the sections that follow.
3.2 Shared Variable IdentiÔ¨Åcation
SVLocator produces a list of variables L0
SVthat can be accessed
by multiple threads in P0. In Java, shared variables can be identi-
Ô¨Åed by using thread escape analysis [20, 38]; we adopted the con-
servative shared variable detection algorithm proposed in [16], that
uses the ThreadLocalObjectAnalysis API [15] provided
by S OOT [44] to compute variables that can potentially be read
from and written to by multiple threads simultaneously. We also
used the alias analysis [28] provided by S OOT to identify a set of
escaping variables that can potentially access the same location.
Each shared variable ( SV) is deÔ¨Åned as a 7-tuple < C:M ,N,D,
L,A,B,I> where Cis a class name, Mis a method signature,
Nis the name of the SV,Dis a memory access identiÔ¨Åer ( SVs
that potentially access the same memory location share the same
identiÔ¨Åer), Lis a line number at which the SVoccurs, Adenotes
the access operation (read or write) performed on the SV,Bis the
block ID (a unique number in C:M ) inPwhere SVis mapped
to, and Iis a boolean value indicating whether SVis impacted.
Note that C:M must exist in both P and P‚Äô. If a method M0in
P0in which a SV occurs does not exist in P, we locate, in the
call graphs for P0, the method that most directly calls M0that also
exists in P, and, if no such method is found, C:M is deÔ¨Åned as?.
In the HMprogram, shared variables in P0are displayed as the
following initialized tuples:
SV1:<HM.containsValue(Object), table, 1, 2, read, ?,?>
SV2:<HM.containsValue(Object), table, 1, 15, read, ?,?>
SV3:<HM.containsValue(Object), tab[i], 2, 17, read, ?,?>
SV4:<HM.containsValue(Object), next, 3, 17, read, ?,?>
SV5:<HM.clear(), table, 1, 24, read, ?,?>
SV6:<HM.clear(), tab[i], 2, 26, write, ?,?>
SV7:<HM.clear(), modCount, 4, 28, read, ?,?>
SV8:<HM.clear(), modCount, 4, 28, write, ?,?>
Here, <HM.containsValue(Object), table, 1, 15,
read,?,?>indicates that in line 15 in P0, variable table is
read, with access identiÔ¨Åer 1. The method HM.containsNull()
in which table occurs does not exist in P; thus, HM.contains -
Value(Object) is Ô¨Ålled into C:M . Elements BandIare not
speciÔ¨Åed until the comparison algorithm (Section 3.3) is invoked.
3.3 Shared Variable Impact Analysis
Figure 3 displays the algorithm used by I MPANALYZER . The al-
gorithm takes program P, modiÔ¨Åed version P0, and a list of shared
variables L0
SVwith initialized tuples as inputs, and returns the up-
dated L0
SV. First, the algorithm constructs control Ô¨Çow graphs
(CFGs) GandG0for all methods in PandP0(line 4). Each node
in a CFG is represented using a unique block ID. Next, the algo-
rithm compares each CFG GinPto the corresponding G0inP0by
calling Compare (line 6). The comparison begins with entry nodes
EandE0. Given two CFG nodes NandN0,Compare determines
whether NandN0have successors ( SandS0) whose code differs
along pairs of identically labeled edges (lines 14-20, 23).
Up to this point, ImpAnalyzer behaves identically to the origi-
nal D EJAVU algorithm [36]. However, ImpAnalyzer uses different
mechanisms to handle node comparisons. ImpAnalyzer obtains a
set of line numbers in block node S0(line 22). If the code asso-
ciated with SandS0is the same (line 23), the algorithm iterates
overL0
SV, and picks the SVs that are contained in S0(line 24-26).
For each of these SVs, tuple element Bis set to the block ID of S
(line 27), and tuple element Iis set to false (line 28), indicatingprocedure UpdateSVInfo
1:Inputs:P,P0,L0
SV2:Outputs:L0
SVforP0/*updated*/
3:begin
4: construct CFGs for PandP0
5:foreachG2CFG andG02CFG0
6: Compare(E,E0)/*start from entry nodes*/
7:endfor
8: returnL0
SV9:end
procedure Compare
10:Inputs:N,N0/*nodes inGandG0*/
11:Outputs:L0
SVforG0/*updated*/
12:begin
13:isVisted( N)=true /*whetherNis visited*/
14:foreach successor SofN2G
15: if(N,S) is labeled
16: L= the label on edge (N;S )
17: else
18: L=
19: endif
20:S0= the node in G0such that (N0,S0) has labelL
21: ifisVisited( S)isfalse
22: LNS S0=getBlockLineNumbers( S0)
23: ifLEquivalent( S,S0)/*if code is equivalent*/
24: foreachSV2L0
SV25: lnSV=getSVLineNumber( SV)
26: iflnSV2LNS S0
27: setOldBlockID( SV,S)
28: setIsImpacted( SV,false )
29: break
30: endif
31: endfor
32: Compare ( S,S0)
33: else
34: foreachSV2L0
SV35: ifisImpactedBy( SV,S0)
36: setOldBlockID( SV,S)
37: setIsImpacted( SV,true)
38: break
39: endif
40: endfor
41: endif
42: endif
43:endfor
44:end
Figure 3: ImpAnalyzer algorithm
that this shared variable is not impacted.
If the code associated with SandS0differs along some pair of
identically labeled edges, ImpAnalyzer iterates over L0
SVand de-
termines the impacted shared variables (line 34-35). If a shared
variable SVis impacted by the change, tuple element Bis set to
old block ID S(line 36), and tuple element Iis set to true (line 37),
indicating the existence of impact. A shared variable is considered
impacted (line 35) in the following cases:
1. If a change to P0involves an SVinP0and this SVis either
added to or modiÔ¨Åed in P0;anSVis considered impacted.
2. If a change to P0involves adding or modifying a method, all
SVs in this method are considered impacted.
3. If a change to P0involves a synchronization statement, all
theSVs in the entire region of the synchronized block are
considered impacted. For example, if line 23 in Program
Pis omitted, all shared variables inside the synchronization
block ( SV5andSV6) are considered impacted. In addition
to considering blocks using the synchronized keyword, we
also consider blocks using explicit locks, including lock() ,
unlock() , and trylock() , and locks implementing a51ReadWriteLock interface ( readLock() ,writeLock() ).
We do not consider changes involving wait() ,notify() ,
ornotifyAll() ; these must be called inside a synchro-
nized block [14], and changing them does not affect SVs in
synchronized blocks.
4. If a change to P0involves removing a volatile keyword in
the declaration of an SV; all subsequent uses of this SVare
considered impacted.
5. If a change to P0does not involve an SV but its full im-
pact set in P0(obtained by traditional impact analysis such
as static forward slicing) includes one or more SVs, these
SVs are considered impacted.
In the example shown in Figure 1, I MPANALYZER compares the
method containsValue(Object) inPandP0. The algo-
rithm begins by visiting the node with block ID 1 in P(lines 2-3)
and compares it to its corresponding node in P0. The two blocks are
equal, so the algorithm updates tuple elements BandIfor all SVs
in lines 2-3 of P0. Thus, <HM.containsValue(Object),
table, 1, 2, read, ?,?>is set to <HM.contains -
Value(Object), table, 1, 2, read, 1, false> .
Here, B= 1 is the block ID in Pthattable is mapped to, and
I=false indicates that table is not impacted.
Next, I MPANALYZER compares the nodes at line 4 in both Pand
P0. Although line 4 is changed in P0, the change does not impact
any shared variables. Thus, the comparison proceeds to the next
node in P. When tI MPANALYZER visits the node corresponding
to block 4 in HM.containsValue(Object) inP, it discov-
ers that line 9 in P0is changed by addition of the call to method
containsNull() . Thus, all SVs in this method are considered
impacted. For example, <HM.containsValue(Object),
next, 3, 17, read, ?,?>is set to <HM.contains -
Value(Object), next, 3, 17, read, 4, true> .
After the algorithm returns, the two undeÔ¨Åned elements in each
SVare noted. In the example shown in Figure 1, the list of shared
variables is updated as follows:
SV1:<HM.containsValue(Object), table, 1, 2, read, 1, false> ,
SV2:<HM.containsValue(Object), table, 1, 15, read, 4, true> ,
SV3:<HM.containsValue(Object), tab[i], 2, 17, read, 4, true> ,
SV4:<HM.containsValue(Object), next, 3, 17, read, 4, true> ,
SV5:<HM.clear(), table, 1, 24, read, 3, false> ,
SV6:<HM.clear(), tab[i], 2, 26, write, 4, false> ,
SV7:<HM.clear(), modCount, 4, 28, read, 5, true> ,
SV8:<HM.clear(), modCount, 4, 28, write, 5, true> .
3.4 Matching Shared Variables
Because race detection involves pairs of shared variable accesses,
the next step is to use Matcher to identify a set of impacted shared
variable pairs I0
SV P serving as coverage targets for the Selector
andRanker modules. We deÔ¨Åne an impacted shared variable pair
ISV P as a pair of shared variables SViandSVj, such that at least
one of them is impacted, and at least one of them is a write access.
To illustrate, Matcher takes the updated shared variable list L0
SV
computed in Figure 3 as input, and outputs the potential impacted
shared variable pairs I0
SV P . For each shared variable SViinL0
SV,
ifSViis impacted (determined by querying element Iin the SVi
tuple), the algorithm iterates over L0
SVto identify each SVjinL0
SV
that is shared with SVi, regardless of whether SVjis impacted. As
a result, each qualiÔ¨Åed ISV P = (SVi,SVj) is added to I0
SV P . In
our example, Ô¨Åve variables are impacted ( SV2,SV3,SV4,SV7, and
SV8), but only SV3,SV7, andSV8qualify for pairing, so I0
SV P =
<(SV3,SV6), (SV7,SV8), (SV8,SV8)>.procedure TestSelection
45:Inputs:T,HP,I0
SV P46:Outputs:T0
47:begin
48:T0=
49:foreach (SVi;SVj)2I0
SV P50:B1=getOldBlockID( SVi)
51:B2=getOldBlockID( SVj)
52: foreachtc2T
53: ifHP(tc;B 1) =true andHP(tc;B 2) =true
‚Äî: and HP(tc;Tid B1;Tid B2) =true
54: T0=T0[tc
55: endif
56: endfor
57:endfor
58:end
Figure 4: Regression test selection algorithm
3.5 Regression Test Selection
The next step is to use Selector to select test cases that are rele-
vant to race detection. A naive approach is to select every test case
that traverses any (SVi; SVj)2I0
SV P . However, this may include
test cases that execute shared variables but involve only one thread.
Instead, S IMRT selects test cases by traversing the tuple elements
BofSViandSVjinvolved in different threads. To facilitate this,
when collecting coverage information for each block in the original
program P, which is done during the preliminary phase of testing,
we also record the thread IDs that cover each block. As such, the
constructed test history indicates (1) whether a block is covered,
and (2) the IDs of the threads that cover this block.
Figure 4 shows the Selector algorithm. The algorithm takes three
inputs: test suite T, the impacted shared variable pairs I0
SV P , and
the test coverage history that indicates which test cases in Tcov-
ered which statements in Pwith which thread. HPis denoted by
tci= <(C:M ,B,TID )>, where tciis the test case with number i,
C:M is the class name combined with the method signature, Bis
the block ID that tcicovers, and TID is the ID of the thread that
covers B. For each shared variable pair ( SVi,SVj) inI0
SV P , the
algorithm obtains the matching block IDs B1andB2forSViand
SVj(line 50-51) as coverage targets. Based on the coverage history
HP, the algorithm selects all test cases from Tthat traversed B1
andB2(the Ô¨Årst two conditions in line 53) with different threads
(the third condition in line 53), and adds them to T0(line 54).
In our example, suppose the coverage history HPholds the fol-
lowing coverage information for each of the six test cases:
tc1= <(HM.clear(), 3, 1), (HM.clear(), 4, 1), (HM.clear(), 5, 1),
(HM.containsValue(Object), 1, 2), (HM.containsValue(Object), 4, 2)>,
tc2= <(HM.clear(), 3, 1), (HM.clear(), 4, 1), (HM.clear(), 5, 1),
(HM.containsValue(Object), 1, 2), (HM.containsValue(Object), 4, 2)>,
tc3= <(HM.clear(), 3, 1), (HM.clear(), 4, 1), (HM.clear(), 5, 1),
(HM.clear(), 3, 2), (HM.clear(), 4, 2), (HM.clear(), 5, 2)>,
tc4= <(HM.clear(), 3, 1), (HM.clear(), 5, 1),
(HM.containsValue(Object), 1, 2), (HM.containsValue(Object), 2, 2)>,
tc5= <(HM.containsValue(Object), 1, 1), (HM.containsValue(Object), 4, 1),
(HM.containsValue(Object), 1, 2), (HM.containsValue(Object), 4, 2)>,
tc6= <(HM.containsValue(Object), 1, 1), (HM.containsValue(Object), 2, 1),
(HM.containsValue(Object), 1, 2), (HM.containsValue(Object), 2, 2)>.
In this case, both tc1andtc2cover one target, ( SV3,SV6), with
different threads, and tc3covers two targets, ( SV7,SV8) and ( SV8,
SV8), with different threads. Test cases tc5, andtc6do not cover
any targets. Test case tc4covers targets ( SV7,SV8) and ( SV8,
SV8) but with only one thread. Therefore, T0= <tc1,tc2,tc3>. In
contrast, most traditional RTS techniques would select all six test
cases for T0because they all cover changed blocks. By running52teststc1,tc2andtc3we can detect all three races, while tc4,tc5
andtc6do not help expose races.
Note that we select both tc1andtc2even though they cover
the same target. The reason for this is because covering (execut-
ing) the two shared variables in the target pair under one test input
is not necessarily sufÔ¨Åcient to determine whether they access the
same memory addresses [57]. Different inputs could cause differ-
ent states to exist in the same code region, causing two instructions
in the target pair to access different memory locations under one
input and the same memory location under a different input.
3.6 Test Case Prioritization
Ranker prioritizes test cases, beginning with those identiÔ¨Åed by
Selector .Ranker iterates over T0, selects the test case tcithat cov-
ers the most impacted shared variable pairs ( ISV P s) inI0
SV P , and
places tciinT0
P. Next, Ranker iteratively selects the test case tci
that covers the most ISV P s and appends tcitoT0
P. When multi-
ple test cases cover the same number of ISV P s,Ranker chooses
one randomly. If each ISV P has been covered by at least one test
case, and the remaining unprioritized test cases cannot add addi-
tional coverage, Ranker resets the coverage vectors for all unpriori-
tized test cases to their initial values, and reapplies the prioritization
approach, ignoring previously prioritized test cases.
In our example program, tc1andtc2cover ( SV3,SV6) with dif-
ferent threads, and tc3covers ( SV7,SV8) and ( SV8,SV8) with
different threads. Ranker Ô¨Årst places tc3inTP. Because tc1and
tc2achieve the same additional coverage, the algorithm randomly
selects one of these (say, tc1) and appends it to T0
P. Next, the algo-
rithm resets the coverage data. As such, tc2becomes the test case
that covers the most ISV P s and is appended to T0
P. Therefore, the
output of Ranker isT0
P= <tc3,tc1,tc2>. When running T0
P, all
three races can be exposed by the Ô¨Årst two test cases.
As noted in Section 2, the safety of RTS techniques depends
on certain conditions, and these include the condition that the pro-
gram under test be executed deterministically. This condition can-
not generally be met for the class of programs that we consider, and
thus, S IMRT may omit test cases from T0that could reveal races
inP0. In this context, it is important to note that even a retest-
all approach can ‚Äúomit‚Äù race-revealing test cases, because a given
test case may or may not expose a fault in a given run when non-
determinism affects thread behavior. The motivation for selecting
a subset of test cases, however, is to select test cases that are more
likely to reveal races than others, allowing testers to focus on more
worthwhile pursuits. Nevertheless, if test engineers wish, they can
useRanker to further prioritize the remaining test cases ( T T0),
and execute those test cases as well. This process is performed us-
ing the approach just described, applied to T T0, in two steps,
where step 1 focuses on test cases that cover the most impacted
shared variables, and step 2 focuses on test cases that cover only
non-impacted shared variables.
In our example, when prioritizing the remaining test cases (hav-
ing already set TPto <tc3,tc1,tc2>),Ranker notes that (step 1)
tc4covers two impacted shared variables, SV7andSV8, and tc5
covers three impacted shared variables, SV2,SV3, and SV4, and
appends these to TP, obtaining < tc3,tc1,tc2,tc5,tc4>. In step 2,
sincetc6covers SV1, a shared variable that is not impacted, Ranker
appends that to TP. Ultimately, TP= <tc3,tc1,tc2,tc5,tc4,tc6>.
4. EMPIRICAL STUDY
We wish to determine whether S IMRT is cost-effective, and ide-
ally such an assessment would involve comparisons with exist-
ing state-of-the-art approaches for detecting races in modiÔ¨Åed soft-
ware. There are, however, no existing approaches that have thisspeciÔ¨Åc goal. In the absence of such approaches, we can instead
compare S IMRT to processes that are currently used in general re-
gression testing applications, that might likewise be employed in
our setting.
A second important issue regarding S IMRT involves whether ei-
ther or both of its component techniques, selection and prioritiza-
tion, play a role in its cost-effectiveness (or lack thereof), and if so,
to what extent. Understanding this issue is important to any efforts
to extend the approach further.
We thus designed a study focusing on three research questions:
RQ1: How do the efÔ¨Åciency andeffectiveness of S IMRT, consider-
ing only its regression test selection component, compare to those
of the retest-all technique and a state-of-the-art RTS technique.
RQ2: How does the effectiveness of the TCP technique employed
by S IMRT compare to that of random test case orders when just the
prioritized selected test cases are considered?
RQ3: How does the effectiveness of the TCP technique employed
by S IMRT compare to that of traditional additional-block-coverage
prioritization and random test case orders when the entire priori-
tized test suite is considered?
RQ1 lets us consider the overall efÔ¨Åciency and effectiveness of
SIMRT focusing on its regression test selection component, com-
pared to the baseline approach in which no selection is performed
and to a state-of-the-art RTS technique. RQ2 lets us consider whet-
her prioritization of just those test cases selected by S IMRT helps
detect races more quickly than leaving them unprioritized. RQ3
lets us consider the overall effectiveness of prioritization if com-
plete test suites are used.
4.1 Objects of Analysis
We chose nine open source Java objects, which are representa-
tive of real-world code and have been widely used in academic re-
search. These included one object downloaded from the Software-
artifact Infrastructure Repository (SIR) [5], Ô¨Åve objects from JDK,
and three larger objects [18, 25, 49]. The objects include both
closed code units (code units equipped with test drivers) and open
code units (libraries that require test drivers to close them).
Among the closed objects, W EBLECH is a multi-threaded web
site download and mirror tool; it accepts both command line op-
tions and a conÔ¨Åguration Ô¨Åle that speciÔ¨Åes settings such as URL,
search options (e.g., BFS, DFS), and number of threads. J IGSAW is
a leading-edge Web server platform; it accepts conÔ¨Åguration Ô¨Åles
on both client and server sites, and multiple main entry points with
command line options.
Among the open objects, L ANG is part of the Apache Common
Lang project. L OG4Jis a Java logging application. H ASHMAP ,
TREEMAP ,ARRAYLIST ,HASHTABLE , and BITSET are synchro-
nized collection classes provided by Sun Microsystems‚Äô JDK. To
close these objects, we wrote test drivers that utilize unit test cases
generated by R ANDOOP (described later). Because R ANDOOP does
not support multi-threaded programs, the test drivers also create
sets of threads that concurrently execute the methods.
We utilized two versions of each of the nine objects. Table 1
lists our object versions along with some of their characteristics,
including the number of lines of non-comment code (NLOC, col-
umn 2) and the number of shared variables identiÔ¨Åed in the modi-
Ô¨Åed versions (SVs, column 6) using the technique described in Sec-
tion 3 (the numbers in parentheses indicate the number of impacted
shared variables). Other columns are described later.
To address our questions, we also required multi-threaded test
cases. Our objects, however, were either not equipped with such
test cases, or the test suites supplied with them were too small (e.g.,53Table 1: Objects of Analysis and their Characteristics
Program NLOC |T| PRs RRs SVs (ISVs) ISVcovs
LANG (V) 993 29104 - - - -
LANG (V‚Äô) 990 8 8 19 (6) 6
HASHMAP (V1.2) 852 26405 - - - -
HASHMAP (V1.59) 1,014 - 59 10 230 (61) 51
TREEMAP (V1.2) 1,321 28203 - - - -
TREEMAP (V1.56) 1,384 - 16 0 458 (17) 12
ARRAYLIST (V1.2) 727 34202 - - - -
ARRAYLIST (V1.43) 747 - 22 3 352 (33) 24
HASHTABLE (V1.2) 3,041 28757 - - - -
HASHTABLE (V1.55) 4,111 - 3 2 412 (36) 28
BITSET (V1.2) 194 30500 - - - -
BITSET (V1.55) 486 - 4 2 240 (13) 10
LOG4J(V1.2.13) 15,331 32065 - - - -
LOG4J(V1.2.8) 15,366 - 6 2 171 (63) 44
WEBLECH (V0.02) 16,393 9601 - - - -
WEBLECH (V0.03) 16,633 - 29 1 23 (14) 12
JIGSAW (V2.2.0) 90,331 18805 - - - -
JIGSAW (V2.2.6) 101,207 - 69 3 3452 (488) 209
fewer than 50 test cases) to allow S IMRT to operate as intended.
To simulate a resource-constrained testing environment in which
it makes sense to utilize approaches such as S IMRT, we chose to
create test cases using a testing time budget, that limits the maxi-
mum number of test cases that can be executed for a given object.
The testing time budgets we chose were selected to be relevant to
the size and complexity of the objects. For the six smaller and less
complex objects, we chose a testing budget of 12 hours (i.e., test-
ing that can be performed overnight). For the three larger and more
complex objects, we chose a testing budget of 60 hours (i.e., test-
ing that can be completed over a weekend.) Both of these are prac-
tically realistic choices that engineers could make given that the
testing required by our approach can be conducted automatically
without the need for human intervention, yet must (like any valida-
tion effort) be conducted within some Ô¨Åxed time period. Note that
the testing time we measured is based on the time required to run
the instrumented original object with race detectors in place; thus,
the numbers of generated test cases depend on the instrumentation
overhead for different object objects.
The test cases we used were created using test case generation
techniques relevant to the objects. Because our goal is to detect
races, a test case must include two components: test input data and
speciÔ¨Åed thread interleavings [41]. For objects WEBLECH and J IG-
SAW, which accept conÔ¨Åguration Ô¨Åles, we used incremental cover-
ing arrays [12] to generate test inputs. They also accept inputs via
command line options, and such inputs were randomly generated.
For the closed objects, which were not equipped with test drivers,
we Ô¨Årst used R ANDOOP [31] to generate unit test cases for each
method, and then we created multiple threads to concurrently ex-
ecute these methods. Note that the number of threads is another
input argument. For all object objects, the number of threads (if
mutable) associated with each test input was randomly generated
in a range from 2 to 100. Column 3 of Table 1 (|T|) lists the num-
bers of test cases ultimately utilized for each object.
To address our research questions we also needed to know what
races (if any) existed in our object objects. To determine this, we
ran all test cases on the original version of each object using the
modiÔ¨Åed version of R ACEFUZZER (described in Section 2). We
discovered that the original object versions contained many poten-
tial and real races. In this work we are interested only in regression
races (i.e., races introduced by code modiÔ¨Åcations); not residual
races (races that persist across versions of the modiÔ¨Åed objects).
Hence, we located the causes of such races in the original object
versions, and corrected the code in both versions to ensure that they
would not occur in either. Next, we executed all of the test cases
associated with each original object on its modiÔ¨Åed version withthe race detector and veriÔ¨Åer enabled. This yielded a set of regres-
sion races that had been introduced by the code changes made to
the modiÔ¨Åed versions.
Because non-determinism may cause race detectors to report dif-
ferent results on different object executions, we repeated the fore-
going process ten times for each object pair, and accumulated any
newly detected races. (The Ô¨Çuctuation in numbers of races reported
on different runs was actually quite small, and is discussed further
in Section 5.) Columns 4 and 5 of Table 1 list the numbers of po-
tential and real regression races (PRs and RRs) discovered in the
foregoing process. Column 7 lists the number of impacted vari-
ables (ISV covs) that are covered in the modiÔ¨Åed objects.
4.2 Variables and Measures
4.2.1 Independent Variable
Our independent variable involves the techniques used in our
study. As noted earlier, we consider S IMRT, the traditional retest-
all technique (RTA), and the traditional D EJAVU RTS technique
(hereafter referred to as RTSC). With all three of these, we enable
the race detector and veriÔ¨Åer. We use S IMRTSto denote S IMRT
with just the RTS technique enabled.
We refer to S IMRT when it also employs a TCP technique as
SIMRTSP. When considering RQ2 and just selected test cases, we
compare S IMRTSPto a random test case ordering applied to just
the selected test cases (denoted here by R ANDOM SP).
When considering RQ3, in which all test cases are prioritized,
we compare S IMRT (denoting this version of the technique by
SIMRTAP) to two approaches: a random test case ordering applied
to all test cases (denoted here by R ANDOM AP) and an application
of the additional block coverage prioritization technique (denoted
here by ABC AP) described in Section 2.
4.2.2 Dependent Variables
As dependent variables, we chose metrics allowing us to answer
each of our three research questions.
Regression test selection. To measure the efÔ¨Åciency of regression
test selection we measured testing time by adding relevant mea-
sures, including the time required for escape and impact analyses
(if any), the time required for running regression test selection al-
gorithms (if any) and the time required to execute test cases.1To
obtain these times, we applied RTSC and S IMRTSand measured
relevant analysis and test selection times. Then, to account for pos-
sible differences in testing times per execution of sets of test cases,
we executed each object ten times on the various sets of selected
test cases (or in the case of RTA, on all test cases), and calculated
the average time required to execute the test cases.
To measure race detection effectiveness , we compare the num-
bers of potential and real races detected by test cases selected by
RTA, RTSC, and S IMRTS.
Test case prioritization. To measure the effectiveness of test case
prioritization we considered the rate at which test cases detect both
potential and real races. To measure such rates, we use the well-
known metric APFD (Average Percentage Fault Detected) [37].
LetTbe a test suite containing ntest cases, let Tpbe a prioritized
version of T, and let Rbe a set of mraces revealed by T. LetTRi
1When measuring testing time, we do not include time spent on
activities performed in the preliminary phase of regression testing
(prior to the time at which the modiÔ¨Åed object is available for test-
ing, and when testing time becomes a critical issue); these include
collection of test history information and construction of control
Ô¨Çow graphs for the original object.54be the Ô¨Årst test case in ordering TpofTthat reveals race i. The
APFD for test suite Tpis given by the equation:
APFD =jm 1jP
i=1jTRij
nm+1
2n
APFD ranges from 0 to 100, with higher values indicating faster
rates of race detection.
TheAPFD metric can be applied to orderings of the entire test
suiteT, or to orderings of just the set of selected test cases T0. In
the former case, n=jTj, and in the latter case, n=jT0j.
4.3 Study Operation
We conducted our experiment runs on a Linux cluster with 1440
AMD cores housed in 42 nodes with 128GB per node. We used
a modiÔ¨Åed version of R ACEFUZZER for race detection and veri-
Ô¨Åcation (see Section 2). Among our objects, the instrumentation
overhead incurred by our R ACEFUZZER ranged from 1.5X to 20X.
The remaining components in our framework (i.e., ImpAnalyzer ,
Matcher ,Selector andRanker ) were implemented using Java by
following the algorithms described in Section 3.
As noted in [4], executing a program once per input is sufÔ¨Åcient
to detect most, if not all, of the concurrent faults detectable under
that input, because progams tend to follow the same interleaving
patterns during different executions. As we discuss in Section 5,
executing a program on the same input additional times may occa-
sionally uncover additional races, but the beneÔ¨Åt of doing this may
be outweighed by the increased cost of testing. Therefore, the data
reported in this study for each technique was obtained by executing
each object once. However, we discuss the impact of race detection
effectiveness given multiple test runs in Section 5.
4.4 Threats to Validity
The primary threat to external validity for this study involves the
representativeness of our objects and test cases. Other objects and
test cases may exhibit different behaviors and cost-beneÔ¨Åt trade-
offs. However, we do reduce this threat to some extent by using
several varieties of well studied open source code objects for our
study, and test suites generated by practical approaches.
The primary threat to internal validity for this study is possible
faults in the implementation of our approach and in the tools that we
use to perform evaluation. We controlled for this threat by exten-
sively testing our tools and verifying their results against a smaller
program for which we can manually determine the correct results.
Where construct validity is concerned, our measurements of ef-
Ô¨Åciency of regression test selection focus on the time required for
analysis and test execution. However, other costs such as test setup
and maintenance costs can play a role in technique efÔ¨Åciency. Our
measurement of effectiveness for test case prioritization is APFD .
Although APFD does have certain limitations [6], it does provide
a simple, intuitive measurement for rapid race detection.
4.5 Results and Analysis
4.5.1 RQ1: Regression Test Selection
EfÔ¨Åciency. Figures 5 and 6 show technique execution times in
minutes for each of the three techniques and nine objects. For the
six small objects using overnight testing, RTSC required less time
than RTA, with savings ranging from 30.8% to 96.9%, and an aver-
age savings of 32.3% across all six objects. S IMRT achieved even
greater savings than RTA on the smaller objects, with an average
savings of 89.9%, and savings on individual objects ranging from
46.2% to 99.9%. For the three objects using over-the-weekend test-
ing, RTSC required less time than RTA on only one object ( LOG4J)
0	 ¬†100	 ¬†200	 ¬†300	 ¬†400	 ¬†500	 ¬†600	 ¬†700	 ¬†800	 ¬†
lang	 ¬†hashmap	 ¬†treemap	 ¬†arraylist	 ¬†hashtable	 ¬†bitset	 ¬†Time	 ¬†(mins)	 ¬†RTA	 ¬†RTSC	 ¬†SimRT_S	 ¬†Figure 5: EfÔ¨Åciency: testing times for smaller objects
0	 ¬†500	 ¬†1000	 ¬†1500	 ¬†2000	 ¬†2500	 ¬†3000	 ¬†3500	 ¬†4000	 ¬†
log4j	 ¬†weblech	 ¬†jigsaw	 ¬†Time	 ¬†(mins)	 ¬†RTA	 ¬†RTSC	 ¬†SimRT_S	 ¬†
Figure 6: EfÔ¨Åciency: testing times for larger objects
with a savings of 44.8%. S IMRT achieved greater savings than
RTA on all three objects, with savings of 96.5% on LOG4J, 47.9%
onWEBLECH , and 52.9% on JIGSAW .
We also measured the time required by RTSC and S IMRT to
perform analysis tasks. Due to space limitations we do not present
all of the data, but overall, while S IMRT required greater analysis
times than RTSC, its analysis times never exceeded 45 seconds,
which accounted for less than 0.5% of technique runtime overall.
The time spent on escape analysis and impact analysis never ex-
ceeded 308 seconds. We also measured the time spent by S IMRT
on race veriÔ¨Åcation, and the results ranged from 7% to 11.9% of
total technique runtime. The remaining time was spent on analysis
tasks and race detection. VeriÔ¨Åcation time varied with the number
of detected potential races and the number of test runs needed to
verify each potential race.
Testing times did vary across objects. The number of test cases
selected by techniques depends on the program locations in which
changes occur or in which shared variables are impacted, and this,
in turn, affects testing time. For example, in W EBLECH , many
changes occur inside the main function, causing all test cases to
be selected by RTSC; thus, in this case, no time is saved.
Effectiveness. We consider whether races (both potential and real)
detected by RTA and RTSC can also be detected by S IMRT. Ta-
ble 2 shows the numbers of potential and real races detected for
each of the nine objects by RTA, RTSC and S IMRT. Because the
data reported for each technique is based on a single run, the num-
bers of races detected by RTA do not necessarily equal the numbers
in Table 1; negative numbers in parentheses indicate the numbers
of races missed compared to those known to be detectable.
For all nine objects, RTSC detected 100% of the potential and
real races detected by RTA. S IMRT, on the other hand, detected
100% of the potential races on seven of nine objects and 100% of
the real races on all nine. On LOG4Jand JIGSAW SIMRT missed
one and two potential races, respectively. This sacriÔ¨Åce in effec-
tiveness is relatively small, with only 1.3% of the potential races
detected by RTA and RTSC missed across the nine objects. Mean-
while, the associated savings in terms of testing time was large.55Table 3: APFD Values for Selected and All Test Cases
Prog. Potential Races Real Races
SimRT SP Random SP SimRT AP ABCAP Random AP SimRT SP Random SP SimRT AP ABCAP Random AP
LANG 96.9 50.3 100.0 71.5 52.2 96.9 50.3 100.0 71.5 52.2
HASHMAP 94.2 58.9 95.7 57.4 59.1 96.8 66.2 97.2 58.8 60.0
TREEMAP 98.1 39.8 100.0 48.5 40.8 - - - - -
ARRAYLIST 89.1 54.8 91.4 70.6 48.4 92.3 64.5 93.8 72.9 51.4
HASHTABLE 92.2 42.4 94.2 58.9 54.2 92.2 42.4 94.3 58.9 54.8
BITSET 97.5 64.6 96.8 88.4 69.2 97.8 70.4 96.8 88.6 71.5
LOG4J 85.7 47.5 90.0 47.8 48.5 89.2 56.5 90.6 48.4 48.8
WEBLECH 100.0 99.8 100.0 83.2 64.4 100.0 99.8 100.0 83.4 64.5
JIGSAW 88.4 52.8 91.2 72.9 56.6 93.6 58.6 94.8 78.0 61.4
Table 2: Race Detection Effectiveness
Prog. Potential races Real races
RTA RTSC SimRT RTA RTSC SimRT
LANG 8 8 8 8 8 8
HASHMAP 59 59 58 (-1) 10 10 10
TREEMAP 16 16 16 0 0 0
ARRAYLIST 22 22 22 3 3 3
HASHTABLE 3 3 3 2 2 2
BITSET 4 4 4 2 2 2
LOG4J 6 6 6 2 2 2
WEBLECH 29 29 29 1 1 1
JIGSAW 68(-1) 68(-1) 66(-3) 3 3 3
4.5.2 RQ2 and RQ3: Test Case Prioritization
Table 3 summarizes the APFD values computed for the test suites
and test case orders utilized in our study across all nine objects, for
both potential and real races, with SimRT SPandRandom SP
applied to selected test cases, and with SimRT AP,ABC APand
Random APapplied to the entire test suites.
To compare the effectiveness of different techniques, we dis-
play results in terms of improvement in race detection rate , calcu-
lated as:APFD T1 APFD T2
APFD T2*100%. This indicates the percentage
improvement in APFD achieved by technique T1over technique
T2. For example, for potential race detection, on object LANG , the
improvement in APFD achieved by SimRT SPoverRandom SP
was96:9 50:3
50:3*100% = 92.6%.
RQ2: Prioritization Results on Selected Test Cases.
On the six small objects, S IMRTSPoutperformed R ANDOM SP
in terms of APFD. The improvement ranged from 50.9% to 146.5%
for potential races, and from 38.9% to 117.5% for real races, and
the average improvement across the Ô¨Åve objects was 88.3% for po-
tential races and 67.7% for real races. On the three large objects,
SIMRTSPoutperformed R ANDOM SPin terms of rate of race de-
tection, with improvements of 80.4%, 0.2% and 67.4% for potential
races, and 57.9%, 0.2% and 59.7% for real races.
On WEBLECH , RANDOM SPperformed almost as effectively as
SIMRTSPbecause in that case, ISV P s were concentrated in sev-
eral methods, making it easier to achieve high coverage per selected
test case, and difÔ¨Åcult to achieve additional coverage.
RQ3: Prioritization Results on Entire Test Suites.
On the six small objects, when compared to ABC AP, SIMRTAP,
improved the potential race detection rate by 9.5% to 106.2%, with
an average improvement of 52.0%. When compared to R ANDOM AP,
SIMRTAPimproved the potential race detection rate by between
39.9% and 145.1%, with an average of 83.5%. Where detection of
actual races is concerned, when compared to ABC AP, SIMRTAP
improved the detection rate by 9.3% to 65.3%, with an average of
40.7%. When compared to R ANDOM AP, SIMRTAPimproved the
detection rate by 35.4% to 91.6%, with an average of 68.7%.
On the three large objects, when compared to ABC AP, SIMRTAP
improved the potential race detection rate by 88.3%, 20.2% and
25.1%, respectively. When compared to R ANDOM AP, SIMRTAP
improved the potential race detection rate by 85.6%, 55.2% and61.1%, respectively. Where detection of real races is concerned,
when compared to ABC AP, SIMRTAPimproved the race detec-
tion rate by 87.2%, 19.9% and 21.5%, respectively. When com-
pared to R ANDOM AP, SIMRTAPimproved the race detection rate
by 85.7%, 55.0% and 54.4%, respectively.
5. SUMMARY AND DISCUSSION
Summary of Results.
SIMRT‚Äôs RTS component was substantially more efÔ¨Åcient in ter-
ms of testing time than the retest-all and traditional RTS techniques.
Meanwhile, S IMRT detected the same sets of real races as the base-
line techniques. Although a few potential races were missed, the
number was small.
SIMRT‚Äôs TCP component, employed on selected test cases, was
substantially more effective in terms of APFD than the random
test case ordering. When employed on entire test suites, the TCP
component was substantially more effective in terms of APFD
than both additional-block-coverage and random techniques. Mean-
while, in this case, S IMRT was able to detect potential races that
were not detectable when operating on a subset of the test cases.
If these results generalize to other real objects and RTS and TCP
techniques, then if engineers wish to target race detection, SIMRT
is the best technique to utilize.
We now explore additional observations relevant to our study.
Multiple Runs. In our study, we executed each object once un-
der the selected test cases for each technique to assess race de-
tection effectiveness; a practical approach in resource-limited test-
ing environments. Although empirical studies have shown that the
Ô¨Çuctuation in race detection among multiple runs does not exceed
0.7% [4], we wished to determine whether multiple test runs could
impact the effectiveness of the techniques studied.
To do this, for each RTS technique, we ran each modiÔ¨Åed object
with the potential race detector and veriÔ¨Åer ten times. The results
indicated no differences in real race detection across the ten runs,
and differences in potential race detection were revealed in only
two cases. SpeciÔ¨Åcally, on HASHMAP , SIMRT detected one extra
race in two of the ten test suite runs, and missed one race in one
test suite run. RTA and RTSC did not display any differences. On
JIGSAW , RTA detected one extra race in two of the ten runs and
missed one race in one run. RTSC detected one extra race in three
of the ten runs, and S IMRT detected two extra races in three of
the ten runs, missing one race in one run. These results imply that
conducting one run for each test case is likely to be a reasonable
level of effort in resource-limited testing environments.
Effects of Recording Thread Coverage. SIMRT selects test cases
that potentially cover impacted shared variable pairs in the modi-
Ô¨Åed object with different threads. As such, S IMRT records thread
IDs for test history construction. To determine whether this is
useful, we further investigated whether savings could be attained
by considering thread coverage. We considered percentages of
test cases selected without using thread coverage, labeling this ap-56proach S IMRTN. SIMRTNselects test cases that potentially cover
impacted shared variable pairs in the modiÔ¨Åed object, regardless of
the number of threads involved in covering each pair.
For all nine objects, S IMRT yielded savings over S IMRTN; these
ranged from 16.5% to 83.7% with an average of 77.4% for the six
smaller objects and from 34.4% to 56.6% with an average of 44.9%,
for the three larger objects. Therefore, the approach substantially
improved the efÔ¨Åciency of regression test selection.
Numbers of Selected Test Cases. The numbers of test cases se-
lected by various techniques is another metric for measuring the ef-
Ô¨Åciency of regression test selection, providing an implementation-
independent view of efÔ¨Åciency. For the nine objects, RTSC se-
lected smaller test suites than RTA in many cases, with overall se-
lection percentages ranging from 45.8% to 100%. On four of the
nine objects, however, RTSC selected more than 90% of the test
cases, and in two cases it selected 100% of the test cases. S IMRT,
on the other hand, pruned away larger portions of the test suites,
with selection percentages ranging from 0.1% to 51.8%. In fact, for
seven objects, S IMRT selected fewer than 15% of the test cases.
Exposing Other Faults. SIMRT speciÔ¨Åcally targets races, and
therefore, may not be effective at detecting other classes of faults
that RTA or RTSC can reveal. As such, it is worth investigating
the performance of RTSC when combined with S IMRT. We thus
compared the testing time required by RTSC (with instrumenta-
tion for race detection) to that required by the use of both S IMRT
and RTSC without instrumentation (denoted by RTSC N). Both
RTSC and RTSC Ndetect races and other output-based regression
faults. This comparison showed that the savings of RTSC Nover
RTSC ranged from 43.5% and 67.6% (average 57.36%) across the
six smaller objects. On the larger objects, the savings were 68.5%,
22.9% and 31.2%, respectively. This suggests that race detection
and traditional fault detection should be considered separately.
Results such as these should be qualiÔ¨Åed. If S IMRT does not
substantially reduce the number of selected test cases over RTSC,
RTSC Nmay not achieve savings. Further, if engineers wish to
detect both data races and other regression faults, and S IMRT sub-
stantially reduces the number of test cases selected over traditional
RTS techniques, the best approach to use is: 1) run test cases se-
lected by traditional RTS techniques on uninstrumented objects and
2) run test cases selected by S IMRT on instrumented objects. Race
detection overhead varies across different types of applications. For
example, I/O-intensive applications tend to have small race detec-
tion overhead, and hence would beneÔ¨Åt less from S IMRT.
Further Discussion. We have used S IMRT to target data races, but
it can be adapted to detect other types of concurrency faults such
as atomicity violations [32, 33] and deadlocks [1, 19] by adjusting
the contents of coverage targets. For example, to detect atomicity
violations, instead of identifying potential impacted shared variable
pairs as coverage targets, S IMRT can identify potential unserializ-
able interleavings [33] as coverage targets.
As Table 1 shows, for some objects, there were a few uncovered
impacted shared variables remaining in the modiÔ¨Åed versions after
running the existing test cases. These variables may also cause data
races. Covering the impacted shared variables pairs associated with
these variables requires additional test cases or possibly thread in-
terleavings. To address this, we intend to further explore extending
SIMRT using test suite augmentation techniques [39].
6. RELATED WORK
There has been a great deal of work on analyzing, detecting, and
testing for data races [9, 34, 40, 41, 43]; however, existing tech-
niques do not consider software evolution.There has been some work on selecting and prioritizing sched-
ules for testing multithreaded programs such that faults can be ex-
posed faster [3, 26]. For example, CHESS prioritizes the explo-
ration of program state spaces to detect potentially erroneous inter-
leavings. Again, these techniques do not consider code changes.
There have been several techniques presented for systematically
exploring schedules in multi-threaded programs across versions [13,
17]. Gligoric et al. [13] reuse results from exploration of one pro-
gram version to speed up exploration of the next program version.
Jagannath et al. [17] use information about program changes in
software evolution to prioritize the exploration of schedules. These
techniques, however, target exploration of schedules within indi-
vidual test cases and do not address the challenges of regression
testing involving large sets of test cases. That said, we believe these
techniques can also be utilized by S IMRT. For example, once test
inputs have been selected or prioritized by S IMRT, we can further
apply these techniques to select or prioritize thread schedules for
each of the test cases.
Recent work by Deng et al. [4] studies how existing concurrency
fault detection tools work for a set of test inputs. They propose
a technique that Ô¨Årst measures coverage of a program, and then
selects a subset of test inputs to test for data races and atomicity
violations on that program. This technique focuses, however, on
single version programs and does not consider code changes. In
contrast, we reuse coverage information on the old programs and
select test inputs to test the new programs.
There has been a great deal of research on improving regression
testing through regression test selection and test case prioritization
(e.g., [7, 8, 21, 24, 30, 36, 37, 46, 51, 53]) ‚Äì Yoo and Harman [52]
provide a recent survey. Here, we restrict our attention to tech-
niques that share similarities with ours.
Some RTS techniques target speciÔ¨Åc fault classes [23, 50, 55].
Our own previous work [55] selects test cases associated with sys-
tem changes, but only those that are relevant to a set of internal
oracles that are known to be important for the system under test.
Staats et al. [47] propose a TCP technique that favors test order-
ings in which many variables impact the test oracle‚Äôs result early
in test execution. However, all the foregoing techniques focus on
sequential programs, and do not address concurrency faults.
7. CONCLUSION
We have presented an automated regression testing framework,
SIMRT, for use in detecting races that are induced due to code
changes. S IMRT employs both RTS and TCP techniques. We have
conducted an empirical study applying S IMRT to nine concurrent
code objects. We have empirically compared S IMRT to traditional
baseline techniques, and our results suggest that S IMRT‚Äôs test se-
lection component is more effective and efÔ¨Åcient than other ap-
proaches in terms of race detection. Our results also show that the
TCP technique employed by S IMRT is more effective than a tra-
ditional TCP technique and a random ordering in terms of rate of
race detection.
In the future, we intend to perform more extensive empirical
studies to evaluate S IMRT and extend S IMRT to detect other types
of concurrency faults such as atomicity violations and deadlock.
Finally, we will investigate the use of other regression testing tech-
niques such as test suite augmentation in conjunction with S IMRT.
8. ACKNOWLEDGMENTS
This work has been supported in part by the Air Force OfÔ¨Åce
of ScientiÔ¨Åc Research through award FA9550-10-1-0406 and the
Army Research OfÔ¨Åce through award W911NF-13-1-0154.579. REFERENCES
[1] R. Agarwal and S. D. Stoller. Run-time detection of potential
deadlocks for programs with locks, semaphores, and
condition variables. In Proceedings of the 2006 Workshop on
Parallel and Distributed systems: Testing and Debugging ,
2006.
[2] M. D. Bond, K. E. Coons, and K. S. McKinley. PACER:
Proportional Detection of Data Races. In Proceedings of the
2010 ACM SIGPLAN Conference on Programming
Language Design and Implementation , 2010.
[3] K. E. Coons, S. Burckhardt, and M. Musuvathi. Gambit:
Effective unit testing for concurrency libraries. In
Proceedings of the 15th ACM SIGPLAN Symposium on
Principles and Practice of Parallel Programming , 2010.
[4] D. Deng, W. Zhang, and S. Lu. EfÔ¨Åcient concurrency-bug
detection across inputs. In International Conference on
Object-Oriented Programming, Systems, Languages and
Applications , 2013.
[5] H. Do, S. G. Elbaum, and G. Rothermel. Supporting
controlled experimentation with testing techniques: An
infrastructure and its potential impact. Empirical Software
Engineering , 10(4):405‚Äì435, 2005.
[6] H. Do and G. Rothermel. On the use of mutation faults in
empirical assessments of test case prioritization techniques.
IEEE Transactions on Software Engineering , 32(9), Sept.
2006.
[7] H. Do and G. Rothermel. On the use of mutation faults in
empirical assessments of test case prioritization techniques.
IEEE Transactions on Software Engineering , 32(9), Sept.
2006.
[8] S. Elbaum, A. G. Malishevsky, and G. Rothermel. Test case
prioritization: A family of empirical studies. IEEE
Transactions on Software Engineering , 28(2), Feb. 2002.
[9] D. Engler, B. Chelf, A. Chou, and S. Hallem. Checking
system rules using system-speciÔ¨Åc, programmer-written
compiler extensions. In Proceedings of the 4th International
Conference on Symposium on Operating System Design and
Implementation , 2000.
[10] J. Erickson, M. Musuvathi, S. Burckhardt, and K. Olynyk.
Effective data-race detection for the kernel. In Proceedings
of the 9th USENIX Conference on Operating systems Design
and Implementation , 2010.
[11] C. Flanagan and S. N. Freund. FastTrack: EfÔ¨Åcient and
Precise Dynamic Race Detection. In Proceedings of the 2009
ACM SIGPLAN Conference on Programming Language
Design and Implementation , 2009.
[12] S. Fouch√©, M. B. Cohen, and A. Porter. Towards incremental
adaptive covering arrays. In The 6th Joint Meeting on
European Software Engineering Conference and the ACM
SIGSOFT Symposium on the Foundations of Software
Engineering: Companion Papers , 2007.
[13] M. Gligoric, V . Jagannath, and D. Marinov. Mutmut:
EfÔ¨Åcient exploration for mutation testing of multithreaded
code. In Proceedings of the Third International Conference
on Software Testing, VeriÔ¨Åcation and Validation , 2010.
[14] Guarded Blocks. http://docs.oracle.com/javase/tutorial
/essential/concurrency/guardmeth.html.
[15] R. L. Halpert, C. J. F. Pickett, and C. Verbrugge.
Component-based lock allocation. In Proceedings of the 16th
International Conference on Parallel Architecture and
Compilation Techniques , 2007.
[16] J. Huang, P. Liu, and C. Zhang. Leap: Lightweightdeterministic multi-processor replay of concurrent java
programs. In Proceedings of the 18th ACM SIGSOFT
International Symposium on Foundations of Software
Engineering , 2010.
[17] V . Jagannath, Q. Luo, and D. Marinov. Change-aware
preemption prioritization. In Proceedings of the 2011
International Symposium on Software Testing and Analysis ,
2011.
[18] http://jigsaw.w3.org.
[19] P. Joshi, C.-S. Park, K. Sen, and M. Naik. A randomized
dynamic program analysis technique for detecting real
deadlocks. In Proceedings of the 2009 ACM SIGPLAN
Conference on Programming Language Design and
Implementation , 2009.
[20] V . Kahlon, Y . Yang, S. Sankaranarayanan, and A. Gupta.
Fast and accurate static data-race detection for concurrent
programs. In Proceedings of the 19th International
Conference on Computer Aided VeriÔ¨Åcation , 2007.
[21] B. Korel, G. Koutsogiannakis, and L. Tahat. Application of
system models in regression test suite prioritization. In IEEE
International Conference on Software Maintenance , 2008.
[22] O. Laadan, N. Viennot, C.-C. Tsai, C. Blinn, J. Yang, and
J. Nieh. Pervasive detection of process races in deployed
systems. In Proceedings of the 23th ACM Symposium on
Operating Systems Principles , 2011.
[23] H. K. N. Leung and L. White. Insights into testing and
regression testing global variables. Journal of Software
Maintenance: Research and Practice , 2(4), 1990.
[24] Z. Li, M. Harman, and R. M. Hierons. Search algorithms for
regression test case prioritization. IEEE Transactions on
Software Engineering , 33(4), Apr. 2007.
[25] http://logging.apache.org/log4.
[26] M. Musuvathi and S. Qadeer. Iterative context bounding for
systematic testing of multithreaded programs. In
Proceedings of the 2007 ACM SIGPLAN Conference on
Programming Language Design and Implementation , 2007.
[27] G. J. Myers and C. Sandler. The Art of Software Testing . John
Wiley & Sons, 2004.
[28] M. Naik, A. Aiken, and J. Whaley. Effective static race
detection for java. In Proceedings of the 2006 ACM
SIGPLAN Conference on Programming Language Design
and Implementation , 2006.
[29] R. O‚ÄôCallahan and J.-D. Choi. Hybrid dynamic data race
detection. In Proceedings of the 8th ACM SIGPLAN
Symposium on Principles and Practice of Parallel
Programming , 2003.
[30] A. Orso, N. Shi, and M. J. Harrold. Scaling regression testing
to large software systems. In Proceedings of the 12th ACM
SIGSOFT International Symposium on Foundations of
Software Engineering , 2004.
[31] C. Pacheco and M. D. Ernst. Randoop: Feedback-directed
random testing for java. In Companion to the 22nd ACM
SIGPLAN Conference on Object-oriented Programming
Systems and Applications Companion , 2007.
[32] C.-S. Park and K. Sen. Randomized active atomicity
violation detection in concurrent programs. In Proceedings
of the 16th ACM SIGSOFT International Symposium on
Foundations of Software Engineering , 2008.
[33] S. Park, S. Lu, and Y . Zhou. Ctrigger: Exposing atomicity
violation bugs from their hiding places. In Proceedings of the
14th International Conference on Architectural Support for58Programming Languages and Operating Systems , 2009.
[34] R. Raman, J. Zhao, V . Sarkar, M. Vechev, and E. Yahav.
Scalable and precise dynamic datarace detection for
structured parallelism. In Proceedings of the 33rd ACM
SIGPLAN Conference on Programming Language Design
and Implementation , 2012.
[35] G. Rothermel and M. J. Harrold. Analyzing regression test
selection techniques. IEEE Transactions on Software
Engineering , 22(8), Aug. 1996.
[36] G. Rothermel and M. J. Harrold. A safe, efÔ¨Åcient regression
test selection technique. ACM Transactions on Software
Engineering and Methodology , 6(2), Apr. 1997.
[37] G. Rothermel, R. J. Untch, and C. Chu. Prioritizing test cases
for regression testing. IEEE Transactions on Software
Engineering , 27(10), Oct. 2001.
[38] A. Salcianu and M. Rinard. Pointer and escape analysis for
multithreaded programs. In Proceedings of the Eighth ACM
SIGPLAN Symposium on Principles and Practices of
Parallel Programming , 2001.
[39] R. Santelices, P. K. Chittimalli, T. Apiwattanapong, A. Orso,
and M. J. Harrold. Test-suite augmentation for evolving
software. In Proceedings of the 23rd IEEE/ACM
International Conference on Automated Software
Engineering , 2008.
[40] S. Savage, M. Burrows, G. Nelson, P. Sobalvarro, and
T. Anderson. Eraser: A Dynamic Data Race Detector for
Multithreaded Programs. ACM Transactions on Computer
Systems , 15(4), 1997.
[41] K. Sen. Race directed random testing of concurrent
programs. In Proceedings of the 2008 ACM SIGPLAN
Conference on Programming Language Design and
Implementation , 2008.
[42] K. Serebryany and T. Iskhodzhanov. ThreadSanitizer: Data
race Detection in Practice. In Proceedings of the Workshop
on Binary Instrumentation and Applications , 2009.
[43] T. Sheng, N. Vachharajani, S. Eranian, R. Hundt, W. Chen,
and W. Zheng. RACEZ: A Lightweight and Non-invasive
Race Detection Tool for Production Applications. In
Proceedings of the 33rd International Conference on
Software Engineering , 2011.
[44] Soot: a Java Optimization Framework.
http://www.sable.mcgill.ca/soot/.
[45] F. Sorrentino, A. Farzan, and P. Madhusudan. Penelope:
Weaving threads to expose atomicity violations. In
Proceedings of the 18th ACM SIGSOFT International
Symposium on Foundations of Software Engineering , 2010.
[46] A. Srivastava and J. Thiagarajan. Effectively prioritizing tests
in development environment. In Proceedings of the 2002
ACM SIGSOFT International Symposium on Software
Testing and Analysis , 2002.
[47] M. Staats, P. Loyola, and G. Rothermel. Oracle-centric test
case prioritization. In Proceedings of the 23rd IEEE
International Symposium on Software Reliability
Engineering , 2012.[48] J. W. V oung, R. Jhala, and S. Lerner. Relay: Static race
detection on millions of lines of code. In Proceedings of the
15th ACM SIGSOFT International Symposium on
Foundations of Software Engineering , 2007.
[49] http://weblech.sourceforge.net.
[50] L. White and B. Robinson. Industrial real-time regression
testing and analysis using Ô¨Årewalls. In Proceedings of the
20th IEEE International Conference on Software
Maintenance , 2004.
[51] W. E. Wong, J. R. Horgan, S. London, and H. A. Bellcore. A
study of effective regression testing in practice. In
Proceedings of the Eighth International Symposium on
Software Reliability Engineering , 1997.
[52] S. Yoo and M. Harman. Regression testing minimisation,
selection and prioritisation: A survey. Software Testing,
VeriÔ¨Åcation and Reliability , 22(2), 2012.
[53] S. Yoo, M. Harman, P. Tonella, and A. Susi. Clustering test
cases to achieve effective and scalable prioritisation
incorporating expert knowledge. In Proceedings of the
Eighteenth International Symposium on Software Testing and
Analysis , 2009.
[54] J. Yu, S. Narayanasamy, C. Pereira, and G. Pokam. Maple: A
coverage-driven testing tool for multithreaded programs. In
Proceedings of the ACM International Conference on Object
Oriented Programming Systems Languages and
Applications , 2012.
[55] T. Yu, , X. Qu, M. Acharya, and G. Rothermel. Oracle-based
regression test selection. In Proceedings of the Sixth IEEE
International Conference on Software Testing, VeriÔ¨Åcation
and Validation , 2013.
[56] T. Yu, W. Srisa-an, and G. Rothermel. Simtester: A
controllable and observable testing framework for embedded
systems. In Proceedings of the 8th ACM SIGPLAN/SIGOPS
Conference on Virtual Execution Environments , 2012.
[57] T. Yu, W. Srisa-an, and G. Rothermel. An empirical
comparison of the fault-detection capabilities of internal
oracles. In IEEE 24rd International Symposium on Software
Reliability Engineering , 2013.
[58] T. Yu, W. Srisa-an, and G. Rothermel. Simracer: An
automated framework to support testing for process-level
races. In Proceedings of the 2013 International Symposium
on Software Testing and Analysis , 2013.
[59] W. Zhang, C. Sun, and S. Lu. Conmem: Detecting severe
concurrency bugs through an effect-oriented approach. In
Proceedings of the 15th International Conference on
Architectural Support for Programming Languages and
Operating Systems , 2010.
[60] P. Zhou, R. Teodorescu, and Y . Zhou. Hard:
Hardware-assisted lockset-based race detection. In
Proceedings of the 2007 IEEE 13th International Symposium
on High Performance Computer Architecture , 2007.59