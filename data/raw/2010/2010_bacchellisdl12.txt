Content Classiï¬cation of Development Emails
Alberto Bacchelli, Tommaso Dal Sasso, Marco Dâ€™Ambros, Michele Lanza
REVEAL @ Faculty of Informatics â€” University of Lugano, Switzerland
Abstract â€”Emails related to the development of a software
system contain information about design choices and issues
encountered during the development process. Exploiting the
knowledge embedded in emails with automatic tools is chal-
lenging, due to the unstructured, noisy and mixed language
nature of this communication medium. Natural language text
is often not well-formed and is interleaved with languages with
other syntaxes, such as code or stack traces.
We present an approach to classify email content at line
level. Our technique classiï¬es email lines in ï¬ve categories ( i.e.,
text, junk, code, patch, and stack trace) to allow one to sub-
sequently apply ad hoc analysis techniques for each category.
We evaluated our approach on a statistically signiï¬cant set
of emails gathered from mailing lists of four unrelated open
source systems.
Keywords -Empirical software engineering; Unstructured
Data Mining; Emails
I. Introduction
Software repositories supply information useful for sup-
porting software analysis and program comprehension [17].
Dierent repositories o er dierent perspectives on systems:
For example, issue reports open a view on defective entities,
thus enabling studies on defect location and prediction [14],
while repositories archiving communication occurred among
developers contain valuable â€œinformation [that helps] develop-
ers resolve crucial questions about the history, rationale, and
future plans for source codeâ€ [36]. In particular, development
emails contain discussions on topics ranging from implemen-
tation details to high-level design. The main maintainer of the
Real-Time Linux explains that â€œmailing list archives provide
a huge choice of technical discussionsâ€ and that developers
do not write documentation because they believe that â€œit is
all documented in the [...] mailing listâ€ [18]. Email data helps
to understand a system, its history, and its design rationale,
thus supporting program comprehension [4]; email data also
â€œoers the best opportunity for a researcher to observe the
development process,â€ as â€œdevelopers reveal their thought
processes most naturally when communicating with other
software developersâ€ [30].
However, obtaining objective and accurate results from
software repositories is not trivial: The information extracted
must be relevant ,unbiased , and its contribution comprehen-
sible . Researchers are studying repositories to understand
what information is more relevant ( e.g., in bug reports [37]
or in code changes [22]) and the impact of data quality on
mining approaches and analyses [10], [34]. In particular,
extracting valuable data from communication repositories(e.g., IRC chat logs, mailing lists) require the most care, as
the documents leave complete freedom to the authors. For
example, Bettenburg et al. presented the risks of using email
data without a proper cleaning pre-processing phase [9].
NL documents are usually treated as bags of words â€“a
count of termsâ€™ occurrences. This simpliï¬cation is proven
to be e ective in the information retrieval (IR) ï¬eld, where
techniques are tested on well-formed NL documents [25]. In
software engineering, although e ective for some tasks ( e.g.,
traceability between documents and code [1]), this approach
reduces the quality, reliability, and comprehensibility of the
available information, as NL text is often not well-formed
and is interleaved with languages with di erent syntaxes:
code fragments, stack traces, patches, etc.
We present a work for advancing the analysis of the
contents of development emails. We argue that we should not
create a single bag with terms indiscriminately coming from
NL parts, code fragments, email signatures, patches, etc.and
treat them equally. We need to recognize every language in
an email to enable techniques exploiting the peculiarities of
each category. This work contributes to a deeper and more
detailed analysis of email communication among developers.
We propose an approach, based on a combination of
parsing techniques and machine-learning (ML) methods,
to classify the contents of development emails in ï¬ve
categories: NL, source code, patch, stack trace, and
junk (text that does not add valuable information, such
as auto-generated disclaimers, authorsâ€™ signatures, or
erroneous characters). Our technique works at the line level,
whichâ€”by inspecting hundreds of emailsâ€”we found to be
the appropriate granularity for email content classiï¬cation.
We created a web application to manually classify email
content in the chosen categories. We classiï¬ed a statistically
signiï¬cant set of emails from four javaopen source software
(OSS) systems, used to evaluate the accuracy of our approach.
The contributions of this paper are:
1)a novel approach that fuses parsing and ML techniques
for classiï¬cation of email lines;
2) a web application to manually classify email content;
3)the manual classiï¬cation of a statistically signiï¬cant
sample set of emails (for a total of 67,792 lines) from
mailing lists of four di erent software systemsâ€“in the
form of a freely available benchmark; and
4)the empirical evaluation of our approach against the
benchmark.Structure of the paper. In Section II we motivate our
work. In Section III we describe the related work. In Sec-
tion IV we show how we collected and manually annotated
the email data. In Section V we detail our classiï¬cation
methods and their evaluation. We discuss threats to validity
in Section VI and conclude in Section VII.
II. M otivation
Figure 1 shows the body of an example development
email. Due to the variety of languages used, if we consider
the content of such email as a single bag of words, we would
obtain a motley set of ï¬‚attened terms without a clear context,
thus severely reducing quality and amount of available
information. Inversely, by automatically distinguishing the
parts composing the email, we support many tasks, such as:
(1)  Alice wrote:(2)  > On Mon 23, Bob wrote:(3)  >> Dear list,(4)  >> When starting up ArgoUML on my MacOS X system (Java 2)  (5)  >> it throws a NullPointerException very soon. You'll ï¬nd the (6) >> trace below. I hope someone knows a solution. Thanks a lot!(7) >> Exception in thread "main" java.lang.NullPointerException(8) >> at(9) >> javax.swing.event.SwingSupport.ï¬reChange(SwingChange.java)(10)>> at javax.swing.AbstractAction.setEnabled(AbstractAction.java)[...](11)>> at uci.uml.Main.main(Main.java:148)(12)> I'm sorry I can't help you Bob but thanks for sharing the stack...(13)> Alice.(14)> -- (15)> "Beware of programmers who carry screwdrivers." --L. Brandwein(16)Alice, I believe we must change Explorer.java to ï¬x Bob's problem:(17) public void setEnclosingFig(Fig each) {(18)  super.setEnclosingFig(each);(19)  if (each != null || (each.getOwner() instanceof MPackage)) {(20)   m = (MPackage) each.getOwner(); }(21)The problem is in the condition, I attach the diff with this version:(22)--- src/org/argouml/ui/explorer/Explorer.java(revision 14338)(23)+++ src/org/argouml/ui/explorer/Explorer.java (working copy)(24)@@ -147,1 +147,1 @@[...](25)    super.setEnclosingFig(each);(26)  - if (each != null || (each.getOwner() instanceof MPackage)) {(27)  + if (each != null && (each.getOwner() instanceof MPackage)) {(28)    m = (MPackage) each.getOwner(); }(29)I hope this change is ï¬ne by you, if so, please apply it =)(30)Cheers, Carl.(31)-- I used to have a sig, but it took up much space so I got rid of it!(32)---------------------------------------------------------------------(33) To unsubscribe, e-mail: dev-...@argouml.tigris.org(34) For additional commands, e-mail: dev-...@argouml.tigris.orgjunkNL textpatchstack tracesource code
Figure 1. Example development email with mixed content
Traceability recovery. In Figure 1, the email is referring
to several classes ( e.g., Main ,Fig, and MPackage ), but only
the class Explorer is critical to the discussion: It causes the
failure and the emailâ€™s author is changing it to provide a
solution. We realize the importance of Explorer by reading
the NL line 16. As part of our ongoing investigation on
email archives [4], we often found this pattern: Artifactsmentioned in NL parts of emails are more relevant to the
discussion than artifacts mentioned in other contexts ( e.g.,
stack traces). A traceability method based on bags of words
(e.g., [5]) cannot recognize whether references to artifacts
appear in a NL context, to increase the link relevance. Such
a method can only use the number of occurrences to weight
more certain terms [25], leading to imprecise results. In
Figure 1, a weighting based on occurrences would give the
most relevance to class MPackage (mentioned 5 times), which
is actually marginal to the discussion.
By recognizing the context in which a term appears, one
can elicit weights for words appearing in a document dy-
namically and more accurately, improving the traceability
linksâ€™s quality and giving more information to the user.
Stop words removal. To better characterize documents,
IR research invites to remove stop words ,i.e.,very common
words [25], thus weighting more the peculiar terms of a
document. This approach is less beneï¬cial when applied to
development emails: By removing stop words, one reduces
the noise in NL parts, but also deletes information in parts
with a di erent vocabulary ( e.g., source code). For example,
deleting the stop word â€œeachâ€ from the content of Figure 1
means also deleting a variable name in a code fragment
(lines 17â€“20) and a patch (25â€“28). This is suboptimal, since
variable names provide relevant information [23]. Similarly,
we delete important information by removing programming
language keywords from NL.
By recognizing the di erent parts that compose an email,
one can use di erent common terms removal techniques,
exposing the most relevant information.
Artifact summarization. Due to the amount of data
produced during a systemâ€™s evolution, researchers investigated
how to expose only the signiï¬cant parts to reduce information
overload ( e.g., [28]). The proposed techniques are tailored to
speciï¬c types of artifact ( e.g., code [19], NL documents [20])
and cannot be applied to mixed documents, such as emails.
By recognizing the di erent parts of an email, one can
use the most suited summarization technique according
to each partâ€™s type and extract correct information.
Fact extraction. To know the facts expressed in code
fragments, patches, or stack traces, one can use ad hoc parsers.
In Figure 1, using a parser for patches, one recognizes that
the ï¬le being modiï¬ed is Explorer (lines 22â€“23). Similarly,
NL text can be analyzed with NLP techniques [21]. However,
ad hoc parsers cannot be applied to mixed content, as they
are not robust enough to manage unexpected data.By distinguishing the type of each email line, we can
exploit ad hoc analysis techniques to extract precise
information.
Non-essential information removal. In Figure 1, 8 lines
out of 34 contain irrelevant dataâ€“â€œjunkâ€. Previous research in-
dicated how some changes in version history are not essential,
and how their detection and ï¬ltering can improve change-
based analysis techniques [22]. Similarly, the detection and
removal of junk from email content increase the quality of
the data [9], thus improving the quality of analyses.
By recognizing the noise in emails, the important data
emerges, improving the information extraction quality.
III. R elated Work
Researchers applied NL analysis techniques to software-
related documents and devised approaches to improve the
comprehension of the NL parts. For example, Dekhtyar et
al.[15] discussed the promises and perils of text mining
for NL software artifacts. Here we focus on research on the
recognition of the di erent parts that compose NL artifacts.
The work by Bettenburg et al. [9] focuses on making the
research community aware of the noise in email data and
presents the importance of a proper cleaning pre-processing
phase. The authors suggest possible ï¬ltering heuristics to
recognize noise and irrelevant information. Later, Bettenburg
et al. devised infoZilla, a tool to recognize and extract
patches, stack traces, source code snippets, and enumerations
in the textual descriptions that accompany issue reports [8].
It is composed of four independent ï¬lters, one per category,
which are used in cascade to process the text. The source code
ï¬lter exploits an approach inspired by island parsing [27],
while the others are based on text matching implemented
through regular expressions. In the task of di erentiating
documents ( i.e.,deciding whether they contain or not each
category), infoZilla reached almost perfect results, with
precision and recall values above 0.95 in all the categories.
InfoZilla has been e ectively applied to investigate relevant
features of text in issue reports [37].
Compared to bug comments, development emails present
the following di erences: (1) they contain a larger NL
vocabulary, since the discussion is not limited to bug related
issues; (2) they present more noise, generated for example
by email headers and authorsâ€™ signatures; and (3) emails
pose greater challenges in text recognition, since many email
clients automatically wrap long lines of text, thus breaking
the right formatting [11]. Bird et al. proposed an approach to
measure the acceptance rate of patches submitted via email in
OSS projects [11]. They extracted code patches from emails
and used them to analyze the developersâ€™ interactions.
Some information retrieval approaches targeted the clas-
siï¬cation of text or the recognition of information withspeciï¬c patterns [21], exploiting probabilistic and ML models
(e.g., Maximum Entropy Models [7] or Hidden Markov
Models [6]). Tang et al. addressed the issue of cleaning
the email data for text mining [33]. The authors proposed a
four-step approach to clean emails: (1) non-NL text ï¬ltering,
(2) paragraph recognition, (3) sentence boundaries detection,
and (4) word normalization. Their method ï¬rst ï¬lters out
email headers, signatures, and program code ( without a
distinction from patches or stack traces); then it recognizes the
paragraphs and sentences that compose the remaining NL text;
ï¬nally, it corrects misspelled words. The authors randomly
chose a total of 5,459 emails from 14 unrelated sources
(e.g., newsgroups at Google) and created 14 data sets in
which they manually labeled headers, signatures, quotations,
and program codes. Given the labelled data, the authors
implemented a classiï¬er for each step of their approach. All
the classiï¬ers use Support Vector Machines (SVM) and are
based on speciï¬c features ( e.g., number of words). At line
level classiï¬cation, they achieved an f-measure of 0.81 in
recognizing code, and 0.98 and 0.90 for header and signature.
Carvalo and Cohen devised methods to recognize signature
blocks and reply lines in emails [13]. They worked at the
line level and tested the e ectiveness of a set of features
with many ML classiï¬ers. In the signature detection task the
methods reached an f-measure value of 0.97.
In our previous work we proposed Besc, alexical approach
to recognize the lines of development emails that contain
Javacode fragments [3]. Even though Bescachieves good
results in terms of e ectiveness and practical performance,
it speciï¬cally focuses on recognizing code and can only be
partially used in the context of a more comprehensive email
text classiï¬cation. For example, Bescmerges lines of stack
traces, patches, and actual source code, under the umbrella
of code fragments: In Figure 1, it would indiscriminately
recognize lines 9â€“11, 17â€“20, and 25â€“28 as code fragments.
Although such an approach can be useful for certain system
analyses (see [3]), it generates a classiï¬cation that does not
allow one to (1) distinguish lines written in NL; (2) recognize
patch context and headers ( e.g., lines 22â€“24 in Figure 1);
(3) distinguish complete blocks of stack traces, ( e.g., lines
7â€“11, to use ad hoc parsers); (4) remove the non-relevant
information ( i.e.,â€œjunkâ€).
Summing up, previous work di ers from the current as it:
addressed more compact classiï¬cation tasks, for example
only detecting patches [11] or signatures [13];
considered a larger granularity or di erent data sources
(e.g., bug reports [8]);
did not distinguish structured data forms ( e.g., by
merging patches, code, and stack traces [3], [33]);
hard-code all the classiï¬cation rules, thus not covering
unexpected cases ( e.g., [3]).
We strive for an approach with a ï¬ne granularity and a wide
breadth, able to provide a robust classiï¬cation, which can be
used for increasing the quality of subsequent analyses.IV . D ataCollection and Classification
Since we strive for devising a method for reliably and
precisely classifying email lines, with the aim of improving
data quality and comprehension, we need data sets that are
accurate ,comprehensive , and of statistically signiï¬cant sizes.
This is critical for the validation and leads to more reliable
training for the supervised classiï¬cation methods. To this
aim, we implemented a web application to assist the manual
classiï¬cation of email content in categories.
A. Data Collection
Dierent software systems often use di erent applications
to manage email repositories. We tackled this issue by
importing data from MarkMail (http: //markmail.org), a web
service storing more than 8,000 up-to-date mailing lists.
Table I
Email data sets used in the experiment ,by system
SystemURLMailing listMailing listMailing listMailing listSystemURLInceptionEmailsEmailsEmailsSystemURLInceptionTotalAfter FilteringSampleArgoUMLargouml.tigris.orgJan 200025,53825,538379Freenetfreenetproject.orgApr 200023,13423,134378JMeterjmeter.orgJan 200624,0055,814361Minaorg.apache.mina.devFeb 200121,38414,499375Total94,06168,9851,493
Table I shows the four software systems and mailing lists
we considered. We selected unrelated systems emerging
from the context of di erent free software communities,
i.e., Apache, ArgoUML, and Freenet. The development
environment and paradigms, and the usage of the mailing lists
are likely to di er, thus mitigating external validity threats.
We imported all the messages starting from the mailing list
inception (second column in Table I) to the end of 2010. The
only pre-processing conducted on the emails was ï¬ltering
out messages automatically generated by the bug tracking
system and the versioning system.
From each ï¬ltered mailing list, we extracted statistically
signiï¬cant sample sets (last column, Table I), which were
used by the approach without any pre-processing on the text.
Since we had no prior knowledge on the distribution of line
categories in the populations, we opted for simple random
sampling [35] to pick the emails. The chosen sizes have a
95% conï¬dence level1and a 5% error margin.
B. Data Classiï¬cation
To test our approach and train supervised ML classiï¬ers,
we needed to manually classify the 1,493 sample emails. To
ease this manual task and alleviate its error-proneness, we
devised Mailpeek , a web application written in Smalltalk
using the Seaside framework [16]).
1See [5], [35] for more information about sample size determination.
IV
I
III
V
VI
IIFigure 2. Mailpeek: our web app for classifying email content
Figure 2 shows the main window of Mailpeek , as it appears
in a web browser after a user selects a mailing list of interest
and the application extracts a random email among those not
automatically ï¬ltered. Mailpeek displays the email metadata
(point I) and content (point II), with vertical bars to show
indentation levels and increase readability.
Users conduct the classiï¬cation task at the character
level: To label a block, they (1) click on starting and
ending characters, (2) verify the correctness of the selection
(which is shown in a yellow background), and (3) apply
the appropriate category, either by clicking on a button in
the left menu (point III), or using keyboard shortcuts. The
character granularity provided us the basis to decide which
granularity was appropriate for the automatic classiï¬cation,
i.e.,line granularity (see Section IV-C).
When users hover with the mouse on any character in
the email content area (point II), the character font size
triples (point IV). According to Fittsâ€™ Law [24], this eases
the selection, thus decreasing fatigue and errors.
Once an email is completely classiï¬ed, the user clicks on
save (point V) and Mailpeek loads another random email
among those not yet classiï¬ed. The skip link allows the user
to leave out non-valid emails that were not removed by the
ï¬ltering phase. The top menu (point VI) allows users to
change mailing list or trigger the importer.
Two graduate students from the REVEAL Research Group
at the University of Lugano, with several years of Java
programming experience, conducted the manual classiï¬cation
task on two distinct sets of emails. We evaluated the inter-
rater agreement by asking them to also classify 5% of the
emails analyzed by the other person. In this sample, we
found 12 non concordant lines (less than 0.2%).C. Data Distribution
Table II reports categoriesâ€™ distributions in the sample sets.
Table II
Distribution of the categories per line ,by system
ArgoUMLArgoUMLFreenetFreenetJMeterJMeterMinaMinaTotalTotalNL Text10,94547.2%7,92359.6%7,77841.8%6,49651.2%33,14248.9%Junk11,12247.9%4,09630.8%9,73452.3%4,63336.5%29,58543.6%Patch4702.0%9867.4%3391.8%2872.3%2,0823.1%Source Code3041.3%290.2%5913.2%9907.8%1,9142.8%Stack Trace3641.6%2541.9%1650.9%2862.3%1,0691.6%TotalTotal23,20523,20513,28813,28818,60718,60712,69212,69267,79267,792
Most lines are NL; more than 30% of lines are junk, thus
stressing the impact of noise on email data; the frequency of
other categories is lower and the ranking changes according
to the mailing list. The di erent composition of the email
setsâ€™ contents reï¬‚ects the di erent usage of mailing lists
among the communities. Some lines are hybrid : they belong
to more than one category, and are mostly composed of junk
not separated by the NL text. They account for less than 5%
of the population ( i.e.,3,362 lines). To mitigate the bias in
the experiment we include them as separated instances.
V . E xperiment
We created a number of techniques based on ideas gathered
both from the IR ï¬eld, which we reshaped and adapted,
and from language programming parsing. Even though the
techniques can be used in isolation, we achieved the best
results by creating a uniï¬ed approach.
A. Term Based Classiï¬cation
In IR systems, documents are considered as bags of words,
where syntactic information, ordering, and constituency of the
words play no role in determining their meaning. In practice
each document is modeled as a vector of features, which
correspond to terms in the corpus vocabulary. For example, if
we consider a document ( d), the cardinality of the vocabulary
(jCj), and how many times each term ( ti) occurs in d, we could
deï¬ne the document vector as: vd=[t1(d);t2(d); : : : ; tC(d)].
This simple vector modeling has been widely used with
supervised ML algorithms to achieve very e ective results in
automatic text classiï¬cation [25], [31]. We ground the ï¬rst
techniques on the same basis: We consider lines as vectors
of terms and use ML for their classiï¬cation.
In the following we we describe and motivate our choices
in terms of the used ML technique and vector features ( i.e.,
terms), which cannot be based on results from the IR ï¬eld,
as they refer to other domains and classiï¬cation tasks.
1) Machine-learning method: We employ Na Â¨Ä±ve Bayes,
a method of supervised learning ( i.e.,ML algorithms that
use classiï¬ed training examples to infer the classiï¬cation
function). Na Â¨Ä±ve Bayes relies on the conditional independence
assumption: The presence of a feature is unrelated to theoccurrence of the other features. Even though the assumption
is a strong simpliï¬cation, the method often outperforms
more sophisticated techniques [21]; in particular in text
classiï¬cation, NaÂ¨ Ä±ve Bayes achieves signiï¬cant results [12].
An asset of Na Â¨Ä±ve Bayes is its linear complexity, which
allows training and classiï¬cation to be performed e ciently,
even with a very large number of features.
The method uses Bayesâ€™ rule [21] to compute the proba-
bility that a line l, made of tkterms, belongs to class c:
P(cjl)P(c)Y
kP(tkjc) (1)
It computes the posterior probability P(cijl) for each class
and chooses the one with the highest probability. This is the
maximum a posteriori (MAP) hypothesis [21]:
CMAP=arg max
cj2CP(cjl)arg max
cj2CP(c)Y
kP(tkjc) (2)
If we want to classify the line d=â€œAlice wrote :â€ as text,
junk, orcode , the algorithm ï¬rst computes the probabilities
as:P(textjl)=0:43,P(junkjl)=0:55 and P(codejl)=0:02,
then selects the value 0.55, thus classifying lasjunk.
2) Selection of the terms: Words: They are the funda-
mental tokens of all the languages we want to classify. We
judge the words in our corpus of 67,792 non-empty lines
to be proper features for line modeling. Contrarily to most
IR methods, we do perform neither stop word removal (i.e.,
excluding very common words), as we expect very frequent
words to be representative of a class ( e.g., Javakeywords
in code), nor stemming (i.e.,collapsing the morphological
variants of a word), as we expect some variants to be more
characteristic of certain classes ( e.g., verb tenses in NL text).
Punctuation: We must distinguish lines written in languages
with di erent syntaxes, thus we consider punctuation to be a
valuable aspect. Unless the punctuation marks are separated
by words or spaces ( e.g., the dots in javax.swing. , are two
occurrences of the feature â€œ.â€), we consider them as a single
term, thus recognizing special characters, such as â€œ@@â€ in
line 24 in Figure 1. We do not consider email reply threading
characters ( e.g., >and >>in lines 2-15 in Figure 1) at this
point, as they do not have a deï¬nite role for line classiï¬cation.
Bi-grams: NaÂ¨Ä±ve Bayes relies on the conditional indepen-
dence assumption, which makes the modeling of NL text
features feasible. However, the other considered languages
have a stricter syntax with patterns of terms appearing to-
gether ( e.g., â€œpublic voidâ€ in code). To model this dependency
characteristic of some terms, thus also reducing the negative
eects of Na Â¨Ä±ve Bayesâ€™ assumption, we also consider bi-
grams (i.e.,pairs of terms appearing one after the other).
Context: All the features considered so far are extracted
only from the line under classiï¬cation. However, some of
the considered classes ( i.e.,patch and stack trace) have a
structure recognizable only if considering surrounding lines.
For example, line 18 and line 25 have the same content,thus can be mapped to the correct class only considering the
context lines. Researchers proposed to solve a similar problem
by adding features with characteristics of lines close to the
one under classiï¬cation [13], [33]. We adapt this approach
to our case by considering what appears in the preceding
and following lines. For example, in addition to â€œ@@â€, we
have the features â€œ@@-lineBeforeâ€, and â€œ@@-lineAfterâ€.
Table III
Results with term based classification ,by feature sets
Numberof Features10-fold cross validation10-fold cross validation10-fold cross validationMailing list cross validationMailing list cross validationMailing list cross validationNumberof FeaturesCorrect LinesCorrect LinesImpr. sig.Correct LinesCorrect LinesImpr. sig.Words12,65846,55568.6%46,05667.9%Words, Punctuation19,38462,93892.8%p < 0.00158,17285.8%p < 0.001Words, Punctuation, Bi-grams145,18763,41393.5%p < 0.00158,56886.4%p < 0.001Words, Punctuation, Bi-grams, Context435,56163,70893.9%p < 0.00160,58089.4%p < 0.001
3) Line modeling: After deï¬ning the aforementioned
features, we modeled each line as vector a of n+1 dimensions.
The ï¬rst nelements are the chosen features, while the last
one is the manual classiï¬cation value ( e.g., â€œpatchâ€). The
ï¬rst column of Table III shows the values of naccording to
the considered subset of features. Each feature is populated
with the corresponding termâ€™s occurrences in the line.
B. Training and Testing
Since we use a supervised ML algorithm, we need to
train it on classiï¬ed data. We use two di erent approaches
for training the model and show how this a ects the results
when testing of the modelâ€™s accuracy. To evaluate the modelâ€™s
accuracy, we count the number of correctly classiï¬ed lines
and we use two IR metrics [25]: precision (P=jT Pj
jT P+FPj) and
recall (R=jT Pj
jTOTj).T P(true positives) are correctly classiï¬ed
lines, FP(false positives) are not correctly classiï¬ed lines,
and TOT is the total number of lines. F-measure is the
weighted harmonic mean of PandR[25].
1) Ten-fold stratiï¬ed cross-validation: As a ï¬rst step, we
apply 10-fold stratiï¬ed cross validation [35]: We split the
dataset in 10 folds, use 9 folds (90% of the lines) to train
the prediction model, and use the remaining fold to test the
modelâ€™s accuracy. This process is repeated 10 times rotating
the training and testing folds. The distribution of classes
is kept equal in training and test sets. Columns 2 and 3
in Table III show the results. Each subset of features adds
information that increases the results in a signiï¬cant way
(column 3). When considering all the features, the ratio of
correctly classiï¬ed instances reaches almost 94%.
2) Mailing list cross-validation: Dierent mailing lists
discuss about di erent systems and are likely to use di erent
words and jargon. For example the mailing list signature ( e.g.,
lines 32â€“34 in Figure 1) have di erent terms in each mailing
list. Thus, term-features that work for one mailing list may not
be useful for others. To better test the generalizability of the
results achieved by the classiï¬er, we conduct a â€œmailing listcross validation.â€ In practice, it is a 4-folds cross validation,
in which folds are neither stratiï¬ed nor randomly taken, but
correspond exactly to the di erent mailing list: We train the
classiï¬ers on three mailing lists and we try to predict the
classiï¬cation of the remaining mailing list. We do this four
times rotating the mailing lists and we measure the average
results. Columns 4 and 5 in Table III show the results.
As expected, testing with mailing list cross validation, the
performance of the classiï¬er drops, even when considering
all the features. However, this is a more relevant test to
understand the results of the classiï¬er applied to unseen Java
development mailing lists, and we use it in following.
Table IV
Mailing list cross validation on the best set of features
classified as â™classified as â™NL TextJunkPatchSourceCodeStackTracePrecisionRecallF-MeasureNL Text32,0621,04620860.8940.9670.929Junk3,26926,2255414230.9420.8860.913Patch20734394658510.4520.4540.453Source Code3091211,07441000.4030.2140.280Stack Trace3597009370.9690.8770.920
Table IV reports confusion matrix [25], precision, recall,
and F-measure values for the classiï¬cation with all the term-
features ( i.e.,words, punctuation, bi-grams, and context). The
best results are achieved in classifying text, junk, and stack
trace, while patch and code are often misclassiï¬ed among
themselves. This is reasonable, since recognizing those lines
requires a large context: Even a human reader could not
determine to which class line 28 in Figure 1 belongs without
inspecting many lines. However, di erentiating code and
patches might be useful for various tasks, such as improving
traceability links or automatically estimating the topic and
purpose of the email (see Section II).
C. Term Based Features and Overï¬tting
By considering the entire set of features ( i.e., words,
punctuation, bi-grams, and context), we obtain a complex clas-
siï¬cation model with more features than training instances. In
such a scenario, overï¬tting is likely to occurâ€”this hypothesis
is supported by the reduced performances of the classiï¬er in
mailing list cross validation (see Table III). By reducing the
features that are not valuable to correctly predict instances
outside the training set, we decrease overï¬tting and increase
the generalizability of the results.
Since we use words and punctuation to describe the
common traits of each language, we suppose that the terms
that rarely occur in the corpus are less relevant and can be
removed. We investigate this hypothesis by gradually ï¬ltering
out features (from all four kinds) that appear in less than t
lines and inspecting the results.
Figure 3 shows the average classiï¬erâ€™s performance in
mailing list cross validation, with tranging from 1 to 4,587
(higher values reduce the number of features to less than 104600 0 1000 2000 3000 40001
0.650.70.80.9Best result on Training, 1 line threshold
Best result on Testing, 11 lines threshold
Highest ratio between results
of testing and training, 548 lines thresholdTraining results
Testing results
Minimum number of lines in which a term must appear to be considered as a featureRatio of correctly classiï¬ed linesFigure 3. Results on training and test sets, by line threshold for features
greatly reducing the results). The blue dashed line (above)
is the average percentage of correctly classiï¬ed lines on the
training set, while the red solid line (below) is the average
percentage on the test set. The best result on the training set
(i.e.,96.1%) is set at tvalue of 1, ( i.e.,we consider all the
features, 115,864 on average when training on three mailing
lists), while the best result on the testing set ( i.e.,89.9%) is
set at tvalue of 11 ( i.e.,5,618 features on average), which
reduces some noise. The optimal tvalue for the best testing
set results, however, changes according to the mailing list:
Two lists have a tvalue of 2, one of 25, and one of 46. A
valid approach to ï¬nd a good value for t, also for unseen
data, is to consider the point with the highest ratio between
testing results and training results [35]. We ï¬nd this hot spot
with a threshold of 548 lines ( i.e.,122 features on average).
Interestingly the number of features is a tiny fraction of the
initial ones, but the testing results are reduced only by a 1.5%
(i.e.,88.3%). Higher thresholds lead to lower performances.
D. Parsing Based Classiï¬cation
We tackle the classiï¬cation from a di erent perspective
and use a di erent approach: parsing. In fact, three of the
considered classes ( i.e.,stack trace, patch, and source code),
which are either produced or consumed by a machine, present
a clearly structured and deï¬ned syntax that may be recognized
even if embedded in a noisy unstructured context. We use a
technique from our previous work [2] to write specialized
parsers per each class (excluding NL text), based on the
concept of island parsing [27]. For space reasons, we detail
only the most salient features of each parser. The complete
source code is available at http: //mucca.inf.usi.ch.
D.1. Stack Trace Parsing
We deï¬ne a terminology: The exceptionMessage is the NL
message included at the beginning of stack traces ( e.g., line
7 in Figure 1); the atLine is a method invocation within a
speciï¬c ï¬le ( e.g., lines 8â€“11); the ellipsisLine reduces lengthy
stack traces and has the form: â€œ. . . <number >moreâ€; the
causedByLine may be in any point in a trace and introduces
a nested trace and has the form: â€œCaused by: <stacktrace >â€.Among these elements, atLine s and ellipsisLine s are the
most recognizable ones. By using the concept of island pars-
ing[32] and the Smalltalk parser generator PetitParser [29],
we deï¬ned a grammar to obtain a parser to extract these two
elements, even if embedded in the noisy content of emails
or arbitrarily split on more lines, because of erroneous line
breaks. By testing our approach on the whole corpus we
found no errors in this parsing phase.
The exceptionMessage and the causedByLine elements
have an unpredictable structure ( e.g., dierent Javavirtual
machine versions may output the same error message
dierently), thus they cannot be parsed with a speciï¬c
grammar. We use a double-pass approach: First, we mark all
theatLine s and ellipsisLine s, then we look for each line that
contains strings such as â€œexceptionâ€, â€œerrorâ€, etc.When such
a line exists, if the next nlines belong to those lines marked
in the ï¬rst step, we classify it and all the lines up to the ï¬rst
atLine asstack trace . Since exceptionMessage s are made of
not more than three lines, we use an nof 3. The value can
be adapted if a system uses another message length.
For example, when we apply our stack trace parser to the
email in Figure 1, in the ï¬rst pass, it classiï¬es lines 8â€“11
asstack trace ; in the second pass, it considers lines 5 and 7
asexceptionMessage candidates, since they both contain the
string â€œexceptionâ€. Finally, it only picks line 7, because in
the next 3 lines there is an atLine (in this heuristic, we also
count the empty lines, such as the line between 6 and 7).
D.2. Patch Parsing
We deï¬ne a terminology: The ï¬rst two lines of a patch
(e.g., lines 22 and 23) are the patchHeader , and contain the
reference to the modiï¬ed ï¬le and, optionally, the revision
versions ( e.g., lines 22 and 23 in Figure 1); the lines
showing the changes done by the patch ( e.g., line 22) are
thepatchBlockHeader ; and all the lines in the chunk ( e.g.,
lines 25â€“28) are the patchBlock .
Similarly to the previous approach, we start from the
most recognizable lines and expand to include the more
ambiguous ones. The parsing is done in a single pass: We
wrote a grammar to generate a parser that recognizes the
patchHeader by using â€œâ€”â€, â€œ +++ â€, and â€œ@@â€ as hooks;
then it recognizes the patchBlockHeader (thanks to its clear
structure), then it matches the following patchBlock . The
patchBlock s are problematic, since they have variable length
and an unclearly deï¬ned ending. In fact, after the deleted
and added lines (which are marked with initial â€œ +â€ or â€œ-â€
signs, as in lines 26 and 27), patches include some contextual
lines: Their number may vary between zero and three, or
more if not well formatted. We implemented a lookahead
heuristic that checks whether the lines after the â€œ +â€ or â€œ-â€
signs might be good candidates as patch. The heuristic checks
whether the lines are source code, through a reduced version
of the code parser described later, and in the positive case it
classiï¬es them as patch .D.3. Source Code Parsing
Among the three classes with the most structured language
(i.e.,stack trace, patch, and source code), code is the most
ambiguous. This is due to the fact that email authors usually
do not report complete compilation units ( e.g., a whole Java
class deï¬nition), but only selected fragments ( e.g., the method
declaration in lines 17â€“20 in Figure 1). These fragments may
present more ambiguities, with respect to NL and junk, than
blocks of patches or stack traces: For example, if a line
comprises only the words â€œpublic classâ€, it can be either the
beginning of a class declaration or a simple NL sentence.
We devised a parser based on the technique detailed in
our previous work [2]: We wrote a complete Javagrammar
for PetitParser, by implementing the latest speciï¬cation
of the o cial Javalanguage. Then, we implemented an
island parser able to recognize most of the constructs of
the grammar (including single and multi-line comments, but
excluding constructs that are too ambiguous with NL, such
as expressions), starting from the most comprehensive ( i.e.,
compilation unit) down to very speciï¬c ones ( e.g., expression
statements). We also added rules to recognize incomplete
constructs (such as method declarations without the bodyâ€“
common in email discussions).
Compared to Besc[3], this island parser reaches higher
precision and is also able to locate constructs that span on
more lines. For example, Besccannot classify a line with
only â€œpublic classâ€ as code, while our island parser classiï¬es
it as code depending on the surrounding lines.
This parser matches most of the content of patchBlock s,
as they also contain valid source code. This raises a number
of false positives. We avoid this by chaining the code parsing
to the patch parsing: First we detect the patches, then, on the
lines that are notclassiï¬ed as patch, we use the code parser.
As a beneï¬cial side e ect, this chained procedure reduces
the text and the ambiguities to be managed by the island
parser, thus increasing the performances.
D.4. Junk Parsing
Noisy text, such as authorsâ€™ signatures, is hard to auto-
matically distinguish from NL text; however, some peculiar
common patterns can be matched with a parser. This approach
is made of three steps: (1) matching and classiï¬cation of
email headers ( e.g., lines 1 and 2 in Figure 1) with a regular
expression; (2) identiï¬cation and extraction of signatures of
mailing lists ( e.g., common lines added to the end of every
email sent to the same list, such as lines 32â€“34) and authors;
and (3) usage of the recognized signatures to automatically
compose a grammar for generating a parser to match them,
under any possible formatting or position in the email body.
To recognize signatures, we consider all the emails whose
last block of text is not quoted from previous emails (this
can be easily achieved by considering lines that do not
end with email reply threading characters, such as >and
>>in lines 2-15 in Figure 1). In these emails, the authorsthemselves conclude the message and most probably include
their signatures. For example, the email in Figure 1 contains
the authorâ€™s signature in the last block. Among the selected
emails, we only consider the last not quoted block. We
analyze it backward starting from the last line ( e.g., from
line 34 up to 16). When we encounter a line that starts with,
or is only composed of, two or more dashes, underscores,
or stars, we take out the lines up to the bottom and consider
this as a signature. The process continues until it reaches
the top of the not quoted block. For example, the algorithm,
applied to the email in Figure 1, would extract lines 32 to
34, and line 31 as block signatures.
By classifying these blocks as junk, we would miss the
cases in which signatures are in quoted text ( e.g., lines 14â€“
15). We, thus, conduct the third step: We use each extracted
string to automatically deï¬ne a grammar able to recognize
the signature in any possible position or formatting the text;
then, we use these grammars to automatically generate the
relative parsers; ï¬nally we classify matched lines as junk.
D.5. Results
Table V reports the results of each parser in the classiï¬ca-
tion of the lines into the corresponding type. For example,
the ï¬rst line covers the results in using the Stack trace parser
to classify lines as stack trace . The false positives ( e.g., 4
in the ï¬rst row) are lines classiï¬ed as stack trace by the
method, but with a di erent manual classiï¬cation.
Table V
Single classification results achieved by using parsers
Total InstancesTrue PositivesFalse PositivesPrecisionRecallF-MeasureStack trace parserPatch parserSource code parserJunk parser1,0691,05440.9960.9860.9912,0821,99601.0000.9590.9791,9141,715740.9590.8960.92629,58520,3722260.9890.6890.812
All the parsers reach high classiï¬cation values, while being
mailing list independent and requiring no training. However,
parsers have limitations: (1) They are manually implemented,
and for this reason they cannot predict or cover all the
possible variants of the patterns that they match, especially
due to truncated content; (2) the values reached in classifying
junk are lower than those achieved with the ML approach.
Next, we present a method that overcomes these issues by
fusing ML and parser-based approaches.
E. Uniï¬ed Approach
This approach fuses characteristics of the term based
classiï¬cation and the parser-based approaches.
1) Adding parsing results to NaÂ¨ Ä±ve Bayes: NaÂ¨Ä±ve Bayes
is not limited to use terms as features: One can include
any relevant aspect as a feature in the classiï¬cation process.
Given these premises, we add the parser-based classiï¬cation
output to improve the Na Â¨Ä±ve Bayes ML process. We do this
by adding four new features to the feature-vectors, in additionNaÃ¯ve Bayes machine-learningLearner
Classiï¬er
Decision tree machine-learningLearner
Classiï¬er
................
...............
Parser classiï¬ersParser classiï¬ers....
..................................
...
Manual Classiï¬cationManual Classiï¬cationAutomatic Classiï¬cationComparison
3
4
5
6
9
10MergeMerge
7
8Testing PhaseTraining PhaseCommit CommentsCommit CommentsEmailsMailing List 1
1
Commit CommentsCommit CommentsTesting SetCommit CommentsCommit CommentsTraining SetCommit CommentsCommit CommentsTraining Set
2Commit CommentsCommit CommentsEmailsMailing List 2Commit CommentsCommit CommentsEmailsMailing List 3Commit CommentsCommit CommentsEmailsMailing List 4Figure 4. Training and Test Process of the Uniï¬ed Classiï¬cation Approach
those presented in Section V-A. Each new feature maps the
output of a parser: The value is 1 when the corresponding
parser matches the speciï¬c line, 0 otherwise. We used Na Â¨Ä±ve
Bayes and performed mailing list cross validation.
Varying the value of the threshold t(see Section V-C),
we found the best average results to be at t=11. Table VI
shows the confusion matrix on the best results achieved by
adding the four parser-based features to the Na Â¨Ä±ve Bayes
approach. It correctly classiï¬ed 62,093 instances (91.3%),
1,513 more than the previous approach.
Table VI
Results adding parser -based features
classified as !classified as !NL TextJunkPatchSourceCodeStackTracePrecisionRecallF-MeasureNL Text31,89896097158290.9080.9620.934Junk3,08725,7873252031830.9620.8720.915Patch55291,71927810.7390.8260.780Source Code78131851,63620.7190.8550.781Stack Trace96101,0530.8300.9850.901
Comparing the confusion matrices of the ML approaches
(Table VI and IV), we see that the new features helped to
decrease the instances wrongly classiï¬ed as NL text. Being
NL the most frequent class (see Table II), it has a strong
impact on the evaluation of the MAP hypothesis of Na Â¨Ä±ve
Bayes (see Section V-A1); since the new features reduced the
NL class impact, they play a major role in the classiï¬cation.
Although achieving the best results so far, this approach has
drawbacks. First, we note that both patch andcode have more
than 150 wrongly classiï¬ed instances: This contradicts the
high precision value reached by the single parser classiï¬ers.
It is probably due to the fact that, even if these parser features
have a high weight in the computation, they are at the same
level of the other features that, being a large number, also
inï¬‚uence the results. We expect an approach not havingthe conditional independence assumption of Na Â¨Ä±ve Bayes
to better model the new features, which are highly inter-
dependent. In the following we explore a two-pass classiï¬er
approach to better exploit parsers, yet relying on Na Â¨Ä±ve Bayes
qualities.
2) Uniï¬ed Classiï¬cation Approach: To explain our uniï¬ed
classiï¬cation approach, we refer to Figure 4. The idea behind
this approach is using Na Â¨Ä±ve Bayes to evaluate a partial
classiï¬cation only on the features based on terms , and then
use another ML classiï¬er to model the fusion of Na Â¨Ä±ve Bayes
results and parser-based classiï¬cations.
Training: We ï¬rst (Point 1) extract the emails from the three
mailing lists on which we want to train the ML algorithms,
then, we provide themâ€”along with the manual classiï¬cationâ€”
both to the parser-based classiï¬ers (Point 2) and to the Na Â¨Ä±ve
Bayes learning algorithm (Point 3), in the form of feature-
vector on words, punctuation, bi-grams, and context. Na Â¨Ä±ve
Bayes trains a classiï¬er, but instead of returning the instance
classiï¬cations, it outputs a 5-dimension vector for every line:
Each dimension represents a class ( e.g., junk ) and the value
is the probabilityâ€”evaluated by Na Â¨Ä±ve Bayesâ€”that the line
belongs to that class. In other words, instead of picking
the highest value and providing the ï¬nal classiï¬cation, we
output all the 5 probabilities and we map them to features,
thus reducing the initial features to 5. At the same time, the
parsers create other four features, as in Section V-E1. Once
both feature sets are evaluated, they are merged into a vector
of 9 dimensions, plus the manual classiï¬cation (Point 5).
This vector is treated by another ML algorithm to train the
ï¬nal classiï¬er (Point 6): The actual output of the training.
The choice of the ML technique for the second step is
critical: We need an algorithm to correctly model the peculiar
characteristics of our features. We tried a number of di erent
ML approaches. The decision tree [26] (broadly used in data
mining) is the most suited algorithms, because it is favorableto the parsersâ€™ features, which are almost mutually exclusive.
Testing: The test process is depicted in the bottom half
of Figure 4. We take emails from the fourth mailing list
and we remove the manual classiï¬cation. Then, we provide
the emails to the parsers (Point 8) and create the feature-
vectors, to be given as an input to the previously trained
NaÂ¨Ä±ve Bayes classiï¬er (Point 7). Subsequently, the output
of the two technique is merged in a uniï¬ed 9-dimensions
vector, which it is used as input to the second ML classiï¬er,
previously trained, which outputs the ï¬nal classiï¬cation. We
compare this classiï¬cation (Point 10) to the manual one
(Point 9) and we evaluate the results. The training and test
phases are repeated 4 times rotating the four mailing lists.
Table VII
Results of the unified approach on mailing list cross validation
classified as â™classified as â™NL TextJunkPatchSourceCodeStackTracePrecisionRecallF-MeasureNL Text31,5841,47008710.9370.9530.945Junk1,95827,4981211520.9430.9290.936Patch68491,9353000.9900.9290.959Source Code8611881,70200.8800.8890.885Stack Trace1812001,0390.9970.9720.984
We tested the approach with a range of tvalues and the
highest ratio of correct instances (94.1%) at a tvalue of
120, which lies within the range described in Section V-C.
The lowest ratio of correct instances with a tvalue ( i.e.,11)
within the range is 92.1%; out of the range, values are lower.
Table VII shows the results achieved by the approach on
the best tvalue. This two-steps approach, which di erently
merges and model the information, improves the results for
all the classes by increasing not only the results related to the
parser classiï¬ers ( i.e.,patch, stack trace, and code), but also
those connected to the Na Â¨Ä±ve Bayes algorithm. The F-measure
values are all increased, with a decrease in precision of junk
classiï¬cation and in recall of NL classiï¬cation, probably due
to the overall lower weight given to Na Â¨Ä±ve Bayes results.
VI. T hreats to Validity
Construct Validity threats regard the relation between
theory and observation, i.e., measured variables may not
measure conceptual variables.
To classify email content we rely on error-prone human
judgment. To alleviate this issue, we devised a web applica-
tion to ease the annotation process. Two annotators cross-
inspected 10% of the emails. They found only 12 erroneously
classiï¬ed lines. We corrected these 12 errors in the set of
email that was used for the experiments. We expect the same
low error proportion in the rest of the sample, which may
aect the accuracy of the results.
Statistical Conclusion threats are concerned with whether
we have enough data to support our claims.
We took samples of email populations representative with
a 95% conï¬dence and a 5% error level, which are standardvalues. On the number of lines, our corpus has 67,792 not
empty lines.
External Validity threats are concerned with the general-
izability of the results.
The approaches we tried may show di erent results when
applied to other software systems and mailing lists. To alle-
viate this, we chose 4 systems with unrelated characteristics
and developed by separate communities. The usage of the
mailing list varies, as conï¬rmed by the di erent line class
distributions. To test the generalizability of our approach
we conducted cross mailing list validation. A second threat
concerning the generalizability is that our approach is tailored
to a single object-oriented programming language, i.e.,Java.
However, since most of the language related line recognition
relies on island parsers (see Section V-D), it can be easily
adapted to other programming languages that have a similar
structure ( e.g., C#,Python ), without the need of changing
the ground concepts we used.
VII. C onclusion
Email communications contain valuable information to
support software development, comprehension, and analysis.
In this paper, we contribute a novel technique to automate
the analysis of such valuable, but also voluminous, data that
is speciï¬cally tailored for software engineering.
In particular, we presented a uniï¬ed 2-step approach
that fuses automated supervised ML approaches with island
parsing to perform automatic classiï¬cation of the content
of development emails into ï¬ve language categories: NL
text, source code fragments, stack traces, code patches, and
junk. The results obtained are very positive, even with cross
mailing list validation. In fact, parser-based classiï¬ers are
mailing list independent and o er a solid basis made more
robust by the probabilistic ML approach.
This work is a step toward a more e ective exploitation of
email data, for example by allowing improved traceability re-
covery techniques, reï¬ned artifact summarization approaches,
and more precise fact extraction methods.
As a future work, we plan to investigate whether other
classiï¬cation techniques (such as infoZilla [8]) can be
included in our uniï¬ed approach to improve and strengthen
the overall results.
All data sets of the experiment and source code can be
found at the paperâ€™s companion website located at http: //
mucca.inf.usi.ch.
Acknowledgment
Bacchelli gratefully acknowledges the Swiss National
Science foundationâ€™s support for the project â€œSOSYAâ€ (SNF
Project No. 132175) and the European Smalltalk User Group
(http: //www.esug.org). Dal Sasso gratefully acknowledges the
support of CHOOSE, the Swiss Group for Object-Oriented
Systems and Environments (http: //choose.s-i.ch).References
[1]G. Antoniol, G. Canfora, G. Casazza, A. D. Lucia, and
E. Merlo. Recovering traceability links between code and
documentation. IEEE Transactions on Software Engineering
(TSE) , 28(10):970â€“983, 2002.
[2]A. Bacchelli, A. Cleve, M. Lanza, and A. Mocci. Extracting
structured data from natural language documents with island
parsing. In Proceedings of ASE 2011 (International Conference
On Automated Software Engineering) , 2011.
[3]A. Bacchelli, M. Dâ€™Ambros, and M. Lanza. Extracting source
code from e-mails. In Proceedings of ICPC 2010 (18th IEEE
International Conference on Program Comprehension) , pages
24â€“33. IEEE Computer Society, 2010.
[4]A. Bacchelli, M. Lanza, and V . Humpa. RTFM (Read The
Factual Mails)â€“Augmenting program comprehension with
REmail. In Proceedings of CSMR 2011 (15th IEEE European
Conference on Software Maintenance and Reengineering) ,
pages 15â€“24, 2011.
[5]A. Bacchelli, M. Lanza, and R. Robbes. Linking e-mails and
source code artifacts. In Proceedings of ICSE 2010 (32nd
International Conference on Software Engineering) , pages
375â€“384. ACM, 2010.
[6]M. J. Beal, Z. Ghahramani, and C. E. Rasmussen. Factorial
hidden markov models. In Machine Learning , pages 29â€“245.
MIT Press, 1997.
[7]A. L. Berger, V . J. D. Pietra, and S. A. D. Pietra. A
maximum entropy approach to natural language processing.
Computational Linguistics , 22(1):39â€“71, 1996.
[8]N. Bettenburg, R. Premraj, T. Zimmermann, and S. Kim.
Extracting structural information from bug reports. In
Proceedings of MSR 2008 (5th International Workshop on
Mining Software Repositories) , pages 27â€“30. ACM, 2008.
[9]N. Bettenburg, E. Shihab, and A. E. Hassan. An empirical
study on the risks of using o -the-shelf techniques for
processing mailing list data. In Proceedings of ICSM 2009
(25th International Conference on Software Maintenance) ,
pages 539 â€“542. IEEE, 2009.
[10] C. Bird, A. Bachmann, E. Aune, J. Du y, A. Bernstein,
V . Filkov, and P. Devanbu. Fair and balanced? Bias in bug-ï¬x
datasets. In Proceedings of ESEC-FSE 2009 , pages 121â€“130.
ACM, 2009.
[11] C. Bird, A. Gourley, and P. Devanbu. Detecting patch
submission and acceptance in OSS projects. In Proceedings
of MSR 2007 , pages 26â€“29. IEEE Computer Society, 2007.
[12] R. Caruana and A. Niculescu-Mizil. An empirical comparison
of supervised learning algorithms. In Proceedings of ICML
(23rd International Conference on Machine learning) , pages
161â€“168. ACM, 2006.
[13] V . R. Carvalho and W. W. Cohen. Learning to extract signature
and reply lines from email. In Proceedings of CEAS 2004
(1st Conference on Email and Anti-Spam) , 2004.
[14] M. Dâ€™Ambros, M. Lanza, and R. Robbes. Evaluating
defect prediction approaches: a benchmark and an extensive
comparison. International Journal on Empirical Software
Engineering (EMSE) , x(x):to be published, 2011.
[15] A. Dekhtyar, J. H. Hayes, and T. Menzies. Text is software
too. In Proceedings of MSR 2004 , pages 22â€“26, 2004.
[16] S. Ducasse, A. Lienhard, and L. Renggli. Seaside: A ï¬‚exible
environment for building dynamic web applications. IEEE
Software , 24(5):56â€“63, 2007.[17] T. GË†Ä±rba and S. Ducasse. Modeling history to analyze software
evolution. Journal of Software Maintenance and Evolution ,
18:207â€“236, 2006.
[18] T. Gleixner. The realtime preemption patch: Pragmatic
ignorance or a chance to collaborate? In Keynote of ECRTS
2010 (22nd Euromicro Conference on Real-Time Systems) ,
2010. http: //lwn.net /Articles /397422 /.
[19] S. Haiduc, J. Aponte, and A. Marcus. Supporting program
comprehension with source code summarization. In Proceed-
ings of ICSE 2010 , pages 223â€“226. ACM, 2010.
[20] K. S. Jones. Automatic summarising: The state of the
art.Information Processing and Management , 43:1449â€“1481,
2007.
[21] D. Jurafsky and J. H. Martin. Speech and Language
Processing: An Introduction to Natural Language Processing,
Computational Linguistics and Speech Recognition . Prentice
Hall, 2nd edition, 2009.
[22] D. Kawrykow and M. P. Robillard. Non-essential changes
in version histories. In Proceedings of ICSE 2011 , pages
351â€“360, 2011.
[23] A. Kuhn, S. Ducasse, and T. G Â´Ä±rba. Semantic clustering:
Identifying topics in source code. Information and Software
Technology , 49(3):230â€“243, 2007.
[24] W. Lidwell, K. Holden, and J. Butler. Universal Principles of
Design . Rockport, 2003.
[25] C. Manning, P. Raghavan, and H. Sch Â¨utze. Introduction to
Information Retrieval . Cambridge University Press, 2008.
[26] T. Mitchell. Machine Learning . McGraw Hill, 1997.
[27] L. Moonen. Generating robust parsers using island grammars.
InProceedings of WCRE 2001 (8th Working Conference on
Reverse Engineering) , pages 13â€“22. IEEE CS, 2001.
[28] S. Rastkar, G. C. Murphy, and G. Murray. Summarizing
software artifacts: a case study of bug reports. In Proceedings
of ICSE 2010 , pages 505â€“514. ACM, 2010.
[29] L. Renggli, S. Ducasse, G Ë†Ä±rba, and O. Nierstrasz. Practical
dynamic grammars for dynamic languages. In Proc. of DYLA
2010 (4th Workshop on Dynamic Languages) , 2010.
[30] C. B. Seaman. Qualitative methods in empirical studies of
software engineering. IEEE TSE , 25:557â€“572, 1999.
[31] F. Sebastiani. Machine learning in automated text categoriza-
tion. ACM Computing Surveys , 34:1â€“47, 2002.
[32] O. Stock, R. Falcone, and P. Insinnamo. Island parsing
and bidirectional charts. In Proc. of the 12th Conf. on
Computational Linguistics , pages 636â€“641, 1988.
[33] J. Tang, H. Li, Y . Cao, and Z. Tang. Email data cleaning. In
Proceedings of KDD 2005 (11th ACM SIGKDD international
conference on Knowledge discovery in data mining , pages
489â€“498. ACM, 2005.
[34] A. E. H. Thanh H. D. Nguyen, Bram Adams. A case study
of bias in bug-ï¬x datasets. In Proceedings of WCRE 2010 ,
pages 259 â€“268. IEEE CS Press, 2010.
[35] M. Triola. Elementary Statistics . Addison-Wesley, 2006.
[36] G. Venolia. Textual allusions to artifacts in software-related
repositories. In Proceedings of MSR 2006 , pages 151â€“154.
ACM, 2006.
[37] T. Zimmermann, R. Premraj, N. Bettenburg, S. Just,
A. Schroter, and C. Weiss. What makes a good bug report?
IEEE TSE , 36(5):618â€“643, 2010.