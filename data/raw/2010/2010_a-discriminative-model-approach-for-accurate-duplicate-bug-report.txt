Singapor e Management Univ ersity Singapor e Management Univ ersity 
Institutional K nowledge at Singapor e Management Univ ersity Institutional K nowledge at Singapor e Management Univ ersity 
Resear ch Collection School Of Computing and 
Information Systems School of Computing and Information Systems 
5-2010 
A discriminativ e model appr oach for accur ate duplicate bug A discriminativ e model appr oach for accur ate duplicate bug 
repor t retrie val repor t retrie val 
Chengnian SUN 
David L O 
Singapor e Management Univ ersity , davidlo@smu.edu.sg 
Xiao yin W ANG 
Siau-Cheng KHOO 
Follow this and additional works at: https:/ /ink.libr ary.smu.edu.sg/sis_r esear ch 
 Part of the Softwar e Engineering Commons 
Citation Citation 
SUN, Chengnian; L O, David; W ANG, Xiao yin; and KHOO , Siau-Cheng. A discriminativ e model appr oach for 
accur ate duplicate bug r epor t retrie val. (2010). Proceedings of the 32nd Acm/Ieee International 
Conf erence on Softwar e Engineering, Cape T own, South Africa, 2010, Ma y 1 - 8 . 1, 45-54. 
Available at:Available at:  https:/ /ink.libr ary.smu.edu.sg/sis_r esear ch/3721 
This Conf erence Pr oceeding Ar ticle is br ought t o you for fr ee and open access b y the School of Computing and 
Information Systems at Institutional K nowledge at Singapor e Management Univ ersity . It has been accepted for 
inclusion in Resear ch Collection School Of Computing and Information Systems b y an authoriz ed administr ator of 
Institutional K nowledge at Singapor e Management Univ ersity . For mor e information, please email 
cher ylds@smu.edu.sg . See	discussions,	stats,	and	author	profiles	for	this	publication	at:	
https://www.researchgate.net/publication/221554436
A	discriminative	model	approach	for	accurate
duplicate	bug	report	retrieval
Conference	Paper
		
in
		
Proceedings	-	International	Conference	on	Software	Engineering
	·	January	2010
DOI:	10.1145/1806799.1806811
	·	
Source:	DBLP
CITATIONS
120
READS
103
5	authors
,	including:
Some	of	the	authors	of	this	publication	are	also	working	on	these	related	projects:
Tark:	A	Tool	Kit	to	Mine	Linear	Temporal	Rules
	
View	project
Recommender	systems
	
View	project
David	Lo
Singapore	Management	University
235
	
PUBLICATIONS
			
3,452
	
CITATIONS
			
SEE	PROFILE
Xiaoyin	Wang
University	of	California,	Berkeley
22
	
PUBLICATIONS
			
717
	
CITATIONS
			
SEE	PROFILE
Siau-cheng	Khoo
National	University	of	Singapore
107
	
PUBLICATIONS
			
1,562
	
CITATIONS
			
SEE	PROFILE
All	content	following	this	page	was	uploaded	by	
Siau-cheng	Khoo
	on	18	June	2014.
The	user	has	requested	enhancement	of	the	downloaded	file.A Discriminative Model Approach for Accurate Duplicate
Bug Report Retrieval
Chengnian Sun1, David Lo2, Xiaoyin Wang3, Jing Jiang2, Siau-Cheng Khoo1
1School of Computing, National University of Singapore
2School of Information Systems, Singapore Management University
3Key laboratory of High Conﬁdence Software T echnologies (Peking University), Ministry of Education
suncn@comp.nus.edu.sg, davidlo@smu.edu.sg, wangxy06@sei.pku.edu.cn,
jingjiang@smu.edu.sg, khoosc@comp.nus.edu.sg
ABSTRACT
Bug repositories are usually maintained in software projects.
Testers or users submit bug reports to identify various issueswith systems. Sometimes two or more bug reports corre-spond to the same defect. To address the problem with du-plicate bug reports, a person called a triager needs to man-
ually label these bug reports as duplicates, and link them
to their ”master”reports for subsequent maintenance work.However, in practice there are considerable duplicate bug re-ports sent daily; requesting triagers to manually label these
bugs could be highly time consuming.
To address this issue, recently, several techniques have
be proposed using various similarity based metrics to detect
candidate duplicate bug reports for manual veriﬁcation. Au-tomatingtriaginghasbeenprovedchallengingastworeports
of the same bug could be written in various ways. There is
still much room for improvement in terms of accuracy of du-
plicate detection process. In this paper, we leverage recent
advances on using discriminative models for information re-
trieval to detect duplicate bug reports more accurately. We
have validated our approach on three large software bug
repositories from Firefox, Eclipse, and OpenOﬃce. We show
that our technique could result in 17–31%, 22–26%, and 35–
43%relativeimprovementoverstate-of-the-arttechniquesin
OpenOﬃce, Firefox, and Eclipse datasets respectively using
commonly available natural language information only.
Categories and Subject Descriptors
D.2.7[Software Engineering ]: Distribution, Maintenance,
and Enhancement
General Terms
Management, Reliability
1. INTRODUCTION
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.Due to complexities of systems built, software often comes
with defects. Software defects have caused billions of dollars
lost [20]. Fixing defects is one of the most frequent reasonsfor software maintenance activities which also goes to 70billion US dollars in the United States alone [19].
In order to help track software defects and build more reli-
able systems, bug tracking tools have been introduced. Bugtracking systems like Bugzilla
1enable many users to serve
as “testers” and report their ﬁndings in a uniﬁed environ-ment. These bug reports are then used to guide software
corrective maintenance activities and result in more reliable
software systems. Via the bug tracking systems, users areable to report new bugs, track statuses of bug reports, andcomment on existing bug reports.
Despite the beneﬁts of a bug reporting system, it does
cause some challenges. As bug reporting process is oftenuncoordinated and ad-hoc, often the same bugs could bereported more than once by diﬀerent users. Hence, there isoftenaneedformanualinspectiontodetectwhetherthebug
has been reported before. If the incoming bug report is not
reported before then the bug should be assigned to a devel-
oper. However, if other users have reported the bug before
then the bug would be classiﬁed as being a duplicate and
attached to the original ﬁrst-reported “master” bug report.
This process referred to as triaging often takes much time.
For example, for the Mozilla programmers, it has been re-
ported in 2005 that“everyday, almost 300 bugs appear thatneed triaging. This is far too much for only the Mozilla
programmers to handle”[2].
In order to alleviate the heavy burden of triagers, there
have been recent techniques to automate the triaging pro-
cess in two ways. The ﬁrst one is automatically ﬁlteringduplicates to prevent multiple duplicate reports from reach-
ing triagers [11]. The second is providing a list of similar
bug reports to each incoming report under investigation [17,
22, 11]; with the help, rather than checking against the en-
tire collection of bug reports, a triager could ﬁrst inspect the
top-k most similar bug reports returned by the systems. If
there is a report in the list that reports about the same de-fect as the new one, then the one is a duplicate, The triagerthen marks it as a duplicate and adds a link between the
two duplicates for subsequent maintenance work. In our
paper, instead of ﬁltering duplicates, we choose the second
approach as duplicate bug reports are not necessarily bad.
As stated in [3], one report usually does not carry enough
information for developers to dig into the reported defect,
1http://www.bugzilla.org/ICSE’10 , May 2–8, 2010, Cape Town, South Africa. 
Copyright © 2010 ACM 978-1-60558-719-6/10/05 ... $10.00. 
45while duplicate reports can complement one another.
To achieve better automation and thus save triagers’ time,
it is important to improve the quality of the ranked list of
similar bug reports given a new bug report. There have been
several studies on retrieving similar bug reports. However,the performance of these systems is still relative low, makingit hard to apply them in practice. The low performance ispartly due to the following limitations of the current meth-
ods. First, all the three techniques in [17, 22, 11] employ
one or two features to describe the similarity between re-ports, despite the fact that other features are also available
foreﬀectivemeasurementofsimilarity. Second, diﬀerentfea-
tures contribute diﬀerently towards determining similarities.
For example, the feature capturing similarity between sum-
maries of two reports are more eﬀective than that between
descriptions, as summaries typically carry more concise in-
formation. However, as project contexts evolve, the relative
importance of features might vary. This can cause the past
techniques, which are largely based on absolute rating ofimportance, to deteriorate in their performance.
More accurate results would mean more automation and
less eﬀort by triagers to ﬁnd duplicate bug reports. To ad-
dress this need, we propose a discriminative model based
approach that further improves accuracy in retrieving dupli-
cate bug reports by up to 43% on real bug report datasets.
Diﬀerent from the previous approaches that rank similar
bug reports based on similarity score of vector space rep-
resentation, we develop a discriminative model to retrievesimilar bug reports from a bug repository. We make useof the recent advances in information retrieval community
that uses a classiﬁer to retrieve similar documents from a
collection[15]. We build a model that contrasts duplicatebug reports from non-duplicate bug reports and utilize thismodel to extract similar bug reports, given a query bug re-port under consideration.
We strengthen the eﬀectiveness of bug report retrieval
system by introducing many more relevant features to cap-ture the similarity between bug reports. Moreover, with the
adoption of the discriminative model approach, the relative
importance of each feature will be automatically determined
by the model through assignment of an optimum weight.
Consequently, as bug repository evolves, our discriminative
model also evolves to guarantee the all the weights remainoptimum at all time. In this sense, our process is more adap-
tive, robust, and automated.
We evaluate our discriminative model approach on three
large bug report datasets from large programs including
Firefox, anopensourcewebbrowser, Eclipse, apopularopen
source integrated development environment, and OpenOf-
ﬁce, a well-known open source rich text editor. In termsof the range of types of programs considered for evaluation,to the best of our knowledge, we are the ﬁrst to investi-gate the applicability of the approach on diﬀerent types
of systems. We show that our technique could result in
17–31%, 22–26%, and 35–43% improvement over state-of-the-art techniques [17, 22, 11] in OpenOﬃce, Firefox, andEclipse datasets respectively using commonly available nat-
ural language information alone.
We summarize our contributions as follows:
1. We employ in total 54 features to comprehensively
evaluate the similarity between two reports.
2. We propose a discriminative model based solution toretrieve similar bug reports from a bug tracking sys-
tem. Our model can automatically assign optimumweighttoeachfeatureandevolvealongwiththechangesof bug repositories.
3. We are the ﬁrst to analyze the applicability of dupli-
cate bug report detection techniques across diﬀerent
sizable bug repositories of various large open source
programs including OpenOﬃce, Firefox, and Eclipse.
4. Weimprovetheaccuracyofstate-of-the-artautomated
duplicate bug detection systems by up to 43% on dif-ferent open-source datasets.
The paper is organized as follows. Section 2 presents
some background information on bug reports, informationretrieval, and discriminative model construction. Section 3
presents our approach to retrieving similar bug reports for
duplicate bug report detection. Section 4 describes our case
study on sizable bug repositories of diﬀerent open source
projects and shows the utility of the proposed approach in
improving the state-of-the-art detection performance. Sec-tion 5 discusses some important consideration about our ap-proach. Section 6 discusses related work, and ﬁnally, Sec-
tion 7 concludes and describes some potential future work.
2. BACKGROUND
In general, duplicate bug report retrieval involves infor-
mation extraction from and comparison between documents
in natural language. This section covers the necessary back-ground and foundation techniques to perform the task in our
approach.
2.1 Duplicate Bug Reports
A bug report is a structured record consisting of sev-
eral ﬁelds. Commonly, they include summary, description,
project, submitter, priority and so forth. Each ﬁeld carries
a diﬀerent type of information. For example, summary is a
concise description of the defect problem while description
is the detailed outline of what went wrong and how it hap-
pened. Both of them are in natural language format. Otherﬁelds such as project, priority try to characterize the defect
from other perspectives.
In a typical software development process, the bug track-
i n gs y s t e mi so p e nf o rt e s t e r so re v e nf o ra l le n du s e r s ,s oi tis unavoidable that two people may submit diﬀerent reportson the same bug. This causes the problem of duplicate bug
reports. As mentioned in [17], duplicate reports can be di-
vided into two categories. One describes the same failure,and the other depicts two diﬀerent failures both originated
from the same root cause. In this paper, we only handle the
ﬁrst category. As an example, Table 1 shows three pairs of
duplicate reports extracted from Issue Tracker of OpenOf-
ﬁce. Only the summaries are listed.
Usually, newbugreportsarecontinuallysubmitted. When
triagers identify that a new report is a duplicate of an old
one, the new one is marked as duplicate. As a result, given
a set of reports on the same defect, only the oldest one in
the set is not marked as duplicate. We refer to the oldest
one as masterand the others as its duplicates .
Abugrepositorycouldbeviewedascontainingtwogroups
of reports: masters and duplicates. Since each duplicate
must have a corresponding master and both reports are on
the same defect, the defects represented by all the duplicates
46Table 1: Examples of Duplicate Bug Reports
ID Summary
85064[Notes2] No Scrolling of document content by
use of the mouse wheel
85377[CWS notes2] unable to scroll in a note with the
mouse wheel
85502Alt+<letter >does not work in dialogs
85819Alt-<key >no longer works as expected
85487connectivity: evoab2 needs to be changed to
build against changed api
85496connectivity fails to build (evoab2) in m4
in the repository belong to the set of the defects representedby all the masters. Furthermore, typically each master re-port represents a distinct defect.
2.2 Information Retrieval
Information retrieval (IR) aims to extract useful infor-
mation from unstructured documents, most of which are
expressed in natural language. IR methods typically treatdocuments as “bags of words” and subsequently represent
them in a high-dimensional vector space where each dimen-sion corresponds to a unique word or term. In this paper, weuse term and word interchangeably. The following describes
some commonly used strategies to pre-process documents
and methods to weigh terms.Pre-processing. In order to computerize retrieval task, a
sequence of actions should be taken ﬁrst to preprocess doc-
uments using natural language processing techniques. Usu-
ally, this sequence comprises tokenization, stemming andstop word removal. A word token is a maximum sequenceof consecutive characters without any delimiters. A delim-iter in turn could be a space, punctuation mark, etc. Tok-
enization is the process of parsing a character stream into a
sequence of word tokens by splitting the stream by the de-limiters. Stemming is the process to reduce words to theirgroundforms.Themotiv ationtodosoisthatdiﬀeren tforms
of words derived from the same root usually have similarmeanings. By stemming, computers can capture this simi-larity via direct string equivalence. For example, a stemmercan reduce both “tested” and “testing” to “test”. The last
action is stop word removal. Stop words are those words
carrying little helpful information for information retrieval
task. These include pronouns such as “it”, “he” and “she”,link verbs such as“is”,“am”and“are”, etc. In our stop wordlist, in addition to removing 30 common stop words, we also
drop common abbreviations such as “I’m”, “that’s”, “we’ll”,
etc.
Term-weighting. TF-IDF (Term Frequency-Inverse Doc-
ument Frequency) is a common term-weighting scheme. It
is a statistical approach to evaluating the importance of a
term in a corpus. TF is a localimportance measure. Given
a term and a document, in general, TF corresponds to the
numberoftimesthetermappearswithinthedocument. Dif-ferent from TF, IDF is a globalimportance measure most
commonly calculated by the formula within the corpus,
idf(term)= log
2(Dall
Dterm)( 1)
In (1),Dallis the number of the documents in the cor-
pus while Dtermis the number of documents containing the
Figure 1: Maximum-Margin Hyperplane Calculatedby SVM in Two-Dimensional Space
term. Given a term, the fewer documents it is contained in,
the more important it becomes. In our approach, we employanidf-based formula to weigh terms.
2.3 Building Discriminative Models via SVM
Support Vector Machine (SVM) is an approach to build-
ing a discriminative model or classiﬁer based on a set oflabeled vectors. Given a set of vectors, some belonging to a
positive class and others belonging to a negative class, SVM
tries to build a hyperplane that separates vectors belongingto the positive class from those of the negative class withthe largest margin. Figure 1 shows such kind of a hyper-
plane built by SVM with the maximum margin in a two-dimensional space. The resultant model could then be used
to classify other unknown data points in vector representa-tion and label them as either positive or negative. In thisstudy, we use libsvm [6], a popular implementation of SVM.
3. OUR APPROACH
Duplicate bug report retrieval can be viewed as an ap-
plication of information retrieval (IR) technique to the do-
main of software engineering, with the objective of improv-
ing productivity of software maintenance. In classical re-trieval problem, the user gives a query expressing the infor-mation he/she is looking for. The IR system would then
return a list of documents relevant to the query. For du-
plicate report retrieval problem, the triager receives a newreport and inputs it to the duplicate report retrieval systemas a query. The system then returns a list of potential du-plicate reports. The list should be sorted in a descending
order of relevance to the queried bug report.
Our approach adopts recent development on discrimina-
tive models for information retrieval to retrieve duplicate
bug reports. Adapted from [15], we consider duplicate bug
report retrieval as a binary classiﬁcation problem, that is,
given a new report, the retrieval process is to classify all ex-isting reports into two classes: duplicate and non-duplicate.We compute 54 types of textual similarities between reportsand use them as features for training and classiﬁcation pur-
pose.
The rest of this section is structured as follows: Sub-
section 3.1 gives a bird’s eye view of the overall framework.
Sub-section 3.2 explains how existing bug reports in the
repository are organized. Sub-section 3.3 elaborates on how
a discriminative model is built. Sub-section 3.4 describeshow the model is applied for retrieving duplicate bug re-
ports. Finally, Sub-section 3.5 describes how the model is
updated when new triaged bug reports arrive.
47Figure 2: Overall Framework to Retrieve Duplicate
Bug Reports
Figure 3: Bucket Structure
3.1 Overall Framework
Figure 2 shows the overall framework of our approach. In
general, there are three main steps in the system, preprocess-
ing,training a discriminative model andretrieving duplicate
bug reports .
The ﬁrst step, preprocessing, follows a standard natural
language processing style – tokenization, stemming and stop
words removal – described in Sub-section 2.2. The second
step, training a discriminative model , trains a classiﬁer to
answer the question“How likely are two bug reports dupli-cates of each other?”. The third step, retrieving duplicate
bug reports , makes use of this classiﬁer to retrieve relevant
bug reports from the repository.
3.2 Data Structure
Allthereportsintherepositoryareorganizedintoabucket
structure. The bucket structure is a hash-map-like datastructure. Each bucket contains a master report as the keyand all the duplicates of the master as its value. As ex-plained in Sub-section 2.1, diﬀerent masters report diﬀerentdefects while a master and its duplicates report the same
defect. Therefore, each bucket stands for a distinct defect,
while all the reports in a bucket correspond to the same de-fect. The structure of the bucket is shown diagrammaticallyin Figure 3. New reports will also be added to the struc-
ture after they are labeled as duplicate or non-duplicate by
triagers. If a new report is a duplicate, it will go to thebucket indexed by its master; otherwise, a new bucket willbe created to include the new report and it becomes a mas-ter.
3.3 Training a Discriminative Model
Given a set of bug reports classiﬁed into masters and du-
plicates, we would like to build a discriminative model ora classiﬁer that answers the question: “How likely are two
input bug reports duplicate of each other?”. This question
is essential in our retrieval system. As described in Subsec-tion 3.4, the answer is a probability describing the likelihoodof these two reports being duplicate of each other. When a
new report comes, we ask the question for each pair between
Figure 4: Training a Discriminative Model
the new report and all the existing reports in the repository
and then retrieve the duplicate reports based on the proba-
bility answers. To get the answer we follow a multi-step ap-
proach involving example creation, feature extraction, anddiscriminative model creation via Support Vector Machines(SVMs).
The steps are shown in Figure 4. Based on the buck-
ets containing masters associated with corresponding dupli-cates, we extract positive and negative examples. Positive
examples correspond to pairs of bug reports that are dupli-
cates of each other. Negative examples correspond to pairs
of bug reports that are not duplicates of each other. Next,
a feature extraction process is employed to extract features
from the pairs of bug reports. These features must be richenough to be able to discriminate between cases where bugreports are duplicate of one another and cases where they
are distinct. These feature vectors corresponding to dupli-
cates and non-duplicates are then input to an SVM learningalgorithm to build a suitable discriminative model. The fol-lowing sub-sections describe each of the steps in more detail.
3.3.1 Creating Examples
To create positive examples, for each bucket, we perform
the following:
1. Create the pair (master, duplicate), where duplicate is
one of the duplicates in the bucket and master is theoriginal report in the bucket.
2. Create the pairs (duplicate
1,duplicate 2) where the two
duplicates belong to the same bucket.
To create negative examples, one could pair one report
from one bucket with another report from the other bucket.The number of negative examples could be much larger thanthe number of positive examples. As there are issues relatedto skewed or imbalanced dataset when building classiﬁcation
models (c.f. [13]), we choose to under-sample the negative
examples, thus ensure that we have the same number ofpositive and negative examples.
At the end of the process, we have two sets of examples:
one corresponds to examples of pairs of bug reports that are
48duplicates, and the other corresponds to examples of pairs
of bug reports that are non-duplicates.
3.3.2 Feature Engineering & Extraction
At times, limited features make it hard to diﬀerentiate be-
tween two contrasting datasets: in our case, pairs that areduplicates and pairs that are non-duplicates. Hence a richenough feature set is needed to make duplicate bug reportretrieval more accurate. Since we are extracting features
corresponding to a pair of textual reports, various textualsimilarity measures between the two reports are good fea-
ture candidates. In our approach, we employ the followingformula as the textual similarity.
sim(B
1,B2)=/summationdisplay
w∈B1∩B2idf(w)( 2)
In (2),sim(B1,B2) returns the similarity between two bags
of words B1andB2. The similarity is the sum of idfvalues
of all the shared words between B1andB2.T h eidfvalue
for each word is computed based on a corpus formed from allthe reports in the repository, which will be detailed furtherbelow. The rational why the similarity measure does notinvolve TF is that the measure with only IDF yields better
performance indicated by Fisher score which will be detailed
in Sub-section 5.2, and validated by the experiments.
Generally, each feature in our approach can then be ab-
stracted by the following formula,
f(R
1,R2)=sim(words from R 1,words from R 2)( 3 )
From (3), a feature is actually the similarity between twobags of words from two reports R
1andR2.
Oneobservationisthatabugreportconsistsoftwoimpor-
tant ﬁelds: summary and description. So we can get three
bags of words from one report, one bag from summary, one
from descriptionandonefrom both(summary+description).
To extract a feature from a pair of bug reports, for example,one could compute the similarity between the bag of wordsfrom the summary of one report and the words from the
description of the other. Alternatively, one could use the
similarity between the words from both the summary and
description of one report and those from the summary of
the other. Other combinations are also possible.
Furthermore, we can compute three types of idf,a st h e
bug repository can form three distinct corpora. One corpus
is the collection of all the summaries, one corpus is the col-
lection of all the descriptions, and the other is the collection
of all the both (summary+description). We denote the three
types of idfcomputed within the three corpora by idf
sum,
idfdesc,a n didfbothrespectively.
The output of function fdeﬁned in (3) depends on the
choice of bag of words for R1, the choice of bag of words for
R2and the choice of idf. Considering each of the combi-
nations as a separate feature, the total number of diﬀerentfeatures would be 3 ×3×3, which is equal to 27. Figure 5
shows how the 27 features are extracted from a pair of bug
reports.
Aside from considering words, we also consider bigrams –
two consecutive words. With bigrams, considering diﬀerent
combinations of bag of words coming from and idfcomputed
based on summaries,d escriptions, or both, we would have
another 27 features which would then bring the number of
features extracted to 54.Algorithm 1 Calculate Candidate Reports for Q
Procedure ProposeCandidatesInput:Q: a new report
Rep: the bug repository
N: the expected size of candidate list
Output:result:al i s to f Nmasters of which Q is a likely duplicate
Body:
1:Candidates = an empty min-heap of maximum size N
2:for each bucketB∈Buckets (Rep)do
3:similarity =PredictBucket (Q,B)
4:Master(B).similarity =similarity
5: add Master(B)t oCandidates
6:end for
7: sortCandidates in descending order of ﬁeld similarity
8:returnsortedCandidates
Algorithm 2 Calculate Similarity between Q and a Bucket
Procedure PredictBucketInput:Q: a new report
B: a bucket
Output:max: the maximum similarity between Qand each report
ofB
Body:
1:max=0
2:tests={f54(Q,R)|R∈Reports(B)}
3:for each feature vector t∈testsdo
4:probability =SVMPredict (t)
5:max=MAX(max,probability )
6:end for
7:returnmax
3.3.3 Training Models
Before training, an extra action is taken to normalize the
values of all features in the training set to within the range
[-1,1]. This is to avoid the case that some features in the
bigger range dominate those in the smaller range. We use astandardalgorithmtonormalizethefeaturevectors(c.f.[6]).The same normalization will also be applied when the resul-tant model is used for classifying unknown pairs.
We use libsvm [6] to train a discriminative model from
the training set. As suggested by [15], we choose the linearkernel for SVM as it is eﬃcient and eﬀective in performingclassiﬁcation for information retrieval. A typical classiﬁer
would only give binary answers; in our case, whether two
bug reports are duplicate or not. We are notinterested in
binary answers; rather, we are interested in knowing howlikely two bug reports are duplicates. To do this we enablethe probability estimation functionality of libsvm to train a
discriminative model which is able to produce a probability
of two bug reports being duplicates of each other.
3.4 Applying Models for Duplicate Detection
When a new report Qarrives, we can apply the trained
model to retrieve a list of candidate reports of which Qis
likely a duplicate. The retrieval details are displayed in Al-
gorithm 1 and Algorithm 2.
Algorithm 1 returns a duplicate candidate list for a new
49Figure 5: Feature Extraction: First 27 Features
report. In general, it iterates over all the buckets in the
repository and calculates the similarity between the new
report and each bucket. At last, a list of masters whosebuckets have the biggest similarity is returned. In Line 2,Buckets (Rep) returns all the buckets in the bug repository,
andMaster(B)i nL i n e4i st h em a s t e rr e p o r to fb u c k e t B.
The algorithm makes a call to Algorithm 2 which computesthe similarity between a report and a bucket of reports. Tomake the algorithm eﬃcient, we only keep top Ncandidate
buckets in memory while the buckets in the repository are
being analyzed. This is achieved by a minimum heap of
maximum size N.I ft h es i z eo f Candidates is less than N,
new bucket will be directly added. When the size equals
toN, if the new bucket whose similarity is greater than
the minimum similarity in Candidates, it will replace the
bucket with the minimum similarity; otherwise, the requestof adding the bucket will be ignored by Candidates.
Given a new report Qand a bucket B, Algorithm 2 re-
turns the similarity between QandB. This algorithm ﬁrst
creates candidate duplicate pairs between Qand all the re-
ports in the bucket B(Line 2). Each pair is represented
by a vector of features, which is calculated by the function
f54. The trained discriminative model is used to predict
the probability for each candidate pair. Finally, the max-
imum probability between Qand the bug reports in Bis
returned as the similarity between QandB.Reports(B)
denotes all the reports in B. The operation f54(Q,R)r e -
turns a vector of the 54 similarity features from the pair ofbug reports QandR, including 27 features based on sin-
gle words and 27 features based on bigrams described in
Sub-section 3.3.2. The procedure SVMPredict in Line 4 is
the invocation to the discriminative model and it returns aprobability in which the pair of reports the feature vector t
corresponds to are duplicates of each other.
3.5 Model Evolution
As time passes, new bug reports will come and be triaged.
This new information could be used as new training data toupdate the model. Users could perform this process period-ically or every time after a new bug report has been triaged.In general, such newly created training data should be use-ful. However, we ﬁnd that such information is not alwaysbeneﬁcial to us for the following reason: our retrieval modelis based on lexical similarity rather than semantic similar-
ity of text. In another word, if two bug reports refer to
the same defect but use diﬀerent lexical representations, i.e.words, then our retrieval model does not give a high similar-ity measure to this pair of bug reports. Therefore, includinga pair of duplicate bug reports that are not lexically sim-
ilar in the training data would actually bring noise to our
trained classiﬁer. We therefore consider the following twokinds of update:
1. Light update. If our retrieval engine fails to retrieve
the right master for a new report before the triager
marks the report as a duplicate, we perform this up-
date. When the failure happens, the new bug reportcould syntactically be very far from the master. For
this case, we only perform a light update to the model
by updating the idfscores of the training examples
and re-training the model.
2. Regular update. We perform this update if our re-
trieval engine is able to retrieve the right master for
the new duplicate bug report. When this happens,the new bug report is used to update the idfAND to
create new training examples. An updated discrimi-
native model is then trained based on the new idfand
training examples.
Via our experiments, we have empirically validated that
theaboveheuristicsworkeﬀectivelyinimprovingthequality
of the model after updating.
4. CASE STUDIES
Weapplyourdiscriminativemodelapproachtothreelarge
bug repositories of open source projects, OpenOﬃce, Firefox
and Eclipse. For comparison purpose, we also implemented
the algorithms in [17, 22, 11] to the best of our knowledge.To evaluate the performance of diﬀerent approaches, we em-ployed the notion of recall rate deﬁned in [17].
recall rate =N
detected
Ntotal(4)
(4) shows how recall rate is calculated. Ndetectedis the
number of duplicate reports whose masters are successfully
50detected, while Ntotalis the total number of duplicate re-
ports for testing the retrieval process. Given a candidate
list size, the recall rate can be interpreted as the percent-
age of duplicates whose masters are successfully retrieved inthe list. In terms of this measure, the result shows that ourapproach can bring remarkable improvement over the otherthree approaches.
4.1 Experimental Setup
In our experiment, we used the bug repositories of three
large open source projects: the Eclipse project, the Fire-
fox project and the OpenOﬃce project. Among the three
projects, Eclipse is a popular open source integrated devel-opmentenvironmentwritteninJava; Firefoxisawell-knownopen source web browser written in C/C++, and OpenOf-
ﬁce is an open source counterpart of Microsoft Oﬃce.
These three projects are from diﬀerent domains, written
in diﬀerent languages and used by diﬀerent types of users.
Thus, carrying out the experiment on them helps to general-ize our conclusions. On top of that, all of the three projects
havelargebugrepositoriessoastoprovideampledataforan
evaluation. Weselectedasubsetofeachrepositoryincludingboth defect reports and feature requests within a period oftime to set up an experimental bug set in our study. Specif-
ically, we use the bug report set submitted to OpenOﬃce
in year 2008 (including 12,732 bug reports), the bug reportset of Eclipse in year 2008 (including 44,652 bug reports)
to evaluate our approach. Furthermore, to study how our
training based approach works in the long run (in case the
property of bug reports change over time), we further eval-
uated our approach on the whole bug report set of Firefox
(including 47,704 bug reports submitted since Firefox was
started in 2002) before June 2007 as a long-run evaluation
set.
Table 2 gives the details of the three datasets. We refer to
thetimeperiodofthedatasetsas Time Frame , asthesecond
column Time Frame displays. To run our approach, we
select the ﬁrst Mreports of which 100 reports are duplicates
as training set to train a discriminative model. The thirdcolumn of Training Reports in the table shows the ratio of
duplicates and all reports in the training set. Besides of
serving as a training set in our approach, those Mreports
are also used to simulate the initial bug repository for allexperiment runs.
Selecting reports within a time frame introduces a prob-
lem. If a duplicate report r
1is within the time frame while
i t sm a s t e ri sn o t ,t h e n r1is not detectable as its master is
not in the dataset, which will only decrease the recall rate.For this case, we simply re-mark r
1as non-duplicate. How-
ever, there is a more complex case. Suppose there are two ormore duplicate reports on the same defect within the frame
while their common master is excluded from the dataset. If
we still simply mark them as non-duplicate, we will lose twoor more duplicates. So in this case, we ﬁrst make the oldestreportr
oldestbecome the master report, and then mark the
others as duplicates of roldest.
4.2 Experimental Details and Result
Weevaluatetheperformanceofourapproachascompared
to previous techniques over the three datasets. The previousapproaches come with a parameter setting corresponding tothe weight being assigned to the words from the summaryand those from the description of the bug reports. Theyconsider two weights, one is: equal weight between the sum-mary and description, second is: summary carries doubleweight.
Also, in the three approaches, they consider a non bucket-
based retrieval, where relevant bug reports are retrieved
rather than relevant buckets. As one bucket contains bug re-
ports corresponding to the same defect, we believe it is bestto consider bucket-based retrieval. Report-based retrievalcould potentially return more than one report referring to
the same defect in the list of candidate duplicate bug reports
causing redundant eﬀort for the triager. Also, in [22], theauthors use both IR and execution trace to detect duplicatereports. Asexecutiontraceishardto get forexistingreportsand especially for our large datasets, we only compare based
on textual summary and description of the bug reports.
In the three datasets, training reports are used as ini-
tial bug repository. They are also used to construct the
training set and further to build a discriminative model. At
each experimental run, we iterate over the testing reports
in chronological order. Once reaching a duplicate report R,
we apply the corresponding technique to get a top Nlist of
R’s potential master reports. After each detection is done,
we record the result whether R’s master is detected success-
fullyforfuture recallrate calculation, andthenadd Rtothe
repository. After the last iteration is over, the recall rates
for diﬀerent top list size are calculated.
Figure 6 shows the experiment results of the seven runs
on the three datasets. In the ﬁgure, the horizontal axis isthe top N list’s size, and the vertical axis is the recall rate;Jstands for [11], Rstands for [17] and Wis [22]. Suﬃx -1
or -2 represents weighing summaries 1 or 2. Ocorresponds
to our approach. From the three sub ﬁgures, we can easily
come to the ﬁrst conclusion that our technique brings evi-
dence of improvement. We have 17–31% relative improve-ment in OpenOﬃce dataset, 22–26% in Firefox dataset and35–43% in Eclipse dataset. The second conclusion is that
manually empirically weighing summaries for traditional IR
techniques can help improve limited performance as the sixcurves near the bottom of each sub ﬁgure are very close toone another.
Compared to other six runs, the improvement achieved in
our run is due to (1) the 54 similarity features to compre-hensively measure how similar two reports are (2) and the
use of discriminative model to automatically assign weights
to each feature to discriminate duplicate reports from non-
duplicate ones, which is rigorous and has theoretical support
from machine learning area.
5. DISCUSSION
This section will ﬁrst discuss issues related to runtime
overhead, followed by the rationale behind the choice of the
54 similarity scores as features rather than other types of
similarity scores.
5.1 Runtime Overhead
There is no free lunch. With the big increase of recall
rates, the runtime overhead of our detection algorithm isalso higher than past approaches. In the largest datasetFirefox in the experiment run of our approach, the initialcost to detect a duplicate report is one second. However, as
the training set continually grows and the repository gets in-
creasingly large, the detection cost also increases over time.When the experiment considers detecting the last duplicate
51Table 2: Summary of Datasets
Dataset Time Frame Training Reports (Duplicate/All) Testing Reports(Duplicate/All)
OpenOﬃce Jan/02/2008–Dec/30/2008 100/3160 529/9572
Firefox Apr/04/2002–Jul/07/2007 100/962 3207/46359
Eclipse Jan/02/2008–Dec/30/2008 100/4265 1913/40387
(a) OpenOﬃce
(b) FireFox
(c) Eclipse
Figure 6: Recall Rates Comparison between Various
Techniques with Certain Top List Size
bug report, the cost becomes oneminute.
The major overhead is due to the fact that we consider
54 diﬀerent similarity features between reports. In contrast,
previous works [17, 22] consider the similarity between sum-mary+description and summary+description, and [11] con-siders the similarity between summary and summary, and
the similarity between description and description. The run-
time overhead is higher at the later part of the experiment asthe number of bug report pairs in the training set is larger.Consequently, SVM will need more time to build a discrim-inative model.
However, a higher runtime overhead does not mean thatthisapproachisnotpractical. Infact, itisacceptableforrealworld bug triaging process for two reasons. First, we haveexperimented with real world datasets. The dataset fromFirefox spans from April 04, 2002 to July 07, 2007 and con-
tains more than 47,000 reports in total. For an active soft-
wareproject, consideringreportsinone-yearframeisenoughas bug reports are usually received for new software releases.Although the Eclipse dataset with 44652 reports is withinone-yeartimeframe, itcontainsmultiplesub-projects, which
meansthat it canbesplit into smallerdatasets. Second, new
bugreportsdonotcomeeveryminute. Inourthreedatasets,the average frequency of new report arrival for OpenOﬃceis 1.5 reports/hour, the one for Firefox is 1 report/hour, and
the one for Eclipse is 5 reports/hour. Therefore, our system
still has enough time to retrain the model before processingan e wb u gr e p o r t .
5.2 Feature Selection
Feature selection is the process to identify a set of features
which can bring the best classiﬁcation performance. A com-monly used metric for feature selection is Fisher score [16],
deﬁned in the following formula,
F
r=/summationtextc
i=1ni(μi−μ)2
/summationtextci=1niσ2
i(5)
whereniis the number of data examples in class i,μiis the
average feature value in class i,σiis the standard deviation
of the feature value in class i,a n dμis the average feature
value in the whole dataset. Assume xijis the attribute
value for the jth instance in class i, thenμ,μiandσiare
deﬁned as μ=/summationtext
i/summationtext
jxij/summationtext
ini,μi=/summationtext
jxij
ni,σi=/radicalBig/summationtext
j(xij−μi)2
ni,
respectively. The higher the ﬁsher score the better is the
feature for classiﬁcation.
In our approach, we use 54 idf-based formulas to calcu-
late similarity features between two bug reports. One mightask why tfortf∗idfmeasures are not used in the formu-
las. The reason to choose an idf-only solution is that this
setting yields the best performance during our empirical val-
idation. To further demonstrate that idfis a good measure,
we replace the idfmeasure in the 54 features with tfand
tf∗idfmeasure respectively. We then calculate the Fisher
score of each of the 54 features using idf,tf,a n dtf∗idf.
TheresultsonEclipsedatasetshowsthat idf-basedformulas
outperform tf-based and ( tf∗idf) - b a s e df o r m u l a si nt e r m s
of Fisher score. This supports our decision in choosing the
idf-based formulas as feature scores.
6. RELATED WORK
One of the pioneer studies on duplicate bug report detec-
tion is by Runeson et al. [17]. Their approach ﬁrst cleanedthetextualbugreportsvianaturallanguageprocessingtech-
niques – tokenization, stemming and stop word removal.
52The remaining words were then modeled as a vector space,
where each axis corresponded to a unique word. Each re-port was represented by a vector in the space. The value of
each element in the vector was computed by the following
formula on the tfvalue of the corresponding word.
weight(word)=1+log
2(tf(word))
After these vectors were formed, they used three measures
–cosine, diceandjaccard–tocalculatethedistancebetweentwo vectors as the similarity of the two corresponding re-
ports. Given a bug report under investigation, their system
would return top-k similar bug reports based on the similar-ities between the new report and the existing reports in therepository. A case study was performed on defect reports at
Sony Ericsson Mobile Communications, which showed that
the tool was able to identify 40% of duplicate bug reports.
It also showed that cosine outperformed the other two sim-
ilarity measures.
In [22], Wang et al. extended the work by Runeson et al.
in two dimensions. First they considered not only TF, but
IDF. Hence, in their work, the value of each element in avector corresponded to the following formula,
weight(word)=tf(word)∗idf(word)
Second, they considered execution information to detect du-
plicates. A case study based on cosine similarity measure on
a small subset of bug reports from Firefox showed that theirappraoch could detect 67%–93% of duplicate bug reports byutilizing both natural language information and executiontraces.
In [11], Jalbert and Weimer extended the work by Rune-
son et al. in two dimensions. First, they proposed a newterm-weighting scheme for the vector representation,
weight(word)=3+2 ∗log
2(tf(word))
The cosine similarity was adopted to extract top-k similar
reports. Aside from the above, they also adopted clustering
and classiﬁcation techniques to ﬁlter duplicates.
Similarities and Diﬀerences. Similar to the above
three studies, we address the problem of retrieving simi-
lar bug reports from repositories for duplicate bug report
identiﬁcation. Similar to the work in [17, 11], we only con-
sider natural language information which is widely avail-
able. The execution information considered in [22] is oftennot available and hard to collect, especially for binary pro-
grams. For example, in the OpenOﬃce, Firefox and Eclipse
datasets used in our experiment, the percentages of reportshaving execution information are indeed low (0.82%, 0.53%and 12% respectively). Also, for large bug repositories, cre-
ation of execution information for legacy reports can be time
consuming.
Compared to the three approaches, there are generally
three diﬀerences. First, to retrieve top-k similar repots,they used similarity measure to compute distances between
reports while we adopt an approach based on discrimina-
tive model. We train a discriminative model via SVMs toclassify whether two bug reports are duplicates of one otherwith a probability. Based on this probability score, we re-
trieve and rank candidate duplicate bug reports. What is
more, The approach in [11] also employed a classiﬁer trainedby SVMs, but for a diﬀerent purpose. Their classiﬁer wasused to return a boolean ﬂag of DUPLICATE or NEW fora new report, if the ﬂag was DUPLICATE, the new reportwould be ﬁltered and would not reach triagers. They re-ported that 8% of the duplicate reports could be ﬁltered intheir experiment. The second diﬀerence is that we introduce54 features, out of which 27 are based on bigrams, to better
discriminate duplicate bug reports. Then the trained dis-
criminative model can automatically infer optimum weightsfor each feature. This mechanism enables our approach tobe robust to bad features, adaptive to report repository evo-lution over time, and eﬀective for diﬀerent software projects.
Finally, instead of returning similar bug reports, we return
similar buckets. As each bucket represents a distinct defect,our approach can easily avoid two or more reports in thetop-k similar report list referring to the same defect.
From the point of performance view, we show that our
discriminative model approach outperforms all previous ap-proaches using natural language information alone by up to43% on bug report repositories of three large open sourceapplications including Firefox, Eclipse, and OpenOﬃce.
Besides the eﬀort on duplicate bug report detection, there
has also been eﬀort on bug report mining. Anvik et al. [1]and Cubranic and Murphy [8] and Lucca [9] all proposedsemi-automatic techniques to categorize bug reports. Based
on categories of bug reports, their approaches helped assign
bug reports to suitable developers. Menzies and Marcus alsosuggested a classiﬁcation based approach to predicting theseverity of bug reports [14]. Bettenburg et al. proposed awork on extracting structural information including stack
traces, patches, and codes from the descriptions of the bug
reports [5]. Ko and Myers analyzed the linguistic character-istics of bug-report summaries, and proposed a technique todiﬀerentiate between failure reports and feature calls [12].
They also emphasized on the need of duplicate bug report
detection, but did not propose a solution. While the aboveworksmentionedtheneedofduplicatebugdetectionorsomeoftheirtechniquesmaybeneﬁtduplicatebugdetection, noneofthemworkeddirectlyontheduplicatebugdetectionprob-
lem.
There have been several statistical studies and surveys of
existing bug repositories. Anvik et al. reported a statis-
tical study on open bug repositories with some interesting
results such as the proportion of diﬀerent resolutions and
the number of bug reports that a single reporter submit-ted [2]. Sandusky et al. studied the relationships betweenbug reports and reported some statistic results on duplicatebugs in open bug repositories [18]. Additionally, Hooimeijer
and Weimer suggested a statistics based model to predict
the quality of bug reports [10]. After that, Bettenburg et al.made a survey on the developers of several well-known opensource projects (Eclipse, Mozilla, and Apache) to study the
factors that developers cared most on dealing with bug re-
ports [3]. Bettenburg et al. also suggested that duplicatebug reports were actually not harmful but useful for thedevelopers [4]. So the requirement for duplicate bug reportdetection became even stronger because it could not only re-
duce the waste of developer’s time on duplicate bug reports
but also helped developers to gather more related informa-tion to solve the bug more quickly. In general, none of thesework proposed any approaches to duplicate-bug-report de-
tection, but some of the work pointed out the motivation
and eﬀect of detecting duplicate bug reports.
7. CONCLUSION & FUTURE WORK
In this work, we consider a new approach to detecting
53duplicate bug reports by building a discriminative model
that answers the question “Are two bug reports duplicatesof each other?”. The model would report a score on the
probability of A and B being duplicates. This score is then
used to retrieve similar bug reports from a bug report repos-itory for user inspection. We have investigated the utilityof our approach on 3 sizable bug repositories from 3 largeopen-source applications including OpenOﬃce, Firefox, and
Eclipse. The experiment shows that our approach outper-
forms existing state-of-the-art techniques by a relative im-provement of 17–31%, 22–26%, and 35–43% on OpenOﬃce,
Firefox, and Eclipse dataset respectively.
As a future work, we plan to investigate the utility of
paraphrases in discriminative models for potential improve-ment in accuracy. We have developed a technique to ex-
tract technical paraphrases [21] and is currently investigat-
ing their utility in improving detection of duplicate bug re-
ports. What is more, an interesting direction is incorpo-
rating response threads to bug reports as further sourcesof information. The other interesting direction is adoptingpattern-based classiﬁcation [7, 13] to extract richer feature
set that enables better discrimination and detection of du-
plicate bug reports.
8. REFERENCES
[1] J. Anvik, L. Hiew, and G. Murphy. Who should ﬁx
this bug? In proceedings of the International
Conference on Software Engineering , 2006.
[2] J. Anvik, L. Hiew, and G. C. Murphy. Coping with an
open bug repository. In eclipse ’05: Proceedings of the
2005 OOPSLA workshop on Eclipse technology
eXchange , pages 35–39, 2005.
[3] N. Bettenburg, S. Just, A. Schr ¨o t e r ,C .W e i s s ,
R. Premraj, and T. Zimmermann. What makes a good
bug report? In SIGSOFT ’08/FSE-16: Proceedings of
the 16th ACM SIGSOFT International Symposium onFoundations of software engineering, pages 308–318,
2008.
[4] N. Bettenburg, R. Premraj, T. Zimmermann, and
S. Kim. Duplicate bug reports considered harmful ...
really? In ICSM08: Proceedings of IEEE International
Conference on Software Maintenance , pages 337–345,
2008.
[5] N. Bettenburg, R. Premraj, T. Zimmermann, and
S. Kim. Extracting structural information from bugreports. In MSR ’08: Proceedings of the 2008
international working conference on Mining softwarerepositories, pages 27–30, 2008.
[6] C.-C. Chang and C.-J. Lin. LIBSVM: a library for
support vector machines , 2001. Software available at
http://www.csie.ntu.edu.tw/ ∼cjlin/libsvm.
[ 7 ]H .C h e n g ,X .Y a n ,J .H a n ,a n dC . - W .H s u .
Discriminative frequent pattern analysis for eﬀective
classiﬁcation. In ICDE, 2007.
[8] D. Cubranic and G. C. Murphy. Automatic bug triage
using text categorization. In Proceedings of the
Sixteenth International Conference on Software
Engineering & Knowledge Engineering , pages 92–97,
2004.
[9] L. G. An approach to classify software maintenance
requests. In ICSM ’02: Proceedings of the
International Conference on Software Maintenance ,page 93, 2002.
[10] P. Hooimeijer and W. Weimer. Modeling bug report
quality. In ASE ’07: Proceedings of the twenty-second
IEEE/ACM international conference on Automatedsoftware engineering, pages 34–43, 2007.
[11] N. Jalbert and W. Weimer. Automated Duplicate
Detection for Bug Tracking Systems. In proceedings of
the International Conference on Dependable Systemsand Networks, 2008.
[12] A. Ko and B. Myers. A linguistic analysis of how
people describe software problems. In IEEE
Symposium on Visual Languages and Human-Centric
Computing , pages 127–134, 2006.
[13] D. Lo, H. Cheng, J. Han, S.-C. Khoo, and C. Sun.
Classiﬁcation of Software Behaviors for Failure
Detection: A Discriminative Pattern MiningApproach. In proceedings of the SIGKDD Conference
on Knowledge Discovery and Data Mining, 2009.
[14] T. Menzies and A. Marcus. Automated severity
assessment of software defect reports. In ICSM08:
Proceedings of IEEE International Conference onSoftware Maintenance , pages 346–355, 2008.
[15] R. Nallapati. Discriminative models for information
retrieval. In SIGIR ’04: Proceedings of the 27th annual
international ACM SIGIR conference on Research and
development in information retrieval , 2004.
[16] P. R.Duda and D.Stork. Pattern Classiﬁcation .W i l e y
Interscience, 2nd edition, 2000.
[17] P. Runeson, M. Alexandersson, and O. Nyholm.
Detection of Duplicate Defect Reports Using Natural
Language Processing. In proceedings of the
International Conference on Software Engineering ,
2007.
[18] R. J. Sandusky, L. Gasser, R. J. S, U. L. Gasser, and
G. Ripoche. Bug report networks: Varieties,strategies, and impacts in a f/oss developmentcommunity. In International Workshop on Mining
Software Repositories , 2004.
[19] J. Sutherland. Business objects in corporate
information systems. In ACM Computing Surveys ,
2006.
[20] G. Tassey. The economic impacts of inadequate
infrastructure for software testing. National Institute
of Standards and Technology. Planning Report
02-3.2002, 2002.
[21] X. Wang, D. Lo, J. Jing, L. Zhang, and H. Mei.
Extracting Paraphrases of Technical Terms from Noisy
Parallel Software Corpora. In proceedings of the Joint
conference of the 47th Annual Meeting of theAssociation for Computational Linguistics and the 4th
International Joint Conference on Natural Language
Processing of the Asian Federation of Natural
Language Processing, 2009.
[22] X. Wang, L. Zhang, T. Xie, J. Anvik, and J. Sun. An
Approach to Detecting Duplicate Bug Reports using
Natural Language and Execution Information. In
proceedings of the International Conference onSoftware Engineering , 2008.
54
View publication statsView publication stats