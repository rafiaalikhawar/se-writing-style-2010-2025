Leveraging Usage Similarity for Effective Retrieval of
Examples in Code Repositories
Sushil K Bajracharya, Joel Ossher, and Cristina V Lopes
Bren School of Information and Computer Sciences
University of California, Irvine
Irvine, CA, USA
sbajrach@ics.uci.edu, jossher@ics.uci.edu, lopes@ics.uci.edu
ABSTRACT
Developers often learn to use APIs (Application Programming In-
terfaces) by looking at existing examples of API usage. Code repos-
itories contain many instances of such usage of APIs. However,
conventional information retrieval techniques fail to perform well
in retrieving API usage examples from code repositories. This pa-
per presents Structural Semantic Indexing (SSI), a technique to as-
sociate words to source code entities based on similarities of API
usage. The heuristic behind this technique is that entities (classes,
methods, etc.) that show similar uses of APIs are semantically re-
lated because they do similar things. We evaluate the effective-
ness of SSI in code retrieval by comparing three SSI based retrieval
schemes with two conventional baseline schemes. We evaluate the
performance of the retrieval schemes by running a set of 20 can-
didate queries against a repository containing 222,397 source code
entities from 346 jars belonging to the Eclipse framework. The re-
sults of the evaluation show that SSI is effective in improving the
retrieval of examples in code repositories.
Categories and Subject Descriptors
D.2.3 [ Software Engineering ]: Coding Tools and Techniques;
D.2.6 [ Software Engineering ]: Programming Environments;
H.3.3 [ Information Storage and Retrieval ]: Information Search
and Retrieval
General Terms
Algorithms, Design, Experimentation, Measurement
Keywords
API Usage, Code Search, SSI, Software Information Retrieval, Struc-
tural Semantic Indexing
This work has been supported in part by the National Science
Foundation‚Äôs grants CCF-0347902 and CCF-0725370
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proÔ¨Åt or commercial advantage and that copies
bear this notice and the full citation on the Ô¨Årst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciÔ¨Åc
permission and/or a fee.
FSE-18, November 7‚Äì11, 2010, Santa Fe, New Mexico, USA.
Copyright 2010 ACM 978-1-60558-791-2/10/11 ...$10.00.1. INTRODUCTION
The large availability of software on the Web is having a fun-
damental impact on software development in at least two ways.
First, web search engines have enabled the retrieval of software
that would otherwise be undiscoverable. Second, thanks to the
wide availability of all sorts of libraries, developers often prefer
to reuse components than to write something from scratch. These
days, it is possible to Ô¨Ånd libraries that implement virtually every
well-known piece of functionality ‚Äì both because those libraries
exist and because they are Ô¨Åndable via search engines. For exam-
ple, if we develop a program that needs to perform Singular Value
Decomposition (SVD), we need only to type ‚Äújava singular value
decomposition‚Äù in Google or any other search engine. Google re-
turns as second hit a link to JAMA, a basic linear algebra package
for Java. It is then the developer‚Äôs task to import that package and
call the appropriate Application Programming Interfaces (APIs) in
it; the knowledge of how SVD works is abstracted in the API. This
allows developers to focus on the core of what they are developing
(for example, an indexer) without having to become experts in top-
ics like linear algebra, and without having to implement that piece
of expertise.
Indeed, a signiÔ¨Åcant part of programming activity now revolves
around Ô¨Ånding and using the right APIs. Some research studies
have conÔ¨Årmed that. Hoffmann et al. [24] showed that when de-
velopers search the Web, the majority of their queries are about
APIs. Other studies of developers‚Äô needs and practices have shown
that developers seek and use code examples to learn APIs [35, 32,
39, 34, 36, 44].
Finding the right packages is easy; however, it is only one half
of the problem. The other half has to do with learning how to use
the APIs of those packages. For example, in the case of JAMA,
the developer needs to Ô¨Ånd out what classes to instantiate and what
methods to call in order to get an SVD of a given matrix. In order
to do this, the developer might look into the documentation, if it
exists in any form (Javadoc, tutorials, examples, etc.) on the Web
site where the package is hosted. Often it doesn‚Äôt exist, or it is too
brief or too long.
Existing approaches that try to tackle this second part of the
problem work by harvesting words from documents on the Web
that describe API usage. While these approaches improve the pro-
cess of using packages that have tutorials or other documents avail-
able elsewhere on the Web, they fail to leverage a potentially large
and rich source of information highly relevant to what the develop-
ers are looking for: the collection of projects that are also available
on the Web and that usethose packages. In these projects there is
a variety of concrete examples of how to use the APIs of interest.
Moreover, in many cases the developer‚Äôs needs include combina-
tions of packages, seldom documented in tutorials; retrieving API
157
usage examples from actual projects can serve this need, because
these combinations are bound to exist out there.
Retrieving useful code fragments from source code alone can be
very difÔ¨Åcult. Compared to natural language text, source code has
a scarcity of words that describe the entities deÔ¨Åned and used in
the code. This makes it more prone to the vocabulary mismatch
problem which states that the likelihood of two people choosing
the same keyword for a familiar concept is only between 10-15%
[19]. In the context of Ô¨Ånding API usage information, the vocab-
ulary mismatch problem manifests itself as the gap between the
situation and solution models [18]. API designers implement the
APIs using words that describe the solution models while the de-
velopers seeking to use APIs might be using words that come from
their situation model. This mismatch hinders the very Ô¨Årst step of
locating the right APIs. In general-purpose Web search engines
this mismatch is mitigated partly due to the abundance of linguis-
tically rich documents. Sophisticated information retrieval models
that work well on natural language text tend to be infeasible and
not scalable for source code because source code is linguistically
sparse. However, unlike natural language documents, source code
is rich in structure. Therefore, a retrieval technique that leverages
this property of source code is likely to address the problem of lo-
cation better.
This paper presents a technique that we call Structural Semantic
Indexing (SSI). The key idea is the following: code entities that
share common usage of APIs are functionally similar, and can share
the terms used to deÔ¨Åne each other. Simply put, if both AandB
use a common set of APIs in a similar manner, then AandBare
semantically related even if their names are different. As such, a
query for Ashould retrieve Band vice-versa. In SSI, we build an
index taking these relations into account. This way we are able to
mitigate the scarcity of words in source code and retrieve more, and
more relevant, code fragments.
To illustrate this idea with an example, consider a use case where
a developer needs to write some Java code to extract contents of a
zipped archive. The standard way to do this is to use some APIs
from the standard Java library, namely the classes: ZipInputStream ,
FileInputStream ,ZipEntry and the respective methods in those
classes. Using SourcererDB, an aggregated database of usage re-
lations in large open source Java projects [33], we found out that
some of the methods that use these APIs are named as follows:
extract ,unzip ,unpackZip ,explodeJarFile ,decompress ,unpack-
Archive etc. These methods with similar functionality are given
different names. However, the terms that can be extracted from
one method name are seemingly relevant in describing the imple-
mentation in another. This example suggests that using API usage
similarity, we can harvest similar words to describe code entities.
The primary hypothesis of this paper is that a source code re-
trieval scheme that uses SSI can be more effective. A retrieval
scheme deÔ¨Ånes how a user‚Äôs query is processed and matched with
the documents in a corpus to retrieve a ranked list of relevant re-
sults. To validate our hypothesis we introduce three new retrieval
schemes based on SSI that incorporate different measures of usage
similarity. We compare these schemes with two baseline schemes
that do not use similarity information. The Ô¨Årst baseline scheme
uses a combination of three heuristics based on code structure that
we introduced in our earlier work [27]. The second baseline adds
a heuristic based on using API calls to improve retrieval in code
search. This heuristic has been used recently in various code search
systems [13, 21, 20]. The three SSI based retrieval schemes we in-
troduce in this paper add the heuristic of usage similarity on top of
structure and usage. By comparing these schemes with two base-
Figure 1: Code snippets as examples to teach API usage
line schemes using existing heuristics to enhance code retrieval, we
are able to properly validate our hypothesis.
We implemented all the retrieval schemes in an example retrieval
system. The retrieval system consists of a code search tool that
takes a keyword query and hands it over to a retrieval scheme. The
retrieval scheme returns a ranked list of code entities as a result.
The search tool extracts a code snippet from the returned entities.
The retrieval schemes were evaluated on the basis of their ability
to retrieve code entities that produced code fragments showing the
APIs relevant to the given query. Our results show that the SSI
based retrieval schemes improve code retrieval.
In summary, the paper makes the following contributions:
It introduces Structural Semantic Indexing, a novel approach
to tackle the problem of source code retrieval by harvesting
words for a code entity based on API usage similarity.
It provides empirical evidence that SSI can enhance the ef-
fectiveness of retrieving relevant code snippets.
It introduces a technique to extract code snippets showing
API usage information from search results of a code search
engine.
The rest of the paper is organized as follows. Section 2 intro-
duces code snippets as API usage example and gives an overview of
the example retrieval system we implemented. Section 3 presents
the heuristics to facilitate retrieval of API usage examples, and the
retrieval schemes that we devised. Section 4 elaborates the idea of
usage based similarity and discusses its implementation in our re-
trieval system. Section 5 provides details on the evaluation method-
ology and the results. Remaining sections discuss the validity of
our work, related work and conclusion.
2. RETRIEVING API USAGE EXAMPLES
The de facto method of explaining the usage of APIs is to show
a code snippet where the APIs are used. All books and tutorials on
teaching APIs follow this style. Figure 1 shows such a snippet. The
snippet is taken from ‚ÄúThe Java Developer‚Äôs Guide to Eclipse‚Äù, one
of the standard books that teaches using Eclipse APIs. It demon-
strates the task of programmatically writing to the Eclipse work-
bench‚Äôs log. Using this snippet as an example, the book shows that
the primary API to achieve this task is to call the logmethod avail-
able in the interface org.eclipse.core.runtime.ILog .logtakes
a parameter of type org.eclipse.core.runtime.IStatus , and an
instance of ILog can be obtained by calling the getDefault method
available in a plugin instance.
For the purpose of this paper, an ‚ÄúAPI usage example‚Äù is de-
Ô¨Åned as a code snippet that shows relevant APIs being used in the
code. The search tool we implemented extracts code snippets that
look like the one shown in Figure 1. The extracted snippets in-
clude additional comments showing the fully qualiÔ¨Åed names of
the used API entities. Such a snippet is shown in Figure 2 that cor-
responds to the same task of writing to Eclipse workbench‚Äôs log.
158With this deÔ¨Ånition of API usage example, the performance of a
retrieval scheme to locate API usage examples can be measured in
terms of its ability to retrieve a relevant code snippet from a code
repository.
Retrieval System Overview
We implemented an example retrieval system on top of Sourcerer,
an infrastructure for large-scale indexing and analysis of open source
code [10, 33, 9, 27]. Sourcerer was designed by our research group
to easily support the creation of next-generation code search tools.
We will brieÔ¨Çy describe how the retrieval system was implemented.
Further details on the Sourcerer infrastructure can be found in our
previous publications.
For the retrieval system, we Ô¨Årst create a repository of Ô¨Åles con-
sisting of Java binaries and source code for those binaries. The
Sourcerer Feature Extractor is used to process the Ô¨Åles in the repos-
itory, building a relational model of their structure and reference in-
formation. This model captures entities, such as classes and meth-
ods, and the relations between them, such as inheritance or method
calls. Once extraction is completed, the results are imported into
SourcererDB.
Next, we produce a similarity index that is used to perform the
similarity computation. For each entity, this index contains the
fully qualiÔ¨Åed name of that entity, as well as the fully qualiÔ¨Åed
names of each entity that it uses. This similarity calculation, as dis-
cussed in section 4, uses both the index and SourcererDB to com-
pute for each entity a set of similar code entities. Once computed,
this information is stored back in SourcererDB. Lastly, a search in-
dex is created based on the updated SourcererDB that is used to
support the retrieval system itself.
The retrieval schemes themselves are implemented on top of this
Ô¨Ånal search index. A search tool is used to invoke these retrieval
schemes using standard keyword queries. Each scheme deÔ¨Ånes ex-
actly how a standard keyword query is matched against the search
index to retrieve a ranked list of results. The ranked list of results
returned by a retrieval scheme is processed by the search tool to
produce a list of code snippets, which are returned as the Ô¨Ånal re-
sult of the search. One such code snippet extracted by the retrieval
system is shown in Figure 2.
3. RETRIEVAL SCHEMES
A retrieval scheme is a mechanism for processing a keyword
query in order to retrieve relevant documents from a document cor-
pus. Table 1 lists the Ô¨Åve retrieval schemes we tested, along with the
heuristics that each scheme uses. The Ô¨Årst column of the table con-
tains the identiÔ¨Åer of the scheme, and the remaining columns list
the heuristics. A check mark ( X) indicates that a retrieval scheme
uses the corresponding heuristic. Schemes B1andB2are baseline
schemes, indicating levels of performance without any usage sim-
ilarity comparisons. Schemes S1,S2, and S2are the code retrieval
schemes that implement SSI using three different usage similarity
measures.
3.1 Heuristics
In this section we will describe Ô¨Åve different heuristics that can
be used to improve code retrieval. These heuristics capture cer-
tain characteristics of source code artifacts. The Ô¨Årst three heuris-
tics leverage the structural and textual content available in source
code, while the fourth instead focuses on the relations (e.g. API
calls) between entities. These four heuristics capture some of the
well known techniques for enhancing code retrieval. The Ô¨Årst three
ID
Code As Text
Focusing on Names
SpeciÔ¨Åcity
Usage
Usage Similarity
(TF-IDF)
Usage Similarity
(Hamming Distance)
Usage Similarity
(Tanimoto CoefÔ¨Åcient)
B1
X
X
X
B2
X
X
X
X
S1
X
X
X
X
X
S2
X
X
X
X
X
S3
X
X
X
X
X
Table 1: Retrieval Schemes Used in the Evaluation
comes from our earlier work in [27], while the fourth has been used
in various other code search tools [13, 21, 20]. The Ô¨Åfth heuristic
embodies SSI, the primary contribution of this work, and is based
on the idea of usage similarity introduced in this paper.
Code as Text : One of the most well-known and successful meth-
ods for ranking generic text documents is term frequency - inverse
document frequency (TF-IDF) [30]. TF-IDF‚Äôs heuristic deÔ¨Ånes the
relevance of a document with respect to a search term as a function
of the number of occurrences of the search term in that document
(TF) multiplied by inverse document frequency (IDF). This is usu-
ally computed as logN
d f; where, Nis the total documents in the
collection, and d fis the document frequency - the number of doc-
uments in the collection that contains the term.
Focus on Names : Developers often give meaningful names to
the entities they deÔ¨Åne in the code. This suggests that one should
focus the search on the names of packages, classes, and methods,
giving them higher weight compared to words taken from other
parts of the code. To capture this concept of meaningful names we
use the Fully QualiÔ¨Åed Names (FQNs) for all entities in the code.
SpeciÔ¨Åcity : Source code is structured such that there is a strict
containment relation between packages, classes, and methods, which
developers use this to organize their thoughts/designs. Therefore
a result due to a matching package name is likely less signiÔ¨Åcant
than a result due to a matching class name. Similarly, a class-based
match is likely less valuable than a method-based one. This leads
to a heuristic that treats the terms extracted found in the right most
part of the FQN as more signiÔ¨Åcant. For example, in the FQN
java.lang.StringBuffer , terms ‚Äòstring‚Äô and ‚Äòbuffer‚Äô extracted
from the simple name StringBuffer is considered more relevant
than terms ‚Äòlang‚Äô or ‚Äòjava‚Äô extracted from the left parts of the FQN.
Usage : This heuristic emphasizes the usage relations that exist
in source code. An entity‚Äôs usage of APIs or other external entities
can provide important insight into the functionality of the entity
itself. This heuristic seeds an entity with extra terms extracted from
the FQNs and the Javadoc documentation of all the API elements
(classes, methods and interfaces) that it uses.
Usage Similarity : In a large code repository there are many in-
stances of similar API usage. This heuristic captures the idea that
source code entities sharing common API usage patterns are func-
tionally similar. Therefore, the terms associated with one entity can
be shared with the other. This heuristic seeds a code entity with the
terms extracted from the simple names of other code entities that
have similar usage proÔ¨Åles. We use three different measures to
compute usage similarity: similarity based on TF-IDF of the FQNs
of used entities, similarity based on the Hamming Distance mea-
sure, and the Tanimoto CoefÔ¨Åcient measure. These measures are
discussed in detail in Section 4.
159/ / USES org . e c l i p s e . c o r e . r u n t i m e . I S t a t u s
 / / INSTANTIATES org . e c l i p s e . c o r e . r u n t i m e . S t a t u s . < i n i t >( i n t , j a v a . l a n g . S t r i n g , i n t , j a v a . l a n g . S t r i n g , j a v a . l a n g . Throwable ) 
}catch ( B a c k i n g S t o r e E x c e p t i o n e ) {
I S t a t u s s t a t u s = new S t a t u s ( I S t a t u s .ERROR,
UIPlugin . g e t D e f a u l t ( ) . g e t B u n d l e ( ) . getSymbolicName ( ) , I S t a t u s .ERROR,
e . g e t L o c a l i z e d M e s s a g e ( ) , e ) ;
/ / CALLS org . e c l i p s e . c o r e . r u n t i m e . ILog . l o g ( org . e c l i p s e . c o r e . r u n t i m e . I S t a t u s )
/ / CALLS org . e c l i p s e . c o r e . r u n t i m e . P l u g i n . getLog ( )
UIPlugin . g e t D e f a u l t ( ) . getLog ( ) . l o g ( s t a t u s ) ;
Figure 2: Annotated API usage example for the task of programmatically writing to Eclipse workbench‚Äôs log
3.2 Incorporating Heuristics in Search Index
To incorporate the heuristics into the retrieval schemes, we cre-
ated a full-text search index based on Lucene. Lucene is an efÔ¨Åcient
information retrieval engine [5]. The search index stores code en-
tities as a collection of documents. Each document consists of a
number of Ô¨Åelds. These Ô¨Åelds store terms that are either extracted
from various parts of a code entity, or harvested from other related
code entities. To populate the Ô¨Åelds, various tokenizers were im-
plemented that extract meaningful terms from the FQNs and the
textual content of the code entities in the repository. Common prac-
tices in naming, such as the use of camel-case and the use of spe-
cial characters (eg; ‚Äú_‚Äù, ‚Äú-‚Äù) were used to split the names into these
terms.
Table 2 lists 12 Ô¨Åelds that exist in the search index. Table 3
shows the relationship between the index Ô¨Åelds and the heuris-
tics described above. To capture the heuristic ‚ÄúCode as Text‚Äù, the
search index contains an index Ô¨Åeld that stores the terms extracted
from the full text of a code entity. This Ô¨Åeld is populated by re-
trieving the source code from the Sourcerer repository. To cap-
ture ‚ÄúFocusing on Names‚Äù, the index contains a Ô¨Åeld that stores
the terms extracted from the FQN of a code entity, and to capture
‚ÄúSpeciÔ¨Åcity‚Äù, it contains a Ô¨Åeld that stores the terms extracted from
the simple name of a code entity. To populate these two Ô¨Åelds, the
FQN of a code entity is retrieved from SourcererDB which is then
processed using the tokenizers.
To capture the heuristic ‚ÄúUsage‚Äù, the search index contains Ô¨Åve
different Ô¨Åelds. These Ô¨Åelds store the terms extracted from the
FQNs and documentation of used entities. To populate these Ô¨Åelds
for an entity ‚Äòe‚Äô, a request is sent to SourcererDB to list all of its
dependencies on the JSL (Java Standard Library) and non JSL en-
tities. The returned FQNs are then tokenized and stored in the re-
spective Ô¨Åelds. For Class entities, the following relations are con-
sidered as dependencies: Extends (inheritance), Implements (in-
terface implementation), and Uses (type reference). The following
relations are considered as dependencies for methods and construc-
tors: Calls (method call), Instantiates (object creation) and Uses
(Type Reference).
To capture the heuristic ‚ÄúUsage Similarity‚Äù, the search index
contains four different Ô¨Åelds. These Ô¨Åelds store the terms that are
extracted from the simple names of other entities that have similar
usage proÔ¨Åle.
3.3 Implementation
A retrieval scheme converts a simple keyword query to a more
complex boolean query that matches the terms in the query with
the terms stored in various index Ô¨Åelds. Matches in some Ô¨Åelds are
regarded as more signiÔ¨Åcant than others. Each Ô¨Åeld in the index is
assigned a boost value according to its signiÔ¨Åcance. The speciÔ¨Åc
Ô¨Åelds that are searched depend on the heuristics that are included
in a given retrieval scheme.
To illustrate this with an example, consider a keyword query such
as ‚Äúrestart workbench‚Äù. In the context of learning about Eclipse
APIs, this query might express the information need of Ô¨Ånding a
Id
Stores the terms extracted from the . . .
F1
full text of the code and comments of an entity
F2
FQN of an entity
F3
simple name of an entity
F4
FQNs of the used JSL (Java Standard Library) entities
F5
FQNs of used non non-JSL entities
F6
simple name of the used JSL entities
F7
simple names of the used non-JSL entities
F8
Javadoc of the used non-JSL entities
F9
simple names of other entities that have similar usage of JSL entities
(similarity based on the Vector-space model)
F10
simple names of other entities that have similar usage of non-JSL
entities (similarity based on the Vector-space model)
F11
simple names of other entities that have similar usage (similarity
based on Hamming Distance)
F12
simple names of other entities that have similar usage (similarity
based on the Tanimoto CoefÔ¨Åcient)
Table 2: Search index Ô¨Åelds
Heuristics Captured
Index Ô¨Åelds searched
Code as Text
F1
Focusing on Names
F2
SpeciÔ¨Åcity
F3
Usage
F4, F5, F6, F7, F8
Usage Similarity (TF-IDF)
F9, F10
Usage Similarity (Hamming Distance)
F11
Usage Similarity (Tanimoto Coeffcient)
F12
Table 3: Index Ô¨Åelds used to capture the Heuristics
code example for restarting the Eclipse workbench. The retrieval
scheme B1includes three heuristics. To capture the ‚ÄúCode as Text‚Äù
heuristic it searches an index Ô¨Åeld that stores the terms extracted
from the full text of a code entity (F1); to capture ‚ÄúFocusing on
Names‚Äù it searches an index Ô¨Åeld that stores the terms extracted
from the FQN of a code entity (F2), and to capture ‚ÄúSpeciÔ¨Åcity‚Äù
it searches a Ô¨Åeld that stores the terms extracted from the simple
name of a code entity (F3). To apply the ‚ÄúSpeciÔ¨Åcity‚Äù heuristic, a
match in the Ô¨Åeld F3 is weighted twice more than matches in F1
and F2.
To retrieve code entities from the index, scheme B1transforms
and executes the term query ‚Äúrestart workbench‚Äù as the following
Lucene query:
( F1 : r e s t a r t ^ 2 5 . 0 OR F2 : r e s t a r t ^ 2 5 . 0
OR F3 : r e s t a r t ^ 5 0 . 0 )
AND ( F1 : workbench ^ 2 5 . 0 OR F2 : workbench ^ 2 5 . 0
OR F3 : workbench ^ 5 0 . 0 )
This query searches for code entities that contain both the terms
"restart" and "workbench" in any one of its Ô¨Åelds: F1, F2, and F3.
Code entities that contain the query terms in their simple names
(F3) are ranked higher (see the 50 :0 boost value for Ô¨Åeld F3) com-
pared to those where the terms appear in the full text (Ô¨Åeld F1 with
25:0 boost value), or other parts of the FQN (Ô¨Åeld F2 with 25 :0
boost value).
The general procedure can be described as follows. Given a
query made up of ‚Äòn‚Äô terms: T1 ... Tn; a retrieval scheme that
searches in ‚Äòm‚Äô Ô¨Åelds: F1 .. Fm, each with the boost values of
160BV_FS1 .. BV_Fm respectively, returns the results of running a
Lucene query that is generated with the following form:
( F1 : T1^BV_F1 OR F2 : T1^BV_F2 . . OR Fm: T1^BV_Fm)
AND ( F1 : T2^BV_F1 OR F2 : T2^BV_F2 . . OR Fm: T2^BV_Fm)
. .
AND ( F1 : Tn^BV_F1 OR F2 : Tn^BV_F2 . . OR Fm: Tn^BV_Fm)
Table 4 lists the boost values that are assigned to each of the in-
dex Ô¨Åelds. These values were chosen after some experimentation
to see how changing these values affected the results. We can fully
understand how each retrieval scheme searches the search index us-
ing four tables: Table 1 shows the list of heuristics for each retrieval
scheme, Table 2 explains what information is stored in each index
Ô¨Åeld, Table 3 shows what index Ô¨Åelds are used by each heuristic,
and Table 4 shows the boost values assigned to each index Ô¨Åeld.
Ranking: The retrieval schemes rank the retrieved entities based
on the scores of their similarity to the query. The retrieval schemes
use the scoring that is built into Lucene. Lucene uses a combina-
tion of the Boolean Model (BM) and Vector Space Model (VSM)
of information retrieval [30] for scoring. Lucene Ô¨Årst uses the
BM to narrow down the documents using the boolean logic in the
query speciÔ¨Åcation, and then uses the VSM to compute the simi-
larity score based on the cosine distance measure [30]. The cosine
distance measure incorporates the TF-IDF values computed from
the full text of the retrieved entities. Full details on this scoring can
be obtained from [3].
3.4 Rationale for the Schemes
Scheme B1searches the code index using only the information
available in the code entity itself. It retrieves a code entity if all of
the terms in a query match the terms extracted from its full text and
the FQN. While doing so, it gives priority to those entities where a
match is made on the simple name. Secondary priority is given to
matches on other portions of the FQN and the full text of the entity.
This combines the Ô¨Årst three heuristics ‚ÄúCode as Text‚Äù, ‚ÄúFocusing
on Names‚Äù, and ‚ÄúSpeciÔ¨Åcity‚Äù. In our past evaluation of code search
engines for Ô¨Ånding implementations, these three heuristics proved
to provide good results [27]. Thus, for this paper, we use their
combination as Scheme B1and consider it to be the Ô¨Årst baseline
with which other schemes can be compared.
Scheme B2searches in all Ô¨Åelds that B1searches in. In addi-
tion, it also searches in Ô¨Åelds that store the terms extracted from the
used entities. This scheme incorporates the heuristic ‚ÄúUsage‚Äù in ad-
dition to the others included in B1. The contents from JSL entities
have minimum boost, and contents from simple name of non-JSL
entities have the maximum boost. This is done to rank entities us-
ing Eclipse APIs more highly compared to those using JSL APIs.
This scheme therefore retrieves code entities based not only on the
content found in a code entity itself, but also found in portions of
entities that it uses. It further prioritizes those results where a term
in a query matches the contents from simple name of a used entity.
Usage information, in particular API calls and Javadoc documen-
tation, have been used to improve search engines to Ô¨Ånd software
applications and retrieve useful code snippets [20, 21, 13]. Lever-
aging usage information is a well known technique in improving
software retrieval systems, therefore we add the usage heuristic in
B2and use it as a second baseline.
Schemes S1,S2, and S3are three different variants of retrieval
schemes based on SSI. These schemes retrieve all the code entities
thatB2does. But these schemes also match the query terms to ad-
ditional Ô¨Åelds that store the terms imported from simple names of
other entities with similar usage proÔ¨Åles as the code entity being
indexed. This scheme applies the heuristic ‚ÄúUsage Similarity‚Äù on
top of others. Since there is more than one way to measure usageID
F1
F2
F3
F4
F5
F6
F7
F8
F9
F10
F11
F12
 BV 25 25 50 1 50 1 100 50 1 50 50 50 
Table 4: Boost Values (BV) Applied to Fields (ID = Field Id)
similarity, we introduce three schemes that incorporate three dif-
ferent usage similarity measures. On one hand, comparing S1,S2,
andS3with the baselines help validate our hypothesis about the
effectiveness of SSI. On the other hand, comparing these schemes
with each other will tell whether there is a difference in picking a
particular usage similarity measure to be used for SSI.
In terms of standard information retrieval measures of precision
and recall [30], SSI based schemes ( S1,S2, and S3) are designed
to provide better recall values than B2, and B2is designed to pro-
vide better recall than B1. The improvement in recall is expected
because these schemes harvest more terms to describe a code entity
either from used entities or similar entities. It is expected that pre-
cision and ranking of relevant results are not compromised while
increasing recall.
4. USAGE BASED SIMILARITY
A notion of similarity can be deÔ¨Åned for code entities based on
their common usage proÔ¨Åle. Three different measures of similar-
ity based on common usage are used to incorporate the heuristic
‚ÄúUsage Similarity‚Äù in the code search index.
4.1 TF-IDF based Similarity
This measure of similarity is based on the Vector Space Model
representation of code entities. Each code entity is considered to
be a document, and FQNs of other entities used by the code entity
is considered to be the words in the document. The similarity of an
entity with other entities is computed based on a TF-IDF measure
over the vector space as deÔ¨Åned by the FQNs.
input : eid = entity id; f = Ô¨Åeld storing used APIs
output : sim = list of similar entities using same APIs as the entity with eid
begin
q = a priority queue, stores terms sorted by their weight;
forall term t IN f of entity with entity id = eid do
w = idf(t) * df(t);
//w = weight of term ‚Äôt‚Äô
//idf = inverse document frequency from lucene index
//df = document frequency, stored in term vector
insert t with weight w in q;
new_query = top 25 terms from q;
/*get top 30 results from running the query ‚Äúf:(new_query)‚Äù in the Lucene
index */
sim = topHitsFromLucene(f, new_query, 30);
end
Algorithm 1 : Mechanics of Computing Similar Entities using
Lucene‚Äôs MLT query
Our implementation of this similarity measure is based on Lucene.
For this purpose we created a Similarity Index for entities contain-
ing a set of documents. Each document has three Ô¨Åelds storing the
following information: (i) a unique id for an entity ( Feid), (ii) full
FQNs of all JSL (Java Standard Library) entities used by the entity
(Fs1), and (iii) full FQNs of all non-JSL entities used by the entity
(Fs2). For a given entity with entity id ‚Äòentity_id‚Äô, entities similar to
it can be computed by issuing a ‚Äúmore like this‚Äù (MLT) query that
Lucene supports. The underlying mechanism of how MLT query
works for computing similar entities is shown in Algorithm 1. In
the algorithm, choosing one of the following Ô¨Åelds: Fs1orFs2, as
a value for the input ‚Äòf‚Äô (Ô¨Åeld) returns a list of entities that have
similar usage of JSL and non-JSL entities respectively.
1614.2 Similarity using Feature Vectors
The remaining two measures of similarity are based on a similar-
ity model commonly used in collaborative recommendation [11].
A usage proÔ¨Åle for a code entity is represented as a binary fea-
ture vector whose entries denote use of other entities. The infor-
mation about relations stored in SourcererDB is used to compute
the feature vectors for each entity that exists in our corpus. For
each entity the feature vector is calculated by querying Sourcer-
erDB for the following relations: ‚ÄòCalls‚Äô (method call), ‚ÄòExtends‚Äô
(inheritance), ‚ÄòImplements‚Äô (interface implementation), ‚ÄòInstanti-
ates‚Äô (object instantiation), and ‚ÄòUses‚Äô (type reference). Following
the same approach as used by Bruch et al :in [12], usage infor-
mation for each code entity is encoded as a binary vector. After
obtaining this binary feature vector for each code entity, a neigh-
borhood of similar users is obtained using two similarity measures
for binary vectors: (i) based on a modiÔ¨Åed version of Hamming
Distance (adapted from [12]), and (ii) the Tanimoto CoefÔ¨Åcient
[48] measure. The FQNs of top 45 entities from such a neighbor-
hood are extracted and stored back in SourcererDB for each code
entity.
Hamming Distance for two given bit vectors is deÔ¨Åned as the
number of bits in which they differ. Bruch et al :modiÔ¨Åed this mea-
sure by calculating bit difference on a partial feature space, and
showed the modiÔ¨Åed measure to be effective in their work of API
recommendation [12]. We use the same technique of using only
the partial feature space while computing similarity. Our imple-
mentation differs from theirs in terms of the relations and the types
of entities that are used in constructing the bit vector. We use the
following distance measure: Distance HD(e1; e2) = Pe1 jFe1\Fe2j
where, Distance HD(e1; e2)is the modiÔ¨Åed hamming distance be-
tween e1ande2,Feis the bit vector for entity e, andPeis the num-
ber of true bits in Fe. Similarity measure based on this distance is
computed as follows: Similarity HD= 1=(1 + Distance HD)
Tanimoto CoefÔ¨Åcient also known as the extended Jaccard coef-
Ô¨Åcient can be used as a similarity measure for two bit vectors. It is
given as follows: Similarity TC(e1; e2) =jFe1\Fe2j
jFe1j+jFe2j jFe1\Fe2j
5. EVALUATION
The goal of our evaluation is to validate our hypothesis that SSI
enhances the effectiveness of code retrieval by leveraging usage
based similarity. We also seek to compare the performance of the
retrieval schemes to observe the effect of using structure and sim-
ilarity based heuristics in code retrieval. The effectiveness of a re-
trieval scheme is measured based on its ability to produce relevant
code snippets for a given set of candidate queries.
We Ô¨Årst discuss the snippet extraction technique and then present
the details of the evaluation.
5.1 Snippet Extraction
A retrieval scheme takes a keyword query and returns a ranked
list of code entities as search result. This ranked list of entities
is called hits and each entry in the list is called a hit. The re-
trieval scheme also returns the total number of entities in the index
that match the query. For each hit the corresponding ‚Äòentity_id‚Äô
(a unique identiÔ¨Åer for a code entity) is available. Further details
about the code entity can be queried from SourcererDB using the
‚Äòentity_id‚Äô. The search tool uses the information returned by a re-
trieval scheme to extract a corresponding code snippet for each hit
(entity) in the list.
Snippet extraction proceeds in two steps. First, given a set of
hits, a list of top used entities is generated. This process is shown
in Algorithm 2. The maximum of 10 or 10% of the total number
of hits is processed to generate the list of top APIs. The searchinput : hits = top ‚Äòn‚Äô hits returned as search results; where, n = max_of(10,
10% of total hits)
output : top_used = list of top used entities
begin
list_eid = all entity ids from hits;
/*getTopApis(..) selects top 5 non-JSL (Java Standard Library) entities of
each type (Interface, Method, Constructor, Classes) from SourcererDB
such that they are used by at least 3 entities in the hits */
top_used = getTopApis(hits);
end
Algorithm 2 : Getting the list of top used entities
tool queries SourcererDB for the top non Java Standard Library
entities that are used by the entities in the list. For each entity
top 5 Interfaces, Methods, Constructors, and Classes are selected.
Among all these used entities in the list, only those entities that
are used by at least 3 different entities are returned as the top used
entities.
input : eid = entity id, top_used = top used entities
output : snip = an annotated code snippet
begin
snip = empty string;
/*getUsedPositions(..) looks up SourcererDB and returns all positions in
the code where top_used entities are used. Positions are mapped to a list
of used entities */
used_pos_map = getUsedPositions(top_used, eid);
forall position IN used_pos_map do
rationale = empty string;
forall used_entity IN used_pos_map[position] do
/*Below, append(a,b) returns a new string by appending string
‚Äòb‚Äô to ‚Äòa‚Äô.createRationale(..) selects relation type and FQN of
used entity and creates a rationale as a comment */
rationale = append(rationale, createRationale(used_entity, eid));
/*extractFragment(..) extracts the surrounding expression in a code
entity from position */
snip_fragment = extractFragment(eid, position);
/*appendSnip(..) works same as append(..) and returns true if
rationale and snip_fragment do not already exist in snip */
ifappendSnip(rationale, snip_fragment) =2snipthen
snip = appendSnip(snip, rationale);
snip = appendSnip(snip, snip_fragment);
end
Algorithm 3 : Snippet Extraction
Second, given the list of top used entities and the ‚Äòentity_id‚Äô,
a code snippet is extracted from a code entity. The algorithm for
this process is shown in Algorithm 3. The procedure Ô¨Årst queries
SourcererDB to locate all the positions in the source of an entity
where any of the top APIs are used. Few surrounding lines of code
are extracted from each such starting position. On top of these
extracted lines comments indicating the FQNs and relation types
of the top used entities are inserted. Finally, a sequence of these
commented code fragments is returned as an example code snippet.
A sample code snippet generated using a hit returned for a query
‚Äúwrite to workbench error log‚Äù is shown in Figure 2.
5.2 Evaluation Methodology
The evaluation methodology consists of building a corpus to test
the retrieval schemes, creating a set of candidate queries, executing
the queries using all retrieval schemes to generate code snippets,
assessing the relevancy of each code snippet, and Ô¨Ånally selecting a
set of metrics to compare the effectiveness of the retrieval schemes.
This makes our methodology similar to the widely used CranÔ¨Åeld
style evaluation of information retrieval systems [14, 38].
Corpus: The corpus for the evaluation consists of: (i) A Ô¨Åle
repository consisting of 346 jars from the Eclipse framework. Of
these 346 jar Ô¨Åles, 146 of them were found to contain source code.
As Eclipse separates source code and binaries (class Ô¨Åles) into dif-
162ID
Query
Q1 copy paste data from clipboard 
Q2
open url inhtml browser
Q3
track mouse hover
Q4
write toworkbench error log
Q5
track job status change
Q6
open Ô¨Åle inexternal editor
Q7
batch workspace changes insingle operation
Q8
remove problem marker from resource
Q9
highlight text range ineditor
Q10
update status line
Q11
prompt user toselect directory
Q12
use shared image from workbench
Q13
open dialog andask yes noquestion
Q14
parse source string ast node
Q15
extract return type from method declaration inast node
Q16
Ô¨Åll table background thread
Q17
platform debug tracing forplugin
Q18
get display created incurrent thread
Q19
run job inui thread
Q20
open external Ô¨Åle
Table 5: Candidate Queries Used in the Evaluation
ferent jar Ô¨Åles, this indicates that there were 200 different plugins
(jars), of which 54 did not have associated source code. These 346
jar Ô¨Åles were taken from the plugins directory of the 64-bit Linux
version of Eclipse Galileo (V3.5.1). (ii) SourcererDB storing all
entities, relations, and similarity calculation results extracted from
the source code and binaries from the 346 jars in the Ô¨Åle repository.
To populate SourcererDB a full extraction (entities and relations
among the entities) was performed on plugins with source code.
For the plugins without source, we extracted all the entities, and
the relations to them from the entities extracted from the plugins
with source. The database consisted of 996,477 code entities out
of which 785,600 had source. It stored 5,775,514 relations among
the entities. (iii) A code search index that is used to run the retrieval
schemes. The code index is built using the information stored in the
SourcererDB and the Ô¨Åle repository. The code index prepared for
evaluation consisted of all methods, classes and constructors from
the plugins that had source. This resulted in an index with 178,884
methods, 27,015 classes, and 16,498 constructors.
Candidate Queries: A set of 20 candidate queries was prepared
referring to various sources that teach using Eclipse APIs. Table
5 shows the list of the queries. Queries Q11 - Q18 are created re-
sembling tasks that are described in the Eclipse FAQ web site [4]
and the book ‚ÄúOfÔ¨Åcial Eclipse FAQs‚Äù [8]. Queries 9 and 10 are
based on tasks described in [17]; Queries 14, 15, 19, 20 are based
on tasks used to evaluate Strathcona, a recommender tool that sug-
gests API examples [26]; and, Queries 1, 2, 3 are created based on
examples found in sites [7, 1, 2]. The queries are formulated as a
list of natural language terms that describe the tasks to be solved.
As such, they represent possible queries a developer who is unfa-
miliar with the APIs but understands the task to be completed might
formulate. The italicized terms in the queries are stop words and
are not included in the query during retrieval. We show them here
to clarify the intent of the queries.
Ranked Results: We followed the pooling approach [15, 30]
to evaluate the retrieval schemes. In pooling approach, a set of
documents is collected in a pool by retrieving top ‚Äòk‚Äô results from
each retrieval scheme that is to be evaluated. There are two pri-
mary reasons for adopting this approach: (i) A large retrieval sys-
tem contains numerous documents, all of which is impossible to
check for relevancy, and (ii) in search engines, usually the top hits
are of importance. We set ‚Äòk‚Äô to be 10 in our evaluation consider-
ing the fact that most search engines present top 10 results in theirÔ¨Årst result page. The search tool in our retrieval system fetches
the top 10 results for a candidate query using all retrieval schemes.
It then extracts code snippets from the ranked results returned by
the retrieval schemes, and puts them in a pool. Finally, a list of
ranked snippets is produced for every candidate query using all Ô¨Åve
retrieval schemes.
Relevancy Judgement: The original sources used to create the
queries also contain solutions to solve the tasks that the queries rep-
resent. These solutions are code snippets along with the list of APIs
to be used. This list of suggested APIs and code snippets is used as
an oracle to refer to evaluate the snippets returned by the evaluation
tool. One of the authors went through all the query-snippet pairs in
the pool to mark them as either relevant or not relevant using the or-
acle based on the following two criteria: (i) its comments show the
right APIs as mentioned in the original solution, and (ii) the code
in the snippet resembles the pattern of API usage as shown in the
original solution. For example, Figure 2 is a code snippet returned
by a retrieval scheme, and was marked as relevant after consulting
the oracle that contained the original solution given in [17].
Performance Metrics: The lists of ranked results and judge-
ments are sufÔ¨Åcient to compute various measures of effectiveness
for the retrieval schemes. We look at 4 such measures: (i) Cover-
age, (ii) Precision, (iii) Recall, and (iv) Normalized Discounted Cu-
mulative Gain (NDCG). Coverage is deÔ¨Åned as the percentage of
total queries used in evaluation for which a retrieval scheme is able
to generate a relevant result. This measures how good a scheme
is across a range of queries. NDCG measures the ranking perfor-
mance of a retrieval scheme by measuring the cumulative useful-
ness of search results based on their position in the list of results.
Relevant documents appearing higher in the list get higher score
whereas relevant documents appearing lower are penalized. Ex-
act formula to compute NDCG and further details on the metric is
available in [30, 15].
Precision and recall in evaluation by pooling approach is com-
puted as follows.
P recision s=jRetrieved s\Relevant poolj
jRetrieved sj=jRelevant sj
jRetrieved sj
Recall s=jRetrieved s\Relevant poolj
jRelevant poolj=jRelevant sj
jRelevant poolj
where, Retrieved sis the set of results retrieved by a scheme ‚Äòs‚Äô
that exists in the pooled results, Relevant pool is the set of rel-
evant results in the pool, and Relevant sis the set of all relevant
results retrieved by a scheme ‚Äòs‚Äô in top ‚Äòk‚Äô results. The Ô¨Åve retrieval
schemes are compared for precision and recall for top 10 results.
Precision measures the ability of a retrieval scheme to Ô¨Ålter noise
in the hits that it retrieves. Recall can be used to see which retrieval
scheme dominates in being able to fetch more relevant results (re-
gardless of noise).
5.3 Results
Table 6 lists the aggregate measures of performance for all the
retrieval schemes. The computed values for coverage show that SSI
based schemes ( S1, S2, S3 ) have the highest coverage. Schemes S3
andS1are able to retrieve a relevant results for 85% of the total can-
didate queries, almost as twice as the baseline scheme B1which
retrieves relevant results for only 45% of the queries. B2does
slightly better than B1on coverage. This indicates that applying the
heuristic of ‚ÄúUsage‚Äù can increase the coverage of a retrieval scheme
(Coverage B2> Coverage B1), and using ‚ÄúUsage Similarity‚Äù in-
creases coverage even more ( Coverage S1;S2;S3> Coverage B2).
NDCG values in Table 6 indicate that adding the heuristic of ‚ÄúUs-
age‚Äù on top of the baseline ( B1) deteriorates the ranking. How-
ever, adding the ‚ÄúUsage Similarity‚Äù heuristic on top of ‚ÄúUsage‚Äù
163Figure 3: Precision and Recall
improves the ranking compared to the baseline scheme B1. This is
only true for the cases when either TF-IDF based or Tanimoto Co-
efÔ¨Åcient is used as the similarity metric for the heuristic of ‚ÄúUsage
Similarity‚Äù ( NDCG S3;S1> NDCG B1> NDCG B2). Using
the Hamming Distance as a measure of similarity seems to make
ranking worse compared to baseline ( B1). The different values of
coverage and NDCG for the three SSI based schemes ( S1, S2, S3 )
show that the choice of a similarity measure used in SSI can have
different effect on its coverage and NDCG measures.
Figure 3 plots the precision and recall values obtained for each
schemes. Each box plot shows the distribution of the computed
measure (precision/recall) for all queries and a particular scheme.
The thick horizontal line inside a boxplot corresponds to the me-
dian value of a measure; the circles scattered vertically represents
an individual measure for a query; a dark diamond shape marks the
mean of the measure, and the dark line drawn across the diamonds
show how the mean of the measure changes over schemes. The re-
call plot is obtained by plotting measures of recall for all queries.
Since precision is not deÔ¨Åned when a retrieval scheme fails to re-
trieve any result (i.e. Retrieved sis 0), the precision plot is ob-
tained based on the values for 12 queries that had non zero values
forRetrieved sin all schemes.
The plot for precision indicates that precision increases with the
inclusion of ‚ÄúUsage‚Äù heuristic ( Precision B2> Precision B1).
Including ‚ÄúUsage Similarity‚Äù heuristic seem to slightly decrease
the average precision achieved by using the ‚ÄúUsage Heuristic‚Äù alone
(average (Precision S3;S1)< average (Precision B2)). How-
ever, the median scores for precision seem to be increasing with
‚ÄúUsage Similarity‚Äù.
To summarize, improvement in recall due to SSI seems to be
more signiÔ¨Åcant than the corresponding deterioration in precision.
Statistical Tests: Evaluation of Precision, Recall and NDCG for
each query over the Ô¨Åve different schemes resembles a one-way
repeated measure experiment where precision and recall are mea-
sured repeatedly for each scheme using a same query. Shapiro-
Wilk normality test shows that the distributions of precision, recall
and NDCG values are non-normal for some schemes. Therefore
we choose Friedman test, a non-parametric one-way repeated mea-
sure analysis of variance, to test the statistical signiÔ¨Åcance of these
measures.
Friedman test on recall measures shows some evidence that sig-
niÔ¨Åcant difference exists in the measures of recall across different
retrieval schemes ( 2= 15:6495 , df=4, p-value=0.0035). Posthoc
analysis using the Wilcoxon-Nemenyi-McDonald-Thompson test
[25] shows signiÔ¨Åcant difference exists between recall values of
schemes S3 and B1 (p-value=0.0047) and schemes S1 and B1 (p-
value=0.0102). Friedman test on precision measures shows no evi-
dence that precision measures are signiÔ¨Åcantly different across schemes
(2= 2:4921 , df = 4, p-value = 0.646). Friedman test on NDCGMeasure
B1
B2
S1
S2
S3
 Coverage 45% 55% 85% 65% 85% 
NDCG
0.27
0.22
0.29
0.25
0.34
Table 6: Performance Measures for Various Schemes
values shows no signiÔ¨Åcant differences in NDCG values across the
schemes ( 2= 3:7295 , df = 4, p-value = 0.4439).
The above results indicate that the increments in recall across the
various schemes are signiÔ¨Åcant for two of the SSI based schemes:
S1 and S3, while the changes in precision and NDCG are not.
Therefore we can conclude that while there is a signiÔ¨Åcant improve-
ment in recall with SSI it does not have an adverse effect on preci-
sion and NDCG (ranking). For two choices of similarity measures,
SSI increases coverage, ranking, precision and recall compared to
the baselines. SSI performs best with Tanimoto CoefÔ¨Åcient as the
similarity measure.
5.4 Implicationss
The poor coverage of scheme B1(Table 6) shows that additional
heuristics are necessary to retrieve relevant API usage examples
across a broad range of queries. A non-signiÔ¨Åcant decrease in pre-
cision but a signiÔ¨Åcant increase in recall could mean that the use
of SSI (or, adding both the heuristics of ‚ÄúUsage‚Äù and ‚ÄúUsage Sim-
ilarity‚Äù) might overall be beneÔ¨Åcial across a wide range of queries.
This is especially important when a more precise retrieval scheme
fails to retrieve any relevant result for most of the queries. B1has a
low coverage because it fails to retrieve any result at all for most of
the queries. Since the SSI based schemes also seem to improve
ranking, irrelevant results introduced by adding these heuristics
would mostly appear lower in the hits. This knowledge about the
performance behavior of the various retrieval schemes can also be
used to devise adaptive retrieval schemes, when a stricter but more
precise scheme could be used at Ô¨Årst, and if retrieval fails a less
restrictive but slightly imprecise scheme could be used.
6. VALIDITY
Certain assumptions and choices made in the evaluation could
affect the validity of the work presented here.
Corpus: The repository used in this paper is relatively small,
since it consists of entities from a single big project. For the ‚ÄúUs-
age‚Äù and ‚ÄúUsage Similarity‚Äù heuristics to work well, it is assumed
that the underlying software repository is sufÔ¨Åciently large as to
contain many instances of usage of APIs. We plan to rerun our
evaluation on the larger Sourcerer repository to investigate how size
impacts the heuristics‚Äô performance.
Types of Entities Retrieved: During evaluation, we limited the
retrieval to method and constructor entities. When the results con-
tained class entities, we observed two things. First, very few of the
top entities were classes. Second, when class entities were found
in the results, they often produced duplicate snippets, as methods
within the class were also included in the results. This created con-
fusion regarding whether the duplicate snippets should be consid-
ered the same result, as they come from the same method, or two
different results. We found it easier and of less impact to the eval-
uation to limit the retrieval to method and constructor entities only.
Although this simpliÔ¨Åed our evaluation, we believe properly hand-
ing containment and hierarchic structure in code retrieval seems to
be an important topic that needs more study.
Queries: Another major factor that inÔ¨Çuences the evaluation re-
sults is the choice of queries and terms used for the queries. We cre-
ated the candidate queries based on tasks related to Eclipse APIs.
These tasks were taken from various standard resources on the Web
and from published materials. We therefore believe they represent
164a wide range of tasks from which representative candidate queries
can be created. However, we deliberately chose the queries to con-
tain only natural terms, omitting names of APIs in the queries, in
order to capture the scenario of developers writing queries about
unfamiliar APIs. Our queries are relatively long, with about 4 av-
erage terms per query (ignoring the stop words in italics). Experi-
enced developers who are somewhat familiar with the APIs might
mix API names with natural terms, or write shorter and more pre-
cise queries. The queries for a same task can also vary among dif-
ferent individuals. Therefore, our results could have been biased
by the choice of words we used, or by our decision to give them a
more natural form. One way to mitigate this problem could be to
obtain real queries from developers (to accommodate variations in
query terms and forms), and use them in evaluation. We are con-
sidering to take this approach when we rerun our evaluation in a
bigger repository.
Relevancy judgement: When judging the relevancy of a snippet,
we had to make some decisions regarding what to classify as rele-
vant. If we conÔ¨Årmed that a snippet used valid APIs not found in
the oracle solution, we still marked it as relevant. However, if a
snippet used an API marked as internal, we marked it as not rele-
vant. Changing these criteria might have an effect on the results.
Other parameters: There are many variables that can alter the
relevancy judgements. For example, boost values for the Ô¨Åelds, the
number of search results processed to get the list of mostly used en-
tities, the types of entities chosen to generate the snippets, the types
of relations considered while computing usage proÔ¨Åle for similar-
ity, etc. We Ô¨Åxed most of these parameters to be same in all re-
trieval schemes after some trial and error with few queries. As
future work, we plan to investigate the inÔ¨Çuence of these values on
the sensitivity of the results.
Comparing similarity measures: A thorough investigation of var-
ious similarity measures to be used for the ‚ÄúUsage Similarity‚Äù heuris-
tic is out of scope of the work presented here. The similarity mea-
sure based on modiÔ¨Åed hamming distance has a different effect on
retrieval performance compared to the other two. More analysis is
required to understand why such a difference exists.
All of the necessary resources to repeat our experiments are avail-
able online. The materials used in this work include: the candidate
queries, the oracle with solutions, and the evaluation results. They
are available at [6]. The entire implementation starting from the
Sourcerer infrastructure to the search tool with snippet extraction
has been open sourced and is available for others to use or extend.
Further details about obtaining these resources are available at [6].
7. RELATED WORK
The work presented in this paper is an extension of our exist-
ing work in evaluating ranking schemes for Internet-scale code
search [27]. The Ô¨Årst three heuristics are borrowed from that work.
As mentioned earlier, the heuristics ‚ÄúUsage‚Äù represents recent ap-
proaches of using API calls relations in building software retrieval
systems [13, 21, 20]. The similarity measures used in the ‚ÄúUsage
Similarity‚Äù heuristic is similar to the methods often used in rec-
ommendation systems for software [31, 12]. XFinder is another
tool that uses a combination of structural similarity and existing
documentation to help developers Ô¨Ånd examples of framework ex-
tensions [16]. Like most of these work we leverage structural simi-
larity, but with the goal of attacking the problem of location in code
retrieval. To the best of our knowledge, our is the Ô¨Årst work to use
usage similarity for this purpose.
Existing approaches to tackle the vocabulary problem in code
search either use complex information retrieval models such as La-
tent Semantic Indexing (LSI) or instead rely on information sourceson the web that contain textually richer descriptions of code snip-
pets. CodeBroker [49, 50] is an example of the Ô¨Årst kind. Mica
[40] and Assieme [24] are two recent tools that follow the second
approach of relying on information on the web describing code
snippets and APIs used those snippets. Unlike these approaches,
SSI does not rely on existing documentation and works solely with
source code.
Extracting code snippets based on popular APIs in the search re-
sults bear some resemblance with the snippet generation techniques
used in Sniff [13]. While Sniff‚Äôs goal is to synthesize a single code
snippet from fragments extracted from many entities, our approach
is to be able to locate useful API usage in the original Ô¨Åles. Since,
‚ÄúUsage‚Äù heuristic essentially capture the approach that Sniff uses to
retrieve code entities, our result that ‚ÄúUsage Similarity‚Äù improves
code retrieval imply that tools such as Sniff can beneÔ¨Åt by adopting
our approach.
Our use of popular APIs in generating the code snippets is sim-
ilar to the use of popularity information about APIs to generate
documentation in Jadeite [41], and the work of using API hotspots
to detect interesting parts of frameworks [43]. A similar technique
to list relevant APIs and code snippet exist in the code search tool
MICA [40]. Commenting code snippets with used APIs is mo-
tivated by the ‚ÄúRationale‚Äù view in Strathcona that shows similar
information about the code example it returns [26].
There are a number of tools that assist developers with prob-
lems other than location in working with API examples. For ex-
ample, various tools aid comprehension; the step where develop-
ers need to understand protocols and right patterns of using APIs
[29, 26, 37, 42, 28]. A basic requirement of these tools is that they
require an initial seed in the form of names of APIs to retrieve use-
ful examples. Thus, they are of less help to developers who are
unfamiliar with APIs or those who do not remember them. Lo-
cating APIs being the very Ô¨Årst step, our solution to the location
problem is largely complementary to the work behind these tools.
Furthermore, most of these other tools also work solely with the
information available in the source code. Combining our approach
in code retrieval with these other tools will give a better end-to-
end solution for working with APIs that is entirely driven by the
information that can be extracted from the source code.
Recent commercial work is bringing the capabilities of web search
engines into code search. Koders [46], Krugle [47], and Google
Code Search [45] are the few prominent ones. Since the underly-
ing repository of these code search engines can be extremely large
(billions of lines of code), the retrieval schemes for these systems
need to be scalable and automatic. Older approaches to improve
retrieval in code repositories do not meet this requirement. For
example, CodeFinder [22] one of the earlier code retrieval sys-
tem, pioneered the idea of adaptive indexing. However, the reposi-
tory construction process is semi-manual requiring human input to
create a good index, and the matching scheme is computationally
expensive that slows down exponentially as the repository grows
[23]. Our approach in this paper is simple and scalable. It uses a
semi-structure full-text index based on the vector space model, all
computations to build the repository can be done ofÔ¨Çine and is par-
allelizable. We implemented the core retrieval engine using Lucene
which actually drives most of the commercial code search engines
mentioned earlier. This, makes our approach highly suitable to be
adapted for commercial code search engines.
There are alternative approaches that try to fulÔ¨Åll the developers‚Äô
information need about APIs. Most prominent ones are those that
rely on active participation and social interaction among develop-
ers [41, 51]. These approaches are promising and can be successful
with enough participation and interest among developers to use the
165tools and approaches they propose. We provide an alternative solu-
tion that is more lightweight, and harvests information about API
usage from the primary source (code that use APIs) where most of
the information about real API usage exists.
8. CONCLUSION
In this paper we presented Structural Semantic Indexing (SSI),
a novel approach to tackle the problem of source code retrieval
by leveraging API usage information extracted from code in large
repositories. By sharing terms between entities with similar usage
proÔ¨Åles, SSI partially ameliorates the vocabulary mismatch prob-
lem that plagues source code search. Our goal with SSI was to be
able to build an effective code retrieval scheme that uses no docu-
ments other than source code. Large source code repositories such
as Sourceforge and Github serve both as motivation and target of
our approach: Can we use them, and them alone, to Ô¨Ånd the code
we need? Our evaluation demonstrates that SSI is indeed a feasible
and effective approach. Furthermore, we presented a technique for
dynamically generating API usage snippets which has the potential
to zoom in on exactly the information that the developers want: a
variety of concrete examples. We also discussed some open ques-
tions (Section 6) that warrants further exploration.
References
[1]StackoverÔ¨Çow Web Site. http://stackoverÔ¨Çow.com .
[2]Java2s Web Site. http://java2s.com/ .
[3]Apache lucene - scoring web page http://lucene.apache.org/java/2_4_0/scoring.html ,
Mar 2010.
[4]Eclipse faqs web site http://wiki.eclipse.org/index.php/Eclipse_FAQs , Jan 2010.
[5]Lucene web site. http://lucene.apache.org , Jan 2010.
[6]Sourcerer wiki page on api location http://wiki.github.com/sourcerer/Sourcerer/locating-
apis, Jan 2010.
[7]Swt snippets example web site http://www.eclipse.org/swt/snippets/ , Jan. 2010.
[8]J. Arthorne and C. Laffra. OfÔ¨Åcial Eclipse 3.0 FAQs . Addison-Wesley Profes-
sional, July 2004.
[9]S. Bajracharya, T. Ngo, E. Linstead, Y . Dou, P. Rigor, P. Baldi, and C. Lopes.
Sourcerer: a search engine for open source code supporting structure-based
search. pages 681‚Äì682, New York, NY , USA, 2006. ACM Press.
[10] S. Bajracharya, J. Ossher, and C. Lopes. Sourcerer: An internet-scale software
repository. In First Intl. Workshop on Search Driven Development - Users, In-
frastructure, Tools and Evaluation . ICSE 2009, (under review), 2009.
[11] M. Balabanovi ¬¥c and Y . Shoham. Fab: content-based, collaborative recommen-
dation. Commun. ACM , 40(3):66‚Äì72, 1997.
[12] M. Bruch, M. Monperrus, and M. Mezini. Learning from examples to improve
code completion systems. In Proceegings of FSE , pages 213‚Äì222, Amsterdam,
The Netherlands, 2009. ACM.
[13] S. Chatterjee, S. Juvekar, and K. Sen. SNIFF: A Search Engine for Java Using
Free-Form Queries. In Fundamental Approaches to Software Engineering , pages
385‚Äì400. 2009.
[14] C. W. Cleverdon. Factors determining the performance of indexing systems .
1966.
[15] B. Croft, D. Metzler, and T. Strohman. Search Engines: Information Retrieval
in Practice . Addison Wesley, 1 edition, Feb. 2009.
[16] B. Dagenais and H. Ossher. Automatically locating framework extension exam-
ples. In Proceedings of the 16th ACM SIGSOFT International Symposium on
Foundations of software engineering , pages 203‚Äì213, Atlanta, Georgia, 2008.
ACM.
[17] J. D‚ÄôAnjou, S. Fairbrother, D. Kehn, J. Kellerman, and P. McCarthy. The Java
Developer‚Äôs Guide to Eclipse, 2nd Edition . Addison-Wesley Professional, 2 edi-
tion, Nov. 2004.
[18] G. Fischer, S. Henninger, and D. Redmiles. Cognitive tools for locating and
comprehending software objects for reuse. In Proceedings of the 13th inter-
national conference on Software engineering , pages 318‚Äì328, Austin, Texas,
United States, 1991. IEEE Computer Society Press.
[19] G. W. Furnas, T. K. Landauer, L. M. Gomez, and S. T. Dumais. The vocabulary
problem in human-system communication. Commun. ACM , 30:964‚Äì971, 1987.
[20] M. Grechanik, K. M. Conroy, and K. A. Probst. Finding Relevant Applications
for Prototyping. In Proceedings of the Fourth International Workshop on Mining
Software Repositories , page 12. IEEE Computer Society, 2007.
[21] M. Grechanik and D. Poshyvanyk. Evaluating recommended applications. In
Proceedings of the 2008 international workshop on Recommendation systems
for software engineering , pages 33‚Äì35, Atlanta, Georgia, 2008. ACM.
[22] S. Henninger. An evolutionary approach to constructing effective software reuse
repositories. ACM Trans. Softw. Eng. Methodol. , 6(2):111‚Äì140, 1997.[23] S. R. Henninger. Locating relevant examples for example-based software design .
PhD thesis, University of Colorado at Boulder, 1993.
[24] R. Hoffmann, J. Fogarty, and D. S. Weld. Assieme: Ô¨Ånding and leveraging
implicit references in a web search interface for programmers. In Proceedings
of the 20th annual ACM symposium on User interface software and technology ,
pages 13‚Äì22, Newport, Rhode Island, USA, 2007. ACM.
[25] M. Hollander and D. A. Wolfe. Nonparametric Statistical Methods, 2nd Edition .
Wiley-Interscience, 2 edition, Jan. 1999.
[26] R. Holmes and G. C. Murphy. Using structural context to recommend source
code examples. In ICSE ‚Äô05: Proceedings of the 27th international conference
on Software engineering , pages 117‚Äì125, New York, NY , USA, 2005. ACM
Press.
[27] E. Linstead, S. Bajracharya, T. Ngo, P. Rigor, C. Lopes, and P. Baldi. Sourcerer:
mining and searching internet-scale software repositories. Data Mining and
Knowledge Discovery , 18(2):300‚Äì336, Apr. 2009.
[28] G. Little and R. C. Miller. Keyword programming in java. Automated Software
Engg. , 16(1):37‚Äì71, 2009.
[29] D. Mandelin, L. Xu, R. Bod√≠k, and D. Kimelman. Jungloid mining: helping to
navigate the api jungle. In PLDI ‚Äô05 , pages 48‚Äì61, New York, NY , USA, 2005.
ACM Press.
[30] C. D. Manning, P. Raghavan, and H. Sch√ºtze. Introduction to Information Re-
trieval . Cambridge University Press, 1 edition, July 2008.
[31] F. McCarey, M. O. Cinneide, and N. Kushmerick. A recommender agent for soft-
ware libraries: An evaluation of memory-based and model-based collaborative
Ô¨Åltering. pages 154‚Äì162. IEEE Computer Society, 2006.
[32] J. Nykaza, R. Messinger, F. Boehme, C. L. Norman, M. Mace, and M. Gordon.
What programmers really want: results of a needs assessment for SDK documen-
tation. In Proceedings of the 20th annual international conference on Computer
documentation , pages 133‚Äì141, Toronto, Ontario, Canada, 2002. ACM.
[33] J. Ossher, S. Bajracharya, and C. Lopes. SourcererDB: An aggregated repository
of statically analyzed and cross-linked open source java projects. In MSR 2009:
6th IEEE Working Conference on Mining Software Repositories , 2009.
[34] D. F. Redmiles. Reducing the variability of programmers‚Äô performance through
explained examples. In Proceedings of the INTERACT ‚Äô93 and CHI ‚Äô93 con-
ference on Human factors in computing systems , pages 67‚Äì73, Amsterdam, The
Netherlands, 1993. ACM.
[35] M. P. Robillard. What Makes APIs Hard to Learn? Answers from Developers.
IEEE Softw. , 26(6):27‚Äì34, 2009.
[36] M. B. Rosson and J. M. Carroll. The reuse of uses in smalltalk programming.
ACM Trans. Comput.-Hum. Interact. , 3(3):219‚Äì253, 1996.
[37] N. Sahavechaphan and K. Claypool. Xsnippet: mining for sample code. In
OOPSLA ‚Äô06: Proceedings of the 21st annual ACM SIGPLAN conference on
Object-oriented programming systems, languages, and applications , pages 413‚Äì
430, New York, NY , USA, 2006. ACM Press.
[38] G. Salton. The state of retrieval system evaluation. Inf. Process. Manage. ,
28(4):441‚Äì449, 1992.
[39] F. Shull, F. Lanubile, and V . R. Basili. Investigating Reading Techniques for
Object-Oriented Framework Learning. IEEE Trans. Softw. Eng. , 26(11):1101‚Äì
1118, 2000.
[40] J. Stylos and B. A. Myers. Mica: A Web-Search tool for Ô¨Ånding API compo-
nents and examples. In Proceedings of the Visual Languages and Human-Centric
Computing , pages 195‚Äì202. IEEE Computer Society, 2006.
[41] J. Stylos, B. A. Myers, and Z. Yang. Jadeite: improving API documentation
using usage information. In Proceedings of the 27th international conference
extended abstracts on Human factors in computing systems , pages 4429‚Äì4434,
Boston, MA, USA, 2009. ACM.
[42] S. Thummalapenta and T. Xie. Parseweb: a programmer assistant for reusing
open source code on the web. In Proceedings of the twenty-second IEEE/ACM
international conference on Automated software engineering , pages 204‚Äì213,
Atlanta, Georgia, USA, 2007. ACM.
[43] S. Thummalapenta and T. Xie. SpotWeb: detecting framework hotspots and
coldspots via mining open source code on the web. In Automated Software Engi-
neering, 2008. ASE 2008. 23rd IEEE/ACM International Conference on , pages
327‚Äì336, 2008.
[44] M. Umarji, S. Sim, and C. Lopes. Archetypal Internet-Scale source code search-
ing. In Open Source Development, Communities and Quality , volume 275/2008
ofIFIP International Federation for Information Processing , pages 257‚Äì263.
Springer Boston, 2008.
[45] Web site for Google Code Search. http://www.google.com/codesearch , 2010.
[46] Web site for Koders. http://www.koders.com , 2010.
[47] Web site for Krugle. http://www.krugle.com , 2010.
[48] P. Willett, J. M. Barnard, and G. M. Downs. Chemical Similarity Searching.
Journal of Chemical Information and Computer Sciences , 38(6):983‚Äì996, Nov.
1998.
[49] Y . Ye and G. Fischer. Reuse-conducive development environments. Automated
Software Engg. , 12:199‚Äì235, 2005.
[50] Y . Ye, G. Fischer, and B. Reeves. Integrating active information delivery and
reuse repository systems. pages 60‚Äì68, New York, NY , USA, 2000. ACM Press.
[51] Y . Ye, Y . Yamamoto, K. Nakakoji, Y. Nishinaka, and M. Asada. Searching the li-
brary and asking the peers: learning to use java APIs on demand. In Proceedings
of the 5th international symposium on Principles and practice of programming
in Java , pages 41‚Äì50, Lisboa, Portugal, 2007. ACM.
166