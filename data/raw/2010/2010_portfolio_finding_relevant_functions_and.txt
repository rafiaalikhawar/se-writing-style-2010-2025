Portfolio: Finding Relevant Functions and Their Usages
Collin McMillan
College of William
& Mary
Williamsburg, VA 23185
cmc@cs.wm.eduMark Grechanik
Accenture Technology Lab
Chicago, IL 60601
mark.grechanik@
accenture.comDenys Poshyvanyk
College of William
& Mary
Williamsburg, VA 23185
denys@cs.wm.eduQing Xie, Chen Fu
Accenture Technology Lab
Chicago, IL 60601
{qing.xie,chen.fu}
@accenture.com
ABSTRACT
Different studies show that programmers are more intereste d in
ﬁnding deﬁnitions of functions and their uses than variable s, state-
ments, or arbitrary code fragments [30, 29, 31]. Therefore, pro-
grammers require support in ﬁnding relevant functions and d eter-
mining how those functions are used. Unfortunately, existi ng code
search engines do not provide enough of this support to devel opers,
thus reducing the effectiveness of code reuse.
We provide this support to programmers in a code search sys-
tem called Portfolio that retrieves and visualizes relevant functions
and their usages. We have built Portfolio using a combinatio n of
models that address surﬁng behavior of programmer and shari ng
related concepts among functions. We conducted an experime nt
with 49 professional programmers to compare Portfolio to Go ogle
Code Search and Koders using a standard methodology. The re-
sults show with strong statistical signiﬁcance that users ﬁ nd more
relevant functions with higher precision with Portfolio th an with
Google Code Search and Koders.
Categories and Subject Descriptors
D.2.9 [ Software Engineering, Management ]: Productivity; D.2.m
[Software Engineering, Miscellaneous ]: Reusable software
General Terms
Algorithms, Experimentation
Keywords
Code search, portfolio, pagerank, function call graph, ran king.
1. INTRODUCTION
Different studies show that programmers are more intereste d in
ﬁnding deﬁnitions of functions and their uses than variable s, state-
ments, or arbitrary fragments of source code [31]. More spec if-
ically, programmers use different tools including code sea rch en-
gines to answer three types of questions [30, 29]. First, pro gram-
mers want to ﬁnd initial focus points such as relevant functi ons that
Permission to make digital or hard copies of all or part of thi s work for
personal or classroom use is granted without fee provided th at copies are
not made or distributed for proﬁt or commercial advantage an d that copies
bear this notice and the full citation on the ﬁrst page. To cop y otherwise, to
republish, to post on servers or to redistribute to lists, re quires prior speciﬁc
permission and/or a fee.
ICSE ’11, May 21–28, 2011, Honolulu, Hawaii, USA
Copyright 2011 ACM 978-1-4503-0445-0/11/05 ...$10.00.implement high-level requirements. Second, programmers m ust
understand how a function is used in order to use it themselve s.
Third, programmers must see the chain of function invocatio ns in
order to understand how concepts are implemented in these fu nc-
tions. It is important that source code search engines suppo rt pro-
grammers in ﬁnding answers to these questions.
In general, understanding code and determining how to use it , is
a manual and laborious process that takes anywhere from 50% t o
80% of programmers’ time [5, 8]. Short code fragments that ar e
returned as results to user queries do not give enough backgr ound
or context to help programmers determine how to reuse these c ode
fragments, and programmers typically invest a signiﬁcant i ntellec-
tual effort (i.e., they need to overcome a high cognitive dis tance
[17]) to understand how to reuse these code fragments. On the
other hand, if code fragments are retrieved as functions, it makes it
easier for developers to understand how to reuse these funct ions.
A majority of code search engines treat code as plain text whe re
all words have unknown semantics. However, applications co n-
tain functional abstractions that provide a basic level of c ode reuse,
since programmer deﬁne functions once and call them from dif fer-
ent places in source code. The idea of using functional abstr actions
to improve code search was proposed and implemented elsewhe re
[3, 10, 23, 32]; however, these code search engines do not aut o-
matically analyze how functions are used in the context of ot her
functions, despite the fact that understanding the chains o f function
invocations is a key question that programmers ask. Unfortu nately,
existing code search engines do little to ensure that they re trieve
code fragments in a broader context of relevant functions th at in-
voke one another to accomplish certain tasks.
Our idea is that since programmers frequently ask various qu es-
tions about functions, a code search engine should incorpor ate in-
formation about these functions that is used to answer the pr ogram-
mers’ questions. Browsing retrieved functions that are rel evant to
queries means that programmers follow function calls and re view
declarations, deﬁnitions, and uses of these functions to co mbine
them in a solution to a given task. That is, programmers want t o
accomplish the whole task quickly, rather than obtain multi ple ex-
amples for different components of the task.
For example, consider the query “ mip map dithering
texture image graphics ,” which we use as an example
query throughout this paper. Programmers don’t want to just see
examples that implement mip map techniques, and others that ren-
der texture, and others that manipulate graphic images. A pr ogram-
mer wants to accomplish the complete task of dithering mip ma p
images that accompany a texture. However, among relevant re -
sults there are functions that implement mipmapping, funct ions that
manipulate texture, and there are multiple functions that d eal with
graphic images. Typically, programmers investigate these functionsto determine which of them are relevant and determine how to c om-
pose these functions to achieve the goal that is expressed wi th the
query. That is, a programmer wants to see code for the whole ta sk
of how to mip map images that accompany a texture in computer
graphics. A search engine can support programmers efﬁcient ly if
it incorporates in its ranking how these functions call one a nother,
and displays that information to the user.
We created a code search system called Portfolio that supports
programmers in ﬁnding relevant functions that implement hi gh-
level requirements reﬂected in query terms (i.e., ﬁnding in itial fo-
cus points), determining how these functions are used in a wa y
that is highly relevant to the query (i.e., building on found focus
points), and visualizing dependencies of the retrieved fun ctions to
show their usages. Portfolio ﬁnds highly relevant function s in close
to 270 Millions LOC in projects from FreeBSD Ports1by combin-
ing various natural language processing (NLP) and indexing tech-
niques with PageRank andspreading activation network (SAN) al-
gorithms. With NLP and indexing techniques, initial focus p oints
are found that match key words from queries; with PageRank, w e
model the surﬁng behavior of programmers, and with SAN we el-
evate highly relevant chains of function calls to the top of s earch
results. We have built Portfolio and conducted an experimen t with
49 professional C++ programmers to evaluate Portfolio and c om-
pare it with the well-known and successful engines Google Co de
Search and Koders. The results show with strong statistical sig-
niﬁcance that users ﬁnd more relevant code with higher preci sion
with Portfolio than those with Google Code Search and Koders .
To the best of our knowledge, we are not aware of any existing
code search engines that have been evaluated against and sho wn
to be more accurate than widely used commercial code search e n-
gines, with strong statistical signiﬁcance and over a large codebase
and using a standard information retrieval methodology [22 , pages
151-153]. Portfolio is free and available for public use2.
2. THE MODEL
The search model of Portfolio uses a key abstraction in which
the search space is represented as a directed graph with node s as
functions and directed edges between nodes that specify usa ges of
these functions (i.e., a call graph ). For example, if the function gis
invoked in the function f, then a directed edge exists from the node
that represents the function fto the node that represents the func-
tiong. Since the main goal of Portfolio is to enable programmers
to ﬁnd relevant functions and their usages, we need models th at ef-
fectively represent the behavior of programmers when navig ating a
large graph of functional dependencies. These are navigati on and
association models that address surﬁng behavior of program mers
and associations of terms in functions in the search graph.
2.1 Navigation Model
When using text search engines, users navigate among pages b y
following links contained in those pages. Similarly, in Por tfolio,
programmers can navigate between functions by following ed ges
in the directed graph of functional dependencies using Port folio’s
visual interface. To model the navigation behavior of progr ammers,
we adopt the model of the random surfer that is used in popular
search engines such as Google. Following functional depend encies
helps programmers to understand how to use found functions. The
surfer model is called random because the surfer can “jump” t o a
new URL, or in case of source code, to a new function. These
random jumps are called teleportations , and this navigation model
is the basis for the popular ranking algorithm PageRank [2, 1 9].
1http://www.freebsd.org/ports
2http://www.searchportfolio.net
Figure 1: Example of associations between different functi ons.
In the random surfer model, the content of functions and quer ies
does not matter, navigations are guided only by edges in the g raph
that speciﬁes functional dependencies. Accordingly, Page Rank re-
ﬂects only the surﬁng behavior of users, and this rank is base d on
the popularity of a function that is determined by how many fu nc-
tions call it. However, the surﬁng model is query independen t since
it ignores terms that are used in search queries. Taking into con-
sideration query terms may improve the precision of code sea rch-
ing. That is, if different functions share concepts that are related to
query terms and these functions are connected using functio nal de-
pendencies, then these functions should be ranked higher. We need
a search model that should automatically make embedded conc epts
explicit by using associations between functions that shar e related
concepts, and then we combine this model with the surﬁng mode l
in Portfolio.
2.2 Association Model
The main idea of an association model is to establish relevan ce
among facts whose content does not contain terms that match u ser
queries directly. Consider the query “ mipmap dithering
texture image graphics .” Among relevant results there
are functions that implement mip map techniques, and others that
render texture, and there are multiple functions that manip ulate
graphic images. This situation is schematically shown in Fi gure 1,
where the function Fcontains the term mip map , the function G
contains the term dithering , the function Pcontains the terms
graphics andimage , and the term texture is contained in
the function Q. FunctionFcalls the function G, which in turn calls
the function H, which is also called from the function Q, which is in
turn called from the function P. The functions F,P, andQwill be
returned by a search engine that is based on matching query te rms
to those that are contained in documents. Meanwhile, the fun c-
tionHmay be highly relevant to the query but it is not retrieved
since it has no words that match the search terms. In addition , the
functionGcan be called from many other functions since its dither-
ing functionality is generic; however, its usage is most val uable for
programmers in the context of the function that is related to query
terms. A problem is how to ensure that the functions HandGend
up on the list of highly relevant functions.
To remedy this situation we use an association model that is
based on a Spreading Activation Network (SAN) [4, 6]. In SANs,
nodes represent documents, while edges specify properties that
connect these documents. The edges’ direction and weight re ﬂect
the meaning and strength of associations among documents. F or
example, an article about clean energy and a different artic le about
the melting polar ice cap are connected with an edge that is la beled
with the common property “climate change.” Once applied to S AN,
spreading activation computes new weights for nodes (i.e., ranks)
that reﬂect implicit associations in the networks of these n odes.
In Portfolio, we view function call graphs as SANs where node s
represent functions, edges represent functional dependen cies, and
weights represent a strength of associations , which includes the
number of shared terms. After the user enters a query, a list o f
functions is retrieved and sorted based on the score that reﬂ ects theFigure 2: Portfolio architecture.
match between query terms and terms in functions. Once Portf olio
identiﬁes top matching functions, it computes SAN to propag ate
concepts from these functions to others. The result is that e very
function will have a new score that reﬂects the associations between
concepts in these functions and user queries.
2.3 The Combined Model
The ranking vectors for PageRank /bardblπ/bardblPRand spreading activa-
tion/bardblπ/bardblSANare computed separately and later are linearly com-
bined in a single ranking vector /bardblπ/bardblC=f(/bardblπ/bardblPR,/bardblπ/bardblSAN). PageR-
ank is query independent and is precomputed automatically f or a
function call graph, while /bardblπ/bardblSANis computed automatically in re-
sponse to user queries. Assigning different weights in the l inear
combination of these rankings enables ﬁne-tuning of Portfo lio by
specifying how each model contributes to the resulting scor e.
3. OUR APPROACH
In this section we describe the architecture of Portfolio an d show
how to use Portfolio.
3.1 Portfolio Architecture
The architecture for Portfolio is shown in Figure 2. The main
elements of the Portfolio architecture are the database hol ding soft-
ware applications (i.e., the Projects Archive), the Metada ta Builder,
the Function Graph Builder, the SAN and PageRank algorithms ,
the Visualizer and the key word search engine. Applications meta-
data describes functions that are declared, deﬁned and invo ked in
the applications and words that are contained in the source c ode
of these functions and comments. Portfolio is built on an int ernal,
extensible database of 18,203 C/C++ projects that contain c lose to
2.3Mil ﬁles with close to 8.6Mil functions that contain 2,49 6,172
indexed words. Portfolio indexes and searches close to 270M il
LOC in these C/C++ projects that are extracted from FreeBSD’ s
source code repository called ports3. It is easy to extend Portfolio
by adding new projects to the Projects Archive. The user inpu t to
Portfolio is shown in Figure 2 with the arrow labeled (7). The
output is shown with the arrow labeled (18) .
Portfolio works as follows. The input to the system is the set of
applications from the Projects Archive that contain variou s func-
tions(1). The Function Graph Builder analyzes the source code
of these applications statically and it outputs (2) thefunction call
graph (FCG) that contains functional dependencies. This opera-
tion is imprecise since resolving dynamic dispatch calls an d func-
tion pointers statically is an undecidable problem [18]. Si nce this
3http://www.freebsd.org/ports - last checked August 17,20 10.is done ofﬂine, precise program analysis can be accommodate d in
this framework to achieve better results in obtaining corre ct func-
tional dependencies. We conduct the sensitivity analysis o f Portfo-
lio and its constituent algorithms in Section 5.7.1. Next, t he algo-
rithm PageRank is run (3) on the FCG, and it computes (4) the
rank vector, /bardblπ/bardblPR, in which every element is a ranking score for
each function in the FCG.
The Metadata Builder reads in (5) the source code of applica-
tions, applies NLP techniques such as stemming and identiﬁe r split-
ting, and indexes the source code as text resulting (6) in Projects
Metadata. When the user enters a query (7), it is passed to the
key word search component along with the Projects Metadata (8).
The key word search engine searches the metadata using the wo rds
in the query as keys and outputs (9) the set of Relevant Functions
whose source code and comments contain words that match the
words from the query. These relevant functions (10) along with
the FCG(11) serve as an input to the algorithm SAN. The algo-
rithm SAN computes (12) spreading activation vector of scores
/bardblπ/bardblSAN for functions that are associated with the relevant func-
tions(10) . Ranking vectors /bardblπ/bardblPR(14) and/bardblπ/bardblSAN(13) are
combined into the resulting vector /bardblπ/bardbl(15) that contains ranking
scores for all relevant functions. The Visualizer takes (16) the list
of relevant functions that are sorted in descending order us ing their
ranking scores and (17) the metadata, in order to present (18)
the resulting visual map to the user as it is shown in Figure 3.
3.2 Portfolio Visual Interface
After the user submits a search query, the Portfolio search e n-
gine presents functions relevant to the query in a browser wi ndow
as it is shown in Figure 3. The left side contains the ranked li st of
retrieved functions and project names, while the right side contains
a static call graph that contains these and other functions. Edges
of this graph indicate the directions of function invocatio ns. Hov-
ering a cursor over a function on the list shows a label over th e
corresponding function on the call graph. Font sizes reﬂect the
combined ranking; the higher the ranking of the function, th e big-
ger the font size used to show it on the graph. Clicking on the l abel
of a function loads its source code in a separate browser wind ow.
4. RANKING
In this section we discuss our ranking algorithm.
4.1 Components of Ranking
There are three components that compute different scores in the
Portfolio ranking mechanism: a component that computes a sc ore
based on word occurrences (WOS), a component that computes a
score based on the random surfer navigation model (PageRank )
described in Section 2.1, and a component that computes a sco re
based on SAN connections between these calls based on the ass o-
ciation model described in Section 2.2. WOS ranking is used t o
bootstrap SAN by providing rankings to functions based on qu ery
terms. The total ranking score is the weighted sum of the Page R-
ank and SAN ranking scores. Each component produces results
from different perspectives (i.e., word matches, navigati on, associ-
ations). Our goal is to produce a uniﬁed ranking by putting th ese
orthogonal, yet complementary rankings together in a singl e score.
4.2 WOS Ranking
The purpose of WOS is to enable Portfolio to retrieve functio ns
based on matches between words in queries and words in the sou rce
code of applications. This is a bootstraping ranking proced ure that
serves as the input to the SAN algorithm.Figure 3: A visual interface of Portfolio. The left side contains a list of ranked retrieved functions f or the motivating example query and the
right side contains a call graph that contains these functio ns; edges of this graph indicate the directions of function i nvocations. Hovering a cursor
over a function on the list shows a label over the correspondi ng function on the call graph. Font sizes reﬂect the score; th e higher the score of the
function, the bigger the font size used to show it on the graph . Clicking on the label of a function loads its source code in a separate browser window.
The WOS component uses the Vector Space Model (VSM), which
is a ranking function typically used by search engines to ran k match-
ing documents according to their relevance to a given search query.
This function is implemented in the Lucene Java Framework wh ich
is used in Portfolio. VSM is a standard bag-of-words retriev al func-
tion that ranks a set of documents based on the relative proxi mity
of query terms (e.g., without dependencies) appearing in ea ch doc-
ument. Each document is modeled as a vector of terms containe d
in that document. The weights of those terms in each document are
calculated using the Term Frequency/Inverse Document Frequency
(TF/IDF) formula. Using TF/IDF, the weight for a term is calc u-
lated as t f=n
∑knkwhere nis the number of occurrences of the term
in the document, and ∑knkis the sum of the number of occurences
of the term in all documents. Then the similarities among the docu-
ments are calculated using the cosine distance between each pair of
documents cos(θ) =d1·d2
/bardbld1/bardbl/bardbld2/bardblwhere d1andd2are document vec-
tors.
4.3 PageRank
PageRank is widely described in literature, so here we give i ts
concise mathematical explanation as it is related to Portfo lio [2,
19]. The original formula for PageRank of a function Fi, denoted
r(Fi), is the sum of the PageRanks of all functions that invoke Fi:
r(Fi) =∑Fj∈BFir(Fj)
|Fj|, where BFiis the set of functions that invoke
Fiand|Fj|is the number of functions that the function Fjinvokes.
This formula is applied iteratively starting with r0(Fi) =1/n, where
nis the number of functions. The process is repeated until Pag eR-
ank converges to some stable values or it is terminated after some
number of steps. Functions that are called from many other fu nc-
tions have a signiﬁcantly higher score than those that are us ed in-
frequently or not at all.4.4 Spreading Activation
Spreading activation computes weights for nodes in two step s:
pulses and termination checks. Initially, a set of starting nodes is
selected using a number of top ranked functions using the WOS
ranking. During pulses, new weights for different nodes are tran-
sitively computed from the starting nodes using the formula Nj=
Σif(Niwi j), where the weight of the node Njis equal to the sum
of all nodes Nithat are incident to the node Njwith edges whose
weights are wi j. This edge weight serves to give a reduced value
to nodes further away from the initial nodes. Therefore, the weight
is a value between 0 and 1. The function fis typically called the
threshold function that returns nonzero value only if the va lue of
the argument is greater than some chosen threshold, which ac ts as
a termination check preventing “ﬂooding” of the SAN.
4.5 Example of SAN Computation
Consider an example of SAN computation that is shown in Fig-
ure 4. This example is closely related to the motivating exam ple
query “mip map dithering texture image graphics .”
The ﬁrst ranking component, WOS, assigned the weights 0 .65 and
0.52 to the two functions TiledTexture andImageTexture
correspondingly. We label these functions with 1/circlecopyrt. All weights
are to the right (rounded off to the second digit). Their subs cripts
indicate the order in which weights are computed from the ﬁrs t
function weights. For example, the weight is computed for th e
functionCreateTextureFromImage by multiplying the WOS
weight for the function TiledTexture by the SAN edge weight
0.8. Several functions (e.g., load ,initRendered ) get different
weights by following different propagation paths from the i nitial
function nodes. In these cases, we use the highest value for e ach
node; the ﬁnal value assigned to initRenderer is 0.27.
4.6 Combined Ranking
The combined rank is S=λPR/bardblπ/bardblPR+λSAN/bardblπ/bardblSAN, where λis
the interpolation weight for each type of the score. These we ightsFigure 4: Example of SAN weight computation, wi j=0.8.
are determined independently of queries unlike the scores W OS
and SAN, which are query-dependent. Adjusting these weight s en-
ables experimentation with how underlying structural and t extual
information in application affects resulting ranking scor es. Exper-
imentation with PageRank involves changing the teleportat ion pa-
rameter that we brieﬂy discussed in Section 2.1.
5. EXPERIMENTAL DESIGN
Typically, search engines are evaluated using manual relev ance
judgments by experts [22, pages 151-153]. To determine how e f-
fective Portfolio is, we conducted an experiment with 49 par tici-
pants who are C/C++ programmers. Our goal was to evaluate how
well these participants could ﬁnd code fragments or functio ns that
matched given tasks using three different search engines: G oogle
Code Search (or simply, Google)4, Koders5and Portfolio6. We
chose to compare Portfolio with Google and Koders because th ey
are popular search engines with the large open source code re posi-
tories, and these engines are used by tens of thousands of pro gram-
mers every day.
5.1 Methodology
We used a cross validation experimental design in a cohort of 49
participants who were randomly divided into three groups. T he ex-
periment was sectioned in three experiments in which each gr oup
was given a different search engine (i.e., Google, Koders, o r Port-
folio) to ﬁnd code fragments or functions for given tasks. Ea ch
group used a different task in each experiment. The same task was
performed by different participants on different engines i n each ex-
periment. Before the experiment we gave a one-hour tutorial on
using these search engines.
In the course of each experiment, participants translated t asks
into a sequence of keywords that described key concepts they needed
to ﬁnd. Once participants obtained lists of code fragments o r func-
tions that were ranked in descending order, they examined th ese
functions to determine if they matched the tasks. Each parti cipant
accomplished this step individually, assigning a conﬁdenc e level,
4http://www.google.com/codesearch
5http://www.koders.com
6http://www.searchportfolio.netC, to the examined code fragments or functions using a four-le vel
Likert scale. We asked participants to examine only the top t en
code fragments that resulted from their searches since the t ime for
each experiment was limited to two hours.
The guidelines for assigning conﬁdence levels are the follo wing.
1. Completely irrelevant - there is absolutely nothing that the
participant can use from this retrieved code fragments, not h-
ing in it is related to keywords that the participant chose
based on the descriptions of the tasks.
2. Mostly irrelevant - a retrieved code fragment is only remo tely
relevant to a given task; it is unclear how to reuse it.
3. Mostly relevant - a retrieved code fragment is relevant to a
given task and participant can understand with some modest
effort how to reuse it to solve a given task.
4. Highly relevant - the participant is highly conﬁdent that code
fragment can be reused and s/he clearly see how to use it.
Forty four participants are Accenture employees who work on
consulting engagements as professional programmers for di ffer-
ent client companies. Five participants are graduate stude nts from
the University of Illinois at Chicago who have at least six mo nths
of C/C++ experience. Accenture participants have differen t back-
grounds, experience, and belong to different groups of the t otal Ac-
centure workforce of approximately 211,000 employees. Out of 49
participants, 16 had programming experience with C/C++ ran ging
from six months to two years, and 18 participants reported mo re
than three years of experience writing programs in C++. Ten p ar-
ticipants reported prior experience with Google Code Searc h and
three participants with Koders (which are used in this exper iment
thus introducing a bias toward these code search engines), n ine par-
ticipants reported frequent use of code search engines, and 16 said
that they never used code search engines. All participants h ave
bachelor degrees and 28 have master degrees in different tec hnical
disciplines.
5.2 Precision
Two main measures for evaluating the effectiveness of retri eval
are precision and recall [36, page 188-191]. The precision i s cal-
culated as Pr=# of retrieved functions that are relevant
total # of retrieved functions,
i.e., the precision of a ranking method is the fraction of the topr
ranked documents that are relevant to the query, where r=10 in
this experiment. Relevant code fragments or functions are c ounted
only if they are ranked with the conﬁdence levels 4or3. The pre-
cision metrics reﬂects the accuracy of the search. Since we l imit
the investigation of the retrieved code fragments or functi ons to top
ten, the recall is not measured in this experiment.
We created the variable precision, Pas a categorization of the re-
sponse variable conﬁdence, C. We did it for two reasons: improve
discrimination of subjects in the resulting data and additi onally val-
idate statistical evaluation of results. Precision, Pimposes a stricter
boundary on what is considered reusable code. For example, c on-
sider a situation where one participant assigns the level tw o to all
returned functions, and another participant assigns level three to
half of these functions and level one to the other half. Even t hough
the average of C=2 in both cases, the second participant reports
much higher precision, P=0.5 while the precision that is reported
by the ﬁrst participant is zero. Achieving statistical sign iﬁcance
with a stricter discriminative response variable will give assurance
that the result is not accidental.(a) Conﬁdence level, C.
 (b) Precision, P.
Figure 5: Statistical summary of the results of the experime nt for Cand P.The central box represents the values from the lower to upper
quartile (25 to 75 percentile). The middle line represents t he median. The thicker vertical line extends from the minimu m to the maximum value.
The ﬁlled-out box represents the values from the minimum to t he mean, and the thinner vertical line extends from the quart er below the mean to
the quarter above the mean. An outside value is deﬁned as a val ue that is smaller than the lower quartile minus 1.5 times the interquartile range, or
larger than the upper quartile plus 1.5 times the interquart ile range (inner fences). A far out value is deﬁned as a value t hat is smaller than the lower
quartile minus three times the interquartile range, or larg er than the upper quartile plus three times the interquartil e range (outer fences).
5.3 Variables
The main independent variable is the search engine (Portfol io,
Google Code Search, and Koders) that participants use to ﬁnd rel-
evant C/C++ code fragments and functions. The other indepen dent
variable is participants’ C++ experience. Dependent varia bles are
the values of conﬁdence level, C, and precision, P. We report these
variables in this section. The effects of other variables (t ask de-
scription length, prior knowledge) are minimized by the des ign of
this experiment.
5.4 Hypotheses
We introduce the following null and alternative hypotheses to
evaluate how close the means are for the Cs and Ps for control and
treatment groups. Unless we specify otherwise, participan ts of the
treatment group use Portfolio, and participants of the cont rol group
use either Google or Koders. We seek to evaluate the followin g
hypotheses at a 0 .05 level of signiﬁcance.
H0The primary null hypothesis is that there is no difference in
the values of conﬁdence level and precision per task between
participants who use Portfolio, Google, and Koders.
H1An alternative hypothesis to H0is that there is statistically sig-
niﬁcant difference in the values of conﬁdence and precision
between participants who use Portfolio, Google, and Koders .
Once we test the null hypothesis H0, we are interested in the
directionality of means, µ, of the results of control and treatment
groups. We are interested to compare the effectiveness of Po rtfolio
versus Google Code Search and Koders with respect to the valu es
of conﬁdence level, C, and precision, P.
H1 (C of Portfolio versus Google) The effective null hypothesis
is that µPort
C=µG
C, while the true null hypothesis is that µPort
C≤
µP
C. Conversely, the alternative hypothesis is µPort
C>µG
C.H2(P of Portfolio versus Google) The effective null hypothesis is
thatµPort
P=µG
P, while the true null hypothesis is that µPort
P≤
µG
P. Conversely, the alternative hypothesis is µPort
P>µG
P.
H3 (C of Portfolio versus Koders) The effective null hypothesis
is that µPort
C=µK
C, while the true null hypothesis is that µPort
C≤
µK
C. Conversely, the alternative is µPort
C>µK
C.
H4(P of Portfolio versus Koders) The effective null hypothesis is
thatµPort
P=µK
P, while the true null hypothesis is that µPort
P≥
µK
P. Conversely, the alternative is µPort
P<µK
P.
The rationale behind the alternative hypotheses to H1–H4is that
Portfolio allows users to quickly understand how queries ar e related
to retrieved functions. These alternative hypotheses are m otivated
by our belief that if users see visualization of functional d epen-
dencies in addition to functions whose ranks are computed hi gher
using our ranking algorithm, they can make better decisions about
how closely retrieved functions match given tasks.
5.5 Task Design
We designed 15 tasks for participants to work on during exper i-
ments in a way that these tasks belong to domains that are easy to
understand, and they have similar complexity. The authors o f this
paper visited various programming forums and internet grou ps to
extract descriptions of tasks from the questions that progr ammers
asked. In addition, we interviewed several programmers at A ccen-
ture who explained what tasks they worked on in the past year.
Additional criteria for these tasks is that they should repr esent real-
world programming tasks and should not be biased towards any of
the search engines that are used in this experiment. These ta sks and
the results of the experiment are available for download7.
7http://www.searchportfolio.net, follow the Experiment l ink.H Var Approach Samples Min Max Median µ StdDev σ2DF PCC p T Tcrit
H1 CPortfolio 1276 1 4 3 2.86 1.07 1.151372 0.04 4.2·10−10824 1.96Google 1373 1 4 2 1.97 1.11 1.23
H2 PPortfolio 184 0 1 0.7 0.65 0.28 0.08197 0.12 3·10−2210.9 1.97Google 198 0 1 0.25 0.35 0.33 0.11
H3 CPortfolio 1276 1 4 3 2.86 1.07 1.151485 0.06 1.1·10−2610.9 1.96Koders 1486 1 4 2 2.45 1.12 1.25
H4 PPortfolio 184 0 1 0.7 0.65 0.28 0.8207 0.041 3·10−85.76 1.97Koders 208 0 1 0.5 0.49 0.3 0.09
Table 1: Results of t-tests of hypotheses , H, for paired two sample for means for two-tail distributio n, for dependent variable speciﬁed in the
column Var (either CorP) whose measurements are reported in the following columns. Extremal values, Median, Means, µ, standard deviation,
StdDev, variance, σ2, degrees of freedom, DF, and the pearson correlation coefﬁc ient, PCC, are reported along with the results of the evaluat ion of
the hypotheses, i.e., statistical signiﬁcance, p, and the Tstatistics.
5.6 Tasks
The following three tasks are examples from the set of 15 task s
we used in our experiment.
•Implement a module for reading and playing midi ﬁles8.
•Implement a module that adjusts different parameters of a
picture, including brightness, contrast and white balance9.
•Build a program for managing USB devices. The program
should implement routines such as opening, closing, writin g
and reading from an USB device10.
5.7 Threats to Validity
In this section, we discuss threats to the validity of this ex peri-
ment and how we address these threats.
5.7.1 Internal Validity
Internal validity refers to the degree of validity of statem ents
about cause-effect inferences. In the context of our experi ment,
threats to internal validity come from confounding the effe cts of
differences among participants, tasks, and time pressure.
Participants. Since evaluating hypotheses is based on the data
collected from participants, we identify two threats to int ernal va-
lidity: C++ proﬁciency and motivation of participants.
Even though we selected participants who have working knowl -
edge of C++ as it was documented by human resources, we did not
conduct an independent assessment of how proﬁcient these pa rtici-
pants are in C++. This threat is mitigated by the fact that out of 44
participants from Accenture, 31 have worked on successful c om-
mercial projects as C++ programmers for more than two years.
The other threat to validity is that not all participants cou ld be
motivated sufﬁciently to evaluate retrieved code fragment s or func-
tions. We addressed this threat by asking participants to ex plain in
a couple of sentences why they chose to assign certain conﬁde nce
level to retrieved, and we discarded 27 results for all searc h engines
that were not properly explained.
Time pressure. Each experiment lasted for two hours. For some
participants, this was not enough time to explore all 50 retr ieved
code fragments for ﬁve tasks (ten results for each of ﬁve task s).
Therefore, one threat to validity is that some participants could try
to accomplish more tasks by shallowly evaluating retrieved code
8http://www.codeproject.com/Messages/1427393/How-Can -I-
Read-Midi-File.aspx
9http://www.codeguru.com/forum/showthread.php?t=4323 39
10http://www.cplusplus.com/forum/general/25172/fragments and functions. To counter this threat we notiﬁed p ar-
ticipants that their results would be discarded if we did not see
sufﬁcient reported evidence of why they evaluated retrieve d code
fragments and functions with certain conﬁdence levels.
Sensitivity of Portfolio. Recovering functional dependencies
automatically introduces imprecision, since it is an undec idable
problem to recover precise functional dependencies in the p resence
of dynamic dispatch and functional pointers [18]. Since the preci-
sion of Portfolio depends on the quality of recovered functi onal de-
pendencies, we conducted an evaluation of these recovered d epen-
dencies with twelve graduate computer science students at D ePaul
university. We randomly selected a representative sample o f 25 dif-
ferent projects in Portfolio and we asked these students to m anually
inspect source code of these projects to determine the preci sion of
FCG computed in Portfolio.
The results of this evaluation show that the precision of rec ov-
ered functional dependencies is approximately 76%. While t he pre-
cision appears to be somewhat lower than desired, it is known that
Pagerank is resilient to incorrect links. Link farms, for ex ample,
are web spam where people create fake web sites that link to on e
another in an attempt to skew the PageRank vector. It is estim ated
that close to 20% of all links on the Internet are spam [11, 28, 1].
However, it is shown that the PageRank vector is not affected sig-
niﬁcantly by these spam links since its sensitivity is contr olled by
different factors, one of which is teleportation parameter [9]. To
evaluate the effect of incorrect links on Pagerank vector we con-
ducted experiments where we randomly modiﬁed 25% and 50% of
links between functions. Our results show that the metric le ngth
of the Pagerank vector (computed as the square root of the sum of
squares of its components) changes only by approximately 7% for
50% of perturbed functional dependencies. A brief explanat ion is
that by adding or removing a couple of links to functions that are
either well-connected or not connected at all, their Pagera nk score
is not strongly affected. Investigating the sensitivity of Portfolio as
well as improving recovery of functional dependencies is th e sub-
ject of future work.
5.7.2 External Validity
To make the results of this experiment generalizable, we mus t
address threats to external validity, which refer to the gen eraliz-
ability of a casual relationship beyond the circumstances o f our ex-
periment. The fact that supports the validity of this experi mental
design is that the participants are highly representative o f profes-
sional C/C++ programmers. However, a threat to external val idity
concerns the usage of search tools in the industrial setting s, where
requirements are updated on a regular basis. Programmers us eC/C++ Cs - Level 1 Cs - Level 2 Cs - Level 3 Cs - Level 4TotalExperts Google Koders Portf Google Koders Portf Google Koders Portf Google Koders Portf
Yes 450 269 130 178 252 185 189 272 229 139 247 339 2,879
No 222 131 56 79 101 92 65 108 106 49 98 135 1,242
Total 672 400 186 257 353 277 254 380 335 188 345 474 4,121
Table 2: The numbers of the different levels of conﬁdence, Cfor participants with and without expert C/C++ experience.
these updated requirements to reﬁne their queries and locat e rele-
vant code fragments or functions using multiple iterations of work-
ing with search engines. We addressed this threat only parti ally, by
allowing programmers to reﬁne their queries multiple times .
In addition, participants performed multiple searches usi ng dif-
ferent combinations of keywords, and they select certain re trieved
code fragments or functions from each of the search results. We
believe that the results produced by asking participants to decide
on keywords and then perform a single search and rank code fra g-
ments and functions do not deviate signiﬁcantly from the sit uation
where searches using multiple (reﬁned) queries are perform ed.
Another threat to external validity comes from different si zes of
software repositories. Koders.com claims to search more th an 3
Billion LOC, which is also close to the number of LOC reported
by Google Code Search. Even though we populated Portfolio’s
repository with close to 270 Mil LOC, it still remains a threa t to
external validity.
6. RESULTS
In this section, we report the results of the experiment and e valu-
ate the hypotheses. We use one-way ANOV A, t-tests for paired two
sample for means, and χ2to evaluate the hypotheses that we stated
in Section 5.4.
6.1 Testing the Null Hypothesis
We used ANOV A to evaluate the null hypothesis H0that the
variation in an experiment is no greater than that due to norm al
variation of individuals’ characteristics and error in the ir measure-
ment. The results of ANOV A conﬁrm that there are large differ -
ences between the groups for Cwith F=261.3>Fcrit=3 with
p≈5·10−108which is strongly statistically signiﬁcant. The mean
Cfor the Google Code Search is 1 .97 with the variance 1 .14, which
is smaller than the mean Cfor Koders, 2 .45 with the variance 1 .26,
and it is smaller than the mean Cfor Portfolio, 2 .86 with the vari-
ance 0 .99. Also, the results of ANOV A conﬁrm that there are large
differences between the groups for Pwith F=52.5>Fcrit=3.01
with p≈8.6·10−22which is strongly statistically signiﬁcant. The
mean Pfor the Google Code Search is 0 .35 with the variance 0 .1,
which is smaller than the mean Pfor Koders, 0 .49 with the variance
0.09, and it is smaller than the mean Pfor Portfolio, 0 .65 with the
variance 0 .07. Based on these results we reject the null hypothesis
and we accept the alternative hypothesis H1.
A statistical summary of the results of the experiment for Cand
T(median, quartiles, range and extreme values) is shown as bo x-
and-whisker plots in Figure 5(a) and Figure 5(b) correspond ingly
with 95% conﬁdence interval for the mean. Even though the num -
bers of sample sizes are slightly different since some users missed
one experiment, we replaced missing values with their avera ges.
Even though replacing missing data introduces an error, giv en ex-
tremely low values of p, this error is highly unlikely to affect our
results.6.2 Comparing Portfolio with Google
To test the null hypothesis H1andH2we applied two t-tests for
two paired sample means, in this case CandPfor participants who
used Google Code Search and Portfolio. The results of this te st for
Cand for Pare shown in Table 1. The column Samples shows
different values that indicate that not all 49 participants participated
in all experiments (three different participants missed tw o different
experiments). Based on these results we reject the null hypo theses
H1andH2and we accept the alternative hypotheses that states that
participants who use Portfolio report higher relevance and pre-
cision on ﬁnding relevant functions than those who use Googl e
Code Search .
6.3 Comparing Portfolio with Koders
To test the null hypotheses H3andH4, we applied two t-tests for
two paired sample means, in this case CandPfor participants who
used Portfolio and Koders. The results of this test for Cand for
Pare shown in Table 1. Based on these results we reject the null
hypotheses H3andH4that say that participants who use Port-
folio report higher relevance and precision on ﬁnding relev ant
functions than those who use Koders .
6.4 Experience Relationships
We construct contingency tables to establish a relationshi p be-
tween Cfor participants with (2+ years) and without (less than 2
years) expert C++ experience who use different search engin es.
These tables are retrieved from the table that is shown in Tab le 2
that shows the numbers of the different levels of conﬁdence, Cfor
participants with and without expert C/C++ experience. To t est the
null hypotheses that the categorical variable Cis independent from
the categorical variable Java experience, we apply three χ2-tests,
χ2
G,χ2
K, and χ2
Pfor the search engines Google, Koders, and Port-
folio respectively. We obtain χ2
G≈6.7 for p<0.09,χ2
K≈2.6 for
p<0.47, and χ2
P=2.09 for p<0.56. The insigniﬁcant values of
χ2and large values of p>0.05 allow us to accept these null hy-
potheses suggesting that there is no statistically strong relation-
ship between expert C++ programming experiences of partici -
pants and the values of reported Cs for the code search engines
Google Code Search, Koders, and Portfolio .
6.5 Usefulness of Visualization
Thirty three participants reported that the visualization of func-
tional dependencies in Portfolio is useful and helped them t o evalu-
ate potential reuse of retrieved functions, while 12 respon dents did
not ﬁnd this visualization useful. Out these 33 participant s who
found it useful, 27 had more than one year of C++ experience,
while out of these 12 participants who did not ﬁnd this visual ization
useful, only two had more than one year of C++ experience.
7. RELATED WORK
Different code mining techniques and tools have been propos ed
to ﬁnd relevant software components as it is shown in Table 3.CodeFinder iteratively reﬁnes code repositories in order t o improve
the precision of returned software components [12]. Unlike Portfo-
lio, CodeFinder heavily depends on the descriptions (often incom-
plete) of software components to use word matching, while Po rt-
folio uses Pagerank and SANs to help programmers navigate an d
understand usages of retrieved functions.
Codebroker system uses source code and comments written by
programmers to query code repositories to ﬁnd relevant arti facts
[37]. Unlike Portfolio, Codebroker is dependent upon the de scrip-
tions of documents and meaningful names of program variable s
and types, and this dependency often leads to lower precisio n of
returned projects.
Even though it returns code snippets rather than functions, Mica
is similar to Portfolio since it uses API calls from Java Deve lop-
ment Kit to guide code search [32]. However, Mica uses help do c-
umentation to reﬁne the results of the search, while Portfol io auto-
matically retrieves functions from arbitrary code reposit ories and it
uses more sophisticated models to help programmers evaluat e the
potential of code reuse faster and a with higher precision.
Exemplar, SNIFF, and Mica use documentation for API calls fo r
query expansion [10, 32, 3]. SNIFF then performs the interse c-
tion of types in these code chunks to retain the most relevant and
common part of the code chunks. SNIFF also ranks these pruned
chunks using the frequency of their occurrence in the indexe d code
base. In contrast to SNIFF, Portfolio uses navigation and as socia-
tion models that reﬂect behavior of programmers and improve the
precision of the search engine. In addition, Portfolio offe rs a visu-
alization of usages of functions that it retrieves automati cally from
existing source code, thus avoiding the need for third-part y docu-
mentation for API calls.
Web-mining techniques have been applied to graphs derived f rom
program artifacts before. Notably, Inoue et al. proposed Co mpo-
nent Rank[16] as a method to highlight the most-frequently u sed
classes by applying a variant of PageRank to a graph composed of
Java classes and an assortment of relations among them. Qual ity
of match (QOM) ranking measures the overall goodness of matc h
between two given components [33], which is different from P ort-
folio in many respects, one of which is to retrieve functions based
on surﬁng behavior of programmers and associations between con-
cepts in these functions.
Gridle[24] also applies PageRank to a graph of Java classes. In
Portfolio, we apply PageRank to a graph with nodes as functio ns
and edges as call relationships among the functions. In addi tion,
we use spreading activation on the call graph to retrieve cha ins of
relevant function invocations, rather than single fragmen ts of code.
Programming task-oriented tools like Prospector, Hipikat , Strath-
cona, and xSnippet assist programmers in writing complicat ed code
[21, 7, 14, 27]. However, their utilities are not applicable when
searching for relevant functions given a query containing h igh-level
concepts with no source code.
Robillard proposed an algorithm for calculating program el e-
ments of likely interest to a developer [26]. Portfolio is si milar
to this algorithm in that it uses relations between function s in the
retrieved projects to compute the level of interest (rankin g) of the
project, however, Robillard does not use models that reﬂect the
surﬁng behavior of programmers and association models that im-
prove the precision of search. We think there is a potential i n ex-
ploring connections between Robillard’s approach and Port folio.
S6is a code search engine that uses a set of user-guided program
transformations to map high-level queries into a subset of r elevant
code fragments [25], not necessarily functions. Like Portf olio, S6
uses query expansion, however, it requires additional low- level de-
tails from the user, such as data types of test cases.Approach Granularity Search Result
Unit Usage Method
AMC [13] U N W T
CodeBroker [37] P,U Y W,Q T
CodeFinder [12] F,U Y W,Q T
CodeGenie [20] P N W T
Exemplar [10] A Y W,Q T
Google Code Search U N W T
Gridle [24] U N W T
Hipikat [7] P Y W,Q T
Koders U N W T
Krugle U N W T
MAPO [38] F N W,Q T
Mica [32] U,F Y W,Q T
ParseWeb [34] U,F N W,Q T
Portfolio F,P Y P,S,W G
Prospector [21] F N T T
S6[25] F,P,U Y W,Q T
SNIFF [3] F,U Y T,W T
Sourceforge A N W T
Sourcerer [23] F,P,U Y P,W T
SPARS-J [15][16] F Y P T
SpotWeb [35] U N W T
Strathcona [14] F Y W T
xSnippet [27] F Y T,W T
Table 3: Comparison of Portfolio with other related ap-
proaches. ColumnGranularity speciﬁes how search results are
returned by each approach (P rojects, F unctions, or U nstructured text),
and if the usage of these resulting code units is shown (Y es or N o). The
columnSearch Method speciﬁes the search algorithms or techniques
that are used in the code search engine, i.e., P agerank, S preading ac-
tivation, simple W ord matching, parameter T ype matching, or Q uery
expansion techniques. Finally, the last column tells if the search engine
shows a list of code fragments as T ext or it uses a G raphical represen-
tation of search results to illustrate code usage for progra mmers.
8. CONCLUSION
We created an approach called Portfolio for ﬁnding highly re le-
vant functions and projects from a large archive of C/C++ sou rce
code. In Portfolio, we combined various natural language process-
ing (NLP) and indexing techniques with a variation of PageRank
andspreading activation network (SAN) algorithms to address the
need of programmers to reuse retrieved code as functional ab strac-
tions. We evaluated Portfolio with 49 professional C/C++ pr ogram-
mers and found with strong statistical signiﬁcance that it p erformed
better than Google Code Search and Koders in terms of reporti ng
higher conﬁdence levels and precisions for retrieved C/C++ code
fragments and functions. In addition, participants expres sed strong
satisfaction with using Portfolio’s visualization techni que since it
enabled them to assess how retrieved functions are used in co ntexts
of other functions.
Acknowledgments
We warmly thank nine graduate students, Luca DiMinervino, A run-
rajkumar Dharumar, Rohan Dhond, Sekhar Gopisetty, Harihar an
Subramanian, Ameya Barve, Naresh Regunta, Ashim Shivhare,
Denzil Rodrigues, from the University of Illinois at Chicag o who
contributioned to Portfolio as part of their work towards th e com-pletion of thier Master of Science in Computer Science degre es.
We also thank Bogdan Dit from the College of William and Mary
for his help in building parts of Portfolio. We are grateful t o the
anonymous ICSE’11 reviewers for their relevant and useful c om-
ments and suggestions, which helped us to signiﬁcantly impr ove
an earlier version of this paper. This work is supported by NS F
CCF-0916139, CCF-0916260, and Accenture.
9. REFERENCES
[1] L. Becchetti, C. Castillo, D. Donato, R. Baeza-YATES, an d
S. Leonardi. Link analysis for web spam detection. ACM
Trans. Web , 2(1):1–42, 2008.
[2] S. Brin and L. Page. The anatomy of a large-scale
hypertextual web search engine. Computer Networks ,
30(1-7):107–117, 1998.
[3] S. Chatterjee, S. Juvekar, and K. Sen. Sniff: A search eng ine
for java using free-form queries. In FASE , pages 385–400,
2009.
[4] A. M. Collins and E. F. Loftus. A spreading-activation th eory
of semantic processing. Psychological Review , 82(6):407 –
428, 1975.
[5] T. A. Corbi. Program understanding: Challenge for the
1990s. IBM Systems Journal , 28(2):294–306, 1989.
[6] F. Crestani. Application of spreading activation techn iques in
information retrieval. Artiﬁcial Intelligence Review ,
11(6):453–482, 1997.
[7] D. Cubranic, G. C. Murphy, J. Singer, and K. S. Booth.
Hipikat: A project memory for software development. IEEE
Trans. Software Eng. , 31(6):446–465, 2005.
[8] J. W. Davison, D. Mancl, and W. F. Opdyke. Understanding
and addressing the essential costs of evolving systems. Bell
Labs Technical Journal , 5(2):44–54, 2000.
[9] D. F. Gleich, P. G. Constantine, A. D. Flaxman, and
A. Gunawardana. Tracking the random surfer: empirically
measured teleportation parameters in pagerank. In WWW ,
pages 381–390, 2010.
[10] M. Grechanik, C. Fu, Q. Xie, C. McMillan, D. Poshyvanyk,
and C. M. Cumby. A search engine for ﬁnding highly
relevant applications. In ICSE (1) , pages 475–484, 2010.
[11] Z. Gyöngyi and H. Garcia-Molina. Link spam alliances. I n
VLDB ’05 , pages 517–528. VLDB Endowment, 2005.
[12] S. Henninger. Supporting the construction and evoluti on of
component repositories. In ICSE , pages 279–288, 1996.
[13] R. Hill and J. Rideout. Automatic method completion. In
ASE, pages 228–235, 2004.
[14] R. Holmes and G. C. Murphy. Using structural context to
recommend source code examples. In ICSE , pages 117–125,
2005.
[15] K. Inoue, R. Yokomori, H. Fujiwara, T. Yamamoto,
M. Matsushita, and S. Kusumoto. Component rank: Relative
signiﬁcance rank for software component search. In ICSE ,
pages 14–24, 2003.
[16] K. Inoue, R. Yokomori, T. Yamamoto, M. Matsushita, and
S. Kusumoto. Ranking signiﬁcance of software components
based on use relations. IEEE Trans. Softw. Eng. ,
31(3):213–225, 2005.
[17] C. W. Krueger. Software reuse. ACM Comput. Surv. ,
24(2):131–183, 1992.
[18] W. Landi. Undecidability of static analysis. LOPLAS ,
1(4):323–337, 1992.
[19] A. N. Langville and C. D. Meyer. Google’s PageRank andBeyond: The Science of Search Engine Rankings . Princeton
University Press, Princeton, NJ, USA, 2006.
[20] O. A. L. Lemos, S. K. Bajracharya, J. Ossher, R. S. Morla,
P. C. Masiero, P. Baldi, and C. V . Lopes. Codegenie: using
test-cases to search and reuse source code. In ASE ’07 , pages
525–526, New York, NY , USA, 2007. ACM.
[21] D. Mandelin, L. Xu, R. Bodík, and D. Kimelman. Jungloid
mining: helping to navigate the API jungle. In PLDI , pages
48–61, 2005.
[22] C. D. Manning, P. Raghavan, and H. Schtze. Introduction to
Information Retrieval . Cambridge University Press, New
York, NY , USA, 2008.
[23] J. Ossher, S. Bajracharya, E. Linstead, P. Baldi, and
C. Lopes. Sourcererdb: An aggregated repository of
statically analyzed and cross-linked open source java
projects. MSR , 0:183–186, 2009.
[24] D. Puppin and F. Silvestri. The social network of java
classes. In SAC ’06 , pages 1409–1413, New York, NY , USA,
2006. ACM.
[25] S. P. Reiss. Semantics-based code search. In ICSE , pages
243–253, 2009.
[26] M. P. Robillard. Automatic generation of suggestions f or
program investigation. In ESEC/FSE , pages 11–20, 2005.
[27] N. Sahavechaphan and K. T. Claypool. XSnippet: mining f or
sample code. In OOPSLA , pages 413–430, 2006.
[28] H. Saito, M. Toyoda, M. Kitsuregawa, and K. Aihara. A
large-scale study of link spam detection by graph algorithm s.
InAIRWeb ’07 , pages 45–48, New York, NY , USA, 2007.
ACM.
[29] J. Sillito, G. C. Murphy, and K. De V older. Asking and
answering questions during a programming change task.
IEEE Trans. Softw. Eng. , 34(4):434–451, 2008.
[30] J. Sillito, G. C. Murphy, and K. D. V older. Questions
programmers ask during software evolution tasks. In
SIGSOFT FSE , pages 23–34, 2006.
[31] S. Sim, C. Clarke, and R. Holt. Archetypal source code
searches: A survey of software developers and maintainers.
ICPC , 0:180, 1998.
[32] J. Stylos and B. A. Myers. A web-search tool for ﬁnding
API components and examples. In IEEE Symposium on VL
and HCC , pages 195–202, 2006.
[33] N. Tansalarak and K. T. Claypool. Finding a needle in the
haystack: A technique for ranking matches between
components. In CBSE , pages 171–186, 2005.
[34] S. Thummalapenta and T. Xie. Parseweb: a programmer
assistant for reusing open source code on the web. In ASE
’07, pages 204–213, New York, NY , USA, 2007. ACM.
[35] S. Thummalapenta and T. Xie. Spotweb: Detecting
framework hotspots and coldspots via mining open source
code on the web. In ASE ’08 , pages 327–336, Washington,
DC, USA, 2008. IEEE Computer Society.
[36] I. H. Witten, A. Moffat, and T. C. Bell. Managing Gigabytes:
Compressing and Indexing Documents and Images, Second
Edition . Morgan Kaufmann, 1999.
[37] Y . Ye and G. Fischer. Supporting reuse by delivering
task-relevant and personalized information. In ICSE , pages
513–523, 2002.
[38] H. Zhong, T. Xie, L. Zhang, J. Pei, and H. Mei. MAPO:
Mining and recommending API usage patterns. In ECOOP
2009 , July 2009.