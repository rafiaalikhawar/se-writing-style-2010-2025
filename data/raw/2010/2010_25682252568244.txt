Us and Them: A Study of Privacy Requirements Across
North America, Asia, and Europe
Swapneel Sheth, Gail Kaiser
Department of Computer Science
Columbia University
New Y ork, NY , USA
{swapneel, kaiser}@cs.columbia.eduWalid Maalej
Department of Informatics
University of Hamburg
Hamburg, Germany
maalej@informatik.uni-hamburg.de
ABSTRACT
Data privacy when using online systems like Facebook and
Amazon has become an increasingly popular topic in the last
few years. However, only a little is known about how users
and developers perceive privacy and which concrete measures
would mitigate their privacy concerns. To investigate privacy
requirements, we conducted an online survey with closed
and open questions and collected 408 valid responses. Our
results show that users often reduce privacy to security,
with data sharing and data breaches being their biggest
concerns. Users are more concerned about the content of
their documents and their personal data such as location
than about their interaction data. Unlike users, developers
clearly prefer technical measures like data anonymization
and think that privacy laws and policies are less eective.
We also observed interesting dierences between people from
dierent geographies. For example, people from Europe
are more concerned about data breaches than people from
North America. People from Asia/Pacic and Europe believe
that content and metadata are more critical for privacy
than people from North America. Our results contribute to
developing a user-driven privacy framework that is based on
empirical evidence in addition to the legal, technical, and
commercial perspectives.
Categories and Subject Descriptors
D.2.1 [ Software Engineering ]: Requirements/Specica-
tions; K.4.1 [ Computers and Society ]: Public Policy Is-
sues| Privacy
General Terms
Human Factors
Keywords
Human factors in software engineering, requirements engi-
neering, privacy, user developer collaboration, interaction
data, empirical studies
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proï¬t or commercial advantage and that copies
bear this notice and the full citation on the ï¬rst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciï¬c
permission and/or a fee.
ICSE â€™14, May 31 â€“ June 7, 2014, Hyderabad, India
Copyright 2014 ACM 978-1-4503-2756-5/14/05 ...$15.00.1. INTRODUCTION
As systems that collect and use personal data, such as
Facebook and Amazon, become more pervasive in our daily
lives, users are starting to worry about their privacy. There
has been a lot of media coverage about data privacy. One of
the earliest articles in the New York Times reported how it
was possible to break the anonymity of AOL's search engine's
users [7]. A more recent article mentions privacy concerns
about Google Glass [29]. Both technical and, especially, non-
technical users are nding it increasingly hard to navigate
this privacy mineeld [21]. This is further exacerbated by
well-known systems periodically making changes that breach
privacy and not allowing users to opt out a-priori [19].
There is a large body of research on privacy in vari-
ous research communities. This ranges from data anony-
mization techniques in dierent domains [13, 23, 35, 44] to
novel approaches to make privacy settings more understand-
able [18, 34]. Recent studies have shown that there is a
discrepancy between users' intentions and reality for privacy
settings [24,27]. The assumption behind most of this work
is that privacy is well-specied and important. However,
there is very little evidence about what exactly are the user
concerns, priorities, and trade-os, and how users think these
concerns can be mitigated. In particular, in the software en-
gineering community, there have been no systematic studies
to nd out what privacy requirements are and how these
requirements should be addressed by developers.
This research aims to understand the privacy expectations
and needs for modern software systems. To this end, we
conducted an online survey. We received 595 responses and
selected 408 of them as valid. The responses represented
diverse populations including developers and users, and peo-
ple from North America, Europe, and Asia. The results of
our study show that the biggest privacy concerns are data
sharing and data breaches. However, there is a disagreement
on the best approach to address these concerns. With respect
to types of data that are critical for privacy, respondents are
least concerned about metadata and interaction data and
most concerned about their personal data and the content of
documents. Most respondents are not willing to accept less
privacy in exchange for fewer advertisements and nancial
incentives such as discounts on purchases.
The main contribution of this paper is threefold. First,
it illustrates and quanties the general trends on how users
understand privacy and on how they assess dierent privacy
concerns and measures to address them. Second, the paper
identies dierences in privacy expectations between various
groups: developers versus users and people from dierentPermission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proï¬t or commercial advantage and that copies bear this notice and the full citation
on the ï¬rst page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior speciï¬c permission and/or a
fee. Request permissions from Permissions@acm.org.
ICSEâ€™14 , May 31 â€“ June 7, 2014, Hyderabad, India
Copyright 2014 ACM 978-1-4503-2756-5/14/05...$15.00
http://dx.doi.org/10.1145/2568225.2568244
859
geographic regions. Finally, the paper gives insights into how
software developers and managers can identify, analyze, and
address privacy concerns of their users { building a rst step
towards a software engineering privacy framework.
Our analysis for geographic regions, for example, shows
that there is a signicant dierence between respondents
from North America, Europe, and Asia/Pacic. People
from Europe and Asia/Pacic rate dierent types of data
such as metadata, content, and interaction data being a
lotmore critical for privacy than respondents from North
America. People from Europe are a lot more concerned about
data breaches than data sharing whereas people from North
America are equally concerned about the two. Similarly, our
analysis for developers versus users shows a marked dierence
between the two groups. For example, developers believe
that privacy laws and policies are lesseective for reducing
privacy concerns than data anonymization.
The rest of the paper is organized as follows. Section 2
describes the design of our study. Sections 3, 4, and 5 high-
light its key results. Section 6 discusses the implications of
the results and their limitations. Finally, Section 7 describes
related work and Section 8 concludes the paper.
2. STUDY DESIGN
We describe the research questions, methods, and respon-
dents of our study.
2.1 Research Questions
There have been many dierent denitions of privacy over
time. One of the earliest denitions was the \right to be
left alone" as described by Warren and Brandeis [50]. Solove
claims that \privacy is an umbrella term, referring to a wide
and disparate group of related things". The author proposes
a taxonomy of privacy in the context of harmful activities
such as information collection, information processing, infor-
mation dissemination, and invasion [38]. According to the
Merriam-Webster dictionary, privacy is the \freedom from
unauthorized intrusion". We are interested specically in
data privacy and other notions of privacy such as physical
privacy are beyond the scope of our work.
The goal of this study is to gather and analyze privacy
requirements for modern software systems. In particular, we
want to study the perception of dierent groups of people
on privacy. We focused on the following research questions:
RQ 1 : What are developers' and users' perceptions
of privacy? What aspects of privacy are more impor-
tant and what are the best measures to address them?
(Section 3)
RQ 2 : Does software development experience have any
impact on privacy requirements? (Section 4)
RQ 3 : Does geography have any impact on privacy
requirements? (Section 5)
By perception, we mean the subjective understanding and
assessment of privacy aspects. Since privacy is a very broad
term, we are interested in specic aspects , in particular, types
of concerns, measures to mitigate these concerns, types of
data that are critical to privacy, and whether people would
give up privacy. We think these aspects are most related to
software and requirements engineering concerns.2.2 Research Method
We designed an online survey with 16 questions, which
took 5{10 minutes to answer. Out of the 16 questions, 14
were closed and respondents had to choose an answer from a
list of options. The survey also had two open-ended questions.
This helped us get qualitative insights about privacy and
gave an opportunity for respondents to report aspects that
were not already included in the closed questions.
We chose a survey instead of observations or interviews for
the following reasons. First, surveys are scalable and allow
to get a large number and broad cross-section of responses.
Second, we were interested in the subjective opinion of people
and this can be dierent from real behavior. Third, the
closed questions were purely quantitative and allowed us to
analyze general trends and correlations. We did not aim for a
representative report of the opinions. This would have been
possible only through a well-dened population and random
representative sampling. Instead, we were interested in the
priority trends and inter-relations, which can be analyzed
through a cross-tabulation of the survey answers.
We used semantic scales for the closed questions, allowing
for the measurement of subjective assessments while giving
respondents some exibility of the interpretation [37]. For
example, one question was: \Would users be willing to use
your system if they are worried about privacy issues?" and
the answer options were: \Denitely yes | Users don't care
about privacy", \Probably yes", \Unsure", \Probably not",
and \Denitely not | if there are privacy concerns, users
will not use this system". To reduce the complexity of matrix
questions (which include multiple answer options) we used
a 3-point scale consisting of \Yes", \No", and \Uncertain".
When we observed in the dry runs that higher discriminative
powers were needed, we used a 5-point scale [22].
Respondents could choose to ll out our survey in two
languages: English or German. For each language, there
were two slightly dierent versions based on whether the
respondents had experience in software development or not.
The dierence in the versions was only in the phrasing of
the questions in order to reduce confusion. For example,
developers were asked: \Would users be willing to use your
system if they are worried about privacy issues?" whereas
users were asked: \Would you be willing to use the system if
you are worried about privacy issues?"
To increase the reliability of the study [37], we took the
following measures:
Pilot Testing: We conducted pilot testing in four itera-
tions with a total of ten users that focused on improving
the timing and understandability of the questions. We
wanted to reduce ambiguity about the questions and
answers and ensure that none of the semantics were lost
in translation. We used the feedback from pilot testing
to improve the phrasing and the order of questions for
the English and German versions.
Random order of answers: The answer options for the
closed questions were randomly ordered. This ensures
that answer order does not inuence the response [48].
Validation questions: To ensure that respondents did
not ll out the answers arbitrarily, we included two
validation questions [3]. For example, one of the val-
idation questions was: \What is the sum of 2 and 5?"
Respondents who did not answer these correctly were
not included in the nal set of valid responses.860Table 1: Summary of study respondents based on
location and software development experience
Developers Users
North America 85 44
Europe 116 65
Asia 61 30
South America 3 2
Africa 2 0
Post sampling: We monitored the number of respon-
dents from each category of interest: developers, users,
and geographic location. We conducted post-sampling
and stratication to ensure that we got sucient re-
sponses for each category and that the ratio of develop-
ers to users for each geographic location was roughly
similar. For categories that did not have sucient re-
spondents, we targeted those populations by posting
the survey in specic channels. We stopped data col-
lection when we had a broad spectrum of respondents
and sucient representation in all the categories.
Finally, to corroborate our results, we conducted a number
of statistical tests. In particular, we used the Z-test for
equality of proportions [40] and Welch's Two Sample t-test
to check if our results are statistically signicant.
2.3 Survey Respondents
We did not have any restrictions on who could ll out the
survey. We wanted, in particular, people with and without
software development experience and people from dierent
parts of the world. We distributed our survey through a
variety of channels including various mailing lists, social
networks like Facebook and Twitter, personal contacts, and
colleagues. We circulated the survey across companies with
which we are collaborating. We also asked specic people
with many contacts (e.g., with many followers on Twitter) to
forward the survey. As an incentive, two iPads were raed
among the respondents.
In total, 595 respondents lled out our survey between
November 2012 and September 2013. Filtering out the incom-
plete and invalid responses resulted in 408 valid responses
(68.6%). Table 1 shows the respondents based on location
and software development experience. The four versions of
the survey along with raw data and summary information
are available on our website1. Among the respondents, 267
have software development experience and 141 do not. For
respondents with development experience, 28 have less than
one year of experience, 129 have 1-5 years, 57 have 5-10
years, and 53 have more than ten years of experience. 129
respondents live in North America, 181 in Europe, and 91 in
Asia/Pacic. 166 are aliated with industry or public sector,
182 are in academia and research, and 56 are students.
3. PRIVACY PERCEPTIONS
We asked respondents: \How important is the privacy issue
in online systems?" They answered using a 5-point semantic
scale ranging from \Very important" to \Least important".
Two thirds of the respondents chose \Very Important", 25.3%
1http://mobis.informatik.uni-hamburg.de/privacy-
requirements/chose \Important", and the remaining three options (\Aver-
age", \Less Important", \Least Important") combined were
chosen by a total of 8.1% of the respondents.
The location of the data storage was a key concern for
the respondents. We asked respondents whether privacy
concerns depend on the location of where the data is stored
and provided a 5-point semantic scale with options: \Yes",
\Maybe yes", \Unsure", \Maybe not", and \No". 57.7% of
the respondents chose \Yes", 28.6% chose \Maybe yes", while
only 13.7% of the respondents chose the remaining three
options.
On the other hand, there was disagreement about whether
users would be willing to use such systems if there were
privacy concerns. The answer options were: \Denitely yes
| Users don't care about privacy", \Probably yes", \Unsure",
\Probably not", and \Denitely not | if there are privacy
concerns, users will not use this system". 20.8% of the
respondents choose \Unsure", while 34.8% and 29.4% chose
\Probably yes" and \Probably not" respectively.
3.1 Factors that Increase and Reduce Privacy
Concerns
We asked respondents if the following factors would in-
crease privacy concerns:
Data Aggregation: The system discovers additional
information about the user by aggregating data over a
long period of time.
Data Distortion: The system might misrepresent the
data or user intent.
Data Sharing: The collected data might be given to
third parties for purposes like advertising.
Data Breaches: Malicious users might get access to
sensitive data about other users.
For each concern, the respondents could answer using a
3-point semantic scale with the options: \Yes", \Uncertain",
and \No". We also asked respondents if the following would
help to reduce concerns about privacy:
Privacy Policy, License Agreements, etc.: Describing
what the system will/won't do with the data.
Privacy Laws: Describing which national law the sys-
tem is compliant with (e.g., HIPAA in the US, European
privacy laws).
Anonymizing all data: Ensuring that none of the data
has any personal identiers.
Technical Details: Describing the algorithms/source
code of the system in order to achieve higher trust (e.g.,
encryption of data).
Details on usage: Describe, e.g., in a table how dierent
data are used.
Figure 1 shows the overall answers for both questions.
In the gure, each answer option is sorted by the number
of \Yes" respondents. Most respondents agreed that the
biggest privacy concerns are data breaches and data sharing.
There is disagreement about whether data distortion and
data aggregation would increase privacy concerns. To check
if these results are statistically signicant, we ran Z-tests
for equality of proportions. This would help us validate, for
example, if there is a statistically signicant dierence in
the number of respondents who said \Yes" for two dierent861# responsesAggregationDistortionSharingBreachIncrease ConcernIncrease Concern
Technical detailsPolicyLawsUsageAnonymization
200 100 0 100 200 300 400Reduce ConcernReduce Concern
No Uncertain YesFigure 1: What increases and reduces privacy concerns?
Table 2: What increases privacy concerns? For each
privacy concern, X > Y indicates that Xis a bigger
concern than Yfor the respondents. We used the Z-
test for equality of proportions and only statistically
signicant results for p <0:01are shown.
Privacy concerns p-values
Sharing >Aggregation p= 1:231e 12
Sharing >Distortion p= 6:036e 14
Breach >Aggregation p <2:2e 16
Breach >Distortion p <2:2e 16
options. The results for increasing concerns about privacy are
shown in Table 2. For all of these tests, the null hypothesis
is that a similar fraction of the respondents chose \Yes" for
both options. The results show that the concerns about data
breaches and data sharing are signicantly higher than data
aggregation and data distortion ( p1:231e 12).
Hypothesis 1 : People are more concerned about the
security aspects of privacy, in particular, about data sharing
and data breaches than data distortion and data aggregation.
Forreducing concerns, respondents consider technical de-
tails the least eective option (with p-values ranging from
7:218e 10for comparing to policy to 2 :2e 16for comparing
to anonymization). Respondents think that anonymization is
the most eective option for mitigating privacy concerns and
signicantly better than privacy laws ( p= 0:003) and privacy
policy ( p= 0:002). There is, however, no statistically signi-
cant dierence between anonymization and providing usage
details ( p >0:15). The remaining three options (privacy pol-
icy, privacy laws, and usage details) had similar responses and
none of their combinations for the Z-test yielded statistically
signicant results for p <0:01.
Hypothesis 2 : There is less agreement on the best mea-
sures for mitigating privacy concerns.
3.2 Qualitative Feedback
From the 408 respondents, we collected 135 comments on
our open questions about additional privacy concerns and
measures to reduce them. We analyzed these comments
manually in three steps. First, we read each comment andannotated it with a few keywords. Thereby, we tried to
reuse the keywords whenever possible. Second, we unied
and grouped the keywords into topics, making sure that no
important comments are lost. Finally, we read the comments
again and assigned each of them to the identied topics.
3.2.1 Additional Privacy Concerns
We collected 66 comments on additional privacy concerns.
15 comments were useless as they just repeated the standard
response options, were not understandable, or without con-
tent (e.g., \no comment", or \nothing more"). The remaining
51 comments gave interesting insights, which can be grouped
into the following topics:
Authorities and intelligent services : 13 respondents
mentioned authorities and intelligent services as an addi-
tional privacy concern. One wrote: \Government access is
not exactly a data breach, but still a privacy concern". An-
other commented: \anyway there is prism". It is important
to mention that about half of the responses were collected
after the NSA PRISM scandal [14,16].
APIs, program correctness, and viruses : Nine respon-
dents mentioned concerns related to the program behavior,
including malicious programs and viruses. Respondents also
mentioned that privacy concerns are \transmitted" through
the application programming interfaces of the tools collecting
data. One respondent wrote: \Sharing data over API" while
others mentioned specic systems such as Google Analytics
or Facebook API. Three respondents specically pointed the
correctness of privacy implementation as a specic concern.
Unusable and nontransparent policies : Seven users
complained about unusable privacy implementations with
unclear, nontransparent policies. These respondents were
concerned because most users simply do not know which
data is being collected about them and for what purposes.
One respondent wrote: \Companies and software developers
shield themselves [. . . ] by making consumers agree on a
long, convoluted, and often a hard to understand, hard to
read [. . . ] policy. Companies know that people do not read
them a tactic on which they are banking". Another gave a
more concrete example: \Sometimes sharing sensitive data862is activated by default in applications (unaware users would
leave it so)". One respondent wrote: \Transparency and
letting the user choose make a huge dierence. Maybe not
in the beginning and maybe not for all users but denitely
for a specic user group".
Intentional or unintentional misuse : At least seven re-
spondents mentioned dierent forms of misusing the data
as main concerns. This includes commercial misuse such as
making products of interest more expensive, but it could
also be misused for social and political purposes. Apart
from abusing the data to put pressure on users, respondents
mentioned using fake data to manipulate public opinions or
inferencing sensitive information about groups of people and
minorities. One respondent wrote: \Whenever something
happen [sic] the media uses their data accessible online to
`sell' this person as good or evil".
Lack of control : Seven respondents mentioned the lack of
control and in particular, options to delete data collected
about them as their main concern. One wrote: \if we agree
to give the data, we are not able anymore to revise this
decision and delete the data. Even if the service conrms
the deletion, we don't have any mean of control". Another
respondent explicitly mentioned the case where companies
owning their data are bankrupt or sold and in this case, the
privacy of their data is also lost: \Company A has a decent
privacy policy, Company B acquires the company, and in
doing so, now has access to Company A's data".
Combined data sources : Five respondents explicitly men-
tioned combining data about users from dierent sources
as a main privacy concern. In most cases, this cannot be
anticipated when developing or using a single system or a
service. One respondent wrote: \It's dicult to anticipate or
assess the privacy risk in this case". Another claimed: \Con-
tinuous monitoring combined with aggregation over multiple
channels or sources leads to complex user proling. It's dis-
turbing to know that your life is monitored on so many levels".
Collecting and storing data : Five respondents wrote that
collecting and storing data is, on its own, a privacy concern.
In particular, respondents complained about too much data
being collected about them and stored for too long time. One
respondent mentioned: \The sheer amount of cookies that
are placed on my computer just by landing on their website".
Another claimed: \Collecting the data and storing for a long
period of time is seen more critical than just collecting".
Other issues : Three respondents mentioned problems with
the legal framework and in particular, the compatibility of
laws in the developer and user countries. Three respondents
said that in some cases there is no real option to not use a
system or service, e.g., due to a \social pressure as all use
Facebook" or since \we depend on technology".
3.2.2 Suggestions for Reducing Privacy Concerns
In total, 69 respondents answered the open question on
additional measures to reduce user concerns about privacy.
Ten of these answers either repeated the standard options or
were useless. The remaining 59 comments showed more con-
vergence in the opinion than the comments on the additional
concerns, possibly because this question was more concrete.The suggestions can be grouped into the following measures:
Easy and ne-grained control over the data, includ-
ing access and deletion : 17 respondents recommended
allowing the users to easily access and control the collected
and processed data about them. In particular, respondents
mentioned the options of deactivating the collection and
deleting the data. One respondent wrote: \To alleviate pri-
vacy concerns, it should be possible to opt out of or disagree
with certain terms". Another wrote: \Allow users to access
a summary of all the data stored on their behalf, and allow
them to delete all or part of it if they desire". The respon-
dents also highlighted that this should be simple and easy to
do and embedded into the user interface at the data level.
Certication from independent trusted organizations :
14 respondents suggested introducing a privacy certication
mechanism by independent trusted authorities. A few also
suggested the continuous conduction of privacy audits similar
to other elds such as safety and banking. Respondents also
suggested that the results of the checks and audits should
be made public to increase the pressure on software vendors.
One respondent even suggested \having a privacy police to
check on how data is handled".
Transparency and risk communication, open source :
13 respondents mentioned increased transparency about the
collection, aggregation, and sharing of the data. In par-
ticular, respondents mentioned that the risks of misusing
the data should be also communicated clearly and continu-
ously. Three respondents suggested that making the code
open source would be the best approach for transparency.
One wrote: \tell users (maybe in the side-bar) how they are
being tracked. This would educate the general public and
ideally enable them to take control of their own data". The
spectrum of transparency was from the data being collected
to physical safety measures of servers and qualications of
people handling data to how long the data is stored.
Period and amount of data : 11 respondents recom-
mended always limiting and minimizing the amount of data
and the period of storage, referring to the principle of min-
imality. The period of time for storing the data seems to
be crucial for users. One wrote: \Not allowing users data
being stored in servers. Just maintaining them in the source".
Security and encryption : We noted that respondents
strongly relate privacy issues to information security. At
least seven suggested security measures, mainly complete
encryption of data and communication channels.
Trust and education : Seven respondents mentioned build-
ing trust in the system and vendor as well as education of
users on privacy as eective means to reduce privacy concerns.
Short, usable, precise and understandable descrip-
tion, in the UI : At least six respondents mentioned increas-
ing the usability to access data and policy as an important
measure to reduce privacy concerns. One wrote: \the dis-
claimer should be directly accessible from the user interface
when conducting a function which needs my data". Another
respondent wrote: \short understandable description and no
long complicated legal text".863# responsesMetadataInteractionPreferencesLocationPersonal DataContent
200 100 0 100MetadataInteractionPreferencesLocationPersonal DataContent
200 100 0 100 200 100 0 100 
Very Critical Critical Neutral Somewhat Uncritical UncriticalFigure 2: How critical would you rate the collection of the following data?
3.3 Criticality of Different Types of Data
To get a deeper insight into the privacy criticality of dier-
ent types of data, we asked respondents to rate the following
types of data on a 5-point semantic scale ranging from \Very
critical" to \Uncritical".
Content of documents (such as email body)
Metadata (such as date)
Interaction (such as a mouse click to open or send an
email)
User location (such as the city from where the email
was sent)
Name or personal data (such as email address)
User preferences (such as inbox or email settings)
The results are shown in Figure 2. Respondents chose
content as most critical, followed by personal data, loca-
tion, preferences, and interaction and metadata are the least
critical as far as privacy is concerned.
We used Welch's Two Sample t-test to compare if the
dierence among the dierent types of data is statistically
signicant. The null hypothesis was that the dierence in
means was equal to zero. Table 3 summarizes the results. It
shows, for example, that there is no statistically signicant
dierence between content and personal data. On the other
hand, there is a statistically signicant dierence between
content and location for p <0:01.
Hypothesis 3 : People are more concerned about content
and personal data than interaction and metadata.
3.4 Giving up Privacy
We asked respondents if they would accept lessprivacy
for the following:
Monetary discounts (e.g., 10% discount on the next
purchase)
\Intelligent" or added functionality of the system (such
as the Amazon recommendations)
Fewer advertisements
For each option, the respondents could answer using a
3-point semantic scale having options: \Yes", \Uncertain",
and \No". The results are shown in Figure 3.
# responsesAdsMoneyFunctionality
200 100 0 100# responses200 100 0 100# responses 
No Uncertain YesFigure 3: Would users accept lessprivacy for the
following?
36.7% of the respondents said they would accept less pri-
vacy for added functionality of the system while only 20.7%
and 13.7% would accept less privacy for monetary discounts
and fewer advertisements respectively. Added functionality
seems to be the most important reason to accept less privacy.
These results are statistically signicant using the Z-test for
equality of proportions ( p <3:882e 5for monetary discounts
andp <1:854e 9for fewer advertisements). It is important
to note that less than half of the respondents would accept
less privacy for added functionality of the system.
Previous studies, such as the one conducted by Acquisti et
al. [1], have shown, however, that people's economic valua-
tions of privacy vary signicantly and that people doaccept
less privacy for monetary discounts. This contrast in results
might be due to a dierence between people's opinion and
their actual behavior.
Hypothesis 4 : There are dierent groups of opinions
about accepting less privacy for certain benets. The largest
group of users say that they are not inclined to give up privacy
for additional benets. However, their actual behavior might
be dierent.
4. DEVELOPER VS USER PERCEPTIONS
The results from the previous section describe the broad
concerns for all respondents of our survey. In this section, we
report on the important results from a dierential analysis
between two groups of respondents: developers (267 out of
408) versus users of software systems (141 out of 408). We
used Z-tests for equality of proportions for the rest of this
section, unless otherwise noted.864Table 3: The signicance in the dierence between the criticality of collecting dierent data. p-values: ` + + + '
forp < e 11, `++' for p < e 6, `+' for p <0:01, and ` ' for p >0:01.
The rows and columns are ordered from most to least critical. For each cell, t-tests compare if the dierence in criticality is
statistically signicant. For example, the dierence between interaction and content is statistically signicant for p < e 11.
Content Personal Data Location Preferences Interaction Metadata
Content {
Personal Data {
Location + + {
Preferences +++ ++ + {
Interaction +++ +++ ++ + {
Metadata +++ +++ +++ ++ + {
4.1 Privacy Concerns
Data distortion : 49.1% of developers believe that data
distortion is an important privacy concern. The percentage
of users, on the other hand, is 64.5%. The dierence between
these two groups is statistically signicant ( p= 0:003).
Data aggregation : 52.1% of developers believe that data
aggregation is an important privacy concern. The percentage
of users, on the other hand, is 63.1%. The dierence between
them is statistically signicant ( p= 0:04185). It seems that
developers trust their systems more than users when it comes
to wrong interpretation of sensitive data.
Data criticality : Developers believe that \name and per-
sonal data" ( p= 0:038) and \interaction" ( p= 0:082) are
more critical for privacy compared to users. On the other
hand, for the remaining four categories (content, location,
preferences, metadata), there is no statistically signicant
dierence between the perceptions of developers and users
(p >0:2 for all). We used Welch's Two Sample t-test here.
Less privacy for added functionality : A larger fraction
of developers (43.3%) would accept less privacy for added or
intelligent functionality of the system compared to 31.2% of
users ( p= 0:002).
Hypothesis 5 : Developers are more concerned about
interaction, name, and personal data whereas users are more
concerned about data distortion and data aggregation.
4.2 Measures to Reduce Concerns
Developers and reducing concerns : A larger fraction of
developers (71.2%) feel that data anonymization is a better
option to reduce privacy concerns as compared to privacy
policies or privacy laws (both, 56.9%) ( p= 0:0006). 66.3% of
developers prefer providing details on data usage for mitigat-
ing privacy concerns compared to privacy policies (56.9%)
(p= 0:03).
Similarly, 20.2% of developers feel that privacy policies will
notreduce privacy concerns whereas only 11.2% feel that pro-
viding details on data usage will notbe benecial ( p= 0:004).
Users and reducing concerns : In contrast, for users,
there is no statistically signicant dierence between their
perception on privacy policies, laws, anonymization, and
providing usage details. (0 :6< pfor all combinations).
Hypothesis 6 : Developers prefer anonymization and pro-
viding usage details as measures to reduce privacy concerns.
Users, on the other hand, do not have a strong preference.5. THE ROLE OF GEOGRAPHY
In this section, we present the results of the dierential
analysis based on the geography of respondents. We asked
respondents to specify with which region of the world they
identify themselves. The options were North America, South
America, Europe, Asia/Pacic, Africa, and other. Since we
have only seven responses from South America and Africa
combined, we focus on the dierences between the others.
We used the Z-test for equality of proportions for the rest of
this section, unless otherwise noted.
Data criticality : We asked respondents to rate the criti-
cality of the dierent types of data for privacy (i.e., content
of documents, metadata, interaction, user location, name or
personal data, user preferences) using a semantic scale from
1{5, with 1 being \Very Critical" and 5 being \Uncritical".
There is a statistically signicant dierence between re-
spondents from North America, Europe, and Asia/Pacic.
We used Welch's Two Sample t-test to compare the ratings
given by respondents. Respondents in North America think
that all types of data are less critical (overall mean across
the six types of data is 2.31) than respondents in Europe
(overall mean is 1.87) for p= 3:144e 8.
Similarly, respondents from North America think all items
are less critical that those in Asia/Pacic (overall mean: 2.01)
forp= 0:037. On the other hand, there is no statistically
signicant dierence between respondents in Europe and
Asia/Pacic ( p >0:28).
Less privacy for added functionality : A larger fraction
of respondents in Europe (50.6%) claim that they would not
give up privacy for added functionality. In North America, on
the other hand, this fraction is 24.1%. The dierence between
the two regions is statistically signicant ( p= 0:0001).
Hypothesis 7 : People from North America are more will-
ing to give up privacy and feel that dierent types of data
are less critical for privacy compared to people from Europe.
Concerns about data sharing versus data distortion :
A larger fraction of respondents in North America (88.9%)
feel that data sharing is a concern compared to 46.3% for
data distortion ( p= 6:093e 6). On the other hand, there is
no statistically signicant dierence among respondents in
Asia/Pacic ( p >0:67).
Concerns about data sharing versus data breach :
In Europe, a larger fraction of the respondents (94.3%)
are concerned about data breaches as compared to 76.4%865for data sharing. The dierence is statistically signicant
(p= 5:435e 6). On the other hand, there is no statistically
signicant dierence among respondents in North America
(p >0:12).
Laws versus usage details : In Europe, a larger fraction
of respondents (75.9%) feel that providing details on how
the data is being used will reduce privacy concerns as op-
posed to 58.1% who feel that privacy laws will be eective
(p= 0:00063 ). On the other hand, there is no statistically
signicant dierence among respondents in North America,
where the percentage of respondents are 67.9% and 64.2%
respectively ( p >0:43).
Usage details versus privacy policies : A larger fraction
of respondents in Europe (75.9%) feel that providing usage
details can mitigate privacy concerns compared to 63.2% for
using a privacy policy ( p= 0:015). On the other hand, there
is no statistically signicant dierence among respondents in
North America ( p >0:32).
Hypothesis 8 : People from Europe feel that providing
usage details can be more eective for mitigating privacy
concerns than privacy laws and privacy policies whereas
people from North America feel that these three options are
equally eective.
6. DISCUSSION
We discuss our results, potential reasons, and the implica-
tions for software developers and analysts. We also reect
on the limitations and threats to validity of our results.
6.1 Privacy Interpretation Gaps
Data privacy is often an implicit requirement: everyone
talks about it but no one species what it means and how it
should be implemented. This topic also attracts the interests
of dierent stakeholders including users, lawyers, sales people,
and security experts, which makes it even harder to dene
and implement. One important result from our study is that
while almost all respondents agree about the importance
of privacy, the understanding of the privacy issues and the
measures to reduce privacy concerns are divergent. This calls
for an even more careful and distinguished analysis of privacy
when designing and building a system.
Our results from Sections 4 and 5 show there is a denite
gap in privacy expectations and needs between users and
developers and between people from dierent regions of the
world. Developers have assumptions about privacy, which
do not always correspond to what users need. Developers
seem to be less concerned about data distortion and aggrega-
tion compared to users. It seems that developers trust their
systems more than users when it comes to wrong interpreta-
tion of privacy critical data. Unlike users, developers prefer
anonymization and providing usage details for mitigating
privacy concerns. If the expectations and needs of users do
not match those of developers, developers might have wrong
assumptions and might end up making wrong decisions when
designing and building privacy-aware software systems.
In addition, privacy is not a universal requirement as it
appears to have an internationalization aspect to it. Dierent
regions seem to have dierent concrete requirements and
understanding for privacy. Our results conrm that there
exist many cultural dierences between various regions of theworld as far as privacy is concerned. The recent NSA PRISM
scandal has also brought these dierences into sharp focus. A
majority of Americans considered NSA's accessing personal
data to prevent terrorist threats more important that privacy
concerns [14]. In contrast, there was widespread \outrage" in
Europe over these incidents [16]. It also led to an article in
the New York Times by Malte Spitz, a member of the German
Green Party's executive committee, titled \Germans Loved
Obama. Now We Don't Trust Him" [39]. These dierences,
both in terms of laws and people's perceptions, should be
considered carefully when designing and deploying software
systems.
We think that privacy should become an explicit require-
ment, with measurable and testable criteria. We also think
that privacy should also become a main design criteria for
developers as software systems are collecting more and more
data about their users [15]. To this end, we feel that there is
a need to develop a standard survey for privacy that software
teams can customize and reuse for their projects and users.
Our survey can be reused to conduct additional user studies
on privacy for specic systems. Our results can also serve as
a benchmark for comparing the data. This can help build
a body of knowledge and provide guidelines such as best
practices.
6.2 The Security Dimension of Privacy
We think that people are more concerned about data
breaches and data sharing as there have been many recent
instances that have received a lot of media coverage. To list
a few recent examples, Sony suered a massive data breach
in its Playstation network that led to the theft of personal
information belonging to 77 million users [6]. One hundred
and sixty million credit card numbers were stolen and sold
from various companies including Citibank, the Nasdaq stock
exchange, and Carrefour [36]. The Federal Trade Commission
publicly lent support to the \Do-Not-Track" system for adver-
tising [4]. Compared to these high-prole stories, we feel that
there have been relatively few \famous" instances of privacy
problems caused by data aggregation or data distortion yet.
There is a large body of research that has advanced the
state-of-the-art in security (encryption) and authorization.
One short-term implication for engineers and managers is to
systematically implement security solutions when designing
and deploying systems that collect user data, even if it is not
a commercially or politically sensitive system. This would
signicantly and immediately reduce privacy concerns. For
the medium-term, more research should be conducted for
deployable data aggregation and data distortion solutions.
As far as mitigating privacy concerns, our results show
that there is more disagreement. We believe that the reason
for this is that online privacy concerns are a relatively recent
phenomenon. Due to this, people are not sure which approach
works best and might be benecial in the long run.
6.3 Privacy Framework
We feel that an important direction for software and re-
quirements engineering researchers is to develop a universal,
empirically grounded framework for collecting, analyzing,
and implementing privacy requirements. This study is the
rst building block towards such a framework. Some of the
lessons learned from our study can be translated into con-
crete qualities and features, which should be part of such a
framework. This includes:866Anonymization : This is perhaps the most well-known
privacy mitigating technique and seems to be perceived
as an important and eective measure by both users
and developers. Developers should therefore use anony-
mization algorithms and libraries.
Data usage : Although anonymization is perceived
as the most eective measure for addressing privacy
concerns, this is currently not practical as approaches
like dierential privacy are computationally infeasible
[30, 45]. In such situations, it is better to provide
users with data usage details and make these more
transparent and easier to understand. Our ndings
show that there is no statistical dierence between
anonymization and providing usage details as far as
users are concerned. Thus, in terms of future research,
it is perhaps better to focus on improving techniques for
providing data usage details rather than (or in addition
to) making anonymization computationally feasible.
Default encryption : As users are mainly concerned
about the loss and abuse of their data, systems collect-
ing user data should implement and activate encryption
mechanism for storing and transmitting these data. In
Facebook, e.g., the default standard communication
protocol should be HTTPS and not HTTP.
Fine-grained control over the data : Users become
less concerned about privacy if the system provides a
mechanism to control their data. This includes acti-
vating and deactivating the collection at any time, the
possibility to access and delete the raw and processed
data, and dene who should have access to what data.
Interaction data rst : Users have a rather clear pref-
erence of the criticality of the dierent types of data
collected about them. Therefore, software researchers
and designers should rst try to implement their sys-
tems based on collecting and mining interaction data
instead of content of les and documents. Research has
advanced a lot in this eld in, especially, recommender
systems [25].
Time and space-limited storage : The storage of
data about users should be limited in time and space.
The location where the data is stored is an important
factor for many respondents. Therefore, systems should
provide options for choosing the location of storing
privacy sensitive data.
Privacy policies, laws, and usage details : Users
rated all these options as equally eective for mitigating
their privacy concerns. Therefore, developers could
utilize any of these options, thus giving them better
exibility in the design and deployment of software
systems.
6.4 Limitations and Threats to Validity
There are several limitations to our study, which we discuss
in this section. The rst limitation is a potential selection
bias. Respondents who volunteered to ll out our survey
were self-selected. Such selection bias implies that our results
are only applicable to the volunteering population and may
not necessarily generalize to other populations. The sum-
maries have helped us identify certain trends and hypotheses
and these should be validated and tested by representative
samples, e.g., for certain countries. In contrast, the dieren-
tial analysis (also called pseudo-experimentation) conductedwithin our set of respondents, enabled us to identify sta-
tistically signicant relationships and correlations. Hence,
many of our results deliberately focus on correlations and
cross-tabulations between dierent populations.
As for internal validity, we are aware that by lling out
a brief survey, we can only understand a limited amount
of concerns that the respondents have in mind. Similarly,
the format and questions of the survey might constrain the
expressiveness of some of the respondents. We might have
missed certain privacy concerns and measures to reduce
concerns by the design of the survey. We tried to mitigate
this risk by providing open-ended questions that respondents
could use to express additional aspects they had in mind.
Moreover, we designed the survey in a highly iterative process
and tested it in dry runs to ensure that all options are
understandable and that we did not miss any obvious option.
As with any online survey, there is a possibility that re-
spondents did not fully understand the question or chose
the response options arbitrarily. We conducted several pilot
tests, gave the option to input comments, and the incomple-
tion rate is relatively small. We included a few validation
questions and we only report responses in this paper from
respondents who answered these questions correctly. We also
provided two versions of the survey, in English and German,
to make it easier for non-native speakers.
In spite of these limitations, we managed to get a large
and diverse population that lled out our survey. This gives
us condence about the overall trends reported in this paper.
7. RELATED WORK
There has been a lot of research about privacy and secu-
rity in dierent research communities. We summarize the
important related work focussing on usability and economic
aspects of privacy, anonymization techniques, and work from
the software and requirements engineering community.
Many recent studies on online social networks show that
there is a (typically, large) discrepancy between users' in-
tentions for what their privacy settings should be versus
what they actually are. For example, Madejski et al. [24,27]
report in their study of Facebook that 94% of their partici-
pants ( n= 65) were sharing something they intended to hide
and 85% were hiding something that they intended to share.
Liu et al. [24] found that Facebook's users' privacy settings
match their expectations only 37% of the time. A recent
longitudinal study by Stutzman et al. [42] shows how privacy
settings for Facebook users have evolved over a period of
time. These studies have focused on privacy settings in a
specic online system whereas our study was designed to be
agnostic to any modern system collecting user sensitive data.
Further, the main contribution of these studies is to show
that there is a discrepancy between what the settings are
and what they should be and how settings evolve over time.
Our study aims to gain a deeper understanding of what the
requirements are and how they change across geography and
depending on software development experience.
Fang and LeFevre [18] proposed an automated technique
for conguring a user's privacy settings in online social net-
working sites. Paul et al. [34] present using a color coding
scheme for making privacy settings more usable. Squicciarini,
Shehab, and Paci [41] propose a game-theoretic approach
for collaborative sharing and control of images in a social
network. Toubiana et al. [46] present a system that auto-
matically applies users' privacy settings for photo tagging.867All these papers propose new approaches to make privacy
settings \better" from a user's perspective (i.e., more us-
able and more visible). Our results help development teams
decide when and which of these techniques should be im-
plemented. We focus more on a broader requirements and
engineering perspective of privacy than on a specic technical
perspective.
There has been a lot of recent work on the economic
ramications of privacy. For example, Acquisti et al. [1]
(and the references therein) conducted a number of eld and
online experiments to investigate the economic valuations of
privacy. In Section 3.4, we discussed whether users would
give up privacy for additional benets like discounts or fewer
advertisements. Our study complements and contrasts the
work of Acquisti et al. as described earlier.
There has also been a lot of work about data anonymization
and building accurate data models for statistical use (e.g.,
[2,17,23,35,49]). These techniques aim to preserve certain
properties of the data (e.g., statistical properties like average)
so they can be useful in data mining while trying to preserve
privacy of individual records. Similarly, there has also been
work on anonymizing social networks [8] and anonymizing
user proles for personalized web search [52]. The broad
approaches include aggregating data to a higher level of
granularity or adding noise and random perturbations. There
has been research on breaking the anonymity of data as well.
Narayanan and Shmatikov [32] show how it is possible to
correlate public IMDb data with private anonymized Netix
movie rating data resulting in the potential identication
of the anonymized individuals. Backstrom et al. [5] and
Wondracek et al. [51] describe a series of attacks for de-
anonymizing social networks.
Also in the software engineering community, recent papers
on privacy mainly focused on data anonymization techniques.
Clause and Orso [13] propose techniques for the automated
anonymization of eld data for software testing. They extend
the work done by Castro et al. [12] using novel concepts of
path condition relaxation and breakable input conditions re-
sulting in improving the eectiveness of input anonymization.
Taneja et al. [44] and Grechanik et al. [20] propose using k-
anonymity [43] for privacy by selectively anonymizing certain
attributes of a database for software testing. They propose
novel approaches using static analysis for selecting which
attributes to anonymize so that test coverage remains high.
Our work complements these papers as respondents in our
study considered anonymization an eective technique for
mitigating privacy concerns and these techniques could be
used as part of a privacy framework.
There have been some recent papers on extracting privacy
requirements from privacy regulations and laws [9,10]. These
could be part of the privacy framework as well and help in
reducing the impact due to cultural dierences for privacy.
While this work focus on legal requirements, we focus on the
users' understanding of privacy and how it diers from devel-
opers' views. A few recent papers have also discussed privacy
requirements, mainly in the context of mobile applications.
Mancini et al. [28] conducted a eld study to evaluate the im-
pact of privacy and location tracking on social relationships.
Tun et al. [47] introduce a novel approach called \privacy
arguments" and use it to represent and analyze privacy re-
quirements in mobile applications. Omoronyia et al. [33]
propose an adaptive framework using privacy aware require-
ments, which will satisfy runtime privacy properties. Ourfocus is broader than the rst two papers as we don't limit
our scope to mobile applications; nonetheless, many of our
ndings would apply directly. Our work is complementary
to the last paper where our ndings could be used as part of
the adaptive framework.
Finally, many authors in the software engineering and
requirements engineering communities mention privacy in
the discussion or challenges section of their papers (e.g.,
[11, 25, 26, 31]. But in most cases, there is little evidence
and grounded theory about what, how, and in which con-
text privacy concerns exist and what the best measures for
addressing them are. Our study helps in clarifying these
concerns and measures as well as comparing the dierent
perceptions of people.
8. CONCLUSION
In this paper, we conducted a study to explore the privacy
requirements for users and developers in modern software
systems, such as Amazon and Facebook, that collect and
store data about the user. Our study consisted of 408 valid
responses representing a broad spectrum of respondents:
people with and without software development experience
and people from North America, Europe, and Asia. While the
broad majority of respondents (more than 91%) agreed about
the importance of privacy as a main issue for modern software
systems, there was disagreement concerning the concrete
importance of dierent privacy concerns and the measures to
address them. The biggest concerns about privacy were data
breaches and data sharing. Users were more concerned about
data aggregation and data distortion than developers. As far
as mitigating privacy concerns, there was little consensus on
the best measure among users. In terms of data criticality,
respondents rated content of documents and personal data
as most critical versus metadata and interaction data as least
critical.
We also identied dierence in privacy perceptions based
on the geographic location of the respondent. Respondents
from North America, for example, consider all types of data
as less critical for privacy than respondents from Europe or
Asia/Pacic. Respondents from Europe are more concerned
about data breaches than data sharing whereas respondents
from North America are equally concerned about the two.
Finally, we gave some insight into a framework and a set
of guidelines on privacy requirements for developers when
designing and building software systems. This is an impor-
tant direction for future research and our results can help
establish such a framework, which can be a catalog of pri-
vacy concerns and measures, a questionnaire to assess and
ne-tune them, and perhaps a library of reusable privacy
components.
9. ACKNOWLEDGMENTS
We would like to thank all the respondents for lling out
our online survey. We would also like to thank Timo Johann,
Mathias Ellman, Zijad Kurtanovic, Rebecca Tiarks, and
Zardosht Hodaie for help with the translations. Sheth and
Kaiser are members of the Programming Systems Laboratory
and are funded in part by NSF CCF-1302269, NSF CCF-
1161079, NSF CNS-0905246, NIH 2 U54 CA121852-06. This
work is a part of the EU research project MUSES (grant
FP7-318508).86810. REFERENCES
[1] A. Acquisti, L. John, and G. Loewenstein. What is
privacy worth? In Workshop on Information Systems
and Economics (WISE) , 2009.
[2] D. Agrawal and C. C. Aggarwal. On the design and
quantication of privacy preserving data mining
algorithms. In PODS '01: Proceedings of the twentieth
ACM SIGMOD-SIGACT-SIGART symposium on
Principles of database systems , pages 247{255, New
York, NY, USA, 2001. ACM.
[3] T. Anderson and H. Kanuka. E-research: Methods,
strategies, and issues. 2003.
[4] J. Angwin and J. Valentino-Devries. FTC Backs
Do-Not-Track System for Web.
http://online.wsj.com/article/
SB10001424052748704594804575648670826747094.
html, December 2010.
[5] L. Backstrom, C. Dwork, and J. Kleinberg. Wherefore
art thou r3579x?: anonymized social networks, hidden
patterns, and structural steganography. In WWW '07:
Proceedings of the 16th international conference on
World Wide Web , pages 181{190, New York, NY, USA,
2007. ACM.
[6] L. B. Baker and J. Finkle. Sony PlayStation suers
massive data breach.
http://www.reuters.com/article/2011/04/26/us-
sony-stoldendata-idUSTRE73P6WB20110426 , April
2011.
[7] M. Barbaro, T. Zeller, and S. Hansell. A face is
exposed for AOL searcher no. 4417749.
http://www.nytimes.com/2006/08/09/technology/
09aol.html?_r=1 , August 2006.
[8] S. Bhagat, G. Cormode, B. Krishnamurthy, and
D. Srivastava. Privacy in dynamic social networks. In
Proceedings of the 19th international conference on
World wide web , WWW '10, pages 1059{1060, New
York, NY, USA, 2010. ACM.
[9] T. D. Breaux and A. I. Anton. Analyzing regulatory
rules for privacy and security requirements. IEEE
Transactions on Software Engineering , 34(1):5{20,
2008.
[10] T. D. Breaux and A. Rao. Formal analysis of privacy
requirements specications for multi-tier applications.
InRE'13: Proceedings of the 21st IEEE International
Requirements Engineering Conference (RE'13) ,
Washington, DC, USA, July 2013. IEEE Society Press.
[11]R. P. L. Buse and T. Zimmermann. Information Needs
for Software Development Analytics. In Proceedings of
the 2012 International Conference on Software
Engineering , ICSE 2012, pages 987{996, Piscataway,
NJ, USA, 2012. IEEE Press.
[12] M. Castro, M. Costa, and J.-P. Martin. Better bug
reporting with better privacy. In Proceedings of the
13th international conference on Architectural support
for programming languages and operating systems ,
ASPLOS XIII, pages 319{328, New York, NY, USA,
2008. ACM.
[13] J. Clause and A. Orso. Camouage: automated
anonymization of eld data. In Proceeding of the 33rd
international conference on Software engineering , ICSE
'11, pages 21{30, New York, NY, USA, 2011. ACM.[14] J. Cohen. Most Americans back NSA tracking phone
records, prioritize probes over privacy.
http://www.washingtonpost.com/politics/most-
americans-support-nsa-tracking-phone-records-
prioritize-investigations-over-
privacy/2013/06/10/51e721d6-d204-11e2-9f1a-
1a7cdee20287_story.html , June 2013.
[15] L. F. Cranor and N. Sadeh. A shortage of privacy
engineers. Security & Privacy, IEEE , 11(2):77{79, 2013.
[16] S. Erlanger. Outrage in Europe Grows Over Spying
Disclosures. http://www.nytimes.com/2013/07/02/
world/europe/france-and-germany-piqued-over-
spying-scandal.html , July 2013.
[17] A. Evmievski, J. Gehrke, and R. Srikant. Limiting
privacy breaches in privacy preserving data mining. In
PODS '03: Proceedings of the twenty-second ACM
SIGMOD-SIGACT-SIGART symposium on Principles
of database systems , pages 211{222, New York, NY,
USA, 2003. ACM.
[18] L. Fang and K. LeFevre. Privacy wizards for social
networking sites. In Proceedings of the 19th
international conference on World wide web , WWW
'10, pages 351{360, New York, NY, USA, 2010. ACM.
[19] D. Fletcher. How Facebook Is Redening Privacy.
http://www.time.com/time/business/article/0,
8599,1990582.html , May 2010.
[20] M. Grechanik, C. Csallner, C. Fu, and Q. Xie. Is data
privacy always good for software testing? Software
Reliability Engineering, International Symposium on ,
0:368{377, 2010.
[21] S. Grobart. The Facebook Scare That Wasn't.
http://gadgetwise.blogs.nytimes.com/2011/08/10/
the-facebook-scare-that-wasnt/ , August 2011.
[22] J. Jacoby and M. S. Matell. Three-point likert scales
are good enough. Journal of Marketing Research ,
8(4):pp. 495{500, 1971.
[23]N. Lathia, S. Hailes, and L. Capra. Private distributed
collaborative ltering using estimated concordance
measures. In RecSys '07: Proceedings of the 2007 ACM
conference on Recommender systems , pages 1{8, New
York, NY, USA, 2007. ACM.
[24] Y. Liu, K. P. Gummadi, B. Krishnamurthy, and
A. Mislove. Analyzing facebook privacy settings: user
expectations vs. reality. In Proc. of the 2011
SIGCOMM Conf. on Internet measurement conf. ,
pages 61{70, 2011.
[25] W. Maalej, T. Fritz, and R. Robbes. Collecting and
processing interaction data for recommendation
systems. In M. Robillard, M. Maalej, R. Walker, and
T. Zimmerman, editors, Recommendation Systems in
Software Engineering , pages 173{197. Springer, 2014.
[26] W. Maalej and D. Pagano. On the socialness of
software. In Proceedings of the International Software
on Social Computing and its Applications . IEEE
Computer Society, 2011.
[27]M. Madejski, M. Johnson, and S. M. Bellovin. A study
of privacy settings errors in an online social network.
Pervasive Computing and Comm. Workshops, IEEE
Intl. Conf. on , 0:340{345, 2012.
[28] C. Mancini, Y. Rogers, K. Thomas, A. N. Joinson,
B. A. Price, A. K. Bandara, L. Jedrzejczyk, and
B. Nuseibeh. In the best families: Tracking and869relationships. In Proceedings of the SIGCHI Conference
on Human Factors in Computing Systems , CHI '11,
pages 2419{2428, New York, NY, USA, 2011. ACM.
[29]C. C. Miller. Privacy Ocials Worldwide Press Google
About Glass. http:
//bits.blogs.nytimes.com/2013/06/19/privacy-
officials-worldwide-press-google-about-glass/ ,
June 2013.
[30] I. Mironov, O. Pandey, O. Reingold, and S. Vadhan.
Computational dierential privacy. In Advances in
Cryptology-CRYPTO 2009 , pages 126{142. Springer,
2009.
[31]H. Muccini, A. Di Francesco, and P. Esposito. Software
testing of mobile applications: Challenges and future
research directions. In Automation of Software Test
(AST), 2012 7th International Workshop on , pages
29{35, June 2012.
[32] A. Narayanan and V. Shmatikov. How to break
anonymity of the netix prize dataset. CoRR ,
abs/cs/0610105, 2006.
[33] I. Omoronyia, L. Cavallaro, M. Salehie, L. Pasquale,
and B. Nuseibeh. Engineering adaptive privacy: On the
role of privacy awareness requirements. In Proceedings
of the 2013 International Conference on Software
Engineering , ICSE '13, pages 632{641, Piscataway, NJ,
USA, 2013. IEEE Press.
[34] T. Paul, M. Stopczynski, D. Puscher, M. Volkamer,
and T. Strufe. C4ps: colors for privacy settings. In
Proceedings of the 21st international conference
companion on World Wide Web , WWW '12
Companion, pages 585{586, New York, NY, USA, 2012.
ACM.
[35] H. Polat and W. Du. Privacy-preserving collaborative
ltering using randomized perturbation techniques. In
Data Mining, 2003. ICDM 2003. Third IEEE
International Conference on , pages 625{628, Nov. 2003.
[36] N. Popper and S. Sengupta. U.S. Says Ring Stole 160
Million Credit Card Numbers.
http://dealbook.nytimes.com/2013/07/25/arrests-
planned-in-hacking-of-financial-companies/ ,
July 2013.
[37] R. L. Rosnow and R. Rosenthal. Beginning behavioral
research: A conceptual primer . Prentice-Hall, Inc,
1996.
[38] D. J. Solove. A Taxonomy of Privacy. University of
Pennsylvania Law Review , pages 477{564, 2006.
[39]M. Spitz. Germans Loved Obama. Now We Don't Trust
Him. http://www.nytimes.com/2013/06/30/opinion/
sunday/germans-loved-obama-now-we-dont-trust-
him.html , June 2013.
[40] R. C. Sprinthall and S. T. Fisk. Basic statistical
analysis . Prentice Hall Englewood Clis, NJ, 1990.[41] A. C. Squicciarini, M. Shehab, and F. Paci. Collective
privacy management in social networks. In Proceedings
of the 18th international conference on World wide web ,
WWW '09, pages 521{530, New York, NY, USA, 2009.
ACM.
[42]F. Stutzman, R. Gross, and A. Acquisti. Silent listeners:
The evolution of privacy and disclosure on facebook.
Journal of Privacy and Condentiality , 4(2):2, 2013.
[43] L. Sweeney. k-anonymity: a model for protecting
privacy. Int. J. Uncertain. Fuzziness Knowl.-Based
Syst. , 10(5):557{570, 2002.
[44] K. Taneja, M. Grechanik, R. Ghani, and T. Xie.
Testing software in age of data privacy: a balancing act.
InProceedings of the 19th ACM SIGSOFT symposium
and the 13th European conference on Foundations of
software engineering , SIGSOFT/FSE '11, pages
201{211, New York, NY, USA, 2011. ACM.
[45] C. Task and C. Clifton. A guide to dierential privacy
theory in social network analysis. In Proceedings of the
2012 International Conference on Advances in Social
Networks Analysis and Mining (ASONAM 2012) , pages
411{417. IEEE Computer Society, 2012.
[46] V. Toubiana, V. Verdot, B. Christophe, and
M. Boussard. Photo-tape: user privacy preferences in
photo tagging. In Proceedings of the 21st international
conference companion on World Wide Web , WWW '12
Companion, pages 617{618, New York, NY, USA, 2012.
ACM.
[47] T. T. Tun, A. Bandara, B. Price, Y. Yu, C. Haley,
I. Omoronyia, and B. Nuseibeh. Privacy arguments:
Analysing selective disclosure requirements for mobile
applications. In Requirements Engineering Conference
(RE), 2012 20th IEEE International , pages 131{140,
Sept 2012.
[48] T. L. Tuten, D. J. Urban, and M. Bosnjak. Internet
surveys and data quality: A review. Online social
sciences , page 7, 2000.
[49]V. S. Verykios, E. Bertino, I. N. Fovino, L. P. Provenza,
Y. Saygin, and Y. Theodoridis. State-of-the-art in
privacy preserving data mining. SIGMOD Rec. ,
33(1):50{57, 2004.
[50]S. D. Warren and L. D. Brandeis. The Right to Privacy.
Harvard law review , pages 193{220, 1890.
[51] G. Wondracek, T. Holz, E. Kirda, and C. Kruegel. A
practical attack to de-anonymize social network users.
InSecurity and Privacy (SP), 2010 IEEE Symposium
on, pages 223{238, 2010.
[52] Y. Zhu, L. Xiong, and C. Verdery. Anonymizing user
proles for personalized web search. In Proceedings of
the 19th international conference on World wide web ,
WWW '10, pages 1225{1226, New York, NY, USA,
2010. ACM.870