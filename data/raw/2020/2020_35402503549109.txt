On-the-FlySyntaxHighlightingusingNeuralNetworks
MarcoEdoardoPalma
UniversityofZurich
Switzerland
marcoepalma@ifi.uzh.chPasqualeSalza
UniversityofZurich
Switzerland
salza@ifi.uzh.chHaraldC.Gall
UniversityofZurich
Switzerland
gall@ifi.uzh.ch
ABSTRACT
With the presence of online collaborative toolsfor software devel-
opers,sourcecodeissharedandconsultedfrequently,fromcode
viewers to merge requests and code snippets. Typically, code high-
lighting quality in such scenarios is sacrificed in favor of system
responsiveness.Intheseon-the-flysettings,performingaformal
grammatical analysis of the source code is not only expensive, but
alsointractableforthemanytimestheinputisaninvalidderivation
ofthelanguage.Indeed,currentpopularhighlightersheavilyrelyon
asystemofregularexpressions,typicallyfarfromthespecification
ofthelanguageâ€™slexer.Duetotheircomplexity,regularexpressions
need to be periodically updated as more feedback is collected from
theusersandtheirdesignunwelcomethedetectionofmorecom-
plexlanguageformations.Thispaperdeliversadeeplearning-based
approachsuitableforon-the-flygrammaticalcode highlightingof
correctandincorrectlanguagederivations,suchascodeviewers
and snippets. It focuses on alleviating the burden on the devel-
opers, who can reuse the languageâ€™s parsing strategy to produce
thedesiredhighlightingspecification.Moreover,thisapproachis
comparedtonowadaysonlinesyntaxhighlightingtoolsandformal
methodsintermsofaccuracyandexecutiontime,acrossdifferent
levelsofgrammaticalcoverage,forthreemainstreamprogramming
languages. The results obtained show how the proposed approach
can consistently achieve near-perfect accuracy in its predictions,
thereby outperformingregularexpression-basedstrategies.
CCSCONCEPTS
â€¢Computingmethodologies â†’Neuralnetworks ;â€¢Software
anditsengineering â†’Automatedstaticanalysis .
KEYWORDS
Syntax highlighting, neural networks, deep learning, regular ex-
pressions
ACMReferenceFormat:
Marco Edoardo Palma, Pasquale Salza, and Harald C. Gall. 2022. On-the-
FlySyntaxHighlightingusingNeuralNetworks.In Proceedingsofthe30th
ACMJointEuropeanSoftwareEngineeringConferenceandSymposiumonthe
FoundationsofSoftwareEngineering(ESEC/FSEâ€™22),November14Å›18,2022,
Singapore,Singapore. ACM,NewYork,NY,USA, 12pages.https://doi.org/
10.1145/3540250.3549109
ESEC/FSEâ€™22,November14Å›18,2022,Singapore,Singapore
Â©2022Copyrightheldbytheowner/author(s).
ACMISBN978-1-4503-9413-0/22/11.
https://doi.org/10.1145/3540250.35491091 INTRODUCTION
Today,softwaredevelopersoftenturntoonlinewebapplications
for support on several aspects concerning their source code manip-
ulationtasks.Sourcecoderepositoryhostingservices,e.g., GitLab,
BitBucket ,aretypicallyconcernedwithmanagingversioncontrol
instances, DevOps lifecycles, code reviews, continuous integra-
tion, and deployment pipelines. Some extend these functionalities
by including issue tracking, knowledge bases, and chats, among
other non-software-related features. Also, some Q&A platforms,
e.g.,StackOverflow , provide the possibility to query the commu-
nity aboutcode-relatedissues.
With the ability to boost productivity [ 32], code syntax high-
lighting(SH) ispopular inonline scenariossuch as these described.
Formally,SH is a form of secondary notation in which portions of
thetextaredisplayedindifferentcolors,eachrepresentingsome
feature of the language. Due to the majority of features only being
inferablefromthegrammaticalstructureoftheinput,thetaskof
deciding what color should annotate what portion is non-trivial.
Therefore,resolversinferthecolorassignmentsfromsomeinternal
grammaticalrepresentation ofthecode.Intuitively, themorethis
analysisrestrictsthebelongingofasubsequencetosomegrammat-
icalproductions,thehigheristheaccuracyofitscomputation.Asa
resultofthehigherthenumberofsuchproductionsitcanrecognize,
andtherefore annotate,the higheris the strategyâ€™s coverage.
Unfortunately,therearetwomainchallengesinperformingsuch
analysisinthiscontext.First,thereisavaryinglevelofgrammatical
validityofthecodehighlighted.Duetoonlinecodebeingembed-
ded in multiple contexts, its grammatical correctness cannot be
guaranteed.Indeed,althoughinversioncontroliterationssource
code might tend towards being of higher quality, in other cases,
such as discussions in code review or chats, this might not carry
a valid language derivation, i.e., an Abstract Syntax Tree (AST)
might not be derivable [ 18,31,37Å›39]. This inherently induces SH
strategiesinbeinglessreliantontheabilitytoderiveacomplete
andwell-formedrepresentation of the code.
A brute-force (BF) approach towards performing accurate SH is
to use the languageâ€™s grammar for the derivation of ASTs, binding
acolortoeachtoken,basedonitslocationinthetree.However,not
only is thisoften a computationally expensive strategy, but it also
cannotbeeasilyportedtoeffectivelyordeterministicallyrecover
errors in scenarios of severely incorrect or incomplete language
derivations, e.g., code snippets [ 11,12,22]. Also, given the rich
syntax of modern mainstream programming languages, parsing
strategiesbettersuitedfordealingwithnoisylanguagederivations,
e.g.,islandparsing , wouldexpect developers of SH tools to produce
viableencodingsofthelanguagesâ€™originalgrammars[ 24,25],while
stillrequiringtoexecuteaparsingroutine.
Asforthesecondchallenge,onlyasmalltimedelayisallowed
for this frequent process to terminate, which BF approaches might
This work is licensed under a Creative Commons Attribution-
NonCommercial 4.0 InternationalLicense.
269
ESEC/FSEâ€™22,November14Å›18,2022,Singapore,Singapore MarcoEdoardoPalma,PasqualeSalza,andHaraldC.Gall
(a)Pygments ,64.76%characteraccuracy.
 (b)BRNN(16) ,100.00%characteraccuracy.
Figure1:Anexampleof task T4ofJavaSH,usingthestate-of-practiceandproposedapproaches.
exceed.Å‚On-the-flyÅ¾SHreferstocodebeinghighlightedasthisis
beingretrieved bythe user.Theadherence tosuch computational
schemaresultsinSHresolvershavinganindirectimpactonuserex-
perience[ 16,19].Fortheabove-mentionedreasons,stateofpractice
SH strategies are mainly built around (per-language)ad-hoc lexers,
whichheavilyrelyonsystemsofregularexpressions(regexes).Such
design allows to achieve excellent computational performances
whilstprovidingaSHcapableofcapturingsomecontainednumber
ofgrammaticalstructuresandaccuracylevels.Anexampleofthe
effects of this strategy is visible in Figure 1. Here, the SH produced
by a popular regexes-based resolver (Figure 1a) is compared to one
producing perfect highlighting for a grammatical coverage resem-
blingthosefoundonIntegratedDevelopmentEnvironments(IDEs)
(Figure1b).Thelowcoverageoftheformerisperceivedbyitsinabil-
ity to detect identifiers for types, method and variable declarations.
In addition, it cannot distinguish severely distant grammatical con-
structionssuchasfieldaccesses,methodinvocations,andreference
types.Inturn,thiscontributestolowannotationaccuracy.More-
over,thespecificationoftheselexersisoftenfarfromthatofthelan-
guage, inducing a tedious and error-prone regexes design process,
withthegeneralizabilityofthefinalproductrelyingonthemanual
compilation oftest casesandmultiple iterations ofuserfeedback.
Therefore,motivatedbythesechallengesandshortcomings,it
is desirable to have an approach that is: (1) simpletoimplement ,
providingadeterministic,reusable,andlow-effortprocessforde-
velopers to create and customize highlighters; (2) abletoreachhigh
grammaticalcoverages ,enablingefficienthighlightingofmorecom-
plex grammatical structures than those computed in nowadays
online highlighters; (3) highly accurate , closely reproducing the
highlightingaccuracyofaformalASTanalysisprocess;(4) input
flexible, reaching high accuracy on correct and incomplete/invalid
derivationsofthe target programminglanguage.
This paper proposesa solution that exploits lightweight Recur-
rentNeuralNetworks(RNNs)modelstoencodethehighlighting
behaviorsofformalSHbrute-force(BF)methods.ABFapproachis
user-definedandexploitsthelanguageâ€™sexistinglexingandpars-
ing tools to assign each token in the source code to a SH class, i.e.,
an abstraction of the SH color, based on its location in the AST.
Therefore,itisaformalprocess,which,ifwell-formedtomatchthe
intended highlighting scheme, is always guaranteed to generate
the correct SHsfor files carryinga validderivationof the language.
After having used BF to compile highlighting assignments (SH)
for multiple sample files, an RNN is trained to bind sequences of
tokenstosequencesofSHclasses. Thetrainingprocessonlyoccurs
once and produces an RNN model that is reusable for all futureSH tasks. For the training hardware used for the experiments in
this work, all the proposed models can be trained in the order of
minutes, and comfortably within the one hour mark. In the case
ofthenon-bidirectionalflavors,thedelayiscutinhalfcompared
to their bidirectional counterpart. This delay substitutes to todayâ€™s
state-of-practice resolvers which involve the development of te-
dious systems of Regex. Once trained, the RNN computes the SH
ofsourcecodebyinferringSHclassestothetokenstreamproduced
bythe languageâ€™soriginallexer.
Thisnovelapproachtoon-the-flySHistestedwithregardsto
its accuracy across four types of grammatical coverages, explor-
ingthedetectionofdifferent combinationsof lexicalfeaturesand
variousgrammaticalconstructionsforidentifiers,declarations,and
annotations.Tosupportthesuitabilityoftheproposedapproachin
the deployment scenarios previously envisioned, this is also tested
withregardstoitsexecutiontimewhenpredicting.Moreover,the
same metrics are measured across three mainstream programming
languages: Java,Kotlin,andPython.Allthemetricsarealsocom-
puted for a highly popular SH tool, i.e., Pygments [8], based on
the well-establishregex strategyusedby alarge numberof online
vendorssuch as GitLab,BitBucket ,andWikipedia .
Tosummarize,the main contributionsof this paper are:
â€¢a dataset for SH benchmarking for three popular programming
languages, i.e., Java,Kotlin, andPython, obtained through
formalBFstrategies;
â€¢the design of an Neural Network (NN)-based approach for SH,
withnear-perfecthighlightingaccuracyandsuitableprediction
delays;
â€¢the comparison with the state of practice SH strategy in terms
ofaccuracy,coverage,andexecutiontime;
â€¢the performance analysis of the approaches in case of incorrec-
t/incomplete sourcecodes.
The implementation, benchmark datasets, and results are avail-
able in the replication package [ 27] and published at the address
https://hlnn.netlify.app .
Therestofthispaperisstructuredasfollows.Section 2,presents
the design of the approach. Section 3describes the experimental
setup, whereas Section 4shows and discusses the results. Section 5
surveys the related work, and Section 6concludes with a summary
ofthefindingsandcontributions,aswellasanoutlookonfuture
researchinthis area.
270On-the-FlySyntaxHighlightingusingNeuralNetworks ESEC/FSEâ€™22,November14Å›18,2022,Singapore,Singapore
2 APPROACH
The strategy designed to tackle the challenges raised in this paper
aims at deriving Neural Networks (NNs) capable of statistically in-
ferringtheperfectbehaviorofbrute-force(BF)models.Forthispur-
pose,anoracleofSHsolutionsisgeneratedusingthelanguageâ€™sBF
resolver. The following section presents in detail thespecification
of both BF and NN models, as wellas providing some motivations
for the design.
2.1 OraclesforSyntaxHighlighting
Brute-force(BF) referstothedeterministicprocessofproducingthe
correcttokenclassification,orsyntax highlighting(SH),forsome
language derivation fromwhichanAbstractSyntax Tree(AST) is
derivable. These are the sole components for the generation of the
SHoracleand are createdby reusing thelanguageâ€™sexisting lexing
and parsing tools. The two components respectively represent the
inputsourcecodeasatokenstreamandorderthemintoanAST.
Subsequently,atreewalkerexploitsthestructuralinformationof
the AST to assign each token to its SH class. This process assumes
thataBFresolverisguaranteedtocomputethecorrectSHofany
validinputfile,hencesettingthehighestachievablehighlighting
accuracyforanycoveragespecification.Itisimportanttonotethat
suchadesignmerelyrequiresthedevelopertoimplementawalker
consistingofonlyahandfulofdetectionrules,asreportedinthe
replication package [ 27]. As a result, the process of producing a BF
highlighterisdeterministicandonlyasksforabasicunderstanding
of the languageâ€™s grammar, as it already exists. It is a significant
departurefromthetediousanderror-proneworkflowofdefining
systemsorregularexpressions.
TheBFalgorithmisintegralinthegenerationofthe oracle,i.e.,a
collectionoflanguageâ€™ssourcecodefilesandrespectiveSH.Forthis
purpose,eachsamplefileispipedthroughthelanguageâ€™slexerand
thentokenized.Fromeachtokenanewentityisderivedintheform:
ETA={ğ‘–ğ‘ ,ğ‘–ğ‘’,ğ‘¡,ğ‘¡ğ‘Ÿ}where the ExtendedTokenAnnotation(ETA) object
is atupleof: (1) ğ‘–ğ‘ andğ‘–ğ‘’,denotingthetokenâ€™s characterstart and
end indexes respectively, according to the file that contains it; (2) ğ‘¡,
theexacttextthetokenreferences;(3) tr,thetokenâ€™s TokenRule ,en-
codedasanaturalnumber,orinotherwords,the IDthelanguageâ€™s
lexer consistently assigns, through a dictionary, to tokens of the
sametype,amongalltypesdefinedinthelexer(e.g.,atokenoftext
Â¼mightcorrespondstoalexertype OpENÂ BRACE hencetothetoken
rule,orunique ID,20).Forexample, SÂµÂ³ÂªÂ¯Â¨Â­Â¢Â¯Â¨="JÂ¢Â·Â¢.";mightre-
sultinthesetofETA: Â¼0,5,SÂµÂ³ÂªÂ¯Â¨, Q02Â¾,Â¼7,Q0,Â­Â¢Â¯Â¨,Q02Â¾,Â¼Q2,
Q2,=,73Â¾,Â¼Q4,20,"JÂ¢Â·Â¢.", 55Â¾, andÂ¼2Q,2Q,;,63Â¾. This repre-
sentation allows the generalization of SH patterns based on the
sequence of language features in the form of token types. It does it
byabstractingawaythe otherwiseÅ‚noiseÅ¾,injectedbythetokensâ€™
specific textfeatures, transparentto the parsingofthe file.
Subsequently, the languageâ€™s parser organizes the tokens into
an AST. Walking the AST through patterns such as, VisitororLis-
tener, all previously computed ETAs are mapped to Highlighted
ExtendedTokenAnnotation(HETA) objects.TheseextendETAsto
includea HighlightingClass hc ,correspondingtothegrammatical
SH class to which the token being referenced is part of. Tokens
that are not part of any grammatical construction are bounded
to the unique hcANy, representing text, i.e., no highlighting. As aresultHETA={ğ‘–ğ‘ ,ğ‘–ğ‘’,ğ‘¡,ğ‘¡ğ‘Ÿ,â„ğ‘}. Continuing on the above-mentioned
example, the following HETA set might be computed as: Â¼0,5,
SÂµÂ³ÂªÂ¯Â¨, Q02,QÂ¾,Â¼7,Q0,Â­Â¢Â¯Â¨,Q02,2Â¾,Â¼Q2,Q2,=,73,0Â¾,Â¼Q4,20,
"JÂ¢Â·Â¢.", 55,3Â¾,Â¼2Q,2Q,;,63,0Â¾,wherehcof:0decodestosome
nothighlightedtokens, Qtotypeidentifiers, 2tovariabledeclaration
identifiers, and 3to stringliterals.
A BF resolver for some language ğ¿is a function of the form:
ğ‘ğ‘™ğ¿:{ğ‘},ğ‘™ğ‘’ğ¿,ğ‘™ğ¿,ğ‘ğ¿,ğ‘¤ğ‘ ğ¿â†’{HETA}, where: (1) for the lexerencoder
ğ‘™ğ‘’ğ¿:ğ‘™ğ¿,{ğ‘}â†’{ETA}, (1.1)ğ‘™ğ¿is the lexer of ğ¿, (1.2){ğ‘}is the charac-
tersetoftheinputfile,(1.3) {ETA}theresultingsetofETAs.(2)for
theparserğ‘ğ¿:ğ‘™ğ¿,{ğ‘}â†’ğ´ğ‘†ğ‘‡ğ¿,ğ´ğ‘†ğ‘‡ğ¿isthe derivedASToftheinput
file,(3)andforthe walkingstrategy ğ‘¤ğ‘ ğ¿:ğ´ğ‘†ğ‘‡ğ¿,{ğ¸ğ‘‡ğ´}â†’{HETA},
{HETA}isthe oracle for the inputâ€™s file
2.2 RNNsforSyntaxHighlighting
In order to efficiently perform SH for a given file, this approach
seeks to obtain a Neural Network(NN) model capableofmapping
a sequence of token rules {ğ‘¡ğ‘Ÿ}to a sequence of SH classes {â„ğ‘}, as
performedbysomeBFresolver.Hence,theprocessofcomputing
SH becomes a statistical inference on the expected grammatical
structure ofthe token sequenceininput.
The motivation behind the use of NNs for such a task relies
onthehighlystructurednatureofprogramminglanguagesâ€™files.
Indeed,theflowoftheincomingcharactersis:(1)representedas
anentitystreamselectedfromafinitesetofterminalsymbols {ğ‘¡ğ‘Ÿ},
and(2)orderedbyanunderlyingpureorderingfunctionasaformal
grammar.SHcanbeviewedasthegrammarforwhichtherealways
existsacorrectlanguagederivationwheneverthereexistsavalid
derivationoftheoriginalgrammar.Thisistrueasforsomegram-
marğ‘”, its highlighter is the grammar â„ğ‘”that sequentially parses
sub-productions ğ‘ â„ğ‘”ofğ‘”,whichareenoughtodiscriminatea trsub-
sequencetosometargethighlightingconstruction;orotherwise,
mapeverytokennotconsumablebyany ğ‘ â„ğ‘”toaterminalsymbol.In
thisnovelapproachtoSH,theeffortofproducingsuchSHgrammar
is lifted from the shoulders of the developers and instead delegated
totheNNwhichinfersitfromthebehaviorobservedfromsomeBF.
The taskof SH isreduced toaÅ‚sequence-to-sequenceÅ¾ translation
task[36], i.e.,from {ğ‘¡ğ‘Ÿ}to{â„ğ‘}.
To tackle this new problem reduction, the following proposes
the use of Recurrent Neural Networks (RNNs) [ 9], for the learning
of SHsequence bindings. These offer a baseapproach to sequence
translationbyiteratingthrougheachvalueoftheinputsequence
while outputting aunit of translation and carryingforward differ-
entiallyoptimizedinformationtoaidthepredictionoffutureinputs.
Furthermore,forthosegrammarsproducingsequencedistributions
for which the binding of an hcfor some trmay require the look
aheadofanarbitrarynumberoftokens,thisapproachresortstothe
use of Bidirectional Recurrent Neural Networks (BRNNs) [ 33] in
place of traditional RNNs. Indeed, these also aim at addressing this
specificissueby behaving astraditionalRNNs, howeverinferring
the translation of each input from the extra information carried
fromnavigatingtheinputsequenceinreverse.Finally,themodelis
designedtooutputforeach tr,acategoricalprobabilitydistribution
overthesetofavailable hc.Theabsolutevaluesofsuchdistributions
are normalized by a softmaxfunction, resulting in the sequence
ofhcfor some sequence of trbeing the set of maxvalues of the
271ESEC/FSEâ€™22,November14Å›18,2022,Singapore,Singapore MarcoEdoardoPalma,PasqualeSalza,andHaraldC.Gall
distributioncomputedforeach tr.Consequently,withregardsto
SH,an RNNmodel ğ‘€isafunctionofthe form: ğ‘€:{ğ‘¡ğ‘Ÿ}â†’{â„ğ‘}.
AlthoughbaseRNNsarenolongerthestateoftheartinmany
translation applications, with current solutions mostly utilizing
convolutional layers, the encoder-decoder architectures, or relying
on the attention mechanism [ 4,5,14,21,36,42], these still offer
a lightweight model compared to more recent techniques. More-
over, as it is later shown, the number of well-formed structural
features NN are expected to infer from the SH oracle samples is
small. It means that the extra infrastructure of deeper networks
would result in no appreciable SH accuracy increases, but rather in
computational overheads and non-trivial hyperparameter/training
configurations. Instead, RNNs and BRNNs provide a baseline solu-
tion for this novel challenge, delivering predictions with contained
overheads.Inaddition,thetrainingbehaviorofsuchmodelsallows
this approach to maintain a constant training configuration. Not
onlydoesitresultinstableperformancesacrossdifferentlanguages
andcoveragesettings,butalsoinasolutionthatisaccessibletoa
broaderaudience ofdevelopers [ 3].
3 EXPERIMENTS
Theeffectivenessofthisproposedapproachisevaluatedinterms
of its prediction accuracy and speed for four types of SH coverage.
Moreover, in the interest of providing a clearer view on how the
performances of this approach might generalize, all experiments
wereconductedonthreemainstreamprogramminglanguages: Java,
Kotlin,andPython.Torepresentthestateofpracticeapproach
usingregexes,the Pygments SHlibrary[ 8]isalsoevaluatedagainst
the same metrics. Pygments is highly popular in online and offline
scenariosandfoundinanarrayoftoolssuchas GitLab,BitBucket ,
andWikipedia .Thefollowingresearchquestionsareconsidered
for the formalanalysisofthe solution:
RQ1HowaccuratelycantheproposedNNapproachreplicatetheSH
behaviourofaBFmodel?
Thisquestionaimsatevaluating,intermsofSHaccuracy,forall
the defined coverage levels, to what extent the proposed approach
can be asubstituteto pure brute-forcemethods.
RQ2HowdoestheproposedNNapproachcomparetonowadaysstate
ofpractice,orregex,approaches?
ThisquestionneedsthecomputationoftheSHaccuracy,forallthe
definedcoverage levels,to understand to whatextentthe proposed
approach can be asubstituteto the state ofpractice.
RQ3Howdothespeedofcomputationofthethreeapproaches,NN,
BF,andregexcompare?
It provides insights into the time delays required when performing
SH withthe proposedNNs,regex-based,andBFapproaches.
RQ4HowaccuratelycantheproposedNNapproachperformSHof
incompletelanguagederivations,comparedtotheregexandBF
approaches?
An advantage of both the proposed and regex-based approaches is
their natural portability to estimate SH schemes for incorrect/in-
complete sequences of tokens. Hence, this question evaluates, in
terms of accuracy, for all the defined coverage levels, how these
approachescompare to the theoreticalperfectSH solution.3.1 CoverageTasksDefinition
Althoughaninfinitenumberofcoverageschemescouldbegener-
ated and tested for, the initial iteration of this novel approach to
SH investigates the highlighting of language features as done in
themostcommonIDEsfortheselectedlanguages,suchas IntelliJ
IDEA,PyCharm ,andVisualStudioCode .
EachCoverageTask (T) is therefore created by combining one
or many of the following language feature groups. Each feature
represents a unique hc(HighlightingClass , see Section 2.1), or in
visual terms, acolor.
LÂ¦Â¹ÂªÂ¤Â¢Â­:thisgroupincludestokenclassesthatarelexicallyidenti-
fiable, meaning that for a given token, nothing but its trvalue is
requiredto binditornot to any of such classes:
â€¢kEywORD, thereby only referring to strong keywords, as soft key-
words may also be used as user-defined identifiers in some
allowedlanguagecontexts.Inthisclass,alsotokensofprimi-
tivetypes,e.g., ÂªÂ¯Âµ,Â§Â­Â°Â¢Âµ,areincludedifthelanguageidentifies
themas such;
â€¢LITERAL,anyliteralvalueofthelanguage,e.g.,numbers(integers,
floating,binary,hexadecimals),booleanvalues( ÂµÂ³Â¶Â¦,Â§Â¢Â­Â´Â¦),null
constants ( Â¯Â¶Â­Â­,NÂ°Â¯Â¦);
â€¢ChARÂ STRINGÂ LITERAL ,anyuser-definedstringorcharacterliter-
als,includingthosepart of stringinterpolation sequences;
â€¢COMMENT.
For this group,all classes are assigned using the same criterion
that isappliedto allthe selectedprogramming languages.
IÂ¥Â¦Â¯ÂµÂªÂ§ÂªÂ¦Â³:thegroupincludesclassesforspecialtypesofidentifiers:
â€¢TypEÂ IDENTIFIER , matching all the identifier tokens within all
the languagesâ€™productions representing atype entity;
â€¢FUNCTIONÂ IDENTIFIER ,alltheidentifiersusedinfunctionormeth-
odscalls;
â€¢FIELDÂ IDENTIFIER , referring to those identifiers that the gram-
mars understand being references to an attribute of an ob-
ject or entity. These are usually preceded by a entity naviga-
tion operator, e.g., in Javaâ€™sOÂ£Â«Â¦Â¤ÂµÂ°=Â¢.Â£.Â¤().Â¥; ,Â£andÂ¥are
suchFIELDÂ IDENTIFIER , whereas Â¤might be considered a FUNC-
TIONÂ IDENTIFIER .
DÂ¦Â¤Â­Â¢Â³Â¢ÂµÂ°Â³: it includes classes for the classification of token iden-
tifiersthat carry the name of newtop-level features of programs:
â€¢CLASSÂ DECLARATOR ,referencingidentifiersboundedtosomenewly
defined declaration of any form of class, objects, enumerations,
data classes,structures, etc.;
â€¢FUNCTIONÂ DECLARATOR , for identifiers bounded to some newly de-
finedmethodorfunction;
â€¢VARIABLEÂ DECLARATOR ,tosomenewlydefinedvariable.Notethe
exclusion of this class from Pythonexperiments due to its
intrinsic ambiguity of valueto identifier assignments.
AÂ¯Â¯Â°ÂµÂ¢ÂµÂªÂ°Â¯: this includes the baseannotation components:
â€¢ANNOTATIONÂ DECLARATOR ,asitiscommonpracticetomarkupan-
notationsinallthreeselectedlanguages,thisclassreferences
thetokenidentifiersandprefixedsymbolssuchasthe `,ofan
annotation.
Finally,hcANygathers all tokens not belonging to any of the cat-
egories mentionedabove.
272On-the-FlySyntaxHighlightingusingNeuralNetworks ESEC/FSEâ€™22,November14Å›18,2022,Singapore,Singapore
Table1:Metricsfor Java,Kotlin,andPython normalizedSHoracles
MetricJava Kotlin Python
Mean SD Min Median Max Mean SD Min Median Max Mean SD Min Median Max
Chars 6239 11575 0 2932 504059 2455 4385 80 1490 176176 7390 34324 0 3398 3987090
Whitespaces 1207 24170529 72702 57512766282 47495 199912941 08291465856
Lines 190 332 0 94 14628 70 121 1 43 4734 208 873 0 104 89373
Tokens 88217451371 45229 737155923 327 72484 1161 49971525448562
From the hcgroups, four coverage tasks are defined to evaluate
the flexibility of the RNN approach to comply with some arbitrary
SHcoverage.Thefour CoverageTasks aredefinedtodemandthe
identification ofthe following groups:
â€¢T1:{ANy},LÂ¦Â¹ÂªÂ¤Â¢Â­,andDÂ¦Â¤Â­Â¢Â³Â¢ÂµÂ°Â³;
â€¢T2:{ANy},LÂ¦Â¹ÂªÂ¤Â¢Â­,andIÂ¥Â¦Â¯ÂµÂªÂ§ÂªÂ¦Â³;
â€¢T3:{ANy},LÂ¦Â¹ÂªÂ¤Â¢Â­,DÂ¦Â¤Â­Â¢Â³Â¢ÂµÂ°Â³,andIÂ¥Â¦Â¯ÂµÂªÂ§ÂªÂ¦Â³;
â€¢T4:{ANy},LÂ¦Â¹ÂªÂ¤Â¢Â­,DÂ¦Â¤Â­Â¢Â³Â¢ÂµÂ°Â³,IÂ¥Â¦Â¯ÂµÂªÂ§ÂªÂ¦Â³,andAÂ¯Â¯Â°ÂµÂ¢ÂµÂªÂ°Â¯.
It is important to note that, for the reported tasks configura-
tion,giventheoracle ğ‘‚T4carryingallofthelanguageclassification
groups,theoracleofanyotherclass ğ‘‚T[1..3]canbederiveddirectly
fromğ‘‚T4through means of a TaskAdapter ğ‘‡ğ´T4,T[1..3]. For any
taskğ‘‚Ti|ğ‘–âˆˆ{1..3}ağ‘‡ğ´T4,Timaps every target class hcto itself if it
is a possible target class for Ti, otherwise to the hcclassANy(text).
More detailsaboutthe above languagegroups, and theirdetec-
tionstrategyforallthreeinvestigatedlanguages,areavailablein
the replication package [ 27].
3.2 DataCollectionandPreprocessing
Thefollowingdescribestheprocedureproducethedatasetsusedin
theexperiments.Thefulldetails,togetherwiththedownloadable
data,are available inthe relatedreplication package [ 27].
Data mining. In order to generate SH oracles for testing the ap-
proacheswithregardstotheiraccuracy,speedofevaluation,and
training of the RNN models, samples for the three programming
languagesselectedareminedfrom GitHubâ€™spublicrepositories,
throughGitHubâ€™s Application Program Interface (API). In this
process,therepositoriesarepulledbyfilteringperprogramming
language and sortingby descending order ofstars rating.Forev-
erymainbranch,filesmatchingthelanguageâ€™sfileextensionare
downloadedintheirnaturalorder.
Withtheultimategoalofconvertingeachfiletoitsequivalent
set of HETA, the data collection process filters only files for which
the BF strategy can derive an AST. Of all files, only one instance
of the same token rule ( tr) sequence is kept: this prevents giving
an advantageto the RNNapproach, which works at a trsequence
levelinsteadofatacharacterlevel.Indeed,twoprogramfilesmight
carry different text but share thesame structure; notice how these
twoPythoncodearestructurallyequal: Â¢=Â£.Â¤[3].Â¥() andÂ¶Â Â§ÂªÂ¦Â­Â¥
=Â¶Â´Â¦Â³.Â§ÂªÂ­Â¦Â´[0].Â¯Â°Â³Â®Â¢Â­ÂªÂ´Â¦Â¥() .
For each programming language, the data collection pipeline
runs until it has sampled 20000files. This sample size is in the
interest of creating oracles that are both of large statistically mean-
ing,fortheaveragefilecontentsofeachlanguage,butcouldalso
allow for the execution of extensive accuracy and performance
testing. Statistics on the number of characters, whitespace, linesof code, and tokens, of the datasets collected for each language are
summarizedinTable 1.
Brute-Force and Oracle Generation. To create an oracle for each
language, given a set of valid input files, a BF method must be
created.Asoneofthegoalsofthisproposedapproachistoreutilize
the existing lexing and parsing strategies, the ANTLR4 [29] parser
generatortoolisused,poolingtherespectiveofficial ANTLR4 lexer
andparsergrammarsofeachlanguage.Using ANTLR4 provedto
be a winning solution to kick-start the creation of all three oracles.
Notonlyisitawidelypopularparsergenerator,butalsousedby
official language specifications, such as Kotlin, and benefits of an
activecommunitydevelopinggrammarsformostofthemainstream
programminglanguages.However,itisessentialtonotethatthe
operability of this approach does not strictly rely on this particular
tool, as any preprocessing program could be usedifmildlyadapted
to outputthe requiredandlargely genericoracle information.
The obtained lexers and parsers, of which version details are
availableinthereplicationpackage[ 27],arekeptlargelyunchanged.
The most significant changes interest the lexers, which were in-
structedtopushtheskippedtokens,e.g.,comments,throughthe
lexersâ€™ hidden channel. Such a (minor) modification enables the ap-
proachtoobtaintokensfortheseotherwisedroppedentities,which
mightstillrequirehighlighting,asreportedinSection 2.Shouldthis
workflow not be available ina languageâ€™s parsing implementation,
orshoulditsintroductioncripplethestructureoftheparser,tokens
can be lexedbyadedicatedlexer.
InadditiontothepipelineforobtainingtheETAssetandAST
for a given file, a tree walker is created, which aids the conversion
ofeachETAintoitsgrammaticallyhighlightedHETAderivative.
Although multiple walking strategies are available, for the high-
lightingofthegrammaticalfeaturesconsideredinthisfirstiteration
ofthisnovelRNNapproachtoSH,thiscanmosteasilybeachieved
through the Å‚listener pattern.Å¾ It limits the process to providing
highlighting logic for the productions that are expected to contain
tokensbelongingtoanyofthetargetSHclasses.Allothertokens
areinsteadimplicitlymappedtothe ANyclass.Asthereportingof
the fine details of such implementations would lead to a large and
mainlyuninteresting listing of tree analysis rules, this can instead
be consultedinthe replication package [ 27].
For each language, the BF methods are created for the cover-
agespecifiedbytask T4.Thisleadstothegenerationofanoracle
carrying highlighting targets for each SH classes present in any
given source files. The TaskAdapter method described earlier is
therefore used to derive the oracles for the other sub coverages
of taskT1,T2andT3. This method not only has no effect on the
correctnessofthederivedoraclesbutitalsoavoidsthedefinition
273ESEC/FSEâ€™22,November14Å›18,2022,Singapore,Singapore MarcoEdoardoPalma,PasqualeSalza,andHaraldC.Gall
ofanewtreewalkerandrespectivetimeandspaceexpansionfor
computing further oracles.
Data organization. As the proposed RNN approach involves the
trainingofNNs,itisimportanttoreportonwhatstrategiesareput
in place to ensure that not only the generalizability of the solution
is verified but that there also exists an unbiased setting when its
accuracyiscomparedwiththe otherapproaches.
For these reasons, the oracles are randomly shuffled and then
split into three folds. Folds ensure that 33%of the oracleâ€™s samples
are used for testing only, whereas of the remaining 66%,90%is
reservedfortrainingand 10%forvalidation.Thesethreesetsnever
intersect,accordingtothedatacollectionstrategyemployed.More-
over,allfoldsusedintheexperimentsareconstantandpersisted.
ThishelpsensurereproducibilityandallowseachRNNmodelto
be comparedwhen trainedonequal datasets.
Incompletefilesgeneration. AlthoughboththeproposedRNNap-
proachandthestateofpractice,basedonregexes,arecapableof
computingSHforincorrectprogramfiles,theiraccuracyinthese
casescannotbe checkedexactly,as deterministicoraclesforsuch
filesarenotalwaysderivable.Forthisreason,thefocusisshifted
from mining for incorrect file derivations towards generating in-
valid languagederivationsfrom the setofvalid sampledfiles.
In order to compare the accuracy of the SH computed by the
proposedandregex-basedstrategies,whenfedfilescarryingincom-
plete (hence invalid) language derivations against the target SH
computedbyapureprocesswithaccesstorequiredextrafilestruc-
ture, the files in each test fold are sampled line-wise to generate
onecodesnippetsizedfiles.Thesearedrawnfromthetestdatasets,
as in this first iteration of this approach, the network is not trained
on these incomplete files but only tested; however, sampling for
trainingdatasetsofthefoldsmighthavegivenanunfairadvantage
to the RNNapproach.
Atthis stage, itis also important to note thatitwouldnot have
been tractabletosamplesuch snippet-sizedfilefrom distributions
ofnaturalsnippetsgeneratingprocesses,astheBFmethodwould
not have been availablefor theformal computation of thecorrect
SH,forthereasonshighlightedabove.Therefore,withthetarget
number of newly generated files of 5000from each fold test set,
thus15000perlanguage,eachtestfileisdrawnrandomly,andfrom
it,arandom sub-sequenceoflinesischosen.
The lengths of the snippets are drawn normally according to
the languageâ€™s mean, standard deviation, minimum and maximum
number of snippets lines, determined by number of lines found by
querying the StackExchangeDataExplorer [35], focusing on
snippets from StackOverflow . In particular, at the time of the
experiments,thesenumberswere(mean, standarddeviation,min-
imum,maximum): 17.00,28.75,1,and1117forJava;15.00,22.05,
1,and703forKotlin;14.00,20.39,1,and1341forPython.
Both test files and lines are sampled with replacement. Given
the lines selected, the process gathers the set of HETAs in range
andproduces aneworacle instance.
3.3 ComparedApproaches
Multiple variations ofbaselineRNNsmodels are investigated.
An initialconfigurationfor the RNNsandtraining wasderived
byimprovingtheconvergenceofthenetworksonthevalidationset ofonly the first fold of the Javadataset. Theinitial embedding
layer was kept at 128, i.e., the smallest power of two larger than
the numberof token ids for the languages, while the hiddenunits
wereaddedinincreasingpoweroftwo.Withaconstantlearning
rateof10âˆ’3andAdamoptimizer, 16and32(B)RNNswerefound
to produce near-perfect accuracy, with the latter not improving
in wider models. Accuracy converged after the second epoch. A
finalinvestigationinvolvedthecommonpracticeofreducingthe
learningrateafterconvergencebyafactorof 10,i.e.,10âˆ’4.Itfur-
therhelpedimprovetheaccuracyofthemodel,whichagainwas
observedto convergewithin the following twoepochs.
As a result, the RNN models evaluated consist of a fixed 128
embedding layer. The output of the embedding layer is mapped to
a single layer RNNs or BRNNs, of widths evaluated among 16and
32hidden units. The output of all the RNNs or BRNNs is passed
throughafullyconnectedlinearlayerreducingittoacategorical
distribution of the available hc, depending on the CoverageTask .
Thisresultsinthetestingoffourmodels,identifiedbyitsdirection-
alityandwidthofRNNlayer: RNN(16),RNN(32),BRNN(16) ,and
BRNN(32) . Every model is trained sequentially on each training
sample, with cross-entropy loss andAdam optimizer. Thetraining
sessionforanySHRNN,languageandcoverage,wasaccordingly
settotrainfortwoepochswithalearningrateof 10âˆ’3,andfora
subsequent two epochs with a learning rate of 10âˆ’4. It is in respect
oftheapproachâ€™sinitialguaranteeofdeliveringatrainingconfig-
urationcapableofachievingtheperformanceadvertisedwithout
the tweaking of the training session by expert developers. All mod-
els commence the training process from a randomly initialized
state, according to the deep learning framework utilized, i.e., Py-
Torch[30], while a constant seed ensures the reproducibility of
the experiments.
To contextualize the performances produced by the RNNs ap-
proaches, the vastly popular and well-established regex-based syn-
tax highlighter Pygments is tested [ 8] using its latest available
version at the time of testing 2.10.0. In the following, Pygments
is being referred to as Regex. Its output was manually adjusted
tooutputthesameclassesincludedin T4,ofwhichdetailscanbe
foundinthereplicationpackage[ 27].Hence,thesame TaskAdapter
usedduringtheconversionoftheoracletoanyothertaskisused
to mapeach Pygments â€™prediction to its task-specific class.
Finally,thesame BFmethodsusedforthegenerationoftheor-
aclesarereusedfortheoutlinedcomparisonswiththeRNNsand
regex-basedapproaches.Theuseof ANTLR4 isnotonlyinduced
bythelargeavailabilityoflanguagegrammar,butalsobyitshighly
efficientLL(*)parsing[28]strategy,andnativeerrorrecoverylogic,
bothofwhichunderminethereal-worldperformanceadvantage
of,otherwisetheoreticallyregardedasmostefficient,ParsingEx-
pressionGrammar (PEG) parsers [ 13]inboth fronts [ 6,17,22].
3.4 EvaluationMetrics
The quality of an SH can be measured with regards to its coverage,
accuracy,andspeed,describedinthe following.
Coverage. Theabsolutenumberofuniquegrammarconstructions
the highlighter isableto recognize.
274On-the-FlySyntaxHighlightingusingNeuralNetworks ESEC/FSEâ€™22,November14Å›18,2022,Singapore,Singapore
Accuracy. Givenacoveragespecification,thedegreetowhichthe
highlighter can bind each character in the input text to its cor-
rectSHclass.Italsoresolves theissueof BFandRegexstrategies
possiblyproducing differenttokenizations ofthe same file.
Speed.The time delay for the computing of SH. Prediction speed
for allmethodsevaluated during the experimentationsis measured
astheabsolutetimeinnanosecondsrequiredtopredicttheSHof
an input file once this has been supplied. For each SH method, the
following time delays are measured:
â€¢BF:thetimetonativelyparsetheinputfileandperformaSH
walkofthe obtainedAST;
â€¢Regex: the time to compute the output vector of SH classes,
oncegiventhefileâ€™ssourcetext,butexcludingthetimerequired
to format the output to any specification. The latter is achieved
by defining a new Pygments Formatter object which accepts
the computed SH, but does not invest computational time into
outputtingit,henceremovingtheaddedtimecomplexityany
specific format might introduce, thereby highlighting the com-
plexityofthe approachâ€™s underlying SH strategy;
â€¢RNNs: the time for the ANTLR4 inherited lexer (the same used
bytheBFapproach)totokenizetheinputfileintoasequenceof
tokenrules,plusthetimefortheRNNmodeltocreatetheinput
tensor,andpredict the complete outputvector ofSH classes.
3.5 ExecutionSetup
All RNN models are trained on a machine equipped with an AMD
EPYC7702 64-CoreCPUclockedat 2.00GHz,64GBofRAM,and
a single Nvidia Tesla T4 GPU with 16GBof memory. Instead, all
performance testing for all of the compared approaches was car-
ried out on the same machine with an 8-Core Intel Broadwell CPU
clockedat 2.00GHzwith62GBofRAM.
3.6 ThreatstoValidity
With regards to the problem statement raised in this paper, i.e.,
on-the-fly SH, ANTLR4 undoubtedly represents the package of
technologies andstrategies requirednot onlyfor thedefinitionof
BFmodelsbutalsotheirevaluation.Despitethebestintentionto
consider all viable options, one should not exclude the existence
of, perhaps language-specific, parsing tools that might scale the
performance ofBFresolvers.
The impossibility to generate testing oracles from snippets pro-
ducedbyonlineuserprocesses,resultedinafirstexperimentsetup
whichsyntheticallygeneratesincomplete/incorrectlanguagederiva-
tions from the set of parsable derivations. Therefore, it is crucial to
notethatRQ4onlyintendstoprovideaninitialperspectiveonhow
thethreeapproachesmightperformonfilesegments,andatthat
the formal measure of closeness between this synthetic process
to that observable in online code snippets is unknown. Moreover,
human annotator processes are likely to employ their statistical
inference about the missing context of some codefragment. Hence,
onemayarguethatconductingsuchanassessmentwithamanually
composed,andthereforeinconsistent,datasetwouldinsteadvali-
dateamodelâ€™sabilitytomeetthelevelof program-comprehension of
the sample of users that created the dataset. Instead, the synthetic
dataset created here indirectly validates the modelâ€™s ability to infer
the statisticallymostlikely missingcontext.Pygments providessyntaxhighlightingfor 534languages.How-
ever,itisacollectionofimplementationsoflanguage-specific Regex
SH, and not a single generic SH resolver. This work compares with
three of such highlighters, i.e., Java,Kotlin, andPython, but
promises to be applicable to other languages, as language-specific
BFcanbeusedtotrainnewlanguagemodels.Thevalidationofthe
proposedapproachacrossallthelanguagessupportedbythe Regex-
based counterpart would extensively assess thegeneralizability of
the strategy. Therefore, this aspect is considered a limitation of
the experimental setup, which does not prove the absolute generic
performances of this novel strategy but instead delivers seminal
evidenceofits applicability.
Benchmarks for prediction delays might only give a general
perspective of the performances of such tools, but exclude spe-
cificimplementationoptimizationsthatdevelopersmightdesign.
Itmayalsoincludefilesizelimitsforonlineconsumption,which
mightbeplatformdependent.Othervariablesmightconcernthe
efficiency of the integration of SH resolvers with the rest of the
service,cachingstrategies,orhardwarespecifications.Forexample,
the proposed RNN solution might perform differently if run on
more production-focuseddeep learninglibraries [ 1], oronGPUs.
4 RESULTS
Developing from the experiment setups described in Section 3, this
section individually addresses the performance of the proposed
approachwithregardstothefourresearchquestionsidentified.For
eachquestion,itsspecificvalidationworkflowisdescribed,andthe
results are presentedanddiscussed.
To compare the observations, the Å‚Kruskal-Wallis HÅ¾ test [ 23]
was applied with the Å‚Vargha-Delaney Ë†ğ´12Å¾ test [40], for the ef-
fect size to characterize the magnitude of such differences. For this
reason, the following reports the evaluation metrics in terms of
median values,being thesetests basedonthe median differences.
4.1 RQ1Å›Comparisonwith BFâ€™sAccuracy
RQ1 aims at evaluating the SH accuracy of the proposed approach
when compared to the theoretical perfect BF resolver, on language
derivation for which an AST is derivable. Such aspect is validated
regardingallthreeprogramminglanguages,aswellastothefour
CoverageTasks . Every candidate RNN model is first individually
trainedonthetrainingsetofeachfold,anditsaccuracyisrecorded
aboutits predictions onthe corresponding test set.
As reported inTable 2,forall thelanguagesandcoveragetasks
selectedinthisexperiment,theproposedapproachiscapableofpro-
ducing near-perfect SH solutions. The bidirectional variants prove
to be the most eclectic model, which, even in the narrowest tested
configuration ( RNN(16)), achieve a perfect score more consistently
thananybaseRNNmodel,acrossalllanguagesandtasks.Itisas
expected,withbidirectionallyextendingthecontextaroundeach
token. Hence, it enables the resolution of ambiguous syntactical
structures ofwhichtype isdependent onthe nexttokens.
Furthermore, the BRNN variant promotes a significant improve-
mentinthestabilityofthisstrategy,withtheaccuracydistribution
more concentrated around the perfect mark and the outliers being
notonlyfewerbutalsoofgenerallyhigheraccuracythanotherwise
obtainablewithbaseRNNs.This isclearlyvisibleinFigure 2.
275ESEC/FSEâ€™22,November14Å›18,2022,Singapore,Singapore MarcoEdoardoPalma,PasqualeSalza,andHaraldC.Gall
Table2:Medianvaluesover 3foldsfortheaccuracy.Themaximumscorespertaskarehighlighted
ModelJava Kotlin Python
T1 T2 T3 T4 T1 T2 T3 T4 T1 T2 T3 T4
Regex 0.8662 0.7606 0.7233 0.7230 0.8009 0.6998 0.6787 0.6781 0.9364 0.8189 0.8189 0.8165
RNN(16) 0.99870.97160.96760.96681.00000.96270.95980.96051.00000.95600.95590.9550
RNN(32) 1.0000 0.9751 0.9710 0.97061.0000 0.9648 0.9640 0.96311.0000 0.9572 0.9571 0.9570
BRNN(16) 1.00001.00001.00001.00001.00001.00001.00001.00001.00001.00001.00001.0000
BRNN(32) 1.00001.00001.00001.00001.00001.00001.00001.00001.00001.00001.00001.0000
PythonKotlinJava
0.00 0.10 0.20 0.30 0.40 0.50 0.60 0.70 0.80 0.90 1.00BRNN(32)BRNN(16)RNN(32)RNN(16)Regex
BRNN(32)BRNN(16)RNN(32)RNN(16)Regex
BRNN(32)BRNN(16)RNN(32)RNN(16)Regex
Accuracy
Figure2:Accuracyvaluescomparisonfor T4.
Therefore,fortheaveragecase,theproposedRNNstrategytoSH
ismostoftenabletoperformaswellasthepureBFstrategy.Nev-
ertheless, being thisanondeterministic approach, somecontained
levels ofinconsistencyshould be expected.
4.2 RQ2Å›Comparisonwith Regexâ€™sAccuracy
Addressing RQ2 allows for the contextualization of the accuracy
valuesobtainablebytheproposedstrategy,withwhatisachievable
withtodayâ€™sstateofpractice,i.e.,regexes.Sucharesearchquestion
isthereforetackledbyevaluatingtheSHaccuracyof Pygments on
thesametestdatasetsusedtoestimatethegeneralizingaccuracy
ofthe RNNmodels inRQ1.
AssupportedbytheevidencedisplayedinTable 2,whichreports
the median accuracy values per SH method, the regex-based strat-
egyconsistentlyperformstheworstacrossalltestedscenarios.Itisalsoessentialtonoticehowthe Regexapproachissignificantly
more prone tovariabilityinitslevel ofaccuracy, compared to any
of the RNN models tested, as visualized in Figure 2.Pygments
yields its best performance across all languages when its output is
evaluatedaboutcoveragetask T1.
Anotherobservationconcerns Pygments â€™saccuracydecaying
significantlyforalltasksotherthan T1.Comparedtotheothertasks,
T1requirestheidentificationofonlylexicalfeaturesanddeclarator
identifiers.However,unlikedeclarations,lexicalcomponentsare
always deterministically identifiable through lexing, except soft
keywords. T1is,therefore,theleastcomplextaskoutofallofthose
testedas,perfile,onlyahandfulofdeclarationidentifiersarefound,
requiringtheresolverstoidentifymainlylexicalfeatures.Hence,
the accuracy of Regexresolver converges considerably fortasks
T2,T3andT4, as all other grammatical features are reasonably
consistently boundedto incorrect hcvalues.
Overall, the evidence collected for RQ2 supports the fact that
theproposedapproachiscapableofquiteconsistentlyboostingthe
SH accuracyotherwiseachievablewiththe state of practice.
4.3 RQ3Å›SpeedComparison
Theinvestigationintothepredictionspeedofalltheavailableap-
proachesaidsincontextualizingatwhatresponsivenesscoststhe
proposed approach to SH can deliver its coverage and accuracy
performances. Thus, each resolver is set to produce SH for each
languageâ€™soracle 30times,andtheirpredictiondelaysarerecorded.
Theexperimentsarecarriedoutonthesamemachine,andnoGPU
isusedfortheevaluationoftheexecutionofNNbasedresolvers.
FromtheresultsobtainedandsummarisedinTable 3,severalob-
servations can be made.
TheRNNbasedapproachesprovidesignificantspeed-upsover
theBFresolvers.Infact,inthecaseof Javapredictiondelaysare
25timessmallerforRNNsofboth 16and32hiddenunits;and 13
timessmallerinthecaseofthebidirectionalvariants.Moreover,the
standarddeviationofthepredictiondelaysoftheproposedsolution
is also significantly smaller than the BF counterparts. Both RNN
modelsreducethismetricbyafactorof 38andtheBRNNmodels
by a factory of 25.Kotlinleads to similar conclusions, although
withtheBFsolutionyieldingbetterperformances,butstillworst
compared to the proposed solution. In particular, the gains in favor
of the RNN models, which do remain consistent with the delays
recordedin Java,decreasetoanaveragespeed-upof 4fortheRNN
models, and 2for the BRNN models. Standard deviation is also
downbyafactor of 3and2for the RNNandBRNNrespectively.
276On-the-FlySyntaxHighlightingusingNeuralNetworks ESEC/FSEâ€™22,November14Å›18,2022,Singapore,Singapore
Table3:Descriptivestatisticsofexecutiontime(ms)
ModelJava Kotlin Python
Mean SD Min Median Max Mean SD Min Median Max Mean SD Min Median Max
BF 225.684 894 .046 0.004 45 .903 49618 .222 30 .950 87 .893 0.011 8 .080 14119 .526 52 .798 242 .363 0.033 24 .022 23628 .056
Regex 0.0150.0400.0040.011 22.9750.0100.0470.0040.009 27.4680.0160.0300.0030.013 7.048
RNN(16) 9.195 18 .704 0.206 3 .877 689 .178 8 .383 31 .805 0.370 3 .612 12755 .019 66 .313 288 .904 0.182 32 .597 27164 .357
RNN(32) 9.20218.5810.1953.887 677.8338.43930.2310.3843.66612067.89363.522276.8670.17631.68226279.598
BRNN(16) 17.506 36 .176 0.270 7 .241 1269 .607 14 .997 40 .814 0.586 6 .537 12120 .509 75 .235 333 .959 0.217 35 .742 32334 .076
BRNN(32) 17.72836.5650.2787.3961341.98415.66442.0900.6056.82912243.09076.895344.0680.21936.30132475.535
Java Kotlin Python
0 5000 10000 15000 20000 0 5000 10000 15000 20000 0 5000 10000 15000 20000050100150200250300350400
0255075100125150175200
02004006008001000120014001600Time (ms)BF Regex RNN(16) BRNN(16)
Figure3:Executiontime(ms)valuestrendscomparisonfor T4.
Nevertheless, such a narrative changes when comparing the
performance of the NN approach to the BF resolver for Python.
AccordingtoTable 3,theproposedRNNapproachisnotsuperiorto
theBFapproach.Infact,theparsingprovestobesignificantlymore
efficientthan it isin thecasesof JavaandKotlin. Withthetech-
nologiesconstantforallBFresolvers,thissuggeststhegrammarof
thePythonlanguage is the main promoter for the efficiency gains
observed.Nonetheless,theproposedapproachprovescapableof
nearing such stellar performance of the BF resolver, however with
some contained slowdowns: 1.3and1.4on average for the RNN
andBRNNmodelsrespectively.Standarddeviationisalsomildly
downby 1.2and1.4for the RNNandBRNNmodels.
Asexpected,thecomputationaloverheadsoftheproposedNN
approach are more significant compared to the ones that accom-
panyRegex.However,withtheRNNstrategyfocusedondelivering
greaterSHaccuracyandcoverage,andasignificantlysmallerdevel-
opmenteffortfordevelopers,thefocusisshiftedonthesuitabilityof
this approach to the task. Considering the average delays recorded
duringthisexperiment,thesearefoundtoberelativelysmall.For
theRNNapproachespredictionsareonaveragedeliveredin 9ms,
8ms, and66ms, forJava,KotlinandPythonrespectively; and
the medians 4ms,4msand33ms. Such computational delays would
most comfortably belong with the Seowâ€™s response-time catego-
rization of instantaneous [34]. In this category includes human and
computer interactions that are expected to complete within 100ms
and200ms,e.g.,clickingandtyping;whilstlongerdelays,within
500msand1000ms,beingcategorizedasimmediate,thislastone
includingnavigation actions [ 10,34].
Figure3shows a smoothed line plotto represent theexecution
timesforalltheexperiments.Asitshows,theproposedapproach
iscapableofdeliveringSH results well within the averagehumandeadlines,withtheserequiringdelaystobewithin 2sto5stomain-
tain flow[ 10,34], andtolerating awebpage response of 2s[26].
4.4 RQ4Å›IncompleteDerivationsHighlighting
RQ4 considers SH accuracy of the highlighters with incomplete/in-
correct language derivations. Likewise, for RQ1 and RQ2, all ap-
proachesaresettoproducehighlightingforallthreelanguagesand
four coverage tasks. The dataset used for this RQ4 is the generated
snippetdataset,for whichperfecttarget solutions are known.
As it possible to notice by comparing Table 4, related to RQ4,
with Table 2, related to RQ1, the results show how the RNN-based
approachesarecapableofmaintainingaccuracyperformanceson
par with those obtainable on language derivations for which an
ASTisderivable.Infact,alsointhisscenariotheRNNmodelscom-
puteSHwithanaccuracywithin 94%to96%,andthebidirectional
variants always reaching a perfect median accuracy value. The
stateofpractice,i.e., Regex,registersadecreaseinaccuracy,which,
similarly to RQ2, is considerably far from those obtainable with
theproposedNNmodels.Itisespeciallynoticeablefortaskswith
larger grammatical coverage,such as T4.
Figure4informs best about not only how consistently poorer
the results of the Regexapproach are compared to those of the
RNNsandBRNNs,butalsohowmuchmorevariabletheycanbe
expected to be. Instead, the BF resolvers proved to be the least
eclectic strategy. For Java, median performance values are close
tothoseobtainablewiththe Regexresolver,however,atthecost
of much greater variability than the latter. BF strategy performs
theworstwith Kotlin,yielding 0medianaccuracyvalueandyet
againasignificantaccuracyvariance.Finally,inthecaseof Python,
theBFapproachiscapableofoutperformingboth Regexandthe
277ESEC/FSEâ€™22,November14Å›18,2022,Singapore,Singapore MarcoEdoardoPalma,PasqualeSalza,andHaraldC.Gall
Table4:Medianvaluesover 3foldsfortheaccuracyforsnippets.Themaximumscorespertaskarehighlighted
ModelJava Kotlin Python
T1 T2 T3 T4 T1 T2 T3 T4 T1 T2 T3 T4
BF 0.9211 0.7421 0.6586 0.6440 0.0000 0.0000 0.0000 0.00001.00001.00001.00001.0000
Regex 0.87000.68590.63460.63400.81170.65770.62850.62790.93380.78900.78900.7860
RNN(16) 1.0000 0.9582 0.9512 0.95061.0000 0.9503 0.9469 0.94671.0000 0.9605 0.9595 0.9587
RNN(32) 1.00000.96340.95570.95551.00000.95340.95130.95121.00000.96180.96140.9617
BRNN(16) 1.00001.00001.00001.00001.00001.00001.00001.00001.00001.00001.00001.0000
BRNN(32) 1.00001.00001.00001.00001.00001.00001.00001.00001.00001.00001.00001.0000
PythonKotlinJava
0.00 0.10 0.20 0.30 0.40 0.50 0.60 0.70 0.80 0.90 1.00BRNN(32)BRNN(16)RNN(32)RNN(16)RegexBF
BRNN(32)BRNN(16)RNN(32)RNN(16)RegexBF
BRNN(32)BRNN(16)RNN(32)RNN(16)RegexBF
Accuracy
Figure 4: Accuracy values comparison for incomplete
languagederivations.
baseRNNapproaches,nearingthepredictionsoftheBRNNmodels.
However,the latter presents amildlysmallernumber ofoutliers.
5 RELATEDWORK
The main goal of this proposed approach is to show that deep
learningcanbeusedtoperformsyntaxhighlightingeffectivelyand
efficiently.Inthefollowing,thecurrentstate-of-the-artapproaches
that mostrelate to the proposedapproach are listed.
Deeplearningtypeinference. Similarapplicationsofdeeplearning
(DL) models have been utilized in the field of TypeInference ; an
exampleofthisis DeepTyper [15].Inthiscase,motivatedbythe
maintainabilityandreadabilitybenefitsofastaticallytypedcode-
base, the model aims at aiding developers in the transition of codeofdynamicallytypedlanguagessupportingtypeannotationtotheir
annotated equivalent. Similar to how the proposed approach to SH
learnstoinferthebehaviorofaparserontokenIDsequences, Deep-
Typeraimsatstatisticallyinferringthecompilerâ€™stypeinference
process. Such capability becomes especially useful in languages
suchasJavaScript ,whichcannotdeterministicallyhandle duck-
typingevenduringruntime.Thearchitectureusedin DeepTyper
is also based on BRNNs, however including extra infrastructure for
the handlingof more complex predictions.In fact, this consistsof
bidirectional Gated Recurrent Unit (GRU) [ 9], with2hidden layers
of650hidden units each. To proxy between the two hidden layers,
an extra layer is introduced: the Consistency Layer . This pushes
forwardanextrainputforthesecondBRNNlayer,intheformof
theaveragetokenrepresentation(embeddings)ofthefirstBRNN
layer, thereby promoting the model to use long-range values in the
input. Furthermore, the model maps its input vector through an
embeddinglayerofsize 300.Finally,DeepTyper mapsthevalues
of its output layer through a softmaxfunction to obtain for each
input token a categorical probability distribution over the types in
some vocabulary. The oracle is also generated analytically, with
TypeScript files first annotated by the compiler and then stripped
oftheirtype annotations to obtain JavaScript files.
Unlike the approach proposed in this paper, DeepTyper uses
tokens as inputs, complete of identifiers: this also allows it to com-
pute type names. However, this extra information is not needed
in the SH scope, in which structure is directly dependent on the
sequence of token rule or type, i.e., tr. The adaptation of the Deep-
TypermodeltothetaskofSH,althoughobviouslypossible,isvain
duetotheevidencebeingreported.BaseBRNNmodelsarenever
saturated in their ability to reach perfect SH accuracy. It means
the extra infrastructure of a DeepTyper model would likely not
generate better results but would lead to larger and slower models.
Learninglenientparsingandtypingviaindirectsupervision. Type-
Fixisatransformer[ 41]decodernetworkdeveloped aspart ofan
approach to leniently parse and type Javacode fragments [ 2]. It
developsfromthearchitectureandtaskof DeepTyper ,andderives
adeepermodelbasedona 6layerdecodernetwork,witheachlayer
having multi-head attention and feed-forward. By design, such fla-
vorsof encoder-decodermodels promote the outputof each inner
layertobeafunctionofallcombinationsofunitsintheprevious
layer. It promotes the learning of generalizable reductions of the
relationshipsamongelementsintheinputsequence.Morelevelsof
relationshipsbetweentheinputsmayalsobelearnedthrough multi-
headed attention , by adding more attention layers to the model.
278On-the-FlySyntaxHighlightingusingNeuralNetworks ESEC/FSEâ€™22,November14Å›18,2022,Singapore,Singapore
Moreover, this mechanism allows the model to be more easily
trainedonlongsequence, unlikeRNNmodels,which,due totheir
recurrentevaluationofaninputvector,sufferfromvanishinggradi-
ents [7]. Similarly to the proposed strategy to SH, and DeepTyper ,
TypeFixis trained over a synthetically derived oracle. In particular,
thisconsistsofbindingsof Javatokenidentifiersandtherespective
deterministicallyderivedtype.Hence,themodelistrainedtobinda
categoricalprobabilitydistributionoversomefixedtypevocabulary.
Thereasonsforwhichsucharchitectureisnotbeingevaluated
inthisfirstiterationtowardson-the-flySH,areinlinewiththose
given for DeepTyper .
Generatingrobustparsersusingislandgrammars. Islandgrammars[ 24]
aregrammarswhichdefineboth islandandwaterproductions. is-
landrulesdefinehowtoconsumespecificsubsequencesofsome
inputsequence.Instead, waterrulesdefinehowtoconsumeallof
those tokens that could not be bounded to any islandrule. Such
grammarstructuremightbeusedforthetaskofSH.Infact,givena
language, one can define the set of islandrules as the collection of
thosesub-productionswhichconsumehighlightablesequencesand
map every other token to a particular production that consumes
any terminalsymbol.
Nevertheless, this strategy is outside the goals of this work. Pro-
ducingan islandgrammar wouldinduce adevelopment workflow
similartothecurrentstateofpractice,requiringdeveloperstohave
adeepunderstandingofthegrammaticalstructureandundertakea
tediousprocessforthedefinitionofproductionswithhighcoverage
andaccuracy.Itissignificantlymorechallengingthanproviding
a tree walker for relevant constructions of the original grammar,
which by design correctly consumes all the valid iterations for the
samefeature.Moreover,the islandapproachwouldstillleavethe
handing of incomplete language derivations in the hands of the
developer. Similarly to the state of practice, island-like solutions
represent the workflowthis paper wishesto avoid.
6 CONCLUSIONSANDFUTUREWORK
Theproposedapproachiscapableofconsistentlycomputingper-
fectSHschemesfortheaverageinputfilesfor allthemainstream
languages considered. Thereby, it comfortably outperforms the SH
accuracy achievable with the here tested state of practice. Further-
more,thissolutiontoSHiscapableofproducingsuchoutputsin
expectedtimedelayssignificantlyfasterandwithlowervariance
than formal approaches, i.e., brute-force (BF), capable of equal out-
puts. However, it isverified that for cases in whichthe languageâ€™s
grammar results in an efficient parsing of the input, as it is true for
Python,thedeepstrategydoesnotrepresenta superioralternative
totheBFwithregardstothepredictiondelays, withbothsolutions
yielding time delays suitable for thesescenarios.
Future work might investigate further the accuracy with regard
to the distribution of online snippets: an aspect that, due to the
strictdesignofaBFmethod,atthisstagewasnotachievable.For
thispurpose,theautomated APIzation protocolofcodefragments
presented by Terragni and Salza [ 39], might be used for the con-
structionofgrammaticallycorrectversionsofonlinesnippets,from
which a formal oracle could be derived. Moreover, the native paral-
lelisationofConvolutionalNeuralNetworks(CNNs)[ 20],alreadyemployedinsequencetosequencetranslationtasks[ 14],maybe
exploitedfor the achievingof smallerprediction delays.
ACKNOWLEDGEMENTS
The research leading to these results has received funding from
the Swiss National Science Foundation (SNSF) project Å‚Melise -
Machine Learning Assisted Software DevelopmentÅ¾ (SNSF204632).
REFERENCES
[1]MartÃ­n Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen,
CraigCitro,GregS.Corrado,AndyDavis,JeffreyDean,MatthieuDevin,Sanjay
Ghemawat, Ian Goodfellow, Andrew Harp, Geoffrey Irving, Michael Isard,
YangqingJia,RafalJozefowicz,LukaszKaiser,ManjunathKudlur,JoshLevenberg,
DandelionManÃ©,RajatMonga,SherryMoore,DerekMurray,ChrisOlah,Mike
Schuster, Jonathon Shlens, Benoit Steiner, Ilya Sutskever, Kunal Talwar, Paul
Tucker,VincentVanhoucke,VijayVasudevan,FernandaViÃ©gas,OriolVinyals,
PeteWarden,MartinWattenberg,MartinWicke,YuanYu,andXiaoqiangZheng.
2015. TensorFlow:Large-ScaleMachineLearningonHeterogeneousSystems.
https://www.tensorflow.org
[2]Toufique Ahmed, Premkumar Devanbu, and Vincent J Hellendoorn. 2021.
LearningLenientParsing&TypingViaIndirectSupervision. EmpiricalSoftware
Engineering 26,2(2021),1Å›31.
[3]KanavAnand,ZiqiWang,MarcoLoog,andJanvanGemert.2020. BlackMagic
inDeepLearning:HowHumanSkillImpactsNetworkTraining. arXiv:2008.05981
[cs.CV](2020).https://arxiv.org/abs/2008.05981
[4]MikelArtetxe,GorkaLabaka,EnekoAgirre,andKyunghyunCho.2018. Unsu-
pervised Neural Machine Translation. In International Conference on Learning
Representations(ICLR) .
[5]DzmitryBahdanau,KyunghyunCho,andYoshuaBengio.2015. NeuralMachine
TranslationbyJointlyLearningtoAlignandTranslate.In InternationalConference
onLearningRepresentations(ICRL) .
[6]Ralph Becket and Zoltan Somogyi. 2008. DCGs+ Memoing= Packrat Parsing
butIsItWorthIt?.In InternationalSymposiumonPracticalAspectsofDeclarative
Languages(PADL) .182Å›196.
[7]YoshuaBengio,PatriceSimard,andPaoloFrasconi.1994. LearningLong-Term
Dependencies with Gradient Descent Is Difficult. Ieee Transactions on Neural
Networks 5,2(1994),157Å›166.
[8] GeorgBrandl.2022. Pygments .https://pygments.org
[9]KyunghyunCho,BartvanMerrienboer,CaglarGulcehre,DzmitryBahdanau,Fethi
Bougares,HolgerSchwenk,andYoshuaBengio.2014. LearningPhraseRepresen-
tationsUsingRNNEncoderÅ›DecoderforStatisticalMachineTranslation.In Con-
ferenceonEmpiricalMethodsinNaturalLanguageProcessing(EMNLP) .1724Å›1734.
[10]JimDabrowskiandEthanVMunson.2011. 40YearsofSearchingfortheBest
ComputerSystemResponseTime. InteractingwithComputers 23,5(2011),555Å›564.
[11]SÃ©rgio Queiroz de Medeiros, Gilney de Azevedo Alvez Junior, and Fabio
Mascarenhas.2020. AutomaticSyntaxErrorReportingandRecoveryinParsing
ExpressionGrammars. ScienceofComputerProgramming 187(2020).
[12]SÃ©rgioQueirozdeMedeirosandFabioMascarenhas.2018. TowardsAutomatic
Error Recovery in Parsing Expression Grammars. In Brazilian Symposium on
ProgrammingLanguages(SBLP) .3Å›10.
[13]BryanFord.2004. ParsingExpressionGrammars:ARecognition-BasedSyntactic
Foundation.In ACMSIGPLANSymposiumonPrinciplesofProgrammingLanguages
(POPL).111Å›122.
[14]JonasGehring,MichaelAuli,DavidGrangier,DenisYarats,andYannNDauphin.
2017. ConvolutionalSequencetoSequenceLearning.In InternationalConference
onMachineLearning(ICML) .1243Å›1252.
[15]VincentJHellendoorn,ChristianBird,EarlTBarr,andMiltiadisAllamanis.2018.
Deep Learning Type Inference. In ACM Joint European Software Engineering
ConferenceandSymposiumontheFoundationsofSoftwareEngineering(ESEC/FSE) .
152Å›162.
[16]JohnAHoxmeierandChrisDiCesare.2000. SystemResponseTimeandUser
Satisfaction:AnExperimentalStudyofBrowser-BasedApplications.In Americaâ€™s
ConferenceonInformationSystems(AMCIS) .
[17]Luke AD Hutchison. 2020. Pika Parsing: Reformulating Packrat Parsing as a
DynamicProgrammingAlgorithmSolvestheLeftRecursionandErrorRecovery
Problems. arXiv:2005.06444[cs.PL] (2020).https://arxiv.org/abs/2005.06444
[18]JiwoonJeon,WBruceCroft,JoonHoLee,andSoyeonPark.2006. AFramework
to Predict the Quality of Answers with Non-Textual Features. In International
ACMConferenceonResearchonResearchandDevelopmentinInformationRetrieval
(SIGIR).228Å›235.
[19]YuKang,YangfanZhou,MinGao,YixiaSun,andMichaelRLyu.2016. Experience
Report:DetectingPoor-ResponsiveUiinAndroidApplications.In International
SymposiumonSoftwareReliabilityEngineering(ISSRE) .490Å›501.
279ESEC/FSEâ€™22,November14Å›18,2022,Singapore,Singapore MarcoEdoardoPalma,PasqualeSalza,andHaraldC.Gall
[20]YannLeCunandYoshuaBengio.1995.ConvolutionalNetworksforImages,Speech,
andTimeSeries. TheHandbookofBrainTheoryandNeuralNetworks 3361,10(1995).
[21]Minh-Thang Luong, Ilya Sutskever, Quoc V Le, Oriol Vinyals, and Wojciech
Zaremba.2015.AddressingtheRareWordProbleminNeuralMachineTranslation.
InAnnualMeetingoftheAssociationforComputationalLinguistics(ACL) .11Å›19.
[22]SÃ©rgioMedeirosandFabioMascarenhas.2018. SyntaxErrorRecoveryinParsing
ExpressionGrammars.In ACM/SIGAPPSymposiumonAppliedComputing(SAC) .
1195Å›1202.
[23] DouglasCMontgomery.2017. DesignandAnalysisofExperiments . Wiley.
[24]Leon Moonen. 2001. Generating Robust Parsers Using Island Grammars. In
WorkingConferenceonReverseEngineering(WCRE) .13Å›22.
[25]LeonMoonen.2002. LightweightImpactAnalysisUsingIslandGrammars.In
InternationalWorkshoponProgramComprehension .219Å›228.
[26]FionaFui-HoonNah.2004.AStudyonTolerableWaitingTime:HowLongAreWeb
UsersWillingtoWait? Behaviour&InformationTechnology 23,3(2004),153Å›163.
[27]Marco Edoardo Palma, Pasquale Salza, and Harald C. Gall. 2022. On-
the-Fly Syntax Highlighting Using Neural Networks Å› Replication Package .
https://doi.org/10.5281/zenodo.6958312
[28]TerenceParr,SamHarwell,andKathleenFisher.2014. AdaptiveLL(*)Parsing:
ThePowerofDynamicAnalysis. ACMSIGPLANNotices 49,10(2014),579Å›598.
[29] TerenceParrm.2022. ANTLR.https://www.antlr.org
[30]AdamPaszke,SamGross,FranciscoMassa,AdamLerer,JamesBradbury,Gregory
Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban
Desmaison,AndreasKopf,EdwardYang,ZacharyDeVito,MartinRaison,Alykhan
Tejani,SasankChilamkurthy,BenoitSteiner,LuFang,JunjieBai,andSoumith
Chintala.2019. PyTorch:AnImperativeStyle,High-PerformanceDeepLearning
Library.In AdvancesinNeuralInformationProcessingSystems(NIPS) .8024Å›8035.
[31]Luca Ponzanelli, Andrea Mocci, Alberto Bacchelli, Michele Lanza, and David
Fullerton.2014. ImprovingLowQualityStackOverflowPostDetection.In IEEE
InternationalConferenceonSoftwareMaintenanceandEvolution(ICSME) .541Å›544.
[32]AdvaitSarkar.2015. TheImpactofSyntaxColouringonProgramComprehension.
InAnnualMeetingofthePsychologyofProgrammingInterestGroup(PPIG) .[33]Mike Schuster and Kuldip K Paliwal. 1997. Bidirectional Recurrent Neural
Networks. IeeeTransactionsonSignalProcessing 45,11(1997),2673Å›2681.
[34]StevenCSeow.2008. DesigningandEngineeringTime:ThePsychologyofTime
PerceptioninSoftware . Addison-WesleyProfessional.
[35]Stack Exchange, Inc. 2022. StackExchange Data Explorer .https:
//data.stackexchange.com
[36]Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. 2014. Sequence to Sequence
LearningwithNeuralNetworks.In InternationalConferenceonNeuralInformation
ProcessingSystems(NIPS) .3104Å›3112.
[37]MohammadRezaTavakoli,AbbasHeydarnoori,andMohammadGhafari.2016.
Improving the Quality of Code Snippets in Stack Overflow. In ACM/SIGAPP
SymposiumonAppliedComputing(SAC) .1492Å›1497.
[38]ValerioTerragni,YepangLiu,andShing-ChiCheung.2016.CSNIPPEX:Automated
Synthesis of Compilable Code Snippets from Q&A Sites. In ACM SIGSOFT
InternationalSymposiumonSoftwareTestingandAnalysis(ISSTA) .118Å›129.
[39]ValerioTerragniandPasqualeSalza.2021. APIzation:GeneratingReusableApis
from StackOverflow Code Snippets. In IEEE/ACM International Conference on
AutomatedSoftwareEngineering(ASE) .542Å›554.
[40]AndrÃ¡s Vargha and Harold D. Delaney. 2000. A Critique and Improvement of
the"CL"CommonLanguageEffectSizeStatisticsofMcGrawandWong. Journal
ofEducationalandBehavioralStatistics 25,2(2000),101Å›132.
[41]AshishVaswani,NoamShazeer,NikiParmar,JakobUszkoreit,LlionJones,AidanN
Gomez,ÅukaszKaiser,andIlliaPolosukhin.2017. AttentionIsAllYouNeed.In
ConferenceonNeuralInformationProcessingSystems(NIPS) ,I.Guyon,U.V.Luxburg,
S.Bengio,H.Wallach,R.Fergus,S.Vishwanathan,andR.Garnett(Eds.).5998Å›6008.
[42]Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V. Le, Mohammad Norouzi,
Wolfgang Macherey, MaximKrikun,Yuan Cao, Qin Gao, Klaus Macherey, Jeff
Klingner,ApurvaShah,MelvinJohnson,XiaobingLiu,LukaszKaiser,Stephan
Gouws,YoshikiyoKato,TakuKudo,HidetoKazawa,KeithStevens,GeorgeKurian,
NishantPatil,WeiWang,CliffYoung,JasonSmith,JasonRiesa,AlexRudnick,Oriol
Vinyals,GregCorrado,MacduffHughes,andJeffreyDean.2016. Googleâ€™sNeural
MachineTranslationSystem:BridgingtheGapBetweenHumanandMachine
Translation. arXiv:1609.08144[cs.CL] (2016).https://arxiv.org/abs/1609.08144
280