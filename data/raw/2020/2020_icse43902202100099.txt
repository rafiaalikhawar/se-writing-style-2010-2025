White-Box Performance-InÔ¨Çuence Models:
A ProÔ¨Åling and Learning Approach
Max Weber
Leipzig University
GermanySven Apel
Saarland University
Saarland Informatics Campus
GermanyNorbert Siegmund
Leipzig University
Germany
Abstract ‚ÄîMany modern software systems are highly conÔ¨Åg-
urable, allowing the user to tune them for performance and
more. Current performance modeling approaches aim at Ô¨Ånding
performance-optimal conÔ¨Ågurations by building performance
models in a black-box manner. While these models provide
accurate estimates, they cannot pinpoint causes of observed
performance behavior to speciÔ¨Åc code regions. This does not only
hinder system understanding, but it also complicates tracing the
inÔ¨Çuence of conÔ¨Åguration options to individual methods.
We propose a white-box approach that models conÔ¨Åguration-
dependent performance behavior at the method level. This allows
us to predict the inÔ¨Çuence of conÔ¨Åguration decisions on individual
methods, supporting system understanding and performance
debugging. The approach consists of two steps: First, we use a
coarse-grained proÔ¨Åler and learn performance-inÔ¨Çuence models
for all methods, potentially identifying some methods that are
highly conÔ¨Åguration- and performance-sensitive, causing inaccu-
rate predictions. Second, we re-measure these methods with a
Ô¨Åne-grained proÔ¨Åler and learn more accurate models, at higher
cost, though. By means of 9 real-world J AVA software sys-
tems, we demonstrate that our approach can efÔ¨Åciently identify
conÔ¨Åguration-relevant methods and learn accurate performance-
inÔ¨Çuence models.
Index Terms‚ÄîConÔ¨Åguration management, performance, soft-
ware variability, software product lines
I. I NTRODUCTION
Many software systems today are conÔ¨Ågurable, supporting
multiple application scenarios, hardware platforms, and soft-
ware stacks. ConÔ¨Åguration options are used to tailor a system‚Äôs
behavior and its non-functional properties by (de-)activating
or tuning corresponding code. Performance measures, such as
response time and throughput, are among the most important
non-functional properties of software systems [1], [2]. So,
it is crucial to know how individual conÔ¨Åguration decisions
will inÔ¨Çuence a system‚Äôs performance. Several approaches of
accurately modeling and learning the performance behavior of
conÔ¨Ågurable systems have been proposed in the literature [3]‚Äì
[7]. The underlying idea is to sample a set of conÔ¨Ågurations
from the conÔ¨Åguration space and measure their performance.
Machine-learning techniques, such as multi-variable regres-
sion [7], [8] or classiÔ¨Åcation and regression trees [5], [6], [8]
can be used to learn a performance-inÔ¨Çuence model from these
measurements, to accurately predict the performance of unseen
conÔ¨Ågurations or to Ô¨Ånd performance-optimal conÔ¨Ågurations
with search-based techniques [9]‚Äì[12].
System runtime
B E C E‚ãÖC BCBE Cm3 m1 m2
E‚ãÖCBC
PerformanceSystem runtimeSystem-Level Performance-
Influence ModelsMethod-Level Performance-
Influence Models
PerformanceSystem
Methods
Options
Which methods 
are relevant?Fig. 1: Comparison of the black-box performance-inÔ¨Çuence
modeling at system level (left) and white-box performance-
inÔ¨Çuence modeling at method level (right), explaining how
feature values at system level are composed of feature values
at method level.
These and similar approaches based on parameter tun-
ing [13] and algorithm selection [] have in common that they
conceive the conÔ¨Ågurable software system as a black box.
That is, they model the performance of a software system
as a function of its conÔ¨Åguration (i.e., a set of selected
conÔ¨Åguration options) without knowledge of the software‚Äôs
internals. For illustration, let us consider a database system
with three features Base (B), Encryption (E), Compression
(C), and EC (interaction between EandC), as illustrated in
Figure 1 on the left side. From such a system-level model, we
can infer the most inÔ¨Çuential options (e.g., Compression) and
possible interactions. However, we have no information about
the root cause of interactions or inÔ¨Çuential options. We do not
know where in the system‚Äôs code base we spent execution time
depending on the conÔ¨Åguration. Essentially, developers want
information at the level of individual methods, as illustrated
in Figure 1 at right-hand side.
Knowing performance inÔ¨Çuences at the method level helps
detecting performance bottlenecks [14], [15], pinpointing per-
formance bugs [16], [17], or assigning performance tests in a
CI pipeline to speciÔ¨Åc conÔ¨Ågurations. This is a developer‚Äôs
perspective, which is not supported by current black-box
approaches.
From a performance-analysis perspective, there are mon-
itoring and proÔ¨Åling techniques whose goal is to identify
performance hot spots [18], [19]. Specialized performance
engineers typically supervise performance of Web and cloud
applications or identify bottlenecks using stress tests [20],
[21]. Unfortunately, state-of-the-art approaches in this area
10592021 IEEE/ACM 43rd International Conference on Software Engineering (ICSE)
1558-1225/21/$31.00 ¬©2021 European Union
DOI 10.1109/ICSE43902.2021.00099
usually disregard the fact that today‚Äôs software systems have
huge conÔ¨Åguration spaces. Typically, only a single conÔ¨Ågu-
ration of a system is considered, which is insufÔ¨Åcient since
performance bugs and related issues are often conÔ¨Åguration-
dependent [17]. Few approaches aim at creating white-box
performance models of conÔ¨Ågurable systems [3], [22], [23],
but they either require explicit tracing information about which
code regions are affected by which conÔ¨Åguration options [3]
or they rely on expensive and potentially imprecise static and
dynamic program analysis [22], [23].
Our goal is to devise a white-box performance analysis
technique for conÔ¨Ågurable software systems that, with high
precision and at low cost, infers which methods are most
affected by conÔ¨Åguration decisions and which source-code
regions exhibit the highest conÔ¨Åguration-dependent perfor-
mance variation. Our approach substantially widens the ap-
plication scenarios of former black-box approaches [3]‚Äì[5]
and introduces the concept of conÔ¨Ågurability and performance
predictions to current white-box approaches [18], [19]. The
right part of Figure 1 shows the essence of the approach: it
creates a performance-inÔ¨Çuence model for each method of the
system.
To realize our goal, we combine approaches from two
Ô¨Åelds: (1) proÔ¨Åling program behavior in a white-box manner
and (2) predictive modeling of performance of conÔ¨Ågurable
software systems. We use a two-step approach to direct the
proÔ¨Åling activities to methods that are performance-relevant
(i.e., contribute to a system‚Äôs performance) and that are highly
affected by conÔ¨Åguration options. This way, we substantially
reduce the inÔ¨Çuence of measurement overhead, thereby in-
creasing prediction accuracy. In a nutshell, we draw samples
from the conÔ¨Åguration space based on well-established sam-
pling strategies [24] and measure the selected system conÔ¨Ågu-
ration with a low-overhead proÔ¨Åling tool. We use classiÔ¨Åcation
and regression trees (CART) to learn a performance-inÔ¨Çuence
model per method based on the measured performance data.
In this course, we identify methods that exhibit high perfor-
mance variance, possibly caused by conÔ¨Åguration options. In
a second step, we re-measure all methods with high variance
to improve the accuracy of the Ô¨Ånal performance models, this
way, focusing on the difÔ¨Åcult-to-learn methods. Here is the
key: Methods are affected to different extents by conÔ¨Åguration
options, and only for a fraction of methods, a more Ô¨Åne-
grained performance analysis is required. We use machine
learning to Ô¨Ånd those methods.
Focusing on methods with high performance variation,
we need to distinguish three causes: conÔ¨Åguration variance,
measurement variance, and a method‚Äôs context variance. Con-
Ô¨Åguration variance emerges from the (de-)selection of conÔ¨Åg-
uration options. This is what we aim to learn as inÔ¨Çuences of
options and interactions on individual methods. Measurement
variance corresponds to the measurement setup‚Äôs inherent
systematic bias. Context variance represents the performance
variation of a method execution due to varying input parame-
ters or program states. So, given these three types of variance,
a method‚Äôs performance can vary for each run of a softwaresystem, even with the same conÔ¨Åguration. To learn accurate
performance models for conÔ¨Ågurable software systems, we
need to measure, distinguish, and control sources for all three
types of variance.
To provide a robust empirical foundation as a base line
for evaluation, we measured the performance of 9 software
systems from various application domains, resulting in 19
years of CPU time of continuous measurements. We demon-
strate that our approach can efÔ¨Åciently identify conÔ¨Åguration-
relevant methods and learn accurate performance models at
the method level.
We contribute and evaluate not only an approach for learn-
ing white-box performance models, but also important empiri-
cal Ô¨Åndings about the distribution and nature of conÔ¨Åguration-
dependent performance-relevant methods. We contribute an
analysis that reveals the inÔ¨Çuence of the types of variance
on the runtime of methods. These Ô¨Åndings shall inform further
work on tailoring and guiding sampling techniques, static code
analyses, and performance-anomaly detection.
To summarize, we make the following contributions:
An approach to learn white-box performance-inÔ¨Çuence
models of conÔ¨Ågurable software systems at method level
using proÔ¨Åling and prediction modeling;
A performance analysis providing insights into statistical
performance properties of methods related to conÔ¨Ågura-
tion decisions;
An evaluation of our approach with respect to prediction
accuracy and scalability for 9 real-world software sys-
tems;
A replication package including our implementation and
measurements1.
II. P RELIMINARIES AND RELATED WORK
A black-box performance-inÔ¨Çuence model does not contain
information to map performance to source code. Yet, there
exists substantial work conducting root-cause analysis, trying
to locate, for example, performance bugs or memory leaks.
However, prior work rarely takes conÔ¨Ågurability into account.
A. White-Box Performance Analysis
There are only few approaches that tackle performance
analysis for conÔ¨Ågurable software systems. Reisner et al. [25]
and Meinicke et al. [26] use symbolic and variational execu-
tion to analyze the behavior of interactions of conÔ¨Åguration
options at the level of control and data Ô¨Çow. They found that
software systems in practice do have a much smaller relevant
conÔ¨Åguration space than theoretically possible, because only a
few options interact at all. Many approaches in this area rely
on this fact.
Hoffmann et al. [27] use dynamic inÔ¨Çuence tracing to
convert static parameters to dynamic control variables (global
variables) with the goal of adapting properties of an applica-
tion. They do not consider interactions among parameters and
1The supplementary material can be found at https://git.io/JtnTa or
an archived version at https://archive.softwareheritage.org/browse/revision/
2e61f8ce57498194c2af0cd76e87498a174f07fa/
1060cannot pinpoint code regions of interest. Their approach works
only when static parameters are convertible, whereas our
approach is agnostic to the type of conÔ¨Åguration parameters.
Family-based performance measurement [3] aims at apply-
ing family-based analysis [28] to performance analysis. The
idea is to create a variant simulator, which converts compile-
time variability into run-time variability [29]. Then, the variant
simulator is executed incorporating variability constraints. This
way, multiple variants can be executed and measured in a
single run by informing the analysis which method‚Äôs per-
formance is conÔ¨Åguration-speciÔ¨Åc. On the downside, family-
based performance measurement requires the construction of
a variant simulator (or other variational representation), which
is, in general, a non-trivial task [30].
Lillack et al. [31] use taint analysis to identify which code
fragments are executed depending on which conÔ¨Åguration
options. This static code analysis technique is used in ConÔ¨Åg-
Crusher for deriving performance models [22]. SpeciÔ¨Åcally,
ConÔ¨ÅgCrusher employs static data-Ô¨Çow analysis to identify
code regions whose performance is likely to be inÔ¨Çuenced
by conÔ¨Åguration decisions. It traces individual conÔ¨Åguration
options (represented by program variables)‚Äîfollowing the call
graph‚Äîand taints code regions inÔ¨Çuenced by conÔ¨Åguration
options or combinations thereof. Subsequently, ConÔ¨ÅgCrusher
merges regions to larger ones, such that it can efÔ¨Åciently
measure performance of each individual region. Performance
is measured by weaving instructions into the byte-code rep-
resentation of the target program at the beginning and end of
each region.
In contrast to our approach, ConÔ¨ÅgCrusher requires modi-
fying the source code of the target system to make the taint
analysis run. Beside integrating an interface between anal-
ysis and target system, further substantial code refactorings
are required to achieve scalability and precision [32]. The
background is that conÔ¨Åguration options are often stored in
complex data structures (lists, maps, structs, etc.), causing the
taint analysis to no longer differentiate among the options
stored in the data structures. The result is that either all
accesses to the data structure are tainted with all conÔ¨Åguration
options or the analysis stops tainting at this point. The Ô¨Årst
variant leads to memory explosion and timeouts, the second
results in incomplete and possibly very short taints, rendering
the resulting performance models inaccurate and effectively
useless. To circumvent this problem, one can always refactor
the entire system such that conÔ¨Åguration options are stored
in individual variables, which is usually infeasible in practice.
Finally, in contrast to our approach, ConÔ¨ÅgCrusher does not
support numeric conÔ¨Åguration options and is limited to single-
threaded applications.
Velez et al. [23] proposed C OMPREX , a tool that builds
white-box performance models based on dynamic taint anal-
ysis and local performance measurement. C OMPREX requires
expensive dynamic analysis and focuses only on conÔ¨Åguration
speciÔ¨Åc code, whereas our approach covers the whole code
base by building a model for each method of the system.B. Black-Box Performance Analysis
Obtaining accurate performance models requires a series
of measurements. Conducting measurements for each possible
conÔ¨Åguration, however, is infeasible due to the combinatorial
complexity of the problem. Instead, sampling a representative
subset of conÔ¨Ågurations can achieve high accuracy. For con-
Ô¨Ågurable software systems, there are various sampling strate-
gies that are suitable for learning performance models [24],
[33]: random sampling [5], [12], solver-based sampling [9],
coverage-based sampling [34], [35], and distance-based sam-
pling [36].
Courtois and Woodside [37] use regression splines to model
the black-box performance behavior of a software system
without taking conÔ¨Åguration options into account. In the same
vein, Israr et al. [38] and Mizan and Franks [39] obtain perfor-
mance models at a coarser granularity using Layered Queuing
Networks without considering variability, though. Instead, they
produce models that provide event sequences for distributed
systems. Westermann et al. [40] aim at Ô¨Ånding optimal soft-
ware conÔ¨Ågurations with performance modeling at the black-
box level. However, they consider only little variability. Also,
Krogmann et al. [41] build parameterized performance models
at the component level. That is, they build black-box models
for components and do not address Ô¨Åne-grained, possibly
cross-cutting conÔ¨Åguration options among several components.
Ackermann et al. [42] propose an approach to automatically
Ô¨Ånd a suitable machine-learning technique to learn black-
box performance models using monitoring data. Grohmann
et al. [43] use feature selection in the context of machine
learning to obtain black-box performance dependencies. In
contrast to these approaches, we use proÔ¨Åling information to
build performance models at the method level, which is one
of the main challenges and novelties of our approach.
Siegmund et al. [4], [44], [45] propose SPLConqueror, an
approach to construct performance-inÔ¨Çuence models as linear
functions over binary and numeric conÔ¨Åguration options (or
more complex combinations thereof). The key is to combine
binary and numeric sampling and to symbolically learn the
inÔ¨Çuence model in an iterative manner. Other approaches
propose learning techniques based on classiÔ¨Åcation and re-
gression trees [5], [6] and spectral learning [11], or even
learn when to stop the learning procedure [8]. Our approach
takes advantage of these modeling capabilities to pinpoint
performance properties, but at the method level.
Nair et al. [46] reduce the number of measurements by iter-
atively measuring and only adding conÔ¨Ågurations that improve
accuracy the most. Another approach by Nair et al. [11] ex-
plores the conÔ¨Åguration space by clustering, therefore, requir-
ing measurements of only few representative conÔ¨Ågurations
per cluster. This way, the sampling procedure can be directed
to unveil performance inÔ¨Çuences and near-optimal conÔ¨Ågu-
rations efÔ¨Åciently. However, all these approaches consider a
software system as a black box, not allowing for pinpointing
performance behavior and root causes in code.
1061C. ProÔ¨Åling
ProÔ¨Åling refers to the white-box analysis of the run-time
behavior of a program execution with respect to memory
consumption or execution time [47]. By contrast, measuring
execution time of a system as a whole refers to black-box
performance measurement.
Mytkowic et al. [48] analyzed the accuracy of Java pro-
Ô¨Ålers by comparing four commonly used proÔ¨Ålers regarding
their agreement on which methods are performance-critical.
The proÔ¨Ålers reported different sets of methods as hot-spots.
Reasons for the disagreement are implementation details of
the proÔ¨Ålers (e.g., whether native methods are treated as part
of the program) and the measurement overhead of the proÔ¨Åler
(observer effect).
Some proÔ¨Åling approaches aim at automatically Ô¨Ånding
speciÔ¨Åc inefÔ¨Åcient structures in the source code. Song and
Lu [49] designed LD OCTOR , and Selakovic et al. [19] de-
signed D ECISION PROF. Both tools search for redundant loops
and optimization opportunities in the order of evaluating
expressions. They focus only on speciÔ¨Åc code structures, but
provide also suggestions for improvements.
There are many other approaches of how to proÔ¨Åle different
properties of software systems. However, these approaches do
not consider conÔ¨Åguration dependent performance variation.
Still, proÔ¨Ålers have been shown an important tool in industry
to debug software systems with respect to resource usage. We
do not want to replace but build on industry-strength proÔ¨Ålers
(i.e., JP ROFILER ) to reach our goal.
III. U NTANGLING PERFORMANCE VARIANCE
At the method level, learning performance models is a task
that is highly sensitive to measurement bias. As execution
times can be short, inÔ¨Çuences of concurrent processes can
easily distort measurements. In the context of conÔ¨Ågurability
and method context, many sources contribute to the overall
variation in performance. To use machine learning effectively
on measurement data, we need to quantify possible sources
of performance variance and adjust our model accordingly.
To this end, we conducted an analysis of possible causes
of performance variance: measurement variance, conÔ¨Åguration
variance, and context variance. To devise an approach for
learning method-level performance models, it is necessary to
know how the variance in the execution time is composed and
how to control the three contributing factors to pin down the
inÔ¨Çuence of conÔ¨Åguration options on performance.
For illustration, Figure 2 shows the execution time of
a single method executed in three different conÔ¨Ågurations,
repeated three times each. Each histogram shows the dis-
tribution of the execution time of all calls to that method.
Measurement variance becomes visible when comparing rows:
The performance distribution of a row‚Äôs plots should not
change since the measurement setup is the same. That is,
any change here can only be caused by the measurement
process (e.g., overhead) or measurement environment (e.g.,
context switches). Context variance is represented by the shape
of the histogram. That is, for different contexts during a
050100Configuration 1 Configuration 2Repetition 1Configuration 3
050100# Method callsRepetition 2
100 200 300 400050100
100 200 300 400
Execution time (ms)100 200 300 400Repetition 3Fig. 2: Performance variances for different executions of
method waitUntilSynced of P REVAYLER . Columns represent
different conÔ¨Ågurations (conÔ¨Åguration variance), rows repre-
sent different repetitions of a program execution (measurement
variance), and each cell depicts the performance distribution
of multiple executions of the same method in a single program
run (context variance).
single program run, we might call the method with different
parameters, different cache states, etc., leading to different
execution times. So, a histogram shows the distribution of
execution times for a single method in a single program
run. Finally, conÔ¨Åguration variance manifests as differences
among plots of different columns. If the plots differ across
the columns, the root cause of the performance changes are
due to changes in the system‚Äôs conÔ¨Åguration. In what follows,
we provide an in-depth analysis of three real-world subject
systems (C ATENA , H2, and P REVAYLER ; see Section V-C
for more details) regarding the three sources of variations.
The goal is to obtain insights into which variance needs to
be controlled when learning performance-inÔ¨Çuence models at
method level. This helps us in devising sensible means for
sampling, measuring, and learning instead of blindly applying
an off-the-shelf machine-learning approach.
A. Measurement Variance
Measurement variance affects the accuracy with which we
get stable results while repeating experiments. High measure-
ment variance adversely affects the accuracy of performance-
inÔ¨Çuence models. Therefore, it is crucial to estimate mea-
surement variance with a sufÔ¨Åcient number of experiment
repetitions. The aim of our analysis is to determine the number
of repetitions needed to trust the estimated measurement
variance.
Our analysis setup is as follows: We proÔ¨Åle a given con-
Ô¨Ågurable software system with 50 repetitions and report the
coefÔ¨Åcient of variation (c v=;represents the standard
deviation of all method executions; the mean of all method
executions in a single run) as a standardized measure of
dispersion of a probability distribution to quantify the stability
of measurement results [50]. To check whether measurement
variance is independent of conÔ¨Åguration variance, we ran-
10620 10 20 30 40 50
# Measurement repetitions0102030Coefficient of variationCatena
H2
PrevaylerFig. 3: Measurement variance with increasing number of repe-
titions. The dashed red line denotes the maximal measurement
variance (4 %).
domly select 100 conÔ¨Ågurations for each of the three software
systems.
To better visualize the effect of repeating experiments,
we compute the coefÔ¨Åcient of variation after each repetition
and show it for our three conÔ¨Ågurable software systems in
Figure 3. For all three systems, the variance is below 5 %. This
indicates a reliable measurement setup and the need for only
a limited number of repetitions (INSIGHT 1) as the coefÔ¨Åcient
remains stable already around three repetitions. Moreover, the
coefÔ¨Åcient is equal for all conÔ¨Ågurations, so conÔ¨Ågurations
have no effect on measurement bias. Hence, we can neglect
possible hidden dependencies here.
B. ConÔ¨Åguration Variance
ConÔ¨Åguration variance captures the variation in a method‚Äôs
execution time due to selecting different conÔ¨Åguration options.
Our analysis setup is as follows: We proÔ¨Åle a given con-
Ô¨Ågurable software system by measuring the execution time of
each method. We aggregate the execution time per method and
repeat this process Ô¨Åve times to account for measurement bias.
We repeat this process for different conÔ¨Ågurations. Again, we
use the coefÔ¨Åcient of variation per method as a measure to
determine whether methods have a constant average execution
time across different conÔ¨Ågurations. If the coefÔ¨Åcient of vari-
ation is higher than the measurement variance (4 % for all of
our subject systems; see Section III-A), the method‚Äôs execution
time is conÔ¨Åguration-dependent.
Figure 4 depicts the coefÔ¨Åcient of variation (y-axis) for each
method (x-axis) ordered from low variation to high variation.
We observe that a large number of methods have only limited
variance (INSIGHT 2) and only few methods exhibit high
performance variance (INSIGHT 3). Interestingly, the variance
for these few methods is huge, and the percentage of methods
affected by conÔ¨Åguration decisions is also sensitive to the
software system. From this analysis, we infer that method-level
performance-inÔ¨Çuence models should concentrate on these
highly varying methods; proÔ¨Åling all methods of the program
would be wasteful. This is good news as proÔ¨Åling is usually
expensive and affects measurement results. Hence, we con-
clude that an efÔ¨Åcient and accurate learning approach would
0 20 40 60 80 100
# Methods in percent0102030Coeficient of variationCatena
H2
PrevaylerFig. 4: ConÔ¨Åguration variance of the methods per subject
system sorted by their coefÔ¨Åcient of variation. The dashed red
line denotes the maximal measurement variance (4 %) such
that all method execution times above the line change due to
conÔ¨Åguration decisions.
Ô¨Årst need to Ô¨Ånd the relevant methods and then concentrate
learning these.
C. Context Variance
Context variance of a method‚Äôs execution time originates
from changes in the method‚Äôs calling context (e.g., method
parameters and cache state). In Figure 2, we visualize context
variance as a histogram of performance values for each method
execution in a single program run. We observe that the
execution time of some methods remains constant during a
program run, whereas other methods show high variance.
Overall, we observe highly skewed execution times that
heavily affect a method‚Äôs average execution time (INSIGHT 4).
That is, we observe few but very large outliers, which are
several orders of magnitude slower than about 99 % of the
other method executions. This resembles a Cauchy distribu-
tion with no deÔ¨Åned mean and standard deviation for these
methods [51]. The problem for learning is that the Cauchy
distribution is a well-known case where maximum-likelihood
estimation fails and, subsequently, the likelihood principle
in general [52], [53]. So, this can cause highly unreliable
performance models.
Finally, we also see an interaction between context and
conÔ¨Åguration variance when analyzing Figure 2: Not only the
execution times vary, but also the number of method executions
(INSIGHT 5), so an algorithm that takes only the average
execution time of a method in to account for learning the
inÔ¨Çuence of options is doomed to fail. Instead, an accurate
approach needs to account for the number of method calls in
relation to their execution time.
D. Summary
From our analysis of variance, we can learn two important
things: First, we see that the distribution of method executions
changes for different conÔ¨Ågurations. That is, conÔ¨Åguration
inÔ¨Çuences a method‚Äôs context causing a variation in the
method‚Äôs execution time. Second, based on our insight that
some methods‚Äô performance values are Cauchy distributed,
we cannot resort to a sample-based proÔ¨Åling technique, but
10630.025 0.030 0.035 0.040 0.045 0.050
Execution time (ms)0200400600# Method calls99% mass
5 10
Execution time (ms)1% massFig. 5: Context variance of method rotr64; right: the 1 %
longest running method executions; left: the remaining 99 %
method executions.
rather need to tap the entire performance distribution using
an instrumentation-based approach (INSIGHT 6). This is nec-
essary as there might be large outliers that could skew the
average method execution time substantially and would need
to be Ô¨Åltered out. This is hardly possible with sample-based
proÔ¨Åling.
IV. P ERFORMANCE -INFLUENCE MODELING AT THE
METHOD LEVEL
Our goal is to learn performance-inÔ¨Çuence models at the
method level, so that we can pinpoint methods with high
conÔ¨Åguration-dependent performance variability and identify
code regions that cause performance interactions. Our method-
level performance-inÔ¨Çuence modeling approach builds on the
insights that we gained from our variance analysis and is
separated into two steps, as illustrated in Figure 6: coarse-
grained analysis and Ô¨Åne-grained analysis.
We know that the execution time of only few methods
varies for different conÔ¨Ågurations (I NSIGHT 3), so we aim
at identifying exactly these methods in the Ô¨Årst step. For
this purpose, we use a light-weight, coarse-grained proÔ¨Åler
(JPROFILER ) to obtain performance measures for all methods
under different conÔ¨Ågurations using an established sampling
approach. Then, we extract those methods that exhibit (i)
a performance-relevant execution time (e.g., we Ô¨Ålter out
getter/setter methods) and (ii) a performance variation across
different conÔ¨Ågurations. In the second step, we instrument the
source code (I NSIGHT 6) using the tool K IEKER to obtain an
execution time distribution (an execution time for each method
call). We Ô¨Ålter from this distribution long running outliers
(INSIGHT 4) and summarize the distribution as a histogram
(INSIGHT 5). Finally, we learn one performance-inÔ¨Çuence
model per method based on these Ô¨Åne-grained values.
A. Sampling ConÔ¨Ågurations
As a prerequisite to step 1, we sample a set ^Cfrom the set
Cof all valid conÔ¨Ågurations. There is a substantial corpus
of approaches that successfully applied different sampling
strategies to obtain a representative set of conÔ¨Ågurations [8],
[12], [24], [36]. Following previous work, we opt for feature-
wise and pair-wise sampling for binary and Plackett-Burmansampling for numeric options [4], but other sampling strategies
might be appropriate, as well.
With feature-wise sampling, we obtain a set of conÔ¨Ågura-
tions in which each option is enabled once. With pair-wise
sampling, also called t-wise sampling with t = 2, this set is
enriched by all pair-wise combinations of conÔ¨Åguration op-
tions. We use the extended Plackett-Burman design, proposed
by Wang and Wu [54], for sampling numeric conÔ¨Åguration
options. Compared to binary options, adding a numeric option
withndifferent values increases the conÔ¨Åguration space by
factorninstead of factor 2. The Plackett-Burmann design
selects a Ô¨Åxed set of conÔ¨Ågurations determined by a pre-chosen
seed, which strongly reduces the effect of the combinatorial
explosion.
B. Coarse-grained ProÔ¨Åling
Our approach automatically runs a given software system
for each conÔ¨Åguration of the sample set (denoted as run) with
JPROFILER2, a coarse-grained proÔ¨Åler that uses the JVMTI
interface of the JVM. For each run, we obtain the absolute
execution time and the number of calls for each method. We
repeat each run Ô¨Åve times and report the mean time to account
for measurement bias (I NSIGHT 1).
Next, we learn a performance-inÔ¨Çuence model per method
m2M(whereMis the set of all methods of a software
system) from these measurements using classiÔ¨Åcation and
regression trees (CART) as the learning method [5]. In the
case that all methods have been learned accurately, there is
no need to continue with the second step. However, we have
seen in our variance analysis that, typically, some methods are
highly sensitive to context variance (I NSIGHT 4AND 5), which
makes a second learning step necessary.
C. Filtering
To identify methods that are hard to learn and that contribute
substantially to a system‚Äôs performance (I NSIGHT 5), we apply
a Ô¨Ålter to all methods Mof a system obtaining a subset
MhardMfor further measurement and learning (cf. Eq. 1).
The Ô¨Ålter relies on a predicate (m;;; ), with;;2R
(cf. Eq. 2), which states whether a given method m2M
belongs to the set of performance-relevant methods.
Mhard=
mjm2M^(m;;; )	
(1)
err(m; )  abs(m; )_rel(m; )(m;;; )(2)
err(m; )evaluates whether the error of the corresponding
method‚Äôs performance model exceeds the given threshold 
(cf. Eq. 3). All methods that have been learned with a pre-
diction error (mean absolute percentage error, MAPE, Eq. 4)
ofor worse get selected. For this purpose, we compare
the measured performance for method mof conÔ¨Åguration c
denoted with c(m) with the performance 0
c(m) predicted
with the model of step 1. Following Siegmund et al. [4], we
2https://www.ej-technologies.com/products/jproÔ¨Åler/overview.html
1064ProÔ¨Åling
(coarse-grained)Performance
Modeling
ProÔ¨Åling
(Ô¨Åne-grained)Performance
ModelingConÔ¨Ågurable
Software System
Filter Methods
Filter
OutlierSampling
All
MethodsFig. 6: Method-level white-box-modeling pipeline for conÔ¨Ågurable software systems
Ô¨Åxto 5 % in our experiments, which represents an already
strict Ô¨Ålter criterion just above the measurement bias (cf.
Section III-A). Increasing decreases the number of methods
that have to be analyzed further, but this way more inaccurate
performance models are accepted.
err(m; ) =MAPE(m) (3)
MAPE(m) =100
j^CjX
c2^Cc(m) 0
c(m)
c(m)(4)
abs(m; )(Eq. 5) evaluates whether a method‚Äôs execution
time is longer than . A method‚Äôs execution time here is
deÔ¨Åned as the accumulated execution times over a run (Eq. 6).
By setting, we control to which extent we want to invest
measurement effort for short-running methods.
abs(m; ) =absPerf(m) (5)
absPerf(m) =1
j^CjX
c2^Cc(m) (6)
rel(m; )(Eq. 7) evaluates whether a method has a
relative run-time (Eq. 8) of more than in relation to the
accumulated black-box time of the overall software system
(sum of performance values of all methods, Eq. 9). Adjusting
enables us to focus on methods that contribute the most to
the overall performance of the system.
rel(m; ) =relPerf(m) (7)
relPerf(m) =1
j^CjX
c2^Cc(m)
blackBoxPerf(c)(8)
blackBoxPerf(c) =X
m2Mc(m) (9)
To sum up, predicate selects methods that have been
inaccurately learned ( ), have a total run time of, at least,
, and contribute to the overall software‚Äôs performance by, at
least,percent.
D. Fine-Grained ProÔ¨Åling and Learning
We use K IEKER [18], an aspect-oriented J AVA performance
proÔ¨Åling tool, to measure the methods Mhard obtained fromthe Ô¨Åltering step. To analyze the variation across all calls of a
method, we extended K IEKER by logging the assignment of
values to method arguments of each method call together with
the measured execution time.
ProÔ¨Åling with K IEKER involves three steps: First, we in-
clude an annotation (pointcut) into the source code of the
subject system at the beginning of each selected method.
Second, we compile the software system into an executable.
Third, we execute the software with K IEKER as JVM argument
(JAVA agent) to weave the monitoring code (advice) around
method executions. We run our experiments with the same set
of conÔ¨Ågurations and workloads as used in the coarse-grained
proÔ¨Åling phase, obtaining performance data per method execu-
tion of the relevant methods. Based on our variance analysis,
we Ô¨Ålter outliers that make up 1 % of the longest execution
times and we learn new models with the remaining aggregated
data using CART.
V. E XPERIMENT SETUP
In this section, we present the measurement setup that we
use for proÔ¨Åling as well as the software systems that we
selected for evaluation.
A. Measurement Setup
All measurements ran on a cluster of 27 computers, each
of which has an Intel Quad-Core processor, an SSD running
a headless operating system (Ubuntu 18.04.3 LTS), an HDD
to store experiment data, and 8 or 16GB of RAM.3
B. Measurement Procedure
For each subject system, we generate two sample sets
for learning (according to the two sampling strategies of
Section IV-A), as shown in Table I. For each learning set,
we measure the runtime with our two-step approach, learning
a model per method. We sample an additional test set of
100 fresh conÔ¨Ågurations at random. We use the test set to
evaluate the prediction error of the models learned based on
the learning sets. Furthermore, we repeat all measurements Ô¨Åve
times, which results in 94,000 measurement runs (RQ 1and
RQ2). Additionally, we measure the black-box execution time
of all conÔ¨Ågurations to determine the execution time without
proÔ¨Åling (RQ 3).
3For a single software system, we conducted all measurements either on
the systems with 8 or 16GB memory.
1065TABLE I: Overview of subject systems. jFjdenotes the
number of conÔ¨Åguration options (binary and numeric); jCj
denotes the number of the valid conÔ¨Ågurations C;j^CFWj
denotes the number of conÔ¨Ågurations sampled feature-wise;
j^CPWjdenotes the number of conÔ¨Ågurations sampled in a
pair-wise manner.
System Domain jFj jCj j ^CFWj j^CPWj
BATIK SVG rasterizer 31 9:610428 337
CATENA Password hashing 12 1:0109875 2625
CPD Copy-paste detector 7 1:110440 115
DC Image density converter 24 3:41061600 9700
H2 Database 166:51011375 2275
KANZI Data compression 40 4:310334 458
PMD Source-code analyzer 11 5:110236 104
PREVAYLER Database 121:3105250 400
SUNFLOW Rendering engine 6 5:4106125 n/a
C. Subject Systems
We evaluate our approach with 9 real-world software sys-
tems. Our selection includes conÔ¨Ågurable J AVA applications,
covering different domains, including databases, rendering
engines, and static code analyzers. Our selection was driven
by covering a diverse set of domains, having memory and
CPU-intensive tasks, and providing conÔ¨Åguration options that
affect performance of the system. We provide an overview of
the software systems in Table I. Next, we present the systems
and benchmark workloads. When possible, we reused existing
workloads provided by the respective software systems.4
The B ATIK rasterizer converts SVG Ô¨Åles to a raster format.
As workload, we used the D ACAPO benchmark suite [55],
which contains a set of SVG images of different sizes that
can be used for performance tests.
CATENA is a secure password scrambling framework that
implements a corresponding hashing function. As workload,
we used its password hashing benchmark. With the provided
conÔ¨Åguration options, it is possible to select one out of four
graphs as well as different seeds and security values that
inÔ¨Çuence how much main memory has to be used to encrypt
or decrypt a password and how long this process takes.
CPDis a code duplication detector. It detects duplicate
source code sections to support developers with code refac-
toring. As a benchmark workload, we detect code duplicates
in C ATENA ‚Äôs code base.
DENSITY -CONVERTER (DC) is an image density converter
that, given an image or folder, converts these inputs into image
formats with different resolutions. As workload, we used a set
of high resolution images provided by the developers.
H2 is an open-source relational database system that can
operate both in an embedded and a client-server setting. As
workload, we use the subset of tests of the P OLEPOSITION
benchmark, with which developers compare H2 to other
database applications.
4We provide all benchmarks on the supplementary Website: https://git.io/
JtnTa or an archived version at https://archive.softwareheritage.org/browse/
revision/2e61f8ce57498194c2af0cd76e87498a174f07fa/KANZI is a lossless data compressor. It provides various
conÔ¨Åguration options for composing and tuning the com-
pression process. As workload, we used the S ILESIA corpus
benchmark5.
PMDis an extensible cross-language static code analyzer
that checks source code against a set of rules. As a workload,
we selected all rules that try to identify performance violations
in the system to analyze. The system that we analyzed is
PREVAYLER .
PREVAYLER is an open-source object persistence library
for J AVA supporting in-memory storage. It provides a scal-
ability and performance benchmark consisting of transaction-
processing and query scalability tests that are applied to a
JDBC-compatible database.
SUNFLOW is an open-source global illumination rendering
system. It provides a selection of example scenes (objects
to illuminate and render), of which we selected the golden
scene (a teapot in a colored room with one light source and
128128 pixels).
VI. E VALUATION
The goal of our approach is to pin down the inÔ¨Çuence of
conÔ¨Åguration options on individual methods. In our evaluation,
we address three research questions:
RQ1:Can we learn accurate performance-inÔ¨Çuence models at
the method level?
Previous work has shown that performance can be accurately
modeled for a system as a whole. However, it is unclear
whether this level of accuracy can be achieved when modeling
performance at the method level, due to measurement overhead
and the various sources of variance that we have described in
Section III.
RQ2: How do system-level and method-level models compare
in terms of information they provide?
Knowing the inÔ¨Çuence of a conÔ¨Åguration option at the system
level is helpful for tuning a system‚Äôs performance. However,
identifying the root cause of the inÔ¨Çuence of features helps
developers, for example, to spot performance bugs and to focus
on speciÔ¨Åc performance tests in a CI pipeline.
RQ3: What is the relation of the runtimes of proÔ¨Åled and
unproÔ¨Åled methods?
An important measure for validity is whether the actual
(unproÔ¨Åled) method execution time relates to the measured
(proÔ¨Åled) method execution time. ProÔ¨Åling introduces over-
head, and therefore the models we learn may be biased. With
this question, we aim at quantifying the extent of the proÔ¨Åling
overhead.
As ConÔ¨ÅgCrusher is closest to our approach (cf. Section II),
it would be a natural candidate for comparison. In Sec-
tion VI-D, we report on why a comparison is not feasible,
though.
5Silesia Corpus: http://sun.aei.polsl.pl/~sdeor/index.php?page=silesia
1066Batik
CatenacpddcH2
Kanzipmd
PrevaylerSunflow101
100101102103104MAPE [%]
All methods (M)
Batik
CatenacpddcH2
Kanzipmd
PrevaylerSunflowFiltered methods (M hard)Fig. 7: Error (MAPE) of method-level performance-inÔ¨Çuence
models of all methods (M ) and of the Ô¨Åltered methods
(Mhard). Dashed red line denotes 5 % MAPE.
TABLE II: Method and correlation analysis. jMjdenotes the
total number of methods and jMhardjthe number of Ô¨Åltered
methods. ConÔ¨Åguration-wise linear correlation (LC) and rank
correlation (RC) of black-box measurements (BB) using a
coarse-grained proÔ¨Åler (CG) and a Ô¨Åne-grained proÔ¨Åler (FG).
System jMj jM hardj BB vs. CG BB vs. FG
LC RC LC RC
BATIK 122 9 0.82 0.89 0.76 0.87
CATENA 128 7 0.94 0.98 0.81 0.95
CPD 125 11 0.25 0.55 0.57 0.76
DC 435 3 0.51 0.91 0.53 0.96
H2 238 43 0.32 0.42 0.12 0.34
KANZI 386 27 0.65 0.87 0.38 0.77
PMD 218 5 0.32 0.72 0.18 0.38
PREVAYLER 166 7 0.94 0.97 0.84 0.88
SUNFLOW 128 6 0.62 0.62 0.26 0.38
A. Method-Level Performance Models (RQ 1)
a) Operationalization: To answer RQ 1, we calculate the
MAPE of the performance-inÔ¨Çuence models of each method
separately. We follow the measurement procedure described in
Section V-B. We assume an inÔ¨Çuence model to be sufÔ¨Åciently
accurate if it predicts the execution time of the method in the
test set with an average error below 5 %, which is stricter than
10 % as used in previous work [5].
b) Results: Figure 7 summarizes the results regarding
RQ1. The plot on the left shows the prediction-error distribu-
tion of all method-level models based on data measured with
the coarse-grained proÔ¨Åler. All methods that are below the
dashed red marker (5 % prediction error) can be accurately
learned based on a single proÔ¨Åling run per conÔ¨Åguration,
which makes up 84.8 % of all methods. Hence, the err
condition of our Ô¨Ålter selects about 15 % of the methods for
step 2. When using the additional conditions with respect
to performance relevance ( absandrel), we obtain a set
comprising of about 6 % of all methods. Table II depicts for
each system the set of Ô¨Åltered methods, denoted as Mhard.
Applying the second step, we are able to model nearly all
of the remaining methods accurately as shown in Figure 7 on
the right-hand side. From a total number of jMhardj= 118
methods across all systems, only 5 methods cannot be learnedaccurately in step 2. A closer manual analysis of these methods
revealed that they depend either heavily on thread-dependent
Ô¨Åle IO operations in the case of H2 or nested loops (number
of loops depends exponentially on an option) that copy an
internal state array (width of array depends exponentially on
a conÔ¨Åguration option) in the case of C ATENA .Answering RQ 1, the coarse-grained proÔ¨Åling step is able
to learn models with a MAPE below 5 % for 84.8 % of
all methods. Applying the Ô¨Åne-grained proÔ¨Åler in a second
step, the MAPE is below 5 % for 95.8 % of the performance-
relevant methods.
B. Tracing Option InÔ¨Çuences (RQ 2)
a) Operationalization: To answer RQ 2, we focus on the
importance of conÔ¨Åguration options and interactions in white-
box models. SpeciÔ¨Åcally, we learn one random forest [56],
consisting of 100 classiÔ¨Åcation and regression trees, at system-
wide level as well as one random forest for each method
and extract all options and interactions. For this analysis and
without loss of generality, we concentrate on the two options
(or interactions) per system that have the largest performance
inÔ¨Çuence on the system determined by the black-box model
(similar to the scenario sketched in Figure 1). We aim at
identifying the root cause for high inÔ¨Çuences by analyzing
all performance-inÔ¨Çuence models at method level. Since there
might be hundreds of methods per system, we analyze only
performance-relevant methods, whose total sum can explain
80 % of the system performance. Having determined inÔ¨Çu-
ential options and methods, we count those methods as root
cause for which the conÔ¨Åguration options (or interactions)
we are interested in, have an inÔ¨Çuence that exceeds the
measurement error. Furthermore, we sort the methods by the
options‚Äô inÔ¨Çuences revealing, which method contributes most
to an option‚Äôs inÔ¨Çuence.
b) Results: Tracing the inÔ¨Çuence of an option from the
system level to the method level uncovers the cause of its
inÔ¨Çuence. We present the results for C ATENA in Figure 8.
There are some conÔ¨Åguration options and interactions that
have an high inÔ¨Çuence on performance compared to the others;
12 options have no relevant inÔ¨Çuence. Focusing on an option
of interest‚Äîthe most important option in our example‚Äî
reveals that only a small portion (12 out of 128) of the
methods contribute to the option‚Äôs system-wide performance
inÔ¨Çuence. These 12 methods are responsible for more than
2/3 of the system‚Äôs performance. This kind of information
is not available in black-box models. It does not only help
selecting important conÔ¨Åguration options for guiding sampling
and performance tuning, but also identifying the small set
of methods that causes possible performance bugs, this way,
facilitating performance bug detection of conÔ¨Ågurable systems.Answering RQ 2, white-box performance-inÔ¨Çuence models
can successfully guide us to performance-relevant methods
that are dependent on inÔ¨Çuential conÔ¨Åguration options and
interactions.
1067Fig. 8: Overview of the inÔ¨Çuence of options and interactions
on C ATENA ‚Äôs performance. The background plot shows the
distribution of all inÔ¨Çuential options and interactions (inÔ¨Çuence
greater then the measurement error). The small inner plots
focus on the interaction gammagarlic, which is the most
inÔ¨Çuential option/interaction of the model. The left plot shows
the number of methods that contribute and do not contribute
to the interaction‚Äôs performance. The right plot shows the
performance portion of these methods.
C. ProÔ¨Åled vs. UnproÔ¨Åled Methods (RQ 3)
a) Operationalization: To answer RQ 3, ideally we would
need to compare the execution time between a proÔ¨Åled method
and an unproÔ¨Åled method. Since we cannot know an unproÔ¨Åled
method‚Äôs execution time, we use a proxy to infer the actual un-
proÔ¨Åled method execution times. For this purpose, we consider
the black-box execution time of a system as the aggregation
of all true method‚Äôs execution times. We compare this time
against the aggregated (i.e., system-wide) predictions of white-
box performance models for all methods. By repeating this
process for all measured conÔ¨Ågurations, we approximate the
relation of our estimates to the actual method execution times.
We use two different indicators to quantify the relation:
Pearson‚Äôs correlation coefÔ¨Åcient and Spearman‚Äôs rank correla-
tion coefÔ¨Åcient. The former tests whether there is a linear de-
pendency between the aggregated, system-wide execution time
with and without proÔ¨Åling. A high linear correlation would
indicate that it is possible to infer the unproÔ¨Åled execution
time from the proÔ¨Åled execution time with a constant factor.
This way, white-box performance inÔ¨Çuence models could even
provide a precise prediction of method performance running
in operation (without proÔ¨Åler). A high linear correlation is un-
likely, though, because the overhead during proÔ¨Åling increases
while the number of proÔ¨Åled methods grows. Spearman‚Äôs rank
correlation coefÔ¨Åcient tests whether the order between proÔ¨Åled
and unproÔ¨Åled execution times is preserved. A high rank
correlation indicates that fast conÔ¨Ågurations measured without
proÔ¨Åling stay fast, even if proÔ¨Åling is enabled. This would
mean that our approach has accurately learned the relative
inÔ¨Çuences of conÔ¨Åguration options per method.
b) Results: In Table II, we show the correlation between
system-wide unproÔ¨Åled execution time compared to using the
500 1000
jProfiler configs (time in sec)1012141618Unprofiled configs (time in sec)
0 2000 4000
Kieker configs (time in sec)1012141618Unprofiled configs (time in sec)Fig. 9: ConÔ¨Åguration-wise execution time of S UNFLOW : black-
box measurements vs. proÔ¨Åling. Left: using JPROFILER , right:
using K IEKER . Different symbols visualized different numeric
values of conÔ¨Åguration option SAMPLES .
coarse-grained proÔ¨Åler (BB vs. CG) and unproÔ¨Åled execution
time compared to using the Ô¨Åne-grained proÔ¨Åler (BB vs. FG).
As expected, rank correlation is higher than linear correlation
across all subject systems and for both proÔ¨Ålers. That is,
the fastest conÔ¨Ågurations remain the fastest independently of
whether we use our learned models for predicting execution
times or measuring execution time. This is good news as
this property has been shown the main tuning objective
for conÔ¨Ågurable systems [57]. Furthermore, we can see that
different subject systems exhibit different correlations. Some
systems, such as B ATIK , PREVAYLER , and C ATENA , exhibit
even nearly perfect linear correlation. For them, the execution
time depends strongly on the conÔ¨Åguration for both types
of experiments: measuring with a proÔ¨Åler and measuring the
overall execution time of a program. There are also subject
systems for which rank correlation is much higher than linear
correlation: D ENSITY CONVERTER , C PD, and P MD. There
are two systems stand out with a generally low correlation:
SUNFLOW and H2, which we analyze next.
Figure 9 shows the dependency between unproÔ¨Åled exe-
cution time and aggregated execution time measured with a
proÔ¨Åler per conÔ¨Åguration for S UNFLOW . We concentrate on
the measurement overhead as the main cause for a low correla-
tion. Interestingly, when highlighting conÔ¨Åguration options, we
observe a strong pattern for the overhead. There are multiple
groups that follow a linear trend, but with different slopes.
The determining factor for this slope is the conÔ¨Ågured value
of the numeric conÔ¨Åguration option SAMPLES . This option is
used as a seed for method calculatePhotons that compute
the global illumination of the scene as part of the rendering
process. By increasing the numeric value for this option, the
overhead increases disproportionally. For H2, there is also a
strong pattern, again, caused by a numeric conÔ¨Åguration option
(ANALYZE AUTO)6. This is important for other studies in this
area: ProÔ¨Åling overhead of conÔ¨Ågurable system depends on
conÔ¨Åguration options.
6More details on the supplementary Website.
1068Answering RQ 3, we observed a generally high rank cor-
relation between system-wide proÔ¨Åled and unproÔ¨Åled per-
formance, demonstrating that the use of a proÔ¨Åler does
not change the relative importance of conÔ¨Åguration options
and that white-box models are able to reveal performance-
relevant methods. Some show even a linear correlation.
A notable exception to this rule is a numeric option in
SUNFLOW , which directly impacts the overhead introduced
by the proÔ¨Åler.
D. Comparison to CONFIG CRUSHER
Closest to our approach is C ONFIG CRUSHER [22]. The main
difference to our approach is that C ONFIG CRUSHER relies
on static taint analysis with the goal of determining which
code regions are affected by which conÔ¨Åguration options to
weave measurement code at according statements. Due to
this conceptual difference, we face both qualitative as well as
technical challenges that render a comparison infeasible. As
explained in Section II-A, there are three ways of propagating
taints through a program: (A) taint every access to the data
structure(s) that hold(s) the conÔ¨Åguration options; (B) stop
tainting at this point; and (C) rewrite the program, such that all
options are stored in individual variables and are accordingly
accessed across the code-base.
Variant C is infeasible in practice and also not in our case, as
a substantial rewrite is not only impractical for larger systems
such as H2, but this would also change the program structure
such that a performance comparison has no longer a common
ground. For the purpose of comparison, we tried variant A
Ô¨Årst, but quickly run into timeouts and memory limitations,
due to the inherent limitations of static code analysis. We
communicated with C ONFIG CRUSHER ‚Äôs main author, who
conÔ¨Årmed our Ô¨Åndings.
In a second attempt, we followed variant B. We again
consulted the main author of C ONFIG CRUSHER for guid-
ance to avoid introducing bias and setting up the subject
system consistently with the original approach. As a result,
we obtained reasonable taints for D ENSITY -CONVERTER . For
H2, the largest of our subject systems, we ran into memory
overÔ¨Çows for all analyzed conÔ¨Åguration options. The analysis
of S UNFLOW , BATIK , and P REVAYLER produced tainted code
regions of size 1, which are basically useless. The reason was
that the conÔ¨Åguration options are immediately stored in a data
structure, leading to a termination of the taint analysis. We
provide all analysis log Ô¨Åles at our supplementary Website.
In addition to the limitations of the taint analysis, C ONFIG -
CRUSHER can handle only binary conÔ¨Åguration options,
whereas our approach can handle numeric options (as we do
in our evaluation). Furthermore, C ONFIG CRUSHER is capable
of tainting only single-threaded applications due to the under-
lying taint analysis. Our approach can produce performance
models also for multi-threaded applications, such as H2.
E. Threats to Validity
The selection of the proÔ¨Åler represents a threat to construct
validity. We mitigated this threat by a pre-study (not shown inthe paper) where we evaluated several proÔ¨Ålers. JP ROFILER is
an industrial-strength proÔ¨Åler with low overhead, which turned
out to be the best choice for the Ô¨Årst phase. However, to obtain
Ô¨Åne-grained performance data, we required more Ô¨Çexibility
and opted for K IEKER . A threat to internal validity arises
from the measurement overhead introduced by the proÔ¨Åler.
We reduce this threat by selecting a low-overhead proÔ¨Åler for
measurement in the coarse-grained step and devoted a whole
research question to analyze its inÔ¨Çuence.
The selection of subject systems threatens external valid-
ity. Although we cannot claim that we can learn white-box
models accurately for all J AVA systems with proper proÔ¨Åling
capabilities. Our results show that this is in principle possible
for a large, industry-relevant branch of conÔ¨Ågurable software
systems.
VII. CONCLUSION
We have proposed an approach to learn white-box
performance-inÔ¨Çuence models at the method level, enabling
tracing conÔ¨Åguration effects from the system level to individ-
ual methods. Based on a pre-study on 3 software systems, we
analyzed possible causes of performance variance of method
execution times to design an integrated proÔ¨Åling and learning
approach. We found that the majority of methods can be easily
learned, as they either do not contribute much to the system‚Äôs
overall performance or do not contribute to conÔ¨Åguration
variance. Based on these insights, we have devised a two step
approach in which we learn performance-inÔ¨Çuence models
for all methods of a software system using a cheap, coarse-
grained proÔ¨Åler in a Ô¨Årst step, and Ô¨Ålter inaccurate and relevant
methods to be measured and learned again with an expensive,
Ô¨Åne-grained proÔ¨Åler in a second step. We found that, despite
the overhead introduced by proÔ¨Åling, the correlation between
proÔ¨Åled and unproÔ¨Åled method execution time is high and
that white-box models can accurately predict the proÔ¨Åled
execution time. More importantly, we were able to show
that performance models at the method level can be used to
pinpoint the contribution of individual conÔ¨Åguration options
and interactions to individual methods, helping developers
to chase conÔ¨Åguration-related performance bugs or to focus
performance testing on speciÔ¨Åc conÔ¨Ågurations.
ACKNOWLEDGMENT
Apel‚Äôs work has been supported by the German Research
Foundation (DFG) under the contract AP 206/11-1. Sieg-
mund‚Äôs work has been supported by the DFG under the
contracts SI 2171/2 and SI 2171/3-1 and by the German
Ministry of Education and Research (BMBF, 01IS19059A and
01IS18026B) by funding the competence center for Big Data
and AI ‚ÄúScaDS.AI Dresden/Leipzig‚Äù. We thank our reviewers
for their thoughtful comments. Especially, we thank Miguel
Velez for his helpful comments on the speciÔ¨Åcs of the taint
analysis and for supporting the set-up of ConÔ¨ÅgCrusher for
comparison.
1069REFERENCES
[1] C. U. Smith, ‚ÄúSoftware performance engineering,‚Äù in Performance
Evaluation of Computer and Communication Systems. Springer, 1993,
pp. 509‚Äì536.
[2] M. Woodside, G. Franks, and D. C. Petriu, ‚ÄúThe future of software
performance engineering,‚Äù in Future of Software Engineering (FOSE).
IEEE, 2007, pp. 171‚Äì187.
[3] N. Siegmund, A. von Rhein, and S. Apel, ‚ÄúFamily-based performance
measurement,‚Äù in Proc. Int. Conf. Generative Programming and Com-
ponent Engineering (GPCE). ACM, 2013, pp. 95‚Äì104.
[4] N. Siegmund, A. Grebhahn, S. Apel, and C. K√§stner, ‚ÄúPerformance-
inÔ¨Çuence models for highly conÔ¨Ågurable systems,‚Äù in Proc. Europ. Soft-
ware Engineering Conference and ACM SIGSOFT Symp. Foundations
of Software Engineering (ESEC/FSE). ACM, 2015, pp. 284‚Äì294.
[5] J. Guo, K. Czarnecki, S. Apel, N. Siegmund, and A. W Àõ asowski,
‚ÄúVariability-aware performance prediction: A statistical learning ap-
proach,‚Äù in Proc. Int. Conf. Automated Software Engineering (ASE) .
IEEE, 2013, pp. 301‚Äì311.
[6] J. Guo, D. Yang, N. Siegmund, S. Apel, A. Sarkar, P. Valov, K. Czar-
necki, A. Wasowski, and H. Yu, ‚ÄúData-efÔ¨Åcient performance learning for
conÔ¨Ågurable systems,‚Äù Empirical Software Engineering (EMSE), vol. 23,
no. 3, pp. 1826‚Äì1867, 2018.
[7] N. Siegmund, M. Rosenm√ºller, C. K√§stner, P. Giarrusso, S. Apel, and
S. Kolesnikov, ‚ÄúScalable prediction of non-functional properties in soft-
ware product lines: Footprint and memory consumption,‚Äù Information
and Software Technology (IST), vol. 55, no. 3, pp. 491‚Äì507, 2013.
[8] A. Sarkar, J. Guo, N. Siegmund, S. Apel, and K. Czarnecki, ‚ÄúCost-
efÔ¨Åcient sampling for performance prediction of conÔ¨Ågurable systems
(t),‚Äù in Proc. Int. Conf. Automated Software Engineering (ASE). IEEE,
2015, pp. 342‚Äì352.
[9] C. Henard, M. Papadakis, M. Harman, and Y . Le Traon, ‚ÄúCombining
multi-objective search and constraint solving for conÔ¨Åguring large soft-
ware product lines,‚Äù in Proc. Int. Conf. Software Engineering (ICSE).
IEEE, 2015, pp. 517‚Äì528.
[10] A. S. Sayyad, J. Ingram, T. Menzies, and H. Ammar, ‚ÄúScalable product
line conÔ¨Åguration: A straw to break the camel‚Äôs back,‚Äù in Proc. Int. Conf.
Automated Software Engineering (ASE). IEEE, 2013, pp. 465‚Äì474.
[11] V . Nair, T. Menzies, N. Siegmund, and S. Apel, ‚ÄúFaster discovery of
faster system conÔ¨Ågurations with spectral learning,‚Äù Automated Software
Engineering, vol. 25, no. 2, pp. 247‚Äì277, 2018.
[12] J. Oh, D. Batory, M. Myers, and N. Siegmund, ‚ÄúFinding near-optimal
conÔ¨Ågurations in product lines by random sampling,‚Äù in Proc. Europ.
Software Engineering Conference and ACM SIGSOFT Symp. Founda-
tions of Software Engineering (ESEC/FSE). ACM, 2017, pp. 61‚Äì71.
[13] S. Wang, C. Li, H. Hoffmann, S. Lu, W. Sentosa, and A. I. Kistijan-
toro, ‚ÄúUnderstanding and auto-adjusting performance-sensitive conÔ¨Åg-
urations,‚Äù in Proc. Int. Conf. Architectural Support for Programming
Languages and Operating Systems (ASPLOS). ACM, 2018, pp. 154‚Äì
168.
[14] D. Shen, Q. Luo, D. Poshyvanyk, and M. Grechanik, ‚ÄúAutomating per-
formance bottleneck detection using search-based application proÔ¨Åling,‚Äù
inProc. Int. Symp. Software Testing and Analysis (ISSTA). ACM, 2015,
pp. 270‚Äì281.
[15] O. Ibidunmoye, F. Hern√°ndez-Rodriguez, and E. Elmroth, ‚ÄúPerformance
anomaly detection and bottleneck identiÔ¨Åcation,‚Äù ACM Computing Sur-
veys, pp. 1‚Äì35, 2015.
[16] G. Jin, L. Song, X. Shi, J. Scherpelz, and S. Lu, ‚ÄúUnderstanding and
detecting real-world performance bugs,‚Äù Proc. Int. Conf. Programming
Language Design and Implementation (PLDI), vol. 47, pp. 77‚Äì88, 2012.
[17] X. Han and T. Yu, ‚ÄúAn empirical study on performance bugs for highly
conÔ¨Ågurable software systems,‚Äù in Proc. Int. Symp. Empirical Software
Engineering and Measurement (ESEM). ACM, 2016, pp. 1‚Äì10.
[18] A. Van Hoorn, J. Waller, and W. Hasselbring, ‚ÄúKieker: A framework for
application performance monitoring and dynamic software analysis,‚Äù in
Proc. Int. Conf. Performance Engineering (ICPE). ACM, 2012, pp.
247‚Äì248.
[19] M. Selakovic, T. Glaser, and M. Pradel, ‚ÄúAn actionable performance
proÔ¨Åler for optimizing the order of evaluations,‚Äù in Proc. Int. Symp.
Software Testing and Analysis (ISSTA). ACM, 2017, pp. 170‚Äì180.
[20] N. Snellman, A. Ashraf, and I. Porres, ‚ÄúTowards automatic performance
and scalability testing of rich Internet applications in the cloud,‚Äù in Proc.
Europ. Conf. Software Engineering and Advanced Applications (SEAA).
IEEE, 2011, pp. 161‚Äì169.[21] M. B. Chhetri, S. Chichin, Q. B. V o, and R. Kowalczyk, ‚ÄúSmart
cloudbench‚Äìautomated performance benchmarking of the cloud,‚Äù in
Proc. Int. Conf. Cloud Computing (CLOUD). IEEE, 2013, pp. 414‚Äì421.
[22] M. Velez, P. Jamshidi, F. Sattler, N. Siegmund, S. Apel, and C. K√§stner,
‚ÄúConÔ¨ÅgCrusher: towards white-box performance analysis for conÔ¨Åg-
urable systems,‚Äù Automated Software Engineering, vol. 27, no. 3, pp.
265‚Äì300, 2020.
[23] M. Velez, P. Jamshidi, N. Siegmund, S. Apel, and C. K√§stner, ‚ÄúWhite-
box analysis over machine learning: Modeling performance of conÔ¨Åg-
urable systems,‚Äù in Proc. Int. Conf. Software Engineering (ICSE). IEEE,
2021.
[24] F. Medeiros, C. K√§stner, M. Ribeiro, R. Gheyi, and S. Apel, ‚ÄúA
comparison of 10 sampling algorithms for conÔ¨Ågurable systems,‚Äù in
Proc. Int. Conf. Software Engineering (ICSE). IEEE, 2016, pp. 643‚Äì
654.
[25] E. Reisner, C. Song, K.-K. Ma, J. S. Foster, and A. Porter, ‚ÄúUsing
symbolic evaluation to understand behavior in conÔ¨Ågurable software
systems,‚Äù in Proc. Int. Conf. Software Engineering (ICSE). ACM,
2010, pp. 445‚Äì454.
[26] J. Meinicke, C.-P. Wong, C. K√§stner, T. Th√ºm, and G. Saake, ‚ÄúOn
essential conÔ¨Åguration complexity: Measuring interactions in highly-
conÔ¨Ågurable systems,‚Äù in Proc. Int. Conf. Automated Software Engi-
neering (ASE). ACM, 2016, pp. 483‚Äì494.
[27] H. Hoffmann, S. Sidiroglou, M. Carbin, S. Misailovic, A. Agarwal, and
M. Rinard, ‚ÄúDynamic knobs for responsive power-aware computing,‚Äù in
Proc. Int. Conf. Architectural Support for Programming Languages and
Operating Systems (ASPLOS). ACM, 2011, pp. 199‚Äì212.
[28] T. Th√ºm, S. Apel, C. K√§stner, I. Schaefer, and G. Saake, ‚ÄúA classiÔ¨Åcation
and survey of analysis strategies for software product lines,‚Äù ACM
Computing Surveys, vol. 47, no. 1, pp. 1‚Äì45, 2014.
[29] A. von Rhein, T. Th√ºm, I. Schaefer, J. Liebig, and S. Apel, ‚ÄúVariability
encoding: From compile-time to load-time variability,‚Äù Journal of Logi-
cal and Algebraic Methods in Programming, vol. 85, no. 1, pp. 125‚Äì145,
2016.
[30] A. von Rhein, J. Liebig, A. Janker, C. K√§stner, and S. Apel, ‚ÄúVariability-
aware static analysis at scale: An empirical study,‚Äù ACM Trans. Software
Engineering and Methodology (TOSEM), vol. 27, no. 4, pp. 1‚Äì33, 2018.
[31] M. Lillack, C. K√§stner, and E. Bodden, ‚ÄúTracking load-time conÔ¨Ågura-
tion options,‚Äù IEEE Trans. Software Engineering (TSE), vol. 44, no. 12,
pp. 1269‚Äì1291, 2018.
[32] V . Avdiienko, K. Kuznetsov, A. Gorla, A. Zeller, S. Arzt, S. Rasthofer,
and E. Bodden, ‚ÄúMining apps for abnormal usage of sensitive data,‚Äù in
Proc. Int. Conf. Software Engineering (ICSE). IEEE, 2015, pp. 426‚Äì
436.
[33] C. Kaltenecker, A. Grebhahn, N. Siegmund, and S. Apel, ‚ÄúThe interplay
of sampling and machine learning for software performance prediction,‚Äù
IEEE Software, vol. 37, no. 4, pp. 58‚Äì66, 2020.
[34] M. F. Johansen, √ò. Haugen, and F. Fleurey, ‚ÄúAn algorithm for generating
t-wise covering arrays from large feature models,‚Äù in Proc. Int. Software
Product Line Conference (SPLC). ACM, 2012, pp. 46‚Äì55.
[35] D. Marijan, A. Gotlieb, S. Sen, and A. Hervieu, ‚ÄúPractical pairwise
testing for software product lines,‚Äù in Proc. Int. Software Product Line
Conference (SPLC). ACM, 2013, pp. 227‚Äì235.
[36] C. Kaltenecker, A. Grebhahn, N. Siegmund, J. Guo, and S. Apel,
‚ÄúDistance-based sampling of software conÔ¨Åguration spaces,‚Äù in Proc.
Int. Conf. Software Engineering (ICSE). IEEE, 2019, pp. 1084‚Äì1094.
[37] M. Courtois and M. Woodside, ‚ÄúUsing regression splines for software
performance analysis,‚Äù in Proc. Int. Workshop Software and Performance
(WOSP). ACM, 2000, pp. 105‚Äì114.
[38] T. A. Israr, D. H. Lau, G. Franks, and M. Woodside, ‚ÄúAutomatic gener-
ation of layered queuing software performance models from commonly
available traces,‚Äù in Proc. Int. Workshop Software and Performance
(WOSP). ACM, 2005, pp. 147‚Äì158.
[39] A. Mizan and G. Franks, ‚ÄúAn automatic trace based performance
evaluation model building for parallel distributed systems,‚Äù in Proc. Int.
Conf. Performance Engineering (ICPE). ACM, 2011, pp. 61‚Äì72.
[40] D. Westermann, J. Happe, R. Krebs, and R. Farahbod, ‚ÄúAutomated
inference of goal-oriented performance prediction functions,‚Äù in Proc.
Int. Conf. Automated Software Engineering (ASE). ACM, 2012, pp.
190‚Äì199.
[41] K. Krogmann, M. Kuperberg, and R. Reussner, ‚ÄúUsing genetic search
for reverse engineering of parametric behavior models for performance
prediction,‚Äù IEEE Transactions on Software Engineering, vol. 36, no. 6,
pp. 865‚Äì877, 2010.
1070[42] V . Ackermann, J. Grohmann, S. Eismann, and S. Kounev, ‚ÄúBlackbox
learning of parametric dependencies for performance models,‚Äù in MOD-
ELS Workshops, 2018.
[43] J. Grohmann, S. Eismann, S. ElÔ¨Çein, J. V . Kistowski, S. Kounev,
and M. Mazkatli, ‚ÄúDetecting parametric dependencies for performance
models using feature selection techniques,‚Äù in Proc. Int. Symp. Modeling,
Analysis, and Simulation of Computer and Telecommunication Systems
(MASCOTS). IEEE, 2019, pp. 309‚Äì322.
[44] N. Siegmund, M. Rosenmuller, C. Kastner, P. G. Giarrusso, S. Apel, and
S. S. Kolesnikov, ‚ÄúScalable prediction of non-functional properties in
software product lines,‚Äù in Proc. Int. Software Product Line Conference
(SPLC). IEEE, 2011, pp. 160‚Äì169.
[45] N. Siegmund, S. S. Kolesnikov, C. K√§stner, S. Apel, D. Batory,
M. Rosenm√ºller, and G. Saake, ‚ÄúPredicting performance via automated
feature-interaction detection,‚Äù in Proc. Int. Conf. Software Engineering
(ICSE). IEEE, 2012, pp. 167‚Äì177.
[46] V . Nair, Z. Yu, T. Menzies, N. Siegmund, and S. Apel, ‚ÄúFinding faster
conÔ¨Ågurations using FLASH,‚Äù IEEE Trans. Software Engineering (TSE),
vol. 46, no. 7, pp. 794‚Äì811, 2018.
[47] J. Du, N. Sehrawat, and W. Zwaenepoel, ‚ÄúPerformance proÔ¨Åling of
virtual machines,‚Äù in Proc. Int. Conf. Virtual Execution Environments
(VEE). ACM, 2011, pp. 3‚Äì14.
[48] T. Mytkowicz, A. Diwan, M. Hauswirth, and P. F. Sweeney, ‚ÄúEvaluating
the accuracy of Java proÔ¨Ålers,‚Äù in Proc. Int. Conf. Programming Lan-
guage Design and Implementation (PLDI). ACM, 2010, pp. 187‚Äì197.
[49] L. Song and S. Lu, ‚ÄúPerformance diagnosis for inefÔ¨Åcient loops,‚Äù in
Proc. Int. Conf. Software Engineering (ICSE). IEEE, 2017, pp. 370‚Äì
380.
[50] B. Everitt and A. Skrondal, The Cambridge Dictionary of Statistics.
Cambridge University Press, 2002, vol. 106.
[51] N. S. Pillai and X.-L. Meng, ‚ÄúAn unexpected encounter with Cauchy
and L√©vy,‚Äù The Annals of Statistics, pp. 2089‚Äì2097, 2016.
[52] T. S. Ferguson, ‚ÄúMaximum likelihood estimates of the parameters of
the Cauchy distribution for samples of size 3 and 4,‚Äù Journal of the
American Statistical Association, vol. 73, no. 361, pp. 211‚Äì213, 1978.
[53] J. A. Reeds, ‚ÄúAsymptotic number of roots of Cauchy location likelihood
equations,‚Äù The Annals of Statistics, pp. 775‚Äì784, 1985.
[54] J. Wang and C. J. Wu, ‚ÄúA hidden projection property of Plackett-Burman
and related designs,‚Äù Statistica Sinica, pp. 235‚Äì250, 1995.
[55] S. M. Blackburn, R. Garner, C. Hoffmann, A. M. Khan, K. S. McKinley,
R. Bentzur, A. Diwan, D. Feinberg, D. Frampton, S. Z. Guyer, M. Hirzel,
A. L. Hosking, M. Jump, H. B. Lee, J. E. B. Moss, A. Phansalkar, D. Ste-
fanovic, T. VanDrunen, D. von Dincklage, and B. Wiedermann, ‚ÄúThe
DaCapo benchmarks: Java benchmarking development and analysis,‚Äù
inProc. Int. Conf. Object-Oriented Programming, Systems, Languages,
and Applications (OOPSLA). ACM, 2006, pp. 169‚Äì190.
[56] P. Geurts, D. Ernst, and L. Wehenkel, ‚ÄúExtremely randomized trees,‚Äù
Machine learning, vol. 63, no. 1, pp. 3‚Äì42, 2006.
[57] V . Nair, T. Menzies, N. Siegmund, and S. Apel, ‚ÄúUsing bad learners to
Ô¨Ånd good conÔ¨Ågurations,‚Äù in Proc. Europ. Software Engineering Con-
ference and ACM SIGSOFT Symp. Foundations of Software Engineering
(ESEC/FSE). ACM, 2017, pp. 257‚Äì267.
1071