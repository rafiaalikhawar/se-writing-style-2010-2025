CLEAR: C ontrastive Le arning for A PI Recommendation
Moshi Wei
York University
Toronto, Canada
moshiwei@yorku.caNima Shiri Harzevili
York University
Toronto, Canada
nshiri@yorku.caYuchao Huang
Institute of Software Chinese
Academy of Sciences
yuchao2019@iscas.ac.cn
Junjie Wang
Institute of Software Chinese
Academy of Sciences
junjie@iscas.ac.cnSong Wang
York University
Toronto, Canada
wangsong@yorku.ca
ABSTRACT
Automatic API recommendation has been studied for years. There
aretwoorthogonallinesofapproachesforthistask,i.e.,information-
retrieval-based (IR-based) and neural-based methods. Although
theseapproacheswerereportedhavingremarkableperformance,
our observation shows that existing approaches can fail due to
the following tworeasons: 1) most IR-based approachestreat task
queries as bag-of-words and use word embedding to represent
queries,whichcannotcapturethesequentialsemanticinformation.
2) both the IR-based and the neural-based approaches are weak
atdistinguishingthesemanticdifferenceamonglexicallysimilar
queries.
In this paper, we propose CLEAR, which leverages BERT sen-
tenceembeddingandcontrastivelearningtotackletheabovetwois-
sues.Specifically,CLEARembedsthewholesentenceofqueriesandStackOverflow(SO)postswithaBERT-basedmodelratherthanthebag-of-word-basedwordembeddingmodel,whichcanpreservethe
semantic-related sequential information. In addition, CLEAR uses
contrastive learning to train the BERT-based embedding model for
learning precise semantic representation of programming termi-
nologies regardless of their lexical information. CLEAR also builds
a BERT-based re-ranking model to optimize its recommendationresults. Given a query, CLEAR first selects a set of candidate SO
posts via the BERT sentence embedding-based similarity to reduce
search space. CLEAR further leverages a BERT-based re-ranking
model to rank candidate SOposts and recommends the APIs from
the ranked top SO posts for the query.
Our experimentresults on three differenttest datasets confirm
theeffectivenessofCLEARforbothmethod-levelandclass-level
API recommendation. Compared to the state-of-the-art API recom-
mendation approaches, CLEAR improves the MAP by 25%-187% at
method-level and 10%-100% at class-level.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation
on the first page. Copyrights for components of this work owned by others than ACMmustbehonored.Abstractingwithcreditispermitted.Tocopyotherwise,orrepublish,topostonserversortoredistributetolists,requirespriorspecificpermissionand/ora
fee. Request permissions from permissions@acm.org.
ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
Â© 2022 Association for Computing Machinery.
ACM ISBN 978-1-4503-9221-1/22/05...$15.00
https://doi.org/10.1145/3510003.3510159CCS CONCEPTS
â€¢Computingmethodologies â†’Semanticnetworks ;â€¢Applied
computing â†’Document searching ;â€¢Information systems
â†’Recommender systems.
KEYWORDS
API recommendation, contrastive learning, semantic difference
ACM Reference Format:
Moshi Wei, Nima Shiri Harzevili, Yuchao Huang, Junjie Wang, and Song
Wang. 2022. CLEAR: Contrastive Learning for APIRecommendation. In
44thInternationalConferenceonSoftwareEngineering(ICSEâ€™22),May21â€“
29, 2022, Pittsburgh, PA, USA. ACM, New York, NY, USA, 12 pages. https:
//doi.org/10.1145/3510003.3510159
1 INTRODUCTION
Overthepastdecades,open-sourcesoftwaredevelopmenthasre-
ceived extensive attention from the software engineering com-
munity.Thisattentionleadstoatremendousdemandforalready
devised libraries or APIs which facilitate software development
and maintenance. Developers often search for existing APIs or
codesnippetsontheInternettoobtainthefunctionstheywishto
implement [51].
To help with API search, many automated API recommendation
approacheshavebeenproposed[ 11,14,16,22,24,34,35,39].There
aretwoorthogonallinesofapproachesforthistask,i.e.,informa-
tion retrieval based, e.g., BIKER [ 14], and neural-based methods,
e.g.,DeepAPI[ 11].BIKER[ 14]usesbag-of-word-basedwordem-
bedding (i.e., a word2vec model built on Java SO posts) and IDF
(inversedocumentfrequency)vocabularytocalculatethesimilarity
scorebetweentwotextdescriptionsandthenleveragesaqueryâ€™s
similaritywithbothSOpostsandAPIdocumentationstorecom-
mend appropriate APIs for the query. DeepAPI [ 11] formulates the
APIrecommendationasamachinetranslationproblem,i.e.,givena
naturallanguagequery,itaimstotranslateitintoanAPIsequence.
Specifically, it adapts a Recurrent Neural Network (RNN) Encoder-
Decodermodeltoencodeaqueryintoafixed-lengthcontextvector
and recommends an API sequence based on the context vector for
the query. Although these approaches achieved remarkable perfor-
mance, by replicating these studies, we found two major problems
that can affect their effectiveness.
3762022 IEEE/ACM 44th International Conference on Software Engineering (ICSE)
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 07:54:11 UTC from IEEE Xplore.  Restrictions apply. ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA Moshi Wei, Nima Shiri Harzevili, Yuchao Huang, Junjie Wang, and Song Wang
The first problem is that these IR-based approaches (e.g., BIKER)
treatqueriesandSOpostsasbag-of-wordsandusewordembed-
ding to represent queries [ 12], which cannot capture the semantic-
related sequential information. For example, given a real-world
query â€œConvert String to Calendar Object in Java â€1, BIKER can-
not recommend the correct API and the top API recommended by
BIKER is â€œ java.time.LocalDate.parse â€ from the most similar
post identified by BIKER, i.e., â€œConvert Java Gregorian Calendar to
Stringâ€2, whose intent is opposite to the intent of the query. BIKER
fails to retrieve the correct answer for the above query becauseof its bag-of-words-based representation, which cannot capture
the semantic-related sequential information. To properly represent
the semantic-related sequential information of the text descrip-
tions, the embedding of queries and SO posts has to be considered
comprehensively instead of using bag-of-words.
The second problem is that both the IR-based and the neural-
basedapproachesareweakatdistinguishingthesemanticdifference
amongquiresthatarelexicallysimilar.Forexample,givenareal-
worldqueryâ€œFilereader.read()methodnotworking â€3,neitherBIKER
nor DeepAPI can recommend a correct API. Specifically, The most
likelyAPIrecommendedbyBIKERisâ€œ java.io.RandomAccessFile
.readâ€fromthepostâ€œBufferedReaderread()notworking â€4,asthe
text descriptions of the query and the post are almost identical ex-
cept the terminology â€œ Filereader â€ and â€œ BufferedReader â€. How-
ever, the answer to this query is â€œjava.io.OutputStreamWriter.flush
â€. The root cause of such a failure of BIKER is that the
two quires are lexically close but semantically different. BIKERâ€™s
word2vec embedding relies on the context of the words in a text
description.However,theaboveexampleshowsthatonlyusingthecontextofthewordsisnotenoughtodistinguishthesemanticofthe
query in API recommendation tasks. For DeepAPI, we experiment
withtheabovetwoqueriesâ€œFilereader.read()methodnotworking â€
andâ€œBufferedReaderread()not working â€,whileDeepAPI generates
thesameAPIsequenceforbothqueries,i.e.,{â€œString.lengthâ€,â€œOb-
ject.toStringâ€ },whichisincorrectasthesetwoquerieshavedifferent
semantics.OneofthereasonsforsuchafailureisthatDeepAPIuses
an RNN Encoder-Decoder base architecture to encode every query
intoafixed-lengthcontextvectorandgeneratesanAPIsequence
basedontheoverallcontextofthequery.Thus,duetotheabove
nature of RNN, DeepAPI often fails for similar queries that have
different key words [1, 10].
To alleviate the above two problems, we propose CLEAR, an
API recommendation approach based on BERT sentence embed-
ding [7] and contrastive learning [ 25]. Specifically, to solve the
first issue, CLEAR uses a BERT-based model to embed text de-
scriptions of queries and SO posts, which produces the embedding
of the whole sentence of an API query while taking sequential
informationintoconsiderationratherthancombiningtheembed-
ding of each word (i.e., bag-of-words). For solving the second issue,
CLEARusescontrastivelearningtotraintheBERTsentenceem-
bedding model for learning semantically equivalent representation
1https://stackoverflow.com/questions/5301226/convert-string-to-calendar-object-in-
java
2https://stackoverflow.com/questions/24741696/convert-java-gregorian-calendar-to-
string
3https://stackoverflow.com/questions/36427839/filereader-read-method-not-
working
4https://stackoverflow.com/questions/43190995/bufferedreader-read-not-workingofqueriesorSOpostsregardlesstheirlexicalinformation.Given
a query, CLEAR first selects a set of candidate SO posts via the
BERT sentence embedding-based similarity to reduce search space.
CLEARfurtherleverages aBERT-basedclassification modeltore-
rank candidate SO posts and recommend the APIs from the ranked
top SO posts for the query.
InordertoevaluatetheeffectivenessofCLEAR,were-usethe
dataset from BIKER [ 14]. Specifically, we have developed three test
sets derived from BIKERâ€™s datasetfor testing. The first is BIKERâ€™s
manuallycreatedtestdataset,thesecondisrandomlyselected1k
sample SO posts to alleviate potential human bias. Since around
10% posts in SO contain multiple APIs, in order to test the per-
formanceonthescenarioofmulti-APIanswers,weaddedathird
testdataset,whichis1KrandomlyselectedsampleSOpostswith
multiple APIs in answers. We use the corpus that excludes thesetesting data as our training dataset to train CLEAR. The resultsshow that CLEAR outperforms the state-of-the-art information
retrievalbasedandneural-basedapproaches(i.e.,BIKER[ 14]and
DeepAPI [ 11] respectively) significantly at both method-leveland
class-level on all three test sets. We also conduct a case study to
evaluateCLEARagainstthelatestSOposts,andtheresultsconfirm
the effectiveness and practical values of CLEAR. This paper makes
the following contributions:
â€¢WeproposeCLEAR,anovelAPIrecommendationapproach,
which usesthe BERT sentence embeddingmodel to repre-
sent queries for capturing sequential semantic information
and leverages contrastive training to train the BERT model
for learning precise semantic representation of queries re-
gardless of their lexical information.
â€¢We evaluate CLEAR using three different test datasets, in-
cludingtestdatafrompreviousstudies,1krandomlyselectedSOposts,and1krandomlyselectedSOpostswithmulti-API
answers. Our experiment results confirm that CLEAR can
significantly outperform the state-of-the-art baselines.
â€¢WeconductacasestudyonthelatestSOpoststoevaluatetheperformance of CLEAR and our results suggest the practical
value of CLEAR.
â€¢We release the source code of CLEAR and the dataset of our
experimentstohelpotherresearchersreplicateandextend
our study5.
Therestofthispaperisstructuredasfollows.Section2describes
the background ofthis study. Section 3presents the framework of
the proposed CLEAR. Section 4 introduces experimental design,
baselines,andresearchquestions.Section5analyzestheexperiment
results.Section6discussesopenquestionsandthethreatstothe
validityofthiswork.Section7surveystherelatedworkandSection
8 summarizes this paper.
2 BACKGROUND
2.1 Language Embedding
Language embedding technique is a method for converting words
orsentencesintonumericalvectors[ 7,28,36].Thedeeplearning-
based language models have been widely examined to be useful
in capturing implicit semantics for natural language sentences.
5Replication package link: https://github.com/Moshiii/CLEAR-replication
377
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 07:54:11 UTC from IEEE Xplore.  Restrictions apply. CLEAR: C ontrastive Le arning for A PI Recommendation ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
There exist studies of language embedding on both word-level [ 20,
27]andsentence-level[ 7,28,36].Typicaldeeplearninglanguage
embedding models include GPT [32] and BERT [7].
GPT [32] introduces minimal task-specific parameters and is
trained on the downstream tasks by simply fine-tuning all pre-
trainedparameters.BERT[ 7]isdeeplearninglanguageembedding
based on transformer units. It uses a 12-layer or 24-layer trans-
former layer with a multi-head attention mechanism as feature
extraction,andthenusesaregressionfunctiontogeneratethefinal
output.BERTmodelcanbeusedformultipletasks,e.g.,sentence
embedding, classification, question-answer tasks, sentence tagging,
etc., with different minor adaptions [7].
In this paper, we use the BERT model for two different tasks.
First, we use BERT as sentence embedding to represent the text
of queries and SO posts for preserving their semantic information
regardlessoftheirlexicalinformation.Second,weuseBERTasa
binary SO post classifier to re-rank the retrieved SO posts for a
givenquery.Forboththetwotasks,weuseRoBERTamodel,astate-of-the-artBERTvariant[
17].Forsentenceembedding,weadoptthe
contrastivelearningprocesstotrainthemodel,weprovideaninputsampletothemodelandtaketheoutputvectorofthemodelasthe
sentenceembeddingoftheinput.Forre-rankingposts,following
existingwork[ 7],weusethejointembeddingtrainingprocessto
train the classifier, which takes paired posts as input and the label
is whether or not they have the same APIs.
ThedifferenceofRoBERTatotheoriginalBERTmodelisthat
RoBERTaapplieddifferenttrainingprocessesanddistillationsintraining [
29], which reduces the number of parameters while in-
creasing the robustness of the BERT model.
2.2 Contrastive Learning
Contrastivelearning[ 25]isadeepneuralnetworktrainingprocess
that takes paired sentences as input and uses the similarity inthe paired sentences as labels. The training goal is to learn therelationship between sentences, i.e., whether two sentences aresemantically similar regardless of their lexical similarity. Hoffer
etal. [13]proposedthe tripletnetworkforcontrastive training.It
requires a triplet (ğ‘†,ğ‘ƒ,ğ‘)as the input, where ğ‘†corresponds to the
originalquery, ğ‘ƒreferstothepositiveequivalentof ğ‘†,andğ‘isthe
negative one.
Inthiswork,weusecontrastivelearningtotrainaRoBERTa[ 17]
modelforsentenceembedding.Foragivenpostinthetrainingdata,itspositivepostsarepostswiththesameanswerandnegativeposts
are posts with different answers.
2.3 Joint Embedding Training
Joint embedding training [ 7] was widely used to train BERT as
a classification model. Figure 1 shows the architecture of jointembedding training for BERT. BERT [
7] provides a special token
[ğ‘†ğ¸ğ‘ƒ],whichallowstwopoststobeconcatenatedasinput.Injoint
embedding training, [ğ‘†ğ¸ğ‘ƒ]is used to identifythe end of the first
post. The process of joint embedding training is fine-tuning the
modelwithpairsofpoststothetargetthatifgiventwosemantic
equivalent posts, the model returns 1, otherwise returns 0. Theloss function we use for joint embedding training is the classic
Figure 1: Joint embedding training
cross-entropy loss function (i.e., ğ¿ğ‘œğ‘ ğ‘ ):
ğ¿ğ‘œğ‘ ğ‘ =âˆ’(ğ‘¦âˆ—ğ‘™ğ‘œğ‘”(Ë†ğ‘¦)+(1âˆ’ğ‘¦)âˆ—ğ‘™ğ‘œğ‘”(1âˆ’Ë†ğ‘¦)) (1)
whereğ‘¦indicates whether the given two posts are semantically
equivalent, and Ë†ğ‘¦is the prediction of the re-ranking model.
In this work, we leverage joint embedding training to train a
RoBERTa based classification model to re-rank the retrieved SO
posts for a given query.
3 APPROACH
Figure 2 shows the pipeline of CLEAR, which consists of two parts:
language model building (section 3.1) and searching relevant APIs
(section 3.2). The language model building process contains four
steps, i.e., post triplets construction (Section 3.1.1), BERT sentence
embeddingwithcontrastivelearning(Section3.1.2),candidateposts
filtering (Section 3.1.3), and the joint embedding training based re-
ranking model (Section 3.1.4).
3.1 Building BERT-base Language Models
3.1.1 Post Triplets Construction .The format of the training
datausedinthecontrastivetrainingprocessisdifferentfromthe
traditionalnaturallanguageprocessingtasks,e.g.,sentimentanaly-
sis,wheretheinputsaresentencesandtheoutputsarethelabels.
Contrastivetrainingrequirestripletsasinputs[ 13].Everysingle
triplet is a combination of three posts, which are an input query ğ‘†,
a positive sample post ğ‘ƒthat is semantically equivalent to ğ‘†, and a
negativesamplepost ğ‘thatisnotrelatedto ğ‘†andğ‘ƒ.Therefore,the
trainingcorpusneedstobeconvertedtotriplets.Forexample,givenaninputqueryâ€œJavastringsplitwithmultipledelimeters â€,thetriplet
(ğ‘†,ğ‘ƒ,ğ‘)can be (â€œJava string split with multiple delimeters â€,â€œHow
tosplit apathusing StringTokenizer? â€,â€œHowto loadafile acrossthe
network and handle it as a String â€). Algorithm 1 shows the process
of generating training triplets.
Ourtripletsgenerationalgorithmhastwoparameters,i.e., ğ‘is
the number of positive samples and ğ‘›is the number of negative
samplingforatraininginstance.Whengeneratingthetriplets,eachquestionneedstobepairedwithpositiveandnegativesamples.For
each question ğ‘–ğ‘¡ğ‘’ğ‘šinğ‘‡, we use function get_equivalent_subset()
to get its positive posts, i.e., posts that have the same answer with
ğ‘–ğ‘¡ğ‘’ğ‘š. In addition, we consider posts that have a different answer
fromğ‘–ğ‘¡ğ‘’ğ‘šas the negative posts of ğ‘–ğ‘¡ğ‘’ğ‘š.
378
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 07:54:11 UTC from IEEE Xplore.  Restrictions apply. ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA Moshi Wei, Nima Shiri Harzevili, Yuchao Huang, Junjie Wang, and Song Wang
Figure 2: Overview of our proposed CLEAR.
Algorithm 1: Triplets Generator
Result:Tuple list of element (S, P, N)
defgetTriplets(p: int, n: int, T: list <question, answer >):
result_list = new list() // initialize empty result list
foritem in T do
S = item[0] //question sentence
answer = item[1]
T_P= get_equivalent_subset(T, answer)
T_N = set(T) - set(T_P)
P_list = random_sample(T_P, p)N_list = random_sample(T_N, n)foritem P in P_list do
foritem N in N_list do
result_list.append(S, P, N)
end
end
endreturn result_list
Note that, the ratio between positive and negative samples is
important in contrastive training, different configurations may im-
pacttheresultsignificantly[ 15,41].Inouralgorithm,Tofindthe
bestconfigurations,i.e., ğ‘andğ‘›,weperformagridsearchwitha
listofcandidatevaluesforboth ğ‘andğ‘›,whichare1,3,5,10,and
15. We use ğ‘andğ‘›that can achieve the best performance in our
experiment(detailsareinSection4.2).ForAPIsthatdonothave
ğ‘positivesamples,weusealltheirpositivesamples.Weperform
randomsamplingontheAPIsthatexceed ğ‘orğ‘›tolimitthenumber
of positive or negative samples.
3.1.2 BERT Sentence Embedding Model .Inthis step,weuse
contrastivetrainingtotraintheRoBERTabasedsentenceembed-
dingmodelwiththeposttripletscreatedinSection3.1.1.Thegoalof
this process is to learn a semantic presentation, with which similar
samplesstayclosetoeachother,whiledissimilaronesarefarapart.
Figure 4 shows an illustration for this process. In the Figure, greenpoints are positive posts that have the same API â€œ
Arrays.asList â€
with query ğ‘†, and the red points are negative posts of ğ‘†. With
contrastivelearning,thecentergreenpoint ğ‘†playstheroleofan
anchor, the positive samples are pulled towards the anchor and the
negative samples are pushed away from the anchor.
Figure3:ArchitectureforcontrastivelytrainningRoBERTabased sentence embedding model
Figure 3 shows the architecture of the contrastive learning used
inourwork,inwhichtheRoBERTa[ 17]modelisthebasemodel
for sentence embedding, and we use a Pooling layer to connect the
RoBERTa model and the triple network. Triple network has two
layers, the first layer is three identical deep neural network models
for feature extraction of input sentences. The feature extractionlayer can also be replaced with other models or algorithms. The
secondlayerofthetripletnetworkisalossfunctionbasedonthe
cosine distance operator. The purpose of the loss function is to
minimizethedistancebetweensimilarsentencesandmaximizethe
distance between unrelated sentences. The training objective is to
fine-tune the network so that the distance between the question
ğ‘†andthepositivequestion ğ‘ƒiscloserthanthedistancebetween
the question ğ‘†and the negative question ğ‘. Formally, the training
objective is to minimize the following function:
ğ‘šğ‘ğ‘¥(||ğ¸ğ‘ âˆ’ğ¸ğ‘||âˆ’||ğ¸ğ‘ âˆ’ğ¸ğ‘›||+ğœ–,0) (2)
whereğ¸ğ‘ ,ğ¸ğ‘, andğ¸ğ‘›are the sentence embeddings of question ğ‘†,
ğ‘ƒ, andğ‘.ğœ–is the margin of the distance between ğ‘†andğ‘.B y
default,ğœ–is set to 1, which means that the cosine distance between
a question and its irrelevant question should be 1.
379
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 07:54:11 UTC from IEEE Xplore.  Restrictions apply. CLEAR: C ontrastive Le arning for A PI Recommendation ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
S
N
NP
P
how to convert foreign characters in java?
java.lang.Integer.toStringhow to convert comma-separated string to list?java.util.Arrays.asListhow to initialize list<string> object in java?java.util.Arrays.asListconverting array to list in javajava.util.Arrays.asList
how to convert a number to a string collectionjava.lang.Integer.toString
Figure 4: Contrastive training for a single post.
3.1.3 Candidate Posts Filter .In this step, with the BERT sen-
tenceembedding built-inSection3.1.2, wefurther filteroutirrele-
vantquestionsforagivenqueryandkeeptop-kquestionsforde-
tectingcandidateAPIs.Followingexistingwork[ 14],CLEARkeeps
top-50similarquestionsasthecandidates,sinceretrievingtoomany
questionscanintroducenoisetotherecommendationprocess.In
thisstep,weusetheeuclideandistancebetween twoquestionsas
the metric to filter out irrelevant questions.
Note that, although our experimental results show that directly
using the 50 candidate questions from the filter for API recom-
mendationcanachievebetterperformancethanbothBIKERand
DeepAPI (details are in Section 5.1), we observe that there exist
noisy questions in the retrieved 50 candidate questions from the
filter(oneofthepossiblereasonsisthelowqualityofSOposts[ 30]),
whichcouldhurttheperformanceofAPIrecommendation.Thus,a
re-ranking model for the candidate posts is needed and details are
in the next section.
3.1.4 Candidate Post Re-ranking Model .Theobjectiveofour
filtermodel(detailsareinSection3.1.3)istofilteroutthenumberof
irrelevant posts from the entire search space, while this re-ranking
model is to optimize the ranking of the left ğ‘˜candidate posts from
the filter model.
Forsemanticembeddingre-rankingtasks,wechoosethesame
BERT model, i.e., RoBERTa [ 17], the state-of-the-art BERT-based
model for semantic embedding re-ranking tasks, as the base model.
Then,wefine-tuneitwithjointembeddingtraining,whichturns
theRoBERTaintoaclassificationmodel(detailsareinSection2),
thelabeliswhethertwopostshavethesameAPIs.Fortrainingthe
model,wefirstusethefiltermodel(detailsareinSection3.1.3)to
find the top-50 similar posts, i.e., ğ‘‡ğ‘ ={ğ‘ 1,ğ‘ 2,ğ‘ 3,...ğ‘ 50}, for a postğ‘
inour trainingdataset.Wethencreate50 pairsfromthepost, i.e.,
ğ‘ğ‘ğ‘–ğ‘Ÿğ‘  ={<ğ‘,ğ‘ 1>,<ğ‘,ğ‘ 2>,<ğ‘,ğ‘ 3>,...<ğ‘,ğ‘ 50>}andthelabel
of each pair is whether they have the same APIs. In total, we have
around 1.7M pairs to train the RoBERTa based classification model.
We use the predicted possibility to rank the 50 candidate posts.
3.2 Search APIs
Given a natural language described query ğ‘„, the first step is to re-
trieve the top-k candidate questions from SO. CLEAR first uses the
trained RoBERTa based sentence embedding model to transform itintoanembedding. CLEARthenusesthefilter modeltofilterout
irrelevantpostsandgetalistofcandidatesâ€™postsbasedontheBERTsentence embedding. Then, in the re-ranking phase, the re-ranking
model calculates the probability that ğ‘„and a given candidate post
havethesamelabel,weusetheprobabilitytorankthe50candidate
posts. We then extract the APIs from the ranked posts and output
themastherecommendationtothequery ğ‘„.Afterobtainingthe
ranked list of candidate APIs, CLEAR also summarizes supplemen-
tary information for ğ‘„to describe the API usage examples and
help users decide which API should be chosen for their tasks. The
supplementary information summarized by CLEAR considers two
aspects, i.e., the title of similar questions and code snippets from
these questions.
Note that, CLEAR recommends APIs at method-level by default.
It can be easily adapted to class-level recommendations as well. In
thecaseofAPIclasssearching,weremovethemethodnameofthe
candidate API to adjust the candidate API to the class level.
4 EXPERIMENT DESIGN
4.1 Dataset
To evaluate the performance of CLEAR, we reuse SO data from the
state-of-the-artapproachBIKER[ 14],whichwerecollectedfrom
theofficialdatadumpofSObyfollowingcriteria:1)thequestion
is related to Java JDK programming, 2) the question should have
apositivescore,and3)atleast1answertothequestioncontains
API entities and the answerâ€™s score should be positive.
TheAPIswereextractedfromthecodesnippetsinmarkdown
scripts of the accepted answers in SO. In a markdown script, code
snippets are wrapped by <code> tags. One can use regular expres-
sions to localize the code snippets and further extract the APIs. In
total, BIKERâ€™sdataset contains 33K Java-relatedquestions. BIKER
alsoprovidedatestdatasetforevaluatingitsperformance,which
was manually created with a set of well-designed criteria, e.g., one
of their criteria is the score of the question itself should be at least
five, the details about the process are in their Section 4. The test
datacontains413questionsalongwiththeirgroundtruthAPIs.We
use the title of these 413 questions as the query for API search.
Notethat,BIKERâ€™stestdatasetmainlycontainsSOpostswith
highquality,whichcannotreflecttheoverallqualityofSOposts.
Thus,wehavealsocreatedtwodifferentrandomtestdatasetswhich
contain randomlyselected SOposts forremoving human bias(de-
tails are in Section 4.3).
4.2 Experiment Settings
WeuseGoogleColab[ 2]professionalversionforfine-tuningthe
models. The CPU we use is two Intel Xeon 2.20GHz CPU with
5Gcache.TheGPUresourceweuseisoneNVIDIAV100graphic
cardwith 13Gmemory.Wefine-tunethefilteringmodel andthere-ranking model for five epochs each and then select the model
with the best performance on the validation set.
ThetripletsgenerationalgorithminCLEARhastwoparameters,
i.e., the number of positive samples ( ğ‘) and the number of nega-
tive samples ( ğ‘›), which could affect the performance of CLEAR.
To find the best values of these two parameters, we tune them
together. For ğ‘andğ‘›, we experiment with five discrete values,
i.e.,1,3,5,10,and15,whichresultsinacombinationof25model
380
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 07:54:11 UTC from IEEE Xplore.  Restrictions apply. ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA Moshi Wei, Nima Shiri Harzevili, Yuchao Huang, Junjie Wang, and Song Wang
Table1:Performancecomparisonofdifferent(P)ositivesam-
pling and (N)egative sampling settings.
P\N1351015
10.0040.0320.0270.0360.027
30.1840.3320.3920.4630.416
50.3760.5120.5520.7660.76
100.5240.7040.6520.8280.784
150.6240.6840.7360.720.784
configurations. Because the fine-tuning on the full data is very
time-consuming, we perform the grid search on the model witha quarter of the training data. We train the filtering model until
full convergence or up to 5 epochs to sufficiently train the models.
The random seed is locked across the models to make sure the
random samplingon positive and negativesamples is consistent.
Werandomlyselect5Kpostsasthetestdatafortuningthesetwo
parameters,andweusetheaccuracyofourfiltermodelasametric
during our tuning. Following existing studies [ 6,40], we use the
ğ‘ğ‘ğ‘ğ‘¢ğ‘Ÿğ‘ğ‘ğ‘¦@1asthemetricforparametertuning,whichiscalculated
as #(ğ‘“ğ‘–ğ‘Ÿğ‘ ğ‘¡ğ‘šğ‘ğ‘¡ğ‘â„ğ‘–ğ‘ ğ‘ğ‘œğ‘Ÿğ‘Ÿğ‘’ğ‘ğ‘¡ )/#(ğ‘¡ğ‘’ğ‘ ğ‘¡ğ‘–ğ‘›ğ‘ ğ‘¡ğ‘ğ‘›ğ‘ğ‘’ğ‘  ).
Table 1 shows the result of ğ‘ğ‘ğ‘ğ‘¢ğ‘Ÿğ‘ğ‘ğ‘¦@1 based on different pa-
rameter settings, in which the row and column indices are the
numbersofpositiveandnegativesamplesrespectively.Overall,the
performance of CLEAR increases with the increase of positive and
negative samples, and the performance of CLEAR reaches the peakatthepointwherethenumberofpositiveandnegativesamplesare
both equal to 10. Thus, we set 10 positive samples and 10 negative
samples for each training instance when training CLEAR in ourexperiments. In the case that there are less than 10 positive and
negative samples, we include all positive and negative samples.
4.3 Evaluation Datasets
TocomprehensivelyevaluatetheperformanceofCLEAR,weadopt
three test datasets covering three different scenarios, i.e., high-
qualitySOposts(i.e.,BIKERâ€™stestdataset),real-woldrandomSO
posts, and SO posts with multi-API answers as our observationshows that around 10% posts in SO contain multiple APIs. The
details of the three datasets are as follows:
â€¢BIKER test dataset: is the evaluation dataset of BIKER,
whichcontains413manuallyselectedandverifiedSOqueries
with API answers.
â€¢Random test dataset: contains 1K random selected SO
queries with API answers from BIKERâ€™s training dataset.
â€¢Multi-API test dataset: contains 1K random SO queries
with multi-API answers from BIKERâ€™s training dataset.
During our experiments, questions from the test datasets and
their duplicate questions were excluded from the training dataset.
4.4 Baselines
We compared CLEAR with BIKER [ 14], RACK [ 35], and Deep-
API [11], which are three state-of-the-art API recommendation
techniques. To show the impact of contrastive training, we alsointroduce a variant of the filter model without adopting the con-trastive training, which is the pre-trained RoBERTa model. Notethat,BIKERandourCLEARshareacommonprocedure,i.e.,afilter
model to retrieve top k candidate posts and a re-ranking modelto re-rank the candidate posts. Thus, we also introduce the filer
models of BIKER and our CLEAR as the baselines.
Baseline1 (BIKER) [14]: first uses a mixture of TF-IDF and
a trained Word2vec model to calculate the similarity of a givenquery and the SO posts and then the top 50 posts are selectedas the candidates. Finally, it re-ranks the 50 candidates by usingthe similarity between the query and the corresponding official
APIdocumentdescriptions.Tocomprehensivelycompareitwith
CLEAR, we employ two related baselines, i.e., BIKER-filter (BIKER
without re-ranking) and BIKER-complete (the whole approach).
Baseline2(RACK) [35]:isakeyword-APImappingsystemthat
recommends APIs by matching keywords from the query. The
keyword-API isconstructed bymining thestatistical relationship
betweentheSOquestionsandtheacceptedanswersofquestions.
Please note that RACK only recommends API at the class level.
Baseline3(DeepAPI) [11]:modelsAPIrecommendationtaskas
amachinetranslationproblem.ItusesaRecurrentNeuralNetwork
(RNN)Encoder-Decodermodeltoencodeagivenqueryintoafixed-
lengthcontextvector,andgenerateanAPI-methodsequencebased
onthecontextvector.TheauthorofDeepAPIprovidedanonline
toolfortestingandevaluation.However,thewebsiteisnotavailablecurrentlyduetothebudgetlimit.Initially,wecontactedtheauthors
for their trained models, unfortunately, the author claimed that
they did not maintain the trained models anymore. Then, we used
its reproduction package6and rigorously follow its instruction
tore-traintheDeepAPImodelfromscratchwithitsdataset.The
training process takes 15 days and we achieve similar performance
(regarding BLUE scores) as reported in the paper of DeepAPI. The
reproductionmodelrepresentsthebesteffortwemadetoreproduce
the DeepAPI model. In this work, the evaluation of the DeepAPI
model is performed on the reproduced model.
Baseline5 (Pre-trained RoBERTa filter) : is the pre-trained
RoBERTamodel.WecompareCLEAR-filterwithRoBERTatoex-
plore the performance increase introduced by contrastive learning.
We use the same pre-trained RoBERTa model as used in CLEAR.
Baseline5(CLEAR-filter) :SinceCLEARhastwosteps,i.e.,the
filtermodel,andthere-rankingmodel,weseparatethefiltermodelfromthere-rankingmodeltoshowtheperformanceincreaseintro-
duced by both of them.
4.5 Performance Measures
Followingexistingstudies[ 14],weuseMeanreciprocalrank(MRR)
[31,45], Mean average precision(MAP) [ 38],ğ‘ƒğ‘Ÿğ‘’ğ‘ğ‘–ğ‘ ğ‘–ğ‘œğ‘› @ğ‘˜, and
ğ‘…ğ‘’ğ‘ğ‘ğ‘™ğ‘™@ğ‘˜, to evaluate the performance of API recommendation
approaches. MRR and MAP are the widely accepted measurements
forinformationretrieval.MRRmeasurestheeffortneededtofind
thefirstcorrectanswerintherecommendedlistandMAPconsiders
the ranks of all correct answers.
Wealsoevaluatetheperformancewith ğ‘ƒğ‘Ÿğ‘’ğ‘ğ‘–ğ‘ ğ‘–ğ‘œğ‘› @ğ‘˜andğ‘…ğ‘’ğ‘ğ‘ğ‘™ğ‘™@ğ‘˜,
whereğ‘˜can be 1, 3, 5, and 10. For the search result of a query, pre-
cision and recall can be defined as follows:
ğ‘ƒğ‘Ÿğ‘’ğ‘ğ‘–ğ‘ ğ‘–ğ‘œğ‘› @ğ‘˜=#(ğ‘Ÿğ‘’ğ‘™ğ‘’ğ‘£ğ‘ğ‘›ğ‘¡ğ‘–ğ‘¡ğ‘’ğ‘šğ‘ ğ‘Ÿğ‘’ğ‘¡ğ‘Ÿğ‘–ğ‘’ğ‘£ğ‘’ğ‘‘ )@ğ‘˜
#(ğ‘Ÿğ‘’ğ‘¡ğ‘Ÿğ‘–ğ‘’ğ‘£ğ‘’ğ‘‘ğ‘–ğ‘¡ğ‘’ğ‘šğ‘  )(3)
6https://github.com/guxd/deepAPI
381
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 07:54:11 UTC from IEEE Xplore.  Restrictions apply. CLEAR: C ontrastive Le arning for A PI Recommendation ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
ğ‘…ğ‘’ğ‘ğ‘ğ‘™ğ‘™@ğ‘˜=#(ğ‘Ÿğ‘’ğ‘™ğ‘’ğ‘£ğ‘ğ‘›ğ‘¡ğ‘–ğ‘¡ğ‘’ğ‘šğ‘ ğ‘Ÿğ‘’ğ‘¡ğ‘Ÿğ‘–ğ‘’ğ‘£ğ‘’ğ‘‘ )@ğ‘˜
#(ğ‘Ÿğ‘’ğ‘™ğ‘’ğ‘£ğ‘ğ‘›ğ‘¡ğ‘–ğ‘¡ğ‘’ğ‘šğ‘  )(4)
where the # (ğ‘Ÿğ‘’ğ‘™ğ‘’ğ‘£ğ‘ğ‘›ğ‘¡ğ‘–ğ‘¡ğ‘’ğ‘šğ‘ ğ‘Ÿğ‘’ğ‘¡ğ‘Ÿğ‘–ğ‘’ğ‘£ğ‘’ğ‘‘ )refers to the number of cor-
rectlyrecommendedAPI,the# (ğ‘Ÿğ‘’ğ‘¡ğ‘Ÿğ‘–ğ‘’ğ‘£ğ‘’ğ‘‘ğ‘–ğ‘¡ğ‘’ğ‘šğ‘  )referstothenum-
beroftotalretrievedAPIs,andthe# (ğ‘Ÿğ‘’ğ‘™ğ‘’ğ‘£ğ‘ğ‘›ğ‘¡ğ‘–ğ‘¡ğ‘’ğ‘šğ‘  )referstothe
number of APIs in the answers of the queries.
4.6 Research Questions
To evaluate theperformance of CLEAR, we design experiments to
answer the following research questions:
RQ1:How effective is CLEAR comparing with existing API recom-
mendation baselines at method-level?RQ2:
How effective is CLEAR comparing with existing API recom-
mendation baselines at class-level?RQ3:
How does random sampling of triplet generation affect the
performance of CLEAR?
In RQ1 and RQ2, we set out to investigate the performance of
the CLEAR on method- and class-level API recommendation tasks.
To demonstrate its advantages,we compare CLEAR with state-of-
the-art baselines (details are in Section 4.4). In RQ3, we explore the
impact of the random sampling process in the triplet generation
algorithm(detailsareinSection3.1.1)ontheperformanceofCLEAR.
4.7 Statistical Testing
In this paper, we use a parametric test to check the statistically sig-
nificant difference inthe performance of differentAPI recommen-
dationbaselines.WeusetheparametricWilcoxonsignedranked
test[50],whichhasbeenwidelyusedinmanysoftwareengineering
studies [21,46â€“48]. The advantage of the Wilcoxon test is that it
doesnotrequiretheresultstofollowanyspecificdistribution.A
p-value smaller than 0 .05 indicates that the difference between the
two baselinesâ€™ performance is statistically significant.
5 RESULT ANALYSIS
Thissection presentsourexperiment results andanswersthe three
research questions asked in Section 4.6 regarding the effectiveness
ofCLEARatmethod-levelAPIrecommendation(Section5.1)and
class-level API recommendation (Section 5.2) and the impact of
randomness in CLEAR (Section 5.3).
5.1 RQ1: Effectiveness of CLEAR at
Method-level
ExperimentalMethod .Toanswerthisresearchquestion,wecom-
pare CLEAR with the baselines listed in Section 4.4 on the three
different test datasets listed in Section 4.3. Note that, we exclude
RACKinthisresearchquestionasitrecommendsAPIatclass-level
only. Since BIKERâ€™s authors have published the replication pack-
age7, we directly use it to conduct experiments and compare with
CLEAR. For DeepAPI, as we described in Section 4.4, we use the
re-trained model for our experiments. Since DeepAPI recommends
API sequence for a given query, we consider a recommendation is
correctifanyoneoftheAPIsinthesequenceisthegroundtruth
7https://www.dropbox.com/s/fr4gdbyfn58ytm8/BIKER.zip?dl=0API of the query (the same comparison manner has also been used
in the comparison of BIKER and DeepAPI in BIKERâ€™s paper [14]).
Results. Table 2 shows the result of CLEAR compared with the
other baselines. As shown in the Table 2, overall CLEAR outper-
formsbothBIKER(includingbothitsfiltermodelandre-ranking
model) and DeepAPI. Notethat, BIKER has the same performance
reportedinthisworkanditsoriginalpaper[ 14].However,different
from the comparison reported in BIKERâ€™s paper [ 14], where Deep-
APIâ€™s MRR and MAP are 0.183 and 0.155, in this study DeepAPI
reportsmuchworseperformance,i.e.,allMRRsandMAPsarebelow0.1.Thereasonisthatinpaper[
14],DeepAPIwasevaluatedonthe
online tool released by DeepAPIâ€™s authors, we re-trained DeepAPI
withitsreproductionpackage(detailsareinSection4.4).OnBIKER
test data, the ğ‘Ÿğ‘’ğ‘ğ‘ğ‘™ğ‘™@1 of CLEAR-complete is 0.6309, indicating
that there is at least one right answer in the first candidates in
63.09% cases. Comparing the BIKER-filter model and CLEAR-filter
model,theCLEAR-filtermodeloutperformstheBIKER-filtermodel
by 46.43% and 50.18% on MAR and MAP. In terms of precision
andrecall,CLEAR-filtermodelimprovesthe ğ‘ğ‘Ÿğ‘’ğ‘ğ‘–ğ‘ ğ‘–ğ‘œğ‘› @1,3,5,10
by51.45%,120.44%,151.37%,166.49%, ğ‘Ÿğ‘’ğ‘ğ‘ğ‘™ğ‘™@1,3,5,10by52.61%,
31.63%, 22.25%, 4.005% respectively, which shows the effectiveness
of our filter model.
Ontherandomtestdata,CLEAR-completemodeloutperforms
BIKER-complete model in all the measurements. Comparing to
BIKER-completemodel,CLEAR-completemodel improvesb y185.88%
on MRR, 195.15% on MAP, 314.94%, 541.88% 732.24% 1132.05%
onğ‘ğ‘Ÿğ‘’ğ‘ğ‘–ğ‘ ğ‘–ğ‘œğ‘› @1,3,5,10,and326.29%,180.50%,133.18%,87.45%on
ğ‘Ÿğ‘’ğ‘ğ‘ğ‘™ğ‘™@1,3,5,10respectively.Onmulti-APItestdata,CLEAR-complete
mode outperforms BIKER-complete mode by 104.09% on MRR and
105.24%onMAP.Intermsofprecisionandrecall,CLEAR-complete
improves the ğ‘ğ‘Ÿğ‘’ğ‘ğ‘–ğ‘ ğ‘–ğ‘œğ‘› @1,3,5,10 by 287.09%, 506.31%, 711.52%,
1126.38%and ğ‘Ÿğ‘’ğ‘ğ‘ğ‘™ğ‘™@1,3,5,10by301.99%,165.70%,130.77%,87.35%
respectively. Compared to the RoBERTa model, CLEARâ€™s filtermodel achieves better performance on all the three test datasets,
whichindicatesthatcontrastivelearningcanhelplearnaprecise
semantic representation of programming tasks.
We have also conducted the Wilcoxon signed-rank test (ğ‘<
0.05)tocomparetheperformanceofCLEARandbaselines.thetest
result suggests that CLEAR achieves significantly better perfor-
mance than all the baselines.
CLEAR significantly outperforms the state-of-the-art baselines
at method-level API recommendation and CLEARâ€™s perfor-
mance remains stable across different test datasets.
5.2 RQ2: Effectiveness of CLEAR at Class-level
ExperimentalMethod .Toanswerthisresearchquestion,weper-
formthesameevaluationmethodonthebaselinesand CLEAR.WeusethesamethreetestdatasetswithAPImethodsremovedtocom-pareAPIanswersattheclasslevel.TocomparewithRACK,werunexperimentswithRACKâ€™sreplication
8.ForbothBIKEandDeepAPI,
we use the same manner as the experiment at method-level API
recommendation in Section 5.1.
8https://github.com/masud-technope/RACK-Replication-Package
382
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 07:54:11 UTC from IEEE Xplore.  Restrictions apply. ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA Moshi Wei, Nima Shiri Harzevili, Yuchao Huang, Junjie Wang, and Song Wang
Table 2: Performance comparison at method-level (RQ1)
Method-level BIKER-filter BIKER-complete DeepAPI RoBERTa CLEAR-filter CLEAR-complete
BIKER test dataMRR 0.4318 0.6225 0.0313 0.4098 0.6319 0.7551
MAP 0.4260 0.6175 0.0102 0.4088 0.6398 0.7655
PrecisionP@1 0.2777 0.4642 0.0088 0.2341 0.4206 0.4682
P@3 0.2328 0.2486 0.0073 0.2632 0.5132 0.5502
P@5 0.2071 0.1698 0.0140 0.2563 0.5206 0.5531
P@10 0.1928 0.0956 0.0123 0.2305 0.5138 0.5563
RecallR@1 0.2678 0.4503 0.0029 0.2321 0.4087 0.6309
R@3 0.5019 0.7142 0.0066 0.4980 0.6607 0.7638
R@5 0.5972 0.8134 0.0227 0.6130 0.7301 0.7956
R@10 0.7440 0.9166 0.0403 0.7182 0.7738 0.8551
Random test dataMRR 0.2448 0.2813 0.0336 0.2912 0.7573 0.8042
MAP 0.2357 0.2724 0.0104 0.2855 0.7612 0.8040
PrecisionP@1 0.1420 0.1740 0.0080 0.1940 0.6680 0.7220
P@3 0.1266 0.1103 0.0057 0.1746 0.6669 0.7080
P@5 0.1160 0.0830 0.0131 0.1673 0.6495 0.6909
P@10 0.1074 0.0546 0.0137 0.1524 0.6233 0.6727
RecallR@1 0.1298 0.1620 0.0023 0.1783 0.6383 0.6906
R@3 0.2673 0.3011 0.0052 0.3093 0.8078 0.8446
R@5 0.3298 0.3791 0.0203 0.3791 0.8523 0.8840
R@10 0.4418 0.4976 0.0423 0.4724 0.8954 0.9328
Multi-API test dataMRR 0.2296 0.2879 0.0355 0.2988 0.6495 0.5876
MAP 0.2212 0.2804 0.0115 0.2895 0.6392 0.5755
PrecisionP@1 0.1280 0.1860 0.004 0.1970 0.6770 0.7200
P@3 0.1166 0.1156 0.0073 0.1766 0.6489 0.7009
P@5 0.1162 0.0850 0.018 0.1692 0.6365 0.6898
P@10 0.1120 0.0542 0.0153 0.1585 0.6183 0.6647
RecallR@1 0.1155 0.1703 0.0011 0.1800 0.6406 0.6846
R@3 0.2440 0.3126 0.0065 0.3183 0.7806 0.8306
R@5 0.3188 0.3795 0.0278 0.3891 0.8335 0.8758
R@10 0.4443 0.4856 0.0472 0.4979 0.8793 0.9098
Results. Table 3 shows the result of CLEAR compared with the
otherbaselineapproachesattheclasslevel.Overall,CLEAR out-
performs other baselines on each of the three datasets. Among
the three baselines, similar to method-level API recommendations,
BIKER reports better performance than RACK and DeepAPI.
On BIKER test data, the recall@1 of CLEAR-complete is 80.95%,
indicating that there is at least one right answer in the top three
candidatesin80.95%cases.ComparingtheCLEAR-completemodel
with RACK, the CLEAR-complete model outperforms RACK by
187.65% in MRR and 196.76% in MAP. In terms of precision and re-
call,CLEAR-completeimprovesthe ğ‘ğ‘Ÿğ‘’ğ‘ğ‘–ğ‘ ğ‘–ğ‘œğ‘› @1,3,5,10by236.11%,
566.25%,906.04%,1684.76%and ğ‘Ÿğ‘’ğ‘ğ‘ğ‘™ğ‘™@1by242.86%,158.87%,144.96%,
129.77% respectively. Comparing the CLEAR-complete model with
the BIKER-complete model, the CLEAR-complete model outper-
forms the BIKER-complete model by 7.70% in MRR and 9.43% in
MAP. On the random test data, CLEAR outperforms RACK, BIKER,
and DeepAPI in all the measurements. Comparing the CLEAR-
complete model with RACK, CLEAR-complete outperforms RACK
by 273.41% in MRR, 298.55% in MAP, 432.89% in precision@1,and 455.62% in Recall@1. On the multiple-API test data, CLEAR-complete model outperforms RACK by 187.33% in MRR, 197.04%
in MAP. In terms of precision and recall, CLEAR-complete outper-
formsRACKthe ğ‘ğ‘Ÿğ‘’ğ‘ğ‘–ğ‘ ğ‘–ğ‘œğ‘› @1,3,5,10by393.20%,642.09%,941.02%,
1617.85%and ğ‘Ÿğ‘’ğ‘ğ‘ğ‘™ğ‘™@1,3,5,10by399.01%,193.87%,159.94%,124.76%
respectively.ComparedtotheRoBERTamodel,CLEARfiltermodelachieves consistently better performance on each of the three
datasets, indicating the effectiveness of contrastive learning.
The Wilcoxon signed-rank test (ğ‘<0.05)also suggests that
CLEAR achieves significantly better performance than all other
baseline approaches.
CLEAR significantly outperforms the state-of-the-art baselines
atclass-levelAPIrecommendationandCLEARâ€™sperformance
remains stable across the three test datasets.
5.3 RQ3: Impact of Random Sampling
ExperimentalMethod .InCLEARâ€™stripletgeneration,forqueries
with more than 10 positive or negative samples, CLEAR randomly
selects10foreachquery.TounderstandhowdoesrandomsamplingaffectstheperformanceofCLEAR,were-runthetripletgeneration
100 times. Please note that fine-tuning the model with full training
triplets is very time-consuming so we perform this experiment on
a subset of the training triplets containing 92k pairs (i.e., a quarter
of the full training triplets).
Result. Table 4 shows the impact of random sampling on the
performance of CLEAR measured by the Average Error and Coeffi-
cientofVariation(CV).Aswecanseefromthetable,theaverage
error on MRR is 0.85%, indicating that the difference of MRR in-
troducedbyrandomsamplingbetweendifferentrunsis0.85%on
383
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 07:54:11 UTC from IEEE Xplore.  Restrictions apply. CLEAR: C ontrastive Le arning for A PI Recommendation ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
Table 3: Performance comparison at class-level (RQ2)
Class-level BIKER-filter BIKER-complete RACKDeepAPI RoBERTa CLEAR-filter CLEAR-complete
BIKER test dataMRR 0.6397 0.8138 0.3047 0.0172 0.5761 0.7059 0.8765
MAP 0.6343 0.8138 0.3001 0.0008 0.5769 0.7156 0.8906
PrecisionP@1 0.2777 0.4642 0.2420 0.0044 0.3690 0.7777 0.8134
P@3 0.2328 0.2486 0.1203 0.0014 0.4404 0.7513 0.8015
P@5 0.2071 0.1698 0.0777 0.0079 0.4269 0.7380 0.7817
P@10 0.1928 0.0956 0.0420 0.0070 0.4095 0.7182 0.7496
RecallR@1 0.4623 0.6865 0.2361 0.0001 0.3611 0.5436 0.8095
R@3 0.7559 0.9067 0.3472 0.0001 0.7202 0.8253 0.8988
R@5 0.8531 0.9563 0.3750 0.0018 0.8214 0.8750 0.9186
R@10 0.9424 0.9880 0.4067 0.0035 0.9226 0.8988 0.9345
Random test dataMRR 0.4060 0.4515 0.2343 0.0206 0.4426 0.8467 0.8749
MAP 0.3961 0.4408 0.2207 0.0010 0.4410 0.8536 0.8796
PrecisionP@1 0.1420 0.1740 0.1520 0.0060 0.3030 0.7800 0.8100
P@3 0.1266 0.1103 0.0989 0.0026 0.2976 0.7719 0.8093
P@5 0.1160 0.0830 0.0722 0.0076 0.2925 0.7611 0.7989
P@10 0.1074 0.0546 0.0431 0.0085 0.2810 0.7400 0.7809
RecallR@1 0.2473 0.3103 0.1395 0.0002 0.2833 0.7473 0.7751
R@3 0.4573 0.4881 0.2728 0.0003 0.4988 0.8806 0.9113
R@5 0.5568 0.5571 0.3308 0.0016 0.5908 0.9133 0.9403
R@10 0.6633 0.6880 0.3968 0.0039 0.7063 0.9395 0.9606
Multi-API test dataMRR 0.3829 0.4458 0.2511 0.0193 0.4351 0.7702 0.7215
MAP 0.3763 0.4371 0.2406 0.001 0.4288 0.7684 0.7147
PrecisionP@1 0.1280 0.1860 0.1620 0.002 0.3040 0.7720 0.7990
P@3 0.1166 0.1156 0.1069 0.0013 0.2843 0.7513 0.7933
P@5 0.1162 0.0850 0.0758 0.0094 0.2803 0.7415 0.7891
P@10 0.1120 0.0542 0.0448 0.0087 0.2698 0.7312 0.7696
RecallR@1 0.2263 0.3048 0.1525 0.0077 0.2811 0.7340 0.7610
R@3 0.4341 0.4901 0.3003 0.0016 0.4830 0.8458 0.8825
R@5 0.5298 0.5620 0.3548 0.0024 0.5756 0.8853 0.9223
R@10 0.6688 0.6668 0.4208 0.0045 0.6905 0.9226 0.9458
Table 4: Impact the random sampling in triplet generation
Metric MRRMAP
Average Error 0.85%0.84%
Coefficient of Variation (CV) 0.0040.011
average.TheCoefficientofVariationiscalculatedby ğ¶ğ‘‰=ğœ/ğœ‡[8],
whereğœisthestandarddeviationand ğœ‡isthemean.TheCVofour
result suggests that the difference introduced by random sampling
is negligible.
Theimpactofrandomsamplingintripletgenerationontheper-
formance of CLEAR is negligible, which shows the robustness
of CLEAR.
6 DISCUSSIONS
Thissectiondiscussesopenquestionsregardingtheperformance
and threads to validity of CLEAR .
6.1 Why CLEAR Outperforms Existing
Baselines?
To understand why CLEAR significantly outperforms the base-
linesintroducedinSection4.4,wevisualizetheembeddingofthe
API search space of the model before and after contrastive train-
ing. Specifically, we use the Uniform Manifold Approximation and
Projection(UMAP)[ 18]approachtoreducethedimensionoftheBERT-basedsentenceembeddingtotwodimensions.Thenwela-
bel the embedding vectors with the Hierarchical Density-Based
SpatialClusteringofApplicationswithNoise(HDBSCAN)[ 3],an
unsupervised cluster classification approach for the coloring.
Figure5showsthevisualization,inwhichtheuppergraphshows
thesentenceembeddingvisualizationofthetrainingsamplesonthe
modelbeforeweapplycontrastivetraining,inwhichthepointsrep-
resentthesentenceembeddingvectorsintwo-dimensionalspaceandthecolorofthepointsindicatestheAPIs.Fromthevisualiza-
tion, we can see that the majority of the APIs are mixed and the
boundary of each API is not clear. This graph shows clearly that it
is very hard to draw the decision boundary for different clusters in
themodelbeforecontrastivetraining.Sincethetrainingtargetof
contrastivetrainingistominimizethedistancebetweensemanti-
cally equivalent sentences and maximize the distance between the
irrelevantsentence,themarginbetweenclustersshouldbelarger
and clearer after training.
Tosupporttheabovehypothesis,wealsoapplythesamevisu-
alization approach to the fine-tuned model after we applied con-
trastive training. Figure 5 lower graph shows the sentence embed-
dingvisualizationofthetrainingsamplesaftercontrastivetraining.
From this figure, we can see clear cluster patterns of the query em-
beddingvectors.Mostofthe APIsarefromdenseclustersandthe
marginspacebetweenclustersisrelativelyclear.Thisvisualization
supportsourhypothesisofcontrastivetraining,meaningthatthe
contrastive training does pull semantic equivalent queries together
and separates the irrelevant vectors apart.
384
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 07:54:11 UTC from IEEE Xplore.  Restrictions apply. ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA Moshi Wei, Nima Shiri Harzevili, Yuchao Huang, Junjie Wang, and Song Wang
Table5:Recommendationresults(i.e.,APIsandtopsimilarquestions)ofCLEARforexamplequeries. indicatestheground-
truth API and indicates the recommended API is incorrect.
Question API answers
input query How to convert DateFormat "Fri Jan 08 13:48:16 GMT+05:30 2021" to java.sql.Date java.text.SimpleDateFormat.parse
1st How to parse "Thu Aug 04 00:00:00 IST 2011" to "04-08-2011"? java.text.SimpleDateFormat.parse
2nd Converting "2010-02-15T20:05:28.000Z"in GMT format using Java java.text.SimpleDateFormat.parse
3rd Convert String date into java.util.Date in the dd/MM/yyyy format java.text.DateFormat.format
4th Date format and the hour is always 12:00:00.000 java.time.Instant.parse
input query How to retrive value from property file which are present outside of the app java.util.Properties.load
1st How to close the fileInputStream while reading the property file java.util.Properties.load
2nd Using Maven properties to connect to a database java.lang.System.getProperty
3rd Why do we need Properties class in java? java.util.Properties.load
4th Issue reading a file path from a Properties file java.util.Properties.store
Figure5: VisualizationofAPIquestionsentenceembeddingbefore
(i.e., the upper image) and after (i.e., the lower image) contrastive
training.
6.2 CLEAR in the Real-world Practice
We run CLEAR, BIKER, RACK, and DeepAPI on 50 recent Java-
related questions from Stack Overflow9. Comparing the top 10
recommended APIs, CLEAR successfully recommends APIs for
34 queries, BIKER successfully recommends APIs for 23 queries,
RACKsuccessfullyrecommendsAPIsfor4queries,andDeepAPI
successfully recommends APIs for 2 queries.
WeselectedtworandomexamplesthatcanbesolvedbyCLEARonly
fordemonstration.Table5showstherecommendationresultsof
CLEAR for the two example latest SO posts. The first example is
about converting date formats, we can see that CLEAR can under-
standtheconceptoftimeinmultipleformatsandpickthekeyword
â€œconvertâ€correctly.TheresultshowsthatCLEARisnotsuffering
fromthelexicalsimilaritypitfallconcerningthetimeformatand
9*The full list can be found in the reproduction packageis able to recommend correct APIs. The second example is aboutproperty file access,the semantic of the question is â€œhow to load
property filesâ€ and the CLEAR is able to get the keywords that are
themostrelatedtothe question,i.e.,â€œpropertyfileâ€andâ€œretrieveâ€.
We also see that the keyword â€œreadingâ€, the synonym of â€œretrieveâ€,
is correctly recognized as well.
Throughtheabovetwocasestudies,wecanseethattheCLEARis
more effective in capturing the semantic of the API queries regard-
less of the lexical information, thus can be used for API recommen-
dation in a real-world application.
6.3 Threat to Validity
InternalValidity .Ourcodehasbeencheckedtoensureourimple-
mentationiscorrectandthequestionsinthetestingdatasetarenotincluded in the question base. We reuse the replication packages of
the baselines to ensure their correctness. In addition, although the
dataset collected from SO is being filtered by heuristic rules, there
arestillnoisesinthedatasetduetotheopennessofSO,whichmay
affect the performance of the CLEAR.External Validity
. In this work, we used the dataset published
byBIKERtodemonstratetheeffectivenessofCLEAR,whichonlysupportsJavaAPI recommendations.TheperformanceofCLEAR
canbedifferentonAPIrecommendationforotherprogramming
languages. In addition, as the dataset only contains questions from
StackOverflow,CLEARmightperformdifferentlyondatacollected
fromotheronlineforums.Futurestudyisneededtoexaminethe
performance of CLEAR on data from other sources.ConstructValidity
WeuseMRR,MAP, ğ‘ğ‘Ÿğ‘’ğ‘ğ‘–ğ‘ ğ‘–ğ‘œğ‘› @ğ‘˜,andğ‘Ÿğ‘’ğ‘ğ‘ğ‘™ğ‘™@ğ‘˜
tomeasuretheperformanceofAPIrecommendation[ 14,35],our
approachmighthavedifferentperformanceunderothermetrics.In
this work, we assume that SO questions with the same API answer
assemanticallyequivalentwhencontrastivelytrainingourBERT-
based sentence embedding. Future study is needed to examine our
assumption on API Q&A pairs from other sources or other tasks.
7 RELATED WORK
7.1 API Recommendation
TherearemanyexistingstudiesonAPIrecommendation,includ-
ing API invocation sequences mining [ 19], dependency graph-
based API phrases mining [ 4], API recommendation for feature
385
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 07:54:11 UTC from IEEE Xplore.  Restrictions apply. CLEAR: C ontrastive Le arning for A PI Recommendation ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
requests [ 43], query-API keyword mapping with crowed knowl-
edge [35], code snippet synthesis [ 33], similarity-based API recom-
mendation with language model [ 14], and API recommendation
basedonsimilarityoffunctionalityverbphrasesinfunctionality
descriptions and user queries [53].
McMillan et al. [ 19] first presented portfolio, an API recommen-
dation tool that returns code snippets for a programming query.
Thungetal.[ 43]introducedhistoricalfeaturerequestscombined
with official API documents information for API recommendation
for new feature requests. Nguyen et al. [ 23] proposed GRALAN, a
graph-based language model for object-oriented source codes. Liu
et al. [16] improved the ranking of the top-10 result of GRALAN
by introducing API usage path information to the graph system.
Nguyen et al. [ 22] used statistical learning on the commit changes
informationforAPIrecommendation.Guetal.[ 11]firstintroduced
a deep learning model to API learning which achieves end-to-end
API sequence generation. CLEAR uses RoBERTa as the base model,
which is different from DeepAPI. Rahman et al. [ 35] presented
RACK, an API recommendation tool leveraging the real API usage
datafromStackOverflow[ 26].ThedifferencebetweentheRACK
andCLEARisthatCLEARusesalanguagemodelinsteadofkey-
wordmapping. Huang et al. [ 14] proposed BIKER,which filters the
candidate APIs based on the similarity against SO questions and
then re-ranks the candidates based on the similarity against offi-
cialAPIdocumentationdescription.Themaindifferencebetween
BIKER and CLEAR is that CLEAR uses contrastive training instead
of unsupervised training in the model building stage.
7.2 API Usage Pattern Mining
Xieetal.[ 52]proposedMAPO,anAPIusagepatternminingtool
withvariouscodepatternminingalgorithms.Thummalapentaet
al. [42] proposed PARSEWeb, a java code reuse example genera-
tiontoolbuilduponopen-sourcejavacodedata.Tsengetal.[ 44]
proposed UP-miner, a toolset that contains thirteen java utility
codepatternminingalgorithmsthatimprovetheperformanceof
UP-miner.Fowkesetal.[ 9]presentedPAM,aparameter-freeproba-
bilisticalgorithmforminingtheAPIusagepatterns.Wenetal.[ 49]
proposedanAPImiss-usedetectiontoolthatcandetectAPImisuse
patternsofJavalibraries.Chenetal.[ 5]firstappliedanunsuper-
visedtechniquetocreateanalogicalAPImappingsofthird-party
libraries.Renetal.[ 37]builtanAPI-constraintknowledgegraph
for API-misuse detection purpose.
8 CONCLUSION
In this paper, we propose CLEAR, a novel approach for API rec-
ommendation. CLEAR uses the BERT-based model for embedding,
which produces the embedding of the whole sentence of an API
querywhileconsideringsemantic-relatedsequentialinformation.Ituses contrastive training to better capture the semantics of the API
queries regardless of the lexical information. Our experiment re-
sultsconfirmtheeffectivenessoftheCLEARforbothmethods-and
class-levelAPIrecommendation.OurcasestudywithCLEARon
the latest SO posts further demonstrates its practical value.
Inthe future,we plantoextendCLEARtoother taskssuchas
third-party API recommendation, Linux command search, code
snippet search, and program patch search.9 ACKNOWLEDGMENTS
The authors thank the anonymous reviewers for their feedback
which helped improve this paper. This work is supported by the
Natural Sciences and Engineering Research Council of Canada
(NSERC), the National Natural Science Foundation of China under
grantNo.62072442,andYouthInnovationPromotionAssociation
Chinese Academy of Sciences.
REFERENCES
[1]Laura Aina, Kristina Gulordava, and Gemma Boleda. 2019. Putting words
in context: LSTM language models and lexical ambiguity. arXiv preprint
arXiv:1906.05149 (2019).
[2]EkabaBisong.2019. Googlecolaboratory. In BuildingMachineLearningandDeep
Learning Models on Google Cloud Platform. Springer, 59â€“64.
[3]RicardoJGBCampello,DavoudMoulavi,andJÃ¶rgSander.2013. Density-based
clusteringbasedonhierarchicaldensityestimates.In Pacific-Asiaconferenceon
knowledge discovery and data mining. Springer, 160â€“172.
[4]Wing-Kwan Chan,Hong Cheng, andDavid Lo.2012. Searchingconnected API
subgraph via text phrases. In Proceedings of the ACM SIGSOFT 20th International
Symposium on the Foundations of Software Engineering. 1â€“11.
[5]ChunyangChen,ZhenchangXing,YangLiu,andKentLongXiongOng.2019.Mining likely analogical apis across third-party libraries via large-scale unsu-pervisedapisemanticsembedding. IEEETransactionsonSoftwareEngineering
(2019).
[6]Ting Chen, Simon Kornblith, Kevin Swersky, Mohammad Norouzi, and Geoffrey
Hinton.2020. Bigself-supervisedmodelsarestrongsemi-supervisedlearners.
arXiv preprint arXiv:2006.10029 (2020).
[7]Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert:
Pre-trainingofdeepbidirectionaltransformersforlanguageunderstanding. arXiv
preprint arXiv:1810.04805 (2018).
[8]BrianEverittandAndersSkrondal.2002. TheCambridgedictionaryofstatistics.
Vol. 106. Cambridge University Press Cambridge.
[9]Jaroslav Fowkes and Charles Sutton. 2016. Parameter-free probabilistic API
miningacrossGitHub.In Proceedingsofthe201624thACMSIGSOFTinternational
symposium on foundations of software engineering. 254â€“265.
[10]XiaodongGu,HongyuZhang,andSunghunKim.2018. Deepcodesearch.In 2018
IEEE/ACM 40th International Conference on Software Engineering (ICSE). IEEE,
933â€“944.
[11]XiaodongGu,HongyuZhang,DongmeiZhang,andSunghunKim.2016. Deep
API learning. In Proceedings of the 2016 24th ACM SIGSOFT International Sympo-
sium on Foundations of Software Engineering. 631â€“642.
[12] Zellig S Harris. 1954. Distributional structure. Word10, 2-3 (1954), 146â€“162.
[13]Elad Hoffer and Nir Ailon. 2015. Deep metric learning using triplet network. In
Internationalworkshop on similarity-based pattern recognition . Springer, 84â€“92.
[14]QiaoHuang,XinXia,ZhenchangXing,DavidLo,andXinyuWang.2018. API
methodrecommendationwithoutworryingaboutthetask-APIknowledgegap.In
2018 33rd IEEE/ACM International Conference on Automated Software Engineering
(ASE). IEEE, 293â€“304.
[15]PrannayKhosla,PiotrTeterwak,ChenWang,AaronSarna,YonglongTian,PhillipIsola,AaronMaschinot,CeLiu,andDilipKrishnan.2020. Supervisedcontrastive
learning. arXiv preprint arXiv:2004.11362 (2020).
[16]Xiaoyu Liu, LiGuo Huang, and Vincent Ng. 2018. Effective API recommendation
without historical software repositories. In Proceedings of the 33rd ACM/IEEE
InternationalConference on Automated Software Engineering . 282â€“292.
[17]YinhanLiu,MyleOtt,NamanGoyal,JingfeiDu,MandarJoshi,DanqiChen,Omer
Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: Arobustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692
(2019).
[18]Leland McInnes, John Healy, and James Melville. 2018. Umap: Uniform man-ifold approximation and projection for dimension reduction. arXiv preprint
arXiv:1802.03426 (2018).
[19]CollinMcMillan,MarkGrechanik,DenysPoshyvanyk,QingXie,andChenFu.
2011. Portfolio:finding relevantfunctions andtheir usage.In Proceedingsof the
33rd International Conference on Software Engineering. 111â€“120.
[20]Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013.
Distributed representations of words and phrases and their compositionality.
arXiv preprint arXiv:1310.4546 (2013).
[21]JaechangNamandSunghunKim.2015. Clami:Defectpredictiononunlabeled
datasets(t).In 201530thIEEE/ACMInternationalConferenceonAutomatedSoft-
ware Engineering (ASE). IEEE, 452â€“463.
[22]AnhTuanNguyen,MichaelHilton,MihaiCodoban,HoanAnhNguyen,LilyMast,EliRademacher,TienNNguyen,andDannyDig.2016. APIcoderecommendationusingstatisticallearningfromfine-grainedchanges.In Proceedingsofthe201624th
386
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 07:54:11 UTC from IEEE Xplore.  Restrictions apply. ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA Moshi Wei, Nima Shiri Harzevili, Yuchao Huang, Junjie Wang, and Song Wang
ACM SIGSOFT International Symposium on Foundations of Software Engineering.
511â€“522.
[23]AnhTuanNguyenandTienNNguyen.2015. Graph-basedstatisticallanguage
model for code. In 2015 IEEE/ACM 37th IEEE International Conference on Software
Engineering, Vol. 1. IEEE, 858â€“868.
[24]Phuong T Nguyen, Juri Di Rocco, Davide Di Ruscio, Lina Ochoa, Thomas
Degueule, and Massimiliano Di Penta. 2019. Focus: A recommender system
for mining api function calls and usage patterns. In 2019 IEEE/ACM 41st Interna-
tional Conference on Software Engineering (ICSE). IEEE, 1050â€“1060.
[25]Aaron van den Oord, Yazhe Li, and Oriol Vinyals. 2018. Representation learning
with contrastive predictive coding. arXiv preprint arXiv:1807.03748 (2018).
[26] Stack Overflow. 2008. https://stackoverflow.com/.[27]
JeffreyPennington,RichardSocher,andChristopherDManning.2014. Glove:
Globalvectorsforwordrepresentation.In Proceedingsofthe2014conferenceon
empirical methods in natural language processing (EMNLP). 1532â€“1543.
[28]Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, ChristopherClark, Kenton Lee, and Luke Zettlemoyer. 2018. Deep contextualized word
representations. arXiv preprint arXiv:1802.05365 (2018).
[29]AntonioPolino,RazvanPascanu,andDanAlistarh.2018. Modelcompression
via distillation and quantization. arXiv preprint arXiv:1802.05668 (2018).
[30]Luca Ponzanelli, Andrea Mocci, Alberto Bacchelli, Michele Lanza, and David
Fullerton.2014. Improvinglowqualitystackoverflowpostdetection.In 2014IEEE
internationalconference on software maintenance and evolution . IEEE, 541â€“544.
[31]Dragomir R Radev, Hong Qi, Harris Wu, and Weiguo Fan. 2002. Evaluating
Web-based Question Answering Systems.. In LREC. Citeseer.
[32] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. 2018. Im-
proving language understanding by generative pre-training. (2018).
[33] Mukund Raghothaman,YiWei,and YoussefHamadi.2016. Swim:Synthesizing
whatimean-codesearchandidiomaticsnippetsynthesis.In 2016IEEE/ACM38th
InternationalConference on Software Engineering (ICSE) . IEEE, 357â€“367.
[34]Mohammad Masudur Rahman and Chanchal Roy. 2018. Effective reformulation
of query for code search using crowdsourced knowledge and extra-large dataanalytics. In 2018 IEEE International Conference on Software Maintenance and
Evolution (ICSME). IEEE, 473â€“484.
[35]Mohammad Masudur Rahman, Chanchal K Roy, and David Lo. 2016. Rack:
Automatic api recommendation using crowdsourced knowledge. In 2016 IEEE
23rd International Conference on Software Analysis, Evolution, and Reengineering
(SANER), Vol. 1. IEEE, 349â€“359.
[36]RadimÅ˜ehÅ™ek,PetrSojka,etal .2011. Gensimâ€”statisticalsemanticsinpython.
Retrieved from genism. org (2011).
[37]Xiaoxue Ren, Xinyuan Ye, Zhenchang Xing, Xin Xia, Xiwei Xu, Liming Zhu, and
JianlingSun.2020. API-MisuseDetectionDrivenbyFine-GrainedAPI-Constraint
KnowledgeGraph.In 202035thIEEE/ACMInternationalConferenceonAutomated
Software Engineering (ASE). IEEE, 461â€“472.
[38]Hinrich SchÃ¼tze, Christopher D Manning, and Prabhakar Raghavan. 2008. Intro-
duction to information retrieval. Vol. 39. Cambridge University Press Cambridge.[39]RodrigoSilva,ChanchalRoy,MohammadRahman,KevinSchneider,Klerisson
Paixao, and Marcelo Maia. 2019. Recommending comprehensive solutions for
programmingtasksbyminingcrowdknowledge.In 2019IEEE/ACM27thInterna-
tional Conference on Program Comprehension (ICPC). IEEE, 358â€“368.
[40]Jake Snell, Kevin Swersky, and Richard S Zemel. 2017. Prototypical networks for
few-shot learning. arXiv preprint arXiv:1703.05175 (2017).
[41]Fadi Thabtah, Suhel Hammoud, Firuz Kamalov, and Amanda Gonsalves. 2020.
Dataimbalanceinclassification:Experimentalevaluation. InformationSciences
513 (2020), 429â€“441.
[42]SureshThummalapentaandTaoXie.2007. Parseweb:aprogrammerassistant
for reusing open source code on the web. In Proceedings of the twenty-second
IEEE/ACM international conference on Automated software engineering . 204â€“213.
[43]Ferdian Thung, Shaowei Wang, David Lo, and Julia Lawall. 2013. Automatic
recommendationofAPImethodsfromfeaturerequests.In 201328thIEEE/ACM
InternationalConferenceonAutomatedSoftwareEngineering(ASE) .IEEE,290â€“300.
[44]VincentSTseng,Cheng-WeiWu,Jun-HanLin,andPhilippeFournier-Viger.2015.
UP-miner: a utility pattern mining toolbox. In 2015 IEEE International Conference
on Data Mining Workshop (ICDMW). IEEE, 1656â€“1659.
[45]EM Voorhees. 1999. Proceedings of the 8th text retrieval conference. TREC-8
Question Answering Track Report (1999), 77â€“82.
[46]JunjieWang,SongWang,JianfengChen,TimMenzies,QiangCui,MiaoXie,and
Qing Wang. 2019. Characterizing crowds to better optimize worker recommen-
dation in crowdsourced testing. IEEE Transactions on Software Engineering 47, 6
(2019), 1259â€“1276.
[47]Song Wang, Chetan Bansal, and Nachiappan Nagappan. 2021. Large-scale in-
tentanalysisforidentifyinglarge-review-effortcodechanges. Informationand
Software Technology 130 (2021), 106408.
[48]SongWang,NishthaShrestha,AbarnaKucheriSubburaman,JunjieWang,Moshi
Wei, and Nachiappan Nagappan. 2021. Automatic Unit Test Generation for Ma-
chineLearning Libraries:HowFar AreWe?. In 2021IEEE/ACM43rdInternational
Conference on Software Engineering (ICSE). IEEE, 1548â€“1560.
[49]MingWen,YepangLiu,RongxinWu,XuanXie,Shing-ChiCheung,andZhendong
Su. 2019. Exposing library API misuses via mutation analysis. In 2019 IEEE/ACM
41st InternationalConference on Software Engineering (ICSE) . IEEE, 866â€“877.
[50]FrankWilcoxon.1992. Individualcomparisonsbyrankingmethods. In Break-
throughs in statistics. Springer, 196â€“202.
[51]Xin Xia, Lingfeng Bao, David Lo, Pavneet Singh Kochhar, Ahmed E Hassan, and
ZhenchangXing.2017. Whatdodeveloperssearchforontheweb? Empirical
Software Engineering 22, 6 (2017), 3149â€“3185.
[52]Tao XieandJian Pei.2006. MAPO:Mining APIusages fromopensource repos-
itories. In Proceedings of the 2006 international workshop on Mining software
repositories. 54â€“57.
[53]WenkaiXie,XinPeng,MingweiLiu,ChristophTreude,ZhenchangXing,Xiaoxin
Zhang, and Wenyun Zhao. 2020. API method recommendation via explicitmatching of functionality verb phrases. In Proceedings of the 28th ACM Joint
Meeting on European Software Engineering Conference and Symposium on the
Foundationsof Software Engineering . 1015â€“1026.
387
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 07:54:11 UTC from IEEE Xplore.  Restrictions apply. 