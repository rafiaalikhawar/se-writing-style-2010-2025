SapientML: Synthesizing Machine Learning Pipelines by
Learning from Human-Written Solutions
Ripon K. Sahaâ€ , Akira UraÂ§, Sonal Mahajanâ€ , Chenguang Zhuâ˜…Â¶, Linyi Liâ€¡Â¶, Yang Huâ˜…Â¶,
Hiroaki Yoshidaâ€ , Sarfraz Khurshidâ˜…, Mukul R. Prasadâ€ 
â€ Fujitsu Research of America, Inc.,Â§Fujitsu Ltd.,â˜…The University of Texas at Austin,
â€¡University of Illinois at Urbana-Champaign
{rsaha,ura.akira,smahajan,hyoshida,mukul}@fujitsu.com
{cgzhu,huyang,khurshid}@utexas.edu,linyi2@illinois.edu
ABSTRACT
Automaticmachinelearning,orAutoML,holdsthepromiseoftruly
democratizingtheuseofmachinelearning(ML),bysubstantially
automating the work of data scientists. However, the huge com-
binatorialsearchspaceofcandidatepipelinesmeansthatcurrent
AutoML techniques, generate sub-optimal pipelines, or none at
all, especially on large, complex datasets. In this work we propose
an AutoML technique SapientML, that can learn from a corpus
of existing datasets and their human-written pipelines, and effi-ciently generate a high-quality pipeline for a predictive task ona new dataset. To combat the search space explosion of AutoML,
SapientML employs a novel divide-and-conquer strategy realized
as athree-stage programsynthesis approach, thatreasons on suc-
cessively smallersearch spaces. The firststage uses meta-learning
topredictasetofplausibleMLcomponentstoconstituteapipeline.
In the second stage, this is then refined into a small pool of vi-able concrete pipelines using a pipeline dataflow model derived
fromthecorpus.Dynamicallyevaluatingthesefewpipelines,inthe
third stage, provides the best solution. We instantiate SapientML
as part of a fully automated tool-chain that creates a cleaned, la-beled learning corpus by mining Kaggle, learns from it, and uses
the learned models to then synthesize pipelines for new predictive
tasks.Wehavecreatedatrainingcorpusof1,094pipelinesspanning
170datasets,andevaluatedSapientMLonasetof41benchmark
datasets, including 10 new, large, real-world datasets from Kaggle,
and against 3 state-of-the-art AutoML tools and 4 baselines. Our
evaluationshowsthatSapientMLproducesthebestorcomparable
accuracy on27 of thebenchmarks while thesecond best toolfails
to even produce a pipeline on 9 of the instances. This difference
isamplifiedonthe10mostchallengingbenchmarks,whereSapi-
entML wins on 9 instances with the other tools failing to produce
pipelines on 4 or more benchmarks.
Â¶These authors contributed to this work as interns at Fujitsu Research of America.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation
on the first page. Copyrights for components of this work owned by others than ACMmustbehonored.Abstractingwithcreditispermitted.Tocopyotherwise,orrepublish,topostonserversortoredistributetolists,requirespriorspecificpermissionand/ora
fee. Request permissions from permissions@acm.org.
ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
Â© 2022 Association for Computing Machinery.
ACM ISBN 978-1-4503-9221-1/22/05...$15.00
https://doi.org/10.1145/3510003.3510226CCS CONCEPTS
â€¢Software and its engineering â†’Automatic programming.
KEYWORDS
Program Synthesis, Machine Learning, AutoML, Program Analysis
1 INTRODUCTION
The explosive growth in machine learning (ML) applications, over
the past decade, has created a huge demand for data scientists
(DS) and ML practitioners to develop real-world ML solutions. The
2018 LinkedIn Workforce Report showed a shortage of 151,717 DS,
nationwide[ 26],thathadgrownto250,000by2020[ 32].Automatic
machine learning, or AutoML, holds the promise of addressing this
shortfall [ 16,19,51]. AutoML can improve productivity of data
science teams and cover gaps in expertise.
Given a dataset and a predictive task (e.g., classification or re-
gression) AutoML aims to create an ML pipeline that trains an
optimized ML model for the given task. Simply put, the pipeline is
a sequence of ML operators that processes data to make it suitable
for learning (feature engineering (FE)), fits a suitable ML model on
it(modelselection),andcalculatesthepredictiveperformanceof
the model. One of the prominent instances of AutoML, the subject
of much research recently, is creating supervised ML pipelines for
tabular data [ 7,10,15,30,40,49,50]. This paper also focuses on
this formulation of AutoML.
AutoML has been traditionally solved as a search and optimiza-
tion problem â€“ selecting the best pipeline from a space of candi-
dates [15,30,43,49,50]. However, ML pipelines are also programs,
infactrelativelysmall,highlystructureddomain-specificprograms,
thatcouldbeamenabletoprogramsynthesis.Further,publicreposi-torieslikeKaggle[
21]andGitHubcontainhundredsofthousandsof
human-written ML pipelines that could serve as starting points for
synthesizingnewpipelines.Indeed,programsynthesisbymining
orlearningfromexistingprogramcorporahasbeensuccessfully
deployedforotherendpointsofsynthesis[ 3,25,28,29,33].Emerg-
ingresearch[ 7,10]demonstratesthepromiseofthisperspective
forMLpipelinesynthesis.Ourworkalsofollowsthisphilosophy
but offers a novel take on the core challenge of AutoML.
The central challenge of AutoML is the massive combinato-
rialsearchspaceofcandidateMLpipelinesitexposesâ€“composi-
tions of different potential FE operators, each applied on different
columnsofthesubjectdataset,composedwithoneofseveralpoten-
tialmodelsortheirensembles.Further,eachpipelinecomponent
may have its own space of hyper-parameters. Previous AutoML
19322022 IEEE/ACM 44th International Conference on Software Engineering (ICSE)
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:52:10 UTC from IEEE Xplore.  Restrictions apply. ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA Saha et al.
techniques adopt several approaches to combat this combinato-
rial explosion. Some try to search a restricted search space byexcluding FE from consideration [
15,43,49], searching specific
pipelinetopologies[ 30],orapre-compiledexplicitcorpusofsyn-
theticpipelines[ 15,49,50].Otherstrytoprunethesearchspaceby
usinglearnedlanguagemodelscoupledwithaggressivedynamic
evaluationof partialpipelines [10]orbywarm-startingsearchus-
ing constraints mined from human-written pipelines [ 7]. However,
navigatingthehugecombinatorialsearchspaceofAutoMLremains
an open problem. In fact, noneof the above techniques apply FE
transformstospecificdatasetcolumns,asahumanDSwould.In-
stead,theyareblindlyappliedtothecompletedataset,ostensibly
to avoid injecting another set of hyper-parameters into the search
space.
Insight. Our key insight is that the root cause of the AutoML
search-space explosion is because previous AutoML techniques
reasononcompleteMLpipelines(combinations ofvariousMLcom-
ponents) as a single entity, ostensibly to capture dependencies be-
tweenMLcomponents.However,weobservethatinmanypractical
instances,thedecisiononwhethertoincludeaparticularcompo-
nent (say an imputer) in a pipeline can be made based primarily
on properties of the dataset (whether or not it has missing values),
independent of other components. Indeed, human DS often employ
suchbestpractices whenmanuallyconstructingMLpipelines.Once
thesetof plausiblecomponents touseforagivendatasetareknown,
theycanbeusedtoassembleatargetpipelineorasmallpopulation
of plausible target pipelines to choose from. This would substan-
tially mitigate the combinatorial explosion coming from exploring
arbitrary combinations of components. Further, we hypothesizethat these DS best practices are represented in publicly available
human-written MLpipelines (sayon Kaggle). Thus,these pipelines
can potentially be mined to learn and then replicate human DS
decision making to create viable pipelines for new datasets.
Proposed approach. Pursuant to the above insight, we pro-
pose an AutoML technique SapientML1, that can learn from a
corpusofexistingdatasetsandtheirpipelines,andgenerateahigh-
qualitypipelineforapredictivetask onanewdataset.Tocombat
thesearchspaceexplosionofAutoML,SapientMLemploysanoveldivide-and-conquerstrategy,realizedasathree-stageprogramsyn-
thesis approach that reasons on successively smaller search spaces.
The first stage uses meta-learning to train a meta-model (offline
phase) which is then used to independently predict the suitabil-
ityof each ML component with respect to the given dataset (in
the online phase). Specifically, this meta-model captures the re-lationship between features of the dataset (e.g., the presence of
missingdatavalues)anddesiredcomponentsinthepipeline(e.g.,
adatainterpolationcomponent).Thispredictionyieldsarankedlistofpipelineskeletons.Eachpipelineskeletonisan(unordered)
set of plausible ML components, to constitute a pipeline, each com-
ponent mapped to specific (or all) dataset columns on which itshould be applied. In the second stage, the skeletons are then in-
stantiated into a small pool of viable concrete pipelines using a
pipelinedataflowmodel minedfrom thecorpus,and asmalllibrary
of standard implementations for each ML component. For each
candidate skeleton the pipeline components are correctly ordered
1anAutoMLapproachharnessingthewisdom(sapere )ofhuman(sapien )datascientists.and incompatible components discarded using the dataflow model,
and each component instantiated using code templates from the
library.Dynamicallyevaluatingthesefewpipelines(themostex-
pensive operation), in the third stage, yields the best pipeline. The
concept of a multi-stage approach employing more expensive anal-
ysesonsuccessivelysmallerspaceshasbeensuccessfullyusedin
otherdomains,includingautomaticprogramrepair[ 28]andcode
search [27], among others. Our specific design is customized for
ML pipeline synthesis.
WeinstantiateSapientMLaspartofafullyautomaticend-to-
end tool-chain that mines datasets and corresponding pipelines
from Kaggle, automatically cleans and labels each pipeline, learns
from this corpus and then synthesizes ML pipelines for predictive
tasks on new datasets. We evaluate SapientML on a set of 41benchmarkdatasets,including10new,large,real-worlddatasets
fromKaggleandagainst3stateoftheartAutoMLtools(AL[ 10],
auto-sklearn[ 15],TPOT[30])and4baselines.Ourevaluationshows
thatSapientMLproducesthebestorcomparableaccuracyon27
of the benchmarks while the second best tool (AL), fails to evenproduce a pipeline on 9 of the instances. Further, on the most
challenging10benchmarksSapientMLwinson9instanceswith
the other AutoML tools failing on 4 or more benchmarks.
This paper makes the following main contributions:
â€¢Technique: A learning-based AutoML technique SapientML,
thatcanefficientlysynthesizehigh-qualitysupervisedMLpipelines,
using a novel divide-and-conquer approach to circumvent the
combinatorial state-space explosion of AutoML.
â€¢Tool:Animplementationof SapientMLaspartofanautomated
tool-chain that creates a cleaned, labeled learning corpus bymining Kaggle, learns from it, and uses the learned models to
then synthesize pipelines for predictive tasks on new datasets.
â€¢Evaluation: Asubstantialevaluationof SapientMLonabench-
mark of 41 datasets, including 10 new, large, real-world datasets
from Kaggle, comparing it to 3 state of the art AutoML tools and
4 baseline techniques for creating ML pipelines.
2 PROBLEM DEFINITION
Atabular dataset, ğ·=(ğ‘‹Ã—ğ‘Œ)âˆˆDis sampled from a distribution
overadomain XÃ—YwhereXandYdenoteaninputdomainandan
outputdomainrespectively. ğ‘‹iscomprisedof ğ‘›rowsandğ‘‘columns,
calledfeatures,whereeachrowrepresentsanobservationconsisting
ofğ‘‘valuesfrom X.Similarly ğ‘Œiscomprisedof ğ‘›rowsand ğ‘¡columns
whereeachrowisa ğ‘¡-tupleofvaluesorlabelsfrom Y.Asupervised
predictivetask onğ·istolearnapredictionfunction â„:ğ‘‹âˆ’ â†’ğ‘Œsuch
thatğ‘¦â‰ˆâ„(ğ‘¥). A predictive task is called a classification task when
theğ‘¦isdiscreteandcalleda regression taskwhen ğ‘¦iscontinuous.
For multi-label classification and multivariate regression, |ğ‘¡|>1.
Applying supervised machine learning (ML) to a predictive task
requires a training version of the dataset ğ·ğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘›to train an ML
model,andaheldouttestdataset, ğ·ğ‘¡ğ‘’ğ‘ ğ‘¡toevaluateitsperformance.
A single dataset ğ·can also be split into ğ·ğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘›andğ·ğ‘¡ğ‘’ğ‘ ğ‘¡.
Given a dataset ğ·,a nML pipeline (ğ‘ƒâˆˆP) is a sequence of FE
componentsfollowedbyamodelcomponentthatrealizesagiven
predictivetask.Hence, ğ‘ƒ=[ğ‘1
ğ‘“,ğ‘2
ğ‘“,..,ğ‘ğ‘˜
ğ‘“,ğ‘ğ‘š]representsapipeline
withğ‘˜FE components and one model. A pipeline component ğ‘âˆˆC
is comprised of one or more API calls, and associated glue code,
thattogetherperformsanatomicdata-specificpipelinetask,e.g.,
1933
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:52:10 UTC from IEEE Xplore.  Restrictions apply. SapientML: Synthesizing Machine Learning Pipelines by Learning from Human-Written Solutions ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
/angbracketleftFE:OrdinalEncoder (card4, . . . ), 0.73/angbracketright ğ¶/prime
ğ‘“
/angbracketleftFE:OneHotEncoder (card4, . . . ), 0.70/angbracketright
/angbracketleftFE:Imputer (card2, card3,. . . ), 0.81/angbracketright
/angbracketleftFE:LinearScaler (ğ‘‹),0.69/angbracketright
/angbracketleftFE:DataBalancer (ğ‘‹), 0.58/angbracketright
/angbracketleftMODEL:CatBoostClassifier (ğ‘‹), 1/angbracketright
/angbracketleftMODEL:ExtraTreesClassifier (ğ‘‹), 2/angbracketright ğ¶/prime
ğ‘š
/angbracketleftMODEL:XGBClassifier (ğ‘‹), 3/angbracketright
(a) Predicted FE and Model components by the skeleton
predictor with their probability scores and rank respectively
ğ¶/prime
ğ‘“âˆª/angbracketleftMODEL:CatBoostClassifier (ğ‘‹)/angbracketright
ğ¶/prime
ğ‘“âˆª/angbracketleftMODEL:ExtraTreesClassifier (ğ‘‹)/angbracketright
ğ¶/prime
ğ‘“âˆª/angbracketleftMODEL:XGBClassifier (ğ‘‹)/angbracketright
(b) Three skeletons generated by the pipeline seeding phase
/angbracketleftFE:Imputer (card2, card3,. . . )/angbracketright
/angbracketleftFE:OrdinalEncoder (card4, . . . )/angbracketright
/angbracketleftFE:LinearScaler (ğ‘‹)/angbracketright
/angbracketleftFE:DataBalancer (ğ‘‹)/angbracketright
/angbracketleftMODEL:CatBoostClassifier (ğ‘‹)/angbracketright
(c) First skeleton after ordering and redundancy removal
Figure 1: Artifacts of Pipeline Seeding and Pipeline
Instantiation Phases for IEEE-CIS-Fraud-Detection Example
filling missing values or transforming a categorical column to a
set of numeric columns. There are two kinds of components: i)the FE components (
ğ‘ğ‘“) that transforms a feature ( ğ‘¥) or a set of
features(ğ‘‹/primeâŠ‚ğ‘‹)includingdatawranglingtasks,andii)themodel
components ( ğ‘ğ‘š) that performs the actual learning and prediction.
Given dataset ğ·=ğ·ğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘›âˆªğ·ğ‘¡ğ‘’ğ‘ ğ‘¡, a predictive task on ğ·, and
an accuracy metric ğœ(e.g., F1 score for classification and ğ‘…2for
regressionproblemsrespectively),ouraimistocreateanexecutablemachinelearningpipeline
ğ‘ƒforthisdatasetandtaskthatmaximizes
ğœonğ·ğ‘¡ğ‘’ğ‘ ğ‘¡.Weposethis pipelinesynthesis problemasaprogram
synthesis problem with quantitative objectives, akin to [17].
3 MOTIVATING EXAMPLE
Inthissection,weillustratetheuse-caseandmechanicsofour
technique using a real-world dataset IEEE CIS Fraud Detection [44],
providedbythecompanyVestaandhostedonKaggle.Itcontains
591K rows of data, each corresponding to an e-commerce trans-action represented by a rich set of 394 features. The features aremainly numeric (e.g., transaction amount) and string categorical
values (e.g., device type). Some features are missing in some trans-
actions.Thepredictivetaskistolabelatransactionasfraudulent
or not, based on its features, i.e., a binary classification task.
Use Case. Creating a pipeline for a predictive task may take
a long time for a DS. The DS needs to decide on the appropriateset of feature engineering (FE) (
ğ‘ğ‘“) and model ( ğ‘ğ‘š) components
touse,therightdatasetcolumns(features)toapplyeachofthem
on, and then instantiate them in the right order so the pipeline
executesonthedataset( ğ·)withouterrors.Giventhehugespace
of possibilities forthese decisions, data scientiststypically rely on
their understanding of ğ·, past experience, and often brute-forcetrialanderror,tocompletethislaborioustask[ 24].AutoMLtools
can accelerate this process substantially, especially for a novice
DS,byprovidingherwithagood-quality,executablepipelinefor
potential last-mile optimization.
KeyChallenge. Real-world,large,complexdatasetslike IEEE
CIS Fraud Detection present particularly challenging cases for cur-
rent AutoML tools. In order to navigate the huge combinatorial
searchspaceofpossiblecandidatepipelinestoolssuchasTPOT[ 30]
andauto-sklearn[ 15]restrictthemselvestonumericdata,whichen-
sures smaller, simpler pipelines. Thus, they cannot even run on the
givendatasetsinceithasstringcategoricalfeatures.Thestate-of-
the-art tool AL [ 10] uses a combination of learned language model
andaggressivedynamicevaluationofpartialpipelinestosearchfor
a viable solution. However, in this case, it evaluates 1,641 partial
and1,310completepipelinesin1.5hours(ona8vCPUand32GB
memorymachine)andfinallycrashesduetoaninternaltimeout
without producing any pipeline.
SapientMLâ€™s three-stageprogramsynthesisapproachproves
to be quite effective on this example. In the first stage (Section 4.3),
SapientML uses a machine-learned model, trained on its meta-
learningcorpus ofhuman-writtenpipelines, togeneratearanked-
listofpipelineskeletons,toconstructviablepipelines.Forthepresent
example, SapientML first predicts five potential FE components
andthe topthree mostappropriatemodels inFigure1a togenerate
three pipeline skeletons in Figure 1b. The predicted components
broadly agree with human intuition. For instance, OrdinalEncoder
andOneHotEncoder are reasonable transforms to encode the String-
basedfeaturesinthedatasetandtheuseof DataBalancer comports
withthe significantimbalance betweenthenumber offraudulent
and valid transactions in the dataset. Further, the choice of the
CatBoost model is consistent with the previous research [ 42] show-
ingthat CatBoost performswellforclassificationonlarge,imbal-
anceddata. SapientML furtheruses thedecisionrules learnedby
theskeletonpredictortoinfertherelevantfeaturesinthedataset
where each FE transform will be applied. For example, Figure 1a
shows that SapientML targets card2, . . . forSimpleImputer .
In the second stage (Section 4.4), SapientML concretizes the
pipeline skeleton into a set of executable pipelines. To this end,it uses the confidence scores included in the skeleton as well asapipeline dataflow meta-model mined (offline) from the learning
corpus to discard redundant FE components, and order the compo-
nentsinasyntacticallycorrectfashion,toproduceorderedskele-
tons, as shown in Figure 1c. For instance, the analysis concludesthat both
OrdinalEncoder andOneHotEncoder are to be applied on
thesamedatasetcolumnsbutcannotbesimultaneouslyused.Thus,
OneHotEncoder whichhasalowerconfidencescore,isdiscarded.As
anotherexample, Imputerisorderedbefore OrdinalEncoder ,follow-
ingtheminedcomponentorderrelation.Nexttheorderedskeletons
are transformed into a set of concrete pipelines (three in this case).
Inthefinalstage,SapientMLevaluatesthesecandidatepipelines
on a held-out validation dataset (derived from only the trainingdataset,notthetestingdataset)andreturnsthehighestaccuracy
pipeline.Figure2showsanabridgedversionofthispipeline.The
pipeline implements a rich set of five FE components each applied
to its appropriate columns and paired with a CatBoost gradient-
boosting classification model. SapientML takes only 8 mins to
produce this pipeline and produces a respectable 0.82 F1 score.
1934
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:52:10 UTC from IEEE Xplore.  Restrictions apply. ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA Saha et al.
import pandas as pd # LOAD DATA
__train_dataset=pd.read_csv("training.csv", delimiter=",")
__test_dataset=pd.read_csv ("test.csv", delimiter=",")
from sklearn.impute import SimpleImputer # FE TRANSFORM 1
import numpy as np
_NUMERIC_COLS_WITH_MISSING_VALUES = ['card2', 'card3', .. ' V339']
for_col in _NUMERIC_COLS_WITH_MISSING_VALUES:
__imputer = SimpleImputer(missing_v alues=np.nan, strategy=' mean')
__train_dataset[ _col] = __imputer. fit_transform(__train_dataset[ _col].values.reshape(-1 ,1))[:,0]
__test_dataset[_col] = __imputer.transform(__test_dat aset[_col].astype(
__train_dataset[ _col].dtypes).values.reshape(-1,1))[:,0]
_STRING_COLS_WITH_MISSING_VALUES = ['card4', 'card6',..., 'M9'] # FE TRANSFORM 2...## Apply SimpleImputer forstring columns similar to the FE transform 1 s hown above
from sklearn. preprocessing import OrdinalEncoder # FE TRANSFORM 3...## Apply OrdinalEncoder to categorical columns ['ProductCD', 'card4', .., 'M9']
__feature_train = __train_dataset. drop(['isFraud'], axis=1) # DETACH TARGET
__target_train =__train_dataset['isFraud']
__feature_test, __target_test = __test_dataset. drop(['isFraud'], axis=1), __test_dataset['isFraud']
from sklearn. preprocessing import StandardScaler # FE TRANSFORM 4...## Apply StandardScaler to __feature_train and __feature_test
from imble arn.over_sampling import SMOTE # FE TRANSFORM 5
__feature_train, __target_train = S MOTE().fit_resample(__feature_train, __target_train)
from catbo ost import CatBoostClassifier # MODEL
__model = CatBoostClassifier()__model.fit(__feature_train, __target_train)
__y_pred = __model.predict(__feature_test)
from sklearn import metrics
# EVALUATION
print(metrics.f1_score(__target_test, __y_pred, a verage='macro'))
Figure 2: Abridged version of pipeline generated by SapientML for the IEEE-CIS-Fraud-Detection prediction task
4 APPROACH
4.1 Overview
Figure3presentsahigh-leveloverviewoftheSapientMLsystem.It
has an offline and an online phase. In the offline phase SapientML
creates a corpus of human-written pipelines and their datasets,
calledthe meta-learningcorpus,byminingdata-sciencerepositories
(Kaggleinourcase)andautomaticallycuratingthedataforlearning,
through denoising, augmentation, and labeling. The meta-learning
corpus is then used to build two meta-models, namely the skeleton
predictor meta-model and the pipeline dataflow meta-model. In the
onlinephase,givenanewdatasetandapredictivetask(classifica-
tionorregression)definedonit,SapientMLusestwometa-models
to synthesize a supervised ML pipeline for the given dataset and
task, which maximizes some accuracy metric (e.g., F1 or R2).
SapientML navigates the huge combinatorial search space of
AutoML through a novel three-stage program synthesis approach
that reasons on successively smaller search spaces. The first stage,
calledpipeline seeding , uses the skeleton predictor derived through
meta-learning on the meta-corpus, to independently predict the
suitability ofeachMLcomponenttoappearinanMLpipelinefor
thegivendataset,basedonthemeta-featuresofthedataset.This
predictionyieldsa pipelineskeleton,anunorderedsetofplausible
components,toconstituteasolutionpipeline.Inthesecond, pipeline
instantiation stage, this skeleton is concretized into a small pool of
viablecandidate pipelines, using the dataflow meta-model mined
fromthecorpus,tocorrectlyorder,minimize,andinstantiatethepipelinecomponents.Thefinal, pipelinevalidation stageselectsthe
highest accuracy ML pipeline among the candidate pipelines by
dynamically evaluating them. The following sub-sections describe
the meta-corpus creation and the three pipeline synthesis stages.
4.2 Creation of the Meta-Learning Corpus
Thisstepautomaticallyminesandcuratesahigh-qualitycorpusthat
includes human-written ML pipelines and their datasets, to power
themeta-learningof SapientML.Thesepipelinesnaturallycapture
theexpertiseanddomainknowledgeofhumanDSasopposedto
creating a relatively small, homogeneous, synthetic ML pipeline
corporausedbysomeotherAutoMLtechniques[ 15,50]thatincurs
significantcomputationalcost.Tobuildthecorpus,wefirstmine
the datasets and their pipelines from Kaggle [ 23]â€“apopular data-
science repository. Specifically, we collected top 350 datasets based
on user votes, and up to 100 top-voted pipelines per dataset, giving
us around 2,500 initial pipelines. These raw pipelines and datasets
arefurtherdenoised,augmented,andlabelledtomakethemsuitable
for learning by SapientML. Our final corpus is comprised of 1,094
pipelines across 170 datasets.
4.2.1 Denoisingpipelines. Human-writtennotebooksonKaggle
oftencontain noiseintheformofexploratorydataanalysis,visu-
alization, and debugging code that while useful for human com-
prehension,areirrelevanttoMLmodelexecution.Further,some
pipelinesmaynolongerbeexecutableduetovariousissuessuch
asdeprecatedAPIsanddifferencesintheruntimeenvironment.To
1935
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:52:10 UTC from IEEE Xplore.  Restrictions apply. SapientML: Synthesizing Machine Learning Pipelines by Learning from Human-Written Solutions ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
Figure 3: Overview of SapientML system.
constructacleanmeta-learningcorpus,wefirstdiscardanypipeline
that fails to run successfully on our environment. Then to remove
the noise in each executable pipeline, ğ‘ƒ, we first heuristically iden-
tify a criteria line ( ğ‘™ğ‘ğ‘Ÿ), which performs the final prediction task.
InpipelinesusingthepopularpythonMLlibrariessuchasScikit-
learn[34] and XGBoost [ 48] this is typically a call to the predict
API function. Then, we compute a forward slice ğ‘ƒğ‘“ğ‘œ ğ‘Ÿğ‘¤and a back-
ward slice ğ‘ƒğ‘ğ‘ğ‘ğ‘˜fromğ‘™ğ‘ğ‘Ÿ, by applying standard dynamic program
slicing [1] and concatenate ğ‘ƒğ‘“ğ‘œ ğ‘Ÿğ‘¤,ğ‘™ğ‘ğ‘Ÿ, andğ‘ƒğ‘ğ‘ğ‘ğ‘˜to yield the clean
pipelineğ‘ƒğ‘ğ‘™ğ‘’ğ‘ğ‘›. We compare the accuracy scores of ğ‘ƒandğ‘ƒğ‘ğ‘™ğ‘’ğ‘ğ‘›
and discard ğ‘ƒğ‘ğ‘™ğ‘’ğ‘ğ‘›if it obtains a lower score than ğ‘ƒ.
4.2.2 Augmentingpipelines. Thisstepismotivatedbytheobser-
vation that human-written pipelines may not contain the best rep-
resentativeMLmodelchoiceforeachdataset.Thiscouldhappen
in pipelines written by novice data scientists or because of the
availability of newer, better models after thepipeline was written.
Presenceof sub-optimalMLmodelsinour meta-learningcorpus
inturndegradesthequalityofthepipelinessynthesizedbySapi-
entML. To alleviate this problem we employ a data augmentation
technique to systematically replace sub-optimal models in meta-
corpus pipelines with better, i.e., higher-accuracy, models. Dataaugmentation [
36] is commonly employed in machine learning
flows to improve the predictive quality of training data.
Generation of candidates. To improve the performance score
ofadenoisedpipeline ğ‘ƒğ‘ğ‘™ğ‘’ğ‘ğ‘›withmodel ğ‘ğ‘š,ourdataaugmentation
techniquesystematicallyreplacesthemodel ğ‘ğ‘šinğ‘ƒğ‘ğ‘™ğ‘’ğ‘ğ‘›byeach
viable model in the corpus, Cğ‘š={ğ‘1ğ‘š,...,ğ‘ğ‘ğ‘š}one at a time,t o
create a set of candidate pipelines Pğ‘šğ‘¢ğ‘¡ğ‘ğ‘¡ğ‘’ğ‘‘ ={ğ‘ƒ1
ğ‘ğ‘™ğ‘’ğ‘ğ‘›...ğ‘ƒğ‘
ğ‘ğ‘™ğ‘’ğ‘ğ‘›}.
Toidentifythecodefor ğ‘ğ‘š,westartwith ğ‘™ğ‘ğ‘Ÿ(definedinSection4.2.2)
andcomputeabackwardsliceuptothemodeldeclaration.Nextweidentifythevariablenamesofthemodelobject,
ğ‘‹ğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘›,ğ‘Œğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘›,ğ‘‹ğ‘¡ğ‘’ğ‘ ğ‘¡,
andğ‘Œğ‘¡ğ‘’ğ‘ ğ‘¡throughstaticanalysis.Finallywereplacethedeclaration
of the old model ğ‘ğ‘šwith a new model ğ‘ğ‘–ğ‘što generate ğ‘ƒğ‘–
ğ‘ğ‘™ğ‘’ğ‘ğ‘›.
Selection. Each mutated pipeline in Pğ‘šğ‘¢ğ‘¡ğ‘ğ‘¡ğ‘’ğ‘‘is run on the cor-
responding dataset, and the best mutated pipeline, ğ‘ƒğ‘ğ‘’ğ‘ ğ‘¡
ğ‘ğ‘™ğ‘’ğ‘ğ‘›, showing
the highest improvement in the performance score replaces the
original pipeline in the corpus.
4.2.3 Creationofabstractpipelinesformeta-learning. Theobjec-
tive of creating a meta-learning corpus is to provide SapientML
necessarytrainingdatatolearntherelationshipbetweenvarious
dataset properties and ML comp onents. However , since human-
writtenMLpipelinesarediverseintermsofimplementation,itis
challengingtolearntherelationshipsbetweenvariousdatasetprop-
ertiesandrawcodesnippets.Tokeepthemeta-learningtractable,we represent a pipeline at an abstract level as a sequence of ML
components, ğ‘ƒ=[ğ‘1
ğ‘“,ğ‘2
ğ‘“,..,ğ‘ğ‘˜
ğ‘“,ğ‘ğ‘š]whereğ‘ğ‘–
ğ‘“andğ‘ğ‘šrepresent the
labels of FE and model components respectively.
To this end, first we automatically annotate each com-
ponent with a label that has two pieces of information:
/angbracketleftcomponent_type :API_Name /angbracketright.Weprimarilydistinguishtwotypes
of components: feature engineering ( FE) andMODEL. The automatic
labelingprocessinvolvestwosteps:i)extractingtheAPIname,and
ii)identifyingwhetheraparticularAPIisan FEoraMODELcompo-
nent. We perform an AST analysis to extract API names from each
statement, ignoring any APIs that are part of boilerplate code. For
example, almostevery FEengineeringAPIs inScikit-learn library
areaccompaniedbyatemplatecodethatcontains fit,transform ,
orfit_transform APIs. Once we have all the API names, we first
annotate the model component (already identified in Section 4.2.2
for each pipeline in our corpus). All components appearing before
the MODEL component are labeled as FE components.
Atthispoint,theabstractpipeline ğ‘ƒispresentedattheAPIlevel.
However, there are many labels in the corpus that are function-
allysimilaracrosspipelines.Forexample,adatascientistcanuse
either the fillnaAPI from pandasorSimpleImputer fromsklearn
to fill out the missing values in a dataset. To learn meaningfulpatterns of meta-features with respect to these labels, we have
togroupthecomponentsthataresemanticallysimilar.Applying
domain-knowledge in ML is a standard practice and meta-learning
isnoexception.Therefore,weinvestigatedthelabelsinourproject
corpus and group them based on their functionality by studyingthe API documentation. Then we assigned each group a func-tional label. For example, we grouped
FE:fillna ,FE:interpolate ,
FE:SimpleImputer andFE:KNNImputer togetherandmappedtoauni-
fied label FE:Imputer since they all are used to impute missing
values in a dataframe.
4.3 Stage 1: Pipeline Seeding
Givenadataset( ğ·)andpredictivetask,theobjectiveofthepipeline
seedingstageistoproduce aranked-listof pipelineskeletons, S=
[ğ‘†1,...ğ‘† ğ‘˜]. This is used to constitute concrete candidate pipelines
in the subsequent pipeline instantiation stage (Section 4.4).
A pipeline skeleton ( ğ‘†) is a (unordered) set of plausible ML com-
ponents that includes zero or more FE components and one model
component (Definition 4.3). To predict the ML components in ğ‘†,
SapientML uses a meta-learning model, called the skeleton predic-
tor, trained during the offline meta-training phase. The skeleton
predictorisarchitectedasasetofsub-models,eachofwhichlearns
amappingbetweenproperties(meta-features)ofadataset ğ·and
1936
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:52:10 UTC from IEEE Xplore.  Restrictions apply. ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA Saha et al.
Table 1: Summary of Meta-Features
High-Level Property Meta-features
Shape of dataset (3) Number of rows, features, and targets
Missing entries (1) Presence of missing values
Feature types (10) Presenceand#features,whosedatatypeisnumeric,
number category, string category, text, and date.
Measure of symmetry (4) Skewness and Kurtosis (normal, uniform, and
tailed)
Measure of Distribution (6) Normal, Uniform, and Poisson distribution for fea-
tures and target
TendencyandDispersion(3) Normalized mean, standard deviation, variation
across columns
Correlated features (3) Pearson correlation (min, max, number of corre-
lated features)
Outliers (2) #features that contains few or many outliers.
Value frequency (3) Numberoffeatureswhosevaluesaresparse,imbal-
anced, dominant
Target property (3) Imbalanced, continuous or categorical.
Table 2: Meta-Targets(C: Classification, R: Regression)
Feature Engg. Model C R Model C R
Imputer RandomForest x x SVM x x
OrdinalEncoder ExtraTrees x x LinearSVM x x
OneHotEncoder LightGBM x x LogisticRegression x x
TextVectorizer XGBoost x x Lasso - x
TextPreprocessor CatBoost x x SGD x x
DateFeaturization GradientBoosting x x MLP x x
LinearScaler AdaBoost x x MultinomialNB x -
LogScaler DecisionTree x x GaussianNB x -
DataBalancer -- --- -
the likelihood of a specific ML component (meta-target) appearing
in a pipeline for ğ·.
Definition 4.1 (Meta-features). A set of meta-features, Î¦=
{ğ›¼1,ğ›¼2,...,ğ›¼ ğ‘™},quantifythecharacteristicsofadatasetwhereeach
meta-featureiscomputedbyafunction ğ›¼ğ‘–:ğ·âˆ’ â†’Rthattakesthe
dataset as input and outputs a real number.
4.3.1 DesignofMeta-Features. Agoodsetofmeta-featuresshould
havethreeproperties:i)theyareexpressiveenoughtocharacterize
the dataset, ii) they are succinct enough so that there exist some
meaningful patterns that skeleton predictor can learn with respect
totheMLcomponents,andiii)theyareefficienttocompute.For
example,thechoiceof FEandMODELcomponentoftendependson
the meta-features such as number of records and features in ğ·
and feature types. Based on the existing literature [ 10,15] and
ourexperience,wecompute38meta-featurestocharacterizethe
datasets in our meta-training corpus. Table 1 presents the list of
meta-features used in SapientML.
Definition4.2(Meta-Targets). AsetofMLcomponents C=Cğ‘“âˆª
Cğ‘šthatdefinethepredictionspaceofskeletonpredictorwhere Cğ‘“
andCğ‘šrepresent FE and model components respectively.
4.3.2 Meta-targets. Each ML component in the abstract pipelines
created in Section 4.2.3 is a valid meta-target for SapientML. How-
ever,tolearnanymeaningfulpatternbetweenmeta-features( Î¦)
andaparticularMLcomponent ğ‘,weneedsufficientoccurrences
ofğ‘in the meta-corpus. Therefore, we excluded any ML compo-
nentsthatappearedlessthanfivetimesinourcorpus.Thisfiltering
criteria provided us 9 FE components and 29 model components
(15 classification models and 14 regression models) as meta-targets.
Table2summarizestheMLcomponentsthatSapientMLâ€™smeta-
model predicts.4.3.3 Design of Skeleton Predictor(Meta-Models). Given a setof
meta-features, Î¦computedfrom ğ·,theobjectiveofskeletonpre-
dictor is to predict a set of plausible FE components and model
componentstogeneratepipelineskeletonsdefinedinDefinition4.3.
Definition 4.3 (Skeleton). A skeleton, ğ‘† =
{/angbracketleftğ‘1
ğ‘“(ğ‘‹1),ğœŒ1/angbracketright,...,/angbracketleftğ‘ğ‘
ğ‘“(ğ‘‹ğ‘),ğœŒğ‘/angbracketright,ğ‘ğ‘š(ğ‘‹)}isasetoftuplescomprised
ofğ‘FE components and one model component where /angbracketleftğ‘ğ‘–
ğ‘“(ğ‘‹ğ‘–),ğœŒğ‘–/angbracketright
represents that the FE component ğ‘ğ‘–
ğ‘“will be used in the pipeline
with a probability ğœŒğ‘–and applied on ğ‘‹ğ‘–âŠ‚ğ‘‹features in ğ·.
Weusethefollowinginsightstodesignourskeletonpredictor.
First,apipelinemayrequireseveralFEcomponentsandinmany
cases the decision of using a particular FE component can be made
based on a few meta-features without depending on other FE com-
ponents.Although occasionallythere canbesome dependencies
betweentheMLcomponents,ourexperimentalresultsshowthat
this design decision leads to faster generation of pipelines without
sacrificing accuracy. To this end, we design the FE component pre-
dictorasasetofbinaryclassifiers {ğœ†1,...ğœ†9}thatpredictswhether
aparticularFEcomponent ğ‘ğ‘–
ğ‘“âˆˆğ¶ğ‘“shouldbeusedinthegenerated
pipelineforthetargetdataset, ğ·.Ontheotherhand,sincebydesign
SapientML allows only one model ğ‘ğ‘šâˆˆğ¶ğ‘šin a skeleton, we cast
the model selection problem as a ranking problem and design a
learning-to-rank model to rank all the model components for ğ·.
Definition4.4(SkeletonPredictor). Theskeletonpredictoriscom-
prisedofasetofsub-models, Î›={ğœ†1,...ğœ†9,ğœ†ğ‘š}whereeachsub-
model approximates a function, ğœ†ğ‘–:Rğ‘™âˆ’ â†’ğ‘¦/prime
ğ‘–where Rğ‘™is a set of
meta-featurevaluesand ğ‘¦/prime
ğ‘–isaprobabilityscoreofanMLcompo-
nent (meta-target) appearing in a pipeline for ğ·.
Sub-modelstopredicttheFEcomponents. FEcomponents
are generally applied on a sub-set of features of ğ·. For example,
an<FE:Imputer> isgenerallyappliedonthefeatureswithmissing
values. Therefore, SapientML aims to predict not only an FE com-
ponent (ğ‘ğ‘–
ğ‘“) forğ·but also infers the subset of features ğ‘‹/primeâŠ‚ğ‘‹
inğ·on which ğ‘ğ‘–
ğ‘“would be instantiated on. To facilitate the de-
terminationof ğ‘‹/primeforğ‘ğ‘–
ğ‘“,aDecisionTreeclassifier isanaturalfit
since the classifier would learn a set of precise conditions w.r.t. the
meta-featurestoselect ğ‘ğ‘–
ğ‘“forğ·andlaterwecananalyzethosecon-
ditions to infer ğ‘‹/primeforğ‘ğ‘–
ğ‘“. However, DecisionTree models tend to
overfit with a large number of features [ 35]. To minimize the effect
ofirrelevantfeatures,wefirstperforma pointbiserialcorrelation
analysis between the meta-features ( Î¦) and each FE component
(ğ‘ğ‘–
ğ‘“) and use only the meta-features ( Î¦/primeâŠ‚Î¦) that exceeds a certain
correlation threshold to train ğœ†ğ‘–. As a result, we get a set of binary
classifiers {ğœ†1,...ğœ†9}astheFEcomponentspredictorwhere ğœ†ğ‘–is
used to predict ğ‘ğ‘–
ğ‘“.
Sub-model to rank MODEL components. Unlike the predic-
tionofFEcomponents,whichoftendependsonafewmeta-features,
it is challenging to determine a few meta-features that can predict
the performance of a particular model on ğ·[49]. Therefore, in-
stead of predicting one model based on a few meta-features, we
design a learning-to-rank sub-model that considers all the meta-features
Î¦to rank all the model components in our corpus. Con-
sidering the size of our meta-training dataset, which is not very
1937
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:52:10 UTC from IEEE Xplore.  Restrictions apply. SapientML: Synthesizing Machine Learning Pipelines by Learning from Human-Written Solutions ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
largeandthefactthattheensemblemodelsarebetterthanasin-
gle model for complex learning task [ 5], we designed an ensemble
ofLogisticRegression andSupportVectorMachine to rank the
model components. More specifically, these meta-models first com-
pute a probability score for each model component and use the
average score to sort the target model components.
4.3.4 TrainingtheSkeletonPredictor(Offline). Wetrainedallthe
sub-modelsintheskeletonpredictorusingthemeta-trainingcorpus.
Since the proportion of pipelines having and not having a ML
component is not equal in the corpus, we used balanced weighting
strategytosolvethe classimbalance problem.Further,wetunedthe
hyper-parameters through5-fold crossvalidation andgrid-search.
4.3.5 Generation of Pipeline Skeletons (Online). During pipeline
generation,SapientMLfirstcomputesthesetofmeta-features, Î¦
fromğ·and passes it to the skeleton predictor ( Î›), which returns a
set of plausible FE components {ğ‘1
ğ‘“...ğ‘ğ‘
ğ‘“}with probability scores
and a ranked-list of the model components ğ¶/primeğ‘š=[ğ‘1ğ‘š,...ğ‘ğ‘Ÿğ‘š].
Inferringrelevantfeatures. Foreachğ‘ğ‘–
ğ‘“inthepredictedset,
SapientML infers the relevant features, ğ‘‹/prime
ğ‘–âŠ‚ğ‘‹inğ·on which ğ‘ğ‘–
ğ‘“
canbesuccessfullyappliedandcreatethesemi-instantiatedsetofFE
components, ğ¶/prime
ğ‘“={/angbracketleftğ‘1
ğ‘“(ğ‘‹1),ğœŒ1/angbracketright,...,/angbracketleftğ‘ğ‘
ğ‘“(ğ‘‹ğ‘),ğœŒğ‘/angbracketright}.Suchinference
is important to help avoid pipeline failures caused by infeasible
transforms,suchas StringVectorizer appliedtoanumericcolumn.
Further,ithelpspreciselyidentifythecolumnsmostsuitableforthe
FEtransform,e.g.,applying SimpleImputer toonlythosecolumns
that have missing values.
To infer relevant features for ğ‘ğ‘–
ğ‘“, first SapientML access the
thesub-model ğœ†ğ‘–,whichisadecision treeclassifierthatpredicted
ğ‘ğ‘–
ğ‘“forğ·. Then SapientML extracts the decision path that led to
the prediction. A decision path is a list of conditions in form of
[ğ›¼1ğ‘œğ‘ ğ‘£1,...ğ›¼ ğ‘¢ğ‘œğ‘ ğ‘£ ğ‘¢]whereğ›¼,ğ‘œğ‘, andğ‘£correspond to a meta-
feature,>=or<, and a real number respectively. Then SapientML
iterates over each feature ğ‘¥ğ‘–âˆˆğ‘‹and selects ğ‘¥ğ‘–only if it satisfies
at least one of the conditions in the decision path. For example,
SapientMLcorrectlyappliesthe OrdinalEncoder oncard4,which
is a string categorical feature whereas it marks the TransactionAmt
feature as irrelevant, which is indeed a numeric feature.
Generation. Finally SapientML selects Top- ğ‘˜models from ğ¶/primeğ‘š
and adds one-by-one to the selected FE components to generate ğ‘˜
number of pipeline skeletons, ğ‘†=[{ğ¶/prime
ğ‘“âˆªğ‘1ğ‘š},...{ğ¶/prime
ğ‘“âˆªğ‘ğ‘˜ğ‘š}].
4.4 Stage 2: Pipeline Instantiation
This stage synthesizes a set of concrete pipelines for the given
user dataset ( ğ·) and its predictive task. Given a ranked-list Sof
pipeline skeletons produced by pipeline seeding (Section 4.3), this
stage instantiates each skeleton ğ‘†into a concrete pipeline ğ‘ƒby
firstcreatinganorderedskeleton ğ‘†ğ‘‚representingasyntactically
viable data flow, and then instantiating the components in ğ‘†ğ‘‚into
apipelinetemplate,alongwithnecessarygluecode.Thisyieldsa
set ofğ‘˜candidate pipelines, Pğ‘ğ‘ğ‘›ğ‘‘={ğ‘ƒ1,ğ‘ƒ2,...,ğ‘ƒ ğ‘˜}.
4.4.1 Create Ordered Skeleton. The goal of this step is to order
thecomponentsof ğ‘†,anddiscard incompatiblecomponents,ifany,to produce an ordered skeleton ğ‘†ğ‘‚, such as the one shown in Fig-
ure1c.Thisoperationusesapipeline dataflowmeta-model extracted
bySapientML,fromthemeta-learningcorpus,duringtheoffline
phase.Wedevelopthedescriptionusingthefollowingterminology.
Definition 4.5 (Dataflow dependence). There exists a dataflow
dependence between components ğ‘ğ‘–andğ‘ğ‘—of a pipeline ğ‘ƒfor a
datasetğ·iff there exists feature ğ‘¥ğ‘–ofğ·on which both ğ‘ğ‘–andğ‘ğ‘—
are applied in ğ‘ƒ.
There exists a dataflow from ğ‘ğ‘–toğ‘ğ‘—inğ‘ƒ, denoted ğ‘ğ‘–ğ‘ƒâ†’ğ‘ğ‘—,i ff
thereisadataflowdependencebetween ğ‘ğ‘–andğ‘ğ‘—andğ‘ğ‘–precedes
ğ‘ğ‘—inğ‘ƒ. Dataflow dependence, as defined above, can be inferred
throughasimplestaticanalysis.Thedetailsareelidedforbrevity.
Although neither sound nor complete, this definition provides a
simple,efficientwaytocapturedataflowinallbutthemostcompli-
cated pipelines.
The dataflow meta-model is a partial-order relation Î”capturing
the dataflow between pipeline components, as observed in the
corpus pipelines. Specifically,
Î”={(ğ‘ğ‘–,ğ‘ğ‘—)âˆˆCÃ—C|âˆƒ ğ‘ƒâˆˆP ğ¿,ğ‘ğ‘–ğ‘ƒâ†’ğ‘ğ‘—and/nexistsğ‘ƒ/primeâˆˆP ğ¿,ğ‘ğ‘—ğ‘ƒ/prime
â†’ğ‘ğ‘–}
The dataflow meta-model is represented as a directed acyclic
graph(DAG), GÎ”,whosenodesarethecomponents Canddirected
edges(ğ‘ğ‘–,ğ‘ğ‘—)âˆˆÎ”.
Askeleton ğ‘†producedbypipelineseedingistransformedinto
anorderedskeleton ğ‘†ğ‘‚usingthefollowingsteps.First,alldataflow
dependenciesarecapturedbetweenpotentialskeletoncomponents,
usingDefinition4.5.Ifthereisanycomponentpair ğ‘ğ‘–,ğ‘ğ‘—thathas
a dataflowdependence butno edgebetween ğ‘ğ‘–,ğ‘ğ‘—inGÎ”, thisindi-
cates anincompatiblecomponent. Hencethe componentwith the
lowerpredictedprobabilityisdiscarded.Inourmotivatingexample
(Figure1),components OneHotEncoder andOrdinalEncoder ,which
happen to be semantic substitutes of each other (e.g., convert cate-
goricalcolumns tonumeric) presentsuch aninstance.Hence, the
lower probabilitycomponent OneHotEncoder is discarded.Discard-
ing all such components yields a reduced skeleton ğ‘†/prime.
Next, a sub-graph of the dataflow meta-model GÎ”with only
the nodes in the reduced skeleton ğ‘†/primeis extracted. Finally, a topo-
logicalsortonthis sub-graphprovidesacomponentorderforthe
reduced skeleton ğ‘†/primeconsistent with GÎ”. This order is to create
the ordered skeleton ğ‘†ğ‘‚. For our motivating example, Imputerpre-
cedesOrdinalEncoder inğ‘†ğ‘‚.Reversingtheorderforacolumnwith
missing values would result in a pipeline crash.
4.4.2 Generate concrete pipeline. Each ordered skeleton ğ‘†ğ‘‚is con-
verted into a concrete pipeline ğ‘ƒby instantiating each component
ğ‘âˆˆğ‘†ğ‘‚in order, into a pipeline template of the kind shown in
Figure 2. Specifically, each ğ‘âˆˆC ğ‘“âˆªğ‘ğ‘šis instantiated using a
parameterized snippet drawn from a small pre-compiled library
ofstandardcomponentimplementations,byappropriatelyfilling
the parameter holes. For example, OrdinalEncoder is instantiated
by filling the columns hole with relevant columns (â€˜ProductCDâ€™,
â€˜card4â€™,...).SapientMLcanalsohandletype-basedinstantiationof
components. For example, SimpleImputer is instantiated differently
for filling missing values in numeric vs. string columns.
1938
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:52:10 UTC from IEEE Xplore.  Restrictions apply. ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA Saha et al.
4.5 Stage 3: Pipeline Validation
Each candidate pipeline ğ‘ƒâˆˆP ğ‘ğ‘ğ‘›ğ‘‘is dynamically evaluated to
computeanaccuracyscore (F1/ ğ‘…2),tofindthebest pipeline, ğ‘ƒbest.
SapientML internally splits the user-provided training data ğ·ğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘›
intointernaltraining and validation sets which are used for the
training and validation of candidate pipelines within this stage.
Therefore, the held out testdataset,ğ·ğ‘¡ğ‘’ğ‘ ğ‘¡(shown as â€œtest.csv" in
Figure 2) is completely unseen to SapientML. Finally, ğ‘ƒbestis used
to train on ğ·ğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘›and evaluated on ğ·ğ‘¡ğ‘’ğ‘ ğ‘¡for the accuracy score
returned to the user.
5 EVALUATION
Our evaluation addresses the following research questions:
RQ1:How does SapientML perform compared to the existing
state-of-the-art techniques?
RQ2:How robust SapientML is in producing good quality
pipelines across trials?
RQ3:DoesSapientMLuseitssearchspacewelltopredictadiverse
set of FE and model components?
RQ4:Does each of the novel technology components of Sapi-
entML contribute to its effectiveness?
5.1 Experimental Set-up
5.1.1 Implementation. SapientMLisimplementedinthePython
programming language using approximately 5,000 lines of code.
It includes a crawler to download ML projects, a set of tools for
requiredstaticanddynamicanalysissuchasminingtheorderof
ML components from a corpus, denoising pipelines, a meta-feature
extractor, and machine learning models for the skeleton predictor.
SapientMLusesKagglePublicAPIs[ 22]forautomaticallydown-
loading data, the Python-PLlibrary [ 9] to instrument sourcecode
for dynamic program slicing, the scikit-learn [ 31,34] library to
implementmeta-modelsfortheskeletonpredictor,andLibCST[ 20]
for static analysis. SapientML uses Pandas, Numpy, and Scipy for
computing meta-features and its own data analysis.
5.1.2 Benchmarks. Weuseasetof41benchmarkdatasetstoevalu-
ate SapientML. This includes the set of 31 datasets used in AL[ 10].
They include 12 datasets from the OpenML suite, 9 from PMLB,
4 from Mulan, and 6 from Kaggle. However, since most of these
datasets are small and simple in nature, we have added 10 new
datasetsfromKaggleasrepresentativesoflarge,real-worlddatasets
whichmodernAutoMLtoolsshouldhandle.Tosystematicallyse-
lectthe10newbenchmarkdatasetswecollectedallthe Featured
andPlayground Kagglecompetitionscompletedsincetheyear2015.
From these we selected ones operating only on tabular data and
wherethelicensepermitsacademicresearchanduseoutsidethe
competition. Finally, we selected 10 datasets based on size either in
termsoflargenumberofrowsorcolumns,orhavevarioustypes
ofcolumns.Table3presentsthesize,predictiontask,andsource
repository for each benchmark dataset.
5.1.3 Experimental Methodology. SapientML is trained on our
meta-learning corpus of 1,094 pipelines and corresponding cleaned
datasets. Therefore, the 41 benchmark datasets are completely un-
seento SapientML. Similar to AL [ 10] and auto-sklearn [ 15], we
performed10trialsofeachexperimentforeachbenchmarkwitha one hour time out. For each trial, we randomly split the user-
provideddatasetinto trainingandtestingdataina75:25split.Then
SapientMLgeneratedapipelineusing onlytheuser-provided train-
ingdataandthenreporteditsaccuracyonthe user-provided testing
data.Allthebaselinesandexistingtoolswererunusingthesame
train-test split of data in each trial to ensure a fair comparison. We
usestandard macroF1scoresand ğ‘…2scoresforclassificationand
regressiontasksrespectively,andusedthemeanscoreof10runsto
comparetheresults,followingexistingliterature[ 10,15].Weran
all tools on 4 vCPUs of Xeon E5-2697A v4 (2.60GHz) with 16GB
memory for OpenML, PMLB, and Mulan datasets and on 8 vCPUs
with 32GB memory for Kaggle datasets.
5.2 RQ1: SapientML versus state of the art
We compared the performance of SapientML to three state-of-the-
art AutoML systems: auto-sklearn [ 15] (ver. 0.12.2), TPOT [ 30]( v e r .
0.11.7), and AL [ 10], from its public distribution [ 8] using the same
configurationsasin[ 10].auto-sklearnisanactivelymanagedopen-
sourceprojectonGithubwithmorethan6Kstars.ALrepresents
the most recent AutoML technique that also learns from human-
written pipelines to generate supervised pipelines. In addition, we
implementedtwobaselinetoolsBasic-MLandDefault,represent-
ingbasicMLtechniques,followingthemethodologydescribedin
[10]. Specifically, Basic-ML applies SimpleImputer to fill numeric
and string missing values with 0 and empty string respectively,
CountVectorizer to transform all string columns to token counts,
and thenuses the LogisticRegression andLinearRegression mod-
elsforclassificationandregressiontasksrespectively.Defaultal-
ways predicts the most frequent label for classification tasks or the
mean value for regression tasks.
5.2.1 Quantitative Comparison. Table 3 presents the evaluation
results in terms of average macro F1 and ğ‘…2scores over 10 runs
for classification and regression tasks respectively. Highest scores
for each benchmark are marked as bold. We call them as champion.
Furthermore,weperformedapair-wiseWilcoxon-signed-rankTest
(ğ›¼=0.05)toseewhetherthescoredifferencebetweenthechampion
and another tool for a benchmark is statistically significant across
10 trials. The underlined numbers represent the scores that are
statistically similar to the champion. We call them as winners.
We start by comparing the two baseline tools: Basic-ML and De-
fault to all other tools. As expected, Defaultâ€™s simplistic prediction
performedtheworst.Interestingly,Basic-MListhechampionon
4datasets,sincesomeofthedatasetsaresimpleanddonotneed
any sophisticated pipelines. However, Basic-MLâ€™s overall perfor-
mance is poor compared to any other AutoML tools, in terms of
meanF1/R2scores.Therefore,thisresultsupportsthe nofreelunch
hypothesis [47] that no single pipeline is good for every dataset.
Comparing the performance of SapientML to other AutoML
tools, Table 3 shows that SapientML outperforms the state-of-the-
artAutoML toolsintermsof successfulpipelinegeneration,num-
ber of champions, and winners. SapientML generated a successful
pipelineforeachbenchmarkandtrial,whereasAL,auto-sklearn,
andTPOTfailedon9, 17,and12datasetsrespectively.There are
several reasons for failures including not being able to handle vari-
ous types of data, unexpected exceptions, applying FE components
on inappropriate columns, or timeout.
1939
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:52:10 UTC from IEEE Xplore.  Restrictions apply. SapientML: Synthesizing Machine Learning Pipelines by Learning from Human-Written Solutions ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
Table 3: Effectiveness of SapientML compared to the state-of-the-art AutoML tools on the benchmark datasets. Bold numbers
indicatethebestscores;UnderlinednumbersarenotstatisticallydifferentfromthebestscoresaccordingtoaWilcoxon-signed-
rank Test ( ğ›¼=0.05).Failedmeans the tool has failed on all the 10 trials whereas F[n]means the tool has failed on ğ‘›trials.
Dataset SapientML AL auto-sk. TPOT Basic ML Default Base. 1 Base. 2 Metric Source Rows Cols
1049 0.78 0.73 0.79 0.78 0.63 0.47 0.78 0.64 F1 OpenML 1458 37
1120 0.87 0.83 0.87 0.86 0.75 0.39 F[5] 0.75 F1 OpenML 19020 10
1128 0.95 0.94 0.97 0.96 0.94 0.44 F[5] 0.94 F1 OpenML 1545 10935
179* 0.79 0.77 Failed Failed 0.43 0.43 F[7] 0.79 F1 OpenML 48842 14
184 0.77 0.55 Failed Failed Failed 0.02 0.67 0.37 F1 OpenML 28056 6
293* 0.96 0.78 0.91 0.75 0.75 0.34 0.96 0.75 F1 OpenML 581012 54
38 0.95 0.94 Failed Failed 0.57 0.48 0.87 0.83 F1 OpenML 3772 29
389 0.83 0.63 0.83 0.79 0.81 0.02 0.83 0.81 F1 OpenML 2463 2000
46 0.95 0.95 Failed Failed Failed 0.23 0.96 0.94 F1 OpenML 3190 60
554 0.98 0.91 0.98 0.83 0.92 0.02 0.98 0.92 F1 OpenML 70000 784
772 0.51 0.51 0.51 0.51 0.41 0.35 0.51 0.49 F1 OpenML 2178 3
917 0.90 0.90 0.900.90 0.65 0.35 0.90 0.65 F1 OpenML 1000 25
Hill_Valley_with_noise 0.95 0.73 1.00 0.99 0.95 0.33 0.95 0.95 F1 PMLB 1212 100
Hill_Valley_without_noise 0.99 0.69 1.00 1.00 1.00 0.32 1.00 1.00 F1 PMLB 1212 100
breast_cancer_wisconsin 0.97 0.95 0.97 0.96 0.93 0.38 0.97 0.93 F1 PMLB 569 30
car_evaluation 0.95 0.97 0.98 0.99 0.76 0.21 0.95 0.79 F1 PMLB 1728 21
glass 0.74 0.67 0.63 0.70 0.48 0.10 0.71 0.45 F1 PMLB 205 9
ionosphere 0.94 0.91 0.94 0.94 0.84 0.39 0.94 0.86 F1 PMLB 351 34
spambase 0.96 0.94 0.95 0.95 0.92 0.38 0.96 0.94 F1 PMLB 4601 57
wine_quality_red 0.35 0.33 0.33 0.34 0.22 0.10 0.34 0.29 F1 PMLB 1599 11
wine_quality_white 0.44 0.42 0.41 0.43 0.16 0.10 0.44 0.34 F1 PMLB 4898 11
detecting-insults-in-social-comm... 0.71 0.76 Failed Failed 0.77 0.42 Failed 0.42 F1 Kaggle 3947 2
housing-prices 0.89 0.85 Failed Failed 0.80 -0.00 Failed F[6] R2 Kaggle 1460 80
mercedes-benz 0.520.53 Failed Failed -2.0E+23 -0.00 -0.80 Failed R2 Kaggle 4209 377
sentiment-analysis-on-movie-rev...* 0.49 0.39 Failed Failed F[7] 0.13 Failed 0.02 F1 Kaggle 156060 3
spooky-author-identification 0.78 0.80 Failed Failed 0.81 0.19 0.78 0.19 F1 Kaggle 19579 2
titanic 0.79 0.71 Failed Failed 0.81 0.38 Failed 0.79 F1 Kaggle 891 11
enb 0.98 0.98 0.99Failed 0.89 -0.01 0.98 0.96 R2 Mulan 768 8
jura 0.600.76 0.48 Failed 0.52 -0.01 0.59 0.60 R2 Mulan 359 15
sf1 -0.09 F[4] Failed Failed Failed -0.01 -0.10 -0.05 R2 Mulan 323 10
sf2 0.05 F[3] Failed Failed Failed -0.00 -1.0E+23 -4.6E+22 R2 Mulan 1066 10
costa-rica* 0.91 0.87 Failed Failed 0.21 0.19 0.92 0.55 F1 Kaggle 9557 142
Categorical-Feature-Enc...-Chal...-II* 0.58 0.54 Failed Failed Failed 0.45 Failed Failed F1 Kaggle 600000 24
Porto-Seguros-Safe-Driver-Pred... 0.49 F[2] 0.52 0.52 0.49 0.49 0.49 0.49 F1 Kaggle 595212 58
Kobe-Bryant-Shot-Selection* 0.64Failed Failed Failed 0.36 0.36 Failed 0.62 F1 Kaggle 30697 24
whats-cooking 0.71Failed Failed Failed 0.71 0.02 0.71 0.02 F1 Kaggle 39774 2
PUBG-Finish-Placement-Prediction* 0.93Failed -0.00 0.86 0.83 -0.00 0.93 0.83 R2 Kaggle 4446966 28
Santander-Value-Prediction-Chal... 0.12 Failed 0.26 0.27-7.2E+17 -0.00 0.12 Failed R2 Kaggle 4459 4993
IEEE-CIS-Fraud-Detection* 0.82Failed Failed Failed 0.49 0.49 Failed Failed F1 Kaggle 590540 394
Quora-Insincere-Questions-Class...* 0.75 0.63 Failed Failed Failed 0.48 Failed 0.06 F1 Kaggle 1306122 3
DonorsChoose.org-App...-Screening* 0.50 F[5] Failed Failed Failed 0.46 Failed Failed F1 Kaggle 182080 16
#champions 19 2 9 3 4 1 3 0
#winners 27 9 17 12 5 1 14 3
#failures 0 9 19 21 8 0 12 6
Intermsofperformancescore,SapientMLischampionfor18
subjects, whereas the second best tool, AL, based on the number
of successful pipelines, is champion for only 2 datasets. On theother hand, although auto-sklearn failed on highest number of
datasets,itischampionfor9datasets.Theseresultsindicatethat
auto-sklearnperformswellinalimitedscope.However,although
AL has a broader scope, it has overall performed moderately. Inter-
estingly, SapientML outperforms them both in terms of scope and
performance. The same findings also hold in terms of number of
winners. SapientML performed the best or comparable to the best
for 27 datasets, which is the highest among all tools.
For the 10 more difficult datasets (marked with a * in Table 3)
â€“ the largest ( ğ‘Ÿğ‘œğ‘¤Ã—ğ‘ğ‘œğ‘™ğ‘¢ğ‘šğ‘›ğ‘ ) datasets requiring at least one FE
component â€“ SapientML performs even better relative to other
tools.SapientMLproducesbestorcomparableperformanceon9
of them, with AL failing to produce a pipeline on 4 of them and
TPOT, auto-sklearn on most of them. These results illustrate the
value of SapientMLâ€™s divide-and-conquer synthesis to produce
viable pipelines especially for large, complex datasets.5.2.2 Qualitative Analysis. We analyze the results qualitatively us-
ing a few concrete examples. Benchmark OpenML-293 presents an
interesting case where every tool produced a pipeline since thedataset contains only numeric values. AL predicted and selected
XGBoostClassifier through dynamic evaluation, which achieved
a macro mean F1 score of 0.78. ALâ€™s prediction may suffer sinceit uses language model which depends on the previous two com-ponents for prediction. However, there is no need for FE compo-nents for this dataset. auto-sklearn selected a pipeline based on
datasetsimilaritythatperformsstandardscalingfirstandthenuses
GradientBoostingClassifier . It achieved an F1 score of 0.91, better
than AL. However, SapientML predicted an even better model:
RandomForestClassifier , which achieved the best F1 score: 0.96.
For the sentiment-analysis-on-movie-reviews dataset, auto-
sklearnsimplyfailedsinceitcannothandletextualdata.Incontrast,
AL and SapientML both successfully generated pipelines by using
aTextVectorizer componenttoconverttexttonumericcolumns.
However, AL selected the LinearSVC model that resulted in an F1
scoreof0.39.Ontheotherhand,SapientMLselectedanadditional
1940
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:52:10 UTC from IEEE Xplore.  Restrictions apply. ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA Saha et al.
(SD: Standard Deviation, ET: Execution Time)
Figure 4: Robustness of SapientML0 0.05 0.1 0.15 0.2 0.25 0.3 0.35OrdinalEncoderOneHotEncoderImputerLogScalarDataBalancerStringVectorizerTextPreprocessingLinearScalarDateFeaturization
Figure 5: Distribution of FE predictionsExtraTreesRegressorCatBoostRegressorLinearRegressionGradientBoostingRegressorLGBMRegressorRandomForestRegressor
0 0.05 0.1 0.15 0.2 0.25		


Figure 6: Distribution of model predictions
text preprocessing components that performs some basic clean-
ing and normalization of text. Further, it selected a better model
CatBoost that provided an F1 score of 0.49.
Finally,the juradatasetpresentsanegativeexampleforSapi-
entML where AL achieved a significantly better score. On investi-
gating the reason, we found that AL selected a model called Ridge,
whichisnotusedinourprojectcorpus.EvenforAL,thisparticular
model was not highly ranked by its meta-model. However, since
AL set a beam_size of 30 for this dataset, i.e., it evaluated 30 dif-
ferent models to select the best model, AL was successful in this
case. AL could afford to perform such extensive evaluation for this
dataset simply because the dataset is small and hence the dynamic
evaluation is fast. However, for this extensive dynamic evaluation,
ALfailedtoproduceanypipelineforlargedatasetssuchas IEEE
andDonorsdue to timeout. In contrast, SapientML uses only top
3 models based on its meta-model and performs overall the best.
5.3 RQ2: Robustness of SapientML
Weanalyzetherobustnessof SapientMLingeneratingpipelines
in terms of variation of performance scores and execution timeacross 10 trials. To investigate how much SapientML fluctuates
across trials, we calculated the standard deviation of performance
scores and execution time across 10 trials for each benchmark
dataset.Figure4presentsthedistributionofthestandarddeviations
across10trialsfor41benchmarkdatasets.Theresultsshowthat
SapientMLisoverallverystableacrossthe10trialsintermsofboth
accuracyand execution time.Boththe50th percentile(mean)and
75th percentile standard deviation for macro F1/R2 scores across
allthebenchmarkareonly0.02,whichismorestable thanthatof
AL,whichare0.03and0.04respectively.Thesameistrueforthe
executiontime.The50thpercentileand75thpercentilestandard
deviationofexecutiontimeareonly3and9secondsrespectively
for SapientML, which are 9 and 41 seconds respectively for AL.
Finally,weinvestigatewhetherthegeneratedpipelinesareover-
fitted to the corresponding training data. To prevent overfitting,
we already made sure that SapientML generates pipeline only us-
ing75%data,andthegeneratedpipelineistestedon25% held-out
test data. However, in this RQ, we investigate even further. Gen-erally overfitting happens when a model performs very good onthe validation data but performs poorly on the test data [
39]. To
this end, we compute the internal validation score based on whichSapientMLselectedthebestpipeline.Thenwecomparethevali-
dation accuracy with held-out test accuracy. As the fifth boxplot
in Figure 4 shows, the 50th and 75th percentile difference between
test and validation accuracy are 0.01 and 0.02 respectively. Andinterestingly, the differences are positive, which means that thefinal test scores are better than the validation scores for most of
the subjects. We could not compare this result with any other tools
since we do not have access to their validation scores.
5.4 RQ3: Diversity in Meta-Prediction
Figure 5presentsthe distributionof predictedML components in
pipelines skeletons for all benchmarks and trials. The results show
thattheskeletonpredictorwasabletopredictallthe9FEcompo-
nentssuccessfully.Amongthesecomponents,accurateprediction
ofImputer,OrdinalEncoder orOneHotEncoder ,TextVectorizer ,and
DateFeaturization isimportantsinceanyfalsenegativepredictions
may lead to a crash during pipeline execution. Since SapientML is
abletoproduceasuccessfulpipelineforeachtrialineachbench-
mark dataset(Table 3),it isevident thatthe skeletonpredictor ac-
curately predicted these components. Similarly, as Figure 6 shows,
the skeleton predictor is able to predict a wide range of model
components.Morespecifically,itpredicted11differentclassifica-
tion and 6 regression models for 33 classification and 8 regression
tasks respectively. As expected, some models such as CatBoost and
RandomForest are dominant since they are fundamentally better.
However,traditionalmodelssuchas LogisticRegression orSVCare
alsopredicteddependingonthedataset.Theoverallresultssuggest
that the predictions were effective for most of the datasets.
5.5 RQ4: Impact of novel components
This research question investigates the contribution of Sapi-
entMLâ€™s two main components: i) pipeline seeding and ii) pipeline
instantiation. To this end, we create two baselines:
Baseline1. Thisbaselineusestheskeletonpredictedbypipeline
seeding but instantites each FE component on the entire dataset.
Baseline2. Inthisbaseline,wefurtherrelaxBaseline1byreplac-
ingthe pipelineseeding bya commonskeletonto understandthe
combinedeffectofpipelineseedingandinstantiation.Tocreatethe
default skeletons, we take three most frequently used FE compo-
nentsinourcorpus,oneatatime,withthemostfrequentmodel.
Thus, we try with three skeletons and take the best accuracy.
Table3showstheresultsofBaseline1andBaseline2(columns
8 and 9). From the results, it can be seen that the two baselines
1941
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:52:10 UTC from IEEE Xplore.  Restrictions apply. SapientML: Synthesizing Machine Learning Pipelines by Learning from Human-Written Solutions ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
failon6and12datasets,respectively.Further,theirperformance
is poor due to the use of FE components on all columns in the
dataset.Baseline1achievescomparableperformancetoSapientML
for datasets that are simple and do not require any FE components.
However,theoverallresultsshowthatbothpipelineseedingand
pipeline instantiation are important for SapientML to succeed.
6 LIMITATIONS & THREATS TO VALIDITY
External validity. Our framework has only been instantiated
for ML pipelines in Python and evaluated on our 41 benchmark
datasets.Thusourresultsmaynotholdoutsidethisscope.Wetried
to mitigate this risk by using standard benchmarks from previous
work[10,15,30]augmentedwithlarge,diverse,real-worlddatasets
used on public data science competitions hosted on Kaggle.
Qualityofdata. Beingadata-driventechniqueSapientMLâ€™s
performanceisinherentlylimitedbythequalityofitstrainingdata.
ItisawellknownproblemthatmostnotebooksavailableonKaggle
orGitHubcannotbelocallyre-executed[ 45,46].Thus,wecould
also mine only a fraction of the data (i.e., pipelines) potentially
available on Kaggle. Further, the notebooks we did obtain vary sig-
nificantlyinqualityandtheiruseofspecificlibrariesversuscustom
code. These differences manifest as noise in our analysis. We tried
tomitigatetheseissuesbydevelopingsimplebuteffectivecorpus
augmentation (Section 4.2.2), pipeline denoising (Section 4.2.1) and
by using semantic components classes (Section 4.3)to canonical-ize pipelines. However, using a larger, cleaner data corpus could
significantly strengthen our results.
Simple skeleton predictor model. Currently, our skeleton
predictor uses a rather simple model that prioritizes features of
thedatasetandignorescorrelationsbetween(predicted)pipeline
components. This approximation allows the model to perform well
withlimiteddata,asitdidonourbenchmarks.However,generating
muchmoredeeperorsophisticatedpipelinesmightnecessitatea
moreexpressivemodeltrainedonsubstantiallylarger,cleanerdata.
Manual definition of the pipeline space. Currently, we use a
manual methodology to define the synthesis space of SapientML,
includingcreatingtheclustersofAPIsconstitutingthesemanticFEclasses(Section4.3).Wenotethatthisisconsistentwiththepractice
of previous AutoML techniques [ 10,15,30]. However, we follow a
transparentandsystematicprocess(Section4.3),sothatSapientML
can be easily generalized to other ML components once viablepipeline data demonstrating their use is available. However, this
wouldstill belimitedtoAPI-based MLcomponents.The problem
ofminingandre-usingarbitrary,customMLtransformsinpipeline
synthesis remains a very interesting, open problem.
Hyper-parameteroptimization(HPO). SapientMLfocuses
on ML component selection and end-to-end pipeline instantiation.
HPO is currently out of its scope. However, standard Bayesian
optimization HPO [2] could be added as a post-processing step.
7 RELATED WORK
AutoMLfortabulardata. PreviousAutoMLtechniquesusediffer-
enttechniquestoexplorethehugecombinatorialsearchspaceof
potential candidate pipelines. TPOT [ 30] uses evolutionary search
while ReinBo [ 40] uses Reinforcement Learning combined with
Bayesian Optimization [ 18]. Auto-WEKA [ 43], and later Auto-
Sklearn [ 13,15], employ meta-learning on a corpus of syntheticoptimized pipelines to selectthe most appropriate pipeline and
then tune hyper-parameters using Bayesian Optimization. Ten-sorOBOE [
49,50] builds on this approach using low rank ten-
sor decomposition as a surrogate model for efficient pipeline
search. AL [ 10] uses language models learned from human-written
pipelines,incombinationwithaggressivedynamicevaluationofpartial pipelines, to explore the pipeline space. AMS [
7] mines
constraintsfromcorporaofhuman-writtenpipelinestohelpwarm-
startsearch-basedAutoMLlikeTPOT.SapientMLsharesALand
AMSâ€™s goal of learning from human-written pipelines. However,
unlike all of the above approaches, which essentially reason oncomplete pipelines, SapientML combats AutoML combinatorial
statespaceexplosionthroughanoveldivide-and-conquerapproach
of first reasoning on individual ML components and subsequently
assembling a small pool of candidate pipelines for final analysis.
AutoMLforDLmodels. Thisareaisreviewedextensivelyin
[16,51]. This research focuses on synthesizing the neural network
models themselves, through neural architecture search (NAS) [4,19,
52],oronhyper-parameteroptimization(HPO)[ 14,19].Bycontrast,
SapientML addresses ML component selection and end-to-end
pipeline instantiation, treating ML components as black-boxes.
Program synthesis for data wrangling. These techniques
typically use input-output examples of data-frames as an input
specification to synthesize programs implementing data wrangling
operations(datapre-processing,cleaning,transformation)forthe
given dataset. They prune or navigate the synthesis program space
by manually specified API constraints coupled with constraint-
solving [12], automatically learning lemmas during synthesis [ 11],
or using more general neural-network-backed program genera-
tors [6]. However, the PbE paradigm common to these techniques
is not applicable to ML pipeline synthesis.
ML-based program synthesis. One class of approaches, such
as [25,33], use probabilistic models trained on programs extracted
from large open repositories (e.g., Github and StackOverflow) to
rank the space of candidate programs generated by the synthesizer.
Anotherbodyofwork[ 29,37,38,41]leveragesuser-providedinput-
outputexamples,ornaturallanguagedescription,tocreateasearch
spaceforneuralprogramsynthesis,typicallyforsimpledomains
suchasstring-manipulatingprograms.Bycontrast,oursynthesis
techniqueisspecificallyengineeredtouseagivendatasetandits
predictive task as the (only) specification for synthesis.
8 CONCLUSIONS
In this work we proposed a learning-based AutoML technique
SapientML,togeneratesupervisedMLpipelinesfortabulardata.
SapientML combats the huge combinatorial search space of Au-toML through a novel divide-and-conquer three-stage programsynthesis approach that reasons on successively smaller searchspaces. We have instantiated SapientML as part of a fully auto-mated tool-chain that creates a cleaned, labeled learning corpusby mining Kaggle, learns from it, and uses the learned models tothensynthesizepipelinesfornewpredictivetasks.Weevaluated
SapientMLonasetof41 benchmarkdatasetsandagainst3state-
of-the-art AutoML tools and 4 baselines. Our evaluation showed
thatSapientMLproducedthebestorcomparableaccuracyin27ofthebenchmarkswhilethesecondbesttoolfailedtoevenproducea
pipeline on 9 of the instances.
1942
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:52:10 UTC from IEEE Xplore.  Restrictions apply. ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA Saha et al.
REFERENCES
[1]Hiralal Agrawal and Joseph R Horgan. 1990. Dynamic program slicing. ACM
SIGPlan Notices 25, 6 (1990), 246â€“256.
[2]AutoML.org.2019. SMACv3Project. https://github.com/automl/SMAC3Accessed
in 2021.
[3]JohannesBader,AndrewScott,MichaelPradel,andSatishChandra.2019. Getafix:
LearningtoFixBugsAutomatically. Proc.ACMProgram.Lang. 3,OOPSLA,Article
159 (Oct. 2019), 27 pages. https://doi.org/10.1145/3360585
[4]Bowen Baker, Otkrist Gupta, Nikhil Naik, and Ramesh Raskar. 2017.Designing Neural Network Architectures using Reinforcement Learning.
arXiv:1611.02167 [cs.LG]
[5]SVBaraiandYoramReich.1999. Ensemblemodellingorselectingthebestmodel:
Many could be better than one. AI EDAM 13, 5 (1999), 377â€“386.
[6]RohanBavishi,CarolineLemieux,RoyFox,KoushikSen,andIonStoica.2019.
AutoPandas:Neural-BackedGeneratorsforProgramSynthesis. Proc.ACMPro-
gram. Lang. 3, OOPSLA, Article 168 (Oct. 2019), 27 pages. https://doi.org/10.
1145/3360594
[7]JosÃ©Cambronero,JÃ¼rgenCito,andMartinRinard.2020. AMS:GeneratingAu-
toML search spaces from weak specifications. In Proceedings of the 2020 28th
ACM Joint Meeting on European Software Engineering Conference and Symposium
ontheFoundationsofSoftwareEngineering (Sacramento,California) (ESEC/FSE
2020). Association for Computing Machinery, New York, NY, USA.
[8]JosÃ© P. Cambronero. 2019. Public distribution of AL. https://github.com/
josepablocam/AL-public Accessed: August, 2020.
[9]JosÃ©P.Cambronero.2020. python-pl. https://github.com/josepablocam/python-pl
Accessed: January 2021.
[10]JosÃ© P. Cambronero and Martin C. Rinard. 2019. AL: Autogenerating Supervised
Learning Programs. Proc. ACM Program. Lang. 3, OOPSLA, Article 175 (Oct.
2019), 28 pages. https://doi.org/10.1145/3360601
[11]Yu Feng, Ruben Martins, Osbert Bastani, and Isil Dillig. 2018. Program Synthesis
UsingConflict-DrivenLearning.In Proceedingsofthe39thACMSIGPLANCon-
ference on Programming Language Design and Implementation (Philadelphia, PA,
USA)(PLDI2018).AssociationforComputingMachinery,NewYork,NY,USA,
420â€“435.
[12]Yu Feng, Ruben Martins, Jacob Van Geffen, Isil Dillig, and Swarat Chaudhuri.
2017. Component-BasedSynthesisofTableConsolidationandTransformation
Tasks from Examples. In Proceedings of the 38th ACM SIGPLAN Conference on
ProgrammingLanguageDesignandImplementation (Barcelona,Spain) (PLDI2017).
Association for Computing Machinery, New York, NY, USA, 422â€“436.
[13]MatthiasFeurer,KatharinaEggensperger,StefanFalkner,MariusLindauer,and
Frank Hutter. 2020. Auto-Sklearn 2.0: The Next Generation. arXiv:2007.04074v1
[cs.LG](2020).
[14]MatthiasFeurerandFrankHutter.2018. HyperparameterOptimization. https:
//www.ml4aad.org/wp-content/uploads/2018/09/chapter1-hpo.pdf
[15]Matthias Feurer, Aaron Klein, Katharina Eggensperger, Jost Tobias Springen-berg, Manuel Blum, and Frank Hutter. 2015. Efficient and Robust Automated
MachineLearning.In Proceedingsofthe28th InternationalConferenceonNeural
Information Processing Systems - Volume 2 (Montreal, Canada) (NIPSâ€™15).M I T
Press, Cambridge, MA, USA, 2755â€“2763.
[16]Xin He, Kaiyong Zhao, and Xiaowen Chu. 2019. AutoML: A Survey of the
State-of-the-Art. arXiv:1908.00709 [cs.LG]
[17]Qinheping Hu and Loris Dâ€™Antoni. 2018. Syntax-Guided Synthesis with Quanti-
tativeSyntacticObjectives.In ComputerAidedVerification,HanaChocklerand
Georg Weissenbacher (Eds.). Springer International Publishing, Cham, 386â€“403.
[18]FrankHutter,HolgerH.Hoos,andKevinLeyton-Brown.2011. SequentialModel-
BasedOptimizationforGeneralAlgorithmConfiguration.In Proceedingsofthe
5thInternationalConferenceonLearningandIntelligentOptimization (Rome,Italy)
(LIONâ€™05). Springer-Verlag, Berlin, Heidelberg, 507â€“523.
[19]Frank Hutter, Lars Kotthoff, and Joaquin Vanschoren (Eds.). 2018. Automated
Machine Learning: Methods, Systems, Challenges. Springer. In press, available at
http://automl.org/book.
[20]Instagram.2019. LibCST:AConcreteSyntaxTree(CST)parserandserializerlibrary
for Python. https://github.com/Instagram/LibCST Accessed in 2021.
[21] Kaggle. 2010. Kaggle. https://www.kaggle.com Accessed in 2021.
[22]Kaggle. 2010. Kaggle Public API. https://www.kaggle.com/docs/api Accessed:
January, 2021.
[23]Kaggle. 2010. Meta Kaggle. https://www.kaggle.com/kaggle/meta-kaggle Ac-
cessed: January, 2021.
[24]Udayan Khurana, Horst Samulowitz, and Deepak Turaga. 2018. Feature engi-
neering for predictive modeling using reinforcement learning. In Proceedings of
the AAAI Conference on Artificial Intelligence, Vol. 32.
[25]Woosuk Lee, Kihong Heo, Rajeev Alur, and Mayur Naik. 2018. Accelerating
Search-Based Program Synthesis Using Learned Probabilistic Models. In Proceed-
ings of the 39th ACM SIGPLAN Conference on Programming Language Design and
Implementation (Philadelphia, PA, USA) (PLDI 2018). Association for Computing
Machinery, New York, NY, USA, 436â€“449.[26]LinkedIn.2018. LinkedInWorkforceReport. https://economicgraph.linkedin.com/
resources/linkedin-workforce-report-august-2018
[27]Sifei Luan, Di Yang, Celeste Barnaby, Koushik Sen, and Satish Chandra. 2019.
Aroma: Code Recommendation via Structural Code Search. Proc. ACM Program.
Lang.3, OOPSLA, Article 152 (Oct. 2019), 28 pages.
[28]ThibaudLutellier,HungVietPham,LawrencePang,YitongLi,MoshiWei,and
Lin Tan. 2020. CoCoNuT: Combining Context-Aware Neural Translation Models
Using Ensemble for Program Repair. 101â€“114.
[29]Vijayaraghavan Murali, Letao Qi, Swarat Chaudhuri, and Chris Jermaine. 2018.
NeuralSketchLearningforConditionalProgramGeneration.In 6thInternational
ConferenceonLearningRepresentations,ICLR2018,Vancouver,BC,Canada,April30
-May3,2018,ConferenceTrackProceedings.OpenReview.net. https://openreview.
net/forum?id=HkfXMz-Ab
[30]Randal S. Olson, Nathan Bartley, Ryan J. Urbanowicz, and Jason H. Moore. 2016.
Evaluation of a Tree-based Pipeline Optimization Tool for Automating Data
Science.In ProceedingsoftheGeneticandEvolutionaryComputationConference
2016(Denver, Colorado, USA) (GECCO â€™16). ACM, New York, NY, USA, 485â€“492.
https://doi.org/10.1145/2908812.2908918
[31]F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M.
Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cour-
napeau,M.Brucher,M.Perrot,andE.Duchesnay.2011. Scikit-learn:Machine
Learning in Python. Journal of Machine Learning Research 12 (2011), 2825â€“2830.
[32]QuantHub.2020. TheDataScientistShortagein2020. https://quanthub.com/data-
scientist-shortage-2020/
[33]VeselinRaychev,MartinVechev,andEranYahav.2014. CodeCompletionwith
Statistical Language Models. In Proceedings of the 35th ACM SIGPLAN Confer-
enceonProgrammingLanguageDesignandImplementation (Edinburgh,United
Kingdom) (PLDIâ€™14).AssociationforComputingMachinery,NewYork,NY,USA,
419â€“428.
[34]Scikit-Learn. 2007. scikit-learn: Machine Learning in Python. https://scikit-
learn.org Accessed in 2021.
[35]scikit-learn1.0.2.2022. DecisionTrees. https://scikit-learn.org/stable/modules/
tree.html
[36]Connor Shorten and Taghi M Khoshgoftaar. 2019. A survey on image data
augmentation for deep learning. Journal of Big Data 6, 1 (2019), 1â€“48.
[37]Chengxun Shu and Hongyu Zhang. 2017. Neural Programming by Example.
InProceedingsoftheThirty-FirstAAAIConferenceonArtificialIntelligence (San
Francisco, California, USA) (AAAIâ€™17). AAAI Press, 1539â€“1545.
[38]Rishabh Singh and Pushmeet Kohli. 2017. AP: Artificial Programming. In 2nd
SummitonAdvancesinProgrammingLanguages(SNAPL2017) (2ndsummiton
advances in programming languages (snapl 2017) ed.).
[39]JyothiSubramanianandRichardSimon.2013. Overfittinginpredictionmodelsâ€“is
itaproblemonlyinhighdimensions? Contemporaryclinicaltrials 36,2(2013),
636â€“641.
[40]XudongSun,JialiLin,andBerndBischl.2019. ReinBo:MachineLearningpipeline
search and configuration with Bayesian Optimization embedded Reinforcement
Learning. arxiv preprint, https://arxiv.org/abs/1904.05381 1904.05381 (2019).
[41]Zeyu Sun, Qihao Zhu, Lili Mou, Yingfei Xiong, Ge Li, and Lu Zhang. 2019. A
Grammar-BasedStructuralCNNDecoderforCodeGeneration. Proceedingsof
the AAAI Conference on Artificial Intelligence 33 (07 2019), 7055â€“7062.
[42]Jafar Tanha, Yousef Abdi, Negin Samadi, Nazila Razzaghi, and Mohammad Asad-
pour.2020. Boostingmethodsformulti-classimbalanceddataclassification:an
experimental review. Journal of Big Data 7, 1 (2020), 1â€“47.
[43]ChrisThornton,FrankHutter,HolgerH.Hoos,andKevinLeyton-Brown.2013.
Auto-WEKA: Combined Selection and Hyperparameter Optimization of Clas-
sification Algorithms. In Proceedings of the 19th ACM SIGKDD International
Conference on Knowledge Discovery and Data Mining (Chicago, Illinois, USA)
(KDD â€™13). Association for Computing Machinery, New York, NY, USA, 847â€“855.
[44]Vesta.[n.d.]. IEEE-CISFraudDetection. https://www.kaggle.com/c/ieee-fraud-
detection Accessed: February, 2021.
[45]Jiawei Wang, Li Li, Kui Liu, and Haipeng Cai. 2020. Exploring How Deprecated
Python Library APIs Are (Not) Handled. In Proceedings of the 28th ACM Joint
Meeting on European Software Engineering Conference and Symposium on the
Foundations of Software Engineering (ESEC/FSE 2020). Association for Computing
Machinery, New York, NY, USA, 233â€“244.
[46]JiaweiWang,TzuyangKuo,LiLi,andAndreasZeller.2020. AssessingandRestor-
ingReproducibilityofJupyterNotebooks.In Proceedingsofthe35thIEEE/ACM
International Conference on Automated Software Engineering (Melbourne, Aus-
tralia)(ASE â€™20).
[47]David H Wolpert and William G Macready. 1997. No free lunch theorems for
optimization. IEEE transactions on evolutionary computation 1, 1 (1997), 67â€“82.
[48]XGBoost.2016. XGBoost:eXtremeGradientBoosting. https://github.com/dmlc/
xgboost Accessed in 2021.
[49]Chengrun Yang, Yuji Akimoto, Dae Won Kim, and Madeleine Udell. 2019. OBOE:
CollaborativeFilteringforAutoMLModelSelection.In Proceedingsofthe25th
ACM SIGKDD International Conference on Knowledge Discovery & Data Min-ing(Anchorage, AK, USA) (KDD â€™19). Association for Computing Machinery,
1173â€“1183.
1943
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:52:10 UTC from IEEE Xplore.  Restrictions apply. SapientML: Synthesizing Machine Learning Pipelines by Learning from Human-Written Solutions ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
[50]Chengrun Yang, Jicong Fan, Ziyang Wu, and Madeleine Udell. 2020. AutoML
PipelineSelection:EfficientlyNavigatingtheCombinatorialSpace.In Proceedings
ofthe26thACMSIGKDDInternationalConferenceonKnowledgeDiscovery&Data
Mining (KDD â€™20). Association for Computing Machinery, 1446â€“1456.[51]Quanming Yao, Mengshuo Wang, Yuqiang Chen, Wenyuan Dai, Yu-Feng Li,
Wei-Wei Tu, Qiang Yang, and Yang Yu. 2018. Taking Human out of Learning Ap-
plications: A Survey on Automated Machine Learning. arXiv:1810.13306 [cs.AI]
[52]BarretZophandQuocV.Le.2017.NeuralArchitectureSearchwithReinforcement
Learning. arXiv:1611.01578 [cs.LG]
1944
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:52:10 UTC from IEEE Xplore.  Restrictions apply. 