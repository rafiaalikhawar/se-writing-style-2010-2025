arXiv:2409.03220v2  [cs.LG]  11 Oct 2024FairQuant: Certifying and Quantifying Fairness of
Deep Neural Networks
Brian Hyeongseok Kim
University of Southern California
Los Angeles, USAJingbo Wang
Purdue University
West Lafayette, USAChao Wang
University of Southern California
Los Angeles, USA
Abstract —We propose a method for formally certifying and
quantifying individual fairness of deep neural networks (DNN).
Individual fairness guarantees that any two individuals wh o are
identical except for a legally protected attribute (e.g., g ender
or race) receive the same treatment. While there are existin g
techniques that provide such a guarantee, they tend to suffe r from
lack of scalability or accuracy as the size and input dimensi on
of the DNN increase. Our method overcomes this limitation
by applying abstraction to a symbolic interval based analys is
of the DNN followed by iterative reﬁnement guided by the
fairness property. Furthermore, our method lifts the symbo lic
interval based analysis from conventional qualitative certiﬁcation
toquantitative certiﬁcation, by computing the percentage of
individuals whose classiﬁcation outputs are provably fair , instead
of merely deciding if the DNN is fair. We have implemented our
method and evaluated it on deep neural networks trained on fo ur
popular fairness research datasets. The experimental resu lts show
that our method is not only more accurate than state-of-the- art
techniques but also several orders-of-magnitude faster.
I. I NTRODUCTION
The problem of certifying the fairness of machine learning
models is more important than ever due to strong interest in
applying machine learning to automated decision making in
various ﬁelds from banking [1] and healthcare [2] to public
policy [3] and criminal justice [4]. Since the decisions are
socially sensitive, it is important to certify that the mach ine
learning model indeed treats individuals or groups of indiv id-
uals fairly. However, this is challenging when the model is a
deep neural network (DNN) with a large number of hidden
parameters and complex nonlinear activations. The challen ge
is also exacerbated as the network size and input dimension
increase. In this work, we aim to overcome the challenge by
leveraging abstract interpretation techniques to certify fairness
both qualitatively and quantitatively.
Our work focuses on individual fairness which, at a high
level, requires that similar individuals are treated simil arly [5].
Here, similar individuals are those who differ only in some
legally protected input attribute (e.g., gender or race) but agree
in the unprotected attributes, and being treated similarly means
that the DNN generates the same classiﬁcation output.1Let
the DNN be a function f:X→Yfrom input domain X
to output range Y, where an individual x∈Xis an input
and a class label y∈Yis the output. Assume that each input
1This notion can be understood as causal fairness [6] or depen dency
fairness [7], which is a non-probabilistic form of counterf actual fairness [8].Certiﬁcation
Problem
/an}bracketle{tf,xj,X/an}bracketri}ht∃P∈stackS
to certify?Fair, Unfair, and
Undecided rates (%)initial partition
P←X
added to S No
Yes
Certiﬁcation
Subproblem
/an}bracketle{tf,xj,P/an}bracketri}ht
Abstraction
(Forward Analysis)Reﬁnement
(Backward Analysis)Quantiﬁcation
(Rate Computation)
UndecidedFair Unfairnew partitions
Pl,Pu
added to S
Fig. 1. FairQuant for certifying and quantifying fairness of a DNN model f
wherexjis a protected attribute and Xis the input domain.
x=/an}bracketle{tx1,...,x D/an}bracketri}htis aD-dimensional vector, and xj, where
1≤j≤D, is a protected attribute. We say that the DNN
isprovably fair (certiﬁed) for the entire input domain Xif
f(x) =f(x′)holds for any two individuals x∈Xandx′∈X
that differ only in xjbut agree in the unprotected attributes
(∀xiwherei/ne}ationslash=j). Conversely, the DNN is provably unfair
(falsiﬁed) for input domain Xiff(x)/ne}ationslash=f(x′)holds for any
two individuals ( x∈Xandx′∈X) that differ only in the
protected attribute. If the DNN is neither certiﬁed nor fals iﬁed,
it remains undecided .
Given a DNN f, a protected attribute xj, and an input
domainX, aqualitative certiﬁcation procedure aims to de-
termine whether fisfair,unfair , orundecided for allx∈X.
Qualitative analysis is practically important because, if fis
provably fair, the model may be used as is, but iffis provably
unfair, the model should not be used to make decision for any
x∈X. When the result of qualitative analysis is undecided ,
however, there is a need for quantitative analysis, to compute
the degree of fairness. For example, the degree of fairness m ay
be measured by the percentage of individuals in input domain
Xwhose classiﬁcation outputs are provably fair.
Both qualitative certiﬁcation and quantitative certiﬁcation
are hard problems for deep neural networks. While there
are many veriﬁcation tools for deep neural networks, exist-
ing veriﬁers such as ReluVal [9], DeepPoly [10], and α-
β-CROWN [11] focus on certifying perturbation robustness,
which is a fundamentally different property, and cannot cer tifyindividual fairness. To the best of our knowledge, the only
existing technique for certifying individual fairness of a DNN
isFairify [12]. However, since it directly analyzes the behavior
of a DNN in the concrete domain using the SMT solver, the
computational cost is extremely high; as a result, Fairify can
only certify tiny networks. Furthermore, it cannot quantif y
the degree of fairness. Prior works on quantitative analysi s
of fairness focus on either testing [13], [14], [15], which d o
not lead to sound certiﬁcation, or statistical parity [16], [17]
that concerns another type of fairness, group fairness, whi ch
differs signiﬁcantly from individual fairness.
To ﬁll the gap, we propose the ﬁrst scalable method
forcertifying andquantifying individual fairness of a DNN.
Our method, named FairQuant , takes a certiﬁcation problem
(consisting of the DNN f, protected attribute xj, and input
domainX) as input and returns one of the following three
outputs: (1) certiﬁed (fair) for all input x∈X; (2) falsiﬁed
(unfair) for all input x∈X; or (3) undecided , meaning
thatfis neither 100% fair nor 100% unfair . In the third
case, our method also computes the percentage of inputs
inXwhose classiﬁcation outputs are provably fair. More
speciﬁcally, our method provides a lower bound of the certiﬁed
percentage, which can guarantee that the DNN meets a certain
requirement, e.g., the DNN is individually fair for at least 80%
of all inputs in X.
As shown in Fig. 1, FairQuant iterates through three steps:
abstraction (forward analysis), reﬁnement (backward anal y-
sis), and quantiﬁcation (rate computation). Assuming that the
legally protected attribute xjhas two possible values (e.g.,
male and female), forward analysis tries to prove that, for e ach
xin the input partition P(which is the entire input domain
Xinitially), ﬂipping the value of the protected attribute of
xdoes not change the model’s output. This is accomplished
by propagating two symbolic input intervals I(∀x∈Pthat
are male) and I′(∀x′∈Pthat are female) to compute the
two corresponding output intervals that are overapproxima ted.
If the classiﬁcation labels (for all xandx′) are the same,
our method returns certiﬁed (fair) . On the other hand, if the
classiﬁcation labels (for all xandx′) are different, our method
returns falsiﬁed (unfair) . In these two cases, 100% of the inputs
in the partition Pare resolved.
Otherwise, we perform reﬁnement (backward analysis) by
splittingPinto partitions PlandPuand apply forward analy-
sis to each of these new partitions. Since smaller partition s
often lead to smaller approximation errors, reﬁnement has
the potential to increase the number of certiﬁed (or falsiﬁe d)
inputs and decrease in the number of undecided inputs. To
ensure that our method terminates quickly, we propose two
early termination conditions based on the reﬁnement depth of
each partition P⊆X. The reﬁnement depth is the number of
timesXis partitioned to produce P. There are two predeﬁned
thresholds. Once the reﬁnement depth exceeds the higher
threshold, we classify the partition Pasundecided and avoid
splitting it further. But if the reﬁnement depth exceeds the
lower threshold without exceeding the higher threshold, we
use random sampling to try to ﬁnd a concrete example x∈Pthat violates the fairness property. If such a counterexamp le
is found, we classify Pasundecided and avoid splitting it
further. Otherwise, we keep splitting Pinto smaller partitions.
We have evaluated our method on a large number of
deep neural networks trained using four widely-used datase ts
for fairness research: Bank [18] (for predicting marketing ),
German [19] (for predicting credit risk), Adult [20] (for
predicting earning power), and Compas [21] (for predicting
recidivism risk). For comparison, we apply Fairify [12] since
it represents the current state-of-the-art in certifying i ndividual
fairness; we also apply α-β-CROWN since it is currently the
best robustness veriﬁer for deep neural networks. Our resul ts
show that α-β-CROWN is not effective in certifying individual
fairness. As for Fairify , our method FairQuant signiﬁcantly
outperforms Fairify in terms of both accuracy and speed for
all DNN benchmarks. In fact, FairQuant often completes
certiﬁcation in seconds, whereas Fairify often times out after
30 minutes and certiﬁes nothing or only a tiny fraction of the
entire input domain.
To summarize, this paper makes the following contributions :
•We propose the ﬁrst scalable method for certifying and
quantifying individual fairness of DNNs using symbolic
interval based analysis techniques.
1) For forward analysis, we propose techniques for
more accurately deciding if the DNN is fair/unfair
for all inputs in an input partition.
2) For reﬁnement, we propose techniques for more
effectively deciding how to split the input partition.
3) For quantiﬁcation, we propose techniques for efﬁ-
ciently computing the percentages of inputs whose
outputs can be certiﬁed and falsiﬁed.
•We demonstrate the advantages of our method over
the current state-of-the-art on a large number of DNNs
trained using four popular fairness research datasets.
The remainder of this paper is organized as follows. First,
we motivate our work in Section II using examples. Then,
we present the technical background in Section III. Next, we
present the high-level procedure of our method in Section IV ,
followed by detailed algorithms of the abstraction, reﬁnem ent,
and quantiﬁcation subroutines in Sections V, VI and VII. We
present the experimental results in Section VIII, review th e
related work in Section IX, and ﬁnally give our conclusions
in Section X.
II. M OTIVATION
In this section, we use an example to illustrate the limita-
tions of existing methods.
A. The Motivating Example
Fig. 2 (left) shows a DNN for making hiring decisions. It
has three input nodes ( i1,i2andi3), two hidden neurons ( h1
andh2) and one output node ( o). The values of h1andh2
are computed in two steps: ﬁrst, the values of i1,i2andi3
are multiplied by the edge weights before they are added up;
then, the result is fed to an activation function. For instan ce,i1
i2
i3h2h1
oi1∈[x1,x1]
i2∈[0,0]
i1∈[x3,x3]2.0
-0.2
0.5
0.7
1.2
0.40.2
-1.0i1
i2
i3h2h1
oi1∈[x1,x1]
i2∈[1,1]
i1∈[x3,x3]2.0
-0.2
0.5
0.7
1.2
0.40.2
-1.0
Fig. 2. Symbolic interval analysis of an example DNN for maki ng hiring
decisions: the left ﬁgure is for female applicants ( i2∈[0,0]), and the right
ﬁgure is for male applicants (where i2∈[1,1]). Except for the protected
attributei2, the symbolic intervals of the other attributes are the same .
the activation function may be ReLU (z) =max(0,z). The
output of the entire network fis based on whether the value
ofois above 0; that is, positive label is generated if o >0;
otherwise, negative label is generated.
The DNN takes an input vector xwith three attributes: x1
is the interview score of the job applicant, x2is the gender (0
for female and 1 for male), and x3is the number of years of
experience. Furthermore, x2is the protected attribute while x1
andx3are unprotected attributes. In general, the input domain
may be unbounded, e.g., when some attributes are real-value d
variables. However, for illustration purposes, we assume t hat
the input domain is X={x|x1∈ {1,2,3,4,5},x2∈
{0,1},andx3∈ {0,1,2,3,4,5}}, meaning that Xhas a total
of5×2×6 = 60 individuals.
Consider the individual x=/an}bracketle{t5,0,5/an}bracketri}ht, meaning that x1= 5,
x2= 0 andx3= 5. According to the DNN in Fig. 2, the
output is the positive label. After ﬂipping the value of the
protected attribute x2from 0 to 1, we have the individual x′=
/an}bracketle{t5,1,5/an}bracketri}ht, for which the DNN’s output is also the positive label.
Since the DNN’s output is oblivious to the gender attribute,
we say that it is fair for this input x.
Consider another individual x=/an}bracketle{t1,0,5/an}bracketri}htwhose gender-
ﬂipped counterpart is x′=/an}bracketle{t1,1,5/an}bracketri}ht. Since the DNN produces
the negative label as output for both, it is still fair for thi s
inputx.
To summarize, the DNN fmay be fair regardless of whether
a particular x∈Xreceives a positive or negative output; as
long asxreceives the same label as its counterpart x′, the
DNN is considered fair.
In contrast, since the individual x=/an}bracketle{t1,0,3/an}bracketri}htand its
counterpart x′=/an}bracketle{t1,1,3/an}bracketri}htreceive different outputs from f,
wherexgets the positive label but x′gets the negative label,
the DNN is not fair for this input x. Furthermore, this pair
(x,x′)serves as a counterexample.
B. Limitations of Prior Work
One possible solution to the fairness certiﬁcation problem
as deﬁned above would be explicit enumeration of the (x,x′)
pairs. For each x∈X, we may ﬂip its protected attribute
to generate x′and then check if f(x) =f(x′). However,
since the size of the input domain Xmay be extremely largeor inﬁnite, this method would be prohibitively expensive in
practice.
Another possible solution is to leverage existing DNN
robustness veriﬁers, such as ReluVal [9], [22], DeepPoly [1 0],
andα-β-CROWN [11]. However, since robustness and indi-
vidual fairness are fundamentally different properties, a pplying
a robustness veriﬁer would not work well in practice. The
reason is because a robustness veriﬁer takes an individual x
and tries to prove that small perturbation of x(often deﬁned
by||x−x′||< δ, whereδis a small constant) does not change
the output label. However, during fairness certiﬁcation, w e are
not given a concrete individual x; instead, we are supposed to
check for all x∈Xandx′∈X, wherexj/ne}ationslash=x′
j. If we force
a robustness veriﬁer to take a symbolic input I(∀x∈X), it
would try to prove that the DNN produces the same output
label for all inputs in X(implying that the DNN makes the
same decision for all inputs in X).
Recall our example network fin Fig. 2. While our method
can prove that fis fair for an input domain that contains x=
/an}bracketle{t5,0,5/an}bracketri}htandx′=/an}bracketle{t5,1,5/an}bracketri}ht(both receive a positive outcome) as
well asx=/an}bracketle{t1,0,5/an}bracketri}htandx′=/an}bracketle{t1,1,5/an}bracketri}ht(both receive a negative
outcome), this cannot be accomplished by a robustness veriﬁ er
(since it is almost never possible for all individuals in the input
domain to have the same outcome).
The only currently available method for (qualitatively) ce r-
tifying individual fairness of a DNN is Fairify [12], which
relies on the SMT solver and may return one of the following
results: SAT (meaning that there exists a counterexample th at
violates the fairness property), UNSAT (meaning that there
is no counterexample), or UNKNOWN (meaning that the
result remains inconclusive). The main problem of Fairify is
that it works directly in the concrete domain by precisely
encoding the non-linear computations inside the DNN as
logical formulas and solving these formulas using the SMT
solver. Since each call to the SMT solver is NP-complete, the
overall computational cost is high. Although Fairify attempts
to reduce the computational cost by partitioning input doma in
a priori and heuristically pruning logical constraints, it does
not scale as the network size and input dimension increase.
Indeed, our experimental evaluation of Fairify shows that only
tiny networks (with ≤100neurons) can be certiﬁed.
C. Novelty of Our Method
We overcome the aforementioned (accuracy and scalability)
limitations by developing a method that is both scalable and
able to quantify the degree of fairness.
First, FairQuant relies on abstraction to improve efﬁ-
ciency/scalability while maintaining the accuracy of sym-
bolic forward analysis. This increases the chance of quickl y
certifying more input regions as fair or falsifying them as
unfair, and decreases the chance of leaving them as undecide d.
Speciﬁcally, we use symbolic interval analysis, instead of the
SMT solver used by Fairify . The advantage is that symbolic
interval analysis focuses on the behavior of the DNN in
an abstract domain, which is inherently more efﬁcient and
scalable than analysis in the concrete domain.input domain X
partition{x|x1∈{1,2,3}}
partition{x|x1∈{1,2}}
. . . . . .partition{x|x1∈{3}}
fair (6
30= 20% )partition{x|x1∈{4,5}}
fair (12
30= 40% )
Fig. 3. Iterative reﬁnement tree for the example DNN in Fig. 2 , to increase
the chance of certifying or falsifying the DNN within an inpu t partition.
Second, FairQuant relies on iterative reﬁnement (partition-
ing of the input domain) to improve the accuracy of forward
analysis. Instead of creating input partitions a priori , it con-
ducts iterative reﬁnement on a “need-to” basis guided by the
fairness property to be certiﬁed. This makes it more effecti ve
than the static partitioning technique of Fairify , which divides
the input domain into a ﬁxed number of equal chunks even
before verifying any of them.
To see why iterative reﬁnement can improve accuracy,
consider our running example in Fig. 2. Initially, forward
analysis is applied to the DNN in the entire input domain
X, for which the certiﬁcation result is undecided . During
reﬁnement, our method would choose x1(overx3) to split,
based on its impact on the network’s output. After splitting
x1∈ {1,2,3,4,5}intox1∈ {1,2,3}andx1∈ {4,5}, we
apply forward analysis to each of these two new partitions.
As shown in Fig. 3, while the partition for x1∈ {1,2,3}
remains undecided , the partition for x1∈ {4,5}is certiﬁed as
fair. This partition has 12 pairs of x∈Xandx′∈X, where
x2/ne}ationslash=x′
2. Therefore, from the input domain Xwhich has 30
(x,x′)pairs, we certify 12/30 = 40% asfair. Next, we split
theundecided partition x1∈ {1,2,3}intox1∈ {1,2}and
x1∈ {3}and apply forward analysis to each of these two new
partitions. While the ﬁrst new partition remains undecided ,
the second one is certiﬁed as fair. Since this partition has six
(x,x′) pairs, it represents 6/30 = 20% of the input domain.
This iterative reﬁnement process continues until one of
the following two termination conditions is satisﬁed: eith er
there is no more partition to apply forward analysis to, or a
predetermined time limit (e.g., 30 minutes) is reached.
III. P RELIMINARIES
In this section, we review the fairness deﬁnitions as well as
the basics of neural network veriﬁcation.
A. Fairness Deﬁnitions
Letf:X→Ybe a classiﬁer, where Xis the input domain
andYis the output range. Each input x∈Xis a vector in
theD-dimensional attribute space, denoted x=/an}bracketle{tx1,...,x D/an}bracketri}ht,
where1,...,D are vector indices. Each output y∈Yis
a class label. Some attributes are legally protected attrib utes
(e.g., gender and race) while others are unprotected attrib utes.
LetPbe the set of vector indices corresponding to protected
attributes in x. We say that xjis a protected attribute if and
only ifj∈ P.Deﬁnition 1 (Individual Fairness for a Given Input): Given
a classiﬁer f, an input x∈X, and a protected attribute j∈ P,
we say that fis individually fair for xif and only if f(x) =
f(x′)for anyx′∈Xthat differs from xonly in the protected
attributexj.
This notion of fairness is local in the sense that it requires
the classiﬁer to treat an individual xin a manner that is
oblivious to its protected attribute xjofx.
Deﬁnition 2 (Individual Fairness for the Input Domain):
Given a classiﬁer f, an input domain X, and a protected
attribute j∈ P , we say that fis individually fair for the
input domain Xif and only if, for all x∈X,f(x) =f(x′)
holds for any x′∈Xthat differs from xonly in the protected
attributexj.
This notion of fairness is global since it requires the
classiﬁer to treat all x∈Xin a manner that is oblivious
to the protected attribute xjofx. The method of explicit enu-
meration would be prohibitively expensive since the number
of individuals in Xmay be astronomically large or inﬁnite.
B. Connecting Robustness to Fairness
Perturbation robustness, which is the most frequently
checked property by existing DNN veriﬁers, is closely relat ed
to the notion of adversarial examples. The idea is that, if
the DNN’s classiﬁcation output were robust, then applying a
small perturbation to a given input xshould not change the
classiﬁer’s output for x.
Deﬁnition 3 (Robustness): Given a classiﬁer f, an input
x∈X, and a small constant δ, we say that fis robust against
δ-perturbation if and only if f(x) =f(x′)holds for all x′∈X
such that ||x−x′|| ≤δ.
By deﬁnition, perturbation robustness is a local property
deﬁned for a particular input x, where the set of inputs deﬁned
by||x−x′|| ≤δis not supposed to be large. While in theory, a
robustness veriﬁer may be forced to check individual fairne ss
by setting δto a large value (e.g., to include the entire input
domainX), it almost never works in practice. The reason
is because, by deﬁnition, such a global robustness property
requires that all inputs to have the same classiﬁcation outp ut
returned by the DNN – such a classiﬁer fwould be practically
useless.
This observation has been conﬁrmed by our experiments
usingα-β-CROWN [11], a state-of-the-art DNN robustness
veriﬁer. Toward this end, we have created a merged network
that contains two copies of the same network, with one input
for one protected attribute group (e.g., male) and the other
input for the other group (e.g., female). While the veriﬁer
ﬁnds counterexamples in seconds (and thus falsiﬁes fairnes s
of the DNN), it has the same limitation as Fairify : it merely
declares the DNN as unsafe (unfair, in our context) as soon as
it ﬁnds a counterexample, but does not provide users with any
meaningful, quantitative information. In contrast, our me thod
provides a quantitative framework for certiﬁed fairness by
reasoning about all individuals in the input domain.IV. O VERVIEW OF OURMETHOD
In this section, we present the top-level procedure of our
method; detailed algorithms of the subroutines will be pre-
sented in subsequent sections.
Let the DNN y=f(x)be implemented as a series of
afﬁne transformations followed by nonlinear activations, where
each afﬁne transformation step and its subsequent nonlinea r
activation step constitute a hidden layer. Let lbe the total
number of hidden layers, then f=fl(fl−1(...f2(f1(x·W1)·
W2)...·Wl−1). For each k∈[1,l],Wkdenotes the afﬁne
transformation and fk()denotes the nonlinear activation. More
speciﬁcally, W1consists of the edge weights at layer 1 and
x·W1= Σixiw1,i. Furthermore, f1is the activation function,
e.g., ReLU (x·W1) =max(0,x·W1).
A. The Basic Components
Similar to existing symbolic interval analysis based DNN
veriﬁers [9], [10], [11], our method consists of three basic
components: forward analysis, classiﬁcation, and reﬁneme nt.
a) Forward Analysis: The goal of forward analysis is
to compute upper and lower bounds of the network’s output
for all inputs. It starts by assigning a symbolic interval to
each input attribute. For example, in Fig. 2, i1= [x1,x1]is
symbolic, where x1∈ {0,1,2,3,4,5}. Compared to concrete
values, symbolic values have the advantages of making the
analysis faster and more scalable. They are also sound in tha t
they overapproximate the possible concrete values.
b) Classiﬁcation: In a binary classiﬁer, e.g., the DNN in
Fig. 2 for making hiring decisions, the output is a singular
nodeowhose numerical values needs to be turned to either
the positive or the negative class label based on a threshold
value (say 0). For example, if o∈[0.3,0.4], the output label
is guaranteed to be positive since o >0always holds, and if
o∈[−0.7,−0.6], the output label is guaranteed to be negative
sinceo <0always holds. However, if o∈[−0.7,0.4], the
output label remains undecided – this is when our method
needs to conduct reﬁnement.
c) Reﬁnement: The goal of reﬁnement is to partition
the input domain of the DNN to improve the (upper and
lower) symbolic bounds computed by forward analysis. Since
approximation error may be introduced when linear bounds
are pushed through nonlinear activation functions (e.g., u nless
ReLU is always on or always off), by partitioning the input
domain, we hope to increase the chance that activation func-
tions behave similar to their linear approximations for eac h
of the new (and smaller) input partitions, thus reducing the
approximation error.
B. The Top-Level Procedure
Algorithm 1 shows the top-level procedure, whose input
consists of the network f, the protected attribute xj, and the
input domain X. Together, these three parameters deﬁne the
fairness certiﬁcation problem, denoted /an}bracketle{tf,xj,X/an}bracketri}ht.
Within the top-level procedure, we ﬁrst initialize the inpu t
partition PasXand push it into the stack S. Each input
partition is associated with a reﬁnement depth. Since PisAlgorithm 1: Overview Method of FairQuant
Input: neural network f, protected attribute xj, input domain X
Result:rcer,rfal,rund, which are percentages of certiﬁed,
falsiﬁed, and undecided inputs
1Initial partition P←Xwith reﬁnement depth 0
2PushPinto an empty stack S
// initially, 100% undecided
3rcer←0,rfal←0,rund←1
4whileSis not empty and not-yet-timed-out do
5 PopPfrom the stack S
// certify current partition
6result←SYMBOLIC FORWARD (f,xj,P)
7 ifresult = Undecided then
// split current partition
8 Pl,Pu←BACKWARD REFINEMENT (f,P)
9 PushPlandPuinto the stack S
// update the percentages
10rcer,rfal,rund←QUANTIFY FAIRNESS (result,P,X )
11returnrcer,rfal,rund
initially the entire input domain X, its reﬁnement depth is
set to 0. Subsequently, the reﬁnement depth increments ever y
timePis bisected to two smaller partitions. In general, the
reﬁnement depth of P⊆Xis the number of times that Xis
bisected to reach P.
In Lines 4-10 of Algorithm 1, we go through each partition
stored in the stack S, until there is no partition left or a time
limit is reached. For each partition P, we ﬁrst apply symbolic
forward analysis (Line 6) to check if the DNN fis fair for
all individuals in P. There are three possible outcomes: (1)
fair(certiﬁed), meaning that f(x) =f(x′)for allx∈Pand
its counterpart x′; (2) unfair (falsiﬁed), meaning that f(x)/ne}ationslash=
f(x′)for allx∈Pand its counterpart x′; or (3) undecided .
Next, if the result is undecided (Line 7), we apply backward
reﬁnement by splitting Pinto two disjoint new partitions Pl
andPu. By focusing on each of these smaller partitions in a
subsequent iteration step, we hope to increase the chance of
certifying it as fair (or falsifying it as unfair).
Finally, we quantify fairness (Line 10) by updating the
percentages of certiﬁed ( rcer), falsiﬁed ( rfal) and undecided
(rund) inputs of X. Speciﬁcally, if the previously-undecided
partitionPis now certiﬁed as fair, we decrease the undecided
raterundby|P|/|X|and increase the certiﬁed rate rcerby the
same amount. On the other hand, if Pis falsiﬁed as unfair ,
we decrease rundby|P|/|X|and increase the falsiﬁed rate
rfalby the same amount.
In the next three sections, we will present our detailed algo -
rithms for forward analysis (Section V), backward reﬁnemen t
(Section VI), and quantiﬁcation (Section VII).
C. The Correctness
Before presenting the detailed algorithms, we would like to
make two claims about the correctness of our method. The
ﬁrst claim is about the qualitative result of forward analysis,
which may be fair, unfair, or undecided.
Theorem 1: When forward analysis declares an input par-
titionP⊆Xasfair, the result is guaranteed to be sound in
thatf(x) =f′(x)holds for all x∈Pand its counterpartx′, Similarly, when forward analysis declares Pasunfair , the
result is guaranteed to be sound in that f(x)/ne}ationslash=f′(x)holds
for allx∈Pand its counterpart x′.
The above soundness guarantee is because S YMBOLIC FOR-
WARD soundly overapproximates the DNN’s actual behavior.
That is, the upper bound UBis possibly-bigger than the actual
value, and the lower bound LB is possibly-smaller than the
actual value. As a result, the symbolic interval [LB,UB]
computed by S YMBOLIC FORWARD guarantees to include all
concrete values. In the next three sections, we shall discus s in
more detail how the symbolic interval is used to decide if P
isfair,unfair , orundecided .
When an input partition Pisundecided , it means that some
individuals in Pmay be treated fairly whereas others in Pmay
be treated unfairly. This brings us to the second claim about
thequantitative result of our method, represented by the rates
rcer,rfalandrund.
Theorem 2: The certiﬁcation rate rcercomputed by our
method is guaranteed to be a lower bound of the percentage of
inputs whose outputs are actually fair. Similarly, the fals iﬁed
raterfalis a lower bound of the percentage of inputs whose
outputs are actually unfair.
In other words, when our method generates the percentages
offairandunfair inputs, it guarantees that they are provable
lower bounds of certiﬁcation and falsiﬁcation, respective ly.
The reason is because S YMBOLIC FORWARD soundly overap-
proximates the actual value range. When the output interval s
indicate that the model is fair(unfair ) for all inputs in P, it is
deﬁnitely fair(unfair ). Thus, both rcerandrfalare guaranteed
to be lower bounds.
Since the sum of the three rates is 1, meaning that rund=
1−rcer−rfal, the undecided rate rundis guaranteed to be
an upper bound.
V. S YMBOLIC FORWARD ANALYSIS
Algorithm 2 shows our forward analysis subroutine, which
takes the subproblem /an}bracketle{tf,xj,P/an}bracketri}htas input and returns the
certiﬁcation result as output.
Algorithm 2: Subroutine S YMBOLIC FORWARD ()
Input: neural network f, protected attribute xj, input partition P
Result: certiﬁcation result , which may be fair/unfair/undecided
1I←P|xj∈[0,0]andI′←P|xj∈[1,1]
2O←FORWARD PASS(f,I)// for x∈Pwithxj∈[0,0]
3O′←FORWARD PASS(f,I′)// for x′∈Pwithxj∈[1,1]
4if(Olb>0∧O′
lb>0)∨(Oub<0∧O′
ub<0)then
5result←Fair
6else if(Olb>0∧O′
ub<0)∨(Oub<0∧O′
lb>0)then
7result←Unfair
8else
9result←Undecided
10returnresult
A. The Two Steps
Our forward analysis consists of two steps. First, a standar d
symbolic interval based analysis is invoked twice, for the0
OO′
both negative
(fair)O′O
both positive
(fair)0
O O′Oneg,O′pos
(unfair)
O′OOpos,O′neg
(unfair)
Fig. 4. Sufﬁcient conditions for deciding fairness based on symbolic output
intervals OandO′, and the threshold 0: there are two fair conditions (left)
and two unfair conditions (right).
symbolic inputs IandI′, to compute the corresponding
symbolic outputs OandO′. Second, OandO′are used to
decide if the certiﬁcation result is fair,unfair , orundecided .
In the ﬁrst step, the symbolic input I=P|xj∈[0,0]is deﬁned
as the subset of input partition Pwhere all inputs have the
protected attribute xjset to 0. In contrast, I′=P|xj∈[1,1]is
deﬁned as the subset of Pwhere all inputs have xjset to
1. The output Ois a sound overapproximation of f(x)for
x∈I, whereas the output O′is a sound overapproximation
off(x′)forx′∈I′. The subroutine F ORWARD PASS used
to compute OandO′is similar to any state-of-the-art neural
network veriﬁer based on symbolic interval analysis; in our
implementation, we used the algorithm of ReluVal [9].
In the second step, the two output intervals, O= [Olb,Oub]
andO′= [O′
lb,O′
ub], are used to compute the certiﬁcation
result. To understand how it works, recall that in the concre te
domain, the numerical value of the DNN’s output node is
compared against a threshold, say 0, to determine if the outp ut
label should be positive or negative. In the symbolic interv al
abstract domain, the upper and lower bounds of the numerical
values are used to determine if the model is fair, unfair, or
undecided.
Below are the ﬁve scenarios:
1) IfOlb>0andO′
lb>0, bothOandO′have the positive
label, meaning that fis fair for P.
2) IfOub<0andO′
ub<0, bothOandO′have the
negative label, meaning that fis fair for P.
3) IfOlb>0andO′
ub<0,Ois positive but O′is negative,
meaning that fis unfair for P.
4) IfOub<0andO′
lb>0,Ois negative but O′is positive,
meaning that fis unfair for P.
5) Otherwise, fremains undecided for P.
Fig. 4 illustrates the ﬁrst four scenarios above. Speciﬁcal ly,
the horizontal line segments represent the value intervals ofO
andO′, whose upper/lower bounds may be either >0or<0.
The vertical lines represent the threshold value 0.
We can extend out method from two protected attribute (PA)
groups (e.g., male and female) to more than two PA groups.
For example, if the protected attribute xjhas three values,
we will have three symbolic inputs ( I,I′andI′′) and three
corresponding symbolic outputs ( O,O′andO′′). To decide if
fis fair (or unfair) in this multi-PA group setting, we check
if (1) individuals in each PA group receive the same output
label; and (2) the output labels for the three PA groups are th e
same.We can also extend our method from binary classiﬁcation
to multi-valued classiﬁcation. For example, if there are th ree
possible output labels, we will have O1,O2,andO3as the
symbolic intervals for the three values for one PA group ( I)
andO′
1,O′
2andO′
3for the other PA group ( I′). To decide
iffis fair (or unfair) for this multi-valued classiﬁcation, we
check (1) which output labels are generated for IandI′; and
(2) whether these two output labels ( OandO′) are the same.
B. The Running Example
For our running example in Fig. 2, consider the initial
input partition P=X. For ease of understanding, we denote
the symbolic expressions for a neuron nasSin(n)after the
afﬁne transformation, and as S(n)after the ReLU activation.
Furthermore, Swill be used for I, andS′will be used for I′.
LetI=P|xj=0andI′=P|xj=1. After afﬁne transfor-
mation in the hidden layer, we have Sin(h1) = 2x1+1.2x3
andS′
in(h1) = 2x1+ 1.2x3+ 0.5. If we concretize these
symbolic expressions, we will have Sin(h1) = [2,16]and
S′
in(h1) = [2.5,16.5]. Based on these concrete intervals, we
know that h1is always active for both IandI′. Since the
activation function is ReLU, we have S(h1) =Sin(h1)and
S′(h1) =S′
in(h1).
For the hidden neuron h2, we have Sin(h2) =−0.2x1+
0.4x3andS′
in(h2) =−0.2x1+0.4x3+0.7, whose correspond-
ing concrete bounds are [−1,1.8]and[−0.3,2.5], respectively.
In both cases, since h2is nonlinear (neither always-on nor
always-off), we must approximate the values using linear
expressions to obtain S(h2)andS′(h2). While we use the
sound overapproximation method of Wang et al. [22], other
techniques (e.g., [23], [10]) may also be used.
After overapproximating the ReLU behavior of h2, we
obtainS(h2) = [−0.128x1+0.257x3,−0.128x1+0.257x3+
0.643] andS′(h2) = [−0.178x1+ 0.357x3+ 0.625,
−0.178x1+0.357x3+0.893].
Finally, we compute Sin(o) = [0.528x1−0.017x3−0.643,
0.528x1−0.017x3]andS′
in(o) = [0.578x1−0.117x3−0.793,
0.578x1−0.117x3−0.525]. From these symbolic bounds, we
obtain the concrete bounds of O= [−0.2,2.64]andO′=
[−0.8,2.368]. Since these output intervals are not tight enough
to determine the output labels for IandI′, which are needed
to decide if the model is fair or unfair for the partition P, the
model remains undecided.
To improve the accuracy, we need to split Pinto smaller
input partitions and then apply symbolic forward analysis t o
each partition again. How to split Pwill be addressed by the
iterative backward reﬁnement method presented in the next
section.
VI. I TERATIVE BACKWARD REFINEMENT
The goal of iterative backward reﬁnement is to split the
currently-undecided input partition Pinto smaller partitions,
so that for each of these smaller partitions, symbolic forwa rd
analysis will obtain a more accurate result. Algorithm 3 sho ws
the pseudo code, which takes the network fand the partition
Pas input and returns two smaller partitions PlandPuasoutput. Inside this procedure, Lines 7-14 are related to spl itting
P, and Lines 2-6 are related to early termination conditions.
Algorithm 3: Subroutine B ACKWARD REFINEMENT ()
Input: neural network f, input partition P
Result: smaller partitions PlandPu, if any
1Letcexcount be the total number of counterexamples found
2if(P.depth≥max reﬁnement depth) then
3 return (null,null) // do not split P
4else if (P.depth≥min sample depth) and SAMPLED CEX (P)then
5 cexcount += 1
6 return (null,null) // do not split P
7else
//R,R′are gradient masks computed for I,I′
8gI←BACKWARD PASS(f,R)
9gI′←BACKWARD PASS(f,R′)
10g= (gI+gI′)/2
// Best input attribute to bisect
11xi←argmaxxig(xi)∗|ub(xi)−lb(xi)|
12Pl←P|xi∈[lb,(lb+ub)/2]
13Pu←P|xi∈[(lb+ub)/2,ub]
14 returnPl,Pu
A. Early Termination Conditions
In Lines 2-6 of Algorithm 3, we check if P.depth
exceeds the predeﬁned max reﬁnement depth . If the an-
swer is yes, we avoid splitting Pfurther. For example, if
max reﬁnement depth= 20 , it means the current partition P
occupies only |P|/|X|=1
220of the entire input domain
X. By increasing the reﬁnement depth, we can decrease the
percentage of undecided inputs over X.
IfP.depth has not exceeded the maximal reﬁne-
ment depth, we check if P.depth exceeds the predeﬁned
min sample depth , which is set to a value (e.g., 15) smaller
than max reﬁnement depth . When P.depth exceeds this
threshold, we start searching for counterexamples in Pvia
random sampling.
Inside the random sampling subroutine S AMPLED CEX(P)
shown in Line 4, we sample up to 10 concrete inputs in P
and check if xand its counterpart x′satisfyf(x)/ne}ationslash=f(x′). If
this condition is satisﬁed, a counterexample is found (but P
remains undecided); in this case, we increment cexcount and
stop splitting P. If no counterexample is found, we continue
splittingPinto smaller partitions.
Note that in both early termination cases (Lines 3 and 6),
the partition Pwill be marked as undecided since we are not
able to decide whether the DNN model is fair or unfair to all
individuals inP.
B. Splitting Input Intervals
In Lines 7-14 of Algorithm 3, we split Pinto smaller
partitions PlandPuby ﬁrst identifying the input attribute
xithat has the largest inﬂuence on the output (Lines 8-12)
and then bisecting its input interval xi∈[lb,ub].
Our method for identifying the input attribute xiis based on
maximizing the impact of an input attribute on the network’s
output. One way to estimate the impact is taking the product
of the gradient g(xi)and the input range |ub(xi)−lb(xi)|. Inthe literature, the product is often called the smear value [24],
[25]. Unlike existing methods such as Wang et al. [9], howeve r,
our computation of the smear value is different because we
must consider both inputs IandI′, which may have different
gradients.
Speciﬁcally, during forward analysis, we store the neuron
activation information in two gradient mask matrices denot ed
RandR′, whereR[i][j]is [1,1] if the j-th neuron at i-th layer
is always active, [0,0] if it is always inactive, and [0,1] if it is
unknown. The neuron activation information is used later to
perform backward reﬁnement for this partition P.
During reﬁnement, we ﬁrst compute the two gradients gI
andgI′and then take the average. Our goal is to identify
the input attribute that has the largest overall inﬂuence on the
network’s output.
C. The Running Example
Consider our running example in Fig. 2 again. To compute
the smear value, we start with the output layer’s edge weight s,
which are 0.2 for h1and -1 for h2. Since the ReLU associated
withh1is always-on, gI(h1)andgI′(h1)are set to the interval
[0.2,0.2]. However, since the ReLU associated with h2is
nonlinear, as indicated by the gradient mask matrices Rand
R′,gI(h2)andgI′(h2)are set to the interval [−1.0,0].
Then, we propagate these gradient intervals backwardly, to
getgI(i1) =gI′(i1) = [(0.2×2) + (0×−0.2),(0.2×2) +
(−1×−0.2)] = [0.4,0.6]andgI(i2) =gI′(i2) = [−0.6,0.1]
andgI(i3) =gI′(i3) = [−0.16,0.24].
Next, we compute the average g, based on which we
compute the smear values. Since x1has the smear value of
0.6×4 = 2.4andx3has the smear value of 0.24×5 = 1.2,
we choose to partition Pby bisecting the input interval of x1.
This leads to the smaller partitions shown in Fig. 3.
D. Generalization
While we only consider ReLU networks in this paper, our
reﬁnement technique can be extended to non-ReLU activa-
tions. Recall that, by deﬁnition, ReLU (z) = 0 (inactive) if
z <0, and ReLU (z) = 1 (active) if z >0. Letσ(z)be a
non-ReLU activation function. To compute the gradient mask
matricesRandR′, we use thresholds ( ǫ1,ǫ2) to approximate
the on/off behavior: the mask is [0,0] (inactive) if z < ǫ1and
[1,1] (active) if z > ǫ2.
Although the approximate on/off behavior of non-ReLU
activation function σ(z)is not the same as the on/off behavior
of ReLU(z), it serves as a practically-useful heuristic to rank
the input attributes. Furthermore, this generalization wi ll not
affect the soundness of our method, since the gradient masks
computed in this manner are only used for picking which input
attribute to split ﬁrst.
VII. F AIRNESS QUANTIFICATION
We now present our method for updating the percentages of
certiﬁed and falsiﬁed inputs, when the DNN model is found to
be fair or unfair for the current input partition P. The pseudo
code is shown in Algorithm 4.Algorithm 4: Subroutine Q UANTIFY FAIRNESS ()
Input: certiﬁcation result , input partition P, input domain X
Result: Percentages rcer,rfal,rund
1partition size=/producttext
∀xi∈P,xi/negationslash=xj(UB(xi)−LB(xi))
2domain size=/producttext
∀xi∈X,xi/negationslash=xj(UB(xi)−LB(xi))
// percentage of inputs in partition P
3rP=(partition size / domain size)
4ifresult=Fair then
5rcer+=rp
6rund−=rp
7else ifresult=Unfair then
8rfal+=rP
9rund−=rP
10returnrcer,rfal,rund
There are three cases. First, if the current partition Pis
found to be fair, meaning that all inputs in Pare treated
fairly, we compute the percentage of input domain Xcovered
by the partition P, denoted rP, and then add rPtorcer, the
percentage of certiﬁed inputs. Second, if the current parti tion
Pis found to be unfair, meaning that all inputs in Pare treated
unfairly, we add rPtorfal, the percentage of falsiﬁed inputs.
In both cases, we also subtract rpfromrund. Otherwise, the
current partition Premains undecided and the percentages
remain unchanged.
Consider our running example with input partition Pde-
ﬁned asx1∈[4,5]∧x2∈[0,1]∧x3∈[0,5], as shown by
the right child of the root node in Fig. 3. This partition has
a total of 24 individuals, and its corresponding I=P|x2=0
andI′=P|x2=1contain 12 individuals each. In contrast, the
entire input domain Xhas 60 individuals, or 30 pairs of x
and its counterpart x′(wherex2/ne}ationslash=x′
2).
For this input partition P,O= [1.49,2.65]andO′=
[1.0,2.36]are the output intervals. Assuming that the decision
threshold is 0, the bounds of OandO′imply that the DNN
model will generate the positive label for both IandI′,
meaning that the DNN model is fair for all individuals in P.
Since the input partition size is 12 and the input domain
size is 30, the rate rP=12
30= 40% . After certifying Pto
be fair, we can add 40% to rcer, the certiﬁcation rate, and
consequently subtract 40% from rund, the undecided rate.
While the above computation assumes that population dis-
tribution for each feature is uniform and thus the percentag e
(e.g., 40%) is computed directly from the partition size (e. g.,
12) and the domain size (e.g., 30), the method can be easily
extended to consider a non-uniform population distributio n.
Furthermore, note that the method works regardless of wheth er
the input attributes have integer or real values.
VIII. E XPERIMENTS
We have implemented FairQuant in a software tool written
in C, by leveraging the OpenBlas2library for fast matrix
multiplication and symbolic representation of the upper an d
lower bounds. Our forward analysis follows that of Wang et
al. [9], [22]. For experimental comparison, we also run Fairify
2http://www.openblas.netTABLE I
STATISTICS OF THE DATASETS AND DNN S USED IN OUR EXPERIMENTS .
Dataset (PA) # Inputs DNN # Layers # Neurons Accuracy (%)
Bank (age) 16BM-1 [12] 2 80 89.20
BM-2 [12] 2 48 88.76
BM-3 [12] 1 100 88.22
BM-4 [12] 3 300 89.55
BM-5 [12] 2 32 88.90
BM-6 [12] 2 18 88.94
BM-7 [12] 2 128 88.70
BM-8 [12] 5 124 89.20
German (age) 20GC-1 [12] 1 50 72.67
GC-2 [12] 1 100 74.67
GC-3 [12] 1 9 75.33
GC-4 [12] 2 10 70.67
GC-5 [12] 5 124 69.33
Adult (gender) 13AC-1 [12] 2 24 85.24
AC-2 [12] 1 100 84.70
AC-3 [12] 1 50 84.52
AC-4 [12] 2 200 84.86
AC-5 [12] 2 128 85.19
AC-6 [12] 2 24 84.77
AC-7 [12] 5 124 84.85
AC-8 [12] 2 10 82.15
AC-9 [12] 4 12 81.22
AC-10 [12] 4 20 78.56
AC-11 [12] 4 40 79.25
AC-12 [12] 9 45 81.46
Compas (race) 6compas-1 2 24 73.46
compas-2 5 124 72.82
compas-3 3 600 72.98
compas-4 9 90 72.98
compas-5 10 2000 72.01
compas-6 4 4000 73.95
compas-7 10 10000 72.49
which is the only currently-available tool for DNN individu al
fairness certiﬁcation. Since Fairify cannot quantify the degree
of fairness, we compute the certiﬁed/falsiﬁed/undecided r ates
based on its reported statistics.
It is worth noting that Fairify and our method ( FairQuant )
have a fundamental difference in falsiﬁcation. Fairify stops
and declares an input partition as SAT as soon as it ﬁnds
a counterexample in that partition; thus, the number of coun -
terexamples that it ﬁnds is always the same as the number
of SAT partitions it reports. However, SAT partitions are no t
necessarily unfair partitions, since unfair partitions require all
inputs to be counterexamples, but a SAT partition, excludin g
the one counterexample, still remains undecided .
FairQuant checks if an entire partition is unfair . Moreover,
when the partition is undecided , it can minimize the amount
ofundecided inputs by only sampling for counterexamples
after it reaches a deep enough reﬁnement depth. This is made
possible through our iterative reﬁnement.
For example, in a DNN model named GC-3, Fairify ﬁnds 194
SAT partitions (together with 6 UNSAT and 1 UNKNOWN
partitions). However, none of these 194 SAT partitions are
unfair partitions. Instead, the percentage of falsiﬁed inputs is
close to being 0% (representing 194 counterexamples out of
over 435 trillion individuals in the input domain), the perc ent-
age of certiﬁed inputs is 2.985% (6 UNSAT partitions out of
201 partitions), and the rest remains undecided. FairQuant , on
the other hand, ﬁnds 25,963 counterexamples; furthermore, it
is able to formally certify 58.44% of the inputs as fair.A. Benchmarks
Table I shows the statistics of the benchmarks, including
32 deep neural networks trained on four popular datasets for
fairness research. Among the 32 networks, 25 came from
Fairify [12] and the other 7 were trained by ourselves using
TensorFlow. All of these networks have a single node in
the output layer, to determine the binary classiﬁcation re-
sult. Columns 1-2 show the name of each dataset with its
considered protected attribute (PA) and the number of input
attributes. Columns 3-6 show the name of each DNN model,
its number of hidden layers, number of hidden neurons, and
classiﬁcation accuracy. The accuracy for DNNs trained on
Bank ,German , and Adult was provided by the Fairify paper.
For the models we trained using Compas , we have reserved
10% of the data for testing. All the networks coming directly
from Fairify onBank ,German , and Adult datasets are small,
where the largest has only 200 hidden neurons. Moreover,
most of them have only 1 or 2 hidden layers. Thus, we
additionally trained much larger networks, with up to 10,00 0
hidden neurons, using the Compas dataset.
Details of the four datasets are given as follows. Bank [18]
is a dataset for predicting if a bank client will subscribe to
its marketing; it consists of 45,000 samples. German [19] is
a dataset for predicting the credit risk of a person; it consi sts
of 1,000 samples. Adult [20] is a dataset for predicting if a
person earns more than $50,000; it consists of 32,561 sample s.
Finally, Compas [21] is a dataset for predicting the risk of
recidivism; it consists of 6,172 samples.3
We evaluate our method using three legally-protected input
attributes. For Bank andGerman , we use age; for Adult , we use
gender ; and for Compas , we use race.4These are consistent
with Fairify and other prior works in the fairness research.
B. Experimental Setup
We ran all experiments on a computer with 2 CPU, 4GB
memory, and Ubuntu 20.04 Linux operating system. We set
a time limit of 30 minutes for each DNN model. Our experi-
ments were designed to answer three research questions:
1) Is FairQuant more accurate than the current state-of-the-
art in certifying individual fairness of a DNN model?
2) Is FairQuant more scalable than the current state-of-
the-art in handling DNN models, especially when the
network size increases?
3) Is FairQuant more effective than the current state-of-
the-art in providing feedback, e.g., by quantitatively
measuring the percentages of certiﬁed, falsiﬁed, and
undecided inputs?
Fairify requires a parameter MS(maximum size of an input
attribute) based on which it creates a ﬁxed number of input
partitions prior to certiﬁcation. On the DNN models trained
forBank ,German , and Adult , we used the default MSvalues
3We used the preprocessed Compas data provided by [26].
4For Bank and German, we use binarized ageattribute provided by Fairify .
For Compas, we binarize race attribute into{white, non-white}as done
in [27].TABLE II
RESULTS FOR FAIRNESS CERTIFICATION :FAIRIFY VIS-`A-VISFAIRQUANT
Dataset DNNFairify [12] FairQuant (new)
Time Cex #Cex Cer% Fal% Und% Time Cex #Cex Cer% Fal% Und%BankBM-1 30m ✓ 11 10.00 0 90.00 4.82s ✓ 2820 94.23 0 5.76
BM-2 31m ✓ 28 16.07 0 83.93 3.23s ✓ 2479 93.41 0 6.58
BM-3 31m ✓ 27 19.60 0 81.40 1.21s ✓ 1864 95.69 0 4.30
BM-4 35m ✓ 4 3.72 0 96.18 71.12s ✓ 5135 87.03 0 12.96
BM-5 23m ✓ 114 77.25 0 22.75 1.03s ✓ 1474 96.27 0 3.72
BM-6 12m ✓ 155 69.41 0 30.59 0.44s ✓ 1426 96.44 0 3.55
BM-7 30m ✓ 57 9.41 0 90.59 12.26s ✓ 7017 83.65 0 16.34
BM-8 30m ✓ 1 0.98 0 99.02 18.99s ✓ 3074 90.75 0 9.24GermanGC-1 32m ✓ 22 0 0 100 9.73s ✓ 31585 32.67 0 67.33
GC-2 33m ✓ 6 0 0 100 31.72s ✓ 32655 42.21 0 57.79
GC-3 8m ✓ 194 2.98 0 97.02 6.77s ✓ 25963 58.44 0 41.55
GC-4 4m ✓ 299.00 0 1.00 0.29s ✓ 77 99.65 0 0.34
GC-5 30m ✗ 0 0 0 100 1.24s ✓ 999.80 0 0.19AdultAC-1 32m ✓ 3 0.03 0 99.97 3.23s ✓ 6151 90.68 0 9.31
AC-2 31m ✓ 9 0.01 0 99.99 30.04s ✓ 13008 79.93 0 20.06
AC-3 32m ✓ 20 0 0 100 37.12s ✓ 60494 33.29 0 66.70
AC-4 36m ✗ 0 0 0 100 8m ✓ 61324 24.79 0 75.20
AC-5 33m ✗ 0 0 0 100 4m ✓ 71012 19.12 0 80.87
AC-6 33m ✓ 4 0.01 0 99.99 10.20s ✓ 31593 58.82 0 41.17
AC-7 30m ✗ 0 0.01 0 99.99 4m ✓ 25588 31.72 0 68.27
AC-8 30m ✓ 39 0.03 0 99.97 11.18s ✓ 26179 66.50 0 33.49
AC-9 30m ✓ 126 0.64 0 99.36 3.50s ✓ 5470 91.13 0 8.86
AC-10 32m ✓ 8 0.03 0 99.97 5.01s ✓ 9033 87.65 0 12.34
AC-11 30m ✗ 0 0 0 100 36.44s ✓ 24516 58.01 0 41.98
AC-12 30m ✗ 0 0.02 0 99.98 0.91s ✓ 8824 70.82 0 29.17Compascompas-1 17m ✓ 280.00 0.32 19.68 0.01s ✓ 17 97.27 2.72 0
compas-2 31m ✗ 0 0 0 100 0.01s ✓ 15 97.59 2.40 0
compas-3 30m ✗ 0 0 0 100 0.30s ✓ 12 98.07 1.92 0
compas-4 30m ✗ 0 0 0 100 0.01s ✓ 14 97.75 2.24 0
compas-5 T/O ✗ 0 0 0 100 5.24s ✓ 11 98.23 1.76 0
compas-6 M/O ✗ 0 0 0 100 9.19s ✓ 12 98.07 1.92 0
compas-7 M/O ✗ 0 0 0 100 101.25s ✓ 15 97.59 2.40 0
(100, 100, and 10) for Fairify to create 510, 201, and 16000
partitions, respectively. On the new DNN models trained for
Compas , we set MSto a small value of 2 to create 20 partitions
forFairify . This was done to maximize Fairify ’s performance
such that it does not “choke” in verifying each input partiti on.
By default, Fairify uses 100 seconds as “soft timeout” for
each input partition and uses 30 minutes as “hard timeout” fo r
the entire DNN. This means that it takes at most 100 seconds
to verify a single input partition, and if unsolved, it just m oves
to the next partition, until the entire 30 minutes runs out.
To run FairQuant , we set the parameters min check depth to
15 and max reﬁnement depth to 20 for all DNN models. We
also use 30 minutes as “hard timeout”, but FairQuant always
ﬁnished before the limit.
C. Experimental Results
Table II shows the results of our method ( FairQuant ) in
comparison with Fairify5. Columns 1-2 show the names of the
dataset and the DNN model. Columns 3-5 show the statistics
reported by Fairify , including the time taken, whether a coun-
terexample is found ( Cex) and the number of counterexamples
found ( #Cex ). T/O or M/O in Column 3 respectively means
that Fairify either spent all 30m or ran out of memory in the
network pruning step prior to verifying any input partition .
Columns 6-8 show the percentage of certiﬁed, falsiﬁed and
undecided inputs ( Cer%, Fal%, Und% ). Columns 9-14 show
the corresponding results from FairQuant .
5The order in which Fairify sorts the partitions before running the veriﬁca-
tion query is random and non-deterministic, so there may be m inor difference
in the reported counterexamples in the original evaluation and ours.BM-1 GC-1 AC-1 compas-10102030Runtime (m)
BM-1 GC-1 AC-1 compas-1020406080100Undecided (%)
Fig. 5. Comparing the runtime overhead (left) and accuracy ( right) of
Fairify [12], in red, and FairQuant (new), in blue.
1) Results for RQ 1: To answer the ﬁrst research question
(RQ 1), i.e., whether our method is more accurate than the
current state-of-the-art, we need to compare the results sh own
in Columns 4-5 (for Fairify ) with the results shown in Columns
10-11 (for FairQuant ). Speciﬁcally, Columns 4 and 10 indicate
whether the tool is able to ﬁnd a counterexample ( ✓) or not
(✗) within the time limit.
While our method ( FairQuant ) found counterexamples for
all 32 DNNs, Fairify found counterexamples for only 20 of the
32 models. In addition, it found counterexamples for only on e
of the 7 newly added models. Moreover, Columns 5 and 11
show that, on models where both tools found counterexamples ,
the number of counterexamples found by FairQuant is often
thousands of times more. For example, the largest number
of counterexamples found by Fairify is 194 (for GC-3) but
the large number of counterexamples found by FairQuant is
71,012 (for AC-5).
2) Results for RQ 2: To answer the second research ques-
tion (RQ 2), i.e., whether our method is more scalable than th e
current state-of-the-art, we need to compare the running ti me
shown in Column 3 (for Fairify ) with the running time shown
in Column 9 (for FairQuant ). While our method ( FairQuant )
always ﬁnished within the time limit of 30 minutes, Fairify
timed out on compas-5 and ran out of memory on compas-6
and compas-7. Even on the models where both tools ﬁnished,
the time taken by Fairify is signiﬁcantly longer.
To illustrate the scalability advantage of our method, we
took a subset of the models for which both FairQuant and
Fairify ﬁnished, and plot the running time in a bar chart, shown
on the left side of Fig. 5. Here, the red bars represent the tim e
taken by Fairify and the blue bars represent the time taken by
FairQuant . The results show that FairQuant is many orders of
magnitude faster, and can certify DNN models that are well
beyond the reach of Fairify .
3) Results for RQ 3: To answer the third research question
(RQ 3), i.e., whether our method is more effective in providi ng
feedback to the user, we need to compare the results in
Columns 6-8 (for Fairify ) with the results in Columns 12-
14 (for FairQuant ), which show the certiﬁed, falsiﬁed, and
undecided percentages. Since Fairify was not designed to
quantitatively measure the degree of fairness, it did poorl y
in almost all cases. Except for a few DNN models for Bank,GC-4, and compas-1, its certiﬁed percentages are either 0 or
close to 0, and its undecided percentages are almost 100%. It
means that, for the vast majority of individuals in the input
domain, whether they are treated by the DNN model fairly or
not remains undecided.
In contrast, the certiﬁed percentages reported by our metho d
(FairQuant ) are signiﬁcantly higher. For the models trained
using the Compas dataset, in particular, the certiﬁed perce nt-
ages are around or above 90%, and more importantly, the
undecided percentages are always 0. It means that FairQuant
has partitioned the input domain in such a way that each
partition is either certiﬁed as being fair, or falsiﬁed as be ing
unfair. Even on the subset of DNN models where some inputs
remain undecided by FairQuant , the undecided percentages
reported by our method are signiﬁcantly lower than Fairify ,
as shown on the right side of Fig. 5.
D. Summary
The results show that our method is more accurate and
more scalable than the current state-of-the-art technique s for
qualitative certiﬁcation. In addition, our method is able to
formally quantify the degree of fairness, which is a capability
that existing methods do not have.
For some DNN models, FairQuant still has a signiﬁcant
percentage of inputs left undecided. This is because we set
min check depth=15 and max reﬁnement depth=20 for all
benchmarks. Thus, as soon as FairQuant reaches the reﬁnement
depth 15 (see the reﬁnement tree shown in Fig. 3) and ﬁnds
a counterexample in the input partition, it will stop reﬁnin g
further; at that moment, all inputs in the partition are trea ted
conservatively as undecided.
In general, a smaller reﬁnement depth allows FairQuant to
terminate quickly. During our experiments, FairQuant termi-
nated after 0.29s and 1.24s for GC-4 and GC-5, respectively,
compared to the 4 minutes and 30 minutes taken by Fairify and
yet returned better results. In fact, for GC-5, Fairify spent 30
minutes but failed to ﬁnd any counterexample. If we increase
the reﬁnement depth, by increasing the two threshold values
ofFairQuant , its quantiﬁcation results will get even better.
IX. R ELATED WORK
Our method is the ﬁrst scalable method for certifying and
quantifying individual fairness of a deep neural network, a nd
it outperforms the most closely related prior work, Fairify [12].
To the best of our knowledge, no other methods can match the
accuracy, scalability, and functionality of our method.
Our method differs from existing techniques for verifying
individual fairness properties for neural networks. Libra [7],
[28] uses abstract domains to perform veriﬁcation but is
limited in scalability due to the expensive pre-analysis, w here
a network of 20 hidden nodes takes several hours even
with leveraging multiple CPUs. DeepGemini [29], which
outperforms Libra, is built on top of Marabou [30], a SMT-
based neural network veriﬁcation tool, and thus shares the
same limitations of Fairify . Furthermore, it only evaluates on
networks with up to around 250 hidden neurons. Other workshave tackled neural network veriﬁcation of different deﬁni tions
of individual fairness. Benussi et al. [31] and Khedr et al. [ 32]
proposed different methods to certify a deﬁnition of global
individual fairness proposed in [33]. Ruoss et al. [34] veri fy
a type of a local individual fairness property that is similar to
local robustness, given an input xand a small constant εfor
perturbation. This is different from a global perspective we
have discussed so far in this paper.
Group fairness is yet another type of fairness property,
which can be veriﬁed using probabilistic techniques [16],
[17], [35]. The difference between individual fairness and
group fairness is that, while individual fairness requires similar
individuals to be treated similarly, group fairness requires
similar demographic groups to be treated similarly.
There are other prior works related to fairness veriﬁcation
of other types of machine learning models [33], [36], [8],
but they are not applicable to deep neural networks. Testing
techniques can quickly detect fairness violations in machi ne
learning models including neural networks [37], [6], [13], [38],
[14], [39], but it does not provide formal guarantee that is
important for certain applications. There are also techniq ues
for improving fairness of machine learning models [40], [41 ],
[42], [43], [44], [45], [46], which are orthogonal to our met hod
that focuses on certifying and quantifying fairness of exis ting
DNN models.
At a high level, our method is related to the large number
of robustness veriﬁers for deep neural networks based on
interval analysis [9], [22], [10], [47], SMT solving [23],
[48], [30], [49], and mixed-integer linear programming [50 ],
[51]. While these veriﬁers can decide if a model is robust
against adversarial perturbation, they cannot directly ce rtify
individual fairness, as explained earlier in Section II. Ot her
neural network veriﬁers that deal with differential [52], [ 53],
[54] or equivalence veriﬁcation [55] are also different, si nce
they evaluate over two networks instead of one network.
X. C ONCLUSION
We have presented FairQuant , a scalable method for cer-
tifying and quantifying individual fairness of a deep neura l
network over the entire input domain. It relies on sound
abstraction during symbolic forward analysis to improve sc al-
ability, and iterative reﬁnement based on backward analysi s to
improve accuracy. In addition to certifying fairness, it is able to
quantify the degree of fairness by computing the percentage s
of inputs whose classiﬁcation outputs can be certiﬁed as fai r or
falsiﬁed as unfair. We have evaluated the method on a large
number of DNN models trained using four popular fairness
research datasets. The experimental results show that the
method signiﬁcantly outperforms state-of-the-art techni ques in
terms of both accuracy and scalability, as well as the abilit y
to quantify the degree of fairness.
ACKNOWLEDGMENTS
This research was supported in part by the U.S. National
Science Foundation (NSF) under grant CCF-2220345. We
thank the anonymous reviewers for their constructive feedb ack.REFERENCES
[1] A. Castelnovo, R. Crupi, G. D. Gamba, G. Greco, A. Naseer, D. Regoli,
and B. S. Miguel Gonzalez, “Befair: Addressing fairness in t he banking
sector,” in 2020 IEEE International Conference on Big Data (Big Data) ,
pp. 3652–3661, 2020.
[2] J. K. Paulus and D. M. Kent, “Predictably unequal: unders tanding and
addressing concerns that algorithmic clinical prediction may increase
health disparities,” NPJ digital medicine , vol. 3, no. 1, p. 99, 2020.
[3] K. T. Rodolfa, H. Lamba, and R. Ghani, “Empirical observa tion of
negligible fairness–accuracy trade-offs in machine learn ing for public
policy,” Nature Machine Intelligence , vol. 3, no. 10, pp. 896–904, 2021.
[4] C. Wang, B. Han, B. Patel, and C. Rudin, “In pursuit of inte rpretable,
fair and accurate machine learning for criminal recidivism prediction,”
Journal of Quantitative Criminology , vol. 39, no. 2, pp. 519–581, 2023.
[5] C. Dwork, M. Hardt, T. Pitassi, O. Reingold, and R. Zemel, “Fairness
through awareness,” in Proceedings of the 3rd innovations in theoretical
computer science conference , pp. 214–226, 2012.
[6] S. Galhotra, Y . Brun, and A. Meliou, “Fairness testing: t esting software
for discrimination,” in Proceedings of the 2017 11th Joint Meeting on
Foundations of Software Engineering , (Paderborn Germany), pp. 498–
510, ACM, Aug. 2017.
[7] C. Urban, M. Christakis, V . W¨ ustholz, and F. Zhang, “Per fectly parallel
fairness certiﬁcation of neural networks,” Proceedings of the ACM on
Programming Languages , vol. 4, no. OOPSLA, pp. 1–30, 2020.
[8] M. J. Kusner, J. Loftus, C. Russell, and R. Silva, “Counte rfactual
fairness,” Advances in neural information processing systems , vol. 30,
2017.
[9] S. Wang, K. Pei, J. Whitehouse, J. Yang, and S. Jana, “Form al security
analysis of neural networks using symbolic intervals,” in 27th USENIX
Security Symposium (USENIX Security 18) , pp. 1599–1614, 2018.
[10] G. Singh, T. Gehr, M. P¨ uschel, and M. Vechev, “An abstra ct domain for
certifying neural networks,” Proceedings of the ACM on Programming
Languages , vol. 3, pp. 1–30, Jan. 2019.
[11] S. Wang, H. Zhang, K. Xu, X. Lin, S. Jana, C.-J. Hsieh, and J. Z.
Kolter, “Beta-CROWN: Efﬁcient bound propagation with per- neuron
split constraints for complete and incomplete neural netwo rk veriﬁ-
cation,” Advances in Neural Information Processing Systems , vol. 34,
2021.
[12] S. Biswas and H. Rajan, “Fairify: Fairness veriﬁcation of neural net-
works,” in 2023 IEEE/ACM 45th International Conference on Software
Engineering (ICSE) , pp. 1546–1558, IEEE, 2023.
[13] R. Angell, B. Johnson, Y . Brun, and A. Meliou, “Themis: a utomatically
testing software for discrimination,” in Proceedings of the 2018 26th
ACM Joint Meeting on European Software Engineering Confere nce and
Symposium on the Foundations of Software Engineering , (Lake Buena
Vista FL USA), pp. 871–875, ACM, Oct. 2018.
[14] P. Zhang, J. Wang, J. Sun, G. Dong, X. Wang, X. Wang, J. S. D ong,
and T. Dai, “White-box fairness testing through adversaria l sampling,”
inProceedings of the ACM/IEEE 42nd International Conference on
Software Engineering , ICSE ’20, (New York, NY , USA), p. 949–960,
Association for Computing Machinery, 2020.
[15] V . Monjezi, A. Trivedi, G. Tan, and S. Tizpaz-Niari, “In formation-
theoretic testing and debugging of fairness defects in deep neural
networks,” in Proceedings of the 45th International Conference on
Software Engineering , ICSE ’23, p. 1571–1582, IEEE Press, 2023.
[16] A. Albarghouthi, L. D’Antoni, S. Drews, and A. V . Nori, “ Fairsquare:
probabilistic veriﬁcation of program fairness,” Proceedings of the ACM
on Programming Languages , vol. 1, no. OOPSLA, pp. 1–30, 2017.
[17] O. Bastani, X. Zhang, and A. Solar-Lezama, “Probabilis tic veriﬁcation
of fairness properties via concentration,” Proceedings of the ACM on
Programming Languages , vol. 3, no. OOPSLA, pp. 1–27, 2019.
[18] S. Moro, P. Rita, and P. Cortez, “Bank Marketing.” UCI Ma chine
Learning Repository, 2012. DOI: https://doi.org/10.2443 2/C5K306.
[19] H. Hofmann, “Statlog (German Credit Data).” UCI Machin e Learning
Repository, 1994. DOI: https://doi.org/10.24432/C5NC77 .
[20] B. Becker and R. Kohavi, “Adult.” UCI Machine Learning R epository,
1996. DOI: https://doi.org/10.24432/C5XW20.
[21] J. Angwin, J. Larson, S. Mattu, and L. Kirchner, “Machin e bias,” 2016.
[22] W. Shiqi, K. Pei, W. Justin, J. Yang, and S. Jana, “Efﬁcie nt formal safety
analysis of neural networks,” in 32nd Conference on Neural Information
Processing Systems (NIPS) , (Montreal, Canada), 2018.[23] R. Ehlers, “Formal veriﬁcation of piece-wise linear fe ed-forward neural
networks,” in Automated Technology for Veriﬁcation and Analysis: 15th
International Symposium, ATVA 2017, Pune, India, October 3 –6, 2017,
Proceedings 15 , pp. 269–286, Springer, 2017.
[24] R. B. Kearfott and M. Novoa III, “Algorithm 681: INTBIS, a portable
interval Newton/bisection package,” ACM Transactions on Mathematical
Software , vol. 16, no. 2, pp. 152–157, 1990.
[25] R. B. Kearfott, Rigorous global search: continuous problems , vol. 13.
Springer Science & Business Media, 2013.
[26] J. A. Adebayo, FairML: ToolBox for diagnosing bias in predictive
modeling . PhD thesis, Massachusetts Institute of Technology, 2016.
[27] T. Le Quy, A. Roy, V . Iosiﬁdis, W. Zhang, and E. Ntoutsi, “ A survey on
datasets for fairness-aware machine learning,” Wiley Interdisciplinary
Reviews: Data Mining and Knowledge Discovery , vol. 12, no. 3,
p. e1452, 2022.
[28] D. Mazzucato and C. Urban, “Reduced products of abstrac t domains
for fairness certiﬁcation of neural networks,” in Static Analysis: 28th
International Symposium, SAS 2021, Chicago, IL, USA, Octob er 17–19,
2021, Proceedings 28 , pp. 308–322, Springer, 2021.
[29] X. Xie, F. Zhang, X. Hu, and L. Ma, “Deepgemini: Verifyin g dependency
fairness for deep neural network,” Proceedings of the AAAI Conference
on Artiﬁcial Intelligence , vol. 37, pp. 15251–15259, Jun. 2023.
[30] G. Katz, D. A. Huang, D. Ibeling, K. Julian, C. Lazarus, R . Lim,
P. Shah, S. Thakoor, H. Wu, A. Zelji´ c, D. L. Dill, M. J. Kochen derfer,
and C. Barrett, “The marabou framework for veriﬁcation and a nalysis
of deep neural networks,” in Computer Aided Veriﬁcation (I. Dillig
and S. Tasiran, eds.), (Cham), pp. 443–452, Springer Intern ational
Publishing, 2019.
[31] E. Benussi, A. Patane’, M. Wicker, L. Laurenti, and M. Kw iatkowska,
“Individual fairness guarantees for neural networks,” in Proceedings of
the Thirty-First International Joint Conference on Artiﬁc ial Intelligence,
IJCAI-22 (L. D. Raedt, ed.), pp. 651–658, International Joint Confer -
ences on Artiﬁcial Intelligence Organization, 7 2022. Main Track.
[32] H. Khedr and Y . Shoukry, “Certifair: A framework for cer tiﬁed global
fairness of neural networks,” in Proceedings of the AAAI Conference on
Artiﬁcial Intelligence , pp. 8237–8245, 2023.
[33] P. G. John, D. Vijaykeerthy, and D. Saha, “Verifying ind ividual fairness
in machine learning models,” in Conference on Uncertainty in Artiﬁcial
Intelligence , pp. 749–758, PMLR, 2020.
[34] A. Ruoss, M. Balunovic, M. Fischer, and M. Vechev, “Lear ning certi-
ﬁed individually fair representations,” Advances in neural information
processing systems , vol. 33, pp. 7584–7596, 2020.
[35] M. Feldman, S. A. Friedler, J. Moeller, C. Scheidegger, and S. Venkata-
subramanian, “Certifying and Removing Disparate Impact,” inProceed-
ings of the 21th ACM SIGKDD International Conference on Know ledge
Discovery and Data Mining , (Sydney NSW Australia), pp. 259–268,
ACM, Aug. 2015.
[36] Y . Li, J. Wang, and C. Wang, “Certifying the fairness of K NN in the
presence of dataset bias,” in International Conference on Computer
Aided Veriﬁcation. Springer , 2023.
[37] A. Aggarwal, P. Lohia, S. Nagar, K. Dey, and D. Saha, “Bla ck box
fairness testing of machine learning models,” in Proceedings of the 2019
27th ACM Joint Meeting on European Software Engineering Con ference
and Symposium on the Foundations of Software Engineering , ESEC/FSE
2019, (New York, NY , USA), p. 625–635, Association for Compu ting
Machinery, 2019.
[38] S. Udeshi, P. Arora, and S. Chattopadhyay, “Automated d irected fairness
testing,” in Proceedings of the 33rd ACM/IEEE International Conference
on Automated Software Engineering , ASE ’18, (New York, NY , USA),
p. 98–108, Association for Computing Machinery, 2018.
[39] H. Zheng, Z. Chen, T. Du, X. Zhang, Y . Cheng, S. Ji, J. Wang , Y . Yu, and
J. Chen, “Neuronfair: Interpretable white-box fairness te sting through
biased neuron identiﬁcation,” in Proceedings of the 44th International
Conference on Software Engineering , pp. 1519–1531, 2022.
[40] M. Yurochkin, A. Bower, and Y . Sun, “Training individua lly fair ml
models with sensitive subspace robustness,” in International Conference
on Learning Representations , 2020.
[41] J. Wang, Y . Li, and C. Wang, “Synthesizing fair decision trees via
iterative constraint solving,” in Computer Aided Veriﬁcation: 34th In-
ternational Conference, CAV 2022, Haifa, Israel, August 7– 10, 2022,
Proceedings, Part II , (Berlin, Heidelberg), p. 364–385, Springer-Verlag,
2022.
[42] X. Gao, J. Zhai, S. Ma, C. Shen, Y . Chen, and Q. Wang, “Fair Neuron:
Improving deep neural network fairness with adversary game s onselective neurons,” in Proceedings of the 44th International Conference
on Software Engineering , ICSE ’22, (New York, NY , USA), p. 921–933,
Association for Computing Machinery, 2022.
[43] F. Ranzato, C. Urban, and M. Zanella, “Fairness-aware t raining of
decision trees by abstract interpretation,” in Proceedings of the 30th
ACM International Conference on Information & Knowledge Ma nage-
ment , CIKM ’21, (New York, NY , USA), p. 1508–1517, Association fo r
Computing Machinery, 2021.
[44] P. Lahoti, A. Beutel, J. Chen, K. Lee, F. Prost, N. Thain, X. Wang,
and E. Chi, “Fairness without demographics through adversa rially
reweighted learning,” Advances in neural information processing sys-
tems, vol. 33, pp. 728–740, 2020.
[45] T. Hashimoto, M. Srivastava, H. Namkoong, and P. Liang, “Fairness
without demographics in repeated loss minimization,” in International
Conference on Machine Learning , pp. 1929–1938, PMLR, 2018.
[46] M. Hardt, E. Price, and N. Srebro, “Equality of opportun ity in supervised
learning,” Advances in neural information processing systems , vol. 29,
2016.
[47] P. Yang, R. Li, J. Li, C.-C. Huang, J. Wang, J. Sun, B. Xue, and L. Zhang,
“Improving neural network veriﬁcation through spurious re gion guided
reﬁnement,” in International Conference on Tools and Algorithms for
the Construction and Analysis of Systems , pp. 389–408, Springer, 2021.
[48] G. Katz, C. Barrett, D. L. Dill, K. Julian, and M. J. Koche nderfer,
“Reluplex: An efﬁcient SMT solver for verifying deep neural networks,”
inComputer Aided Veriﬁcation: 29th International Conferenc e, CAV
2017, Heidelberg, Germany, July 24-28, 2017, Proceedings, Part I 30 ,
pp. 97–117, Springer, 2017.
[49] X. Huang, M. Kwiatkowska, S. Wang, and M. Wu, “Safety ver iﬁcationof deep neural networks,” in Computer Aided Veriﬁcation: 29th Interna-
tional Conference, CAV 2017, Heidelberg, Germany, July 24- 28, 2017,
Proceedings, Part I 30 , pp. 3–29, Springer, 2017.
[50] O. Bastani, Y . Ioannou, L. Lampropoulos, D. Vytiniotis , A. Nori,
and A. Criminisi, “Measuring neural net robustness with con straints,”
Advances in neural information processing systems , vol. 29, 2016.
[51] R. Bunel, I. Turkaslan, P. H. S. Torr, M. P. Kumar, J. Lu, a nd P. Kohli,
“Branch and bound for piecewise linear neural network veriﬁ cation,” J.
Mach. Learn. Res. , vol. 21, Jan. 2020.
[52] B. Paulsen, J. Wang, and C. Wang, “Reludiff: Differenti al veriﬁcation
of deep neural networks,” in Proceedings of the ACM/IEEE 42nd
International Conference on Software Engineering , pp. 714–726, 2020.
[53] B. Paulsen, J. Wang, J. Wang, and C. Wang, “Neurodiff: sc alable differ-
ential veriﬁcation of neural networks using ﬁne-grained ap proximation,”
inProceedings of the 35th IEEE/ACM International Conference on
Automated Software Engineering , pp. 784–796, 2020.
[54] S. Mohammadinejad, B. Paulsen, J. V . Deshmukh, and C. Wa ng,
“DiffRNN: Differential veriﬁcation of recurrent neural ne tworks,” in
Formal Modeling and Analysis of Timed Systems - 19th Interna tional
Conference, FORMATS 2021, Paris, France, August 24-26, 202 1, Pro-
ceedings (C. Dima and M. Shirmohammadi, eds.), vol. 12860 of Lecture
Notes in Computer Science , pp. 117–134, Springer, 2021.
[55] C. Eleftheriadis, N. Kekatos, P. Katsaros, and S. Tripa kis, “On neural
network equivalence checking using SMT solvers,” in International Con-
ference on Formal Modeling and Analysis of Timed Systems , pp. 237–
257, Springer, 2022.
[56] S. Weisberg, Applied Linear Regression . John Wiley & Sons, 3rd ed.,
2005.