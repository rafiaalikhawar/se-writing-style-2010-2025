DeepDiagnosis: AutomaticallyDiagnosingFaults and
RecommendingActionable Fixes in Deep LearningPrograms
MohammadWardat
wardat@iastate.edu
Dept. ofComputerScience, Iowa State University
226AtanasoÔ¨Ä Hall, Ames, IA,USABreno DantasCruz
bdantasc@iastate.edu
Dept. ofComputerScience, Iowa State University
226AtanasoÔ¨Ä Hall, Ames, IA,USA
Wei Le
weile@iastate.edu
Dept. ofComputerScience, Iowa State University
226AtanasoÔ¨Ä Hall, Ames, IA,USAHridesh Rajan
hridesh@iastate.edu
Dept. ofComputerScience, Iowa State University
226AtanasoÔ¨Ä Hall, Ames, IA,USA
ABSTRACT
Deep Neural Networks (DNNs) are used in a wide variety of ap-
plications. H owever, as in any software application, DNN-based
appsareaÔ¨Ñictedwithbugs.PreviousworkobservedthatDNNbug
/f_ix patterns are diÔ¨Äerent from traditional bug /f_ix patterns. Further-
more,thosebuggymodelsarenon-trivialtodiagnoseand/f_ixdue
to inexplicit errors with several options to /f_ix them. To support
developers in locating and /f_ixing bugs, we propose DeepDiagnosis,
anovel debuggingapproach thatlocalizesthefaults,reportserror
symptoms and suggests /f_ixes for DNN programs. In the /f_irst phase,
our technique monitors a training model, periodically checking for
eighttypesoferrorconditions.Then,incaseofproblems,itreports
messages containingsuÔ¨Écientinformationto performactionable
repairstothemodel.Intheevaluation,wethoroughlyexamine444
models‚Äì53real-worldfromGitHuband StackOver/f_low ,and391
curatedbyAUTOTRAINER.DeepDiagnosisprovidessuperioraccu-
racywhencomparedtoUMLUATandDeepLocalize.Ourtechnique
is faster than AUTOTRAINER for fault localization. The results
show that our approach can support additional types of models,
whilestate-of-the-artwasonlyabletohandleclassi/f_icationones.
Our technique was able to report bugs that do not manifest as
numericalerrorsduringtraining.Also,itcanprovideactionablein-
sights for /f_ix whereasDeepLocalize can only report faults that lead
tonumericalerrorsduringtraining.DeepDiagnosismanifeststhe
best capabilities of fault detection, bug localization, and symptoms
identi/f_icationwhencompared to other approaches.
CCSCONCEPTS
‚Ä¢Computingmethodologies ‚ÜíNeuralnetworks ;‚Ä¢Software
andits engineering ‚ÜíSoftware testinganddebugging .
Permissionto makedigital or hard copiesof allorpart ofthis work forpersonal or
classroom use is granted without fee provided that copies are not made or distributed
forpro/f_itorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation
on the /f_irst page. Copyrights for components of this work owned by others than ACM
mustbehonored.Abstractingwithcreditispermitted.Tocopyotherwise,orrepublish,
topostonserversortoredistributetolists,requirespriorspeci/f_icpermissionand/ora
fee.Request permissions frompermissions@acm.org.
ICSE‚Äô22,May21‚Äì29,2022,Pittsburgh,PA, USA
¬©2022Association forComputingMachinery.
ACMISBN 978-1-4503-9221-1/22/05...$15.00
https://doi.org/10.1145/3510003.3510071KEYWORDS
deep neural networks, fault location, debugging, program analysis,
deeplearningbugs
ACMReference Format:
MohammadWardat,BrenoDantasCruz,WeiLe,andHrideshRajan.2022.
DeepDiagnosis:AutomaticallyDiagnosingFaultsandRecommendingAc-
tionable Fixes in Deep Learning Programs. In 44th International Conference
on Software Engineering (ICSE ‚Äô22), May 21‚Äì29, 2022, Pittsburgh, PA, USA.
ACM,NewYork,NY,USA, 12pages.https://doi.org/10.1145/3510003.3510071
1 INTRODUCTION
Deep Neural Networks (DNNs) are becoming increasingly popular
due to their successful applications in manyareas, suchas health-
care [27,34], transportation [ 43], and entertainment[ 21]. But, the
intrinsic complexity of deep learning apps requires that developers
build DNNswithintheirsoftware systemstofacilitate integration
anddevelopmentwithotherapplications.Theconstructionofsuch
systemsrequires popular DeepLearninglibraries [ 18,32].
Despite the increasing popularity and many successes for using
Deep Learning libraries and frameworks, DNN applications still
suÔ¨Äer from reliability issues [ 25,26,47]. These faults are harder to
detectanddebugwhencomparedtotraditionalsoftwaresystems,
as the bugs are often obfuscated within the DNNs. Therefore, it
is important and necessary to diagnose their faults, and provide
actionable /f_ixes. To that end, software engineering research has
recently focused on improving the reliability of DNN-based soft-
ware. For instance, there have been studies on characterizing DNN
bugs [25,26,47], on testing frameworks for deep learning [ 42],
on debugging deep learning using diÔ¨Äerential analysis [ 30], and
/f_ixingDNNs[ 15,46,48].Therearealsoframeworksandtoolsfor
inspecting and detecting unexpected behavior in DNNs. However,
they requirethatspecialistsverify thevisualization,whichis only
availableupon completingthetraining phase[ 3‚Äì5,31,39].
Due to the complexity of using existing frameworks to debug
and localize faults in deep learning software, recent SE research
has introduced techniques for automatically localizing bugs [ 44,
49]. DeepLocalize performs dynamic analysis during training to
localize bugsby monitoring values produced atthe intermediate
nodes of the DNNs [ 44]. If there is a numerical error, then this
approachtracesthatbacktothefaultylayer.DEBAR[ 49]isastatic
analysis tool that detects numerical errors in the DNNs. While
both approaches have signi/f_icantly advanced the state of the art in
5612022 IEEE/ACM 44th International Conference on Software Engineering (ICSE)ICSE‚Äô22,May21‚Äì29,2022,Pi/t_tsburgh,PA,USA MohammadWardat,BrenoDantas Cruz,WeiLe,andHridesh Rajan
debuggingDNNs,theydonotdetectbugsthatmanifestastrends
ofvalues(e.g.vanishinggradient,explodinggradient,accuracynot
increasing)and donotoÔ¨Äerpossible/f_ixes.
We propose DeepDiagnosis (DD), an approach for localizing
faults, reporting error symptoms, diagnosing problems, and pro-
vidingsuggestionsto/f_ixstructural bugsin DNNs.Our approach
introducesthreenewsymptomsofstructuralbugsandde/f_inesnew
rulestomapfault locationtoits rootcause inDNNprograms.We
implemented DD as a dynamic analysis tool and compared and
contrasted it against state-of-the-art approaches. DD outperforms
UMLAUT [ 38] and DeepLocalize [ 44] in terms of eÔ¨Éciency and
AUTOTRAINERintermsofperformance[ 48].Forexample,assume
theunchanged weight symptom , which occurs when the weights in
the network are not changing for several iterations. In that case,
DDwouldidentifytherootcauseasthatthe learningrateistoolow
orthattheoptimizerisincorrect andthenrecommenda/f_ix.
Insummary,this paper makes thefollowing contributions:
‚Ä¢WestudydiÔ¨Äerenttypesofsymptomsandproposeadynamic
analysisfordetectingerrorsandrecommending/f_ixes.
‚Ä¢WeintroducedDeepDiagnosis(DD)thereferenceimplemen-
tationof ourapproach.
‚Ä¢WeevaluatedDDagainstSoTA.We foundthatDDismore
eÔ¨Écient than UMLAUT [ 38] and DeepLocalize [ 44]. Also,
DD has better performance than AUTOTRAINER [ 48].
‚Ä¢We provide a set of 444 models that practitioners can use to
evaluatetheir fault localizationapproaches.
‚Ä¢We make DD available, its source code, evaluation results,
andtheproblem solutions for444buggymodelsat[ 6].
To the best of our knowledge, DeepDiagnosis provides the /f_ixed
location and the concrete /f_ix at the DNN source code level. Our
approach detects problems during the training process, and can
handle a broad class of problems, e.g., compared to DeepLocal-
ize [44], that do not manifest themselves as numerical errors. It
is challenging to provide a correct /f_ix for an observed symptom.
Islamet al.[26] show that solving a single problem may lead to
additional ones. DeepDiagnosis addresses the issue by building the
connectionbetweensymptomstorootcauses.Toobtainsuitable
solutions,weproposea Decision Tree to mapsymptoms to /f_ix.
The rest of the paper is organized as follows: ¬ß 2describes the
motivationofourapproach.¬ß 3describesourdynamicfailuresymp-
toms detection algorithm. ¬ß 4describes the evaluation of our ap-
proach compared with prior works. ¬ß 5discusses the threats to
validity.¬ß 6discussesrelatedworks,and¬ß 7concludesanddiscusses
futurework.
2 A MOTIVATING EXAMPLE
In this section, we motivate our work by providing an example
to illustrate the complexity oflocalizing faults and reporting their
symptomsin DNN programs.
1model = Sequential()
2model.add(Dense(128, 50))
3model.add(Activation( 'relu '))
4model.add(Dropout (0.2))
5model.add(Dense(50, 50))
6model.add(Activation( 'relu '))
7model.add(Dropout (0.2))
8model.add(Dense(50, 1))
9model.add(Activation( 'softmax ' ))10model.compile (loss='binary_crossentropy ' , optimizer=RMSprop() )
11model. fit(X,Y,batch_size ,epoch,validation_data=( X_test ,Y_ test))
Listing1: BadResultfor Simple Model[ 2]
ConsiderthecodesnippetinListing 1fromStackOver/f_low [2]
with an example of a DNN. This model showed erratic behavior
during training and returns bad results. At line 1, the developer
constructedasequentialmodelandaddedadenseinputlayeratline
2 with the activation functions reluspeci/f_ied at line 3. Then the
developer added a dropout layer at lines 4 and 7. Lines 5 and 8 are
densehiddenlayerswiththeactivationfunctions reluandsoftmax
speci/f_iedatlines6and9,respectively.Thedeveloperthencompiled
the model at line 10 and trained it using the fit()function at
line 11. When compiling, the developer must specify additional
properties, such as loss function and optimizer. In this example,
the developer used as loss binary_crossentropy and optimizer
RMSprop() at line 10. Finally, at line 11, the developer speci/f_ies the
trainingdata, batch_size ,epoch,and validation_data .
The developer noticed that the DNN program was providing
badaccuracyandcouldnotdiagnosethe problemnor/f_ixit( Stack
Over/f_low post [2]) while following the Keras MNIST example [ 18].
The main issue with the code in Listing 1is that it handles a
binaryclassi/f_ication problem,andthereforeitshould notusethe
activation function softmax in line 9. As the softmax works for
multi-class classi/f_ications problems. Instead, it should use sigmoid,
as it is the best suited for binary classi/f_ication and will provide the
bestaccuracyforthetask.
Table 1: ResultfromMotivating Example
Approach Ouput
UMLAUT NoOutput
DeepLocalize layer 7:NumericalErrorin delta Weights
AUTOTRAINERsolution,times,issue_list,train_result,describe
1. selu,0,[‚Äôrelu‚Äô],0.5,Using ‚ÄôSeLU‚Äô activationin eachlayers‚Äô
2. bn,0,[‚Äôrelu‚Äô],0.5,Using ‚ÄôBatchNormalization‚Äô
...
Unsolved.. For moredetails [ 6]
DeepDiagnosisLayer 7:NumericalErrorin delta Weights
Change the activation function at layer:8
The current state-of-the-art for DNN fault localization is limited
in terms of s peed,accuracy, and eÔ¨Éciency. Table 1summarizes the
analysis results from three tools (DeepLocalize [ 44], UMLUAT [ 38],
AUTOTRAINER [ 48]) and our approach DeepDiagnosis to diag-
nose the DNN model in Listing 1. To apply UMLUAT for the above
example, we made semantic changes that were validated by the
authors [ 38]. After 104.65 seconds, the training was terminated,
withUMLAUTnotreportinganyproblems.ToapplyDeepLocalize,
we followed the instructions in the GitHub repository [ 7]. DeepLo-
calize prints the following message after 2.14 seconds: ‚ÄúLayer 7:
NumericalErrorindeltaWeights.‚ÄùThismessageindicatesthatthere
isanumericalerrorinthebackpropagationstageduringtraining.
Indicating fault location, butit does not help developersto/f_ix the
problem.ToapplyAUTOTRAINER,wefollowedtheinstructions
in the GitHub repository [ 8]. After performing the training phase,
AUTOTRAINERdidnotsolvetheproblemandtook495.83seconds.
Speci/f_ically,AUTOTRAINERdetectsaDyingReLUsymptom,butit
doesnotprovidethefaultlocation‚Äìwhetheritisinline3or6.AU-
TOTRAINER tries to automatically /f_ix the issue by trying diÔ¨Äerent
562DeepDiagnosis: AutomaticallyDiagnosingFaultsandRecommending Actionable Fixes inDeepLearningPrograms ICSE‚Äô22, May21‚Äì29,2022,Pi/t_tsburgh, PA,USA
'11 0RGHO 
&DOOEDFN$3,
EDFN $
LQ:HLJKW,QLWLDOL]DWLRQ/RFDWLRQWR)L[
,QFRUUHFW$FWLYDWLRQ
/HDUQLQJ5DWH
,PSURSHU'DWD
,QFRUUHFW/RVV
'HWHFW)DLOXUH6\PSWRPV
6DWXUDWHG
$FWLYDWLRQ([SORGLQJ
7HQVRU'HDG1RGH$FFXUDF\1RW
,QFUHDVLQJ
([SORGLQJ
*UDGLHQW8QFKDQJHG
:HLJKW9DQLVKLQJ
*UDGLHQW/RVV1RW
'HFUHDVLQJ
6\PSWRP
3KDVH
/RFDWLRQ
^
(O G L
'110RGHO 
9L K L
7UDLQLQJ
8K G
^
'HWHFWLQJ
Figure 1: Overviewof DeepDiagnosis.
strategies(i.e., substitutingactivationfunctions, adding batchnor-
malization layer, and substituting initializer), which, unfortunately,
areunsuccessful.
OurapproachDeepDiagnosiscorrectlyreportsthefaultlocation
anditssymptomsafter35.03seconds.Also,itprovidesasuggestion
toperforma/f_ixintheformofamessage.Speci/f_ically,DeepDiag-
nosisreportsthatthebugislocatedinthebackpropagationstage
oflayer7atline8.Also,itprintsoutanumericalmessage:‚ÄúError
in delta Weights‚Äù, which indicates the type of the symptom. It also
reportsthattherootcauseistheactivationfunctioninlayer8at
line9(softmax).Finally,itanswersthedeveloper‚Äôsquestion‚Äìthere
is indeed a problem with the activation function in the last layer
andnotin thetraining dataset.
3 APPROACH
Inthissection,weprovideanoverviewofourapproachforfault
localization.Weprovidedescriptionsoffailuresymptomsandtheir
rootcauses.Also,wedescribetheprocessofmappingsymptoms
to their root causes.
Our approach monitors the key values during training, like
weights and gradients. During training, it analyzes the recorded
valuetodetectsymptomsanddeterminewhetheratrainingprob-
lem exists. If a symptom is detected, our approach invokes a De-
cision Tree to diagnose/repair information based on a set of pre-
determinedrules.Otherwise,thetrainingwillterminatewiththe
trained model and reportthemodel iscorrect.
3.1 AnOverview
Figure1shows an overview of our approach for fault localiza-
tion, DeepDiagnosis, and for suggesting locations /f_ix. DD starts
by receiving as input the initial model architecture with a train-
ing dataset and passing our callback method as a parameter to the
fit()method(Figure 1leftcomponent).Keras callbacksarea set
ofmethodsthatenable developerstocheck theirmodel‚Äôsinterme-
diatefeatures(e.g.,weights,gradients).Also,callbacksenablethe
developers to inspect the model‚Äôs behavior during training. Our
callbackapproachisinspiredbypriorwork[ 14,44].Inparticular,
our callbacks allow capturing and recording the key values (i.e.,
weight, gradient, etc.) during feed-forward and backward prop-
agation stages (Figure 1middle component). Then DD applies a
dynamicdetectorduringtrainingtoreportdiÔ¨Äerentsymptomsat
diÔ¨Äerent stages based on error conditions (see Section 3.2for more
details). If DD detects a symptom, it further analyzes the recorded
key values to determine the input model‚Äôs probable location forthe/f_ix(Figure 1rightcomponent).Finally,DDreportsthesymp-
tomtype,whichlayersandstagethesymptomwasdetected,and
suggestsalocation /f_ix.
3.2 Failuresymptoms androot causes
Ourgoalistodetectfailuresymptomsassoonaspossibleduring
development. So that if the model is incorrect, developers would
not have to wait until the end of the training to /f_ind that model
haslowaccuracy,thuswastingcomputationalresources.Tothat
end,wecollected8typesoffailuresymptomsandtheirrootcauses
frompreviouswork intheAI research community [ 23,24,33,40].
We provide more details of each of the symptoms and their root
causesbelow.
3.2.1Symptom#1DeadNode .TheDeadNodesymptomtakes
place when most of a neural network is inactive. For example,
assume that most of the neurons of a DNN are using the ReLU
activationfunction,whichreturnszerowhenreceivinganynega-
tive input. If the majority of the neurons receive negative values
(e.g., due to a high learning rate), they would become inactive and
incapableofdiscriminatingtheinput.TheDNNwouldendupwith
poor performance [ 48]. To identify this symptom, we compute the
percentage of inactive neurons per layer. If the majority of the
neuralnetwork isinactive,thenwecallit Dead Node.
Root Causes: This problem is likely to occur when [ 16]: (1)
learningrateistoohigh/low.(2)thereisalargenegativebias.(3)
improper weight orbiasinitialization.
3.2.2Symptom #2 Saturated Activation .The Saturated Acti-
vationsymptomtakesplacewhentheinputtothelogisticactivation
function(e.g., tanhorsigmoid)reachedeitheraverylargeoravery
small value [ 23]. At the saturated point, the function results would
equal zero or be close to zero, thus leading to no weight updates.
Ourexperimentsshow[ 6]thatthebehaviorof sigmoidandtanh
have a minimum saturated point at x=-5 and a maximum saturated
point at x=5. Previous work showed that the saturated function
aÔ¨Äects the network‚Äôs performance and makes the network diÔ¨Écult
to train[23,45].
RootCauses: This problem is likely to occur when [ 19]: (1) the
inputdataaretoolargeortoosmall;(2)improperweightorbias
initialization;(3) learningrateistoo high ortoo small.
3.2.3Symptom #3 Exploding Tensor: The Exploding Tensor
symptomtakesplacewhenthetensors‚Äôvaluesbecometoolarge,
leadingtonumericalerrorsinafeed-forwardstage. Forexample, if
563ICSE‚Äô22,May21‚Äì29,2022,Pi/t_tsburgh,PA,USA MohammadWardat,BrenoDantas Cruz,WeiLe,andHridesh Rajan
Table 2: Methodsfor DetectingFailure Symptoms
ID Method Name Input Output Description
S1 ExplodingTensor() Weight, /uni0394
Weight, and
LayerOutputT|FTheproceduredetectsanynumericalerrorsuchasin/f_inite,NaN(NotaNumber),orzero.Tothatend,itcomputes
the input‚Äôs mean value. Then, it checks for a numerical error is detected. In case of error, it returns True,
otherwise False.
S2 UnchangeWeight () Weight, /uni0394
Weight,Layer
OutputT|FThe procedure stores the value for a given numberof steps (N= 5). Then it compares the value for the current
step with the mean value stored in for previous (N = 5) steps. The evaluation takes place for every given number
ofsteps.Theprocedure returns Trueif the valueis not changing, otherwise False.
S3SaturatedActivation() Input of Ac-
tivation Func-
tionT|FThe procedure detects if the tanh or sigmoid activation functions, or other logistic functions are becoming
saturated. It does so by checking if their input has reached either a maximum or minimum value. Saturated
functions‚Äô derivatives would be equal to zero at those points. The procedure counts the activity of a close or
greater node than to the (Max_Threshold = 5) or less than (Min_Threshold = -5) of the activation function; If the
percentage of total activity nodes is greater than the (Threshold_Layer = 0.5) percent of the nodes are saturated,
theprocedure returns True,otherwise False.
S4 DeadNode() ReluOutput T|FThis procedure takes the output of Recti/f_ied Linear Unit (ReLU) activation function as input, then computes
howmanyinactivenodesdroppedbelow(Threshold=0.0).Ifthepercentageofinactivenodesisgreaterthan
(Layer_Threshold= 0.7)it returns True,otherwise False.
S5 OutofRange() Outputoflast
layerT|FThe procedure detects if the activation function‚Äôs output is becoming out of range for the labeling training
dataset Y. To that end, it /f_inds the range (maximum and minimum) of the activation function‚Äôs output. Then
compareitwithYlabelingdata.Ifthevalueisoutoftheboundary,theprocedurereturns True,otherwise False.
S6LossNotDecreasing () LossValue T|FThe procedure stores the loss value for every number of steps (N = 5), then compares the loss value for the
currentstepwiththemeanvalueoflossesstoredintheprevious(N=5)steps.Theevaluationhappensforevery
numberofsteps (N= 5).Theprocedure returns Trueif the lossis not decreasing,otherwise False.
S7AccuracyNotIncreasing() Accuracy
ValueT|FTheprocedurestorestheaccuracyvalueforeverynumberofsteps(N=5),thencomparestheaccuracyvalue
for the current step with the mean value of accuracy stored in previous (N = 5) steps. The evaluation happened
every numberofsteps (N= 5).Theprocedure returns Trueif accuracyis not increasing,otherwise False.
S8 VanishingGradient() DeltaWeight T|FThis procedure detects the Vanishing Gradient problem by checking the gradients when they become extremely
smallor drop to zero.Theprocedure computesthe mean ofthe gradients‚Äô absolutevalues, thenchecksif their
means drop below a speci/f_ied (Threshold = 0.0000001). In the case of a positive detection, it returns True,
otherwise False.
Thistableshows proceduresdescriptions from [ 1,10,20,37,48].T|F indicatesthat theprocedurereturnsTrue |Falserespectively.
Table 3: Methodsfor Mapping fromFailure Symptoms to LocationFix
No Method Name Input Output Description
C1 ImproperData() TrainingData T|FCheckifthemaximumandminimumvalueoftrainingdatasetliewithinspeci/f_icrangeof[-1,1].Ifthevalue
withintheboundary,theprocedure returns True.Otherwise,False.
C2WeightInitialization () Weight for
eachlayerT|FThisprocedurechecksthevarianceofweightinputsacrosslayerstodetermineifaneuralnetworkhasbeenpoorly
initialized. The procedure checks if the variance of weights per layer is equal or very close to 0 (Min_Threshold
=0.00001),orifitexceedsthe(Min_Threshold =10),theprocedure returns True.Otherwise, False.
C3 TuneLearn () Learning rate,
Weight, and /uni0394
WeightL|HThe procedure evaluates the learning rate heuristically by computing the ratio of the norm of the gradient
weight to the norm of weight for each layer. This ratio should be somewhere around (Learn_Threshold =
1e-3).Ifitislowerthan(Learn_Threshold=1e-3),thenthelearningratemightbetoo Low.Ifitishigherthan
(Learn_Threshold=1e-3),thelearningrateis likely too High.
This tableis showing all the functionality of the procedures. T |F indicates the procedure returnTrue |False respectively. L |H indicates the procedure return
Low|Highrespectively. Weborrowed thesemethods from existing literature[ 1,10,20,37,48]
theweightoroutputlayergrowsexponentiallymorethanexpected,
becoming either in/f_inite or NaN(not a number). Eventually, this
problem causes a numerical error, making it impossible for the
model to learn.
RootCauses: This problem is likely to occur when [ 20,28]: (1)
the learning rate is too large; (2) there exist improper weight or
biasinitialization,orimproper inputdata.
3.2.4Symptom #4 Accuracy Not Increasing &Symptom #5
LossNotDecreasing .BothsymptomsAccuracyNotIncreasing
and Loss Not Decreasing are very similar. The Accuracy Not In-
creasingsymptomtakesplacewhentheaccuracyofatargetmodel
is not increasing for N steps, but instead, it is decreasing or /f_luctu-
ating during training. While for the Loss Not Decreasing symptom,
the loss metric is the one that is not decreasing for N steps but
is/f_luctuating.Thesebehaviorsindicatethatthenetworkwillnot
achievehighperformance.Thesesymptomsareoftencausedbythe
incorrectselectionofDNNhyperparameters[ 37],suchaslossfunc-
tion, activation function for the last layer, learning rate, optimizer,
orbatch size.RootCauses: This problem is likely to occur when [ 20,28]: (1)
thereexistimpropertrainingdata;(2)thenumberoflayersistoo
large/small;and(3)thelearningrateisveryhigh/low;and(4)there
existincorrectactivation functions.
3.2.5Symptom#6UnchangedWeight .TheUnchangedWeight
symptom takes place when the DNN weights do not have a no-
ticeablein/f_luenceontheoutputlayers.Thisbehaviorleadstoun-
changing parameters andnetworkstacks, whichfurther prevents
themodel from learning[ 19,44].
RootCauses: This problem is likely to occur when [ 19,44]: (1)
learning rate is very low; (2) the optimizer is incorrect; (3) there
existincorrectweights initialization;and(4)there existsincorrect
loss/activationatthelast layer.
3.2.6Symptom#7ExplodingGradient .Thisproblemoccurs
during the back-propagation stage. In it, gradients are growing
exponentially from the last layer to the input layer, which leads to
non-/f_inite values, either in/f_inite or NaN(not a number). This issue
makes learning unstable and sometimes even impossible. Conse-
quently, updating the weights becomes very hard, and the training
model ends up withahigh loss orvery low accuracyvalues.
564DeepDiagnosis: AutomaticallyDiagnosingFaultsandRecommending Actionable Fixes inDeepLearningPrograms ICSE‚Äô22, May21‚Äì29,2022,Pi/t_tsburgh, PA,USA
RootCauses: This problem is likely to occur when [ 20,28]: (1)
the learning rate is very high; (2) there is an improper weight or
bias initialization; (3) there are improper data input; and (4) the
batchsizeisvery large.
3.2.7Symptom#8 Vanishing Gradient .The Vanishing Gradi-
entproblemoccursduringthebackwardstage.Whencomputing
the gradient of the loss concerning weights using partial deriva-
tives, the value of the gradient turns out to be so small or drops
to zero. The problem causes major diÔ¨Éculty ifit reachesthe input
layer,whichwillpreventtheweightfromchangingitsvalueduring
training. Since the gradients control how much the network learns
during training, the neural network will end up without contribut-
ingto thepredictiontask orleadingto poorperformance[ 41,48].
RootCauses: This problem is likely to occur when [ 29]: (1) the
network has too many layers; (2) the learning rate is low; (3) the
hiddenlayersimproperlyused TanhorSigmoid;and(4)thereexists
theincorrectweight initializationproblem.
3.3 Detecting FailureSymptoms
In Table2from Method S1 to S8, we describe the failure symptoms
discussedinSection 3.2,usingitsname,input/output,andthede-
tection procedure. Algorithm 1shows an example of a dynamic
analysis procedure, which DeepDiagnosis uses to detect failure
symptoms during training (Table 2Description ). Also, the Algo-
rithm1reports failure locations, such as in which layer and phases
(i.e.,feed-forwardandbackwardpropagation).Incaseafailureis
detected, the algorithm will trigger the Mapping() procedure to
identify the location in the original DNN source code. By doing so,
it willlocalizethebugand determinetheoptimal /f_ix.
Atline1,Algorithm 1iteratesoverthetrainingepochs,withthe
trainingdatasetdivided intobatches.Line3showsthedivisionof
the training dataset into a mini-batch. On lines 2-28, the algorithm
runs one batch of the training dataset before updating the internal
model parameters. The neural network can be divided into two
stages:First,theforwardstage,inwhichthealgorithmperforms
dynamic analysis and symptom detection, including Numerical
Error,Deadnode,SaturatedActivation,andOut ofRange,at lines
4-12. Second, the backward stage, in which the algorithm performs
dynamicanalysistodetectadditionalsymptoms,suchasNumerical
Error,VanishingGradient, and Unchanged weight atlines23-28.
3.3.1 Feed-forward stage. At lines 5 & 6 of the Algorithm 1,i t
computestheoutputofafeed-forwardbeforeandafterapplying
theactivationfunction.Atline7,itinvokesthe ExplodingTensor()
procedure (S1 in Table 2) to determine if the output contains a nu-
merical error obtained from the output value before/after applying
activation function, respectively. If there is an error, the algorithm
reportstheNSmessageasshowninTable 5.Next,itinvokesthe
Mapping()procedurefromthedecisiontreeinFigure 2byproviding
thesymptom(NS),location,stage(FW),andlayer(L).Thedecision
tree returns the best actionable /f_ix for the model (see Section 3.4
formoredetails).
Atline8,theAlgorithm 1invokesthe UnchangeWeight() (S2in
Table2)proceduretodetectwhethertheoutputbefore/afterapply-
ingtheactivationfunctionisnolongerchangingacrosssteps.Ifthe
procedureindicatesthatthevaluedoesnotchangeforNiterations,wefollow[ 44]andsetN=5.The UnchangeWeight() procedurecan
be applied either to the output before/after the activation function.
ThealgorithmreportsthemessageUCS,asshowninTable 5.Atline
9, the Algorithm invokes the SaturatedActivation () procedure (S3
in Table2)forthelayer thathasalogistic activation function(i.e.,
tanhorsigmoid)todetermineifthelayerisbecomingsaturated.
Thisproceduretakestwoarguments,thevaluebeforeapplying the
activationfunction(V_1)andthenameoftheactivation function
(V_2.name). If the procedure determines that the layer is saturated,
thealgorithmreports themessageSAS as shown in Table 5.
Atline10,theAlgorithm 1invokesthe DeadNode() procedure
(S4 in Table 2) to check the layers that use the Recti/f_ied Linear
Unit (ReLU) activation function. The goal is to determine if the
output after applying the activation function has dropped below
a threshold [ 48]. This procedure is invoked only after applying
the activation function. The algorithm reports the message DNS
as shown in Table 5when the error is detected. Similarly, at line
11, it invokes the OutofRange() procedure (S5 in Table 2) in the
last layer. The goal is to determine if thedeveloper has chosen the
correctactivationfunction.ThealgorithmreportsthemessageORS
as shown in Table 5if theerrorisdetected.
In lines 13 & 15 the algorithm interprets and validates how well
the model is doing by computing the loss and accuracy metrics,
respectively.Thenitdeterminesifthereisanynumericalerrorin
those metrics at lines 14& 16, respectively. Thealgorithm invokes
LossNotDecreasing() andAccuracyNotIncreasing() (S6 & S7 in Ta-
ble2)tocheckifthelossortheaccuracyhasnotchangedforalong
time.Inbothcases,thealgorithmreportsamessageLNDSorANIS
as shown in Table 5.
3.3.2 Backpropagationstage. Duringthisstage,theAlgorithm 1
computes the gradient of loss function /uni0394Weight for the weight
bychainrulesineachiteration.Atline24,thealgorithminvokes
Backward() toapply stochasticgradientdescent, andthisfunction
returns the Weight and /uni0394Weight in each iteration. At line 25,
the algorithm invokes the VanishingGradient() procedure (S8 in
Table2) and passes /uni0394Weight to check if the gradients become
extremely small or close to being zero. In the same way, at line
26, the algorithm can determine if there is a numerical error in
the Weight or the gradient weight in each layer by invoking the
ExplodingTensor() procedure(S1 inTable 2).Thebackpropagation
algorithmworksiftheWeightisupdatedusingthegradientmethod
and the loss value keeps reducing, to check if the backpropagation
works eÔ¨Äectively. In the backward propagation, we also invoke
theUnchangeWeight() procedure(S2inTable 2)todetectwhether
theweightor /uni0394Weightisnolongerchangingacrosssteps.Ifany
procedure decides that there is an issue, then the algorithm will
return a message to indicate the type of symptom as shown in
Table5, L represents a faulty layer number. Then the algorithm
invokesMapping() and passes the symptom, location, and layer
to /f_ind the best actionable change to /f_ix the issue in the model.
Finally, if the algorithm did not detect any type of symptom, it will
terminateafter/f_inishingthetrainingatline29andprintamessage
indicatingthatthereisno issue in themodel (CM).
565ICSE‚Äô22,May21‚Äì29,2022,Pi/t_tsburgh,PA,USA MohammadWardat,BrenoDantas Cruz,WeiLe,andHridesh Rajan
Table 4: Abbreviation of Actionable Changes
NoMessageGuideline Abbreviation
1Improper Data MSG0
2Changethelossfunction MSG1
3Changetheactivationfunction MSG2
4Changethelearningrate MSG3
5Changetheinitializationof weight MSG4
6Changethelayer number MSG5
7Changetheoptimizer MSG6
Table 5: Abbreviation of Failure Symptoms
NoSymptoms Abbreviation
1NumericalErrors NS
2Unchanged weight UCS
3Saturated Activation SAS
4DeadNode DNS
5Outof Range ORS
6LossNotDecreasing LNDS
7Accuracy Not Increasing ANIS
8VanishingGradient VGS
9InvalidLoss ILS
10InvalidAccuracy IAS
11CorrectModel CM
3.4 MappingSymptomsto Location /f_ix
DecisionTree: Themaingoalofthisstepistomitigatemanual
eÔ¨Äortandreducethetimefordiagnosing and/f_ixingbugs.Tothat
end,theMapping() procedureinAlgorithm1provides/f_ixsugges-
tions based on the detected failure symptoms. Figure 2shows a
representationoftheDecisionTreewhichthe Mapping() procedure
usesto providea/f_ix recommendation.
The Decision Tree consists of 24 rules, which corresponds to
decision paths. Each rule provides a mapping from failure symp-
tomsanddetectedlocationstoactionablechanges.Thetreede/f_ines
a binary classi/f_ication rule which maps instances in the format
problem (Symptom, Location, Layer) into one of seven classes of
changes (Table 4). In the decision tree, the root node represents
the problem, orange nodes the symptoms, blue nodes the locations,
gray nodes the layer type, green nodes, the conditions, and red
nodes theactionablechanges. Table 3showsthemethods Data(),
Weight() andLearn(), which are used to compute the conditions.
EachDecisionTreeinstancemapsapathfromtheroottooneof
theleaves.
Forexample,assumethatadeveloperwantstocheckthecode
in Listing 1. To that end, the developers can use the Algorithm 1to
verifythemodel.Thealgorithminvokesthe Mapping() procedure
(line26)bypassingthesymptomNS,location,stageBW(backward),
and layer (7). This procedure traverses the path under the NS node
intheDecisionTree(Figure 2).Sincetheproblemoccurredinthe
BWstage,thealgorithmtakestherightpathtosatisfythecondition.
Then,itveri/f_iesthelayertype(7).Sinceit/f_indsanissueinthelayer,
theprocedure returns the message MSG2 ‚Äì Changethe activation
function(Table 4).
Heuristics: We developed a set of heuristics based on the root
causes (see Section 3.2). There are three main root causes: (1) Data
Preparation; (2) Parameter Tuning; and (3) Model Architecture. ForAlgorithm1: FailureSymptomsDetection
input :Training data(input,label), DNN program
output:Failuresymptoms andlocations(layers, iterations, epoch)
1for/u1D452‚Üê0to/u1D452/u1D45D/u1D45C/u1D450/uni210E/u1D460do
2for/u1D456‚Üê0to/u1D43F/u1D452/u1D45B/u1D454/u1D461/uni210E(/u1D456/u1D45B/u1D45D/u1D462/u1D461)Step/u1D44F/u1D44E/u1D461/u1D450/uni210E/u1D460/u1D456/u1D467/u1D452 do
3/u1D44B‚Üê/u1D456/u1D45B/u1D45D/u1D462/u1D461[/u1D456];/u1D44C‚Üê/u1D459/u1D44E/u1D44F/u1D452/u1D459[/u1D456]
4 for/u1D43F‚Üê0to/u1D43F/u1D452/u1D45B/u1D454/u1D461/uni210E(/u1D43F/u1D44E/u1D466.alt/u1D452/u1D45F/u1D460)do
5 /u1D4491‚Üê/u1D43F/u1D44E/u1D466.alt/u1D452/u1D45F[/u1D43F]./u1D439/u1D45C/u1D45F/u1D464/u1D44E/u1D45F/u1D451 (/u1D44B)
6 /u1D4492=/u1D43F/u1D44E/u1D466.alt/u1D452/u1D45F[/u1D43F]./u1D434/u1D450/u1D461/u1D456/u1D463/u1D44E/u1D461/u1D456/u1D45C/u1D45B (/u1D4491)
7 if/u1D438/u1D465/u1D45D/u1D459/u1D45C/u1D451/u1D456/u1D45B/u1D454/u1D447/u1D452/u1D45B/u1D460/u1D45C/u1D45F (/u1D4492|/u1D4491)then return NS,
/u1D440/u1D44E/u1D45D/u1D45D/u1D456/u1D45B/u1D454 (/u1D441/u1D446,/u1D439/u1D44A,/u1D43F )
8 if/u1D448/u1D45B/u1D450/uni210E/u1D44E/u1D45B/u1D454/u1D452/u1D44A/u1D452/u1D456/u1D454/uni210E/u1D461 (/u1D4492|/u1D4491)then return UCS,
/u1D440/u1D44E/u1D45D/u1D45D/u1D456/u1D45B/u1D454 (/u1D448/u1D436/u1D446,/u1D439/u1D44A,/u1D43F )
9 if/u1D446/u1D44E/u1D461/u1D462/u1D45F/u1D44E/u1D461/u1D452/u1D451 (/u1D4491,/u1D4492./u1D45B/u1D44E/u1D45A/u1D452)then return SAS,
/u1D440/u1D44E/u1D45D/u1D45D/u1D456/u1D45B/u1D454 (/u1D446/u1D434/u1D446,/u1D439/u1D44A,/u1D43F )
10 if/u1D437/u1D452/u1D44E/u1D451/u1D441/u1D45C/u1D451/u1D452 (/u1D4492)then return DNS,
/u1D440/u1D44E/u1D45D/u1D45D/u1D456/u1D45B/u1D454 (/u1D437/u1D441/u1D446,/u1D439/u1D44A,/u1D43F )
11 if/u1D442/u1D462/u1D461/u1D45C/u1D453/u1D445/u1D44E/u1D45B/u1D454/u1D452 (/u1D4492,/u1D44C)&&/u1D43F==/u1D43F/u1D44E/u1D460/u1D461then return
ORS,/u1D440/u1D44E/u1D45D/u1D45D/u1D456/u1D45B/u1D454 (/u1D442/u1D445/u1D446,/u1D439/u1D44A,/u1D43F )
12 /u1D44B‚Üê/u1D4492
13/u1D43F/u1D45C/u1D460/u1D460‚Üê/u1D436/u1D45C/u1D45A/u1D45D/u1D462/u1D461/u1D452/u1D43F/u1D45C/u1D460/u1D460 (/u1D4492,/u1D44C)
14 if/u1D43F/u1D45C/u1D460/u1D460is equal to NaN OR /u1D456/u1D45B/u1D453then return ILS,
/u1D440/u1D44E/u1D45D/u1D45D/u1D456/u1D45B/u1D454 (/u1D43C/u1D43F/u1D446)
15/u1D434/u1D450/u1D450/u1D462/u1D45F/u1D44E/u1D450/u1D466.alt ‚Üê/u1D436/u1D45C/u1D45A/u1D45D/u1D462/u1D461/u1D452/u1D434/u1D450/u1D450/u1D462/u1D45F/u1D44E/u1D450/u1D466.alt (/u1D4492,/u1D44C)
16 if/u1D434/u1D450/u1D450/u1D462/u1D45F/u1D44E/u1D450/u1D466.alt is equal to NaN OR inf OR 0then
17 returnIAS,/u1D440/u1D44E/u1D45D/u1D45D/u1D456/u1D45B/u1D454 (/u1D43C/u1D434/u1D446)
18 if/u1D43F/u1D45C/u1D460/u1D460/u1D441/u1D45C/u1D461/u1D437/u1D452/u1D450/u1D45F/u1D452/u1D44E/u1D460/u1D456/u1D45B/u1D454 (/u1D43F/u1D45C/u1D460/u1D460)then
19 returnLNDS,/u1D440/u1D44E/u1D45D/u1D45D/u1D456/u1D45B/u1D454 (/u1D43F/u1D441/u1D437/u1D446)
20 if/u1D434/u1D450/u1D450/u1D462/u1D45F/u1D44E/u1D450/u1D466.alt/u1D441/u1D45C/u1D461/u1D43C/u1D45B/u1D450/u1D45F/u1D452/u1D44E/u1D460/u1D456/u1D45B/u1D454 (/u1D434/u1D450/u1D450/u1D462/u1D45F/u1D44E/u1D450/u1D466.alt )then
21 returnANIS,/u1D440/u1D44E/u1D45D/u1D45D/u1D456/u1D45B/u1D454 (/u1D434/u1D441/u1D43C/u1D446)
22/u1D451/u1D466.alt‚Üê/u1D44C
23 for/u1D43F‚Üê/u1D43F/u1D452/u1D45B/u1D454/u1D461/uni210E(/u1D43F/u1D44E/u1D466.alt/u1D452/u1D45F/u1D460)to0do
24 /u1D4493,/u1D44A[/u1D43F]‚Üê/u1D43F/u1D44E/u1D466.alt/u1D452/u1D45F[/u1D43F]./u1D435/u1D44E/u1D450/u1D458/u1D464/u1D44E/u1D45F/u1D451 (/u1D451/u1D466.alt)
25 if/u1D449/u1D44E/u1D45B/u1D456/u1D460/uni210E/u1D456/u1D45B/u1D454/u1D43A/u1D45F/u1D44E/u1D451/u1D456/u1D452/u1D45B/u1D461 (/u1D44A[/u1D43F])then return VGS,
/u1D440/u1D44E/u1D45D/u1D45D/u1D456/u1D45B/u1D454 (/u1D449/u1D43A/u1D446,/u1D435/u1D44A,/u1D43F )
26 if/u1D438/u1D465/u1D45D/u1D459/u1D45C/u1D451/u1D456/u1D45B/u1D454/u1D447/u1D452/u1D45B/u1D460/u1D45C/u1D45F (/u1D4493|/u1D44A[/u1D43F])then return NS,
/u1D440/u1D44E/u1D45D/u1D45D/u1D456/u1D45B/u1D454 (/u1D441/u1D446,/u1D4493|/u1D437/u1D44A,/u1D43F)
27 if/u1D448/u1D45B/u1D450/uni210E/u1D44E/u1D45B/u1D454/u1D452/u1D44A/u1D452/u1D456/u1D454/uni210E/u1D461 (/u1D4493|/u1D44A[/u1D43F])then return UCS,
/u1D440/u1D44E/u1D45D/u1D45D/u1D456/u1D45B/u1D454 (/u1D448/u1D436/u1D446,/u1D449 3|/u1D437/u1D44A,/u1D43F)
28 /u1D451/u1D466.alt‚Üê/u1D4493
29returnCM
DataPreparation,thealgorithmchecksifthedataisnormalized(C1
-ImproperData() inTable3).ForParameterTuning,ourapproach
checksifthehyperparameters(suchaslearningrate)wereassigned
correctly. Also, to check if the weights were initialized correctly,
thealgorithminvokesthe WeightInitialization() .TheTuneLearn()
procedureveri/f_ieswhetherthelearningrateisveryhighorvery
low (C2 and C3 in Table 3, respectively). For model architecture,
the algorithm searches for a relation between the location and the
stage of the symptom. It performs this step to pinpoint which APIs
arebeingmisused in themodel (e.g.,loss, activation function).
Wecollectedtherootcausesforeachsymptomfromprevious
work [23,24,33,40] (more details in Section 3.2). To arrive at a
possible/f_ixforagivensymptom,wechoosethemostfrequentroot
566DeepDiagnosis: AutomaticallyDiagnosingFaultsandRecommending Actionable Fixes inDeepLearningPrograms ICSE‚Äô22, May21‚Äì29,2022,Pi/t_tsburgh, PA,USA
cause. We follow this approach as our /f_indings show that changes
in the order we check for the possible root causes do not aÔ¨Äect the
results,onlyonthetotaltimeto arriveatasolution.For example,
assume that a model has the Dead Node symptom. In terms of
frequency, improper data tends to happen more often than weight
andlearningrate.Ifthethreerootcausesarecorrect,ourapproach
checksthemodelarchitecture,whichistheleastcommoninthis
case. Thus, arrivingat animproperactivation functionastheroot
causeof this symptom.
4 EVALUATION
Intheevaluation, weanswer thefollowing researchquestions:
‚Ä¢RQ1(Validation):Canourtechniquelocalizethefaultsand
reportthesymptomsinDeepLearningprogramseÔ¨Äectively?
‚Ä¢RQ2(Comparison):Howdoesourtechniqueforfaultlocal-
izationcomparedtoexistingmethodologiesintermsoftime
andeÔ¨Äectiveness?
‚Ä¢RQ3 (Limitation): In which cases do our technique fail to
localizethefaultsand reportthesymptoms?
‚Ä¢RQ4 (Ablation): To what extent does each type of symp-
tomwedeveloped contribute to theoverallperformanceof
DeepDiagnosis?
4.1 Experimentalsetup
4.1.1 Implementation. WeimplementedDeepDiagnosisontopof
Keras2.2.0 [18] andTensorFlow 2.1.0 [32]. Also, we implemented
Algorithm 1by overriding the method called (on_epoch_end(epoch,
logs=None) .FortheDecisionTreeinFigure 2,weimplementedit
as a decision rule consisting of a set of conditional statements. The
overridden method invokesthe decision tree onceupon detecting
a symptom. Then it passes the symptom type as a parameter for
thedecisiontree.
Weconductedalltheexperimentsonacomputerwit ha4GHz
Quad-Core Intel Core i7 processor and 32 GB 1867 MHz DDR3 GB
of RAM runningthe64-bit iMac version 4.11.
4.1.2 Benchmark. In total, we collected 548 models from prior
work [7,38,48]. From these, we removed 104 RNN models, as
our approach does not support them. The resulting 444 models
are composed of 53, which are known to have bugs from [7,38].
We refer to these 53 models SGS benchmark as they come from
StackOver/f_low, GitHub,and Schoop etal. [38].Also,the391models
from[48]areinthecompiled*.h5format.Theremaining391models
are divided into two sets. In particular, the /f_irst with 188 correct
models ‚Äì without any known bugs ‚Äì and the second with 203
buggymodels‚Äì withbugs.
Mostmachine learning developers share the sourcecode or the
trainedweightsoftheirmodelsin*.h5format.Toallowothersto
improve the understanding of how a model operates and inspect it
withnewdata,weimplementedthe Extractor tool[6].Itextracts
theDNNsourcecodefroma*.h5/f_ile.The Extractor followsthree
stepstogeneratetheKerassourcecode:/f_irst,itsavesthemodel‚Äôs
layer information to a JSON /f_ile. Then, it generates the Abstract
Syntax Tree (AST)from the JSON/f_ile. Finally, it converts the AST
to thesource code.
To build the ground truth for the SGS benchmark, we manu-
allyreviewedallmodelsandtheirrespectiveanswersfrom StackOver/f_low andcommitsfrom GitHub. Weperformthis veri/f_ication
processtodeterminetheexactbuglocationanditsrootcauses.For
the remaining 391 models - 203 buggymodels and 188 not buggy
models, we usedour Extractor togenerate thesourcecode for each
modelbefore/afterperforminga/f_ix;weusedthediÔ¨Ñib[ 22]module
to generate the diÔ¨Ä /f_ile from the /f_ixed model. We use the diÔ¨Ä to
locate the changes in the model, thus locating the root causes and
the actual location of its corresponding /f_ix. We consider a model
successfullyrepaired if itsaccuracyhasimproved after the /f_ix.
4.1.3 Results Representation. Table6shows the summarized eval-
uation results of the following approaches: UMLUAT [ 38], DeepLo-
calize[44],AUTOTRAINER[ 48],andourapproachDeepDiagnosis.
Pleaserefertothereproducibilityrepository[ 6]forthecomplete
table.The/f_irstcolumn shows thesource ofthe buggymodel.The
secondcolumnliststhemodelID.Thethirdcolumnprovidedthe
Stack Over/f_low post # and the model name from GitHub reposi-
tories, collected by Wardat et al.[44], and also the name of the
modelintroducedbySchoop etal.[8],respectively.Tocompareour
approach with the results generated from previous approaches, we
reportedtheresultsinthefollowingcolumns(fromlefttoright):
Time,IdentifyBug(IB),FaultLocalization(FL),FailureSymptom
(FS), and Location Fix (LF), and Ablation (AB). Time, in seconds,
reports how long each tool takes to report its results. The columns
Identify Bug (IB) and Fault Localization (FL) show whether the
approach successfullyidenti/f_ies thebugand the faultlocation. Fail-
ure Symptom (FS) and Location Fix (LF) columns show whether
the tool correctly reports a symptom and an actionable change
(model repair /f_ix). Finally, the Ablation (AB) column shows which
of the procedures listed in Table 2detects the failure symptoms.
Under each approach, the ‚ÄúYes‚Äù and ‚ÄúNo‚Äù status indicates whether
ithassuccessfullyreportedthetargetproblem.Also,the‚Äú‚Äî‚Äùstatus
denotes if the approach does not yet support the target problem.
Lastly, we use method ID in Table 2to indicate which procedure is
used to detectthefailuresymptom.
Table7summarizes the analysis results from four approaches
usingbenchmarkscollectedfromthreediÔ¨Äerentsources[ 38,44,48].
The second column (Total) lists the total number of buggy mod-
els for each dataset. To compare our approach with previous ap-
proaches,wereportedTime,inseconds,theaveragetimeeachtool
takes to report its results for each dataset. To mitigate randomness
during the training model, we followed the procedure described
in[48]andraneachmodel5times.ThecolumnIdentifyBug(IB)
shows how many each approach successfully identi/f_ies the bug
from each dataset. Our approach is capable of handling eight types
of symptoms with diÔ¨Äerent types of datasets usingdiÔ¨Äerent types
of model architectures. Table 8shows the number of symptoms
detected from diÔ¨Äerent types of datasets.
4.2 RQ1 (Validation)andRQ2 (Comparison)
Table6and7show theevaluation results forRQ1and RQ2.
DeepDiagnosis (DD)hascorrectlyidenti/f_ied46outof53buggy
modelsfromtheSGSbenchmark.DDcorrectlyreportedthefault
location for 34 models and the failure symptoms for 37 models.
Also,DDcorrectlyidenti/f_iedtheactionablechangesfor28outof
53 faulty models. Lastly, DD identi/f_ied 138 out of the 203 buggy
567ICSE‚Äô22,May21‚Äì29,2022,Pi/t_tsburgh,PA,USA MohammadWardat,BrenoDantas Cruz,WeiLe,andHridesh Rajan
Figure 2: Mapping Symptoms to FixLocation
Table 6: Comparing the Results from UMLAUT (UM), DeepLocalize (DL), AUTOTRAINER (AT) and DeepDiagnosis (DD)
NoPost#Time IdentifyBug(IB) FaultLocalization (FL) FailureSymptom (FS) LocationFix(LF) AB
UM DL AT DDUMDLATDDUMDLATDDUMDLATDDUMDLATDD
StackOverflow[7]148385830 0.39 2.14103.91 8.27YesYesYesYesYesYes‚ÄîYesYesYesYesYesYesNoNoYes#1
244164749 188.61 111.56 197.90 242.34 NoYesNoNoNoYes‚ÄîNoNoYesNoNoNoNoNoNo‚Äî
331556268 ‚Äî 1.2 ‚Äî12.48 ‚ÄîYes‚ÄîYes‚ÄîNo‚ÄîYes‚ÄîYes‚ÄîYes‚ÄîNo‚ÄîYes#7
450306988 1.93.5793.60 1.75NoYesYesYesNoYes‚ÄîYesNoYesYesYesNoNoYesYes#1
548251943 ‚Äî706.83 ‚Äî1.61‚ÄîNo‚ÄîYes‚ÄîNo‚ÄîYes‚ÄîNo‚ÄîYes‚ÄîNo‚ÄîYes#5
638648195 5.425.92 85.38 15.12YesYesNoYesNoYes‚ÄìYesNoYesNoYesNoNoNoYes#1
GitHub[7]7GH #1 128.67 11.806524.21 44.90YesYesYesYesNoNo‚ÄîNoNoNoYesNoNoNoYesNo#1
8GH #2 ‚Äî8432.06 ‚Äî1001.40 ‚ÄîNo‚ÄîNo‚ÄîNo‚ÄîNo‚ÄîNo‚ÄîNo‚ÄîNo‚ÄîNo‚Äì
9GH #3 ‚Äî31.69 ‚Äî2.17‚ÄîYes‚ÄîYes‚ÄîYes‚ÄîYes‚ÄîNo‚ÄîYes‚ÄîNo‚ÄîYes#5
10GH #4 36.58102.44 315.61 102.96 YesYesYesYesNoNo‚ÄîNoYesNoYesNoNoNoYesNo#4
11GH #5 18.95164.70 173.92 140.58 YesYesNoYesNoYes‚ÄîYesNoYesNoYesNoNoNoNo#2
12GH #6 ‚Äî9568.09 12.57118.59 ‚ÄîNoNoNo‚ÄîNo‚ÄîNo‚ÄîNonoNo‚ÄîNoNoNo‚Äì
Schoopetal.[8]13A1 (C-10) 1.7718.39 43.96 2.75YesYesYesYesYesYes‚ÄîYesYesYesNoYesYesNoNoYes#5
14A2 (C-10) 1.5044.93 18.36 10.44YesYesNoYesNoYes‚ÄîYesYesYesNoNoYesNoNoNo#1
15A3 (C-10) 348.88 44.89119.54 5.03YesYesYesYesNoNo‚ÄîNoYesNoNoYesYesNoNoNo#1
16B1(C-10) 347.21 10.65 80.38 2.17YesYesYesYesNoNo‚ÄîNoYesYesYesYesYesNoNoNo#1
17B2(C-10) 3.4245.02 16.90 5.44YesYesNoYesNoYes‚ÄîYesYesNoNoYesYesNoNoYes#1
18B3(C-10) 1605.99 45.54 15.49 15.49YesYesNoNoNoNo‚ÄîNoYesNoNoNoYesNoNoNo‚Äî
Total 26452446326‚Äî3417231937150828‚Äî
C-10:indicatesto themodel using CIFAR-10 dataset, andF-M:indicatesto themodel using Fashion-MNISTdataset.
Table 7: Runtime Overhead vs. Problem Detects
Time Identify Bug(IB)Dataset TotalUM DL AT DDUMDLATDD
Stack Overflow [7]2946.16421.39771.56 103.74 10271626
GitHub [ 7] 1146.162613.6148.41 137.82 4739
Schoopet al.[8]12193.5293.173491.32 1020.20 1211511
Blob [9] 48‚Äî113.14112.6 564.19 ‚Äî444834
Circle[9] 71‚Äî148.6384.37 1078.14 ‚Äî637147
MNIST [ 9] 38290.8716.684741.53 1265.02 26383831
CIFAR-10 [ 9]46121.3922.410653.63 3282.83 46464626
models from the AUTOTRAINER dataset, correctly reporting fault
location,failuresymptoms, and actionablechanges.
DeepLocalize (DL) [44] identi/f_ied 45 out of the 53 models from
the SGS benchmark and indicated fault locations for 26. It reported
symptoms for only 23 models, but it cannot provide any sugges-
tions to /f_ix these faults. Regarding the AUTOTRAINER dataset, DL
identi/f_ied 191outofthe 203buggymodels andcorrectlyreported
theirfaultlocation.However,DLdidnotprovideanysuggestionsfor /f_ixing thosemodels.Lastly, DLcan onlydetectbugs related to
numericalerrors.
AUTOTRAINER (AT)[48]Forthe53models(SGSbenchmark),
AT identi/f_ied 24 buggy models. Out of these, AT successfully re-
ported symptoms for only 19. AT was only able to repair 8 models.
DDcanhandlemorevarietiesofsemanticallyrelatederrorsthan
AT, as shown in Table 6. Please refer to [ 48] for AT‚Äôs evaluation
results whileanalyzing itsdataset.
UMLUAT (UM) [38] identi/f_ied 26 buggy models out of the 53
fromthe SGSbenchmarkand found the fault locationsfor 3.Also,
UMreportedthesymptomsfor17modelsandprovidedthelocation
/f_ix for 15 out of 53. UM correctly identi/f_ied models and reported
possible /f_ix solutions to problems from 72 out of 203 buggy models
of the AUTOTRAINER dataset. UM only supports classi/f_ication
problems,whileDDsupportsadditionaltypes,suchasregression
andclassi/f_ication.
Toevaluatetheapproaches‚Äôoverallperformance,wecollected
their total execution time while analyzing the benchmarks. Fig-
ure3shows the results. UM, DL, AT, and DD require on average
46.16, 421.39, 771.56, and 103.74 seconds, respectively, for all the
568DeepDiagnosis: AutomaticallyDiagnosingFaultsandRecommending Actionable Fixes inDeepLearningPrograms ICSE‚Äô22, May21‚Äì29,2022,Pi/t_tsburgh, PA,USA
			

	

	



		


	
	



 

	
	     
 	
Figure 3: Comparison between UMLUAT (UM) VS DeepLo-
calize (DL) VS AUTOTRAINER (AT) VS DeepDiagnosis (DD)
interms of seconds
StackOver/f_low (SOF)benchmarks.FortheGitHub(GH)benchmark,
the four approaches require on average 46.16, 2613.60, 148.41, and
137.78 seconds, respectively. For the Schoop et al.‚Äôs [38] bench-
mark,thefourapproachestakeonaverage193.52,93.17,3491.32,
and 1020.80 seconds, respectively. For the AUTOTRAINER dataset,
thefourapproachesrequire,onaverage,4159.25,4157.36,170156.70,
and74408.07seconds,respectively,tocompletetheiranalysis.Lastly,
theoverallaveragetimeforUM,DL,AT,andDD,forallbenchmarks
is2972.23, 8388.21, 106490.05,and 44914.17 seconds, respectively.
DD‚Äôs runtime overhead is mainly due to its online dynamic
analysis.DDrunsitsdynamicanalysisontheinternalparametersof
the neural networks, such as the changes of weights and gradients,
duringthetrainingphase.DDisthemosteÔ¨Écientfor StackOver/f_low
and Schoop et al.‚Äôs model and is slower than UM on the GitHub
models. The reason is that DD collects more information than UM
duringtrainingand checksadditionaltypes of errorconditions.
DD is faster than AT on all benchmarks except for the Blob and
Circledatasets.ThatisbecauseATchecksthetargetmodelafter
/f_inishingthetrainingphase.DDrequiresadditionaltimebecause
itvalidatesthemodelattheendofeachepochduringtraining,and
thenumber of epochs forthese modelsisbetween200to 500.
4.3 RQ3(Limitation)
Out of 52 programs, our technique failed to identify faults in 6 and
localize faults in 18. DD failed to report symptoms for 15 programs
andfailedtoprovidethelocationof/f_ixfor24(Tables 2and6).Inthe
following,weprovideafew examples of failed fault localization.
Ourtechniquedoesnotyetsupportmodelwith fit_generator()
insteadof fit()function. fit_generator() isusedforprocessing
a large training dataset that is unable to load into the memory [ 17].
Inthefuture,weplantocovermoreAPIs(suchas fit_generator() ).
Both#47(B3(C10)),and#53(B3(C10))programsarerelatedto
checkingvalidationaccuracy[ 38].Themodelsplitsthetraindata
intotrainingandvalidationdata,andthenprovidethevalidation
databypassing validation_data=(x_val,y_val) intothe/f_it()method.
The buggymodelreported high accuracy for the validation dataset.
TheremayexistanoverlapbetweentrainingdataandvalidationTable 8: TheSymptoms ResultsfromDeepDiagnosis
DD -Symptoms
DatasetNSUCSSASDNSORSLNDSANISVGSIASILS
Stack Overflow [7]15151201010
GitHub[ 7] 2121100110
Schoopet al.[8]6003200000
Blob [9] 572000101000
Circle[9] 1212310011800
MNIST [ 9]16007000800
CIFAR-10 [ 9]17400000500
data. But our approach would not indicate any symptom, as it does
notsupportproblems related to training and validation.
Both #43 (A2 (C10)), and #49 (A2 (C10)) programs are related to
the dropout rate in the Dropoutlayer [38]. The idea of the dropout
istoremoveacertainpercentageofneuronsduringiterationsto
prevent over/f_itting. The buggy model sets a high dropout rate =
0.8 which is more than the acceptable rate of 50%. Our approach
isnotabletoprovideacorrectsuggestionto/f_ixthemodel.Inour
future work, we plan to investigate more hyperparameters such as
the batch size, epoch, and dropout rate to handle the above models.
DD supports deep learning models of various structures, includ-
ing convolutional neural networks (CNNs) and fully connected
layers. But,RecurrentNeuralNetworks (RNNs) are notsupported
byourcurrentreferenceimplementation.Developerscanextend
ourDDto supportRNNs and other architectures.
UMonlysupportsclassi/f_icationproblems,inwhichthelastlayer
issoftmax. Otherwise, it reports false alarms. DL only supports
numericalproblems,anditdoesprovideanysuggestionsonhowto
/f_ixadetectedproblem.ATsupportsclassi/f_icationproblemsanddoes
notsupportproblemsinthemodelarchitecture (i.e.,lossfunction,
activationfunctionatlastlayer,andsomeAPIs(e.g., /f_it_generator()) ).
In terms of eÔ¨Éciency, AT takes longer to /f_ind a /f_ix, as it tries all
possiblesolutionsuntilarrivingatthecorrectone.Incaseitdoes
not/f_indanimprovement, it markstheproblem as unsolvable.
4.4 RQ4 (Ablation)
The "Ablation" column of Table 6shows which procedure in Ta-
ble2isusedtoreportthesymptomineachbuggymodelforSGS
dataset. We found that ExplodingTensor () detects 23 buggy models,
SaturatedActivation () detects 7, DeadNode () reports 5, OutofRange
()detects 5, UnchangeWeight () /f_inds 2,InvalidAccuracy () detects 2,
AccuracyNotIncreasing() ,andVanishingGradient() reportsonlyone
buggymodel.Table 8showsdatasetnames,andcolumnscontain
the number of symptoms, which were detected successfully by the
correspondingprocedureinTable 2.FromTable 8,wefoundthat
ExplodingTensor() detects73buggymodels, VanishingGradient()
detects 32, UnchangeWeight () /f_inds 25, AccuracyNotIncreasing ()
22,DeadNode() reports13, SaturatedActivation() detects12, Out-
ofRange() detects5,and InvalidAccuracy() reportsonlytwobuggy
models. Although the incorrect DNN models related to parameters
and structures often manifest as numerical errors during training,
DDprovidedfurtherreasoningandcategoriesofcausesusingthese
procedures, which can help quickly /f_ix the bugs. Our study also
foundthatdatapreparationisafrequentlyoccurringissueandthus
theImproperData() procedure is frequently invoked. SGS bench-
markdoesnothaveaverydeepmodelthatcontainsmanylayers.
Thus we did not use VanishingGradient () detector very frequently.
On the other hand, VanishingGradient () is invoked very frequently
inAUTOTRAINERmodels,becausethisdatasethasmanylayers
569ICSE‚Äô22,May21‚Äì29,2022,Pi/t_tsburgh,PA,USA MohammadWardat,BrenoDantas Cruz,WeiLe,andHridesh Rajan
using sigmoid and tanh as activation functions. H owever, when
NlayersuseaLogisticactivationfunction(likesigmoidortanh),
Nsmall derivativesaremultiplied together. Thus, thegradientde-
creasesexponentiallyand propagates down to theinputlayer.
4.5 ResultsDiscussions
We compared and contrasted three approaches [ 37,44,48] against
our approach (DD). From Table 7, we found our approach detected
more problems in the SGS dataset than AUTOTRAINER. Also, it
detectedfewerproblemsinAUTOTRAINERdatasetthantheATap-
proach. The reason is that our approach only reported the problem
andsolutionifitdetectedoneof8symptoms.Ontheotherhand,AT
inspects themodel based on thetraining accuracythreshold[ 48].
For our evaluation, we used 188 normal models from [ 48]. From
those,78areMNIST,35areCIFAR-10,36areCircle,and39areBlob.
UM reported the message: ‚Äú<Warning: Possible over/f_itting>‚Äù for 68
out 78 MNIST models. It reported the following message: ‚Äú[<Error:
Input data exceeds typical limits>]‚Äù for 35 out 35 CIFAR-10 models,
becausethetrainingdataisnotintherange [-1,1].DLreportedthe
message: ‚ÄúMDL: Model Does not Learn‚Äù for 4 out 34 Circle models
and 16 out 39 Blob models. For all MNIST and CIFAR-10 models,
DL reported diÔ¨Äerent messages. AT checks if a model has training
accuracy less than or equal to the threshold of 60%. To make a
fair comparison between the approaches, we changed the training
accuracy threshold to 100%. AT reported diÔ¨Äerent symptoms for
10 out of 36 Circle, 5 out of 39 Blob models, and 2 models with
problems out of the 78 MNIST models. Our approach reported one
saturated symptom for 36 Circle, which is not supported in AT,
reported8symptoms-6‚Äúsaturatedactivations‚Äùand2the‚Äúaccuracy
is not increasing.‚Äù For the MNIST model, our approach reported 37
symptoms - 35 ‚Äúdead nodes‚Äù and one is a ‚Äúnumerical problem;‚Äù we
investigatedthismodelandfounditsaccuracyis20%.ForCIFAR-10
models, DD reported 21 models with ‚Äúdead node‚Äù out of 35 models.
Alldetailed experimentresults arepubliclyavailable [ 6].
4.6 Summary
DDsigni/f_icantlyoutperformedthebaselinesUM,DL,andATinthe
SGSdataset(Tables 6and7).Inparticular,identi/f_ied46outof53
buggy models, correctly performed fault localization in 34 models,
andreportedsymptomsfor37ofthose.DDalsoprovidedalocation
to /f_ix 28 out of 53 faulty models. Regarding total analysis time, DD
outperformed AT because it does not require the training phase
to /f_inish to detect bugs. Also, DD uses a Decision Tree (Figure 2)
approachtoreducethesearchspacewhenmappingsymptomsto
theirroot causes.
Furthermore,DDismorecomprehensivethanpriorwork,asit
supports several varieties and semantically related errors in classi-
/f_ication andregression models. Also,DDsupports8 failure symp-
toms,whilepriorapproachessupportfewer (in Section 3).
Finally, DD does not support some APIs (e.g., /f_it_generator() )
as weconsiderproblemsrelatedto hyperparameters, forexample,
epoch,batch size,and dropoutrate,as outof scope.
5 THREATS TO VALIDITY
External Threat: We have collected 53 real-world buggy DNN
models from Stack Over/f_low , GitHub and 496 models from priorwork[38,44,48].Thesemodelscoveravarietyoffailuresymptoms
andlocationtoperform/f_ixes;however,ourdatasetmaynotinclude
alltypesofDNNAPIsandtheirparameters.Tomitigatethethreatof
behaviorchangescausedbytheExtractortool,wemanuallyveri/f_ied
theaccuracyofeachmodelbeforeandaftertheirconversion.We
used the Extractor to extract the source code from the 496 models
from AUTOTRAINER [ 48]. In terms of execution time, diÔ¨Äerent
hardware con/f_igurations may oÔ¨Äer varying response times. We
mitigatedthisthreatbyexecutingourexperimentsseveraltimes
andcalculated their averages.
Internal Threat: When implementing Algorithm 1, Decision
Tree(Figure 2),andTables 2and3,weusedtheparametersde/f_ined
by prior works [ 1,10,20,37,48]. These selected values may not
workforsomeunseenexamples.Tomitigatethisthreat,wehave
validated these selected parameters against our benchmarks col-
lectedfromadiversesetofsources[ 38,44,48].Foreachofthese
benchmarks,ourselectedparametersworkconsistentlywell.Al-
though we have carefully inspectedourcode, ourimplementation
may still contain some errors. We manually constructed ground
truthsregardingfaultlocation,failuresymptoms,andlocationto
/f_ix for all the buggy models based on the data from the previous
research[ 38,44,48]. This processmay haveintroduced errors.
6 RELATEDWORK
FaultlocalizationinDeepNeuralNetworks: Therecentincrease
inthepopularityof deeplearningappshasmotivatedresearchers
to adapt fault localization techniques to this context. With the
intent of validating diÔ¨Äerent parts of DL-based systems and discov-
eringfaultybehaviors.Thegoaloffaultlocalizationistoidentify
suspicious methods and statements to isolate the root causes of
program failures and reduce the eÔ¨Äort of /f_ixing the fault [ 36]. War-
datetal.[44]presentedanautomaticapproachforfaultlocalization
called DeepLocalize. It performsdynamic analysis during training
to determinesif a targetmodelcontainsanybugs. Itidenti/f_ies the
root causes by catching numerical errors during DNN training.
WhileDeepLocalizefocusesonidentifyingbugsandfaultsbased
on numerical errors, DeepDiagnosis aims to perform fault local-
izationbeyondthatscope.Furthermore,ourapproachcanreport
symptomsandprovideactionable/f_ixesto aproblem.
DEBAR [49] is a static analysis approach that detects numerical
bugs in DNNs. DEBAR uses two abstraction techniques to improve
its precision and scalability. DeepDiagnosis uses dynamic analysis
tolocalizefaultsandreportsymptomsofamodelduringtraining.In
contrast,DEBARusesastaticanalysisapproachtodetectnumerical
bugswithtwo abstraction techniques.
Schoopetal.[38]proposedUMLUAT,auserinterfacetoolto/f_ind,
understand and /f_ixdeep learning bugs usingheuristics. Itenables
users to check the structure of DNN programs and model behav-
ior during training. Then, it provides readable error messages to
assistusersinunderstandingand/f_ixingbugs.Section¬ß 4showsthe
comparison between UMLUAT [ 38] and DeepDiagnosis. DeepDiag-
nosisismorecomprehensive,eÔ¨Écient,andeÔ¨ÄectivethanUMLAUT,
whichonlysupportsclassi/f_icationmodels.
DeepFault [ 15] is an approach that identi/f_iessuspicious neurons
of a DNN and then /f_ixes these errors by generating samples for
retrainingthemodel.DeepFaultisinspiredbyspectrum-basedfault
570DeepDiagnosis: AutomaticallyDiagnosingFaultsandRecommending Actionable Fixes inDeepLearningPrograms ICSE‚Äô22, May21‚Äì29,2022,Pi/t_tsburgh, PA,USA
localization.Itcountsthenumberoftimesaneuronwasactive/i-
nactivewhenthenetworkmadeasuccessfulorfaileddecision.It
then calculates a suspiciousness score such as the spectrum-based
faultlocalizationtoolTarantula.Incontrast,DeepDiagnosisfocuses
on identifying faults and reporting diÔ¨Äerent types of symptoms for
structurebugs.
Bug Repair in Deep Neural Networks: Zhanget al.[46]p r o -
posed Apricot, an approach for automatically repairing deep learn-
ingmodels.Apricotaimsto/f_ixill-trainedweightswithoutrequiring
additional training data or any arti/f_icial parameters in the DNN.
MODE [30] is a white-box approach that focuses on improving the
model performance. It is an automated debugging technique in-
spired by state diÔ¨Äerential analysis. MODE can determine whether
a model has over/f_itting or under-/f_itting problems. Compared with
MODE and Apricot, which focus on training bugs (e.g., insuÔ¨É-
cient training data), DeepDiagnosis focuses on structure bugs (e.g.,
activationfunctionmisused).
Zhangetal.[48]introducedAUTOTRAINER,anapproachfor
/f_ixingclassi/f_icationproblems.Zhang etal.de/f_ine/f_ivesymptoms,
andprovideasetofpossiblesolutionsto/f_ixeachone.OnceAUTO-
TRAINER detects a problem, it tries the candidate solutions, one
byone,untilitaddressestheproblem.Ifnoneofthesolutions/f_ix
theproblem,itreportsafailuremessage.Theevaluationusedsix
populardatasetsandshowedthatAUTOTRAINERdetectsandre-
pairsthemodelsbasedonaspeci/f_icthreshold.AUTOTRAINERwas
abletoimprovetheaccuracyforallrepairingmodelsonaverage
47.08%.DeepDiagnosisanalyzesthemodel‚Äôssourcecodeduringthe
trainingphasetolocalizethebug.DeepDiagnosissupportseight
symptoms, while AUTOTRAINER supports /f_ive. DeepDiagnosis
does not perform automated /f_ixes, but it provides actionable rec-
ommendationsthatdeveloperscanfollow.AUTOTRAINERtriesall
possible strategies in its search space to /f_ix a problem and outputs
whether or notthe/f_ixwas successful. Incontrast, DeepDiagnosis
usesadecisiontreetoreducethesolutionsearchspace,thussav-
ing time and computational resources. In summary, the goals of
DeepDiagnosisandAUTOTRAINERarediÔ¨Äerent;DeepDiagnosis
focuses on fault localization while AUTOTRAINER on automati-
callyrepairing amodel.
7 CONCLUSIONS AND FUTUREWORK
This paper introduces a dynamic analysis approach called DeepDi-
agnosisthatanon-expertcanusetodetecterrorsandreceiveuseful
messages for diagnosing and /f_ixing the DNN models. DeepDiagno-
sis provides a list of veri/f_ication procedures to automatically detect
8 types of common symptoms. Our results show that DeepDiagno-
sis can successfullydetect diÔ¨Äerent types of symptoms and report
actionable changes. It outperforms the state of the art tool such as
UMLUATandDeepLocalize,anditisfasterthanAUTOTRAINER
forfaultlocalizationand providesuggestions to /f_ix theissue.
Wehaveidenti/f_iedseveralfutureworkdirections.First,wewould
like to extend our approach to support additional model types,
failure symptoms, and automatic repair. Second, we would like
to conduct studies on how to improve DNN bug repair on non-
functional bugs such as fairness bugs [ 11,12]. Third, we would
liketoextendourapproachtosupportadditionaltypesofbugsin
diÔ¨Äerent stages of the ML pipeline [ 13]. Lastly, we would like toexplorehowtoleverageour/f_indingstoimprovetheperformance
of AutoML models[ 35].
8 ACKNOWLEDGMENT
This work was supported in part by the US National Science Foun-
dation (NSF) through grants CNS-21-20448, and CCF-19-34884. All
opinionsareoftheauthorsanddonotre/f_lecttheviewofsponsors.
REFERENCES
[1]2015.https://cs231n.github.io/neural-networks-3/ . [Online; accessed 20-Aug-
2020].
[2]2016. How to prepare a dataset for Keras? https://stackover/f_low.com/questions/
31880720/ . [Online;accessed 19-Aug-2020].
[3] .2020. Manifold. https://github.com/uber/manifold .
[4] .2020. Tensorwatch. https://github.com/microsoft/tensorwatch .
[5] .2020. Visdom. https://github.com/fossasia/visdom .
[6]2021.https://github.com/DeepDiagnosis/ICSE2022 . [Online;accessed12-August-
2021].
[7]2021.https://github.com/Wardat-ISU/DeepLocalize . [Online;accessed12-Aug-
2021].
[8]2021.https://github.com/BerkeleyHCI/umlaut . [Online; accessed 12-Aug-2021].
[9]2021.https://github.com/shiningrain/AUTOTRAINER . [Online; accessed 12-
August-2021].
[10]Amazon.2017. AmazonSageMaker. https://docs.aws.amazon.com/sagemaker/
latest/dg/whatis.html .
[11]Sumon Biswas and Hridesh Rajan. 2020. Do the Machine Learning Models ona
CrowdSourcedPlatformExhibitBias?AnEmpiricalStudyonModelFairness.In
ESEC/FSE‚Äô2020:The28thACMJointEuropeanSoftwareEngineeringConferenceand
Symposium on the Foundationsof Software Engineering (Sacramento, California,
United States).
[12]Sumon Biswas and Hridesh Rajan. 2021. Fair Preprocessing: Towards Under-
standing Compositional Fairness of Data Transformers in Machine Learning
Pipeline.In ESEC/FSE‚Äô2021:The29thACMJointEuropeanSoftwareEngineering
ConferenceandSymposiumontheFoundationsofSoftwareEngineering (Athens,
Greece).
[13]Sumon Biswas, Mohammad Wardat, and Hridesh Rajan. 2022. The Art and Prac-
tice of Data Science Pipelines: A Comprehensive Study of Data Science Pipelines
In Theory, In-The-Small, and In-The-Large. In ICSE‚Äô22: The 44th International
Conferenceon SoftwareEngineering (Pittsburgh,PA, USA).
[14]Fran√ßois Chollet. 2019. Writing your own callbacks. https://keras.io/guides/
writing_your_own_callbacks/ . [Online;accessed 20-Apil-2021].
[15]Hasan Ferit Eniser, Simos Gerasimou, and Alper Sen. 2019. DeepFault: fault
localization for deep neural networks. In Fundamental Approaches to Software
Engineering ,Reiner H√§hnleand Wil van der Aalst(Eds.). Springer International
Publishing,Cham,171‚Äì191.
[16]Utku Evci. 2018. Detecting dead weights and units in neural networks. arXiv
preprintarXiv:1806.06068 (2018).
[17] FrancoisChollet.2015. Keras documentation. https://keras.io/ .
[18]FrancoisChollet.2015. Keras:thePythonDeepLearninglibrary. https://keras.io/ .
[19]Xavier Glorot and Yoshua Bengio. 2010. Understanding the diÔ¨Éculty of training
deep feedforward neural networks. In Proceedings of the thirteenth international
conferenceonarti/f_icialintelligenceandstatistics .JMLRWorkshopandConference
Proceedings, 249‚Äì256.
[20]IanGoodfellow,YoshuaBengio,AaronCourville,andYoshuaBengio.2016. Deep
learning.Vol.1. MIT press Cambridge.
[21]ToviGrossman,GeorgeFitzmaurice,andRamtinAttar.2009.Asurveyofsoftware
learnability:metrics,methodologiesandguidelines.In Proceedingsofthesigchi
conferenceon human factorsin computingsystems .649‚Äì658.
[22]GuidovanRossum.2019. ModulediÔ¨Ñib. https://github.com/python/cpython/
blob/3.9/Lib/diÔ¨Ñib.py .
[23]CaglarGulcehre,MarcinMoczulski,MishaDenil,andYoshuaBengio.2016. Noisy
activationfunctions.In Internationalconferenceonmachinelearning .PMLR,3059‚Äì
3068.
[24]ArnekvistIsac,CarvalhoJFrederico,DanicaKragic,andJohannesAndreasStork.
2020. The eÔ¨Äect ofTarget Normalization and Momentum onDying ReLU.In The
32ndannualworkshopof the Swedish Arti/f_icialIntelligence Society (SAIS) .
[25]Md Johirul Islam, Giang Nguyen, Rangeet Pan, and Hridesh Rajan. 2019. A
Comprehensive Study on Deep Learning Bug Characteristics. In ESEC/FSE‚Äô19:
The ACM Joint European Software Engineering Conference and Symposium on the
Foundationsof SoftwareEngineering(ESEC/FSE) (ESEC/FSE 2019) .
[26]Md Johirul Islam, Rangeet Pan, Giang Nguyen, and Hridesh Rajan. 2020. Repair-
ing Deep Neural Networks: Fix Patterns and Challenges. In ICSE‚Äô20: The 42nd
InternationalConferenceon SoftwareEngineering (Seoul,SouthKorea).
571ICSE‚Äô22,May21‚Äì29,2022,Pi/t_tsburgh,PA,USA MohammadWardat,BrenoDantas Cruz,WeiLe,andHridesh Rajan
[27]Andrew Janowczyk and Anant Madabhushi. 2016. Deep learning for digital
pathology image analysis: A comprehensive tutorial with selected use cases.
Journalof pathology informatics 7(2016).
[28]Steven W Knox. 2018. Machine learning: a concise introduction . Vol. 285. John
Wiley&Sons.
[29]Shumin Kong and Masahiro Takatsuka. 2017. Hexpo: A vanishing-proof activa-
tion function. In 2017 International Joint Conference on Neural Networks (IJCNN) .
IEEE,2562‚Äì2567.
[30]ShiqingMa,YingqiLiu,Wen-ChuanLee,XiangyuZhang,andAnanthGrama.
2018. MODE:automatedneuralnetworkmodeldebuggingvia statediÔ¨Äerential
analysisandinputselection.In Proceedingsofthe201826thACMJointMeeting
on European Software Engineering Conference and Symposium on the Foundations
of SoftwareEngineering .175‚Äì186.
[31] DMan√©etal.2015. TensorBoard:TensorFlow‚Äôs visualization toolkit.
[32]Martin Abadiet al.2015. TensorFlow: large-Scale MachineLearning onHetero-
geneousSystems. https://www.tensor/f_low.org/ .
[33]John Miller and Moritz Hardt. 2018. Stable recurrent models. arXiv preprint
arXiv:1805.10369 (2018).
[34]RiccardoMiotto,FeiWang,ShuangWang,XiaoqianJiang,andJoelTDudley.2018.
Deep learning for healthcare: review, opportunities and challenges. Brie/f_ings in
bioinformatics 19, 6(2018), 1236‚Äì1246.
[35]Giang Nguyen, Johir Islam, Rangeet Pan, and Hridesh Rajan. 2022. Manas:
MiningSoftwareRepositoriestoAssistAutoML.In ICSE‚Äô22:The44thInternational
Conferenceon SoftwareEngineering (Pittsburgh,PA, USA).
[36]Spencer Pearson, Jos√© Campos, Ren√© Just, Gordon Fraser, Rui Abreu, Michael D
Ernst, Deric Pang, and Benjamin Keller. 2017. Evaluating and improving fault
localization.In 2017IEEE/ACM39thInternationalConferenceonSoftwareEngi-
neering(ICSE) .IEEE, 609‚Äì620.
[37]EldonSchoop,ForrestHuang,andBj√∂rnHartmann.2020.SCRAM:SimpleChecks
forRealtimeAnalysisofModelTrainingforNon-ExpertMLProgrammers.In
ExtendedAbstractsofthe2020CHIConferenceonHumanFactorsinComputing
Systems.1‚Äì10.
[38]Eldon Schoop, Forrest Huang, and Bj√∂rn Hartmann. 2021. UMLAUT: Debug-
gingDeepLearning ProgramsusingProgramStructure andModelBehavior.In
Proceedingsofthe2021CHIConferenceExtendedAbstractsonHumanFactorsinComputingSystems .
[39]ShanqingCai.2017. DebugTensorFlowModelswithtfdbg. https://developers.
googleblog.com/2017/02/debug-tensor/f_low-models-with-tfdbg.html .
[40]David Sussillo and LF Abbott. 2014. Random walk initialization for training very
deep feedforward networks. arXivpreprintarXiv:1412.6558 (2014).
[41]Hong Hui Tan and King Hann Lim. 2019. Vanishing gradient mitigation with
deep learningneuralnetwork optimization.In 2019 7th InternationalConference
on Smart Computing& Communications(ICSCC) .IEEE, 1‚Äì4.
[42]YuchiTian,KexinPei,SumanJana,andBaishakhiRay.2018. DeepTest:automated
Testing of Deep-Neural-Network-Driven Autonomous Cars. In Proceedings of
the 40th International Conference on Software Engineering (Gothenburg, Sweden)
(ICSE‚Äô18) . Associationfor ComputingMachinery, NewYork,NY, USA,303‚Äì314.
https://doi.org/10.1145/3180155.3180220
[43]MatthewVeresandMedhatMoussa.2019. Deeplearningforintelligenttrans-
portation systems: A survey of emerging trends. IEEE Transactions on Intelligent
transportationsystems 21, 8(2019), 3152‚Äì3168.
[44]Mohammad Wardat, Wei Le, and Hridesh Rajan. 2021. DeepLocalize: fault local-
izationfordeepneuralnetworks.In ICSE‚Äô21:The43ndInternationalConference
on SoftwareEngineering .
[45]Bing Xu, Ruitong Huang, and Mu Li. 2016. Revise saturated activation functions.
arXivpreprintarXiv:1602.05980 (2016).
[46]Hao Zhang and WK Chan. 2019. Apricot: a weight-adaptation approach to
/f_ixing deep learningmodels. In 2019 34th IEEE/ACMInternational Conferenceon
AutomatedSoftwareEngineering(ASE) .IEEE, 376‚Äì387.
[47]Xiangyu Zhang, Neelam Gupta, and Rajiv Gupta. 2006. Locatingfaultsthrough
automatedpredicateswitching.In Proceedingsofthe28thInternationalConference
on SoftwareEngineering .272‚Äì281.
[48]XiaoyuZhang,JuanZhai,ShiqingMa,andChaoShen.2021. AUTOTRAINER:
AnAutomaticDNN Training Problem Detectionand RepairSystem.In ICSE‚Äô21:
The43ndInternationalConferenceon SoftwareEngineering .
[49]YuhaoZhang,LuyaoRen,LiqianChen,YingfeiXiong,Shing-ChiCheung,and
Tao Xie. 2020. Detecting numerical bugs in neural network architectures. In
Proceedings of the 28th ACM Joint Meeting on European Software Engineering
Conference and Symposium on the Foundations of Software Engineering . 826‚Äì837.
572