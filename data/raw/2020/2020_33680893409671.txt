Correlationsbetween Deep NeuralNetwork ModelCoverage
Criteriaand Model Quality
Shenao Yan
shenao.yan@rutgers.edu
Rutgers University
USAGuanhong Tao
taog@purdue.edu
PurdueUniversity
USAXuwei Liu
liu2598@purdue.edu
PurdueUniversity
USA
JuanZhai
juan.zhai@rutgers.edu
Rutgers University
USAShiqing Ma
shiqing.ma@rutgers.edu
Rutgers University
USALei Xu
xlei@nju.edu.cn
Nanjing University
China
XiangyuZhang
xyzhang@cs.purdue.edu
PurdueUniversity
USA
ABSTRACT
Inspiredbythegreatsuccessofusingcodecoverageasguidancein
softwaretesting,alotofneuralnetworkcoveragecriteriahavebeen
proposed to guide testing of neural network models (e.g., model
accuracyunderadversarialattacks).However,whilethemonotonic
relation between code coverage and software quality has been sup-
portedbymanyseminalstudiesinsoftwareengineering,itremains
largelyunclearwhethersimilarmonotonicityexistsbetweenneural
network model coverage and model quality. This paper sets out to
answer this question. Specifically, this paper studies the correla-
tionbetweenDNNmodelqualityandcoveragecriteria,effectsof
coverage guided adversarial example generation compared with
gradient decent based methods, effectiveness of coverage based
retraining compared with existing adversarial training, and the
internal relationships among coveragecriteria.
CCS CONCEPTS
Â·Software and its engineering â†’Software testing and de-
bugging;Â·Computingmethodologies â†’Neuralnetworks .
KEYWORDS
Software Testing, DeepNeuralNetworks
ACMReference Format:
Shenao Yan, Guanhong Tao, Xuwei Liu, Juan Zhai, Shiqing Ma, Lei Xu,
and Xiangyu Zhang. 2020. Correlations between Deep Neural Network
Model Coverage Criteria and Model Quality. In Proceedings of the 28th
ACMJointEuropeanSoftwareEngineeringConferenceandSymposiumonthe
FoundationsofSoftwareEngineering(ESEC/FSEâ€™20),November8Å›13,2020,
Permissionto make digitalor hard copies of allorpart ofthis work for personalor
classroom use is granted without fee provided that copies are not made or distributed
forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation
onthefirstpage.Copyrights forcomponentsofthisworkownedbyothersthanthe
author(s)mustbehonored.Abstractingwithcreditispermitted.Tocopyotherwise,or
republish,topostonserversortoredistributetolists,requirespriorspecificpermission
and/or a fee. Request permissions from permissions@acm.org.
ESEC/FSE â€™20, November 8Å›13, 2020, Virtual Event, USA
Â©2020 Copyright heldby the owner/author(s). Publicationrightslicensed to ACM.
ACM ISBN 978-1-4503-7043-1/20/11...$15.00
https://doi.org/10.1145/3368089.3409671VirtualEvent,USA. ACM,NewYork, NY,USA, 13pages.https://doi.org/10.
1145/3368089.3409671
1 INTRODUCTION
DeepNeuralNetwork(DNN)isbecomingan integralpartofthe
new generation of software systems, such as self-driving vehicle
systems,computervisionsystems,andvariouskindsofbotsystems.
Just like software testing is a key step in traditional software devel-
opmentlife-cycle,manyresearchersand practitioners believe that
DNNmodeltestingiscriticaltomodelqualityandhencethewhole
system quality [ 39,47,59,71]. Software testing can be classified as
black-box testing andwhite-box testing , with the former generating
test cases from the specification without looking into the imple-
mentation whereas the later generating test cases based on the
implementation. Specifically, white-box testing aims to generate
testcasestoimprove codecoverage. Highcode coverageprovides
moreconfidenceaboutthesubjectsoftwareâ€™squality.Itiswidely
used in practice because progress can be easily quantified and test
generationismoreamenabletoautomation(comparedtoblack-box
testing). The upper half of Figure 1shows a typical life-cycle of
software testing. Given a subject software, various test generation
engines,suchasrandominputgeneration,fuzzing,symbolicexe-
cution,searchbasedtestgeneration,canbeusedtogeneratetest
cases.Thegeneratedtestsuiteisexecutedandthecodecoverageis
measured and provided as feed-back to the test generation engine,
whosegoalishencetogeneratemoretestcasesthatcanimprove
coverage. The failing test cases are reported to the developers who
fix the corresponding faults/bugs/defects, leading to a new version
ofthe subjectsoftware.
Inspiredbythegreatsuccessofwhite-boxsoftwaretesting,re-
searchers have proposed white-box DNN testing to improve model
quality [15,39,47,59,71,79]. The life-cycle of DNN model testing
closelyresemblesthatofsoftwaretesting,asshowninthelower
half of Figure 1. Specifically, the subject becomes a DNN model
instead of a program. Model input generation techniques are used
to generate inputs. The generation is guided by some coverage cri-
terion just like in software test generation. A failing input example
775
ESEC/FSE â€™20, November8Å›13,2020,VirtualEvent, USA S.Yan, G.Tao, X.Liu, J. Zhai, S.Ma, L.Xu,X.Zhang
SoftwareTest Generation 
EngineTest Case Code Coverage Bug Fixing
DNN modelModel Input 
GenerationTest Case Retraining Neuron Coverage
Figure 1:Software Development VS. DNN Development
istheone thatcausesmodelmis-classificationandcanbeusedto
retrainthemodel.Theretrainingprocedureisanalogoustothebug
fixingprocedureinthesoftwaretestinglife-cycle.Ityieldsanew
versionofthe model.
Codecoveragecriteriaplayacriticalroleinsoftware(white-box)
testing.Alargenumberofcoveragecriteriahavebeenproposedand
used. For example, statement coverage measures the percentage of
statementsthatareexecutedbyatestsuite;edgecoveragemeasures
the percentage of exercised control flow edges; and path coverage
measuresthepercentageofprogrampathsthatgetexecuted.Differ-
entcriteriahavedifferentlevelsofstrength(indisclosingbugs)and
entail various amount of efforts. Specifically, statement coverage is
the simplest and also the weakest, whereas path coverage is one
of the most expensive and most powerful. These criteria provide a
spectrum of options for developers when they are balancing devel-
opmentcostandproductquality.Inspiredbythesecodecoverage
criteria,researchershaveproposedalargesetofDNNmodelcov-
erage criteria [ 32,39,47,71]. Specifically, a DNN model consists of
an input layer, an output layer, and a number of inner layers, each
containing a set of neurons. During model inference/prediction,
neuron activation values at a layer are computed based on those
fromthepreviouslayer.Duringtheactivationvaluecomputation
foraneuron,ifthevalueissmallerthan0,itissetto0andhence
has 0 contribution to the later layer(s). We say the neuron is not
activated. Researchers observe the analogy between activating a
neuron and covering a software artifact (e.g., statement). There-
fore, they propose a number of coverage criteria based on how
neurons are activated. For example, neuron coverage [47] measures
the percentage of neurons that are activated, analogous to state-
mentcoverage;and neuronpatterncoverage [39,71]measuresthe
activation path,analogous to pathcoverage.
Despite theinspiring correspondence betweensoftware testing
andDNNmodeltesting,thereareafewopenquestionsthatneed
to be answered. Specifically, the effectiveness of white-box soft-
waretestingisbuiltonabasicassumptionfortherelationbetween
codecoverageandsoftwarequality.Specifically,whilethedevel-
opers generate more test cases to achieve higher code coverage,
morebugsaredisclosedandfixedandhencethesoftwarequality
ismonotonically improved (without considering regression). As
such, coverage driven test generation is one of the most popular
test generation strategies. While such monotonicity assumption is
largely proved (empirically) in software testing, its counter-partin model testing is unclear (as far as we know). In fact, the seman-
tics of DNN models is substantially different from the semantics
ofprograms.Thesyntacticanalogybetweensoftwarestatements
andneuronsmaynotdirectlytranslatetotheirsemanticanalogy.
Intuitively, program behaviors are largelydiscrete whereas model
behaviors are continuous. A statement being covered suggests that
new functionality is exercised, which is a discrete and modular
event. However, a neuron being activated may not have similar
implication.Itmaywellbethatthesemanticsofamodelliesinthe
distributionofthe entireactivationvector insteadofasetofdiscrete
eventsofwhether individualneuronsare activated.
Therefore, this paper aims to study the following research ques-
tions that we believe are important for white-box DNN model
testing. First, we want to study if there is monotonic relation be-
tweenmodelcoverageimprovementandmodelquality:isittrue
thattrainingsetswithincreasingmodelcoverageleadtoincreasing
model quality. Second, we want to study if coverage driven test
generationiseffectiveinmodeltesting,whencomparedtoexisting
test generation techniques that are solely based on optimization
and leverage continuity and differentiability of model behaviors.
Third, we want to study if test cases generated using model cov-
eragecriteriahaveuniqueadvantageinimprovingmodelquality
(analogous to bug fixing). Last, we aim to understand if there is
correlation inthe variousmodelcoveragecriteria.
In our study, we make use of 8 models and 3 datasets. We lever-
age model coverage based test generation techniques in DeepX-
plore [47], DeepGauge[ 39], DeepHunter [ 71]andSADL [ 32], and
thestate-of-the-artoptimizationbasedadversarialexamplegener-
ation techniques C&W [ 11] and PGD [ 41], to generate test cases
thatleadtodifferentlevelsofcoverage.Thesetestcasesareusedto
retrain models. Then we measure the quality improvement using a
setofwell-establishedmetricsandstudytheaforementionedfour
researchquestions.Throughour study,we find:
â€¢DNNcoveragecriteriadonothavemonotonicrelationswith
model quality (measured by model accuracy in the presence
ofadversarialexamples).
â€¢Although DNN coverage criteria can be used as guidance
to find adversarial examples, effective adversarialexamples
maynot leadto higher coverage.
â€¢Existing methods used to generate adversarial examples
based on coverage criteria usually add larger (i.e., measured
byâ„“ğ‘distance) and human visible (i.e., measured by vi-
sualsimilarity)perturbations,comparedtoexistinggradient
based methods. Using such inputs in model testing is analo-
goustohavingprograminputsthatmayviolatethesoftware
inputpreconditions.
â€¢Adversarial examples generated by DNN coverage guided
methods can be used to retrain a model to improve model
robustness againsttheadversarial method used to generate
the training inputs (e.g., performing semantic preserving
operationslikechangingblurrinesstomaximizecoverage).
However, such models are not robust against gradient based
attacks(e.g.,PGD).Ontheotherhand,PGDbasedadversarial
training can improve the model robustness against PGD
attacksbut not attacksbyusing coverageas guidance.
776Correlations betweenDeep Neural Network Model Coverage CriteriaandModel Quality ESEC/FSE â€™20, November8Å›13,2020,VirtualEvent, USA
â€¢Most existing DNN coverage criteria correlate with each
other,somehavingstrongcorrelations.Thishelpsexplain
their similar behaviors in all experiments. However, it is
unclearwhetherthereexistspartialorderamongthemlike
code coveragecriteria.
2 BACKGROUND
2.1 DeepNeuralNetwork
A deep neural network(DNN) is a parameterized function Fthat
mapsanğ‘›âˆ’dimensionalinput ğ‘¥âˆˆRğ‘›tooneofthe koutputclasses.
TheoutputoftheDNN ğ‘ƒâˆˆRğ‘˜isaprobabilitydistributionoverthe
kclasses. In particular, ğ‘ƒ(ğ‘¥)ğ‘—is the probability of the input belong-
ingtoclassğ‘—.Aninputğ‘¥isdeemedasclass ğ‘—withthehighestproba-
bilitysuchthattheoutputclasslabel ğ‘¦hasğ‘¦=argmaxğ‘—âˆˆ[1,ğ‘˜]ğ‘ƒ(ğ‘¥)ğ‘—.
During training, with the assistance of a training dataset of
inputswithknownground-truthlabels,theparametersincluding
weights and bias of the DNN model are determined. Specifically,
givenalearningtask,supposethetrainingdatasetisaset, Dğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘›=
{ğ‘¥ğ‘–,ğ‘¦ğ‘–}ğ‘
ğ‘–=1, ofğ‘input samples ğ‘¥={ğ‘¥1,ğ‘¥2,...,ğ‘¥ğ‘} âˆˆRğ‘›and the
corresponding ground-truth labels ğ‘¦={ğ‘¦1,ğ‘¦2,...,ğ‘¦ğ‘} âˆˆ [1,ğ‘˜].Fis
the deep learning model that predicts the corresponding outcomes
ğ‘¦â€²based on the given input ğ‘¥,ğ‘–.ğ‘’.,ğ‘¦â€²=F(ğ‘¥). Within the course of
model training, there is a loss function L=/summationtext.1
1â‰¤ğ‘–â‰¤ğ‘›||ğ‘¦â€²
ğ‘–âˆ’ğ‘¦ğ‘–||2. So
the processofmodeltraining can be formalizedas:
ğ‘šğ‘–ğ‘›/summationdisplay.1
1â‰¤ğ‘–â‰¤ğ‘›||ğ‘¦â€²
ğ‘–âˆ’ğ‘¦ğ‘–||2
For a neuron ğ‘œ, if it is activated (i.e., activation value is larger
than some threshold value) by some input examples (in a set), it
becomes an activated neuron for the set. When we provide the
training dataset to the model, the range of the observed neuron
activation valuesisrepresentedas [ ğ‘™ğ‘œğ‘¤ğ‘œ,â„ğ‘–ğ‘”â„ğ‘œ].
Whenweprovidethetestdataset ğ‘‡,theneuronactivationvalues
maynotbelimitedin[ ğ‘™ğ‘œğ‘¤ğ‘œ,â„ğ‘–ğ‘”â„ğ‘œ].Instead,thevaluescanalsofall
in(âˆ’âˆ,ğ‘™ğ‘œğ‘¤ğ‘œ)or(â„ğ‘–ğ‘”â„ğ‘œ,+âˆ).Wereferto (âˆ’âˆ,ğ‘™ğ‘œğ‘¤ğ‘œ)âˆª(â„ğ‘–ğ‘”â„ğ‘œ,+âˆ)
asthecornercaseregions(oftheneuron).Let ğœ™(ğ’™,ğ‘œ)betheoutput
valueofneuron ğ‘œforinputğ‘¥,thenğ‘ˆğ‘ğ‘ğ‘’ğ‘Ÿğ¶ğ‘œğ‘Ÿğ‘›ğ‘’ğ‘Ÿğ‘ğ‘’ğ‘¢ğ‘Ÿğ‘œğ‘› (ğ‘ˆğ¶ğ‘)and
ğ¿ğ‘œğ‘¤ğ‘’ğ‘Ÿğ¶ğ‘œğ‘Ÿğ‘›ğ‘’ğ‘Ÿğ‘ğ‘’ğ‘¢ğ‘Ÿğ‘œğ‘› (ğ¿ğ¶ğ‘), which represent the set of neurons
that ever fall into the corner case regions, respectively, given some
test inputs.Formally,
ğ‘ˆğ¶ğ‘={ğ‘œâˆˆğ‘‚| âˆƒğ’™âˆˆğ‘‡:ğœ™(ğ’™,ğ‘œ) âˆˆ (â„ğ‘–ğ‘”â„ğ‘œ,+âˆ)}
ğ¿ğ¶ğ‘={ğ‘œâˆˆğ‘‚| âˆƒğ’™âˆˆğ‘‡:ğœ™(ğ’™,ğ‘œ) âˆˆ (âˆ’âˆ,ğ‘™ğ‘œğ‘¤ğ‘œ)}
The symbolswe use inthis paper are shownas follows:
LISTOFSYMBOLS
FA DL classifier onk classes,where F(ğ‘¥ğ‘–)=ğ‘¦ğ‘–
ğ´ğ‘Activatedneuron
ğ¿ğ¶ğ‘Setofactivatedneuronsthatfallinthecorner-caseregions
ğ‘The number ofsamples ininputdataset
ğ‘‚Universal setofneuronsofaDNN
ğ‘œNeuron
ğ‘ƒSoftMax layer outputof F,whereF(ğ‘¥)=argmax
ğ‘—ğ‘ƒ(ğ‘¥)ğ‘—
ğ‘ƒ(ğ‘¥)ğ‘—The j-thprobability of ğ‘ƒ(ğ‘¥),whereğ‘—âˆˆ {1,...,ğ‘˜}
ğ‘ˆğ¶ğ‘Setofactivatedneuronsthatfallinthecorner-caseregionsğ‘¥ğ‘
ğ‘–An adversarialexamples of ğ‘¥ğ‘–
ğ‘¥ğ‘–Inputsamples, ğ‘–âˆˆ (1,ğ‘)
ğ‘¦ğ‘–The corresponding ground-truth label of ğ‘¥ğ‘–, whereğ‘¦ğ‘–=
1,...,ğ‘˜
2.2 AdversarialExamples andDNN Robustness
DNN models are vulnerable to adversarial examples. That is, given
an original input ğ‘¥ğ‘–and a small adversarial perturbation ğ›¿, a DNN
modelFhas:
F(ğ‘¥ğ‘
ğ‘–)=F(ğ‘¥ğ‘–+ğ›¿)=ğ‘¦ğ‘¡â‰ ğ‘¦ğ‘–=F(ğ‘¥ğ‘–)
Here,weuse ğ‘¥ğ‘
ğ‘–torepresent ğ‘¥ğ‘–+ğ›¿,whichisusuallyreferredtoas
anadversarialexample.Theperturbation ğ›¿addedtotheinputisma-
liciouslymanipulatedbyanadversaryanditiscommonlybounded
byâ„“ğ‘-norm,i.e., ||ğ›¿||ğ‘<ğœ–.Symbolğ‘¦ğ‘¡denotesthepredictedlabel
of adversarial example ğ‘¥ğ‘
ğ‘–, different from the original prediction
ğ‘¦ğ‘–. There exists a large body of different adversarial attacks. We
discuss two widelyused attacks in this paper and employ them in
our experiment evaluation.
ğ‘ªğ‘¾attackisasetofpowerfulattacksbasedondifferentnorm
measurements on the magnitude of perturbations introduced by
Carlini and Wagner [ 11]. In particular, CW is formalized as an
optimization problem to search for high confidence adversarial
examples with small magnitude of perturbations. They leverage
the logits Z(Â·)(i.e., outputs right before the softmax layer) instead
of the final prediction F(Â·)for generating perturbation. The objec-
tive function for optimization is the combination of target label
(|F(ğ‘¥ğ‘
ğ‘–) âˆ’ğ‘¦ğ‘¡|) and small perturbation ( ||ğ›¿||ğ‘), which is achieved
using optimizersuch as Adam.
ğ‘·ğ‘®ğ‘«attack isa first-order universal adversary attack based on
FastGradientSignMethod( ğ‘­ğ‘®ğ‘ºğ‘´)[57].FGSMperformsasingle
step update on the original sample ğ‘¥along the direction of the
gradient ofalossfunction.The lossfunctionisusually definedas
the cross-entropy between the output of a network and the true
labelğ‘¦. PGD [41] is an iterative variant of FGSM, which applies
the projected gradient descent algorithm with random starts to
FGSM.Thatis,foreachattackiteration,givenaninput ğ‘¥ğ‘–,itfirst
adds a small random perturbation within given bound ||ğ‘Ÿ||ğ‘<ğœ–
to the input, i.e., ğ‘¥â€²
ğ‘–=ğ‘¥ğ‘–+ğ‘Ÿ. It then performs one step of FGSM
and applies the gradient ğ‘‘ğ‘¥to the input, i.e., ğ‘¥â€²â€²
ğ‘–=ğ‘¥â€²
ğ‘–+ğ‘‘ğ‘¥. The
updated sample is subsequently projected to the original bound,
i.e.,ğ‘¥ğ‘
ğ‘–=ğ‘ğ‘™ğ‘–ğ‘(ğ‘¥â€²â€²
ğ‘–,ğ‘¥ğ‘–âˆ’ğœ–,ğ‘¥ğ‘–+ğœ–). Theğ‘ğ‘™ğ‘–ğ‘(Â·)function sets small
out-of-bound values to ğ‘¥ğ‘–âˆ’ğœ–and large ones to ğ‘¥ğ‘–+ğœ–. The process
continues until an adversarialexample isgeneratedortime-out.
2.3 AdversarialTraining
Adversarial training, introduced by Goodfellow et al. [ 21] is one of
the most effective ways to improve DNN model robustness. The
overarchingideaofadversarialtrainingistoincorporateadversarial
examplesformodeltraining.Thatis,duringeachtrainingiteration,
adversarialexamplesarefirstgeneratedagainstthecurrentstate
ofthemodel,andthenusedas trainingdata for optimizingmodel
parameters.Madryetal.[ 41]leveragethePGDattackwithmultiple
steps for adversarial training. Adversarial training has been shown
effective for large scaledataset such as ImageNet [ 34].
777ESEC/FSE â€™20, November8Å›13,2020,VirtualEvent, USA S.Yan, G.Tao, X.Liu, J. Zhai, S.Ma, L.Xu,X.Zhang
3 COVERAGEBASEDDNN TESTING
In this section, we first introduce a number of popular neuron
coveragecriteriaanddiscuss theirintendedusage.
3.1 DeepXplore
Pei etal.[ 47]introducedthe neuroncoverage(NC):
ğ‘ğ¶=|ğ´ğ‘|
|ğ‘‚|
where|ğ´ğ‘|denotesthenumberofactivatedneuronsand |ğ‘‚|means
thetotalnumberofneurons.Basically, ğ‘ğ¶measuresthepercentage
ofactivatedneurons(i.e.,whoseactivationvalueislargerthan0)for
agiventestsuiteandDNNmodel.DeepXploreviewsNCas thefirst
white-boxtestingmetricforDLsystemsthatcanestimatetheamount
of DL logic explored by a set of test inputs. And then, DeepXplore
triestogeneratenewtestinputsthatcanmaximizeNCaswellas
triggering differential behaviors in multiple DL systems that are
designedtohavesimilarfunctionality.ThesedifferentDNNmodels
areusedascross-referencetoavoidmanuallylabelingdatasets.The
generationprocessappliesthreepre-definedimagetransformations:
addingasingleblackrectangle,changingallpixelvaluesbyacertain
degree and adding multiple small black rectangles, with the goal
of covering more neurons. After that, DeepXplore mixes these
generated examples with benign inputs to re-train the model to
improve modelaccuracy.
3.2 DeepGaugeandDeepHunter
Ma et al. extended the coverage concept to different levels (i.e.,
neuronlevel,layerlevel)andproposedmanynewDNNcoverage
based testing criteria in DeepGauge [ 39], and demonstrated that (a
testsuitewith) ahighercoverageofthesecriteriapotentiallyindicates
a higher chance to detect the DNNâ€™s defects. Here, a defect is defined
asmodelmispredictions.DeepHunter[ 71]leveragessuchcoverage
metricsasthefeedbacktofuzzDNNmodelstoproduceadversarial
samples.Thesenewmetrics include:
â€¢ğ’Œâˆ’multisection Neuron Coverage (KMNC) . Given a neuron
ğ‘œâˆˆğ‘‚, theğ‘˜âˆ’multisection neuron coverage measures how thor-
oughlythegivensetoftestinputs ğ‘‡coverstherange[ ğ‘™ğ‘œğ‘¤ğ‘œ,â„ğ‘–ğ‘”â„ğ‘œ].
To quantify KMNC, the range [ ğ‘™ğ‘œğ‘¤ğ‘œ,â„ğ‘–ğ‘”â„ğ‘œ] is divided into ğ‘˜equal
sections (i.e., ğ‘˜âˆ’multisections), with ğ‘˜>0. Alsoğ‘†ğ‘œğ‘šdenotes the
ğ‘šâˆ’th section with 1 â‰¤ğ‘šâ‰¤ğ‘˜. Thenğœ™(ğ’™,ğ‘œ) âˆˆğ‘†ğ‘œğ‘šmeans theğ‘šâˆ’th
section iscoveredbyat leastone input ğ’™âˆˆğ‘‡.
ğ¾ğ‘€ğ‘ğ¶=/summationtext.1
ğ‘œâˆˆğ‘‚|{ğ‘†ğ‘œğ‘š| âˆƒğ’™âˆˆğ‘‡:ğœ™(ğ’™,ğ‘œ) âˆˆğ‘†ğ‘œğ‘š}|
ğ‘˜Ã— |ğ‘‚|
â€¢Neuron Boundary Coverage (NBC) .
ğ‘ğµğ¶=|ğ‘ˆğ¶ğ‘| + |ğ¿ğ¶ğ‘|
2Ã— |ğ‘‚|
Fromthedefinition,itâ€™seasytoseethat ğ‘ğµğ¶showshowthor-
oughlythegivensetoftestinputs ğ‘‡coversthecorner-caseregions.
â€¢Strong Neuron Activation Coverage (SNAC) .
ğ‘†ğ‘ğ´ğ¶=|ğ‘ˆğ¶ğ‘|
|ğ‘‚|
Similartoğ‘ğµğ¶,ğ‘†ğ‘ğ´ğ¶measuresthepercentageofuppercorner-
caseregionsthat are coveredbythe setoftest inputs ğ‘‡.â€¢Topâˆ’ğ’ŒNeuron Coverage (TKNC).
ğ‘‡ğ¾ğ‘ğ¶=|/uniontext.1
ğ’™âˆˆğ‘‡(/uniontext.1
1â‰¤ğ‘™â‰¤ğ¿ğ‘¡ğ‘œğ‘ğ‘˜(ğ’™,ğ‘™))|
|ğ‘‚|
Here,ğ¿denoteslayersofaDNN and ğ‘™âˆˆ (1,ğ¿)istheğ‘™âˆ’thlayer
of a DNN. Function ğ‘¡ğ‘œğ‘ğ‘˜(ğ’™,ğ‘™)denotes the neurons that have the
largestkoutputsonlayer ğ‘™givenğ’™.ğ‘‡ğ¾ğ‘ğ¶measuresthepercentage
ofneuronsthathaveeverbeenthetop ğ‘˜neuronswithinitslayer
for agiven inputset ğ‘‡.
â€¢Topâˆ’ğ’ŒNeuron Patterns (TKNP). Given a test input ğ’™, the
sequence of the top âˆ’ğ‘˜neurons on each layer also forms a pattern.
Apatternisanelementof2ğ‘¢
1Ã—2ğ‘¢
2Ã—Â·Â·Â·Ã—2ğ‘¢
ğ‘™,where2ğ‘¢
ğ‘™isthesetof
subsetsoftheneuronsonthe ğ‘™âˆ’thlayer,for1 â‰¤ğ‘™â‰¤ğ¿.Givenatest
inputsetğ‘‡,thenumberoftop âˆ’ğ‘˜neuronpatternsfor ğ‘‡isdefined
as follows.
ğ‘‡ğ¾ğ‘ğ‘ƒ=|{(ğ‘¡ğ‘œğ‘ğ‘˜(ğ’™,1)),...,(ğ‘¡ğ‘œğ‘ğ‘˜(ğ’™,ğ¿)) |ğ’™âˆˆğ‘‡|}|
Intuitively,ğ‘‡ğ¾ğ‘ğ‘ƒmeasuresthenumberofdifferentactivation
patternsforthemostactive ğ‘˜neuronsoneachlayer.Itisnotaratio
but ratheranumber.
DeepHunter is a fuzz testing framework for finding potential
DNNdefects.Itmostlyleveragestheabovecoveragecriteriaasfeed-
backtoguidethetestgeneration,andtogenerate newsamples.It
performspixelvaluetransformations(i.e.,changingimagecontrast,
brightness,blurandnoise)andaffinetransformations(i.e.,image
translation,scaling,shearingandrotation).Togenerateimagesthat
preserve its original semantics, a â„“âˆbased threshold is set. If the
differencebetweentheoriginalandthegeneratedimageislarger
thanthe threshold,the generatedimageisdiscarded.
3.3 SADL
Kim et al. introduced surprise adequacy to measure the coverage of
discreditedinputsurpriserange forDLsystems[ 32].Thesetermsare
explainedinthefollowing.AtestexampleisÅ‚ goodÅ¾ifitissufficiently
butnotoverlysurprising comparingwiththetrainingdata,thatis,
sufficiently but not overly deviant from the training distribution.
Two measurements of surprise were introduced: one is based on
Keneral Density Estimation (KDE) to approximate the likelihood
of the systemhavingseen asimilar inputduring training, and the
otherisbasedonthedistancebetweenthevectorsrepresentingthe
neuron activation traces of the given input and the training data
(e.g.,Euclideandistance ).Theproposedmetricswerealsocompared
withothermetricsinDeepGaugeandDeepXplore.Theresultsshow
that they are correlated. Moreover, they were used to guide the
retrainingofDNN models to improve robustness.
â€¢Likelihood-basedSurpriseAdequacy(LSA). Letğ›¼ğ‘œ(ğ‘¥)denote
theactivationvalueofasingleneuron ğ‘œwithrespecttoaninput
ğ‘¥.ForasetofneuronsinalayeroftheDNN,denotedas ğ‘‚â€²âŠ†ğ‘‚,
ğ›¼ğ‘‚â€²(ğ‘¥)denotes a vector of activation values that represents the
ActivationTrace(AT)of ğ‘¥overneuronsin ğ‘‚â€².Forasetofinputs
ğ‘‹,ğ´ğ‘‚â€²(ğ‘‹)={ğ›¼ğ‘‚â€²(ğ‘¥)|ğ‘¥âˆˆğ‘‹}denotes the set of activation traces
observed for neurons in ğ‘‚â€². Given a training set ğ‘‡, a bandwidth
matrixğ»and a Gaussian kernal function ğ¾, the activation trace of
anewinput ğ‘¥,KDEproduces adensity function Ë†ğ‘“as follows.
Ë†ğ‘“(ğ‘¥)=1
|ğ´ğ‘‚â€²(ğ‘‡)|/summationdisplay.1
ğ‘¥ğ‘–âˆˆğ‘‡ğ¾ğ»(ğ›¼ğ‘‚â€²(ğ‘¥) âˆ’ğ›¼ğ‘‚â€²(ğ‘¥ğ‘–))
778Correlations betweenDeep Neural Network Model Coverage CriteriaandModel Quality ESEC/FSE â€™20, November8Å›13,2020,VirtualEvent, USA
Intuitively,thefunctionmeasuresanormalizeddistancebetween
the activation values of ğ‘¥and those of individual inputs in ğ‘‡(re-
gardingğ‘‚â€²). Then LSA isdefinedas follows.
ğ¿ğ‘†ğ´(ğ‘¥)=âˆ’ğ‘™ğ‘œğ‘”(Ë†ğ‘“(ğ‘¥))
â€¢Distance-based Surprise Adequacy (DSA). AssumeaDLsys-
temğ¹, which consists of a set of neurons ğ‘‚, is trained for a classifi-
cationtaskwithasetofclasses ğ‘Œ,usingatrainingdataset ğ‘‡.Given
thesetofactivationtraces ğ´ğ‘‚(ğ‘‡),anewinput ğ‘¥,andapredicted
class of the new input ğ¶âˆˆğ‘Œ. The closest neighbor of ğ‘¥that shares
thesameoutputclass,denotedas ğ‘¥ğ‘,andtheirdistance,aredefined
as follows.
ğ‘¥ğ‘=argmin
ğ¹(ğ‘¥ğ‘–)=ğ‘¦||ğ›¼ğ‘‚(ğ‘¥) âˆ’ğ›¼ğ‘‚(ğ‘¥ğ‘–)||
ğ‘‘ğ‘–ğ‘ ğ‘¡ğ‘=||ğ›¼ğ‘‚(ğ‘¥) âˆ’ğ›¼ğ‘‚(ğ‘¥ğ‘)||
The closest neighbor in a class other than ğ‘¦, denoted by ğ‘¥ğ‘, and
theirdistance ğ‘‘ğ‘–ğ‘ ğ‘¡ğ‘,are definedas follows.
ğ‘¥ğ‘=argmin
ğ¹(ğ‘¥ğ‘–)âˆˆğ‘Œ\{ğ‘¦}||ğ›¼ğ‘‚(ğ‘¥) âˆ’ğ›¼ğ‘‚(ğ‘¥ğ‘–)||
ğ‘‘ğ‘–ğ‘ ğ‘¡ğ‘=||ğ›¼ğ‘‚(ğ‘¥) âˆ’ğ›¼ğ‘‚(ğ‘¥ğ‘)||
ThentheDSAisdefinedinthefollowing.Intuitively,itmeasures
ifğ‘¥iscloser to the target class oradifferentclass.
ğ·ğ‘†ğ´(ğ‘¥)=ğ‘‘ğ‘–ğ‘ ğ‘¡ğ‘
ğ‘‘ğ‘–ğ‘ ğ‘¡ğ‘
3.4 Research Questions
Testing is a critical step in software development life-cycle to eval-
uatesoftwarequality,findbugsandhelpdeveloperstoimprovethe
software.IntraditionalSoftwareEngineering,testcoveragecriteria
(including path coverage, basic block coverage etc.) are a set of
metricsusedtodescribethedegreetowhichthesubjectsoftware
is tested, given a test suite. These criteria are usually computed
as the number of some software artifacts (e.g., statements) that
are executed by at least one test case, divided by the total num-
berofartifacts.Animportanthypothesis,whichhasbeenproven
by numerous seminal studies [ 6,16,17,30,33,35,42,43,45,67],
is thathigher test coverage suggests better software quality , given
the same subject software. This is because a test suite achieving
higher coverage is believed to have a better chance of disclosing
defectsinthesubjectsoftware.Differentsoftwarecoveragecriteria
denote the various trade-offs between the difficulty (to achieve
highcoverage)andthecapabilityofdisclosingdefects.Forexample,
statementcoverageistheleastdifficulttoachievewiththeweakest
effectiveness in finding bugs, whereas path coverage is much more
difficulttoachieve buthasstrongerbug-finding capabilities.DNN
coverage metrics were introduced with the goal of serving deep
learningmodelengineeringinawaysimilartohowsoftwarecover-
age criteria have been serving software engineering. For example,
in DeepXplore, the authors believe that Å‚ neuron coverage is a good
metric for DNN testing comprehensiveness Å¾.In DeepGauge [ 39], the
paperstatesthattheproposedcriteriacanÅ‚ effectivelycapturethe
difference between the original test data and adversarial examples,
where DNNs could and could not correctly recognize, respectively,
demonstratingthat ahighercoverage ofourcriteriapotentiallyin-
dicate a higher chance to detect the DNN s defects Å¾. And in SADL,thepaperconcludesthatÅ‚ SAcanprovideguidanceformoreeffective
retrainingagainstadversarialexamplesbasedonourinterpretation
of the observed trend Å¾. In software testing, the correlations between
coverage and software quality are well established, which have
beendrivingdecadesofpracticeandresearch.Inthispaper,weaim
to study if the correlations between DNN coverage metrics and the
intendedobjectives can be established.
Basedontheabovequoted usageofDNN coveragecriteria,we
propose the following research questions.
3.4.1 RQ: Are DNN coverage metrics correlated with DNN model
robustness? In software testing, the correlation between test cover-
ageandsoftwarequalityisintuitivelyestablishedasfollows.Given
asubjectprogramandaninitialtestsuite,developersgeneratenew
test cases to cover software artifacts that have not been covered
before (e.g., new statements). These new test cases may disclose
defectsinthenewlycoveredcomponents.Fixingthesedefectsleads
totheimprovementofsoftwarequality.Tovalidatethecorrelations
between DNN model robustness and coverage criteria, we draw
the following analogies: subject model (in DNN testing) vs. subject
program(insoftwaretesting);trainingsetvs.initialsoftwaretest
suite; adversarialexample generationorGANbasedexamplegen-
erationvs.softwaretestgeneration;misclassifications(causedby
adversarialexamples)vs.softwarebugs;andadversarialtraining
vs.softwarebugfixing.Withsuchanalogies,wehavethefollowing
experiment design.
Experiment Design: Assume the subject model is ğ¹0. We use
adversarialexamplegenerationtechniquestogeneratealargesetof
adversarialexamples.GivenaDNNcoveragecriterion,weperform
input selection to select the examples that can lead to the most
substantial coverage improvement, and add them to the training
suiteğ‘‡0andacquireğ‘‡1.Theprocessrepeatstoacquire ğ‘‡2,ğ‘‡3,...,and
ğ‘‡ğ‘Ÿuntilthecoverageisfullorcannotimproveanymore. Here, ğ‘‡2
is a superset of ğ‘‡1,ğ‘‡3is a superset of ğ‘‡2, and so on. This process is
analogous to how the developers enhance their test suite overtime
toimprovecoverage.Wethenperformadversarialtrainingusing
ğ‘‡1,ğ‘‡2, ... andğ‘‡ğ‘Ÿ, yieldingğ¹1,ğ¹2, ... andğ¹ğ‘Ÿ, respectively, just like
fixing software bugs disclosed by new test inputs. Then we use
existingmethods[ 37]tomeasuremodelrobustnessandstudythe
correlationsbetweenrobustnessandcoverage.Ideally,wewould
expectto see thesemodels have increasing levels of robustness.
3.4.2 RQ: Is coverage driven test generation effective in disclosing
DNN defects? How does it compare to the other commonly used
adversarialexamplegeneration techniques? In software testing,an
importantfunctionalityofcodecoverageistoguidetestgeneration.
A large body of existing software test generation techniques, such
as symbolic/concolic execution [ 9,13], fuzzing[ 7,73], andsearch
based testing [ 5,26], make use of code coverage as the guidance.
Analogously, researchers of DeepXplore tries to Å‚ generate inputs
thatmaximizingneuroncoverage Å¾andbelievethatÅ‚ neuroncoverage
helpsin increasing thediversityofgenerated inputs Å¾[47].
DeepHunter [ 71] Å‚leverages multiple plugable coverage criteria
asfeedbacktoguidethetestgenerationfromdifferentperspectives Å¾.
However,whilesoftwarebehaviorsarelargelydiscrete,DNNmodel
behaviorsarecontinuous.Assuch,thereexisthighlyeffectiveinput
generation techniques built on optimizations (e.g., based on gra-
dients).Whilethesetechniquesdonotexplicitlyutilizecoverage,
779ESEC/FSE â€™20, November8Å›13,2020,VirtualEvent, USA S.Yan, G.Tao, X.Liu, J. Zhai, S.Ma, L.Xu,X.Zhang
optimization algorithms have the implicit capabilities of exploring
differentactivationpatternsbyfollowingthedirectionofgradients.
Assuch,weareinterestedinstudyingifcoverageguidedtesting
hasadvantagesoverthepopulargradientdescentbasedmethods,in
disclosingdefects(i.e.,generatingadversarialexamples).Moreover,
gradientbasedtestgenerationoftenmakesuseof â„“ğ‘-normtobound
thescaleofperturbationsuchthatadversarialexamplesdonotlook
substantially different from the original input (in humansâ€™ eyes). In
contrast, existingcoverage guided testgeneration uses predefined
imagetransformationssuchasaddingblackrectangles,changing
imagepixels byacertaindegree [ 47],changing imageblurriness
androtating images[ 71].See detailsin subsection 3.1 andsubsec-
tion3.2.NoticeDeepHunteralsousesa â„“âˆbasedthresholdvalue
to limit the change of the image. However, because the pre-defined
operationsusuallychangetheimagesignificantly,thisthreshold
valueislargerthanwhatisusedingradientbasedmethods.Also,
gradient based methods will try to minimize the change (e.g., â„“ğ‘
distance) while DeepHunter only checks if the value is smaller
than the threshold. Therefore, we also want to study the quality of
generated examples, in comparison with existing gradient descent
basedtechniques.
Experiment Design. Toanswer RQ 2,weutilizethe test genera-
tiontechniquesinDeepHunter[ 71](coveragecriteriabased)and
PDG[41](gradientbased)togeneratetestcases, Dğ»andDğ‘ƒ,re-
spectively,againstthe sameDNNmodel.Then,we compare Dğ»
andDğ‘ƒfrom a few aspects. First, we compare the effectiveness
of these methods.Namely, we calculatethe percentage of samples
inDğ»andDğ‘ƒthat can lead to discovery of DNN defects (i.e.,
mis-prediction). Second, we compare the quality of generated sam-
plesbycalculatingtheir similaritywiththe originalbenign image.
Third, we calculate the coverage metrics for Dğ‘ƒand try to see if
newadversarialexamplesleadto the growth ofcoverage.
3.4.3 RQ: Are the adversarial examples generated by DNN coverage
based testing effective in improving model robustness? How are they
compared to those generated by popular gradient descent based tech-
niques?In software testing, one aspect to measure effectiveness
oftestgenerationtechniquesistheeffectivenessofthegenerated
counter-examples (failing test cases) in bug fixing. Similar objec-
tives are explored in DNN testing. Particularly, DeepXplore and
SADL use the generated adversarial examples as part of the new
trainingdatasettoretrainthemodelsothatitcanachievebetter
accuracy against adversarial examples (i.e., more robust). For ex-
ample,theDeepXplorepaperstatesthatÅ‚ testinputsgeneratedby
DeepXplorecanalsobeusedtoretrainthecorrespondingDLmodel
toimprovethemodelâ€™saccuracybyupto3% Å¾,andtheSADLpaper
statesthat(SADL)Å‚ canimproveclassificationaccuracyofDLsystems
againstadversarialexamplesbyupto77.5%viaretraining Å¾.There-
foreinthisresearchquestion,wewanttostudytheeffectiveness
of DNN coverage based test generation in comparison with the
existing popular alternatives.
Experiment Design. To answer RQ 3,we conductthe following
experiment. First, we use DeepXplore and SADL to generate adver-
sarialexamples,andmixthemwithoriginaltrainingdatatoretrain
themodel.Wereusetheparameters(e.g.,ratioofnewadversarial
examples) from the original papers. Second, we train the models
with adversarial training. More specifically, we use the PGD basedadversarial training. The parameters used in our retraining are
adopted from the original paper [ 41]. Then, we compare the model
effectiveness ofthe twosetsof hardenedmodels.
3.4.4 RQ:HowarethedifferentDNNcoveragecriteriacorrelated?
In software testing, there is a semi-lattice for the strength of the
differentcodecoveragecriteria.Forexample,statementcoverage <
edge coverage <path coverage; and condition coverage that aims
tocoverthetrue/falsevaluesofeachcomparativeexpressionina
predicate is stronger than statement coverage, weaker than path
coverage,andnotcomparablewithedgecoverage.Wesaycriterion
ğ¶1 is stronger than ğ¶2 if 100%ğ¶1 coverage must imply 100% ğ¶2
coverage.Suchrelationsareimportantforchoosingtheappropriate
techniques in software testing. For example, path coverage may be
desiredforsafetycriticalcodedespiteitshighcost.Inthisresearch
question,weaimtolookforcorrelationorevenpartialorderamong
the variousDNN coveragecriteria.
Experiment Design. To answer RQ 4, for a given model ğ¹, we
keepaddingnewinputsamplestotestthemodel,andgatherthe
coverage information to get a sequence of values for each test
criteria. For example, for NC, we can get a sequence of values
ğ‘ğ¶ğ¹=(ğ‘ğ¶ğ¹
0,ğ‘ğ¶ğ¹
1,ğ‘ğ¶ğ¹
2,...,ğ‘ğ¶ğ¹ğ‘›)where n is thetotalnumberof
added input sample sets. Similarly, we can get ğ¾ğ‘€ğ‘ğ¶ğ¹,ğ‘ğµğ¶ğ¹etc.
Then,weperformcorrelationanalysisonthesecollecteddata ğ‘ğ¶ğ¹,
ğ¾ğ‘€ğ‘ğ¶ğ¹,ğ‘ğµğ¶ğ¹andsoontoseeiftheyhavestrongcorrelations
orpartialorder.
4 DNN MODEL QUALITYMETRICS
In software testing, quality of software is often measured by the
number of bugs found within a certain period of time. The qual-
ity metrics of DNN models are more diverse. Most DNN testing
techniques draw analogy between bugs/defects insoftware code
andadversarialexamplesinDNNmodel.Anadversarialexample
is considered manifestation of some undesirable behavior of the
model, as it causes misclassification. Just like software quality met-
rics are based on bugs, model quality metrics are also centered
around adversarial examples. They fall into three categories: model
accuracy in the presence ofadversarial examples ,adversarialexam-
ple impreceptiblity that measures if an adversarial example looks
natural,and adversarialexamplerobustness .Thesearethemetrics
commonlyusedbyadversarialmachinelearning[ 37,39].Details
are explainedinthe subsections.
4.1 Model AccuracyforAdversarialExamples
State-of-the-artadversarialexamplegenerationtechniques,suchas
C&WandPGD,optimizelogitsinordertogenerateinputs.Since
logits still have to go through a soft-max layer to produce the final
classification outputs, the generated adversarial examples may not
yieldtheintendedmis-classificationeventhoughtheoptimizercan
successfully reachits objective.Modelaccuracy inthe presence of
adversarialexamplesmeasureshowoftenthegeneratedadversarial
examplesleadtocorrectclassificationresults.Highaccuracymeans
that the model is robust. The analogy in software testing is to
measurehowoftenthesubjectsoftwarefailswhenitisstresstested.
Specifically,weconsiderthefollowingmetricsthatarerelatedto
modelaccuracy.
780Correlations betweenDeep Neural Network Model Coverage CriteriaandModel Quality ESEC/FSE â€™20, November8Å›13,2020,VirtualEvent, USA
â€¢Misclassification Ratio (MR).
ğ‘€ğ‘…=1
ğ‘ğ‘/summationdisplay.1
ğ‘–=1ğ‘ğ‘œğ‘¢ğ‘›ğ‘¡(F(ğ‘¥ğ‘
ğ‘–)â‰ ğ‘¦ğ‘–)
Here,ğ‘is the number of generated samples and ğ‘ğ‘œğ‘¢ğ‘›ğ‘¡()is a
functionusedtocountthenumberofmisclassifiedsamples.Basi-
cally, MRcalculatesthepercentageofmisclassifiedinputsamples
inthe wholeinputset.A high qualitymodelhas alowMR.
â€¢Average Confidence ofAdversarial Class(ACAC).
ğ´ğ¶ğ´ğ¶=1
ğ‘›ğ‘›/summationdisplay.1
ğ‘–=1ğ‘ƒ(ğ‘¥ğ‘
ğ‘–)ğ¹(ğ‘¥ğ‘
ğ‘–)
whereğ‘›(ğ‘›â‰¤ğ‘)is the total number of adversarial examples
thatcausemisclassification.And ğ‘ƒ(ğ‘¥ğ‘
ğ‘–)ğ¹(ğ‘¥ğ‘
ğ‘–)isthepredictionconfi-
dencetowardstheincorrectclass ğ¹(ğ‘¥ğ‘
ğ‘–).Ingeneral,ACACmeasures
theaveragepredictionconfidencetowardstheincorrectclassfor
adversarialexamples.
â€¢Average Confidence ofTrue Class(ACTC) .
ğ´ğ¶ğ‘‡ğ¶=1
ğ‘›ğ‘›/summationdisplay.1
ğ‘–=1ğ‘ƒ(ğ‘¥ğ‘
ğ‘–)ğ‘¦ğ‘–
Hereğ‘›(ğ‘›â‰¤ğ‘)is the total number of adversarial examples that
causemisclassification. ğ‘ƒ(ğ‘¥ğ‘
ğ‘–)ğ‘¦ğ‘–isthepredictionconfidenceoftrue
classesforadversarialexamples.JustlikeACAC,ACTCmeasures
the prediction confidence oftrue classesfor adversarialexamples.
4.2 AdversarialExample Imperceptibility
This metric measures how realistic an adversarial example is in
humaneyes. Itis usuallycomputed by using theoriginal example
asareference.Ahighqualityadversarialexampleisonethathas
imperceptible perturbation. We should not say a model is of low
quality is it is susceptible to low quality adversarial examples. The
analogy in software testing is that the subject software may fail
when it is provided with an input that substantially violates the
(implicit)inputpre-conditions.Insuchcases, theinducedfailures
cannotbeusedasevidenceoflowsoftwarequality.Specifically,we
use the following metrics.
â€¢Average ğ‘³ğ’‘Distortion ( ğ‘¨ğ‘³ğ‘«ğ’‘).
ğ´ğ¿ğ·ğ‘=1
ğ‘›ğ‘›/summationdisplay.1
ğ‘–=1âˆ¥ğ‘¥ğ‘
ğ‘–âˆ’ğ‘¥ğ‘–âˆ¥ğ‘
âˆ¥ğ‘¥ğ‘–âˆ¥ğ‘
Here,âˆ¥Â·âˆ¥ğ‘istheâ„“ğ‘normdistance,whichisadoptedasdistortion
metrics for evaluation. Specifically, â„“0calculates the number of
pixels changed by the perturbation; â„“2computes the Euclidean
distance betweenoriginalexamples andadversarialexamples; â„“âˆ
measures the maximum change in all dimensions of adversarial
examples.ğ´ğ¿ğ·ğ‘measurestheaveragenormalized â„“ğ‘distortionfor
alladversarialexamplesthatcausemisclassification.Thesmaller
theğ´ğ¿ğ·ğ‘,the more imperceptibilitythe adversarialexample has.
â€¢Average Structural Similarity(ASS) .
ğ´ğ‘†ğ‘†=1
ğ‘›ğ‘›/summationdisplay.1
ğ‘–=1ğ‘†ğ‘†ğ¼ğ‘€(ğ‘¥ğ‘
ğ‘–,ğ‘¥ğ‘–)
Here,ğ‘†ğ‘†ğ¼ğ‘€isthemetricusedtoquantifythesimilaritybetween
twoimages[ 29].ASScanmeasuretheaverage ğ‘†ğ‘†ğ¼ğ‘€betweenalladversarialexamplesthatcausemisclassificationandtheircorre-
sponding original examples. The larger the ğ‘†ğ‘†ğ¼ğ‘€, the more imper-
ceptibilitythe adversarialexamples has.
â€¢PerturbationSensitivityDistance (PSD) .
ğ‘ƒğ‘†ğ·=1
ğ‘›ğ‘›/summationdisplay.1
ğ‘–=1ğ‘š/summationdisplay.1
ğ‘—=1ğ›¿ğ‘–,ğ‘—ğ‘†ğ‘’ğ‘›(ğ‘…(ğ‘¥ğ‘–,ğ‘—)
Hereğ‘šisthetotalnumberofpixelsforanexample, ğ›¿ğ‘–,ğ‘—denotes
theğ‘—-thpixel ofthe ğ‘–-thexample, ğ‘…(ğ‘¥ğ‘–,ğ‘—)representsthesurround-
ing square region of ğ‘¥ğ‘–,ğ‘—, andğ‘†ğ‘’ğ‘›(ğ‘…(ğ‘¥ğ‘–,ğ‘—))=1/ğ‘ ğ‘¡ğ‘‘(ğ‘…(ğ‘¥ğ‘–,ğ‘—)), with
ğ‘ ğ‘¡ğ‘‘(ğ‘…(ğ‘¥ğ‘–,ğ‘—))denotingthestandarddeviationfunction. ğ‘ƒğ‘†ğ·evalu-
ates human perception of perturbations. The smaller the ğ‘ƒğ‘†ğ·, the
more imperceptibilitythe adversarialexample has.
4.3 AdversarialExample Robustness
In adversarial machine learning, adversarial example robustness
isoftenmeasured[ 34,41,61].Itmeasuresthelevelofresiliencea
successfuladversarialexample (i.e.,aexample thatcausesmisclas-
sification)hasinthepresenceof(input)perturbation.Intuitively,
an adversarial example that is not robust should not be used as
evidenceoflowmodelquality.Theanalogyinsoftwaretestingis
thatatransientfailure(e.g.,causedbynon-deterministicfactors)
maynot indicatelowsoftwarequality.In particular,we measure
the following.
â€¢NoiseTolerance Estimation (NTE) .
ğ‘ğ‘‡ğ¸=1
ğ‘›ğ‘›/summationdisplay.1
ğ‘–=1[ğ‘ƒ(ğ‘¥ğ‘
ğ‘–)F(ğ‘¥ğ‘
ğ‘–)âˆ’ğ‘šğ‘ğ‘¥{ğ‘ƒ(ğ‘¥ğ‘
ğ‘–)ğ‘—}]
Here,ğ‘ƒ(ğ‘¥ğ‘
ğ‘–)F(ğ‘¥ğ‘
ğ‘–)is the probability of misclassified class, and
ğ‘šğ‘ğ‘¥{ğ‘ƒ(ğ‘¥ğ‘
ğ‘–)ğ‘—}isthemaxprobabilityofallotherclasses, ğ‘—âˆˆ {1,...,ğ‘˜},
ğ‘—â‰ F(ğ‘¥ğ‘
ğ‘–).ğ‘ğ‘‡ğ¸measures the amount of noise adversarial exam-
ples can tolerate while keeping their misclassified label unchanged.
Intuitively, larger ğ‘ğ‘‡ğ¸indicates more robust adversarial examples.
â€¢Robustnessto GaussianBlur(RGB) .
ğ‘…ğºğµ=ğ‘ğ‘œğ‘¢ğ‘›ğ‘¡(F(GB(ğ‘¥ğ‘
ğ‘–))â‰ ğ‘¦ğ‘–)
ğ‘ğ‘œğ‘¢ğ‘›ğ‘¡(F(ğ‘¥ğ‘
ğ‘–)â‰ ğ‘¦ğ‘–)
WhereGBdenotestheGaussianblurfunction,analgorithmthat
canreducenoisesinimagesand ğ‘ğ‘œğ‘¢ğ‘›ğ‘¡()isusedtocountthenumber
of specific samples. ğ‘…ğºğµcounts how many adversarial examples
can maintain their misclassification function after Gaussian blur.
The greater the ğ‘…ğºğµis, the more robust adversarialexamples are.
â€¢Robustnessto ImageCompression(RIC) .
ğ‘…ğ¼ğ¶=ğ‘ğ‘œğ‘¢ğ‘›ğ‘¡(F(IC(ğ‘¥ğ‘
ğ‘–))â‰ ğ‘¦ğ‘–
ğ‘ğ‘œğ‘¢ğ‘›ğ‘¡(F(ğ‘¥ğ‘
ğ‘–)â‰ ğ‘¦ğ‘–
Here,ICis a specific image compression function. Like ğ‘…ğºğµ,
ğ‘…ğ¼ğ¶counts how many adversarial examples can maintain their
misclassification function after the image compression function.
The greater the ğ‘…ğ¼ğ¶,the more robust the adversarialexamples.
781ESEC/FSE â€™20, November8Å›13,2020,VirtualEvent, USA S.Yan, G.Tao, X.Liu, J. Zhai, S.Ma, L.Xu,X.Zhang
11.021.041.061.081.11.121.141.161.181.2
Tâ‚€ Tâ‚ Tâ‚‚ Tâ‚ƒ Tâ‚„ Tâ‚… Tâ‚† Tâ‚‡ Tâ‚ˆ Tâ‚‰ Tâ‚â‚€RELATIVE COVERAGENC(0.5) NC(0.7)
NC(0.9) TKNC
TKNP KMNC
NBC SNAC
Figure 2:Coverage VS. TrainingDatasets
0.70.80.911.11.21.31.41.5
TĞ… TĞ† TĞ‡ TĞˆ TĞ‰ TĞŠ TĞ‹ TĞŒ TĞ TĞ TĞ†Ğ…ATTACK METRICS VALUESMR ACAC ACTC NTE
ALP_LĞ… ALP_Li ASS RGB
RIC ALP_LĞ‡ PSD
Figure 3:RobustnessVS. TrainingDatasets
5 EXPERIMENTS AND RESULTS
5.1 Setup
Datasets and Models. We use MNIST [32], CIFAR-10[48] and
SVHN[1]asourdatasets.Thesearepopulardatasetsusedinthelit-
erature of model coverage [ 32,39,47,71]. MNIST is a handwritten
digit recognition dataset, a popular dataset for image classifica-
tion. The CIFAR-10 dataset is widely used for easy image classi-
fication task/benchmark in the research community. It contains
60,000 32 Ã—32 color images in 10 different classes. The Street View
HouseNumbers(SVHN)datasetisobtainedfromhousenumbersin
GoogleStreet Viewimages.Itconsists of73,257trainingsasmples
and 26,032 testing samples. For MNIST, we use three pre-trained
LeNet family models, i.e., LeNet-1, LeNet-4, and LeNet-5 as the
baseline model. For CIFAR-10, we use VGG-16 [ 52] and ResNet-20
[27] models. For SVHN, we also use three CNN model, and their
modelarchitecturesareadoptedfrompreviouswork[ 32].Wedi-
rectly use pre-trained models if possible. Our trained models are
alsoavailable online[ 1].
Configurations. Theconfigurationsforcoveragecriteriaisshown
inTable1.Forallresearchquestions,wesetthethresholdforNCto
be0.1,0.3,0.5,0.7and 0.9.For KMNC,the ğ‘˜value(i.e.,numberof
multisections)we useis10.For TKNP and TKNC, the ğ‘˜value(i.e.,
the top-ğ‘˜neuron coverage) is 2. For LSC and DSC, the layers we
analyze are shown in column 6 in Table 1, the numbers of buckets
forLSCandDSCareshownincolumns7and9,respectively,andthe
upperboundsofSAareshownincolumns8and10,respectively.
These are the same settings published in the original papers or
published in their open source repository. For C&W attacks, we
use the implementation from CleverHans and use their default
MR
ACAC
 ACTC
ALP_L0
 ALP_L2
 ALP_Li
ASS
 PSD
 NTE
 RGB
RIC
NC(0.1)
NC(0.3)
NC(0.5)
NC(0.7)
NC(0.9)
TKNC
TKNP
KMNC
NBC
SNAC
DSA
LSA0.2 0.2 -0.3 -0.1 0.2 -0.2 -0.2 0.2 0.3 0.3 0.2
0.2 0.1 -0.2 0.1 0 -0.2 0 0.1 0.2 0.2 0.3
0.2 0.4 -0.4 0.1 0 -0.2 0.1 0.1 0.4 0.3 0.1
0 -0.2 0.2 -0.1 -0.2 -0.1 0.1 -0.2 -0.2 0 -0.2
-0.2 0 0.2 -0.1 0 0.1 0.1 -0.2 0 -0.1 -0.3
-0.2 0.2 0 0 0.1 0.3 0 -0.1 0.1 -0.1 -0.1
0.2 -0.2 -0.1 0.1 -0.1 -0.3 -0.1 0.1 -0.1 0.3 0
0.1 -0.2 0.1 0 -0.4 -0.4 0.1 -0.2 -0.2 0.2 -0.3
0.1 -0.2 0.1 0.2 -0.4 -0.4 0.1 -0.2 -0.2 0.2 -0.3
0.1 -0.3 0.1 0.1 -0.4 -0.3 0.1 -0.2 -0.2 0.1 -0.3
0.2 0 0 -0.1 0 0 -0.2 0 0 0 -0.2
0.1 0 -0.2 0.2 -0.1 -0.3 0.1 0.2 0.1 0.1 0.2
âˆ’1.0
âˆ’0.5
0.0
0.5
1.0Figure 4:Correlation ofCoverage Criteria and Robustness
parameters.ForPDGattackandPDGbasedadversarialtraining,we
also usethe defaultparameters usedin their paperandrepository.
5.2 Results andAnalysis
5.2.1 RQ: Are DNN coverage metrics correlated with DNN model
robustness? As mentioned in subsection 3.4 , to answer RQ1, we
obtain a set of new training suites, ğ‘‡1,...,ğ‘‡ğ‘›. Each one has more
trainingdatathatcanenlargethecoveragemetrics.Inourexperi-
ment, we obtain 11 training datasets in total for each model (i.e.,
ğ‘›=10)includingtheoriginalone(i.e., ğ‘‡0).Ineachstep,weadd250
new images to the dataset, thus in total we add 2,500 new training
images to the training dataset. Compared with DeepXplore and
SADL, itintroduces more newtraining samples.
Figure2showsthecoveragemetricvaluechangesw.r.t.different
trainingdatasets.Inthisgraph,they-axisistherelativecoverage
valueandthebasevalueweuseistheoneobtainedon ğ‘‡0.Thisis
becausetheresultsareinwidevalueranges.Forexample,TKNP
isanabsolutenumberwhosevalueislargerthanafewthousand,
and many others are ratios whose value range is [0,1]. For NC,
we only show lines with threshold values being 0 .5, 0.7 and 0.9.
Thisisbecausefor0 .1and0.3,theoriginaldataset ğ‘‡0canachieve
100%coverage.Forsomemetrics,theoriginalcoveragevaluesare
alreadyveryhigh(over98%),andhenceinthegraph,itdoesnot
showsignificantgrowth.Overall,wecanseethatwithnewtraining
samples,allcoveragemetrics are growing.
Figure 3shows the changes of attack metrics (i.e., model robust-
ness) w.r.t. different training datasets. Similarly, the values are also
normalized based on the values obtained in ğ‘‡5(for better visualiza-
tion).For MR,ACAC, ğ´ğ¿ğ·ğ‘andPSD, largeryvaluesindicateless
robust. While for ACTC, ASS, NTE, RGB and RIC, a larger y value
always indicates a more robust model. From the graph, we observe
thatnone of them is monotonous . It means from the attackâ€™s
pointofview,addingnewsamplestoimprovecoveragedoesnot
alwaysleadto the improvement of modelrobustness.
Duetothespacelimit, Figure2andFigure3showtheresultsfor
theMNISTdatasetontheLeNet-1model.Othermodelsanddatasets
782Correlations betweenDeep Neural Network Model Coverage CriteriaandModel Quality ESEC/FSE â€™20, November8Å›13,2020,VirtualEvent, USA
Table 1:Configurations forRQs
Dataset ModelNC
(threshold)KMNC
(k)TKNC/TKNP
(k)LSC DSC
Layer n ub n ub
MNISTLeNet-1 0.1,0.3,0.5,0.7,0.9 10 2 conv_2 1000 2000 1000 5
LeNet-4 0.1,0.3,0.5,0.7,0.9 10 2 conv_2 1000 2000 1000 5
LeNet5 0.1,0.3,0.5,0.7,0.9 10 2 conv_2 1000 2000 1000 5
CIFARVGG-16 0.1,0.3,0.5,0.7,0.9 10 2 conv_2 1000 2000 1000 5
ResNet-20 0.1,0.3,0.5,0.7,0.9 10 2 block2_conv1 1000 2000 1000 5
SVHNSADL-1 0.1,0.3,0.5,0.7,0.9 10 2 pool_1 1000 2000 1000 5
SALD-2 0.1,0.3,0.5,0.7,0.9 10 2 pool_1 1000 2000 1000 5
SALD-3 0.1,0.3,0.5,0.7,0.9 10 2 pool_1 1000 2000 1000 5Table 2:ComparisonofAttack Images
Dataset ModelMR â„“âˆ
ğ·ğ»ğ·ğ‘ƒğ·ğ»ğ·ğ‘ƒ
MNISTLeNet-1 92.5% 100% 0.986 0.3
LeNet-4 90.0% 100% 0.983 0.3
LeNet-5 84.3% 100% 0.963 0.3
CIFARVGG-16 94.4% 88.7% 3.114 0.031
ResNet-20 94.4% 99.8% 3.242 0.031
SVHNSADL-1 61.7% 100% 0.689 0.031
SADL-2 92.4% 99.9% 0.714 0.031
SADL-3 64.4% 100% 0.675 0.031
do have a similar pattern. Our generated datasets, trained models,
and original results are available in GitHub [ 1]. To have a better
understanding of whether coverage criteria are correlated with
robustness, we also perform a correlation analysis on all trained
models and datasets using the Kendallâ€™s ğœmethod, which is a stan-
dardstatisticalmethodused tomeasure thelinearandnon-linear
relationships between two different variables. The result is shown
inFigure 4. In this figure, each label in x-axis represents one attack
criterion and each label in y-axis represents one coverage criterion.
Eachcellrepresentsthecorrelationbetweencriteria.Weuseblue
color to represent negative correlation (i.e., variables change in
opposite directions) and red labels to represent positive correlation
(i.e., both variables change in the same direction). According to the
definitionofcorrelationinGuildfordscale[ 24],ifthecorrelationis
lessthan0.4,thepositiveornegativecorrelationislow;valuesin
[0.4,0.7]indicate that thecorrelation is moderate; and high corre-
lationvalues(i.e.,0.7 âˆ¼0.9orabove0.9)representstrongcorrelation.
Aswecansee,mostcellsareinlightcolorsandhavelowcorrelation
values,indicatingneutralcorrelations.Inotherwords,thereisno
clear relationship between thesevariables.
5.2.2 RQ: Is coverage driven test generation effective in disclosing
DNNdefects?Howdoesitcomparetotheothercommonlyusedadver-
sarial example generation techniques? We utilize DeepHunter [ 71]
and PDG [ 41] to generate test cases, Dğ»andDğ‘ƒ, respectively,
againstthesameDNNmodel.For Dğ»andDğ‘ƒ,wefirstcompare
theattacksuccessrateusingMR.Foreachmethodandeachmodel,
we generate 1,000 images and then testMR. For both methods, we
limittheâ„“âˆbyusingthedefaultvalueprovidedbyDeepHunter[ 71]
(i.e., 0.2âˆ—255). The results are shown in columns 3 and 4 in Ta-
ble 2. As we can see, PGD achieves almost 100% success rate on
all models and datasets,while DeepHunterachieves lower success
rate(i.e.,lowerMRvalue,about60%forSADL-1andSADL-3model
andabout90%forothermodels).Wealsocomparethequalityof
thegeneratedadversarialexamplesbycalculatingthe average â„“âˆ
distances of benign inputs and adversarial examples. The results
areshownincolumns5to6in Table2.Theaverage â„“âˆdistances
for PGD ( Dğ‘ƒ) is 3 to 100 times smaller than that of DeepHunter
(Dğ»), indicating that the adversarial image quality generated by
PGDis betterthan DeepHunter.In Figure 5,we showtheoriginal
images(Figure5a ),attackimagesbyDeepHunter( Figure5b )and
PGD (Figure 5c ), respectively. The reason why DeepHunter has
largerâ„“âˆperturbation is because of the affine transformations it
uses,whichresultsinlarger â„“âˆvaluesbutmaintainsthesemantics.
In this sense, â„“âˆdistanceisnot an ideal metric touse,andhowto
(a)Original Images
 (b) DeepHunter
 (c) PGD
Figure 5:Generated Adversarial Examples
02000400060008000
00.20.40.60.81
0
401
1001
1601
2201
2801
3401
4001
4601
5201
5801
6401
7001
7601
8201
8801
9401TKNP VALUECOVERAGE VALUES
#NEW SAMPLESNC KMNC
NBC SNAC
TKNC TKNP
Figure 6:Coverage VS. # Adversarial Examples
chooseabettermetricisoutofthescope.Here,weuseitbecauseit
isusedbybothPGDandDeepHuntertodetermineifthegenerated
example isavalid input.
To understand howPGD generated adversarial examplescorre-
latewithcoveragecriteria,wealsocalculatethecoveragemetric
value change by adding 10 images in each single step. The result is
shown in Figure 6. The primary y-axis shows the percentage for
ratiovalues(i.e.,NC,KMNC,NBC,SNAC,TKNC)andthesecond
y-axis shows the value for TKNP. From the graph we can see that
the lines for NC, NBC, KMNC, SNAC and TKNC show a similar
pattern: they grow rapidly at the beginning and then plateau af-
terwards.Thistellsusthatnewadversarialexamplesdoincrease
thecoverage.However,adversarialexamplesmaynotnecessarily
increase coverage. To future demonstrate this, we use adaptive
gradient based attacks to limit the coverage change during opti-
mization while generating the adversarial examples. The results
show that it can still generate adversarial examples with almost
100% successrateonallmodels anddatasets.
On the other hand, TKNP shows a almost linear relationship
with thenumber ofnewsamples.To furtherstudythis, we design
another experiment, which is to calculate the growth of these cov-
eragemetricswhenweaddbenignsamplesinsteadofadversarial
sample. In our case, we fix the setting (i.e., ğ‘˜=2), and then we
783ESEC/FSE â€™20, November8Å›13,2020,VirtualEvent, USA S.Yan, G.Tao, X.Liu, J. Zhai, S.Ma, L.Xu,X.Zhang
02000400060008000
1
601
1201
1801
2401
3001
3601
4201
4801
5401
6001
6601
7201
7801
8401
9001
9601TKNPVALUE
#NEWSAMPLESadv_TKNP
benign_TKNP
Figure 7:TKNPVS. # New Inputs
Table 3:ModelAccuracyunder DifferentScenarios
Dataset Model Benign ğ·ğ» PGD
MNISTLeNet-1 98.7%(+0.09%) 90.3%(+2.37%) 0%(+0%)
LeNet-4 98.7%(+0.07%) 90.1%(+2.3%) 0%(+0%)
LeNet-5 98.71%(-0.26%) 91%(+1.1%) 0%(+0%)
LeNet-1Adv. 97.6%(-1.07%) 91.3%(+3.4%) 19.2%(+19.2%)
LeNet-4Adv. 96.9%(-1.72%) 88.9%(+1.1%) 9.6%(+9.6%)
LeNet-5Adv. 97%(-1.97%) 90.1%(+0.2%) 30.5%(30.3%)
CIFARVGG-16 10%(-82.8%) 9.89%(-47.41%) 9.99%(+8.8%)
ResNet-20 75.4%(-16.4%) 60.5%(+1.4%) 0.13%(+0.13%)
VGG-16Adv. 87.8%(-5%) 55.9%(-1.4%) 40.9%(+39.7%)
ResNet-20 Adv. 86.7%(-5.04%) 57.5%(-1.7%) 36.6%(+36.6%)
SVHNSADL-1 93.97%(+4.27%) 82.95%(+6.9%) 0%(+0%)
SADL-2 91.55%(+3.83%) 77.65%(+5.9%) 0.1%(+0.1%)
SADL-3 94.28%(+1.71%) 84.47%(+5.1%) 1.1%(+1.1%)
SADL-1 Adv. 85.8%(-3.9%) 71.25%(-4.8%) 46.6%(+46.6%)
SADL-2 Adv. 81.7%(-6.12%) 66.73%(-4.8%) 44.6%(+44.6%)
SADL-3 Adv. 87.7%(-4.87%) 74.13%(-5.23%) 50.6%(+50.6%)
addthecorrespondingseedinputasthenewsample.Theresults
is shown in Figure 7. As we can see, adding benign samples and
addingadversarialsampleshavealmostthesameeffects,indicat-
ing that this criteria cannot really distinguish the differences of
benignsamplesandadversarialsamples.Inotherwords,itisalmost
equivalentto the count ofsamples.
5.2.3 RQ: Are the adversarial examples generated by DNN coverage
based testing effective in improving model robustness? How are they
compared to those generated by popular gradient descent based tech-
niques?To answer RQ3, we retrain the models using two different
approaches: for one set, we use the adversarial examples gener-
ated by coverage guided testing, and for the other set, we use PGD
basedadversarialtraining.Theresultsareshownin Table3.The
firstcolumnshowsthedatasets.Thesecondcolumnpresentsthe
models including those retrained with coverage guided adversarial
examples(rows2-4,8-9,12-14)andthoseretrainedbyPGD(rows
5-7,10-11,15-17).Foreachmodel,weshowtheaccuracyonbenign
samples,adversarialexamplesgeneratedbycoverageguidanceand
under PGD attack. In each cell, we show the model accuracy as
wellasthedifferencecomparedwiththeoriginalmodel(without
retraining).Numberswith +meansthattheaccuracyishigherthan
theoriginalmodelandnumberswith âˆ’meansthatthemodelaccu-
racy is lower than the original one. From the table, we can see that
most models with the same architecture have similar accuracy on
benign testing datasets (which is also similar to the original model
accuracy).Insomecases,PGDbasedadversarialtraininggetslower
NC(0.1)
 NC(0.3)
 NC(0.5)
 NC(0.7)
 NC(0.9)
TKNC
 TKNP
 KMNC
NBC
SNAC
DSA
 LSA
NC(0.1)
NC(0.3)
NC(0.5)
NC(0.7)
NC(0.9)
TKNC
TKNP
KMNC
NBC
SNAC
DSA
LSA0.69
0.29 0.42
0.22 0.32 0.05
-0.26-0.35-0.03 0.27
-0.13-0.16 -0.0690.19 0.4
0.11 0.1 0.01 -0.16-0.46-0.46
0.15 0.1 0.0930.18 -0.12-0.28 0.44
0.14 0.25 0.16 0.21 -0.16-0.28 0.44 0.79
0.11 0.22 0.14 0.21 -0.14-0.27 0.43 0.77 0.94
0.09 -0.08 0.04 0.02 0.250.057-0.01 0.08 0.04 0.04
0.26 0.38 0.12 -0.24 -0.57 -0.39 0.43 0.17 0.25 0.2 -0.21
âˆ’1.0
âˆ’0.5
0.0
0.5
1.0Figure 8:Correlation between Coverage Criteria
accuracy on benign testing datasets. This is mainly because adver-
sarialtrainingishardtoconverge[ 66].Wetrainedseveralversions
andgotsimilarresults.AsimilarissuehappenedforVGG-16model
on the CIFAR dataset. We found that images generated by Deep-
Hunterhavesignificantlylargeperturbations,andthere-training
processisdifficult to converge.
Most models also have a higher accuracy (compared with the
originalmodels)ontheadversarialexamplesgeneratedbycover-
age guided testing. VGG-16 on CIFAR-10 does not converge and
that is why the model accuracy is low. We find that adversarial
training does not necessarily improve the model accuracy against
suchimages.Forexample,allthreemodelsonSVHNendupwith
aloweraccuracyonthisdataset.Onemorefindingisthatmodels
retrained with coverage guided testinghas nearly 0% accuracy un-
derPGDattack,whilePGDbasedadversarialtrainingincreasesthe
modelaccuracyby30%formostcases.Thisindicatesthatcoverage
criteria guided retraining can improve model robustness under at-
tacksthatusethesameimageperturbationstrategy,butdoesnot
increasethemodelrobustnessundergradientbasedattacks.Simi-
larly, PGD based retraining can improve model accuracy against
PGDattacksbutcannotimproveitsaccuracyagainstadversarial
inputsgeneratedbycoverageguidedmethods.
5.2.4 RQ:HowarethedifferentDNNcoveragecriteriacorrelated? If
the coverage criteria have partial order relationship, they ought to
be correlated. Thus, we first analyze the correlation among all the
coverage criteria. The results are shown in Figure 8. The meaning
ofthegraphisthesameas Figure4.Fromthegraph,wecanseethat
NBC and SNAC are correlated with each other with high strength,
whereastheyhaveweakornocorrelationwiththeothermetrics.
NC, KNC, TKNC, LSA and DSA are also correlated with each other
to a certain degree, but they do not show very strong correlations.
Suchresults are alsoconsistent withexisting work [ 14,32,39].
6 THREATTO VALIDITYAND DISCUSSION
ThreattoValidity. First,mostexistingcoveragecriteriafocuses
on DNN based image classifications, and so is this work. Second,
the results are acquired with a limited set of models, data sets and
784Correlations betweenDeep Neural Network Model Coverage CriteriaandModel Quality ESEC/FSE â€™20, November8Å›13,2020,VirtualEvent, USA
specific training hyper parameter settings. They may not be repre-
sentative for other modelsorsettings.Tomitigatethe threat, we
publishallthesetupdetails,implementation,data(e.g.,thegener-
ated tests) at [ 1] for reproduction. Third, our study is largely based
on existing model coverage criteria and test generation techniques.
A possible threat is that we may not faithfully reproduce these
existing works. To mitigate the threat, we use the models, datasets
and implementations (when available) from the original papers.
We also validate our results by cross-checking with those in the
originalpapers.
Discussion and suggestions. According to results from RQ2 and
RQ3,weknowthatitisrelativelyeasytogenerateadversarialeven
whenthemodelhasachieveover100%coverage.Thisisdifferent
from traditional software testing where software are generally less
vulnerable after achieving high coverage. This brings the doubt
abouttheusefulnessofexistingcoveragecriteriainDNNtesting.
However,weareuncertainiftherewillbeothercoveragecriteria
that showsavery positive correlation withDNN robustness.
Oneimportantmessagefromthisstudyisthatourcommunity
shall establish a set of standards for evaluating future proposals.
Theyshallincludetestingvariousmodelproperties(e.g.,robustness,
accuracyandfairness), andcover various modelarchitecturesand
taskssothattheresultscanbecomparable.Asaninitialstep,we
have create a GitHub reposotiry [ 2]for this purpose, andstrongly
encourage our readers to contribute.
7 RELATED WORK
DNN Testing and Validation. The relationship between cover-
ageandDNNtestinghasbeenstudiedbyothersaswell.Donget
al.[14]showthatthereislimitedcorrelationbetweencoverageand
robustness for DNNs. Li et al. [ 36] argues that previously reported
fault-detectionÅ‚capabilitiesÅ¾conjecturedfromhighcoveragetest-
ing are more likely due to the adversary-oriented search but not
the real Å‚highÅ¾ coverageitself.
Besidescoveragecriteria,Alarge body oftestingmethodswas
proposed for testing machine learning models, such as fuzzing [ 18,
25,44,62,63,68,71,80],symbolicexecution[ 3,23,51,55],runtime
validation [ 54,64], fairness testing [ 3,62,77], etc. DeepTest [ 59]
utilizesninetypesofrealisticimagetransformationsforgenerating
test images, which discovered more than 1,000 erroneous behav-
iors of DNNs used in autonomous driving systems. DeepRoad [ 76]
leverages generative adversarial networks (GANs) to generate test
cases simulating different weather conditions.DeepBillboard [ 79]
manipulates contents on billboards to cause wrong steering an-
glesofautonomousdrivingsystems.DeepImportance[ 20]selects
importantneuronsfrompre-trainedmodelsandclustersthoseneu-
rons with respect to their value regions. The coverage of those
important neurons is then used for testing model behaviors. Model
testinghasalsobeenappliedonotherdomainssuchasautomatic
speech recognition [ 15], text classification [ 62], image classifica-
tion [55,60], machine translation [ 28,56]. More related works can
be foundinthis survey [ 75].
To validate safety of DNN models, researchers leverage verifi-
cationtechniquestoprovideformalguarantees[ 19,22,31,46,48,
49,65,68]. Reluplex [ 31] transforms DNN models to numerical
constraintsandutilizesSMT-solvertoverifyrobustnessofDNNs.ModelsevaluatedinReluplexweresmallwith8layersand300ReLU
nodes. DeepSafe [ 22] was built on Reluplex and cannot scale to
largemodels.ReluVal[ 65]leveragesintervalarithmetictoverifyro-
bustnessofDNNs,whichis200timesfasterthanReluplex.AI2[49]
uses abstract interpretation to approximate the data region after
ReLU activation function. Due to its over-approximation nature,
AI2mayfailtoverifyaninputwhichhascertainproperty.Deep-
Poly[53]combinesfloatingpointpolyhedronwithintervals,which
canscaletolargemodels.DISSECTOR[ 64]generatessub-models
forintermediatelayersoforiginalmodelsandvalidatesgiveninputs
bymeasuring prediction probabilitiesfrom sub-models.
Adversarial Machine Learning. The vulnerability of machine
learning models has been an extensively discussed topic [ 8,10,12,
21,38,40,57,58,69,78]. As we elaborate in subsection 2.2 that
an attacker can use gradient to perturb original inputs to induce
misclassification of DNNs. There also exist other types of adver-
saries suchas black-box attacks [ 8], back-doorattacks [ 38], etc.A
lot of defense mechanisms were proposed to mitigate the threat
of adversarial examples such as input transformations [ 70,72], dif-
ferential certificate [ 50], adversarial training [ 34,41,61], model
internal checking [ 40,58], etc.More details regarding attacks and
defensescan be foundinthesepapers [ 4,74].
DNNtestingtechniqueshavealsobeenappliedongenerating
counterexamples. The proposed neuron coverage based metrics
were utilized to guide the search [ 32,39,47]. In this paper, we
studytherelationshipbetweenthosecoveragebasedmetricsand
adversarialexamplesin RQ2.Wealsoleverageadversarialtraining
approach to study the robustness of DNN models trained with
coveragebasedexamplesandgradientbasedones,respectively(see
details in RQ3).
8 CONCLUSION
Inthispaper,westudyexistingneuralnetworkcoveragecriteria,
and find that they are not correlated with model robustness. Al-
though they can be used for adversarial example generation and
improvemodelrobustnessinlimitedscenarios(i.e.,attackersuse
the same perturbationstrategy), they tendtogenerateadversarial
examplesthathavesubstantialperturbationsandhenceperceptible
by humans. In contrast, existing optimization based adversarial
example generation techniques can generate less perceptible ex-
amples.Thereare correlations betweenexistingcoveragecriteria.
However, it remains unclear if they have partial order relations as
code coverage criteria. Our experiments and data are public for
reproduction andvalidation.
ACKNOWLEDGMENTS
Wesincerelythanktheanonymousreviewersfortheirconstructive
comments,feedbacksandsuggestions.Wealsothanktheauthors
of tools used in this paper for their open source efforts, without
which thisresearchisimpossible.This researchwassupported, in
partbyNSFChinaNo.61802166,DARPAFA8650-15-C-7562,NSF
1748764, NSF 1901242, NSF 1910300, ONR N000141410468, ONR
N000141712947, and Sandia National Lab under award 1701331.
Any opinions, findings, and conclusions made in this paper are
thoseoftheauthorsonlyanddonotnecessarilyreflecttheviews
ofour sponsors.
785ESEC/FSE â€™20, November8Å›13,2020,VirtualEvent, USA S.Yan, G.Tao, X.Liu, J. Zhai, S.Ma, L.Xu,X.Zhang
REFERENCES
[1]2020. DNNTesting/CovTesting. https://github.com/RU-System-Software-and-
Security/CovTesting . (Accessed on09/10/2020).
[2]2020. SE4DLBenchmark. https://github.com/RU-System-Software-and-Security/
Benchmark .
[3]Aniya Aggarwal, Pranay Lohia, Seema Nagar, Kuntal Dey, and Diptikalyan Saha.
2019. Blackboxfairnesstestingofmachinelearningmodels.In Proceedingsof
the201927thACMJointMeetingonEuropeanSoftwareEngineeringConference
and Symposiumonthe FoundationsofSoftwareEngineering . 625Å›635.
[4]Anish Athalye, Nicholas Carlini, and David Wagner. 2018. Obfuscated Gradients
GiveaFalseSenseofSecurity:CircumventingDefensestoAdversarialExamples.
InInternationalConference onMachineLearning . 274Å›283.
[5]Arthur Baars, Mark Harman, Youssef Hassoun, Kiran Lakhotia, Phil McMinn,
Paolo Tonella, and Tanja Vos. 2011. Symbolic search-basedtesting. In 2011 26th
IEEE/ACMInternationalConferenceonAutomatedSoftwareEngineering(ASE2011) .
IEEE,53Å›62.
[6]AntoniaBertolino.2007. Softwaretestingresearch:Achievements,challenges,
dreams.In FutureofSoftwareEngineering (FOSEâ€™07) . IEEE,85Å›103.
[7]Marcel BÃ¶hme, Van-Thuan Pham, and Abhik Roychoudhury. 2017. Coverage-
basedgreyboxfuzzingasmarkovchain. IEEETransactionsonSoftwareEngineering
45,5 (2017), 489Å›506.
[8]Wieland Brendel, Jonas Rauber, and Matthias Bethge. 2018. Decision-based
adversarial attacks: Reliable attacks against black-box machine learning models.
InInternationalConference onLearning Representations (ICLR) .
[9]Cristian Cadar, Daniel Dunbar, and Dawson Engler. 2008. KLEE: unassisted
and automatic generation of high-coverage tests for complex systems programs.
InProceedings of the 8th USENIX conference on Operating systems design and
implementation . USENIXAssociation, 209Å›224.
[10]Nicholas Carlini and David Wagner. 2017. Adversarial examples are not eas-
ilydetected:Bypassingtendetectionmethods.In Proceedingsofthe10thACM
Workshop onArtificial Intelligenceand Security . 3Å›14.
[11]NicholasCarliniandDavidWagner.2017. TowardsEvaluatingtheRobustnessof
Neural Networks.In IEEE SymposiumonSecurityand Privacy (S&P) . 39Å›57.
[12]SenChen,MinhuiXue,LinglingFan,ShuangHao,LihuaXu,HaojinZhu,and
Bo Li. 2018. Automated poisoning attacks and defenses in malware detection
systems: An adversarial machine learning approach. computers & security 73
(2018), 326Å›344.
[13]Vitaly Chipounov, Volodymyr Kuznetsov, and George Candea. 2011. S2E: A
platform for in-vivo multi-path analysis of software systems. ACM Sigplan
Notices46,3 (2011), 265Å›278.
[14]YizhenDong,PeixinZhang,JingyiWang,ShuangLiu,JunSun,JianyeHao,Xinyu
Wang,LiWang,JinSongDong,andDaiTing.2019. ThereisLimitedCorrelation
between Coverage and Robustness for Deep Neural Networks. arXiv preprint
arXiv:1911.05904 (2019).
[15]Xiaoning Du, Xiaofei Xie, Yi Li, Lei Ma, Yang Liu, and Jianjun Zhao. 2019. Deep-
stellar:model-basedquantitativeanalysisofstatefuldeeplearningsystems.In
Proceedingsofthe201927thACMJointMeetingonEuropeanSoftwareEngineering
Conference and Symposium on the Foundations of Software Engineering . 477Å›487.
[16]Irwin S Dunietz, Willa K Ehrlich, BD Szablak, Colin L Mallows, and Anthony
Iannino.1997. Applyingdesignofexperimentstosoftwaretesting:experience
report. In Proceedings of the 19th international conference on Software engineering .
205Å›215.
[17]GordonFraserandAndreaArcuri.2012. Soundempiricalevidenceinsoftware
testing.In 201234thInternationalConferenceonSoftwareEngineering(ICSE) .IEEE,
178Å›188.
[18]Xiang Gao, Ripon Saha, Mukul Prasad, and Roychoudhury Abhik. 2020. Fuzz
TestingbasedDataAugmentationtoImproveRobustnessofDeepNeuralNet-
works. In 2020 IEEE/ACM 42nd International Conference on Software Engineering
(ICSE). IEEE.
[19]TimonGehr, MatthewMirman,DanaDrachsler-Cohen,PetarTsankov, Swarat
Chaudhuri, and Martin Vechev. 2018. Ai2: Safety and robustness certification of
neuralnetworkswithabstractinterpretation.In 2018IEEESymposiumonSecurity
and Privacy (SP) . IEEE,3Å›18.
[20]Simos Gerasimou, Hasan Ferit Eniser, Alper Sen, and Alper Cakan. 2020.
Importance-Driven Deep Learning System Testing. In 2020 IEEE/ACM 42nd Inter-
nationalConference onSoftwareEngineering (ICSE) . IEEE.
[21]Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. 2015. Explaining
andHarnessingAdversarialExamples.In InternationalConferenceonLearning
Representations (ICLR) .
[22]DivyaGopinath,GuyKatz,CorinaSPasareanu,andClarkBarrett.2017. Deepsafe:
A data-driven approach for checking adversarial robustness in neural networks.
arXiv preprint arXiv:1710.00486 (2017).
[23]DivyaGopinath,CorinaSPasareanu,KaiyuanWang,MengshiZhang,andSarfraz
Khurshid.2019. Symbolicexecutionforattributionandattacksynthesisinneural
networks.In 2019IEEE/ACM41stInternationalConferenceonSoftwareEngineering:
Companion Proceedings(ICSE-Companion) . IEEE,282Å›283.
[24]Joy Paul Guilford. 1950. Fundamental statistics in psychology and education.
(1950).[25]JianminGuo,YuJiang,YueZhao,QuanChen,andJiaguangSun.2018. Dlfuzz:Dif-
ferential fuzzing testing of deep learning systems. In Proceedings of the 2018 26th
ACM Joint Meeting on European Software Engineering Conference and Symposium
onthe FoundationsofSoftwareEngineering . 739Å›743.
[26]Mark Harman and Phil McMinn. 2009. A theoretical and empirical study of
search-based testing: Local, global, and hybrid search. IEEE Transactions on
SoftwareEngineering 36,2 (2009), 226Å›247.
[27]KaimingHe,XiangyuZhang,ShaoqingRen,andJianSun.2016. Deepresidual
learning for image recognition. In Proceedings of the IEEE conference on computer
vision and pattern recognition . 770Å›778.
[28]Pinjia He, Clara Meister, and Zhendong Su. 2020. Structure-Invariant Testing
for Machine Translation. In 2020 IEEE/ACM 42nd International Conference on
SoftwareEngineering (ICSE) . IEEE.
[29]Alain Hore and Djemel Ziou. 2010. Image quality metrics: PSNR vs. SSIM. In
201020thInternationalConference onPatternRecognition . IEEE,2366Å›2369.
[30]RenÃ©Just,DarioushJalali,LauraInozemtseva,MichaelDErnst,ReidHolmes,and
GordonFraser.2014. Aremutantsavalidsubstituteforrealfaultsinsoftware
testing?. In Proceedings of the 22nd ACM SIGSOFT International Symposium on
FoundationsofSoftwareEngineering . 654Å›665.
[31]Guy Katz, Clark Barrett, David L Dill, Kyle Julian, and Mykel J Kochenderfer.
2017. Reluplex:AnefficientSMTsolverforverifyingdeepneuralnetworks.In
InternationalConference onComputer AidedVerification . Springer, 97Å›117.
[32]Jinhan Kim, Robert Feldt, and Shin Yoo. 2019. Guiding deep learning system
testing using surprise adequacy. In 2019 IEEE/ACM 41st InternationalConference
onSoftwareEngineering (ICSE) . IEEE,1039Å›1049.
[33]D Richard Kuhn, Dolores R Wallace, and Albert M Gallo. 2004. Software fault
interactions and implications for software testing. IEEE transactions on software
engineering 30,6 (2004), 418Å›421.
[34]AlexeyKurakin,IanGoodfellow,andSamyBengio.2017. AdversarialMachine
LearningatScale.In InternationalConferenceonLearningRepresentations(ICLR) .
[35]HaretonKNLeungandLeeWhite.1989. Insightsintoregressiontesting(software
testing).In Proceedings.Conference onSoftwareMaintenance-1989 .IEEE,60Å›69.
[36]Zenan Li, Xiaoxing Ma, Chang Xu, and Chun Cao. 2019. Structural coverage
criteria forneuralnetworkscouldbemisleading. In 2019IEEE/ACM 41stInter-
national Conference on Software Engineering: New Ideas and Emerging Results
(ICSE-NIER) . IEEE,89Å›92.
[37]XiangLing,ShoulingJi,JiaxuZou,JiannanWang,ChunmingWu,BoLi,andTing
Wang. 2019. Deepsec: A uniform platform for security analysis of deep learning
model. In 2019IEEE SymposiumonSecurityand Privacy (SP) . IEEE,673Å›690.
[38]YingqiLiu,ShiqingMa,YousraAafer,Wen-ChuanLee,JuanZhai,WeihangWang,
and Xiangyu Zhang. 2018. Trojaning Attack on Neural Networks. In Proceedings
of the 25nd Annual Network and Distributed System Security Symposium (NDSS) .
[39]Lei Ma, Felix Juefei-Xu, Fuyuan Zhang, Jiyuan Sun, Minhui Xue, Bo Li, Chun-
yangChen,TingSu,LiLi,YangLiu,etal .2018. Deepgauge:Multi-granularity
testingcriteriafordeeplearningsystems.In Proceedingsofthe33rdACM/IEEE
InternationalConference onAutomatedSoftwareEngineering . 120Å›131.
[40]Shiqing Ma, YingqiLiu, Guanhong Tao, Wen-Chuan Lee, and XiangyuZhang.
2019. NIC:DetectingAdversarialSampleswithNeuralNetworkInvariantCheck-
ing..InProceedingsofthe25ndAnnualNetworkandDistributedSystemSecurity
Symposium(NDSS) .
[41]Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and
AdrianVladu.2018.Towardsdeeplearningmodelsresistanttoadversarialattacks.
InInternationalConference onLearning Representations (ICLR) .
[42]David Martin, John Rooksby, Mark Rouncefield, and Ian Sommerville. 2007.
â€™Goodâ€™organisationalreasonsforâ€™Badâ€™softwaretesting:Anethnographicstudyof
testing in a small software company. In 29th international conference on software
engineering (ICSEâ€™07) . IEEE,602Å›611.
[43]Glenford J Myers, Corey Sandler, and Tom Badgett. 2011. The art of software
testing. John Wiley& Sons.
[44] AugustusOdena,CatherineOlsson,DavidAndersen,andIanGoodfellow.2019.
TensorFuzz: Debugging Neural Networks with Coverage-Guided Fuzzing. In
InternationalConference onMachineLearning . 4901Å›4911.
[45]AJeffersonOffutt.1992. Investigationsofthesoftwaretestingcouplingeffect.
ACM Transactions on Software Engineering and Methodology (TOSEM) 1, 1 (1992),
5Å›20.
[46]Brandon Paulsen, Jingbo Wang, and Chao Wang. 2020. ReluDiff: Differential
Verification of Deep Neural Networks. In 2020 IEEE/ACM 42nd International
Conference onSoftwareEngineering (ICSE) . IEEE.
[47]Kexin Pei, Yinzhi Cao, Junfeng Yang, and Suman Jana. 2017. Deepxplore: Au-
tomated whitebox testing of deep learning systems. In proceedings of the 26th
SymposiumonOperatingSystemsPrinciples . 1Å›18.
[48]KexinPei,YinzhiCao,JunfengYang,andSumanJana.2017. Towardspractical
verification of machine learning: The case of computer vision systems. arXiv
preprint arXiv:1712.01785 (2017).
[49]LucaPulinaandArmandoTacchella.2010.Anabstraction-refinementapproachto
verificationofartificialneuralnetworks.In InternationalConferenceonComputer
AidedVerification . Springer, 243Å›257.
786Correlations betweenDeep Neural Network Model Coverage CriteriaandModel Quality ESEC/FSE â€™20, November8Å›13,2020,VirtualEvent, USA
[50]AditiRaghunathan,Jacob Steinhardt,andPercyLiang.2018. Certifieddefenses
against adversarial examples. In International Conference on Learning Representa-
tions.
[51]ArvindRamanathan,LauraLPullum,FarazHussain,DwaipayanChakrabarty,
and Sumit Kumar Jha. 2016. Integrating symbolic and statistical methods for
testingintelligentsystems:Applicationstomachinelearningandcomputervision.
In2016Design,Automation&TestinEuropeConference&Exhibition(DATE) .IEEE,
786Å›791.
[52]KarenSimonyanandAndrewZisserman.2014.Verydeepconvolutionalnetworks
for large-scaleimage recognition. arXiv preprint arXiv:1409.1556 (2014).
[53]GagandeepSingh,TimonGehr,MarkusPÃ¼schel,andMartinVechev.2019. An
abstract domain for certifying neural networks. Proceedings of the ACM on
ProgrammingLanguages 3,POPL(2019), 1Å›30.
[54]AndreaStocco,MichaelWeiss,MarcoCalzana,andPaoloTonella.2020. Misbe-
haviour Prediction for Autonomous Driving Systems. In 2020 IEEE/ACM 42nd
InternationalConference onSoftwareEngineering (ICSE) . IEEE.
[55]Youcheng Sun, Min Wu, Wenjie Ruan, Xiaowei Huang, Marta Kwiatkowska, and
DanielKroening.2018. Concolictestingfordeepneuralnetworks.In Proceedings
ofthe33rdACM/IEEEInternationalConferenceonAutomatedSoftwareEngineering .
109Å›119.
[56]Zeyu Sun, Jie M Zhang, Mark Harman, Mike Papadakis, and Lu Zhang. 2020.
Automatic Testing and Improvement of Machine Translation. In 2020 IEEE/ACM
42nd InternationalConference onSoftwareEngineering (ICSE) . IEEE.
[57]ChristianSzegedy,WojciechZaremba,IlyaSutskever,JoanBruna,DumitruErhan,
Ian Goodfellow, and Rob Fergus. 2014. Intriguing Properties of Neural Networks.
InInternationalConference onLearning Representations (ICLR) .
[58]Guanhong Tao, Shiqing Ma, Yingqi Liu, and Xiangyu Zhang. 2018. Attacks meet
interpretability: Attribute-steered detection of adversarial samples. In Advances
inNeural InformationProcessingSystems . 7717Å›7728.
[59]YuchiTian,KexinPei,SumanJana,andBaishakhiRay.2018. Deeptest:Automated
testing of deep-neural-network-driven autonomous cars. In Proceedings of the
40thinternational conference onsoftwareengineering . 303Å›314.
[60]YuchiTian,ZiyuanZhong,VicenteOrdonez,GailKaiser,andBaishakhiRay.2020.
Testing DNN Image Classifier for Confusion & Bias Errors. In 2020 IEEE/ACM
42nd InternationalConference onSoftwareEngineering (ICSE) . IEEE.
[61]Florian TramÃ¨r, Alexey Kurakin, Nicolas Papernot, Ian Goodfellow, Dan Boneh,
andPatrickMcDaniel.2018. Ensembleadversarialtraining:Attacksanddefenses.
InInternationalConference onLearning Representations .
[62]SakshiUdeshi,PryanshuArora,andSudiptaChattopadhyay.2018. Automated
directedfairnesstesting.In Proceedings ofthe33rd ACM/IEEE InternationalCon-
ference onAutomatedSoftwareEngineering . 98Å›108.
[63]Jonathan Uesato, Ananya Kumar, Csaba Szepesvari, Tom Erez, Avraham Rud-
erman, Keith Anderson, Nicolas Heess, Pushmeet Kohli, et al .2018. Rigorous
agentevaluation:Anadversarialapproachtouncovercatastrophicfailures. arXiv
preprint arXiv:1812.01647 (2018).
[64]Huiyan Wang, Jingwei Xu, Chang Xu, Xiaoxing Ma, and Jian Lu. 2020. DIS-
SECTOR: Input Validation for Deep Learning Applications by Crossing-layer
Dissection. In 2020 IEEE/ACM 42nd International Conference on Software Engi-
neering (ICSE) . IEEE.
[65]Shiqi Wang, Kexin Pei, Justin Whitehouse, Junfeng Yang, and Suman Jana. 2018.
Formal security analysis of neural networks using symbolic intervals. In 27th{USENIX}SecuritySymposium( {USENIX}Security18) . 1599Å›1614.
[66]Yisen Wang, Xingjun Ma, James Bailey, Jinfeng Yi, Bowen Zhou, and Quan-
quanGu.2019. Onthe convergence androbustnessofadversarial training.In
InternationalConference onMachineLearning . 6586Å›6595.
[67]James A Whittaker and Michael G Thomason. 1994. A Markov chain model
for statisticalsoftwaretesting. IEEE Transactions onSoftwareengineering 20,10
(1994), 812Å›824.
[68]MatthewWicker,XiaoweiHuang,andMartaKwiatkowska.2018. Feature-guided
black-box safety testing of deep neural networks. In International Conference
onToolsandAlgorithmsfortheConstructionandAnalysisofSystems .Springer,
408Å›426.
[69]Chaowei Xiao, Jun Yan Zhu, Bo Li, Warren He, Mingyan Liu, and Dawn Song.
2018. Spatially transformed adversarial examples. In 6th International Conference
onLearning Representations, ICLR2018 .
[70]CihangXie,JianyuWang,ZhishuaiZhang,ZhouRen,andAlanYuille.2018. Mit-
igatingAdversarialEffects Through Randomization. In InternationalConference
onLearning Representations .
[71]XiaofeiXie,LeiMa,FelixJuefei-Xu,MinhuiXue,HongxuChen,YangLiu,Jianjun
Zhao,BoLi,JianxiongYin,andSimonSee.2019. DeepHunter:acoverage-guided
fuzztestingframeworkfordeepneuralnetworks.In Proceedingsofthe28thACM
SIGSOFTInternationalSymposiumonSoftwareTestingand Analysis . 146Å›157.
[72]Weilin Xu, David Evans, and Yanjun Qi. 2018. Feature Squeezing: Detecting
AdversarialExamplesinDeepNeuralNetworks.In Proceedingsofthe25ndAnnual
Network and DistributedSystemSecuritySymposium(NDSS) .
[73]WeiYou,XuweiLiu,ShiqingMa,DavidPerry,XiangyuZhang,andBinLiang.
2019. SLF:fuzzingwithoutvalidseedinputs.In 2019IEEE/ACM41stInternational
Conference onSoftwareEngineering (ICSE) . IEEE,712Å›723.
[74]XiaoyongYuan,PanHe,QileZhu,andXiaolinLi.2019. Adversarialexamples:
Attacks and defenses for deep learning. IEEE transactions on neural networks and
learningsystems 30,9 (2019), 2805Å›2824.
[75]JieMZhang,MarkHarman,LeiMa,andYangLiu.2020. Machinelearningtesting:
Survey, landscapes and horizons. IEEE Transactions on Software Engineering
(2020).
[76]Mengshi Zhang, Yuqun Zhang, Lingming Zhang, Cong Liu, and Sarfraz Khur-
shid. 2018. DeepRoad: GAN-based metamorphic testing and input validation
frameworkforautonomousdrivingsystems.In Proceedingsofthe33rdACM/IEEE
InternationalConference onAutomatedSoftwareEngineering . 132Å›142.
[77]PeixinZHANG,JingyiWANG,JunSUN,GuoliangDONG,XinyuWANG,Xingen
WANG,JinSongDONG,andDaiTING.2020. White-boxfairnesstestingthrough
adversarial sampling. (2020).
[78]Xiyue Zhang, Xiaofei Xie, Lei Ma, Xiaoning Du, Qiang Hu, Yang Liu, Jianjun
Zhao, and Meng Sun. 2020. Towards Characterizing Adversarial Defects of
DeepLearningSoftwarefromtheLensofUncertainty.In 2020IEEE/ACM42nd
InternationalConference onSoftwareEngineering (ICSE) . IEEE.
[79]HushengZhou,WeiLi,ZelunKong,JunfengGuo,YuqunZhang,LingmingZhang,
BeiYu,andCongLiu.2020. DeepBillboard:SystematicPhysical-WorldTesting
ofAutonomousDrivingSystems.In 2020IEEE/ACM41stInternationalConference
onSoftwareEngineering (ICSE) . IEEE.
[80]Zhi Quan Zhou and Liqun Sun. 2019. Metamorphic testing of driverless cars.
Commun. ACM 62,3 (2019), 61Å›67.
787