HeteroRefactor: Refactoring for
Heterogeneous Computing with FPGA
Jason Lau*, Aishwarya Sivaraman*, Qian Zhang*,
Muhammad Ali Gulzar, Jason Cong, and Miryung Kim
University of California, Los Angeles
{lau, dcssiva, zhangqian, gulzar, cong, miryung}@cs.ucla.edu
*Equal co-first authors in alphabetical order
ABSTRACT
Heterogeneous computing with field-programmable gate-arrays
(FPGAs) has demonstrated orders of magnitude improvement in
computingefficiencyformanyapplications.However,theuseof
suchplatformssofarislimitedtoasmallsubsetofprogrammers
withspecializedhardwareknowledge.High-levelsynthesis(HLS)
toolsmadesignificantprogressinraisingthelevelofprogramming
abstraction from hardware programming languages to C/C++, but
theyusuallycannotcompileandgenerateacceleratorsforkernel
programs with pointers, memory management, and recursion, and
require manual refactoring to make them HLS-compatible. Besides,
experts alsoneed to provide heavilyhandcrafted optimizations to
improve resource efficiency, which affects the maximum operating
frequency, parallelization, and power efficiency.
We propose a new dynamic invariant analysis and automated
refactoringtechnique,called HeteroRefactor .First,HeteroRefac-
tormonitorsFPGA-specificdynamicinvariants—therequiredbit-
width of integer and floating-point variables, and the size of re-
cursivedatastructuresandstacks.Second,usingthisknowledge
of dynamic invariants, it refactors the kernel to make tradition-
ally HLS-incompatible programs synthesizable and to optimizethe accelerator’s resource usage and frequency further. Third, to
guarantee correctness, it selectively offloads the computation from
CPUtoFPGA,onlyifaninputfallswithinthedynamicinvariant.
On average, for a recursive program of size 175 LOC, an expertFPGA programmer would need to write 185 more LOC to imple-
mentanHLScompatibleversion,while HeteroRefactor automates
such transformation. Our results on Xilinx FPGA show that Het-
eroRefactor minimizes BRAM by 83% and increases frequency by
42%forrecursiveprograms;reducesBRAMby41%throughinteger
bitwidthreduction;andreducesDSPby50%throughfloating-point
precision tuning.
KEYWORDS
heterogeneouscomputing,automatedrefactoring,FPGA,high-level
synthesis, dynamic analysis
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation
onthe firstpage.Copyrights forcomponentsof thisworkowned byothersthan the
author(s)mustbehonored.Abstractingwithcreditispermitted.Tocopyotherwise,or
republish,topostonserversortoredistributetolists,requirespriorspecificpermission
and/or a fee. Request permissions from permissions@acm.org.
ICSE ’20, May 23–29, 2020, Seoul, Republic of Korea
©2020 Copyright held by the owner/author(s). Publication rights licensed to Associa-
tion for Computing Machinery.
ACM ISBN 978-1-4503-7121-6/20/05...$15.00
https://doi.org/10.1145/3377811.3380340ACM Reference format:
JasonLau*,AishwaryaSivaraman*,QianZhang*,MuhammadAliGulzar,
Jason Cong, Miryung Kim. 2020. HeteroRefactor: Refactoring for Heteroge-
neousComputingwithFPGA.In Proceedingsof42ndInternationalConference
onSoftwareEngineering,Seoul,RepublicofKorea,May23–29,2020(ICSE’20),
13 pages.
https://doi.org/10.1145/3377811.3380340
1 INTRODUCTION
In recent years, there has been a growing interest in architectures
thatincorporateheterogeneityandspecializationtoimproveperfor-
mance,e.g.,[ 12,14,16,22].FPGAsarereprogrammable hardware
that often exceeds the performance of general-purpose CPUs by
several orders of magnitude [ 8,33,57] and offer lower cost across
awidevarietyofdomains[ 7,9,17].Tosupportthedevelopmentof
sucharchitectures,hardwarevendorssupportCPU+FPGAmulti-
chippackages(e.g.,IntelXeon[ 35,58])andcloudproviderssupport
virtual machines with FPGA accelerators and application develop-
ment frameworks (e.g., Amazon F1 [3]).
AlthoughFPGAsprovidesubstantialbenefitsandarecommer-
cially available to a broad user base, they are associated with a
high development cost [ 64]. Programming an FPGA is a difficult
task; hence, it is limited to a small subset of programmers withspecialized knowledge on FPGA architecture details. To addressthis issue, there has been work on high-level synthesis (HLS) for
FPGAs [21]. HLS tools take a kernel written in C/C++ as input and
automatically generates an FPGA accelerator. However, to meet
the HLS synthesizability requirement, significant code rewriting isneeded. For example, developers must manually remove the use of
pointers,memorymanagement,andrecursion,sincesuchcodeis
notcompilablewithHLS.Toachievehighefficiency,theusersmust
heavilyrestructurethekerneltosupplyoptimizationinformation
manually at the synthesis time. Carefully handcrafted HLS opti-
mizationsarenon-trivialandoutofreachforsoftwareengineers
who usually program with CPUs [13, 15].
Our observation is that software kernels are often over-engi-
neeredinthesensethataprogramisgeneralizedtohandlemore
inputs than what is necessary for common-case inputs. While this
approachhasnoorlittleimpactontheprogramefficiencyonaCPU,
in an FPGA accelerator, the design efficiency could be impacted
considerably by the compiled size that depends on actual ranges of
valuesheldbyprogramvariables,theactualsizeofrecursivedata
structures observed at runtime, etc. For example, a programmermay choose a 32-bit integer data type to represent a human age,
whose values range from 0 to 120 in most cases. Consider another
example, where in 99% of executions, the size of a linked list is
4932020 IEEE/ACM 42nd International Conference on Software Engineering (ICSE)
bounded by 2k; however, the programmer may manually flatten it
to an array with an overly-conservative size of 16k.
We propose a novel combination of dynamic invariant analysis,
automated refactoring, and selective offloading approach, called
HeteroRefactor toguideFPGAacceleratorsynthesis.Thisapproach
guarantees correctness—behavior preservation, as itselectively of-
floads the computation from CPU to FPGA, only if the invariant is
met, but otherwise keeps the computation on CPU. It also does not
require having a representative data set for identifying dynamic in-
variants, as its benefit is to aggressively improve FPGA accelerator
efficiency for a common case input without sacrificing correctness.
Inthisapproach,aprogrammerfirstimplementsherkernelcodeinahigh-levellanguagelikeC/C++.Thensheexecutesthekernelcode
on existing tests or a subset of input data to identify FPGA-specific
dynamic invariants. HeteroRefactor automatically refactors the
kernelwithpointersintoapointerless,non-recursiveprogramto
make it HLS-compatible and to reduce resource usage by lowering
bitwidth for integers and floating-points, which in turn reduces
resource usages and increases the frequency at the FPGA level.
We evaluate HeteroRefactor on ten programs, including five
handwritten recursive programs, three integer-intensive programs
fromRosettabenchmark[ 84],andtwofloating-point-intensivepro-
grams from OpenCV [ 6]. We generate kernels targeting to a Xilinx
VirtexUltraScale+XCVU9PFPGAonaVCU1525Reconfigurable
Acceleration Platform [80] and achieve the following results:
(1)For recursive programs that are traditionally unsynthesiz-
able,HeteroRefactor refactorspointers andrecursionwith
the accesses to a flattened, finite-size array, making themHLS-compatible. On average, for a recursive program of
size175LOC,anexpertFPGAprogrammerwouldneedto
write185 moreLOCtoimplementanHLS-compatiblever-
sion, while HeteroRefactor requires no code change. Using
atightboundforarecursivedatastructuredepth,theresult-ingacceleratorisalsoresource-efficient—anacceleratorwith
acommon-caseboundof2ksizecanachieve83%decrease
in BRAM and 42% increase in frequency compared to the
baseline accelerator with an overly conservative size of 16k.
(2)Forintegers, HeteroRefactor performstransparentoptimiza-
tion andreduces the number ofbits by 76%, which leads to
25% reduction in flip-flops (FF), 21% reduction in look-up
tables, 41% reduction in BRAM, and 52% decrease in DSP.
(3)Forfloating-points, HeteroRefactor automaticallyreduces
the bitwidth while providing a probabilistic guarantee for a
user-specificqualitylossandconfidencelevel.Theoptimizedacceleratorcanachieveupto61%reductioninFF,39%reduc-
tion in LUT, and 50% decrease in DSP when an acceptable
precision loss is specified as 10−4at 95% confidence level.
In summary, this work makes the following contributions:
•Traditionally, automated refactoring has been used to im-prove software maintainability. We adapt and expand au-tomated refactoring to lower the barriers of creating cus-
tomized circuits using HLS and to improve the efficiency of
the generated FPGA accelerator.
•Whilebothdynamicinvariantanalysisandautomatedrefac-
toring have a rich literature in software engineering, we
designanovelcombinationofdynamicinvariantanalysis,automatedkernelrefactoring,andselectiveoffloading,for
transparent FPGA synthesis and optimization withcorrect-
nessguarantee,whichisuniquetothebestofourknowledge.
•WedemonstratethebenefitsofFPGA-specificdynamicin-
variant and refactoring in three aspects: (1) conversion of
recursive data structures, (2) integer optimization, and (3)
floating-point tuning with a probabilistic guarantee.
HeteroRefactor ’ssourcecodeandexperimentalartifactsarepub-
licly available at https://github.com/heterorefactor/heterorefactor.
2 BACKGROUND
Thissectionoverviewsadeveloperworkflowwhenusingahigh-
level synthesis (HLS) tool for FPGA and describes the types of
manual refactoring a developer must perform to make their kernel
synthesizable and efficient on FPGA.
2.1 Overview of FPGA Programming with HLS
Modern FPGAs include millions of look-up tables (LUTs), thou-
sands of embedded block memories (BRAMs), thousands of digital-
signal processing blocks (DSPs), and millions of flip-flop registers
(FFs) [78]. Each k-input LUT can implement any Boolean function
uptokinputs.AnFPGAmustbeprogrammedwithaspecificbinary
bitstream tospecifyalltheLUT,BRAM,DSP,andprogrammable
switch configurations to achieve the desired behavior. Fortunately,
HLS has been developed in recent years to aid the translation of
algorithmicdescriptions(e.g.,kernelcodeinC/C++)toapplication-
specific bitstreams [ 21,28,50]. Specifically, HLS raises the abstrac-
tion of hardware development by automatically generating RTL
(Register-Transfer Level) descriptions from algorithms. Generation
of FPGA-specific bitstream consists of a frontendresponsible for C
simulationanda backendresponsibleforhardwaresynthesis.Inthe
frontend,afteranalysisofC/C++code,HLS schedules eachopera-
tion from the source code to certain time slots (clock cycles). Next,
itallocates resources,i.e.,thenumberandtypeofhardwareunits
used for implementing functionality, like LUTs, FFs, BRAMs, DSPs,
etc.Finally,the bindingstagemapsalloperationstotheallocated
hardware units. This frontend process generates an RTL, whichis sent to a backend to perform logic synthesis, placement, and
routingtogenerateFPGAbitstreams.Softwaresimulationisfast;
however, hardware synthesis can take anywhere from a few hours
toacoupleofdays,dependingonthecomplexityofthealgorithm.
For example, even for tens of lines of code, hardware synthesis can
take hours for our subjects in Section 4.
Therefore,suchlonghardwaresynthesistimejustifiesthecost
ofmanualrewritingofkernelsforoptimizedresourceallocation,frequency, and power utilization. In other words, this motivates
HeteroRefactor toinvest timein a-prioridynamic analysisas op-
posed to just-in-time compilation to optimize FPGA, as frequent
iterations of hardware synthesis are prohibitively expensive.
2.2 Refactoring for High-Level Synthesis
HLS tools aim to narrow the gap between the software program
anditshardwareimplementation.WhileHLStoolstakekernelcode
in C or C++, a developer must perform a substantial amount ofmanual refactoring to make it synthesizable and efficient on anFPGA chip. Such refactoring is error-prone and time-consuming
494Refactoring 
for HLSSynthesizabilityPointer Support
Memory ManagementRecursion FunctionDevice & Host Interface*
Efﬁciency 
OptimizationParallelization*
Optimization for 
Data Movement*
Reducing 
Resource ConsumptionBit-width
PrecisionArray Size
*in related work
Figure 1: Overview of refactoring for high-level synthesis.
sincecertainlanguageconstructsforreadabilityandexpressiveness
inC/C++arenotallowedinHLS[ 25].Adevelopermusthaveinter-
disciplinary expert knowledge in both hardware and software and
knowobscureplatform-dependentdetails[ 15].Below,wecatego-
rizemanualrefactoringsforHLSintotwokinds:(1)synthesizabilityand(2)efficiencyoptimization.Inthispaper,wefocusonimproving
the Vivado HLS tool from Xilinx [ 21,79], which is the most widely
used FPGA HLSin the community, althoughour techniques can be
easilygeneralizedtootherHLStools,suchasIntelHLSCompiler,
Catapult HLS from Mentor, and CyberWorkBench from NEC.
2.2.1 Synthesizability.
Pointer support. To transformkernel code intoits equivalent
HLSsynthesizableversion,adevelopermustmanuallyeliminate
pointerdeclarationsandusages;thereareonlytwotypesofpointers
thatarenativelysupportedinHLS—pointerstohardwareinterfaces
such as device memory or pointers to variables. Pointer reinter-pretationislimitedtoprimitivedatatypes.Arraysofpointersor
recursive data structures are strictly forbidden in Vivado HLS.
Memory management and recursion. Because Vivado HLS
hasnocapabilityof memorymanagement,functioncallstomemory
allocation such as malloccannot be synthesized. Thus, develop-
ersmustcreateanoverlyconservative,large-sizedstaticarrayin
advance and manage data elements manually. Similarly, Vivado
HLScannotsynthesizerecursions.Thus,developersmustmanually
convert recursions into iterations or create a large stack to store
program states and manage function calls manually.
Device and host interface. Vivado HLS requires a strict de-
scription of parameters of the top-level function that acts as the
deviceandhostinterface.Thefunctioniscalledfromthehostandis
offloadedintoFPGA.Afunctionparametercanbeeitherascalar
or pointer to the device memory with a data size in the power of 2
bytes, and a developer must write specific pragmas—e.g., #pragma
HLS interface m_axi port=input touseAXI4interconnectin-
terfaceforpassingtheparameternamed inputtotheFPGAdesign.
2.2.2 Efficiency Optimization.
Parallelization. Reprogrammable hardware provides an inher-
ent potential to implement parallelization. Such parallelization can
be done through pipelining of different computation stages and
by duplicating processing elements or data paths to achieve aneffect similar to multi-threading. To guide such parallelization, a
developermustmanuallywriteHLSpragmassuchas #pragma HLS
pipeline and#pragma HLS unroll for suitable loops or must ex-
pose parallelization opportunities through polyhedral model-based
loop transformations [5, 23, 56, 85].HETERO REFACTOR  Workﬂow
HETERO REFACTOR
Recursive
Data
Structure
Support and
OptimizationDynamic Analysis and Software Refactoring
Invariant-
based
Char/Integer
Bit Width
OptimizationProbabilistic
Sampling-
based
Floating Point
OptimizationC/C++ Program Input Data
double Det(double **a, int n) {
  m = malloc(…); m[i] = …;  det += Det(m, n-1) * …;}</>
C/C++ 
ProgramTraditional Workﬂow
Rewrite 
by expertSynthesizable
FPGA Code
Resource Eﬃcient FPGA Code
Understand 
AlgorithmSeveral per‐
son-month Rewrite
CodeInput Data
</>
</>
</>
   Resource Eﬃcient FPGA    a
flopoco<22,8> Det(
    flopoco <22,8> **a, 
    ap_uint <9> n) { while(…){
  m=elemAlloc(…);array[m+i]=…;  push(Det, m, n-1);  continue; restoreHere:  det += pop(Det) * …; } }</>Selective Oﬄoading to FPGA .
bool deviceFail = false;if (hostCheck(…))  hostKernel(…);else  FPGAKernel(…, &deviceFail);if (deviceFail)  hostKernel(…);</>ROSE & Kvasir-based
Invariants Detection
ROSE-based 
Automatic Refactoring
Figure 2: Approach overview of HeteroRefactor .
Optimization of data movement. Accessing of the device
memory can be more efficient by packing bits into the width of
DRAMaccessof512bits.Tooverlapcommunicationwithcompu-
tation,adevelopercouldexplicitlyimplementadoublebuffering
technique[ 15].Tocachedata,developersneedtoexplicitlystore
them on chip through data tiling, batching or reusing [10, 56, 65].
Reducingresourceconsumption. Provisioningmoreprocess-
ing elements or a larger cache will require using more on-chip
resources, limiting the potential of parallelization and data move-
mentoptimizationsbyduplicatingprocessingelementsoradding
cache. A higher resource utilization ratio can lower the maximum
operatingfrequencyandconsumemorepower;thus,itdegrades
the performance and efficiency. Besides, a resource-efficient design
is economical as it can to be implemented on a smaller FPGA chip.
Traditionally, developers allocate integers and floating-point vari-
ables with a fixed size bitwidth large enough for all possible input
values, or create a static array for the largest-possible size. Such
a practice may cause wasting on-chip resources. In particular, in
modernapplicationssuchasbigdataanalyticsandMLapplications
whereon-chipresourceusageisinput-dependent,FPGAresource
optimization becomes increasingly difficult.
Figure1illustratesournewcontributions,highlightedwith bold
andred, relative to the prior HLS literature. There exists many
automated approaches for generating device and host interfaces
[20,61,83], exploring parallelization opportunities [ 24,34,46,83],
495Integer Recursive Data Structure Floating PointsCollect 
Invariants TransformationGuard 
CheckingSource C/C++ Program
Refactoring-based instrumentation Kvasir-based instrumentation
Rewrite 
Memory 
Management
Monitor
malloc FailureInput Check
on Host
Selective Oﬄoading Program to FPGAData
Structure ShapeRecursion
Depth
Transformed Device Program</>
</>Modify
Pointer
AccessConvert
RecursionModify
TypeModify
OperatorAssess
FP Error
Pre-transformed Programs
with Diﬀerent Precisions
Precision Loss from
Diﬀerential Execution</>
Monitor
Stack OverﬂowIntermediate
Check on Device
</>Probabilistic Veriﬁcation
Oﬄoading to FPGA </>Value Range Unique Elements
Modify Integer Type
Figure 3: HeteroRefactor incorporates three techniques—dynamic invariant detection, kernel refactoring, and selective of-
floading with guard checking. Its profiling concerns three aspects: (1) the length of recursive data structures, (2) required
integer bitwidth, and (3) required floating-point bitwidth to meet a specified precision loss.
and optimizing data movement [ 10,20,24,46,55,56]. But gen-
eral methods for reducing resource consumption, pointer support,
memory management and recursion support remain as open re-
search questions and no automated kernel refactoring exists yet.
HeteroRefactor addresses three important scopes of such refactor-
ing transformations: (1) converting a program with pointers and
recursion to a pointerless and non-recursive program by rewriting
memorymanagementandfunctioncalls,(2)reducingon-chipre-
source consumptionof integer bitwidth,and (3) reducingon-chip
resource consumption by tuning floating-point precision.
3 APPROACH
HeteroRefactor ,asshowninFigure2,isanovelend-to-endsolution
that combines dynamic invariant analysis, automated refactoring,
andselectiveoffloading forFPGA.Itaddressesthree kindsofHLS
refactorings: rewriting a recursive data structure to an array of
finite size (Section 3.1); reducing integer bitwidth (Section 3.2); and
tuningvariable-widthfloating-pointoperations(Section3.3).All
three refactorings are based on the insight that a-priori dynamicanalysis improves FPGA synthesizability and resource efficiency
andthatdynamic,input-dependentoffloadingcanguaranteecor-
rectness.Figure3detailsthethreecomponentsthatworkinconcert:
(A) instrumentation for FPGA-specific dynamic invariant analysis,
(B) source-to-source transformation using dynamic invariants, and
(C) selective offloading that checks the guard condition when of-
floading from CPU to FPGA. The first two kinds of refactorings
followsimilarimplementationforselectiveoffloadingusingaguard
condition check, described in Section 3.4. For floating-point opera-
tions, our dynamic analysis provides a probabilistic guarantee that
the precision loss is within a given bound.
3.1 Recursive Data Structure
Many applications use recursive data structures built on malloc,
free, and recursive function calls. As mentioned in Section 2.2,
HLS tools have strict restrictions on the types of pointers allowed
and do not support memory allocation and recursion. For example,Vivaldo HLS throws the following error for Figure 4a: an unsynthe-
sizable type ’[10 x %struct.Node.0.1.2]*. This severely limits the type
of programs that can be automatically ported for heterogeneous
computing.ExpertFPGAdevelopersmanuallyrewritetherecursive
data structure into a flattened array to be HLS-compliant; however,as they may not know the common maximum size required for the
application,theyoftenover-provisionanddeclareanunnecessarily
large size. They also have to manually convert recursion into loop
iterationsandover-provisionthestackrequiredforkeepingtrack
of program state involved in recursive calls.
HeteroRefactor uses a source to source compiler framework,
ROSE [59] toinstrument code foridentifying thesize of recursive
data structures and the corresponding stack depth and performs
source-to-source transformation based on the size.
3.1.1 Refactoring-basedInstrumentation. HeteroRefactor in-
struments memory allocation and de-allocation function calls (e.g.,
allocation of a linked list node), and adds tracing points at the
entry and exit of recursive functions to monitor a stack depth. Het-
eroRefactor then determines the number of elements allocated
for each data structure based on the collected sizes. In Figure 4a,
HeteroRefactor sets a tracing point at line 3 to record the number
ofallocatednodesandanothertracingpointatline16torecordthe
released count. To monitor the recursion depth, HeteroRefactor
insertstracingpoints callatthefunctionentrypointand retat
thefunctionexitpointofeachrecursivefunction.InFigure4a, call
is inserted before line 6, and retis inserted at line 6 and after line
9.HeteroRefactor then maintains a variable stack_size for each
function, which is incremented every time the program reaches
callanddecrementedwhenitreaches ret.Thehighestvalueat-
tainedby stack_size duringexecutionisreportedandusedasthe
bound for a flattened array and the corresponding stack.
3.1.2 Refactoring. HeteroRefactor isimplementedbasedon
ROSE [59] to rewrite recursive data structures. It takes C/C++ ker-
nelcodeandthearraysizesandrecursiondepthsfoundviadynamicanalysis,andoutputsanHLS-compatibleversionwithon-chipmem-
ory allocation, removes all pointers except for those with native
4961struct Node { Node *left, *right; int val; };
2void init(Node **root) {
3*root = (Node *)malloc (sizeof(Node)); }
4void insert(Node **root, int n,int *arr);
5void traverse(Node *curr) {
6if(curr == NULL) return ;
7visit(curr ->val);
8traverse (curr ->left );
9traverse(curr->right); }
10void top(int n,int *output_if) {
11#pragma HLS interface m_axi port=output_if
12 Node *root; init(&root); // ...
13 int values[3] = {5, 4, 3};
14 insert(&root, 3, values);
15 int *curr = output_if; traverse(root); // ...
16 free (root); }
(a) Original kernel code using pointers and memory allocation.
1bool guard_error = false;
2struct Node { Node_ptr left, right; int val; };
3struct Node Node_arr[NODE_SIZE];
4typedef unsigned int Node_ptr;
5Node_ptr Node_malloc(size_t size);
6void Node_free(Node_ptr); // buddy allocation
7void init(Node_ptr *root) {
8*root = (Node_ptr)Node_malloc (sizeof(Node));
9if(!root) guard_error = true;}
10void insert(Node_ptr *root, int n,int *arr);
11void traverse(Node_ptr curr) {
12 stack<context> s(TRAVERSE_STACK_SIZE, {curr:curr,loc:0});
13 while (!s.empty()) { context c = s.pop(); goto L{c.loc};
14 L0:if (c.curr == NULL) continue ;
15 visit(Node_arr [c.curr -1]. _data .val);
16 if(s.full()) { guard_error=true; return;}
17 c.loc =1;s.push (c);s.push ({curr :Node_arr [
18 c.curr -1]. _data .left ,loc:0});continue ;// traverse(left)
19 L1:// traverse(right) ...
20 L2:; } }
21void top(int n,int *output_if, bool *fail) {
22#pragma HLS interface m_axi port=output_if
23 Node_ptr root; init(&root); // ...
24 int values[3] = {5, 4, 3};
25 insert(&root, 3, values);
26 int *curr=output_if; traverse(root); // ...
27 Node_free (root); *fail = guard_error; }
(b) Refactored kernel code (schematic).
Figure 4: Example of recursive data structures: binary tree.
HLSsupport(tobeexplainedfurtherunderRule2),andrewrites
recursive functions. The transformation is semantics-preserving
and consists of the following transformation rules:
Rule1:RewriteMemoryManagement.Toreplacecallsto malloc
andfree,foreachdatatype,wepre-allocateanarraywhosesize
is guided by instrumentation (line 3 in Figure 4b). The per-type
allocationstrategywithanarrayisbasedontworeasons—HLSonly
supportspointerreinterpretationonprimitivedatatypes,anditcanoptimizearrayaccessesifthesizeofoneelementisknown.Foreach
node allocation and de-allocation, we implement a buddy memory
system[54]andallocatefromthearray.Thebuddymemorysystem
requires less overhead and has little external fragmentation [ 77],
making it suitable for FPGA design. We identify all calls to malloc
andfree,therequestedtypesandelementcounts,andtransform
themintocallstoourlibraryfunction Node_malloc (line8inFig-
ure4b)whichreturnsanavailableindexfromthearray.Section4.1
detailsperformancebenefitsintermsofincreasedfrequencyand
reducedresourceutilizationusinganarraysizeguidedbydynamic
analysis rather than declaring an overly conservative size.
Rule2:ModifyPointerAccesstoArrayAccess.Thereareonlytwotypesofpointers nativelysupported inHLS,andwedonotneed
to convert them into array access. One is a pointer of interfaces,whichwecanidentifybylookinguppragmasinthecode(line22in
Figure 4b).Second isa pointerto variables,which canbe detected
by finding all address-of operators or array references in the code.
Before modifying pointer access to array access, we identify these
nativelysupportedpointersusingabreadth-firstsearchonthedata
flow graph and exclude them from our transformation.
Wetransformthepointerstoanunsignedintegertypethattakes
value less than the size of the pre-allocated array from dynamic
analysis.Thisintegerrepresentstheoffsetofthepointedelement
in the pre-allocated array. There are three locations where this
type of transformation is applied: (1) variable declarations (line 23
Figure 4b), typecasting (line 8 Figure 4b), and function parameters
(line 10 Figure 4b) and the return value in both declarations and
thedefinition.Weperformabreadth-firstsearchonthedataflow
graphtopropagatethetype changes.Sinceweuseanarrayoffset
toreferenceallocatedelements,weneedtochangeallpointerderef-
erencesintoarrayaccesseswiththerelativeindex.Wetransform
indirection operator ( *ptr) and structure dereference operators
(ptr->, ptr->* ) into array accesses with pointer integer as the
array index. Similarly, the subscript operators ( ptr[]) are trans-
formed into array accesses with the pointer integer added with the
given offset as the array index. For example, we modify pointer
access (line 7 in Figure 4a) to array access (line 15 in Figure 4b).
Rule3:ConvertRecursiontoIteration.Totransformrecursive
functions into non-recursive ones, we create a stack (line 12 Fig-
ure 4b) foreach function with alllocal variables. The depth ofthe
allocated stack is determined through the dynamic analysis step.
All references to local variables are transformed into references to
elementsonthetopofthestack(line14Figure4b).Tosimulatethe
savedcontextoftheprogramcounterandreturnvalueinaCPU
callstack,wereservetwomembervariablesinourstacktostore
the location indicating which line of code we need to restore to,
and the return value of the called function.
Withastack,wecanimplementfunctioncallslikeinCPU.En-
teringafunctionpushesthecurrentcontextandnewparameters
to the top of the stack (line 17 Figure 4b), then continue to the first
lineofthefunction(line18Figure4b).Afunction returnwrites
the return value to the stack, pops the top item from the stack, and
returns to the saved context (lines 13 Figure 4b).
3.2 Integer
3.2.1 Kvasir-based Instrumentation. Daikon is a dynamic in-
variantdetectiontool[ 27]thatreportslikelyprograminvariants
duringaprogram’sexecution.Itconsistsoftwoparts:(a)alanguage-specificfront-endand(b)alanguage-independentinferenceengine.
A front end instruments the program and extracts the program
stateinformationbyrunningtheprogram.FPGAkernelsarepro-
grammed in C/C++ for HLS; hence, we use Kvasir [ 27], a C/C++
front-end for Daikon, to instrument the target program’s binary.
3.2.2 FPGA-SpecificInvariants. Daikonisoftenusedforgen-
eralprogramcomprehensionandtesting,andthereforeitoutputs
invariants such as an array size or binary comparison, e.g., i>0,
i<0, size(array)=0, size(array)>0 . However, such general
invariants must be adapted for the purpose of FPGA synthesis. For
example, reducing a variable bitwidth leads to resource reduction
in FPGA directly [45].
4971int weakClassifier(int stddev, int coord[12], int haarC, int w_id);
2int cascadeClassifier(int SUM1_data[IMG_HEIGHT][IMG_WIDTH],
3 int SQSUM1_data[IMG_HEIGHT][IMG_WIDTH], MyPoint pt) { // ...
4int stddev = int_sqrt(stddev); // ..
5}
(a) Original kernel code using int.
1bool guard_error = false;
2void guard_check(ap_int<65> value, int size, int sign) {
3#pragma HLS inline off
4if(sign==1) { if(value<0) {
5 if(value < -(1LL<<(size-1)) guard_error = true;
6}else {/*...*/ }}else {/*...*/ }}
7int weakClassifier(ap_uint <9> stddev, ap_uint <23> coord[12], ap_uint <7>
haarC, ap_uint <8> w_id);
8int cascadeClassifier(ap_uint <18> SUM1_data[HEIGHT][WIDTH],
9 ap_uint <18> SQSUM1_data[HEIGHT][WIDTH], MyPoint pt) { //...
10 ap_uint <18> stddev = int_sqrt(stddev);
11 guard_check (ap_int <65>( int_sqrt (stddev )),18,0) ;// ...
12}
(b) Refactored kernel code using ap_(u)int.
Figure 5: Example of integers: face detection.
Therefore, to optimize FPGA synthesis, we design three types
of FPGA-specific invariants: (1) the minimum and maximum value
of a variable based on a range analysis, (2) the number and type
of unique elements in an array, and (3) the size of an array. For
example,firstconsiderFigure5a.Aprogrammermayover-engineer
and use a 32-bit integer by default, which is a higher bitwidth than
whatisactuallynecessary.Whiletheinstructionsetarchitecture
(ISA)forCPUdefinesintegerarithmeticsat32bitsbydefault,in
FPGA, individual bitwidths could be programmed.
3.2.3 Refactoring. RuleModifyVariableType.Toconvertan
integer to an arbitrary precision integer, we leverage ap_uint <k>
orap_int<k>providedbyVivadoHLS,whichdefinesanarbitrary
precisionintegerof kbits.Asanexample,theinput haar_counter
to method weakClassifier in Figure 5a is declared as a 32-bit
integer by the programmer. However, suppose that HeteroRefac-
torfinds that it has a min value of 0 and a max value of 83—it then
only needs 7 bits instead of 32 bits. It parses the program’s ASTus-
ing ROSE [ 59], identifies the variable declaration node for stddev,
coord,haarC,and w_id,andthenmodifiesthecorrespondingtype
as shown in Figure 5b.
3.3 Floating Point
UnlikethereductionofintegerbitwidthinSection3.2,reducingthe
bitwidthforfloating-point(FP)variablescanleadtoFPprecision
loss. Estimating the error caused by lowering a FP bitwidth can bedone reliably only through differential execution, because existing
static analysis tends to over-approximate FP errors. Therefore, we
design a new probabilistic,differential execution-based FP tuning
approach,which consistsoffour steps:(1)source-to-source trans-
formation for generating program variants with different biwidths,
(2) estimation of the required number of input data samples based
onHoeffding’sinequality[ 37],(3)testgenerationanddifferential
execution, and (4) probabilistic verification for FP errors.
Prior work on reducing FP precision in CPU [ 62,63] used dy-
namicanalysis;however,sincetheyuseagoldentestset,theydo
notprovideanyguaranteeonrunningthereducedprecisionpro-
gram on unseen inputs. The key insight behind HeteroRefactor ’s1float l2norm(float query[], float data[], int dim) {
2float dist = 0.0;
3for (int j = 0; j < dim; j++)
4 dist += ((query[j] - data[j]) * (query[j] - data[j]));
5return sqrt(dist); }
(a) Original kernel code using float.
1using namespace thls; typedef policy_flopoco <16,5>:: value_t LOWBIT;
2float low_l2norm(float query[], float data[], int dim) {
3LOWBIT dist = 0.0;
4for (int j = 0; j < dim; j++) {
5 LOWBIT fp_query_j = to<LOWBIT , policy>(query[j]);
6 LOWBIT fp_data_j = to<LOWBIT , policy>(data[j]);
7 LOWBIT fp_neg_1 = neg(fp_data);
8 dist += (fp_query + fp_neg_1) * (fp_query + fp_neg_1); }
9return sqrt(to<float>(dist)); }
10int main() {
11 for (...) { // ...
12 float highValue = l2norm(args[]);
13 float lowValue = low_l2norm(args[]);
14 float error = highValue - lowValue;
15 if(fabs(error) > acceptableError) Failed++; else Passed++; }
16 if(double(Passed) / Samples > requiredProbability) {
17 /* Passed verification */ }else {/* Failed verification */ }}
(b)Refactoredkernelcodethatperformsdifferentialexecutionand
probabilistic verification (schematic).
Figure 6: Example of FP numbers: l2norm from KNN.
probabilistic verification approach is that we can draw program
inputsamplestoempiricallyassesswhethertherelativeerrorbe-
tween a low precision program and a high precision program is
within a given acceptable precision loss ewith probability p. Given
aprogramwithhigh-precisionFPoperations, hp,weconstructa
lower precision copy of the program, lp, by changing the corre-
sponding type of all FP variables, constants and operations. Foreach input
i∈I, we compute the actual error between the high
andthelowbitwidthvariants, hp(i)−lp(i).Wethencheckwhether
this FP error is within the acceptable precision loss e, indicated by
a predicate ci=(hp(i)−lp(i)<e)which forms a distribution B.
When the empirical measurement ciof the given input samples is
higher than the target probability p, the verification is passed.
HeteroRefactor takesasinputs:(1)aprogram,(2)asetofsam-
pled inputs Ior a statistical distribution, (3) an acceptable loss
(error)e,(4)arequiredprobability p,(5)arequiredconfidencelevel
(1−α)and(6)deviation ϵ.WeuseHoeffding’sinequality[ 37]to
computetheminimumnumberofsamplesrequiredtosatisfythe
givenconfidencelevel(1- α)anddeviation ϵ.Equation1showsthe
probability that the empirical measurement ciof the distribution B
deviatesfromitsactualexpectation E[ci]byϵ,whichshouldbeless
thanαtoachieveourtarget.SimilartoSampsonetal.’sprobablistic
assertion [ 66], we use Hoeffding’s inequality since it provides a
conservative,generalboundfor expectationsof anyarbitrarydis-
tribution and relies only on probability and deviation. Therefore,
itissuitableforoursituationwherewehavenopriorknowledge
abouttheFPlossdistribution,incurredbyreducingthebitwidth.
Equation 2 calculates the minimum number of samples required to
verify whether the error is within the acceptable loss.
P[|ci−E[ci]| ≥ϵ]≤2e−2nϵ2(1)
n≥ln(2/α)/(2ϵ2) (2)
498Forexample,whenauserwantstheactualFPerrorbetweenthe
originalversion(Figure6a)andthelowprecisionvariant(Figure6b)
to be less than 10−4with 95% probability, 95% confidence level and
0.03 deviation, the minimum number of samples required is 2049.
During differential testing with respect to input I, if the propor-
tionciofpassingsamplesto |I|isgreaterthan p,weprobabilistically
guaranteethatitissafetolowertheFPprecisiontothegivenlower
bitwidth.Thefollowingtransformationrulesareappliedtoidentify
a lower precision configuration for FP variables.
Rule 1. Duplicate Method and Modify Type. To create multiple
copiesofmethod l2norminFigure6a, HeteroRefactor traversesits
ASTandredefinesthetypeofvariable query,data,and dimorigi-
nally declared as floatusing thls::fp_flopoco<E, F> , whose
library is based on Thomas’ work on templatized soft floating-point
type for HLS [75].Eis the number of exponent bits and Fis the
number of fractional bits (excluding 1 implicit bit). For example,
thls::fp_flopoco<8,23> is 32 bit floattype, and thls::fp_-
flopoco<5,16> uses22bitsintotal(5forexponent,16forfraction
and 1 for implicit bit) instead.
Rule 2: Modify Arithmetic Operators. While addition, multi-
plication,anddivisionoperatorsareimplementedby thls::fp_-
flopoco<E,F> ,subtractionisnotsupported[ 75].Hence,wecon-
vert subtraction in l2norm(line 4 in Figure 6a) to corresponding
negandadd, i.e.,subtract(a,b)=add(a,neд(b))using a variable
fp_neg_1 to store the intermediate result (lines 7-8 in Figure 6b).
Rule3:AssessFPErrorforDifferentialExecution.Wedefine
a skeleton method thatcomputes the relative error andprobabilis-
ticallyverifiesiftheerroriswithintheusergivenacceptableloss
(lines 11-17 Figure 6b). This involves adding code to invoke the
original and generated low precision variants of the function.
3.4 Selective Offloading with Guard Check
To selectively offload the computation that fits the reduced size,
we insert guard conditions in the host (function sending data from
CPU to FPGA) and the kernel (algorithm) to be mapped to FPGA.
For recursive programs, as illustrated at line 9 and line 16 in Fig-
ure 4b, we insert a guard condition at Node_malloc . The condition
setsaglobalvariable guard_error totrue,ifthearrayisfulland
moreallocationisrequired.Similarly,theglobalvariableissetto
true,ifthestacksizegrowsbeyondthereducedsize.Forinteger-
intensiveprograms,asshownatline11inFigure5b,weaddaguardconditioninthekernelandhostprogram.Weguardtheuseofeach
input,output,andintermediatevalueinthekerneltoproactively
prevent overflow (lines 4-6 in Figure 5b). For this, we first identify
all expressions containing the reduced bitwidth variables, and if
the expression contains binary operations, we insert a guard.
4 EVALUATION
Our evaluation seeks to answer the following research questions:
RQ1DoesHeteroRefactor effectively enlarge the scope of HLS
synthesizability for recursive data structures?
RQ2Howmuchmanualeffortcan HeteroRefactor savebyauto-
matically creating an HLS-compatible program?
RQ3How much resource reduction does HeteroRefactor pro-
vide for recursive data structures, integer optimization, and
floating-point optimization?Benchmarks. We choose ten programs, listed in Table 1 as
benchmarks for our main evaluation. For recursive data struc-
tures,weusethefollowingfivekernels:(R1) Aho-Corasick [2]is
a string pattern searching algorithm that uses breadth-first search
withadynamicqueue,arecursiveTrietree[ 26]andafinitestate
machine. (R2) DFSis depth-first search implemented with recur-
sion. (R3) Linked List is insertion, removal, and sorting on a
linked list. (R4) Merge Sort is performed on a linked list. (R5)
Strassen’s [40] is a recursive algorithm for matrix multiplica-
tion. For integer optimization, we use face detection and 3D
rendering from Rosetta [ 69,84] (I6 and I7). We also write (I8)
bubble sort .ForFPbitwidthreduction,wemodifytwoprograms—
(F9)KNN-l2norm and (F10) RGB2YUV from OpenCV examples [6].
Thesesubject programsdemonstrate HeteroRefactor ’scapabil-
ityon improving synthesizablity andresource efficiency .For recur-
sive data structures, the original programs are not synthesizable
and cannot run on FPGA prior to our work. Thus, we compare our
results against manually ported code in terms of human effort and
resource utilization. The hand-optimized programs are written by
experienced graduate students from an FPGA research group at
UCLA.Originalprogramsforintegerandfloating-pointcanalready
runonFPGA.Forintegers,wecompareresourceutilizationtoboth
original(unoptimized)andmanuallyoptimizedprograms,which
are directly from Rosetta [ 69,84]. For floating-point, there is no
comparisonwithhand-optimizedversions,becauseamanualop-
timizationattemptwillbesimilartotheverificationprocedureof
HeteroRefactor .
Though the code size of subject programs looks small, these
programsarerathersizablecomparedagainstwell-knownFPGA
HLSbenchmarks[ 36,60].Similartocreatinganewinstructiontype
intheCPUinstructionsetarchitecture,theroleofFPGAistocreatehighperformance,customoperatorsatthehardwarecircuitlevel.Infact,inausualFPGAdevelopmentworkflow,developersinstrumentsoftwareonCPU,findoutitshotspotcorrespondingtotensoflines
of code, and extract it as a separate kernel for FPGA synthesis.
Therefore,ourworkcannotbejudgedunderthesamescalability
standard used for pure software refactoring (e.g., handling GitHub
projects with millions of lines of code).
Experimental Environment. Allexperimentsareconducted
onamachinewithIntel(R)Core(TM)i7-8750H2.20GHzCPUand16
GB of RAM running Ubuntu 16.04. The dynamic invariant analysis
isbasedoninstrumentationusingDaikonversion5.7.2withKvasir
as front-end. The automated refactoring is implemented based on
ROSE compiler’s version 0.9.11.0. The refactored programs are
synthesizedto RTLtoestimate theresourceutilization byVivado
DesignSuite2018.03.ThegeneratedkernelsaretargetedtoaXilinx
VirtexUltraScale+XCVU9PFPGAonaVCU1525Reconfigurable
Acceleration Platform.
4.1 Recursive Data Structure
Toanswer RQ1,weassesshowmanyrecursivedatastructurepro-
gramsarenowsynthesizableusing HeteroRefactor thatfailcompi-
lationwithVivadoHLS.For RQ2,wemeasuremanualportingeffort
as LOC and characters in the code. For RQ3, we assess reduction in
resourceutilizationandincreaseinfrequencyoftheresultingFPGA
499Table 1: Resource utilization for HeteroRefactor
ID/Program #LUT #FF BRAM DSP
R1/ Orig Not Synthesizable
Aho- Manual 32874666 1939 7
Corasick HR-8K 54925085 678 10
HR-2K 52345006 206 10
R2/ Orig Not Synthesizable
DFS Manual 14711961 221 0
HR-8K 26342901 254 0
HR-2K 25632881 69 0
R3/ Orig Not Synthesizable
Linked Manual 29933732 534 0
List HR-8K 37714044 318 0
HR-2K 36553936 83 0
R4/ Orig Not Synthesizable
Merge Manual 27552878 519 0
Sort HR-8K 27512958 367 0
HR-2K 26032951 105 0
R5/ Orig Not Synthesizable
Strassen’s Manual 2163113722 919 12
HR-8K 2030314899 223 12
HR-2K 1959114654 68 12
I6/ Orig 11325 5784 49 39
Face Manual 10158 4800 49 37
Detection HR 10298 4770 47 28
I7/ Orig 38282033 123 36
3D Manual 22391357 67 12
Rendering HR 1907 878 39 9
I8/ Orig 313125 2 0
Bubble Manual 306125 1 0
Sort HR 302125 1 0
F9/ Orig 8884318591 30 32
KNN- p e=10−2
l2norm 0.95 8016315257 30 16
0.99 8222815626 30 16
0.999 8222815626 30 16
p e=10−4
0.95 8895217102 30 32
0.99 8895217102 30 32
0.999 8895217855 30 32
p e=10−6
0.95 8884318591 30 32
0.99 8884318591 30 32
0.999 8884318591 30 32
F10/ Orig 398444 73437 30 288
RGB2YUV p e=10−4
0.95243516 28379 30 144
0.99250044 28827 30 144
0.999 250044 28827 30 144
p e=10−5
0.95304956 49468 30 144
0.99304956 49468 30 144
0.999 311532 49964 30 144
p e=10−6
0.95372236 66381 30 288
0.99398444 73437 30 288
0.999 398444 73437 30 288Table2:Recursivedatastructurekernels,noextracodewith
HeteroRefactor v.s. effort for manual refactoring
ID/ProgramOrig.
LOCManual
LOCΔ
LOCOrig.
CharsManual
CharsΔ
Chars
R1/A.-C. 190 291 33% 5673 8776 35%
R2/DFS 86 198 57% 2236 5699 61%
R3/L. List 131 235 44% 3061 6686 54%
R4/M. Sort 128 342 63% 3267 9124 64%
R5/Strassen’s 342 735 53% 1002640971 76%
Geomean 49% 56%
design code, compared to the FPGA design based on a manually
written kernel with a conservative size.
Table 2 shows how many lines of code (Manual LOC) and char-
acters (Manual Chars) we need to write in total, if we manually
refactor a synthesizable version in Vivado HLS. These manual ver-
sions have only a naïve allocator that returns the first unallocated
element in the array. If we add a buddy memory system to the
manuallyrefactoredcodetoachievethesamefunctionalityasin
HeteroRefactor , about 100 additional lines of code are required,
and thus manual refactoring effort would be even greater.
To evaluate reduction in resource utilization, we instrument
the programs using randomly generated input data with typicalsize of
1k,2k,4kor8k. The profiled information is then passed
toHeteroRefactor , which automatically generates Vivado HLS-
compilablevariantsoftheoriginalprogram.AsmentionedinSec-
tion3.1,FPGAprogrammersmanuallytransformpointertonon-
pointer programs with an overly conservative estimate for the size
of the data structure. To compare traditional code rewriting to Het-
eroRefactor ,wemanuallyconvertandoptimizetheprogramsin
Table 2 for a conservative data structure size of 16k.
Rows R1-R5 in Table 1 summarize reduction in pre-allocated ar-
ray size and resource utilization for each of these variants. Manual
shows resource usage numbers for the hand-optimized program
with a conservative size of 16k, and HR-8kandHR-2kshow re-
sourceusageof HeteroRefactor with 8kand2ktypicaldatasize.If
the typical input data size is 2k, there is 83% reduction on average
in BRAM, compared to the manually refactored program with a
conservativearraysize.ThisdecreaseissignificantbecauseVivado
HLS stores most of the large array in BRAM. On the other hand,
thereisanincreaseof302unitsinLUTand494unitsinFFonaver-
age compared to the hand-optimized version. This small overhead
is caused by the fixed-sized buddy memory system which does not
increase, as the user design scales.
We implement FPGA accelerator on-board with a target fre-
quencyof300MHz.Figure7reportsthemaximumoperatingfre-
quency after placement and routing by Xilinx Vivado for eachtypical input data size. The frequency is calculated statically byusing the worst negative slack (WNS) in the report file:
Fmax=
1/(1/300MHz+WNS).Iftheinputrecursivedatastructuresizeis
2k, on average, there is 42% increase in frequency compared to the
hand-written code with a conservative size of 16k. The frequency
improvement comes from reducing communication time among
distributedstorageresources.Whenthearrayislarge,therequired
storageismoredistributed,andthustheroutingpathsarelonger,
which harms timing.
500Manual HR-8K HR-4K HR-2K HR-1K100200300400
Program (Manually / HeteroRefactor-optimized)Frequency (MHz)Aho-Corasick
DFS
Linked List
Merge Sort
Strassen’s
Figure 7: Operating frequency of a hand-optimized version
with a conservative size, and HeteroRefactor -optimized
versions with different sizes.
If an expert FPGA programmer were to use an overly conserva-
tivesizeof 32k,twokernels Merge Sort andStrassen’s willeven
fail to generate any bitstream, as they require too many resources,
justifyingtheneedsofdynamicanalysisincreatingacustomcircuit.
Summary 1
Byidentifyinganempiricalboundfortherecursivedata
structure size, HeteroRefactor makes programs HLS-
synthesizable. The accelerators optimized for common-
caseinputsare83%morememory-efficientwith42%higher
frequencythanhand-writtencodewithaconservativesize.
4.2 Integer
We assess the hypothesis that reducing bitwidth based on dynamic
invariantsleadstoreductioninresourceutilizationforintegers.We
measureresourceutilizationforeachprogramusingVivadoHLS
2018.03 targeting a Xilinx XCVU9P FPGA.
Forintegerbitwidthreduction, HeteroRefactor takesasinput
(1) the kernel under analysis and (2) input data. We use dynamic
invariants(Table3)tocreateabitwidthoptimizedprogram(e.g.,Fig-ure 5b). Table 3 reports the FPGA-specific dynamic invariants for in-
tegervariables.Intermsofinputdata,weeithergeneratesyntheticdata ofa fixedsize oruse anexisting testset. For
Face Detection
weuseheximagesgeneratedfrom[ 69]andresizethemtothedi-
mension of 16x16. The program uses pre-trained weights declared
asanintegerarray. HeteroRefactor identifiesthatoneoftheweight
arrays requires only unsigned 14 bits based on the max and min
valueandhasonlytwouniquevalues.For 3D Rendering ,w euse
the test input available in the benchmark [ 84] and split it into sub-
sets of 100 for each instrumented run. HeteroRefactor identifies
thattheinputmodelhasarangeof (38,150) andsizeof100.For
Table 3: FPGA’s specific invariants for integer optimization
Program Variable FPGA-specific Invariants
Min Max Unique Size
I6/F. D. Weights Array 1 8192 12288 2 2913I6/F. D. Stddev Variable 305 369 N/A N/AI6/F. D. Coord 0 6746969 21 12
I7/3D R. Triangle 3D (x0) 38 255 49 100
I8/B.Sort Input Array 0 10 11 400Bubble Sort , we generate 400integers based onChi Squaredistri-
bution [47]. The invariants identified by HeteroRefactor reconcile
with the distribution parameters and fixed size of the input set.
Rows I6-I8 in Table 1 summarize the bitwidth reduction and
resourceutilization.Foreachresourcetype,wereportthenumbers
for(1)an original,unoptimizedprograminrow Orig,(2)amanu-
allyoptimizedprograminrow Manual,and(3)a HeteroRefactor
optimized version in row HR. On average, HeteroRefactor leads to
25%reductioninFF,21%reductioninLUT,41%reductioninBRAM,
and 52% decrease in DSP compared to an unoptimized program.
Comparedtocarefullyhand-craftedprogramsbyexperts,itleads
to 12% reduction in FF, 5% reduction in LUTs, 15% reduction in
BRAM, and16% decreasein DSP.Due tothe areareduction, more
processing elements can be synthesized in one single chip.
We then implement these FPGA accelerators on-board with a
target frequency of 300 MHz. All of the refactored programs canmeet this target; however, the original version of
3D Rendering
failsthetimingconstraintsandcanonlyworkwithafinalfrequency
of240.6MHz.Thisvalidatesthatthefrequencyimprovementcan
be achieved by HeteroRefactor .
Summary 2
HeteroRefactor reducesthemanualrefactoringeffortby
automatically finding the optimized bit width for integers.
It reduces 25% FF,21% LUTs,41% BRAM,and 52%DSP in
resourceutilization,whicharebetterthanhand-optimized
kernels written by experts.
4.3 Floating Point
We evaluate the effectiveness of HeteroRefactor in providing a
probabilistic guarantee while lowering a bitwidth, and reducing
resource utilization compared to original programs.
We begin with the given float (32-bit) precision and generate
program variants with a reduced operand bitwidth. Reducing man-
tissa bits leadsto precision loss, whereas reducingexponent leads
toasmallerdynamicrange.Hence,inourexperiments,weincre-
mentally reduce mantissa and verify if the loss is within a usergiven loss,
e, with probability p. As described in Section 3.3, we
useHoeffdingsinequalitytodeterminethenumberofinputdata
samples for given (1−α)andϵ. In our experiments, we fix ϵto
be 0.03, vary pto be 0.95, 0.99, and 0.999, and keep the confidence
level the same as p, i.e.,(1−α)=p, which require at least 2049,
2943 and 4222 samples, respectively. We also consider the input
featuresofFPGAkernelstodeterminethefinalnumberofsamples.Forexample,
RGB2YUV requiresthattheinputsmustbemultiplesof
16,sothefinalnumberofsamplesis2064ratherthan2049when
pis0.95.Inourevaluation,wedrawrandomtestinputswithin0
to 255. The bitwidth reduction is verified by HeteroRefactor with
an acceptable loss ( e) (10−2,1 0−4,o r1 0−6) for KNN-l2norm and an
acceptable loss ( e) (10−4,1 0−5,o r1 0−6) for RGB2YUV.
Table 4 summarizes the probabilistic verification results for dif-
ferenteandpconfigurations. For each configuration, we report
verification results for 8 and 16 bits floating-point, where Nindi-
cates a verification failure, and the column HR reports the smallest
verifiedbitwidth.Asexpected,ahigherprecisionandconfidence
requirement leads to a higher FP bitwidth. The results show that a
501Table 4: Probabilistic floating-point verification
Program pe=10−210−410−6
81 6H R81 6H R81 6H R
F9/KNN 0 . 9 5NN 2 4NN 2 9NN 3 2
0 . 9 9NN 2 5NN 2 9NN 3 2
0.999 N N 25 N N 30 N N 32
pe=10−410−510−6
F10/R2Y 0 . 9 5NN 2 1NN 2 5NN 3 00 . 9 9NN 2 2NN 2 5NN 3 2
0.999 N N 22 N N 26 N N 32
32bitfloating-pointvariablecouldbereducedtousing21bitswith
an acceptable loss of 10−4at 95% confidence level.
WethensynthesizetherefactoredprogramusingVivadoHLS
2018.03 targetinga XilinxXCVU9P FPGA. RowsF9-F10 inTable 1
summarize theresourceutilization for eachsubject program. The
Origrow indicates the original program with 32-bit float type and
prepresents the probability and the confidence level (1−α). Then
we report the resource utilization of FF, LUT, and DSP for each
combination of pand accuracy loss e.HeteroRefactor can achieve
up to 61% reduction in FF, 39% reduction in LUT, and 50% decrease
inDSP.AsexistingHLSflowdoesnotsupportarbitraryfloating-
pointtype,sowecouldnotfindanyhand-optimizedkernels,and
thuswecanonlycompareagainstthedefaulthighbitwidthversion.
Summary 3
HeteroRefactor reduces the floating-point bitwidth while
providing a probabilistic guarantee for a user-specified
qualityloss,probabilityandconfidencelevel.Itcanachieve
up to 61% reduction in FF, 39% in LUT, and 50% in DSP.
4.4 Overhead and Performance
Table 5 summarizes the instrumentation overhead and refactor-
ing overhead for R1-I8 compared against the synthesis time of the
original programs. For recursive data structures, both the instru-
mentation and refactoring overhead are less than 1%. For integers,
HeteroRefactor induces less than 1% refactoring overhead, and its
instrumentation overhead comes from Kvasir. For floating-pointprograms F9-F10, Table 6 summarizes the differential execution
overhead compared against the synthesis time of the original pro-
gramswithaspecificqualityloss eandprobability p,becausethere
isnoinstrumentationrequired. HeteroRefactor induceslessthan
2% overhead on floating-point bitwidth tuning.
We compare the execution performance of the refactored kernel
againstrunningtheoriginalprogramonCPU.Forfloating-point
programs, our experiment shows a significant speedup up to 7 ×
and19×inKNN-l2norm andRGB2YUV.ThisisbecausetheseFPpro-
gramscanbenefitfrominherentparallelcomputation.Forrecursiveprograms,ourrefactoredkernelsareslowerthanCPU,because
Het-
eroRefactor usesasequentialmemoryallocation,thesekernelsare
memory-bound,andthefrequencyofFPGAislowerthanthatof
CPU. For integer intensive programs, the end-to-end performance
depends on whether data parallelism could be easily utilized for
integer-typedataprocessing.ThekernelsweselectedareslightlyTable 5: Runtime overhead for recursions and integers
Instrumentation Refactoring
Program time (min) ratio time (sec) ratio
R1/Aho-Corasick 0.10 0.26% 5.1 0.26%
R2/DFS 0.06 0.26% 4.7 0.34%
R3/Linked List 0.12 0.49% 4.5 0.31%
R4/Merge Sort 0.05 0.20% 4.5 0.29%
R5/Strassen’s 0.09 0.20% 10 0.38%
I6/Face Detection 0.15 0.62% 10 0.69%I7/3D Rendering 13.66 64.76% 10 0.79%
I8/Bubble Sort 10
−3∼01 0−3∼0
Table 6: Differential execution overhead for FP (sec / %)
Program pe=10−210−410−6
F9/KNN 0.95 60.8 / 0.3% 29.4 / 0.2% 11.7 / 0.1%
0.99 58.2 / 0.3% 30.4 / 0.2% 11.7 / 0.1%
0.999 60.8 / 0.3% 25.9 / 0.1% 12.8 / 0.1%
pe=10−410−510−6
F10/R2Y 0.95 83.5 / 1.8% 59.3 / 1.3% 26.8 / 0.6%
0.99 81.5 / 1.7% 58.5 / 1.2% 12.9 / 0.3%
0.999 82.3 / 1.7% 54.5 / 1.2% 13.7 / 0.3%
slower than running on CPU because I6 and I7 in Rosetta are de-
signedtoachievehigherenergyefficiencybutnothigherprocessing
throughput compared to CPU [ 84].HeteroRefactor aims to reduce
resourceusage,whilepriorwork[ 19,24]achieveshigherperfor-
mance than CPU by leveraging more on-chip resources to achieve
parallelism. HeteroRefactor could be used jointly with other tools
to produce fast and resource-efficient FPGA accelerators.
5 RELATED WORK
Automated Refactoring. Since pioneering work on automated
refactoringintheearly90s[ 32,49,53],recentstudiesfindthatreal-
world refactorings are generally not semantics-preserving [ 43,44],
aredonemanually[ 76],areerror-prone[ 42,51],andarebeyondthe
scopeandcapabilityofexistingrefactoringengines.Arecentstudy
with professional developers finds that almost 12% of refactorings
areinitiatedbydevelopers’motivationtoimproveperformance[ 44].
HeteroRefactor builds on this foundation [ 49] but repurposes it to
improve performance in the new era of heterogeneous computing
withre-programmablecircuits.While HeteroRefactor ’srefactoring
is not semantics-preserving, it guarantees semantics-preservationby leveraging selective offloading from CPU to FPGA in tandem.
DynamicInvariant.
Determiningprograminvariantshasbeen
explored widely using both static and dynamic techniques. Het-
eroRefactor isinspiredbyDaikon[ 27],whichgeneratesinvariants
of 22 kinds for C/C++/Java programs. Kataoka et al. [ 41] detect the
symptoms ofa narrow interfaceby observingdynamic invariants
and refactors the corresponding API. Different from these, Het-
eroRefactor doesnotrequirehavingrepresentativedataa-priori,as
itleveragesselectiveoffloadingtoguaranteecorrectness.Therefore,
a developer may use systematic test generation tools [ 29,30,67]
ortestminimization[ 38,73]toinferFPGA-specificinvariants,as
representative data is not required for correctness.
502HLSOptimization. Klimovicetal.[ 45]optimizeFPGAacceler-
atorsforcommon-caseinputsbyreducingbitwidthsusingbothbit-
mask analysis and program profiling [ 31]. When inputs exceed the
common-caserange,asoftwarefallbackfunctionisautomatically
triggered. Their simulation results estimate that an accelerator’s
areamaybereducedby28%onaverage.Whiletheirapproachissimilar to
HeteroRefactor , its scope is limited to monitoring in-
teger values, and they do not implement a systematic approachto monitor bitwidth invariants and the size of a recursive data
structure at the kernel level, nor automatically assess the impact of
tuningvariable-widthfloating-pointprecisionwithagivenerror
bound. While we present real hardware results on Xilinx Virtex
UltraScale+ XCVU9P FPGA, they only present estimated software
simulationresults.Toourknowledge, HeteroRefactor isthefirst
toolfor heterogeneous computing with FPGA that incorporates
dynamicinvariantanalysis,automatedkernelrefactoring,selective
offloading, and synthesized FPGA.
Several approaches provide HLS libraries for implementing vari-
able-width floating-point computation units, but leave it to theprogrammer to specify which parameters to use and to rewrite
their kernel code manually. For example, Thomas [ 75] presents an
HLSbackendforgeneratingacustomizedfloating-pointaccelerator
using C++ template-based, parameterized types. This approach
requirestheusertomanuallyspecifythebitwidthsforanexponent
and fraction, which is automated in HeteroRefactor .
HeteroRefactor differsfromstaticanalysismethodswhichre-
sults in over-approximation. For example, Bitwise [ 71] propagates
bitwidth constraints to variables based on the flow graph of bits.MiniBit [
48] minimizes integer and fixed-point data signals with
a static method based on affine arithmetic. Cong et al. [ 18] uses
affine arithmetic, general interval arithmetic and symbolic arith-
meticmethodstooptimizeforfixed-pointdata.IncontrasttoJIT
compilation techniques [ 4],HeteroRefactor uses an ahead-of-time
profiling phase, due to a long FPGA synthesis time.
Recursion in Heterogeneous Computing. Enabling recur-
sive data structures in FPGA has been a long challenge becausethe address space for each array is separate in HLS/FPGA unlike
traditionalCPUarchitectures.SynADT[ 82]isanHLSlibraryfor
representinglinkedlists,binarytrees,hashtables,andvectorsfrom
pointers, and it internally uses arrays and a shared system-widememory allocator [
81]. However, SynADT supports only a lim-
ited set of data structures and requires developers to manuallyrefactor. In contrast,
HeteroRefactor automatically monitors an
appropriate size of a recursive data structure and performs fullyautomatedkerneltransformationtoconvertpointerusagetoop-
erations on a finite-sized array and implements a guard-condition
based offloading. Thomas et al. [74] use C++ templates to create a
domain-specific language to support recursion in HLS. However, it
requires extensive rewriting of control statements using lambdas.
Similar limitation existed on GPU with CUDA [ 52] and OpenCL
[72]. For example, dynamic memory management on device global
memoryusing mallocwasnotsupporteduntilCUDA3.2[ 52],and
thereisnoimplementationof malloconsharedon-chipmemory.However,onemaywritetheirownuniversalallocatorforarbitrary
types as a replacement for mallocon any memory [ 1,39,70]be -
causeGPUallowsasingleaddressspacewithregularaccesswidths,similartoCPU,whileFPGAdoesnot.Suchapproaches[
1,39,52,70]
stillrequiremanuallyspecifyingaheapsize. HeteroRefactor au-
tomatically detects the required size of FPGA on-chip memory for
recursivedatastructuresusingdynamicinvariantdetection,and
fallbackstoCPUcomputationwhenthesizeinvariantsareviolated.
Tuning Floating-point Precision. FPTuner [ 11] uses static
analysis for automatic precision-tuning [ 68] of real valued expres-
sions. It supports a single, double, or quadruple precision ratherthan an arbitrary-width FP type. Precimonious [
63] is a floating-
pointprecisiontuningtoolthatusesdynamicanalysisanddelta-
debugging to identify lower precision instruction that satisfies the
user-specified acceptable precision loss constraint. HeteroRefac-
tor’sFPtuningisinspiredbythesuccessofPrecimonious.However,
HeteroRefactor extendsthisideabyaddingaprobabilisticverifica-
tionlogictoprovidestatisticalguaranteeonprecisionloss.While
Pecimonious is a software-only analysis tool for FP tuning, Het-
eroRefactor is an end-to-end approach that integrates dynamic
invariant analysis, automated refactoring, and FPGA synthesis.
6 CONCLUSION
Traditionally,automatedrefactoringhasbeenusedtoimprovesoft-
waremaintainability.Tomeettheincreasingdemandfordevelop-
ingnewhardwareacceleratorsandto enablesoftware engineerstoleverageheterogeneouscomputingenvironments,weadaptandex-pand the scope of automated refactoring.
HeteroRefactor provides
anovel,end-to-endsolutionthatcombines(1)dynamicanalysisfor
identifyingcommon-casesizes,(2)kernelrefactoringtoenhance
HLSsynthesizabilityandtoreduceon-chipresourceusageonFPGA,
and(3) selectiveoffloadingwith guardcheckingtoguarantee cor-
rectness. For the transformed recursive programs, HeteroRefactor
reduces BRAM by 83% and increases frequency by 42%. For integer
optimization, it reduces the number of bits for integers by 76%,
leading to 41% decrease in BRAM. For floating-point optimization,
itreducesDSPusageby50%,whileguaranteeingauser-specified
precision loss of 0.01 with 99.9% confidence.
7 ACKNOWLEDGEMENT
WewouldliketothankGuyVandenBroeck,BrettChalabian,Todd
Millstein, Peng Wei, Cody Hao Yu and anonymous reviewers fortheir valuable feedback. We thank Janice Wheeler for proofread-ing the draft. This work is in part supported by NSF grants CCF-
1764077,CCF-1527923,CCF-1723773,ONRgrantN00014-18-1-2037,
IntelCAPAgrant,andSamsunggrant.Thisworkisalsopartially
supported by CRISP, one of six centers in JUMP, a Semiconductor
ResearchCorporation(SRC)programandthecontributionsfrom
themembercompaniesundertheCenterforDomain-SpecificCom-
puting (CDSC) Industrial Partnership Program, including Xilinx
and VMWare. Jason Lau is supported by UCLA Computer Science
DepartmentalFellowshipandMuhammadGulzarissupportedby
Google PhD Fellowship.
503REFERENCES
[1]AndrewVAdinetzandDirkPleiter.2014. Halloc:ahigh-throughputdynamic
memoryallocatorforGPGPUarchitectures.In GPUTechnologyConference(GTC) ,
Vol. 152.
[2]Alfred V Aho and Margaret J Corasick. 1975. Efficient string matching: an aid to
bibliographic search. Commun. ACM 18, 6 (1975), 333–340.
[3]Amazon.com. 2019. Amazon EC2 F1 Instances: Run Custom FPGAs in the AWS
Cloud. https://aws.amazon.com/ec2/instance-types/f1. (2019).
[4]MatthewArnold,StephenJFink,DavidGrove,MichaelHind,andPeterFSweeney.
2005. A survey of adaptive optimization in virtual machines. Proc. IEEE 93, 2
(2005), 449–466.
[5]UdayBondhugula,MuthuBaskaran,SriramKrishnamoorthy,JagannathanRa-
manujam, Atanas Rountev, and Ponnuswamy Sadayappan. 2008. Automatic
transformationsforcommunication-minimizedparallelizationandlocalityop-
timization in the polyhedral model. In International Conference on Compiler
Construction. Springer, 132–146.
[6]Gary Bradski and Adrian Kaehler. 2008. Learning OpenCV: Computer vision with
the OpenCV library. O’Reilly Media, Inc.
[7]Jared Casper and Kunle Olukotun. 2014. Hardware acceleration of database
operations.In Proceedingsofthe2014ACM/SIGDAinternationalsymposiumon
Field-programmable gate arrays. ACM, 151–160.
[8]AdrianMCaulfield,EricSChung,AndrewPutnam,HariAngepat,JeremyFowers,
Michael Haselman, Stephen Heil, Matt Humphrey, Puneet Kaur, Joo-Young Kim,
etal.2016. Acloud-scaleaccelerationarchitecture.In The49thAnnualIEEE/ACM
International Symposium on Microarchitecture. IEEE Press, 7.
[9]ZheChen,HughTBlair,andJasonCong.2019. LANMC:LSTM-AssistedNon-
RigidMotionCorrectiononFPGAforCalciumImageStabilization.In Proceedings
of the 2019 ACM/SIGDA International Symposium on Field-Programmable Gate
Arrays. 104–109.
[10]Yuze Chi, Jason Cong, Peng Wei, and Peipei Zhou. 2018. SODA: stencil with
optimizeddataflowarchitecture.In 2018IEEE/ACMInternationalConferenceon
Computer-Aided Design (ICCAD). IEEE, 1–8.
[11]Wei-Fan Chiang, Mark Baranowski, Ian Briggs, Alexey Solovyev, Ganesh
Gopalakrishnan, and Zvonimir Rakamariundefined. 2017. Rigorous Floating-Point Mixed-Precision Tuning. In Proceedings of the 44th ACM SIGPLAN Sym-
posium on Principles of Programming Languages (POPL 2017). Association for
Computing Machinery, New York, NY, USA, 300–315.
[12]Andrew A Chien, Allan Snavely, and Mark Gahagan. 2011. 10x10: A general-
purpose architectural approach to heterogeneity and energy efficiency. Procedia
Computer Science 4 (2011), 1987–1996.
[13]Young-kyu Choi and Jason Cong. 2017. HLScope: High-Level performance
debugging for FPGA designs. In 2017 IEEE 25th Annual International Symposium
on Field-Programmable Custom Computing Machines (FCCM). IEEE, 125–128.
[14]EricSChung,PeterAMilder,JamesCHoe,andKenMai.2010. Single-chiphetero-geneouscomputing:Doesthefutureincludecustomlogic,FPGAs,andGPGPUs?.
In2010 43rd annual IEEE/ACM international symposium on microarchitecture.
IEEE, 225–236.
[15]Jason Cong, Zhenman Fang, Yuchen Hao, Peng Wei, Cody Hao Yu, Chen Zhang,
andPeipeiZhou.2018. Best-EffortFPGAProgramming:AFewStepsCanGoa
Long Way. arXiv preprint arXiv:1807.01340 (2018).
[16]JasonCong,MohammadAliGhodrat,MichaelGill,BeaynaGrigorian,Karthik
Gururaj,andGlennReinman.2014. Accelerator-richarchitectures:Opportunities
and progresses. In Proceedings of the 51st Annual Design Automation Conference.
ACM, 1–6.
[17]Jason Cong, Licheng Guo, Po-Tsang Huang, Peng Wei, and Tianhe Yu. 2018.SMEM++: A Pipelined and Time-Multiplexed SMEM Seeding Accelerator for
GenomeSequencing.In 201828thInternationalConferenceonFieldProgrammable
Logic and Applications (FPL). 210–2104.
[18]JasonCong,KarthikGururaj,BinLiu,ChunyueLiu,ZhiruZhang,ShengZhou,
and YiZou. 2009. Evaluationof static analysistechniques for fixed-point preci-
sion optimization. In 2009 17th IEEE Symposium on Field Programmable Custom
Computing Machines. IEEE, 231–234.
[19]JasonCong,MuhuanHuang,PeichenPan,YuxinWang,andPengZhang.2016.
Source-to-source optimization for HLS. In FPGAs for Software Programmers.
Springer, 137–163.
[20]JasonCong,MuhuanHuang,PeichenPan,DiWu,andPengZhang.2016. Soft-
ware infrastructure for enabling FPGA-based accelerations in data centers. In
Proceedings of the 2016 International Symposium on Low Power Electronics and
Design. ACM, 154–155.
[21]JasonCong,BinLiu,StephenNeuendorffer,JuanjoNoguera,KeesVissers,and
Zhiru Zhang. 2011. High-level synthesis for FPGAs: From prototyping to de-
ployment. IEEETransactionsonComputer-AidedDesignofIntegratedCircuitsand
Systems30, 4 (2011), 473–491.
[22]Jason Cong, Vivek Sarkar, Glenn Reinman, and Alex Bui. 2010. Customizable
domain-specific computing. IEEE Design & Test of Computers 28, 2 (2010), 6–15.
[23]JasonCongandJieWang.2018. PolySA:polyhedral-basedsystolicarrayauto-
compilation. In 2018 IEEE/ACM International Conference on Computer-Aided De-
sign (ICCAD). IEEE, 1–8.[24]Jason Cong, Peng Wei, Cody Hao Yu, and Peng Zhang. 2018. Automated ac-celerator generation and optimization with composable, parallel and pipelinearchitecture. In Proceedings of the 55th Annual Design Automation Conference
(DAC). IEEE, 1–6.
[25]Jeferson Santiago da Silva, François-Raymond Boyer, and JM Langlois. 2019.
Module-per-Object:aHuman-DrivenMethodologyforC++-basedHigh-Level
Synthesis Design. arXiv preprint arXiv:1903.06693 (2019).
[26]Rene De La Briandais. 1959. File searching using variable length keys. In Papers
presented at the the March 3-5, 1959, Western Joint Computer Conference. ACM,
295–298.
[27]MichaelDErnst,JeffHPerkins,PhilipJGuo,StephenMcCamant,CarlosPacheco,
Matthew S Tschantz, and Chen Xiao. 2007. The Daikon system for dynamicdetection of likely invariants. Science of computer programming 69, 1-3 (2007),
35–45.
[28]DanielDGajski,NikilDDutt,AllenCHWu,andSteveYLLin.2012. High—Level
Synthesis:IntroductiontoChipandSystemDesign. SpringerScience&Business
Media.
[29]Patrice Godefroid, Nils Klarlund, and Koushik Sen. 2005. DART: Directed Auto-
mated Random Testing. In Proceedings of the 2005 ACM SIGPLAN Conference on
ProgrammingLanguageDesignandImplementation(PLDI’05).ACM,NewYork,NY, USA, 213–223. https://doi.org/10.1145/1065010.1065036
[30]
PatriceGodefroid,MichaelY.Levin,andDavidAMolnar.2008. AutomatedWhite-
boxFuzzTesting.In NetworkDistributedSecuritySymposium(NDSS).Internet
Society. http://www.truststc.org/pubs/499.html
[31]Marcel Gort and Jason H Anderson. 2013. Range and bitmask analysis for
hardwareoptimizationinhigh-levelsynthesis.In 201318thAsiaandSouthPacific
Design Automation Conference (ASP-DAC). IEEE, 773–779.
[32]William G. Griswold. 1991. Program Restructuring as an Aid to Software Mainte-
nance. Ph.D. Dissertation. University of Washington.
[33]LichengGuo,JasonLau,ZhenyuanRuan,PengWei,andJasonCong.2019. Hard-
wareaccelerationoflongreadpairwiseoverlappingingenomesequencing:A
racebetweenfpgaandgpu.In 2019IEEE27thAnnualInternationalSymposium
on Field-Programmable Custom Computing Machines (FCCM). IEEE, 127–135.
[34]Licheng Guo, Jason Lau, Jie Wang, Cody Hao Yu, Yuze Chi, Zhe Chen, Zhiru
Zhang, and Jason Cong. 2020. Analysis and Optimization of the Implicit Broad-
castsinFPGAHLStoImproveMaximumFrequency.In Proceedingsofthe28th
ACM/SIGDAInternationalSymposiumonField-ProgrammableGateArrays(FPGA) .
ACM.
[35]PrabhatGupta.2019. Xeon+FPGAPlatformfortheDataCenter. https://www.
archive.ece.cmu.edu/~calcm/carl/lib/\exe/fetch.php?media=carl15-gupta.pdf.
(2019).
[36]YukoHara,HiroyukiTomiyama,ShinyaHonda,HiroakiTakada,andKatsuya
Ishii. 2008. Chstone: A benchmark program suite for practical c-based high-level
synthesis.In 2008IEEEInternationalSymposiumonCircuitsandSystems.IEEE,
1192–1195.
[37]WassilyHoeffding.1994. Probabilityinequalitiesforsumsofboundedrandom
variables. In The Collected Works of Wassily Hoeffding. Springer, 409–426.
[38]Hwa-YouHsuandAlessandroOrso.2009. MINTS:AGeneralFrameworkand
ToolforSupportingTest-suiteMinimization.In Proceedingsofthe31stInterna-
tional Conference on Software Engineering (ICSE ’09). IEEE Computer Society,
Washington, DC, USA, 419–429. https://doi.org/10.1109/ICSE.2009.5070541
[39]Xiaohuang Huang, Christopher I Rodrigues, Stephen Jones, Ian Buck, and Wen-
mei Hwu. 2010. Xmalloc: A scalable lock-free dynamic memory allocator for
many-coremachines.In 201010thIEEEInternationalConferenceonComputerand
Information Technology. IEEE, 1134–1139.
[40]StevenHuss-Lederman,ElaineMJacobson,JeremyRJohnson,AnnaTsao,and
Thomas Turnbull. 1996. Implementation of Strassen’s algorithm for matrix mul-
tiplication.In Supercomputing’96:Proceedingsofthe1996ACM/IEEEConference
on Supercomputing. IEEE, 32–32.
[41]Yoshio Kataoka, Michael D. Ernst, William G. Griswold, and David Notkin. 2001.
Automated support for program refactoring using invariants. In ICSM 2001,
Proceedings of the International Conference on Software Maintenance. Florence,
Italy, 736–743.
[42]Miryung Kim, Dongxiang Cai, and Sunghun Kim. 2011. An Empirical Inves-tigation into the Role of Refactorings during Software Evolution. In ICSE’ 11:
Proceedings of the 2011 ACM and IEEE 33rd International Conference on Software
Engineering.
[43]MiryungKim,ThomasZimmermann,andNachiappanNagappan. 2012. Afield
study of refactoring challenges and benefits. In Proceedings of the ACM SIGSOFT
20th International Symposium on the Foundations of Software Engineering (FSE
’12). ACM, New York, NY, USA, Article 50, 11 pages. https://doi.org/10.1145/
2393596.2393655
[44]Miryung Kim, Thomas Zimmermann, and Nachiappan Nagappan. 2014. AnEmpirical Study of Refactoring Challenges and Benefits at Microsoft. IEEE
TransactionsonSoftwareEngineering 40,7(2014),1–1. https://doi.org/10.1109/
TSE.2014.2318734
504[45]Ana Klimovic and Jason H Anderson. 2013. Bitwidth-optimized hardware
accelerators with software fallback. In 2013 International Conference on Field-
Programmable Technology (FPT). IEEE, 136–143.
[46]David Koeplinger, Raghu Prabhakar, Yaqi Zhang, Christina Delimitrou, Christos
Kozyrakis, and Kunle Olukotun. 2016. Automatic generation of efficient acceler-
atorsforreconfigurablehardware.In 2016ACM/IEEE43rdAnnualInternational
Symposium on Computer Architecture (ISCA). Ieee, 115–127.
[47]Henry Oliver Lancaster and Eugene Seneta. 2005. Chi-square distribution. Ency-
clopedia of biostatistics 2 (2005).
[48]Dong-ULee, AltafAbdul Gaffar,Oskar Mencer,and WayneLuk. 2005. MiniBit:
bit-width optimization via affine arithmetic. In Proceedings of the 42nd annual
Design Automation Conference. ACM, 837–840.
[49]Tom Mens and Tom Tourwe. 2004. A Survey of Software Refactoring. IEEE
Transactions on Software Engineering 30, 2 (2004), 126–139. https://doi.org/10.
1109/TSE.2004.1265817
[50]GiovanniDeMicheli.1994. Synthesisandoptimizationofdigitalcircuits. McGraw-
Hill Higher Education.
[51]EmersonMurphy-Hill,ChrisParnin,andAndrewP.Black.2009. Howwerefactor,
and how we know it. In ICSE ’09: Proceedings of the 31st International Conference
onSoftwareEngineering.IEEEComputerSociety,Washington,DC,USA,287–297.
https://doi.org/10.1109/ICSE.2009.5070529
[52]Nvidia.2011. NvidiaCUDACprogrammingguide. NvidiaCorporation 120,18
(2011), 8.
[53]William F. Opdyke. 1992. Refactoring Object-Oriented Frameworks. Ph.D. Disser-
tation.UniversityofIllinois,Urbana-Champaign,IL,USA. citeseer.ist.psu.edu/
opdyke92refactoring.html
[54]James L Peterson and Theodore A Norman. 1977. Buddy systems. Commun.
ACM20, 6 (1977), 421–431.
[55]NamKhanhPham,AmitKumarSingh,AkashKumar,andMiMiAungKhin.2015.
Exploitingloop-arraydependenciestoacceleratethedesignspaceexploration
withhighlevelsynthesis.In 2015Design,Automation&TestinEuropeConference
& Exhibition (DATE). IEEE, 157–162.
[56]Louis-Noel Pouchet, Peng Zhang, Ponnuswamy Sadayappan, and Jason Cong.
2013. Polyhedral-baseddatareuseoptimizationforconfigurablecomputing.In
ProceedingsoftheACM/SIGDAinternationalsymposiumonFieldprogrammable
gate arrays. ACM, 29–38.
[57]AndrewPutnam,AdrianMCaulfield,EricSChung,DerekChiou,KyprosCon-
stantinides,JohnDemme,HadiEsmaeilzadeh,JeremyFowers,GopiPrashanth
Gopal, Jan Gray, et al. 2014. A reconfigurable fabric for accelerating large-scale
datacenter services. ACM SIGARCH Computer Architecture News 42, 3 (2014),
13–24.
[58]Weikang Qiao, Jieqiong Du, Zhenman Fang, Michael Lo, Mau-Chung Frank
Chang, and Jason Cong. 2018. High-throughput lossless compression on tightly
coupledCPU-FPGAplatforms.In 2018IEEE26thAnnualInternationalSymposium
on Field-Programmable Custom Computing Machines (FCCM). IEEE, 37–44.
[59]Dan Quinlan and Chunhua Liao. 2011. The ROSE source-to-source compiler
infrastructure. In Cetus users and compiler infrastructure workshop, in conjunction
with PACT, Vol. 2011. Citeseer, 1.
[60]Brandon Reagen, Robert Adolf, Yakun Sophia Shao, Gu-Yeon Wei, and DavidBrooks. 2014. Machsuite: Benchmarks for accelerator design and customized
architectures.In 2014IEEEInternationalSymposiumonWorkloadCharacterization
(IISWC). IEEE, 110–119.
[61]ZhenyuanRuan,TongHe,BojieLi,PeipeiZhou,andJasonCong.2018. ST-Accel:
A high-level programming platform for streaming applications on FPGA. In
2018IEEE26thAnnualInternationalSymposiumonField-ProgrammableCustom
Computing Machines (FCCM). IEEE, 9–16.
[62]CindyRubio-González,CuongNguyen,BenjaminMehne,KoushikSen,James
Demmel, William Kahan, Costin Iancu, Wim Lavrijsen, David H Bailey, andDavid Hough. 2016. Floating-point precision tuning using blame analysis. In
Proceedingsofthe38thInternationalConferenceonSoftwareEngineering.ACM,
1074–1085.
[63]Cindy Rubio-González, Cuong Nguyen, Hong Diep Nguyen, James Demmel,
WilliamKahan,KoushikSen,DavidHBailey,CostinIancu,andDavidHough.
2013. Precimonious: Tuning assistant for floating-point precision. In SC’13:
Proceedings of the International Conference on High Performance Computing, Net-
working, Storage and Analysis. IEEE, 1–12.
[64]KyleRupnow,YunLiang,YinanLi,andDemingChen.2011. Astudyofhigh-level
synthesis: Promises and challenges. In 2011 9th IEEE International Conference on
ASIC. IEEE, 1102–1105.
[65]GiacintoPaoloSaggese,AntoninoMazzeo,NicolaMazzocca,andAntonioGM
Strollo. 2003. An FPGA-based performance analysis of the unrolling, tiling,and pipelining of the AES algorithm. In International Conference on Field Pro-
grammable Logic and Applications. Springer, 292–302.
[66]Adrian Sampson, Pavel Panchekha, Todd Mytkowicz, Kathryn S. McKinley, Dan
Grossman,andLuisCeze.2014. ExpressingandVerifyingProbabilisticAssertions.
InPLDI.
[67]Koushik Sen, Darko Marinov, and Gul Agha. 2005. CUTE: A Concolic Unit
TestingEngineforC.In Proceedingsofthe10thEuropeanSoftwareEngineering
Conference Held Jointly with 13th ACM SIGSOFT International Symposium on
FoundationsofSoftwareEngineering(ESEC/FSE-13).ACM,NewYork,NY,USA,
263–272. https://doi.org/10.1145/1081706.1081750
[68]AlexeySolovyev,MarekSBaranowski,IanBriggs,CharlesJacobsen,Zvonimir
Rakamarić, and Ganesh Gopalakrishnan. 2018. Rigorous estimation of floating-
pointround-off errorswithsymbolictaylor expansions. ACMTransactions on
Programming Languages and Systems (TOPLAS) 41, 1 (2018), 1–39.
[69]Nitish Srivastava, Steve Dai, Rajit Manohar, and Zhiru Zhang. 2017. Acceler-
ating Face Detection on Programmable SoC Using C-Based Synthesis. In 25th
ACM/SIGDA International Symposium on Field-Programmable Gate Arrays.
[70]Markus Steinberger, Michael Kenzel, Bernhard Kainz, and Dieter Schmalstieg.
2012. ScatterAlloc: Massively parallel dynamic memory allocation for the GPU.
In2012 Innovative Parallel Computing (InPar). IEEE, 1–10.
[71]Mark Stephenson, Jonathan Babb, and Saman Amarasinghe. 2000. Bidwidth
Analysis with Application to Silicon Compilation. In Proceedings of the ACM
SIGPLAN 2000 Conference on Programming Language Design and Implementation
(PLDI’00).ACM,NewYork,NY,USA,108–120. https://doi.org/10.1145/349299.
349317
[72]John E Stone, David Gohara, and Guochun Shi. 2010. OpenCL: A parallel pro-
gramming standard for heterogeneous computing systems. Computing in science
& engineering 12, 3 (2010), 66–73.
[73]Sriraman Tallam and Neelam Gupta. 2005. A Concept Analysis Inspired Greedy
Algorithm for Test Suite Minimization. In Proceedings of the 6th ACM SIGPLAN-
SIGSOFTWorkshoponProgramAnalysisforSoftwareToolsandEngineering(PASTE
’05). ACM, New York, NY, USA, 35–42. https://doi.org/10.1145/1108792.1108802
[74]David B Thomas. 2016. Synthesisable recursion for C++ HLS tools. In 2016 IEEE
27thInternationalConferenceonApplication-specificSystems,Architecturesand
Processors (ASAP). IEEE, 91–98.
[75]DavidBThomas.2019. TemplatisedSoftFloating-PointforHigh-LevelSynthesis.
In2019IEEE27thAnnualInternationalSymposiumonField-ProgrammableCustom
Computing Machines (FCCM). IEEE.
[76]MohsenVakilian,NicholasChen,StasNegara,BalajiAmbreshRajkumar,BrianP.
Bailey,andRalphE.Johnson.2012. Use,disuse,andmisuseofautomatedrefac-
torings.In SoftwareEngineering(ICSE),201234thInternationalConferenceon.233
–243. https://doi.org/10.1109/ICSE.2012.6227190
[77]PaulRWilson,MarkSJohnstone,MichaelNeely,andDavidBoles.1995. Dynamic
storage allocation: A survey and critical review. In International Workshop on
Memory Management. Springer, 1–116.
[78] Xilinx. 2019. UltraScale Architecture and Product Data Sheet: Overview.
https://www.xilinx.com/support/documentation/data_sheets/
ds890-ultrascale-overview.pdf. (2019).
[79]Xilinx.2019. VivadoHigh-LevelSynthesis. https://www.xilinx.com/products/
design-tools/vivado/integration/esl-design.html. (2019).
[80] Xilinx. 2019. Xilinx Virtex UltraScale+ FPGA VCU1525.
https://www.xilinx.com/products/boards-and-kits/vcu1525-a.html. (2019).
[81]Zeping Xue and David B Thomas. 2015. SysAlloc: A hardware manager for
dynamicmemoryallocationinheterogeneoussystems.In 201525thInternational
Conference on Field Programmable Logic and Applications (FPL). IEEE, 1–7.
[82]Zeping Xue and David B Thomas. 2016. SynADT: Dynamic Data Structures
inHighLevelSynthesis.In 2016IEEE24thAnnualInternationalSymposiumon
Field-Programmable Custom Computing Machines (FCCM). IEEE, 64–71.
[83]CodyHaoYu,PengWei,MaxGrossman,PengZhang,VivekSarker,andJason
Cong. 2018. S2FA: an accelerator automation framework for heterogeneous
computingindatacenters. In Proceedingsofthe 55thAnnualDesignAutomation
Conference (DAC). ACM, 153.
[84]Yuan Zhou, Udit Gupta, Steve Dai, Ritchie Zhao, Nitish Srivastava, Hanchen Jin,
JosephFeatherston,Yi-HsiangLai,GaiLiu,GustavoAngaritaVelasquez,Wenping
Wang, and Zhiru Zhang. 2018. Rosetta: A Realistic High-Level Synthesis Bench-
markSuiteforSoftware-ProgrammableFPGAs. Int’lSymp.onField-Programmable
Gate Arrays (FPGA) (Feb 2018).
[85]Wei Zuo, Peng Li, Deming Chen, Louis-Noël Pouchet, Shunan Zhong, and Jason
Cong.2013. Improvingpolyhedralcodegenerationforhigh-levelsynthesis.In
2013InternationalConferenceonHardware/SoftwareCodesignandSystemSynthesis
(CODES+ ISSS). IEEE, 1–10.
505