Is Historical Data an Appropriate Benchmark for
Reviewer Recommendation Systems?
A Case Study of the Gerrit Community
Ian X. Gauthier
McGill University
Montreal, Canada
ian.gauthier@mail.mcgill.caMaxime Lamothe
Polytechnique Montreal
Montreal, Canada
maxime.lamothe@polymtl.caGunter Mussbacher
McGill University
Montreal, Canada
gunter.mussbacher@mcgill.caShane McIntosh
University of Waterloo
Waterloo, Canada
shane.mcintosh@uwaterloo.ca
Abstract —Reviewer recommendation systems are used to sug-
gest community members to review change requests. Like several
other recommendation systems, it is customary to evaluaterecommendations using held out historical data. While history-based evaluation makes pragmatic use of available data, historicalrecords may be: (1) overly optimistic, since past assignees mayhave been suboptimal choices for the task at hand; or (2) overlypessimistic, since “incorrect” recommendations may have beenequal (or even better) choices.
In this paper, we empirically evaluate the extent to which
historical data is an appropriate benchmark for reviewer recom-mendation systems. We replicate the
CHR EVand WLRR ECap-
proaches and apply them to 9,679 reviews from the G ERRIT open
source community. We then assess the recommendations withmembers of the G
ERRIT reviewing community using quantitative
methods (personalized questionnaires about their comfort levelwith tasks) and qualitative methods (semi-structured interviews).
We ﬁnd that history-based evaluation is far more pessimistic
than optimistic in the context of G
ERRIT review recommenda-
tions. Indeed, while 86% of those who had been assigned to areview in the past felt comfortable handling the review, 74%of those labelled as incorrect recommendations also felt thatthey would have been comfortable reviewing the changes. Thisindicates that, on the one hand, when reviewer recommendationsystems recommend the past assignee, they should indeed beconsidered correct. Y et, on the other hand, recommendationslabelled as incorrect because they do not match the past assigneemay have been correct as well.
Our results suggest that current reviewer recommendation
evaluations do not always model the reality of software develop-ment. Future studies may beneﬁt from looking beyond repositorydata to gain a clearer understanding of the practical value ofproposed recommendations.
I. I NTRODUCTION
Software repository data is an invaluable resource that ac-
cumulates as a software project ages. Version Control Systems
(VCSs) store commits that record changes to codebases. Issue
Tracking Systems (ITSs) accrue issue reports that describe
product defects, enhancement requests, and other potentialimprovements. Review Tracking Systems (RTSs) archive thepeer code reviews that take place during software development.
Mining Software Repositories (MSR) approaches aim to
uncover and communicate insights from historical softwarerepository data to support knowledge acquisition and futurestakeholder decisions. A popular mechanism for communicat-ing those insights is through recommendation systems [34].
For instance, recommendation systems have been proposedto aid practitioners with code navigation [11], [45], taskprioritization [15], [16], and task assignment [2], [37].
Reviewer recommendation is a popular and recent variant
of the task assignment problem, where the recommendationsystem provides a list of potential reviewers (ordered by theirrelevance) for newly submitted change requests to an RTS.
It is challenging to evaluate (reviewer) recommendation
systems in software engineering settings [13]. An optimalevaluation would be: (1) reproducible, allowing the research
community to compare recommendation approaches in animpartial manner; (2) meaningful, improving the practical
implications of the evaluation; and (3) pragmatic, making use
of available data and resources to the largest extent possible.Maximizing all three of these characteristics with a single typeof evaluation is impractical, since they present inherent trade-offs. For example, a highly meaningful evaluation would inviteactive stakeholders to weigh in on the suggestions that aregenerated by the recommendation system. However, such anevaluation would not be pragmatic, since recruiting a largenumber of participants for such a study is difﬁcult.
Broadly speaking, recommendation systems in software
engineering are often evaluated using held out historicaldata [22]. After being trained using a subset of the availabledata (the training corpus), the recommendation system is testedusing another subset that was not used for training (the testingcorpus). In the context of reviewer recommendation systems,the correct answer that the system is expected to generate isthe list of reviewers who appear in the historical data.
The optimal reviewer recommendation system according to
a history-based evaluation suggests exactly (and only) whathappened in the past. Since past decisions may not havebeen optimal, relying on such a solution would encouragestakeholders to repeat past mistakes. Moreover, Kovalenkoet al. [24] argue that recommending popular past reviewers
provides limited value in proprietary settings, where suchreviewers are often well known. In our estimation, solelyrelying on historical records to evaluate reviewer recommen-
302021 36th IEEE/ACM International Conference on Automated Software Engineering (ASE)
DOI 10.1109/ASE51524.2021.000142021 36th IEEE/ACM International Conference on Automated Software Engineering (ASE) | 978-1-6654-0337-5/21/$31.00 ©2021 IEEE | DOI: 10.1109/ASE51524.2021.9678640
978-1-6654-0337-5/21/$31.00  ©2021  IEEE
dation systems suffers from two potential limitations. First,
the evaluation may be overly optimistic since the solution
that stakeholders applied may have been a suboptimal choice.Second, the evaluation may be overly pessimistic, since the
“incorrect” suggestions may have been as appropriate (if notmore so) than the solution that stakeholders had applied.
If history-based evaluations are overly optimistic or pes-
simistic, performance scores will overestimate or underes-timate the value that the reviewer recommendation systemprovides. Therefore, we pose the following central question:
Is historical data an appropriate benchmark
for reviewer recommendation systems?
As a concrete ﬁrst step towards addressing this question, in
this paper, we perform a case study of reviewer recommen-dation in the context of the G
ERRIT open source community.
We conduct our study by (a) replicating the CHR EV[44] and
WLRR EC[1] reviewer recommendation approaches, and (b)
applying them to 9,679 review records spanning the last twoyears of G
ERRIT development.
We classify the examples in the history-based evaluation
into “correct” and “incorrect” recommendations. Using repre-sentative samples from each category, we issue personalizedquestionnaires to reviewers in the G
ERRIT community.1These
questionnaires asked reviewers to assess their comfort levelwith both reviews to which they were assigned (i.e., “correct”examples) and reviews to which they were not assigned butwere recommended by a reviewer recommendation approach(i.e., “incorrect” examples). By triangulating observationsfrom this quantitative data with qualitative data collectedfrom semi-structured interviews with G
ERRIT stakeholders, we
address the following research questions:
(RQ1) Optimism: How often are past assignees that were
deemed correct actually suboptimal?
We ﬁnd that, in the vast majority of cases, review-ers who were deemed to be “correct” were actuallycomfortable reviewing the given change. Indeed, 86%of reviews received comfort level ratings of either‘high’ or ‘expert’. Conversely, only 8% of the reviewsreceived a comfort level rating of either ‘low’ or‘not comfortable’. This suggests that history-basedevaluations of reviewer recommendation are notoverly
optimistic.
(RQ2) Pessimism: How often are incorrect recommenda-
tions actually appropriate?We ﬁnd that 74% of reviews that were initially deemed“incorrect” received a comfort level rating of either‘high’ or ‘expert’ from respondents. Only 10% ofreviews received a comfort level of either ‘low’ or‘no comfort’. These ﬁndings suggest that history-basedevaluations of reviewer recommendation are overly
pessimistic.
1We obtained Institutional Review Board (IRB) approval for this study from
the McGill Research Ethics Board-1 (REB # 382-0218).Broadly speaking, our results suggest that the current
method of evaluating reviewer recommendation systems isfar more pessimistic than optimistic. Indeed, history-basedperformance values are likely to be an under-approximation ofthe true value that reviewer recommendation system sugges-tions would provide to the G
ERRIT community. In one sense,
this is a positive development, suggesting that in general,the performance of reviewer recommendation systems may beunder-approximated in the literature. In another sense, under-approximated performance scores in the literature may leadpractitioners to draw incorrect conclusions about the qualityof current reviewer recommendation systems.
The remainder of the paper is organized as follows: Sec-
tion II situates this work with respect to the literature onrecommendation systems in software engineering. Section IIIdescribes the design of our case study, while Sections IVand V present the results. Section VI discusses the broaderimplications of our study results. Section VII discusses threatsto the validity of our study. Finally, Section VIII drawsconclusions and proposes directions for future work.
II. B
ACKGROUND ANDRELATED WORK
In this section, we provide an overview of recommendation
systems in software engineering generally (Section II-A) andreviewer recommendation speciﬁcally (Section II-B).
A. Recommendation Systems in Software Engineering
At their core, most recommendation systems [34] (i.e., sys-
tems that suggest to stakeholders future actions to take based
on patterns or trends that have been mined from historical data)operate in a similar manner. A corpus of historical examples(i.e., the training corpus) is provided as input to a supervised orunsupervised data mining approach (e.g., statistical or machinelearning regression, clustering, or classiﬁcation techniques).This step produces a model, which can produce recommen-
dations for future examples. While recommendation systemstake a variety of forms, in our estimation, the content discoveryand decision support variants, which we describe below, havereceived plenty of attention within the software engineeringdomain [16], [19], [21], [27], [40], [45].
Content Discovery Recommendation Systems. Content Dis-
covery Recommendation Systems (CDRSs) are at the core
of solutions, such as bug localization [40], where availableinformation about a bug (e.g., issue report content) is providedas a query to an engine that searches for matches in the codebase. CDRSs have also been proposed to guide developers asthey make changes to a code base, suggesting other relevantlocations to view/modify based on historical co-change ten-dencies [45] or the recency of viewing/modiﬁcation [21].
Decision Support Recommendation Systems. Software prac-
titioners make decisions to balance trade-offs regularly. Al-
though these decisions often impact their organizations, itis not uncommon for decisions to be made based on intu-ition. Decision Support Recommendation Systems (DSRSs)aid practitioners in making more data-grounded decisions.Broadly speaking, these DSRSs fall into two categories.
314Beckham, Mylie …ID Reviewer …
19 Carlee …
20Samara, Molly …
12 Leila, Ayana …
15 Vincent …
5 Ayana …
3 Devyn …
6 Dana, Ross …
18 Ross, Samara …
17 Victor …
13 Juliette …
16 Renee, Devyn …
2Ayana, Kenya …
11 Lorelai …
8Londyn, Devyn …
9 Israel …
1Kenya, Lorelai …
14 Kenya …ID Reviewer …
10 Nick, Victor …
7 Danica …(1)
Generate
FoldsModel Fitting
(2)
Train
ModelRecommendation 
systemID Reviewer …
1Kenya, Lorelai …
2Ayana, Kenya …
3 Devyn …
4Beckham, Mylie …
5 Ayana …
6 Dana, Ross …
7 Danica …
8Londyn, Devyn …
9 Israel …
10 Nick, Victor …
11 Lorelai …
12 Leila, Ayana …
13 Juliette …
14 Kenya …
15 Vincent …
16 Renee, Devyn …
17 Victor …
18Ross, Samara …
19 Carlee …
20Samara, Molly …Model Evaluation
(3)
Suggest
ReviewersID Suggestion
10 Devyn, Nick
7Danica, Londyn(4)
Classify
SuggestionsCorrect
ID Suggestion
10 Nick
7 DanicaPrepared?
Incorrect
ID Suggestion
10 Devyn
7 LondynPrepared?
Fig. 1. An overview of how historical data is split into testing and training corpora for evaluating a recommendation system.
Effort prioritization DSRSs help practitioners to allocate
constrained resources in a cost-effective manner. For example,
defect prediction models [19] can be used to prioritize thetesting of areas of the code base that are: (a) most likelyto be defective [19]; (b) estimated to have the most defectsin the future [16], or (c) estimated to have the highestdefect density [27]. Moreover, effort prioritization DSRSs havebeen proposed to triage notiﬁcations that require stakeholderattention, such as posts on a mailing list [17], updates to issuereports [30], or review requests on code review platforms [39].
Expert assignment DSRSs strive to support stakeholders in
the allocation of tasks to qualiﬁed personnel. Expert assign-ment DSRSs have been proposed to suggest: (a) team membersto whom new issue reports should be assigned [2]; (b) expertswho may be able to answer questions on developer Q&Aforums [9]; and (c) reviewers to whom changelists should beassigned for code review [37].
Evaluation of Recommendation Systems. The evaluation of
recommendation systems is not simple. Since future examples
are not available and in situ evaluation is expensive and
impractical, recommendation systems in software engineeringare often evaluated using a corpus of held out historical data.Figure 1 shows how a corpus of changes is split into trainingand testing corpora for evaluating a reviewer recommendationsystem. A subset of the data (in the case of the ﬁgure,10%) is held out of the training corpus at the beginning. Thetraining corpus is then used to prepare the recommendationsystem. Each example in the testing corpus is then fed tothe recommendation system to gather its recommendations.If the recommendations do (not) appear in the actual list ofreviewers, the change is considered “correct” (“incorrect”).
However, the examples labelled “correct” (i.e., the recom-
mendations that appear in the actual list for a given change)may not have been an optimal assignee for the task. Likewise,the examples labelled “incorrect” (i.e., the recommendationsthat did not appear within the actual list) may have been just ascapable of performing the task as the actual assignee(s). Thisleads to four possible categories: those considered “correct”and well suited for the task (Nick in Figure 1); those con-sidered “correct” but not well suited for the task (Danica inFigure 1); those considered “incorrect” and not well suited forthe task (Devyn in Figure 1); and those considered “incorrect”but actually well suited for the task (Londyn in Figure 1).B. Reviewer Recommendation
In recent years, a lightweight and ﬂexible variant of the
practice of code review has become popular [7], [33]. Unlikethe rigid formal code inspections of the past [12], modern prac-tices embrace the asynchrony of global software developmentteams [5]. This lightweight variant, often referred to as ModernCode Review, revolves around a change-based model [4]. Foreach change to a system, team members are asked to critiquethe premise, structure, and content. Investment in this reviewprocess has been shown to pay off in qualitative [6], [7] andquantitative ways [26], [29], [33], [38]. Assigning ill-suitedreviewers to review a change can lead to suboptimal reviewoutcomes, such as slow [37] or less useful feedback [8], [23].
Considerable research effort has been invested in the prob-
lem of reviewer recommendation—a variant of expert assign-
ment DSRSs, which recommend appropriate personnel forreview tasks. For example, ReviewBot [3] and RevFinder [37]suggest reviewers based on how often they have contributed tothe lines or modules that were modiﬁed by the patch, respec-tively. CoRReCT [32] suggests reviewers with relevant techno-logical experience and experience with other related projects.Tie [41] combines ﬁle location and patch content analyses tosuggest relevant reviewers. Ouni et al. [31] treat the reviewer
recommendation problem as a multi-objective optimizationproblem, and solve it using evolutionary algorithms. Yanget al. [42] further specify what role the suggested reviewer
should play (e.g., managerial, technical). Yu et al. [43] explore
reviewer recommendation in the context of GitHub. Lipcaket al. [25] further analyze the level of success achieved by
reviewer recommendation systems for GitHub projects.
We select reviewer recommendation as an exemplar of
a popular research area where history-based evaluation iscustomary. We use the recent
CHR EV[44] and WLRR EC[1]
approaches as concrete reviewer recommendation solutions.
Prior work has shown that history-based evaluation of
reviewer recommendation approaches may yield misleadingresults. Indeed, Kovalenko et al. [24] found that, in the setting
of large software organizations, team members often knowwho should review their code, limiting the usefulness of therecommendation list. They argue that, in such settings, there isa need to move beyond accuracy measures when assessing thevalue of a reviewer recommendation system. Inspired by theirwork, we set out to assess the agreement between historicalmeasures and developer perceptions.
32Code 
Review 
Repository(DC1)
Extract 
Historical 
Data
(DC3a)
Train 
Recommendation 
System(PA1)
Identify 
Candidate
Participants(PA2)
Issue
Personalized
Questionnaires(PA3)
Conduct
Follow-UpInterviews(DC2)
Select 
Assigned 
Reviewers
(DC3b)
Identify 
Suggested 
ReviewersData Collection Perception Analysis
ID Reviewer …
1 Gary …
1 Sally …
2 Alice …
… … …
SuggestedAssigned Assigned 
Reviewers
Unassigned 
SuggestionsReviewer Comfort …
Gary 5 …
Sally 2 …
Alice 1 …
… … …
Candidate Comfort …
Bob 5 …
Alice 2 …
Jerry 1 …
… … …Results
Recommendation 
system
Fig. 2. An overview of our study workﬂow.
III. S TUDY DESIGN
In this section, we present our rationale for focusing our
study on the G ERRIT software community (Section III-A),
as well as our approach to the collection (Section III-B) and
analysis (Section III-C) of data.
A. Subject Community
We strive to analyze data from a large and active software
community that actively invests in the practice of code re-
view. In selecting our subject community, we identiﬁed threeimportant criteria that needed to be satisﬁed:
•Criterion 1: Track record of code review data. To
perform a robust history-based evaluation of a reviewerrecommendation system, a large amount of data aboutprior reviews must have accrued.
•Criterion 2: Traceable code review process. Re-
viewer recommendation systems often rely upon richhistorical context to produce suggestions. For example,RevFinder [37], ReviewBot [3], and xFinder [18] requireaccess to version control history to compute code ten-dency and experience heuristics. Thus, the communitythat we select must provide review records that are welllinked to the commits on which they were performed.
•Criterion 3: Responsiveness. Our perception analysis re-
lies on soliciting a large quantity of high quality data fromthe studied community. An unresponsive community maynot provide a reliable and complete perspective.
We have found that the G
ERRIT community—and by ex-
tension the data derived from the G ERRIT project—is able to
satisfy all of our evaluation criteria. In terms of Criterion 1, the
GERRIT project completed 9,847 reviews from January 2018
to December 2019. In that same period, 98% of the commitsin the version control system have an accompanying reviewrecord in the review tracking system, satisfying Criterion2. Finally, since the last author has fostered professionalrelationships and an awareness of code review research withinthe G
ERRIT community, and because Gerrit is a code review
platform, we believed that the G ERRIT community would be
amenable to and welcoming of our study, which would in turnlead to a strong response rate, satisfying Criterion 3.
B. Data Collection
The ﬁrst step in our study is to collect data from the G
ERRIT
community. Figure 2 shows that the process involves the
extraction of historical data (DC1), from which the assignedreviewers can be selected (DC2). We can use the historicaldata to train (DC3a) and evaluate (DC3b) our selected reviewerrecommendation systems. We describe each step below.TABLE I
AN OVERVIEW OF THE YEARLY REVIEW ACTIVITY OF THE GERRIT
COMMUNITY .THE YEARS WITH THE SHADED CELL BACKGROUND ARE
SELECTED FOR OUR ANALYSIS .
Year # Changes # Reviewed Percent Reviewed # Unique Reviewers
2012 1,395 1,128 90.86% 80
2013 3,324 2,877 86.55% 120
2014 2,607 2,201 84.43% 93
2015 2,983 2,569 86.12% 117
2016 4,426 4,069 91.93% 133
2017 5,184 4,407 86.85% 143
2018 5,074 4,992 98.38% 140
2019 4,773 4,687 98.20% 153
Total
(analyzed)9,847 9,679 98.29% 224
(DC1) Extract Historical Data. To obtain a corpus of data
with which to train and test our recommendation systems, weﬁrst set out to extract historical review data from the G
ERRIT
community. The community practices “dogfooding” by usingthe G
ERRIT software that the community develops. Thus, we
ﬁrst set out to extract data from the community’s G ERRIT
instance using the REST API.2For each change, the extracted
data includes: (a) a timestamp; (b) the author and reviewers;and (c) the impacted ﬁles and lines.
Table I provides an overview of the extracted G
ERRIT data.
The table shows that the community has been growing over theyears, attracting more than 4,000 contributions per year since2016. In addition, during this time, a very large proportion ofthe changes (> 85%) made to the G
ERRIT system can also be
linked to a review reported within the system. While this highproportion of linked reviews would allow us to use all of thechanges and reviews available within the G
ERRIT system, we
choose to use only the data from 2018 and 2019.3Similarly to
prior work [20], we consider that more recent changes indicatemore familiarity with a given change. Indeed, prior work [20]has posited that the experience of a developer with regards toa given change should be weighted by its age (e.g., developerexperience for a change twice as old would be halved). Wetherefore seek to solicit feedback from participants about datafrom recent changes. We use a two-year period to balance thetradeoff between the quantity of data and limiting experiencedegradation, since it may be difﬁcult for the participant torecall their experience for older changes. Thus, we chooseto focus our historical analysis on the reviews that occurredduring this two-year period.
2gerrit-review.googlesource.com
3The bulk of the data collection and analysis for this work was conducted
in 2020. Hence, data from 2020 was not included.
33(DC2) Select Assigned Reviewers. History-based evaluation
of reviewer recommendation systems requires the list of team
members who reviewed each change. To obtain this list, weﬁrst extract the user IDs listed as reviewers of each change.
We apply both reviewer and review ﬁltering to clean the raw
extracted lists of reviewers prior to training our recommenda-tion systems. First, we remove users from reviewer lists whoare known to be accounts that represent automated bots. Forexample, the GerritCI account is associated with the G
ERRIT
Continuous Integration system, not a human reviewer. Theﬁrst author also manually veriﬁed the emails, ﬁrst names, andlast names within the data to merge any accounts that couldbelong to the same individual; no such instances were detected.Second, Table I shows that a small percentage (1.71%) of thechanges made to the system during the study period were notreviewed. Since these changes cannot be used to train or testour recommendation system, we ﬁlter them out of our corpus.A corpus of 9,679 reviews spanning 224 unique reviewerssurvives the ﬁltering process.(DC3a) Train Recommendation System. There have been
several reviewer recommendation systems proposed in theliterature (see Section II). We choose to re-implement
CHR EV [44] and WLRR EC[1], since they are recently
proposed, high performance recommendation approaches.
The CHR EVapproach produces suggestions by computing a
user score for each ﬁle in the system based on how frequentlythe user has participated in reviews containing the ﬁle andhow recently they have done so. Users are given a score basedon the proportion of the total number of comments on a ﬁle(across all reviews) that the user has made. Users are alsoscored based on the number of days in which they have madea comment to a ﬁle as a proportion of the total number of daysin which the ﬁle has received review comments. Finally, usersare scored based on the most recent day that they commentedon a ﬁle during a review relative to the most recent commentmade to that ﬁle by any user.
Meanwhile, WLRR
ECuses multi-objective optimization
to consider ﬁve input measures: code ownership, reviewingexperience, patch author familiarity, review participation rate,and a metric representing the review workload. The ﬁrst fourmeasures are maximized to increase the chance of participatingin a review, and the last measure is minimized to reduce work-load. WLRR
ECrecommendations are not ranked. More details
for each approach can be found in the original CHR EV[44]
and WLRR EC[1] papers.
(DC3b) Identify Suggested Reviewers. For CHR EV, the user
scores are used to identify reviewers to suggest. For eachreview in the testing corpus, expertise and recency scoresare computed for each potential reviewer and a ranking isproduced. For WLRR
EC, the output is a list of potential
reviewers, which are all considered to be as equally valid.
To evaluate the performance of these recommendation sys-
tems, we aim to compare the history-based evaluation scoresreported in the literature with those that our models achieve.To do so, we need to split our historical data into training andtesting corpora. The selection of the testing corpus is criticalbecause it is the basis from which we will sample reviewers forour perception analysis. We allocate the latest 1,000 changesto the testing corpus (
∼10% of the data set). Rather than
randomly sampling reviews, we allocate the latest changesbecause this most closely matches the evaluation scenarioin reality. Moreover, to avoid other impractical evaluationscenarios, only those reviews that were completed prior tothe creation of the review being evaluated are used by therecommendation system. A random sampling of training andtesting corpora may create an unrealistic scenario where futuredata (e.g., changes from 2019) is used to suggest reviewers forpast events (e.g., changes from 2018).
To identify the suggested reviewers for
CHR EV, we need
to set a cutoff value kfor the ordered list of suggestions. To
compare with the initial CHR EVstudy [44], we experiment
withk=1,2,3,5settings (Section IV). We use the most
stringent k=1 setting when selecting reviewers for our
perception analysis (Section V) to avoid overburdening oursubject community with unnecessary survey requests.
C. Perception Analysis
After collecting the recommended and assigned reviewers,
we perform our perception analysis. Figure 2 shows that
the analysis is composed of identifying candidate participants(PA1), issuing personalized questionnaires (PA2), and conduct-ing follow-up interviews (PA3). Below, we describe each step.
(PA1) Identify Candidate Participants. We begin by com-
paring the recommended reviewers (see DC3b) to the lists of
actual reviewers (see DC2). We label each review with one oftwo categories. If the suggested reviewer appears in the listof actual reviewers, we label that change—and its suggestedreviewer—as “correct”. Alternatively, if the suggested re-viewer does not appear in the list of actual reviewers, we labelthat change—and the suggested reviewer—as “incorrect”.
This ﬁrst step generates a stratiﬁed data set from which
to sample, i.e., a set of 408 “correct” changes and a set of592 “incorrect” changes. We then apply stratiﬁed samplingto each category to select a representative set of changesfor further evaluation. We draw a representative sample toachieve a conﬁdence level of 95% with a conﬁdence intervalof±5% from each category independently. A sample size
calculation indicates that we need 198 “correct” changes and233 “incorrect” changes.
To satisfy our sample size, we began by randomly selecting
changes from each of the categories. However, because asmall number of reviewers perform a large proportion ofthe reviews, we found that random sampling would lead toa sample that over-represented (and overburdened) the mostactive G
ERRIT reviewers. Placing too large of a burden on very
active reviewers would make them less likely to participate.Moreover, it increases risk, since a lack of response fromsuch a reviewer would produce a large gap in study data.For these reasons, we set an upper limit of 20 reviews percandidate participant. More speciﬁcally, we continue to drawsamples randomly until we reach our target sample size (i.e.,198 “correct” and 233 “incorrect”), while ensuring that the
34upper limit is not exceeded for any reviewer. In the end, the
samples that we drew include 50 candidate participants, 26 ofwhom are present in both “correct” and “incorrect” lists, andthree and 21 of whom are only present in the “correct” and“incorrect” lists, respectively.(PA2) Issue Personalized Questionnaires. With the two
samples of changes obtained, we create the personalizedquestionnaires for each candidate participant. To ensure thatthe study is consistent for all reviewers involved, we cre-ate a template for the questionnaire. For each change, theparticipants are asked how prepared they were—or wouldhave been—to review the change on a ﬁve-point Semanticscale, whether they know of any other reviewers who mighthave been a good reviewer for the given change, and if theyhave any speciﬁc comments about the change which theyfelt were important to note. The questionnaire provides theoption to stop and submit the questionnaire after any changein the hope that this will deter candidates from abandoning thequestionnaire without submitting anything. We emailed eachcandidate participant individually soliciting their feedback viathe personalized questionnaire. We then use the responses tothese questionnaires to answer our RQs. A template for ourquestionnaires is available online [14].
Since reviewers with plenty of expertise will generally feel
comfortable with most reviews, we solicit participation fromusers with varying rates of prior review participation withinthe G
ERRIT community. Figure 3(a) shows the distribution
of the number of reviews that our candidate participants per-formed within the two-year study period. The expertise of ourreviewers broadly varies, some of whom performed very fewreviews (the ﬁrst quartile is 34 reviews) and others performedseveral reviews (the third quartile is 546 reviews). We believethat this provides an accurate cross-section of the G
ERRIT
community and reduces the likelihood of skew due to the(over)conﬁdence of individual reviewers. We used a two-tailedMann–Whitney U test to determine whether the distributionsof both populations (i.e., reviewers who performed reviews andrespondents to our survey) are equal. We obtained a p-valueof 0.9442, and therefore accept the null hypothesis that thesamples are not drawn from statistically distinct populations.In addition, Figure 3(b) shows the distribution of the samemetric for those who responded to our questionnaire. Thecandidate and respondent plots are visually similar, leadingus to conclude that reviewers with a similar cross-section ofexpertise actually participated in our questionnaire.(PA3) Conduct Follow-Up Interviews. Having obtained the
results of the questionnaire outlined within (PA2), we con-duct a series of follow up interviews intended to tackle keyemergent themes from within the results of the questionnaire.To this end, we reach out to a group of respondents to thequestionnaire who showed interest in the content of the studyand/or included responses that we felt would beneﬁt fromfurther elaboration. In total, we contacted nine members ofthe G
ERRIT community with six agreeing to participate—ﬁve
over teleconference and one over email. Table II shows thatwe interviewed participants with different roles within G
ERRITFig. 3. The distributions of the number of reviews that had been performed
in the studied period by each of the (a) candidate respondents; and (b) actualrespondents. The median, ﬁrst, and third quartiles are labelled.
TABLE II
A
N OVERVIEW OF THE PARTICIPANTS IN OUR INTERVIEWS .
Participant Participation
MediumRole within
GERRIT
P1 Email Maintainer
P2 Video Call Contributor
P3 Video Call Contributor
P4 Video Call Maintainer
P5 Video Call Contributor
P6 Video Call Contributor
including maintainers and users without a speciﬁed position,
experienced reviewers and new members of the community.
We apply a semi-structured approach to our interviews. The
ﬁrst and last authors conducted and were present for all ofthe interviews. An outline of key questions and interviewtopics were created prior to the interview session, but inter-viewers and interviewees had the freedom to follow (relevant)divergent topics if they arose based on responses. Questionswere initially formulated based on trends in the questionnaireresponses, as well as the individual interviewee responses.
The interviews were recorded, transcribed, and coded to
extract themes. We use these themes to deepen the insightsthat we glean from our questionnaires. As such, we code thetranscripts based on whether the responses relate speciﬁcallyto our research questions. When responses relate directly to aresearch question, we check if multiple community membersmention similar points regarding the same topic. In the cases inwhich they do, we report these ﬁndings along with the partici-pants who stated them (participants are labeled as P1–P6 whenoutlining their responses) in order to corroborate or contradictthe observations that we draw from the questionnaire.
35IV . P RELIMINARY EV ALUATION
To test if our recommendation system implementations are
on par with the original implementations [1], [44], we evaluate
our replicated solutions on a sample of historical data.
A. Experimental Setup
Sample size. We test both approaches on 150 G ERRIT changes
to mimic the testing corpus size outlined in the original
CHR EVpaper, where three of the four systems tested had
sample sizes of roughly 150 changes. Furthermore, sampling
the most recent 150 changes allows for the largest possiblemodel to be created in each case. We therefore use 150 changesto provide a fair comparison with the original paper.
Evaluation metrics. To test the recommendation systems in
a comparative manner, we use the same evaluation metrics
as the original
CHR EVpaper, i.e., precision, recall, F1-score,
and Mean Reciprocal Rank (MRR). For a given review x, the
precision, recall and F1-score are:
precision( x)=|Suggested( x)∩Actual(x)|
|Suggested( x)|(1)
recall(x)=|Suggested( x)∩Actual(x)|
|Actual(x)|(2)
F1-score( x)=2×precision( x)×recall(x)
precision( x)+recall( x)(3)
where Suggested (x) is the list of suggested reviewers for x
and Actual (x) is the list of team members who reviewed x.
Precision, recall, and F1-score are rank-agnostic perfor-
mance measures, which ignore the position at which a revieweris suggested. MRR is a rank-aware measure that accounts forthe rank at which correct suggestions appear. The higher therank of correct suggestions, the better (larger) the MRR value.A perfect MRR of one indicates that, for all reviews, theﬁrst suggested reviewer appears in the list of actual reviewers,while MRR values that approach zero would imply that the“correct” suggestions rank near the bottom of the list. Morespeciﬁcally, MRR is deﬁned as:
MRR =1
|n||n|/summationdisplay
i=11
rank i(4)
wherenis the number of reviews on which the recommen-
dation systems are evaluated and rank iis the position of
the ﬁrst actual reviewer in the list of suggested reviewers.Since WLRR
ECdoes not rank candidates, this metric is only
computed for our CHR EVimplementation.
B. Experimental Results
Table III shows that our implementation of CHR EVis
able to achieve a similar level of performance to that ofthe original implementation. For all measures and studiedksettings, our implementation achieves performance scores
within±5 percentage points of those reported in the original
paper, suggesting that our replication is largely successful.
Similarly, Table IV shows that our implementation of WL-
RR
ECis able to achieve similar results to those of the originalTABLE III
COMPARISON OF C HR EV ON THE GERRIT (VALUE )PROJECT COMPARED
TO THE ORIGINAL ’S PERFORMANCE ON THE ANDROID PLATFORM (Δ).
kPrecision Recall F1-Score MRR
Value Δ Value Δ Value Δ Value Δ
1 0.52 +0.02 0.25 -0.02 0.34 -0.01 0.62 -0.03
2 0.40 -0.01 0.37 -0.05 0.39 -0.02 0.62 -0.03
3 0.35 0.00 0.46 -0.04 0.39 -0.02 0.62 -0.035 0.28 -0.02 0.56 -0.05 0.37 -0.03 0.62 -0.03
TABLE IV
COMPARISON OF WLRR EC ON THE GERRIT PROJECT (VALUE )COMPARED
TO THE ORIGINAL ’S PERFORMANCE ON THE ANDROID PLATFORM (Δ).
Evaluation TypePrecision Recall F1-Score
Value Δ Value Δ Value Δ
Actual 0.15 -0.05 0.71 +0.41 0.25 -0.03
Potential 0.31 0.0 0.49 -0.01 0.38 +0.03
implementation. The ‘Actual’ and ‘Potential’ reviewers rows
reﬂect the different choices of ground truth data as describedin the original paper [1]. Although the recall values are muchlarger in our ’Actual Reviewers’ ground truth setting, webelieve that the consistent values for all other ﬁelds suggestthat our replication was successful in this case as well. Webelieve that the differences in results are likely due to thedifference in the average size of the list of actual reviewersfor each change, and the differences in datasets.
We ﬁnd that 63% of the WLRR
ECrecommendations over-
lap with the CHR EVrecommendations with the k=1 setting.
Since higher ksettings reduce the overlap (59% (k =2), 56%
(k=3), 51% (k =5)), we concentrate our analysis on the k=1
setting. Due to the large overlap in results, and because the
CHR EVresults are ranked and therefore allow us to target
speciﬁc developers from within the recommendation list, wechoose to use the recommendations from
CHR EVto select
participants for our questionnaires and interviews.
V. S TUDY RESULTS
In this section, we describe the results of our study with
respect to our research questions. For each question, wepresent our rationale, followed by our approach for addressingthe question, and ﬁnally, we present the results.
(RQ1) Optimism: How often are past assignees that were
deemed correct actually suboptimal?
Motivation. When performing an evaluation of the correctness
of a reviewer recommendation system, researchers often count
a recommendation as correct when the reviewer suggested bythe system for a past change actually performed the review.However, the reviewers who reviewed past changes may not bethe best choice to perform the review. For example, reviewersmay participate in a review to learn about a module [7] or tocover for the optimal reviewer who is unavailable at the time ofthe review. These non-technical factors may introduce noise inthe “correct” signal recorded in past review participation. This
36TABLE V
AN OUTLINE OF THE FREQUENCY OF RESPONSES FOR BOTH
“CORRECT ”AND “INCORRECT ”CHANGES
Comfort scoreComfort score frequency
“Correct” changes “Incorrect” changes
11 1
25 535 941 3 2 2
55 2 2 1
type of noise would lead history-based evaluations to be overly
optimistic, over-estimating the true performance of a system,since some of the cases labelled as “correct” by the evaluationmay actually be incorrect. Hence, we set out to quantify andcharacterize the optimism of history-based evaluations.
Approach. To address RQ1, we select the sample of 198
“correct” cases, i.e., where the suggested reviewer matches
a reviewer who performed a past review. We follow oursampling strategy from Section III-C (see step PA1). Foreach unique reviewer in our sample, we create a personalizedquestionnaire, which consisted of one page for each review inour sample. Each page asked the participant to (a) rate howcomfortable they were when performing the review in question(on a ﬁve-point Semantic scale); (b) optionally recommendanother team member who may have been able to perform thereview; and (c) optionally provide their rationale for eitheror both of their prior responses (free-form text). We use theSemantic scale responses to gain an overview of the optimismof history-based recommendation evaluations. We then codethe free-form responses using an open coding approach [35].We use the most frequent codes to generate hypotheses aboutthe nature of misassigned reviews. We test the validity of thesehypotheses qualitatively during follow-up interviews (see stepPA3 in Section III-C) and quantitatively using Spearman’s ρ
rank-based correlation coefﬁcient when appropriate.
Results. We contacted 29 candidate participants of various
levels of seniority within the G
ERRIT community. We received
twelve responses (a response rate of 41%). These responses
contained assessments of 76 of the 198 subject changes (38%).
It is rare for past assignees who were deemed correct to
have been uncomfortable with their past reviews. Table V
shows the distribution of comfort score responses. The meancomfort level for the 76 changes that received responses is4.45. Moreover, in 65 of those 76 cases (86%), the level ofconﬁdence reported by respondents was four or ﬁve. Sinceour sample contains 76 instead of 198 cases, and our originalpopulation is 402 “correct” changes, with a conﬁdence level of95%, our conﬁdence interval unfortunately expands to 7.03%.Nevertheless, this bodes well for history-based evaluationsof review recommendation, since our responses suggest thatcurrent ground truth approaches are not overly optimistic.
The follow-up interviews further corroborate these ﬁndings.
Multiple interviewees (P5, P6) mentioned that they lookthrough a change’s contents and the other reviewers alreadyworking on a change to determine whether they will bringvalue to a review discussion before accepting a review. Onereviewer (P6) stated that they often use about a quarter of thetotal time spent on a change ﬁguring out if they will providevalue. This suggests that when reviewers accept a review, theyare rarely a bad candidate for evaluating it.
Comfort score values are difﬁcult to estimate. In their
free-form responses, respondents often mentioned that theyconsider the change size and its type (e.g., merge vs. normalcommit) when considering whether they would be an appro-priate reviewer. Thus, we tested for correlations between sizeheuristics (number of changed lines, ﬁles) and change type(i.e., merge commit or not) and the comfort scores reported.For the number of lines changed and ﬁles changed, we observeweak-to-negligible correlations of ρ=−0.12(p=0.31)
andρ=−0.29(p=0.01), respectively. While the number
of ﬁles has a statistically signiﬁcant correlation (p<0. 05),
its magnitude (ρ) is considered weak [36]. Finally, 18 ofthe 76 changes with a response were merge commits. Themean comfort score for merge commits was actually slightlyhigher (4.58) than non-merge commits (4.41). We perform anunpaired two-tailed Mann-Whitney U-test (a non-parametricstatistical hypothesis test) to compare the comfort scores inthe merge and non-merge groups. The results do not supportthe rejection of the null hypothesis that both groups are drawnfrom the same population (p =0.35). Moreover, the Cliff’s
delta [10] (an effect size measure) is 0.05, which indicates thatthe practical difference between the samples is negligible.
During follow-up interviews, all participants mentioned
encountering trivial changes when ﬁlling out the questionnaire.Reviewing these trivial changes does not require the samerigour. However, the interviewees did not share a commondeﬁnition of what constitutes a trivial change. Deﬁnitionstouched upon (a) which components a change modiﬁes (P1);(b) how many stakeholders are potentially impacted (P1);(c) the content of the change, i.e., whether it is routinemaintenance, which can be reviewed by a larger group thannew features added to the system (P6); and (d) the amount ofdiscussion that is garnered from the community (P4). Intrigu-ingly, two interviewees felt that merge changes are generallytrivial to review (P2, P4), while another interviewee felt thatmerge commits are rarely trivial and often required a carefulreview to catch potential regressions (P5). This general beliefthat a set of trivial changes exists without a clear deﬁnitionconverges with our ﬁndings from the questionnaire. Indeed,many of the reviewers who participated in the questionnairefelt very comfortable reviewing a large set of changes, buta clear pattern of what caused certain changes to be morebroadly “reviewable” than others did not emerge.
Reviewers tend to carefully select the reviews that theyaccept. Thus, reviewers were often highly comfortablewith reviews that reviewer recommendation evaluationswould deem “correct” recommendations. This suggeststhat optimism is not a serious problem in the context ofthe evaluation of reviewer recommendation systems.
37(RQ2) Pessimism: How often are incorrect recommendations
actually appropriate?
Motivation. Reviewer recommendation evaluations often con-
sider a recommendation to be incorrect when the reviewer
recommended by the model did not review the change. How-ever, the results from RQ1 suggest that reviewers who areknowledgeable about a system will likely have many changesfor which they are the most qualiﬁed reviewer. Since theymust balance reviewing time with other tasks, reviewers maynot be able to participate in the review process of everychange for which they are suitable. Thus, relying solely onthe reviewer who performed the review as the ground truthmay underestimate the value of a reviewer recommendationmodel. To assess this, we set out to study if the ground truthdata for reviewer recommendation systems is too pessimistic.
Approach. Similar to RQ1, we contact the reviewers whose
changes had been sampled (see Section III-C). However,
unlike RQ1, the changes that we sampled for RQ2 are changesfor which the top-ranked reviewer suggested by the recom-mendation system did not perform the review. As was done forRQ1, we apply an open coding procedure to code the free-formsurvey results and then perform quantitative and qualitativeevaluations of the most commonly occurring themes usingcorrelation analyses and follow-up interviews, respectively.
Results. We contacted 47 reviewers (16 (34%) responded).
The responses cover 58 of the targeted 233 changes (25%).
A large proportion of the recommended reviewers that
are labelled as “incorrect” would have been comfortable
assessing the given change. Table V shows the distribution
of the comfort score responses. The mean comfort score forthe 58 changes is 3.98. Moreover, 43 of 58 (74%) received acomfort score of 4 or 5. Unfortunately, since our sample sizeincludes 58 instead of 233 changes, with an initial populationsize of 598 “incorrect” changes and a conﬁdence level of 95%,our conﬁdence interval increases to 10.74%. Nonetheless, theresponses still suggest that a large proportion of “incorrect”reviewer recommendations are actually reviewers who wouldhave been comfortable to assess a given change.
The follow-up interviews suggest that reviewers often have
reasons other than their comfort level for not performingreviews. Three interviewees (P1, P2, P5) mentioned that theyare quite often not able to review all of the changes they wishto due to time constraints. These interviewees emphasized thata review takes a considerable amount of time to perform welland thus, when there are a large number of changes withintheir area of expertise, they will not be able to review all ofthem. This suggests that the current approach to building aground truth for reviewer recommendation is likely overlook-ing suitable candidates.
The difference in the comfort scores of “correct” and
“incorrect” recommendations is not large. Indeed, the
difference in mean comfort score between “correct” (RQ1)and “incorrect” (RQ2) cases is 0.47. Moreover, the differencein the proportion of reviews for which the reviewer is com-fortable (providing a comfort score of 4 or 5) is only twelvepercentage points. An unpaired two-tailed Mann-Whitney U-test comparing the comfort scores in the two groups indicatesthat there is a statistically signiﬁcant difference (p =0.004);
however, the Cliff’s delta [10] is considered small ( δ=0.31).
While these are non-negligible differences, their differencein magnitude is exaggerated in the evaluation of reviewerrecommendations, where one case will be considered correctand the other incorrect.
The typical ground truth approaches do not account for
the way in which reviews are handled in practice. Four
of the six interviewees (P1, P4, P5, P6) mentioned that thechoice to review a change is not solely about having completeknowledge of a change’s content. They involve themselves inreviews to force themselves to learn more about the codebaseor to steer the evolution of the project. Treating a reviewer as“correct” or “incorrect” may overlook such nuanced reasonsfor a community member’s participation on a review.
Review size does not share a signiﬁcant correlation
with the comfort score. Similar to RQ1, we ﬁnd participants
often mentioned that size was a determining factor in theircomfort level. However, when we measured the correlationbetween common size heuristics and the reported comfortscore, we do not observe any strong quantitative evidence ofthat relationship. Indeed, the number of lines of code changedby a patch and the comfort score share a negligible, statisticallyinsigniﬁcant negative correlation (ρ =−0.05,p=0.73).
Moreover, the number of ﬁles changed and the comfort scoreshare a weak, statistically insigniﬁcant negative correlation(ρ=−0.17,p=0.21).
Recommended reviewers are often considered incorrectwhen the suggested reviewer did not perform the review.However, we ﬁnd that in the majority of such cases, therecommended reviewers were actually quite comfortableto review those changes. Indeed, the difference in thecomfort scores of “correct” and “incorrect” recommen-dations is small.
VI. P
RACTICAL IMPLICATIONS
In our estimation, our study has three key implications
for researchers in the areas of reviewer recommendation andrecommendation systems for software engineering.
A. Reviewer Recommendation
Current evaluation procedures systematically under-
estimate the performance of reviewer recommendation
systems. Our ﬁndings indicate that reviewers who performed
the review rarely felt uncomfortable with the task (RQ1).This suggests that the common ground truth for reviewer rec-ommendation evaluations (i.e., the reviewers who performedthe task) are generally sound. Perhaps more interestingly,our ﬁndings also indicate that suggested reviewers who didnot perform reviews were rarely uncomfortable with thosetasks (RQ2). This suggests that history-based evaluations willunderestimate the performance of a reviewer recommendationsystem, since such cases will be labelled as “incorrect”.
38Recommending the “correct” reviewer may not be the
best way to evaluate reviewer recommendation systems.
Our participants pointed out that they participate in reviewsfor various reasons that do not relate to being the areaexpert. For example, they use assigned reviews as a forcingfunction to learn about an area of the codebase (RQ2). Whilethere is a growing literature illustrating the non-technicalbeneﬁts and challenges of code reviewing practices [4], [6],[7], [33], its impact on reviewer recommendation is not yetfully understood. Indeed, Kovalenko et al. [24] observe that
recommended reviewers rarely provide value for the authorsof changes. Since we observe that reviewer recommendationsystems rarely suggest reviewers who are uncomfortable withthe reviewing task (RQ1, RQ2), we believe that identifyingthe “correct” reviewer is a relatively easy target to achieve—in fact, our data suggests performance in the literature islikely being underestimated. Instead, we recommend that thenext goal for reviewer recommendation systems should be tobalance the reviewing workload while optimizing for team-based constraints. A recent example of such an approach wasexplored by Mirsaeedi and Rigby [28], who propose methodsto mitigate turnover risk, i.e., the cost associated with thedeparture of a team member, by simulating the reassignmentof reviewers to different reviewing tasks.
B. Recommendation Systems for Software Engineering
The “status quo” history-based evaluations may not be
appropriate. History-based evaluations are almost ubiquitous
in evaluating recommendation systems in software engineer-
ing. While our ﬁndings are speciﬁc to the reviewer recom-mendation problem, we suspect that they could generalize to(at least) most other expert assignment DSRSs. Nonetheless,our ﬁndings serve as an existence proof that history-basedevaluations are imperfect. Based on our results, we recommendthat researchers evaluate the appropriateness of a history-based evaluation using qualitative as well as quantitativeevaluations [13], [22] before adopting it.
VII. T
HREATS TO VALIDITY
Below, we discuss the threats to the validity of our study.
A. Construct V alidity
Threats to construct validity jeopardize the certainty that
an operationalization measures what it set out to measure. Toconduct our study, we reimplemented two reviewer recom-mendation approaches. While we believe that they are validreimplementations, we did not have access to the originalsource code and could only implement the systems based onwhat was described in their respective papers [1], [44]. As aresult, it is possible that our reviewer recommendation systemsare not exact replicas. On the other hand, our preliminaryevaluations achieved performance scores that were similar tothose reported in the original papers.
In addition, our sampling procedure for our questionnaire
analysis has limitations. First, we chose to limit the numberof reviews to which a given contributor was asked to respond.While we took measures to ensure the set of sampled changesmeaningfully represents the overall set of reviewed changes,limiting the number of reviews was a hindrance. Second,we only evaluated recommendations from the k=1 setting
(i.e., quantity of suggested reviewers). While we understandthat another ksetting may be more reﬂective of what is
done in the Gerrit community, we selected k=1 to avoid
overburdening our participants. We veriﬁed the inﬂuence onthe results of the two approaches by testing various kvalues
for WLRR
ECand CHR EVand found that larger kvalues
still contained high overlap between the two recommendationsystems. Therefore, the k=1 setting allowed us to reduce the
burden on the community while having a limited impact onthe study scope. Furthermore, each unit increase in kwould
require 431 more requests for information from reviewersto retain our sample size. These additional requests riskedantagonizing the community, potentially reducing our overallresponse rate. We attempted to mitigate this issue by not onlyconducting a survey but also conducting developer interviewsto get clear insights from the developers. We believe that thisdecision was warranted as we have been able to achieve ahigh response rate (41% of reviewers in the optimistic caseand 34% in the pessimistic case participated).
In addition, several participants could only respond to a
subset of the changes that we selected for them, i.e., 35%of optimistic sampled reviews and 25% of pessimistic sam-pled reviews received responses. While this did expand ourconﬁdence intervals (i.e., ±5% was expanded to ±7.03%
and 10.74%, respectively), we were still able to computemeaningful estimates. We acknowledge that our sample sizescould have been calibrated to reﬂect prior software engineeringsurvey response rates. However, we believe that this threatwas mitigated by our comparatively high response rate, andbecause we were still able to compute meaningful estimatesfrom the data that we did obtain.
Finally, we rely on the self-assessments of the comfort levels
of our respondents to estimate their suitability for tasks. Sinceself-assessments may not accurately reﬂect true ability levels,these values may be skewed.
B. Internal V alidity
Threats to internal validity pertain to alternative hypotheses
that could also explain the results that we observe. One such
threat is that the most experienced reviewers will generallyfeel comfortable reviewing most changes to the G
ERRIT code
base. We acknowledge this threat and strive to mitigate it bysoliciting participation from a broad cross-section of partici-pants with varying expertise levels from within the G
ERRIT
community (see Section III-C).
C. External V alidity
Threats to external validity have to do with the general-
izability of our conclusions to other contexts. Due to the
practical cost of our analysis procedure, we chose to focusour study on one community. We acknowledge that the speciﬁccharacteristics of the Gerrit community may have inﬂuenced
39the overall results of the study and thereby affect the gener-
alizability of our conclusions. However, we believe that theﬁndings presented in this paper apply to a diverse selectionof developers because our questionnaire participants and in-terviewees are employed by several organizations includingmultinationals like Google, Apple, and Ericsson, as well assmall to medium-sized enterprises like GerritForge. While theyall participate in the G
ERRIT community, their perspectives are
informed by the internal reviewing processes at their employersites. Nevertheless, replication of our study in the context ofother organizations may prove useful.
VIII. C
ONCLUSION AND FUTURE WORK
Like any system, reviewer recommendation systems require
evaluation approaches to understand how well they are able toperform. The most prominent evaluation scheme for recom-mendation systems in software engineering relies heavily onhistorical records to act as a ground truth on which to test. Thisground truth may lead to incorrectly labelled recommenda-tions, as in the context of reviewer recommendation, reviewerswho reviewed past changes may not have been comfortabledoing so. In addition, candidates who were well prepared toreview a change may not have had the opportunity to do so.
Hence, in this paper, we investigate the following question:
Is historical data an appropriate benchmark
for reviewer recommendation systems?
Through a case study of the G
ERRIT community, we ﬁnd
that the answer is no because:
•The current ground truth is overly pessimistic and oftenmislabels reviewers as incorrect when they would havebeen comfortable reviewing the change.
•The difference in comfort level of reviewers labelled“correct” and “incorrect” by the original ground truthis small (Mean comfort scores of 4.45 vs 3.98, Mann-Whitney U-test p=0.004, small effect size (δ =0.31)).
These results suggest that recommending “correct” review-
ers may not be a difﬁcult hurdle to clear. This might ex-plain why reviewer recommendations usually do not providevaluable support for developers in commercial settings [24].However, we do not believe that reviewer recommendationsystems are without an application. By pivoting the goal ofsuch recommendation systems away from identifying “correct”reviewers to aiding with other non-technical goals, such asminimizing the workload of the core team or mitigatingthe risk of knowledge loss [28], we believe that reviewerrecommendation systems may ﬁnd a more useful niche.
R
EFERENCES
[1] W. H. A. Al-Zubaidi, P. Thongtanunam, H. K. Dam, C. Tantithamtha-
vorn, and A. Ghose, “Workload-aware reviewer recommendation using a
multi-objective search-based approach,” in Proceedings of the 16th ACM
International Conference on Predictive Models and Data Analytics inSoftware Engineering, ser. PROMISE 2020. New York, NY , USA:Association for Computing Machinery, 2020, p. 21–30.
[2] J. Anvik, L. Hiew, and G. C. Murphy, “Who should ﬁx this bug?” in
Proceedings of the International Conference on Software Engineering,2006, pp. 361–370.[3] V . Balachandran, “Reducing human effort and improving quality in peer
code reviews using automatic static analysis and reviewer recommenda-tion,” in 2013 35th International Conference on Software Engineering
(ICSE), 2013, pp. 931–940.
[4] T. Baum, O. Liskin, K. Niklas, and K. Schneider, “A faceted classiﬁ-
cation scheme for change-based industrial code review processes,” in2016 IEEE International Conference on Software Quality, Reliabilityand Security (QRS), 2016, pp. 74–85.
[5] T. Baum and K. Schneider, “On the need for a new generation of code
review tools,” in Product-Focused Software Process Improvement,1 1
2016, pp. 301–308.
[6] O. Baysal, O. Kononenko, R. Holmes, and M. W. Godfrey, “Investigating
technical and non-technical factors inﬂuencing modern code review,”Empirical Software Engineering, vol. 21, no. 3, pp. 932–959, 2016.
[7] C. Bird and A. Bacchelli, “Expectations, outcomes, and challenges
of modern code review,” in Proceedings of the International
Conference on Software Engineering. IEEE, May 2013. [On-line]. Available: https://www.microsoft.com/en-us/research/publication/expectations-outcomes-and-challenges-of-modern-code-review/
[8] A. Bosu, M. Greiler, and C. Bird, “Characteristics of useful code
reviews: An empirical study at microsoft,” in Proceedings of the
International Conference on Mining Software Repositories, 2015, pp.146–156.
[9] M. Choetkiertikul, D. Avery, H. K. Dam, T. Tran, and A. Ghose, “Who
will answer my question on stack overﬂow?” in Proceedings of the
Australasian Software Engineering Conference, 2015, pp. 155–164.
[10] N. Cliff, “Dominance statistics: Ordinal analyses to answer ordinal
questions,” Psychological bulletin, vol. 114, no. 3, p. 494, 1993.
[11] D. Cubranic, G. C. Murphy, J. Singer, and K. S. Booth, “Hipikat:
A project memory for software development,” IEEE Transactions on
Software Engineering, vol. 31, no. 6, pp. 446–465, 2005.
[12] M. E. Fagan, “Design and code inspections to reduce errors in program
development,” IBM Systems Journal, vol. 15, no. 3, pp. 182–211, 1976.
[13] J. Garcia-Gathright, C. Hosey, B. S. Thomas, B. Carterette, and F. Diaz,
“Mixed methods for evaluating user satisfaction,” in Proceedings of
the 12th ACM Conference on Recommender Systems, ser. RecSys ’18.New York, NY , USA: Association for Computing Machinery, 2018, p.541–542.
[14] I. Gauthier, M. Lamothe, G. Mussbacher, and S. McIntosh, “Is historical
data an appropriate benchmark for reviewer recommendation systems?a case study of the gerrit community,” Aug 2021. [Online]. Available:https://ﬁgshare.com/articles/conference
contribution/Is Historical
Data anAppropriate Benchmark forReviewer Recommendation
Systems ACase Study oftheGerrit Community/14473575/1
[15] P. J. Guo, T. Zimmermann, N. Nagappan, and B. Murphy, “Characteriz-
ing and predicting which bugs get ﬁxed: An empirical study of microsoftwindows,” in Proceedings of the International Conference on Software
Engineering, 2010, pp. 495–504.
[16] A. E. Hassan and R. C. Holt, “The top ten list: Dynamic fault
prediction,” in Proceedings of the International Conference on Software
Maintenance, 2005, pp. 263–272.
[17] W. M. Ibrahim, N. Bettenburg, E. Shihab, B. Adams, and A. E.
Hassan, “Should i contribute to this discussion?” in Proceedings of the
International Conference on Mining Software Repositories, 2010, pp.181–190.
[18] H. Kagdi, M. Hammad, and J. I. Maletic, “Who can help me with
this source code change?” in 2008 IEEE International Conference on
Software Maintenance, 2008, pp. 157–166.
[19] Y . Kamei and E. Shihab, “Defect prediction: Accomplishments and
future challenges,” in Proceedings of the Future of Software Engineering
track of the International Conference on Software Analysis, Evolution,and Reengineering, 2016, pp. 33–45.
[20] Y . Kamei, E. Shihab, B. Adams, A. E. Hassan, A. Mockus, A. Sinha,
and N. Ubayashi, “A large-scale empirical study of just-in-time qualityassurance,” IEEE Transactions on Software Engineering, vol. 39, no. 6,
pp. 757–773, 2013.
[21] M. Kersten and G. C. Murphy, “Mylar: A degree-of-interest model
for ides,” in Proceedings of the International Conference on Aspect-
Oriented Software Development, 2005, pp. 159–168.
[22] B. P. Knijnenburg, M. C. Willemsen, Z. Gantner, H. Soncu, and
C. Newell, “Explaining the user experience of recommender systems,”User Modeling and User-Adapted Interaction, vol. 22, no. 4, pp. 441–504, Oct 2012.
40[23] O. Kononenko, O. Baysal, and M. W. Godfrey, “Code review quality:
How developers see it,” in Proceedings of the International Conference
on Software Engineering, 2016, pp. 1028–1038.
[24] V . Kovalenko, N. Tintarev, E. Pasynkov, C. Bird, and A. Bacchelli,
“Does reviewer recommendation help developers?” IEEE Transactions
on Software Engineering, pp. 1–1, 2018.
[25] J. Lipcak and B. Rossi, “A large-scale study on source code reviewer
recommendation,” in 2018 44th Euromicro Conference on Software
Engineering and Advanced Applications (SEAA), 2018, pp. 378–387.
[26] S. McIntosh, Y . Kamei, B. Adams, and A. E. Hassan, “The impact of
code review coverage and code review participation on software quality:
A case study of the qt, vtk, and itk projects,” 11th Working Conference
on Mining Software Repositories, MSR 2014 - Proceedings, 05 2014.
[27] T. Mende and R. Koschke, “Effort-aware defect prediction models,” in
Proceedings of the European Conference on Software Maintenance andReengineering, 2010, pp. 107–116.
[28] E. Mirsaeedi and P. Rigby, “Mitigating turnover with code review recom-
mendation: Balancing expertise, workload, and knowledge distribution,”inProceedings of the International Conference on Software Engineering ,
2020, p. To appear.
[29] R. Morales, S. McIntosh, and F. Khomh, “Do code review practices
impact design quality? a case study of the qt, vtk, and itk projects,”inProceedings of the International Conference on Software Analysis,
Evolution, and Reengineering, 2015, pp. 171–180.
[30] D. Mukherjee and M. Garg, “Which work-item updates need your
response?” in Proceedings of the International Conference on Mining
Software Repositories, 2013, pp. 12–21.
[31] A. Ouni, R. G. Kula, and K. Inoue, “Search-based peer reviewers
recommendation in modern code review,” in 2016 IEEE International
Conference on Software Maintenance and Evolution (ICSME), 2016, pp.367–377.
[32] M. M. Rahman, C. K. Roy, and J. A. Collins, “Correct: Code reviewer
recommendation in github based on cross-project and technologyexperience,” in Proceedings of the 38th International Conference
on Software Engineering Companion, ser. ICSE ’16. New York,NY , USA: Association for Computing Machinery, 2016, p. 222–231.[Online]. Available: https://doi.org/10.1145/2889160.2889244
[33] P. C. Rigby and C. Bird, “Convergent software peer review
practices,” in Proceedings of the the joint meeting of the
European Software Engineering Conference and the ACMSIGSOFT Symposium on The Foundations of Software Engineering(ESEC/FSE). ACM, August 2013, preprint available uponrequest to cbird@microsoft.com or peter.rigby@concordia.ca. [On-line]. Available: https://www.microsoft.com/en-us/research/publication/convergent-software-peer-review-practices/
[34] M. P. Robillard, W. Maalej, R. J. Walker, and T. Zimmermann, Eds.,
Recommendation Systems in Software Engineering. Springer, 2014.
[35] A. Strauss and J. M. Corbin, Grounded theory in practice. Sage, 1997.
[36] R. Taylor, “Interpretation of the correlation coefﬁcient: A basic review,”
Journal of Diagnostic Medical Sonography, vol. 6, no. 1, pp. 35–39,1990.
[37] P. Thongtanunam, C. Tantithamthavorn, R. G. Kula, N. Yoshida, H. Iida,
and K. Matsumoto, “Who should review my code? a ﬁle location-basedcode-reviewer recommendation approach for modern code review,”in2015 IEEE 22nd International Conference on Software Analysis,
Evolution, and Reengineering (SANER), 2015, pp. 141–150.
[38] P. Thongtanunam, S. McIntosh, A. E. Hassan, and H. Iida, “Investigating
code review practices in defective ﬁles: An empirical study of the qtsystem,” in Proceedings of the International Conference on Mining
Software Repositories, 2015, pp. 168–179.
[39] R. Wen, D. Gilbert, M. G. Roche, and S. McIntosh, “BLIMP Tracer:
Integrating Build Impact Analysis with Code Review,” in Proceedings of
the International Conference on Software Maintenance and Evolution,2018, pp. 685–694.
[40] W. E. Wong, R. Gao, Y . Li, R. Abreu, and F. Wotawa, “A survey on
software fault localization,” IEEE Transactions on Software Engineering,
vol. 42, no. 8, pp. 707–740, 2016.
[41] X. Xia, D. Lo, X. Wang, and X. Yang, “Who should review this
change? putting text and ﬁle location analyses together for more accuraterecommendations,” in Proceedings of the International Conference on
Softwar e Maintenance and Evolution, 2015, pp. 261–270.
[42] C. Yang, X.-h. Zhang, L.-b. Zeng, Q. Fan, T. Wang, Y . Yu, G. Yin, and
H.-m. Wang, “Revrec: A two-layer reviewer recommendation algorithm
in pull-based development model,” Journal of Central South University,
vol. 25, pp. 1129–1143, 05 2018.
[43] Y . Yu, H. Wang, G. Yin, and T. Wang, “Reviewer recommendation for
pull-requests in github: What can we learn from code review and bug
assignment?” Information and Software Technology, vol. 74, 01 2016.
[44] M. B. Zanjani, H. Kagdi, and C. Bird, “Automatically recommending
peer reviewers in modern code review,” IEEE Transactions on Software
Engineering, vol. 42, no. 6, pp. 530–543, 2016.
[45] T. Zimmermann, A. Zeller, P. Weissgerber, and S. Diehl, “Mining
version histories to guide software changes,” IEEE Transactions on
Software Engineering, vol. 31, no. 6, pp. 429–445, 2005.
41