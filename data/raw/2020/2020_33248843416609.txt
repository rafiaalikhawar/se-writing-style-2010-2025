Cats Are Not Fish: Deep Learning Testing Calls for
Out-Of-Distribution Awareness
DavidBerend
NanyangTechnological University,
SingaporeXiaofei Xie‚àó
Nanyang Technological University,
SingaporeLei Ma
Kyushu University, Japan
Lingjun Zhou
TianjinUniversity, ChinaYang Liu
Nanyang Technological University,
Zhejiang Sci-Tech University, ChinaChi Xu
Singapore Institute of Manufacturing
Technology, A*Star
Jianjun Zhao
Kyushu University, Japan
ABSTRACT
As Deep Learning (DL) is continuously adopted in many industrial
applications, its quality and reliability start to raise concerns. Simi-
lar to the traditional software development process, testing the DL
software to uncover its defects at an early stage is an effective way
to reduce risks after deployment. According to the fundamental
assumption of deep learning, the DL software does not provide
statistical guarantee and has limited capability in handling data
that falls outside of its learned distribution, i.e., out-of-distribution
(OOD) data. Although recent progress has been made in designing
noveltestingtechniquesforDLsoftware,whichcandetectthou-
sandsoferrors,thecurrentstate-of-the-artDLtestingtechniques
usually do not take the distribution of generated test data into
consideration. It is therefore hard to judge whether the ‚Äùidentified
errors‚ÄùareindeedmeaningfulerrorstotheDLapplication(i.e.,due
toqualityissuesofthemodel)oroutliersthatcannotbehandled
bythecurrentmodel(i.e.,duetothelackoftrainingdata).Tofill
this gap, we take thefi rst step and conduct a large scale empirical
study,withatotalof451experimentconfigurations,42deepneural
networks (DNNs) and 1.2 million test data instances, to investigate
and characterize the impact of OOD-awareness on DL testing. We
furtheranalyzetheconsequenceswhenDLsystemsgointoproduc-
tionbyevaluatingtheeffectivenessofadversarialretrainingwith
distribution-awareerrors.Theresultsconfirmthatintroducingdata
distribution awareness in both testing and enhancement phases
outperformsdistributionunaware retraining by up to 21.5%.
CCSCONCEPTS
‚Ä¢Computingmethodologies ‚ÜíNeuralnetworks ;‚Ä¢Software
and its engineering ‚ÜíSoftware testing and debugging .
KEYWORDS
Deep learning testing, quality assurance, out of distribution
ACM Reference Format:
DavidBerend,XiaofeiXie,LeiMa,LingjunZhou,YangLiu,ChiXu,andJian-
jun Zhao. 2020. Cats Are Not Fish: Deep Learning Testing Calls for Out-
Of-DistributionAwareness.In 35thIEEE/ACMInternationalConferenceon
AutomatedSoftwareEngineering(ASE‚Äô20),September21‚Äì25,2020,Virtual
Event, Australia. ACM, New York, NY, USA, 12pages.https://doi.org/10.
1145/3324884.3416609
‚àóXiaofeiXie (xfxie@ntu.edu.sg) is the corresponding author.1 INTRODUCTION
Recently,deeplearning(DL)achievedtremendoussuccessandis
continuously adopted in many applications, such as image classifi-
cation[3],speechr ecognition[ 47],naturallanguageprocessing[ 6],
video gaming [ 7], etc. Serviceoperations are supported bysimple
administrative tasks outsourced to deep learning software while
manufacturing further accelerates through automationvia intelli-
gentrobotics[ 5].Furthermore,anincreasingdemandforautoma-
tionandintelligentsupportisalsowitnessedinsomesafety-critical
areas, such as autonomous driving [45, 46] and healthcare [1].
As more and more DL software is applied to diverse application
domains, impacting our daily activities and lives, its quality and
reliabilityquicklyraiselotsofconcerns,especiallyinthecontextof
safety-critical and security-critical scenarios. We have already wit-
nessed the accidents and negative social impacts that were caused
by quality issues of DL software, e.g., Tesla/Uber accidents [ 49,50],
wrongdiagnosisinhealthcare,e.g.cancerordiabetes[ 1].Therefore,
systematic testing to uncover the incorrect behavior and under-
standthecapabilityoftheDLsoftwareispressingandimportant,
which not only provides confidence in its quality but also reduces
the risksafter deployment.
However, different from traditional software whose decision
logicismostlyprogrammedbythedeveloper,deeplearningadoptsadata-drivenprogrammingparadigm.Inparticular,themajortasks
of a DL developer are preparing the training data, labeling thedata, programming the architecture of the deep neural network
(DNN),andspecifyingthetrainingconfiguration.Allthedecision
logic is automatically learned during the runtime training phase
and encoded in the obtained DNN (e.g., by weights, bias, and their
combinations).Duetothedifferencesofprogrammingparadigm,
the logic encoding format, and the tasks that a DNN is often devel-
opedfor(e.g.,imagerecognition),testingtechniquesfortraditionalsoftware cannot be directly applied and new testing techniques are
needed for DNNs.
While somerecent progress hasbeen madein proposing novel
testing criteria [ 17,25,33,35] and test generation techniques for
quality assurance of DNNs [ 8,33,35,43,48,55,58], it still lacks
interpretation and understanding on the detected errors by such
techniquesandtheirimpact.Forexample,itisnotclearwhethererrors are indeed caused by missing training data or insufficient
training, etc. The fundamental assumption of deep learning is that
UI*&&&"$.*OUFSOBUJPOBM$POGFSFODFPO"VUPNBUFE4PGUXBSF&OHJOFFSJOH	"4&
ASE ‚Äô20, September 21‚Äì25, 2020, Virtual Event, Australia David Berend, Xiaofei Xie, Lei Ma, Lingjun Zhou, Yang Liu, Chi Xu, and Jianjun Zhao
the training data follows some distribution, based on which the
learning algorithms train the DNN to obtain its decision logic and
are able to handle future data that follow the similar distribution.
If the new unseen input data has a similar distribution as the
training data, deep learning provides some statistical guarantee on
itspredictioncorrectnessintermsofaccuracy.However,ifthenew
inputdatadoesnotfollowthetrainingdata(i.e.,out-of-distribution
(OOD)), deep learning does not provide statistical guarantee on its
prediction. For example, if a DNN is only trained on cat and dog
data for binary classification, given an input data offi sh, the DNN
can still produce a prediction result. However, this input data does
notfollowthedistributionofcatanddogdata.Hence,handlingthefish datagoesbeyond the capabilityof this DNN andshould not be
considered as valid input.
Intuitively,erroneousinputsthatfollowthedistributionoftrain-
ingdatamayreveal thereal weaknessofthe DNNsince theDNN
is expected to handle such data. On the other hand, input errors
thatareconsideredout-of-distributionmayeitherinheritnewin-
formation benefitting generalization as well as a domain shift or
aresimplyirrelevanttotheDLapplication.Thereby,therootcause
of an error may be identified through analyzing its distribution
behavior, which makes us rethink how to define errorsand how to
test the DNN by considering the effect on its trained distribution.
So far, the data in- and out-of-distribution analysis is still an
earlyandactiveresearcharea[ 11].ThechallengeofOODdetection
is that there is no perfect ground truth for validating whether one
sampleisin-distribution(ID)orout-of-distribution.Thecommon
approach of existing techniques to overcome this problem is utiliz-
ingsignificantlydifferentdatasetstoapproximatethegroundtruth.Forexample,CIFAR-10isusedastheIDdataandMNISTisusedas
OODdata.WhenmovingintothefieldofDLtesting,thedifferences
betweendatacanbecomemuchlessasonlyminorperturbations
are employed for generating new test cases [ 57], making the OOD
analysisofDLtestdataevenmoredifficult.Tothebestofourknowl-
edge,itiscurrentlystillunknownhowstate-of-the-artDNNtesting
techniquesareperformingunderconsiderationoftheirdistribution
behavior using existing OOD-detection techniques.
To bridge the gap from data distribution to DL testing activities,
we conduct a large scale empirical study of the impact of data
distributionawarenessonthestate-of-the-artDLtestingtechniques.
Inparticular,weinvestigatethefollowingresearchquestionsalong
four important perspectives:
‚Ä¢RQ1. Accuracy on the OOD Detection Techniques. Canex-
isting OOD detection techniques detect the OOD data that is
close and far to the training data? Which technique can achieve
the best performance in the context of DL testing?
‚Ä¢RQ2. Relationship between Mutation Operators and Data
Distribution. Mutation operators are used to generate new test
cases. Thus, which mutation operator is more likely to generate
OOD data andwhich one is more likely to generate ID data?
‚Ä¢RQ3.RelationshipbetweenTestingCriteriaandDataDis-
tribution. Testingcriteriaprovidethecoverageguidance,which
filter new test cases to cover diverse internal behaviors of DNN.
Thus,whatistherelationoftestingcriteriaanddatadistribution,
i.e., which testing criterion is more likely to keep OOD data and
whichone is more likely to keep ID data?‚Ä¢RQ4. Root Cause Estimation for ID and OOD Errors and
Robustness Enhancement. Finally, we estimate root causes
for ID and OOD errors and ask: which type of errors in terms
ofdistributionismoreeffectivewhenusedforretraininginen-
hancingrobustness?
Throughansweringthesequestions,weaimtoidentifytheim-
pacts of the data distribution in deep learning testing. In particular,
we use three popular datasets from computer vision domain as
subjectbenchmarksandnineOODdatasets,togetherwithatotal
of42DNNsforevaluation,amongwhichwetrained32DNNsto
identifytheoptimalOODdetectiontechniqueforDLtesting.Then,
to evaluate the effect of OOD for DL testing, we generate a total of
over1.2milliontestcasesandtrain10DNNforrobustnessenhance-
ment. Regarding DL testing, our study further focuses on two of
its key elements, i.e., 8 mutation operators for new test generation
and6coveragecriteria.Allthedatasetsandresultscanbefound
on our website [34].
To summarize, this paper makes the following contributions:
‚Ä¢We perform a large scale empirical study on how deep learning
testingaffectsthedatadistributionofthegeneratedtestcases;and how distribution aware testing influences DNN model ro-
bustness.
‚Ä¢Our study identifies the impact of mutation operators and cover-
age criteria on the distribution of the generated test cases. We
findthatimagerotation,contrastandbrightnesstendtogenerate
moreIDdatawhileimageblurismorelikelytogenerateOOD
data. In terms of the coverage criteria, NBC and SNAC facilitate
to generate more OOD data than others.
‚Ä¢Wedemonstratetheeffectivenessofdistributionawareretrain-
ing, outperforming the state-of-the-art by up to 21.5%. Based on
our results, we provide guidelines on distribution-aware error
selectionforrobustnessenhancement,by studyingthe effectof
root cause of ID and OOD errors.
Tothebestofourknowledge,thisisthefirstworkthatperformed
alarge-scalestudyontheimpactofdatadistributionbehavioron
DLtesting.Thisworkpointsoutanimportantdirectionandcalls
fortheattentionofdata-awarenesswhendesigningnewDLtesting
techniques.
2 BACKGROUND
2.1 Deep Learning Testing
To test the data-driven deep neural networks, a common way is to
generatenewdatainputssoastocapturetheDNNmodelbehavior
and identifyerrors (e.g., incorrect prediction).The simplistic form
of deep learning testing involves splitting the collected data into
a training and testing set. After training the DNN model withthe training set, its accuracy is tested with the testing set. One
drawback is that it relies solely on the initially collected spectrum
of information that usually does not cover all of the observable
cases foran intended application.
Currently, quite a few techniques [ 8,25,33,35,43,48,55,58]
areproposedtotestthenewdata-drivenDLsoftware.Coverage-
guidedtestingisarepresentativeandwidelyusedtechnique,whichusuallycontainsthreemaincomponents:the mutationoperator,the
coveragecriteria,andthe oracle.Themutationoperatorisusedto
Cats Are Not Fish: Deep Learning Testing Calls for Out-Of-Distribution Awareness ASE ‚Äô20, September 21‚Äì25, 2020, Virtual Event, Australia
"&'&%  $%%#%  % &$%$$!# '%%!
' $%
$$
 

 '#
#%#

	

&#("!'%%  !$(&  %
&$ "!%%"%&!
 &$%$$% 
  	
 


	
  
 	






	
	
			

 '#&$%
	
$$
## #
$$
 
 






	
%#


!
 #'

	

	
#&'!&"%	(**#
(!'% &'!	% )"%&##"%
'"!&#')! $%

#%#!%$%#&% '"!&#')!
&%% #$!%$%#&% 
Figure 1: Study Workflow and Research Questions
generate diverse test cases such that more behaviors can be tested.
For example, in image classification, mutation operators such as
image brightness, blur or contrast are applied under consideration
of realistic settings. The coverage criteria measure the degree of
howmuchtheDNNistested.Thenewlygeneratedtestcasesare
keptwhentheyhaveachievednewcoverageoftheDNN.Atlast,
the oracle is used to judge whether a new test case is a benign test
case,(i.e.,correctlypredicted),oranerrortestcase,(i.e.,incorrectly
predicted).
The assumption is that the test cases are generated by adding
minor perturbations on the original input, so they should have the
similar prediction result. However, the existing testing tools do not
consider the distribution of the training data, which determines
what data can or cannot be handled by the target DNN. The er-
rors may be caused by the defects of the DNN model itself (e.g.,
inappropriatemodelarchitecture,learningprocess)orthelackof
the training data. Hence, it is important to distinguish the different
types of errors (e.g., the ID and OOD errors), which provides more
feedback for the developers.
2.2 DataDistributionand AnalysisTechniques
GivenadatasetGiventwodatabsets AandB,whichfollowthedata
distributionof DAandDB,respectively,aDNNistrainedon A.If
AandBhavesimilardistributions,thewelltrainedDNNismore
likelytohandledatafrom Bcorrectly.Iftheyhaveatotallydifferent
distribution (e.g., cat andfi sh images), the DNN is not expected to
handle the data from B. Out-of-distribution techniques are mostly
evaluated by distinguishing two totally different datasets from one
another, where a large gap in corresponding distributions of scores
between DAandDBis considered OOD and a large overlap is
considered ID. More specifically, it calculates an OOD score for
thenewinput.Ifthescoreisbelowthedefinedthreshold,itisID.
Otherwise, it is OOD.
Inpractice,detectingtheout-of-distributiondataisachallenging
problem,especiallyforthehigh-dimensionaldata.SomeOODde-
tectionhasbeenrecentlyproposedtoaddressthehigh-dimensional
issues, such as [ 4,15,16,20,22,22‚Äì24,31,36,40,44,51]. These
techniques provide different ways to evaluate the distribution of
trainingdata. This work inherits those techniques and studies the
distribution of the test cases generated by different DL testing
strategies(e.g., mutation operators, coverage criteria).3 OVERVIEW OF OUR STUDY
Fig.1shows the overview of our study that focuses on the data
distribution and its effect on test cases generated by the cover-
age guided testing (CGT). Specifically, we focus on the three main
componentsoftheCGTforDL:1)theeffectof mutation ondatadis-
tributionofthetestcases,2)theeffectof coveragecriteria and3)the
effectiveness of the outputtestcaseson robustness enhancement.
To perform the study, we select three widely used datasets (i.e.,
MNIST,CIFAR-10andFashionMNIST[ 18,21,53])andfivestate-of-
the-art OOD detection techniques (i.e., Baseline [ 15], ODIN [ 24],
Mahalanobis[ 23],OutlierExposure[ 16]andLikelihood-Ratio[ 40]).
These OOD detection techniques are mainly proposed to distin-
guish two totally different datasets (e.g., CIFAR-10 and MNIST).
However, in this work, the generated test cases are often similar to
thetrainingdata.Therefore,wefirstdesignanexperimenttoinves-
tigate the effectiveness of existing OOD techniques in a novel and
morechallengingscenariowherethedifferencebetweendatasets
forcomparisonis low (i.e., RQ1).
Basedontheresultsof RQ1,weselectthebestOODmetricto
evaluatetherelationshipbetweenthedatadistributionandthemu-
tationoperators.Inthiswork,weselectthedatasetsintheimage
classificationdomain.Hence,weselect8popularimagetransfor-
mations, which are mainly used in the existing CGT tools (e.g.,
DeepTest[ 48],DeepHunter[ 55],andTensorFuzz[ 33]).Then,we
studywhichmutatorstendtogenerateIDdataandwhichonestendtogenerateOODdata(i.e.,
RQ2).Next,weevaluatetherelationship
betweenthedatadistributionandthecoveragecriteriaguidance
in CGT. We select 6 popular testing criteria [ 25,33,35] to study
whichcoverage criteria are more likely to guide the generation of
OOD or ID data (i.e., RQ3). Adversarial training is a common way
to enhance the robustness of DNNs by including the detected error
dataduringtraining.Therefore,wefinallystudythepossibleroot
cause for ID and OOD errors and study the effectiveness of the
OODand ID data for DNN robustness enhancement (i.e., RQ4).
3.1 Subject Datasets and DNN Models
We select three publicly available datasets (i.e., MNIST[21],CIFAR-
10[18] andFashion-MNIST [53]), that are widely used in previous
work. For each dataset, we follow the best DL practice and choose
diverse DNN models that are able to achieve competitive results in
ASE ‚Äô20, September 21‚Äì25, 2020, Virtual Event, Australia David Berend, Xiaofei Xie, Lei Ma, Lingjun Zhou, Yang Liu, Chi Xu, and Jianjun Zhao
Table 1: Subject Datasets and DNN Models.
Dataset Description DNN Train/Test Acc. (%)
CIFAR-10Generalimages VGG-11 97.16 / 87.92
(e.g., cats, dogs) DenseNet-121 99.97 / 94.46
ResNet-18 98.14/ 91.45
MNIST Digitimages LeNet-5 99.49/ 98.94
FashionMNIST FashionImages LeNet-5 92.53/ 90.25
terms of training and testing accuracy. Table 1shows the details
about the datasets and the DNN models.
3.2 OOD Detection Techniques
Weselect5state-of-the-artOOD-detectiontechniquesthatarecom-
monlyusedamongrelatedliterature[ 4,16,23,24,36,37,40].OOD
techniquesusedifferentapproachestoretrieveanOODscore.Some
use input perturbation, and others require a specifically trained
new DNN.Therefore, this workincludes techniqueswith various
approachesas follows:
‚Ä¢Simple Baseline [15] . The baseline identifies that in and out-
of-distributionsamplesareclassifiedwithdifferentprobability
distributions.Thesoftmaxpredictionprobabilityisusedtode-
terminewhetheran input is ID or OOD.
‚Ä¢ODIN[24] . In addition to calculate the softmax prediction prob-
ability proposed by the baseline, ODIN adds temperature scaling
totheinputaswellassmallinputperturbations.Theyshowthat
small perturbations have stronger effects on in-distribution sam-
ples rather than out-of-distribution samples, achieving higher
ID/OODclassificationperformance.
‚Ä¢Mahalanobis [23] . Mahalanobis detection technique integrates
the information from all layers into the score calculation. It
takes the closest class for each layer, adds small noise to the
test sample andfinally computes the score by measuring the Ma-
halanobisdistance[ 29]betweenthetestsampleandtheclosest
class-conditionalGaussiandistribution.
‚Ä¢OutlierExposure[ 16].OutlierExposurestandsoutbyclassify-
inginputswithaseparatelytrainedDNNwhichisexposedtothe
sametrainingdataastheDNNusedfortheapplication.However,
inaddition,out-of-distributiondataisintegratedintothetraining
procedure of the outlier exposure DNN model. Afterward, the
maximumsoftmaxprobabilityistakensimilartothebaselinefor
out-of-distributiondetection.
‚Ä¢Likelihood-Ratio [40] . The latestcontribution of thefi eld uti-
lizes a separately trained DNN, namely a generative DNN model
withPixelCNN++architecture[ 38].Theyuseanestimateofin-
putcomplexitytoderiveaparameter-freeOODscore,whichcan
be seen as a likelihood-ratio [40].
3.3 Evaluation Metrics of OOD Detection
Out-of-distributiondetectionforDLtestingimposesnewchallenges
to the OOD-detectionfi eld as the compared data inherits more
similarities,whiletheOOD-detectiontechniquesaredesignedon
datasetswithsignificantdifferencessuch ascomparing imagesof
birds (CIFAR-10) and street signs of houses (SVHN). Therefore, we
firstselectAUROCtocomparetheeffectivenessofdifferentOOD
detectiontechniques(for RQ1)ingeneral,andadditionally TPRNto
selectathresholdbasedonwhichtheOODdetectorcandistinguish
IDdataandOODdata(for RQ2,3,4).Aswewillseelater,havingTable 2: Mutation Operators and Coverage Criteria.
MutationPixel-Level Affine Trans . Tools
Contrast,Blur Translation, Scale DeepTest [48], DeepHunter
Brightness,Noise Shear, Rotation TensorFuzz
CriteriaNeuralCoverage (NC) DeepXplore[35], DeepTest
k-MultisectionNeuron Coverage (KMNC)DeepGauge[25]StrongNeuron Activation Coverage (SNAC)
Top-kNeuron Coverage (TKNC)DeepHunter[55]Neuron Boundary Coverage (NBC)
FastApproximate Nearest Neighbor (FANN) TensorFuzz[33]
multiple thresholds available is beneficial for analyzing differences
formore similar data.
‚Ä¢AUROC.Givenanunknowninput,OODdetectiontechniques
need to identify a threshold to classify it as ID or OOD. The area
under the receiver operating characteristic curve (AUROC) [ 14]
is usually used to evaluate the performance of a classification
method across multiple thresholds. The AUROC can be thought
ofastheprobabilitythatananomalousexampleisgivenahigher
OODscorethananin-distributionexample[ 16].Thus,thehigher
AUROC,thebetter the OOD detector.
‚Ä¢TPRN,whichisthetruepositiverateatN%truenegativerate
(TPRN). We regard OOD data as the positive class. First, we
use N% true negative rate to select one threshold for the OOD
detector. Then, with this threshold, we evaluate the true positive
rateof the detector.
Note that, for the parameter NinTPRN, a larger Nmeans we
select a bigger threshold such that more data is perceived un-
der the threshold as ID (i.e., higher true negative rate). Thus, a
larger Nprovides more confident measurement for detecting OOD
datawhileasmaller Nprovidesmoreconfidentmeasurementfor
detecting ID data.
3.4 Mutation Operators and Coverage Criteria
Fora thorough analysis of DL testing, we select 8 mutation opera-
tors and 6 coverage criteria, which represent the state-of-the-art
andarewidelyusedintheexistingtestingtools,i.e.,DeepTest[ 48],
DeepHunter[ 55]andTensorFuzz[ 33].Table2showsthedetailed
information about the selected mutation operators and coverage
criteria. Column Toolsrepresents which techniques are utilized by
whichtools.Allmutationoperatorparametersarecarefullychosen
byfollowingpreviousworkandmaintainingrealisticbounds,e.g.
rotation iscapped at40 degree.All configuration canbe foundon
ourwebsite [34].
3.5 Study Design
Theempiricalevaluation for each RQ is designed as follows:
RQ1. Accuracy on OOD Detection Techniques. For RQ1, we select
five aforementionedOOD detection techniques,which are widely
used to distinguish two totally different datasets. To compare their
effectiveness, we design three different experiments as follows:
(1)First,liketheusualway,weevaluatethetechniquesindistin-
guishing the ID data and OOD data in two different datasets,
i.e., the training data as ID data and another dataset as OOD
datawithsignificantlydifferentfeatures.Forthetargetdatasets
CIFAR-10, MNIST and FashionMNIST, we select 4 different
datasetsthatare regarded as OOD datasets.
(2)Second, we evaluate the inverse extreme case, by taking the
distribution difference between the training data and test data
Cats Are Not Fish: Deep Learning Testing Calls for Out-Of-Distribution Awareness ASE ‚Äô20, September 21‚Äì25, 2020, Virtual Event, Australia
ofthesamebenchmarkdataset(e.g.CIFAR-10trainvsCIFAR-10
test).Sincewecanexpectbothdatasetstofollowthesimilardis-tribution,wevalidatethattheOOD-techniquesareabletoiden-
tify in-distributions inputs, too, highlighting that the trained
distributionencompassesunknowndata,whichisrelevantto
the DL application.
(3)Third, we present an evaluation technique by splitting thebenchmark‚Äôs training dataset into 2 subsets based on theirclasses, i.e., half of the classes are taken as the training data
and the rest half of the classes are taken for OOD test set. Even
though the other half of classes are not trained, overall simi-larities exist as they are from the same domain. Thereby, we
present a similar scenario as encountered for deep learning
testing.
The three settings are designed to showcase a difference in data
distribution. In Setting 1, the two datasets are expected to have
totally different distributions. In Setting 2, the two distributionsshould be almost the same. Finally, Setting 3 should lie between
thefi rst two settings, as the classes are not known, however, the
compared datasets are from the same domain.
RQ2.RelationshipBetweenMutationOperatorsandDistribution.
In DL testing, mutation operators are used to generate diverse test
cases.Hence,RQ2aimstostudytheeffectofthemutationoperator
byobservingthedatadistributionofthegeneratedtestcases.We
use each of the aforementioned mutation operators to randomly
generate 2,000 test cases based on 200 seed inputs from the test
set. Based on the results of RQ1, we select the best OOD detection
technique to compare the distribution of generated test cases with
the original training data.
RQ3. Relationship Between Testing Criteria and Distribution. In
deep learning testing, coverage criteria provide the guidance for
testcasegeneration.Specifically,theyareusedtofiltersometest
casesfromrandomlygeneratedmutantsandkeeponlythemutants
that can improve the coverage. RQ3 aims to study which coverage
criteriaaremorelikelytogenerateIDorOODdata.Toanswerthis
question,wegenerateforeachDNNmodel2,000testcasesforeach
mutationoperatorforeachcoveragecriterionbasedontheseedsofRQ2. Then,we compare the distribution of the test cases generated
with different coverage criteria guidance.
RQ4. Root Cause for ID and OOD Errors and Robustness Enhance-
ment.Afteridentifyingthe optimal OOD detectiontechnique set-
ting for deep learning testing (from RQ1) and analyzing the effect
ofmutationoperatorsandcoveragecriteria(fromRQ2andRQ3),
weaimtostudytherootcauseforIDandOODerrorsandwhether
distribution-aware test cases are more effective in enhancing ro-
bustness by retraining.
For robustness enhancement we select 1,000 uniformly class-
distributed seeds extracted from the training set. Based on these
seeds,wegenerate5differentsetsofdata: ID-errorswhere TPR85=
0,ID-errorswhere TPR95=0,OOD-errorswhere TPR95=1,OOD-
errorswhere TPR99=1andrandomerrors.Eachsetcontains10,000
errortestcases.Then,weaddeachsetoferrortestcasesintothe
original training set consisting of 50,000 samples for retraining the
DNN.


  
#$ "
 "%$"&! #%"
	
 ###$ 



###$ 		
 "



#$	
 "



$"$## 

Figure 2: Visualization of the distribution difference be-tween different datasets on DenseNet-121
4 EMPIRICAL STUDY
We summarize the important results andfi ndings in this paper,
while complete results can be found on our website [34].
Table 3: Average Results (AUROC) of different OOD detec-
tion techniques (in %)
Dataset DNN Base. ODIN Maha. OE Like.
CIFAR-10DenseNet 98.8 98.9 94.1 99.568.0
ResNet 95.8 97.1 99.797.3 68.0
VGG-11 92.5 94.3 87.5 97.968.0
MNIST LeNet-5 98.7 98.7 98.6 99.793.4
Fashion-MNIST LeNet-5 91.2 91.9 99.999.4 63.0
4.1 RQ1: OOD Detection Accuracy.
Wefi rst evaluate the state-of-the-art of OOD detection techniques
to identify the optimal technique. The results (see Section 3.5)a r e
visualized by examples in Figure 2and are as follows:
4.1.1 Results on totally different datasets. First, we perform a com-
parative study to investigate the performance of existing OOD-
detection techniques. We prepare frequently used OOD datasets
[15,16,23,24,40] and the training data of the DNN as ID data. We
selecttheOODdatasets: SVHN[32],iSUN[54],Picsum[39]andOm-
niglot [19]. Detailed description of each OOD dataset can be found
in [34]. In addition, we scale the benchmark datasets Imagenet,
CIFAR-10,MNISTandFashionMNISTtothesamedimensionsas
thetraineddatasetandconvertthemintograyscalewhennecessary,
e.g., for MNIST or FashionMNIST. Due to the significant difference
in datasets, a ground truth of ID and OOD can be assumed.
For each DNN, we evaluate the OOD detection performance on
differentOODdatasets.Table 3showstheaverageAUROCscoreof
eachOODdetectiontechniqueoneachDNN.Thebestresultsare
inbold.Wecanobservethat,exceptfortheLikelihood-Ratio,other
OOD detectiontechniques are effective(91%+) in detectingsignif-
icantly different OOD data. Overall, Outlier Exposure shows the
highest overall performance while Mahalanobis is the second best.
For example, forVGG-11, OE achieves 97.9% AUROCscore that is
muchhigherthanothersandMahalanobishas87.5%AUROCscore.Likelihood-RatiohasthesameresultsforthethreeDNNsofCIFAR-
10asitsDNN-independent.OurresultsshowthatLikelihood-Ratio
performs the worst for CIFAR-10, MNIST and FashionMNIST (theaverage score of AUROC is 74.8%).
4.1.2 Results on the training data and test data. In the previous
section,weevaluatetheOODtechniquesfordetectingOODdata
ASE ‚Äô20, September 21‚Äì25, 2020, Virtual Event, Australia David Berend, Xiaofei Xie, Lei Ma, Lingjun Zhou, Yang Liu, Chi Xu, and Jianjun Zhao
Table 4: Results of OOD detection on the test set. (in %)
OODTech. Base. ODIN Maha. OE Like.
TPRN 99 99.9 99 99.9 99 99.9 99 99.9 99 99.9
DenseNet-121 5.92 3.02 0.74 0.02 0.80 0.03 4.94 0.10 1.52 0.22
ResNet-18 2.06 0.08 0.64 0.00 1.29 0.00 1.46 0.08 1.52 0.22
VGG-11 2.40 0.26 0.90 0.08 1.14 0.00 0.96 0.14 1.52 0.22
MN.LeNet-5 1.48 0.38 1.24 0.06 1.37 0.03 2.00 0.38 0.42 0.02
FMN.LeNet-5 1.44 0.24 0.74 0.00 0.54 0.00 1.22 0.20 1.12 0.12
Table5:ResultsofOODdetectionwhentraininghalfofthe
classes of the training set while testing OOD with the other
half of untrained classes. (in %)
OODTech. Base. ODIN Maha. OE Like.
TPRN 95 99 95 99 95 99 95 99 95 99
DenseNet-121 22.7 7.6 25.0 9.2 13.7 2.5 23.8 3.4 1.8 0.1
ResNet-18 33.7 14.9 34.4 15.6 15.2 3.9 27.7 8.2 1.8 0.1
VGG-11 30.8 11.6 31.7 11.8 19.6 1.2 10.0 1.8 1.8 0.1
MN.LeNet-5 71.7 45.8 73.7 51.4 92.3 78.7 86.0 64.9 4.9 0.8
FMN.LeNet-5 15.3 2.5 18.5 4.0 98.9 93.3 82.8 76.8 14.5 3.2
that is very different from the training data. In this section, weevaluate the techniques for distinguishing ID/OOD data in the
test set ofthe same benchmark datasetthat follows a very similar
distribution as the training set.
Table4summarizes the results on the test data for each dataset.
Weselecttwometrics TPR99andTPR99.9(inthesecondrow),which
meansthatweselecttwothresholdswithhighaccuracyindetecting
ID data (i.e., 99% and 99.9% high accuracy in detecting ID data).
Then, we observe how much of the test data is detected as OOD
data under these two thresholds. The results show that the overall
valuesareverysmallandfollowourexpectationsthatthereislittle
OODdatainthetestset.Italsodemonstratesthatalltechniquesare
effective in identifying ID data. Most importantly, it demonstrates
that the test set data follows almost an identical distribution as the
trainingdata,highlightingthatthetraineddistributionoftheDNNintegrates unknown data, which is considered relevant to the DNN
application (middle of Figure 2).
4.1.3 Results of splitting the training data. After evaluating the
ability in detecting OOD and ID data in their extreme cases (i.e.,Section4.1.1and4.1.2), we design afi nal study that is between
these two extreme cases. We split the training dataset equally in
two separate sets based on their labels. Then, we train the same
DNNarchitecturesasbeforebutwithonlyfiveoutputs,sincewe
only use half of the classes (e.g., 0-4). Similarly, we evaluate the
capabilityofthedetectiontechniquesbydistinguishingthetrained
classes from the non-trained classes of data.
Table5showshowmuchofthenon-trainedclassdataisdetected
as OOD data. We can clearly see that the values lie between the
values in Table 3and Table 4. Another observation is that more
OOD data can be identified in the grayscale images (i.e., MNIST
andFashionMNIST)whilelessOODdataisidentifiedinthecolor
images.Thereasonmaybethatthegrayscaleimageshavelower
dimensionality. Thereby, it is easier to capture the content changes
betweenthetwosubsets.However,forthecomplexcolorimages,
theymayshareasimilarstyleinthesamedomain(e.g.,theback-
ground), which makes it more difficult in distinguishing the twoclasses. We also found that TPR95 can identify more OOD dataTable 6: OOD Data by Different Mutation Operators. (in %)
MutatorsDenseNet-121 ResNet-18 LeNet-5(MNIST)
TPR85 TPR99 TPR85 TPR99 TPR85 TPR99BenignTranslation (-3, 3) 32 7 36 9 41 13
Scale(0.7,1.2) 75 36 74 43 46 10
Shear(-0.6, 0.6) 65 18 53 17 84 54
Rotation(-40, 40) 63 13 45 11 67 21
Contrast(0.5,0.13) 28 8 47 6 80 41
Brightness(-32, 32) 19 4 17 5 88 68
Blur(1,10) 87 77 91 83 96 87
Noise(1,4) 37 0 17 0 29 0
Average 50.6 20.5 47.3 21.7 66.2 36.8ErrorTranslation (-3, 3) 77 48 59 21 97 84
Scale(0.7,1.2) 99 90 96 85 96 66
Shear(-0.6, 0.6) 78 31 76 27 99 87
Rotation(-40, 40) 76 25 79 25 99 76
Contrast(0.5,0.13) 77 66 54 28 100 99
Brightness(-32, 32) 94 1 64 43 100 99
Blur(1,10) 96 86 98 91 100 100
Noise(1,4) 97 77 83 64 100 18
Average 86.9 53.1 76.3 48.1 98.9 78.7
than TPR99 as TPR95 selects a smaller threshold of the trained
distribution.
Theoverallresultsgiveusdirectionsonhowtoselectthethresh-
old for different datasets. If the two datasets are very similar but
suspectedtobefromdifferentdistributions,wecanselectasmaller
NinTPRN,whichcandetectmoreOODdata.Ifthetwodatasets
are very different, we can select a larger Nthat can distinguish the
two datasets.
AnswertoRQ1: Overall,ourresultsshowthatOutlierExpo-
sure on Densenet-121 architecture performs the best and the
results are consistent on all benchmark datasets. The existing
techniquescandetecttheIDdata effectivelywheremostofthetestdataarecorrectlyclassifiedas in-distribution.Split-
ting the classes of the training set imposes a challenge to the
detection techniques and grants a new perspective on their
performance for application-realistic settings.
4.2 RQ2. Relationship between Mutation
Operators and Data Distribution.
In the following experiments, we select Outlier Exposure as the op-
timal OOD-detection technique for DL testing based on the results
ofRQ1.Inaddition,duetothespacelimit,formutationoperator
evaluationweonlyshowtheresultsofDenseNet-121,ResNet-18
andLeNet-5forMNIST.VGG-11haslowercomplexityandFashion-
MNISTisverysimilartoMNIST.Toevaluatetheeffectofthemuta-
tionoperators,werandomlyselect200datasampleswithuniformly
distributedclassesfromthebenchmark‚Äôstestsetastheseedimages.
Then, we apply each mutation operator to the seeds, generating
2,000 benign test cases and 2,000 error test cases. Table 6shows
theresultsoftheOODdatageneratedbyeachmutationoperator.
ColumnMutators showseachmutationoperator.Theparameters
arechosen very conservativelyandfollowprevious contributions
while changing the original image slightly [ 55]. The exact settings
for realistic mutation is at [34].
The generated test cases and the original dataset are similar.
Therefore, webuild on ourfi ndingsfrom RQ1 andintroduce both
TPR85 and TPR99 settings to detect the OOD test cases. We can
be more certain that samples tend to be OOD with the threshold
Cats Are Not Fish: Deep Learning Testing Calls for Out-Of-Distribution Awareness ASE ‚Äô20, September 21‚Äì25, 2020, Virtual Event, Australia
of 99%. Nevertheless, if TPR85 shows a low score, the likelihood is
more samples tend to be in-distribution.
Considering the results of the benign test cases and error test
cases, wefi nd that the errors test cases are considered out-of-
distributionatahigherratethanthebenigntestcases.Errortest
cases for DNNs trained on CIFAR-10, namely DenseNet-121 and
ResNet-18,haveanaverageTPR99-scoreof50.6%,whilethebenign
test cases only show 21.1%. For example, focusing on the muta-
tionoperatorImageNoise,wecanobservethatbenigntestcases
seem to be entirely in-distribution ( TPR99=0) while the error test
cases show a TPR99-score of 77%, 64% and 18% respectively for all
three DNNs. This behavior indicates that error test cases are more
likely to be out-of-distribution and thus they are more likely to be
predicted incorrectly.
Considering the results between different DNNs, wefi nd that
DenseNet-121andResNet-18(trainedonCIFAR-10)sharesimilar
averages between all three evaluation metrics. However, they have
different trends when compared with LeNet-5 which is trained
on MNIST. This behavior shows that the results are data-driven,
highlydependingonthetraineddatasetsingeneral.Forasimple
grayscale image (MNIST) which has low dimensionality, the muta-
tionoperatorsmaychangeitalotandgenerateOODdata.However,
formorecomplexcolorimages,themutationoperators(withthe
defined conservative parameter setting) will change little on the
high dimensional data, which makes lower OOD data in CIFAR-10.
For example, in MNIST, the average results are 36.8% and 78.7%for benign test cases and error test cases, respectively. While for
ResNet-18on CIFAR-10, the results are 21.7% and 48.1%.
Comparing different mutation operators individually, wefind
thatimageblurtendstogenerate themost OODdata. Thebenign
anderrortestcasesofBlurareconsidered77%and86%asOODdata
(basedonTPR99). ImageScalefollowsa similarpatternespecially
forerrortestcasesonCIFAR-10(85%and90%forDensenet-121and
ResNet-18 respectively). For Image Scale, after the image is becom-ing smaller, the black color is used to complement the background,
andthereforeismorelikelytobeOOD.However,inerrortestcases
ofMNIST,thebackgroundoftheoriginalimagesisblackalready.
Hence, the Image Scale only generates 66% OOD data, which is the
smallestvalueinallmutationoperators.Inaddition,wefindthat
mutationoperators,i.e.,Translation,Shear,RotationandBrightness
tend to generate fewer OOD data. For example, Brightness only
generates 1% OODdata for DenseNet-121 and Rotationgenerates
25%OODdatafor both, DenseNet-121 and ResNet-18.
AnswertoRQ2 :Thedatadistributiongeneratedbymutation
operators is dependent on the datasets. Considering the same
mutation operators, more test cases tend to be more OODfor grayscale images and less for color images. Image blur
and Image Scale are the mutations strategies where the high-
est OOD-score is observed, whereas Image Rotation, Shear,
Brightness and Contrast generate fewer OOD data. The error
testcasesare more likely to be OOD than benign test cases.Table7:OODdata(underTPR99)generatedbydifferentcov-
erageguidance. (in %)
Rand NC KMNC NBC SNAC TKNC FANNBenign
DenseNet-121Rotation 13 8 14 33 22 7 8
Contrast 8 24 9 52 59 17 21
Brightness 4 14 5 40 42 13 14
Blur 7736 58 77 51 40 53
All 14 38 11 34 42 14 13
Average 23 24 20 47 43 18 22ResNet-18Rotation 11 6 20 13 10 10 12
Contrast 6 8 9 43 44 14 9
Brightness 5 4 3 11 15 11 5
Blur 83 24 69 81 71 34 47
All 8 33 11 59 52 18 11
Average 23 15 22 41 38 17 17LeNet-5Rotation 21 1 11 17 7 12 15
Contrast 41 1 3 16 4 15 25
Brightness 68 2 39 29 1 28 32
Blur 87 34 34 1 3 25 5
All 31 3 25 69 37 25 22
Average 49 22 42 71 02 33 0Error
DenseNet-121Rotation 13 26 29 32 46 36 34
Contrast 8 95 40 75 85 72 62
Brightness 4 13 31 48 85 40 38
Blur 77 93 87 86 90 89 87
All 14 66 55 67 72 58 57
Average 23 59 48 61 76 59 56ResNet-18Rotation 25 35 26 26 34 37 29
Contrast 28 65 50 95 92 67 56
Brightness 4320 9 13 10 33 36
Blur 91 86 89 93 89 87 85
All 56 60 52 66 68 59 55
Average 48 53 45 59 59 57 52LeNet-5Rotation 76 56 59 68 59 73 77
Contrast 99 100 87100 100 98 98
Brightness 99 100 100 100 100 100 100
Blur 100 81 95 91 80 97 97
All 82 73 74 91 86 82 78
Average 9182 83 90 85 90 90
4.3 RQ3. Relationship between Testing Criteria
and Data Distribution.
In RQ2, we have identified the behavior of the mutation operators
withoutapplyinganyguidancetotheDLtestingflow.Now,weadd
coveragecriteriatothetestingflowtoguidethetestcasegeneration.
Therefore,weevaluatetheeffectofcoveragecriteriaonthedata
distribution and compare it with the results of RQ2 as the baseline.
Based on the results of RQ2, we select 4 mutation operators,
i.e., Rotation, Contrast and Brightness which are more likely togenerateIDdataandBlurwhichismorelikelytogenerateOODdata. Table 7shows the results of how many test cases are OOD
data with different coverage guidance. The results are evaluated byOutlierExposurewithTPR99.Thethirdcolumnshowsthemutation
operators. Row Allmeans we use all default mutation operators
fortestcasegeneration.Column Randshowstheresultswithout
coverage criteria guidance which is from Table 6. The columns
following Randshow the results for each coverage criterion. Note
that, in coverage-guided testing, benign test cases are generated
withtheguidanceofthecoveragecriteriawhileerrortestcasesare
notfi ltered by the coverage criteria.
Considering the results of benign test cases (i.e., under coverage
guidance),wefindthat,comparedwiththerandomgeneration(i.e.,
without coverage guidance), KMNC TKNC and FANN decrease
the OOD data ratio, whereas NBC and SNAC increase the OOD
dataratio.Forexample,inDenseNet-121,theaveragevaluewith
random generation is 23% while the average values with KMNC,
TKNC and FANN guidance are 20%, 18% and 22%, respectively. The
average values with NBC and SNAC are 47% and 43%, respectively.
ASE ‚Äô20, September 21‚Äì25, 2020, Virtual Event, Australia David Berend, Xiaofei Xie, Lei Ma, Lingjun Zhou, Yang Liu, Chi Xu, and Jianjun Zhao
        
	
	
	


Figure 3: Train/Test and Error Test Case Distributions col-
ored in different ranges defined by quantiles of train/testdistribution
Itis consistentwiththeirdefinitions.Forexample,KMNC mainly
considers major behaviors of the DNN and FANN generates test
cases thatare near the originalseeds. Thus, IDdata is more likely
tobegeneratedwiththeguidanceofthesetwocoveragecriteria.However, NBC and SNAC mainly consider the boundary of the
DNN.Hence, they can generate more OOD data.
Consideringthespecificmutationoperatorsindividually,wefind
thatmutation andcoverage criteria influence eachother. For exam-
ple, for DenseNet-121 and ResNet-18 when using Image Contrast,
mostofthecoveragecriteriaincreasetheOODdataratioincluding
KMNC, TKNC and FANN, where usually these coverage criteria
tendtodecreasetheOODdataratioinalldata.Itindicatesthatthe
coveragecriteriaguidethetestcasegenerationbycoveringmore
diverse DNN behaviors for this particular mutation operator. In
the contrary, Image Blur changes the image a lot causing most test
cases to be OOD under the random mutation setting already. In
this case, all coverage criteria decrease the OOD data ratio com-
pared to the random mutation setting. This behavior means that
mostOODtestcasesgeneratedbyImageBlurmaybefilteredby
thecoveragecriteria.Forexample,thecoveragecriteriainDeep-
Gauge [25] are defined based on the profiling of training data. The
blurred images are far from the training data, thus they cannot
achievenewcoverageunderthesecriteria.Theresultsdemonstrate
that existing coverage criteria have obvious effects on mutation
operatorsbutvaryintheirbehaviordependingontheunderlyingmutationoperator.
Another interesting observation is found for MNSIT trained on
LeNet-5whichisdifferentfromtheresultsforCIFAR-10.Forthe
benigntestcasesofLeNet-5,almostallcoveragecriteriadecrease
theOODdataratio,whereforCIFAR-10thisbehaviorcouldonlybe
observedforselectedmutationoperators.Itseemsthatforsimple
blackandwhiteorgrayscaleimages,therandommutationrequires
much change to produce a test case, which is why a lot of OOD
dataaregenerated.However,thetestingcriteriafiltermostofthese
datapoints as they do not contribute to increasing the coverage.
Fortheerrortestcases,wefindthat,comparedwithrandommu-
tation, almost all the coverage criteria increase the OOD data ratio
in ResNet-18 and DenseNet-121. In LeNet-5, most of the erroneous
testcasestendtobeOODdata(TPR99).Actually,errortestcases
havenodirectrelationshipwiththecoveragecriteriaastheyare
notfi ltered by the criteria directly . However, the y are generated by
mutating the benign data that are generated under the coverage
guidance.Answer to RQ3 : Our results show that, existing coverage
criteria affect the data distribution of generated test cases,
which is important to address when designing a test scenario.
KMNC, TKNC, NC and FANN tend to decrease the number ofOODbenigntestcaseswhileNCandNBCtendtoincreasethe
OODbenigntest cases.Forthemutation operatorsthattend
togeneratefewerOODdatasuchasrotationandcontrast,the
existing coverage criteria can increase the number of OOD
databycoveringmorebehaviorsoftheDNN.Forthemuta-
tionthattendstogeneratemoreOODdatasuchasblur,the
existingcoveragecriteriacandecreasethenumberbyfiltering
some data with the coverage guidance. For grayscale images,
thecoveragecriteriamaydecreasethenumberofOODdata
withrandommutationoperators.Thecoveragecriteriamay
increasetheOODdataforgenerated error test cases.
4.4 RQ4. Root Cause of ID and OOD Errors and
RobustnessEnhancement.
WehypothesizethatIDerrortestcasestendtobearesultofdefects
intheDNNmodelwhileOODerrortestcasestendtobearesult
of missing data in the training set. Therefore, two experiments are
designed to study the hypothesis. First, we use other DNN models
withdifferentarchitecturesbuttrainedonthesametrainingdatato
predict the ID and OOD errors of the model under test. Following
our hypothesis, we expect other DNN models to predict ID errors
more correctly than OOD errors. Second, we retrain the model
under test with additional ID and OOD error test cases. We expect
thatthenewlyaddeddatahelpsincorrectlypredictingOODerrors
more effectively.
4.4.1 RobustnessEnhancementwithAdjustingModels. Forthefirst
hypothesis, i.e., ID error test cases tend to be a result of defects in
the DNN model, we select six other DNN variants (VGG-11, VGG-
13, ResNet-18, ResNet-34, DenseNet-121, DenseNet-169), which
differintheirarchitecturebutarelearnedfromthesametraining
dataset.Notethatthesemodelscanberegardedasthesimulation
ofthepotentialimprovementoftheoriginalmodel(e.g.,finetuneor
change in architecture). We expect errors found on, e.g. ResNet-18,
tobepredictedcorrectlybysomeoftheotherfiveDNNvariants
with special attention on whether ID errors tend to be predicted
correctly more likely than OOD errors.
Table8shows the results of the cross validation on otherfi ve
models. Full evaluation can be found on the website [ 34]. For each
model,wecollect10,000IDerrorsand10,000OODerrors,respec-
tively.Theaccuracyshowshowmuchdataiscorrectlypredicted
on average by the other models (i.e., it could be handled well by
improvingthemodel).Overall,theresultsindicatethatIDerrors
tend to befi xed at a higher likelihood than OOD errors through
DNNadjustmentssuchaschangingitsneuralarchitecture,which
is consistent with our hypothesis. For example, by changing the
models,32.4%IDerrorscouldbefixedwhileonly20.4%OODerrors
couldbefi xed.
4.4.2 RobustnessEnhancementbyAddingTrainingData. Forthe
second hypothesis, i.e., OOD error test cases tend to be a result
Cats Are Not Fish: Deep Learning Testing Calls for Out-Of-Distribution Awareness ASE ‚Äô20, September 21‚Äì25, 2020, Virtual Event, Australia
Table 8: DNN Model Agreement on ID and OOD Errors.
Test Model Error Type Cross Validation
TPR85 TPR99 Accuracy (%)
ResNet-18ID-Error 29.7
OOD-Error 21.2
DenseNet-121ID-Error 32.4
OOD-Error 20.4
Table 9: Results for robustness enhancement on different
datasetandDNNs.(in%)
Test Set CIFAR-10 RANDOM ID85 ID95 OOD95 OOD99ResNet-18CIFAR-10 Test 91.5 90.1 90.5 90.2 90.1 89.8
ID Error 0.0 50.9 56.9 50.5 60.4 13.3
OOD Error 0.0 64.1 54.3 54.3 62.6 10.9
RAND Error 0.0 58.1 53.5 50.0 61.9 12.0DenseNetCIFAR-10 Test 94.5 89.2 89.6 89.9 89.7 89.6
ID Error 0.0 60.7 47.3 47.0 60.5 49.1
OOD Error 0.0 46.6 55.1 49.9 59.0 58.3
RAND Error 0.0 48.7 53.5 47.7 59.2 55.0
Total Average 0 54.9 53.4 49.9 60.6 33.1
of missing training data, we generate multiple ID/OOD data and
evaluate the robustness by retraining with them.
Specifically, we proposefi ve datasets, each of which contains
original training data and 10,000 error test cases, which are gen-
eratedfrom1,000initialseedinputsbutvaryintheirOODscore
(presentedbycolorandthresholdinFigure 3:Orangeandyellow
areasindicateIDerrortestcaseswithTPR85andTPR95tobe0%
respectively (related to as ID85 and ID95). Green and red areasindicateOODerrortestcaseswithTPR95andTPR99tobe100%,respectively (related to as OOD95 and OOD99). We further use
errorsdrawnrandomlyfrom the distribution as another dataset.
In addition, we prepare four test sets: the original test set, 2,000
ID errors, 2,000 OOD errors and random errors, which are usedto test the new DNNs. Here, we only include two DNNs trained
onCIFAR-10duetomostoftheerrorsforLeNet-5onMNISTare
considered OOD.
Table9shows the results of the retrained DNNs. Overall, for
thenewDNNs,theaccuracyontestsetisreducedonaverageby
1.5%. At the same time, the performance on correctly classifying
randomerrorsisimprovedupto61.9%percentagepoints.However,
theresultsvaryquitea lotwith thedatadistribution. OODerrortest cases (green area, column OOD95) show the highest overall
accuracywith60.6%averageaccuracy,whileID85andID95only
classify53.4%and49.9%correctly.ThispromotestheideathatOOD
errors are more effective in generalizing the model towards new
data.However,notallOODerrorscanbeconsideredeffectivefor
retraining. Column OOD99 shows the lowest total average, which
indicates, that at some point error test cases can not be considered
directly benefiting the overall DNN application as they are too
different from the overall distribution.
Compared with random retraining which can be considered a
baseline of recent work, distribution aware retraining increases
robustness on average by 10.2% and up to 21.5% for Random Error
Test Set on DenseNet-121.AnswertoRQ4 :TheresultsdemonstratethatID-errorstend
tobefixedviaDNNadjustments,whileOOD-errorsseemtore-quirefurthertrainingdataforbeingcorrectlyclassified.When
retraining,OODerrorstendtobeonaverage10.4%moreef-
fectiveinimprovingtherobustnessoftheDNNthanIDerrors
or randomly chosen ones. Furthermore, not all OOD errors
helpthemodeltogeneralize,indicatingthattheOOD-score
distance towards the trained/tested DNN distribution matters
whenchoosingtheright dataforenhancingrobustness.
4.5 Discussion and Research Guidance
Basedonourresults,wepinpointthefollowingresearchdirections:
‚Ä¢OODDetectionforDL Testing (RQ1). In DL testing, it is still
challenging to distinguish ID and OOD data especially when
more similarities between the two tested data types exist. There-
fore,fi ne-grained thresholds seem helpful in gaining a better
understanding in similar cases. Our results in Fig. 2provide the
following guidance: if the testing tool aims at generating ID test
cases, a smaller Nshould be selected. If we want to generate
OODtestcases,a larger Nshouldbe selected.
Research Guidance: a possible direction is to develop OOD
techniques, which can effectively detectfi ne-grained OOD datafor deep learning testing.
‚Ä¢Mutation Operators and Coverage Criteria (RQ2&3).
Our
results show that the existing mutation and coverage criteriahave different effects on ID data or OOD data generation. To
buildthedistribution-awareDLtestingtools,wecoulddevelop
distribution-based coverage criteria that canfi lter some OOD
dataor ID data.ResearchGuidance:
DL testing tools should be aware of distri-
bution. A promising direction is to develop morefi ne-grained
distribution-aware criteria for the test selection.
‚Ä¢RobustnessEnhancement(RQ4.) Ourinitialresultshaveshown
thatdistribution-awareretrainingismoreeffectiveinrobustness
enhancement than the distribution-unaware retraining. It seems
thatrootcausesforIDerrorsarepartiallymodeldependentwhile
OODerrorscan be effectivelyfi xed with new training data.
Research Guidance: A future research direction is to further
analyze the root cause of ID and OOD errors, especially in an
evenmorefinegrainedsettingwhichcanprovideguidanceforre-
pairing the model from a data and DNN architecture perspective
underregard of the presented threshold of this work.
4.6 Threatto Validity
TheselectionofthedatasetsandDNNscouldbeathreattovalidity.
Wetrytocounterthisbyusingeightpubliclyavailableandpopu-
larlyuseddatasetsandcross-validatingresultsonthreedifferent
DNNarchitectures.OODdetectionisaverychallengingproblemas
thereisnoperfectgroundtruth,whichcouldbeathreattovalidity.
To this end, we select multiple state-of-the-art OOD techniques for
a comparative study. In addition, we also design threefi ne-grained
experiments (in RQ1) where the ground truth can be approximated
bytheinheriteddifference.Thereby,wecanidentifytheoptimal
OODdetectiontechniqueforDLtestingtocomparethedistribution
performance for DL testing associated data. The results of Tables 4,
ASE ‚Äô20, September 21‚Äì25, 2020, Virtual Event, Australia David Berend, Xiaofei Xie, Lei Ma, Lingjun Zhou, Yang Liu, Chi Xu, and Jianjun Zhao
5,6,7,8and9maybebiasedontheseedsandthegenerateddata.
We try to counter this by randomly selecting the same number
of seeds for each class, generate a large number of mutants, and
compare the averaged results.
5 RELATED WORKS
Deep Learning Testing. Adversarial attacks have been exten-
sively studied to perform perturbation on input data to fool a DNN
indifferentapplications[ 2,10,13,30,41,52].However,suchper-
turbationsareoftenobtainedthroughgradient-oroptimized-based
searching, which may rarely happen in a physical environment.
In addition, it has been demonstrated that there are many issuesduring the DL development and depolyment phases [
12], which
callsfortherequirementofsystematicDLtesting.Differentfrom
theadversarialattack,DLtestingconsidersgeneratingnewtests
by performing mutations that simulate noise patterns from the
physicalenvironment(e.g.,imagebrightnesschange,rotation)with
definedboundstomaintainrealism,e.g.,rotationislimitedto40
degrees. To estimatethe DLtesting sufficiencyand providingtest-
ing guidance, many testing criteria have been proposed. DeepX-
plore [35] originally proposed the neuron coverage. Inspired by
this,DeepGauge[ 25]proposedasetofmorefine-grainedtesting
criteria such as KMNC,NBC, etc. DeepConcolic proposed MC/DC
testcriteria[ 42].Furthermore,combinatorialtestingcriteria[ 26]
and Surprise Adequacy [ 17] are also proposed. The testing criteria
above mainly focus on feed-forward neural networks, while Deep-
Stellar[8]proposedthemodel-basedtestingcriteriaforrecurrent
neuralnetworks.
TheseproposedtestingcriteriaforDNNareusedtoguidethe
test generation process, such as in [ 27,28,33,35,48,55,56,59]. In
addition,DeepTest[ 48]andDeepRoad[ 58]alsogenerateimages
withGenerativeAdversarialNetworks(GANs).Comparedwiththe
basic transformations (e.g., adding noise, rotation), the GAN-based
techniques can perform advanced scene transformation, but are
computation-intensive requiring training a GAN, the quality of the
generated images can not be easily guaranteed.
Similarly, we also leverage the basic mutation operators and
coveragecriteriainexistingtestingtoolsforthestudy.However,
this paper is orthogonal to existing DL testing work in that our
focusistoinvestigatetheimportanceofdatadistributionandhowit
impactsexistingtestingtechniques.Ourresultsshowthat,although
existing testing techniques are able to detect thousands of errors as
discussed in their original papers, a large portion of these errors
maynotcontributedirectlytothedesiredresultwhenretraining
or to the overall DL application. Therefore, considering the data
distributionduringDLtestingisofgreatimportancetoproperly
identifythereal weakness of a DNN for further processing.Out-of-Distribution-Detection Techniques.
While being im-
portant,theout-of-distributionanalysisischallengingespecially
for high-dimensional data. Dan Hendrycks et. al introduced a base-
line approach [ 15], which utilizes the maximum Softmax prob-
ability. Correctly classified examples tend to have greater maxi-mum Softmax probabilities than erroneously classified and out-
of-distributionexamples,allowingfortheirdetection.TheODIN
[24]andMahalanobistechnique[ 23]proposetoapplyinputper-
turbationsbyaddingnoiseortemperaturetotheinput,bywhich
theyintensifytheability ofthebaselinealgorithmtodifferentiateconfidencebetweeninandout-of-distributionerrors.OutlierExpo-sure[
16]takesadifferentapproach.Here,aseparateDNNistaken,
andtrainedwithanadditionalinfusionofdeclaredOOD-samples,
such aslarge scale data imagesTinyImages [ 9], whilethe score is
calculated in a similar fashion to the baseline. One advantage of
thetechniqueisindependencetowardstheDNNusedfortheappli-
cation. Just towards the data. Thereby, a bias for OOD-detection
caused by a given DNN may be overcome more efficiently.
Finally, more recent contributions propose to use likelihood-
ratiosattheircore[ 36,40]andutilizegenerativePixelCNN++ar-
chitecturetoretrievebitsperdimensiontocalculateOODscores.
Othertechniques,whicharenotcapableofclassifyingsingleinputs
[31], require heavy DNN architectural adjustments, such as adding
an additional class [ 51] or taking multiple techniques as ensemble
[4,20]. This is not considered in this work due to their imposed
limitationstowards DL testing.
ExistingOODdetectionmethodsaremostlyproposedtoworkon
datasetswithalargedifference.Therefore,itisstillunclearwhetherandtowhatextentexistingOODdetectionmethodscanbeusedfor
the challenging DL testing scenario, where the generated test data
oftendiffersfromitsoriginalcounterpartinaminorway.Inthis
work,weselectedthestate-of-the-artOOD-detectiontechniquesto
investigatetheireffectiveness,itsconnectionandusefulnessforDL
testingpurposes.Wefindthatdata-distributionawarenesscouldbe a key for more effective and interpretable DL testing towards
providing better quality assurance.
6 CONCLUSION
Inthispaper,weconductalarge-scaleempiricalstudyonthestate-
of-the-art OOD techniquestowards understanding the datadistri-
butionanditsimpactonDLtestingactivities.Ourresultsshowthat
the existing OOD detection techniques can distinguish the OODdata from the newly generated test cases, even for challenging
caseswherethetestdataisverysimilartothetrainingdata.Our
study further shows that existing image mutation operators and
testingcriteriacangreatlyaffectthedistributionofthegenerated
test cases. Finally, we demonstrate that distribution-aware dataset
tendstobemoreeffectiveinrobustnessenhancement.Thisstudy
makesthefirststepalongthisdirectiontowardsunderstandingthedata-driven nature of DL software for testing activities. The results
ofthispapercallfortheattentionofdata-distributionawareness
during designing testing and analysis techniques for DL software,
which builds the foundation towards developing more effective DL
testingtechniques.
ACKNOWLEDGMENTS
This work was supported by Singapore Ministry of Education Aca-
demicResearchFundTier1(AwardNo.2018-T1-002-069),theNa-
tionalResearch Foundation,PrimeMinisters Office,Singaporeun-
der its National Cybersecurity R&D Program (Award No. NRF2018
NCR-NCR005-0001), the Singapore National Research Foundation
underNCRAwardNumberNSOE003-0001andNRFInvestigator-
shipNRFI06-2020-0022.ItwasalsosupportedbyJSPSKAKENHI
Grant No.20H04168, 19K24348, 19H04086, and JST-Mirai Program
Grant No.JPMJMI18BB, Japan. We also gratefully acknowledge the
support of NVIDIA AI Tech Center (NVAITC) to our research.
Cats Are Not Fish: Deep Learning Testing Calls for Out-Of-Distribution Awareness ASE ‚Äô20, September 21‚Äì25, 2020, Virtual Event, Australia
REFERENCES
[1]BBC.2020. AI‚Äôoutperforms‚Äôdoctorsdiagnosingbreastcancer. https://www.bbc.
com/news/health-50857759
[2]NicholasCarliniandDavidA.Wagner.2016. TowardsEvaluatingtheRobustness
of Neural Networks. CoRRabs/1608.04644 (2016). arXiv:1608.04644 http://arxiv.
org/abs/1608.04644
[3]carnegieendowment. 2019. The Global Expansion of AI Surveillance.
https://carnegieendowment.org/2019/09/17/global-expansion-of-ai-
surveillance-pub-79847
[4]Hyunsun ChoiandEric Jang.2019. Generative EnsemblesforRobust Anomaly
Detection. https://openreview.net/forum?id=B1e8CsRctX
[5]McKinsey Co. 2019. AI Adoption Advances but Foundational Barriers Re-
main.https://www.mckinsey.com/featured-insights/artificial-intelligence/ai-
adoption-advances-but-foundational-barriers-remain
[6]Datamonsters. 2017. 10 Applications of Artificial Neural Networks in Natural
Language Processing. https://medium.com/@datamonsters/artificial-neural-
networks-in-natural-language-processing-bcf62aa9151a
[7]Google Deepmind. 2019. AlphaStar: Grandmaster level in StarCraft II using multi-
agent reinforcement learning. https://deepmind.com/blog/article/AlphaStar-
Grandmaster-level-in-StarCraft-II-using-multi-agent-reinforcement-learning
[8]Xiaoning Du, Xiaofei Xie, Yi Li, Lei Ma, Yang Liu, and Jianjun Zhao. 2019. Deep-
Stellar:Model-BasedQuantitativeAnalysisofStatefulDeepLearningSystems.In
Proceedingsofthe201927thACMJointMeetingonEuropeanSoftwareEngineering
Conferenceand SymposiumontheFoundationsof SoftwareEngineering (Tallinn,
Estonia)(ESEC/FSE 2019) . Association for Computing Machinery, New York, NY,
USA,477‚Äì487. https://doi.org/10.1145/3338906.3338954
[9]RobFergus,YairWeiss,andAntonioTorralba.2009. Semi-SupervisedLearninginGiganticImageCollections. In AdvancesinNeuralInformationProcessingSystems
22, Y. Bengio, D. Schuurmans, J. D. Lafferty, C. K. I. Williams, and A. Culotta
(Eds.). Curran Associates, Inc., 522‚Äì530. http://papers.nips.cc/paper/3633-semi-
supervised-learning-in-gigantic-image-collections.pdf
[10]IanGoodfellow,JonathonShlens,andChristianSzegedy.2015. Explainingand
Harnessing Adversarial Examples. In International Conference on Learning Repre-
sentations. http://arxiv.org/abs/1412.6572
[11]Google. 2019. Improving Out-of-Distribution Detection in Machine Learning Mod-
els.https://ai.googleblog.com/2019/12/improving-out-of-distribution-detection.
html
[12]QianyuGuo,SenChen,XiaofeiXie,LeiMa,QiangHu,HongtaoLiu,YangLiu,
JianjunZhao,andXiaohongLi.2019. AnEmpiricalStudytowardsCharacterizing
Deep Learning Development and Deployment across Different Frameworks
andPlatforms.In Proceedingsofthe34thIEEE/ACMInternationalConferenceon
Automated Software Engineering (ASE ‚Äô19). 810‚Äì822.
[13]Qing Guo, Xiaofei Xie, Felix Juefei-Xu, Lei Ma, Zhongguo Li, Wanli Xue, Wei
Feng, and Yang Liu. 2019. SPARK: Spatial-aware Online Incremental Attack
Against Visual Tracking. arXiv:cs.CV/1910.08681
[14]J. A. Hanley and B. J. McNeil. 1982. The meaning and use of the area under
a receiver operating characteristic (ROC) curve. Radiology 143, 1 (April 1982),
29‚Äì36.http://www.ncbi.nlm.nih.gov/pubmed/7063747
[15]Dan Hendrycks and Kevin Gimpel. 2016. A Baseline for Detecting Misclassified
and Out-of-Distribution Examples in Neural Networks. CoRRabs/1610.02136
(2016). arXiv:1610.02136 http://arxiv.org/abs/1610.02136
[16]Dan Hendrycks, Mantas Mazeika, and Thomas Dietterich. 2019. Deep Anom-aly Detection with Outlier Exposure. In International Conference on Learning
Representations. https://openreview.net/forum?id=HyxCxhRcY7
[17]JinhanKim,RobertFeldt,andShinYoo.2019.GuidingDeepLearningSystemTest-
ing Using Surprise Adequacy. In Proceedings of the 41st International Conference
on Software Engineering (Montreal,Quebec, Canada) (ICSE‚Äô19). 1039‚Äì1049.
[18]Alex Krizhevsky. 2009. Learning multiple layers of features from tiny images.
Technical Report.
[19]Brenden M. Lake, Ruslan Salakhutdinov, and Joshua B. Tenenbaum. 2015.
Human-level concept learning through probabilistic program induction. Sci-
ence350, 6266 (2015), 1332‚Äì1338. https://doi.org/10.1126/science.aab3050
arXiv:https://science.sciencemag.org/content/350/6266/1332.full.pdf
[20]Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell. 2017. Sim-ple and Scalable Predictive Uncertainty Estimation using Deep Ensembles. In
AdvancesinNeuralInformationProcessingSystems30,I.Guyon,U.V.Luxburg,
S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (Eds.). Cur-
ranAssociates,Inc.,6402‚Äì6413. http://papers.nips.cc/paper/7219-simple-and-
scalable-predictive-uncertainty-estimation-using-deep-ensembles.pdf
[21]Yann LeCun and Corinna Cortes. 2010. MNIST handwritten digit database.
http://yann.lecun.com/exdb/mnist/. (2010). http://yann.lecun.com/exdb/mnist/
[22]KiminLee,HonglakLee,KibokLee,andJinwooShin.2018. TrainingConfidence-
calibrated Classifiers for Detecting Out-of-Distribution Samples. In Interna-
tionalConferenceonLearningRepresentations. https://openreview.net/forum?id=
ryiAv2xAZ
[23]Kimin Lee, Kibok Lee, Honglak Lee, and Jinwoo Shin. 2018. ASimple Unified Framework for Detecting Out-of-Distribution Samplesand Adversarial Attacks. In Advances in Neural Information Process-
ing Systems 31, S. Bengio, H. Wallach, H. Larochelle, K. Grauman,
N. Cesa-Bianchi, and R. Garnett (Eds.). Curran Associates, Inc., 7167‚Äì
7177. http://papers.nips.cc/paper/7947-a-simple-unified-framework-for-
detecting-out-of-distribution-samples-and-adversarial-attacks.pdf
[24]ShiyuLiang,YixuanLi,andR.Srikant.2018. EnhancingTheReliabilityofOut-
of-distribution Image Detection in Neural Networks. In International Conference
on Learning Representations. https://openreview.net/forum?id=H1VGkIxRZ
[25]Lei Ma, Felix Juefei-Xu, Jiyuan Sun, Chunyang Chen, Ting Su, Fuyuan Zhang,
Minhui Xue, Bo Li, Li Li, Yang Liu, et al .2018. DeepGauge: Multi-Granularity
Testing Criteria for Deep Learning Systems. ASE, 120‚Äì131.
[26]Lei Ma, Felix Juefei-Xu, Minhui Xue, Bo Li, Li Li, Yang Liu, and Jianjun Zhao.
2019. DeepCT: Tomographic Combinatorial Testing for Deep Learning Systems.
In2019 IEEE 26th International Conference on Software Analysis, Evolution and
Reengineering (SANER). 614‚Äì618.
[27]L. Ma, F. Juefei-Xu, M. Xue, B. Li, L. Li, Y. Liu, and J. Zhao. 2019. DeepCT:
Tomographic Combinatorial Testing for Deep Learning Systems. In 2019 IEEE
26thInternationalConferenceonSoftwareAnalysis,EvolutionandReengineering
(SANER). 614‚Äì618.
[28]LeiMa,FuyuanZhang,JiyuanSun,MinhuiXue,BoLi,FelixJuefei-Xu,ChaoXie,
LiLi,YangLiu,JianjunZhao,andYadongWang.[n.d.]. DeepMutation:Mutation
Testing of Deep Learning Systems. In 29th IEEE International Symposium on
SoftwareReliabilityEngineering(ISSRE),Memphis,USA,Oct.15-18,2018.100‚Äì111.
[29]G. Mclachlan. 1999. Mahalanobis Distance. Resonance 4 (06 1999), 20‚Äì26. https:
//doi.org/10.1007/BF02834632
[30]Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, and Pascal Frossard. 2015.
DeepFool: a simple and accurate method to fool deep neural networks. CoRR
abs/1511.04599(2015). arXiv:1511.04599 http://arxiv.org/abs/1511.04599
[31]EricNalisnick,AkihiroMatsukawa,YeeWhyeTeh,andBalajiLakshminarayanan.
2020. Detecting Out-of-Distribution Inputs to Deep Generative Models Using
Typicality. https://openreview.net/forum?id=r1lnxTEYPS
[32]Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and An-
drew Y. Ng. 2011. Reading Digits in Natural Images withUnsupervised Feature
Learning. In NIPS Workshop on Deep Learning and Unsupervised Feature Learning
2011.http://ufldl.stanford.edu/housenumbers/nips2011_housenumbers.pdf
[33]AugustusOdena,CatherineOlsson,DavidAndersen,andIanJ.Goodfellow.2019.
TensorFuzz: Debugging Neural Networks with Coverage-Guided Fuzzing. In
ICML. 4901‚Äì4911.
[34]AccompaniedAnonymousWebsiteofDeepLearningTestingCallsforOut-Of-
Distribution Awareness. 2020. This Work‚Äôs Website. https://sites.google.com/
view/oodtesting/home
[35] KexinPei,YinzhiCao,JunfengYang,andSumanJana.2017. DeepXplore:Auto-
mated Whitebox Testing of Deep Learning Systems. CoRRabs/1705.06640 (2017).
arXiv:1705.06640 http://arxiv.org/abs/1705.06640
[36]Jie Ren, Peter J. Liu, Emily Fertig, Jasper Snoek, Ryan Poplin, Mark Depristo,
Joshua Dillon, and Balaji Lakshminarayanan. 2019. Likelihood Ratios for Out-of-
Distribution Detection. In Advances in Neural Information Processing Systems 32,
H. Wallach, H. Larochelle, A. Beygelzimer, F. d 'Alch√©-Buc, E. Fox, and R. Garnett
(Eds.).CurranAssociates,Inc.,14707‚Äì14718. http://papers.nips.cc/paper/9611-
likelihood-ratios-for-out-of-distribution-detection.pdf
[37]Ryne Roady, Tyler L. Hayes, Ronald Kemker, Ayesha Gonzales, and Christopher
Kanan.2019. AreOut-of-DistributionDetectionMethodsEffectiveonLarge-Scale
Datasets? CoRRabs/1910.14034(2019). arXiv:1910.14034 http://arxiv.org/abs/
1910.14034
[38]Tim Salimans, Andrej Karpathy, Xi Chen, and Diederik P. Kingma. 2017. Pix-
elCNN++: Improving the PixelCNN with Discretized Logistic Mixture Likeli-
hood and Other Modifications. CoRRabs/1701.05517 (2017). arXiv:1701.05517
http://arxiv.org/abs/1701.05517
[39]Vikash Sehwag, Arjun Nitin Bhagoji, Liwei Song, Chawin Sitawarin, Daniel
Cullina,MungChiang,andPrateekMittal.2019. AnalyzingtheRobustnessof
Open-World Machine Learning. In Proceedings of the 12th ACM Workshop on
Artificial Intelligence and Security (London, United Kingdom) (AISec‚Äô19).A s -
sociation for Computing Machinery, New York, NY, USA, 105‚Äì116. https:
//doi.org/10.1145/3338501.3357372
[40]JoanSerr√†,David√Ålvarez,Vicen√ßG√≥mez,OlgaSlizovskaia,Jos√©F.N√∫√±ez,and
Jordi Luque. 2020. Input Complexity and Out-of-distribution Detection with
Likelihood-based Generative Models. In International Conference on Learning
Representations. https://openreview.net/forum?id=SyxIWpVYvr
[41]Jianwen Sun, Tianwei Zhang, Xiaofei Xie, Lei Ma, Yan Zheng, Kangjie Chen,and Yang Liu. 2020. Stealthy and Efficient Adversarial Attacks against Deep
ReinforcementLearning.In AAAI. 5883‚Äì5891.
[42]Youcheng Sun, Xiaowei Huang, and Daniel Kroening. 2018. Testing Deep Neural
Networks. arXivpreprint arXiv:1803.04792 (2018).
[43]Youcheng Sun, Min Wu, Wenjie Ruan, Xiaowei Huang, Marta Kwiatkowska, and
DanielKroening.2018. ConcolicTestingforDeepNeuralNetworks.In Automated
Software Engineering (ASE). ACM, 109‚Äì119.
[44]Engkarat Techapanurak and Takayuki Okatani. 2019. Hyperparameter-Free
Out-of-Distribution Detection Using Softmax of Scaled Cosine Similarity. CoRR
abs/1905.10628(2019). arXiv:1905.10628 http://arxiv.org/abs/1905.10628
ASE ‚Äô20, September 21‚Äì25, 2020, Virtual Event, Australia David Berend, Xiaofei Xie, Lei Ma, Lingjun Zhou, Yang Liu, Chi Xu, and Jianjun Zhao
[45]TechCrunch. 2019. Didi Chuxing to launch self-driving rides in Shanghai and
expand them beyond China by 2021. https://techcrunch.com/2019/08/30/didi-
chuxing-to-launch-self-driving-rides-in-shanghai-and-expand-them-beyond-
china-by-2021/
[46]TechCrunch. 2019. Waymo‚Äôs robotaxi pilot surpassed 6,200 riders in itsfirst
monthinCalifornia. https://techcrunch.com/2019/09/16/waymos-robotaxi-pilot-
surpassed-6200-riders-in-its-first-month-in-california/
[47]TechCrunch. 2020. Nearly 70% of US smart speaker owners use Amazon Echo
devices.https://techcrunch.com/2020/02/10/nearly-70-of-u-s-smart-speaker-
owners-use-amazon-echo-devices/
[48]YuchiTian,KexinPei,SumanJana,andBaishakhiRay.2017.DeepTest:Automated
TestingofDeep-Neural-Network-drivenAutonomousCars. CoRRabs/1708.08559
(2017). arXiv:1708.08559 http://arxiv.org/abs/1708.08559
[49]New York Times. 2018. Self-DrivingUber Car Kills Pedestrian in Arizona, Where
RobotsRoam. https://www.nytimes.com/2018/03/19/technology/uber-driverless-
fatality.html
[50]New York Times. 2020. Tesla Autopilot System Found Probably at Fault in 2018
Crash.https://www.nytimes.com/2020/02/25/business/tesla-autopilot-ntsb.html
[51]Sachin Vernekar, Ashish Gaurav, Taylor Denouden, Buu Phan, Vahdat Abdelzad,
Rick Salay, and Krzysztof Czarnecki. 2019. Analysis of Confident-Classifiers for
Out-of-distribution Detection. CoRRabs/1904.12220 (2019). arXiv:1904.12220
http://arxiv.org/abs/1904.12220
[52]Run Wang, Felix Juefei-Xu, Qing Guo, Yihao Huang, Xiaofei Xie, Lei Ma,
and Yang Liu. 2019. Amora: Black-box Adversarial Morphing Attack.
arXiv:cs.CV/1912.03829
[53]Han Xiao, Kashif Rasul, and Roland Vollgraf. 2017. Fashion-MNIST:
a Novel Image Dataset for Benchmarking Machine Learning Algorithms.arXiv:cs.LG/cs.LG/1708.07747
[54]JianxiongXiao,KristaA.Ehinger,JamesHays,AntonioTorralba,andAudeOliva.
2016. SUN Database: Exploring a Large Collection of Scene Categories. Int. J.
Comput. Vision 119, 1 (Aug. 2016), 3‚Äì22. https://doi.org/10.1007/s11263-014-
0748-y
[55]XiaofeiXie,LeiMa,FelixJuefei-Xu,MinhuiXue,HongxuChen,YangLiu,Jianjun
Zhao,BoLi,JianxiongYin,andSimonSee.2019.DeepHunter:ACoverage-Guided
FuzzTestingFrameworkforDeepNeuralNetworks.In Proceedingsofthe28th
ACMSIGSOFTInternationalSymposiumonSoftwareTestingandAnalysis (Beijing,
China)(ISSTA 2019). Association for Computing Machinery, New York, NY, USA,
146‚Äì157. https://doi.org/10.1145/3293882.3330579
[56]Xiaofei Xie, Lei Ma, Haijun Wang, Yuekang Li, Yang Liu, and Xiaohong Li. 2019.
Diffchaser: Detecting disagreements for deep neural networks. In Proceedings
of the 28th International Joint Conference on Artificial Intelligence. AAAI Press,
5772‚Äì5778.
[57]J. M. Zhang, M. Harman, L. Ma, and Y. Liu. 2020. Machine Learning Testing:
Survey, Landscapes and Horizons. IEEE Transactions on Software Engineering
(2020).https://doi.org/10.1109/TSE.2019.2962027
[58]MengshiZhang,YuqunZhang,LingmingZhang,CongLiu,andSarfrazKhurshid.
2018. DeepRoad: GAN-based Metamorphic Testing and Input Validation Frame-
work for Autonomous Driving Systems. In Proceedings of the 33rd ACM/IEEE
International Conference on Automated Software Engineering (ASE 2018). 11.
[59]Xiyue Zhang, Xiaofei Xie, Lei Ma, Xiaoning Du, Qiang Hu, Yang Liu, Jianjun
Zhao, and Meng Sun. 2020. Towards Characterizing Adversarial Defects of Deep
Learning Software from the Lens of Uncertainty.
