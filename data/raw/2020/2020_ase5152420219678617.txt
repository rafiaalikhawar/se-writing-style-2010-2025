FRUGAL: Unlocking Semi-Supervised Learning
for Software Analytics
Huy Tu, and Tim Menzies∗
∗Department of Computer Science, North Carolina State University, Raleigh, USA
hqtu@ncsu.edu, timm@ieee.org
Abstract —Standard software analytics often involves having a
large amount of data with labels in order to commission models
with acceptable performance. However, prior work has shownthat such requirements can be expensive, taking several weeksto label thousands of commits, and not always available whentraversing new research problems and domains. UnsupervisedLearning is a promising direction to learn hidden patterns withinunlabelled data, which has only been extensively studied in defectprediction. Nevertheless, unsupervised learning can be ineffectiveby itself and has not been explored in other domains (e.g., staticanalysis and issue close time).
Motivated by this literature gap and technical limitations, we
present FRUGAL, a tuned semi-supervised method that builds ona simple optimization scheme that does not require sophisticated(e.g., deep learners) and expensive (e.g., 100% manually labelleddata) methods. FRUGAL optimizes the unsupervised learner’sconﬁgurations (via a simple grid search) while validating our de-sign decision of labelling just 2.5% of the data before prediction.
As shown by the experiments of this paper FRUGAL outper-
forms the state-of-the-art adoptable static code warning recog-nizer and issue closed time predictor, while reducing the costof labelling by a factor of 40 (from 100% to 2.5%). Hence weassert that FRUGAL can save considerable effort in data labellingespecially in validating prior work or researching new problems.
Based on this work, we suggest that proponents of complex and
expensive methods should always baseline such methods againstsimpler and cheaper alternatives. For instance, a semi-supervisedlearner like FRUGAL can serve as a baseline to the state-of-the-art software analytics.
Index T erms—Software Analytics, Data Labelling Efforts,
Semi-Supervised Learning
I. I NTRODUCTION
Software analytics can guide improvements to software
quality, maintenance and security. For example, analytics can
discover which static code warnings are adoptable [81, 73];whether the new issues can be easily ﬁxed [83, 42]; wheresoftware defects are likely to occur [1, 55]; which commentslikely to contain technical debts [40, 84]; what the currenthealth conditions of these open-source projects [76]; or howto distinguish security bug reports [65].
However, models that perform these software analytics tasks
typically learn from labelled data. Generating such labels can
be extremely slow and expensive. For instance, Tu et al. [70]reported that manually reading and labelling 22,500+ com-
mits required 175 person-hours (approximately nine weeks),including cross-checking among labellers. Due to the labor-intensive nature of the process, researchers often reuse datasetslabelled from previous studies. For instance, Lo et al. [79],Y ang et al. [82], and Xia et al. [80] certiﬁed their methodsusing data generated by Kamei et al. [30]. While this practiceallows researchers to rapidly test new methods, it leaves thepossibility for any labelling mistake to propagate to otherrelated works. In fact, in technical debts identiﬁcation, beforereusing prior work’s data [40], Y u et al. [84] discoveredthat more than 98% of the false positives were actually truepositives, casting doubt on work that used the original dataset.Hence, it is timely to ask:
Can we reduce the labelling effort associated with
building models for software analytics?
Unsupervised learning techniques that learns patterns fromunlabelled data is a promising direction for software analytics.Such learning has been used for buggy/non-buggy classiﬁca-tion [82, 77, 78]. The state-of-the-art (SOTA) unsupervisedlearner is Nam and Kim [53]’s CLA(C ) method. CLA is based
on the binary split of the output space at the aggregated median(C= 50%) of all features’ median in the data. However,
other areas and different datasets may not share the same datacharacteristics for the default CLA (C = 50%) to perform
well. To address this gap, our study adopts and extends CLAfrom defect prediction to other software analytics like staticcode warnings and issues close time.
Promising extensions for unsupervised learning involves
ﬁnding different control settings to conﬁgure the system(hyperparameter tuning) and validating on small labelled dataregions (e.g., 2.5%) before applying the best setting to thetest data. Recent software engineering (SE) research showsmany domains’ SOTA can be improved with hyperparametertuning [1, 52]. DODGE is one prominent optimizer whichshows the output space of the models on low-dimensionaldata can be easily surveyed through dodging away from (1)prior options or options that resulted (2) in statistically similarperformance. Simply, the central function of CLA (binary splitof the output space via aggregated C) is synonymous to SE’s
SOTA optimizer DODGE with less information required, areduction of 97.5% train data’s labels. Speciﬁcally, our workproposed FRUGAL(C ) = three different modes of CLA (as
shown in Figure 1) with C={5% to95%, increments by
5%} where all the combinations can be easily executed in a
grid search manner.
To understand and validate the FRUGAL system, we
investigate the following research questions:
RQ1: How much labelled data (L%) that FRUGAL
requires?
From our investigation of various Lvalues, FRUGAL’s
performance plateaus beyond L≥2.5% and FRU-
GAL’s success is not altered by large changes to L.Result:
3942021 36th IEEE/ACM International Conference on Automated Software Engineering (ASE)
DOI 10.1109/ASE51524.2021.000432021 36th IEEE/ACM International Conference on Automated Software Engineering (ASE) | 978-1-6654-0337-5/21/$31.00 ©2021 IEEE | DOI: 10.1109/ASE51524.2021.9678617
978-1-6654-0337-5/21/$31.00  ©2021  IEEE
Fig. 1: Three different modes of CLA devised from Nam and Kim [53] for defect prediction.
RQ2: How does FRUGAL perform in adoptable static code
warnings identiﬁcation?
When comparing to the SOTA solution in EMSE’20[81], FRUGAL wins on recall, loses in AUC, anddraws in FAR with 2.5% of the labelled train data.Result:
RQ3: How well does FRUGAL predict issue close time?
When comparing to the SOTA solution in EMSE’20[83] (which was compared to ICSE’10 [20],PROMISE’11 [43], MSR’16 [31], COMAD’19 [42]),FRUGAL outperforms in FAR, recall, and AUC whileperforming similarly in accuracy with only 2.5% ofthe labelled train data.Result:
In summary, our work’s contributions to the ﬁeld of software
analytics are as follows:
1) This work is the ﬁrst to assess the usage of unsupervised
learning to reduce the labelling efforts to commission mod-els building in adoptable static code warnings identiﬁcationand issues close time prediction.
2) FRUGAL surpasses the SOTA issues close time predictor
and performs similarly to the SOTA adoptable static codewarning identiﬁer with 97.5% less information.
3) FRUGAL reduces the labelling efforts to commission new
models building by 97.5%. In another word, FRUGAL is40 times cheaper than SOTA methods in issue close timeand static code warning analysis areas.
4) The performance of our framework suggests that many
more domains in SE could beneﬁt from unsupervisedlearning solutions in the semi-supervised learning mannerbeyond defect prediction [53, 82, 77, 54, 78].
5) To better support other researchers our scripts and data are
on-line at https://github.com/SE-Efforts/SE
SSL.
II. B ACKGROUND AND RELA TED WORK
A. Studying Static Code Warnings
1) Background: Static code warning tools detect potential
static code defects in source code or executable ﬁles at thestage of software product development. This covers a rangeof potential defects such as common programming errors,code styling, in-line comments common programming anti-patterns, style violations, and questionable coding decisions.The distinguishing feature of these tools is that they make theircomments without reference to a particular input. Nor do theyuse feedback from any execution of the code being studied.Examples of these tools include PMD
1, Checkstyle2and the
FindBugs3tool.
One issue with static code warnings is that they generate
a large number of false positives. Many programmers rou-tinely ignore most of the static code warnings, ﬁnding themirrelevant or spurious [73]. Such warnings are considered as“unadoptable” since programmers just ignored them. Between35% and 91% of the warnings generated from static analysistools are known to be unadoptable. This high false alarm rateis one of the most signiﬁcant barriers for developers to usethese tools [3, 29]. Hence it is prudent to learn to recognizewhat kinds of warnings programmers usually act upon so thetools can be made more useful by ﬁrst pruning away theunadoptable warnings. V arious approaches have been tried
1https://pmd.github.io/latest/index.html
2https://checkstyle.sourceforge.io/
3http://ﬁndbugs.sourceforge.net
395TABLE I: Summary of Yang et al. [81]’s data distribution.
The gray cells are median values for the correspondingcolumns.
training set test set
Dataset FeaturesInstance
CountsAdoptable
Ratio(%)Instance
CountsAdoptable
Ratio(%)
commons 39 725 7 786 5
phoenix 44 2235 18 2389 14
mvn (maven) 47 813 8 818 3
jmeter 49 604 25 613 24
cass (cassandra) 55 2584 15 2601 14
ant 56 1229 19 1115 5
lucence 57 3259 37 3425 34
derby 58 2479 9 2507 5
tomcat 60 1435 28 1441 23
to reduce these false alarms including graph theory [6, 5],statistical models [11], and ranking schemes [34]. Previouswork [81] referred to the target warnings found by theseapproaches as “actionable” warnings, but we found that itactually refer to “adoptable” warnings that were adopted.That means the warnings that are adopted by developersdo not necessarily mean the warnings are actionable (somedevelopers still need to consult external sources to ﬁgure outthe solutions).
2) Data and Algorithms: The data for this paper comes
from a recent study by Wang et al. [73]. They conducted asystematic literature review to collect all public available staticcode features generated by widely-used static code warningtools (116 in total):
•All the values of these collected features were extractedfrom warning reports generated by FindBugs based on 60revisions of 12 projects.
•To ensure the difference between prior and later revisionintervals of a project is adequate for the solid conclusions tobe drawn, Wang et al. [73] set revision intervals for differentprojects, e.g., 3 months for Lucene and 6 months for Mvn.
Each project in this study has at least two-years commithistory.
•To eliminate ineffective features to the results of thoselearners, a greedy backward selection algorithm is applied.Then they isolated 23 features as the most useful ones foridentifying adoptable static code warnings.
•They called these features the “golden set”; i.e. the fea-tures most important for recognizing adoptable static codewarnings.
To the best of our knowledge, this is the most exhaustiveresearch about static warning characteristics yet published. Asshown in Table 1 of [81], the “golden set” features fall intoeight categories. These features are the independent variablesused in this study. To assign dependent labels, we applied themethods of Liang et al. [37]. They deﬁned a speciﬁc warningas adoptable if it is closed after the later revision interval.
By analyzing FindBugs output from two consecutive re-
leases of nine software projects, collecting the features ofTable 1 from [81], and then applying the Liang et al.’sdeﬁnitions, we created the data of Table I. In this table, the“training set” refers to release i−1and the “test set” is
releasei. In this study, we only employ two latest releases.One of many extensive studies exploring the usage of MachineLearning (ML) in this area is Heckaman et al. [23]. Theyapplied 15 ML algorithms to recognize the adoptable warnings(programmers can act upon) based on 51 features derived fromstatic analysis tool, they achieved recalls of 83-99 % (averageacross 15 data sets). The SOTA system that we will compareagainst is from Y ang et al. [81] where they took advice fromGhotra et al. [19] to compare several representative non-neurallearners (Table 9 of [19]) in software analytics with variouspopular neural-network models. They found that all treatmentsperformed similarly to each other but non-neural learners didthat with less time than deep learners.
Note that, for any particular data set, the 23 features can
grow. For example, consider the “return type” feature in the“code analysis” category. This can include numerous returntypes extracted from a given project, which could be void, int,URL, boolean, string, printStream, ﬁle, and date (or a list ofany of these periods). Hence, as shown in Table I, the numberof features in our data varied from 39 to 60.
B. Predicting Bugzilla Issue Close Time
1) Background: When programmers work on repositories,
predicting issue close time has multiple beneﬁts for the devel-
opers, managers, and stakeholders since it is helpful for (1)end-users who are directly affected by the product; (2) de-velopers prioritize work; (3) managers allocate resources andimprove consistency of release cycles; and (4) stakeholdersunderstand changes in project timelines and budgets:
•Although bugs have an assigned severity, this is not a sufﬁ-cient predictor for the lifetime of the issue. For example, theauthor who issued the bug may be signiﬁcant contributorsto the project.
•Alternatively, an issue deemed more visible to end-users
may be given higher priorities. It is therefore insufﬁcientsimply to consider the properties of the issue itself ( issue
metrics), but also of its environment (context metrics). Sim-ilarly, recent work showed how process metrics are better
measurements for predicting than product metrics [39].
An example of such issue close times estimator can notify
involved parties if the recently created issue is an easy ﬁx.
2) Data and Algorithms: The state-of-the-art system for
predicting issue close time comes from a recent study byY edida et al. [83]. They conducted a literature review of99 research papers that are comprised of (1) from Watson’sliterature reviews; and (2) top venues listed in Google Scholarmetrics for Software Systems, Artiﬁcial Intelligence, andComputational Linguistics in the last three years with at least10 citations per years:
•Traditional or non-neural approaches include (1) Guo et al.[21]’s study on a large closed-source project (MicrosoftWindows) to predict whether or not a bug will be ﬁxed;and (2) Marks et al. [43] used ensemble method of decisiontrees, i.e., random forests, on Eclipse and Mozilla data.
•As to deep learning or neural network approach areDASENet [35] and DeepTriage [42].
396TABLE II: An overview of the data used in the Lee et al.
[35] and Yedida et al. [83] study. Note that because of themanner of data collection, i.e., using bin-sequences for eachday for each report, there are many more data samplesgenerated from the number of reports mined.
Project Observation Period # Reports # Train # Test
Eclipse Jan 2010–Mar 2016 16,575 44,545 25,459
Chromium Mar 2014–Aug 2015 15,170 44,801 25,200
Firefox Apr 2014–May 2016 13,619 44,800 25,201
•Only a minority of deep learning papers (39.4%) performed
any sort of hyper-parameter optimization, i.e., varied fewnumbers of parameters, such as the number of layers ofthe deep learner, to edge out the best performance ofdeep learning. Even fewer papers (18.2%) applied hyper-parameter optimization in a non-trivial manner; i.e., notusing a hold-out set to assess the tuning before assessingthe separate test set).
To obtain a fair comparison with the prior state-of-the-art,
we use the same data as used in the Lee et al. [35], Mani et al.[42], Y edida et al. [83]’s studies. The data was collected fromthe three projects of Firefox, Chromium, and Eclipse:
•Preprocessing involves standard text mining to removespecial characters or stack traces, tokenization, and pruningthe corpus to a ﬁxed length.
•The activities per day were collected into two bins includ-ing user activity (e.g., comments), system records (e.g.,added/removed labels), and metadata (e.g., the user was thereporter, days from opening, etc). Given issue close times(1, 2, ... k days), they are grouped into S1 and S2 suchthat|S1|≈|S2|andS1={1,...i},S2={i+1,...,k}.
For instance, S1 includes 1-43 days and S2 includes 44-365 days, if the number of issues closed in 1 to 43 days ≈
number of issues closed in 43 to 365 days.
•Along with the numerical metadata, user and system recordsare transformed to machine-readable data for the models toexecute through word2vec [48, 47].
In the same manner as prior work, the target class is
discretized into two bins (so that each bin has roughly thesame number of samples). This yields datasets that are near-perfectly balanced (e.g., in the Chromium dataset, we observeda 49%-51% class ratio).
C. Evaluation
1) Measures of Performance: Since we wish to compare
our approach to prior work, we take the methodological step
of adopting the same performance scores as that seen in priorwork. Let TP , TN, FP , FN are the true positives, true negatives,false positives, and false negatives (respectively), then Y anget al. [81] used AUC, recall, and false-alarm while Y edidaet al. [83] using only accuracy for their studies. We alsoadd precision and f1 for additional validation (but see ourcautionary note at the end of this list):
•AUC (Area Under the ROC Curve) measures the two-
dimensional area under the Receiver Operator Characteristic(ROC) curve [75, 24]. It provides an aggregate and overallevaluation of performance across all possible classiﬁcationthresholds to report the discrimination of a classiﬁer [73].
•Precision =TP/ (TP+FP) represents the ability of one
algorithm to identify instances of positive class among theretrieved positive instances.
•Recall =TP/ (TP+FN)represents the ability of one
algorithm to identify instances of positive class from thegiven dataset.
•F1=(2∗Precision ∗Recall )/(Precision +Recall )is the
harmonic mean of both precision and recall metrics.
•False Alarms (FAR) =TN/ (TN +FP) measures the
instances that are falsely classiﬁed by an algorithm aspositive which are actually negative. This is an importantindex used to measure the efﬁciency of a model.
•Accuracy =(TP+TN)/(TP+TN+FP+FN)is the
percentage of correctly classiﬁed samples.
•In the effort-aware theme of this paper, we are interestedin the labelling effort to commission new models building
which is Cost =
|{human veriﬁed comments}|
|{comments}|.
•Except for FAR and Cost, for the rest of these metrics
(Accuracy, Recall, and AUC), the higher the value, the
better the performance.
Cautionary note: Menzies et al. [45] warns that precision
can be misleading for imbalanced data sets like that studiedhere (e.g. Table I reports that for static warning analysis, themedian of target class is 15%). Hence, while we do not placemuch weight on classiﬁers that fail on precision or F1.
2) Statistical Analysis: With the deterministic nature, we
employed Cohen’d effect size test to determine which results
are similar by calculating Medium
step2across those seven
metrics. As to what dto use for this analysis, we take the
advice of a widely accepted Sawilowsky et al.’s work [61].That paper asserts that “small” and “medium” effects can bemeasured using d=0.2andd=0.5(respectively). Splitting
the difference, we will analyze this data looking for differenceslarger than d=( 0.5+0.2)/2=0.35:
Medium step2 orM=0.35 ·StdDev (All results) (1)
The SOTA adoptable code warnings identiﬁer and the SOTAissue close time predictor also validated their results with thistest but with d=0.35andd=0.3respectively.
III. L
ABELLING
One of the goals of industrial analytics is that new conclu-
sions can be quickly obtained from new data just by applyingdata mining algorithms. As shown in Figure 2, there are at leastnine separate stages that must be completed before that goalbe reached [2]. Each of these stages offers unique and separatechallenges, each of which deserves extensive attention. Manyof these steps have been extensively studied in the literature[84, 58, 13, 14, 15, 40, 41, 28]. However, the labelling workof step 4 has been receiving scant attention. In literature, thereare several approaches for executing the labelling process:
1) Manual labelling;2) Crowdsourcing;3) Reuse of labels;
397Fig. 2: Nine stages of the machine learning workﬂow from a case study at Microsoft by Amershi et al. [2]. Some stages
are data-oriented (e.g., data collection, cleaning, and labelling) and others are model-oriented (e.g., model requirements,features engineering, model training, evaluation, evaluation, deployment and monitoring).
4) Automatic labelling;5) Active learning (a special kind of semi-supervised learning)
All of these approaches have their drawbacks; e.g. they are
error-prone or will not scale. In response to these shortcom-ings, this study will take two directions:
•First, we will try a label-free approach using a plain
unsupervised learning technique to label the data;
•If the label-free approach fails, then we try tuned semi-
supervised learning, called FRUGAL, which optimizes theunsupervised learner’s conﬁgurations in the grid searchmanner while validating the results on only 2.5% of thelabelled data.
A. Manual Labelling
In manual labelling, a team of (e.g.) graduate students
assigns labels then (a) cross-checks their work via say, a Kappastatistic; then (b) use some skilled third person to resolve anylabelling disagreements [38, 69, 70].
Manual labelling can be very slow. Tu et al. recently studies
a corpus of 678 Github projects [70, 69]. A random selectionof 10 projects from that corpus had 22,500 commits, which
took 175 hours (includes cross-checking) to manually labelthe commits buggy, non-buggy. That is, manual labelling of
those 500 projects would have required 90 weeks of work.
B. Crowdsourcing
Tu et al. [70] offers a cost estimate of what resources
would be required to sub-contract that effort to dozens of
crowdsourced workers via tools like Mechanical Turk (MT).Applying best practices in crowdsourcing [10], assuming (a) atleast USA minimum wages [66]; and (b) our universitytaking a 50% overhead tax on grants; then crowd sourcingthe labelling of the issues from 500 projects would require$320,000 of grant reserve.
C. Reusing Labels
Since manual labelling is time consuming and crowdsourc-
ing is too expensive, researchers often reuse labels from previ-
ous studies. E.g. for defect prediction, researchers [80, 82, 79]certiﬁed their methods using data generated by Kamei et al.[30]. This approach fails, in two cases. Firstly, when exploringa new domain, there may be no relevant old labels to reuse.Secondly, reusing labels means incorrectly labelled examplescan contamiante other research. For example, Y u et al. [84]were exploring self-admitted technical debt and found thattheir classiﬁers had an alarming high false-positive rate. Butwhen they manually checked the labels of their data (which isfrom a prior study by Maldonado et al. [40]), they found thatover 98% of the reused false-positive labels were incorrect.
D. Automatic Labelling
If labels cannot be generated manually or reused from other
papers, using automatic labelling processes is an attractive
alternative. For example, defect prediction papers [59, 32,49, 30, 25] can label a commit as “bug-ﬁxing” when thecommit text contains certain keywords (e.g. ”bug”, ”ﬁx”,”wrong”, ”error”, ”fail” etc [70]). V asilescu et al. [72] notedthat these keywords are used in a somewhat ad hoc manner(researchers peek at a few results, then tinker with regularexpressions that combine these keywords). Tu et al. [70] hadfound that these simplistic keyword approaches can introducemany errors, perhaps due to the specialization of the projectnature or the ad-hoc nature of their creation. In technicaldebts identiﬁcation, Y u et al. proposed a pattern-based methodthat automatically identiﬁed 20-90% of self-admitted techni-cal debts (SA TDs) by ﬁnding patterns associated with highprecision from the labelled training sets (close to 100%).This approach does need extensively labelled training data toﬁnd quality patterns that are associated with technical debtbecause it relies on precision. Another automatic approach isML which involves supervised learning models to train onexisting labelled datasets to learn the underlying rules of thedata. However, this also requires having access to a substantialamount of labelled data (especially for deep learners) whichis not always available in new domains (e.g., the success ofopen-source projects).
E. Active Learning
A third approach is to (a) only label a representative sample
of the data; then (b) build a classiﬁer from that sample; then
(c) use that classiﬁer to label the remaining data [74]. To ﬁndthat representative example, some unsupervised learners likean associations rule learner or a clustering algorithm or aninstance selection algorithm is used to ﬁnd repeated patternsin the data [32]. Then a human oracle is asked to label justone exemplar from each pattern. More sophisticated versionsof this scheme include active learners, where an AI tool
rushes ahead of the human to fetch the most information nextexamples to be labelled [33, 63]. If humans ﬁrst label mostinformative examples, then better models can be built faster.This means, in turn, that humans have to label fewer examples.
The more general term for active learning issemi-
supervised learning. Both terms mean “do what you can witha small sample of the labels” while active learning adds a
398TABLE III: Differences between FRUGAL and Zhang
et al. [86]
FRUGAL (this paper) Zhang et al. [86]
Core
AssumptionApplies SE domainknowledge; i.e.higher complexityis associated withtarget instances (whichwe measure as beingaboveC)Applies graph the-ory; i.e. continuityand clustering (simi-lar things have simi-lar properties).
Hyperparameter Ye s No
Optimization
ClassRebalancing No Y es (with the Lapla-cian score samplingstrategy)
feedback loop that checks new labels, one at a time, with some
oracle. Semi-supervised learning relies on partially labelleddata and mostly unlabelled data.
Since 2012, active learning approaches have been received
scarce attention in SE [33, 70, 85, 84]. Initially, it seems to bea promising method for addressing the cost of label checkingand generating. For self-admitted technical debt identiﬁcation,only 24% on the median of the training corpus had to belabelled [84]; Also, using active learning, effort estimation forNprojects only needed labels on 11% of those projects [33];
Further, while seeking 95% of the vulnerabilities in 28,750Mozilla Firefox C and C++ source code ﬁles, humans only hadto inspect 30% of the code [85]. That said, after much work, itmust be reported that active learning still produces disappoint-ing results. It is still daunting to “only” label (say) 5% to 2.5%of the projects in the 1,857,423 projects in RepoReapers [50]or the 9.6 million links explored by Hata et al. [22]. Also,consider the Firefox study mentioned in the last paragraph.The human effort of inspecting 28,750×30% = 8,625 sourcecode ﬁles (needed to ﬁnd identify 95% of the vulnerabilities)it is beyond the resources of most analysts (but it might bejustiﬁed for mission-critical projects). Finally, for defect pre-diction, Zhang et al. [86] proposed NSGLP and certiﬁed theirmethod by varying the size of labeled software modules from10-30% of all the NASA datasets. The differences between ourapproach and their are listed in the Table III. They claimedthat the proposed method outperformed several representativeSOTA semi-supervised ones for software defect prediction.However, reproducing that paper is complex since it waswritten before the current focus on research paper artifacts(so that paper has no reproduction package). Moreover, recentand widely cited studies argue that the datasets used in thatanalysis are of dubious quality [57, 64].
It is opportune that in this adoptable static code warnings
identiﬁcation and issue close time prediction work, we aim toreduce the reviewing cost of these labelling methods by twomethods (1) label-free approach with unsupervised learning,or (2) tuned semi-supervised learning to optimize the unsuper-vised learner’s conﬁgurations in the grid search manner whilevalidating the results on a small amount of the labelled data,i.e., 2.5%.Algorithm 1: Pseudocode of FRUGAL
Input: train data, tune data, test data
Output: result
1percentiles = range(5, 100, 5)
2methods = [CLA, CLA ML, CLAFI ML]
3best result = -1
4best model = None
5forM in methods do
6 forC in percentiles do
7 model = M(C).ﬁt(train data)
8 temp result = model.predict(tune data)
9 ifisBetter(temp result, best result) then
10 best result = temp result
11 best model = model
12return best model.predict(test data)
IV . M ETHODOLOGY
A. General Framework
Our approach, shown in Algorithm 1, extends unsupervised
learning with some semi-supervised learning and tuning. Namet al.’s CLA is the SOTA unsupervised learner for defectprediction, which is also conﬁrmed by Xu et al. [77]’s large-scale study. As shown in Figure 1, CLA consists of threemodes: CLA, CLA+ML, and CLAFI+ML. This study shalladopt and extend CLA with tuning in the grid search mannerof (1) three modes of CLA while varying (2) the C%percentile
parameter. Simply, as illustrated in Algorithm 1, FRUGALﬁnds the best combination of unsupervised learners = {CLA,
CLA+ML, CLAFI+ML} andC={5% to95% increments
by5%}. The author only proposed CLA and CLAFI+ML but
CLA+ML is a natural medium that can be useful during thetuning process.
We explain the details of our approach in .B, .C, and .D.
B. CLA
In the SOTA comparative study of unsupervised models in
defect prediction, CLA starts with two steps of (1) C
lustering
the instances and (2) LAbelling those instances accordingly
to the cluster. In the setting with no train data available, wecan label or predict all new/test instances, as shown in the
ﬁrst block of Figure 1.
Clustering :
1) Find the median of feature F1,F2,...,F n
(percentile( Fi,C)) where C= 50% across the whole
dataset.
2) For each data instance Xi, go through each feature value
of the respective data instance to count the time when thefeatureF
i>p e r c e n t i l e (Fi,C)asKi.
Labelling : label the instance Xias the positive class if Ki>
median(K), else label it as the negative class.
The intuition of such methods is based on the defect prone-
ness tendency that is often found in defect prediction research,that is the higher complexity is associated with the proneness
of the defects [53]. Simply, there is a tendency where the
problematic instance’s feature values are higher than the non-problematic ones. This tendency and CLA ’s/CLAFI+ML’s
399effectivenesses are conﬁrmed via the recent literature and com-
parative study of 40 unsupervised models in defect predictionacross 27 datasets and three types of features by Xu et al. [77].They found CLA ’s/CLAFI+ML’s performances are superiorto other unsupervised methods while similar to supervisedlearning approaches. Therefore, this study investigated andfound that the hypothesized tendency is also applicable inissues close time prediction and adoptable static code warningidentiﬁcation data but not with Cat the median (C = 50%).
This opens opportunities for hyperparameter tuning.
C. CLA + ML
If there is an abundant train data in the wild but without
labels, CLA can pseudo-label the train data before applying
any machine learner in the “supervised” manner (as shown inthe
second block in Figure 1). For this step, we take Nam
and Kim [53]’s advice to incorporate Random Forest [8] (RF,described in .E.1), an ensemble of tree learners method, as themachine learner of choice.
D. CLAFI + ML
CLAFI is an extension of CLA which is a fullstack frame-
work that also include (3) F
eatures selection and (4) Instances
selection. The setting is similar to CLA+ML, as shown in the
third block of Figure 1, the pseudo-labelled train data (from
CLA) and unlabelled test data will be processed with FIandF
respectively. Finally, machine learner can train the processed
pseudo-labelled train data and then predict on the processedtest data.
Feature Selection
: Calculate the violation score per feature,
called metric in the original proposal of Nam et al. [53]. The
process is done on both the train and the test dataset.
1) For each Fi, go through all instances of Xj, a violation hap-
pens when FiatXjis higher than the percentile (Ki,C)
whereC= 50% butYj=1 and vice-versa.
2) Sum all the violations per feature across the whole dataset
and sort it in ascending order.
3) Select the feature with the lowest violation score, if multiple
of them have the same score then pick all of them.
Instance Selection :
1) With the selected features, go through each instance Xiand
check if the respective Fjvalues violated the proneness
assumption then remove that instance Xi.
2) If the dataset do not have instances with both classes at the
end then pick the next minimum violation score to selectmetrics.
3) This process is only done on the train dataset.
After selecting features with the minimum violation scores
and removing the instances that violated the proneness ten-dency, a practitioner can train an RF model on the processedtrain data to identify the target classes from the processed testdataset.
E. Machine Learning Models
1) Random Forest (RF): is an ensemble learning method
that operates by constructing a multitude of decision trees,each time with different subsets of the data rows Rand
columnsC
4. Each decision tree is recursively built to ﬁnd the
features that reduce most of entropy, where a higher entropy
indicates less ability to draw conclusions from the data being
processed [7]. Test data is then passed across all Ntrees and
the conclusions are determined (say) a majority vote across allthe trees [8]. Holistically, RF is based on bagging (bootstrapaggregation) which averages the results over many decisiontrees from sub-samples (reducing variance).
2) Support Vector Machine (SVM): is a classiﬁer deﬁned
by a separating hyperplane [67]. Soft-margin linear SVMsare commonly used in text classiﬁcation given the high di-mensionality of the feature space. This was recommended byY ang et al. [81] as the state of the art for our adoptable staticcode warning identiﬁcation domain. A soft-margin linear SVMlooks for the decision hyperplane that maximizes the marginbetween training data of two classes while minimizing thetraining error (hinge loss):
minλ/bardblw/bardbl
2+/bracketleftBigg
1
nn/summationdisplay
i=1max (0, 1−yi(w·xi−b))/bracketrightBigg
(2)
where the class of xis predicted as sgn(w·x−b).
Both SVM and RF are popular in the ﬁeld of ML and
implemented in the popular open-source toolkit Scikit-learnby [56].
3) Feedforward Neural Networks: is the ﬁrst and simplest
technology devised from artiﬁcial neural network [62]. Theinformation moves in the forward direction only, startingfrom the input nodes through the hidden nodes and to theoutput nodes. At each node of these networks, the inputsare multiplied with weights that are learned, and then anactivation function is applied. The weights are learned bythe backpropagation algorithm [60]. This uses just a fewlayers while the “deep” learners use many layers. Also, theolder methods use a threshold function at each node, whilefeedforward networks typically use the Rectiﬁed Linear Unitfunction [51] of f(x)=m a x ( 0 ,x). This is the base learner
for our second domain’s state of the art where Y edida et al.[83] proposed a framework combining different preprocessorsand different conﬁgurations of the simple feedforward neuralnetwork.
V. R
ESULTS
In order to make sure our proposed method’s effectiveness
is not affected by the bias between deterministic and non-deterministic models or the bias of uncertainty, we randomlyshufﬂe train/test sets and incorporate stratiﬁed sampling withﬁve bins (ensuring that the class distribution of the wholedata is replicated in each bin). The process is repeated for thetrain data but also includes an extra 2.5% validating partitionfor each 97.5% tuning partition. The median 2.5% of labelledtrain data for static warning analysis and issue close time are36 and 1120 respectively. During the simulation, the tunepartition will not review labels for our unsupervised learningand semi-supervised learning candidates. FRUGAL does have
4Speciﬁcally, using log2Cof the columns, selected at random.
400AUC in Actionable Static
Warning identiﬁcation.
“medium effect” or M=5 %
Accuracy in Issues Close
Time prediction.
“medium effect” or M=2 %
%
Fig. 3: RQ1 results on two domains with FRUGAL(L ∈
{1%, 2.5%,5%,10%, 20%}). FRUGAL with L> 1% per-
form similarly. However, for adoptable static warning
identiﬁcation, FRUGAL(L =2.5%) does perform the
best across 7 datasets except in Lucene and Cass (the
highlighted ones) where FRUGAL(L=20%) outperforms
FRUGAL(L=2. 5%).
access to the corresponding 2.5% labelled validation partitionwhile deciding on the best conﬁgurations. For each 20% ofthe test data, the process learns a model on ﬁve stratiﬁedsamples of the train data. This process is done for bothdomains in this paper.
RQ1: How much labelled data (L%) that FRUGAL
requires?
Our hypothesis is “there are few key data regions where
extra data would lead to indistinguishable results”. We test thedifferent amounts of the train data’s labels that are requiredfor FRUGAL’s performance to plateaus. Let Lbe 1%, 2.5%,
5%, 2.5%, or 20%, Figure 3 reports FRUGAL’s performanceon both adoptable static warning identiﬁcation (in AUC) andissues close time prediction (in accuracy). Both metrics arederived from the SOTA ’s evaluation metrics. Speciﬁcally, wepick AUC as the representation metric for adoptable staticwarning analysis since AUC measures the area under the curvewhereas other metrics only calculate a single point on thecurve. From Figure 3:
•The lower bound for both domains is L=1 % .
•For adoptable static warning identiﬁcation, FRUGAL’s per-formance improves initially and plateaus beyond L=
2.5% across 7 datasets. FRUGAL(L =2.5%) loses to
FRUGAL(L = 20%) in only Lucene and Cass projects but
reduces 8 (20%/2.5%) times the labelling efforts.
•For issues close time prediction, FRUGAL surpris-ingly performs statistically similar across all L∈
{2.5%,5%,2.5%,20%}.
The same effect is absent in issue close time prediction,
this is highly likely due to the balanced nature of the data’sclass distribution. However, the data in static warning analysisis more imbalanced (with a median of 15% for the adoptablestatic warning class ratio). This is consistent with the moti-vations for oversampling and undersampling techniques forimbalanced data [9].
From our investigation of various Lvalues, FRUGAL’s
performance plateaus beyond L≥2.5% and FRU-
GAL’s success is not altered by large changes to L.In summary, our answer to RQ1 is:
RQ2: How does FRUGAL perform in adoptable static
warnings identiﬁcation?
Wang et al. [73] proposed the “golden set” features along
with the ML study where they employed RF, Decision Tree,and SVM (with the RBF kernel) with median AUC perfor-mances at 70%, 64%, and 50%. Y ang et al. [81] extensivelyinvestigated different deep learners (DNN, CNN, RF, DecisionTree, and SVM) that pushed Wang et al.’s results to new higherwatermarks in the area with median AUC performances at99.5%, 95.9%, and 99.5% with almost 45%, 55%, and 100%relative improvements to the same learner choices as Wanget al. [73]. The default parameters in Weka (used by Wanget al. [73]) are different to those used in SciKit-Learn (usedby Y ang et al. [81]). For instance, Wang et al.’s SVM usedRBF kernel while Y ang et al. [81]’s SVM used linear kernel.
Y ang et al. [81] proposed the standard linear SVM as the
SOTA ’s adoptable static warning identiﬁer. Table IV reportsthe comparison of our proposed method FRUGAL, the SOTA ’sSVM (with 100% labelled data and 2.5% labelled data), andthe baseline unsupervised learners (CLA & CLAFI+RF) acrossFAR, recall, precision, F1 and AUC. In those results:
•Standard unsupervised learner CLA/CLAFI+RF performsthe worst as their default behavior is clustering based on themedian of the data which may not apply for all the data andespecially in the static warning analysis. However, CLA ’srecalls are almost 100% in a few cases (Mvn, Cass, and
Commons) and CLAFI+RF’s FAR are almost 0% in morethan half cases ( Derby, Lucene, Cass, Jmeter and Tomcat ).
This indicates promising areas for tuning conﬁgurations ofunsupervised learners.
•The SOTA work originally evaluated their method on FAR,recall, and AUC. With the same comparison, FRUGALperforms better than the SOTA ’s SVM as FRUGAL winsin Recall and FAR while losing in AUC. However, whenconsidering precision and F1, FRUGAL underperforms.Recalling our cautionary note (from the end of §II-C1),precision (and hence F1) can be misleading for data setswhere the target class is rare [45] (e.g. as shown in Table I,the median of target class is 15%). Therefore, overall, wesay FRUGAL performs similarly to the SOTA ’s SVM.
401TABLE IV: Comparison between CLA[53], SVM[81], and FRUGAL in terms of FAR, Recall, Precision, F1, and AUC
for identifying adoptable static warning. In this table, the FRUGAL results were found after labelling just 2.5% of thedata. Except for FAR, the higher the results the better the performance of the treatment. Medians and IQRs (deltabetween 75th and 25th percentile, lower the better) are calculated for easy comparisons. Here, the
highlighted cells
show best performing treatments.
Metrics Treatment
Derby
Mvn
Lucene
Phoenix
Cass
Jmeter
Tomcat
Ant
Commons
Median
IQR
CLA 43.3 44.9 30.8 39.1 42.1 39.4 43 45.8 43 43 4.2
FAR CLAFI+RF 0.4 18.1 0.8 29.4 0.5 4.1 0.4 22.5 13.6 4.1 18.5
(M=6 % ) FRUGAL 1 4.8 3.1 2.1 0.6 8.0 2.8 3.7 7.8 3.1 2.9
SVM [81] (L=2.5%) 0.7 100 5.1 0.5 0.8 10.5 2.1 0.2 100 2.1 32.2
SVM [81] 1.3 1.2 6.9 3.5 1.4 2.1 3.2 0.5 5.8 2.1 2.7
CLA 45.8 100 64.5 66.7 98.6 75.9 63.1 80 100 75.9 32.8
Recall CLAFI+RF 57.2 93.3 67.1 62.1 83.7 73.1 64.4 77.8 77.5 73.1 12.9
(M=9 % ) FRUGAL 93.7 92.9 100 98.7 92.6 88.4 98.8 94.2 99 94.2 6
SVM [81] (L=2.5%) 0 0 73.6 65.2 31.9 31.4 78.9 58.5 0 31.9 43.9
SVM [81] 97.8 97 87.1 96.1 90.3 93.3 98.2 95 99.5 96.1 4.8
CLA 4.8 6.6 49.6 22.3 27.3 38.3 31.9 8.2 11.1 22.3 23.1
Precision CLAFI+RF 1.9 4.3 39.3 15.5 24.1 22 18 4.6 9 15.5 14.6
(M= 17%) FRUGAL 61.4 63.5 60.9 61.3 67 50.6 67.9 32.6 78.4 61.4 8.6
SVM [81] (L=2.5%) 0 0 86.4 85 90.8 54 91.8 94 0 85 55.6
SVM [81] 86.4 100 72.7 84.3 100 71.4 83.1 90 33.3 84.3 20
CLA 8.7 12.3 54.2 33.5 42.9 51.7 42.7 14.8 20 33.5 26.4
F1 CLAFI+RF 3.3 8.1 39.2 26.5 35.6 27.3 22.7 8.4 15.9 22.7 15.4
(M= 16%) FRUGAL 63.6 72.4 62.1 45.6 55.3 45.4 69.1 25.1 58.9 58.9 18.3
SVM [81] (L=2.5%) 0 0 66.4 34.3 27 22.8 72.7 71.1 0 27 50.5
SVM [81] 86.4 100 72.7 84.3 100 71.4 83.1 90 72.8 84.3 20.1
CLA 54.9 88.9 66.8 64.7 83.8 72 69.7 68.5 81.1 69.7 13.7
AUC CLAFI+RF 66.8 89 77.3 69.4 78.4 78 75.2 75.9 84.3 77.3 4.2
(M= 11%) FRUGAL 95.3 94.6 82.5 78.6 90.4 74 97 89.6 94.1 90.4 12.7
SVM [81] (L=2.5%) 77 0 88.3 89.7 87.1 65.6 95.1 75.3 0 77 39.5
SVM [81] 99.5 99.6 97.3 98.8 99.7 98.8 99.6 99.7 99 99.5 0.8
•The SOTA work that was trained on only 2.5% labelleddata (i.e., SVM with L=2.5%) underperforms both
the SOTA work with 100% labelled data and FRUGAL.This illustrates that FRUGAL’s effectiveness is not due torandom sampling of the data.
•In term of labelling efforts, CLA is label-free, FRUGALcosts 2.5%, and Y ang et al.’s method costs 100% becauseFRUGAL and the SOTA require 2.5% and 100% of thedata to be labelled.
FRUGAL improves signiﬁcantly from standard unsu-pervised learner CLA with 2.5% of data labelled asa tradeoff while performing similarly to the SOTAwith 97.5% fewer information. A simple method thatexplores small regions of data does no worse thanmethods that extensively learn the whole space.In summary, our answer to RQ2 is:
RQ3: How well does FRUGAL predict issue close time?
Mark et al.[42] proposed DeepTriage as SOTA deep learning
solution extended from bidirectional LSTMs with an “attention
mechanism” to predict issue close time. A Long Short-TermMemory (LSTM) [27] is a recurrent neural network withan additional “gate” mechanisms to allow the network tomodel connections between long-distance tokens in the input.Bidirectional variants of recurrent models, such as LSTMs,use token stream in both forward and backward directions;allowing for the network to model both previous and followingcontexts for each input token. Attention mechanisms[4] uselearned weights to help the network “pay attention” to tokensthat are more important than others in a context.TABLE V: Comparison between CLA[53], SIMPLE[83],and FRUGAL in terms of Accuracy, FAR, Recall, and AUCfor predicting issue close time. Note that in this table, theFRUGAL results were obtained after labelling just 2.5% ofthe data. Except for FAR, the higher the results the betterthe performance of the treatment. Medians and IQRs(delta between 75th and 25th percentile, lower the better)are calculated for easy comparisons. Here, the
highlighted
cells show best performing treatments.
Metrics Treatment
Chromium
Firefox
Eclipse
Median
IQR
CLA 53.6 57.1 57.6 57.4 2
Accuracy CLAFI+RF 50.2 53.9 51.3 52.6 1.9
(M=3 % ) FRUGAL 65.3 74.2 68.2 68.2 4.5
SIMPLE [83] 70.3 68.3 68.8 68.6 1
CLA 34.9 26.9 37.3 32.1 5.2
FAR CLAFI+RF 35.1 3.1 32.9 18 16
(M=5 % ) FRUGAL 2.2 2 2 2 0.1
SIMPLE [83] 33.1 32.1 22.5 27.3 5.3
CLA 54.9 88.9 66.8 77.9 17
Recall CLAFI+RF 38.3 45.4 38.1 41.8 3.7
(M=8 % ) FRUGAL 99.9 97.9 97 97.9 1.5
SIMPLE [83] 71.7 74.1 54 64.1 10.1
CLA 61 76.4 62.9 62.9 7.7
Precision CLAFI+RF 57.4 72.6 57.9 57.9 7.6
(M=3 % ) FRUGAL 69.1 79.9 69.2 69.2 5.4
SIMPLE [83] 63.5 67 70.4 67 3.5
CLA 51.3 59.1 57.7 57.7 3.9
F1 CLAFI+RF 46 55.9 45.9 46 5
(M=4 % ) FRUGAL 75 82.3 74.8 75 3.75
SIMPLE [83] 63.5 61.4 57.9 61.4 2.8
CLA 58.8 66.4 62.2 64.3 3.8
AUC CLAFI+RF 53.4 60.6 54.2 57.4 3.6
(M=3 % ) FRUGAL 72.1 80.2 75.8 75.8 4.1
SIMPLE [83] 67.3 70.4 65.6 68 2.4
Y edida et al. [83]’s SIMPLE extended basic 1980s’ style
feedforward neural network with state-of-the-art SE’s opti-
402mizer DODGE [1] to automatically select the preprocessors
(normalizer, binarizer, etc) and the neural network model’shyperparamters (num
layers, num units inlayer, batch size).
SIMPLE outperformed DeepTriage and other non-neural net-work methods from Marks et al. [43] and Guo et al. [21].
SIMPLE is employed as the SOTA solution for predicting
issue close time. Y edida et al. [83] only compared solutionsby the accuracy metric. In order to ensure the generalizabilityof our proposed solution, we also compared different methodswith metrics from static warning analysis in RQ2 (FAR, recall,and AUC). Hence, Table V reports the comparison of ourproposed method’s FRUGAL, the SOTA ’s SIMPLE, and thebaseline unsupervised learners (CLA & CLAFI+RF) acrossaccuracy, FAR, recall, and AUC. We observe:
•The unsupervised learners CLA/CLAFI+RF performedworst as it’s default behavior uses clustering based on themedian of the data (which may not apply for all data).While CLAFI+RF performed better than CLA in static codewarnings, that effect was not seen here (i.e. what works forone domain may not work for another). Additionally, onaverage, CLA underperformed SIMPLE by approximately1%, 4%, 4%, 5%, 5%, and 11% in FAR, precision, f1,recall, AUC, and accuracy without access to the train data.Altogether, both points indicate promising areas for tuningconﬁgurations of unsupervised learners.
•FRUGAL outperforms the SOTA ’s SIMPLE as FRUGALwins in recall, precision, f1, AUC, and FAR while draw-ing in accuracy. FRUGAL, on average, improves relativeSOTA ’s precision, AUC, f1, and recall by 9% 14%, 30%,and 52% respectively while reducing FAR by 94% rela-tively.
•In term of labelling efforts, CLA is label-free, FRUGALcosts 2.5%, and the Y edida et al. [83]’s method costs 100%because FRUGAL and the SOTA need 2.5% and 100% ofthe data labelled to execute.
FRUGAL exceeds both standard unsupervised learnerCLA and the SOTA SIMPLE (EMSE’20 [83]which outperformed a decade of research includingICSE’10 [20], PROMISE’11 [43], MSR’16 [31], CO-MAD’19 [42]) in predicting issues close time. FRU-GAL requires only 2.5% of the train data to be labelledwhen being compared against unsupervised learningwhile using 97.5% less information than the SOTAtuned deep learning method. Hence, FRUGAL is notonly effective in static warning analysis, but also inissue close time prediction. The success in both areaslet this study hypothesizes that other areas of SE mayalso beneﬁt from FRUGAL.In summary, our answer to RQ3 is:
VI. T HREA TS OF VALIDITY
There are several validity threats [17] to the design of
this study. Any conclusion made from this work must beconsidered with the following issues in mind:
Conclusion validity focuses on the signiﬁcance of the
treatment. To enhance conclusion validity, we run experimentson 12 different target projects across stratiﬁed sampling (25runs) and ﬁnd that our proposed method always performedbetter than the state-of-the-art approaches. More importantly,we apply a similar statistical testing of Cohen’d as the SOTAwork [83, 81] from the two domains to obtain fair compar-ison. In addition, we have taken into generalization issuesof single evaluation metrics (e.g., recall and precision) intoconsideration and instead evaluate our methods on metrics thataggregate multiple metrics like AUC while being effort-awarevia cost. As future work, we plan to test the proposed methodswith additional analyses that are endorsed within SE literature(e.g., P-opt20 [70]) or general ML literature (e.g., MCC [12]).
One of the possible explanations for the simple effec-
tiveness of both binary split of the output space (CLA/-CLAFI+ML/FRUGAL’s centrality) and 2.5% labelled traindata requirement is highly due to the intrinsic dimensionality.Levina et al. [36] argued that datasets embedded in high-dimensional spaces can be compressed without signiﬁcant in-formation loss (similar to the PCA [44]). To compute Levina’sintrinsic dimensionality, a 2-d plot is created where the x-axisshowsr; i.e. the radius of two conﬁgurations while the y-axis
showsC(r)as the number of conﬁgurations after spreading
out some distance raway from any of ndata instances:
y=C(r)=2
n(n−1)n/summationdisplay
i=1n/summationdisplay
j=i+1I[/bardblxi,xj/bardbl<r] (3)
The maximum slope of lnC(r)vs.lnris then reported as
the intrinsic dimensionality, D. Note that I[·]is the indicator
function (i.e., I[x]=1 ifxis true, otherwise it is 0); xiis
theith sample in the dataset. Applying this calculation to the
12 datasets of two domains (reports in Table VI), we foundthe intrinsic or latent dimensionality (D ) of our data is very
low (median around one, no more than three). Agrawal et al.’sDODGE [1] is the SOTA optimizer for SE, DODGE executesby binary splitting the tuning space, each chop moves in thebounds for numeric choices by half the distance from most dis-tant value to the value that produced the “best” performance.According to Agrawal et al., DODGE’s effectiveness rootsin how the performance score generated from SE data canbe divided into a few regions (low dimensional). FRUGAL’scentral function of binary splitting is similar to DODGEas FRUGAL compresses the data dimensions (features) viaaggregated percentile Cand survey the whole space by varying
C({5% to95% increments by 5%}). Menzies et al. [46] and
Hindle et al. [26] also reported on how several SE data arelow dimensional and the beneﬁts from building effective toolsfrom such data. This work extends those ﬁndings: the labellingefforts to commission to tools building can be reduced greatlybecause of the low dimensionality of SE data.
Internal validity focuses on how sure we can be that the
treatment caused the outcome. To enhance internal validity, weheavily constrained our experiments to the same dataset, withthe same settings, except for the treatments being compared.
403TABLE VI: Summary of intrinsic dimensions (D ) of this
study’s 12 datasets from Levina and Bickel [36].
Static Code Warnings Issue Close TimeDerby
Mvn
Lucene
Phoenix
Cass
Jmeter
Tomcat
Ant
Commons
Chromium
Firefox
Eclipse
D0.78 1.10 0.15 0.62 1.94 1.54 0.73 0.82 1.04 1.95 2.10 1.9
Construct validity focuses on the relation between the
theory behind the experiment and the observation. To enhance
construct validity, we compared solutions with and without ourstrategies in Table IV and V while showing that both compo-nents (unsupervised learning with CLA/CLAFI+ML [53] andtuned semi-supervised method of FRUGAL) and in variousamounts of labelled data required for the proposed mehtod toimprove the overall performance. Moreover, we also bench-marked our solution with the SOTA ’s solution that is trainedon the same L=2.5% to ensure that our proposed solution’s
effectiveness is not due to random sampling of the data.However, we only show that with our default parameterssettings of random forest learner. The performance can geteven better by tuning the parameters, employing differentlearners (e.g., deep learners), and introducing a variety ofdata preprocessors (e.g., synthetic minority over-sampling orSMOTE that is known to help with imbalanced datasets [9]like our static code warnings domains). We aim to explorethese in our future work.
External validity concerns how widely our conclusions
can be applied. In order to test the generalizability of ourapproach, we always kept a project as the holdout test setand never used any information from it in training. Moreover,we have validated our proposed method on two importantsoftware analytics domains: adoptable static code warningsidentiﬁcation and issues close time prediction. Our experi-ments with default CLA/CLAFI+ML [53] demonstrates thedanger of treating all data with the state-of-the-art method,especially when switching domain (from defect prediction toissue close time prediction and adoptable static code warningidentiﬁcation).
VII. C
ONCLUSION AND FUTURE WORK
There is much recent advance for software analytics re-
search with automated and semi-automated methods. However,these methods are built on a sufﬁciently large amount ofdata labelled. Generating such labels can be labor-intensiveand expensive (as discussed in §III). Such requirement canintroduce barrier for entering new research domains (e.g.,the success of open-source projects). In order to reduce thelabel famine and human effort, FRUGAL is recommended.FRUGAL tunes the state-of-the-art unsupervised learner fromdefect prediction (CLA/CLA+ML/CLAFI+ML) and it’s cor-responding percentile parameter Cin the grid search manner
while validating on only 2.5% of the labelled data. Ourﬁndings include:1) Unsupervised Learners without access to the train data’s
labels performed approximately 10% less than the SOTAmethods on average. The results are promising but still noteffective enough.
2) FRUGAL performed similarly to the SOTA adoptable static
code warning identiﬁer while surpassing the SOTA issueclose time predictor with 97.5% less information.
3) FRUGAL reduced the labelling efforts needed for the
software analytics tools by 97.5%. Simply, FRUGAL is40(100% / 2.5%) times cheaper than the SOTA methodsin issue close time and static code warnings analysis areas.
4) The success of FRUGAL for the two domains here suggests
that many more domains in software analytics could beneﬁtfrom unsupervised learning. As mentioned above, thosebeneﬁts include the ability to commission new modelswith less human efforts and costs. By restricting humaninvolvement in the process, we also reduced erroneouslabels that can cascade to the whole research communitysince human are still error-prone (Y u et al. [84] found 98%of the false-positive labels within Maldonado and Shihab[40] were actually true-positive labels).
5) Overall, our proposed method restated the beneﬁt in explor-
ing low dimensional SE data [26, 1, 81, 83] and extendedtheir ﬁndings that the labelling efforts can be reducedgreatly because of the low dimensionality of SE data.
That said, FRUGAL still suffers from the validity threats
discussed in §VI. To reduce those threats and to further thisresearch, we propose the following future work:
•Comparing FRUGAL to other semi-supervised learningmethods within software engineering, e.g., NSGLP [86].
•Test whether replacing the Random Forest model in FRU-GAL with a deep learning model will further improve itsperformance.
•Explore non-SE or high-dimensional SE data with FRU-GAL to see if our current conclusions still hold.
•Apply non-trivial hyper-parameter tuning (e.g., DODGE [1]or FLASH [52]) on various data preprocessors and machinelearners with FRUGAL to test whether tuning can furtherimprove the performance [68, 71].
•Extend the work to other software engineering domains(e.g., security [18], technical debts [40], software conﬁgu-rations [16], etc) and compare it with other state-of-the-artmethods which continue to appear.
A
CKNOWLEDGEMENTS
This work was partially funded by an NSF CISE Grant
#1931425.
REFERENCES
[1] A. Agrawal, W. Fu, D. Chen, X. Shen, and T. Menzies.
How to ”dodge” complex software analytics. TSE, 2019.
[2] S. Amershi, T. Zimmermann, et al. Software engineering
for machine learning: A case study. In ICSE, 2019.
[3] P . Avgustinov, A. I Baars, J. Tibble, et al. Tracking
static analysis violations over time to capture developercharacteristics. In ICSE, 2015.
404[4] D. Bahdanau et al. Neural machine translation by jointly
learning to align and translate. arXiv, 2014.
[5] P . Bhattacharya, M. Iliofotou, I. Neamtiu, and M. Falout-
sos. Graph-based analysis and prediction for software
evolution. In ICSE, 2012.
[6] C. Boogerd and L. Moonen. Assessing the value of
coding standards: An empirical study. In ICSME, 2008.
[7] L. Breiman et al. Classiﬁcation and regression trees.
Cytometry, 1987.
[8] Leo Breiman. Random forests. Machine Learning , 2001.
[9] Nitesh V Chawla, Kevin W Bowyer, Lawrence O Hall,
and W Philip Kegelmeyer. Smote: synthetic minorityover-sampling technique. JAIR, 2002.
[10] Di Chen, Kathyrn T Stolee, and Tim Menzies. Repli-
cation can improve prior results: A github study of pullrequest acceptance. In ICPC, 2019.
[11] W. Chen, S. Tseng, and C. Wang. A novel manufacturing
defect detection method using association rule miningtechniques. ESA, 2005.
[12] D. Chicco and G. Jurman. The advantages of the
matthews correlation coefﬁcient (mcc) over f1 scoreand accuracy in binary classiﬁcation evaluation. BMC
Genomics, 2020.
[13] M. A. de Freitas Farias, R. O. Sp ´ınola, et al. A contex-
tualized vocabulary model for identifying technical debton code comments. In MTD, 2015.
[14] M. A. de Freitas Farias, R. O. Sp ´ınola, et al. Investigating
the identiﬁcation of technical debt through code commentanalysis. In ICEIS, 2016.
[15] M. A. de Freitas Farias, R. O. Sp ´ınola, et al. Identify-
ing self-admitted technical debt through code commentanalysis with a contextualized vocabulary. IST, 2020.
[16] J. Estublier, D. Leblang, A. Hoek, R. Conradi, G. Clemm,
W. Tichy, and D. Wiborg-Weber. Impact of softwareengineering research on the practice of software conﬁg-uration management. TSE, 2005.
[17] R. Feldt and A. Magazinius. V alidity threats in empirical
software engineering research-an initial survey. In SEKE,
2010.
[18] Michael Gegick, Pete Rotella, and Tao Xie. Identifying
security bug reports via text mining: An industrial casestudy. In MSR, 2010.
[19] B. Ghotra, S. McIntosh, and A. E. Hassan. Revisiting the
impact of classiﬁcation techniques on the performance ofdefect prediction models. In 2015 37th ICSE.
[20] E. Giger, M. Pinzger, and H. Gall. Predicting the ﬁx time
of bugs. In RSSE, 2010.
[21] P . J Guo, T. Zimmermann, N. Nagappan, and B. Murphy.
Characterizing and predicting which bugs get ﬁxed: anempirical study of microsoft windows. In ICSE, 2010.
[22] Hideaki Hata, Christoph Treude, Raula Gaikovina Kula,
and Takashi Ishio. 9.6 million links in source codecomments: Purpose, evolution, and decay. In ICSE, 2019.
[23] S. Heckman and L. Williams. A model building process
for identifying actionable static analysis alerts. In ICST,
2009.[24] S. Heckman and L. Williams. A systematic literature
review of actionable alert identiﬁcation techniques forautomated static code analysis. IST, 2011.
[25] A. Hindle, D. M. German, and R. Holt. What do large
commits tell us?: A taxonomical study of large commits.MSR, 2008.
[26] A. Hindle, E. T Barr, Z. Su, M. Gabel, and P . Devanbu.
On the naturalness of software. In ICSE, 2012.
[27] S. Hochreiter and J. Schmidhuber. Long short-term
memory. Neural computation , 1997.
[28] Q. Huang, E. Shihab, X. Xia, D. Lo, and S. Li. Identify-ing self-admitted technical debt in open source projectsusing text mining. EMSE, 2018.
[29] B. Johnson, Y . Song, E. Murphy-Hill, and R. Bowdidge.
Why don’t software developers use static analysis toolsto ﬁnd bugs? In ICSE, 2013.
[30] Y . Kamei et al. A large-scale empirical study of just-in-
time quality assurance. TSE, 2013.
[31] Riivo Kikas, Marlon Dumas, and Dietmar Pfahl. Using
dynamic and contextual features to predict issue lifetimein github projects. In MSR, 2016.
[32] S. Kim, E. J. Whitehead, Jr., and Y . Zhang. Classifying
software changes: Clean or buggy? IEEE Trans SE, 2008.
[33] E. Kocaguneli, T. Menzies, et al. Active learning and ef-
fort estimation: Finding the essential content of softwareeffort estimation data. TSE, 2013.
[34] T. Kremenek, K. Ashcraft, J. Y ang, and D. Engler.
Correlation exploitation in error ranking. In SEN, 2004.
[35] Y . Lee, S. Lee, C. Lee, I. Y eom, and H. Woo. Continual
prediction of bug-ﬁx time using deep learning-basedactivity stream embedding. IEEE Access, 2020.
[36] E. Levina and P . Bickel. Maximum likelihood estimation
of intrinsic dimension. NeurIPS, 2004.
[37] G. Liang, L. Wu, Q. Wu, Q. Wang, T. Xie, and H. Mei.
Automatic construction of an effective training set forprioritizing static analysis warnings. In ASE, 2010.
[38] Robyn R Lutz and In ´es Carmen Mikulski. Empirical
analysis of safety-critical anomalies during operations.TSE, 2004.
[39] Suvodeep Majumder, Pranav Mody, and Tim Menzies.
Revisiting process versus product metrics: a large scaleanalysis. arXiv preprint arXiv:2008.09569, 2020.
[40] E. da S Maldonado and E. Shihab. Detecting and quan-
tifying different types of self-admitted technical debt. InMTD, 2015.
[41] E. da S. Maldonado, E. Shihab, and N. Tsantalis. Using
natural language processing to automatically detect self-admitted technical debt. TSE, 2017.
[42] Senthil Mani, Anush Sankaran, and Rahul Aralikatte.
Deeptriage: Exploring the effectiveness of deep learningfor bug triaging. In COMAD, 2019.
[43] L. Marks, Y . Zou, and A.s E Hassan. Studying the ﬁx-
time for bugs in large open source projects. In ICPS,
2011.
[44] A. Ma ´ckiewicz and W. Ratajczak. Principal components
analysis (pca). Computers & Geosciences, 1993.
405[45] T. Menzies, A. Dekhtyar, J. Distefano, and J. Greenwald.
Problems with precision: A response to ”comments on
’data mining static code attributes to learn defect predic-tors’”. TSE, 2007.
[46] Tim Menzies, David Owen, and Julian Richardson. The
strangest thing about software. Computer, 2007.
[47] T. Mikolov, I. Sutskever, K. Chen, G. S Corrado, and
J. Dean. Distributed representations of words and phrasesand their compositionality. In NeurIPS, 2013.
[48] Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. Efﬁcient estimation of word representations invector space. arXiv preprint arXiv:1301.3781, 2013.
[49] A. Mockus and L. V otta. Identifying reasons for software
changes using historic databases. In ICPC, 2000.
[50] N. Munaiah, S. Kroh, C. Cabrey, and M. Nagappan.
Curating github for engineered software projects. EMSE,
2017.
[51] Vinod Nair and Geoffrey E Hinton. Rectiﬁed linear units
improve restricted boltzmann machines. In ICML, 2010.
[52] Vivek Nair, Zhe Y u, and Tim Menzies. Flash: A faster
optimizer for sbse tasks. TSE, 2017.
[53] Jaechang Nam and Sunghun Kim. Clami: Defect predic-
tion on unlabeled datasets. In ASE, 2015.
[54] C. Ni, X. Xia, D. Lo, X. Chen, and Q. Gu. Revisiting
supervised and unsupervised methods for effort-awarecross-project defect prediction. TSE, 2020.
[55] T. J Ostrand, E. J Weyuker, and R. M Bell. Where the
bugs are. In ISSTA, 2004.
[56] F. Pedregosa, G. V aroquaux, E. Duchesnay, et al. Scikit-
learn: Machine learning in python. JMLR, 2011.
[57] J. Petri ´c, D. Bowes, T. Hall, B. Christianson, and N. Bad-
doo. The jinx on the nasa software defect data sets. InEASE, 2016.
[58] A. Potdar and E. Shihab. An exploratory study on self-
admitted technical debt. In ICSME, 2014.
[59] C. Rosen et al. Commit guru: Analytics and risk
prediction of software commits. EFSE, 2015.
[60] D. E Rumelhart, G. E Hinton, and R. J Williams.
Learning internal representations by error propagation.Technical report, 1985.
[61] S. Sawilowsky. New effect size rules of thumb. Journal
of Modern Applied Statistical Methods, 8:26, 2009.
[62] J ¨urgen Schmidhuber. Deep learning in neural networks:
An overview. Neural Networks, 2015.
[63] Burr Settles. Active learning literature survey. 2009.[64] M. Shepperd, Q. Song, Z. Sun, and C. Mair. Data quality:
Some comments on the nasa software defect datasets.TSE, 2013.
[65] R. Shu et al. Improved recognition of security bugs via
dual hyperparameter optimization. EMSE, 2019.
[66] M S. Silberman et al. Responsible research with crowds:
pay crowdworkers at least minimum wage. Comms ACM,
2018.
[67] J. AK Suykens and J. V andewalle. Least squares support
vector machine classiﬁers. Neural processing letters,
1999.[68] Huy Tu and Vivek Nair. While tuning is good, no tuner
is best. In FSE SWAN, 2018.
[69] Huy Tu, Rishabh Agrawal, and Tim Menzies. The
changing nature of computational science software, 2020.
[70] Huy Tu, Zhe Y u, and Tim Menzies. Better data labelling
with emblem (and how that impacts defect prediction).IEEE Transactions on Software Engineering, 2020.
[71] Huy Tu, G. Papadimitriou, M. Kiran, C. Wang, A. Man-
dal, E. Deelman, and T. Menzies. Mining workﬂows for
anomalous data transfers. In MSR, 2021.
[72] B. V asilescu, Y . Y u, H. Wang, P . Devanbu, and V . Filkov.
Quality and productivity outcomes relating to continuous
integration in github. In FSE, 2015.
[73] Junjie Wang, Song Wang, and Qing Wang. Is there a
golden feature set for static warning identiﬁcation?: anexperimental evaluation. In ESEM, 2018.
[74] I Witten, Eibe Frank, M Hall, and C Pal. : Data mining:
practical machine learning tools and techniques. elsevierinc. 2017.
[75] Ian H Witten, Eibe Frank, Mark A Hall, and Christo-
pher J Pal. Data Mining: Practical machine learning
tools and techniques. Morgan Kaufmann, 2016.
[76] T. Xia et al. Predicting project health for open source
projects (using the decart hyperparameter optimizer),2020.
[77] Z. Xu, L. Li, M. Y an, J. Liu, X. Luo, J. Grundy, Y . Zhang,
and X. Zhang. A comprehensive comparative study ofclustering-based unsupervised defect prediction models.JSS.
[78] Jun Y ang and Hongbing Qian. Defect prediction on
unlabeled datasets by using unsupervised clustering. InHPCC/SmartCity/DSS, 2016.
[79] X. Y ang, D. Lo, X. Xia, Y . Zhang, and J. Sun. Deep
learning for just-in-time defect prediction. In QRS, 2015.
[80] X. Y ang, D. Lo, X. Xia, and J. Sun. Tlel: A two-
layer ensemble learning approach for just-in-time defectprediction. IST, 2017.
[81] Xueqi Y ang, Jianfeng Chen, Rahul Y edida, Zhe Y u, and
Tim Menzies. How to recognize actionable static codewarnings (using linear svms). In EMSE, 2020.
[82] Y . Y ang, Y . Zhou, J. Liu, Y . Zhao, H. Lu, L. Xu,
B. Xu, and H. Leung. Effort-aware just-in-time defectprediction: Simple unsupervised models could be betterthan supervised models. FSE, 2016.
[83] R. Y edida, X. Y ang, and T. Menzies. When simple is
better than complex: A case study on deep learning forpredicting bugzilla issue close time. In EMSE, 2021.
[84] Z. Y u, F. M. Fahid, H. Tu, and T. Menzies. Identifying
self-admitted technical debts with jitterbug: A two-stepapproach. TSE, 2020.
[85] Zhe Y u, Christopher Theisen, Laurie Williams, and Tim
Menzies. Improving vulnerability inspection efﬁciencyusing active learning. TSE, 2019.
[86] Zhi-Wu Zhang, Xiao-Y uan Jing, and Tie-Jian Wang.
Label propagation based semi-supervised learning forsoftware defect prediction. ASE, 2017.
406