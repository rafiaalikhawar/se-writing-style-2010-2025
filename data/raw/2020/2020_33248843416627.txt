A Deep Multitask Learning Approach for Requirements
Discovery and Annotation from Open Forum
Mingyang Li1,3, Lin Shi1,3,‚àó,Y eY a n g4, Qing Wang1,2,3
1Laboratory for Internet Software Technologies,2State Key Laboratory of Computer Sciences,
Institute of Software Chinese Academy of Sciences, Beijing, China;
3University of Chinese Academy of Sciences, Beijing, China;‚àóCorresponding author;
4Stevens Institute of Technology, Hoboken, NJ, USA;
mingyang@itechs.iscas.ac.cn,shilin@iscas.ac.cn,wq@iscas.ac.cn,yyang4@stevens.edu
ABSTRACT
The ability in rapidly learning and adapting to evolving user needs
iskeytomodernbusinesssuccesses.Existingmethodsarebased
on text mining and machine learning techniques to analyze user
commentsandfeedback,andoftenconstrainedbyheavyrelianceonmanuallycodifiedrulesorinsufficienttrainingdata.Multitask
learning(MTL)isaneffectiveapproachwithmanysuccessfulap-
plications, with the potential to address these limitations associ-
atedwith requirementsanalysistasks. Inthispaper,we proposea
deepMTL-basedapproach,DEMAR,toaddresstheselimitations
when discovering requirements from massive issue reports and
annotating the sentences in support of automated requirements
analysis. DEMAR consists of three main phases: (1) data augmenta-
tion phase, for data preparation and allowing data sharing beyond
single task learning; (2) model construction phase, for construct-
ing the MTL-basedmodel for requirements discovery andrequire-
ments annotation tasks; and (3) model training phase, enablingeavesdropping by shared loss function between the two related
tasks.Evaluationresultsfromeightopen-sourceprojectsshowthat,
theproposedmultitasklearningapproachoutperformstwostate-
of-the-art approaches (CNC and FRA) and six common machine
learningalgorithms,withtheprecisionof91%andtherecallof83%
for requirements discovery task, and the overall accuracy of 83%
for requirements annotation task. The proposed approach provides
a novel and effective way to jointly learn two related requirements
analysis tasks. We believe that it also sheds light on further di-
rectionsof exploringmultitasklearninginsolving othersoftware
engineering problems.
KEYWORDS
Requirementsdiscovery,Requirementsannotation,Multitasklearn-
ing, Deep learning
ACM Reference Format:
Mingyang Li1,3, Lin Shi1,3,‚àó,Y eY a n g4, Qing Wang1,2,3. 2020. A Deep Multi-
task Learning Approach for Requirements Discovery and Annotation from
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation
on the first page. Copyrights for components of this work owned by others than ACMmustbehonored.Abstractingwithcreditispermitted.Tocopyotherwise,orrepublish,topostonserversortoredistributetolists,requirespriorspecificpermissionand/ora
fee. Request permissions from permissions@acm.org.
ASE ‚Äô20, September 21‚Äì25, 2020, Virtual Event, Australia
¬© 2020 Association for Computing Machinery.
ACM ISBN 978-1-4503-6768-4/20/09...$15.00
https://doi.org/10.1145/3324884.3416627Open Forum. In 35th IEEE/ACM International Conference on Automated Soft-
ware Engineering (ASE ‚Äô20), September 21‚Äì25, 2020, Virtual Event, Australia.
ACM,NewYork,NY,USA,13pages.https://doi.org/10.1145/3324884.3416627
1 INTRODUCTION
Modernapplicationsbecomeincreasinglydependentonknowledge
learned from user community to improve and innovate services. In
general,suchknowledgerevealshowusersdiscusstheirexperience
with a specific version/feature of the application, and propose feed-
back or new features. Dozens of data sources have been identified
andusedforrequirementsdiscoveryandanalysis,includingchat
messages,emails,forums,projectdigests,etc[ 48].Suchdatasources
contain large amount of textual data. For example, Cleland-Huang
et al.reported that forums such as SourceForge and JIRA typically
consistofthousandsofrequirements[ 8],andVlas etal.reported
that16projectsfromSourceForgeaverage146,716wordsjustinfea-
turerequests[ 63].Itwouldbeanextremelylabor-intensiveprocess
formanualextractionofrequirements.Inaddition,suchtextualdoc-umentsarefrequentlynoisy,consistingoftrivial,non-requirements
information,suchassocialcommunications,slang,typos,etc[ 8].
Suchnoiseneedstobeappropriatelyhandledinordertosupport
efficient and effective requirements discovery and analysis [ 50,62].
Meanwhile,intensiveapproacheshavebeenproposedtosupport
automatic requirements analysis from user comments and issue re-
ports,employingtechniquessuchastextmining,machinelearning,
social network analysis, and feedback mechanisms, etc.
Forthisstudy,wedefinetworequirementsanalysistasks,accord-
ing to the underlying problem addressed in existing studies. One is
requirements discovery (RD), which is a binary-classification task
foridentifyingwhetheranewdocumentisavalidrequirement.Theotherisrequirementsannotation(RA),whichisamulti-labelclassifi-cationtaskforannotatingthesemanticcategoriesofthesentence(s)
within a new document. Most existing RD and RA approaches are
rule-basedorlearning-basedtechniquestoexplorevaluableuserre-quirements.Rule-basedapproachesrelyheavilyonasetofheuristic
rules pre-codified by human experts [ 9]. While, learning-based ap-
proachesemploythemachinelearningalgorithms,suchasRandom
Forest [4] and convolution neural network [ 18], to train model for
future predictions.Two example rule-based RA approaches include
a fuzzy rule-based approach for annotating sentences in featurerequests [
51], and an ontology-based approach consisting of six
levels of classification patterns [62].
There are four major limitations in existing approaches. First,
intheRDtask,existingstudiestypicallymodeldocumentsusing
bag-of-words[ 49]whichvectorizesthetextsundertheassumption
3362020 35th IEEE/ACM International Conference on Automated Software Engineering (ASE)
thatthewordsareindependent.Itignorestheabundantsemantic
information containing in requirements descriptions. Second, in
theRA task,manually-built rulesare labor-intensiveand difficult
to rebuilt across domains in practices. Third, some methods aim
at using supervised training instances on a single task, but un-
fortunately,theyignoretherichcorrelationinformationbetween
thetasks.Forth,thetrainingdataforRDandRAisoftenlimited.
The issue reports in the repositories are typically large in size. But
only a few labeled ones are categorized into requirements types.How to make the maximum use of the few labeled requirements
toaccuratelyclassifytheunlabeledissuesbecomesanimportant
problem. To address these challenges, this study proposes to adopt
deepmultitasklearning(MTL) approach, whichcombinesthedeep
learning and MTL techniques, which has led to successes in many
applicationsofmachinelearning,fromnaturallanguageprocess-
ing[55],speechr ecognition[ 42],tocomputervision[ 70].MTLis
a subfield of machine learning in which multiple learning tasksare solved at the same time, while exploiting commonalities and
differences across tasks [ 44]. This can result in improved efficiency
and prediction accuracy, when compared to training the models
separately [6, 58, 72].
Inthispaper,wepresentadeepMTL-basedapproach,DEMAR
(DEepMultitask Le Arning for Requirements Discovery and An-
notation),toautomatedsupportforRDandRA.DEMARconsists
of three main phases: (1) data augmentation phase, for data prepa-
rationandallowingdatasharingbeyondsingletasklearning;(2)
modelconstructionphase,for constructingtheMTL-basedmodel
forrequirementsdiscoveryandrequirementsannotation;and(3)
model training phase, enabling eavesdropping by shared loss func-
tionbetweenthetworelatedtasks.Evaluationresultsfromeight
open-source projects show that DEMAR outperforms two state-of-the-art approaches (CNC and FRA) and six common machinelearning algorithms, with the precision of 91% , the recall of 83%
forRDtask,andoverallaccuracyof83%forRAtask.Specifically,
by jointly learning the two tasks, the performance of the discovery
task increases 4% of F1, and the annotation task increases 8% ofaccuracy. In summary, the key contributions of this paper are: 1)
DEMAR, a novel paradigm to support multitask learning with two
relatedrequirementsanalysistasks,includingdataaugmentation,
MTL-based model construction, and MTL-based model trainingphases. 2) A novel deep MTL-based model to enable sharing in-
formation through shared layers and task-specific layer for jointly
learningRDandRAtasks;3)AdemonstrationofthepowerofMTL
in combination with deep learning for RD and RA tasks; 4) The
publiclyaccessiblematerials1tofacilitateitsapplicationinother
contexts.
2 BACKGROUND
This section introduces a motivational example, and the related
techniques used in our study.
2.1 A Motivational Example
In practices, a typical requirements process in online open-source
communities goes like the following. First, project contributors
1https://github.com/DEMAR-requirements/DEMARmanuallybrowsethroughthelistofnewlyopenedissuesandas-
signa‚Äúfeaturerequest‚Äùor‚Äúenhancement‚Äùlabelwhendiscoveringa
new requirement. This manual process is very labor-intensive and
time-consuming. The RD task aims at predicting whether a newissue is a requirement or not, which can expedite the process of
issuelabeling.Second,givenalistoflabeledissuesorrequirements,
Release team review, annotate, and integrate important sugges-tions from a requirement to a future release. The RA task aimsat expediting the process of requirements understanding by pro-
viding semantic annotations at sentence level. Thus, the release
team memberscan obtain an initialunderstanding of theunstruc-
tured textual artifacts quickly. They could pay attentions to the
relevant information while ignoring the less important parts from
thesubmittedartifacts[ 51].Figure1illustratesanexampleofthe
tworelatedRDandRAtasksonasetofnewlyopened,unlabeled
issue reports in Keras project from Github. The RD task focuseson the entire issue report, and the output is true or false. Mean-
while, the RA task focuses on the sentences in each issue report,
and the output is a set of high-level annotations for each sentence,
characterizing whether each sentence depicts the intent, benefit,
example,explanation,drawback,or triviaaspectofarequirement.
Generally speaking, sentences with ‚Äúintent‚Äù and ‚Äúbenefit‚Äù anno-tations reflect more essential requirements, and sentences with
‚Äútrivia‚Äùannotations maybe largelyignored. As shownin Figure1,
the identified requirement from the RD task corresponds to seven
more sentence-level annotation labels produced from the RA task,
with the 2ùëõùëëand 6ùë°‚Ñésentences highlighting the major intents of
this requirement.
Figure 1: An Example of RD and RA
Asintroducedearlier,existingRDandRAmethodsmostlyem-
ploy rule-based or learning-based approaches to resolve these two
tasksindependently.Butunfortunately,theyignoretherichcorrela-tioninformationbetweentasksandareoftenconstrainedbylimited
amountsoftrainingdata.Infact,theRDtaskisrelatedtotheRA
task as they share the same inputs of the textual requirements, and
theyhavesharedinformationrelevanttoprediction.Specifically,
the sequences of sentence annotations may comply with certain
patterns when expressing requirements, which can benefit RA task
of judging whether an issue report is requirement or not. Takenthe requirement in Figure 1 as an example, the user first writes
337an intent-sentence to directly express a request, and then gives
someexplanationsaswellaswhatthebenefitstherequestcould
deliver. Such ‚Äúintent-explanation-benefit‚Äù pattern might largely
exist in requirements rather than in non-requirements like bug re-
ports. Conversely, the category information of an issue report may
also benefit to determining the annotations of its sentences. For
example,if anissuereport ispredictedas arequirement,it would
bemorelikelytocontain‚Äúintent‚Äùannotation.Ifanissuereportis
predictedasanon-requirementreport,itwouldbemorelikelyto
contain ‚Äúdrawback‚Äù and ‚Äúcurrent behavior‚Äù annotations. Taking all
the above into consideration, we make the first attempt to propose
anMTL-basedapproachthatcanjointlylearnthetwotasks,aiming
to improve learning efficiency and prediction accuracy for both
tasks.
2.2 Multitask Learning
Multitasklearning(MTL)isawidely-usedapproachtoinductive
transfer, which can be used in combination with machine learning
algorithms. Given ùëörelated learning tasks, MTL takes the correla-
tioninformationintoconsideration,andbooststheperformance
of each task by sharing the knowledge contained in the ùëötasks
[7,72]. It achieves this by learning multiple tasks in parallel while
using a shared representation between the related tasks to produce
models with better generality.KLGGHQUHSUHVHQWDWLRQ7DVN$ 7DVN% 7DVN&96
96
96&RQYHUJH Õæ<
16KDUHG5HSUHVHQWDWLRQ
OD\HUV7DVNVSHFLILF
OD\HUV
-RLQW
/RVV
Figure 2: Paradigm of MTL with hard parameter sharing
Inthecontextofsuperviseddeeplearning,therearetwocommon
waystoperformMTL: hardparametersharing andsoftparameter
sharingofhiddenlayers.MTLwith hard parameter sharing ,as
illustratedinFigure2,hasbeenwidelyusedinmanyapplications
such as natural language processing [ 55], speech recognition [ 42],
and computer vision [ 70], etc. This paradigm consists of two parts
in constructing the network, i.e., shard presentation layer(s) and
task-specificlayer(s).Throughthesharedrepresentationlayer(s),
the general hidden vectors for input could be obtained. The hidden
vectors areused as the inputof the task-specific layer(s)for many
related tasks. After that, the shared information among tasks is
learned by optimizing the model parameters using joint loss. After
several iterations, the training stops until the model parameters
stabilize.AnotherparadigmisMTLwith softparametersharing ,
where each task has its own model with its own parameters. Since
the RA and RD tasks share the same inputs of the textual issuereports, they are naturally suitable to the hard parameter mode
that sharing the same representation layer(s) and process multiple
tasks in the task-specific layer(s). Therefore, we choose the hard
parameter sharing to jointly learn the RD and RA tasks.
3 APPROACH
As shown in Figure 3, the proposed approach, DEMAR, consists
of three main phases: (1) data augmentation phase, for data prepa-
rationandallowingdatasharingbeyondsingletasklearning;(2)
modelconstructionphase,for constructingtheMTL-basedmodel
forrequirementsdiscoveryandrequirementsannotationtasks;and
(3) model training phase, enabling eavesdropping by shared loss
function between the two related tasks. We will present details on
each of the three phases next.
3.1 Data Augmentation
Inthisstep,eachissuereportwillbeintroducedwithaugmented
information beyond the scope of single task learning.
3.1.1 Data Pre-processing .Atypicalissuereportmaycontain
two types of information: textual content of the issue (i.e., title and
description), and embedded source information (e.g., code snippets,
patches,stacktraces).Inthisstudy,wefocusonthetextualcontents
toavoidnoiseassociatedwiththeembeddedsourceinformation.
Specifically,thefollowingstepsareconductedtopre-processthe
issuereportdata:(1)Retainingtextualcontentsandexcludeembed-
ded source information using a text-parsing tool [ 36]; (2) Splitting
issuedescriptiontextintoindividualsentences;and(3)Applying
standarddatacleaningpipelineincludingspecialcharacterremoval,
tokenization, and lowercase conversion. After data pre-processing,
eachissuereport,denotedas ùê∑,isconsideredasaninstanceforthe
learning tasks.
3.1.2 Instance Label Sharing .Designed based on supervised
learning, each issue report, ùê∑, comes with ground truth labels
for RD and RA tasks, respectively. While RD task is a binary-
classificationtask(i.e.,Yes/No),DEMARalsoemploysasetofsevensentencecategoriesforRAtask,assummarizedinTable1.Thefirst
six are adopted from on Shi et al.‚Äôs work [ 51], which are identified
from practice guidelines [ 66]. Other than the six categories, a new
category of ‚Äúcurrent behavior‚Äù is introduced, and highlighted in
boldinTable1.Thereasonforthisadditionisthat,whilethema-
jorityofShi etal‚Äôscategoriesandtheirfuzzyrulesarestilleffective,
refinementontheexplanationcategoryisneededtohighlightmore
semantic meanings in requirements.
At the outset, DEMAR allows to share instance labels across
related tasks. Thus, the issue data is explicitly augmented in the
following two aspects:
‚Ä¢Augmented RD data. By sharing labels, the data used to
traintheRDmodelisautomaticallyaugmentedbyincluding
additional information on each sentence. The original RD
dataistheplainissuereports,andtheaugmentedRDdata
is the issue report with its sentences being tagged to one of
the seven categories in Table 1.
‚Ä¢Augmented RA data. Similarly,thedatausedtotrainthe
RA model is augmented by adding the category information
of the issue report that each sentence belongs to.
338Figure 3: An Overview of DEMAR
Table 1: Description of RA categories.
Category Definition
Intent(INT) Ideas or needs to improve system functionalities.
Benefit (BE) Results or effects if a proposed feature is deliver.
Drawback (DR) Disadvantages or negative aspects of the current system.
Example (EXA) Examples or references in support of a proposed feature.
Explanation (EXP) Detailed information about the proposed feature.
Trivia (TRI) Information not directly related to a feature or system.
Current Behavior (CB) The current behavior of a system.
3.2 Model Construction
As illustrated in Figure 3, the model construction phase starts with
a mini-batched sampling component, and a two-step MTL com-
ponent, i.e.,shared layer learningand task-specific layerlearning,
to train the MTL-based model. An elaborated architecture of the
MTL-based model is shown in Figure 4. There lower layers - thesentence embedding layer and the context embedding layer- are
thegloballysharedlayersforRDandRAtasks.Thesharedlayers
takeinputsfrom themini-batchedsamplingcomponent,andpro-
duce the hidden representation for each input sentence. Then, the
task-specific layer is to map the hidden representation into two
groups of outputs. Next, we introduce the details of this phase.
3.2.1 Mini-Batched Sampling .Mini-batching is a very popular
technique in machine learning, due to its ability to boost perfor-mances and accelerate the training process [
39]. Following this
convention,DEMARemploysmini-batchedsamplinginitstraining
process. Rather than sending one training instance into the model
atatime,DEMARequallysplitsallthesamplesintobatcheswith
the same size (batch_size). All the training samples are divided
into‚åàùëõùë¢ùëöùëèùëíùëü ùë†
ùëèùëéùë°ùëê‚Ñé_ùë†ùëñùëßùëí‚åâbatches,where ùëõùë¢ùëöùëèùëíùëü ùë†isthenumberofallthe
trainingsamples.Moredetailsonthetuningofthebatch_sizepa-
rameter will be presented in the later section (Section 3.3.2). For
one training iteration, each batch containing batch_size instances
is sent into the model for training, which could avoid the bias of a
single instance.3.2.2 Shared Layer Learning .The shared layers consist of two
layer:(1)sentenceembeddinglayerwhereDEMARembedseach
sentencetoasemanticvector;(2)contextembeddinglayerwhere
DEMAR encodes the contextual information for each sentence.
Following introduces two layers in detail.
SentenceEmbeddingLayer :Thislayeraimstoembedeachsen-
tenceintoasemanticvector.Foragivenissuereport,wedenoteallsentences in it as [
ùë†1,ùë†2, ...,ùë†ùëõ](ùëõis the number of sentences in the
issue report). Traditionally, a sentence is vectorized using a vector
spacemodel[ 47],andrepresentedbytheweightedwordswithin
thesentence.Inthisway,thedimensionofthesentencevectoris
thevocabularysize.However,duetothelargesizeofvocabulary,
itisveryexpensivetolearnsuchkindofmodels.Unlikethetradi-
tional vector space model, DEMAR introduces a pre-trained BERT
model[19]toembedthesentence.TheBERTmodelusedinDEMAR
hasbeenpre-trainedonalargevolumeofnaturallanguagetexts
(BooksCorpus [ 73] containing 800M words and English Wikipedia
Figure 4: The MTL-based model
339containing 2,500M words), and has been successfully adopted in
manynaturallanguageprocesstaskssuchastextclassification[ 32],
automaticquestionanswering[ 20]andnamedentityrecognition
[53]. Given a sentence, the BERT model returns the corresponding
sentence vector with the dimension of 768 (the default value in the
BERT model), which carries the semantic information. Please note
that there are different sizes of sentences in issue reports, which
leads to different lengths of instances. To ensure the consistency of
inputlength,DEMARuses paddingandtruncation toaligninput
length,whichisoneofthecommonpracticesindeeplearning[ 12].
Due to 90% sampled issue reports (see Section 4.2 for details) have
lessthan10sentences,wesetthefixedinputlength ùëõas10.Ifan
instancehaslessthan10sentences,DEMARaddszerovectorsto
theendoftheinstanceforpadding.Ifithasmorethan10sentences,
DEMAR truncates the ending sentences.
Context Embedding Layer : The purpose is to encode the contex-
tual information of each sentence. Existing rule-based approaches
typically capture high-level contextual information and express
them using rules, derived based on direct observation from empiri-
caldata orexpertexperience. Forexample,if asentencedescribes
an intent, the next sentence is likely to belong to the ‚Äúbenefit‚Äù. On
thecontrary,foranissuereport,ifan‚Äúintent‚Äùisfollowedby‚Äúbene-
fit‚Äùsentence(s),itislikelyarequirement.Withtheconsideration,
weemployaBi-LSTMlayertoembedthecontextualinformation
and enable richer information sharing in MTL. Given the semantic
vectors produced by the sentence embedding layer, the context
embedding layer learns and converts them into the corresponding
vectorsequence [‚Ñé1,‚Ñé2....,‚Ñé ùëõ],where‚Ñéùëñisthehiddenrepresenta-
tionfortheinputsentence ùë†ùëñ.Thedimensionofthesehiddenvector,
denotedas LSTMdimension,isakeyhyper-parameter.Moredetails
of tuning this hyper-parameter are presented in Section 3.3.2. The
shared hidden representation is used as the input for the following
task-specific layer learning.
3.2.3 Task-Specific Layer Learning .As shown in Figure 4, the
task-specific layer is to map the hidden vector representations
produced by the shared layers into two groups of classification
outputs, i.e., predictions for the RD and RA tasks respectively.
ClassificationfortheRDtask. AsRDisabinary-classificationtask,
DEMAR employs an fully connected (FC) layer, which is popularly
usedasthefinallayerindeeplearningforclassificationtasks[ 24,
26,71].TheFClayertakesthehiddenvector,i.e.,[ ‚Ñé1,‚Ñé2,...,‚Ñéùëõ]as
the input, and outputs 2-dimension vector corresponding to the
two values, indicating whether the issue report is a requirement or
not.Then,usingthesoftmaxfunction[ 75],DEMARnormalizesthe
2-dimensionvectorintoaprobabilityscore ùëÉ(ùëÖ|ùê∑),andclassify
whether the issue report ùê∑is a requirement.
ClassificationfortheRAtask. Asintroducedearlier,RAisamulti-
label classification task to assign each sentence into the annotation
categories, as illustrated in Table 1. Similarly, DEMAR leveragesthe FC layer to map each hidden representation into one of the
seven categories. The output of the FC layer is a vector [ ùëÉ(ùê¥1|ùê∑),
ùëÉ(ùê¥2|ùê∑), ...,ùëÉ(ùê¥ùëõ|ùê∑)], whereùëÉ(ùê¥ùëñ|ùê∑)(1‚â§ùëñ‚â§ùëõ) is also a
vector of probability across all seven sentence categories for an
inputsentence ùë†ùëñoftheissuereport ùê∑.Theprobabilityvector ùëÉ(ùê¥ùëñ|
ùê∑)is then processed through the softmax function to generate the
corresponding sentence labels of the issue report ùê∑.3.3 Model Training
3.3.1 Eavesdropping by shared loss function .Oneofthemain
advantagesofMTListhatitallowsthemodeltoeavesdropwhen
learning fromrelated tasks.For theRD task,the lossis calculated
by the cross-entropy of using the following equation:
ùêøùëúùë†ùë† ùëÖùê∑=‚àíùë¶ùëñ¬∑log(ùëÉ(ùëÖ|ùê∑)) (1)
whereùë¶ùëñistheground-truthrequirementslabelforeachinstance
ùê∑, andùëÉ(ùëÖ|ùê∑)is the predicted probability of the issue report
being a requirement.
Similarly, the loss of the RA task is calculated by the average
cross-entropy using the following equation:
ùêøùëúùë†ùë† ùëÖùê¥=1
ùëÅùëÅ/summationdisplay.1
ùëó=1‚àíùë¶ùëó¬∑logùëÉ(ùê¥ùëó|ùê∑) (2)
whereùëÅis the number of sentences in the instance ùê∑(covering
requirements and non-requirements issue reports), ùë¶ùëóis the true
sentence label of the sentence ùë†ùëó,ùëÉ(ùê¥ùëó|ùê∑)is the predicted classi-
fication of each sentence ùë†ùëó. Then DEMAR employs a principled,
sharedlossfunctiontotaketheweightedsumofthetwoindividual
losses, using the following equation:
ùêøùëúùë†ùë†=ùúÜùêøùëúùë†ùë† ùëÖùê∑+(1‚àíùúÜ)ùêøùëúùë†ùë† ùëÖùê¥ (3)
whereùúÜisharmonicparameterintherange(0,1).Inthisstudy,the
hyper-parameter ùúÜisdeterminedbytheautomatichyper-parameter
tuningintroducedinSection3.3.2.Withthelossfunction,thetrain-ingisconductedinaniterativemanner.Ineachiteration,bysharing
the loss,one task could eavesdropthe information fromthe other,
and learn some features which are difficult to learn from itself but
easy from the other task.
Then,allmini-batchesarefedintothemodelonebyone,andthe
lossiscalculatedusingthelossfunction ùêøùëúùë†ùë†.Next,withtheloss
ùêøùëúùë†ùë†beingbackpropagatedthroughthemodellayers,themodel
parameters in the task-specific layer and context embedding layer
areupdated usingtheStochasticGradient Descentalgorithm[ 41].
Thetrainingprocessrepeatstheabovestepsandstopsiftheloss
function does not decrease obviously on 10 consecutive batches.
3.3.2 Hyper-parameter tuning strategy .Previousstudieshave
shownthathyper-parametertuningisimportantforaprediction
modeltoachievebetterperformance[ 10].Therearethreehyper-
parameters in DEMAR, i.e., the dimension of the hidden vectorin MTL model (refer to Section 3.2.2), batch_size (refer to Sec-tion 3.2.1), and the harmonic factor
ùúÜfor the loss function (refer
to Section 3.3.1). DEMAR adopts a greedy strategy to tune these
hyper-parameters.Given ùëõhyper-parameters{ ùëÉ1,ùëÉ2,...,ùëÉ ùëõ}andthe
corresponding ùëõsetsof candidatetuning values{ ùëâ1,ùëâ2,...,ùëâ ùëõ},we
performùëõiterations of automated tuning. In each iteration, we
randomly select one ùëÉùëñwithout replacement, enumerate all its cor-
respondingcandidatevalues ùë£ùëóinùëâùëñ={ùë£1,ùë£2,...,ùë£ ùëö},choosethe
relatively good performances, as the determined value of ùëÉùëñ. More
details and results on hyper-parameter tuning will be presented in
Section 4.3.3 and Section 5.3, respectively.
4 EXPERIMENTAL DESIGN
Thissectiondescribestheresearchquestions,thesubjectdataset,
thedesignedexperimentstobeconducted,thebaselineapproaches,
340andthemetricsforperformancemeasurement,inordertoverify
the performance of the proposed DEMAR approach.
4.1 Research Questions
Three research questions are formulated as following:
RQ1:What are the benefits of DEMAR compared to sin-
gle tasklearning? This question aims at evaluating the effective-
ness of the multitask learning approach in requirements discovery
and annotation. Specifically, in order to investigate the benefits of
multitasklearning approach,weconduct experimentstocompare
DEMAR with its two single task modes, respectively.
RQ2:How does the performance of DEMAR compare to
existingapproaches? Thisquestionaimsatevaluatingtheadvan-
tagesofDEMAR.Specifically,wecompareDEMARwithexisting
state-of-the-art approaches, as well as a set of widely-adopted ma-
chine learning classifiers.
RQ3:How sensitive is the performance of DEMAR to the
hyper-parameter values? This question aims at evaluating the
performanceconcerningdifferentvaluesofitshyper-parameters,
andtuningtowardsimprovedperformance.Specifically,weemploy
agreedystrategyfor automatichyper-parametertuning,inorder
to determine optimal values of hyper-parameters for achieving
promising results.
4.2 Dataset
The dataset used in this study contains issue report data extracted
from eight open-source projects, as summarized in Table 2. Specifi-
cally, we construct our RD and RA datasets and label ground truth,
through the following steps: (1) For each project, we randomly
retrievetheequalamountofpositiveinstances(issuereportswhichare labeled as ‚Äúfeature request‚Äù or ‚Äúenhancement‚Äù in project reposi-
tories)andnegativeinstances,andremovetheinstanceswithout
any natural language descriptions; (2) We manually inspect thelabels, and correct 29 mis-classified requirements from negative
instancesintopositiveinstances;(3)Foreachissuereport,weman-
ually label the sentences with the pre-defined seven categories. To
guaranteethecorrectnessofthelabelingresults,aninspectionteam
with two senior researchers and two PhD students jointly work
inthisprocess.Oncedifferentlabelingopinionsarise,thefinalre-
sult is determined based on a team discussion and majority voting
mechanism.Intotal,thereare1,067issuereports(IR),including573
requirements (R), and 494 non-requirements (NR) issues in the RDand RA dataset. The RA dataset includes the sentences taken from
the 1,067 issue reports, as well as their corresponding categories
introduced earlier in Table 1.
Table 2: The Statistics of RD Dataset and RA Dataset
project #IRRDdataset RAdataset
#R/NR #INT#CB#BE#DR#EXA#EXP#TRI
ActiveMq 125 65/60 568016181311629
Archiva 83 42/41 6563386707
Aspectj 191 97/94 12614722201930144
HDFS 162 104/58 11290342011132 8
Hibernate ORM 175 89/86 108991861325921
Log4j 120 61/59 67841863215015
Mopidy 114 65/49 565518315719
SWT 97 50/47 507445912012
TOTAL 1067 573/494 640692133861181219 1454.3 Experiment Design
4.3.1 Benefit analysis of MTL (RQ1). To answer RQ1, we first con-
figureDEMARintothreeworkingmodes.Oneismultitasklearningmode(i.e.,DEMAR).Twoareitssingletasklearningmodes,i.e.,sin-
gle RD task ( ùê∑ùê∏ùëÄùê¥ùëÖ ùëÜùëÖùê∑) and single RA task ( ùê∑ùê∏ùëÄùê¥ùëÖ ùëÜùëÖùê¥). Then,
we conductexperiments tocompare theperformances among the
threemodes.Totrainthetwosingletasklearningmodes,weuse
thesingletaskviewofthedataset,i.e.,RD-viewwithonlyRDlabels
andRA-viewwithonlyRAlabels,totrainthetwocorresponding
ùê∑ùê∏ùëÄùê¥ùëÖ ùëÜùëÖùê∑modeland ùê∑ùê∏ùëÄùê¥ùëÖ ùëÜùëÖùê¥model,respectively.Then,we
conduct 10-fold cross validation on the RD and RA dataset. Finally,
the performances are compared among the three modes to investi-
gatewhethermultitasklearningmodeoutperformeithersingletask
learning mode. Note that, for the RA task, the 10-fold cross valida-tion is conducted on the whole dataset, i.e., including sentences in
bothrequirementsandnon-requirementsissues.However,inorder
to evaluate performances on RA task, we only calculate the evalua-
tionmetricson thesubsetofsentencesin requirements.Thisisto
avoidtheside-effectRApredictionofDEMARonnon-requirements
instances, and we will discuss more in Section 6.2.
4.3.2 Baseline Comparison (RQ2). ToanswerRQ2,weselectasetof
eight comparison baselines, including two state-of-art approaches,
i.e.,CNN-basedClassifier(CNC)andRule-basedClassifier(FRA),
asprimarybaselines,andsixpopulartextclassificationapproaches
to compare their performances with DEMAR. Specifically, CNCis used to compare the performance of DEMAR in the RD task,
and FRA is used to compare in RA task. In addition, we implement
sixadditionalclassifierstosupportbothRDandRAtasks,sothat
they can be used to compare with DEMAR in both RD and RAtasks. The details of the baselines are presented in Section 4.4.
The performance for each baseline is evaluated using 10-fold cross
validation. Similar to RQ1, in order to evaluate performances on
the RA task, we only calculate the evaluation metrics on the subset
of sentences in requirements.
4.3.3 Hyper-parameter Sensitivity Analysis (RQ3). Asintroduced
inSection3.3.2,therearethreehyper-parametersinDEMAR.To
answer RQ3, we design a step-wise tuning experiment to automati-
cally tune their values respectively. First, we tune the dimensionof hidden vector, with the other two hyper-parameters fixed at
default values (i.e., 64 for batch_size and 0.5 for ùúÜ). Specifically, we
enumerate the dimension of hidden vector ‚Ñéùëñwith values in the
range of [32, 64, 96, 128, 160, 192, 224, 256, 512, 1024], and train
themodel.Amongallthecandidatedimensions,wechoosetheone
whose corresponding model achieves the best performance on the
test set. Second, tune the batch size hyper-parameter. Following a
similar enumeration approach, we experiment each value in the
set [1, 2, 4, 8, 16, 32, 64, 128, 256, 512], with the optimal dimension
of hidden vector identified from the previous step, as well as the
default value of ùúÜ. Among all the candidate batch sizes, we choose
theonewhosecorrespondingmodelachievesthebestperformance
onthetestset.Last,tune ùúÜ.Theoptimalvaluesfortheothertwo
hyper-parameters are applied in this step, and we experiment val-
uesofùúÜfromtherangeof0.1to0.9withanincreaseddeltaof0.1
in each round. The value which leads to a corresponding model
that achieves the best performance will be selected as the optimal
341value. Following this greedy strategy, the performance of DEMAR
under different hyper-parameter combinations is evaluated, and
the hyper-parameter values producing the best performance are
selected and used in DEMAR.
4.4 Baselines
ToevaluatetheperformanceofDEMAR,wechoosethefollowing
approaches as comparison baselines.
CNN-based Classifier (CNC) . This is the latest state-of-art
approach for intent mining, proposed by Huang et al.[18]. CNC
is based on the convolution neural network (CNN) to automati-
callyclassifysentencelabeltobeoneofsevenpre-definedintent
categories (‚ÄúInformation Giving‚Äù, ‚ÄúInformation Seeking‚Äù, ‚ÄúFeature
Request‚Äù,‚ÄúSolutionProposal‚Äù,‚ÄúProblemDiscovory‚Äù,‚ÄúAspectEvalua-tion‚Äù,and‚ÄúMeaningless‚Äù).Ifasentenceisclassifiedintothe‚ÄúFeature
Request‚Äùcategory,theissuereportwhichthesentencebelongsto
isconsideredasarequirement.Itisusedastheprimarybaseline
forcomparisonintheRDtask.Inourexperiment,wepredictthe
label of each sentence using the trained model [ 59], and classify an
issuereportasarequirementifatleastoneofitssentenceshasa
label of ‚ÄúFeature Request‚Äù.
Rule-basedClassifier(FRA) .Thisisthestate-of-artapproach
to requirements annotation, proposed by Shi et al.[51]. FRA is
built upon a fuzzy method and natural language processing to
automatically classify the content of feature requests according to
sixpre-definedcategories,including‚ÄúIntent‚Äù,‚ÄúBenefit‚Äù,‚ÄúDrawback‚Äù,
‚ÄúExample‚Äù, ‚ÄúExplanation‚Äù, and ‚ÄúTrivia‚Äù. It is used as the primary
baselineforcomparisonintheRAtask.WeapplytheFRAclassifier
by using the source code provided by the authors.
Machine-learning-basedClassifiers. Besides the above two
primary baselines, we additionally introduce six text classification
algorithms as baselines to provide more comprehensive perspec-
tivesofcomparison.Theseclassificationalgorithmsarealsowidely
used in previous studies [ 13,18,23]. The six classification algo-
rithms are applied for RD and RA tasks separately. Logistic Regres-
sion (LR) [28] is a statistical method that is employed to conduct
appropriate regression analysis when the dependent variable is
binary.NaiveBayesian(NB) [33]isasimplegenerationmodelfor
textclassificationbasedonbag-of-wordsassumptionsandBayesian
rules. It conducts the joint probability ofa sentence through prior
probabilityandconditionalprobabilitylearnedbythemodeland
trainingdata.Then,givenasentence,itcandeducetheprobabili-
tiesofallthetaxonomies. RandomForest(RF) [30]isanensemble
machine learning method that is constructed with several trees,
andeachtreecancontributetothefinalclassificationresult. Sup-
port Vector Machine (SVM) [60] is a supervised machine learning
algorithm that can be used for both classification and regression
purposes,whichisanon-binarylinearclassifier. TextCNN [25]is
the convolutional neural network for text classification, and it is a
useful deep learning algorithm for many natural language process
tasks such as sentiment analysis and question classification. Tex-
tRNN[26] is the recurrent neural network for text classification,
where the connections between nodes form a directed graph along
a text sequence.4.5 Evaluation Metrics
We use four commonly-used measurements to evaluate the per-
formance, i.e., Accuracy, Precision, Recall, F1. (1) Accuracy is the
proportion of sentences which are correctly classified among allsentences. It is used as the overall evaluation metric for the RA
task.(2)Precision,whichreferstotheratioofthenumberofcorrect
predictions to the total number of predictions; (3) Recall, which
refers to the ratio of the number of correct predictions to the total
numberofsamplesinthegoldentestset;and(4) F1,whichisthe
harmonic mean of precision and recall.
5 RESULTS
This section presents the evaluation results in order to answer the
research questions.
5.1 Benefit of MTL (RQ1)
Figure 5(a) illustratesthe performance comparisonbetweenDE-
MARandits singletasklearningmode( ùê∑ùê∏ùëÄùê¥ùëÖ ùëÜùëÖùê∑)forRD task,
in terms of precision, recall, and F1. It is clear that DEMAR outper-
formsitssingletaskmodeacrossallmetrics.Theimprovementson
theprecision,recall,andF1-scoreare5%,4%,and4%respectively.
These increases are directly attributed to the benefits associated
with the internal jointly learning setup of DEMAR, i.e., learning
RD task simultaneously with a counterpart RA task.

3UHFLVLRQ 5HFDOO )'(0$5 '(0$565'
(a)
$FF ,17 &% %( '5 (;3 (;$ 75,
)'(0$5 '(0$565$
(b)
Figure 5: Comparison of different learning modes
Figure 5 (b) shows the performance comparison results between
DEMAR and its single task learning mode, i.e., ùê∑ùê∏ùëÄùê¥ùëÖ ùëÜùëÖùê¥.A s
introduced earlier, the RA task is a multi-label classification task
containingsevencategoriesofsentences.Therefore,wecompare
theoverallaccuracy(Acc.)andF1acrossthesevensentencecate-
gories,asindicatedinthehorizontalaxis.Significantincreasesin
the performanceof DEMAR canbe observedin 4 ofthe 8 metrics,
i.e., accuracy and F1 in three categories including "Intent (INT)",
"Current Behavior (CB)" and "Benefits (BE)". More specifically, the
overall accuracy is increased by 7.7%, i.e., from 75.6% to 83.3%, and
the F1 in the three categories including "Intent (INT)", "Current
Behavior(CB)"and"Benefits(BE)"areincreasedby14%,12%,and
10%,respectively.Comparedtothethreecategories,theF1values
forthe "Explanation(EXP)" and"Example(EXA)"categorieshave
342less improvement, and remains the same for the other two cate-
gories,i.e.,"Drawback(DR)"and"Trivia(TRI)".Thereasonofbetter
performancesinINT,CB,andBEcategoriesisthat,thesecategoriesaremorediscriminativebetweenrequirementandnon-requirementreports.Thus,incorporatingthepredictionresultsfromtheRDtask
would contribute potential confidences to annotate those tags in
the RA task by joint learning.
Answers to RQ1: Compared with its alternative single task
learningmodes,themultitasklearningmodeofDEMARcanachieve
higher performance. On average, DEMAR increases the perfor-manceofthesingleRDtaskby4%inF1,andtheperformanceof
the single RA task by 8% of in overall accuracy.
5.2 Baseline Comparison (RQ2)
As introduced in Section 4.4, two sets of baseline approaches are
selectedforcomparingwiththeproposedDEMARinRDandRA
tasks. We will present the comparison results next.

3UHFLVLRQ 5HFDOO )
Figure 6: Comparison in RD task
Performance comparison in RD task: Figure 6 shows the
precision,recall,andF1achievedbyDEMAR,comparedwiththe-
state-of-artapproachCNC,andsix otherpopularclassifiers.Itis
easytoobservethatDEMARachievesthehighestperformanceinall
three metrics. CNC is the second-best performer, only its precision
slightly worse than the random forest (RF) classifier. The results
indicatethatDEMARoutperformsallsevenbaselineapproaches,
including the state-of-art CNC approach and six popular text clas-
sifiers.Thisisattributedtothebenefitsofmultitasklearning,i.e.,
by leveraging additional information that can only be learned in
a jointly learning setting. More specifically, the model learns abetter-shared representation (the hidden vector
‚Ñéùëñ) by multitask
learning,whichimplicitlybooststheperformanceoftheRDtask,
even compared with the latest state-of-art CNC approach.
Performance comparison in RA task: Table 3 summarizes
the comparison results of overall accuracy and F1 across seven cat-
egoriesamongDEMAR,FRA,andsixpopularclassifiers.Itshows
that DEMAR achieves the highest overall accuracy, i.e., with a 1.8%increasethanthatofFRAbaseline.Inaddition,DEMARalsohasthe
highestperformances infourcategories including"Intent(INT)","Current Behavior (CB)", "Explanation (EXP)", and "Trivia (TRI)".
Itis observedthatFRA alsoachievesrelatively goodperformanceTable 3: Comparison in RA task
ApproachF1AccuracyINTBEDREXPEXATRI CB
DEMAR 91.4%76.2%68.9%87.2%69.7%88.0%91.5% 83.3%
FRA 87.5%84.9%83.0%83.0%80.5%82.0% NA 81.5%
LR 70.5%48.6%56.8%57.4%71.0%79.2%77.4% 68.6%
NB 70.0%45.7%57.6%46.7%73.5%79.0%76.8% 69.4%
RF 75.9%64.4%72.5%54.0%60.4%80.0%78.5% 72.0%
SVM 75.8%59.4%69.4%57.4%71.8%74.1%77.9% 75.7%
TEXTCNN74.9%51.5%61.7%67.2%54.9%83.8%78.0% 73.2%
TEXTRNN 57.7%38.0%58.5%61.0%53.4%82.0%47.3% 58.3%
intheRAtask.Thisisbecauseitisarule-basedapproach,which
may be more effective, but building those fuzzy rules require extra
involvement of requirements experts. Considering DEMAR is a
fully automated, learning-based approach, we believe the compar-
ison result demonstrates DEMAR is sufficiently effective in com-
paringwithandpotentialasreplacementofmanualexperteffort.
Compared with the other six learning-based classifiers, DEMARis 7.6%-25% higher in terms of accuracy. Such improvement may
benefitfromthedataaugmentationandeavesdroppingmechanism,
thus making the learning process more effective.
Performancecomparisonacrosseightindividualprojects:
Table 4 summarizes the performance comparison results across
eight individual projects, among DEMAR and all baselines. The
comparison is conducted in the RD and RA tasks, respectively. The
‚Äúapproach‚Äù column lists the corresponding baselines, as mentioned
aboveandinSection4.4.ForRDtask,DEMARachievesthehighest
F1 in most (5/8) of the individual projects, and the highest average
of F1 (85%) among all the eight projects. For RA task, DEMAR
achievesthehighestaccuracy(83%)onaverage,andalsothehighest
accuracyon4ofthe8projects,whileFRAisthehighestonother
three project, and TextCNN is the highest on one project.
AnswerstoRQ2: Onaverage,DEMARoutperformsallselected
baselines in RD and RA tasks, which indicates the advantages of
the proposed approach.
5.3 Hyper-parameter Sensitivity Analysis(RQ3)









5') 5$$FFXUDF\
(a) LSTM Dimension








5') 5$$FFXUDF\
(b) Batch size        
5') 5$$FFXUDF\
(c) Lamda
Figure 7: Results of hyper-parameter tuning
Figure7(a)showsthevariationofDEMAR‚Äôsperformanceunder
different values for the first hyper-parameter, i.e., the dimension
of the hidden vector. It is clear that DEMAR produces the best
performanceforbothtasks,whenthevalueofthishyper-parameter
‚Ñéùëñis set as 256. Therefore, this is identified as the optimal value
343Table 4: Summary of baseline comparison across eight projects
Task Name ApproachActiveMq ArchivaAspectj HDFS Hibernate Log4jMopidy SWTAvg.
RD(F1)DEMAR 84% 72% 78% 88% 95% 83% 94% 82%85%
CNC 80% 65% 73% 85% 89% 85% 88% 83%81%
LR 71% 72% 55% 80% 82% 80% 75% 73%74%
NB 69% 74% 52% 70% 75% 70% 73% 79%70%
RF 75% 77% 75% 83% 82% 86% 84% 68%79%
SVM 79% 77% 76% 85% 92% 78% 82% 75%81%
TEXTCNN 78% 60% 68% 80% 85% 83% 85% 80%77%
TEXTRNN 65% 63% 62% 73% 76% 84% 61% 66%69%
RA(Accuracy)DEMAR 80% 82% 79% 88% 86% 76% 82% 88%83%
FRA 83% 77% 80% 85% 87% 72% 73% 86%80%
LR 67% 52% 69% 72% 64% 59% 66% 79%66%
NB 62% 58% 60% 80% 59% 55% 61% 72%63%
RF 78% 70% 65% 77% 68% 68% 78% 75%72%
SVM 82% 68% 70% 82% 59% 57% 69% 79%71%
TEXTCNN 72% 70% 79% 77% 65% 79% 76% 87%76%
TEXTRNN 53% 55% 62% 70% 72% 65% 60% 72%64%
of‚Ñéùëñ, which is used in training DEMAR. Figure 7 (b) shows the
variationofDEMAR‚Äôsperformanceunderdifferentvaluesforthe
second hyper-parameter, i.e., batch_ size. It is observed that the
valueof32leadstothehighestperformancesofDEMARonboth
tasks, we chose 32 as the value of the batch_size parameter. Figure
7(c)showsthattheimbalancedsharingofthetwolossfunctions
(ùêøùëúùë†ùë† ùëÖùê∑andùêøùëúùë†ùë† ùëÖùê¥) could result in performance decreasing on
either RA task or RD task. The curves show that if the ùúÜis smaller
than0.5,theperformanceoftheRDtaskdecreases.Ifthe ùúÜislarger
than0.5,theperformanceoftheRAtaskdecreases.Itindicatesthat
inthisstudy,abalancedweightsharinglossfunctioncouldachieve
thebestperformanceonbothrelatedtasks.Therefore,wechoose
the value 0.5 as the parameter value of ùúÜ. Finally, the combination
of the three values is selected as the tuned parameter values andused to train DEMAR, whose promising performance has been
demonstrated in previous sections for answering RQ1 and RQ2.
Answers to RQ3: DEMAR employs a greedy strategy and en-
ables automatic tuning-up of its hyper-parameters, which enables
theidentificationofthebestparametervaluestoachieveitspromis-
ing performance.
6 DISCUSSION
6.1 Advantages of Deep MTL
Resultsintheprevioussectiondemonstratethat,leveragingdeep
MTL,DEMARoutperformsexistingbaselinesaswellasitssingle
tasklearningmodes,inbothrequirementsanalysistasks(i.e.,RD
and RA). This confirms findings in most studies on MTL‚Äôs applica-
tions[6,72].BesidestheexplicitdataaugmentationphaseinDE-
MAR,webelievethatotherimplicitaugmentationcharacteristics
ofMTLalsocontributestotheincreasedpredictionperformance.
First, the shared layer learning, as detailed in Section 3.2.2 and
Section3.2.3,allowssimultaneouslyprocessingrelatedtasksand
introducing extra information to each single task. Second, consid-
ering that there are different noise patterns in RD and RA dataset,
DEMARcanleveragetheMTL-basedmodeltomitigateandbalanceextremenoiseandfocusmoreonsharedfeaturesthatmatter.Third,
theprincipledlossfunctioninDEMARallowstheMTLmodelto
eavesdrop the information from other tasks and learns featureswhichareeasytocapturebyothertasksbutdifficultbyitself.All
thesecharacteristicsofMTLjointlycontributetotheincreasedper-
formance when simultaneously learning RD and RA classification
tasks.
6.2 Side-effect Prediction
Existing RA methods focus on requirements annotation, therefore,
onlytakerequirements(R)asinputs.Unlikethese,DEMARcanalsotakenon-requirements(NR)typesofissuesasinputs,andproduceasetofpredictedRAlabels.Inanotherword,evenifanissueisaNR,
i.e., a negative RD instance, DEMAR can still learn and generate
itssentence-levelannotations.Thiskindofside-effectprediction
maysoundchaoticorconfusing,asNRsarenotvalidrequirements
and can not get sentence-level RA labels. (Recall in Section 4.3,this study only takes requirements instances into account when
evaluating the performance of RA task.)
However,therearecertainfrequentcasesthatsuchsideinforma-
tionmaybebeneficialtodeveloperactivitiessuchasbugresolution
orinformation sharing.Considerthe largevariety oftopicsdevel-
opers/users are communicating through issue repositories. Besides
submitting a requirement, they often discuss other types of NR
issues, such as reporting a bug, fixing a bug, asking for help, sched-
ulingatask,etc[ 15,74].Basedonourpreviousobservationsand
experiences from the eight open-source projects, the annotationcategories in DEMAR, as listed in Table 1, seems widely general-izable in covering and representing sentences in NR issues. For
example,mostbugreportscontainsentencesdescribingthe‚Äúcur-
rentbehavior‚Äù,‚Äúdrawback‚Äù,or‚Äúintent‚Äù(a.k.aexpectedbehavior);
typical ask-for-help issues may contain sentences related to some
‚Äúexample‚Äù and/or ‚Äúexplanation‚Äù. In these cases, theside-effect pre-
dictioninDEMARcanbeusedasissueannotation(IA)tohighlight
importantsentenceswithcorrespondinglabels,andimprovesthe
efficiency in issue resolution.
Weconductafurtheranalysistocompareandunderstandthe
overall performance of DEMAR in not only RA, but also IA. In this
analysis, we take into consideration of the side-effect prediction of
the NR instances. More specifically, we first calculate the F1 andaccuracy using NR instances from the test data, and then do the
344Table 5: Performance with side-effect prediction
TASKF1AccuracyINTBEDREXPEXATRICB
RA(R)91.4%76.2%68.9%87.2%69.7%88.0%91.5% 83.3%
IA(NR)83.6%72.9%88.6%86.2%72.5%91.0%92.1% 83.5%
IA(R+NR) 88.7%75.3%81.3%86.9%70.2%89.3%91.7% 83.4%
same using both R and NR instances from the test data. As shown
in Table 5, DEMAR can achieve 83.6% and 92.1% F1 on the ‚ÄúIntent‚Äù
and‚ÄúCurrentBahevior‚Äùsentencesrespectively.Besides,DEMAR
alsoachievesrelativelygoodperformancesonothertypes,withthe
overallaccuracyof83.5%.Moreover,theF1of‚ÄúDrawback‚Äùincreases
20% when applying on the NR dataset, mainly due to there are
more‚ÄúDrawback‚Äùsentencesinbugreports.Theresultsindicatethat
DEMAR has the potential to benefit the issue resolution process.
6.3 Adaptability for Different Data Sources
AlthoughtheevaluationofDEMARisonlyconductedinRDand
RA tasks using issue reports, the potential to adapt and apply it to
handlingotherdatasourcesisabundant.Nomatterwhatsources
or format the data comes from, people tend to express their de-siredfeaturesinarelativelyconsistentway.Forexample,people
frequently use similar expressions such as ‚Äúneed implement sth. in
next release‚Äù and ‚Äústh. will be a solution/improvement‚Äù, to indicate
requirements in other resources such as chat messages, develop-
mentemails,etc.TheproposedDEMARapproachcanbeadapted
to other data sources containing similar representative require-ments. Besides, DEMAR can also be applied to handle differentlanguages since DEMAR does not make use of language-relatedfeatures to train the model. When switching to other languages,
DEMARusersonlyneedtoapplyadditionalpre-processingaccord-ingtothespecificlanguage,e.g.,switchtotheBERTtrainedbythe
specific language corpus.
6.4 Relevance to Industry Practices
As a fully automated approach, DEMAR can be applied in support
different issue review and resolution scenarios. Our ongoing work
includes the implementation of DEMAR and develops a dialog bot
to continuously monitor Github issue reports, and automatically
analyze,classify,andalertdevelopersaboutimportantrequirements.Suchautomatedtoolcanbeeasilyintegratedwithtypicalworkflow
of software development teams.
6.5 Extension to Other MTL Applications
Current consensus is that MTL is applicable if there is consider-
able understandingof taskrelatedness [ 6], i.e.,their similarity, re-
lationship, etc. The authors believe that, in the field of software
engineering (SE), there are a few related tasks that can readily take
advantage of MTL.
Bug Prediction Analysis. Bugpredictionisoneoftheclassic
SEtopics,andexistingresearchhasproposedintensivemethods,
models,andtoolstoaddresstotalbugprediction[ 17,29,46,67],bug
classification [ 11,16,57], bug triage [ 3,5,21,21,27,56], bug fixing
effort prediction [ 69], and so on. In these related tasks, textual bug
description documents are common key inputs. Nonetheless, each
differenttypeoftasksmayrequireadditionalinformation,whichcan be shared in a multitask learning setting to avoid individual
representationbiasandimprovejointlearningperformance.This
would be an potential topic for further exploration.
CommitMessage Analysis. Anotherpossibleextensionisto
explore MTL in commit message analysis, which is essential to
many SE tasks such as commit classification [ 45,68], commit-issue
linking [43,54], commit review recommendation [ 1,14,65], etc. In
thiscase,thecommontextualinputsarecommitmessages,followed
byothercommitmeta-datasuchasauthor,filesincluded,andthe
commit diff, etc. Similarly, the SE field has developed intensiveunderstanding of each individual task, using most of the leadingmachine learning techniques, the additional exploration of MTL
seems to be promising.
6.6 Limitations
Nonetheless, there are several limitations to this study. The firstlimitation is the generalizability of the proposed approach. It is
only evaluated using eight open-source projects, which might not
berepresentativeofclosed-sourceprojectsandotheropen-source
projects.Theresultsmaybedifferentifappliedtootherprojects.
However, the dataset comes from four communities in eight differ-
ent fields. The variety of projects relatively reduce this threat.
The second limitation may come from data pre-processing to
remove the code snippets, patches, stack trace, and other technical
informationfromthenaturallanguagetext.Itispossiblethatthe
naturallanguagecontentisalsoremovedfromthetext,whichleads
to errors when evaluating the actual performance. To mitigate this,
weusethestate-of-the-arttool[ 36]whoseperformancecouldreach
82% F1, which relatively alleviates the threats.
Thethirdlimitationrelatestothefixedinstancelength.DEMAR
sets instance length as the fixed value 10, and truncates the ending
sentencesifainstancehasmorethan10sentences.Itmeansthat
DEMARwoulddiscardthepredictionstothetruncatedsentences
intheRAtask.However,nearlyalltheinstanceshavemorethan
10 sentences in our dataset. Thus, the not-available predictions
causedbytruncationcouldbeasmallminority.Furthermore,whenapplyingourapproachintootherdataset,theinstancelengthcould
be variable and determined by statistics to minimize the threat.
Theforthlimitationrelatestothesuitabilityofevaluationmet-
rics. We utilize accuracy, precision, recall, and F1 to evaluate theperformance,inwhichweusethedocumentlabelsandsentence
labels that are manually labeled as ground truth when calculating
the performance metrics. The threatscan be largely relieved as all
the instances are reviewed and only the instances which reach the
agreement on are kept in our dataset.
7 RELATED WORK
Our work is related to previous studies that focused on (1) re-
quirements discovery; and (2) requirements annotation. We briefly
review the recent studies in each category.
7.1 Requirements Discovery
This section reviews two typical categories of approaches (rule-
basedapproachesandlearning-basedapproaches)forautomated
requirements discovery.
3457.1.1 Rule-based approaches. Di Sorbo et al.[52] proposed a tax-
onomyofintentionstoclassifysentencesindevelopermailinglists
into six categories: ‚ÄúFeature Request‚Äù, ‚ÄúOpinion Asking‚Äù, ‚ÄúProb-
lem Discovery‚Äù, ‚ÄúSolution Proposal‚Äù, ‚ÄúInformation Seeking‚Äù, and
‚ÄúInformationGiving‚Äù.Theycodified36linguisticrulesfordiscov-
eringfeaturerequestsfromemails,e.g.,[someone]wantstohave
[something].Morales-Ramirez etal.[37,38]identifiedrequirements-
related information in OSS issue discussion using 20 speech-actrulessupportedbynaturallanguageprocessingtechniques.Vlas
andRobinson[ 64]proposedagrammar-baseddesignofsoftware
automation for the discovery and classification of natural language
requirements found in open source projects repositories. Previous
rule-basedapproacheshighlyrelyontheinvolvementofhumanex-
pertise, which is labor-intensive and difficult to rebuild in practice.
Whileourworkfocusesonprovidinganadvancedlearning-based
solution to fully automated the discovery task.
7.1.2 Learning-based approaches. Aryaetal.[4]identified16in-
formation types including potential newfeature requests through
quantitativecontentanalysisof15issuediscussionthreads.They
also provided a supervised classification solution by using Random
Forestwith14conversationalfeaturesthatcanclassifysentences
expressing new feature requests with 0.66 F1-score. Rodegheroet al.[
34] presented an automated technique that extracted use-
ful information from the transcripts of developer-client spoken
conversationstoconstructuserstories.Theyusedmachinelearn-
ingclassifierstodeterminewhetheraconversationcontainsuser
storyinformationornot.Merten etal.[35]investigatednaturallan-
guage processing and machine learning features to detect software
feature requests in issue tracking systems. Their results showed
that software feature requests detection can be approached on the
level of issues and data fields with satisfactory results. Antoniol
etal.[2]investigatedwhetherthetextoftheissuespostedinbug
tracking systems is enough to classify them into corrective mainte-
nance and other kinds of activities. They alternated among various
machinelearningapproachessuchasdecisiontrees,naiveBayes
classifiers,andlogisticregressiontodistinguishenhancementapartfromotherissuespostedinthesystem.MaalejandNabil[
31]lever-
aged probabilistic techniques as well as text classification, natural
language processing, and sentiment analysis techniques to classify
app reviews into bug reports, feature requests, user experiences,
and ratings. Their results showed that the classification can reach
theprecisionbetween70-95%andrecall80-90%actualresults.Otherstudieshavebeenfoundtoalsocaptureuserneedsfromappreviewsautomatically[
9,22,40,61].Huang etal.[18]foundthatDiSorbo‚Äôs
workcannotbegeneralizedtodiscussionsinissuetrackingsystems,
andtheyaddressedthedeficienciesofDiSorbo etal.‚Äôstaxonomy
byproposingaconvolutionneuralnetwork(CNN)-basedapproach.
Insummary,mostlearning-basedapproachesrelyonasetofpre-
definedfeaturestotrainmachinelearningmodelsforspecifictasks.
While Huang et al.[18] took the first attempt to utilize deep learn-
ing techniques to avoid explicitly extract features, and yieldingsignificant improvements against Di Sorbo et al.‚Äôs rule-based ap-
proachandotherninecommonly-usedclassificationapproaches.
Ourworkdiffersfrom existinglearning-basedapproachesinthat,
instead of using supervised training objectives on a single task, wefocus onexploring and utilizing therich correlation information
between RD and its related RA tasks.
7.2 Requirements Annotation
Shietal.[51]proposed81fuzzyrulesthatcanclassifysentencesin
issuesintosixcategories:‚ÄúIntent‚Äù,‚ÄúBenefit‚Äù,‚ÄúDrawback‚Äù,‚ÄúExam-
ple‚Äù, ‚ÄúExplanation‚Äù, and ‚ÄúTrivia‚Äù. Their work designed to help ana-
lyzing real intents of feature requests, which can also expedite the
process of feature requests understanding. Our work extends their
categories by distilling ‚ÄúCurrent Behavior‚Äù from the ‚ÄúExplanation‚Äù
category.Moreover,weproposedamultitasklearningapproachthatcan automated classify feature-request sentences in a better perfor-
mance.Vlas etal.[62]definedsixlevelsofclassificationpatterns
based on ontology rules, e.g., they used a subject-action-objectassertion to extract operability and expandability requirements.
Although the rules worked well with the 16 projects taken from
Sourceforge, it requires new rules to work effectively with new
datasets.Moreover,Theontologyrulesarecodifiedbyhumanex-
pertise, which is labor-intensive and difficult to rebuild in practice.
In summary, existing requirements annotation studies are all
rule-basedsolutions,ourworkprovidesaneffectivelearning-basedsolutionbyjointlylearningtheRAtaskwiththeRDtaskthatresults
in better generalization performance than learning separately.
8 CONCLUSION AND FUTURE WORK
In this paper, we proposed a deep multitask learning approach,DE-MAR,forrequirementsdiscoveryandannotation.DEMARconsists
of three main phases: (1) data augmentation phase, for data prepa-
rationandallowingdatasharingbeyondsingletasklearning;(2)
modelconstructionphase,for constructingtheMTL-basedmodel
forrequirementsdiscoveryandrequirementsannotationtasks;and
(3) model training phase, enabling eavesdropping by shared loss
function between the two related tasks.
Evaluation results from eight open-source projects show that:
(1) DEMAR outperforms all state-of-the-art approaches, i.e., with a
precisionof91%andarecallof83%forrequirementdiscoverytask,andanoverallaccuracyof83.3%forrequirementannotationtask;(2)Byjointlylearningthetwotasks,theperformanceofdiscoverytask
increases 4% of F1, and annotation task increases 8% of accuracy;
DEMAR provides a novel and effective way of learning two
relatedrequirementsanalysistasks.Webelievethatitalsosheds
light on further directions in exploring the application of multitask
learning in solving other related software engineering problems.Future directions include, but not limited to, further evaluation
DEMAR with more intensive data, implementation and integrationofDEMARprototypewithmoderndevelopmentenvironmentsuch
asGithub;applyingandextendingMTL-basedapproachestonotonlyrequirementstasks,butothertypesofsoftwareengineering
tasks, e.g., issue analysis, and so on.
ACKNOWLEDGMENT
ThisworkissupportedbytheNationalScienceFoundationofChina
under grant No.61802374, No.61432001, No.61602450, the National
Key Research and Development Program of China under grant
No.2018YFB1403400, and China Merchants Bank.
346REFERENCES
[1]ToufiqueAhmed,AmiangshuBosu,AnindyaIqbal,andShahramRahimi.2017.
SentiCR:acustomizedsentimentanalysistoolforcodereviewinteractions.In
Proceedingsofthe32ndIEEE/ACMInternationalConferenceonAutomatedSoftware
Engineering,ASE2017,Urbana,IL,USA,October30-November03,2017.106‚Äì111.
[2]Giuliano Antoniol, Kamel Ayari, Massimiliano Di Penta, Foutse Khomh, and
Yann-Ga√´l Gu√©h√©neuc. 2008. Is It a Bug or an Enhancement?: a Text-based
ApproachtoClassifyChangeRequests.In Proceedingsofthe2008conferenceof
the centerfor advancedstudieson collaborativeresearch: meetingof minds.ACM,23.
[3]
John Anvik, Lyndon Hiew, and Gail C. Murphy. 2006. Who should fix this bug?.
In28thInternationalConferenceonSoftwareEngineering(ICSE2006),Shanghai,
China, May 20-28, 2006. 361‚Äì370.
[4]Deeksha Arya, Wenting Wang, Jin L. C. Guo, and Jinghui Cheng. 2019. Analysis
and detection of information types of open source software issue discussions.
InProceedings ofthe41st InternationalConference onSoftware Engineering,ICSE
2019, Montreal, QC, Canada, May 25-31, 2019. 454‚Äì464.
[5]Gerald Bortisand Andr√© vander Hoek. 2013. PorchLight:a tag-basedapproach
tobugtriaging.In 35thInternationalConferenceonSoftwareEngineering,ICSE
‚Äô13, San Francisco, CA, USA, May 18-26, 2013. 342‚Äì351.
[6] R Caruana. 1997. Multitask Learning. Machine Learning 28, 1 (1997), 41‚Äì75.
[7] Rich Caruana. 1998. Multitask Learning. In Learning to Learn. 95‚Äì133.
[8]Jane Cleland-Huang, Horatiu Dumitru, Chuan Duan, and Carlos Castro-Herrera.
2009.Automatedsupportformanagingfeaturerequestsinopenforums. Commun.
ACM52, 10 (2009), 68‚Äì74.
[9]AndreaDiSorbo,SebastianoPanichella,CarolV.Alexandru,JunjiShimagaki,Cor-
rado A. Visaggio, Gerardo Canfora, and Harald C. Gall. 2016. What Would Users
Change in My App? Summarizing App Reviews for Recommending Software
Changes. In Proceedings of the 2016 24th ACM SIGSOFT International Symposium
on Foundations of Software Engineering. 499‚Äì510.
[10]WeiFu,TimMenzies,andXipengShen.2016. Tuningforsoftwareanalytics:Isit
really necessary? Inf. Softw. Technol. 76 (2016), 135‚Äì146. https://doi.org/10.1016/
j.infsof.2016.04.017
[11]BaljinderGhotra,ShaneMcIntosh,andAhmedE.Hassan.2015. Revisitingthe
Impact of Classification Techniques on the Performance of Defect Prediction
Models.In 37thIEEE/ACMInternationalConferenceonSoftwareEngineering,ICSE
2015, Florence, Italy, May 16-24, 2015, Volume 1. 789‚Äì800.
[12]Ian Goodfellow, Yoshua Bengio, and Aaron Courville. 2016. Deep Learning. The
MIT Press.
[13]Jiawei Han, Micheline Kamber, and Jian Pei. 2011. Data Mining: Concepts and
Techniques, 3rd edition. Morgan Kaufmann.
[14]ChristophHannebauer,MichaelPatalas,SebastianSt√ºnkel,andVolkerGruhn.
2016. Automatically recommending code reviewers based on their expertise: an
empirical comparison. In Proceedings of the 31st IEEE/ACM International Confer-
enceonAutomatedSoftwareEngineering,ASE2016,Singapore,September3-7,2016.
99‚Äì110.
[15]Kim Herzig, Sascha Just, and Andreas Zeller. 2013. It‚Äôs not a bug, it‚Äôs a fea-ture: How Misclassification Impacts Bug Prediction. In Proceedings of the 2013
International Conference on Software Engineering. IEEE Press, 392‚Äì401.
[16]LiGuo Huang, Vincent Ng, Isaac Persing, Ruili Geng, Xu Bai, and Jeff Tian. 2011.
AutoODC:AutomatedgenerationofOrthogonalDefectClassifications.In 26th
IEEE/ACMInternationalConferenceonAutomatedSoftwareEngineering(ASE2011),
Lawrence, KS, USA, November 6-10, 2011. 412‚Äì415.
[17]QiaoHuang,XinXia,andDavidLo.2019. Revisitingsupervisedandunsuper-
vised models for effort-aware just-in-time defect prediction. Empirical Software
Engineering 24, 5 (2019), 2823‚Äì2862.
[18]QiaoHuang,XinXia,DavidLo,andGailC.Murphy.2018. AutomatingIntention
Mining.IEEE Transactions on Software Engineering PP, 99 (2018), 1‚Äì1.
[19] huggingface. 2020. https://github.com/huggingface/transformers.[20]
Eric Hulburd. 2020. Exploring BERT Parameter Efficiency on the Stanford Ques-
tion Answering Dataset v2.0. CoRRabs/2002.10670 (2020).
[21]Gaeul Jeong, Sunghun Kim, and Thomas Zimmermann. 2009. Improving bugtriage with bug tossing graphs. In Proceedings of the 7th joint meeting of the
EuropeanSoftwareEngineeringConferenceandtheACMSIGSOFTInternational
SymposiumonFoundationsofSoftwareEngineering,2009,Amsterdam,TheNether-
lands, August 24-28, 2009. 111‚Äì120.
[22]NishantJhaandAnasMahmoud.2017. MiningUserRequirementsfromApplica-
tion Store Reviews Using Frame Semantics. (2017), 273‚Äì287.
[23]TianJiang,LinTan,andSunghunKim.2013. Personalizeddefectprediction.In
2013 28th IEEE/ACM International Conference on Automated Software Engineering,
ASE 2013, Silicon Valley, CA, USA, November 11-15, 2013. 279‚Äì289.
[24]YoonKim.2014. Convolutionalneuralnetworksforsentenceclassification. arXiv
preprint arXiv:1408.5882 (2014).
[25]Yoon Kim. 2014. Convolutional Neural Networks for Sentence Classification.
InProceedings of the 2014 Conference on Empirical Methods in Natural Language
Processing, EMNLP2014, October 25-29,2014, Doha, Qatar, A meetingof SIGDAT,a
Special Interest Group of the ACL. 1746‚Äì1751.[26]SiweiLai,LihengXu,KangLiu,andJunZhao.2015. RecurrentConvolutionalNeural Networks for Text Classification. In Proceedings of the Twenty-Ninth
AAAIConferenceonArtificialIntelligence,January25-30,2015,Austin,Texas,USA.
2267‚Äì2273.
[27]Sun-Ro Lee, Min-Jae Heo, Chan-Gun Lee, Milhan Kim, and Gaeul Jeong. 2017.
Applying deep learning based automatic bug triager to industrial projects. In
Proceedings of the 2017 11th Joint Meeting on Foundations of Software Engineering,
ESEC/FSE 2017, Paderborn, Germany, September 4-8, 2017. 926‚Äì931.
[28]StanleyLemeshow,DavidWHosmer,andWiley.2000. Appliedlogisticregression.
J. Wiley.
[29]ZhiqiangLi,Xiao-YuanJing,XiaokeZhu,HongyuZhang,BaowenXu,andShi
Ying.2019. Heterogeneousdefectprediction withtwo-stage ensemblelearning.
Autom. Softw. Eng. 26, 3 (2019), 599‚Äì651.
[30]AndyLiaw,MatthewWiener,etal .2002. Classificationandregressionbyran-
domForest. Rn e w s2, 3 (2002), 18‚Äì22.
[31]Walid Maalej and Hadeer Nabil. 2015. Bug report, Feature request, or Simply
Praise?onAutomaticallyClassifyingAppReviews.In 2015IEEE23rdinternational
requirements engineering conference (RE). IEEE, 116‚Äì125.
[32]Harish Tayyar Madabushi, Elena Kochkina, and Michael Castelle. 2020. Cost-
Sensitive BERT for Generalisable Sentence Classification with Imbalanced Data.
CoRRabs/2003.11563 (2020).
[33]Andrew McCallum, Kamal Nigam, et al .1998. A comparison of event models
for naive bayes text classification. In AAAI-98 workshop on learning for text
categorization, Vol. 752. Citeseer, 41‚Äì48.
[34]CollinMcmillan,CollinMcmillan,CollinMcmillan,andCollinMcmillan.2017.
Detecting User Story Information in Developer-client Conversations to Gen-erate Extractive Summaries. In Ieee/acm International Conference on Software
Engineering. 49‚Äì59.
[35]ThorstenMerten,Mat√∫≈°Falis,PaulH√ºbner,ThomasQuirchmayr,SimoneB√ºrsner,
and Barbara Paech. 2016. Software Feature Request Detection in Issue Tracking
Systems.In RequirementsEngineeringConference(RE),2016IEEE24thInternational.
IEEE, 166‚Äì175.
[36]ThorstenMerten,BastianMager,SimoneB√ºrsner,andBarbaraPaech.2014. Clas-sifyingunstructureddataintonaturallanguagetextandtechnicalinformation.In
11th Working Conference on Mining Software Repositories, MSR 2014, Proceedings,
May 31 - June 1, 2014, Hyderabad, India, Premkumar T. Devanbu, Sung Kim, and
Martin Pinzger (Eds.). ACM, 300‚Äì303.
[37]Itzel Morales-Ramirez, Fitsum Meshesha Kifetew, and Anna Perini. 2017. Analy-
sis of Online Discussions in Support of Requirements Discovery. In Advanced
Information Systems Engineering. 159‚Äì174.
[38]ItzelMorales-Ramirez,FitsumMesheshaKifetew,andAnnaPerini.2018. Speech-acts based analysis for requirements discovery from online discussions. Informa-
tion Systems (2018).
[39]LiMu.2014. EfficientMini-batchTrainingforStochasticOptimization.In Acm
Sigkdd International Conference on Knowledge Discovery & Data Mining.
[40]Fabio Palomba, Mario Linares V√°squez, Gabriele Bavota, Rocco Oliveto, Massim-
ilianoDiPenta,DenysPoshyvanyk,andAndreaDeLucia.2015. UserReviews
Matter!TrackingCrowdsourcedReviewstoSupportEvolutionofSuccessfulApps.
In2015 IEEE International Conference on Software Maintenance and Evolution,
ICSME 2015, Bremen, Germany, September 29 - October 1, 2015. 291‚Äì300.
[41] Paras. 2014. Stochastic Gradient Descent. Optimization (2014).
[42]JinfengRao,FerhanT√ºre,andJimmyLin.2018. Multi-TaskLearningwithNeural
Networks for Voice Query Understanding on an Entertainment Platform. InProceedings of the 24th ACM SIGKDD International Conference on Knowledge
Discovery & Data Mining, KDD 2018, London, UK, August 19-23, 2018. 636‚Äì645.
[43]Hang Ruan, Bihuan Chen, Xin Peng, and Wenyun Zhao. 2019. DeepLink: Recov-
ering issue-commit links based on deep learning. J. Syst. Softw. 158 (2019).
[44]SebastianRuder.2017. Anoverviewofmulti-tasklearningindeepneuralnet-
works.arXiv preprint arXiv:1706.05098 (2017).
[45]AntoninoSabettaandMicheleBezzi.2018.APracticalApproachtotheAutomaticClassificationofSecurity-RelevantCommits.In 2018IEEEInternationalConference
on Software Maintenance and Evolution, ICSME 2018, Madrid, Spain, September
23-29, 2018. 579‚Äì582.
[46]RiponK.Saha,MatthewLease,SarfrazKhurshid,andDewayneE.Perry.2013.
Improvingbuglocalizationusingstructuredinformationretrieval.In 201328th
IEEE/ACMInternationalConferenceonAutomatedSoftwareEngineering,ASE2013,
Silicon Valley, CA, USA, November 11-15, 2013. 345‚Äì355.
[47]G. Salton, A. Wong, and C. S. Yang. 1975. A vector-space model for information
retrieval. Communications of the Acm 18, 11 (1975), 13‚Äì620.
[48]Walt Scacchi. 2002. Understanding the requirements for developing open source
software systems. IEE Proceedings - Software 149, 1 (2002), 24‚Äì39.
[49]Abhinav Sethy and Bhuvana Ramabhadran. 2008. Bag-of-word normalized n-
grammodels.In NinthAnnualConferenceoftheInternationalSpeech Communi-
cation Association.
[50]Lin Shi, Celia Chen, Qing Wang, Shoubin Li, and Barry W. Boehm. 2017. Under-
standing feature requests by leveraging fuzzy method and linguistic analysis. In
Proceedingsofthe32ndIEEE/ACMInternationalConferenceonAutomatedSoftware
Engineering, ASE 2017, Urbana, IL, USA, October 30 - November 03, 2017. 440‚Äì450.
https://doi.org/10.1109/ASE.2017.8115656
347[51]Lin Shi, Celia Chen, Qing Wang, Shoubin Li, and Barry W Boehm. 2017. Under-
standing feature requests by leveraging fuzzy method and linguistic analysis.
automated software engineering (2017), 440‚Äì450.
[52]Andrea Di Sorbo, Sebastiano Panichella, Corrado Aaron Visaggio, Massim-
iliano Di Penta, Gerardo Canfora, and Harald C. Gall. 2015. DevelopmentEmails Content Analyzer: Intention Mining in Developer Discussions (T). In
30th IEEE/ACM International Conference on Automated Software Engineering, ASE
2015, Lincoln, NE, USA, November 9-13, 2015. 12‚Äì23.
[53]CongSunandZhihaoYang.2019. TransferLearninginBiomedicalNamedEntity
Recognition:AnEvaluationofBERTinthePharmaCoNERtask.In Proceedings
ofThe5thWorkshoponBioNLPOpenSharedTasks,BioNLP-OST@EMNLP-IJNCLP
2019, Hong Kong, China, November 4, 2019. 100‚Äì104.
[54]Yan Sun, Qing Wang, and Ye Yang. 2017. FRLink: Improving the recovery ofmissing issue-commit links by revisiting file relevance. Inf. Softw. Technol. 84
(2017), 33‚Äì47.
[55]Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. 2014. Sequence to SequenceLearningwithNeuralNetworks.In AdvancesinNeuralInformationProcessing
Systems 27: Annual Conference on Neural Information Processing Systems 2014,
December 8-13 2014, Montreal, Quebec, Canada. 3104‚Äì3112.
[56]Ahmed Tamrawi, Tung Thanh Nguyen, Jafar M. Al-Kofahi, and Tien N. Nguyen.
2011. Fuzzy set-based automatic bug triaging. In Proceedings of the 33rd Interna-
tionalConferenceonSoftwareEngineering,ICSE2011,Waikiki,Honolulu,HI,USA,
May 21-28, 2011. 884‚Äì887.
[57]Chakkrit Tantithamthavorn, Shane McIntosh, Ahmed E. Hassan, and Kenichi
Matsumoto.2016. Automatedparameteroptimizationofclassificationtechniques
fordefectpredictionmodels.In Proceedingsofthe38thInternationalConference
on Software Engineering, ICSE 2016, Austin, TX, USA, May 14-22, 2016. 321‚Äì332.
[58]Sebastian Thrun. 1996. Is learning the n-th thing any easier than learning the
first?. InAdvances in neural information processing systems. 640‚Äì646.
[59] tkdsheep. 2020. https://github.com/tkdsheep/Intention-Mining-TSE.[60]
Vladimir Naumovich Vapnik. 2000. The Nature of Statistical Learning Theory,
Second Edition. Springer.
[61]LorenzoVillarroel,GabrieleBavota,BarbaraRusso,RoccoOliveto,andMassimil-iano Di Penta. 2016. Release Planning of Mobile Apps based on User Reviews. In
Proceedings of the 38th International Conference on Software Engineering. 14‚Äì24.
[62]RaduVlasandWilliamN.Robinson.2011. ARule-BasedNaturalLanguageTech-
nique for Requirements Discovery and Classification. In in Open-Source Software
DevelopmentProjects.Proceedingsofthe44thHawaiiInternationalConferenceon
Systems Science.
[63]RaduE.VlasandWilliamN.Robinson.2011. ARule-BasedNaturalLanguage
TechniqueforRequirementsDiscoveryandClassificationinOpen-SourceSoft-
wareDevelopmentProjects.In 44thHawaiiInternationalInternationalConferenceonSystemsScience(HICSS-442011),Proceedings,4-7January2011,Koloa,Kauai,
HI, USA. 1‚Äì10.
[64]RaduEVlasand WilliamNRobinson.2012. TwoRule-basedNaturalLanguage
StrategiesforRequirementsDiscoveryandClassificationinOpenSourceSoft-
wareDevelopmentProjects. Journalofmanagementinformationsystems 28,4
(2012), 11‚Äì38.
[65]MinWang,ZeqiLin,YanzhenZou,andBingXie.2019. CoRA:Decomposingand
Describing Tangled Code Changes for Reviewer. In 34th IEEE/ACM International
Conference on Automated Software Engineering, ASE 2019, San Diego, CA, USA,
November 11-15, 2019. 1050‚Äì1061.
[66]Werner. [n.d.]. How do I Write a Good Feature Request. https:
//meta.stackexchange.com/questions/7656/how-do-i-write-a-good-answer-to-
a-question.
[67] Tingting Yu,Wei Wen,XueHan, andJaneHuffman Hayes.2019. ConPredictor:
Concurrency Defect Prediction in Real-World Applications. IEEE Trans. Software
Eng.45, 6 (2019), 558‚Äì575.
[68]SarimZafar,MuhammadZubairMalik,andGursimranSinghWalia.2019. To-
wardsStandardizingandImprovingClassificationofBug-FixCommits.In 2019
ACM/IEEE International Symposium on Empirical Software Engineering and Mea-
surement, ESEM 2019, Porto de Galinhas, Recife, Brazil, September 19-20, 2019.
1‚Äì6.
[69]HongyuZhang,LiangGong,andStevenVersteeg.2013. Predictingbug-fixing
time:anempiricalstudyofcommercialsoftwareprojects.In 35thInternational
Conference on Software Engineering, ICSE ‚Äô13, San Francisco, CA, USA, May 18-26,
2013. 1042‚Äì1051.
[70]TianzhuZhang,BernardGhanem,SiLiu,andNarendraAhuja.2012.Robustvisual
tracking via multi-task sparse learning. In 2012 IEEE Conference on Computer
Vision and Pattern Recognition, Providence, RI, USA, June 16-21, 2012 . 2042‚Äì2049.
[71]Xiang Zhang, Junbo Jake Zhao, and Yann LeCun. 2015. Character-level Con-
volutionalNetworksforTextClassification.In AdvancesinNeuralInformation
ProcessingSystems28:AnnualConferenceonNeuralInformationProcessingSystems
2015, December 7-12, 2015, Montreal, Quebec, Canada. 649‚Äì657.
[72]YuZhangandQiangYang.2018. Anoverviewofmulti-tasklearning. National
Science Review v.5, 1 (2018), 34‚Äì47.
[73]YukunZhu,RyanKiros,RichardS.Zemel,RuslanSalakhutdinov,RaquelUrtasun,
Antonio Torralba, and Sanja Fidler. 2015. Aligning Books and Movies: Towards
Story-likeVisualExplanationsbyWatchingMoviesandReadingBooks. CoRR
abs/1506.06724 (2015).
[74]ThomasZimmermann,RahulPremraj,NicolasBettenburg,SaschaJust,Adrian
Schr√∂ter, and Cathrin Weiss. 2010. What Makes a Good Bug Report? IEEE Trans.
Software Eng. 36, 5 (2010), 618‚Äì643.
[75]RodolfoZuninoandPaoloGastaldo.2002. AnalogimplementationoftheSoftMax
function. In IEEE International Symposium on Circuits & Systems.
348