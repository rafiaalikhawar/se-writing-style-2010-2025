Interactive Cross-language Code Retrieval with
Auto-Encoders
Binger Chen
TU Berlin
Berlin, Germany
chen@tu-berlin.deZiawasch Abedjan
Leibniz Universit ¨at Hannover & L3S Research Center
Hannover, Germany
abedjan@dbs.uni-hannover.de
Abstract —Cross-language code retrieval is necessary in many
real-world scenarios. A major application is program translation,
e.g., porting codebases from an obsolete or deprecated languageto a modern one or re-implementing existing projects in one’spreferred programming language. Existing approaches based onthe translation model require large amounts of training dataand extra information or neglects signiﬁcant characteristics ofprograms. Leveraging cross-language code retrieval to assistautomatic program translation can make use of Big Code.However, existing code retrieval systems have the barrier toﬁnding the translation with only the features of the input programas the query. In this paper, we present B
IGPT for interactive
cross-language retrieval from Big Code only based on raw code
and reusing the retrieved code to assist p rogram t ranslation. We
build on existing work on cross-language code representationand propose a novel predictive transformation model based onauto-encoders. The model is trained on Big Code to generate atarget-language representation, which will be used as the queryto retrieve the most relevant translations for a given program.Our query representation enables the user to easily update andcorrect the returned results to improve the retrieval process.Our experiments show that B
IGPT outperforms state-of-the-art
baselines in terms of program accuracy. Using our novel queryingand retrieving mechanism, B
IGPT can be scaled to the large
dataset and efﬁciently retrieve the translation.
I. I NTRODUCTION
The number of open-source program resources on the
internet is constantly growing. The most well-known are open
source code repository hosts, such as GitHub and Bitbucket.The GitHub database, i.e., the Public Git Archive [5], containsmore than 260,000 GitHub repositories, which are written in455 different programming languages and include more than16 billion lines of code, in the HEAD ﬁles alone. Further,community question answering sites, such as Stack Overﬂow,contain a large number of executable binaries that amountto billions of code snippets. These and similar resourcesare referred to as “Big Code” [37], which are created andmodiﬁed by programmers with much effort and time. Reuseof code from these abundant databases provides opportuni-ties for new applications, such as workﬂow generation [16],data preparation [44], programming assistance [25], databasemanagement [20], and transformation retrieval [43]. Anotherapplication that has recently emerged in this context is programtranslation [11], [39].
A useful technique that can support several of the afore-
mentioned applications is code retrieval. In particular, cross-language retrieval is becoming more prominent for use cases,
such as program translation and code-clone detection. Numer-ous programs are being developed and require correspondingversions in different languages. In cases when the developersdo not make the translation efforts themselves, users haveto manually rewrite the software in the needed language.For example, there are plenty of open-source prototypesdeveloped in academia, especially in the current boomingﬁeld of big data. To port codebases written in obsolete ordeprecated languages to a modern one [39], or further study,reproduce, or apply them on various platforms, researchersusually need to rewrite these programs in their preferredprogramming languages. Manually rewriting software is time-consuming and error-prone. For instance, the CommonwealthBank of Australia spent around $750 million and 5 years totranslate its platform from COBOL to Java [39]. Therefore,new approaches for automated program translation and codemigration are emerging [33]. The traditional methods arehardwired, rule-based compilers or cross-language interpreters,which require heavy human intervention for adaptation and arelimited to a small set of programming languages [3]. However,if we leverage a cross-language retrieval system, we can makethe most of the existing Big Code resources to support theprogram translation use case. In this paper, we discuss thepotentials of an effective cross-language retrieval system inassisting program translation.
State of the art. Our work is inspired by two lines of research,
code retrieval and supervised program translation.
Most existing code retrieval systems, such as Sourcerer [26],
lack the proper capabilities for code-to-code search
and/or cross-language retrieval. The cross-language systemYOGO [35] requires users to provide pre-deﬁned handwrittenpattern queries or feedback on several preset metrics andquestions as many other code search systems [18], [28], [40],[41], which increase the workload of users. Our recentlyproposed system RPT [11] aims to retrieve the translationonly with the feature of the source code and ignores thelanguage-speciﬁc differences to the target language. Allof the aforementioned techniques exclude users from theretrieval/translation loop.
Similar to natural language translation, the mainstream
data-driven program translation approaches train a transla-
1672021 36th IEEE/ACM International Conference on Automated Software Engineering (ASE)
DOI 10.1109/ASE51524.2021.000252021 36th IEEE/ACM International Conference on Automated Software Engineering (ASE) | 978-1-6654-0337-5/21/$31.00 ©2021 IEEE | DOI: 10.1109/ASE51524.2021.9678929
978-1-6654-0337-5/21/$31.00  ©2021  IEEE
tion model from large amounts of code data either in a
supervised [12], [31], [32] or weakly-supervised fashion [39].Supervised approaches require a parallel dataset to train the
translation model. In parallel datasets, programs in differ-ent languages are considered to be “semantically aligned”.Obtaining the parallel datasets in programming languages ishard because the translations have to be handwritten most ofthe time. A recent weakly-supervised method by FacebookAI [39] pretrains the translation model on the task of denois-ing randomly corrupted programs and optimizes the modelthrough back-translation. However, this method still relies onhigh-quality training data and directly reuses natural languageprocessing (NLP) approaches that neglect the special featuresof programming languages, such as code syntax. All theseapproaches require human efforts on the input, or additionalinformation for training and evaluation, such as annotations,program descriptions, API usages, and use cases, which can-not always be provided. Furthermore, compared to retrievalmethods, these machine-generated program translations sufferfrom grammar mistakes because of the rigor of programminglanguages.
Our methodology is to reuse existing Big Code resources
and developing cross-language retrieval techniques to assisttranslation with retrieved similar code in target language.Note that such a method might fail to ﬁnd the translationfor very speciﬁc long and complex programs because theachievable performance is directly depending on the richnessof the given repository. Nevertheless, due to the modularnature of programs and the huge data volume of the big code,it should be possible to retrieve the translation for smallerfragments of an input program, such as methods and functions.The user then can use these building blocks to assemble thecomplete translation. We improve and extend our preliminaryattempt [11] on supporting code translation through Big Codeby addressing the following open challenges and requirements:
•Big Code resources are typically imperfect and disordered.Thus, it is hard to obtain correct translations in absence ofsufﬁcient information, such as training data, hand-craftedpatterns, and semantic annotations.
•Because of the different syntax structures and naming mech-anisms in different programming languages, it is hard tocapture program features in a uniﬁed way. Ideally one has toresort to an intermediate representation that is automaticallyextractable from raw code. With this, it would be possibleto use similarity metrics already in place for code clone de-tection. But generating such an intermediate representationis yet an open task.
•Representation techniques that are in place for naturallanguage are not designed to deal with the special syntaxand grammatical rigor of code.
•The feature representation has to be incrementally updatableto enable user interaction.
In this paper, we present B
IGPT, an interactive cross-
language code retrieval system that reusing Big Code resources
to assist the p rogram t ranslation task. Given a raw piece
of code in the source language, B IGPT uses a novel querytransformation approach based on auto-encoders to retrievethe most similar piece of code in the selected target languagefrom the existing Big Code resources. We propose a querytransformation model, which can be trained in an unsuper-vised manner on Big Code to transform the input programrepresentation to a representation that is closer to propertiesof the target language. Due to the succinct form of the programrepresentation, the user can interact with B
IGPT and the
query can be automatically adapted to user annotations in thetarget code. As B
IGPT is a heuristic methodology based on
retrieving real programs, it is less prone to grammar mistakesthan approaches based on code generation. Our experimentsshow that our approach outperforms existing cross-languageretrieval techniques and statistical translation models thatrequire a large amount of training data. To this end, we makethe following main contributions:
•We present a program feature representation for effectiveretrieval of possible translations in imperative programminglanguages. With the help of Big Code, we propose a novelauto-encoder-based query transformation model, which cantransform the input code into the representation of itstranslation.
•We design the feature representation in a way that it canbe incrementally updated based on user corrections to efﬁ-ciently retrieve the translation from Big Code.
•We further expand our feature representation with a weight-ing scheme to enable user feedback that accelerates B
IGPT’ s
ability to improve its retrieval results.
II. R ELA TED WORK
Our work is related to code search, data-drive program
translation, and program representation. We also discuss re-lated work in cross-language code clone detection and naturallanguage retrieval to show the originality of our work.
General code search. Most of the existing methods cannot
ﬁnd translations with only raw code as input. Krugle and
Codase are commercial engines that apply the capabilitiesof web search engines for code search [2], [4]. Lucene is afamous conventional code search engine behind many existingtools such as Sourcerer [26]. It retrieves code based on textand code properties, such as fully qualiﬁed name and codepopularity. S6 retrieves code based on the user’s speciﬁca-tions and modiﬁes it through a set of transformations [38].CodeHow is a text-based code search engine that incorpo-rates an extended boolean model and API matching [27].DEEPCS trains a neural network model for code snippetsand their natural language description, which are used asqueries [19]. FaCoY is a code-to-code search engine thatrequires Q&A posts from Stack Overﬂow as query addition tothe code snippets [23]. YOGO provides a cross-language graphrepresentation, which however requires handwritten semanticrules and patterns written in a domain-speciﬁc language foreach query [35]. All these works require users or additionalresources to provide keywords, speciﬁcations, rules, or naturallanguage descriptions. Recently, we proposed RPT, whichuses cross-language retrieval for translations [11]. In this
168preliminary work, we used the features of the input program
to retrieve the translation and did not support user interactionalthough retrieval-based methods cannot always provide off-the-shelf translations. Except YOGO and RPT, all of theaforementioned systems are designed for the mono-languagesetting. In contrast to these systems, B
IGPT only takes raw
code as the query and aims to perform cross-language retrieval.
Interactive code search. Wang et al. reﬁne a query based on
user’s feedback on each result and reorder the ranking list [41].
Nie et al. extract relevant feedback from StackOverﬂow for theinitial query and reformulate it using Rocchio expansion [34].Dietrich et al. utilize a novel form of association rule miningto learn a set of query transformation rules from user feed-back to improve the search queries. CodeExchange leveragesthe context from previous retrieval results to reformulate agiven query and breaks it into several parts for the user tofeedback [28]. Sivar et al. propose an active learning systemALICE to iteratively reﬁne a query based on positive ornegative labels [40]. All of the aforementioned methods areonly applicable to mono-language scenarios and work withnatural language queries. As for interaction, these methods askthe user to give feedback on preset metrics based on Likertscale or questions, e.g., StackOverﬂow Q&A pairs. Thus,the performance mainly depends on the developer’s abilityto formulate queries. B
IGPT directly uses user’s corrections
to the retrieved results as feedback and integrates an activelearning-based query transformation model to reﬁne the query.
Data-Driven program translation. Existing work mainly
focuses on building a translation model. Nguyen et al. applied
the phrase-based statistical machine translation (SMT) modelon the lexemes of source code to translate Java code toC# [31]. In their follow-up work, they develop a multi-phase, phrase-based SMT method that infers and applies bothstructure and API mapping rules [32]. But they are limitedto languages that are similar on either structural or textuallevel, such as Java/C#. Chen et al. binarize the code treeof a piece of code and translate the code with an LSTM-based encoder-decoder model [12]. All the above methodsrequire a large parallel dataset for training. In contrast to them,the weakly-supervised system TransCoder [39] ﬁrst trains across-language model through the task of predicting randomlymasked words, then acquires a pre-trained translation modelfrom denoising randomly corrupted program task. Finally,they improve this model through back-translation. AlthoughTransCoder does not need parallel translation data, this transferlearning method highly relies on the similarity of the datafor pre-training. Their approach processes code like naturallanguage, which might leave out some programming-speciﬁclanguage features. We propose a system that can assist pro-gram translation without parallel datasets and additional infor-mation. By reusing Big Code, it can comply with the rigorousgrammar without machine-generated translation. Further, oursystem outperforms the aforementioned techniques with anovel program representation that captures the most crucialfeatures of programs.Program representation. By constructing program represen-
tations, one can enable the application of data processing to awide range of programming-language tasks including programtranslation and code search. Kamiya et al. and Allamanis etal. treat a program as plain text and use the sequence oftokens as representation to detect code clones and summa-rize code [8], [22]. Allamanis et al. present a Gated GraphNeural Network in which program elements are representedby graph nodes and their semantic relations are edges in thegraph to predict variable name and select correct variable [7].These methods rely on semantic knowledge, which requiresexpert analysis and is not generalizable across programminglanguages. A recent approach uses paths in the program’sabstract syntax trees (AST) as code representation to predictprogram properties such as names or expression types [9].And they further propose Code2vec that leverages a tree-based neural network to encode these paths and generatemore abstract representations [10]. Yin et al. employ neuralnetworks to express source code edits [45]. However, thesemethods are only designed to represent the features in oneprogramming language and do not capture the commonalitiesof multiple languages. And some methods are too abstract sothat important information for effective retrieval is missing,such as low-level program syntax or token type [8], [22]. Theprogram representation we use as the query is inspired bythe work in [9] and [11]. In addition to AST, we considerfeatures of concrete syntax trees (CST) and text to enrich theinformation for cross-language search. And we train a querytransformation model on Big Code to transform the featuresbetween different languages.
Cross-language code clone detection. This line of research
aims at identifying duplicates of a given piece of code.
However, this line of research has so far only focused onlanguages with similar intermediate representation, such asthe .NET language family [6], [24]. Others only calculatesimilarity on the textual level [13], [14] same as most codesearch methods. They simply treat programs as plain text andmake certain assumptions that limit their usage in practice,i.e., they try to identify different revisions of the same pieceof code. As our goal is to ﬁnd program translations, ourrequirements go beyond the state-of-the-art in clone detection.Nevertheless, we still compare our program representation toplain text representation used in code clone methods [13], [14]in our benchmarks showing its superiority.
Cross-language text retrieval. There is a body of work on
cross-language retrieval for natural language [21], [36], [42].
They take plain text as input and generate feature representa-tions based on natural language syntax. However, code has itsunique properties. Not only the code syntax can differ acrosslanguages, but the text in code does not exactly follow thesame vocabulary and writing formats as in natural language.Therefore, directly using these methods on code search canlead to incorrect results or failures. We also evaluate usingnatural language methods in our experiment by processingcode as plain text using bag-of-words and word2vec.
169Fig. 1: B IGPT overview
III. S YSTEM OVERVIEW
We propose B IGPT, an interactive cross-language code
retrieval system that assists program translation by reusing
Big Code. Given a piece of source program Pswritten in
language Ls, a selected target language Lt, and a large
program repository Dp={P1,P2,...,P n}, the goal is to ﬁnd
the best possible translation PtofPsinLtfromDp. The
essential problem is to design an effective program featurerepresentation that generalizes to many languages, can beupdated through user feedback, and enables efﬁcient retrievalin the scale of big code.
The workﬂow of B
IGPT is shown in Figure 1. B IGPT ﬁrst
constructs a feature representation for input programs to formthe query (Section IV-A). Since the target is to identify a simi-lar program in the target language, B
IGPT then applies a query
transformation model (QTM) to transform this representationinto an estimated feature representation of the translation (Sec-tion IV). QTM is trained in an unsupervised manner on BigCode but can also be updated dynamically through activelearning. This new representation will be used as a query toretrieve potential translations from the database. For efﬁcientretrieval (Section V), B
IGPT leverages an index structure that
captures key feature elements and is constructed in the ofﬂinephase. As an interactive system, B
IGPT allows the user to give
feedback on the retrieved translation (Section VI). The usercan either accept the result or make some corrections. Based onour structured and informative feature representation, B
IGPT
can easily and quickly adapt the query based on local usercorrections. Note that the user is not necessarily correctingthe whole program but only some local spots that they deemwrong. With this partial correction, B
IGPT may identify a
more appropriate translation candidate that can be acceptedby the user in the second retrieval attempt.
IV . Q
UERY CONSTRUCTION
As the user only inputs the raw source code, to retrieve a
relevant translation for it from a large code database, B IGPT
needs to automatically construct an effective cross-language
Fig. 2: Simpliﬁed syntax tree of a JavaScript program
Fig. 3: CST and AST of a fragment of the program in Figure 2
query. In this section, we will ﬁrst introduce the fundamental
program feature representation, then discuss how our querytransformation model (QTM) generates the search query, andhow B
IGPT further optimizes the model with active learning.
A. Program Representation
As already suggested by the recent cross-language code re-
trieval system RPT [11], B IGPT takes both structural features
and textual features as well as their dependencies into consid-eration to capture special aspects of programming languagesyntax and program semantics expressed in the text.
Structural Features. To capture the structural features of a
program, we resort to a representation based on the syntax
tree of a program. Each program can be represented by itssyntax tree where each tree node denotes a code construct.The syntax tree depicts the structural dependencies of the codeconstructs. One could also use control ﬂow graph (CFG) thatcaptures the dependency between code blocks and proceduresto approximate the code behavior. However, our goal is toassist program translation for any granularity of a program.Code behavior is hard to measure when the code fragment isnot an independently executable code block. Considering thatconstructing CFGs requires more complex analysis than syntaxtrees, we pick syntax trees as the basis of our representationto also capture the low-level syntactic structure within codeblocks. The syntax tree can be either a concrete syntaxtree (CST) or an abstract syntax tree (AST) [9], [10], [12].A CST depicts nodes with complete structural information,such as all the tokens in the code. As such the CST ishighly language-dependent. The AST on the other hand ismore abstract and misses information, such as the intermediatesyntax and the type of each token.
Therefore, the CST is quite verbose and the AST may
lose informative syntax and does not generalize to multiplelanguages. To develop a compromise solution that keeps thebest of both worlds, and as previously suggested [11], we fallback on the low-level CST as a basis and take the philosophy
170TABLE I: Paths extracted from the tree in Figure 2
Root-Node Leaf-Nodes Path-Type
ifStatementidentiﬁer, identiﬁer p1
identiﬁer, literal p2
literal, identiﬁer
literal, literal p3
equalityExpression identiﬁer, literal p4
assignmentExpression identiﬁer, literal p5
of AST as an inspiration. Speciﬁcally, we ﬁrst take CST of the
program as the base structure. Then we simplify the CST byonly removing semantically repetitive nodes so that redundantinformation can be removed and necessary information canbe retained. Figure 2 shows the generated syntax tree of aJavaScript program. We use the same approach as proposedfor RPT [11]. Figure 3 shows the original CST and AST of afragment of this program. We can see that the new syntax treeis more succinct than CST and more informative than AST.However, this syntax tree is still complicated for representingthe code features and retrieving the translation. And becauseof different control ﬂow elements in different programminglanguages, it is unlikely to ﬁnd programs that share the exactsame syntax tree. Therefore, imitating the idea of AST, wefurther abstract the tree. First, we simplify the representationand transform the two-dimensional tree structure into a setof one-dimensional paths that connect the program elementsinside the tree. B
IGPT extracts abstract paths as follows: for
each pair of leaf-nodes in the CST, B IGPT keeps the nodes
themselves, their values, and the root-node of the statementand drops all other intermediate nodes on this path. The root-node is the summary of the whole path and the leaf-nodesdirectly indicate the content on this path. These three nodesenclose the most critical information on a path. Moreover,we extract all paths between two leaf-nodes from the treeto represent the features. In this way, all the intermediatenodes have the chance to be the root-nodes, which facilitatesto capture more complete structural information. The pathsextracted from the trees in Figure 2 are shown in Table. Thenwe classify these paths into different types and replace thesepaths with their path-type as shown in Table. In this way,we can further generalize and simpliﬁes the features. Usingthe same method in RPT [11], B
IGPT considers two paths
are the same type if their root-nodes and leaf-nodes are thesame or have the same meaning, such as ifStatement and
if_stmt. Also, as the writing habit of programmers and thecoding conventions for a programming language might differ,
B
IGPT ignores the order of left and right leaf-nodes, such as
p2. Thus, the structural feature of a program can be succinctly
represented by a set of path-types p1,p2,...,p jextracted from
its syntax tree.
Textual Features. In contrast to existing work [13], [14] that
suggest to extract all the text from a program and do not con-
sider any context from the program structure, B IGPT considers
textual features only in strong dependency with the structuralfeatures and leverages context from the structure [11]. To doso, B
IGPT only processes text that appears in the extracted
paths and marks the path-type where they belong. Becausethe text appears on or connects to those removed semanticallyrepetitive nodes only brings in redundant information. Andconsidering the dependency can encode the text with thefeatures of the programming languages, not only the naturallanguages. This methodology can also simplify the featuresand improve system efﬁciency.
Similar to [11], ﬁrst uses word tokenization and lemmatiza-
tion to tokenize and stem all the words in the text. In addition,our tokenization process also considers camel case, spaces, andunderlines to accommodate code-speciﬁc language. However,it does not remove and tokenize numeric values, such as hard-coded ﬂoating points and integers, as they might be integralto the purpose of a program. Then B
IGPT vectorizes these
generated tokens based on the Bag-of-words model (BoW).One could also resort to more sophisticated and complexembeddings, such as word2vec (W2V) [30] and BERT [17].However, in programming languages, the structural featuresare more important than the textual ones and word ordercan be ignored to accommodate different programming styles.Besides, we only compare text for every single path-type,signiﬁcantly reducing the number of words for each similaritycalculation. BoW is sufﬁcient for this process. In our ex-periment, W2V does not show worthwhile improvements butrather introduces extra training time for building the languagemodel. To avoid repeated computation for every new inputand to improve the efﬁciency, B
IGPT precalculates the BoW
model in the ofﬂine phase. To build the BoW model, we needto prepare a vocabulary of unique words in each program fromthe repository. As we consider the dependency with structuralfeatures, two textually identical tokens from different types ofpaths are regarded as different tokens. To this end, we countall the text tokens t
1,t2,...,t kof each program and store the
results during the ofﬂine phase. In the online phase, B IGPT
only needs to run word statistics on the input program andbuild the BoW model.
Feature Representation. Unlike RPT that uses a list of path-
types and text collections as the ﬁnal representation [11],
we construct a numerical feature vector to represent theprogram. The ﬁnal feature representation is thus more com-pendious as one single vector, which can be directly usedto further train a learning model. Our ﬁnal representationis constructed as follows: we regard the tokens (textual)together with different types of paths (structural) as fea-ture elements eof a program and generate a feature vec-
tor consisting of the feature element frequencies. Let fbe
the occurrence frequency of feature elements, then the ﬁ-nal vectorized feature representation of a program will be[f
e1,fe2,...,f en]=[fp1,fp2,...,f pnp,ft1,ft2,...,f tnt]. In our
experiments, the number of different path-types is about 5,000on average for each language pair. In the subsequent retrievalprocess, the potential translation in the target language can beidentiﬁed by calculating the similarity of feature vectors.
171Fig. 4: Query transformation model (QTM)
B. Query Transformation Model
One could directly use the feature representation described
in Section IV-A as a query to retrieve translation candidates.
However, the feature vector will fail to accommodate somecross-language hurdles. For example, C# supports goto state-
ments while its Java translation has to use so-called labelledstatements with break orcontinue [1] instead of goto.
In this case, B
IGPT cannot directly use the features of the
C# program to retrieve its Java translation. The result canbe improved if the retrieval can be conducted based on thefeatures of the translation. While the translation of a completeprogram is our original problem and hard to solve withoutlarge amounts of training data, we propose a QTM that solvesa smaller problem. QTM transforms the original query, i.e., thefeature vector of the input program, into an optimized query,which will be the estimated feature vector of the translation.
Model Description. As shown on the left part of Figure 4,
in the online phase, B
IGPT extracts the features as Fsof a
program from source language Lsand feeds it into the QTM
to translate the program features to features in language Lt.
The QTM ﬁrst selects previously trained auto-encoders (see
the next paragraph for details) of LsandLtrespectively, then
extracts the encoder of the former and the decoder of the latterto compose a new encoder-decoder model. In this model, aone-layer encoder (red in Figure 4) maps the original featurevector to a low dimensional latent space and produces a shorterhidden vector H. ThenHis reconstructed to the estimated
translation feature vector F
tby a one-layer decoder (yellow
in Figure 4). Ftwill be used as a query to retrieve potential
translation in target language Lt.
Since there is no available training data for QTM, we lever-
age an unsupervised method based on - auto-encoders (AE) totrain an encoder and a decoder. An AE is an encoder-decodersystem that aims to reproduce its input. That is, it encodes theinput to a hidden vector, then reconstructs the input from thishidden vector. Therefore, no extra label for the training datais needed. With this approach, B
IGPT learns the weights of
the encoders and decoders separately. As shown in the rightpart of Figure 4, in the ofﬂine phase, B
IGPT trains a separate
AE ifor each programming language Liin the database on
all programs that are written in Li. Thus it obtains a pair ofEncoder iandDecoder ifor each programming language. For
the actual translation task, we combine the appropriate encoderand decoder depending on the source and target language ofa translation task. In Figure 4, the QTM selects the encoderEncoder
sof the source language LsfromAE sto transform
Fsinto the hidden layer representation. And it picks the trained
decoderDecoder tof the target language LtfromAE tto
estimate the Ft. This way, we can build a pre-trained model
with an encoder that learns signiﬁcant information from thefeature vector of the input program and a decoder that cangenerate features of its translation.
Active Learning Mode. To increase the accuracy of QTM,
we also provide an active learning mode to enable the user to
ﬁne-tune the model. For each pair of languages, it is possibleto train the corresponding QTM via active learning. As shownin Figure 1, during each translation retrieval, the most usefulinput programs in the source language are selected with asampling strategy. Then B
IGPT will retrieve its translation in
the target language. The user either accepts the retrieved resultor annotates the correct translation herself. Then with thisuser-approved correct translation as the label of the input, wecan further train the QTM and update the weights. To choosethe appropriate input program, we propose an aggregation offour sampling strategies that capture the informativeness of aprogram as follows:
•Coverage sampling picks the programs that cover a wider
range of different feature elements, which may reveal moreinformation. We consider programs that cover more than50% of the total amount of all feature elements as programswith high coverage. In a database, if the average amount offeature elements contained in a program Aisλand the
program contains more than λ/2 different feature elements,
it is a qualiﬁed sample.
•Rarity sampling considers programs with rare feature ele-
ments, i.e. programs that contain features that appear in atmost/epsilon1%of the program database. For example, if feature
elemente
1from program Aappears in x% (x</epsilon1 )o ft h e
database programs, Ais a qualiﬁed sample.
•Uncertainty sampling picks retrieved programs with low
certainty, i.e., lower similarity score than 75%. For example,if program Bis the top retrieved translation of program A,
but their similarity score is 50%, which is lower than 75%,programAis a qualiﬁed sample.
•Random sampling randomly selects a program [46].
BIGPT employs the query-by-committee method to aggre-
gate the results of the four sampling methods [15]. With thisapproach, we make sure to have incorporated a diverse set ofcharacteristics that might be relevant for sampling. The ﬁnaldecision is made by selecting program data where the largestdisagreement occurs among those sampling strategies.Thelevel of disagreement of a program xis measured by vote
entropyVE [15]:
VE(x)=−V(x)
NslogV(x)
Ns−Ns−V(x)
NslogNs−V(x)
Ns(1)
V(x)is the number of sampling strategies that select/vote x
172Fig. 5: Frequency histogram of path-types in one program
as a valuable sample. Nsis the total number of sampling
strategies, which equals 4in our case. Programs with higher
vote entropy are returned as samples.
V. T RANSLA TION RETRIEV AL
The output of the QTM is an approximate representation of
the translation. B IGPT uses this output as a query to retrieve
the candidate with the highest feature similarity. To avoid a
full scan while retrieving the most relevant translations, we usea path-type-aware index and an efﬁcient retrieval mechanism.
Index Structure. In the ofﬂine phase, B
IGPT constructs repre-
sentations as described in Section IV-A for each program from
the code database and stores them inside a feature database.To avoid a brute-force similarity computation, we need anindex structure. For two programs to be similar, they haveto share similar structure features, which are captured throughcommon path-types as described in Section IV-A. So we needto ﬁrst ﬁnd all the programs that share at least one path-typewith the source program. A naive approach is to index eachpath-type as the key. However, there are millions of programsinside the database, and there are only about 5,000 types ofpaths on average for each language pair, which makes thisindex structure highly sparse and ineffective. Inspired by [11],we design our index structure to also harbor the frequency ofeach path-type inside a program as many programs share thesame path-type but differ in the frequency of such path-types.Since the source program and its translation candidate maynot always share the amount of the same path-type, it wouldbe too strict to have an index entry for every combination ofpath-type and frequency. Instead, we divide the frequency ofpath-types into multiple buckets and use the bucket intervalsas indexes. We observed the frequency of each path-type ineach program roughly obeys exponential distribution as shownin Figure 5. To ensure the size of each bucket is equal, weﬁx a bucket size and create as many buckets as are needed.Then, we sort the programs based on the frequency of thecorresponding path-type and add them gradually to the sortedﬁx-sized buckets. In our experiments, a bucket size of 200already shows a high-performance gain.
Example 1:
p1occurs [0,1)times in 6 programs, [1,2)times
in 3 programs, [2,3)times in 2 programs, and [3,4)times
in one program. If we evenly divide the interval, we will
have 9 programs in [0,2)and 3 programs in [2,4). But if we
Fig. 6: User feedback mechanism
divide the interval into [0,1)and[1,4)based on the frequency
distribution, each interval contains 6 programs. This intervalleads to a more balanced index structure.
Efﬁcient Retrieval. For each pair of query program and
candidate program, B
IGPT calculates the weighted sum of
structural similarity and textual similarity to measure the
overall similarity. To make the retrieval process as efﬁcientas possible, we ﬁrst use the index that ﬁlters all programs thatdo not contain a similar amount of the same path-types. Thenumber of candidates for the similarity calculation is typicallystill quite high. Thus, B
IGPT consecutively calculates the
independent similarity components - structural similarity andtextual similarity and drops candidates that fail to meet aminimum threshold concerning any of the two. The thresholdsfor both components are chosen based on the inﬂection pointsof each score distribution, respectively. Similar to RPT [11],
B
IGPT ﬁrst calculates the structural similarity to each can-
didate, because the syntax structure of a program is morediscriminative than textual features. This will signiﬁcantlyreduce the number of irrelevant programs and avoid theunnecessary calculation of textual similarity with them. Forthe remaining candidates, a textual similarity ﬁlter is employedwhich uses a weighted Jaccard index that accommodates therelevance of common textual features within the same path-types. For all the ﬁnal remaining programs, the weighted sumof both previously calculated similarities will be generated toobtain the ﬁnal similarity score.
VI. Q
UERY ADAPTION WITH USER FEEDBACK
It can happen that the desired translation is not among
the top-k retrieved results. In this case, B IGPT can change
the query based on the user’s feedback. Existing interactivecode retrieval methods ask the user to give feedback on presetmetrics and questions as discussed in related work [34], [40],[41]. As each feature element in our query directly maps to acode fragment, B
IGPT can directly pass the user’s corrections
on the code to adapt the query. The simplest form of usingthe user corrections is to just update the feature vector of thecorrected program. However, we can also make use of thefact that user corrections lead to manually curated features.To reﬂect this in our feature representation, we extend it witha weighting scheme. As shown in Figure 6, we obtain a new
173Fig. 7: An example of user feedback
feature representation R, where each element consists of a
feature element feiand its weight wi. The initial weights are
uniform. After the user makes one or several corrections to
the result, B IGPT featurizes each correction the same way and
compares it with the original code to generate the weights. Weclassify corrections into three categories and the weights aretuned accordingly:
•Emphasize. If the correction increases the number of a
feature element fei∈R, its weight will be increased.
•Add. If the correction adds a feature element fei/∈R,fei
and its initial weight wiwill be added to R.
•Delete. If the correction decreases feature element fei∈R,
its weight wiwill also be decreased.
After a correction, the feature representation is updated
and used for a new round of retrieval. When calculatingthe similarity between query and candidates in the database,
B
IGPT prefers the candidate that has a higher similarity in
higher weighted features.
Example 2: Figure 7 is an example of the user feedback
module. The input is a greatest common divisor function in
JavaScript. The ground truth in Python is also shown in the ﬁg-ure. In the ﬁrst round, B
IGPT retrieved an Ackermann function
as its Python translation. The possible reasons are the variablenames are different in the ground truth and the weight of eachfeature element is not assigned properly. After user correctsthe ﬁrst wrong line (change return n+1 toreturn n),
B
IGPT constructs feature representation for the corrected
code. Then B IGPT compares it with the feature representation
of the input code and summarizes user’s corrections. Basedon this, B
IGPT updates the query: in Figure 7, the correc-
tions are deleting the path-type {additiveExpression,
{identifier, literal}} and token 1on this path-type.
As a result, these two feature elements will be removed and theweight of other feature elements will be increased accordingly.Finally, with the updated query, B
IGPT will run a new round
of retrieval. Without more precise features, the ground truthwill be more likely to be retrieved.
VII. E
XPERIMENTS
To show the feasibility of our B IGPT and evaluate its
effectiveness and efﬁciency in assisting program translation,we conducted a series of experiments:
A. We compare different variations of B
IGPT with existing
work from program translation and code search;
B. We evaluate B IGPT on more languages;
C. We discuss the inﬂuence of user interaction;
D. We evaluate the scalability and efﬁciency of B IGPT.
All experiments have been carried out on a PC with an IntelXeon E5-2650 v2 2.60GHz CPU and an NVIDIA Tesla K40mGPU.
Datasets. We have two different types of datasets:
1) Dataset for training the auto-encoders in QTM, Word2vec,
and Code2vec:
•Public Git Archive (PGA). We use this database with
more than 260,000 bookmarked repositories [5] to train
the AEs of QTM. We cleaned the dataset by gradu-ally removing duplicates at ﬁles, and ﬁles that cannotbe successfully parsed due to format, errors, versioncompatibility. We then collected data in four popularlanguages (JavaScript, Python, Java, C++). Finally, weobtain a dataset with a size of 260GB. We split all theﬁles into methods or functions.
2) Datasets for evaluation: Datasets with ground truth are
generally scarce. We run our experiments on two smallparallel datasets and one larger unlabeled dataset:
•Java-C# used in experiment A, C, D. It was used in
previous studies [11], [12], [31], [32]. We use the samedump that was used for the cross-language retrieval en-gine RPT [11], which contains 39,797 matched methods.
•GeekforGeeks used in experiment B. It was used in
TransCoder [39], which gathered and aligned 698 codingproblems and their solutions in Java, Python, and C++.
•PGAS used in experiment D. To save experiment equip-
ment and time, under the premise of ensuring the validityof the experiment and the reproducibility of the data, werandomly pick 1% ﬁles from PGA to obtain a datasetincluding 2,023,546 methods/functions. As this datasethas no labels, we manually judge the correctness.
Effectiveness metric. We use program accuracy (PA) as
proposed by prior work [12], [32]. PA is the percentageof the predicted translation that is the same as the groundtruth. Note that, PA is an underestimation because it does notaccount for programs that only differ in writing habits andstyle. We use this standard to manually judge the correctnessfor the dataset without ground truth. For our retrieval-basedmethod, we consider the top-1 retrieved result as the “predictedtranslation” and report the percentages of search where thecorrect translation was the top-ranked result. Note that PA isan underestimation of computational accuracy which evaluates
174whether the translation generates the same outputs as the
source program when given the same inputs [39].
A. Comparison with Baselines
We compare B IGPT with one rule-based tool (J2C# [3]) and
four data-driven program translation baselines (1pSMT [31],
mppSMT [32], Tree2tree [12], TransCoder [39]), which use
a translation model to generate the results. The supervised
baselines use 90% matched method pairs as training datato predict the translations for the rest of the programs. Weused the openly available implementation of Tree2tree.F o r
TransCoder, we follow their method to pre-train the cross-language model on the Public Git Archive dataset (30GBof Java and C# data). For the program translation baselines1pSMT and mmpSMT, we report the results from their workon the same dataset as their code and conﬁgurations are notavailable. Finally, we report the results of two mono-languagecode search systems (Sourcerer [26], CodeHow [27]) and one
cross-language system (RPT [11]).
We generate different versions of B
IGPT with variations in
feature representation and interaction:
•Representations: We analyze B IGPT WORD 2VEC and
BIGPT CODE 2VEC as two feature representations variations of
BIGPT that retrieving translation based on Word2vec [29]
and Code2vec [10], respectively.
•Interactive versions: We discuss three different interactiveversions of B
IGPT. All of which use the same feature
representation and the QTM module. B IGPT noALuses the
original QTM that has not been improved by active learning.
BIGPT is the default setting of our system. B IGPT +FB is
the full-ﬂedged interactive system when user feedback isavailable as described in Table II.
Table II shows the results and the degree of supervision.
We observe that the full-ﬂedged B
IGPT with at most one
user correction per task and optimized QTM outperforms allthe baselines. The improvement in program accuracy rangesfrom 19.5% to 65.5%. As expected, the mono-language codesearch baselines perform poorly because they are designedfor retrieval with more accurate and detailed input than rawcode in another language. The cross-language code searchsystem RPT performs better than other baselines, which showsthe feasibility of the translation retrieval methodology. Theresults of partial components of B
IGPT are encouraging.
BIGPT noAL, which does not leverage any supervision, out-
performs the Tree2tree, which shows the effectiveness ofour program feature representation and QTM module. Ourfeature representation equipped with QTM successfully im-proves the result by 7.5% compared to RPT showing thatgenerating features in the target language is more promisingthan using features of the source language. In our defaultsystem (B
IGPT), the QTM is further trained by active learning,
the accuracy can increase by 8.5%. The table also showsthat the Word2vec and Code2vec variants of B
IGPT cannot
perform better. Word2vec is designed for natural languages sothat it can not capture the special features of programminglanguages. Although Code2vec is designed speciﬁcally torepresent code, their model can only be trained within thesame programming language, which makes it less suitable forcross-language similarity comparisons.
We further explored the supervision impact on B
IGPT. In
this experiment, we let the user give at most one correctionto each retrieved task. With such limited user feedback,
B
IGPT +FB can still slightly improves on B IGPT. Compared
to the fully supervised methods 1pSMT, mmpSMT, andTree2tree, B
IGPT and B IGPT +FB leverage very limited human
supervision to achieve better results. In the ﬁrst retrieval round,where the user does not make corrections to any wrong results,
B
IGPT achieves 87.1% accuracy with only 80 labels for QTM.
Also, the reproduced weakly-supervised approach TransCoderdoes not achieve better results than B
IGPT. We observed that
their model often generates invalid translations with regardto grammar. For example, it often mistakes the input typeof a function. This phenomenon is also acknowledged intheir own paper and can be attributed to the fact that onlytextual features have been used. B
IGPT avoids this problem
by reusing existing code.
B. Evaluation on Multiple Programming Languages
We further evaluate B IGPT on more languages. We compare
BIGPT and the state-of-the-art methods TransCoder and RPT
on the GeeksforGeeks benchmark that contains ground truth
for Java, Python, and C++. Table III shows that the accuracyof B
IGPT is signiﬁcantly higher than TransCoder on their
own datasets as reported in their own paper. Note that, forTransCoder the authors report computational accuracy instead
of program accuracy. As the dataset has ground truth, thereported program accuracy for our method, which is an under-
estimation of the possible computational accuracy for B
IGPT.
We infer that TransCoder has two drawbacks: (1) TransCoderoutputs machine-generated translations while B
IGPT directly
retrieves existing programs as translations, which makes
BIGPT always output syntactically correct programs. (2)
TransCoder generally considers programming languages asplain text and aims to generate semantically similar programs.The black-box neural network model may neglect some non-trivial syntactical features of programming languages. B
IGPT
also outperforms the cross-language retrieval system RPTwith over 10%. The reason is that B
IGPT uses a query that
represents the features of the translation rather than the inputprogram. Also, the user-interaction mechanism can improvethe performance in most cases. This experiment further showsthat B
IGPT is generalizable for multiple languages including
dynamic languages, such as Python.
C. Inﬂuence of User Interaction
In Table II, we showed the inﬂuence of a single user
correction on the result. We further investigate the required
number of user corrections to retrieve the correct translations.We simulate the user correction with the ground truth in ourparallel dataset and for each returned result we ﬁx the ﬁrstdiffering line between true result and returned result.
175TABLE II: Comparison of different methods on PA and supervision extent (Java-C#)
Genre Method Description PA Supervision Extent
Rule-based J2C# manually deﬁned translation rules 16.8% fully supervised
Data-drivenprogram
translation1pSMT Phrase-based SMT 24.1%fully supervisedmmpSMT multi-phase phrase-based SMT 41.7%
Tree2tree tree-to-tree neural networks 70.1%
TransCoder weakly-supervised neural translation 49.9% weakly supervised
Code searchsystemSourcerer Lucene-based code search, free-text queries 13.5% no labels (directly
retrieve translation
with input)CodeHow free-text queries 13.5%
RPT cross-language code search 71.1%
V ariations
of B IGPTBIGPT WORD 2VEC word2vec as queries 67.7% no labels (directly
retrieve translation
with input)BIGPT CODE 2VEC code2vec as queries 63.4%
BIGPT noAL QTM without active learning 78.6%
BIGPT the default system 87.1% 80 labels for QTM
BIGPT +FB user feedback is available 89.6% 80 labels for QTM + at most 1 correction per task
TABLE III: Comparison of accuracy on GeeksforGeeks
C++ C++ Java Java Python Python
-Java -Python -C++ -Python -C++ -Java
TransCoder 60.9% 44.5% 80.9% 35.0% 32.2% 24.7%
RPT 69.2% 65.3% 70.9% 59.3% 55.4% 54.2%
BIGPT noAL76.6% 74.2% 78.1% 68.1% 59.2% 59.6%
BIGPT 87.2% 79.5% 84.8% 72.5% 66.2% 68.8%
BIGPT +FB 84.8% 83.0% 90.5% 77.5% 67.8% 68.1%
Fig. 8: Required amount of user feedback
Figure 8 shows the number of required user corrections
to obtain the correct result for all 5,134 failed translation
tasks from the ﬁrst retrieval round and the improvement inthe overall accuracy. We observe that in most cases, B
IGPT
only requires a single user correction to successfully completethe translation task. About 98% of the failed retrieval taskscan succeed after 10 user corrections. Considering the averagelength of an input program is 168 lines, we can concludethat with a limited number of user corrections the accuracyof B
IGPT can be signiﬁcantly improved. Note that, we might
even achieve better results if we do not restrict the users toﬁx the ﬁrst difference each time. A real user might ﬁx moresigniﬁcant errors that lead to faster convergence.
D. Evaluation of scalability and efﬁciency
To evaluate B
IGPT on a larger dataset with multiple lan-
guages, we run it on the PGAS dataset. As we have to
manually judge the correctness, we carry out a samplinginspection to make it feasible. We randomly pick 270 programsand B
IGPT retrieves the best possible translation from the
dataset for each of these programs. As we spent up to 15
Fig. 9: Relationship between the efﬁciency and database scale
minutes on each result to make the judgement as accurate
as possible, we had to limit the labeling process due to timeand human resources.To show the advantage of B
IGPT, we
compare its results to B IGPT WORD 2VEC and B IGPT CODE 2VEC.
In Tables IV, we observe that B IGPT can successfully
retrieve the correct translation for 58.8% programs among fourlanguages. Considering B
IGPT is positioned as a translation
assistant system, the accuracy is considerable. Translationtasks that cannot be fully automated can still be assisted by theresults returned by B
IGPT. This result shows that it is feasible
to scale B IGPT to more data volume. Compared to the other
program representations, our novel representation performssigniﬁcantly better. W2V does not contribute much to theresults and introduces extra model training time compared tothe simple BoW model. In B
IGPT, we retain BoW to trade-off
response time for a slight decrease of accuracy. Code2vec alsoconsiders the structure of programming languages. However,their model can only be trained within the same language,which limits its performance in the cross-language setting.
We also compare the average runtime of each translation
retrieval task between RPT and B
IGPT. Table V shows that
both RPT and B IGPT can complete the translation task on
average in 0.2s on the small Java-C# dataset. On the larger
PGAS dataset, B IGPT can retrieve the translation within 2.57s.
We further investigate how the runtime increases with the scaleof the database. We randomly pick 20%, 40%, 60%, and 80%of the PGAS dataset and run the retrieval process on thesesubsets. For each subset, we retrieve translation candidates
176TABLE IV: Comparison of program accuracy on the PGAS dataset
Source language JS Python Java C++
Target language C++ Python Java JS C++ Java JS Python C++ JS Python Java
BIGPT CODE 2VEC 51.0% 32.0% 40.2% 41.9% 43.4% 43.1 50.0% 38.6% 63.4% 58.9% 47.5% 64.4%
BIGPT WORD 2VEC 61.4% 53.9% 60.2% 47.9% 54.7% 48.3% 59.1% 57.1% 67.7% 64.0% 63.6% 66.9%
BIGPT 61.4% 52.3% 59.8% 48.4% 54.2% 47.9% 60.6% 57.5% 69.1% 64.0% 63.5% 67.0%
TABLE V: Efﬁciency of B IGPT (runtime per retrieval)
MethodJava-C# PGAS
(39,797 matched methods) (2,023,546 methods/functions)
RPT 0.20s 5.95s
BigPT 0.20s 2.57s
for 1,000 randomly picked input programs and calculate the
average runtime per retrieval task. Including the results on thewhole dataset, all the results are shown in Figure 9. B
IGPT
is here signiﬁcantly faster than RPT because it ﬁlters moreirrelevant programs due to its advanced QTM. And the runtimeis slowly growing with the database scale with an approximateexponential pattern. In addition, we measured the efﬁciencyduring user interaction. The average response time of B
IGPT
to each intermediate user feedback is 9.4ms, which shows that
BIGPT can respond to corrections in real-time.
Scope of application. Theoretically, B IGPT can be applied to
all static and dynamic imperative languages. According to ourexperiments, B
IGPT is more effective in ﬁnding translation
candidates for grammatically similar programming languages,such as C++ and Java. Furthermore, when the target languageis a low-level programming language, such as C++, the ac-curacy is generally higher than Python, which is a high-leveldynamic programming language.
Failed cases. There are cases where our approach fails:
•Programs with special structures that do not exist in the
target language. For example, deterministic destruction andpointer arithmetic in C++ cannot be translated to Java.
•Programs with APIs that do not exist in the target languagecannot be translated.
•Short programs, i.e., fewer than 3 lines, are highly ambigu-ous and lead to more candidates with similar scores.
VIII. C
ONCLUSION
We presented a novel interactive cross-language code re-
trieval system that can assist program translation by reusingBig Code. We propose a novel cross-language program repre-sentation with a QTM that can learn a succinct but informativefeature vector to retrieve the possible translation of an inputraw program. Our querying and retrieving mechanism makesthe system scalable and efﬁcient on Big Code. Further, thissuccinct representation can be easily adapted to user correc-tions for interactive retrieval improvements. Our experimentsshow that B
IGPT outperforms existing solutions and requires
no parallel training dataset and additional user input.
Limitations. Although B IGPT’s performance is promising, it
still has some limitations. A translation task cannot be assisted
if there is no potential translation for any subset of the inputcode inside the database. Besides, the quality of the data canaffect the results. For large program inputs, a postprocessingstep for veriﬁcation might be necessary. B
IGPT cannot work
well on functional programming languages like Haskell andErlang because they typically do not contain control ﬂowelements.
Future work. First and foremost, there is a potential re-
search direction on creating more convenient and intuitive
user interfaces that allow users to make relevant correctionsto translation suggestions and enable the system to convergefaster to the desired result. Second, it might also be interestingto look into other forms of user interaction that do not requirethe user to provide corrections in the target language. Third,there is still room to explore post-processing of cross-languagecode retrieval and aggregation of retrieval results to obtaintranslations for larger programs. Finally, developing a usererror model that can imitate user errors in code retrievalseems like a promising research direction to better assess thelimitations of retrieval-assisted programming.
A
CKNOWLEDGMENT
This work was funded by the German Ministry for Ed-
ucation and Research as BIFOLD - Berlin Institute for theFoundations of Learning and Data (ref. 01IS18025A and ref.01IS18037A).
R
EFERENCES
[1] Branching statements. website. https://docs.oracle.com/javase/tutorial/
java/nutsandbolts/branch.html [Online; accessed 20-April-2021].
[2] codase. website. http://www.Codase.com [Online; accessed 20-April-
2021].
[3] Java2csharp. website. https://sourceforge.net/projects/j2cstranslator/
[Online; accessed 20-April-2021].
[4] krugle. website. http://www.krugle.com [Online; accessed 20-April-
2021].
[5] Public git archive. website. https://github.com/src-d/datasets/tree/master/
PublicGitArchive [Online; accessed 20-April-2021].
[6] Farouq Al-Omari, Iman Keivanloo, Chanchal K. Roy, and Juergen
Rilling. Detecting clones across microsoft .net programming languages.
In19th Working Conference on Reverse Engineering (WCRE). IEEE
Computer Society, 2012.
[7] Miltiadis Allamanis, Marc Brockschmidt, and Mahmoud Khademi.
Learning to represent programs with graphs. In 6th International
Conference on Learning Representations (ICLR). OpenReview.net, 2018.
[8] Miltiadis Allamanis, Hao Peng, and Charles A. Sutton. A convolutional
attention network for extreme summarization of source code. InProceedings of the 33nd International Conference on Machine Learning(ICML). JMLR.org, 2016.
[9] Uri Alon, Meital Zilberstein, Omer Levy, and Eran Yahav. A general
path-based representation for predicting program properties. In Proceed-
ings of the 39th ACM SIGPLAN Conference on Programming LanguageDesign and Implementation (PLDI). ACM, 2018.
[10] Uri Alon, Meital Zilberstein, Omer Levy, and Eran Yahav. code2vec:
learning distributed representations of code. Proc. ACM Program. Lang.,
3(POPL):40:1–40:29, 2019.
[11] Binger Chen and Ziawasch Abedjan. Rpt: Effective and efﬁcient retrieval
of program translations from big code. In 43nd International Conference
on Software Engineering, Companion V olume (ICSE). ACM, 2021.
177[12] Xinyun Chen, Chang Liu, and Dawn Song. Tree-to-tree neural networks
for program translation. In 6th International Conference on Learning
Representations (ICLR). OpenReview.net, 2018.
[13] Xiao Cheng, Zhiming Peng, Lingxiao Jiang, Hao Zhong, Haibo Y u,
and Jianjun Zhao. Mining revision histories to detect cross-language
clones without intermediates. In Proceedings of the 31st IEEE/ACM
International Conference on Automated Software Engineering (ASE).ACM, 2016.
[14] Xiao Cheng, Zhiming Peng, Lingxiao Jiang, Hao Zhong, Haibo Y u,
and Jianjun Zhao. CLCMiner : Detecting cross-language clones without
intermediates. IEICE Trans. Inf. Syst., 100-D(2):273–284, 2017.
[15] Ido Dagan and Sean P . Engelson. Committee-based sampling for
training probabilistic classiﬁers. In Machine Learning, Proceedings
of the Twelfth International Conference on Machine Learning (ICML).Morgan Kaufmann, 1995.
[16] Behrouz Derakhshan, Alireza Rezaei Mahdiraji, Ziawasch Abedjan,
Tilmann Rabl, and V olker Markl. Optimizing machine learning work-loads in collaborative environments. In Proceedings of the 2020
International Conference on Management of Data (SIGMOD). ACM,2020.
[17] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
BERT: pre-training of deep bidirectional transformers for languageunderstanding. In Proceedings of the 2019 Conference of the North
American Chapter of the Association for Computational Linguistics:Human Language Technologies (NAACL-HLT). Association for Compu-tational Linguistics, 2019.
[18] Timothy Dietrich, Jane Cleland-Huang, and Y onghee Shin. Learning
effective query transformations for enhanced requirements trace re-trieval. In 2013 28th IEEE/ACM International Conference on Automated
Software Engineering (ASE). IEEE, 2013.
[19] Xiaodong Gu, Hongyu Zhang, and Sunghun Kim. Deep code search.
InProceedings of the 40th International Conference on Software Engi-
neering (ICSE). ACM, 2018.
[20] Zack Ives, Yi Zhang, Soonbo Han, and Nan Zheng. Dataset relationship
management. In 9th Biennial Conference on Innovative Data Systems
Research (CIDR). www.cidrdb.org, 2019.
[21] Zhuoren Jiang, Y ue Yin, Liangcai Gao, Yao Lu, and Xiaozhong Liu.
Cross-language citation recommendation via hierarchical representationlearning on heterogeneous graph. In The 41st International ACM
SIGIR Conference on Research & Development in Information Retrieval(SIGIR). ACM, 2018.
[22] Toshihiro Kamiya, Shinji Kusumoto, and Katsuro Inoue. Ccﬁnder: A
multilinguistic token-based code clone detection system for large scalesource code. IEEE Trans. Software Eng., 28(7):654–670, 2002.
[23] Kisub Kim, Dongsun Kim, Tegawend ´e F. Bissyand ´e, Eunjong Choi,
Li Li, Jacques Klein, and Yves Le Traon. Facoy: a code-to-code searchengine. In Proceedings of the 40th International Conference on Software
Engineering (ICSE). ACM, 2018.
[24] Nicholas A. Kraft, Brandon W. Bonds, and Randy K. Smith. Cross-
language clone detection. In Proceedings of the Twentieth International
Conference on Software Engineering & Knowledge Engineering (SEKE).Knowledge Systems Institute Graduate School, 2008.
[25] Jing Li, Aixin Sun, Zhenchang Xing, and Lei Han. API caveat explorer
- surfacing negative usages from practice: An api-oriented interactiveexploratory search system for programmers. In The 41st International
ACM SIGIR Conference on Research & Development in InformationRetrieval (SIGIR). ACM, 2018.
[26] Erik Linstead, Sushil Krishna Bajracharya, Trung Chi Ngo, Paul Rigor,
Cristina Videira Lopes, and Pierre Baldi. Sourcerer: mining andsearching internet-scale software repositories. Data Min. Knowl. Discov.,
18(2):300–336, 2009.
[27] Fei Lv, Hongyu Zhang, Jian-Guang Lou, Shaowei Wang, Dongmei
Zhang, and Jianjun Zhao. Codehow: Effective code search based onAPI understanding and extended boolean model (E). In 30th IEEE/ACM
International Conference on Automated Software Engineering (ASE).IEEE Computer Society, 2015.
[28] Lee Martie, Thomas D. LaToza, and Andr ´e van der Hoek. Code-
exchange: Supporting reformulation of internet-scale code queries in
context (T). In 30th IEEE/ACM International Conference on Automated
Software Engineering (ASE). IEEE Computer Society, 2015.
[29] Tom ´as Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efﬁcient
estimation of word representations in vector space. In 1st International
Conference on Learning Representations (ICLR), 2013.[30] Tomas Mikolov, Ilya Sutskever, Kai Chen, Gregory S. Corrado, and
Jeffrey Dean. Distributed representations of words and phrases andtheir compositionality. In Annual Conference on Neural Information
Processing Systems (NeurIPS), 2013.
[31] Anh Tuan Nguyen, Tung Thanh Nguyen, and Tien N. Nguyen. Lex-
ical statistical machine translation for language migration. In Joint
Meeting of the European Software Engineering Conference and theACM SIGSOFT Symposium on the F oundations of Software Engineering(ESEC/FSE). ACM, 2013.
[32] Anh Tuan Nguyen, Tung Thanh Nguyen, and Tien N. Nguyen. Divide-
and-conquer approach for multi-phase statistical migration for sourcecode (T). In 30th IEEE/ACM International Conference on Automated
Software Engineering (ASE). IEEE Computer Society, 2015.
[33] Anh Tuan Nguyen, Zhaopeng Tu, and Tien N. Nguyen. Do contexts
help in phrase-based, statistical source code migration? In 2016
IEEE International Conference on Software Maintenance and Evolution(ICSME). IEEE Computer Society, 2016.
[34] Liming Nie, He Jiang, Zhilei Ren, Zeyi Sun, and Xiaochen Li. Query
expansion based on crowd knowledge for code search. IEEE Trans.
Serv. Comput., 9(5):771–783, 2016.
[35] V arot Premtoon, James Koppel, and Armando Solar-Lezama. Semantic
code search via equational reasoning. In Proceedings of the 41st ACM
SIGPLAN International Conference on Programming Language Designand Implementation (PLDI). ACM, 2020.
[36] Razieh Rahimi and Azadeh Shakery. Online learning to rank for cross-
language information retrieval. In Proceedings of the 40th International
ACM SIGIR Conference on Research and Development in InformationRetrieval, Shinjuku (SIGIR). ACM, 2017.
[37] V eselin Raychev, Martin T. V echev, and Andreas Krause. Predicting
program properties from ”big code”. In Proceedings of the 42nd Annual
ACM SIGPLAN-SIGACT Symposium on Principles of ProgrammingLanguages (POPL). ACM, 2015.
[38] Steven P . Reiss. Semantics-based code search. In 31st International
Conference on Software Engineering (ICSE). IEEE, 2009.
[39] Baptiste Rozi `ere, Marie-Anne Lachaux, Lowik Chanussot, and Guil-
laume Lample. Unsupervised translation of programming languages.In Annual Conference on Neural Information Processing Systems
(NeurIPS), 2020.
[40] Aishwarya Sivaraman, Tianyi Zhang, Guy V an den Broeck, and Miryung
Kim. Active inductive logic programming for code search. In Pro-
ceedings of the 41st International Conference on Software Engineering(ICSE). IEEE / ACM, 2019.
[41] Shaowei Wang, David Lo, and Lingxiao Jiang. Active code search:
incorporating user feedback to improve code search relevance. InACM/IEEE International Conference on Automated Software Engineer-ing (ASE). ACM, 2014.
[42] Jingfang Xu, Feifei Zhai, and Zhengshan Xue. Cross-lingual information
retrieve in sogou search. In Proceedings of the 40th International
ACM SIGIR Conference on Research and Development in InformationRetrieval (SIGIR). ACM, 2017.
[43] Cong Yan and Yeye He. Synthesizing type-detection logic for rich
semantic data types using open-source code. In Proceedings of the 2018
International Conference on Management of Data (SIGMOD). ACM,2018.
[44] Cong Yan and Yeye He. Auto-suggest: Learning-to-recommend data
preparation steps using data science notebooks. In Proceedings of the
2020 International Conference on Management of Data (SIGMOD).ACM, 2020.
[45] Pengcheng Yin, Graham Neubig, Miltiadis Allamanis, Marc
Brockschmidt, and Alexander L. Gaunt. Learning to representedits. In 7th International Conference on Learning Representations
(ICLR). OpenReview.net, 2019.
[46] Xingquan Zhu, Peng Zhang, Xiaodong Lin, and Y ong Shi. Active
learning from data streams. In Proceedings of the 7th IEEE International
Conference on Data Mining (ICDM). IEEE Computer Society, 2007.
178