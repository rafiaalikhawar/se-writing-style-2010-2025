CodeFill: Multi-token Code Completion by Jointly Learning
from Structure and Naming Sequences
Maliheh Izadi
M.Izadi@tudelft.nl
Delft University of Technology
Delft, NetherlandsRoberta Gismondi
R.Gismondi@student.tudelft.nl
Delft University of Technology
Delft, NetherlandsGeorgios Gousios
G.Gousios@tudelft.nl
Delft University of Technology
Delft, Netherlands
ABSTRACT
CodecompletionisanessentialfeatureofIDEs,yetcurrentauto-
completers are restricted to either grammar-based or NLP-based
single token completions. Both approaches have significant draw-
backs: grammar-based autocompletion is restricted in dynamically-
typed language environments, whereas NLP-based autocompleters
struggletounderstandthesemanticsoftheprogramminglanguage
and the developerâ€™s code context.
Inthiswork,wepresent CodeFill,alanguagemodelforautocom-
pletionthatcombineslearnedstructureandnaminginformation.
UsingaparallelTransformerarchitectureandmulti-tasklearning,
CodeFillconsumessequencesofsourcecodetokennamesandtheir
equivalent AST token types. Uniquely, CodeFill is trained both for
single-tokenandmulti-token(statement)prediction,whichenables
it to learn long-range dependencies among grammatical and nam-
ing elements. We train CodeFill on two datasets, consisting of 29M
and 425M lines of code, respectively. To make the evaluation more
realistic, we develop a method to automatically infer points in the
source code at which completion matters. We compare CodeFill
against four baselines and two state-of-the-art models, GPT-Cand
TravTrans+. CodeFill surpasses all baselines in single token predic-
tion (MRR: 70 .9% vs. 66.2% and 67.8%) and outperforms the state of
theartformulti-tokenprediction(ROUGE-L:63 .7%vs.52.4%and
59.2%,forğ‘›=4tokens).Wepubliclyreleaseoursourcecodeand
datasets.
CCS CONCEPTS
â€¢Software and its engineering â†’Software notations and
tools.
KEYWORDS
AutomaticCodeCompletion,Transformers,Multi-TaskLearning,
Types, Dynamically-typed Languages
ACM Reference Format:
Maliheh Izadi, Roberta Gismondi, and Georgios Gousios. 2022. CodeFill:
Multi-tokenCodeCompletionbyJointlyLearningfromStructureandNam-
ing Sequences. In 44th International Conference on Software Engineering
(ICSE â€™22), May 21â€“29, 2022, Pittsburgh, PA, USA. ACM, New York, NY, USA,
12 pages. https://doi.org/10.1145/3510003.3510172
This work is licensed under a Creative Commons Attribution-NonCommercial 
International 4.0 License.
ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
Â© 2022 Copyright held by the owner/author(s).
ACM ISBN 978-1-4503-9221-1/22/05.
https://doi.org/10.1145/3510003.35101721 INTRODUCTION
Automatic code completion (also called autocompletion) is the task
ofcompleting sourcecodestatementsby predictingwhatthede-
veloper would write given the current context. It helps developers
finish theirprogramming tasksfaster bydecreasingthe typingef-
fort and saving keystrokes, correcting typographical errors, and
enablingthemtoexploreAPIsinacontext-sensitivemanner[ 5].Au-
tocompletion has therefore emerged as one of the most prominent
features in Integrated Development Environments (IDEs).
To support autocompletion, current IDEs exploit the regular
structureofprogramminglanguages.Forexample,anIDEknows
thatanopeningparenthesischaracter(â€˜ (â€˜)atafunction-callposi-
tionmustbefollowedbyenoughargumentstomatchthefunctionâ€™s
arity.Itcanthereforeproposeargumentnamesforvariablesthat
are in scope. The availability of types in the host programming
languagehelpsincreasetheprecisionof suggestions;continuing
with the example above, the IDE will only propose variable names
for variables whose types match the function argument. Recent au-
tocompletion systems also take into account past completions [ 43]
andanalyzelargecodebases[ 9]toranksuggestionsaccordingto
their past popularity. Despite the best efforts of researchers and
IDEdevelopers,developersfindrule-basedcodecompletionmecha-
nisms lacking. Ranking suggestions based on alphabetical or usage
frequency (or even the suggestion list length [ 23]) neglects the
current context, leading to unrelated recommendations [ 3]. These
problems are exacerbated in dynamicallytyped language settings,
as the IDE is lacking significant information to provide accurate
suggestions.
To mitigate rule-based autocompletion issues, researchers have
proposedstatistical[ 17,37]andlearning-based[ 6,17,29,31,32,51]
autocompletion models. Motivated by the naturalness hypothe-
sis [19], learning-based models treat source code as natural lan-
guage text, hence code completion becomes an instance of the
well-studied text completion problem. However, treating source
code as text deprives learning-based models of important code
structureandsemanticinformation[ 18].Moreover,theopen-ended
nature of code leads to extremely large prediction spaces due to
developers constantly inventing identifier names [24].
In an illuminating study, Hellendoorn et al. [ 18] identified a set
of issues with current research in code completion. Initially, the
currentapproachofevaluatingaccuracyasmaskedtokenpredictiondoesnotreflecthowautocompletionisused;developersonlytrigger
autocompletion after specific, and certainly not arbitrary, pointsin a programâ€™s syntax (e.g., after an opening parenthesis). Thus,treating all tokens equally masks the fact that some tokens (e.g.,
punctuation)aremucheasiertopredictthanothers(e.g.,identifiers).Moreover,mostapproaches(especiallylearning-basedones)donot
4012022 IEEE/ACM 44th International Conference on Software Engineering (ICSE)
ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA Izadi, et al.
accountfornamescomingfromdependencies,whichdeprivesthem
of important context.
In this work, we propose CodeFill, a novel learning-based ap-
proachthataimstoaddresstheproblemsidentifiedabove.CodeFill
borrows from the bimodality hypothesis [ 12] to model source code
inputs. Specifically, CodeFill exploits that information is conveyed
bysourcecodethroughtwochannels;the naturallanguagechan-
nel(variable names, functions, etc.), and the code structure channel
(inheritance, containment, etc.). Inputs are fed into the model si-
multaneouslyasbothsequencesoftokenvalues,whichenableitto
learnrelationshipsamongtokenvalues,and,uniquely,sequencesoftokentypes,whichenableittolearnassociationsbetweensyntactic
elements.CodeFillisthenaskedtopredicteitherthevalueorthe
type of the next ntokens. To enable CodeFill to learn name depen-
dencies across longer ranges, we also train it with an additional
task,multi-tokenstatementcompletionatthevaluelevel.TheinputtokennamestoCodeFillisencodedwithByte-PairEncoding(BPE),
whichenablesCodeFilltobothcompresstheinputnamespaceand
generate names that are not in the input vocabulary. To present
suggestions relevant to the developerâ€™s context, CodeFill includes
apost-processingstepthatre-ranksthepredictionsbasedonthe
context visible to the model at the completion point. CodeFill is in-
stantiatedasasetofthreeTransformers(GPT2-based)trainedwith
soft parameter sharing Multi-Task Learning (MTL) setting. Eachtransformer models one of the three tasks, namely token value,
token type, and multi-token prediction; a joint loss function across
all three tasks updates the weights of all three model components.
During eachepoch, themodelis trainedon onetask accordingto a
configurable task-picking policy. Our target language is Python, to
bothdemonstratetheefficiencyofthemodelwhentypeinforma-
tion is missing and also make our work comparable with the state
of the art.
We pit CodeFill against four baseline models and two the-state-
of-the-art models, namely GPT-C [ 49] and TravTrans+ [ 25]. We
use two deduplicated datasets: the ETH150K dataset (deduplicated:
PY117K) and a manually collected dataset consisting of practically
all non-forked Python repositories on GitHub ( PY1690K). We eval-
uate all models on two tasks: Token-Level andStatement-Level Pre-
dictions (TLP and SLP). For TLP, we evaluate for i) next token
prediction (TLP-A), ii) next token type prediction (TLP-B), iii) next
tokenvalueprediction(TLP-C).Toensurethattheevaluationset-
ting reflects real-world use of autocompletion, we also evaluatecompletions after specific syntactic elements, e.g., a dot
.or an
AWAITkeyword (TLP-D). We devise an algorithm to identify those
syntacticelements( cardinalpoints )automaticallygivenacorpus.
We use top-1 Accuracy and the Mean Reciprocal Rank (MRR) as
evaluationmetrics.FortheSLPtask,weassessthemodelsonstate-
mentcompletionwith ğ‘›tokensandwecomparethemusingME-
TEORandROUGE-Lmeasures.Toshowthateachcomponentin
the CodeFill model is necessary, we perform an ablation study.
The results demonstrate that CodeFill outperforms all the com-
peting approaches in alltasks. Indicatively, for each of the TPL-A,
TPL-B, TPL-C, and TPL-D evaluation tasks, CodeFill achieves a
stateoftheartMRRof81 .7%,87.2%,69.5%,70.2%whileTravTrans+,
a current state of the art, scores 79 .4%, 83.6%, 63.8%, and 66.2%,
respectively. In the SLP evaluation task, for completing statements
with four tokens (the average completion length in our datasets)CodeFillobtains70 .2%and63.8%fortheMETEORandROUGE-L
metricsrespectively,andthussignificantlysurpassesTravTrans+
(64.5% and 52.4%).
The main contributions of this work are:
â€¢CodeFill, a model that unifies learning of structural and
name-based information for the autocompletion task.
â€¢An implementation of CodeFill, including training proce-
dures, for the Python programming language. We make our
code and datasets available.1
â€¢An extensive evaluation of CodeFill against four baseline
models and two state-of-the-artapproaches, demonstrating
its superior performance.
2 BACKGROUND AND RELATED WORK
In this section, we briefly review the background work relating to
ourapproach.Then,wepresentthemainapproachestoautocom-
pletion, including the baselines we used for comparison.
2.1 Language Models and Transformers
Statistical Language Modeling (LM) is the task of developing aprobabilistic model for predicting the next tokens in a sequencegiven its preceding tokens, i.e., the context [
14]. This context for
simplerLMsisashortsequenceofwords,whileitcanbesentences
or paragraphs for larger models [ 46]. LMs are either used without
modification, e.g., in a text generation task, or used inside a down-
stream task which requires language understanding. Programming
languages also contain predictable statistical properties which can
be learned using LMs [19].
Recently, NeuralLMs havegained popularity dueto their supe-
rior performance and generalization capabilities [ 14,35]. Neural
LMs address the n-gram data sparsity problem through param-eterization of words as vectors [
26]. A real-valued vector (word
embedding)is usedtorepresent eachword inavector space.This
representationofwordsislearnedbasedontheirusage.Thisallows
wordswithasimilarmeaningtohaveasimilarrepresentation.Note
thattraditionalstatisticalLMswerenotabletoachievethislevel
of generalization [ 47]. Moreover, the distributed representation ap-
proachmakesiteasierfortheembeddingrepresentationtoscale
with the vocabulary size. This is specifically useful with source
code, where the vocabulary size can be unlimited due to the use of
arbitrary identifiers. Initially, feed-forward neural network mod-els, then Recurrent Neural Networks (RNNs) and next, networks
withlong-termmemory,suchasLongShortTermMemory(LSTM)
networks were used.
Mostrecently,therehavebeensignificantimprovementswith
the introduction of self-attention architectures in the Transformer
which is a sequence-to-sequence architecture for transforming a
given sequence of elements to another form [ 53]. Attention enable
Transformerstofocusonselectivepartsofaninput,thusgenerating
morerelevantoutputs[ 34].Transformersoutperformpreviousdeep
models such as RNNs and LSTMs on multiple NLP tasks [ 53]. A
Transformer consists of two main components, an encoder, and
a decoder. GPT-2 introduced by OpenAI2, is a large generative
Transformer-based LM trained on a dataset of 8M web pages [ 39].
1https://github.com/saltudelft/codefill
2https://openai.com/
402CodeFill: Multi-token Code Completion by Jointly Learning from Structure and Naming Sequences ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
GPT-2 has been successfully exploited for various NLP and source
code analysis tasks [10, 16, 28, 49].
2.2 Multi-Task Learning
Multi-Task Learning (MTL) is a model training technique that com-
bines multiple tasks and a joint loss function, with the goal of
maximizingperformanceononeorallofthetasks.MTLenables
knowledgetransferacrossrelatedtasksandimprovesgeneraliza-
tionbyleveragingthedomain-specificinformationcontainedinthe
trainingsignalsofrelatedtasks[ 11].AnMTLmodelcapturesthe
commonfeaturesamongallthetasksthroughsharinghiddenlayers
among them. MTL has been applied successfully in both NLP [ 13]
and source code analysis [ 32,33]. There are two approaches to
jointlytrainmodelsusingMTL, hard-parameter andsoft-parameter
sharing. In the former, the hidden layers are shared between alltasks while keeping several task-specific output layers. For the
latter, each task has its own model with its own parameters. How-
ever, the distance between them is regularized to encourage the
parameters to be similar. In the soft-parameter sharing case, train-
ing can happen either sequentially (one task per training round) or
alternatively (one task per epoch).
2.3 Related Work
Autocompletion is an active research area for both practitioners
and researchers. Below, we review the latest approaches to auto-
completion.
2.3.1 Conventional Autocompletion. Traditionally,autocompleters
usedheuristicrulesstatictypeinformation[ 20],similarcodeexam-
ples [9], and program history data [ 42] for suggesting completions.
For instance, IDEs conventionally return a list of type-checked
names either based on the order of alphabet or usage frequency.
2.3.2 Statistical LMs and Grammar-based Models. Several studies
use statistical LMs for modeling source code [ 17,19,37,52]. Tu
et al. [52] built upon an n-gram model using a cache mechanism
tocapturelocalityinsourcecode.HellendoornandDevanbu[ 17]
improved the n-gram model by exploiting various techniques in-
cluding nested scopes, locality, and unlimited vocabulary. Raychev
etal.[40]proposedaprobabilisticmodelbasedondecisiontrees
and domain-specific grammars. Researchers also studied the useof syntactic structure through exploiting probabilistic graphical
models.Allamanisetal.[ 4]employprobabilisticcontext-freegram-
mars, while Raychev et al. [ 8,40,41] use probabilistic higher order
grammars to this end.
2.3.3 Deep Learning for Autocompletion. Recently, deep neural
networks suchas RNNs,LSTMs and Transformers are beingeffec-
tivelyusedformodelingsourcecode[ 6,17,24,25,29].In2018,Li
et al. [29] proposed a pointer mixture model to mitigate the Out-
Of-Vocabulary (OOV) problem. They trained two LSTM models on
types and tokens. Karampatsis et al. [ 24] presented a large-scale
open-vocabulary neural LM. They incorporated BPE, beam search,and cache mechanism to address the OOV problem. Most recently,
Kim et al. [ 25], incorporated the syntactic structure of trees into
their Transformer-based model to better learn from source code.2.3.4 Multi-token Autocompletion. Although most research on
codecompletionisfocusedonsingle-tokenprediction,severalstud-iesaimedtocompleteentirestatementsorblocksofcode[
36,49,54,
55]. Yang et al. [ 55] proposed PCCand introduced an intermediate
representation for source code, to put tokens into groups usinglexeme and variable relative order. Nguyen et al. [
36] proposed
AUTOSC to combine program analysis and software naturalness
and fill in a partially completed statement with frequent and valid
recommendations. Svyatkovskiy et al. [ 49] recently proposed a
GPT-2basedmulti-lingualmodel, GPT-C,forcompletinglines.Wen
etal.[54]introduced FeaRSwhichrecommendsthenextmethod
given the current code in an IDE using implementation patterns
learned through mining open source projects.
2.3.5 MTL for Autocompletion. MTLhasbeenusedinvariousNLP-
related tasks [ 45,48,56]. Recently, it has also been employed for
programminglanguageprocessingtasks.Liuetal.[ 32,33]proposed
twoapproachesbasedonMTLforautocompletion.Inthefirststudy,
theauthorsusedaTransformer-XLandanRNNforpredictingnext
token type and value [ 32]. They develop a partial AST encoder
and a path2root encoder and use them in their MTL framework. In
their second study, Liu et al. [ 33] pre-train their model with hybrid
objective functions for code understanding and code generation
tasks. Next, they fine-tune it on code completion. The pre-training
tasksaremaskedbidirectionalLM,nextcodesegmentprediction,
and unidirectional LM. The fine-tuning tasks are unidirectional
masked LM, and unidirectional LM.
2.3.6 Practical Aspects of Autocompletion. Hellendoorn et al. [ 18]
claimtheaccuracyofautocompletersevaluatedonsyntheticdata
can drop on real-world data. Aye et al. [ 7], trained models on real-
world code completion examples of an internal dataset (Facebook).
They showed that models trained on data distributions that are
closer to those of where the model will be deployed can outper-
form models trained on committed source code in public reposi-
tories.Svyatkovskiyetal.[ 51]integratedPythia,anLSTMmodel,
toIntelliCode, an extension to Microsoft VSCode IDE. In afollow-
up study [ 49], they introduced IntelliCode Compose as a general-
purpose multilingual autocompletion using Transformers. The im-
proved modelpredicts sequences ofcode tokens,generating up to
entirestatements.IntelliCodeComposeisintegratedintotheMi-
crosoftVSCodeIDE.Finally,Svyatkovskoyetal.[ 50]implemented
and evaluated several neural code completion models, which offer
varying trade-offs in terms of memory, speed,and accuracy. Com-
mercial autocompletion tools, such as TabNineandGitHub Copilot
also exist, but very little technical information has been shared
about them.
2.4 Baselines
We include six recent models as baselines to provide a comprehen-
siveevaluation.Forallbaselines,weusethereplicationpackages
provided bythe authorsand set theparameters as definedin each
respective study. For the statement level prediction task, we modi-
fied the output layer of the baselines to predict up until the end of
a statement.
N-gram+LSTM(FSE,2017) :Hellendoornetal.[ 17]claimthat
a well-engineered and simple approach (n-gram based language
403ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA Izadi, et al.
models)canprovidebetterperformancethanmorecomplexmodels
(deep neural networks). The authors show that the combination
of an n-gram and LSTM-based model outperforms the rest of their
models.
PointerMixture(IJCAI,2018) :Lietal.[ 29],proposeapointer
mixture model to address the OOV problem. They also try to in-
corporatestructuralinformationintheirmodelsbytrainingtwo
models (token types and values) separately.
T-XL + Bi-LSTM (ICPC, 2020) : Liu et al. [ 32,33], propose
two models based on the MTL technique. The first study uses
Transformer-XL and a Bi-LSTM to train two models for tokens
and AST paths for dynamically-typed languages such as Python.
The second study by the same group presents a pre-trained lan-
guage model which is fine-tuned for code completion. The authors
usestaticanalysisandtypeannotationsfortheirtypeprediction
task,forJava.Wecompareagainstthefirstmodelonly,asitmost
closely matches our setup.
OpenVocab(ICSE,2020) :ToaddresstheOOVproblem,Karam-
patsis et al. [ 24] present a BPE-based language model. We include
it here for completeness, even though their model is not tuned for
autocompletion.
IntelliCode Compose (FSE, 2020) : Svyatkovskiy et al. [ 49]
propose a general-purpose, multi-lingual autocompletion support-
ingmulti-tokenstatementcompletion.TheytrainaGPT-2model
on1.2ğµLOCwritteninPython,C#,TypeScript,andJavaScript.This
toolisdeployedasacloud-basedwebserviceandusesclient-side
cachingandparallelimplementationtospeedupthe predictions.
As the source code is not publicly available, we trained a GPT-2
model for source code and did our best to adhere to the settingsreported in the study. As the focus of our study is mono-lingual,
we only train this model on Python code.
TravTrans+(ICSE,2021) :Kimetal.[ 25]proposeatransformer-
basedapproachwhichexploitsASTpaths.Weusetheirbestmodel,
TravTrans+, as the state of the art in our evaluation.
3 APPROACH
TheCodeFillpipelinecomprisestwomainphases; pre-processing,
model training. Figure 1 presents the overall workflow. Initially,CodeFill pre-processes, tokenizes and converts the input sourcecode to equivalent syntax token sequences. Training consists of
two main phases pre-training with 3 tasks (token sequence type
and name completion, statement completion) and fine-tuning on 2
tasks(nameandstatementcompletion).Forbothstages,CodeFill
uses soft-parameter sharing MTL to learn from different represen-
tationsofsourcecode.Atevaluationtime,Codefillalsore-orders
recommendations based on their type and the visible context.
Inthefollowingsection,wepresenthowtheproposedapproach
works in detail.
3.1 Pre-processing
Duringpre-processing,CodeFillconvertstheinputprogramfilesto
an equivalent format where keywords and identifiers are swapped
with their AST equivalents. The algorithm starts by removing com-
mentsections,blankspaces,andblanklines.Itthenextractsthelist
ofmodules,libraries,andtheir aliasesusingthePython ASTlibrary.
Those are stored in a dictionary and, using it, CodeFill replaces all
3UHSURFHVVLQJ 0RGHO7UDLQLQJ 3RVWSURFHVVLQJ
5HUDQNWKHILQDO
UHFRPPHQGHGOLVW
3UHWUDLQD*37EDVHG07/PRGHOZLWKWDVNVRQVRXUFH
FRGHIURPVFUDWFK2QHWRRQHFRQYHUVLRQ
RISURJUDPWRNHQVWRWKHLUFRUUHVSRQGLQJKLJKOHYHOW\SHV5HPRYHGXSOLFDWH
SURJUDPILOHV
&ROOHFWLQJGDWD
IURP*LW+XE
)LQHWXQHWKH*37EDVHG07/
PRGHOZLWKWDVNV
7RSQ
UHFRPPHQGHG
FRPSOHWLRQV
Figure 1: CodeFill Workflow
1deftransform(node, ctx):
2node=qual_names.resolve(node)
3node=CallTreeTransformer(ctx).visit(node)
4 returnnode
Type Value #Line Position
RETURN return 4 1
NAME node 4 2
Figure2:Samplecodesnippetandtheextractedinformation
theiroccurrences incodewith theirrespectivetypes (i.e., MODULE,
LIBRARY, and ALIAS).
CodeFill also pre-processes and tokenizes the input source code.
For each line, it reads the tokenized information and stores four
typesofinformationabouteachtokennamely(1)itsvalue,(2)its
type,(3)itslinenumber,and(4)itspositionintheline.Forinstance,
for the statement return node in Figure 2, it stores two tokens as
shown in the table following the code example. Moreover, variable
visibility information(e.g.,global vs. localvariables), is maintained,
to differentiate between different name usages in the same context.
To address the OOV problem, CodeFill uses a BPE-encoded
name representation. Exploiting word segmentation, BPE itera-
tivelymergesthemostfrequentlyoccurringcharactersequences.
Prior to applying BPE encoding, and similarly to other studies [ 21,
22,49], CodeFill normalizes the input strings by replacing string,
andnumericliteralswithrespectivespecialtokens,i.e., STRINGand
NUMBER.
AuniquecharacteristicofthePythonlanguageisthatindenta-
tiondefinescodeblocks;itisthereforeimportantforsourcecode
models to learn to encode indentation as part of their learned rep-
resentation.Todoso,CodeFillstoresthepositioningofindentation
markers. For the first line with an indentation, it adds a special to-
ken/angbracketleftğ¼ğ‘ğ·ğ¸ğ‘ğ‘‡ /angbracketrightatthebeginningofthegivenline.Itpassesthrough
the following lines with the same indentation, to reach the next
indentation or a dedentation position, at which point it adds a
respective /angbracketleftğ¼ğ‘ğ·ğ¸ğ‘ğ‘‡ /angbracketrightor/angbracketleftğ·ğ¸ğ·ğ¸ğ‘ğ‘‡ /angbracketrighttoken.
Thepre-processingstepresultsintwofilesforeachinputsource
codefile;(1)onecontainingsequencesoftokennamesminusthe
comments and extra blank lines, and (2) one containing sequences
of token types. Both are fed into CodeFill as two different but
corresponding representations of source code. Figure 3 shows a
404CodeFill: Multi-token Code Completion by Jointly Learning from Structure and Naming Sequences ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
1# Raises an error when the required variable is missing
2defrequired_env(var):
3
4value=os.environ.get(var)
5 ifvalue isNone:
6 raise RuntimeError("Var is required to start the service.")
7 returnvalue
1defrequired_env(var):
2value=os.environ.get(var)
3 ifvalue isNone:
4 raise RuntimeError("STRING")
5 returnvalue
1DEF FUNCTION_NAME(LOCAL_VARIABLE): EOS
2INDENT LOCAL_VARIABLE =LIB.MODULE.FUNCTION_NAME(LOCAL_VARIABLE)
EOS
3IF LOCAL_VARIABLE IS NONE: EOS
4INDENT RAISE ERRORTOKEN("STRING") EOS
5DEDENT RETURN LOCAL_VARIABLE EOS
Figure3:Anexamplecodesnippetanditsconvertedversion
samplefunctionanditscorrespondingtypeinformationwiththe
correct indention.
3.2 Model Training
In this phase, CodeFill learns from two granularity levels; token-
and statement-level completions with three simultaneous tasks,
namely (1) next Token ValuePrediction (TVP), (2) next Token Type
Prediction (TTP), and (3) Statement Completion (SC). Model train-
ing follows a two-stage process; First, a generic language modeling
objective is used onthe unlabeled data to learnthe initial parame-
ters.Then,theseparametersareadaptedtothetargettasksusing
the corresponding objective. Thus, while pre-training, CodeFilllearns from all three tasks while fine-tuning is restricted to the
TVP and SC tasks. The reason for excluding the TTP task is that
the number of types for all the program files is limited. Hence, the
model quickly learns how to properly predict these type sequences
(i.e., learns an effective representation of the Python grammar),
eliminating the need for further fine-tuning.
Themainneuralnetworkarchitectureforalltasksisbasedon
theGPT-2Transformerwith ğ¿layers.CodeFillusesthreedistinct
GPT-2 transformers, each with its own input and training objec-
tive. The models are initialized with random weights. Transformer
blocksincludeself-attentionlayer,feed-forwardneuralnets,and
anormalizationlayer.Self-attentionblocksidentifywhichtokens
tofocuson.Feed-forwardneuralnetsconsistofaninputlayerto
acceptinformation,hiddenlayerstocapturethehiddencorrelations
between each data point, and finally, an output layer to transmit
information. The parameters are transferred to the next decoder in
the stack after being regularised (with ğ‘™2 norm) to be similar to the
respective decoderâ€™sparameters. CodeFill uses softmaxactivation
functionintheoutputlayertogenerateprobabilitydistributions
over the vocabulary.
Totrainthemodeltopredictasequenceoftokens, {ğ‘£ğ‘¡}âŠ‚ğ·,ğ‘¡âˆˆ
[1,...,ğ‘], withğ·as the vocabulary, and ğ¶as the existing code
context, CodeFill estimates the following conditional probabilitydistribution, ğ‘ƒ:
ğ‘ƒ(ğ‘£0,...,ğ‘£ ğ‘|ğ‘0,...,ğ‘ ğ‘‡)=ğ‘/productdisplay.1
ğ‘–=1ğ‘ƒ(ğ‘£ğ‘–|ğ‘0,...,ğ‘ ğ‘‡,ğ‘£0,...,ğ‘£ ğ‘–âˆ’1).(1)
Weuseastandardlanguagemodelingobjective,predictingthe
next token given a context, and maximize the following likelihood
based on our unsupervised corpus of tokens. In Equation 2, ğ‘šis
the length of the predicted sequence of code token values and ğœƒis
the set of parameters that is learned through stochastic gradient
descent optimization to model ğ‘ƒ[44].
ğ¿(ğ‘‰)=/summationdisplay.1
ğ‘–logğ‘ƒ(ğ‘£ğ‘–|ğ‘0,...,ğ‘ ğ‘‡,ğ‘£ğ‘–âˆ’ğ‘š,...,ğ‘£ ğ‘–âˆ’1;ğœƒ).(2)
Ineachlayer,multi-attentionheadsareusedtoaggregatetheout-
put of the previous layer for each transformer block. Multi-headed
self-attention is applied over the input context tokens followed by
position-wisefeed-forwardlayerstoproducetheoutputdistribu-
tion.
â„0=ğ¶ğ‘Šğ‘’+ğ‘Šğ‘, (3)
â„ğ‘™=ğ‘¡ğ‘Ÿğ‘ğ‘›ğ‘ ğ‘“ğ‘œğ‘Ÿğ‘šğ‘’ğ‘Ÿ _ğ‘ğ‘™ğ‘œğ‘ğ‘˜(â„ğ‘™âˆ’1),ğ‘™âˆˆ[1,...,ğ¿],(4)
ğ‘ƒ(ğ‘£ğ‘¡)=ğ‘ ğ‘œğ‘“ğ‘¡ğ‘šğ‘ğ‘¥(â„ğ¿ğ‘Šğ‘‡
ğ‘’),ğ‘¡âˆˆ[0,...,ğ‘] (5)
whereğ¶isthecontextvectoroftokens, ğ¿isthenumberoflayers, ğ‘Šğ‘’
is the token embedding matrix, and ğ‘Šğ‘is the position embedding
matrix.
For training with MTL, CodeFill uses the alternative training
strategy,whichaimstopreventcatastrophic forgetting(asopposed
to the sequential strategy). With a probability of 20%, 40%, and
40% for each of the TTP, TVP, and SC tasks, respectively, CodeFill
picks a random task for each epoch. TTP requires fewer epochs as
itsvocabularyisfairlylimited.Furtheron,forTVPandSCtasks,
CodeFillusesbeamsearchtoidentifythemostlikely(sub-)token
sequences.
Lossis sharedamong alltasks. Duringpre-training, theparam-
eters are tuned to minimize the absolute minimum of the crossentropy losses among the three pre-training tasks, namely, TVP,
TTP, and SC (Equation 6). When fine-tuning, only TVP and SC
losses are used.
ğ¿ğ‘œğ‘ ğ‘  ğ‘“ğ‘– ğ‘› ğ‘ ğ‘™ =|min(ğ¿ğ‘œğ‘ ğ‘  ğ‘‡ğ‘‰ğ‘ƒ,ğ¿ğ‘œğ‘ ğ‘  ğ‘‡ğ‘‡ğ‘ƒ,ğ¿ğ‘œğ‘ ğ‘  ğ‘†ğ¶)| (6)
3.2.1 Token Value Prediction Task (TVP). CodeFill uses different
representationsofprogramsforeachtaskwithinthesoft-parameter
sharingMTLframework.CodeFilltreatstheTVPtaskasmasked
unidirectional prediction; left-side context is used to predict the
next token. The inputs to the task are sequences of token values,
represented as real-valued vectors of [ğ‘£1,ğ‘£2,...,ğ‘£ ğ‘›].
3.2.2 Token Type Prediction Task (TTP). Similarly to TVP, TTP
is also treated as left-to-right masked unidirectional prediction.
The input are corresponding token type representations as real-
valued vector of [ğ‘¡1,ğ‘¡2,...,ğ‘¡ ğ‘›]As both the TTP and TVP models
are trained jointly, CodeFill is capable of exploiting token types
when the ultimate goal is to predicting token values.
405ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA Izadi, et al.
Í³ÜªC
Ç’ÆÇ¬Ç‹Æ§Æ£ÜªÇ’Ç¶ÇÇ³Æ½ÜªÇ¬Æ§ÇÆ³ÜˆÆÇ³Ç³Æ§Ç“Ç³Æ½Ç™Ç“ÇÆÍ´Æ§Ç©ÜªÇ“Ç™Ç©Ç’Æ³Æ§Æ§Æ£ÜˆÆ³Ç™Ç©Í®ÆÇ©Æ£ÇÆÍ´Æ§Ç©ÜªÇ“Ç™Ç©Ç’WRNHQW\SHSUHGLFWLRQ
Í³ÜªCKHDGOD\HUWRNHQQDPHSUHGLFWLRQRUVWDWHPHQWFRPSOHWLRQEHDPVHDUFK
(äˆº<äˆº
77Â«
Â«7 7
7 7 77
(äˆ»(äˆ¼ ( <äˆ»<äˆ¼ <äˆ½
0DVNHGVHOIDWWHQWLRQ
DWWHQGRQO\WROHIWFRQWH[W
 3RVLWLRQDOHQFRGLQJ
7RNHQYDOXHHPEHGGLQJ 3RVLWLRQDOHQFRGLQJ
7RNHQW\SHHPEHGGLQJÇ¬Æ§ÇÆ³ÜˆÆÇ³Ç³Æ§Ç“Ç³Æ½Ç™Ç“Æ³Æ§Æ§Æ£ÜˆÆ³Ç™
Ç’ÆÇ¬Ç‹Æ§Æ£ÜªÇ’Ç¶ÇÇ³Æ½ÜªÇ¬Æ§ÇÆ³ÜˆÆÇ³Ç³Æ§Ç“Ç³Æ½Ç™Ç“ÇÆÍ´Æ§Ç©ÜªÇ“Ç™Ç©Ç’Æ³Æ§Æ§Æ£ÜˆÆ³Ç™Ç©Í®ÆÇ©Æ£ÇÆÍ´Æ§Ç©ÜªÇ“Ç™Ç©Ç’
Ç¬Æ§ÇÆ³ÜˆÆÇ³Ç³Æ§Ç“Ç³Æ½Ç™Ç“Ç™Ç©Í®ÆÇ©Æ£Æ§Í³Ç³Ç©ÆÜªÇ©Æ§ÜˆÇ©ÆÇ“Ç‹Æ½Ç“Æ´ÜªÇÆÍ´Æ§Ç©
Í³ÜªC
Ç’ÆÇ¬Ç‹Æ§Æ£ÜªÇ’Ç¶ÇÇ³Æ½ÜªÇ¬Æ§ÇÆ³ÜˆÆÇ³Ç³Æ§Ç“Ç³Æ½Ç™Ç“ÇÆÍ´Æ§Ç©ÜªÇ“Ç™Ç©Ç’Æ³Æ§Æ§Æ£ÜˆÆ³Ç™Ç©Í®ÆÇ©Æ£ÇÆÍ´Æ§Ç©ÜªÇ“Ç™Ç©Ç’
Ç’ÆÇ¬Ç‹Æ§Æ£ÜªÇ’Ç¶ÇÇ³Æ½Ç™Ç©Í®ÆÇ©Æ£Æ§Í³Ç³Ç©ÆÜªÇ©Æ§ÜˆÇ©ÆÇ“Ç‹Æ½Ç“Æ´ÜªÇÆÍ´Æ§Ç©
7RNHQYDOXHV&RUUHVSRQGLQJ
WRNHQW\SHV5HFRPPHQGHG
FRPSOHWLRQV
&RQVWUDLQHG
OD\HUV
7DVN7RNHQOHYHO
SUHGLFWLRQ7DVN6WDWHPHQWOHYHO
SUHGLFWLRQ7DVN7RNHQW\SHSUHGLFWLRQ
Figure 4: Model training
3.2.3 Statement Completion Task (SC). As useful as next-token
prediction may be, developers can also benefit from getting longer
suggestions to complete code statements [ 6,36,49]. Correspond-
ingly, CodeFill can also benefit from training to predict longer
sequences, as training will enable it to better prioritize context use.
Thus,weaddathirdtasktotrainCodeFilltoprovidecompletion
suggestionsupanduntiltheendofastatement.Topredictawhole
statement given the existing code context ğ¶, and the vocabulary
ğ·, CodeFill attempts to generate token values {ğ‘£ğ‘¡}âŠ‚ğ·, condi-
tioned on the sequence of preceding token values {ğ‘ğ‘¡}âŠ‚ğ·, For
thistask,thepre-processingstepsintroduceaspecialtoken( /angbracketleftğ¸ğ‘‚ğ‘†/angbracketright)
to demarcate the end of a statement. CodeFill is trained to keep
predicting sequences of token names until it produces an /angbracketleftğ¸ğ‘‚ğ‘†/angbracketright
token.
3.2.4 Beam search. CodeFill uses greedy (beam) search to identify
the most probable sequences given a sequence of probabilistic pre-
dictions. Specifically, |ğµ|(width of the beam) top probabilities, are
recorded partially for every step. This heuristic algorithm does not
necessarilyoptimizeresults;however,itscomputationalcomplexityequalsto
ğ‘‚(|ğµ|Ã—|ğ‘‰|)whichismuchfasterthancomputingallcases.
As|ğµ|increases,thequalityofgeneratedsummariesimproves,how-
ever, the learning time increases as well. We experimented with
several beam values (3, 5, and 10), and settled to 5, as it provided a
goodbalanceofaccuracy andspeed.
3.3 Post-processing
Re-ranking Recommendations. For a recommendation system to
be useful, predictions should be ranked similarly to user expec-
tations.Tooptimizeranking,CodeFillincludesapost-processinglayer to re-rank the leaf nodes in the final recommendation listbased on the visible scope (i.e., the current file). This is based on
theobservationthatmostcompletionsshouldbelocaltotheedited
file, as naming visibility rules should force names to cluster.
Tore-rankthesuggestions,CodeFillhierarchicallydividesthe
visiblescopetofile,class,andclosestfunction.Theintuitionhere
is,whenthemodelispredictingthenexttokenanditstypeisex-
pectedtobeavariablename,candidatesintheclosestscopehavea
higher probability of being correct. However, when the next token
is predicted to be a function name, candidates from the same class
(functions defined in the same class) should be probably at the top
ofthelist.There-rankingprocessconsistsofmultiplyingthepre-
dictionprobabilitiesofthetop-10predictionswithacorresponding
weightcoefficient.Theweightsareselectedbasedonthetypeof
thepredictedtokenandthescopeofthedeclarationoftheidenti-
fier.Eachpredictionconsistsofa <token, type, probability>
triplet with respect to the prediction point that it is made available,
Wegeneratethelistofallvisiblenamesandtheirhierarchicalscope
(function, class, file). Each prediction is then cross-checked with
this list, in the case where the predicted identifier is indeed alreadydeclaredinthefile(andthusinthelist),itspredictionprobabilityismultipliedbyaweightdependingonthetypeofthepredictedtoken
andthescopeassociatedwiththeiteminthelist.Astheweights
impact the quality of predictions, we first defined a range/ratio
for different categories based on our programming intuition. Then,
we experimented with this range and selected the best performing
weights. Table 1 presents the weights used in this process.
406CodeFill: Multi-token Code Completion by Jointly Learning from Structure and Naming Sequences ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
Table1:Weightsinthepost-processinglayerforre-ranking
Leaf node type Function Class File
Attribute Access 1.625 1.250 1.125
Variable names 1.625 1.125 1.500
Function names 1.125 1.625 1.500
Algorithm 1 Re-ranking final recommendations
1:inputPredictions, WeightsList
2:outputPredictions âŠ²List of updated predictions
3:Namesâ†getSignificantNames() âŠ²Get the list of important
names in left context from the file
4:forpredinPredictions do
5:whiletruedo
6:Namesâ†getSignificantName.pop()
7:ifsignificantName.token = prediction.token then
8: typeCategory â†getTypeCategory()
9: weightâ†weights[typeCategory][scope]
10: pred.probability â†pred.probability Ã—weight
11: break
12:end if
13:end while
14:end for
Although the current weights improve the predictions, this only
sets the minimum bar. Future work can exploit automatic learning
of these weights.
4 EXPERIMENTAL SETUP
TotrainandevaluateCodeFill,weusetwoPythondatasets.Weeval-
uate the models based on different evaluation scenarios, to achieve
a more realistic and comprehensive outlook on the performance of
code completion models to benefit developers in real-world cases.
4.1 Evaluation Tasks
WeevaluateCodeFillontwotasks,namely Token-Level andStatement-
LevelPredictions (TLP and SLP).
4.1.1 Token-Level Prediction. WeuseTLPtoassesstheabilityof
themodeltopredictasinglenexttoken.Wesplitthispartofthe
evaluation into four subtasks presented below.
Any token prediction. Our first sub-task is to evaluate the pre-
dictions of any token irrespective of its type (TLP-A). This is the
baseline evaluation task employed in the literature, but as research
has shown[ 18], itis notrepresentativeof real-world autocomple-
tion use. For this reason, we resort to more detailed evaluations, as
presented below.
Token Type Prediction. To assess the modelâ€™s ability to learn
grammaticalsequences,weevaluatehowwellamodelcanpredicta
correctASTtokengivenacontext(TLP-B).WegrouptogetherAST
tokens in the following categories: Identifiers, Keywords, Operators,
Punctuation, and finally numerals and string Literals.Figure 5: Length of statements in the PY117K dataset
Leaf Node Prediction. Inspired by the evaluation setup of the
state-of-the-artstudybyKimetal.[ 25],weinvestigatetheabilityof
modelswhenpredictingASTleafnodes(TLP-C),including Attribute
access,Names,Function parameters, and Constants.
CardinalPointPrediction. Thethreetaskspresenteduptonow
give a comprehensive view of the prediction ability of a model.However, in practical settings, autocompletion is only triggered
atspecificpoints(e.g.,afteradot,orafterspecifickeywordssuchas
for)whilethedeveloperiseditingsourcecode.Toensurethat
predictions translate to practical benefits for the developers, we
evaluatecompletionsoncardinalpoints(TLP-D).Toobtainalist
ofkeywordsafterwhichautocompletionislikelytobetriggered,
wefirstselectthelistofpunctuationandkeywordstokensthatcan
be completed. We then compute the frequency of all bi-grams with
any of these tokens as their first token in our dataset. Then, weremove three sets of bi-grams; (1) those that are mostly written
togetherwithoccurrencefrequencyabove95%(e.g., async def ),
(2)thosethatarenormallynotpredictable(e.g., class NAME ordef
FUNCTION-NAME ),andfinally(3)thosethatareusuallynotpractical
completions (e.g., TRUE :). The resulting list of tokens after which
itismostbeneficialforautocompletiontobetriggeredisasfollows.
DOT, AWAIT, ASSERT, RAISE, DEL, LAMBDA, YIELD, RETURN,
EXCEPT,WHILE,FOR,IF,ELIF,ELSE,GLOBAL,IN,AND,NOT,
OR, IS, BINOP, WITH, ;, â€ [, (, {, ~
Evaluation Metrics. As the model only predicts a single token
in the TLP task, we include two evaluation metrics, namely the
Accuracy ofthetoppredictionandtheMeanReciprocalRank( MRR)
for the top-10 recommendations.
Accuracy measures the proportion of samples for which the
suggested completion token exactly matches the single target label.
MRRassesses the whole top ğ‘recommended completions and
takesintoaccountthefirstpositionthetargetismatched[ 38].Fora
singlequery,thereciprocalrankis1
ğ‘Ÿğ‘ğ‘›ğ‘˜whereğ‘Ÿğ‘ğ‘›ğ‘˜istheposition
of the highest-ranked answer (1,2,3,...,ğ‘forğ‘answers). If no
correct answer exists in top- ğ‘, then the reciprocal rank is 0. For
multiple queries ğ‘„, the MRR is the mean of the ğ‘„reciprocal ranks.
4.1.2 Statement Level Prediction (SLP). The SLP task assesses a
modelâ€™s ability to complete statements with up to ğ‘›tokens. The
boxplot in Figure 5 shows the distribution of number of tokens for
completions in the evaluation dataset ( ğ‘ƒğ‘Œ117ğ¾). In our datasets,
statementsare4 .2tokenslongonaverage(median:4,maximum:
13).Toprovideacomprehensiveview,weevaluatetheperformance
ofthemodelswhenpredictingnext- ğ‘›tokenswithğ‘›âˆˆ[2,3,...,8].
Evaluation Metrics: On absence of code-specific metrics, we use
twometricscommonly-usedforautomaticevaluationoftextgen-
eration, namely Metric for Evaluation of Translation with Explicit
407ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA Izadi, et al.
Table 2: Datasets used for training and evaluation
PY1690K PY117K
#Repositories 32 .7K 24.9K
#Files 1 .7M 117K
#LOC 425M 29M
#Tokens (unique) 5 .7M 766K
#Types (unique) 103 103
ORdering(METEOR)[ 27]andRecall-OrientedUnderstudyforGist-
ing Evaluation (ROUGE-L) [30].
ROUGE:ROUGE-Nreferstooverlappingn-grams.ROUGE-L,
oneofthevariationsoftheROUGEmetric,countslongestmatch-
ing sequence of words using the Longest Common Subsequence
algorithm. It considers sentence-level structure similarity and auto-
maticallyidentifiesthelongestco-occurringchainofinsequence
n-grams. Thus, it does not require consecutive matches but in-
sequence matches that reflect sentence-level word order.
METEOR isbasedontheterm-to-termmappingofthegener-
ated code with its corresponding reference code. It focuses mainly
on recall. Lavie et al. [ 27] showed metrics based on recall consis-
tently achieve higher correlation with user preferences than those
based on precision alone.
4.2 Datasets
We use two Python datasets for training and evaluation:
â€¢The ETH 150K Python dataset [ 40] for compatibility with
previouswork.TheauthorscollectedPythonprogramsfrom
GitHub repositories and removed duplicate files, project
forks, files that do not parse and have more than 30K nodes
intheirASTs.Theyalsoremovedobfuscatedfilesandonly
used repositories with permissive licenses including MIT,
BSD, and Apache.
â€¢TheCodeFilldataset,whichwascollectedbyqueryingGHTor-
rent[15]forallnon-forkedPythonrepositorieswithmore
than 20 stars (58k repositories).
Afterdeduplication,usingthemethodproposedbyAllamanis[ 2],
we ended up with two versions of the original datasets, ğ‘ƒğ‘Œ117ğ¾
andğ‘ƒğ‘Œ1690ğ¾fortheETHandCodeFilldatasets,respectively.Note
thatğ‘ƒğ‘Œ1690ğ¾andğ‘ƒğ‘Œ117ğ¾do not have any common files. Table 2
presents an overview of the contents of the datasets.
Weuseğ‘ƒğ‘Œ1690ğ¾exclusivelyforpre-trainingourLM.Wethen
use90%ofğ‘ƒğ‘Œ117ğ¾forfine-tuningthemodelonthetaskspresented
inSection4.1,andfinallythelast10%of ğ‘ƒğ‘Œ117ğ¾forevaluation.For
the baselines, we concatenate ğ‘ƒğ‘Œ1690ğ¾with the same 90% portion
ofğ‘ƒğ‘Œ117ğ¾as above for training, and evaluate on the remaining
10% ofğ‘ƒğ‘Œ117ğ¾.Table 3: TPL-A results: Any token prediction
Approach Venue Accuracy MRR
n-gram + LSTM [17] (FSE, 2017) 65.1 67.9
Pointer Mixture [29] (IJCAI, 2018) 65.8 70.0
OpenVocab [24] (ICSE, 2020) 67.2 69.8
T-XL + Bi-LSTM [32] (ICPC, 2020) 75.0 76.4
GPT-C [49] (FSE, 2020) 79.8 80.0
TravTrans+ [25] (ICSE, 2021) 78.9 79.4
CodeFill Proposed 80.6 81.7
4.3 Implementation and Configuration
WeusePythonâ€™sAST3,Tokenize4,andtheDIS5librariesinour
conversiontool.Moreover,weusethe HuggingFace6libraryforthe
implementationofourGPT-2andMTLmodels.Wesetthelearning
rateto0.00001,maximumsequencelengthto2048,andtrainedour
model for 100 epochs. We set the remaining parameters to default
values.Ourexperimentsareconductedonamachineequippedwith
two GeForce GTX 1080 Ti GPUs, an Intel(R) Xeon(R) CPU E5-2690
v4 @ 2.60GHz CPU with 14 core processors, and 128G RAM.
5 RESULTS AND DISCUSSION
In this section, we present the results for each evaluation task,
along with an ablation study and a characterization of the modelsâ€™
performance.
5.1 Token-level Prediction (TLP)
5.1.1 Any token prediction. The most basic form of evaluation for
an autocompletion model is to gauge its ability to predict the next
tokengivensomecontextasinput.TLP-Acanprovideanoverview
on the ability of an autocompleter to predict, however, it does not
account for the prior probabilities of different types of tokens. We
present this taskfor compatibility with existingwork, and further
elaborateCodeFillâ€™sperformanceinthefollowingtasks.Theresults
can be seen in Table 3; our model outperforms all the baselines
across all metrics.
5.1.2 Token Type Prediction. We investigate the performance of
the models when predicting different types of tokens, i.e., their
abilitytoassimilatehowdevelopersusegrammartoexpresscon-
cepts. Models generally struggle more with specific token types.
Forinstance,itisknownthat predictingidentifiersisharderthan
predictingkeywords[ 18].Table4presenttheAccuracyandMRR
results based on all token types. As demonstrated, CodeFill out-performs the baselines for all token types based on both metrics
(except forMRRonkeywordsandpunctuation,whereitsperfor-
manceisonpar).Transformer-basedapproachesarehighlycapableofpredictingspecifictypesoftokens,namelykeywordsandpunctu-
ation; effectively, this means that given enough training examples,
theycanefficientlylearnsyntacticalpatterns.Predictingidentifiers
andliteralsacrossallmodelsismorechallenging.Foridentifiers,
3https://docs.python.org/3/library/ast.html
4https://docs.python.org/3/library/tokenize.html
5https://docs.python.org/3/library/dis.html
6https://huggingface.co
408CodeFill: Multi-token Code Completion by Jointly Learning from Structure and Naming Sequences ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
Table 4: TPL-B Results: Token type predictionsMetricApproach
Identifier
Keyword
Punctuation
Literals
Operators
All
Token Percentage 21% 28% 33% 5% 13% -AccuracyN-gram+LSTM [17] 40.2 74.2 81.4 46.2 62.7 66.6
Pointer Mixture [29] 37.0 85.3 80.0 43.9 62.8 68.4
OpenVocab [24] 42.3 89.8 93.4 54.4 65.0 76.0
T-XL + Bi-LSTM [32] 47.4 93.1 92.4 59.4 68.7 78.4
GPT-C [49] 50.0 96.5 95.1 62.0 71.0 81.2
TravTrans+ [25] 51.1 95.9 97.0 59.3 71.3 81.8
CodeFill 54.4 97.3 98.0 65.8 71.4 83.8MRRN-gram+LSTM [17] 40.6 76.8 84.6 49.8 64.2 68.8
Pointer Mixture [29] 38.5 85.9 85.2 46.7 64.5 71.0
OpenVocab [24] 43.2 90.3 96.0 57.0 67.1 77.6
T-XL + Bi-LSTM [32] 49.8 96.1 96.6 61.3 70.0 81.4
GPT-C [49] 52.3 98.8 98.8 64.0 73.3 83.9
TravTrans+ [25] 53.7 97.1 98.6 62.2 73.0 83.6
CodeFill 56.098.1 98.0 66.1 74.4 87.2
all modelsâ€™ result range from 37% to 56% accuracy. In both cases,
CodeFill maintains a non-trivial edge over the baselines, which
we attribute to the statement completion task. We believe it helps
CodeFill to learn syntactical patterns over longer ranges.
5.1.3 Leaf Node Prediction. Wecompareeachmodelâ€™sperformance
inpredictingdifferenttypesof leafnodes inanAST,e.g.,function
calls, variables, and attribute names. Tables 5 present the Accuracy
and MRR results for this task. CodeFill is the best model in both ac-
curacy, and, especially, MRR. This means that its name predictions,
whichisarguablythemostimportantfeatureforanautocompleter,
are 2 out of 3 times correct and have a high probability ( >70%) of
being included in the top suggestions.
5.1.4 Cardinal Point Completion. In Table 6, we report the perfor-
mance of modelswhen predicting at cardinalpoints (described in
Section 4.1). As indicated, CodeFill outperforms all the baselines.
Consequently,itismorecapableofpresentingcorrectrecommenda-tionsatpointswhereautocompletionismorelikelytobetriggered.
5.2 Statement-Level Prediction (SLP)
We reportthe resultsfor autocompletingcode statements,by pre-
dicting the remaining ğ‘›tokens at a given statement position (with
ğ‘›ranging between 2 and 8). Figure 6 presents the results of this
experiment based on the achieved METEOR and ROUGE-L scores.
AllTransformer-basedmodels[ 25,32,49],areconsistentlymore
capablethanthethreebaselineapproaches.CodeFillimprovesoverallcompetitors.Themargingrowswiderasthenumberoftokensre-quiredtocompletestatementsincrease(especiallyintheROUGE-Lcase).Thisresulthighlightsthemeritsofourstatementcompletion
task.Inturn,thiscanhelpdeveloperscodefasterbyreducingthe
number ofrequiredkeystrokes; theexperience ofusing statement
completionshouldbereminiscentoftextlinecompletioninpopularTable 5: TLP-C results: Leaf node predictionMetricApproach
Attribute
Access
Names
Function
names
Numeric
constant
All
Token Percentage 32% 13% 33% 22%AccuracyN-gram + LSTM [17] 56.3 61.8 63.5 45.1 56.9
Pointer Mixture [29] 53.5 62.0 59.8 42.0 54.2
OpenVocab [24] 59.8 63.7 66.2 51.7 60.6
T-XL + Bi-LSTM [32] 59.9 58.1 62.8 54.8 59.5
GPT-C [49] 60.0 59.9 64.0 56.060.4
TravTrans+ [25] 60.2 65.4 68.3 52.7 61.7
CodeFill 64.0 67.3 72.2 53.166.3MRRN-gram + LSTM [17] 57.9 64.7 65.2 47.5 58.9
Pointer Mixture [29] 57.1 59.0 60.2 43.1 55.3
OpenVocab [24] 61.2 64.8 70.1 51.7 62.5
T-XL + Bi-LSTM [32] 61.9 65.3 69.9 55.3 63.5
GPT-C [49] 63.4 62.9 66.5 57.263.0
TravTrans+ [25] 62.8 65.4 70.0 55.2 63.8
CodeFill 72.0 69.7 76.9 56.069.5
Table 6: TPL-D Results: Cardinal Points Completion
Approach Accuracy MRR
N-gram + LSTM [17] 49.0 52.3
Pointer Mixture [29] 51.3 52.4
OpenVocab [24] 52.2 53.5
T-XL + Bi-LSTM [32] 64.0 64.7
GPT-C [49] 66.1 67.8
TravTrans+ [25] 65.0 66.2
CodeFill 70.0 70.9
online email or document editors. Statistically, more than 2 out of 3
statement completions of 4 or fewer tokens will be correct.
5.3 Ablation Study
Weperformanablationstudytoexaminetheimpactofdifferent
components ofCodeFill .Table 7presents the resultsof thisstudy.
We include the performance of a vanilla GPT-2 model to show
the importance of employing the MTL approach to jointly train
modelsondifferentrepresentationsofsourcecode.Theresultsshow
thatemployingtheMTLtechniquetotrainthemodelsjointlyon
multipletasksindeedhelpsthemodellearnbetter.Next,weconduct
experimentstocomparehard-parameterandsoft-parametermodels
with the two-task MTL model. It is worth mentioning that for
thehard-parametersharingvariation,weneedtoinputaunified
representationtothemodels.Thus,weconcatenatethetypeand
value of each token as ğ‘¥ğ‘–=[ğ‘¡ğ‘–,ğ‘£ğ‘–]and then feed the vectors of this
concatenatedrepresentationtotheMTLmodel.Theresultsindicate
that the soft-parameter sharing works better in our case. This is
probablybecausethissettingallowseachtasktohaveitsownmodel
and parameters and then regularizes the distance between them to
409ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA Izadi, et al.
Figure 6: Results for the SLP task
Table7:EffectivenessofDifferentComponentsoftheModel
Approach Tasks Train Time Accuracy MRR
GPT-2 Value 12h 77.7 78.2
MTL HP Value, Type 17h 78.3 79.6
MTL SP Value, Type 19h 78.9 79.5
MTL SP Value, Type, Statement 24h 80.6 81.7
encourage the parameters to be similar. Finally, to verify whether
adding information regarding statements helps, we investigate the
effect of adding the third task, statement completion. The results
demonstratethattrainingontwodifferentgranularity(single-token
and statement) also helps them learn better. To conclude, each
componentof theproposedmodeladds toitsvalue.Although the
training time increases, it can be argued that training time is a one-
timecost,andcanbesignificantlyreducedwithparalleltraining
on multiple GPUs.
5.4 Runtime Characteristics
An important aspect of ML-based autocompletion tools is their
predictionlatency.Averyaccuratemodelthattakes1secondper
prediction will not be very useful in practice as it will interfere
with the developerâ€™s workflow. As Table 8, all models feature an
averagelatencyoflessthan100milliseconds,whichisconsidered
the golden standard in the industry.
Moreover, the model size and number of parameters are impor-
tantpracticalaspectsthataffectamodelâ€™sdeployment;ifthemodel
istoobig,itwillneedtobedeployedcentrallyandclientsshould
connecttothemodelserveroveranetworkconnection(whichmay
affectlatencynegatively),otherwise,itcouldbedistributedtothe
clients. As Table 8 shows, CodeFillâ€™s number of parameters is moreTable 8: Runtime Characteristics
Approach Train Time (hr) Latency (ms) #Params
n-gram + LSTM [17] 23 75 168M
Pointer Mixture [29] 18 62 177M
OpenVocab [24] 21 61 145M
T-XL + Bi-LSTM [32] 24 79 173M
GPT-C [49] 23 74 125M
TravTrans+ [25] 15 53 119M
CodeFill 24 73 258M
thanotherbaselinesduetoourarchitecturespecification.However,
thesizeofallTransformer-basedmodelsmakesthemimpractical
for distribution to clients, necessitating centralized deployments.
6 CONTRIBUTIONS AND IMPLICATIONS
Autocompletionisapopularresearcharea,however,theexisting
challenges leave substantial margin for improvement, particularly
for recommending identifiers or completing longer sequences [18].
Inthisstudy,CodeFilllearnsfromsequencesofbothtokentypes
andtokennamessimultaneouslyusingMTL.Thecontributionof
this work is twofold;
Technical novelty : Similar to the state-of-the-art [25, 49],w e
use transformers for learning a name-based sequencing model,
and similar to the studies by Liu et al. [32, 33], we use the MTL
technique to condition our models under different tasks. However,
IntelliCodeCompose [49]treats code as natural text, neglecting the
rich structure inherent in programs. Moreover they focus on multi-
lingual LMs. TravTrans+ [25]uses serialized ASTs in an attempt
to learn from structure, however, we show that our novel transfor-
mation,whichwedesignedsothatitisclosertohowdevelopers
410CodeFill: Multi-token Code Completion by Jointly Learning from Structure and Naming Sequences ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
treat source code structure, outperforms TravTrans+. CodeFill also
learnsfromournovelstatementcompletiontasktoconsiderlonger
contexts. Both Figure 6andTable 7show that this technique im-
proves the model, probably by helping it better utilize completion
context. The combination of the above demonstrably results in
higher evaluation scores and better recommendations.
Evaluation : We propose two novel evaluation tasks, cardinal
point,and statementcompletion,toaddressdeficienciesincurrent
autocompletion evaluation setups. We also collect, pre-process,
deduplicate, and sharean large Python dataset, consistingof prac-
tically all Python code on GitHub.
7 THREATS TO THE VALIDITY
Threatstointernalvalidity :Theseincludethethreatspertaining
to the parameters affecting the performance of the model. Another
threat in this section relates to the errors in the implementation
of the baselines. For all of these approaches, we have used the
replication packages provided by these studies.
Threats toexternal validity : These threatsrelate to the qual-
ityofthedatasetsweusedandthegeneralizabilityoftheresults.
We used two Python datasets; PY117K is a benchmark dataset [ 40]
frequently used in the literature [ 24,25,29,32]. PY1690K, our sec-
ond dataset, is ten times larger with approximately 1 .7ğ‘€program
files.Moredatacanleadtomoregeneralizableresults.Furthermore,
as Allamanis. [ 2] suggests, we have de-duplicated both datasets to
avoid biasing the models. All of the programs in both datasets are
collected from open-source GitHub repositories. However, further
studies are needed to validate and generalize our findings to other
programming languages.
Threats to construct validity : These relate to the suitability
oftheevaluationsettingandmetrics.Inthiswork,wehavetried
toincorporatediverseevaluationmeasures.FortheTLPtask,we
have used standard evaluation metrics, namely Accuracy and MRR
in the top-oneand top-ten recommendations which are both fre-
quently used in the literature [ 24,25,29,32]. Furthermore, we use
ROUGE-L and METEOR scores for evaluation in the SLP task asused in previous studies on source sequence of code generation,
summarization, and translation [1, 49].
8 CONCLUSION AND FUTURE WORK
Unlike natural language text, source code is more structured, its
grammarismorewelldefinedbutitsvocabularyisordersofmag-
nitudebigger.Consequently,NLP-basedmodelsandcorresponding
evaluation methods need to be adapted to the particular case of
source code.
In this work, we proposed CodeFill, a Transformer-based gen-
erativeLMforsourcecodepre-trainedonthreetaskscloselyrele-
vanttoprogramming.Givenacontextoftokens(andtheirtypes),
CodeFillistrainedtopredict(1)thetypeofthenexttoken,(2)its
value, and (3) the values of up to ğ‘›next tokens. We employ the
MTL approach to jointly train CodeFill on the above tasks. We also
propose 2 novel evaluation tasks, cardinal point prediction and
statement-level multi-tokenprediction, whichwe arguethat they
better represent how autocompletion systems are used in practice.
WeextensivelyevaluateCodeFillagainstsixbaselinesonbothtasks.
OurresultsindicatethatCodeFilloutperformsallthebaselinesinallscenarios,achievingstateoftheartscoresonbothaccuracy(80.6%)andMRR(81.7%)inthebasictoken-levelpredictiontask.Moreover,
weshowthatCodeFillalsolearnstoautocompletestatementsofup
to4tokenswithover70%accuracy ,asignificantimprovementover
the baselines, making it practical to offer statement completions as
an IDE feature.
Inthefuture,weplantoincorporatemoredomainspecificknowl-
edgeonaspectsoftrainingandevaluatingatrainingMLmodels.
Forinstance,onecanlimitthecontextfedtothemodelbasedonthe
programminglanguagetobetterincorporaterelatedinformation
offunctionsandnestedscopesinapieceofcode.Wealsoplanto
further investigate statement completion, including better metrics
for its evaluation.
ACKNOWLEDGMENTS
ThisworkhasreceivedfundingfromtheEuropeanUnionâ€™sHorizon
2020 research and innovation programme under grant number
825328(FASTENproject),andalsotheNWOMIPLproject,grant
number 628.008.003.
REFERENCES
[1]Alireza Aghamohammadi, Maliheh Izadi, and Abbas Heydarnoori. 2020. Gener-
ating summaries for methods of event-driven programs: An Android case study.
Journal of Systems and Software 170 (2020), 110800. https://doi.org/10.1016/j.jss.
2020.110800
[2]Miltiadis Allamanis. 2019. The adverse effects of code duplication in machine
learningmodelsofcode.In Proceedingsofthe2019ACMSIGPLANInternational
SymposiumonNewIdeas,NewParadigms,andReflectionsonProgrammingand
Software. 143â€“153.
[3]Miltiadis Allamanis, Earl T Barr, Premkumar Devanbu, and Charles Sutton. 2018.
A survey of machine learning for big code and naturalness. ACM Computing
Surveys (CSUR) 51, 4 (2018), 1â€“37.
[4]MiltiadisAllamanisandCharlesSutton.2014. Miningidiomsfromsourcecode.
InProceedingsofthe22ndACMSIGSOFTInternationalSymposiumonFoundations
of Software Engineering. 472â€“483.
[5]Sven Amann, Sebastian Proksch, Sarah Nadi, and Mira Mezini. 2016. A study
ofvisualstudiousageinpractice.In 2016IEEE23rdInternationalConferenceon
SoftwareAnalysis,Evolution, andReengineering(SANER),Vol.1.IEEE, 124â€“134.
[6]Gareth Ari Aye and Gail E Kaiser. 2020. Sequence model design for code comple-
tion in the modern IDE. arXiv preprint arXiv:2004.05249 (2020).
[7]GarethAriAye,SeohyunKim,andHongyuLi.2021. Learningautocompletion
from real-world datasets. In 2021 IEEE/ACM 43rd International Conference on
Software Engineering: Software Engineering in Practice (ICSE-SEIP). IEEE, 131â€“
139.
[8]Pavol Bielik, Veselin Raychev, and Martin Vechev. 2016. PHOG: probabilistic
model for code. In InternationalConference on Machine Learning . 2933â€“2942.
[9]MarcelBruch,MartinMonperrus,andMiraMezini.2009.Learningfromexamples
to improve code completion systems. In Proceedings of the 7th joint meeting of
theEuropeansoftwareengineeringconferenceandtheACMSIGSOFTsymposium
on the foundations of software engineering. 213â€“222.
[10]PaweÅ‚BudzianowskiandIvanVuliÄ‡.2019. Hello,Itâ€™sGPT-2-HowCanIHelpYou?
Towards the Use of Pretrained Language Models for Task-Oriented Dialogue
Systems.In Proceedingsofthe3rdWorkshoponNeuralGenerationandTranslation.
15â€“22.
[11] Rich Caruana. 1997. Multitask learning. Machine learning 28, 1 (1997), 41â€“75.
[12]SantanuKumarDash,MiltiadisAllamanis,andEarlT.Barr.2018. RefiNym:Using
Names to Refine Types. In Proceedings of the 2018 26th ACM Joint Meeting on
European Software Engineering Conference and Symposium on the Foundations of
SoftwareEngineering (LakeBuenaVista,FL,USA) (ESEC/FSE2018).Association
for Computing Machinery, New York, NY, USA, 107â€“117. https://doi.org/10.
1145/3236024.3236042
[13]Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert:
Pre-trainingofdeepbidirectionaltransformersforlanguageunderstanding. arXiv
preprint arXiv:1810.04805 (2018).
[14]Yoav Goldberg. 2017. Neural network methods for natural language processing.
Synthesis lectures on human language technologies 10, 1 (2017), 1â€“309.
[15]GeorgiosGousiosandDiomidisSpinellis.2012. GHTorrent:GitHubâ€™sDatafroma
Firehose.In MSRâ€™12:Proceedingsofthe9thWorkingConferenceonMiningSoftware
Repositories (Zurich,Switzerland), MichaelW.Godfreyand JimWhitehead(Eds.).
IEEE, 12â€“21. https://doi.org/10.1109/MSR.2012.6224294
411ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA Izadi, et al.
[16]Donghoon Ham, Jeong-Gwan Lee, Youngsoo Jang, and Kee-Eung Kim. 2020.
End-to-end neural pipeline for goal-oriented dialogue systems using GPT-2.InProceedings of the 58th Annual Meeting of the Association for Computational
Linguistics. 583â€“592.
[17]VincentJHellendoornandPremkumarDevanbu.2017. Aredeepneuralnetworks
thebestchoiceformodelingsourcecode?.In Proceedingsofthe201711thJoint
Meeting on Foundations of Software Engineering. 763â€“773.
[18]Vincent J Hellendoorn, Sebastian Proksch, Harald C Gall, and Alberto Bacchelli.
2019. Whencodecompletionfails:Acasestudyonreal-worldcompletions.In
2019IEEE/ACM41stInternationalConferenceonSoftwareEngineering(ICSE).IEEE,
960â€“970.
[19]AbramHindle,EarlTBarr,ZhendongSu,MarkGabel,andPremkumarDevanbu.
2012. On the naturalness of software. In 2012 34th International Conference on
Software Engineering (ICSE). IEEE, 837â€“847.
[20]DaqingHouandDavidMPletcher.2010.Towardsabettercodecompletionsystem
byAPIgrouping,filtering,andpopularity-basedranking.In Proceedingsofthe
2nd International Workshop on Recommendation Systems for Software Engineering .
26â€“30.
[21]Maliheh Izadi, Kiana Akbari, and Abbas Heydarnoori. 2022. Predicting the
objectiveandpriorityofissuereportsinsoftwarerepositories. EmpiricalSoftware
Engineering 27, 2 (2022), 1â€“37. https://doi.org/10.1007/s10664-021-10085-3
[22]Maliheh Izadi, Abbas Heydarnoori, and Georgios Gousios. 2021. Topic recom-
mendation for software repositories using multi-label classification algorithms.
EmpiricalSoftwareEngineering 26,5(2021),1â€“33. https://doi.org/10.1007/s10664-
021-09976-2
[23]Xianhao Jin and Francisco Servant. 2018. The Hidden Cost of Code Completion:
Understanding the Impact of the Recommendation-List Length on Its Efficiency.
InProceedingsofthe15thInternationalConferenceonMiningSoftwareRepositories
(Gothenburg,Sweden) (MSRâ€™18).AssociationforComputingMachinery,New
York, NY, USA, 70â€“73. https://doi.org/10.1145/3196398.3196474
[24]Rafael-Michael Karampatsis,HlibBabii,RomainRobbes,CharlesSutton,andAn-
drea Janes. 2020. Big code!= big vocabulary: Open-vocabulary models for source
code.In2020IEEE/ACM42ndInternationalConferenceonSoftwareEngineering
(ICSE). IEEE, 1073â€“1085.
[25]Seohyun Kim, Jinman Zhao, Yuchi Tian, and Satish Chandra. 2021. Code Pre-
dictionbyFeedingTreestoTransformers.In 2021IEEE/ACM43rdInternational
Conference on Software Engineering (ICSE). 150â€“162. https://doi.org/10.1109/
ICSE43902.2021.00026
[26]YoonKim,YacineJernite,DavidSontag,andAlexanderMRush.2016. Character-
aware neurallanguage models. In Thirtieth AAAIconference on artificialintelli-
gence.
[27]Alon Lavie, Kenji Sagae, and Shyamsundar Jayaraman. 2004. The significance of
recallinautomaticmetricsforMTevaluation.In ConferenceoftheAssociation
for Machine Translation in the Americas. Springer, 134â€“143.
[28]Jieh-Sheng Lee and Jieh Hsiang. 2020. Patent claim generation by fine-tuning
OpenAI GPT-2. World Patent Information 62 (2020), 101983.
[29]Jian Li, Yue Wang, Michael R Lyu, and Irwin King. 2017. Code completion with
neural attention and pointer networks. arXiv preprint arXiv:1711.09573 (2017).
[30]Chin-YewLin.2004. Rouge:Apackageforautomaticevaluationofsummaries.
InText summarization branches out. 74â€“81.
[31]ChangLiu,XinWang,RichardShin,JosephEGonzalez,andDawnSong.2016.
Neural code completion. (2016).
[32]FangLiu,GeLi,BolinWei,XinXia,ZhiyiFu,andZhiJin.2020. ASelf-Attentional
Neural Architecture for Code Completion with Multi-Task Learning. In Proceed-
ings of the 28th International Conference on Program Comprehension. 37â€“47.
[33]Fang Liu, Ge Li, Yunfei Zhao, and Zhi Jin. 2020. Multi-task Learning based
Pre-trained Language Model for Code Completion. In 2020 35th IEEE/ACM Inter-
national Conference on Automated Software Engineering (ASE) . IEEE, 473â€“485.
[34]Minh-Thang Luong, Hieu Pham, and Christopher D Manning. 2015. Effec-tive approaches to attention-based neural machine translation. arXiv preprint
arXiv:1508.04025 (2015).
[35]TomÃ¡Å¡ Mikolov, Martin KarafiÃ¡t, LukÃ¡Å¡ Burget, Jan ÄŒernock `y, and Sanjeev Khu-
danpur. 2010. Recurrent neural network based language model. In Eleventh
annual conference of the international speech communication association.
[36]Son Nguyen, Tien Nguyen, Yi Li, and Shaohua Wang. 2019. Combining program
analysisandstatisticallanguagemodelforcodestatementcompletion.In 2019
34thIEEE/ACMInternationalConferenceonAutomatedSoftwareEngineering(ASE) .
IEEE, 710â€“721.
[37]TungThanhNguyen,AnhTuanNguyen,HoanAnhNguyen,andTienNNguyen.2013. Astatisticalsemanticlanguagemodelforsourcecode.In Proceedingsofthe
2013 9th Joint Meeting on Foundations of Software Engineering . 532â€“542.
[38]Dragomir R Radev, Hong Qi, Harris Wu, and Weiguo Fan. 2002. Evaluating
Web-based Question Answering Systems.. In LREC. Citeseer.
[39]Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya
Sutskever. 2019. Language Models are Unsupervised Multitask Learners. (2019).
[40]Veselin Raychev, Pavol Bielik,and Martin Vechev. 2016. Probabilisticmodel for
code with decision trees. ACM SIGPLAN Notices 51, 10 (2016), 731â€“747.[41]VeselinRaychev,PavolBielik,MartinVechev,andAndreasKrause.2016.Learning
programs from noisy data. ACM Sigplan Notices 51, 1 (2016), 761â€“774.
[42]Romain Robbes and Michele Lanza. 2008. How program history can improve
code completion. In 2008 23rd IEEE/ACM International Conference on Automated
Software Engineering. IEEE, 317â€“326.
[43]Romain Robbes and Michele Lanza. 2010. Improving code completion with
program history. Automated Software Engineering 17, 2 (2010), 181â€“212.
[44]HerbertRobbinsandSuttonMonro.1951. Astochasticapproximationmethod.
The annals of mathematical statistics (1951), 400â€“407.
[45]SebastianRuder.2017. Anoverviewofmulti-tasklearningindeepneuralnet-
works.arXiv preprint arXiv:1706.05098 (2017).
[46]Hinrich SchÃ¼tze, Christopher D Manning, and Prabhakar Raghavan. 2008. Intro-
duction to information retrieval. Vol. 39. Cambridge University Press Cambridge.
[47]Holger Schwenk and Jean-Luc Gauvain. 2002. Connectionist language modeling
forlargevocabularycontinuousspeechrecognition.In 2002IEEEInternational
ConferenceonAcoustics,Speech,andSignalProcessing , Vol. 1. IEEE, Iâ€“765.
[48]Ozan Sener and Vladlen Koltun. 2018. Multi-task learning as multi-objective
optimization. arXiv preprint arXiv:1810.04650 (2018).
[49]AlexeySvyatkovskiy,ShaoKunDeng,ShengyuFu,andNeelSundaresan.2020.
Intellicodecompose:Codegenerationusingtransformer.In Proceedingsofthe28th
ACM Joint Meeting on European Software Engineering Conference and Symposium
on the Foundations of Software Engineering. 1433â€“1443.
[50]AlexeySvyatkovskiy,SebastianLee,AnnaHadjitofi,MaikRiechert,JulianaVi-
centeFranco,andMiltiadisAllamanis.2021. Fastandmemory-efficientneural
code completion. In 2021 IEEE/ACM 18th International Conference on Mining
Software Repositories (MSR). IEEE, 329â€“340.
[51]AlexeySvyatkovskiy,YingZhao,ShengyuFu,andNeelSundaresan.2019. Pythia:
Ai-assisted code completion system. In Proceedings of the 25th ACM SIGKDD
InternationalConference on Knowledge Discovery & Data Mining . 2727â€“2735.
[52]ZhaopengTu,ZhendongSu,andPremkumarDevanbu.2014. Onthelocalness
ofsoftware.In Proceedingsofthe22ndACMSIGSOFTInternationalSymposiumon
Foundationsof Software Engineering . 269â€“280.
[53]Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all
youneed. arXiv preprint arXiv:1706.03762 (2017).
[54]Fengcai Wen, Emad Aghajani, Csaba Nagy, Michele Lanza, and Gabriele Bavota.
2021. Siri,WritetheNextMethod.In 2021IEEE/ACM43rdInternationalConference
on Software Engineering (ICSE). IEEE, 138â€“149.
[55]Yixiao Yang, Yu Jiang, Ming Gu, Jiaguang Sun, Jian Gao, and Han Liu. 2017.
A language model for statements of software code. In 2017 32nd IEEE/ACM
International Conference on Automated Software Engineering (ASE). IEEE, 682â€“
687.
[56]Yu Zhang and Qiang Yang. 2021. A survey on multi-task learning. IEEE Transac-
tions on Knowledge and Data Engineering (2021).
412