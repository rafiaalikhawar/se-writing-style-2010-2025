AutoTransform: Automated Code Transformation to Support
Mo
dern Code Review Process
Patanamon Thongtanunamâˆ—
patanamon.t@unimelb.edu.au
The University of Melbourne
AustraliaChanathip Pornprasit2
chanathip.pornprasit@monash.edu
Monash University
AustraliaChakkrit Tantithamthavorn
chakkrit@monash.edu
Monash University
Australia
ABSTRACT
Codereviewiseffective,buthuman-intensive(e.g.,developersneed
tomanuallymodifysourcecodeuntilitisapproved).Recently,prior
work proposed a NeuralMachine Translation (NMT) approach to
automaticallytransformsourcecodetotheversionthatisreviewed
and approved (i.e., the after version). Yet, its performance is still
suboptimal when the after version has new identifiers or liter-
als(e.g.,renamedvariables)orhasmanycodetokens.Toaddress
these limitations, we propose A utoTransform which leverages
aByte-PairEncoding(BPE)approachtohandlenewtokensanda
Transformer-basedNMTarchitecturetohandlelongsequences.We
evaluate our approach based on 14,750 changed methods with and
withoutnewtokensforbothsmallandmediumsizes.Theresults
showthat whengeneratingone candidateforthe afterversion(i.e.,
beam width = 1), our AutoTransform can correctly transform
1,413 changed methods, which is 567% higher than the prior work,
highlighting the substantial improvement of our approach for code
transformationinthecontextofcodereview.Thisworkcontributes
towardsautomatedcodetransformationforcodereviews,which
could help developers reduce their effort in modifying source code
during the code review process.
ACM Reference Format:
PatanamonThongtanunam,ChanathipPornprasit,andChakkritTantithamtha-
vorn.2022.AutoTransform:AutomatedCodeTransformationtoSupport
Modern Code Review Process. In 44th International Conference on Software
Engineering (ICSE â€™22), May 21â€“29, 2022, Pittsburgh, PA, USA. ACM, New
York, NY, USA, 12 pages. https://doi.org/10.1145/3510003.3510067
1 INTRODUCTION
Code review is one of the important quality assurance practices
ina softwaredevelopmentprocess. Oneofthemain goalsofcode
review is to ensure that the quality of newly developed code meets
astandardbeforeintegratingintothemainsoftwarerepository[ 11,
37].Hence,inthecodereviewprocess,newsourcecodewrittenby
a code author has to be examined and revised until reviewers (i.e.,
developersotherthanacodeauthor)agreethatthissourcecodeisof
âˆ—Acorresponding author.
2The first and second authors contributed equally to this research.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation
on the first page. Copyrights for components of this work owned by others than ACM
mustbehonored.Abstractingwithcreditispermitted.Tocopyotherwise,orrepublish,
topostonserversortoredistributetolists,requirespriorspecificpermissionand/ora
fee. Request permissions from permissions@acm.org.
ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
Â© 2022 Association for Computing Machinery.
ACM ISBN 978-1-4503-9221-1/22/05...$15.00
https://doi.org/10.1145/3510003.3510067sufficientqualitytobeintegrated(i.e.,thecodeisapproved).Several
studies also showed that during the code review process, source
codeisrevisednotonlytoremovefunctionaldefects[ 15,17,40,47],
butalsotoimprovedesignquality[ 41],maintainability[ 50],and
readability [16, 50, 57].
Despite the benefits of code review, the code review process
is still human-intensive where developers have to manually re-
view and revise code. To support reviewing activities, prior studies
proposed approaches to save the reviewersâ€™ effort (e.g., review pri-
oritization[ 14,23,55]).However,therevisingactivitiesstillrequire
manualeffortfromcodeauthors.Indeed,givenalargenumberof
reviews(e.g.,3KreviewspermonthatMicrosoftBing[ 47]),prior
studies found that it is challenging for code authors to revise their
code without introducing new defects, while switching contexts
and keeping track of other reviews [ 21,32]. Thus, automated code
transformationwouldbebeneficialtoaugmentcodeauthorsandto
savetheireffort byautomaticallyapplyingthecommonrevisions
duringcodereviewsinthepasttothenewly-writtencode,while
allowing code authors to focus on revising more complex code.
Tothebestofourknowledge,theworkofTufano etal.[60]is
the mostrecentwork thatproposed anapproach toautomatically
transformcodetotheversionthatisreviewedandapproved.Inthe
priorwork,theyleveragedcodeabstractionandaRecurrentNeural
Network (RNN) architecture. They have shown that their NMT
approachcancorrectlytransformcodebyapplyingawiderange
ofmeaningfulcodechangesincludingrefactoringandbug-fixing.
Whiletheresultsofthepriorwork[ 60]highlightedthepotentialof
using NMT to help code authors automatically modify code during
thecodereviewprocess,theapplicabilityoftheirapproachisstill
limited. Specifically, their code abstraction hinders the Tufano et
al.approachintransformingsourcecodewhennewidentifiersor
literalsappearintheversionthathasbeenapproved.Inaddition,
duetothenatureoftheRNNarchitecture,theTufano etal.approach
may be suboptimal when the size of source code is longer (i.e.,
source code has many tokens).
In this paper, we propose a new framework called AutoTrans-
form,whichleveragesByte-PairEncoding(BPE)subwordtokeniza-
tion[52]andTransformer[ 62]toaddressthelimitationsoftheprior
approach[ 60].Weevaluatedour AutoTransform basedontwo
typesofchangedmethods:(1)thechangedmethods without newly-
introducedidentifiers/literals(i.e.,changedmethodsw/onewto-
kens); and (2) the changed methods withnewly-introduced identi-
fiers/literals(i.e.,thechangedmethodsw/newtokens).Through
a case study of 147,553 changed methods extracted from three
Gerrit code review repositories of Android, Google, and Ovirt, we
addressed the following research questions:
2372022 IEEE/ACM 44th International Conference on Software Engineering (ICSE)
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 07:52:58 UTC from IEEE Xplore.  Restrictions apply. ICSE â€™22, May 21Å›29, 2022, Pittsburgh, PA, USA Patanamon Thongtanunam, Chanathip Pornprasit, and Chakkrit Tantithamthavorn
(RQ1) Can A utoTransform transform code better than
Tufanoet al.approach?
Results. Whengeneratingonecandidateoftheafterversionfor
14,750changedmethodsinthetestingdatasets,our AutoTrans-
formcan correctly transform source code for 1,413 changed meth-
odswhichis567%higherthantheTufano etal.approach.Specif-
ically,forthechangedmethodsw/newtokens,our AutoTrans-
formcancorrectlytransform1,060methods,whiletheTufano et
al.approachcouldnotcorrectlytransformanyofthechangedmeth-
ods w/ new tokens. For the changed methods w/o new tokens, our
AutoTransform can correctly transform 353 changed methods,
while theTufano et al.approach cancorrectly transform only 212
changed methods.
(RQ2) What are the contributions of AutoTransformâ€™s
components?
Results. UsingsubwordtokenizationbyBPEenablesour Auto-
Transform to achieve perfect predictions 284% higher than using
codeabstractionliketheTufano etal.approach.Furthermore,using
the Transformer architecture can increase perfect predictions at
leastby17%,comparedtousingtheRNNarchitecture.Inparticular,
wefoundthatthepercentageimprovementinperfectpredictions
is much higher for the medium methods (i.e., 183% - 507%).
Significance & Contributions. The results of our work high-
lightthesubstantialimprovementofourapproachforcodetrans-
formation in the context of code review. More specifically, our
AutoTransform cantransformawiderrangeofchangedmethods
(i.e.,methodswithnewtokensandmethodswithlongsequences)
than the state-of-the-art approach [ 60]. The proposed approach,
results, and insights presented in this paper contribute towards
automated code transformation for code reviews, which could help
developersreduce theireffort inmodifyingsourcecodeand expe-
dite the code review process.
Novelty. This paper is the first to present:
â€¢AutoTransform, i.e., a Transformer-based NMT approach
to transform source code from the version before the imple-
mentation of code changes to the version that is reviewed
and eventually merged.
â€¢The use of Byte-Pair Encoding (BPE) subword tokenization
toaddressthelimitationoftransformingchangedmethods
that have newly-introduced identifiers/literals.
â€¢An empirical evaluation of our AutoTransform and the
Tufanoet al.approach [ 60] based on a large-scale dataset
with two types of changed methods.
â€¢Anablationstudytoquantifythecontributionsofthetwo
components (i.e., BPE and Transformer) in our proposed
approach.
OpenScience. Tofacilitatefuturework,thedatasetsof147,553
extractedchangedmethods,scriptsofour AutoTransform,and
experimental results (e.g., raw predictions) are available online [ 4].
PaperOrganization. Section2presentsanddiscussesthelimi-
tations of the state-of-the-art approach [ 60]. Section 3 presents our
AutoTransform.Section4describesourcasestudydesign.Section
5 presents the results, while Section 6 discusses the results. Section
7 discusses related work. Section 8 discusses possible threats to the
validity. Section 9 draws the conclusions.2 THE STATE-OF-THE-ART CODE
TRANSFORMATION FOR CODE REVIEWS
Recently,Tufano etal.[60]proposedaNeuralMachineTranslation
(NMT) approach to automatically transform the source code of the
before version of a method (i.e., the version before the implemen-
tation of code changes) to its after version (i.e., the version after
the code changes are reviewed and merged). Broadly speaking, Tu-
fanoet al.(1) performed code abstraction to reduce vocabulary
size by replacing actual identifier/literals with reusable IDs and
(2) built an NMT model based on a Recurrent Neural Network
(RNN)architecture totransformthetokensequenceofthebefore
version to the token sequence of the after version. Their approach
wasevaluatedbasedonthechangedmethodsthatwereextracted
from three Gerrit code review repositories, namely Android [ 3],
Google[6],andOvirt[ 7].Webrieflydescribetheirapproachbelow.
(Step 1) Code Abstraction: Since NMT models are likely to be
inaccurateandslowwhendealingwithalargevocabularysize[ 26],
Tufanoet al.proposed to abstract code tokens into reusable IDs
to reduce the vocabulary size. More specifically, for both before
and after versions (ğ‘šğ‘,ğ‘šğ‘)of each changed method, Tufano et
al.replaced identifiers (i.e., type, method, and variable names) and
literals(i.e.,int,double,char,stringvalues)withareusableID.A
reusableIDmeansthatanIDisallowedtobereusedacrossdifferent
changed methods (e.g., the first variable appearing in a changed
methodwillbealwaysreplacedwith VAR_1).Attheendofthisstep,
from the original source code of (ğ‘šğ‘,ğ‘šğ‘), theabstracted code
sequences (ğ‘ğ‘šğ‘,ğ‘ğ‘šğ‘)wereobtained.Inaddition,foreachchanged
method, a map ğ‘€(ğ‘ğ‘šğ‘)was also generated, which will be used to
map the IDs back to the actual identifiers/literals.
(Step 2) Build an NMT model: To build an NMT model, Tufano et
al.usedaRecurrentNeuralNetwork(RNN)Encoder-Decoderar-
chitecture with the attention mechanism [ 12,18,36,53]. Given
the abstracted code sequences (ğ‘ğ‘šğ‘,ğ‘ğ‘šğ‘)from Step 1, an RNN
Encoderwilllearnthesesequencesbyestimatingtheconditional
probability ğ‘(ğ‘¦1,...,ğ‘¦ğ‘¡â€²|ğ‘¥1,...,ğ‘¥ğ‘¡), whereğ‘¥1,...,ğ‘¥ğ‘¡is the sequence
ofamğ‘andğ‘¦1,...,ğ‘¦ğ‘¡â€²is the sequence of amğ‘while the sequence
lengthsğ‘¡andğ‘¡â€²may differ. A bi-directional RNN Encoder [ 18] was
used to learn the abstracted code sequence of the before version
ğ‘ğ‘šğ‘from left-to-right and right-to-left when creating sequence
representations.AnRNNDecoderwasthenusedtoestimatethe
probability for each token ğ‘¦ğ‘–in the abstracted code sequence of
the after version ğ‘ğ‘šğ‘based on the recurrent state ğ‘ ğ‘–, the previous
tokensğ‘¦1..ğ‘–,andacontextvector ğ‘ğ‘–.Thevector ğ‘ğ‘–istheattention
vector which is computed as a weighted average of the hidden
states, allowing the RNN model to pay attention to particular parts
ofğ‘ğ‘šğ‘, when predicting token ğ‘¦ğ‘–forğ‘ğ‘šğ‘.
(Step3)Generatepredictions: Giventheabstractedcodesequence
of the before version of an unseen method (i.e., a testing instance)
ğ‘ğ‘šğ‘¡
ğ‘,theRNNmodelintheTufano etal.approachgeneratedthe
abstractedcode sequenceof theafterversion ğ‘ğ‘šğ‘¡ğ‘.A beamsearch
strategywasusedtoobtain ğ‘˜sequencecandidatesfor ğ‘ğ‘šğ‘¡ğ‘.Since
the generated output sequences are the abstracted code sequences,
Tufanoet al.replaced the IDs found in ğ‘ğ‘šğ‘¡ğ‘back to the actual
identifier/literals based on the map ğ‘€(ğ‘ğ‘šğ‘¡
ğ‘).
Limitations. AlthoughthestudyofTufano etal.shedslightthat
theirNMTapproachcantransformsourcecodewithmeaningful
238
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 07:52:58 UTC from IEEE Xplore.  Restrictions apply. AutoTransform: Automated Code Transformation to Support Modern Code Review Process ICSE â€™22, May 21Å›29, 2022, Pittsburgh, PA, USA
Before version After version
Reusable IDs
Byte-Pair-Encoding (BPE)Generated Sequence from RNN
Generated Sequence from TransformerdTufano et al. Approach (Code Abstraction + RNN)
Our AutoTransform (Subword Tokenization + Transformer)236
237public void onSuccess ( final TYPE_1 result ) { METHOD_1 ( VAR_1 , VAR_2 ,
line,
VAR_3 ) ; }241
242
243
244Original code
publicvoidonSuccess (finalcom.google.gwtjsonrpc.client.VoidResult result)
{
createCommentEditor (suggestRow ,column,line,sidePanel ) ;
}
Abstracted code233
234
235Original code
publicvoidonSuccess (finalcom.google.gwtjsonrpc.client.VoidResult result)
{
createCommentEditor (suggestRow ,column,line,side) ;
}
245
246Abstracted code
public void onSuccess ( final TYPE_1 result ) { METHOD_1 ( VAR_1 , VAR_2 , line
,
VAR_4 ) ; }
239
240public void on@@ Success ( final com.google.@@ gwtjsonrpc.@@ client.@@
VoidResult result ) { create@@ Comment@@ Editor ( suggest@@ Row , col@@ umn ,
line , side ) ; }247
248
249public void on@@ Success ( final com.google.@@ gwtjsonrpc.@@ client.@@
VoidResult
result ) { create@@ Comment@@ Editor ( suggest@@ Row , col@@ umn ,
line , side@@ Panel ) ; }
Figure 1: A motivating example for an unknown identifier/literal for the newly-introduced abstracted token (Limitation 1).
T
able 1: The number of changed methods with and without
new tokens in the after version.
Dataset Method size w/o New Tokens w/ New Tokens
Android Small 4,437 (18%) 20,640 (82%)
Me
dium 4,593 (16%) 24,547 (84%)
Google Small 2,289 (20%) 9,070 (80%)
Me
dium 2,833 (20%) 11,624 (80%)
Ovirt Small 4,734 (17%) 23,282 (83%)
Me
dium 6,229 (16%) 33,275 (84%)
The detail of data preparation is provided in Section 4.2.
code changes (e.g., bug-fix, refactoring) [ 60], their approach still
has the following limitations.
Limitation 1: Unknownidentifiers/literalsforthenewto-
kens
appearing in the after version. We hypothesize that the
Tufanoet al.approach may not be able to correctly transform code
whennew tokensappearin the afterversion butdid notappear in
thebeforeversion.Forsuchacase,thecodeabstractionapproach
withReusableIDsofTufano etal.cannotmaptheIDofanewtoken
back to the actual identifiers or actual literals.
Indeed,itispossiblethatdevelopersintroducednewtokensthat
didnotappearinthebeforeversion.Figure1illustratesamotivating
exampleforLimitation1.Asshownintheexample,the sidevariable
is changed to sidePanel . Thus, the sidePanel variable is a new
token appearing inthe after version. Bythe code abstraction with
ReusableIDofTufano etal.,anewIDwillbeassignedto sidePanel ,
i.e.,VAR_4 = sidePanel . Note that this limitation is different from
the Out-Of-Vocabulary (OOV) problem [ 31] since the Tufano et
al.approachmaystillbeabletopredictthecorrectID(i.e., VAR_4)
for the after version ğ‘ğ‘šğ‘instead of assigning a special tokens (e.g.,
<UNK>). However, this VAR_4cannot be realistically mapped back to
the actual identifier (i.e., sidePanel ) since it does not appear in the
before version ğ‘šğ‘nor its mapping ğ‘€(ğ‘ğ‘šğ‘).
Moreover,whenweanalyzethechangedmethodsinthedatasets
ofTufano etal.[60],wefindthatasmuchas80%-84%ofthechanged
methodhavenewtokensappearingintheafterversion(seeTable1).
However,Tufano etal.excludethesechangedmethodswithnew
tokens,whileusingonlytheremaining16%-20%ofthechanged
methodswithoutnewtokensfortheir experiment[ 60].Thus, the
Tufanoet al.approach may be applicable to only a small set of
changed methods, limiting its applicability in real-world contexts.Limitation 2: Suboptimalperformancewhenthesequences
b
ecomelonger. Priorstudieshaveraisedaconcernthattheperfor-
manceoftheRNNarchitecturewillbesuboptimalwhensequences
become longer [ 12,36]. Although Tufano et al.leveraged the at-
tention mechanism to handle changed methods that have long
sequences, a recent study by Ding et al.[22] noted that such atten-
tion mechanism for the RNN architecture still has difficulties in re-
membering long-term dependencies between tokens in a sequence.
In particular, the attention mechanism only computes attention
weightsbasedonthefinalhiddenstatesoftheRNNarchitecture,
instead of using any given intermediate states from the encoder,
causingtheRNNmodeltoforgettokensseenlongago.Thus,we
hypothesize that the performance of the Tufano et al.approach
which is based on the RNN architecture may be suboptimal for the
changed methods with long sequences.
3 AUTOTRANSFORM
Inthissection,wepresentour AutoTransform,aNeuralMachine
Translation(NMT)approachthatcantransformcode(1)whennew
tokens appear in the after version; and (2) when code sequences
becomelonger. AutoTransform leveragesaByte-Pair-Encoding
(BPE) approach [ 52] to handle new tokens that may appear in
the after version; and leverages a novel NMT architecture called
Transformer [ 62] to address the suboptimal performance of the
RNN architecture used in the Tufano et al.approach.
Overview. Figure2providesanoverviewofour AutoTrans-
formapproach,whichconsistsoftwostages:trainingandinference.
Duringthetrainingstage,our AutoTransform performstwomain
steps. In Step 1, we perform subword tokenization on the original
sourcecodeofthebeforeandafterversionsofachangedmethod
(ğ‘šğ‘,ğ‘šğ‘)usingBPE,whichproducessubwordsequences( ğ‘ ğ‘šğ‘,ğ‘ ğ‘šğ‘).
InStep 2,weusethesesubwordsequences( ğ‘ ğ‘šğ‘,ğ‘ ğ‘šğ‘)totraina
Transformer model. The inference stage is for transforming the
before version ( ğ‘šğ‘¡
ğ‘) of given methods (i.e., testing data) to the after
version (ğ‘šğ‘¡ğ‘). To do so, we perform subword-tokenization on the
before version in Step 1bto produce a subword sequences ğ‘ ğ‘šğ‘¡
ğ‘.
Then, inStep 3, weuse the TransformerNMT model togenerate
a prediction for the source code of the after version ğ‘šğ‘¡ğ‘. Below, we
provide details for each step in our approach.
239
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 07:52:58 UTC from IEEE Xplore.  Restrictions apply. ICSE â€™22, May 21Å›29, 2022, Pittsburgh, PA, USA Patanamon Thongtanunam, Chanathip Pornprasit, and Chakkrit Tantithamthavorn
smb
Generate Merge 
Operations(â€˜câ€™,â€™oâ€™) â†’ â€˜coâ€™
(â€˜coâ€™,â€™lâ€™) â†’ â€˜colâ€™
.
.
.mb ma
Original source codeâ€¨
of the befor
e and after 
versions in training dataA list of  
merge operations
smbsma
Subword sequencesâ€¨
of the befor
e and after 
versions
mb
Original source code of the 
befor
e version in testing datasmb
A subword sequence of 
the befor
e versionApply Merge
OperationsSubword Tokenization by BPETraining
InferencesmaBefore version 
(Input) 
After version 
(T
arget) Embedding & 
Positional EncodingMulti-Headâ€¨
Self-AttentionFeed-ForwardEmbedding 
vectors
Apply Merge 
OperationsEncoder Layer x N eEncoder Block 
Embedding & 
Positional Encoding Embedding 
vectorsMasked Multi-Headâ€¨
Self-AttentionFeed-ForwardDecoder Layer x N dDecoder Block 
Multi-Headâ€¨
Self-AttentionLearning Code Transformation by Transformer
smb
Before version 
(Input) 
Vocab
Position   1    2      â€¦â€¦â€¦  nv1
v2
v3
v4
Word probabilitiesGenerate Target 
Sequence
Generated sequenceConvert 
Subwords to Codema
Prediction
A Transformer modelGenerating Predictions12
31a
1b
1b2a2b
Attention 
vectors
2c2d
3a 3b3c
sma
Figure 2: An overview of our A ut oTransform.
3.1 Subword Tokenization
To reduce the vocabulary size and handle new tokens that may
appearintheafterversion,weperformsubwordtokenizationusing
Byte-Pair-Encoding (BPE) [ 52]. BPE is a tokenization approach
that splits a word (i.e., a code token) into a sequence of frequently
occurring subwords. For example, a code token columnin Figure
1 will be split into a sequence of col@@andumn, where @@is a
subwordseparator.WhenusingBPE,thevocabularywillmainly
contain subwords which are more frequently occurring than the
whole code tokens. Prior studies also have shown that BPE can
effectivelyaddressthelargevocabularysizethanothertokenization
approaches (e.g., camel-case splitting) [ 31]. Moreover, BPE will
enable our approach to address the new token by allowing the
NMT model to generate a new code token based on a combination
of subwords existing across all methods in the training data. For
example, the code token sidePanel which is introduced in the after
version(seeFigure1)canbeconstructedbasedonacombinationof
side@@andPanelif these two subwords exist in the training data.
Thesubwordtokenizationconsistsoftwomainsteps(seeFigure
2).Step 1aisforgeneratingmergeoperationswhichareusedto
determine how a code token should be split. To generate merge
operations, BPE will first split all code tokens into characters se-
quences. Then, BPE generates a merge operation by identifying
the most frequent symbol pair (e.g., the pair of two consecutive
characters) that should be merged into a new symbol. For example,
giventhat(â€˜ câ€™,â€˜oâ€™)isthemostfrequentsymbolpairinthecorpus,
the merge operation will be (â€˜ câ€™, â€˜oâ€™)â†’â€˜coâ€™. Then, BPE replaces all
oftheco-occurrenceof(â€˜ câ€™,â€˜oâ€™)withâ€˜coâ€™withoutremovingâ€˜ câ€™or
â€˜oâ€™ (which may still appear alone). After the previous merge opera-
tion is applied, BPE generates a new merge operation based on the
frequency of current symbol pairs, e.g., (â€˜ coâ€™, â€˜lâ€™)â†’â€˜colâ€™. This step
is repeated until it reaches a given number of merge operations.
To ensure the consistency of subword tokenization between the
before and after versions of a changed method, we use jointBPE
to generate merge operations based on the union of code tokens
from both before and after versions. Note that we generated merge
operations based on the training data.
In Step 1b, we apply merge operations to split code tokens
into subwords. To do so, BPE will first split all code tokens into
sequences of characters. Then, the generated merge operations are
appliedtothebeforeandafterversionsinthetrainingdata.Notethatthelowerthenumberofmergeoperationsapplied,thesmaller
the size of vocabulary is. We also apply the same list of merge
operationstothebeforeversioninthetestingdata.Notethatwe
didnotsplitJavakeywordssincetheyarecommonlyusedacross
changed methods.
3.2 Learning Code Transformation
To learn code transformation,we train a Transformer-based NMT
modelusingthesubwordsequences( ğ‘ ğ‘šğ‘,ğ‘ ğ‘šğ‘)ofthebeforeand
after versions. Unlike the RNN architecture, the Transformer archi-
tecture entirely relies on an attention mechanism without using
thesequence-alignedRNNsorconvolutionnetworks[ 62],which
allows the Transformer model to better pay attention to any set of
tokens across arbitrarily long distances. Transformer uses a self-
attention functiontocomputeattentionweightsbasedonalltokens
in a sequence, where attention weights indicate how each token
isrelevanttoallothertokensinthesequence.Thisself-attention
function enables the Transformer model to capture the contextual
relationshipbetweenalltokensinthewholesequenceinsteadofre-
lying on the limited number of the final hidden states like the RNN
architecture. In addition, instead of using a single self-attention,
theTransformerarchitectureemploysamulti-headself-attention,
which calculates attention weights â„times (where â„is the number
of heads) based on the different parts of input data.
TheTransformerarchitectureconsistsoftwomajorcomponents:
an Encoder block which encodes a subword sequence into a vector
representation,andaDecoderblockwhichdecodestherepresen-
tation into another subword sequence. The Transformer performs
thefollowingfourmainsteps(seeFigure2).First,inStep 2a,given
a subword sequence of the before version ğ‘ ğ‘šğ‘=(ğ‘ 1,...,ğ‘ ğ‘¡), the
Transformer embeds the tokens into vectors and uses positional
encoding to add information about the token position of the se-
quenceintotheembeddingvectors.Second,theembeddingvectors
are then fed into the encoder block 2b, which is composed of a
stack of multiple Encoder layers (where ğ‘ğ‘’is the number of En-
coderlayers).Eachlayerconsistsoftwosub-layers:amulti-head
self-attention and a fully-connected feed forward network (FFN)
which computes attention weights and generates attention vectors.
The attention vectors will be used to inform the Decoder block
about which tokens that should be paid attention.
240
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 07:52:58 UTC from IEEE Xplore.  Restrictions apply. AutoTransform: Automated Code Transformation to Support Modern Code Review Process ICSE â€™22, May 21Å›29, 2022, Pittsburgh, PA, USA
The subword sequence of the after version ğ‘ ğ‘šğ‘=(ğ‘¢1,...,ğ‘¢ğ‘¡â€²)
is used as an input of the Decoder block. Similarly, in Step 2c,
the subword sequence of ğ‘ ğ‘šğ‘is embeded into vectors with the
position information. Then, the embedding vectors are fed into the
Decoder block 2dto generate an encoded representation of the
target sequence. The Decoder block is also composed of a stack of
multipleDecoderlayers(where ğ‘ğ‘‘isthenumberofDecoderlayers).
EachDecoderlayeralsoconsistsofmulti-headself-attentionand
FFN sub-layers. However, before the embedding vectors are fed
intothemulti-headself-attentionsub-layer,themaskedmulti-head
self-attention layer is used to ensure that only previous tokens
(i.e.,ğ‘¢1,...,ğ‘¢ğ‘–âˆ’1)areusedindecodingforacurrenttoken ğ‘¢ğ‘–.After
that, the multi-head self-attention with subsequent FFN generates
an encoded representation of the target sequence. The encoded
representation are converted into word probabilities and the final
output sequence using the linear and softmax functions. Finally,
thelossfunctionwillcomparethisoutputsequencewiththetarget
sequence to estimate an error.
3.3 Generating Predictions
Theinferencestageaimstogeneratetheafterversionforagiven
methodbasedonthebeforeversion ğ‘šğ‘¡
ğ‘inthetestingdata.Starting
from the beforeversion ğ‘šğ‘¡
ğ‘, we performsubword tokenization by
applying the list of the merge operations generated in the training
stage to produce a subword sequence ( ğ‘ ğ‘šğ‘¡
ğ‘) for the before version
inStep1b.Then,inStep 3,weuseourTransformermodeltogen-
erate the target sequence, i.e., the after version ğ‘šğ‘¡ğ‘. In particular, in
Step3a, given the subword sequence ğ‘ ğ‘šğ‘¡
ğ‘, the Transformer model
will estimate a probability of each subword in the vocabulary at
each position in the output sequence. Then, based on the gener-
ated subword probabilities, in Step 3b, we use the beam search
approach [53] to generate the target sequence.
Broadlyspeaking,beamsearchgeneratesthetargetsequenceby
selectingthebestsubwordforeachpositionintheoutputsequence
basedonthegeneratedsubwordprobabilities.Inparticular,beam
searchselectsasubwordforacurrentpositionbasedontheselected
subwords in the previous positions. The beam width ğ‘˜is used to
specifythenumberofpreviouspositionsthatshouldbeconsidered
andthenumberof sequencecandidatesthatwillbegenerated. In
other words, the selection of a subword for a position ğ‘–is based on
the conditional probabilities of selected subwords in the positions
ğ‘–âˆ’ğ‘˜toğ‘–âˆ’1.Finally,thebeamsearchgeneratesthebest ğ‘˜sequence
candidates for the target sequence.
The sequence candidates generated in Step 3bare the subword
sequences.Hence,inStep 3c,weconvertthesesubwordsequences
back to the code sequences. This step is simply performed by con-
catenating the subwords ending with â€˜@@â€™with the subsequent
subword. For example, the subwords [â€˜col@@â€™, â€˜umnâ€™] are con-
verted back to column.
4 CASE STUDY DESIGN
Inthissection,wepresentthemotivationofourresearchquestions,
data preparation, and experimental setup for our case study.4.1 Research Questions
To evaluate our AutoTransform, we formulate the following two
research questions.
(RQ1) Can A utoTransform transform code better than
Tufanoet al.approach?
Motivation. Inthiswork,weproposedour AutoTransform to
address the limitations of the state-of-the-art approach [ 60]. In
particular, the goal of our AutoTransform is to allow an NMT
model to better transform code (1) when new tokens appear in
theafterversion;and(2)whenthecodesequencebecomelonger.
Hence, we set out this RQ to evaluate our AutoTransform based
on the two aforementioned aspects.
(RQ2) What are the contributions of AutoTransformâ€™s
components?
Motivation. Toaddress thelimitationsof thestate-of-the-artap-
proach [60], we used two different techniques in our AutoTrans-
form, i.e., (1) BPE [ 52] to reduce the vocabulary size and handle
newtokensappearingintheafterversionand(2)Transformer[ 62]
to learn codetransformation. In this RQ, we setout to empirically
evaluate the contribution of each technique to the performance of
ourAutoTransform,comparedagainstthetechniquesusedinthe
Tufanoet al.approach.
4.2 Datasets
In this work, we obtain the datasets from the work of Tufano et
al.[5]. The datasets consist of 630,858 changed methods which
wereextractedfrom58,728reviewsacrossthreeGerritcodereview
repositories, namely Android [ 3], Google [ 6], and Ovirt [ 7]. For
each changed method, the datasets consist of the source code of
thebeforeandafterversions,i.e., (ğ‘šğ‘,ğ‘šğ‘).Forourexperiment,we
performdatapreparationontheseobtaineddatasetstoclassifyand
selectthechangedmethods.Notethatwedidnotusethefiltered
datasetsthatwereusedintheexperimentofTufano etal.[60],since
theirfiltereddatasetsdonotincludethechangedmethodsofwhich
the after version contains a new token (which is considered in this
work).Table1providesanoverviewofthestudieddatasetsafter
the data preparation, which we describe in details below.
Data Preparation. We first classify the changed methods into
two types of changed methods: (1) the changed methods of which
theafterversion doesnotcontainnewidentifiers/literals addition-
allyfromthebeforeversion(i.e.,changedmethods without new
tokens)and(2)thechangedmethodsofwhichtheafterversion con-
tainsnewidentifiers/literalsadditionallyfromthebeforeversion
(i.e., changed methods withnew tokens). The changed methods
withoutnew tokenswere usedto fairlyevaluate our AutoTrans-
formagainst the Tufano et al.approach under the same condition
as in the prior experiment [ 60], while the changed methods with
newtokenswereusedtoevaluatewhetherour AutoTransform
cantransformsourcecodewhenanewtokenappearsintheafter
version (i.e., evaluating the hypothesis discussed in Limitation 1).
Forthechangedmethods withoutnewtokens,weusedthesame
selection approach as in the prior work [ 60]. More specifically, the
changed methods withoutnew tokens are those methods of which
theafterversion ğ‘šğ‘containsonly(1)Javakeywords;(2)Top-300fre-
quent identifiers/literals [ 2]; and (3) identifiers and literals that are
already available in the before version ğ‘šğ‘. The remaining changed
241
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 07:52:58 UTC from IEEE Xplore.  Restrictions apply. ICSE â€™22, May 21Å›29, 2022, Pittsburgh, PA, USA Patanamon Thongtanunam, Chanathip Pornprasit, and Chakkrit Tantithamthavorn
Table2:Vocabularysizefortheoriginal,subwordtokenized,
and abstracted methods in the training datasets.
Dataset Subword Tokenized
(Method size) Change Type Original BPE2K BPE5K Abs
Android w/o new tokens 12,052 2,702 4,230356
(Small) w/ new tokens 43,795 7,448 9,247408
Google w/o new tokens 5,012 1,719 2,751333
(Small) w/ new tokens 13,737 3,417 4,884383
Ovirtw/o new tokens 9,772 1,992 3,575306
(Small) w/o new tokens 30,562 4,243 6,042355
Android w/o new tokens 22,296 5,165 6,860447
(Me
dium) w/ new tokens 76,264 15,585 17,874 496
Google w/o new tokens 9,340 2,831 4,046371
(Me
dium) w/ new tokens 22,140 6,334 8,052422
Ovirtw/o new tokens 17,680 3,231 4,958353
(Me
dium)w/o new tokens 44,317 7,528 9,674422
*The w/ and w/o new tokens change types are mutually exclusive sets.
methodsofwhichtheafterversioncontainstokensnotlistedinthe
aforementionedtokencategoriesareclassifiedasthechangedmeth-
odswithnewtokens.Notethatweexcludethechangedmethods
that were completely added or deleted (i.e., ğ‘šğ‘=âˆ…orğ‘šğ‘=âˆ…) and
thechangedmethodsofwhichbeforeandafterversionsappearthe
same (i.e., ğ‘šğ‘=ğ‘šğ‘) since NMT models would not be able to learn
any code transformation patterns from these methods. In addition,
weremovetheduplicatechangedmethods(i.e.,themethodswhose
(ğ‘šğ‘,ğ‘šğ‘)are exactly same) to ensure that none of the duplicate
methods in testing will appear in training.
Afterselectingandclassifyingthechangedmethods,wesepa-
ratethedatasetsintotwomethodsizes,i.e.,smallandmediumto
evaluate the hypothesis discussed in Limitation 2 (see Section 2).
Thesmallmethods are those methods of which before and after
versions have a sequence length no longer than 50 tokens. The
medium methods are those methods of which before and after
versions have a sequence length between 50-100 tokens. Similar
to prior work [ 60], we disregard the changed methods that have
a sequence longer than 100 tokens because large methods have
a long tail distribution of sequence lengths with a high variance,
which might be problematic when training an NMT model.
In total, we conduct an experiment based on 12 datasets, i.e.,
3 repositories (Android, Google, Ovirt) Ã—2 method sizes (small,
medium) Ã—2changetypes(w/oandw/newtokens).Eachofthe
datasets is then partitioned into training (80%), validation (10%),
and testing (10%).
4.3 Experimental Setup
This section provides setup details for our experiment.
Source Code Pre-processing. Before we perform subword
tokenization (for our AutoTransform) and code abstraction (for
Tufanoet al.approach), we perform word-level tokenization to
converttheformattedcodeintoasequenceofcodetokens.Todo
so,foreachversionofeachchangedmethod,wesimplyseparate
code lines, identifiers, literals, Java keywords, and Java reserved
characters(e.g., ;(){})byaspace.Wedonotconvertcodetokens
tolowercasesbecausetheprogramminglanguage(i.e.,Java)ofthe
studied datasets is case-sensitive. We also do not split code tokensTable 3: Hyper-parameter settings for the Transformer and
RNN models.
Hyper-Parameter Transformer Model RNN Model
#Encoder Layers ( ğ‘ğ‘’) {1, 2} {1, 2}
#De
coder Layers ( ğ‘ğ‘‘) {2, 4} {2, 4}
Cell
Types n/a{GRU, LSTM}
#Cells n/a {256, 512}
Emb
edding Size n/a {256, 512}
Attention
Size n/a {256, 512}
#Attention
Heads (â„) {8, 16} n/a
Hidden
Size (â„ğ‘ ) {256, 512} n/a
Total #Settings 8 10
withcompoundwords(e.g.,camel-case)sinceour AutoTransform
already performs subword tokenization and such compound-word
splitting is not performed in Tufano et al.approach.
For ourAutoTransform, we use the implementation of Sen-
nrichetal.[52]toperformsubwordtokenizationusingByte-Pair
Encoding(BPE)[ 1].Inthiswork,weexperimentwithtwoencoding
sizes,i.e.,thenumberofmergeoperations:2,000(BPE2K )and5,000
(BPE5K),whichsubstantiallyreducethevocabularysize(atleast
approximately by 50%; see Table 2). For Tufano et al.approach, we
usesrc2abs[9]toabstract tokenswithreusableIDs(Abs )andgen-
erateamap ğ‘€foreachchangedmethod.Similartopriorwork[ 60],
we do not abstract Java keywords and the top-300 frequent iden-
tifier/literals[ 2].Toensurethattheidentifier/literalappearingin
bothversionshasthesameID,weuseapairmodeof src2absfor
thetrainingandvalidationdata,whileasinglemodeisusedforthe
before version in the testing data.
NMTModels&Hyper-ParameterSettings. TobuildaTrans-
former model in our AutoTransform, we use the implementation
of theTensor2Tensor library [10]. To build an RNN model in
Tufanoet al.approach, we use the implementation of the seq2seq
library [8] which is also used in the prior work [ 60]. To ensure a
fair comparison, we use similar combinations of hyper-parameters
(whereapplicable)forbothTransformerandRNNmodels(seeTable
3). Therefore, we experiment with eight hyper-parameter settings
for our Transformer model in AutoTransform. For the RNN mod-
els, we use ten hyper-parameter settings which are originally used
in the experiment of the prior work [60].
When training the models for both approaches, we set the maxi-
mumnumberofepochssimilarasinthepriorwork[ 60].1Toavoid
the overfitting of our models to the training data, we select the
modelcheckpoint(i.e.,themodelthatwastraineduntilaparticular
number of epochs) that achieves the lowest loss value computed
based on the validation data (not the testing data).
Evaluation Measure. To evaluate the performance of our Au-
toTransform and Tufano et al.approach, we measure the number
of methods for which an approach achieves a perfect prediction, i.e.,
thegeneratedafterversion exactlymatchestheground-truth(i.e.,
the actual after version). Note that we convert the generated after
version(i.e.,thesubwordsequenceofour AutoTransform,and
1Note that #epochs are calculated based on the size of training data, batch size
and #train steps which are calculated differently for the Tensor2Tensor andseq2seq
libraries. The details are provided in the Supplementary Materials [4].
242
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 07:52:58 UTC from IEEE Xplore.  Restrictions apply. AutoTransform: Automated Code Transformation to Support Modern Code Review Process ICSE â€™22, May 21Å›29, 2022, Pittsburgh, PA, USA
T
able 4: Perfect predictions (#PP) of our A utoTransform and Tufano et al.approach approach for the small and medium
changed method with and without new tokens in the after version. The percentage value in the parenthesis indicates the
percentage improvement of our AutoTransform.
Beam width = 1 Beam width = 5 Beam width = 10
Dataset AutoTransform T ufanoet al.AutoTransform T ufanoet al.AutoTransform T ufanoet al.
(Method Size) Change Type #Test #PP #PP #PP #PP #PP #PP
Android w/o new tokens 443 84 53 125 83 130 107
(Small)
w/ new tokens 2,064 108 0 206 0 233 0
Google w/o new tokens 228 11 14 22 36 29 42
(Small)
w/ new tokens 907 40 0 81 0 97 0
Ovirt w/o new tokens 473 73 86 132 173 145 200
(Small)
w/ new tokens 2,328 352 0 618 0 715 0
Android w/o new tokens 459 58 32 85 67 89 78
(Me
dium) w/ new tokens 2,454 124 0 247 0 289 0
Google w/o new tokens 283 16 9 28 18 33 22
(Me
dium) w/ new tokens 1,162 18 0 46 0 63 0
Ovirt w/o new tokens 622 111 18 179 49 199 62
(Me
dium) w/ new tokens 3,327 415 0 833 0 992 0
Totalw/o new tokens 2,508 353 212 571 426 625 511
w/
new tokens 12,242 1,060 0 2,031 0 2,389 0
Both
14,750 1,413 (+567%) 212 2,602 (+511%) 426 3,014 (+490%) 511
theabstractedcodesequenceoftheTufano etal.approach)backto
the code sequence before matching it with the ground-truth.
In our experiment, we use three different beam widths (i.e., ğ‘˜=
{1,5,10})whengenerating theafterversion.Thus, ifoneofthe ğ‘˜
sequencecandidatesexactlymatchestheground-truth,weconsider
that the NMT approach achieves a perfect prediction, i.e., the code
iscorrectlytransformed.Wedonotuseothermetrics,e.g.,BLEU
which measures the overlap (or similarity) between the generated
and ground-truth sequences, since similarity cannot imply that the
generatedsequencesareviableforcodeimplementation.Ding et
al.also argue that BLEU should not be used to evaluate the code
transformationsincethesequencesthatare similar(i.e.,fewcode
tokens are different between the two sequences) may have largely-
different intentions or semantics [22].
5 RESULTS
RQ1: Can AutoTransform transform code
better than Tufano et al.approach?
Approach. To address our RQ1, we evaluate how well our Auto-
Transform can transform the source code of the before version
ğ‘šğ‘totheafterversion ğ‘šğ‘ofgivenmethods(inthetestingdata),
compareagainsttheapproachofTufano etal.[60].Therefore,we
build an NMT model for each of the 12 datasets, i.e., small and
medium methods with and without new tokens across three repos-
itories(see Table1).For thisRQ,we usethemaximum numberof
merge operations of 2,000 (i.e., BPE2K) in our AutoTransform. In
total,wetrain96Transformermodelsforour AutoTransform (i.e.,
12 datasets Ã—8 hyper-parameter settings); and 120 RNN models
forTufano etal.approach(i.e.,12datasets Ã—10hyper-parameter
settings).Then,foreachapproachandforeachdataset,weselect
the model with the best hyper-parameter setting that achieves the
lowest loss value computed based on the validation data. Finally,
we measure perfect predictions based on the testing data. To quan-
tifythemagnitudeofimprovementforour AutoTransform,wecompute a percentage improvement of perfect predictions using a
calculation of(#ğ‘ƒğ‘ƒourâˆ’#ğ‘ƒğ‘ƒTufano)Ã—100%
#ğ‘ƒ ğ‘ƒTufano.
Results. Table 4 shows the results of perfect predictions of our
AutoTransform and Tufano et al.approach of 14,750 changed
methodsacrossthe12datasets.Theresultsarebasedonthreebeam
widths,ğ‘˜={1,5,10},whereğ‘˜isthenumberofsequencecandidates
for a given method.
Whenconsideringbothchangetypes(i.e.,w/oandw/new
tokens), our AutoTransform achieves a perfect prediction
for1,413methodswhichis567%higherthantheperfectpre-
dictionsachievedbytheTufano etal.approach. Table4shows
that when a beam width is 1 and both change types are considered,
ourAutoTransform achieves a perfect prediction for 34 - 526
methods which accounted for 2% (34
1,445; for Google Medium) - 13%
(526
3,949; for Ovirt Medium) of the methods in testing data. On the
otherhand,Tufano etal.approachachievesaperfectpredictionfor
9-86methodswhichaccountedforonly0.62%(9
1,445;forGoogle
Medium)-3%(86
2,801;forOvirtSmall).Intotalacrossthe12datasets,
ourAutoTransform cancorrectlytransform1,413methods,which
is567%higherthantheperfectpredictionsachievedbyTufano et
al.approach.
Evenwhenweincreasethebeamwidthto5and10,our Auto-
Transform canachievehigherperfectpredictions,i.e.,5%(74
1,445for
GoogleMedium)-28%(1,102
3,949forOvirtMedium)atbeamwidth=5
and7%(96
1,445forGoogleMedium)-30%(1,191
3,949forOvirtMedium)at
beamwidth=10.Theperfectpredictionsareimprovedby511%for
thebeamwidthof5(and490%forthebeamwidthof10)whencom-
paredtotheperfectpredictionsachievedbyTufano etal.approach.
Theseresultsindicatethatthenumberofmethodsforwhichour
AutoTransform canachieve aperfect predictionissubstantially
higher than Tufano et al.approach.
For the changed methods withnew tokens appearing in
theafterversion,our AutoTransform achieveaperfectpre-
dictionfor18-415methods. At beam width is 1, Table 4 shows
243
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 07:52:58 UTC from IEEE Xplore.  Restrictions apply. ICSE â€™22, May 21Å›29, 2022, Pittsburgh, PA, USA Patanamon Thongtanunam, Chanathip Pornprasit, and Chakkrit Tantithamthavorn
that ourAutoTransform achieves a perfect prediction for 18 -
415 methods which accounted for 2% (18
1,162) - 12% (415
3,327) of the
changed methods withnew tokens in the testing data. In total, our
AutoTransform achievesaperfectpredictionfor1,060methods.
Similarly,our AutoTransform achieveshigherperfectpredictions
when the beam width is increased to 5 and 10, i.e., 4% (46
1,162) - 25%
(833
3,327) at the beam width of 5; and 5% (63
1,162) - 30% (992
3,327) at the
beam width of 10. On the other hand, Table 4 shows that Tufano et
al.approachcouldnotachieveaperfectpredictionforanyofthe
changedmethodswithnewtokensappearingintheafterversion.
This is because the IDs of the new tokens cannot be mapped back
the actual identifiers/literals as they did not exist in the before ver-
sion (see Limitation 1 in Section 2). These results highlight that
ourAutoTransform cantransformsourcecodeevenwhennew
tokens appear in the after version.
Forthechangedmethods without newtokensintheafter
version, our A utoTransform achieves a perfect prediction
for 11 - 111 methods, while Tufano et al.approach achieves
a perfect prediction for 9 - 86 methods. At the beam width
of 1, Table 4 shows that our AutoTransform achieves a perfect
predictionfor11-111methodswhichaccountedfor5%(11
228)-18%
(111
622) of the changed methods withoutnew tokens in the testing
data.On theotherhand, Tufano etal.approachachieves aperfect
predictionfor9-86methodswhichaccountedfor3%(9
283)-18%
(86
473). For the small methods, our AutoTransform achieves more
perfect predictions in the Android dataset, but fewer in the Google
andOvirtdatasets. Wewillfurtherdiscuss thisresultinSection6.
Nevertheless, it is worth noting that for the medium methods, our
AutoTransform achieves perfect predictions 78% (16âˆ’9
9) - 517%
(111âˆ’18
18) higher than the perfect predictions achieved by Tufano et
al.approach. The results are similar when the beam width is 5 and
10. These results suggest that when a sequence becomes longer,
ourAutoTransform transforms code better than the Tufano et
al.approach,highlightingthatour AutoTransform canaddress
Limitation 2 discussed in Section 2.
RQ2: What are the contributions of
AutoTransformâ€™s components?
Approach. ToaddressourRQ2,weexamineperfectpredictions
when a component in our AutoTransform is varied. Specifically,
we examine the percentage difference of perfect predictions when
subword tokenization is changed to code abstraction (BPE âˆ’ â†’Abs);
andwhentheNMTarchitectureischangedfromTransformerto
RNN (Transformer âˆ’ â†’RNN). We also investigate the case when
the maximum number of merge operations is changed from 2,000
to 5,000 (BPE2K âˆ’ â†’BPE5K), i.e., the vocabulary size increases.
Thus,weevaluatetheperfectpredictionsoffouradditionalcom-
binations: BPE5K+Transformer, Abs+Transformer, BPE2K+RNN,
BPE5K+RNN. We build an NMT model using each combination
foreachofthe12datasets.Intotal,forRQ2,wefurtherbuild192
Transformer models (i.e., 2 Transformer-based combinations Ã—12
datasetsÃ—8hyper-parametersettings);and240RNNmodels(i.e.,
2 RNN-based combinations Ã—12 datasets Ã—10 hyper-parametersettings). Similar to RQ1, we select the model with the best hyper-
parametersettingbasedonthevalidationdata;andmeasureperfect
predictions based on the testing data.
Results. Figure 3 shows the perfect predictions of our Auto-
Transform (BPE2K+Transformer),Tufano etal.approach(Abs+RNN),
and the four additional combinations.
Using subword tokenization by BPE can increase perfect
predictions at least by 284%, compared to the code abstrac-
tionwithreusableIDs. Figure3showsthatatbeamwidthof1,the
perfect predictions of our AutoTransform (BPE2K+Transformer)
is284%(192âˆ’50
50forAndroidSmall)-2,290%(526âˆ’22
22forOvirtMedium)
higherthanAbs+Transformer.ConsideringthecaseswhentheRNN
architecture is used with BPE (i.e., BPE2K+RNN and BPE5K+RNN),
the perfect predictions are also higher than Tufano et al.approach
(Abs+RNN). For example, for the small methods, Figure 3 shows
thatBPE2K+RNNachievesperfectpredictions29%(18âˆ’14
14)-323%
(364âˆ’86
86) higher than Tufano et al.approach. Figure 3 also shows
similar results when the beam width was increased to 5 and 10.
These results indicate that regardless of the NMT architecture, sub-
wordtokenizationbyBPElargelycontributestotheperformance
in transforming code of our AutoTransform.
When using a different number of merge operations (BPE2K âˆ’ â†’
BPE5K),wefindthatperfectpredictionswereslightlydifferent.For
example,forthesmallmethods(atbeamwidthof1),thepercentage
difference of perfect prediction between our AutoTransform and
BPE5K+Transformer is 7% (55âˆ’51
51) - 19% (192âˆ’156
192). Figure 3 also
shows that the results are similar for the RNN-based approaches
(i.e.,BPE2K+RNNandBPE5K+RNN).Theseresultssuggestthatthe
number of merge operations has an impact (but relatively small)
on the performance of our AutoTransform.
Using the Transformer architecture can increase perfect
predictions at least by 17%, compared to the RNN architec-
ture.Figure3showsthatatthebeamwidthof1andforsmallmeth-
ods,ourAutoTransform achievesperfectpredictions17%(425âˆ’364
364
for Ovirt) - 183% (51âˆ’18
18for Google) higher than BPE2K+RNN. It is
alsoworthnotingthatthepercentagedifferenceismuchhigherfor
themediummethods,i.e.,183%(34âˆ’12
12forGoogle)-507%(182âˆ’30
30
forAndroid).Figure3alsoshowsalargedifferenceofperfectpre-
dictions between our AutoTransform and BPE2K+RNN when
the beam width is increased to 5 and 10. The results are also
similarwhencomparingtheperfectpredictionsbetweenBPE5K+
Transformer and BPE5K+RNN, i.e., the Transfomer models tend
to achieve higher perfect predictions than the RNN models. These
resultssuggestthattheTransformerarchitecturealsocontributes
to the performance of our AutoTransform in transforming code,
especially for the methods with a relatively long sequence like
medium methods.
6 DISCUSSION
In this section, we discuss our AutoTransform in several aspects
including its advantage, performance, and practicality.
Advantage: WhydoesourA utoTransform workforthechanged
methods with new tokens? Table 4 shows that in total, our Auto-
Transform can correctly transform the methods that have new
tokens appearing in the after version. We further analyze these
methodstobetterunderstandthecharacteristicsofthenewcode
244
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 07:52:58 UTC from IEEE Xplore.  Restrictions apply. AutoTransform: Automated Code Transformation to Support Modern Code Review Process ICSE â€™22, May 21Å›29, 2022, Pittsburgh, PA, USA192
331
363156
295
353159
240
275167
263
28850
100
12153
83
107182
332
378214
385
42330
45
4862
79
8720
36
4432
67
78
51
103
12655
105
13218
39
4818
42
4811
37
4314
36
4234
74
9619
58
7712
39
4128
61
687
12
159
18
22
425
750
860364
664
753364
629
703339
617
69673
167
18686
173
200526
1012
1191531
1007
1170154
334
428254
532
61422
50
5918
49
62Android (M) Google (M) Ovirt (M)Android (S) Google (S) Ovirt (S)
Beam = 1 Beam = 5 Beam = 10 Beam = 1 Beam = 5 Beam = 10 Beam = 1 Beam = 5 Beam = 10Beam = 1 Beam = 5 Beam = 10 Beam = 1 Beam = 5 Beam = 10 Beam = 1 Beam = 5 Beam = 1002505007501000
050010001500050100150
03060901200100200300400
0100200300400500#Pefect PredictionsAutoTransform (BPE2K+Transformer) BPE5K+Transformer BPE2K+RNN BPE5K+RNN Abs+Transformer Tufano et al (Abs+RNN)
Figure 3: The perfect prediction of our A ut oTransform when a component is varied. The y-axis shows the total number of
perfect predictions of changed methods withandwithout new tokens.
tokensappearingintheafterversion.Wefindthat43%(960
1,689)ofthe
new code tokens are the identifiers/literals that already exist in the
trainingdata(i.e., knowncodetokens),suggestingthatour Auto-
Transform can reuse the code tokens existing across all methods
thatAutoTransform havelearnt.Ontheotherhand,theTufano et
al.approach cannot generate these new code tokens because their
approach is restricted by the code abstraction with reusable IDs to
useonlytheidentifiers/literalsthatexistinthebeforeversion;or
that are the top-300 frequent identifiers/literals.
Furthermore, the other 57% of the new code tokens appear-
ing in the after version are new identifiers/literals that do not
exist in the training data, suggesting that our AutoTransform
can generate these new code tokens based on a combination of
knownsubwordsinthetrainingdata.Weobservethatthesenew
code tokens are related to changing a Java package/library (e.g.,
org.junit.Assert âˆ’ â†’org.hamcrest.MatcherAssert ), changing iden-
tifier(e.g., getlog_type_name âˆ’ â†’getLogTypeName ,dâˆ’ â†’dt.mID),oreven
adding new statements (e.g., instantiating a new object).
Limitation: Whydoesour AutoTransform achievefewerperfect
predictions than Tufano et al. approach for small methods without
new tokens inthe Google and Ovirt datasets? Table 4 showsthat, for
thesmallmethodswithoutnewtokensintheafterversion,our Au-
toTransform achievesfewerperfectpredictionsthantheTufano et
al.approachintheGoogleandOvirtdatasets.Tobetterunderstand
the methods for which our AutoTransform cannot achieve a per-
fect prediction, we manually examine 9 of the 14 methods (Google)
and54ofthe86methods(Ovirt)forwhichour AutoTransform
cannot achievea perfectprediction but theTufano et al.approach
could. We find that there are only 2 (out of 9) and 5 (out of 54)
methods that are incorrectly predicted by our AutoTransform.
On the other hand,for the remaining 7 (out of9) and 49 (out of
54) methods, we find that our AutoTransform almost achieves a
perfect prediction, i.e., the generated sequence is very similar tothe ground-truth with minor errors. Broadly speaking, we observe
thatformostofthesemethods,ourapproachtransformssomecode
tokens that should remain unchanged in the after version. One
possible reason is that these code tokens are a rare token and BPE
splitsthemintomanysubwords,i.e.,oneraretokenbecomesalong
subwordsequence.Then,duetothelargesearchspaceofsubwords,
our approach may unnecessarily generate a new combination of
subwords.Forexample,weobservethat FixturesTool.DATA_CENTER
is split into 9 subwords (i.e., FixturesTool.@@, DA@@, ..., TE@@,
R). Then, our approach inserts a subword A@@, resulting in a new
tokenFixturesTool.A DATA_CENTER .Basedonthisobservation,anap-
proachtoreducethelengthofsubwordsequences(e.g.,fine-tuning
the number of merge operations) may improve the performance.
Hyper-ParameterSensitivity: Howsensitivethehyper-parameter
settingisinour AutoTransform? Deeplearningmodelsareknown
forbeingsensitivetohyper-parametersettings.Whileweselectthe
besthyper-parametersettingbasedonthevalidationdataforour
experiment, we are interested in examining the impact of hyper-
parameter settings on the performance of our AutoTransform.
Hence, we analyze the perfect predictions of our AutoTransform
when each of the eight hyper-parameter settings in Table 3 is used.
We find that a setting of ( ğ‘ğ‘’= 2,ğ‘ğ‘‘= 4,â„= 8,â„ğ‘ = 512,) allows
ourAutoTransform achieves the highest perfect predictions for
7outof12datasets,whileasettingof( ğ‘ğ‘’=1,ğ‘ğ‘‘=2,â„=8,â„ğ‘ =
512) allows our AutoTransform to achieve the highest perfect
predictions for 4 out of 12 datasets. Nevertheless, we observe that
the perfect predictions is decreased by only 1 - 3 percentage points
when using the other hyper-parameter settings instead of the best
setting.
Performance: How long does our AutoTransform take to train
and infer? Model training and inference time can be one of the
important factors when considering the adoption of our approach
in practice. Hence, we measure the training and inference time for
245
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 07:52:58 UTC from IEEE Xplore.  Restrictions apply. ICSE â€™22, May 21Å›29, 2022, Pittsburgh, PA, USA Patanamon Thongtanunam, Chanathip Pornprasit, and Chakkrit Tantithamthavorn
the Transformer models in our AutoTransform. We find that the
training time for each Transformer model in our AutoTransform
usedinRQ1israngingfrom30minutesto2hoursdependingon
the size of the datasets and the number of epochs. The average
inference time of AutoTransform per method is ranging from 15
to60millisecondsforgeneratingonesequencecandidate(i.e.,Beam
width=1).Similarly,whengenerating5and10sequencecandidates
per method (i.e., Beam width = {5,10}), the average inference time
per input sequence is ranging from 42 to 200 milliseconds. Note
thatthetrainingandinferencetimeisbasedonacomputerwith
an Nvidia GeForce RTX 3090 graphic card.
Practicality: To what extent AutoTransform can support the
modern code review process? Our RQ1 has shown that AutoTrans-
formcancorrectlytransform1,413methods,whichissubstantially
betterthanthepriorwork.Thisresulthighlightsagreatpotentialof
AutoTransform toaugment codeauthorsby recommendingthe
commonrevisionsthatoccurredduringthecodereviewsinthepast
to apply to the newly-written code without waiting for reviewersâ€™
feedback. Nevertheless, at this stage of the work, AutoTransform
may be suitable for code changes of software components that
toleratefalsepositivesas AutoTransform willprovidearecom-
mendation for every changed method which is subject to produce
many false positives compared to humans. Indeed, prior work [ 51]
reported that developers may decide to not use a supporting tool if
itsfalsepositiverateisabove25%.Furthermore,similartotheprior
work[60],theapplicabilityof AutoTransform isstilllimitedto
smallandmediummethodsizes.Thus,tobroadenthepracticalityof
AutoTransform , future work should aim to develop an approach
thatismoreselectivetoachievehigheraccuracy(e.g.,below25%of
false positive rate) for any method sizes (including large methods).
7 RELATED WORK
AutomatedCodeReview. Codereviewiseffective,butstillhuman-
intensiveandtime-consuming[ 11,37,50].Thus,recentworklever-
aged machine learning techniques to support various activities
throughout the code review process, for example, reviewer recom-
mendation[ 13,43,46,49,56,59],reviewtaskprioritizationbased
oncodechangecharacteristics[ 23,38,58]anddefect-proneness[ 30,
39,44,45,65]. Several studies also proposed approaches to support
reviewerswhenreadingandexaminingcode[ 14,27,55,63,64].Al-
thoughtheseapproachescanreducethemanualeffortofreviewers,
codeauthorsstillneedtomanuallymodifythesourcecodeuntil
itisapprovedbyreviewers.Yet, fewstudiesfocusondeveloping
anapproachtoautomaticallytransformsourcecodetohelpcode
authors reduce their effort in the code review context.
Tothebestofourknowledge,onlytworecentapproaches[ 60,61]
are proposed to automatically transform the source code to the
version that is approved by reviewers (i.e., the after version). How-
ever,both recent approachesuse code abstraction(ABS+ RNN[ 60],
ABS+Transformer[ 61])toreducethevocabularysize,whereour
results show that code abstraction hinders the NMT approaches in
correctly transforming code if the after version has a new token
(e.g., renamed variables). Thus, these recent approaches still have a
limitedusagetoautomaticallytransformcodeinthecodereview
process.Differentfrompriorwork,weleverageBPEtoaddressthis
limitation of prior work. Importantly, our RQ2 shows that usingBPE (BPE+Transformer, BPE+RNN) achieves perfect predictions at
least by 284% higher than using the code abstraction with reusable
IDs (ABS+Transformer, ABS+RNN), highlighting the important
contribution of this paper to the automated code transformation
for code review.
NeuralMachineTranslation(NMT)inSoftwareEngineer-
ing.NMT approaches have been developed to support various
software engineering tasks, which can be categorized into four
types of transformation: (1) Text â†’Text (e.g., language transla-
tion of code documentation [ 35], query reformulation [ 19]); (2)
Textâ†’Code(e.g.,codesearch[ 24,42]);(3)Code â†’Text(e.g.,code
summarization[ 25],commit messagegeneration [ 29,34]);and (4)
Codeâ†’Code (e.g., automated program repair [ 20,28,33], program-
minglanguagetranslation[ 48],codecompletion[ 54]).Although
automated program repair (APR) approaches and our AutoTrans-
formshareasimilarconceptofusingNMTforaCode â†’Codetask,
APRapproaches[ 20,28,33]onlyaimtoautomaticallytransform
buggy code to clean code for bug-fixing purposes, which may not
be related to other types of code changes in code review. Different
fromAPR,our AutoTransform aimstoautomaticallytransform
source code that is changed during the code review process (e.g.,
refactoring) to improve readability, maintainability, and design
quality [16, 41, 50, 57].
8 THREATS TO VALIDITY
ConstructValidity. Thesourcecodegranularityusedinthiswork
is at the method level. The results may be varied if the changed
source code is extracted at a different granularity (e.g., changed
lines).However,priorworkpointedoutthatanNMTmodelrequires
codecontextandusingonlychangedlinesmayleadtheNMTmodel
tosufferfromtheOut-Of-Vocabularyproblem,eventhoughBPE
is used [22]. Hence, we believe that training an NMT model at a
method level would provide a reasonable range of code context
than changed lines.
Wedefinethemethodsizebasedonthenumberoftokens(i.e.,
1-50tokensforsmalland51-100tokensformedium).Thissize
definition may not reflect the actual method sizes in practices, e.g.,
amethodwith100tokensmayactuallyhavefewlinesofcode.The
performance of AutoTransform may differ if other definitions of
method size are used. Nevertheless, for the sake of fair comparison
inourexperiment,weopttousethesamedefinitionofmethodsize
as used in the prior work [ 60]. Moreover, from the aspect of the
NMTalgorithm,transforminglongsequencesarenottrivialasit
requires higher memory and computation power [66].
Internal Validity. We experiment with only two settings of
merge operations (i.e., BPE2K and BPE5K); and 8 hyper-parameter
settingswhicharebasedonacombinationsoffourhyper-parameters.
Theresultsmaybevariedifthenumberofmergeoperationsand
the hyper-parameter settings of both approaches are optimized.
However,findinganoptimalsettingcanbeverycomputationally
expensive given a large search space of the number of merge op-
erationsandallavailablehyper-parameters.Inaddition,thegoal
of our work is not to find the best setting, but to fairly compare
the performanceof ourapproachwith theprior approachbased on
similar settings as used in the prior work [ 60]. Nevertheless, our
analyses in Sections 5 and 6 have shown that the number of merge
246
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 07:52:58 UTC from IEEE Xplore.  Restrictions apply. AutoTransform: Automated Code Transformation to Support Modern Code Review Process ICSE â€™22, May 21Å›29, 2022, Pittsburgh, PA, USA
operationsandhyper-parametersettingshaveasmallimpacton
the performance of our approach.
ExternalValidity. Weevaluateour AutoTransform basedon
thechangedmethodsthatwereextractedfromthreeGerritcode
repositories.Inaddition,weonlyexperimentedwithJavaprograms.
Our results may not be generalized to other code review reposi-
tories or other programming languages. However, our approach
is based on the techniques (i.e., BPE and Transformer) that are
language-independent. In addition, we provided a replication pack-
age to facilitate future work to replicate our approach on different
repositories and programming languages.
9 CONCLUSION
Priorwork[ 60]proposedanNMTapproachtoautomaticallytrans-
formagivenmethodfromthebeforeversion(i.e.,theversionbefore
theimplementationofcodechanges)totheafterversion(i.e.,the
versionthatisreviewedandmerged).Yet,itsperformanceisstill
suboptiomal when the after version has new identifiers or literals,
orwhenthesequencebecomeslonger.Hence,inthispaper,wepro-
poseAutoTransform which leverages BPE to handle new tokens
and a Transformer to handle long sequences.
Throughanempiricalevaluationbasedon14,750changedmeth-
odsthatareextractedfromthreeGerritcodereviewrepositories,
wefindthat(1)our AutoTransform cancorrectlytransform1,060
methods of which the after version has new tokens while the prior
work can not correctly transform any of these methods; and (2) for
thechangedmethodsofwhichtheafterversiondonothavenew
tokens, our AutoTransform can correctly transform 353 meth-
ods, which is 67% higher than the prior work. Furthermore, our
ablation study also shows that BPE and Transformer substantially
contribute to the performance improvement of our AutoTrans-
form,whencomparedtothecomponentsusedinthepriorwork.
These results highlight that our AutoTransform effectively ad-
dressthelimitationsofthepriorwork,allowingtheNMTapproach
tobeappliedtoawiderrangeofchangedmethods(i.e.,methods
with new tokens and methods with long sequences). The proposed
approachandtheresultsofthispapercontributetowardautomated
codetransformationforcodereviews,whichcouldhelpdevelopers
to reduce their effort on modifying source code during the code
review process.
ACKOWLEDGEMENT
Patanamon Thongtanunam was supported by the Australian Re-
search Councilâ€™s Discovery Early Career Researcher Award (DE-
CRA) funding scheme (DE210101091). Chakkrit Tantithamthavorn
was supported by the Australian Research Councilâ€™s Discovery
EarlyCareerResearcherAward(DECRA)fundingscheme(DE200100941).
REFERENCES
[1][n.d.]. A library for subwod tokenization using Byte-Pair Encoding. https:
//github.com/rsennrich/subword-nmt.
[2][n.d.]. AlistofTop-300frequentidentifier/literalsforeachofthestudieddatasets.
https://sites.google.com/view/learning-codechanges/data#h.p_r-R_Z4sKJC2L.
[3][n.d.]. Androidâ€™s Gerrit Code Review Repositories. https://android-review.
googlesource.com/.
[4][n.d.]. AutoTransformâ€™sReplicationPackage. https://github.com/awsm-research/
AutoTransform-Replication.[5][n.d.]. Datasets of the paper titled Å‚On Learning Meaningful Code Changes
Via Neural Machine TranslationÅ¾. https://sites.google.com/view/learning-
codechanges/data#h.p__6KdV38lN05N.
[6][n.d.]. Googleâ€™s Gerrit Code Review Repositories. https://gerrit-review.
googlesource.com/.
[7] [n.d.]. Ovirtâ€™s Gerrit Code Review Repositories. https://gerrit.ovirt.org/.
[8][n.d.]. Seq2Seq: A library for RNN-based NMT models. https://google.github.io/
seq2seq/.
[9][n.d.]. Src2Abs: A library for abstracting code with reusable IDs. https://github.
com/micheletufano/src2abs.
[10][n.d.]. Tensor2Tensor: A library for Transfomer-based NMT models. https:
//github.com/tensorflow/tensor2tensor.
[11]AlbertoBacchelliandChristianBird.2013. Expectations,Outcomes,andChal-
lenges of Modern Code Review. In Proceedings of ICSE. 712Å›721.
[12]DzmitryBahdanau,KyungHyunCho,andYoshuaBengio.2015. NeuralMachine
TranslationbyJointlyLearningtoAlignandTranslate.In ProceedingsofICLR.
1Å›15.
[13]VipinBalachandran.2013. ReducingHumanEffortandImprovingQualityinPeer
Code Reviews using Automatic Static Analysis and Reviewer Recommendation.
InProceedings of ICSE. 931Å›940.
[14]Tobias Baum, Kurt Schneider, and Alberto Bacchelli. 2019. Associating Working
MemoryCapacityandCodeChangeOrderingwithCodeReviewPerformance.
EMSE24, 4 (2019), 1762Å›1798.
[15]GabrieleBavotaandBarbaraRusso.2015. FourEyesAreBetterThanTwo:On
TheImpactofCodeReviewsonSoftwareQuality.In ProceedingsofICSME.81Å›90.
[16]Moritz Beller, Alberto Bacchelli, Andy Zaidman, and Elmar Juergens. 2014. Mod-
ern Code Reviews in Open-Source Projects: Which Problems do They Fix?. In
Proceedings of MSR. 202Å›211.
[17]Amiangshu Bosu, Michaela Greiler, and Christian Bird. 2015. Characteristics of
UsefulCode Reviews:An EmpiricalStudy atMicrosoft.In ProceedingsofMSR .
146Å›156.
[18]DennyBritz,AnnaGoldie,MinhThangLuong,andQuocV.Le.2017. MassiveEx-
ploration of Neural Machine Translation Architectures. In Proceedings of EMNLP.
1442Å›1451.
[19]KaiboCao,ChunyangChen,SebastianBaltes,ChristophTreude,andXiangChen.
2021. Automated Query Reformulation for Efficient Search based on Query Logs
From Stack Overflow. In Proceedings of ICSE. 1273Å›1285.
[20]Zimin Chen, Steve James Kommrusch, Michele Tufano, Louis-NoÃ«l Pouchet,
Denys Poshyvanyk, and Martin Monperrus. 2019. Sequencer: Sequence-to-
Sequence Learning for End-to-End Program Repair. TSE(2019).
[21]Czerwonka,JacekandGreiler,MichaelaandTilford,Jack.2015. CodeReviews
Do Not Find Bugs How the Current Code Review Best Practice Slows Us Down.
InProceedings of ICSE. 27Å›28.
[22]YangruiboDing,BaishakhiRay,PremkumarDevanbu,andVincentJ.Hellendoorn.
2020. PatchingasTranslation:TheDataandtheMetaphor.In ProceedingsofASE.
275Å›286.
[23]YuanruiFan,XinXia,DavidLo,andShanpingLi.2018.EarlyPredictionofMerged
Code Changes to Prioritize Reviewing Tasks. EMSE23, 6 (2018), 3346Å›3393.
[24]XiaodongGu,HongyuZhang,andSunghunKim.2018. DeepCodeSearch.In
Proceedings of ICSE. 933Å›944.
[25]Sakib Haque, Alexander LeClair, Lingfei Wu, and Collin McMillan. 2020. Im-
provedAutomaticSummarizationofSubroutinesviaAttentiontoFileContext.
InProceedings of MSR. 300Å›310.
[26]Vincent J Hellendoorn and Premkumar Devanbu. 2017. Are Deep Neural Net-
workstheBestChoiceforModelingSourceCode?.In ProceedingsofFSE.763Å›773.
[27]Yang Hong, Chakkrit Tantithamthavorn, and Patanamon Thongtanunam. 2022.
WhereShouldILookat?RecommendingLinesthatReviewersShouldPayAt-
tention To. In Proceeding of SANER.
[28]Nan Jiang, Thibaud Lutellier, and Lin Tan. 2021. CURE: Code-Aware Neural
MachineTranslationforAutomaticProgramRepair.In ProceedingsofICSE.1161Å›
1173.
[29]Siyuan Jiang, Ameer Armaly, and Collin McMillan. 2017. Automatically Generat-
ing Commit Messages from Diffs using Neural Machine Translation. In Proceed-
ings of ASE). 135Å›146.
[30]Yasutaka Kamei, Emad Shihab, Bram Adams, Ahmed E. Hassan, Audris Mockus,
Anand Sinha, and Naoyasu Ubayashi. 2013. A Large-Scale Empirical Study of
Just-In-Time Quality Assurance. TSE39, 6 (2013), 757Å›773.
[31]Rafael Michael Karampatsis, Hlib Babii, Romain Robbes, Charles Sutton, and
AndreaJanes.2020. BigCode!=BigVocabulary:Open-VocabularyModelsfor
Source Code. In Proceedings of ICSE). 1073Å›1085.
[32]OleksiiKononenko,OlgaBaysal,andMichaelW.Godfrey.2016. CodeReview
Quality: How Developers See It. In Proceedings of ICSE. 1028Å›1038.
[33]Yi Li, Shaohua Wang, and Tien N Nguyen. 2020. Dlfix: Context-based Code
Transformation Learning for Automated Program Repair. In Proceedings of ICSE.
602Å›614.
[34]Zhongxin Liu, Xin Xia, Ahmed E Hassan, David Lo, Zhenchang Xing, and Xinyu
Wang.2018. Neural-Machine-Translation-basedCommitMessageGeneration:
How Far are We?. In Proceedings of ASE. 373Å›384.
247
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 07:52:58 UTC from IEEE Xplore.  Restrictions apply. ICSE â€™22, May 21Å›29, 2022, Pittsburgh, PA, USA Patanamon Thongtanunam, Chanathip Pornprasit, and Chakkrit Tantithamthavorn
[35]Shuai Lu, Daya Guo, Shuo Ren, Junjie Huang, Alexey Svyatkovskiy, Ambro-
sio Blanco, Colin Clement, Dawn Drain, Daxin Jiang, Duyu Tang, et al .2021.
CodeXGLUE: A Machine Learning Benchmark Dataset for Code Understanding
and Generation. arXiv preprint arXiv:2102.04664 (2021).
[36]MinhThangLuong,HieuPham,andChristopherD.Manning.2015. Effective
ApproachestoAttention-basedNeuralMachineTranslation.In Proceedingsof
EMNLP. 1412Å›1421.
[37]Laura MacLeod, Michaela Greiler, Margaret-Anne Storey, Christian Bird, and
JacekCzerwonka.2017. CodeReviewingintheTrenches:ChallengesandBest
Practices. IEEE Software 35, 4 (2017), 34Å›42.
[38]ChandraMaddila, ChetanBansal, andNachiappan Nagappan.2019. Predicting
Pull Request Completion Time: A Case Study on Large Scale Cloud Services. In
Proceedings of ESEC/FSE. 874Å›882.
[39]Shane McIntosh and Yasutaka Kamei. 2017. Are Fix-Inducing Changes a Moving
Target?ALongitudinalCaseStudyofJust-In-TimeDefectPrediction. TSE(2017),
412Å›428.
[40]Shane McIntosh, Yasutaka Kamei, Bram Adams, and Ahmed E Hassan. 2016. An
Empirical Study of the Impact of Modern Code Review Practices on Software
Quality.EMSE21, 5 (2016), 2146Å›2189.
[41]RodrigoMorales,ShaneMcIntosh,andFoutseKhomh.2015. DoCodeReview
PracticesImpactDesignQuality?:ACaseStudyoftheQt,VTK,andITKProjects.
InProceedings of SANER. 171Å›180.
[42]Thanh Nguyen, Peter C Rigby, Anh Tuan Nguyen, Mark Karanfil, and Tien N
Nguyen. 2016. T2API: Synthesizing API Code Usage Templates from English
Texts with Statistical Translation. In Proceedings of FSE. 1013Å›1017.
[43]Ali Ouni, Raula Gaikovina Kula, and Katsuro Inoue. 2016. Search-based Peer
ReviewersRecommendationinModernCodeReview.In ProceedingsofICSME.
367Å›377.
[44]Chanathip Pornprasit and Chakkrit Tantithamthavorn. 2021. JITLine: A Simpler,
Better, Faster, Finer-grained Just-In-Time Defect Prediction. In Proceedings of
MSR. To Appear.
[45]ChanathipPornprasitandChakkritTantithamthavorn.2022. DeepLineDP:To-
wardsaDeep LearningApproachforLine-Level DefectPrediction. IEEETrans-
actions on Software Engineering (2022).
[46]MohammadMasudurRahman,ChanchalKRoy,andJasonACollins.2016. COR-
RECT:CodeReviewerRecommendationinGitHubbased onCross-Projectand
Technology Experience. In Proceedings of ICSE (Companion). 222Å›231.
[47]Peter C Rigby and Christian Bird. 2013. Convergent Contemporary Software
Peer Review Practices. In Proceedings of FSE. 202Å›212.
[48]BaptisteRoziere,Marie-AnneLachaux,LowikChanussot,andGuillaumeLample.
2020. Unsupervised Translation of Programming Languages.. In NeurIPS.
[49]ShadeRuangwan,PatanamonThongtanunam,AkinoriIhara,andKenichiMat-
sumoto. 2018. The Impact of Human Factors on the Participation Decision of
Reviewers in Modern Code Review. EMSE(2018), In press.
[50]Caitlin Sadowski, Emma SÃ¶derberg, Luke Church, Michal Sipko, and Alberto
Bacchelli. 2018. Modern Code Review: A Case Study at Google. In Proceedings of
ICSE (Companion). 181Å›190.
[51]Caitlin Sadowski, Jeffrey Van Gogh, Ciera Jaspan, Emma SÃ¶derberg, and Collin
Winter. 2015. Tricorder: Building a program analysis ecosystem. In Proceeding of
the International Conference on Software Engineering (ICSE). 598Å›608.
[52]Rico Sennrich, Barry Haddow, and Alexandra Birch. 2016. Neural Machine
Translation of Rare Words with Subword Units. In Proceedings of ACL, Vol. 3.
1715Å›1725.
[53]Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. 2014. Sequence to Sequence
Learning with Neural Networks. In NIPS. 3104Å›3112.
[54]AlexeySvyatkovskiy,ShaoKunDeng,ShengyuFu,andNeelSundaresan.2020.
IntelliCode Compose: Code Generation Using Transformer. In Proceedings of
ESEC/FSE.
[55]Yida Tao and Sunghun Kim. 2015. Partitioning Composite Code Changes to
Facilitate Code Review. In Proceedings of MSR. 180Å›190.
[56]Patanamon Thongtanunam, Raula Gaikovina Kula, Ana Erika Camargo Cruz,
NorihiroYoshida,andHajimuIida.2014. Improvingcoderevieweffectiveness
through reviewer recommendations. In Proceedings of CHASE. 119Å›122.
[57]PatanamonThongtanunam,ShaneMcIntosh,AhmedE.Hassan,andHajimuIida.
2015. InvestigatingCodeReviewPracticesinDefectiveFiles:AnEmpiricalStudy
of the Qt System. In MSR. 168Å›179.
[58]Patanamon Thongtanunam, Shane McIntosh, Ahmed E Hassan, and Hajimu Iida.
2017. ReviewParticipationinModernCodeReview. EMSE22,2(2017),768Å›817.
[59]Patanamon Thongtanunam, Chakkrit Tantithamthavorn, Raula Gaikovina Kula,
NorihiroYoshida, HajimuIida, andKen-ichiMatsumoto. 2015. WhoShouldRe-
viewMyCode?AFileLocation-basedCode-reviewerRecommendationApproach
for Modern Code Review. In Proceedings of SANER. 141Å›150.
[60]MicheleTufano,JevgenijaPantiuchina,CodyWatson,GabrieleBavota,andDenys
Poshyvanyk. 2019. On Learning Meaningful Code Changes Via Neural Machine
Translation. In Proceedings of ICSE. 25Å›36.
[61]Rosalia Tufano, Luca Pascarella, Michele Tufano, Denys Poshyvanykz, and
Gabriele Bavota. 2021. Towards Automating Code Review Activities. In Pro-
ceedings of ICSE. To appear.[62]Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
Aidan N. Gomez, Åukasz Kaiser, and Illia Polosukhin. 2017. Attention is All
You Need. In NIPS. 5999Å›6009.
[63]DongWang,TaoXiao,PatanamonThongtanunam,RaulaGaikovinaKula,and
Kenichi Matsumoto. 2021. Understanding Shared Links and Their Intentions to
Meet Information Needs in Modern Code Review. In EMSE. to appear.
[64]Min Wang, Zeqi Lin, Yanzhen Zou, and Bing Xie. 2019. CORA: Decomposing
andDescribingTangledCodeChangesforReviewer.In ProceedingsofASE .1050Å›
1061.
[65]Supatsara Wattanakriengkrai, Patanamon Thongtanunam, Chakkrit Tan-
tithamthavorn,HideakiHata,andKenichiMatsumoto.2020. Predictingdefective
linesusingamodel-agnostictechnique. IEEETransactionsonSoftwareEngineering
(2020).
[66]Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua Ainslie, Chris
Alberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang,
andAmrAhmed.2020. BigBird:TransformersforLongerSequences.In Advances
in Neural Information Processing Systems, H. Larochelle, M. Ranzato, R. Hadsell,
M. F. Balcan, and H. Lin (Eds.), Vol. 33. Curran Associates, Inc., 17283Å›17297.
248
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 07:52:58 UTC from IEEE Xplore.  Restrictions apply. 