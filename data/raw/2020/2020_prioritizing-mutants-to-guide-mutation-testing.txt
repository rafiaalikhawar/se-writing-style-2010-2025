Prioritizing Mutants to Guide Mutation Testing
Samuel J. Kaufman
kaufmans@cs.washington.edu
University of Washington
Seattle, Washington, USARyan Featherman
feathr@cs.washington.edu
University of Washington
Seattle, Washington, USAJustin Alvin
jalvin@umass.edu
University of Massachusetts
Amherst, Massachusetts, USA
Bob Kurtz
rkurtz22@gmu.edu
George Mason University
Fairfax, Virginia, USAPaul Ammann
pammann@gmu.edu
George Mason University
Fairfax, Virginia, USARen√© Just
rjust@cs.washington.edu
University of Washington
Seattle, Washington, USA
ABSTRACT
Mutationtestingoffersconcretetestgoals(mutants)andarigorous
test efficacy criterion, but it is expensive due to vast numbers of
mutants, many of which are neither useful nor actionable. Prior
work has focused on selecting representative and sufficient mutant
subsets,measuringwhetheratestsetthatismutation-adequateforthesubsetisequallyadequatefortheentireset.However,noknown
industrial application of mutation testing uses or even computes
mutation adequacy, instead focusing on iteratively presenting very
few mutants as concrete test goals for developers to write tests.
Thispaper(1)articulatesimportantdifferencesbetweenmuta-
tion analysis, where measuring mutation adequacy is of interest,and mutation testing, where mutants are of interest insofar asthey serve as concrete test goals to elict effective tests; (2) intro-
ducesanewmeasureofmutantusefulness,calledtestcompleteness
advancement probability (TCAP); (3) introduces an approach to
prioritizingmutantsbyincrementallyselectingmutantsbasedon
theirpredictedTCAP;and(4)presentssimulationsshowingthat
TCAP-basedprioritization ofmutantsadvances testcompleteness
more rapidly than prioritization with the previous state-of-the-art.
CCS CONCEPTS
‚Ä¢Software and its engineering ‚ÜíSoftware testing and de-
bugging; Empirical software validation.
KEYWORDS
mutationtesting,mutantselection,mutantutility,testcompleteness
advancement probability, TCAP, machine learning
ACM Reference Format:
Samuel J. Kaufman, Ryan Featherman, Justin Alvin, Bob Kurtz, Paul Am-
mann, and Ren√© Just. 2022. Prioritizing Mutants to Guide Mutation Test-ing. In44th International Conference on Software Engineering (ICSE ‚Äô22),
May 21‚Äì29, 2022, Pittsburgh, PA, USA. ACM, New York, NY, USA, 12 pages.
https://doi.org/10.1145/3510003.3510187
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation
onthe firstpage.Copyrights forcomponentsof thisworkowned byothersthan the
author(s)mustbehonored.Abstractingwithcreditispermitted.Tocopyotherwise,or
republish,topostonserversortoredistributetolists,requirespriorspecificpermission
and/or a fee. Request permissions from permissions@acm.org.
ICSE ‚Äô22, May 21‚Äì29, 2022, Pittsburgh, PA, USA
¬© 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 978-1-4503-9221-1/22/05...$15.00
https://doi.org/10.1145/3510003.35101871 INTRODUCTION
Mutationtestinggeneratesasetofprogramvariantscalled mutants
and challenges a developer to create tests that detectthem‚Äîthat
is,distinguishesthevariantsfromtheoriginalprogram.Thereis
strongempiricalevidencethatmutantsarecoupledtorealfaults
and that mutant detection is positively correlated with real faultdetection [
4,10,23,46]. This correlation is stronger than is the
case for code coverage criteria commonly used in practice (e.g.,
statement and branch coverage [19, 55]).
Mutationtestingisexpensiveduetothelargenumberofmutants
thatcanbegeneratedforagivensoftwaresystem.Recentresearch,
focusing on the practicality of presenting mutants as test goals,
identifiedthetotalnumberofmutantsandthefactthatmostofthem
are not useful test goals as key challenges to adoption [ 7,46,48].
Further, equivalent and redundant mutants make it difficult fora developer to assess how close they are to achieving mutation
adequacy [ 32].Equivalent mutants are functionally identical to the
original program, and therefore cannot be detected by any test.
Redundant mutantsarefunctionallyidenticaltoothermutants,and
arethereforealwaysdetectedbyteststhatdetecttheothermutants.
To make mutation testing feasible for practitioners, it is nec-
essaryto select justa fewof thenumerous mutantsproducedby
currentmutationsystems.Giventhatmanymutantsarenotuseful,
such a selection strategy must be biased toward useful mutants.
Furthermore, neither achieving nor measuring mutation adequacy
is among the reported desiderata of industrial mutation testing
systems,whichareinsteadconcernedwithiterativelypresenting
one or a small number of mutants as test goals to incrementally
improve test quality over time [7, 45, 46, 48].
Thispaperproposesanewmeasureformutantusefulness,which
values mutants according to their likelihood of eliciting a test that
advances test completeness, and evaluates the measure‚Äôs ability to
effectively prioritize mutants. Specifically, this paper contributes:
‚Ä¢Anarticulationofthedifferencesbetweentwomutationuse
cases‚Äîmutationanalysis vs.mutationtesting ‚Äîandimplica-
tions for empirically evaluating them (Section 3).
‚Ä¢Anewmeasure, testcompletenessadvancementprobability
(TCAP), that quantifies mutant usefulness (Section 4).
‚Ä¢A mutation testing approach that prioritizes mutants for
selection based on their predicted TCAP (Section 5).
‚Ä¢An evaluation showing that, for a variety of different initial
testsetcoverages,prioritizingmutantsaccordingtoTCAP
improvestest completenessmorerapidly thantheprevious
state-of-the-art (random selection) (Section 6).
17432022 IEEE/ACM 44th International Conference on Software Engineering (ICSE)ICSE ‚Äô22, May 21‚Äì29, 2022, Pittsburgh, PA, USA Samuel J. Kaufman, Ryan Featherman, Justin Alvin, Bob Kurtz, Paul Ammann, and Ren√© Just
2 BACKGROUND
Amutation operator is a program transformation rule that gen-
erates amutant(i.e., program variant) of a given program based
ontheoccurrenceofaparticularsyntacticelement.Oneexample
of a mutation operator is the replacement of an instance of the
arithmetic operator ‚Äú *‚Äù with ‚Äú /‚Äù. Specifically, if a program contains
an expression ‚Äú a*b‚Äù, for arbitrary expressions ‚Äú a‚Äù and ‚Äú b‚Äù, this
mutation operator creates a mutant where ‚Äú a/b‚Äù replaces this
expression.The mutation isthesyntacticchangethatamutation
operator introduces. A mutation operator is applied everywhere it
ispossibletodoso.Intheexampleabove,ifthearithmeticoperator
‚Äú*‚Äù occurs multiple times, the mutation operator will create a sepa-
ratemutantforeachoccurrence.Thispaperconsidersfirst-order
mutants, where each mutant contains exactly one mutation, as op-
posed to higher-order mutants, where each mutant is the product
of multiple applications of mutation operators.
Amutation operator group is a set of related mutation operators.
For example, the AOR (arithmetic operator replacement) mutation
operator group contains all mutation operators that replace an
arithmetic operator, including the example above.
A mutant may behave identically to the original program on all
inputs. Such a mutant is called an equivalent mutant and cannot
be detected by any test. As an example, consider the ifstatement
inFigure1a,whichdeterminesthesmallervalueoftwointegers:
numbers[i] < min .Replacingtherelationaloperator <with <=results
intheequivalentmutant numbers[i] <= min ‚Äîifnumbers[i] and min
areequal,assigningeithervalueto miniscorrect,andhenceboth
implementations are equivalent.
Atrivial mutant is one that is detected due to an exception
by every test case that executes the mutated code location. As
anexample,considerthe forloopinFigure1a,whichincludesa
boundarycheckforanarrayindex( for int i=1; i<numbers.length;
++i). If the index variable iis used to access the array numbersthen
a mutation i<=numbers.length always results in an exception, as
the last value for iis guaranteed to index numbersout of bounds.
Hence, thismutant istrivial, asany testthat reachesthe loopwill
terminate with an exception.
2.1 Dominator Mutants
Given a set of mutants ùëÄ, a test set ùëáismutation-adequate with
respectto ùëÄiffforeverynon-equivalentmutant ùëöinùëÄ,thereis
sometest ùë°inùëásuchthat ùë°detectsùëö.However,mutationoperators
generate far more mutants than are necessary: the cardinality of
the mutant set is much larger than the cardinality of the mutation-
adequate test set. This redundancy among generated mutants was
formallycapturedinthenotionof minimalmutation [3].Givenany
set of mutants ùëÄ,adominator set of mutants ùê∑is a minimal subset
ofùëÄsuch that any test set that is mutation-adequate for ùê∑is also
mutation-adequate for ùëÄ.
Computing a dominator set is an undecidable problem, but it
is possible to approximate it with respect to a test set [ 31]‚Äîthe
morecomprehensivethetestset,thebettertheapproximation.This
approximation is not useful to a developer interested in writing
tests (theydo notyet have atest setwith which toapproximate a
dominator set). However, from a research and analysis perspective,
a dominator set provides a precise measure for redundancy in a set1public int getMin(int[] numbers) {
2int min = numbers[0];
3for (int i=1; i < numbers.length ;+ + i ){
4 if(numbers[i] < min ){
5 min = numbers[i];
6 }
7}
8 return min;
9}
(a) Mutated relational operator in two different program contexts.
Tests Mutant properties
ùë°1ùë°2ùë°3ùë°4MutOp Ctx TCAP Equi. Triv. Dom.
ùëö1 <‚Ü¶‚àí‚Üí!= for 0.0yes‚Äî‚Äî
ùëö2 <‚Ü¶‚àí‚Üí== for 0.5 ‚Äî ‚Äî ‚Äî
ùëö3 <‚Ü¶‚àí‚Üí<= for 0.5 ‚Äî yes‚Äî
ùëö4 <‚Ü¶‚àí‚Üí> for 1.0 ‚Äî ‚Äî yes
ùëö5 <‚Ü¶‚àí‚Üí>= for 0.5 ‚Äî ‚Äî ‚Äî
ùëö6 <‚Ü¶‚àí‚Üítrue for 0.5 ‚Äî yes‚Äî
ùëö7 <‚Ü¶‚àí‚Üífalse for 1.0 ‚Äî ‚Äî yes
ùëö8 <‚Ü¶‚àí‚Üí!= if 1.0 ‚Äî ‚Äî yes
ùëö9 <‚Ü¶‚àí‚Üí== if 1.0 ‚Äî ‚Äî yes
ùëö10 <‚Ü¶‚àí‚Üí<= if 0.0yes‚Äî‚Äî
ùëö11 <‚Ü¶‚àí‚Üí>i f 1.0 ‚Äî ‚Äî ‚Äî
ùëö12 <‚Ü¶‚àí‚Üí>= if 1.0 ‚Äî ‚Äî ‚Äî
ùëö13 <‚Ü¶‚àí‚Üítrue if 1.0 ‚Äî ‚Äî yes
ùëö14 <‚Ü¶‚àí‚Üífalse if 1.0 ‚Äî ‚Äî yes
Symbols indicate test results: indicates that ùë°ùëñpasses on ùëöùëó;indicates
that ùë°ùëñfails on ùëöùëówith an assertion failure; indicates that ùë°ùëñfails on ùëöùëó
with an exception (i.e., the mutant crashes during execution).
(b) Test results and mutant properties.
ùëö4,ùëö7,
ùëö9,ùëö14ùëö8,ùëö13
ùëö2,ùëö5 ùëö11,ùëö12
ùëö3,ùëö6
ùëö1,ùëö10
(c) Dynamic mutant subsumption graph for the 14 mutants. Intu-
itively, mutants high in the graph are dominating other mutants,
whereas mutants low in the graph are subsumed.
Figure 1: Motivating example for program context and testcompleteness advancement probability (TCAP).
1744Prioritizing Mutants to Guide Mutation Testing ICSE ‚Äô22, May 21‚Äì29, 2022, Pittsburgh, PA, USA
of mutants, and hence the dynamic approximation approach is an
important research tool for analyzing mutation testing techniques.
Givenafinitesetofmutants ùëÄandafinitesetoftests ùëá,mutant
ùëöùëñdynamically subsumes mutantùëöùëóif some test in ùëádetectsùëöùëñ
andeverytestin ùëáthatdetects ùëöùëñalsodetects ùëöùëó.Ifùëöùëñdynamically
subsumes ùëöùëóbuttheconverseisnottrue,thesubsumptionis strict.
If two mutants ùëöùëñandùëöùëóinùëÄare detected by exactly the same
tests inùëá,ùëöùëñandùëöùëó, the subsumption is not strict.
TheDynamicMutantSubsumption Graph (DMSG)capturesthe
subsumption relationship among mutants [ 31]. Each node in a
DMSG represents a maximal set of redundant mutants and each
edge represents the dynamic subsumption relationship between
twosetsofmutants.Morespecifically,if ùëöùëñstrictlysubsumes ùëöùëó,
then there is an edge from the node containing ùëöùëñto the node
containing ùëöùëó. If a test detects any arbitrary mutant in the DMSG,
it is guaranteed to detect all the subsumed mutants [ 3], i.e., all
mutants in the same node or below it in the graph.
Figure 1b shows an example detection matrix that indicates
whichtestdetectswhichmutants.Inthisexample,theset ùëÄconsists
of14mutantsandtheset ùëáconsistsof4tests.Everytestthatdetects
ùëö12alsodetects ùëö3,ùëö6,andùëö11.Hence,ùëö12dynamicallysubsumes
these mutants. In the case of the first two mutants, the dynamic
subsumptionisstrict.However, ùëö11andùëö12aredetectedbyexactly
the same tests, so the subsumption is not strict.
The DMSG shown in Figure 1c visualizes the subsumption re-
lationships. Mutants ùëö1andùëö10are not detected by any of the
testsinùëá‚Äîshownintheunconnectednodewithadashedborder.
These mutants are equivalent with respect to ùëábut they may be
detectablebyatestthatisnotanelementof ùëá.TheDMSGisbased
on a finite test set and can only make claims about equivalence
with respect to ùëá.
Dominator mutants appear in the graph inside dominator nodes,
stylizedwithdoubleborders.Figure1chastwodominatornodes;
anycombinationofonemutantfromeachdominatornode,suchas
{ùëö4,ùëö8}or{ùëö9,ùëö13},formsadominatorset.Figure1chas4 ‚àó2=8
distinct dominator sets. Ultimately, only two of the 14 mutantsmatter‚Äîif a test set detects the mutants in a dominator set, it is
guaranteed to detect all non-equivalent mutants in the DMSG.
3 MUTATION ANALYSIS VS. MUTATION
TESTING
Thispaperdistinguishesbetweentwomutationusecases: mutation
analysis, which we define as a (research-based) use of mutation
techniquestoassessandcomparethemutationadequacyofexistingtestsets,and mutationtesting,whichwedefineasatestingapproach
in which a developer interprets mutants as test goals and writes
tests to satisfy those goals.
Thisdistinctionisblurredintheliteratureand,asaconsequence,
priorworkusuallyappliedthesamemethodsforevaluatingmutant
selection techniques to both analysis and testing use cases. Specifi-
cally, evaluations usually compare mutant selection strategies torandom mutant selection [
1,9,16,51,54] by sampling a fraction
(often less than 10%) of all mutants once, and then evaluating how
effective a randomly sampled test set, which achieves full or x%mutation adequacy on the sample of mutants, is for the full setof mutants. In other words, these evaluations assess appropriatesampling thresholds and how representative the sampled set ofmutants is of the entire set with respect to measuring mutationadequacy. Examples include E-selective [
41,42], N% random [ 1],
andSDL[ 14,49]approaches,allofwhichareevaluatedwithrespect
tomutationadequacyor,morerecently,withrespecttominimal
mutation adequacy [33].
We argue that this evaluation methodology is appropriate for
mutationanalysis,butnotmutationtesting.Mutantselectionap-
proaches for mutation analysis and mutation testing differ both in
their overall goals (selecting a representative subset of mutants to
measure an existing test set‚Äôs mutation adequacy vs. selecting atest goal that elicits an effective test) and presumed use cases (apriorivs.incremental mutant selection). This paper accounts for
these differences in the design of a selection strategy for mutation
testing and the methods used to evaluate its efficacy.
Goals The goal of mutation analysis is to assess and compare
existingtestsetsortestingapproachesbymeasuringmutationade-
quacy.Inthecontextofmutationtesting,however,achievingfull
mutation adequacy is neither realistic nor desirable [ 33,45]. Devel-
opersdonot writemutation-orevencoverage-adequate testsets,
and for good reasons [ 19,34,48]. The problems with achieving full
coverage adequacy (e.g., test goals that are unsatisfiable or simply
not worth satisfying) apply equally to mutation adequacy, with
mutation adequacy having the additional burden of equivalent mu-
tants,whichposeanunrealisticworkload[ 48].Instead,industrial
mutation testing systems present few mutants at a time and do not
even compute mutation adequacy [ 7,45,46]. As a result, mutation
testing requires selecting the most useful of all mutants according
to some measure. (Section 4 proposes such a measure: TCAP.)Evaluation
Mutation analysis usually selects an entire subset
of mutants just once, a priori, while mutation testing repeatedly
andincrementally selects a few mutants. In the mutation testing
use case, a developer is presented with one or a few mutants at
a time, then resolves them by writing a test or labeling them as
equivalent[ 7,46].Ifamutantispresentedasatestgoalandresolved
at a given time, then subsequent selections reflect the fact that the
mutant, as well as all subsumed mutants, are detected.
A key difference between a-priori and incremental mutant se-
lection is the effect of redundancy among mutants on the effort
requiredtoresolvethosemutants.Asanexample,considera-priori
selecting three mutants with two selection strategies ùëÜ1andùëÜ2.
Suppose the first mutant of ùëÜ1elicits a test that detects all three
mutants, whereas the first mutant of ùëÜ2elicits a test that detects
onlythatmutant,thesecondmutantisequivalent,andthethirdmu-
tant elicits an additional test. The mutant sets of ùëÜ1andùëÜ2contain
theexactsamenumberofmutants,buttheefforttoresolvethose
mutantsdiffers‚Äî ùëÜ1requiredresolvingonlyonemutantwhereas ùëÜ2
required resolving all three.
Prior evaluations of mutant selection techniques were largely
basedona-priorimutantselectionandconsideredtwomeasures:
(1) the number or ratio of selected mutants and (2) the mutation
adequacy achieved by a corresponding test set. This is appropriate
forthemutationanalysisusecase.Forthemutationtestingusecase,
however,anevaluationshouldbeprincipallyconcernedwiththe
effortrequiredtoresolvemutantsandoverallprogresswithrespecttoadvancing testcompleteness. Inotherwords,such anevaluation
1745ICSE ‚Äô22, May 21‚Äì29, 2022, Pittsburgh, PA, USA Samuel J. Kaufman, Ryan Featherman, Justin Alvin, Bob Kurtz, Paul Ammann, and Ren√© Just
shouldbe agnostictoredundancyand measuretestcompleteness
over actual effort (as opposed to the number of selected mutants).
Our evaluation (Section6) is aligned withthe mutation testing
usecaseandadoptsthemodelproposedbyKurtzetal.[ 33].Specif-
ically, it operationalizes effort as a sequence of worksteps wherein
adeveloperispresentedasinglemutantandtheneitherwritesa
test to detect it or labels it as equivalent. The amount of work to
resolve each mutant is presumed to be equal, and the total amount
of work is therefore the sum of the number of tests written and
the number of equivalent mutants. This model has two advantages.
First, it is agnostic to redundancy: after a test is introduced, all de-
tected mutants are removed from further consideration and never
subsequentlyselected.Second,equivalentmutantshaveacostin
that they consume work but do not advance test completeness1.
4 TEST COMPLETENESS ADVANCEMENT
PROBABILITY (TCAP)
We propose a new measure of mutant usefulness: test completeness
advancement probability (TCAP), which enables prioritizing mu-
tants for incremental selection. For a given mutant, TCAP is the
probability that a mutant, if presented as a test goal, will elicit a
testthatadvancestestcompleteness.Theprobabilitydistributionof
TCAPisgenerallyunknowable,butitispossibletoestimateitwith
respect to an existing set of developer-written tests. For simplicity,
TCAP refers to its estimate in the remainder of this paper.
Kurtzetal.[ 33]defined testcompleteness intermsofwork:what
fraction of the expected number of tests necessary for mutation
adequacy have been written? Kurtz et al. [ 33] further showed that
dominator score, which is the fraction of dominator nodes detected
by a test set, enjoys a linear relationship with test completeness.
Thisisincontrasttothetraditionalmutationadequacyscore,which
rises rapidly with the first few tests due to redundant mutants, and
henceisapoormeasureoftestcompleteness.Consequently,this
paper uses dominator score as a proxy for test completeness.
Unlikepriorworkwhichdefinesmutantusefulnessasaproperty
solely of the mutant itself (e.g., [ 3,25,29,36]), TCAP quantifies
the usefulness of a mutant in terms of the value of its detecting
tests.Thiscapturesakeyidea:amutantisusefulasatestgoalonlyinsofarasitelicitsusefultests,andausefultestisonethatadvances
test completeness. Notice four properties of TCAP:
(1)Equivalent mutants have a TCAP of 0. This is desirable and
capturesthefactthatequivalentmutantsdonotleadtotests.
(2)Dominator mutants have a TCAP of 1. This is in line with
thedefinitionoftestcompleteness:selectingatestforadom-
inator mutant is guaranteed to advance test completeness.
(3)SubsumedmutantshaveaTCAPofstrictlygreaterthan0,but less than or equal to 1. Since subsumed mutants are
always detected by at least one test that also detects a domi-
natormutant,theirTCAPisstrictlygreaterthan0.Notethat
TCAP can be 1 for subsumed mutants, which is desirable
andcapturestheideathatamutant‚Äôsusefulnessisdefined
by the tests that it elicits.
1This model considers equivalent mutants as useless because they do not lead to tests.
This is a simplification: equivalent mutants can be useful and, for example, expose
code defects or lead to code refactorings [45, 48].(4)The TCAP of a subsumed mutant is smaller than or equal to
that of its dominating mutant(s). This is desirable becausesubsumed mutants can impose overhead if a dominatingmutant is later presented as a test goal: a developer might
write a weak test to detect a subsumed mutant and later
writeastrongertesttodetectadominatingmutant;itwould
have been more efficient to simply write the stronger test.
RecallthemotivatingexampleinFigure1:onlytheequivalent
mutantsùëö1andùëö10reachtheminimumTCAPof0;allsixdomina-
tormutantshaveaTCAPof1;thesubsumedmutants ùëö2andùëö5
as well as mutants ùëö3andùëö6have a TCAP of 0 .5‚Äîhalf of the tests
thatdetectthesemutantsalsodetectadominatornode.Incontrast,
the subsumed mutants ùëö11andùëö12have a TCAP of 1, despite not
being dominators‚Äîboth tests that detect these mutants also detect
a dominator mutant.
We do not value mutants according to their ability to detect
known faults (fault coupling) for three reasons. First, any bench-
mark that provides a set of known real faults is inevitably a subset
of thefaults thatcould havebeen introducedor thatalready exist
butare yettobe detected.Asa result,biasingmutant selectionto
a limited set of mutants may cause parts of the programs undertest to not be mutated and tested at all. Second, the core idea ofmutation testing is to systematically mutate a program to guard
against fault classes, not just a few known faults. Finally, the set of
dominator mutants subsumes all fault-coupled mutants.
5 PREDICTING TCAP
ToevaluatethebenefitsofusingTCAPtoprioritizemutants,we
trainedasetofmachinelearningmodels‚Äîlinearmodelsandrandom
forests‚Äî that predict TCAP from mutants‚Äô static program context.
Notethatthepurposeoftrainingthesemachinelearningmodels
is to evaluate whether program context is predictive of TCAP and
whetherpredictionperformancetranslatestodownstreamimprove-
mentsinmutationtestingwhenincrementallyselectingmutants
based on said predictions. The goal is neither to maximize anyparticular metric of model performance, nor to comprehensivelyexplore the design space of machine learning models to identify
themodeldesignthatwouldbebestsuitedforthistask.Weconjec-
ture that more sophisticated machine learning models and a richer
featuresetwilllikelyyieldlargerimprovements,butweleavean
in-depthexplorationof modelingchoicesandfeature importance
as future work.
5.1 Dataset
Toproduceadataset,wegeneratedmutantsforsubjectsdrawnfrom
Defects4J[ 22]andtransformedthemintofeaturevectorsdescribing
themutationandprogramcontext.Additionally,weassociatedeach
mutant with a label for TCAP, derived from detection matrices.
Wechose9subjects(projects)fromtheDefects4Jbenchmark[ 22]
(v2.0.0), which provides a set of 17 open-source projects accom-
panied by thorough test suites. We selected the latest version of
eachprojectandgeneratedmutantsfortheentireproject.Outof
17 projects, we filtered out 5 because of technical limitations in the
mutationframework(e.g.,JVMlimitsonthesizeofanindividual
method)and3forwhichthecomputationofafulldetectionmatrix
was computationally too expensive.
1746Prioritizing Mutants to Guide Mutation Testing ICSE ‚Äô22, May 21‚Äì29, 2022, Pittsburgh, PA, USA
Table1:Summaryofsubjectclassesandcoveredmutants. Covering givesthemeannumberoftestscoveringeachmutant,and
detecting gives the mean number of tests detecting each mutant.
Project Classes Mutants Tests
Total Equivalent Detectable
Dominator All Total Covering Detecting
Chart 44665,300 47.5% 25.5% 52.5% 2,193 24.8 6.9
Cli 202,838 16.0% 25.1% 84.0% 355 39.9 18.3
Codec 5024,281 30.5% 29.9% 69.5% 776 10.3 3.9
Collections 257 19,480 22.9% 29.0% 77.1% 16,069 26.9 14.5
Csv 91,986 17.5% 19.1% 82.5% 293 52.0 25.9
Gson 418,434 22.6% 24.0% 77.4% 1,035 98.2 41.0
JacksonXml 29 3,117 30.1% 26.1% 69.9% 183 39.2 15.7
JxPath 13518,530 28.3% 20.1% 71.7% 386 76.6 28.0
Lang 9738,532 17.8% 43.4% 82.2% 2,291 10.8 4.9
Overall 1,084 182,498 32.1% 29.6% 67.9% 23,581 29.6 11.1
We used the Major mutation framework [ 21] to generate all
possible mutants for each of the 9 subjects and to compute the
detectionmatrices.Ofthe2,033,496entriesinthedetectionmatrices,
3.2% were inconclusive due to a timeout of 16 ùë°ùëú+1 seconds, where
ùë°ùëúis a sample of the test runtime before mutation. Mutants that
time out are considered detected.
Weretainedonlymutantsthatarecoveredbyatleastonetest
fortheevaluationandtrainingsets.Coveredbutundetectedmu-tants are deemed equivalent, which is a common approximation
inmutationtestingresearch.Sinceuncoveredmutantscannotbe
detectedbuttherearenoteststhatwouldincreaseconfidenceinthe
approximation of mutant equivalence, these are excluded. Table 1
provides a detailed summary of our final dataset, showing only
retained (covered) mutants.
5.2 Program Context Features
Adopting the modeling approach and extending the model feature
setofJustetal.[ 25],wemodeledprogramcontextfeaturesusing
information available in a program‚Äôs AST. Given a mutated ASTnode, we consider syntactic context (i.e., parent and child nodesin the AST as well as nesting information) and semantic context
(i.e., the data type of expressions and operands or the data type of
method parameters) of that node.
Notethatpriorwork(e.g.,[ 53])usedinformationderivedfrom
test executions (e.g., code coverage) when making predictions in
a mutation analysis context. Because our goal is to identify useful
mutants for tests that have not yet been written (mutation testing),
we necessarily avoid features derived from tests.
Specifically, we chose the following set of features:
‚Ä¢MutationOperator. Thespecificprogramtransformationthat
generates a mutant (e.g., <‚Ü¶‚àí‚Üí!=in Figure 1).
‚Ä¢MutationOperatorGroup. OneofAOR,COR,EVR,LOR,LVR,
ORU, ROR, SOR, or STD.
‚Ä¢Node Data Types. The summary data type of the mutated
node in the AST (e.g., int,class,o rboolean(int,int) )
encoding the return and parameter types of methods andoperators, as well as a more abstract version which mapscomplextypestotheirresulttypes(e.g., boolean(int,int)
becomes boolean).
‚Ä¢Node Kind. The kind of the mutated AST node (e.g., ‚Äòbinary
operator‚Äô, ‚Äòliteral‚Äô, ‚Äòmethod call‚Äô).
‚Ä¢AST Context. Four features, each of which is a categorical
variable:(1)thesequenceofASTnodekindsfromthemu-
tatednode(inclusive)totherootnodeoftheAST;(2)extends
the first feature by including edge annotations describing
noderelationships;e.g.,a forloophaschildnodes for:init ,
for:cond ,for:inc,o rfor:body ; (3) and (4) correspond to
thefirsttwofeatures,butprovideahigherlevelofabstraction
andincludeonlythosenodesthataretop-levelstatements
(as opposed to individual expressions).
‚Ä¢ParentContext. Versionsof ASTcontext featuresthatconsider
only the immediate parent of the mutated node.
‚Ä¢ChildrenContext. Threefeaturesthatindicatethenodekinds
oftheimmediatechildnodesofthemutatedASTnode:(1)
animmediatechildnodeisa literal;(2)animmediatechild
nodeisa variable;(3)animmediatechildnodeisan operator.
‚Ä¢Relative Position. The relative line number of the mutated
nodeinsideitsenclosingmethod,dividedbythetotalnumber
of lines in that method.
‚Ä¢Nesting.Sevenfeatures intotal: (a)the maximumblock nest-
ingdepthanywhereintheenclosingmethod;(b)thenesting
depth(numberof enclosingblocks)consideringonly loops,
only conditionals, and any enclosing block (i.e., the sumof the previous two); and (c) an additional three features
dividing the previous three by the maximum nesting depth.
5.3 Machine Learning Models
We evaluated a small number of model design choices and training
settingsagainstanintrinsicmeasureofmodelperformancewiththeultimategoalofchoosingasinglemodelfordownstreamevaluation.
Each training setting was a kind of hold-one-out train-test split,
using each held out project (or class in a given project) at a time as
the evaluation set. We trained all models using Scikit-Learn [ 44]
and evaluated every combination of the following choices:
1747ICSE ‚Äô22, May 21‚Äì29, 2022, Pittsburgh, PA, USA Samuel J. Kaufman, Ryan Featherman, Justin Alvin, Bob Kurtz, Paul Ammann, and Ren√© Just
(1)Model Choice. We compared ridge regression ( ùõº= 1.0) to a
random forest regression (max depth of 3 and 10 trees).
(2)Feature Set. We used a comprehensive set of features (All
features,enumeratedinSection5.2)andasubsetofthesefea-
tures(Few features,usedin[ 25]:onlythemutationoperator
and detailed parent statement context).
(3)TrainingSetting. Weevaluatedthefollowingtrain-testsplits:
(a)AllProjects :trainingonmutantsfromallprojects,includ-
ing those in non-held-out classes in the same project,
and testing on mutants in the held-out class;
(b)Between Projects : training on mutants strictly in other
projects and testing on mutants in the held-out project;
(c)Project-Only :trainingonmutantsinallbutoneheld-out
class in a single project and testing on mutants in the
held-out class.
These training-test splits correspond to three realistic muta-
tiontestingsettings,inwhichadeveloperintendstotesta
new class in an existing project, with or without access to
other projects, or a completely new project.
Weevaluatedeachcombinationofmodelchoice,featureset,and
trainingsettingbytrainingonemodelforeachpossibletraining-test
split. For example, for a given model choice and feature set, the All
Projectstraining-testsplitsresultedinatotalof1,084models,one
for each held-out class in the dataset, whereas the Between Projects
splits resulted in just 9 models, one for each held-out project.
We compared the combinations based on the Spearman rank
correlation coefficient between the TCAP predictions and labels.
Therankcorrelationcoefficientisanappropriateintrinsicmeasure
becauseweareprincipallyinterestedintheorderinwhichmutants
willbeselected.Figure2showsthe distributions oftheresulting
coefficients across all models. We find that:
(1)ModelChoice. Alinearmodelissufficientforthefeaturesets
considered,anobservationconsistentwithpriorwork[30].
(2)FeatureSet. Alarger featuresetmodestlyimproves theper-
formanceoflinearmodels,butitreducesvariationincorrela-tioncoefficientsbetween splits,perhaps byinducing models
thatare lesssensitivetonoise inindividualfeatures.Many
ofthese featuresarecollinear(e.g., thevariousNestingfea-
tures).Whilethisdoesnotaffectthesuitabilityofourmodel
designs to the downstream evaluation, collinearity meansthat linear model coefficients and permutation feature im-portances are not individually interpretable and the highcardinality of our categorical features mean the same for
random forest impurities. Still, an ad-hoc analysis suggests
that syntactic context features are the most important ones.
(3)Training Setting. The median performance of Project-Only is
consistentlythebest,especiallyforthenon-linearmodels,
though the difference is less pronounced for linear models.
Whilemanyof themodelconfigurationsperformsimilarly, we
choosethefollowingconfigurationfortheexperimentsinSection6:
Model=Linear, Features=All, Training Setting=Project-Only
Project-Onlyisacommonevaluationscenarioandthechosencom-
bination has the highest median correlation coefficient.0.00.10.20.30.4Median Corr. CoefficientsModel = LinearFeatures Used = AllModel = Random Forest
AllProjects Between Projects Project-Only
Training Setting0.00.10.20.30.4Median Corr. Coefficients
AllProjects Between Projects Project-Only
Training SettingFeatures Used = Few
Figure2:Intrinsicevaluationofmodelperformance,brokendownbymodelchoice,featureset,andtrainingsetting.Eachboxplotcontainsninedatapoints,showingthedistributionof Spearman‚Äôs ùúåfor all projects. Whiskers extend up to 1.5
times the interquartile range.
6 EVALUATION
Considerdevelopingcodeforalargeprojectforwhichasignificant
body of code, along with associated mutants and tests, alreadyexists. A realistic mutation testing scenario employing a TCAP-
based mutant selection approach is:
(1)Train a model that predicts TCAP on a project‚Äôs existing
mutants and tests.
(2) Generate new mutants for a developer‚Äôs current source file.(3)
PredictTCAPofthesenewmutants,usingthetrainedmodel.
(4)Provide the developer with the highest TCAP mutant asa test goal; if they write a test, then remove all mutants
detectedbythattestfromfurtherconsideration.Repeatuntilastoppingconditionismet(e.g.,somefixedamountofwork
or a predicted TCAP threshold).
AsmotivatedinSection3,thisscenariocorrespondstomutation
testing,usingTCAPtoprioritizemutantsforincrementalmutant
selection, and each iteration corresponds to a single work unit.
Work Simulation ToevaluateTCAP-basedmutantprioritiza-
tion, we simulated the aforementioned scenario. A work simulation
begins with some initial‚Äîpossibly empty‚Äîtest set and the set ofmutants not detected by those tests, selects the highest-scoring
mutant according to some prioritization strategy, adds a randomly
selected test (without replacement) that detects that mutant to the
test set, and repeats with the remaining set of undetected mutants.
We evaluated TCAP-based mutant prioritization against two
baselines: OptimalandRandom. Optimal prioritizes mutants based
on the TCAP labels in the dataset, and Random simply producesa randomized order. Optimalestablishes an upper bound on the
performance of any mutant prioritization strategy and Random,
perhaps surprisingly, corresponds to the state-of-the-art [18, 33].
Allmutantprioritizationstrategiesinourevaluationarestochas-
tic:tiesfor TCAParebrokenrandomly,and thetestintroducedat
each work step is chosen uniformly at random (without replace-
ment)fromthosethatdetecttheselectedmutant.Ourevaluation
repeatseachworksimulation1,000timesperclass,forallstrategies.
1748Prioritizing Mutants to Guide Mutation Testing ICSE ‚Äô22, May 21‚Äì29, 2022, Pittsburgh, PA, USA
(a)Aworksimulationforasingleclass,plottingallindividual1,000
runs for each strategy.
(b) The mean test completeness (over the 1,000 individual runs) per
unit of work. The dashed line indicates the predicted TCAP.
Figure 3: An example work simulation for the CollectionUtils class in Apache Collections.
Figure 3 shows an example work simulation for a single class,
illustrating how test completeness increases over the course of the
simulation.Theoptimalstrategyincreasessmoothlyandrapidlyto
atestcompletenessof100%sinceeachunitofworkdetectsone‚Äîor
occasionallymorethanone‚Äîdominatornode.TCAP-basedselec-
tiondoesnotincreaseasrapidly,despitemakingconstantprogress,
since the choices are imperfect. However, it increases substantially
fasterthantherandomstrategy.Considertheunitsofworkrequired
by each strategy to reach 0.75 test completeness. The optimal strat-
egy requires about 50 units of work, and TCAP-based selection re-
quires no more than 60. The random selection strategy, in compari-son,requiresabout100unitsofwork.Inthissimulation,adeveloperinterestedinreachingatestcompletenessof0.75neededtoresolveabout20%moremutantswiththeTCAP-basedselection,compared
to the optimal strategy, whereas randomly selecting mutants as
test goals would have required resolving 100% more mutants.
Efficiency Inordertosummarizeaworksimulationandquantify
the efficacy of a prioritization strategy, we define efficiency to be a
measurementofastrategy‚Äôsimprovementoverprioritizingmutants
randomly,normalizedbytheperformanceoftheoptimalstrategy.
Specifically, the efficiency of the TCAP-based strategy ùë°over a
sequence of work units ùë¢is:
/summationdisplay.1
ùë¢(ùëêùë¢
ùë°‚àíùëêùë¢
ùëü)//summationdisplay.1
ùë¢(ùëêùë¢
ùëú‚àíùëêùë¢
ùëü) (1)
whereùëêùë¢ùëú,ùëêùë¢ùëü, andùëêùë¢
ùë°are test completeness at work unit ùë¢for the
optimal, random, and TCAP strategies respectively. An optimal
strategyhasanefficiencyof1,astrategynobetterthanrandomhas
an efficiency of 0, and a strategy that performs worse than random
has a negative efficiency. Intuitively, a positive efficiency indicates
how much of the gap (wasted work ) between random and optimal
thestrategycloses.Incontrast,anegativeefficiencyindicatesan
underperformingstrategy,butitisnotnormalizedbecausewedo
not include the worst possible strategy. The subsequent sections
showthatTCAP-basedprioritizationismoreefficientthanrandom,
and they also investigate outliers of negative efficiency results.‚àí2‚àí101
Chart Cli Codec Collections Csv Gson JacksonXml JxPath Lang
ProjectEfficiency
Figure4: Efficiencyperclass, groupedbyproject (7extreme
outliers with efficiency < -2 are removed from the plot forclarity; see Section 6.1.2 for details.)
Notethat46outof1,084classeshadeitherexactlyonegenerated
mutant or very few, yet only equivalent, mutants. In either case,
all three prioritization strategies are trivially equivalent, and hence
we discarded these classes, leaving 1,038 classes for analysis.
6.1 Simulating Work
6.1.1 MeasuringAverageEfficiency. Ourgoalistoevaluatewhether
TCAP-basedmutantprioritizationislikelytosaveworkforade-
vlopertestinganarbitraryclasswithanarbitrarytestbudget.To
that end, we simulated the efficiency of TCAP-based prioritization
for all 1,038 retained classes in our data set. We derived TCAP
predictions from the model described in Section 5.
Figure 4 shows the distribution of efficiency per class, broken
down by project. The results show that TCAP-based prioritization
consistently and meaningfully outperforms the random strategyacross all projects, reducing median wasted work by between a
third and a half.
6.1.2 Inspecting Extreme Outliers. We encountered seven extreme
outliers, with efficiencies ranging from -2.04 to -6.49, which are
1749ICSE ‚Äô22, May 21‚Äì29, 2022, Pittsburgh, PA, USA Samuel J. Kaufman, Ryan Featherman, Justin Alvin, Bob Kurtz, Paul Ammann, and Ren√© Just
0.000.250.500.751.00
1234
WorkTest completenessMutant prioritization Optimal TCAP Random
Figure5:Worksimulationforanextremeoutlierclass(Con-
stantFactory), with a negative efficiency of -4.04.
Table2:Efficienciescalculatedasthesumofeveryworkunit
acrossallclasses,perproject. Totalreferstoallclassesacross
allprojects.Theseefficienciesaccountforclasssizeandcor-respondingtotalnumberofworkunits.ThisisincontrasttoFigure 4, which plots distributions over class-level efficien-cies irrespective of the number of work units required toachieve 100% test completeness.
ChartCli Codec Collections Csv Gson JacksonXml JxPath Lang Total
0.410.19 0.19 0.35 0.45 0.15 0.22 0.11 0.32 0.33
removed from Figure 4 for clarity. The asymmetry in the efficiency
measure motivates us to understand whether these extreme out-
liersindicateapotentialformeaningfulcostsinpracticeoverthe
random approach. One possible explanation for these values lies
in features that can make a mutant set well tailored to the random
approach.Forexample,allextremeoutliershavefiveorfewerdom-
inatornodes,withfouroutliersonlyhavingone.Thepercentage
of covered mutants that are dominators is over 30% for all outliers.
Consequently, when most mutant choices increase completion and
problemsizeissmall,anearlysuboptimalchoicemadebyTCAP-
based selection can result in a substantial negative efficiency score
since random exhibits steady progress. Figure 5 demonstrates this
by showing an example for an extreme outlier (ConstantFactory
from Apache Collections) with a computed efficiency of -4.04.
To introduce some perspective, we can quantify the potential
real-world cost of these outliers for a developer generating the
handfuloftestsneededtoreachfulltestcompletenessonclasses
ofthissize.For5outof7outliers,themediantimetocompletion
for TCAP-based selection is only 1-2 steps behind random (4-5
steps for the remaining 2 outliers). However, this measure ignores
the high variance present in the random approach, compared to
TCAP-based selection. When considering the mean completion
percentage, TCAP-based selection finishes with or before random
for 6 outliers, with the final outlier taking only one additionalstep. In summary, none of these results suggest the potential for
significant performance concerns arising from these outlier values.
Figure6:TCAPthresholdasapredictoroftestcompleteness
(of the tests elicited by all mutants whose predicted TCAP
isabovethatthreshold).NotetheinvertedTCAPThresholdaxis.AllSpearman‚Äôsrankcorreletioncoefficients( ùúå)aresig-
nificant at ùëù<0.01.
6.1.3 AccountingforClassSize. Apotentialweaknessoftheeffi-
ciencyanalysisinSection6.1.1isthatittreatsallclassesasbeing
equal, even though they differ in size and therefore perhaps in
importanceortestingdifficulty.Toaccountforthis,wecomputed
the overall efficiency per project and across all projects‚Äîthat is,the efficiency over all mutants per project and over all mutantsin the entire data set. This means that larger classes, with moremutants and work steps, have a higher weight. Recall Figure 3b,
which visualized efficiency as the ratio of two areas. Intuitively,
overall efficiency is the ratio of the sums of these two areas across
all classes, as opposed to the average ratio per class. Table 2 shows
theresults.Whilethe efficiencyvariesbetweenprojects,itiscon-
sistentlypositive.Furthermore,thevariationinoverallefficiency
aligns with the variation of median efficiency across projects. In
conclusion,TCAP-basedmutationtestingreducesthetotalamount
of wasted work by a third.
6.2 Deciding When to Stop Testing
SelectingmutantsinorderofdescendingpredictedTCAPallows
a developer to focus time and effort on the most important test
goals. However, the ranking alone does not answer the fundamen-
tal question of when resolving additional mutants provides onlymarginal benefits. In other words, when should a developer stop
testing and how well tested is a software system after writing tests
for all mutants with a TCAP above a given threshold.
In order to understand whether TCAP thresholds are predictive
of expected test completeness, we correlated the two for all classes
in our data set. Figure 6 shows the results, indicating a strong
associationbetweenTCAPthresholdsandtestcompleteness.Atthe
same time, the variance of TCAP thresholds is quite high for some
projects.Forexample,adeveloperinterestedinreachingatleast50%
1750Prioritizing Mutants to Guide Mutation Testing ICSE ‚Äô22, May 21‚Äì29, 2022, Pittsburgh, PA, USA
Figure 7: Test sampling ratio vs. line coverage.
test completeness for an arbitrary class in Csv could confidently
use a TCAP of about 0.6 as a stopping criterion. However, it‚Äôsless clear what TCAP threshold a developer of Chart should use;a substantial amount of TCAP density spans the range from 0.6to 0.3. Nonetheless, these results are promising and suggest that
accounting for project-specific characteristics may reduce varianceandimprovethesepredictivemodels.Weleaveadeeperexploration
as future work.
6.3 Simulating Work with Initial Test Sets
Our evaluation so far has focused on the efficiency of TCAP-based
mutantprioritizationinascenariowheremutationtestingisapplied
from scratch. However, mutation testing often happens after some
testshavealreadybeenwritten.Toassesswhethertheefficiencyof
TCAP-based mutant prioritization is sensitive to the efficacy of an
existingtestset,weperformedanadditionalsimulation.Specifically,
we ran an additional 1,000 work simulations for each class, with
arandomlyselected,line-coverage-guidedinitialtestsetselected
from that class‚Äô existing test set. For the purpose of this simulation,
we measure test set efficacy as the line-coverage ratio.
Our goal is to understand the relationship between an initial
testset‚ÄôscodecoverageandtheefficiencyofTCAP-basedmutant
selection for all remaining mutants not detected by that initial test
set.Consequently,weaimtogenerate,foreachclass,acollection
of initial test sets with a uniform distribution of code-coverage
ratios.AsnotedbyChenetal.[ 10],therelationshipbetweentest
setsizeandcodecoverageisnotlinear.Auniformrandomselection
overtestsetsizewoulddrasticallyoversampletestsetswithvery
highcode-coverage ratios.Figure 7visualizesthis problem.Given
thelog-linearrelationship,weresortedtoinversetransformation
sampling: (1) we fitted a log-linear model for each class, predictingthe log of the test sampling ratio from a given code-coverage ratio;
(2)wesampled1,000code-coverageratiosuniformlyatrandom;(3)
we computed the corresponding test sampling ratio by sampling
from the inverse of the fitted model. The outcome of this sampling
approachwasapoolof1,000testsetsforeachclass,whosecode-
coverage distribution was approximately uniform. Performing a
work simulation for each of these initial test sets yielded efficiency
values that correspond to a given code-coverage ratio.
Figure 8: Efficiencies per class, broken down by project andcoverage quartile.
For simplicity, Figure 8 shows these efficiency results, binned by
quartilesofthecode-coveragedistribution.Aregressionanalysis,
using thenon-binneddata, confirmedthe visualtrend: thedegree
of code-coverage, achieved by the initial test suite, is either uncor-
relatedwithefficiencyoritisweaklypositivelycorrelated.ThekeytakeawayisthattheefficiencyofTCAP-basedmutantprioritization
is agnostic to the efficacy of the existing test set.
7 THREATS TO VALIDITY
External Validity Our results are potentially limited to the
projects in our empirical study, all of which are Java programs.
While we chose a variety of different projects, our results do not
necessarily apply to other languages or even to other Java projects
with different characteristics. Additionally, our estimates of TCAP
rely on existing, developer-written tests, which were likely notdeveloped in a mutation testing setting. It is possible that thesetestsarenotrepresentativeof thosethat developerswould write
whenresolvingamutant.However,astudyofPetroviƒáetal.[ 46],
inanindustrialsetting,foundnoqualitativedifferencesbetween
tests written specifically for mutants vs. tests written for other
objectives.
ConstructValidity Asiscommoninmutationtestingresearch,
weapproximateequivalentmutantsanddominatormutantswith
a set of existing tests. Since test sets generally have to be quite
thoroughbeforethedynamicapproximationofthesubsumption
relations convergesto the truesubsumption relation[ 31], werisk
identifying some non-dominator mutants as dominators, and vice-
versa. To counter this threat, we relied on projects that come with
thorough test sets. Additionally, as described in Section 3, we as-
sumethatdominatorscoreisavalidproxyfortestcompleteness
(w.r.t.mutationtesting).Whileanychoiceofproxyposesathreat
to construct validity, dominator score is a better proxy than the
alternative, the mutation score.
Internal Validity We rely on an idealized model of mutation
testing.Forexample,weassumethatworkunitscorrespondtosome
constant amount of actual engineering effort, but there is some
variancearoundthetimerequiredtoresolveasinglemutant[ 48].
Whilewehavenoreasontobelievethisisthecase,itispossiblethat
ourevaluatedmodelisbiasedtowardmutantswhichsystematically
take either more or less actual effort to resolve.
1751ICSE ‚Äô22, May 21‚Äì29, 2022, Pittsburgh, PA, USA Samuel J. Kaufman, Ryan Featherman, Justin Alvin, Bob Kurtz, Paul Ammann, and Ren√© Just
8 RELATED WORK
Thelargenumberofgeneratedmutantshaslongbeenarecognized
probleminmutationanalysisandmutationtestingresearch.This
section first discusses the most closely related work by Just et
al. [25] and Zhang et al. [ 53], and then provides a more general
overview of related work.
Closely Related Work Just et al. [ 25] demonstrated that pro-
gram context, derived from the abstract syntax tree, can signifi-
cantly improve estimates of a mutant‚Äôs expected usefulness. In this
context, usefulness was formally quantified as mutant utility along
three dimensions: equivalence, triviality, and dominance. In short,
Justetal.showedthatmutantusefulnessiscontext-dependent:a
mutation that leads to a useful mutant in one context does not
necessarily do so in another. While Just et al. showed that program
context correlates with mutant utility, they did not make use of
that correlation. They did not build a predictive model and didnot evaluate the benefits of using such a model for downstream
applications such as mutation testing.
Zhang et al. [ 53] used machine learning to predict mutation
scoresbothwithinandacrossprojectsforafixedtestsuite,using
featuresderivedbothfromprogramcontextand,critically,runtime
information such as code coverage which is only available after
tests have been written. Our predictions are very different: rather
than predicting expected mutation scores based on existing test
suites, we predict which mutants are useful and likely lead to addi-
tional, effective test cases. This distinction goes to the heart of the
differences between mutation analysis and mutation testing.
MutationTestinginPractice Petroviƒáetal.reportedonalarge-
scale deployment of mutation testing at Google [ 45‚Äì48]. Their
mutation testing system was integrated into a commit-oriented
development workflow. To combat the problems of generating too
manymutants,linesofcodenotchangedbyacommitwereignored,
as were lines in the commit not covered by at least one existing
test.Foreachremaininglineofchangedcode,thesystemgenerated
at most one mutant. Our work is immediately applicable to this
usecase.TCAP-basedmutantselectionallowsforpickingthemost
useful mutant for any given line.
Beller et al. [ 7] report on a similar large-scale deployment at
Facebook.Theyintegratedamutationtestingsystemintoacommit-
oriented development workflow and evaluated the system‚Äôs accep-
tancebyworkingengineers.Theyinnovatedbysemi-automatically
learningasmallnumberofcontext-sensitivemutationoperators
from real faults, reducing the total number of mutants generated.Useful Mutants
Researchers have addressed the notion that
somemutantsaremoreusefulthanothers: disjointmutants(Kin-
tisetal.[ 29]),stubbornmutants(Yaoetal.[ 52]),difficult-to-detect
mutants(Naminetal.[ 36]),minimal(akadominator )mutants(Am-
mannetal.[ 3]andKurtzetal.[ 31])andsurfacemutants(Gopinath
et al. [17]). Disjoint, stubborn, difficult-to-detect, dominator, and
surface mutants are suitable in a research context, but are not
directly applicable to the engineer in practice. Namin et al. [ 36]
described MuRanker,atooltoidentifydifficult-to-detectmutants
basedonthesyntacticdistancebetweenthemutantandtheoriginal
artifact.Theypostulatedtheexistenceof‚Äúhardmutants‚Äùthatare
difficulttodetectandforwhichadetectingtestmayalsodetectanumberofothermutants.ThisiscloselyrelatedtowhatKurtzet
al. have formalized as dominator mutants [3,31]. The key idea that
distinguishes TCAP from other proxy measures for mutant useful-
nessisthatamutantisusefulasatestgoalinmutationtestingonly
insofar as it elicits a test that advances test completeness.
MutantSelection Priorworkonmutantselectionhasfocused
on using a subset of the mutation operators (Mathur [ 35], Offutt
et al. [41,42], Wong et al. [ 50,51], Barbosa et al. [ 6], Namin et
al.[37‚Äì39],Untch[ 49],Dengetal.[ 14],Delamaroetal.[ 13],and
Delamaro et al. [ 12]) and choosing a random subset of mutants
(Acree [1], Budd [9], and Wong and Mathur [ 51]). Comparisons of
the two approaches (Zhang et al. [ 54], Gopinath et al. [ 15,16], and
Kurtz et al. [ 33]) ultimately showed the counterintuitive result that
existingmutantselectionapproachesdonotoutperformrandom
selection. Just et al.‚Äôs results [ 25] are consistent with these find-
ings:theyshowedthatcontext-agnosticmutantselectionshowsno
appreciable improvement over random selection. However, their
results also demonstrated that program context is predictive of
mutant usefulness, which motivated our work.
Fault Coupling While the numbers of mutants generated by
mutationoperatorsarealreadylarge,evenmoremutationoperators
areneededtogeneratemutantsthatarecoupledtorealfaults:empir-icalstudiesofmutationadequacy(DaranandTh√©venod-Fosse[
11],
NaminandKakarla[ 40],Andrewsetal.[ 4,5]andJustetal.[ 23])
showedthatfaultcouplingishigh,butalsothatthereisroomforim-provement.ThetailoredmutantworkofAllamanisetal.[
2]andthe
wild-caughtmutantworkofBrownetal.[ 8]demonstratedmutation
approachesthatcanclosethisgap,butatthecostofsubstantially
increasingthenumberofgeneratedmutants.Context-basedmutantselectionhasthepotentialtomakethesehigh-couplingapproachespractical.Papadakisetal.[
43]investigatedtherelationshipbetween
various quality indicators (measures of usefulness) for mutants; of
particularrelevancehereisthat,forthefaultsintheirstudy,only
17% of dominator mutants were fault-revealing (a property closely
related to fault coupling). As discussed in Section 4, we did notinclude a dimension for fault-coupling in our model precisely toavoid the consequent blind spots. The results of Papadakis et al.
suggested that these blind spots may be quite large.
EquivalentandRedundantMutants Jiaetal.surveyedmuta-
tiontesting ingeneral andprovideda detailedreviewof mutation
equivalence detection techniques [ 20]. Reducing the number of
equivalent mutants presented as test goals is a key goal in mak-
ing mutationtesting practical, and hence a keyfocus ofour work.
Ourworkcanbeappliedinadditiontoexisting,context-agnostic
techniques:prioritizingmutantswithrespecttoTCAPallowsan
incrementalmutantselectionapproachtoavoidmutantsthatare
likelyequivalentinaparticularcontext.Researchershavealsocon-
sideredredundancyofmutantswithrespectto‚Äúweak‚Äùmutation:
Kaminskiet al.[ 27,28] forrelationaloperator replacement(ROR),
Justetal.[ 24]forconditionaloperatorreplacement(COR),Yaoet
al. [52] for the arithmetic operator replacement (AOR), and Just
and Schweiggert [ 26] over COR, UOI, and ROR. Weak redundancy
analysistechniquesaresound,butonlyapplytoasmallsubsetof
the mutation operators, do not consider propagation, and do not
address the equivalent mutant problem.
1752Prioritizing Mutants to Guide Mutation Testing ICSE ‚Äô22, May 21‚Äì29, 2022, Pittsburgh, PA, USA
9 CONCLUSIONS
Tomakemutationtestingfeasibleforpractitioners,itisnecessaryto
address the vast number of mutants produced by current mutation
systems and the fact that most of these mutants are not useful.
This paper introduces a new measure for mutant usefulness,
called test completeness advancement probability (TCAP), built
on the insight that a mutant is useful as a test goal in mutation
testingonlyinsofarasitelicitsausefultest‚Äîonethatadvancestestcompleteness.Furthermore,thispaperdemonstratesthatamutant‚Äôs
staticprogramcontextispredictiveofTCAP andthatprioritizing
mutants with TCAP can effectively guides mutation testing.
The main results of this paper are as follows:
(1)Programcontext,modeledassyntacticandsemanticfeatures
of a program‚Äôs abstract syntax tree, is predictive of TCAP.
(2)TCAP-based mutant prioritization, independently of initial
test set code coverage ratios, improves test completeness
farmorerapidlythanrandomprioritization‚Äîwhichperhaps
surprisingly had prevailed as the state-of-the-art despite
decades of research on selective mutation.
(3)PredictedTCAPandachievedtestcompletenessarestrongly
correlated.WhilepredictedTCAPshowshighvariancefor
some subjects, the results suggest that an improved pre-diction model could render predicted TCAP as a practical
stopping criterion.
DATA & SOFTWARE AVAILABILITY
Toaidreuseandreplication,weprovidethedataandsourcecodeun-
derlying this work at: https://doi.org/10.6084/m9.figshare.19074428.
ACKNOWLEDGMENTS
We thank Audrey Seo and Benjamin Kushigian for development
support, and the anonymous reviewers for their valuable feedback.
This work is supported in part by National Science Foundation
grant CCF-1942055.
REFERENCES
[1]Alan T. Acree. 1980. On Mutation. Ph.D. Dissertation. Georgia Institute of
Technology, Atlanta, GA.
[2]MiltiadisAllamanis,EarlT.Barr,Ren√©Just,andCharlesSutton.2016. Tailored
mutants fit bugs better. arXiv preprint arXiv:1611.02516 (2016).
[3]PaulAmmann,MarcioE.Delamaro,andJeffOffutt.2014. EstablishingTheoretical
MinimalSetsofMutants.In 7thIEEEInternationalConferenceonSoftwareTesting,
Verification and Validation (ICST 2014). Cleveland, Ohio, USA, 21‚Äì31.
[4]James H. Andrews, Lionel C. Briand, and Yvan Labiche. 2005. Is Mutation an
AppropriateToolforTestingExperiments?.In Proceedingsofthe27thInternational
Conference on Software Engineering, (ICSE 2005). St. Louis, MO, 402‚Äì411.
[5]James H. Andrews, Lionel C. Briand, Yvan Labiche, and Akbar Siami Namin.
2006. Using Mutation Analysis for Assessing and Comparing Testing Coverage
Criteria.IEEE Transactions on Software Engineering 32, 8 (August 2006), 608.
[6]Ellen Francine Barbosa, Jos√© C. Maldonado, and Auri Marcelo Rizzo Vincenzi.
2001. TowardthedeterminationofsufficientmutantoperatorsforC. Software
Testing, Verification, and Reliability, Wiley 11, 2 (June 2001), 113‚Äì136.
[7]MoritzBeller,Chu-PanWong,JohannesBader,AndrewScott,MateuszMachalica,
Satish Chandra, and Erik Meijer. 2021. What It Would Take to Use Mutation
Testing in Industry‚ÄîA Study at Facebook. In Proceedings of the International
ConferenceonSoftwareEngineering(ICSE).268‚Äì277. https://doi.org/10.1109/ICSE-
SEIP52600.2021.00036
[8]DavidBinghamBrown,MichaelVaughn,BenLiblit,andThomasReps.2017. The
CareandFeedingofWild-caughtMutants.In ProceedingsoftheSymposiumon
the Foundations of Software Engineering (FSE). 511‚Äì522.
[9]TimA.Budd.1980. MutationAnalysisofProgramTestData. Ph.D.Dissertation.
Yale University, New Haven, Connecticut, USA.[10]Yiqun T. Chen, Rahul Gopinath, Anita Tadakamalla, Michael D. Ernst, ReidHolmes, Gordon Fraser, Paul Ammann, and Ren√© Just. 2020. Revisiting the
Relationship Between Fault Detection, Test Adequacy Criteria, and Test Set Size.
InProceedings of the International Conference on Automated Software Engineering
(ASE).
[11]Murial Daran and Pascale Th√©venod-Fosse. 1996. Software Error Analysis: A
RealCaseStudyInvolvingRealFaultsandMutations. ACMSIGSOFTSoftware
Engineering Notes 21, 3 (May 1996), 158‚Äì177.
[12]Marcio E. Delamaro, Lin Deng, Serapilha Dureli, Nan Li, and Jeff Offutt. 2014.
Experimental Evaluationof SDLand One-OpMutation forC. In 7th IEEEInter-
nationalConferenceonSoftwareTesting,VerificationandValidation(ICST2014).
Cleveland, Ohio.
[13]M√°rcioE.Delamaro,LinDeng,NanLi,andViniciusH.S.Durelli.2014. Grow-
ing a Reduced Set of Mutation Operators. In Proceedings of the 2014 Brazilian
Symposium on Software Engineering (SBES). Macei√≥, Alagoas, Brazil, 81‚Äì90.
[14]LinDeng,JeffOffutt,andNanLi.2013. EmpiricalEvaluationoftheStatement
Deletion Mutation Operator. In 6th IEEE International Conference on Software
Testing, Verification and Validation (ICST 2013). Luxembourg, 80‚Äì93.
[15]Rahul Gopinath, Iftekhar Ahmed, Amin Alipour, Carlos Jensen, and Alex Groce.
2017. Mutation Reduction Strategies Considered Harmful. IEEE Transactions on
Reliability 66, 3 (Sept 2017), 854‚Äì874.
[16]Rahul Gopinath, Amin Alipour, Iftekhar Ahmed, Carlos Jensen, and Alex Groce.
2015.Do Mutation Reduction Strategies Matter? Technical Report. School of
ElectricalEngineeringandComputerScience,OregonStateUniversity,Corvallis,
Oregon, USA.
[17]Rahul Gopinath, Amin Alipour, Iftekhar Ahmed, Carlos Jensen, and Alex Groce.
2016. MeasuringEffectivenessofMutantSets.In 2016IEEENinthInternational
ConferenceonSoftwareTesting,VerificationandValidationWorkshops(ICSTW).
132‚Äì141.
[18]Rahul Gopinath, Mohammad Amin Alipour, Iftekhar Ahmed, Carlos Jensen, and
AlexGroce.2016. OntheLimitsofMutationReductionStrategies.In Proceedings
of the International Conference on Software Engineering (ICSE) (Austin, Texas).
511‚Äì522.
[19]Marko Ivankoviƒá, Goran Petroviƒá, Ren√© Just, and Gordon Fraser. 2019. CodeCoverage at Google. In Proceedings of the Joint Meeting of the European Soft-
wareEngineeringConferenceandtheSymposiumonthe FoundationsofSoftware
Engineering (ESEC/FSE). 955‚Äì963.
[20]Yue Jia and Mark Harman. 2011. An Analysis and Survey of the Development of
Mutation Testing. IEEE Transactions of Software Engineering 37, 5 (September
2011), 649‚Äì678.
[21]Ren√© Just. 2014. The Major mutation framework: Efficient and scalable mutation
analysisforJava.In ProceedingsoftheInternationalSymposiumonSoftwareTesting
and Analysis (ISSTA). 433‚Äì436.
[22]Ren√© Just, Darioush Jalali, and Michael D. Ernst. 2014. Defects4J: A database
ofexistingfaultstoenablecontrolledtestingstudiesforJavaprograms.In Pro-
ceedings of the International Symposium on Software Testing and Analysis (ISSTA).
437‚Äì440.
[23]Ren√© Just, Darioush Jalali, Laura Inozemtseva, Michael D. Ernst, Reid Holmes,
andGordonFraser.2014. Aremutantsavalidsubstituteforrealfaultsinsoftware
testing?. In FSE 2014, Proceedings of the ACM SIGSOFT 22nd Symposium on the
Foundations of Software Engineering. Hong Kong, 654‚Äì665.
[24]Ren√© Just, Gregory M Kapfhammer, and Franz Schweiggert. 2012. Using non-redundant mutation operators and test suite prioritization to achieve efficient
and scalable mutation analysis. In Proceedings of the International Symposium on
Software Reliability Engineering (ISSRE). 11‚Äì20.
[25]Ren√© Just, Bob Kurtz, and Paul Ammann. 2017. Inferring Mutant Utility fromProgram Context. In Proceedings of the International Symposium on Software
Testing and Analysis (ISSTA). 284‚Äì294.
[26]Ren√© Just and Franz Schweiggert. 2015. Higher accuracy and lower run time:
efficientmutationanalysisusingnon-redundantmutationoperators. Software
Testing, Verification, and Reliability, Wiley 25, 5-7 (2015), 490‚Äì507.
[27]Garrett Kaminski, Paul Ammann, and Jeff Offutt. 2011. Better Predicate Testing.
InSixthWorkshoponAutomationofSoftwareTest(AST2011).HonoluluHI,57‚Äì63.
[28]GarrettKaminski,PaulAmmann,andJeffOffutt.2013. ImprovingLogic-Based
Testing.JournalofSystemsandSoftware,Elsevier 86(August2013),2002‚Äì2012.
Issue 8.
[29]Marinos Kintis, Mike Papadakis, and Nicos Malevris. 2010. Evaluating Muta-
tion Testing Alternatives: A Collateral Experiment. In 17th Asia Pacific Software
Engineering Conference (APSEC2010). Sydney, Australia.
[30]Bob Kurtz. 2018. Improving Mutation Testing with Dominator Mutants. Ph.D.
Dissertation. George Mason University, Fairfax, VA.
[31]BobKurtz,PaulAmmann,MarcioE.Delamaro,JeffOffutt,andLinDeng.2014.
Mutation Subsumption Graphs. In Tenth IEEE Workshop on Mutation Analysis
(Mutation 2014). Cleveland, Ohio, USA, 176‚Äì185.
[32]Bob Kurtz, Paul Ammann, Jeff Offutt, and Mariet Kurtz. 2016. Are We There
Yet? How Redundant and Equivalent Mutants Affect Determination of Test
Completeness.In TwelfthIEEEWorkshoponMutationAnalysis(Mutation2016).
Chicago, Illinois, USA.
1753ICSE ‚Äô22, May 21‚Äì29, 2022, Pittsburgh, PA, USA Samuel J. Kaufman, Ryan Featherman, Justin Alvin, Bob Kurtz, Paul Ammann, and Ren√© Just
[33]Robert Kurtz, Paul Ammann, Jeff Offutt, M√°rcio E. Delamaro, Mariet Kurtz, and
Nida G√∂k√ße. 2016. Analyzing the Validity of Selective Mutation with Dominator
Mutants. In FSE 2016, Proceedings of the ACM SIGSOFT International Symposium
on the Foundations of Software Engineering. Seattle, Washington, 571‚Äì582.
[34] Brian Marick. 1999. How to misuse code coverage. In Proc. of ICTCS .
[35]AdityaMathur.1991. Performance,Effectiveness,andReliabilityIssuesinSoft-
ware Testing. In Proceedings of the Fifteenth Annual International Computer Soft-
ware and Applications Conference. Tokyo, Japan, 604‚Äì605.
[36]Akbar Namin, Xiaozhen Xue, Omar Rosas, and Pankaj Sharma. 2015. MuRanker:
A mutant ranking tool. Software Testing, Verification, and Reliability 25, 5-7
(August 2015), 572‚Äì604.
[37]AkbarSiamiNaminandJamesH.Andrews.2006. FindingSufficientMutation
Operators via Variable Reduction. In Second Workshop on Mutation Analysis
(Mutation 2006). Raleigh, NC.
[38]Akbar Siami Namin and James H. Andrews. 2007. On Sufficiency of Mutants. In
Proceedings of the 29th International Conference on Software Engineering, Doctoral
Symposium. ACM, Minneapolis, MN, 73‚Äì74.
[39]AkbarSiamiNamin,JamesH.Andrews,andDuncanJ.Murdoch.2008. Sufficient
mutationoperatorsformeasuringtesteffectiveness.In Proceedingsofthe30th
International Conference on Software Engineering . ACM, Leipzig, Germany, 351‚Äì
360. https://doi.org/10.1145/1368088.1368136
[40]Akbar Siami Namin and SahityaKakarla. 2011. TheUse of Mutation in Testing
Experiments and Its Sensitivity to External Threats. In Proceedings of the 2011
International Symposium on Software Testing and Analysis (Toronto, Ontario,
Canada).ACM,NewYork,NY,342‚Äì352. https://doi.org/10.1145/2001420.2001461
[41]Jeff Offutt, Ammei Lee, Gregg Rothermel, Roland Untch, and Christian Zapf.
1996. AnExperimentalDeterminationofSufficientMutationOperators. ACM
Transactions on Software Engineering Methodology 5, 2 (April 1996), 99‚Äì118.
[42]JeffOffutt,GreggRothermel,andChristianZapf.1993. AnExperimentalEval-
uation of Selective Mutation. In Proceedings of the International Conference on
Software Engineering (ICSE). Baltimore, MD, 100‚Äì107.
[43]Mike Papadakis, Thierry Titcheu Chekam, and Yves Le Traon. 2018. Mutant
Quality Indicators. In Thirteenth IEEE Workshop on Mutation Analysis (Mutation
2018). Vasteras, Sweden, 32‚Äì39.
[44]Fabian Pedregosa, Ga√´l Varoquaux, Alexandre Gramfort, Vincent Michel,
BertrandThirion,OlivierGrisel,MathieuBlondel,PeterPrettenhofer,RonWeiss,
Vincent Dubourg, et al .2011. Scikit-learn: Machine learning in Python. Journalof Machine Learning Research 12, Oct (2011), 2825‚Äì2830.
[45]Goran Petroviƒá and Marko Ivankoviƒá. 2018. State of Mutation Testing at Google.
InProceedings of the International Conference on Software Engineering‚ÄîSoftware
Engineering in Practice (ICSE SEIP).
[46]Goran Petroviƒá, Marko Ivankoviƒá, Gordon Fraser, and Ren√© Just. 2021. Does
mutationtestingimprovetestingpractices?.In ProceedingsoftheInternational
Conference on Software Engineering (ICSE).
[47]Goran Petrovic, Marko Ivankovic, Gordon Fraser, and Rene Just. 2021. Practical
MutationTestingatScale:AviewfromGoogle. IEEETransactionsonSoftware
Engineering (2021).
[48]GoranPetroviƒá,MarkoIvankoviƒá,BobKurtz,PaulAmmann,andRen√©Just.2018.
AnIndustrialApplicationofMutationTesting:Lessons,Challenges,andResearch
Directions.In ThirteenthIEEEWorkshoponMutationAnalysis(Mutation2018).
Vasteras, Sweden, 47‚Äì53.
[49]Roland Untch. 2009. On Reduced Neighborhood Mutation Analysis Using a
Single Mutagenic Operator. In ACM Southeast Regional Conference. Clemson, SC,
19‚Äì21.
[50]W. Eric Wong, M√°rcio E. Delamaro, Jos√© C. Maldonado, and Aditya P. Mathur.
1994. ConstrainedMutationinCPrograms.In Proceedingsofthe8thBrazilian
Symposium on Software Engineering. Curitiba, Brazil, 439‚Äì452.
[51]W.EricWongandAdityaP.Mathur.1995. ReducingtheCostofMutationTesting:
AnEmpiricalStudy. JournalofSystemsandSoftware,Elsevier 31,3(December
1995), 185‚Äì196.
[52]XiangjuanYao,MarkHarman,andYueJia.2014. AStudyofEquivalentandStub-
born Mutation Operators Using Human Analysis of Equivalence. In Proceedings
of the 36th International Conference on Software Engineering, (ICSE 2014). IEEE
Computer Society Press, Hyderabad, India.
[53]Jie Zhang, Ziyi Wang, Lingming Zhang, Dan Hao, Lei Zang, Shiyang Cheng, and
Lu Zhang.2016. PredictiveMutation Testing.In Proceedings oftheInternational
Symposium on Software Testing and Analysis (ISSTA). 342‚Äì353.
[54]Lu Zhang, Shan-San Hou, Jun-Jue Hu, Tao Xie, and Hong Mei. 2010. Is operator-
based mutant selection superior to random mutant selection?. In Proceedings of
theInternationalConferenceonSoftwareEngineering(ICSE) .CapeTown,South
Africa, 435‚Äì444.
[55]Hong Zhu, Patrick AV Hall, and John HR May. 1997. Software unit test coverage
and adequacy. ACM Computing Surveys (CSUR) 29, 4 (1997), 366‚Äì427.
1754