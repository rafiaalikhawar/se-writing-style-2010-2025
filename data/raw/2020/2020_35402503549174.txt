HierarchicalBayesian Multi-kernelLearningforIntegrated
Classification and Summarization ofAppReviews
Moayad Alshangiti
Universityof Jeddah,Saudi Arabia
Rochester Institute of Tech.,USA
mshangiti@uj.edu.saWeishi Shi
Rochester Institute of Technology,
Rochester, USA
ws7586@rit.eduEduardo Lima
Rochester Institute of Technology,
Rochester, USA
eduardo.lima@rit.edu
Xumin Liu
Rochester Institute of Technology,
Rochester, USA
xumin.liu@rit.eduQiYu
Rochester Institute of Technology,
Rochester, USA
qi.yu@rit.edu
ABSTRACT
App stores enable users to share their experiences directly with
the developers in the form of app reviews. Recent studies have
shownthatthefeedbackreceivedfromusersisavaluablesource
of information for requirements extraction, which encourages app
developerstoleveragethereviewsforappupdateandmaintenance
purposes.Follow-upstudiesproposedautomatedtechniquestohelp
developersilterthelargevolumeofdailyandnoisyreviewsand/or
summarizetheircontent.However,allpreviousstudiesapproached
the app reviews classiication and summarization as separate tasks,
whichcomplicatedtheprocessandintroducedunnecessaryover-
head.Moreover,noneofthoseapproachesexploredthepotentialof
utilizingthehierarchicalrelationshipsthatexistbetweenthelabels
ofappreviewsforthepurposeofbuildingamoreaccuratemodel.
In this work, we propose Hierarchical Multi-Kernel Relevance Vec-
tor Machines (HMK-RVM), a Bayesian multi-kernel technique that
integrates app review classiication and summarization using a
uniiedmodel.Moreover,itcanprovideinsightsintothelearned
patternsandunderlyingdataforeasiermodelinterpretation.We
evaluatedourproposedapproachontworeal-worlddatasetsand
showed that in addition to the gained insights, the model produces
equal orbetterresults thanthe state ofthe art.
CCS CONCEPTS
·Software andits engineering →Requirementsanalysis ;
KEYWORDS
Bayesian Modeling, Multi-Kernel Learning, Relevant Vector Ma-
chines, App Reviews,UserRequirements
ACMReference Format:
Moayad Alshangiti, Weishi Shi, Eduardo Lima, Xumin Liu, and Qi Yu.
2022. Hierarchical Bayesian Multi-kernel Learning for Integrated Classi-
ication and Summarization of App Reviews. In Proceedings of the 30th
Permissionto make digitalor hard copies of allorpart ofthis work for personalor
classroom use is granted without fee provided that copies are not made or distributed
forproitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation
ontheirstpage.Copyrights forcomponentsofthisworkownedbyothersthanthe
author(s)mustbehonored.Abstractingwithcreditispermitted.Tocopyotherwise,or
republish,topostonserversortoredistributetolists,requirespriorspeciicpermission
and/or a fee. Request permissions from permissions@acm.org.
ESEC/FSE ’22, November 14ś18,2022, Singapore, Singapore
©2022 Copyright heldby the owner/author(s). Publicationrightslicensed to ACM.
ACM ISBN 978-1-4503-9413-0/22/11...$15.00
https://doi.org/10.1145/3540250.3549174ACM Joint European Software Engineering Conference and Symposium on
the Foundations of Software Engineering (ESEC/FSE ’22), November 14ś18,
2022, Singapore, Singapore. ACM, New York, NY, USA, 12pages.https:
//doi.org/10.1145/3540250.3549174
1 INTRODUCTION
User opinions on mobile apps are highly valued by app developers
due to thecompetitive natureofthemarket [ 4],where developers
attempt to attract the highest possible user base and maintain their
satisfactionlevel. Thus, developers would like to analyze thefeed-
back received from users in the form of app reviews to understand
theusers’requirements,preferences,andcomplaints[ 20,31].How-
ever, the large volume of app reviews received on a daily basis has
madeamanualanalysisofreviewstootime-consuming.Assuch,it
became favorable to have automated approaches that can facilitate
quicker andeasier access to the feedbackfoundinapp reviews.
Existingefortsonthistaskfallintotwodirections.Intheirst
one,aclassiicationmodelisconstructedtoassignreviewsintoa
predeined list of labels considered to be useful for app develop-
ers (e.g.,bug reports and feature requests) as a way to automate
the process [ 11,16,20,24,25,33,40,41]. However, assigning such
generallabelsisinadequatetoextractusefulrequirementsasone
can easily ind thousands of reviews that fall under one of those
labels and signiicant manual work is still needed to ind the actual
requirements. Thus, the second direction aims to summarize or
group together user reviews with similar topics for easier require-
mentextraction[ 9,13].Visualizationtechniqueshavebeenused
tohighlightthemostfrequenttermsusedinthosereviewsandit
is leftto thedevelopers to infer therequested feature(s).Similarly,
clusteringhasbeenleveragedtogroupreviewsthatcoverthesame
set of topics but the developers still have to analyze each cluster
to identifythe requirements embeddedin the reviewcontent. In a
moreend-to-endresearch,boththeclassiicationandsummariza-
tion tasks were attempted [ 15,32,34,43]. We align our work with
this direction. However, unlike previous work where the classiica-
tionandsummarizationtaskswerehandledseparately,wepropose
to merge the twotaskstogetherinasinglelearningprocess.
In this work, we present a novel approach to facilitate the ex-
traction of user requirements from app reviews in which both
the classiicationand summarizationare achieved simultaneously
usingauniiedmodel.Inparticular,weproposeHierarchicalMulti-
kernelRelevantVectorMachines(HMK-RVM),inwhichthreemain
558
ESEC/FSE ’22, November14–18, 2022,Singapore, Singapore Moayad Alshangiti,WeishiShi,Eduardo Lima,Xumin Liu, andQi Yu
goalsare accomplished.First,weexploitthe hierarchicalrelation-
ships between the labels during the learning process to build a
more accurate classiier. Second, we adopt a Bayesian multi-kernel
learning approach that encapsulates a rich feature space into sepa-
rate kernels, which achieved improved model interpretability and
understandingofpredictions.Third,inadditiontoacompetitive
classiication accuracy, the proposed approach can identify a small
set of most representative reviews as part of its learning process
that can efectively summarize the content of all available reviews.
Weextensivelyevaluateourapproachontworeal-worlddatasets
andshowthatitcanproduceequalorbetterclassiicationaccuracy
thanthe stateof theart while identifying the most informative re-
viewstogreatlyfacilitaterequirementsextraction.Wesummarize
our contributionsas follows:
•Weleveragethehierarchicalrelationshipsthat existinthe app
reviewlabelsaspartofthelearningprocess,whichisanewper-
spectiveontheclassiicationtaskthatwasnotconsideredbefore.
Wefurtherdemonstratethatusingthehierarchicalrelationships
between the labels can leadto amore accuratemodel.
•We propose a multi-kernel Bayesian approach that integrates
classiication and summarization under a uniied framework.
Weshowthatourproposedapproachcanofertwoadditional
beneitsbeyondaccuracy:(i)aninsightintothelearnedtaskand
the underlying data for better model interpretability, and (ii) an
insightintothemostrepresentativereviewsthatbestsummarize
the usersfeedback,for easier requirements extraction.
•Weevaluateourproposedapproach(HMK-RVM)ontworeal-
world datasets and show that it can provide better classiication
results while providing signiicantly better summarization re-
sultsthanthe state ofthe art.
Theremainderofthispaperisorganizedasfollows.Wepresenta
summary of related work in Section 2. We discuss our proposed
approachinSection 3.Weevaluateourapproachandpresentour
results in Section 4. We then discuss the signiicance of the results
and the potential threats to validity in Section 5and Section 6.
Finally,we conclude the paper inSection 7.
2 RELATED WORK
Inthissection,wesummarizeexistingstudiesrelatedtoappreviews
classiication and/or summarization. We divide existing works into
two major categories: (i) classiication only and (ii) classiication
followed by summarization. For the former, a key limitation is
thattheydonotaddressrequirementextraction.Hence,develop-
ers need to manually analyze all informative reviews (which is
usually around 35%-40% of all reviews) that leads to signiicant
overhead. As for the latter, they usually rely on a complex pipeline
that requires the implementation, tuning, and maintenance of two
diferentMLmodels,oneforclassiicationandoneforextraction
throughclustering/visualization orotherrelevantstrategies.
2.1 Summarizing User Reviews
Thereareseveralexistingworkswithafocusonsummarizingor
visualizingtheoveralltopicsfoundinuserreviews.In[ 19]anap-
proachtosummarizethemostdiscussedaspectsofaproductand
thecorrespondinguseropinions( i.e.,positiveornegative)ispre-
sented. In [ 9], topic modeling is exploited to discover the topicsfoundinthereviewsalongwithrepresentativesentences.In [ 11],
DBSCANclusteringisusedtogrouptogethersimilarreviews.In
[44],theauthorsproposedaninformationretrievalframeworkthat
processes thereviews andputthem ina knowledgedatabase. The
framework returns the most relevantreviews thatdiscussthe pro-
vided topics given the developer-selected keywords. In [ 13,15,34],
diferent visualization tools/techniques are presented. For example,
in [34], an HTML tool was presented that visualizes the content of
reviews by showing terms formatted in a word cloud. In [ 13,45]
they focused on providing an interface that summarizes and tracks
thechangeinreviewsunderspeciictopicsbetweendiferentver-
sionstohighlightabnormalchanges( e.g.,version2hassigniicantly
higher bugreports thanallotherversions).
2.2 Classifying User Reviews
As for app review classiication, [ 11] is among the irst attempts
to classify app reviews to be either informative ornon-informative .
The authors used a bag-of-words (BoW) representation, similar to
otherstudies[ 24,25,43].In[43],theauthorsleveragedN-gramsin
the BoW representation to account for context that requires two or
three words ( e.g.,not laggy ).Ifwe processthose words separately,
wewillnotunderstandtheactualintention.In[ 24],thetenseofthe
verbwasincorporatedinto thefeature space.Theauthorsargued
thatverbsinthepastareusuallyassociatedwithusersreporting
bugs, whereas, verbs in the future are usually correlated with hope
andrequestsforadditions( i.e.,featurerequests).In[ 33],theauthors
claimed that most reviews follow a speciic linguistic patterns and
identifying those patterns can help to improve classiication per-
formance.Thus,theycreated246linguisticpatternsthatdescribe
the general form in which a review would be in to fall under a
speciiclabel( e.g.,[someone]shouldadd[something] ).In[15],the
BoW representation is replaced with a representation generated
from parsing sentences as parsing trees and then traversing the
treetoconstructtherepresentation.Theauthorsclaimedthisap-
proach can take word semantics into consideration. In [ 40,41], the
authorssuggestedtoclassifyonthesentencelevelinsteadofthe
reviewleveltoallowformulti-labelclassiication.Itisalsoworth
mentioning that some studies investigated connections beyond the
classiicationofappreviews.Forexample,in[ 32],theauthorsinves-
tigatedthepossibilityoflinkinguserfeedbacktothesourcecode
components. Diferent from the studies above, we argue that there
is still room to improve the automation of requirements extraction
from app reviews. Therefore, we extend this line of work by in-
tegratingsummarizationandclassiicationtaskswhileexploring
unique characteristics of the problem, such as the hierarchical rela-
tionshipsbetweenthelabels.Bydoingso,weuncovernewways
to further improve the automation of such tasks.
2.3 OtherRelatedStudies on App Reviews
Thereisanumberofstudiesthatfocusonanalyzingappstores[ 18],
typesoffeedbackinuserreviews[ 10,12,17,21],andtheinteraction
betweenthesetwo[ 27].Wediferfromthosestudiesinthatweare
not analyzing the feedback itself.Instead, we focus on app review
classiicationandsummarizationtolargelyautomatetheextraction
ofuserrequirements.
559BayesianLearning forIntegratedClassification andSummarizationof App Revs. ESEC/FSE ’22, November14–18, 2022,Singapore, Singapore
Classifying Into 
Predefined LabelsIdentifying Most 
Representative SubsetSeparating Informative  
from Non-informativeDataset
App ReviewsPreprocessing Data
As many before
asked, we need: 
1) support for .vlc 
2) add dark theme 
3) allow us to ...... Review #1920
Presenting  
Example-based 
SummaryClassificationSummarization
Figure 1: Overview of the proposed approach for require-
ments extractionfromapp reviews
3 METHODOLOGY
Overview. In this section, we present the proposed HMK-RVM
model as seen in Figure 1. Given a set of app reviews,the proposed
approachirstclassiieseachreviewintothepredeinedlabels.This
would help developers separate informative reviews from non-
informativeones.Additionally,thepredeinedlabels( e.g.,feature
request,bugreport, etc.)canfurtherguidethedeveloperstoextract
therelevantrequirements.Alongwiththeclassiicationprocess,the
modelalsoidentiiesasetofthemostrepresentativereviewsthat
summarizetheentirecollection.Asaresult,developerscanonly
focus on these representative reviews for requirements extraction
astheyareexpectedtocapturemostofthediscussedrequirements.
Fortherestofthissection,weirstdiscussthediferencebetween
a lat and a hierarchical approach and justify how the latter its
betterwithappreviewclassiication.Wethenpresentourproposed
multi-kernelRVM,whichleveragesBayesiansparselearningand
multiple kernels to integrate classiication andsummarization.
3.1 Hierarchical User ReviewClassiication
Forcommonclassiicationtaskswithasetofbalancedclasses,stan-
dardmulti-classmodelscanbestraightforwardlyapplied.However,
for problems with highly imbalanced classes ( e.g.,those that in-
volve rare classes), standard techniques may sufer from a poor
performance due to lack of attention given to the minority class.
Meanwhile,ithasbeenshownthatleveragingexistinghierarchical
relationshipbetweenclassescanimprovetheperformanceofthe
classiier[ 22,29].Intraditionallatclassiication,thehierarchical
relationshipbetweenclassesisignored.Forexample,abinarylat
classiier would attempt to distinguish app reviews with feature
requestsfrom all other classes. This ignores the fact that reviewswithfeature requests and/orbug reports are all considered as in-
formative reviews,i.e.,theyshareacommonparentclass.Taking
thisinformationintoconsiderationwhentrainingtheclassiiercan
help us build a better classiier that attempts to irst distinguish
theinformative reviewsfromthe non-informative reviewsand then
further classify those informative reviews into their appropriate
class. In this way,classesat both levels tendto be more balanced.
Root
Feature 
RequestUsabilityBug ReportFunctionalNon-
functional
Security
Performance EnergyNon-
informativeInformative
Information
SeekingSentimental Spam
Figure 2:The hierarchical structure inapp review classes
Weobservethatallthepreviousworkshaveapproachedtheprob-
lemas alatclassiication problem.In[ 11],a binaryclassiierthat
determineswhetheranappreviewis informative ornon-informative
was used, introducing the irst two types of classes. A follow up
work [25] further studied the app reviews and introduced a new
set of labels rating,bug reports ,feature requests , anduser experience .
A more recent study used feedback from the industry to further
break down user experience into reviews reporting securitycon-
cerns,energyconcerns,andsoon.However,noexisting work has
attempted toleveragethehierarchicalrelationship betweenthose
classes as part of the learning process. Based on the analysis of
previous work, it is clear that the classes of app reviews can be
organized into a fairly complex hierarchy as shown in Figure 2.
It has been reported in multiple studies [ 31] that the informative
subset of app reviews represent at most 30%-35% of the whole cor-
pus. If we further break down the informative subset into multiple
classes, we can observe that some classes can be as rare as 5%-10%.
As such, using traditional lat classiication will create classiiers
dominatedbythenegative( i.e.,non-informative)class,leadingtoa
poorclassiicationperformance.Thislimitationcanbeaddressed
when ahierarchical classiication approach isused.
3.2 Multi-KernelRelevanceVectorMachines
Notations. LetX={x1,x2,..xN}denote a set of Ntraining in-
stances, where xi∈RD. We limit the introduction to Relevance
Vector Machines (RVM) [ 42] to binary classiication problem for
simplicity where each data instance xiis assigned with a label
ti∈ {0,1}.Later,thebinaryclassiicationsolutioncanbedirectly
generalized to multi-class problem with the one-vs-the-rest formu-
lation.TheRVMisaBayesianmodelinwhichthelabelfollowsthe
560ESEC/FSE ’22, November14–18, 2022,Singapore, Singapore Moayad Alshangiti,WeishiShi,Eduardo Lima,Xumin Liu, andQi Yu
Bernoullidistribution ti∼Bernoulli (σ):
p(ti=1)=yi=σ/parenleftBiggM/summationdisplay.1
m=1ϕm(xi)wm/parenrightBigg
=σ(w⊤ϕ(xi))(1)
whereϕ(xi)isavectorofMbasisfunctionsthatprojectsthefeature
space fromRDtoRM:ϕ(xi)=[ϕ1(xi),ϕ2(xi),..ϕM(xi)]. Typical
basis functions include polynomial, Gaussian, and sigmoidal [ 6]. In
RVM,thebasisfunctionsarespeciiedwithakernelfunction k(·,·):
ϕi(x)=k(x,xi).Wedenote K∈RN×Nasthegrammatrixwhose
i-th row is given by [k(x1,xi),k(x2,xi),..k(xN,xi)]⊤. The kernel
viewof (1) isgiven by:
p(ti=1)=yi=σ/parenleftBiggN/summationdisplay.1
n=1wnk(x,xn)/parenrightBigg
(2)
whereware model parameters that follow a Gaussian distribu-
tionp(w;α) ∼ N(0,A−1), withAbeing a diagonal matrix A=
diag(α1,...,αN). The goal of RVM is to learn the posterior distribu-
tionp(w|t,α)aswellastoestimatethehyper-parameter α.Here,
we omit the dependency on X,whichisimplied.
The posteriordistributioncan be derivedviathe Bayes’ rule:
lnp(w|t,α) ∝lnp(t|w)+lnp(w|α) (3)
By applying Laplace approximation, the posterior distribution also
follows a Gaussian distribution N(w∗,Σ), whose mean and covari-
ance are given by w∗=A−1K⊤(t−y)andΣ=(K⊤BK+A)−1,
respectively,where B=diag(y⊙ (1−y)).
The hyper-parameter αcanbe derived using type II maximiza-
tion.Todothat, we irstcompute the modelevidence
p(t|α)=∫
p(t|w)p(w|α)dw≃∫
p(t|w∗)p(w∗|α)dw(4)
where we used Taylor expansion on the integrant at w∗to remove
the integral. Then the optimal value of αis obtained by solving
∂p(t|α)
∂α=0:
α∗
i=1−αiΣii
(w∗
i)2(5)
Training of RVM is achieved through an iterative process of updat-
ingw∗,Σ,andα∗
iuntilconvergence.Inthepredictionphase,thepre-
dictive distribution of a test data point x′is given by p(t′|x′,w∗)=
Bernoulli (σ(w∗⊤x′)). The prior distribution adopted by RVM is
commonly referred as auto relevance detection (ARD). It makes
themodelprefersimplerexplanationsthancomplexexplanations
so that over-itting can be automatically addressed. Speciically,
duringthetrainingprocess,acertainnumberof α’scomponents
will be driven to ininity, making their corresponding training data
instances independent fromtheprediction and theremainingfew
importanttraining data instancesare called relevancevectors .
WemakeageneralextensiontoRVMtohandletheinputwith
multiple modalities ( e.g.,diferent representations). Suppose the
inputXis now given in three diferent representations XI,XII,
andXIII. Then, we construct a overall gram matrix as the linear
combination ofthe gram matrixfor eachrepresentation.
K=θ1K(XI,XI)+θ2K(XII,XII)+θ3K(XIII,XIII)(6)
Replacing the gram matrix in standard RVM with (6), we have
the RVM for multi-modality data input. The hyper-parameters
θ=(θ1,θ2,θ3)⊤can be solved by type II maximization similartosolving α.However,theobjectivefunctionisnotconvexwith
respect to θand may cause the optimization either to trap into the
localoptimaorcommittoslowconvergence.Toaddressthis,we
adopt a gradient-free method, simplex [ 5], to directly search the
optimalθinthe hyper-parameter space.
3.3 Why Extend RVM?
AfundamentalreasonforusingandextendingRVMforourproblem
is its ability to identify the most representative points that can
summarizetheunderlyingdataset.Thisalignswellwiththetaskof
inding the best subset of reviews that can be used for requirement
extraction. During model training, it ensures both the sparsityand
thequalityof the selected data points. The former, i.e.,sparsity,
allows us to identify a small subset of reviews to summarize the
entiredatasetinacompactway.Asaresult,thedevelopercansafely
ignore a large portion of the reviews to signiicantly reduce the
manual analysis efort when performing requirement extraction.
Thelatter, i.e.,quality,furtherhelpstoidentifythemostinformative
reviewsthatcanensuretheaccuracyandqualityoftheextracted
requirements. At the end of this training process, a set of points
are selected which the model uses for classiication. We propose
to use those points forsummarization aswell. Consequently, we
wouldachieve both aspectsusing asinglelearningmodel.
3.4 Constructing theKernels
The number of kernels used with the approach and their types
can be selected based on the available data and given task. For the
purpose of app reviews classiication, we constructed four kernels
to buildacomprehensive kernel space.
The irst isa metakernel, whichcaptures simple metainforma-
tion about the review, e.g.,the rating and the number of words.
The second is a kernel that utilizes the textual content of the re-
viewbycapturingtheimportantrecurringterms, e.g.,add,crash,
etc.To construct this kernel, we applied a standard natural lan-
guageprocessingmethods,suchasstop-wordremovalandword
stemming, on the textual content of the title and body of an app
review to generate such a representation using the term frequency-
inverse document frequency (TF-IDF) approach [ 26]. However, one
potential disadvantage withthis kernel isthatthe textualnature
of app reviews isquite noisy, whichcan leadto a largeand sparse
dictionary.Forthatpurpose,weconstructedathirdakernelthat
wouldprovideuswithalesssparserepresentationbyattemptingto
capture the broad topics within the reviews, in contrast to relying
on the exact terms. To construct this kernel, we used the topic
modeling technique, Latent Dirichlet Allocation (LDA) [ 7]. LDA
been widely used in many previous studies [ 2,3,38] to summarize
the topicsof a largedocument corpus. The intuition behind LDA is
that it leverages the textual content of a set of documents to group
together the frequently co-occurring words into an approximation
ofareal-world concept, i.e.,atopic.
Eventhoughthosekernelswouldprovideacomprehensiverepre-
sentation,theylackoneimportantaspect,andthatisthesemantics
of the used words. In [ 25], they foundthat classifying the reviews
comingfromtheiOSappstorewassigniicantlymoreaccuratethan
those coming from the Android store. They attribute this difer-
encetothelanguageandvocabularydiferencefromthosetwoapp
561BayesianLearning forIntegratedClassification andSummarizationof App Revs. ESEC/FSE ’22, November14–18, 2022,Singapore, Singapore
stores.TheyclaimthattheiOSstorereviewswerelessnoisy( e.g.,
havinglesstypos)andusedamuchmorehomogeneousvocabulary
ofterms. This observationhighlightstheefectofthenoisefound
inuserreviewsontheclassiicationtaskandtheimpactithason
thelearningprocess.In[ 44],thisobservationwasstudiedfurther
as the authors also highlighted and described the observation that
app reviews sufer from a high percentage of typos, acronyms, and
abbreviations. They performed a preliminary analysis of 300,000
reviews and compared their textual content against an English dic-
tionaryof150,000commonwords,andfoundthatalargeportion
of the used words in app reviews do not match any words in the
English dictionary, mainly due to abbreviations and typos.Having
such high noise and unique language ( e.g., wait is written as w8)
createsanissuefortraditionaldataminingtechniquesthatrelies
on stemming and dictionary creation as both waitandw8will still
exist as two unique diferent words. They hypothesized that this
observation might be due to the fact that reviews are written using
mobile devices which lack a physical keyboard, hence, it is more
likely to have typos, acronyms, and abbreviations. To overcome
thisissue,theauthorsin[ 44]manuallycreatedacustomdictionary
that attempts to replace the most frequent out-of-dictionary words
with their dictionary-equivalent ( e.g.,replacing exelentwithexcel-
lent). Moreover, in [ 15], a similar observation was made, and the
authorsmanuallyconstructedacollectionof60diferenttyposand
contractions,andreplacedthemusing regularexpressions.
Wehaveobservedasimilarpatternofnoisewithappreviews,
wherealargeportionofwordsinthepost-processingandstemming
dictionary seem to represent the same word but written diferently
duetomisspelledwords( e.g.,fantastic vsfantastick )oralternatively
spelledwordseitherforabbreviationspurposes( e.g.,thanks vsthx),
or to represent a stronger emotion, ( e.g., loved vsloooved). In Table
1,we showafewadditionalexamples.
Table 1:Examples ofmis- oralternativelyspelled words
Word Observed noise
amazing amaazing, amaaazing, amassing, amazeng
thanks thx, thanx, tx,tnx,10x,thnx, ty
awesome awasome,awesomeeee,awsome,owesome,
asssome
love lov,luv,lovve,looove,loveee
because bc, b/c,cuz, coz, bcz,caus
While merging misspelled or alternatively spelled words would
improvethetextualrepresentationandthemodel’sperformance,
wearguethatusingamanuallycreatedcustomdictionarywould
betoodiicultto createandmaintainoverthetime.To overcome
this issue, we constructeda fourthkernel that leverageswordem-
bedding techniques to create a representation that captures the
semanticsofwords.Forexample,theword2vec[ 28],theGloVe[ 36],
orthe FastText[ 8]modelarealltechniquesthattakeinto consider-
ation word semantics and meanings. These techniques are built on
thenotionthatwordswithsimilarsemanticmeaningswillhavethe
samesetofwordsaroundthem.Forexample,thewords loveandlikeareusedinsimilarmanners, i.e.,Ilovethatapp andIlikethatapp .
Asaresult,theywouldbecloselyplacedintheembeddingspace
as they share a similar semantic meaning. To construct this kernel
space,weneedtoeitheruseapre-trainedmodelfromadiferent
domain(e.g.,Tweets),ortrainourownproblem-speciicmodelto
generatethewordembeddingsfortheappreviews.Accordingto
[23,39],whichstudiedthisspeciicconcernamongotherchoices
withwordembeddingmodels,itisrecommendedtouseamodel
that is speciic to your domain as it can better capture the relevant
vocabulary and their unique usage. We believe this is especially
trueforapp reviewsandanimportantfactortoovercomethe issue
of misspelled and alternatively spelled words. To the best of our
knowledge, there’sno publicly availableword embedding model
that was trained on app reviews. For that reason, we decided to
create such a model and make it publicly available as part of our
replicationpackage. We trainedaFastText[ 8]model on1,673,672
app reviews collected from [ 35] and [14]. We chose FastText be-
cause it is most efective in settings where out-of-dictionary words
arecommon,likeours.Formoredetailsonthetrainingprocessand
the selectedparameters,kindlyrefer to the replication package1.
Finally,itisworthnotingthatourapproachhasthelexibilityto
useanynumberofkernels.Webelievethesuggestedfourkernels
representappreviewsquitewellandcancapturebothhigh-level
concepts( e.g.,LDA) and low-level characteristics( e.g.,meta). As a
result,thefourkernelsofersuicientexpressivepowertoconstruct
a comprehensive feature space that can help the machine learning
modelachieve agoodlevel of robustnessandgeneralization.
4 EVALUATION AND RESULTS
Inthissection,weplantoevaluatetheproposedmodel,speciically,
byinvestigating the following research questions:
RQ1:Dowe gainany app reviews predictionaccuracy fromhier-
archical classiication versuslatclassiication?
RQ2:Howaccurateisthe classiication oftheproposedHMK-RVM
approach comparedto the state of the art?
RQ3:Howaccurateisthe summarization oftheproposedHMK-
RVM approach comparedto the state of the art?
RQ4:Beyond accuracy, what insights can we gain from using the
proposedhierarchical multi-kernelRVM approach?
The source code and data used in the experiment section are
available onlinefor easyreplication/validation purposes1.
Table 2:Statistics ofthe used datasets
Maalej Panichella
Feature Request 252(7%) 391(13%)
Bug Report 370(10%) 271(9%)
UserExperience 607(16%) 334(11%)
Total Info 1229 (33%) 880(30%)
Total Non-Info 2455 (67%) 2024 (70%)
TotalReviews 3684 2904
1https://tinyurl.com/qup3h4l
562ESEC/FSE ’22, November14–18, 2022,Singapore, Singapore Moayad Alshangiti,WeishiShi,Eduardo Lima,Xumin Liu, andQi Yu
4.1 Datasets
To address our questions, we will report results on two real-world
datasetsthatwereprovidedbypreviousresearch.Theirstisthe
Maalejdataset [24,25],wherereviewswererandomlyselectedfrom
bothAppleandGooglePlaystores.Theauthorscrawledoveramil-
lion app reviews and followed a sampling strategy with the goal of
picking a stratiied and a representative sample ( e.g.,equal number
offreeandpaidapps,equalnumber ofiOSandAndroidapp, etc.).
The second is the Panichella dataset [34,40], where the authors
favoredanappspeciicsamplingapproach.Thedatasetcontains
reviews of 17 apps coming from Google Play, Microsoft, and Apple
appstores.Unfortunately,thegroundtruthwasnotprovidedfor
thisdatasetsoweaskedtwoteamsofgraduatestudentstolabelthe
dataset separately according to a labelling guide that can be found
withthereplicationpackage1.Theguidefollowscloselytheguid-
ance of the original paper. Once the teams completed the labelling
task,we compared thelabels andaddressed all disagreements.We
foundagoodinter-rateragreement(kappa=0.68)betweenthean-
notators. The statisticsofboth datasets can be foundinTable 2.
4.2 Experiment andResults
RQ1:Dowegain any app reviewsprediction
accuracy from hierarchical classiication versus
lat classiication?
Experimental Setup: To evaluate the model’s accuracy gained
from leveraging the hierarchical relationship embedded within the
labels, we will use a simple feature space consisting of a bag-of-
words representation using TF-IDF [ 37]. For this evaluation, we
willnotattempttoaddanyadditionalfeaturessuchasmeta-data
features( e.g.,rating,reviewlength, etc.)aswewanttofocusonthe
added beneit of hierarchical versus lat app reviews classiication.
Moreover, to make sure the results are not due to a speciic clas-
siierortoaspeciicdataset,wewillevaluateonboththe Maalej
andPanichella datasets, andonfourdiferentclassiiers:Logistic
Regression ( L1Regularization ), RandomForest( 200 trees), Support
Vector Machines ( Linear Kernel) , Relevant Vector Machines ( Linear
Kernel), and Naive Bayes ( Multinomial ). As we are limited to the
three mutual labels (bug report, feature request, and user experi-
ence)providedwiththosedatasets,wewillbuildtheexperiment
around them. Finally, to make sure both the lat and hierarchical
classiiers were exposed to the same set of reviews during training
andtesting,we usedasingletrain/test split of80/20for both.
For lat classiication, as shown in Figure 3(a), we are training
three one-vs-the-rest binary classiiers, one classiier per label ( e.g.,
bugreportornot).Weprefertousebinaryclassiiersinsteadofa
multi-classclassiierasthissetupallowsformulti-labelclassiica-
tion.Thismeansanappreviewcanbegivenasingleormultiple
labels. For example, an app review with multiple labels from the
Panichella dataset is ł This is a great app for keeping track of weight
...thereshouldbeawaytoturnofdailyreminder...alsoInoticeit
keeps changing the year I was born... ł. However, using this setup, it
isalsopossibleforanappreviewnottobeassignedanyofthethree
classes. For that purpose, in Figure 3(a)we show a non-informative
node that captures allsuch cases.Forhierarchicalclassiication,asshowninFigure 3(b),weuse
a top-down approach for training and classiication purpose. At
the irst level, we are using a binary classiier that classiies all
app reviews as either informative ornon-informative , and on the
second level we use three one-vs-the-rest binary classiiers that
attempt to further classify what passes as informative under one
ornoneofthethreeclasses:bugreport,featurerequest,anduser
experience. Thus, in hierarchical classiication, we are training one
more classiier than lat classiication. This may seem as added
complexity,however,thetopdownapproachactuallyhasabetter
overallcomputationalcostbecauseonlythe informative classiieris
trainedonallthetrainingexamples,theremainingthreeclassiiers
train only on the informative subset. For example, if we had a
training data set of 10k app reviews, 3k of those are informative,
thentheirstlevelclassiierwilltrainonall10kappreviews,butthe
secondlevelwillonlyhavetotrainonthe3kappreviews.Whereas,
inlatclassiication,eachoftheclassiierswouldneedtobetrained
onthe complete 10kdataset.
Root
Feature 
RequestUser
ExperienceBug Report
(a) Flat ClassiicationRoot
Feature 
RequestUser
ExperienceBug ReportInformative
(b) HierarchicalClassiication
Figure3:Evaluationoflatandhierarchicalappreviewclas-
siication,where eachnodeis apotentiallabel
ExperimentResults: WereporttheaverageAUCcomputedfrom
precision and recall ( AUCPR), macro F1 (M F1), and macro recall
(MR)inTable 3.Wecanmakeacoupleofobservations.First,Naive
Bayes seems to outperform the other classiiers when a simple bag
of words model is used, which was also observed in a previous
study [25], because a term count representation aligns perfectly
withhowNaiveBayesworks.Second,overall,formulatingtheprob-
lemusinghierarchicalclassiicationincreasesthemodel’saccuracy,
especiallywithrecall( i.e.,increasingthechancethatwedonotmiss
any informative app reviews). On the Maalej dataset, we observed
onaveragea8.4%better AUCPR,49.8%betterF1measure,and108%
betterrecall.SimilarlyonthePanichelladatasetweobserved13%
betterAUCPR,17%betterF1,and33%betterrecall.Tobetterunder-
stand theresults, weanalyzed theperformanceof RandomForest
on the Panichella dataset where the recall had an improvement
of61%.It’simportanttomentionthatinappreviewclassiication,
the ability to label all existing informative reviews correctly ( i.e.,
recall)ismoreimportantthanmis-classifyingafew non-informative
reviews as informative (i.e.,precision) because all reviews labelled
asnon-informative are usually disregarded ( i.e.,feedback would be
lost with low recall). Thus, this signiicant improvement on the
recallwhen using ahierarchical approach is aperfect matchwith
the app reviewclassiication problem.
563BayesianLearning forIntegratedClassification andSummarizationof App Revs. ESEC/FSE ’22, November14–18, 2022,Singapore, Singapore
Table 3:Classiication results oflatandhierarchical app review classiiers
ClassiierMaalejDataset Panichella Dataset
Flat Hierarchical Flat Hierarchical
AUCPRMF1MRAUCPRMF1MRAUCPRMF1MRAUCPRMF1MR
Logistic
Reg.0.349 0.369 0.381 0.393
(+12%)0.433
(+17%)0.562
(+47%)0.622 0.599 0.594 0.699
(+12%)0.681
(+13%)0.731
(+23%)
Random
Forest0.399 0.195 0.136 0.433
(+8%)0.531
(+172%)0.603
(+343%)0.739 0.541 0.428 0.768
(+4%)0.699
(+29%)0.692
(+61%)
SVM 0.346 0.358 0.385 0.353
(+2%)0.423
(+18%)0.561
(+45%)0.482 0.523 0.572 0.625
(+30%)0.617
(+17%)0.701
(+22%)
Naive
Bayes0.458 0.474 0.529 0.497
(+8%)0.507
(+7%)0.623
(+17%)0.681 0.630 0.624 0.768
(+13%)0.705
(+10%)0.736
(+17%)
RVM 0.459 0.375 0.309 0.514
(+12%)0.505
(+35%)0.591
(+91%)0.686 0.591 0.512 0.734
(+7%)0.702
(+18%)0.747
(+45%)
Root
Feature 
RequestBug ReportUser
ExperienceClassifier 1Given 52 reviews with bug reports
20 classified as bug reports
30 discarded as non-informative
(a) Flat ClassiicationRoot
Feature 
RequestBug ReportUser
ExperienceClassifier 1Given 52 reviews with bug reports
44 classified as informative
8 discarded as non-informativeInformative
Classifier 2
30 classified as bug reports
14 remained labeled as informative w/o a subclass
(b) HierarchicalClassiication
Figure 4:Given 52app reviewswith bugreports,how were they classiied inlatvs hierarchical?
WereportinTable 4therecallofeachclassiier.Inlatclassiica-
tion, we can observe that the classiier’s abilityto correctly classify
all thebug report anduser experience instances is quite poor. As we
believe the bugreport is a more criticalcategory,we further inves-
tigated the instances and how they were labelled in both classiiers
as shown in Figure 4. In our experiment, the testing sample had
52 app reviews with bug reports. In the case of lat classiication,
we clearly observed that the classiier missed 32 of the bug reports
(62%).However,thehierarchicalclassiiermislabelled8bugreports
out of the 52 as non-informative , and mislabeled 14 bug reports outofthe 44 informative reviews as othertype of informative reviews.
Overall,theclassiiermislabelled42%ofthebugreports,amuch
better recall than the lat classiier. Upon further checking, we can
observethattheirstlevelperformanceinthehierarchicalclassiier
is excellent as we were able to capture 85% of the bug reports as
informativereviews.However,thesecondlevelperformancewas
less ideal ( i.e.,missing 14 out of 44), but we can argue that it is still
betterthan the lat classiier as we were still able to label thoseapp
reviewsas informative ,i.e.,theywerenotcompletelymissed,but
were incorrectlyclassiiedas othertypes of informative reviews.
564ESEC/FSE ’22, November14–18, 2022,Singapore, Singapore Moayad Alshangiti,WeishiShi,Eduardo Lima,Xumin Liu, andQi Yu
Table 4: Analyzing random forest: the lat vs. the hierarchi-
calclassiiers on thePanichella dataset
Type Info.Feature
RequestBug
ReportUser Ex-
perience
RecallMeasure
Flat 0.5170.666 0.385 0.145
Hierarchical 0.7270.831 0.682 0.526
We credit the better performance of the hierarchical classiier
to two main factors. First, it is not afected as much by the class
imbalance as the lat classiier. In the case of lat classiication, the
frequency of each class is dominated by the negative class, e.g.,the
bug report classiier had 91% instances of the negative class so it
needstodistinguishfromthe non-informative andother informa-
tiveclasses, which is quite challenging. However, in hierarchical
classiication, the irst level uses the combined knowledge from all
threeclassestoirstilterout informative fromnon-informative app
reviews,whichisaneasiertask, i.e.,duetothediferentnatureof
non-informative reviewsfrom informative alongwithamuchhigher
positiveclassfrequency.Second,weobservedthatthe bugreport
classiier can distinguish itself better from other feature request
anduser experience reviews (i.e.,informative reviews) when non-
informative reviews are removed, which is what the hierarchical
top-downclassiication isinherently doing.
RQ2:Howaccurate istheclassiication ofthe
proposedHMK-RVMapproach compared to the
stateoftheart?
Baselines: Toevaluateourproposedapproach,wecomparedagainst
ive baselines and using two diferent datasets. The proposed ap-
proach and all the baselines presented are trained using the textual
contentofthe reviewsandthe meta data information.
First, the AR-Miner [11] baseline used a Naive Bayes model
[30], where the hidden topics of the reviews were discovered using
LatentDirichletAllocation(LDA)[ 7]andusedalongsidetherating
of the app review to construct the feature space. To implement
theirapproach,weselectedthenumberoftopics kforLDAusing
cross-validation. Speciically,we chose85 topics for both datasets.
Second, the Maalej[25] baseline also adopted a Naive Bayes
modelduetoitspreviouslyreportedhighperformancewithtext
classiication. However, [ 25] used a bag of words approach and
extracted the ratio of past, present, and future tenses in the review
to represent the textual content, claiming that reviews with bug re-
portstendtousepasttenses,whereasreviewswithfeaturerequests
tendtousefuturetenses.Additionally,theyusedthereview’srating,
length,andsentiment score as part oftheirfeatures.
Third,the ARdoc[33,34]baselineleveragedadecisiontree(J48)
model.Theauthorsmanuallyconstructed246linguisticpatterns
each mapping to a speciic app review label, e.g.,reviews with
pattern[someone] should add [something] are mapped to feature
requests.Moreover,theygeneratedaTF-IDFrepresentationfromthe
textualcontentofthereviewsandusedthereview’ssentimentscore
intheirfeaturespace.Duetothediicultiesinrecreatingthe246linguisticpatterns,we didnotimplement thisapproach ourselves
but rather used the tool provided by the authors to generate labels.
As such, we do not have the AUC and ROC scores for this baseline,
sincecomputingthemrequiresaccesstothemodelitselftoevaluate
performance underdiferentdecision thresholds.
Finally, we include the proposed approach with two variant
baselines. The RVMbaseline, where the Relevance Vector Ma-
chines (RVM) [ 42] with fast marginal likelihood maximization is
usedasabaselineusingourcompletefeaturespace(featuresare
concatenated into a single large feature space) and presented as
an alternative to using the multi-kernel learning approach. The
(MK-RVM ) baselinewhere the multi-kernelapproach isaddedto
the previous baseline and the problem is approached using a lat
classiication approach as an alternative to using a hierarchical
approach.Finally,( HMK-RVM ),whichisourproposedmodel,the
hierarchical multi-kernel RVM which combines the power of RVM
with multi-kernel learning and follows a hierarchical classiication
approach that leveragesthe existing hierarchical structure.
Experiment Setup: We formulated the learning task as a bi-
nary one-vs-the-rest problem by following earlier work. We chose
this formulation due to two reasons: the irst is that it supports
multi-labelclassiication( e.g.,areviewcancontainbothabugre-
port and a feature request), and the second is due to its higher
reported performance than multi-class classiication. For example,
[25]reportedthatusingmultiplebinaryclassiiersforappreview
classiication performed signiicantly better than a single multi-
class classiierinall cases.Tomeasure theaccuracyofthemodels,
weusedatrain/testsplitof80/20.Tomeasuretherobustness, i.e.,
performanceondiferentdatasets,weconductedtheexperiment
onbothMaalejandPanichella datasets.
Experiment Results: In Table5, we show a summary of the
results. We can observe that the traditional RVM that uses our
proposedfeaturespaceperformsonparwiththeotherbaselines
introducedinpriorwork.Thishighlightstheusefulnessoflever-
agingtheinformationfrommultipleaspectsandshowsthatRVM
is on equal footing to other models such as Naive Bayes and De-
cision Trees in terms of accuracy. Moreover, it shows that using
alargerfeaturespaceonitsownisnotenoughtogainacompet-
itive advantage as the diference between it and other baselines
isnotthatsigniicant.Onceweutilizethemulti-kernelapproach
we can observe a 2%-4%improvement in the overall model’s per-
formance ( AUCPR) over traditional RVM on both datasets, and a
3%-15% improvement with the proposed hierarchical version of
themulti-kernelRVMclassiier.Wecanalsoobservethatmostof
this improvement is due to a boost in the recall (93% increase on
Maalejand40%onPanichella).Aswe discussedearlier,thisisthe
main advantage of leveraging the existing hierarchical relationship
between labels. Breaking the prediction task into multiple levels,
wherebyintheirst,we predict(informativevs.non-informative)
and use the collective knowledge between the diferent children of
eachbranchcansigniicantlyboostthemodel’srecall, i.e.,increases
our chance ofidentifying informative reviewscorrectly.
Overall, we can observe that the proposed hierarchical multi-
kernel RVM is outperforming all the baselines as it can ofer a
boost through the combination of two aspects. First, the multi-
kernel learning technique allows it to choose the best kernel(s)
for the current learning task through the assigned weights ( e.g.,
565BayesianLearning forIntegratedClassification andSummarizationof App Revs. ESEC/FSE ’22, November14–18, 2022,Singapore, Singapore
Table 5:Summary oftheresults comparing proposed approachto the start ofthe art
ApproachMaalejDataset Panichella Dataset
AUCPRAUCROCmF1MF1MPMRAUCPRAUCROCmF1MF1MPMR
AR-miner [11]0.402 0.804 0.496 0.445 0.363 0.634 0.432 0.806 0.472 0.444 0.345 0.699
Maalej[25]0.472 0.843 0.565 0.513 0.463 0.597 0.668 0.898 0.677 0.640 0.645 0.647
ARdoc[33,34]- - 0.338 0.267 0.341 0.325 - - 0.376 0.307 0.642 0.344
RVM 0.506 0.869 0.433 0.399 0.5830.308 0.702 0.927 0.655 0.617 0.736 0.536
MK-RVM 0.516 0.870 0.421 0.377 0.502 0.304 0.729 0.930 0.685 0.652 0.7410.567
HMK-RVM 0.519 0.798 0.615 0.541 0.503 0.594 0.806 0.882 0.771 0.729 0.7090.753
meta-informationmightbemoreusefultobugreportsthanfeature
requests,leadingtoahigherweightforthecorrespondingkernel
thanotherkernels, or LDA topicsmay introducemore noise than
true signals for bug reports, hence setting the LDA kernel’s weight
to very small can improve the model’s accuracy). Second, the hi-
erarchical approach ofers a boost in the model’s recall through
leveraging existing hierarchical relationships between the difer-
entlabels.Moreover,throughthelearningprocess,theproposed
approach has identiied, on average, 45 relevant vectors (variesby
classiier/dataset).Thoserelevantvectorsshouldbethemostrepre-
sentative reviews ( i.e.,reviews that best summarize the content),
which provides us with two additional advantages beyond accu-
racy. The irst is a computational advantage, as we can limit future
training and prediction to those relevant vectors since other points
arealreadyrepresentedbythem,whichsigniicantlycutsdownthe
original dataset size. The second is a summarization advantage, as
those reviews should highlight the reviews that best summarize
the dataset,whichdevelopers can use for requirement extraction.
RQ3:Howaccurate isthesummarizationofthe
proposedHMK-RVMapproach compared to the
stateoftheart?
Building on the classiication step, which helped us identify the set
of informative reviews and ilter out the non-informative ones, the
next goal is to summarize the feedback in the set of informative
reviews for thepurpose ofrequirement extraction. We proposeto
leverage the set of relevant vectors, which HMK-RVM learns as
part of the classiication task as a way to potentially summarize
the users’ feedback.As a result, we achieve both the classiication
and summarization tasks simultaneously using the same model. In
this section, we will evaluate the set of reviews identiied as the
mostinformativebyHMK-RVMforrequirementextractionagainst
multiple baselines that were used in the literature for this purpose
orfor summarizationingeneral.
Baselines: We willuse thesetof relevantvectors identiiedby
theHMK-RVM modelpresentedinRQ2asourproposedapproach
andcompare itto the following baselines:
First,wewillcompareagainstapproachesthatwereproposed
by prior work. We will build upon the classiication experiment tofurther summarize the content of the reviews basedonthe recom-
mendationoftheoriginalauthors.For AR-Miner [11]andARdoc
[33,34],Latent Dirichlet Allocation (LDA) will be used to group the
setofreviewspredictedas informative ,andthenthereviewwith
the highest probability for each topic will be picked as the most
informativeone.Thesizeoftheinallistofselectedreviewswill
be equal to the number of topics. As for Maalej[25], where the
original authors did not propose any summarization approach, we
will apply K-meansto the set of reviews classiied as informative to
cluster them, andthen usethe reviewat the centerof eachcluster
as the most informative review. We will also compare against Star
Clustering [1],whichcreatesagraphwhereeachnodeisareview
and an edge is created if the cosine similarity between two reviews
is larger than a given alpha, and then use the set of nodes with the
highest degree to be the set of center stars ,i.e.,most informative
reviewsfor requirement extraction.
Second, for the purpose of completeness, we will use random
samplingasa baselinewherewerandomly picked npointsasthe
set of most informative reviews. Additionally, we will compare
against widely used summarization techniques such as K-means
andLatentDirichletAllocation(LDA)inthesamewaydescribed
earlierbut appliedto the complete dataset.
Tokeepthiscomparisonfair,wemadetheselectionofthehyper-
parameters, e.g.,numberoftopicsforLDA,inamannerthatpro-
vided us with a inal set of informative reviews that is equal in size
for allbaselines.
ExperimentalSetup: Toevaluatethisaspectoftheproposed
HMK-RVM approach, we asked two graduate Ph.D. students (in
computing)toreadthereviewsinthePanichelladatasetandgener-
ate a list of the requirements discussed, and then label each review
with 1) a requirement id(s), 2) a level of informativeness ranging
fromonetothree,where oneisareviewwithnorequirements, two
isareviewthatisrelevanttoarequirementbutwithoutenoughin-
formationtoextractit( e.g.,duetomissinginfo,poorreadability,or
notbeingexplicitenough, i.e.,requiringthedevelopertoguess/infer
the meaning), and threeis a review with an explicit requirement
and enough information to extract it. We show examples of this la-
belinginTable 6.Thetwostudentsannotatedthedatasetseparately
and then compared their labels. Disagreements were resolved in
566ESEC/FSE ’22, November14–18, 2022,Singapore, Singapore Moayad Alshangiti,WeishiShi,Eduardo Lima,Xumin Liu, andQi Yu
Table6:Examplesofreal-worldreviewsfromthePanichella
datasetandhowtheywerelabeledforRQ3.RequirementID
5 refersto users’ request foradditional loginoptions.
Review Req. Id Informative
Level
Blinq Okay NA 1(Low)
Login Facebook? Nope.App 5 2(Medium)
immediately deleted
FBandwithoutFBcan Blinq 5 3(High)
not work?? There mustalso
be an alternative logonoptions!
Ratio of most informative reviews (lvl 3)Ratio of most informative reviews (lvl 3)Ratio of most informative reviews (lvl 3)Ratio of most informative reviews (lvl 3)Ratio of most informative reviews (lvl 3)Ratio of most informative reviews (lvl 3)Ratio of most informative reviews (lvl 3)Ratio of most informative reviews (lvl 3)Ratio of most informative reviews (lvl 3)Ratio of most informative reviews (lvl 3)Ratio of most informative reviews (lvl 3)Ratio of most informative reviews (lvl 3)Ratio of most informative reviews (lvl 3)Ratio of most informative reviews (lvl 3)Ratio of most informative reviews (lvl 3)Ratio of most informative reviews (lvl 3)Ratio of most informative reviews (lvl 3)Ratio of most informative reviews (lvl 3)Ratio of most informative reviews (lvl 3)Ratio of most informative reviews (lvl 3)Ratio of most informative reviews (lvl 3)Ratio of least informative reviews (lvl 1)Ratio of least informative reviews (lvl 1)Ratio of least informative reviews (lvl 1)Ratio of least informative reviews (lvl 1)Ratio of least informative reviews (lvl 1)Ratio of least informative reviews (lvl 1)Ratio of least informative reviews (lvl 1)Ratio of least informative reviews (lvl 1)Ratio of least informative reviews (lvl 1)Ratio of least informative reviews (lvl 1)Ratio of least informative reviews (lvl 1)Ratio of least informative reviews (lvl 1)Ratio of least informative reviews (lvl 1)Ratio of least informative reviews (lvl 1)Ratio of least informative reviews (lvl 1)Ratio of least informative reviews (lvl 1)Ratio of least informative reviews (lvl 1)Ratio of least informative reviews (lvl 1)Ratio of least informative reviews (lvl 1)Ratio of least informative reviews (lvl 1)Ratio of least informative reviews (lvl 1)
20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%
48%48%48%48%48%48%48%48%48%48%48%48%48%48%48%48%48%48%48%48%48%76%76%76%76%76%76%76%76%76%76%76%76%76%76%76%76%76%76%76%76%76%
15%15%15%15%15%15%15%15%15%15%15%15%15%15%15%15%15%15%15%15%15%63%63%63%63%63%63%63%63%63%63%63%63%63%63%63%63%63%63%63%63%63%
24%24%24%24%24%24%24%24%24%24%24%24%24%24%24%24%24%24%24%24%24%47%47%47%47%47%47%47%47%47%47%47%47%47%47%47%47%47%47%47%47%47%
28%28%28%28%28%28%28%28%28%28%28%28%28%28%28%28%28%28%28%28%28%71%71%71%71%71%71%71%71%71%71%71%71%71%71%71%71%71%71%71%71%71%
13%13%13%13%13%13%13%13%13%13%13%13%13%13%13%13%13%13%13%13%13%72%72%72%72%72%72%72%72%72%72%72%72%72%72%72%72%72%72%72%72%72%
18%18%18%18%18%18%18%18%18%18%18%18%18%18%18%18%18%18%18%18%18%69%69%69%69%69%69%69%69%69%69%69%69%69%69%69%69%69%69%69%69%69%
17%17%17%17%17%17%17%17%17%17%17%17%17%17%17%17%17%17%17%17%17% 0%25%50%75%100%
AR−miner HMK−RVM K−meansLDA
Maalej w/ Kmeans Random SamplingStar ClusteringCoverageLevel 1 (Low) Level 2 (Medium) Level 3 (High)                                Informativeness
Figure 5: The results of the summarization evaluation. The
Y-axis represents the coverage, i.e.,percentage of require-
mentsthatwerecapturedbytheselectedsetofinformative
reviews. The higher the score the better. The color provides
a visual representation of how informative are the reviews
selectedbyeachapproach.Themoregreenandthelessred,
thebetter theapproach.
group discussions. More details on this process can be found in the
replication package1. We found a substantial inter-rater agreement
(kappa=0.87) between the annotators. It is important to know that
the reviews are sharing the same context (same app, same version)
to assume that they are discussing the same requirement, which is
why we only used the Panichella dataset for this evaluation as it
provides the app information in addition to the review, whereas,
this information ismissinginthe Maalej dataset.
We use two metrics to evaluate each approach. First, how in-
formative aretheselectedreviewsforrequirementextraction, i.e.,
were the selected reviews mostly of level two and three of informa-
tiveness (medium and high),or were they mostly level one (noise).
Second, as part of the labeling process we compiled a list of re-
quirementsthatarediscussedinthereviews,andusingthisground
truth, we want to evaluate the coverageof each approach, i.e.,how
many of the existing discussed requirements were mentioned in
theselectedset.However,wearguethatnotallrequirementsareequal.Themorementioned/discussedarequirementis,themore
valuable,andviceversa.Assuch,wemeasuredcoverageonlyfor
requirements mentioned in three or more reviews. As most ma-
chine learning models require a certain level of statistical presence
to learn patterns, two maynot be suicient to show the statistical
signiicance. Meanwhile, setting a higher threshold ( e.g.,four or
more)maymiss somemeaningfulrequirements.
ExperimentalResults: We show the evaluation results in Fig-
ure5.WecanobservethattheproposedHMK-RVMsigniicantly
outperformsallthebaselines.First,lookingat coveragewherethe
higher the score the better the model at capturing all the discussed
requirements,wecanseethatitis11%betterthanLDA(thesecond
bestmodel)androughly50%betterthanallotherbaselines.This
means it is able to select at least one review for each discussed
requirement with a much higher success rate than the state of
theart.Second,lookingatinformativeness,whichisakeyaspect
of requirement extraction, we can see that the reviews selected
byHMK-RVMhavethehighestlevelofinformativeness,andthe
least level ofnoise. HMK-RVM had 74% more informative reviews
thanthesecond-bestbaseline.Additionally,itpicked50%lessnoisy
reviews than the second-best baseline. This means that it is far
superior at picking the most informative reviews and avoiding the
least informative (noisy) reviews for requirements extraction than
the state ofthe art.
Table 7: Analyzing the model’s insight: What and the
learned weights tell usaboutthe underlying data?
Maalej Dataset ϕwe(x)ϕmeta(x)ϕtfidf(x)ϕlda(x)
Informative 0.496 0.533 0.474 0.511
Feature Request 0.489 0.504 0.522 0.521
Bug Report 0.512 0.475 0.513 0.512
User Experience 0.505 0.383 0.001 0.892
RQ4:Beyondaccuracy,whatinsightscanwegain
from usingtheproposedHMK-RVMapproach?
ExperimentalSetup: To addressthisquestion, weevaluatedthe
weightsassignedto eachof the kernels.
Experimental Results: For the irst aspect, we report the as-
signedweightsperkernel inTable 7fortheMaleejdataset.Wecan
observe that the learned weights per kernel vary between roughly
12%onaverage,whichindicatesadiferentprioritybasedonthe
learnedtask.Forexample,forthe informative classiier,kernelswith
higher representation ( i.e.,Meta and LDA) were assigned higher
weights, whichcan be due to the fact that the majority of reviews
at the irst level of classiication are non-informative reviews (70%),
mostlyratingreviews( i.e.,astrongpositiveornegativeratingwith
a short sentimental text). As such, they can be easily identiied
with a more broad view of the reviews. Also, for the feature request
andbug report classiiers, we can observe a higher assigned weight
to the TF-IDF kernel, which may be due to the fact that such re-
viewscanbeidentiiedthroughafewfrequentlyusedwordsthat
arecapturedbyTF-IDF( e.g.,add,feature,bug,crash, etc.).Finally,
theuser experience classiier shows a signiicant weight diference
567BayesianLearning forIntegratedClassification andSummarizationof App Revs. ESEC/FSE ’22, November14–18, 2022,Singapore, Singapore
between kernels. The LDA kernel and word embedding kernel are
highly utilized, whereas the TF-IDF kernel is essentially ignored.
Webelievethatthisisduetotherichandlengthynatureofsuch
reviews(reviewswithuserexperienceareverydescriptive).Having
thatnaturein mindwiththefact thatappreviewsareusually full
oftyposandalternativelyspelledwordswouldputarepresentation
thatreliesonexacttermssuchasTF-IDFatadisadvantage,whereas
a semantic capturing representation such as word embedding or
a topic capturing representation such as LDA is at a clear advan-
tage.As such,wecanconclude thatthe userexperience classiier’s
predictionsrelyheavilyontheLDAandwordembeddingrepresen-
tation,i.e.,inorderto maintain a healthy user experience classiier,
weneedtomaintainthoserepresentations.Suchinsightintothe
classiier’slearningpatterns isvaluable tothe understanding and
interpretationofthe classiier’s behavior.
5 DISCUSSION
How is the proposed approach diferent from the current
state of the art (SOTA) ? The existing SOTA approaches use a
pipeline of two dedicated models, one for classiication and an-
otherforsummarization.Thisallowsthemtoine-tuneeachmodel
for its speciic task. However, this also complicates the process
ofimplementationandmaintainability.Incontrast,ourproposed
approach is designed to achieve both the classiication and sum-
marization tasks using a single model. Although we do not have
the option to ine-tune the results for each task, we still demon-
strated that we were able to provide equal or better results on one
task(i.e.,classiication)andoutperformallbaselinesontheother
(i.e.,summarization), which shows that we did not compromise
on the accuracy when we attempted to merge the two tasks. In
fact, the summarization aspect is the most important aspect for
requirements extraction, in which our approach outperforms all
baselinesinbyalarge margin.
Howdoestheproposedapproachimprovetheextractionof
requirements?Howisthistested ?Thekeyimprovementliesin
theamountofefortthattheproposedapproachcanreduceinterms
ofthehumanefortforrequirementsextraction.Tomeasurethis
aspect, we introduced two metrics, coverage and informative level.
Fortheformer,ahighcoverageimpliesthatanalyzingthemodel-
identiiedsubsetofreviewswouldallowthedevelopertoextract
mostrequirements.Thesavingofefortisachievedastherestof
the reviews can be safely ignored. As for the latter, reviews with
ahigherlevelofinformativenesscanhelpdevelopersmoreeasily
and accurately extract the requirements without cross-checking
other reviews. In our experiment, the set of informative reviews
constitutesaround35%-40%oftheentiredatasetwhereastheset
ofrepresentativereviewsthatHMK-RVMidentiiedincludesonly
around 5% of the dataset. This implies that by using HMK-RVM,
we can efectively reduce the human efort needed to extract the
requirementsfrommanuallyanalyzing35%-40%ofthedatasetto
only 5%. Inaddition,the reviewsidentiiedbyHMK-RVMareofa
high level of informativeness, which can improve the easiness and
accuracyofrequirement extractionfrom thesereviews.
What are the limitations of the approach? One limitation is
that RVM tends to pick from highly representative regions as aresultofmaximizingthemodelevidenceinEq.4.Whileitishighly
desirable to choose a small number of reviews to represent the
whole set, it may also miss some requirements from less represen-
tative regions. Our results show that HMK-RVM achieved a 70%
coveragebyjustusingasmallnumberofrepresentativereviews,
which clearly demonstrates its efectiveness. An interesting future
direction is to augment RVM’s learning process to include a few
reviewsfromlessrepresentativeregionstoenhancethecoverage
further.Anotherlimitationisthataswegodownthehierarchy,we
areexpectedtohavelessdatawhichmayafecttheperformance.
Thus, anotherinterestingdirection isto studytheefect ofadding
more hierarchical levels onthe performance of the model.
6 THREATS TO VALIDITY
Intermsof internal validity,themainthreatisthatweusedtwo
datasetsinourexperimentcomingfrompreviouswork.Wedidnot
participate in the collection or preparation of those datasets. Thus,
any issues with the reviews content or the labels are a potential
riskfactor.The Maalejdataset providedboththereviewsandthe
labels,whereas,the Panichelladataset providedonlythereviews.
As such, we had to manually label the reviews ourselves for the
Panichella dataset . In both cases, whether the ground truth was
handedtous,orwhetherwemanuallylabeledthereviews,there
istheriskofhumancodersmistakes.Toreducethisthreattoour
labels, we created a coding guide that precisely deines the app
reviewtypeswithanexampleofeach,andweemployedtwoteams
each with two members to label the dataset separately. Once both
teamscompletedtheir task, we satdown and extensivelydiscussed
anydisagreements.Intermsof external validity,webelieveour
results should have high generalizability for app reviews as we
evaluateditontwodiferentreal-worlddatasetsthatwerecarefully
constructed, i.e., sampled randomly from diferent apps and app
stores. As such,they shouldprovide a reasonable approximation of
the generalpopulation.
7 CONCLUSION
In this paper,we proposedHierarchical Multi-Kernel RVM (HMK-
RVM) where we extended and customized the use of RVM in a
novel way to facilitate requirement extraction from app reviews
by ofering an integrated process that is easier to implement, inter-
pret,andmaintain.Theproposedapproachclassiies reviewsina
hierarchical fashion, leading to a more accurate model. In addition,
we showed that the assigned weights to each kernel can provide
an insight into what the classiier has learned from the underlying
data. Moreover, we leveraged RVM’s inner working mechanism
toaccomplishthesummarizationtaskaspartoftheclassiication
learningprocess,andwehavedemonstrateditsabilitytooutper-
form the state of the art in terms of summarization accuracy while
achievingacompetitive classiication accuracy.
ACKNOWLEDGMENTS
ThisresearchwassupportedinpartbyanNSFIISawardIIS-1814450
and an ONR award N00014-18-1- 2875. The views and conclusions
contained in this paper are those of the authors and should not be
interpretedas representing any fundingagency.
568ESEC/FSE ’22, November14–18, 2022,Singapore, Singapore Moayad Alshangiti,WeishiShi,Eduardo Lima,Xumin Liu, andQi Yu
REFERENCES
[1]Javed A. Aslam, Katya Pelekhov, and Daniela Rus. 1998. Static and Dynamic
Information Organizationwith Star Clusters.In CIKM. ACM,208ś217.
[2]Kartik Bajaj, Karthik Pattabiraman, and Ali Mesbah. 2014. Mining questions
askedbywebdevelopers.In Proceedingsofthe11thWorkingConferenceonMining
SoftwareRepositories, MSR . ACM,112ś121.
[3]Anton Barua, Stephen W. Thomas, and Ahmed E. Hassan. 2014. What are
developers talking about? An analysis of topics and trends in Stack Overlow.
EmpiricalSoftwareEngineering 19,3 (2014), 619ś654.
[4]Andrew Begel and Thomas Zimmermann. 2014. Analyze this! 145 questions for
datascientistsinsoftwareengineering.In Proceedingsofthe36thInternational
Conference onSoftwareEngineering, ICSE . ACM,12ś23.
[5]DimitriPBertsekas.1997. Nonlinearprogramming. JournaloftheOperational
Research Society 48,3 (1997), 334ś334.
[6]Christopher M Bishop and Nasser M Nasrabadi. 2006. Pattern recognition and
machinelearning . Vol. 4. Springer.
[7]David M. Blei, Andrew Y. Ng, and Michael I. Jordan. 2003. Latent Dirichlet
Allocation. Journal ofMachineLearning Research 3 (2003), 993ś1022.
[8]Piotr Bojanowski, Edouard Grave, Armand Joulin, and Tomas Mikolov. 2017.
Enriching Word Vectors with Subword Information. TACL5 (2017), 135ś146.
[9]LauraV.GalvisCarreñoandKristinaWinbladh.2013. Analysisofusercomments:
an approach for software requirements evolution. In Proceedings of the 35th
InternationalConferenceonSoftwareEngineering,ICSE .IEEEComputerSociety,
582ś591.
[10]Eya Ben Charrada. 2016. Which One to Read? Factors Inluencing the Use-
fulness of Online Reviews for RE. In Proceedings of the 24th IEEE International
Requirements Engineering Conference, RE . IEEE Computer Society, 46ś52.
[11]Ning Chen, JialiuLin, Steven C. H. Hoi, XiaokuiXiao,and Boshen Zhang. 2014.
AR-miner: mining informative reviews for developers from mobile app market-
place. InProceedings of the 36th International Conference on Software Engineering,
ICSE. ACM,767ś778.
[12]BinFu,JialiuLin,LeiLi,ChristosFaloutsos,JasonI.Hong,andNormanM.Sadeh.
2013. Why people hate your app: making sense of user feedback in a mobile
app store. In Proceedings of the 19th ACM SIGKDD International Conference on
KnowledgeDiscoveryand Data Mining,KDD . ACM,1276ś1284.
[13]CuiyunGao,JichuanZeng,DavidLo,Chin-YewLin,MichaelR.Lyu,andIrwin
King.2018. INFAR:insightextractionfromappreviews.In Proceedingsofthe2018
ACM Joint Meeting on European Software Engineering Conference and Symposium
onthe Foundations of Software Engineering,ESEC/SIGSOFTFSE2018,Lake Buena
Vista,FL,USA, November 04-09, 2018 . ACM,904ś907.
[14]GiovanniGrano,AndreaDiSorbo,FrancescoMercaldo,CorradoAaronVisag-
gio,GerardoCanfora,andSebastianoPanichella.2017. Androidappsanduser
feedback: a datasetfor softwareevolutionandquality improvement.In Proceed-
ingsofthe2ndACMSIGSOFTInternationalWorkshoponAppMarketAnalytics,
WAMA@ESEC/SIGSOFT FSE 2017, Paderborn, Germany, September 5, 2017 . ACM,
8ś11.
[15]Xiaodong Gu and Sunghun Kim. 2015. "What Parts of Your Apps are Loved
byUsers?" (T).In Proceedingsof the30thIEEE/ACMInternationalConference on
AutomatedSoftwareEngineering, ASE . IEEE Computer Society, 760ś770.
[16]Emitza Guzman and Walid Maalej. 2014. How Do Users Like This Feature? A
Fine Grained Sentiment Analysis of App Reviews. In Proceedings of the IEEE
22nd International Requirements Engineering Conference, RE , Tony Gorschek and
Robyn R. Lutz (Eds.).IEEE Computer Society, 153ś162.
[17]Elizabeth Ha and David A. Wagner. 2013. Do Android users write about electric
sheep?ExaminingconsumerreviewsinGooglePlay.In Proceedingsofthe10th
IEEEConsumerCommunicationsandNetworkingConference,CCNC .IEEE,149ś
157.
[18]MarkHarman,YueJia,andYuanyuanZhang.2012.Appstoreminingandanalysis:
MSR for app stores. In Proceedings of the 9th IEEE Working Conference of Mining
SoftwareRepositories, MSR . IEEE Computer Society, 108ś111.
[19]Minqing Hu and Bing Liu. 2004. Mining and summarizing customer reviews. In
ProceedingsoftheTenthACMInternationalConferenceonKnowledgeDiscovery
and DataMining,SIGKDD . ACM,168ś177.
[20]Claudia Iacob and Rachel Harrison. 2013. Retrieving and analyzing mobile
apps feature requests from online reviews. In Proceedings of the 10th Working
ConferenceonMiningSoftwareRepositories,MSR .IEEEComputerSociety,41ś44.
[21]Hammad Khalid, Emad Shihab, Meiyappan Nagappan, and Ahmed E. Hassan.
2015. WhatDoMobileAppUsersComplainAbout? IEEESoftware 32,3(2015),
70ś77.
[22]Daphne Koller and Mehran Sahami. 1997. Hierarchically Classifying Documents
Using Very Few Words. In Proceedings of the Fourteenth International Conference
onMachineLearning (ICML . Morgan Kaufmann,170ś178.
[23]Johannes V. Lochter, Pedro R. Pires, Carlos Bossolani, Akebo Yamakami, and
TiagoA.Almeida.2018. Evaluatingtheimpactofcorporausedtotraindistributed
textrepresentationmodelsfornoisyandshorttexts.In Proceedingsofthe2018
InternationalJoint Conference onNeural Networks,IJCNN . IEEE,1ś8.[24]WalidMaalej,ZijadKurtanovic,HadeerNabil,andChristophStanik.2016. On
the automatic classiication of appreviews. Requir. Eng. 21,3 (2016), 311ś331.
[25]Walid Maalej and Hadeer Nabil. 2015. Bug report, feature request, or simply
praise? On automatically classifying app reviews. In Proceedings of the 23rd IEEE
InternationalRequirementsEngineering Conference, RE . IEEE ComputerSociety,
116ś125.
[26]Christopher D. Manning, Prabhakar Raghavan, and Hinrich Schütze. 2008. Intro-
duction toInformationRetrieval .
[27]StuartMcIlroy,WeiyiShang,NasirAli,andAhmedE.Hassan.2017. Userreviews
of top mobile apps in Apple and Google app stores. Commun. ACM 60, 11 (2017),
62ś67.
[28]Tomas Mikolov, Kai Chen, Greg Corrado, and Jefrey Dean. 2013. Eicient
Estimation of Word Representations in Vector Space. In ICLR (Workshop Poster) .
[29]AzadNaikandHuzefaRangwala.2018. LargeScaleHierarchicalClassiication:
State ofthe Art . Springer.
[30]Kamal Nigam, Andrew McCallum, Sebastian Thrun, and Tom M. Mitchell. 2000.
Text Classiication from Labeled and Unlabeled Documents using EM. Machine
Learning 39,2/3 (2000), 103ś134.
[31]DennisPaganoandWalidMaalej.2013. Userfeedbackintheappstore:Anempir-
ical study. In Proceedings of the 21st IEEE International Requirements Engineering
Conference,RE . IEEE Computer Society, 125ś134.
[32]Fabio Palomba, Pasquale Salza, Adelina Ciurumelea, Sebastiano Panichella, Har-
aldC. Gall,Filomena Ferrucci,andAndrea DeLucia.2017. Recommendingand
localizing change requests for mobile apps based on user reviews. In Proceedings
ofthe39thInternationalConferenceonSoftwareEngineering,ICSE .IEEE/ACM,
106ś117.
[33]Sebastiano Panichella, Andrea Di Sorbo, Emitza Guzman, Corrado Aaron Vis-
aggio,GerardoCanfora,andHaraldC.Gall.2015. Howcaniimprovemyapp?
Classifying user reviews for software maintenance and evolution. In Proceedings
of the 2015 IEEE International Conference on Software Maintenance and Evolution,
ICSME. IEEE Computer Society, 281ś290.
[34]Sebastiano Panichella, Andrea Di Sorbo, Emitza Guzman, Corrado Aaron Visag-
gio,GerardoCanfora,andHaraldC.Gall.2016. ARdoc:appreviewsdevelopment
oriented classiier. In Proceedings of the 24th ACM SIGSOFT International Sympo-
sium onFoundationsofSoftwareEngineering, FSE . ACM,1023ś1027.
[35]Dae Hoon Park, Mengwen Liu, ChengXiang Zhai, and Haohong Wang. 2015.
Leveraging User Reviews to Improve Accuracy for Mobile App Retrieval. In
Proceedings of the 38th International ACM SIGIR Conference on Research and
Development in Information Retrieval, Santiago, Chile, August 9-13, 2015 . ACM,
533ś542.
[36]JefreyPennington,RichardSocher,andChristopherD.Manning.2014. Glove:
Global Vectors for Word Representation.In EMNLP. ACL,1532ś1543.
[37]AnandRajaramanandJefreyDavidUllman.2011. MiningofMassiveDatasets .
CambridgeUniversityPress,NewYork, NY, USA.
[38]ChristoferRosenandEmadShihab.2016. Whataremobiledevelopersasking
about? A large scale study using stack overlow. Empirical Software Engineering
21,3 (2016), 1192ś1223.
[39]Dwaipayan Roy, Debasis Ganguly, Sumit Bhatia, Srikanta Bedathur, and Mandar
Mitra. 2018. Using Word Embeddings for Information Retrieval: How Collection
andTermNormalization ChoicesAfectPerformance.In Proceedingsofthe 27th
ACM International Conference onInformation and Knowledge Management, CIKM .
ACM,1835ś1838.
[40]Andrea Di Sorbo, Sebastiano Panichella, Carol V. Alexandru, Junji Shimagaki,
CorradoAaronVisaggio,GerardoCanfora,andHaraldC.Gall.2016. Whatwould
users change in my app? summarizing app reviews for recommending software
changes. In Proceedings of the 24th ACM SIGSOFT International Symposium on
FoundationsofSoftwareEngineering, FSE . ACM,499ś510.
[41]Andrea Di Sorbo, Sebastiano Panichella, Carol V. Alexandru, Corrado Aaron
Visaggio,andGerardoCanfora.2017. SURF:summarizerofuserreviewsfeedback.
InProceedings of the 39th International Conference on Software Engineering, ICSE .
IEEE Computer Society, 55ś58.
[42]MichaelE.Tipping.1999. TheRelevanceVectorMachine.In AdvancesinNeu-
ralInformationProcessingSystems12,[NIPSConference,Denver,Colorado,USA,
November 29 -December 4, 1999] . The MITPress,652ś658.
[43]Lorenzo Villarroel, Gabriele Bavota,Barbara Russo, Rocco Oliveto, andMassim-
iliano Di Penta. 2016. Release planning of mobile apps based on user reviews.
InProceedings of the 38th International Conference on Software Engineering, ICSE .
ACM,14ś24.
[44]PhongMinhVu, TamTheNguyen, Hung VietPham, andTungThanh Nguyen.
2015. MiningUserOpinionsinMobileAppReviews:AKeyword-BasedApproach
(T).InProceedingsof the30thIEEE/ACMInternationalConferenceonAutomated
SoftwareEngineering, ASE . IEEE Computer Society, 749ś759.
[45]PhongMinhVu, Hung VietPham, TamTheNguyen, andTungThanh Nguyen.
2016. Phrase-based extraction of user opinions in mobile app reviews. In Pro-
ceedingsofthe31stIEEE/ACMInternationalConferenceonAutomatedSoftware
Engineering, ASE . ACM,726ś731.
569