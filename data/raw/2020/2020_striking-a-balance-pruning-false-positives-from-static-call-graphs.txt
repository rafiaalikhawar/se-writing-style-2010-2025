Striking a Balance:
Pruning False-Positives from Static Call Graphs
Akshay Utture
University of California, Los Angeles
U.S.A.
akshayutture@ucla.eduShuyang Liu
University of California, Los Angeles
U.S.A.
sliu44@cs.ucla.edu
Christian Gram Kalhauge
DTU
Denmark
chrg@dtu.dkJens Palsberg
University of California, Los Angeles
U.S.A.
palsberg@ucla.edu
ABSTRACT
Researchershave reportedthat staticanalysis toolsrarely achieve
a false-positive rate that would make them attractive to developers.
Weovercomethisproblembyatechniquethatleadstoreporting
fewer bugs but also much fewer false positives. Our technique
prunes the static call graph that sits at the core of many staticanalyses. Specifically, static call-graph construction proceeds as
usual, after which a call-graph pruner removes many false-positive
edges but few true edges. The challenge is to strike a balance be-tween being aggressive in removing false-positive edges but notso aggressive that no true edges remain. We achieve this goal by
automatically producing a call-graph pruner through an automatic,ahead-of-timelearningprocess.Weaddedsuchacall-graphprunertoasoftwaretoolfornull-pointeranalysisandfoundthatthefalse-
positive rate decreased from 73% to 23%. This improvement makes
the tool more useful to developers.
CCS CONCEPTS
•Softwareanditsengineering →Automatedstaticanalysis ;
•Computing methodologies →Supervisedlearning byclassifi-
cation.
ACM Reference Format:
AkshayUtture,ShuyangLiu,ChristianGramKalhauge,andJensPalsberg.
2022.StrikingaBalance:PruningFalse-PositivesfromStaticCallGraphs.
In44th International Conference on Software Engineering (ICSE ’22), May
21–29, 2022, Pittsburgh, PA, USA. ACM, New York, NY, USA, 13 pages. https:
//doi.org/10.1145/3510003.3510166
1 INTRODUCTION
The Problem. Christakis and Bird [ 14] interviewed developers
about program analysis tools and they concluded:
Programanalysisdesignshouldaimforafalse-
positive rate no higher than 15–20%.
This work is licensed under a Creative Commons Attribution International 4.0 
License.
ICSE ’22, May 21–29, 2022, Pittsburgh, PA, USA
© 2022 Copyright held by the owner/author(s).
ACM ISBN 978-1-4503-9221-1/22/05.
https://doi.org/10.1145/3510003.3510166Otherempiricalstudieshavefoundsimilarresults[ 6,25,40].Un-
til now, this goal has been particularly hard to achieve for static
analyses, which are tools that analyze programs without executing
them.
As a motivating experiment, we tried Wala [ 47], which is one of
thebesttoolsforstaticanalysisofJavabytecode,onasubsetofthe
NJR-1 benchmark suite [ 35]. For each benchmark, we compared
theedgesinthestaticcallgraphwiththeedgesfoundbyexecuting
the benchmark. With a context-insensitive analysis, Wala has a
false-positive rate of 76%, while with a better but also much slower
context-sensitive analysis, the false-positive rate is 70%. Those re-
sults are disappointing though we must emphasize that call graphs
are usually fed to client tools rather than directly to developers. So,
we did a second experiment to see how the high false-positive rate
of call-graphs affects client tools. Specifically, we implemented a
versionofastaticanalysisforwarningaboutnull-pointerproblems
[21] that is a client of the context-insensitive call graphs produced
byWala.WeranthistoolonthesamesubsetofNJR-1andagain
haddisappointingresults:60bugsamong223warnings,onaver-
age, so a false-positive rate of 73%. We can easily imagine how a
developerwilltireofinvestigatingwarningsthatinnearlythree
ofeveryfourcasesarefalsealarms.Thefalsealarmshaveseveral
causes, but an important cause is the high false-positive rate in the
underlyingstaticcallgraph.Hence,wecanalsoseeaglimmerof
hope: if we can reduce the false-positive rate of static call-graph
constructors,wemaybeabletomoveclienttoolsclosertothegoal
of a false-positive rate of 15–20%.
OurIdea. OurapproachstemsfromanotherconclusionbyChris-
takis and Bird [14] who reported a preference of developers:
When forced to choose between more bugs orfewer false positives, they typically choose the
latter.
Thisquoteinspiredourideaforho wtoimprove thefalse-positive
rate: we will report fewerbugs but also much fewer false positives.
Indirectsupportforthisideacomesfrompreviousworkthatshowedthatpracticalstaticanalysesaren’ttotallysound[
31,43]andthere-
fore may miss bugs. Thus, developers expect bug reports to be
incomplete so reporting fewer bugs seems acceptable.
We want to reduce the false-positive rate in a modular way that
leavesexistingcall-graphconstructorsunchanged.Thisbringsus
to our idea of a call-graph pruner that statically post-processes a
20432022 IEEE/ACM 44th International Conference on Software Engineering (ICSE)
ICSE ’22, May 21–29, 2022, Pittsburgh, PA, USA Akshay Utture, Shuyang Liu, Christian Gram Kalhauge, and Jens Palsberg
static call graph by removing manyfalse-positive edges but few
true edges. The challenge is to strike a balance between being
aggressive in removing false-positive edges but no so aggressive
that no true edges remain. Additionally, we have to do better than
removingedgesatrandombecauserandomremovalswillleavethe
false-positive rate unchanged.
How can we design a call-graph pruner?
OurApproach. Weexecuteanautomatic,ahead-of-timelearning
process on results from both a static and a dynamic call-graph
constructor. The outcome is a call-graph pruner that works as
follows.Thecall-graphprunerdeterminestheprobabilitythatan
edge in the call graph is a false positive, and if this probability is
above a threshold, then the call-graph pruner removes the edge.
We can vary this threshold and thereby tune the call-graph pruner.
In contrast to previous work on using a dynamic analysis to
improve a static analysis [ 3,13,16], we use the dynamic call-graph
constructor only in an ahead-of-time training phase and only on a
trainingsetofprograms.Oncethetrainingphasehasproduceda
call-graph pruner, the combination of the call-graph constructor
and the call-graph pruner is itself a static analysis, as illustrated in
Figure 1.
Our Contributions and the Rest of the Paper. We begin with an
example of how a call-graph pruner works (Section 2) and then we
detail our contributions:
•We present the design (Section 3) and implementation (Sec-
tion 4) of a tool that produces call-graph pruners.
•Weshowexperimentally(Section5)thataddingacall-graph
pruner to a client tool can significantly decrease the false-
positiverate,inonecasefrom73%to23%.Specifically,we
added a call-graph pruner to the tool for warning about
null-pointer problems, after which we got 15 bugs among
20 warnings, on average. Thus we reported 45 fewer bugs
but also 158 fewer false positives.
•We show experimentally (Section 5) that the overhead of
adding a call-graph pruner is 18% of the original call-graph
analysis time.
Weendwithadiscussionofrelatedwork(Section6)andourcon-
clusion (Section 7).
Significance. Call-graphprunersimprovestaticcall-graphssig-
nificantly and thereby make client tools more useful to developers.
2 EXAMPLE
Now we give an example of a call-graph pruner, how it works
on a example call graph, and how it affects a client analysis for
warning about null-pointer problems. Our example program in
Figure 2, shown in full in the Appendix, has three classes A,B,C,
each of which has a method foo, and a main method that contains
amethodcall x.foo(x.f) .Thecallto getObjC() returnsanobject
of type C, which is then assigned to the variable x. On the next
line, the access x.fhappens, but the field A.fmay be uninitialized
hencenull.Thusthecall x.foo(x.f) maypassnullasanargument
toC.foo, which, in turn, at the call c.toString() , may throw a
NullPointerException.Theprogramhastwoadditionalmethods,
including getObjC, that we omitted from Figure 2.Program Call-graph
construction
tool
call-graph
prunerCall-graph
False-positive rate: 76%
True-edges missed: 5%
Call-graph
False-positive rate: 34%
True-edges missed: 34%Balanced call-graph
construction toolnew
Figure 1: Overview of our technique
...
A x = getObjC();
x.foo(x.f);
class A {
    A f;
    foo(A a){
       a.toString();
 }}class B extends A {
    foo(A b){
       b.toString();
    }
 }class C extends B {
    foo(A c){
       c.toString();
    }
 }DECISION TREE
dest-node-in-deg > 2.5
src-node-out-deg > 2.5 10%
55%
70%STATIC-ANALYSIS CALL-GRAPH
70%T
TF
F
dest-node-in-deg > 1.5
40%FT40%10%
Figure 2: Example call graph and call-graph pruner
Null-Pointer Warnings. As we mentioned in Section 1, we im-
plemented a version of a static analysis for warning about null-
pointer problems. This analysis finds null-pointer problems that
stemfromuninitializedfields,liketheproblemwith c.toString()
thatiscausedbytheuninitializedfield A.f.Ifwerunthistoolon
theexampleprogram,weget threewarnings,oneforeachcallof
toString inthe foomethods.Oneofthemisatruewarningbut
the other two are false alarms. Let us investigate how that could
happen and what a call-graph pruner can do about it.
Call Graph. The null-pointer tool uses a static call-graph con-
structor that built the call graph shown in Figure 2. In a call graph,
eachnodeisamethod,andeachedgeisadirectededgefromone
method to another. Such an edge represents a call that may happen
during the execution of the program.
The call-graph constructor uses a data-flow analaysis to analyze
the entire program, including the methods that we omitted from
Figure2.Weskipthedetailsofhowthisworksandinsteadwefocus
on the constructed call graph. Specifically, in Figure 2 we focus on
thefournodesforthemainmethod, A.foo,B.foo,and C.foo.The
call graph has an edge from the main method to each of A.foo,
B.foo, and C.foo, as well as an edge some other method to B.foo
andacoupleofedgesfromsomeothermethodsto A.foo.Theedge
from main to C.foois a true edge, while the edges from main to
A.fooand from main to B.fooare false positives.
Thefalsecall-graphedgesfrommaintoeachof A.fooandB.foo
can arise from difficult-to-analyze methods, one of which is part of
the full example program in the appendix.
2044Striking a Balance:
Pruning False-Positives from Static Call GraphsICSE ’22, May 21–29, 2022, Pittsburgh, PA, USA
TheNull-PointerAnalysisinmoreDetail. Basedonthecallgraph
in Figure 2, the null-pointer analysis derives that x.foo(x.f) may
callanyof A.foo,B.foo,and C.foo.Thenthenull-pointeranalysis
uses the rule that
if a field is not initialized by the end of a con-
structor, it is marked as Uninitialized ; and if an
Uninitialized field is dereferenced, the analysis
gives a null-pointer warning.
Thus, the analysis concludes that each of the foomethods may be
passed null as an argument, and thus it issues a warning for every
one of those methods.
Call-Graph Pruner. The goal of a call-graph pruner is to remove
edges from the call-graph, preferably many false-positive edges
andfewtrueedges.Thekeycomponentofacall-graphpruneris
aclassifierthatcomputestheprobabilitythatacall-graphedgeis
atrue-positive.Basedonthatprobability,acall-graphprunerwill
decide whether to keep or to remove the edge. Figure 2 shows a
classifierthatisrepresentedasadecisiontree.Eachinternalnodeof
the decision tree asks a true-false question about a call-graph edge.
The recursive decision process begins in the root of the decision
tree;iftheanswertothequestionattherootisfalse,wemoveto
the left subtree, while if the answer is true, we move to the right
subtree. When we reach a leaf, we find the probability that the
call-graphedgeisatrue-positive.Theprobabilitiescomputedfor
eachcall-graphedgeinthisfashionaremarkedonthecallgraph
in Figure 2. Based on these probabilities, we will decide whether to
keep or remove the call-graph edge.
ThedecisiontreeinFigure2hasthreeinternalnodesthatarela-
beledwithquestionsabout dest-node-in-deg ,whichisthein-degree
of the destination node of the edge, and about src-node-out-deg ,
whichistheout-degreeofthesourcenodeoftheedge.Forexam-
ple,theedgefrommainto C.foohasdestination-nodein-degree
1andsource-nodeout-degree3.Thisgivesusthepathfalse-true-
false, which assigns the edge the probability 70%. Similarly, the
edges from main to A.fooandB.fooget probabilities 10% and
40%, respectively. The call graph in Figure 2 shows those three
probabilities.
Let us set a threshold of 50% for when we deem an edge to be
a false-positive: ifthe probability of being a true-positiveis below
50%, we removethe edge. Then the call-graphpruner will remove
theedgesfrommainto A.fooandB.foo.Hence,thenull-pointer
analysiswillissuejustasinglewarning,andindeedatruewarning,
namely for the call of toString inC.foo.
3 CALL-GRAPH PRUNERS
Nowwedescribehowweusemachinelearningtoproduceacall-
graph pruner.
3.1 Overview
We will use Program to denote the set of Java bytecode programs.
Acallgraph G∈CallGraph isamulti-graphinwhicheachnode
represents a method and each edge represents a potential transfer
of control at a method call. Two nodes can have multiple edges
betweenthembecauseofmultiplemethodcalls.Eachedgehasa
label that identifies the method call site.We distinguish between two kinds of call-graph constructors
that have the same type:
StaticCallGraphConstructor =Program →CallGraph
DynamicCallGraphConstructor =Program →CallGraph
Here, anelement of StaticCallGraphConstructor constructs acall
graph without running the program, while, in contrast, an element
ofDynamicCallGraphConstructor runsaninstrumentedversion
of the program on one or more inputs and examines the output
from the instrumentation.
Thekeycomponentofeachcall-graphprunerisaclassifier.A
classifier C∈Classifier is afunction that maps avector of feature
valuesforanedgetoaprobabilitythattheedgeisatrue-positive.
In our case, such a vector has 11 elements that we will define in
Section 3.3.
Our tool for generating classifiers implements a function of this
type:
classifier generator :(StaticCallGraphConstructor ×
DynamicCallGraphConstructor ×
Set[Program] )
→Classifier
Ourclassifiergenerator executesanautomatic,ahead-of-timelearn-
ing process on results from running both a static and a dynamic
call-graph constructor on a training set of programs. The dynamic
call graphs serve as ground-truth for the learning process. We will
detail this learning process in Section 3.2.
Oncewehaveaclassifier,wecanuseitinacall-graphprunerof
this type:
call-graph pruner :
(CallGraph ×Classifier ×Threshold )→CallGraph
Algorithm 1 shows how a call-graph pruner works. Intuitively, a
call-graph pruner uses a classifier to determine the probability that
anedgeinastaticcallgraphisatrue-positive.Ifthatprobability
is below a given threshold T∈Threshold , the call-graph pruner
removes the edge.
Algorithm 1: Call-graph Pruner
1Inputs:CallGraph G,Classifier C,Threshold T
2letG/primebeac o p yo f G
3forevery edge einGdo
4v= the feature values for e
5ifC(v)<Tthen
6 removeefromG/prime
7OutputG/prime
Thethresholdparameterenablesustoexploredifferentlevelsof
aggressiveness in removing edges. For our example in Figure 2, we
discussedathresholdof50%inSection2,whichledtotheremovalof
twoedges.Wecouldalsousealowerthresholdof20%,whichwould
lead to the removal of a single edge, namely the one from main
toA.foo. The challenge is to strike a balance between removing
manyfalse-positiveedgesandkeepingmanytrue-positiveedges.In
2045ICSE ’22, May 21–29, 2022, Pittsburgh, PA, USA Akshay Utture, Shuyang Liu, Christian Gram Kalhauge, and Jens Palsberg
Edge f1... fkLabel
e1 10 ... 0.3 1
e2 8 ... 0.7 0
... ... ... ... ...Training Programs
Program-1 Program-n
Compute Features
Concatenate into single Training Set
Trained
ClassifierTrain using Classification AlgorithmStatic 
call-graphDynamic 
call-graph
Edge f1... fkLabel
e3 7 ... 0.1 0
e4 1 ... 0.6 1
... ... ... ... ...Compute FeaturesStatic
call-graphDynamic 
call-graph
Figure 3: Classifier Generator workflow
Section 5 we will show results from an experimental investigation
of how to choose a good threshold.
Noticethatweuseastaticcallgraphconstructor,adynamiccall
graph constructor, and the training set of programs for the sole
purpose of generating a classifier, while those items are no longer
needed when we use the call-graph pruner.
3.2 Our Classifier Generator
Wecasttheedge-pruningproblemasaclassificationproblemfor
which learning a classifier can be done with machine learning. We
proceed in three steps.
Inthefirststep,werunexistingstaticanddynamiccall-graph
constructor tools on every program in the training set (the dataset
of programs is described inSection 4). Theresult isa set ofpairs of
call graphs: each pair consists of a static call graph and a dynamic
callgraph. Weuse thedynamiccall graphasan approximationof
the ground truth: if a static call-graph edge is also present in the
dynamic call graph, we view it as a true-positive, and otherwise as
a false-positive.
In the second step, for each program, we construct a table in
which each row represents a static-call-graph edge. Figure 3 il-
lustrates this table. The last column in each row (titled Labelin
Figure 3) contains a label of 1 or 0, based on whether the edge
exists in the dynamic call graph. The remaining columns (titled
f1tofk)representthesetof featuresofthestaticcall-graphedge.
The example in Figure 2 uses two features: dest-node-in-deg and
src-node-out-deg ;wewilldiscussotherfeaturesbelow.Wecanview
each row in the table as a vector of feature-values. Concatenating
thetablesofeachindividualprogramgivesusasinglelargetraining
dataset of call-graph edges with ground truth labels. This training
dataset consists of a large number of pairs (xe,ye), wherexeis a
vectoroffeaturevaluescorrespondingtoastaticcall-graphedge,andyeisapredictionofwhetheritisafalse-positiveornot.Our
problem is now expressed in a format where it can be cast as a
machine-learning classification problem [28].
Inthethirdstepwerunanoff-the-shelfmachine-learningtool
on the table constructed in second step. The result is a classifier
that for any edge assigns a probability that it is a true-positive.
We picked random forests [19] (ensembles of Decision Trees). One
might try other approaches, which we leave to future work. Our
goalwiththisstepistoshowthatanoff-the-shelfmachine-learning
tool is sufficient to get good results.
Ourclassifiergeneratorcantakeanystaticcall-graphconstructor
as input. For example, we have used the call-graph constructors
WALA [47], Doop [9], and Petablox [ 33] as inputs and generated a
call-graph pruner for each one.
The complexity of generating a classifier based on a training set
withnedges isO(nlogn)[19].
3.3 Our Feature set
Now we describe how we designed the feature set that both our
classifier generator and our generated call-graph pruners use.
Afeatureis information about a static-call-graph edge that may
help predict whether the edge is a true-positive. We would like our
feature set to capture important context and semantic information
about a call-graph edge. Encoding important semantic information
asfeaturesisacommonmachinelearningpracticeforincorporating
domain knowledge into the learning process. For example, since
dynamic dispatch is likely to affect the false-positive probability of
a call-graph edge, we should add features that capture information
aboutthetargetsofamethodcall.Usingthecontextinformationof
agraphedgehasbeenusefulfortherelatedtaskofselectivecontext
andheapsensitivityinpointer-analysis[ 23],andweconsiderita
goodcriteriaforpickingfeatures.Contextinformationcanbelocal
bydescribingtheneighborhoodoftheedge,orglobalbydescribing
the call graph that the edge is a part of. In addition to capturing
context and semantic features, we identify three criteria that we
want our feature set to satisfy:
(1) linear-time computation complexity,
(2) interpretable and generalizable, and
(3) black-box.
The time-complexity guideline is particularly important given that
someofourbenchmarkscanhaveseveralhundredthousandcall-
graphedges.Interpretabilitygivesusanunderstandingofwhich
call-graph edges are being dropped, and generalizability ensures
thatwhatislearnedforthetrainingedgesalsoappliestocall-graph
edgesofunseenprograms.Theblack-boxcriterionimpliesthatthe
features should only be designed on the output call graph, and not
onsomeinternalstateorrepresentationofatool.Thisallowsus
to post-process the results without being specific to a particular
algorithm or tool. Using these criteria, we arrived at the following
features for an edge.
Figure4presentsourfeaturesetforanedgeinastaticcallgraph
G, where the edge is from a caller method callerto a callee method
callee. The node for the main method in Gismain. The first seven
features describe local information while the last four describe
globalinformation.NotethattheL-fanoutofanedgeisthenumber
ofoutgoingedgesatthecall-siteofthatparticularedge,whereas
2046Striking a Balance:
Pruning False-Positives from Static Call GraphsICSE ’22, May 21–29, 2022, Pittsburgh, PA, USA
Feature Description
src-node-in-deg number of edges ending in caller
src-node-out-deg number of edges out of caller
dest-node-in-deg number of edges ending in callee
dest-node-out-deg number of edges out of callee
depth length of shortest path from maintocaller
repeated-edges number of edges from callertocallee
L-fanout number of edges from the same call-site
node-count number of nodes in G
edge-count number of edges in G
avg-degree average src-node-out-deg in G
avg-L-fanout average L-fanout value in G
Figure 4: Our feature set
src-node-out-deg is the number of outgoing edges from all the
call-sites of an entire source method.
Our selection process started with a much longer list of features
that allsatisfy thethree criterialisted above. Wepicked fromthat
list the ones that helped the most with removing false-positives.
Ourprocessusedthetrainingsetascasestudiestofindthemain
reasons why tools give false positives. The result was the eleven
features in Figure 4.
4 IMPLEMENTATION AND DATASET
Static Call-Graph Constructors. We used the static call-graph
constructors WALA [ 47], Doop [9], and Petablox [ 33]. In each case
we used the default setting, which implements 0-CFA for meth-
odsthatareestimatedtobereachablefromthemainmethodand
without any special handling of reflection. Those tools produce
significantly different call graphs and so we generate a separate
call-graph pruner for each tool.
Reflection. Inpreliminaryexperiments,wefoundthatenabling
specialhandling ofreflection inthe staticcall-graph constructors
introduces many false-positive edges in the call graphs. Our gener-
ated classifiers tend to assign each of those edges a low probability
of beinga true-positive, andtherefore our call-graphpruners will
correctlyremovemostofthem.Therefore,specialhandlingofre-
flectionpresentsnoadditionalchallengeforcall-graphpruningand
wedecidedtogowiththedefaultsettingofeachstaticcall-graph
constructor.
Dynamic Call-Graph Constructor. We used the open-source tool
Wiretap[ 26]toinstrumenttheJavabytecodeandtherebyenable
dynamic call-graph construction. Next, we ran the instrumented
bytecodeandcollecteddataabouttherun,particularlyaboutthe
method calls.
Standard Library. The Java standard library is large and has the
potential to dominate the measurements for every benchmark,
whichiscounterproductive.So,whenwedoourmeasurementsand
training, we omit nodes from the standard library as well as edges
between standard library nodes. We preserve aspects of the edges
to and from the standard library in the following way. For every
path of the form
v→/angbracketleft... standard library nodes ... /angbracketright→wFigure5:HistogramofEdge-countsinthe100TrainingPro-
grams.
wherev,warenodesoutsidethestandardlibrary,wecreateasingle
edge from vtow.
RandomForestClassifier. OurclassifiergeneratorusestheRan-
dom Forest algorithm [ 19] implemented with the Scikit-Learn [ 36]
library (v0.21.3). The Random Forest algorithm works as follows: it
trains several decision-trees using Bagging [ 10], and makes predic-
tions by a “majority vote” across the decision trees. The training
took 4 minutes. We tuned the hyper-parameters using Random
Hyper-Parameter Search [ 5] with 4-fold cross-validation on the
training set. We list the chosen hyper-parameters in the appendix.
Dataset.Our dataset consists of 141 programs from the NJR-1
benchmark suite [ 35], of which we used 100 programs for gener-
atingthreecall-graph prunersandtheremaining41programs for
ourevaluation.Weselectedthose141programsfromthe293NJR-1
programs according to the following criteria:
•consistsatleast1,000methodsandatleast2,000staticcall-
graph edges according to Wala,
•executes at least 100 distinct methods at runtime, and
•has high coverage: executes a large percentage of the meth-
odsthatarereachablefromthemainmethodaccordingto
Wala; for our benchmarks, the coverage is 68%, on average.
Each program consists of 560,000 lines of code, on average (not
countingthestandardlibrary).Inmoredetail,eachprogramconsists
of the main application, which is 8,000 lines of code, on average,
in addition to third-party libraries which account for an estimated
552,000 lines of code, on average.
The total number of static-call-graph edges (not counting the
standard library) that are reachable from the main methods of the
141 programs is 1.3 million. For our classifier generator, each edge
from 100 of those programs is a data point, which is 860,000 edges.
Note that manual creation of ground truth about those 860,000
edges infeasible.
Large Benchmarks. The histogram in Figure 5 gives the distri-
butionoftheedgecountsinthetrainingprograms.TheX-axisis
plottedonalogarithmicscaleduetotheskewinthedistribution.
Amongthe100trainingprograms,7ofthemhaveaverylargenum-
ber of call-graph edges (> 20,000). This gives them the potential to
dominate how the classifiers work. To overcome this, we randomly
sample20,000edgesfromtheedge-setsofthese7programs.Notice
that this sampling is done only during generation of call-graph
2047ICSE ’22, May 21–29, 2022, Pittsburgh, PA, USA Akshay Utture, Shuyang Liu, Christian Gram Kalhauge, and Jens Palsberg
pruners,whileweusealltheedgesfromthe41programsthatwe
use for evaluation.
Analysis Time. Running the three static call-graph constructors
andthedynamiccall-graphconstructoronalltheprogramstakes
four days of compute time.
PrecisionandRecall. Weestimatethequalityofastaticcallgraph
using the standard notions of precisionandrecall. In our setting, if
Sis the edge set produced by a static call-graph constructor, and
Wis the edge set produced by Wiretap, then:
Precision =|S∩W|
|S|Recall =|S∩W|
|W|
Therateoffalse-positivesis (1−Precision).Wecomputetheaverage
precision and recall values for the entire test-set by taking the
arithmetic mean over the precision and recall values of individual
programs.
Figure6showsahistogramoftheoriginalprecisionandrecall
scoresforWALAonthe41individualprogramsofthetestset.Note
thattheprecisionvaluesvarysignificantly,butalmostallprograms
getbelow40%precision.Hence,thereisalotofscopeforimproving
the precision. The recall is close to 100% for most programs, but
low for some due to heavy use of reflection, dynamic class-loading
or native code.
5 EXPERIMENTAL RESULTS
In this section, we discuss our experimental results that validate
the following claims.
(1) Our generated call-graph pruners for WALA, Doop, and
Petablox produce call graphs with balanced 66% precision
and 66% recall.
(2)Forprecision-sensitiveclients,ourgeneratedcall-graphpruners
are significantly better at boosting precision than context-
sensitive analyses, and have a much smaller overhead.
(3)Theprecision improvementisconsistentacross thetestset.
(4)Thecall-graphprunerenablesamonomorphiccall-siteclient
tobalanceitsskewed52%precisionand93%recalltoamore
balanced 68% precision and 68% recall.
(5)The call-graphpruner enables anull-pointer analysis tore-
duceitsaveragewarningcountfrom223to20,whileincreas-
ing precision from 27% to 77%.
All experiments are run on a separate test set of 41 programs
whichwerenotusedduringtraining.Theexperimentswerecarried
outonamachinewith24Intel(R)Xeon(R)Silver4116CPUcores
at 2.10GHz and 188 Gb RAM. A minimum RAM size of 32Gb is
essential for ensuring that the static analyses run in reasonable
time. The artifact for the paper is available here [ 46] and the NJR-1
dataset can be downloaded from [45].
5.1 Main Result
Figure 8 gives the main result of the paper: a call-graph pruner can
be successfully used to boost precision and to balance the goals of
precision and recall for the 0-CFA call-graph analysis of WALA,
Doop and Petablox. The plot is used to represent the precision
andrecallvaluesofvarioustools,whereinallprecisionandrecall
values are reported as averages over the test-set programs. Theblack triangle marks the WALA 0-CFA analysis (23.8% Precision,
95.3% Recall), the green triangle marks the Doop 0-CFA analysis
(23.1% Precision, 92.6% Recall) and the blue triangle marks the
Petablox 0-CFA analysis (29.8% Precision, 88.8% Recall). They all
haveclosetoperfectrecall,butpoorprecision.Theredplussign
marks the WALA 1-CFA analysis (29.6%. 95.4%). The black curve
representstheprecision-recalltrade-offpointsobtainedwhenacall-
graphprunerisappliedtotheWALA0-CFAoutput.Theoriginal
WALA-0CFA output is a single point on the precision-recall graph,
but the call-graph pruner gives a curve instead. This is because the
call-graphprunergivesaprobabilityscoreforeachedgebeingin
the ground-truth call-graph, and by setting different thresholds (i.e.
cutoffsbelowwhichanedgeisremoved),wecanobtaindifferent
pointsontheprecision-recallcurve.Joiningallthesedifferentpoints
gives us the black curve in the figure. Setting a low-probability
thresholdforacceptinganedge,givesuspointsneartheleftend
ofthe blackcurve,because weaccept alargepercentage ofedges,
thereby giving us higher recall but lower precision. Setting a high-
probabilitythresholdgivesuspointsneartherightendofthecurve
because we accept only very few edges which are very likely to be
intheground-truthcall-graph,andthisgivesushigh-precisionand
lowrecall.Thegreenandbluecurvesrepresenttheprecision-recall
trade-offobtainedbyapplyingthecall-graphprunertotheDoop
andPetabloxcall-graphsrespectively,andthecaseisverysimilar
to the black WALA curve.
Thesecurveswhichtrade-offrecallforprecisionshowthatthe
classifier has assigned probabilities meaningfully. In contrast, a
tool that randomly assigns probabilities to edges would result in
acurvethatgoesstraightdowntozerorecallwithoutimproving
any precision. This is because it results in a random removal of
edges, which keeps the ratio of true-positives (i.e. precision) the
same. Boosting precision requires the ratio of false-positive edges
in the removed edge set to be higher than the rest of the edges.
There are 2 particularly interesting points on the black (WALA)
curve in Figure 8. The first is the one marked by the black (WALA)
square (66.0% Precision, 66.0% Recall), which represents the point
with balancedprecision and recall.Such a pointwill beuseful to
a precision-sensitiveclient analysis.Ascompared tothe original
WALA 0-CFA (black-triangle), this point has over 72% of the edges
fromtheoriginalcall-graphremoved,andoutoftheremovededges,
lessthan10%aretruepositives.Thispointisata0.45probability
threshold.SimilarpointsforDoopandPetablox,markedbyagreen
square (hidden behind the black square) and blue square (also hid-
denbehindtheblacksquare)respectively,areat(66.2%Precision,
66.2% Recall) and (66.4% Precision, 66.4% Recall) respectively. A
second interesting point is the right-most point on the curve after
which recall starts dropping faster, represented by a black circle
(50% Precision, 92% Recall). Such a point would be useful for a
client analysis that needs to increase a little precision, without los-
ingmuch recall.Similar pointsfor DoopandPetablox aremarked
by the green circle (50% Precision, 88% Recall) and blue circle (50%
Precision, 87% Recall) respectively.
Both these points give larger precision boosts than the 1-CFA
analysis. However, in general, the best precision-recall trade-off
pointisdecidedbytheneedsoftheclientofthecallgraph.Precision-
sensitive clients would benefit more from our call-graph pruner
2048Striking a Balance:
Pruning False-Positives from Static Call GraphsICSE ’22, May 21–29, 2022, Pittsburgh, PA, USA
Figure 6: Precision and recall for 41 test programs.Figure 7: Precision and recall after call-graph pruning.
Figure 8: Main Result for the WALA, Doop and Petablox
staticanalysistools.Thebaselineprecision-recallvaluesfor
the3tools,alongwiththeprecision-recallcurveobtainedaf-
ter applying a call-graph pruner (averaged over all test pro-
grams)
since it gives a larger precision boost, but clients that need high
recall may prefer the 1-CFA call graph.Our call-graph pruner adds an overhead of 18% to the WALA
0-CFA analysis, whereas moving to a 1-CFA analysis adds 292%
overhead.Priorresearchalsofindsthatcontext-sensitivityincreases
analysis time by many folds [30].
For completeness, we also ran this experiment for WALA’s RTA
implementation and it gets similar results (that we show in the
supplementarymaterial).Sincethethreetoolsshowsimilarcharac-
teristics, we only present numbers for the WALA 0-CFA call graph
in the rest of this section. The corresponding graphs for Doop and
Petablox are available in the supplementary material.
Picking a Cutoff value. We picked the balanced precision-recall
point because it gave good results for a null-pointer analysis client,
but different precision-recall trade-off points may be suitable for
differentclientanalyses.Figure9helpsauserpicktherighttrade-off
point for their client. It plots the probability cutoff values on the X-
axis, and the Precision, Recall and F-score on the Y-axis. The graph
shows what values each of these metrics takes at every probability
cutoff value, as well as what the expected cutoff would be for a
giventargetPrecision,RecallorF-score.Forexample,bylookingat
the figure, we can say that to obtain an expected Precision of 60%,
we can set a cutoff value of 0.4. At this point we would get a Recall
ofapproximately75%andF-scoreofaround65%.Thisgraphalso
shows that the balanced precision-recall point is also very close to
the point with maximum F-score.
FeatureImportance. Figure10givestheimpurity-basedimpor-
tance[42]foreachfeatureusedintherandom-forestindescending
order.The L-fanoutanddest-node-in-deg arethemostimportantfea-
turesandthefourglobalfeaturesaretheleastimportant.Dropping
2049ICSE ’22, May 21–29, 2022, Pittsburgh, PA, USA Akshay Utture, Shuyang Liu, Christian Gram Kalhauge, and Jens Palsberg
Figure 9: Probability cutoff plotted vs Precision, Recall and
F-score curves for WALA
Feature Importance
L-fanout 0.182
dest-node-in-deg 0.114
src-node-out-deg 0.094
repeated-edges 0.092
src-node-out-deg 0.090
depth 0.084
dest-node-out-deg 0.079
node-count 0.071
edge-count 0.067
avg-L-fanout 0.036
avg-degree 0.028
Figure10:ImportanceofeachfeatureintheRandomForest
Classifier in descending order.
thefourglobalfeaturesdecreasestheareaundertheprecision-recall
curve from Figure 8 by 6%.
Human-Interpretable Explanation of the Classifiers. We can give
a human-interpretable explanation of the main aspects of the Ran-
dom Forest classifiers that were learned in the experiment. In each
case, the top-level decisions center around the following generic
classifier:
if ((L-fanout >m)∧(dest-node-in-deg >n)) then 0 else 1
Theaboveexpressionsaysthatifanedgehas L-fanoutgreaterthan
manddestination-nodein-degree greaterthan n,thentheprobabilityFigure 11: Historgram of Percentage Improvement in Preci-
sion scores for individual programs.
that it is a true edge is 0, and otherwise 1. For each of the static
call-graph constructors, we can identify the constants mandn:
WALA:
if ((L-fanout >3.5)∧(dest-node-in-deg >9.5)) then 0 else 1
Doop:
if ((L-fanout >3.5)∧(dest-node-in-deg >16.5)) then 0 else 1
Petablox:
if ((L-fanout >3.5)∧(dest-node-in-deg >20.5)) then 0 else 1
The orange cross (49% precision, 92% recall) in Figure 8 gives
theprecision-recalltrade-offwhenusingthegenericclassifierfor
WALA. This generic classifier has a slightly worse trade-off and
is much less tunable than the black line (WALA with call-graph
pruner). However, its pruning rules are also much simpler and
easily understandable. The use of L-fanoutanddest-node-in-deg in
the generic classifier aligns with the fact that these are the most
important features according to Figure 10.
5.2 Distribution of Precision and Recall for
individual programs
Figure 7 gives a histogram of the precision and recall scores of
individual programs when a call-graph pruner is used to prune the
WALA call graph at the balanced precision-recall point (marked by
the black square in Figure 8). Most of the programs get at least 50%
precision,andaseveralevenreachthe70%precisiongoal.Contrast
thistotheprecisioninFigure6wherealmostallprogramsfailto
cross the 40% precision point.
Asexpected,therecallscoresfromFigure7droppedascompared
to Figure 6. However, most programs still get at least 50% recall,
implyingthattheyretainagoodportionoftheirtrueedges.Note
that it is impossible to improve recall using a call-graph pruner
since it cannot find new edges that WALA did not find.
ThehistogramfromFigure11illustratesthepercentageimprove-
mentinprecisionscores.TheX-axisisplottedonalogarithmicscale.
By using a call-graph pruner, 30 out of the 41 programs have their
precision score boosted by at least 2 times their original precision
score.Allbut2programshavetheirprecisionscoreboostedbyat
least20%.Nobenchmarkgetsaworseprecision.Thus,asignificant
majority of the individual programs consistently get a large pre-
cisionimprovementwithoutloosingtoomuchrecall,andachieve
a better precision-recall balance. The Doop and Petablox graphs
havesimilarcharacteristicsandareshowninthesupplementary
material.
2050Striking a Balance:
Pruning False-Positives from Static Call GraphsICSE ’22, May 21–29, 2022, Pittsburgh, PA, USA
Call-graph tool Precision Recall
WALA 0-CFA 51.8% 92.6%
WALA 0-CFA + call-graph pruner 67.7% 68.4%
Figure 12: Impact of improved call-graph precision on a
monomorphic call-sites client
ID Warnings True-Positives in a sample of 10
Before After Before After
B1 137 12 2 10
B2 365 31 4 5
B3 190 15 2 8
B4 308 44 7 10
B5 204 16 0 10
B6 429 42 0 7
B7 404 136 7 10
B8 70 10 0 0
B9 231 10 0 9
B10 102 34 5 8
Average 2.7 7.7
Figure13:Totalwarningcountsandamanualclassification
of a sample of 10 warnings for the null-pointer analysis be-
fore and after applying a call-graph pruner
5.3 Effect on Client Analyses
Next, we look at the effect of improved call-graph precision on the
monomorphic call-site detection and null-pointer analysis clients.
Monomorphic call-site client. This client is based on the WALA-
generated0-CFAcallgraph,anditusesthedynamicanalysisasthe
ground-truth.Figure12givetheprecisionandrecallofamonomor-
phic call-site client with and without the call-graph pruner. The
call-graph pruner helps the client boost precision from 52% to 68%
and balance its goals of precision and recall.
Applications of the monomorphic call-sites client include devir-
tualizationandinlining.Sincethecall-graphanalysisisneversound
in practice [ 31], these applications require some safety checks, re-
sulting in overheads. For example, if devirtualization is used for
optimization,run-timechecksneedtobeinsertedtoensurecorrect-
ness[22].Higherprecisionforthemonomorphiccall-sitesclient
implies that more of the call-sites declared monomorphic by the
static analysis actually turn out monomorphic in the ground-truth.
Thisinturnimpliesthatwheneverweincurtheoverheadofinlining
or devirtualization, we are also more likely to realize its benefits.
Null pointer analysis. This analysis is based on the paper by Hu-
bertetal.[ 21].ItisimplementedinWALA,andisusedtofindnull-
pointer errors originating from uninitialized instance fields. The
analysis is context-insensitive, field-insensitive and flow-sensitive.
It only reports potential null-pointer dereferences in application
code, and not for the standard library.
The original WALA call graph gives us, on average, 223 null
pointerwarningsperprogram.Thehighvolumeofwarningsmakes
itcumbersomefordeveloperstomanuallyinspectandinpractice
thisresultsindevelopersignoringthetooloutputentirely[ 6,25].Using the call graphs produced after pruning gives us much fewer
(on average 20 per program) warnings.
Two of the authors manually inspected a random sample of
10 null-pointer warnings from 10 of the 41 test programs when
usedwithandwithoutthecall-graphpruner.The10programswere
chosenwiththecriteriathattheyhadatleast10warningsbothwith
and without the call-graph pruner, and the ratio of warnings with
and without the call-graph pruner was close to (20/223). Figure 13
givesthetotalwarningcountsaswellasthetrue-positivecounts
(from a sample of 10 warnings) for each of these 10 programs. The
useofacall-graphprunerhelpedthenull-pointeranalysisimprove
its precision from 27% to 77%
Thecriteriaformarkingawarningasatrue-positivewasthat
the author could trace the backward slice of a dereference to an
instance fieldwhich wasuninitialized bythe endof a constructor.
Warningsthateithercouldnotbeverifiedin10minutes,raninto
another exception before triggering the null exception, or other-
wiseunverifiablebytheauthors,wereconsideredasfalse-positives.
Reachability from the main method was not considered because it
is hard to verify manually.
We leave to future work to try other clients, including other
approaches to null-pointer analysis such as NullAway [4].
5.4 Threats to Validity
The first threat is the use of a dynamic analysis as a proxy for
thecall-graphgroundtruth.Itassumesgoodcoverageofthetrue
ground-truthcall-graphandaffectstheprecision-recallcalculations.
Ifthedynamicanalysishadhighercoverage,moreofthestaticanal-
ysisedgeswouldbeinthedynamiccall-graph.Asaconsequence,
both the baseline precision scores as well as the pruned-call-graph
precision scores would be higher. In contrast, we expect the recall
scorestoremainsimilar.However,improvingdynamicanalysiscov-
erage is a non-trivial and orthogonal problem and any techniques
improving coverage will automatically improve our technique and
evaluation.Symbolicexecution[ 27]isoneoptiontoimprovecover-
age,butitdoesn’tscaletothesizeofourprograms.Instead,weusea
subsetoftheNJR-1benchmarksetwhichgetsgoodcoverage.Note
that this threat does not affect the evaluation of the null-pointer
analysis.
Thesecondthreatisthemanualinspectionofthenull-pointer
warnings,whicharevulnerabletohumanerrors.Theauthorsin-
spectingtheerrorshavealimitedfamiliaritywiththecode-bases
of the examined program. This could lead to misclassification of
both true and false errors, and affect the precision score accord-
ingly.Further,the precisionscoresarereportedfor asampleof10
programs.
Thethirdthreattovalidityisthegeneralizabilityoftheresults
to programs outside the NJR dataset. Our assumption is that our
learningandevaluationresultsgeneralizetootherprogramsoutside
the dataset.
The fourth threat to validity is that programs in the training
set and evaluation set share some third-party libraries. On average
(geometricmean),3.6percentofthemethodsofaprograminthe
evaluationsetalsooccurinsometrainingprogram.Webelievethat
thisoverlapislowenoughtonotsignificantlyaffecttheconclusions
of our evaluation.
2051ICSE ’22, May 21–29, 2022, Pittsburgh, PA, USA Akshay Utture, Shuyang Liu, Christian Gram Kalhauge, and Jens Palsberg
6 RELATED WORK
Ourtechniqueisthefirsttoapplymachinelearningtoboostcall-
graphprecision.Inourdiscussionofrelatedwork,wefocusonthree
areas:combiningstaticanddynamicanalyses,applyingmachine
learningtoremovestatic-analysisfalse-positives,andimproving
the precision of call-graph construction.
Combining static and dynamic analysis. Prior research has used
a dynamic analysis to improve the precision of a static analysis.
Grechet.al[ 16]generatedynamicheapinformationandusethis
as a drop-in replacement for the heap modeling part in an existing
static analysis tool to improve its precision. Artzi et. al [ 3] use a
dynamic analysis to confirm the mutability information computed
by a static analysis.Chen et. al [ 13] use the informationfrom test-
executionstoprioritizethealarmsgivenbyastaticanalysis.The
main drawback that these tools face is that they need the dynamic
analysis to be run every single time the tool is run. In contrast, our
technique needs the dynamic analysis only for generating a call-
graph pruner. After that, a call-graph pruner is purely a static tool,
andhencedoesnotsufferfromtheusualdrawbacksofadynamic
analysis like long execution times or finding good inputs.
Applyingmachinelearningtoimprovestatic-analysisbyremov-
ingfalse-positives. Thetechniqueoffilteringstatic-analysisfalse-
positives by casting it to a classification problem with hand-picked
featureshasbeenusedforstaticbug-analysistools[ 15,18,39,44,
49].Eachof theseworksfollowsthe sameworkflow:collectstatic
analysiserror-reports,getaprogrammertolabelthemastrueor
false-positives, design a feature-set for the error reports, and then
train a classifier on these labeled error-reports to identify false-
positives.However,theyhaveminordifferencesamongthemselves
in terms of the feature-set chosen, the bug-reporting tool used and
the benchmarks used forthe training data. Ruthruff et. al[39] use
the FindBugs [ 20] bug-reporting tool and the set of Java programs
at Google as their dataset. Heckman and Williams [ 18] also use
FindBugs reported bugs on 2 open-source Java projects. Yuksel
andSozer[ 49]classifybug-alertsforadigitalTVsoftware.Flynn
etal.[15]combinethebug-alertsfrommultipletools,inaddition
to using the hand-picked features. Tripp et. al [ 44] work with a
JavaScriptsecuritychecker’swarningsfrompopularWebsitesas
its dataset.
Our work differs in three ways: it uses an estimate of ground-
truthproducedbydynamicanalysis,ithasageneralizableapproach
to picking afeature set, and ithas a tunable precision-recalltrade-
off, as we discuss next.
The key bottleneck faced by each of these prior works was that
theyreliedonthecollectionofhuman-labeledground-truth,which
does not scale. This restricted their dataset to a handful of projects
and a couple of thousand data-points (bug reports) at best. In fact,
foreachtypeoferror,thereistypicallylessthanafewhundredbugs
in each of the datasets. In contrast, our technique uses an estimate
of ground-truth produced by dynamic analysis, which allows it to
scale to a much larger number of programs with a million data
points (call-graph edges).
Thesecondmajordifferenceisinthechoiceofthefeature-set.
This is partly a consequence of the fact that the previous work
focuses on static-analysis error report data, which is different fromthegraphoutputgeneratedbycall-graphconstructiontools.Hence
some of the common features used in these works are the bug-
priority level, file-modification-frequency, coding-style metrics,
and lexical features (like method or package names). These fea-
tures, though appropriate, violate generalizability and black-box
guidingprincipleslistedinSection3.3.Non-black-boxfeatureslike
bug-priority level will not generalize across different tools or al-
gorithms,andnon-generalizablefeatureslikelexicalfeaturesare
unlikely to generalize to programs outside the dataset. In contrast,
we use a systematic approach to selecting features, as described in
Section 3.3, and as a consequence, our approach generalizes eas-
ily across multiple programs and multiple call-graph construction
tools.
The third difference is that these prior works, except for [ 44],
provide a single precision-recall point. [ 44] provide eight differ-
entprecision-recallpoints,byvaryingtheclassifierused.Instead,
our approach has a tunable precision-recall trade-off by predict-
ing edge-probabilities and pruning edges with probability lower
thanathreshold.Further,weonlyuseasingleclassifier(Random
Forests) sinceitachieves superiorprecision-recall trade-offsthan
the classifiers used in [44].
Another areathat uses machine learningfor filtering falsepos-
itive is the work by Raghothaman et al. [ 37]. They predict the
probabilitiesofstatic-analysisalarmsusingBayesianinferenceand
updatetheseastheuserresolvesalarmsastrueorfalsepositives.
Thisparadigmofonlinelearning,wherethemodelislearnedand
improved as the user gives feedback, is quite different from our
fully-automatedofflinelearningparadigm,wherewedoaone-time
trainingonalargedatasetofstaticanddynamicanalysisoutputs
and require no user input.
Recently data-driven techniques have also been used to selec-
tively apply context- and flow-sensitivity [12, 24] to methods that
willbenefititthemost.Thesetechniquescanpotentiallyprovide
the precision improvement of a 1-CFA at a lower overhead, but as
seen in Figure 8, this improvement is still much lower than what is
achieved by our call-graph pruner.
Improving the precision of call-graph construction. Lhotak [29]
designed an interactive tool to qualitatively understand the root
cause of differences between different static and dynamic analysis
tools. This is then used in a case study to understand the main
cause of imprecision in a static analysis tool as compared to its
correspondingdynamicanalysisoutput.Incontrast,ourclassifier
generator is fully automated, using machine learning, and doesn’t
require a skilled programmer to use an interactive tool to figure
out the cause of the imprecision.
Sawin and Rountev [ 41] propose certain heuristics to deal with
dynamic features like reflection, dynamic class loading and native
method-callsin Java,which helpsto improvecall-graphprecision
of the CHA algorithm without sacrificing much recall. Similarly,
a call-graph pruner trades of a little recall for a large boost in
precision,butitachievesthisthroughautomatedmachinelearning
on a dataset of call graphs instead, and is able to boost precision
by a much larger amount. Additionally, we work with a 0-CFA
baseline (with no handling of dynamic features like reflection),
which already has a large precision gain over a CHA algorithm
with reflection handling.
2052Striking a Balance:
Pruning False-Positives from Static Call GraphsICSE ’22, May 21–29, 2022, Pittsburgh, PA, USA
ZhangandRyder[ 50]createpreciseapplication-onlycallgraphs
byidentifyingwhichedgesfromthestandardlibrarytotheapplica-
tionarereallyfalse-positive.Thisissimilartotheprecisionboost
we gain for the edges that go via the standard library. However,
wegenerateaclassifierthatlearnsthisonitsownfromdata,and
we use the classifier in a call-graph pruner that is able to boost
precision even further.
The patent by Reif et. al [ 32] uses probabilities to quantify anal-
ysis imprecision. Each analysis constraint is assigned a probability
heuristically or via user configuration, and the probabilities for
call-graphedges arederived fromthese usingatype-propagation
graph. In contrast, our call-graph pruner learns all its edge prob-
abilities from data about static and dynamic call-graphs. Further,
while their technique calls for a new static analysis, our call-graph
pruner works as a black-box post-processor for existing call-graph
construction tools.
MoredistantlyrelatedistheworkbyBlackshearet.al[ 8],which
prunescontrol-flowedgesrepresentinginterleavingsbetweenevents
in an event-driven system. This pruning task is different from our
task which focuses on pruning call-graphs edges for sequential
code.
There has alsobeen prior work that usesa dynamic analysis to
evaluate call-graph related static analysis tools [ 1,11,16,38,43].
Our tool additionally uses the dynamic analysis results as training
labels to prune the result from a static call-graph construction tool.
7 CONCLUSION AND FUTURE WORK
Ourapproachtogeneratingahigh-precisioncallgraphfirstruns
anexistingblack-boxcall-graphconstructorandthenprunesthe
resulting call graph. A call-graph pruner uses a classifier, which is
trainedonalargenumberofstaticanddynamiccallgraphs,topre-
dicttheprobabilityofanedgebeingatrue-positive.Usingdifferent
thresholds for the edge probabilities we can tune the precision-
recall trade-off of the call graph. We empirically showed how a
call-graphprunercanbeusedtoboostprecisionandbalancethe
recallandprecisionofcallgraphsproducedbyWALA,Doopand
Petablox, which are otherwise skewed towards high recall and low
precision. We also ran a null-pointer analysis and a monomorphic
call-sites analysis with these pruned call graphs, and we showed
that they got much closer to the high-precision expectations of
their users.
Futureworkincludesautomaticallylearningafeature-setforuse
byourprunergeneratorandourgeneratedcall-graphpruners.A
particularly promising avenue for future work is to explore graph
neuralnetworksforautomaticfeature-learning.Recentworkhas
used graph neural networks [ 17] for program analysis tasks like
program similarity [ 34], variable misuse prediction [ 2,48] variable
nameprediction[ 2],andmethodnameprediction[ 48].Thefeatures
thatarediscoveredinthosepapersarenotfeaturesofcallgraphs
and hence this remains an open problem.
A second futuredirection could be to replace dynamic-analysis
ground-truth labeling with developer-labeling for call-graph edges.
Thechallengehereisthatthecumulativenumberofedgesinthe
training dataset is nearly a million, and developer-labeling doesn’t
scale to such a large dataset.Athirdfuturedirectioncouldbetoadaptourtechniquetoheap-
reachability queries [7].
ACKNOWLEDGMENTS
This work was supported by the U.S. NSF Award 1823360, and the
ONRAwardN00014-18-1-2037.WethankAishwaryaSivaraman,
AaronBerdyandAshwinDharneformanyhelpfuldiscussions.We
also thank the ICSE’22 reviewers fortheir constructive comments
that helped improve the paper.
APPENDIX
The example in Figure 2 is an excerpt of from the program that
Figure 14 shows in full.
Our classifier generator uses the Random Forest algorithm [ 19]
implemented with the Scikit-Learn [ 36] library (v0.21.3). We tuned
the hyper-parameters using Random Hyper-Parameter Search [ 5].
The score for which we optimized was the area under the precision-
recall curve and Figure 15 lists the chosen hyper-parameters.
class A{
Af ;
void foo(A a){
a.toString();
}
}
class B extends A{
void foo(A b){
b.toString();
}
}
class C extends B{
void foo(A c){
c.toString();
}
}public class Main{
static A id(A a){
new A().foo(a);
return a;
}
static A getObjC(){
new A().foo(new A());
new B().foo(new A());
A p = id(new A());
A q = id(new B());
A r = id(new C());
return r;
}
public static void main(
String[] args){
A x = getObjC();
x.foo(x.f);
x.f = new A();
}
}
Figure 14: Program for the example in Section 2
Hyperparameter Value
Number of Trees 1000
Maximum Depth 10
Bootstrapping False
Minimum samples for split 2
Maximum features for split sqrt(feature count)
Minimum samples for leaf 1
Split quality criterion Entropy
Other hyper-parameters Library default
Figure 15: Hyper-parameters for Random-Forests
2053ICSE ’22, May 21–29, 2022, Pittsburgh, PA, USA Akshay Utture, Shuyang Liu, Christian Gram Kalhauge, and Jens Palsberg
REFERENCES
[1]Karim Ali and Ondřej Lhoták. 2012. Application-Only Call Graph Construction.
InECOOP2012–Object-OrientedProgramming ,JamesNoble(Ed.).SpringerBerlin
Heidelberg, Berlin, Heidelberg, 688–712.
[2]MiltiadisAllamanis,MarcBrockschmidt,andMahmoudKhademi.2018. Learning
to Represent Programs with Graphs. In 6th International Conference on Learning
Representations,ICLR2018,Vancouver,BC,Canada,April30-May3,2018,Con-
ferenceTrackProceedings .OpenReview.net. https://openreview.net/forum?id=
BJOFETxR-
[3]Shay Artzi, Adam Kiezun, David Glasser, and Michael D. Ernst. 2007. Combined
Static and Dynamic Mutability Analysis. In Proceedings of the Twenty-Second
IEEE/ACM International Conference on Automated Software Engineering (Atlanta,
Georgia,USA) (ASE’07).AssociationforComputingMachinery,NewYork,NY,
USA, 104–113. https://doi.org/10.1145/1321631.1321649
[4]Subarno Banerjee, Lazaro Clapp, and Manu Sridharan. 2019. NullAway: Prac-
tical Type-Based Null Safety for Java. In Proceedings of the 2019 27th ACM
Joint Meeting on European Software Engineering Conference and Symposium
on the Foundations of Software Engineering (Tallinn, Estonia) (ESEC/FSE 2019) .
Association for Computing Machinery, New York, NY, USA, 740–750. https:
//doi.org/10.1145/3338906.3338919
[5]JamesBergstraandYoshuaBengio.2012. RandomSearchforHyper-parameter
Optimization. J. Mach. Learn. Res. 13, 1 (Feb. 2012), 281–305. http://dl.acm.org/
citation.cfm?id=2503308.2188395
[6]AlBessey,KenBlock,BenChelf,AndyChou,BryanFulton,SethHallem,Charles
Henri-Gros, Asya Kamsky, Scott McPeak, and Dawson Engler. 2010. A Few
BillionLines ofCode Later:UsingStatic AnalysistoFind Bugsinthe RealWorld.
Commun.ACM 53,2(Feb.2010),66–75. https://doi.org/10.1145/1646353.1646374
[7]Sam Blackshear, Bor-Yuh Evan Chang, and Manu Sridharan. 2013. Thresher:
PreciseRefutationsforHeapReachability.In Proceedingsofthe34thACMSIG-
PLAN Conference on Programming Language Design and Implementation (Seattle,
Washington, USA) (PLDI ’13) . Association for Computing Machinery, New York,
NY, USA, 275–286. https://doi.org/10.1145/2491956.2462186
[8]Sam Blackshear, Bor-Yuh Evan Chang, and Manu Sridharan. 2015. Selective
Control-Flow Abstraction via Jumping. In Proceedings of the 2015 ACM SIGPLAN
InternationalConferenceonObject-OrientedProgramming,Systems,Languages,
and Applications (Pittsburgh, PA, USA) (OOPSLA 2015) . Association for Comput-
ingMachinery,NewYork,NY,USA,163–182. https://doi.org/10.1145/2814270.
2814293
[9]Martin Bravenboer and Yannis Smaragdakis. 2009. Strictly Declarative Spec-
ification of Sophisticated Points-to Analyses. In Proceedings of the 24th ACM
SIGPLAN Conference on Object Oriented Programming Systems Languages and
Applications (Orlando,Florida,USA) (OOPSLA’09) .ACM,NewYork,NY,USA,
243–262. https://doi.org/10.1145/1640089.1640108
[10]LeoBreiman.1996. Baggingpredictors. MachineLearning 24,2(01Aug1996),
123–140. https://doi.org/10.1007/BF00058655
[11]RaymondP.L.BuseandWestleyWeimer.2009. TheRoadNotTaken:Estimating
Path Execution Frequency Statically. In Proceedings of the 31st International
Conference on Software Engineering (ICSE ’09) . IEEE Computer Society, USA,
144–154. https://doi.org/10.1109/ICSE.2009.5070516
[12]SooyoungCha,SehunJeong,andHakjooOh.2018. Ascalablelearningalgorithm
fordata-drivenprogramanalysis. InformationandSoftwareTechnology 104(2018),
1–13. https://doi.org/10.1016/j.infsof.2018.07.002
[13]Tianyi Chen, Kihong Heo, and Mukund Raghothaman. 2021. Boosting Static
Analysis Accuracy with Instrumented Test Executions. In Proceedings of the 29th
ACM Joint Meeting on European Software Engineering Conference and Symposium
on the Foundations of Software Engineering (Athens, Greece) (ESEC/FSE 2021) .
AssociationforComputingMachinery,NewYork,NY,USA,1154–1165. https:
//doi.org/10.1145/3468264.3468626
[14]Maria Christakis and Christian Bird. 2016. What Developers Want and Need
from Program Analysis: An Empirical Study. In Proceedings of the 31st IEEE/ACM
InternationalConferenceonAutomatedSoftwareEngineering (Singapore,Singa-
pore)(ASE 2016) . Association for Computing Machinery, New York, NY, USA,
332–343. https://doi.org/10.1145/2970276.2970347
[15]Lori Flynn, William Snavely, David Svoboda, Nathan VanHoudnos, Richard Qin,
Jennifer Burns, David Zubrow, Robert Stoddard, and Guillermo Marce-Santurio.
2018. PrioritizingAlertsfromMultipleStaticAnalysisTools,UsingClassification
Models. In Proceedings of the 1st International Workshop on Software Qualities
and Their Dependencies (Gothenburg, Sweden) (SQUADE ’18) . Association for
Computing Machinery, New York, NY, USA, 13–20. https://doi.org/10.1145/
3194095.3194100
[16]Neville Grech, George Fourtounis, Adrian Francalanza, and Yannis Smaragdakis.
2018.ShootingfromtheHeap:Ultra-ScalableStaticAnalysiswithHeapSnapshots.
InProceedings of the 27th ACM SIGSOFT International Symposium on Software
Testing and Analysis (Amsterdam, Netherlands) (ISSTA 2018) . Association for
ComputingMachinery,NewYork,NY,USA,198–208. https://doi.org/10.1145/
3213846.3213860[17]Aditya Grover and Jure Leskovec. 2016. Node2vec: Scalable Feature Learning
for Networks. In Proceedings of the 22nd ACM SIGKDD International Conference
on Knowledge Discovery and Data Mining (San Francisco, California, USA) (KDD
’16).AssociationforComputingMachinery,NewYork,NY,USA,855–864. https:
//doi.org/10.1145/2939672.2939754
[18]Sarah Heckman and Laurie Williams. 2009. A Model Building Process for Identi-
fyingActionableStaticAnalysisAlerts.In Proceedingsofthe2009International
Conference onSoftware Testing Verificationand Validation (ICST ’09) . IEEECom-
puter Society, USA, 161–170. https://doi.org/10.1109/ICST.2009.45
[19]Tin Kam Ho. 1995. Random Decision Forests. In Proceedings of the Third Interna-
tionalConferenceonDocumentAnalysisandRecognition(Volume1)-Volume1
(ICDAR ’95) . IEEE Computer Society, USA, 278.
[20]David Hovemeyer and William Pugh. 2004. Finding Bugs is Easy. SIGPLAN Not.
39, 12 (Dec. 2004), 92–106. https://doi.org/10.1145/1052883.1052895
[21]LaurentHubert,ThomasJensen,andDavidPichardie.2008. SemanticFounda-
tions and Inference of Non-null Annotations. In Formal Methods for Open Object-
Based Distributed Systems , Gilles Barthe and Frank S. de Boer (Eds.). Springer
Berlin Heidelberg, Berlin, Heidelberg, 132–149.
[22]Kazuaki Ishizaki, Motohiro Kawahito, Toshiaki Yasue, Hideaki Komatsu, and
ToshioNakatani.2000. AStudyofDevirtualizationTechniquesforaJavaJust-In-
TimeCompiler.In Proceedingsofthe15thACMSIGPLANConferenceonObject-
Oriented Programming, Systems, Languages, and Applications (Minneapolis, Min-
nesota,USA) (OOPSLA’00) .AssociationforComputingMachinery,NewYork,
NY, USA, 294–310. https://doi.org/10.1145/353171.353191
[23]Minseok Jeon, Myungho Lee, and Hakjoo Oh. 2020. Learning Graph-Based
HeuristicsforPointerAnalysiswithoutHandcraftingApplication-SpecificFea-
tures.Proc.ACMProgram.Lang. 4,OOPSLA,Article179(Nov.2020),30pages.
https://doi.org/10.1145/3428247
[24]Sehun Jeong, Minseok Jeon, Sungdeok Cha, and Hakjoo Oh. 2017. Data-Driven
Context-SensitivityforPoints-toAnalysis. Proc.ACMProgram.Lang. 1,OOPSLA,
Article 100 (oct 2017), 28 pages. https://doi.org/10.1145/3133924
[25]Brittany Johnson, Yoonki Song, Emerson Murphy-Hill, and Robert Bowdidge.
2013. WhyDon’tSoftwareDevelopersUseStaticAnalysisToolstoFindBugs?.
InProceedings of the 2013 International Conference on Software Engineering (San
Francisco, CA, USA) (ICSE ’13) . IEEE Press, 672–681.
[26]ChristianGramKalhaugeandJensPalsberg.2018. SoundDeadlockPrediction.
Proc.ACMProgram.Lang. 2,OOPSLA,Article146(Oct.2018),29pages. https:
//doi.org/10.1145/3276516
[27]Sarfraz Khurshid, Corina S. Păsăreanu, and Willem Visser. 2003. Generalized
SymbolicExecutionforModelCheckingandTesting.In Proceedingsofthe9th
InternationalConferenceonToolsandAlgorithmsfortheConstructionandAnalysis
of Systems (Warsaw, Poland) (TACAS’03) . Springer-Verlag, Berlin, Heidelberg,
553–568.
[28]S. B. Kotsiantis. 2007. Supervised Machine Learning: A Review of Classifica-
tion Techniques. In Proceedings of the 2007 Conference on Emerging Artificial
Intelligence Applications in Computer Engineering . IOS Press, NLD, 3–24.
[29]Ondrej Lhoták. 2007. Comparing Call Graphs. In Proceedings of the 7th ACM
SIGPLAN-SIGSOFTWorkshoponProgramAnalysisforSoftwareToolsandEngi-
neering(San Diego, California, USA) (PASTE ’07) . Association for Computing
Machinery,NewYork,NY,USA,37–42. https://doi.org/10.1145/1251535.1251542
[30]Yue Li, Tian Tan, Anders Møller, and Yannis Smaragdakis. 2018. Scalability-
First Pointer Analysis with Self-Tuning Context-Sensitivity. In Proceedings of the
201826thACMJointMeetingonEuropeanSoftwareEngineeringConferenceand
Symposium on the Foundations of Software Engineering (Lake Buena Vista, FL,
USA)(ESEC/FSE 2018) . Association for Computing Machinery, New York, NY,
USA, 129–140. https://doi.org/10.1145/3236024.3236041
[31]BenjaminLivshits,ManuSridharan,YannisSmaragdakis,OndřejLhoták,J.Nelson
Amaral,Bor-YuhEvanChang,SamuelZ.Guyer,UdayP.Khedker,AndersMøller,
andDimitriosVardoulakis.2015.InDefenseofSoundiness:AManifesto. Commun.
ACM58, 2 (Jan. 2015), 44–46. https://doi.org/10.1145/2644805
[32]YiLu, DanielWainwright,and MichaelReif.2020. Probabilisticcall-graphcon-
struction. (Jul 2020). US Patent No. 10,719,314 B2.
[33]Ravi Mangal, Xin Zhang, Aditya V. Nori, and Mayur Naik. 2015. A User-Guided
Approach to Program Analysis. In Proceedings of the 2015 10th Joint Meeting on
FoundationsofSoftwareEngineering (Bergamo,Italy) (ESEC/FSE2015) .Association
for Computing Machinery, New York, NY, USA, 462–473. https://doi.org/10.
1145/2786805.2786851
[34]Aravind Nair, Avijit Roy, and Karl Meinke. 2020. FuncGNN: A Graph Neural
Network Approach to Program Similarity. In Proceedings of the 14th ACM / IEEE
International Symposium on Empirical Software Engineering and Measurement
(ESEM)(Bari,Italy) (ESEM’20) .AssociationforComputingMachinery,NewYork,
NY, USA, Article 10, 11 pages. https://doi.org/10.1145/3382494.3410675
[35]Jens Palsberg and Cristina V. Lopes. 2018. NJR: A Normalized Java Resource.
InCompanion Proceedings for the ISSTA/ECOOP 2018 Workshops (Amsterdam,
Netherlands) (ISSTA ’18) . Association for Computing Machinery, New York, NY,
USA, 100–106. https://doi.org/10.1145/3236454.3236501
2054Striking a Balance:
Pruning False-Positives from Static Call GraphsICSE ’22, May 21–29, 2022, Pittsburgh, PA, USA
[36]F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M.
Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cour-
napeau,M.Brucher,M.Perrot,andE.Duchesnay.2011. Scikit-learn:Machine
LearninginPython. JournalofMachineLearningResearch 12(2011),2825–2830.
[37]MukundRaghothaman,SulekhaKulkarni,KihongHeo,andMayurNaik.2018.
User-Guided Program Reasoning Using Bayesian Inference. SIGPLAN Not. 53, 4
(June 2018), 722–735. https://doi.org/10.1145/3296979.3192417
[38]Atanas Rountev, Scott Kagan, and Michael Gibas. 2004. Static and Dynamic
Analysis of Call Chains in Java. In Proceedings of the 2004 ACM SIGSOFT Interna-
tional Symposium on Software Testing and Analysis (Boston, Massachusetts, USA)
(ISSTA ’04) . Association for Computing Machinery, New York, NY, USA, 1–11.
https://doi.org/10.1145/1007512.1007514
[39]Joseph R. Ruthruff, John Penix, J. David Morgenthaler, Sebastian Elbaum, and
Gregg Rothermel. 2008. Predicting Accurate and Actionable Static Analysis
Warnings:AnExperimentalApproach.In Proceedingsofthe30thInternational
ConferenceonSoftwareEngineering (Leipzig,Germany) (ICSE’08) .Association
for Computing Machinery, New York, NY, USA, 341–350. https://doi.org/10.
1145/1368088.1368135
[40]CaitlinSadowski,EdwardAftandilian,AlexEagle,LiamMiller-Cushon,andCiera
Jaspan.2018. LessonsfromBuildingStaticAnalysisToolsatGoogle. Commun.
ACM61, 4 (March 2018), 58–66. https://doi.org/10.1145/3188720
[41]J. Sawin and A. Rountev. 2011. Assumption Hierarchy for a CHA Call Graph
ConstructionAlgorithm.In 2011IEEE11thInternationalWorkingConferenceon
Source Code Analysis and Manipulation . 35–44.
[42]Scikit-learn. [n.d.]. Feature importances with a forest of trees. https://scikit-
learn.org/stable/auto_examples/ensemble/plot_forest_importances.html.
[43]LiSui,JensDietrich,AmjedTahir,andGeorgeFourtounis.2020. OntheRecallof
Static Call Graph Construction in Practice (ICSE ’20) . Association for Computing
Machinery, New York, NY, USA, 1049–1060. https://doi.org/10.1145/3377811.
3380441
[44]OmerTripp,SalvatoreGuarnieri,MarcoPistoia,andAleksandrAravkin.2014.
ALETHEIA: Improvingthe Usabilityof StaticSecurityAnalysis. In Proceedings
ofthe2014ACMSIGSACConferenceonComputerandCommunicationsSecurity
(Scottsdale,Arizona,USA) (CCS’14).AssociationforComputingMachinery,New
York, NY, USA, 762–774. https://doi.org/10.1145/2660267.2660339
[45]Akshay Utture, Christian Gram Kalhauge, Shuyang Liu, and Jens Palsberg. 2020.
NJR-1 Dataset . https://doi.org/10.5281/zenodo.4839913
[46]Akshay Utture, Christian Gram Kalhauge, Shuyang Liu, and Jens Palsberg. 2021.
ArtifactforICSE-22submission"StrikingaBalance:PruningFalse-Positivesfrom
Static Call Graphs" . https://doi.org/10.5281/zenodo.5177161
[47]WALA. 2015. IBM, “T.J. Watson Libraries for Analysis (WALA),”. http://wala.
sourceforge.net.
[48]YuWang,KeWang,FengjuanGao,andLinzhangWang.2020. LearningSemantic
Program Embeddings with Graph Interval Neural Network. Proc. ACM Program.
Lang.4, OOPSLA, Article 137 (Nov. 2020), 27 pages. https://doi.org/10.1145/
3428205
[49]U. Yüksel and H. Sözer. 2013. Automated Classification of Static Code Anal-
ysis Alerts: A Case Study. In 2013 IEEE International Conference on Software
Maintenance . 532–535.
[50]Weilei Zhang and Barbara G. Ryder. 2007. Automatic Construction of Accurate
Application Call Graph with Library Call Abstraction for Java: Research Articles.
J. Softw. Maint. Evol. 19, 4 (July 2007), 231–252.
2055