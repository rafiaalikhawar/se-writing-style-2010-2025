FIRA: Fi ne-Grained Gra ph-Based Code Change Representation
for Automated Commit Message Generation
Jinhao Dong
Key Laboratory of High Confidence Software
Technologies (Peking University), MoE
School of Computer Science,
Peking University
Beijing, China
dongjinhao@stu.pku.edu.cnYiling Louâˆ—
Department of Computer Science,
Purdue University
West Lafayette, IN, USA
lou47@purdue.edu
Qihao Zhu, Zeyu Sun, Zhilin Li
Key Laboratory of High Confidence Software
Technologies (Peking University), MoE
School of Computer Science,
Peking University
Beijing, China
{zhuqh,szy_,1700012439}@pku.edu.cnWenjie Zhang, Dan Haoâˆ—
Key Laboratory of High Confidence Software
Technologies (Peking University), MoE
School of Computer Science,
Peking University
Beijing, China
{zhang_wen_jie,haodan}@pku.edu.cn
ABSTRACT
Commit messages summarize code changes of each commit in nat-
ural language, which help developers understand code changes
withoutdiggingintodetailedimplementations andplayanessen-
tial role in comprehending software evolution. To alleviate human
effortsinwritingcommitmessages,researchershaveproposedvar-
iousautomatedtechniquestogeneratecommitmessages,including
template-based, information retrieval-based, and learning-based
techniques. Although promising, previous techniques have limited
effectivenessduetotheir coarse-grainedcodechangerepresentations.
This work proposes a novel commit message generation tech-
nique,FIRA,whichfirstrepresentscodechangesviafine-grained
graphs and then learns to generate commit messages automati-
cally.Differentfromprevioustechniques,FIRArepresentsthecode
changes with fine-grained graphs, which explicitly describe thecode edit operations between the old version and the new ver-
sion,andcodetokensatdifferentgranularities(i.e.,sub-tokensand
integral tokens). Based on the graph-based representation, FIRA
generates commit messages by a generation model, which includes
a graph-neural-network-based encoder and a transformer-based
decoder. To make both sub-tokens and integral tokens as available
ingredients for commit message generation, the decoder is further
incorporatedwithanoveldualcopymechanism.Wefurtherper-
form an extensive study to evaluate the effectiveness of FIRA. Our
quantitative results show that FIRA outperforms state-of-the-art
âˆ—Corresponding authors
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation
on the first page. Copyrights for components of this work owned by others than ACMmustbehonored.Abstractingwithcreditispermitted.Tocopyotherwise,orrepublish,topostonserversortoredistributetolists,requirespriorspecificpermissionand/ora
fee. Request permissions from permissions@acm.org.
ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
Â© 2022 Association for Computing Machinery.
ACM ISBN 978-1-4503-9221-1/22/05...$15.00
https://doi.org/10.1145/3510003.3510069techniques in terms of BLEU, ROUGE-L, and METEOR; and our
ablationanalysisfurthershowsthatmajorcomponentsinourtech-
nique both positively contribute to the effectiveness of FIRA. In
addition,wefurtherperformahumanstudytoevaluatethequality
of generated commit messages from the perspective of developers,
and the results consistently show the effectiveness of FIRA over
the compared techniques.
CCS CONCEPTS
â€¢Softwareanditsengineering â†’Softwaremaintenancetools .
KEYWORDS
CommitMessageGeneration,GraphNeuralNetwork,CodeChange
Representation
ACM Reference Format:
JinhaoDong,YilingLou,QihaoZhu,ZeyuSun,ZhilinLi,andWenjieZhang,
Dan Hao. 2022. FIRA: Fine-Grained G raph-Based Code Change Representa-
tion for Automated Commit Message Generation. In 44th International Con-
ferenceonSoftwareEngineering(ICSEâ€™22),May21â€“29,2022,Pittsburgh,PA,
USA.ACM,NewYork,NY,USA, 12pages.https://doi.org/10.1145/3510003.
3510069
1 INTRODUCTION
Whendeveloperscommitchangedcodetoaversioncontrolsystem,
eachcommitissupposedtobedocumentedwitha commitmessage.
Commitmessagessummarizecodechangesinnaturallanguage,and
can help developers quickly understand the high-level intention
of code changes without digging into detailed implementations.
Therefore,commitmessagesareprevalentinsoftwaremaintenance
andplayanessentialroleincomprehendingsoftwareevolution[ 4].
However, manually writing commit messages can be very labor-
intensive. High-quality commit messages should precisely describe
the rationales of changed code, which often requires non-trivial
manualeffortsinpractice.Inaddition,modernsoftwarehasbeen
evolving rapidly, and frequent commit submissions put a heavy
9702022 IEEE/ACM 44th International Conference on Software Engineering (ICSE)
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:19:36 UTC from IEEE Xplore.  Restrictions apply. ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA Jinhao Dong, Yiling Lou, Qihao Zhu, Zeyu Sun, Zhilin Li, and Wenjie Zhang, Dan Hao
burdenondevelopers.Therefore,althoughcommitmessagesare
beneficial,theyareoftenneglectedbydevelopersduetothetime
costs. As is reported, almost 14% commit messages in 23K Java
projects are empty [10].
To alleviate manual efforts in writing commit messages, re-
searchers have proposed various techniques to generate commit
messages automatically. Given a code change, these techniques
first represent the old-version and new-version code with specific
formats,suchassequencesoftokensorpathsofabstractsyntaxtree
(AST),andthengeneratecommitmessagesbasedontherepresenta-
tions via different strategies such as template-based [ 4,6], informa-
tion retrieval-based [ 16,29], and learning-based [ 19,28,32,41,44]
techniques.
Although achieving promising performance, the effectiveness of
previoustechniqueshasbeenrestrictedbytheir coarse-grainedcode
change representations. First, existing commit message generation
techniquesrepresentthecodechangesbysimplyputtingold-version
and new-versioncode togetherwithout explicitlyhighlighting fine-
grained edit operations. For example, given an expression â€œ a=1 ;â€
modifiedintoâ€œ b=1 ;â€,token-basedrepresentations[ 19,32,41,44]
represent such code changes by concatenating both expressionsinto one flat sequence of tokens (i.e., â€œ
-a=1 ;+b=1 ; â€),
where -/+ denotes the old/new-version code; the AST-based rep-
resentations [ 28] represent the code changes by concatenating
old-versionandnew-versionASTpathsintoonesequence(i.e.,â€œ -
assignment.variable.a.operator.=.literal.1; + assignment.variable.b.operator.=.literal.1;
â€). Therefore, existing
learning-basedmodelshavetocomparethecoderepresentationsofold and new versions so as to capture the subtle edit operation (i.e.,thetokenâ€œ
aâ€ischangedintoanewtokenâ€œ bâ€)bythemselves,which
makes it more challenging to generate precise commit messages.
Second, existing code change representations mainly focus on coarse-
grainedtokens(i.e.,integraltokens)inthecodewithoutexplicitlyand
individuallydescribingfiner-grainedtokens(i.e.,sub-tokensofinte-graltokens). Infact,itisprevalentthatthecommitmessages may
containsub-tokensoftheinputcodechanges.Forexample,fora
codechangethatcontainsanintegraltokenâ€œ setMinimumSize â€,its
relevantcommitmessagecontainsthreetokensâ€œsetâ€,â€œminimumâ€,
andâ€œsizeâ€,whichareexactlythesub-tokensoftheintegraltoken
â€œsetMinimumSize â€.However,mostprevioustechniques[ 19,32,41]
consideronlyintegraltokensandignoresub-tokensintheircode
changerepresentations;whileafewtechniques[ 28,44]represent
allsub-tokensinacompoundrepresentation(e.g.,onesingleem-
bedding vector) without representing each sub-token individually.
Suchcompoundrepresentationsmakeitchallengingtoutilizeeach
sub-token as available ingredients for commit message generation.
Therefore, they exhibit a poor performance for the cases that com-
mit messages contain sub-tokens of the input code.
To address the limitations above, in this work, we propose a
novel commit message generation technique, FIRA, which first
representscodechangesvia fine-grainedg raphsandthenlearns
to generate commit messages automatically. Compared to previous
codechange representations, FIRAmakes the firstattempt toex-plicitly describe the edit operations between the old-version and
new-version code, along with tokens at different granularities (i.e.,
integraltokensandsub-tokens).Basedontheproposedgraph-based
representations, FIRA then learns to generate commit messagesiterativelywithanencoder-decodermodel.Inparticular,FIRAin-
corporatesthegraphneuralnetworkintheencodersoastodirectly
encodethegraph-structuredinputs;andthedecoderincorporates
the transformer [ 39] and a novel dual copy mechanism, which can
notonlygeneratetokensfromthevocabularybutalsocopyboth
integral tokens and sub-tokens from the input.
We perform an extensive evaluation to compare FIRA with
sixstate-of-the-artcommitmessagetechniquesonawidely-used
benchmark [ 16,19,29,32,41,44]. The results show that FIRA out-
performsallcomparedtechniquesintermsofBLEU,ROUGE-L,and
METEOR. We further analyze the effectiveness of each component
in FIRA by an ablation study and case analysis. The results further
confirm that major components (i.e., explicitly representing edit
operationsandcopyingsub-tokens)bothpositivelycontributeto
the effectiveness of FIRA, and indeed help generate higher-quality
commitmessagesthanprevioustechniques.Inaddition,wefurtherperformahumanstudytoevaluatethequalityofgeneratedcommit
messagesfromtheperspectiveofdevelopers,whichconsistently
shows the effectiveness of FIRA over compared techniques.
In summary, this paper makes the following contributions:
â€¢A fine-grained graph-based code change representa-tion
for commit message generation, which explicitly de-
scribescodeeditoperationsandtokensatdifferent granu-
larities.
â€¢Anovelencoder-decodermodel forcommitmessagegen-
eration, which leverages the graph neural network in theencoder to process the proposed graph-based representa-
tion,andleveragesthetransformerwithanoveldualcopy
mechanisminthedecodertoutilizebothintegraltokensand
sub-tokens.
â€¢Anextensiveexperiment evaluatingourapproachagainst
six state-of-the-art techniques on a widely-used benchmark,
which suggests the effectiveness of our approach by the
quantitative, qualitative, and ablation analysis.
â€¢A human study on the quality of generated commit mes-
sages,whichfurthershowstheeffectivenessofourapproach
from the perspective of developers.
â€¢Areplicationpackage availableat https://github.com/DJjjjhao/
FIRA-ICSE.
2 MOTIVATION
Existing commit message generation techniques represent and uti-
lize code changes in a coarse-grained way. First, they simply put
old-versionandnew-versioncodetogetherwithoutexplicitlyde-
scribing fine-grained edit operations; second, they only focus on
coarse-grainedtokens(i.e.,integraltokens)withoutrepresenting
sub-tokens individually. In this section, we further illustrate these
limitations with several real-world examples.
2.1 Limitation 1: Edit Operations
As shown by the example in Figure 1, we can observe that edit
operationsarehighlyrelevanttodeveloper-writtencommitmes-
sages. The developer makes the edit operation (i.e., adding one
token â€œabstractâ€) and the corresponding commit message indicates
his/herintentionofmakingtheclassabstract.However,existing
techniquescannotalwaysnoticesucheditoperations,sincetheir
971
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:19:36 UTC from IEEE Xplore.  Restrictions apply. FIRA: Fine-Grained Gra ph-Based Code Change Representation
for Automated Commit Message Generation ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA

	
			 -private class FormAuthClientBase extends 
SimpleHttpClient {@@ -421,7 +421,7 @@ public class TestFormAuthenticator
extends TomcatBaseTest {
+  private abstract class FormAuthClientBase extends 
SimpleHttpClient {
protected static final String LOGIN_PARAM_TAG =    
"action=";protected static final String LOGIN_RESOURCE =        "j_security_check";
Figure 1: Motivating example: edit operations
 	 
	 
   	   
(a) CODISUM

 

 	



(b)ATOM
Figure 2: Existing code change representations
code change representations put old-version and new-version code
together without explicitly highlighting the differences. Therefore,
theyhave tocompare old-versionandnew-version codeandthen
infer theedit operations bythemselves. For example, asshown in
Figure2a,CODISUM[ 44],oneofthelearning-basedapproaches,
represents the code change by concatenating the flat token se-
quences of old-version and new-version code into one sequence.
Althougheachtokeninthesequenceisannotatedwitholdversion(-
)ornewversion(+),CODISUMhastolearntoinferthespecificedit
operation (i.e., one token â€œabstractâ€ is added) by itself. Similarly, in
Figure2b,anotherlearning-basedtechniqueATOM[ 28],represents
the code changes by concatenating old-version and new-version
ASTpathstogether,anditalsohastocomparetwopathsbyitselfso
as to capture the edit operations at AST level. Such coarse-grained
codechangerepresentationsareactuallycumbersome,especially
when code changes involve very minor edit operations with the
majorityoftokensunchanged(e.g.,onlyonetokenischangedin
the example). Learning-based techniques cannot always guarantee
to precisely capture such subtle edit operations, which may fur-ther result in imprecise commit message generation. In fact, our
experimentalresultsalsoconfirmthatthesetechniquesallfailto
generate precise commit messages for this example.
Toaddressthislimitation,weproposetoexplicitlyhighlightedit
operationsinthecodechangerepresentation,whichcaninclude
more accurate information for commit message generation.getSupportActionBar().addTab(newTab);
	
	
 
@@ -219,7 +220,7 @@ public class Feature Toggles extends 
SherlockActivity{
newTab.setText("Text!"); 
}
}
+newTab.setTab Listener(Feature Toggles.this);
Figure 3: Motivating example: sub-tokens
2.2 Limitation 2: Sub-tokens
As shown by the example in Figure 3, we can observe that sub-
tokens in the input code can provide very helpful hints for commit
message generation. For example, the developer-written commit
messageâ€œAddtablistenerforfeaturetogglesâ€consistsofthesub-
tokens â€œtab â€, â€œlistenerâ€, â€œfeatureâ€ and â€œtoggles â€ in the input code.
However, existing techniques focus on integral tokens and seldom
treatsub-tokenasequallyimportantasintegraltoken.Forexample,
most existing techniques [ 19,32,41] ignore sub-tokens in their
code change representations, while a few techniques [ 28,44] de-
scribeallsub-tokensinacompoundrepresentation(e.g.,onesingle
embedding vector) without representing sub-tokens explicitly and
individually.Suchcompoundrepresentationsrestricttheutilization
of sub-tokens. For example, with such representations, existing
techniquescanonlygeneratethefrequentsub-tokensthatarein-
cludedinthevocabulary,butoftenfailtogeneratethoseinfrequent
sub-tokens that are excluded in the vocabulary or seldom occurin the training set. Actually, for the generation tasks related toprogramcode,suchinfrequentsub-tokenscanbeveryprevalent
since they are often project specific tokens (e.g., â€œtab â€ and â€œtoggles â€
in the example). Therefore, existing coarse-grained code change
representations make it challenging to generate commit messages
containing such sub-tokens.
Therefore, to fully utilize sub-tokens in the code, we propose
totreatallintegraltokensandsub-tokensequallyimportantand
representsub-tokensindividuallyinthecodechangerepresentation.
In addition, to make both frequent and infrequent sub-tokens asingredients of the commit message, we further leverage a novel
dualcopymechanismadditionallyforsub-tokensinourmodelso
that both integral tokens and sub-tokens can be either copied or
generated from the vocabulary.
3 CODE CHANGE REPRESENTATION
This section presents our fine-grained graph-based code change
representation,whichexplicitlyincludeseditoperationsandsub-
tokens to enable more precise commit message generation.In par-
ticular,thegraphconstructionconsistsoffoursteps,including(1)
buildingchoppedabstractsyntaxtrees(Section 3.1),(2)addingsub-
tokens(Section 3.2),(3)annotatingeditoperations(Section 3.3),and
(4)incorporatingadditionalsequentialinformation(Section 3.4).We
then introduce each step in detail and use the motivating example
in Figure 1for illustration.
3.1 Chopped Abstract Syntax Trees
A typical code change often includes the modified code and its
surroundingcontext.Forexample,acodechangeinGitHub(e.g.,
972
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:19:36 UTC from IEEE Xplore.  Restrictions apply. ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA Jinhao Dong, Yiling Lou, Qihao Zhu, Zeyu Sun, Zhilin Li, and Wenjie Zhang, Dan Hao
 

	




	 	 


	 
Figure 4: Graph ğ´ğ‘†ğ‘‡: chopped AST
Figure1andFigure 3)oftencontainsmultiplelinesofcodeandeach
linestartswithitschangetype.Morespecifically,â€œ-â€denotesthe
deleted old-version code, â€œ+â€ denotes the added new-version code,
and the empty character â€œ â€ denotes the unchanged code both in
oldandnewversion.Acodechangemayinvolvesingleormultiple
hunks.Here,a hunkreferstothecontinuouslineswiththesame
changetype.AmongcommitsinGitHub,somemaymodifyonly
one token in a hunk, while some may modify hundreds of lines in
multiplehunks.Existinglearning-basedtechniquesoftenrepresent
all the changed code together, e.g., CODISUM [ 44] concatenates all
deleted/addedhunksasonesequenceandATOM[ 28]constructs
ASToftheentirecodefileevenifthecodechangesoccurinonly
severallinesinthefile.Representingthecodechangeasawholecanobfuscatedetailsamonghunksandthusmakesitmorechallenging
for the commit message generation model to summarize essential
featuresfromsuchacoarse-grainedrepresentation.Therefore,in
FIRA, we propose to construct AST at hunk level, so that more
detailedinformationcanbereservedwhenthecodechangeinvolves
multiple hunks.
Morespecifically,givenacodechange,wefirstseparateitinto
several hunks according to their change types; then for each hunk,
weparseittoconstructitsownabstractsyntaxtree,i.e.,chopped
AST.Inthisway,weobtainasetofchoppedASTsforthegivencode
change, which are actually a set of graphs with basic semantic and
syntactic informationof each hunk. In particular, forthe chopped
AST of a deleted hunk (i.e., all its lines are deleted), we denote it
as ASTğ‘œğ‘™ğ‘‘since the relevant code only exists in the old version;
for the chopped AST of an added hunk (i.e., all its lines are added),
wedenoteitasAST ğ‘›ğ‘’ğ‘¤sincetherelevantcodeonlyexistsinthe
new version. We denote the graph constructed in this phase as
Graphğ´ğ‘†ğ‘‡. Figure4presents the Graph ğ´ğ‘†ğ‘‡of changed lines in the
illustration example, where â€œclass dcl â€ is the abbreviation of the
AST node type â€œclass_declaration â€, and â€œc 0â€ is the placeholder of
the class name â€œFormAuthClientBase â€.
 	 

	
	
  	
	
 

  
	  	
Figure 5: Graph ğ‘¡ğ‘œğ‘˜ğ‘’ğ‘›: Graph ğ´ğ‘†ğ‘‡extended with sub-tokens
3.2 Sub-tokens
AsmentionedinSection 2,commitmessagesoftencontaincoarse/fine-
grained tokens (i.e., integral tokens and sub-tokens) in the inputcode.Suchaphenomenonisprevalent,sinceitisacommonprac-
tice for developers to name a function or a class with phrases.
Forexample,givena methodnamed asâ€œdeleteOldThreadDumpsâ€,
this integral token consists of four sub-tokens â€œdelete old thread
dumpsâ€, which describe the functionality of the method and might
beadoptedinthecommitmessagewhencodechangesarerelevant
tothismethod.Therefore,inourrepresentation,weconsidernot
onlytheintegraltokensbutalsotheirsub-tokens.Morespecifically,
in eachGraph ğ´ğ‘†ğ‘‡, forthe node withan integraltoken, we splitit
into separated sub-tokens according to the widely-adopted naming
convention (i.e., camel case and snake case), represent these sub-tokens as extra nodes in the graph, and then connect them with
theirbelongingintegraltokennodes.Inthisway,thechoppedAST
is extended with nodes and edges relevant to sub-tokens, where
integraltokensandsub-tokensareequally-importantindividuals
and both can be directly utilized in the subsequent commit mes-
sage generation. We denote the graph constructed in this phase as
Graphğ‘¡ğ‘œğ‘˜ğ‘’ğ‘›.Figure5presentstheGraph ğ‘¡ğ‘œğ‘˜ğ‘’ğ‘›,whichextendsthe
Graphğ´ğ‘†ğ‘‡(i.e., Figure 4) with sub-token information.

	
  
	
	
  	
	
 

  
	  	
 

Figure 6: Graph ğ‘’ğ‘‘ğ‘–ğ‘¡ğ‘–ğ‘œğ‘›: Graph ğ‘¡ğ‘œğ‘˜ğ‘’ğ‘›extended with edit nodes
3.3 Edit Operations
Sofar,allnodesinthegraphrepresentcodeelements,whichare
denoted as code nodes for distinction. Based on Graph ğ‘¡ğ‘œğ‘˜ğ‘’ğ‘›,w e
further introduce edit nodes to explicitly represent fine-grained
edit operations between AST ğ‘œğ‘™ğ‘‘and AST ğ‘›ğ‘’ğ‘¤. In particular, we
considerfiveeditnodes,includingv ğ´ğ·ğ·,vğ·ğ¸ğ¿,vğ‘€ğ‘‚ğ‘‰ğ¸,vğ‘ˆğ‘ƒğ·ğ´ğ‘‡ğ¸,
and vğ‘€ğ´ğ‘‡ğ¶ğ».
â€¢vğ´ğ·ğ·.Ifthecodenode ğ‘£existsinAST ğ‘›ğ‘’ğ‘¤butnotinAST ğ‘œğ‘™ğ‘‘,
ğ‘£isnewly-addedandshouldbeconnectedwithaneditnode
vğ´ğ·ğ·.
â€¢vğ·ğ¸ğ¿.Ifthecodenode ğ‘£existsinAST ğ‘œğ‘™ğ‘‘butnotinAST ğ‘›ğ‘’ğ‘¤,
ğ‘£isdeletedandshouldbeconnectedwithaneditnodev ğ·ğ¸ğ¿.
â€¢vğ‘€ğ‘‚ğ‘‰ğ¸.Ifthecodenode ğ‘£existsinbothAST ğ‘œğ‘™ğ‘‘andAST ğ‘›ğ‘’ğ‘¤
andthepositionsof ğ‘£anditssub-treearemoved,thenode
ğ‘£in both AST ğ‘œğ‘™ğ‘‘and AST ğ‘›ğ‘’ğ‘¤should be connected with an
edit node v ğ‘€ğ‘‚ğ‘‰ğ¸.
â€¢vğ‘ˆğ‘ƒğ·ğ´ğ‘‡ğ¸. If the code node ğ‘£exists in both AST ğ‘œğ‘™ğ‘‘and
ASTğ‘›ğ‘’ğ‘¤anditsvalueisupdated,thenodes ğ‘£inbothAST ğ‘œğ‘™ğ‘‘
andAST ğ‘›ğ‘’ğ‘¤shouldbeconnectedwithaneditnodev ğ‘ˆğ‘ƒğ·ğ´ğ‘‡ğ¸.
â€¢vğ‘€ğ´ğ‘‡ğ¶ğ».I fan od e ğ‘£exists in both AST ğ‘œğ‘™ğ‘‘and AST ğ‘›ğ‘’ğ‘¤and
itsvalueandpositionremainunchanged,thenodes ğ‘£inboth
973
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:19:36 UTC from IEEE Xplore.  Restrictions apply. FIRA: Fine-Grained Gra ph-Based Code Change Representation
for Automated Commit Message Generation ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
ASTğ‘œğ‘™ğ‘‘and AST ğ‘›ğ‘’ğ‘¤should be connected with an edit node
vğ‘€ğ´ğ‘‡ğ¶ğ».
According to the description above, we further insert edit nodes
totheGraph ğ‘¡ğ‘œğ‘˜ğ‘’ğ‘›byconnectingthemwiththeoriginalcodenodes.
WedenotethegraphconstructedinthisphaseasGraph ğ‘’ğ‘‘ğ‘–ğ‘¡ğ‘–ğ‘œğ‘›.Fig-
ure6presentstheGraph ğ‘’ğ‘‘ğ‘–ğ‘¡ğ‘–ğ‘œğ‘›,whichfurtherextendstheGraph ğ‘¡ğ‘œğ‘˜ğ‘’ğ‘›
(i.e., Figure 5) with edit nodes.
 

	
	

 
  
  		  		
Figure 7: Graph ğ‘ ğ‘’ğ‘: one-line graph
 
	
	
  	
	
 

  
	   	
	  	

	
  
Figure 8: Graph ğ‘“ğ‘–ğ‘›ğ‘ğ‘™: fine-grained representation for code
changes
3.4 Additional Sequential Information
So far, the code change has mainly been represented based on
the AST structure. As suggested by previous work [ 19,44,45], the
sequential information (i.e., treating code as a flat token sequence)
can also be helpful for commit message generation, since it can
reservetheadjacentrelationshipandtheorderofthetokens.There-
fore, we further extend Graph ğ‘’ğ‘‘ğ‘–ğ‘¡ğ‘–ğ‘œğ‘›with additional sequential
informationsoastoincludemorehelpfulinformation.Inparticular,
we first build an extra one-line graph (denoted as Graph ğ‘ ğ‘’ğ‘)b y
regarding each token as a node and connecting every two adjacent
nodes. Figure 7presents the Graph ğ‘ ğ‘’ğ‘of the code change hank.
Then,wemergeGraph ğ‘ ğ‘’ğ‘withtheGraph ğ‘’ğ‘‘ğ‘–ğ‘¡ğ‘–ğ‘œğ‘›byusingthenodes
existing in both graphs as the anchor nodes. More specifically, for
each node ğ‘£ğ‘–in Graph ğ‘’ğ‘‘ğ‘–ğ‘¡ğ‘–ğ‘œğ‘›,i fğ‘£ğ‘–matches with the node ğ‘£ğ‘—in
Graphğ‘ ğ‘’ğ‘,ğ‘£ğ‘–is removed from Graph ğ‘’ğ‘‘ğ‘–ğ‘¡ğ‘–ğ‘œğ‘›and all its connected
edges arere-connected with ğ‘£ğ‘—. Inthis way, wecombine the AST-
basedinformation(i.e.,Graph ğ‘’ğ‘‘ğ‘–ğ‘¡ğ‘–ğ‘œğ‘›)andsequentialinformation
(i.e.,Graph ğ‘ ğ‘’ğ‘)intoonelargergraph,i.e.,asGraph ğ‘“ğ‘–ğ‘›ğ‘ğ‘™.Figure8
presents the Graph ğ‘“ğ‘–ğ‘›ğ‘ğ‘™for the illustration example, which further
extends the Graph ğ‘’ğ‘‘ğ‘–ğ‘¡ğ‘–ğ‘œğ‘›(i.e., Figure 6) with additional sequential
information.
4 MODEL ARCHITECTURE
Figure9presents the overview of our model, whose input is the
final graph-based code change representation and output is thegenerated commit message. Overall, the model is in an encoder-
decoder architecture. In the encoder, we adopt graph neural net-
works (GNN) due to its strong capability of processing graph-
structureddata[ 12,13,22,30,34,40,43].Inthedecoder,weleverage
thetransformerarchitecture[ 39],whichisthestate-of-the-artse-
quencetosequencemodelandiswidelyusedinvariousgeneration
tasks [1,37,46], to generate tokens in commit messages iteratively.
When generating the next token, the decoder first performs self-
attention between the current token and the previously generated
tokens,andthenperformscross-attentionoverinputtokensembed-
dedbytheencoder.Inaddition,tofullyutilizetheintegraltokens
and sub-tokens in inputs, the decoder further incorporates a novel
dual copy mechanism, which can copy both integral tokens and
sub-tokensfromtheinputs.Inotherwords,ineachiteration,the
model can choose an integral token or a sub-token with highestprobability from the vocabulary or directly from the inputs. We
then describe each component in detail.
 $#
	! !#%
#

! !!#%
###
	

 $#"$ &
"
$# $#
	
!&

!#
$# $#"!"!!#
#!%
Figure 9: Architecture of the proposed model
4.1 Encoder
Given the final graph-based representation of code changes, i.e.,
Graphğ‘“ğ‘–ğ‘›ğ‘ğ‘™,theencoderfirstembedsthenodeswithanembedding
layer (i.e., in Section 4.1.1); then the graphs are represented by
embedding vectors and an adjacency matrix, which can be further
fed to a graph neural network layer (i.e., in Section 4.1.2); the final
output of the encoder is learned representation vectors for each
node, which can be further used by the decoder.
4.1.1 Embedding Layer. Formally, the final graph-based represen-
tation of code changes Graph ğ‘“ğ‘–ğ‘›ğ‘ğ‘™can be defined as G=(V,E),
whereVdenotes the nodes and Edenotes the edges in the graph.
As mentioned above, Vcontains two types of nodes, i.e., code
nodesandeditnodes.Weestablishalookuptableforbothnodes
andcovertthemtoembeddingvectorsbasedonthetable.Inpar-
ticular, the embedding vectors of code nodes can be denoted as
[ğ’„1,ğ’„2,...,ğ’„ğ‘ğ‘]and the embedding vectors of edit nodes can be de-
notedas [ğ’†1,ğ’†2,...,ğ’†ğ‘ğ‘’],whereğ‘ğ‘andğ‘ğ‘’denotethenumbersof
code nodes and edit nodes, respectively. Therefore, the embedding
vectorsğ¸forthegraphcanberepresentedby [ğ’„1,ğ’„2,...,ğ’„ğ‘ğ‘,ğ’†1,ğ’†2,...
974
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:19:36 UTC from IEEE Xplore.  Restrictions apply. ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA Jinhao Dong, Yiling Lou, Qihao Zhu, Zeyu Sun, Zhilin Li, and Wenjie Zhang, Dan Hao
,ğ’†ğ‘ğ‘’], whereğ¸âˆˆRğ‘‘ğ‘¥Ã—ğ‘,ğ‘=ğ‘ğ‘+ğ‘ğ‘’, andğ‘‘ğ‘¥denotes the em-
bedding size. Note that embedding vectors are learnable and are
initialized randomly.
4.1.2 Graph Neural Network. Theembedding vectorsare further
fedintoagraphconvolutionnetwork(GCN)layer.GCN[ 22]isa
variantofthegraphneuralnetwork(GNN)anditleveragesthefirst-
order approximation of Chebyshev Spectral CNN (ChebNet) [ 7]t o
aggregate the feature information among all neighbor nodes [43].
Hereweusean adjacencymatrix andthe embeddingvectors ğ¸
toidenticallyrepresenttheinputgraph G,sothatGCNcandirectly
processtheinput.Foranadjacencymatrix ğ´ofG(ğ´âˆˆ{0,1}ğ‘Ã—ğ‘),
ğ´ğ‘–,ğ‘—meanswhetherthereexistsanedgebetweenthenode ğ‘£ğ‘–and
the node ğ‘£ğ‘—inG. In order to preserve the information of each
node itself, we further include self-connections to each node in
the graph and obtain an enhanced adjacency matrix /tildewideğ´with self-
connections. In addition, to avoid gradient explosion caused by
accumulated degrees, we apply symmetric normalization to /tildewideğ´and
get the normalized adjacency matrix Ë†ğ´, as shown in Equation 1./tildewideğ·
denotesthedegreematrixof /tildewideğ´,whichcanbecomputedby /tildewideğ·ğ‘–ğ‘–=/summationtext.1
ğ‘—/tildewideğ´ğ‘–ğ‘—.
Ë†ğ´=/tildewideğ·âˆ’1
2/tildewideğ´/tildewideğ·âˆ’1
2. (1)
The output of GCN in the ğ‘™th iteration can be computed as
Equation 2.ğ‘Šğ‘”âˆˆRğ‘‘ğ‘¥Ã—ğ‘‘ğ‘¥is the trainable parameters. ğ‘‹ğ‘™âˆ’1denotes
theembeddingvectorsofthenodesinthelastiteration,andinitially
ğ‘‹0is the embedding vectors of all nodes (i.e., ğ¸). In addition, to
boostthelearningprocess,weemployresidualconnection[ 15]and
layernormalization[ 2]similartothetransformerarchitecture[ 39].
ğ‘‹ğ‘™=ğ‘Šğ‘”ğ‘‹ğ‘™âˆ’1Ë†ğ´, (2)
Afterğ¿iterations,thefinalrepresentationofthenodescanbe
denoted as ğ‘‹ğ¿, i.e.,ğ‘‹ğ¿âˆˆRğ‘‘ğ‘¥Ã—ğ‘.
4.2 Decoder
The decoder is built on top of a transformer architecture with a
noveldualcopymechanismforbothintegraltokensandsub-tokens.
4.2.1 Transformer Layer. Here we use the decoder part of trans-
former[39],whichisstackedbymulti-headself-attention,multi-
headattentionovertheoutputoftheencoder,andafully-connected
feed-forward network.
Forbetterillustration,wedenotetheoutputoftheencoderas ğ‘‹ğ‘’,
i.e.,ğ‘‹ğ‘’=ğ‘‹ğ¿.Thedecoderdecideseachtokeninthecommitmes-
sageiteratively,whichisbasedonboththeoutputoftheencoder
ğ‘‹ğ‘’andthecurrentlygeneratedtokens.Whengeneratingthe ğ‘˜th
token in the commit message, we denote the output of the decoder
asğ’™ğ‘˜
ğ‘‘,(i.e., ğ’™ğ‘˜
ğ‘‘âˆˆRğ‘‘ğ‘¥),whichcanbecomputedasEquation 3.For
better illustration, we use ğ‘‹ğ‘˜âˆ’1
ğ‘‘to represent the already generated
output[ğ’™1
ğ‘‘,ğ’™2
ğ‘‘,...,ğ’™ğ‘˜âˆ’1
ğ‘‘]of the decoder.
ğ’™ğ‘˜
ğ‘‘=Transformer (ğ‘‹ğ‘’,ğ‘‹ğ‘˜âˆ’1
ğ‘‘) (3)
Next, we introduce the detailed computation process of trans-
former.First,transformercomputesmulti-headself-attention(i.e.,
ğ‘ğ‘˜
ğ‘‘).ğ‘ğ‘˜
ğ‘‘istheconcatenationofmultiplesingleattention ğ‘ğ‘˜
ğ‘‘(ğ‘–),which
istheweightedsumofthealreadygeneratedoutput ğ‘‹ğ‘˜âˆ’1
ğ‘‘,asshown
in Equation 4and Equation 5.ğ‘Šğ‘„(ğ‘–)âˆˆRğ‘‘ğ‘¥Ã—ğ‘‘ğ‘¥,ğ‘Šğ¾(ğ‘–)âˆˆRğ‘‘ğ‘¥Ã—ğ‘‘ğ‘¥,ğ‘Šğ‘‰(ğ‘–)âˆˆRğ‘‘ğ‘¥Ã—ğ‘‘ğ‘¥,ğ‘Šğ‘‚âˆˆRğ‘‘ğ‘¥Ã—â„ğ‘‘ğ‘¥denote the projection parameters,
andâ„is the number of heads.
ğ’‚ğ‘˜
ğ‘‘(ğ‘–)=ğ‘Šğ‘‰(ğ‘–)ğ‘‹ğ‘˜âˆ’1
ğ‘‘Â·softmax/parenlefttpA/parenleftexA/parenleftexA
/parenleftbtA/parenleftBig
ğ‘‹ğ‘˜âˆ’1
ğ‘‘/parenrightBigğ‘‡
ğ‘Šğ¾(ğ‘–)ğ‘‡Â·ğ‘Šğ‘„(ğ‘–)ğ’™ğ‘˜âˆ’1
ğ‘‘âˆšğ‘‘ğ‘¥/parenrighttpA/parenrightexA/parenrightexA
/parenrightbtA
(4)
ğ’‚ğ‘˜
ğ‘‘=ğ‘Šğ‘‚[ğ’‚ğ‘˜
ğ‘‘(1);ğ’‚ğ‘˜
ğ‘‘(2);...;ğ’‚ğ‘˜
ğ‘‘(â„)] (5)
Second, transformer computes multi-head attention between ğ‘ğ‘˜
ğ‘‘
andtheoutputoftheencoder ğ‘‹ğ‘’,whichisdenotedas ğ‘ğ‘˜ğ‘’andshown
in Equation 6and Equation 7.
ğ’‚ğ‘˜
ğ‘’(ğ‘–)=ğ‘Šğ‘‰(ğ‘–)ğ‘‹ğ‘’Â·softmax/parenleftBigg
ğ‘‹ğ‘’ğ‘‡ğ‘Šğ¾(ğ‘–)ğ‘‡Â·ğ‘Šğ‘„(ğ‘–)ğ’‚ğ‘˜
ğ‘‘âˆšğ‘‘ğ‘¥/parenrightBigg
(6)
ğ’‚ğ‘˜
ğ‘’=ğ‘Šğ‘‚[ğ’‚ğ‘˜
ğ‘’(1);ğ’‚ğ‘˜
ğ‘’(2);...;ğ’‚ğ‘˜
ğ‘’(â„)] (7)
Third,ğ‘ğ‘˜ğ‘’passes a fully connected feed-forward network to get
theoutput ğ’™ğ‘˜
ğ‘‘,asshowninEquation 8.ğ‘Š1âˆˆRğ‘‘ğ‘¥Ã—ğ‘‘ğ‘¥,ğ‘Š2âˆˆRğ‘‘ğ‘¥Ã—ğ‘‘ğ‘¥,
ğ‘1âˆˆRğ‘‘ğ‘¥,ğ‘2âˆˆRğ‘‘ğ‘¥are trainable parameters.
ğ’™ğ‘˜
ğ‘‘=ğ‘Š2Â·max(0,ğ‘Š1ğ’‚ğ‘˜
ğ‘’+ğ‘1)+ğ‘2 (8)
ğ’™ğ‘˜
ğ‘‘is then fed to a linear layer and transformed into a |ğ‘‰|-
dimensionvector ğ’ğ‘˜ğ‘£asshowninEquation 9.|ğ‘‰|denotesthesize
of the vocabulary and ğ‘Šğ‘£âˆˆR|ğ‘‰|Ã—ğ‘‘ğ‘¥is a trainable parameter.
ğ’ğ‘˜
ğ‘£=ğ‘Šğ‘£ğ’™ğ‘˜
ğ‘‘(9)
At last, for each token in the vocabulary, the decoder calculates
its probability of being selected as the next token by passing ğ’ğ‘˜ğ‘£
to asoftmaxlayer. ğ’‘ğ‘˜ğ‘£denotes the probability distribution across
the vocabulary, and ğ’‘ğ‘˜ğ‘£(ğ‘–)denotes the probability of the ğ‘–th token
being selected, which is computed as Equation 10.
ğ’‘ğ‘˜
ğ‘£(ğ‘–)=ğ‘’ğ‘¥ğ‘{ğ’ğ‘˜ğ‘£(ğ‘–)}
/summationtext.1|ğ‘‰|
ğ‘—=1ğ‘’ğ‘¥ğ‘{ğ’ğ‘˜ğ‘£(ğ‘—)}(10)
4.2.2 Dual Copy Mechanism. To fully utilize both integral tokens
andsub-tokensduringcommitmessagegeneration,weproposeand
includeanoveldualcopymechanisminthedecoder.Inthisway,
when generating each token in the commit message, the candidate
tokens can be selected not only from the vocabulary but also from
the integral tokens or sub-tokens in the input.
More specifically, in the ğ‘˜th iteration, the probability of each
input token being copied is computed according to the current
output of the decoder (i.e., ğ’™ğ‘˜
ğ‘‘). In FIRA, we consider the input
token which is the most similar to ğ’™ğ‘˜
ğ‘‘with the highest probability
of being copied. Given an input token (i.e., a code node ğ‘£ğ‘—inG),
itssimilarityto ğ’™ğ‘˜
ğ‘‘canbecomputedbythesumofitsembedding
vector ğ’™ğ‘—
ğ‘’andtheoutputofthedecoder ğ’™ğ‘˜
ğ‘‘,asshowninEquation 11.
ğ‘Š1âˆˆRğ‘‘ğ‘¥Ã—ğ‘‘ğ‘¥,ğ‘Š2âˆˆRğ‘‘ğ‘¥Ã—ğ‘‘ğ‘¥,ğ’—âˆˆRğ‘‘ğ‘¥are learnable parameters.
ğ‘ ğ‘˜(ğ‘—)=ğ’—ğ‘‡tanh(ğ‘Š1ğ’™ğ‘˜
ğ‘‘+ğ‘Š2ğ’™ğ‘—
ğ‘’) (11)
975
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:19:36 UTC from IEEE Xplore.  Restrictions apply. FIRA: Fine-Grained Gra ph-Based Code Change Representation
for Automated Commit Message Generation ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
Thesimilarityof ğ‘˜thtoken ğ’”ğ’Œisfurtherfedtoa softmaxlayer,
which generates the probability of each input token being copied,
i.e.,ğ’‘ğ‘˜ğ‘=softmax(ğ’”ğ‘˜).
At the end of the iteration, we combine the probability distri-
butionacrossthevocabularytokens(i.e., ğ’‘ğ‘˜ğ‘£)andtheprobability
distribution across input tokens (i.e., ğ’‘ğ‘˜ğ‘) as Equation 13.ğ‘”are
learnedaccording totheoutput ofthedecoder, asshownin Equa-
tion12.ğ’˜âˆˆR1Ã—ğ‘‘ğ‘¥isthelearnableparameter.Inthisway,the ğ‘˜th
token to be selected in the commit message would be a token from
the vocabulary or copied from inputs.
ğ‘”=1
1+ğ‘’ğ‘¥ğ‘{ğ’˜ğ’™ğ‘˜
ğ‘‘}(12)
ğ’‘ğ‘˜=[ğ‘”âˆ—ğ’‘ğ‘˜
ğ‘£;(1âˆ’ğ‘”)âˆ—ğ’‘ğ‘˜
ğ‘] (13)
5 EXPERIMENTAL SETUP
5.1 Research Question
â€¢RQ1:Overalleffectiveness. HowdoesFIRAperformcom-
pared to the state-of-the-art commit message generation
techniques?
â€¢RQ2: Ablation analysis. How does each component of
FIRA contribute to the effectiveness?
â€¢RQ3: Human evaluation. How does FIRA perform from
the perspective of developers?
5.2 Dataset
Ourexperimentsareevaluatedonthewell-establishedbenchmark[ 20,
44], which has been widely used in previous commit message gen-
erationtechniques[ 16,19,29,32,41,44].Thedatasetisbasedon
the commits from top 1,000 popular Java projects in GitHub, ex-
cluding rollback/merge commits and duplicated code changes. For
eachcommit, itincludesthe firstsentenceof therelevantcommit
message. In total, the dataset contains 90,661 pairs of commits and
the relevant commit messages. Following existing work [ 44], we
randomly select 75,000 commits as the training set, 8,000 commits
asthevalidationset,andtheremaining7,661commitsasthetesting
set.
5.3 Compared Techniques
We compare FIRA with six state-of-the-art commit message gener-
ation techniques as follows.
Information retrieval-based techniques leverage informa-
tion retrieval (IR) to adopt existing commit messages from similar
code changes. We consider two representative IR-based techniques
NNGen [29] and LogGen [16] for comparison.
Learning-basedtechniques leverageneuralmachinetransla-
tion (NMT) models to generate commit messages automatically.
We consider four state-of-the-art learning-based techniques, i.e.,
CODISUM [ 44], ATOM [ 28], CoreGen [ 32], and CoRec [ 41] for
comparison.
5.4 Implementation
Representations. FIRA applies GumTree [ 11] to map ASTs of old-
version and new-version code and then to identify edit operations.GumTree [ 11] is a representative AST mapping algorithm and has
been widely adopted in various tasks [5, 14,18,24,26,31].
Model.Intheencoder,wesetthesizeoftheinputgraphs(i.e.,the
maximumnumberofnodes)upto650,containingupto370code
nodes and 280 edit nodes, which is more than the number of the
graph nodes of each training data so that the largest graphs in the
trainingsetcanbeincluded.Inthedecoder,wesetthemaximum
length of each commit message as 30, which is longer than the
length of all commit messages in the training set. For the hyper-parameters, we configure the six-layer GNN with 0.20 dropout
rate[36],andthesix-layereight-headtransformerwith0.10dropout
rate and 256-dimension hidden states. In the training phase, we
adopt the cross-entropy loss function and the Adam optimizer [ 21]
with 0.0001 learning rate. We tune these hyper-parameters and
select the best performing model in the validation set.
Compared techniques. Wedirectly reusethe implementations of
the compared techniques from their reproducible packages, if their
packages are available and executable [ 28,29,32,41]; otherwise,
we re-implement the techniques strictly following the description
in their papers.
Environment. The experiments are performed on a Dell worksta-
tion with Intel Xeon CPU E5-2680 v4 @ 2.40GHz, running Ubuntu
16.04.6 LTS.The models aretrained ontwo 24G GPUsof GeForce
RTX 3090 and two 24G GPUs of NVIDIA TITAN RTX.
5.5 Evaluation Metrics
We use the commit messages (i.e., manually written by developers)
inthedatasetasthegroundtruth.Inparticular,givenacodechange,
we compare the similarity between the generated commit message
withthegroundtruth.Followingpreviousworkoncommitmessage
generation[ 16,19,28,29,32,41,44],weusethewidely-usedmetrics,
BLEU, ROUGE-L, and METEOR to measure the similarity. Their
computation details are presented as follows.
BLEUmeasures the precision of generated sequences by calcu-
lating its average of the modified n-gram precision (i.e., 1-gram,2-grams, 3-grams and 4-grams forBLEU-4) [
33]. The modifiedn-
gramprecisionreferstotheratioofthenumberofmatchedn-grams
to the number of all the n-grams in the generated sequence. So far,
researchers have proposed several variants of BLEU. According to
a recent human study [ 38], the B-Norm BLEU exhibits the most
consistentlywithhumanjudgementsonthequalityofcommitmes-
sages. Therefore, in this paper, we use B-Norm BLEU as one of the
metrics.
ROUGE-L calculatestheF-scoreofprecisionandrecallbasedon
thelongestcommonsub-sequences(LCS)betweenthegenerated
sequence and the ground truth [ 25]. A longer LCS indicates the
higher similarity between two sentences.
METEOR calculates the harmonic mean of 1-gram precision
and 1-gram recall of the generated sequence against the ground
truth [3]. Italso includesa penaltymechanism whenthe matched
tokens are not adjacent.
6 RESULTS AND ANALYSIS
In this section, we first present the overall results of FIRA (RQ1) in
Section6.1, the results of the ablation study (RQ2) in Section 6.2
and human evaluation (RQ3) in Section 6.3.
976
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:19:36 UTC from IEEE Xplore.  Restrictions apply. ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA Jinhao Dong, Yiling Lou, Qihao Zhu, Zeyu Sun, Zhilin Li, and Wenjie Zhang, Dan Hao
Table 1: Overall commit message generation results
Model BLEU ROUGE-L METEOR
LogGen [16] 8.95 10.50 8.34
NNGen [29] 9.16 11.24 9.53CoreGen [32] 14.15 18.22 12.90CODISUM [44] 16.55 19.73 12.83ATOM [28] 8.35 10.17 8.73CoRec [41] 13.03 15.47 12.04
FIRA 17.67 21.58 14.93
6.1 RQ1: Overall Effectiveness
Table1presentsaverageofBLEU,ROUGE-LandMETEORonall
the commit messages generated by FIRA and compared techniques.Figure10furthershowsthedistributionofROUGE-Linaboxplot.
Figure 10: Box plot of ROUGE-L
As shown by the table and the figure, FIRA outperforms all the
comparedtechniquesincludingthebestIR-basedtechniqueNNGen
and the best learning-based technique CODISUM on all metrics,
indicatingeffectivenessbothinprecision(i.e.,BLEU,ROUGE-LandMETEOR)andrecall(i.e.,ROUGE-LandMETEOR).Forexample,theimprovementsachievedbyFIRArangefrom7%to112%,9%to112%,
and 16% to 79% in BLEU, ROUGE-L, and METEOR, respectively. In
addition, it is notable that the IR-based approaches LogGen and
NNGenperformsignificantlyworse.Apotentialreasonisthatthese
techniquescanonlyretrieveexistingmessagesfromtheretrieved
database instead of generating new commit messages. In other
words, they are no longer effective, once there exists no similar
code change as the given code change.
Table 2: Penalty-BLEU of all approaches
Model LogGen NNGen CoreGen CODISUM ATOM CoRec FIRA
Penalty-BLEU 7.15 8.07 11.15 12.07 7.42 10.49 13.30
BLEU may overrate the precision of the cases that the actual
number of the matched n-grams is small but the length of the
commitmessageisevenshorter,whichmayresultinbiasedaverageofallcommitmessages.Therefore,forthoseshortcommitmessages,
we introduce a penalty mechanism by multiplying their originalBLEU with a penalty factor, to reduce their impact on the final
averagescore.Thepenaltyfactor ğ‘“ğ‘–oftheğ‘–-thcommitmessagecan
be computed as Equation 14, which is the ratio of the length of the
ğ‘–-th ground truth commit message to the total length of all ground
truth commit messages.
ğ‘“ğ‘–=ğ‘™ğ‘’ğ‘›(ğ‘šğ‘ ğ‘”ğ‘–)//summationdisplay.1
ğ‘—ğ‘™ğ‘’ğ‘›(ğ‘šğ‘ ğ‘”ğ‘—) (14)
WedenotetheBLEUwithanenhancedpenaltyaspenalty-BLEU
fordistinction.Table 2presentsthepenalty-BLEUofallapproaches.
From the table, we can find that our approach also outperforms
other approaches in terms of the penalty-BLEU, indicating FIRA is
consistently effective on generating commit messaging of different
lengths.
In summary, our quantitative results show that FIRA outper-
formsallsixcomparedtechniquesintermsofallstudiedmetrics;
meanwhile FIRA is consistently effective on generating commit
messages of different lengths.
6.2 RQ2: Ablation Study
Inthissection,wefurtherperformanablationstudytoinvestigate
the effectiveness of each component in FIRA. The major noveltyof FIRA is explicitly including and analyzing (1) edit operations
between old and new versions, and (2) copying sub-tokens with a
dual copy mechanism. Therefore, to investigate their contribution,
we further build two variants of FIRA by (1) removing the edit
operationsfromthecodechangerepresentationgraph(i.e.,denoted
as FIRA ğ‘’ğ‘‘ğ‘–ğ‘¡âˆ’), and (2) degrading the dual copy mechanism into
single copy mechanism for integral tokens which cannot copysub-tokens anymore (i.e., denoted as FIRA
ğ‘ ğ‘¢ğ‘âˆ’). In addition, we
build a naive model by removing both components (i.e., denoted
as FIRA ğ‘ğ‘œğ‘¡â„âˆ’) for comparison. Table 3presents the effectiveness of
thedefaultFIRAandvariants.Inthefollowingsections,wethenanalyze the contribution of each component quantitatively and
qualitatively.
Table 3: Results of the ablation study
Model BLEU ROUGE-L METEOR
FIRAğ‘’ğ‘‘ğ‘–ğ‘¡âˆ’17.39 21.15 14.54
FIRAğ‘ ğ‘¢ğ‘âˆ’17.36 20.97 14.09
FIRAğ‘ğ‘œğ‘¡â„âˆ’16.82 20.15 13.42
FIRA 17.67 21.58 14.93
6.2.1 Contribution of edit operations. As shown in Table 3, the
effectiveness of FIRA ğ‘’ğ‘‘ğ‘–ğ‘¡âˆ’becomes worse in terms of all three
metrics, indicating that including edit operations is helpful for
commit message generation.
We further look into some cases that FIRA ğ‘’ğ‘‘ğ‘–ğ‘¡âˆ’exhibits less
effective than the default FIRA in terms of these metrics. Figure 11
presentssuchareal-worldcase,whichincludesthecodechanges,
the ground truth, the commit messages generated by the defaultFIRA, FIRA
ğ‘’ğ‘‘ğ‘–ğ‘¡âˆ’, and other compared techniques, including the
bestIR-basedtechniqueNNGenandthebestlearning-basedtech-
niqueCODISUM.Inthisexample,developersrenamethemethod
from â€œ getInputEventListener â€t oâ€œ getInputEventHandler â€i n
977
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:19:36 UTC from IEEE Xplore.  Restrictions apply. FIRA: Fine-Grained Gra ph-Based Code Change Representation
for Automated Commit Message Generation ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
return mInputEventHandler;
}throw new IllegalStateException("Fragment InputEventListener
already present");
}mFragment.setInputEventHandler(mInputEventHandler);
@@ -264,7 +264,7 @@ public class PlaybackOverlayFragment extends 
DetailsFragment {          @@ -290,7 +290,7 @@ public abstract class PlaybackControlGlue {
throw new IllegalStateException("Fragment              OnItemViewClickedListener already present");             
}mFragment.setOnItemViewClickedListener(mOnItemViewClickedListener);
+  if (mFragment. getInputEventHandler () != null) {
- public final InputEventHandler getInputEventListener () {
+  public final InputEventHandler getInputEventHandler () {- if (mFragment. getInputEventListener () != null) {



""! 



 !

	

 



 !

	

 $!!


#"!#   	$!%%!"
Figure 11: Case analysis: edit operations
two involving files, and the manual commit message exactly de-
scribes such edit operations. As shown in the figure, the default
FIRAcanpreciselygeneratetheexactlysamemessageasdevelopers,
whereasafterremovingexplicitrepresentationofeditoperations
FIRAğ‘’ğ‘‘ğ‘–ğ‘¡âˆ’failstogeneratesuchcorrectmessage.Inaddition,we
canobservethatothercomparedtechniquescannotgeneratethe
precisecommitmessageneither,sincenoneofthemrepresentsedit
operationsexplicitly.Suchobservationsfurtherconfirmourintu-
itionthatrepresentingeditoperationsexplicitlycanhelpthemodel
to capture the fine-grained code changes, enabling more precise
commitmessagegeneration.Onthecontrary,iftheold-versionand
new-version code are represented in combination without high-lighting their differences, the model has to learn to capture such
edit operations by itself, which can be challenging especially when
there are only a few tokens changed.
import java.awt.event.*;
import java.util.*;
@@ -39,6 +40,8 @@ public class TransferCallDialog {
this.setOkButtonText(GuiActivator.getResources()
.getI18NString("service.gui.TRANSFER"));
+this.setMinimumSize (new Dimension(300, 300));
addOkButtonListener(new ActionListener(){
public void actionPerformed(ActionEvent e)@@ -6,6 +6,7 @@
package net.java.sip.communicator.impl.gui.main.call;
+import java.awt.*;



  
 
 
	

 
 

#


!   "   
	##
Figure 12: Case analysis: copying sub-tokens
6.2.2 Contribution of copying sub-tokens. AsshowninTable 3,the
performanceofFIRA ğ‘ ğ‘¢ğ‘âˆ’declines,indicatingthedualcopymecha-
nism for sub-tokens indeed boosts commit message generation.
WefurtherlookintosomecasesthatFIRA ğ‘ ğ‘¢ğ‘âˆ’exhibitslessef-
fectivethanthedefaultFIRAintermsofthesemetrics.Figure 12
presentssuchareal-worldcaseinourdataset,includingthecode
changes, the ground truth, the commit messages generated by the
default FIRA, FIRA ğ‘ ğ‘¢ğ‘âˆ’, and other compared techniques. In the ex-
ample, the developer commit message contains several sub-tokensinthenewly-addedcode(i.e., setMinimumSize )andtheintegralto-
keninitsbelongingclassnameâ€œ TransferCallDialog â€.Asshown
in the figure, the default FIRA can effectively utilize sub-tokens in
theinputcode,whiletheFIRA ğ‘ ğ‘¢ğ‘âˆ’withoutthedualcopymecha-
nismisincapableofcopyingtheinfrequentsub-token(i.e., minimum)
tothecommitmessage.Inaddition,othercomparedtechniquesfail
to generate precise commit messages neither. For example, NNGen
generates a completely irrelevant commit message. For CODISUM,
it includes only two sub-tokens in the generated commit message
and generates the commit message with poor readability. Since
CODISUM only leverages a single copy mechanism for integral
token,itcanonlygeneratethefrequentsub-tokensfromthevocab-
ulary (e.g., two successfully generated sub-tokens setandsize)
butfailstogeneratetheinfrequentsub-token(e.g., minimum)that
is excluded in the vocabulary or seldom occurs in the training set.
Table 4: Results for copying sub-tokens
Model Copy Ratio (%) #Different Sub-tokens Occurrence Frequency
NNGen 10.53 436 689
CODISUM 3.77 115 1097
FIRAğ‘ ğ‘¢ğ‘âˆ’ 5.40 159 1118
FIRA 11.95 454 643
Toconfirmtheexplanationabove,wefurtherinvestigatewhether
dual copy mechanism has correctly copied sub-tokens into commit
messages.Inparticular,wedenotethesub-tokenappearsbothintheinputcodechangeandthecommitmessageasa copytoken.Wethen
compute the ratio of the number of correctly-copied copy tokens
tothenumberofall copytokens inourtestingset.Ahigherratio
indicatesthetechniqueismoreeffectiveincopyingsub-tokens.We
alsopresentthenumberofdifferentcorrectly-copiedsub-tokens.
In addition, we further present the average number of times of the
correctly-copied sub-tokens occurring in the training messages,
whichcanreflecttheoccurrencefrequencyofthesub-tokens.Ta-
ble4presents the results. Based on the table, we can notice that
FIRA can correctly copy more sub-tokens than other techniques,
andFIRAcancopymoreinfrequentsub-tokens.Furthermore,we
notice that only FIRA can copy the sub-tokens never occurringin the training set. Without the dual copy mechanism, the per-formance of FIRA
ğ‘ ğ‘¢ğ‘âˆ’declines a lot. We can notice that NNGen
performs well in terms of copying sub-token, because instead of
generatingnewcommitmessages,IR-basedtechniquesselectex-
isting messages based on the similarity of the code changes and
similarcodechangesmayhavecommonsub-tokens.However,note
that the overall performance of IR-based techniques (i.e., as shown
inTable1)ismuch worsethanFIRA (i.e.,9.16 v.s17.67in BLEU).
In summary, the results further indicate FIRA can copy sub-tokens
effectively.
6.3 RQ3: Human Evaluation
Tofurtherstudythequalityofgeneratedcommitmessagesfromthe
perspectiveofdevelopers,weperformahumanstudytoevaluate
the commit messages generated by FIRA and compared techniques.
We compare FIRA with the best retrieval-based technique NNGen
and the best learning-based technique CODISUM. We invite six
978
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:19:36 UTC from IEEE Xplore.  Restrictions apply. ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA Jinhao Dong, Yiling Lou, Qihao Zhu, Zeyu Sun, Zhilin Li, and Wenjie Zhang, Dan Hao
developers1toparticipateinthisstudy,whohaveindustrialexperi-
ence in Java programming language ranging from 3 to 5 years.
Table 5: Scoring criterion
ScoreDefinition
0Neitherrelevant in semantic nor having shared tokens.
1Irrelevant in semantic but with some shared tokens.
2Partiallysimilar in semantic, but each contains exclusive information.
3Highlysimilar but not identical in semantic.
4Identical in semantic.
6.3.1 Study Design. Following previous work [ 28,41], we ran-
domlyselect100commitsfromthetestingsetanddesignaques-
tionnaireformanualevaluation.Foreachcommit,thequestionnaire
includesthecodechange,thegroundtruthcommitmessage,and
thecommitmessagesgeneratedbyFIRAaswellasthecompared
techniques(i.e.,thebestIR-basedtechniqueNNGenandthebest
learning-basedtechniqueCODISUM).Eachinvitedparticipantis
asked to score the commit messages generated by three techniques
(i.e., FIRA, CODISUM, and NNGen) based on the code changes and
thegroundtruthcommitmessage.Thescorerangesfrom0to4,and
a higher score indicates a higher similarity between the generated
commitmessage and thegroundtruth.Wefollowtheexistingscor-ingcriterion[
28,29],anddetaileddefinitionisshowninTable 5.T o
avoidbias,allthreetechniquesareanonymousinthequestionnaire
and each participant fills in the questionnaire separately.
Table 6: Results of the human evaluation
Model Low (%) Medium (%) High (%) Average Score
NNGen 71.3 13.2 15.5 0.98
CODISUM 38.0 19.8 42.2 2.06
FIRA 35.5 20.3 44.2 2.15
6.3.2 Results. For each technique, we measure the quality of its
generatedcommitmessagebasedontheaveragescoresofsixpartic-
ipants on that commit message. In particular, in line with previous
work [28,29], we regard the commit messages scored 0 and 1 as
low-quality, scored 2 as medium-quality, and scored 3 and 4 as
high-quality. Table 6presents the ratio of commit messages of dif-
ferentquality.Asshowninthetable,alargeproportion(i.e.,44.2%)
of commit messages generated by FIRA are considered as high-quality by the participants. In addition, FIRA exhibits the largestratio of high-quality commit messages while the lowest ratio of
low-quality commit messages. The average score also indicates the
out-performance of FIRA over compared techniques. To confirmour observations, we further conducted a Wilcoxon signed-rank
test [42] between the scores of FIRA and the other techniques. The
results further confirm that difference between the scores of FIRA
and NNGen/CODISUM is statistically significant at the confidence
level of 95%.
6.3.3 Successful cases. We further present two cases that FIRA
achieveshigherscoresinFigure 13andFigure 14.Eachfigurein-
cludesthecodechange,thegroundtruth,andthecommitmessages
1None of them are co-authors of this paper.@@ -88,6 +88,7 @@ public class DeepLearningAutoEncoderTest extends 
TestUtil {
// cleanup
mymodel.delete();
frame.delete();p.delete();l2_frame.delete();
@@ -95,7 +96,6 @@ public class DeepLearningAutoEncoderTest extends 
TestUtil {
reconstructed.delete();((Frame)DKV.get(Key.make("Difference")).get()).delete();diff.delete();
}
}




!! " 

	
	"

	 
!  
 
	"  +    frame.add("dummy", resp);
- resp.remove(null);
Figure 13: Example of fixing memory leak
generatedbyFIRAandthecomparedtechniques,i.e.,NNGenand
CODISUM.
ThefirstexampleinFigure 13showsthecodechangesforfixinga
memory-leakbug.Inparticular,theold-versioncodeâ€œ resp.remove
(null);â€ fails to delete the object â€œ respâ€, which results in a mem-
ory leak; while the new-version code puts â€œ respâ€ as a member
of â€œframeâ€, which can successfully delete â€œ respâ€ once â€œ frameâ€i s
deleted.Asshowninthefigure,FIRAsuccessfullypredictsthein-
tention of fixing memory leak and also the location of where mem-
oryleakoccurs,whichweconsiderasaprecisecommitmessage;
whereas, other approaches fail to generate such commit messages.
Locale locale = MetricsUtils.getMetricsReporterLocale(stormConf);
if (locale != null) {
builder.formattedFor(locale);@@ -37,11 +36,7 @@ public class ConsolePreparableReporter implements 
PreparableReporter<ConsoleRepo
LOG.debug("Preparing...");ConsoleReporter.Builder builder =
ConsoleReporter.forRegistry(metricsRegistry);
+  builder.outputTo(System.out);- PrintStream stream = System.out;- if (stream != null) {- builder.outputTo(stream);-}
	
 ! 
  
 ! 


	 
	 !
Figure 14: Example of removing unnecessary null check
The second example in Figure 14shows the code changes for re-
movinganunnecessarynullcheck,sincetheobjectâ€œ streamâ€never
becomes null. As shown in the figure, FIRA appears to capture the
functionality ofchanged code(i.e., null check)and thusgenerates
similarmessagewiththegroundtruth.Incontrast,thecompared
approaches fail to generate proper descriptions for the given code
change: NNGen generates completely irrelevant message while
CODISUM generates over-general and uninformative message.
7 DISCUSSION
7.1 Threats to Validity
Theinternalthreattovalidityliesintheimplementationofcom-
pared techniques and our approach. To reduce this threat, we di-
rectly reuse the implementationof the compared techniques from
theirreproduciblepackages,iftheirpackagesareavailableandexe-
cutable [28,29,32,41]; otherwise, we re-implement the techniques
979
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:19:36 UTC from IEEE Xplore.  Restrictions apply. FIRA: Fine-Grained Gra ph-Based Code Change Representation
for Automated Commit Message Generation ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
strictlyfollowingtheirpapers.Wealsobuildourapproachbasedon
existingmature tools/libraries, suchasGumTree[ 11].Inaddition
to code review, we also sampled 100 data items from our dataset
andmanuallyensurethereisnoviolationcasewherecamelcase
or snake case is not applicable.
The external threat to validity lies in the dataset used in the ex-
periment. Tomitigate thisthreat, weuse a well-establisheddataset,
which has been constructed on popular Java projects from GitHub
and well-cleaned by previous work [20, 44].
The construct threat lies in the metrics used in evaluation. To
reduce this threat, we adopt several metrics that have been widely
usedbypriorworkoncommitmessagegeneration[ 16,19,28,29,
32,41,44]. In addition, we further perform a human evaluation
to evaluate the effectiveness from the perspective of developers.
We strictly follow the procedure of previous work [ 28,41] and
inviteexperienceddevelopers,soastoreducethethreatsinhuman
evaluation (e.g., the limited number of participants [23]).
7.2 Limitations
ThesectiondiscussesthelimitationsinFIRA.First,ourapproach
would be less effective when the code change cannot be parsed
into valid AST. In this case, FIRA would utilize only sub-token
identifiers and sequential information during learning. Such cases
areactuallynotobservedinourdatasetaccordingtoourmanual
inspection, which however are still p ossible in practice. Second,
when the training set contains highly-similar code changes as the
givenoneandthefrequencyofthesesimilardataitemsisquitelow
(e.g.,onlyonce),FIRAislesseffectivethantheretrieval-basedap-
proaches. It isa common drawback forlearning-based techniques,
since retrieval-based approaches can inherently retrieve the cor-
rect commit message for the similar inputs from the training set.
Third,whenthecommitmessagecontainstokensabsentfromboth
vocabulary and the input code change, FIRA would fail to generate
these tokens in the commit message.
8 RELATED WORK
Theexistingworkoncommitmessagegenerationcanbecatego-
rized as template-based, information retrieval-based, and learning-
based techniques.
Thetemplate-basedtechniques[ 4,6,35]analyzecodechanges
andgeneratecommitmessageswithpre-definedpatterns.Forex-
ample, Buse and Weimer [4]design pre-defined templates based
onpathpredicates,whileCortÃ©s-Coyetal .[6]proposetemplates
based on method stereotypes [ 9] and commit stereotypes [ 8]. In
general, the template-based techniques tend to describe what ischanged but has weak capability of capturing the rationales and
purposesofcodechanges.Inaddition,theyareeffectiveonlywhen
the cases perfectly fit with the pre-defined rules, but cannot be
general due to the diversity of commit messages.
Theinformationretrieval-basedapproaches[ 16,17,29]leverage
IRtechniquestoadoptexistingcommitmessagesfromsimilarcodechanges.Forexample,givenacodechangeasaquery,Liuetal
.[29]
leveragecosinesimilarityandBLEUtoselectamostsimilarcode
changefromthetrainingset;similarly,Huangetal .[17]useboth
syntaxsimilarityandsemanticsimilarityasthesimilaritymetric.
However, IR-based techniques are no longer effective once there isno similar code change in the retrieved database and they can only
output existing commit messages instead of generating new ones.
More recently, researchers propose to leverage advanced learn-
ing techniques in commit message generation. The learning-based
techniques[ 19,27,28,32,41,44]regardcommitmessagegenera-
tionasatranslationproblem,andadoptneuralmachinetranslation
(NMT) models to generate commit message for the given code
change.Existing learning-basedtechniquesfirst representtheold-
version and new-version code with specific formats respectively,
such as sequences of tokens [ 32] or paths of abstract syntax tree
(AST)[28],concatenatebothrepresentations,andgeneratecommit
messagesviadifferentlearningmodels.Thecoderepresentations
in existing learning-based techniques are coarse-grained, since (1)
they represent the code changes by simply putting old-version and
new-version code together, and thus edit operations have to be
learned by models, and (2) they only focus on integral tokens with-
outindividuallydescribingsub-tokens,andthuscommitmessage
with infrequent sub-tokens cannot be generated. To address these
limitations, we propose a fine-grained graph-based representation
for code changes to enable more powerful commit message genera-
tion.Inadditiontothecodechangerepresentation,weproposea
novel model that is different from prevision work. In particular, we
leverage a graph neural network in the encoder so as to directly
encodethegraph-structuredinputs;andweequipthedecoderwith
thetransformerandanoveldualcopymechanism,whichcannot
only generate tokens from the vocabulary but also directly copy
both integral tokens and sub-tokens from the input.
9 CONCLUSION
In this work, we propose a novel commit message generation tech-
nique,FIRA,whichfirstrepresentscodechangesviafine-grained
graphsandthenlearnstogeneratecommitmessagesautomatically.Comparedtopreviouscodechangerepresentations,FIRAexplicitly
describes the edit operations between the old-version and new-
version code, along with tokens at different granularities. Based
on the proposed graph-based representations, FIRA generates com-
mit messages by a generation model. FIRA incorporates the graph
neuralnetworkintheencodersoastodirectlyencodethegraph-
structured inputs; and the decoder incorporates the transformerand a novel dual copy mechanism, which can not only generatetokens from the vocabulary but also directly copy both integraltokens and sub-tokens from the input. We perform an extensive
evaluationtocompareFIRAwithsixcommitmessagetechniquesonawidely-usedbenchmark.TheresultsshowthatFIRAoutperformsallcomparedtechniquesintermsofBLEU,ROUGE-L,andMETEOR.
WefurtheranalyzetheeffectivenessofeachcomponentinFIRAby
anablationstudyandcaseanalysis.Theresultsfurtherconfirmthat
majorcomponentsbothpositivelycontributetotheeffectiveness
ofFIRA.Inaddition,wefurtherperformahumanstudytoevaluate
the quality of generated commit messages from the perspective of
developers, which consistently shows the effectiveness of FIRA.
ACKNOWLEDGMENTS
This work was supported by National Natural Science Foundation
of China under Grant No. 61872008. We are grateful for Jiashuo
Liang for his help on the implementation of the transformer.
980
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:19:36 UTC from IEEE Xplore.  Restrictions apply. ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA Jinhao Dong, Yiling Lou, Qihao Zhu, Zeyu Sun, Zhilin Li, and Wenjie Zhang, Dan Hao
REFERENCES
[1]WasiUddinAhmad,SaikatChakraborty,BaishakhiRay,andKai-WeiChang.2020.
A transformer-based approach for source code summarization. arXiv preprint
arXiv:2005.00653 (2020).
[2]Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. 2016. Layer normaliza-
tion.arXiv preprint arXiv:1607.06450 (2016).
[3]Satanjeev Banerjee and Alon Lavie. 2005. METEOR: An automatic metric for
MT evaluation withimproved correlation with humanjudgments. In Proceedings
of the acl workshop on intrinsic and extrinsic evaluation measures for machine
translation and/or summarization. 65â€“72.
[4]Raymond PL Buse and Westley R Weimer. 2010. Automatically documenting
program changes. In Proceedings of the IEEE/ACM international conference on
Automated software engineering. 33â€“42.
[5]Eduardo C Campos and Marcelo de A Maia. 2019. Discovering common bug-fix
patterns:Alarge-scaleobservationalstudy. JournalofSoftware:Evolutionand
Process31, 7 (2019), e2173.
[6]Luis Fernando CortÃ©s-Coy, Mario Linares-VÃ¡squez, Jairo Aponte, and DenysPoshyvanyk. 2014. On automatically generating commit messages via sum-marization of source code changes. In 2014 IEEE 14th International Working
Conference on Source Code Analysis and Manipulation. IEEE, 275â€“284.
[7]MichaÃ«l Defferrard, Xavier Bresson, and Pierre Vandergheynst. 2016. Convo-
lutionalneuralnetworksongraphswithfastlocalizedspectralfiltering. arXiv
preprint arXiv:1606.09375 (2016).
[8]Natalia Dragan, Michael L Collard, Maen Hammad, and Jonathan I Maletic. 2011.
Using stereotypes to help characterize commits.In 2011 27th IEEE International
Conference on Software Maintenance (ICSM). IEEE, 520â€“523.
[9]Natalia Dragan, Michael L Collard, and Jonathan I Maletic. 2006. Reverse en-gineering method stereotypes. In 2006 22nd IEEE International Conference on
Software Maintenance. IEEE, 24â€“34.
[10]RobertDyer,HoanAnhNguyen,HrideshRajan,andTienNNguyen.2013. Boa:A
language and infrastructure for analyzing ultra-large-scale software repositories.
In201335thInternationalConferenceonSoftwareEngineering(ICSE).IEEE,422â€“
431.
[11]Jean-RÃ©my Falleri, FlorÃ©al Morandat, Xavier Blanc, Matias Martinez, and Martin
Monperrus. 2014. Fine-grained and accurate source code differencing. In Pro-
ceedings of the 29th ACM/IEEE international conference on Automated software
engineering. 313â€“324.
[12]MarcoGori,GabrieleMonfardini,andFrancoScarselli.2005. Anewmodelfor
learningingraphdomains.In Proceedings.2005IEEEInternationalJointConference
on Neural Networks, 2005., Vol. 2. IEEE, 729â€“734.
[13]WilliamLHamilton,RexYing,andJureLeskovec.2017. Inductiverepresentation
learningonlargegraphs.In Proceedingsofthe31stInternationalConferenceon
Neural Information Processing Systems. 1025â€“1035.
[14]QuinnHanam,FernandoSdeMBrito,andAliMesbah.2016. Discoveringbug
patternsinJavaScript.In Proceedingsofthe201624thACMSIGSOFTinternational
symposium on foundations of software engineering. 144â€“156.
[15]KaimingHe,XiangyuZhang,ShaoqingRen,andJianSun.2016. Deepresidual
learning for image recognition. In Proceedings of the IEEE conference on computer
vision and pattern recognition. 770â€“778.
[16]Thong Hoang, Hong Jin Kang, David Lo, and Julia Lawall. 2020. CC2Vec: Dis-
tributedrepresentationsofcodechanges.In ProceedingsoftheACM/IEEE42nd
International Conference on Software Engineering. 518â€“529.
[17]Yuan Huang, Nan Jia, Hao-Jie Zhou, Xiang-Ping Chen, Zi-Bin Zheng, and Ming-
DongTang.2020.LearningHuman-WrittenCommitMessagestoDocumentCode
Changes. Journal of Computer Science and Technology 35, 6 (2020), 1258â€“1277.
[18]Md Rakibul Islam and Minhaz F Zibran. 2020. How bugs are fixed: exposing
bug-fix patterns with edits and nesting levels. In Proceedings of the 35th Annual
ACM Symposium on Applied Computing. 1523â€“1531.
[19]Siyuan Jiang, Ameer Armaly, and Collin McMillan. 2017. Automatically generat-
ing commit messages from diffs using neural machine translation. In 2017 32nd
IEEE/ACM International Conference on Automated Software Engineering (ASE).
IEEE, 135â€“146.
[20]Siyuan Jiang and Collin McMillan. 2017. Towards automatic generation of
short summaries of commits. In 2017 IEEE/ACM 25th International Conference on
Program Comprehension (ICPC). IEEE, 320â€“323.
[21]Diederik P Kingma and Jimmy Ba. 2014. Adam: A method for stochastic opti-
mization. arXiv preprint arXiv:1412.6980 (2014).
[22]ThomasNKipfandMaxWelling.2016. Semi-supervisedclassificationwithgraph
convolutional networks. arXiv preprint arXiv:1609.02907 (2016).
[23] Russell V Lenth. 2001. Some practical guidelines for effectivesample size deter-
mination. The American Statistician 55, 3 (2001), 187â€“193.
[24]Shanshan Li, Xu Niu, Zhouyang Jia, Ji Wang, Haochen He, and Teng Wang. 2018.
Logtracker: Learning log revision behaviors proactively from software evolution
history.In Proceedingsofthe26thConferenceonProgramComprehension.178â€“188.
[25]Chin-Yew Lin and Franz Josef Och. 2004. Automatic evaluation of machine
translationqualityusinglongestcommonsubsequenceandskip-bigramstatistics.
InProceedingsofthe42ndAnnualMeetingoftheAssociationforComputationalLinguistics (ACL-04). 605â€“612.
[26]KuiLiu,DongsunKim,AnilKoyuncu,LiLi,TegawendÃ©FBissyandÃ©,andYves
LeTraon.2018. Acloserlookatreal-worldpatches.In 2018IEEEInternational
Conference on Software Maintenance and Evolution (ICSME). IEEE, 275â€“286.
[27]QinLiu,ZiheLiu,HongmingZhu,HongfeiFan,BowenDu,andYuQian.2019.
Generatingcommitmessagesfromdiffsusingpointer-generatornetwork.In 2019
IEEE/ACM16thInternationalConferenceonMiningSoftwareRepositories(MSR).
IEEE, 299â€“309.
[28]Shangqing Liu, Cuiyun Gao, Sen Chen, Nie Lun Yiu, and Yang Liu. 2020. ATOM:
Commitmessagegenerationbasedonabstractsyntaxtreeandhybridranking.
IEEE Transactions on Software Engineering (2020).
[29]Zhongxin Liu, Xin Xia, Ahmed E Hassan, David Lo, Zhenchang Xing, and Xinyu
Wang. 2018. Neural-machine-translation-based commit message generation:
how far are we?. In Proceedings of the 33rd ACM/IEEE International Conference on
Automated Software Engineering. 373â€“384.
[30]Yiling Lou, Qihao Zhu, Jinhao Dong, Xia Li, Zeyu Sun, Dan Hao, Lu Zhang, and
Lingming Zhang. 2021. Boosting coverage-based fault localization via graph-
basedrepresentationlearning.In Proceedingsofthe29thACMJointMeetingon
European Software Engineering Conference and Symposium on the Foundations of
Software Engineering. 664â€“676.
[31]Zhen Ni, Bin Li, Xiaobing Sun, Tianhao Chen, Ben Tang, and Xinchen Shi. 2020.
Analyzing bug fix for automatic bug cause classification. Journal of Systems and
Software163 (2020), 110538.
[32]Lun Yiu Nie, Cuiyun Gao, Zhicong Zhong, Wai Lam, Yang Liu, and Zenglin
Xu.2021. CoreGen:ContextualizedCodeRepresentationLearningforCommit
Message Generation. Neurocomputing 459 (2021), 97â€“107.
[33]Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. Bleu: amethod for automatic evaluation of machine translation. In Proceedings of the
40th annual meeting of the Association for Computational Linguistics. 311â€“318.
[34]FrancoScarselli,MarcoGori,AhChungTsoi,MarkusHagenbuchner,andGabriele
Monfardini. 2008. The graph neural network model. IEEE transactions on neural
networks 20, 1 (2008), 61â€“80.
[35]JinfengShen,XiaobingSun,BinLi,HuiYang,andJiajunHu.2016. Onautomatic
summarization of what and why information in source code changes. In 2016
IEEE40thAnnualComputerSoftwareandApplicationsConference(COMPSAC),
Vol. 1. IEEE, 103â€“112.
[36]Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan
Salakhutdinov.2014. Dropout:asimplewaytopreventneuralnetworksfrom
overfitting. The journal of machine learning research 15, 1 (2014), 1929â€“1958.
[37]ZeyuSun,QihaoZhu,YingfeiXiong,YicanSun,LiliMou,andLuZhang.2020.
Treegen:Atree-basedtransformerarchitectureforcodegeneration.In Proceed-
ings of the AAAI Conference on Artificial Intelligence, Vol. 34. 8984â€“8991.
[38]WeiTao,YanlinWang,EnshengShi,LunDu,HongyuZhang,DongmeiZhang,
and Wenqiang Zhang. 2021. On the Evaluation of Commit Message Generation
Models: An Experimental Study. arXiv preprint arXiv:2107.05373 (2021).
[39]Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all
youneed. arXiv preprint arXiv:1706.03762 (2017).
[40]Petar VeliÄkoviÄ‡,Guillem Cucurull,Arantxa Casanova,Adriana Romero,Pietro
Lio, and Yoshua Bengio. 2017. Graph attention networks. arXiv preprint
arXiv:1710.10903 (2017).
[41]Haoye Wang, Xin Xia, David Lo, Qiang He, Xinyu Wang, and John Grundy.
2021. Context-aware Retrieval-based Deep Commit Message Generation. ACM
Transactions on Software Engineering and Methodology (TOSEM) 30, 4 (2021),
1â€“30.
[42]FrankWilcoxon.1992. Individualcomparisonsbyrankingmethods. In Break-
throughs in statistics. Springer, 196â€“202.
[43]Zonghan Wu, Shirui Pan, Fengwen Chen, Guodong Long, Chengqi Zhang, and
S Yu Philip. 2020. A comprehensive survey on graph neural networks. IEEE
transactions on neural networks and learning systems (2020).
[44]Shengbin Xu, Yuan Yao, Feng Xu, Tianxiao Gu, Hanghang Tong, and Jian Lu.
2019. Commit message generation for source code changes. In IJCAI.
[45]Pengcheng Yin, Graham Neubig, Miltiadis Allamanis, Marc Brockschmidt,and Alexander L Gaunt. 2018. Learning to represent edits. arXiv preprint
arXiv:1810.13337 (2018).
[46]Qihao Zhu, ZeyuSun, Yuan-an Xiao, Wenjie Zhang, Kang Yuan, Yingfei Xiong,
and Lu Zhang. 2021. A syntax-guided edit decoder for neural program repair.
InProceedingsofthe29thACMJointMeetingonEuropeanSoftwareEngineering
Conference and Symposium on the Foundations of Software Engineering. 341â€“353.
981
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:19:36 UTC from IEEE Xplore.  Restrictions apply. 