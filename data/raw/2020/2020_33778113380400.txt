Testing DNN Image Classifiers for Confusion & Bias Errors
Yuchi Tianâˆ—
Columbia University
yuchi.tian@columbia.eduZiyuan Zhongâˆ—
Columbia University
ziyuan.zhong@columbia.eduVicente Ordonez
University of Virginia
vicente@virginia.edu
Gail Kaiser
Columbia University
kaiser@cs.columbia.eduBaishakhi Ray
Columbia University
rayb@cs.columbia.edu
ABSTRACT
Image classifiers are an important component of todayâ€™s software,
fromconsumerandbusinessapplicationstosafety-criticaldomains.
The advent of Deep Neural Networks (DNNs) is the key catalyst
behind such wide-spread success. However, wide adoption comes
with serious concerns about the robustness of software systems
dependentonDNNsforimageclassification,asseveralsevereer-
roneous behaviors have been reported under sensitive and critical
circumstances. We argue that developers need to rigorously test
theirsoftwareâ€™simageclassifiersanddelaydeploymentuntilaccept-
able. We present an approach to testing image classifier robustness
based on class property violations.
We found that many of the reported erroneous cases in popular
DNNimageclassifiersoccurbecausethetrainedmodelsconfuse
oneclasswithanotherorshowbiasestowardssomeclassesover
others.Thesebugsusuallyviolatesomeclasspropertiesofoneor
moreofthoseclasses.MostDNNtestingtechniquesfocusonper-
image violations, so fail to detect class-level confusions or biases.
We developed a testing technique to automatically detect class-
basedconfusionandbiaserrorsinDNN-drivenimageclassification
software.Weevaluatedourimplementation,DeepInspect,onsev-
eralpopularimageclassifierswithprecisionupto100%(avg.72.6%)
for confusion errors, and up to 84.3% (avg. 66.8%) for bias errors.
DeepInspectfoundhundredsofclassificationmistakesinwidely-
used models, many exposing errors indicating confusion or bias.
CCS CONCEPTS
â€¢Software and its engineering â†’Software testing and de-
bugging;â€¢Computing methodologies â†’Neural networks .
KEYWORDS
whitebox testing, deep learning, DNNs, image classifiers, bias
âˆ—Both are first authors, and contributed equally to this research.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation
on the first page. Copyrights for components of this work owned by others than ACMmustbehonored.Abstractingwithcreditispermitted.Tocopyotherwise,orrepublish,topostonserversortoredistributetolists,requirespriorspecificpermissionand/ora
fee. Request permissions from permissions@acm.org.
ICSE â€™20, May 23â€“29, 2020, Seoul, Republic of Korea
Â© 2020 Association for Computing Machinery.
ACM ISBN 978-1-4503-7121-6/20/05...$15.00
https://doi.org/10.1145/3377811.33804001 INTRODUCTION
Imageclassificationhasaplethoraofapplicationsinsoftwarefor
safety-critical domains such as self-driving cars, medical diagnosis,
etc.Even day-to-day consumer software includes image classifiers,
suchasGooglePhotosearchandFacebookimagetagging.Image
classificationisawell-studiedproblemincomputervision,wherea
model is trained to classify an image into single or multiple prede-
fined categories [ 26]. Deep Neural Networks (DNNs) have enabled
majorbreakthroughsinimageclassificationtasksoverthepastfew
years,sometimesevenmatchinghuman-levelaccuracyundersomeconditions[
22],whichhasledtotheirubiquityinmodernsoftware.
However,inspiteofsuchspectacularsuccess,DNN-basedimage
classificationmodels,liketraditionalsoftware,areknowntohave
serious bugs. For example, Google faced backlash in 2015 due to
anotoriouserrorinitsphoto-taggingapp,whichtaggedpictures
of dark-skinned people as â€œgorillasâ€ [ 19]. Analogous to traditional
software bugs, the Software Engineering (SE) literature denotes
theseclassificationerrorsas modelbugs [43],whichcanarisedue
to either imperfect model structure or inadequate training data.
At a high-level, these bugs can affect either an individual image,
where a particular image is mis-classified ( e.g.,a particular skier is
mistakenasapartofamountain),oran imageclass,whereaclassof
imagesismorelikelytobemis-classified( e.g.,dark-skinnedpeople
are more likely to be misclassified), as shown in Table 1. The latter
bugsarespecifictoawhole classofimagesratherthanindividual
images, implying systematic bugs rather than the DNN equivalent
ofoff-by-oneerrors.WhilemucheffortfromtheSEliteratureon
Neural Network testing has focused on identifying individual-level
violationsâ€”using white-box [ 29,42,60,79], grey-box [ 43,77], or
concolic testing [ 75], detection of class-level violations remains rel-
ativelylessexplored.Thispaperfocusesonautomaticallydetecting
such class-level bugs, so they can be fixed.
After manual investigation of some public reports describing
the class-level violations listed in Table 1, we determined two root
causes: (i) Confusion : The model cannot differentiate one class
fromanother.Forexample,GooglePhotosconfusesskierandmoun-
tain[44].(ii)Bias:Themodelshowsdisparateoutcomesbetween
two related groups. For example, Zhao et al.in their paper â€œMen
also like shoppingâ€ [ 92], find classification bias in favor of women
onactivitieslikeshopping,cooking,washing, etc.Wefurthernotice
that some class-level properties are violated in both kinds of cases.
For example, in the case of confusion errors, the classification error-
rate between the objects of two classes, say, skier and mountain,issignificantlyhigherthantheoverallclassificationerrorrateof
the model. Similarly, in the bias scenario reported by Zhao et al.,a
11222020 IEEE/ACM 42nd International Conference on Software Engineering (ICSE)
Table 1: Examples of real-world bugs reported in neural image classifiers
Bug Type Name Report Date Outcome
Gorilla Tag [19] Jul 1, 2015 Black people were tagged as gorillas by Google photo app.
Confusion Elephant is detected Aug 9, 2018 Image Transplantation (replacing a sub-region of an image by
in a room [68] another image containing a trained object) leads to mis-classification.
Google Photo [44] Dec 10, 2018 Google Photo confuses skier and mountain.
Nikon Camera [67] Jan 22, 2010 Camera shows bias toward Caucasian faces when detecting peopleâ€™s blinks.Men Like Shopping [92] July 29, 2017 Multi-label object classification models show bias towards women on
Bias activities like shopping, cooking, washing, etc.
Gender Shades[8] 2018 Open-source face recognition services provided by IBM, Microsoft, and Face++
have higher error rates on darker-skin females for gender classification.
DNN model should not have different error rates while classifying
the gender of a person in the shopping category. Unlike individual
image properties, this is a class property affecting all the shopping
imageswithmenorwomen.Anyviolationofsuchapropertyby
definition affects the whole class although not necessarily everyimage in that class,
e.g.,a man is more prone to be predicted as
a woman when he is shopping, even though some individual im-
ages of aman shopping maystill be predicted correctly. Thus,we
need a class-level approach to testing image classifier software for
confusion and bias errors.
The bugs in a DNN model occur due to sub-optimal interactions
between the model structure and the training data [ 43]. To capture
suchinteractions,theliteraturehasproposedvariousmetricspri-
marily based on either neuron activations [ 29,42,60] or feature
vectors [43,53]. However, these techniques are primarily targeted
at the individual image level. To detect class-level violations, we
abstractawaysuchmodel-datainteractionsattheclasslevelandan-alyzetheinter-classinteractionsusingthatnewabstraction.Tothis
end,weproposeametricusingneuronactivationsandabaseline
metric using weight vectors of the feature embedding to capture
the class abstraction.
For a set of test input images, we compute the probability of
activation of a neuron per predicted class. Thus, for each class, we
createavectorofneuronactivationswhereeachvectorelementcor-respondstoaneuronactivationprobability.Ifthedistancebetween
thetwovectorsfortwodifferentclassesistooclose,comparedto
otherclass-vectorpairs,thatmeanstheDNNundertestmaynot
effectively distinguish between those two classes. Motivated byMODEâ€™stechnique[
43],wefurthercreateabaselinewhereeach
class is represented by the corresponding weight vector of the last
linear layer of the model under test.
Weevaluateourmethodologyforbothsingle-andmulti-label
classificationmodelsineightdifferentsettings.Ourexperiments
demonstrate that DeepInspect can efficiently detect both Bias and
Confusion errors in popular neural image classifiers. We furthercheck whether DeepInspect can detect such classification errorsin state-of-the-art models designed to be robust against norm-bounded adversarial attacks [
82]; DeepInspect finds hundreds of
errors proving the need for orthogonal testing strategies to detect
such class-level mispredictions. Unlike some other DNN testing
techniques [ 53,60,75,77], DeepInspect does not need to generate
additional transformed (synthetic) images to find these errors. The
primary contributions of this paper are:â€¢We propose a novel neuron-coverage metric to automatically
detect class-level violations (confusion and bias errors) in DNN-
based visual recognition models for image classification.
â€¢WeimplementedourmetricandunderlyingtechniquesinDeepIn-
spect.
â€¢WeevaluatedDeepInspectandfoundmanyerrorsinwidely-usedDNNmodelswithprecisionupto100%(avg.72.6%)forconfusion
errors and up to 84.3% (avg. 66.8%) for bias errors.
Ourcodeisavailableathttps://github.com/ARiSE-Lab/DeepInspect.
TheerrorsreportedbyDeepInspect areavailable at:
https://www. ariselab.info/deepinspect.
2DNNBACKGROUND
DeepNeuralNetworks (DNNs)areapopulartypeofmachinelearn-
ingmodellooselyinspiredbytheneuralnetworks ofhumanbrains.
ADNNmodellearnsthelogictoperformasoftware taskfroma
largesetoftrainingexamples. Forexample, animagerecognition
modellearnstorecognize cowsthroughbeingshown(trainedwith)
manysampleimagesofcows.
Atypical"feed-forward" DNNconsistsofasetofconnected
computational units,referredasneurons,thatarearranged sequen-
tiallyinaseriesoflayers.Theneuronsinsequential layersare
connected toeachotherthroughedges.Eachedgehasacorrespond-
ingweight.EachneuronappliesÏƒ,anonlinear activation function
(e.g.,ReLU[50],Sigmoid[48]),totheincoming valuesonitsinput
edgesandsendstheresultsonitsoutputedgestothenextlayerof
connected neurons. Forimageclassification,convolutionalneural
networks (CNNs)[37],aspecifictype ofDNN,are typicallyused.
CNNsconsistoflayerswithlocalspatialconnectivity andsetsof
neuronswithsharedparameters.
Whenimplementing aDNNapplication, developers typically
startwithasetofannotated experimental dataanddivideitinto
threesets:(i)training:toconstruct theDNNmodelinasupervised
setting,meaning thetrainingdataislabeled(e.g.,usingstochas-
ticgradient descentwithgradients computed usingback-prop-
agation[69]);(ii)validation: totunethemodelâ€™shyper-parameters,
basicallyconfigurationparametersthatcan bemodified tobetterfit
theexpected application workload; and(iii)evaluation: toevaluate
theaccuracy ofthetrainedmodelw.r.t.toitspredictions onother
annotated data,todetermine whetherornotitpredictscorrectly.
Typically, training,validation, andtestingdataaredrawnfromthe
sameinitialdataset.
1123Forimageclassification,aDNNcanbetrainedineitherofthe
following two settings:
(i)Single-label Classification. In a traditional single-label classi-
ficationproblem,eachdatumisassociatedwithasinglelabel lfrom
a set of disjointlabels Lwhere|L|>1. If|L|=2, the classification
problemiscalledabinaryclassificationproblem;if |L|>2,itisa
multi-classclassificationproblem[ 78].Amongsomepopularimage
classification datasets, MNIST, CIFAR-10/CIFAR-100 [ 32] and Ima-
geNet[70]areallsingle-label,whereeachimagecanbecategorized
into only one class or outside that class.
(ii)Multi-labelClassification. Inamulti-labelclassificationprob-
lem,eachdatumisassociatedwithasetoflabels YwhereYâŠ†L.
COCO[38] and imSitu[ 85] are popular datasets for multi-label clas-
sification. For example, an image from the COCO dataset can be
labeled as car, person, traffic light and bicycle. A multi-label classifi-
cation model is supposed to predict all of car, person, traffic light
and bicycle from a single image that shows all of these kinds of
objects.
Given any single- or multi-label classification task, DNN clas-
sifier software tries to learn the decision boundary between the
classesâ€”all members of a class, say Ci, should be categorized iden-
tically irrespective of their individual features, and members of
anotherclass,say Cj,shouldnotbecategorizedto Ci[6].TheDNN
represents the input image in an embedded space with the feature
vectoratacertainintermediatelayerandusesthelayersafteras
a classifier to classify these representations. The class separation
between two classes estimates how well the DNN has learned to
separate each class from the other. If the embedded distance be-
tween two classes is too small compared to other classes, or lower
thansomepre-definedthreshold,weassumethattheDNNcould
not separate them from each other.
3 METHODOLOGY
WegiveadetailedtechnicaldescriptionofDeepInspect.Wedescribe
a typical scenario where we envision our tool might be used in the
following and design the methodology accordingly.
UsageScenario. Similartocustomertestingofpost-releasesoft-
ware, DeepInspect works in a real-world setting where a customer
gets a pre-trained model and tests its performance in a sample pro-
duction scenario before deployment. The customer has white-box
accesstothemodeltoprofile,althoughallthedataintheproduc-
tion system can be unlabeled. In the absence of ground truth labels,
theclassesaredefinedbythe predictedlabels.Thesepredictedlabels
areusedasclassreferencesasDeepInspecttriestodetectconfusion
andbiaserrorsamongtheclasses.DeepInspecttrackstheactivatedneurons per class and reports a potential class-level violation if the
class-level activation-patterns are too similar between two classes.Such reported errors will help customers evaluate how much they
can trust the modelâ€™s results related to the affected classes. As
elaboratedinSection7,once theseerrorsarereportedbacktothe
developers, they can focus their debugging and fixing effort on
these classes. Figure 1 shows the DeepInspect workflow.
3.1 Definitions
Before we describe DeepInspectâ€™s methodology in detail, we intro-
duce definitions that we use in the rest of the paper. The following
Figure 1: DeepInspect Workflow
table shows our notation.
All neurons set N={N1,N2, ...}
Activation function out(N,c)returns output
for neuron N, inputc.
Activation threshold Th
Neural-Path ( NP).For an input image c, we define neural-path as a
sequence of neurons that are activated by c.
Neural-Path per Class ( NPC).For a class Ci, this metric represents
a set consisting of the union of neural-paths activated by all the
inputs in Ci.
Forexample,consideraclass cowcontainingtwoimages:abrown
cow and a black cow. Letâ€™s assume they activate two neural-paths:
[N1,N2,N3]and[N4,N5,N3]. Thus,theneural-paths forclass cow
would be NPcow={[N1,N2,N3],[N4,N5,N3]}.NPcowis further
represented by a vector (N1
1,N1
2,N2
3,N1
4,N1
5), where the super-
scripts represent the number of times each neuron is activated
byCcow.Thus,eachclass Ciinadatasetcanbeexpressedwitha
neuronactivationfrequencyvector,whichcaptureshowthemodel
interacts with Ci.
Neuron Activation Probability :Leveraginghow frequently aneu-
ronNjis activatedby allthe members from a class Ci, thismetric
estimatestheprobabilityofaneuron Njtobeactivatedby Ci.Thus,
we define: P(Nj|Ci)=|{cik|âˆ€cikâˆˆCi,out(Nj,cik)>Th}|
|Ci|
Wethenconstructa nÃ—mdimensional neuronactivationprobability
matrix,Ï,(nis the number of neurons and mis the number of
classes) with its ij-th entry being P(Nj|Ci).
Ï=C1...Ci...Cm
/parenlefttpA/parenleftexA/parenleftexA/parenleftexA/parenleftexA
/parenleftbtA/parenrighttpA/parenrightexA/parenrightexA/parenrightexA/parenrightexA
/parenrightbtAN
1p11 p1m
... ...
Njpj1 ... pjm
... ...
Nnpn1 pnm(1)
Thismatrixcaptureshowamodelinteractswithasetofinput
data.Thecolumnvectors( ÏÎ±m)representtheinteractionofaclass
Cmwiththemodel.Notethat,inoursetting, Csarepredictedlabels.
Since Neuron Activation Probability Matrix ( Ï) is designed to
representeachclass,itshouldbeabletodistinguishbetweendif-
ferentCs.Next,weusethismetrictofindtwodifferentclassesof
errorsoftenfoundinDNNsystems: confusion andbias(seeTable1).
3.2 Finding Confusion Errors
In an object classification task, when the model cannot distinguish
one object class from another, confusion occurs. For example, as
shown in Table 1, a Google photo app model confuses a skier with
the mountain. Thus, finding confusion errors means checking how
well the model can distinguish between objects of different classes.
1124An error happens when the model under test classifies an object
withawrongclass, orformulti-labelclassificationtask,predicts
two classes but only one of them is present in the test image.
We argue that the model makes these errors because during the
trainingprocessthemodelhasnotlearnedtodistinguishwellbe-
tweenthetwoclasses,say aandb.Therefore,theneuronsactivated
by these objects are similar and the column vectors correspond-
ingtotheseclasses: ÏÎ±aandÏÎ±bwillbeveryclosetoeachother.
Thus, we compute the confusion score between two classes as the
euclidean distance between their two probability vectors:
napvd(a,b)=Î”(a,b)=||ÏÎ±aâˆ’ÏÎ±b||2=/radicaltp/radicalbtn/summationdisplay.1
i=1(P(Ni|a)âˆ’P(Ni|b))2(2)
Ifthe Î”valueis lessthan somepre-definedthreshold (conf_th)
fortwopairsofclasses,themodelwillpotentiallymakemistakes
in distinguishing one from another, which results in confusion
errors.This Î”iscallednapvd( NeuronActivationProbabiliy Vector
Distance).
3.3 Finding Bias Errors
In an object classification task, bias occurs if the model under test
shows disparate outcomes between two related classes. For ex-
ample, we find that ResNet-34 pretrained by imSitu dataset, of-ten mis-classifies a man with a baby as
woman. We observe that
in the embedded matrix Ï,Î”(baby,woman)is much smaller than
Î”(baby,man). Therefore, during testing, whenever the model finds
an image with a baby, it is biased towards associating the babyimage with a woman. Based on this observation, we propose aninter-class distance based metric to calculate the bias learned by
themodel.Wedefinethebiasbetweentwoclasses aandbovera
third class cas follows:
bias(a,b,c):=|Î”(c,a)âˆ’Î”(c,b)|
Î”(c,a)+Î”(c,b)(3)
If a model treats objects of classes aandbsimilarly under the
presenceofathirdobjectclass c,aandbshouldhavesimilardis-
tancew.r.t.cintheembeddedspace Ï;thus,thenumeratorofthe
above equation will be small. Intuitively, the modelâ€™s output can bemoreinfluencedbythenearerobjectclasses,
i.e.ifaandbarecloser
toc.Thus,wenormalizethedisparitybetweenthetwodistances
to increase the influence of closer classes.
This bias score is used to measure how differently the given
model treats two classes in the presence of a third object class. An
average bias (abbreviated as avg_bias) between two objects aand
bfor all class objects Ois defined as:
avĞ´_bias(a,b):=1
|O|âˆ’2/summationdisplay.1
câˆˆO ,c/nequala,bbias(a,b,c)(4)
Theabovescorecapturestheoverallbiasofthemodelbetweentwo
classes.If thebiasscore islargerthansome pre-definedthreshold,
we report potential bias errors.
Notethat,evenwhenthetwoclasses aandbarenotconfused
by themodel, i.e.Î”(a,b)>conf_th, they canstill show bias w.r.t.
another class, say c,i fÎ”(a,c)is very different from Î”(b,c). Thus,
bias and confusion are two separate types of class-level errors that
we intend to study in this work.Table 2: Study Subjects
Dataset Model
Classification CNN Reported
Task Name #classes Models#Neurons #Layers Accuracy
COCO [38] 80ResNet-50[92] 26,56053 Conv 0.73*
Multi-label COCO gender[92] 81ResNet-50[22] 26,56053 Conv 0.71*
classification imSitu[85] 205,095 ResNet-34[85] 8,44836 Conv 0.37â€ 
CIFAR-100[32] 100 CNN[1] 2,916 26 0.74
Robust 10Small CNN[82] 158 80.69
Single-label CIFAR-10[32] Large CNN[82] 1,226 14 0.73
classification ResNet[82] 1,410 34 0.70
ImageNet[70] 1000ResNet-50[73] 26,56053 Conv 0.75
* reported in mean average precision, â€ reported in mean accuracy
Using these above equations we develop a novel testing tool,
DeepInspect, to inspect a DNN implementing image classification
tasksandlookforpotentialconfusionandbiaserrors.Weimple-
mented DeepInspectin thePytorch deeplearning frameworkand
Python 2.7. All our experiments were run on Ubuntu 18.04.2 with
two TITAN Xp GPUs. For all of our experiments, we set the activa-
tion threshold Thto be 0.5 for all datasets and models. We discuss
whywechoose0.5asneuronactivationthresholdandhowdifferent
thresholds affect our performance in the section 7.
4 EXPERIMENTAL DESIGN
4.1 Study Subjects
WeapplyDeepInspectforbothmulti-labelandsingle-labelDNN-
based classifications. Under different settings, DeepInspect auto-
maticallyinspects8DNNmodelsfor6datasets.Table2summarizes
our study subjects. All the models we used are standard, widely-
usedmodelsforeachdataset.Weusedpre-trainedmodelsasshown
in the Table for all settings except for COCO with gender. For
COCOwithgendermodel,weusedthegenderlabelsfrom[ 92]and
trainedthemodelinthesamewayas[ 92].imSitumodelisapre-
trainedResNet-34model[ 85].Thereareintotal11,538entitiesand
1,788 roles in the imSitu dataset. When inspecting a model trained
usingimSitu,weonlyconsideredthetop100frequententitiesor
roles in the test dataset.
Among the 8 DNN models, three are pre-trained relatively more
robust models that are trained using adversarial images along with
regularimages.Thesemodelsarepre-trainedbyprovablyrobust
trainingapproachproposedby[ 82].Threemodelswithdifferent
network structures are trained using the CIFAR10 dataset [82].
4.2 Constructing Ground Truth (GT) Errors
To collect the ground truth for evaluating DeepInspect, we refer to
the test images misclassified by a given model. We then aggregate
thesemisclassifiedimageinstancesbytheirrealandpredictedclass-
labels and estimate pair-wise confusion/bias.
4.2.1 GT of Confusion Errors. ConfusionoccurswhenaDNNof-
ten makes mistakes in disambiguating members of two different
classes. In particular, if a DNN is confused between two classes,the classification error rate is higher between those two classes
thanbetweentherestoftheclass-pairs.Basedonthis,wedefine
two types of confusion errors for single-label classification and
multi-label classification separately:
1125(a) Confusions distribution
 (b) NAPVD distribution
Figure2: IdentifyingType2confusionsformulti-classificationap-
plications.LHSshowshowwemarkedthegroundtrutherrorsbased
on Type2 confusion score. RHS shows DeepInspectâ€™s predicted er-rors based on NAPVD score.
Type1 confusions : In single-label classification, Type1 confusion
occurs when an object of class x(e.g.,violin ) is misclassified to
anotherclass y(e.g.,cello ).Foralltheobjectsofclass xandy,itcan
be quantified as: type1conf (x,y)=mean(P(x|y),P(y|x))â€”DNNâ€™s
probability to misclassify class yasxand vice-versa, and takes the
average value between the two. For example, given two classes
celloandviolin,type1conf estimates the mean probability of
violinmisclassified to celloand vice versa. Note that, this is a
bi-directional score, i.e.misclassification of yasxis the same as
misclassification of xasy.
Type2confusions :Inmulti-labelclassification,Type2confusion
occurswhenaninputimagecontainsanobjectofclass x(e.g.,mouse )
andnoobjectofclass y(e.g.,keyboard ),butthemodelpredictsboth
classes (see Figure 7. For a pair of classes, this can be quantified
as:type2conf (x,y)=mean(P((x,y)|x),P((x,y)|y))tocomputethe
probabilitytodetecttwoobjectsinthepresenceofonlyone.Forex-
ample,giventwoclasses keyboard andmouse,type2conf estimates
the mean probability of mousebeing predicted while predicting
keyboard and vice versa. This is also a bi-directional score.
We measure type1conf andtype2conf by using a DNNâ€™s true
classification error measured on a set of test images. They create
theDNNâ€™strueconfusioncharacteristicsbetweenallpossibleclass-
pairs. We then draw the distributions of type1conf andtype2conf .
For example, Figure 2a shows type2conf distribution for COCO .
Theclass-pairswithconfusionscoresgreaterthan1standarddevia-
tionfromthemean-valuearemarkedaspairstrulyconfusedbythemodelandformourgroundtruthforconfusionerrors.Forexample,
in the COCO dataset, there are 80 classes and thus 3160 class pairs
(80*79/2); 178 class-pairs are ground-truth confusion errors.
Notethat,unlikehowabug/errorisdefinedintraditionalsoft-
ware engineering, our suspicious confusion pairs have an inherent
probabilisticnature.Forexample,evenif aandbrepresentacon-
fusionpair,itdoesnotmeanthatalltheimagescontaining aorb
will be misclassified by the model. Rather, it means that compared
with other pairs, images containing aorbtend to have a higher
chance to be misclassified by the model.
4.2.2 GT of Bias Errors. A DNN model is biasedif it treats two
classes differently. For example, consider three classes: man,woman,
andsurfboard .Anunbiasedmodelshouldnothavedifferenterror
rates while classifying manorwomanin the presence of surfboard .
Tomeasuresuchbiasformally,wedefine confusiondisparity (cd)to measure differences in error rate between classes xandzand
betweenyandz:cd(x,y,z)=|error(x,z)âˆ’error(y,z)|,wheretheer-
rormeasurecanbeeither type1conf ortype2conf asdefinedearlier.
cdessentially estimates the disparity of the modelâ€™s error between
classesx,y(e.g., man, woman)w.r.t.a third class z(e.g., surfboard).
We also define an aggregated measure average confusion dis-
parity(avg_cd)betweentwoclasses xandybysummingupthe
bias between them over all third classes and taking the average:
avg_cd(x,y):=1
|O|âˆ’2/summationdisplay.1
zâˆˆO ,z/nequalx,ycd(x,y,z).
Dependingontheerrortypesweusedtoestimateavg_cd,wereferto
Type1_avg_cdand Type2_avg_cd.Wemeasureavg_cdusingthe
true classificationerror ratereported for thetest images.Similar toconfusionerrors,wedrawthedistributionof avg_cdforallpossibleclasspairsandthenconsiderthepairsas trulybiased iftheiravg_cd
scoreishigherthanonestandarddeviationfromthemeanvalue.
Such truly biased pairs form our ground truth for bias errors.
4.3 Evaluating DeepInspect
We evaluate DeepInspect using a set of test images.
Error Reporting. DeepInspect reports confusion errors based on
NAPVD(see Equation(2))scoresâ€”lower NAPVDindicates errors.
WedrawthedistributionsofNAPVDsforallpossibleclasspairs,as
shown inFigure 2b. Classpairs havingNAPVD scores lowerthan
1 standard deviationfrom the meanscore aremarked as potential
confusion errors.
AsdiscussedinSection3.3,DeepInspectreportsbiaserrorsbased
on avg_bias score (see Equation (4)), where higher avg_bias means
class pairs are more prone to bias errors. Similar to above, from
thedistributionof avg_biasscores,DeepInspectpredictspairswith
avg_bias greater than 1 standard deviation from the mean score to
be erroneous. Note that, while calculating error disparity between
classesa,bw.r.t.c(see Equation (3)), if both aandbare far from c
intheembeddedspace Ï,disparityoftheirdistances( Î”)shouldnot
reflecttruebias.Thus,whilecalculating avg_bias( a,b)wefurther
filter out the triplets where Î”(c,a)>thâˆ§Î”(c,b)>th, wherethis
some pre-defined threshold. In our experiment, we remove all the
class-pairs having Î”larger than 1 standard deviation ( i.e.th)f r o m
the mean value of all Deltas across all the class-pairs.
Evaluation Metric. We evaluate DeepInspect in two ways:
Precision&Recall. WeuseprecisionandrecalltomeasureDeepIn-
spectâ€™saccuracy.Foreacherrortypet,supposethatEisthenumber
of errors detected by DeepInspect and A is the the number of true
errors in the ground truth set. Then the precision and recall of
DeepInspect are|Aâˆ©E|
|E|and|Aâˆ©E|
|A|respectively.
Area Under Cost Effective Curve (AUCEC). Similarly to how
static analysis warnings are ranked based on their priority lev-
els[63],wealsoranktheerroneousclass-pairsidentifiedbyDeepIn-
spect based on the decreasing order of error proneness, i.e.most
error-prone pairs will be at the top. To evaluate the ranking we
useacost-effectivenessmeasure[ 2],AUCEC(AreaUndertheCost-
Effectiveness Curve), which has become standard to evaluate rank-
based bug-prediction systems [27, 63â€“66].
Cost-effectivenessevaluateswhenweinspect/test topn%class-
pairs in the ranked list ( i.e.inspection cost), how many true errors
1126arefound( i.e.effectiveness).Bothcostandeffectivenessarenormal-
izedto100%.Figure6showscostonthex-axis,andeffectivenesson
the y-axis, indicating the portion of the ground truth errors found.
AUCEC is the area under this curve.
Baseline. We compare DeepInspect w.r.t.two baselines:
(i) MODE-inspired: A popular way to inspect each image is to
inspect a feature vector, which is an output of an intermediate
layer[43,90].However,abstractingafeaturevectorperimagetothe
classlevelisnon-trivial.Instead,foragivenlayer,onecouldinspect
theweightvector( wl=[w0
l,w1
l,...,wn
l])ofaclass,say l,wherethe
superscripts represent a feature. Similar weight-vectors are used
in MODE [ 43] to compare the difference in feature importance
between two image groups. In particular, from the last linear layer
before the output layer we extract such per-class weight vectorsand compute the pairwise distances between the weight vectors.Using these pairwise distances we calculate confusion and bias
metrics as described in Section 3.
(ii) Random: We also build a random model that picks random
class-pairs for inspection [81] as a baseline.
For AUCEC evaluation, we further show the performance of an
optimal model that ranks the class-pairs perfectlyâ€”if n% of all the
class-pairsaretrulyerroneous,theoptimalmodelwouldrankthematthetopsuchthatwithlowerinspectionbudgetmostoftheerrors
willbedetected.Theoptimalcurvegivesthelowerupperboundof
the ranking scheme.ResearchQuestions.
Withthisexperimentalsetting,weinvesti-
gatethefollowingthreeresearchquestionstoevaluateDeepInspect
for DNN image classifiers:
â€¢RQ1.Can DeepInspect distinguish between different classes?
â€¢RQ2.Can DeepInspect identify the confusion errors?
â€¢RQ3.Can DeepInspect identify the bias errors?
5 RESULTS
Webeginourinvestigationbycheckingwhetherde-factoneuron
coverage-based metrics can capture class separation.RQ1.CanDeepInspectdistinguishbetweendifferentclasses?
Motivation.
The heart of DeepInspectâ€™s error detection technique
liesinthefactthattheunderlyingNeuronActivationProbability
metric (Ï) captures each class abstraction reasonably well and thus
distinguishes between classes that do not suffer from class-level
violations.InthisRQwecheckwhetherthisisindeedtrue.Wealso
checkwhetheranewmetric Ïisnecessary, i.e.,whetherexisting
neuron-coverage metrics could capture such class separations.
Approach. We evaluate this RQ w.r.t.the training data since the
DNNbehaviorsarenottaintedwithinaccuraciesassociatedwith
the test images. Thus, all the class-pairs are benign. We evalu-ate this RQ in three settings: (i) using DeepInspectâ€™s metrics, (ii)
neuron-coverage proposed by Pei et al.[60], and (iii) other neuron-
activation related metrics proposed by DeepGauge [42].Setting-1. DeepInspect.
Our metric, Neuron Activation Proba-
bility Matrix ( Ï), by construction is designed per class. Hence it
would be unfair to directly measure its capability to distinguish
between different classes. Thus, we pose this question in slightly a
different way, as described below. For multi-label classification,each image contains multiple class-labels. For example, an im-age might have labels for both
mouseandkeyboard . Such coin-
cidence of labels may create confusionâ€”if two labels always ap-
peartogether inthe groundtruthset, noclassifier candistinguish
between them. To check how many times two labels coincide,
we define a coincidence score between two labels LaandLbas:
coincidence (La,Lb)=mean(P(La,Lb|La),P(La,Lb|Lb)).
The above formula computes the minimum probability of labels
LaandLboccurringtogetherinanimagegiventhatoneofthemis
present.Notethatthisisabi-directionalscore, i.e.wetreatthetwo
labels similarly. The meanoperation ensures we detect the least
coincidenceineitherdirection.Alowvalueofcoincidencescore
indicates two class-labels are easy to separate and vice versa.
Now, to check DeepInspectâ€™s capability to capture class separa-
tion,wesimplycheckthecorrelationbetweencoincidencescore
and confusion score (napvd) from Equation 2 for all possible class-
label pairs. Since only multi-label objects can have label coinci-dences, we perform this experiment for a pre-trained ResNet-50
model on the COCO multi-label classification task.
ASpearmancorrelation coefficient betweentheconfusionand
coincidence scores reaches a value as high as 0.96, showing strong
statisticalsignificance.TheresultindicatesthatDeepInspectcan
disambiguate most of the classes that have a low confusion scores.
Interestingly, we found some pairs where coincidence score
is high, but DeepInspect was able to isolate them. For example,
(cup,chair ),(toilet,sink ),etc..Manuallyinvestigatingsuchcases
revealsthatalthoughthesepairsoftenappeartogetherintheinputimages,therearealsoenoughinstanceswhentheyappearbythem-selves.Thus,DeepInspectdisambiguatesbetweentheseclassesand
puts them apart in the embedded space Ï. These results indicate
DeepInspect can also learn some hidden patterns from the context
and, thus, can go beyond inspecting the training data coincidence
forevaluatingmodelbias/confusion,whichisthedefactotechnique
among machine learning researchers [92].
â—â—â—â—â—
â—
â—â—
â—â—â—
â—â—
â—â—â—
â—â—â—â—
â—â—
â—â—
â—â—â—
â—â—
â—â—â—â—
â—â—â—
â—â—â—
â—â—â—
â—â—â—â—
â—
â—â—
â—â—
â—
â—â—
â—â—â—â—â—â—
â—
â—â—â—
â—â—â—
â—â—â—
â—
â—â—
â—â—
â—â—
â—â—
â—â—
â—
â—
â—â—
â—â—â—
â—
â—â—â—â—â—â—â—
â—
â—â—â—â—
â—â—â—
â—â—
â—â—â—â—
â—â—â—
â—â—
â—
â—â—â—
â—â—â—
â—
â—â—
â—â—â—â—â—
â—
â—â—
â—
â—â—
â—
â—â—â—â—
â—â—â—
â—â—
â—â—
â—
â—
â—
â—
â—â—â—â—
â—
â—â—
â—â—â—
â—
â—â—â—
â—
â—â—
â—â—
â—
â—â—
â—â—â—â—
â—â—â—
â—â—
â—â—
â—
â—â—â—â—
â—
â—â—â—
â—â—â—â—
â—
â—â—
â—
â—â—
â—â—â—â—
â—â—â—
â—â—
â—â—
â—â—
â—
â—â—â—â—â—
â—â—
â—â—â—â—
â—â—
â—
â—â—â—â—
â—â—
â—â—â—â—
â—â—
â—â—â—
â—â—â—
â—â—
â—â—
â—â—
â—â—â—â—
â—
0.20.30.40.5
 stop sign cat
 couch chair
 cell phone  refrigerator cup pizza spoon
 snowboard
Class LabelsNeuron Coverage
Figure 3: Distribution of neuron coverage per class label, for 10
randomly picked class labels, from the COCO dataset.
Next, we investigate whether popular white-box metrics can
distinguish between different classes.
Setting-2.NeuronCoverage( NC)[60]computestheratioofthe
union of neurons activated by an input set and the total number
of neurons in a DNN. Here we compute NCper class-label, i.e.for
agivenclass-label,wemeasurethenumberofneuronsactivated
bytheimagestaggedwiththatlabel w.r.t.tothetotalneurons.The
1127activation thresholdwe useis 0.5. We perform thisexperiment on
COCO and CIFAR-100 to study multi- and single-label classifica-
tions.Figure3showsresultsforCOCO.Weobservesimilarresults
for CIFAR-100 .
Each boxplot in the figure shows the distribution of neuron cov-
erage per class-label across all the relevant images. These boxplots
visuallyshowthat differentlabels havevery similarNCdistribution.
We further compare these distributions using Kruskal Test [33],
whichisanon-parametricwayofcomparingmorethantwogroups.
Note that we choose a non-parametric measure as NCs may not
follow normal distributions. (Kruskal Test is a parametric equiv-
alent of the one-way analysis of variance (ANOVA).) The result
reportsapâˆ’value<<0.05,i.e.somedifferencesexistacrossthese
distributions. However, a pairwise Cohendâ€™s effect size for each
class-label pair, as shown in the following table, shows more than
56% and 78% class-pairs for CIFAR-100 and COCO have small to
negligible effect size. This means neuroncoverage cannot reliablydistinguish a majority of the class-labels.
Effect Size of neuron coverage across different classes
Exp Setting negligible small medium large
COCO 40.51% 38.19% 16.96% 4.34%
CIFAR-100 31.94% 25.69% 23.87% 18.48%
Setting-3.DeepGauge[42]. Maet al.[42]arguethateachneuron
hasaprimaryregionofoperation;theyidentifythisregionbyusingaboundarycondition
[low,hiĞ´h]onitsoutputduringtrainingtime;
outputsoutsidethisregion( (âˆ’âˆ,low)âˆª(hiĞ´h,+âˆ))aremarkedas
corner cases. They therefore introduce multi-granular neuron and
layer-level coverage criteria. For neuron coverage they propose: (i)
k-multisection coverage to evaluate how thoroughly the primary
regionofaneuroniscovered,(ii) boundarycoverage tocomputehow
many corner cases are covered, and (iii) strong neuron activation
coverageto measure how many corner case regions are covered in
(hiĞ´h,+âˆ) region. For layer-level coverage, they define (iv) top-k
neuroncoverage toidentifythemostactivek-neuronsforeachlayer,
and (v)top-k neuron pattern for each test-case to find a sequence of
neurons from the top-k most active neurons across each layer.
We investigate whether each of these metrics can distinguish
between different classes by measuring the above metrics for in-
dividualinputclassesfollowingMa et al.â€™smethodology.Wefirst
profiled every neuron upper- and lower-bound for each class using
the training images containing that class-label. Next, we computed
per-classneuroncoverageusingtestimagescontainingthatclass;
for k-multisection coverage we chose k=100 to scale up the anal-
ysis.Itshouldbenotedthatwealsotried k=1000(whichisused
in the original DeepGauge paper) and observed similar results (not
shown here).
For layer-level coverage, we directly used the input images con-
taining each class, where we select k=1.
Figure4 showstheresults asahistogram oftheabove fivecov-
eragecriteriafortheCOCOdataset.Forallfivecoveragecriteria,
there are many class-labels that share similar coverage. For ex-ample,inCOCO,thereare52labelswithk-multisectionneuroncoverage with values between 0
.31 and 0 .32. Similarly, there are
40 labels with 0 neuron boundary coverage. Therefore, none of the
fivecoveragecriteriaareaneffectivewaytodistinguishbetween	
 	 
 	 		


















 		!"	
Figure 4: Histogram of DeepGauge [42] multi-granular coverage
per class label for COCO dataset
differentequivalenceclasses.Thesameconclusionwasdrawnfor
the CIFAR-100 dataset.
Result1: DeepInspectcandisambiguateclassesbetterthan
previouscoverage-basedmetricsfortheimageclassificationtask.
We now investigate DeepInspectâ€™s capability in detecting confu-
sion and bias errors in DNN models.RQ2. Can DeepInspect identify the confusion errors?Motivation.
To evaluate how well DeepInspect can detect class-
levelviolations,inthisRQ,wereportDeepInspectâ€™sabilitytodetect
the first type of violation, i.e., Type1/Type2 confusions w.r.t.to
ground truth confusion errors, as described in Section 4.2.1.
0.0 0.1 0.2 0.3
type 2 confusion0.000.050.100.150.200.250.30NAPVD (Î”)
(a) COCO dataset + ResNet-500.00 0.05 0.10 0.15
type 1 confusion0.00.10.20.3NAPVD (Î”)
(b) Robust CIFAR-10 Small
Figure5: StrongnegativeSpearmancorrelation(-0.55and-0.86)be-
tween napvd and ground truth confusion scores.
Wefirstexplorethecorrelationbetweennapvdandgroundtruth
Type1/Type2confusionscore.Strongcorrelationhasbeenfound
forall8experimentalsettings.Figure5givesexamplesonCOCO
and CIFAR-10. These results indicate that napvd can be used to
detect confusion errorsâ€”lower napvd means more confusion.
Approach. By default, DeepInspect reports all the class-pairs with
napvd scores one standard deviation less than the mean napvdscore as error-prone (See Figure 2b). In this setting, as the resultshown on Table 3, DeepInspect reports errors at high recall un-
der most settings. Specifically, on CIFAR-100 and robust CIFAR-10
ResNet, DeepInspect can reporterrors ashigh as71.8%, and100%,
respectively. DeepInspect has identified thousands of confusion
errors.
1128Table 3: DeepInspect performance on detecting confusion errors
napvd < mean-1std Top 1%
TP FP Precision Recall TP FP Precision Recall
COCO DeepInspect 138 256 0.350 0.775 31 0 1 0.174
MODE 126 382 0.248 0.708 26 5 0.839 0.146
random 22 372 0.056 0.124 1 30 0.032 0.006
COCO gender DeepInspect 139 286 0.327 0.827 32 0 1 0.190
MODE 125 379 0.248 0.744 30 2 0.938 0.179
random 22 403 0.052 0.131 1 31 0.031 0.006
CIFAR-100 DeepInspect 206 584 0.261 0.718 39 10 0.796 0.136
MODE 111 605 0.155 0.387 22 27 0.449 0.077
random 45 745 0.057 0.157 2 47 0.041 0.007
R CIFAR-10 S DeepInspect 4 6 0.400 0.800 -- - -
MODE 3 4 0.429 0.600 -- - -
random 1 9 0.100 0.200 -- - -
R CIFAR-10 L DeepInspect 3 4 0.430 0.600 -- - -
MODE 3 5 0.375 0.600 -- - -
random 07 0 0 -- - -
R CIFAR-10 R DeepInspect 5 3 0.625 1 -- - -
MODE 1 3 0.250 0.200 -- - -
random 08 0 0 -- - -
ImageNet DeepInspect 4014 69957 0.054 0.617 1073 3922 0.215 0.165
MODE 3428 66987 0.049 0.527 1591 3404 0.319 0.245
random 962 73009 0.013 0.148 65 4930 0.013 0.010
imSitu DeepInspect 48 58 0.453 0.165 31 19 0.620 0.107
random 6 100 0.057 0.020 2 48 0.040 0.007
Ifhigherprecisioniswanted,ausercanchoosetoinspectonlya
smallsetofconfusedpairsbasedonnapvd.AsalsoshowninTable3,
when only the top1% confusion errors are reported, a much higher
precision is achieved for all the datasets. In particular, DeepIn-
spect identifies 31 and 39 confusion errors for the COCO model
andtheCIFAR-100modelwith100%and79.6%precision,respec-
tively. The trade-off between precision and recall can be found on
the cost-effective curves shown on Figure 6, which show overall
performanceofDeepInspectatdifferentinspectioncutoffs.Over-
all,w.r.t.a randombaseline mode, DeepInspectis gaining AUCEC
performance from 61 .6% to 85 .7%;w.r.t.a MODE baseline mode,
DeepInspect is gaining AUCEC performance from 10 .2% to 28 .2%.
Figure 6: AUCEC plot of Type1/Type2 Confusion errors in three
different settings. The redvertical line marks 1-standard deviation
lessfrommeannapvdscore.DeepInspectmarksallclass-pairswith
napvd scores less than the red mark as potential errors.
Figure 7 and Figure 8 give some specific confusion errors found
by DeepInspect in the COCO and the ImageNet settings. In par-
ticular,asshowninFigure7a,whenthereisonlyakeyboardbut
(a) (keyboard,mouse)
 (b) (oven,microwave)
Figure7: ConfusionerrorsidentifiedinCOCOmodel.Ineachpair
the second object is mistakenly identified by the model.
no mouse in the image, the COCO model reports both. Similarly,
Figure8ashowsconfusionerrorson(cello,violin).Thereareseveral
cellos in this image, but the model predicts it to show a violin.
(a) (cello, violin)
 (b) (library, bookshop)
Figure 8: Confusion errors identified in the ImageNet model. For
each pair, the second object is mistakenly identified by the model.
AcrossallthreerelativelymorerobustCIFAR-10modelsDeepIn-
spect identifies (cat, dog), (bird, deer) and (automobile, truck) as
buggy pairs, where one class is very likely to be mistakenly classi-
fiedastheotherclassofthepair.Thisindicatesthattheseconfusion
errorsaretobetiedtothetrainingdata,soallthemodelstrained
on this dataset including the robust models may have these errors.
Theseresultsfurthershowthattheconfusionerrorsareorthogonal
tothenorm-basedadversarialperturbationsandweneedadifferent
technique to address them.
Wealsonotethattheperformanceofallmethodsdegradesquite
abitonImageNet.ImageNetisknowntohaveacomplexstructure,
andallthetasks,includingimageclassificationandrobustimage
classification[ 83]usuallyhaveinferiorperformancecomparedwith
simpler datasets like CIFAR-10 or CIFAR-100. Due to such inherent
complexity,theclassrepresentationintheembeddedspaceisless
accurate,andthustherelativedistancebetweentwoclassesmay
not correctly reflect a modelâ€™s confusion level between two classes.
Result 2: DeepInspect can successfully find confusion errors
with precision 21% to 100% at top1% for both single- and multi-
objectclassificationtasks.DeepInspectalsofindsconfusionerrors
in robust models.
RQ3. Can DeepInspect identify the bias errors?
Motivation. To assess DeepInspectâ€™s ability to detect class-level
violations, in this RQ, we report DeepInspectâ€™s performance in
detecting the second type of violation, i.e., Bias errors as described
in Section 4.2.2.
Approach. WeevaluatethisRQbyestimatingamodelâ€™sbias(avg_bias)
using Equation (4) w.r.t.the ground truth (avg_cd), computed as
1129(a) COCO (b) CIFAR-100
Figure9: StrongpositiveSpearmanâ€™scorrelation(0.76and0.62)ex-
ist between avg_cd and avg_bias while detecting classification bias.
in Section 4.2.2. We first explore the correlation between pairwise
avg_cd and our proposed pairwise avg_bias; Figure 9 shows the
resultsforCOCOandCIFAR-10.Similartrendswerefoundinthe
otherdatasetswestudied.Theresultsshowthatastrongcorrelation
exists between avg_cd and avg_bias. In other words, our proposed
avg_bias is a good proxy for detecting confusion errors.
Table 4: DeepInspect performance on detecting bias errors
avg_bias > mean+1std Top 1%
TP FP Precision Recall TP FP Precision Recall
COCO DeepInspect 249 278 0.472 0.759 24 8 0.75 0.073
MODE 145 324 0.309 0.442 12 20 0.375 0.037
random 54 472 0.103 0.167 3 28 0.103 0.010
COCO gender DeepInspect 218 325 0.401 0.568 17 16 0.515 0.044
MODE 151 328 0.315 0.393 13 20 0.394 0.034
random 64 478 0.118 0.168 3 28 0.118 0.010
CIFAR-100 DeepInspect 310 543 0.363 0.380 29 21 0.580 0.036
MODE 69 315 0.180 0.085 5 45 0.100 0.001
random 140 711 0.165 0.172 8 41 0.165 0.010
R CIFAR-10 S DeepInspect 7 4 0.636 0.778 -- - -
MODE 3 10 0.231 0.333 -- - -
random 2 8 0.200 0.222 -- - -
R CIFAR-10 L DeepInspect 6 7 0.462 0.667 -- - -
MODE 8 14 0.364 0.889 -- - -
random 2 9 0.200 0.267 -- - -
R CIFAR-10 R DeepInspect 6 3 0.667 0.667 -- - -
MODE 8 14 0.364 0.889 -- - -
random 1 7 0.200 0.200 -- - -
ImageNet DeepInspect 26704 48913 0.353 0.330 3253 1742 0.651 0.040
MODE 23881 47503 0.335 0.295 2355 2640 0.471 0.029
random 12234 63381 0.162 0.151 808 4186 0.162 0.010
imSitu DeepInspect 408 311 0.567 0.718 43 8 0.843 0.076
random 80 638 0.112 0.142 5 44 0.112 0.010
As in RQ2, we also do a precision-recall analysis w.r.t.find-
ing the bias errors across all the datasets. We analyze the pre-
cision and recall of DeepInspect when reporting bias errors atthe cutoff Top1%(avg_bias) and mean(avg_bias)+standard devia-
tion(avg_bias),respectively.TheresultsareshowninTable4.Atcut-offTop1%(avg_bias),DeepInspectdetectssuspiciouspairswithpre-cisionashighas75%and84%forCOCOandimSitu,respectively.At
cutoffmean(avg_bias)+standarddeviation(avg_bias),DeepInspect
has high recall but lower precision: DeepInspect detects ground
truth suspicious pairs with recall at 75.9% and 71.8% for COCO and
imSitu.DeepInspectcanreport657(=249+408)totaltruebiasbugs
across the two models. DeepInspect outperforms the random base-
linebyalargemarginatbothcutoffs.Asinthecaseofdetecting
confusion errors, there is a significant trade-off between precisionandrecall.Thiscanbecustomizedbasedonuserneeds.Thecost-
effectiveness analysis in Figure 10 shows the entire spectrum.
Figure 10: Bias errors detected w.r.t.the ground truth of avg_cd
beyond one standard deviation from mean.
AsshowninFigure10,DeepInspectoutperformsthebaselineby
alargemargin.TheAUCECgainsofDeepInspectarefrom37 .1%to
76.1%w.r.t.therandombaselineandfrom6 .0%to41 .9%w.r.t.the
MODEbaselineacrossthe8settings.DeepInspectâ€™sperformance
isclosetotheoptimalcurveundersomesettings,specificallythe
AUCEC gains of the optimal over DeepInspectare only 7.11% and
7.95% under the COCO and ImSitu settings, respectively.
Inspired by [ 92], which shows bias exists between men and
women in COCO for the gender image captioning task, we analyze
the most biased third class cforaandbbeing men and women. As
showninFigure11,wefoundthatsportslikeskiing,snowboarding,
andsurfboardingaremorecloselyassociatedwithmenandthus
misleads the model to predict the women in the images as men.
Figure 12 shows results on imSitu, where we found that the model
tends to associate the class â€œinsideâ€ with women while associating
the class â€œoutsideâ€ with men.
Figure11: Themodelclassifiesthewomeninthesepicturesasmen
in the COCO dataset.
We generalize the idea by choosing classes aandbto be any
class-pair. We found that similar bias also exists in the single-label
classificationsettings.Forexample,inImageNet,oneofthehighest
biasesisbetweenEskimo_dogandrapeseed w.r.t.Siberian_husky.
The model tends to confuse the two dogs but not Eskimo_dog and
rapeseed.ThismakessensesinceEskimo_dogandSiberian_husk
are both dogs so more easily misclassified by the model.
One of the fairness violations of a DNN system can be drastic
differences in accuracy across groups divided according to some
sensitivefeature(s).Inblack-boxtesting,thetestercangetanumber
indicating the degree of fairness has been violated by feeding into
the model a validation set. In contrast, DeepInspect provides a new
angle to the fairness violations. The neuron distance difference
1130Figure 12: The model classifies the man in the first figure to be a
woman and the woman in the second figure to be a man.
betweentwoclasses aandbw.r.t.athirdclass cshedslightonwhy
the model tends to be more likely to confuse between one of them
andcthan the other. We leave a more comprehensive examination
on interpreting bias/fairness violations for future work.
Result 3: DeepInspect can successfully find bias errors for
both single- and multi-label classification tasks, and even for
the robust models, from 52% to 84% precision at top1%.
6 RELATED WORK
Software Testing & Verification of DNNs. Prior research pro-
posed different white-box testing criteria based on neuron cov-
erage [42,60,77] and neuron-pair coverage [ 74]. Sunet al.[75]
presentedaconcolictestingapproachforDNNscalledDeepCon-
colic.Theyshowedthattheirconcolictestingapproachcaneffec-
tively increase coverage and find adversarial examples. Odena and
Goodfellow proposed TensorFuzz[ 53], which is a general tool that
combines coverage-guided fuzzing with property-based testing to
generatecasesthatviolateauser-specifiedobjective.Ithasappli-
cations like finding numerical errors in trained neural networks,
exposing disagreements between neural networks and their quan-
tizedversions,surfacingbrokenlossfunctionsinpopularGitHub
repositories, and making performance improvements to Tensor-
Flow. There are also efforts to verify DNNs [25, 28, 61, 80] against
adversarial attacks. However, most of the verification efforts arelimited to small DNNs and pixel-level properties. It is not obvi-ous how to directly apply these techniques to detect class-level
violations.
Adversarial Deep Learning. DNNs areknown to be vulnerable
towell-craftedinputscalledadversarialexamples,wherethedis-
crepanciesareimperceptibletoahumanbutcaneasilymakeDNNs
fail[14,17,24,31,34,40,51,52,54,57,58,62,76,86].Muchwork
has been done to defend against adversarial attacks [ 4,10,15,18,
20,23,45,47,55,59,72,84,93].Ourmethodshavepotentialtoiden-
tify adversarial inputs. Moreover, adversarial examples are usually
out of distribution data and not realistic, while we can find both
out-distributionandin-distributioncornercases.Further,wecan
identify a general weakness or bug rather than focusing on crafted
attacksthatoftenrequireastrongattackermodel( e.g.,theattacker
adds noise to a stop sign image).InterpretingDNNs.
Therehasbeenmuchresearchonmodelinter-
pretabilityandvisualization[ 5,11,39,49,71,91].Acomprehensive
study is presented by Lipton [ 39]. Donget al.[11] observed that
instead of learning the semantic features of whole objects, neurons
tend to react to different parts of the objects in a recurrent manner.Our probabilistic way of looking at neuron activation per classaims to capture holistic behavior of an entire class instead of an
individualobjectsodiversefeaturesofclassmemberscanbecap-
tured.ClosesttooursisbyPapernot et al.[56],whousednearest
training points to explain adversarial attacks. In comparison, we
analyzetheDNNâ€™sdependenciesontheentiretraining/testingdata
andrepresentitinNeuronActivationProbabilityMatrix.Wecan
explain the DNNâ€™s bias and weaknesses by inspecting this matrix.Evaluating Modelsâ€™ Bias/Fairness.
Evaluating the bias and fair-
nessofasystemisimportantbothfromatheoreticalandapractical
perspective [ 7,41,88,89]. Related studies first define a fairness
criteriaandthentrytooptimizetheoriginalobjectivewhilesatisfy-
ingthefairness criteria[ 3,12,13,21,36,46].Theseproperties are
defined eitherat individual [ 13,30,35] or group levels [ 9,21,87].
In this work, we propose a definition of a bias error for image clas-
sification closely related to fairness notions at group-level. Class
membership can be regarded as the sensitive feature and the equal-itythatwewanttoachieveisfortheconfusionlevelsoftwogroups
w.r.t.anythirdgroup.WeshowedthepotentialofDeepInspectto
detect such violations.
Galhotra et al.[16] first applied the notion of software testing
to evaluating software fairness. They mutate the sensitive features
of the inputs and check whether the output changes. One majorproblem with their proposed method, Themis, is that it assumes
themodeltakesintoaccountsensitiveattribute(s)duringtraining
and inference. This assumption is not realistic since most exist-
ing fairness-aware models drop input-sensitive feature(s). Besides,
Themiswillnotworkonimageclassification,wherethesensitiveat-
tribute(e.g.,,gender,race)isavisualconceptthatcannotbeflipped
easily.Inourwork,weuseawhite-boxapproachtomeasurethe
biaslearnedbythemodelduringtraining.Ourtestingmethoddoes
not require the model to take into account any sensitive feature(s).
We propose a new fairness notion for the setting of multi-object
classification, averageconfusiondisparity,andaproxy, averagebias,
to measure for any deep learning model even when only unlabeled
testingdataisprovided.Inaddition,ourmethodtriestoprovideanexplanationbehindthediscrimination.AcomplementaryapproachbyPapernot
et al.[56]showssuchexplainabilitybehindmodelbias
in a single classification setting.
7 DISCUSSION & THREATS TO VALIDITY
Discussion. In the literature, bug detection, debugging, and repair
areusuallythreedistincttasks,andthereisalargebodyofworkin-
vestigatingeachseparately.Inthiswork,wefocusonbugdetection
for image classifier software. A natural follow-up of our work will
bedebuggingandrepairleveragingDeepInspectâ€™sbugdetection.
We present some preliminary results and thoughts.
Acommonlyusedapproachtoimproving( i.e.fixing)imageclas-
sifiers is active learning, which consists of adding more labeleddata by smartly choosing what to label next. In our case, we canuse napvd to identify the most confusing class pairs, and thentarget those pairs by collecting additional examples that contain
individual objects from the confusing pairs. We download 105 sam-
pleimagesfromGoogleImagesthatcontainisolatedexamplesof
these categories so that the model learns to disambiguate them.
We retrain the model from scratch using the original training data
1131and these additional examples. Using this approach, we have some
preliminary results on the COCO dataset. After retraining, we find
that thetype2conf of the top confused pairs reduces. For example,
thetype2conf (baseball bat, baseball glove) is reduced from 0.23
to 0.16, and type2conf (refrigerator, oven) is reduced from 0.14 to
0.10. Unlike traditional active learning approaches that encourage
labelingadditionalexamplesnearthecurrentdecisionboundaryof
the classifier, our approach encourages the labeling of problematic
examples based on confusion bugs.
Another potential direction to explore is to use DeepInspect
in tandem with debugging & repair tools for DNN models like
MODE[43].DeepInspectenablestheusertofocusdebuggingeffort
onthevulnerableclassesevenintheabsenceoflabeleddata.For
instance,onceDeepInspectidentifiesthevulnerableclass-pairs,one
canusetheGAN-basedapproachproposedinMODEtogenerate
more training data from these class-pairs, apply MODE to identify
the most vulnerable features in these pairs to select for retraining.
Wehavealsoexploredhowtheneuroncoveragethreshold( th)
used in computing NAPVDaffects our performance in detecting
confusion and bias errors. We studied one multi-label classification
taskCOCOandonesingle-labelclassificationtaskCIFAR-100.Table
5, 6, 7, 8 show how our precision and recall change when usingdifferent neuron coverage thresholds (
th). We observed that for
CIFAR-100 and COCO that DeepInspectâ€™s accuracies are overallstable at 0
.4â‰¤thâ‰¤0.75. With smaller th( <0.25), too many
neuronsareactivatedpullingtheper-classactivation-probability-
vectors closer to each other. In contrast, with higher th( >0.75),
importantactivationinformationgetslost.Thus,weselect th=0.5
for all the other experiments to avoid either issue.
Table 5: DeepInspect impact of neuron coverage threshold on de-
tecting confusion errors for COCO
NC threshold napvd < mean-1std Top 1%
TP FP Precision Recall TP FP Precision Recall
0.25 36 18 0.67 0.20 23 8 0.74 0.13
0.40 150 215 0.41 0.84 31 0 1 0.17
0.50 138 256 0.35 0.78 31 0 1 0.17
0.60 137 264 0.34 0.77 30 1 0.97 0.17
0.75 135 271 0.33 0.76 29 2 0.94 0.16
Table 6: DeepInspect impact of neuron coverage threshold on de-
tecting confusion errors for CIFAR-100
NC threshold napvd < mean-1std Top 1%
TP FP Precision Recall TP FP Precision Recall
0.25 188 629 0.23 0.66 34 15 0.69 0.12
0.40 197 550 0.26 0.69 39 10 0.80 0.14
0.50 206 584 0.26 0.72 39 10 0.80 0.14
0.60 211 596 0.26 0.74 37 12 0.76 0.13
0.75 195 604 0.24 0.68 37 12 0.76 0.13
ThreatstoValidity. WeonlytestDeepInspecton6datasetsunder
8 settings. We include both single-class and multi-class as wellTable 7: DeepInspect impact of neuron coverage threshold on de-
tecting bias errors for COCO
NC threshold avg_bias > mean+1std Top 1%
TP FP Precision Recall TP FP Precision Recall
0.25 218 280 0.438 0.665 26 6 0.812 0.079
0.40 260 275 0.486 0.793 20 12 0.625 0.061
0.50 249 278 0.472 0.759 24 8 0.75 0.073
0.60 190 273 0.410 0.579 24 8 0.75 0.073
0.75 197 54 0.785 0.601 32 0 1 0.098
0.90 201 102 0.663 0.592 32 0 1 0.094
Table 8: DeepInspect impact of neuron coverage threshold on de-
tecting bias errors for CIFAR-100
NC threshold avg_bias > mean+1std Top 1%
TP FP Precision Recall TP FP Precision Recall
0.25 289 569 0.337 0.355 18 32 0.36 0.022
0.40 272 545 0.333 0.334 27 23 0.54 0.033
0.50 310 543 0.363 0.380 29 21 0.58 0.036
0.60 279 473 0.371 0.342 26 24 0.54 0.032
0.75 276 455 0.378 0.339 29 21 0.58 0.036
0.90 179 587 0.234 0.220 12 38 0.24 0.015
asregularandrobustmodelstoaddressthesethreatsasmuchas
possible.
AnotherlimitationisthatDeepInspectneedstodecidethresholds
forbothconfusionerrorsandbiaserrors,andathresholdfordis-
cardinglow-confusiontripletsintheestimationof avg_bias.Instead
of choosing fixed threshold, we mitigate this threat by choosing
thresholds that are one standard deviation from the corresponding
mean values and, also, reporting performance at top1%.
Thetaskofaccuratelyclassifyinganyimageisnotoriouslydif-
ficult. We simplify the problem by testing the DNN model only
for the classes that it has seen during training. For example, while
training, if a DNN does not learn to differentiate between black vs.
brown cows(i.e.,allthecowimagesonlyhavelabelcowandthey
aretreatedasbelongingtothesameclassbytheDNN),DeepInspect
will not be able to test these sub-groups.
8 CONCLUSION
OurtestingtoolforDNNimageclassifiers,DeepInspect,automat-
ically detects confusion and bias errors in classification models.
We applied DeepInspect to six different popular image classifica-
tion datasets and eight pretrained DNN models, including three
so-calledrelativelymorerobustmodels.WeshowthatDeepInspect
cansuccessfully detectclass-levelviolations forbothsingle- and
multi-label classification models with high precision.
ACKNOWLEDGMENTS
This work is supported in part by NSF CNS-1563555, CCF-1815494,
CNS-1842456, CCF-1845893, and CCF-1822965. Any opinions, find-
ings, conclusions, or recommendations expressed herein are those
oftheauthors,anddonotnecessarilyreflectthoseoftheUSGov-
ernment or NSF.
1132REFERENCES
[1]2017. Basepretrainedmodelsanddatasetsinpytorch. https://github.com/aaron-
xichen/pytorch-playground
[2]Erik Arisholm, Lionel C. Briand, and Eivind B. Johannessen. 2010. A systematic
andcomprehensiveinvestigationofmethodstobuildandevaluatefaultprediction
models.JSS83, 1 (2010), 2â€“17.
[3]SolonBarocas,MoritzHardt,andArvindNarayanan.2018. FairnessandMachine
Learning. fairmlbook.org. http://www.fairmlbook.org.
[4]Osbert Bastani, Yani Ioannou, Leonidas Lampropoulos, Dimitrios Vytiniotis,
Aditya Nori, and Antonio Criminisi. 2016. Measuring neural net robustness with
constraints. In Advances in Neural Information Processing Systems. 2613â€“2621.
[5] David Bau, BoleiZhou, AdityaKhosla, Aude Oliva,and AntonioTorralba. 2017.
NetworkDissection:QuantifyingInterpretabilityofDeepVisualRepresentations.
InComputer Vision and Pattern Recognition.
[6]Yoshua Bengio, Aaron Courville, and Pascal Vincent. 2013. Representation
learning:Areviewandnewperspectives. IEEEtransactionsonpatternanalysis
and machine intelligence 35, 8 (2013), 1798â€“1828.
[7]Yuriy Brun and Alexandra Meliou. 2018. Software Fairness. In Proceedings of the
201826thACMJointMeetingonEuropeanSoftwareEngineeringConferenceand
Symposium on the Foundations of Software Engineering (Lake Buena Vista, FL,
USA)(ESEC/FSE2018).ACM,NewYork,NY,USA,754â€“759. https://doi.org/10.
1145/3236024.3264838
[8]JoyBuolamwiniandTimnitGebru.2018. GenderShades:IntersectionalAccuracy
Disparities in Commercial Gender Classification. In FAT.
[9]T. Calders, F. Kamiran, and M. Pechenizkiy. 2009. Building Classifiers with
Independency Constraints. In 2009 IEEE International Conference on Data Mining
Workshops. 13â€“18.
[10]Nicholas Carlini and David Wagner. 2017. Towards evaluating the robustness
of neural networks. In Security and Privacy (SP), 2017 IEEE Symposium on . IEEE,
39â€“57.
[11]Yinpeng Dong, Hang Su, Jun Zhu, and Fan Bao. 2017. Towards interpretable
deep neural networks by leveraging adversarial examples. arXiv preprint
arXiv:1708.05493 (2017).
[12]Michele Donini, Luca Oneto, Shai Ben-David, John Shawe-Taylor, and Massi-
milianoPontil.2018. EmpiricalRiskMinimizationUnderFairnessConstraints.
InAdvancesinNeuralInformationProcessingSystems31:AnnualConferenceon
Neural Information Processing Systems 2018, NeurIPS 2018, 3-8 December 2018,
MontrÃ©al, Canada. 2796â€“2806. http://papers.nips.cc/paper/7544-empirical-risk-
minimization-under-fairness-constraints
[13]CynthiaDwork,MoritzHardt,ToniannPitassi,OmerReingold,andRichardS.
Zemel.2012. FairnessThroughAwareness. InProceedingsoftheInnovationsin
Theoretical Computer Science Conference abs/1104.3913 (2012), 214â€“226.
[14]Ivan Evtimov, Kevin Eykholt, Earlence Fernandes, Tadayoshi Kohno, Bo Li, Atul
Prakash,AmirRahmati,andDawnSong.2017. RobustPhysical-WorldAttacks
on Machine Learning Models. arXiv preprint arXiv:1707.08945 (2017).
[15]Reuben Feinman, Ryan R Curtin, Saurabh Shintre, and Andrew B Gardner. 2017.
DetectingAdversarialSamplesfromArtifacts. arXivpreprintarXiv:1703.00410
(2017).
[16]Sainyam Galhotra, Yuriy Brun, and Alexandra Meliou. 2017. Fairness testing:
testing software for discrimination. In Proceedings of the 2017 11th Joint Meeting
on Foundations of Software Engineering. ACM, 498â€“510.
[17]Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. 2015. Explainingand harnessing adversarial examples. In International Conference on Learning
Representations (ICLR).
[18]Kathrin Grosse, Praveen Manoharan, Nicolas Papernot, Michael Backes, andPatrick McDaniel. 2017. On the (statistical) detection of adversarial examples.
arXiv preprint arXiv:1702.06280 (2017).
[19]Loren Grush. 2015. Google engineer apologizes after Photos app tags two black
people as gorillas. (2015). https://www.theverge.com/2015/7/1/8880363/google-
apologizes-photos-app-tags-two-black-people-gorillas
[20]Shixiang Gu and Luca Rigazio. 2015. Towards deep neural network architec-tures robust to adversarial examples. In International Conference on Learning
Representations (ICLR).
[21]MoritzHardt,EricPrice,andNathanSrebro.2016. EqualityofOpportunityin
SupervisedLearning.In Proceedingsofthe30thInternationalConferenceonNeural
Information Processing Systems (Barcelona, Spain) (NIPSâ€™16). USA, 3323â€“3331.
[22]KaimingHe,XiangyuZhang,ShaoqingRen,andJianSun.2016. Deepresidual
learning for image recognition. In Proceedings of the IEEE conference on computer
vision and pattern recognition. 770â€“778.
[23]Warren He, James Wei, XinyunChen, Nicholas Carlini,and Dawn Song.2017.
Adversarial Example Defenses: Ensembles of Weak Defenses Are Not Strong. In
Proceedings of the 11th USENIX Conference on Offensive Technologies (Vancouver,
BC, Canada) (WOOTâ€™17). USENIX Association, Berkeley, CA, USA, 15â€“15. http:
//dl.acm.org/citation.cfm?id=3154768.3154783
[24]SandyHuang,NicolasPapernot,IanGoodfellow,YanDuan,andPieterAbbeel.
2017. Adversarial attacks on neural network policies. arXiv preprint
arXiv:1702.02284 (2017).[25]Xiaowei Huang, Marta Kwiatkowska, Sen Wang, and Min Wu. 2017. Safety
verification of deep neural networks. In International Conference on Computer
Aided Verification. Springer, 3â€“29.
[26]PoojaKamavisdar,SonamSaluja,andSonuAgrawal.2013. Asurveyonimage
classification approaches and techniques. International Journal of Advanced
Research in Computer and Communication Engineering 2, 1 (2013), 1005â€“1009.
[27]YasutakaKamei,ShinsukeMatsumoto,AkitoMonden,Ken-ichiMatsumoto,Bram
Adams, and Ahmed E Hassan. 2010. Revisiting common bug prediction findings
using effort-aware models. In 2010 IEEE International Conference on Software
Maintenance. IEEE, 1â€“10.
[28]GuyKatz,ClarkBarrett,DavidL.Dill,KyleJulian,andMykelJ.Kochenderfer.2017.
Reluplex: An Efficient SMTSolverfor VerifyingDeep Neural Networks. Springer
International Publishing, Cham, 97â€“117.
[29]Jinhan Kim, Robert Feldt, and Shin Yoo. 2019. Guiding deep learning system
testingusingsurpriseadequacy.In Proceedingsofthe41stInternationalConference
on Software Engineering. IEEE Press, 1039â€“1049.
[30]Michael P. Kim, Omer Reingold, and Guy N. Rothblum. 2018. Fairness Through
Computationally-BoundedAwareness. 32ndConferenceonNeuralInformation
Processing Systems (NeurIPS 2018) (2018).
[31]JernejKos,IanFischer,andDawnSong.2017.Adversarialexamplesforgenerative
models.arXiv preprint arXiv:1702.06832 (2017).
[32] Alex Krizhevsky. 2012. Learning Multiple Layersof Features from Tiny Images.
University of Toronto (05 2012).
[33]William H. Kruskal and W. Allen Wallis. 1952. Use of Ranks inOne-Criterion Variance Analysis. J. Amer. Statist. Assoc. 47, 260
(1952), 583â€“621. https://doi.org/10.1080/01621459.1952.10483441
arXiv:https://www.tandfonline.com/doi/pdf/10.1080/01621459.1952.10483441
[34]Alexey Kurakin, Ian Goodfellow, and Samy Bengio. 2017. Adversarial examples
in the physical world. In Workshop track at International Conference on Learning
Representations (ICLR).
[35]Matt J Kusner, Joshua Loftus, Chris Russell, and Ricardo Silva. 2017. Counterfac-
tualFairness.In AdvancesinNeuralInformationProcessingSystems30 .4066â€“4076.
[36]Alexandre Louis Lamy, Ziyuan Zhong, Aditya Krishna Menon, and Nakul
Verma. 2019. Noise-tolerant fair classification. CoRRabs/1901.10837 (2019).
arXiv:1901.10837 http://arxiv.org/abs/1901.10837
[37]Yann LeCun, LÃ©on Bottou, Yoshua Bengio, Patrick Haffner, et al .1998. Gradient-
based learning applied to document recognition. Proc. IEEE 86, 11 (1998), 2278â€“
2324.
[38]Tsung-YiLin,MichaelMaire,SergeBelongie,JamesHays,PietroPerona,Deva
Ramanan, Piotr DollÃ¡r, and C Lawrence Zitnick. 2014. Microsoft coco: Common
objects in context. In European conference on computer vision. Springer, 740â€“755.
[39]Zachary C Lipton. 2016. The mythos of model interpretability. Proceedings of the
33rd International Conference on Machine Learning Workshop (2016).
[40]Jiajun Lu, Hussein Sibai, Evan Fabry, and David Forsyth. 2017. No need to
worry about adversarial examples in object detection in autonomous vehicles. In
Spotlight Oral Workshop at Proceedings of the IEEE conference on computer vision
and pattern recognition.
[41]Binh Thanh Luong, Salvatore Ruggieri, and Franco Turini. 2011. k-NN as an
implementation of situation testing for discrimination discovery and prevention.
InProceedingsofthe17thACMSIGKDDinternationalconferenceonKnowledge
discovery and data mining. ACM, 502â€“510.
[42]LeiMa,FelixJuefei-Xu,FuyuanZhang,JiyuanSun,MinhuiXue,BoLi,ChunyangChen,TingSu,LiLi,YangLiu,JianjunZhao,andYadongWang.2018. DeepGauge:
Multi-granularityTestingCriteriaforDeepLearningSystems. (2018),120â€“131.
https://doi.org/10.1145/3238147.3238202
[43]ShiqingMa,YingqiLiu,Wen-ChuanLee,XiangyuZhang,andAnanthGrama.
2018. MODE:AutomatedNeuralNetworkModelDebuggingviaStateDifferential
Analysis andInput Selection. In Proceedings ofthe 2018 26thACM Joint Meeting
on European Software Engineering Conference and Symposium on the Foundations
ofSoftware Engineering (LakeBuena Vista,FL, USA) (ESEC/FSE 2018).ACM,New
York, NY, USA, 175â€“186. https://doi.org/10.1145/3236024.3236082
[44]MalletsDarker. 2018. I took a few shots at Lake Louise today and Google offered
me this panorama. (2018). https://www.reddit.com/r/funny/comments/7r9ptc/i_
took_a_few_shots_at_lake_louise_today_and/dsvv1nw/
[45]Chengzhi Mao, Ziyuan Zhong, Junfeng Yang, Carl Vondrick, and Baishakhi Ray.
2019. MetricLearningforAdversarialRobustness. In AdvancesinNeuralInforma-
tion Processing Systems 32, H. Wallach, H. Larochelle, A. Beygelzimer, F. d 'AlchÃ©-
Buc, E. Fox, and R. Garnett (Eds.). Curran Associates, Inc., 478â€“489. http:
//papers.nips.cc/paper/8339-metric-learning-for-adversarial-robustness.pdf
[46]AdityaKrishnaMenonandRobertC.Williamson.2018. Thecostoffairnessin
binary classification. In Conference on Fairness, Accountability and Transparency,
FAT 2018, 23-24 February 2018, New York, NY, USA. 107â€“118. http://proceedings.
mlr.press/v81/menon18a.html
[47]Jan Hendrik Metzen, Tim Genewein, Volker Fischer, and Bastian Bischoff. 2017.
On detecting adversarial perturbations. In International Conference on Learning
Representations (ICLR).
[48]Thomas M. Mitchell. 1997. Machine Learning (1 ed.). McGraw-Hill, Inc., New
York, NY, USA.
1133[49]GrÃ©goireMontavon,WojciechSamek,andKlaus-RobertMÃ¼ller.2017.Methodsfor
interpreting and understanding deep neural networks. Digital Signal Processing
(2017).
[50]Vinod Nair and Geoffrey E Hinton. 2010. Rectified linear units improve re-
strictedboltzmannmachines.In Proceedingsofthe 27thinternationalconference
on machine learning (ICML-10). 807â€“814.
[51]Nina Narodytska and Shiva Prasad Kasiviswanathan. 2016. Simple black-box
adversarialperturbationsfordeepnetworks.In WorkshoponAdversarialTraining,
NIPS 2016.
[52]AnhNguyen,JasonYosinski,andJeffClune.2015.Deepneuralnetworksareeasily
fooled: High confidence predictions for unrecognizable images. In Proceedings of
the IEEE Conference on Computer Vision and Pattern Recognition. 427â€“436.
[53] Augustus Odena,CatherineOlsson,DavidAndersen,andIanGoodfellow.2019.
TensorFuzz: Debugging Neural Networks with Coverage-Guided Fuzzing. In
Proceedingsofthe36thInternationalConferenceonMachineLearning (Proceedings
ofMachineLearningResearch),KamalikaChaudhuriandRuslanSalakhutdinov
(Eds.),Vol.97.PMLR,LongBeach,California,USA,4901â€“4911. http://proceedings.
mlr.press/v97/odena19a.html
[54]Nicolas Papernot, Nicholas Carlini, Ian Goodfellow, Reuben Feinman, Fartash
Faghri,Alexander Matyasko,KarenHambardzumyan, Yi-LinJuang,AlexeyKu-
rakin, Ryan Sheatsley, et al .2016. cleverhans v2. 0.0: an adversarial machine
learning library. arXiv preprint arXiv:1610.00768 (2016).
[55]Nicolas Papernot and Patrick McDaniel. 2017. Extending Defensive Distillation.
arXiv preprint arXiv:1705.05264 (2017).
[56]Nicolas Papernot and Patrick McDaniel. 2018. Deep k-Nearest Neighbors:
Towards Confident, Interpretable and Robust Deep Learning. arXiv preprint
arXiv:1803.04765 (2018).
[57]Nicolas Papernot, Patrick McDaniel, Ian Goodfellow, Somesh Jha, Z Berkay
Celik, and Ananthram Swami. 2017. Practical black-box attacks against machine
learning. In Proceedings of the 2017 ACM on Asia Conference on Computer and
Communications Security. ACM, 506â€“519.
[58]NicolasPapernot,PatrickMcDaniel,SomeshJha,MattFredrikson,ZBerkayCelik,
and Ananthram Swami. 2016. The limitations of deep learning in adversarial
settings. In 2016 IEEE European Symposium on Security and Privacy (EuroS&P) .
IEEE, 372â€“387.
[59]Nicolas Papernot, Patrick McDaniel, Xi Wu, Somesh Jha, and Ananthram Swami.
2016. Distillationasadefensetoadversarialperturbationsagainstdeepneural
networks. In Security and Privacy (SP), 2016 IEEE Symposium on. IEEE, 582â€“597.
[60]Kexin Pei, Yinzhi Cao, Junfeng Yang, and Suman Jana. 2017. DeepXplore: Au-tomated Whitebox Testing of Deep Learning Systems. (2017), 1â€“18. https:
//doi.org/10.1145/3132747.3132785
[61]KexinPei,YinzhiCao,JunfengYang,andSumanJana.2017. TowardsPractical
Verification of Machine Learning: The Case of Computer Vision Systems. arXiv
preprint arXiv:1712.01785 (2017).
[62]Aditi Raghunathan,Jacob Steinhardt, and PercyLiang. 2018. Certifieddefenses
against adversarial examples. 6th International Conference on Learning Represen-
tations (ICLR) (2018).
[63]Foyzur Rahman and Premkumar Devanbu. 2013. How, and why, process metrics
arebetter.In 201335thInternationalConferenceonSoftwareEngineering(ICSE).
IEEE, 432â€“441.
[64]FoyzurRahman,DarylPosnett,IsraelHerraiz,andPremkumarDevanbu.2013.
Sample size vs. bias in defect prediction. In Proceedings of the 2013 9th joint
meeting on foundations of software engineering. ACM, 147â€“157.
[65]F.Rahman,D.Posnett,A.Hindle,E.Barr,andP.Devanbu.2011. BugCachefor
inspections: hit or miss?. In Proceedings of the 19th ACM SIGSOFT symposium
andthe13thEuropeanconferenceonFoundationsofsoftwareengineering.ACM,
322â€“331.
[66]Baishakhi Ray, Vincent Hellendoorn, Saheel Godhane, Zhaopeng Tu, Alberto
Bacchelli,andPremkumarDevanbu.2016. Onthe"naturalness"ofbuggycode.
In2016IEEE/ACM38thInternationalConferenceonSoftwareEngineering(ICSE).
IEEE, 428â€“439.
[67]AdamRose.2010. AreFace-DetectionCamerasRacist? (2010). http://content.
time.com/time/business/article/0,8599,1954643,00.html
[68]AmirRosenfeld,RichardS.Zemel,andJohnK.Tsotsos.2018. TheElephantin
theRoom. CoRRabs/1808.03305(2018). arXiv:1808.03305 http://arxiv.org/abs/
1808.03305
[69]DavidERumelhart,GeoffreyEHinton,andRonaldJWilliams.1988. Learning
representations by back-propagating errors. Cognitive modeling 5, 3 (1988), 1.
[70]OlgaRussakovsky,JiaDeng,HaoSu,JonathanKrause,SanjeevSatheesh,SeanMa,ZhihengHuang,AndrejKarpathy,AdityaKhosla,MichaelBernstein,AlexanderC.
Berg, andLi Fei-Fei.2015. ImageNet Large ScaleVisual RecognitionChallenge.
International Journal of Computer Vision (IJCV) 115, 3 (2015), 211â€“252. https:
//doi.org/10.1007/s11263-015-0816-y
[71]R. R. Selvaraju, M. Cogswell, A. Das, R. Vedantam, D. Parikh, and D. Batra.
2017. Grad-CAM: Visual Explanations from Deep Networks via Gradient-BasedLocalization. In 2017 IEEE International Conference on Computer Vision (ICCV).
618â€“626. https://doi.org/10.1109/ICCV.2017.74
[72]Uri Shaham, Yutaro Yamada, and Sahand Negahban. 2015. Understanding adver-
sarial training: Increasing local stability of neural nets through robust optimiza-
tion.arXiv preprint arXiv:1511.05432 (2015).
[73]Karen Simonyan and Andrew Zisserman. 2015. Very deep convolutional net-
works for large-scale image recognition. In International Conference on Learning
Representations (ICLR).
[74]Youcheng Sun, Xiaowei Huang, and Daniel Kroening. 2018. Testing Deep Neural
Networks. arXiv preprint arXiv:1803.04792 (2018).
[75]Youcheng Sun, Min Wu, Wenjie Ruan, Xiaowei Huang, Marta Kwiatkowska,and Daniel Kroening. 2018. Concolic Testing for Deep Neural Networks. In
Proceedingsofthe33rdACM/IEEEInternationalConferenceonAutomatedSoftwareEngineering (Montpellier,France) (ASE2018).ACM,NewYork,NY,USA,109â€“119.
https://doi.org/10.1145/3238147.3238172
[76]C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan, I. Goodfellow, and R.
Fergus.2014.Intriguingpropertiesofneuralnetworks.In InternationalConference
on Learning Representations (ICLR).
[77]Yuchi Tian, Kexin Pei, Suman Jana, and Baishakhi Ray. 2018. DeepTest: Auto-
mated testing of deep-neural-network-driven autonomous cars. In International
Conference of Software Engineering (ICSE), 2018 IEEE conference on. IEEE.
[78]GrigoriosTsoumakasandIoannisKatakis.2007. Multi-labelclassification:An
overview. InternationalJournalofDataWarehousingandMining(IJDWM) 3,3
(2007), 1â€“13.
[79]Jingyi Wang, Guoliang Dong, Jun Sun, Xinyu Wang, and Peixin Zhang. 2019.
Adversarial sample detection for deep neural network through model mutation
testing.In Proceedingsofthe41stInternationalConferenceonSoftwareEngineering.
IEEE Press, 1245â€“1256.
[80]Shiqi Wang, Kexin Pei, Justin Whitehouse, Junfeng Yang, and Suman Jana. 2018.
Formal Security Analysis of Neural Networks using Symbolic Intervals. (2018).
[81]IanHWittenandEibeFrank.2005. DataMining:Practicalmachinelearningtools
and techniques. Morgan Kaufmann.
[82]Eric Wong, Frank Schmidt, Jan Hendrik Metzen, and J. Zico Kolter. 2018. Scaling
provable adversarial defenses. In Advances in Neural Information Processing
Systems31,S.Bengio,H.Wallach,H.Larochelle,K.Grauman,N.Cesa-Bianchi,
and R. Garnett (Eds.). Curran Associates, Inc., 8410â€“8419. http://papers.nips.cc/
paper/8060-scaling-provable-adversarial-defenses.pdf
[83]Cihang Xie, Yuxin Wu, Laurens van der Maaten, Alan L. Yuille, and Kaiming
He. 2019. Feature Denoising for Improving Adversarial Robustness. In The IEEE
Conference on Computer Vision and Pattern Recognition (CVPR).
[84]Weilin Xu, David Evans, and Yanjun Qi. 2017. Feature Squeezing: Detecting
Adversarial Examples in Deep Neural Networks. arXiv preprint arXiv:1704.01155
(2017).
[85]MarkYatskar,LukeZettlemoyer,andAliFarhadi.2016. SituationRecognition:Vi-sualSemanticRoleLabelingforImageUnderstanding.In ConferenceonComputer
Vision and Pattern Recognition.
[86]X. Yuan, P. He, Q. Zhu, and X. Li. 2019. Adversarial Examples: Attacks and
Defenses for Deep Learning. IEEE Transactions on Neural Networks and Learning
Systems(2019), 1â€“20. https://doi.org/10.1109/TNNLS.2018.2886017
[87]Muhammad Bilal Zafar, Isabel Valera, Manuel Gomez Rodriguez, and Krishna P.
Gummadi. 2017. Fairness Beyond Disparate Treatment & Disparate Impact:
LearningClassificationWithoutDisparateMistreatment.In Proceedingsofthe
26th International Conference on World Wide Web (Perth, Australia). 1171â€“1180.
[88]Muhammad Bilal Zafar, Isabel Valera, Manuel Gomez Rodriguez, and Krishna P
Gummadi. 2017. Fairness constraints: Mechanisms for fair classification. InProceedings of the 20th International Conference on Artificial Intelligence and
Statistics ((AISTATS) 2017), Vol. 54. JMLR.
[89]Rich Zemel, Yu Wu, Kevin Swersky, Toni Pitassi, and Cynthia Dwork. 2013.
LearningFairRepresentations.In Proceedingsofthe30thInternationalConference
on Machine Learning. 325â€“333.
[90]MengshiZhang,YuqunZhang,LingmingZhang,CongLiu,andSarfrazKhurshid.2018. DeepRoad:GAN-basedMetamorphicAutonomousDrivingSystemTesting.
arXiv preprint arXiv:1802.02295 (2018).
[91]Quan-shi Zhang and Song-Chun Zhu. 2018. Visual interpretability for deep
learning: a survey. Frontiers of Information Technology & ElectronicEngineering
19, 1 (2018), 27â€“39.
[92]JieyuZhao,TianluWang,MarkYatskar,VicenteOrdonez,andKai-WeiChang.
2017. Men Also Like Shopping: Reducing Gender Bias Amplification usingCorpus-level Constraints. In Proceedings of the 2017 Conference on Empirical
MethodsinNaturalLanguageProcessing.2941â€“2951. https://www.aclweb.org/
anthology/D17-1319
[93]StephanZheng,YangSong,ThomasLeung,andIanGoodfellow.2016. Improving
therobustnessofdeepneuralnetworksviastabilitytraining.In Proceedingsof
the IEEE Conference on Computer Vision and Pattern Recognition. 4480â€“4488.
1134