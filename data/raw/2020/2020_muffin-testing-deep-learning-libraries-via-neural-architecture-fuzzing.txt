Muffin: Testing Deep Learning Libraries
via Neural Architecture Fuzzing
Jiazhen Gu
School of Computer Science, Fudan University
Shanghai Key Lab. of Intelligent Information Processing
Shanghai, ChinaXuchuan Luo
School of Computer Science
Fudan University
Shanghai Key Lab. of Intelligent Information Processing
Shanghai, China
Yangfan Zhouâˆ—
School of Computer Science
Fudan University
Shanghai Key Lab. of Intelligent Information Processing
Shanghai, ChinaXin Wang
School of Computer Science
Fudan University
Shanghai Key Lab. of Intelligent Information Processing
Shanghai, China
ABSTRACT
Deeplearning(DL)techniquesareproveneffectiveinmanychal-
lenging tasks, and become widely-adopted in practice. However,
previous work has shown that DL libraries, the basis of building
and executing DL models, contain bugs and can cause severe con-sequences. Unfortunately, existing testing approaches still cannot
comprehensivelyexerciseDLlibraries.Theyutilizeexistingtrainedmodelsandonlydetectbugsinmodelinferencephase.Inthiswork
we propose Muffinto address these issues. To this end, Muffin
appliesaspecifically-designedmodelfuzzingapproach,whichal-
lows it to generate diverse DL models to explore the target library,
insteadofrelyingonlyonexistingtrainedmodels. Muffinmakes
differential testing feasible in the model training phase by tailoring
asetofmetricstomeasuretheinconsistenciesbetweendifferent
DL libraries. In this way, Muffincan best exercise the library code
to detect more bugs. To evaluate the effectiveness of Muffin,w e
conductexperimentsonthreewidely-usedDLlibraries.Theresultsdemonstratethat Muffincandetect39newbugsinthelatestrelease
versions of popular DL libraries, including Tensorflow, CNTK, and
Theano.
CCS CONCEPTS
â€¢Software and its engineering â†’Software testing and de-
bugging; Software libraries and repositories .
KEYWORDS
DeepLearningTesting,LibraryTesting,ModelGeneration,Fuzzing
âˆ—Yangfan Zhou is the corresponding author.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation
on the first page. Copyrights for components of this work owned by others than ACMmustbehonored.Abstractingwithcreditispermitted.Tocopyotherwise,orrepublish,topostonserversortoredistributetolists,requirespriorspecificpermissionand/ora
fee. Request permissions from permissions@acm.org.
ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
Â© 2022 Association for Computing Machinery.
ACM ISBN 978-1-4503-9221-1/22/05...$15.00
https://doi.org/10.1145/3510003.3510092ACM Reference Format:
Jiazhen Gu, Xuchuan Luo, Yangfan Zhou, and Xin Wang. 2022. Muffin:
TestingDeepLearningLibrariesviaNeuralArchitectureFuzzing.In 44th
International Conference on Software Engineering (ICSE â€™22), May 21â€“29, 2022,
Pittsburgh, PA, USA. ACM, New York, NY, USA, 13pages.https://doi.org/10.
1145/3510003.3510092
1 INTRODUCTION
Deep learning (DL) techniques have been proven effective in many
specific tasks, such as image recognition [ 29], video understand-
ing[53]andmachinetranslation[ 49].Asaresult,itbecomesatrend
to include DL-based functionality into traditional software design.
DLsystems(i.e.,softwaresystemsbasedonDLtechniques)have
beenwidelyadoptedinvariousdomainsinpractice, e.g.,self-driving
cars[28],virtualassistants[ 34],andsoftwareoperations[ 11,12,25].
However, DL systems are shown to be lack of robustness, and thus
causereal-worldaccidents.Forinstance,aTesladriverwaskilled
in self-driving mode that failed to brake the car in 2016 [ 6], and
an Uber autonomous driving car killed a pedestrian in 2018 [ 7].
Errors/defectsinDLsystemscancausesevereconsequences,and
evenjeopardizehumanlives.Therefore,itisacriticaltasktotest
DL systems before deploying them in real production scenarios.
Unfortunately, how to test a DL system still remains an open
challenge to the software engineering community. Many recent
approaches focus on testing its core component, the DL model, i.e.,
adeepneuralnetworktrainedwithasetoftrainingdata.Extensive
workaimsatimprovingtherobustnessofDLmodelsviagenerating
adequate test cases, e.g., adversarial inputs or corner cases [ 48,60].
Manystudiesalsofocusondesigningcriteriatomeasurethetest
adequacy [35, 43].
However,theexecutionofDLmodelsreliesontheirback-end
libraries(i.e.,DLlibraries).Evenwithacorrectmodeldesign,the
outputscanbewrongiftheunderlyinglibrarycontainsbugs.Specif-
ically,DLlibrariesprovidehigh-levelinterfacesoftheunderlying
various computation implementations (e.g., matrix transformation,
gradient calculation and weight update) over hardware infrastruc-
ture (e.g., CPU and GPU). Bugs in DL libraries can inevitably cause
unexpectedoutputs,orevenfatalfailureofDLsystems[ 33].But
14182022 IEEE/ACM 44th International Conference on Software Engineering (ICSE)
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:28:46 UTC from IEEE Xplore.  Restrictions apply. ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA Jiazhen Gu, Xuchuan Luo, Yangfan Zhou, and Xin Wang
one may tend to blame the DL model design, instead of its underly-
inglibrary,whendebugging[ 44],incurringmoredifficultytothe
process. Hence, it is critical to investigate how to test DL libraries.
Recentefforts(i.e.,CRADLE[ 44]andLEMON[ 50])onDLlibrary
testing focus on the inference phase of DL models. They adopt
differential testing [ 26] to detect bugs, by comparing the inference
results of existing, already-trained DL models with different DL
libraries. Specifically, CRADLE directly use such models as test
inputs,whileLEMONfurthermutatessuchmodelsastestinputs.
However,evenwiththeseapproaches,bugsstillexistinDLlibraries,
as we have found in this work. The key reason is that they relyon the inference phase of already-trained models, which cannot
exercisethelibrarycodescomprehensively.Suchalready-trained
models typically involve only a small set of DL library functions.Moreover, DL libraries also play an important role in the modeltraining phase, e.g., the library codes for back propagation [
30].
These library codes also cannot be exercised as well. But bugs in
thesecodescancauseincorrecttrainingresults, i.e.,wrongresulting
models.
Unfortunately,solvingtheseconcernsisachallengingtask.First,
itishard,ifnotinfeasible,toobtaintremendous,diversealready-
trained models to comprehensively exercise library codes. Muta-
tions based on such existing models also cannot solve this problem
as they inherit the model structures, limiting the exploration of
library functions. Moreover, as test oracles are not available gener-
ally,existingapproaches[ 44,50]resorttodifferentialtesting,based
on comparing the model outputs with different DL libraries. How-
ever, such outputs are not existing in the training phase, incurring
a huge challenge to applying differential testing.
In this work, we propose Muffin, a fuzzing-based approach to
testDLlibrarieswithhighfunctionalitycoverage.Insteadofrelying
onalready-trainedmodels, Muffinobtainsdiversetestinputs( i.e.,
models)withanautomaticmodelgenerationalgorithm.Itformu-
lates model structure as a Directed Acyclic Graph (DAG), basedon which it builds a model layer by layer with an aim to achieve
high functionality coverage of DL libraries. To perform differential
testing,Muffinreliesondatatraceanalysisinthetrainingphase.
In particular, we divide the model training phase into three differ-
entstages(i.e.,forwardcalculation,losscalculationandgradientcalculation), and accordingly design a set of metrics on the data
tracestomeasuretheconsistencyofresultsbydifferentDLlibraries.
Inconsistencies can thus indicate potential bugs.
We apply Muffinto test 15 release versions of three widely-
used DL libraries, i.e., TensorFlow [ 5], CNTK [ 2], and Theano [ 47].
Muffindetects39newbugs(including21crashbugs)inthelatest
release versions of these libraries. Extensive experiments based on
6populardatasetsshowthatcomparedwithexistingapproaches,
Muffiniscapableofdetectingmoreinconsistencieswithinacom-
parable testing time. Furthermore, we investigate the benefit ofour model generation method through comparing Muffinwith
layer-by-layertesting.Theresultsshowthat Muffiniscapableof
detectingmoreinconsistenciesandcrashes.Ourexperimentsprove
the effectiveness of Muffin.
Muffincontributestothesoftwaretestingartinthefollowing
three aspects:
Figure 1: The training phase of a DL model
â€¢We propose Muffin, a DL library testing approach based on
a novel DL model fuzzing method, which can exercise DL
library functionalities more comprehensively.
â€¢We make differential testing feasible in testing the model
trainingphase,byproposingadatatraceanalysismethodto
detect inconsistencies between different test targets.
â€¢Weimplementourideasasanopen-sourceavailablesoftwaretoolMuffin,whichcanfacilitatereal-worldDLlibrarytesting
tasks, as well as further follow-up research.
â€¢Weconductanextensivestudyon15versionsofthreewidely-usedDLlibraries.Theresultsshowthat Muffincandetect39
new bugs, which cannot be detected by previous methods.
Therestofpaperisorganizedasfollows.Section 2introduces
backgroundknowledgeaboutDLmodelandDLlibrary.Section 3
elaborates the design and implementation details of Muffin.W e
demonstratethe experimentalsetup inSection 4,andanalyze the
results in Section 5. Further discussion is provided in Section 6.
We introduce related work in Section 7and conclude the paper in
Section8.
2 BACKGROUND
2.1 Deep Learning Model
DLmodelsaredesignedtoautomaticallydrawstatisticalrulesfrom
training data [ 23]. A DL model typically consists of a number of
neurons with a layered, connected structure. The neurons between
layers are connected with links. Different links are associated with
different weights, which are obtained through training with input
data. Each layer conducts a specific kind of transformation (e.g.,
convolution andpooling) forthe inputdata with specific weights.
In particular, the same layer can be adopted multiple times in a
DL model, which has different weight values on the links and thus
produces diverse results.
Essentially, a developer would design the architecture of a DL
model such as the types of layers, how layers are connected and
thelossfunction.ThenthetrainingprocessofaDLmodelistofind
theappropriateweightvalues,sothattheoutputscanbestproduce
expected results. The training phase typically consists of a huge
amount of repeated training steps. Figure 1outlines the process of
a single training step, which can be divided into three stages:
â€¢Forward Calculation (FC) : Given a batch of training cases,
the model conducts specific calculations according to the
layer types and get the corresponding outputs.
1419
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:28:46 UTC from IEEE Xplore.  Restrictions apply. Muffin: Testing Deep Learning Libraries via Neural Architecture Fuzzing ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
Figure 2: The structure of DL libraries
â€¢Loss Calculation (LC) : The model calculates the value of the
predefined loss function, which measures the differences
between the model outputs and the ground-truth labels.
â€¢Backward Calculation (BC) : According to the value of the
loss function, the model calculates the gradients of each
neuronandupdatesthecorrespondingweightvaluesfrom
the output layer to the input layer.
Such training steps continue until the weights converge, i.e., the
performance of the model cannot be further improved.
TheweightvaluesdeterminehowtheDLmodelprocessesthe
inputtogenerateoutput.Thus,theperformanceofaDLmodel, i.e.,
whetheritcanproducecorrectresults,islargelydeterminedbythe
weights. Since the weight values are obtained through the training
process,itiscriticaltodetectbugsinmodeltrainingphase.How-
ever, existing work only focuses on detecting bugs in the inference
phase [44, 50].
2.2 Deep Learning Library
Figure2shows the structure of DL libraries. There are two tiers
of libraries (i.e., high-level and low-level). In general, developers
implementthesourceprogramswithhigh-levellibraryAPIs,which
invokethealgorithmsimplementedinlow-levellibraries.Different
low-level libraries are based on different infrastructures, e.g.,C PU ,
GPUandTensorProcessingUnit(TPU)[ 1],thusmayhavedifferent
implementationsforthesamealgorithmspecification.Ontheother
hand, high-level DL libraries can hide the differences between low-
levellibrariesandprovideaconsistentabstractiontofacilitateDLmodel development.
Keras [4] is one of the most popular high-level DL libraries
that has been widely used in various domains [ 15,32,40]. Keras
generally runs ontop of three low-level libraries, i.e., TensorFlow,
CNTK, and Theano, which cover most of the widely-used libraries.
DevelopersimplementsourceprogramsbycallingAPIsprovided
byKeras,whichinvoketheassignedbackendlow-levellibraryto
execute the computation.
Specifically,implementinga DLmodelusingKeras mainlycon-
tains three parts: loading the data, defining the model architecture,
and training the model with the data. It is worth noting that while
the training process includes complicated calculations (i.e., FC, LC
and BC), it can be simply implemented via calling the â€œmodel.fit()â€
function provided by Keras.
A high-level library is relatively simple, which glues the func-
tions in a low-level library that provide concrete, complicated com-
putation. Low-level libraries are not bug free and they are alsonot easy to be tested, due to their complication. Similar to existing
work[44,50],wefocusontestinglow-levellibraries, e.g.,Tensor-
Flow, CNTK, and Theano. We adopt Keras as the high-level library.
Our target is to test the DL library codes involved in the modeltraining phase with high functionality coverage. Specifically, DL
librariescontainmanyauxiliarycodesforvarioustaskssuchaspro-
filing and hardware adaptation, rather than learning-related ones.
LikeexistingtoolstotestDLlibraries,wefocusonlyonlearning-
related APIs. In this paper, we use functionality coverage as the
coveragemetric,whichreferstothepercentageoftheinvokedAPIs
in all the pre-defined, learning-related APIs we considered.
2.3 Challenges
In order to perform comprehensive DL library testing (e.g., test
thelibrarycodesinvolvedinmodeltraining),therearetwomain
challenges.First,itisdifficulttoobtainasetofDLmodelsastesting
inputs that cover most library APIs. A DL model has a layered,
connected structure, which hinders the adoption of traditional test
input generation approaches. Furthermore, many APIs in DL li-braries have specific usage scenarios, e.g.,Convolution Layer for
image processing tasks, Recurrent Layers for text processing and
various activation functions(e.g., ReLU [ 41] and leaky-ReLU[ 57]).
Due to such complication, it is non-trivial to obtain a set of well-
trained models to achieve high functionality coverage.
The second challenge is the test oracle in model training phase.
Existing approaches [ 44,50] utilize differential testing based on
the model outputs with different DL libraries. Unfortunately, as
we have discussed, DL models learn the weight values through
training. Therefore, the model outputs not exist in the training
phase, causing existing differential testing methods infeasible.
Next, we introduceour approach, Muffin, which isdesigned to
address the above two challenges.
3 APPROACH
3.1 Overview
Inthiswork,wepropose Muffin,anovelapproachtoperformcom-
prehensiveDLlibrarytesting, i.e.,testthelibrarycodesrelatedto
model training with high functionality coverage. Figure 3presents
theoverviewof Muffin,whichisspecificallytailoredtosolvethe
two design challenges.
To obtain diverse DL models, we propose a fuzzing-based model
generation method. In contrast to existing methods that adopt
manually-designed models, the proposed model generation ap-
proachallows Muffintoexercisethetargetlibrarywithtremendous,
diverse models. Specifically, we divide the model architecture into
two parts: structure information (i.e., how layers are connected)and layer information (i.e., what layer types are used). Throughformulating the structure information of a DL model as a DAG,
Muffinfirst generates DAGs as the structure information, and then
utilizes a greedy layer selection algorithm to generate the layer
information.Inthisway, MuffincangeneratediverseDLmodels
(Section3.2).
To conduct differential testing, Muffinperforms data trace anal-
ysisinthemodeltrainingphase.Inparticular, Muffinprofilesthe
data traces from different training stages (i.e., FC, LC and BC). Itthen detects the inconsistencies of different libraries based on a
1420
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:28:46 UTC from IEEE Xplore.  Restrictions apply. ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA Jiazhen Gu, Xuchuan Luo, Yangfan Zhou, and Xin Wang
Figure 3: Overview of Muffin
set of proposed metrics, which measures the output variance of
consecutive layers. (Section 3.3).
3.2 Model Generation
AsdiscussedinSection 2.1,aDLmodelhasalayeredstructurewith
connectionsbetweenlayers.InordertogenerateasetofdiverseDL
modelstoexplorelibrarycodes,weneedtodecidewhattypesof
layersareusedinamodel,aswellashowtheselayersareconnected.
Unfortunately,simplyselectingaseriesoflayersandstackingthem
together can easily cause model failure. For example, the â€œAddâ€
layerisusedtoaddalistofinputs.Ifonlyoneinputisfedtothis
layer, the model generation would fail. Besides, the inputs of â€œAddâ€
layershouldalsohavethesameshapetoavoidfailuregeneration.
Therefore, we design a top-down generation algorithm, which first
generates the structure information (i.e., the topology of how layers
areconnectedinthemodel),followedbythegeneratingofthe layer
information (i.e., specific layer types adopted in the model).
3.2.1 Structure Information Generation. Givenasetofinputs,aDL
model performs specific computation layer by layer, so as to yield
the outputs. Therefore, the computation flow of a DL model can be
abstractedasaDAG.Specifically,everyvertexintheDAGrepre-
sentsalayer,andeveryedgebetweentwoverticesrepresentsalink
betweenthecorrespondinglayersintheoriginalmodel.Suchan
abstraction method is also applied in current model representation.
For example, TensorFlow uses a DAG to represent the computa-tional graph of a DL model [
8]. Therefore, we utilize a DAG to
represent the structure information of a DL model.
AlthoughitisnotdifficulttogenerateaDAG,thecorresponding
model structure may be too simple or too complicated, which is
rarely used in practice. Inspired by recent studies in Neural Archi-
tecture Search (NAS) [ 20], that targets on automating the design
of model architectures, we summarize two model structure tem-plates, as shown in Figure 4. Specifically, Figure 4(a)shows the
chainstructurewithskips.Chain-structuredarchitectureisthesim-plestexampleofthemodelstructuretopology.Throughpermitting
arbitrary skip connections between nodes, this template can cover
manycommonly-usedDLmodels(e.g.,fully-connectednetworks,
VGG [45] and DenseNet [ 31]). On the other hand, the cell-based
(a) Chain structure with skips
 (b) Cell-based structure
Figure 4: Examples of DL model structure templates
structure in Figure 4(b)builds upon the observation that many
specifically-designedmodelarchitecturesconsistofrepetitionsof
fixed structures [ 52],e.g., ResNet [ 29]. Each cell in the structure
is a small DAG that conducts a specific transformation, e.g., the
computation cell in Figure 4(b)contains computation layers, while
the reduction cell is used for downsampling. It is worth noting
that originally thesame cells( e.g., computationcells)should have
the same DAG. Since our target is generating diverse structures
insteadoffindingthearchitecturewiththebestperformance,we
remove this restriction in the proposed template (i.e., same cells
may have different DAGs). In addition, we also guarantee that the
generated DAG has only one vertex whose in-degree is 0 as theinput layer, one vertex whose out-degree is 0 as the output layer.
There is also no isolated vertex in the generated DAG. In this way,
Muffingenerates a DAG as the model structure information.
3.2.2 Layer Information Generation. Giventhegeneratedstructure
information,weneedtorefinethelayerinformation, i.e.,determine
the specific layer type for each vertex in the DAG. As discussedbefore, stacking layers without guidance can easily cause model
failure. Specifically, there are two types of restrictions when select-
inglayers.Thefirstrestrictionisthe input number restriction.In
particular, most layers (e.g., â€œConvolutionâ€) are SI (Single-Input)
layers, while some layers (e.g., â€œConcatenationâ€) are MI (Multiple-
Input)ones.IfmorethanoneinputisfedtoanSIlayer,thislayer
1421
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:28:46 UTC from IEEE Xplore.  Restrictions apply. Muffin: Testing Deep Learning Libraries via Neural Architecture Fuzzing ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
canonlyprocessoneoftheinputs,leadingtotheexistenceofin-
valid connections between layers. The corresponding DAG thus
isequivalenttotheDAGwithouttheinvalidconnections,which
lowers the DAG diversity. Therefore, we have to choose the proper
layer according to the number of inputs. Considering that an edge
in a DAG represents the data flow direction, we can determine the
numberofinputsthatthecorrespondinglayertakesbasedonthe
in-degree of the vertex.
Thesecondrestrictionisthe input/output shape restriction.Specif-
ically,MIlayersrequiretheinputstohavethesameshapeinspecific
axis(es)soastoconductthetransformationproperly, e.g.,theinputs
ofâ€œConcatenationâ€layershouldhavethesameshapeexceptforthe
concatenationaxis.Therefore,beforefeedingtheinputstoanMI
layer, we adopt additional â€œReshapingâ€ layers to reshape the inputs
into the same shape. In addition, the input shape of the input layer
(i.e.,thevertexwith0in-degree),andtheoutputshapeoftheoutput
layer (i.e., the vertex with 0 out-degree) need to be properly set
according to the training data and the task type (e.g., classification,
regression). We still resort to the â€œReshapingâ€ layer to reshape the
outputsize,whiletheinputshapeisdirectlysetbasedontheshape
of input data.
Furthermore, in order to increase the diversity of the generated
models,intuitively,weshouldgivealargerchancetothelayerthat
is rarely used before. Based on this intuition, we design a layer
selection procedure based on Fitness Proportionate Selection [ 21].
Specifically, for a specific layer ğ‘™,Muffinrecords the number of
times that ğ‘™has been selected to construct a model, denoted as ğ‘.
ThenMuffincalculates ğ‘ =1
ğ‘+1as the score for ğ‘™. Based on ğ‘ , the
probabilitythat ğ‘™isselectedamongalllayertypescanbecalculated
as follows:
ğ‘=ğ‘ /summationtext.1ğ‘Ÿ
ğ‘˜=1ğ‘ ğ‘˜(1)
whereğ‘Ÿisthetotalnumberofpossiblelayers.Sincewedivide
layertypesintotwocategories(i.e.,SIandMI),score ğ‘ andproba-
bilityğ‘are calculated based on the layers belonging to the same
category.Inthisway, Muffingeneratesthelayerinformationvia
selectingaspecificlayerforeachvertexintheDAG.ADLmodel
can thus be constructed according to the generated structure infor-
mation and layer information.
3.2.3 Entire Algorithm. We formally describe our fuzzing-based
modelgenerationmethodinAlgorithm 1.Thisalgorithmtakes5
parameters,where ğ‘ğ‘šisthetotalnumberofmodelstogenerate,
serving as the terminating condition; ğ‘€ğ´ğ‘‹ ğ‘andğ‘€ğ´ğ‘‹ ğ‘£are param-
eters to control the size of DAG; ğ¿ğ‘–andğ¿ğ‘œshould be manually
set according to the input data and target task. Lines 2-33 itera-
tivelygenerateasetofDLmodels.Specifically,lines3-13randomly
chooseatemplateandgenerateaDAGasthestructureinformation.
Lines17-18settheinputlayer.Lines22-24selectSIlayersforthe
1 in-degree vertices, and Lines 26-29 select MI layers for the ver-
ticeswithmorethan1in-degree.Lines30-31settheoutputlayer.
Finally, lines 32-33 construct a DL model ğ‘šbased on the generated
structure information and layer information, then adding ğ‘što the
result set ğ‘€.Algorithm 1: Model Architecture Generation
Input:ğ‘ğ‘š: Number of generated models
ğ‘€ğ´ğ‘‹ ğ‘: Maximum number of cells in a model
ğ‘€ğ´ğ‘‹ ğ‘£: Maximum number of vertices in a DAG
ğ¿ğ‘–: Input shape
ğ¿ğ‘œ: Output shape
Output:ğ‘€: A set of generated models
1ğ‘€â†âˆ…;
2whileğ‘†ğ‘–ğ‘§ğ‘’(ğ‘€)<ğ‘ğ‘šdo
/* select a template and generate Structure
Information ğ‘†ğ¼ */
3ğ‘â†ğ‘…ğ‘ğ‘›ğ‘‘ğ‘œğ‘š(0,1);
4ifğ‘<0.5then
5ğ‘ğ‘£â†ğ‘…ğ‘ğ‘›ğ‘‘ğ‘œğ‘šğ¼ğ‘›ğ‘¡ (1,ğ‘€ğ´ğ‘‹ ğ‘£);
6ğ‘†ğ¼=ğ¶ğ‘Ÿğ‘’ğ‘ğ‘¡ğ‘’ğ¶â„ğ‘ğ‘–ğ‘›ğ·ğ´ğº (ğ‘ğ‘£);
7else
8ğ‘ğ‘â†ğ‘…ğ‘ğ‘›ğ‘‘ğ‘œğ‘šğ¼ğ‘›ğ‘¡ (1,ğ‘€ğ´ğ‘‹ ğ‘);
9ğºâ†âˆ…;
10 forğ‘–from 1 toğ‘ğ‘do
11 ğºğ‘–=ğ‘…ğ‘ğ‘›ğ‘‘ğ‘œğ‘šğ·ğ´ğº ();
12 ğºâ†ğºâˆª{ğºğ‘–};
13 ğ‘†ğ¼=ğ¶ğ‘Ÿğ‘’ğ‘ğ‘¡ğ‘’ğ¶ğ‘’ğ‘™ğ‘™ğ·ğ´ğº (ğ‘ğ‘,ğº);
14ğ¿ğ¼â†âˆ…; /* Layer Information */
15 /* generate model according to DAG */
16foreach nodeğ‘—inğ‘‡ğ‘œğ‘ğ‘œğ‘™ğ‘œğ‘”ğ‘–ğ‘ğ‘ğ‘™ğ‘†ğ‘’ğ‘ğ‘¢ğ‘’ğ‘›ğ‘ğ‘’ (ğ‘†ğ¼)do
17 ifthe in-degree of node ğ‘—is 0then
18 ğ¿ğ¼â†ğ¿ğ¼âˆª{ğ‘†ğ‘’ğ‘¡ğ¼ğ‘›ğ‘ğ‘¢ğ‘¡ğ¿ğ‘ğ‘¦ğ‘’ğ‘Ÿ (ğ¿ğ‘–,ğ‘—)};
19 else
20 ğ‘ƒğ‘—â†ğºğ‘’ğ‘¡ğ´ğ‘™ğ‘™ğ·ğ‘–ğ‘Ÿğ‘’ğ‘ğ‘¡ğ‘ƒğ‘Ÿğ‘’ğ‘‘ğ‘’ğ‘ğ‘’ğ‘ ğ‘ ğ‘œğ‘Ÿğ‘  (ğ‘—);
21 ifğ‘†ğ‘–ğ‘§ğ‘’(ğ‘ƒğ‘—)==1then /* SI layer */
22 ğ‘ â„ğ‘ğ‘ğ‘’â†ğºğ‘’ğ‘¡ğ‘†â„ğ‘ğ‘ğ‘’ (ğ‘ƒğ‘—[0]);
23 ğ¿ğ¼â†ğ¿ğ¼âˆª{ğ‘†ğ‘’ğ‘¡ğ‘†ğ¼ğ¿ğ‘ğ‘¦ğ‘’ğ‘Ÿ (ğ‘ â„ğ‘ğ‘ğ‘’,ğ‘—)};
24 ğ‘ˆğ‘ğ‘‘ğ‘ğ‘¡ğ‘’ğ‘†ğ¼ğ‘†ğ‘ğ‘œğ‘Ÿğ‘’ ()
25 else /* MI layer */
26 ğ‘ â„ğ‘ğ‘ğ‘’â†ğ‘…ğ‘ğ‘›ğ‘‘ğ‘œğ‘šğ‘†â„ğ‘ğ‘ğ‘’ ();
27 ğ¿ğ¼â†ğ¿ğ¼âˆªğ‘…ğ‘’ğ‘ â„ğ‘ğ‘ğ‘–ğ‘›ğ‘”ğ¿ğ‘ğ‘¦ğ‘’ğ‘Ÿğ‘  (ğ‘ƒğ‘—,ğ‘ â„ğ‘ğ‘ğ‘’);
28 ğ¿ğ¼â†ğ¿ğ¼âˆª{ğ‘†ğ‘’ğ‘¡ğ‘€ğ¼ğ¿ğ‘ğ‘¦ğ‘’ğ‘Ÿ (ğ‘ â„ğ‘ğ‘ğ‘’,ğ‘—)};
29 ğ‘ˆğ‘ğ‘‘ğ‘ğ‘¡ğ‘’ğ‘€ğ¼ğ‘†ğ‘ğ‘œğ‘Ÿğ‘’ ()
30 ifthe out-degree of node ğ‘—is 0then
31 ğ¿ğ¼â†ğ¿ğ¼âˆªğ‘†ğ‘’ğ‘¡ğ‘‚ğ‘¢ğ‘¡ğ‘ğ‘¢ğ‘¡ğ¿ğ‘ğ‘¦ğ‘’ğ‘Ÿ (ğ‘—,ğ¿ğ‘œ);
32ğ‘šâ†ğ¶ğ‘œğ‘›ğ‘ ğ‘¡ğ‘Ÿğ‘¢ğ‘ğ‘¡ğ‘€ğ‘œğ‘‘ğ‘’ğ‘™ (ğ‘†ğ¼,ğ¿ğ¼);
33ğ‘€â†ğ‘€âˆª{ğ‘š};
34returnğ‘€;
3.3 Inconsistency Detection
Inordertodetectinconsistenciesandperformdifferentialtesting
accordingly, Muffinrequiresproper metricsto measurethediffer-
ences between the execution results of different libraries. However,
the metrics proposed by the existing work are designed only for
already-trained models,which calculate theinconsistency between
the ground-truth label and model outputs. Since our target is to
test DL library in the training phase (i.e., without a trained model),
1422
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:28:46 UTC from IEEE Xplore.  Restrictions apply. ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA Jiazhen Gu, Xuchuan Luo, Yangfan Zhou, and Xin Wang
such metrics cannot be directly applied. Instead, we propose a new
metric based on the variance of outputs in consecutive layers.
AsdemonstratedinSection 2.1,themodeltrainingphaseincludes
repeated training steps, and each training step can be divided into
three stages: FC, LC and BC. Specifically, in FC stage, the model
performs calculation from input layer to output layer. In LC stage,
the model calculates the value of loss function. In BC stage, the
model calculates gradients from the output layer to the input layer.
Resorting to dynamic analysis, we can collect the data traces of
different libraries and compare the differences.
In particular, we utilize the Functional API mechanism provided
by Keras to collect dynamic traces. More specifically, we profile
theresults producedbyeverylayerinFCstage,thelossvalue in
LCstage,andthegradientvalueofeachlayerinBCstage.Based
on the dynamic trace, Muffingradually compares the values of
layeroutputs,theloss,andthegradients,soastodetectthesuspect
behavior of specific layers.
However, due to normal uncertain factors such as floating-point
deviation[ 22],wecannotdeterminewhetheravaluedifferenceis
causedbypotentialbugsornormalfactors.Specifically,thereare
manysmalldeviations(lessthan10âˆ’6)inlayeroutputs,whichmay
be gradually amplified or reduced, e.g., by pooling or activation
functions.Weconsiderthatnormalfactorswouldonlyleadtoslightlayeroutputdifference, i.e.,ifthedifferencesoflayeroutputschange
dramatically, it indicates a suspicious behavior. Therefore, instead
of comparing the outputs from one single layer, we consider the
difference-changesbetweentwoconsecutivelayers.Onlywhenthe
deviationisamplifiedwould Muffinconsiderinconsistency.Since
outputsfromdifferentlayersmayhavedifferentshapes,wefirstusethe following Chebyshev distance (i.e.,
ğ¿âˆdistance) [ 9] to measure
the difference of the outputs from the same layer.
ğ·(ğ‘‹,ğ‘Œ)=maxğ‘š(|ğ‘¥ğ‘šâˆ’ğ‘¦ğ‘š|) (2)
In the above equation, ğ‘‹andğ‘Œare two tensors (i.e., output of a
layeristypicallyahigh-dimensionaltensor),while ğ‘¥ğ‘andğ‘¦ğ‘are
elements in ğ‘‹andğ‘Œ, respectively. Chebyshev distance defines that
thedistancebetweentwotensorsisthegreatestoftheirdifferences
along any coordinate dimension. In this way, we can avoid theinfluence of different tensor shapes from different layers when
measuring the differences.
We now describe the inconsistency detection procedure of Muf-
fin. For brevity, we denote ğ‘›as the total number of layers, ğ‘™ğ‘–,ğ‘–âˆˆ
[1,ğ‘›]astheğ‘–ğ‘¡â„layer,ğ‘‚ğ‘–
ğ‘—andğ‘‚ğ‘–
ğ‘˜astheoutputsof ğ‘™ğ‘–usinglibrary
ğ‘—andğ‘˜,respectively. ğ‘ƒ(ğ‘–)denotesthesetoflayersthataredirect
predecessorsof ğ‘™ğ‘–intheDAG, i.e.,eachlayerin ğ‘ƒ(ğ‘–)isğ‘™ğ‘–â€™sprevious
layer.
InFC stage, Muffincomparesthe differencesof theoutput ten-
sorsfrom ğ‘™ğ‘–anditspredecessors ğ‘™ğ‘.Ifthedifferenceof ğ‘™ğ‘issmaller
thanğœ–,whilethedifferenceof ğ‘™ğ‘–islargerthanauser-definedthresh-
oldğ‘¡, thenMuffindetermine that an inconsistency is detected in
ğ‘™ğ‘–. The inconsistency layers detected in FC stage can be formally
defined as follows:
ğ¼ğ‘›ğ‘_ğ¹ğ¶ ={ğ‘™ğ‘–,ğ‘–âˆˆ[1,ğ‘›]|(ğ·(ğ‘‚ğ‘–
ğ‘—,ğ‘‚ğ‘–
ğ‘˜)>ğ‘¡)âˆ§
(ğ·(ğ‘‚ğ‘
ğ‘—,ğ‘‚ğ‘
ğ‘˜)<ğœ–,ğ‘âˆˆğ‘ƒ(ğ‘–))}Table 1: Versions of libraries under test
IDKerasTensorFlow Theano CNTK
E12.3.1 2.0.0 1.0.4 2.7.0
E22.3.1 1.15.0 1.0.3 2.6.0
E32.2.4 1.12.0 1.0.2 2.5.0
E42.2.4 1.11.0 1.0.1 2.4.0
E52.2.4 1.10.0 1.0.0 2.3.0
In LC stage, the model calculates loss value based on the results
from the output layer. To avoid the transmission of errors, Muffin
only performs inconsistency detection in LC stage when the differ-
ence of model outputs is smaller than ğœ–. It is worth noting that a
smalldifferenceinmodeloutputsdoesnotmeanthatthereisno
inconsistencyinmiddlelayers.Largedifferencecouldbemasked
duetotheexistenceofdownsamplinglayerssuchaspooling.Since
theresultofalossfunctionisanumber,wedirectlycomparethe
absolute difference as follows:
ğ¼ğ‘›ğ‘_ğ¿ğ¶={ğ¿|( ( |ğ¿ğ‘‚ğ‘—âˆ’ğ¿ğ‘‚ğ‘˜|>ğ‘¡)âˆ¨(|ğ¿ğºğ‘—âˆ’ğ¿ğºğ‘˜|>ğ‘¡))âˆ§
(ğ·(ğ‘‚ğ‘›
ğ‘—,ğ‘‚ğ‘›
ğ‘˜)<ğœ–)}
Intheaboveequation, ğ¿denotesthelossfunction, ğ¿ğ‘‚ğ‘—andğ¿ğ‘‚ğ‘˜
are the output results of ğ¿,ğ¿ğºğ‘—andğ¿ğºğ‘˜are the gradient results of
ğ¿,ğ‘‚ğ‘›
ğ‘—andğ‘‚ğ‘›
ğ‘˜are the model outputs, using library ğ‘—andğ‘˜.
In BC stage, the model calculates gradients to update weights
from the output layer to the input layer. Similarly, Muffinonly
conducts inconsistency detection if the difference in loss function
issmallerthan ğœ–.WeformulatetheinconsistencydetectioninBC
stage as follows:
ğ¼ğ‘›ğ‘_ğµğ¶={ğ‘™ğ‘–,ğ‘–âˆˆ[1,ğ‘›]|(ğ·(ğºğ‘–
ğ‘—,ğºğ‘–
ğ‘˜)>ğ‘¡)âˆ§
(ğ·(ğºğ‘ 
ğ‘—,ğºğ‘ 
ğ‘˜)<ğœ–,ğ‘ âˆˆğ‘†(ğ‘–))}
whereğ‘†(ğ‘–)denotes the set of layers that are direct successors of
ğ‘™ğ‘–;ğºğ‘–
ğ‘—andğºğ‘–
ğ‘˜denotethegradientresultof ğ‘™ğ‘–usingdifferentlibraries.
Especially, the successor of the output layer is the loss function.
4 EVALUATION SETUP
In the evaluation, we evaluate the performance of Muffinthrough
answering the following research questions.
â€¢RQ1:Howdoes MuffinperformindetectingbugsinDLlibraries?
â€¢RQ2:CanMuffinachievebetterperformancecomparedtoother
methods?
â€¢RQ3:Howdothedifferent parametersettingsaffecttheperfor-
mance of Muffin?
4.1 Libraries and Datasets
4.1.1 Libraries. We use three widely-used DL libraries (i.e., Ten-
sorFlow, Theano, and CNTK) as the back-end low-level libraries as
ourtestingtargets,andKerasasthefront-endhigh-levellibrary.To
sufficiently illustrate the effectiveness of Muffin, we utilize a total
of 15 release versions of the three back-end libraries, and construct
fiveexperimentalenvironmentsfordifferentialtesting, i.e.,E1-E5
in Table1. In particular, in E1, Keras 2.3.1 is the latest version that
supports multiple back-ends; Theano 1.0.4 and CNTK 2.7.0 are the
latest versions, while TensorFlow 2.0.0 is the latest version that
1423
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:28:46 UTC from IEEE Xplore.  Restrictions apply. Muffin: Testing Deep Learning Libraries via Neural Architecture Fuzzing ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
supported by Keras. For the sake of brevity, we use TF, TH, and
CKtorepresentTensorFlow,TheanoandCNTKinthefollowing
figures and tables.
4.1.2 Datasets. Ourapproachisnotsensitivetodatasets, i.e.,theo-
retically any data type can be used for testing. In order to facilitate
subsequent comparative experiments with existing approaches, we
selected 6 widely-used datasets in existing studies [ 50],i.e., MNIST,
F-MNIST, CIFAR-10, ImageNet, Sine-Wave and Stock-Price. Specifi-
cally, the first four are popular image classification datasets, while
thelasttwoaresequencedatasets.Inparticular,Sine-Waveisthe
sine function value sequence, and Stock-Price is the Disneyland
stock price sequence from 1997 to 2016.
4.2 Competitors
In order to demonstrate the effectiveness and efficiency of Muffin,
wecompare Muffinwiththestate-of-the-artapproach,LEMON[ 50].
LEMONperformsDLlibrarytestingthoughmutatingexistingmod-
elstogenerateahugeamountofnewtestinputs.Followingtheeval-uationsetupin[
50],weuse11existingmodels(i.e.,AlexNet,LeNet5,
ResNet50,MobileNetV1,InceptionV3,DenseNet121,VGG16,VGG19,
Xception, LSTM-1, LSTM-2) as the seed models for mutation. By
comparing with LEMON, we evaluate whether Muffin, based on
directedtestcases(i.e.,model)generation,canoutperformLEMON
in exposing bugs. Since LEMON cannot perform testing in the LC
and BC stages, we only compare Muffinwith LEMON through
analyzing the number of inconsistencies and bugs detected in the
FC stage. In addition, Muffinis designed to perform comprehen-
sive library testing, so we also compare the functionality coverage
achieved by Muffinand LEMON.
Besides,ourDAG-basedmodelgenerationisthecorecomponent
ofMuffin. Thus, it is also interesting to investigate the effective-
nessofthiscomponent.Tothisend,weimplement Muffin-UT,a
simplified Muffin-versionmethodbasedonunittesting. Muffin-UT
differs from Muffinonly in the model generation part. Specifically,
inMuffin-UT, a functional layer (e.g., Conv2D) is a to-be-tested
unit.Muffin-UT createsmodelswithonlyonefunctionallayer,and
simple reshapinglayers to copewith input/output, i.e., dimension
transformationtomatchtheinput/outputrequirementsoftheto-
be-tested layer. By comparing with Muffin-UT, we show that unit
testingisstillinadequatetotestDLlibraries. Muffin,bygenerating
diverse models, can expose bugs which are difficult to be detected
by traditional approaches.
4.3 Measurements
4.3.1 Number of inconsistencies. Sincetheproposedapproachcon-
ducts inconsistency detection in layer level during model training,
an inconsistency between two low-level DL libraries means that
they produce different calculation results given the same input un-
deraspecificlayer.Inordertoeliminateduplicatedinconsistencies
caused by the same function, we only count the inconsistenciesproduced by the same layer once. In particular, for Muffinand
Muffin-UT, we compare the number of inconsistencies detectedin different training stages, i.e., FC, LC and BC, respectively. For
LEMON,weonlycounttheinconsistenciesdetectedinFCstage.Al-
though different inconsistencies may be the manifests of the same
potentialbug,morefailure-triggeringtests(i.e.,themodelandinputdata that trigger the inconsistency) reflecting a fault in different
ways provide more information for fault localization. Therefore,
the number of detected inconsistencies can reflect the effects of
these methods to some extent.
4.3.2 Number of detected bugs. Although we count the number
of detected inconsistencies, it is more important to measure thenumber of unique bugs revealed by Muffin. Based on the voting
mechanism of differential testing, we can localize the buggy layer
in the library. To avoid false positives, we further check the buggy
layer manually. Specifically, we save all intermediate layer outputs
duringthetesting.Whenaninconsistencyisreported,twoauthors
check the corresponding source codes in different libraries and
comparetheresults.Iftheiridenticallayerproducesdifferentresults
and their implementationideas are different, the third authorwill
joinmanualinspectionsoastoconcludewhetherthereportistrue
or false positive.
4.3.3 Number of NaN/Crash bugs. Besidesinconsistentcalculation
results,bugsinDLlibrariesmayleadtoNaN(NotaNumber)and
crashesaswell[ 44].GeneratingDLmodelsthattriggerNaN/crashes
can also provide valuable information for identifying potentialbugs. Therefore, we count the number of models with NaN or
crashesgeneratedbythreemethods.Inparticular,weonlycount
the NaN/crash when at least one of the DL library can execute
properly, e.g.,TensorFlowproducesnormalresultswhileTheano
and CNTK produce NaN. In addition, in order to avoid duplication,
the NaNcaused by thesame layer,and crashes withthe same error
message are only counted once.
4.4 Implementations
Intheexperiments,weleteachmethodgenerateatotalof300mod-
els, 50 for each dataset. For LEMON, we use its default parameters.
ForMuffin, we set the maximum number of cells (i.e., ğ‘€ğ´ğ‘‹ ğ‘)t o
5,andthemaximumnumberofnodes( i.e.,ğ‘€ğ´ğ‘‹ ğ‘£)to30.Interms
of inconsistency detection, we set the threshold ğ‘¡to be 0.15, and
ğœ–to be 1ğ‘’âˆ’5. Thisğ‘¡value is relatively large so as to avoid many
falsepositives,asshownin Section 5.3.Inaddition, Muffindonot
considersomelayerssuchasâ€œDropoutâ€andâ€œGaussianNoiseâ€,soas to avoid introducing randomness and affecting the execution
results.
All the experimentsare conducted on theIntel(R) Core(TM) i7-
6700KCPU@4.00GHzmachinewith32GBofRAM,Ubuntu20.04.2
LTS, and one Nvidia GTX 1080 Ti GPU.
Theimplementationof MuffinispubliclyavailableonGitHub1.
5 RESULTS AND ANALYSIS
5.1 Effectiveness of Bug Detection
We first investigate the effectiveness of Muffinin terms of new
bugsdetectedinthelatestversionsofdifferentlibraries, i.e.,E1in
Table1. After manual analysis, Muffindetects 18 bugs in the latest
versionoftheselibraries,including12bugsinFCstage,2bugsin
LC stage, 3 bugs in BC stage and 1 NaN bugs, as shown in Table 2.
Inaddition, Muffinalsodetects21crashbugs,mainlyfromTheano
and CNTK.
1https://github.com/library-testing/Muffin
1424
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:28:46 UTC from IEEE Xplore.  Restrictions apply. ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA Jiazhen Gu, Xuchuan Luo, Yangfan Zhou, and Xin Wang
Table 2: New bugs and crashes detected by Muffin
LibraryFCBug LC Bug BC Bug NaN Crash
TensorFlow 0 2 1 0(1) 1
Theano 8 02 0 10
CNTK 4 00 1 10
Total 18(1) 21
1"FC Bug""LC Bug""BC Bug" respectively refer to new bugs found in Forward
Calculation, Loss Calculation and Backward Calculation stages.
2"NaN" refers to bugs related to NaN calculation.
3The number in parentheses means the bug exists in TensorFlow2.0.0 but has
been fixed in the latest version. Other bugs are all exists in the latest version.
In particular, for the 4 bugs detected in TensorFlow 2.0.0, we
manuallycheckwhetherthesebugscanbereproducedinthelatest
version (i.e., TensorFlow 2.6.0). The results show that among these
bugs,1bughasbeenfixedwhiletheother3bugsstillexist.After
reporting these bugs to the issue repository, 1 bug has been con-
firmed by developers. Among the 4 bugs detected in CNTK, 1 will
befixedinthefutureversion[ 3].Wealsoprovidebugcaseanalysis
according to different bug types.
FCBugs. The12bugsdetectedinFCstageinvolvedifferentlayer
types, including â€œAveragePooling2Dâ€, â€œConv1Dâ€ in Theano, and
â€œLSTMâ€, â€œDepthwiseConv2Dâ€, â€œBatchNormalizationâ€ in CNTK. By
taking the â€œAveragePooling2Dâ€ bug in Theano as an example. This
bugoccurswhensettingthelayerparameter ğ‘ğ‘ğ‘‘ğ‘‘ğ‘–ğ‘›ğ‘”toâ€œsameâ€œand
ğ‘ğ‘œğ‘œğ‘™_ğ‘ ğ‘–ğ‘§ğ‘’tothesameastheshapeoftheinputtensor.Byanalyzing
theresults,wefindthatinthiscase,Theanowouldchooseawrong
pooling location, resulting in large difference (i.e., more than 13
whileğ‘¡=0.15) between the results from other libraries.
LC Bugs. By taking the â€œBinaryCrossentropyâ€ bug in Tensor-
Flow as an example. When passing parameter values ğ‘œğ‘¢ğ‘¡ğ‘ğ‘¢ğ‘¡=[0.,
1., 0.] and ğ‘¡ğ‘ğ‘Ÿğ‘”ğ‘’ğ‘¡=[0.9999999, 0.9999999, 0.0000001] to the â€œBina-
ryCrossentropyâ€ loss function, theano and CNTK return a value
[15.942385,1.1920930e-07,1.1920930e-07]whileTensorFlowreturns
[15.333239,-0.,-0.],amongwhichthedifferenceofthefirstelementisnotnegligible.Byreviewingthesourcecode,wefindthatTensor-
Flow redundantly usesan ğ‘’ğ‘ğ‘ ğ‘–ğ‘™ğ‘œğ‘›parameter to clipinput values,
resulting in errors. This bug has been confirmed by the developers
of TensorFlow.
BC Bugs. Bytakingtheâ€œReLUâ€ buginTheanoasanexample.
When0existsintheinputtensorof ğ‘…ğ‘’ğ¿ğ‘ˆ,theanoback-propagates
a different gradients value, compared with TensorFlow and CNTK.
This bug is caused by the wrong equal sign position of Theano, i.e.,
ğ‘…ğ‘’ğ¿ğ‘ˆ(ğ‘§)=ğ‘§|ğ‘§â‰¥0inTheano,while ğ‘…ğ‘’ğ¿ğ‘ˆ(ğ‘§)=ğ‘§|ğ‘§>0inother
libraries. Although such implementation does not affect the results
in forward calculation, the implementation in Theano would let
the gradient propagate to previous layers in backward calculation
(which should not happen). This bug can only be detected in BC
stage, proving the effectiveness of Muffin.
NaN Bugs. BytakingaTensorFlowbugasanexample.Given
twoğ‘ğ‘ğ‘value, the â€œGlobalMaxPoolingâ€ layer returns âˆ’ğ¼ğ‘ğ¹, lead-
ing to the inconsistency. This bug has been fixed in the latest 2.6.0
version.
Regarding false positives, Muffinreports 19 unique inconsisten-
cies totally, where one false positive is found. The false positive
occursinthe"mean_absolute_percentage_error"lossfunction.This
function returns 100 Ã—ğ‘šğ‘’ğ‘ğ‘›, which amplifies the deviation and
cause the false alarm. In addition, Muffindetects 25 crash bugsTable 3: Comparison of distinct voted layers
Method Lib FC LC BC
MuffinTF 3 (2) 1 (1) 1 (1)
TH 15 (5) 1 (0) 1 (1)
CK 6 (2) 1 (0) 1 (1)
LEMONTF 2 (0) - -
TH 1 (0) - -
CK 1 (0) - -
Muffin-UTTF 4 (2) 2 (2) 2 (2)
TH 11 (1) 1 (0) 2 (2)
CK 4 (0) 1 (0) 0
1The number in parentheses denotes the number of voted layers that ONLY de-
tected by the corresponding method.
totally. Among them four are due to unsupported models Muf-
fingenerates, which can be treated as false positives. But, such
falsepositiveshaveclearerrormessages,thuscanbeautomatically
detected so as to avoid false alarms.
To further illustrate the effectiveness of Muffin, we count the
numberofdistinctvotedlayersdetectedbydifferentapproaches,
asshownintable 3.Wecanobservethatallthe4layersdetected
byLEMONcanbedetectedby MuffinandMuffin-UT,indicating
thatMuffinandMuffin-UT can cover the exploration scope of
LEMON.Ontheotherhand, MuffinandMuffin-UT havetheirown
distinct voted layers that cannotbe detected by the other, proving
theeffectivenessofinconsistencydetectionapproach.Theseresults
indicate that the natural architecture fuzzing approach adopted in
Muffinis a good supplement to unit testing.
5.2 Performance Comparison
Inordertofurtherevaluatetheperformanceof Muffin,wecompare
thenumberofinconsistencies,NaN,andcrashdetectedbydifferentmethods.WepresenttheresultsunderenvironmentE1asanexam-
ple2. Table4showsthe inconsistencies detected by three methods
underdifferentdatasetsandenvironments.Specifically,inthelatest
library versions (i.e., E1), Muffinfinds a total of 54 inconsistencies,
45 of which are found in the FC stage. In comparison, LEMON can
only find7 inconsistencies, muchless than Muffin. Similarresults
can also be observed in other environments, which prove the effec-
tivenessof Muffininlibrarytesting.Themainreasonisthat Muffin
can explore more library functions through the model generation
approach,whileLEMONcanonlymutateseedmodels,andthuscan
hardly cover the functions not being used in seed models. Besides,
LEMONalsocannotexplorethelibrarycodesrelatedtolossandgradient calculation. As a result, LEMON only achieves 35.593%
functionalitycoverage(thepercentageoftheinvokedAPIsinall
thepre-defined,learning-relatedAPIsweconsidered),while Muffin
can achieve 98.305% functionality coverage. The inconsistent APIs
thatcannot beidentifiedby LEMONincludeâ€œDepthwiseConv2Dâ€,
â€œLocallyConnected1Dâ€,â€œConv3Dâ€andvariouslossfunctions.Itis
worth noting that although Muffinis not designed to achieve high
linecoverage,wesummarizeandreportthelinecoverageresults:
Muffinachieves 43.22%, which is 2.07 times of that achieved by
LEMON (20.85%).
Onthe otherhand,comparedwith Muffin-UT, Muffindetects
19 more inconsistencies in E1, which proves the performance of
2More experiment codes and results are available at https://github.com/library-
testing/Muffin
1425
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:28:46 UTC from IEEE Xplore.  Restrictions apply. Muffin: Testing Deep Learning Libraries via Neural Architecture Fuzzing ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
Table 4: Comparison of inconsistency number
IDMethodLibPairCIFAR-10 MNIST F-MNIST ImageNet Sine-Wave Stock-Price Total
FCLC BC FCLC BC FCLC BC FCLC BC FCLC BC FCLC BC FCLC BC
E1MuffinTF-TH400 400 711 110 500 310 1621
TF-CK 310 421 621 310 1200 800 1622
TH-CK 410 400 711 100 600 200 1311
LEMONTF-TH1-- 0-- 1-- 1-- 0-- 0-- 2--
TF-CK 1-- 0-- 0-- 1-- 1-- 0-- 3--
TH-CK 0-- 0-- 1-- 0-- 1-- 0-- 2--
Muffin-U TTF-TH103 422 202 500 100 010 923
TF-CK 211 121 131 220 200 410 531
TH-CK 100 311 201 500 200 400 1011
1"FC""LC""BC" respectively represent the number of inconsistencies detected in the three stages.
2For inconsistencies caused by the same kind of layer (or loss function), we only count once.
Table 5: Comparison of NaN/Crash number
IDMethodLibCIFAR-10 MNIST F-MNIST ImageNet Sine-Wave Stock-Price Total
NaNGC EC NaNGC EC NaNGC EC NaNGC EC NaNGC EC NaNGC EC NaNGC EC
E1MuffinTF201 301 301 501 401 501 701
TH301 0 201 0 103 203 308 304 601 0
CK264 364 343 422 444 544 764
LEMON -0-0 0-0 0-0 0-0 0-0 0-0 0-0
Muffin-U TTF000 000 000 000 400 000 400
TH003 003 002 002 403 003 403
CK014 004 003 002 504 003 514
1"NaN"represents the number of outputs with NaN. For NaN caused by the same kind of layer, we only count once.
2"GC" and "EC" are respectively short for "Generation Crash" and "Execution Crash". The crash number have been deduplicated according to error messages.
3LEMON does not record NaN/crash information for each backend, so we only obtain the total NaN/crash number triggered by LEMON.
Muffin.Itisalsoworthnotingthatthenumberofinconsistencies
detected by Muffinreduces in other environments. The key reason
is that the numbers of NaN and crash triggered by Muffinincrease
in old library versions, as shown in Table 5. Taking NaN and crash
into consideration, Muffincan still trigger more exceptions (i.e.,
inconsistency,NaNandcrash)than Muffin-UT.Inparticular,the
layerfunctionswhere Muffincandetectexceptionswhile Muffin-
UTcannotincludeâ€œAveragePooling1Dâ€,â€œConv3DTransposeâ€and
â€œCategoricalCrossentropyâ€.
Furthermore,wealsocomparetheexecutiontimeofthethree
methodstogenerate50modelsandperformtestingunderdifferent
datasets. The execution time of MuffinandMuffin-UT consists
of the model generation time and the three-stage inconsistency
detection time (i.e., FC, LC and BC). The execution time of LEMON
consists of model mutation time and inconsistency detection time
(only FC). The results are shown in Table 6.
In this table, we can observe that except ImageNet, the execu-
tion time of Muffinis the longest in most cases. The reason is
thatMuffinconducts additional model generation (compared with
Muffin-UT),andinconsistencydetectioninadditionaltwostages
(compare with LEMON). Considering that Muffincan detect much
moreinconsistencies,wethinksuchoverhead(i.e.,aroundtenmin-
utes)isacceptable.Theseresultsalsodemonstratethattheproposed
approach(modelgenerationandinconsistencydetection)donot
bring huge overhead to Muffin.
Moreover, when performing library testing with ImageNet, the
executiontimeofLEMONisgreatlyincreased.ThereasonisthattheTable 6: Comparison of execution time (MIN.)
Dataset Method E1 E2 E3 E4 E5
CIFAR-10Muffin 29.20 20.67 38.75 27.38 16.75
LEMON 27.20 28.02 32.48 34.70 28.60
Muffin-UT 16.27 14.72 13.13 11.00 13.47
MNISTMuffin 32.35 19.82 18.20 14.95 15.78
LEMON 10.58 10.28 9.88 9.67 9.28
Muffin-UT 14.87 14.88 12.40 11.02 13.20
F-MNISTMuffin 24.78 25.00 18.23 15.92 20.38
LEMON 12.62 12.88 12.27 11.67 11.27
Muffin-UT 15.85 15.32 12.90 11.67 12.78
ImageNetMuffin 49.04 40.72 38.12 34.50 28.22
LEMON 80.25 117.62 114.25 117.25 111.93
Muffin-UT 34.03 23.52 30.15 37.05 27.55
Sine-WaveMuffin 25.95 24.37 18.52 15.40 17.45
LEMON 15.37 14.37 13.67 13.37 13.01
Muffin-UT 17.78 16.45 13.40 12.60 14.83
Stock-PriceMuffin 22.03 18.78 16.35 13.28 14.28
LEMON 16.58 15.50 14.35 14.37 13.82
Muffin-UT 16.07 14.22 12.45 10.85 13.25
seedmodelsusedbyLEMONaremuchmorecomplicated,compared
tothoseunderotherdatasets.Thisphenomenonrevealsthatthe
executiontimeofLEMONhighlydependsonthecomplexityofseedmodels.Ontheotherhand, Muffindoesnotsufferfromthisproblem.
Thegeneratedmodelcomplexityof Muffincanbecontrolledvia
setting proper values of ğ‘€ğ´ğ‘‹ ğ‘andğ‘€ğ´ğ‘‹ ğ‘£. Under the same ğ‘€ğ´ğ‘‹ ğ‘
andğ‘€ğ´ğ‘‹ ğ‘£value, the execution time of Muffinis quite stable.
1426
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:28:46 UTC from IEEE Xplore.  Restrictions apply. ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA Jiazhen Gu, Xuchuan Luo, Yangfan Zhou, and Xin Wang
0.01 0 .15 0 .30.41020304050
t#Inconsistencies
FC
LC
BC
Figure5:Performanceof Muffinunderdifferentthresholds
Compared with Muffin-UT, Muffinrequires additional DAG-
based model generation. In addition, the number of layers in the
model also affects the inconsistency detection time. The larger the
model,thelongerthedetectiontime.Asaresult,theexecutiontime
ofMuffinis slightly longer than that of Muffin-UT.
Finally, it is worth noting Muffindoes not consider the final
modelperformance(e.g.,precisionandrecall)inspecifictaskswhen
generating model architectures, since it is not the objective of a
testingtool.Incontrast,existingapproaches(e.g.,mutatingexist-
ing models) typically generate limited model architectures but can
obtain high-performance models, which however, are not more
capableindetectingbugs.Instead, Muffinfocusesmoreonmodel
quality in testing. Muffincan generate high-quality models. In 900
executions(3libraries,eachwith300models),only77executions
(8.5%) cause four unsupported-crashes (by the same reason). More-
over,wehavealsoshownsuchmodelsaremorecapableinexposing
bugs, as discussed in Section 5.1.
5.3 Effect of Different Parameter Settings
Muffinintroducesfourparameters, i.e.,ğ‘€ğ´ğ‘‹ ğ‘andğ‘€ğ´ğ‘‹ ğ‘£tocontrol
the size of model structure, and threshold ğ‘¡andğœ–for inconsistency
detection.Sinceinallexperiments, Muffinachievessatisfyinglayer
coverage (i.e., only one layer cannot be used with all datasets), we
considerthatthevaluesof ğ‘€ğ´ğ‘‹ ğ‘andğ‘€ğ´ğ‘‹ ğ‘£aresetproperly.For
the thresholds, ğœ–is a quite small value (i.e.,1 ğ‘’âˆ’5), thus we only
evaluate the number of inconsistencies detected by Muffinwith
differentğ‘¡values.
Figure5showsthenumberofinconsistenciesdetectedby Muffin
under different ğ‘¡values, ranging from 0 .001 to 0.4. We can observe
thatwhen ğ‘¡issmall,Muffinismoresensitivetosmallvarianceof
differences, thus it detects more inconsistencies. As the value of
ğ‘¡increases, the number of inconsistencies decreases slowly, and
keeps stable when ğ‘¡âˆˆ[0.15,0.4]. Thus, the default ğ‘¡value in
Muffinis 0.15. Although bugs incurring small variances may be
neglected,suchbugscanberevealedunderotherinputvaluesor
model architectures (i.e., variance larger than ğ‘¡).
6 DISCUSSION
6.1 Summary of Evaluation
Asdiscussedbefore, Muffin-UT isdesignedbasedontheideaofunit
testing,whichtestsaspecificlibraryfunctionatatime.Theevalua-
tionresultsshowthat Muffincandetectmorelayerinconsistencies
thanMuffin-UT.Themainreasonisthatmanylayerinconsistencies
canonlybetriggeredbyspecificinputs.Forinstance,thegradientsinconsistencyofâ€œMaxPooling1Dâ€layeronlyhappenswhenmul-
tipleelementsintheinputtensorhavethesamemaximumvalue.
In order to trigger such inconsistencies using Muffin-UT, we have
to fuzzing the inputs. Since layers in DL libraries typically have
hugeinputvalueranges, e.g.,high-dimensionaltensorinputswhere
eachelementrangesfrom (âˆ’âˆ,âˆ),itisquitechallengingtofind
specificinputsthatcantriggercornercases[ 54].Ontheotherhand,
Muffinperforms testing based on generated models. Due to the
existence of different layer types, we thus simulates the real calcu-
lationprocessandreducetheinputranges.Asaresult,thepossible
corner cases (i.e., multiple maximum values) can be triggered by
Muffin.Consideringthatunittestingisnecessarybeforeversion
release,whilebugscanstillbedetectedinthelatestversions,we
believeDLlibrarytestingbasedonmodelgenerationisaneffective
supplement to unit testing.
Among the bugs detected by Muffin, some of them are actually
causedbyunclearspecifications.Forinstance,thegradientcalcu-
lationbug ofâ€œcategorical_hingeâ€loss functionis actuallycaused
by the different specification of calculating the gradients of â€œmax()â€
function. Specifically, when there are multiple maximum elements,
TensorFlow will divide the gradient with the number of maximum
element,whileTheanoandCNTKdonothavethisoperation.Simi-larly,forâ€œMaxPooling1Dâ€layer,whentherearemultiplemaximumelements,TensorFlowandCNTKwouldonlyapplythegradientstooneofthemaximumelements,whileTheanoapplythegradientsto
all maximum elements. Due to unclear specifications, different DL
librarieshavedifferentimplementations.Althoughinmostcases
theresultsoftheseimplementationsareconsistent,robustnessis-
sues may be caused by the corner cases (e.g., easier to generateadversarial inputs [
24]). Therefore, we call for the community to
pay more attention on the unclear specification problems in DL
libraries.
6.2 Threats to Validity
We now discuss possible threats in this work, and the methods we
take to address such threats. First of all, we only evaluate the effec-
tivenessof MuffinonthreeDLlibraries, i.e.,TensorFlow,Theano
and CNTK. These libraries can be called using the same front-end
library (i.e., Keras), which facilitate the implementation and per-forming differential testing. Other libraries that do not support
Keras(i.e.,PyTorch)currentlyarenotsupportedby Muffin.Ho w-
ever, the ideas of model generation and inconsistency detectionadopted in Muffinare general. For instance, in order to test Py-
Torch, it requires to replace the Keras APIs used in Muffinwith
the corresponding PyTorch APIs. To reduce this treat, we evaluate
Muffinwithatotalof15differentreleaseversionsofDLlibraries.
In addition, we also use diverse models (including the models gen-
eratedby Muffin,existingmodelson6realdatasets,aswellastheir
mutants generated by existing work) to evaluate the inconsistency
detection performance of our approach.
Anotherthreatmainlyliesinrandomnessandthresholdsettings
in our experiment. To reduce the randomness, we conduct fiveexperiments with different library versions (i.e., E1-E5, refer to
Table 1, Table2 in Supplementary Material). In each experiment,
every method generates/mutates the same number of models for 6
commonly-useddatasets,andwerecordandcomparetheresults
1427
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:28:46 UTC from IEEE Xplore.  Restrictions apply. Muffin: Testing Deep Learning Libraries via Neural Architecture Fuzzing ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
andexecutiontime.Forthresholdsettings(e.g., ğ‘¡,ğ‘€ğ´ğ‘‹ ğ‘andğ‘€ğ´ğ‘‹ ğ‘£),
since in every experiment Muffinachieves 58/59 function usage,
we do not increase ğ‘€ğ´ğ‘‹ ğ‘andğ‘€ğ´ğ‘‹ ğ‘£. For threshold ğ‘¡, as discussed
in Section 4.4, we choose a quite large threshold. Slightly changing
ğ‘¡(e.g., from 0.15 to 0.4) has little impact on the results.
6.3 Future Directions
Muffincanbepotentiallyimprovedinthefollowingtwoaspects.
First,Muffinonly covers library codes in the layer function granu-
laritythroughtryingtogeneratemodelcoveringalltheprovided
APIs.However,there maystillbealargeportionoflibrarycodes
that cannot be covered (e.g., private methods, branches). In the
future,Muffincan be extended to consider other coverage metrics
(e.g.,linecoverage,branchcoverage),andconductsmodelgenera-
tion/mutation to explore more library codes.
Second,Muffinstill relies on differential testing to solve the test
oracleproblem.However,ifdifferentDLlibrariesproducethesame
wrong results, Muffincannot identify such bugs. Moreover, in the
evaluation, we also notice that under certain circumstances, the
modelgenerated by Muffinmaycause onelibrary tocrash, while
theothertwoproduceinconsistentresults.Insuchcases,itrequires
huge human efforts to identify potential bugs. To get rid of this
limitation,weintendtodesignmetamorphicrelationsbasedonthe
properties of DL models, and conducts metamorphic testing to test
one library accordingly.
7 RELATED WORK
Asdiscussedbefore,CRADLE[ 44]andLEMON[ 50]arethemost
related work to ours that targets DL library testing, both of which
require existing DL models and only detect bugs in model infer-
ence phase. Different from them, Muffindetects DL library bugs in
modeltrainingphaseviaDAG-basedmodelgeneration.Intheliter-
ature,thereisabodyofworkfocusingontestingmachinelearning
(ML) libraries as well [ 17â€“19,56,59]. For instance, Dutta et al.[17]
propose ProbFuzz to test probabilistic programming systems via
generatingprogramsbasedonpre-definedtemplates.Dwarakanath
et al.[19] adopt metamorphic testing to test image classification
applications through mutating the training and testing data. How-
ever, these approaches cannot be directly adopted for DL librariestesting.
Ontheotherhand,thereisagreatdealofresearchesfocusing
on the testing of DL models [ 35â€“38,42,43,48,54,55]. In particu-
lar, many research efforts have been put on designing criteria to
measure test adequacy [ 16,35,37]. For instance, Pei et al.[43] first
propose neuron coverage as the criteria for testing DL models. Ma
et al.[37]furtherdefinebothneuronandlayerlevelcoveragecrite-
riatohelpgaugingthetestingqualityofDLmodels.Kim et al.[35]
propose surprise coverage based on surprise adequacy, which mea-
sures relative surprise of each input with respect to the training
data. Du et al.[16] propose a set of similarity metrics and coverage
criteria to analyze stateful DL systems such as Recurrent Neural
Networks(RNNs)[ 39].Moreover,therearealotofstudiesintend
to reveal defects in DL models via generating adversarial inputsor finding corner cases [
48,55,60]. For instance, Tian et al.[48]
implementDeepTestfordetectingerroneousbehaviorsofDL-based
self-drivingcarsviaautomaticallygeneratingtestcasesbasedonimagetransformations.Similarly,Zhang et al.[60]implementDeep-
Road, which applies Generative Adversarial Networks (GANs) [ 24]
to test DL-basedself-driving cars. Besides, there are alsomany re-
searches focus on detecting different kinds of bugs in model struc-
turesortrainingparametersettings[ 51,62].Forinstance,Zhang
et al.[62] propose DEBAR, a static analysis approach for detecting
numericalbugsinDLmodels.Wardat et al.[51]proposeadynamic
analysisbasedapproachtodetectnumericalerrorswhentraining
DL models. Similarly, Zhang et al.[61] propose AUTOTRAINER, a
toolthatdetectsandauto-repairscommonly-seenmodeltraining
problemssuchasvanishinggradient,explodinggradientsandslow
convergence. Different from them, our work focuses on testing DL
libraries rather than DL models or parameters.
Ourworkisalsorelatedtodifferentialtesting,aneffectivemethod
that use similar programs as cross referencing oracles to detectbugs [
26]. Differential testing has been successful in uncovering
bugs across various types of programs, such as compilers [ 58],
JavaVirtualMachine(JVM)implementations[ 13,14],webappli-
cations [10], and security-related APIs [ 46]. In recent years, re-
searchers also utilize differential testing in the area of DL test-ing [
43,48]. For instance, Pei et al.[43] propose DeepXplore, a
differential testing framework to identify DL model defects via im-
agetransformation.Guo et al.[27]proposeDLFuzz,adifferential
fuzzingtestingframeworkthatexposesDLmodelerrorsthrough
mutating inputs to maximize model output difference. These ap-
proaches focus on testing DL models, while Muffinis designed for
DL library testing with high coverage.
8 CONCLUSION
In this paper, we propose a novel approach to test DL library codes
viadirectmodelgenerationusinglibraryAPIs.Inordertogenerate
diverseDLmodels,weuseDAGtoformulatethemodelstructure
and propose a DAG-based model generation algorithm. In order to
detectbugs,wedividethemodeltrainingphaseintothreestages,
and design different measurements for each stage. In this way, our
approach can detect library bugs related to model training, which
is not covered by previous studies. We implement our approach
as an open-source tool called Muffin. To evaluate the performance
ofMuffin, we conduct a series of experiments based on 15 release
versionsofthreewidely-usedDLlibraries. Muffindetects39new
bugs in the latest versions of these libraries. Besides, Muffinout-
performsothermethodsintermsofthenumberofdetectedunique
inconsistencies.
ACKNOWLEDGMENTS
This work was supported by the National Key R&D Program of
China under Grant 2020YFA0711400 and the Natural Science Foun-
dation of Shanghai (No. 22ZR1407900).
REFERENCES
[1] Accessed: 2021. Cloud TPU. https://cloud.google.com/tpu.
[2] Accessed: 2021. CNTK. https://docs.microsoft.com/cognitive-toolkit.
[3]Accessed: 2021. CNTK ops Pachakge: sqrt. https://docs.microsoft.com/en-us/
python/api/cntk/cntk.ops?view=cntk-py-2.7#sqrt-x--name---- .
[4] Accessed: 2021. Keras. https://keras.io.
[5] Accessed: 2021. TensorFlow. https://www.tensorflow.org.
[6]Accessed: 2021. Tesla driver dies in first fatal crash while using autopilot
mode.https://www.theguardian.com/technology/2016/jun/30/tesla-autopilot-
1428
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:28:46 UTC from IEEE Xplore.  Restrictions apply. ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA Jiazhen Gu, Xuchuan Luo, Yangfan Zhou, and Xin Wang
death-self-driving-car-elon-musk.
[7]Accessed: 2021. Uberâ€™s self-driving operator charged over fatal crash. https:
//www.bbc.com/news/technology-54175359.
[8]MartÃ­nAbadi,PaulBarham,JianminChen,ZhifengChen,AndyDavis,Jeffrey
Dean,MatthieuDevin,SanjayGhemawat,GeoffreyIrving,MichaelIsard,Manju-
nathKudlur,JoshLevenberg,RajatMonga,SherryMoore,DerekGordonMurray,
Benoit Steiner, Paul A. Tucker, Vijay Vasudevan, Pete Warden, Martin Wicke,
Yuan Yu, and Xiaoqiang Zheng. 2016. TensorFlow: A System for Large-Scale
Machine Learning. In Proc. of the 12th USENIX Symposium on Operating Systems
Design and Implementation, OSDI. USENIX Association, 265â€“283.
[9]CyrusDCantrell.2000. Modern mathematical methods for physicists and engineers .
Cambridge University Press.
[10]Peter Chapman and David Evans. 2011. Automated black-box detection of side-
channelvulnerabilities inwebapplications.In Proc. of the 18th ACM Conference
on Computer and Communications Security, CCS. ACM, 263â€“274. https://doi.org/
10.1145/2046707.2046737
[11]Junjie Chen, Xiaoting He, Qingwei Lin, Hongyu Zhang, Dan Hao, Feng Gao,
Zhangwei Xu, Yingnong Dang, and Dongmei Zhang. 2019. Continuous Incident
Triage for Large-Scale Online Service Systems. In Proc. of the 34th IEEE/ACM
International Conference on Automated Software Engineering, ASE. IEEE, 364â€“375.
https://doi.org/10.1109/ASE.2019.00042
[12]Junjie Chen, Shu Zhang, Xiaoting He, Qingwei Lin, Hongyu Zhang, Dan Hao,
YuKang,FengGao,ZhangweiXu,YingnongDang,andDongmeiZhang.2020.
How Incidentalare theIncidents? Characterizingand PrioritizingIncidents for
Large-Scale Online Service Systems. In Proc. of the 35th IEEE/ACM International
Conference on Automated Software Engineering, ASE. IEEE, 373â€“384. https:
//doi.org/10.1145/3324884.3416624
[13]Yuting Chen, Ting Su, and Zhendong Su. 2019. Deep differential testing of
JVM implementations. In Proc. of the 41st International Conference on Software
Engineering, ICSE.IEEE/ACM,1257â€“1268. https://doi.org/10.1109/ICSE.2019.
00127
[14]Yuting Chen, Ting Su, Chengnian Sun, Zhendong Su, and Jianjun Zhao. 2016.
Coverage-directeddifferentialtestingofJVMimplementations.In Proc. of the 37th
ACM SIGPLAN Conference on Programming Language Design and Implementation,
PLDI. ACM, 85â€“99. https://doi.org/10.1145/2908080.2908095
[15]Keunwoo Choi, GyÃ¶rgy Fazekas, Mark B. Sandler, and Kyunghyun Cho. 2017.
Convolutionalrecurrentneuralnetworksformusicclassification.In 2017 IEEE
International Conference on A coustics, Speec h and Signal Processing, ICASSP. IEEE,
2392â€“2396. https://doi.org/10.1109/ICASSP.2017.7952585
[16]Xiaoning Du, Xiaofei Xie, Yi Li, Lei Ma, Yang Liu, and Jianjun Zhao. 2019. Deep-
Stellar:model-basedquantitativeanalysisofstatefuldeeplearningsystems.In
Proc. of the ACM Joint Meeting on European Software Engineering Conference and
Symposium on the Foundations of Software Engineering, ESEC/FSE. ACM, 477â€“487.
https://doi.org/10.1145/3338906.3338954
[17]SaikatDutta,OwolabiLegunsen,ZixinHuang,andSasaMisailovic.2018. Testing
probabilistic programming systems. In Proc. of the 2018 ACM Joint Meeting on
European Software Engineering Conference and Symposium on the Foundations of
Software Engineering, ESEC/FSE. ACM, 574â€“586. https://doi.org/10.1145/3236024.
3236057
[18]SaikatDutta,WenxianZhang,ZixinHuang,andSasaMisailovic.2019. Storm:
programreductionfortestinganddebuggingprobabilisticprogrammingsystems.
InProc. of the ACM Joint Meeting on European Software Engineering Conference
and Symposium on the Foundations of Software Engineering, ESEC/FSE. ACM,
729â€“739. https://doi.org/10.1145/3338906.3338972
[19]Anurag Dwarakanath, Manish Ahuja, Samarth Sikand, Raghotham M. Rao, R. P.
JagadeeshChandraBose,NevilleDubash,andSanjayPodder.2018. Identifying
implementation bugs in machine learning based image classifiers using meta-
morphictesting.In Proc. of the 27th ACM SIGSOFT International Symposium on
Software Testing and Analysis, ISSTA. ACM, 118â€“128. https://doi.org/10.1145/
3213846.3213858
[20]ThomasElsken,JanHendrikMetzen,andFrankHutter.2019. NeuralArchitecture
Search: A Survey. J. Mach. Learn. Res. 20 (2019), 55:1â€“55:21.
[21]David B. Fogel. 1997. Evolutionary algorithms in theory and practice. Complex.
2, 4 (1997), 26â€“27. https://doi.org/10.1002/(SICI)1099-0526(199703/04)2:4<26::
AID-CPLX6>3.0.CO;2-7
[22]David Goldberg. 1991. What Every Computer Scientist Should Know AboutFloating-Point Arithmetic. ACM Comput. Surv. 23, 1 (1991), 5â€“48. https://doi.
org/10.1145/103162.103163
[23]IanJ.Goodfellow,YoshuaBengio,andAaronC.Courville.2016. Deep Learning.
MIT Press. http://www.deeplearningbook.org/
[24]Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-
Farley,SherjilOzair,AaronC.Courville,andYoshuaBengio.2014. Generative
AdversarialNets.In Advances in Neural Information Processing Systems 27: Annual
Conference on Neural Information Processing Systems. 2672â€“2680.
[25]JiazhenGu,JiaqiWen,ZijianWang,PuZhao,ChuanLuo,YuKang,YangfanZhou,LiYang,JeffreySun,ZhangweiXu,BoQiao,LiqunLi,QingweiLin,andDongmeiZhang.2020. Efficientcustomerincidenttriagevialinkingwithsystemincidents.InProc. of the 28th ACM Joint European Software Engineering Conference and
Symposium on the Foundations of Software Engineering, ESEC/FSE.ACM,1296â€“
1307.https://doi.org/10.1145/3368089.3417061
[26]Muhammad Ali Gulzar, Yongkang Zhu, and Xiaofeng Han. 2019. Perception and
practices of differential testing. In Proc. of the 41st International Conference on
Software Engineering: Software Engineering in Practice, ICSE (SEIP).IEEE /ACM,
71â€“80.https://doi.org/10.1109/ICSE-SEIP.2019.00016
[27]JianminGuo,YuJiang,YueZhao,QuanChen,andJiaguangSun.2018. DLFuzz:
differential fuzzing testing of deep learning systems. In Proc. of the 2018 ACM
Joint Meeting on European Software Engineering Conference and Symposium on
the Foundations of Software Engineering, ESEC/FSE.ACM,739â€“743. https://doi.
org/10.1145/3236024.3264835
[28]Abhishek Gupta, Alagan Anpalagan, Ling Guan, and Ahmed Shaharyar Khwaja.
2021. Deep learning for object detection and scene perception in self-drivingcars: Survey, challenges, and open issues. Array10 (2021), 100057. https:
//doi.org/10.1016/j.array.2021.100057
[29]Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep Residual
Learning forImage Recognition. In 2016 IEEE Conference on Computer Vision and
Pattern Recognition, CVPR.IEEEComputerSociety,770â€“778. https://doi.org/10.
1109/CVPR.2016.90
[30]Robert Hecht-Nielsen. 1988. Theory of the backpropagation neural network.
Neural Networks 1, Supplement-1 (1988), 445â€“448. https://doi.org/10.1016/0893-
6080(88)90469-8
[31]Gao Huang, Zhuang Liu, Laurens van der Maaten, and Kilian Q. Weinberger.
2017. DenselyConnectedConvolutionalNetworks.In 2017 IEEE Conference on
Computer Vision and Pattern Recognition, CVPR.IEEEComputerSociety,2261â€“
2269.https://doi.org/10.1109/CVPR.2017.243
[32]JaneHung,AllenGoodman,DeepaliRavel,StefanieLopes,GabrielRangel,Odail-
ton A. Nery, Benoit Malleret, Francois Nosten, Marcus V. G. Lacerda, Marcelo U.
Ferreira, Laurent RÃ©nia, Manoj Duraisingh, Fabio T. M. Costa, Matthias Marti,and Anne E. Carpenter. 2020. Keras R-CNN: library for cell detection in bio-logical images using deep neural networks. BMC Bioinform. 21, 1 (2020), 300.
https://doi.org/10.1186/s12859-020-03635-x
[33]Md Johirul Islam, Giang Nguyen, Rangeet Pan, and Hridesh Rajan. 2019. A
comprehensivestudyondeeplearningbugcharacteristics.In Proc. of the ACM
Joint Meeting on European Software Engineering Conference and Symposium on
the Foundations of Software Engineering, ESEC/FSE.ACM,510â€“520. https://doi.
org/10.1145/3338906.3338955
[34]VetonKepuskaandGamalBohouta.2018. Next-generationofvirtualpersonal
assistants (Microsoft Cortana, Apple Siri, Amazon Alexa and Google Home).InIEEE 8th Annual Computing and Communication Workshop and Conference,
CCWC. IEEE, 99â€“103. https://doi.org/10.1109/CCWC.2018.8301638
[35]Jinhan Kim, Robert Feldt, and Shin Yoo. 2019. Guiding deep learning system
testingusingsurpriseadequacy.In Proc. of the 41st International Conference on
Software Engineering,ICSE.IEEE/ACM,1039â€“1049. https://doi.org/10.1109/ICSE.
2019.00108
[36]LeiMa,FelixJuefei-Xu,FuyuanZhang,JiyuanSun,MinhuiXue,BoLi,ChunyangChen,TingSu,LiLi,YangLiu,JianjunZhao,andYadongWang.2018. DeepGauge:
multi-granularitytestingcriteriafordeeplearningsystems.In Proc. of the 33rd
ACM/IEEE International Conference on Automated Software Engineering, ASE.
ACM, 120â€“131. https://doi.org/10.1145/3238147.3238202
[37]Lei Ma, Fuyuan Zhang, Jiyuan Sun, Minhui Xue, Bo Li, Felix Juefei-Xu, Chao
Xie, Li Li, Yang Liu, Jianjun Zhao, and Yadong Wang. 2018. DeepMutation:
MutationTestingofDeepLearningSystems.In Proc. of the 29th IEEE International
Symposium on Software Reliability Engineering, ISSRE.IEEEComputerSociety,
100â€“111. https://doi.org/10.1109/ISSRE.2018.00021
[38]ShiqingMa,YingqiLiu,Wen-ChuanLee,XiangyuZhang,andAnanthGrama.
2018. MODE: automatedneuralnetworkmodeldebuggingvia statedifferential
analysis and input selection. In Proc. of the 2018 ACM Joint Meeting on European
Software Engineering Conference and Symposium on the Foundations of Software
Engineering, ESEC/FSE. ACM, 175â€“186. https://doi.org/10.1145/3236024.3236082
[39]TomÃ¡s Mikolov, Martin KarafiÃ¡t, LukÃ¡s Burget, Jan CernockÃ½, and Sanjeev Khu-
danpur. 2010. Recurrent neural network based language model. In Proc. of the
11th Annual Conference of the International Speech Communication Association,
INTERSPEECH. ISCA, 1045â€“1048.
[40]AmitaMuralikrishna,LuÃ­sEduardoAntunesVieira,RafaelDuarteCoelhodos
Santos,andAdrianoP.Almeida.2020. TotalSolarIrradianceForecastingwith
KerasRecurrentNeuralNetworks.In 20th International Conference on Compu-
tational Science and Its Applications - ICCSA (Lecture Notes in Computer Science,
Vol. 12253). Springer, 255â€“269. https://doi.org/10.1007/978-3-030-58814-4_18
[41]Vinod Nair and Geoffrey E. Hinton. 2010. Rectified Linear Units Improve Re-stricted Boltzmann Machines. In Proc. of the 27th International Conference on
Machine Learning, ICML. Omnipress, 807â€“814.
[42]AugustusOdena,CatherineOlsson,DavidG.Andersen,andIanJ.Goodfellow.
2019.TensorFuzz:DebuggingNeuralNetworkswithCoverage-GuidedFuzzing.InProc. of the 36th International Conference on Machine Learning, ICML (Proceedings
of Machine Learning Research, Vol. 97). PMLR, 4901â€“4911.
1429
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:28:46 UTC from IEEE Xplore.  Restrictions apply. Muffin: Testing Deep Learning Libraries via Neural Architecture Fuzzing ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
[43] KexinPei,YinzhiCao,JunfengYang,andSumanJana.2017. DeepXplore:Auto-
matedWhitebox Testing ofDeep LearningSystems.In Proc. of the 26th Sympo-
sium on Operating Systems Principles, SOSP. ACM, 1â€“18. https://doi.org/10.1145/
3132747.3132785
[44]HungVietPham,ThibaudLutellier,WeizhenQi,andLinTan.2019. CRADLE:
cross-backendvalidationtodetectandlocalizebugsindeeplearninglibraries.
InProc. of the 41st International Conference on Software Engineering,ICSE. IEEE /
ACM, 1027â€“1038. https://doi.org/10.1109/ICSE.2019.00107
[45]KarenSimonyanandAndrewZisserman.2015. VeryDeepConvolutionalNet-
works for Large-Scale Image Recognition. In Proc. of the 3rd International Confer-
ence on Learning Representations, ICLR, Yoshua Bengio and Yann LeCun (Eds.).
[46]Varun Srivastava, Michael D. Bond, Kathryn S. McKinley, and Vitaly Shmatikov.
2011. A security policy oracle: detecting security holes using multiple API
implementations. In Proc. of the 32nd ACM SIGPLAN Conference on Programming
Language Design and Implementation, PLDI.ACM,343â€“354. https://doi.org/10.
1145/1993316.1993539
[47]The Theano Development Team, Rami Al-Rfou, Guillaume Alain, Amjad Alma-
hairi,ChristofAngermueller,DzmitryBahdanau,NicolasBallas,FrÃ©dÃ©ricBastien,
Justin Bayer, Anatoly Belikov, et al .2016. Theano: A Python framework for
fastcomputationofmathematicalexpressions. arXiv preprint arXiv:1605.02688
(2016).
[48]YuchiTian,KexinPei,SumanJana,andBaishakhiRay.2018. DeepTest:automated
testing of deep-neural-network-driven autonomous cars. In Proc. of the 40th
International Conference on Software Engineering,ICSE. ACM, 303â€“314. https:
//doi.org/10.1145/3180155.3180220
[49]Qiang Wang, Bei Li, Tong Xiao, Jingbo Zhu, Changliang Li, Derek F. Wong, and
LidiaS.Chao.2019. LearningDeepTransformerModelsforMachineTranslation.
InProc. of the 57th Conference of the Association for Computational Linguistics, ACL,
Volume 1: Long Papers. Association for Computational Linguistics, 1810â€“1822.
https://doi.org/10.18653/v1/P19-1176
[50]ZanWang, MingYan,Junjie Chen,ShuangLiu,and DongdiZhang.2020. Deep
learning library testing via effective model generation. In Proc. of the 28th ACM
Joint European Software Engineering Conference and Symposium on the Founda-
tions of Software Engineering, ESEC/FSE. ACM, 788â€“799. https://doi.org/10.1145/
3368089.3409761
[51]Mohammad Wardat, Wei Le, and Hridesh Rajan. 2021. DeepLocalize: Fault
LocalizationforDeepNeuralNetworks.In Proc.of the 43rd IEEE/ACM International
Conference on Software Engineering, ICSE.IEEE,251â€“262. https://doi.org/10.1109/
ICSE43902.2021.00034
[52]Martin Wistuba, Ambrish Rawat, and Tejaswini Pedapati. 2019. A Survey onNeural Architecture Search. CoRRabs/1905.01392 (2019). arXiv:1905.01392
http://arxiv.org/abs/1905.01392[53]Chao-Yuan Wu, Christoph Feichtenhofer, Haoqi Fan, Kaiming He, PhilippKrÃ¤henbÃ¼hl, and Ross B. Girshick. 2019. Long-Term Feature Banks for De-tailed Video Understanding. In IEEE Conference on Computer Vision and Pat-
tern Recognition, CVPR . Computer Vision Foundation / IEEE, 284â€“293. https:
//doi.org/10.1109/CVPR.2019.00037
[54]WeibinWu,HuiXu,SanqiangZhong,MichaelR.Lyu,andIrwinKing.2019. Deep
Validation:TowardDetectingReal-WorldCornerCasesforDeepNeuralNetworks.
InProc. of the 49th Annual IEEE/IFIP International Conference on Dependable
Systems and Networks, DSN.IEEE,125â€“137. https://doi.org/10.1109/DSN.2019.
00026
[55]XiaofeiXie,LeiMa,FelixJuefei-Xu,MinhuiXue,HongxuChen,YangLiu,JianjunZhao,BoLi,JianxiongYin,andSimonSee.2019. DeepHunter:acoverage-guided
fuzz testing framework for deep neural networks. In Proc. of the 28th ACM
SIGSOFT International Symposium on Software Testing and Analysis, ISSTA. ACM,
146â€“157. https://doi.org/10.1145/3293882.3330579
[56]Xiaoyuan Xie, Zhiyi Zhang, Tsong Yueh Chen, Yang Liu, Pak-Lok Poon, andBaowen Xu. 2020. METTLE: A METamorphic Testing Approach to Assessing
and Validating Unsupervised Machine Learning Systems. IEEE Trans. Reliab. 69,
4 (2020), 1293â€“1322. https://doi.org/10.1109/TR.2020.2972266
[57]BingXu,NaiyanWang,TianqiChen,andMuLi.2015. EmpiricalEvaluationof
Rectified Activations in Convolutional Network. CoRRabs/1505.00853 (2015).
arXiv:1505.00853
[58]Xuejun Yang, Yang Chen, Eric Eide, and John Regehr. 2011. Finding and un-
derstandingbugsinCcompilers. In Proc. of the 32nd ACM SIGPLAN Conference
on Programming Language Design and Implementation, PLDI. ACM, 283â€“294.
https://doi.org/10.1145/1993316.1993532
[59]JieMZhang,MarkHarman,LeiMa,andYangLiu.2020. Machinelearningtesting:
Survey, landscapes and horizons. IEEE Transactions on Software Engineering
(2020).
[60]Mengshi Zhang, Yuqun Zhang, Lingming Zhang, Cong Liu, and Sarfraz Khur-shid. 2018. DeepRoad: GAN-based metamorphic testing and input validationframeworkforautonomousdrivingsystems.In Proc. of the 33rd ACM/IEEE In-
ternational Conference on Automated Software Engineering, ASE.ACM,132â€“142.
https://doi.org/10.1145/3238147.3238187
[61]XiaoyuZhang,JuanZhai,ShiqingMa,andChaoShen.2021. AUTOTRAINER:
AnAutomaticDNNTrainingProblemDetectionandRepairSystem.In Proc. of
the 43rd IEEE/ACM International Conference on Software Engineering,ICSE. IEEE,359â€“371. https://doi.org/10.1109/ICSE43902.2021.00043
[62]
YuhaoZhang,LuyaoRen,LiqianChen,YingfeiXiong,Shing-ChiCheung,and
TaoXie.2020. Detectingnumericalbugsinneuralnetworkarchitectures.In Proc.
of the 28th ACM Joint European Software Engineering Conference and Symposium
on the Foundations of Software Engineering, ESEC/FSE. ACM, 826â€“837. https:
//doi.org/10.1145/3368089.3409720
1430
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:28:46 UTC from IEEE Xplore.  Restrictions apply. 