An Empirical Analysis of UI-based Flaky Tests
Alan Romano1, Zihe Song2, Sampath Grandhi2, Wei Yang2, and Weihang Wang1
1University at Buffalo, SUNY2University of Texas at Dallas
Abstract —Flaky tests have gained attention from the research
community in recent years and with good reason. These tests lead
to wasted time and resources, and they reduce the reliability ofthe test suites and build systems they affect. However, most of theexisting work on ﬂaky tests focus exclusively on traditional unittests. This work ignores UI tests that have larger input spacesand more diverse running conditions than traditional unit tests.In addition, UI tests tend to be more complex and resource-heavy, making them unsuited for detection techniques involvingrerunning test suites multiple times.
In this paper, we perform a study on ﬂaky UI tests. We analyze
235 ﬂaky UI test samples found in 62 projects from both web andAndroid environments. We identify the common underlying rootcauses of ﬂakiness in the UI tests, the strategies used to manifestthe ﬂaky behavior, and the ﬁxing strategies used to remedy ﬂakyUI tests. The ﬁndings made in this work can provide a foundationfor the development of detection and prevention techniques forﬂakiness arising in UI tests.
I. I NTRODUCTION
Software testing is a signiﬁcant part of software devel-
opment. Most developers write test suites to repeatedly test
various indicators of functioning software. If a test fails,developers will analyze the corresponding test to debug and ﬁxthe software. However, not all testing results are fully reliable.Sometimes the test may show ﬂakiness, and a test showing thisbehavior is denoted as a ﬂaky test.
Flaky tests refer to tests with unstable test results. That is,
the same test suite sometimes passes and sometimes fails underthe exact same software code and testing code. The existenceof ﬂaky tests destroys the deterministic relationship betweentest results and code quality. Once a ﬂaky test appears, it maylead to tons of efforts wasted in debugging the failed test,which leads to delays in software release cycle and reduceddeveloper productivity [1].
In the past few years, researchers have increased efforts
to address this problem [2, 3, 4]. However, the existingresearch on ﬂaky tests mostly focuses on unit tests. Comparedwith traditional unit testing, the execution environment andautomation process of UI testing are signiﬁcantly different:First, many of the events in these tests, such as handling userinput, making operating system (or browser) API calls, anddownloading and rendering multiple resources (such as imagesand scripts) required by the interface, are highly asynchronousin nature. This means that user events and various other taskswill be triggered in a non-deterministic order.
Second, compared to traditional unit testing, ﬂaky UI tests
are more difﬁcult to detect and reproduce. This is because itis difﬁcult to cover all use-case scenarios by simulating userevents to automate UI testing. UI tests introduce new sourcesof ﬂakiness, either from the layer between the user and theUI or the layer between the UI and the test/application code.Moreover, the execution speed of the UI test in continuousintegration environments is slow, and this difference in execu-tion speed makes detecting and reproducing ﬂaky tests moredifﬁcult. Therefore, researching ﬂaky UI tests can help weband mobile UI developers by providing insights on effectivedetection and prevention methods.
To further investigate ﬂaky UI tests, we collect and analyze
235 real-world ﬂaky UI test examples found in popular weband Android mobile projects. For each ﬂaky test example, weinspect commit descriptions, issue reports, reported causes,and changed code. We focus on the following questions andsummarize our ﬁndings and implications in Table I.
RQ1: What are the typical root causes behind ﬂaky
UI tests? We examine the collected ﬂaky UI test samples to
determine the underlying causes of ﬂaky behavior. We groupsimilar root causes together into 4 main categories: Async Wait,
Environment, Test Runner API Issues, and Test Script Logic
Issues.
RQ2: What conditions do ﬂaky UI tests manifest in
and how are they reproduced? In order to understand how
users report intermittent behaviors, we investigate the commonstrategies used to manifest the ﬂaky UI test samples. The datareveals 5 strategies used to reproduce and report ﬂaky UItest behavior: Specify Problematic Platform, Reorder/Prune
Test Suite, Reset Conﬁguration Between Tests, Provide Code
Snippet, and Force Environment Conditions.
RQ3: How are these ﬂaky UI tests typically ﬁxed? We
identify the bug ﬁx applied to each collected ﬂaky UI testsample and group similar ones together. We ﬁnd 4 main cate-gories for bug ﬁxing strategies: Delay, Dependency, Refactor
Test, and Disable Features.
We investigate the impacts that these UI-speciﬁc features
have on ﬂakiness, and we ﬁnd several distinctions. Basedon the investigation of above research questions, the maincontributions of this study are:
1) Our study provides guidance for developers to create
reliable and stable UI test suites, which can reduce theoccurrence of ﬂaky UI tests.
2) Our study summarizes the commonly-used manifestation
and ﬁx strategies of ﬂaky UI tests to help developerseasily reproduce and ﬁx ﬂaky tests, allowing them toavoid wasted development resources.
3) Our study motivates future work for automated detection
and ﬁxing techniques of UI-based ﬂaky tests.
15852021 IEEE/ACM 43rd International Conference on Software Engineering (ICSE)
1558-1225/21/$31.00 ©2021 IEEE
DOI 10.1109/ICSE43902.2021.00141
TABLE I: Summary of Findings and Implications
Findings Implications
1Of the observed ﬂaky tests collected, 105 tests of the 235 (45.1%)
dataset are caused by an Async Wait issue.This group represents a signiﬁcant portion of the dataset collected and highlights
the need to take this root cause into consideration when designing UI tests.
2Async Wait issues are more prevalent in web projects rather than mobile
projects (W 52.0 % vs M 32.5%).The web presents an environment with less stable timing and scheduling
compared with a mobile environment, so more care must be taken when
network or resource loads are used within web UI tests.
3Platform issues are happening more frequent on mobile projects rather
than web projects (W 10.5 % vs M 21.7%).It may be caused by Android fragmentation problem. So the Android
developers should pay more attention to the environment conﬁguration
when choosing the test model.
4Layout difference (cross-platform) root causes are found more in web
ﬂaky test than in mobile ﬂaky tests (W 5.3 % vs M 1.2%).This difference can be explained by the number of additional platform conditions that
web applications can be exposed compared with the conditions found in
mobile environment, such as different window sizes, different browser
rendering strategies, etc...
5Besides removing ﬂaky test, the most common ﬁxing strategies are
refactoring logic implementations (46.0%) and ﬁxing delays (39.3%).
Among them, refactoring logic implementations can solve most issues
caused by wrong test script logic, and ﬁxing delay strategy can solve
most timing issues.Refactoring logic implementations and ﬁxing delays should be the ﬁrst-
considered strategies for developers when ﬁxing bugs.
6Dependency ﬁxes are more common in mobile projects than web
projects (W 1.3% vs M 21.4%).This trend can be caused by the Android fragmentation problem. Android developers
should pay more attention to this problem when designing test suites.
7Delay ﬁxes are more common in web projects than mobile projects
(W 32.2% vs M 17.9%).This phenomenon is related to the most common test framework in Android
testing, Espresso, which recommends disabling animations during tests.
TABLE II: Summary of Commit Info from UI Frameworks
UI Topic Projects CommitsFlaky Keyword
FilteringUI Keyword
Filtering
web 999 772,901 2,553 210
angular 998 407,434 222 19
vue 998 344,526 52 1
react 997 1,110,993 603 30
svg 995 135,563 24 1
bootstrap 995 98,264 112 0
d3 980 106,160 82 1
emberjs 629 3,961 1 0
Total 7,590 2,979,802 3,649 262
Distinct 7,037 2,613,420 3,516 254
II. B ACKGROUND
A. Impacts of Flaky UI Tests
1) Individual Test Failures: The simplest impact that ﬂaky
test can have on test suites is that the individual test run will
fail. This ﬂaky behavior leads to a minimal amount of time
and resources wasted by attempting to retry the single test.
2) Build Failures: Flaky tests that are part of continuous
integration systems can lead to intermittent build failures.
Flaky tests in this stage lead to wasted time trying to identify
the underlying cause of the build failure only to ﬁnd out that
the failure was not caused by a regression in the code.
3) CI Test Timeouts: Some ﬂaky behaviors do not cause
the tests to fail outright. Instead, they cause hangups in the CI
system that lead to timeouts. These hangups waste time as the
system waits for a process that never ﬁnishes, causing the CI
system to wait until a speciﬁed timeout is met.
III. M ETHODOLOGY
A. Sample Collection
1) Web: In order to collect samples of ﬂaky UI tests, we
retrieve commit samples from GitHub repositories. First, we
obtain a list of the repositories leveraging popular web UI
frameworks using the topic keywords ‘react’, ‘angular’, ‘vue’,
‘emberjs’, ‘d3’, ‘svg’, ‘web’, and ’bootstrap’. These keywordsare used with the GitHub Search API [5] to identify 7,037
distinct repositories pertaining to these topics. From this set of
repositories, we download all of the commits for these projects
giving a total of 2,613,420 commits to search. Next, we follow
a procedure similar to the one used in Luo et al. [2] and search
the commit messages for the patterns ‘ﬂak*’ and ‘intermit*’
and the word ‘test’ to ﬁnd commits marked as ﬂaky. This step
reduces the number of commits to 3,516. In order to conﬁrm
which of these commits were ﬂaky UI tests, manual inspection
was performed on the commits in the list. In order to expedite
the process, another keyword search is performed using the
keywords ‘ui’, ‘gui’, ‘visual’, ‘window’, ‘button’, ‘display’,
‘click’, and ’animation’ to prioritize the commits most likely to
refer to ﬂaky UI tests. This ﬁnal search prioritizes 254 commit
messages to search, but the full 3,516 are searched to increase
the chance of identifying ﬂaky UI test commits. After manual
inspection and removing duplicate commits, the number of
veriﬁed ﬂaky tests is 152. Table II shows the summary of
commit information.
2) Android: Compared with web development, Android
developers are not as consistent with their choice of UI frame-
work. Therefore, to ﬁnd ﬂaky UI tests on the Android platform,
we use the GitHub Archive [6] to perform a massive search on
all commits and issue reports on GitHub instead of focusing
on repositories using popular UI framework. Speciﬁcally, We
limit our search to the list of closed issues, since the ﬂaky
tests in closed issues are more likely to be ﬁxed than the
tests in open issues. We search with the keywords ‘ﬂak*’,
‘intermit*’, and ‘test*’, which is similar to the patterns used
in web searching. In order to ensure the issues we ﬁnd are ﬂaky
UI tests on the Android platform, we also add constraints like
‘android’, ‘ui’, ‘espresso’, ‘screen’, etc.
B. Portion of Flaky UI Tests to Other Tests
We ﬁnd that ﬂaky UI tests collected in our methodology
make up a small portion of all tests available in these
repositories. This small portion can be explained by several
1586TABLE III: Top 10 Projects Containing the Most Flaky Tests
ProjectInspected
CommitsFlaky Tests LOC
Waterfox 937 23 3,949,098
qutebrowser 124 4 45,313
inﬂuxdb 81 12 124,591
angular 69 2 135,253
plotly.js 37 24 760,504
material-components-web 26 5 78,972
components 21 5 34,715
oppia 20 2 132,284
wix-style-react 15 11 19,830
streamlabs-obs 13 1 179,184
material-components-android 358 9 11,682
Focus-android 24 8 30,952
RxBinding 21 6 8,787
Xamarin.Forms 140 5 46,609
FirebaseUI-Android 34 4 4,386
Fenix 20 4 155,50
Detox 38 3 2,254
Components 14 3 34,715
Mapbox-navigation-android 4 2 6,201
Sunﬂower 3 1 354
reasons. Based on GH Archive [6], the number of open issues
(over 70,000) containing potential ﬂaky UI tests outnumbers
those in closed issues (over 30,000). Open issues cannot
be included in our study; however, this large number of
open issues possibly containing ﬂaky UI tests highlights the
signiﬁcance of UI ﬂakiness. Besides, there are ﬂaky UI tests
not captured through the keywords. One example is in the
material-components-web repository [2]. While our
dataset is not exhaustive, we believe the results can provide a
basis for future work to build on.
C. Sample Inspection
After collecting these commits and issues of ﬂaky tests re-
ports from GitHub, we manually inspect the collected samples
to identify the information relevant to our research questions.
In particular, we analyze the collected ﬂaky tests by ﬁrst
inspecting the commits in the web projects and the issue
reports in the Android projects for the following traits: the root
cause of the ﬂakiness, how the ﬂakiness is manifested, how the
ﬂakiness was ﬁxed, the test affected, the testing environment,
and the lines of code of the ﬁx. For the commits, we inspect
the commit message, changed code, and linked issues. For
the issue reports, we inspect the developer comments and the
linked commits. When available, we also inspect the execution
logs from the CI. Table III shows the information of projects
containing ﬂaky tests. Through inspection, we obtained the
sample set of 235 ﬂaky tests, of which 152 were from web
repositories and 83 were from Android repositories.
D. Dataset Composition
Our dataset consists of a diverse set of ﬂaky UI test
samples. The languages of the ﬂaky UI tests analyzed are
JavaScript (63.8%), TypeScript (20.4%), HTML (8.6%), and
others (7.2%) for the web projects and Java (48.2%), Kotlin
(21.7%), and others (30.1%) for the Android projects.IV . C AUSE OF FLAKINESS
We investigate the collected ﬂaky tests to determine the root
cause of the ﬂaky behavior. We manually inspect the related
commits and issues of the test in order to locate the code
or condition that caused the ﬂakiness. We base our root cause
categories on those deﬁned by Luo et al. [2]. We extend the set
of categories to include new categories speciﬁc to UI ﬂakiness
(“Animation Timing Issue”, “DOM Selector Issue”, etc...). The
categorization results are summarized in Table IV.
TABLE IV: Summary of Root Cause Categories Found
Root Cause
CategoriesRoot Cause
SubcategoriesWeb Mobile Total
Async Wait Network Resource Loading 15 4 19
Resource Rendering 47 14 61
Animation Timing Issue 17 9 26
Environment Platform Issue 16 18 34
Layout Difference 9 1 10
Test Runner DOM Selector Issue 13 3 16
API Issue Incorrect Test Runner Interaction 10 14 24
Test Script Unordered Collections 5 0 5
Logic Issue Time 1 0 1
Incorrect Resource Load Order 11 11 22
Test Order Dependency 6 6 12
Randomness 2 3 5
Total 152 83 235
A. Categorization
After manual inspection of the ﬂaky UI tests, we identify
four categories that the root causes of ﬂakiness in these tests
can fall under: (1) Timing Issue, (2) Platform Issue, (3) Test
Runner API Issue, and (4) Test Script Logic Issue. We describe
the categories and provide examples for each below.
1) Async Wait: We have found the root cause for a sig-
niﬁcant portion (45%) of the ﬂaky tests analyzed arise from
issues with async wait mechanisms. The common cause of
such issues is that the tests do not properly schedule fetching
and performing actions on program objects or UI elements,
causing issues with attempting to interact with elements that
have not been loaded completely. The program objects or UI
elements can come from network requests, the browser render-
ing pipeline, or graphics pipeline. This improper ordering of
events results in an invalid action and causes an exception to
be thrown. Among these async wait issues, we identiﬁed three
three subcategories that group similar root causes together.
a) Network Resource Loading: Flaky tests in this cat-
egory attempt to manipulate data or other resources before
they are fully loaded. Attempting to manipulate nonexistent
data causes exceptions in the test. An example is seen in
thering-ui [7] web component library repository. This
library provides custom-branded reusable web components to
build a consistent theme across web pages. In this project,
some components being tested utilize images that are fetched
through network requests; however, depending on the network
conditions, the images may fail to load on time or fail to load
altogether. The code snippet in Figure 1 shows how the url
variable deﬁned in Line 1 is URL for an image network call
to an external web server. The image is an avatar used by
1587thetag component on Line 8 to display on the page. When
the server call occasionally fails to respond in time due to a
heavy network load, the visual test will intermittently fail as
the rendered tag component will be missing the image.
1const url =
`${hubConFigureserverUri }/api/rest/avatar/
default?username=Jet%20Brains` ;,!
,!
2
3class TagDemo extends React.Component {
4render() {
5 return (
6 <div>
7 <Tag>Simple </Tag>
8 <Tag avatar ={url} readOnly ={false }>
9 With avatar
10 </Tag>
11 </div>
12 );
13 }}
Fig. 1: ring-ui Network Resource Loading Example.
Another example is found in the influxdb [8] project.
This project provides a client side platform meant for storing,
querying, and visualizing time series data. Figure 2 shows a
ﬂaky test for the label UI components. The test checks that
labels update properly by ﬁrst creating a new label and then
attempting to modify the label’s name and description through
the UI. Figure 2a presents the view of the test suite being run
on the left with the UI view on the right. Figure 2b shows
the code snippet of the test corresponding to the screenshot.
Lines 6-11 create the label to be used in the test. Lines 13-17
perform assertions on the labels retrieved through a network
call. However, due to the execution timing, the label is not
yet created in the backend store. The network call returns an
empty response which causes the assertion on Line 15 to fail.
b) Resource Rendering: Flaky tests in this category
attempt to perform an action on a UI component before it
is fully rendered. This attempt to interact with the missing
component leads to visual differences detected by screenshot
tests, or exceptions thrown by attempting to access elements
that have not fully loaded yet. An example of this is seen in the
generator-jhipster [9] project. This project provides a
platform to generate modern web application with Java. In this
project, a test script attempts to click on a button and wait for
the button to be displayed instead of the button being clickable.
Normally, these descriptions refer to the same event, but the
modal overlay shown in the UI can block the target button
from being clickable. The faulty code snippet is shown in
Figure 3. The waitUntilDisplayable function on Line
2 pauses the execution until the button is displayed on the
page. The test can fail intermittently if another element is still
above the button when Line 3 is reached, such as an element
acting as a background shade in a conﬁrmation modal.
This issue also appeared on the Android test, in the Volley
[10] project, the code snippet in Figure 4 leads to ﬂaky
behavior because of a short timeout. The listener occasionally
(a) The test for updating a label ﬁrst creates a new label through
the UI. In this case, the backend had not ﬁnished processing the
new label, so the network call to fetch all labels returns an empty
response.
1it('can update a label' , () => {
2 ...
3 const newLabelName ='attribut ()'
4 const newLabelDescription ="..."
5
6 // create label
7 cy.get <Organization >('@org' ).then(({id}) => {
8 cy.createLabel(oldLabelName, id, {
9 description :oldLabelDescription,
10 })
11 })
12
13 // verify name, descr, color
14 cy.getByTestID( 'label-card' )
15 .should( 'have.length' ,1)
16 cy.getByTestID( 'label-card' )
17 .contains(oldLabelName)
18 .should( 'be.visible' )
19 ...
20 // modify
21 cy.getByTestID( 'label-card' )
22 .contains(oldLabelName).click()
23 }
(b)influxdb ”Update Label” test code snippet.
Fig. 2: influxdb Network Resource Loading Example.
1const modifiedDateSortButton =
getModifiedDateSortButton(); ,!
2await
waitUntilDisplayed(modifiedDateSortButton); ,!
3await modifiedDateSortButton.click();
Fig. 3: generator-jhipster Resource Rendering Exam-
ple.
does not ﬁnish executing in 100 ms timeout. This conﬂicts
with the next request for verifying the order of calls.
1verifyNoMoreInteractions (listener );
2verify (listener ,timeout (100))
3 .onRequestFinished (higherPriorityReq );
4verify (listener ,timeout (10))
5 .onRequestFinished (lowerPriorityReq );
Fig. 4: Volley Resource Loading Example.
c) Animation Timing Issue: Flaky tests relying on an-
imations are sensitive to timing differences in the running
environment and may be heavily optimized to skip animation
1588events. The sensitivity to scheduling in animations can lead
to issues where assertions on the events are used to test for
animation progress.
An example of this type of issue is seen in the plotly.js
project [11]. This project provides visualization components
such as bar graphs, line plots, and more for use in web
pages. In the transition tests, the developers ﬁnd that they
intermittently fail due to an underlying race condition between
the call to transition the axes and the call to transition the bar
graphs. Depending on which transition is called ﬁrst, assertions
made on the layout of the graph may fail as the bar graph
elements are in different positions than expected. In Figure 5,
screenshots from a code snippet provided to reproduce the
different states of the animation are shown. In Figure 5a,
we see that graph starts with the ﬁrst bar is on value 3, the
second bar is on value 4, and the third bar is on value 5.
Figure 5b shows the frame immediately after the “react @
step 1” button is clicked, changing the values of the bars to 3,
6, and 5 respectively. In this ﬁgure, the background lines of
the axes have been shifted in order to represent the new scale,
but the bars scale incorrectly to the new axes values. Finally,
Figure 5c, the bars transition to their correct new values on
the new axes. Since the bars are not in the expected positions
during the transition test, the assertions made fail.
Another example is seen in the RXBinding
[12] project for Android’s UI widgets. In the
RxSwipeRefreshLayoutTest , which is used to test the
swipe refresh gesture, the call to stop the refresh animation
could happen anytime between the swipe release and the
actual refresh animation. The behavior is ﬂaky because the
swipe animation timing used in the recorder is unable to
catch up to the listener.
2) Environment: Some ﬂaky tests manifest due to differ-
ences in the underlying platform used to run the tests. The
platform can include the browser used for web projects and
the version of Android, iOS, etc... used for mobile projects.
We found that these issues can also be further divided into
two subcategories.
a) Platform Issue: These ﬂaky tests suffer from an
underlying issue in one particular platform that causes results
obtained or actions performed to differ between consecutive
runs within that same platform. In the ring-ui project [13],
the screenshot tests for a drop-down component fail due to a
rendering artifact bug present on Internet Explorer. This bug
causes a slight variation around the drop-down border in the
screenshots taken that cause the tests to fail when compared.
These tests pass when run on other browsers.
One example on Android is about Androidx navigation
tool [14]. For some versions of Android, Espresso has ﬂaky
behavior when performing a click action on the navigated
page because sometimes it cannot close the navigation drawer
before the click action. However, on other versions of Android,
this test always passed.
b) Layout Difference: Flaky tests can fail when the
layout is different than what is expected due to differences
in the browser environment. An example is found in theretail-ui project [15]. This project contains a set of
reusable components targeted at retail sites. The screenshot
test for its dropdown component fails because different default
window sizes across different browsers causes the dropdown
box to be cut off in some browsers.
3) Test Runner API Issue: Another root cause of ﬂakiness
we found involved an issue when interacting with the APIs
provided by the testing framework that caused it to func-
tion incorrectly. Flaky tests with this root cause either use
the provided APIs incorrectly, or the ﬂaky tests manage to
expose an underlying issue in the provided API that causes
the functionality to differ from what was expected. We also
identify two subcategories among the ﬂaky tests observed.
a) Incorrect Test Runner Interaction: UI tests use APIs
provided by the test runner to interact with UI elements, but
these APIs can hit unexpected behaviors that cause incorrect
behavior. For example, in the Android project FirefoxLite
[16], ﬂakiness appeared because the testing model registered
the click action by Espresso as a long click. Figure 6 shows
the UI layout after performing the click action incorrectly.
A testing site should have opened by clicking “Sample Top
Site” button. However, the “Remove” menu popped up instead
because of the long click action on “Sample Top Site” button.
This behavior difference caused the test to fail.
b) DOM Selector Issue: Flaky tests interacting with
DOM elements are intermittently unable to select the correct
element due to differences in browser implementations or stale
elements blocking focus. An example of the ﬂakiness arising
from an incorrect DOM element selection is found in the
react-datepicker project [17]. This project provides a
reusable date picker component for the React library. The code
under the test incorrectly sets two elements on the page to
auto-focus on, causing a jump on the page that results in a
visual ﬂicker.
4) Test Script Logic Issue: In some ﬂaky tests, ﬂakiness
arose due to incorrect logic within the test scripts. The ﬂaky
tests may have failed to clean data left by previous tests, made
incorrect assertions during the test, loaded resources in an
incorrect order, or incorrectly used a random data generator
to create incompatible data. We ﬁnd that tests in this category
fall under one of four subcategories.
a) Incorrect Resource Load Order: Flaky tests in
this category load resources after the calls that load
the tests, causing the tested resources to be unavail-
able when the test is run. For example, in the project
mapbox-navigation-android [18], the test crashed
with an exception, because they duplicated a resource load
call and then initialized a navigation activity.
b) Time: Flaky tests can fail when performing a com-
parison using a timestamp that may have changed from when
it is created depending on the execution speed of the test.
An example is found in the react-jsonschema-form
project [19]. The project generates forms in React code by
specifying the ﬁelds in JSON. In this project, a test on its date
picker widget intermittently fails due to a strict comparison
of time. Figure 7a shows a screenshot of a test failure
1589(a) Starting State
 (b) Background Axes Transition
 (c) Bars Transition
Fig. 5: An animation timing issue in the plotly.js project. (a) presents the initial state of a bar graph using the library. The
bars start at values 3, 4, and 5, respectively. (b) The value of the bars are changed to 3, 6, and 5, respectively. The background
axes change scale, but the bars are scaled incorrectly. (c) The bars then adjust to the correct scale.
Fig. 6: FirefoxLite Incorrect Test Runner Interaction
Example.
within a CI system resulting from a strict comparison issue.
Figure 7b presents the faulty code snippet of the ﬂaky test that
intermittently fails in the CI system. Depending on when the
test is run and how quickly the statements in the test execute,
the date-time value retrieved on Line 11 with the date-time
value generated in Line 12 can differ by a small amount,
causing the assertion on Line 13 to fail.
c) Test Order Dependency: Flaky tests in this category
can interfere with or be inﬂuenced by surrounding tests in the
test suit. This interference can be done through shared data
stores that are not cleaned well between test runs. As a result,
the data stores may contain values from previous tests and
produce incorrect values as a result. One example of this is
appeared in Android project RESTMock [20]. When trying
to reset the server between tests, the test would sometimes
return an exception, because there would be requests from the
previous test still running as Android shares some processes
between tests.
d) Randomness: Tests can use random data generation,
but these tests may intermittently fail for certain values of the
data generated. An example of this type of failure is found in
thenomad project [21], which provides a platform to deploy
and manage containers. In this project, they ﬁnd that tests
utilizing the job factory component to generate fake tasks can
intermittently fail when a job given a name or URL with spaces
is created. This causes encoding issues later on in the tests.
Since spaces are not valid in these ﬁelds, the spaces generated
by the random string generator are edge cases that should have
been handled.
(a) CI system failure in react-jsonschema-form project when
strictly comparing two date-time values. The values only differ by a
marginal amount due to the time when the test is executed, but since
the comparison is strict, the test will fail intermittently depending on
when it is run.
1it("should set current date when pressing the
Now button" , () => { ,!
2 const { node, onChange } =
createFormComponent({ ,!
3 schema :{
4 type :"string" ,
5 format :"date-time" ,
6 },
7 uiSchema,
8 });
9
10 Simulate.click(node.selector( "a.btn-now" ));
11 const formValue =
onChange.lastCall.args[ 0].formData; ,!
12 const expected =
toDateString(parseDateString( new
Date ().toJSON(), true ));,!
,!
13 expect(comp.state.formData).eql(expected);
14 });
(b)react-jsonschema-form Strict Comparison Code snippet.
Fig. 7: react-jsonschema-form Strict Comparison
Check Example.
B. Results
From these samples, we were able to ﬁnd characteristics
that are particular to ﬂaky UI tests. The most predominant root
cause for these ﬂaky UI tests involved improper handling of
asynchronous waiting mechanisms, such as the mechanisms
used when loading resources. These resources can include
network resources as well as elements that have not yet
been loaded in the page. This behavior resulted in erratic
results in the tests, such as attempting to click buttons that
had not yet opened. Many of these issues were resolved
by refactoring the code to include delays when handling a
1590potentially ﬂaky call. We found that the root cause of the ﬂaky
behavior could present a challenge to ﬁnd and properly ﬁx,
with some issues spanning over months to ﬁx. In addition, the
ﬂaky nature led some of these issue reports to be closed and
reopened in another report as many as ﬁve times. Other root
causes included platform-speciﬁc behavior, layout differences,
test order dependencies, and randomness. Platform-speciﬁc
behavior produces ﬂaky results for different runs in the same
platform. Layout differences behavior causes ﬂaky results
due to inconsistencies across different platforms. Flakiness
resulting from test order dependencies is caused by improper
cleanup of data after runs of previous tests. UI tests involving
random data generation can fail intermittently because of the
characteristics of the data generated.
V. M ANIFESTATION
Reproducing ﬂaky tests is a challenging task due to their
inherit non-deterministic behavior. If developers provide de-
tails on how the ﬂaky behavior was initially encountered and
subsequently reproduced, this information provides possible
strategies to apply to similar cases. We explore the strategies
used by developers to manifest the underlying ﬂaky behavior
and construct categories for similar manifestations actions
taken. These strategies are important when reporting the ﬂaky
test as they are inherently non-deterministic in nature, so it is
challenging to reproduce them compared with regular bugs.
Our categories are summarized in Table V.
TABLE V: Summary of Manifestation Categories
Manifestation Category Web Mobile Total
Unspeciﬁed 101 40 141
Specify Problematic Platform 21 17 38
Reorder/Prune Test Suite 9 3 12
Reset Conﬁguration Between Tests 2 7 9
Provide Code Snippet 14 6 20
Force Environment Conditions 5 10 15
Total 152 83 235
A. Specify Problematic Platform
Some tests are reported to only manifest on a speciﬁc
platform. In this case, the author of the report speciﬁes the
problematic platform version to reproduce the ﬂaky behavior.
An example of this type of manifestation is found in the
waterfox project [22]. This project is a web browser based
on Firefox. In this project, an issue involving animation timing
only manifests on MacOSX platforms. The report provides
details on which ﬁle to run on this particular platform in
order to reliably manifest the ﬂaky behavior seen in the
animation test. Another example in an Android project is
from gutenberg-mobile [23]. This project is the mobile
version for Gutenberg Editor. Figure 8 shows the bug that
only appeared on the Google Pixel device with Android 10.
When deleting the last character, the placeholder text should
reveal as shown in Figure 8b; however, the text does not popup. Instead, the screen appeared as shown in Figure 8a. Users
would need to add an additional backspace key press to show
the placeholder text.
(b) (a)
Fig. 8: Gutenberg-mobile Specify Problematic Platform
issue Example.
B. Reorder/Prune Test Suite
Flakiness arising from test-order dependencies can be man-
ifested by running the tests without the full test suite. This
includes running tests by themselves, running tests in a dif-
ferent order, and changing the state between test runs in order
to show the ﬂaky behavior. An example of this manifestation
strategy is seen in the influxdb project [24]. In this project,
the ﬂaky behavior surrounding table sorting is manifested by
running the tests in the test suite independently. In some cases,
trying to reset the environment conﬁguration between tests
can also lead to ﬂakiness. In the project RESTMock [20],
the developers tried to reduce ﬂakiness by resetting the server
conﬁguration between tests. However, the test became more
unstable because some Android processes were shared among
these tests, and the forced reset caused concurrency conﬂicts.
C. Provide Code Snippet
Among the bug reports we observed, we ﬁnd that some
reports include code snippets. The code snippets extract a
portion of the ﬂaky test into an enclosed sample to make
reproducing the ﬂaky behavior more reliable. An example of
this strategy is used in the project plotly.js project [25].
This projects provides data visualization components for use
in web pages. In this project, a test for a treemap component
contains a ﬂaky image load. In order to manifest this more
reliably, the reporter created a publicly-accessible code snippet
that runs the component with the ﬂaky loading behavior.
D. Force Environment Conditions
Flakiness that displays only when run on a speciﬁc platform
or under certain environment settings can be manifested by
forcibly setting these conditions, such as environment variables
or browser window size, during the test run. An example of
this can be found in the react-datepicker project [26].
This project provides a reusable datepicker component for use
in React apps. A test for the calendar component has ﬂaky
behavior when run on the ﬁrst two days of a new month. This
behavior is manifested by setting the time used in the test to
be one of these affected dates. Another example on Android
1591is a click function in Espresso [27]. If we run an Espresso test
which calls openActionBarOverflowOrOptionsMenu
on a slow device, a long-click action will be accidentally
performed. This bug can be manifested by a short long-click
timeout.
VI. F IXING STRATEGY
In this section, we examine the ﬁxes of the ﬂaky tests.
We identify common ﬁxing patterns and group them into
categories. Through comparative analysis of root causes and
ﬁxing strategies, we ﬁnd that most async wait issues are ﬁxed
by increasing delay or ﬁxing the await mechanism used. The
issues caused by the environments such as platform issues
and layout differences normally could not be solved. The
developers prefer to ﬁx these tests by using a workaround
or changing the library version. Table VI summarizes the cat-
egories and distribution of ﬁxing strategies and are described
in the following paragraphs.
TABLE VI: Summary of Fixing Categories Found
Categories Subcategories Web Mobile Total
DelayAdd/Increase Delay 14 7 21
Fix Await Mechanism 35 8 43
DependencyFix API Access 1 11 12
Change Library Version 1 6 7
Refactor Test Refactor Logic Implementation 49 26 75
Disable Features Disable Animations 1 3 4
Remove Test Remove Test 51 22 73
Total 152 83 235
A. Delay
1) Add or Increase Delay: In order to reduce the chance of
encountering ﬂaky behavior, some tests will add or increase
the delay between actions that involve fetching or loading.
This prevents the rest of the test script from executing until
the delay is up, giving the asynchronous call additional time
to complete before moving on. An example of this ﬁx is
used in the next.js project [28]. This project is used to
generate complete web applications with React as the frontend
framework. The patch increases the delays used in multiple
steps as shown in Figure 9. In the ﬁgure, Line 1 loads a new
browser instance and navigates to the “about” page. Lines 2
and 3 get the text on the page and assert that it is equal to
the expected value. Lines 4 and 5 manipulate the about page’s
component ﬁle on the ﬁlesystem to make it invalid for use.
Line 6 was the delay used before of 3 seconds. If the test is
run during a heavy load on the CI, the operation in Line 5
may take longer than 3 seconds, so the ﬁx is to update the
wait to 10 seconds shown in Line 7. Finally, Line 9 makes the
assertion that the updated text on the page shown matched the
expected error message. While this does not ﬁx the root cause
directly, this code patch does decrease the chance of running
into a timing issue during testing.1const browser =await
webdriver(context.appPort, '/hmr/about' ) ,!
2const text =await
browser.elementByCss( 'p').text() ,!
3expect(text).toBe( 'This is the about page.' )
4const aboutPage =new File(join(__dirname,
'../' ,'pages' ,'hmr' ,'about.js' )) ,!
5aboutPage.replace( 'export default' ,'export
default "not-a-page"\nexport const fn = ' ) ,!
6-await waitFor( 3000 )
7+await waitFor( 10000 ))
8expect(await
browser.elementByCss( 'body' ).text()) ,!
9.toMatch( /The default export is not a React
Component/ ) ,!
Fig. 9: next-js Increase Delay Example.
2) Fix Waiting Mechanism: In order to ﬁx ﬂaky behavior,
some tests ﬁx the mechanisms used to wait on an asyn-
chronous call. This ensures that the call would ﬁnish before
moving forward in the test script. An example is seen in
thegestalt project [29]. This project contains a set of
reusable components used on the Pinterest website. This test
is run using a headless browser, and it is accessed through
thepage variable. In Figure 10, lines 2-10 emit an event
on the page to trigger the action being tested. Line 12 is
supposed to pause the script execution for 200 milliseconds
in order for the page to complete the action from the event
handler. However, the function page.waitFor returns an
asynchronous JavaScript promise, so it requires the await
keyword in order to allow the promise to resolve before the
lines after the call are run. The issue is ﬁxed by adding the
await keyword where needed.
1it('removes all items' , async () => {
2 await page.evaluate(() => {
3 window .dispatchEvent(
4 new CustomEvent( 'set-masonry-items' ,{
5 detail :{
6 items :[],
7 },
8 })
9 );
10 });
11
12 -page.waitFor( 200);
13 +await page.waitFor( 200);
14
15 const newItems =await
page.$$(selectors.gridItem); ,!
16 assert.ok( !newItems newItems.length === 0 );
17 });
Fig. 10: Gestalt Fix Waiting Example.
Another example in an Android project is from project
RXBinding [12]. The developers avoided ﬂakiness in this
refresh layout test by manually removing the callbacks of
stopRefreshing and adding it back after 300 ms delay, if
the motion ACTION_UP has been caught. The code snippet
is shown in Figure 11.
15921+swipeRefreshLayout
2+ . setId (R.id.swipe_refresh_layout );
3+swipeRefreshLayout .setOnTouchListener (new
View .OnTouchListener () { ,!
4+ @Override public boolean onTouch (View v ,
MotionEvent event ) { ,!
5+ if
(MotionEventCompat .getActionMasked (event )
==MotionEvent .ACTION_UP ) {,!
,!
6+
handler .removeCallbacks (stopRefreshing ); ,!
7+ handler .postDelayed (stopRefreshing ,
300); ,!
Fig. 11: RxBinding Fix Waiting Mechanism Example.
B. External Dependency
1) Fix Incorrect API Access: Some tests resolved the ﬂak-
iness by ﬁxing the usage of an incorrect API function. After
switching this function, the test script behaved as expected. An
example is shown in the material-ui project [30], which
provides reusable web components implementing the Material
design system. An API function from the testing library
used to access DOM element children is incorrect. The code
snippet in Figure 12 shows how the incorrect API function is
ﬁxed by calling the proper getPopperChildren function
instead of attempting to get the element’s children directly.
The correct function adds additional selection criteria in order
to work within the template code generated by the third-party
popper.js [31] framework.
1-assert.strictEqual(
2-wrapper.find(Popper)
3-.childAt( 0)
4-.hasClass(classes.tooltip),
5-true
6-);
7
8+function getPopperChildren(wrapper) {
9+return new ShallowWrapper(
10 + wrapper
11 + .find(Popper)
12 + .props()
13 + .children({ popperProps :{ style :{} },
restProps :{} }), ,!
14 + null
15 +);
16 +}
17
18 +const popperChildren =
getPopperChildren(wrapper); ,!
19 +assert.strictEqual(
20 +popperChildren.childAt( 0)
21 +.hasClass(classes.tooltip),
22 +true );
Fig. 12: material-ui Fix Incorrect API Example.
Another example on the Android platform is found in the
Detox project [32]. The action to launch an application
in an existing instance, which has launched an app during
initialization, can lead to ﬂaky behavior. Launching an app
dynamically in UIAutomator is performed by moving to the
recent-apps view and then selecting the app name. However,1-device .pressRecentApps ();
2-UiObject recentApp =
device .findObject (selector
.descriptionContains (appName ));,!
,!
3-recentApp .click ();
4
5+final Activity activity =
ActivityTestRule .getActivity (); ,!
6+final Context appContext =
activity .getApplicationContext (); ,!
7+final Intent intent =new Intent (appContext ,
activity .getClass ()); ,!
8+intent .setFlags (Intent
.FLAG_ACTIVITY_SINGLE_TOP ); ,!
9+launchActivitySync (intent );
Fig. 13: Detox Fix Incorrect API Example.
sometimes the recent-apps view shows the full activity name
(e.g. com.wix.detox.MainActivity), instead of app name (e.g.
Detox), which causes ﬂakiness. To ﬁx this bug, developer
removed the UIAUtomator API and created new instances for
each launch request. Figure 13 shows the code snippet of this
ﬁxing process.
2) Change Library Version: Some tests changed the version
of a dependency used in the test as the developers found that
the new version introduced the ﬂaky behavior.
C. Refactor Test Checks
1) Refactor Logic Implementation: Some tests made
changes to the logic used when performing checks in order
to improve the intended purpose of the test while removing
the ﬂakiness observed in the test. An example is found in the
react-jsonschema-form project [19]. In the repository,
a check between consecutive timestamps is given an additional
error margin to handle the case of slow execution. Figure 14
shows the code snippet changing the exact date-time compar-
ison in Line 3 to the comparsion with an error margin of 5
seconds in Line 8.
1-const expected =toDateString(
2 parseDateString( new Date ().toJSON(),
true )); ,!
3-expect(comp.state.formData).eql(expected);
4+// Test that the two DATETIMEs are within 5
seconds of each other. ,!
5+const now =new Date ().getTime();
6+const timeDiff =now -new
Date (comp.state.formData) ,!
7 .getTime();
8+expect(timeDiff).to.be.at.most( 5000 );
Fig. 14: react-jsonschema-form Refactor Logic Imple-
mentation Example.
D. Disable Features During Testing
1) Disable Animations: In order to remove ﬂakiness caused
by animation timing, some test completely disabled animations
during their run. This change removed the concern of ensuring
an animation had completely ﬁnished before proceeding with
the rest of the script. An example of this is seen in the the
1593wix-style-react project where code is added to disable
CSS animations when the test suite is run [33]. Figure 15
shows the disableCSSAnimation function deﬁned on
Lines 1-15 CSS rules disabling all transitions and animations.
Line 21 adds a call to this function before all tests in the test
suite are run.
1+export const disableCSSAnimation =() => {
2+const css ='*{'+
3+'-webkit-transition-duration: 0s !important;'
+ ,!
4+'transition-duration: 0s !important;' +
5+'-webkit-animation-duration: 0s !important;'
+ ,!
6+'animation-duration: 0s !important;' +
7+'}',
8+head =document .head ||
9+
document .getElementsByTagName( 'head' )[0], ,!
10 +style =document .createElement( 'style' );
11
12 +style.type ='text/css' ;
13 +style.appendChild( document .createNode(css));
14 +head.appendChild(style);
15 +};
16 ...
17 beforeAll(() => {
18 browser.get(storyUrl);
19 +browser.executeScript(disableCSSAnimation);
20 });
Fig. 15: wix-style-react Disable Animations Example.
E. Removing Tests From Test Suite
1) Remove Tests: In order to ﬁx the test suite runs, some
projects choose to remove these tests from the suite. This ﬁx
removes the ﬂakiness in the test suite attributed to the ﬂaky
test being removed but reduces the code coverage.
2) Mark Tests as Flaky: Some tests are not entirely re-
moved from the test suite. Instead, they are marked as being
ﬂaky which means that if the test fails, the entire test suite
does not fail. This allows the test suite to be isolated from
the effects of the ﬂaky test without completely removing the
coverage it provides.
3) Blacklist Tests: In order to conditionally prevent some
tests from running, tests are added to a blacklist. The test in
these blacklists can be skipped from test runs by setting the
appropriate options for when the blacklist should be used.
VII. D ISCUSSION AND IMPLICATIONS
We investigate our collected ﬂaky UI tests to identify
relationships between the root causes, manifestation strategies,
and ﬁxing strategies deﬁned in Sections IV, V, and VI,
respectively.
Through our inspection, we can identify relationships be-
tween the underlying root causes in issues and how the issue
was ﬁxed. These relationships are presented in Figure 16.
The goal of our study on ﬂaky UI tests is to gain insights
for designing automated ﬂaky UI test detection and ﬁxing
approaches, so we analyze our dataset to identify correlations
between manifestation strategies and root causes. However,Test Runner API
Issue
EnvironmentAsync W aitDisable
Animations
Fix API Access
Fixing Strategies Root CausesTest Script
Logic IssueAnimation T iming Issue
Resource Rendering
Network Resource Loading
Incorrect Resource Load Order
Test Order Dependency
Other types
DOM Selector Issue
Incorrect T est Runner Interaction
Platform Issue
Layout Dif ferenceFix Await
MechanismAdd Delay
Change Library
VersionRefactor Logic
Fig. 16: Relationship Between Root Causes and Fixing Strate-
gies.
we ﬁnd that no strong correlations between these two groups
exist in the dataset. Similarly, we could not identify strong
correlations between manifestation strategy and ﬁxing strategy.
This leaves the question of detection strategies for ﬂaky UI
tests left open for future work to address. Our results do
support relationships between root causes and ﬁxing strategies.
If the root cause of a ﬂaky UI test is known, the relationships
we draw in Figure 16 can be used to select an appropriate
ﬁxing strategy.
Preliminary design ideas can be made for some of the ﬁxing
strategies we identify in Section VI. For the Add/Increase
Delay ﬁxing strategy, a possible automated implementation
could identify statements that perform the timing delay and
increase the amount of time speciﬁed. If there is no delaying
statement, then a delay can be after asynchronous function
calls are performed. Granular details such as the amount of
time to add in the delay or reliably identifying asynchronous
function calls requires further analysis on the collected sam-
ples. Using the relationships found in Figure 16, this approach
can be used to ﬁx issues caused by Resource Rendering
(22.4%) and Animation Timing Issue (15.4%). For the Fix
Await Mechanism ﬁxing strategy, an approach for automatic
repair would be to identify statements that implement asyn-
chronous wait mechanisms incorrectly. The details for this
approach would be dependent on the language of the project
and would require further analysis of the collected samples.
This approach for an automated implementation of the Fix
Await Mechanism can be used to ﬁx Incorrect Resource Load
Order (52.4%), Animation Timing Issue (38.5%), Resource
Rendering (20.7%), and DOM Selector Issue (18.8%). The
Disable Animations ﬁx can be implemented by conﬁguring
the test environment to disable animations globally when
setting up. This approach can be used to ﬁx issues caused by
Animation Timing Issue (7.7%), Resource Rendering (1.7%),
andDOM Selector Issue (6.25%). The Change Library Version
ﬁxing strategy could be automated by methodically switching
different versions of the dependencies used in the project. This
approach could be used to address issues caused by Animation
Timing Issue (7.7%) and Test Runner API Issue (6.25%).
1594VIII. T HREATS TO VALIDITY
The results of our study are subject to several threats,
including the representativeness of the projects inspected, the
correctness of the methodology used, and the generalizability
of our observations.
Regarding the representativeness of the projects in our
dataset, we focused on the most popular repositories associated
with popular frameworks on web and Android. We restrict
the repositories to focus on repositories that impact real
applications as opposed repositories under heavy development.
For mobile projects, we searched through GitHub database
with strict and clear condition settings to ensure that the
samples we obtain are targeted and representative.
In respect to the correctness of the methodology used, we
collect all available commits on GitHub from the repositories
related to popular web UI frameworks. We also leveraged the
GitHub Archive repository to ﬁnd all issues related to Android
UI frameworks. We ﬁlter out irrelevant commits and issues
using keywords and then manually inspect the remaining
commits and issues in order to verify the relevance to ﬂaky
UI tests. Each sample was inspected by at least two people in
order to achieve consensus on the data collected.
Regarding the generalizability of the implications made, we
selected ﬂaky test samples from actual projects used in the
wild. In addition, the samples do include large-scale industrial
projects, such as the Angular framework itself. We limit
numeric implications only to the dataset collected, and focus
on qualitative implications made on the features of the test
samples.
IX. R ELATED WORK
Empirical Studies on Software Defects. There have been
several prior studies analyzing the fault-related characteristics
of software systems [34, 35, 36, 37, 38, 39, 40, 41, 42, 43,
44, 45]. For example, in Lu et al. [36] an empirical study was
conducted on concurrency bugs. In Sahoo et al. [37], bugs in
server software were studied, and in Chou et al. [35], operating
system errors were investigated.
Studying Flaky Tests. Flaky tests have gained interest among
the academic community. These tests were ﬁrst looked at in
2014 by Luo et al. [2]. In this study, 201 commits from
51 open-source Java projects were manually inspected and
categorized into 11 categories. Later, Zhang et al. (2014) [46]
performed studied ﬂaky tests speciﬁcally caused by test order
dependencies. In 2015, Goa et al. [47] conducted a study
that concluded that reproducing ﬂaky tests can be difﬁcult.
Thorve et al. (2018) [48] studied 29 Android projects with
77 commits related to ﬂakiness. They found three new cat-
egories differing from the ones identiﬁed in earlier studies:
Dependency, Program Logic, and UI. Lam et al. (2019) [4]
examine the presence of ﬂaky test in large-scale industrial
projects and ﬁnd that ﬂaky test cause a signiﬁcant impact
on build failure rates. Mor ´anet al. (2019) [49] develop the
FlakcLoc technique to ﬁnd ﬂaky tests in web applications by
executing them under different environment conditions. Eck et
al.(2019) [50] survey 21 professional developers from Mozillato learn about the perceptions that developers have on the
impacts that ﬂaky tests cause during development. Dong et al.
(2020) [51] inspect 28 popular Android apps and 245 identiﬁed
ﬂaky tests to develop their FlakeShovel technique that controls
and manipulates thread execution. Lam et al. (2020) [52] study
the lifecycle of ﬂaky tests in large-scale projects at Microsoft
by focusing on the timing between ﬂakiness reappearance, the
runtime of the tests, and the time to ﬁx the ﬂakiness.
Detecting and Fixing Flaky Tests. Bell et al. (2018) [3]
developed the technique DeFlaker to detect ﬂaky tests by
monitoring the coverage of code changes in the executing
build with the location that triggered the test failure. Flaky
tests were those that failed without executing any of the new
code changes. Lam et al. (2019) [4] develop the framework
RootFinder to identify ﬂaky tests and their root causes through
dynamic analysis. The tool iDFlakies can detect ﬂaky tests
and classify the tests into order-dependent and non-order-
dependent categories [53]. Shi et al. (2019) [54] develop
the tool iFixFlakies to detect and automatically ﬁx order-
dependent tests by using code from other tests within a test
suite to suggest a patch. Terragni et al. (2020) [55] proposed
a technique to run ﬂaky tests in multiple containers with
different environments simultaneously.
X. C ONCLUSIONS
This paper performs a study on ﬂakiness arising in UI tests
in both web and mobile projects. We investigated 235 ﬂaky
tests collected from 25 web and 37 mobile popular GitHub
repositories. The ﬂaky test samples are analyzed to identify
the typical root causes of the ﬂaky behavior, the manifestation
strategies used to report and reproduce the ﬂakiness, and the
common ﬁxing strategies applied to these tests to reduce the
ﬂaky behavior. Through our analysis, we present ﬁndings on
the prevalence of certain root causes, the differences that
root causes appear between web and mobile platforms, and
the differences in the rates of ﬁxing strategies applied. We
believe our analysis can provide guidance towards develop-
ing effective detection and prevention techniques speciﬁcally
geared towards ﬂaky UI tests. We make our dataset available
at https://ui-ﬂaky-test.github.io/.
XI. A CKNOWLEDGMENTS
We thank the anonymous reviewers for their constructive
comments. This research was partially supported by NSF
2047980 and Facebook Testing and Veriﬁcation Research
Award (2019). Any opinions, ﬁndings, and conclusions in this
paper are those of the authors only and do not necessarily
reﬂect the views of our sponsors.
REFERENCES
[1] J. Micco, “The state of continuous integration testing@ google,” 2017.
[2] Q. Luo, F. Hariri, L. Eloussi, and D. Marinov, “An empirical analysis
of ﬂaky tests,” in Proceedings of the 22nd ACM SIGSOFT International
Symposium on Foundations of Software Engineering , ser. FSE 2014.
New York, NY , USA: Association for Computing Machinery, 2014, p.
643–653. [Online]. Available: https://doi.org/10.1145/2635868.2635920
1595[3] J. Bell, O. Legunsen, M. Hilton, L. Eloussi, T. Yung, and D. Marinov,
“Deﬂaker: Automatically detecting ﬂaky tests,” in 2018 IEEE/ACM 40th
International Conference on Software Engineering (ICSE) . IEEE, 2018,
pp. 433–444.
[4] W. Lam, P. Godefroid, S. Nath, A. Santhiar, and S. Thummalapenta,
“Root causing ﬂaky tests in a large-scale industrial setting,” in Proceed-
ings of the 28th ACM SIGSOFT International Symposium on Software
Testing and Analysis , 2019, pp. 101–111.
[5] “Search,” 2020. [Online]. Available: https://docs.github.com/en/rest/
reference/search
[6] Github, “Github archive.” [Online]. Available: https://archiveprogram.
github.com/
[7] “chore: use base64 uri decoded avatar to avoid ﬂaky ui tests if image,”
2020. [Online]. Available: https://github.com/JetBrains/ring-ui/commit/
f7bc28af06433ff22e898aacd2b3e8f0534defda
[8] “ﬁx(e2e): ﬁx race conditions,” 2019. [On-
line]. Available: https://github.com/inﬂuxdata/inﬂuxdb/commit/
4f5ff962d69a84f7a6970b02f9e79b09dbad21fe
[9] pascalgrimaud, “react: Fix intermittent e2e failures,”
https://github.com/jhipster/generator-jhipster/commit/
2865e441e4b09335f88f3839ee9147f8b8b9c05e, 2019.
[10] sphill99 and S. Phillips, “google/volley,” 2017.
[11] alexcjohnson, “one more ﬂaky test suite,” https://github.com/plotly/
plotly.js/commit/a2fc07a187c4d26bf2f1bcb3e2aa806b75ad24fc, 2018.
[12] nojunpark, “Fix rxswipedismissbehavior ﬂaky test,”
https://github.com/JakeWharton/RxBinding/commit/
affa7a4f58e5becec4ad8b49d30f525d6ad4c2a6, 2016.
[13] princed, “Concurrent modiﬁcation excep-
tion,” https://github.com/JetBrains/ring-ui/commit/
5d9f96d6ffa3a3c99722047677d5a545c02bdd80, 2017.
[14] chklow, “Espresso is not waiting for drawer to close,” 2019. [Online].
Available: https://github.com/android/testing-samples/issues/289
[15] “test(dropdowncontainer): ﬁx ﬂaky screenshot test,” 2019.
[Online]. Available: https://github.com/skbkontur/retail-ui/commit/
a006fdf0e0e65d5fde07134c6909870666e7947f
[16] cnevinc, “[ui test intermittent],” 2018. [Online]. Available: https:
//github.com/mozilla-tw/FirefoxLite/issues/2549
[17] Hacker0x01, “Remove second autofocus example,” 2018.
[Online]. Available: https://github.com/Hacker0x01/react-datepicker/
pull/1390/commits/8fc31964251944be79f2e8699b79e5f39080272f
[18] Guardiola31337, “Flaky navigationvieworientationtest,” https://github.
com/mapbox/mapbox-navigation-android/issues/1209, 2018.
[19] “merge pull request 167 from mozilla-services/162-
ﬁx-intermittent-da,” 2020. [Online]. Available:
https://github.com/rjsf-team/react-jsonschema-form/commit/
2318786b38ead5eddc7c0e3146825f19013e0beb
[20] Sloy, “Concurrent modiﬁcation exception,” 2019. [Online]. Available:
https://github.com/andrzejchm/RESTMock/issues/103
[21] “Ui: Fix a couple ﬂaky tests,” 2018. [On-
line]. Available: https://github.com/hashicorp/nomad/pull/4167/commits/
69251628f7a3f03ce603abfea5c8f48b4804c39e
[22] MrAlex94, “Bug 1504929 - start animations once after a mozreftestin-
validate,” 2018. [Online]. Available: https://github.com/MrAlex94/
Waterfox/commit/23793e3a2172787eca440889a8c4ec3cc6069862
[23] mchowning, “Deleting heading block content requires extra backspace
to show placeholder,” 2020. [Online]. Available: https://github.com/
wordpress-mobile/gutenberg-mobile/issues/1663
[24] inﬂuxdata, “ﬁx(ui): front end sorting for numeric values now being
handled,” 2019. [Online]. Available: https://github.com/inﬂuxdata/
inﬂuxdb/commit/bba04e20b44dd0f8fd049d80f270424eb266533f
[25] etpinard, “add treemap coffee to list of ﬂaky image tests,”
2020. [Online]. Available: https://github.com/plotly/plotly.js/commit/
66156054cb08b90bc50219ff9a2baeebb674c580
[26] aij, “Fix ﬂaky failing test,” 2017. [Online].
Available: https://github.com/Hacker0x01/react-datepicker/commit/
db64f070d72ff0705239f613bd5bba9602d3742f
[27] “Espresso,” 2020. [Online]. Available: https://developer.android.com/
training/testing/espresso
[28] vercel, “introduce dynamic(() =¿ import()),” 2020.
[Online]. Available: https://github.com/vercel/next.js/commit/
42736c061ad0e5610522de2517c928b2b8af0ed4
[29] pinterest, “masonry: masonryinﬁnite for inﬁnite fetching (307),”
2020. [Online]. Available: https://github.com/pinterest/gestalt/commit/
f6c683b66b2d8b0ec87db283418459e87160a21f[30] “[test] ﬁx ﬂaky popper.js test,” 2020. [On-
line]. Available: https://github.com/mui-org/material-ui/commit/
9d1c2f0ab014c76ddc042dea58a6a9384fc108f4
[31] popperjs, “Tooltip popover positioning engine,” 2020. [Online].
Available: https://github.com/popperjs/popper-core
[32] d4vidi, “Fix consecutive app-launches issue,” 2019. [On-
line]. Available: https://github.com/wix/Detox/pull/1690/commits/
c982798e8904b8384e4966f4ed20700b66921b399
[33] “skip ﬂaky visual eyes test (3306),” 2020. [On-
line]. Available: https://github.com/wix/wix-style-react/commit/
ddebb9fc31f3aaea7b80dea034c3baa256ec2b74
[34] R. Chillarege, W. . Kao, and R. G. Condit, “Defect type and its impact on
the growth curve (software development),” in [1991 Proceedings] 13th
International Conference on Software Engineering , 1991, pp. 246–255.
[35] A. Chou, J. Yang, B. Chelf, S. Hallem, and D. Engler, “An empirical
study of operating systems errors,” Operating Systems Review (ACM) ,
vol. 35, 09 2001.
[36] S. Lu, S. Park, E. Seo, and Y . Zhou, “Learning from mistakes: a
comprehensive study on real world concurrency bug characteristics,”
inASPLOS , 2008.
[37] S. K. Sahoo, J. Criswell, and V . Adve, “An empirical study of reported
bugs in server software with implications for automated bug diagnosis,”
inProceedings of the 32nd ACM/IEEE International Conference on
Software Engineering - Volume 1 , ser. ICSE ’10. New York, NY , USA:
Association for Computing Machinery, 2010, p. 485–494. [Online].
Available: https://doi.org/10.1145/1806799.1806870
[38] M. Sullivan and R. Chillarege, “A comparison of software defects in
database management systems and operating systems,” in [1992] Digest
of Papers. FTCS-22: The Twenty-Second International Symposium on
Fault-Tolerant Computing , 1992, pp. 475–484.
[39] F. Thung, S. Wang, D. Lo, and L. Jiang, “An empirical study of bugs in
machine learning systems,” in 2012 IEEE 23rd International Symposium
on Software Reliability Engineering , 2012, pp. 271–280.
[40] Z. Yin, D. Yuan, Y . Zhou, S. Pasupathy, and L. Bairavasundaram, “How
do ﬁxes become bugs?” in Proceedings of the 19th ACM SIGSOFT
Symposium and the 13th European Conference on Foundations of
Software Engineering , ser. ESEC/FSE ’11. New York, NY , USA:
Association for Computing Machinery, 2011, p. 26–36. [Online].
Available: https://doi.org/10.1145/2025113.2025121
[41] Z. Yin, X. Ma, J. Zheng, Y . Zhou, L. N. Bairavasundaram, and
S. Pasupathy, “An empirical study on conﬁguration errors in commercial
and open source systems,” in Proceedings of the Twenty-Third ACM
Symposium on Operating Systems Principles , ser. SOSP ’11. New
York, NY , USA: Association for Computing Machinery, 2011, p.
159–172. [Online]. Available: https://doi.org/10.1145/2043556.2043572
[42] Y . Zhang, Y . Chen, S.-C. Cheung, Y . Xiong, and L. Zhang, “An
empirical study on tensorﬂow program bugs,” in Proceedings of the
27th ACM SIGSOFT International Symposium on Software Testing
and Analysis , ser. ISSTA 2018. New York, NY , USA: Association
for Computing Machinery, 2018, p. 129–140. [Online]. Available:
https://doi.org/10.1145/3213846.3213866
[43] J. Aranda and G. Venolia, “The secret life of bugs: Going past the errors
and omissions in software repositories,” in 2009 IEEE 31st International
Conference on Software Engineering , 2009, pp. 298–308.
[44] Weining Gu, Z. Kalbarczyk, Ravishankar, K. Iyer, and Zhenyu Yang,
“Characterization of linux kernel behavior under errors,” in 2003 In-
ternational Conference on Dependable Systems and Networks, 2003.
Proceedings. , 2003, pp. 459–468.
[45] C. Sun, V . Le, Q. Zhang, and Z. Su, “Toward understanding compiler
bugs in gcc and llvm,” in Proceedings of the 25th International
Symposium on Software Testing and Analysis , ser. ISSTA 2016. New
York, NY , USA: Association for Computing Machinery, 2016, p.
294–305. [Online]. Available: https://doi.org/10.1145/2931037.2931074
[46] S. Zhang, D. Jalali, J. Wuttke, K. Mus ¸lu, W. Lam, M. D. Ernst, and
D. Notkin, “Empirically revisiting the test independence assumption,”
inProceedings of the 2014 International Symposium on Software
Testing and Analysis , ser. ISSTA 2014. New York, NY , USA:
Association for Computing Machinery, 2014, p. 385–396. [Online].
Available: https://doi.org/10.1145/2610384.2610404
[47] Z. Gao, Y . Liang, M. B. Cohen, A. M. Memon, and Z. Wang, “Making
system user interactive tests repeatable: When and what should we
control?” in 2015 IEEE/ACM 37th IEEE International Conference on
Software Engineering , vol. 1, 2015, pp. 55–65.
1596[48] S. Thorve, C. Sreshtha, and N. Meng, “An empirical study of ﬂaky tests
in android apps,” in 2018 IEEE International Conference on Software
Maintenance and Evolution (ICSME) , 2018, pp. 534–538.
[49] J. Mor ´an, C. Augusto Alonso, A. Bertolino, C. de la Riva, and J. Tuya,
“Debugging ﬂaky tests on web applications,” 01 2019, pp. 454–461.
[50] M. Eck, F. Palomba, M. Castelluccio, and A. Bacchelli, “Understanding
ﬂaky tests: The developer’s perspective,” in Proceedings of the 2019 27th
ACM Joint Meeting on European Software Engineering Conference
and Symposium on the Foundations of Software Engineering ,
ser. ESEC/FSE 2019. New York, NY , USA: Association for
Computing Machinery, 2019, p. 830–840. [Online]. Available: https:
//doi.org/10.1145/3338906.3338945
[51] Z. Dong, A. Tiwari, X. L. Yu, and A. Roychoudhury, “Concurrency-
related ﬂaky test detection in android apps,” 2020.
[52] W. Lam, K. Mus ¸lu, H. Sajnani, and S. Thummalapenta, “A study
on the lifecycle of ﬂaky tests,” in Proceedings of the ACM/IEEE
42nd International Conference on Software Engineering , ser. ICSE ’20.
New York, NY , USA: Association for Computing Machinery, 2020,
p. 1471–1482. [Online]. Available: https://doi.org/10.1145/3377811.
3381749
[53] W. Lam, R. Oei, A. Shi, D. Marinov, and T. Xie, “iDFlakies: A
Framework for Detecting and Partially Classifying Flaky Tests,” in 2019
12th IEEE Conference on Software Testing, Validation and Veriﬁcation
(ICST) , Apr. 2019, pp. 312–322, iSSN: 2159-4848.
[54] A. Shi, W. Lam, R. Oei, T. Xie, and D. Marinov, “iﬁxﬂakies:
A framework for automatically ﬁxing order-dependent ﬂaky tests,”
inProceedings of the 2019 27th ACM Joint Meeting on European
Software Engineering Conference and Symposium on the Foundations
of Software Engineering , ser. ESEC/FSE 2019. New York, NY , USA:
Association for Computing Machinery, 2019, p. 545–555. [Online].
Available: https://doi.org/10.1145/3338906.3338925
[55] P. S. Valerio Terragni and F. Ferrucci, “A container-based infrastructure
for fuzzy-driven root causing of ﬂaky tests,” 2020.
1597