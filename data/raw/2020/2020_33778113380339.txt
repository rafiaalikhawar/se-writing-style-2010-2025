Structure-Invariant Testing for Machine Translation
Pinjia He
Departmentof Computer Science
ETH Zurich
Switzerland
pinjia.he@inf.ethz.chClara Meister
Departmentof Computer Science
ETH Zurich
Switzerland
clara.meister@inf.ethz.chZhendong Su
Departmentof Computer Science
ETH Zurich
Switzerland
zhendong.su@inf.ethz.ch
ABSTRACT
Inrecentyears,machinetranslationsoftwarehasincreasinglybeen
integrated into our daily lives. People routinely use machine trans-
lationforvariousapplications,suchasdescribingsymptomstoa
foreign doctor and reading political news in a foreign language.
However,thecomplexityandintractabilityofneuralmachinetrans-
lation(NMT)modelsthatpowermodernmachinetranslationmake
therobustnessofthesesystemsdifficulttoevenassess,muchless
guarantee. Machine translation systems can return inferior results
that lead to misunderstanding, medical misdiagnoses, threats to
personal safety, or political conflicts. Despite its apparent impor-
tance, validating the robustness of machine translation systems is
very difficult and has, therefore, been much under-explored.
To tackle this challenge, we introduce structure-invariant testing
(SIT),anovelmetamorphictestingapproachforvalidatingmachine
translationsoftware.Ourkeyinsightisthatthetranslationresultsof
â€œsimilarâ€ source sentences should typically exhibit similar sentence
structures.Specifically,SIT(1)generatessimilarsourcesentencesby
substitutingonewordinagivensentencewithsemanticallysimilar,
syntactically equivalent words; (2) represents sentence structureby syntax parse trees (obtained via constituency or dependency
parsing); (3) reports sentence pairs whose structures differ quanti-
tatively by more than some threshold. To evaluate SIT, we use itto test Google Translate and Bing Microsoft Translator with 200
sourcesentencesasinput,whichledto64and70buggyissueswith
69.5%and70%top-1accuracy,respectively.Thetranslationerrors
arediverse,includingunder-translation,over-translation,incorrect
modification, word/phrase mistranslation, and unclear logic.
CCSCONCEPTS
â€¢Softwareverificationandvalidation ;â€¢Machinetranslation ;
KEYWORDS
Metamorphictesting,Machinetranslation,Structuralinvariance
ACM Reference Format:
PinjiaHe, ClaraMeister, andZhendong Su.2020.Structure-Invariant Test-
ingforMachineTranslation.In 42ndInternationalConferenceonSoftware
Engineering(ICSEâ€™20),May23â€“29,2020,Seoul,RepublicofKorea. ACM,New
York, NY, USA, 13 pages. https://doi.org/10.1145/3377811.3380339
Permissionto make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation
on the first page. Copyrights for components of this work owned by others than ACM
mustbehonored.Abstractingwithcreditispermitted.Tocopyotherwise,orrepublish,topostonserversortoredistributetolists,requirespriorspecificpermissionand/ora
fee. Request permissions from permissions@acm.org.
ICSEâ€™20,May23â€“29,2020,Seoul, Republic of Korea
Â© 2020Association for Computing Machinery.
ACM ISBN 978-1-4503-7121-6/20/05...$15.00
https://doi.org/10.1145/3377811.33803391 INTRODUCTION
Machine translation software has seen rapid growth in the last
decade; usersnow rely onmachine translation fora variety ofap-
plications,suchassigningleaseagreementswhenstudyingabroad,
describingsymptomstoaforeigndoctor,andreadingpoliticalnews
inaforeignlanguage.In2016,GoogleTranslate,themostwidely-
used online translation service, attracted more than 500 million
users and translated more than 100 billion words per day [ 81]. On
topofthis,machinetranslationservicesarealsoembeddedintovar-
ious software applications, such as Facebook [ 25] and Twitter [ 82].
The advances in machine translation that are responsible for
such growth can largely be attributed to neural machine transla-tion (NMT) models, which have become the core component of
manymachine translationsystems. Asreportedby researchfrom
Google[86]andMicrosoft[ 32],state-of-the-artNMTmodelsare
approaching human-level performance in terms of accuracy, i.e.,
BLEU[67].Theserecentbreakthroughshaveleduserstostartre-
lying on machine translation software (e.g., Google Translate [ 30]
and Bing MicrosoftTranslator [5]) in their daily lives.
However, NMT models are not as reliable as many may believe.
Recently, sub-optimal and incorrect outputs have been found in
varioussoftwaresystemswithneuralnetworksastheircorecom-
ponents. Typical examples include autonomous cars [ 23,68,79],
sentiment analysis tools [ 2,36,46], and speech recognition ser-
vices [6,71]. These recent research efforts show that neural net-
works can easily return inferior results (e.g., wrong class labels)
givenspecially-craftedinputs(i.e.,adversarialexamples).NMTmod-elsarenoexception;theycanbefooledbyadversarialexamples[
22]
ornaturalnoise (e.g.,typosininput sentences)[ 4].Theseinferior
results(i.e.,sub-optimalorincorrecttranslations),canleadtomisun-
derstanding,embarrassment,financialloss,medicalmisdiagnoses,
threatstopersonalsafety,orpoliticalconflicts[ 17,57,64,65,80].
Thus,assuringtherobustnessofmachinetranslationsoftwareis
an important endeavor.
Yet testing machine translationsoftwareis extremely challeng-
ing.First,differentfromtraditionalsoftwarewhoselogicisencoded
in source code, machine translation software is based on complex
neural networks with millions of parameters. Therefore, testing
techniquesfortraditional software,whichare mostlycode-based,
are ineffective. Second, the line of recent research on testing artifi-
cialintelligence(AI) software [ 2,29,36,37,46,62,68] focuses on
tasks withmuch simpleroutput formatsâ€”forexample,testing im-
age classifiers, which output class labels given an image. However,
asoneofthemostdifficultnaturallanguageprocessing(NLP)tasks,theoutputofmachinetranslationsystems(i.e.,translatedsentences)
issignificantlymorecomplex.Becausetheyarenotstructuredto
handlesuchcomplexoutputs,whenappliedtoNMTmodels,typical
*&&&"$.OE*OUFSOBUJPOBM$POGFSFODFPO4PGUXBSF&OHJOFFSJOH	*$4&
ICSE â€™20, May 23â€“29, 2020, Seoul, Republic of Korea Pinjia He, Clara Meister, and Zhendong Su
 


 	 

	 	 
 
		

	  
 
	

	  
	 
		
Figure 1: Examples of similar source sentences and Google Translate results.
AItestingapproachesalmostsolelyfind"illegal"inputs,suchassen-
tences with syntax errors or obvious misspellings that are unlikely
givenasinput.Yettheseerrorsarenottheproblematiconesinprac-
tice; as reported by WeChat, a messenger app with over one billionmonthlyactive users, itsembedded NMT model can return inferiorresultsevenwhentheinputsentencesaresyntacticallycorrect[
96].
Due to the difficultyof building an effective, automated approach
toevaluatethecorrectnessoftranslation,currentapproachesfor
testingmachinetranslationsoftware have many shortcomings.
Approaches that try to address these aforementioned problems
still have their own deficienciesâ€”namely, the inability to detect
grammaticalerrorsandthelackofreal-worldtestcases.Current
testing procedures for machine translation software typically in-
volvethreesteps[ 96]:(1)collectingbilingualsentencepairs1and
splittingthemintotraining,validation,andtestingdata;(2)calcu-
lating translation quality scores (e.g., BLEU [ 67] and ROUGE [ 48])
ofthetrainedNMTmodelonthetestingdata;and(3)comparing
the scores with predefined thresholds to determine whether thetest cases pass. However, testing based on a threshold score like
BLEU, which is a measurement of the overlap between n-grams
of the target and reference, can easily overlook grammatical er-
rors. Additionally, the calculation of translation quality scores (e.g.,
BLEU) requires bilingual sentence pairs as input, which need to
bemanuallyconstructedbeforehand.Totestwithreal-worlduser
input outside of the training set, extensive manual effort is needed
forground-truthtranslations. Thus,an effectiveand efficient test-
ing methodology that can automatically detect errors2in machine
translationsoftware is in high demand.
Toaddresstheaboveproblems,weintroducestructure-invariant
testing(SIT),anovel,widely-applicablemethodologyforvalidatingmachinetranslationsoftware.Thekeyinsightisthatsimilarsource
sentencesâ€”e.g. sentences that differ by a single wordâ€”typically
havetranslationresultsofsimilarsentencestructures.Forexample,
Fig. 1 shows three similar source sentences in English and their
targetsentencesinChinese.Thefirsttwotranslationsarecorrect,
whilethethirdisnot.Wecanobservethatthestructureofthethird
sentence in Chinese significantly differs from those of the other
two. For each source sentence, SIT (1) generates a list of its similar
sentences by modifying a single word in the source sentence via
NLPtechniques(i.e.,BERT[ 19]);(2)feedsallthesentencestothe
software under test to obtain their translations; (3) uses specialized
data structures (i.e., constituency parse tree and dependency parse
tree)torepresentthesyntaxstructureofeachofthetranslatedsen-
tences; and (4) compares the structures of the translated sentences.
1Byasentencepair,werefertoasourcesentenceanditscorrespondingtargetsentence.
2By atranslation error,we referto mistranslationof someparts ofa source sentence.
Thetranslatedsentence(i.e.,targetsentence)containingtranslationerror(s)isregarded
asabuggysentence.Weuse"errorinthetargetsentence"and"errorinthesentence
pair"interchangeablyin this paper.If a large difference exists between the structures of the translated
original and any of the translated modified sentences, we report
the modified sentence pair along with the original sentence pair as
potential errors.
WeapplySITtotestGoogleTranslateandBingMicrosoftTrans-
lator with 200 source sentences crawled from the Web as input.SIT successfully found 64 buggy issues (defined in Section 3) in
Google Translate and 70 buggy issues in Bing Microsoft Translator
withhighaccuracy(i.e.,69.5%and70%top-1accuracyrespectively).
The reported errors3are diverse, including under-translation, over-
translation, incorrect modification, word/phrase mistranslation,
and unclearlogic, noneof whichcould bedetected bythe widely-
usedmetricsBLEUandROUGE.Examplesofdifferenttranslation
errors are illustrated in Fig. 2. The source code and datasets arealso released for reuse. Note that our results were w.r.t.the snap-
shots of Google Translate and Bing Microsoft Translator when we
performed our testing. After releasing our results dataset in July
2019,wenoticethatsomeofthereportedtranslationerrorshave
recently been addressed.
Thispaper makes the following main contributions:
â€¢Itintroducesstructure-invarianttesting(SIT),anovel,widely
applicablemethodologyforvalidatingmachinetranslation
software;
â€¢ItdescribesapracticalimplementationofSITbyadapting
BERT [19] to generate similar sentences and leveraging syn-
tax parsers to represent sentence structures;
â€¢It presentsthe evaluationof SITusing only200 sourcesen-
tencescrawledfromtheWebtosuccessfullyfind64buggy
issuesinGoogleTranslateand70buggyissuesinBingMi-
crosoft Translator with high accuracy; and
â€¢It discusses the diverse error categories found by SIT, of
which nonecould be found by state-of-the-art metrics.
2 A REAL-WORLD EXAMPLE
Tom planned to take his son David, who is 14 years old, to the
ZurichZoo. Before their zoo visit, he checked the zooâ€™s website4
aboutpurchasing ticketsandsawthe followingGermansentence:
     	
  
 
Tom is from the United States, and he does not understand Ger-
man. To figure out its meaning, Tom used Google Translate, a
populartranslationservicepoweredbyNMTmodels[ 86].Google
Translate returned the following English sentence:
3https://github.com/PinjiaHe/StructureInvariantTesting
4https://www.zoo.ch/de/zoobesuch/tickets-preise
Structure-Invariant Testing for Machine Translation ICSE â€™20, May 23â€“29, 2020, Seoul, Republic of Korea
 Source sentence Target sentence Target sentence meaning
I am very willing to share  my point 
of view.
I had a joke to tell and  I wanted to 
finish it, Draper says.&>$##&5/   
(by Bing)But even so, they remain prisoners 
of privilege .Under-translation
Incorrect 
modification
Word/phrase 
mistranslation
!=4?& 38?&"%(by Google)I am very willing to agree with
 my 
point of view.
I joked that  I want to finish it, 
Draper says.Unclear logicIt is believed in the field that 
Amazon employs more PhD 
economists than any other tech company.%$

7'<;(by Google)Amazon employs more PhD economists than any other tech company.
Entering talks, Brazil hoped to see itself elevated to major non NATO ally status by the Trump administration, a big step that would help it purchase military equipment.Over-translation# 	
91,+)(;5>6
?"*!:5.  (by Bing)Entering talks, Brazil hoped to see itself elevated to major non NATO ally status by the Trump administration, one
 a big step that 
would help it purchase military equipment.
	-?0*251 (by Google)But even so, they remain prisoners'
 priviliege .Error type
Figure 2: Examples of translation errors (English-to-Chinese) detected by SIT.

 











 




 !








 "
	


 



 




 


	




 


  





 	 
	 
 
 
		
	 

 


 




 		

	 

 


	 
Figure 3: Overview of SIT.
	






	
 


	
However,Davidwasdeniedfreeentrybythezoostaffevenwith
avalidID.Theyfoundoutthattheyhadmisunderstoodthezooâ€™s
regulation because of the incorrect translation returned by Google
Translate. The correct translation should be:
	






	

	

 

	
Thisisarealtranslationerrorthatledtoaconfusing,unpleasant
experience.Translationerrorscouldalsocauseextremelyserious
consequences[ 17,57,65,80].Forexample,aPalestinian manwas
arrestedbyIsraelipoliceforapostsaying"goodmorning,"which
Facebookâ€™s machinetranslation serviceerroneously translated as
"attackthem" in Hebrew and "hurt them" in English [ 17,65]. This
demonstrates both the widespread reliance on machine transla-
tion software and the potential negative effects when it fails. To
enhance the reliability of machine translation software, this paper
introduces a general validation approach called structure-invarianttesting, which automatically and accurately detects translation er-
rors without oracles.
3 APPROACH AND IMPLEMENTATION
This section introduces structure-invariant testing (SIT) and de-
scribes our implementation. The input of SIT is a list of unlabeled,
monolingualsentences,whileitsoutputisalistofsuspicious issues.
For each original sentence, SIT reports either 0 (i.e., no buggy sen-
tenceisfound)or1 issue(i.e.,atleast1buggysentenceisfound).
Eachissuecontains: (1) the original source sentence and its transla-
tion;and(2)top-kfarthest5generatedsourcesentencesandtheir
translations.Theoriginalsentencepairisreportedforthefollowing
reasons: (1) seeing how the original sentence was modified may
helptheuserunderstandwhythetranslationsystemmadeamis-
take; (2) the error may actually lie in the translation of the original
sentence.
Fig.3illustratestheoverviewofSIT.Inthisfigure,weuseone
source sentence as input for simplicity and clarity. The key insight
5thedistancemetrichereisbetweenthestructuresoftheoriginalsentencetranslation
andthemodified sentence translations
ICSE â€™20, May 23â€“29, 2020, Seoul, Republic of Korea Pinjia He, Clara Meister, and Zhendong Su
	

	



 



   
     	
	
	
	
   



	
	

Figure 4: Similar sentence generation process.
of SITis thatsimilarsource sentencesoften havetarget sentences
ofsimilarsyntacticstructure.Derivedfromthisinsight,SITcarries
out the following four steps:
(1)Generatingsimilar sentences. Foreach sourcesentence,we
generate a list of its similar sentences by modifying a single
word in the sentence.
(2)Collectingtargetsentences. Wefeedtheoriginalandthegen-
eratedsimilarsentencestothemachinetranslationsystem
undertestand collect their target sentences.
(3)Representing target sentence structures. All the target sen-
tences are encoded as data structures specialized for natural
language processing.
(4)Detecting translation errors. The structures of the translated
modified sentences are compared to the structure of the
translated original sentence. If there is a large difference
between the structures, SIT reports a potential error.
3.1 GeneratingSimilarSentences
Inordertotestforstructuralinvariance,wemustcomparetwosen-
tencesthathavethesamesyntacticstructurebutdifferinatleast
one token. We have found that, given an input sentence, chang-ing one word in the sentence at a time under certain constraints
effectively produces a set of structurally identical and semantically
similarsentences.
Explicitly, the approach we take modifies a single token in an
input sentence, replacing it with another token of the same partof speech (POS),
6to produce an alternate sentence. For example,
we will mask "hairy" in the source sentence in Fig. 4 and replace
it with the top-k most similar tokens to generate k similar sen-
tences.Wedothisforeverycandidatetokeninthesentence;forthe
sake of simplicity and to avoid grammatically strange or incorrect
sentences,we only use nouns and adjectives as candidate tokens.
Now we discuss the problem of selecting replacement tokens.
Perhapsthesimplestalgorithmforselectingasetofreplacementto-kenswouldinvolveusingwordembeddings[
60].Onecouldchoose
words that have high vector similarity with and identical POS tags
to a given token in the original sentence as replacements in themodified sentences. However, since word embeddings have the
samevalueregardlessofcontext,thisapproachoftenproducessen-tencesthatwouldnotoccurincommonlanguage.Forexample,the
word"fork"mighthavehighvectorsimilaritywithandthesame
POStagastheword"plate." However, whilethesentence"Hecame
toaforkintheroad"makessense,thesentence"Hecametoaplate
in the road" does not.
6https://en.wikip edia.org/wiki/Part_of_speechRather, we wanta model that considersthe surrounding words
andcomesupwithasetofreplacementsthat,wheninserted,create
realisticsentences.Amodelthatdoesjustthisisthemaskedlan-
guagemodel(MLM)[ 59],inspiredbytheClozetask[ 78].Theinput
toanMLMisapieceoftextwithasingletokenmasked(i.e.,deletedfromthesentenceandreplacedwithaspecialindicatortoken).The
job of the model is then to predict the token in that position given
thecontext.Thismethodforcesthemodeltolearnthedependen-
ciesbetweendifferenttokens.Sincethereareanumberofdifferentcontextsasinglewordcanfitin,thismodel,inasense,allowsfora
singletokentohavemultiplerepresentations.Wethereforegeta
set of replacement tokens that are context dependent. While the
predictedtokensarenotguaranteedtohavethesamemeaningas
theoriginaltoken,iftheMLMiswelltrained,itishighlylikelythat
the sentence with the new, predicted token is both syntactically
correct and meaningful.
An example of the sentence generation process is demonstrated
inFig.4.Forourimplementation,weuseBERT[ 19],whichisastate-
of-the-art language representation model recently proposed by
Google.Theout-of-boxBERTmodelprovidespre-trainedlanguage
representations that can be fine-tuned by adding an additional
lightweight softmax classification layer to create models for a widerangeoflanguage-relatedtasks,suchasmaskedlanguagemodelling.
BERTwastrainedonahugeamountofdataâ€”aconcatenationof
BooksCorpus(800Mwords)andEnglishWikipedia(2,500Mwords)â€”
withthemaskedlanguagetaskbeingoneoftwomaintasksusedfor
training.Thus,webelievethatBERTfitsthisaspectofourapproach
well.
3.2 Collecting Target Sentences
Oncewehavegeneratedalistofsimilarsentencesfromouroriginalsentence,thenextstepistoinputallthesourcesentencestothema-chinetranslationsoftwareundertestandcollectthecorresponding
translation results (i.e., target sentences). We subsequently analyze
the results to find errors. We use Googleâ€™s and Bingâ€™s machine
translationsystemsastestsystemsforourexperiment.Toobtain
translation results, we invoke the APIs provided by Google Trans-
lateandBingMicrosoftTranslator,whichreturnidenticalresults
as their Web interfaces [5, 30].
3.3 Representationsof the Target Sentences
Next we must model the target sentences obtained from the trans-
lation system under test as this allows us to compare structuresto detect errors. Choosing the structure with which to represent
oursentenceswillaffectourabilitytoperformmeaningfulcompar-
isons.Weultimatelywantarepresentationthatpreciselymodels
the structure of a sentence while offering fast comparison between
two values.
The simplest and fastest approach is to compare sentences in
theirraw form: as strings. Indeed, we test this method and perfor-
mance is reasonable. However, there are many scenarios in which
thismethodfallsshort.Forexample,theprepositionalphrase"on
Friday"inthesentence"OnFriday,wewenttothemovies"canalso
be placed on the end of the sentence as follows: "We went to the
movies on Friday." The sentences are interchangeable but a metric
such as character edit distance would indicate a large difference
Structure-Invariant Testing for Machine Translation ICSE â€™20, May 23â€“29, 2020, Seoul, Republic of Korea
  #


&%
%
     #   
	*









	




!
%
 
 "



 
 
'$ (
'$(
'$('	$(
'$(
'#$(
'#$(
'#$('$#(

 
#
 	 
Figure 5: Representing sentence structures; both depen-
dency& constituency relations can be displayed as trees.
between the strings. Syntax parsing overcomes the above issue.
With a syntax parser, we can model the syntactic structure of a
string and the relationship between words or groups of words. For
example, if parsing is done correctly, our two sample sentencesabove should have identical representations in terms of relation
valuesand parsestructure.
3.3.1 Raw Target Sentence. Forthismethod,weleaveourtarget
sentence in its original format, i.e., as a string. In most cases, wemay expect that editing a single token in a sentence in one lan-
guage would lead to the change of a single token in the translated
sentence. Giventhe syntacticrole of thereplacement tokenis the
same, this would ideally happen in all machine translation sys-tems. However, this is not always the case in practice as prepo-sitional phrases, modifiers, and other constituents can often beplaced in different locations by the translation system and pro-
duce a semantically-equivalent, grammatically correct sentence.
Nonetheless,this method serves as a good baseline.
3.3.2 Constituency Parse Tree. Constituencyparsingisonemethod
for deriving the syntactic structure of a string. It generates a set
of constituency relations, which show how a word or group of
wordsformdifferentunitswithinasentence.Thissetofrelationsis
particularlyusefulforSITbecauseitwillreflectchangestothetype
of phrases in a sentence. For example, while a prepositional phrase
can be placed in multiple locations to produce a sentence withthe same meaning, the set of constituency relations will remain
unchanged.Constituencyrelationscanbevisualizedasatree,as
shown in Fig. 5. A constituency parse tree is an ordered, rootedtree where non-terminal nodes are the constituent relations andterminal nodes are the words. Formally, in constituency parsing,a sentence is broken down into its constituent parts according
tothephrasestructurerules[ 14]outlinedbyagivencontext-freegrammar.Forourexperiments,weusetheshift-reduceconstituency
parserbyZhu etal.[99]andimplementedinStanfordâ€™sCoreNLP
library [31]. It can parse about 50 sentences per second.
3.3.3 Dependency Parse Tree. Dependency parsing likewise de-
rivesthesyntacticstructureofastring.However,thesetofrelations
produceddescribethedirectrelationshipsbetweenwordsrather
thanhowwordsconstituteasentence.Thissetofrelationsgives
us different insights about structure and is intuitively useful forSIT because it will reflect changes between how words interact.
Muchprogresshasbeenmadeoverthepast15yearsondependency
parsing. Speed and accuracy increased dramatically with the intro-
duction of neural network based parsers [ 11]. As with shift-reduce
constituencyparsers,neuralnetworkbaseddependencyparsersuse
a stack-likesystem wheretransitions arechosen using aclassifier.
Theclassifierinthiscaseisaneuralnetwork,likewisetrainedon
annotated tree banks. For our implementation, we use the mostrecent neural network based parsers made available by Stanford
CoreNLP, which can parse about 100 sentences per second. We use
theUniversalDependenciesasourannotationscheme,whichhas
evolved based off the Stanford Dependencies [18].
3.4 Translation Error Detection via Structure
Comparison
Finally, in order to find translation errors, we search for structural
variation by comparing sentence representations. Whether sen-
tences are modelled as raw strings, word embeddings, or parsetrees, there are a number of different metrics for calculating the
distancebetweentwovalues.Thesemetricstendtobequitedomainspecificandmighthavelowcorrelationwitheachother,makingthechoiceofmetricincrediblyimportant.Forexample,ametricsuchas
Word Moverâ€™s Distance [ 41] would give us a distance of 0 between
thetwosentences"Hewenttothestore"and"Storehethewentto"
while character edit distance would give a distance of 14. We ex-
plore several different metrics for evaluating the distance between
sentences:character(Levenshtein)editdistance,constituencyset
difference, and dependency set difference.
3.4.1 Levenshtein Distance between Raw Sentences. The Leven-
shteindistance[ 44],sometimesmoregenerallyreferredtoasthe
"editdistance,"comparestwostringsanddetermineshowclosely
they match each other by calculating the minimum number of
characteredits(deletions,insertions,andsubstitutions)neededto
transform one string into the other. While the method may not
demonstratesyntacticsimilaritybetweensentenceswell,itexploits
the expectation that editing a single token in a sentence in one
language will often lead to the change of only a single token in the
translatedsentence. Therefore,theLevenshteindistance servesas
a good baseline metric.
3.4.2 Relation Distance between Constituency Parse Trees. To eval-
uate the distance between two sets of constituency relations, we
calculate the distance between two lists of constituency grammars
as the sum of absolute difference in the count of each phrasal type,
whichgivesusabasicunderstandingofhowasentencehaschanged
after modification. The motivation behind this heuristic is that the
constituentsofasentenceshouldstaythesamebetweentwosen-
tences where only a single token of the same part of speech differs.
ICSE â€™20, May 23â€“29, 2020, Seoul, Republic of Korea Pinjia He, Clara Meister, and Zhendong Su
In a robust machine translation system, this should be reflected in
the targetsentences as well.
3.4.3 Relation Distance between Dependency Parse Trees. Similarly,
for calculating the distance between two lists of dependencies, we
sumtheabsolutedifferenceinthenumberofeachtypeofdepen-
dency relations. Again, the motivation is that the relationships
between words will ideally remain unchanged when a single token
is replaced. Therefore, a change in the set is reasonable indication
that structural invariance has been violated and presumably there
is a translation error.
3.4.4 Distance Thresholding. Usingoneoftheabovemetrics,we
calculate the distance between the original target sentence and
the generated target sentences. We must then decide whether a
modified target sentence is far enough from the its corresponding
original target sentence to indicate the presence of a translation
error. To do this, we first filter based on a distance threshold, only
keeping sentences that are farther from the original sentence than
the chosen threshold. Then, for a given original target sentence,
we report the top-k (kalso being a chosen parameter) farthest
modified target sentences. We leave the distance threshold as a
manualparametersincetheusermayprioritizeminimizingfalse
positive reports or minimizing false negative reports depending on
their goal. In Section 4.6, we show tradeoffs for different threshold
values. For each original sentence, an issue will be reported if at
least one translated generated sentence is considered buggy.
4 EVALUATION
In this section, we evaluate our approach by applying it to Google
Translate and Bing Microsoft Translator with real-world unlabeled
sentences crawled from the Web. Our main research questions are:
â€¢RQ1:Howeffectiveistheapproachatfindingbuggytransla-
tionsin machine translation software?
â€¢RQ2:Whatkindsoftranslationerrorscanourapproachfind?
â€¢RQ3:How efficient is the approach?
â€¢RQ4:How do we select the distance threshold in practice?
4.1 ExperimentalSetup
ToverifytheresultsofSIT,wemanuallyinspecteachissuereported
andcollectivelydecide:(1)whethertheissuecontainsbuggysen-
tences; and (2) if yes, what kind of translation errors it contains.
AllexperimentsarerunonaLinuxworkstationwith6CoreIntel
Corei7-87003.2GHzProcessor,16GBDDR42666MHzMemory,and
GeForceGTX 1070GPU.The Linuxworkstationis running64-bit
Ubuntu18.04.02with Linuxkernel 4.25.0.
4.2 Dataset
Typically, to test a machine translation system, developers can
adopt SIT with any source sentence as input. Thus, to evaluate the
effectiveness of our approach, we collect real-world source sen-tences from the Web. Specifically, input sentences are extracted
from CNN7(Cable News Network) articles in two categories: poli-
tics and business. The datasets are collected from two categories
7https://edition.cnn.com/of articles because we intend to evaluate whether SIT consistently
performs well on sentences of different semantic context.
For each category, we crawled the 10 latest articles, extracted
their main text contents, and split them into a list of sentences.
Then, we randomly select 100 sentences from each sentence list as
theexperimentaldatasets(200intotal).Inthisprocess,sentences
that contain more than 35 words are filtered because we intend to
demonstrate that machine translation software can return inferior
results even for relatively short, simple sentences. The details of
the collected datasets are illustrated in Table 1.
Table1:Statisticsofinputsentencesforevaluation.Eachcor-
pus contains100 sentences.
# of Words/ Average # of
Corpus Sentence Words/Sentence Total Distinct
Politics 4~32 19.2 1,918 933
Business 4~33 19.5 1,949 944# of Words
4.3 TheEffectiveness of SIT
Ourapproachaimstoautomaticallyfindtranslationerrorsusing
unlabeled sentences and report them to developers. Thus, the ef-
fectiveness of the approach lies in two aspects: (1) how accurateare the reported results; and (2) how many buggy sentences can
SIT find? In this section, we evaluate both aspects by applying SIT
totestGoogleTranslateandBingMicrosoftTranslatorusingthe
datasetsillustrated in Table 1.
4.3.1 Evaluation Metric. TheoutputofSITis alistof issues,each
containing (1) an original source sentence and its translation; (2)
thetop-kreportedgeneratedsentencesandtheirtranslations(i.e.
thekfarthest translations from the source sentence translation).
Here we define top- kaccuracy asthe percentage ofreported issues
whereatleastoneofthetop-kreportedsentencesortheoriginal
sentence contains an error. We use this as our accuracy metric for
SIT.Explicitly,ifthereisabuggysentenceinthetop- kgenerated
sentences of issue i, we consider the issue to be accurate and set
buÐ´Ð´y(i,k)to true; else we set buÐ´Ð´y(i,k)to false. If the original
sentence is buggy and was reported as an issue, then we also set
buÐ´Ð´y(i,k)to true. Given a list of issues I, its top-kaccuracy is
calculated as:
Accuracyk=/summationtext.1
iâˆˆI1{buÐ´Ð´y(i,k)}
|I|, (1)
where|I|is the number of the issues returned by SIT.
4.3.2 Results. Top-kaccuracy. TheresultsaresummarizedinTa-
ble 2. SIT (Raw), SIT (Constituency), and SIT (Dependency) are SIT
implementations with raw sentence, constituency structure, and
dependency structure as sentence structure representation, respec-
tively.Eachiteminthetablepresentsthetop-kaccuracyalongwith
thenumberofbuggyissuesfound.Insubsequentdiscussions,for
brevity,wereferSIT(Constituency)andSIT(Dependency)asSIT
(Con) and SIT (Dep), respectively.
WeobservethatSIT(Con)andSIT(Dep)consistentlyperform
betterthanSIT(Raw),whichdemonstratestheimportanceofthe
Structure-Invariant Testing for Machine Translation ICSE â€™20, May 23â€“29, 2020, Seoul, Republic of Korea
Table 2: Top-k accuracy of SIT.
Top-1 Top-2 Top-3
(#buggy issues) (#buggy issues) (#buggy issues)
SIT (Raw) 55.0% (55) 63.0% (63) 66.0% (66)
SIT (Constituency) 61.3% (62) 66.3% (67) 68.3% (69)
SIT (Dependency) 69.5% (64) 71.7% (66) 73.9% (68)
Top-1 Top-2 Top-3
(#buggy issues) (#buggy issues) (#buggy issues)
SIT (Raw) 58.8% (60) 69.6% (71) 71.5% (73)
SIT (Constituency) 67.0% (67) 71.0% (71) 74.0% (74)
SIT (Dependency) 70.0% (70) 71.0% (71) 78.0% (78)Google Translate
Bing Microsoft 
Translator
structurerepresentationofsentences.ThemetricusedinSIT(Raw),
which is based only on the characters in the sentences, is brittle
andsubjecttooverandunderreporterrors.Forexample,SIT(raw)
may report sentences that are different in word level but similar
in sentence structure, leading to false positives. SIT (Con) and SIT
(Dep)achievecomparableperformanceintermsofbothtop-kaccu-
racy and the number of reported buggy issues. In particular, when
testing Bing Microsoft Translator, SIT (Dep) reports 100 suspicious
issues.Amongtheseissues,70ofthemcontaintranslationerrors
in the first reported sentence or the original sentence, achieving
70%top-1accuracy.SIT(Dep)hasthebestperformanceonTop-1
accuracy for both Google Translate and Bing Microsoft Translator.
Itsuccessfullyfinds64and70buggyissueswith69.5%and71%top-
1 accuracy, respectively. SIT (Dep) also achieves the highest top-3
accuracy(73.9%and78%).Notethatsourcesentencesinthesame
issueonlydifferbyoneword.Thus,inspectingtop-3sentenceswill
not cause more effort compared with inspecting top-1 sentences.
In addition, we study whether SIT can trigger new errors in
thegeneratedsentences.AsillustratedinTable3,inthereported
issues,55and60uniqueerrorsarefoundinthetranslationoforigi-
nalsentencesbyGoogleTranslateandBingMicrosoftTranslator
respectively. Besides these errors, SIT finds 79 and 66 extra unique
errors that are revealed only in the generated sentence pairs but
not in the original. Thus, given its high top-k accuracy and lotsof extra unique errors reported, we believe SIT is very useful in
practice.
We did not compared SITâ€™s accuracy with [ 96] and [97] because
of the following reasons. SIT targets general mistranslation errors,
while [96] focuses on under-/over-translations. Thus, we did not
empirically compare with it. In terms of error type and quantity,
[96]canonlyfindsomeunder-/over-translationerrorsinoriginal
sentencetranslations,whileSITfindsgeneralerrorsintranslations
of both original sentences and their derived similar sentences. [ 97]
requires input sentences with specialized structures and thus it
cannotdetect any errors using our datasets.
4.4 Translation Error Reported by SIT
SIT is capable of finding translation errors of diverse kinds. In our
experiments with Google Translate and Bing Microsoft Translator,
wemainlyfind5kindsoftranslationerrors:under-translation,over-
translation,incorrectmodification,word/phrasemistranslation,and
unclear logic. The error types are derived from error classificationTable 3: Number of unique errors. Top-k unique errors by
SIT are errors only in generated sentences output by SIT
(Dep).
Google 55 45 64 79
Bing 60 32 43 66#Top-3 unique 
errors by SITOriginal 
sentences#Top-1 unique 
errors by SIT#Top-2 unique 
errors by SIT
Table 4: Number of sentences that have specific errors in
each category SIT (Dep).
Top-1 35 \ 17 9 \ 8 4 \ 2 44 \ 54 27 \ 31
Top-2 48 \ 23 12 \ 15 6 \ 3 59 \ 60 44 \ 41
Top-3 61 \ 35 15 \ 21 10 \ 4 75 \ 93 53 \ 59Unclear
logicWord/phrase 
mistranslationGoogle \ Bing Under 
translationOver 
translationIncorrect 
modification
methods for machine translation. Each of the five is a subset of
lexical, syntactic, or semantic errors [ 34]. We rename them in a
more intuitive manner to aid the readers. To provide a glimpseof the diversity of the uncovered errors, this section highlights
examplesforallthe5kindsoferrors.Table4presentsthestatisticsofthetranslationerrorsSITfound.Under-translation,word/phrase
mistranslation,andunclearlogicaccountformostofthetranslation
errors foundby SIT.
4.4.1 Under-Translation. If some words are mistakenly untrans-
lated(i.e.donotappearinthetranslation),itisanunder-translation
error.Fig.6presentsasentencepairthatcontainsunder-translation
error. In this example, "to Congress" is mistakenly untranslated,
which leads to target sentences of different semantic meaning.
Specifically, "lying to Congress" is illegal while "lying" is just an in-
appropriate behavior. Likewise, the real-world example introduced
in Section 2 is caused by an under-translation error.
After pleading guilty in the Manhattan probe, Cohen also later pleaded 
guilty to lying in a case brought by Mueller's website.Target 
meaningSource
TargetAfter pleading guilty in the Manhattan probe, Cohen also later pleaded guilty to lying to Congress
 in a case brought by Mueller's website.

	  
(by Bing)
Figure 6: Example of under-translation errors detected.
4.4.2 Over-Translation. Ifsomewordsareunnecessarilytranslated
multiple times or some words in the target sentence are not trans-
latedfromanywordsinthesourcesentence,itisanover-translation
error. In Fig. 7, "thought" in the target sentence is not translated
from any words in the source sentence, so it is an over-translation
error.Interestingly,wefoundthatanover-translationerroroften
happensalongwithsomeotherkindsoferrors.Theexamplealso
contains an under-translation error because "were right" in the
ICSE â€™20, May 23â€“29, 2020, Seoul, Republic of Korea Pinjia He, Clara Meister, and Zhendong Su
source sentence is mistakenly untranslated. In the second exam-
pleinFig. 2,theword"a" isunnecessarilytranslatedtwice,which
makesit an over-translation error.
The investigators thought  that the airplane itself was safe.Target  
meaningSource
TargetThe investigators were right that the airplane itself was safe.

	 (by Google)
Figure 7: Example of over-translation errors detected.
4.4.3 Incorrect Modification. If some modifiers modify the wrong
element in the sentence, it is an incorrect modification error. In
Fig. 8, the modifier "new" modifies "auto manufacturing" in thesource sentence. However, Google Translate thinks that "new"should modify "hub." In Fig. 2, the third example also shows an
interestingincorrectmodificationerror.Inthisexample("prisoners
of privilege"), "privilege" modifies "prisoners" in the source sen-tence, while Google Translate thinks "prisoners" should modify"privilege." We think that in the training data of the NMT model,
there are some phrases with the similar pattern: "A ofB," where A
modifies B,whichleadstoanincorrectmodificationerrorinthis
scenario. Interestingly, the original source sentence that triggers
thiserroris"Butevenso,theyremain bastionsofprivilege."Inthe
originalsentence,"bastions"modifies"privilege,"whichfitsthesup-
posed archetype. As we might expect, this sentence is correctly
translated by Google Translate.
The South has emerged as a new hub of auto manufacturing  by 
foreign makers thanks to the reducing manufacturing costs and less 
powerful businesses.Target 
meaningSource
TargetThe South has emerged as a hub of new auto manufacturing  by 
foreign makers thanks to lower manufacturing costs and less powerful businesses.
	  
 
(by Google)
Figure8:Exampleofincorrectmodificationerrorsdetected.
4.4.4 Word/phrase Mistranslation. Ifsometokensorphrasesare
incorrectly translated in the target sentence, it is a word/phrase
mistranslationerror.Fig.9presentstwomainsub-categoriesofthis
kind of error: (1) ambiguity of polysemy and (2) wrong translation.
Ambiguityofpolysemy. Eachtoken/phrasemayhavemultiple
correcttranslations.Forexample,admitmeans"allowsomebody
to join an organization" or "agree with something unwillingly."
However, usually in a specific semantic context (e.g., a sentence), a
token/phrase only has one correct translation. Modern translation
softwaredoesnotperformwellonpolysemy.Inthefirstexample
in Fig. 9, Google Translate thinks the "admit" in the source sen-
tence refers to "agree with something unwillingly," leading to a
token/phrasemistranslationerror.
Wrong translation. A token/phrase could also be incorrectly
translatedtoanothermeaningthatseemssemanticallyunrelated.Forexample,inthesecondexampleinFig.9,BingMicrosoftTransla-torthinks"South"refersto"SouthKorea,"leadingtoaword/phrase
mistranslationerror.
The most elite public universities  agree unwillingly that  considerably 
larger percentage of students from lower income backgrounds than do 
the elite private schools.Target 
meaningSource
TargetThe most elite public universities admit  a considerably larger percentage 
of students from lower income backgrounds than do the elite private schools.
%&!$ *%&#$"*!')
(by Google)
SourceThe South
 has emerged as a hub of new auto manufacturing by foreign 
makers thanks to lower manufacturing costs and less powerful unions.
Target 	( *
*	( 	(
!(by Bing)
Target 
meaningThe South Korea  has emerged as a hub of new auto manufacturing by 
foreign makers thanks to lower manufacturing costs and less powerful 
unions.
Figure9:Examplesofword/phrasemistranslationerrorsde-
tected.
4.4.5 Unclear Logic. Ifallthetokens/phrasesarecorrectlytrans-
lated but the sentence logic is incorrect, it is an unclear logic error.
In Fig. 10, Google Translate correctly translates "serving in the
elected office" and "country." However, Google Translate generates
"servingintheelectedofficeasacountry"insteadof"servingthe
country in elected office" because Google Translate does not un-derstand the logical relation between them. Unclear logic errors
existwidelyintranslationsgivenbyNMTmodels,whichistosomeextentasignofwhetheramodeltrulyunderstandscertainsemantic
meanings.
And attacking a dead man who spent five years as a prisoner of war and 
another three decades serving in elected office as a country , is simply 
wrong.Target 
meaningSource
TargetAnd attacking a dead man who spent five years as a prisoner of war and another three decades serving the country in elected office
, is simply 
wrong.
	 

(by Google)
Figure 10: Example of unclear logic errors detected.
4.4.6 Sentences with Multiple Translation Errors. A certain per-
centage of reported sentence pairs contain multiple translation
errors. Fig. 11 presents a sentence pair that contains three kindsof errors. Specifically, "covering" means "reporting news" in the
sourcesentence.However,itistranslatedto"holding,"leadingto
aword/phrasemistranslationerror.Additionally,"church"inthe
target sentence is not the translation of any words from the source
sentence,soitis anover-translationerror.BingMicrosofttransla-
tor also wrongly thinks the subject is "attending a funeral train."
Butthesourcesentenceactuallymeansthesubjectis"coveringa
funeral train," so it is an unclear logic error.
Structure-Invariant Testing for Machine Translation ICSE â€™20, May 23â€“29, 2020, Seoul, Republic of Korea
word/phrase     logic    over
SourceCovering  a memorial service in the nation's capital and then traveling to 
Texas for another service as well as a funeral train  was an honor, he 
says.
Target"! "	  

"(by Bing)
Target 
meaningHolding  a memorial service in the nation's capital and then traveling to 
Texas for attending  another church  service and a funeral train  was an 
honor, he says.Errors
Figure11:Exampleofsentencewithmultipletranslationer-
rors detected.
4.5 The RunningTimeof SIT
In this section, we evaluate the running time of SIT on the two
datasets.WeapplySITwith3differentsentencestructurerepresen-
tations to test Google Translate and Bing Microsoft Translator. We
runeachexperimentsetting10timesandreporttheiraverageas
the results.The overallrunning timeof SITis illustrated in Table5,
and the running time of each step of SIT on Google Translate is
presentedinFig.12(Bingâ€™sresultissimilar).Wecanobservethat
SITusingrawsentencesasstructurerepresentationisthefastest.
This is because SIT (Raw) does not require any structure represen-
tation generation time. SIT using a dependency parser achieves
comparablerunningtimetoSIT(Raw).Inparticular,SIT(Dep)uses
19 seconds to parse 2000+ sentences (as opposed to 0 seconds by
SIT (Raw)), which we think is efficient and reasonable.
Table 5: Average running time of SIT on Politics and Busi-
ness datasets.
Google \ BingRunning time 
(sec)Translation time 
(sec)#Sentence 
translatedTime of other 
SIT steps (sec)
SIT (Raw) 1,469 \ 922 1,417 \ 870 2,012 52 \ 52
SIT (Constituency) 1,524 \ 981 1,417 \ 870 2,012 107 \ 110
SIT (Dependency) 1,488 \ 945 1,417 \ 870 2,012 71 \ 75
In these experiments, we ran the translation step once per trans-
lationsystemandreusedthetranslationresultsinallexperiment
settings since the other settings had no impact on translation time.
Thus,in Table 5, the Translation time valuesare the same for dif-
ferent SIT implementations. We can observe that SIT spends most
of thetime collectingtranslation results.In this step, for eachsen-
tence, we invoked the APIs provided by Google and Bing to collect
the translated sentence. In practice, if users want to test their own
machine translation software with SIT, the running time of this
step will be much less. As indicated in a recent study [ 92], current
NMT model can translate around 20 sentences per second using a
single NVIDIA GeForce GTX 1080 GPU. With more powerful com-
putingresource(e.g,TPU[ 86]),modernNMTmodelscanachieve
thespeedofhundredsof sentencestranslationpersecond,which
would be about 2 magnitudes faster than in our experiments.
The other steps of SIT are quite efficient, as indicated in Table 5
andFig.12.BothSIT(Raw)andSIT(Dep)tookaround1minand
SIT(Con)tookaround2mins.ComparedwithSIT(Dep),SIT(Con)
is slower because models for constituency parsing are slower than
those for dependency parsing. We conclude that as a tool working	

		

 
 	

 	
!
	!		
"		!

			"		!

	
			

 
 	

	
 
	 		
!		 

		"	!		 

Figure 12: Running time details of SIT (excluding transla-
tion time)in testing Google Translate.
in an offline manner, SIT is efficient in practice for testing machine
translationsoftware.
4.6 TheImpactof Distance Threshold
SITreportsthetop-ksentencepairsinanissueifthedistancebe-
tween the translated generated sentence and the original targetsentence is larger than a distance threshold. Thus, this distancethreshold controls (1) the number of buggy issues reported and
(2) the top-k accuracy of SIT. Intuitively, if we lower the thresh-
old, more buggy issues will be reported, while the accuracy will
decrease. Fig. 13 demonstrates the impact of the distance threshold
on these two factors. In this figure, SIT (Dep) was applied to test
theBingMicrosoftTranslatoronourPoliticsandBusinessdatasets
with different distance thresholds. We can observe that both the
numberofbuggyissuesandtop-1accuracyremainstablewhenthe
thresholdiseithersmallorlargewhilethevaluesfluctuateinthe
middle. The impact of changing the distance threshold is similar
whentestingGoogle Translate.
Basedonthese results,wepresentsomeguidance onusingSIT
in practice. First, if we intend to uncover as many translation er-
rors as possible, we should use a small distance threshold. A small
threshold (e.g., 4 for dependency sets) works well on all our exper-
iment settings. In particular, with a small threshold, SIT reports
themostissueswithdecentaccuracy(e.g.,70%top-1accuracy).We
adopt this strategy in our accuracy experiments in Section 4.3.2.
Developers could use SIT with small distance threshold when they
want to intensively test software before a release. Second, if we
intend to make SIT as accurate as possible, we could use a large
threshold(e.g.,15).Withalargethreshold,SITreportsfewerissues
withveryhighaccuracy(e.g.,86%top-1accuracy).Giventhatthe
number of source sentences are unlimited on the Web, we could
keeprunningSITwith alargedistancethresholdandperiodically
report issues. Thus, we think SIT is effective and easy to use in
practice.
4.7 Fine-tuning with Errors Reported by SIT
Inthissection,westudywhetherthereportedbuggysentencescan
act as a fine-tuning set to improve the robustness of NMT models.
Fine-tuningisacommonpracticeinNMT,wheretrainingdataandtargetdatacanoftenoccupydifferentdomains[
15,74].Specifically,
wetrainanencoder-decodermodelwithglobalattention[ 51]â€”a
standard architecture for NMT modelsâ€”on a subset of the CWMT
corpus with 2M bilingual sentence pairs [ 16]. The encoder and
ICSE â€™20, May 23â€“29, 2020, Seoul, Republic of Korea Pinjia He, Clara Meister, and Zhendong Su
Figure 13: Impact of distance threshold when testing Bing
MicrosoftTranslator.
decoder are unidirectional single-layer LSTMs. We train the model
using the Adam optimizer [ 40], calculating the BLEU [ 67] score on
aheldoutvalidationsetaftereachepoch.Weusethemodelwith
parameters from the epoch with the best validation BLEU score.
Note that we did not use Google or Bingâ€™s translation models here
becausetheyarenotopen-source;however,theencoder-decoder
model with attention is a very representative NMT model.
TotesttheNMTmodel,SITisrunon40Englishsentences,which
areselectedfromthevalidationsetofWMTâ€™17[ 85]byremoving
long sentences (i.e., longer than 12 words) and ensuring that all
words are in the NMT modelâ€™s vocabulary. Note that since the
modelwasnottrainedorvalidatedondatafromthisdomain,we
simulatedthepracticalscenariowherereal-worldinputsdifferfrom
model trainingdata. Basedon theseinputs, SITsuccessfully finds
105buggy sentences.We label them with correct translations and
fine-tune the NMT model on these 105 sentences for 15 epochs
withadecreasinglearningrate.Afterthisfine-tuning,allthe105
sentencescan be correctly translated. Meanwhile,the BLEU score
on the original validation set used during training increases by
0.13, which, to some degree, shows that the translation of othersentences has also been improved. This demonstrates the ability
to fix errors reported by SIT in an efficient and easy manner. SITâ€™s
utility on building robust machine translation software will be
furtherelaborated in Section 5.2.
5 DISCUSSIONS
5.1 False Positives
While SIT can accurately detect translation errors, its precision
can be further improved. In particular, the false positives of SIT
come from three main sources. First, the generated sentences may
have strange semanticmeanings, leading to changes in thetarget
sentence structure. For example, based on the phrase "on the way,"
the current implementation of SIT could generate the sentence "on
thefact,"whichnaturallyhasaverydifferenttranslationinChinese.
Using BERT, which at the time of our experiments provided the
state-of-the-art masked language model, helped alleviate this issue.
Second,althoughtheexistingsyntaxparsersarehighlyaccurate,
theymayproducewrongconstituencyordependencystructures,
whichcanleadtoerroneousreportederrors.Third,asourcesen-
tence could have multiple correct translations of different sentence
structures.Forexample,targetsentence"10yearsfromnow"and
"after10years"canbeusedinterchangeablywhiletheirsentence
structuresaredifferent.Tolowertheimpactofthesefactors,SITreturnsthetop-ksuspicioussentencepairsrankedbydistanceto
the originaltargetsentence.
5.2 Building Robust Translation Software
The ultimate goal of testing machine translation, similar to test-
ing traditional software, is to build robust software. Toward this
end,SITâ€™sutilityisasfollows.First,thereportedmistranslations
typically act as early alarms, and thus developers can hard-code
translationmappingsin advance, which isthe quickest bug fixing
solution adopted in industry. Second, the reported sentences could
beusedasafine-tuningset,whichhasbeendiscussedinSection4.7.
Third,developersmayfindthereportedbuggysentencepairsuseful
forfurtheranalysis/debuggingsincethesentencesineachpaironly
differ by one word. This resembles debugging traditional software
via input minimization/localization. Additionally, the structural
invariance concept could be utilized as inductive bias to design
robust NMT models, similar to how Shen et al.[75] introduce bias
to standard LSTMs. Compared with traditional software, the de-
bugging and bug fixing process of machine translation software is
more difficult because the logic of an NMT model mainly lies in its
modelstructureandparameters.Whilethisisnotthemainfocus
of our work, we believe it is an important research direction for
future work.
6 RELATED WORK
6.1 Robustness of AI Software
Thesuccess ofdeeplearningmodelshas ledtothewide adoption
of artificial intelligence (AI) software in our daily lives. Despite
theirhighaccuracies,deeplearningmodelscangenerateinferior
results,someofwhichhaveevenleadtofatalaccidents[ 42,45,100].
Recently, researchers have designed a variety of approaches to at-
tackdeeplearning(DL)systems[ 3,6,7,20,29,89,91].Toprotect
DL systems against these attacks, excellent research has been con-
ductedtotestDLsystems[ 21,26,33,39,53,54,68,69,79,88,94,95],
assistthedebuggingprocess[ 55],detectadversarialexampleson-
line[56,77,84,90],ortrainnetworksinarobustway[ 38,49,58,66].
Comparedwiththeseapproaches,ourpaperfocusesonmachine
translation systems, which these works do not explore. In addition,
mostoftheseapproachesrequireknowledgeofgradientsoracti-
vation valuesin the neuralnetwork under test(white-box), while
our approach does not require any internal details of the model
(black-box).
6.2 Robustness of NLP Algorithms
Deep neural networks have boosted the performance of many NLP
tasks , such as reading comprehension [ 9,10], code analysis [ 1,
35,70], and machine translation [ 32,83,86]. However, in recent
years,inspiredbytheworkonadversarialexamplesinthecomputer
visionfield,researcherssuccessfullyfoundbugsproducedbythe
neuralnetworksusedforvariousNLPsystems[ 2,8,36,36,37,46,
61,62,72]. Compared with our approach, these works focus on
simplertaskssuchas text classification.
Zhengetal.[96]introducedtwoalgorithmstodetecttwospecific
translationerrors: under-translationandover-translation, respec-
tively.Comparatively,ourproposedapproachismoresystematic
andnotlimitedtospecificerrors.Basedontheexperimentalresults,
Structure-Invariant Testing for Machine Translation ICSE â€™20, May 23â€“29, 2020, Seoul, Republic of Korea
wecanfindthefollowingerrors:under-translation,over-translation,
incorrectmodification,ambiguityofpolysemy,andunclearlogic.
Zhou and Sun [ 97] proposed a metamorphic testing approach (i.e.,
MT4MT)for machinetranslation;theyfollowed aconcept similar
tostructuralinvariance.However,MT4MTcanonlybeusedwith
simple sentences in a subject-verb-object pattern (e.g., "Tom likes
Nike").Inparticular,theychangeapersonnameorabrandname
in a sentence and check whether the translation differs by more
thanonetoken.Thus,MT4MTcannotreporterrorsfrommostreal-
world sentences, such asthe data set used in our paper. In addition,
MT4MT doesnot proposegeneral techniquesto realizetheir idea.
Our work introduces an effective realization via nontrivial tech-
niques(e.g.,adaptingBERTforwordsubstitutionandleveraging
language parsers for generating sentence structures), and conducts
an extensive evaluation.
6.3 MachineTranslation
Thepastfewyearshavewitnessedrapidgrowthforneuralmachine
translation (NMT) architectures [ 32,86]. Typically, an NMT model
usesanencoder-decoderframeworkwithattention[ 92].Underthis
framework, researchers have designed various advanced neural
network architectures, ranging from recurrent neural networks
(RNN) [52,76], convolutional neural networks (CNN) [ 27,28], to
full attention networks without recurrence or convolution [ 83].
These existing papers aim at improving the capability of NMT
models. Different from them, this paper focuses on the robustness
of NMT models. We believe robustness is as important as accuracy
formachine translationinpractice. Thus,ourproposedapproach
can complementexisting machine translation research.
6.4 MetamorphicTesting
Metamorphic testing is a way of generating test cases based onexisting ones [
12,13,73]. The key idea is to detect violations of
domain-specific metamorphic relations across outputs from multi-
ple runs of the program with different inputs. Metamorphic test-
inghasbeenappliedfortestingvarioustraditionalsoftware,such
as compilers [ 43,47], scientific libraries [ 93], and database sys-
tems [50]. Due to its effectiveness on testing "non-testable" pro-
grams, researchers have also used it to test AI software, such as
statistical classifiers [ 63,87], search engines [ 98], and autonomous
cars [79,95]. In this paper, we introduce structure-invariant test-
ing,anovel,widelyapplicablemetamorphictestingapproach,for
machinetranslationsoftware.
7 CONCLUSION
Wehavepresentedstructure-invarianttesting(SIT),anew,effective
approach for testing machine translation software. The distinctbenefits of SIT are its simplicity and generality, and thus wideapplicability. SIT has been applied to test Google Translate and
BingMicrosoftTranslators,andsuccessfullyfound64and70buggyissueswith69.5%and70%top-1accuracy,respectively.Moreover,asageneralmethodology,SITcanuncoverdiversekindsoftranslation
errors that cannot be found by state-of-the-art approaches. We
believethatthisworkistheimportant,firststeptowardsystematic
testingofmachinetranslationsoftware.Forfuturework,wewill
continue refining the general approach and extend it to other AIsoftware(e.g.,figurecaptioningtoolsandfacerecognitionsystems).
We will also launch an extensive effort to help continuously test
and improve widely-used translation systems.
ACKNOWLEDGMENTS
We would like to thank the anonymous ICSE reviewers for their
valuable feedback on the earlier draft of this paper. In addition, the
tool implementation benefited tremendously from Stanford NLP
Groupâ€™slanguageparsers[ 31]andHuggingFaceâ€™sBERTimplemen-
tation in PyTorch [24].
REFERENCES
[1]Uri Alon, Meital Zilberstein, Omer Levy, and Eran Yahav. 2019. code2vec:
Learning Distributed Representations of Code. Proceedings of the ACM on
Programming Languages POPL(2019).
[2]Moustafa Alzantot, Yash Sharma, Ahmed Elgohary, Bo-Jhang Ho, Mani Sri-
vastava,andKai-WeiChang.2018. GeneratingNaturalLanguageAdversarial
Examples.In Proceedingsofthe2018ConferenceonEmpiricalMethodsinNatural
LanguageProcessing (EMNLP).
[3]AnishAthalye,NicholasCarlini,andDavidWagner.2018. ObfuscatedGradients
GiveaFalseSenseofSecurity:CircumventingDefensestoAdversarialExamples.InProceedings of the 35th International Conference on Machine Learning (ICML).
[4]YonatanBelinkovandYonatanBisk.2018. SyntheticandNaturalNoiseBoth
Break Neural Machine Translation. In Proceedings of the 6th International Con-
ferenceon Learning Representations (ICLR).
[5] Bing.[n.d.]. BingMicrosoftTranslator. https://www.bing.com/translator
[6]Nicholas Carlini, Pratyush Mishra, Tavish Vaidya, Yuankai Zhang, Micah Sherr,
ClayShields,DavidWagner,andWenchaoZhou.2016.HiddenVoiceCommands.
InProceedings of the 25th USENIX Security Symposium (USENIX Security).
[7]Nicholas Carlini and David Wagner. 2017. Towards Evaluating the Robustness
of Neural Networks. In IEEESymposiumon Security and Privacy .
[8]Isaac Caswell, Onkur Sen, and Allen Nie. 2015. Exploring adversarial learning
on neural network models for text classification. (2015).
[9] Danqi Chen. 2018. Neural ReadingComprehension and Beyond . Ph.D. Disserta-
tion.Stanford University.
[10]Danqi Chen, Jason Bolton, and Christopher D. Manning. 2016. A Thorough
Examination of the CNN/Daily Mail Reading Comprehension Task. In Proceed-
ings of the 54th Annual Meeting of the Association for Computational Linguistics
(ACL).
[11]Danqi Chen and Christopher Manning. 2014. A Fast and Accurate Dependency
ParserusingNeuralNetworks.In Proceedingsofthe2014ConferenceonEmpirical
Methods in Natural Language Processing (EMNLP).
[12]TsongY.Chen,ShingC.Cheung,andShiuMingYiu.1998. Metamorphictesting:
anewapproachforgeneratingnexttestcases. TechnicalReport.TechnicalReport
HKUST-CS98-01,DepartmentofComputerScience,HongKongUniversityof
ScienceandTechnology, Hong Kong.
[13]TsongYuehChen,Fei-ChingKuo,HuaiLiu,Pak-LokPoon,DaveTowey,T.H.
Tse,andZhiQuanZhou.2018. MetamorphicTesting:AReviewofChallenges
andOpportunities. ACM Computing Surveys (CSUR) 51 (2018). Issue 1.
[14] N. Chomsky. 1957. SyntacticStructures. Mouton, The Hague.
[15]Chenhui Chu, Raj Dabre, and Sadao Kurohashi. 2017. An empirical compari-son of simple domain adaptation methods for neural machine translation. InProceedings of the 55th Annual Meeting of the Association for Computational
Linguistics(ACL).
[16] CWMT. [n.d.]. CWMT Datasets. http://nlp.nju.edu.cn/cwmt-wmt/
[17]Gareth Davies. 2017. Palestinian man is arrested by police after posting â€™Good
morningâ€™ in Arabic on Facebook which was wrongly translated as â€™attack
themâ€™. https://www.dailymail.co.uk/news/article-5005489/Good-morning-
Facebook-post-leads-arrest-Palestinian.html
[18]Marie-Catherine de Marneffe, Timothy Dozat, Natalia Silveira, Katri Haverinen,
Filip Ginter, Joakim Nivre, and Christopher D. Manning. [n.d.]. Universal
Stanford dependencies: A cross-linguistic typology. In Proceedings of the Ninth
InternationalConferenceon Language Resources and Evaluation (LREC) .
[19]JacobDevlin,Ming-WeiChang,KentonLee,andKristinaToutanova.2018. Bert:
Pre-training of Deep Bidirectional Transformers for Language Understanding.
arXivpreprint arXiv:1810.04805 (2018).
[20]TianyuDu,ShoulingJi,JinfengLi,QinchenGu,TingWang,andRaheemBeyah.
2019. SirenAttack: Generating Adversarial Audio for End-to-End Acoustic
Systems. arXivpreprint arXiv:1901.07846 (2019).
[21]Xiaoning Du, Xiaofei Xie, Yi Li, Lei Ma, Yang Liu, and Jianjun Zhao. 2019.DeepStellar: Model-Based Quantitative Analysis of Stateful Deep Learning
Systems. In Proceedings of the 27th ACM Joint Meeting on European Software En-
gineering Conference and Symposium on the Foundations of Software Engineering
ICSE â€™20, May 23â€“29, 2020, Seoul, Republic of Korea Pinjia He, Clara Meister, and Zhendong Su
(ESEC/FSE).
[22]Javid Ebrahimi, Daniel Lowd, and Dejing Dou. 2018. On Adversarial Examples
for Character-Level Neural Machine Translation. In Proceedings of the 27th
InternationalConferenceon Computational Linguistics (COLING) .
[23]Ivan Evtimov, Kevin Eykholt, Earlence Fernandes, Tadayoshi Kohno, Bo Li,
Atul Prakash, Amir Rahmati, and Dawn Song. 2018. Robust Physical-World
AttacksonDeepLearningModels.In Proceedingsofthe2018IEEEConferenceon
ComputerVisionandPatternRecognition (CVPR) .
[24]Hugging Face. [n.d.]. Transformers: State-of-the-art Natural Language Pro-
cessing for TensorFlow 2.0 and PyTorch. https://github.com/huggingface/
transformers
[25]Facebook. 2019. How do I translate a post or comment written in another
language? https://www.facebook.com/help/509936952489634?helpref=faq_
content
[26]AlessioGambi,MarcMueller,andGordonFraser.2019. Automaticallytesting
self-driving cars withsearch-based procedural content generation. In Proceed-
ings of the 28th ACM SIGSOFT International Symposium on Software Testing and
Analysis (ISSTA).
[27]Jonas Gehring, Michael Auli, David Grangier, and Yann N. Dauphin. 2017. A
ConvolutionalEncoderModelforNeuralMachineTranslation.In Proceedingsof
the 55th Annual Meeting of the Association for Computational Linguistics (ACL).
[28]JonasGehring,MichaelAuli,DavidGrangier,andYannN.Dauphin.2017.Convo-
lutional Sequence to Sequence Learning. In Proceedings of the 34th International
Conferenceon Machine Learning (ICML).
[29]Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. 2015. Explainingand Harnessing Adversarial Examples. Proceedings of the 3rd International
Conferenceon Learning Representations (ICLR).
[30] Google. [n.d.]. Google Translate. https://translate.google.com/
[31]Stanford NLP Group. [n.d.]. Stanford CoreNLP Ã¢Ä‚Åž Natural language software.
https://stanfordnlp.github.io/CoreNLP/
[32]Hany Hassan, Anthony Aue, Chang Chen, Vishal Chowdhary, Jonathan Clark,
Christian Federmann, Xuedong Huang, Marcin Junczys-Dowmunt, William
Lewis, Mu Li, et al .2018. Achieving Human Parity on Automatic Chinese to
EnglishNews Translation. arXivpreprint arXiv:1803.05567 (2018).
[33]JensHenriksson,ChristianBerger,MarkusBorg,LarsTornberg,CristoferEn-
glund,SankarRamanSathyamoorthy,andStigUrsing.2019. TowardsStructured
EvaluationofDeepNeuralNetworkSupervisors.In 2019IEEEInternationalCon-
ferenceOn Artificial Intelligence Testing (AITest).
[34]Jiuan-AnHsu.2014. ErrorClassificationofMachineTranslationACorpus-based
Study on Chinese-English Patent Translation. Translation Studies Quarterly
(2014).
[35]Srinivasan Iyer, Ioannis Konstas, Alvin Cheung, and Luke Zettlemoyer. 2016.
SummarizingSourceCodeusingaNeuralAttentionModel.In Proceedingsof
the 54th Annual Meeting of the Association for Computational Linguistics (ACL).
[36]Mohit Iyyer, John Wieting, Kevin Gimpel, and Luke Zettlemoyer. 2018. Adver-
sarial Example Generation with Syntactically Controlled Paraphrase Networks.
InProceedingsofthe16thAnnualConferenceoftheNorthAmericanChapterof
the Association for Computational Linguistics: Human Language Technologies
(NAACL-HLT) .
[37]Robin Jia and Percy Liang. 2017. Adversarial Examples for Evaluating Reading
Comprehension Systems. In Proceedings of the 2017 Conference on Empirical
Methods in Natural Language Processing (EMNLP).
[38]HariniKannan,AlexeyKurakin,andIanGoodfellow.2018. AdversarialLogit
Pairing.arXivpreprint arXiv:1803.06373 (2018).
[39]Jinhan Kim, Robert Feldt, and Shin Yoo. 2019. Guiding Deep Learning Sys-tem Testing using Surprise Adequacy. In Proceedings of the 41st International
Conferenceon Software Engineering (ICSE).
[40]Diederik P. Kingma and Jimmy Ba. 2014. Adam: A Method for Stochastic
Optimization. http://arxiv.org/abs/1412.6980
[41]MattJ.Kusner,YuSun,NicholasI.Kolkin,andKilianQ.Weinberger.2015. From
WordEmbeddingstoDocumentDistances.In Proceedingsofthe32ndInterna-
tionalConferenceon InternationalConferenceonMachineLearning -Volume37
(ICMLâ€™15). 957â€“966.
[42]Fred.Lambert.2016. UnderstandingthefatalTeslaaccidentonAutopilotand
theNHTSAprobe. https://electrek.co/2016/07/01/understanding-fatal-tesla-
accident-autopilot-nhtsa-probe/
[43]Vu Le, Mehrdad Afshari, and Zhendong Su. 2014. Compiler Validation viaEquivalence Modulo Inputs. In ACM SIGPLAN Conference on Programming
LanguageDesignandImplementation(PLDI) .
[44]Vladimir I. Levenshtein. 1966. Binary codes capable of correcting deletions,
insertions,andreversals. Soviet Physics Doklady 10 (1966), 707â€“710. Issue 8.
[45]SamLevin.2018. Teslafatalcrash:â€™autopilotâ€™modespedupcarbeforedriver
killed, report finds. https://www.theguardian.com/technology/2018/jun/07/
tesla-fatal-crash-silicon-valley-autopilot-mode-report
[46]Jinfeng Li, Shouling Ji, Tianyu Du, Bo Li, and Ting Wang. 2019. TextBugger:
Generating Adversarial Text Against Real-world Applications. In Proceedings of
the26thAnnual Network and Distributed System Security Symposium (NDSS).[47] ChristopherLidbury,Andrei Lascu,NathanChong, andAlastairF. Donaldson.
2015. Many-Core Compiler Fuzzing. In ACM SIGPLAN Conference on Program-
mingLanguageDesignand Implementation(PLDI) .
[48]Chin-Yew Lin. 2004. Rouge: A Package for Automatic Evaluation of Summaries.
Text Summarization Branches Out (2004).
[49]JiLin,ChuangGan,andSongHan.2019. DefensiveQuantization:WhenEffi-
ciencyMeetsRobustness. In Proceedingsofthe7th InternationalConferenceon
LearningRepresentations(ICLR) .
[50]MikaelLindvall,DharmalingamGanesan,RagnarÃƒÄ„rdal,andRobertE.Wie-
gand. 2015. Metamorphic Model-based Testing Applied on NASA DAT-an
experiencereport.In Proceedingsofthe37thInternationalConferenceonSoftware
Engineering(ICSE).
[51]Minh-Thang Luong, Hieu Pham, and Christopher D. Manning. 2015. Ef-
fective Approaches to Attention-based Neural Machine Translation. CoRR
abs/1508.04025(2015). arXiv:1508.04025 http://arxiv.org/abs/1508.04025
[52]Thang Luong, Hieu Pham, and Christopher D. Manning. 2015. Effective Ap-
proaches to Attention-based Neural Machine Translation. In Proceedings of the
2015ConferenceonEmpiricalMethodsinNaturalLanguageProcessing(EMNLP) .
[53]LeiMa,FelixJuefei-Xu,FuyuanZhang,JiyuanSun,MinhuiXue,BoLi,Chunyang
Chen,TingSu,LiLi,YangLiu,JianjunZhao,andWangYadong.2018.Deepgauge:Multi-GranularityTestingCriteriaforDeepLearningSystems.In Proceedingsof
the 33rd ACM/IEEE International Conference on Automated Software Engineering
(ASE).
[54]LeiMa,FuyuanZhang,JiyuanSun,MinhuiXue,BoLi,FelixJuefei-Xu,Chao
Xie,LiLi,YangLiu,JianjunZhao,etal .2018. Deepmutation:Mutationtesting
of deep learning systems. In Proceedings of the 29th International Symposium on
Software Reliability Engineering (ISSRE).
[55]Shiqing Ma,YingqiLiu,Wen-ChuanLee, XiangyuZhang, and AnanthGrama.
2018. MODE: Automated Neural Network Model Debugging via State Differen-
tialAnalysisand InputSelection.In Proceedings ofthe 26thACM JointMeeting
onEuropeanSoftwareEngineeringConferenceandSymposiumontheFoundations
of Software Engineering (ESEC/FSE).
[56]ShiqingMa,YingqiLiu,GuanhongTao,Wen-ChuanLee,andXiangyuZhang.
2019. NIC: Detecting Adversarial Samples with Neural Network InvariantChecking. In Proceedings of the 26th Annual Network and Distributed System
Security Symposium (NDSS).
[57]FionaMacdonald.2015. TheGreatestMistranslationsEver. http://www.bbc.
com/culture/story/20150202-the-greatest-mistranslations-ever
[58]Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras,
and Adrian Vladu. 2018. Towards Deep Learning Models Resistant to Adver-sarial Attacks. In Proceedings of the 6th International Conference on Learning
Representations(ICLR) .
[59]Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. Efficient
Estimationof Word Representations in Vector Space. In ICLRWorkshops.
[60]Tomas Mikolov, Ilya Sutskever,Kai Chen, GregS Corrado,and Jeff Dean.2013.
Distributed Representations of Words and Phrases and theirCompositionality.
InProceedingsofthe29thConferenceonNeuralInformationProcessingSystems
(NeurIPS).
[61]TakeruMiyato,AndrewMDai,andIanGoodfellow.2017. AdversarialTrain-
ing Methods forSemi-Supervised Text Classification. In Proceedings ofthe 5th
InternationalConferenceon Learning Representations (ICLR) .
[62]Pramod K. Mudrakarta, Ankur Taly, Mukund Sundararajan, and Kedar Dhamd-
here. 2018. Did the Model Understand the Question?. In Proceedings of the 56th
Annual Meeting of the Association for Computational Linguistics (ACL).
[63]Christian Murphy, Gail E. Kaiser, Lifeng Hu, and Leon Wu. 2008. Properties of
Machine Learning Applications for Use in Metamorphic Testing. In Proceedings
of the 20th International Conference on Software Engineering and Knowledge
Engineering(SEKE).
[64]Arika Okrent. 2016. 9 Little Translation Mistakes That Caused Big Prob-
lems. http://mentalfloss.com/article/48795/9-little-translation-mistakes-
caused-big-problems
[65]Thuy Ong. 2017. Facebook apologizes after wrong translation sees
Palestinian man arrested for posting â€™good morningâ€™. https:
//www.theverge.com/us-world/2017/10/24/16533496/facebook-apology-
wrong-translation-palestinian-arrested-post-good-morning
[66]NicolasPapernot,PatrickMcDaniel,XiWu,SomeshJha,andAnanthramSwami.
2016. DistillationasaDefensetoAdversarialPerturbationsagainstDeepNeural
Networks.In IEEESymposiumon Security and Privacy.
[67]Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. BLEU: A
Method for Automatic Evaluation of Machine Translation. In Proceedings of the
40thAnnual Meeting on Association for Computational Linguistics (ACL).
[68]Kexin Pei, Yinzhi Cao, Junfeng Yang, and Suman Jana. 2017. Deepxplore: Auto-
matedWhiteboxTestingofDeepLearningSystems.In Proceedingsofthe26th
Symposium on Operating Systems Principles (SOSP).
[69]HungVietPham,ThibaudLutellier,WeizhenQi,andLinTan.2019. CRADLE:
cross-backendvalidationtodetectandlocalizebugsindeeplearninglibraries.In
Proceedings of the 41st International Conference on Software Engineering (ICSE).
[70]Michael Pradel and Koushik Sen. 2018. DeepBugs: A Learning Approach to
Name-basedBugDetection. ProceedingsoftheACMonProgrammingLanguages
Structure-Invariant Testing for Machine Translation ICSE â€™20, May 23â€“29, 2020, Seoul, Republic of Korea
OOPSLA(2018).
[71]YaoQin,NicholasCarlini,IanGoodfellow,GarrisonCottrell,andColinRaffel.
2018. Imperceptible, Robust, and Targeted Adversarial Examples for Auto-
maticSpeec hRecognition.In Proceedingsofthe35thInternationalConferenceon
MachineLearning(ICML) .
[72]Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. 2018. Semantically
Equivalent Adversarial Rules for Debugging NLP Models. In Proceedings of the
56thAnnual Meeting of the Association for Computational Linguistics (ACL).
[73]Sergio Segura, Gordon Fraser, Ana B. Sanchez, and Antonio Ruiz-CortÃƒÄ¾s. 2016.
ASurvey onMetamorphic Testing. IEEETransactions onSoftware Engineering
(TSE)42 (2016). Issue 9.
[74]Rico Sennrich, Barry Haddow, and Alexandra Birch. 2016. Improving neural
machinetranslationmodelswithmonolingualdata.In Proceedingsofthe54th
Annual Meeting of the Association for Computational Linguistics (ACL).
[75]YikangShen,ShawnTab,AlessandroSordoni,andAaronCourville.2019. Or-
dered Neurons: Integrating Tree Structures into Recurrent Neural Networks.
InProceedingsofthe7thInternationalConferenceonLearningRepresentations
(ICLR).
[76]Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014. Sequence to Sequence
Learningwith NeuralNetworks.In Proceedings ofthe30th ConferenceonNeural
InformationProcessing Systems (NeurIPS).
[77]Guanhong Tao, Shiqing Ma, Yingqi Liu, and Xiangyu Zhang. 2018. AttacksMeetInterpretability:Attribute-steeredDetectionofAdversarialSamples.InProceedings of the 34th Conference on Neural Information Processing Systems
(NeurIPS).
[78]WilsonL.Taylor.1953."ClozeProcedure":ANewToolforMeasuringReadability.
JournalismBulletin 30,4 (1953), 415â€“433.
[79]YuchiTian,KexinPei,SumanJana,andBaishakhiRay.2018. Deeptest:Auto-
mated Testing of Deep-Neural-Network-Driven Autonomous Cars. In Proceed-
ingsof the 40th International Conference on Software Engineering (ICSE).
[80]Tree. 2013. Adventures in Mistranslation: HSBCâ€™s Call to "Do Nothing". https:
//contentequalsmoney.com/mistranslation-hsbcs-call-to-do-nothing/
[81]Barak Turovsky. 2016. Ten years of Google Translate. https://blog.google/
products/translate/ten-years-of-google-translate/
[82]Twitter. 2019. About Tweet translation. https://help.twitter.com/en/using-
twitter/translate-tweets
[83]Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
Aidan N. Gomez, and Illia Kaiser, Lukasz abd Polosukhin. 2017. Attention is All
youNeed.In Proceedingsofthe33rdConferenceonNeuralInformationProcessing
Systems(NeurIPS).
[84]JingyiWang,GuoliangDong,JunSun,XinyuWang,andPeixinZhang.2019.
Adversarial Sample Detection for Deep Neural Network through Model Mu-
tation Testing. In Proceedings of the 41st International Conference on Software
Engineering(ICSE).
[85] WMT. [n.d.]. WMT Datasets. http://statmt.org/wmt17/translation-task.html
[86]YonghuiWu,MikeSchuster,ZhifengChen,QuocVLe,MohammadNorouzi,
Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al .2016. Googleâ€™s Neural Machine Translation System: Bridging the Gap Between
HumanandMachineTranslation. arXivpreprint arXiv:1609.08144 (2016).
[87]Xiaoyuan Xie, Joshua WK Ho, Christian Murphy, Gail Kaiser, Baowen Xu, and
TsongYuehChen.2011. TestingandValidatingMachineLearningClassifiersby
MetamorphicTesting. Journalof Systems and Software (JSS) 84 (2011). Issue 4.
[88]XiaofeiXie,LeiMa,FelixJuefei-Xu,MinhuiXue,HongxuChen,YangLiu,Jianjun
Zhao,BoLi,JianxiongYin,andSimonSee.2019. DeepHunter:acoverage-guided
fuzztestingframeworkfordeepneuralnetworks.In Proceedingsofthe28thACM
SIGSOFTInternationalSymposiumon Software Testing and Analysis (ISSTA) .
[89]Chong Xiong, Charles R. Qi, and Bo Li. 2019. Generating 3D Adversarial Point
Clouds. In Proceedings of the 2019 IEEE Conference on Computer Vision and
PatternRecognition (CVPR).
[90]Weilin Xu, David Evans, and Yanjun Qi. 2018. Feature Squeezing: Detecting
Adversarial Examples in Deep Neural Networks. In Proceedings of the 25th
Annual Network and Distributed System Security Symposium (NDSS).
[91]DaweiYang,ChaoweiXiao,BoLi,JiaDeng,andMingyanLiu.2019. Realistic
Adversarial Examples in 3D Meshes. In Proceedings of the 2019 IEEE Conference
on Computer Vision and Pattern Recognition (CVPR).
[92]BiaoZhang,DeyiXiong,andJinsongSu.2018. AcceleratingNeuralTransformer
via an Average Attention Network. In Proceedings of the 56th Annual Meeting of
theAssociation for Computational Linguistics (ACL).
[93]Jie Zhang, Junjie Chen, Dan Hao, Yingfei Xiong, Bing Xie, Lu Zhang, and Hong
Mei. 2014. Search-Based Inference of Polynomial Metamorphic Relations. In
Proceedingsofthe29thACM/IEEEInternationalConferenceonAutomatedSoftware
Engineering(ASE).
[94]Jie M. Zhang, Mark Harman, Lei Ma, and Yang Liu. 2019. Machine Learning
Testing: Survey, Landscapes and Horizons. arXiv preprint arXiv:1906.10742
(2019).
[95]MengshiZhang,YuqunZhang,LingmingZhang,CongLiu,andSarfrazKhurshid.
2018. Deeproad:Gan-BasedMetamorphicAutonomousDrivingSystemTesting.
InProceedings of the 33rd ACM/IEEE International Conference on Automated
Software Engineering (ASE).
[96]Wujie Zheng, Wenyu Wang, Dian Liu, Changrong Zhang, Qinsong Zeng, Yue-
tangDeng, WeiYang, PinjiaHe,and TaoXie.2018. TestingUntestableNeural
MachineTranslation:AnIndustrialCase. arXivpreprintarXiv:1807.02340 (2018).
[97]Zhi Quan Zhou and Liqun Sun. 2018. Metamorphic Testing for Machine Trans-
lations: MT4MT. In Proceedings of the 25th Australasian Software Engineering
Conference(ASWEC).
[98]Zhi Quan Zhou, Shaowen Xiang, and Tsong Yueh Chen. 2016. Metamorphic
Testing for Software Quality Assessment: A Study of Search Engines. IEEE
Transactions on Software Engineering (TSE) 42 (2016). Issue 3.
[99]Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang, and Jingbo Zhu. 2013.
FastandAccurateShift-ReduceConstituentParsing.In Proceedingsofthe51st
Annual Meeting of the Association for Computational Linguistics (Volume 1: Long
Papers). Association for Computational Linguistics, 434â€“443.
[100]Chris. Ziegler. 2016. A Google self-driving car caused a crash for the first
time. https://www.theverge.com/2016/2/29/11134344/google-self-driving-car-
crash-report
