Log Parsing with Generalization Ability under New Log Types
Siyu Yu
School of Computer and Electronic
Information, Guangxi University
ChinaYifan Wu
Peking University
ChinaZhijing Li
School of Data Science, The Chinese
University of Hong Kong, Shenzhen
(CUHK-Shenzhen)
China
Pinjia He
School of Data Science, The Chinese
University of Hong Kong, Shenzhen
(CUHK-Shenzhen)
ChinaNingjiang Chenâˆ—
School of Computer and Electronic
Information, Guangxi University
ChinaChangjian Liu
School of Computer and Electronic
Information, Guangxi University
China
ABSTRACT
Log parsing, which converts semi-structured logs into structured
logs, is the first step for automated log analysis. Existing parsers are
still unsatisfactory in real-world systems due to new log types in
new-coming logs. In practice, available logs collected during system
runtime often do not contain all the possible log types of a system
because log types related to infrequently activated system states are
unlikely to be recorded and new log types are frequently introduced
with system updates. Meanwhile, most existing parsers require
preprocessing to extract variables in advance, but preprocessing
is based on the operatorâ€™s prior knowledge of available logs and
therefore may not work well on new log types. In addition, parser
parameters set based on available logs are difficult to generalize to
new log types. To support new log types, we propose a variable
generation imitation strategy to craft a novel log parsing approach
with generalization ability, called Log3T. Log3T employs a pre-
trained transformer encoder-based model to extract log templates
and can update parameters at parsing time to adapt to new log
types by a modified test-time training. Experimental results on 16
benchmark datasets show that Log3T outperforms the state-of-the-
art parsers in terms of parsing accuracy. In addition, Log3T can
automatically adapt to new log types in new-coming logs.
CCS CONCEPTS
â€¢Computing methodologies â†’Neural networks ;â€¢Software
and its engineering â†’Software maintenance tools .
KEYWORDS
log parsing, self supervised, test-time training, generalization
âˆ—Ningjiang Chen is the corresponding author of this work. E-mail: chnj@gxu.edu.cn
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
ESEC/FSE â€™23, December 3â€“9, 2023, San Francisco, CA, USA
Â©2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-0327-0/23/12. . . $15.00
https://doi.org/10.1145/3611643.3616355ACM Reference Format:
Siyu Yu, Yifan Wu, Zhijing Li, Pinjia He, Ningjiang Chen, and Changjian
Liu. 2023. Log Parsing with Generalization Ability under New Log Types. In
Proceedings of the 31st ACM Joint European Software Engineering Conference
and Symposium on the Foundations of Software Engineering (ESEC/FSE â€™23),
December 3â€“9, 2023, San Francisco, CA, USA. ACM, New York, NY, USA,
13 pages. https://doi.org/10.1145/3611643.3616355
1 INTRODUCTION
Nowadays, with the proliferation of continuously running systems,
such as cloud service systems, users are provided with a wide range
of software services. Despite considerable efforts dedicated to ensur-
ing system reliability, these systems remain vulnerable to failures.
In order to effectively monitor and analyze system performance,
logs play a crucial role as they provide valuable insights into system
behavior, error messages, and operational activities [ 18]. However,
the huge log volume of such systems brings challenges to manual
reading. For instance, Alibaba Inc. can generate about 2 million
logs per hour [ 30]. To cope with the huge log volume, a lot of work
has gone into automated log analysis to help operators diagnose
failures [ 18], such as log-based anomaly detection [ 9,20,34,49]
and failure prediction [ 11]. Log parsing serves as a fundamental
step in automated log analysis, as it transforms semi-structured
logs into structured ones.
Log parsers parse logs into log templates by using wildcards
"<*>â€ to replace predicted variables and keep predicted constants
[50]. For instance, log â€œ connection from 24.54.76.216 â€ will
be parsed into log template â€œ connection from <*> â€ because
â€œ24.54.76.216 â€ is predicted to be generated according to the sys-
tem context. Techniques in existing log parsers can roughly be
categorized into heuristic [ 17], clustering [ 31,37], frequent item
extraction [ 5,32] and deep learning [ 29,33]. For example, Drain
[17] uses a fixed-depth tree to group logs with the same first n
words. Logram [ 5] uses n-gram dictionary to convert each word in
the log into 2-gram, 3-gram and then classifies words according to
the frequency of these items.
Although the superiority of existing log parsers has been illus-
trated in their papers, the performance is far from satisfactory when
applying them in practice, which is mainly caused by new log types
in new-coming logs, i.e., new patterns of existing log templates or
new log templates. In the real world, the systemâ€™s available logs usu-
ally do not contain all log types of the system because many system
425
ESEC/FSE â€™23, December 3â€“9, 2023, San Francisco, CA, USA Siyu Yu, Yifan Wu, Zhijing Li, Pinjia He, Ningjiang Chen, and Changjian Liu.
Figure 1: Continuous emergence of new log types within 7
days of Mac
states are infrequently activated and evolving systems frequently
update logging statements to bring new log types to the log parser
[24]. As shown in Figure 1, an average of 46.7 new log types ap-
pear in Mac datasetâ€™s logs every day. We summarize the challenges
posed by new log types as follows: 1) Invalid prior knowledge.
To avoid parsing errors and improve efficiency, most log parsers
need regular expression filters (abbreviated as regex filter in the
following), which are designed by operators according to their prior
knowledge of the available logs of the system to convert some vari-
ables into " <*>" in the preprocessing stage [ 5,33,50]. However,
these regex filters designed on the prior knowledge of available
logs cannot avoid new parsing errors caused by new log types.
Consequently, log parsers would become ineffective and inefficient
when new log types appear. 2) Labor-intensive model tuning.
For unsupervised approaches (e.g., heuristic-based AEL [ 21] and
frequent item extraction-based logram [5]), the model parameters
(e.g., frequency threshold) need to be continuously adjusted to adapt
to new log types, otherwise the parsing accuracy will drop a lot.
For example, we applied AELâ€™s best-performing parameters in the
first half of the BGL benchmark dataset to the second half, and
the resulting accuracy is 31.6% lower than the best. For supervised
approaches (e.g., Uniparser [ 29]), new log types may be generated
by log statements with completely different styles in untriggered
system components, so they may not be in the same distribution
as historical logs (i.e., training set). These new log types require
laborious labeling work to retrain the model to generalize. 3) Viola-
tion of underlying assumptions. Some log parsers do not work
well enough when some new log types arrived due to the violation
of their underlying assumptions. For example, Drain, Drain+ [ 13],
and SPINE [ 43] consider logs with the same first nwords should
belong to the same template. However, logs starting with variables
are common in practice, such as â€œ cupsd startup succeeded â€ and
â€œsm-client startup succeeded â€ from Linux logs. In addition,
Drain, LenMa [ 37] and AEL assume logs belonging to the same
template should have the same length. However, there are various
lengths in logs belonging to the same template, like â€œ en0: support-
-ed channels: 1 2 â€ and â€œ en0: supported channels: 36 40 44 â€
from Mac. These assumptions neglect the existence of two log types,
i.e., "logs starting with variables" and "various lengths in logs be-
longing to the same template".
To address these challenges, we incorporate the idea of test-
time training to propose a novel log parsing approach supportingnew log types, called Log3T. Log3T mainly consists of offline pre-
training and online parsing. During offline pre-training, we pre-
train a model with labeled historical logs. During online parsing,
Log3T extracts the few words that are most likely to be constants.
Subsequently, the logs are partitioned into log groups based on
these words. Whenever a new log is added to a log group, its repre-
sentative template is updated accordingly. In addition, we applied
the idea of test-time training [ 35,38] to online parsing, which can
enable model parameters to be automatically updated in the online
parsing stage.
To evaluate the performance of log3T, we conducted compre-
hensive experiments on 16 benchmark datasets [ 16,50]. The exper-
imental results demonstrate that Log3T achieves the best average
Grouping Accuracy [ 50] and Edit Distance [ 33] across 16 bench-
mark datasets. Log3T exhibits the best Edit Distance on 12 out of
the 16 benchmark datasets, while its Grouping Accuracy outper-
forms all other log parsers on 7 out of the 16 benchmark datasets.
Compared with the Log3T model trained only on historical logs,
modified test-time training can help the model achieve better pars-
ing accuracy under new log types.
The contribution of this paper can be concluded as follows:
â€¢We propose Log3T, the first log parser that has generalization
ability under new log types without human intervention. Its
core techniques are as follows.
â€“We propose a imitation variable generation strategy to
generate labeled imitated logs to pre-train model to accu-
rately extract variables in different log types requiring no
prior knowledge.
â€“We apply modified test-time training, which can automat-
ically update model parameters to avoid labor-intensive
model tuning when new log types occur.
â€¢Extensive experiments have been conducted on 16 bench-
mark datasets to evaluate Log3T. Compared to existing log
parsers, Log3T achieves the best average GA 0.943 and Edit
Distance 6.061 across all 16 benchmark datasets. The imple-
mentation of Log3T is publicly available for better repro-
ducibility and further research.
The rest of the paper is organized as follows. Section 2 introduces
the preliminaries of this paper. Section 3 describes the details of
Log3T. Section 4 presents our experimental results. We discuss the
limitations and future work in Section 5 and Section 6 introduces
related works of this paper. Finally, Section 7 concludes this paper.
2 PRELIMINARIES
2.1 Log Parsing
As shown in Figure 2, logs are generated by logging statements (e.g.,
logInfo(), print() ) in the source code, and consist of log header
(e.g., time " 17/06/09 20:10:48 ", PID, log level " INFO ") and log con-
tent (e.g., â€œ Block broadcast_0_piece0 stored as bytes in me-
-mory (estimated size 93.0B, free 317.1 KB) â€). Log content
is composed of constant words (e.g., â€œ Block â€) and variable words
(e.g., â€œ 93.0, 317.1 â€). Log parsing parses semi-structured logs into
structured logs by keeping predicted constant words and replacing
predicted variable words with wildcards: â€œ <*>â€, and the parsing re-
sult is called log template, like log template: â€œ Block <*> stored as
bytes in memory (estimated size <*>B, free <*> KB) â€.
426Log Parsing with Generalization Ability under New Log Types ESEC/FSE â€™23, December 3â€“9, 2023, San Francisco, CA, USA
Figure 2: An illustrative example of log parsing
2.2 Tokenization
The preprocessing step of log parsing involves tokenizing the input
log to a set of tokens and then predicting their corresponding clas-
sifications. Most log parsers use delimiters to split log and achieve
tokenization. However, this tokenization methods can cause certain
log parsers to fail when encountering new tokens. For example,
Nulog needs to establish a vocabulary for all the tokens obtained
from historical logs because it needs to vectorize logs using the
index values of the tokens in the vocabulary. When new log types
emerge, there may be new tokens not in the old vocabulary, making
them difficult to be vectorized.
WordPiece. In natural language processing, there is a widely
used tokenization technology WordPiece [ 44], which can improve
the generalization ability of vocabulary. WordPiece is a subtoken-
level tokenization technology introduced by Google, aiming to
address the issue of out-of-vocabulary (OOV) tokens. The basic
idea of WordPiece is to divide the text into smaller meaningful
subtoken units instead of traditional word units. WordPiece can
use approximately 30,000 subtoken units to represent all words. For
example, there is a word â€™ JK2_init â€™ in the log not in the vocabulary,
it can be represented by combining the subtoken units " "J","##k" ,
"##2" ,"_","in" ,"##it" " from the vocabulary.
2.3 Regular Expression Filter
Most log parsers need regular expressions to pre-parse common
variable anderror-prone variable words into wildcards to avoid
parsing errors. (1) Common variables, such as block IDs and IP
addresses. Researchers commonly assume that the patterns of this
category of variables remain consistent. Thus, they can easily set
up regex filters to pre-parse all common variables. For example,
the regular expression "râ€™([\w-]+\.)+[\w-]+(:\d+)?â€™" is used in 2K
Proxifier dataset to match URLs (Uniform Resource Locators) or
domain names (including port numbers), where "([\w-]+.)+ " cap-
tures subdomains or subcomponents of a domain, "[\w-]+" captures
the last component of a domain, and "(:\d+)?" captures the port
number. (2) Error-prone variables. This category of variables is
prone to causing parsing errors in log parsers. Researchers design
regex filters to pre-parse these variables based on parsing errors
caused by them in the historical logs. For example, the regex filter
"râ€™<\d+\ssecâ€™" is used to transform patterns like " <1 sec " into " <*>".
The words " <1 sec " is used to record the duration of a lifecycle.
However, in general, words representing lifecycles are in the formatof "00:01 ", "02:01 ", where there is no space between the characters.
The presence of a space in " <1 sec " leads to varying log lengths
after tokenization, while log parsers like Drain, Brain [ 48], Drain+,
SPINE, AEL, LenMa assume that logs belonging to the same tem-
plate have the same length. Therefore, such a filter can prevent
these log parsers from making errors.
Without these regex filters, the log parser would generate a
substantial number of parsing errors and become ineffective.
For example, after removing the regex filters, the Group Accu-
racy of Spell decreases from 0.789 to 0.619. Therefore, when new
log types appear, the effectiveness of a log parser may decrease if
the regex filters are not updated or adapted to the new log types:
(1) Regex filters designed to filter common variables may
fail to match new patterns of the common variables in new
logs. For example, in the original Proxifier dataset, there are a
substantial number of URLs and domain names that do not con-
form to the regular expression pattern "râ€™([\w-]+\.)+[\w-]+(:\d+)?â€™",
which is set based on 2K sampled dataset, such as " wpad:80 " and
"dshytnh:80 ".(2) The patterns of the words that lead to the
same parsing errors can vary. For example, in the variable posi-
tion of " en0: supported channels: <*> ", different contexts may
generate a different number of variables separated by spaces, such
as "01 03 " and " 36 40 44 ". Regular expression "râ€™<\d+\ssecâ€™" is un-
able to solve this issue of various-length in this case. (3) Different
datasets require significant variations in regex filters. Con-
sidering that an operator typically maintains multiple systems and
components with different log types, it can be laborious to set differ-
ent regex filters for each component and system. For example, the
regex filters used by Drain in Android and Spark are "râ€™(/[ \w-]+)+â€™,
râ€™([\w-]+\.)2,[\w-]+â€™, râ€™\b(\-?\+?\d+)\b|\b0[Xx][a-fA-F\d]+\b|\b[a-fA-
F\d]4,\bâ€™" and "râ€™(\d+\.)3\d+â€™, râ€™\b[KGTM]?B\bâ€™, râ€™([\w-]+\.)2,[\w-]+â€™"
respectively, indicating that they are completely different.
In summary, the superior performance of current log parsers is
largely attributed to the use of regex filters. However, when new
log types appear, regex filters based on historical logs may become
ineffective. Therefore, an effective log parser that does not require
regex filters is needed in the industry.
2.4 Test-Time Training
The challenge of labor-intensive model tuning arises due to the
distribution shift of new log types compared to the available logs
(i.e., training and test data come from different distributions). The
performance of predictive models is often unsatisfactory when
training and test data come from different distributions. A lot of
efforts have been dedicated to solving the problem, such as domain
generalization [ 27,36], unsupervised domain adaptation [ 2,3], and
test-time training [ 38]. The reason for adopting test-time training
in this paper is that it does not require new logs and historical logs
to possess common features. Instead, it utilizes a self-supervised
task to capture the features present in the new logs. We will provide
a concise overview of these concepts as follows:
Domain generalization [27,36] studies the setting where a
meta distribution generates multiple environment distributions,
some of which are available during training (source), while others
are used for testing (target). In log parsing, it is challenging to
determine whether logs from different systems belong to the same
427ESEC/FSE â€™23, December 3â€“9, 2023, San Francisco, CA, USA Siyu Yu, Yifan Wu, Zhijing Li, Pinjia He, Ningjiang Chen, and Changjian Liu.
Figure 3: Standard version of test-time training
Figure 4: Workflow of Log3T
meta distribution, or if newly generated logs resulting from software
changes belong to the same meta distribution as historical logs.
Unsupervised domain adaptation [2,3] studies the problem
of distribution shifts, when an unlabeled dataset from the test dis-
tribution (target domain) is available at training time, in addition
to a labeled dataset from the training distribution (source domain).
However, the limitation of the problem formulation is that general-
ization might only be improved for this specific distribution, which
can be difficult to anticipate in advance. In log parsing, evolving
systems make it hard to anticipate distributions.
Test-time training [38] can provide generalization under distri-
bution shifts using a self-supervised auxiliary task to update model
parameters at test time, where self-supervised learning studies how
to create labels from the data, such as context prediction [ 8], and
rotation prediction [1, 14]. Figure 3 shows the standard version of
test-time training. Specifically, at training time, we jointly optimize
the loss of the main task ( ğ¿ğ‘œğ‘ ğ‘ ğ‘š) and the loss of the self-supervised
auxiliary task ( ğ¿ğ‘œğ‘ ğ‘ ğ‘). At testing time, we will update the shared
parameters Î˜ğ‘ and the parameter Î˜ğ‘of the auxiliary task by op-
timizingğ¿ğ‘œğ‘ ğ‘ ğ‘. For instance, in the case of the main task being
object recognition, when confronted with images belonging to a
new distribution online, the model is updated by restoring rotated
new images. This approach enables the model to capture features
present in the new images, thereby facilitating the generalization
of the main task to these new images.
3 OUR APPROACH
In this section, we introduce the proposed log parser, Log3T in
detail. As shown in Figure 4, Log3T consists of offline pre-training
and online parsing. In the offline pre-training stage, we formulated
Figure 5: An example of tokenization
the log parsing to a binary classification task, utilizing labeled
historical logs to train a Transformer encoder. In the online parsing
stage, we extract the most likely constant words from the new-
coming logs based on the probabilities assigned to the words by
the model. Subsequently, we partition the new-coming logs into
different log groups based on these constant words. Each log group
has its representative log template. To enhance the modelâ€™s ability
to generalize to new log types, we further developed a version of
online parsing that incorporates test-time training. In the following
sections, we will provide a detailed description of Log3T.
3.1 Offline Pre-training
In the offline pre-training stage, historical logs are used to pre-
train a Transformer encoder. We introduce offline pre-training
stage through three main components: 1). Tokenization, 2). Context
features integration, 3). Loss function.
Tokenization. We first split the up-coming logs into separated
words using delimiters, such as comma and space. Then, we use
WordPiece [ 44] to tokenize each word into sub-tokens. As shown
in Figure 5, we design a fixed-length window for each word to pad
sub-tokens. For example, the word â€œ config â€ is tokenized into â€œ conâ€,
â€œ##fi â€ and â€œ ##gâ€, the remaining positions will be filled with â€œ [PAD] â€.
This fixed-length window facilitates the model to identify which
sub-tokens belong to the same word. Before entering the model,
each token needs to be embedded into a vector by an embedding
layer. The embedding layer is a fully connected network and its
dimension is based on a vocabulary.
Context features integration. With the help of context, we
are more likely to accurately identify whether the word is vari-
able. Therefore, in order to integrate context features to identify
whether the word is variable, the model we use needs to be able
to integrate context features well. Compared with RNNs (recur-
sive neural networks) [ 4,10,19] that can only consider adjacent
contexts, Transformer [ 7,41] can integrate context features with-
out restrictions. Thus, we use transformer encoder-based model to
predict the variable word.
Loss function. As a binary classification task (variable or con-
stant), we will feed the output of the model into a sigmod function
and then compute the binary cross-entropy loss. The detailed loss
function is as follows:
ğ¿ğ‘œğ‘ ğ‘ =âˆ’1
ğ‘›ğ‘–âˆ‘ï¸
ğ‘–=1(ğ‘¦ğ‘–lnğ‘¥ğ‘–+(1âˆ’ğ‘¦ğ‘–)ln(1âˆ’ğ‘¥ğ‘–)), (1)
428Log Parsing with Generalization Ability under New Log Types ESEC/FSE â€™23, December 3â€“9, 2023, San Francisco, CA, USA
whereğ‘¦ğ‘–is the label of this sub-token, ğ‘¥ğ‘–is the predicted probability
of this sub-token assigned by the model and ğ‘›is the number of
sub-tokens of this log. If a word is labeled as variable, all of the
wordâ€™s sub-tokensâ€™ labels ğ‘¦ğ‘–will be 1, and if it is not, the label ğ‘¦ğ‘–
will be 0.
3.2 Online Parsing
In the online parsing stage, the model takes the up-coming log
as input and outputs the probability after sigmod layer (referred
to as variable-probability hereafter). Then, a word is regarded as
a constant if its variable-probability is lower than a predefined
probability boundary. However, establishing a probability boundary
to extract all the constant words in the log is challenging, as this
probability boundary may vary across different logs. Therefore, this
paper adopts a partial constant-based approach for online parsing,
which can only utilize partial constants to accurately group logs.
Each log group has a representative log template, which will be
checked for updates when new logs are added. Online parsing can
be concluded as three main steps: partial constant words extraction,
online log grouping and log template updating.
Partial constant words extraction. In this step, we will extract
top n words from the log in ascending order of variable-probability.
The value of n is typically set to be between 2 and 5, and it is deter-
mined by operators and related to the average length of available
logs of the system. Figure 6 provides a detailed illustration of an
example for this step. For example, if the parameter n is set to 2,
"ask" and " delete " will be considered as partial constant words of
ğ‘™ğ‘œğ‘”5. In our implementation, if a log contains identical words, these
words will be assigned different tags for distinction. As an example
in Figure 6, " ğ‘¡â„ğ‘’0" represents the first occurrence of the word " the"
inğ¿ğ‘œğ‘” 0. In addition, pure numbers will not be extracted as constant
words. The predicted variable-probability of a word is defined as
the maximum variable-probability value among its sub-tokens. For
example, â€œ 20KB â€ is a word in the log, then WordPiece divides it into
two sub tokens â€œ 20â€ and â€œ KBâ€, the predicted variable-probability of
the â€œ 20â€ is 0.3, and the predicted variable-probability of the â€œ KBâ€
is 0.01. Therefore, the predicted variable-probability of the word
â€œ20KB â€ is 0.3.
Online log partition. In this step, each log will find its log
group based on the extracted partial constant words. Determining
the log group based solely on the presence of these partial constants
is insufficient, as an example in Figure 6, ğ‘™ğ‘œğ‘”1andğ‘™ğ‘œğ‘”2have the
same partial constant words, but they do not belong to the same log
template. Thus, we need to conduct further comparisons between
the input log and additional features of the existing log groups to
accurately identify the appropriate log group. As shown in Figure 7,
four modules: 1) word comparison, 2) order comparison, 3) length
comparison, and 4) consecutive variable detection, are employed to
match existing log groups for the input log.
â€¢Word comparison is used to find which existing log tem-
plates contain these extracted words. As shown in Figure 6,
the constant words extracted from ğ‘™ğ‘œğ‘”2are â€œ source â€ and
â€œdestination â€. Therefore, the matched log template for ğ‘™ğ‘œğ‘”2
should include the words â€œ source â€ and â€œ destination â€.
â€¢Order comparison is used to check if the partial constant
words in the same order as they do in the log template.For instance, in ğ‘™ğ‘œğ‘”1andğ‘™ğ‘œğ‘”2, the same extracted words
â€œsource â€ and â€œ destination â€ have different orders: â€œ source ,
destination â€ inğ‘™ğ‘œğ‘”1and â€œ destination, source â€ inğ‘™ğ‘œğ‘”2.
In log parsing, this module is more suitable compared to po-
sition indexing. It accommodates situations where a single
position yields a varying number of variables. For example,
inğ‘™ğ‘œğ‘”4andğ‘™ğ‘œğ‘”5, the word â€œ delete â€ will have different posi-
tion indexes, but the order of their constant words remains
consistent.
â€¢Length comparison is used to check whether the length of
the input log matches the length of the log template.
â€¢Consecutive word detection is used to check if there are
consecutive variables generated at a single variable posi-
tion. We assume that consecutive variables generated from
a single position have the consistent pattern and use regular
expressions to represent this pattern. " [ğ‘âˆ’ğ‘§ğ´âˆ’ğ‘]" will
be used to replace letter, " âˆ’?\\ğ‘‘âˆ—" will be used to replace
consecutive numbers, and special characters are reserved. If
multiple words are successfully matched, these words will
be considered as consecutive variables at a single variable
position.
If the match is successful, the input log will be added to the
matched log group and proceed to the next step of log template
updating. If multiple templates are matched, the proportion of
tokens that are identical to the input log will be used to determine
the final template. However, if the match fails, the input log will
form a new log group, where the log template will be the log itself.
In addition, we used the example shown in Figure 8 to provide
an easy understanding of each module in the log grouping process.
Figure 8 illustrates the process of finding a appropriate log group
forğ‘™ğ‘œğ‘”5within the existing log groups. The "word comparison"
module initially identifies that only Group2â€™s log template contains
the words " ask" and " delete ". Then, the "order comparison" mod-
ule verifies that the order of these two words in ğ‘™ğ‘œğ‘”5and Group2â€™s
log template is consistent. Next, the "length comparison" module
detects a discrepancy in the length between Group2â€™s log template
andğ‘™ğ‘œğ‘”5. As a result, the "consecutive word detection" module is
utilized to check whether ğ‘™ğ‘œğ‘”5contains consecutive variables gen-
erated from a single position. In this case, there are multiple words
between " ask" and " delete " inğ‘™ğ‘œğ‘”5, letters, special characters, and
numbers of the first word " blk_422 " will be transformed into the
corresponding regular expression " [ğ‘âˆ’ğ‘§ğ´âˆ’ğ‘]{3}âˆ’?\\ğ‘‘âˆ—. Since the
regular expression can match all the remaining words, " blk_422 "
and " blk_354 " can be considered as consecutive variables at a single
position and ğ‘™ğ‘œğ‘”5can be considered as belonging to Group2.
Log template updating. Once a new log is added to the log
group, the log template of that group will be checked for the updates.
In each log group, the consecutive variables generated from a single
variable position are considered as one variable position, ensuring
alignment of all constant words in the columns. Once the addition
of a new log leads to different words appearing in a column, the
corresponding word in the log template will be replaced with the
wildcard " <*>" at the respective position.
We formally present the online parsing algorithm in Algorithm
1. We define a log set as ğ‘†=ğ¿1,ğ¿2,...ğ¿ğ‘š, whereğ¿represents a log
message and ğ‘šrepresents how many log messages this set has.
429ESEC/FSE â€™23, December 3â€“9, 2023, San Francisco, CA, USA Siyu Yu, Yifan Wu, Zhijing Li, Pinjia He, Ningjiang Chen, and Changjian Liu.
Figure 6: Illustration of extracting partial constant words from the log in the log stream.
Figure 7: The workflow for online log grouping
Figure 8: Illustration of how ğ‘™ğ‘œğ‘”5in Figure 6 matches with
the corresponding log group from the existing log groups.
The input of Algorithm 1 is the log ğ¿=[ğ‘Š1,ğ‘Š2,...,ğ‘Šğ‘›]and the
extracted partial constant words ğ‘ƒğ¶=[ğ‘Šğ‘Ÿ1,...,ğ‘Šğ‘Ÿğ‘], whereğ‘Šâˆ—
represents the word in ğ¿,ğ‘›represents how many words ğ¿has,ğ‘
represents how many constant words extracted, ğ‘Ÿğ‘˜âˆˆ[1,ğ‘›]|ğ‘˜âˆˆ
[1,ğ‘]. Algorithm 1 implements the four modules of log partition,
word comparison (line 3-5), order comparison (line 6-10), length
comparison (line 11), consecutive variable detection (line 12-19)
and log template updating (line 28). The time complexity of the
consecutive variable detection module is the highest within the
log partition process, being ğ‘‚(ğ‘šğ‘›), as it requires traversing every
word in the log (line 12). Log template updating involves checking
if there are different words in each aligned column, with a time
complexity of ğ‘‚(ğ‘šğ‘›).Algorithm 1: Online parsing
Input: Logğ¿=[ğ‘Š1,ğ‘Š2,...,ğ‘Š ğ‘›], partial constant words ğ‘ƒğ¶=[ğ‘Šğ‘Ÿ1,...,ğ‘Š ğ‘Ÿğ‘],
existing template ğ¸ğ‘‡=[ğ‘‡1,...,ğ‘‡ ğ‘š]
Output: updatedğ¸ğ‘‡
1ğ‘šğ‘ğ‘¡ğ‘â„ğ‘’ğ‘‘â†[]
2forğ‘‡ğ‘inğ¸ğ‘‡do
3 ifğ‘ƒğ¶not inğ‘‡ğ‘then
4 continue
5 else
6 ğ‘œğ‘Ÿğ‘‘ğ‘’ğ‘Ÿâ†[]
7 forğ‘Šğ‘Ÿğ‘˜inğ‘ƒğ¶do
8 order.append( ğ‘‡ğ‘.index(ğ‘Šğ‘Ÿğ‘˜))
9 end
10 ifğ‘œğ‘Ÿğ‘‘ğ‘’ğ‘Ÿ.ğ‘–ğ‘  _ğ‘ğ‘ ğ‘ğ‘’ğ‘›ğ‘‘ğ‘–ğ‘›ğ‘”()then
// Check whether the position indices stored in the "order"
are in ascending order.
11 iflen(ğ‘‡ğ‘)â‰ len(ğ¿)then
12 ğ‘ğ‘œğ‘ â†Positions of words in " ğ‘‡ğ‘" that are not present in " ğ¿".
13 ğ‘ğ‘œğ‘ ğ‘ğ‘œğ‘›ğ‘ ğ‘’ğ‘ğ‘¢ğ‘¡ğ‘–ğ‘£ğ‘’â†Consecutive values in list "pos".
14 Merge the words with the same pattern that exist at the positions
indexed in â€ğ‘ğ‘œğ‘ ğ‘ğ‘œğ‘›ğ‘ ğ‘’ğ‘ğ‘¢ğ‘¡ğ‘–ğ‘£ğ‘’ â€within log â€ğ¿â€.
15 iflen(ğ‘‡ğ‘)â‰ len(ğ¿)then
16 ğ‘šğ‘ğ‘¡ğ‘â„ğ‘’ğ‘‘. append(ğ‘‡ğ‘)
17 else
18 continue
19 end
20 else
21 ğ‘šğ‘ğ‘¡ğ‘â„ğ‘’ğ‘‘. append(ğ‘‡ğ‘)
22 end
23 else
24 continue
25 end
26 end
27end
28final_templateâ†Template from list " ğ‘šğ‘ğ‘¡ğ‘â„ğ‘’ğ‘‘ " with the highest ratio of identical words
to those in log " ğ¿".
29return final_template.update( ğ¿)
Figure 9: Modified version of test-time training in Log3T
3.3 Online Parsing with Test-Time Training
In this work, we take advantage of test-time training to generalize
under new log types. To design a self-supervised auxiliary task for
log parsing, we proposed an imitation variable generation strategy
to create labeled imitated logs from the new log types. These imi-
tated logs can be used to train the model and enable it to learn the
features specific to the new log types.
430Log Parsing with Generalization Ability under New Log Types ESEC/FSE â€™23, December 3â€“9, 2023, San Francisco, CA, USA
Imitation variable generation strategy. For each logging
statement, the generated constants will not change, while the gener-
ated variables will change according to the system contexts. There-
fore, we consider that the most crucial characteristic of variable
positions, compared to constant positions, is their ability to gener-
ate distinct words. We imitate the variable generation by replacing
randomly selected words in the new logs with historical variable
words. To enrich the historical variable word repository, we create
imitated variables. Specifically, we extract the pattern for each type
of historical variable words as regular expressions to generate imi-
tated variables. "[ğ‘âˆ’ğ‘§ğ´âˆ’ğ‘]âˆ—" will be used to replace consecutive
letters, "âˆ’?\\ğ‘‘âˆ—" will be used to replace consecutive numbers, and
special characters are reserved. Each regular expression generates
a set of randomly words that conform to that regular expression,
which are used as imitated variables. For example, the real vari-
able word "ğ‘ğ‘™ğ‘˜_5646" could generate imitated variable words like
"ğ‘ğ‘‘ğ‘_5951" and "ğ‘™ğ‘œğ‘_6484".
Modified test-time training in Log3T. As shown in Figure 3,
the standard test-time training follows a branching structure where
the main task and the auxiliary task are entirely different. For
instance, in the test-time training framework composed of object
recognition and rotation prediction, during the test time, there is
no labeled test data to train the model for accurate identification
of new images. However, the self-supervised rotation prediction
task can provide the model with labeled data. By training the model
on labeled rotated images, researchers consider that the bottom
features learned by the model from labeled rotated images can
be transferred to the main task. Consequently, the main task and
the auxiliary task share certain bottom parameters. In the online
parsing of Log3T with test-time training, both the main task and
the auxiliary task are binary classification tasks of words in the logs,
where the difference is that the auxiliary task uses imitated logs
instead of real logs. In Log3T, unlike standard test-time training, the
main task and auxiliary task share the same optimization objective,
which is to train the model for accurate word classification. As the
imitated logs possess the crucial characteristics of real logs, we
consider them as labeled real logs. Therefore, as depicted in Figure
9, the model parameters updated using imitated logs can be fully
utilized for online parsing of new logs.
4 EVALUATION
In this section, we evaluate our approach by answering the follow-
ing research questions:
â€¢RQ1: Is the parsing accuracy of Log3T better than existing
log parsers without the help of prior knowledge?
â€¢RQ2: Can test-time training help Log3T generalize under
new log types?
â€¢RQ3: To what extent does each component of Log3T improve
performance in Log3T?
â€¢RQ4: Can the efficiency of Log3T meet the requirements for
log parsing in practice?Table 1: Data statistics and configuration
Dataset Data Size Messages Delimiter Threshold Epoch
HDFS 1.47GB 11,175,629 , 4 1
Apache 4.96MB 56,481 , 3 1
Zookeeper 9.95MB 74,380 :=, 5 1
Mac 16.09MB 117,283 ,[] 8 2
HealthApp 22.44MB 253,395 :=, 4 2
Android 183MB 1,546,686 :=,() 2 1
OpenStack 60.01MB 207,820 , 4 2
BGL 708.76MB 4,747,963 =, 4 1
Proxifier 2.42MB 21,329 :, 4 1
Linux 2.25MB 25,567 =, 1 1
HPC 32.00MB 433,489 =, 3 1
Hadoop 48.61MB 394,308 :=, 5 1
Windows 26.09GB 114,608,388 :=,[] 8 1
Thunderbird 29.60GB 211,212,192 :=, 2 1
Spark 2.75GB 33,236,604 , 3 1
OpenSSH 70.02MB 655,146 , 6 1
4.1 Experimental Setting
Dataset. Our experiments are conducted on the most widely-used
benchmark datasets from LogPAI. The entire dataset includes dis-
tributed system logs (HDFS, Zookeeper, Spark, Hadoop, Open-
Stack), supercomputer logs (HPC, BGL, Thunderbird), operating
system logs (Mac, Linux, Windows), mobile system logs (Android,
HealthApp), server application logs (Apache, OpenSSH) and stan-
dalone software logs (Proxifier), more detail statistics about the
benchmark dataset is available in Table 1. And each benchmark
dataset provides a 2K sampled dataset where logs are labeled with
the log template they belong to as the ground truth. The researchers
[26] corrected some errors in the ground truth, and based on that,
we further corrected some minor errors.
Evaluation metrics. We used Grouping Accuracy (abbreviated
as GA in the following) and Edit Distance (also known as Lev-
enshtein Edit Distance [ 33]) to evaluate the log parserâ€™s parsing
accuracy. For GA, a log is considered correctly parsed if and only if
its log template corresponds to the same group of logs as the ground
truth does. For example, if we parse the normal log sequence [e1,
e2, e2] as [e1, e4, e5], we get GA = 1/3 because the second and
third messages are not grouped. GA can evaluate whether a log
parser can facilitate downstream tasks (e.g., log sequence anomaly
detection) because only logs belonging to the same template are
considered to be the same log key, we can get the real log key se-
quence. For example, incorrect parsing result [e1, e4, e5] will be
considered as an anomaly log sequence because its pattern does not
align with the normal log sequence pattern [e1, e2, e2]. Edit Dis-
tance was proposed by Nedelkoski [ 33] to address the limitations
of GA, which only evaluates whether logs are parsed into the same
template, without considering the distance between the extracted
template and the ground truth. Edit Distance is used to evaluate
the template extraction in terms of string comparison.
Log parser selection for comparative experiments. We con-
ducted comparative experiments by selecting all the online log
parsers from LogPAI, namely Drain, Spell, LenMa, and SHISO,
along with the latest open-source log parsers Logram and Nulog.
(1) Logram generates n-gram dictionaries for all logs, finds high-
frequency n-gram combinations, then determines log variables and
constants. (2) Drain is an online log parser and employs a fixed-
depth tree structure to assist in dividing logs into different groups.
431ESEC/FSE â€™23, December 3â€“9, 2023, San Francisco, CA, USA Siyu Yu, Yifan Wu, Zhijing Li, Pinjia He, Ningjiang Chen, and Changjian Liu.
Table 2: Group Accuracy and Edit Distance of log parsers on 16 benchmark datasets with and without regular expression filters.
( The higher the GA, the better, and the lower the Edit Distance, the better. â†“for worse performance. )
Dataset LenMa SHISO Spell Drain Logram Nulog Log3T
Rex use âœ“ âœ˜ âœ“ âœ˜ âœ“ âœ˜ âœ“ âœ˜ âœ“ âœ˜ âœ“ âœ˜
GA ED GA ED GA ED GA ED GA ED GA ED GA ED GA ED GA ED GA ED GA ED GA ED
HDFS 0.998 2.883 0.860 16.036 0.845 6.197 0.998 0.784 1.000 1.386 0.280 32.826 0.998 0.940 0.998 2.843 0.998 19.366 0.538 6.798 0.998 2.950 1.000 0.202
BGL 0.690 8.399 0.329 8.201 0.346 11.063 0.348 11.027 0.787 7.982 0.426 7.783 0.963 4.973 0.963 4.973 0.673 15.267 0.312 15.065 0.986 3.112 0.985 3.681
HPC 0.830 2.728 0.681 3.076 0.325 6.544 0.3254 6.579 0.654 4.942 0.657 4.192 0.887 1.831 0.741 2.777 0.918 3.514 0.770 3.735 0.937 1.474 0.906 1.194
Apache 1.000 13.552 1.000 13.616 1.000 10.218 1.000 10.234 1.000 10.234 1.000 10.250 1.000 10.218 1.000 10.234 1.000 11.750 1.000 11.750 1.000 4.486 1.000 0.034
HealthApp 0.174 16.519 0.174 16.519 0.397 21.740 0.397 21.740 0.639 8.468 0.639 8.468 0.780 18.476 0.780 28.476 0.281 15.709 0.281 15.709 0.876 11.635 0.999 4.242
Mac 0.698 21.126 0.686 21.936 0.588 19.246 0.695 19.715 0.757 23.663 0.763 23.475 0.787 20.998 0.783 22.274 0.599 25.186 0592 26.649 0.829 21.544 0.931 11.966
Proxifier 0.517 9.046 0.001 49.900 0.527 10.260 0.011 24.151 0.527 12.842 0.511 52.505 0.527 10.138 0.504 29.054 0.504 28.205 0.504 28.205 0.527 8.968 1.000 15.569
Zookeeper 0.841 4.844 0.819 6.227 0.368 10.299 0.368 10.382 0.964 3.188 0.776 4.670 0.967 2.288 0.967 2.314 0.736 5.511 0.586 6.105 0.995 2.239 0.993 1.583
Thund... 0.943 8.928 0.943 9.318 0.576 14.877 0.534 20.379 0.844 15.684 0.844 15.828 0.955 14.632 0.955 15.023 0.554 16.952 0.554 19.910 0.950 14.204 0.968 10.711
Spark 0.884 10.281 0.885 10.828 0.906 2.662 0.906 2.443 0.905 5.465 0.905 5.695 0.92 2.629 0.923 2.468 0.796 11.373 0.796 11.373 0.998 2.876 0.923 3.135
Android 0.834 7.087 0.648 24.900 0.566 13.574 0.231 45.515 0.863 12.574 0.543 32.836 0.831 6.940 0.588 22.976 0.811 18.457 0.444 29.915 0.929 10.934 0.808 18.985
Linux 0.701 10.788 0.752 11.784 0.217 16.027 0.217 15.201 0.605 18.539 0.152 23.103 0.69 14.895 0.690 15.186 0.361 17.880 0.361 17.887 0.381 12.483 0.853 12.152
Hadoop 0.885 19.338 0.885 19.437 0.867 16.194 0.867 16.2925 0.775 23.967 0.775 27.398 0.948 15.399 0.948 15.498 0.531 24.111 0.531 27.447 0.989 19.538 0.979 4.958
OpenStack 0.732 20.908 0.389 59.720 0.722 20.601 0.206 50.336 0.764 30.400 0.230 96.790 0.733 30.759 0.839 25.46 0.326 65.176 0.284 58.556 1.000 5.470 0.935 3.846
Windows 0.566 19.465 0.566 20.955 0.701 3.169 0.701 5.297 0.989 3.200 0.988 5.169 0.997 4.966 0.996 7.377 0.712 21.205 0.712 23.389 0.996 6.110 0.993 1.647
OpenSSH 0.927 8.071 0.679 11.124 0.621 3.835 0.119 8.855 0.556 7.331 0.4300 11.835 0.789 7.543 0.996 5.3535 0.642 34.296 0.586 38.153 0.417 7.873 0.818 3.075
Average 0.764 11.941 0.643 18.973 0.598 11.656 0.495 16.808 0.789 11.866 0.619 22.676 0.861 10.476 0.854 13.230 0.653 20.872 0.553 21.290 0.863 8.494 0.943 6.061
Rex impact â†“0.121â†“7.032 â†“0.103â†“5.152 â†“0.170â†“10.810 â†“0.007â†“2.754 â†“0.100â†“0.418
(3) Spell uses the longest common sub-sequence algorithm to ex-
tract log templates. (4) SHISO used a tree-form structure to guide
the parsing process, where each node was correlated with a log
group and a log template. (5) LenMa is a clustering-based log parser
and focuses on the word length feature and converts the log into
a vector of the number of word letters. (6) Nulog utilizes masked
word recovery for pre-training a transformer model. During the
parsing process, each word is masked once, and the probability
of correctly recovering the masked word by the model is used to
determine whether the masked word is a variable.
Implementation and configuration. All experiments are con-
ducted on a GPU server with A100-SXM4-80GB and CUDA 11.3.
We implement Log3T based on Python 3.8 and PyTorch 1.10. We
set the initial learning rate to 0.001 and use Adam optimizer. The
thresholds (refer to section 3.2) we used for each dataset are shown
in Table 1. The parsing accuracy of the sampled dataset was ob-
tained when only the variables in the first 100 logs were labeled.
All the imitated variables are generated based on first 100 logs.
4.2 RQ1: The Parsing Accuracy of Log3T
To answer RQ1, we followed the guidance in [ 50] and collected pars-
ing accuracy for Log3T and 6 other log parsers on 16 2K sampled
datasets. In our reproduction, the optimal parameters for Nulog
on the 16 benchmark datasets are not fully available, as the au-
thors [ 33] only provides optimal parameters for 10 datasets. We
made efforts to modify the parameters of Nulog for the remaining
6 datasets; however, we cannot guarantee that these parameters
will lead to the best performance for Nulog on these datasets. In ad-
dition, we found that the parameters provided in the Logram code
repository resulted in poor performance, leading us to believe that
the uploaded parameters by the authors were incorrect. We madeefforts to modify the Logramâ€™s parameters to achieve the better
performance. The epochs for the Log3T model on each dataset are
shown in Table 1.
We evaluated the GA and Edit Distance of all the log parsers on
16 benchmark datasets both with and without their standard regex
filters. We did not record parsing accuracy for Nulog without regex
filters because it is challenging to apply Nulog to online parsing
scenarios with new tokens. (refer to section 2.2)
Table 2 shows the experimental results. Log3T achieves the best
GA on 7 out of the 16 benchmark datasets and the best Edit Dis-
tance on 12 out of the 16 benchmark datasets. Log3T also achieves
the best average GA 0.943 and Edit Distance 6.061 across all 16
benchmark datasets. As indicated in Table 2â€™s "Regex impact" row,
we can conclude that regex filters have an impact on the effective-
ness of all the log parsers. In terms of average GA across the 16
benchmark datasets, Spell without regex filters exhibits a decrease
of 22% compared to Spell with regex filters, while the Edit Distance
increases by 91%. Removing all regex filters results in an average
GA drop of 10% and an average Edit Distance increase of 4.45 across
the 16 benchmark datasets for the 6 log parsers. In this paper, we
consider regex filters as barriers to applying log parsers to new
log types. However, the parsing accuracy of existing online parsers
heavily relies on these regex filters.
Answer to RQ1: Log3T without regex filters is effective
and outperforms existing online log parsers. The effec-
tiveness of existing online parsers tends to decrease when
regex filters are removed, which poses challenges for their
application to new log types.
432Log Parsing with Generalization Ability under New Log Types ESEC/FSE â€™23, December 3â€“9, 2023, San Francisco, CA, USA
(a) HDFS
 (b) HPC
(c) HealthAPP
 (d) Spark
(e) Linux
 (f) OpenSSH
Figure 10: Group accuracy of Log3T without test-time training and with test-time training when only one batch of data is
available as historical data (In each subplot, left: batch size = 100, right: batch size = 200)
4.3 RQ2: Model Generalization
To answer RQ2, we simulated real-world scenarios to input new log
types into the Log3T model and collected GA. Specifically, we parti-
tioned the benchmark dataset into multiple equally-sized batches in
chronological order. The first batch served as the systemâ€™s available
logs, while the subsequent batches are considered as newly arrived
logs to input to the model. The GA for each batch was calculated
based on the current batch and all previous batches. For example,
if the batch size is 100, when processing the third batch, the GA we
collected was based on the first 300 logs. This approach is adopted
because the definition of GA requires all logs belonging to the same
log template to be grouped together. Thus, assessing GA within a
single batch holds no meaningful significance. In our implementa-
tion, we collected the GA of the Log3T model trained on the first
batch when parsing subsequent batches, the GA of the Log3T with
test-time training and the GA of the Log3T model trained with all
logs. Batch size is set as 100 and 200. The experimental results are
shown in Figure 10.
The experimental results indicate that the improvement brought
by test-time training varies across different categories of datasets.
1) For simple datasets where variables are mostly pure numbers,
models trained with the first batch of logs can achieve the similar
performance to models trained with all logs, as well as models with
test-time training. This is because Log3T does not consider pure
numbers as extracted constants for subsequent log partition. For
example, in datasets such as HDFS, Apache, and Proxifier, over
70% of the variables are pure numbers, the three curves (withTTT,withoutTTT, trained with all logs) overlap completely. In datasets
like HealthApp, Spark, over 50% of the variables are pure numbers.
There is a slight performance gap, approximately 0.1-0.5%, between
a model trained without testing-time training and a model trained
using all logs. Testing-time training can help bridge this gap. 2)
For the remaining datasets with much less pure numbers variables,
models trained with the first batch of logs perform worse compared
to models trained with all logs, while test-time training can provide
a 10-20% GA improvement to the model. In datasets like OpenSSH,
Linux, Android, over 70% of the variables are notpure numbers.
Models trained with the first batch of logs show approximately 25%
lower GA compared to models trained with all logs. Test-time train-
ing can improve the modelâ€™s GA by around 20%. In the Linux dataset
with batchsize set as 200, models trained with the first batch and
models with test-time training achieve the same performance. This
is because the log types that can be correctly parsed by Log3T are
predominantly included within the first 200 logs. Figure 10 displays
that certain model "without test-time training" also experience an
increase in accuracy in subsequent batches. This is connected to
the distribution of logs within the dataset that are easy to parse, as
a similar trend is observed for model trained on all logs.
In addition to the model parameters, the adjustment of the hy-
perparameter threshold (refer to section 3.2) also links to whether
the log parser extensive effort under new logs types [ 6]. Therefore,
we conducted a sensitivity analysis on the threshold. We set the
threshold values uniformly to 3, 4, 5, 6, 7, 8, and 9, respectively, and
433ESEC/FSE â€™23, December 3â€“9, 2023, San Francisco, CA, USA Siyu Yu, Yifan Wu, Zhijing Li, Pinjia He, Ningjiang Chen, and Changjian Liu.
Figure 11: Sensitivity analysis on threshold
collected the average GA and Edit Distance of Log3T on 16 bench-
mark datasets. As shown in Figure 11, Log3T with different thresh-
old values consistently achieves an average GA of 0.8 or higher
across the 16 benchmark datasets. Furthermore, it maintains an
average Edit Distance ranging from 6 to 7. The experimental results
indicate that Log3T is not significantly affected by the threshold.
Answer to RQ2: Log3T can utilize our modified test-
time training to automatically update model parameters
to generalize to new log types.
4.4 RQ3: Effectiveness of Each Component
To answer RQ3, we conducted four evaluation experiments. Firstly,
we collected predicted variable-probabilities generated by the Log3T
model for constant and variable words in each log to examine if
there are noticeable differences in their variable-probabilities. Sec-
ondly, we compared the GA obtained based on random word order,
original word order, and word order sorted by the Log3T model to
check whether the Log3T model can extract constant words first.
Thirdly, we compared the GA obtained from training the Log3T
model using only real variables with the GA obtained from training
the Log3T model using both imitated and real variables to evaluate
the effectiveness of imitated variables on the modelâ€™s performance.
Finally, we evaluated whether the modified test-time training con-
tributes more to the performance of Log3T compared to standard
test-time training.
Table 4 presents the average predicted variable-probabilities gen-
erated by the Log3T model for constant words and variable words
in each log from the 16 benchmark datasets. The average predicted
variable-probabilities for constant words across the 16 benchmark
datasets is 0.029, while the average predicted variable-probabilities
for variable words is 0.477. There is a significant difference.
Table 3 presents the GA achieved by different methods for word
order in the logs. Keeping the word order unchanged yields a GA
of 0.844. After randomly shuffling the word order in each log from
the 16 benchmark datasets five times, the average GA obtained
is 0.751. Significant improvement in GA is observed when using
Log3T model for word ordering, reaching a GA of 0.943.
Table 5 presents the average GA achieved by the Log3T model
trained with imitated logs generated based on real variables from
the first 100 logs in the 2K dataset, compared to the Log3T model
trained with imitated logs generated based on real and imitated
variables. Compared to the Log3T model trained with imitated logs
based on real variables, utilizing both real and imitated variables
in imitated logs generation resulted in a GA improvement of 0.033.Table 3: Parsing accuracy for different word order on 16 bech-
mark datasets
Random order Original order Log3T
Average GA 0.751 0.844 0.943
Table 4: Variable probabilities assigned by Log3T to constants
and variables on 16 datasets
Dataset Constant Variable difference
HDFS 0.022 0.583 +0.561
Hadoop 0.040 0.369 +0.329
Spark 0.017 0.398 +0.381
Zookeeper 0.017 0.392 +0.375
BGL 0.087 0.432 +0.345
HPC 0.026 0.165 +0.139
Thunderbird 0.036 0.373 +0.337
Proxifier 0.003 0.513 +0.510
Windows 0.025 0.305 +0.28
Linux 0.011 0.428 +0.417
Android 0.086 0.743 +0.657
HealthApp 0.015 0.615 +0.600
Apache 0.000 0.704 +0.704
OpenSSH 0.013 0.471 +0.458
OpenStack 0.020 0.749 +0.729
Mac 0.058 0.381 +0.323
Average 0.029 0.477 +0.447
Table 5: Parsing accuracy and variable probabilities for dif-
ferent word extractions methods on 16 datasets
training method Average GA Probability
difference
With real variables 0.910 0.371
With real and imitated variables 0.943 0.447
Furthermore, the difference in predicted variable-probabilities be-
tween constants and variables was expanded by 0.076 with the
inclusion of imitated variables.
Figure 12 illustrates the integration of standard test-time training
into the Log3T model, wherein the classifier of the model is con-
figured with non-shared parameters, while the attention block is
configured with shared parameters. We set the batch size to 100 and
compare the final GA of all batches achieved by two versions of test-
time training on the benchmark datasets. The Log3T model using
standard test-time training achieved an average GA of 0.914, while
the Log3T model using the modified test-time training achieved an
average GA of 0.943 across 16 benchmark datasets.
Answer to RQ3: The predicted variable-probabilities gen-
erated by the Log3T model can be utilized to distinguish
between constants and variables, leading to good parsing
accuracy. The training strategy of Log3T, which incorpo-
rates imitated variables, also contributes to its effectiveness
in log parsing.
4.5 RQ4: Efficiency of Log3T
To answer RQ4, we verified whether the log parsing speed of Log3T
is higher than the log generation rate of large-scale systems in
the real world. Figure 13 shows the running time of Log3T on
434Log Parsing with Generalization Ability under New Log Types ESEC/FSE â€™23, December 3â€“9, 2023, San Francisco, CA, USA
Figure 12: Illustration of standard test-time training used in
the Log3T model for comparative experiments(Shared pa-
rameters: attention block; Non-shared parameters: classifier)
Figure 13: Running time of Log3T facing different log volume
different volumes of BGL datasets. We designed batch processing
for Log3T to parse logs. Experimental results show that Log3T
takes around 80S to process 100K logs when the batch size is set to
128. The computational power of the graphic processing unit is an
important factor in determining the training and inference time of
deep learning-based approaches. During parsing, if we set batch
size to 1, Log3T takes 200 seconds to process 100K logs. According
to the positive correlation between computational power and the
efficiency of Log3T, we believe that having more computational
power within the company can indeed result in higher efficiency
for Log3T. The aforementioned example of log generation speed
of Alibaba Inc. requires the log parser to process 100K logs in less
than 180 seconds.
Answer to RQ4: The parsing speed of Log3T is capable
of matching the log generation rate of large-scale systems
in the real world.
5 DISCUSSION
Log parser whose source code is hidden. SPINE is an efficient
and scalable log parser, and it can evolve from human feedback. In
order to improve the efficiency of SPINE, logs with the same first n
words are grouped together when creating the initial groups. How-
ever, logs starting with variables are common in practice. Drain+
can adaptively generate delimiters but still does not support logs
starting with variables. Uniparser is not open sourced, and the tech-
nical detail of Uniparser is not described well in their paper so it
is hard for us to reproduce their experimental result (e.g., the 96
characters they used to cover the most of tokens formed by their
combinations, whether the parser uses tokenization filters). Both
Uniparser and Drain+ lack generalization ability.
Threats to validity. We have identified the following major
threats to validity. 1). External validity. Threats to external validity
relate to the generalization of experimental results, that is, the effec-
tiveness of Log3T in the 16 benchmark datasets does not necessarily
guarantee its effectiveness in an industrial setting. 2). Constructvalidity. Researchers have found that the performance of log parsers
on existing metrics does not necessarily correlate with their use-
fulness for downstream tasks [ 23]. Therefore, in the future, new
metrics may be required to evaluate log parsers [22, 23].
6 RELATED WORK
Automated Log parsing. Automated log parsing has replaced
manual log reading and the manual setting of regular expressions
through source code [ 46]. The main approaches of automated log
parsing include heuristics [ 17], clustering [ 12,15,37,39,45], fre-
quent item extraction [ 5,32,40] and deep learning [ 29] [33]. For
unsupervised methods such as clustering and frequent item ex-
traction, their parameters that require prior knowledge to set are
difficult to modify under new log types. LPV [ 45] uses skip-grams to
convert logs into vectors, which are then grouped according to the
similarity of vectors. Brain [ 48] is a latest unsupervised approach
incorporating heuristic rules and frequent item extraction. In recent
years, supervised log parsing has gradually developed, LogPPT [ 26]
achieved impressive performance by utilizing a small number of
samples to provide prompts for fine-tuning a large language model.
Researchers have confirmed that Chatgpt performs log parsing well
[25], but the efficiency is difficult to match the real-world system
log generation rate.
Model generalization. In real-world applications, deep learning
models often run in non-stationary environments where the target
data distribution continually shifts over time (e.g., system logs).
Domain generalization methods study the learning problem on
multiple data domains, where the source domain is available and
target domain is unavailable. Domain adaptation methods study
continual data drifts in dynamic environments. Test-Time or Online
domain adaptation [ 42] can improve the target model performance
with a small training cost. In addition, researchers [ 28] handled the
poor performance on new domains before and during adaptation.
There are many excellent studies in this field, and in the future,
we may continue to explore the integration of other generalization
techniques into log parsing
7 CONCLUSION
In this paper, we propose Log3T, the first log parsing approach
with generalization ability under new log types without human
intervention. Log3T incorporates the idea of test-time training to
automatically update model parameters to adapt to new log types.
Extensive experimental results demonstrate that Log3T is effective,
and can generalize to new log types. The generalization ability of
log parsers under new log types is a promising research direction,
which we will further explore in the future.
ACKNOWLEDGEMENT
This work was supported by the National Natural Science Founda-
tion of China (No.62162003, No. 62102340), and the Nanning Science
and Technology project (No. 20221031).
DATA AVAILABILITY
Codes and data of Log3T can be found at [47].
435ESEC/FSE â€™23, December 3â€“9, 2023, San Francisco, CA, USA Siyu Yu, Yifan Wu, Zhijing Li, Pinjia He, Ningjiang Chen, and Changjian Liu.
REFERENCES
[1]Yuki M Asano, Christian Rupprecht, and Andrea Vedaldi. 2019. A critical analysis
of self-supervision, or what we can learn from a single image. arXiv preprint
arXiv:1904.13132 (2019). https://doi.org/10.48550/arXiv.1904.13132
[2]Minmin Chen, Kilian Q Weinberger, and John Blitzer. 2011. Co-training for
domain adaptation. Advances in neural information processing systems 24 (2011).
https://doi.org/10.5555/2986459.2986733
[3]Xilun Chen, Yu Sun, Ben Athiwaratkun, Claire Cardie, and Kilian Weinberger.
2018. Adversarial deep averaging networks for cross-lingual sentiment classi-
fication. Transactions of the Association for Computational Linguistics 6 (2018),
557â€“570. https://doi.org/10.1162/tacl_a_00039
[4]Kyunghyun Cho, Bart Van MerriÃ«nboer, Caglar Gulcehre, Dzmitry Bahdanau,
Fethi Bougares, Holger Schwenk, and Yoshua Bengio. 2014. Learning phrase
representations using RNN encoder-decoder for statistical machine translation.
arXiv preprint arXiv:1406.1078 (2014). https://doi.org/10.48550/arXiv.1406.1078
[5]Hetong Dai et al .2020. Logram: Efficient log parsing using n-gram dictionaries.
IEEE Transactions on Software Engineering (2020). https://doi.org/10.1109/TSE.
2020.3007554
[6]Hetong Dai, Yiming Tang, Heng Li, and Weiyi Shang. 2023. PILAR: Studying and
Mitigating the Influence of Configurations on Log Parsing. In 2023 IEEE/ACM
45th International Conference on Software Engineering (ICSE) . IEEE, 818â€“829.
[7]Jacob Devlin et al .2018. Bert: Pre-training of deep bidirectional transformers
for language understanding. arXiv preprint arXiv:1810.04805 (2018). https:
//doi.org/10.48550/arXiv.1810.04805
[8]Carl Doersch, Abhinav Gupta, and Alexei A Efros. 2015. Unsupervised visual rep-
resentation learning by context prediction. In Proceedings of the IEEE international
conference on computer vision . 1422â€“1430. https://doi.org/10.1109/ICCV.2015.167
[9]Min Du et al .2017. Deeplog: Anomaly detection and diagnosis from system
logs through deep learning. In Proceedings of the 2017 ACM SIGSAC conference
on computer and communications security . 1285â€“1298. https://doi.org/10.1145/
3133956.3134015
[10] Jeffrey L Elman. 1990. Finding structure in time. Cognitive science 14, 2 (1990),
179â€“211. https://doi.org/10.1207/s15516709cog1402_1
[11] Ilenia Fronza et al .2013. Failure prediction based on log files using random
indexing and support vector machines. Journal of Systems and Software 86, 1
(2013), 2â€“11. https://doi.org/10.1016/j.jss.2012.06.025
[12] Qiang Fu et al .2009. Execution anomaly detection in distributed systems through
unstructured log analysis. In 2009 ninth IEEE international conference on data
mining . IEEE, 149â€“158. https://doi.org/10.1109/ICDM.2009.60
[13] Ying Fu, Meng Yan, Jian Xu, Jianguo Li, Zhongxin Liu, Xiaohong Zhang, and Dan
Yang. 2022. Investigating and improving log parsing in practice. In Proceedings
of the 30th ACM Joint European Software Engineering Conference and Symposium
on the Foundations of Software Engineering . 1566â€“1577. https://doi.org/10.1145/
3540250.3558947
[14] Spyros Gidaris, Praveer Singh, and Nikos Komodakis. 2018. Unsupervised repre-
sentation learning by predicting image rotations. arXiv preprint arXiv:1803.07728
(2018). https://doi.org/10.48550/arXiv.1803.07728
[15] Hossein Hamooni et al .2016. Logmine: Fast pattern recognition for log analytics.
InProceedings of the 25th ACM International on Conference on Information and
Knowledge Management . 1573â€“1582. https://doi.org/10.1145/2983323.2983358
[16] Pinjia He et al .2016. An evaluation study on log parsing and its use in log mining.
In2016 46th annual IEEE/IFIP international conference on dependable systems and
networks (DSN) . IEEE, 654â€“661. https://doi.org/10.1109/DSN.2016.66
[17] Pinjia He et al .2017. Drain: An online log parsing approach with fixed depth
tree. In 2017 IEEE international conference on web services (ICWS) . IEEE, 33â€“40.
https://doi.org/10.1109/ICWS.2017.13
[18] Shilin He et al .2021. A survey on automated log analysis for reliability engineer-
ing. ACM Computing Surveys (CSUR) 54, 6 (2021), 1â€“37. https://doi.org/10.1145/
3460345
[19] Sepp Hochreiter and JÃ¼rgen Schmidhuber. 1997. Long short-term memory. Neural
computation 9, 8 (1997), 1735â€“1780. https://doi.org/10.1007/978-3-642-24797-2_4
[20] Tong Jia et al .2021. LogFlash: Real-time Streaming Anomaly Detection and
Diagnosis from System Logs for Large-scale Software Systems. In 2021 IEEE 32nd
International Symposium on Software Reliability Engineering (ISSRE) . IEEE, 80â€“90.
https://doi.org/10.1109/ISSRE52982.2021.00021
[21] Zhen Ming Jiang, Ahmed E Hassan, Parminder Flora, and Gilbert Hamann. 2008.
Abstracting execution logs to execution events for enterprise applications (short
paper). In 2008 The Eighth International Conference on Quality Software . IEEE,
181â€“186. https://doi.org/10.1109/QSIC.2008.50
[22] Zanis Ali Khan, Donghwan Shin, Domenico Bianculli, and Lionel Briand. 2022.
Guidelines for Assessing the Accuracy of Log Message Template Identification
Techniques. In Proceedings of the 44th International Conference on Software Engi-
neering (ICSEâ€™22) . ACM. https://doi.org/10.1145/3510003.3510101
[23] Zanis Ali Khan, Donghwan Shin, Domenico Bianculli, and Lionel Briand. 2023.
Impact of Log Parsing on Log-based Anomaly Detection. arXiv preprint
arXiv:2305.15897 (2023).[24] Van-Hoang Le et al .2022. Log-based anomaly detection with deep learning:
How far are we?. In Proceedings of the 44th International Conference on Software
Engineering . 1356â€“1367. https://doi.org/10.1145/3510003.3510155
[25] Van-Hoang Le and Hongyu Zhang. 2023. An Evaluation of Log Parsing with
ChatGPT. arXiv preprint arXiv:2306.01590 (2023). https://doi.org/10.48550/arXiv.
2306.01590
[26] Van-Hoang Le and Hongyu Zhang. 2023. Log Parsing with Prompt-based Few-
shot Learning. arXiv preprint arXiv:2302.07435 (2023). https://doi.org/10.48550/
arXiv.2302.07435
[27] Ya Li, Xinmei Tian, Mingming Gong, Yajing Liu, Tongliang Liu, Kun Zhang,
and Dacheng Tao. 2018. Deep domain generalization via conditional invariant
adversarial networks. In Proceedings of the European Conference on Computer
Vision (ECCV) . 624â€“639. https://doi.org/10.1007/978-3-030-01267-0_38
[28] Chenxi Liu, Lixu Wang, Lingjuan Lyu, Chen Sun, Xiao Wang, and Qi Zhu. 2022.
Deja Vu: Continual Model Generalization for Unseen Domains. In The Eleventh
International Conference on Learning Representations . https://doi.org/10.48550/
arXiv.2301.10418
[29] Yudong Liu et al .2022. UniParser: A Unified Log Parser for Heterogeneous
Log Data. In Proceedings of the ACM Web Conference 2022 . 1893â€“1901. https:
//doi.org/10.1145/3485447.3511993
[30] Haibo Mi et al .2013. Toward fine-grained, unsupervised, scalable performance
diagnosis for production cloud computing systems. IEEE Transactions on Parallel
and Distributed Systems 24, 6 (2013), 1245â€“1255. https://doi.org/10.1109/TPDS.
2013.21
[31] Masayoshi Mizutani. 2013. Incremental mining of system log format. In 2013
IEEE International Conference on Services Computing . IEEE, 595â€“602. https:
//doi.org/10.1109/SCC.2013.73
[32] Meiyappan Nagappan et al .2010. Abstracting log lines to log event types for
mining software system logs. In 2010 7th IEEE Working Conference on Mining
Software Repositories (MSR 2010) . IEEE, 114â€“117. https://doi.org/10.1109/MSR.
2010.5463281
[33] Sasho Nedelkoski et al .2020. Self-supervised log parsing. In Joint European
Conference on Machine Learning and Knowledge Discovery in Databases . Springer,
122â€“138. https://doi.org/10.1007/978-3-030-67667-4_8
[34] Sasho Nedelkoski, Jasmin Bogatinovski, Alexander Acker, Jorge Cardoso, and
Odej Kao. 2020. Self-attentive classification-based anomaly detection in unstruc-
tured logs. In 2020 IEEE International Conference on Data Mining (ICDM) . IEEE,
1196â€“1201. https://doi.org/10.1109/ICDM50108.2020.00148
[35] Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar. 2018.
Do cifar-10 classifiers generalize to cifar-10? arXiv preprint arXiv:1806.00451
(2018). https://doi.org/10.48550/arXiv.1806.00451
[36] Shiv Shankar, Vihari Piratla, Soumen Chakrabarti, Siddhartha Chaudhuri, Preethi
Jyothi, and Sunita Sarawagi. 2018. Generalizing across domains via cross-gradient
training. arXiv preprint arXiv:1804.10745 (2018). https://doi.org/10.48550/arXiv.
1804.10745
[37] Keiichi Shima. 2016. Length matters: Clustering system log messages using
length of words. arXiv preprint arXiv:1611.03213 (2016). https://doi.org/10.48550/
arXiv.1611.03213
[38] Yu Sun, Xiaolong Wang, Zhuang Liu, John Miller, Alexei Efros, and Moritz
Hardt. 2020. Test-time training with self-supervision for generalization under
distribution shifts. In International conference on machine learning . PMLR, 9229â€“
9248. https://doi.org/10.48550/arXiv.1909.13231
[39] Liang Tang et al .2011. LogSig: Generating system events from raw textual
logs. In Proceedings of the 20th ACM international conference on Information and
knowledge management . 785â€“794. https://doi.org/10.1145/2063576.2063690
[40] Risto Vaarandi. 2003. A data clustering algorithm for mining patterns from event
logs. In Proceedings of the 3rd IEEE Workshop on IP Operations & Management
(IPOM 2003)(IEEE Cat. No. 03EX764) . Ieee, 119â€“126. https://doi.org/10.1109/IPOM.
2003.1251233
[41] Ashish Vaswani et al .2017. Attention is all you need. Advances in neural
information processing systems 30 (2017). https://doi.org/10.5555/3295222.3295349
[42] Qin Wang, Olga Fink, Luc Van Gool, and Dengxin Dai. 2022. Continual test-time
domain adaptation. In Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition . 7201â€“7211. https://doi.org/10.1109/CVPR52688.2022.
00706
[43] Xuheng Wang, Xu Zhang, Liqun Li, Shilin He, Hongyu Zhang, Yudong Liu,
Lingling Zheng, Yu Kang, Qingwei Lin, Yingnong Dang, et al .2022. SPINE: a
scalable log parser with feedback guidance. In Proceedings of the 30th ACM Joint
European Software Engineering Conference and Symposium on the Foundations of
Software Engineering . 1198â€“1208. https://doi.org/10.1145/3540250.3549176
[44] Yonghui Wu et al .2016. Googleâ€™s neural machine translation system: Bridging
the gap between human and machine translation. arXiv preprint arXiv:1609.08144
(2016). https://doi.org/10.48550/arXiv.1609.08144
[45] Tong Xiao et al .2020. Lpv: A log parser based on vectorization for offline and
online log parsing. In 2020 IEEE International Conference on Data Mining (ICDM) .
IEEE, 1346â€“1351. https://doi.org/10.1109/ICDM50108.2020.00175
[46] Wei Xu et al .2009. Detecting large-scale system problems by mining console
logs. In Proceedings of the ACM SIGOPS 22nd symposium on Operating systems
436Log Parsing with Generalization Ability under New Log Types ESEC/FSE â€™23, December 3â€“9, 2023, San Francisco, CA, USA
principles . 117â€“132. https://doi.org/10.1145/1629575.1629587
[47] Siyu Yu. 2023. Codes and data of Log3T . https://github.com/logpai/logparser/
tree/master/logparser/Log3T
[48] Siyu Yu, Pinjia He, Ningjiang Chen, and Yifan Wu. 2023. Brain: Log Parsing
with Bidirectional Parallel Tree. IEEE Transactions on Services Computing (2023).
https://doi.org/10.1109/TSC.2023.3270566
[49] Xu Zhang et al .2019. Robust log-based anomaly detection on unstable log data. In
Proceedings of the 2019 27th ACM Joint Meeting on European Software EngineeringConference and Symposium on the Foundations of Software Engineering . 807â€“817.
https://doi.org/10.1145/3338906.3338931
[50] Jieming Zhu et al .2019. Tools and benchmarks for automated log parsing. In
2019 IEEE/ACM 41st International Conference on Software Engineering: Software
Engineering in Practice (ICSE-SEIP) . IEEE, 121â€“130. https://doi.org/10.1109/ICSE-
SEIP.2019.00021
Received 2023-02-02; accepted 2023-07-27
437