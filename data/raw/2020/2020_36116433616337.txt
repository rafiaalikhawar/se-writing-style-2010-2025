NeuRI:DiversifyingDNNGenerationviaInductiveRuleInference
Jiawei Liu
Universityof Illinois
Urbana-Champaign,USA
jiawei6@illinois.eduJinjun Peng∗
Columbia University
New York, USA
jinjun.peng@columbia.eduYuyaoWang∗
Nanjing University
Nanjing,China
yuyao6@outlook.comLingming Zhang
Universityof Illinois
Urbana-Champaign,USA
lingming@illinois.edu
ABSTRACT
Deep Learning (DL) is prevalently used in various industries to
improve decision-making and automateprocesses, driven by the
ever-evolving DL libraries and compilers. The correctness of DL
systems is crucialfor trust in DL applications. As such, the recent
waveofresearchhasbeenstudyingtheautomatedsynthesisoftest-
cases(i.e.,DNNmodelsandtheirinputs)forfuzzingDLsystems.
However,existingmodelgeneratorsonlysubsumealimitednumber
ofoperators,forlackingtheabilitytopervasivelymodeloperator
constraints.Toaddressthischallenge,wepropose NeuRI,afully
automated approach for generating valid and diverse DL models
composed of hundreds of types of operators. NeuRIadopts a three-
stepprocess:(i)collectingvalidandinvalidAPItracesfromvarious
sources; (ii)applying inductive programsynthesisoverthetraces
toinfertheconstraintsforconstructingvalidmodels;and(iii)using
hybrid model generation which incorporates both symbolic and
concrete operators. Our evaluation shows that NeuRIimproves
branchcoverage ofTensorFlow andPyTorchby 24% and15% over
the state-of-the-art model-level fuzzers. NeuRI/f_inds 100 newbugs
for PyTorch and TensorFlow in four months, with 81 already /f_ixed
orcon/f_irmed.Ofthese,9bugsarelabelledas highpriority orsecurity
vulnerability ,constituting10%ofallhigh-prioritybugsoftheperiod.
Open-source developers regard error-inducing tests reported by us
as “high-quality” and“common inpractice”.
CCS CONCEPTS
•Softwareanditsengineering →Softwaretestinganddebug-
ging;•Computingmethodologies →Neuralnetworks .
KEYWORDS
Fuzzing,CompilerTesting, DeepLearningCompilers
ACMReference Format:
JiaweiLiu,JinjunPeng,YuyaoWang,andLingmingZhang.2023. NeuRI:
DiversifyingDNNGenerationviaInductiveRuleInference.In Proceedingsof
the31stACMJointEuropeanSoftwareEngineeringConferenceandSymposium
on the Foundations of Software Engineering (ESEC/FSE ’23), December 3–9,
2023, San Francisco, CA, USA. ACM, New York, NY, USA, 13pages.https:
//doi.org/10.1145/3611643.3616337
∗The workwas performed during a remote internshipat Universityof Illinois.
Permissionto make digitalor hard copies of allorpart ofthis work for personalor
classroom use is granted without fee provided that copies are not made or distributed
forpro/f_itorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation
onthe/f_irstpage.Copyrights forcomponentsofthisworkownedbyothersthanthe
author(s)mustbehonored.Abstractingwithcreditispermitted.Tocopyotherwise,or
republish,topostonserversortoredistributetolists,requirespriorspeci/f_icpermission
and/or a fee. Request permissions from permissions@acm.org.
ESEC/FSE ’23, December 3–9, 2023, San Francisco, CA,USA
©2023 Copyright heldby the owner/author(s). Publicationrightslicensed to ACM.
ACM ISBN 979-8-4007-0327-0/23/12...$15.00
https://doi.org/10.1145/3611643.36163371 INTRODUCTION
TheriseofDeep-Learning(DL)librariesandcompilershasenabled
emerging AI applications, such as AI chatbots [ 36], art genera-
tors [37] and autonomous driving, powering hundreds of millions
ofusers.Thesecomplexsystemshavebecomeincreasinglyadopted
and everevolving. Forexample, PyTorch[ 38]and TensorFlow[ 9],
themostpopularDLsystemswith62kand171kGitHubstarsre-
spectively,aremovingtowardtheirnextmajorversion( i.e.,PyTorch
2 [5] and TensorFlow 3 [ 2]), aiming at better model compilation
support. However, taking PyTorch’s new compiler [ 41] as an exam-
ple,sincebirth( i.e.,17months)itisinsuﬃcientlytestedbyatest
suiteineightthousandLoC.Consequently,itiscrucialtoharness
the correctness of DL systems via extensive and automated testing.
Thetest-casegenerationproblem forDLsystemsistosynthesize
aDNNmodelanditscomputationalinputs.Additionally,generating
diverseandvalidmodels is essential for making high-quality tests.
(1)Modeldiversity :EﬀectiveDLsystemtestingasksformodeldi-
versitycomingfromthevarietyofAPIs,aswellasthewaythey
are composed. Additionally, to test the complicated DL compil-
ers, itis importanttogenerate models withmultiple operators
ofvarioustypes for practicing the compilerpasses[ 29].
(2)Validity: DNN models are programs [ 47] – for well-formedness
theyneed tocomplywithvalidity constraints. Arbitrarilycon-
structing and composing operators, such as creating pooling
operators with negative kernel sizes or “connecting” operators
withunwantedtensorshapes,oftentimesviolatetheconstraints
for constructing a well-de/f_ined model. As a result, argument
errors (for DL libraries) or parser errors (for DL compilers) are
raisedbefore deeper systembehavioursare tested.
Single-APIStrongly constrained
WeaklyconstrainedManualoperatorrules.This paper:Auto. rule inference!Our Goal
Operator Diversity
ModelDiversity
Figure 1:Test-case diversity.Motivation. The model diversity
primarilydependsonthecompre-
hensiveness of operators, which
arethebuildingblockstoamodel.
Prior work on single-API test-
ing [14,53] can generate a large
body of API invocations (includ-
ingbothoperatorandutilityAPIs)
viamutationorgenerationwhich
complywithhigh-leveltypecon-
straints or the plausible value
sets. Can we directly apply such
high-level information to gener-
ate valid DNN models? Unfortu-
nately, it is impractical. Because
constructing a validAPIinvocation further requires satisfying/f_ine-
grained constraints between operator attributes ( i.e.,non-tensor
argumentssuchaskernelsizesandstrides)andinputtensortypes1
1Followingpriorwork[ 29], atensortypeis a tuple of its shapeand data type.
657
ESEC/FSE ’23, December3–9, 2023,San Francisco, CA, USA JiaweiLiu, JinjunPeng,Yuyao Wang,andLingming Zhang
(particularly shapes). For example, while single-API testers may
understandthat conv2dacceptsanimageandaweighttensorof
/f_loating-points( i.e.,typeconstraints),their newlycreatedconv2d
invocationsarenotguaranteedtohavethechanneldimensionof
theimagematchingthatoftheweight( i.e.,shapeconstraints).Con-
sequently,suchattributesviolatethevaliditypropertiesrequired
byconv2dand lead to invocation failures. Without understanding
such /f_ine-grainedconstraints,itisunlikelytocorrectlycompose
variousAPIsfor constructing well-formedanddiversemodels.In-
tuitively, in Figure 1prior single-API testers can achieve ideal
APIdiversitywhenAPIsbeingvalidlyconstructed.However,the
diversity hardly extends at model-wise which requires multiple
APIs to be constructedand“connected” correctlysimultaneously.
Meanwhile, there are two categories of proposals for construct-
ingvalidmodels. Weaklyconstrainedmodelgeneration [19,32,51]
limits the use of APIs to those with simple and straight-forward
constraints. For example, LEMON [ 51] only uses shape-preserving
operatorsthathavenoinputconstraints.Consequently,suchop-
eratorscanbearbitrarilyconstructedandaddedtobuildamodel.
Morerecentwork[ 19,32]additionallyinserts“reshaping”layers
suchthatreshapedoutputtensorscanstayincompatibleshapes.
However, it still may not construct operators with valid attributes
(non-tensor arguments). Even worse, using such “layer wrappers”
compromisesthestructuraldiversityofthemodels,whichcanover-
lookcompilerpassesactivatedbyspeci/f_icpatterns.Tosupportmore
diverse APIs correctly, NNSmith [29], as astrongly constrained
approach,de/f_inesaspeci/f_icationfordescribinginputconstraints
and shape propagation(elaborated in § 2). Nonetheless, it requires
manual eﬀortsfor specifyingthose rules. For example, while a DL
framework, e.g.,PyTorch, can de/f_ine over two thousand APIs, only
about sixty are supported by NNSmith after its /f_irst-year devel-
opment.Hence,itcantakeyearsfor NNSmith [29]tocompletely
support aframework, i.e.,fromto,whichisunscalable.
Insight.Canwescalethediversityofmodelgenerationbyenabling
more operators ( e.g.,by hundreds) fully automatically ? We start to
answerthisquestionfromtwoinsights: (i)Empiricallyweobserved
that most operator rules are simple, e.g.,consisting of arithmetic
expressionsforshapecomputationandif-elsebranchesforhandling
conditions incurred by some attributes. As a result, it is feasible to
search a program that functions as operator rules, given the size of
the problem is acceptable. Speci/f_ically, by instrumenting DL API
invocations, we can obtain a set of input-output examples, with
whichtheinferenceofoperatorrulescanberegardedasainductive
program synthesis problem [ 28,52].(ii)Can an operator still be
usedfor modelgenerationevenifitsoperatorrule is not available?
We /f_ind it feasibleby inserting a “concrete” operator initialized
by recorded invocation traces. To make use of both symbolically
andconcretelyobtainedoperators,wecanapplya concolicmodel
generationapproach to constructmodels withboth sources.
Summary. This work makesthe following contributions:
•Inthiswork,wepresenttheurgencyforimprovingAPIdiversity
of model generation and formally introduce the essential proper-
ties for generating valid DNNs – operator rules . Furthermore, we
open the /f_irst proposal of automatically inferring operator rules
for diversifying andscalingvalid modelgeneration.
Figure 2:The symbolicview of avg_pool2d .
•Webuild NeuRI(NeuralNetworkSynthesisvia RuleInference),a
fuzzerfortestingDLsystemswiththreesteps:(i)aninstrumenter
thatcollectsandaugmentsAPIinvocationsfromvarioussources;
(ii) an optimized rule synthesizer that eﬃciently infers operator
rules with inductive program synthesis; and (iii) a hybrid model
generator that compiles both symbolic and concrete information
for producing valid anddiverseDNNs.
•We extensively and rigorously evaluated NeuRI. Within four
months,NeuRI/f_inds 100 newbugs for PyTorch and TensorFlow,
with 81 /f_ixed or con/f_irmed. 9 of the PyTorch bugs are labelled as
highpriority orsecurityvulnerability ,constitutingaround10%of
all high-priority bugs in PyTorch’s bug tracker of the period. By
evaluating branch coverage, NeuRIimproves the state-of-the-art
model-level fuzzerby15%(PyTorch)/ 24%(TensorFlow).
2 OPERATOR RULES
The functionalityofa deep-learningmodel ( i.e.,DNN) canberep-
resented as a list of operations, each of which transforms one or
multiple input tensors ( i.e.,multi-dimensional arrays) to output
tensors. Accordingly,a test-case inDL systems constructs aDNN
and is evaluated over some computational inputs, expecting the
modelcan be successfully executedandproduce correctresults.
For generating eﬀective test-cases automatically, it is crucial
togenerateanddiversifyvalidDNNmodels.State-of-the-art NN-
Smith[29]constructsvalidDNNswithoperatorconstraintsand
shape propagation rules. With SMT solvers, such rules can help
staticallyconstructanoperatorwhichcanbesafelyinsertedtoa
given model. Because in DL frameworks such operator rules are
implicitde/f_inedandcannotbeexporteddirectly,theyaremanually
speci/f_ied in NNSmith . However, crafting them from scratch is un-
scalable.Forexample,inthe/f_irst-yeardevelopmentof NNSmith ,
onlyaroundsixtyoperatorsareimplementedwithrules,despitethe
factthatmanyrulesareevenrepetitive.Asaresult,fordiversifying
operatorsbeingusedandsavingmanualeﬀortofdomainexperts,
we aim at inferring those operator rules automatically . We now
formalize andelaboratethe operator rules:
Symbolizing operators. As is shown in Figure 2, an operator is a
function which takes input tensors ( e.g.,input) and con/f_igurations
(e.g.,kh) as arguments. The con/f_igurations, also known as operator
attributes, describe high-level semantics for performing an oper-
ation and can impact the operator rules. For example, kwandkh
de/f_ine the size for applying the “avg” /f_ilter over the input image
whichmustbenosmallerthanthekernelsize(assumenopadding).
Forbeingevaluated statically,operatorrulesonlyleverageandsym-
bolize an operator’s compile-time information: (i) operator type
(e.g.,avg_pool2d ), (ii)/u1D43Cwhichisalistofinputshape vectors,and
(iii)/u1D434asthesetofoperatorattributes.Runtimeinformationsuch
as the detailed element values inside the input tensors is too costly
to bemodelled.Meanwhile, we use /u1D442to denotetheoutputshapes
producedbythe shape propagationrule.
Rule #1: Input constraints. The input constraints of an operator
are a set of predicate functions C={/u1D4501,/u1D4502,···}over/u1D434∪/u1D43C. For
658NeuRI: DiversifyingDNN GenerationviaInductive RuleInference ESEC/FSE ’23, December3–9, 2023,San Francisco, CA, USA
example, constraints in avg_pool2d require the kernel size to be
nolarger thanthe paddedimagesize ( e.g.,/u1D456/uni210E+2×/u1D44Epadh), namely:
/u1D450/u1D458/parenleftbig/u1D434={/u1D44Ekh,/u1D44Epadh,···},/u1D43C=[[/u1D456/u1D450,/u1D456/uni210E,/u1D456/u1D464]]/parenrightbig=/u1D44Ekh≤/u1D456/uni210E+2/u1D44Epadh
Theargumentsof /u1D450/u1D458consistoftheattributesto avg_pool2d and
theshapelistwiththeshapeoftheonlyinputtensor( i.e.,|/u1D43C|=1).It
is worth noting that for clarity we assume the input is a non-batch
image with only three dimensions ( i.e.,the channel, height and
width);however,inpractice,2d-poolingalsoacceptsbatchedinputs
withan extra batchdimension.Meanwhile,someoperatorscould
takea variablelength ofinputs ( e.g.,concatenate)oroutputs( e.g.,
split). For being general, a predicate may not assume the tensor
signature,i.e.,# of input/output tensors and their ranks, to be /f_ixed.
Thus,operatorruleswithsuchpatternsmayneedtobedescribed
with conditional branches ( e.g.,the syntax of PyTea [ 25]). We later
in §3.2introduce how to leverage partial operators to simplify such
branches whichare diﬃcult to handle inruleinference.
Rule #2: Shape propagation. Because evaluating Rule 1 requires
knowingthedimensionsofoperatorinputs,whichareoutputsof
other operators fromthe DNN underconstruction, output shapes
of operators also need to be evaluated. The shape propagation rule
for anoperatorcanbedescribedby afunction Pover/u1D434∪/u1D43C, which
returnsalistofpropagatedoutputshapesas /u1D442.Forinstance,the
shape propagationfor avg_pool2d can be describedas:
P/parenleftbig/u1D434={/u1D44Ekh,/u1D44Epadh,···},/u1D43C=[[/u1D456/u1D450,/u1D456/uni210E,/u1D456/u1D464]]/parenrightbig=[[/u1D45C/u1D450,/u1D45C/uni210E,/u1D45C/u1D464]]
where 
/u1D45C/u1D450=/u1D456/u1D450
/u1D45C/uni210E=/floorleftBig/u1D456/uni210E+2×/u1D44Epadh−/u1D44Ekh
/u1D44Estride+1/floorrightBig
/u1D45C/u1D464=/floorleftBig/u1D456/u1D464+2×/u1D44Epadw−/u1D44Ekw
/u1D44Estride+1/floorrightBig(1)
For example,given inputshapeof [3,3,3],we can tellthe corre-
spondingoutputshapeforoperator inFigure 2(assuming /u1D44Estride
is1)is[3,2,2]withoutinvokingit.
3 APPROACH
Figure3showsthe overviewof NeuRI’swork/f_low.
•NeuRIimprovesthesearchspaceofmodelgenerationbymaking
useofconcreteinvocation.Tocollectthose desiredinvocations
of tensor APIs, we instrument various sources such as developer
tests. Next, we /f_ilter out invocations that do not meet properties
suchasdeterminism,tofacilitateruleinferenceandbugdetec-
tion in later phases. For the convenience of rule inference, we
summarizetheinvocationrecordstoasimpli/f_iedstructureand
further augment data diversityviamutation (§ 3.1).
•These records are discrete data points, with which we can induc-
tivelysynthesizearithmeticexpressionsintheircorresponding
operatorrules.Theinductiveprogramsynthesisproblemis,how-
ever, an NP-hard [ 24] problem, whose complexity rests with
the grammar under enumeration. For aﬀordability, we split a
complete operator rule into multiple sub-rules, in order to be
describable by a simple arithmetic grammar. Furthermore, we
prune the enumeration space over equivalence andrarity, and as
a shortcut, reuse rules when possible. Additionally, redundant
inputconstraintsare removedfor runtimeeﬃciency.•Next we apply Hybrid DNN Generation which performs DNN
generationover(i)symbolicoperators, i.e.,thosewithoperator
rules;and(ii)concreteoperators, i.e.,thosewhoserulesarenot
inferred but with concrete invocation records. To achieve this,
we perform concolic operator insertion where symbolic operators
areinsertedwithSMTsolvingwhileconcreteonesareinserted
by searching a compatible tensor type ( i.e.,shape and data type).
•Lastlythegeneratedmodel,aftermaterialization,iscross-checked
between the interpreter andcompilerviaoracles in§ 3.4.
3.1 Instrumentation
Invocation collection. Following prior work [ 14], we instrument
the desired APIs to store successful invocations locally. In contrast
toFreeFuzzwhichcomprehensivelyinstrumentsAPIsfromboth
tensorandhighlevels,wefocuson tensorAPIs sincehigh-levelAPIs
canbedecomposedtoaseriesoftensoroperations.Additionally,
thesetensorAPIsmustbedeterministicandvalue-independent(de-
tailedin§ 4).WeperforminstrumentationatPythonlevel( e.g.,over
the Python test-suite) since Python is commonly used as the front-
endofDLframeworks.Meanwhile,wesimplifythedisorganized
and super/f_luous raw invocation (Figure 4) by dropping concrete
tensorvaluesandtherebyonlypreservethetensortypes,functor
and other arguments. For instance of avg_pool2d (Figure2), the
layoutofits simpli/f_iedrecord2isillustratedinFigure 4.
Data augmentation via mutation. The robustness of inferred
rulesdependsonthequantityandqualityofrecords.Unfortunately,
by only using instrumented records, each rule on average only
shares5-7records,whichareinsuﬃcient.Moreimportantly,allcol-
lected records are passing examples ; however, counter examples are
alsorequiredforinferringinputconstraints(§ 3.2).Consequently,
wefurtherdiversifytherecordsbymutatingexistingrecords,where
validmutantsareusedas passingexamples (denotedbyR✓)andin-
validonesareusedas counterexamples (denotedbyR✗).Speci/f_ically,
weperformthreephasesofmutationsoverinputshapedimensions
andattributes ofrecords( i.e.,/u1D434∪/u1D43C):
(1)Oﬀset-based : The goal of oﬀset-based mutation is to quickly
build a large set of (preferably) passing examples. To achieve
this, we enumerate subsets over /u1D434∪/u1D43C, for each of which we
increment the elements by 1 until a desired number of records
(e.g.,100 in our experiments) or time budget runs out. The
hypothesis behind is validity locality : oftentimes validity is
preservedafter alight-weightmutation ( i.e.,increment by1).
(2)Swapping :Exchangingtwovaluesfrom /u1D434∪/u1D43Ccanquicklyverify
simple inequalities. For example, assuming /u1D44E>/u1D44Fholds in all
collectedpassingexamples,weinvalidatetheinequalityifthe
recordisstillvalid after exchangingthe valuesin /u1D44Eand/u1D44F.
(3)Specialvalues :Lastly,werandomlyassignattributeswithspecial
values(e.g.,0/-1) inorder to test attributes’negativity.
Meanwhile ifno counter examples are produced after suﬃcient
mutation,itmeansprobabilisticallyithasno(orextremelyweak)
inputconstraints.Consequently,forcounter-example-freeopera-
tors, we directly assign an empty set as its input constraints ( i.e.,
no need to infer input constraints with inductive synthesis in § 3.2).
2Forclarity,weuse“record”torepresent“simpli/f_iedinvocationrecords”fromnowon.
659ESEC/FSE ’23, December3–9, 2023,San Francisco, CA, USA JiaweiLiu, JinjunPeng,Yuyao Wang,andLingming Zhang
Rule SynthesisUnit Tests
Model Hub…
Filter
Simplify
AugmentInvocationsTrace Tensor APIs
§3.1  Instrumentation
Summarized
RecordsExpr Prune
Rule Reuse
Deduplicate
§3.2  
Rule InferenceShape
PropagationInput
ConstraintsRules
Invocations
ForwardBackward
§3.3  Hybrid DNN Generation
Concolic Op Insertion
GraphIR ModelsInterpreter
CompilerInconsistent
Results
Runtime
Error
Sanitizer
Error
§3.4  OraclesBug 
Reports
Figure 3:Overview of NeuRI
Figure 4:Layoutofrecordsbefore andafter simpli/f_ication.
3.2 RuleInference
Wenowexplainhowtoperformruleinferenceviainductivepro-
gram synthesis and make it aﬀordable with a line of optimizations.
Inductive synthesis of operator rules. Input constraints and
shape propagation rules can be described by functions (over /u1D434∪/u1D43C)
whose bodies are oftentimes arithmetic expressions (§ 2). Speci/f_i-
cally,we can de/f_inesuch an arithmetic grammar Gas follows:
⟨expr⟩::=⟨op⟩⟨expr⟩⟨expr⟩ | ⟨item⟩
⟨op⟩::=+|-| × | ÷ | min|max|mod
⟨item⟩::=⟨symbol⟩ | ⟨constant⟩
⟨symbol⟩::=Symbolsfrom /u1D43Cand/u1D434
⟨constant⟩::=Constantintegers
Withthegrammar,wecaninferanoperatorruleviainductive
program synthesis, i.e.,by enumeratingGto /f_ind an expression
thatmatches inputs/outputsoftherecords,incertain timebudget
and program size. Because expressions in operator rules tend to be
short,weperform bottom-up enumerativesearch[ 10,49]which/f_irst
constructssmallterms( e.g.,⟨item⟩)andcomposethemgraduallyfor
generatinglargerones.Forclaritywedenotethesetofenumerated
expressions to be E. Meanwhile, there are a few hypotheses to
consider, e.g.,|/u1D434|,|/u1D43C|and|/u1D442|areassumedbe/f_ixed.Wewilldetail
themlaterinthe “partialoperator” paragraph.Formula2describes a shape propagation rule with a set of ex-
pressionsforcomputingcorrespondingoutputdimensions.Toinfer
thepropagationexpressionforthei-thoutputdimension( i.e.,/u1D45C/u1D456),
we enumerateEuntilexpr/u1D458∈Eis found to match all records (For-
mula3).Otherwise,wesay therules arenotinferredandwillnot
be usedfor inserting symbolicoperators duringmodelgeneration.
P/simequal{/u1D45C1=expr1(/u1D43C,/u1D434),···,/u1D45C/u1D45B=expr/u1D45B(/u1D43C,/u1D434)|expr/u1D456∈E}(2)
∃expr∈E,∀⟨/u1D434★,/u1D43C★,/u1D442★⟩∈R✓,expr[/u1D434★∪/u1D43C★//u1D434∪/u1D43C]=/u1D45C/u1D456[/u1D442★//u1D442](3)
Similarly, input constraints Care predicates of equalities and
inequalities, which can be normalized to 0=exprand0<expr
respectively.Algorithm 1illustrateshowC,startingwithanempty
set(Line2),isinferred.Byenumeratingpredicatesorientedfrom E
withinthetimelimit(Line 3-4),we/f_indpredicatesthataresatis/f_ied
byallpassingexamples(Line 5).Speci/f_ically,ifanypassingexample
does not match the predicate under enumeration ( i.e.,/u1D450) (Line6),/u1D450
isthenundesired,andconsequentlywerestarttheloopforthenext
predicate(Line 7).Otherwise,weinclude /u1D450inC(Line8).Meanwhile,
forsoundnessinputconstraintsshouldrejectinvalidinputs(ifany).
Consequently,Cshouldrejectall(ifany)counterexamples(Line 9-
10). Otherwise,the ruleisnot inferred(Line 11).
Algorithm1: Inference of inputconstraints C
1Function InferInputConstraints( E,R✓,R✗):
2C←∅
3LABEL:for/u1D450∈{0=expr,0<expr;∀expr∈E}do
4 iftimeoutthen break
5 for⟨/u1D434★,/u1D43C★⟩←R✓do
6 ifevaluate(/u1D450[/u1D434★∪/u1D43C★//u1D434∪/u1D43C])is falsethen
7 continue LABEL// Go to next /u1D450at Line 3
8C←C∪{ /u1D450}
9for⟨/u1D434★,/u1D43C★⟩←R✗do
10 ifevaluate(/logicalandtext.1C[/u1D434★∪/u1D43C★//u1D434∪/u1D43C])is truethen
11 raiseInference failure
12returnC
Partial operator. InNNSmith , operator rules are directly written
inPython,whosegrammarismuchmorecomplicatedthan G.Run-
ninginductiveprogramsynthesisoversuchacomplexgrammar,
660NeuRI: DiversifyingDNN GenerationviaInductive RuleInference ESEC/FSE ’23, December3–9, 2023,San Francisco, CA, USA
Figure5:Examplesofsimilarbutdistinctpartialoperators.
“f32[3]” stands for a /f_loat32 tensor variable whose rank is 3.
though being more capable, is impractical. To preserve a grammar
as simple asG, we split an operator into multiple partial operators ,
by “/f_ixing” components whose variation incurs a more compli-
catedgrammar.Wecanempiricallysummarizesuchcomponents
for de/f_ining partial operators. For example, branches in operator
rulesareusedforhandlingvariablelengthsofinputsorinputranks.
Additionally,rulesofoperatorswithdimension-sensitiveattributes,
e.g.,diminmax(x, dim) , often requires array operations. With
Figure5, in addition to API names ( e.g.,1and2), we identify a
partialoperator withthe following properties:
(1)Tensorsignature :Recall§2thatanoperatorcouldtakeandreturn
avariable length oftensorsin variousranks. Because incorpo-
rating such variability is costly, we let each partial operator
havea/f_ixedtensorsignature(andthusa/f_ixedformof /u1D43Cand/u1D442).
Forexample, 2and3havethesameAPInamebutarediﬀerent
partial operators for having diﬀerent input/output ranks ( i.e.,
3versus4).Itisalsoworthnotingthatwedonotdistinguish
partial operator over the data types of input/output tensors
whichare often orthogonalto the operator rules.
(2)Symbolicattributes :Besidesinputtensors,weregardotherargu-
mentsthatcanbesymbolizedto symbolicintegers assymbolic
attributes , which are the free variables in operator rules ( i.e.,/u1D434
in §2). Therefore, invocations with diﬀerent sets of symbolic
attributes are associatedwithdiﬀerentpartialoperators.
(3)Other arguments : We further classify the rest of arguments
(i.e.,non-tensor and non-symbolic-integer) into two categories:
(i)rule-orthogonalarguments( e.g.,/f_loat-pointscalarssuchas
“bias”)and (ii)(likely-)rule-dependentarguments( e.g.,image
layoutin“NCHW”or“NHWC”).Only (ii)willbeusedfor iden-
tifying partial operators for its potential impact on operator
rules.Ingeneral,thesub-categoryofan otherargument isde-
termined by its type and value. For instance, in Figure 5, the
ceilargument, as a boolean, falls into the (ii)category, which
makes 3and4diﬀerentpartialoperators.Infact, ceilimpacts
the shape propagation rule of avg_pool2d , where being true
makestheoutputheightandwidthroundedby ceilinsteadof
/f_loor(see Formula 1). Further details willbe elaboratedin§ 4.
Pruning.Ecanbetoolargetoenumerate.Foreﬃciency,weprune
semantic-equivalentduplicates( i.e.,equivalence ),aswellasthose
thatareuncommoninoperatorrules( i.e.,rarity ).Speci/f_ically,we
listthe pruning methodsintheirorder ofbeing applied:
(1)Bound: Without constraints, Eis in/f_inite. Therefore, we bound
Ebylimitingthenumberof ⟨/u1D45C/u1D45D⟩andthesetofconstantliterals.For example, in our default experimental setting the maximum
numberof⟨/u1D45C/u1D45D⟩beingusedis5andweuse ⟨/u1D450/u1D45C/u1D45B/u1D460/u1D461/u1D44E/u1D45B/u1D461⟩←{1,2}.
Additionally, expressions describing inequality are further lim-
ited to have at most one ⟨/u1D45C/u1D45D⟩because(i)inequalities are of-
tentimes simple; and (ii)a larger upper limit will lead to many
false-positives as inequalities are more /f_lexiblethanequalities.
(2)Rarity:Weempiricallypruneexpressionswiththesamesym-
bolsoccurringmorethanonce,whichareuncommon.Those
withconstantsub-expression( i.e.,⟨/u1D45C/u1D45D⟩⟨/u1D450/u1D45C/u1D45B/u1D460/u1D461/u1D44E/u1D45B/u1D461⟩⟨/u1D450/u1D45C/u1D45B/u1D460/u1D461/u1D44E/u1D45B/u1D461⟩)
are alsoprunedfor being constant foldable.
(3)Equivalence :We/f_indsemantically equivalentexpressionsin E
and only keep the simplest one. Speci/f_ically, we leverage a two-
pass approach inspired by Ruler [ 34]: First, for the expressions
withthesamefreevariables,we quicklyevaluatethemovera
number of randomly generated assignments and group them
according to the outputs ( i.e.,often known as characteristic
vectorsor/f_inger-prints ).Foreachgroup,wethenrigorously/f_ind
equivalentsbyapplyingan SMTprover.
Emayvaryfordiﬀerent /u1D434∪/u1D43C.WhilepruningEforeachpartial
operator is costly, we make it one-time eﬀort by: (i) pruning E□
with “holes” ( i.e.,symbol placeholders); and (ii) extending E□toE
byreplacingtheholeswithactualsymbolsofeachpartialoperators.
For example, assume that E□is the pruned set of expressions with
holes of{□1,□2,□3},i.e.,⟨symbol⟩::=□1|□2|□3. To infer an
operator rule with /u1D434∪/u1D43C←{/u1D4601,···,/u1D4604}, we get the actual Eby
extendingE□,bymappings{/u1D4601,···,/u1D4604}to{□1,□2,□3}invarious
ways.Morespeci/f_ically,foreach exprwith/uni210Eholes(symbols),we
select/uni210Esymbols from /u1D434∪/u1D43Cto “/f_ill” the holes ( i.e.,substitution).
Because of the one-time-occurrence hypothesis, each symbol in
/u1D434∪/u1D43Cwill not be selected to /f_ill multiple holes ( i.e.,injective). In
addition,themappingfromtheselectedsymbols(in /u1D434∪/u1D43C)toholesis
determined according to the relative order of indices. For example,
for□1÷□2, by selecting{/u1D4601,/u1D4602}we only get /u1D4601÷/u1D4602. Why not
consider permutation over the mappings? Consider if we allow
/u1D4602÷/u1D4601as an extension, when extending □2÷□1(indices swapped)
we get the same duplicated expression. As a result, for each expr
with/uni210Eholes, we can extend/parenleftbig|/u1D434∪/u1D43C|
/uni210E/parenrightbigexpressions. Meanwhile, when
|/u1D434∪/u1D43C|is smaller than the maximum number of holes, we only
considerextending exprwhere/uni210E≤|/u1D434∪/u1D43C|.
Rule reusing. Partial operators can share equivalent rules. Before
running inference from scratch, we can /f_irst test if the records can
bematchedbyalreadyinferredrules(iftheysharethesameformof
/u1D434,/u1D43Cand/u1D442).Ifanexistingrulecanbematched,wecansimply“copy
and paste” it for the new Partial operator as a short-cut; otherwise,
we can still run inference from scratch. Furthermore, since this
optimizationisorthogonaltothegrammar,wecanalsoreusethose
expert-craftedrulesfrom NNSmith (Python grammar).
Algorithm2: Predicate deduplicationfor C
1Function Deduplicate(C):
2repeat
3 for/u1D450∈Cdo
4 ifProve(/logicalandtext.1[C]⇔/logicalandtext.1[C−{/u1D450}])then
5 C←C−{ /u1D450}
6untilCunchanged
661ESEC/FSE ’23, December3–9, 2023,San Francisco, CA, USA JiaweiLiu, JinjunPeng,Yuyao Wang,andLingming Zhang
Deduplication. Theinferredinputconstraintscouldhavemanyre-
dundant predicates, which slows down SMT solving when fuzzing
online and makes it less readable. Therefore, Algorithm 2dedu-
plicates the predicates obtained from Algorithm 1. We each time
remove a predicate /u1D450ifCis equivalent to that without /u1D450(Line4-5).
We run thealgorithm until a /f_ixed point whennopredicatesfrom
Care removedafter an iteration.
3.3 HybridDNN Generation
Following the Algorithm 1 in NNSmith [29], the generation of a
DNN can be regarded as a problem of inserting a validoperator
correctlytoanalready validDNNmodel.In NNSmith ,sincealloper-
atorshastheircorrespondingrulesimplementedbydomainexperts,
a DNNmodelis synthesized symbolically ,i.e.,allshapedimensions
and attributes are viewed as symbols during construction and later
materialized by a set of assignments oﬀered by the SMT solver. For
NeuRI, we adopt a concolic[17,43] style of DNN generation in
order incorporateboth symbolic andconcrete operators3.
Speci/f_ically, the hybrid DNN generator inserts operators concol-
icly such that each operator, after insertion, is immediately con-
cretizedbythemodelfromthesolver,insteadofdeferringitwhena
fullDNNisbuilt.Therefore,theDNNunderconstructionisalways
concrete, i.e.,all shape dimensions, data types and attributes are
concreteatconstructiontime,whichmakesitapplicabletoinserta
concreteoperator.Thereareafewbene/f_itswithaconcreteDNN: (i)
the insertion of concrete operators can be eﬃciently implemented
by looking up compatible tensor types between traced records and
themodelunderconstruction;and (ii)theoreticallySMTsolving
isincurredlessintensively(thusfaster)withlesssymbols.In NN-
Smith, an operator can be inserted in two directions: 1) forward
insertionthat inserts anoperator which consumesexisting values;
and 2)backward insertion that lets an operator be a producer by
occupying existing placeholders. Next, we elaborate how symbolic
andconcreteoperatorsare forwardinsertedandforclarityomitthe
detailsforbackwardinsertion,whichcanberegardedasareversed
versionover the placeholders(instead ofarbitrary tensors).
Inserting a symbolic operator. Both the manually written op-
eratorrules( i.e.,NNSmith )andinferredones( i.e.,NeuRI)canbe
used to insert symbolic operators. Operators in both groups are
selected separately in equal amount of probability. Each time, to
insert a selected operator /u1D719, we /f_irst enumerate arity-sized com-
binations of tensor variables as the input candidates to /u1D719, where
each of them must respect the data type and rank requirements of
/u1D719. Taking the batched 2D-convolution as an example, whose input
tensormusthavefourdimensions,anyothertensorswhoserank
is not four will not be taken into the enumeration. Next, each of
theinputcandidatetuplesischeckedbytheinputconstraints C/u1D719
until one satis/f_iesC/u1D719and thus becomes the tensor inputs Ito/u1D719.
Furthermore,weasktheSMTsolvertoprovideamodelfromthe
inputconstraintsandusetheassignmentsofoperatorattributes /u1D434★
toinitialize /u1D719★←/u1D719[/u1D434★//u1D434].Wetheninsert /u1D719★,takingIasinputs,
tothe DNNunderconstruction. We alsopropagateitsoutputten-
sortypeswiththeshapepropagationrule(Formula 2)formaking
futureinsertionfeasible(§ 2).Ofcourse,ifnoneofthecandidates
3Asymbolic operator is an operator with inferred rules; while a concrete operator does
not haverulessuccessfullyinferred butstill has its corresponding validated records.canmaketherulesatis/f_iable,the insertionof /u1D719willbediscarded
andthe algorithm willre-try anotheroperator.
Inserting a concrete operator. The feasibility of inserting a con-
creteoperatorisassimpleas/f_indinganintersectionoftensortypes
between inputs ( i.e.,/u1D43C) in records and visible variables in the work-
ingDNN.Forexample,givena avg_pool whichhasinputtensor
type offloat32[1,3,224,224] in the records, if there is a ten-
sor variable with a shape of [1,3,224,244] and a data type of
float32 in the DNN under construction, we can safely insert it to
thetargetplace.However,becauseofthelargevolumeofrecords,
checkingthesatis/f_iabilityoperatorbyoperatorandrecordbyrecord
isineﬃcient.Instead,wecanbuildamappingfromtensortype( i.e.,
shapeplusdatatype)toasetofpartialoperators,anyofwhichhas
aninputtensorofsuchatype.Then,byaccumulatingthesetofpar-
tialoperatorsmappedfromtensortypesavailableintheworking
DNN,wegetareducedsetofoperatorcandidateswhichexclude
thosewithunsatis/f_iableinputtypes.Consequently,weonlyneed
toenumeraterecordsofreducedsetsofpartialoperators.Oncea
recordiffound tobematchable,weinitialize thepartialoperators
withthe recordandinsert itto the DNN underconstruction.
3.4 TestOracle
In this section, we listthree test oracles for manifesting bugs.
Resultinconsistency. InadditiontorunningtheDNNs eagerly
with the pre-compiled library functions ( i.e., interpreter ), Tensor-
Flow and PyTorch can further optimize the models via compilation
for better performance. Hence, we cross-check the results obtained
by running the same model and inputs from the interpreter and
compiler,where asubtle/f_loating-pointerrorisallowed.
Runtimeerror. Weidentifyaruntime-errorbugifthecompilation
orexecutionofamodelabortsunexpectedly.Thecorresponding
symptoms include a crash or an unexpected Python exceptions
not incurred by incompatibility ( e.g.,“Not-implemented” error).
Furthermore, interpreter exceptions are not considered as bugs as
itcould be causedthe use of an incorrectoperator rule.
Sanitizer error. We also enrich the test oracles with sanitizers,
such as ASan [ 44] (memory error), UBSan [ 7] (use of compiler’s
unde/f_inedbehavior),andCSan[ 3](CUDAerror).Sanitizererrors
are reported by sanitizer-injected checkers at runtime. Without
sanitizers,bugsmaynotmanifestthemselvesviaacrash( e.g.,buﬀer
over/f_low) or occur at a late stage, making debugging challenging.
4 IMPLEMENTATION
NeuRIimplementsthreecomponentsincludinganinstrumentation
tool, a rule synthesizer and a fuzzer, via a total eﬀort of 14.9k LoC.
APIinstrumentation. The instrumentationtool is implemented
with 1.8k LoC in Python. The instrumentation is performed by
insertinganAPI-hijackingcodesnippetinthe __init__ /f_ilesofDL
framework packages. Speci/f_ically, we run the instrumentation over
thedevelopertestsfromtheopen-sourcerepositoriesofPyTorch
and TensorFlow. As soon as the DL packages are imported, the
API-hijacking code adds a function wrapper to all functions within
the package. During execution ( i.e.,running the regression tests
ofDLframeworks),theinvocationsnapshotstodesiredAPIsare
serialized for reproduction. In post-processing, a /f_ilter is applied to
removeinvocationsthatdonotcomplywith determinism andvalue
662NeuRI: DiversifyingDNN GenerationviaInductive RuleInference ESEC/FSE ’23, December3–9, 2023,San Francisco, CA, USA
independence .Todetect determinism ,eachinvocationisreplayed
forthreetimesforcheckingoutputconsistency.Fortesting value
independence ,i.e.,the operator rules are independent to the val-
ues/elementsintheinputtensor,eachAPIistestedbythreegroups
of randominputs(initialized from −106to106)andisexpected to
output the same tensor types without runtime failure. This compo-
nentalsoincludesutilitiesforparsingandcomposing/replayinga
DL API, inorder to constructnewinvocation.
Rulesynthesizer. Weimplementedthe synthesizer in1.5k LoC
in Python. Before the actual rule synthesis, we /f_irst apply data
augmentationforenrichingtherecordsondemand( i.e.,until100
records for each partial operator). In § 3.2it is found that not all ar-
gumentsimpacttherules,consequentlyweidentify rule-orthogonal
argumentsinapartialoperatorthroughtheargumenttype:fora
/f_loating-pointargument( e.g.,bias)weassumeitdoesnotimpact
operator rules. Furthermore, the arithmetic expressions are rep-
resented as binary trees and for memory eﬃciency smaller trees
arere-usedtocomposelargertreesviapointersinthebottom-up
enumerativesearch.Weconstrainthemaximumnumberof ⟨op⟩to
5 (i.e.,6 symbols atmost). Asa result,as a one-time eﬀortwe /f_irst
enumerateGbyregardingthesymbolsas6“holes”andpruneiton
the /f_ly to getE□. Meanwhile, we also leverage commutativity of
{+,×,min,max}toskiptheenumerationof operandswappingand
associativityover{+,−,×,÷,min,max},inordertoacceleratethe
equivalence-based pruning. With E□obtained as a one-time eﬀort,
for anynewoperatorruleunderinference,we canquicklyextend
E□toEby/f_illingitsactualsymbolsintothe“holes”asdiscussed
in §3.2. Speci/f_ically, for each partial operator we set a timeout of
1000 secondsto inferthe shape propagationorinputconstraints.
Fuzzer.The fuzzing engine of NeuRIis built by extending the
NNSmith prototype [ 1] with 11.6k new LoC (and removing 7.9k
oldLoC).Majoreﬀortsarespenttoimprovetheextensibilityand
debuggibility of the original NNSmith for bene/f_iting algorithm
prototyping and bug /f_inding. Previously NNSmith uses directed
multi-graphsin networkx [21]fordescribingDLmodelsinternally.
However, the graph data structure is not suitable for manipulating
model structures and being translated to real-world model formats.
Additionally, DL models are fundamentally programs [ 47] which is
notnecessarilyalwayspuredata-/f_lowgraphs.Forexample,in-place
operators for reproducibility require a total order during execu-
tion whereas traversinga graph cannot guarantee. Asa result, we
buildanSSA-basedintermediaterepresentation,namelyGraphIR,
to describe DNN structures. Following the LLVM [ 27] interface
style,DNNmanipulationismadesafeandconvenientviathreefun-
damentalAPIsof insert,remove_unused ,andreplace_alluse .
Thankstotheextensibility,/f_ivegraphgenerationstrategiesused
in this paper, including three NeuRIvariants and two NNSmith
variants,are implementedinmerely1.1kLoC.
5 EVALUATION
We evaluate NeuRIbyasking following researchquestions:
•RQ1(§5.2):Howdoes NeuRIcompareagainststateoftheartin
DL compilerfuzzinginterms ofcode coverage?
•RQ2 (§5.3): How many APIs, partial operators and records
arecollectedandeventuallyinferredwithoperatorrules?Howeﬃcient and eﬀective is our rule synthesizer compared with
general-purpose program synthesis toolssuch as Rosette[ 48]?
•RQ3 (§5.4): How eﬀective is NeuRIwhen detecting previously
unknownbugsfor real-world DL compilers?
5.1 ExperimentalSetup
Systemsundertest. Wetesttheemerging compilers ofthemost
popular DL frameworks, i.e.,TensorFlow [ 9] and PyTorch [ 38],
whichfor clarity are denotedby“TF”and“PT” respectively.
(1)TensorFlow XLA compiler converts a TensorFlow model ( e.g.,
SavedModel)toitsgraph-levelIR( i.e.,HLO)forrunningvarious
optimizationpasses.TensorFlowde/f_inesover1500operators.
Of these, around 450 are supported by XLA. This is because
DL compilers often focus on a small set of primitive operators,
from whichotherhigh-level operators can be composed.
(2)PyTorch JIT , the PyTorch’s equivalent of XLA, supports around
1310 APIs (including alias, e.g.,torch.max(a) anda.max())
outofatotalofover 2000 PyTorch operators.
Metrics. We evaluate NeuRIover various metrics. Speci/f_ically, we
explainthe mostimportanttwohere anddefer the others.
•# Found bugs : We count the bugs at the basis of bug reports,
whichareclassi/f_iedtofourstatuses:1) /f_ixed:Apatchhasbeen
eﬀectivelyappliedto/f_ixthebug;2) con/f_irmed :Inadditionto/f_ixed
bugs,we conservatively (i.e.,lowerbound)identifyacon/f_irmed
bugiﬀit has been reproduced/diagnosed as a fault or directly
assignedtodevelopersfor/f_ixingit;3) won’t/f_ix:Developersclaim
the potential of not /f_ixing it ( i.e.,upper bound); and 4) the rest of
bugsare alltriagedbut require further investigation.
•Branch coverage : Following [ 29,31], we evaluate fuzzers with
branchcoverage,astrongercriterion(thanlinecoverage)fortest
adequacy[ 57], over DL frameworks’C++sourcecode.
Baselines. In end-to-end benchmarks, we compare NeuRIwith
thestate-of-the-artmodel-levelfuzzers(namely NNSmith [29]and
Muﬃn[19])andthestate-of-the-artoperator-levelfuzzer( i.e.,Deep-
REL [15]).For ablationstudy we alsoevaluate NeuRI’svariants.
•NNSmith performs model generation with over 60 operators
whoserulesare manually craftedbydomainexperts.Speci/f_ically,
theoﬃcial NNSmith performs puresymbolic generationwhere
theallsymbolsarematerializedtogetherwhenalloperatorsin
thegrapharesymbolicallyinserted.Inthispaperwealsopropose
concolic generation (§ 3.3), consequently we also implemented a
concolic version of NNSmith which immediately materializes
thesymbolsperinsertion.Becausetheconcolicvariantperforms
similarlyasthepure-symbolicversion,forclarityweomittedthe
resultsinevaluation.Onereasoncanbethatconcolicinsertion
does not bringmore operator supports in NNSmith asNeuRI.
•Muﬃn based on 11 seed models including DenseNet [ 23] and
LSTM [22], performs mutation-based model generation. Similar
toNNSmith ,itsupports over60operators byhand-craftingthe
shapeinferencerules.However,Muﬃn-createdmodelsarenot
guaranteed to be valid for the lack of input constraints. Notably,
Muﬃn is only implemented on TensorFlow. As a result, we only
compare Muﬃnagainst othersonTensorFlow.
•DeepRELisanoperator-levelmutation-basedfuzzersimilarto
FreeFuzz. As an improvement to FreeFuzz whose seeds purely
663ESEC/FSE ’23, December3–9, 2023,San Francisco, CA, USA JiaweiLiu, JinjunPeng,Yuyao Wang,andLingming Zhang
comefrominstrumentation,DeepRELextendstheseedinvoca-
tions by matching similar APIs and exchanging their arguments.
•NeuRI/u1D45Fis a variant of NeuRIwhere the use of inferred operator
rules is disabled. In other words, NeuRI/u1D45Fconstructs DNNs by ei-
therusingconcreteoperatorsdeterminedbycollectedrecordsor
symbolicoperatorsfromtheoriginal NNSmith andbothmethods
share equal probability for being selected.
•TheNeuRI/u1D456variant disables concrete operators and only uses
symbolic operators from automatedinference or NNSmith .
Inaddition,inRQ3wealsocompareourrulesynthesizerwith
Rosette, a solver-aided programming system, which supports in-
ductiveprogramsynthesis.Speci/f_ically,wegiveRosette Gasthe
grammarunderabitvectortheory.Thenumberofbitsforthedata
and operations is 32 given that the maximum number in records
is232−1(i.e.,INT_MAX).Next,foreachpartialoperatorweletits
recordsbetheconstraintsandrunRosettewitha1000-secondtime
budget.More speci/f_ically,we onlycomparewith Rosetteoverthe
inference of shape propagation, e.g.,given an partial operator with
koutputdimensions,bothRosetteand NeuRIwillrunktimeseach
of which trying to search expr/u1D458that matches records of /u1D45C/u1D458. We did
notinferinputconstraintsforRosettesincemultiplematchedpred-
icatescanbereturnedinonepasswhileRosettedirectlyterminates
when the /f_irstmatchedpredicate isfound.
Con/f_iguration. We run all experiments on Ubuntu 22.04 pow-
eredbya64-threadAMDThreadripperCPU,256GBofmemory,
and 4 TB of PCIe-4 SSD. Our approach is evaluated over the up-
to-date frameworks and versions: TensorFlow v2.12-nightly (git:
5a6fc06bf8 )andPyTorchv2.1-nightly(git: f7520cb51e ).Dueto
the diﬀerent tool-chain /f_lavours, we compiled TensorFlow with
GCC-12.2andGCOV[ 4],whilePyTorchiscompiledwithClang-14
anditssource-codebasedcoveragetool [ 6].To preciselymeasure
the test adequacy of compilation, for TensorFlow we instrument
/f_ilesintensorflow/compiler (over800kLoCs),andforPyTorch,
non-kernel-function /f_iles under pytorch/csrc andaten/are con-
servatively instrumented since PyTorch’s passes are “everywhere”.
Following prior work [ 29,31] we by default run fuzzing for four
hours and generate models with /f_ive operators to balance between
eﬃcacyanddebuggability(Figure 6c).Fordetectingresultinconsis-
tency,ouroracleusesabsoluteerrorof 10−3andrelativeerrorof
1%.Withmodelsgeneratedassmallas/f_ivenodes,wedidnotsee
false-positives brought bypropagated/f_loating-pointerrors.
5.2 RQ1: EvaluatingCoverage
Overall coverage. Figure6aand6bshow the coverage growth
(y-axis)infourhours(x-axis).Amongall model-level fuzzers,NeuRI
improves the prior SOTA ( i.e.,NNSmith ) by 24% on TensorFlow
and15%onPyTorch.Theresultsindicatethat NeuRIcansynthe-
size more diverse model structures to exercise various compiler
passes. In addition, NeuRIalso outperforms the SOTA operator-
levelfuzzer DeepREL by 7% on TensorFlow and 2% on PyTorch.
Though existing operator-level fuzzers [ 14,15,53] is known for
constructing invocations for thousands of APIs, NeuRIshows that
by only supporting a smaller but essential set of operators, similar
andevenbettercodecoveragecanbeachievedthroughmodel-level
generation.DespitethemarginalcoverageimprovementoverDeep-
REL,NeuRI,viamodel-levelfuzzing,canexplorevarioussystembehaviorsthatarenotcoveredbyDeepREL.Speci/f_ically,62.9%of
thePyTorchbugsand35.7%ofTensorFlowbugsfoundby NeuRI
aremanifestedbymodelsofmultipleoperatorswhichcannotbe
coveredbysingle-operatorfuzzerslikeDeepREL.Overall,infour
hoursNeuRIautomatically covered 10.8% and 17.4% of the com-
piler code branches in TensorFlow and PyTorch respectively. It
isworthnotingthatachieving10-18%of totalbranch coverageis
non-trivial,giventhecomplexityofDLframeworks.Asareference,
fuzzers[26]forLinux( i.e.,alsoamillion-LoCproject)commonly
achievearound10%ofblockcoverage.Themissingbranchescan
comefrompassesdesignedforotherhardwaretargets( e.g.,GPU
whereasCPUisusedinourexperiments)andunusedexperimental
compilation pipelines ( e.g.,MLIR).
Ablationstudy. Bycomparing NNSmith withNeuRI/u1D45FandNeuRI/u1D456,
weshowthatbothsymbolicandconcreteoperatorsareeﬀectivefor
improvingcoverage( 18.8∼20.9%forTensorFlowand 12.8∼13.5%
for PyTorch). By combining both of them together ( i.e.,NeuRI),
NeuRI/u1D45FandNeuRI/u1D456canbefurtherimprovedtocover 2.2%/1.3%
morebranchesforTF/PT.Thoughthisimprovementmightlook
marginal, we argue that the additionally covered branches are
harder-to-reach ones, as we later show that a certain number of
bugsareexclusivelycontributedbyinferredrules.Table 1shows
thatNeuRIhas a slightly lower validity rate ( <5.2%) and runs
17.6%(TF) /67.7%(PT) slower than NNSmith . The validity rate
is lower as some inferred rules are partially correct. The speed
ofNeuRIisofcourseslowerasitsmodelgenerationtacklesover
75×more symbolic operators than NNSmith and even much more
concrete operators.However, NeuRIstill achieves better coverage
asitgeneratestest-caseofhigherqualityoverquantity.Notethat
annon-prefectvalidityratedoesnotintroducefalse-positives,as
we identify and immediately discard an invalid test-case if it raises
exceptions ineagermode.
Speed.The time costs for generating, evaluating and saving a test-
case are shown in Table 2. It shows that constrain-based model
generationisfast,takinganaverageof88ms(TF)/69ms(PT)for
creatingandrunningatest-case,despitethesingle-threadnatureof
our model generator. Speci/f_ically, the generation phase on average
takes53%(TF)/86%(PT)ofthetime,whichisoveralldominated
by constraint solving. Notably, the SMT solving time is long-tailed:
theP99showsthatfor1%oftest-cases,thesolvertimedeteriorates
byover12.5×(TF)/15.5×(PT) comparedto the average.
Impact ofmodelsize. NeuRIandNNSmith bydefault generate
models with 5 nodes. How about other sizes? Figure 6cshows that
the model size used in NeuRIimpacts the coverage on PyTorch
(TensorFlow is omitted for clarity but shares similar trends). For
example,onlygeneratingone-operatormodels( i.e.,single-API)gets
worsecoverage,sincecompilerpasseslookformultiple-operator
patterns( e.g.,operatorfusion).Onthecontrary,thebene/f_itscon-
vergeswhenmodelsizegetslargerandlarger( e.g.,0.1%coverage
diﬀerencein#node5-13).Consequently,becausesmallermodels
areeasiertodiagnose,webydefaultuseamodelsizeof5in NeuRI.
5.3 RQ2: EvaluatingRuleInference
Statistics. Table3displays the statistics of APIs, partial operators
and records at diﬀerent phases. The “Collected” row indicates that
the developer tests ( i.e.,the instrumented code) incorporate 758
664NeuRI: DiversifyingDNN GenerationviaInductive RuleInference ESEC/FSE ’23, December3–9, 2023,San Francisco, CA, USA
0 50 100 150 200 250
Time (Minute)20253035# Coverage (1000 branches)36kbest/335kmax=10.8%
NeuRI
NeuRIrNeuRIi
DeepRELNNSmith
Muﬃn
(a)TensorFlow0 50 100 150 200 250
Time (Minute)363840424446
47kbest/272kmax=17.4%
NeuRI
NeuRIrNeuRIi
DeepRELNNSmith
(b) PyTorch0 50 100 150 200 250
Time (Minute)45.546.046.547.0
47kbest/272kmax=17.4%
#Node 13
#Node 9#Node 5
#Node 1
(c) Impact of model size.
Figure 6:4-hourcoverage trend offuzzing.
Table 1:Numberofvalidtests generated in4 hours.
TensorFlow PyTorch
%Validity #Tests %Validity #Tests
NeuRI 94.8% 108,572 98.9% 206,486
NeuRI/u1D45694.2% 78,083 98.6% 134,912
NeuRI/u1D45F98.1% 125,059 99.8% 409,872
NNSmith 100% 131,799 100% 639,434
Muﬃn 95.9% 302 — —
Table 2:Testing timebreakdown(millisecond).
Gen.(SMT) Eval. Save Total
TFAvgerage 67 (33) 21 38 126
P90 74 (37) 29 42 145
P99 673(411) 58 59 733
Percentage 53%(26% ) 17% 30% 100%
PTAvgerage 60 (38) 9 0 70
P90 45 (26) 11 0 56
P99 930(631) 16 1 942
Percentage 87%(55% ) 13% 0% 100%
(out of 1310) APIs supported by PyTorch JIT and 248 (out of 450)
for XLA respectively. 42-45% APIs are not collected due to the lack
oftests (e.g.,untestedaliased APIs)orbeing non-tensor APIs ( e.g.,
image encoder and decoder APIs). By focusing on these, around
63k / 34k records can be collected for PyTorch / TensorFlow. After
/f_iltering out unwanted records (§ 4), 47% (PT) / 38% (TF) of the
records remained for 90% (PT) / 86% (TF) of the APIs. Furthermore,
data augmentation improves the unique records by 15×(PT) /7.7×
(TF),outofwhich57%(PT) / 67%(TF)are counter examples.
Withinthe1000-secondbudgetperrule, NeuRIcaninfer76%(PT)
/84%(TF)ofrulesatthepartialoperatorleveland91%(PT)/90%
(TF)ofrulesattheAPIlevel(Table 3).Toestimatetheusefulnessof
rules,weuse“fuzzing⊤”todenotethenumberforAPIsthatareused
duringfuzzing( i.e.,NeuRI/u1D456inonenode).Toindicatecorrectness,
“fuzzing⊥” denotes those in “fuzzing⊤” that always construct valid
usages. It turns out 97% (PT) / 90% (TF) APIs out of the inferred
onescanbe usedand96%(both)tendtobeused validly,showing
the overalleﬀectiveness ofruleinference.
4The data points of input constraints are fewer because some partial operator have an
empty set of input constraintsand arenot visualized.Table 3:# API/partial operator/record at diﬀerentstages.
#API #PartialOp. #Record
PT TF PT TF PT TF
Collected 758 248 — — 63,136 33,973
Filtering681 214 5,875 1,79929,589 12,908
Augment. 1,041,459 303,314
Inference 620 192 4,475 1,507 — —
Fuzzing⊤604 185 4,186 1,434 — —
Fuzzing⊥582 176 4,144 1,415 — —
Figure 7:Inferencetimeand #symbols ofinferred rules4.
Table4:Numberofinferredshapepropagationrulesin1000s.
InferredTimeout Unsat.
<1s <10s <100s <1000s
NeuRI 4,660 4,700 4,716 4,758 994 123
Rosette 0 83 2,832 4,461 1,414 0
Scalability. Figure7depicts the distribution of inferred rules in
terms of the inference time and symbol size. It shows 95% of in-
ferred partial operators have less than 10 (PT) / 11 (TF) symbols,
indicating the problem size is small overall. Additionally, inferring
shapepropagationrulesishighlyaﬀordable( i.e.,95%ofthemcan
be solved within 17ms). However, input constraint inference tends
tobemorecostly,sincepredicatecandidatesarethoroughlyenu-
merated in Algorithm 1while shape propagation terminates when
the/f_irstfeasiblesolutionisfound.Meanwhile,theinferencetimeof
inputconstraintsgrowswith#symbolsbecausepartialoperators
withmore symbolsincur more expressionsto validate (Table 5).
Table4compares NeuRIandRosetteininferringshapepropa-
gation for partial operator in PyTorch. It shows that 79% of partial
operators are inferred by NeuRIwithin one second, while Rosette
takes over 10 seconds for 99% of cases. Speci/f_ically, the “unsat”
665ESEC/FSE ’23, December3–9, 2023,San Francisco, CA, USA JiaweiLiu, JinjunPeng,Yuyao Wang,andLingming Zhang
Table 5:Sizes ofexpressions after pruning.
|/u1D434∪/u1D43C|None Equiv. Rarity Both
14.77×1085.35×1063.54×10578
22.88×1099.04×1075.90×1061.25×105
31.11×10105.18×1085.22×1072.28×106
43.32×10101.91×1093.13×1081.94×107
58.36×10105.49×1091.40×1091.06×108
61.86×10111.33×10104.66×1093.89×108
Percent. 100% 7.15% 2.51% 0.21%
presents the number of cases where no expression is applicable
afterthefullenumeration.Rosettehaszero“unsat”rulesbecause
itfails to /f_inish the enumerationwithin 1000s.
Impact of pruning. Table5shows the number of expressions
afterbeingprunedbydiﬀerentmethods.Forexample,foranpartial
operatorwithsixsymbols( i.e.,|/u1D434∪/u1D43C|=6),withoutanypruning
therearehundredsofbillionsofexpressionstoverify( i.e.,1.86×1011,
recall that we have at most 5 operations). Our pruning method
(i.e.,the“both”)isabletoprunethesearchspacesmallerby 478×.
The ablation study shows that both equivalence andraritycan
suﬃcientlycontributetoshrinkingthesearchspaceevenifapplied
independently.Interestingly,wefoundthatourmethodpruneseven
betterpruningratioforfewersymbols, e.g.,1.64×10−5%unpruned
for one symbol and 0.21% unpruned for six symbols. This is a good
news since from Figure 7we see the number of symbols tend to be
small,i.e.,approximatelyclusteredwithin 1to 10.
Examples. We found most partial operators have very simple
rules being inferred, e.g.,70% (PT) / 51% (TF) of which only have
expressions with at most one symbol. Nevertheless, we show some
complicatedrepresentativestoindicatethecapabilityof NeuRI’s
rule synthesizer. Given an arbitrary input tensor x, say its shape
is[/u1D4561,/u1D4562,/u1D4563,/u1D4564],bothx.flatten() andtorch.ravel(x) /f_lattenthe
tensorintoa1-Darraywhoseshapeis [/u1D4561×/u1D4562×/u1D4563×/u1D4564],whichcanbe
inferred by us. For more complicated cases such as x.unfold(dim,
size, step) , the shape propagation of /u1D45Cdim=1+(/u1D456dim−size)
stepcan
alsobecorrectlylearnt5.Therearealsocaseswherepartiallycorrect
rules are learnt.Consider avg_pool3d(x, ksize=[kT, kH, kW],
pad=[pT, pH, pW]) where both the input and output have /f_ive
dimensions ( i.e.,[/u1D456/u1D441,/u1D456/u1D436,/u1D456/u1D447,/u1D456/u1D43B,/u1D456/u1D44A]→[/u1D45C/u1D441,/u1D45C/u1D436,/u1D45C/u1D447,/u1D45C/u1D43B,/u1D45C/u1D44A]). While
NeuRIcorrectly inferred that /u1D45C/u1D447=/u1D456/u1D447+2pT
kT, it over/f_its the H and W
dimension with /u1D45C/u1D43B=/u1D456/u1D43B
kH+min(1,pH), due to the lack of records
where/u1D456/u1D43B
kH+min(1,pH)≠/u1D456/u1D43B+2pH
kH.However,inour4-hourfuzzingthe
corresponding partialoperators were used and didnot leadto any
invalidmodels.Thisisbecausethisincorrectexpressionstillgets
agoodchanceofbeingvalid.Forexample, NNSmith [29]reveals
that SMT solvers like Z3 [ 11] tend to return boundary models, e.g.,
pH= 0 which makes the two equations equivalent. There are of
courseuninferred operatorrules.For example, one partialoperator
oftorch.stack takes hundreds of input tensors, where NeuRI
cannothandle hundredsoforientedsymbolsin1000s.
5.4 RQ3: Bug Finding
Overview. Infourmonths, NeuRIhasfound 100newbugs,with
51 /f_ixed and 81 con/f_irmed (Table 6). Links to all bug reports in this
5Divisiondemonstrated in thisparagraph is /f_loordivision, i.e.,for integers.Table 6:Overview ofreported bugsinfour months.
Symptom(§ 3.4)Total Con/f_irmed(Fixed) Won’tFix
PTInconsistency 19 19 (17) 0
Runtimeerror 43 32 (25) 2
Sanitizer error 20 13 (7) 0
TFInconsistency 7 6(0) 1
Runtimeerror 7 7(0) 0
Sanitizer error 1 1(0) 0
Total 100 81 (51) 3
work are included in our artifact6. Of these, 76 are found during
fuzzingand24arebyproducts( e.g.,crashesbycounterexamplesin
argumentation). Among the 85 PyTorch bugs, 8 have been labelled
withhighpriority ,constituting 10%(8/83)ofallhigh-prioritybugs
forthe entirePyTorchissuetrackerinfourmonths.Besides,one
PyTorch bug has been tagged with a CVE number when there was
only one other CVE published in PyTorch. PyTorch developers say:
“...the bugs you’ve reported are high quality , and ...
don’tlooklikespeciallyfuzzedsetthat’simpossibletosee
in practice. They didreveal a few common themes that
areeasy to encounter in practice...”Meanwhile, to report bugs responsibly [ 42], we discontinued
bug reporting to TensorFlow when none of our /f_irst 15 reports (14
con/f_irmed)were /f_ixedinamonth.Hence,the“15”bugsshouldbe
regardedas alower bound for bug/f_indingeﬃcacy inTensorFlow.
Uniquebugs. WeillustratethepatternsofexclusivePyTorchbugs
(sincewediscontinuedTensorFlowbug/f_inding)foundby NeuRI
during fuzzing ( i.e.,byproducts not included). Among these 62
fuzzing bugs in PyTorch, 39 bugs (62.9%) are only manifested by
models with multiple operators – these are not able to be detected
by prior single API fuzzers [ 14,15,53]. Meanwhile, 41 (66.1%) of
thebugswouldnotbecoveredby NNSmith foritslimitedAPIsup-
ports7. For example, torch.reciprocal(torch.dstack([1, 1,
1]))“concatenates” the only inputand getsthe reciprocal, which
shouldhavereturned [1, 1, 1] .However,aftercompilationthe
thirdoutputelementbecomesnon-deterministic.Thisiscon/f_irmed
tobeamiscompilationbug8(now/f_ixed)wheretheCkernel func-
tiongeneratedbyPyTorchhaserroneouspointeraliasesforinput
and output buﬀers. This inconsistency bug is neither detectable by
single-API testersnor NNSmith (unsupportedAPIs).
In addition, 17 bugs (27.4%) are exclusive to NeuRI/u1D456,i.e.,the
shapesandattributesofthebug-inducingmodelsarenotdirectly
obtainedfromtherecords,butbysolvingconstraintsfrominferred
rules. It shows that enabling rule inference, though not bringing
surprising coverage improvement (§ 5.2), does help /f_ind more bugs.
Forinstance,a high-priority bugdetectedby NeuRI9(now/f_ixed)re-
quirestheinputshapeof torch.histogramdd tobespeci/f_ically [5,
6]fortriggeringacompilerfailure.Theinputshape, i.e.,[5, 6],
comesfromthesolver-providedmodelandnoneofthesixcollected
records of torch.histogramdd can trigger the bug. As another
6https://github.com/ise-uiuc/neuri-artifact/blob/main/docs/rq3-bug-reports.md
7Muﬃn sharessimilar limitationsas NNSmith in termsof limitedAPI supports, and
isnot directlycomparableherebecause it only supports TensorFlow.
8https://github.com/pytorch/pytorch/issues/93078
9https://github.com/pytorch/pytorch/issues/93274
666NeuRI: DiversifyingDNN GenerationviaInductive RuleInference ESEC/FSE ’23, December3–9, 2023,San Francisco, CA, USA
example,the *-unfold-abs_ modelpattern(“ *”meansanyoper-
ators and the “ _” inabs_means it is an in-place operation) can
manifestaresultinconsistencybug10(now/f_ixed) sincethe graph
functionalization in the PyTorch compiler was not able to identify
certainoperatorpatternsthathavememoryoverlapping.Speci/f_i-
cally,itisdetectedbyusingasetofsolver-providedarguments, i.e.,
tensor.unfold(1, 3, 2) . Notably, both examples here require
operators that are unavailable in NNSmith , showing that operator
diversityfurther powers modeldiversityto detectmore bugs.
Furthermore, one heap-over/f_low bug is assigned by PyTorch
with a CVE identi/f_ication number ( GHSA-6655-44g2-4gc8 ) due to
its security impact. This bug is induced by searchsorted(arr,
val, sorted_idx) ,whichaimstobinarysearch valinarr(e.g.,
an array) where sorted_idx are the sorted indices of val. Speci/f_i-
cally,boundarychecksfor sorted_idx wereabsentintheprevious
implementation. Therefore, a large enough index, if “lucky”, can
lead to a segmentation fault, terminating the program without fur-
ther impact. However,a carefully designed index allows attackers
toaccessandstealdatafromothermemoryaddressesbesidesthe
array range when performingthe binary search.
“Won’t-/f_ix”bugs. Threeofourreportsarerejectedordeprioritized.
Forexample,bothaninconsistencybugin tf.cast[46]andacrash
bug intorch.linalg.eigvals [40] were rejected for using NaNs
asinput,incurringunde/f_inedbehaviours.AnotherPyTorchJITbug
wasdeprioritizedbecausedeveloperssuggestedustouseandtest
the newcompiler[ 41](andconsequentlywe did).
6 RELATED WORK
Inrecentyears,fuzzing[ 33]hasbeenextensivelystudiedfortesting
DLlibrariesandDLcompilers,whichcanbemainlycategorizedinto
operatorandmodellevels.Operator-leveltechniques[ 14,15,53]
aimtotesteachDLAPIinisolation.FreeFuzz[ 14],afullyautomated
operator-levelfuzzerfortestingDLlibraries(suchasTensorFlow
andPyTorch),collectsDLAPItracesfromsourcessuchasdeveloper
tests andmodelzoo,andfurther mutatesthe tracedinputstogen-
erateadditionalvalid/invalidtest-casesforfuzzingeachoperator.
Similarly,DocTer[ 53]alsoaimstotesteachDLoperatorindivid-
ually, by extracting their input constraints from documentation,
incurring manual inspectionof the mined rules for30% of the API
arguments.Nonetheless,theinputconstraintsforeachargument
are de/f_ined by DocTer as a potential set of types and values, which
cannotmodel /f_ine-grainedshape constraints.Additionally, recent
work has also been improving the oracles of DL system testing via
APIrelation[ 15]andgradientchecking[ 54].Thougheﬀectivein
bug /f_inding, operator-level fuzzing hardly uncover bugs induced
bymultiple operators together, e.g.,bugsinDL compilers.
Model-level fuzzing techniques generate DLmodels with multi-
pleoperators.ThepioneerCradle[ 39]directlyrunspre-builtDL
models programmed in Keras [ 18] and cross-check results from
various backends. Built on Cradle, LEMON [ 51] and Audee [ 20]
generate models via pre-de/f_ined mutation rules. Muﬃn [ 19] per-
forms layer-by-layer model generation for testing both training
and inference. Recently, NNSmith [29] annotates each operator
with input constraints and shape transformation, and generates
well-formed models (following Csmith [ 55]) aided by constraint
10https://github.com/pytorch/pytorch/issues/98143solving.Furthermore, GenCoG [ 50] simpli/f_ies such annotations via
adomain-speci/f_iclanguagewhichstillneedsmanualannotation.
While they complement operator-level fuzzing, the model muta-
tion/generation rules are restrictive, e.g.,they typically only target
naive shape-preserving operators [ 19,32,51], or require certain
manualannotations[ 29],leadingtoalimitedsetofoperatorsbeing
used. This work proposes to infer such operator rules, and then
apply them to generate valid models with all possible operators.
Ourwork can cover asmanyoperators asoperator-level fuzzing
while creatingvalid models with covered operators fully automat-
ically,i.e.,a step forward for bridging the gap between operator-
andmodel-level fuzzingfor DL systems.
More recently, there has been concurrent work [ 12,13] on di-
rectlyleveraginglargelanguagemodels(LLMs)tosynthesizePython
programs to construct valid DL models. Such techniques aim to
implicitly solvethevalidityconstraintsviadirectlylearningfrom
valid samples. Compared with LLM-based model generation, de-
spite the technical complexity, by explicitly solving the constraints,
NeuRIprovides stronger validity guarantee within the covered
model space and is more aﬀordable ( i.e.,<100ms per model on
CPU).Meanwhile,LLMs,trainedoverbillionsoflinesofcodes,can
be used to easily test beyond the model space carefully crafted and
exhaustivelyexploredby NeuRI. Therefore, thesetwoapproaches
can be further combinedfor maximizedfuzzinginthe future.
Lastly,programsynthesishasbeenusedbyrelatedareassuchas
synthesizinguser-facingtensor-manipulationprograms[ 35,45,56].
Similar to NNSmith , they also require manual operator speci/f_i-
cations. This paper applies inductive program synthesis [ 28,52]
toinfersuchspeci/f_ications,andcanpotentiallyimproveallmeth-
ods targeting model generation, e.g.,DL system fuzzing, tensor-
manipulationprogramsynthesisandneuralarchitecturesearch[ 16].
7 CONCLUSION
Wepresent NeuRI,anovelapproachtoautomaticallyinferoperator
rules for diversifying model generation in order to test DL systems.
NeuRIgeneratestest-casesbycreatingstructurallyvalidmodelsus-
ingadiversesetofoperatorsforexercisingdeepsystembehaviours.
Theprimarysourceofthediversitycomesfromourautomatedrule
inferenceandconcolicmodelgeneration.Theruleinferenceengine
inductively and eﬃciently discoversoperatorrules for generating
valid models symbolically. Meanwhile, our concolic model genera-
tor further uses concrete operator invocations in combination with
the symbolic operators to maximize the model diversity. As a re-
sult,NeuRI/f_inds many high-priority and -qualitybugs appreciated
by DL-framework developers. We show NeuRIis promising for
long-term fuzzing – high-quality test-cases can be generated in
millisecondsonasinglethreadandnewoperatorscanbeautomati-
cally integrated. To date, NeuRIhas already detected 100 newbugs
for PyTorchandTensorFlow,with81 /f_ixedorcon/f_irmed.
DATA AVAILABILITY
Our artifact isavailable at Zenodo [ 8]andGitHub[ 30].
ACKNOWLEDGMENTS
ThisworkwaspartiallysupportedbyNSFgrantsCCF-2131943and
CCF-2141474,as well as research awardsfrom Google andMeta.
667ESEC/FSE ’23, December3–9, 2023,San Francisco, CA, USA JiaweiLiu, JinjunPeng,Yuyao Wang,andLingming Zhang
REFERENCES
[1]2022.ASPLOS2023 Artifact for "NNSmith: Generating Diverse and Valid Test Cases
for DeepLearning Compilers" . Zenodo. https://doi.org/10.5281/zenodo.7222132
[2]2022. Building the Future of TensorFlow — The TensorFlow Blog. https:
//blog.tensor/f_low.org/2022/10/building-the-future-of-tensor/f_low.html .
[3]2022. "Compute Sanitizer :: CUDA Toolkit Documentation". https://docs.nvidia.
com/cuda/compute-sanitizer/index.html .
[4] 2022. GCOV. https://gcc.gnu.org/onlinedocs/gcc/Gcov.html .
[5] 2022. PyTorch2.0|PyTorch. https://pytorch.org/get-started/pytorch-2.0/ .
[6]2022. Source-based Code Coverage — Clang 15.0.0 documentation. https://
releases.llvm.org/15.0.0/tools/clang/docs/SourceBasedCodeCoverage.html .
[7]2022. "Unde/f_inedBehaviorSanitizer — Clang 16.0.0git documentation". https:
//clang.llvm.org/docs/Unde/f_inedBehaviorSanitizer.html .
[8]2023.ESEC/FSE’23 Artifact for "NeuRI: Diversifying DNN Generation via Inductive
Rule Inference" . Zenodo. https://doi.org/10.5281/zenodo.8319975
[9]MartínAbadi,PaulBarham,JianminChen,ZhifengChen,AndyDavis,Jeﬀrey
Dean,MatthieuDevin,SanjayGhemawat,GeoﬀreyIrving,MichaelIsard,etal .
2016. Tensor/f_low: A system for large-scale machine learning. In 12th USENIX
symposium on operating systems design and implementation (OSDI 16) . 265–283.
[10]Rajeev Alur, Arjun Radhakrishna, and Abhishek Udupa. 2017. Scaling enumera-
tiveprogramsynthesisviadivideandconquer.In Internationalconferenceontools
and algorithmsfor the construction and analysis ofsystems . Springer, 319–336.
[11]LeonardoDeMouraandNikolajBjørner.2008. Z3:AnEﬃcientSMTSolver.In
ProceedingsoftheTheoryandPracticeofSoftware,14thInternationalConference
onToolsandAlgorithmsfortheConstructionandAnalysisofSystems (Budapest,
Hungary) (TACAS’08/ETAPS’08) . Springer-Verlag, Berlin, Heidelberg, 337–340.
[12]Yinlin Deng, Chunqiu Steven Xia, Haoran Peng, Chenyuan Yang, and Lingming
Zhang. 2023. Large Language Models Are Zero-Shot Fuzzers: Fuzzing Deep-
LearningLibrariesviaLargeLanguageModels.In Proceedingsofthe32ndACM
SIGSOFT International Symposium on Software Testing and Analysis (Seattle, WA,
USA)(ISSTA 2023) . Association forComputing Machinery, New York, NY, USA,
423–435. https://doi.org/10.1145/3597926.3598067
[13]YinlinDeng,ChunqiuStevenXia,ChenyuanYang,ShizhuoDylanZhang,Shujing
Yang, and Lingming Zhang.2023. Large languagemodelsare edge-case fuzzers:
Testingdeeplearninglibrariesviafuzzgpt. arXivpreprintarXiv:2304.02014 (2023).
[14]Yinlin Deng, Chenyuan Yang, Anjiang Wei, and Lingming Zhang. 2022. Fuzzing
Deep-Learning Libraries via Automated Relational API Inference. In Proceedings
of the 30th ACM Joint European Software Engineering Conference and Symposium
on the Foundations of Software Engineering (Singapore, Singapore) (ESEC/FSE
2022). Association for Computing Machinery, New York, NY, USA, 44–56. https:
//doi.org/10.1145/3540250.3549085
[15]Yinlin Deng, Chenyuan Yang, Anjiang Wei, and Lingming Zhang. 2022. Fuzzing
deep-learning libraries via automated relational api inference. In Proceedings of
the 30th ACM Joint European Software Engineering Conference and Symposium on
the FoundationsofSoftwareEngineering . 44–56.
[16]ThomasElsken,JanHendrikMetzen,andFrankHutter.2019. Neuralarchitecture
search:Asurvey. TheJournalofMachineLearningResearch 20,1(2019),1997–
2017.
[17]Patrice Godefroid, Nils Klarlund, and Koushik Sen. 2005. DART: Directed Au-
tomated Random Testing. In Proceedings of the 2005 ACM SIGPLAN Confer-
enceonProgrammingLanguageDesignandImplementation (Chicago,IL,USA)
(PLDI ’05) . Association for Computing Machinery, New York, NY, USA, 213–223.
https://doi.org/10.1145/1065010.1065036
[18] Google. 2015. Keras. https://keras.io .
[19]JiazhenGu,XuchuanLuo,YangfanZhou,andXinWang.2022. Muﬃn:Testing
DeepLearningLibrariesviaNeuralArchitectureFuzzing.In Proceedingsofthe
44th International Conference on Software Engineering (Pittsburgh, Pennsylvania)
(ICSE’22) .AssociationforComputingMachinery,NewYork,NY,USA,1418–1430.
https://doi.org/10.1145/3510003.3510092
[20]Qianyu Guo, Xiaofei Xie, Yi Li, Xiaoyu Zhang, Yang Liu, Xiaohong Li, and Chao
Shen.2021. Audee:AutomatedTestingforDeepLearningFrameworks.In Pro-
ceedingsofthe35thIEEE/ACMInternationalConferenceonAutomatedSoftware
Engineering (Virtual Event, Australia) (ASE ’20). Association for Computing Ma-
chinery, New York, NY, USA, 486–498. https://doi.org/10.1145/3324884.3416571
[21]AricHagberg,PieterSwart,andDanielSChult.2008. Exploringnetworkstructure,
dynamics, and function using NetworkX . Technical Report. Los Alamos National
Lab.(LANL), LosAlamos, NM(United States).
[22]SeppHochreiterandJürgenSchmidhuber.1997. Longshort-termmemory. Neural
computation 9,8 (1997), 1735–1780.
[23]Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger.
2017. Densely connected convolutional networks. In Proceedings of the IEEE
conference oncomputer vision and pattern recognition . 4700–4708.
[24]SusmitJhaandSanjitA.Seshia.2017. ATheoryofFormalSynthesisviaInductive
Learning. Acta Inf.54, 7 (nov 2017), 693–726. https://doi.org/10.1007/s00236-
017-0294-5
[25]Ho Young Jhoo, Sehoon Kim, Woosung Song, Kyuyeon Park, DongKwon Lee,
andKwangkeunYi.2022. AStaticAnalyzerforDetectingTensorShapeErrorsinDeep Neural Network Training Code. In Proceedings of the ACM/IEEE 44th Inter-
national Conference on Software Engineering: Companion Proceedings (Pittsburgh,
Pennsylvania) (ICSE’22) . Association for ComputingMachinery, NewYork, NY,
USA,337–338. https://doi.org/10.1145/3510454.3528638
[26]KyungtaeKim, DaeR.Jeong,ChungHwan Kim,YeongjinJang, InsikShin,and
Byoungyoung Lee. 2020. HFL: Hybrid Fuzzing on the Linux Kernel. In 27th
AnnualNetworkandDistributedSystemSecuritySymposium,NDSS2020,SanDiego,
California,USA,February23-26,2020 .TheInternetSociety. https://www.ndss-
symposium.org/ndss-paper/h/f_l-hybrid-fuzzing-on-the-linux-kernel/
[27]Chris Lattner and Vikram Adve. 2004. LLVM: A Compilation Framework for
Lifelong Program Analysis & Transformation. In Proceedings of the International
SymposiumonCodeGenerationandOptimization:Feedback-DirectedandRuntime
Optimization (Palo Alto, California) (CGO ’04) . IEEE Computer Society, USA, 75.
[28]Tessa A. Lau and Daniel S. Weld. 1999. Programming by Demonstration: An
InductiveLearningFormulation.In Proceedingsofthe4thInternationalConference
on Intelligent User Interfaces (Los Angeles, California, USA) (IUI ’99). ACM, New
York, NY, USA,145–152. https://doi.org/10.1145/291080.291104
[29]JiaweiLiu, JinkunLin,Fabian Ruﬀy, ChengTan,JinyangLi,AurojitPanda, and
LingmingZhang.2023. NNSmith:GeneratingDiverseandValidTestCasesfor
DeepLearningCompilers.In Proceedingsofthe28thACMInternationalConfer-
ence on Architectural Support for Programming Languages and Operating Systems,
Volume2 (Vancouver,BC,Canada) (ASPLOS2023) .AssociationforComputingMa-
chinery, New York, NY, USA, 530–543. https://doi.org/10.1145/3575693.3575707
[30]Jiawei Liu, Jinjun Peng, Yuyao Wang, and Lingming Zhang. 2023. Artifact for
ESEC/FSE’23 paper "NeuRI: Diversifying DNN Generation via Inductive Rule
Inference". https://github.com/ise-uiuc/neuri-artifact .
[31]Jiawei Liu, Yuxiang Wei, Sen Yang, Yinlin Deng, and Lingming Zhang. 2022.
Coverage-GuidedTensorCompilerFuzzingwithJointIR-PassMutation. Proc.
ACM Program. Lang. 6, OOPSLA1, Article 73 (apr 2022), 26 pages. https://doi.
org/10.1145/3527317
[32]Weisi Luo, Dong Chai, Xiaoyue Run, Jiang Wang, Chunrong Fang, andZhenyu
Chen. 2021. Graph-Based Fuzz Testing for Deep Learning Inference Engines. In
Proceedings of the 43rd International Conference on Software Engineering (Madrid,
Spain)(ICSE ’21) .IEEEPress,288–299. https://doi.org/10.1109/ICSE43902.2021.
00037
[33]Barton P. Miller, Lars Fredriksen, and Bryan So. 1990. An Empirical Study
of the Reliability of UNIX Utilities. Commun. ACM 33, 12 (dec 1990), 32–44.
https://doi.org/10.1145/96267.96279
[34]ChandrakanaNandi,MaxWillsey,AmyZhu,YisuRemyWang,BrettSaiki,Adam
Anderson,AdrianaSchulz,Dan Grossman,and Zachary Tatlock.2021. Rewrite
Rule Inference Using Equality Saturation. Proc. ACM Program. Lang. 5, OOPSLA,
Article119(oct 2021),28pages. https://doi.org/10.1145/3485496
[35]AnsongNi,DanielRamos,AidanZ.H.Yang,InêsLynce,VascoManquinho,Ruben
Martins,andClaireLeGoues.2021.SOAR:ASynthesisApproachforDataScience
API Refactoring. In 2021 IEEE/ACM 43rd International Conference on Software
Engineering (ICSE) . 112–124. https://doi.org/10.1109/ICSE43902.2021.00023
[36]OpenAI. 2022. ChatGPT: Optimizing Language Models for Dialogue. https:
//openai.com/blog/chatgpt/ .
[37] OpenAI. 2022. DALL ·E2.https://openai.com/dall-e-2/ .
[38]AdamPaszke,SamGross,FranciscoMassa,AdamLerer,JamesBradbury,Gregory
Chanan,TrevorKilleen,ZemingLin,NataliaGimelshein,LucaAntiga,etal .2019.
Pytorch: An imperative style, high-performance deep learning library. Advances
inneuralinformation processingsystems 32(2019), 8026–8037.
[39]HungVietPham,ThibaudLutellier,WeizhenQi,andLinTan.2019. CRADLE:
Cross-BackendValidationtoDetectandLocalizeBugsinDeepLearningLibraries.
In2019IEEE/ACM41stInternationalConferenceonSoftwareEngineering(ICSE) .
1027–1038. https://doi.org/10.1109/ICSE.2019.00107
[40]PyTorch. 2022. Numerical accuracy — PyTorch master documenta-
tion.https://pytorch.org/docs/master/notes/numerical_accuracy.html#linear-
algebra-torch-linalg
[41]PyTorch.2022. TorchDynamoandTorchInductorTutorial. https://pytorch.org/
tutorials/intermediate/dynamo_tutorial.html .
[42]John Regehr. 2020. Responsible and Eﬀective Bug/f_inding. https://blog.regehr.
org/archives/2037 .
[43]Koushik Sen, Darko Marinov, and Gul Agha. 2005. CUTE: A Concolic Unit
TestingEngineforC.In Proceedingsofthe10thEuropeanSoftwareEngineering
Conference Held Jointly with 13th ACM SIGSOFT International Symposium on
FoundationsofSoftwareEngineering (Lisbon,Portugal) (ESEC/FSE-13) .Association
for Computing Machinery, New York, NY, USA, 263–272. https://doi.org/10.
1145/1081706.1081750
[44]Konstantin Serebryany, Derek Bruening, Alexander Potapenko, and Dmitriy
Vyukov. 2012.{AddressSanitizer}: A Fast Address Sanity Checker. In 2012
USENIXAnnual TechnicalConference (USENIXATC 12) . 309–318.
[45]KensenShi,DavidBieber,andRishabhSingh.2022. TF-Coder:ProgramSynthesis
forTensorManipulations. ACMTrans.Program.Lang.Syst. 44,2,Article10(may
2022),36pages. https://doi.org/10.1145/3517034
[46]TensorFlow. 2022. tf.cast | TensorFlow v2.11.0. https://www.tensor/f_low.org/api_
docs/python/tf/cast .
668NeuRI: DiversifyingDNN GenerationviaInductive RuleInference ESEC/FSE ’23, December3–9, 2023,San Francisco, CA, USA
[47]TensorFlow Contributors. 2023. Using TensorFlow Securely – TensorFlow mod-
els are programs. https://github.com/tensor/f_low/tensor/f_low/security/policy#
tensor/f_low-models-are-programs [Online;accessed 3-Jan-2023].
[48]EminaTorlakandRastislavBodik.2013. Growingsolver-aidedlanguageswith
Rosette.In Proceedingsofthe2013ACMinternationalsymposiumonNewideas,
newparadigms,andre/f_lectionsonprogramming&software (Indianapolis,Indiana,
USA). Association for Computing Machinery, New York, NY, USA, 135–152.
https://doi.org/10.1145/2509578.2509586
[49]AbhishekUdupa,ArunRaghavan,JyotirmoyV.Deshmukh,SelaMador-Haim,
MiloM.K.Martin,andRajeevAlur.2013. TRANSIT:SpecifyingProtocolswith
ConcolicSnippets. SIGPLANNot. 48,6(jun2013),287–296. https://doi.org/10.
1145/2499370.2462174
[50]Zihan Wang, Pengbo Nie, Xinyuan Miao, Yuting Chen, Chengcheng Wan, Lei
Bu, and Jianjun Zhao. 2023. GenCoG: A DSL-Based Approach to Generating
Computation Graphs for TVM Testing. In Proceedings of the 32nd ACM SIGSOFT
International Symposium on Software Testing and Analysis (Seattle, WA, USA)
(ISSTA2023) .AssociationforComputingMachinery,NewYork,NY,USA,904–916.
https://doi.org/10.1145/3597926.3598105
[51]ZanWang, MingYan,Junjie Chen,ShuangLiu,and DongdiZhang.2020. Deep
LearningLibraryTestingviaEﬀectiveModelGeneration.In Proceedingsofthe28th
ACM Joint Meeting on European Software Engineering Conference and Symposium
on the Foundations of Software Engineering (Virtual Event, USA) (ESEC/FSE 2020) .
Association for Computing Machinery, New York, NY, USA, 788–799. https:
//doi.org/10.1145/3368089.3409761
[52]Patrick H. Winston. 1970. Learning Structural Descriptions From Examples . Tech-
nical Report. Cambridge, MA,USA.[53]Danning Xie, Yitong Li, Mijung Kim, Hung Viet Pham, Lin Tan, Xiangyu Zhang,
and Michael W. Godfrey. 2022. DocTer: Documentation-Guided Fuzzing for
TestingDeepLearningAPIFunctions.In Proceedingsofthe31stACMSIGSOFT
International Symposium on Software Testing and Analysis (Virtual, South Korea)
(ISSTA2022) .AssociationforComputingMachinery,NewYork,NY,USA,176–188.
https://doi.org/10.1145/3533767.3534220
[54]Chenyuan Yang, Yinlin Deng, Jiayi Yao, Yuxing Tu, Hanchi Li, and Ling-
ming Zhang. 2023. Fuzzing Automatic Diﬀerentiation in Deep-Learning Li-
braries. In Proceedings of the 45th International Conference on Software En-
gineering (Melbourne, Victoria, Australia) (ICSE ’23) . IEEE Press, 1174–1186.
https://doi.org/10.1109/ICSE48619.2023.00105
[55]Xuejun Yang, Yang Chen, Eric Eide, and John Regehr. 2011. Finding and Un-
derstanding Bugs in C Compilers. In Proceedings of the 32nd ACM SIGPLAN
Conference on Programming Language Design and Implementation (San Jose, Cali-
fornia,USA) (PLDI’11) .AssociationforComputingMachinery,NewYork,NY,
USA,283–294. https://doi.org/10.1145/1993498.1993532
[56]Zhanhui Zhou, Man To Tang, Qiping Pan, Shangyin Tan, Xinyu Wang, and
TianyiZhang.2022. INTENT:InteractiveTensorTransformationSynthesis.In
Proceedingsofthe35thAnnualACMSymposiumonUserInterfaceSoftwareand
Technology (Bend,OR,USA) (UIST’22) .AssociationforComputingMachinery,
NewYork,NY,USA,Article89,16pages. https://doi.org/10.1145/3526113.3545653
[57]Hong Zhu, Patrick A. V. Hall, and John H. R. May. 1997. Software Unit Test
CoverageandAdequacy. ACMComput.Surv. 29,4(dec1997),366–427. https:
//doi.org/10.1145/267580.267590
Received 2023-02-02; accepted 2023-07-27
669