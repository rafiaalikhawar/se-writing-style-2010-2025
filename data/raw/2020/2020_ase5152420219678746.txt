Graph-based Incident Aggregation for Large-Scale
Online Service Systems
Zhuangbin Chen‚àó, Jinyang Liu‚àó, Yuxin Su‚àó‚Ä†, Hongyu Zhang‚Ä°
Xuemin Wen¬∂, Xiao Ling¬∂, Yongqiang Yang¬∂, Michael R. Lyu‚àó
‚àóThe Chinese University of Hong Kong, Hong Kong, China, {zbchen, jyliu, yxsu, lyu}@cse.cuhk.edu.hk
‚Ä°The University of Newcastle, NSW, Australia, hongyu.zhang@newcastle.edu.au
¬∂Huawei, China, {wenxuemin, lingxiao1, yangyongqiang}@huawei.com
Abstract ‚ÄîAs online service systems continue to grow in terms
of complexity and volume, how service incidents are managed will
signiÔ¨Åcantly impact company revenue and user trust. Due to thecascading effect, cloud failures often come with an overwhelmingnumber of incidents from dependent services and devices. Topursue efÔ¨Åcient incident management, related incidents shouldbe quickly aggregated to narrow down the problem scope. To thisend, in this paper, we propose GRLIA, an incident aggregationframework based on graph representation learning over the cas-cading graph of cloud failures. A representation vector is learnedfor each unique type of incident in an unsupervised and uniÔ¨Åedmanner, which is able to simultaneously encode the topologicaland temporal correlations among incidents. Thus, it can be easilyemployed for online incident aggregation. In particular, to learnthe correlations more accurately, we try to recover the completescope of failures‚Äô cascading impact by leveraging Ô¨Åne-grainedsystem monitoring data, i.e., Key Performance Indicators (KPIs).The proposed framework is evaluated with real-world incidentdata collected from a large-scale online service system of HuaweiCloud. The experimental results demonstrate that GRLIA iseffective and outperforms existing methods. Furthermore, ourframework has been successfully deployed in industrial practice.
Index T erms‚ÄîCloud computing, online service systems, inci-
dent management, graph representation learning
I. I NTRODUCTION
In recent years, IT enterprises started to deploy their appli-
cations as online services on cloud, such as Microsoft Azure,
Amazon Web Services, and Google Cloud Platform. Thesecloud computing platforms have beneÔ¨Åted many enterprisesby taking over the maintenance of IT and infrastructure andallowing them to improve their core business competence.However, for large-scale online service systems, failures areinevitable, which may lead to performance degradation orservice unavailability. Whether or not the service failures areproperly managed will have a great impact on the company‚Äôsrevenue and users‚Äô trust. For example, an hour episode ofdowntime in Amazon led to a loss of 100+ million dollars [1].
When a failure happens, system monitors will render a large
number of incidents to capture different failure symptoms [2]‚Äì[4], which can help engineers quickly obtain a big picture ofthe failure and pinpoint the root cause. For example, ‚ÄúSpecialinstance cannot be migrated‚Äù is a critical network failure inVirtual Private Cloud (VPC) service, and the incident ‚ÄúTunnel
‚Ä†Corresponding author.bearing network pack loss‚Äù is a signal for this network failure,which is caused by the breakdown of a physical network cardon the tunnel path. Due to the large scale and complexity ofonline service systems, the number of incidents is overwhelm-ing the existing incident management systems [2], [4], [5].When a service failure occurs, aggregating related incidentscan greatly reduce the number of incidents that need to beinvestigated. For example, linking incidents that are caused bya hardware issue can provide engineers with a clear pictureof the failure, e.g., the type of the hardware error or eventhe speciÔ¨Åc malfunctioning components. Without automatedincident aggregation, engineers may need to go through eachincident to discover the existence of such a problem and collectall related incidents to understand it. Moreover, incident aggre-gation can also facilitate failure diagnosis. In cloud systems,some trivial incidents are being generated continuously, andmultiple (independent) failures can happen at the same time.Identifying correlated incidents can therefore accelerate theprocess of root cause localization.
To aggregate related incidents, one straightforward way is
to measure the text similarity between two incident reports [3],[4]. For example, incidents that share similar titles are likelyto be related. Besides textual similarity, system topology (e.g.,service dependency, network IP routing) is also an importantfeature to resort to. Due to the dependencies among onlineservices, failures often have a cascading effect on other inter-dependent services. A service dependency graph can help trackrelated incidents caused by such an effect. However, as cloudsystems often possess certain ability of fault tolerance, someservices may not report incidents, impeding the tracking offailures‚Äô impact (to be explained in Section II). This issueis ubiquitous in production systems, which has not yet beenproperly addressed in existing work. Moreover, the patterns ofincidents are collectively inÔ¨Çuenced by different factors, suchas their topological and temporal locality. Existing work [3],[4] combine them by a simple weighted sum, which may notbe able to reveal the latent correlations among incidents.
In this work, we propose GRLIA (stands for Graph Repre-
sentation Learning-based Incident Aggregation), which is anincident aggregation framework to assist engineers in failureunderstanding and diagnosis. Different from the existing workof alert storm handling [4] and linked incident identiÔ¨Åca-
4302021 36th IEEE/ACM International Conference on Automated Software Engineering (ASE)
DOI 10.1109/ASE51524.2021.000462021 36th IEEE/ACM International Conference on Automated Software Engineering (ASE) | 978-1-6654-0337-5/21/$31.00 ¬©2021 IEEE | DOI: 10.1109/ASE51524.2021.9678746
978-1-6654-0337-5/21/$31.00  ¬©2021  IEEE
tion [3], we do not rely on incidents‚Äô textual similarity. More-
over, we learn incidents‚Äô topological and temporal correlationsin a uniÔ¨Åed manner (instead of by a weighted combination).Traditional applications of graph representation learning oftenlearn the semantics of a Ô¨Åxed graph. Unlike them, we proposeto learn a feature representation for each unique type ofincident, which can appear in multiple places of the graph.The representation encodes the historical co-occurrence ofincidents and their topological structure. Thus, they can benaturally used for incident aggregation in online scenarios. Totrack the impact graph of a failure (i.e., the incidents triggeredby the failure), we exploit more Ô¨Åne-grained system signals,i.e., KPIs, as a piece of auxiliary information to discover thescope of its cascading effect. KPIs proÔ¨Åle the impact of failuresin a more sophisticated way. Therefore, if two services exhibitsimilar abnormal behaviors (characterized by incidents andKPIs), they should be suffering from the same problems even ifno incidents have been reported. Finally, we apply communitydetection algorithms to Ô¨Ånd the scope of different failures.
To sum up, this work makes the following major contribu-
tions:
‚Ä¢We propose to identify service failures‚Äô impact graph,which consists of the incidents that originate from thesame failures. Such an impact graph helps us obtain acomplete picture of failures‚Äô cascading effect. To this end,we combine incidents with KPIs to measure the behav-ioral similarity between services. Community detectionalgorithms are then applied to determine the failure-impact graph of different failures automatically.
‚Ä¢We propose GRLIA, an incident aggregation frameworkbased on graph representation learning. The embeddingvector for each unique type of incident is learned inan unsupervised and uniÔ¨Åed fashion, which encodes itsinteractions with other incidents in temporal and topo-logical dimensions. Online incident aggregation can thenbe naturally performed by calculating their distance. Theimplementation of GRLIA is available on GitHub [6].
‚Ä¢We conduct experiments with real-world incidents col-lected from Huawei Cloud, which is a large-scale cloudservice provider. The results demonstrate the effective-ness of the proposed framework. Furthermore, our frame-work has been successfully incorporated into the incidentmanagement system of Huawei Cloud. Feedback fromon-site engineers conÔ¨Årms its practical usefulness.
The remainder of this paper is organized as follows. Sec-
tion II introduces the background and problem statement ofthis paper. Section III describes the proposed framework.Section IV shows the experiments and experimental results.Section V presents our success story and lessons learnedfrom practice. Section VI discusses the related work. Finally,Section VII concludes this work.
II. B
ACKGROUND AND PROBLEM STATEMENT
A. Topology of Large-scale Online Service Systems
Cloud vendors provide a variety of online services to
customers worldwide. In general, there are three main models


	



 



	




 



 




Fig. 1. An illustration of service failures‚Äô cascading effect. The irregular
circle in the third subÔ¨Ågure shows the failure-impact graph.






	


	




Fig. 2. An example of incomplete failure-impact graph
of cloud-based services, namely, Software as a Service (SaaS),
Platform as a Service (PaaS), and Infrastructure as a Service(IaaS) [7]. Online service systems often possess a hierarchicaltopology, i.e., the stack of application, platform, and infrastruc-ture layers. Each service embodies the integration of code anddata required to execute a complete and discrete functionality.For example, in the application layer, the services provided tocustomers can be a user application, a microservice, or even afunction; in the platform layer, the services can be a containeror a database; in the infrastructure layer, the services can bea virtual machine or storage. Different services communicatethrough virtual networks using protocols such as HypertextTransfer Protocol (HTTP) and Remote Procedure Call (RPC).Such communications among services constitute the complextopology of large-scale online service systems.
B. Cascading Effect of Service Failures
With such a topology, a failure occurring to one service
tends to have a cascading effect across the entire system.
Representative service failures include slow response, requesttimeout, service unavailability, etc., which could be caused bycapacity issues, conÔ¨Åguration errors, software bugs, hardwarefaults, etc. To quickly understand failure symptoms, a largenumber of monitors are conÔ¨Ågured to monitor the states ofdifferent services in a cloud system [2]. A monitor will renderan incident when certain predeÔ¨Åned conditions (e.g., ‚ÄúCPUutilization rate exceeds 80%‚Äù) are met. Typical conÔ¨Ågurationsof monitors include setting thresholds for speciÔ¨Åc metrics(e.g., RPC latency, error counter), checking service/deviceavailability or status, etc. When a failure happens, the monitorsoften render a large number of incidents. These incidents aretriggered by the common root cause and describe the failurefrom different aspects. Thus, they can be aggregated to helpengineers understand and diagnose the failure.
In this paper, we model the set of incidents triggered by a
failure as its impact graph (or failure-impact graph), as illus-
431TABLE I
EXAMPLES OF INCIDENT AGGREGATION
1R ,QFLGHQW7LWOH 7LPH 3RG 6HYHULW\

,QFLGHQWJURXS9LUWXDOPDFKLQHLVLQDEQRUPDOVWDWH  SRG /RZ
 9LUWXDOQHWZRUNLQWHUIDFHUHFHLYHORVWUDWLRRYHU  SRG +LJK
 7UDIILFEXUVWVHHQLQ1JLQ[QRGH  SRG /RZ
 7UDIILFEXUVWVHHQLQ/96/LQX[9LUWXDO6HUYHUQRGH  SRG 0HGLXP
 263)2SHQ6KRUWHVW3DWK)LUVWSURWRFROVWDWHFKDQJH  SRG 0HGLXP

,QFLGHQWJURXS([FHVVLYH,2GHOD\RIVWRUDJHGLVN  SRG 0HGLXP
 &RPSRQHQWIDLOXUH  SRG +LJK
 +DUGGLVNIDLOXUH  SRG 0HGLXP
 'DWDEDVHDFFRXQWORJLQHUURU  SRG 0HGLXP
 0RQLWRUGHWHFWHGFXVWRPHULPSDFWLQJLQFLGHQWVIRU6WRUDJHLQ>$=@  SRG 0HGLXP
trated by Fig. 1. SpeciÔ¨Åcally, service Aencounters a failure,
and the impact propagates to other services along the system
topology. The circled area indicates the impact graph of thefailure, where irrelevant incidents (in blue) in service DandG
are excluded. In general, the system topology can have manydifferent forms, such as the dependencies of services [8], theconÔ¨Ågured IP routing of a cloud network [9], etc. Intuitively,it might seem that the impact graph can be easily constructedby tracing incidents along the system topology. However, ourindustrial practices reveal that they are usually incomplete. Anexample is given in Fig. 2, where service Boccasionally fails
to report any incident during the failure. Existing approachesmay perceive it as two separate failures, which is undesirable.We have summarized the following two main reasons for themissing incidents:
‚Ä¢System monitors that report incidents are conÔ¨Ågured withrules predeÔ¨Åned by engineers. Due to the diversity ofcloud services and user behaviors, the impact of a failuremay not meet the rules of some monitors. For example,if an application triggers an incident when its CPU usagerate exceeds 80%, then any value below the threshold willbe unqualiÔ¨Åed. As a consequence, the monitors will notreport any incident, and thus, the tracking of the failure‚Äôsimpact is blocked.
‚Ä¢To ensure the continuity of online services, cloud systemsare designed to have a certain fault tolerance capability. Inthis case, service systems can bear some abnormal condi-tions, and thus, no incidents will be reported. Therefore,the impact of a failure may not manifest itself completelyover the system topology.
Recent studies on cloud incident management [2], [10] have
demonstrated the incompleteness and imperfection of monitordesign and distribution in online service systems. Thus, alongthe service dependency chain, some services in the middlemay remain silent (i.e., report no incident), which impedesthe tracking of failure‚Äôs cascading effect. Therefore, althoughonline service systems generate many incidents, they are oftenscattered.C. Problem Statement
This work aims to assist engineers in failure understanding
and diagnosis with online incident aggregation, which is toaggregate incidents caused by the common failure. Whenservices encounter failures, incidents that capture differentfailure symptoms constitute an essential source for engineersto conduct a diagnosis. However, it is time-consuming andtedious for engineers to manually examine each incident forfailure investigation when faced with such an overwhelmingnumber of incidents. Online incident aggregation is to clusterrelevant incidents when they come in a streaming manner (i.e.,continuously reported by the system). Examples are presentedin Table I, where items in blue and gray belong to two groupsof aggregated incidents. Particularly, the Ô¨Årst group shows avirtual network failure. Note that only the No.3 and No.4incidents share some words in common, while the others donot. Meanwhile, the second group describes a hardware failure,and more speciÔ¨Åcally, a storage disk error. Engineers canbeneÔ¨Åt from such incident aggregation as the problem scopeis narrowed down to each incident cluster.
However, accurately aggregating incidents for online service
systems is challenging. We have identiÔ¨Åed three main reasons.
Background noise. Although related incidents are indeed
generated around the same time, many other cloud componentsare also constantly rendering incidents. These incidents aremostly trivial issues and therefore become background noise.Incident aggregation based on temporal similarity would sufferfrom a high rate of false positives.
Dissimilar textual description. Text (e.g., incident title
and summary) similarity is an essential metric for incidentcorrelation, which has been widely used in existing work [3],[4]. However, in reality, related incidents, especially the criticalones, do not necessarily have similar titles. Failing to correlatesuch critical incidents greatly hinders root cause diagnosis.
Unclear failure-impact graph . To correlate incidents ac-
curately, we need to estimate the impact graph of servicefailures. As discussed in Section II-B, this task is challenging.Incidents alone are insufÔ¨Åcient to completely reÔ¨Çect the impact
432		
	 		
	
 

  



	
 		
	
 	 		 
		

 
Fig. 3. The overall framework of GRLIA
of failures on the entire system. Therefore, we need to utilize
more Ô¨Åne-grained information of the failures.
III. M ETHODOLOGY
A. Overview
In cloud systems, a large number of monitors are conÔ¨Åg-
ured to continuously monitor the states of its services fromdifferent aspects. Many incidents rendered by the monitorstend to co-occur due to their underlying dependencies. Forexample, some failure symptoms often appear together, andsome incidents may develop causal relationship. Our main ideais to capture the co-occurrences among incidents by learningfrom historical failures. In online scenarios, such correlationscan be leveraged to distinguish correlated incidents that aregenerated in streams.
The overall framework of GRLIA is illustrated in Fig. 3,
which consists of four phases, i.e., service failure detection,
failure-impact graph completion, graph representation learn-
ing, and online incident aggregation. The Ô¨Årst phase tries
to identify the occurrence of service failures and retrievesdifferent types of monitoring data, including incidents, KPItime series, and service system topology. In the second phase,we try to identify the incidents that are triggered by eachindividual failure detected above. More often than not, itis hard to precisely identify the impact scope of failures(as discussed in Section II-B), which hinders the learningof incidents‚Äô correlations. Therefore, we utilize the trendsobserved in KPI curves to auto-complete the failure-impactgraphs. After obtaining the set of incidents associated witheach failure, in the third phase, an embedding vector is learnedfor different types of incidents by leveraging existing graphrepresentation learning models [11], [12]. Such representationencodes not only the temporal locality of incidents, but alsotheir topological relationship. In the Ô¨Ånal phase, the learnedincident representation will be employed for online incidentaggregation by considering their cosine similarity and topo-logical distance. In particular, we do not explicitly considerthe dynamic change of a system topology because the changesoften happen to a small area of the topology, e.g., containercreation or kill. GRLIA essentially learns the correlationsamong incidents, which are also applicable to the changedportion of the topology. Nevertheless, when the system topol-ogy goes through a signiÔ¨Åcant alteration, our framework isefÔ¨Åcient enough to support quick model retraining.
B. Service Failure Detection
Due to the cascading effect, when service failures occur,
a large number of incidents are often reported in a short
period of time. Thus, setting a Ô¨Åxed threshold for the averagenumber of reported incidents (e.g., #incidents/min> 50) could
be a reasonable criterion to detect failures. However, such adesign suffers from a trade-off between false positives andfalse negatives due to online service systems‚Äô complex andever-changing nature [4]. For example, different services havedistinct sensitivity to the number of incidents, and continuoussystem evolution/feature upgrades could change the threshold.Thus, a self-adaptive algorithm is more desirable.
For time-series data, anomalies often manifest themselves as
having a large magnitude of upward/downward changes. Ex-treme Value Theory (EVT) [13] is a popular statistical tool toidentify data points with extreme deviations from the medianof a probability distribution. It has been applied to predictingunusual events, e.g., severe Ô¨Çoods and tornado outbreaks [14],by Ô¨Ånding the law of extreme values that usually reside atthe tail of a distribution. Moreover, it requires no hand-setthresholds and makes no assumptions on data distribution. Inthis work, we follow [4], [13] to detect bursts in time series ofthe number of incidents per minute. As a typical time seriesanomaly detection problem, other approaches (e.g., [15], [16])in this Ô¨Åeld are also applicable. The bursts are regarded as theoccurrence of service failures. This algorithm can automati-cally learn the normality of the data in a dynamic environmentand adapt the detection method accordingly. Fig. 3 (phaseone) presents an example of service failure detection, whereall abnormal spikes are successfully found by the decisionboundary (the orange dashed line). For consecutive bins thatare marked as anomalies, we regard them as one failurebecause failures may last for more than one minute. Thenext phase will distinguish multiple (independent) failures thathappen simultaneously. Particularly, the detection algorithm isonly required to have a high recall, and the precision is of lessimportance. It is because the goal of the follow-up two phasesis to Ô¨Ånd the correlations between incidents. Such correlationrules will not be violated even incidents are not appearingtogether during actual cloud failures.
433C. Failure-Impact Graph IdentiÔ¨Åcation
In the Ô¨Årst phase, the number of incidents per minute is
calculated, and incident bursts are regarded as the occurrence
of service failures. For each failure, the incidents collectedfrom the entire system are not necessarily related to it. Thisis because: 1) while some services are suffering from thefailure, others may continuously report incidents (could betrivial and unrelated issues); and 2) multiple service failurescould happen simultaneously. Therefore, we need to identifythe set of incidents for each individual failure that is generateddue to the cascading effect.
To this end, the concept of community detection is ex-
ploited. Community detection algorithms aim to group thevertices of a graph into distinct sets, or communities, suchthat there exist dense connections within a community andsparse connections between communities. Each communityrepresents a collection of incidents rendered by the commonservice failure, in which the correlations among incidents canbe explored. A comparative review of different communitydetection algorithms is available in [17]. In this work, we em-ploy the well-known Louvain algorithm [18], which is based
upon modularity maximization. The modularity of a graphpartition measures the density of links inside communitiescompared to links between communities. For weighted graphs,the modularity can be calculated as follows [18]:
M=1
2m/summationdisplay
i,j[Wi,j‚àíkikj
2m]Œ¥(ci,cj) (1)
whereWijis the weight of the link between node iandj,
ki=/summationtext
jWijsums the weights of the links associated with
nodei,ciis the community to which node iis assigned to,
m=1
2/summationtext
ijWij, and theŒ¥(u,v)=1 ifu=vand 0 otherwise.
To better understand the identiÔ¨Åcation of failure-impact
graph using community detection, an illustrating example isdepicted in Fig. 3 (phase two). In this case, except for nodesBandF, other nodes all report incidents. By conducting
community detection, we obtain two communities: {A,B,C}
and{C,E,F,G }, which are regarded as the complete impact
graph of their respective failure. The weight between nodes isprovided with their link. We can see that intra-community linksall have a relatively large weight. Such partition can achievethe best modularity score for this example. Particularly, nodeHis excluded from the second community due to the small
weight of its connection to node F.
To apply community detection, the weight between two
nodes should be deÔ¨Åned. Inspired by [19], we combine KPIswith incidents to calculate the behavioral similarity betweentwo nodes and use the similarity value as the weight. Specif-ically, the weight is composed of two parts, i.e., incidentsimilarity and KPI trend similarity.
1) Incident similarity: Incident similarity is to compare
the incidents reported by two nodes. Typically, if two nodesencounter similar errors, they will render similar types of in-cidents. Jaccard index is employed to quantify such similarity,Fig. 4. CPU usage curve of four servers
which is deÔ¨Åned as the size of the intersection divided by thesize of the union of two incident sets:
Jaccard( i,j)=|inc(i)‚à©inc(j)|
|inc(i)‚à™inc(j)|(2)
whereinc(i)is the incidents reported by node i. In particular,
we allow duplicate types of incidents in each set by assigningthem a unique number. This is because the distribution ofincident types also characterizes the failure symptoms.
2) KPI trend similarity: As discussed in Section II, some
services may remain silent when failures happen, hinderingthe tracking of related incidents. To bridge this gap, we resortto KPIs, which are more sophisticated monitoring signals.Intuitively, the KPI trend similarity measures the underlyingconsistency of cloud components‚Äô abnormal behaviors, whichcannot be captured by incidents alone. An example is shownin Fig. 4, which records the CPU utilization of four servers.Clearly, the curve of the Ô¨Årst three servers exhibits a highlysimilar trend, while such a trend cannot be observed in serverfour. The implication is that the Ô¨Årst three servers are likelyto be suffering from the same issue and thus should belongto the same community. We adopt dynamic time warping(DTW) [20] to measure the similarity between two temporalsequences with varying speeds. We observe the issue oftemporal drift between two time series, which is common asdifferent cloud components may not be affected by a failuresimultaneously during its propagation. Therefore, DTW Ô¨Åtsour scenario.
The remaining problem is which KPIs should be utilized for
similarity evaluation. Normal KPIs which record the system‚Äôsnormal status should be excluded as they provide trivial andnoisy information. Therefore, EVT introduced in phase oneis utilized again to detect anomalies for each KPI. Only theabnormal KPIs shared by two connected cloud componentswill be compared. Particularly, when there exists more thanone type of abnormal KPIs, we use the average similarity scorecalculated as follows:
DTW(i,j)=1
KK/summationdisplay
k=1dtw(ti
k,tj
k) (3)
whereKis the number of KPIs to compare for node iand
j,ti
kis thekthKPI of node i, anddtw(u,v)measures the
434DTW similarity between two KPI time series uandv, which
is normalized for path length. The weight Wijbetween node
iandjis computed by taking the weighted sum of the two
types of similarities as follows:
Wij=Œ±√óJaccard( i,j)+(1‚àíŒ±)√óDTW(i,j) (4)
where the balance weight Œ±is a hyper-parameter. In our
experiments, if two nodes both report incidents, we set it
as 0.5; otherwise, it is set to be 0, i.e., only the KPI trendsimilarity is considered.
Finally, for each discovered community, the incidents inside
it form the complete impact graph of the service failure.Note that in online scenarios, we cannot directly adopt thetechniques introduced in this phase for incident aggregation.This is because they involve a comparison between differentKPIs, which are not complete until the failures fully mani-fest themselves. Thus, the comparison is often delayed andinefÔ¨Åcient. Moreover, they can be error-prone without fullyconsidering the historical cases.
D. Graph-based Incident Representation Learning
After obtaining the impact graph for each service failure
(i.e., the actual incidents triggered by it), we can learn the
correlations among incidents. Such correlations describe thesets of incidents that tend to appear together. FP-Growthproposed by Han et al. [21] is a standard algorithm to minesuch frequent item sets. However, our analysis reveals thefollowing drawbacks it possesses for our problem:
‚Ä¢It is vulnerable to background noise. In production en-vironments, some simple incidents are constantly beingreported, e.g., ‚ÄúHigh CPU utilization rate‚Äù. These inci-dents will appear in many transactions (a collection ofitems that appear together) for FP-Growth. As a result,unrelated incidents might be put into the same frequentitem set due to sharing such incidents. These simpleincidents cannot be trivially removed as they providenecessary information about a system, and a burst of suchincidents can also indicate serious problems.
‚Ä¢It cannot handle incidents with a low frequency. FP-Growth has a parameter called support, which describes
how frequently an item set is in the dataset. Incident setswith a low support value will be excluded to guaranteethe statistical signiÔ¨Åcance of the results. However, moreoften than not, such incident sets are more important,as they report some critical failures that do not happenfrequently.
In online service systems, different resources (e.g., mi-
croservices and devices) are naturally structured in graphicalforms, such as service dependency and network IP routing.Therefore, graph representation learning [11] can be an idealsolution to deal with the above issues. Graph representationlearning is an essential and ubiquitous task with applicationsranging from drug design to friendship recommendation insocial networks. It aims to Ô¨Ånd a representation for graphstructure that preserves the semantics of the graph. A typicalgraph representation learning algorithm learns an embeddingvector for all nodes of a graph. For example, Chen et al. [3]employed node2vec [22] to learn a feature representation for
cloud components. Different from them, we propose to learna representation for each unique type of incident, which canappear in multiple places of the graph. In our framework, weemploy DeepWalk [23] because of its simplicity and superior
performance. DeepWalk belongs to the class of shallow em-bedding approaches that learn the node embeddings based onrandom walk statistics. The basic idea is to learn an embeddingœë
ifor node viin graph Gsuch that:
EMB(œëi,œëj)/defineseœëi¬∑œëj
/summationtext
vk‚ààVeœëi¬∑œëk‚âàpG,T(vj|vi) (5)
whereVis the set of nodes in the graph and pG,T(vj|vi)is the
probability of visiting vjwithinThops of distance starting at
vi. The loss function to maximize such probability is:
L=/summationdisplay
(vi,vj)‚ààD‚àílog(EMB(œëi,œëj)) (6)
whereDis the training data generated by sampling random
walks starting from each node. Readers are referred to theoriginal paper [23] for more details.
For each failure-impact graph, incident sequences are gener-
ated through random walk starting from every node inside. Inreality, each node usually generates more than one incidentwhen failures happen. Our tailored random walk strategytherefore contains two hierarchical steps. In the Ô¨Årst step, anode is chosen by performing random walks on node level;in the second step, an incident will be randomly selectedfrom those reported by the chosen node. Duplicate types ofincidents in a node will be kept because frequency is alsoan important feature of incidents (it impacts the probabilityof being selected). Following the original setting of [22], weset the walk length as 40, i.e., each incident sequence willcontain 40 samples. Finally, the incident sequences will be fedinto a Word2Vec model [24] for embedding vector learning.The Word2Vec model has two important hyper-parameters:the window size and the dimension of the embedding vector.We set the window size as ten by following [22] and set thedimension as 128. In particular, by considering the topologicaldistance between incidents, we can alleviate the problem ofbackground noise. This is because as the distance increases,the impact of noisy incidents gradually weakens, while in FP-Growth, all incidents play an equivalent role in a transaction.
E. Online Incident Aggregation
With the learned incident representation from the last phase,
we can conduct incident aggregation in production environ-
ments, where the incidents come in a streaming manner. Eachgroup of aggregated incidents represents a speciÔ¨Åc type ofservice issue, such as hardware issue, network trafÔ¨Åc issue,network interface down, etc. The EVT-based method also playsa role in this phase by continuously monitoring the number ofincidents per minute. If it alerts a failure, the online incident
435aggregation will be triggered. When two incidents, say iandj,
appear consecutively, GRLIA measures their similarity. If the
similarity score is greater than a predeÔ¨Åned threshold, they willbe grouped together immediately. In particular, the similarityscore consists of two parts, i.e., historical closeness (HC) and
topological rescaling (TR), which are deÔ¨Åned as follows:
HC(i,j)=œë
i¬∑œëj
/bardblœëi/bardbl√ó/bardblœëj/bardbl
TR(i,j)=1
max(1,d(i,j)‚àíT)(7)
whereœëiandœëjare the embedding vectors of incident iand
j(as described in Section III-D), respectively; d(i,j)is the
topological distance between iandj, which is the number of
hops along their shortest path in the system topology; and T
is the threshold for considering the penalty of long distance.That is, the topological rescaling becomes effective (i.e., <1)
only if their distance is larger than T. In our experiments, T
is set as four. Incorrect correlations will be learned if Tis
too large, while important correlations will be missed if Tis
too small. Our experiments show similar results when Tis in
[3,6]. Cosine similarity is adopted to calculate the historical
closeness, which is related to their co-occurrences in the past.Finally, the similarity between iandjcan be obtained by
taking the product of TR(i,j)andHC(i,j):
sim(i,j)=TR(i,j)√óHC(i,j)
=1
max(1,d(i,j)‚àíT)√óœëi¬∑œëj
/bardblœëi/bardbl√ó/bardblœëj/bardbl(8)
We set an aggregation threshold Œªforsim(i,j)to consider
whether or not two incidents are correlated:
cor(i,j)=/braceleftBigg
1,i f s i m (i,j)‚â•Œª
0,o t h e r w i s e(9)
In our experiments, Œªis empirically set as 0.7. In particular,
the distance of an incident to a group of incidents is deÔ¨Åned asthe largest value obtained through element-wise comparison.
IV . E
XPERIMENTS
In this section, we evaluate our framework using real-
world incidents collected from industry. Particularly, we aimat answering the following research questions.
RQ1: How effective is the service failure detection module
of GRLIA?
RQ2: How effective is GRLIA in incident aggregation?
RQ3: Can the failure-impact graph help incident aggrega-
tion?
A. Experiment Setting
1) Dataset: Incident aggregation is a typical problem across
different online service systems. In this experiment, we select a
representative, large-scale system, i.e., the Networking serviceof Huawei Cloud, to evaluate the proposed framework. Besidesoffering traditional services such as Virtual Network, VPNGateway, it also features intelligent IP networks and othernext-generation network solutions. In particular, the servicesystem comprises a large and complex topological structure.In the layer of infrastructure, platform, and software, ithas multiple instances of virtual machines, containers, andapplications, respectively. In each layer, their dependenciesform a topology graph. The cross-layer topology is mainlyconstructed by their placement relationships, i.e., the mappingsbetween applications, containers, and virtual machines. Likeother cloud enterprises, Huawei Cloud‚Äôs resources are hostedin multiple regions and endpoints worldwide. Each regionis composed of several availability zones (isolated locationswithin regions from which public online services originateand operate) for service reliability assurance. The incidentmanagement of the Networking service is also conducted insuch a multi-region way, with each region having relativelyisolated issues. In this paper, we collect incidents generatedbetween May 2020 and November 2020, during which theNetworking service reported a large number of incidents.Although we conduct the evaluation on a single online servicesystem, we believe GRLIA can be easily applied to otheronline service systems and bring them beneÔ¨Åts.
To evaluate the effectiveness of GRLIA, experienced do-
main engineers manually labeled related incidents. Thanksto the well-designed incident management system with user-friendly interfaces, the engineers can quickly perform thelabeling. Note that the manual labels are only required forevaluating the effectiveness of our framework, which is unsu-pervised. To calculate the KPI trend similarity, we adopt thefollowing KPIs, which are suggested by the engineers:
‚Ä¢CPU utilization refers to the amount of processing re-
sources used.
‚Ä¢Round-trip delay records the amount of time it takes to
send a data packet plus the time it takes to receive anacknowledgement of that data packet.
‚Ä¢Port in-bound/out-bound trafÔ¨Åc rate refers to the average
amount of data coming-in to/going-out of a port.
‚Ä¢In-bound packet error rate calculates the error rate of the
packet that a network interface receives.
‚Ä¢Out-bound packet loss rate calculates the loss rate of the
packet that a network interface sends.
These KPIs are representative that characterize the basic
states of the Networking service system. In particular, CPUutilization is monitored for different containers and virtualmachines, while the remaining KPIs are monitored for thevirtual interfaces of each network device. Each KPI is calcu-lated or sampled every minute. We collect two hours of datato measure the KPI trend similarity. Note that the set of KPIscan be tailored for different systems. For example, a databaseservice may also care about the number of failed databaseconnection attempts, the number of SQL queries, etc.
In this paper, we select the largest ten availability zones for
experiments, each of which contains a large system topology.Six months of production incidents are collected from theNetworking service of Huawei Cloud. The number of distinct
436TABLE II
DATASET STATISTICS
'DWDVHW 7UDLQLQJSHULRG (YDOXDWLRQPRQWK ,QFLGHQWV )DLOXUHV
'DWDVHW 0D\-XO\ $XJ aNaN 
'DWDVHW 0D\$XJ 6HSW aNaN 
'DWDVHW 0D\6HSW 2FW aNaN 
incident types is more than 3,000. Similar to [4], [15], [25], we
conduct three groups of experiments using incidents reportedin the Ô¨Årst four months, the Ô¨Årst Ô¨Åve months, and all months,respectively. In all periods, incident aggregation is appliedto the failures that happened in the last month based onthe incident representations learned from previous months.Table II summarizes the dataset. For column #Incidents (resp.
#Failures), the Ô¨Årst Ô¨Ågure calculates the incidents (resp. fail-ures) captured during the training period, while the secondÔ¨Ågure shows that of the evaluation month. Particularly, somefailures are of small scale and can be quickly mitigated, whileothers are cross-region and become an expensive drain onthe company‚Äôs revenue. We can see each failure is associatedwith roughly 200 incidents, demonstrating a strong need forincident aggregation.
2) Evaluation Metrics: For RQ1, which is a binary classi-
Ô¨Åcation problem, we employ precision, recall, and F1 score
for evaluation. SpeciÔ¨Åcally, precision measures the percentageof incident bursts that are successfully identiÔ¨Åed as servicefailures over all the incident bursts that are predicted asfailures:precision =
TP
TP+FP. Recall calculates the portion
of service failures that are successfully identiÔ¨Åed by GRLIAover all the actual service failures: recall=
TP
TP+FN. Finally,
F1 score is the harmonic mean of precision and recall:F1score=
2√óprecision √órecall
precision +recall.TPis the number of service
failures that are correctly discovered by GRLIA; FP is the
number of trivial incident bursts (i.e., no failure is actuallyhappening) that are wrongly predicted as service failures byGRLIA; FN is the number of service failures that GRLIA
fails to discover.
For RQ2 and RQ3, we choose Normalized Mutual In-
formation (NMI) [26], which is a widely used metric forevaluating the quality of clustering algorithms. The value ofNMI ranges from 0 to 1 with 0 indicating the worst result(no mutual information) and 1 the best (perfect correlation):
NMI(Œ©,C)=
2√óI(Œ©;C)
H(Œ©)+H(C), whereŒ©is the set of clusters, C
is the set of classes, H(¬∑)is the entropy, and I(Œ©;C)calculates
and mutual information between Œ©andC.
3) Implementation: Our framework is implemented in
Python. We parallelize our experiments by assigning availabil-
ity zones to different processors. The output of each processoris a list of incident sequences generated through random walk,which we merge and feed to a Word2Vec model implementedwith Gensim [27], an open-source library for topic modelingand natural language processing. We run our experiments ona machine with 20 Intel(R) Xeon(R) Gold 6148 CPU @2.60GHz, and 256GB of RAM. The results show that eachphase of our framework takes only a few seconds. The lastphase can even produce results in a real-time manner as itonly involves simple vector calculation. Thus, our frameworkcan quickly respond in online scenarios. This demonstratesthat GRLIA is of high efÔ¨Åciency.
B. Comparative Methods
The following methods are selected for comparative evalu-
ation of GRLIA.
‚Ä¢FP-Growth [21]. FP-Growth is a widely-used algorithm
for association pattern mining. It is utilized as an analyt-
ical process that Ô¨Ånds a set of items that frequently co-occur in datasets. In our experiments, each impact graphis regarded as a transaction for this algorithm. Given a setof impact graphs, it searches incidents that often appeartogether, regardless of their distance.
‚Ä¢UHAS [4]. This approach is proposed by Zhao et al. aim-
ing at handling alert storms for online service systems.Similar to incident bursts, alert storms also serve as asignal for service failures. Particularly, UHAS employsDBSCAN for alert clustering based on their textual andtopological similarity. The textual similarity between twoalerts is measured by Jaccard distance. The topologicalsimilarity considers two types of topologies, i.e., softwaretopology (service) and hardware topology (server). Thetopological distance is computed by the shortest pathlength between two nodes. Finally, a weighted combi-nation of the two types of similarities yields the Ô¨Ånalsimilarity score.
‚Ä¢LiDAR [3]. LiDAR is a supervised method proposedby Chen et al. to identify linked incidents in large-scale online service systems. SpeciÔ¨Åcally, LiDAR iscomposed of two modules, i.e., textual encoding moduleand component embedding module. The Ô¨Årst moduleproduces similar representations for the text descriptionof linked incidents, which are labeled by engineers. Inthe evaluation stage, the textual similarity between twoincidents is measured by the cosine distance of their rep-resentations. The second module learns a representationfor the system topology (instead of incidents). The Ô¨Ånalsimilarity is calculated by taking a weighted sum of bothparts. As LiDAR is supervised, it would be unfair tocompare it with other unsupervised methods. Consideringthe success of Word2Vec model [24], [28] in identifyingsemantically similar words (in an unsupervised manner),we alter LiDAR to be unsupervised to Ô¨Åt our scenario byrepresenting the text of incidents with off-the-shelf wordvectors [29].
C. Experimental Results
1) RQ1: The Effectiveness of GRLIA‚Äôs Service Failure De-
tection: To answer this research question, we compare GRLIA
with the Ô¨Åxed thresholding method on three datasets andreport precision, recall, and F1 score. Thresholding remains aneffective way for anomaly detection in production systems andserves as a baseline in many existing work. Since both methodsrequire no parameter training, we use them to detect failures
437TABLE III
EXPERIMENTAL RESULTS OF SERVICE FAILURE DETECTION
'DWDVHW 0HWULF 7KUHVKROGLQJ *5/,$
3UHFLVLRQ  
'DWDVHW 5HFDOO  
)6FRUH  
3UHFLVLRQ  
'DWDVHW 5HFDOO  
)6FRUH  
3UHFLVLRQ  
'DWDVHW 5HFDOO  
)6FRUH  
for both the training data and evaluation data. Particularly, the
threshold of the baseline method is #incidents/min> 50, which
is recommended by Ô¨Åeld engineers. Moreover, the ground truthis obtained directly from the historical failure tickets, whichare stored in the incident management system.
The results are shown in Table III, where GRLIA outper-
forms the Ô¨Åxed thresholding in all datasets and metrics. Inparticular, GRLIA achieves an F1 score of more than 0.93in different datasets, demonstrating its effectiveness in servicefailure detection. Indeed, we observe that some failures maynot always incur a large number of incidents at the beginning.However, if ignored, they could become worse and end upyielding more severe impacts across multiple services. Fixedthresholding does not possess the merit of threshold adaptationbased on the context and thus produces many false positives.GRLIA outperforms it for being able to adjust the thresholdautomatically.
2) RQ2: The Effectiveness of GRLIA in Incident Aggre-
gation: We compare the performance of GRLIA against a
series of baseline methods for incident aggregation. Table IVshows the NMI values of different experiments. From dataset1 to 3, GRLIA achieves an NMI score of 0.831, 0.866, and0.912, respectively, while the best results from the baselinemethods are 0.742, 0.758, and 0.826, all attained by Li-DAR. LiDAR outperforms UHAS by explicitly consideringthe entire system topology. Except for UHAS, all approachesachieve better performance with more training data available.This is because UHAS directly works on alert storms whenfailures are detected. Without learning from the history, itcannot handle complicated scenarios. Recall that both UHASand LiDAR rely on the textual similarity between incidents.However, in our system, related incidents do not necessarilypossess similar text descriptions. For example, there is a clearcorrelation between the incident ‚ÄúTrafÔ¨Åc drops sharply invRouter‚Äù and ‚ÄúOS network ping abnormal‚Äù in VPC service,which tends to be missed by them. Moreover, monitors thatrender incidents are conÔ¨Ågured by multiple service teams,which further damages the credit of textual similarity. Thisis particularly true for some critical incidents because theyare often tailored for special system errors, which may not beshared across different services. On the other hand, althoughTABLE IV
EXPERIMENTAL RESULTS OF INCIDENT AGGREGATION
0HWKRG 'DWDVHW 'DWDVHW 'DWDVHW
)3*URZWK   
8+$6   
/L'$5   
*5/,$   
GRLIA does not explicitly leverage incident‚Äôs textual features,
our experiments show that it is capable of correlating incidentsthat share some common words, e.g., ‚ÄúVPC service tomcatport does not exist‚Äù and ‚ÄúVPC service tomcat status is dead‚Äù.This is because such a relationship is reÔ¨Çected in their temporaland topological locality, which can be precisely captured byincidents‚Äô representation vectors.
Another observation is that FP-Growth does not Ô¨Åt the
task of incident aggregation, whose best NMI score is 0.546.As discussed in Section III-D, this method is not robustagainst background noise. Indeed, in the system, some trivialincidents (e.g., ‚ÄúVirtual machine is in abnormal state‚Äù) arecontinuously being reported, which may connect incidentsfrom distinct groups. Furthermore, many essential incidentsare excluded by this method due to low frequency, whichis undesirable. This problem can be effectively alleviatedby leveraging the topological relationship between incidentsas done by other approaches. According to Eq. 5, the im-pact of background noise weakens with distance. However,in FP-Growth, each incident co-occurrence will be countedequally towards the Ô¨Ånal association rules. UHAS considersthe topological similarity by simply calculating the distance.LiDAR employs a more expressive machine learning model,i.e., node2vec [22], an algorithmic framework for learning a
continuous representation for a network‚Äôs nodes. However,they both ignore the problem of incomplete failure-impactgraph, which is a common issue in online service systemsaccording to our study. The necessity of completing the impactgraph will be demonstrated in RQ3. Moreover, different fromthe traditional applications of graph representation learning,we learn a representation for each unique type of incident,which compactly encodes its relationship with others.
3) RQ3: The Necessity of the Failure-Impact Graph for
Incident Aggregation: We demonstrate the importance of
impact graphs by creating a variant of GRLIA without thephase of failure-impact graph completion (i.e., phase two inFig. 3), denoted as GRLIA
/prime. We follow LiDAR to remove
this feature, which considers two incidents as related onlywhen they are directly connected in the system topology.The experimental results are presented in Table V, where wecan see a noticeable drop in the NMI score for all datasets.Due to the high complexity and large scale of online servicesystems, monitors are often conÔ¨Ågured in an ad-hoc manner.These monitors may not be able to accommodate to the ever-changing systems and environments. Thus, some incidents arenot successfully captured by them. System engineers may
438TABLE V
EXPERIMENTAL RESULTS OF INCIDENT AGGREGATION USING GRLIA ( W/
AND W /O FAILURE -IMPACT GRAPH COMPLETION )
0HWKRG 'DWDVHW 'DWDVHW 'DWDVHW
*5/,$   
*5/,$
   
incorrectly perceive the service as healthy, which is a typical
situation of gray failures [10]. Without completing the impactgraph of failures, the true correlations among incidents cannotbe fully recovered.
D. Threats to V alidity
During our study, we have identiÔ¨Åed the following major
threats to the validity.
Labeling noise. Our experiments are conducted based on
six months of real-world incidents collected from Huawei
Cloud. The evaluation requires engineers to inspect and labelthe incidents manually. Label noises (false positives/falsenegatives) may be introduced during the manual labelingprocess. However, the engineers we invite are cloud systemprofessionals and have years of system troubleshooting expe-rience. Moreover, the labeling work can be done quickly andconÔ¨Ådently thanks to the incident management system whichhas user-friendly interfaces. Therefore, we believe the amountof noise is small (if it exists).
Selection of study subjects. In our experiments, we only
collect incidents from one online service of Huawei Cloud,i.e., the Networking service. This is a large-scale service thatsupports many upper-layer services such as web application,virtual machine. SufÔ¨Åcient data can be collected from thisservice system. Another beneÔ¨Åt we can enjoy is that thetopology of the Networking service system is readily availableand accurate. Although we use the Networking service as thesubject in this paper, our proposed framework is generalizable,as this service is a typical, representative online service. Thus,we believe GRLIA can be applied to other services and cloudcomputing platforms and bring them beneÔ¨Åts.
The second type of subject that could threaten the validity
is the KPI. In production systems, there is a large amountof KPIs available to gauge the similarity between two nodes.Although we only select six representative KPIs (as presentedin Section IV-A1), they record the basic and critical states ofa service component. Thus, we believe they are able to proÔ¨Ålethe service system comprehensively.
Implementation and parameter setting. The implemen-
tation and parameter setting are two critical internal threatsto the validity. To reduce the threat of implementation, weemploy peer code review. SpeciÔ¨Åcally, the authors are invitedto carefully check others‚Äô code for mistakes. In terms ofparameter setting, we conduct many groups of comparativeexperiments with different parameters. We choose the param-eters by following the original work or empirically based onthe best experimental results. In particular, we found GRLIAis not very sensitive to the parameter setting.V. D
ISCUSSION
A. Success Story
GRLIA has been successfully incorporated into the inci-
dent management system of Huawei Cloud. Based on thepositive feedback we have received, on-site engineers (OSEs)highly appreciated the novelty of our approach and beneÔ¨Åtedfrom it during their daily system maintenance. SpeciÔ¨Åcally,OSEs conÔ¨Årmed the difÔ¨Åculty of the auto-detection of ser-vice failures in the existing monitoring system. This is be-cause simple detection techniques (e.g., Ô¨Åxed thresholding)are widely adopted. GRLIA introduces more intelligence andautomation by leveraging EVT-based incident burst detection.Interestingly, OSEs found problems for some monitors bycomparing their conÔ¨Ågurations with the aggregated incidents,including wrong names, missing information, etc. Meanwhile,during failure diagnosis, incident aggregation assists OSEsin reducing their investigation scope. Before the deploymentof GRLIA, they would have to examine a large number ofincidents to locate the failures.
To quantify the practical beneÔ¨Åts conveyed to the Network-
ing service system, we further collect failure tickets generatedduring November 2020. In total, 26 failures are recorded.We calculate the average failure handling time in Novemberand compare it with that in August, September, and October.Results show that the time reduction rate is 24.8%, 21.9%, and18.6%, respectively, demonstrating the effectiveness of GRLIAin accelerating the incident management of Huawei Cloud.
B. Lessons Learned
Optimizing monitor conÔ¨Ågurations. Today, popular online
services are serving tens of millions of customers. During daily
operations, they can produce terabytes and even petabytes oftelemetry data such as KPIs, logs, and incidents. However, themajority of these data does not contain much valuable infor-mation for service failure analysis. For example, a signiÔ¨Åcantportion of KPIs only record plain system runtime states; mostof the incidents are trivial and likely to mitigate automaticallywith time. The conÔ¨Åguration of system monitors should beoptimized to report more important yet fewer incidents. Inthe meantime, monitor conÔ¨Ågurations show different stylesacross different service teams, making the monitoring dataheterogeneous. Standards should be established for monitorconÔ¨Ågurations so that high-quality incidents can be created tofacilitate the follow-up system analysis, e.g., fault localization.
Building data collection pipeline . In online service sys-
tems, IT operations play a critical role in system mainte-nance. Since it is data-driven by nature, modern cloud ser-vice providers should build a complete and efÔ¨Åcient pipelinefor monitoring data collection. Common data quality issuesinclude extremely imbalanced data, small quantity of data,poor signal-to-noise ratio, etc. In general, we are facing thefollowing three challenges: 1) What data should be collected?
We need to identify what metrics and events that are mostrepresentative for cloud resource health. Not everything thatcan be measured needs to be monitored. 2) How to collect
439and label data? Labeling incidents (e.g., incident linkages,
culprit incidents) requires OSEs to have a decent knowledge
about the cloud systems. Since they often devote themselvesto emerging issue mitigation and resolution, tools should bedeveloped to facilitate the labeling process, such as labelrecommendation and friendly interfaces. 3) How to store and
query data? Today‚Äôs cloud monitoring data are challenging the
conventional database systems. To save space, domain-speciÔ¨Åccompression techniques should be developed, for example, logcompression [30]‚Äì[32].
VI. R
ELATED WORK
A. Problem IdentiÔ¨Åcation
To provide high-quality online services, many researchers
have conducted a series of investigations, including problemidentiÔ¨Åcation and incident diagnosis from runtime log dataand alerts [25], [33], [34]. For example, to identify problemsfrom a large volume of log data, Lin et al. [33] proposedLogCluster to cluster log sequences and pick the center ofeach cluster. Rosenberg et al. [35] extended LogCluster byincorporating dimension reduction techniques to solve thehigh-dimension challenge of log sequence vectors. Inspired byLogCluster [33], Zhao et al. [4] clustered online service alertsto identify the representative alerts to engineers. Different fromthe clustering techniques, Jiang et al. [34] proposed an alertprioritization approach by ranking the importance of alertsbased on the KPIs in alert data. The top-ranked alerts are morevaluable to identifying problems. However, this approach hasa limited scope of application because it is only practical toKPI alerts generated from manually deÔ¨Åned threshold rules.To conduct problem identiÔ¨Åcation more aggressively, Chen etal. [36] proposed an incident diagnosis framework to predictgeneral incidents by analyzing their relationships with differentalerting signals. Zhao et al. [37] considered a more practicalscenario where there are plenty of noisy alerts in online servicesystems. They proposed eWarn to Ô¨Ålter out the noisy alerts andgenerate interpretable results for incident prediction.
B. Incident Management
In recent years, cloud computing has gained unprecedented
popularity, and incidents are almost inevitable. Thus, incident
management becomes a hotspot topic in both academia andindustry. Massive amount of effort has been devoted to incidentdetection [15], [37]‚Äì[39] and incident triage [38], [40]‚Äì[42].For example, Lim et al. [43] utilized Hidden Markov RandomField for performance issue clustering to identify representa-tive issues. Chen et al. [42] proposed DeepCT, a deep learning-based approach that is able to accumulate knowledge fromincidents‚Äô discussions and automate incident triage. However,due to high manual examination costs, these methods cannothandle the overwhelming number of incidents. Many existingwork [4], [44] address this problem by reducing the duplicatedor correlated alerts. For example, Zhao et al. [45] aimed torecommend the severe alerts to engineers. Lin et al. [39]proposed an alert correlation method to cluster semi-structuredalert texts to gain insights from the clustering results.Similar to our method, Zhao et al. [4] conducted alert
reduction by calculating their textual and topological simi-larity. The centroid alert of each cluster is then selected asthe representative incident to engineers. SpeciÔ¨Åcally, they Ô¨Årstleveraged conventional methods to detect alert storms and theassociated anomalous alerts, and then adopted DBSCAN [46]to cluster alerts based on their textual and topological sim-ilarity. Another similar work is LiDAR proposed by Chenet al. [3], which links relevant incidents by incorporating therepresentation of cloud components. Their framework consistsof two modules, a textual encoding module and a componentembedding module. The Ô¨Årst module learns a representationvector for incident‚Äôs description in a supervised manner. Thetextual similarity between two incidents is measured by thecosine distance of their representation vectors. Similarly, thesecond module learns a vector for system components. TheÔ¨Ånal similarity is calculated by leveraging two parts of infor-mation. However, these methods employ a simple weightedsum to combine the information from different sources, andstill hardly capture the relationship between incidents. Differ-ently, our method utilizes sophisticated graph representationlearning to obtain the semantic relationship of incidents fromdiverse sources, including temporal locality, topological struc-ture, and KPI metric data. Moreover, many existing incidentmanagement methods rely on supervised machine learningtechniques to detect anomalies or conduct incident triage.More intelligent approaches with weak-supervision or evenunsupervised frameworks are still largely unexplored.
VII. C
ONCLUSION
In this paper, we propose GRLIA, an incident aggrega-
tion framework based on graph representation learning. Therepresentation for different types of incidents is learned inan unsupervised and uniÔ¨Åed fashion, which encodes the in-teractions among incidents in both temporal and topologicaldimensions. Online incident aggregation can be efÔ¨Åcientlyperformed by calculating their distance. We have conductedexperiments with real-world incidents collected from HuaweiCloud. Compared with Ô¨Åxed thresholding, GRLIA achievesbetter performance for being able to adjust the thresholdautomatically. In terms of online incident aggregation, GRLIAalso outperforms existing methods by a noticeable margin,conÔ¨Årming its effectiveness. Furthermore, our framework hasbeen successfully incorporated into the incident managementsystem of Huawei Cloud. Feedback from on-site engineersconÔ¨Årms its practical usefulness. We believe our proposedincident aggregation framework can assist engineers in failureunderstanding and diagnosis.
A
CKNOWLEDGEMENT
The work was supported by Key-Area Research and
Development Program of Guangdong Province (No.2020B010165002), the Research Grants Council of theHong Kong SAR, China (CUHK 14210920), and AustralianResearch Council (ARC) Discovery Project (DP200102940).
440REFERENCES
[1] S. Wolfe, ‚ÄúAmazon‚Äôs one hour of downtime on prime
day may have cost it up to $100 million in lost
sales,‚Äù 2018. [Online]. Available: https://www.businessinsider.com/amazon-prime-day-website-issues-cost-it-millions-in-lost-sales-2018-7
[2] Z. Chen, Y . Kang, L. Li, X. Zhang, H. Zhang, H. Xu, Y . Zhou, L. Yang,
J. Sun, Z. Xu et al., ‚ÄúTowards intelligent incident management: why
we need it and how we make it,‚Äù in Proceedings of the 28th ACM Joint
Meeting on European Software Engineering Conference and Symposiumon the F oundations of Software Engineering, 2020, pp. 1487‚Äì1497.
[3] Y . Chen, X. Yang, H. Dong, X. He, H. Zhang, Q. Lin, J. Chen, P. Zhao,
Y . Kang, F. Gao et al., ‚ÄúIdentifying linked incidents in large-scale
online service systems,‚Äù in Proceedings of the 28th ACM Joint Meeting
on European Software Engineering Conference and Symposium on theF oundations of Software Engineering, 2020, pp. 304‚Äì314.
[4] N. Zhao, J. Chen, X. Peng, H. Wang, X. Wu, Y . Zhang, Z. Chen,
X. Zheng, X. Nie, G. Wang et al., ‚ÄúUnderstanding and handling alert
storm for online service systems,‚Äù in Proceedings of the ACM/IEEE
42nd International Conference on Software Engineering: Software En-gineering in Practice, 2020, pp. 162‚Äì171.
[5] Z. Chen, Y . Kang, F. Gao, L. Yang, J. Sun, Z. Xu, P. Zhao, B. Qiao,
L. Li, X. Zhang et al., ‚ÄúAiops innovations of incident management for
cloud services,‚Äù 2020.
[6] O. Team, ‚ÄúGrlia: Graph-based incident aggregation for large-
scale online service systems,‚Äù 2021. [Online]. Available: https://github.com/OpsPAI/grlia
[7] M. J. Kavis, Architecting the cloud: design decisions for cloud com-
puting service models (SaaS, PaaS, and IaaS). John Wiley & Sons,2014.
[8] S.-P. Ma, C.-Y . Fan, Y . Chuang, W.-T. Lee, S.-J. Lee, and N.-L. Hsueh,
‚ÄúUsing service dependency graph to analyze and test microservices,‚Äù in2018 IEEE 42nd Annual Computer Software and Applications Confer-ence (COMPSAC), vol. 2. IEEE, 2018, pp. 81‚Äì86.
[9] A. Natarajan, P. Ning, Y . Liu, S. Jajodia, and S. E. Hutchinson, NSD-
Miner: Automated discovery of network service dependencies. IEEE,2012.
[10] P. Huang, C. Guo, L. Zhou, J. R. Lorch, Y . Dang, M. Chintalapati, and
R. Yao, ‚ÄúGray failure: The achilles‚Äô heel of cloud-scale systems,‚Äù inProceedings of the 16th Workshop on Hot Topics in Operating Systems,2017, pp. 150‚Äì155.
[11] W. L. Hamilton, R. Ying, and J. Leskovec, ‚ÄúRepresentation learning on
graphs: Methods and applications,‚Äù arXiv preprint arXiv:1709.05584,
2017.
[12] J. Zhou, G. Cui, Z. Zhang, C. Yang, Z. Liu, L. Wang, C. Li, and M. Sun,
‚ÄúGraph neural networks: A review of methods and applications,‚Äù arXiv
preprint arXiv:1812.08434, 2018.
[13] A. Siffer, P.-A. Fouque, A. Termier, and C. Largouet, ‚ÄúAnomaly detec-
tion in streams with extreme value theory,‚Äù in Proceedings of the 23rd
ACM SIGKDD International Conference on Knowledge Discovery andData Mining, 2017, pp. 1067‚Äì1075.
[14] L. De Haan and A. Ferreira, Extreme value theory: an introduction.
Springer Science & Business Media, 2007.
[15] Q. Lin, K. Hsieh, Y . Dang, H. Zhang, K. Sui, Y . Xu, J.-G. Lou, C. Li,
Y .W u ,R .Y a oet al., ‚ÄúPredicting node failure in cloud service systems,‚ÄùinProceedings of the 26th ACM Joint Meeting on European Software
Engineering Conference and Symposium on the F oundations of SoftwareEngineering (ESEC/FSE). ACM, 2018, pp. 480‚Äì490.
[16] K. Hundman, V . Constantinou, C. Laporte, I. Colwell, and T. Soder-
strom, ‚ÄúDetecting spacecraft anomalies using lstms and nonparametricdynamic thresholding,‚Äù in Proceedings of the 24th ACM SIGKDD
international conference on knowledge discovery & data mining , 2018,
pp. 387‚Äì395.
[17] Z. Yang, R. Algesheimer, and C. J. Tessone, ‚ÄúA comparative analysis
of community detection algorithms on artiÔ¨Åcial networks,‚Äù ScientiÔ¨Åc
reports, vol. 6, p. 30750, 2016.
[18] V . D. Blondel, J.-L. Guillaume, R. Lambiotte, and E. Lefebvre, ‚ÄúFast
unfolding of communities in large networks,‚Äù Journal of statistical
mechanics: theory and experiment, vol. 2008, no. 10, p. P10008, 2008.
[19] P. Liu, Y . Chen, X. Nie, J. Zhu, S. Zhang, K. Sui, M. Zhang, and D. Pei,
‚ÄúFluxrank: A widely-deployable framework to automatically localizingroot cause machines for software service failure mitigation,‚Äù in 2019
IEEE 30th International Symposium on Software Reliability Engineering(ISSRE). IEEE, 2019, pp. 35‚Äì46.[20] E. Keogh and C. A. Ratanamahatana, ‚ÄúExact indexing of dynamic time
warping,‚Äù Knowledge and information systems, vol. 7, no. 3, pp. 358‚Äì
386, 2005.
[21] J. Han, J. Pei, and Y . Yin, ‚ÄúMining frequent patterns without candidate
generation,‚Äù ACM sigmod record, vol. 29, no. 2, pp. 1‚Äì12, 2000.
[22] A. Grover and J. Leskovec, ‚Äúnode2vec: Scalable feature learning for
networks,‚Äù in Proceedings of the 22nd ACM SIGKDD international
conference on Knowledge discovery and data mining, 2016, pp. 855‚Äì864.
[23] B. Perozzi, R. Al-Rfou, and S. Skiena, ‚ÄúDeepwalk: Online learning
of social representations,‚Äù in Proceedings of the 20th ACM SIGKDD
international conference on Knowledge discovery and data mining,2014, pp. 701‚Äì710.
[24] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean,
‚ÄúDistributed representations of words and phrases and their composi-tionality,‚Äù in Advances in neural information processing systems, 2013,
pp. 3111‚Äì3119.
[25] S. He, Q. Lin, J.-G. Lou, H. Zhang, M. R. Lyu, and D. Zhang,
‚ÄúIdentifying impactful service system problems via log analysis,‚Äù inProceedings of the 2018 26th ACM Joint Meeting on European SoftwareEngineering Conference and Symposium on the F oundations of SoftwareEngineering - ESEC/FSE 2018. ACM Press, pp. 60‚Äì70.
[26] Stanford, Evaluation of clustering, 2008 [Online; accessed
November-2020], https://nlp.stanford.edu/IR-book/html/htmledition/evaluation-of-clustering-1.html.
[27] R. ÀáRehÀöuÀárek, ‚ÄúGensim: topic modelling for humans,‚Äù 2009. [Online].
Available: https://radimrehurek.com/gensim/
[28] T. Mikolov, E. Grave, P. Bojanowski, C. Puhrsch, and A. Joulin, ‚ÄúAd-
vances in pre-training distributed word representations,‚Äù arXiv preprint
arXiv:1712.09405, 2017.
[29] A. Joulin, E. Grave, P. Bojanowski, M. Douze, H. J ¬¥egou, and T. Mikolov,
‚ÄúFasttext. zip: Compressing text classiÔ¨Åcation models,‚Äù arXiv preprint
arXiv:1612.03651, 2016.
[30] J. Liu, J. Zhu, S. He, P. He, Z. Zheng, and M. R. Lyu, ‚ÄúLogzip: extract-
ing hidden structures via iterative clustering for log compression,‚Äù in
2019 34th IEEE/ACM International Conference on Automated SoftwareEngineering (ASE). IEEE, 2019, pp. 863‚Äì873.
[31] R. Christensen and F. Li, ‚ÄúAdaptive log compression for massive log
data.‚Äù in SIGMOD Conference, 2013, pp. 1283‚Äì1284.
[32] P. He, Z. Chen, S. He, and M. R. Lyu, ‚ÄúCharacterizing the natural
language descriptions in software logging statements,‚Äù in 2018 33rd
IEEE/ACM International Conference on Automated Software Engineer-ing (ASE). IEEE, 2018, pp. 178‚Äì189.
[33] Q. Lin, H. Zhang, J.-G. Lou, Y . Zhang, and X. Chen, ‚ÄúLog clustering
based problem identiÔ¨Åcation for online service systems,‚Äù in Proceedings
of the 38th International Conference on Software Engineering, ICSE2016, Austin, TX, USA, May 14-22, 2016 - Companion V olume. ACM,pp. 102‚Äì111.
[34] G. Jiang, H. Chen, K. Yoshihira, and A. Saxena, ‚ÄúRanking the impor-
tance of alerts for problem determination in large computer systems,‚Äùvol. 14, no. 3, pp. 213‚Äì227.
[35] C. M. Rosenberg and L. Moonen, ‚ÄúImproving problem identiÔ¨Åcation
via automated log clustering using dimensionality reduction,‚Äù in Pro-
ceedings of the 12th ACM/IEEE International Symposium on EmpiricalSoftware Engineering and Measurement. ACM, pp. 1‚Äì10.
[36] Y . Chen, X. Yang, Q. Lin, H. Zhang, F. Gao, Z. Xu, Y . Dang, D. Zhang,
H. Dong, Y . Xu et al., ‚ÄúOutage prediction and diagnosis for cloud service
systems,‚Äù in Proceedings of the 2019 International Conference on World
Wide Web (WWW), 2019, pp. 2659‚Äì2665.
[37] N. Zhao, J. Chen, Z. Wang, X. Peng, G. Wang, Y . Wu, F. Zhou, Z. Feng,
X.Nie, W. Zhang et al., ‚ÄúReal-time incident prediction for online service
systems,‚Äù in Proceedings of the 28th ACM Joint Meeting on European
Software Engineering Conference and Symposium on the F oundationsof Software Engineering, 2020, pp. 315‚Äì326.
[38] J. Gu, C. Luo, S. Qin, B. Qiao, Q. Lin, H. Zhang, Z. Li, Y . Dang, S. Cai,
W. Wu et al., ‚ÄúEfÔ¨Åcient incident identiÔ¨Åcation from multi-dimensional
issue reports via meta-heuristic search,‚Äù in Proceedings of the 28th
ACM Joint Meeting on European Software Engineering Conference andSymposium on the F oundations of Software Engineering, 2020, pp. 292‚Äì303.
[39] D. Lin, R. Raghu, V . Ramamurthy, J. Yu, R. Radhakrishnan, and J. Fer-
nandez, ‚ÄúUnveiling clusters of events for alert and incident managementin large-scale enterprise it,‚Äù in The 20th ACM SIGKDD International
Conference on Knowledge Discovery and Data Mining, KDD ‚Äô14, New
441York, NY, USA - August 24 - 27, 2014, S. A. Macskassy, C. Perlich,
J. Leskovec, W. Wang, and R. Ghani, Eds. ACM, pp. 1630‚Äì1639.
[40] J. Gao, N. Yaseen, R. MacDavid, F. V . Frujeri, V . Liu, R. Bianchini,
R. Aditya, X. Wang, H. Lee, D. Maltz et al., ‚ÄúScouts: Improving
the diagnosis process through domain-customized incident routing,‚Äù inProceedings of the Annual conference of the ACM Special Interest Groupon Data Communication on the applications, technologies, architectures,and protocols for computer communication, 2020, pp. 253‚Äì269.
[41] J. Chen, X. He, Q. Lin, Y . Xu, H. Zhang, D. Hao, F. Gao, Z. Xu,
Y . Dang, and D. Zhang, ‚ÄúAn empirical investigation of incident triagefor online service systems,‚Äù in Proceedings of the 41st International
Conference on Software Engineering: Software Engineering in Practice(ICSE-SEIP). IEEE Press, 2019, pp. 111‚Äì120.
[42] J. Chen, X. He, Q. Lin, H. Zhang, D. Hao, F. Gao, Z. Xu, Y . Dang, and
D. Zhang, ‚ÄúContinuous incident triage for large-scale online service sys-tems,‚Äù in Proceedings of the 34th IEEE/ACM International Conference
on Automated Software Engineering (ASE). IEEE, 2019, pp. 364‚Äì375.
[43] M.-H. Lim, J.-G. Lou, H. Zhang, Q. Fu, A. B. J. Teoh, Q. Lin, R. Ding,
and D. Zhang, ‚ÄúIdentifying recurrent and unknown performance issues,‚Äùin2014 IEEE International Conference on Data Mining, ICDM 2014,
Shenzhen, China, December 14-17, 2014, R. Kumar, H. Toivonen, J. Pei,J. Z. Huang, and X. Wu, Eds. IEEE Computer Society, pp. 320‚Äì329.
[44] J. Xu, Y . Wang, P. Chen, and P. Wang, ‚ÄúLightweight and adaptive service
api performance monitoring in highly dynamic cloud environment,‚Äù in2017 IEEE International Conference on Services Computing, SCC 2017,Honolulu, HI, USA, June 25-30, 2017, X. F. Liu and U. Bellur, Eds.IEEE Computer Society, pp. 35‚Äì43.
[45] N. Zhao, P. Jin, L. Wang, X. Yang, R. Liu, W. Zhang, K. Sui, and D. Pei,
‚ÄúAutomatically and adaptively identifying severe alerts for online servicesystems,‚Äù in 39th IEEE Conference on Computer Communications,
INFOCOM 2020, Toronto, ON, Canada, July 6-9, 2020. IEEE, pp.2420‚Äì2429.
[46] M. Ester, H.-P. Kriegel, J. Sander, and X. Xu, ‚ÄúA density-based algorithm
for discovering clusters in large spatial databases with noise,‚Äù in Pro-
ceedings of the Second International Conference on Knowledge Discov-ery and Data Mining (KDD-96), Portland, Oregon, USA , E. Simoudis,
J. Han, and U. M. Fayyad, Eds. AAAI Press, pp. 226‚Äì231.
442