Groot: An Event-graph-based Approach for Root
Cause Analysis in Industrial Settings
Hanzhang Wang∗, Zhengkai Wu†, Huai Jiang∗, Yichao Huang∗,
Jiamu Wang∗, Selcuk Kopru∗, Tao Xie§
∗eBay,†University of Illinois at Urbana-Champaign,§Peking University
Email: {hanzwang,huajiang,yichhuang,jiamuwang,skopru}@ebay.com, zw3@illinois.edu, taoxie@pku.edu.cn
Abstract —For large-scale distributed systems, it is crucial to
efﬁciently diagnose the root causes of incidents to maintain
high system availability. The recent development of microservicearchitecture brings three major challenges (i.e., complexities ofoperation, system scale, and monitoring) to root cause analysis(RCA) in industrial settings. To tackle these challenges, in thispaper, we present G
ROOT , an event-graph-based approach for
RCA. G ROOT constructs a real-time causality graph based
on events that summarize various types of metrics, logs, andactivities in the system under analysis. Moreover, to incorporatedomain knowledge from site reliability engineering (SRE) engi-neers, G
ROOT can be customized with user-deﬁned events and
domain-speciﬁc rules. Currently, G ROOT supports RCA among
5,000 real production services and is actively used by the SRE
teams in eBay, a global e-commerce system serving more than159 million active buyers per year. Over 15 months, we collect
a data set containing labeled root causes of 952 real productionincidents for evaluation. The evaluation results show that G
ROOT
is able to achieve 95% top-3 accuracy and 78% top-1 accuracy. Toshare our experience in deploying and adopting RCA in industrialsettings, we conduct a survey to show that users of G
ROOT ﬁnd
it helpful and easy to use. We also share the lessons learnedfrom deploying and adopting G
ROOT to solve RCA problems in
production environments.
Index T erms—microservices, root cause analysis, AIOps, ob-
servability
I. I NTRODUCTION
Since the emergence of microservice architecture [1], it has
been quickly adopted by many large companies such as Ama-
zon, Google, and Microsoft. Microservice architecture aims toimprove the scalability, development agility, and reusability ofthese companies’ business systems. Despite these undeniablebeneﬁts, different levels of components in such a systemcan go wrong due to the fast-evolving and large-scale natureof microservices architecture [1]. Even if there are minimalhuman-induced faults in code, the system might still be at riskdue to anomalies in hardware, conﬁgurations, etc. Therefore, itis critical to detect anomalies and then efﬁciently analyze theroot causes of the associated incidents, subsequently helpingthe system reliability engineering (SRE) team take furtheractions to bring the system back to normal.
In the process of recovering a system, it is critical to conduct
accurate and efﬁcient root cause analysis (RCA) [2], thesecond one of a three-step process. In the ﬁrst step, anomalies
§Tao Xie is also afﬁliated with Key Laboratory of High Conﬁdence
Software Technologies (Peking University), Ministry of Education, China.
Hanzhang Wang is the corresponding author.are detected with alerting mechanisms [3]–[5] based on mon-
itoring data such as logs [6]–[10], metrics/key performanceindicators (KPIs) [11]–[15], or a combination thereof [16],[17]. In the second step, when the alerts are triggered, RCAis performed to analyze the root cause of these alerts andadditional events, and to propose recovery actions from theassociated incident [6], [18], [19]. RCA needs to considermultiple possible interpretations of potential causes for theincident, and these different interpretations could lead todifferent mitigation actions to be performed. In the last step,the SRE teams perform those mitigation actions and recoverthe system.
Based on our industrial SRE experiences, we ﬁnd that RCA
is difﬁcult in industrial practice due to three complexities,particularly under microservice settings:
•Operational Complexity. For large-scale systems, thereare typically centered (aka infrastructure) SRE and do-main (aka embedded) SRE engineers [20]. Their com-munication is often ineffective or limited under the mi-croservice scenarios due to a more diversiﬁed tech stack,granular services, and shorter life cycles than traditionalsystems. The knowledge gap between the centered SREteam and the domain SRE team gets further enlargedand makes RCA much more challenging. Centered SREengineers have to learn from domain SRE engineerson how the new domain changes work to update thecentralized RCA tools. Thus, adaptive and customizableRCA is required instead of one-size-ﬁts-all solutions.
•Scale Complexity. There could be thousands of ser-vices simultaneously running in a large microservicesystem, resulting in a very high number of monitoringsignals. A real incident could cause numerous alerts tobe triggered across services. The inter-dependencies andincident triaging between the services are proportionallymore complicated than a traditional system [15]. To detectroot causes that may be distributed and many steps awayfrom an initially observed anomalous service, the RCAapproach must be scalable and very efﬁcient to digesthigh volume signals.
•Monitoring Complexity. A high quantity of observabilitydata types (metrics, logs, and activities) need to be mon-itored, stored, and processed, such as intra-service andinter-service metrics. Different services in a system may
4192021 36th IEEE/ACM International Conference on Automated Software Engineering (ASE)
DOI 10.1109/ASE51524.2021.000452021 36th IEEE/ACM International Conference on Automated Software Engineering (ASE) | 978-1-6654-0337-5/21/$31.00 ©2021 IEEE | DOI: 10.1109/ASE51524.2021.9678708
978-1-6654-0337-5/21/$31.00  ©2021  IEEE
produce different types of logs or metrics with different
patterns. There are also various kinds of activities, suchas code deployment or conﬁguration changes. The RCAtools must be able to consume such highly diversiﬁed andunstructured data and make inferences.
To overcome the limited effectiveness of existing ap-
proaches [2], [3], [14], [16], [21]–[31] (as mentioned inSection II) in industrial settings due to the aforementionedcomplexities, we propose G
ROOT , an event-graph-based RCA
approach. In particular, G ROOT constructs an event causality
graph, whose basic nodes are monitoring events such asperformance-metric deviation events, status change events,and developer activity events. These events carry detailedinformation to enable accurate RCA. The events and thecausalities between them are constructed using speciﬁed rulesand heuristics (reﬂecting domain knowledge). In contrast tothe existing fully learning-based approaches [3], [10], [23],
G
ROOT provides better transparency and interpretability. Such
interpretability is critical in our industrial settings becausea graph-based approach can offer visualized reasoning withcausality links to the root cause and details of every eventinstead of just listing the results. Besides, our approach canenable effective tracking of cases and targeted detailed im-provements, e.g., by enhancing the rules and heuristics usedto construct the graph.
G
ROOT has two salient advantages over existing graph-
based approaches:
•Fine granularity (events as basic nodes). First, unlike
existing graph-based approaches, which directly use ser-vices [25] or hosts (VMs) [30] as basic nodes, G
ROOT
constructs the causality graph by using monitoring eventsas basic nodes. Graphs based on events from the ser-vices can provide more accurate results to address themonitoring complexity. Second, for the scale complexity,
G
ROOT can dynamically create hidden events or addi-
tional dependencies based on the context, such as addingdependencies to the external service providers and theirissues. Third, to construct the causality graph, G
ROOT
takes the detailed contextual information of each eventinto consideration for analysis with more depth. Doingso also helps G
ROOT incorporate SRE insights with the
context details of each event to address the operationalcomplexity.
•High diversity (a wide range of event types supported).
First, the causality graph in G ROOT supports various
event types such as performance metrics, status logs, anddeveloper activities to address the monitoring complexity.This multi-scenario graph schema can directly boost theRCA coverage and precision. For example, G
ROOT is
able to detect a speciﬁc conﬁguration change on a serviceas the root cause instead of performance anomaly symp-toms, thus reducing triaging efforts and time-to-recovery(TTR). Second, G
ROOT allows the SRE engineers to in-
troduce different event types that are powered by differentdetection strategies or from different sources. For therules that decide causality between events, we designa grammar that allows easy and fast implementationsof domain-speciﬁc rules, narrowing the knowledge gapof the operational complexity. Third, G
ROOT provides a
robust and transparent ranking algorithm that can digestdiverse events, improve accuracy, and produce resultsinterpretable by visualization.
To demonstrate the ﬂexibility and effectiveness of G
ROOT ,
we evaluate it on eBay’s production system that serves morethan 159 million active users and features more than 5,000
services deployed over three data centers. We conduct experi-ments on a labeled and validated data set to show that G
ROOT
achieves 95% top-3 accuracy and 78% top-1 accuracy for 952real production incidents collected over 15 months. Further-more, G
ROOT is deployed in production for real-time RCA,
and is used daily by both centered and domain SRE teams,with the achievement of 73% top-1 accuracy in action. Finally,the end-to-end execution time of G
ROOT for each incident in
our experiments is less than 5 seconds, demonstrating the highefﬁciency of G
ROOT .
We report our experiences and lessons learned when us-
ing G ROOT to perform RCA in the industrial e-commerce
system. We survey among the SRE users and developers of
GROOT , who ﬁnd G ROOT easy to use and helpful during the
triage stage. Meanwhile, the developers also ﬁnd the G ROOT
design to be desirable to make changes and facilitate newrequirements. We also share the lessons learned from adopting
G
ROOT in production for SRE in terms of technology transfer
and adoption.
In summary, this paper makes four main contributions:
•An event-graph-based approach named G ROOT for root
cause analysis tackling challenges in industrial settings.
•Implementation of G ROOT in an RCA framework for
allowing the SRE teams to instill domain knowledge.
•Evaluation performed in eBay’s production environ-ment with more than 5,000 services, for demonstrating
G
ROOT ’s effectiveness and efﬁciency.
•Experiences and lessons learned when deploying andapplying G
ROOT in production.
II. R ELA TED WORK
Anomaly Detection. Anomaly detection aims to detect po-
tential issues in the system. Anomaly detection approachesusing time series data can generally be categorized into threetypes: (1) batch-processing and historical analysis such asSurus [32]; (2) machine-learning-based, such as Donut [12];(3) usage of adaptive concept drift, such as StepWise [33].
G
ROOT currently uses a combination of manually written
thresholds, statistical models, and machine learning (ML)algorithms to detect anomalies. Since our approach is event-driven, as long as fairly accurate alerts are generated, G
ROOT
is able to incorporate them.
Root Cause Analysis. Traditional RCA approaches (e.g.,
Adtributor [34] and HotSpot [35]) ﬁnd the multi-dimensionalcombination of attribute values that would lead to certainquality of service (QoS) anomalies. These approaches are
420effective at discrete static data. Once there are continuous data
introduced by time series information, these approaches wouldbe much less effective.
To tackle these difﬁculties, there are two categories of
approaches based on ML and graph, respectively.
ML-based RCA. Some ML-based approaches use features
such as time series information [23], [30] and features ex-tracted using textual and temporal information [3]. Some otherapproaches [12] conduct deep learning by ﬁrst constructingthe dependency graph of the system and then representingthe graph in a neural network. However, these ML-basedapproaches face the challenge of lacking training data. Gan etal. [10] proposed Seer to make use of historical tracking data.Although Seer also focuses on the microservice scenario, itis designed to detect QoS violations while lacking supportfor other kinds of errors. There is also an effort to useunsupervised learning such as GAN [12], but it is generallyhard to simulate large, complicated distributed systems to givemeaningful data.
Graph-based RCA. A recent survey [2] on RCA approaches
categorizes more than 20 RCA algorithms by more than10 theoretical models to represent the relationships betweencomponents in a microservice system. Nguyen et al. [21]proposed FChain, which introduces time series informationinto the graph, but they still use server/VM as nodes in thegraph. Chen et al. [22] proposed CauseInfer, which constructsa two-layered hierarchical causality graph. It applies metricsas nodes that indicate service-level dependency. Schoenﬁschet al. [24] proposed to use Markov Logic Network to expressconditional dependencies in the ﬁrst-order logic, but still builddependency on the service level. Lin et al. [36] proposedMicroscope, which targets the microservice scenario. It buildsthe graph only on service-level metrics so it cannot get fulluse of other information and lacks customization. Brandon etal. [25] proposed to build the system graph using metrics,logs, and anomalies, and then use pattern matching against alibrary to identify the root cause. However, it is difﬁcult toupdate the system to facilitate the changing requirements. Wuet al. [15] proposed MicroRCA, which models both servicesand machines in the graph and tracks the propagation amongthem. It would be hard to extend the graph from machines tothe concept of other resources such as databases in our paper.
As mentioned in Section I, by using the event graph,
G
ROOT mainly overcomes the limitations of existing graph-
based approaches in two aspects: (1) build a more accurateand precise causality graph use the event-graph-based model;(2) allow adaptive customization of link construction rules toincorporate domain knowledge in order to facilitate the rapidrequirement changes in the microservice scenario.
Our G
ROOT approach uses a customized page rank algo-
rithm in the event ranking, and can also be seen as an unsu-pervised ML approach. Therefore, G
ROOT is complementary
to other ML approaches as long as they can accept our eventcausality graph as a feature.
Settings and Scale. The challenges of operational, scale,
and monitoring complexities are observed, especially beingT ABLE I: The scale of experiments in existing RCA ap-proaches’ evaluations (QPS: Queries per second)
Approach Year Scale Validated on Real Incidents?
FChain [21] 2013 <= 10 VMs No
CauseInfer [22] 2014 20services on 5 servers No
MicroScope [36] 2018 36services, ∼5000 QPS No
APG [30] 2018 <=20 services on 5 VMs No
Seer [10] 2019 <=50 services on 20 servers Partially
MicroRCA [15] 2020 13services, ∼600 QPS No
RCA Graph [25] 2020 <=70 services on 8 VMs No
Causality RCA [31] 2020 <=20 services No
substantial in the industrial settings. Hence, we believe thatthe target RCA approach should be validated at the enterprisescale and against actual incidents for effectiveness.
Table I lists the experimental settings and scale in ex-
isting RCA approaches’ evaluations. All the listed existingapproaches are evaluated in a relatively small scenario. Incontrast, our experiments are performed upon a system con-taining 5,000 production services on hundreds of thousands ofVMs. On average, the sub-dependency graph (constructed inSection IV -A) of our service-based data set is already 77.5services, more than the total number in any of the listedevaluations. Moreover, 7 out of the 8 listed approaches areevaluated under simulative fault injection on top of existingbenchmarks such as RUBiS, which cannot represent real-worldincidents; Seer [10] collects only the real-world results withno validations. Our data set contains 952 actual incidentscollected from real-world settings.
III. M
OTIV A TING EXAMPLES
In this section, we demonstrate the effectiveness of event-
based graph and adaptive customization strategies with twomotivating examples.
Figure 1 shows an abstracted real incident example with the
dependency graph and the corresponding causality graph con-structed by G
ROOT . The Checkout service of our e-commerce
system suddenly gets an additional latency spike due to a codedeployment on the Service-E. The service monitor is reporting
API Call Timeout detected by the ML-based anomaly detection
system. The simpliﬁed sub-dependency graph consisting of 6services is shown in Figure 1a. The initial alert is triggeredon the Checkout (entrance) service. The other nodes Service-*
are the internal services that the Checkout service directly or
indirectly depends on. The color of the nodes in Figure 1aindicates the severity/count of anomalies (alerts) reported oneach service. We can see that Service-B is the most severe
one as there are two related alerts on it. The traditional graph-based approach [25], [30] usually takes into account only thegraph between services in addition to the severity informationon each service. If the traditional approach got applied onFigure 1a, either Service-B, Service-D,o rService-E could be
a potential root cause, and Service-B would have the highest
possibility since it has two related alerts. Such results are notuseful to the SRE teams.
G
ROOT constructs the event-based causality graph as shown
in Figure 1b. The events in each service are used as the nodeshere. We can see that the API Call Timeout issue in Checkout
is possibly caused by API Call Timeout inService-A, which is
421(a) Dependency graph
 (b) Causality graph
Fig. 1: Motivating example of event causality graph
Fig. 2: Example of event type addition
further caused by Latency Spike inDataCenter-A ofService-
C.G ROOT further tracks back to ﬁnd that it is likely caused by
Latency Spike inService-E, which happens in the same data
center. Finally G ROOT ﬁgures out that the most probable root
cause is a recent Code Deployment event in Service-E. The
SRE teams then could quickly locate the root cause and roll
back this code deployment, followed by further investigations.
There are no casual links between events in Service-B and
Service-A, since no causal rules are matched. The API Call
Timeout event is less likely to depend on the event type High
CPU and High GC. Therefore, the inference can eliminate
Service-B from possible root causes. This elimination shows
the beneﬁt of the event-based graph. Note that there is anotherevent Latency Spike inService-D, but not connected to Latency
Spike inService-C in the causality graph. The reason is that
the Latency Spike event in Service-C happens in DataCenter-
A, not DataCenter-B.
Figures 2 and 3 show how SRE engineers can easily change
G
ROOT to adapt to new requirements, by updating the events
and rules. In Figure 2, SRE engineers want to add a newtype of deployment activity, ML Model Deployment. Usually,
the SRE engineers ﬁrst need to select the anomaly detectionmodel or set their own alerts and provide alert/activity datasources for the stored events. In this example, the event can bedirectly fetched from the ML model management system. Then
G
ROOT also requires related properties (e.g., the detection
time range) to be set for the new event type. Lastly, theSRE engineers add the rules for building the causal linksbetween the new event type and existing ones. The bluebox in Figure 2 shows the rule, which denotes the edge
Fig. 3: Example of event and rule update
Fig. 4: Workﬂow of G ROOT
direction, target event, and target service (self, upstream, anddownstream dependency).
Figure 3 shows a real-world example of how G
ROOT is able
to incorporate SRE insights and knowledge. More speciﬁcally,SRE engineers would like to change the rules to allow G
ROOT
to distinguish the latency spikes from different data centers.As an example in Figure 1b, Latency Spike events propagate
only within the same data center. During G
ROOT development,
SRE engineers could easily add new property DataCenter to
the Latency Spike event. Then they add the corresponding
“conditional” rules to be differentiated with the “basic” rulesin Figure 3. In conditional rules, links are constructed onlywhen the speciﬁed conditions are satisﬁed.
422IV . A PPROACH
Figure 4 shows the overall workﬂow of G ROOT . The
triggers for using G ROOT are usually alert(s) from automated
anomaly detection, or sometimes an SRE engineer’s suspi-
cion. There are three major steps: constructing the servicedependency graph, constructing the event causality graph, androot cause ranking. The outputs are the root causes ranked bythe likelihood. To support fast human investigation experience,we build an interactive UI as shown in Figure 8: the servicedependency, events with causal links and additional detailssuch as raw metrics or the developer contact (of a codedeployment event) are presented to the user for next steps.As an ofﬂine part of human investigation, we label/collect adata set, perform validation, and summarize the knowledge forfurther improvement on all incidents on a daily basis.
A. Constructing Service Dependency Graph
The construction of the service dependency graph starts
with the initial alerted or suspicious service(s), denoted as I.
For example, in Figure 1a, I={Checkout}. Ican contain
multiple services based on the range of the trigger alerts or
suspicions. We maintain domain service lists where domain-level alerts can be triggered because there is no clear service-level indication.
At the back end, G
ROOT maintains a global service depen-
dency graph Gglobal via distributed tracing and log analysis.
The directed edge from nodes AtoB(two services or system
components) in the dependency graph indicates a serviceinvocation or other forms of dependency. In Figure 1a, theblack arrows indicate such edges. Bi-directional edges andcycles between the services can be possible and exist. In thiswork, the global dependency graph is updated daily.
The service dependency (sub)graph Gis constructed using
G
global andI. An extended service list Lis ﬁrst constructed
by traversing each service in IoverGglobal for a radius range
r. Each service u∈Lcan be traversed by at least one
servicev∈Iwithinrsteps:L={u|∃v∈I, dist (u,v)≤
r or dist (v,u)≤r}. Then, the service dependency subgraph
Gis constructed by the nodes in Land the edges between
them inGglobal . In our current implementation, ris set to 2,
since this dependency graph may be dynamically extended inthe next steps based on events’ detail for longer issue chainsor additional dependencies.
B. Constructing Event Causality Graph
In the second step, G
ROOT collects all supported events
for each service in Gand constructs the causal links between
events.
1) Collecting Events: Table II presents some example event
types and detection techniques for G ROOT ’s production im-
plementation. For detection techniques, “De Facto” indicates
that the event can be directly collected via a speciﬁc API orstorage. The detection either runs passively in the back endto reduce delay and improve accuracy, or runs actively foronly the services within the dependency graph range to saveresources.T ABLE II: List of example event types used in G
ROOT
Type Event Type Detection Technique
Performance MetricsHigh GC (Overhead) Rule-based
High CPU Usage Rule-based
Latenc y Spike Statistical Model
TPS Spike Statistical Model
Database Anomaly ML Model
Business Metric Anomaly ML Model
Status LogsWebAPI Error Statistical Model
Internal Error Statistical Model
ServiceClient Error Statistical Model
Bad Host ML Model
Developer ActivitiesCode Deployment De Facto
Conﬁguration Change De Facto
Execute URL De Facto
There are three major categories of events: performance
metrics, status logs, and developer activities:
•Performance metrics represent an anomaly of monitored
time series metrics. For example, high CPU usage in-dicates that the service is causing high CPU usage ona certain machine. In this category, most events arecontinuously and passively detected and stored.
•Status logs are caused by abnormal system status, such
as spike of HTTP error code metrics while accessingother services’ endpoints. Different types of error metricsare important and supported in G
ROOT , including third-
party APIs. For example, Bad Host indicates abnormalpatterns on some machines running the service, and canbe detected by a clustering-based ML approach.
•Developer activities are the events generated when a
certain activity of developers is triggered, such as codedeployment and conﬁg change.
In Groot, there are more than a dozen event types such
asLatency Spike as listed in the column 2 of Table II.
Each event type is characterized by three aspects: Name
indicates the name of this event type; LookbackPeriod in-
dicates the time range to look back (from the time whenthe use of G
ROOT is triggered) for collecting events of
this event type; PropertyType indicates the types of the
properties that an event of this event type should hold.PropertType is characterized by a vector of pairs, each of
which indicates the string type for a property’s name andthe primitive type for the property’s value such as string,integer, and ﬂoat. Formally, an event type is deﬁned as a tuple:ET =< Name,LookbackPeriod,PropertyType > where
PropertyType =<(string, type
1),...,(string, typen)>(n
is the number of properties that an event of this event typeholds).
Each event of a certain event type ET is characterized by
four aspects: Service indicates the service name that the event
belongs to; Type indicates ET ’sName; StartTime indicates
the time when the event happens; Properties indicates the
properties that the event holds. Formally, an event is deﬁnedas a tuple: e=< Service,Type,StartTime,Properties >
whereProperties is an instantiation of ET ’sPropertyType.
For example, in Figure 1, the generated event for
Latency Spike in DataCenter-A inService-C would be
< “Service-C
/prime/prime,“Latency Spike/prime/prime,2021/08/01-12:36:04,<
(“DataCenter/prime/prime,“DC-1/prime/prime),... >>.
423Fig. 5: Example of dynamic rule
2) Constructing Causal Link: After collecting all events on
all services in G, in this step, causal links between these events
are constructed for RCA ranking. The causal links (red arrows)
in Figure 1b are such examples. A causal link represents thatthe source event can possibly be caused by the target event.SRE knowledge is engineered into rules and used to createcausal links between the pairs of events.
A rule for constructing a causal
link is deﬁned as a tuple: Rule =<
Target -Type,Source -Events,Target -Events,Direction,
Target -Service,Condition > (Condition can be optionally
speciﬁed). Target -Type indicates the type of the rule, being
eitherStatic orDynamic (explained further later).
Source -Events indicates the type of the causal link’s source
event (Source -Events are listed in the names of the rules
shown in Figures 2, 3 and 5). Target -Events indicates the
type of the causal link’s target event. Direction indicates
the direction of the casual link between the target event andsource event. Target -Service indicates the service that the
target event should belong to. Note that Target -Service in
Static rules can be Self , which indicates that the target
event would be within the same service as the source event,orOutgoing /Incoming , which indicates that the target event
would belong to the downstream/upstream services of theservice that the source event belongs to in G.
There are two categories of special rules. The ﬁrst cat-
egory is dynamic rules (i.e., rules whose Target -Type is
set toDynamic) to support dynamic dependencies. Here
Target -Service does not indicate any of the three possible
options listed earlier but indicates the name of the targetservice that G
ROOT would need to create. For example, live
DB dependencies are not available due to different tech stacksand high volume. In Figure 5, a DB issue (DB Markdown) isshown in Service-A. Based on the listed dynamic rule, G
ROOT
creates a new “service” DB-1 inG, a new event “Issues” that
belongs to DB-1, and a causal link between the two events.
In practice, the SRE teams use dynamic rules to cover alot of third-party services and database issues since the livedependencies are not easy to maintain.
The second category of special rules is conditional rules.
Conditional rules are used when some prerequisite conditions
should be satisﬁed before a certain causal link is created. Inthese rules, Condition is speciﬁed with a boolean predicate.
As shown in Figure 3, the SRE teams believe Latency Spike
events from different services are related only when bothevents happen within the same data center. Based on thisobservation, G
ROOT would ﬁrst evaluate the predicate inCondition and build only the causal link when the predicate
is true. A conditional rule overwrites the basic rule on thesame source-target event pair.
When constructing causal links, G
ROOT ﬁrst applies the
dynamic rules so that dynamic dependencies and events are
ﬁrst created at once. Then for every event in the initial services(denoted as I), if the rule conditions are satisﬁed, one or many
causal links are created from this event to other events from thesame or upstream/downstream services. When a causal link iscreated, the step is repeated recursively for the target event (asa new origin) to create new causal links. After no new causallinks are created, the construction of the event causality graphis ﬁnished.
C. Root Cause Ranking
Finally, G
ROOT ranks and recommends the most probable
root causes from the event causality graph. Similar to how
search engines infer the importance of pages by page links,we customize the PageRank [37] algorithm to calculate theroot cause ranking; the customized algorithm is named asGrootRank. The input is the event causality graph from theprevious step. Each edge is associated with a weighted scorefor weighted propagation. The default value is set as 1, and is
set lower for alerts with high false-positive rates.
Based on the observation that dangling nodes are more
likely to be the root cause, we customize the personalizationvector as P
n=fnorPd=1 , wherePdis the personalization
score for dangling nodes, and Pnis for the remaining nodes;
andfnis a value smaller than 1 to enhance the propagation
between dangling nodes. In our work, the parameter setting isf
n=0.5,α=0.85,max iter= 100 (which are parameters for
the PageRank algorithm). Figure 6 illustrates an example. Thegrey circles are the events collected from three services andone database. The grey arrows are the dependency links andthe red ones are the causal links with the weight of 1. Both
of the PageRank and GrootRank algorithms detect event 5
(DB issue) as the root cause, which is expected and correct.However, the PageRank algorithm ranks event 4higher than
event 3. Butevent 3ofService-C is more likely to be the
second most possible root cause (besides event 5), because the
scores on dangling nodes are propagated to all others equallyin each iteration. We can see that event 3is correctly ranked
as second using the GrootRank algorithm.
The second step of GrootRank is to break the tied results
from the previous step. The tied results are due to the fact thatthe event graph can contain multiple disconnected sub-graphswith the same shape. We design two techniques to untie theranking:
1) For each joint event, the access distance (sum) is calcu-
lated from the initial anomaly service(s) to the servicewhere the event belongs to. If any “access” is notreachable, the distance is set as d
m+1 wheredmis
the maximum possible distance. The one with shorteraccess distance (sum) would be ranked higher and viceversa. Figure 7 presents an example, where Service-A
and Service-B are both initial anomaly services. Since
424Fig. 6: Example of personalization vector customization
Fig. 7: Example of using access distance to untie the ranking
results
GROOT suspects that event 2is caused by either event 3
orevent 1with the same weight. The scores of event 3
andevent 1are tied. Then, event 3has a score of 1(i.e.,
0+1 ) andevent 1has a score of 2 (i.e., 0+2 ), since
it is not reachable by Service-B). Therefore, event 3is
ranked ﬁrst and logical.
2) For the remaining joint results with the same access dis-
tances, G ROOT continues to untie by using the historical
root cause frequency of the event types under the sametrigger conditions (e.g., checkout domain alerts). Thisfrequency information is generated from the manuallylabeled dataset. A more frequently occurred root causetype is ranked higher.
D. Rule Customization Management
While G
ROOT users create or update the rules, there could
be overlaps, inconsistencies, or even conﬂicts being introducedsuch as the example in Figure 3. G
ROOT uses two graphs to
manage the rule relationships and avoid conﬂicts for users.One graph is to represent the link rules between events inthe same service (Same-Graph) while the other is to representlinks between different services (Diff-Graph). The nodes inthese two graphs are the event types deﬁned in Section IV -B.There are three statuses between each (directional) pair ofevent types: (1) no rule, (2) only basic rule, and (3) conditionalrule (since it overwrites the basic rule). In Same-Graph,
G
ROOT does not allow self-loop as it does not build links
between an event and itself.
When rule change happens, existing rules are enumerated
to build edges in Same-Graph and Diff-Graph based on
Target -Events andTarget -Service . Based on the users’
operation of (1) “remove a rule”, G ROOT removes the corre-
sponding edge on the graphs; (2) “add/update a rule”, G ROOT
checks whether there are existing edges between the givenevent types, and then warns the users for possible overwrites. Ifthere are no conﬂicts, G
ROOT just adds/updates edges between
the event types.
After all changes, G ROOT extracts the rules from the graphs
by converting each edge to a single rule. These rules areautomatically implemented, and then tested against our labeleddata set. The G
ROOT users need to review the changes with
validation reports before the changes go online.
V. E V ALUA TION
We evaluate G ROOT in two aspects: (1) effectiveness (ac-
curacy), which assesses how accurate G ROOT is in detecting
and ranking root causes, and (2) efﬁciency, which assesses how
long it takes for G ROOT to derive root causes and conduct end-
to-end analysis in action. Particularly, we intend to address thefollowing research questions:
•RQ1. What are the accuracy and efﬁciency of G ROOT
when applied on the collected dataset?
•RQ2. How does G ROOT compare with baseline ap-
proaches in terms of accuracy?
•RQ3. What are the accuracy and efﬁciency of G ROOT in
an end-to-end scenario?
A. Evaluation Setup
To evaluate G ROOT in a real-world scenario, we deploy and
apply G ROOT in eBay’s e-commerce system that serves more
than 159 million active buyers. In particular, we apply G ROOT
upon a microservice ecosystem that contains over 5,000 ser-vices on three data centers. These services are built on differenttech stacks with different programming languages, includingJava, Python, Node.js, etc. Furthermore, these services interactwith each other by using different types of service protocols,including HTTP , gRPC, and Message Queue. The distributedtracing of the ecosystem generates 147B traces on average perday.
1) Data Set: The SRE teams at eBay help collect a labeled
data set containing 952 incidents over 15 months (Jan 2020 -Apr 2021). Each incident data contains the input required by
G
ROOT (e.g., dependency snapshot and events with details)
and the root cause manually labeled by the SRE teams. Theseincidents are grouped into two categories:
•Business domain incidents. These incidents are detected
mainly due to their business impact. For example, endusers encounter failed interactions, and business or cus-tomer experience is impacted, similar to the example inFigure 1.
•Service-based incidents. These incidents are detected
mainly due to their impact on the service level, similarto the example in Figure 5.
An internal incident may get detected early, and then
likely get categorized as a service-based incident or evensolved directly by owners without records. On the otherhand, infrastructure-level issues or issues of external serviceproviders (e.g., checkout and shipping services) may not getdetected until business impact is caused.
There are 782 business domain incidents and 170 service-
based incidents in the data set. For each incident, the root cause
425T ABLE III: Accuracy of RCA by G ROOT and baselines
GROOT Nai ve Non-adapti ve
Top 3 Top 1 Top 3 Top 1 Top 3 Top 1
Service-based 92% 74% 25% 16% 84% 62%
Business domain 96% 81% 2% 1% 28% 26%
Combined 95% 78% 6% 3% 38% 33%
is manually labeled, validated, and collected by the SRE teams,
who handle the site incidents everyday. For a case with mul-tiple interacting causes, only the most actionable/inﬂuentialevent is labelled as the root cause for the case. These actualroot causes and incident contexts serve as the ground truth inour evaluation.
2)G
ROOT Setup: The G ROOT production system is de-
ployed as three microservices and federated in three datacenters with nine 8-core CPUs, 20GB RAM pods each onKubernetes.
3) Baseline Approaches: In order to compare G
ROOT with
other related approaches, we design and implement two base-line approaches for the evaluation:
•Naive Approach. This approach directly uses the con-
structed service dependency graph (Section IV -A). Theevents are assigned a score by the severeness of theassociated anomaly. Then a normalized score for eachservice is calculated summarizing all the events relatedto the service. Lastly, the PageRank algorithm is used tocalculate the root cause ranking.
•Non-adaptive Approach. This approach is not context-
aware. It replaces all special rules (i.e., conditional anddynamic ones) with their basic rule versions. Its otherparts are identical to G
ROOT .
The non-adaptive approach can be seen as a baseline forreﬂecting a group of graph-based approaches (e.g., Cau-seInfer [22] and Microscope [36]). These approaches alsospecify certain service-level metrics but lack the context-awarecapabilities of G
ROOT . Because the tools for these approaches
are not publicly available, we implement the non-adaptiveapproach to approximate these approaches.
B. Evaluation Results
1) RQ1: Table III shows the results of applying G
ROOT
on the collected data set. We measure both top-1 and top-3
accuracy. The top-1 and top-3 accuracy is calculated as thepercentage of cases where their ground-truth root cause isranked within top 1 and top 3, respectively, in G
ROOT ’s results.
GROOT achieves high accuracy on both incident categories.
For example, for business domain incidents, G ROOT achieves
96% top-3 accuracy.
The unsuccessful cases that G ROOT ranks the root cause
after top 3 are mostly caused by missing event(s). More thanone-third of these unsuccessful cases have been addressed byadding necessary events and corresponding rules over time.For example, initially, we had only an event type of generalerror spike, which mixes different categories of errors and thuscauses high false-positive rate. We then have designed differentevent types for each category of the error metrics (includingvarious internal and client API errors). In many cases thatT ABLE IV: Comparison of G
ROOT results on the dataset and
end-to-end scenario
Service-based Business Domain
Dataset End-to-End Dataset End-to-End
Top-1 Accuracy 74% 73% 81% 73%
Top-3 Accuracy 92% 91% 96% 87%
Average Runtime Cost 1.06s 3.16s 0.98s 2.98s
Maximum Runtime Cost 1.69s 4.56s 1.14s 3.61s
GROOT ranks the root cause after top 1, the labeled root cause
is just one of the multiple co-existing root causes. But forfairness, the SRE teams label only a single root cause in eachcase. According to the feedback from the SRE teams, G
ROOT
still facilitates the RCA process for these cases.
Our results show that the runtime cost of applying G ROOT
is relatively low. For a service-based incident, the averageruntime cost of G
ROOT is 1.06s while the maximum is 1.69s.
For a business domain incident, the average runtime cost is0.98s while the maximum is 1.14s.
2) RQ2: We additionally apply the baseline approaches
on the data set. Table III also shows the evaluation results.The results show that the accuracy of G
ROOT is substantially
higher than that of the baseline approaches. In terms of thetop-1 accuracy, G
ROOT achieves 78% compared with 3% and
33% of the naive and non-adaptive approaches, respectively. Interms of the top-3 accuracy, G
ROOT achieves 95% compared
with 6% and 38% of the naive and non-adaptive approaches,respectively.
The naive approach performs worst in all settings, because
it blindly propagates the score at service levels. The accuracyof the non-adaptive approach is much worse for businessdomain incidents. The reason is that for a business domainincident, it often takes a longer propagation path since theincident is triggered by a group of services, and new dynamicdependencies may be introduced during the event collection,causing more inaccuracy for the non-adaptive approach. Therecan be many non-critical or irrelevant error events in an actualproduction scenario, aka “soft” errors. We suspect that thesenon-critical or irrelevant events may be ranked higher by thenon-adaptive approach since they are similar to injected faultsand hard to be distinguished from the actual ones. G
ROOT
uses dynamic and conditional rules to discover the actualcausal links, building fewer links related to such non-criticalor irrelevant events for leading to higher accuracy.
3) RQ3: To evaluate G
ROOT under an end-to-end scenario,
we apply G ROOT upon actual incidents in action. Table IV
shows the results. The accuracy has a decrease of up to9 percentage points in the end-to-end scenario, with somefailures caused by production issues such as missing dataand service/storage failures. In addition, the runtime cost isincreased by up to nearly 3 seconds due to the time spent onfetching data from different data sources, e.g., querying theevents for a certain time period.
VI. E
XPERIENCE
GROOT currently supports daily SRE work. Figure 8 shows
al i v eG ROOT ’s “bird’s eye view” UI on an actual simple
checkout incident. Service Chas the root cause (ErrorSpike )
426Fig. 8: G ROOT UI in production
and belongs to an external provider. Although the domain
serviceAalso carries an error spike and gets impacted, G ROOT
correctly ignores the irrelevant deployment event, which hasno critical impact. The events on Care virtually created based
on the dynamic rule. Note that all causal links (yellow) in theUI indicate “is cause of”, being the opposite of “is caused by”as described in Section IV -B to provide more intuitive UI forusers to navigate through. G
ROOT visualizes the dependency
and event causality graph with extra information such as anerror message. The SRE teams can quickly comprehend theincident context and derived root cause to investigate G
ROOT
further. A mouseover can trigger “event enrichment” based onthe event type to present details such as raw metrics and otheradditional information.
We next share two major kinds of experience:
•Feedback from G ROOT users and developers, reﬂect-
ing the general experience of two groups: (1) domain SREteams who use G
ROOT to ﬁnd the root cause, and (2) a
centered SRE team who maintains G ROOT to facilitate
new requirements.
•Lessons learned, representing the lessons learned fromdeploying and adopting G
ROOT in production for the real-
world RCA process.
A. Feedback from GROOT Users and Developers
We invite the SRE members who use G ROOT for RCA in
their daily work to the user survey. We call them users in thissection. We also invite different SRE members responsible formaintaining G
ROOT to the developer survey. We call them
developers in this section. In total, there are 14 users and 6developers
1who respond to the surveys.
For the user survey, we ask 14 users the following 5
questions (Questions 4-5 have the same choices as Question1):
•Question 1. When G ROOT correctly locates the root
cause, how does it help with your triaging experience?Answer choices: Helpful(4), Somewhat Helpful(3), NotHelpful(2), Misleading(1).
•Question 2. When G ROOT correctly locates the root
cause, how does it save/extend your or the team’s triaging
1The G ROOT researchers and developers who are authors of this paper are
excluded.01234Question 5Question 4Question 3Question 2Question 1
(a) From 14 G ROOT users01234Question 5Question 4Question 3Question 2Question 1
(b) From 6 G ROOT developers
Fig. 9: Survey results
time? (Detection and remediation time not included)
Answer choices: Lots Of Time Saved(4), Some TimeSaved(3), No Time Saved(2), Waste Time Instead(1).
•Question 3. Based on your estimation, how much triage
time G ROOT would save on average when it correctly
locates the root cause? (Detection and remediation timenot included) Answer choices: More than 50%(4), 25-50%(3), 10-25%(2), 0-10%(1), N/A(0).
•Question 4. When G ROOT correctly locates the root
cause, do you ﬁnd that the result “graph” provided by
GROOT helps you understand how and why the incident
happens?
•Question 5. When G ROOT does not correctly locate the
root cause, does the result “graph” make it easier for yourinvestigation of the root cause?
Figure 9a shows the results of the user survey. We can see
that most users ﬁnd G
ROOT very useful to locate the root
cause. The average score for Question 1 is 3.79, and 11 outof 14 participants ﬁnd G
ROOT very helpful. As for Question
3, G ROOT saves the triage time by 25-50%. Even in cases
that G ROOT cannot correctly locate the root cause, it is still
helpful to provide information for further investigation withan average score of 3.43 in Question 5.
For the developer survey, we ask the 6 developers the
following 5 questions (Questions 2-5 have the same choicesas Question 1):
•Question 1. Overall, how convenient is it to change and
customize events/rules/domains while using G ROOT ? An-
swer choices: Convenient(4), Somewhat Convenient(3),Not Convenient(2), Difﬁcult(1).
•Question 2. How convenient is it to change/customize
event models while using G ROOT ?
•Question 3. How convenient is it to add new domains
while using G ROOT ?
•Question 4. How convenient is it to change/customize
causality rules while using G ROOT ?
•Question 5. How convenient is it to change/customize
GROOT compared to other SRE tools?
Figure 9b shows the results of the developer survey. Overall,
most developers ﬁnd it convenient to make changes on andcustomize events/rules/domains in G
ROOT .
B. Lessons learned
In this section, we share the lessons learned in terms of tech-
nology transfer and adoption on using G ROOT in production
environments.
427Embedded in Practice. To build a successful RCA tool in
practice, it is important to embed the R&D efforts in the live
environment with SRE experts and users. We have a 30-minuteroutine meeting daily with an SRE team to manually test andreview every site incident. In addition, we actively reach outto the end users for feedback. For example, the users foundour initial UI hard to understand. Based on their suggestions,we have introduced alert enrichment with the detailed contextof most events, raw metrics, and links to other tools for thenext steps. We also make the UI interactive and build userguides, training videos, and sections. As a result, G
ROOT has
become increasingly practical and well adopted in practice. Webelieve that R&D work on observability should be incubatedand grown within daily SRE environments. It is also vitalto bring developers with rich RCA experience into the R&Dteam.
V ertical Enhancements. High-conﬁdence and automated ver-
tical enhancements can empower great experiences. G
ROOT is
enhanced and specialized in critical scenarios such as groupedrelated alerts across services or critical business domain issues,and large-scale scenarios such as infrastructure changes ordatabase issues. Furthermore, the end-to-end automation isalso built for integration and efﬁciency with anomaly detec-tion, RCA, and notiﬁcation. For notiﬁcation, domain businessanomalies and diagnostic results are sent through communi-cation apps (e.g., slack and email) for better reachability andexperience. Within 18 months of R&D, G
ROOT now supports
18 business domains and sub-domains of the company. Onaverage, G
ROOT UI supports more than 50 active internal
users, and the service sends thousands of results every month.Most of these usages are around the vertical enhancements.
Data and Tool Reliability. Reliability is critical to G
ROOT
itself and requires a lot of attention and effort. For example, ifa critical event is missing, G
ROOT may infer a totally different
root cause, which would mislead users. We estimate the alertaccuracy to be greater than 0.6 in order to be useful. Recall iseven more important since G
ROOT can effectively eliminate
false positive alerts based on the casual ranking. Since thereare hundreds of different metrics supported in G
ROOT ,w e
spend time to ensure a robust back end by adding partialand dynamic retry logic and high-efﬁciency cache. G
ROOT ’s
unsuccessful cases can be caused by imperfect data, ﬂawedalgorithms, or simply code defects. To better trace the reasonbehind each unsuccessful case, we add a tracing component.Every G
ROOT request can be traced back to atomic actions
such as retrieving data, data cleaning, and anomaly detectionvia algorithms.
Trade-off among Models. The accuracy and scalability trade-
off among anomaly detection models should be carefullyconsidered and tested. In general, some algorithms such asdeep-learning-based or ensemble models are more adaptiveand accurate than typical ones such as traditional ML orstatistical models. However, the former requires more com-putation resources, operational efforts, and additional systemcomplexities such as training or model ﬁne-tuning. Due to theactual complexities and fast-evolving nature of our context,it is not possible to scale each model (e.g., deep-learning-based models), nor have it deeply customized for every metricat every level. Therefore, while selecting models, we mustmake careful trade-off in aspects such as accuracy, scalability,efﬁciency, effort, and robustness. In general, we ﬁrst setdifferent “acceptance” levels by analyzing each event’s impactand frequency, and then test different models in staging andpick the one that is good enough. For example, a few alertssuch as “high thread usage” are deﬁned by thresholds and workjust ﬁne even without a model. Some alerts such as “serviceclient error” are more stochastic and require coverage on everymetric of every service, and thus we select fast and robuststatistical models and actively conduct detection on the ﬂy.
Phased Incorporation of ML. In the current industrial
settings, ML-powered RCA products still require effectiveknowledge engineering. Due to the higher complexity andlower “signal to noise ratio” of real production incidents, manyexisting approaches cannot be applied in practice. We be-lieve that the knowledge engineering capabilities can facilitateadoption of technologies such as AIOps. Therefore, G
ROOT
is designed to be highly customizable and easy to infuse SREknowledge and to achieve high effectiveness and efﬁciency.Moreover, a multi-scenario RCA tool requires various andinterpretable events from different detection strategies. Auto-ML-based anomaly detection or unsupervised RCA for largeservice ecosystems is not yet ready in such context. As forthe path of supervised learning, the training data is tricky tolabel and vulnerable to potential cognitive bias. Lastly, theend users often require complete understanding to fully adoptnew solutions, because there is no guarantee of correctness.Many recent ML algorithms (e.g., ensemble and deep learning)lack interpretability. Via the knowledge engineering and graphcapabilities, G
ROOT is able to explain diversity and causality
between ML-model-driven and other types of events. Movingforward, we are building a white-box deep learning approachwith causal graph algorithms where the causal link weightsare parameters and derivable.
VII. C
ONCLUSION
In this paper, we have presented our work around root
cause analysis (RCA) in industrial settings. To tackle threemajor RCA challenges (complexities of operation, systemscale, and monitoring), we have proposed a novel event-graph-based approach named G
ROOT that constructs a real-time
causality graph for allowing adaptive customization. G ROOT
can handle diversiﬁed anomalies and activities from the systemunder analysis and is extensible to different approaches ofanomaly detection or RCA. We have integrated G
ROOT into
eBay’s large-scale distributed system containing more than5,000 microservices. Our evaluation of G
ROOT on a data
set consisting of 952 real production incidents shows that
GROOT achieves high accuracy and efﬁciency across different
scenarios and also largely outperforms baseline graph-basedapproaches. We also share the lessons learned from deployingand adopting G
ROOT in production environments.
428REFERENCES
[1] A. Balalaie, A. Heydarnoori, and P . Jamshidi, “Microservices architec-
ture enables DevOps: Migration to a cloud-native architecture,” IEEE
Software, vol. 33, no. 3, pp. 42–52, 2016.
[2] M. Solé, V . Muntés-Mulero, A. I. Rana, and G. Estrada, “Survey
on models and techniques for root-cause analysis,” arXiv preprint
arXiv:1701.08546, 2017.
[3] N. Zhao, P . Jin, L. Wang, X. Y ang, R. Liu, W . Zhang, K. Sui, and
D. Pei, “Automatically and adaptively identifying severe alerts for online
service systems,” in Proceedings of 2020 IEEE Conference on Computer
Communications. IEEE, 2020, pp. 2420–2429.
[4] J. Xu, Y . Wang, P . Chen, and P . Wang, “Lightweight and adaptive service
api performance monitoring in highly dynamic cloud environment,”inProceedings of 2017 IEEE International Conference on Services
Computing. IEEE, 2017, pp. 35–43.
[5] L. Tang, T. Li, F. Pinel, L. Shwartz, and G. Grabarnik, “Optimizing sys-
tem monitoring conﬁgurations for non-actionable alerts,” in Proceedings
of 2012 IEEE Network Operations and Management Symposium. IEEE,2012, pp. 34–42.
[6] M. K. Aguilera, J. C. Mogul, J. L. Wiener, P . Reynolds, and A. Muthi-
tacharoen, “Performance debugging for distributed systems of blackboxes,” ACM SIGOPS Operating Systems Review, vol. 37, no. 5, pp.
74–89, 2003.
[7] H. Zawawy, K. Kontogiannis, and J. Mylopoulos, “Log ﬁltering and
interpretation for root cause analysis,” in Proceedings of 2010 IEEE
International Conference on Software Maintenance. IEEE, 2010, pp.1–5.
[8] V . Nair, A. Raul, S. Khanduja, V . Bahirwani, Q. Shao, S. Sellamanickam,
S. Keerthi, S. Herbert, and S. Dhulipalla, “Learning a hierarchicalmonitoring system for detecting and diagnosing service issues,” inProceedings of the 21th ACM SIGKDD International Conference onKnowledge Discovery and Data Mining. ACM, 2015, pp. 2029–2038.
[9] S. Lu, B. Rao, X. Wei, B. Tak, L. Wang, and L. Wang, “Log-
based abnormal task detection and root cause analysis for Spark,” inProceedings of 2017 IEEE International Conference on Web Services
. IEEE, 2017, pp. 389–396.
[10] Y . Gan, Y . Zhang, K. Hu, D. Cheng, Y . He, M. Pancholi, and C. Delim-
itrou, “Seer: Leveraging big data to navigate the complexity of perfor-mance debugging in cloud microservices,” in Proceedings of the 24th
International Conference on Architectural Support for ProgrammingLanguages and Operating Systems. ACM, 2019, pp. 19–33.
[11] J. Mace, R. Roelke, and R. Fonseca, “Pivot tracing: Dynamic causal
monitoring for distributed systems,” in Proceedings of the 25th ACM
Symposium on Operating Systems Principles. ACM, 2015, pp. 378–393.
[12] H. Xu, W . Chen, N. Zhao, Z. Li, J. Bu, Z. Li, Y . Liu, Y . Zhao, D. Pei,
Y . Feng et al., “Unsupervised anomaly detection via variational auto-
encoder for seasonal KPIs in web applications,” in Proceedings of the
2018 World Wide Web Conference. ACM, 2018, pp. 187–196.
[13] M. Ma, W . Lin, D. Pan, and P . Wang, “Ms-rank: Multi-metric and
self-adaptive root cause diagnosis for microservice applications,” inProceedings of 2019 IEEE International Conference on Web Services
. IEEE, 2019, pp. 60–67.
[14] Y . Meng, S. Zhang, Y . Sun, R. Zhang, Z. Hu, Y . Zhang, C. Jia,
Z. Wang, and D. Pei, “Localizing failure root causes in a microservicethrough causality inference,” in Proceedings of 2020 IEEE/ACM 28th
International Symposium on Quality of Service. IEEE, 2020, pp. 1–10.
[15] L. Wu, J. Tordsson, E. Elmroth, and O. Kao, “Microrca: Root cause
localization of performance issues in microservices,” in Proceedings
of 2020 IEEE/IFIP Network Operations and Management Symposium
. IEEE, 2020, pp. 1–9.
[16] M. Kim, R. Sumbaly, and S. Shah, “Root cause detection in a service-
oriented architecture,” ACM SIGMETRICS Performance Evaluation Re-
view, vol. 41, no. 1, pp. 93–104, 2013.
[17] H. Wang, P . Nguyen, J. Li, S. Kopru, G. Zhang, S. Katariya, and
S. Ben-Romdhane, “Grano: Interactive graph-based root cause analysisfor cloud-native distributed data platform,” Proceedings of the V ery
Large Data Base Endowment, vol. 12, no. 12, pp. 1942–1945, 2019.
[18] H. Baek, A. Srivastava, and J. V an der Merwe, “Cloudsight: A tenant-
oriented transparency framework for cross-layer cloud troubleshooting,”inProceedings of 2017 17th IEEE/ACM International Symposium on
Cluster , Cloud and Grid Computing. IEEE, 2017, pp. 268–273.[19] G. Da Cunha Rodrigues, R. N. Calheiros, V . T. Guimaraes, G. L. d.
Santos, M. B. De Carvalho, L. Z. Granville, L. M. R. Tarouco, andR. Buyya, “Monitoring of cloud computing environments: Concepts,solutions, trends, and future directions,” in Proceedings of the 31st
Annual ACM Symposium on Applied Computing. ACM, 2016, pp.378–383.
[20] How SRE teams are organized, and how to get started, Ac-cessed: 2020-12-10, https://cloud.google.com/blog/products/devops-sre/how-sre-teams-are-organized-and-how-to-get-started/.
[21] H. Nguyen, Z. Shen, Y . Tan, and X. Gu, “Fchain: Toward black-box
online fault localization for cloud systems,” in Proceedings of 2013
IEEE 33rd International Conference on Distributed Computing Systems.IEEE, 2013, pp. 21–30.
[22] P . Chen, Y . Qi, P . Zheng, and D. Hou, “Causeinfer: Automatic and
distributed performance diagnosis with hierarchical causality graph inlarge distributed systems,” in Proceedings of 2014 IEEE Conference on
Computer Communications. IEEE, 2014, pp. 1887–1895.
[23] M. Ma, Z. Yin, S. Zhang, S. Wang, C. Zheng, X. Jiang, H. Hu, C. Luo,
Y . Li, N. Qiu et al., “Diagnosing root causes of intermittent slow
queries in cloud databases,” Proceedings of the V ery Large Data Base
Endowment, vol. 13, no. 8, pp. 1176–1189, 2020.
[24] J. Schoenﬁsch, C. Meilicke, J. von Stülpnagel, J. Ortmann, and H. Stuck-
enschmidt, “Root cause analysis in it infrastructures using ontologies andabduction in markov logic networks,” Information Systems, vol. 74, pp.
103–116, 2018.
[25] Á. Brandón, M. Solé, A. Huélamo, D. Solans, M. S. Pérez, and
V . Muntés-Mulero, “Graph-based root cause analysis for service-orientedand microservice architectures,” Journal of Systems and Software, vol.
159, p. 110432, 2020.
[26] D. Y . Y oon, N. Niu, and B. Mozafari, “Dbsherlock: A performance
diagnostic tool for transactional databases,” in Proceedings of the 2016
International Conference on Management of Data. ACM, 2016, pp.1599–1614.
[27] V . Jeyakumar, O. Madani, A. Parandeh, A. Kulshreshtha, W . Zeng, and
N. Y adav, “Explainit!–a declarative root-cause analysis engine for timeseries data,” in Proceedings of the 2019 International Conference on
Management of Data. ACM, 2019, pp. 333–348.
[28] H. Jayathilaka, C. Krintz, and R. Wolski, “Performance monitoring and
root cause analysis for cloud-hosted web applications,” in Proceedings
of the 26th International Conference on World Wide Web. ACM, 2017,pp. 469–478.
[29] M. A. Marvasti, A. V . Poghosyan, A. N. Harutyunyan, and N. M.
Grigoryan, “An anomaly event correlation engine: Identifying rootcauses, bottlenecks, and black swans in IT environments,” VMware
Technical Journal, vol. 2, no. 1, pp. 35–45, 2013.
[30] J. Weng, J. H. Wang, J. Y ang, and Y . Y ang, “Root cause analysis of
anomalies of multitier services in public clouds,” IEEE/ACM Transac-
tions on Networking, vol. 26, no. 4, pp. 1646–1659, 2018.
[31] J. Qiu, Q. Du, K. Yin, S.-L. Zhang, and C. Qian, “A causality mining and
knowledge graph based method of root cause diagnosis for performanceanomaly in cloud applications,” Applied Sciences, vol. 10, no. 6, p. 2166,
2020.
[32] Surus, Accessed: 2020-08-15, https://github.com/Netﬂix/Surus.
[33] M. Ma, S. Zhang, D. Pei, X. Huang, and H. Dai, “Robust and rapid
adaption for concept drift in software system anomaly detection,” inProceedings of 2018 IEEE 29th International Symposium on SoftwareReliability Engineering. IEEE, 2018, pp. 13–24.
[34] R. Bhagwan, R. Kumar, R. Ramjee, G. V arghese, S. Mohapatra,
H. Manoharan, and P . Shah, “Adtributor: Revenue debugging in advertis-ing systems,” in Proceedings of 11th USENIX Symposium on Networked
Systems Design and Implementation. USENIX, 2014, pp. 43–55.
[35] Y . Sun, Y . Zhao, Y . Su, D. Liu, X. Nie, Y . Meng, S. Cheng, D. Pei,
S. Zhang, X. Qu et al., “Hotspot: Anomaly localization for additive KPIs
with multi-dimensional attributes,” IEEE Access, vol. 6, pp. 10 909–
10 923, 2018.
[36] J. Lin, P . Chen, and Z. Zheng, “Microscope: Pinpoint performance issues
with causal graphs in micro-service environments,” in Proceedings of
International Conference on Service-Oriented Computing. Springer,2018, pp. 3–20.
[37] C. Manning, P . Raghavan, and H. Schütze, “Introduction to information
retrieval,” Natural Language Engineering, vol. 16, no. 1, pp. 100–103,
2010.
429