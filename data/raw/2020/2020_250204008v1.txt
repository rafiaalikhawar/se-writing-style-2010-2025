Automating a Complete Software Test Process
Using LLMs: An Automotive Case Study
Shuai Wang1, Yinan Yu1, Robert Feldt1, Dhasarathy Parthasarathy2
1Chalmers University of Technology2V olvo Group
Gothenburg, Sweden
shuaiwa@chalmers.se, yinan@chalmers.se, robert.feldt@chalmers.se, dhasarathy.parthasarathy@volvo.com
Abstract ‚ÄîVehicle API testing verifies whether the interactions
between a vehicle‚Äôs internal systems and external applications
meet expectations, ensuring that users can access and con-
trol various vehicle functions and data. However, this task is
inherently complex, requiring the alignment and coordination
of API systems, communication protocols, and even vehicle
simulation systems to develop valid test cases. In practical
industrial scenarios, inconsistencies, ambiguities, and interde-
pendencies across various documents and system specifications
pose significant challenges. This paper presents a system designed
for the automated testing of in-vehicle APIs. By clearly defining
and segmenting the testing process, we enable Large Language
Models (LLMs) to focus on specific tasks, ensuring a stable and
controlled testing workflow. Experiments conducted on over 100
APIs demonstrate that our system effectively automates vehicle
API testing. The results also confirm that LLMs can efficiently
handle mundane tasks requiring human judgment, making them
suitable for complete automation in similar industrial contexts.
Index Terms ‚Äîsoftware testing, vehicle API testing, test au-
tomation, large language model
I. I NTRODUCTION
Large Language Models (LLMs) are revolutionizing soft-
ware engineering. In the past few years, we have witnessed
the application of LLMs for assisting or automating numer-
ous software engineering tasks like requirements engineering,
software design, coding, and testing [1] [2]. Software testing,
in particular, is one area where LLMs have been applied
with vigor. Facing ever-increasing needs for automation due
to the volume and intensity of work involved, testing is
rapidly benefiting from the generative capabilities of LLMs.
As systematically surveyed in [3], LLMs have been applied in
many testing tasks including system input generation, test case
generation, test oracle generation, debugging, and program
repair.
While a considerable amount of recent literature has focused
on applying LLMs in narrowly scoped tasks [4] ‚Äì such as
specific unit tests [5][6], isolated integration tests [7], or
individual verification scenarios [8][9] ‚Äì few have reported
on their application to automate a complete test process.
Practical testing processes are a diverse mix of steps that are
mechanical, creative, and anything in between [10][11]. They
also involve several (teams of) engineers and tools, whose
harmonious cooperation is essential to ensure the quality
and cadence of testing. The challenge is only greater when
testing automotive embedded systems, where software coexists
with mechatronics and other physical systems. Under such
Fig. 1. We present the case of automatically testing SPAPI, an in-vehicle web
server. Previously, the multistep process of testing SPAPI was largely manual.
Using LLMs to automate each manual step, we achieve complete automation.
heterogeneous conditions, it is not immediately apparent how
one can effectively integrate LLMs into a testing process and
gain efficiencies. In response to these challenges, we present
a case study that (1) focuses upon a real-world test process
in the automotive industry that is largely performed manually,
and (2) automates it using a recipe that seamlessly combines
selective use of LLMs with conventional automation.
The focus of this case study ‚Äì our system under test ‚Äì is
SPAPI, a web server that is deployed in trucks made by a
leading vehicle manufacturer. SPAPI exposes a set of REST
APIs which can be used by clients to read or write selected
vehicle states. For example, SPAPI exposes /speed that can
be used to read the vehicle speed, and /climate that can be
used to change the cabin climate. Essentially, SPAPI serves as
a gateway between web clients (like apps on a tablet) on one
side, and in-vehicle control and monitoring applications on
the other side. More importantly for the purposes of this pa-
per, since SPAPI enables crucial customer-facing applications,
considerable effort is spent in ensuring its quality.
Testing SPAPI requires a dedicated team of 2-3 full-time
engineers. As shown in Figure 1 (left), when new APIs
are released, the team first reviews the API specifications.
They then (2-3) consult multiple documentation sources to
understand the associated vehicle states, (4-5) organize this
information to determine appropriate mocks and test inputs,arXiv:2502.04008v1  [cs.SE]  6 Feb 2025and (6-7) write and integrate test cases into a nightly regression
suite. Finally, they assess results (8), particularly test failures,
to identify valid problems. Notably, as highlighted in Figure
1, most of this process is still performed manually.
These observations prompt the question ‚Äì why is such in-
tense manual effort needed to test an arguably simple gateway
server? The main reasons are structural. First, as a gateway,
SPAPI‚Äôs engineering spans multiple teams with overlapping re-
sponsibilities. The three core components‚Äîthe server, vehicle
state system, and mocking system‚Äîare developed by separate
teams, while testing falls to a fourth team that must interpret
disparate documentation from each. Second, SPAPI bridges
web applications and traditional in-vehicle systems, which
differ fundamentally in documentation style. SPAPI APIs are
specified in Swagger, making them machine-readable, whereas
vehicle states are documented in a mix of natural and formal
languages, often requiring human interpretation. Third, SPAPI
testers rely heavily on implicit knowledge built over years to
manage inconsistencies across systems and teams, leading to
highly specialized expertise that intensifies manual effort and
complicates team turnover.
In SPAPI testing, the potential for full automation presents
two significant benefits: (1) SPAPI testing can be fully au-
tomated, increasing the cadence with which APIs can be
delivered to customers, and (2) SPAPI testers can be unbur-
dened of their tedious job, allowing their creative talents to
be applied elsewhere. Our observations on SPAPI highlight
that full automation is not only beneficial but essential under
certain test process conditions.
Specifically, full automation is crucial in scenarios where
testers function as a ‚Äúglue‚Äù between tools, systems, and
stakeholders in tasks that rely on judgment rather than cre-
ativity. Here, automation enhances engineering quality while
improving the testers‚Äô experience. Additionally, in testing
workflows with extensive manual steps, partial automation
offers limited gains, reinforcing the need for a comprehensive,
all-or-nothing automation approach. Furthermore, when testers
navigate legacy processes weighed down by technical debt,
partial debt mitigation falls short; complete automation is
necessary to address and eliminate debt effectively, benefiting
both testers and the organization as a whole.
Recognizing these advantages and the rapid advancements
in LLMs for automating manual processes, we explore the
central question: can LLMs serve as the key to fully automat-
ing a largely manual test process? To address this, we make
the following contributions:
1) We argue that a test process with clearly decomposed
tasks, many of which are executed manually, is a prime
candidate for complete automation based on LLMs.
2) When these criteria are satisfied, we propose a recipe for
full automation that involves (a) retaining the test process
structure, (b) leveraging LLMs as a general-purpose tool
to automate each manual step, and (c) combining LLMs
with conventional automation when required.
3) We present in-vehicle web server testing as a case study,
illustrating how a real-world testing process aligns with
Json /XMLSelected From
Requested DataClient Server Database
API
(1)
Web API in 
classic three -tier 
architecture
Json /XMLGateway ECUIn-vehicle embedded system
API
(2)
SPAPI server in a 
real vehicle
Json /XMLAPI
(3)
SPAPI server in a 
test rigECU X
ECU YCAN linkApp A
App B
Gateway ECUVirtual Vehicle (VV) system
CAN link
Test API‚Äôs responses
Test API‚Äôs responses
Test API‚Äôs responsesTest virtual 
vehicle‚Äôs statusFig. 2. A comparative illustration of the SPAPI architecture ‚Äì (1) a web server
in the classic three-tier architecture, (2) SPAPI in a real in-vehicle embedded
system, and (3) SPAPI in a test rig with vehicle state mocked by a Virtual
Vehicle (VV) system. Compared to traditional API testing, vehicle API testing
requires not only verifying the API‚Äôs responses but also checking the vehicle‚Äôs
status.
our criteria and demonstrating its full automation using
our proposed recipe.
4) As the test process structure remains largely intact, we
highlight how evaluating the quality of AI-driven automa-
tion can be simplified by independently assessing each
step where an LLM is applied.
As the following sections will demonstrate, using a real
industrial example of in-vehicle embedded software testing,
we show that a manual process like SPAPI testing can be fully
automated (see Figure 1) to deliver practical improvements.
II. B ACKGROUND
Since SPAPI is a web server that exposes REST APIs, our
case study falls within the ambit of API testing [12]. Aspects
of the SPAPI test process are therefore recognizable within
the larger universe of API testing, but there are also several
case-specific adjustments, which we now highlight.
A. System architecture
As jointly illustrated in Figures 2 and 3, SPAPI follows
the typical 3-tier architecture of decoupling presentation [13],
business logic, and data, each of which we discuss below.
Presentation ‚Äì Like any web server, SPAPI presents REST-
ful endpoints with GET and PUT methods and JSON pay-
loads/responses. Each API transacts an object of the form S=
{(ki, vi)}N
i=1, with Nattribute-value pairs. Each pair (ki, vi)
in the object corresponds to some vehicle state (k‚àó
i, v‚àó
i)that is
managed by a control or monitoring application deployed in an
Electronic Control Unit (ECU) in the vehicle. Figure 3 shows
an example where /speed endpoint provides a GET method
that returns the instantaneous speed of the vehicle which, in
turn, is calculated by a SpeedEstimation application in a
vehicle master control ECU. The same figure also illustrates
the/climate endpoint with a PUT method that sets different
cabin climate states by communicating with an ACControl
application in a climate control ECU. Thus, the essence of
SPAPI is presenting APIs for reading or writing an object
S={(ki, vi)}N
i=1. This corresponds to interacting with vehicleFig. 3. Three tiers of SPAPI operation (1) presentation - SPAPI objects, (2)
data access - CAN signals, and (3) data - vehicle states.
statesS‚àó={(k‚àó
i, v‚àó
i)}N
i=1managed by applications distributed
across the in-vehicle embedded system.
Data and data access ‚Äì The typical web server may hold its
data in a database, but, clearly, ‚Äòdata‚Äô for SPAPI is vehicle
state information managed by different in-vehicle control ap-
plications. As shown in Figure 2, these in-vehicle applications
are distributed across several ECUs, interconnected using
Controller Area Network (CAN) links. While the typical web
server may access data by executing database queries, SPAPI
accesses data by exchanging CAN signals S‚Ä≤={(k‚Ä≤
i, v‚Ä≤
i)}N
i=1
with in-vehicle applications. A CAN signal is a pre-defined
typed quantity sent through a CAN link between designated
sender and receiver applications. In the simplest case, each
vehicle state (k‚àó
i, v‚àó
i)maps to one CAN signal and value
pair (k‚Ä≤
i, v‚Ä≤
i), which SPAPI sends or receives to access the
state. We also clarify that this case study focuses upon testing
SPAPI in a rig, and not in the real vehicle. In the test rig
(see Figure 2), vehicle state is emulated by a Virtual Vehicle
(VV) system, which maintains the superset Nof all vehicle
statesS‚àó={(k‚àó
i, v‚àó
i)}N
i=1in a single table, emulating the
state managed by distributed control applications. To maintain
consistency of interaction, VV allows state (k‚àó
i, v‚àó
i)to be
accessed using the same CAN signal (k‚Ä≤
i, v‚Ä≤
i)that SPAPI uses
in the real vehicle. In addition to easing testing using virtual
means, unlike many other API testing cases, VV offers the
advantage of being able to freely mock vehicle state for testing
purposes. Due to the continuous evolution of CAN signals and
the VV platform, it is essential to monitor the vehicle‚Äôs state
to accurately capture relevant state changes.
API logic ‚Äì Since SPAPI is a gateway, the logic for each
endpoint is relatively lean. When a client invokes an endpoint,
SPAPI does the mapping (ki, vi)‚Üí(k‚Ä≤
i, v‚Ä≤
i)of each attribute-
value pair in the API object to the corresponding CAN signal-
value pair. Then, by sending or receiving the CAN signal and
value (k‚Ä≤
i, v‚Ä≤
i), SPAPI reads or writes the corresponding vehicle
state (k‚àó
i, v‚àó
i). Based upon the result of state manipulation,
SPAPI sends an appropriate response to the client.
B. Current manual API testing
The current manual workflow for API testing, as shown
in Figure 1, involves steps such as: understand the APIspecification, look up related information, write test cases,
run and access the test cases. Specifically, the tester should
first identify the specific object set Sby understanding the
documentation. Following this, the tester will retrieve the
corresponding CAN signal documentation S‚Ä≤and the VV
system documentation S‚àó. It is crucial to ensure that each
attribute in Scan be mapped to both S‚Ä≤andS‚àó. This means
verifying that every attribute can be converted into a CAN
signal and can be simulated in the VV system, and testers can
write test cases based on the matched results. Typically, two
key aspects need to be checked during API testing. The first
aspect is to verify whether the virtual vehicle‚Äôs state aligns
with expectations after setting certain attributes to specific
values via API:
S‚àó‚ÜêPUT (S)
S‚àó?=Sexpected(1)
The second aspect is to check whether the API returns the
expected values under a specific virtual vehicle state:
S=GET ()
S?=Sexpected(2)
In the following content, we will introduce the details of
each step.
1) Understand API specification: Test engineers need to
understand the API documentation to extract the basic objects
about the API. The documentation, like Swagger file, always
details each API‚Äôs essential information, such as all available
endpoints, expected request formats, and possible response
formats for each endpoint. Additionally, Swagger defines the
data structures used in the API, including objects, properties,
and their types. An example of a Swagger file snippet de-
scribing the Climate object is shown in Figure 5(a). In this file,
testers should parse the object‚Äôs acMode and its corresponding
details in the pairs. In summary, a thorough understanding the
API documentation manually is essential for constructing a
comprehensive object set S={(ki, vi)}N
i=1from the original
system documentations.
2) Retrieve related information: After obtaining the at-
tributes and values corresponding to the object, denoted as S,
it is necessary to search for related documentation, including
the information about CAN signals and the details about the
virtual vehicle. The search process is illustrated in Figure 4.
First, the tester needs to locate the relevant CAN signal
documentation from CAN signal table. Then, by matching the
corresponding key and value, the original state Sis converted
into the CAN signal S‚Ä≤. Afterward, the relevant virtual vehicle
documentation is consulted, and the corresponding key and
value are mapped to obtain the specific operation S‚àóthat needs
to be performed on the VV .
Look up information: The three main components of
SPAPI testing are the server, vehicle state system, and VV
system. Correspondingly, system information, CAN signal
specifications and mocking documentation are needed to be
retrieved.API 
InformationCAN TablesVirtual 
Vehicle
S 
{ùëòùëñ,ùë£ùëñ}look up look up
Sample object‚Äôs 
statesTransform to CAN 
signalGET/PUT the 
states in VV S‚Äô 
{ùëòùëñ‚Ä≤,ùë£ùëñ‚Ä≤}S* 
{ùëòùëñ‚àó,ùë£ùëñ‚àó}Fig. 4. The process of setting and getting vehicle status according to the API
information.
When testing an attribute, the corresponding values should
be looked up in both the CAN signal and VV tables. For
example, our goal is to set the vehicle‚Äôs status to ECONOMY .
First, we locate the relevant attribute acMode in the system
documentation S. Then, we look up acMode in the CAN
signal table S‚Ä≤and find its corresponding value for ECONOMY ,
which might be 1. We then transmit this information to the VV
system via the CAN signal. Subsequently, in the VV system,
we read the corresponding CAN signal and look up the VV
tableS‚àóto find the value of the acMode under the ECONOMY
state, which might be 2. Finally, we set the value of acMode
to 2 in the VV system. Finally, the acMode in the VV system
is set to 2 to achieve the desired vehicle state of ECONOMY .
Information organizing: In automotive systems, to trans-
mit signals via CAN and utilize VV system correctly, we need
to ensure that each attribute and its corresponding value in
the system document S={(ki, vi)}N
i=1can be looked up in
the CAN signal specifications for getting S‚Ä≤={(k‚Ä≤
i, v‚Ä≤
i)}N‚Ä≤
i=1.
Simultaneously, each attribute and its value in S‚Ä≤should
be looked up in the mocking documentations to get S‚àó=
{(k‚àó
i, v‚àó
i)}N‚àó
i=1. Formally, our goal is to find a mapping such
that:
‚àÄ(ki, vi),‚àÉ(k‚Ä≤
j, v‚Ä≤
j)‚ààS‚Ä≤where (ki, vi)‚Üí(k‚Ä≤
j, v‚Ä≤
j)
‚àÄ(k‚Ä≤
i, v‚Ä≤
i),‚àÉ(k‚àó
k, v‚àó
k)‚ààS‚àówhere (k‚Ä≤
i, v‚Ä≤
i)‚Üí(k‚àó
k, v‚àó
k)(3)
This ensures that every key kifromSmaps to a corresponding
key in S‚Ä≤and every key k‚Ä≤
ifromS‚Ä≤maps to a corresponding
key in S‚àó.
However, these three components are developed by different
teams, and the corresponding document table may not match
exactly, e.g. the names of the attributes in each table may not
be consistent since some attributes is recorded using a mixture
of natural language and formal language. Table I summarizes
5 common types of records with different forms. Besides,
there can be discrepancies in the number of values for an
attribute. For example, the acMode attribute may have two
states, STANDARD andECONOMY , in the system document,
but there are 3 modes (also TURBO ) in the CAN signal
specification. In such cases, it is also needed to match the
values with equivalent meanings. Moreover, there are instances
of missing attributes, where a corresponding mapping key
cannot be found. Since such issues are diverse and irregular,testers need to carry out such fuzzy matching cautiously based
on their own knowledge and experience.
TABLE I
5TYPES OF PROBLEMS THAT REQUIRE FUZZY MATCHING .
CategoryExample
Key 1 Key 2
Spelling errors DriverTimeSetting DriverTimeSeting
Abbreviations standard STD
Similar writing formats standard mode STANDARDMODE
Logical equivalents OFF NOT ON
Semantic equivalents AutoStart AutoLaunch
3) Write Test Cases: Based on the organized information,
testers can write reasonable and comprehensive test cases.
Specifically, the two main methods of a vehicle API, PUT
and GET, need to be tested separately. The PUT method is
used to set the car‚Äôs state, while the GET method is used to
retrieve the car‚Äôs current state. To verify the effectiveness of
the PUT method, we set the car‚Äôs state to Susing the PUT
method and then check whether all the virtual vehicle‚Äôs states
S‚àóin the VV system are as expected.
To verify whether the GET method is valid, we directly call
the GET method to retrieve the car‚Äôs current states, and check
if the retrieved states Smatch the expected states.
The process of writing test cases requires testers to have
a comprehensive understanding of the organized information
and a background in computer science, such as ensuring the
correctness of data types in test cases. In addition, testers need
to consider all test situations consider as many test situations as
possible to ensure high coverage of test cases. Finally, testers
write the test code to execute the test cases.
4) Running code and Evaluating results: Once the envi-
ronment and code are prepared, the code can be executed to
automatically test the API. Existing test frameworks and tools
can be used to organize the results, allowing to directly obtain
the final outcomes.
C. Obstacles to automation
Based on the current API testing process, the obstacles
to achieving automated API testing can be summarized as
follows:
‚Ä¢Fuzzy Matching: Since the system information, CAN
bus specifications and VV documentations are recorded
by different teams, the names of the attributes (i.e., the
key in the table) are sometimes inconsistent, as shown in
Table I. In addition, the value also needs to be mapped
based on the semantics of the key. For instance, the
attribute isAlarmActive may be TRUE /FALSE in
system files but Active /Inactive in CAN specifi-
cations. Such inconsistency makes it difficult to achieve
exact matching, necessitating the implementation of a
fuzzy matching mechanism.
‚Ä¢Informal Pseudocoded Mappings: In the CAN signal
table, data is often represented in the form of informal
pseudocode, leading to situations where a single key-
value pair maps to multiple counterparts. For exam-
ple, activating the car‚Äôs alarm clock requires setting theattribute and value as {AlarmActive:True }. However,
the corresponding data in the CAN signal table could
be represented as {AlarmClockStat:Active OR Alarm-
ClockStat:Ringing OR AlarmClockStat:Snoozed }. In this
scenario, it is necessary not only to parse the CAN
signal table but also to match the original key-value pair
with each entry in the CAN signal table. This requires
recognizing and parsing these pseudocode forms and
being able to handle one-to-many mappings.
‚Ä¢Inconsistent Units: Automotive values are often associ-
ated with units, such as speed, which can be measured
inkm/h orm/s. When units are inconsistent, direct
mapping of values between tables is not possible. Values
must be converted to the corresponding units before
mapping. Thus, the variety of units and the different
conversions required between them make detecting unit
inconsistencies and performing conversions a major chal-
lenge.
‚Ä¢Inter-Parameter Dependencies: Parameters often have
complex interdependencies, requiring coordinated set-
tings. For example, the attribute alarmTime might be
represented as a date-time string in system files, but in
CAN files, it might need to be mapped separately to
hours andminutes . Capturing and managing these
parameter relationships is not an easy task.
III. F ULLY AUTOMATED SPAPI TESTING WITH LLM S
This section presents the details of our automated testing
tool, SPAPI-Tester, which can integrate with LLMs to fully
automate the entire API testing process. The overall process,
as shown in Figure 5, can be divided into four main steps.
These steps are detailed in Process 1.
Process 1: Overall Workflow of SPAPI-Tester
1TestTracker = InitializeTestTracker()
2For APISpec inList(APISecifications)
3 S = ExtractTestObjects(APISpec)
4 S‚Äô = APIToCANMapping(S, CANTable)
5 S* = CANToVVMapping(S‚Äô, VVTable)
6 TestCases = GenerateTestCase(S, S*)
7 TestCode = WritingTestCode(TestCases)
8 TestTracker.analyzeTestRun(TestCode)
9TestReport = PushToTestRepo(TestTracker)
After initializing SPAPI ( line 1 ), the entire testing process
is divided into four parts: (1)Documentation understanding
(line 3 ): This part involves identifying test objects based on
the API specifications. (2)Information matching ( lines 4, 5 ):
This part entails look up relevant CAN table and virtual
vehicle documents to matching all these objects. (3)Test
case generation ( line 6 ): Using the matched data, this step
focuses on generating test cases for the API‚Äôs return results
and verifying the virtual vehicle‚Äôs status. (4)Executing test
cases and generating test reports ( line 7, 8, 9 ).A. Documentation understanding
The purpose of documentation understanding is to extract
the test objects from the API documentation. Standard API
documentation, commonly in YAML or Json format [14],
as shown in Figure 5(a), is structured to list attributes and
values associated with various objects. This structured format
lends itself well to template-based parsing. We parse these
documents and use predefined templates to extract the relevant
attributes and values. Based on existing templates [15], we
define a few simple and common rules to ensure the method‚Äôs
general applicability. These templates focus on fundamental
elements, such as endpoint names, attribute names, and data
types. Additionally, if sample API calls are provided in the
documentation, we extract these directly to test the basic
accessibility and functionality of the API.
However, using templates alone is insufficient for deter-
mining reasonable attribute values. We have identified the
following issues with relying only on templates:
(1) Cannot utilize attribute description: API documentation
often includes natural language descriptions of attributes that
templates cannot interpret or utilize. These descriptions typ-
ically contain constraints on the attributes, which are crucial
to prevent generating incorrect values.
(2) Lack of robustness: API documentation can some-
times be informal or inconsistent. For instance, attributes of
enumeration types are usually presented as [‚ÄùSTANDARD‚Äù,
‚ÄùECONOMY‚Äù], but some documents might incorrectly use
‚ÄùSTANDARD or ECONOMY‚Äù. Only using templates makes
it difficult to address these random and informal issues effec-
tively.
To overcome these two issues, we introduce LLMs to en-
hance the process. LLMs are utilized to analyze the entire API
documentation, leveraging natural language descriptions to
understand attribute constraints more effectively. Since LLMs
are capable of semantic understanding, they also mitigate
the impact of informal formatting or inconsistencies. This
allows the system not only to parse API properties but also to
map them to CAN signals, which is covered in detail in the
subsequent section. LLMs further generate constraints based
on attribute descriptions, producing reasonable values within
these constraints. The contextual insights provided by LLMs
help create a broader set of valid test values, thereby improving
the coverage and reliability of our test cases.
In practice, to ensure the stability of LLM outputs
and reduce the effect of the specific prompt formulations,
we employ DSPy [16] to automate prompt optimization.
DSPy enables us to write declarative LLM invocations as
Python code. Figure 6 illustrates a simplified example of
one of our prompts, along with the DSPy Signature. This
APIPropertyToCANSignal signature outlines the process
of converting structured API properties to CAN signals, which
automates the time-consuming task of constructing an API
property (ki, vi)and mapping it to a corresponding CAN
signal (k‚Ä≤
i, v‚Ä≤
i).
To further improve the accuracy and ease of extracting
structured data from the LLM, we format the LLM inputsClimateObject :
type : object
description : Manipulate climate 
settings on the truck.
required :
-type
properties :
acMode :
type : string
enum : ["STANDARD" , 
"ECONOMY" ]
autoFanLevel :
type : string
enum : ["LOW" , "NORMAL" , 
"HIGH" ]
isAuxiliaryHeaterActivated :
type : boolean"ClimateObject" : [{
"api_property" : "acMode" ,
"api_property_mappings" : {
"can_signal" : 
"APIACModeRqst" ,
"vv_state" : 
"apiacmode_rqst"
},
"api_value_mappings" : [ {
"api_value" : "ECONOMY" ,
"can_value" : "LOW" ,
"vv_state_value" : "1"},
{
"api_value" : ‚ÄùSTANDARD" ,
"can_value" : ‚ÄùHIGH" ,
"vv_state_value" : ‚Äù2"},]
}]"ClimateAPIObject ": 
{
    "type" : "Climate" ,
    "acMode ": 
"ECONOMY"
  }
"ClimateVVObject ": 
{
    "apiacmode_rqst ": 
"1"
  }import pytest
import json
import time
def test_put_climate (spapi_setup_teardown, 
api_client, vv):
response = api_client.put(
url= "/api/climate" ,
data=json.dumps({ "type": "Climate" , 
"acMode": "ECONOMY" })
)
# Check for correct status cod==e
assert response.status_code 200
# Assert VV attributes to verify correct behavior
assert vv.climate_control.apiacmode_rqst == 1Test  rig(a) API Specification (b) M atching Results (c) Test Cases
Virtual vehicleAPI response
(d) Test CodeDoc understanding  &  Matching Test case gen Test code writing Test execution
JinjaFig. 5. Architecture and workflow of SPAPI-Tester: The pipeline largely preserves the manual process and selectively uses LLMs to automate discrete steps.
class API Property ToCANSignal (dspy.Signature ):
    """    Given an API table and an API property -> CAN 
signal map, generate a list of API properties -> CAN 
signal(s). """
api_to_can_dict : dict = dspy.InputField (desc="A dictionary 
containing the mappings between an API attribute and its 
corresponding CAN signal. ")
input_example : dict = dspy.InputField ()
output_example : list[dict] = dspy.OutputField ()
mapped_api_to_can : list[dict] = dspy.OutputField (desc="A 
list of API properties, with corresponding CAN signals .")
Fig. 6. A DSPy Signature for automating API to CAN lookup (simplified).
API_CAN_INPUT_EXAMPLE = {
"ABCObject::valueOne ": "CANSignal1 ",
"ABCObject::valueFour::TRUE ": "AASignal:BB OR PV_AnotherSignal:CC ",
"ABCObject::valueFour::False ": "AASignal:AA ",}
API_CAN_OUTPUT_EXAMPLE = [
{"api_property ": "valueOne ", "can_signals ": [{"can_name ": 
"CanSignal1 "}]},
{"api_property ": "valueFour ","can_signals ": [{
"can_name ": "AASignal ",
"can_mappings ": [
{"api_value ": "true", "can_value ": ["BB"]},
{"api_value ": "false", "can_value ": ["AA"]},],},
{"can_name ": "PV_AnotherSignal ",
"can_mappings ": [{"api_value ": "true", "can_value ": 
["CC"]}],},],},
]
Fig. 7. Templatized examples for guiding API to CAN look up.
and outputs as dictionaries. We define dictionary-based prompt
templates to make tasks more comprehensible for the LLM
[17], as demonstrated in Figure 7. By specifying the expected
output fields, the signature directs the LLM to navigate incon-
sistencies in documentation and accurately associate API prop-
erties with CAN signal values. Furthermore, by typing fields
in the signature, we enable the use of a TypedPredictor
in DSPy, which validates the LLM response. If the response
does not conform to the specified types, DSPy re-prompts the
LLM, repeating this up to a maximum threshold until compli-
ance is achieved. This structured approach capitalizes on the
improved format adherence of LLMs, enhancing consistency
and reliability.B. Information Matching
As illustrated in Figure 4, the mapping of information in
our system encompasses two stages: mapping API properties
to CAN signals and mapping CAN signals to Virtual Vehicle
(VV) signals. These mappings are crucial for enabling signal
transmission within the vehicle as well as setting or verifying
the vehicle‚Äôs state. Since the processes and methods for these
two mappings are similar, we will detail the approach for
mapping API properties to CAN signals as an example.
First, we retrieve a set of candidate CAN signal key-value
pairs{(k‚Ä≤
i, v‚Ä≤
i)}from a CAN signal library through solely
matching the name of endpoint. Subsequently, we use the
extracted API attributes S={(ki, vi)}N
i=1and the candidate
CAN signals {(k‚Ä≤
i, v‚Ä≤
i)}as input to an LLM, enabling many-
to-many matching between API properties and CAN signals.
In many cases, attributes may have multiple enumerated val-
ues. For instance, as shown in Figure 7, an API property
‚ÄòvalueFour‚Äô might take the values ‚ÄòTrue‚Äô or ‚ÄòFalse‚Äô, while
the corresponding CAN signal might represent these states
as ‚ÄòAA‚Äô and ‚ÄòBB‚Äô. This type of mapping is common, and to
increase the stability of SPAPI-Tester, we utilize a separate
DSPy module specifically for matching enumerated values.
The input consists of enumerated values from both the API
property and the CAN signal, and the output is a mapping of
these values.
As discussed in Section II.C, there are several challenges
in the mapping process. First, for fuzzy matching , the LLM‚Äôs
strong semantic understanding is well-suited to handle these
cases. Second, for pseudocode mappings , we enhance template
robustness by embedding examples directly into the prompt,
as shown in Figure 8. For example, we map ‚ÄúAAsignal:BB OR
PVAnotherSignal:CC‚Äù to ‚Äúcan value‚Äù : ‚ÄúBB‚Äù, thereby mini-
mizing document noise while extracting relevant information.
Third, for unit inconsistencies , we apply a dedicated DSPy
module that uses a Chain-of-Thought (CoT) approach to ex-
tract and normalize units within values. This module converts
units (e.g., ‚ÄòkW‚Äô to ‚ÄòKilowatts‚Äô), ensuring unit alignment in
the test case generation phase.The final output is structured as a list, as defined in Figure
6, with each element containing a fully matched pair. The
same approach is then applied to map information between
CAN signals and VV signals, ultimately yielding complete
matching results S‚Ä≤andS‚àó.
C. Test case generation
Let's think step by step to generate the values for the API 
properties. 
1. Identify dependencies and rules in the API spec, such as 
a property setting the unit for another property.
2. Set property values based on descriptions to maintain 
consistency among dependent properties.
3. Set values for properties:
   - For strings, follow the format (e.g., date -time, enum) 
and choose a random value.
   - For numbers, select a value based on ' can_min', 
'can_max', and ' can_resolution '.
   - For properties with the same CAN signal, set values 
using logical consistency and dependency rules.
Fig. 8. Chain-of-Thought prompt for test case generation (simplified).
After matching all the necessary information, we integrate
these details into a structured document, as illustrated in Figure
5(b), which then serves as the basis for generating test cases.
Given the need to address multiple constraints during test
case generation‚Äîsuch as unit consistency‚Äîwe employ a
stepwise CoT approach to progressively incorporate these
constraints. Specifically, for inconsistent units , we prompt
the LLM to identify relationships between units and perform
any necessary conversions. For inter-parameter dependencies ,
the LLM captures relationships among parameters, ensuring
compatibility and avoiding value conflicts. Additionally, the
LLM identifies property types and manages specialized for-
mats, such as date-time strings. Finally, we guide the LLM in
handling cases common in industrial contexts, such as shared
CAN signals among multiple properties or specific constraints
on value ranges.
To ensure these constraints are applied consistently, we
leverage DSPy‚Äôs TypedChainOfThought method, which
consolidates all conditions within a single prompt. Figure 8
provides a simplified example of this prompt. For ease of use,
we specify that the module outputs test cases in dictionary
format, as depicted in Figure 5(c).
After generating the test cases, we use them to create test
code. The test code generally consists of two sections: a setup
section, which includes essential elements such as package
imports and requests to enable program execution, and a
validation section containing assertions. Since the setup code
remains consistent across tests, we design distinct Jinja1
templates for PUT and GET test cases. Using a simple code
renderer, we inject the generated API and VV test objects into
theJinja template to render the Pytest test case. Figure
5(d) shows an example test case rendered by the test-writing
module.
1https://palletsprojects.com/projects/jinja/D. Executing test cases and generating test reports
To ensure the automation of the entire process, the system
automatically executes the test code on the test rig [18], and
then generates a comprehensive test report. This report docu-
ments the details of the automated testing process, including
the test objects, the matching results, the generated test cases,
and the execution logs. Such documentation ensures that our
system maintains a high level of transparency, rather than
functioning as a black box.
IV. E XPERIMENTS
Our evaluation investigates the following questions.
RQ1: What are the pass rate, coverage, and failure-detection
capability of the test cases generated by the SPAPI-
Tester?
RQ2: To what extent can LLMs overcome the obstacles out-
lined in Section II.C to achieve end-to-end automated
testing?
RQ3: How efficient is this automated API testing?
RQ4: How effective is SPAPI-Tester in testing real-world in-
dustrial APIs?
Specifically, RQ1-RQ3 focus on ablation studies of SPAPI-
Tester, using controlled experiments to evaluate its capabilities
and performance. RQ4 examines the application of SPAPI-
Tester in the real-world, industrial setting with newly devel-
oped (and thus guaranteed to be unseen) APIs to demonstrate
the effectiveness of our end-to-end automated testing system.
A. Experimental Setup
In this section, we describe our experimental setup.
1) Subjects: Our research focuses on automating vehicle
API testing within an industrial setting, addressing unique
challenges such as inconsistencies across documentation and
system specifications. As no existing methods directly address
these issues in vehicle API automation, we could not compare
our approach with general API testing techniques, as they
lack the capability to handle the specific requirements of our
industrial setting.
We evaluated the quality of generated test cases for 41 truck
APIs using metrics such as pass rate and coverage. To assess
SPAPI-Tester‚Äôs error detection capabilities, we annotated an
additional 109 APIs developed by a leading vehicle manufac-
turer. These APIs were supported by system documentation
from in-house truck experts, CAN signal protocols from the
CAN-bus team, and virtual vehicle documentation from the
Virtual Vehicle team.
We tested four LLMs: two classic models‚ÄîGPT-3.5-turbo
(OpenAI, 2023-07-01-preview) and LLaMA3-70B (2024-04-
18)‚Äîand two recent advancements, GPT-4o (2024-05-13) and
LLaMA3.1-70B (2024-07-23). To ensure flexibility and reduce
maintenance, we opted not to fine-tune these models with
company-specific data, allowing seamless adaptation to new
models or data without retraining.TABLE II
PASS RATE ON DIFFERENT TYPES OF API S.
API Type Num.LLMs
GPT-3.5 LLaMA3 LLaMA3.1 GPT-4o
Energy 8 0.88 1.0 0.88 1.0
Driver Settings 6 0.83 0.83 1.0 0.83
Visibility Control 11 0.91 1.0 0.91 1.0
Software Control 3 1.0 1.0 1.0 1.0
Vehicle Condition 9 1.0 1.0 1.0 1.0
Other 4 1.0 1.0 1.0 1.0
Total/Average 41 0.93 0.98 0.95 0.98
2) Metrics: We evaluate our SPAPI-Tester both at the API
level and at the test case level. At API level, we use the pass
rate of the APIs as our metric. If all generated test cases for a
given API pass the tests, we consider that API to have passed.
Conversely, if any test case fails, the API is considered to have
failed. Therefore, the pass rate is defined as the proportion of
APIs that pass the tests.
At test case level, we primarily assess the quality and
coverage of the generated test cases. For these evaluations,
we employ precision and recall as our key metrics. Precision
measures the quality of the test cases generated, while recall
measures their coverage of API properties.
B. Pass Rate, Coverage, and Failure Detection (RQ1)
Pass rate: Since APIs with similar functions typically call
the same electronic control unit (ECU) in embedded systems
and, thus, share documentation within the same domain, we
grouped 41 truck APIs into 6 categories based on their
functions to present the results more clearly. Table II details
the pass rates for each category.
These 41 APIs are online and pre-verified, ensuring that any
failures observed during testing were due to issues within the
generated test cases or code. Results show that for the majority
of categories, all the APIs can pass the tests successfully, with
all four LLMs achieving high pass rates. Notably, SPAPI-
Tester achieved a 98% pass rate when using LLaMA3 and
GPT-4o, demonstrating the method‚Äôs accuracy in generating
valid test samples. However, GPT-3.5 exhibited slightly lower
performance in handling structured input-output, failing in two
cases due to improper CAN connection settings. Additionally,
a common error across all LLMs stemmed from missing unit
descriptions in API specifications. For example, when docu-
mentation omitted units for battery power, LLMs incorrectly
defaulted to watts (W) instead of kilowatts (kW), leading to
test case failures. Broad patterns of errors like this could likely
be addressed by further refining the prompts.
Coverage: In addition to pass rate analysis, we evaluated
the coverage of generated test cases to assess whether they
adequately test each API. A vehicle expert group was invited
to create ground truth test cases for 12 representative APIs,
each including 5 to 30 test cases across 4 categories. The
results are presented in Table III. All LLMs demonstrated high
precision, with precision rates exceeding 0.97 across the board
and reaching 1.0 for half of the APIs, showcasing the high
quality of test cases generated by our model. For cases whereprecision was below perfect, errors originated from limitations
in the fuzzy matching step.
However, recall rates did not reach optimal levels primarily
due to missing information in the API documentation, such as
absent units or variable types for some attributes. To maintain
high precision, SPAPI-Tester skips samples that lack sufficient
context for accurate matching, resulting in a recall loss of
approximately 15 percentage points. All untested attributes are
logged in the testing report, allowing developers to trace and
address these underlying issues.
Failure detection: To further assess the effectiveness of
the generated test cases in detecting failures, vehicle experts
labeled 109 additional truck APIs, being developed, iden-
tifying 38 as buggy. SPAPI-Tester created test cases that
successfully detected all buggy APIs with only four false
positives, achieving a 96% accuracy rate.
All models performed comparably, highlighting that our
stepwise, structured pipeline design reduces dependence on
specific LLM choices. We seamlessly migrated SPAPI-Tester
to different LLMs without requiring additional adaptation.
This largely model-agnostic pipeline design allows us to focus
on refining the testing process rather than selecting specific
LLMs, given the abundance of options.
C. LLMs‚Äô ability of overcoming obstacles (RQ2)
Fuzzy matching presents a significant challenge in auto-
mated API testing. We categorized common fuzzy matching
examples into five classes, selecting 20 test samples per class,
supplementing with manually written samples if needed. The
results, shown in Table IV (upper part), indicate that all
models achieved high precision rates, highlighting the LLMs‚Äô
capability to accurately recognize and match fuzzy inputs, a
key requirement for full automation. For semantic equivalents,
logical equivalents, and similar writing formats , all the models
attained an accuracy of 1.0 or nearly so, demonstrating their
strong pattern matching abilities in semantics and logic. How-
ever, for spelling errors , accuracy slightly dropped as some
errors altered word semantics, like mistaking date fordata .
In the abbreviations category, some abbreviations were too
short to discern, complicating the matching process.
For the inconsistent units issue, we selected 200 samples
for experiment. The results in Table IV (lower part) indicate
that while SPAPI-Tester achieves a high precision rate, the
recall remains suboptimal. The reason is that some documen-
tation explicitly annotates units for each attribute, while others
omit these details. In these cases, it becomes necessary to infer
the units based on descriptions or other contextual information,
which can affect the performance.
Another notable challenge is informal pseudocoded map-
pings , where a single test case may correspond to multiple
values. We selected 100 representative test cases for this
experiment. Each test case consists of two sets with multiple
(key, value) pairs, and the goal is to map elements between
these sets as accurately and comprehensively as possible. To
increase complexity, we intentionally selected test cases where
the sets contained different numbers of elements, creatingTABLE III
TEST CASE COVERAGE OF DIFFERENT TYPES OF API S. ‚ÄôP‚Äô ISPRECISION , ‚ÄôR‚Äô ISRECALL ,AND ‚ÄôF1‚Äô IS THE F1SCORE .
API TypeGPT-3.5 LLaMA3 LLaMA3.1 GPT-4o
P R F1 P R F1 P R F1 P R F1
Energy 0.96 0.69 0.78 0.98 0.76 0.85 0.96 0.74 0.84 0.96 0.79 0.87
Visibility Control 0.97 0.70 0.78 0.96 0.70 0.79 0.97 0.74 0.84 0.96 0.80 0.87
Vehicle Condition 1.0 0.95 0.97 1.0 0.9 0.95 1.0 0.95 0.97 1.0 0.95 0.97
Other 1.0 0.63 0.77 1.0 0.85 0.92 1.0 0.83 0.91 1.0 0.80 0.89
Average 0.97 0.73 0.80 0.98 0.79 0.88 0.98 0.81 0.89 0.97 0.85 0.90
TABLE IV
PERFORMANCE ON DIFFERENT TYPES OF FUZZY MATCHING (UPPER PART )AND INCONSISTENT UNITS (LOWER PART ).
API TypeGPT-3.5 LLaMA3 LLaMA3.1 GPT-4o
P R F1 P R F1 P R F1 P R F1
Spelling errors 0.89 0.76 0.82 0.92 0.73 0.81 0.91 0.78 0.84 0.91 0.83 0.87
Abbreviations 0.93 0.68 0.79 0.88 0.74 0.80 0.92 0.75 0.83 0.98 0.74 0.84
Similar writing formats 0.95 0.95 0.95 1.0 0.95 0.97 1.0 0.95 0.97 0.95 0.95 0.95
Logical equivalents 1.0 0.75 0.86 0.95 0.78 0.86 0.92 0.70 0.80 0.95 0.75 0.84
Semantic equivalents 1.0 0.70 0.82 1.0 0.73 0.84 0.94 0.73 0.82 1.0 0.70 0.82
Average 0.95 0.77 0.85 0.95 0.79 0.86 0.94 0.78 0.85 0.96 0.80 0.87
Inconsistent Units 0.95 0.67 0.79 0.95 0.59 0.73 0.95 0.67 0.79 0.98 0.70 0.82
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
Recall0.880.900.920.940.960.981.00Precision
GPT-3.5
LLaMA3-70B
GPT-4
LLaMA-31
Fig. 9. Matching performance on informal pseudocoded mappings.
scenarios where matching was non-trivial and recall could
fluctuate. To explore this, we conducted experiments under
both strict (precision-focused) and relaxed (recall-focused)
matching conditions. Examples reflecting different levels of
strictness were included in the prompts, and the strictness
level (e.g., strict, moderate, relaxed) was explicitly stated in
the prompts. The results are presented in Figure 9.
The experimental results indicate that under very strict con-
ditions, precision can reach up to 100%; however, recall drops
significantly, even below 20%. As the conditions are relaxed,
precision slightly decreases, but recall increases substantially,
reaching up to 55%. Under the most relaxed conditions, recall
rates for all the models approach 90%. This demonstrates that
our method can achieve high recall rates while maintaining a
high level of precision.
D. Time efficiency (RQ3)
In practical industrial scenarios, time consumption is an
important criterion for measuring tool efficiency. Therefore,
we measured the total time and the time taken at each stage
of the SPAPI-Tester in the testing process. Given that the
LLaMA model relies on local computational resources and
that the processing speeds of GPT-3.5 and GPT-4o do not
significantly differ in this pipeline, we report only the resultsTABLE V
TIME TO GENERATE TEST CASES ,PER STEP (SECONDS ).DU IS
DOCUMENT UNDERSTANDING ;RIIS RETRIEVAL INFORMATION ;TSG IS
TEST CASE GENERATION ;RUN MEANS RUNNING THE TEST CASES .
Requests Total DU & RI TSG Run
GET 55.0 6.6 3.6 44.8
PUT 56.3 6.8 4.3 45.2
Average 55.7 6.7 4.0 45
for GPT-3.5. We calculated the average time spent on testing
all APIs. Additionally, we separately computed the time for the
two major types of requests, i.e., PUT and GET. The results
are shown in Table V.
The results indicate that most of the time is consumed
during the execution of test cases, with a significant portion
dedicated to environment setup. SPAPI‚Äôs complexity requires
the appropriate configuration of embedded system environ-
ments, such as setting up the CAN bus for signal transmission.
Additionally, the VV system needs to read CAN signals and
complete the reading or setting of the virtual vehicle‚Äôs state,
which consumes a large amount of time.
The time required for PUT and GET requests is almost
identical, as our approach batch-generates matching results
or test cases for these requests, effectively minimizing time
differences. In the full pipeline, the DSPy module, which
leverages LLM-based capabilities, is called six times: once
for documentation comprehension, four times for information
matching, and once for test case generation. Additionally,
DSPy‚Äôs retry mechanism re-calls the module if the output does
not adhere to the predefined format. On average, the entire
process from initial input to test case generation takes about 11
seconds, which is remarkably fast for automated API testing.
Manual vehicle API testing in the automotive industry is
traditionally a time-consuming process, as it requires con-
sideration of numerous conditions. To better understand the
time demands of manual testing, we surveyed experts in our
industry, including three senior engineers and one technical
lead. They estimated that creating test cases for each APItakes approximately 0.1 to 3 FTE workdays, with most APIs
requiring about two hours. They generate 5 to 30 test cases
for each API.
In contrast, our SPAPI-Tester achieves remarkable efficiency
improvements. The system generates a complete set of test
cases for a single API in just 11 seconds, representing a
dramatic reduction in time and effort. This substantial speedup
not only reduces the time and effort required for API testing
but also alleviates the traditionally high time burden associated
with manual test case creation, greatly enhancing the API
testing process.
E. Performance on real-world industry APIs (RQ4)
To demonstrate the capability of SPAPI-Tester in an real-
world setting, we collected 193 newly developed and unveri-
fied truck APIs and their corresponding documentation from
a leading truck manufacturing facility. We then employed
SPAPI-Tester to conduct end-to-end automated testing, aiming
to identify issues within these APIs.
SPAPI-Tester identified 23 test failures. The test report
indicates that 22 test cases failed due to issues within the API
implementation, and one test case failed due to an error while
parsing the API documentation. On consultation with the API
developers, these were determined to be legitimate bugs in the
API implementation. The team has already started addressing
these issues upon receiving the checking results.
In addition, this demonstrates that SPAPI-Tester not only
has a high accuracy in detecting API errors but also provides
detailed reports that help quickly identify the root causes
of failures. Even when SPAPI-Tester was unable to generate
correct code, the detailed reports can help to identify the
failure causes quickly, thereby minimizing misdiagnoses. This
capability significantly enhances the practical utility of SPAPI-
Tester by providing precise and actionable insights. In sum-
mary, these results underscore the robust practical applicability
of SPAPI-Tester in real industrial environments.
F . Performance comparison with manual testing
To illustrate the advantages of SPAPI-Tester over man-
ual API testing, we conducted a comparative evaluation.
As described in Section IV .B, an expert team created
ground truth test cases for 12 APIs. To measure the pass
rate of manual testing, two additional engineers indepen-
dently created test cases for these APIs. Results showed
that one engineer‚Äôs test cases passed 10 APIs, while the
other‚Äôs passed 11. Both engineers missed one or two APIs
due to confusion over similar data entries. For instance,
attributes like reducedWeeklyRestsForCurrentWeek
and regularWeeklyRestsForCurrentWeek proved
challenging for human testers to differentiate, whereas SPAPI-
Tester‚Äôs LLMs handled them effortlessly. This led to an
average pass rate of 87.5% for manual testing at the API level,
while SPAPI-Tester, with test cases generated by four different
LLMs, achieved pass rates between 93% and 98%.
In terms of coverage, the average rate for manually created
test cases was 82%, with human testers occasionally skippingproperties due to incomplete API documentation (e.g., missing
units). SPAPI-Tester reached 85% coverage with GPT-4o,
while other models ranged between 73% and 81%.
To evaluate failure detection, we selected 10 APIs (5 of
which contained known bugs) from the 109 APIs mentioned
in Section IV .B. Both engineers identified all buggy APIs,
although one created a test case that falsely flagged a correct
API as erroneous, resulting in a recall rate of 100% and a
precision rate of 91% for manual testing. Similarly, SPAPI-
Tester achieved a recall rate of 100% with a slightly lower
precision of 90%.
In summary, SPAPI-Tester consistently generates high-
quality test cases, demonstrating comparable performance to
manual testing in terms of pass rate, coverage, and failure
detection.
V. D ISCUSSION
On complete test process automation ‚Äì Perhaps the most
significant finding from this case study is that our recipe is
capable of completely automating a real world test process.
Put simply, SPAPI testing ‚Äì a process that currently takes 2-
3 FTEs ‚Äì has effectively been substituted by SPAPI-Tester,
a fully automatic pipeline. This success stems from com-
bining LLMs with conventional automation, allowing SPAPI
testing to proceed without human intervention. Key to this
achievement is the nature of the SPAPI test process: it is
well-structured, decomposable, and requires human judgment
but not creativity. In such cases, LLMs serve as the critical
link to full automation by systematically replacing manual
steps. Maintaining the existing process structure further aids
automation in two ways. First, it defines clear, verifiable steps
where LLMs can be applied. Second, preserving the status
quo ensures that automation is achievable without imposing
possibly unreasonable costs of changing the test process ‚Äì an
observation that is crucial for real world application.
On the generality of LLMs as problem solvers ‚Äì Preserving
the design of the process no doubt identifies discrete tasks
where LLMs can be used. However, the clear enabler for
complete automation is that the LLM automates all manual
tasks with little practical regard to the actual nature of the task.
Alternative automation methods exist, such as using fuzzy
matching for inconsistent key-value mappings or a formal
language to specify cardinality in key-value relationships.
However, LLMs, as general problem solvers, eliminate the
need for multiple specialized solutions, simplifying real-world
implementations. While there is a cost to recast an LLM to
solve a specific problem ‚Äì like defining prompts or signatures
‚Äì the cost turns out to be manageable.
On implications on dependent processes ‚Äì If SPAPI testing
can be fully automated, its impact on adjacent processes
becomes a natural consideration. API implementation directly
precedes SPAPI testing, while integration within user-facing
subsystems follows it. Given SPAPI‚Äôs simplicity, LLMs could
potentially automate these dependent processes, extending
automation across much of the development lifecycle‚Äîan
important step for in-vehicle software engineering. Further,automating SPAPI-dependent applications could create a cas-
cade of fully automated lifecycles, reshaping automotive soft-
ware development. While promising, this vision comes with
challenges. Our results demonstrate LLMs‚Äô ability to automate
well-defined tasks and connect dependent processes, but also
highlight the effort required to adapt them for specific, veri-
fiable problems. These insights encourage further exploration
toward realizing this ambitious potential.
On the transferability of this recipe ‚Äì We may have
showcased completely automatic testing of an in-vehicle em-
bedded software application, but it is clear that many of
our observations and findings are transferable. Our proposed
criteria for automation‚Äîa decomposable process with steps
requiring judgment but not creativity‚Äîcan extend to other
domains. Additionally, our approach involves six distinct LLM
interactions: three align with general API testing workflows,
while the others, though tailored to automotive scenarios,
require minimal adaptation for different contexts. For example,
applying this method to another vehicle manufacturer would
take roughly one full workday (1 FTE). Certain aspects may
also benefit web server testing. Finally, our recipe of largely
preserving a test process and using LLMs to verifiably auto-
mate discrete manual steps is transferable to any test process
that meets the criteria we propose.
VI. R ELATED WORK
Existing research on API testing mainly focus on black-
box and white-box testing, depending on whether the source
code of the API is accessible [19]. White-box testing typically
involves generating test cases to thoroughly test the logic
within the code [20] [21]. For example, EvoMaster [22]
uses the Many Independent Objective (MIO) evolutionary
algorithm to optimize multiple metrics simultaneously, such
as line coverage, branch coverage, HTTP status coverage, and
the number of errors. Building on this, some studies have
employed additional tools for code instrumentation, such as
JVM [23] [24] and NodeJS programs [25] [26]. Atlidakis et
al. [27] calculate code coverage by pre-configuring basic block
locations and use this feedback to guide test generation.
Currently, most studies focus on black-box API testing,
aiming to enhance test case coverage for more comprehensive
API testing [28]. Template-based methods, such as fixed test
specifications and JSON schemas, are commonly used for gen-
erating accurate test cases [29] [30] [31] [32] [33]. However,
these approaches struggle to capture parameter dependencies.
To address this, Stallenberg et al. [34] proposed a hierarchical
clustering method, while Lin et al. [35] introduced a tree-
based representation of parameter relationships. Martin et al.
[36] further improved test diversity by integrating external
knowledge bases to generate reasonable values. Despite these
advancements, traditional methods often fail to achieve robust
and comprehensive testing.
Recently, LLMs have emerged as a promising direction
for API testing [37] [38]. Kim et al. [39] demonstrated
the utility of LLMs in interpreting natural language API
documentation to generate test values. Building on this, Leet al. [40] proposed constructing dependency graphs from
documentation to enhance test coverage. Other studies fine-
tuned LLMs using Postman test cases [41] or applied masking
techniques to predict test values [42]. However, these methods
face challenges in ensuring the validity and robustness of
generated test cases [43].
However, existing methods focus solely on test case gen-
eration, which is only one part of the API testing process,
and do not address the automation of the entire process.
In practical applications, these methods require significant
manual verification. For instance, some approaches need to
retrieve relevant yet often ambiguous information from ex-
ternal databases. Moreover, these methods lack robustness; if
the API specification is missing parameters or contains minor
errors, the process may fail. Unlike previous approaches, we
are the first to explore the automation of the entire API testing
process, focusing on current bottlenecks in API automation
and considering how to leverage LLMs to address these
challenges robustly.
VII. C ONCLUSION
Automated API testing is a critical process in software engi-
neering, essential for ensuring the reliability and functionality
of software systems. Despite its importance, API testing is
often time-consuming, labor-intensive, and prone to errors.
In practical applications, API testing involves retrieving and
organizing relevant documents, and writing test cases based
on the organized information. Due to the fuzzy matching of
information across documents, manual intervention is required,
hindering the automation of the entire testing process.
In this paper, we introduced SPAPI-Tester, the first system
designed for the automated testing of automotive APIs. We
decomposed the API testing process into a series of steps,
identifying the obstacles to automation at each stage. By
leveraging LLMs, we addressed these challenges, enabling full
automation of the testing workflow. The results from real-
world industrial API testing demonstrate that SPAPI-Tester
achieves high detection accuracy. Our comprehensive experi-
ments show that our system is highly robust and effective.
Our system offers valuable insights for other automated
API testing tasks and can be extended to web server API
testing. The findings underscore the potential of LLMs to
transform API testing by reducing manual effort and im-
proving efficiency, paving the way for broader adoption and
implementation in various testing environments.
ACKNOWLEDGMENT
This work was partially funded by the Wallenberg AI,
Autonomous Systems and Software Program (WASP), sup-
ported by the Knut and Alice Wallenberg Foundation, and
the Chalmers Artificial Intelligence Research Centre (CHAIR).
The authors also thank Earl T. Barr for his insightful discus-
sions.REFERENCES
[1] A. Fan, B. Gokkaya, M. Harman, M. Lyubarskiy, S. Sengupta, S. Yoo,
and J. M. Zhang, ‚ÄúLarge language models for software engineering:
Survey and open problems,‚Äù in IEEE/ACM International Conference
on Software Engineering: Future of Software Engineering, ICSE-FoSE
2023, Melbourne, Australia, May 14-20, 2023 , pp. 31‚Äì53, IEEE, 2023.
[2] X. Hou, Y . Zhao, Y . Liu, Z. Yang, K. Wang, L. Li, X. Luo, D. Lo, J. C.
Grundy, and H. Wang, ‚ÄúLarge language models for software engineering:
A systematic literature review,‚Äù CoRR , vol. abs/2308.10620, 2023.
[3] J. Wang, Y . Huang, C. Chen, Z. Liu, S. Wang, and Q. Wang, ‚ÄúSoftware
testing with large language models: Survey, landscape, and vision,‚Äù IEEE
Trans. Software Eng. , vol. 50, no. 4, pp. 911‚Äì936, 2024.
[4] J. Wang, Y . Huang, C. Chen, Z. Liu, S. Wang, and Q. Wang, ‚ÄúSoftware
testing with large language models: Survey, landscape, and vision,‚Äù IEEE
Transactions on Software Engineering , 2024.
[5] X. Chen, M. Lin, N. Sch ¬®arli, and D. Zhou, ‚ÄúTeaching large language
models to self-debug,‚Äù arXiv preprint arXiv:2304.05128 , 2023.
[6] Z. Yuan, Y . Lou, M. Liu, S. Ding, K. Wang, Y . Chen, and X. Peng,
‚ÄúNo more manual tests? evaluating and improving chatgpt for unit test
generation,‚Äù arXiv preprint arXiv:2305.04207 , 2023.
[7] D. Ajiga, P. A. Okeleke, S. O. Folorunsho, and C. Ezeigweneme,
‚ÄúEnhancing software development practices with ai insights in high-tech
companies,‚Äù 2024.
[8] J. Yoon, R. Feldt, and S. Yoo, ‚ÄúIntent-driven mobile gui testing with
autonomous large language model agents,‚Äù in 2024 IEEE Conference
on Software Testing, Verification and Validation (ICST) , pp. 129‚Äì139,
IEEE, 2024.
[9] R. Feldt, S. Kang, J. Yoon, and S. Yoo, ‚ÄúTowards autonomous test-
ing agents via conversational large language models,‚Äù in 2023 38th
IEEE/ACM International Conference on Automated Software Engineer-
ing (ASE) , pp. 1688‚Äì1693, IEEE, 2023.
[10] M. Fani Sani, M. Sroka, and A. Burattin, ‚ÄúLlms and process mining:
Challenges in rpa: Task grouping, labelling and connector recommen-
dation,‚Äù in International Conference on Process Mining , pp. 379‚Äì391,
Springer, 2023.
[11] M. Boukhlif, N. Kharmoum, and M. Hanine, ‚ÄúLlms for intelligent
software testing: a comparative study,‚Äù in Proceedings of the 7th Inter-
national Conference on Networking, Intelligent Systems and Security ,
pp. 1‚Äì8, 2024.
[12] A. Golmohammadi, M. Zhang, and A. Arcuri, ‚ÄúTesting restful apis: A
survey,‚Äù ACM Trans. Softw. Eng. Methodol. , vol. 33, nov 2023.
[13] X. Liu, J. Heo, and L. Sha, ‚ÄúModeling 3-tiered web applications,‚Äù in 13th
IEEE international symposium on modeling, analysis, and simulation of
computer and telecommunication systems , pp. 307‚Äì310, IEEE, 2005.
[14] OpenAPI, ‚ÄúOpenapi standard,‚Äù 2023. https://www.openapis.org.
[15] OpenAPI, ‚ÄúOpenapi template,‚Äù 2024. https://openapi-generator.tech/docs
/templating.
[16] O. Khattab, A. Singhvi, P. Maheshwari, Z. Zhang, K. Santhanam,
S. Vardhamanan, S. Haq, A. Sharma, T. T. Joshi, H. Moazam, et al. ,
‚ÄúDspy: Compiling declarative language model calls into self-improving
pipelines,‚Äù arXiv preprint arXiv:2310.03714 , 2023.
[17] OpenAI, ‚ÄúIntroducing structured outputs in the api.‚Äù https://openai.com/
index/introducing-structured-outputs-in-the-api/, 2023. Accessed: 2024-
10-21.
[18] M. Asyraf, M. Ishak, M. Razman, and M. Chandrasekar, ‚ÄúFundamentals
of creep, testing methods and development of test rig for the full-scale
crossarm: A review,‚Äù Jurnal Teknologi , vol. 81, no. 4, 2019.
[19] A. Golmohammadi, M. Zhang, and A. Arcuri, ‚ÄúTesting restful apis: A
survey,‚Äù ACM Transactions on Software Engineering and Methodology ,
vol. 33, no. 1, pp. 1‚Äì41, 2023.
[20] M. Zhang, B. Marculescu, and A. Arcuri, ‚ÄúResource-based test case
generation for restful web services,‚Äù in Proceedings of the genetic and
evolutionary computation conference , pp. 1426‚Äì1434, 2019.
[21] M. Zhang, B. Marculescu, and A. Arcuri, ‚ÄúResource and dependency
based test case generation for restful web services,‚Äù Empirical Software
Engineering , vol. 26, no. 4, p. 76, 2021.
[22] A. Arcuri, ‚ÄúAutomated black-and white-box testing of restful apis with
evomaster,‚Äù IEEE Software , vol. 38, no. 3, pp. 72‚Äì78, 2020.
[23] A. Arcuri, ‚ÄúTest suite generation with the many independent objec-
tive (mio) algorithm,‚Äù Information and Software Technology , vol. 104,
pp. 195‚Äì206, 2018.[24] A. Arcuri, ‚ÄúRestful api automated test case generation with evomaster,‚Äù
ACM Transactions on Software Engineering and Methodology (TOSEM) ,
vol. 28, no. 1, pp. 1‚Äì37, 2019.
[25] M. Zhang, A. Belhadi, and A. Arcuri, ‚ÄúJavascript instrumentation for
search-based software testing: A study with restful apis,‚Äù in 2022 IEEE
Conference on Software Testing, Verification and Validation (ICST) ,
pp. 105‚Äì115, IEEE, 2022.
[26] A. M√∏ller and M. T. Torp, ‚ÄúModel-based testing of breaking changes in
node. js libraries,‚Äù in Proceedings of the 2019 27th ACM joint meeting
on european software engineering conference and symposium on the
foundations of software engineering , pp. 409‚Äì419, 2019.
[27] V . Atlidakis, R. Geambasu, P. Godefroid, M. Polishchuk, and B. Ray,
‚ÄúPythia: grammar-based fuzzing of rest apis with coverage-guided feed-
back and learning-based mutations,‚Äù arXiv preprint arXiv:2005.11498 ,
2020.
[28] E. Viglianisi, M. Dallago, and M. Ceccato, ‚ÄúResttestgen: automated
black-box testing of restful apis,‚Äù in 2020 IEEE 13th International
Conference on Software Testing, Validation and Verification (ICST) ,
pp. 142‚Äì152, IEEE, 2020.
[29] C. Benac Earle, L.- ÀöA. Fredlund, ¬¥A. Herranz, and J. Mari Àúno, ‚ÄúJsongen: A
quickcheck based library for testing json web services,‚Äù in Proceedings
of the Thirteenth ACM SIGPLAN workshop on Erlang , pp. 33‚Äì41, 2014.
[30] S. K. Chakrabarti and P. Kumar, ‚ÄúTest-the-rest: An approach to testing
restful web-services,‚Äù in 2009 Computation World: Future Computing,
Service Computation, Cognitive, Adaptive, Content, Patterns , pp. 302‚Äì
308, IEEE, 2009.
[31] T. Fertig and P. Braun, ‚ÄúModel-driven testing of restful apis,‚Äù in
Proceedings of the 24th International Conference on World Wide Web ,
pp. 1497‚Äì1502, 2015.
[32] A. Arcuri, ‚ÄúTest suite generation with the many independent objec-
tive (mio) algorithm,‚Äù Information and Software Technology , vol. 104,
pp. 195‚Äì206, 2018.
[33] P. Godefroid, B.-Y . Huang, and M. Polishchuk, ‚ÄúIntelligent rest api data
fuzzing,‚Äù in Proceedings of the 28th ACM joint meeting on European
software engineering conference and symposium on the foundations of
software engineering , pp. 725‚Äì736, 2020.
[34] D. Stallenberg, M. Olsthoorn, and A. Panichella, ‚ÄúImproving test case
generation for rest apis through hierarchical clustering,‚Äù in 2021 36th
IEEE/ACM International Conference on Automated Software Engineer-
ing (ASE) , pp. 117‚Äì128, IEEE, 2021.
[35] J. Lin, T. Li, Y . Chen, G. Wei, J. Lin, S. Zhang, and H. Xu, ‚Äúfor-
est: A tree-based approach for fuzzing restful apis,‚Äù arXiv preprint
arXiv:2203.02906 , 2022.
[36] A. Martin-Lopez, S. Segura, and A. Ruiz-Cort ¬¥es, ‚ÄúRestest: Black-
box constraint-based testing of restful web apis,‚Äù in Service-Oriented
Computing: 18th International Conference, ICSOC 2020, Dubai, United
Arab Emirates, December 14‚Äì17, 2020, Proceedings 18 , pp. 459‚Äì475,
Springer, 2020.
[37] N. Li, J. Wang, C. Chen, and H. Hu, ‚ÄúApplication of api automation test-
ing based on microservice mode in industry software,‚Äù in Proceedings
of the International Conference on Algorithms, Software Engineering,
and Network Security , pp. 460‚Äì464, 2024.
[38] T. Olasehinde and S. Shekhar, ‚ÄúOptimizing microservices and api testing
pipelines with ai,‚Äù
[39] M. Kim, T. Stennett, D. Shah, S. Sinha, and A. Orso, ‚ÄúLeveraging large
language models to improve rest api testing,‚Äù in Proceedings of the 2024
ACM/IEEE 44th International Conference on Software Engineering:
New Ideas and Emerging Results , pp. 37‚Äì41, 2024.
[40] T. Le, T. Tran, D. Cao, V . Le, T. N. Nguyen, and V . Nguyen, ‚ÄúKat:
Dependency-aware automated api testing with large language models,‚Äù
in2024 IEEE Conference on Software Testing, Verification and Valida-
tion (ICST) , pp. 82‚Äì92, IEEE, 2024.
[41] S. Deepika Sri, M. Aadil S, S. Varshini R, R. CSP Raman, G. Rajagopal,
and S. Taranath Chan, ‚ÄúAutomating rest api postman test cases using
llm,‚Äù arXiv e-prints , pp. arXiv‚Äì2404, 2024.
[42] A. Decrop, G. Perrouin, M. Papadakis, X. Devroey, and P.-Y . Schobbens,
‚ÄúYou can rest now: Automated specification inference and black-box
testing of restful apis with large language models,‚Äù arXiv preprint
arXiv:2402.05102 , 2024.
[43] A. Pereira, B. Lima, and J. P. Faria, ‚ÄúApitestgenie: Automated api
test generation through generative ai,‚Äù arXiv preprint arXiv:2409.03838 ,
2024.