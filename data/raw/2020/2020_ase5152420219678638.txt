Unsupervised Labeling and Extraction of
Phrase-based Concepts in Vulnerability Descriptions
Sofonias Yitagesu
College of Intelligence and Computing
Tianjin University, Tianjin, China
sofoniasyitagesu@yahoo.comZhenchang Xing
Research School of Computer Science
Australian National University,
Data61 CSIRO, Australia
zhenchang.xing@anu.edu.auXiaowang Zhang∗
College of Intelligence and Computing
Tianjin University, Tianjin, China
xiaowangzhang@tju.edu.cn
Zhiyong Feng
College of Intelligence and Computing
Tianjin University, Tianjin, China
zyfeng@tju.edu.cnXiaohong Li
College of Intelligence and Computing
Tianjin University, Tianjin, China
xiaohongli@tju.edu.cnLinyi Han
College of Intelligence and Computing
Tianjin University, Tianjin, China
hanly2@foxmail.com
Abstract —People usually describe the key characteristics of
software vulnerabilities in natural language mixed with domain-
speciﬁc names and concepts. This textual nature poses a signiﬁ-cant challenge for the automatic analysis of vulnerabilities. Au-tomatic extraction of key vulnerability aspects is highly desirablebut demands signiﬁcant effort to manually label data for modeltraining. In this paper, we propose an unsupervised approachto label and extract important vulnerability concepts in texturalvulnerability descriptions (TVDs). We focus on three types ofphrase-based vulnerability concepts (root cause, attack vector,and impact) as they are much more difﬁcult to label and extractthan name- or number-based entities (i.e., vendor, product, andversion). Our approach is based on a key observation that thesame-type of phrases, no matter how they differ in sentencestructures and phrase expressions, usually share syntacticallysimilar paths in the sentence parsing trees. Therefore, we proposetwo path representations (absolute paths and relative paths) anduse an auto-encoder to encode such syntactic similarities. Toaddress the discrete nature of our paths, we enhance traditionalVariational Auto-encoder (V AE) with Gumble-Max trick forcategorical data distribution, and thus creates a Categorical V AE(CaV AE). In the latent space of absolute and relative paths,we further use FIt-TSNE and clustering techniques to generateclusters of the same-type of concepts. Our evaluation conﬁrmsthe effectiveness of our CaV AE for encoding path representationsand the accuracy of vulnerability concepts in the resultingclusters. In a concept classiﬁcation task, our unsupervisedlylabeled vulnerability concepts outperform the two manuallylabeled datasets from previous work.
Index T erms——Textual Vulnerability Descriptions, Vulnera-
bility Concepts, Unsupervised Representation Learning, ConceptLabeling and Extraction.
I. I NTRODUCTION
Software vulnerabilities, once disclosed, can be documented
in security databases, such as NVD [1], IBMXForce [2],
ExploitDB [3]. People usually describe the key characteristicsof a vulnerability in natural languages, such as the examplesshown in Fig. 1. Key characteristics often include vulnerableproduct and versions, product vendor and root cause, attack
∗Corresponding author.
Fig. 1. Motivating Example: Reported Vulnerabilities in NVD
vector and impact of the vulnerability. Although these vul-nerability databases provide rich information about knownvulnerabilities, security analysts have to manually identify andextract key information of their interests from textual vulnera-bility descriptions (TVD). Automatic information extraction ishighly desirable to expedite vulnerability analysis and securityresearch, for example, ﬁnding all vulnerabilities of a productwith certain impact, or establishing traceability links betweenrelated vulnerabilities in different databases, or detecting dis-crepancies between vulnerability reports regarding the samevulnerability created by different people [4]–[11].
The desired automatic information extraction is faced with
two challenges. First, TVDs have large variations in bothsentence structure and phrase expression. As the examplesin Fig. 1 show, whether a key entity or concept is men-tioned or the order of different entities and concepts beingmentioned vary greatly from one TVD to another. For exam-ple, TVD1 starts with “Buffer overﬂow” (root cause), whileother TVDs start with the vendor, product, or componentnames. TVD1/TVD2/TVD3 have similar sentence structures,but TVD4/TVD5/TVD6 have different sentence structures.
9432021 36th IEEE/ACM International Conference on Automated Software Engineering (ASE)
DOI 10.1109/ASE51524.2021.000872021 36th IEEE/ACM International Conference on Automated Software Engineering (ASE) | 978-1-6654-0337-5/21/$31.00 ©2021 IEEE | DOI: 10.1109/ASE51524.2021.9678638
978-1-6654-0337-5/21/$31.00  ©2021  IEEE
Furthermore, aspect descriptions vary greatly in details and
phrase expressions. For example, impact can be expressed“allow [A TT ACKER] to [IMP ACT]”, “which result in [IM-P ACT]”, or “leading to [IMP ACT]”. All such sentence andphrase variations make it hard to develop a comprehensive setof linguistic rules for concept extraction in TVDs.
Machine learning techniques can learn distinct features
from TVDs for automatic information extraction. For example,Dong et al. [12] trains the Named Entity Recognition (NER)model for extracting product names and versions in TVDs.However, machine learning techniques require TVDs withlabeled entities and concepts for model training. Unlike abun-dant labeled general text, there are no large datasets of labeledTVDs for training information extraction models, especiallyfor phrase-based concepts like root cause, attack vector, andimpact. As shown in Fig. 1, TVDs are full of software-speciﬁcnames, number sequences, and domain-speciﬁc terms. Modelstrained with only general text but not vulnerability descriptionssuffer from sharp performance degrade [13]. However, manuallabeling a large corpus of TVDs is a labor-intensive task.
To develop novel ways to automatic information extraction
in TVDs, we make two key observations over TVDs. First,vendor, product, and version information are almost alwayspresent in TVDs, because they are the most basic informationfor identifying a vulnerability. Product and vendor are usuallydistinct names, and known vulnerable product and vendornames are well archived in Common Platform Enumeration(CPE) dictionary. V ersions are number sequences and havedistinct orthographic features. Therefore, it is easy to developa gazetteer or rules to precisely recognize vendor, product,and version in TVDs. Second, although the surface formsof the root cause, attack vector, and impact vary greatlyat both sentence and phase level, the syntactic relationshipsbetween a speciﬁc type of phrase-based concepts and ven-dor/product/version are stable and manifest high regularities.Such syntactic relationships can be encoded in similar pathsof Part-of-Speech (POS) tags in the sentence parsing tree.Take TVD1 and TVD3 in Fig. 1 as an example. The rootcause, “buffer overﬂow” and “generates RSA key pairs” areincompletely different sentence structures and have very dif-ferent phrase expressions. However, their absolution paths inthe sentence parsing trees of TVD1 and TVD3 are “ROOT -S-NP-NP” and “ROOT -S-VP-NP-NP ,” and their relative pathsfrom Andres Huggel [VENDOR] and Alibaba [VENDOR] are“NP-PP-NP-PP-NP-NP” and “NP-PP-NP-PP-NP-NP ,” whichare very similar (see Section II-D for how we construct pathrepresentations).
Based on the above two observations, we propose an unsu-
pervised method for labeling and extracting phrase-based con-cepts in TVDs. First, we build a gazetteer based on CPE dic-tionary and version orthographic features, and use gazetteer-based matching to identify vendor, product, and version inTVDs. Then, we use a vulnerability-speciﬁc POS tagger [14],which can generate meaningful POS tags for TVDs. Next, weobtain the parsing tree of a TVD sentence. For each terminalnoun phrase in the tree, we obtain an absolute path of POStags from tree root to this noun phrase, and a relative pathof POS tags from each found vendor/product/version phraseto this noun phrase. We focus on noun phrases because theyusually constitute the most informative part of the root cause,attack vector, and impact. We build a vocabulary of frequentabsolute paths and a vocabulary of frequent relative paths. Aspaths are discrete data, we use Categorical V ariational Auto-Encoder (CaV AE) [15]–[17] to learn path embeddings in anunsupervised way. CaV AE places paths in a latent space inwhich we use clustering algorithms to cluster similar paths,which usually represent the same-type of concepts. Finally,based on the human label of a path cluster, our approach labelsand extracts all noun phrases in TVDs corresponding to thepaths in the cluster as a certain type of vulnerability concept.We conduct extensive experiments to evaluate our approachon different setups, which achieves 83%, 86.3%, 87%, and84% accuracy for attack vector, impact, root cause, and others,respectively.
This paper makes the following three contributions:
•We make two key observations on the data characteristicsof TVDs, which inspire the design of our unsupervisedmethod for concept labeling and extraction.
•We design and implement a novel method for labelingand extracting phrase-based concepts in TVDs based onunsupervised machine learning techniques (CategoricalV ariational Autoencoder and clustering).
•We evaluate our approach on a dataset of 275,344 vul-nerability sentences from NVD. Our evaluation showsthe high accuracy of the root cause, attack method, andimpact labeled using our approach and the effectivenessof these labeled concepts in the training concept classiﬁer.
We provide all our implementations; it can be found at https://bitbucket.org/Materakemia/concept
labeling/src/master/
II. O UR APPROACH
This section describes our proposed approach for unsu-
pervised labeling and extraction of phrase-based concepts inTVDs, as shown in Fig. 2. In this work, we are interestedin three phrase-based concepts: root cause, attack vector, andimpact. We crawl and build a TVD corpus from vulnera-bility databases (e.g., NVD [1] or CVE [18]). We use asoftware-speciﬁc tokenizer [14], a vulnerability-speciﬁc POStagger [14] and the Context-Free Grammar (CFG) sentenceparser by Stanford Parser [19] to process the TVDs and obtainall noun phrases in the TVDs. We build a gazetteer of vendors,products, and versions from the CPE dictionary and Wikipediaand use it to identify vendor, product, and version noun phrases(referred to as VPV noun phrases) in TVDs. If VPV nounphrases are found, we calculate relative paths from each VPVnoun phrase to the rest of non-VPV noun phrases and alsocalculate absolute paths from tree root to these non-VPV nounphrases. Distinct frequent absolute paths and relative paths arefed into a CaV AE for unsupervised representation learning. Inthe latent space of absolute paths and relative paths, clusteringalgorithms (e.g., FIt-TSNE and DBSCAN) are used to clustersimilar paths, which usually represent same-type of concepts.
944Fig. 2. Approach Overview
Finally, based on the human label of a path cluster, our
approach labels all noun phrases in TVDs corresponding tothe paths in the cluster and outputs these labeled noun phrasesas concerning concepts.
A. Phrase-based Vulnerability Concepts
The concepts that describe vulnerability characteristics in
TVDs are usually noun phrases. For example, “a denial of
service (application crash)” is a noun phrase that describesthe impact of an attack.
T ABLE I
TYPES OF PHRASE -BASED VULNERABILITY CONCEPTS
Type Examples
Root Cause not properly close client connections
Attack V ector via a series of interrupted requests to a Large Object URL
Impact a denial of service (application crash)
Table I shows three types of phrase-based vulnerability
concepts and some representative examples we aim to extractin this work. These vulnerability concepts give essential in-formation to security analysts and software developers aboutthe root cause, attack vector (or means), and impact (orconsequence) of a vulnerability and attack [20], [21]. RootCause is a program error or software weakness, such as “bufferoverﬂow”, “not properly close client connections” that canbe exploited to attack a system or network. Attack V ectordescribes various methods of executing an attack, for example,“via images with crafted IPTC metadata.” Impact describes thepossible consequences of an attack, such as “denial of service”,or “proxy-server resource consumption.” We also consider anOthers type which includes all noun phrases not belonging tothe root cause, attack vector, or impact. As a by-product, ourapproach also identiﬁes three types of entities (i.e., vendor,product, and version) through gazetteer-based matching.
B. Sentence Tokenization, POS Tagging and Parsing
We use the vulnerability-speciﬁc tokenizer implemented
in [14] to tokenize TVD sentences. We used a pre-trained
vulnerability-speciﬁc POS tagger [14] to assign POS tags totokens in TVDs. We use the Stanford parser
1to convert a
1https://nlp.stanford.edu/software/lex-parser.shtmlPOS annotated TVD sentence into a parsing tree. The parsergenerates grammatical syntactic constructs of phrases such asa sentence (S), prepositional phrase (PP), noun-phrases (NP),verb phrases (VP), adjective phrases (AdjP), adverb phrases(AdvP), conjunctions phrases (ConjP) with their particularPOS tags. In this work, we focus on a terminal ngram-wordsheader by noun phrases (NP). Although there are noun phrasesthat do not contain TVD concepts, almost all TVD conceptsare expressed as (part of) a noun phrase.
C. Gazetteer-based V endor/Product/V ersion Recognition
Gazetteers are a dictionary of names or name patterns.
In this work, we build a gazetteer for the vendor, product,
and version, respectively. Information sources are the CPEdictionary and Wikipedia. Our gazetteer includes not onlyfull names but also known abbreviations and synonyms, forexample, OS, UNIX. For version, in addition to speciﬁcversion numbers, we also abstract them into number patternslike digit.digit.digit where a digit is a placeholder for any digit.
Assume there are some vendor, product, or version noun
phrases in some branches of a sentence parsing tree. We usea gazetteer-based searching method that hierarchically drillsdown to sub-branches of the parsing tree until it ﬁnds anexact match of a name or name pattern in the gazetteer. If amatch is found, the algorithm returns the discovered gazetteername (e.g., vendor “Andreas Huggel” Product “Exiv2” and‘V ersion “0.9”). We refer to the found vendor/product/versionphrases as VPV phrases. As the examples in Fig. 1 show, aTVD may mention all three types of information (e.g., TVD1,TVD3, and TVD5) or just some types (e.g., TVD2). We use thefound VPV phrases as “landmarks” to recognize other typesof vulnerability information, such as root cause, attack vector,and impact, which are non-VPV noun phrases. If no VPVphrases are found in a TVD sentence, this sentence will bediscarded. But we never see TVDs that do not mention anyvendor, product, or version information in our dataset.
D. Path Representation of non-VPV Phrases
Based on the sentence parsing tree and the found VPV
phrases, we construct an absolution path and some relative
paths (one for each VPV phrase) for each non-VPV phrase.
945Fig. 3. Illustration of Path Construction and Path Pattern
T ABLE II
ABSOLUTE AND RELA TIVE PAT H REPRESENT A TION
TVD1 Absolute Paths Relative Paths
X0: Buffer overﬂow ROOT -S-NP-NP NP-PP-NP-PP-NP-NP
X2: not null terminate strings ROOT -S-VP-NP NP-PP-NP-PP-NP-S-VP-NP
X4: the sscanf function ROOT -S-VP-PP-S-VP-NP-NP NP-PP-NP-PP-NP-S-VP-PP-S-VP-NP-NP
X6:denial of service application crash ROOT -S-VP-PP-S-VP-NP-SBAR-S-VP-S-VP-VP-NP NP-PP-NP-PP-NP-S-VP-PP-S-VP-NP-SBAR-S-VP-S-VP-VP
X11:via crafted IPTC ROOT -S-VP-NP-SBAR-S-VP-S-VP-VP-NP-PP-NP-PP-NP NP-PP-NP-PP-NP-S-VP-PP-S-VP-NP-SBAR-S-VP-S-VP-VP-PP-NP
TVD2
X2: not properly close client connections ROOT -S-VP-NP-NP NP-PP-NP-S-VP-NP-NP
x4: proxy-server resource consumption ROOT -S-VP-NP-SBAR-S-VP-S-VP-VP-NP NP-PP-NP-S-VP-NP-SBAR-S-VP-S-VP-VP-NP
X5: a denial of service ROOT -S-VP-NP-SBAR-S-VP-S-VP-VP-NP-NP NP-PP-NP-S-VP-NP-SBAR-S-VP-S-VP-VP-NP-NP
X6: service proxy-server resource consumption ROOT -S-VP-NP-SBAR-S-VP-S-VP-VP-NP-PP-NP NP-PP-NP-S-VP-NP-SBAR-S-VP-S-VP-VP-NP-PP-NP
X9: interrupted requests to a Large Object ROOT -S-VP-NP-SBAR-S-VP-S-VP-VP-NP-PP-NP-PP-NP NP-PP-NP-S-VP-NP-SBAR-S-VP-S-VP-VP-NP-PP-NP-PP-NP
TVD3
X2: an exponent of 1 ROOT -S-NP-PP-NP NP-NP-PP-NP-PP-NP-VP-S-NP-PP-NP
X3: RSA key pairs ROOT -S-VP-NP-NP NP-PP-NP-PP-NP-NP
X5: transactions that are sent in cleartext ROOT -S-VP-NP-SBAR-S-VP-PP-NP NP-PP-NP-PP-NP-SBAR-S-VP-PP-NP
TVD4
X1: reset arbitrary passwords ROOT -S-VP-PP-S-VP-NP-SBAR-S-VP-S-VP-VP-NP NP-PP-NP-S-VP-NP-SBAR-S-VP-S-VP-VP-NP
X2: remote attackers ROOT -S-VP-S-NP NP-PP-NP-S-VP-S-NP
X3: arbitrary passwords ROOT -S-VP-S-VP-VP-NP NP-PP-NP-S-VP-S-VP-VP-NP
X4: an associated e-mail address ROOT -S-VP-S-VP-VP-SBAR-S-NP NP-PP-NP-S-VP-S-VP-VP-SBAR-S-NP
TVD5
X1: another tab then switching back to the original ROOT -S-NP-PP-NP-NP NP-NP-PP-ADJP-NP-NP
X2: the Screen Deﬁnition ROOT -S-NP-PP-NP-ADJP-PP-NP-SBAR-S-NP NP-NP-SBAR-S-NP
X3: local users ROOT -S-VP-S-NP NP-NP-PP-ADJP-NP-PP-NP-S-VP-S-NP
X4: certain options ROOT -S-VP-S-VP-VP-NP NP-NP-PP-ADJP-NP-PP-NP-S-VP-S-VP-VP-NP
TVD6
X0: mishandles a short descriptor ROOT -S-VP-NP-NP-PP-NP NP-PP-NP-PP-NP-NP
X1: leading to out-of-bounds memory access ROOT -S-VP-NP-SBAR-S-VP-S-VP-VP-NP-PP-NP NP-PP-NP-S-VP-NP-SBAR-S-VP-S-VP-VP-NP-PP-NP
X3: out-of-bounds memory access ROOT -S-VP-NP-SBAR-S-VP-S-VP-VP-NP NP-PP-NP-S-VP-NP-SBAR-S-VP-S-VP-VP-NP
1) Absolute Path Representation: In a sentence parsing tree,
an absolution path is identiﬁed by the sequence of POS tags
from the tree root to the node of a terminal noun phrase. Fig. 3shows partially the parsing trees of TVD1, TVD2, and TVD3.First, we determine terminal noun phrases as a sequence ofterminal words in a branch of the parsing tree rooted at anoun-phrase (NP) node. For example, NP1 “buffer overﬂow”and NP5 “not null terminate strings” are two terminal nounphrases in the parse tree of TVD1. NP4 “RAS key pairs” isa terminal noun phrase in the parsing tree of TVD3. Giventhe NP node of a terminal noun phrase, an absolute path canbe constructed by traversing the parsing tree from the ROOTnode to this NP node. For example, the absolution path for“buffer overﬂow” is ROOT0-S0-NP0-NP1. As we do not careabout the speciﬁc index of nodes, we can abstract this path asROOT -S-NP-NP . In the same way, the absolute path for “RSAkey pairs” is constructed as ROOT -S-VP-NP-NP .
We make a key observation on the absolution path repre-sentation. That is, absolute paths can abstract away the surfacedifferences of sentence structures and phrase expressions, andsame-type non-VPV concepts likely have the same or similarabsolute paths. For example, the absolute paths for “not nullterminate strings” in TVD1 and “RSA key pairs” in TVD3differ only in one NP , and both two phrases are the root cause.We refer to such similar paths as path patterns. Note that thelocations of similar paths in the parsing trees, the index ofnodes on the paths, and speciﬁc expressions of correspondingterminal noun phrases can all be very different.
Table II shows more examples. TVD1 and TVD2 have 5
non-VPV phrases; TVD3 and TVD6 have 3 non-VPV phrases,and TVD4 and TVD5 have 4 non-VPV phrases. For example,terminal noun phrases such as “not properly close clientconnections” in TVD2 and “RSA key pairs” in TVD3 bothhave the same absolution path (i.e., ROOT -S-VP-NP-NP), andthey are mostly root causes. The terminal noun phrase suchas “remote attackers” in TVD4 and “local users” in TVD5
946both have the same absolution path (i.e., ROOT -S-VP-S-NP),
and they are mostly attack methods. The terminal noun phrasesuch as “arbitrary passwords” in TVD4 and “certain options”in TVD5 both have the same absolution path (i.e., ROOT -S-VP-S-VP-VP-NP), and they are mostly impacts. In the sameway, terminal noun phrases such as “interrupted requests to aLarge Object” in TVD2 and “via crafted IPTC” in TVD1 bothhave the same absolution path (i.e., ROOT -S-VP-NP-SBAR-S-VP-S-VP-VP-NP-PP-NP-PP-NP), and they are mostly relatedto attack vectors of vulnerability concepts.
On the other hand, phrases such as “a denial of service”
and “service proxy-server resource consumption” in TVD2have nearly similar absolute path patterns (i.e., ROOT -S-VP-NP-SBAR-S-VP-S-VP-VP-NP-NP), they only differ with ”PP-NP”, and these phrases are mostly consequences of vulnera-bility, i.e., impacts.
2) Relative Path Attributes: A relative path is identiﬁed by
the sequence of POS tags from a VPV phrase (i.e., vendor,product, or version) to a non-VPV noun phrase. If the VPVphrase and the non-VPV phrase share the same branch of theparsing tree (i.e., they have a common ancestor), all commonancestor nodes are truncated from their absolute paths. Thisgives truncated paths. Then, the truncated path of the VPVphrase is reversed to obtain the reversed-truncated path. Therelative path is then obtained by concatenating the reversed-truncated path of the VPV phrase, the lowest common ancestornode, and the truncated path of the non-VPV phrase.
Take the version phrase (0.9) and the non-VPV phrase
“buffer overﬂow” in TVD1 in Fig. 3 as an example. Theabsolute path of “0.9” is ROOT -S-NP-PP-NP-PP-NP , andthe absolute path of “buffer overﬂow” is ROOT -S-NP-NP .Common ancestor nodes ROOT -S-NP will be removed, andthe lowest common ancestor is NP . The reversed-truncatedpath of “0.9” is NP-PP-NP-PP , and the truncated path of“buffer overﬂow” is NP . Finally, the relative path from “0.9” to“buffer overﬂow” is NP-PP-NP-PP-NP-NP . In the same way,the relative path from version phrase (2.4.0) to “not properlyclose client connections” is constructed as ROOT -NP-PP-NP-S-VP-NP-NP .
Relative paths capture syntactic relationships between found
VPV phrases in a TVD sentence and non-VPV phrases. Simi-lar to absolute paths, relative paths also abstract differencesin sentence structures and phrase expressions, and same-type non-VPV concepts likely share similar relative paths(i.e., syntactic relationships) with found vendor, product, andversion phrases in TVDs. For example, “buffer overﬂow”in TVD1, “mishandles a short descriptor” in TVD6, and“RSA key pairs” in TVD3 have similar relative paths (i.e.,NP-PP-NP-PP-NP-NP), and they are all root cause. As an-other example, the relative path of “proxy-server resourceconsumption” in TVD2, “out-of-bounds memory access” inTVD6, and “reset arbitrary passwords” in TVD4 have a similarrelative path pattern (i.e., NP-PP-NP-S-VP-NP-SBAR-S-VP-S-VP-VP-NP), and they are all impacts of vulnerability.E. Unsupervised Representation Learning
Although absolute and relative paths abstract away many
detailed sentences and phrase differences, they are still tooﬁne-grained to determine their similarities. There are generallya large number of distinct paths in a large TVD corpus, andsimilar paths vary greatly in length and POS tags. Therefore,it is hard to measure path similarity using approximate stringmatching methods. In this work, we consider each distinct pathas a word in a vocabulary and use an auto-encoding techniqueto project discrete paths (i.e., high-dimensional one-hot vector)into a low-dimensional continuous vector space in which pathsimilarity can be measured using Euclidean distance.
1) Categorical V ariational Autoencoders (CaVAE): Auto-
encoder is a network architecture which include an encoderand a decoder. The encoder compresses a high-dimensionalinput vector xinto a low-dimensional continuous vector in
a latent feature space. The decoder then tries to decode thelow-dimensional continuous vector and reconstruct the high-dimensional input vector x
/prime. The encoder and decoder are
trained together with the reconstruction loss between xand
x/prime, that is, how similar the reconstructed output matches the
input. The model training is unsupervised in the sense that itrequires only input paths, but no label is needed. We can alsosay the model training is supervised by the reconstructed pathsas similar as the input paths. Once the model is trained, we usethe trained encoder to embed unseen paths in the latent vectorspace. Due to the reconstruction goal, similar paths would havesimilar latent vectors, which can be further clustered.
We adopte the network structure of V ariational Auto-
encoder (V AE) [15], [16].The V AE assumes Gaussian dis-tribution. However, it does not ﬁt our data as our paths arehighly-sparse categorical data. Therefore, we extend V AE withGumbel-Max trick [22], [23] which suits modeling highly-sparse categorical data with encoder-decoder architecture.By using Gumbel-SoftMax trick [22], [23], we can samplefrom the latent space, feed these samples to the decoder,and generate new output. Essentially, Gumbel-Softmax [17]approximate categorical samples and compute gradients viathe re-parameterization trick. We use binary-cross entropy tocompute the loss between the reconstructed paths and the inputpaths. If the decoder does not reconstruct the input satisfac-torily, it incurs much reconstruction loss. We also computethe KL-divergence [24] for non-differentiable gradient, andwe minimize both at the same time.
2) Input to CaVAE: We build two dictionaries: one for
distinct absolute paths and the other for distinct relative paths.For each path, we count its occurrence frequency in the entirecorpus. We sort the paths by their occurrence frequenciesand take up to 5000 paths (cover paths that occur at leasttwice in our corpus) to build the two vocabularies: absolute-path vocabulary and relative-path vocabulary. Each path inthe vocabulary is regarded as a word, and represented ina one-hot vector. Given each non-VPV phrase in a TVDsentence, we represent the phrase’s absolute path as the one-hot vector of this path. We represent the phrase’s relative paths
947by concatenating the one-hot vectors of the relative paths of
this phrase. We have two CaV AEs jointly trained: one forembedding absolute paths and the other for embedding relativepaths. We concatenate the latent vectors of the two CaV AEsas the latent vector of the input non-VPV phrase.
F . Clustering and Concept Labeling
The trained CaV AE encoder can be used to place all non-
VPV phrases in a TVD corpus into the latent space in terms of
the phrases’ absolute path and relative paths. We can clusternon-VPV phrases based on their closeness in the latent space.We use three different clustering settings: CaVAE−>FIt−
TSNE ,CaVAE−>F I t−TSNE−>D B S C A N , and
CaVAE−>D B S C A N . FIt-TSNE [25] is a t-distributed
stochastic neighbor embedding method for visualizing high-dimensional data points by giving each data point a locationin a two-dimensional map. The CaVAE−>FIt−TSNE
setting allows us to observe the latent space learned bythe CaV AE. The two-dimensional map can also be regardedas a clustering output. CaVAE−>F I t−TSNE−>
DBSCAN further applies DBSCAN (Density-based Spatial
Clustering of Applications with Noise) [26] to the two-dimensional map of FIt-TSNE to cluster data points in thistwo-dimensional space. CaVAE−>D B S C A N applies
DBSCAN directly to high-dimensional late space by CaV AE.Rahmahs algorithm [27] is applied to determine the bestparameters for the DBSCAN algorithm.
Based on the observation of path patterns and their relation-
ships with vulnerability concept types, we assume that non-VPV phrases with similar paths in a cluster should representa distinct concept type. The last step of our approach is toask a security expert to sample non-VPV phrases in eachcluster and label the cluster with the concept type (rootcause, attack vector, or impact) that the majority of non-VPVphrases represent. If the cluster is a mix of several differentconcept types with similar phrase amounts, the cluster will belabeled as Others. Based on the cluster labels and the phraseboundaries, our approach can label and extract root cause,attack vector and impact phrases from the TVDs in the corpus,as visualized in Fig. 1. As a by-product, VPV phrases detectedby gazetteer matching can also be extracted.
III. E
V ALUA TION
We conduct a series of experiment to investigate the follow-
ing research questions:
RQ1: How robust is our CaV AE on embedding syntactic
structures of absolute paths and relative paths of non-VPVphrases? We answer this question by study cluster propertiesin the latent space.
RQ2: Is our hypothesis that syntactically similar paths leads
to same-type of non-VPV concepts valid? We answer thisquestion by manually validating the types of non-VPV phrasesin a cluster against the human-labeled cluster type.
RQ3: To what extent variant model designs affect the
accuracy of unsupervised labeling? We replace the key modelcomponents with alternative designs and compare the perfor-mance of model variants.
RQ4: How well can our unsupervised labeled data support
machine learning tasks? We compare the performance of avulnerability concept classiﬁer trained with our unsupervisedlabeled data and manually labeled data from [21], [28].
A. Experimental Setup
1) Dataset: We crawl a TVD corpus from the NVD web-
site. The corpus includes 162,622 NVD entries from 1999-
2021. We remove a small number of NVD entries that donot reference CVE-ID, as they may not represent securityvulnerabilities. We keep the textual content of each NVD entryas TVDs but remove reference code fragments (if any). Wealso remove “REJECT”, “DISPUTED”, and “RESERVED”NVD entries, as they may create confusion because theirdetails vary accordingly. After text processing, we obtain a textcorpus consisting of 152,439 NVD entries, 275,344 sentenceswith in total 6,167,497 tokens. From this corpus, we obtainabout 2,372,717 terminal noun phrases. If the TVD does notcontain at least one VPV phrase, we discard this TVD.
We obtain a total of 1,107,333 non-VPV noun phrases.
From these non-VPV noun phrases, we collect unique phraseswithin the sentences followed by distinct phrases across thesentence. We obtain 388,204 distinct non-VPV noun phrasesacross the corpus. For each distinct non-VPV phrase, wecalculate its absolute path and relative paths and then countpath frequencies in the corpus. The path frequencies follow along-tail distribution, with about 50% of paths (205,576 paths)occurring 2 or more times. Note that a recurring path doesnot mean the phrases represented by the path are all the same(see Table II for examples). In this work, we consider the pathoccurring only once as accidental and do not consider themin unsupervised path representation learning. In this work,the vocabulary size 5000 will cover all recurring paths inour corpus. Therefore, we set the upper bound of absolute-path vocabulary and relative-path vocabulary to 5000, andexperiment with the vocabulary size 500, 1000, 3000, and5000. These vocabulary sizes cover 25,697, 77,079, 128,485and 205,576 non-VPV phrases respectively.
2) Model conﬁgurations: Our CaV AE is implemented on
TensorFlow [29] and Keras [30]. The input to CaV AE is a5*5000-dimensional matrix where the ﬁrst row is the one-hotvector of the absolute path, and the rest four rows are theone-hot vectors of relative paths. In our dataset, a non-VPVphrase has at most 4 relative paths. Zero-padding is applied forsmaller vocabulary sizes and if the non-VPV phrase has lessthan 4 relative paths. The encoder and decoder networks areconﬁgured with 3 fully-connected layers, each of which has800 neurons. We set the network structure as 5000-200-50 forthe encoder (reversed for the decoder). That is, the latent spaceis 50-dimensional continuous vector. The activation functionis ReLUs [31]. During training, we update model parametersusing a Gumbel-Softmax [17] back-propagation algorithm.
In all cases, the input data is partitioned into training
(70%), validation (10%), and test dataset (20%) our models
948T ABLE III
RESUL TS OF LOSS FUNCTION :MODEL LOSSES AND V ALIDA TION LOSS IN
EACH ITERA TION .
Steps (Iteration) Temperature (τ ) Model Loss Validation Loss
1 0.981 269.9 282.13
5,000 0.773 60.6 93.4
10,000 0.563 61.9 94.8
15,000 0.544 59.6 93.0
20,000 0.5 75.9 89.88
Fig. 4. Training Curves of Reconstruction Loss
are optimized using ADAM [32] with an initial learning rate
set to 0.02 and decay every 10 epoch by a factor of 0.9. Wechoose the model parameters that show the best accuracy onthe validation set, and we report the score on the test set. Agrid search is employed to select appropriate hyperparametervalues. In all cases, a 10-fold cross-validation methodology isapplied. Then, the 10-fold results are averaged and used toselect the best models and variants for comparison. We usethe default hyperparameters of FIt-TSNE and DBSCAN.
B. CaVAE Robustness (RQ1)
1) Motivation: The encoder of CaV AE learns to embed
absolute paths and relative paths into the latent space. The
decoder reconstructs the input path using the latent featurevector from the latent space. The quality of the reconstructiondepends on the quality of the encoded features in the latentspace. Therefore, we measure the reconstruction quality toevaluate the quality of latent space projection.
2) Approach: We estimate the reconstruction loss to mea-
sure how much information is lost during the decoder’s recon-struction phase. We report the log-likelihood (reconstructionloss) to measure the CaV AE model performance and thequality of the encoded features in the latent space. The lowerthe reconstruction loss of the model, the higher the ﬁdelity oflatent space projection.
3) Result: Table III and Fig. 4 show the values of loss
varying with the number of iterations during the trainingprocess of the CaV AE model on the absolute and relativepaths. The red line represents model loss, and the blue linerepresents a validation loss in Fig. 4 (right). Overall, when themodel is iterated to nearly 5,000 steps, the reconstruction erroris optimized to be small. As the training process continues to20,000, the model loss drops rapidly from 269.9 to 75.9, andvalidation loss from 282.13 to 89.88. When the model losserror decreases gradually, we can see that the reconstructionerror increases a little. With further training of the model,the reconstruction error is optimized to a stable and optimalstate in the end. A signiﬁcant loss reduction is observed,indicating that our model addresses how two different prob-ability distributions are minimized. The result shows that theproposed CaV AE achieves a small (89.88) log-likelihood onthe validation set, which shows the effectiveness of our modelto improve the quality of latent space clustering.
Fig. 4 (left) shows that the temperature parameter τgrad-
ually decreases from 0.981 to 0.5. This result allows usto control how closely samples from the Gumbel-Softmaxdistribution [22], [23] approximate data points from the cate-gorical distribution. As the softmax temperature τapproaches
0, samples from the Gumbel Softmax distribution becomeone-hot, and the Gumbel-Softmax becomes the categoricaldistribution. The result shows that the temperature τgradually
decreases to be 0.5 and continues to optimize to a stable state.
Our results indicate that CaV AE are robust in both
learning syntactic structures of absolute and relative path-s and dimension-reduction, thus justifying the use ofCaV AE for the subsequent latent space clustering.
C. The Quality of Resulting Concept Clusters (RQ2)
1) Motivation: Our unsupervised labeling approach is
based on the hypothesis that syntactic relationships betweenVPV phrases and non-VPV phrases help to determine the con-cept type of non-VPV phrases. We want to conﬁrm the validityof our hypothesis. In our approach, discrete syntactic pathsare projected into a latent space. Therefore, if our assumptionholds, non-VPV phrases with similar paths should be closein the latent space. Therefore, we validate our hypothesis byexamining the accuracy of non-VPV phrases in the resultingclusters through both data visualization and manual checking.
2) Approach: First, we visualize the latent space with FIt-
TSNE [25] (i.e., CaVAE →FIt−TSNE ). FIt-TSNE
shows the resulting clusters in a 2-dimensional map. We alsoapply DBSCAN directly on the latent space (CaVAE →
DBSCAN ) and on the 2-dimensional map (CaVAE →
FIt−TSNE→DBSCAN ), and visualize the resulting
clusters again on a 2-dimensional map. We would expect tosee four major clusters, which would correspond to root cause,attack vector, impact, and others, respectively.
Next, we sample the non-VPV phrases corresponding to
the paths in the resulting clusters and manually examine ifthe concept type of the sampled non-VPV phrases matchesthe cluster label given by the human. We perform this manualchecking for all three clustering settings. As we have a largenumber of TVDs and phrases to examine, we adopt a statisticalsampling method [33]. In particular, we examine the minimumnumber (MIN) of data instances for each cluster to ensure
949Fig. 5. Visualization of Resulting Clusters: a) CaVAE −>FIt−TSNE ;
(b)CaVAE −>FI t−TSNE−>D B S C A N , and (c) CaVAE −>
DBSCAN, respectively.
that the estimated accuracy is in a 5% error margin at a 95%
conﬁdence level.
We hire two annotators, one from our research group (not
involved in this study) and one from a local security company,to annotate the concept type of each sampled non-VPV phrase.We request participants to use four concept labels: root cause,attack vector, impact, or others. The two annotators ﬁrst inde-pendently evaluate the label accuracy of the sampled phrases(a binary decision). Then, we compute Cohens Kappa [34]to evaluate the inter-rater agreement. When two annotatorsdisagree on the labels, they discuss to make a ﬁnal judgment.Based on the ﬁnal labels, we compute the accuracy of non-VPV phrases of each cluster.
3) Results: Fig. 5 shows the 2-dimensional visualization
of the clusters obtained in the three experimental settings.Although the shapes and locations of the resulting clustersare different in different settings, we can clearly observe fourmajor clusters with a very small percentage of outliers inall three settings. In the visualization, red points correspondto root cause, rose points to impact, yellow points to attackvector, and green points to other types. The clusters repre-senting root cause and impact are compacted. The clustersrepresenting attack vectors and other types, albeit more sparse,are still well distinguishable. The black points are outliers. Wedesign the CaV AE to be robust in noise ﬁltering and structurallearning; noise phrases are further identiﬁed with DBSCANclustering. Any phrase that is randomly selected by Rahmahsalgorithm [27] is not found to be a core point of the clusters,or a borderline phrase is classiﬁed as an outlier phrase and isnot assigned to any cluster. These outliers are removed fromthe training data we provided.
Table IV reports the label accuracy of non-VPV phrases in
the four clusters. The column MIN is the number of phraseswe randomly sample and examine from each distinct clusteraccording to [33]. The columns AA1 and AA2 show the accu-racy results determined by the two annotators independently,and the column AF is the ﬁnal accuracy after resolving theirdisagreements. The Cohen’s kappa metrics for each type ofconcept between the two annotators are all >0.78, which
indicates a reasonable inter-rater agreement. We observe thatthe disagreement is due to the attack vector class sometimeshas similar structures with impact class. The concept type withthe highest accuracy is selected to be the label of a cluster.
According to the manual annotation, the majority of non-
VPV phrases in each of the four major clusters indeed corre-T ABLE IV
THE CONCEPT -TYPE ACCURACY OF RESUL TING CLUSTERS
Approaches Type MIN AA1 AA1 AF
CaV AE −>F I t −TSNEAttack V ector 2411 81.3% 76.7% 79%
Impact 2660 80.6% 79.4% 80%
Root Cause 2113 86% 79% 82.5%
Others 1000 85.5% 75.5% 80.5%
CaV AE −>F I t −SNE− >D B S C A NAttack V ector 3350 75% 91% 83%
Impact 4965 85% 87.6% 86.3%
Root Cause 3602 87% 87% 87%
Others 1908 85% 83% 84%
CaV AE −>D B S C A NAttack V ector 2391 81% 82.4% 81.7%
Impact 2925 80% 86% 83%
Root Cause 2675 86% 86.9% 86.45%
Others 1008 83% 93% 88%
T ABLE V
ACCURACY OF MODEL VARIANTS
Variant Model Deign Accuracy
CaV AE (absolute path and relative paths) 85.07%
CaV AE-AP (only absolute path) 83%
CaV AE-WE (word embeddings of phrases as input) 57%
TrV AE (not designed to handle categorical data) 76%%
spond to one of the four concept types, respectively. We seehigh label accuracies (79%-88%) for all four clusters in allexperimental settings. The accuracies in the three experimentalsettings are close. Applying DBSCAN to the latent space orthe FIt-TSNE’s 2-dimensional space can improve the label ac-curacies slightly. CaVAE−>FIt−TSNE−>D BSCA N
is overall the best setting, which achieves 83%, 86.3%, 87%and 84% accuracy for attack vector, impact, root cause, andothers, respectively.
Our cluster visualization and manual checking of non-
VPV phrases in the resulting clusters conﬁrm the effec-tiveness of unsupervised labeling method. This conﬁrmsthe validity of our hypothesis that syntactically similarnon-VPV phrases (in terms of absolute paths and relativepaths) usually represent the same type of vulnerabilityconcepts.
D. Comparison of V ariant Model Designs (RQ3)
1) Motivation: Our model design makes several special
considerations: 1) encode syntactic relationships between VPVphrases and non-VPV phrases; 2) use path representation toabstract away differences of sentence structures and phraseexpressions; 3) enhance V AE with Gumbel-Softmax to handlecategorical path representation. We want to investigate howthese special model design considerations affect the perfor-mance of clustering same-type of non-VPV phrases.
2) Approach: We create three model variants: 1) CaV AE-
AP , which is trained only with absolute paths, but not relativepaths of non-VPV phrases; 2) CaV AE-WE, which takes asinput of the pretrained word2vec [35] word embeddings ofnon-VPV phrases, rather than their path representations; 3)TrV AE, which is a traditional V AE that is not designed to
950T ABLE VI
COMP ARISONS WITH CONCEPT CLASSIFICA TION METHODS
Dataset Prec. Recall F1
Labeling Joshi et al. [21] 89.18% 87.66% 89.16%
Labeling Bridges et al. [28] 95.8% 90.73% 92.51%
Our unsupervised labeling 97.76% 95.68% 97.2%
handle categorical data. We perform phrase clustering by
CaVAE−>F I t−TSNE−>D B S C A N and label the
resulting four clusters with the four concept types. We usethe manually annotated non-VPV phrases from RQ2 as theground truth to evaluate the clustering accuracy. Due to thespace limitation, we report only the overall accuracy for thefour concept types (i.e., root cause, attack vector, impact, andothers) as a whole.
3) Result: Table V presents the accuracy results. Com-
pared with the accuracy of the original CaV AE (85.07%),the accuracies of all the three model variants degrade, inparticular CaV AE-AP (83%), CaV AE-WE (57%) and TrA VE(76%). CaV AE-WE has the largest accuracy drop, whichindicates that representing phrases in word embeddings isnot appropriate for unsupervised representation learning. Thiscould be because word embeddings contain too much syntacticand semantic information, which is irrelevant and hard to re-construct by auto-encoder. Furthermore, word embeddings arealready dense continuous vectors on which further dimensionreduction could be difﬁcult. In contrast, our path representationabstracts away detailed differences at both sentence and phraselevels, which allows path patterns to emerge. As such, itbecomes easy for auto-encoder to embed and reconstruct pathpatterns.
However, the performance drop of TrA VE indicates that just
the auto-encoder is still not enough to effectively embed pathpatterns due to its categorical nature. Therefore, our Gumbel-Softmax enhancement is necessary. Finally, without consider-ing relative paths, CaV AE-AP still performs reasonably well.As shown in Table II, absolute paths themselves can capturemany regularities among non-VPV phrases and thus have goodpredictive power for the concept type of non-VPV phrases.However, taking into account relative paths between VPVphrases and non-VPV phrases can further boost the modelperformance. This shows that absolute paths and relative pathsrepresent complementary features.
Our results suggest that all the three special model
design considerations contribute to boost our model per-formance.
E. Usefulness of Unsupervised Labeled Data (RQ4)
1) Motivation: Our approach provides a novel unsupervised
way to label and extract key vulnerability phrases (e.g., rootcause, attack vector, and impact) from TVDs. After conﬁrmingthe high accuracy of our unsupervised information extractionapproach, we want to further investigate how well the phrasesobtained through our unsupervised labeling may be used astraining data for machine learning techniques, as opposed tomanually labeled concept phrases in TVDs.
2) Approach: Closest to our work, the two previous stud-
ies [21], [28] also investigate concept labeling and extraction inTVDs. Different from our work, they manually label conceptsin TVDs and thus produce two manually labeled datasets ofvulnerability concepts, referred to as the Bridges dataset [28]and the Joshi dataset [21]. In Joshi’s work, they label vul-nerability, means, and consequence, which correspond to rootcause, attack vector, and impact in our terminology. Bridgesmanually created a gazetteer of “vulnerability relevant terms”and extracted short phrases that give pertinent informationabout a vulnerability. The Bridges dataset consists of ∼5187
TVDs labeled with “vulnerability relevant term”, which match-es the union of “attack”, “means”, and “consequences” labelsof Joshi. The concept type of “vulnerability relevant term” forthese categories is ambiguous and too general to use; hence,to suit the concept types in our terminology, we manuallynormalize the “vulnerability relevant term” labels to rootcause, attack vector, and impact. The Joshi dataset consistsof 240 CVE/TVDs (∼ 50,000 tokens) labeled with “attack”,
“means”, and “consequences”, which are equivalent to rootcause, attack vector, and impact phrases in our approach.
For a fair comparison, we use only the TVDs labeled in
these three labels of the two datasets. When comparing withBridges et al. [28] or Joshi et al. [21], we use only the TVDsin their respective dataset. We use our approach to label theroot cause, attack vector, and impact phrases in these TVDs,and obtain 2,554, 1,996, and 1,133 root cause, attack vectorand impact phrases for the Bridges dataset and 205, 167,and 151 root cause, attack vector and impact phrases for theJoshi dataset. In this work, we consider a simple conceptclassiﬁcation task that predicts the concept type for an inputphrase. We use the Random Forest classiﬁer [36], and trainthe classiﬁer with the phrases from the Bridges dataset, theJoshi dataset, and our unsupervised labels, respectively. Wecompare prediction precision, recall, and F1. We perform 10-fold cross-validation and report average performance metrics.
3) Results: Table VI presents the classiﬁcation results.
The classiﬁer trained with the phrases extracted through ourunsupervised labeling achieves the best precision (97.7%),recall (95.6%), and F1 (97.2%), followed by the classiﬁertrained with the Bridges dataset, and then the classiﬁer trainedwith the Joshi dataset. By comparing the phrases in the threedatasets, we ﬁnd that the consistent boundary and typing byour method is important for correct classiﬁcation. In contrast,human labeling tends to be more inconsistent. For example, inour method, “a denial of service (application crash),” “a denialof service,” and “application crash” are labeled as impacts, andtheir boundaries are consistent with their structural similarity.In contrast, the human labeling identiﬁed only “a denial ofservice” as impact ignoring other structurally similar phrases.
951T ABLE VII
ERROR ANAL YSIS :EACH CLUSTER SUBCA TEGORIES ARE LABELED AS
ROOT CAUSE (R), A TT ACK VECTOR (A), IMP ACT (I) AND OTHERS (O)
RESPECTIVEL Y .
Cluster 1 Cluster 2 Cluster 3 Cluster 4
RAI O RA I O RA I O RAI O
81.1% 10% 6.5% 2.4% 5% 77.6% 11.4% 6% 8% 14.6% 74.4% 3% 10.1% 7.7% 7.2% 75%
The vulnerability phrases in TVDs extracted by our
unsupervised labeling method constitute a high-quality
dataset for the training concept classiﬁer. Due to the con-sistent boundary and typing by our information extractionmethod, the classiﬁer trained with our extracted phrasesoutperforms the classiﬁer trained with manually labeledphrases.
F . Threats to V alidity
1) Error Analysis: Despite having the best results of the ex-
periments, our approach generates some incorrectly classiﬁedconcept phrases, which is discussed in this section.
Our approach processes unstructured TVDs, which are full
of software-speciﬁc names, number sequences, and domain-speciﬁc terms (see Fig. 1). However, our approach is not sig-niﬁcantly affected. We believe that we minimize these issuesby using vulnerability-speciﬁc tokenizer and POS tagger [14].Our path representation and auto-encoding for noise reductionfurther minimize the problems of domain-speciﬁc terms.
Table VII shows the breakdown of phrase labels in the four
clusters by CaVAE−>FIt−TSNE−>D BSCA N. Our
manual analysis shows that the incorrectly labeled concepts aremostly from the attack vector class, which rarely mixes withthe phrases from the impact class. For example, the phrase“service application crash via images” (ROOT -S-VP-PP-S-VP-NP-SBAR-S-VP-S-VP-VP-NP-PP-NP) is a mix of impactand attack vector phrases. The phrase “service applicationcrash” (ROOT -S-VP-PP-S-VP-NP-SBAR-S-VP-S-VP-VP-NP-PP-NP-NP) is an impact, whereas “via image” (ROOT -S-VP-PP-S-VP-NP-SBAR-S-VP-S-VP-VP-NP-PP-NP-PP-NP) is theattack vector. The model is confused to correctly cluster eitherof them. As can be seen from the long path, the two phrasesdiffer with PP-NP . The error is due to our CaV AE learns thecommon structures of paths and places them close to eachother in the latent space. However, the phrase boundary isdeﬁned by the parser in the early phases. Although the CFGparser has reasonable assumptions for grammatically incorrectsentences, it is sometimes hard to parse TVD sentences dueto domain-speciﬁc vocabularies and jargon.
To investigate the level of errors, we randomly select 891
noun phrases from each cluster from the labeled clusteringresults and re-annotated them from scratch. We observe thatin each human-generated concept cluster, 81.1% root cause,77.6% attack vector, 74.4% impact are correctly labeled,which is close to the cluster quality by our unsupervisedlabeling. This shows that it is a signiﬁcant achievement for ourapproach, which conﬁrms path representation and latent spaceclustering are effective for vulnerability concept labeling.
2) Internal V alidity: In our approach, the ﬁrst threat to
internal validity may begin when vulnerabilities are reportedto the vulnerability databases and cybersecurity blogs byonline users (contributors), i.e., the natural characteristics ofthese online data feeds are noisy. We cannot avoid mistakesdue to human characterizing at the source data reporting,which may affect our approach in recognizing vulnerability-relevant concepts. The popular vulnerability databases suchas NVD have analysts who manually ﬁx such errors beforevulnerability reports are publicly accessible in their repository.In the future, we will extend our approach to minimize thethreat related to more noisy vulnerability information fromonline contributors.
The other threat to internal validity is that of manually
labeling. Concept phrases are challenging to determine in somecases. We invite 2 annotators to manually labeling concepts.This document is regarded as ground truth data to evaluatethe quality of clusters. We obtain the agreement between theannotators to be over 85% for the root cause of vulnerabilities.However, for attack vector and impact, the agreement is 81%,while for attack vector, it is 75%, indicating that there aredisagreements for annotating attack vector. We ask the twoannotators to discuss and resolve their disagreements.
3) External validity: It is concerned with whether the
experimental results can be generalized to other datasets. Tomaximize the validity to some extent, we crawl TVD fromNVD and CVE databases. Although these vulnerability reportscontain 6,167,497 tokens, other vulnerability databases suchas OSVDB, IBMX-Force, are not included in this study dueto the signiﬁcant effort needed to validate the cluster quality.Still, the general structure of TVD and reporting formats aremore or less similar due to the universal characteristics ofvulnerabilities. However, there may be intrinsic differences incharacterizing vulnerabilities in other vulnerability database,which deserve further investigation.
IV . R
ELA TED WORK
Recently, neural network-based named entity recognition
(NER) models achieve great success in the newswire domainand related artifacts [37]; however, they suffer from sharpperformance decline as the input moves to TVD [13], [20],[38]–[40]. The main reason is that TVD often contains textcomposed of mixed language studded with codes and domain-speciﬁc vocabularies. These models do not use TVD trainingdata, which results in the trained neural models’ lack of do-main knowledge to identify the vulnerability-relevant conceptscritical for security analyses [28].
To tackle this problem, neural models have heavily relied on
human annotations and hand-crafted features [41]. For exam-ple, a CRF-based system [21], [28], [42] and LSTM-basedneural network models [43]–[46] are proposed to identifycybersecurity-related entities and concepts. However, trainingneural models for concept extraction requires a large amountof manually labeling training data; obtaining large-labeled
952datasets is particularly challenging in speciﬁc domains, such
as TVD, where human annotations are expensive and time-consuming. One way of minimizing human annotation is touse external knowledge bases, which are limited to particularentities (e.g., CPE for vulnerable products and product ven-dors). Many other vulnerability concepts are phrases such asroot cause, attack vector, and impact could not have a matchingentity by an external knowledge base.
In the cybersecurity domain, there has been growing con-
cern in developing entity and concept extraction methods.More et al. [39] attempt to annotate entities in the CVE de-scriptions using ”off-the-shelf” NER tools. Mulwad et al. [47]extend his idea by proposing a prototype system using anSVM classiﬁer. Joshi et al. [21] proposed a framework toextract cybersecurity-relevant entities from the NVD. Unlikeour approach, their approach involves hand-annotating a smallcorpus fed into the Stanford NER’s for training a CRF entityextractor. Bridges et al. [28] implemented a MEM trained withthe average perceptron algorithm, which has been proven to bebetter than [48]. In their work, cybersecurity relevant concepts,such as root cause, attack vector, and impact, are manuallylabeled as “vulnerability relevant term”, which is ambiguousand too general to be of use. Jones et al. [49] implementeda bootstrapping algorithm to extract security entities from thetext. However, their approach works well for small corpus.McNeil et al. [50] proposed a bootstrapping algorithm thatlearns heuristics to extract information about exploits andvulnerabilities. However, these models over-ﬁt to the trainingdata [20]; that is, hard to generalize to unseen data.
In contrast, this paper proposes a new mechanism, i.e.,
the use of unsupervised auto-encoding and clustering forlabeling phrase-based concepts. The V AE [15], [16] is provenin learning representations in an unsupervised manner, andwe use their principles. However, our input data are (1) high-dimensional, sparse and categorical, and (2) back-propagationis impossible as traditional V AE works only for continuousdata. Therefore, we use Gumblemax [17] to back-propagatingvia discrete variables. Recently, FIt-SNE [25], the recentimplementation of tSNE [51] received much attention as it canaddress high volumes of data and reveal meaningful clusteringstructure. We use the advantage of this algorithm to visualizeand latent space clustering, which DBSCAN then exploits.Compared to previous works, we provide a pipeline of pre-trained models designed to automatically label TVDs fromonline data sources in an unsupervised way.
V. C
ONCLUSION
In this paper, we present an unsupervised method for
labeling phrases-based concepts (root cause, attack vector,and impact) in textual vulnerability descriptions that serveas training data for downstream machine learning tasks. Theproposed approach uses path representations from the sentenceparsing tree to abstract aways ﬁne-grained differences insentence structures and phrase expressions. It learns featurerepresentations through Categorical V ariational Auto-encoder(CaV AE) and uses encoded features in the latent space toinitialize the clustering of the same-type of phrases. Ourapproach can deal with the mixed texts and linguisticallymalformed phrases. Our evaluation shows that our approachcorrectly labels a broad set of vulnerability-relevant phrase-based concepts. We also provide initial evidence of the useful-ness of the labels obtained for downstream machine learningtasks. The ﬁndings from this work, including the publishedresources, such as labeled corpus and trained models, willbe at the forefront of future studies, which will add insightinto the security domain and support automated processingand research on fast-growing vulnerability information. Inthe future, we will introduce additional concept categoriesand extend our work for identifying concepts from softwareengineering and cybersecurity discussion forums.
A
CKNOWLEDGMENTS
This work is supported by the National Natural Science
Foundation of China (NSFC) (61972455, 61832014) and theJoint Project of Bayescom. Xiaowang Zhang is supported bythe program of Peiyang Y oung Scholars in Tianjin University(2019XRX-0032).
R
EFERENCES
[1] C. MITRE, “National vulnerability database (nvd)[ol], https://nvd.nist.
gov/,” https://nvd.nist.gov/, 2017, [Online; accessed 21-January-2017].
[2] IBM, “Ibm x-force exchange[ol], https://exchange.xforce.ibmcloud.
com/,” https://exchange.xforce.ibmcloud.com/, 2019, [Online; accessed
30-June-2019].
[3] O. Security, “Exploit database[ol], https://www.exploit-db.com/,” https:
//www.exploit-db.com/, 2019, [Online; accessed 30-June-2019].
[4] H. Gasmi, J. Laval, and A. Bouras, “Information extraction of cyber-
security concepts: An lstm approach,” Applied Sciences, vol. 9, no. 19,
2019.
[5] S. S. Weerawardhana, S. Mukherjee, I. Ray, and A. E. Howe, “Automat-
ed extraction of vulnerability information for home computer security,”inF oundations and Practice of Security - 7th International Symposium,
FPS 2014, Montreal, QC, Canada, Nov., 2014, pp. 356–366.
[6] T. D. G ¨unes, L. Tran-Thanh, and T. J. Norman, “Identifying vulnera-
bilities in trust and reputation systems,” in Proceedings of the Twenty-
Eighth International Joint Conference on Artiﬁcial Intelligence, IJCAI2019, Macao, China, Aug., 2019, pp. 308–314.
[7] H. Chen, J. Liu, R. Liu, N. Park, and V . S. Subrahmanian, “VEST: A
system for vulnerability exploit scoring & timing,” in Proceedings of the
Twenty-Eighth International Joint Conference on Artiﬁcial Intelligence,IJCAI 2019, Macao, China, Aug, 2019, pp. 6503–6505.
[8] S. Mittal, P . K. Das, V . Mulwad, A. Joshi, and T. Finin, “Cybertwitter:
Using twitter to generate alerts for cybersecurity threats and vulnera-bilities,” in 2016 IEEE/ACM International Conference on Advances in
Social Networks Analysis and Mining, ASONAM 2016, San Francisco,CA, USA, Aug., 2016, pp. 860–867.
[9] L. Neil, S. Mittal, and A. Joshi, “Mining threat intelligence about
open-source projects and libraries from code repository issues and bugreports,” in 2018 IEEE International Conference on Intelligence and
Security Informatics, ISI 2018, Miami, FL, USA, Nov., 2018, pp. 7–12.
[10] S. R. V adapalli, G. Hsieh, and K. S. Nauer, “Twitterosint: automated
cybersecurity threat intelligence collection and analysis using twitterdata,” in Proceedings of the International Conference on Security and
Management (SAM), 2018, pp. 220–226.
[11] A. Pingle, A. Piplai, S. Mittal, A. Joshi, J. Holt, and R. Zak, “Relext:
relation extraction using deep learning approaches for cybersecurityknowledge graph improvement,” in ASONAM ’19: International Confer-
ence on Advances in Social Networks Analysis and Mining, V ancouver ,British Columbia, Canada, 27-30 Aug., 2019, pp. 879–886.
[12] Y . Dong, W . Guo, Y . Chen, X. Xing, Y . Zhang, and G. Wang, “Towards
the detection of inconsistencies in public security vulnerability reports,”in28th USENIX Security Symposium, USENIX Security 2019, Santa
Clara, CA, USA, August 14-16. USENIX Association, 2019, pp. 869–885.
953[13] S. K. Lim, A. O. Muis, W . Lu, and O. C. Hui, “Malwaretextdb: A
database for annotated malware articles,” in Proceedings of the 55th
Annual Meeting of the Association for Computational Linguistics, ACL
2017, V ancouver , Canada, July 30 - August 4, V olume 1: Long Papers,2017, pp. 1557–1567.
[14] S. Yitagesu, X. Zhang, Z. Feng, X. Li, and Z. Xing, “Automatic
part-of-speech tagging for security vulnerability descriptions,” in 18th
IEEE/ACM International Conference on Mining Software Repositories,MSR 2021, Madrid, Spain, May 17-19, 2021, pp. 29–40.
[15] D. P . Kingma and M. Welling, “Auto-encoding variational bayes,” in
2nd International Conference on Learning Representations, ICLR 2014,Banff, AB, Canada, Apr ., 2014.
[16] D. J. Rezende, S. Mohamed, and D. Wierstra, “Stochastic backprop-
agation and approximate inference in deep generative models,” inProceedings of the 31th International Conference on Machine Learning,ICML 2014, Beijing, China, Jun., 2014, pp. 1278–1286.
[17] E. Jang, S. Gu, and B. Poole, “Categorical reparameterization with
gumbel-softmax,” in 5th International Conference on Learning Repre-
sentations, ICLR 2017, Toulon, France, April 24-26, 2017.
[18] C. MITRE, “Common vulnerabilities and exposures (cve)[ol], https://
cve.mitre.org/,” https://cve.mitre.org/, 2019, [Online; accessed 30-June-2019].
[19] D. Klein and C. D. Manning, “Accurate unlexicalized parsing,” in
Proceedings of the 41st Annual Meeting of the Association for Computa-tional Linguistics, 7-12 July 2003, Sapporo Convention Center , Sapporo,Japan. ACL, 2003, pp. 423–430.
[20] R. A. Bridges, K. M. T. Huffer, C. L. Jones, M. D. Iannacone, and J. R.
Goodall, “Cybersecurity automated information extraction techniques:Drawbacks of current methods, and enhanced extractors,” in 16th
IEEE International Conference on Machine Learning and Applications,ICMLA 2017, Cancun, Mexico, Dec., 2017, pp. 437–442.
[21] A. Joshi, R. Lal, T. Finin, and A. Joshi, “Extracting cybersecurity related
linked data from text,” in 2013 IEEE Seventh International Conference
on Semantic Computing, Irvine, CA, USA, Sep., 2013, pp. 252–259.
[22] E. J. Gumbel, Statistical theory of extreme values and some practical
applications: a series of lectures. US Government Printing Ofﬁce,1954, vol. 33.
[23] C. J. Maddison, A. Mnih, and Y . W . Teh, “The concrete distribution: A
continuous relaxation of discrete random variables,” in 5th International
Conference on Learning Representations, ICLR 2017, Toulon, France,April 24-26, 2017.
[24] C. K. Sønderby, T. Raiko, L. Maaløe, S. K. Sønderby, and O. Winther,
“How to train deep variational autoencoders and probabilistic laddernetworks,” CoRR, vol. abs/1602.02282, 2016.
[25] G. C. Linderman, M. Rachh, J. G. Hoskins, S. Steinerberger, and
Y . Kluger, “Fast interpolation-based t-sne for improved visualization ofsingle-cell rna-seq data,” Nature methods, vol. 16, no. 3, pp. 243–245,
2019.
[26] M. Ester, H. Kriegel, J. Sander, and X. Xu, “A density-based algo-
rithm for discovering clusters in large spatial databases with noise,”inProceedings of the Second International Conference on Knowledge
Discovery and Data Mining (KDD-96), Portland, Oregon, USA, 1996,pp. 226–231.
[27] N. Rahmah and I. S. Sitanggang, “Determination of optimal epsilon (eps)
value on DBSCAN algorithm to clustering data on peatland hotspots insumatra,” IOP Conference Series: Earth and Environmental Science,
vol. 31, p. 012012, jan 2016.
[28] R. A. Bridges, C. L. Jones, M. D. Iannacone, and J. R. Goodall,
“Automatic labeling for entity extraction in cyber security,” CoRR, vol.
abs/1308.4941, 2013.
[29] M. Abadi, P . Barham, J. Chen, Z. Chen, A. Davis, and J. Dean, “Ten-
sorﬂow: A system for large-scale machine learning,” in 12th USENIX
Symposium on Operating Systems Design and Implementation, OSDI2016, Savannah, GA, USA, Nov., 2016, pp. 265–283.
[30] F. Chollet et al. (2015) Keras. [Online]. Available: https://keras.io
[31] V . Nair and G. E. Hinton, “Rectiﬁed linear units improve restricted boltz-
mann machines,” in Proceedings of the 27th International Conference on
Machine Learning (ICML-10), Haifa, Israel, Jun., 2010, pp. 807–814.
[32] D. P . Kingma and J. Ba, “Adam: A method for stochastic optimization,”
in3rd International Conference on Learning Representations, ICLR
2015, San Diego, CA, USA, May, 2015.
[33] R. Singh and N. S. Mangat, Elements of survey sampling. Springer
Science & Business Media, 2013, vol. 15.[34] J. R. Landis and G. G. Koch, “An application of hierarchical kappa-
type statistics in the assessment of majority agreement among multipleobservers,” Biometrics, pp. 363–374, 1977.
[35] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean,
“Distributed representations of words and phrases and their composition-ality,” in Advances in Neural Information Processing Systems 26: 27th
Annual Conference on Neural Information Processing Systems 2013,Lake Tahoe, Nevada, United States, Dec., 2013, pp. 3111–3119.
[36] L. Breiman, “Random forests,” Mach. Learn., vol. 45, no. 1, pp. 5–32,
2001.
[37] J. Devlin, M. Chang, K. Lee, and K. Toutanova, “BERT: pre-training
of deep bidirectional transformers for language understanding,” inProceedings of the 2019 Conference of the North American Chapterof the Association for Computational Linguistics: Human LanguageTechnologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7,2019, pp. 4171–4186.
[38] Z. Xiao, “Towards a two-phase unsupervised system for cybersecurity
concepts extraction,” in 13th International Conference on Natural Com-
putation, Fuzzy Systems and Knowledge Discovery, ICNC-FSKD 2017,Guilin, China, July 29-31, 2017, pp. 2161–2168.
[39] S. More, M. Matthews, A. Joshi, and T. Finin, “A knowledge-based
approach to intrusion detection modeling,” in 2012 IEEE Symposium on
Security and Privacy Workshops, San Francisco, CA, USA, May, 2012,pp. 75–81.
[40] X. Liao, K. Y uan, X. Wang, Z. Li, L. Xing, and R. A. Beyah, “Acing
the IOC game: Toward automatic discovery and analysis of open-sourcecyber threat intelligence,” in Proceedings of the 2016 ACM SIGSAC
Conference on Computer and Communications Security, Vienna, Austria,Oct., 2016, pp. 755–766.
[41] P . M. Vu, T. T. Nguyen, and T. T. Nguyen, “Alpaca: Advanced linguistic
pattern and concept analysis framework for software engineering corpo-ra,” in Proceedings of the 2019 ACM Southeast Conference, 2019, pp.
249–252.
[42] AtefehZafarian, A. Rokni, S. Khadivi, and S. Ghiasifard, “Semi-
supervised learning for named entity recognition using weakly labeledtraining data,” in 2015 the international symposium on artiﬁcial intelli-
gence and signal processing (AISP), 2015, pp. 129–135.
[43] G. Kim, C. Lee, J. Jo, and H. Lim, “Automatic extraction of named
entities of cyber threats using a deep bi-lstm-crf network,” Int. J. Mach.
Learn. Cybern., vol. 11, no. 10, pp. 2341–2355, 2020.
[44] Y . Qin, G. Shen, W . Zhao, Y . Chen, M. Y u, and X. Jin, “A network
security entity recognition method based on feature template and cnn-bilstm-crf,” Frontiers Inf. Technol. Electron. Eng., vol. 20, no. 6, pp.
872–884, 2019.
[45] S. K, S. S, V . R, and K. P . Soman, “Deep learning approach for
intelligent named entity recognition of cyber security,” CoRR, vol.
abs/2004.00502, 2020.
[46] S. Y agcioglu, M. S. Seyﬁoglu, B. Citamak, B. Bardak, S. Guldamla-
sioglu, A. Y uksel, and E. I. Tatli, “Detecting cybersecurity events fromnoisy short text,” CoRR, vol. abs/1904.05054, 2019.
[47] V . Mulwad, W . Li, A. Joshi, T. Finin, and K. Viswanathan, “Extracting
information about security vulnerabilities from web text,” in Proceedings
of the 2011 IEEE/WIC/ACM International Joint Conference on WebIntelligence and Intelligent Agent Technology - Workshops, WI-IAT 2011,Campus Scientiﬁque de la Doua, Lyon, France, Aug., 2011, pp. 257–260.
[48] M. Collins, “Discriminative training methods for hidden markov models:
Theory and experiments with perceptron algorithms,” in Proceedings
of the 2002 Conference on Empirical Methods in Natural LanguageProcessing, EMNLP 2002, Philadelphia, PA, USA, Jul., 2002, pp. 1–8.
[49] C. L. Jones, R. A. Bridges, K. M. T. Huffer, and J. R. Goodall,
“Towards a relation extraction framework for cyber-security concepts,”inProceedings of the 10th Annual Cyber and Information Security
Research Conference, CISR ’15, Oak Ridge, TN, USA, Apr ., 2015, pp.11:1–11:4.
[50] N. McNeil, R. A. Bridges, M. D. Iannacone, B. D. Czejdo, N. Perez,
and J. R. Goodall, “P ACE: pattern accurate computationally efﬁcientbootstrapping for timely discovery of cyber-security concepts,” in 12th
International Conference on Machine Learning and Applications, ICM-LA 2013, Miami, FL, USA, Dec., V olume 2, 2013, pp. 60–65.
[51] L. van der Maaten and G. Hinton, “Visualizing data using t-sne,” Journal
ofMachine Learning Research, vol. 9, no. 86, pp. 2579–2605, 2008.
954