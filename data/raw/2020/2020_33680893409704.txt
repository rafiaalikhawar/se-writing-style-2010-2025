Do the MachineLearning Modelsona CrowdSourcedPlatform
Exhibit
Bias?An Empirical Study onModelFairness
Sumon Biswas
Dept. of ComputerScience, IowaStateUniversity
Ames, IA,USA
sumon@iastate.eduHridesh Rajan
Dept. of ComputerScience, IowaStateUniversity
Ames, IA,USA
hridesh@iastate.edu
ABSTRACT
Machine learning models are increasingly being used in important
decision-making software such as approving bank loans, recom-
mending criminal sentencing, hiring employees, and so on. It is
important to ensure the fairness of these models so that no dis-
criminationismadebasedon protectedattribute (e.g.,race,sex,age)
whiledecisionmaking.Algorithmshavebeendevelopedtomeasure
unfairness andmitigate them to a certain extent.In this paper, we
have focused on the empirical evaluation of fairness and mitiga-
tions on real-world machine learning models. We have created a
benchmark of 40 top-rated models from Kaggle used for 5 different
tasks, and then using a comprehensive set of fairness metrics, eval-
uatedtheirfairness.Then,wehaveapplied7mitigationtechniques
on these models and analyzed the fairness, mitigation results, and
impactsonperformance.Wehavefoundthatsomemodeloptimiza-
tion techniques result in inducing unfairness in the models. On the
otherhand,althoughtherearesomefairnesscontrolmechanisms
inmachinelearninglibraries,theyarenotdocumented.Themiti-
gation algorithm also exhibit common patterns suchas mitigation
inthepost-processingisoftencostly(intermsofperformance)and
mitigation in the pre-processing stage is preferred in most cases.
We have also presented different trade-off choices of fairness miti-
gationdecisions. Ourstudysuggestsfuture researchdirectionsto
reduce the gap between theoretical fairness aware algorithms and
the software engineeringmethodsto leverage themin practice.
CCS CONCEPTS
·Softwareanditsengineering →Softwarecreationandman-
agement;· Computingmethodologies →Machine learning.
KEYWORDS
fairness, machine learning,models
ACMReference Format:
Sumon Biswas and Hridesh Rajan. 2020. Do the Machine Learning Models
ona CrowdSourced PlatformExhibitBias?AnEmpirical Study onModel
Fairness.In Proceedingsofthe28thACMJointEuropeanSoftwareEngineer-
ingConferenceandSymposiumontheFoundationsofSoftwareEngineering
(ESEC/FSE’20),November8ś13,2020,VirtualEvent,USA. ACM,NewYork,
NY, USA, 12pages.https://doi.org/10.1145/3368089.3409704
ESEC/FSE ’20, November 8ś13, 2020, Virtual Event, USA
© 2020 Copyright held by the owner/author(s).
ACM ISBN 978-1-4503-7043-1/20/11.
https://doi.org/10.1145/3368089.34097041 INTRODUCTION
Sincemachinelearning(ML)modelsareincreasinglybeingusedin
making important decisions that affect human lives, it is important
to ensure that the prediction is not biased toward any protected
attribute such as race, sex, age, marital status, etc. ML fairness
has been studied for about past 10 years [ 16], and several fairness
metricsandmitigationtechniques[ 8,11,15,20,34,36,50,52,52]
have been proposed. Many testing strategies have been developed
[3,17,49] to detect unfairness in software systems. Recently, a few
toolshavebeenproposed[ 2,4,44,48]toenhancefairnessofML
classifiers. However,we are not aware howmuch fairnessissues
existinMLmodelsfrompractice.Dothemodelsexhibitbias?Ifyes,
whatarethedifferentbiastypesandwhatarethemodelconstructs
related to the bias? Also, is there a pattern of fairness measures
when different mitigation algorithms are applied? In this paper, we
have conducted an empirical study on ML models to understand
these characteristics.
Harrison etal.studiedhowMLmodelfairnessisperceivedby502
Mechanical Turk workers[ 21].Recently, Holstein et al.conducted
anempiricalstudyonMLfairnessbysurveyingandinterviewing
industrypractitioners [ 22]. Theyoutlined thechallengesfaced by
the developers and the support they need to build fair ML systems.
They also discussed that it is important to understand the fairness
ofexistingMLmodelsandimprovesoftwareengineeringtoachieve
fairness.Inthispaper,wehaveanalyzedthefairnessof40MLmod-
els collected from a crowd sourced platform, Kaggle, and answered
the following researchquestions.
RQ1: (Unfairness) What are the unfairness measures ofthe ML
models in the wild, and which of them are more or less prone to
bias?
RQ2: (Bias mitigation) What are the root causes of the bias in
ML models, and what kind of techniques can successfully mitigate
thosebias?
RQ3: (Impact) What are the impacts of applying different bias
mitigating techniques onML models?
First, we have created a benchmark of ML models collected
fromKaggle.Wehavemanuallyverifiedthemodelsandselected
appropriate ones for the analysis. Second, we have designed an
experimental setup to measure, achieve, and report fairness of
the ML models. Then we have analyzed the result to answer the
research questions. The key findings are: model optimization goals
are configured towards overall performance improvement, causing
unfairness.Afewmodelconstructsaredirectlyrelatedtofairnessof
themodel.However,MLlibrariesdonotexplicitlymentionfairness
in documentation. Models with effective pre-processing mitigation
algorithm are more reliable and pre-processing mitigations always
retain performance. We have also reported different patterns of
642This work is licensed under a Creative Commons Attribution International 4.0 License.
ESEC/FSE ’20, November8–13,2020,VirtualEvent, USA SumonBiswas andHrideshRajan
exhibiting bias and mitigating them. Finally, we have reported the
trade-offconcernsevident for thosemodels.
The paper is organized asfollows: ğ 2describesthebackground
and necessary terminology used in this paper. In ğ 3, we have de-
scribed the methodology of creating the benchmark and setting
upexperiment,anddiscussedthefairnessmetricsandmitigation
techniques. ğ 4describes the fairness comparison of the models,
ğ5describes the mitigation techniques, and ğ 9describes the im-
pacts of mitigation. We have discussed the threats to validity in ğ 7,
describedthe relatedwork inğ 8,andconcludedinğ 9.
2 BACKGROUND
The basic idea of ML fairness is that the model should not discrim-
inatebetweendifferentindividualsorgroupsfromtheprotected
attribute class [ 16,17].Protected attribute (e.g., race, sex, age, re-
ligion) is an input feature, which should not affect the decision
making of the models solely. Chen et al.listed 12 protected at-
tributes for fairness analysis [ 10]. One trivial idea is to remove the
protectedattributefromthedatasetandusethatastrainingdata.
Pedreshiet al.showed that due to the redundant encoding of train-
ingdata,itispossiblethatprotectedattributeispropagatedtoother
correlatedattributes[ 39]. Therefore,we needfairnessawarealgo-
rithmstoavoidbiasinMLmodels.Inthispaper,wehaveconsidered
both group fairness and individual fairness. Group fairness mea-
sureswhetherthemodelpredictiondiscriminatesbetweendifferent
groups in theprotected attributeclass(e.g., sex: male/female )[14].
Individualfairness measureswhethersimilarpredictionismadefor
similarindividualsthoseareonlydifferentinprotectedattribute
[14]. Based on different definitions of fairness, many group and
individualfairnessmetricshavebeenproposed.Additionally,many
fairness mitigation techniques have been developed to remove un-
fairnessorbiasfromthemodelprediction.Thefairnessmetricsand
mitigationtechniques have been describedinthe nextsection.
3 METHODOLOGY
Inthissection,first,wehavedescribedthemethodologytocreate
thebenchmarkofMLmodelsforfairnessanalysis.Thenwehave
described our experiment design and setup. Finally, we have dis-
cussedthefairnessmetricsweevaluatedandmitigationalgorithms
we appliedoneachmodel.
3.1 Benchmark Collection
We have collected ML models from Kaggle kernels [ 25]. Kaggle
is one of the most popular data science (DS) platform owned by
Google. Data scientists, researchers, and developers can host or
takepartinDScompetition,sharedataset,task,andsolution.Many
Kaggle solutions resulted in impactful ML algorithms and research
suchasneuralnetworksusedbyGeoffreyHintonandGeorgeDahl
[12],improvingthesearchfortheHiggsBosonatCERN[ 23],state-
of-the-art HIV research [ 9], etc. There are 376 competitions and
28,622datasetsinKaggletodate.Theuserscansubmitsolutions
forthecompetitionsanddataset-specifictasks.Tocreateabench-
mark to analyze the fairness ofML models, we have collected 40
kernelsfromtheKaggle.Eachkernelprovidessolution(codeand
description) for a specific data science task. In this study, we have
analyzed ML models that operate on 1) datasets utilized by prior
- German Credit(75)
- Adult Census(302)
- Bank Marketing(73)Popular fairness 
datasets from 
literature [3,16,48]
Protected 
attribute in 
dataset: 7
competitionkernel contains 
predictive model
more than 5 
upvotes
more than 65% 
accuracy
best model from 
a kernel
Total376 Kaggle  
competition
Kaggle kernels
Prediction 
is favorable 
to a group/
individualKaggle kernels
- Home Credit(697)
- Titanic ML(940)Filtering criteria
3 dataset, 450 kernels
2 dataset, 1637 kernelsSelect top 
voted 8 
models for 
each dataset 
5 datasets
40 models
Figure 1:Benchmarkmodelcollection process
studies on fairness, and 2) datasets with protected attribute (e.g.,
sex, race). With this goal, we have collected the ML models with
different filtering criteria for each category. The overall process of
collecting the benchmarkhas been depictedinFigure 1.
Toidentifythedatasetsusedinpriorfairnessstudies,werefer
totheworkonfairnesstestingbyGalhotra etal.[17],wheretwo
datasets, German Credit and Adult Census have been used. Udeshi
et al.experimented on models for the Adult Census dataset [ 49].
Aggarwal et al.used six datasets: German Credit, Adult Census,
Bank Marketing, US Executions, Fraud Detection, and Raw Car
Rentals)[ 3].Amongthesedatasets,GermanCredit,AdultCensus
and Bank Marketing dataset are available on Kaggle. From the
solutionsforthesedatasets,wehavecollected440kernels(65for
German Credit, 302 for Adult Census, and 73 for Bank Marketing).
Furthermore,wehavefilteredthekernelsbasedonthreecriteria
to select the top-rated ones: 1) contain predictive models (some
kernelsonlycontainexploratorydataanalysis),2)atleast5upvotes,
and 3) accuracy ≥65%. Often a kernel contains multiple models
andtriestofindthebestperformingone.Inthesecases,wehave
selected the best performing model from every kernel. Thus, we
haveselectedthetop8modelsbasedonupvotesforeachofthe3
datasets andgot 24 ML models.
Chenet al.[10] listed 12 protected attributes, e.g., age, sex, race,
etc. forfairness analysis.We have found 7competitions inKaggle,
that contain any of these attributes. From the selected ones, we
havefilteredoutthecompetitionsthatinvolvepredictiondecisions
not being favorable to individuals or a specific group. For example,
although this competition [ 28] has customers ageandsexin the
dataset, the classification task is to recommend an appropriate
product to the customers, which we can not classify as fair or
unfair.Thus,wehavegottwoappropriatecompetitionswithseveral
kernels. To select ML models from these competitions, we have
utilizedthesamefilteringcriteriausedbeforeandselected8models
for each dataset based on the upvotes. Finally, we have created a
benchmark containing 40 top-rated Kaggle models that operate
on5datasets.Thecharacteristicsofthedatasetsandtasksinthe
benchmarkare showninTable 1.
3.2 Experiment Design
Aftercreatingthebenchmark,wehaveexperimentedonthemodels,
evaluatedperformanceandfairnessmetrics,andapplieddifferent
bias mitigation techniques to observe the impacts. Our experiment
designprocessisshowninFigure 2.Theexperimentsonthebench-
markhave been peer reviewedandpublishedas an artifact [ 7].
In our benchmark, we have models from five dataset categories.
Tobeabletocomparethefairnessofdifferentmodelsineachdataset
category,wehaveusedthesamedatapreprocessingstrategy.We
643Dothe MachineLearning ModelsonaCrowdSourcedPlatform Exhibit Bias? AnEmpirical StudyonModel Fairness ESEC/FSE ’20, November8–13,2020,VirtualEvent, USA
Table 1:The datasetsused inthefairness experimentation. # F: Feature count. PA: Protected attribute.
Dataset Size# FPADescription
German Credit[ 29]1,00021age,
sexThisdatasetcontainspersonalinformationaboutindividualsandpredictscreditrisk(good
or bad credit). The ageprotected attribute is categorized into young ( <25) and old ( ≥25)
basedon[ 16].
AdultCensus [ 26]32,561 12race,
sexThis dataset comprises of individual information from the 1994 U.S. census. The target
featureofthisdatasetistopredictwhetheranindividualearns ≥$50,000ornotinayear.
BankMarketing[ 27]41,188 20ageThis dataset contains the direct marketing campaigns data of a Portuguese bank. The goal
isto predict whether aclientwillsubscribe for aterm depositornot.
Home Credit[ 30]3,075,11 240sexThis dataset contains data related to loan applications for individuals who do not get loan
from the traditional banks. The target feature is to predict whether an individual who can
repaythe loan,getthe applicationacceptedornot.
Titanic ML [ 31] 89110sexThis dataset contains data about the passengers of Titanic. The target feature is to predict
whether the passenger survived the sinking of Titanic or not. The target of the test set is
not published.So,we have taken the training data andfurther split itintotrainandtest.
post-processing pre-processing5 Dataset
- Data
- KernelFor each 
dataset - data encoding
- process missing
  valuesShuffle & 
split data
- train (70%)
- test (30%)- select protected attribute
- select (un)privileged class
- select favorable label(s)Preprocess data Preprocess data for fairness
Kernel
- models8 kernels
Evaluate model
Performance
- accuracy
- f1 score7 Fairness metrics
- DI, SPD, EOD,
AOD, ERR, CNT, TIApply 7 bias mitigation algorithms
in-processingFor each datasetExtract 
best 
modelTrain 
modelTestTrain
Conduct this experiment 10 times on each model and take mean of results
Fairness 
report for 
each 
model
Figure 2:Experimentationto computeperformance,fairness andmitigationimpacts ofmachinelearning models.
have processed the missing or invalid values, transformed con-
tinuousfeaturestocategorical(e.g.,age <25:young,age ≥25:old),
and converted non-numerical features to numerical (e.g., female: 0,
male:1).Wehavedonesomefurtherpreprocessingtothedataset
to be used for fairness analysis: specify the protected attributes,
privilegedandunprivilegedgroup,andwhatarethefavorablelabel
or outcome of the prediction. For example, in the Home Credit
dataset,sexis the protected attribute, where maleis the privileged
group,femaleis the unprivileged group, and the prediction label is
creditriskofthepersoni.e.,good(favorablelabel)orbad.Forall
thedatasets,wehaveusedshufflingandsametrain-testsplitting
(70%-30%)before feeding the data to the models.
For each dataset category, we have eight Kaggle kernels. The
kernelscontainsolutioncodewritteninPythonforsolvingclassifi-
cationproblems.Ingeneral,thekernelsfollowthesestages:data
exploration, preprocessing, feature selection, modeling, training,
evaluation, and prediction. From the kernels, we have manually
extractedthecodeformodeling,training,andevaluation.Forex-
ample,thiskernel[ 33] loadstheGermanCreditdataset, performs
exploratoryanalysisandselectsasubsetofthefeaturesfortraining,
preprocesses data, and finally implements XGBoost classifier for
predictingthecreditriskofindividuals.Wehavemanuallysliced
thecodefor modeling, training, and evaluation. Often thekernels
trymultiplemodels,evaluateresults,andfindthebestmodel.From
a single kernel, we have only sliced the best performing model
found by the kernel.Some kernels donot specify the best model.
In this case, we have selected the model with the best accuracy.Forexample,thiskernel[ 32]worksonAdultCensusdatasetand
implements four models (Logistic Regression, Decision Tree, K-
Nearest Neighbor and Gradient Boosting) for predicting income
of individuals. We have selected the Gradient Boosting classifier
modelsince itgives the bestaccuracy.
Afterextractingthebestmodel,wetrainthemodelandevaluate
performance (accuracy, F1 score). We have found that the model
performance in our experiment is consistent with the prediction
made in the kernel. Then, we have evaluated 7 different fairness
metricsdescribedinğ 3.3.2.Next,wehaveapplied7differentbias
mitigation algorithms separately and evaluated the performance
and fairness metrics. Thus, we collect the result of 9 metrics (2 per-
formance metric, 7 fairness metric) before applying any mitigation
algorithmandafterapplyingeachmitigationalgorithm.Foreach
model, we have done this experiment 10 times and taken the mean
oftheresultsassuggested by [ 16]. We have usedtheopen-source
PythonlibraryAIF360[ 4]developedbyIBMforfairnessmetrics
andbiasmitigationalgorithms.Allexperimentshavebeenexecuted
on a MAC OS 10.15.2, having 4.2 GHz Intel Core i7 processor with
32 GB RAM andPython3.7.6.
3.3 Measures
Wehavecomputedthealgorithmicfairnessofeachsubjectmodel
in our benchmark. Let, D=(X,Y,Z)be a dataset where Xis the
training data, Yisthe binaryclassification label ( Y=1 if the label
isfavorable,otherwise Y=0),Zistheprotectedattribute( Z=1
forprivilegedgroup,otherwise Z=0),andˆYisthepredictionlabel
644ESEC/FSE ’20, November8–13,2020,VirtualEvent, USA SumonBiswas andHrideshRajan
(1forfavorabledecisionand0forunfavorabledecision).Ifthereare
multiplegroupsforprotectedattributes,wehaveemployedabinary
groupingstrategy(e.g.,raceattributeinAdultCensusdatasethas
been changedto white/non-white).
3.3.1 Accuracy Measure. Before measuring the fairness of the
model, we have computedthe performance in terms ofaccuracy,
andF1 score.
Accuracy : Accuracy is given by the ratio of truly classified items
andtotalnumber ofitems.
Accuracy=(#Truepositive+#Truenegative )/# Total
F1Score:Thismetricisgivenbytheharmonicmeanofprecision
andrecall.
F1=2∗(Precision ∗Recall)/(Precision+Recall)
3.3.2 FairnessMeasure. Manyquantitativefairnessmetricshave
been proposed in the literature [ 6] based on different definitions
of fairness. For example, AIF 360 toolkit has APIs for computing
71fairnessmetrics[ 4].Inthispaper,withoutbeingexhaustive,a
representative list of metrics have been selected to evaluate the
fairness of ML models. We have adopted the metrics recommenda-
tion of Friedler et al.[16] and further added the individual fairness
metrics.
Metrics based on baserates:
DisparateImpact(DI): Thismetricisgivenbytheratiobetweenthe
probability of unprivileged group gets favorable prediction and the
probability ofprivilegedgroup getsfavorable prediction [ 15,50].
DI=P[ˆY=1|Z=0]/P[ˆY=1|Z=1]
Statistical Parity Difference (SPD): This measure is similar to DI but
instead ofthe ratioofprobabilities,difference iscalculated[ 8].
SPD=P[ˆY=1|Z=0]−P[ˆY=1|Z=1]
Metrics based on group conditioned rates:
Equal Opportunity Difference (EOD): This is given by the true-
positive rate (TPR) difference between unprivileged and privileged
groups.
TPRu=P[ˆY=1|Y=1,Z=0]; TPRp=P[ˆY=1|Y=1,Z=1]
EOD=TPRu−TPRp
Average Odds Difference (AOD): This is given by the average of
false-positiverate(FPR)differenceandtrue-positiveratedifference
between unprivilegedandprivilegedgroups[ 20].
FPRu=P[ˆY=1|Y=0,Z=0]; FPRp=P[ˆY=1|Y=0,Z=1]
AOD=1
2{(FPRu−FPRp)+(TPRu−TPRp)}
Error Rate Difference (ERD): Error rate isgiven by the addition of
false-positive rate(FPR) andfalse-negative rate(FNR) [ 11].
ERR=FPR+FNR
ERD=ERRu−ERRp
Metrics based on individual fairness:
Consistency(CNT): Thisindividualfairnessmetricmeasureshowsimilarthepredictionsarewhentheinstancesaresimilar[ 51].Here,
n_neiдhbors isthe number of neighborsfor the KNN algorithm.
CNT=1−1
n∗n_neiдhborsn/summationdisplay.1
i=1|ˆyi−/summationdisplay.1
j∈Nn_neiдhbors (xi)ˆyj|
TheilIndex(TI): Thismetricisalsocalledtheentropyindexwhich
measuresboththegroupandindividualfairness[ 45].Theilindex
isgiven bythe following equation where bi=ˆyi−yi+1.
TI=1
nn/summationdisplay.1
i=1bi
µlnbi
µ
3.4 Bias Mitigation Techniques
Inthissection,wehavediscussedthebiasmitigationtechniques
that have been applied to the models. These techniques can be
broadlyclassifiedintopreprocessing,in-processing,andpostpro-
cessing approaches.
PreprocessingAlgorithms. Preprocessingalgorithmsdonotchange
themodelandonlyworkonthedatasetbeforetrainingsothatmod-
els can produce fairer predictions.
Reweighing[ 34]:Inabiaseddataset,differentweightsareassigned
to reduce the effect of favoritism of a specific group. If a class of in-
puthasbeenfavored,thenalowerweightisassignedincomparison
to the class not been favored.
Disparate Impact Remover [ 15]: This algorithm is based on the
conceptofthemetricDIthatmeasuresthefractionofindividuals
achieves positive outcomes from an unprivileged group in compar-
ison to the privileged group. To remove the bias, this technique
modifies the value of protected attribute to remove distinguishing
factors.
In-processingAlgorithms. In-processingalgorithmsmodifythe
ML modelto mitigate the biasinthe originalmodelprediction.
Adversarial Debiasing[ 52]: This approach modifies the ML model
by introducing backward feedback (negative gradient) for predict-
ing the protected attribute. This is achieved by incorporating an
adversarial model that learns the difference between protected and
otherattributes that can be utilizedto mitigate the bias.
PrejudiceRemoverRegularizer[ 36]:IfanMLmodelreliesonthede-
cisionbasedontheprotectedattribute,wecallthatdirectprejudice.
In order to remove that, one could simply remove the protected
attribute or regulate the effect in the ML model. This technique
appliesthelatterapproach,wherearegularizerisimplementedthat
computes the effectofthe protectedattribute.
Post-processing Algorithms. This genre of techniques modifies
the prediction result instead of the ML models orthe inputdata.
EqualizedOdds(E)[ 20]:Thisapproachchangestheoutputlabels
tooptimizetheEODmetric.Inthisapproach,alinearprogramis
solvedto obtain the probabilitiesof modifyingprediction.
Calibrated Equalized Odds [ 41]: To achieve fairness, this technique
also optimizes EOD metric by using the calibrated prediction score
producedbythe classifier.
Reject Option Classification [ 35]: This technique favors the in-
stancesinprivilegedgroupoverunprivilegedonesthatlieinthe
decision boundary withhigh uncertainty.
645Dothe MachineLearning ModelsonaCrowdSourcedPlatform Exhibit Bias? AnEmpirical StudyonModel Fairness ESEC/FSE ’20, November8–13,2020,VirtualEvent, USA
4 UNFAIRNESS INML MODELS
Inthissection,wehaveexploredtheanswerofRQ1byanalyzing
different fairness measures exhibited by the ML models in our
benchmark.Dothemodelshavebiasintheirprediction?Ifso,which
models are fairer andwhich are more biased? What iscausing the
models to be more prone to bias? What kind of fairness metric
is sensitive to different models? To answer these questions, we
haveconductedexperimentontheMLmodelsandcomputedthe
fairness metrics. The result is presented in Table 2. The unfairness
measuresforallthe40modelsaredepictedinFigure 3.Tobeableto
compareallthemetricsinthesamechart,disparateimpact(DI),and
consistency(CNT)havebeenplottedinthelogscale.Ifthevalueof
a fairness metric is 0, there is no bias in the model according to the
corresponding metric. If the measure is less than or greater than 0,
biasexists.Thenegativebiasdenotesthatthepredictionisbiased
towards privileged group and positive bias denotes that prediction
isbiasedtowardsunprivilegedgroup.
Wehavefoundthatallthemodelsexhibitunfairnessandmod-
elsspecifictoadatasetshowsimilarbiaspatterns.FromFigure 3,
we can see that all the models exhibit bias with respect to most
ofthefairnessmetrics.Foramodel,metricvaluesvarysincethe
metrics follow different definitions of fairness. Therefore, we have
compared bias of different models both cumulatively and using
the specific metric individually. To compare total bias across all
the metrics, we have taken the absolute value of the measures and
computedthesumofbiasforeachmodel.InFigure 4,wecansee
the total bias exhibited by the models. Although the bias exhibited
bymodelsforeachdatasetfollowsimilarpattern,certainmodels
are fairer thanothers.
Finding 1: Model optimization goals seek overall perfor-
mance improvement,whichiscausing unfairness.
Model GC1 exhibits the lowest bias among German Credit models.
GC1 is a Random Forest (RFT) classifier model, which is built by
usingagridsearchoveragivenrangeofhyperparameters.After
the grid search, the bestfoundclassifier is:
1RandomForestClassifier ( bootstrap=True , ccp_alpha =0.0 ,
class_weight=None , criterion= ' gini ', max_depth=3,
max_features=4, max_leaf_nodes=None , max_samples=None ,
min_impurity_decrease =0.0 , min_impurity_split=None ,
min_samples_leaf=1, min_samples_split =2,
min_weight_fraction_leaf =0.0 , n_estimators =25, n_jobs =None ,
oob_score=False , random_state=2, warm_start=False )
We have found that GC6 is also a Random Forest classifier built
throughgridsearch.However,GC6 isless fairin termsofcumula-
tivebias(Figure 4),andindividualmetrics(Figure 3)excepterror
ratedifference(ERD).Wehaveinvestigatedthereasonofthefair-
nessdifferencesinthesetwomodelsbyrunningbothofthemby
changing one hyperparameter at a time. We have found that the
fairness difference is caused by the scoring mechanism used by
thetwo models.GC1uses scoring='recall' ,whereasGC6uses
scoring='precision' ,as showninthe following code snippet.
1# Model GC1
2param_grid = { "max_depth" : [3 ,5 , 7 , 10 ,None] , " n_estimators "
:[3 ,5 ,10 ,25 ,50 ,150] , " max_features " : [4 ,7 ,15 ,20]}
3GC1 = RandomForestClassifier ( random_state=2)
4grid_search = GridSearchCV (GC1, param_grid=param_grid , cv=5,
scoring= ' recall ' , verbose =4)5# Model GC6
6params = { ' n_estimators ' :[25 ,50 ,100 ,150 ,200 ,500] , 'max_depth '
:[0.5 ,1 ,5 ,10] , ' random_state ' :[1 ,10 ,20 ,42] , ' n_jobs ' :[1 ,2]}
7GC6 = RandomForestClassifier ()
8grid_search_cv = GridSearchCV (GC6, params , scoring= ' precision ' )
Further investigation shows, in German Credit dataset, the data
rows are personal information about individuals and task is to pre-
dicttheircreditrisk.Thedataitemsarenotbalancedwhen sexofthe
individualsisconcerned.Thedatasetcontains69%datainstances
ofmaleand 31%femaleindividuals. When the model is optimized
towards recall (GC1) rather than precision (GC6), the total num-
ber of true-positives decreasesandfalse-negative increases.Since
thenumberofinstancesforprivilegedgroup( male)ismorethan
the unprivileged group ( female), decrease in the total number of
true-positives also increases the probability of unprivileged group
to be classified as favorable. Therefore, the fairness of GC1 is more
than GC2, although the accuracy is less. Unlike other group fair-
ness metrics, error rate difference (ERD) accounts for false-positive
and false-negative rate difference between privileged and unprivi-
legedgroup.Asdescribedbefore,optimizingthemodelforrecall
increasesthetotalnumberoffalse-negatives.Wehavefoundthat
the percentage of malecategorized as favorable is less than the
percentage of femalecategorized as favorable. Therefore, an in-
crease intheoverallfalse-negative alsoincreased theerrorrate of
unprivilegedgroup,whichinturncausedGC1tobemorebiased
thanGC2 interms ofERD.
From the above discussion, we have observed that the model
optimization hyperparameter only considers the overall rates of
the performance. However, if we split the data instances based
on protected attribute groups, then we see the change of rates
vary for different groups, which induces bias. The libraries for
modelconstructionalsodonotprovideanyoptiontospecifymodel
optimization goals specific to protected attributes and make fairer
prediction.
Here, we have seen that GC1 has less bias than GC6 by com-
promising little accuracy. Do all the models achieve fairness by
compromising withperformance? We have found thatmodels can
achievefairnessalongwithhighperformance.Tocomparemodel
performancewiththeamountofbias,wehaveplottedtheaccuracy
and F1 score of the models with the cumulative bias in Figure 4.
We can see that GC6 is the most efficient model in terms of perfor-
mance and has less bias than 5 out of 7 other models in German
Credit data. AC6 has more accuracy and F1 score than any other
modelsinAdultCensus,andexhibitslessbiasthanAC1,AC2,AC4,
AC5, and AC7. Therefore, models can have better performance and
fairnessat the same time.
Finding 2: Libraries for model creation do not explicitly
mentionfairnessconcernsinmodelconstructs.
From Figure 3, we can see that HC1 and HC2 show difference in
mostofthefairnessmetrics,whileoperatingonthesamedataset
i.e., Home Credit. HC2 is fairer than HC1 with respect to all the
metrics except DI. From Table 2, we can see that HC1 has positive
bias,whereasHC2exhibitnegativebias.ThisindicatesthatHC1
is biased towards unprivileged group and HC2 is biased towards
privileged group. We have found that HC1 and HC2 both are using
646ESEC/FSE ’20, November8–13,2020,VirtualEvent, USA SumonBiswas andHrideshRajan
-0.75-0.5-0.2500.250.50.75
GC1 GC2 GC3 GC4 GC5 GC6 GC7 GC8 AC1 AC2 AC3 AC4 AC5 AC6 AC7 AC8 BM1 BM2 BM3 BM4 BM5 BM6 BM7 BM8 HC1 HC2 HC3 HC4 HC5 HC6 HC7 HC8 TM1 TM2 TM3 TM4 TM5 TM6 TM7 TM8DI SPD EOD AOD ERD CNT TI
Figure 3:Unfairness exhibited by theML models with respectto differentmetrics
Light Gradient Boost (LGB) model for prediction. The code for
buildingthe twomodels are:
1# Model HC1
2HC1 = lgb . LGBMClassifier ( n_estimators =10000, objective =' binary ' ,
class_weight= ' balanced ' , learning_rate =0.05 , reg_alpha =0.1 ,
reg_lambda =0.1 , subsample =0.8 , n_jobs= −1, random_state=50)
3HC1. fit ( X_train , y_train , eval_metric = 'auc ',
categorical_feature = cat_indices , verbose = 200)
4# Model HC2
5HC2 = LGBMClassifier ( n_estimators =4000, learning_rate = 0.03 ,
num_leaves=30, colsample_bytree =.8 , subsample =.9 , max_ depth
=7, reg_alpha =.1 , reg_lambda =.1 , min_split_gain =.01 ,
min_child_weight=2, silent = −1, verbose= −1)
6HC2. fit ( X_train , y_train , eval_metric= 'auc ', verbose= 100)
We have executed both the models with varied hyperparam-
eter combinations and found that class_weight='balanced' is
causing HC1 not to be biased towards privileged group. By spec-
ifyingclass_weight , we can set more weight to the data items
belongingtoan infrequentclass.Higherclassweightimplies that
thedataitemsaregettingmoreemphasisinprediction.Whenthe
class weight is set to balanced , the model automatically accounts
forclassimbalanceandadjusttheweightofdataitemsinversely
proportionaltothefrequencyoftheclass[ 24,42].Inthiscase,HC1
mitigates the male-female imbalance in its prediction. Therefore,
it does not exhibit bias towards the privileged group ( male). On
the other hand, HC2 has less bias but it is biased towards privi-
legedgroup.Althoughwewantmodelstobefairwithrespectto
allgroupsandindividuals,trade-offmightbeneededandinsome
cases,biastowardunprivilegedmaybe adesirable trait.
Wehaveobservedthat class_weight hyperparameterinLGBM-
Classifierallowsdeveloperstocontrolgroupfairnessdirectly.How-
ever, the library documentation of LGB classifier suggests that this
parameterisusedforimprovingperformanceofthemodels[ 42,46].
Though the library documentation mentions about probability cali-
bration of classes to boost the prediction performance using this
parameter, however, there is no suggestion regarding the effect on
the biasintroduceddueto the wrongchoiceofthis parameter.
00.20.40.60.811.21.41.61.8
GC1 GC2 GC3 GC4 GC5 GC6 GC7 GC8 AC1 AC2 AC3 AC4 AC5 AC6 AC7 AC8 BM1 BM2 BM3 BM4 BM5 BM6 BM7 BM8 HC1 HC2 HC3 HC4 HC5 HC6 HC7 HC8 TM1 TM2 TM3 TM4 TM5 TM6 TM7 TM8
German Credit Adult Census Bank Marketing Home Credit Titanic MLAccuracy F1 score Total bias
Figure 4:Cumulative bias andperformanceofthemodels
Fromthediscussions,wecanconcludethatlibrarydevelopers
still do not provide explicit ways to control fairness of the mod-
els. Although someparameters directlycontrol the fairness ofthe
models,libraries donot explicitly mentionthat.Finding 3: Standardizingfeaturesbeforetrainingmodels
canhelptoremovedisparitybetweengroupsintheprotected
class.
FromFigure 3andFigure 4,weobservethatexceptBM5,othermod-
els in Bank Marketing exhibit similar unfairness. BM5 is a Support
Vector Classifier (SVC) tuned using a grid search over given range
of parameters. In the modeling pipeline, before training the best
found SVC, the features are transformed using StandardScalar .
Below is the model construction code for BM5 with the best found
hyperparameters:
1tuned_parameters = [{ ' kernel ' : [' rbf '] ,'gamma ' : [0.1] , 'C ':
[1]}]
2SVC = GridSearchCV (SVC() , tuned_parameters , cv=5, scorin g='
precision ' )
3# Best found SVC after grid search
4# SVC(C=1, break_ties=False , cache_size =200, class_weig ht=None ,
coef0 =0.0 , decision_function_shape ='ovr ' , degree=3, ga mma
=0.1 , kernel =' rbf ' , max_iter= −1, probability=True ,
random_state=None , shrinking=True , tol =0.001)
5model = make_pipeline ( StandardScaler () , SVC)
6mdl = model . fit ( X_train , y_train )
We have found that the usage of StandardScalar in the model
pipelineiscausingthemodelBM5tobefairer.EspeciallyDIofBM5
is0.14whereas,themeanofothersevenBMmodelsisveryhigh
(0.74).StandardScalar transformsthedatafeaturesindependently
so that the mean value becomes 0 and the standard deviation be-
comes1.Essentially,ifafeaturehasvarianceinordersofmagnitude
thananotherfeature,themodelmightlearnfromthedominating
feature more, which might not be desirable [ 43]. In this case, Bank
Marketing dataset has 55 features among which 41 has mean close
to 0 ([0, 0.35]). However, ageis the protected attribute having a
mean value 0.97 ( older: 1,younger: 0), since the number of older is
significantlymorethan younger. Therefore, ageisthedominating
feature in these BM models. BM5 mitigates that effect by using
standard scaling to all features. Therefore, balancing the impor-
tanceofprotectedfeaturewithotherfeaturescanhelptoreduce
bias in the models. This example also shows the importance of
understanding the underlying properties of protected features and
theireffectiveness onprediction.
Finding4: Droppingafeaturefromthedatasetcanchange
the modelfairnesseffectively.
Both themodels AC5 and AC6 are using XGB classifier for predic-
tion but AC6 is fairer than AC5. Among the metrics, in terms of
consistency (CNT), AC5 shows bias 3.61 times more than AC6. We
have investigated the model construction and found that AC5 and
AC6 differ in three constructs: features used in the model, number
647Dothe MachineLearning ModelsonaCrowdSourcedPlatform Exhibit Bias? AnEmpirical StudyonModel Fairness ESEC/FSE ’20, November8–13,2020,VirtualEvent, USA
oftreesusedintherandomforest,andlearningrateoftheclassifier.
Wehaveobservedthatthenumberoftreesandlearningratedid
not change the bias of the models. In AC5, the model excluded one
feature from the training data. Bank Marketing dataset contains
personalinformationaboutindividualsandpredictswhetherthe
personhasanannualincomemorethan50Kdollarsornot.InAC5,
the model developer dropped one feature that contains number of
yearsofeducation,sincethereisothercategoricalfeaturewhich
represents education of the person (e.g., bachelors, doctorate, etc.).
AC6 is using all the features in the dataset. CNT measures the indi-
vidual fairness of the modelsi.e., how two similar individuals (not
necessarilyfromdifferentgroupsofprotectedattributeclass)are
classifiedtodifferentoutcomes.Therefore,droppingthenumber
ofyearsofeducationiscausingthemodeltoclassifysimilarindi-
vidualstodifferentoutcome,whichinturngeneratingindividual
unfairness.
Finding5: Differentmetricsareneededtounderstandbias
indifferentmodels.
From Figure 3, we can see thatthe models show different patterns
ofbiasintermsofdifferentfairnessmetrics.Forexample,compared
to any Bank Marketing models, BM5 has disparity impact (DI) less
than half but the error rate difference (ERD) more than twice. If
themodeldeveloperonlyaccountsforDI,thenthemodelwould
appearfairerthanwhatitactuallyis.Asanotherexample,GC6is
fairer than 90% of all the models in terms of total bias but if we
only consider consistency (CNT), GC6 is fairer than only 50% of
all the models. However, previous studies show that achieving fair-
ness with respect to all the metrics is difficult and for some pair of
metrics, mathematically impossible [ 5,11,37]. Also, the definition
offairnesscanvary dependingontheapplicationcontextandthe
stakeholders.Therefore,itisimportanttoreportoncomprehensive
set of fairness measures and evaluate the trade-off between the
metricstobuildfairer.Wehaveplottedthecorrelationbetweendif-
ferentmetricsfromtwodatasetsinFigure 5.Afewmetricpairshave
a similar correlation in both the datasets such as (SPD, EOD), (SPD,
AOD). This is understandable from the definitions of these metrics
because they are calculated using same or correlated group condi-
tioned rates (true-positives and false-positives). Although there are
many metric pairs which are positively or negatively correlated,
there is no pattern in correlation values between the two datasets.
Forinstance,CNTandTIarehighlynegativelycorrelatedinGer-
man Credit models but positively correlated in Titanic ML models.
Therefore, we need a comprehensive set of metrics to evaluate fair-
ness.
Finding6: Except DI, EOD, and AOD, all the fairness mea-
suresremainconsistentovermultipletrainingandprediction.
Tomeasurethestabilityofthefairnessandperformancemetrics,
wehavecomputedthestandarddeviationofeachmetricover10
runssimilarto[ 16].Ineachrun,thedatasetisshuffledbeforethe
train-test split, and model is trained on a new randomized training
set.Wehaveseenthatthemodelsarestablefortheperformance
metrics and most of the fairness metrics. In particular, the average
German Credit  Titanic ML 
Figure 5: Corelation between the metrics. Bottom diagonal
isforGermanCreditmodels,topdiagonalisforTitanicML
models.
of the standard deviations of accuracy, F1 score, DI, SPD, EOD,
AOD, ERD, CNT and TI over all the models are 0.01, 0.01, 0.12,
0.03,0.04,0.04,0.03,0.01,0.01,respectively.ExceptforDI,EODand
AOD, the average standard deviation is very low (less than 0.03).
For these three metrics, we have plotted the standard deviations in
Figure6.Wecanseethatthetrendofstandarddeviationsissimilar
tothemodelsofaspecificdataset.Inourbenchmark,thelargest
dataset is Home Credit, which has the lowest standard deviation
andthesmallestdatasetisTitanicML,whichhasthemost.Sincein
largerdataset,evenaftershufflingthetrainingdataremains more
consistent, the deviation is less. On the other hand, the Titanic ML
datasetis the smallest insize, having 891 data instances.The class
distribution of data instances do not remain consistent when a ran-
domtrainingsetischosen.Therefore,whiledealingwithsmaller
datasets, it is important to choose a training set that represents the
originaldata andevaluate fairnessmultiple times.
Model00.10.20.30.4
GC1GC2GC3GC4GC5GC6GC7GC8
AC1AC2AC3AC4AC5AC6AC7AC8
BM1BM2BM3BM4BM5BM6BM7BM8HC1HC2HC3HC4HC5HC6HC7HC8
TM1TM2TM3TM4TM5TM6TM7TM8DI EOD AOD
Figure 6: Standard deviation of the metrics: DI, EOD and
AOD over multiple experiments. Other metrics have very
low standard deviation.
DI has more standard deviation than other metrics. DI is com-
putedusingtheratiooftwoprobabilities, Pu/Pp,wherePuisthe
probability of unprivileged group getting favorable label, and Pp
is the probability of privileged group getting favorable label. Even
theprobabilitydifferenceisverylow,thevalueofDIcanbevery
high.Therefore,DI fluctuatesmore frequently thanothermetrics.
648ESEC/FSE ’20, November8–13,2020,VirtualEvent, USA SumonBiswas andHrideshRajan
Table 2:Unfairness measuresofthemodels before andafter the mitigations
Beforemitigation AftermitigationModelAccF1DISPDEODAODERDCNT TIAccF1DISPDEODAODERDCNT TIRank
GC1-RFT .687.814.002.002 0.004.052-.002.058.683.811.002.002 0.004-.032-.002.058RAOD/PCE
GC2-XGB .743.828-.076-.058-.039-.036.047-.282.142.709.829 0000.067 0.057AORD/PCE
GC3-XGB .742.827-.105-.079-.043-.065.036-.173.149.729.831-.045-.040-.006-.043.037-.095.100AR/DPOCE
GC4-SVC .753.832-.138-.104-.081-.068.070-.338.153.716.834 0000.090 0.057AORD/PEC
GC5-EVC .743.826-.148-.116-.075-.089.067-.286.127.687.814 0000.112 0.058AORD/PEC
GC6-RFT .761.845-.103-.083-.023-.085.005-.183.121.759.844-.071-.058-.023-.085-.027-.183.121RD/APCEO
GC7-XGB .751.831-.073-.056.009-.072-.033-.293.144.709.829 0000.047 0.057ADR/POCEGerman Credit(Sex)*GC8-KNN .698.815.003.002 0.011.081-.041.090.702.825 0000.086 0.057AR/DPCOE
AC1-LRG .845.657-.654-.104-.100-.069-.050-.045.127.261.399.023.023.017.021.120-.019.040ORCDAP/E
AC2-RFT .846.657-.582-.098-.047-.046-.060-.236.119.787.249-.354-.014.007.003-.086-.005.232AROC/DPE
AC3-GBC .858.677-.496-.079-.041-.031-.045-.010.120.858.675-.131-.024-.041-.031-.004-.010.120ROAC/DPE
AC4-CBC .869.712-.616-.102-.077-.056-.044-.069.107.805.683-.127-.044.080.044-.001-.102.082ORAC/PDE
AC5-XGB .867.708-.588-.097-.073-.051-.043-.224.111.865.705-.203-.039-.073-.051-.002-.224.111ROAC/PDE
AC6-XGB .871.717-.570-.096-.044-.036-.047-.062.106.808.691-.132-.046.072.044.009-.094.078ORAC/PDE
AC7-RFT .852.678-.615-.104-.078-.059-.051-.235.117.638.329-.289-.024-.005-.009-.039-.009.187AORCD/PEAdultCensus (Race)*AC8-DCT .853.675-.519-.086-.040-.035-.050-.068.121.852.673-.153-.029-.040-.035-.010-.068.121ROAC/DPE
BM1-XGB .906.582.627.087.074.053.051-.078.074.905.581.274.032.074.053.017-.078.074ROCPD/EA
BM2-LGB .908.606.593.083.004.022.069-.034.072.772.498.076.026-.037-.037-.031-.040.066ORDC/PAE
BM3-GBC .908.604.688.100.083.056.051-.032.072.852.529.066.013-.059-.052 006-.089078CODR/APE
BM4-XGB .887.330.810.048.067.042.074-.010.111.887.328.442.022.067.042.001-.010.111RCA/OPDE
BM5-SVC .875.175.139.003-.077-.031.126-.032.126.873.002.139 0-.001 0.110 0.136ERCDO/AP
BM6-GBC .908.612.698.105.030.038.076-.033.071.795.521.110.034-.072-.053-.019-.039.065OCRD/PAE
BM7-XGB .910.611.713.107.051.052.072-.047.070.829.485.022.004-.037-.044-.007-.122.085CODRA/PEBank Marketing(Age)BM8-RFT .899.435.834.066.091.058.064-.023.097.795.462.289.042-.048-.027.005-.052.073ORACDP/E
HC1-LGB .883.249.574.046.065.052.051-.110.083.238.132-.025-.002-.003-.002-.020-.006.030APECR/OD
HC2-LGB .920.094-.698-.006-.016-.010-.032-.012.081.919.002.076 000-.033 0.084PECROA/D
HC3-GNB .913.010.974.999.007.005.006-2.449 0.732.194.181.857.047.019.031-2.285 0OA/DECPR
HC4-XGB .919.046.868.994.003.013.007-2.482 0.918.012-.103.998 0-.003-.002-2.468 0CEDRP/OA
HC5-CBC .870.302.744.865.085.140.106-2.524 0.552.075-.134.999-.025-.017-.021-2.772.001ACEPR/DO
HC6-CBC .869.305.735.085.144.107.068-.147.080.583.074.021 000.007 0.056ACPER/DO
HC7-XGB .911.211.953.953.033.084.054-2.533 0.907.090.408.966.009-.052-.019-2.453 0ECPR/DOAHome Credit(Sex)HC8-RFT .661.239.383.719.147.129.133-2.449.001.645.226.337.681.133.098.112-2.426.001CPRD/AEO
TM1-XGB .807.720-2.247-.705-.631-.559-.056-.341.153.649.580-.082-.039.027.177.115-.272.189OAERDP/C
TM2-RFT .816.753-2.013-.709-.635-.515.022-.293.142.644.566-.106-.045.059.166.023-.269.223OAERDP/C
TM3-EBG .799.725-2.125-.674-.637-.514-.017-.333.165.647.572-.108-.045.031.148.050-.317.223OAERD/PC
TM4-LRG .800.732-2.439-.808-.729-.694-.051-.381.144.658.577-.075-.034.072.160.038-.327.207OAEPRD/C
TM5-GBC .816.740-2.268-.708-.647-.542-.022-.357.151.651.572-.087-.033.097.174.029-.332.205OAERD/CP
TM6-XGB .804.730-1.948-.665-.583-.499-.042-.345.146.625.568-.079-.038.075.157.092-.367.190OAERD/CP
TM7-RFT .825.747-2.232-.639-.555-.411-.029-.285.161.653.577-.099-.043.100.188.003-.261.219OAERDP/CTitanicML(Sex)
TM8-RFT .814.732-2.306-.716-.633-.563-.051-.321.149.649.596-.082-.042.011.166.157-.327.172OAERD/PC
*Experiment has been conducted for multiple protected attributes. RFT:RandomForest,XGB:XGBoost, SVC: Support Vector Classifier, EVC: EnsembleVoting Classifier, KNN:
K-Nearest Neighbors, LRG: Logistic Regression, GBC: Gradient Boosting Classifier, CBC: Cat Boost Classifier, DCT: Decision Tree, LGB: Light Gradient Boost, GNB: Gaussian Naive
Bayes, EBG:EnsembleBagging. Mitigation techniquesapplied to the models areas follows. Resultisshown for the best mitigation. Rank of mitigation uses acronymbelow
(mitigations before ‘/’ havebeen able to mitigate bias, rest havenot.)
Reweighing (R) DIRemover (D) Adversarial Debiasing(A) PrejudiceRemover (P) Equalized Odds(E) Calibrated Equalized Odds (C) Reject OptionClassification (O)
Finding7: Afairmodelwithrespecttooneprotectedat-
tributeisnotnecessarilyfairwithrespecttoanotherprotected
attribute.
To understand the behavior of the same models on different pro-
tectedattributes,wehaveanalyzedthefairnessofGermanCredit
and Adult Censusmodels ontwo protected attributes. In Figure 7,
we have plotted the fairness measures of German Credit models
onsexandageand Adult Census models on sexandrace. We have
foundthatthemodelscanshowdifferentfairnesswhendifferent
protectedattributeisconsidered.ThetotalbiasexhibitedbyGer-
manCredit dataset are: for sexattribute 4.82andfor ageattribute
7.72.ForAdultCensus,thetotalbiasare:for sexattribute15.15and
forraceattribute 8.56. However, most of the models exhibit similar
trendofdifferenceinthefairnesswhenconsideringtwodifferent
attributes.
GC1 and GC6 show cumulative bias 0.12 and 0.60 when sexis
considered. Surprisingly, GC1 and GC6 shows cumulative bias 0.85and 0.88 when ageis considered. GC1 is much fairer model than
GC6inthefirstcasebutinthesecondcase,thefairnessisalmost
similar. We have discussed the behavior of these two models in
Finding1andexplainedhowGC1isfairerwhen sexistheprotected
attribute.However,thefairprediction does not persist for the age
becausethereisnoimbalanceinGermanCreditwithrespectto age
groups. Therefore, GC1 and GC6 showsimilar fairness when ageis
considered.
-0.4-0.3-0.2-0.100.1
Sex
Age
Sex
Age
Sex
Age
Sex
Age
Sex
Age
Sex
Age
Sex
Age
Sex
Age
Sex
Race
Sex
Race
Sex
Race
Sex
Race
Sex
Race
Sex
Race
Sex
Race
Sex
RaceGC1GC2GC3GC4GC5GC6GC7GC8AC1AC2AC3AC4AC5AC6AC7AC8
DISPDEODAODERDCNTTI
Figure7:FairnessofMLmodelswithrespecttodifferentpro-
tected attributes
649Dothe MachineLearning ModelsonaCrowdSourcedPlatform Exhibit Bias? AnEmpirical StudyonModel Fairness ESEC/FSE ’20, November8–13,2020,VirtualEvent, USA
5 MITIGATION
Inthissection,wehaveinvestigatedthefairnessresultsofthemod-
els after applying bias mitigation techniques. We have employed7
differentbiasmitigationalgorithmsseparatelyon40modelsand
comparedthefairnessresultswiththeoriginalfairnessexhibitedby
themodels.Foreachmodel,wehaveselectedthemostsuccessful
mitigationalgorithmandplottedthefairnessvaluesaftermitiga-
tion in Figure 8. We have observed that similar to Figure 3, the
fairness patterns are similar for the models in a dataset. DI, SPD,
andCNTare the mostdifficult metrics to mitigate.
To understand the root causes of unfairness, we have focused
on the models which exhibit more or less bias and then investi-
gatedtheeffectsofdifferentmitigationalgorithms.Here,among
the mitigation algorithms, the preprocessing techniques operate
on the training data and retrain the original model to remove bias.
Ontheotherhand,post-processingtechniquesdonotchangethe
training data or original model but change the prediction made by
the model. The in-processing techniques do not alter the dataset or
predictionresultbutemploy completelynew modeling technique.
Finding8: Models with effective preprocessing mitigation
techniqueispreferable thanothers.
We have found that Reweighing algorithm has effectively debiased
many models: GC1, GC6, AC3, AC5, AC8, BM1 and BM4. These
models produce fairer results when the dataset is pre-processed
usingReweighing.Inotherwords,thesemodelsdonotpropagate
bias themselves. In other cases where pre-processing techniques
arenoteffective,wehadtochangethemodeloraltertheprediction,
which implies that bias is induced or propagated by the models.
Another advantage is that in these models, after mitigations the
modelshave retainedtheaccuracyand F1score.Othermitigation
techniques often hampered the performance of the model. For a
few other models (GC3, GC8, AC1, AC2, AC4, AC6, BM2, BM5,
BM8), Reweighing has been the second most successful mitigation
algorithm. Among these models, in AC1, AC2, BM2, and BM5, the
mostsuccessfulalgorithmtomitigatebiaslossaccuracyorF1score
at least 22%. In all of these cases, Reweighing has retained both
accuracyandF1 score.
Finding9: Models with more bias are debiased effectively
bypost-processingtechniques,whereasoriginallyfairermod-
elsaredebiasedeffectivelybypreprocessingorin-processing
techniques.
From Table 2, we can see that 21 out of 40 models are debiased
by one of the three post-processing algorithms i.e., Equalized odds
(EO), Calibrated equalized odds (CEO), and Reject option classi-
fier(ROC).Thesealgorithmshavebeenabletomitigatebias(not
necessarilythemostsuccessful)in90%ofthemodels.Especially,
ROCand CEOarethedominantpost-processingtechniques.ROC
takes the model prediction, and gives the favorable outcome to the
unprivilegedgroupandunfavorableoutcometoprivilegedgroup
with a certain confidence around the decision boundary [ 35]. CEO
takestheprobabilitydistribution score generatedby theclassifier
andfindtheprobabilityofchangingoutcomelabelandmaximizeequalizedodds[ 41].EOalsochangestheoutcomelabelwithcer-
tainprobabilityobtainedbysolvingalinearprogram[ 20].Wehave
foundthatthesepost-processingmethodshavebeenabletomiti-
gate bias more effectively when the original model produces more
biased results. From Figure 4, we can see that the most biased 5
models are TM4, TM8, TM5, TM1, HC7, where the post-processing
hasbeenthemostsuccessfulalgorithms.Onthecontrary,incase
ofthe5leastbiasedmodel(GC1,GC8,BM5,GC6,GC3),ratherthan
mitigating, allthree post-processing techniques increasedbias.
In Table2, we have shown the rank of mitigation algorithms
todebiaseachmodel.InTable 3,wehaveshownthemeanofthe
ranksofeachmitigationalgorithms,whererankofmostsuccessful
algorithmis1andleastis7.Wecanseethatformostbiasedmodels,
Reject option classification and Equalized odds have been more
successful than all others. For the least biased models, both prepro-
cessing algorithmsand Adversarial Debiasing have been effective,
andthe post-processing algorithms have been ineffective.
Table3:Meanrankofeachbiasmitigationalgorithmfor10
least biased models (LBM), 10 most biased models (MBM),
andoverall.
Stage Algorithms LBMMBM All
Reweighing (R) 2.14.5 3.03PreprocessingDisparateImpactRemover (D) 3.74.8 4.58
Adversarial Debiasing(A) 32.9 3In-processingPrejudiceRemover Regularizer(P) 4.55.3 4.98
Equalized Odds (E) 5.82.8 5.18
Calibrated Equalized Odds (C) 4.85.1 4.33 Post-processing
Reject OptionClassification (O) 4.12.6 2.93
6 IMPACT
While mitigating bias, there is a chance that the performance of
the model is diminished. The most successful algorithm in debias-
inga modeldoes not always give good performance. So, often the
developers have to trade-off between fairness and performance. In
this section, we have investigatedthe answerto RQ3. Whatare the
impactswhenthebiasmitigationalgorithmsareappliedtothemod-
els?WehaveanalyzedtheaccuracyandF1scoreofthemodelsafter
applying the mitigation algorithms. First, for each model, we have
analyzedthe impactsof the most effectivemitigationalgorithmsin
removing bias. In Figure 9, we have plotted the change in accuracy,
F1score,andtotalbiaswhenthemostsuccessfulmitigatingalgo-
rithms are applied. We can see that while mitigating bias, many
models are losing their performance. From Table 2, pre-processing
algorithms,especiallyReweighinghasbeenthemosteffectivein
modelGC1,GC6,AC3,AC5,AC8,BM1,andBM3.FromFigure 9,
thesemodels alwaysretain their performance after mitigation.
Finding10: Whenmitigatingbiaseffectively,in-processing
mitigationalgorithmsshowuncertainbehaviorintheirper-
formance.
Amongin-processingalgorithms,Adversarialdebiasinghasbeen
themosteffectivein11models(GC2,GC3,GC4,GC5,AC2,AC7,
HC1,HC5,HC6),andPrejudiceremoverhasbeenthemosteffective
in1model(HC2).WehavefoundthatforGermanCreditmodelsAd-
versarialdebiasinghasbeeneffectivewithoutlosingperformance.
650ESEC/FSE ’20, November8–13,2020,VirtualEvent, USA SumonBiswas andHrideshRajan

*& *& *& *& *& *& *& *& $& $& $& $& $& $& $& $& %0 %0 %0 %0 %0 %0 %0 %0 +& +& +& +& +& +& +& +& 70 70 70 70 70 70 70 70', 63' (2' $2' (5' &17 7,
Figure 8: The fairness exhibited by the models after applying the bias mitigation techniques. The color coding in Table 2is
used to denote themostsuccessfulmitigationalgorithm foreachmodel.
-1.6-1.4-1.2-1-0.8-0.6-0.4-0.200.2
GC1GC2GC3GC4GC5GC6GC7GC8
AC1AC2AC3AC4AC5AC6AC7AC8
BM1BM2BM3BM4BM5BM6BM7BM8HC1HC2HC3HC4HC5HC6HC7HC8
TM1TM2TM3TM4TM5TM6TM7TM8Accuracy F1 score Bias
Figure 9: Change of performance and bias after applying
bias mitigation technique (negative value indicates reduc-
tion)
Butinother cases,AC1,AC7,HC1, andHC7, the accuracyhas de-
creasedatleast21.4%.InHC2,PrejudiceremoveralsolosesF1score
whilemitigatingthebias.Since,in-processingtechniquesemploy
newmodelandignorethepredictionoftheoriginalmodel,inallsit-
uations(datasetandtask),itisnotgivingbetterperformance.Inour
evaluation, adversarial debiasing is giving good performance with
German Credit dataset but not on Adult Census or Home Credit
dataset. Therefore, in-processing techniques are not appropriate
whenwecannotchangetheoriginalmodeling.Also,sincethese
techniques are uncertain in retaining performance, the developers
shouldbecarefulabouttheaccuracyofpredictionaftertheinter-
vention.
Finding11: Althoughpost-processingalgorithmsarethe
mostdominatingindebiasing,theyarealwaysdiminishing
the modelaccuracyandF1 score.
From Table 2, we can see that in 21 out of 40 models, one of the
post-processingalgorithmsarebeingthemostsuccessful.Butinall
of the cases they are losing performance. The average accuracy re-
duction in these models is 7.49% and average F1 decrease is 10.07%.
Forexample,inAC1,themostbiasmitigatingalgorithmisReject
optionclassificationbutthemodelisloosing26.1%accuracyand
40% F1 score. In these cases, developers should move to the next
best mitigation algorithm. In a few other cases such as HC8, the
RejectOptionclassificationmitigatesbiaswithonly1.6%lossinac-
curacyand1.3%lossinf1score.Insuchsituations,post-processing
techniques can be appliedto mitigate the bias.
Finding 12: Trade-offbetweenperformanceandfairness
exists,andpost-processingalgorithmshavemostcompetitive
replacement.Since some most mitigating algorithms are having performance
reduction,foreachmodel,wehavecomparedthemostsuccessful
algorithm with the next best mitigation algorithm in Figure 10.
We have found that for 18 out of 40 models, the performance of
the 2nd ranked algorithm is same or better than the 1st ranked
algorithm. Among them, in AC4, AC6, BM5, HC5, and HC8, the
2nd ranked algorithm has bias very close (not more than 0.1) to
the 1st ranked one. All of these, except HC5, the 1st ranked bias
mitigation algorithmisa post-processingtechnique.We observe
that competitive alternative mitigation technique is more common
forpost-processingmitigationalgorithms.Therefore,ifweincrease
the tolerable range of bias, then other mitigation techniques would
be betteralternative interms of performance.
-0.200.20.4
GC1GC2GC3GC4GC5GC6GC7GC8
AC1AC2AC3AC4AC5AC6AC7AC8
BM1BM2BM3BM4BM5BM6BM7BM8HC1HC2HC3HC4HC5HC6HC7HC8
TM1TM2TM3TM4TM5TM6TM7TM8Accuracy F1 score Bias
Figure 10: Change of performance and bias between the 1st
and 2nd most successful mitigation algorithms (negative
value indicates reduction)
7 THREATS TO VALIDITY
BenchmarkCreation. Toavoidexperimentingonlow-qualityker-
nels,wehave onlyconsideredthekernelswith morethan5votes.
Inaddition,wehaveexcludedthekernelswherethemodelaccu-
racyisverylow(lessthan65%).Finally,wehaveselectedthetop
voted ones from the list. We have also verified that the collected
kernelsarerunnable.ToensurethemodelscollectedfromKaggle
are appropriate for fairness study, we have first selected the fair-
nessanalysisdatasetsfrompreviousworksandsearchedmodels
for those datasets. Finally, we have searched competitions that use
dataset withprotectedattributes usedinthe literature.
Fairnessandperformanceevaluation. Ourcollectedmodelsgive
the same performance, as mentioned in the corresponding Kaggle
kernels.Forevaluatingfairnessandapplyingmitigationalgorithms
we have used AIF 360 toolkit [ 4] developed by IBM. Bellamy et
al.presentedfairnessresults(4metrics)fortwomodels(Logistic
regressionandRandomforest)onAdultCensusdatasetwithpro-
tected attribute race[4]. We havedone experimentwiththesame
setupandvalidatedourresult[ 4].Similarto[ 16],foreachmetric,
651Dothe MachineLearning ModelsonaCrowdSourcedPlatform Exhibit Bias? AnEmpirical StudyonModel Fairness ESEC/FSE ’20, November8–13,2020,VirtualEvent, USA
we have evaluated 10 times and taken the mean of the values. The
stability comparison ofthe results isshowninğ 4.
Fairness comparison. As different metrics are computed based
ondifferentdefinitionsoffairness,wehavecomparedbiasusing
a specific metric or cumulatively. Finally, in this paper, we have
focusedoncomparingfairnessofdifferentmodels.Therefore,for
each dataset, we followed the same method to pre-process training
andtestingdata.
8 RELATED WORKS
SEforFairnessinML. Thislineofworkistheclosesttoourwork.
FairTest [ 48] proposes methodology to detect unwarranted fea-
tureassociationsandpotentialbiasesinadatasetusingmanually
written tests. Themis [ 17] generates random tests automatically
to detectcausal fairness using black-box decision makingprocess.
Aequitas [ 49] is a fully automated directed test generation module
togeneratediscriminatoryinputsinMLmodels,whichcanbeused
tovalidateindividualfairness.FairML[ 1]introducesanorthogonal
transformationmethodologytoquantifytherelativedependence
ofblack-boxmodelstoitsinputfeatures,withthegoalofassessing
fairness.A morerecentwork [ 3]proposes black-boxfairnesstest-
ingmethodtodetectindividualdiscriminationinMLmodels.They
[3] propose a test case generation algorithm based on symbolic
execution and local explainability. The above works have proposed
noveltechniquestodetectandtestfairnessinMLsystems.How-
ever, we have focused on empirical evaluation of fairness in ML
models written by practitioners and reported our findings. Friedler
et al.also worked on an empirical study but compared between
fairness enhancing interventions and not models [ 16]. Harrison
etal.conductedsurveybasedempiricalstudytounderstandhow
fairnessofdifferentmodelsisperceivedbyhumans[ 21].Holstein
et al.also conducted survey on industry developers to find the
challenges for developing fairness-aware tools and models [ 22].
However,noempiricalstudyhasbeenconductedtomeasureand
comparefairnessofMLmodelsinpractice,andanalyzetheimpacts
ofmitigationalgorithms onthe models.
Fairness measure and algorithms. The machine learning com-
munityhasfocusedonnoveltechniquestoidentify,measureand
mitigate bias [ 8,11,13ś15,18,20,36,38,50]. This body of work
concentrate on the theoretical aspects of bias in ML classifiers.
Differentfairnessmeasuresandmitigationalgorithmshavebeen
discussed in ğ 3.3and ğ3.4. In this work, we have focused on the
software engineeringaspectsofML models usedinpractice.
MLmodeltesting. DeepCheck[ 19]proposeslightweightwhite-
box symbolic analysis to validate deep neural networks (DNN).
DeepXplore [ 40] proposes a white-box framework to generate test
input that can exploit the incorrect behavior of DNNs. DeepTest
[47]usesdomain-specificmetamorphicrelationstodetecterrorsin
DNN based software. These works have focused on the robustness
propertyofMLsystems,whereaswehavestudiedfairnessproperty
that isfundamentallydifferentfrom robustness[ 49].
9 CONCLUSION
ML fairness has received much attention recently. However, ML
libraries do not provide enough support to address the issue in
practice.Inthispaper,wehaveempiricallyevaluatedthefairnessofMLmodels anddiscussed ourfindings ofsoftware engineering
aspects. First, we have created a benchmark of 40 ML models from
5 different problem domains. Then, we have used a comprehensive
set of fairness metrics to measure fairness. After that, we have
applied7mitigationtechniquesonthemodelsandcomputedthe
fairness metric again. We have also evaluated the performance
impactofthemodelsaftermitigationtechniquesareapplied.We
have found what kind of bias is more common and how they could
be addressed. Our study also suggests that further SE research and
library enhancements are neededto make fairnessconcernsmore
accessible to developers.
ACKNOWLEDGMENTS
This work was supported in partby US NSF undergrants CNS-15-
13263, and CCF-19-34884. All opinions are of the authors and do
not reflectthe viewofsponsors.
REFERENCES
[1]JuliusAdebayoandLalanaKagal.2016. Iterativeorthogonalfeatureprojection
for diagnosing bias in black-box models. arXiv preprint arXiv:1611.04967 (2016).
[2]Julius A Adebayo et al .2016.FairML: ToolBox for diagnosing bias in predictive
modeling. Ph.D. Dissertation. MassachusettsInstituteof Technology.
[3]Aniya Aggarwal, Pranay Lohia, Seema Nagar, Kuntal Dey, and Diptikalyan Saha.
2019. Blackboxfairnesstestingofmachinelearningmodels.In Proceedingsof
the201927thACMJointMeetingonEuropeanSoftwareEngineeringConference
and Symposiumonthe FoundationsofSoftwareEngineering . 625ś635.
[4]Rachel KE Bellamy, Kuntal Dey, Michael Hind, Samuel C Hoffman, Stephanie
Houde, Kalapriya Kannan, Pranay Lohia, Jacquelyn Martino, Sameep Mehta,
Aleksandra Mojsilovic, et al .2018. AI Fairness 360: An extensible toolkit for
detecting, understanding, and mitigating unwanted algorithmic bias. arXiv
preprint arXiv:1810.01943 (2018).
[5]Richard Berk, Hoda Heidari, Shahin Jabbari, Michael Kearns, and Aaron Roth.
2018.Fairnessincriminaljusticeriskassessments:Thestateoftheart. Sociological
Methods& Research (2018), 0049124118782533.
[6]Reuben Binns. 2017. Fairness in machine learning: Lessons from political philos-
ophy.arXiv preprint arXiv:1712.03586 (2017).
[7]SumonBiswasandHrideshRajan.2020. ML-Fairness:AcceptedArtifactforES-
EC/FSE 2020 Paper on Fairness of Machine Learning Models .https://doi.org/10.
5281/zenodo.3912064
[8]Toon Calders and Sicco Verwer. 2010. Three naive Bayes approaches for
discrimination-free classification. Data Mining and Knowledge Discovery 21,
2 (2010), 277ś292.
[9]JenniferCarpenter.2011. Maythebestanalystwin. AmericanAssociationfor
the Advancement of Science.
[10] Jiahao Chen,Nathan Kallus, XiaojieMao, Geoffry Svacha, and Madeleine Udell.
2019. Fairness under unawareness: Assessing disparity when protected class
isunobserved.In ProceedingsoftheConferenceonFairness,Accountability,and
Transparency . 339ś348.
[11]AlexandraChouldechova.2017. Fairpredictionwithdisparateimpact:Astudy
of biasin recidivism prediction instruments. Big data5,2 (2017), 153ś163.
[12]GeorgeEDahl,NavdeepJaitly,andRuslanSalakhutdinov.2014. Multi-taskneural
networks for QSARpredictions. arXiv preprint arXiv:1406.1231 (2014).
[13]LucasDixon,JohnLi,JeffreySorensen,NithumThain,andLucyVasserman.2018.
Measuringandmitigatingunintendedbiasintextclassification.In Proceedingsof
the 2018AAAI/ACMConference onAI, Ethics,and Society . 67ś73.
[14]Cynthia Dwork, Moritz Hardt, Toniann Pitassi, Omer Reingold, and Richard
Zemel.2012. Fairnessthroughawareness.In Proceedingsofthe3rdinnovationsin
theoretical computer scienceconference . 214ś226.
[15]MichaelFeldman,SorelleAFriedler,JohnMoeller,CarlosScheidegger,andSuresh
Venkatasubramanian.2015. Certifyingandremovingdisparateimpact.In pro-
ceedingsofthe21thACMSIGKDDinternationalconferenceonknowledgediscovery
and datamining . 259ś268.
[16]Sorelle A Friedler, Carlos Scheidegger, Suresh Venkatasubramanian, Sonam
Choudhary, Evan P Hamilton, and Derek Roth. 2019. A comparative study
offairness-enhancinginterventionsinmachinelearning.In Proceedingsofthe
Conference onFairness,Accountability, and Transparency . 329ś338.
[17]Sainyam Galhotra, Yuriy Brun, and Alexandra Meliou. 2017. Fairness testing:
testing software for discrimination. In Proceedings of the 2017 11th Joint Meeting
onFoundationsofSoftwareEngineering . 498ś510.
[18]GabrielGoh,AndrewCotter,MayaGupta,andMichaelPFriedlander.2016. Satis-
fyingreal-worldgoalswithdatasetconstraints.In AdvancesinNeuralInformation
652ESEC/FSE ’20, November8–13,2020,VirtualEvent, USA SumonBiswas andHrideshRajan
ProcessingSystems . 2415ś2423.
[19]DivyaGopinath,KaiyuanWang,MengshiZhang,CorinaSPasareanu,andSarfraz
Khurshid. 2018. Symbolic execution for deep neural networks. arXiv preprint
arXiv:1807.10439 (2018).
[20]Moritz Hardt, Eric Price, and Nati Srebro. 2016. Equality of opportunity in
supervised learning. In Advances in neural information processing systems . 3315ś
3323.
[21]GalenHarrison,JuliaHanson,ChristineJacinto,JulioRamirez,andBlaseUr.2020.
An empirical study on the perceived fairness of realistic, imperfect machine
learning models. In Proceedings of the 2020 Conference on Fairness, Accountability,
and Transparency . 392ś402.
[22]Kenneth Holstein, Jennifer Wortman Vaughan, Hal Daumé III, Miro Dudik, and
Hanna Wallach. 2019. Improving fairness in machine learning systems: What do
industrypractitionersneed?.In Proceedingsofthe2019CHIConferenceonHuman
Factors inComputingSystems . 1ś16.
[23]Kathryn Jepsen. 2014. The machine learning community takes on the
Higgs. https://www.symmetrymagazine.org/article/july-2014/the-machine-
learning-community-takes-on-the-higgs .
[24] PrateekJoshi.2017. Artificial intelligence with python . Packt Publishing Ltd.
[25]Kaggle.2010. Theworld’slargestdatasciencecommunitywithpowerfultools
and resourcesto helpyou achieveyourdata science goals. www.kaggle.com .
[26]Kaggle. 2017. Adult Census Dataset. https://www.kaggle.com/uciml/adult-
census-income .
[27]Kaggle. 2017. Bank Marketing Dataset. https://www.kaggle.com/c/bank-
marketing-uci .
[28]Kaggle. 2017. Competition: Santander Product Recommendation. https://www.
kaggle.com/c/santander-product-recommendation/overview .
[29]Kaggle.2017. GermanCreditDataset. https://www.kaggle.com/uciml/german-
credit.
[30]Kaggle. 2017. Home Credit Dataset. https://www.kaggle.com/c/home-credit-
default-risk .
[31] Kaggle.2017. TitanicMLDataset. https://www.kaggle.com/c/titanic/data .
[32]Kaggle. 2019. Adult Census Kernel: Multiple ML Techniques and Analy-
sis.https://www.kaggle.com/bananuhbeatdown/multiple-ml-techniques-and-
analysis-of-dataset .
[33]Kaggle.2019. Kernel:GermanCreditRiskAnalysis. https://www.kaggle.com/
pahulpreet/german-credit-risk-analysis-beginner-s-guide .
[34]Faisal Kamiran and Toon Calders. 2012. Data preprocessing techniques for
classificationwithoutdiscrimination. KnowledgeandInformation Systems 33,1
(2012), 1ś33.
[35]FaisalKamiran,AsimKarim,andXiangliangZhang.2012. Decisiontheoryfor
discrimination-awareclassification.In 2012IEEE12thInternationalConferenceon
DataMining . IEEE,924ś929.
[36]Toshihiro Kamishima, Shotaro Akaho, Hideki Asoh, and Jun Sakuma. 2012.
Fairness-awareclassifierwithprejudiceremoverregularizer.In JointEuropean
Conference on Machine Learning and Knowledge Discovery in Databases . Springer,
35ś50.[37]Jon Kleinberg, Sendhil Mullainathan, and Manish Raghavan. 2016. Inherent
trade-offs in the determination of risk scores. arXiv preprint arXiv:1609.05807
(2016).
[38]Judea Pearl et al .2009. Causal inference in statistics: An overview. Statistics
surveys3 (2009), 96ś146.
[39]DinoPedreshi,SalvatoreRuggieri,andFrancoTurini.2008.Discrimination-aware
data mining. In Proceedings of the 14th ACM SIGKDD international conference on
Knowledgediscoveryand datamining . 560ś568.
[40]Kexin Pei, Yinzhi Cao, Junfeng Yang, and Suman Jana. 2017. Deepxplore: Au-
tomated whitebox testing of deep learning systems. In proceedings of the 26th
SymposiumonOperatingSystemsPrinciples . 1ś18.
[41]GeoffPleiss,ManishRaghavan,FelixWu,JonKleinberg,andKilianQWeinberger.
2017. Onfairness andcalibration.In Advances in Neural InformationProcessing
Systems. 5680ś5689.
[42]ScikitLearn.2019. LightGBMAPIDocumentation. https://lightgbm.readthedocs.
io/en/latest/pythonapi/lightgbm.LGBMClassifier.html .
[43]Scikit Learn. 2019. SVC API Documentation. https://scikit-learn.org/stable/
modules/generated/sklearn.preprocessing.StandardScaler.html .
[44]Kacper Sokol, Raul Santos-Rodriguez, and Peter Flach. 2019. FAT Forensics:
APythonToolboxforAlgorithmicFairness,AccountabilityandTransparency.
arXiv preprint arXiv:1909.05167 (2019).
[45]TillSpeicher,HodaHeidari,NinaGrgic-Hlaca,KrishnaPGummadi,AdishSingla,
AdrianWeller,andMuhammadBilalZafar.2018. Aunifiedapproachtoquantify-
ing algorithmic unfairness: Measuring individual &group unfairness via inequal-
ityindices.In Proceedingsofthe24thACMSIGKDDInternationalConferenceon
KnowledgeDiscovery& Data Mining . 2239ś2248.
[46]Stack Overflow. 2016. How does the class_weight parameter in scikit-learn
work? https://stackoverflow.com/questions/30972029/how-does-the-class-
weight-parameter-in-scikit-learn-work .
[47]YuchiTian,KexinPei,SumanJana,andBaishakhiRay.2018. Deeptest:Automated
testing of deep-neural-network-driven autonomous cars. In Proceedings of the
40thinternational conference onsoftwareengineering . 303ś314.
[48]FlorianTramer,VaggelisAtlidakis,RoxanaGeambasu,DanielHsu,Jean-Pierre
Hubaux,MathiasHumbert,AriJuels,andHuangLin.2017. FairTest:Discovering
unwarranted associations in data-driven applications. In 2017 IEEE European
SymposiumonSecurityand Privacy (EuroS&P) . IEEE,401ś416.
[49]SakshiUdeshi,PryanshuArora,andSudiptaChattopadhyay.2018. Automated
directedfairnesstesting.In Proceedings ofthe33rd ACM/IEEE InternationalCon-
ference onAutomatedSoftwareEngineering . 98ś108.
[50]MuhammadBilal Zafar, Isabel Valera, Manuel GomezRodriguez, and KrishnaP
Gummadi.2015. Fairness constraints:Mechanismsforfairclassification. arXiv
preprint arXiv:1507.05259 (2015).
[51]Rich Zemel, Yu Wu, Kevin Swersky, Toni Pitassi, and Cynthia Dwork. 2013.
Learningfairrepresentations.In InternationalConferenceonMachineLearning .
325ś333.
[52]Brian Hu Zhang, Blake Lemoine, and Margaret Mitchell. 2018. Mitigating un-
wantedbiaseswithadversariallearning.In Proceedingsofthe2018AAAI/ACM
Conference onAI, Ethics,and Society . 335ś340.
653