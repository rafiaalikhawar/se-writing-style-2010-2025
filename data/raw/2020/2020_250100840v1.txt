arXiv:2501.00840v1  [cs.SE]  1 Jan 2025Distilled Lifelong Self-Adaptation for Conﬁgurable
Systems
Yulong Ye1,2, Tao Chen1,2∗, Miqing Li2
1IDEAS Lab, University of Birmingham, United Kingdom
2School of Computer Science, University of Birmingham, Unit ed Kingdom
yxy382@student.bham.ac.uk, t.chen@bham.ac.uk, m.li.8@ bham.ac.uk
Abstract —Modern conﬁgurable systems provide tremendous
opportunities for engineering future intelligent softwar e systems.
A key difﬁculty thereof is how to effectively self-adapt the
conﬁguration of a running system such that its performance
(e.g., runtime and throughput) can be optimized under time-
varying workloads. This unfortunately remains unaddresse d in
existing approaches as they either overlook the available p ast
knowledge or rely on static exploitation of past knowledge
without reasoning the usefulness of information when plann ing
for self-adaptation. In this paper, we tackle this challeng ing
problem by proposing DLiSA , a framework that self-adapts
conﬁgurable systems. DLiSA comes with two properties: ﬁrstly,
it supports lifelong planning, and thereby the planning pro cess
runs continuously throughout the lifetime of the system, al lowing
dynamic exploitation of the accumulated knowledge for rapi d
adaptation. Secondly, the planning for a newly emerged work load
is boosted via distilled knowledge seeding, in which the kno wledge
is dynamically puriﬁed such that only useful past conﬁgurat ions
are seeded when necessary, mitigating misleading informat ion.
Extensive experiments suggest that the proposed DLiSA signif-
icantly outperforms state-of-the-art approaches, demons trating
a performance improvement of up to 229% and a resource
acceleration of up to 2.22 ×on generating promising adaptation
conﬁgurations. All data and sources can be found at our
repository: https://github.com/ideas-labo/dlisa .
Index Terms —Self-adaptive systems, search-based software
engineering, dynamic optimization, conﬁguration tuning
I. I NTRODUCTION
Software systems are often highly conﬁgurable [1]–[5].
However, their operation environment is often confronted w ith
dynamic and uncertain conditions that change over time [6],
[7], which is crucial to their performance (e.g., runtime [8 ]).
Taking the H2 database system as an example, its real-time
workloads are known to highly ﬂuctuate, prompting the syste m
to dynamically adjust its conﬁguration options to accommo-
date such changes [9].
To mitigate this, one promising way is to engineer self-
adaptive conﬁgurable systems—a speciﬁc type of self-adapt ive
systems that, when the workload changes, self-adapt their c on-
ﬁgurations to meet different performance needs [10]–[13]. The
critical challenge of self-adaptation lies in planning [14 ]–[16],
i.e., how to identify the most effective conﬁguration (a.k. a.
adaptation plan) amidst constantly changing workload at ru n-
time. Recently, Search-Based Software Engineering (SBSE)
∗Corresponding author.has been considered a promising direction for solving this
challenge, which tries to iteratively search for and reﬁne c on-
ﬁgurations to locate the optimal one by using tailored searc h
algorithms [17]–[20]. The inherent search and optimizatio n
properties of SBSE make it well-suited to addressing the com -
plexities of huge conﬁguration spaces encountered in adapt ing
conﬁgurable systems. Importantly, SBSE exhibits highly ex -
tensible potentials for complementing other approaches, s uch
as control theoretical [21]–[23] and learning-based metho ds
[24]–[29], to achieve integrated schemes, thereby providi ng a
comprehensive solution for runtime planning.
Beyond the exponentially growing search space and the
non-linear interaction among conﬁguration options, the ev er-
changing workload further intensiﬁes the planning difﬁcul -
ties when self-adapting conﬁgurable systems. Particularl y, the
landscape of the search space may shift dramatically across
different workloads, suggesting that a conﬁguration optim ized
for one workload may become suboptimal or even perform
poorly in another [11]. This dynamic nature requires planni ng
to not only search for an optimal conﬁguration under a newly
emerged workload but should be doing so rapidly.
A promising resolution to that end is to reuse “past knowl-
edge”, i.e., conﬁgurations that were optimized under the pr e-
vious workloads, for the planning to start working with unde r
the current workload [20]. However, unfortunately, existi ng
works often assume a stationary adaptation, which restarts the
planning process from scratch following each workload chan ge
or at a ﬁxed frequency. Such methods may be inefﬁcient, as
they fail to fully utilize historical search experiences, r esulting
in repetitive effort and a waste of valuable information tha t
could inform more effective adaptation planning [17], [18] ,
[30]. Indeed, certain approaches have followed a dynamic
adaptation [19], [20], [31] that exploit conﬁgurations fou nd
previously to speedup the planning (i.e., seeding). This, w hile
running continuously, can still generate negative outcome s as
the way how knowledge is exploited follows a static strategy :
all (or randomly selected) conﬁgurations from the most rece nt
past workload are used while any of those from earlier
workloads are discarded. That said, the idea seems intuitiv e—
the latest workload that has been changed may provide more
useful information for the current newly emerged one while
those older workloads may often be less useful. Yet, since th e
order of workload arriving in the system is uncertain, there isno guarantee that the conﬁgurations from the latest workloa d
are all promising for the current one nor those from earlier
workloads are completely irrelevant, as what has been impli ed
in prior work [9] and observed from our study in Section-II.C .
To ﬁll the above gap, in this paper, we propose a frame-
work, dubbed DLiSA , to self-adapt conﬁgurable systems at
runtime based on the MAPE-K loop [32]. DLiSA comes with
a combination of two properties that makes it distinctive:
(1)lifelong planning , where we leverage an evolutionary
algorithm in the planning that runs continuously throughou t
the lifetime of the system while providing the foundation
for seeding; and (2) distilled knowledge seeding —a truly
dynamic knowledge exploitation strategy such that it not on ly
seeds past conﬁgurations when there is evidence that they ca n
be beneﬁcial but also extracts the most useful ones to seed
from all historical workloads, hence mitigating the mislea ding
noises while keeping the most useful information. In this wa y,
DLiSA ensures that the exploitation of past knowledge is
neither static nor completely abandoned, which ﬁts with the
characteristics of conﬁgurable systems.
In a nutshell, our main contributions are as follows:
•We show, by examples of conﬁgurable systems’ land-
scapes, the key characteristic faced by self-adapting con-
ﬁguration at runtime under changing workloads.
•We develop a ranked workload similarity analysis to
excavate correlations and patterns of past workloads,
helping to extrapolate the traits of the new workload for
more informed adaptation planning.
•We propose a weighted conﬁguration seeding that distills
the past knowledge, seeding only the most useful conﬁg-
urations and mitigating the misleading ones.
•DLiSA is experimentally evaluated against four state-
of-the-art approaches on nine real-world systems with
different performance objectives, scales, and complexity ,
including 6–13 time-varying workloads. This leads to a
total of 93 cases to investigate.
Experimental results encouragingly demonstrate that
DLiSA exhibits signiﬁcant improvements in both efﬁcacy (up
to 2.29×) and efﬁciency (up to 2.22 ×).
The rest of this paper is organized as follows. Section II
introduces the background and motivation. Section III prov ides
the details of our proposed DLiSA . Section IV presents our
experiment methodology, followed by the experimental resu lts
in Section V. Section VI discusses the most noticeable aspec ts
ofDLiSA . Threats to validity, related work, and conclusion
are presented in Sections VII, VIII, and IX, respectively.
II. B ACKGROUND AND MOTIVATION
In this section, we discuss the preliminaries and main
motivation of this work.
A. Self-Adaptive Conﬁgurable Systems
In this work, we focus on self-adaptive conﬁgurable sys-
tems. According to a well-known taxonomy [10], the self-
adaptive conﬁgurable systems differ from the other concept s
as follows:Budget Constraint 
Configuration 
Self-Adaptive Configurable System Performance 
Workload !
Timestep !
W1 W2 W3 W4 Wt! !
Fig. 1: Self-adaptation planning for conﬁgurable systems.
•Self-Adaptive Systems: These systems adapt to changes
by modifying their behaviors, which could be any sys-
tem’s states (including structure and parameters) at run-
time [10].
•Self-Reconﬁgurable Systems: A special type of self-
adaptive systems that primarily alter their struc-
ture/architecture (including parameters) to adapt [33].
•Self-Adaptive Conﬁgurable Systems: Unlike others,
these systems speciﬁcally adapt by adjusting conﬁgura-
tion parameters [34].
Clearly, the self-adaptive conﬁgurable system is a type of
self-adaptive/self-reconﬁgurable systems that primaril y adjust
conﬁguration parameters at runtime to optimize their perfo r-
mance [10], [33], [34], focusing at the intersection betwee n
self-optimized and self-conﬁgured systems [10], which have
been frequently studied in prior work involving dynamic
workloads [11]–[13].
B. Problem Formalization
Without loss of generality, self-adaptation planning for a
given conﬁgurable system involves the following key concep ts,
as shown in Figure 1.
•System: A conﬁgurable system with conﬁgurable options
that can be adjusted at runtime.
•Workload: The time-varying and uncertain receiving
jobs for which the system handles. The concrete instance
can vary. For example, the workloads refer to different
types and volumes of queries that emerged for database
system H2; for ﬁle compressors such as KANZI , this
becomes the incoming ﬁles to be compressed, which
could be of diverse formats and sizes.
•Conﬁguration: An instance of variability for a con-
ﬁgurable system, formed by a set of values for the
conﬁgurable options. In this work, we consider both the
conﬁgurations (and options) that require system rebooting
and those that do not.
•Performance: The metric(s) that evaluates the behavior
of the system, such as runtime (i.e., the time taken by the
system to process a given workload) and throughput.
•Budget constraint: The budget of cost allowed for self-
adaptation planning under the workload for a particulartimestep. While the deﬁnition of budget varies, in this
work, we use the number of system measurements dur-
ing planning as the budget, which means that we can
only measure a certain number of conﬁgurations as our
constraint. The measurement is chosen because: (1) it is
independent of the implementation, such as language and
hardware; (2) it eliminates the interference of clock time
caused by the running system to be adapted, when it is
run at the same machine as the planning process; (3) it
has been widely used in existing work [2], [35].
When a single performance objective is of concern, the goal
of planning when self-adapting a conﬁgurable system Sis, for
each timestep tin which the system handles a workload over
the time horizon, to identify a conﬁguration that optimizes
the speciﬁc performance attribute, e.g., minimizing runti me
or maximizing throughput, of the target system, subject to a
budget constraint for planning. Formally, this can be deﬁne d
as:
argminft(x)orargmaxft(x),
s.t.rt≤Rt,(1)
where x= (x1,x2,...,x n) is a conﬁguration with the values
ofnoptions (e.g., xn) in search space X.ftrepresents the
performance attribute of the target system. rtandRtrespec-
tively denote the cost consumption and the budget allowed fo r
planning at timestep t.
C. Motivation and Challenges
It is well-known that, when self-adapting conﬁgurable sys-
tems, the conﬁgurations produced under one workload might
be useful to the other workloads [9], [12], [27]. However, it re-
mains unclear how to explicitly extract the key knowledge an d
whether there exists irrelevant or even misleading informa tion,
i.e., noises. To uncover these underlying issues, we analyz e the
datasets collected from commonly used conﬁgurable systems
and their workload from prior studies [9], [36]–[38]. The go al
is to investigate what are the similarities and discrepanci es
between the conﬁguration landscapes of different workload s.
Figure 2 shows the top 50 performing conﬁgurations for two
systems under different workloads and we observe the follow -
ing patterns (similar observations exist in other systems) :
•There could be a strong overlap of the promising con-
ﬁgurations across workloads (the connected points). That
said, a promising conﬁguration under a workload could
also be promising under the others. For example, the
points connected by dashed lines for workloads large ,
vmlinux , andmisc ofKANZI in Figure 2a.
•It is also possible that the promising conﬁgurations
for each individual workload differ signiﬁcantly. For
instance, the points under workloads of H2 in Figure 2b
rarely overlap with each other. The same phenomenon
occurs even for the workloads of the same system, e.g.,
workloaddeepfield against the others for KANZI .
The above leads to a key characteristic for conﬁgurable
systems, which motivates our work:0500 1000 1500 2000 2500 3000 
3500 
4000 Conﬁguration Id 
deepfield misc 
large vmlinux 
Workload 0.10.20.30.40.5
Runtime 
deepﬁeld 
large vmlinux 
misc 
(a)KANZI0
250 
500 
750 
1000 
1250 
1500 
1750 Conﬁguration Id 
smallbank-10 smallbank-1 voter-2 voter-16 
Workload 20000 25000 30000 35000 40000 45000 50000 
Throughput 
smallbank-10 
smallbank-1 voter-2 
voter-16 
(b) H2
Fig. 2: An illustration of similarities and discrepancies i n
top 50 performing conﬁgurations across workloads. The same
conﬁgurations are connected by dashed lines.
Top-performing conﬁgurations between workloads can
be very similar or very discrepant, depending on
both the systems (e.g., KANZI and H2) and the work-
loads within a single system (e.g., between workload
deepfield and the others for KANZI ).Key Characteristic
Since the order of workloads arriving at a system is uncer-
tain, the above suggests that “seeding”1promising conﬁgura-
tions optimized for the past workloads to the planning under
the current workload can be beneﬁcial, as long as we can:
•Challenge 1: extract the most useful conﬁgurations dis-
covered previously (the conﬁgurations that are promising
across workloads), if any, while doing so without inject-
ing misleading information (the conﬁgurations that are
“good” under the past workloads only);
•Challenge 2: and detect when it is generally more
harmful to seed than simply restart planning.
Nevertheless, existing approaches have failed to explicit ly
handle the above characteristics and challenges of conﬁgur able
systems when running under changing workloads: on one
hand, stationary adaptation approaches (e.g., FEMOSAA [30])
restarts a new search/planning from scratch with each work-
load change, but clearly, according to the above characteri s-
tic, this would waste the valuable knowledge from the past
workload instances that could have been exploited [39]. On
the other hand, the dynamic adaptation approaches (e.g.,
Seed-EA [20]) rely on a static assumption for the knowledge
exploitation strategy: all conﬁgurations accumulated to t he
most recent past workload are useful for seeding, while thos e
from previous ones are discarded. Besides, they always trig ger
seeding even when the beneﬁts are unjustiﬁed. Because the
seeds retain the planning state, indeed, the dynamic approa ches
1Seeding is a mechanism that beneﬁts the planning for the curr ent workload
by reusing conﬁgurations optimized under the past workload s [20], [31].TABLE I: Comparing DLiSA against other approaches.
Approach Knowledge Exploitation Seeding Workloads Conﬁgu rations
FEMOSAA [30] N/A N/A N/A N/A
Seed-EA [20] Static Always Most recent past All
D-SOGA [40] Static Always Most recent past Random
LiDOS [11] Static Always Most recent past All
DLiSA Dynamic On-demand All historical Distilled
can also be executed in a “lifelong” manner where the plan-
ning process runs continuously and adapts to the changes in
the workload, but they may pick up potentially misleading
information (optimal conﬁgurations found in the most recen t
past workload but are no longer promising) or missing useful
hints (generally promising conﬁgurations found under earl ier
workloads), hindering the planning in the current workload .
The above, therefore, are the key challenges and limitation s
we address in this paper. Table I summarizes the novelty of
DLiSA against the properties of state-of-the-art approaches.
III. T HEDLISAFRAMEWORK
To tackle the current limitation and handle the key char-
acteristic/challenges discussed in Section II-C, we propo se
DLiSA —a distilled lifelong planning framework for self-
adapting conﬁgurable systems with time-varying workloads .
DLiSA comes with two unique properties:
•Lifelong planning: The planning runs continuously and
adapts to workload changes—a typical case of dynamic
optimization [41]—in which the state optimized across
different workloads can be preserved. This provides the
foundation for addressing Challenge 1 .
•Distilled knowledge seeding: The knowledge of seeding
is dynamically distilled, i.e., DLiSA extracts the most
useful conﬁgurations from all past workloads to seed
into the current planning process; or triggers randomly-
initialized planning from scratch when the overall dis-
tilled knowledge is deemed not sufﬁciently useful. This
tackles both Challenge 1 andChallenge 2 .
Next, we will articulate DLiSA ’s designs in great detail.
A. Architecture Overview
We design DLiSA using the typical MAPE-K architec-
ture [32], as shown in Figure 3 and Algorithm 1 . In a nutshell,
MAPE-K distinguishes two sub-systems—the managed system
refers to the conﬁgurable systems that should be managed; an d
the managing system governs the self-adaptation, i.e., DLiSA .
Once a workload change has been detected (e.g., a new
incoming job), the Monitor informs the Analyzer to analyze
the current and past status, which then triggers Planner for
reasoning about the best self-adaptation plan (conﬁgurati on),
subject to a given budget constraint. Finally, the best-opt imized
conﬁguration is set to the managed system via Executor . The
Knowledge refers to the preserved data that can be used by
any phases in the MAPE loop. In this work, the knowledge we
retain is the workloads experienced by the systems and all th e
corresponding conﬁgurations that were measured/discover ed
in the planning previously (line 7).Managed System 
Evolutionary Planning 
Cyber Twin of the Managed System measuring guiding 
       The Best Configuration: Analyzer D1D1 D2D2 D3D3 Dt−1Dt−1…Knowledge Base 
Ranked Workload Similarity Analysis 
High Similarity? 
Weighted 
Configuration Seeding Random Configuration 
Initialization 
Configurations for Planning Planner Monitor Executor 
No Yes Genetic Algorithm Knowledge Distillation 
Managing System (DLiSA) 
Fig. 3:DLiSA architecture for conﬁgurable systems.
Algorithm 1: DLiSA Framework
Input: Cyber-Twin of the managed system S; the budge constraint
Rt; threshold for triggering seeding α; population size N.
Declare: The best conﬁguration at current workload sbest; the set of
conﬁgurations preserved at workload t,Dt. Seeded set of
conﬁgurations P, and the knowledge base K.
1while the managed system is running do
2t= 0
3 ifworkload change then
4 t=t+1
5 P=KNOWLEDGE DISTILLATION (K,α,N)
6 Dt,sbest←EVOLUTIONARY PLANNING (S,K,P,Rt)
7 K=K∪Dt
8 SEND FORADAPTATION (sbest)
9 end
10end
DLiSA specializes two key phases in MAPE-K (lines 5-6):
•Analyzer: The Knowledge Distillation component iden-
tiﬁes whether seeding is beneﬁcial, and if that is the case,
extracts the most useful conﬁgurations preserved previ-
ously to seed the planning under the current workload,
realizing distilled knowledge seeding (see Section III-B).
•Planner: Here, we leverage Evolutionary Planning com-
ponent to evolve the conﬁgurations into better ones in
the search space based on the given seeds, if any. We
adopt a population-based optimizer where a set of the
most promising conﬁgurations found is preserved and the
best one is used for self-adaptation under a workload (see
Section III-C). In particular, the search-based planning i s
conducted on a sandbox, which is often a Cyber-Twin or
a surrogate system model, that allows expedited measure-
ment of the conﬁgurations while emulating the behavior
of the managed system under the given workload [42],
[43]. Through those seeds, the planning status for the
previous workloads can be kept, hence rendering the
entire process as lifelong planning over the time horizon.
B. Knowledge Distillation
As shown in Figure 3, with the dynamic exploitation strat-
egy realized by knowledge distillation, DLiSA seeks to distillAlgorithm 2: KNOWLEDGE DISTILLATION
Input: The knowledge base K; threshold for triggering seeding α;
initial/population size N
Output: The set of initial conﬁguration for planning P
/*Ranked workload similarity analysis */
1fort=1 to SIZE (K)−1do
2 Dt+1
t←common conﬁgurations from those evaluated in the
planning of two adjacent workloads DtandDt+1fromK
3St+1
t←calculate the similarity between workloads tandt+1
by (2) and (3)
4Ssum =Ssum +St+1
t
5end
6Save=AVERAGING (Ssum)
/*Weighted configuration seeding */
7ifSave≥αand SIZE (K)>0then
8 C←theN/2best conﬁgurations under each workload
9 foreach∀c∈Cdo
/*Get quality weight for cby (4)-(6) */
10 Cw←/angbracketleftc,wc/angbracketright←wc=wc,r+wc,t
11 end
12 P←stochastically pick Nconﬁgurations from Cwusingwc
/*Only happens with one past workload */
13 if|P|/negationslash=Nthen
14 P←randomly initialize conﬁgurations till |P|=N
15 end
16else
17 P←randomly initialize conﬁgurations till |P|=N
18end
19return P
the conﬁgurations optimized for all past workloads in two
steps during planning: ﬁrstly, it selects representative c onﬁgu-
rations evaluated to assess the overall similarity amongst their
performance across the workloads using a ranked similarity
metric at the workload level . A high value of the metric
represents a higher likelihood of the seeding being useful.
Next, if there is convincing evidence that seeding can be
beneﬁcial for planning, we probabilistically extract Nmost
useful conﬁgurations amongst those preserved for seeding
using a quality weight at the conﬁguration level ; otherwise, a
random initialization process is used instead. An algorith mic
illustration has also been shown in Algorithm 2 .
Ranked Workload Similarity Analysis (When to seed?): For
the trigger of seeding, the idea is that, if the majority of th ose
conﬁgurations that were discovered under the past workload s
are “similarly good”, then it is likely that there is a strong
chance for certain promising conﬁgurations optimized prev i-
ously to be equally good under the current, newly emerged
workload, i.e., the seeding should be beneﬁcial.
As a result, we propose a ranked workload similarity analy-
sis at the workload level using all the common conﬁgurations
searched across workloads (including those that were ruled
out). In particular, we quantify the similarity level betwe en two
workloads by using a pairwise ranking loss [44]. The rationa le
is that while the concrete performance of a conﬁguration may
ﬂuctuate with workload changes, the relative rankings can
remain indicative of similarity while being scale-free.
We do so via the following steps (lines 1-6):
1) For every pair of adjacent workloads (e.g., tandt+1),
retrieve all the evaluated conﬁgurations DtandDt+1.
2) Identify their common conﬁgurations Dt+1
t(line 2).3) Compute the ranking loss by quantifying the number of
misranked pairs in Dt+1
t(line 3):
L(Dt+1
t) =Nt+1
t/summationdisplay
j=1Nt+1
t/summationdisplay
k=11((ft(xj)< ft(xk))⊕(ft+1(xj)< ft+1(xk))),
(2)
whereby ⊕is the exclusive-or operator; Nt+1
t is the
number of conﬁgurations evaluated in both workloads
tandt+ 1 (i.e., the size of Dt+1
t). The ranking loss
L(Dt+1
t)represents the number of misranked pairs of
conﬁgurations between adjacent workloads at timesteps
tandt+1, reﬂecting the discrepancy among them.
4) Assess the similarity between tandt+1(St+1
t) using
the percentage of the order-preserving pairs, as follows:
St+1
t= 1−L(Dt+1
t)
Npairs, (3)
whereNpairs is the number of conﬁguration combina-
tion in Dt+1
t(line 3).
5) Calculate the average similarity score for all pairs of
adjacent workloads, i.e., Ssav(line 6).
The seeding is said to be beneﬁcial and should be triggered
only ifSsav≥α, whereαis a given threshold. Note that, if no
common conﬁgurations are found between a pair of adjacent
workloads, we set their similarity St+1
twith a random value
that is less than α, serving as a reasonable guess when no
reliable information can be extracted.
Weighted Conﬁguration Seeding (What to seed?): When
DLiSA determines that the seeding is necessary, we need to
further select the most useful conﬁgurations for seeding th e
current workload. As observed in Section II-C, there could
be a strong overlap of good conﬁgurations across different
workloads, yet sometimes, the promising ones for different
workloads can be highly discrepant. Our idea here is to
design a weighting scheme, such that it can discriminate the
past conﬁgurations based on the likelihood of them being
promising under the current workload. To this end, we design
a two-stage weighted seeding that operates at the conﬁgurat ion
level, considering only the good conﬁgurations preserved. As
such, we say a past conﬁguration is useful for seeding if (1)
it is good within its own workload (line 8) while (2) being
robust and timely across all past workloads (lines 9-12).
The ﬁrst stage—the local stage weighting—focuses on
selecting the best conﬁgurations locally (based on the per-
formance objective) under each workload. This is because
those conﬁgurations that perform badly in a workload would
be less meaningful for seeding. To that end, we ﬁlter the
preserved conﬁgurations at each workload by 50%, i.e., only
N
2conﬁgurations are considered where Nis the number of
conﬁgurations to be seeded in the end (line 8).
In the second stage, we seek to globally weight the conﬁg-
urations across all the past workloads. The hypotheses are:
•preserved conﬁgurations that have demonstrated robust-
ness in many past workloads (as they were not ruled out)
are likely to perform well in the new workload;•since planning under a later workload might have evolved
by integrating previously accumulated knowledge, conﬁg-
urations preserved in such a later workload are likely to
exhibit good performance in the new workload.
Therefore, we use quality weight to sort the previously
selected conﬁgurations from the ﬁrst stage and it has two
components: a robustness weight and a timeliness weight
(lines 9-12). Speciﬁcally, a robustness weight is allocate d to
each conﬁguration based on its recurrence across multiple
past workloads (line 10). Conﬁgurations that appear in a
larger number of workloads receive higher robustness weigh ts,
reﬂecting their robustness for being preferred frequently and
the likelihood of successful performance:
wc,r=Oc
H, (4)
whereOcis the count of past workloads in which the conﬁg-
urationcis preserved and Hdenotes the total number of past
workloads. In contrast, the timeliness weight of a conﬁgura tion
is calculated based on the chronological occurrence of the
latest workload where the conﬁguration is preserved (line
10). Conﬁgurations associated with more recent workloads
are presumed to have integrated prior knowledge and are thus
given higher timeliness weights:
wc,t=Sc
H, (5)
whereScis the sequential number of the latest workload that
the conﬁguration cis associated with, indicating the most
recent (largest) order in which the conﬁguration appears ac ross
the past workloads.
Since both wc,randwc,trange between 0 and 1, the quality
weight of conﬁguration cis then computed as:
wc=wc,r+wc,t (6)
In the end, we stochastically select Nconﬁgurations ac-
cording to wcfor seeding, where a greater value of wcstands
a higher probability of being favored. This, compared with
selecting them deterministically, still retains a low poss ibility
of selecting “less useful” conﬁgurations for seeding, henc e
maintaining diversity to escape from the local optima.
Notably, when there is exactly one previous workload, only
the ﬁrst stage would work as the number of conﬁgurations to
be chosen (N
2) is smaller than the number required ( N). The
remainingN
2conﬁgurations are then randomly generated. Of
course, there will be no seeding under the very ﬁrst workload .
C. Evolutionary Planning
As mentioned, DLiSA works the best with using evolu-
tionary algorithms for planning because (1) they are based o n
population which ﬁts well with the seeding—it caters to a set
of conﬁgurations instead of one; (2) they have been widely
studied in SBSE/self-adaptation [43], [45]. In this work, w e
employ Genetic Algorithm (GA) [46] for seeded planning. In a
nutshell, GA works by iteratively reproducing from promisi ng
conﬁgurations via crossover and mutation, as evaluated on t heTABLE II: Subject system characteristics. The details
of their workloads can be found in our repository:
https://github.com/ideas-labo/dlisa .
System Lang. Domain Perf. Version #O #C #W
JUMP3R [49] Java Audio Encoder Runtime 1.0.4 16 4196 6
KANZI [50] Java File Compressor Runtime 1.9 24 4112 9
DCONVERT [51] Java Image Scaling Runtime 1.0.0- α7 18 6764 12
H2 [52] Java Database Throughput 1.4.200 16 1954 8
BATLIK [53] Java SVG Rasterizer Runtime 1.14 10 1919 11
XZ [54] C/C++ File Compressor Runtime 5.2.0 33 1999 13
LRZIP [55] C/C++ File Compressor Runtime 0.651 11 190 13
X264 [56] C/C++ Video Encoder Runtime baee400. . . 25 3113 9
Z3 [57] C/C++ SMT Solver Runtime 4.8.14 12 1011 12
#O:No. of options; # C:No. of conﬁgurations; # W:No. of workloads tested.
Cyber-Twin (see Section IV-C), to evolve into even better on es.
We adopt elitist-based GA where only the top-performing
conﬁgurations are preserved in each iteration. Since GA has
also been used for tuning conﬁgurable systems, interested
readers can refer to prior work for a more detailed elaborati on
[11], [19], [47].
IV. E XPERIMENTAL SETUP
We experimentally assess the performance of DLiSA by
unraveling the following research questions (RQs):
•RQ1: How effective is DLiSA against state-of-the-art
approaches?
•RQ2: How efﬁcient is DLiSA compared with others?
•RQ3: What beneﬁts do ranked workload similarity anal-
ysis and weighted conﬁguration seeding each provide?
•RQ4: How does αaffectDLiSA ’s performance?
All experiments are run in a Python environment on MacOS
with a quad-core 1.4 GHz CPU and 8GB RAM.
A. Subject Systems, Workloads, and Conﬁgurations
1) Systems: We follow all the systems from a prior empir-
ical study [9] as our subjects, which investigates a range of
widely studied conﬁgurable systems [36]–[38]. Our selecti on
aligns with this study to ensure consistency and comprehen-
siveness. Speciﬁcally, these systems are carefully chosen to
span a variety of application domains, performance objecti ves,
and programming languages, including both Java and C/C++,
thereby providing a solid foundation for our investigation into
systems with diverse characteristics, as shown in Table II.
More details on how to use these systems for conducting
experiments can be found in [9], [48].
2) Workloads: The workloads we studied are diverse and
domain-speciﬁc. For example, for SMT solver Z3, the work-
load can be different SMT instances that are of diverse
complexity, e.g., QF_RDL_orb08 andQF_UF_PEQ018 ; for
database system H2, the workloads are requests with different
rates and types (e.g., read-only or read-write), such as tpcc-2
andycsb-2400 . In this study, we use the same various
workloads as in [9], which range from 6 to 13 depending on
the systems (denoted as W1, W2,. . . , W13). Self-adaptations
are triggered as those different workloads arrive at the sys temin a certain order. Here, we randomly shufﬂe the order of all
workloads to arrive at a system and test self-adaptation the rein.
3) Conﬁgurations: Each system in our study features a
distinct conﬁguration space, covering different option ty pes
(e.g., integers, boolean, and enumerates) and dimensions.
Overall, these diverse systems and workloads provide a
robust foundation for assessing the efﬁcacy and efﬁciency o f
our approach across different contexts and conﬁgurations.
B. Compared Adaptation Approaches
For the comparative analysis in our study, we compare
DLiSA with the following state-of-the-art approaches:
•FEMOSAA2(Stationary Adaptation) [30]: The approach
responds to workload changes by triggering a new search
from scratch with randomly initialized conﬁgurations.
•Seed-EA (Dynamic Adaptation) [20]: By using an
evolutionary algorithm, the approach seeds all the con-
ﬁgurations preserved from the most recent past workload
for planning under the new workload.
•D-SOGA (Mixed Adaptation): As a single-objective
variant of D-NSGA-II [40], this approach retains 80%
randomly chosen conﬁgurations from the most recent past
workload with 20% new randomly initialized conﬁgura-
tions to preserve diversity when the workload change.
•LiDOS (Dynamic Adaptation) [11]: The approach
transforms single-objective problems into multi-objecti ve
ones via an auxiliary objective, thereby leveraging non-
dominance relations to retain local optimal conﬁgurations
under the most recent past workload to be seeded for the
new workload.
C. Component and Parameter Settings
For a fair comparison across all approaches, the parameters
of all stochastic search algorithms in the planning are stan -
dardized, where binary tournament is employed for mating
selection, together with the boundary mutation and single-
point crossover. The mutation and crossover rates are set at
0.1 and 0.9, respectively, with a population size of 20, whic h
is widely used in prior works [5], [30]. For DLiSA , we set its
only parameter α= 0.3, unless otherwise stated, as this tends
to be the generally best setting (see Section V-D).
In this study, we use Cyber-Twin to mimic the behaviors of
the managed systems, which aims to expedite conﬁguration
evaluation in planning by using less time/resources withou t
interfering with the managed system. There are different wa ys
to create such a Cyber-Twin [58], e.g., (1) building a data
driven surrogate model; (2) using existing benchmarks; or
(3) creating a low-cost simulator/replica. Here, we chose
existing benchmarks from [9] as the Cyber-Twin for all the
compared adaptation approaches in experiments, which is
straightforward and easy to implement, providing reliable per-
formance data for accurate conﬁguration measurements [11] ,
[35]. Particularly, the budget constraint is 80 measuremen ts
(i.e.,Rt= 80 ), which is sufﬁcient for the approaches to
2We use the single-objective version and pair it with GA.converge while allowing them to timely self-adapt the syste m.
The achieved performance at the end of planning is recorded.
For the purpose of a controlled experiment, the workload
would change shortly after the best conﬁguration from the
previous planning has been used to adapt the system.
In the search/optimization procedure of the planning un-
der a particular workload, for all approaches, we do not
explore duplicate conﬁgurations, i.e., only the newly eval -
uated/measured conﬁgurations from the Cyber-Twin would
consume the budget. When the planning ﬁnds invalid conﬁg-
urations, we give a purposely worsened performance to those
conﬁgurations, hence they would naturally be ruled out duri ng
the search/optimization process.
To obtain a statistically sound comparison, all experiment s
in this study are run 100 times independently. In particular ,
to test the self-adaptation of the systems, each of the runs
follows a randomly shufﬂed order of the workloads, allowing
us to alleviate the bias introduced by a speciﬁc occurrence
sequence of the workloads. The performance results under
each workload are used but it might appear at a different
position across the repeated runs.
D. Statistical Validation
We employed different statistical validation for examinin g
the 100 runs of results.
1) Pairwise Comparisons: For this, we use the following:
•Non-parametric test: We use Wilcoxon rank-sum test—
a common test widely used in SBSE for its strong statisti-
cal power on pairwise comparisons [59]. The signiﬁcance
level over 100 runs is set at 0.05 and p <0.05indicates
signiﬁcant performance differences in the comparison.
•Effect size: In addition to statistical signiﬁcance, we use
ˆA12to measure the effect size [60]. Particularly, ˆA12≤
0.44orˆA12≥0.56suggests a non-trivial effect.
Thus, we say a comparison is statistically signiﬁcant only
if it has ˆA12≥0.56(orˆA12≤0.44) andp <0.05.
2) Three or More Comparisons: We leverage the Scott-
Knott test [61] to compare multiple approaches. In a nutshel l,
it ﬁrst ranks the approaches based on the mean performance
scores and then iteratively partitions this ordered list in to sta-
tistically distinct subgroups. These subgroups are determ ined
by maximizing the inter-group mean square difference ∆and
their effect sizes. For example, for three approaches A, B, a nd
C, the Scott-Knott test may yield two groups: {A, B}with rank
1 and{C}with rank 2, meaning that A and B are statistically
similar but they are both signiﬁcantly better than C.
V. R ESULTS AND ANALYSIS
A. RQ1: Effectiveness
1) Method: To answer RQ1 , we compare DLiSA with
four state-of-the-art approaches discussed in Section IV- B. We
aggregate and scrutinize the best-performing conﬁguratio ns
from 100 independent runs (each with randomly ordered
workloads) under every workload, across a total of 93 cases
(9 systems and each with 6 to 13 workloads). We also use theTABLE III: The Mean and Standard deviation (Std) of performa nce objectives between DLiSA and other state-of-the-art
approaches for all cases over 100 runs. For each case, green cells mean DLiSA has the best mean performance; or red cells
otherwise. The one(s) with the best rank ( r) from the Scott-Knott test is highlighted in bold.
LRZIP XZ Z3 DCONVERT BATLIK KANZI X264 H2 JUMP 3RWorkload ApproachrMean (Std) rMean (Std) rMean (Std) rMean (Std) rMean (Std) rMean (Std) rMean (Std) rMean (Std) rMean (Std)
FEMOSAA 2 3.214 (0.127) 3 4.931 (1.445) 1 5.881 (0.131) 4 1.952 (0.147) 3 0.953 (0.043) 3 1.830 (1.330) 3 1.038 (0.299) 3 25641.195 (1612.807) 2 2.979 (1.030)
Seed-EA 2 3.134 (0.034) 2 4.636 (1.427) 2 5.898 (0.317) 3 1.867 (0.124) 2 0.912 (0.031) 2 1.346 (1.286) 2 0.937 (0.190) 2 26395.457 (1332.090) 2 2.636 (0.836)
D-SOGA 13.132 (0.015) 1 4.486 (1.251) 2 5.951 (0.953) 11.844 (0.093) 2 0.912 (0.024) 2 1.252 (1.078) 2 0.933 (0.189) 2 26485.351 (1090.741) 2 2.666 (0.848)
LiDOS 2 3.136 (0.032) 2 4.924 (1.986) 3 5.993 (0.628) 4 1.892 (0.135) 2 0.916 (0.025) 2 1.386 (1.252) 2 0.965 (0.195) 2 26287.284 (1466.595) 2 2.665 (0.789)W1
DLiSA 2 3.135 (0.035) 13.813 (0.849) 15.856 (0.011) 2 1.849 (0.105) 10.907 (0.014) 10.986 (0.866) 10.890 (0.140) 126721.450 (705.601) 12.573 (0.828)
FEMOSAA 2 0.031 (0.002) 2 0.014 (0.005) 11.77 (0.089) 2 1.186 (0.071) 3 1.38 (0.037) 3 0.166 (0.048) 3 3.954 (0.842) 3 18273.524 (1026.898) 3 1.093 (0.314)
Seed-EA 2 0.030 (0.000) 2 0.013 (0.005) 4 2.463 (0.668) 2 1.12 (0.061) 2 1.340 (0.020) 2 0.143 (0.041) 2 3.775 (0.806) 2 18573.352 (1625.003) 2 0.915 (0.279)
D-SOGA 2 0.030 (0.000) 2 0.013 (0.005) 1 2.051 (0.491) 2 1.118 (0.056) 3 1.341 (0.019) 2 0.141 (0.033) 1 3.751 (0.819) 1 18847.953 (865.318) 2 0.944 (0.264)
LiDOS 2 0.030 (0.000) 3 0.015 (0.006) 3 2.375 (0.616) 2 1.126 (0.062) 3 1.345 (0.028) 2 0.147 (0.045) 2 3.927 (0.845) 2 18488.896 (1651.846) 2 0.915 (0.232)W2
DLiSA 10.030 (0.000) 10.011 (0.003) 2 2.254 (0.608) 11.115 (0.049) 11.338 (0.019) 10.131 (0.032) 13.590 (0.567) 118972.982 (758.262) 10.846 (0.197)
FEMOSAA 3 3.335 (0.037) 2 4.804 (1.461) 2 0.629 (0.999) 3 0.384 (0.007) 2 4.326 (0.153) 10.293 (0.12) 3 1.534 (0.421) 3 908.110 (46.396) 3 1.629 (0.514)
Seed-EA 13.304 (0.013) 1 4.558 (1.412) 2 0.417 (0.802) 2 0.375 (0.007) 2 4.197 (0.040) 3 0.458 (0.821) 2 1.374 (0.327) 1 943.275 (53.605) 2 1.375 (0.421)
D-SOGA 3 3.311 (0.023) 1 4.543 (1.394) 10.352 (0.611) 3 0.376 (0.007) 2 4.199 (0.042) 3 0.322 (0.168) 2 1.400 (0.322) 1 946.655 (43.526) 3 1.441 (0.414)
LiDOS 2 3.309 (0.019) 1 4.641 (1.438) 2 0.429 (0.875) 3 0.377 (0.008) 2 4.206 (0.056) 3 0.659 (0.706) 2 1.422 (0.348) 2 938.221 (54.261) 2 1.431 (0.372)W3
DLiSA 2 3.305 (0.014) 13.835 (0.966) 2 0.364 (0.660) 10.375 (0.008) 14.196 (0.056) 2 0.308 (0.129) 11.286 (0.248) 1948.344 (38.602) 11.309 (0.368)
FEMOSAA 3 7.268 (0.227) 1 13.679 (3.906) 1 2.375 (0.245) 3 1.657 (0.081) 3 1.233 (0.030) 4 2.625 (1.845) 3 1.770 (0.438) 3 963.053 (92.521) 3 0.741 (0.157)
Seed-EA 2 7.165 (0.088) 1 13.133 (3.830) 2 2.422 (0.274) 3 1.608 (0.071) 11.191 (0.022) 2 1.851 (1.789) 1 1.702 (0.430) 2 1012.475 (108.587) 2 0.685 (0.145)
D-SOGA 2 7.170 (0.078) 1 13.132 (4.012) 3 2.468 (0.428) 11.603 (0.063) 3 1.195 (0.023) 2 1.598 (1.403) 1 1.652 (0.347) 2 1015.531 (86.929) 2 0.697 (0.136)
LiDOS 3 7.171 (0.039) 2 14.171 (8.349) 2 2.384 (0.254) 3 1.622 (0.08) 3 1.200 (0.021) 3 1.963 (1.883) 2 1.757 (0.413) 2 1006.740 (104.145) 2 0.679 (0.105)W4
DLiSA 17.159 (0.032) 111.102 (2.730) 12.324 (0.150) 2 1.605 (0.067) 2 1.193 (0.026) 11.173 (0.697) 11.586 (0.236) 11032.006 (45.261) 10.642 (0.076)
FEMOSAA 3 33.581 (0.386) 2 14.266 (3.910) 2 3.339 (0.655) 2 0.522 (0.021) 3 2.492 (0.066) 3 1.884 (1.057) 3 4.073 (2.727) 2 46866.246 (3439.525) 3 1.446 (0.699)
Seed-EA 133.395 (0.016) 1 13.657 (4.411) 2 3.180 (0.330) 10.502 (0.015) 2 2.409 (0.040) 2 1.281 (1.076) 2 3.414 (0.729) 1 47332.765 (3793.023) 2 1.136 (0.417)
D-SOGA 2 33.397 (0.017) 1 13.815 (3.875) 2 3.172 (0.223) 2 0.503 (0.015) 2 2.413 (0.042) 2 1.182 (0.994) 2 3.438 (0.795) 1 47491.315 (3459.523) 2 1.210 (0.553)
LiDOS 3 33.424 (0.148) 1 14.018 (4.555) 2 3.195 (0.342) 2 0.505 (0.016) 3 2.420 (0.041) 2 1.362 (1.152) 2 3.530 (0.808) 2 47021.762 (4273.182) 2 1.179 (0.44)W5
DLiSA 2 33.421 (0.150) 111.702 (3.297) 13.150 (0.111) 2 0.503 (0.019) 12.404 (0.036) 10.938 (0.604) 13.222 (0.514) 147835.194 (2491.758) 11.045 (0.246)
FEMOSAA 4 0.978 (0.012) 3 2.172 (0.634) 2 1.406 (0.228) 2 0.391 (0.013) 3 3.323 (0.157) 4 0.687 (0.448) 3 0.111 (0.018) 3 47199.317 (2383.084) 4 0.319 (0.045)
Seed-EA 2 0.971 (0.003) 1 1.926 (0.513) 2 1.330 (0.135) 10.375 (0.011) 2 3.158 (0.059) 3 0.528 (0.406) 2 0.104 (0.014) 3 47446.104 (3798.389) 2 0.304 (0.028)
D-SOGA 10.97 (0.002) 2 2.077 (0.696) 2 1.327 (0.137) 2 0.376 (0.012) 3 3.158 (0.049) 2 0.519 (0.363) 2 0.103 (0.012) 2 47844.701 (2876.854) 2 0.309 (0.033)
LiDOS 3 0.972 (0.006) 1 2.053 (0.724) 2 1.337 (0.158) 2 0.376 (0.01) 3 3.170 (0.049) 3 0.581 (0.440) 2 0.105 (0.015) 3 47119.407 (4400.199) 3 0.310 (0.030)W6
DLiSA 3 0.971 (0.003) 11.638 (0.375) 11.322 (0.130) 2 0.376 (0.011) 13.152 (0.042) 10.433 (0.263) 10.100 (0.013) 148335.083 (488.968) 10.298 (0.018)
FEMOSAA 3 0.198 (0.005) 3 0.215 (0.023) 1 0.278 (0.212) 2 19.015 (3.692) 2 1.170 (0.036) 3 0.243 (0.106) 3 0.679 (0.206) 2 18499.348 (1805.899)
Seed-EA 10.192 (0.004) 1 0.205 (0.023) 1 0.320 (0.504) 2 17.485 (2.864) 11.136 (0.016) 2 0.210 (0.108) 2 0.606 (0.154) 1 19979.516 (1662.321)
D-SOGA 2 0.192 (0.004) 2 0.206 (0.019) 10.265 (0.194) 117.269 (2.668) 2 1.136 (0.013) 2 0.205 (0.104) 2 0.606 (0.161) 1 19952.252 (1608.535)
LiDOS 3 0.193 (0.005) 2 0.214 (0.031) 1 0.269 (0.253) 2 18.333 (3.433) 2 1.139 (0.022) 2 0.212 (0.116) 2 0.632 (0.173) 1 19878.876 (1701.235)W7
DLiSA 3 0.192 (0.004) 10.196 (0.015) 1 0.292 (0.458) 1 17.366 (2.734) 2 1.137 (0.016) 10.177 (0.078) 10.572 (0.110) 120037.040 (1584.735)N/A
FEMOSAA 3 10.966 (0.092) 1 29.552 (8.366) 3 8.751 (0.016) 3 1.057 (0.033) 3 7.249 (0.251) 4 5.386 (4.813) 3 0.162 (0.086) 2 26235.578 (2089.717)
Seed-EA 3 10.910 (0.043) 1 28.616 (8.807) 3 8.747 (0.015) 11.030 (0.026) 2 7.079 (0.112) 2 3.869 (4.276) 2 0.142 (0.029) 1 27973.092 (2149.536)
D-SOGA 2 10.909 (0.039) 1 28.969 (8.508) 3 8.806 (0.590) 2 1.032 (0.026) 3 7.084 (0.105) 2 3.227 (2.843) 2 0.143 (0.025) 128147.533 (1915.831)
LiDOS 3 10.919 (0.048) 2 29.977 (9.191) 18.746 (0.005) 3 1.039 (0.029) 3 7.095 (0.093) 3 4.002 (4.353) 2 0.146 (0.032) 1 27926.014 (2064.011)W8
DLiSA 110.907 (0.020) 123.789 (5.998) 2 8.746 (0.005) 3 1.032 (0.027) 17.076 (0.077) 12.347 (2.228) 10.133 (0.019) 1 28129.890 (1669.565)N/A
FEMOSAA 4 9.492 (0.478) 1 27.128 (7.707) 3 3.184 (0.005) 3 0.491 (0.015) 3 1.068 (0.018) 3 1.092 (0.695) 3 0.261 (0.050)
Seed-EA 19.180 (0.282) 1 25.640 (7.097) 2 3.181 (0.003) 2 0.474 (0.015) 11.047 (0.01) 2 0.852 (0.615) 1 0.249 (0.038)
D-SOGA 3 9.279 (0.321) 1 26.330 (7.248) 13.181 (0.003) 3 0.474 (0.014) 2 1.049 (0.012) 2 0.813 (0.583) 2 0.249 (0.039)
LiDOS 4 9.358 (0.296) 2 27.838 (16.49) 3 3.182 (0.004) 3 0.477 (0.016) 2 1.049 (0.012) 2 0.912 (0.745) 2 0.253 (0.040)W9
DLiSA 2 9.197 (0.314) 121.324 (5.188) 3 3.181 (0.003) 10.473 (0.014) 2 1.051 (0.014) 10.709 (0.585) 10.240 (0.031)N/A N/A
FEMOSAA 3 5.595 (0.381) 1 13.081 (3.964) 16.795 (0.232) 2 1.441 (0.011) 3 1.145 (0.037)
Seed-EA 2 5.358 (0.233) 1 12.952 (4.033) 1 6.822 (0.233) 1 1.439 (0.009) 11.116 (0.015)
D-SOGA 3 5.429 (0.299) 1 12.878 (3.446) 1 6.840 (0.241) 1 1.44 (0.008) 3 1.117 (0.017)
LiDOS 2 5.398 (0.276) 2 13.191 (4.421) 1 6.845 (0.265) 2 1.441 (0.009) 3 1.120 (0.018)W10
DLiSA 15.358 (0.228) 110.605 (2.606) 1 6.816 (0.236) 11.438 (0.009) 2 1.117 (0.017)N/A N/A N/A N/A
FEMOSAA 2 2.126 (0.047) 3 3.388 (0.990) 1 8.248 (1.047) 3 1.464 (0.02) 3 1.693 (0.053)
Seed-EA 2 2.092 (0.035) 2 3.254 (0.913) 2 8.410 (1.719) 11.442 (0.018) 11.627 (0.035)
D-SOGA 2 2.094 (0.029) 1 3.190 (0.943) 1 8.232 (1.282) 2 1.446 (0.019) 3 1.630 (0.032)
LiDOS 2 2.093 (0.029) 2 3.311 (0.962) 1 8.327 (1.378) 3 1.447 (0.021) 3 1.636 (0.041)W11
DLiSA 12.089 (0.022) 12.804 (0.775) 17.948 (0.654) 2 1.444 (0.019) 2 1.628 (0.038)N/A N/A N/A N/A
FEMOSAA 3 3.547 (0.121) 3 7.159 (2.183) 3 3.989 (0.344) 2 0.49 (0.009)
Seed-EA 13.474 (0.059) 2 6.327 (1.822) 2 3.890 (0.063) 1 0.487 (0.009)
D-SOGA 2 3.488 (0.079) 2 6.541 (1.967) 2 3.898 (0.150) 2 0.487 (0.01)
LiDOS 3 3.501 (0.154) 2 6.567 (2.086) 2 3.897 (0.148) 2 0.489 (0.01)W12
DLiSA 2 3.477 (0.065) 15.341 (1.318) 13.878 (0.009) 10.487 (0.007)N/A N/A N/A N/A N/A
FEMOSAA 3 2.546 (0.024) 2 3.621 (0.990)
Seed-EA 2 2.529 (0.018) 1 3.440 (1.002)
D-SOGA 12.527 (0.016) 1 3.554 (0.992)
LiDOS 3 2.534 (0.019) 1 3.612 (1.169)W13
DLiSA 3 2.530 (0.018) 12.939 (0.721)N/A N/A N/A N/A N/A N/A N/A
FEMOSAA 0 4 6 0 0 1 0 0 0
Seed-EA 5 9 2 6 5 0 2 4 0
D-SOGA 3 9 6 4 0 0 2 5 0
LiDOS 0 4 4 0 0 0 0 2 0Summary
(r= 1)
DLiSA 5 13 8 6 6 8 9 8 6
Scott-Knott test [61] for our analysis. All other settings a re
the same as discussed in Section IV-C.
2) Result: The experimental results are summarized in
Table III. As can be seen, overall, DLiSA demonstrates
superior performance, ranking ﬁrst in 69 out of 93 cases, whi le
FEMOSAA ,Seed-EA ,D-SOGA , andLiDOS achieve the best
ranks in 11, 33, 29, and 10 out of 93 cases, respectively.
Notably, within the 69 cases where DLiSA achieved ﬁrst
rank, it also realized the best performance values in 65 case s,
highlighting the efﬁcacy and robustness of DLiSA in self-
adaptation. In particular, DLiSA achieves up to 2.29 ×en-
hancement compared with its counterparts (W8 of the KANZI ).
The above efﬁcacy of DLiSA lies in its knowledge distil-
lation for seeding, tailored to the characteristics of chan ging
workloads in conﬁgurable systems. This empowers DLiSA tojudiciously and dynamically distill the most useful conﬁgu ra-
tions and ignore the misleading ones to enhance planning or
engage a conservative stance in case the past conﬁgurations are
generally useless. Other approaches, in contrast, either i gnore
the valuable past knowledge or leverage it without catering to
the noise, due to the stationary setting and the static knowl edge
exploitation strategy.
However, there are also some edge cases where other
approaches are competitive to DLiSA . For instance, in LRZIP ,
DCONVERT , and BATLIK systems, the relatively high similarity
across different workloads suggests that consistently eff ective
conﬁguration in one workload tends to perform well in others .
This consistency favors the Seed-EA approach, which simply
seeds all the conﬁgurations preserved in preceded planning
without speciﬁc responses to workload changes. An interest ingTABLE IV: Comparing resource efﬁciency of
DLiSA with respect to the state-of-the-art approaches.
Detailed results can be found in our repository:
https://github.com/ideas-labo/dlisa .
SystemFEMOSAA Seed-EA D-SOGA LiDOSs >1
s= 1
0< s < 1
s= N/A
s >1
s= 1
0< s < 1
s= N/A
s >1
s= 1
0< s < 1
s= N/A
s >1
s= 1
0< s < 1
s= N/A
LRZIP 13000 2218 620510300
XZ 13000 130001300013000
Z3 8103 8301 6204 7302
DCONVERT 12000 1605 160511001
BATLIK 11000 2405 3602 6401
KANZI 8001 9000 9000 9000
X264 9000 9000 9000 9000
H2 8000 7100 5201 8000
JUMP 3R 6000 6000 6000 6000
Total 88104 5716119 5818017 791004
Range of ss∈[1,2.16]s∈(0,2.22]s∈[1,2.05]s∈[1,2.05]
observation arises within Z3 system, in which D-SOGA ex-
hibits relatively superior performance. This could be attr ibuted
to a possible moderate similarity across workloads, which
allows the combination of historical insights and random
conﬁgurations in D-SOGA to thrive.
Based on the above analysis, we can conclude that:
RQ1:DLiSA is effective as it is generally ranked better
(in the statistical sense) than state-of-the-art in 74% cas es
(69 out of 93) with signiﬁcant performance improvements
of up to 2.29 ×.
B. RQ2: Efﬁciency
1) Method: To evaluate the resource efﬁciency in RQ2 , for
each case out of the 93, we employ the following procedure:
•A baseline, b, is identiﬁed for each counterpart approach,
representing the smallest number of measurements nec-
essary for it to reach its best performance, denoted as T,
averaging over 100 runs.
•ForDLiSA , ﬁnd the smallest number of measurements,
denote as m, at which the average result of the perfor-
mance (over 100 runs) is equivalent to or better than T.
•The speedup of DLiSA over a counterpart is reported as
s=b
m, which is a common metric used in [5], [62].
IfDLiSA is efﬁcient, then we expect s >1;0< s <1and
s= 1meansDLiSA has worse efﬁciency and they are equally
efﬁcient, respectively. We use s=N/A to denote the case
whereDLiSA cannot achieve the Treached by its counterpart.
All other settings are the same as RQ1 .
2) Result: The results are depicted in Table IV, clearly,
DLiSA consistently outperforms or equalling its counter-
parts in the majority of cases. Speciﬁcally, compared with
FEMOSAA ,DLiSA attained superior speedup in 88 cases (up
to 2.16×) with equal efﬁciency in 1 case. When contrasted
toSeed-EA ,DLiSA excelled in 57 cases with a maximum
of 2.22×speedup and matched in 16. For the comparisons
withD-SOGA andLiDOS ,DLiSA maintained a remarkableTABLE V: Comparing DLiSA against its two variants over
100 runs; “+”, “=”, and “ −” respectively indicate DLiSA per-
forming signiﬁcantly better than, similarly to, or worse th an
the variants. Detailed results can be found in our repositor y:
https://github.com/ideas-labo/dlisa .
System DLiSA vsDLiSA -I DLiSA vsDLiSA -II
LRZIP 5+/8=/0− 5+/8=/0−
XZ 13+/0=/0− 5+/8=/0−
Z3 0+/12=/0 − 3+/8=/1−
DCONVERT 2+/10=/0− 5+/7=/0−
BATLIK 5+/6=/0− 10+/1=/0−
KANZI 8+/1=/0− 6+/3=/0−
X264 9+/0=/0 − 0+/9=/0−
H2 2+/6=/0 − 2+/6=/0−
JUMP 3R 6+/0=/0− 3+/3=/0−
Total 50+/43=/0 − 39+/53=/1−
speedup within 58 and 79 (up to 2.05 ×) out of 93 cases,
respectively. These observations illustrate DLiSA ’s ability to
deliver robust performance in utilizing resources for self -
adapting conﬁgurable systems. Therefore, we say:
RQ2:DLiSA is considerably more efﬁcient than state-of-
the-art approaches in the majority of the cases, achieving
up to2.22×speedup.
C. RQ3: Ablation Analysis
1) Method: To understand which parts in the knowledge
distillation of DLiSA work, in RQ3 , we design two variants
to compare with the original DLiSA over the 93 cases:
•DLiSA -I:We replace the weighted conﬁguration seeding
with a random seeding of preserved past conﬁgurations.
•DLiSA -II:We disable the ranked workload similarity
analysis but randomly trigger the seeding of planning.
Since there are only pairwise comparisons, we leverage the
Wilcoxon rank-sum test and ˆA12effect size across 100 runs.
2) Result: The results are summarized in Table V. Clearly,
we see that DLiSA exhibits a remarkable improvement over
its variants from the 93 cases: DLiSA winsDLiSA -Iin 50
cases with 43 ties, reﬂecting the effectiveness of weighted
conﬁguration seeding. Against DLiSA -II,DLiSA wins in
39 cases; draws in 53 cases; and loses only in one case,
indicating the usefulness of the workload similarity analy sis.
These results prove the beneﬁt of seeding only when needed
and the positive implication of considering the most useful
conﬁgurations while excluding the misleading ones—all of
which are speciﬁcally designed in our knowledge distillati on
according to the key characteristic of conﬁgurable systems
under changing workloads discussed in Section II-C.
In light of these observations, we can conclude that:
RQ3: Each individual parts in the knowledge distillation
inDLiSA contribute signiﬁcantly to its superiority.
D. RQ4: Sensitivity to α
1) Method: The parameter αdetermines the likelihood of
triggering seeding, in RQ4 , we examine the sensitivity of0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.902040
α# cases ranked the best
Fig. 4: The sensitivity of DLiSA to different αvalues.
DLiSA toαby comparing the cases of α∈ {0,0.1,...,0.9}.
Again, we use the Scott-Knott to compare the results against
different αvalues over 100 runs for all cases.
2) Result: As illustrated in Figure 4, DLiSA exhibits
obviously superior performance when the αis set to 0.3 in
terms of Scott-Knott ranks. We see that neither too small
nor too large αis optimal. This is because, in the former
case, there would be too many unnecessary seeding, making
it difﬁcult to eliminate the misleading information even wi th
the weighted conﬁguration seeding (e.g., α= 0means seeding
constantly). In the latter case, it is simply due to the fact t hat
it becomes rather difﬁcult to trigger seeding, but instead r ely
merely on random initialization and hence waste the valuabl e
accumulated knowledge. Clearly, the degradation of having
largerαis more serious than setting it small, since not being
able to seed is more inﬂuential than seeding misleading nois es
on the systems/workloads considered. Overall, we show that :
RQ4: Settingαto 0.3 yields the most effective performance
forDLiSA , as it reaches a better balance between the
beneﬁt of seeding and seeding misleading information.
VI. D ISCUSSION
A. How Workload Similarity Analysis Helps?
To understand why the ranked workload similarity analysis
can help, Figure 5a shows the changing similarity scores
on two exampled orders of the time-varying workloads for
Z3. As can be seen, the similarity scores differ depending
on the sequence of the emergent workloads—in some cases,
they are higher than the threshold α= 0.3while in some
other cases, they are lower. Such a discrepancy reﬂects the
likelihood of seeding being beneﬁcial: in the former cases, the
seeding is constantly triggered because conﬁgurations fou nd
via the planning are sufﬁciently similar; while in the latte r
cases, randomly initialized conﬁgurations are used instea d as
seeding would likely be more harmful due to the misleading
information caused by rather different landscapes between the
workloads. In this way, DLiSA retains robust adaptability to
diverse and changing workloads on conﬁgurable systems.
B. Why Weighted Conﬁguration Seeding Work?
To demonstrate how conﬁgurations are weighted for knowl-
edge distillation, Figure 5b visualizes the seeds extracti on pro-
cess for self-adaptation planning under workload artificlW7 W6 W2 W8 W1 W9 W5 W3 W10 W12 W11 W400.20.40.60.81
Workloads order 1Similarity score ( Ssav)W11 W1 W4 W10 W9 W6 W12 W3 W8 W5 W7 W2Workloads order 2
(a) workload similarity ( Z3)0 5 10 15 20 25 30 35 4000.20.40.60.81
Conﬁguration indexWeightWeight
00.20.40.60.81
RuntimeRuntime
(b) conﬁguration weights ( KANZI )
Fig. 5: Examples illustrating the workload similarity and
seeded conﬁgurations with respect to the weights. In (b), th e
dotted lines highlight the selected ones for seeding.
on the KANZI . The weights of distilled conﬁgurations selected
for seeding and their performance under the current workloa d
are connected by dashed lines, in which we see that conﬁgura-
tions with higher weights are often selected, and they gener ally
yield excellent performance (i.e., smaller runtime) than m ost
of the remaining ones. Since we select the conﬁgurations
stochastically based on the weights, we see that a small set
of those with lower weights is also selected. Those lower
weighted conﬁgurations, albeit slightly worse than the oth ers
on performance, help to prevent the selection of too many
similar conﬁgurations for seeding. The above is what makes
the weighted conﬁguration effective in DLiSA .
C. What Are the Implications of DLiSA ?
Lifelong self-adaptation (or self-evolving systems), as h igh-
lighted in prior work [11], [43], is an emerging paradigm for
ensuring that systems can evolve autonomously under unan-
ticipated changes. This study works along this direction fr om
the perspective of seeding under changing workloads. It sho ws
that the evolutionary planning that runs continuously for s elf-
adaptation is beneﬁcial. We have demonstrated the effectiv e-
ness of the two key contributions designed in DLiSA —ranked
workload similarity analysis andweighted conﬁguration seed-
ing—in achieving lifelong self-adaptation for conﬁgurable
systems:DLiSA considerably enhances system performance
by identifying and leveraging useful historical knowledge
while alleviating the impact of misleading information. Th ese
results hold substantial implications for the ﬁeld of Softw are
Engineering, as they support the development of more resili ent
and dependable software systems that make use of existing
useful knowledge while ﬁltering out useless knowledge at
varying workloads. As such, we anticipate that our results
will further advance the existing research in engineering s elf-
adaptive conﬁgurable systems.
VII. T HREATS TO VALIDITY
Our investigation acknowledges the potential threats to
internal validity associated with the parameter α, which we
have set to 0.3. This choice is grounded in empirical evidenc e
from our experiments in RQ4 , whereα= 0.3is a “rule-of-
thumb” that yields generally favorable outcomes. We admit
that the best value for αmay differ on a system-by-systembasis and that exploring different settings for αmight enhance
the robustness of our results. For all experiments, we also u se
statistical tests and effect sizes to mitigate this threat.
Threats regarding external validity may arise from the
speciﬁc conﬁgurable systems and workloads selected for our
study. To mitigate potential biases, we included nine syste ms
in our study. These systems span different domains, scales, and
performance objectives, and we tested them across 93 unique
workloads, following benchmarks set by prior studies [9]. T his
diverse selection aims to enhance the generalizability of o ur
results, though expanding the range of systems examined cou ld
further deepen our insights.
VIII. R ELATED WORK
Here, we discuss the related work in light of DLiSA .
A. Stationary Adaptation for Conﬁgurable Systems
Stationary adaptation in self-adaptive conﬁgurable syste ms
has been the cornerstone of strategies aiming at tuning the
conﬁgurations under time-varying workloads [63]–[66]. Tr adi-
tional approaches, such as those proposed by Chen et al. [30]
and similar frameworks [14], [17] primarily focus on planni ng
from scratch when changes in workloads are detected or at
regular intervals, neglecting the accumulation of valuabl e his-
torical insights. While this simpliﬁes the optimization pr ocess,
it may lead to repetitive effort and ineffective planning.
In contrast, our proposed DLiSA framework adopts a more
holistic view that goes beyond the stationary paradigm. By h ar-
nessing the wealth of information available from past searc h
experiences, DLiSA aims to construct a lifelong planning
trajectory for the system. In this way, DLiSA sidesteps the
inefﬁciencies of stationary planning, fostering a more int elli-
gent optimization process that leverages historical infor mation
to facilitate future adaption planning.
B. Dynamic Adaptation for Conﬁgurable Systems
Dynamic adaptation is characterized by algorithms designe d
for planning continuously and adapting in real time. For
example, Ramirez et al. [19] propose P LATO , a framework
for adaptation planning using SOGA, which can automati-
cally achieve adaptation planning by detecting the changes
in ﬁtness. Chen et al. [31] and Kinneer et al. [20] also
use the concept of seeding to expedite the planning. This
process, inspired by the principles of natural evolution an d
state preservation, is aimed at seamlessly self-adapting t he
workload changes, thereby achieving “lifelong optimizati on”.
Although existing methods are dynamic in nature, they
diverge from the true deﬁnition of dynamic optimization in
the work [67]. This deviation stems from the static knowledg e
exploitation strategy, where all (or randomly chosen) conﬁ gu-
rations from the most recent past workload are seeded for the
current one, while those from earlier workloads are simply
discarded. In contrast, DLiSA introduces distilled knowledge
seeding, a truly dynamic knowledge exploitation strategy,
designed to navigate planning under time-varying workload s.
Rather than naively assuming that only the conﬁgurations fr omthe most recent past workload are useful, it proactively ext racts
the useful conﬁgurations for seeding from all past workload s
while doing so only when it is necessary.
C. Control Theoretical Conﬁguration Adaptation
Control theory has been recognized as an effective solution
for the planning of conﬁgurable systems [21]–[23]. Among
others, Maggio et al. [22] employ Kalman ﬁlters to reﬁne
and update the state values of the controller model, which
are central to model predictive control schemes. Shevtsov
and Weyns [23] expand on this by incorporating the simplex
optimization method, which targets global optima in system
states, thereby enhancing the precision of control mechani sms.
The application of control theory in self-adaptation plann ing is
promising, however, it faces signiﬁcant challenges due to t he
complex, non-linear dynamics of real systems that can only
be prescribed with advanced domain knowledge [68].
D. Conﬁguration Performance Learning
Conﬁguration performance learning for conﬁgurable sys-
tems is a distinct research trajectory that focuses on model ing
the correlation between conﬁguration and performance [69] .
Several methods have been used, such as support-vector ma-
chines [70], decision trees [71], neural network [24], [72] ,
[73], and ensemble learning [28], [73], [74], with the goal o f
crafting a function that accurately encapsulates the corre lation
between adaptation options and the performance of the targe t
system. In contrast, DLiSA emphasizes optimization to self-
adapting the changes of systems—a complementary aspect to
performance learning [75].
IX. C ONCLUSION
This paper proposes DLiSA , a distilled lifelong planning
framework with ranked workload similarity analysis and
weighted conﬁguration seeding components for self-adapti ng
conﬁgurable systems. The goal is to dynamically determine
when it is generally more promising to seed with historical
knowledge ( when to seed? ) and extract what knowledge
should be redirected for planning without injecting mislea ding
information ( what to seed? ). Empirical studies conducted on
nine real-world conﬁgurable systems, spanning various do-
mains and encompassing a total of 93 workloads, demonstrate
that compared with state-of-the-art approaches, DLiSA is:
•more effective, as it achieves considerably better adapta-
tion planning than its four competitors with up to 2.29 ×.
•more efﬁcient, as it exhibits up to 2.22 ×speedup on
producing promising conﬁgurations.
This work sheds light on the importance of automatically
leveraging distilled past knowledge to self-adapt conﬁgur able
systems in planning for future workloads. Looking ahead, we
aim to delve into landscape analysis methods to better handl e
workload evolution and explore feedback mechanisms for
more precise identiﬁcation of beneﬁcial planning informat ion.
ACKNOWLEDGEMENT
This work was supported by a NSFC Grant (62372084) and
a UKRI Grant (10054084).REFERENCES
[1] T. Xu, L. Jin, X. Fan, Y . Zhou, S. Pasupathy, and R. Talwadk er, “Hey,
you have given me too many knobs!: Understanding and dealing with
over-designed conﬁguration in system software,” in Proceedings of the
2015 10th Joint Meeting on Foundations of Software Engineer ing, 2015,
pp. 307–319.
[2] T. Chen and M. Li, “Multi-objectivizing software conﬁgu ration
tuning,” in ESEC/FSE ’21: 29th ACM Joint European Software
Engineering Conference and Symposium on the Foundations of
Software Engineering, Athens, Greece, August 23-28, 2021 , D. Spinellis,
G. Gousios, M. Chechik, and M. D. Penta, Eds. ACM, 2021, pp.
453–465. [Online]. Available: https://doi.org/10.1145/ 3468264.3468555
[3] ——, “Do performance aspirations matter for guiding soft ware
conﬁguration tuning? an empirical investigation under dua l performance
objectives,” ACM Transactions on Software Engineering and
Methodology , vol. 32, no. 3, pp. 68:1–68:41, 2023. [Online].
Available: https://doi.org/10.1145/3571853
[4] ——, “Adapting multi-objectivized software conﬁgurati on tuning,”
Proceedings of ACM Software Engineering , vol. 1, no. FSE, pp.
539–561, 2024. [Online]. Available: https://doi.org/10. 1145/3643751
[5] P. Chen, T. Chen, and M. Li, “MMO: meta multi-objectiviza tion
for software conﬁguration tuning,” IEEE Transactions on Software
Engineering , vol. 50, no. 6, pp. 1478–1504, 2024. [Online]. Available:
https://doi.org/10.1109/TSE.2024.3388910
[6] D. Weyns, R. Calinescu, R. Mirandola, K. Tei, M. Acosta, A . Bennaceur,
N. Boltz, T. Bures, J. Camara, A. Diaconescu et al. , “Towards a research
agenda for understanding and managing uncertainty in self- adaptive
systems,” ACM SIGSOFT Software Engineering Notes , vol. 48, no. 4,
pp. 20–36, 2023.
[7] J. Garc´ ıa-Gal´ an, L. Pasquale, G. Grispos, and B. Nusei beh, “Towards
adaptive compliance,” in Proceedings of the 11th International Sympo-
sium on Software Engineering for Adaptive and Self-Managin g Systems ,
2016, pp. 108–114.
[8] L. Lesoil, M. Acher, X. T¨ ernava, A. Blouin, and J.-M. J´ e z´ equel,
“The interplay of compile-time and run-time options for per formance
prediction,” in Proceedings of the 25th ACM International Systems and
Software Product Line Conference-Volume A , 2021, pp. 100–111.
[9] C. S. M¨ uhlbauer, F. Sattler, and N. Siegmund, “Analyzin g the impact
of workloads on modeling the performance of conﬁgurable sof tware
systems,” in Proceedings of the International Conference on Software
Engineering (ICSE), IEEE , 2023.
[10] M. Salehie and L. Tahvildari, “Self-adaptive software : Landscape and
research challenges,” ACM transactions on autonomous and adaptive
systems (TAAS) , vol. 4, no. 2, pp. 1–42, 2009.
[11] T. Chen, “Lifelong dynamic optimization for self-adap tive systems:
fact or ﬁction?” in 2022 IEEE International Conference on Software
Analysis, Evolution and Reengineering (SANER) . IEEE, 2022, pp. 78–
89.
[12] ——, “Planning landscape analysis for self-adaptive sy stems,” in
International Symposium on Software Engineering for Adapt ive
and Self-Managing Systems, SEAMS 2022, Pittsburgh, PA,
USA, May 22-24, 2022 , B. R. Schmerl, M. Maggio, and
J. C´ amara, Eds. ACM/IEEE, 2022, pp. 84–90. [Online]. Avail able:
https://doi.org/10.1145/3524844.3528060
[13] P. Jamshidi, M. Velez, C. K¨ astner, N. Siegmund, and P. K awthekar,
“Transfer learning for improving model predictions in high ly conﬁg-
urable software,” in 2017 IEEE/ACM 12th International Symposium
on Software Engineering for Adaptive and Self-Managing Sys tems
(SEAMS) . IEEE, 2017, pp. 31–41.
[14] T. Chen, R. Bahsoon, and X. Yao, “Synergizing domain exp ertise
with self-awareness in software systems: A patternized arc hitecture
guideline,” Proceedings of the IEEE , vol. 108, no. 7, pp. 1094–1126,
2020.
[15] T. Chen, R. Bahsoon, S. Wang, and X. Yao, “To adapt or not t o adapt?
technical debt and learning driven self-adaptation for man aging runtime
performance,” in Proceedings of the 2018 ACM/SPEC International
Conference on Performance Engineering , 2018, pp. 48–55.
[16] T. Chen, M. Li, K. Li, and K. Deb, “Search-based software engineering
for self-adaptive systems: Survey, disappointments, sugg estions and
opportunities,” arXiv preprint arXiv:2001.08236 , 2020.
[17] A. Elkhodary, N. Esfahani, and S. Malek, “Fusion: a fram ework for en-
gineering self-tuning self-adaptive software systems,” i nProceedings of
the eighteenth ACM SIGSOFT international symposium on Foun dations
of software engineering , 2010, pp. 7–16.[18] S. Gerasimou, R. Calinescu, and G. Tamburrelli, “Synth esis of proba-
bilistic models for quality-of-service software engineer ing,” Automated
Software Engineering , vol. 25, pp. 785–831, 2018.
[19] A. J. Ramirez, D. B. Knoester, B. H. Cheng, and P. K. McKin ley,
“Applying genetic algorithms to decision making in autonom ic com-
puting systems,” in Proceedings of the 6th international conference on
Autonomic computing , 2009, pp. 97–106.
[20] C. Kinneer, D. Garlan, and C. L. Goues, “Information reu se and stochas-
tic search: Managing uncertainty in self-* systems,” ACM Transactions
on Autonomous and Adaptive Systems (TAAS) , vol. 15, no. 1, pp. 1–36,
2021.
[21] A. Filieri, H. Hoffmann, and M. Maggio, “Automated mult i-objective
control for self-adaptive software design,” in Proceedings of the 2015
10th Joint Meeting on Foundations of Software Engineering , 2015, pp.
13–24.
[22] M. Maggio, A. V . Papadopoulos, A. Filieri, and H. Hoffma nn, “Auto-
mated control of multiple software goals using multiple act uators,” in
Proceedings of the 2017 11th joint meeting on foundations of software
engineering , 2017, pp. 373–384.
[23] S. Shevtsov and D. Weyns, “Keep it simplex: Satisfying m ultiple goals
with guarantees in control-based self-adaptive systems,” inProceedings
of the 2016 24th ACM SIGSOFT International Symposium on Foun da-
tions of Software Engineering , 2016, pp. 229–241.
[24] T. Chen and R. Bahsoon, “Self-adaptive and sensitivity -aware qos mod-
eling for the cloud,” in 2013 8th International Symposium on Software
Engineering for Adaptive and Self-Managing Systems (SEAMS ). IEEE,
2013, pp. 43–52.
[25] H. Ha and H. Zhang, “Deepperf: Performance prediction f or conﬁgurable
software with deep sparse neural network,” in 2019 IEEE/ACM 41st
International Conference on Software Engineering (ICSE) . IEEE, 2019,
pp. 1095–1106.
[26] T. Chen, R. Bahsoon, and X. Yao, “Online qos modeling in t he cloud:
A hybrid and adaptive multi-learners approach,” in 2014 IEEE/ACM 7th
International Conference on Utility and Cloud Computing . IEEE, 2014,
pp. 327–336.
[27] P. Jamshidi, N. Siegmund, M. Velez, C. K¨ astner, A. Pate l, and Y . Agar-
wal, “Transfer learning for performance modeling of conﬁgu rable sys-
tems: An exploratory analysis,” in 2017 32nd IEEE/ACM International
Conference on Automated Software Engineering (ASE) . IEEE, 2017,
pp. 497–508.
[28] T. Chen and R. Bahsoon, “Self-adaptive and online qos mo deling
for cloud-based software services,” IEEE Transactions on Software
Engineering , vol. 43, no. 5, pp. 453–475, 2016.
[29] Q. Zhang, I. Stefanakos, J. Camara Moreno, and R. Caline scu, “Towards
lifelong social robot navigation in dynamic environments, ” 2023.
[30] T. Chen, K. Li, R. Bahsoon, and X. Yao, “Femosaa: Feature -guided
and knee-driven multi-objective optimization for self-ad aptive software,”
ACM Transactions on Software Engineering and Methodology ( TOSEM) ,
vol. 27, no. 2, pp. 1–50, 2018.
[31] T. Chen, M. Li, and X. Yao, “On the effects of seeding stra tegies: a case
for search-based multi-objective service composition,” i nProceedings of
the genetic and evolutionary computation conference , 2018, pp. 1419–
1426.
[32] J. O. Kephart and D. M. Chess, “The vision of autonomic co mputing,”
Computer , vol. 36, no. 1, pp. 41–50, 2003.
[33] D. Garlan, S.-W. Cheng, A.-C. Huang, B. Schmerl, and P. S teenkiste,
“Rainbow: Architecture-based self-adaptation with reusa ble infrastruc-
ture,” Computer , vol. 37, no. 10, pp. 46–54, 2004.
[34] S. Wang, H. Hoffmann, and S. Lu, “Agilectrl: a self-adap tive frame-
work for conﬁguration tuning,” in Proceedings of the 30th ACM Joint
European Software Engineering Conference and Symposium on the
Foundations of Software Engineering , 2022, pp. 459–471.
[35] V . Nair, Z. Yu, T. Menzies, N. Siegmund, and S. Apel,
“Finding faster conﬁgurations using FLASH,” IEEE Trans. Software
Eng., vol. 46, no. 7, pp. 794–811, 2020. [Online]. Available:
https://doi.org/10.1109/TSE.2018.2870895
[36] J. Alves Pereira, M. Acher, H. Martin, and J.-M. J´ ez´ eq uel, “Sampling
effect on performance prediction of conﬁgurable systems: A case
study,” in Proceedings of the ACM/SPEC International Conference on
Performance Engineering , 2020, pp. 277–288.
[37] M. Velez, P. Jamshidi, F. Sattler, N. Siegmund, S. Apel, and C. K¨ astner,
“Conﬁgcrusher: Towards white-box performance analysis fo r conﬁg-
urable systems,” Automated Software Engineering , vol. 27, pp. 265–300,
2020.[38] M. Weber, S. Apel, and N. Siegmund, “White-box performa nce-
inﬂuence models: A proﬁling and learning approach,” in 2021
IEEE/ACM 43rd International Conference on Software Engine ering
(ICSE) . IEEE, 2021, pp. 1059–1071.
[39] R. Liu and L. Tahvildari, “Towards an uncertainty-awar e adaptive
decision engine for self-protecting software: an pomdp-ba sed approach,”
arXiv preprint arXiv:2308.02134 , 2023.
[40] K. Deb, U. B. Rao N, and S. Karthik, “Dynamic multi-objec tive
optimization and decision-making using modiﬁed nsga-ii: A case study
on hydro-thermal power scheduling,” in International conference on
evolutionary multi-criterion optimization . Springer, 2007, pp. 803–817.
[41] T. T. Nguyen, S. Yang, and J. Branke, “Evolutionary dyna mic
optimization: A survey of the state of the art,” Swarm
Evol. Comput. , vol. 6, pp. 1–24, 2012. [Online]. Available:
https://doi.org/10.1016/j.swevo.2012.05.001
[42] J. Ahlgren, K. Bojarczuk, S. Drossopoulou, I. Dvortsov a, J. George,
N. Gucevska, M. Harman, M. Lomeli, S. M. M. Lucas, E. Meijer,
S. Omohundro, R. Rojas, S. Sapora, and N. Zhou, “Facebook’s c yber-
cyber and cyber-physical digital twins,” in EASE 2021: Evaluation and
Assessment in Software Engineering, Trondheim, Norway, Ju ne 21-24,
2021 , R. Chitchyan, J. Li, B. Weber, and T. Yue, Eds. ACM, 2021,
pp. 1–9. [Online]. Available: https://doi.org/10.1145/3 463274.3463275
[43] D. Weyns, T. B¨ ack, R. Vidal, X. Yao, and A. N. Belbachir, “The
vision of self-evolving computing systems,” J. Integr. Des. Process.
Sci., vol. 26, no. 3-4, pp. 351–367, 2022. [Online]. Available:
https://doi.org/10.3233/JID-220003
[44] Y . Li, Y . Shen, J. Jiang, J. Gao, C. Zhang, and B. Cui, “Mfe s-hb: Efﬁcient
hyperband with multi-ﬁdelity quality measurements,” in Proceedings of
the AAAI Conference on Artiﬁcial Intelligence , vol. 35, no. 10, 2021,
pp. 8491–8500.
[45] M. Harman, S. A. Mansouri, and Y . Zhang, “Search-based s oftware
engineering: Trends, techniques and applications,” ACM Comput.
Surv. , vol. 45, no. 1, pp. 11:1–11:61, 2012. [Online]. Available:
https://doi.org/10.1145/2379776.2379787
[46] T. B¨ ack and H.-P. Schwefel, “An overview of evolutiona ry algorithms
for parameter optimization,” Evolutionary computation , vol. 1, no. 1,
pp. 1–23, 1993.
[47] K. M. Bowers, E. M. Fredericks, and B. H. C. Cheng, “Autom ated
optimization of weighted non-functional objectives in sel f-adaptive
systems,” in Search-Based Software Engineering - 10th International
Symposium, SSBSE 2018, Montpellier, France, September 8-9 , 2018,
Proceedings , ser. Lecture Notes in Computer Science, T. E. Colanzi
and P. McMinn, Eds., vol. 11036. Springer, 2018, pp. 182–197 .
[Online]. Available: https://doi.org/10.1007/978-3-31 9-99241-9 9
[48] “Experimental guideline,” https://zenodo.org/reco rds/7504284, retrieved
on December 14, 2023.
[49] “The audio encoder: Jump3r,” https://sourceforge.ne t/projects/jump3r/,
retrieved on December 14, 2023.
[50] “The ﬁle compressor: Kanzi,” https://github.com/ﬂan glet/kanzi, retrieved
on December 14, 2023.
[51] “The density image converter: Dconvert,”
https://github.com/patrickfav/density-converter, ret rieved on December
14, 2023.
[52] “The database: H2,” https://github.com/h2database/ h2database, retrieved
on December 14, 2023.
[53] “The svg rasterizer: Batlik,” https://github.com/ap ache/xmlgraphics-batik,
retrieved on December 14, 2023.
[54] “The ﬁle compressor: Xz,” https://github.com/xz-mir ror/xz, retrieved on
December 14, 2023.
[55] “The ﬁle compressor: Lrzip,” https://github.com/cko livas/lrzip, retrieved
on December 14, 2023.
[56] “The video encoder: X264,” https://github.com/mirro r/x264, retrieved on
December 14, 2023.
[57] “The smt solver: Z3,” https://github.com/Z3Prover/z 3, retrieved on De-
cember 14, 2023.[58] R. Eramo, F. Bordeleau, B. Combemale, M. van Den Brand, M . Wim-
mer, and A. Wortmann, “Conceptualizing digital twins,” IEEE Software ,
vol. 39, no. 2, pp. 39–46, 2021.
[59] A. Arcuri and L. Briand, “A practical guide for using sta tistical tests to
assess randomized algorithms in software engineering,” in Proceedings
of the 33rd international conference on software engineeri ng, 2011, pp.
1–10.
[60] A. Vargha and H. D. Delaney, “A critique and improvement of the cl
common language effect size statistics of mcgraw and wong,” Journal
of Educational and Behavioral Statistics , vol. 25, no. 2, pp. 101–132,
2000.
[61] N. Mittas and L. Angelis, “Ranking and clustering softw are cost
estimation models through a multiple comparisons algorith m,” IEEE
Transactions on software engineering , vol. 39, no. 4, pp. 537–551, 2012.
[62] Y . Gao, Y . Zhu, H. Zhang, H. Lin, and M. Yang, “Resource-g uided con-
ﬁguration space reduction for deep learning models,” in 2021 IEEE/ACM
43rd International Conference on Software Engineering (IC SE). IEEE,
2021, pp. 175–187.
[63] S. Kumar, T. Chen, R. Bahsoon, and R. Buyya, “Datesso: se lf-adapting
service composition with debt-aware two levels constraint reasoning,”
inProceedings of the IEEE/ACM 15th International Symposium o n
Software Engineering for Adaptive and Self-Managing Syste ms, 2020,
pp. 96–107.
[64] S. Kumar, R. Bahsoon, T. Chen, K. Li, and R. Buyya, “Multi -tenant
cloud service composition using evolutionary optimizatio n,” in 2018
IEEE 24th international conference on parallel and distrib uted systems
(ICPADS) . IEEE, 2018, pp. 972–979.
[65] K. Li, Z. Xiang, T. Chen, and K. C. Tan, “Bilo-cpdp: Bi-le vel program-
ming for automated model discovery in cross-project defect prediction,”
inProceedings of the 35th IEEE/ACM International Conference on
Automated Software Engineering , 2020, pp. 573–584.
[66] T. Chen and R. Bahsoon, “Self-adaptive trade-off decis ion making
for autoscaling cloud-based services,” IEEE Transactions on Services
Computing , vol. 10, no. 4, pp. 618–632, 2015.
[67] T. T. Nguyen, S. Yang, and J. Branke, “Evolutionary dyna mic opti-
mization: A survey of the state of the art,” Swarm and Evolutionary
Computation , vol. 6, pp. 1–24, 2012.
[68] X. Zhu, M. Uysal, Z. Wang, S. Singhal, A. Merchant, P. Pad ala, and
K. Shin, “What does control theory bring to systems research ?”ACM
SIGOPS Operating Systems Review , vol. 43, no. 1, pp. 62–69, 2009.
[69] J. Gong and T. Chen, “Deep conﬁguration performance lea rning: A
systematic survey and taxonomy,” ACM Transactions on Software En-
gineering and Methodology , 2024.
[70] N. Yigitbasi, T. L. Willke, G. Liao, and D. Epema, “Towar ds machine
learning-based auto-tuning of mapreduce,” in 2013 IEEE 21st Interna-
tional Symposium on Modelling, Analysis and Simulation of C omputer
and Telecommunication Systems . IEEE, 2013, pp. 11–20.
[71] V . Nair, T. Menzies, N. Siegmund, and S. Apel, “Faster di scovery of
faster system conﬁgurations with spectral learning,” Automated Software
Engineering , vol. 25, pp. 247–277, 2018.
[72] J. Gong and T. Chen, “Predicting conﬁguration performa nce in multiple
environments with sequential meta-learning,” Proceedings of ACM
Software Engineering , vol. 1, no. FSE, pp. 359–382, 2024. [Online].
Available: https://doi.org/10.1145/3643743
[73] ——, “Predicting software performance with divide-and -learn,” in
Proceedings of the 31st ACM Joint European Software Enginee ring
Conference and Symposium on the Foundations of Software Eng ineering,
ESEC/FSE 2023, San Francisco, CA, USA, December 3-9, 2023 ,
S. Chandra, K. Blincoe, and P. Tonella, Eds. ACM, 2023, pp.
858–870. [Online]. Available: https://doi.org/10.1145/ 3611643.3616334
[74] J. Gong, T. Chen, and R. Bahsoon, “Dividable conﬁgurati on performance
learning.” IEEE Transactions on Software Engineering , 2024.
[75] P. Chen, J. Gong, and T. Chen, “Accuracy can lie: On the im pact of
surrogate model in conﬁguration tuning,” IEEE Transactions on Software
Engineering , 2025.