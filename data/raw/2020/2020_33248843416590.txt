Automated Patch Correctness Assessment: How Far are We?
Shangwen Wangâˆ—
wangshangwen13@nudt.edu.cn
College of Computer Science,
National University of Defense
Technology
Changsha, ChinaMing Wenâˆ—â€ 
mwenaa@hust.edu.cn
School of Cyber Science and
Engineering,HuazhongUniversityof
Science and Technology
Wuhan, China, /LetterBo Lin
linbo19@nudt.edu.cn
College of Computer Science,
National University of Defense
Technology
Changsha, China
Hongjun Wu
wuhongjun15@nudt.edu.cn
College of Computer Science,
National University of Defense
Technology
Changsha, ChinaYihao Qin
qinyihao15@nudt.edu.cn
College of Computer Science,
National University of Defense
Technology
Changsha, ChinaDeqing Zouâ€ â€¡
deqingzou@hust.edu.cn
School of Cyber Science and
Engineering,HuazhongUniversityof
Science and Technology
Wuhan, China
Xiaoguang Mao
xgmao@nudt.edu.cn
College of Computer Science,
National University of Defense
Technology
Changsha, ChinaHai Jinâ€ Â§
hjin@hust.edu.cn
School of Computer Science and
Technology, Huazhong University of
Science and Technology
Wuhan, China
ABSTRACT
Test-based automated program repair (APR) has attracted huge
attention from both industry and academia. Despite the significant
progressmadeinrecentstudies,theoverfittingproblem(i.e.,the
generated patch is plausible but overfitting) is still a major and
long-standingchallenge.Therefore,plentyoftechniqueshavebeen
proposed to assess the correctness of patches either in the patch
generation phase or in the evaluation of APR techniques. However,theeffectivenessofexistingtechniqueshasnotbeensystematicallycomparedandlittleisknowntotheiradvantagesanddisadvantages.
To fill this gap, we performed a large-scale empirical study in this
paper. Specifically, we systematically investigated the effectiveness
of existing automated patch correctness assessment techniques,
includingbothstaticanddynamicones,basedon902patchesau-
tomaticallygeneratedby21APRtoolsfrom4differentcategories.
âˆ—Thefirsttwoauthorscontributedequallytothiswork,andMingWenisthecorre-
sponding author.
â€ NationalEngineeringResearchCenterforBigDataTechnologyandSystem,Services
Computing Technology and System Lab, Hubei Engineering Research Center on Big
Data Security, HUST, Wuhan, 430074, China
â€¡Shenzhen HUST Research Institute, Shenzhen, 518057, China
Â§Cluster and Grid Computing Lab, HUST, Wuhan, 430074, China
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation
on the first page. Copyrights for components of this work owned by others than ACMmustbehonored.Abstractingwithcreditispermitted.Tocopyotherwise,orrepublish,topostonserversortoredistributetolists,requirespriorspecificpermissionand/ora
fee. Request permissions from permissions@acm.org.
ASE â€™20, September 21â€“25, 2020, Virtual Event, Australia
Â© 2020 Association for Computing Machinery.
ACM ISBN 978-1-4503-6768-4/20/09...$15.00
https://doi.org/10.1145/3324884.3416590Ourempiricalstudyrevealedthefollowingmajorfindings:(1)static
codefeatureswithrespecttopatchsyntaxandsemanticsaregener-allyeffectiveindifferentiatingoverfittingpatchesovercorrectones;
(2) dynamic techniques can generally achieve high precision while
heuristics based on static code features are more effective towards
recall; (3) existing techniques are more effective towards certain
projectsandtypesofAPRtechniqueswhilelesseffectivetotheoth-ers;(4)existingtechniquesarehighlycomplementarytoeachother.Forinstance,asingletechniquecanonlydetectatmost53
.5%ofthe
overfitting patches while 93 .3% of them can be detected by at least
onetechniquewhentheoracleinformationisavailable.Basedon
our findings, we designed an integration strategy to first integrate
static code features via learning, and then combine with othersby themajority voting strategy. Our experiments show that the
strategycanenhancetheperformanceofexistingpatchcorrectness
assessment techniques significantly.
CCS CONCEPTS
â€¢Software and its engineering â†’Software verification and
validation; Software testing and debugging.
KEYWORDS
Patch correctness, Program repair, Empirical assessment.
ACM Reference Format:
Shangwen Wang, Ming Wen, Bo Lin, Hongjun Wu, Yihao Qin, Deqing
Zou, Xiaoguang Mao, and Hai Jin. 2020. Automated Patch Correctness
Assessment: HowFar areWe?.In 35th IEEE/ACMInternational Conference
onAutomatedSoftwareEngineering(ASEâ€™20),September21â€“25,2020,Virtual
Event, Australia. ACM, New York, NY, USA, 13pages.https://doi.org/10.
1145/3324884.3416590
9682020 35th IEEE/ACM International Conference on Automated Software Engineering (ASE)
1 INTRODUCTION
Automated Program Repair (APR) has gained huge attention from
both industry and academia recently. Over the years, substantial
APRtoolshavebeenproposed[ 27,38,44,46,47,74,77,80,82]with
theaimtoreducetheexcessivelyhighcostinbugfixing.APRtools
have shown to be promising towards both practical significance
andresearchvalue.Forinstance,SapFix,whichwasproposedby
Facebook to automatically generate and suggest fixes [ 52], has
already been deployed inreal products that collectively consist of
millionsoflinesofcodeandareusedbymillionsofusersworldwide.
Despite the tremendous effectiveness achieved, existing APR
toolsstillfaceasignificantandlong-standingchallenge: theover-
fitting problem [37,68,71]. The overfitting problem arises when
themeasurementsusedtoassessthecorrectnessofanautomated
generatedpatchisimperfect.Duetotheabsenceofformalspeci-
fications of the desired behavior, most of the APR tools leverage
the developer-provided test suite as partial specifications to assess
whether a patch is correct currently. Such a measurement assumes
a patch that passes all the test cases to be correct, and incorrect
otherwise. However, test suites in real world projects are often
weak and inadequate [ 26,64], and thus a patched program passing
all the tests might be simply overfitting to the test suite and still be
faulty. Later on, people denote a patch that passes the test suite as
aplausible patch[64].Aplausiblepatchthatindeedfixesthetarget
bug is deemed correct, otherwise is regarded as an overfitting patch.
As revealed by recent studies [ 26,37,64], existing APR techniques
are widely suffering from the overfitting problem, that is they gen-
erate more overfitting patches than correct ones on real bugs, thus
leading to a low precision of their generated patches.
Thelowprecisionofthegeneratedpatchessignificantlyaffected
the practical usefulness of existing APR techniques. Therefore,
growingresearcheffortshavebeenmadetoidentifycorrectpatchesamongplausibleonesautomatically[
33,34,70,77,79,81,85,86,88].
Forinstance, DiffTGen wasinitiallyproposedtoidentifyoverfitting
patchesthroughautomatedtestgenerationbasedonthedeveloper-
providedpatch[ 79],whichisdenotedasthe oracle patch inthis
study. It first tries to enhance the adequacy of the provided testsuite via generating an extra set of test cases, and then assumes
ageneratedpatchwhosetestoutcomebasedonsuchtestsdiffers
fromthatoftheoraclepatchtobeanoverfittingpatch.Toeaseour
presentation, we denote those techniques designed for Automated
Patch Correctness Assessment as APCAtechniques in this study.
ExistingAPCAtechniquesdifferentiatethemselvesindiversede-
signspaces.First,theycanbeeither staticordynamicdependingon
whethertheyexecutetestcases.Statictechniquesprioritizeorfilter
out incorrect plausible patches via analyzing the characteristics of
patchesstatically(e.g., Anti-patterns [70]).Onthecontrary,dy-
namictechniquesgenerallyleverage automatedtestgeneration tools,
suchasRandoopandEvosuite,toidentifycorrectpatchesamong
plausible ones (e.g., DiffTGen [79],PATCH-SIM [81]). Second, they
canbedifferentiatedinwhethertheoraclepatchisrequired.Thosetechniquesrequiretheoraclepatch,suchas
DiffTGen [79],runau-
tomatedtestgenerationtoolsbasedonthe oracleprogram (i.e.,the
programafterapplyingtheoraclepatch),andthenleveragethose
automated generated tests to identify overfitting patches. Thosetechniques do not require the oracle patch (e.g.,
PATCH-SIM [81]andOpad[86])generateextratestsbasedonthebuggyversion,and
then leverage other information or certain heuristics as the oracle
to identify overfitting patches. Techniques designed without theoracle patch can be applied to the process of
patch generation
with the aim to prioritize and filter out overfitting patches, and
thustoenhancetheprecisionofAPRtechniques[ 77,81].Onthe
contrary,techniquesrequiringtheoraclepatchareoftenusedfor
patch evaluation ,thatistoassesstheeffectivenessofAPRtech-
niques[33,88].BothofthesetwodirectionsofAPCAtechniquesare
reportedtobesignificanttotheAPRcommunity[ 33,79,81,86,88].
Forinstance,manuallyannotatingthecorrectnessofpatchesissub-jectiveandratherexpensiveasreportedbyexistingstudies[
33,88],
and thus APCA techniques, with the oracle information, are useful
to facilitate the evaluation of APR techniques.
Althoughhugeeffortshavebeenmadetowardsautomatedpatch
correctness assessment, the effectiveness of existing techniques
hasnotbeensystematicallystudiedandcompared.Besides,little
is known to their advantages and disadvantages. A recent study
reported that DiffTGen andRandoop can only identify fewer than
a fifth of the overfitting patches through an empirical study based
on 189 patches [ 33]. However, the behind reasons and the effec-
tiveness of other advanced APCA techniques remain unknown.
Therefore, there is an urge need for a comprehensive empirical
study comparing and analyzing the effectiveness of all the state-
of-the-artAPCAtechniquesbasedonalargernumberofpatches.
Such a study is necessary and essential, which can help us find the
answers to important questions when designing APCA techniques.
Forinstance,whetherexistingAPCAtechniquesaremoreeffective
towardscertaintypesofpatchesorAPRtechniques?Besides,are
existing techniques complementary to each other and whether the
integrationofthemcanenhancetheperformance?Answeringsuchquestionscanguideresearcherstodesignmoreeffectivetechniques.
This study aims to bridge this gap, which performs a system-
atic empirical study for automated patch correctness assessment
including9differenttechniquesand3heuristicsbasedon8static
code features on the most comprehensive patch benchmark so far
(i.e.,902patchesintotal).Viainvestigatinghowmanyoverfitting
patches can be identified by each APCA technique, we understood
theeffectivenessofexistingtechniques,includingtheadvantages
and disadvantages, and pointed out how they can be improved.
For instance, PATCH-SIM can be enhanced by test case purification
whileDaikoncanbeimprovedbyconsideringthecharacteristicsof
invariants. Our study also makes the following important findings:
F1:Heuristicsbasedonstaticfeaturesgaugingpatchsyntaxandsemanticsaregenerallyeffectiveindifferentiatingoverfitting
patches over correct ones, which can achieve high recalls.
F2:DynamicAPCAtechniquescanachievehighprecisionwhilemostofthemwilllabelcorrectpatchesasoverfitting.Besides,those techniques with the oracle information are generating
a fewer number of false positives (i.e., fewer than 10 .0%).
F3:ExistingtechniquesaremoreeffectivetowardscertainprojectsandtypesofAPRtechniqueswhilelesseffectivetotheothers
(e.g.,project Langandconstraint-based APRtechniquesfor
staticcodefeatures;andproject Closureandlearning-based
APR techniques for dynamic ones).
969F4:Existingtechniquesarehighlycomplementarytoeachother.
A single technique can only detect at most 53 .5% of the
overfittingpatcheswhile93 .3%ofthemcanbedetectedbyat
leastonetechniquewhentheoracleinformationisavailable.
Based on our findings, we designed an integration strategy to first
integrate different static code features via leveraging machine-
learningmodels,andthencombinethelearnedmodelwithother
APCA techniques by the majority voting strategy. Our experiments
show that the strategy can enhance the performance of existing
APCA techniquessignificantly. Specifically,our strategy caniden-
tify66.5%oftheoverfittingpatcheswhilepreserveahighprecision
of99.1%withtheoracleinformation.Suchahighrecalloutperforms
the current most effective technique by 25.0%.
2 BACKGROUND AND RELATED WORKS
This section presents the background and related works.
2.1 Automated Program Repair Techniques
Generally,APRtechniquescanbedividedintotwocategorieswhich
aresearch-based techniques andsemantic-based techniques, respec-
tively [39]. Search-based techniques aim to search for candidate
patches within a predefined space, with or without templates as
the guidance for code transformation [ 45]. When without tem-
plates, the applied techniques (also known as heuristic-basedap-
proach)leveragegeneticprogramming[ 38],randomsearch[ 63],
or multi-objectivegenetic programming[ 90] toguide thesearch of
correctpatches.Researchersalsominefixtemplates(alsoknown
astemplate-based approach ) from history of large-scale open-
source projects [ 29,36] or from static analysis tools [ 42]. These
templates are applied to generate patches, aiming at producing
morecorrectpatches[ 27,32,41,43,77].Semantic-basedtechniques
(also known as constraint-based approach ) synthesize a patch
directly using semantic information via symbolic execution and
constraint solving [ 16,34,56,58,82,83]. These approaches usually
focusonsingleconditionalstatementsorassignmentstatements
[56,58,83]. Recently, approaches have been proposed to generate
patches directly via using deep learning models (e.g., SequenceR
[10]), which is denoted as learning-based approach [10,45,65].
2.2 The Overfitting Problem
Traditionally,apatchisconsideredascorrectifitcanpassallthe
test cases [ 38,63,75]. Qi et al. [ 64] first investigated the quality
ofautomatedgeneratedpatches.Longetal.[ 48]firstpointedout
thatconventionalcriteriontoexaminepatchcorrectnessisques-
tionable since test suites in practice are usually inadequate to guar-
anteethecorrectnessofthegeneratedpatches.Asaresult,those
generatedpatchesthatpassallthetests(alsoknownas plausible
patches) may fix the bug incorrectly; not fix the bug completely or
breaksomeintendedfunctionalities,andthusbecome overfitting
patches[79,89]. After that, researchers begin to adopt plausibil-
ity (i.e., how many plausible patches an APR tool can generate)and correctness (i.e., how many generated plausible patches arereally correct) as metrics for assessing the repairability of APRtools [
9,23,25,27,32,42,43,46,77]. Meanwhile, an increasing
numberofstudiesaimedatdetectingoverfittingpatcheshavebeen
proposed [ 70,72,79,81,85,86], including DiffTGen ,PATCH-SIMTable 1: Selected Techniques in this Study.
OracleRequired NoOracle Required
DynamicEvosuite [19],Randoop [59],
DiffTGen [79],Daikon[18]PATCH-SIM [81],E-PATCH-SIM,
R-Opad[86],E-Opad[86]
Static/circlemultiplytext.1 ssFixâ€ [80],CapGenâ€ [77],
Anti-patterns [70],S3â€ [34]
â€ WeusessFix,CapGen,and S3todenotetheheuristicsbasedonthecorrespondingstaticfeatures.
andsoon.Recently,Yuetal.[ 89]introducedamethodtoclassify
overfittingpatches intotwocategorizations (i.e., incompletefixing
andregression introduction). However, their method can only be
appliedtothoseoverfittingpatchesthatcanbedetectedbythegen-
eratedtestcases.Consequently,itcannotbeappliedtothewhole
set of patches included in this study. As a result, we do not analyze
the performance (i.e., precision andrecallas we will introduce in
Section3.3) of different APCA techniques from this perspective.
2.3 Empirical Studies in APR
In recent years, plenty of empirical studies have been conductedconcerning different aspects of APR [
15,41,45,73]. For instance,
Liuetal.foundthatFaultLocalization(FL)strategies[ 76,78]uti-
lized by different APR techniques are diverse and the FL results
can significantly influence the repair results [ 41]. Long et al. inves-
tigated the search space of repair tools and revealed that correctpatches are sparse while the overfitting patches are much moreabundant [
48], which is further confirmed by [ 45]. Durieux et al.
[15]andWangetal.[ 73]focusedonbenchmarkoverfittingproblem
andreachedtheconclusionthatmorebugsshouldbeconsidered
when evaluating APR techniquesâ€™ performance. Recently, Lou et al.
exploredtheideaof unifieddebugging tocombinefaultlocalization
andprogramrepairintheotherdirectiontoboosttheperformance
of both areas[5, 49].
3 STUDY DESIGN
This section presents the design details of this empirical study.
3.1 APCA Techniques Selection
Our study selects all the state-of-the-art techniques targeting as-
sessingpatchcorrectnessofJavaprogram.Thisstudyfocuseson
Java since it is the most targeted language in the community of
program repair. Furthermore, there is a wide range of APR toolsthathavebeenevaluatedinreal-worldJavaprograms,providing
on-hand patches for our study. Specifically, we consider the living
review of APR by Monperrus [57] to identify these techniques.
3.1.1 Inclusion. Overall,ourstudytakestotally9APCAtechniques
and 3 heuristics based on 8 static code features into consideration,
which can be classified from two aspects as mentioned in Introduc-
tion.First,itcanbecategorizedbywhetheritrequirestheoracle
patch.Thiscorrespondstotwodifferentapplicationscenarios:those
do not require oracles can be integrated into patch generation
process and thus help to increase the precision of APR tools while
thoserequireoraclesareusuallyusedfor patch evaluation that
istohelpassesstheeffectivenessofAPRtechniques.Second,itcan
beeitherdynamicorstatic,whichisdifferentiatedbywhetheritneeds to execute test cases. Table 1lists the categorized selected
techniques and the following presents the detail of each of them.
970Simple Test Case Generation: Theintuitionofthismethodis
straightforward:sincemostoverfittingpatchesaregenerateddueto
theinadequacyoftestsuitesprovidedbyreal-worldprograms[ 48],
researchers proposed to utilize automated test generation tools to
generate independent test suites based on the oracle program to
examine whether patches are overfitting [ 35,68,69]. If a plausible
patch fails in any of these test cases, it is detected as overfitting.
Followingthepreviousstudies[ 66,88],inourexperiment,weselect
Randoop [59] andEvosuite [19] as the test generation tools since
they are widely-used in software testing tasks [20, 21,81].
DiffTGen: DiffTGen is a tool that identifies overfitting patches
throughtestcasegeneration[ 79].Thetoolemploysanexternaltest
generator(i.e.,Evosuite)togeneratetestinputwhichisdesigned
touncoverthesyntacticdifferencesbetweenthepatchedandthe
originalbuggyprogram(notethatthisisthedifferencebetweenthis
toolandsimpletestcasegeneration wherethetestsaregenerated
randomly). To achieve so, DiffTGen creates an extended version
of the patched program with dummy statements inserted as thecoverage goals to advocate test generator. When executing thegenerated test inputs on the buggy and the patched programs, if
the output of the patch is not the same with that of the oracle, it is
regarded as overfitting.
Daikon: Some recent studies concentrate on applying program
invariant to APR tasks [ 8,14,85]. Specially, Yang et al. focus on
the impacts on program runtime behaviors from different patches
[85]. They found that a large amount (92/96) of overfitting patches
willexposedifferentruntimebehaviors(capturedbyDaikon[ 18],
aninvariantgenerationtool)comparedwiththeircorresponding
correct versions. Based on their findings,in this study, we adopt a
simpleheuristicthatistoseeiftheinferredinvariantofageneratedpatchisdifferentfromthatoftheoracleprogram.Ifdifferenceexists,
wethenconsideritasoverfitting.Intherestofthispaper,weuse
Daikonto represent this method.
Opad:Opadusesfuzzingtestingtogeneratenewtestcasesbased
on the buggy program, it then uses two predetermined oracles thatpatchesshouldnotintroducenewcrashormemory-safetyproblems
to detect overfitting patches [ 86]. To apply it on Java, we adopt the
method provided by a recent study [ 81] that is to uniformly detect
whether a patch introduces any new runtime exception on test
runs.Note thattheoriginal fuzztechnique doesnotwork onJava
programs. As a result, in this study, we use Randoop and Evosuite
to generatetest caseson thebuggy programsand denotethem as
R-OpadandE-Opadrespectively.
PATCH-SIM: PATCH-SIM isasimilarity-basedpatchvalidation
technique [ 81] which does not require the oracle information. It
firstutilizesatest generationtool(Randoopintheoriginalstudy)
to generate new test inputs. It then automatically approximates
without the oracle under the hypothesis that tests with similar
executionsarelikelytohavethesameresults.Finally,itusesthe
enhancedtestsuitetoassesspatchcorrectnessconsideringthata
correct patch may behave similarly on passing tests while differ-ently on failing tests compared with the buggy program. In our
study, to better explore the performance of this technique, we also
implemented another version of this tool by replacing the adopted
RandoopwithEvosuitefortestgeneration.Weuse E-PATCH-SIM
to represent our Evosuite-based PATCH-SIM .Anti-patterns: Anti-patterns isoriginallydesignedforClan-
guage [70]. The authors defined seven categories of program trans-
formation(fordetails,cf.Table1in[ 70]).ToapplyitonJava,we
followthestrategyadoptedbyarecentstudy[ 81]thatisifthecode
transformation in the patch falls into any category, it is considered
as overfitting.
Static CodeFeatures: Many studies have proposed to leverage
static code features to prioritize correct patches over overfitting
ones [34,77,80] and a recent study [ 2] demonstrates the effective-
ness of these features. For instance, ssFix[80] proposed to utilize
thetoken-based syntax representation of code to identify syntax-
related code fragments with the aim to generate correct patches.
S3proposed six features to measure the syntactic and semantic
distance between a candidate solution and the original buggy code
[34],andthenleveragedsuchfeaturestoprioritizeandidentifycor-
rectpatches.Thesefeaturesarenamedas ASTdifferencing, cosine
similarity, localityofvariablesandconstants, modelcounting, output
coverageandanti-patterns. CapGenproposedthreecontext-aware
models to prioritize correct patches over overfitting ones, which
arethegenealogymodel,variablemodel,anddependencymodelre-
spectively [ 77]. Although such features are often used to prioritize
overfittingpatchesduringpatchgeneration,westillincludetheminthisstudywiththeaimtoinvestigatepatchcorrectnessassessment
from the view of static features. Note that for S3, the proposed
model counting can only be applied to Boolean expressions, and
output coverage can only be applied to program-by-examples based
APR.Therefore,theycannotbegeneralizedtoallthepatchesgener-
ated by a wide range of APR techniques. Besides, Anti-patterns
is used as a stand-alone technique in this study. As a result, we
exclude those features for S3in this study, and Table 2displays the
detailsoftheselectedeightfeaturesintotal.Wefollowtheoriginal
studies [34,77,80] to compute the values for each feature, and we
do not list the formulas in detail due to page limit.
3.1.2 Exclusion. Inour study,wealsodiscard somemethodsthat
have been exploited by previous studies.
Wenotethatresearchersalsoutilize AgitarOne [1],anothertest
generation tool which is reported to be able to achieve 80% code
coverage,togeneratetests[ 66].Wedonottakeitintoconsidera-
tionsinceitisacommercialproduct. KATCH[53]andKLEE[7]ar e
both test generators which leverage symbolic execution and can
achieve high coverage. We discard them since they only supportC language currently while we focus on Java.
UnSatGuided is a
method that utilizes test case generation to alleviate overfittingin test suite based program repair [
89]. We discard this method
since that it only works for synthesis-based repair techniques such
asNopol,andthuscannotbegeneralizedtoawiderangeofAPR
techniques selected inthis study. Arecent studyalso utilizes code
embedding technique toidentify correct patches[ 13]. However, it
requirestremendouseffortstotraintheembeddingmodelandthusisdiscardedinthisstudy.Besides,wenotethatYeetal.proposedtouse4,199staticcodefeaturestoidentifyoverfittingpatchesrecently[
87]. However, their tool and data is not publicly available. Besides,
theirproposedfeaturesareatomiconeswhichareencodedatthe
levelofASTwhilethoseselectedinthisstudyarehigh-levelfeatures
used to encode code syntax and semantics. Therefore, we exclude
this method in our empirical study. We also note there are many
971Table 2: Static Code Features Used to Prioritize Correct Patches Over Overfitting Ones
ShortName Metric Source Description
TokenStrct Structural token similarity ssFix [80] The similarity between the two vectors representing the structural tokens obtained from the buggy code chunk and the generated patch.
TokenConpt Conceptualtokensimilarity ssFix [80] Thesimilaritybetweenthetwovectorsrepresentingtheconceptualtokensobtainedfromthebuggycodechunkandthegeneratedpatch.
ASTDist AST Difference S3 [34] The number of the AST node changes introduced by the patch.
ASTCosDist Cosine Similarity Distance S3 [34] One minus the cosine similarity between the vectors representing the occurrences of distinct AST node types before and after the patch.
VariableDist Localityofthevariablesand
constantsS3 [34] The distance is measured by the Hamming distance between the vectors representing the locations of variables and constants.
VariableSimi Variable Similarity CapGen[77] The similarity between the variables involved in the original buggy code element and the applied patch.
SyntaxSimi Genealogy Similarity CapGen[77] The similarity between the syntactic structures (the ancestor and sibling nodes of the corresponding AST) of the original buggy codeelements and the applied patch.
SemanticSimi
Dependency Similarity CapGen[77] Thesimilaritybetweenthecontextualnodesaffectedbythebuggycodeelementsandtheappliedpatchwithrespecttotheirdependencies.
other test generation tools in the literature such as JCrasher [12]
andTestFul[4].Wediscardthemsincetheyarenotthestate-of-
the-artandmanyofthemarenolongermaintained(e.g., TestFul).
Our selected subjects (i.e., Randoop andEvosuite ) are the most
widely-used open-source tools to generate tests [11, 21,33,67].
3.2 Patch Selection
This section presents the large-scale patches selected in our study.
3.2.1 Patch Benchmark. In this study, we focus on the patches
generatedforawidelyusedbenchmarkbyexistingAPRtechniques
which is Defects4j [28]. Specifically, we select all the patches
prepared by a recent large-scale study [ 45] where 16 APR systems
are evaluated under the same configuration. To better explore the
overfitting problem, we also include 269 patches collected by Ye
et al. [88] that were not included in [ 45], including the ones of
JAID[9],SketchFix [25],CapGen[77],SOFix[46],andSequenceR
[10]. The following two steps are performed based on the selected
patches. First, we removed those patches generated for Mockito,
assimilarlyadoptedby[ 81],sincesomeofourstudiedsubject(e.g.,
Randoop)cannotgenerateanyvalidtestforthisproject.Second,we
performed a plausibility check to see whether the selected patches
are indeed plausible (i.e., can be compiled and pass all the original
test suite). To achieve so, we ran each patch on the original testsuites again. In total, we discarded 8 patches after the two steps,
2generatedfor Mockitoand6failedintheplausibilitycheck(we
haveconfirmedthiswiththeauthorsof[ 45]).Ourpatchbenchmark
is: (1)large-scale: this benchmark contains in total 902 patches for
correctness assessment, and such a number is around 30% more
thantherecordingnumber(i.e.,[ 87]contains713examinedpatches
for correctness) in the literature. (2) of high coverage w.r.t APR tools:
this benchmark contains the patches generated by 21 distinct APR
tools which can be mainly divided into four categories, namely,
heuristic-based, constraint-based, template-based,and learning-based,
assummarizedbyarecentstudy[ 45].Table3showsthedetailed
informationoftheseAPRtools.(3) ofhighcoveragew.r.tdistinctbugs:
this benchmark contains the patches generated for 202 different
bugsinDefects4j,accountingforoverhalfofthebugsinthedataset
(202/395).Suchahighcoverageprovidesgreatpatchdiversityfor
evaluation. Although the learning-based category contains only
SequenceR , it contains 73 patches in total, which is almost the
largest number for a single APR tool.
3.2.2 Patch Sanity Check. The precise oracle information of the
selectedpatches(i.e.,whetherapatchisoverfittingorcorrect)is
criticalinfairlycomparingtheeffectivenessofAPCAtechniques.
Thelabelinformationoftheaforementionedcollectedpatchesis
annotated by the associated researchers manually. However, asTable 3: Covered APR Tools in Our Benchmark.
Category APR Tools for Java Programs
Heuristic-basedjGenProg [54], jKali [54], jMutRepair [54], SimFix [27], ARJA [90],
GenProg-A [90], Kali-A [90], RSRepair-A [90], CapGen [77].
Constraint-basedDynaMoth [16], Nopol [83], ACS [82],
Cardumen [55], JAID [9], SketchFix [25].
Template-based kPAR [41], FixMiner [32], AVATAR [ 42], TBar [43], SOFix [46].
Learning-based SequenceR [10].
previous studies have pointed out [ 33,88], author annotation may
produce wrong labels due to subjectivity (i.e., assessing an over-fitting patch as a correct one). To reduce such bias in our study,
we further performed a sanity check to examine the correctness
of the collected patches. To achieve such a goal, we followed the
strategies adopted by a recent study [ 88]. Specifically, we adopted
Randoop and Evosuite to generate extra test cases automatically
basedontheoracleprogramofthebug(theprogramafterapplying
the developer-provided patch), and then executed such extra tests
againstthepatchedversioncollectedinourdatasetthataremarkedascorrectbytheauthors.Wethenexaminediftherewereanygener-atedtestthatpassedontheoracleprogrambutfailedonthepatched
version.Suchcasesindicatedthatthepatchannotatedascorrect
by the authors might be actually overfitting. Therefore, we further
manuallycheckedeachofthem,toseewhethertheyareoverfitting
via understanding the programs. Three authors were involved and
the process ended when they reached consensus. A patch is still
consideredascorrectifthreeauthorsadmititiscorrect.Otherwise,
we sendthe patchwith thegenerated teststo theoriginal authors
tosee iftheyagreewith ourjudgement.Wedeema correctpatch
in our dataset is actually overfitting if the original authors also
confirm with our judgment. In total, our sanity check identified 12
patches that are mistakenly labeled as correct. Due to page limit,
wedonotanalyzethe12cases inthispaper.Detailscanbefound
in our project page at http://doi.org/10.5281/zenodo.3730599.
Aftertheprocessofthesanitycheck,thepatchescanbeprecisely
classified to correct patches, which are denoted as ğ‘ƒğ‘ğ‘œğ‘Ÿğ‘Ÿğ‘’ğ‘ğ‘¡, and
overfittingpatches,whicharedenotedas ğ‘ƒğ‘œğ‘£ğ‘’ğ‘Ÿğ‘“ğ‘–ğ‘¡ğ‘¡ğ‘–ğ‘›ğ‘” .Intotal,there
are 654ğ‘ƒğ‘œğ‘£ğ‘’ğ‘Ÿğ‘“ğ‘–ğ‘¡ğ‘¡ğ‘–ğ‘›ğ‘” patches and 248 ğ‘ƒğ‘ğ‘œğ‘Ÿğ‘Ÿğ‘’ğ‘ğ‘¡ones.
3.3 Research Questions
BasedontheselectedAPCAtechniquesandthepreciselylabeled
patches(i.e., ğ‘ƒğ‘œğ‘£ğ‘’ğ‘Ÿğ‘“ğ‘–ğ‘¡ğ‘¡ğ‘–ğ‘›ğ‘” andğ‘ƒğ‘ğ‘œğ‘Ÿğ‘Ÿğ‘’ğ‘ğ‘¡),weseektoanswerthefol-
lowing research questions (RQs) with the aim to investigate and
enhance the effectiveness of existing assessment techniques:
(1)RQ1: How effective are existing static code features in
prioritizing correct patches? We first systematically inves-
tigate the effectiveness of each individual static code featureas summarized in Table 2in prioritizing correct patches over
overfittingones.To answerthisquestion,wecomputethe val-
ueswithrespecttoeachofthemforallthepatches,andthen
972compare whether the values obtained over the correct patches
aresignificantdifferentfromthoseobtainedovertheoverfitting
ones.Ifsignificanceobserved,thedesignedfeatureispromisingindifferentiatingcorrectpatchesfromoverfittingones.Besides,
we also investigate whether the effectiveness of existing fea-
turesareaffectedbydifferentprojectsordifferenttypesofAPR
tools. Answering such a question can guide us better apply ex-
istingtechniquestodifferentdomains,andunderstandtowards
which direction should existing techniques be improved.
(2)RQ2: How effective are existing techniques in identify-ing overfitting patches?
We systematically investigate the
performanceofthestate-of-the-artAPCAtechniquesondetect-ingoverfittingpatches,andwhethertheyarecomplementarytoeach other. This question is rather essential to provide valuableguidanceforthedesignoffuturemethods.Specifically,wemea-sure theprecision and recallto answer thisquestion, which aredefinedbythefollowingmetrics:
TruePositive(TP): Anover-
fitting patch in ğ‘ƒğ‘œğ‘£ğ‘’ğ‘Ÿğ‘“ğ‘–ğ‘¡ğ‘¡ğ‘–ğ‘›ğ‘” is identified as overfitting. False
Positive(FP): Acorrectpatchin ğ‘ƒğ‘ğ‘œğ‘Ÿğ‘Ÿğ‘’ğ‘ğ‘¡isidentifiedasoverfit-
ting.FalseNegative(FN): Anoverfittingpatchin ğ‘ƒğ‘œğ‘£ğ‘’ğ‘Ÿğ‘“ğ‘–ğ‘¡ğ‘¡ğ‘–ğ‘›ğ‘”
is identified as correct. True Negative (TN): A correct patch
inğ‘ƒğ‘ğ‘œğ‘Ÿğ‘Ÿğ‘’ğ‘ğ‘¡is identified as correct.
ğ‘ƒğ‘Ÿğ‘’ğ‘ğ‘–ğ‘ ğ‘–ğ‘œğ‘› =ğ‘‡ğ‘ƒ/(ğ‘‡ğ‘ƒ+ğ¹ğ‘ƒ) (1)
ğ‘…ğ‘’ğ‘ğ‘ğ‘™ğ‘™ =ğ‘‡ğ‘ƒ/(ğ‘‡ğ‘ƒ+ğ¹ğ‘) (2)
Ithasbeenemphasizedbyrecentstudies[ 81,87,89]thatAPCA
techniques need to avoid dismissing correct patches. Therefore,
we conjecture an APCA technique effective if it can generate
few false positives while preserve a high recall. As mentioned
inSection 3.1,existingAPCAtechniquescanbecharacterized
in different aspects, such as dynamic orstatic,with oracle or
without oracle. In this RQ, we will also investigate the effects of
such design spaces on the effectiveness of existing techniques.
(3)RQ3. Can we enhance the effectiveness of existing tech-niques via integrating static features and dynamic tech-niquestogether?
Basedontheexperimentaloutputsobtained
from the previous RQs, we further seek to investigate whether
combining static and dynamic techniques can achieve better
performance in identifying overfitting patches.
3.4 Experiment Settings
Alltheexperimentswereperformedonthesameconfiguredservers
withUbuntu18.04x64OSand16GBmemory.Thefollowingpresents
the experimental details for each research question.
3.4.1 RQ1 & RQ2. Static Techniques: The four static tools are
described in Section 3.1.1. Specifically, to apply Anti-patterns on
Java, we follow the heuristic adopted by a recent study [ 81] that
is to manually check whether the patches generated by existing
APRtoolsfallintothesepatterns.Ifageneratedpatchbreaksany
predefined anti-pattern rule, it is considered as overfitting. For the
otherthreeapproaches,wecalculatethevalueofeachindividual
featureasdescribedinTable 2.Then,for ssFixandS3,thesumof
the features is used as the final result while the multiplication is
used forCapGen, as adopted by the original paper [34, 77,79].
Evosuite&Randoop: Werunthesetoolsontheoracleprogram
to generate tests with 30 different seeds with a time limit of 300seconds by following previous studies [ 33,88]. After collecting
those test cases, we run them over the oracle program to eliminate
the impact of flaky tests [ 50]. Any test that fails on the oracle
programwillberemovedfromourtestsuite.Followingtheprevious
studies[66,89],westopthisprocessifalltheoraclepatchespass
thewholetestsuiteforfivetimesconsecutively(notethatthesanity
check introduced in Section 3.2.2also went through such a check).
Afterthisprocess,weobtainatestsuitewithmostflakytestsbeing
removedandthenexecutetheautomatedgeneratedpatchesagainst
it. A patch is considered as overfitting if it fails any of the test case.
Notethatfor125bugsinourstudy,wedirectlyreusethe Evosuite
tests generatedby Ye etal. [ 88] sincethey have alreadygenerated
tests for the 125 bugs and removed the flaky tests. Details of these
bugs can be found in our replication package.
R-Opad&E-Opad :Theexperimentalsettingofthesetwotools
issimilartothatofRandoopandEvosuite.Thedifferenceisthatthe
test cases are generated based on the buggy programs since Opad
doesnotrequiretheoracleinformation.Instead,whenexecuting
the generated tests on the patched programs, a patch is considered
as overfitting if it introduces any runtime exception.
DiffTGen :DiffTGen leverages Evosuite togeneratetestcases.
Inourexperiment,weexecute Evosuite for30timesandthesearch-
ingtimeissetto60secondsforeachround.Weusesuchsettings
since it is reported that such a combination is the optimum [80].
PATCH-SIM & E-PATCH-SIM :We note that the performance
of our server is lower than that used in the original study. We thus
follow the instruction in the project page of [ 81] that is to increase
thetimeoutlimit(3minutesinoriginalstudy)to5minutes.Both
PATCH-SIM andE-PATCH-SIM requiretwothresholdstoclassifythe
generated tests into passing or failing tests (K ğ‘¡) and the generated
patchesintocorrectoroverfittingones(K ğ‘).Weadopttheprede-
fineddefaultvaluesasusedin[ 81].Inthisstudy,wedonotexplore
the setting of different thresholds since it has been illustrated in
[81] that the impact from different thresholds is limited.
Daikon:RunningDaikonisextremelytime-consuming[ 85].To
speedupthepro cess and reduce unnecessary computation, in our
experiment, we only infer invariants by executing test classes that
containthefailingtestcases,asadoptedbythepreviousstudy[ 14].
3.4.2 RQ3. To answer RQ3, we first integrate the eight static fea-
turesasdisplayedinTable 2usinglearning-to-rank strategies.Learn-
ingtoranktechniquestrainamachinelearningmodelforarankingtask[
40],whichhasbeenwidelyusedinInformationRetrievaltasks.
We then integrate the learned model with dynamic techniques via
themajority voting strategy [ 61]. To integrate static features, we
select six widely-used classification models in our study, including
Random Forest [ 6], Decision Table [ 93], J48 [60], Naive Bayes [ 60],
Logistic Regression [ 30], and SMO [ 62]. These are popular classifi-
cationmodelsthatarewildlyusedbyexistingstudies[ 3,24,91,92].
Werandomlyseparatethepatchbenchmarkinto10foldswith
identicalnumberofcorrectandoverfittingpatchesineachfold.We
then use 10-fold cross validation [ 31] to evaluate the performance
of each model. The final performance of each model is summedup over the 10 rounds of each training and testing process. Note
thatourpatchbenchmarkisimbalanced(thenumberofoverfitting
patches is around three times as much as that of correct ones), we
thusadoptthestrategyof cost-sensitivelearning [17]toincreasethe
lossofgeneratinganFPto3timesofthatofgeneratinganFN.Since
973(a) TokenStrct
 (b) TokenConpt
 (c) ASTDist
(d) ASTCosDist
 (e) VariableDist
 (f) VariableSimi
(g) SyntaxSimi
 (h) SemanticSimi
Figure1:MeasurementsonOverfittingPatchesandCorrectPatches
for Different Static Code Features Cross Different Projects
The bar denotes the correct patches while the white bar denotes the
overfitting patches. C+T:Chart + Time; CL:Closure;L:Lang;M:Math;
the 10-fold cross validation involves randomness, we repeated this
process for 20 times and results suggest that the impact caused by
therandomnessislimited.Forinstance,themedianofTPgenerated
byRandom Forest is 580 with a standard deviation of 5.05 while
the median of FP is 90.5 with a standard deviation of 3.17. We thus
report the result of the first experiment, in this paper.
To combine with dynamic techniques, we select the model with
the optimum performance, and integrate it with dynamic ones us-
ingthestrategyofmajorityvoting.AsclassifiedinTable 1,there
are two types of dynamic techniques, the one with the oracle in-
formation and the other without. However, static code features
aredesignedwithouttheinformationoforacle.Tointegratewith
thosetechniquesrequiretheoracleinformation,wealsoconduct
another experiment to leverage the oracle information for staticfeatures. Specifically, we re-compute all the feature values based
on the generated patch and the oracle patch (in the original experi-
ment,thefeaturevaluesarecomputedbasedonthebuggyprogramandthegeneratedpatch),andthenadoptthesamemethodologyasdescribedabovetointegrateallthestaticcodefeatures.Finally,the
model learned with the oracle information is combined with the
dynamic techniques that require the oracle information.
4 STUDY RESULTS
Wenowprovidetheexperimentaldataaswellasthekeyinsights
distilled from our research questions.
4.1 RQ1: Effectiveness of Static Code Features
Figure1shows the measurements over overfitting patches and
correct patches with respect to each individual code feature aggre-
gatedbydifferentprojects.Sincethenumberofpatchescollected
forprojectTimeisnotsufficientforstatisticaldifferencetesting(i.e.,onlysixpatchesintotal),wecombinedthepatchesinprojectTime
andChart,anddenotedthemas"C+T"inthefigures.Wecansee
thatforfeatures TokenStrct, TokenConpt, VariableSimi, SyntaxSimi
andSemanticSimi, the values obtained over correct patches are
generallyhigherthanthoseobservedoveroverfittingones.Such
(a) TokenStrct
 (b) TokenConpt
 (c) ASTDist
(d) ASTCosDist
 (e) VariableDist
 (f) VariableSimi
(g) SyntaxSimi
 (h) SemanticSimi
Figure2:MeasurementsonOverfittingPatchesandCorrectPatches
for Different Static Code Features Cross Different APR Techniques
The bar denotes the correct patches while the white bar denotes the
overfitting patches. C:Constraint-based APR; H:Heuristic-based APR; LE:
Learning-based APR; T:Template-based APR;
resultsindicatethatcorrectpatchesaremorelikelytopreservehigh
similaritieswithrespecttotheirsyntaxandsemanticscompared
with the buggy program. For features ASTDist, ASTCosDist and
VariableDist,thevaluesobtainedovercorrectpatchesaregenerally
lowerthanthoseobservedoveroverfittingones.Suchresultsreveal
thatcorrectpatchesoftenmakeasmallnumberofmodifications
(i.e., as measured by ASTDist), and do not often change the loca-
tionsofvariables/constants(i.e.,asmeasuredbyVariableDist),as
compared to overfitting patches.
Toinvestigatewhethersuchdifferencesobservedbetweencor-
rect and overfitting patches are significant, we further performed a
one-sided Mann-Whitney U-Test [ 51] on the results collected from
each project,and Table 4displays thep-values. Aswe canseefrom
thetable,formostofthe cases, thedifferencesaresignificant(i.e.,
p-valueâ‰¤0.05),whichindicatesthatexistingstaticcodefeatures
areeffectiveindifferentiatingoverfittingpatchesfromcorrectones.However, the measurements over the Lang projects with respect tosixoutoftheeightfeaturesareinsignificant.Sucharesultindicates
that existing features are less effective to prioritize patches gener-
atedforprojectLang.Wefurtherinvestigatedthebehindreasonandfoundthatthebuggylocationsofthisprojectaremostlyconcerned
withconditionexpressions,andtheoverfittingpatchesareoften
generatedviamodifyingoperatorsinsuchexpressions.Listing 1
showssomeexamplesofsuchoverfittingpatches.Unfortunately,
existingcodefeaturesareunabletocapturethecharacteristicsof
suchdifferencesembeddedinoperators.Asaresult,thedifferences
measuredovertheoverfittingpatchesandthoseoverthecorrect
patches are insignificant.
1// An overfitting patch generated for Lang 22 by jMutRepair
2- if (Math.abs(u) <= 1 || Math.abs(v) <= 1) {
3+ if (Math.abs(u) <= 1 && Math.abs(v) <= 1) {
4
5// An overfitting patch generated for Lang 51 by TBar
6- if (ch == 'y') {
7+ if (ch <= 'y') {
Listing 1: Overfitting Patches Generated for the Lang Project
974Table 4: P-values over All the Static Code Features
Feature NameProjects Types of APR Technique
C+T CL L M C H LE T
TokenStrct 0.0002 0.0165 0.0000 0.0000 0.0000 0.0011 0.0335 0.0000
TokenConpt 0.0000 0.2003 0.0001 0.0000 0.0006 0.0004 0.0645 0.0000
ASTDist 0.0000 0.0013 0.5245 0.0001 0.0116 0.0000 0.0213 0.0570
ASTCosDist 0.0000 0.0001 0.4123 0.0005 0.0600 0.0000 0.0032 0.0049
VariableDist 0.0045 0.0016 0.9493 0.0000 0.2830 0.0001 0.0001 0.0330
VariableSimi 0.0000 0.0001 0.4194 0.0011 0.6681 0.0000 0.0067 0.0011
SyntaxSimi 0.0000 0.0098 0.7648 0.0000 0.0703 0.0000 0.0007 0.0066
SemanticSimi 0.0000 0.0039 0.3331 0.0002 0.0675 0.0000 0.0045 0.0043
0.0000 denotes the p-value is less than 0.00005
Table 5: Effectiveness of each APCA Technique.
APCA TP FP TN FN Precison Recall
Evosuite 350324530499.15% 53.52%
Randoop 221624243397.36% 33.79%
DiffTGenâ€ 184523241797.35% 30.62%
Daikonâ€ 3373816612089.87% 73.74%
R-Opad 67 0 248 587 100.00% 10.24%
E-Opad 92 0 248 562 100.00% 14.07%
PATCH-SIMâ€ 249 51 186 392 83.00% 38.85%
E-PATCH-SIMâ€ 166 36 202 477 82.18% 25.82%
Anti-patterns 219 37 211 435 85.55% 33.49%
S3 516 135 113 138 79.26% 78.90%
ssFix 515 138 110 139 78.87% 78.75%
CapGen 506 140 108 148 78.33% 77.37%
The green cell denotes the technique requires the oracle information. The bold name
means the technique is dynamic. â€ For these tools, results might not be generated for certain
patches. The reasons for non-result generation are explained in detail in this Section.
We performed similar analysis with respect to different types
ofAPRtechniques.Figure 2showstheresultsandTable 4shows
thecorrespondingp-values.Aswecanseefromthefigures,similar
resultscanbeobservedcomparedwiththoseobtainedwithrespect
todifferentprojects.Specifically,existingstaticcodefeaturesare
generallyeffectiveinprioritizingoverfittingpatchesovercorrect
ones with respect to different types of APR techniques, and thedifferences are mostly significant as shown in Table 4. However,
insignificanceisfrequentlyobservedforthosepatchesgeneratedbyconstraint-based APRtechniques.Weinvestigatedthebehindreason
andfoundthatpatchesinthistypeoftenextendtheconditionof
anifstatementwithrarenewvariablesinvolvedasshowninthe
exampledisplayedinListing 2.Notethatthetokenlistinthispatch
changes significantly compared with that of the buggy program.
Therefore, token-based features are more effectivefor this type of
patches, as shown in Table 4, compared with other features that
encode variables or the related semantics.
1// An overfitting patch generated for Math 28 by ACS
2- } else if(minRatioPositions.size() > 1) {
3+ } else if(minRatioPositions.size()>1& &!(minRatioPositions.size() > 0))
Listing 2: An Overfitting Patch Generated by ACS
The systematic study on static code features reveals that:
(1)existing staticcode featuresaregenerally effectivein differenti-
atingoverfittingpatchesovercorrectones;(2)thesefeaturesarelesseffectivetowardsprojectLangandconstraint-basedAPRtechniques
while more effective with respect to other types of patches.
4.2 RQ2: Effectiveness of APCA Techniques
4.2.1 Evaluation Results. Table5shows the detection results of
each APCA technique on our patch benchmark. As indicated by
the results, among all the 654 overfitting patches, existing dynamicAPCA techniquescan identify adiversenumber of them,ranging
from 67 to 350 for different techniques. For instance, Evosuite
identified 350 overfitting patches (53.52% of the ğ‘ƒğ‘œğ‘£ğ‘’ğ‘Ÿğ‘“ğ‘–ğ‘¡ğ‘¡ğ‘–ğ‘›ğ‘” ) while
italsolabeled3correctpatchesasoverfitting; Daikonidentified337
overfittingpatcheswhileitproduced38falsepositives.Besides,our
twoimplementationsof Opad(i.e.,R-OpadandE-Opad)generated
no false positives but they detected the least number of overfitting
patches.Otherdynamictechniquesthatdonotrequiretheoracle
information (e.g., E-PATCH-SIM andPATCH-SIM ), generated more
numberoffalsepositives.Thebehindreasonsofsuchfalsepositives
for each technique will be discussed in detail subsequently.
Figure 3: Overfitting Patches Dist.We also investigated the dis-
tributions of the overfittingpatches identified by different
APCA techniques, and the re-
sults are shown in Figure 3.
Due to the space limit of the
Venn figure, we do not in-
clude
R-OpadandE-Opadin
thiscomparisonsincetheyde-
tect the least number of over-
fittingpatches.Actually,these
twotoolscanonlyuniquelyde-
tect9and0overfittingpatches,respectively.Aswecanseefrom
this figure, only 11 overfitting patches were detected by all thedisplayed techniques while substantial overfitting patches were
detectedexclusivelybyspecifictechniques.Forinstance,48overfit-
ting patches can be detected by Evosuite orRandoop that cannot
bedetected byother techniques.In total,bycombining allthe dy-
namic APCA techniques plus Anti-patterns , we can detect 610
unique overfitting patches, accounting for 93 .3% of the overfitting
patchesinourbenchmark.Suchresultsindicatethatconsidering
multiple factors when designing APCA techniques can achieve
better performance.
Heuristicsbasedonstaticcodefeaturesarenotdesignedasastan-
dalone technique to identify overfitting patches directly. However,
wealsolisttheresultsinTable 5tocomparewithothertechniques.
Specifically,wecalculatedthevalueforeachpatch,generatedby
ssFix,CapGen,andS3respectively,asintroducedinSection 3.1.1
andthenprioritizedallthepatchesbasedontheobtainedvalues
andtheirrankingstrategies.Thetop248patchesareclassifiedas
correct patches and the remaining ones are regarded as overfitting
ones(Thenumberof248isselectedsinceourpreparedpatchbench-
markisunbalancedwhichonlycontains248correctpatches).As
observed in Table 5, heuristics based on static code features identi-
fied more number of overfitting patches in general compared with
dynamic ones. However, they are less precise as well since they
generatedmorenumberoffalsepositives.Forinstance,although
CapGenclassified506patchesasoverfittingcorrectly,italsolabeled
140correctpatchesasoverfittingones.Applyingsuchtechniques
willcausesignificantdestructiveeffects(i.e.,dismissinganumberof
correctpatches), inwhichcase theeffectivenessof APRtechniques
will be significantly under-estimated.
4.2.2 Analysis of the Results. We further performed a deeper anal-
ysis for the above results, and the following presents our findings.
975â€¢[False positives cases ] We carefully analyzed the false posi-
tivesgeneratedbyAPCAtechniquesandidentifiedthefollowing
reasons.For Evosuite ,itproduced3falsepositives(i.e.,thepatches
forLang-7generatedbyACS,kPAR,andTBar)whicharecausedby
thefactthatthegeneratedtestshavebrokenthetargetprogramâ€™s
preconditions.TheoraclepatchforLang-7addsaconditionalstate-
ment to deal with unexpected input in function createBigDecimal().
These three patches performed the correct modification as the ora-
clepatchbutinadifferentlocation(inthefunction createNumber()).
There exists a precondition of this program that createBigDecimal()
willonlybecalledby createNumber() andthatiswhythesepatches
arecorrect. Evosuite generatedteststhatviolatethisprecondition
bycallingthefunction createBigDecimal() directly.Consequently,
the tests failed on the patched program and these patches are con-
sidered as overfitting. For Randoop, it generated tests that check
the version for bug Closure-115 as shown in Listing 3. These tests
only pass on the oracle program and thus, six generated correct
patches failed on them.
1// A test case generated by Randoop
2String str0 = Compiler.getReleaseVersion();
3assertTrue("'" + str0 + "' != '" + "D4J_Closure_115_FIXED_VERSION" + "'",
str0.equals("D4J_Closure_115_FIXED_VERSION"));
Listing 3: A Test Case Generated by Randoop
ForDiffTGen , besides generating the same three false positives
asEvosuite , italso misclassified anothertwo correct patches.An
example is shown in Listing 4.ACSgenerated a patch which is
semanticequivalenttotheoraclepatchbythrowingthesameex-
ception. However, it did not synthesis the thrown message (i.e.,
â€œObject must implement Comparable â€).DiffTGen generateda
test that checks whether the thrown messages of these two pro-
grams are identical. As a result, this correct patch is mislabeled.
ForDaikon,itintotalgenerated38falsepositivessincetherules
adopted by Daikonto affirm identical invariant is extremely strict.
Note that Daikoninfers invariant at every entry and exit points of
the functions, and if any differences is observed, Daikonwill label
itas anoverfitting patch.Specifically, amongthe 38falsepositive
cases,30ofthemconcerndifferentexitpointsoffunctionsand7
ofthemusedifferentprogramelementstofinishthesametarget.
Note that if a function have multiple exit points, Daikonwill print
the line number of each exit point in its output. Considering the
example in Listing 4,line 6andline 12are the exit points of the
oracleandpatchedprograms.Theirlinenumberswithrespectto
the whole program are different (the former is 113 while the latter
is111).Thisleadstothedifferencesbetweentheoutputsof Daikon,
andthusthiscorrectpatchismislabeledasoverfitting.Anotherun-
common case is patch for Lang-55 generated by Jaid. The program
callsSystem.currentTimeMillis() during execution which generates
differenttimestamps,thuscausingdifferentinvariants.Suchresults
indicate that future work leveraging invariants to identify overfit-
ting patches can consider to weaken the rules to compare identical
invariants, and also consider the characteristics of the inferred
invariants and treat them differently when classifying patches.
1// Oracle patch for Math 89
2public void addValue(Object v) {
3+ if (v instanceof Comparable<?>){
4 addValue((Comparable<?>) v);
5+ } else {
6+ throw new IllegalArgumentException("Object must implement Comparable");
7+}8
9// A correct patch for Math 89 by ACS
10public void addValue(Object v) {
11+ if (!(v instanceof Comparable<?>)){
12+ throw new IllegalArgumentException();}
13 addValue((Comparable<?>) v);
14// Test case generated by DiffTGen
15assertEquals(
16"(E)0,(C)org.apache.commons.math.stat.Frequency,addValue(Object)0,(I)0",
17"java.lang.IllegalArgumentException: Object must implement Comparable",
18((Throwable) target_obj_7au3e).toString());
Listing 4: An Example of False Positive of DiffTGen and Daikon
ForPATCH-SIM andE-PATCH-SIM ,they generated87false posi-
tivesintotal.Weobservedthatthesecasesaremajorlycausedby
themisclassificationofthegeneratedtestsduetothecomplexity
of the original developer-provided test suite. Listing 5shows a con-
creteexample.Thecorrectpatchaddsaconditionalstatementto
dealwiththeunexpectedinput null.However,theoriginalfailing
testiscomplex,whichfeedsthefunction getRangeAxisIndex with
diverseinputsthatcoverbothnormalandunexpectedinputs.Such
a complex test leads to a complex path spectrum of getRangeAx-
isIndexduringexecution. PATCH-SIM generatedatestcasewhich
calls this function only with the input null, leading to an extremely
limitedpathspectrumonthismethod.Asaresult,thisgenerated
test is misclassified as a passing test since the execution trace is
divergent to that of the original failing test. When determining the
correctness of this correct patch, the execution trace of this test
on the buggy program in this function is lines 6, 7, and a return
statement,completelydifferentfromthatofthepatchedprogram
whichislines3and4.Consequently,thistestleadstoanextremely
high value of A ğ‘(the distance of a passing test) which is 0.53, thus
misclassifyingthepatchasoverfitting(cf.Section4.4.2in[ 81]).Sim-
ilarpatternshavebeenobservedformostoftheotherFPcases,thatisthedistancebetweenthepathspectrumobtainedviaexecutinga
passingtestagainstacorrectpatchandthatobtainedagainstthe
buggyprogramissignificantdifferent.Thebehindreasonisthat
thelogicofthedeveloper-providedtestsuiteiscomplex.Therefore,
utilizingtest case purification technique [ 27,84] (i.e., separating
thosenormalinputsfromabnormalonesintheexample)ispromis-
ing to enhance the accuracy to classify the generated tests, thus to
enhance the effectiveness of PATCH-SIM.
1// A correct patch for Chart 19 by AVATAR
2public int getRangeAxisIndex(ValueAxis axis) {
3+ if (axis == null) {
4+ throw new IllegalArgumentException();
5+}
6 int result = this.rangeAxes.indexOf(axis);
7 if (result < 0) {
8// A test case classified as passing test
9int int18 = categoryPlot7.getRangeAxisIndex(null);
10 assertTrue(int18 == 0);
Listing 5: An Example of False Positive of PATCH-SIM
ForAnti-patterns , we further dissected the effectiveness of
eachruleadoptedbyit,includingthedetectednumberofoverfitting
patchesandthenumberoffalsepositivecases.Wefindthatonly
fiverulesareeffectiveindetectingexistingoverfittingpatchesin
theJavalanguageandallthesefiverulesalsoproducefalsepositive
cases, which proves that applying these patterns will inevitably
causecertaindestructiveeffects(i.e.,dismissinganumberofcorrect
patches) as revealed by another study [22].
976Table 6: Performances of Existing APCA Techniques with Respect to Different Projects and Different APR Techniques
APCA TechniquePrecision Recall
C+T CL L M HTCL E C+T CL L M HTCL E
Evosuite 100.00% 100.00% 95.71% 100.00% 100.00% 97.73% 98.75% 100.00% 86.79% 20.51% 63.81% 60.89% 51.55% 53.75% 68.10% 33.93%
Randoop 100.00% 91.04% 100.00% 100.00% 96.06% 98.18% 100.00% 100.00% 38.68% 31.28% 41.90% 30.24% 37.89% 33.75% 37.07% 3.57%
DiffTGen 97.83% 100.00% 91.67% 99.04% 98.89% 96.08% 94.29% 100.00% 43.69% 1.79% 35.87% 43.28% 30.58% 34.27% 28.70% 25.00%
Daikon 92.93% NA 82.29% 92.22% 95.03% 93.52% 78.10% 100.00% 86.79% NA 75.24% 67.48% 79.69% 76.52% 75.23% 4.17%
R-Opad 100.00% 100.00% 100.00% 100.00% 100.00% 100.00% 100.00% 100.00% 18.87% 9.23% 11.43% 6.85% 9.32% 8.75% 8.62% 23.21%
E-Opad 100.00% 100.00% 100.00% 100.00% 100.00% 100.00% 100.00% 100.00% 13.21% 5.13% 12.38% 22.18% 13.98% 16.88% 10.34% 14.29%
Anti-patterns 95.45% 89.32% 63.64% 84.21% 93.98% 69.57% 68.25% 100.00% 39.62% 47.18% 20.00% 25.81% 48.45% 10.00% 37.07% 7.14%
PATCH-SIM 72.92% 75.00% 80.00% 86.17% 87.86% 73.40% 69.84% 68.42% 66.04% 22.22% 54.90% 33.20% 39.05% 44.23% 38.26% 23.64%
E-PATCH-SIM 80.00% 71.93% 62.96% 80.00% 83.00% 68.25% 73.68% 66.67% 52.83% 21.47% 16.83% 21.22% 26.27% 27.39% 24.35% 21.82%
The darker blue of the cell, the higher value; The darker orange of the cell, the lower value; We did not compare with static code features since they have been studied in RQ1.
Different Project: C+T: Chart + Time; CL:Closure;L:Lang;M:Math.Different Techniques: C: Constraint-based APR; H:Heuristic-based APR; LE:Learning-based APR; T:Template-based APR;
â€¢[Resultsincontradictwithpreviousstudies ]Wenotethat
our experiment reports much more false positives of PATCH-SIM
than the original study [81]. We carefully checked our results and
confirmed that for the set of 24 correct patches that are selected in
boththetwostudies,onlytheclassificationresultofonepatchis
different.Consideringtherandomizationof Randoop,weconjecture
itisreasonable.Therefore,ourstudyexposesthethreatstoexternal
validityofthepreviousstudy[ 81]usingalarge-scalebenchmark,
as mentioned in [ 22]. We also note that our experiment reports
much more true positives of two implementations of Opad(67 and
92, respectively)than thenumber reported in[ 81]. Thereason is
that the test suite used in our experiment contains hundreds of
tests for a patch rather than at most 50 as adopted in [81].
â€¢[Reasonsfornon-resultgeneration ]CertaindynamicAPCA
tools did not generate results for certain patches: for Daikon,i t
cannotgenerateinvariantsfor Closureprograms(alsoreportedin
[85],241casesintotal);for PATCH-SIM andE-PATCH-SIM ,theyrun
outofheapspacewhenalargeamountoftestscoverthepatched
method (also reported in [ 22], 24 cases for PATCH-SIM and 21 cases
forE-PATCH-SIM ); forDiffTGen , it does not work if the oracle
patchdeletesamethod(sinceitneedsanoraclemethodtocheck
thecorrectnessofthepatch)orifthetypesandvaluesoftheinputs
and outputs of the patched method cannot be obtained (since it
needs these information to perform the check, cf. Section 3.3.1in [
79]). There are 64 cases in total and we have confirmed these
limitations with the authors of DiffTGen.
â€¢[PerformanceondifferentAPRtypes&Defects4jprojects ]
Table6shows the results of existing APCA techniques in terms
ofprecisionandrecall,inparticular,separatedbydifferenttypesof
APR techniques and different projects. In terms of precision, exist-
ingAPCAtechniquesaregenerallyachievinghighperformances
crossdifferentprojectsandAPRtools.Specifically, Anti-patterns ,
PATCH-SIM andE-PATCH-SIM achieverelativelylowerprecisionval-
ues, which mostly vary from 60% to 90%. Other techniques achieve
better performances, which are mostly greater than 90%, and for
60.4% of the cases, the precision achieved is 100 .0%.
In terms of recall, the performance of existing APCA techniques
divergesalotcrossdifferentprojectsandAPRtechniques.Specif-
ically, the recalls achieved over projects ChartandTimeare gen-
erallyhigherthanthatovertheotherprojectswhiletherecallof
projectClosureisthelowest.Itiscausedbythefactthatthetest
suites on Closure often achieve low coverage [ 66] while dynamic
techniquesrelyheavilyonthequalityofthegeneratedtests.Therecalls achieved over the learning-based APR technique (i.e., Se-
quenceR)aregenerallylowerthanthatovertheothertypesofAPR
techniques. Especially, we found three APCA techniques achieved
ratherlowrecallsonthistypeofpatches(i.e., Randoop,Daikon,and
Anti-patterns ). We observed the following reasons: for Randoop,
the test suite generated for the bugs of this type of patches gen-erally achieve low coverage; for
Anti-patterns , learning-based
APRseldom generatepatches viacode transformationthat canbe
captured by the patterns; for Daikon, the variables modified by the
patches are generally not the return values, in which case their
changesarerarelycapturedbytheinferredinvariants.Suchresults
indicatesthatexistingtechniquesare comparativelylesseffective
inidentifyingoverfittingpatchestowardslearning-basedAPRtech-niques.Overalloursystematicstudyoftheeffectivenessofexisting
APCA techniques reveals that:
(1) most of the dynamic APCA techniques will label correct patches
asoverfittingwhilethosewiththeoracleinformationaregeneratingafewernumberoffalsepositives(i.e.,fewerthan10.0%);(2)existing
APCAtechniquesarehighlycomplementarytoeachothersincea
singletechniquecanonlydetectatmost53.5%overfittingpatches
while93.3%oftheoverfittingonescanbedetectedbyatleastone
techniquewiththeoracle;(3)heuristicsbasedonstaticcodefeatures
canachievehigherrecallsbutarelessprecisebygeneratingmore
number of false positives; (4) existing APCA techniques are less
effectivetowardsprojectClosureandlearning-basedAPRtechniques
while more effective with respect to other types of patches.
4.3 RQ3: Learning & Integration
Previous findings reveal that static features can achieve high recall
whiledynamictechniquesaremoreeffectivetowardsprecision and
existingtechniquesarehighlycomplementarytoeachother.There-
fore, it motivates us to integrate existing techniques together to
take the advantage of each sideâ€™s merits.
4.3.1 Integrating Static Code Features. Table7showstheresultsof
our method to integrate all the static features via learning through
sixmachinelearningmodels.Fromtheresults, RandomForest isthe
most effective model since it achieves the highest recall while still
maintaining a relatively high precision for the two experimental
settings.Inparticular,forthesettingwithouttheoracleinformation,itachievesahighrecallof89
.14%whilepreservingahighprecision
of 87.01%. We find that our model achieves better performance
than a single tool (e.g., Random Forest generates more TP with
977Table 7: Effectiveness of Each Training Model based on the Eight Static Features with and without the Oracle Information
Decision Table J48 LogisticRegression NaiveBayes RandomForest SMO
without with without with without with without with without with without with
TP 496 588 506 584 354 501 130 318 583 623 447 599
FP 85 84 84 57 88 64 31 46 87 64 85 71
Precision 85.37% 87.50% 85.76% 91.11% 80.09% 88.67% 80.75% 87.36% 87.01% 90.68% 84.02% 89.40%
Recall 75.84% 89.91% 77.37% 89.30% 54.13% 76.61% 19.88% 48.62% 89.14% 95.26% 68.35% 91.59%
Theoptimum performance is displayed in bold. â€œwithoutâ€ means the oracle patch is not available. â€œwithâ€ means the oracle is available. The same as Table 8.
Table 8: Integration Results with and without the Oracle
Strategy TPFP Precision RecallwithoutPATCH-SIM 24951 83.00% 38.85%
Anti-patterns 21937 85.55% 33.49%
Integration with the Learned Model 34330 91.96% 52.45%
PATCH-SIM + E-PATCH-SIM + Anti-patterns 18238 82.73% 27.83%withEvosuite 350399.15% 53.52%
Randoop 2216 97.36% 33.79%
Integration with the Learned Model 435499.10% 66.51%
Evosuite + Randoop + Daikon 295398.99% 45.11%
fewer FP than ssFix,CapGen, andS3as shown in Table 5). For
instance,comparedwith S3,themosteffectiveoneamongthethree,
the precision and recall have been improved by 9 .78% and 13 .0%
respectively.Thissuggeststhatintegratingdiversestaticfeatures
via learning to identify overfitting patches is a promising future
direction. We also find that the oracle information can boost the
effectivenessofallthemodelsviaachievingbothahigher precision
andrecall.Forinstance,forthemodelof RandomForest,thenumber
of TP has been increased by 40 while the number of FP has been
reducedby23.Sucharesultindicatesusingsuchmodelstofacilitate
the evaluation of APR techniques is promising.
4.3.2 Integrating with Existing Techniques. We choose the most ef-
fectivemodel(RandomForest)tointegratewithdynamictechniques
andAnti-patterns .Ourintuitionisthattheycanguaranteethe
precisionwhilethetrainedmodelcanhelpenhancetherecall.Asis
wellknown,dynamicmethodscanbeextremelytime-consuming
[88], and thus we only consider the top two most effective tech-
niques among dynamic ones and Anti-patterns for integration
currently. Specifically, for the scenario with the oracle informa-
tion, we integrate our trained model (with the oracle information)
withEvosuite andRandoop. For the scenario without the oracle
information, we integrate our trained model (without the oracle
information) with PATCH-SIM andAnti-patterns . We adopt the
majority voting strategy that is a patch is considered as overfitting
ifithasbeenlabeledasoverfittingbyatleasttwooutofourcon-
sidered techniques. Table 8shows the results, which indicates that
the effectiveness of existing APCA techniques can be significantly
enhanced via integration. Specifically, for the scenario without the
oracle information, while preserving a low FP of 30, the TP can be
improved from 249to 343,thus achievinga higher recall(52 .45%
vs. 38.85%). For the scenario with the oracle information, while
preservingahighprecisionof99 .10%,therecallcanbeimproved
from 53.52% to 66 .51%. To show the contributions of the learned
modelintheintegrationstrategy,wealsocomparedwiththeresultsobtainedbyintegratingthethreemosteffectiveexistingtechniques
via the majority voting. The results are shown in the last row of
each experimental setting which indicate that without the learningmodelbasedonstaticcodefeatures,theeffectivenessofidentifyingoverfittingpatchesisgreatlycompromised.Forinstance,ifweinte-grate
Evosuite ,Randoop,andDaikon,wecanonlyachievearecallof 45.11%, even lower than a single method like Evosuite . Such
resultsreflecttheusefulnessofstaticcodefeatures.Ourintegration
strategy reveals that:
(1) combining static features via learning significantly outperforms
existingheuristicsbasedonstaticcodefeaturesandthusisapromis-
ing direction for both patch generation and patch evaluation; (2)
integrating static features with existing techniques can take the ad-
vantageofeachside,thusachievingahigherrecallwhilepreserving
a high precision. Therefore, it is a future direction worth exploring.
5 THREATS TO VALIDITY
Externalvalidity. Ourstudyonlyconsidersthepatchesgenerated
by Java APCA techniques on the Defects4J benchmark. Thus, allfindingsmightbeonlyvalidforthisconfiguration.Nevertheless,this threat is mitigated by the fact that we use a wide range of
state-of-the-art APCAtechniquesand a most comprehensive patch
benchmark so far.
Internalvalidity. Itiserror-pronetoperformsuchalargescale
study and some of our findings may face the threats from the way
weperformedtheexperiments.Wemitigatethisbyre-checkingthe
processofourexperimentformanytimesandidentifyingthebe-
hindexplanationforeachresult.Besides,wehavereportedsomeofourresultstoauthorsof[
45,79,82]andobtainedpositivefeedback.
Construct validity. The parameters involved in this study may
casteffectsfortheresults.Wemitigatethisbystrictlyfollowingthe
previous studies. For instance, we run simple test case generation
toolsfor30seedsbyfollowing[ 88,89]andadoptthedefaultvalues
of K ğ‘¡and K ğ‘from [81] for PATCH-SIM andE-PATCH-SIM.
6 CONCLUSION
This paper performed a large-scale study on the effectiveness of
9 state-of-the-art APCA techniques and 3 heuristics based on 8
staticcodefeatures,basedonthelargestpatchbenchmarksofar.
Effectivenessisevaluatedandcomparedwithrespectto precision
andrecall. Our study dissects the pros and cons of existing ap-
proachesaswellaspointsoutapotentialdirectionbyintegrating
static features with existing methods.
Artefacts: All data in this study are publicly available at:
http://doi.org/10.5281/zenodo.3730599.
ACKNOWLEDGMENTS
TheauthorsthankHeYefromKTHRoyalInstitutionandQiXin
from Georgia Institute of Technology for their great help in the
experiment.ThisworkissupportedbytheNationalNaturalScience
FoundationofChinaNo.61672529aswellastheFundamentalRe-
searchFundsfortheCentralUniversities(HUST)No.2020kfyXJJS076.
978REFERENCES
[1] 2020. AgitarOne Homepage. http://www.agitar.com/index.html.
[2]MoumitaAsad,KishanKumarGanguly,andKaziSakib.2019. ImpactAnalysis
of Syntactic and Semantic Similarities on Patch Prioritization in Automated
ProgramRepair.In 2019IEEEInternationalConferenceonSoftwareMaintenance
and Evolution (ICSME). IEEE, 328â€“332.
[3]LingfengBao,XinXia,DavidLo,andGailCMurphy.2019. Alargescalestudy
of long-time contributor prediction for GitHub projects. IEEE Transactions on
Software Engineering (2019).
[4]LucianoBaresi,PierLucaLanzi,andMatteoMiraz.2010. TestFul:anEvolutionary
TestApproachforJava.In 3rdInternationalConferenceonSoftwareTesting.IEEE.
[5]Samuel Benton, Xia Li, Yiling Lou, and Lingming Zhang. 2020. On the Effective-
nessofUnifiedDebugging:AnExtensiveStudyon16ProgramRepairSystems.In
the 35thIEEE/ACM InternationalConference onAutomated SoftwareEngineering
(ASE 2020).
[6]LionelBriand.2011. Apracticalguideforusingstatisticalteststoassessrandom-
ized algorithms in software engineering. In International Conference on Software
Engineering.
[7]CristianCadar,DanielDunbar,andDawsonR.Engler.2008.KLEE:Unassistedand
Automatic Generation of High-Coverage Tests for Complex Systems Programs.
In8thUSENIXSymposiumonOperatingSystemsDesignandImplementation,OSDI
2008, December 8-10, 2008, San Diego, California, USA, Proceedings. 209â€“224.
[8]PadraicCashin,CarianneMartinez,WestleyWeimer,andStephanieForrest.2019.
UnderstandingAutomatically-GeneratedPatchesThroughSymbolicInvariant
Differences.In 201934th IEEE/ACMInternationalConference onAutomated Soft-
ware Engineering (ASE). IEEE, 411â€“414.
[9]Liushan Chen, Yu Pei, and Carlo A Furia. 2017. Contract-based program re-
pair without the contracts. In Proceedings of the 32nd IEEE/ACM International
Conference on Automated Software Engineering. IEEE, 637â€“647.
[10]Zimin Chen, Steve James Kommrusch, Michele Tufano, Louis-Noel Pouchet,
DenysPoshyvanyk,andMartinMonperrus.2019. SEQUENCER:Sequence-to-
SequenceLearningforEnd-to-EndProgramRepair. IEEETransactionsonSoftware
Engineering (2019).https://doi.org/10.1109/tse.2019.2940179
[11]HangyuanCheng,PingMa,JingxuanZhang,andJifengXuan.2020. CanThis
FaultBeDetectedbyAutomatedTestGeneration:APreliminaryStudy.In 2020
IEEE 2nd International Workshop on Intelligent Bug Fixing (IBF). 9â€“17.
[12]Christoph Csallner and Yannis Smaragdakis. 2004. JCrasher: an automatic ro-
bustness tester for Java. Software Prac. Experience 34, 11 (2004), 1025â€“1050.
[13]Viktor Csuvik, DÃ¡niel HorvÃ¡th, Ferenc HorvÃ¡th, and LÃ¡szlÃ³ VidÃ¡cs. 2020. Uti-lizing Source Code Embeddings to Identify Correct Patches. In 2020 IEEE 2nd
International Workshop on Intelligent Bug Fixing (IBF). 18â€“25.
[14]Zhen Yu Ding, Yiwei Lyu, Christopher Timperley, and Claire Le Goues. 2019.
Leveraging program invariants to promote population diversity in search-based
automatic program repair. In 2019 IEEE/ACM International Workshop on Genetic
Improvement (GI). IEEE, 2â€“9.
[15]Thomas Durieux, Fernanda Madeiral, Matias Martinez, and Rui Abreu. 2019.
EmpiricalReviewofJavaProgramRepairTools:ALarge-ScaleExperimenton
2,141 Bugs and 23,551 Repair Attempts. In Proceedings of the 2019 27th ACM
JointMeetingonEuropeanSoftwareEngineeringConferenceandSymposiumon
the Foundations of Software Engineering. ACM, 302â€“313. https://doi.org/10.1145/
3338906.3338911
[16]ThomasDurieuxandMartinMonperrus.2016. Dynamoth:dynamiccodesynthe-sisforautomaticprogramrepair.In Proceedingsofthe11thIEEE/ACMInternational
Workshop in Automation of Software Test. IEEE, 85â€“91.
[17]Charles Elkan. 2001. The foundations of cost-sensitive learning. In International
joint conference on artificial intelligence. 973â€“978.
[18]MichaelD.Ernst,JakeCockrell,WilliamG.Griswold,andDavidNotkin.2001.
Dynamicallydiscoveringlikelyprograminvariantstosupportprogramevolution.
IEEE Transactions on Software Engineering 27, 2 (Feb. 2001), 99â€“123.
[19]GordonFraserandAndreaArcuri.2011.EvoSuite:Automatictestsuitegeneration
for object-oriented software. In SIGSOFT/FSEâ€™11 19th ACM SIGSOFT Symposium
on theFoundations of Software Engineering(FSE-19) and ESECâ€™11: 13rdEuropean
SoftwareEngineeringConference(ESEC-13),Szeged,Hungary,September5-9,2011 .
[20]Gordon Fraser and Andrea Arcuri. 2013. Whole Test Suite Generation. IEEE
Transactions on Software Engineering 39, 2 (2013), 276â€“291.
[21]Gordon Fraser, Andrea Arcuri, and Phil McMinn. 2015. A memetic algorithm for
wholetestsuitegeneration. JournalofSystemsandSoftware 103(2015),311â€“327.
[22]AliGhanbari.2019. ValidationofAutomaticallyGeneratedPatches:AnAppetizer.
arXiv:1912.00117
[23]Ali Ghanbari, Samuel Benton, and Lingming Zhang. 2019. Practical program re-
pairviabytecodemutation.In Proceedingsofthe28thACMSIGSOFTInternational
Symposium on Software Testing and Analysis. ACM, 19â€“30.
[24]BaljinderGhotra,ShaneMcIntosh,andAhmedEHassan.2015. Revisitingthe
impact of classification techniques on the performance of defect prediction mod-
els.In2015IEEE/ACM37thIEEEInternationalConferenceonSoftwareEngineering,
Vol. 1. IEEE, 789â€“800.
[25]JinruHua,MengshiZhang,KaiyuanWang,andSarfrazKhurshid.2018. Towardspracticalprogramrepairwithon-demandcandidategeneration.In Proceedingsofthe 40th International Conference on Software Engineering. ACM, 12â€“23.
[26]Jiajun Jiang, Yingfei Xiong, and Xin Xia. 2019. A manual inspection of Defects4J
bugsanditsimplicationsforautomaticprogramrepair. ScienceChinaInformation
Sciences62, 10 (2019), 200102.
[27]Jiajun Jiang, Yingfei Xiong, Hongyu Zhang, Qing Gao, and Xiangqun Chen.
2018. Shapingprogramrepairspacewithexistingpatchesandsimilarcode.In
Proceedingsofthe27thACMSIGSOFTInternationalSymposiumonSoftwareTesting
and Analysis. ACM, 298â€“309.
[28]RenÃ©Just,DarioushJalali,andMichaelDErnst.2014. Defects4J:Adatabaseof
existing faults to enable controlled testing studies for Java programs. In Proceed-
ings of the 23rd International Symposium on Software Testing and Analysis. ACM,
437â€“440.
[29]Dongsun Kim, Jaechang Nam, Jaewoo Song, and Sunghun Kim. 2013. Automatic
patch generation learned from human-written patches. In Proceedings of the 35th
International Conference on Software Engineering. IEEE, 802â€“811.
[30]David G Kleinbaum, K Dietz, M Gail, Mitchel Klein, and Mitchell Klein. 2002.
Logistic regression. Springer.
[31]RonKohavi.1995. Astudyofcross-validationandbootstrapforaccuracyestima-tionandmodelselection.In FourteenthInternationalJointConferenceonArtificial
Intelligence. Montreal, Canada, 1137â€“1145.
[32]AnilKoyuncu,KuiLiu,TegawendÃ©F.BissyandÃ©,DongsunKim,JacquesKlein,
Martin Monperrus, and Yves Le Traon. 2018. Fixminer: Mining relevant fix
patterns for automated program repair. arXiv preprint arXiv:1810.01791 (2018).
[33]Xuan-Bach D. Le, Lingfeng Bao, David Lo, Xin Xia, Shanping Li, and Corina
Pasareanu.2019. Onreliabilityofpatchcorrectnessassessment.In Proceedingsof
the 41st International Conference on Software Engineering. IEEE, 524â€“535.
[34]Xuan-Bach D. Le, Duc-Hiep Chu, David Lo, Claire Le Goues, and Willem Visser.
2017. S3: syntax-and semantic-guided repair synthesis via programming byexamples. In Proceedings of the 11th Joint Meeting on Foundations of Software
Engineering. ACM, 593â€“604.
[35]Xuan Bach D. Le, David Lo, and Claire Le Goues. 2017. Empirical Study onSynthesis Engines for Semantics-Based Program Repair. In Proceedings of the
33th IEEE International Conference on Software Maintenance. IEEE.
[36]XuanBachD.Le,DavidLo,andClaireLeGoues.2016. Historydrivenprogramre-pair.InProceedingsofthe23rdIEEEInternationalConferenceonSoftwareAnalysis,
Evolution, and Reengineering. IEEE, 213â€“224.
[37]XuanBachD.Le,FerdianThung,DavidLo,andClaireLeGoues.2018. Overfitting
insemantics-basedautomatedprogramrepair. EmpiricalSoftwareEngineering
23, 5 (2018), 3007â€“3033.
[38]ClaireLeGoues,ThanhVuNguyen,StephanieForrest,andWestleyWeimer.2012.
GenProg: A generic method for automatic software repair. IEEE Transactions on
Software Engineering 38, 1 (2012), 54â€“72.
[39]Claire Le Goues, Michael Pradel, and Abhik Roychoudhury. 2019. AutomatedProgramRepair. Commun.ACM 62,12(2019),56â€“65. https://doi.org/10.1145/
3318162
[40]Hang Li. 2011. A short introduction to learning to rank. IEICE TRANSACTIONS
on Information and Systems 94, 10 (2011), 1854â€“1862.
[41]KuiLiu,AnilKoyuncu,TegawendÃ©F.BissyandÃ©,DongsunKim,Jacques.Klein,
andYvesLeTraon.2019.YouCannotFixWhatYouCannotFind!AnInvestigation
of Fault Localization Bias in Benchmarking Automated Program Repair Systems.
InProceedings of the 12th IEEE Conference on Software Testing, Validation and
Verification. 102â€“113. https://doi.org/10.1109/ICST.2019.00020
[42]KuiLiu,AnilKoyuncu,DongsunKim,andTegawendÃ©FBissyandÃ©.2019. Avatar:Fixingsemanticbugswithfixpatternsofstaticanalysisviolations.In Proceedings
of the 26th IEEE International Conference on Software Analysis, Evolution and
Reengineering. IEEE, 456â€“467.
[43]Kui Liu, Anil Koyuncu, Dongsun Kim, and TegawendÃ© F. BissyandÃ©. 2019. TBar:
Revisiting Template-based Automated Program Repair. In Proceedings of the 28th
ACM SIGSOFT International Symposium on Software Testing and Analysis. ACM,
31â€“42.
[44] Kui Liu, Anil Koyuncu, Kisub Kim, Dongsun Kim, and TegawendÃ© F. BissyandÃ©.
2018. LSRepair:LiveSearchofFixIngredientsforAutomatedProgramRepair.
InProceedings of the 25th Asia-Pacific Software Engineering Conference . 658â€“662.
https://doi.org/10.1109/APSEC.2018.00085
[45]Kui Liu, Shangwen Wang, Anil Koyuncu, TegawendÃ© F. BissyandÃ©, DongsunKim, Peng Wu, Jacques Klein, Xiaoguang Mao, and Yves Le Traon. 2020. Onthe Efficiency of Test Suite based Program Repair: A Systematic Assessmentof16AutomatedRepairSystemsforJavaPrograms.In Proceedingsofthe42nd
International Conference on Software Engineering. ACM.
[46]Xuliang Liu and Hao Zhong. 2018. Mining stackoverflow for program repair.InProceedings of the 25th IEEE International Conference on Software Analysis,
Evolution and Reengineering. IEEE, 118â€“129.
[47]FanLongandMartinRinard.2015. Stagedprogramrepairwithconditionsynthe-sis.InProceedingsofthe10thJointMeetingonFoundationsofSoftwareEngineering.
ACM, 166â€“178.
[48]FanLongandMartinRinard.2016. AnAnalysisoftheSearchSpacesforGenerate
andValidatePatchGenerationSystems.In Proceedingsofthe38thInternational
Conference on Software Engineering. ACM, 702â€“713. https://doi.org/10.1145/
9792884781.2884872
[49]Yiling Lou, Ali Ghanbari, Xia Li, Lingming Zhang, Haotian Zhang, Dan Hao,
and Lu Zhang. 2020. Can Automated Program Repair Refine Fault Localization?
A Unified Debugging Approach. In Proceedings of the 29th ACM International
Symposium on Software Testing and Analysis (ISSTA 2020).
[50]QingzhouLuo,FarahHariri,LamyaaEloussi,andDarkoMarinov.2014. Anempir-
ical analysis of flaky tests. In Proceedings of the 22nd ACM SIGSOFT International
Symposium on Foundations of Software Engineering (FSE). 643â€“653.
[51]Henry B Mann and Donald R. Whitney. 1947. On a Test of Whether One of
Two Random Variables Is Stochastically Larger than the Other. The Annals
of Mathematical Statistics 18, 1 (1947), 50â€“60. https://doi.org/10.1214/aoms/
1177730491
[52]AlexandruMarginean,JohannesBader,SatishChandra,MarkHarman,YueJia,
Ke Mao, Alexander Mols, and Andrew Scott. 2019. Sapfix: Automated end-to-
endrepairatscale.In 2019IEEE/ACM41stInternationalConferenceonSoftware
Engineering: Software Engineering in Practice (ICSE-SEIP). IEEE, 269â€“278.
[53]PaulDanMarinescuandCristianCadar.2013. KATCH:high-coveragetesting
of software patches. In Proceedings of the 9th Joint Meeting on Foundations of
Software Engineering - ESEC/FSE 2013.
[54]MatiasMartinezandMartinMonperrus.2016. Astor:Aprogramrepairlibraryforjava.In Proceedingsofthe25thInternationalSymposiumonSoftwareTesting
and Analysis. ACM, 441â€“444.
[55]Matias Martinez and MartinMonperrus. 2018. Ultra-LargeRepair Search Space
with Automatically Mined Templates: the Cardumen Mode of Astor. In Proceed-
ingsofthe10thInternationalSymposiumonSearchBasedSoftwareEngineering.
Springer, 65â€“86.
[56]Sergey Mechtaev, Jooyong Yi, and Abhik Roychoudhury. 2016. Angelix: Scalable
MultilineProgramPatchSynthesisviaSymbolicAnalysis.In Proceedingsofthe
38th International Conference on Software Engineering (ICSE â€™16). Association for
ComputingMachinery,NewYork,NY,USA,691â€“701. https://doi.org/10.1145/
2884781.2884807
[57]Martin Monperrus. 2018. The Living Review on Automated Program Repair.
Technical Report. Technical Report hal-01956501. HAL/archives-ouvertes. fr,
HAL/archives.
[58]Hoang Duong Thien Nguyen, Dawei Qi, Abhik Roychoudhury, and Satish Chan-
dra. 2013. Semfix: Program repair via semantic analysis. In Proceedings of the
35th International Conference on Software Engineering. IEEE, 772â€“781.
[59]Carlos Pacheco and Michael D. Ernst. 2007. Randoop: feedback-directed random
testing for Java. In In OOPSLA â€™07 Companion. ACM, 815â€“816.
[60]TinaR.PatilandSwatiSunilSherekar.2013. PerformanceanalysisofNaiveBayes
andJ48classificationalgorithmfordataclassification. Internationaljournalof
computer science and applications 6, 2 (2013), 256â€“261.
[61]Lionel S Penrose. 1946. The elementary statistics of majority voting. Journal of
the Royal Statistical Society 109, 1 (1946), 53â€“57.
[62]John Platt. 1998. Sequential minimal optimization: A fast algorithmfor training support vector machines. MSR-TR-98-14 (1998), 21.https://www.microsoft.com/en-us/research/publication/sequential-minimal-
optimization-a-fast-algorithm-for-training-support-vector-machines/
[63]YuhuaQi,XiaoguangMao,YanLei,ZiyingDai,andChengsongWang.2014. The
strengthofrandomsearchonautomatedprogramrepair.In Proceedingsofthe
36th International Conference on Software Engineering. ACM, 254â€“265.
[64]Zichao Qi, Fan Long, Sara Achour, and Martin Rinard. 2015. An analysis ofpatch plausibility and correctness for generate-and-validate patch generation
systems.In Proceedingsofthe24thInternationalSymposiumonSoftwareTesting
and Analysis. ACM, 24â€“36.
[65]AndrewScott,JohannesBader,andSatishChandra.2019. Getafix:Learningto
fix bugs automatically. arXiv preprint arXiv:1902.06111 (2019).
[66]Sina Shamshiri, RenÃ© Just, JosÃ© Miguel Rojas, Gordon Fraser, and Andrea Arcuri.
2015. Do Automatically Generated Unit Tests Find Real Faults? An Empirical
Study of Effectiveness and Challenges. In International Conference on Automated
Software Engineering (ASE).
[67]Inderjeet Singh. 2012. A mapping study of automation support tools for unit
testing. In School of Innovation Design and Engineering.
[68]Edward K Smith, Earl T. Barr, Claire Le Goues, and Yuriy Brun. 2015. Is the cure
worse than the disease? overfitting in automated program repair. In Proceedings
of the 10th Joint Meeting on Foundations of Software Engineering . ACM, 532â€“543.
[69]Mauricio Soto and Claire Le Goues. 2018. Using a probabilistic model to predict
bugfixes.In Proceedingsofthe25thInternationalConferenceonSoftwareAnalysis,
Evolution and Reengineering. IEEE, 221â€“231.
[70]Shin Hwei Tan, Hiroaki Yoshida, Mukul R. Prasad, and Abhik Roychoudhury.
2016. Anti-patternsinsearch-basedprogramrepair.In Proceedingsofthe201624th
ACM SIGSOFT International Symposium on Foundations of Software Engineering.
727â€“738.
[71]HaoyeTian,KuiLiu,AbdoulKaderKaborÃ©,AnilKoyuncu,LiLi,JacquesKlein,
and TegawendÃ© F. BissyandÃ©. 2020. Evaluating Representation Learning of CodeChangesforPredictingPatchCorrectnessinProgramRepair.In Proceedingsof
the35th IEEE/ACMInternationalConference onAutomated SoftwareEngineering.
ACM.
[72]ShangwenWang,MingWen,LiqianChen,XinYi,andXiaoguangMao.2019.How
DifferentIsItBetweenMachine-GeneratedandDeveloper-ProvidedPatches?:
AnEmpiricalStudyontheCorrectPatchesGeneratedbyAutomatedProgram
RepairTechniques.In Proceedingsofthe13rdACM/IEEEInternationalSymposium
on Empirical Software Engineering and Measurement. IEEE, 1â€“12.
[73]ShangwenWang,MingWen,XiaoguangMao,andDehengYang.2019. Attention
please: Consider Mockito when evaluating newly proposed automated program
repairtechniques.In Proceedingsofthe23rdEvaluationandAssessmentonSoftware
Engineering. ACM, 260â€“266.
[74]WestleyWeimer,ZacharyPFry,andStephanieForrest.2013. Leveragingprogram
equivalence for adaptive program repair: Models and first results. In Proceedings
ofthe28thIEEE/ACMInternationalConferenceonAutomatedSoftwareEngineering.
IEEE, 356â€“366.
[75]WestleyWeimer,ThanhVuNguyen,ClaireLeGoues,andStephanieForrest.2009.
Automatically finding patches using genetic programming. In Proceedings of the
31st International Conference on Software Engineering. IEEE, 364â€“374.
[76]MingWen,JunjieChen,YongqiangTian,RongxinWu,DanHao,ShiHan,and
Shing-Chi Cheung. 2020. Historical Spectrum based Fault Localization. IEEE
Transactions on Software Engineering (TSE) (2020).
[77]Ming Wen, Junjie Chen, Rongxin Wu, Dan Hao, and Shing-Chi Cheung. 2018.
Context-awarepatchgenerationforbetterautomatedprogramrepair.In Proceed-
ings of the 40th International Conference on Software Engineering. ACM, 1â€“11.
[78]MingWen,RongxinWu,andShing-ChiCheung.2016. Locus:Locatingbugsfrom
software changes. In 2016 31st IEEE/ACM International Conference on Automated
Software Engineering (ASE). IEEE, 262â€“273.
[79]QiXinandStevenP.Reiss.2017. Identifyingtest-suite-overfittedpatchesthrough
test case generation. In Proceedings of the 26th ACM SIGSOFT International Sym-
posium on Software Testing and Analysis. ACM, 226â€“236.
[80]Qi Xin and Steven P. Reiss. 2017. Leveraging syntax-related code for automated
program repair. In Proceedings of the 32nd IEEE/ACM International Conference on
Automated Software Engineering. IEEE, 660â€“670.
[81]Yingfei Xiong, Xinyuan Liu, Muhan Zeng, Lu Zhang, and Gang Huang. 2018.
Identifying patch correctness in test-based program repair. In Proceedings of the
40th International Conference on Software Engineering. ACM, 789â€“799.
[82]Yingfei Xiong, Jie Wang, Runfa Yan, Jiachen Zhang, Shi Han, Gang Huang, and
LuZhang.2017. Preciseconditionsynthesisforprogramrepair.In Proceedings
of the 39th IEEE/ACM International Conference on Software Engineering. IEEE,
416â€“426.
[83]JifengXuan,MatiasMartinez,FavioDemarco,MaximeClement,SebastianLame-
las Marcote, Thomas Durieux, Daniel Le Berre, and Martin Monperrus. 2017.
Nopol:Automaticrepairofconditionalstatementbugsinjavaprograms. IEEE
Transactions on Software Engineering 43, 1 (2017), 34â€“55.
[84]Jifeng Xuan and Martin Monperrus. 2014. Test case purification for improv-ing fault localization. In Proceedings of the 22nd ACM SIGSOFT International
Symposium on Foundations of Software Engineering (FSE). ACM, 52â€“63. http:
//doi.acm.org/10.1145/2635868.2635906
[85]Bo Yang and Jinqiu Yang. 2020. Exploring the Differences between Plausible and
Correct Patches at Fine-Grained Level. In 2020 IEEE 2nd International Workshop
on Intelligent Bug Fixing (IBF). IEEE, 1â€“8.
[86]Jinqiu Yang, Alexey Zhikhartsev, Yuefei Liu, and Lin Tan. 2017. Better test cases
for better automated program repair. In Proceedings of the 11th Joint Meeting on
Foundations of Software Engineering. ACM, 831â€“841.
[87]He Ye, Jian Gu, Matias Martinez, Thomas Durieux, and Martin Monperrus. 2019.
Automated Classification of Overfitting Patches with Statically Extracted Code
Features. arXiv:1910.12057
[88]He Ye, Matias Martinez, and Martin Monperrus. 2019. Automated Patch Assess-
ment for Program Repair at Scale. arXiv:1909.13694
[89]ZhongxingYu,MatiasMartinez,BenjaminDanglot,ThomasDurieux,andMartin
Monperrus.2019. Alleviatingpatchoverfittingwithautomatictestgeneration:
a study of feasibility and effectiveness for the Nopol repair system. Empirical
Software Engineering 24, 1 (2019), 33â€“67.
[90]YuanYuanandWolfgangBanzhaf.2018. ARJA:AutomatedRepairofJavaPro-
grams via Multi-Objective Genetic Programming. IEEE Transactions on Software
Engineering (2018).
[91]Jie Zhang, Lingming Zhang, Mark Harman, Dan Hao, Yue Jia, and Lu Zhang.
2018. Predictive mutation testing. IEEE Transactions on Software Engineering 45,
9 (2018), 898â€“918.
[92]Tianchi Zhou,Xiaobing Sun, XinXia, Bin Li,and Xiang Chen.2019. Improving
defect prediction with deep forest. Information and Software Technology 114
(2019), 204â€“216.
[93]Wojciech Ziarko and Shan Ning. 1997. Machine Learning Through Data Classifi-
cation and Reduction. Fundamenta Informaticae 30, 3 (1997), 373â€“382.
980