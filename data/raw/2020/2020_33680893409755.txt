MakingSymbolic Execution PromisingbyLearningAggressive
State-Pruning Strategy
Sooyoung Cha
Korea University
Republicof Korea
sooyoungcha@korea.ac.krHakjooOh∗
Korea University
Republicof Korea
hakjoo_oh@korea.ac.kr
ABSTRACT
We present Homi, a new technique to enhance symbolic execution
bymaintainingonlyasmallnumberofpromisingstates.Inpractice,
symbolic executiontypicallymaintainsasmany statesaspossible
in a fear of losing important states. In this paper, however, we
show that only a tiny subset of the states plays a significant role
inincreasingcodecoverageorreachingbugpoints.Basedonthis
observation, Homiaimstominimizethetotalnumberofstateswhile
keepingpromising statesduringsymbolicexecution.Weidentify
promisingstatesbyalearningalgorithmthatcontinuouslyupdates
theprobabilisticpruningstrategybasedondataaccumulatedduring
thetestingprocess.Experimentalresultsshowthat Homigreatly
increases code coverage and the ability to find bugs of KLEE on
open-sourceC programs.
CCS CONCEPTS
·Software and its engineering →Software testing and de-
bugging.
KEYWORDS
DynamicSymbolic Execution,OnlineLearning
ACMReference Format:
SooyoungChaandHakjooOh.2020.MakingSymbolicExecutionPromising
byLearningAggressiveState-PruningStrategy.In Proceedingsofthe28th
ACMJointEuropeanSoftwareEngineeringConferenceandSymposiumonthe
FoundationsofSoftwareEngineering(ESEC/FSE’20),November8ś13,2020,
VirtualEvent,USA. ACM,NewYork, NY,USA, 12pages.https://doi.org/10.
1145/3368089.3409755
1 INTRODUCTION
Symbolic execution [ 6,7,14,23] is an effective software testing
method to increase code coverage and find subtle bugs. The key
idea of this method is to systematically explore program’s diverse
pathsbysubstitutingprograminputswithsymboliconestoexecute
theprogramsymbolically.Atahigh-level,symbolicexecutionitera-
tively selects, executes, and forks a state while maintaining a set of
∗Corresponding author
Permissionto make digitalor hard copies of allorpart ofthis work for personalor
classroom use is granted without fee provided that copies are not made or distributed
forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation
on the first page. Copyrights for components of this work owned by others than ACM
mustbehonored.Abstractingwithcreditispermitted.Tocopyotherwise,orrepublish,
topostonserversortoredistributetolists,requirespriorspecificpermissionand/ora
fee. Request permissions from permissions@acm.org.
ESEC/FSE ’20, November 8ś13, 2020, Virtual Event, USA
©2020 Associationfor Computing Machinery.
ACM ISBN 978-1-4503-7043-1/20/11...$15.00
https://doi.org/10.1145/3368089.3409755states during itstesting process. In particular, it forksthe state into
one or two separate states according to the feasibility of branch
conditionsencountered during the symbolicexecution. Thanks to
the systematicprocess,symbolicexecutionhasbeen actively used
in a variety of applications: operation systems [ 17], smartphone
apps [1], neuralnetworks [ 26], andsmart contracts [ 20,22].
However, performing symbolic execution on real-world pro-
gramsinevitablyfacestheinfamousstate-explosionproblemthat
exponentiallyincreasesthenumberofstatestobemaintained,lead-
ing to significant increases of memory usage. Hence, in practice,
symbolicexecutor(e.g.,KLEE[ 5])takesasinputthememorybudget
topreventunexpectedmemoryusage,andmaintainsasmanystates
aspossible withinthememorybudgettoreducetheriskoflosing
importantstatesduringtesting.Thisreasonablebehaviorcauses
the symbolic executor to suffer from two practical problems. First,
preservingasmanystates aspossible increases thetotalnumber
ofcandidate states,whichmakesit difficultforsymbolic executor
to decide proper states in a sense of increasing code coverage or
finding bugs. Second, since the accumulated states easily exceed
agivenmemorybudget,numerousstatesarerandomlyprunedto
reducethememoryusage.AswedemonstrateinSection 2,when
performingKLEE [ 5]with thedefault memory budget (2GB) on C
open-source programs, the number of states to maintain is tens of
thousands, and the number of blindly pruned states ranges from
tensofthousandsto hundredsof thousandsonaverage.
To resolve this state-explosion problem, we aim to minimize the
totalnumberofstatesbuttokeep promising statesduringsymbolic
execution.Ofthepreservedstates,weobservedthatthereexista
very few promising states to effectively increase the code coverage
or to reach the bug points; thus, symbolic execution becomes more
effectiveandefficientifweonlymaintainthosesmallnumberof
promisingstatesinasenseofresolvingthestate-explosionproblem.
Toachieveourgoal,thetechnicalchallengesweneedtoaddressare
(1)toestimatehowpromisingeachstateisand(2)todeterminehow
manystates we needto prune.Although diverse approachesexist
withthegoalofreducingthesearchspaceofsymbolicexecution[ 2,
3,15,27,30,31], their goals are not to maintain a small number of
promisingstates;theexistingapproachesaimtoidentifyandprune
onlytheredundantstatesthatmeetthepredefinedcriteriafromthe
totalones.Forinstance,post-conditionedsymbolicexecution[ 30,
31] is to prune only the states having the same path suffixes as
previously explored states, and Jaffar et al . [15]discard program
pathsguaranteedto be unreachable to bugpoints.
We presenta newtechniqueto adaptivelymaintainonly asmall
number of promising states during symbolic execution via on-
line learning. To achieve our goal, we introduce two key ideas:
aprobabilistic pruning strategy and a learning algorithm. First, we
147
ESEC/FSE ’20, November8ś13,2020,VirtualEvent, USA SooyoungChaandHakjoo Oh
define the probabilistic pruning strategy that contains both con-
tinuousanddiscreteprobabilitydistributions.Weusetheformer
distribution to score how promising each state is, and the latter
one todecide how manystates arepruned. That is,we reduce the
problemofsolvingthetwotechnicalchallengesintotheproblem
oflearningbothprobabilisticdistributions.Second,wepresenta
learning algorithm that continuously updates the two probabilistic
distributions online based on data accumulated during symbolic
execution.
Experimental results show that symbolic execution with our
techniquesignificantly improvesbranchcoveragewhilemaintain-
ing a relatively small number of states compared to the general
symbolic execution on open-source C programs. We implemented
our technique in a tool, Homi, on top of KLEE [ 5] and evaluated it
on 9 C programs (10-61KLoC). Symbolic execution with Homisuc-
ceedsin coveringmore branchesandfindingmore realbugsthan
conventional symbolic execution on 9 benchmarks. Forinstance,
our technique is able to generate the bug-triggering inputs that
causeabnormalterminationandsegmentationfaultin grep-2.6
andcombine-0.4.0 , respectively, while conventional symbolic ex-
ecutionfailedto doso.
Contributions .Our contributionsare as follows:
•We present a new technique to maintain only promising
states by continuously learning the probabilistic pruning
strategyonlineduringsymbolic execution.
•Wedemonstratethe effectivenessof Homion9open-source
Cprogramsbycomparingsymbolicexecutionwithvs.with-
outHomi.
•We make our tool, Homi,anddata publiclyavailable.1
2 PRELIMINARIES
Inthissection,wedescribeageneralalgorithmandlimitationof
symbolicexecution,andexplainourobservationtopresentthegoal
ofthis paper.
SymbolicExecution. Themainideaofsymbolicexecution[ 5,7,8]
istosystematicallyexploreprogram’sdiversepathsbyreplacing
program inputs with symbolic ones to execute a program sym-
bolically. Algorithm 1presents a generic algorithm for symbolic
execution, except for a few change, line 6, that stems from our
main approach. Generally, symbolic execution maintains a set S
of program states until the time budget expires, where a single
state consists of a tuple (instr,store,Φ). Each element of a tuple
respectivelydenotesthenextinstructiontobeexecuted( instr),a
symbolic store ( store) which maps the program variables into sym-
bolic values, and a path-condition ( Φ) which is a conjunction of
branchconditionsevaluatedsymbolicallyinthestate.Thesymbolic
executiongeneratestest-casesbyiterativelyselecting,executing
andupdatingthe states in Sduringits testingprocess.
TheRunprocedureinAlgorithm 1takesaprogram Pundertest
and the time budget ( N) as input, and returns a set Tof test-cases
generatedwithinthetimebudget.Atline2,thealgorithminitializes
a setSas an initial state(instr0,store0,true), whereinstr0is the
veryfirstinstructionexecutedintheprogram P,store0istheinitial
1Homi: https://github.com/kupl/HOMI_publicAlgorithm1 Symbolic Execution
Input:Program( P), time budget ( N), andthe probabilisticdata( P).
Output: Asetoftest cases ( T)
1:procedure Run(P,N,P)
2:S←{(instr0,store0,true)} ▷initial states
3:T←∅ ▷initial test cases
4:repeat
5: SP←PruneM(S,M,r) ▷Mis memory and ris ratio.
6: SP←SP∪Prune(S,P,ηt) ▷prune states
7: S←S\SP
8: for each(_,_,Φ)∈SPdo ▷generatetest cases
9: T←T∪{(Φ,Model(Φ))}
10:(instr,store,Φ)←Select(S) ▷choose astate
11: S←S\{(instr,store,Φ)}
12:(instr′,store′,Φ)←Execute(instr,store,Φ)
13: ifinstr′= (if (ϕ)theninstr1elseinstr2)then
14: ifSAT(Φ∧ϕ)thenS←S∪{(instr1,store′,Φ∧ϕ)}
15: ifSAT(Φ∧¬ϕ)thenS←S∪{(instr2,store′,Φ∧¬ϕ)}
16: else ifinstr′= haltthen
17: T←T∪{(Φ,Model(Φ)} ▷generatetest cases
18:untilbudgetNexpires (or S=∅)
19:
20:for each(_,_,Φ)∈Sdo ▷generatetest cases
21: T←T∪{(Φ,Model(Φ))}
22:returnT
mapping information, and Φis set totrue. For instance, suppose
that there existsasmall program Pundertest as follows:
void main( int x , int y ){
if (x>89) printf (" good ");
if (x==2 && y>25) assert (" bad " ) ; }
Withthisprogramasaninput, instr0issettothefirstinstructionof
theprogram,if( x>89),store0is[x/mapsto→α,y/mapsto→β],andΦistrue.At
line 3, the algorithm initializes a set Tof test-cases to an empty set.
At line 5, the PruneMfunction decides a set of states to be pruned;
thatis,itrandomlyselectsasubset SPofSwiththesizeof|S|∗r(e.g.,
r=0.1)when|S|,thesizeof S,exceedsthegivenmemorycapacity
M. Otherwise, it returns an empty set (i.e., SP=∅). At line 7, the
algorithmupdatestheset Swiththedifferencesetbetween Sand
SP.Foreverystateinthepruningset SP,thealgorithmgenerates
atest-case twhichisamodelofthepath-condition Φinthestate
(instr,store,Φ)at line8-9.
After the test-case generation, the Selectfunction, namely a
searchheuristic[ 9,19,24,28],choosesasinglestate (instr,store,Φ)
from the set Sbased on its own selection criteria (line 10). With
the selected state, the Executefunction executes the instruction
instr, and returns the updated state (instr′,store′,Φ). At line 13,
if the instruction instr′is an if/else statement, Algorithm 1first
checks the feasibility of the path-conditions corresponding to both
two branches. Ifthe path-conditionof the ifstatement, (Φ∧ϕ), is
satisfiable,thealgorithmaddsthenewstate (instr1,store′,Φ∧ϕ)
intothe set S(line 14). Likewise, the algorithm adds the newstate
intotheset Sifthepath-conditionoftheelsestatementissatisfiable
(line 15). When both sides of the branch are feasible, the algorithm
forks the single state into two states, where this forking process
causesthestate-explosionprobleminsymbolicexecution.Onthe
otherhand,if instr′isthehaltstatement,thealgorithmgenerates
148MakingSymbolicExecutionPromising by Learning Aggressive State-Pruning Strategy ESEC/FSE ’20, November8ś13,2020,VirtualEvent, USA
Table 1: The number of states and pruned states on C open-
source programs (time budget:5h,memory budget:2GB)
gawk grep vdir ginstall trueprint
#states (|S|) 37K 41K 43K 60K 49K
#pruned states 34K 112K 115K 587K 155K
atest-case tandaddsapairofpath-conditionandatest-case,( Φ,
t), to the set Tof test-cases. For simplicity, we have omitted the
caseswhen instr′istheotherinstructionssuch asload, store,and
call instructions. Algorithm 1repeats this process until the time
budgetNexpires or the set Sbecomes an empty set. When the
loop ends, at line 20-21, the algorithm generates test-cases using
the path-conditions of all remaining states in the set S, which have
not yet reached the halt statement. Lastly, the algorithm returns
the generatedtest-cases Tas an output.
Limitation. The general symbolicexecution attempts to main-
tainasmanystatesaspossiblewithinthememorybudgettoreduce
the loss of critical states during testing. This behavior, however,
significantlydegradestheperformanceofsymbolicexecutionap-
plied to real-world programs as the number of states in both Sand
SPgrows. The greater the number of states in the set S, the harder
it is for the Selectfunction tochoose meaningful states whichare
likelytoincreasethecodecoverageortoreachthebuggylocations.
Furthermore,theincreasesinthesizeofthesetofprunedstates, SP,
mayleadtothelossofpromisingstatesastheyareforciblypruned
from the set Sofcandidate states.
Table1shows the average number of candidate states ( S) and
pruned total states when performing KLEE [ 5], a popular symbolic
execution tool, on open-source C programs for 5 hours with the
defaultmemorycapacity,2GB.Overall,thesizeofthecandidateset
is tens of thousands, and the number of pruned states ranges from
tens of thousands to hundreds of thousands. For instance, when
performingKLEEongrep,the Selectfunctionshouldchooseastate
from about 41,000 candidate states on average for each iteration at
line 4-18; the PruneMfunction blindly prunes about112,000 states
duetoexceedingthememorybudgeteventhoughthepromising
states mayexist among the prunedones.
Goal.Thegoalofthispaperistomaintainonlypromisingstates
via aggressive state-pruning during symbolic execution. In our
work, we define the promising states as having the potential to ef-
fectively increase branch coverage when they are further explored,
and observethat thereare a veryfew promisingstates amongthe
total candidate states. Hence, if we succeed in performing sym-
bolic execution while keeping them only, we are able to maximize
code coverage and to find many bugs. That is, the Prunefunction
in Algorithm 1enables the symbolic execution to maintain the
minimized set Sof candidate states while preserving the promis-
ingstates,andpreventssituationswherethecandidatestatesare
blindlyprunedduetomemoryoverrun.Toachievethisgoal,the
technical challenges we mustaddressare as follows:
(1) Howpromisingeachstate is?
(2) Howmanystates dowe needto prune?Inthispaper,weaddressthechallengesviatheprobabilisticpruning
strategylearnedonlineduringsymbolic execution.
3 OURTECHNIQUE
In this section, we describe our technique, Homi, in detail. Sec-
tion3.1defines the probabilistic pruning strategy ( Prune) used
in Algorithm 1. Section 3.2describes our symbolic execution al-
gorithm(Algorithm 2)withtheonlinelearningtechniqueforthe
probabilisticpruning strategy.
3.1 ProbabilisticPruning Strategy
The pruning function ( Prune) in Algorithm 1decides the set SP
of pruned states based on the probabilistic data P. This function
takesasinputtheset Sofallstates,theprobabilisticdata P,andthe
timecycle ηt.Forevery ηtseconds,thepruningstrategyselectsthe
setSPof łunpromisingž states in two steps: sampling and pruning.
In the experiments, we set the hyper-parameter ηtto 30 seconds
basedonourobservationthattheshortpruningcycle(e.g.,30)is
generally more effective than the large one (e.g., 300) when testing
real-world benchmarks.
Sampling. The first step, sampling, is to obtain the two impor-
tant values from the probabilistic data P, wherePconsists of a
tuple (F,Pstgy,Pratio).Fdenotes a set of nfeatures to represent
each state in the set Sas ann-dimensional boolean vector. Pstgy
is the distribution of an n-dimensional vector θto calculate how
promising eachstate is, and Pratiois thedistribution of theratio r
todecidethenumberofstatestobe pruned.For simplicity,weas-
sumethattheparametervector θandratio raregivenbysampling
stepfromthelearneddistribution PstgyandPratio,respectively.We
explainhowwe obtain thesetwovaluesinSection 3.2.3and3.2.4.
Pruning. The second step is to select the states to be pruned
by using the two sampled values, θandr, and the set Fof features
inP. We define the probabilistic pruning strategy as the following
Prunefunction:
Prune(S,P,ηt)= 
argmin
SP⊆S∧|SP|=|S|∗r/summationdisplay.1
s∈SPscore(s,θ)if(F/nequal∅)
∅ otherwise
wherethefunctionreturnsanemptysetwhentheset Fisempty.
If not, the function scores each state in the set S, and returns the
setSPofthekstates withthe lowestscores in S(e.g.,k=|S|∗r).
To estimate how promising a state is, we first transform each
stateintoafeaturevector.Eachfeaturedenotesabooleanpredicate
that checks whether the path-condition Φof the state scontains
a specific branch condition ϕ. For instance, a feature describes
whether the path-condition Φof the state sinvolves ( α>10), the
branch condition. If true, the feature, feat(s), is 1; otherwise, it is 0.
Formally,the i-th feature isdefinedas:
feati(s)=/braceleftbigg1 if(ϕi∈Φ)∧((_,_,Φ)∈s)
0 otherwise
where it takes a single state sas input and returns 1 or 0. Using the
setFofnfeaturesinthegivenprobabilisticdata P,wecanconvert
astate intoan n-dimensional boolean vector as follows:
feat(s)=⟨feat1(s),feat2(s),...,featn(s)⟩.
149ESEC/FSE ’20, November8ś13,2020,VirtualEvent, USA SooyoungChaandHakjoo Oh
Algorithm2 Our Approach
Input:Aprogram( P), time budget ( N)
Output: Thesetoftest-cases ( T)
1:procedure Homi(P,N)
2:⟨T,D⟩←⟨∅,∅⟩
3:initialize twosample spaces( StimeandSratio)
4:N′←sample fromU(Stime)
5:P←(∅,U([−1,1]n),U(Sratio))
6:repeat
7: T′←Run(P,N′,P)
8: for each(Φ,t)∈T′do
9: D←D∪{(Φ,t,B)} ▷B=Branches(t)
10: GoodD←Extract(D)
11: NewF←FGenenator(GoodD)
12:Pstgy,Pratio,N′←PGenerator(GoodD,NewF)
13:P←(NewF,Pstgy,Pratio)
14: T←T∪T′
15:untilbudgetNexpires
16:returnT
Thefeaturesareautomaticallygeneratedonlinebythedataaccu-
mulatedduringsymbolicexecution.Weexplainhowthesefeatures
are obtainedinSection 3.2.2.
Aftertransformingeachstateintheset Sintoafeaturevector,
wecalculatethescoreofeachstate susingtheinnerproductofthe
feature vector feat(s) andthe sampled n-dimensional vector θas:
score(s,θ)=feat(s)·θ.
For example, when nis 3,θandfeat(s)can be⟨0.4,−0.82,−0.3⟩
and⟨1,0,0⟩, respectively, where the output of score(s,θ)is 0.4. The
featurevector,⟨1,0,0⟩,denotesthatthepath-conditionofthestate s
only contains the branch condition corresponding to 1st feature.
In the vector θ,⟨0.4,−0.82,−0.3⟩, thei-th value represents the
importanceofthe i-th feature.
Finally,the Prunefunctionreturnstheset SPofthe|S|∗rstates
with the lowest scores in the set Sas output, where the pruning
ratioris obtained from the learned distribution Pratio. We remark
that the selection for the set SPcan be done efficiently; after calcu-
lating the score of each state in the total set S, it ranks the states
accordingtotheirscores,andthenpicksthebottom- kstates,where
kis|S|∗r.
Note that we reduce the problem of solving the two technical
problems discussed in Section 2into the problem of learning prob-
abilisticdistributions, PstgyandPratio.
3.2Homi
The key point of our approach, Algorithm 2, is to continuously up-
date the features and the two probabilistic distributions, Pstgyand
Pratio, via online learning during symbolic execution. Except for
theprobabilisticdata( P),theinputandoutputofouralgorithmare
thesameastheonesofAlgorithm 1.UnliketheAlgorithm 1which
performsthe Runprocedureonlyoncewithinthetimebudget N,
ouralgorithmperformsthe Runprocedurentimesbydividing N
intonsmallerbudgets N′.Thisisbecauseouralgorithmterminates
numerousstatesearlythroughtheprobabilisticpruningstrategy
in theRunprocedure; thereby, our algorithm performs the Run
proceduremultipletimeswiththeupdateddata Ptorecovertheterminated promising states. Note that we can also perform Al-
gorithm1in the same way. However, without our state-pruning
strategy,weexperimentallyobservedthatitusuallyperformsbetter
to run Algorithm 1once for a long time period than to do multiple
times for ashort time period(Section 4.4).
WeexplaintheworkflowofhowAlgorithm 2worksindetail.At
line2,Algorithm 2initializestheset Toftest-casesandaccumulated
dataDtoanemptyset,respectively.Atline3,thealgorithminitial-
izes two sample spaces, StimeandSratio; the former Stimedenotes
the sample space for the time budget N′to run the Runprocedure
atline7,andthelatter, Sratio,representsthesamplespaceforthe
pruning ratio used in the probabilistic pruning strategy Prune.
StimeandSratioare definedas:
Stime=[τmax,τitv],Sratio=[ηmax,ηitv]
where the two hyperparameters, τmaxandτitv, are to define the
discrete space of τitvequal intervals with τmaxas the maximum
timebudget.Likewise,thediscretespace Sratioisdefinedequallyby
thetwohyperparameters, ηmaxandηitv.Forinstance,if Stimeand
Sratioare[600,6]and[0.8,4],their discrete spacesare as follows:
Stime=[100,200,300,400,500,600],Sratio=[0.2,0.4,0.6,0.8]
In the experiments, for the space Stime, we setτmaxandτitvto 800
seconds and 4. We respectively set ηmaxandηitvto 0.6 and 3 for
thespace Sratio;thatis,ourstrategyprunes20%or40%or60%of
totalstates for aggressive state-pruning.
At line 4, the algorithm samples the initial time budget N′from
the uniform distribution U(Stime). It initializes the probabilistic
dataPconsisting of a triplet ( F,Pstgy,Pratio) at line 5; initially,
thesetFoffeaturesisanemptysetandeachoftwoprobabilistic
distributions,PstgyandPratio, is a uniform distribution. As the
setFinPissettoanemptyset,thealgorithmperformsthe Run
procedure without any state-pruning on the first iteration of the
loopatlines6-15.Ouralgorithmrepeatsthefollowingtwomain
processes until the time budget Nexpires: 1) performing symbolic
executionwiththeprobabilisticpruningstrategybasedonthedata
P(line7)and2)updatingthedata P(line13).Onthefirstiteration
of the loop, Homiperforms the Runprocedure (Algorithm 1) with
the time budget N′and initial probabilistic data P, and returns
the setT′of generated test-cases at line 7. After the algorithm
calculatestheset Bofbranchescoveredbyeachtest-case tinthe
setT′,we accumulate the tuple ( Φ,t,B) inthe set D(line8-9).
3.2.1 Collecting Promising Data. At line 10, we run the Extract
functiontoextractthemostłpromisingžbutminimalsetofdata,
GoodD,fromtheset Dofaccumulateddata.Conceptually,theset
GoodDis the smallest subset of Dwhere the unions of BinGoodD
is the same with the set of branches covered by all the test-cases in
D.Toformally definethe set GoodD,we firstcalculate D∗as:
D∗=argmax
D′⊆D|/uniondisplay.1
(_,_,B)∈D′B|.
where the notation ‘ argmax’ returns the set D∗of all arguments
thatmaximizetheobjective.Theset D∗isthesetofallsubsetsof
Dwhichcollectivelymaximizethesetofcoveredbranches.Then,
the setGoodDisdefinedas:
GoodD=argmin
D′∈D∗|D′|
150MakingSymbolicExecutionPromising by Learning Aggressive State-Pruning Strategy ESEC/FSE ’20, November8ś13,2020,VirtualEvent, USA
where the notation ‘ argmin’ returns one of the arguments that
minimizetheobjective.Inpractice,calculatingtheset GoodDcor-
responds to solving the set cover problem [ 16], the well-known
np-complete problem. In this paper, we obtain the minimal set
GoodDby applying the greedy algorithm which iteratively selects
theelementhavingthelargestnumberofuncoveredbranchesat
eachstage.
3.2.2 GeneratingFeatures. Atline11, Homigenerates nfeatures
to transform each state into a feature vector in the probabilistic
pruning function, Prune. Intuitively, a feature is a core branch
conditionthatcontributestodeterminingthevalueofatestcase
thateffectivelyincreasesbranchcoverage.Togeneratethefeatures,
weusethecorebranchconditionsinthepath-condition Φcorre-
spondingtoeachpromisingtest-caseintheset GoodD.Wedefine
acorebranchcondition ϕasaconditionthatcanbeexpressedin
the predefinedlanguage Las follows:
ϕ::=cond|cond∧cond|cond∨cond
cond::=lv=n
lv::=α|α[i]
wherethelanguageissmallyetsufficienttorepresenttheminimum
branchconditionsthatarenecessarytodirectlydeterminethevalue
of each test-case. An l-value ( lv) denotes a symbolic value ( α) or
the value of i-th index of an array α(α[i]). A condition ( cond)
consistsofabooleanconditiontoexpressthatthel-valueequalsto
aconstantvalue n. Acore branchcondition( ϕ) isasingle condor
a conjunction (disjunction) of cond. To generate a set of features
from the promising data GoodD, we first collect the set PCof all
path-conditionsfrom GoodDas follows:
PC={Φ|(Φ,_,_)∈GoodD}
Second, we collect the set NewFof new features by extracting core
branchconditions inthe set PCas:
NewF={ϕ∈L|ϕ∈Φ,Φ∈PC}
That is, we extract only the conditions that can be expressed in
thelanguage Lamongthebranchconditionsofeach Φintheset
PC. For instance, suppose that the set PCcontains two sets of path-
conditions as follows:
PC={{(α==3),(α>1)},{(α[2]/nequal3),(α[2]==8)}}.
where the two branch conditions, (α==3)and(α[2]==8), inPC
canbeexpressedinthelanguage L(e.g.,lv=n)whiletheremaining
conditions,(α>1)and(α[2]/nequal3),cannotbe.Hence,wecandefine
the setNewFoffeatures from PCas follows:
NewF={(α==3),(α[2]==8)}
where the two features in the set NewFare the minimal conditions
todetermineamodelofeachpath-condition;forinstance,themodel
of the first path-condition, (α==3)∧(α>1), inPCis equals to
theoneoftheminimalcondition (α==3).Inshort,theset NewF
of generated features at line 11 represents the key evidences of the
minimal test-cases that contribute to maximizing branch coverage
until the currentstate.3.2.3 LearningDistribution. Atline12,thefunction PGenerator
learns the probabilities of two values, n-dimensional weight vector
θandthepruningratio r,andreturnsatuple (Pstgy,Pratio,N′).The
first elementPstgydenotes the probability for the weight vector θ
thatscoreshowpromisingeachstateis,andthesecond, Pratio,is
the probability for the pruning ratio rthat determines the number
ofstatestobepruned.Thetimebudget N′isthenewlyallocated
timebudgetforthe Runprocedureonthenextiterationoftheloop.
The probabilityPstgyconsists of ndistributionsas:
Pstgy=P1×P2×···×P n.
wherePidenotestheprobabilityoftheweightvalue θicorrespond-
ingtothe i-thfeatureintheset NewF.Todefinethe i-thdistribu-
tionPi, we first collect the set of promising test-cases GoodTfrom
GoodDas follows:
GoodT={t| (_,t,_)∈GoodD}.
Whenever each test-case tis generated during symbolic execution,
ouralgorithmadditionallymaintainsaquadrupleofinformation
usedto generateeachtest-case tas follows:
t=(F,θ,r,N′)
whereFisthesetoffeatures, θistheweightvector, risthepruning
ratio, and N′is the time budget. Using this additional information,
we collect the set GoodFof the features which are used at least
once when generating the promising test-case in the set GoodTas
follows:
GoodF=/uniondisplay.1
(F,_,_,_)∈GoodTF.
That is, the set GoodFcontains the features that contribute to gen-
eratingeffectivetest-casesintermsofcodecoverage.Finally,we
can definethe i-th distributionPias:
Pi=/braceleftbiggN(µ(Wi),σ(Wi),−1,1)if(ϕnew
i∈GoodF)
U([−1,1]) otherwise(1)
whereϕnew
idenotes the i-th feature in the set NewFthat hasbeen
generatedat line11 inAlgorithm 2.
Ifthei-thnewfeature( ϕnew
i)belongstotheset GoodFofpromis-
ing features, we learn the probability Piwhich is the truncated
normal distribution with median µ(Wi), standard deviation σ(Wi),
minimum value (-1), and maximum value (1). We define the set Wi
as:
Wi={θk|(ϕnew
i=ϕk)∧({ϕ1,···,ϕn},θ,_,_)∈GoodT}.
Intuitively, Widenotesthesetofweightvaluescorrespondingto
thei-th new feature ϕnew
ithat has already been used for each test-
case inGoodT. Given the set W, the median µ(Wi)and standard
deviation σ(Wi)are calculatedas:
µ(W)=/summationdisplay.1
w∈Ww
|W|,σ(W)=/radicaltp/radicalvertex/radicalbt/summationtext.1
w∈W(w−µ(W))2
|W|.
Ontheotherhand,ifthe i-thnewfeature( ϕnew
i)doesnotbelongto
GoodF, we fix the probability Pito a uniform distribution between
-1 and 1 since there is no accumulated data corresponding to the
i-th new feature for learning. In this way, we learn the probability
Pstgythat consists of the ndistributions from P1toPnbased on
the mostpromisingdata GoodD.
151ESEC/FSE ’20, November8ś13,2020,VirtualEvent, USA SooyoungChaandHakjoo Oh
After the learning process of the probability Pstgy, we calculate
theprobabilityPratioofthe givenpruning ratio r′which is one of
the valuesinthe predefineddiscrete space Sratioas follows:
Pratio(X=r′)=|{(_,_,r,_)∈GoodT|r′=r}|
|GoodT|(2)
Theintuitionisthatthemorethepruningratio risusedtogenerate
promising test-cases GoodT, the higher the probability of the ratio.
Lastly, we sample the new budget N′based on the following
probabilityPtime:
Ptime(X=N′)=|{(_,_,_,N)∈GoodT|N′=N}|
|GoodT|(3)
The intuitionisthe same as the probability Pratioabove.
3.2.4 Sampling Values. We describe how to sample the weight
vector (θ) and ratio ( r) from the two learned distributions, Pstgy
andPratio,inthefirst‘sampling’stepoftheprobabilisticpruning
strategy.First,wesampletheweightvector θbyusingoneofthe
three sampling methods: exploitation, reverse exploitation, and
exploration. The first two methods are to exploit the learned dis-
tributionPstgyas it is or reversely. The last method is to explore
purelyrandom weightvector.
Exploitation. We sample the new weight vector θfrom the
learneddistribution Pstgyitself as:
Sampleexploit(P1×P2×···×P n)=⟨θ1,θ2,···,θn⟩
where the i-th weight value θiis sampled from the i-th probability
Piin(1).Ourexpectationisthatthenewweightvector θstatisti-
cally similar to the promising weight vectors in the set GoodDwill
likely increasethe codecoverageonthe nextiterationoftheloop.
ReverseExploitation. Wesamplethenewweightvector θrby
exploitingthelearneddistribution Pstgyreversely.Wefirstgenerate
thesetof100real-numbers, U,bysamplingtheuniformdistribution
between -1 and1as:
U={r1,r2,...,r100|ri∼U(−1,1)}.
The samplingmethodtakestheprobability Pstgyandtheset Uas
inputandreturns the newweightvector θras:
Samplereverse(P1×P2×···×P n,U)=⟨θ1
r,θ2
r,···,θn
r⟩
We assume that the i-th weight value θiis sampled from the prob-
abilityPidefined in ( 1). Then the i-th reverse weight value θiris
calculatedas follows:
θi
r=argmax
u∈U|u−θi|
where thereverse value θirinUis thefarthest one fromthevalue
θisampledfromthedistribution Pi.Hence,θirrepresentsthevalue
thatis themostunlikelytobe sampledinthelearneddistribution
Pstgy.Weexpectthatthisweightvectorwouldleadthesymbolic
execution to explore the branches uncovered in previous iterations.Table 2:9 benchmarkprograms
Programs LOC # ofBranches
gawk-3.1.4 60,904 11,934
grep-2.6 56,931 7,021
combine-0.4.0 35,756 2,359
trueprint-5.4 12,229 2,518
ginstall(8.31) 22,290 3,652
ptx(8.31) 22,148 5,262
vdir(8.31) 19,378 3,830
pr (8.31) 12,156 1,991
dd (8.31) 10,531 1,547
Exploration. For the last method, we generate a weight vector
θby sampling from the uniform distribution U([−1,1]n), where
thei-th value θiis a random real-number between -1 and 1. In the
experiments,toaccumulateenoughdata Dforlearningthedistribu-
tionPstgy,Algorithm 2repeatstheloop,usingonlytheexploration
methodmtimes (e.g., m=10). After enough data is collected, we set
the same probabilitiesfor the three sampling methods.
Finally,wesamplethepruningratio rbasedontheprobability
in(2)whensamplingtheweightvector θbyexploitationorreverse
exploitation. Otherwise, when sampling the weight vector θby
exploration, the pruning ratio is randomly sampled in the uniform
distributionU(Sratio). Likewise, we obtain the next testing budget
N′on the same basis as sampling the pruning ratio; that is, we
samplethebudgetfromtheuniformdistribution U(Stime)forthe
explorationcaseonly.If not, we sample the one in( 3).
Astheloopatlines6-15inAlgorithm 2iterates,ourtechnique,
Homi, isableto makesmarter decisions on howto representeach
state (F), how promising each state is ( Pstgy), and how many states
are pruned(Pratio).
4 EXPERIMENTS
In this section, we experimentally evaluate our approach, Homi, to
answer the following research questions:
•Effectiveness : How effectively does Homiimprove branch
coverage? How many branches and bugs are reachable by
Homionly? (Section 4.2)
•The number ofstates : How many states does Homimain-
tainduringtestingcomparedtogeneralsymbolicexecution?
(Section4.3)
•Comparisonwith naive approach : How well does Homi
(Algorithm 2)performcomparedtosymbolicexecutionwith
random state-pruning? (Section 4.4)
We implemented our approach in a tool, Homi, on top of KLEE [ 5],
apubliclyavailablesymbolicexecutiontoolfortestingCprograms.
WeconductedallexperimentsonaLinuxmachineequippedwith
twoIntelXeonProcessorsE5-2630and192GBRAM,whereithasa
totalof16 cores and32 threads.
4.1 Settings
Benchmarks. We used 9 GNU open-source C programs for
evaluation.Table 2showsthetotalnumberoflinesandbranchesfor
eachbenchmark,wherethelargestbenchmark, gawk,hasabout12K
152MakingSymbolicExecutionPromising by Learning Aggressive State-Pruning Strategy ESEC/FSE ’20, November8ś13,2020,VirtualEvent, USA
branches.ThelastfivebenchmarksinTable 2areamongthelarger
programsinGNUCoreutils-8.31.Toconstructourbenchmarksuite,
we usedtwocriteria: 1) the benchmarkshave been widely usedin
priorworkondynamicsymbolicexecution[ 4,5,9ś11,21,24,29],
and2)theyarerelativelylargerandmorechallengingthanthose
often usedinexisting work onKLEE.
Baselines. We compared our approach with the general sym-
bolic execution (Algorithm 1) without state-pruning but with 9
different search heuristics. Specifically, we used the following 9
searchheuristics: CPICount (CallPathInstructionCount), CovNew,
MinDistance (Minimal Distance to Uncovered), InstrCount (In-
struction Count), QueryCost ,RandomPath ,Depth,RandomState ,
andRoundRobin ; the last heuristic is the default heuristic of KLEE
thatuses CovNewandRandomPath inaroundrobinfashion.All
theseheuristicsareimplementedinKLEE[ 5].Notethatwedelib-
erately used 9 search heuristics instead of using only the default
heuristic.Thisisbecause,asdemonstratedinSection 4.2,theperfor-
mance of the general symbolic execution varies greatly depending
onboth the subjectprogram andsearchheuristic.
Weapplied Homiontopofthebestsearchheuristicthatachieves
the highest branch coverage for each program. For instance, we
appliedHomiontopofthe MinDistance heuristicfor gawkwhile
applyingHomiontopofthe CPICount heuristicfor grep.Notethat
Homiandsearchheuristicsare orthogonal, andtheyare naturally
combined since Homiworks regardless of search heuristics; in a
generic symbolic execution algorithm (Algorithm 1),Homidecides
whichstatestopruneatline6whilesearchheuristicsdetermine
whichstates to explore further at line10.
Other Settings. For all evaluations, we maintained the same
experimental environments: symbolic arguments, time budget, and
memorycapacity.First,weusedthesymbolicargumentsusedin[ 5]
(e.g., "--sym-args 0 1 10 --sym-args 0 2 2 --sym-files 1 8 --sym-stdin
8 --sym-stdout"). Second, we used the same memory capacity, 2GB,
where it is the default setting of KLEE. Lastly, we allocated 5 hours
toboththebaselines(Algorithm 1)andourtechnique(Algorithm 2)
for all benchmarks as time budget. We repeated all experiments
five times andreportedthe averageresults.
4.2 Effectiveness
We evaluate the effectiveness of our approach, Homi, from two
perspectives:branchcoverageandbug-findingcapability.Insum-
mary,Homiisabletosignificantlyincreasebranchcoverageand
exclusively find bug-triggering inputs, compared to the general
symbolic execution.
4.2.1 Branch Coverage. For each benchmark in Table 2, we report
the average number of total covered branches (Figure 1) and ex-
clusively covered branches (Table 3), byHomiand top-5 search
heuristics, respectively. As both the general symbolic execution
(Algorithm 1) andHomi(Algorithm 2) return as output the set
of test-cases, we plotted the number of branches covered by all
preceding test-cases to depict the coverage graph in Figure 1. In
particular,whenthetimebudget(5h)expired,were-executedthe
binary of the program with each test-case in the set Tsequentially,
wherethe‘sequence’denotesthetimeeachtest-casewascreated.Wecalculatedthecumulativenumberofcoveredbranchescorre-
spondingtothecreationtimeofthetest-case;weused gcov,one
ofthemostpopulartoolsformeasuringcodecoverage.Aswemen-
tionedinSection 3.2,Homiperformsthegeneralsymbolicexecution
withoutstate-pruningonthefirstiterationoftheloop.Hence,to
demonstrate the benefits of state-pruning only, we have plotted
the accumulated number of covered branches after the time for the
first iteration of Algorithm 2elapsed. In our experiments, we first
performthegeneralsymbolicexecutionfor800seconds,recordthe
calculated number of covered branches on the graph, and then run
Algorithm 1fortheremaining timeperiod(e.g.,5h-800seconds).
In other words, the graphs in the Figure 1can clearly demonstrate
thecomparisonoftheperformanceofsymbolicexecutionwithand
withoutstate-pruning technique.
Figure1demonstratesthe average numberof branchescovered
bythesearchheuristicsovertimein9benchmarks.Weusedatotal
of 6 heuristics for each benchmark, consisting of the top five of the
nineoriginalsearchheuristicsandourtechniqueappliedtothebest
one among the five. The experimental results show that the search
heuristicwith Homisucceedsinachievingthehighestbranchcover-
age for all benchmarks. In particular, for the two largest programs,
gawkandgrep,Hominotably increases the number of covered
branches compared to the bestheuristic without it.For instance, in
gawk,thesearchheuristicwith Homi,MinDistance +Homi,covered
about2,884brancheswhile MinDistance heuristicitselfmanaged
to cover about 2,447 branches only. Likewise, in grep, when the
time budget (5h) expired,the best heuristic( CPICount )with and
withoutHomicoveredabout2,851and2,505branches,respectively.
Moreover, as shown in a benchmark trueprint , the rate for the
coverageincreaseovertimeoftheheuristicequippedwith Homi
was noticeably higher than the ones of other five search heuristics.
Inthetwobenchmarks, combineandvdir,applying Homitothe
bestheuristichas successfully coveredabout100more branches.
Figure1showsthatinmostprograms,thenumberofbranches
covered byeach top-5search heuristics risessharply at theend of
the timebudget; this interestingfact is observedbecause we have
reportedthebranchcoveragecoveredovertime.Thisphenomenon
occurssincethealgorithm(Algorithm 1)generatesthetest-cases,
at lines 20-21, for each state that has not yet reached the halt state-
ment after the time budget expires, and the generated test-cases
contribute to increasing the total number of branch coverage a lot.
Thisimplicitlyshowsthatthegeneralsymbolicexecutionfailsto
preferentially explore such promisingstates more.
Note that we applied Homiwith the best search heuristic just
becauseitismorechallengingtoimprovetheperformanceofthe
heuristic that has already achieved high code coverage. In fact,
Homiperformswellregardlessofthesearchheuristic.Forinstance,
applying Homievenwiththe6thsearchheuristic( CPICount )on
gawkinFigure 1cancovermorebranchesthanapplying CPICount
withoutHomi;CPICount +Homicovered2,356branchesonaverage
whileCPICount covered2,093 only.
4.2.2 Exclusively Covered Branches. Table3shows the number
of exclusively covered branches achieved by each technique. In
Table3, thei-th best heuristic on each benchmark corresponds
to the one in Figure 1; for instance, the best (BestH) and second
best heuristic (2ndH) on combine areRandomPath andCPICount ,
153ESEC/FSE ’20, November8ś13,2020,VirtualEvent, USA SooyoungChaandHakjoo Oh
0
 2500
 5000
 7500
 10000
 12500
 15000
 17500
1200
1400
1600
1800
2000
2200
2400
2600
2800# of Covered Branches
gawk
0
 2500
 5000
 7500
 10000
 12500
 15000
 17500
time(s)
0
200
MinDistance+ Homi
MinDistanceQueryCost
InstrCount
CovNew
CPICount
0
 2500
 5000
 7500
 10000
 12500
 15000
 17500
1800
2000
2200
2400
2600
2800# of Covered Branches
grep
0
 2500
 5000
 7500
 10000
 12500
 15000
 17500
time(s)
0
200
CPICount+ Homi
CPICountQueryCost
CovNew
InstrCount
RandomState
0
 2500
 5000
 7500
 10000
 12500
 15000
 17500
300
400
500
600
700
800
900# of Covered Branches
combine
0
 2500
 5000
 7500
 10000
 12500
 15000
 17500
time(s)
0
100
RandomPath+ Homi
RandomPath
CPICount
CovNewQueryCost
RoundRobin
0
 2500
 5000
 7500
 10000
 12500
 15000
 17500
700
800
900
1000
1100
1200
1300
1400
1500
1600# of Covered Branches
vdir
0
 2500
 5000
 7500
 10000
 12500
 15000
 17500
time(s)
0
100
CovNew+ Homi
CovNew
RoundRobin
QueryCost
MinDistance
RandomState
0
 2500
 5000
 7500
 10000
 12500
 15000
 17500
1100
1200
1300
1400
1500
1600
1700
1800# of Covered Branches
ptx
0
 2500
 5000
 7500
 10000
 12500
 15000
 17500
time(s)
0
200
CovNew+ Homi
CovNew
CPICount
QueryCost
RandomState
RoundRobin
0
 2500
 5000
 7500
 10000
 12500
 15000
 17500
400
500
600
700
800
900# of Covered Branches
trueprint
0
 2500
 5000
 7500
 10000
 12500
 15000
 17500
time(s)
0
100
MinDistance+ Homi
MinDistance
RandomPath
RoundRobinQueryCost
CovNew
0
 2500
 5000
 7500
 10000
 12500
 15000
 17500
600
700
800
900
1000
1100
1200# of Covered Branches
ginstall
0
 2500
 5000
 7500
 10000
 12500
 15000
 17500
time(s)
0
100
CPICount+ Homi
CPICount
RoundRobin
RandomPath
Depth
CovNew
0
 2500
 5000
 7500
 10000
 12500
 15000
 17500
600
700
800
900
1000
1100
1200# of Covered Branches
pr
0
 2500
 5000
 7500
 10000
 12500
 15000
 17500
time(s)
0
200
CovNew+ Homi
CovNewQueryCost
RoundRobin
RandomState
CPICount
0
 2500
 5000
 7500
 10000
 12500
 15000
 17500
350
375
400
425
450
475
500
525
550# of Covered Branches
dd
0
 2500
 5000
 7500
 10000
 12500
 15000
 17500
time(s)
0
100
CPICount+ Homi
CPICount
RoundRobin
RandomPath
Depth
RandomState
Figure 1:The average branch coverage achieved by top-5 heuristics and Homion 9 benchmarks
Table 3: The average number of branches exclusively cov-
ered by top-5 heuristics and Homion 9 benchmarks
BestH+HomiBestH 2ndH 3rdH 4thH 5thH
gawk 139 10 26 37 42 26
grep 208 53 22 116 14 1
combine 62 0 15 15 1 6
vdir 118 44 19 14 1 2
ptx 39 4 35 2 0 4
trueprint 147 17 0 0 3 52
ginstall 16 3 1 6 0 0
pr 61 3 1 2 0 3
dd 23 17 2 0 0 0
Total 813 151 121 192 61 94
respectively.Thenumberachievedbyourtechnique(BestH+ Homi)
denotesthenumberofbranchesthatourtechniquecoversbutall
theremainingtop-5heuristicsfailtocover.Theresultsshowthatourtechnique(BestH+ Homi)ishighlyeffectiveinincreasingthe
number of exclusively covered branches. In total, the best heuristic
withHomiwas able to cover 813 branches while the best heuristic
withoutHomicovered only 151 branches; in summary, the former
exclusively covered 5.4 times more branches than the latter. For
instance,inthelargestprogram gawk,Homisignificantlyenhanced
the performance of the best heuristic ( MinDistance ) by about 13.9
times. Likewise, our technique ( CPICount +Homi) ongrepexclu-
sivelycovered208branches, butthe best heuristic alone managed
to exclusivelycover 53 branches.
In addition, the number of exclusive branches covered by our
technique (BestH+ Homi) is even greater than the sum of the num-
bers of branches exclusively covered by each of top-5 heuristics,
wheretheformeris813andthelatteris619,i.e.,151+121+192+61+94.
The number of branches that one of the top-5 heuristics can cover
but our technique (BestH+ Homi) fails is 816. In other words, the
branchcoverageachievedbyapplying Homitothebest-heuristic
onlyfor5hoursisalmostthesameastheoneachievedbyapplying
154MakingSymbolicExecutionPromising by Learning Aggressive State-Pruning Strategy ESEC/FSE ’20, November8ś13,2020,VirtualEvent, USA
Table 4:Comparisonofbug-finding ability oftop-2 heuristics with vs.without Homi.
Benchmarks Crash-Types Bug-Triggering Inputs Error Locations BestH+ HomiBestH 2ndH+ Homi2ndH
gawk-3.14Abnormal-termination "--nostalgi" "-" ‘Line:1044 inmain.c’ ✔ ✔ ✔ ✔
Abnormal-termination "--compat" "-m" "r " ‘Line:526in/libc/stdlib/stdlib.c’ ✔ ✘ ✔ ✘
grep-2.6 Abnormal-termination "\n\w*\'*\n" "-" ‘Line:1432 in/src/dfa.c’ ✘ ✘ ✔ ✘
combine-0.4.0Segmentationfault "--field=,," ‘Line:385in/src/field.c’ ✔ ✔ ✔ ✔
Segmentationfault "--fi=r.o1’" "-r" "" ‘Line:633in/src/df_options.c’ ✔ ✘ ✘ ✘
each of top-5 heuristics for 5 hours (25 hours in total). Despite the
obviousadvantagesof Homi,itisstillnotoptimalsinceitalsofailed
to cover 816 branches achieved by top-5 search heuristics. From
this observation, selective decision on applying Homiwould be an
interestingfuture work.
4.2.3 Bug-Finding Capability. In Table4, we compared the bug-
finding capability of two best heuristics both with and without
Homi,respectively,forthethreelargestbenchmarks: gawk,grep,
andcombine. In summary, Homifound a total of five reproducible
bugsonthethreebenchmarks.Inparticular,thethreebugswere
only detectable by Homiwhile the general symbolic execution
failedto find thesebugs.
Table4showsthebenchmark,thecrash-type,thebug-triggering
input generated by Homi, the error-location, and success or failure
ofbug-findingforeachtechniqueinorder.Inparticular,wemarked
eachtechniqueas‘success’( ✔)whenthetechniquesucceededin
findingthebugatleastonceduringfiveiterationsofthetimebudget
(5h).Onthecontrary,wheneachtechniquetotallyfailedtofindthe
bugduringthetimeperiod(5h ∗5times),wemarkeditas‘failure’
(✘).Theresultsshowthatourtechniquewasabletogenerateatotal
offourdistinctbug-triggeringinputsin gawkandcombine,butthe
best heuristic without Homionly generated the two inputs. We
confirmedthatthefirstbug-triggeringinput("--nostalgi""-")found
ingawkisreproducibleinthelatestversion(gawk-5.0.1).Oneinter-
estingpointisthatthediscoveredbugsaredifferentwhenapplying
Homito the bestandthe secondbestheuristics; the bestheuristic
withHomicausedacrash,abnormal-termination,on combinewhile
thesecondbestonecausedasegmentationfaulton grep.Thatis,
we expect that applying Homito diverse (new) search heuristics
willallowmore bug-detection.
4.3 The NumberofCandidate States
ForeachbenchmarkinTable 2,wecomparethenumberofstates
that our technique and the general symbolic execution maintain
during the testing period. Figure 2shows the average number of
states that can be selected by each technique for every second;
moreprecisely,theaveragenumberdenotesthesetsizeofstates,
|S|,atline10inAlgorithm 1.Theresultsshowthatourtechnique
(BestH+Homi) maintains a relatively small number of states for
most of the time period on all benchmarks compared to the gen-
eral symbolic execution. When performing the general symbolic
execution without the state-pruning at first, which is the first it-
eration of the loop in Algorithm 2, our technique also faced thestate-explosionproblem.Afterthefirstiteration,however,ourshas
successfullymaintainedasmallnumberofstates.Forinstance,in
gawk,ourtechnique( MinDistance +Homi)keptabout1,897states
persecondonaveragewhilethe MinDistance heuristicmaintained
about 37,315 states. In other words, our technique succeeded in
achieving the highest branch coverage in Figure 1while maintain-
ing 19.7 times fewer states than the general symbolic execution.
Ingrep,CPICount +HomiandCPICount retained2,030and41,210
states on average, respectively. In vdir, even after the first itera-
tion of the loop in Algorithm 2, our technique sometimes faced the
state-explosion problem. We confirmed that this problem occurs
becausethenumberofstatesgrowsexponentiallyfasterthanthe
number prunedbyour state-pruning strategy.
Forthegeneralsymbolicexecutionwithoutourtechnique,we
observed that keeping the fewer number of states is not directly
related to improving the branch coverage. For instance, in ptx,
RoundRobin heuristicmaintainsabout4,660statesduringthesym-
bolicexecution,whichisalmostthesamenumberofstatesmain-
tainedbyourtechnique( CovNew+Homi).However,Figure 1shows
thatRoundRobin covered about 300 fewer branches on average
thanCovNew ofwhichthenumberofstatesisabout60,053during
symbolicexecution.Inanotherbenchmark, gawk,thenumbersof
candidate states maintained by CPICount andMinDistance are al-
mostthesame, where theformeris 36,299 andthelatter is 37,315.
However, the difference in the number of covered branches by the
twotechniquesisapproximatelyabout350branches.Thatis,the
key answer for increasing code coverage is not to blindly main-
tain the number of states, but to smartly keep only the łpromisingž
states.
Although Homisuccessfullymaintainssuchpromisingstateson
ourbenchmarksuite,wewerenotabletoprovidehigh-levelinsights
intowhythosestatesarepromising.Inourapproach,wedetermine
how promising a state is based on its corresponding feature vector
andweightvector.However,asourlearningalgorithmrepresents
each state as low-level features that check whether it contains core
branch conditions, it was difficult to decode the learning outcomes
anddescribe the intuitionbehindpromisingstates.
4.4 Comparisonwith NaiveApproaches
We evaluate the efficacy of Homi(Algorithm 2) by comparing it
withtwonaivemethods.Thefirstnaivemethodistoreplacethe
probabilistic pruning strategy ( Prune) in Algorithm 1with the
random pruning strategy ( RandomPrune ), and then to perform
155ESEC/FSE ’20, November8ś13,2020,VirtualEvent, USA SooyoungChaandHakjoo Oh
0
2000
 4000
 6000
 8000
10000
 12000
 14000
 16000
time(s)
0
10000
20000
30000
40000
50000# of States
gawk
MinDistance+ Homi
MinDistanceQueryCost
InstrCount
CovNew
CPICount
0
2000
 4000
 6000
 8000
10000
 12000
 14000
 16000
time(s)
0
20000
40000
60000
80000# of States
grep
CPICount+ Homi
CPICount
QueryCost
CovNew
InstrCount
RandomState
0
2000
 4000
 6000
 8000
10000
 12000
 14000
 16000
time(s)
0
500
1000
1500
2000
2500
3000# of States
combine
RandomPath+ Homi
RandomPath
CPICount
CovNewQueryCost
RoundRobin
0
2000
 4000
 6000
 8000
10000
 12000
 14000
 16000
time(s)
0
10000
20000
30000
40000
50000
60000# of States
vdir
CovNew+ Homi
CovNew
RoundRobin
QueryCost
MinDistance
RandomState
0
2000
 4000
 6000
 8000
10000
 12000
 14000
 16000
time(s)
0
10000
20000
30000
40000
50000
60000
70000# of States
ptx
CovNew+ Homi
CovNew
CPICount
QueryCost
RandomState
RoundRobin
0
2000
 4000
 6000
 8000
10000
 12000
 14000
 16000
time(s)
0
10000
20000
30000
40000
50000
60000# of States
trueprint
MinDistance+ Homi
MinDistance
RandomPath
RoundRobinQueryCost
CovNew
0
2000
 4000
 6000
 8000
10000
 12000
 14000
 16000
time(s)
0
20000
40000
60000
80000
100000# of States
ginstall
CPICount+ Homi
CPICount
RoundRobin
RandomPath
Depth
CovNew
0
2000
 4000
 6000
 8000
10000
 12000
 14000
 16000
time(s)
0
10000
20000
30000
40000
50000
60000
70000# of States
pr
CovNew+ Homi
CovNewQueryCost
RoundRobin
RandomState
CPICount
0
2000
 4000
 6000
 8000
10000
 12000
 14000
 16000
time(s)
0
20000
40000
60000
80000
100000# of States
dd
CPICount+ Homi
CPICount
RoundRobin
RandomPath
Depth
RandomState
Figure 2:The average numberofstatesforeachtechniqueto selecton 9 benchmarks
Algorithm 2withoutonlinelearning(line10-13) as:
RandomPrune(S,r)={s∈SP|SP⊆S∧|SP|=|S|∗r}
whereris sampledfromthe uniformdistribution U(Sratio)defined
in Section 3.2. The second naive method is to perform the general
symbolic execution (Algorithm 1) multiple times by dividing the
total budget (5h) into smaller budgets N′, whereN′is sampled
fromU(Stime)definedinSection 3.2.Ingrep,wecomparedbranch
coverage achieved by ours ( CPICount +Homi), the best heuristic
(CPICount ), the first naive approach ( CPICount +RandomPrune ),
andthe secondone ( CPICount [Divide]),respectively.
Figure3shows that Homi(Algorithm 2) is essential to effec-
tively improve branch coverage. For example, ours covered at
least 300 more branches than the second best method ( CPICount +
RandomPrune ).The secondandthethirdbestmethods( CPICount
+RandomPrune andCPICount ) achieved nearly identical branch
coveragewhentimebudgetexpired.Forthegeneralsymbolicex-
ecution(Algorithm 1)withoutstate-pruning,itismuchbetterto
0
 2500
 5000
 7500
 10000
 12500
 15000
 17500
1800
2000
2200
2400
2600
2800# of Covered Branches
grep
0
 2500
 5000
 7500
 10000
 12500
 15000
 17500
time(s)
0
200
CPICount+ Homi
CPICount+RandomPrune
CPICount
CPICount[Divide]Figure 3:Comparisonwith twonaive approacheson grep
performsymbolicexecutionforalongtimethantoperformsym-
bolicexecutionseveraltimeswithsmallbudget;theformerandthe
latter wasableto cover 2,505 and2,406 branches, respectively.
156MakingSymbolicExecutionPromising by Learning Aggressive State-Pruning Strategy ESEC/FSE ’20, November8ś13,2020,VirtualEvent, USA
4.5 Threatsto Validity
(1) We manually tuned the several hyper-parameters: ηt,ηmax, and
ηitv. To determine each value, we ran Homiwith a few different
values(e.g.,30,300)onthreebenchmarks,andchoseanappropriate
one achieving the highest coverage during the experiments. Then,
weappliedthesamevaluefortheremainingsixbenchmarks.How-
ever,thetunedvaluesmaynotbesuitableforlargeropen-sourceC
programs(e.g.,LOC>100K).(2)Inevaluation,weusedboththede-
faultSMTsolverofKLEE(STP[ 13])andthedefaultmemorybudget
(2GB).However,theperformanceof Homimayvaryfordifferent
SMT solvers and memory budgets. (3) We used 9 C open-source
programsextensivelyusedinpreviousworks[ 4,5,9ś11,21,24,29].
Butthesemaynot be representative.
5 RELATED WORK
Inthissection,wediscussexistingworksthatarecloselyrelatedto
our goaland approach, respectively.At a high level, our goalis to
prunethesearchspaceofsymbolicexecution[ 2,3,15,27,30,31],
andourapproachbelongstothetechniquesthatcombinesymbolic
executionwithmachine learning[ 9ś12,18,25].
ReducingSearchSpaceofSymbolicExecution. Homiisdif-
ferent from and orthogonal to the existing techniques [ 2,3,15,
27,30,31]. These techniques aim to conservatively prune redun-
dant states based onsome predefined criteria. On the other hand,
Homiaims toaggressively prune the states based on adaptivecrite-
rialearnedonlineduringsymbolicexecution.Theread-writeset
(RWset) analysis [ 2] aims to prune program paths that will execute
thesamebasicblocksaspreviouslyexploredpaths.Likewise,the
goalofpost-conditionedsymbolicexecution[ 30,31]istodiscard
the states having the same path suffixes as previously explored
states during testing. Jaffar et al. [ 15] aims to subsume the paths
guaranteed to be unreachable to the annotated assertions in the
program. Chopper [ 27] presents a novel technique to perform sym-
bolicexecutionwhilesafelyexcludingtheirrelevantfunctionsin
theprogramwhicharenotthetargetsofuserstotest.Notethatour
tool,Homi, can further enhance symbolic execution by combining
thesetechniques that safely prune redundantpaths.
Combining Symbolic Execution with Learning. Ourwork
aligns with thisline of researchthat employsmachinelearning to
boost symbolic execution [ 9ś12,18,25]. ParadySE [ 9] presents a
new approachto automatically generate searchheuristics of sym-
bolic execution via offline learning. Chameleon [ 11] is a novel
symbolicexecutionthatadaptivelyswitchessearchheuristicsfor
better performance via online learning. MLB [ 18] uses machine
learning to effectively handle the complex path-conditions that
involve externalfunction callsor floatingpointarithmeticinsym-
bolic execution. LEO [ 12] is a machine-learning based approach to
boost symbolic execution by transforming the program under test
into an easy-to-analyze program while preserving its semantics.
ConTest[ 10]aimstolearnusefultemplatesthatreducetheinput
space of the program under test by selectively generating symbolic
variablesduringtesting.Ontheotherhand,weusealearningal-
gorithm to aggressively prune unpromising states online during
symbolic execution.6 CONCLUSION
We present a new approach, Homiwith the goal of maintaining
promising states only via aggressive state-pruning. The key idea
is to continuously learn the probabilistic pruning strategy based
on the cumulative data during the testing period. Experimental
results on 9 open-source Cprojects showthat symbolic execution
withHomiisabletonotablyincreasebranchcoverageandfindreal
bugs while keeping a relatively small set of states. We believe that
minimizingcandidate states insymbolicexecutionwillemerge as
anewsolution against the state-explosionproblem.
ACKNOWLEDGMENTS
This work was supported by Samsung Research Funding & Incuba-
tionCenterofSamsungElectronicsunderProjectNumberSRFC-
IT1701-51. This work was partly supported by Institute of Informa-
tion& communicationsTechnologyPlanning & Evaluation(IITP)
grant funded by the Korea government(MSIT) (No.2020-0-01337,
(SWSTARLAB)ResearchonHighly-PracticalAutomatedSoftware
Repair)andNext-GenerationInformationComputingDevelopment
Program through the National Research Foundation of Korea(NRF)
fundedbythe Ministry of Science,ICT (2017M3C4A7068175).
REFERENCES
[1]SaswatAnand,MayurNaik,MaryJeanHarrold,andHongseokYang.2012. Auto-
matedConcolicTestingofSmartphoneApps.In ProceedingsoftheACMSIGSOFT
20th International Symposium on the Foundations of Software Engineering (FSE
’12). 1ś11.
[2]Peter Boonstoppel, Cristian Cadar, and Dawson Engler. 2008. RWset: Attacking
pathexplosioninconstraint-basedtestgeneration.In InternationalConference
on Tools and Algorithms for the Construction and Analysis of Systems (TACAS ’08) .
351ś366.
[3]SuhabeBugraraandDawsonEngler.2013. RedundantStateDetectionforDy-
namic Symbolic Execution. In Proceedings of the 2013 USENIX Conference on
Annual TechnicalConference (USENIXATC’13) . 199ś212.
[4]Jacob Burnim and Koushik Sen. 2008. Heuristics for Scalable Dynamic Test Gen-
eration. In Proceedings of23rd IEEE/ACM International Conference on Automated
SoftwareEngineering (ASE ’08) . 443ś446.
[5]Cristian Cadar, Daniel Dunbar, and Dawson Engler. 2008. KLEE: Unassisted and
AutomaticGenerationofHigh-coverageTestsforComplexSystemsPrograms.
InProceedings of the 8th USENIX Conference on Operating Systems Design and
Implementation (OSDI ’08) . 209ś224.
[6]Cristian Cadar and Dawson Engler. 2005. Execution Generated Test Cases:
How to Make Systems Code Crash Itself. In Proceedings of the 12th International
Conference onModelChecking Software (SPIN’05) . 2ś23.
[7]Cristian Cadar, Vijay Ganesh, Peter M. Pawlowski, David L. Dill, and Dawson R.
Engler.2008. EXE:AutomaticallyGeneratingInputsofDeath. ACMTrans.Inf.
Syst.Secur. 12,2 (2008), 10:1ś10:38.
[8]Cristian Cadar and Koushik Sen. 2013. Symbolic Execution for Software Testing:
ThreeDecades Later. Commun. ACM 56,2 (2013), 82ś90.
[9]SooyoungCha,SeongjoonHong,JunheeLee,andHakjooOh.2018.Automatically
Generating Search Heuristics for Concolic Testing. In Proceedings of the 40th
InternationalConference onSoftwareEngineering (ICSE’18) . 1244ś1254.
[10]Sooyoung Cha, Seonho Lee, and Hakjoo Oh. 2018. Template-guided Concolic
TestingviaOnlineLearning.In Proceedingsofthe33rdACM/IEEEInternational
Conference onAutomatedSoftwareEngineering (ASE ’18) . 408ś418.
[11]SooyoungChaandHakjooOh.2019. ConcolicTestingwithAdaptivelyChanging
SearchHeuristics.In Proceedingsofthe201927thACMJointMeetingonEuropean
SoftwareEngineeringConferenceandSymposiumontheFoundationsofSoftware
Engineering (ESEC/FSE ’19) . 235ś245.
[12]JunjieChen,WenxiangHu,LingmingZhang,DanHao,SarfrazKhurshid,andLu
Zhang. 2018. Learning to accelerate symbolic execution via code transformation.
In32nd European Conference onObject-OrientedProgramming (ECOOP’18) .
[13]Vijay Ganesh and David L. Dill. 2007. A Decision Procedure for Bit-Vectors and
Arrays. In Proceedings of the 19th International Conference on Computer Aided
Verification (CAV’07) . 519ś531.
[14]Patrice Godefroid, Nils Klarlund, and Koushik Sen. 2005. DART: Directed Auto-
mated Random Testing. In Proceedings of the 2005 ACM SIGPLAN Conference on
ProgrammingLanguage Designand Implementation (PLDI’05) . 213ś223.
157ESEC/FSE ’20, November8ś13,2020,VirtualEvent, USA SooyoungChaandHakjoo Oh
[15]JoxanJaffar,VijayaraghavanMurali,andJorgeA.Navas.2013. BoostingConcolic
Testing viaInterpolation.In Proceedings ofthe 9thJoint Meetingon Foundations
ofSoftwareEngineering (ESEC/FSE ’13) . 48ś58.
[16]RichardMKarp.1972. Reducibilityamongcombinatorialproblems. In Complexity
ofcomputer computations . 85ś103.
[17]Su Yong Kim, Sangho Lee, InsuYun, Wen Xu, Byoungyoung Lee, Youngtae Yun,
and Taesoo Kim. 2017. CAB-Fuzz: Practical Concolic Testing Techniques for
COTS Operating Systems. In 2017 USENIX Annual Technical Conference (USENIX
ATC ’17). 689ś701.
[18]XinLi,YongjuanLiang,HongQian,Yi-QiHu,LeiBu,YangYu,XinChen,and
Xuandong Li. 2016. Symbolic Execution of Complex Program Driven by Ma-
chineLearningBasedConstraintSolving.In Proceedingsofthe31stIEEE/ACM
International Conference on Automated Software Engineering (ASE ’16) . 554ś559.
[19]YouLi,ZhendongSu,LinzhangWang,andXuandongLi.2013. SteeringSymbolic
Execution to Less Traveled Paths. In Proceedings of the 2013 ACM SIGPLAN
InternationalConferenceonObjectOrientedProgrammingSystemsLanguages,and
Applications (OOPSLA’13) . 19ś32.
[20]Loi Luu, Duc-Hiep Chu, Hrishi Olickel, Prateek Saxena, and Aquinas Hobor.
2016. Making SmartContracts Smarter. In Proceedings ofthe 2016ACM SIGSAC
Conference onComputer and Communications Security (CCS’16) . 254ś269.
[21]Sergey Mechtaev, Alberto Griggio, Alessandro Cimatti, and Abhik Roychoud-
hury.2018. SymbolicExecutionwithExistentialSecond-OrderConstraints.In
Proceedingsofthe201826thACMJointMeetingonEuropeanSoftwareEngineering
Conference and Symposium on the Foundations of Software Engineering (ESEC/FSE
’18). 389ś399.
[22]Ivica Nikoliundefined, Aashish Kolluri, Ilya Sergey, Prateek Saxena, and Aquinas
Hobor.2018. FindingTheGreedy,Prodigal,andSuicidalContractsatScale.In
Proceedingsofthe34thAnnualComputerSecurityApplicationsConference(ACSAC
’18). 653ś663.
[23]KoushikSen,DarkoMarinov,andGulAgha.2005. CUTE:AConcolicUnitTesting
EngineforC.In Proceedingsofthe10thEuropeanSoftwareEngineeringConferenceHeld Jointly with 13th ACM SIGSOFT International Symposium on Foundations of
SoftwareEngineering (ESEC/FSE ’05) . 263ś272.
[24]HyunminSeoandSunghunKim.2014. HowWeGetThere:AContext-guided
Search Strategy in Concolic Testing. In Proceedings of the 22nd ACM SIGSOFT
International SymposiumonFoundations ofSoftwareEngineering (FSE’14) .413ś
424.
[25]ShiqiShen,ShwetaShinde,SoundaryaRamesh,AbhikRoychoudhury,andPra-
teek Saxena. 2019. Neuro-Symbolic Execution: Augmenting Symbolic Execution
withNeuralConstraints..In ProceedingsoftheSymposiumonNetworkandDis-
tributedSystemSecurity(NDSS ’19) .
[26]Youcheng Sun, Min Wu, Wenjie Ruan, Xiaowei Huang, Marta Kwiatkowska,
and Daniel Kroening. 2018. Concolic Testing for Deep Neural Networks. In
Proceedingsofthe33rdACM/IEEEInternationalConferenceonAutomatedSoftware
Engineering (ASE ’18) . 109ś119.
[27]David Trabish, Andrea Mattavelli, Noam Rinetzky, and Cristian Cadar. 2018.
Chopped Symbolic Execution. In Proceedings of the 40th International Conference
onSoftwareEngineering (ICSE’18) . 350ś360.
[28]Xinyu Wang, Jun Sun, Zhenbang Chen, Peixin Zhang, Jingyi Wang, and Yun Lin.
2018. Towards Optimal Concolic Testing. In Proceedings of the 40th International
Conference onSoftwareEngineering (ICSE’18) . 291ś302.
[29]E. Wong, L. Zhang, S. Wang, T. Liu, and L. Tan. 2015. DASE: Document-
Assisted Symbolic Execution for Improving Automated Software Testing. In
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering (ICSE
’15). 620ś631.
[30]Qiuping Yi, Zijiang Yang,ShengjianGuo,ChaoWang,JianLiu,and ChenZhao.
2015. PostconditionedSymbolicExecution.In 2015IEEE8thInternationalConfer-
ence onSoftwareTesting,Verification and Validation (ICST ’15) . 1ś10.
[31]Qiuping Yi, Zijiang Yang,ShengjianGuo,ChaoWang,JianLiu,and ChenZhao.
2018. Eliminating Path Redundancy via Postconditioned Symbolic Execution.
IEEE Transactions onSoftwareEngineering (2018), 25ś43.
158