Learning Patterns in Conﬁguration
Ranjita Bhagwan
Microsoft Research
bhagwan@microsoft.comSonu Mehta
Microsoft Research
someh@microsoft.comArjun Radhakrishna
Microsoft
arradha@microsoft.comSahil Garg
University of California, Berkeley
sahil_garg@berkeley.edu
Abstract —Large services depend on correct conﬁguration to
run efﬁciently and seamlessly. Checking such conﬁguration for
correctness is important because services use a large and contin-uously increasing number of conﬁguration ﬁles and parameters.Y et, very few such tools exist because the permissible values fora conﬁguration parameter are seldom speciﬁed or documented,existing at best as tribal knowledge among a few domain experts.
In this paper, we address the problem of conﬁguration pattern
mining: learning conﬁguration rules from examples. Using pro-gram synthesis and a novel string proﬁling algorithm, we showthat we can use ﬁle contents and histories of commits to learnpatterns in conﬁguration. We have built a tool called ConfMinerthat implements conﬁguration pattern mining and have evaluatedit on four large repositories containing conﬁguration for a large-scale enterprise service. Our evaluation shows that ConfMinerlearns a large variety of conﬁguration rules with high precisionand is very useful in ﬂagging anomalous conﬁguration.
I. I NTRODUCTION
Conﬁguration management is an integral part of the de-
velopment and deployment of large services. These services
depend heavily on correct conﬁguration to run uninterrupted,be ﬂexible to changing environments, and to scale seamlessly.This ubiquitous use of conﬁguration in services poses severaldaunting challenges, one of which is to ensure that everyconﬁguration parameter is set to a suitable value. To makematters worse, the amount of conﬁguration that a serviceneeds to manage grows signiﬁcantly with time as the servicescales out and as developers add new features and capabilities.For instance, the Microsoft 365 email service [1] more thandoubled the number of conﬁguration ﬁles in just a period ofsix months.
Unlike code, for which compilers and static analysis tools
catch several types of errors well before the developer commitstheir changes, very few tools exist to perform such checks onconﬁguration. This is because rules governing which conﬁg-uration values are appropriate for a particular conﬁgurationparameter are very speciﬁc to the scenario in which the serviceuses the conﬁguration value. For instance, a conﬁgurationvalue may capture a timeout for a particular microservicewhich the microservice expects to be a few minutes. If adeveloper were to set this timeout value to a few millisecondsby mistake, while she would be syntactically correct, themicroservice may fail because of a lower-bound check on thetimeout value. Worse, if no such check exists, the microservicewould start timing out much too soon in deployment andthereby cause service disruption. Such requirements of conﬁg-uration correctness are seldom documented. V ery often theyare subtle, very speciﬁc to the context in which they are used,and difﬁcult to catch through speciﬁcation and hard-codedrule-based checking. Consequently, misconﬁgurations in largeservices occur much too often, cause build and test failures,and sometimes signiﬁcant disruption and data breaches [2],[3], [4], [5]. For instance, in January 2020, Microsoft exposed250 million customer records inadvertently because a databasespeciﬁed personally identiﬁable information (e.g. email ad-dresses) in an anomalous format [6].
Towards addressing this, we observe a unique opportunity
driven by two recent trends. First, modern services maintainconﬁguration in ﬁles separate from code, such as yaml, json,or xml ﬁles. Engineers process conﬁguration changes similarto code changes: they commit changes to the conﬁgurationthrough a version control system. We can therefore treatconﬁguration-as-data by tapping into the version control sys-
tem. Through commit logs and ﬁle histories, we have access toa rich history of conﬁguration ﬁle snapshots and changes fromwhich we can learn patterns in conﬁguration values. Also,
since conﬁguration is gated by version control systems, wealso have the opportunity to automate conﬁguration checks atcommit time and catch errors early, well before deployment.
Second, the ﬁeld of program synthesis for data processing
has seen rapid progress in recent years. Tools like FlashFilllearn programs that capture patterns in values, structure andsequences and have been used successfully in various domainssuch as automated manipulation of tabular data [7], [8], [9],[10] and semi-structured data extraction [11], [12].
In this paper, we bring the ideas of program synthesis and
conﬁguration-as-data together to perform conﬁguration pattern
mining. We introduce a novel program synthesis-based stringproﬁling procedure to learn regular expression based rulesthat capture patterns in conﬁguration values. This procedure isbased on the techniques presented in [13], but is signiﬁcantlymore efﬁcient and robust to noise. For instance, given enoughexamples, the string proﬁling procedure can learn that atimeout value in a conﬁguration ﬁle is always speciﬁed as anumber followed by the character, “s" (the regular expression[0-9]+s ). If a developer erroneously speciﬁes a timeout
value of “1ms” or a timeout value of “10” without specifyingthe units, it will not match the learned regular expression andhence, we can ﬂag the mismatch as a potential conﬁgurationerror.
We use two types of input data (or examples) to the string
proﬁling algorithm to learn to different kinds of rules forconﬁguration values. First, we learn history-based rules by
using versions of the same conﬁguration value in previous
8172021 36th IEEE/ACM International Conference on Automated Software Engineering (ASE)
DOI 10.1109/ASE51524.2021.000772021 36th IEEE/ACM International Conference on Automated Software Engineering (ASE) | 978-1-6654-0337-5/21/$31.00 ©2021 IEEE | DOI: 10.1109/ASE51524.2021.9678525
978-1-6654-0337-5/21/$31.00  ©2021  IEEE
commits and detect patterns in them. Second, we learn ﬁle-
based rules that learn patterns in different conﬁguration values
within the same version of a single conﬁguration ﬁle. We ﬁnd
that both rule types capture various conﬁguration patterns andcan be used to ﬂag misconﬁgurations, auto-suggest correctvalues to developers as they edit conﬁguration, and therebycontribute to improving conﬁguration management.
We have built ConfMiner, a tool that uses the algorithms
described here to implement conﬁguration pattern mining. Wehave evaluated ConfMiner on 4 repositories that are usedto manage conﬁguration for a large-scale enterprise serviceused by hundreds of millions of users. Our results show thatConfMiner is very generally applicable and learns a widevariety of rules. We also observe that ConfMiner’s rulescatch many different kinds of misconﬁgurations very earlyin the development process. This can help accelerate thedevelopment cycle of services and improve service reliability.
This paper makes the following novel contributions.
•We introduce the problem of conﬁguration pattern min-ing, i.e. learning patterns in conﬁguration from examplesthat we get from ﬁle version histories and commit histo-ries. To the best of our knowledge, this is the ﬁrst workto target the problem of pattern mining in conﬁgurationvalues. We describe two types of conﬁguration patternmining: value-based and structure-based.
•We develop a novel program synthesis string proﬁlingprocedure and apply this to conﬁguration-as-data to learnvalue-based rules pertaining to conﬁguration. This pro-cedure outperforms the state-of-the-art string proﬁlingalgorithm by 3.3X and is of independent interest outsidethe context of conﬁguration mining.
•We have built a tool called ConfMiner that implementsour algorithms for value-based rule mining. We haveevaluated it on 4 conﬁguration repositories for a large,popular service. Our results show that ConfMiner is in-deed effective in learning patterns in conﬁguration values.
The rest of the paper is organized as follows. Section II
describes the problem of conﬁguration pattern mining, outlinesour solution, and describes the scope of this work. Section IIIdescribes the algorithm and system components in detail.Section IV presents our evaluation of ConfMiner and the stringmatching algorithm. Section V discusses threats to validity.Section VI discusses related literature and section VII providesa conclusion to the paper.
II. P
ROBLEM DEFINITION AND SCOPE
In this section, we ﬁrst explain conﬁguration pattern mining
with an example conﬁguration ﬁle. Next, we provide anoverview of how ConfMiner performs conﬁguration patternmining and how applications can use these patterns. Finally,we discuss the scope of our work and bring out its inherentlimitations.
A. Conﬁguration Pattern Mining
Any conﬁguration ﬁle, be it in json, xml, yaml, or any other
format, can be expressed as a hierarchical tree structure. Each# Google App Engine application config file
application: foobar
version: 5runtime: php55
# Manifest files- url: /(.+ ˙(appcache))
static_files: \1
upload: static/(.+ ˙(appcache))
mime_type: text/cache-manifest
expiration: "0s"
- url: /(.+ ˙webapp)
static_files: \1
upload: (.+ ˙webapp)
mime_type: app/x-web-app-manifest+json
expiration: "5s"
# CSS, Javascript, text etc
- url: /(.+ ˙(css|js|xml|txt))
static_files: \1
upload: (.+ ˙(css|js|xml|txt))
expiration: "5m"
# HTML pages
- url: /(.+ ˙html)
static_files: \1
upload: (.+ ˙html)
expiration: "10m"
Fig. 1. Example conﬁguration ﬁle derived from Google AppEngine’s
app.yaml. We have modiﬁed this ﬁle for illustrative purposes here.
node is a key-value pair, where the key is the conﬁguration
parameter and the value is the value that the conﬁgurationparameter is set to. An edge connecting two nodes capturesa parent-child relationship between conﬁguration parameters.Given this, we envision two types of conﬁguration patternmining: value-based and structure-based.
1) V alue-based pattern mining: This is the process of
learning patterns in the values of a conﬁguration parameter.We use regular expression learning to do this. Consider theapp.yaml conﬁguration ﬁle in Figure 1 which is an abridged
example of a Google App Engine’s conﬁguration ﬁle [14]. Theparameterexpiration (highlighted in blue) is set four times
in different sections of the ﬁle, and all values follow a certainpattern: a number followed by an ‘s’ (for seconds) or an ‘m’(for minutes). From this, we can learn patterns for the value(or a rule) as the regular expression [0-9]+[s|m]. Since we
learn this rule based on the contents of the ﬁle alone, we callthis a ﬁle-based rule. Now, notice the parameter version
(highlighted in red) on the second line. This is speciﬁed onlyonce in the ﬁle as 5, but say previous values of this parameter
in earlier revisions of this ﬁle were 1,2,3and4. From
all these values, we can learn that this parameter is alwaysa number, i.e., follows the pattern speciﬁed as the regular
818Fig. 2. ConfMiner System Overview
expression [0-9]+. We can learn this rule from a history
of commits to this speciﬁc conﬁguration parameter. We call
this a history-based rule.
Notice that the generalizations these rules provide are not
unique. For any given set of values, we can learn multiple regu-lar expressions. For instance, in the case of the expiration
parameter, we could learn a more general expression such as[0-9]+[a-z] which allows any letter, not just ‘s’ or ‘m’.
Or, we could learn a more speciﬁc regular expression e.g.[0|5|10][s|m] which only allows numbers 0,5and10.
2) Structure-based pattern mining: Conﬁguration ﬁles have
a rigid structure. In Figure 1, the ﬁle speciﬁes a list of four ele-ments: each element is preﬁxed by a ‘-’. Each list element hasa speciﬁed set of conﬁguration parameters. All four elementshave parameter expiration speciﬁed (in blue), whereas,
only two out of four elements have mime_type speciﬁed
(in green). From this, we could infer that expiration is
potentially a mandatory parameter whereas mime_type is
not. Another form of structural pattern is that there maybe an implicit ordering requirement of parameters. Certainparameters may have to be speciﬁed before others. A commonexample of this is ﬁrewall conﬁgurations, where the conﬁguredrules are applied in order to determine whether a networkconnection should be allowed or denied. Each rule overridesall other rules that appear after it in the ordering. Thispaper concentrates on value-based mining; we leave work onstructure-based pattern mining algorithms to future work.
B. System Overview
ConfMiner’s goal is to learn patterns for conﬁguration that
can be used by several applications to detect misconﬁguration,
suggest changes, and build automatic checkers. Figure 2 showsan overview of the system. First, ConfMiner runs a stringproﬁling procedure on a speciﬁc ﬁle’s contents to generateﬁle-based rules for every conﬁguration parameter that has alarge enough number of example values in the ﬁle (e.g., theparameterexpiration in Figure 2). Next, ConfMiner uses
commit histories for a speciﬁc conﬁguration parameter to learnhistory-based rules in a similar manner. All learned rules,their conﬁdence and support are stored in a database. Therule-learning algorithm runs periodically on ﬁle contents andhistories, e.g. in our deployment, ConfMiner learns rules oncea day.
Applications query ConfMiner through a simple interface
which, given a parameter and its value, return all rulesthat match that value. Applications can use this interface inmultiple ways. For instance, an auto-checker can, at reviewtime, post an automated comment if a commit to a particularparameter does not match any of the learned rules. Thisapplication is similar to commit recommendation systems suchas Rex [15]. An IDE can also use the rules to suggest changesto conﬁguration parameters as the developer starts to typein the change. This scenario is similar to IntelliSense inVisualStudio [16] or Content Assist in Eclipse [17]. A thirdapplication is to enable building automated conﬁguration com-pilers and veriﬁers which can run along with code compilers,perhaps as plugins, to generate warnings, etc. This usage issimilar to StyleCop [18].
C. Scope
We believe that conﬁguration pattern mining is a vast subject
which needs to be tackled through a signiﬁcant body of re-
search. In this paper, we concentrate primarily on value-basedpatterns that can be captured through regular expressions. Hereis what the paper does not try to achieve.
1) Building a sound, complete system: ConfMiner is funda-
mentally a best-effort system, and since it learns from examplevalues, it cannot be sound or complete. Notice that given a setof sample conﬁguration values, our example in Section II-Ashowed that ConfMiner could learn more than one validregular expression given a set of inputs. An application’sefﬁcacy will vary depending upon how speciﬁc the rules are.The more speciﬁc the rules, the more strict an applicationwill be in enforcing them. The more generic the rules are,the application will be less strict but may miss out on validmisconﬁgurations. To handle both scenarios, ConfMiner learnsa combination of rules that are very speciﬁc as well as verygeneric. An application can then use thresholds to select morespeciﬁc or more general rules depending on its tolerance tofalse-positives and false-negatives.
2) Going beyond regular expressions: Our work concen-
trates only on patterns that can be captured via regular ex-pressions. Not all value-based rules can be captured by regularexpressions. For instance, values of different parameters couldbe correlated. Or, in certain cases, the value of a particularparameter may change in a very speciﬁc way over time, suchas theversion parameter in Section II-A goes up by 1 every
time it is changed. We leave learning such rules and propertiesto future work.
3) Designing applications: Our goal is to design a pattern
mining algorithm for conﬁguration. While we do not concen-trate on building applications that can use the mined conﬁg-uration patterns, we have emulated an automated commentgenerator which, in real time, ﬂags many misconﬁgurationsmade by developers in deployment. Section IV describes ourevaluation and application emulation in detail.
III. C
ONF MINER DESIGN
In this section, we ﬁrst give some background on string
proﬁling and describe our program synthesis-based string pro-ﬁling algorithm, while providing details on the conﬁguration-speciﬁc parameters. Next, we describe the data generationengine and the rule-learning engine. Finally, we describe the
819Fig. 3. ConfMiner components.
API that ConfMiner exposes and that applications can use.
Figure 3 gives a summary of ConfMiner’s components andhow they interact.
A. Proﬁling Conﬁguration V alues
The string proﬁling problem. First, we address the problem
of characterizing all the values a conﬁguration parameter may
take. Given the known set Sof string values for a conﬁguration
parameter, the string proﬁling problem is to produce a set of
disjoint regular expressions Rand a set of outliers O⊆S
such that ∀s∈S.s∈O∨∃ r∈R.s∈r. That is, every string
inSeither matches one of the regular expressions in Ror
is in the outlier set O. Here, we use the notation s∈rto
represent that a string smatches a regular expression r.
The set of regular expressions Ris called the proﬁle ofS.
Intuitively, each r∈R deﬁnes a cluster of similar strings in
S. We will use the set of known values (obtained from the ﬁle
or the history) for a conﬁguration parameter to learn a proﬁleRand use the proﬁle as a speciﬁcation for any new values of
the conﬁguration parameter.
The correctness requirements of the string proﬁling problem
are rather easy to satisfy. The proﬁle consisting the singleregular expression .*, i.e., the expression to match all strings,
is always a valid solution. To obtain useful solutions, weneed to deﬁne an optimality criterion. We do not providethe full formal details of the criterion, but instead refer thereader to [13]. Intuitively, an optimal proﬁle is one such that/summationtext
r∈RScore( r)is minimized, where Score is a custom deﬁned
ranking function. We use the same Score function used in
[13]—in particular, in the Microsoft PROgram Synthesis using
Examples SDK (PROSE) [19] implementation of [13]. Thisversion of Score was tuned using over 100 sample datasets
across different application and data domains beyond justconﬁguration mining.
The function Score assigns scores based on two factors:
speciﬁcity and simplicity. For example, regular expressions
that use the character class ., i.e., class that matches all
characters are given a high score, while expressions usinglong constant strings are given a lower score. On the otherhand, the expression [0-9]*would be given a lower score
than[0-9]{2,9} despite the latter being more speciﬁc: the
former is a simpler pattern.
We extend the optimality criterion to a noisy setting as:/summationtext
r∈RScore( r)+o·|O| where o∈R+is the outlier penalty.
We explicitly penalize outliers using the parameter o, and
tuning the value of ocontrols the balance between patterns
and outliers.Example III.1. Consider the following set of values that the
ResourcePath conﬁguration parameter takes: S={
"resource/2020-08-26/first.xml","resource/2001-11-05/second.xml",...,"resource/1992-03-15/third.xml","deployed/main.xml","deployed/secondary.xml",...,"deployed/tertiary.xml","test_resource.xml"/bracerightbig
Here, the values fall into the following categories: (a) V alues
that match the regular expression r
1=resource/
[0-9]{4}-[0-9]{2}-[0-9]{2}/[a-zA-Z] *[.]xml,
(b) V alues that match the regular expression r2=
deployed/[a-zA-Z] *[.]xml, and (c) the outlier
value test_resource.xml.
Ideally, a string proﬁling procedure will characterize the
patterns in Swith the regular expressions R={r1,r2}and
the outlier set O={text_resource. xml}. This proﬁle would
signify that any new values for the conﬁguration parametershould match either r
1orr2.
The Stochastic String Proﬁling Procedure. Algorithm 1 de-
scribes a stochastic algorithm for the string proﬁling problem.At its core, the algorithm uses the LearnRegex to learn a
single regular expression rfrom a set of generators G⊆S .
In general, the set Gis small, between 2−5strings. We
use the LearnRegex procedure from [13], as implemented
in the Microsoft PROgram Synthesis using Examples SDK(PROSE) [19].
Algorithm 1 Stochastic string proﬁling algorithm
Require: Set of strings S
Require: Ranking score Score
Ensure: Regular expressions Rand outliers O⊆S
1:Clusters←∅
2:while *do
3: G←Sample(S,Clusters )
4: r←LearnRegex( G)
5: Clusters←Clusters∪{G/mapsto→r}
6:end while
7:R← ApproxExactSetCover (Clusters ,Score)
8:O←{ s∈S|∀ r∈R.s/negationslash∈r}
9:return (R,O)
Given a set of strings S, Algorithm 1 maintains a dictionary
Clusters that maps subsets GofSto the regular expression
r=LearnRegex( G). Each item G/mapsto→rinClusters is a
potential cluster in the learned proﬁle, representing the strings{s∈S|s ∈r}. The procedure proceeds through the
following stages:
Generate. We repeatedly sample small subsets (size 2−4)o f
Sand learn a regular expression using the LearnRegex proce-
dure. During sampling, we do not construct Gby uniformly
820sampling from S. Instead, we obtain Gas follows: we start
with an empty Gand extend Gwith one of the following
randomly chosen options 2–4 times:
•String s∈S that does not belong to any cluster,
•All the generators Gof a cluster G/mapsto→rinClusters , and
•String swhere s∈S does belong to a cluster in Clusters ,
This biased sampling attempts to achieve one of the following:
(a) construct a new cluster out of the strings that do not belongto any cluster, (b) merge or extend existing clusters to forma larger one, and (c) construct new clusters independently ofexisting ones. Ideally, the sample-and-learn loop is run untilall patterns in the desired proﬁle are added to the Clusters
collection. However, we do not know the desired patterns: inpractice, we sample until no new regular expressions have beenadded to Clusters for10iterations.
Select. Now, given a set of candidate clusters and the Score
function, we use an approximation algorithm for the minimalexact set cover to pick a near optimal subset of clusters. Givena set Xand a set of its subsets Y={X
1,...,X n}with a
cost function mapping Xito reals, the exact set cover problem
asks to choose a subset of Y/prime⊆Ysuch that each Xi,Xj∈Y/prime
are disjoint and/uniontext
Xi∈Y/primeXi=X. Of all such possible Y/primewe
prefer the one with the minimal total cost. In our setting, (a) X
is the set of all strings S, (b) Ycontains the set of candidate
clusters Clusters , and (c) the cost function is Score.
The approximation algorithm follows standard greedy set
cover algorithms: it maintains a partial solution, in this case,a set of regular expressions {r
1,r2,...,r k}. In each iteration,
the cluster G/mapsto→rwhich maximizes |{s∈S|s∈r∧∀i.s/negationslash∈
ri}|/Score( r)is added to the partial solution, and all clusters
which intersect with riare discarded from Clusters .
However, we do not proceed until all strings in Sare
covered in the solution. Instead, we stop adding to the partialsolution when the value |{s∈S| s∈r∧∀i.s/negationslash∈r
i}|/Score( r)
drops below the outlier penalty o−1for all rinClusters . The
strings in Sthat are not matched by any riin the solution are
deemed outliers O. Our implementation additionally returns
the conﬁdence and support for each pattern, which are deﬁnedas the fraction and the number of strings, respectively, that arematched by the corresponding regular expression.
It should be noted here that the fact that the procedure
ignores a small fraction of outliers is particularly of importanceto us, since this ensures that any rare examples of thatconﬁguration parameter which might be existing or historicalmisconﬁgurations are not matched in the learnt proﬁle.
Example III.2. Consider the set of strings Sfrom
Example III.1. In the ﬁrst phase of the algorithm, we
sample subsets of Sand learn regular expressions from these
samples. Here, we have 4separate cases:
(a) The sample only contains strings of form"resource/{date}/{file_name}.xml In this
case, LearnRegex returns r
1.
(b) The sample only contains strings of formdeployed/{file_name}.xml. In this case, LearnRegex
returns r
2.(c) Sample contains strings of both forms. Here, r3=
[a-zA-Z/]+/[a-zA-Z] *[.]xml is returned.
(d) The sample contains the outlier stringtest\_resource.xml. Here, r
4 =
[a-zA-Z/_] *[.]xml is returned.
By design, the ranking score Score function produces scores
with Score( r1),Score( r2)<Score( r3)<Score( r4),b yt h e
principle of speciﬁcity.
During the second phase, selection, the clusters are cho-
sen using the greed heuristic, with r1and r2picked in
sequence. For the rest of the clusters, the normalized scoreis less than the outlier penalty o
−1. Hence, the string
test_resource.xml is deemed an outlier . Note that the
selection of regular expressions depends heavily on the Score
function. With different Score functions, there are cases where
the preferred cluster may be r3orr4.
Comparison to [13] We do not go into a full comparison
of Algorithm 1 with [13] as it is not related to the mainthesis of this paper. Summarizing the performance aspectof the comparison, Algorithm 1 is 3.3X faster than [13]
on the set of benchmarks from [13] (see Figure 4). Thisperformance improvement can be attributed to avoiding theexpensive agglomerative hierarchical clustering (AHC) basedapproximation, which is O(n
2)in the number of input strings.
The time taken by Algorithm 1 is dominated by the sample
Fig. 4. Performance comparison of Algorithm 1 vs. FlashProﬁle
phase. The number of sampling iterations to produce goodproﬁles depends on the number of patterns in the desiredproﬁle as opposed to the number of input strings. As thenumber of patterns in the proﬁle is signiﬁcantly smaller thanthe number of input strings, Algorithm 1 is often more efﬁcientthan AHC.
Further, Algorithm 1 often produces proﬁles of a higher
quality than [13]. In the string proﬁling setting, AHC issensitive to incorrect decisions in a manner that is not possiblefor numerical data. AHC proceeds by initially considering eachstring to be a cluster by itself, and then iteratively mergingthe two clusters that are the closest to each other. Here, thenotion of distance is given by the Score value of the regular
expression generated by the strings in the cluster. One incorrectmerging decision, often due to outliers or similar strings ofdifferent patterns, has a cascading effect and may produceundesirable proﬁles.
Example III.3. Consider a set of strings representing dates
821of the forms 14 January 2020 and23-Feb-2020.
Suppose the set contains the strings 03-May-1992 and
03 May 1992. It is likely that the ﬁrst merge performed
by AHC will group these strings together to obtain theregular expression 03[ -]May[ -]1992. This is because
the resulting regular expression is very speciﬁc and will havea low Score value.
After this point, all subsequent clusters will either
include both these strings, or exclude both of them.Hence, we can never obtain the desired regularexpressions [0-9]{2}-[A-Z][a-z]{2}-[0-9]{4}
and [0-9]{2} [A-Z][a-z] *[0-9]{4}. Instead,
AHC returns a proﬁle that has the single regular expression[0-9]{2}[ -][A-Z][a-z] *[ -][0-9]{4} which
mixes the two formats. Similarly, in the presence of outliers,one incorrect decision merging an outlier with a non-outliercluster will cause signiﬁcant degradation of the ﬁnal results.Algorithm 1 avoids these issues—even if the sampling groupsstrings incorrectly, the clusters that arise from these groupswill be safely ignored during the selection phase.
B. Token DSL for Conﬁguration Mining.
We use the PROSE SDK implementation of the LearnRegex
procedure. However, this procedure had to be customized
to the context of conﬁguration mining. During the learningprocess, LearnRegex constructs regular expressions using a
domain speciﬁc language DSL of generic tokens such as[0-9] for digits and [A-Z] for uppercase letters. ConfMiner
has had to modify the DSL to make it more conﬁguration-speciﬁc. Consider the following example scenarios.
We found that conﬁgurations often capture names,
such as ﬁle names, test names and ﬁrewall rule names.Names can have an arbitrary number of camel-casedterms. For instance, a parameter testname captures
the names of tests to run on a particular code-base. SayConfMiner’s data collection found three unique values for it:("testComponent","testData","testSystem").Given the generic DSL, ConfMiner would learn the regularexpression test[A-Z][a-z]+ which allows only one
camel-cased term to follow test. Hence if an application
queries ConfMiner with the value "testAppData" for this
parameter, the regular expression will not match it since ithas two camel-cased terms following test. To accommodate
this scenario, the conﬁguration-speciﬁc DSL ignores the token[A-Z] and hence ConfMiner learns the regular expression
test[A-Za-z]+ which is much more general and allows
an arbitrary number of camel-cased terms in the name.
The conﬁguration-speciﬁc DSL also includes new tokens.
For instance, we found that values often capture lists ofarbitrary sizes, where a delimiter such as a comma separatesthe list elements. To allow for lists of arbitrary size, theconﬁguration-speciﬁc DSL includes tokens such as (\w+,)+
which covers comma-separated lists or arbitrary size. If theDSL does not include such list-speciﬁc patterns, ConfMinerlearns regular expressions only towards a speciﬁc number ofelements in the list which again causes incorrect matchingbehavior in a large number of cases.
C. Data Generation Engine
We now describe the data generation process for ﬁle-based
and history-based rules.
1) File-based: Data generation to learn ﬁle-based rules
is triggered every time a conﬁguration ﬁle is changed.
ConfMiner’s data generation engine parses the conﬁgura-tion ﬁle using format-speciﬁc parsers. Currently, our im-plementation supports 11 different ﬁle types including xml,json, yaml and ini. Each parser gives us a tree objectthat captures all the conﬁguration in a structured for-mat. From this, ConfMiner extracts tuples of the form(file_name, param_name, list_of_values).Aparameter name, such as expiration in Figure 1, can exist
under different parent conﬁgurations. We made the choiceto ignore the ancestry of each parameter and, as long astheir names are identical, ConfMiner combines all values ofparameters within the same tuple and into one list of val-ues. Hence, for the mentioned example, ConfMiner generatesthe tuple(app.yaml, expiration, ["0s", "5s",
"5m", "10m"]). This increases the number of values foreach parameter and more data allows for rules with higherconﬁdence and support from the string proﬁling algorithm.
2) History-based: Data generation for history-based
rules is triggered every time a commit changes an existingparameter in a conﬁguration ﬁle. For every ﬁle commit,ConfMiner runs a differential analysis on the ﬁle to detectwhich particular conﬁguration parameter has changed. Atextual difference (which version control systems readilyprovide) does not sufﬁce because it is possible that the valueof a parameter spans multiple lines, and if only some ofthose lines are changed, one cannot tell what the changedparameter is. ConfMiner therefore performs the differenceat a syntactic level. To do this, it uses conﬁguration parsersto learn the tree object for the old version of the ﬁle, doesthe same for the new version of the ﬁle, and compares themusing heuristic approaches. From these comparisons, it ﬁndsa) the changed conﬁguration parameter and b) the new valueit is set to. From this, ConfMiner creates tuples of the formfile_name, param_name, list_of_values).
In addition, ConfMiner also generates new tuples that
combine data across ﬁles if the conﬁguration parameter
name (param_name) is the same. This aggregated data isparticularly useful towards learning rules that govern genericdatatypes such as IP addresses, and DLL version numberswhich could have the same format across different ﬁles.Section IV -C shows several examples of ﬁle-based rules andhistory-based rules that ConfMiner learned in deployment.This includes examples of generic patterns that exist acrossﬁles as well.
Depending on ﬁle format, conﬁgurations could have slightly
varying structure. For instance, in the xml format, a conﬁg-uration parameter, apart from having a value, could have at-
tributes which themselves have set values. ConfMiner accom-
822modates all these speciﬁc details for different formats. More
details on the implementation are provided in Section IV -A.
D. Rule-Learning Engine
Once ConfMiner generates data for ﬁle-based and history-
based rules separately, it uses the string proﬁling algorithm
which returns a list of regular expressions with the conﬁdenceand support for each. While rules with higher conﬁdence andsupport are indicative of “well-behaved" values of a conﬁg-uration parameter, rules with very low conﬁdence or supportmay be equally important and useful. At ﬁrst glance, this ap-pears counter-intuitive. But several conﬁguration parameters,in reality, have very varied patterns like the url parameter
in Figure 1. Learning one regular expression that capturesall these parameters would make the regular expression toogeneric, albeit with high conﬁdence and support. We havefound that it is indeed better to learn a small number ofregular expressions that capture the whole set of values, witheach regular expression having relatively low conﬁdence andsupport. We use the Score parameter in the string proﬁling
algorithm to strike this balance and set a very low threshold onthe conﬁdence and support of the rules learned. The conﬁdencethreshold is set to 0.03, i.e., if a cluster contains 3% of
known ﬁle-based or history-based values for a conﬁgurationparameter, the corresponding rule is considered valid. For ruleswith conﬁdence lower than this threshold, the correspondingvalues are considered accidental or outliers, i.e., even if anew conﬁguration value matches this rule, it is consideredpotentially incorrect and ﬂagged.
Currently, ConfMiner’s rule-learning engine is triggered
once a day. Using all commits within that day, it learns history-based rules using a commit history of 6 months. It learns ﬁle-based rules for any conﬁguration ﬁle that has been changedon that day. Finally, ConfMiner stores all learned rules in acentral database, indexed by ﬁle name and/or parameter name.
E. ConfMiner API
The above sections have described the process by which
ConfMiner learns regular expressions given input examples.
We now describe the API using which applications can useConfMiner. The primary call that ConfMiner supports isFindMatches(file_name, param_name, value).The call returns a list of all ﬁle-based and history-based rulesthat the value matches along with the conﬁdence and supportfor each rule. The application can then further ﬁlter the set ofrules based on its own conﬁdence and support requirements.If no matching rule is found, the function returns a null value.ConfMiner also returns matches using generic history-basedrules, which hold across different ﬁle names. Again, theapplication can decide to keep these rules or eliminate them.
IV . E
V ALUA TION
We ﬁrst describe details of the ConfMiner implementation
and describe our deployment of ConfMiner on 4 repositories ofMicrosoft 365, a large-scale widely used enterprise service. Wethen evaluate the string proﬁling algorithm using real data fromthese repositories. We emulate an automated misconﬁgurationchecker and show that ConfMiner rules ﬂag several miscon-ﬁgurations as and when developers commit them. Finally, weexamine 64 real-world conﬁguration issues as reported in theCtest dataset [20] to determine how often conﬁguration patternmining can help ﬁnd real misconﬁgurations.
A. Implementation
ConfMiner is implemented using 4760 lines of C# code and
works with Git [21]. The data generation engine interfaces
with both Github [22] and Azure DevOps [23]. It supportsa total of 11 ﬁle types that typically store conﬁguration,including xml, json, yaml, csproj, conﬁg and ini. For eachformat, the data generation engine implements parsers whichﬁrst translate the ﬁle contents into the xml format. ConfMinerthen inputs the xml for the old version and the new version to adifference module which implements the differential syntactic
analysis required to learn history-based rules. The differencemodule is built using the XmlDiffAndPatch [24] library.
The ConfMiner API is implemented using approximately 1500lines of C# code. In our Azure DevOps implementation, weuse service hooks [25] to capture commits to conﬁgurationﬁles as and when they happen.
B. Deployment
We have deployed ConfMiner on four repositories belong-
ing to a large continuously deployed service within our enter-
prise, as shown in Table I. R1 contains both code and conﬁgu-ration of core features in the service. R2 contains informationregarding physical conﬁguration, such as datacenter-speciﬁcconﬁguration and network-speciﬁc conﬁguration. R3 containscode and conﬁguration related to the DevOps environment i.e.build, test and deployment pipeline for the service. Finally, R4contains code and conﬁguration for all applications built ontop of the core features that the service provides.
Most conﬁguration for these repositories sits in xml, json
and ini ﬁles. The “conﬁg changes" column tells us the numberof conﬁguration parameters that have been changed in thelast six months. This is as high as 23,521 in the case of R4.The rule-learning engine uses these changes to learn history-based rules. Increasing history beyond six months makes thecomputation of rules slow and also biases the rules towardsolder data that may not be relevant. The “ﬁle changes" columngives the number of conﬁguration ﬁles changed in the last sixmonths. This is much lower than the conﬁguration changesbecause every ﬁle captures multiple conﬁguration parameters.Finally, the “history-based rules" column and the “ﬁle-basedrules" column show the number of rules ConfMiner learns inthese two categories using six months of data.
C. Example Rules
Table II gives some example rules that ConfMiner has
learned in deployment. Both ﬁle-based and history-based
techniques learned similar rules. As can be seen the supporti.e. the number of input examples that match each learnt rulecan vary widely, sometimes reaching a few thousand. Also,
823T ABLE I
DET AILS OF REPOSITORIES THA T CONF MINER IS DEPLOYED ON .F ILE CHANGES AND CONFIGURA TION FILE COMMITS ARE FOR THE LAST SIX MONTHS .
Repository Conﬁg ﬁle counts Conﬁg changes History-based rules File changes File-based rules
json xml ini
R1 4635 55815 5199 21627 13358 976 36089
R2 1905 3104 114 18461 14313 634 17261
R3 598 10150 121 2130 8173 337 5816
R4 6769 2008 1967 23521 13113 661 15443
T ABLE II
EXAMPLE RULES LEARNED .
No. File type Conﬁg name Rule Examples Support
1 xml ServerName SRDC-NLB-0[0-9]A“SRDC-NLB-01A"34“SRDC-NLB-03A"
2 conﬁg Version [0-9]+\.[0-9]+\.[0-9]+\.[0-9]+“19.12.12.151005"466“3.7.25810.101"
3 xml Runtime 00:00:00\.[0-9]7“00:00:00.2178760"7219“00:00:00.3280206"
4 csproj Include tbuild\_DB[0-9a-fA-F]+\.xml“tbuild_DB3.xml"17“tbuild_DB9f.xml"
5 xml Duration 00:00:[0-9]{2}"00:00:24"1 0
“00:00:01"
6- TimeServers 10\.22\.5\.134,10\.89\.0\.13 - 1043
7 json Normal 0x[0-9a-fA-F]{6}“0x212838"33“0x7C68B2"
8 json TestProfiles [A-Z][a-z]+,Search,[A-Z][a-z]+“Suggestions,Search,Teams"45“Recommendations,Search,Suggestions"
9 ini OrderBy UserRelation/[a-zA-Z]+[\s]desc“UserRelation/TrendingWeight desc"14“UserRelation/LastAccessTime desc"
10 ini ItemName AutoSuggest\.[0-9a-zA-Z]+“AutoSuggest.L0RankerControlCsgIndex"108“AutoSuggest.L1v3RankerCsgIndex"
ConfMiner learns many rules across all major conﬁguration
ﬁle types such as xml,json andcsproj. Rules learned
many different kinds of patterns. Row 1 shows how ConfMinerlearns formats in machine names. Row 2 demonstrates thatversion numbers in a particular conﬁguration ﬁle consist offour numbers separated by a “.”. Rows 3 and 5 capture twodifferent time formats. Row 4 shows an example pattern inincluded ﬁle-names in a project ﬁle. Row 6 is an example of anaggregated history-based rule across many ﬁles. It learns twovery speciﬁc IP addresses. Row 7 infers a 7-digit hexadecimalpattern. Row 8 learns a list of strings with a speciﬁc pattern,i.e. the word “Search" is always second in the list. Finally Row9 and row 10 show miscellaneous examples of conﬁgurationvalue patterns that specify an ordering relationship and anauto-suggest algorithm.
As these examples show, patterns of very different types
exist across various conﬁguration values that have a wide arrayof semantics. Using a generic program synthesis frameworkenables ConfMiner to be relevant in a large number of scenar-ios which are very different from each other. Moreover, thisvaried set of rules shows that specifying such rules manuallyis a formidable task which cannot be achieved at scale withoutdevelopers making a huge investment in time and effort.
D. Precision
We perform an online evaluation of ConfMiner on these
four repositories. Notice that since ConfMiner is actually
deployed on these repositories, evaluation is more realisticthan a standard train-test split based evaluation. When adeveloper completes a pull-request that changes a conﬁgu-ration ﬁle, ConfMiner ﬁrst determines which conﬁgurationparameter is changed, and what value it is changed to. To dothis, ConfMiner uses the same difference algorithm as usedto generate the data to learn history-based rules. Then, foreach changed parameter, we call the FindMatches function
provided by the ConfMiner API. ConfMiner checks if thechange matches any of the rules learned in the previous day(ConfMiner’s rule-learning engine runs once a day.). If it does,the change is labeled a true-positive (TP), i.e. ConfMiner
has indeed learned a rule that the new value matches. If nomatch is found, the change is labeled a false-positive (FP),
i.e. ConfMiner was not able to learn a rule that matchesthe new value. The precision of ConfMiner is calculated
as TP/(TP+FP). Note that we assume that if a developer iscompleting a pull-request with a changed conﬁguration ﬁle, itmust be correct. It is of course possible that the value is wrong,and the developer corrects it later. We make the reasonableassumption that such situations arise only rarely.
Table III shows the total precision, precision due to ﬁle-
based rules, and precision due to history-based rules for allconﬁguration changes made from 18th of July 2020 to date.Total precision lies between 0.7 and 0.85. In general, history-based learning shows overall better precision than ﬁle-basedlearning, with the value being as high as 0.92. The slightlylower precision of ﬁle-based rules is because a lot of theserules learn patterns in ﬁle paths and ﬁle names. Thus whendevelopers introduce a new ﬁle path, or change the format
824T ABLE III
OVERALL PRECISION ,FILE -BASED PRECISION AND HISTORY -BASED PRECISION THA T CONF MINER ACHIEVES .
Repository Total File-based History-based
TP FP Precision TP FP Precision TP FP Precision
R1 279 105 0.73 148 82 0.64 129 21 0.86
R2 372 157 0.70 257 140 0.65 112 17 0.87
R3 60 20 0.75 36 15 0.71 24 5 0.83
R4 639 111 0.85 227 73 0.76 408 37 0.92
Fig. 5. Example manual comment that can be replaced by ConfMiner. All
de-anonymizing information is elided.
of a ﬁle name slightly, this is recorded as a false-positive.
Sometimes, these new ﬁle paths or name format changes arerequired and therefore our rules are indeed wrong. However,very often we found that our rules, though marked as false-positives, were indeed valid because they required the formatof the ﬁle path or name to be fully consistent with previouslyseen examples, and the developer had committed a path orname in a slightly different format. We explain one suchexample in detail in Section IV -E3. Such rules, though they donot ﬂag misconﬁgurations, do point to style defects addressingwhich can improve hygiene and readability of conﬁguration.
E. Application Emulation
Section IV -D gives us conﬁdence in the inherent ability of
ConfMiner to learn rules with high precision. Now, we ask
how useful these rules are in ﬂagging misconﬁguration. Forthis, we have built a misconﬁguration detection applicationon top of ConfMiner which tracks every commit to a conﬁg-uration ﬁle. If the commit does not match any rules, a ﬂagis raised that this is a potential misconﬁguration. The ﬂag issilent, i.e. the developer is not informed of the ﬂag. Hence, wecall this an emulation of the application. If, before completing
the pull-request, the developer changes the conﬁguration valueagain so that it now matches a learned rule, it indicates thatthe previous value was indeed incorrect and the ﬂag that ourapplication raised was valid. We now present a few examplemisconﬁgurations that our emulation ﬂagged. We have usedredacted screenshots to keep them anonymous.
1) Path-based misconﬁguration: We notice that conﬁgu-
ration values that hold ﬁle names and ﬁle paths are oftenmisconﬁgured with relative paths rather than absolute paths.ConfMiner has been very effective at ﬂagging this. Figure 5shows an example pull-request that ConfMiner ﬂagged. A
Fig. 6. ConfMiner ﬂagged this since the timestamp did not match the rule.
reviewer manually calls out this error for correction throughthe comment, thereby conﬁrming that the ﬂag was indeedvalid. Given that ConfMiner ﬂagged this error by the devel-oper, we believe that the comments on such errors, which arecurrently entirely manual, can be automated. Moreover, sinceConfMiner takes at most a few seconds to ﬂag such errors,i.e. the time it takes to call the ConfMiner API, it can ﬂagsuch errors much faster than the manual review process.
2) Numerical misconﬁgurations: In Figure 6, we
see that in line 943, the developer changed a valueforBootstrapTimeStamp from1.00:00:00 to
24.00:00:00. However, in multiple other sections of theﬁle, for instance in line 918, the same conﬁguration has beenset to2.00:00:00. A ﬁle-based rule therefore learned that
this conﬁguration should be set such that the ﬁrst number thatappears is a single digit, and not two digits, as in 24. Noticethat the reviewer also made this comment, albeit with muchmore semantics, saying that the developer has set the numberof days as 24, and not the number of hours. This exampleshows that ConfMiner, through its simple regular expresslearning, can sometimes ﬂag subtle misconﬁgurations evenwithout understanding the semantics. The challenge though,if we are to automate comments based on this, is to makeautomated comments capture semantics like the reviewer hasin this example. We leave this to future work.
Fig. 7. ConfMiner ﬂagged this change because of an anomalous ﬁle name.
3) Style recommendations: ConfMiner ﬂags several style-
related recommendations as well. Figure 7 shows an example
825where ConfMiner learns a speciﬁc ﬁle format: a string-based
name, followed by a hyphen and a date. The shown commit online 92 uses an underscore instead of the hyphen. ConfMinerﬂagged this commit, but unlike previous examples, since thisis merely an issue of style, no reviewer speciﬁcally calledthis out. In fact, past interviews we have conducted haveshown that reviewers hesitate from calling out such nits tomaintain professional courtesy. We believe that automatingsuch ﬂagging of format issues can greatly improve readabilityand hygiene for conﬁguration, while directing potential irefrom developers at a bot rather than a human reviewer.
F . Real-world misconﬁgurations
To see how often such pattern-based misconﬁgurations oc-
cur in the wild, we manually examined the CTest dataset [20]
which contains 64 real-world conﬁguration-induced failurescollected from 5 open-source projects. We found that, of the64 issues, 51 were due to misconﬁguration while the remaining13 were due to bugs in code that parsed the conﬁgurations.
We studied the 51 misconﬁgurations in detail, and found 27
misconﬁgurations could potentially be caught using patternmining. Of these, 18 misconﬁgurations were due to incor-rect string formats in the value speciﬁed. For example, inHDFS-7359, the conﬁguration parameter has to be parsedas an http address: any other string would cause a failure.The remaining 9 were due to numerical values lying outsidea permissible range. For example, in HBASE-13320, the
conﬁguration parameter should either be a ﬂoat value lessthan 1.0or an integer greater than 1. Regular expressions can
capture such speciﬁcation.
However, in the other 24 cases, we found that regular
expressions would not capture the speciﬁcation for the conﬁgu-ration. For example, in issue ZOOKEEPER-2264, the user has
to specify two conﬁguration parameters or neither: specifyingonly one of the two caused an error. In issue HADOOP-6566,
a conﬁguration parameter has to be set to a directory path,and not a ﬁle path. Regular expressions cannot tell a ﬁle apartfrom a directory. Hence, of all misconﬁgurations, 53% (27 of51) were amenable to pattern mining. With enough trainingexamples, ConfMiner can learn such patterns.
V. T
HREA TS TO VALIDITY
As mentioned in Section II-C, ConfMiner is a best-effort
system built on an inherent assumption that new conﬁgurationvalues will be similar to previous ones. Hence if a developerknowingly changes the value format, ConfMiner will not ﬁnd amatch and hence will generate a false-positive. However, this isunavoidable unless the developer provides hints to ConfMinerbefore-hand that the pattern is about to change. Manual inputof this nature, while useful, is error-prone and does not scale tolarge services that uses millions of conﬁguration parameters.Hence our effort has been to drive false-positives down asmuch as possible by ﬁne-tuning the string proﬁling algorithm.
Also, since we depend upon commit histories and our differ-
ence module that performs a syntactic analysis of conﬁgurationﬁles, for our implementation to be effective, conﬁgurationshould be stored in well-formed ﬁles that are easy to parse, andwell separated from code. If speciﬁcation of conﬁguration isinter-twined with code, or if they use non-standard formats, itbecomes difﬁcult to ﬁne-tune our difference module to do therequired syntactic analysis. While we do see examples of suchscenarios, in most cases, we observe that developers maintaingood hygiene and keep conﬁguration and code ﬁles separate.
Our approach only considers the syntactic format of con-
ﬁguration values and ignores the semantics altogether. Forexample, even when the format of a timeout parameter iscorrect, the value may be incorrect in practice due to beingvery large or very small. In practice, it is not feasible to builda general and completely automated conﬁguration mining toolthat takes semantics into account.
VI. R
ELA TED WORK
A. Conﬁguration Management
Previous work has used conﬁguration ﬁles to learn “correct"
data types [26] and ﬂag misconﬁgurations when they occur.Conﬁguration SpellCheck [27], [28] uses program analysisto detect conﬁguration data types as well. We believe thatdetecting patterns based on data-types is very useful, butcannot capture the nuanced, ﬁne-grained and varied patterns inconﬁguration that are prevalent in today’s large-scale services,as shown in Table II. Further, apart from the basic data-types,Conﬁguration SpellCheck requires the user to manually enterregular expression speciﬁcations for each conﬁguration pat-tern. We believe that the number of conﬁguration parametersin modern systems make manual authoring of speciﬁcationsdifﬁcult, or impossible. Several other tools exist to check andvalidate a conﬁguration ﬁle against a given speciﬁcation [29],[30]. However, most speciﬁcation is high-level and has to bemanually entered by developers.
Recent work has focused on multiple data-driven and pro-
gram analysis-based techniques to detect various differentkinds of misconﬁguration. REX [15] and Encore [31] useassociation rule mining to detect conﬁgurations that are cor-related and ﬂag misconﬁgurations based on the learned rules.PCheck [31], by performing static analysis on code, generatesfast conﬁguration checkers that emulate the code that usesthe conﬁguration. Code [32] analyzes event logs to detectanomalous event sequences and ﬂag potential errors in con-ﬁguration. Though not related to conﬁguration, Getaﬁx [33]uses pattern mining in code to detect missing null-referencechecks in code. All these techniques are complementary to theprogram synthesis-based approach we take.
Several tools( [34], [35], [36]) address how large services
run by Facebook, Microsoft and Akamai have dealt withthe problem of conﬁguration management. These tools helpengineers manage conﬁguration across large deployments thatspan several geographies. A number of commercially availablethird-party tools also target conﬁguration management [37],[38]. Facebook’s holistic conﬁguration [34] speciﬁcally illus-trates the effort required to detect misconﬁgurations early, byusing automated canary testing for changed conﬁgurations, andusing user-deﬁned invariants to drive conﬁguration changes.
826We believe that techniques such as ours can work well in
tandem with such conﬁguration management systems to checkfor correctness before deploying conﬁguration widely.
B. Proﬁling and Program Synthesis
Previous work on data proﬁling has focused more on
statistical proﬁling of numerical data [39], [40], [41], [42].
See [43] for a survey of techniques. Several works in thedatabases literature have considered mining speciﬁcations thatrelate the values of one attribute to values of another throughfunctional dependencies [44], [45], [46]. While we focus moreon string typed values of a single conﬁguration parameter, onepotential direction for future work is to extend the work tolearn from both numerical and string data, possibly relatingthe conﬁguration values of one parameter to another.
Program synthesis has recently found signiﬁcant success in
the data manipulation, cleaning, and transformation ﬁelds [7],[8], [47]. In these settings, the synthesis takes the form ofprogramming-by-example where the user provides a few input-output examples. However, in the string proﬁling setting theuser does not provide examples of each cluster in the proﬁle—instead, the synthesizer predictively learns the proﬁle. In thismanner, string proﬁling is closer in nature to other predictivesynthesis works in the domain for data extraction [12], [48].
C. Regular expression and Automata Learning.
The L
∗algorithm [49] was the ﬁrst technique that learned
ﬁnite automata from examples. Many variants of L∗have been
studied over the past few decades [50], [51], [52] extending
it to other automata variants including non-deterministic ﬁ-nite automata [53], alternating automata [54], and symbolicautomata [55]. However, unlike our technique, L
∗and its
variants depend on an active teacher, i.e., an oracle that canproduce counter-examples to intermediate guesses made bythe learning algorithm. There have also been recent worksthat learn regular expressions from natural language using bothsequence-to-sequence models [56], [57] and program synthesistechniques [58], [59].
There are several key differences between the current work
and previous techniques driven by the underlying setting andmotivation. The setting of our problem requires a techniquethat can learn multiple simple regular expressions that togethermatch the examples as opposed to a single complex one, whileignoring noise in the provided examples. FlashProﬁle [13]is able to produce multiple regular expressions. However, asdepicted in Section III, our technique produces higher qualityproﬁles more efﬁciently as compared to FlashProﬁle. L
∗and
other language theoretic algorithms optimize either the sizeof the output automata or regular expression or minimalityunder language inclusion, resulting in complex and over-ﬁttedregular expressions, making them unsuitable for our purposes.
VII. C
ONCLUSION
We have described a string proﬁling algorithm that learns
various patterns in conﬁguration used by large services. Wehave realized this through a tool called ConfMiner which isdeployed on four repositories that maintain conﬁguration fora large enterprise service. Using two sets of data that areavailable through version control systems – ﬁle-based andhistory-based – we show that our techniques learn a largenumber of varied patterns in conﬁguration. These patternscapture various kinds of semantics thereby making the casefor a generic algorithm that works across multiple domains.Finally, we also show that using these patterns, we can capturevarious kinds of misconﬁguration at commit-time.
R
EFERENCES
[1] Microsoft, “Microsoft 365.” https://www.microsoft.com/microsoft-365.
[Online; accessed 24-August-2021].
[2] SalesForce, “Acccess issue: May and june 2019.” https://status.
salesforce.com/incidents/3822. [Online; accessed 28-August-2020].
[3] Google, “Google cloud networking incident 19009.” https://status.cloud.
google.com/incident/cloud-networking/19009. [Online; accessed 28-
August-2020].
[4] Sophos, “The state of cloud security 2020.” https:
//secure2.sophos.com/en-us/medialibrary/pdfs/whitepaper/sophos-the-state-of-cloud-security-2020-wp.pdf. [Online; accessed28-August-2020].
[5] O. Moolchandani, “Cloud waterhole - a novel cloud
attack observed on twilio.” https://www.linkedin.com/pulse/cloud-waterhole-novel-attack-observed-twilio-om-moolchandani/.[Online; accessed 28-August-2020].
[6] M. S. R. Center, “Access misconﬁguration for customer
support database.” https://msrc-blog.microsoft.com/2020/01/22/access-misconﬁguration-for-customer-support-database/. [Online;accessed 28-August-2020].
[7] S. Gulwani, “Automating string processing in spreadsheets using input-
output examples,” in Proceedings of the 38th ACM SIGPLAN-SIGACT
Symposium on Principles of Programming Languages, POPL 2011,Austin, TX, USA, January 26-28, 2011 (T. Ball and M. Sagiv, eds.),
pp. 317–330, ACM, 2011.
[8] V . Le and S. Gulwani, “Flashextract: a framework for data extraction by
examples,” in ACM SIGPLAN Conference on Programming Language
Design and Implementation, PLDI ’14, Edinburgh, United Kingdom -June 09 - 11, 2014 (M. F. P . O’Boyle and K. Pingali, eds.), pp. 542–553,
ACM, 2014.
[9] R. Singh, “Blinkﬁll: Semi-supervised programming by example for
syntactic string transformations,” Proc. VLDB Endow., vol. 9, no. 10,
pp. 816–827, 2016.
[10] R. Martins, J. Chen, Y . Chen, Y . Feng, and I. Dillig, “Trinity: An
extensible synthesis framework for data science,” Proc. VLDB Endow.,
vol. 12, no. 12, pp. 1914–1917, 2019.
[11] A. S. Iyer, M. Jonnalagedda, S. Parthasarathy, A. Radhakrishna, and
S. K. Rajamani, “Synthesis and machine learning for heterogeneousextraction,” in Proceedings of the 40th ACM SIGPLAN Conference
on Programming Language Design and Implementation, PLDI 2019,Phoenix, AZ, USA, June 22-26, 2019 (K. S. McKinley and K. Fisher,
eds.), pp. 301–315, ACM, 2019.
[12] M. Raza and S. Gulwani, “Automated data extraction using predictive
program synthesis,” in Proceedings of the Thirty-First AAAI Conference
on Artiﬁcial Intelligence, February 4-9, 2017, San Francisco, California,USA (S. P . Singh and S. Markovitch, eds.), pp. 882–890, AAAI Press,
2017.
[13] S. Padhi, P . Jain, D. Perelman, O. Polozov, S. Gulwani, and T. D.
Millstein, “Flashproﬁle: a framework for synthesizing data proﬁles,”Proc. ACM Program. Lang., vol. 2, no. OOPSLA, pp. 150:1–150:28,2018.
[14] Google, “Google app engine app.yaml reference.” https://cloud.google.
com/appengine/docs/standard/python/conﬁg/appref. [Online; accessed28-August-2020].
[15] S. Mehta, R. Bhagwan, R. Kumar, C. Bansal, C. Maddila, B. Ashok,
S. Asthana, C. Bird, and A. Kumar, “Rex: Preventing bugs and miscon-ﬁguration in large services using correlated change analysis,” in 17th
USENIX Symposium on Networked Systems Design and Implementation(NSDI 20), (Santa Clara, CA), pp. 435–448, USENIX Association, Feb.2020.
827[16] Visual Studio Code, “Intellisense in visual studio code.” https://code.
visualstudio.com/docs/editor/intellisense. [Online; accessed 24-April-
2019].
[17] “Content assist in eclipse.” https://www.eclipse.org/pdt/help/html/code_
assist.htm. [Online; accessed 28-August-2020].
[18] “Stylecop analyzers for the .net compiler platform.” https://github.com/
DotNetAnalyzers/StyleCopAnalyzers. [Online; accessed 28-August-2020].
[19] Microsoft, “Microsoft program synthesis using examples (prose) sdk.”
https://www.microsoft.com/en-us/research/group/prose. [Online; ac-cessed 28-August-2020].
[20] X. Sun, R. Cheng, J. Chen, E. Ang, O. Legunsen, and T. Xu, “Testing
conﬁguration changes in context to prevent production failures,” in 14th
USENIX Symposium on Operating Systems Design and Implementation(OSDI 20), pp. 735–751, USENIX Association, Nov. 2020.
[21] The Git V ersion Control System. https://git-scm.com/.
[22] GitHub Inc. https://github.com. [Online; accessed 24-April-2019].
[23] Microsoft Azure DevOps. https://azure.microsoft.com/en-in/services/
devops/. [Online; accessed 24-April-2019].
[24] Microsoft, “Generating diffgrams of xmlﬁles.” https://www.nuget.org/
packages/XMLDiffPatch/. [Online; accessed 24-April-2019].
[25] Microsoft Azure Cloud Services. https://docs.microsoft.com/en-us/
azure/cloud-services/cloud-services-choose-me. [Online; accessed 24-
April-2019].
[26] M. Santolucito, E. Zhai, and R. Piskac, “Probabilistic automated lan-
guage learning for conﬁguration ﬁles,” in Computer Aided V eriﬁcation
(S. Chaudhuri and A. Farzan, eds.), (Cham), pp. 80–87, SpringerInternational Publishing, 2016.
[27] A. Rabkin, “Using program analysis to reduce misconﬁguration in
open source systems software,” tech. rep., Electrical Engineering andComputer Sciences, University of California at Berkeley, 2012.
[28] A. Rabkin, “The conf_spellchecker tool.” https://github.com/roterdam/
jchord/tree/master/conf\_spellchecker. [Online; accessed 28-August-2020].
[29] “V alidating xml ﬁles using xsd in c#.” https://www.c-sharpcorner.com/
article/how-to-validate-xml-using-xsd-in-c-sharp/. [Online; accessed28-August-2020].
[30] “Conﬁgcop: A swift command line application that veriﬁes .xcconﬁg
ﬁles against a template..” https://github.com/ﬁvegoodfriends/ConﬁgCop.[Online; accessed 28-August-2020].
[31] T. Xu, X. Jin, P . Huang, Y . Zhou, S. Lu, L. Jin, and S. Pasupathy, “Early
detection of conﬁguration errors to reduce failure damage,” in 12th
USENIX Symposium on Operating Systems Design and Implementation(OSDI 16), (Savannah, GA), pp. 619–634, USENIX Association, 2016.
[32] D. Y uan, Y . Xie, R. Panigrahy, J. Y ang, C. V erbowski, and A. Kumar,
“Context-based online conﬁguration-error detection,” in Proceedings of
the 2011 USENIX Conference on USENIX Annual Technical Conference,USENIXA TC’11, (USA), p. 28, USENIX Association, 2011.
[33] A. Scott, J. Bader, and S. Chandra, “Getaﬁx: Learning to ﬁx bugs
automatically,” CoRR, vol. abs/1902.06111, 2019.
[34] C. Tang, T. Kooburat, P . V enkatachalam, A. Chander, Z. Wen,
A. Narayanan, P . Dowell, and R. Karl, “Holistic conﬁguration manage-ment at facebook,” in Proceedings of the 25th Symposium on Operating
Systems Principles, pp. 328–343, ACM, 2015.
[35] A. Sherman, P . A. Lisiecki, A. Berkheimer, and J. Wein, “Acms: The
akamai conﬁguration management system,” in Proceedings of the 2Nd
Conference on Symposium on Networked Systems Design & Implementa-tion - V olume 2, NSDI’05, (Berkeley, CA, USA), pp. 245–258, USENIXAssociation, 2005.
[36] P . Huang, W . J. Bolosky, A. Singh, and Y . Zhou, “Confvalley: A
systematic conﬁguration validation framework for cloud services,” inProceedings of the Tenth European Conference on Computer Systems,EuroSys ’15, (New Y ork, NY , USA), pp. 19:1–19:16, ACM, 2015.
[37] “The puppet conﬁguration management tool.” https://puppet.com/. [On-
line; accessed 28-August-2020].
[38] “Ansible for it automation.” https://www.ansible.com/. [Online; accessed
28-August-2020].
[39] G. Cormode, M. Garofalakis, P . J. Haas, and C. Jermaine, “Synopses for
massive data: Samples, histograms, wavelets, sketches,” Found. Trends
Databases, vol. 4, p. 1–294, Jan. 2012.
[40] P . J. Haas, J. F. Naughton, S. Seshadri, and L. Stokes, “Sampling-
based estimation of the number of distinct values of an attribute,” inProceedings of the 21th International Conference on V ery Large DataBases, VLDB ’95, (San Francisco, CA, USA), p. 311–322, MorganKaufmann Publishers Inc., 1995.
[41] Y . Ioannidis, “The history of histograms (abridged),” in Proceedings of
the 29th International Conference on V ery Large Data Bases - V olume29, VLDB ’03, p. 19–30, VLDB Endowment, 2003.
[42] P . Karras and N. Mamoulis, “The haar+ tree: A reﬁned synopsis
data structure,” in 2007 IEEE 23rd International Conference on Data
Engineering, pp. 436–445, 2007.
[43] Z. Abedjan, L. Golab, and F. Naumann, “Proﬁling relational data: A
survey,” The VLDB Journal , vol. 24, p. 557–581, Aug. 2015.
[44] A. Heise, J.-A. Quiané-Ruiz, Z. Abedjan, A. Jentzsch, and F. Naumann,“Scalable discovery of unique column combinations,” Proc. VLDB
Endow., vol. 7, p. 301–312, Dec. 2013.
[45] T. Papenbrock, J. Ehrlich, J. Marten, T. Neubert, J.-P . Rudolph,
M. Schönberg, J. Zwiener, and F. Naumann, “Functional dependencydiscovery: An experimental evaluation of seven algorithms,” Proc. VLDB
Endow., vol. 8, p. 1082–1093, June 2015.
[46] Y . Zhang, Z. Guo, and T. Rekatsinas, “A statistical perspective on
discovering functional dependencies in noisy data,” in Proceedings of
the 2020 ACM SIGMOD International Conference on Management ofData, SIGMOD ’20, (New Y ork, NY , USA), p. 861–876, Associationfor Computing Machinery, 2020.
[47] R. Singh, “Blinkﬁll: Semi-supervised programming by example for syn-
tactic string transformations,” Proc. VLDB Endow., vol. 9, p. 816–827,
June 2016.
[48] M. Raza and S. Gulwani, “Web data extraction using hybrid program
synthesis: A combination of top-down and bottom-up inference,” inProceedings of the 2020 International Conference on Management ofData, SIGMOD Conference 2020, online conference [Portland, OR,USA], June 14-19, 2020 (D. Maier, R. Pottinger, A. Doan, W . Tan,
A. Alawini, and H. Q. Ngo, eds.), pp. 1967–1978, ACM, 2020.
[49] D. Angluin, “Learning regular sets from queries and counterexamples,”
Inf. Comput., vol. 75, p. 87–106, Nov. 1987.
[50] R. Parekh and V . Honavar, “An incremental interactive algorithm for reg-
ular grammar inference,” in International Colloquium on Grammatical
Inference, pp. 238–249, Springer, 1996.
[51] R. Parekh and V . Honavar, “Learning dfa from simple examples,”
Machine Learning, vol. 44, no. 1, pp. 9–35, 2001.
[52] R. L. Rivest and R. E. Schapire, “Inference of ﬁnite automata using
homing sequences,” in Proceedings of the Twenty-First Annual ACM
Symposium on Theory of Computing, STOC ’89, (New Y ork, NY , USA),p. 411–420, Association for Computing Machinery, 1989.
[53] F. Denis, A. Lemay, and A. Terlutte, “Learning regular languages using
non deterministic ﬁnite automata,” in ICGI, 2000.
[54] D. Angluin, S. Eisenstat, and D. Fisman, “Learning regular languages
via alternating automata.,” in IJCAI, pp. 3308–3314, 2015.
[55] S. Drews and L. D’Antoni, “Learning symbolic automata,” in Tools
and Algorithms for the Construction and Analysis of Systems - 23rdInternational Conference, TACAS 2017, Held as Part of the EuropeanJoint Conferences on Theory and Practice of Software, ETAPS 2017,Uppsala, Sweden, April 22-29, 2017, Proceedings, Part I (A. Legay and
T. Margaria, eds.), vol. 10205 of Lecture Notes in Computer Science,
pp. 173–189, 2017.
[56] N. Locascio, K. Narasimhan, E. DeLeon, N. Kushman, and R. Barzilay,
“Neural generation of regular expressions from natural language withminimal domain knowledge,” 08 2016.
[57] Z. Zhong, J. Guo, W. Y ang, J. Peng, T. Xie, J.-G. Lou, T. Liu,
and D. Zhang, “Semregex: A semantics-based approach for generatingregular expressions from natural language speciﬁcations,” in EMNLP,
2018.
[58] X. Y e, Q. Chen, X. Wang, I. Dillig, and G. Durrett, “Sketch-driven
regular expression generation from natural language and examples,”Trans. Assoc. Comput. Linguistics, vol. 8, pp. 679–694, 2020.
[59] M. Lee, S. So, and H. Oh, “Synthesizing regular expressions from
examples for introductory automata assignments,” in Proceedings of the
2016 ACM SIGPLAN International Conference on Generative Program-ming: Concepts and Experiences, GPCE 2016, (New Y ork, NY , USA),p. 70–80, Association for Computing Machinery, 2016.
828