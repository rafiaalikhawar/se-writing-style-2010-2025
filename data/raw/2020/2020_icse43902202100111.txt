TRANS REGEX : Multi-modal Regular Expression
Synthesis by Generate-and-Repair
Yeting Liyx, Shuaimin Lix, Zhiwu Xu\, Jialun Caoz, Zixuan Chenyx, Yun Hu]x, Haiming Cheny, Shing-Chi Cheungz
yState Key Laboratory of Computer Science, Institute of Software, Chinese Academy of Sciences, Beijing, China
xSchool of Computer Science and Technology, University of Chinese Academy of Sciences, Beijing, China
\College of Computer Science and Software Engineering, Shenzhen University, Shenzhen, China
zThe Hong Kong University of Science and Technology, Hong Kong, China
]Science & Technology on Integrated Infomation System Laboratory,
Institute of Software, Chinese Academy of Sciences, Beijing, China
yfliyt,chenzx,chmg@ios.ac.cn,xlishuaimin17@mails.ucas.ac.cn
\xuzhiwu@szu.edu.cn,zfjcaoap,sccg@cse.ust.hk
]huyun2016@iscas.ac.cn
Abstract ‚ÄîSince regular expressions (abbrev. regexes) are dif-
Ô¨Åcult to understand and compose, automatically generating
regexes has been an important research problem. This paper
introduces T RANS REGEX , for automatically constructing regexes
from both natural language descriptions and examples. To the
best of our knowledge, T RANS REGEX is the Ô¨Årst to treat the
NLP-and-example-based regex synthesis problem as the problem
of NLP-based synthesis with regex repair. For this purpose, we
present novel algorithms for both NLP-based synthesis and regex
repair. We evaluate T RANS REGEX with ten relevant state-of-the-
art tools on three publicly available datasets. The evaluation
results demonstrate that the accuracy of our T RANS REGEX
is 17.4%, 35.8% and 38.9% higher than that of NLP-based
approaches on the three datasets, respectively. Furthermore,
TRANS REGEX can achieve higher accuracy than the state-
of-the-art multi-modal techniques with 10% to 30% higher
accuracy on all three datasets. The evaluation results also indicate
TRANS REGEX utilizing natural language and examples in a more
effective way.
Index Terms‚Äîregex synthesis, regex repair, programming by
natural languages, programming by example
I. I NTRODUCTION
As a versatile mechanism for pattern matching and search-
ing, regular expressions (abbrev. regexes) have been widely
used in different Ô¨Åelds of computer science such as pro-
gramming languages, natural language processing (NLP) and
databases due to the high effectiveness and accuracy [1]‚Äì
[6]. Unfortunately, despite their popularity, regexes can be
difÔ¨Åcult to understand and compose even for experienced
programmers [1], [3], [7]‚Äì[9].
To alleviate this problem, prior research has proposed
techniques to automatically generate regexes. For example,
several techniques generate regexes from natural language
(NL) descriptions [10]‚Äì[13], while others synthesize regexes
from examples [14]‚Äì[20]. Though these techniques help lessen
the difÔ¨Åculties of automatic regex synthesis, they have obvious
drawbacks as follows.
Corresponding author.Existing NLP-based techniques can only generate regexes
similar in shape to the training data and have relatively
low accuracy on simple benchmark datasets (e.g., S OFT-
REGEX [13] achieved only 62.8% accuracy on benchmark
NL-RX-Turk [11]). Furthermore, NLP-based techniques are
impeded by the ambiguity and imprecision of NL even for
stylized English [13], [21], [22]. For example, according to
[13], among 921 incorrectly predicted regexes, over 38.4% are
caused by ambiguity of NL descriptions and 27.8% are from
imprecision. Additionally, Zhong et al. [22] found that NLP-
based techniques may not make correct prediction if words in
these NL descriptions are not covered by the training data.
On the other hand, the example-based synthesis approaches
rely on high quality examples provided by users. The syn-
thesized regexes may be under-Ô¨Åtting or over-Ô¨Åtting when the
given examples do not meet the implicit quality requirements
(e.g, insufÔ¨Åcient or not characteristic enough). However, exam-
ples with high quality are often unavailable in practice. It poses
difÔ¨Åculties in applying purely example-based approaches. In
addition, these approaches have severe restrictions on the kinds
of regexes that they can synthesize (e.g., absence of Kleene
star [14], [15], limited character occurrences [16]‚Äì[18], [20]
or constraining to binary alphabet [19]).
Therefore, to better synthesize regexes it would be ideal to
take advantage of both NL and examples (called NLP-and-
example-based synthesis or multi-modal synthesis): the use
of advanced NLP-based techniques can reduce the amount
of required (characteristic) examples meanwhile alleviate the
amount of effort from users; while the use of examples can
effectively disambiguate or correct errors in the descriptions.
Further, a survey on posts on regex synthesis shows that many
programmers actually use NL descriptions as a major resource,
and leverage some example(s) to resolve the ambiguities
of NL [23]. On the other hand, insufÔ¨Åcient (characteristic)
examples limit the generalization ability of example-based
approaches, while incorporating NL can improve the gen-
eralization ability, and help to drastically narrow down the
12102021 IEEE/ACM 43rd International Conference on Software Engineering (ICSE)
1558-1225/21/$31.00 ¬©2021 IEEE
DOI 10.1109/ICSE43902.2021.00111
search space [23]. Actually there have been recent attempts
in this direction [21], [24], in which they Ô¨Årst translated the
NL description into a sketch1, then searched the regex space
deÔ¨Åned by the sketch guided by the given examples. However,
the forms of translated sketches are restricted. This prevents
regexes from being synthesized correctly when the generated
sketches are inappropriate (e.g., logically-incorrect). In such a
case, the incorrectness will be inherited from sketches to the
subsequent regex. Moreover, while these works [21], [24] have
achieved relatively high accuracy on simple datasets, they did
not perform well on complex and realistic datasets. In latter
datasets, the NL descriptions are longer, more complicated,
and describe the regexes which are more complex in terms of
length and tree-depth [22], [25].
We observe that most of the incorrect regexes generated by
NLP-based techniques are very similar to the target regexes
with subtle differences, and can be made equivalent2to the
target regexes with only minor modiÔ¨Åcations (e.g., reorder-
ing/revising characters or quantiÔ¨Åers). This motivates us to
view the NLP-and-example-based regex synthesis problem as
the problem of NLP-based synthesis with regex repair, and
develop the Ô¨Årst framework, T RANS REGEX , to leverage both
NL and examples for regex synthesis by using NLP-based
and regex repair techniques. T RANS REGEX uses an NLP-
based synthesizer to convert the description into a regex. If
the synthesized regex is inconsistent with the given examples,
it then leverages a regex repairer to modify the synthesized
regex guided by the examples, and returns the revised regex.
Considering that the lack of focus on validity in previous
NLP-based works (e.g., on the dataset StructuredRegex, less
than half of the regexes predicted by D EEP-REGEX (Locascio
et al.) [11] are valid, see Section IV-D), we propose a two-
phase NLP-based synthesis model S2REto solve this problem
as follows. By rewarding the S2REmodel with validity, in
addition to the semantic correctness reward used in previous
works [10], [13], our model is towards generating more valid
regexes than previous models. If the generated regexes are
still invalid after that, the invalid2valid model, wherein the
structure of S2REis reused, is used to transform them into
valid ones to further guarantee the validity of regexes.
There are various drawbacks or limitations in existing repair
techniques, such as only supporting positive or negative ex-
amples [26], [27], not supporting regexes with the conjunction
(&) operator, or having the problems of under-Ô¨Åtting/over-
Ô¨Åtting [20], [28]. To overcome these drawbacks or limitations,
we present a novel and efÔ¨Åcient algorithm, S YNCORR, based
on Neighborhood Search (NS) to repair an incorrect regex to
achieve that the repaired regex is consistent with the examples.
Particularly S YNCORR alleviates the under-Ô¨Åtting/over-Ô¨Åtting
problem, via preserving the integrity of the small sub-regexes.
TRANS REGEX can greatly reduce the aforementioned errors
caused by ambiguity, imprecision or unknown words in NL
descriptions, via using the example-guided regex repairer.
1A sketch is an incomplete regex containing holes to denote missing
components.
2Two regexes are equal iff their corresponding languages are equivalent.In comparison with existing multi-modal works [21], [24],
TRANS REGEX avoids their limitations by not restricting the
sketches of the generated regex. Furthermore, T RANS REGEX
modularizes the synthesis problem as the NLP-based regex
synthesis and example-guided regex repair, allowing one to
use his own algorithms or any other new algorithms instead.
We evaluate T RANS REGEX by comparing T RANS REGEX
against ten state-of-the-art tools on three publicly available
datasets. Our evaluation demonstrates the accuracy of our
TRANS REGEX is 17.4%, 35.8% and 38.9% higher than that of
NLP-based works on the three datasets, respectively. Further,
TRANS REGEX can achieve higher accuracy than the state-of-
the-art multi-modal works [21], [24], with 10% to 30% higher
accuracy on all three datasets. Our evaluation also reveals
our NLP-based model S2REcan generate 100% valid regexes
on complex dataset, whereas other NLP-based tools can syn-
thesize 49.6% to 90.6% valid ones. Finally, the evaluation
results on regex repair also show that our S YNCORR has better
capability than existing repair tools.
The contributions of this paper are listed as follow.
We propose T RANS REGEX , an automatic framework
which can synthesize regular expressions from both NL
descriptions and examples. To the best of our knowledge,
TRANS REGEX is the Ô¨Årst to treat the NLP-and-example-
based regex synthesis problem as the problem of NLP-
based synthesis with regex repair.
We introduce a two-phase algorithm S2RE for regex
synthesis from NL. By rewarding the S2REmodel with
validity and using the invalid2valid model, S2REgener-
ates more valid regexes while having similar or higher
accuracy than the state-of-the-art NLP-based models.
We present a novel algorithm S YNCORR for regex repair
that (i) leverages Neighborhood Search (NS) algorithms
to guide the search for a better regex which is consistent
with the given examples from the neighborhoods of the
incorrect regex, and (ii) utilizes some rewriting rules for
sub-regexes abstraction to preserve the integrity of some
small sub-regexes, thereby alleviating under-Ô¨Åtting/over-
Ô¨Åtting and efÔ¨Åciently reducing the search space.
We conduct a series of comprehensive experiments com-
paring T RANS REGEX with ten state-of-the-art synthe-
sis tools. The evaluation results demonstrate that the
accuracy of our T RANS REGEX is 17.4%, 35.8% and
38.9% higher than that of NLP-based approaches on
the three datasets, respectively, while T RANS REGEX can
achieve higher accuracy than the state-of-the-art multi-
modal techniques with 10% to 30% higher accuracy on
all three datasets. The evaluation results also indicate
TRANS REGEX utilizing natural language and examples
in a more effective way.
II. O VERVIEW
In this section, we present an overview of T RANS REGEX .
As illustrated in Fig. 1, T RANS REGEX consists of two steps,
namely, the NLP-based regex synthesis (Section III-C) and
theexample-guided regex repair (Section III-D). In the Ô¨Årst
1211Natural Language 
DescriptionNLP -based Regex 
Synthesizer
Regex Repairer
Examples
Regex
Output RegexYN
Users
Consistent With
Examples ?Regex
ExamplesFig. 1. An overview of framework T RANS REGEX for regex synthesis.
step, NLP-based regex synthesis takes the given NL descrip-
tion as input and tries to synthesize a regex from the NL
description via an NLP-based synthesizer. After that, if the
synthesized regex is consistent with the given examples, then
TRANS REGEX outputs the regex. Otherwise, example-guided
regex repair modiÔ¨Åes this synthesized regex based on the
provided examples by an example-guided repairer, and returns
the repaired regex. Next, we illustrate the main ideas behind
TRANS REGEX using a motivating example.
Example II.1. Consider the task of constructing a regex
[AEIOUaeiou]. *[0-9]{7,}. *. The NL description
NL, the positive examples P, and the negative examples N
are shown in Fig. 2.
Natural Language Description NL
items with a vowel preceding a numeral at least 7 times
Positive Examples P Negative Examples N
E18043699 u.
U530136382 jz;B
U65972791327 o45
U82433805 FBcW
i3390716928 I4k,S
O789821610 U
U4765749255 I$#].
E6204251 A
e6868266 uV
O50693106874 o20m3u5817
Fig. 2. A pair of a description and examples.
First, T RANS REGEX utilizes our NLP-based synthesizer
S2RE to translate the NL description NL into a regex.
As shown in Fig. 3, the encoder of S2RE generates
latent vectors from the given description NL, and the
decoder of S2RE synthesizes the corresponding regex
([AEIOUaeiou]. *[0-9]. *){7,,} according to the la-
tent vectors from the encoder. However, it is clear that this syn-
thesized regex is invalid. S2REthen converts this invalid regex
into a valid one ([AEIOUaeiou]. *[0-9]. *){7,} usingour pretrained invalid2valid model. It is easy to verify that the
valid regex generated by S2REis inconsistent with the pro-
vided examples, such as the positive example E18043699 =2
L(([AEIOUaeiou]. *[0-9]. *){7,}). Intrinsically the
descriptionNL shown in Fig. 2 is ambiguous‚Äîi.e., it is
unclear what part of the string should appear at least 7 times,
resulting in the incorrect regex generated by S2RE.
*[0-9] ) . .* { 7 , } [AEIOU
aeiou ](
* [0-9] ) . .* { 7 , } [AEIOU
aeiou ]( ,* [0-9] ) . .* { 7 ,} [AEIOU
aeiou ](
avowel preceding at anumeral least times with items 7
Valid ?ùêíùüêùêëùêÑModel
Y
N
Invalid2Valid ModelOutput Regex,
Fig. 3. The process of the algorithm S2RE.
So after that, T RANS REGEX employs the algorithm S YN-
CORR that is based on Neighborhood Search (NS) to repair
the incorrect regex, given in Fig. 4. As we have mentioned
in Section I, the incorrect regexes generated by S2REmay
be very similar to the target regexes with subtle differences.
Therefore, S YNCORR Ô¨Årst converts the above incorrect regex
to the abstract regex r=(<VOW><S><NUM><S>)<Q 7;>
with symbolic symbols obtained by executing the function
preprocess, so that the integrity of small sub-regexes (e.g.,
[AEIOUaeiou] and[0-9]) can be retained as much as
possible in the subsequent steps. Then S YNCORR calls the
function transformations to get the neighbours of r, i.e., some
abstract regexes (e.g., <VOW><S>(<NUM>)<Q 7;><S>) that
1212ùêíùêìùêÑùêè ùüè.
input anincorrect regex ùëü!:AEIOUaeiou 0‚àí9.‚àó7,
ùêíùêìùêÑùêè ùüê.
ùëü‚Üêùëùùëüùëíùëùùëüùëúùëêùëíùë†ùë† ùëü!,ùëô"#$=0:
ùêíùêìùêÑùêè ùüë.
ùêíùêìùêÑùêè ùüí.
ùêíùêìùêÑùêè ùüì.
calculate ùëìforeach ùëü‚ààùëÅùëü:0.00.71.00.7
ùêíùêìùêÑùêèùüî.
return therepaired regex ùëü:AEIOUaeiou 0‚àí97,.‚àó.‚àó
.‚àó
.‚àóùëÅùëü‚Üêùë¢ùëõùëùùëüùëíùëùùëüùëúùëêùëíùë†ùë† ùëÅùëü,ùëô"#$=0:AEIOUaeiou 7,0‚àí9.‚àó
AEIOUaeiou .‚àó7,0‚àí9.‚àó
AEIOUaeiou 0‚àí97,.‚àó
AEIOUaeiou 0‚àí9.‚àó7,< > VOW <>S< > NUM < >Q%, <>S
ùëÅùëü‚Üêùë°ùëüùëéùëõùë†ùëìùëúùëüùëöùëéùë°ùëñùëúùëõùë† ùëü:
( )< > VOW < >Q%,<>S< > NUM <>S
< > VOW <>S< >Q%,< > NUM <>S
< > VOW < >Q%, <>S< > NUM <>S
< > VOW < >Q%, <>S< > NUM <>S.‚àó
.‚àóFig. 4. The process of the algorithm S YNCORR.
are similar to rthrough a series of subtle transformations3
(e.g., quantiÔ¨Åer adjustment orelement replacement, etc.). Next,
SYNCORR maps these abstract regexes into corresponding
concrete regexes via using the function unpreprocess, and cal-
culates thefvalue4of each regex. Finally, S YNCORR returns
the regex [AEIOUaeiou]. *[0-9]{7,}. *withfvalue
of1. Leveraging NLP-based synthesis with regexs repair, our
TRANS REGEX is able to synthesize the correct one mentioned
above. This success case shows that our T RANS REGEX can
well deal with the ambiguity of NL and the invalidity of
regexes produced by some NLP-based synthesizers.
In addition, it is worth noting that on the same
example and the incorrect regex mentioned above, the
repair tool RF IXER [28] produces the incorrect regex
([AEeiO01234U56789]. *[0-9]. *){2,}. SpeciÔ¨Åcally,
RFIXER cannot generalize for some unseen examples (e.g., a
positive example a1234567 ), this will result in the regex
produced by RF IXER without some characters like ‚Äúa‚Äù, i.e., the
generated regexes will be over-Ô¨Åtting. In contrast, S YNCORR
avoids over-Ô¨Åtting well by preserving the integrity of the small
sub-regexes.
However, considering that S YNCORR is an algorithm based
on NS and thus may trap in local optimum, we will continue
to use RF IXER to repair if S YNCORR fails. As demonstrated
in TABLE IV, S YNCORR+RF IXER can achieve more than
10% higher success rate of repair than S YNCORR on the
experimental datasets. Further, if there will be more powerful
repair tool than RF IXER , by combining S YNCORR we can
achieve even higher success rate, which is a future work.
III. R EGEX SYNTHESIS ALGORITHM
In this section, we present the details of our synthesis
algorithm T RANS REGEX . Before that, we Ô¨Årst provide the
background.
3In Fig. 4, we only demonstrate transformation quantiÔ¨Åer adjustment in
STEP 3. For all transformations, see Section III.
4Thefvalue of a regex represents to which degree the regex meets the
given examples. Especially, a regex with fvalue 1will accepts all positive
examplesPand rejects all negative examples N.A. Background
Letbe a Ô¨Ånite alphabet of symbols. The set of all words
over is denoted by . The empty word and the empty set
are denoted by "and?, respectively.
Regular Expression (Regex). Expressions of ",?, anda2
are regular expressions; a regular expression is also formed
using the operators
[C]r1jr2r1r2r1&r2(r)r1r1fm;ng
whereC; C6=f"g or?is a set of characters,
m2N,n2N[f1g, and mn. Besides, r?,r,r+
andrfigwherei2Nare abbreviations of rf0;1g,rf0;1g,
rf1;1gandrfi;ig, respectively. rfm;1gis often simpliÔ¨Åed
asrfm;g. The languageL(r)of a regular expression ris
deÔ¨Åned inductively as follows: L(?) = ?;L(") =f"g;
L(a) =fag;L([C ]) =C;L(r 1jr2) =L(r 1)[L(r 2);
L(r 1r2) =fvwjv2L(r 1);w2L(r 2)g;L(r 1&r2) =
fvjv2L(r 1)^v2L(r 2)g;L(r 1) =fvjv =2L(r 1)g;
L(rfm;ng) =S
m6i6nL(r)i.
If an expression follows the syntax above, then it is a
valid one, otherwise it is invalid. For instance, the expres-
sionab{1,3} is valid, but the expression ab{1,,,,3} is
invalid.
B. The Main Algorithm
Our synthesis algorithm is shown in Algorithm 1, which
aims to synthesize regexes from NL descriptions and exam-
ples. In detail, our algorithm Ô¨Årst employs an NLP-based
synthesizer S2REto generate a regex rfrom the given NL
descriptionNL (line 1), which is introduced in Section III-C.
Then, ifris consistent with the given positive and nega-
tive examples, T RANS REGEX outputsr(line 2). Otherwise,
TRANS REGEX leverages two example-guided repairers to Ô¨Åx
the incorrect regex rbased on the provided positive and
negative examples, which are described in Section III-D, and
returns the repaired regex (lines 3‚Äì6).
Algorithm 1: TRANS REGEX
Input: a natural language description NL, positive
examplesP, and negative examples N
Output: a regex
1r S2RE(NL );
2ifPL(r)andN\L(r) =?then return r;
3else
4r SYNCORR(r);
5 ifPL(r)andN\L(r) =?then return r;
6 else return RFIXER (r);
C. Regex Synthesis from Natural Language Descriptions
To synthesize a regex from the NL description, we build
a seq2seq model with attention mechanism as our regex
synthesis model S2RE. It consists of an encoder and a decoder.
The encoder initializes the words in the NL sequences as
vectors, then it encodes the vectors as hidden states, which
1213represents the semantic representations of the NL sequences.
The decoder generates the corresponding regex according to
the latent representations from the encoder.
The training of our neural S2RE model consists of two
stages, using two different strategies.
Maximum Likelihood Estimation (MLE): In the Ô¨Årst
stage, we use MLE to maximize the likelihood of mapping
the NL description to corresponding regex:
=argmaxX
(NL;r )2Dlog(p(rjNL)) (1)
whereDis the training set, p(rjNL) is the probability that
the seq2seq model generates a regex rfrom a NL description
NL.
Policy Gradient: MLE may fail to consider the semantic
equivalence of the regexes that might be different in syntax and
the validity of the regexes (especially for complex and realistic
datasets). Therefore, in the second stage, we gradually train
ourS2REmodel via policy gradient [29] by rewarding the
model according to the following two indicators.
Semantic Correctness: following Park et al.‚Äôs work [13],
we reward the S2REmodel if it generates a regex that
is semantically equivalent to the ground truth, that is, the
semantic reward RC(r)is1if the regex r is semantically
equal with ground truth and 0 otherwise;
Syntactic Validity: we also reward the S2RE model if
it generates regexes that are valid5, that is, the syntactic
rewardRV(r)is1if the regex r is valid and 0 otherwise.
Finally, the objective of the second stage is to maximize the
following function:
J() =X
(NL;r)2Dp(rjNL)R (r) (2)
R(r) =RC(r) +RV(r) (3)
whereandare hyper-parameters.
TheS2REmodel helps to generate more correct and valid
regexes than the previous models. However, it still can not
guarantee to generate valid regexes for all the input NL
descriptions. To solve this problem, we reuse the structure of
theS2RE model to build our invalid2valid model (wherein
onlyRVis used). SpeciÔ¨Åcally, the invalid2valid model is
trained on 5;000 invalid and valid regex pairs, which are
collected as following:
Get a valid regex randomly from the dataset Structure-
dRegex [25];
Make it invalid by performing some minor changes on it :
adding or deleting or modifying 1-5 positions randomly;
If the changed regex is still valid, then discard it.
The whole process of generating regexes from NL descrip-
tions is shown in Fig. 3. It shows that, the algorithm Ô¨Årst uses
ourS2REmodel to generate a regex. If the generated regex is
invalid, it then transform this regex fast to a valid one using
5The syntax-checking is implemented via the re.compile() function in
Python.the pretrained invalid2valid model. The experiments show that
after the S2REmodel and the invalid2valid model, we obtain
100% valid regexes in syntax.
D. Regex Repair from Examples
The regexes generated by S2REmay be incorrect but very
similar to the target regexes with subtle differences. In this
section, we present our algorithm S YNCORR to repair these
incorrect regexes. The key idea is to search for a better
regex which accepts more positive examples and rejects more
negative examples from the neighborhoods of input regex.
To start with, we deÔ¨Åne the neighborhood and an evaluation
criterion of regex r. Given a regex r, we deÔ¨Åne its neighbor-
hood, denoted as N(r), as the set of regexes, which can be
obtained by applying a transformation on r(transformations
are given later). In order to select a regex ramong a set of
regexes, we deÔ¨Åne a measure fonrwith respects to positive
examplesPand negative examples Nas
f(r;P;N) =jTPj+jTNj jFPj jFNjjPj+jNj(4)
whereTP=fw2L(r)jw2Pg ,TN=fw =2L(r)jw2Ng,
FP=fw =2L(r)jw2 Pg ,FN=fw2L(r)jw2Ng.
Intuitively, the higher the fvalue, the better the regex r.
Especially, the regex rwithfvalue of 1will accepts all
positive examples and rejects all negative examples.
Algorithm 2: SYNCORR
Input: positive examples P, negative examples N,
and an incorrect regex r0
Output: a regex
1forlmax = 2 to 0 do
2r preprocess (r0;lmax);
3 while thestop conditions not meet do
4N(r) apply the transformations on rin
parallel;
5r unpreprocess (r;lmax);
6N(r) unpreprocess (N(r);lmax);
7rm arg max
r02N(r)f(r0;P;N);
8 iff(rm;P;N)>f(r;P;N)then
9 iff(rm;P;N) == 1 then return rm;
10 elser preprocess (rm;lmax);
11 else break;
12returnr0;
The algorithm S YNCORR is shown in Algorithm 2. In order
to facilitate neighborhood search, S YNCORR sets the highest
level of rewriting rules lmax ranges from 2to0(line 1). First,
SYNCORR preprocesses the given regex r0with the given
highest level lmax and keeps the result regex in r(line2). Next,
it applies the transforms on rto get the neighborhood N(r)
and unpreprocess randN(r)(lines 4‚Äì6). Then S YNCORR
selects the regex with the maximum fvalue, denoted as rm,
from the neighborhoods of r(line 7). After that, it compares
1214thefvalues between the current regex rand the selected regex
rm(line 8). If the selected regex rmgets a higher fvalue,
then S YNCORR checks whether the fvalue equals to 1. If it
is, S YNCORR returnsrm(line 9). Otherwise, to start with the
next iteration, the regex rmis preprocessed and assigned to
r(line 10). If the selected regex rmdoes not get a higher f
value, thenrmmay be a local maximum, so S YNCORR fails
to repair regex r0and breaks the loop (line 11). For each lmax,
the processing above runs until the stop conditions meet (lines
3‚Äì11). Finally, S YNCORR returnsr0if S YNCORR fails for all
lmax values (line 12).
Rewriting rules. In order to preserve the integrity of the
small sub-regexes (probably the correct part) and reduce the
search space, we deÔ¨Åne some rewriting rules for the regexes
to abstract some small sub-regexes. The resulting regexes are
called the abstract forms of the original regexes. SpeciÔ¨Åcally,
we rewrite some special small regexes to unique symbolic
nodes, which are listed as follows:
[C]!0<C C>
fm;ng ! 0<Qm;n>
:r!1<SL r>
:r: ! 1<SLR r>
(r:)!2<NSR r>const!0<C const>
(r)!1<N r>
r: ! 1<SR r>
(:r)!2<NSL r>
(:r:)!2<NSLR r>
whereconst denotes a string (in regex) consisting of
characters in (e.g., ‚Äúabc‚Äù), ris[C]or aconst, and the
sufÔ¨Åx indicates the level lof the rule. The rewriting rules
are performed greedily and depending on its level. In our
implementation, we use some meaningful names for some
special regexes (e.g, use <NUM>, <LET>, <CAP>, and <VOW>
for[0-9], [A-Za-z], [A-Z], and [AEIOUaeiou], re-
spectively) and keeps the rewriting mappings in dictionaries
(i.e.,Dict lfor the level l).
Preprocess and unpreprocess. Given a regex rand the
highest level lmax, the preprocess is to abstract (i.e., from
left to right) raccording the rewriting rules whose level l
ranging from lmax to 0 in order. The rules with high level
are performed Ô¨Årst. Take r0in Fig. 4 as an example. If
lmax = 2, the rules with level l= 2 are performed Ô¨Årst,
but without changing r0because of no rules with level l= 2
are applicable. Then the rules with level l= 1 are applied,
yieldingr1=(<SRVOW><SRNUM>){7,}. Finally, the rules
with levell= 0 is applied, yielding the Ô¨Ånal preprocessed
regexr=(<SRVOW><SRNUM>)<Q 7;>. Iflmax = 1, the
same preprocessed regex ris returned. If lmax= 0, directly
apply the rewriting rules with level l= 0, yielding the Ô¨Å-
nal preprocessed regex (<VOW><S><NUM><S>)<Q 7;>. The
unpreprocess is the reversion of the preprocess.
Stop conditions. We can set the maximum number of
iterations, and the maximum running time of the program as
independent or mixed stop conditions.
Transformation. We observe that most of the incorrect
regexes generated by S2RE can be made equivalent to the
target regexes with only minor modiÔ¨Åcations. For that, we
design a series of transformations on regexes, which are listedbellow, where we use element s to denote the small regexes that
the rewriting rules can apply on, and generalized element s to
denote the regexes containing at least a pair of brackets.
Binary Element Insertion: this transformation inserts a
binary element (i.e., disjunction, concatenation, or con-
junction) from a candidate set into the current regex.
(Generalized) Element Deletion: this transformation
deletes a (generalized) element from the current regex.
(Generalized) Element Replacement: this transformation
replaces a (generalized) element in the current regex with
an element from a candidate set.
QuantiÔ¨Åer Insertion: this transformation inserts a quanti-
Ô¨Åer, whose minimum and maximum values are selected
according to the positive and negative examples, to a
(generalized) element in the current regex.
QuantiÔ¨Åer ModiÔ¨Åcation: this transformation modiÔ¨Åes a
quantiÔ¨Åer of a (generalized) element in the current regex,
where the minimum and maximum values are set accord-
ing to the positive and negative examples.
QuantiÔ¨Åer Adjustment: this transformation adjusts the
(generalized) element restricted by a quantiÔ¨Åer in the cur-
rent regex from its original restrict (generalized) element
to another (generalized) element.
Operator Insertion: this transformation inserts an oper-
ator (i.e., negation, disjunction, or conjunction) into the
current regex.
Operator Deletion: this transformation deletes an opera-
tor from the current regex.
Element Adjustment: this transformation adjusts an ele-
ment in the current regex from its original position to
another position.
(Generalized) Element Exchanging: this transformation
swaps two (generalized) elements in the current regex.
In our implementation, we take the elements collected in the
dictionaries as the candidate set. And to search the neighbour
fast, we perform the transformation in parallel, as there are no
data races between each transformation.
IV. E VALUATION
We implemented T RANS REGEX in Python, and conducted
experiments on a machine with 16 cores Intel Xeon CPU
E5620 @ 2.40GHz with 12MB Cache, 24GB RAM, running
Windows 10 operating system. Under this experiment settings,
we then designed our experiments to answer the following
research questions:
RQ1: Can S2REmodel generate correct and valid regexes
from natural language descriptions? (¬ßIV-D)
RQ2: Can S YNCORR repair incorrect regexes from ex-
amples? (¬ßIV-E)
RQ3: Can T RANS REGEX synthesize regexes accurately?
(¬ßIV-F)
RQ4: Can T RANS REGEX synthesize regexes efÔ¨Åciently?
(¬ßIV-G)
1215A. Datasets
In the experiment, we evaluate T RANS REGEX on three
public datasets: KB13 [10], NL-RX-Turk [11], and Structure-
dRegex [25]. Among them, KB13 consists of 824 pairs of
NL descriptions and the corresponding regexes constructed
by regex experts. NL-RX-Turk includes 10,000 pairs of NL
descriptions and regexes collected through crowdsourcing.
StructuredRegex comprises 3,520 long English descriptions
which are 2.9 to 4.0 times longer than the Ô¨Årst two datasets,
paired with complex regexes and associated 6 positive/6 nega-
tive examples using crowdsourcing. As our approach requires
examples which are absent in the Ô¨Årst two datasets, we adopt
the corresponding 10 positive and 10 negative examples for the
Ô¨Årst two datasets provided by Ye et al. [24]. SpeciÔ¨Åcally, the 10
positive examples are enumerated by randomly traversing the
deterministic Ô¨Ånite automaton (DFA) of the given regex (resp.
the 10 negative examples are synthesized by stochastically
traversing the DFA of the negation of the given regex).
B. Training Setting
We train S2RE on the three public datasets shown in
TABLE I. We adopt the same train/validation/test sets as those
used by previous works for the sake of comparison.
TABLE I
THETRAIN , VALIDATION ,AND TESTSETS IN THREE DATASETS .
Dataset Train Set Validation Set Test Set
KB13 618 206 206
NL-RX-T urk 6500 1000 2500
StructuredRe gex 2173 351 996
In detail, the encoder and the decoder of the S2REmodel
consist of two stacked BiLSTM layers, respectively. The
dimension size of the word embeddings is set to 128 and
the hidden size is 256. We train the S2REmodel with MLE
loss for 30, 10 and 15 epochs on KB13, NL-RX-Turk and
StructuredRegex, respectively. Then we further train the model
with policy gradient for 10, 30,10 epochs on KB13, NL-RX-
Turk and StructuredRegex, respectively. The hyper-parameters
andare set to 0.5 and 0.5.
C. Baselines
We compare three variants of T RANS REGEX (i.e., T RAN-
SREGEX (S2RE + S YNCORR), T RANS REGEX (S2RE +
RFIXER ), T RANS REGEX (S2RE + S YNCORR + RF IXER ))
with ten relevant works, including our NLP-based algorithm
S2RE. They are mainly fall into two paradigms. NLP-based
works only used NL descriptions to synthesize regexes, i.e.,
SEMANTIC -UNIFY [10], D EEP-REGEX (Locascio et al.) [11],
DEEP-REGEX (Ye et al.) [24], S EMREGEX [12], S OFT-
REGEX [13], and our S2RE. On the other hand, NLP-and-
example-based works took both NL and examples for regex
synthesis, including D EEP-REGEX (Ye et al.) + E XS[24],
GRAMMAR SKETCH + MLE [24], D EEPSKETCH + MLE [24]and D EEPSKETCH + MML [24]. Among them, S EMANTIC -
UNIFY learns a probabilistic grammar model to parse NL
descriptions into regexes. D EEP-REGEX (Locascio et al.),
DEEP-REGEX (Ye et al.), S EMREGEX , SOFTREGEX , and
ourS2RE are a series of algorithms and the main idea of
these algorithms is to translate NL into regexes based on
the seq2seq model. D EEP-REGEX (Ye et al.) + E XSis an
extension of D EEP-REGEX (Ye et al.), which takes examples
into account by simply Ô¨Åltering the k-best regexes based
on whether regexes consistent with the given examples. In
addition, G RAMMAR SKETCH + MLE, D EEPSKETCH + MLE,
and D EEPSKETCH + MML are a family of algorithms and
these algorithms Ô¨Årst use a grammar-based or neural semantic
parser to parse the NL into sketches, then search the regex
space deÔ¨Åned by the sketches and Ô¨Ånd a regex that is consistent
with the given examples.
For implementation, D EEP-REGEX (Locascio et al.) [11]
and S OFTREGEX [13] are open-source programs, so we are
able to reproduce the results of them. While other baselines
do not release their source code, so we excerpted the statistics
from their paper, and left blanks if they did not report it.
D. RQ1: Effectiveness of S2RE
To answer the Ô¨Årst question, we compare our algo-
rithm S2REwith Ô¨Åve state-of-the-art NLP-based algorithms.
TABLE II shows the evaluation results on accuracy of
SEMANTIC -UNIFY , D EEP-REGEX (Locascio et al.), D EEP-
REGEX (Ye et al.), S EMREGEX , SOFTREGEX , and our model
S2RE. On these three datasets, the accuracy of our S2RE
model is similar to or slightly better than S OFTREGEX , and
is always better than the other four NLP-based models (i.e.,
SEMANTIC -UNIFY , D EEP-REGEX (Locascio et al.), D EEP-
REGEX (Ye et al.), and S EMREGEX ).
Besides, the evaluation results on validity are summarized
in TABLE III. The validity of synthesized regexes is crucial. It
guarantees the quality of regex synthesis from NL . Further, in
our approach, it ensures that the regex synthesized from NL is
provided as a valid input to example-guided regex repair. The
results show that the validity of existing tools (e.g., D EEP-
REGEX (Locascio et al.), and S OFTREGEX ) is unsatisfactory
on the last dataset StructuredRegex which is much more
complex than the Ô¨Årst two. As we can see, less than half of
the regexes generated by D EEP-REGEX (Locascio et al.) are
valid and the most advanced NLP-based model S OFTREGEX
achieves 90.6%. By contrast, our model S2REachieves 100%
validity ratio, because it utilizes both the syntactic validity
reward and invalid2valid model.Summary to RQ1: S2REcan achieve similar or better accu-
racy than the state-of-the-art NLP-based models. Meanwhile,
S2REcan synthesize more valid regexes. The advantage of
high validty of S2REbecomes more obvious on complex
datatsets.
E. RQ2: Effectiveness of SynCorr
To answer the second question, we collected 45,930 and
712 incorrect regexes predicted by S2RE on KB13, NL-
1216TABLE II
THEDFA- EQUIVALENT ACCURACY ON THREE DATASETS .
Approach KB13 NL-RX-T urkStructur ed
Regex
SEMANTIC -UNIFY 65.5% 38.6% 1.8%
DEEP-REGEX (Locascio et al.) 65.6% 58.2% 23.6%
DEEP-REGEX (Ye et al.) 66.5% 60.2% 24.5%
SEMREGEX 78.2% 62.3% ‚Äî
SOFTREGEX 78.2% 62.8% 28.2%
S2RE 78.2% 62.8% 28.5%
DEEP-REGEX (Ye et al.) + E XS 77.7% 83.8% 37.2%
GRAMMAR SKETCH + MLE 68.9% 69.6% ‚Äî
DEEPSKETCH + MLE 84.0% 85.2% ‚Äî
DEEPSKETCH + MML 86.4% 84.8% ‚Äî
TRANSREGEX (S2RE+ S YNCORR) 92.7% 94.2% 63.3%
TRANSREGEX (S2RE+ RF IXER ) 90.3% 94.0% 53.1%
TRANSREGEX (S2RE+ S YNCORR + RF IXER ) 95.6% 98.6% 67.4%
TABLE III
THENUMBER OF VALID REGEXES GENERATED BY THE THREE
NLP- BASED MODELS ON THREE DATASETS .
Approach KB13 NL-RX-TurkStructured
Regex
DEEP-REGEX (Locascio et al.) 205 (99.5%) 2500 (100%) 494 (49.6%)
SOFTREGEX 204 (99.1%) 2500 (100%) 902 (90.6%)
S2RE 206 (100%) 2500 (100%) 996 (100%)
RX-Turk, and StructuredRegex, respectively. We compared
SYNCORR with the state-of-the-art tool RF IXER . TABLE IV
shows the number of successful repairs6. RF IXER success-
fully repaired 55.6%, 83.9%, and 34.4% on dataset KB13,
NL-RX-Turk, and StructuredRegex respectively. In contrast,
SYNCORR can repair 11.1%, 0.5%, and 14.2% more regexes
than RF IXER on the three datasets, respectively. In addition,
the repair success rate of S YNCORR + RF IXER is 11.1%
(22.2%), 11.8% (12.3%), and 5.8% (20.0%) higher than that
of S YNCORR (RF IXER ) alone on dataset KB13, NL-RX-Turk,
and StructuredRegex respectively.
We further demonstrate the advantages and disadvantages
of RF IXER and S YNCORR through a few cases in TABLE V.
As cases #1 and #2 shown in TABLE V, our algorithm
SYNCORR has the high generalization performance compared
with RF IXER . Case #3 in TABLE V illustrates that S YNCORR
can handle regexes with & well but RF IXER cannot. Similar
to case #4 in TABLE V, in some cases, S YNCORR will fall
into a local optimum, and RF IXER can solve them. Based
on the above analysis, we can conclude that S YNCORR and
RFIXER are complementary and simultaneously useful for our
TRANS REGEX .
Summary to RQ2: SYNCORR can more effectively repair
regexes compared with the state-of-the-art tool RF IXER . In
addition, S YNCORR and RF IXER are complementary and
simultaneously useful for our T RANS REGEX .
6The regex that is successfully repaired must not only consistent with the
given examples, but also be equal to the target regex.TABLE IV
THENUMBER OF SUCCESSFUL REPAIRS BY SYNCORR AND RFIXER ON
THREE DATASETS .
Approach KB13 NL-RX-T urkStructur ed
Regex
RFIXER 25/45 (55.6%) 780/930 (83.9%) 245/712 (34.4%)
SYNCORR 30/45 (66.7%) 785/930 (84.4%) 346/712 (48.6%)
RFIXER + S YNCORR 35/45 (77.8%) 895/930 (96.2%) 387/712 (54.4%)
F . RQ3: Effectiveness of TransRegex
To answer this question, we compared T RANS REGEX with
six NLP-based baselines and four multi-modal baselines. We
can see from TABLE II that on average, NLP-based works
performed worse than multi-modal works on all three datasets.
In general, the accuracy on the Ô¨Årst dataset KB13 is much
higher than that on the other two datasets for NLP-based meth-
ods, and the accuracy achieved by NLP-based methods are up
to 30% lower than that achieved by multi-modal methods on
average. Particularly, on the Ô¨Årst two datasets, the accuracy of
NLP-based works range from 38.6% to 78.2%, compared with
68.9% to 86.4% achieved by existing multi-modal works. On
comparison, T RANS REGEX achieved approximate 10% higher
accuracy than these baselines, reaching 90.3% to 98.6% on the
Ô¨Årst two datasets. The superiority of our work is more obvious
on the last dataset, which is much more complex than the Ô¨Årst
two. The accuracy achieved by T RANS REGEX (67.4%) almost
doubled the accuracy achieved by D EEP-REGEX (Ye et al.) +
EXS(37.2%).
Let us present a few example regexes that are synthe-
sized incorrectly from NL descriptions but are synthesized
correctly by our T RANS REGEX to illustrate the beneÔ¨Åts of
leveraging both NL and examples. As case #1 in TABLE VI,
the description is ambiguous, i.e., it is unclear what part
of the string would appear at least 3 times, resulting in
thatS2REpredicted a regex embedding other meanings. Our
TRANS REGEX utilizes examples (e.g., a positive example
AdogBdogCdog) to help disambiguate the description, and
Ô¨Åx the incorrect regex [A-Z]. *(dog){3,}. *to the cor-
rect regex ([A-Z]. *dog. *){3,}. Similar to cases #2,
#3, and #4 in TABLE VI, the errors that are caused by
imprecision/unknown words in descriptions or false prediction
by NLP-based approaches can be corrected by T RANS REGEX
through using the example-guided regex repairer.
Further, we analyze the possible reasons why the other
multi-modal works are not as effective as ours as follows. As
mentioned above, D EEP-REGEX (Ye et al.) + E XSsimply uses
examples to select the k-best regexes among the candidates
produced by D EEP-REGEX (Ye et al.). If there is no correct
regex in the candidates, the examples will not help. The three
variants algorithms (i.e., G RAMMAR SKETCH + MLE, D EEPS-
KETCH + MLE, and D EEPSKETCH + MML) rely heavily on
the quality of the sketches synthesized in their Ô¨Årst step. In
other words, the incorrection of sketches will be inherited by
the generated regex in the next step. While T RANS REGEX
does not have the above-mentioned drawbacks. In addition,
1217TABLE V
EXAMPLES OF REPAIR BY RFIXER AND SYNCORR.
No. Incorr ect Regex Ground Truth RFIXER SYNCORR
#1 ([AEIOUaeiou].*[0-9].*)f7,g [AEIOUaeiou].*[0-9]f7,g .* ([AEeiO01234U56789].*[0-9].*)f2,g 7 [AEIOUaeiou].*[0-9]f7,g.* 3
#2 [A-Za-z]f2,3g[a-z]f2,3g [A-Z]f3,4g [A-Za-z]f2,3g[a-z]f3g [A-Z]f3,4g ([AabBCDEeFGHIJLnXxy]f2,g[abcdeÔ¨Ç
npvwxy])+[a-z]f2,3g[A-Z]f3,4g 7[A-Za-z]f2,3g[a-z]f3,3g [A-Z]f3,4g 3
#3 ([A-Z]j[a-z])f1,g &.f6,8g&(.*([A-Z]j[a-z]).*) .f6,8g&(.*[A-Za-z].*) Not supporting regexes with & .f6,8g&(.*([A-Z]j[a-z]).*) 3
#4 [A-Za-z]f3,g[0-9]f3,gN [A-Za-z]f2,4g [A-Z]f3,g[0-9]f3,g(Njg) [A-Za-z]f2,4g [A-Z]f3,g[0-9]f3,g[gN] [a-zA-Z])f2,4g 3 Trapping in local optimum
TABLE VI
EXAMPLES ILLUSTRATING THE BENEFITS OF LEVERAGING BOTH NATURAL LANGUAGE AND EXAMPLES IN TRANS REGEX .
No. Description Ground Truth S2RE Success Type
#1 items with a capital letter preceding ‚Äúdog‚Äù at least 3 times ([A-Z].*dog.*)f3,g [A-Z].*(dog)f3,g .* Ambiguity of NL
#2 thestring should start with at least 1 or more capital i ,then it is
followed by 3 letters.lf1,g [A-Za-z]f3g [A-Z]f1,g [A-Za-z]f3g Imprecision of NL
#3 thestring must begin with 2 or more letter s. after this grouping, it is
a 3 digit number. after this 3 digit number, there a letter Z. after the
letter Z, the string must contain;or##.the string can end with an
optional string of 3 to 4 capital letters.sf2,g[0-9]f3gZ( ;j##) ([A-
Z]f3,4g)?sf2,g[0-9]f3gZ[0-9]f3g ([A-
Z]f3,4g)?Unkno wn or rare words
#4 alist of 3 comma separated strings of lowercase letters. [a-z]+ (,[a-z]+)(,[a-z]+) [a-z]+ (,[a-z]+)* False prediction by S2RE
although T RANS REGEX is also a two-step algorithm, the
second step of T RANS REGEX is not only not affected by the
errors of the Ô¨Årst step, but also speciÔ¨Åcally correct the errors
of the Ô¨Årst step guided by the given examples.
Summary to RQ3: TRANS REGEX can achieve higher ac-
curacy than the NLP-based works with 17.4%, 35.8% and
38.9%, and the state-of-the-art multi-modal works with 10%
to 30% higher accuracy on all three datasets. The experiment
results also indicate T RANS REGEX utilizing natural language
and examples in a more effective way than other multi-modal
works.
G. RQ4: EfÔ¨Åciency of TransRegex
TABLE VII
AVERAGE RUNNING TIME PER BENCHMARK ON THREE DATASETS .
Approach KB13 NL-RX-T urkStructur ed
Regex
DEEP-REGEX (Locascio et al.) 2.621 s 1.104 s 2.108 s
S2RE 3.578 s 1.656 s 3.313 s
TRANSREGEX (S2RE+ S YNCORR) 4.958 s 3.085 s 8.624 s
TRANSREGEX (S2RE+ RF IXER ) 5.821 s 4.737 s 22.460 s
TRANSREGEX (S2RE+ S YNCORR + RF IXER )6.688 s 4.011 s 13.737 s
To evaluate the efÔ¨Åciency of three variants of T RAN-
SREGEX , we compared the average running time of syn-
thesizing per regex with the open-source baseline D EEP-
REGEX (Locascio et al.) and our S2RE in Table VII. In
general, we can see that T RANS REGEX takes longer time to
synthesize regexes on the last dataset than on the Ô¨Årst two,
and NLP-based baselines take less than T RANS REGEX on
average. In particular, T RANS REGEX takes an average 5.822
seconds and 3.944 seconds on the Ô¨Årst two datasets, compared
with 1.104 to 3.578 seconds achieved by baselines. While
on the last dataset, T RANS REGEX takes longer time due tothe complexity of the dataset. Considering together with the
accuracy, T RANS REGEX achieved the accuracy (67.4%) that
doubles the accuracy (28.5%) achieved by S2RE, taking only
around 10 more seconds running time. Table VII also reveals
the rationality of our algorithms, the adopting of S YNCORR
helps us to accelerate the repair process in some cases, while
RFIXER takes care of the rest.
Summary to RQ4: TRANS REGEX can synthesize regex efÔ¨Å-
ciently. Especially when considering together with accuracy,
TRANS REGEX can takes an average 3.944 to 5.822 seconds
to achieve around 20% more accuracy on simpler datasets,
and takes 30% more accuracy at the cost of 10 more seconds
on the more complex dataset.
V. T HREATS TO TRANS REGEX ‚ÄôSVALIDITY
TRANS REGEX is not guaranteed to generate correct regexes
for each benchmark, mainly due to the following aspects:
Uncharacteristic examples. Although T RANS REGEX
has greatly reduced the amount of required examples, the
quality of T RANS REGEX still depends on characteristic
examples. When the examples provided by users are not
characteristic, it is difÔ¨Åcult for T RANS REGEX to get the
correct regexes. For instance, the positive examples from
case #1 in TABLE VIII belongs to both the incorrect
one[a-z]{3}[A-Za-z]{3,}[A-Za-z]{3,} and
ground truth [a-z]{3}[A-Za-z]{3,}, i.e., fail to
distinguish between the incorrect one and ground truth.
This may cause S YNCORR or RF IXER to fail to repair
the incorrect regex. If the user further provides examples
(e.g., a positive example aaaAAA) that can distinguish
the two ones, our methods can easily get the correct one.
1218TABLE VIII
EXAMPLES OF FAILED CASES GENERATED BY TRANS REGEX .
No. Description Positive Examples Negati ve Examples Ground Truth Predicted Results Failure Type
fsuhoRdKRUGrFIRj qmnVF S2RE [a-z]{3}[A-Za-z]{3,}
#13lower case letters followed by more iptHLAdpPnKUXPrWo qmnVa [a-z]{3}[A-Za-z]{3,} SYNCORR [a-z]{3}[A-Za-z]{3,}[A-Z Uncharacteristic
than 3 letters. a-z]{3,} examples
:: : :: : RFIXER [a-z]{3}[A-Za-z]{3,}
astring that consists of upper or lower ==;0#=#+k0-0 7 !;_ S2RE ([A-Za-z]|[-!@#$%ÀÜ& *()_.]
case letters, special characters (-!@# |0){1,}&.{4,}
#2$%ÀÜ& *()_.) orthe number 0 and sy=;-0M0=!0T0f!;E 7#&0-!7!Kx; ([A-Za-z]|[-!@#$%ÀÜ& *()_.] SYNCORR .{4,}&(Àú([-!@#$%ÀÜ& *()_.]Wrong exampleswhose length is 4 or more characters |0){1,}&.{4,} .*))
long. :: : :: : RFIXER Notsupporting regexes with &
alist of 3 semicolon separated strings, O0Culdvs;0ctycf;H mLNWmxydw;x9e5pdvf;A S2RE ([A-Za-z]|[0-9]{1,})&.{4}
theÔ¨Årst and second strings begin with ([A-Za-z]|[0-9]){4}[a-z] ;[a-z]{2,4};[A-Z]{1,}
#3any combination of 4 letters or digits, 80Oymq;2T2myvsz;DZG 5xSxe;xTC8jv;Q {2,4};([A-Za-z]|[0-9]){4} SYNCORR Time out! Very complex regexes
these strings end with 2 to 4 lower [a-z]{2,4};[A-Z]{1,} ordescriptions
case letters, the third part is any num-
ber of capital letters.:: : :: : RFIXER Notsupporting regexes with &
Wrong examples. Wrong examples provided by users,
resulting in that our algorithms are misled to synthesize
regexes in the wrong direction. More concretely, our
algorithms repair incorrect regexes in the wrong direction
and even Ô¨Åx the correct regexes produced by S2REinto
the incorrect one. As case #2 in TABLE VIII, if we only
use the NL description, we can get the accurate regex
([A-Za-z]|[-!@#$%ÀÜ& *()_.]|0){1,}&.{4,},
which is the same as the ground truth. However,
the user provides some wrong examples (e.g.,
a positive example ==;0#=#+k0-0), which
leads our algorithms to believe that the regex
([A-Za-z]|[-!@#$%ÀÜ& *()_.]|0){1,}&.{4,}
is not accurate, and then our algorithm uses the wrong
examples to Ô¨Åx the accurate regex into an incorrect one.
Very complex regexes or descriptions. Similar to case
#3 in TABLE VIII, if the NL description is very long and
complicated, or the target regex is very complex in terms
of length and tree-depth, the regex synthesized by S2RE
may be very different from the target one. This kind of
regex, which is very different from the target one and
required very large Ô¨Åxes, is difÔ¨Åcult for our algorithms to
repair into a correct one in a limited time and space.
VI. R ELATED WORK
A. Regex Synthesis
Regex synthesis from examples. The problem of automatic
regex synthesis from examples has been explored in many do-
mains [4], [16]‚Äì[20], [30], [31]. AlphaRegex [19] is a search-
based algorithm for synthesizing simple regexes for introduc-
tory automata assignments. AlphaRegex exploits over/under-
approximations to effectively prune out a large search space.
However, all the regexes produced by AlphaRegex are over
alphabets of size 2. RegexGenerator++ [4], [30] is a state-of-
the-art approach for the synthesis of regexes from positive and
negative examples. The fact that RegexGenerator++ utilizes
genetic programming means that it is not guaranteed to gener-
ate a correct solution‚Äìi.e., accepting all the positive examples
while rejecting all the negative examples. Lots of existingworks focus on XML schemas inference [16]‚Äì[18], [31], via
resorting to infer regexes from examples. These approaches
usually aim to tackle restricted forms of regexes from positive
examples only. Li et al. [20] presented a novel algorithm
FLASH REGEX to generate anti-ReDoS regexes from given
positive and negative examples by reducing the ambiguity of
these regexes and using SAT techniques.
However, one of the main issues in the above example-based
techniques is the quality of the synthesis, i.e., whether it would
generalize and correct for unseen examples. SpeciÔ¨Åcally, if
users can not provide sufÔ¨Åcient and characteristic examples,
the synthesized regexes will be under-Ô¨Åtting or over-Ô¨Åtting.
Regex synthesis from NL. Several works from the Natural
Language Processing (NLP) community address the prob-
lem of generating regexes from (NL) speciÔ¨Åcations [10]‚Äì
[13]. Kushman and Barzilay [10] introduced a technique for
learning a probabilistic combinatory categorial grammar model
to parse a NL description into a regex. To avoid domain-
speciÔ¨Åc feature extraction, Locascio et al. [11] described the
DEEP-REGEX model based on standard sequence-to-sequence
(seq2seq) model, which regards the problem of generating
regexes from NL descriptions as a direct machine translation
task. To solve the problem that D EEP-REGEX model may
not generate semantically correct regexes, the S EMREGEX
model [12] based on reinforcement learning method was pre-
sented. It leverages DFA equivalence as a reward function to
encourage the model to generate semantically correct regexes.
To speeds up the training phase of S EMREGEX model, Park
et al. [13] devised the S OFTREGEX model, which determines
the equivalence of two regexes using deep neural networks.
There are three major bottlenecks in existing NLP-based
techniques that affect the quality of synthesis: (i) Ambiguity
and imprecision of NL . Ambiguity of NL results in predicting
a regex embedding other meanings (resp. imprecision of NL
affects the correctness of synthesis); (ii) Unknown words and
rare words. There are unknown words or rare words in NL
descriptions, which will lead to failure to generate correct
regexes; (iii) Seq2Seq model. Seq2Seq-based approaches can
only synthesize regexes similar in shape to the training data.
1219Regex synthesis from NL and examples. Ye et al. [25]
proposed StructuredRegex, a new dataset for regex synthesis
from NL and examples. Ye et al. [24] introduced a baseline
model D EEP-REGEX + F ILTER , which uses D EEP-REGEX as
base model, and considers examples by simply Ô¨Åltering the
k-best regexes. However, the positive and negative examples
are not considered in the training and inference phase. The
latest two works [24], [25] presented new two-step frameworks
for regex synthesis from NL and examples. First, a semantic
parser converts the NL description into an intermediate sketch.
Then a synthesizer searches the regex space deÔ¨Åned by the
sketch and returns a concrete regex that is consistent with the
given examples. Although both adopting a two-step paradigm,
these works [24], [25] have an apparent limitation‚Äîincorrect
sketches generated in the Ô¨Årst step will subsequently induce the
Ô¨Ånal regexes. In other words, the incorrection of sketches will
be inherited by the synthesized regexes in the next step. On
the other hand, our work overcomes this limitation: the second
step of T RANS REGEX (i.e., example-guided regex repair ) Ô¨Åxes
the incorrect regex by examples if needed. In this manner, the
inherited inconsistencies in our Ô¨Årst step (i.e., NLP-based regex
synthesis) will be Ô¨Åxed in our second step.
B. Regex Repair
Regex repair from examples. There are several works [20],
[26]‚Äì[28] targeting at repairing regexes from examples. We
discuss two main paradigms of them. In the Ô¨Årst paradigm,
works only consider either positive or negative examples. Li
et al. [26] proposed ReLIE, which can modify complex regexes
by rejecting the newly-input negative examples. By contrast,
Rebele et al. [27] proposed a novel way to generalize a given
regex so that it accepts the given positive examples. On the
other hand, works in the second paradigm take both positive
and negative examples into consideration. Pan et al. [28]
designed RF IXER , a tool for repairing incorrect regexes using
both examples. It took advantage of skeletons of regexes
(i.e., sketches) to effectively prune out the search space, and
it employed SMT solvers to efÔ¨Åciently explore the sets of
possible character classes and numerical quantiÔ¨Åers. Our work
applies RF IXER in our regex synthesis from NL and examples.
Li et al. [20] described algorithm R EPAIRING RE based on
Neighborhood Search (NS) to repair incorrect or ReDos-
vulnerable regexes from positive and negative examples. Sim-
ilar to R EPAIRING RE, our algorithm S YNCORR also uses NS
to repair regexes, but the difference is that R EPAIRING RE uses
automaton-directed repair, while we use regex-directed repair.
Like the above example-based synthesis algorithms, these
repair algorithms also may cause under-Ô¨Åtting or over-Ô¨Åtting
results. To alleviate the problems of under-Ô¨Åtting/over-Ô¨Åtting,
our S YNCORR leverages some rewriting rules for sub-regexes
abstraction to preserve the integrity of some small sub-regexes.
C. Program Synthesis
Programming by example (PBE). PBE techniques have been
the subject of research in the past few decades [32] and suc-
cessful paradigms for program synthesis, allowing end-usersto construct and run new programs by providing examples of
the intended program behavior [33]. Recently, PBE techniques
have been successfully used for string transformations [34]‚Äì
[36], data Ô¨Åltering [14], data structure manipulations [37], [38],
table transformations [39], [40], SQL queries [41], [42], and
MapReduce programs [43], [44].
Programming by NL (PBNL). There has been a lot of
progress made in PBNL [45]. SpeciÔ¨Åcally, several tech-
niques have been proposed to translate NL descriptions into
Python [46], [47], SQL queries [48]‚Äì[50], shell scripts [51],
[52], spreadsheet formulas [53], test oracles [54], JavaScript
function types [55], and Java expressions [56].
Program synthesis from NL and examples. Sinece program
synthesis from NL and examples techniques can well over-
come the shortcomings of PBE and PBNL techniques, at the
same time, provide a more natural and friendly interface to
the users, recent years they have been widely used in several
areas, for example, string manipulation programs [23], [57],
and program sketches [58]. In this paper, we focus on an
important subtask of the program synthesis from NL and
examples problem: synthesizing regexes from both NL and
examples.
VII. C ONCLUSION
We propose an automatic framework T RANS REGEX , for
synthesizing regular expressions from both natural language
descriptions and examples. To the best of our knowledge,
TRANS REGEX is the Ô¨Årst to treat the NLP-and-example-
based regex synthesis problem as the problem of NLP-based
synthesis with regex repair. For NLP-based synthesis, we
devise a two-phase algorithm S2RE which generates more
valid regexes while having similar or higher accuracy than
the state-of-the-art NLP-based models. While for regex re-
pair, we present a novel algorithm S YNCORR that leverages
NS algorithms to guide the search for a target regex and
uses rewriting rules to alleviate under-Ô¨Åtting/over-Ô¨Åtting and
efÔ¨Åciently reduce the search space. The evaluation results
demonstrate that the accuracy of our T RANS REGEX is 17.4%,
35.8% and 38.9% higher than that of NLP-based works on
the three publicly available datasets, respectively. Further,
TRANS REGEX can achieve higher accuracy than the state-of-
the-art multi-modal works with 10% to 30% higher accuracy
on all three datasets. The evaluation results also indicate
TRANS REGEX utilizing natural language and examples in a
more effective way.
ACKNOWLEDGMENT
The authors would like to thank the anonymous reviewers
for their comments and suggestions. This work is supported in
part by National Natural Science Foundation of China (Grants
#61872339, #61472405, #61932021, #61972260, #61772347,
#61836005), National Key Research and Development Pro-
gram of China under Grant #2019YFE0198100, Guangdong
Basic and Applied Basic Research Foundation under Grant
#2019A1515011577, and Huawei PhD Fellowship, MSRA
Collaborative Research Grant.
1220REFERENCES
[1] L. G. M. IV , J. Donohue, J. C. Davis, D. Lee, and F. Servant, ‚ÄúRegexes
are Hard: Decision-Making, DifÔ¨Åculties, and Risks in Programming
Regular Expressions,‚Äù in 34th IEEE/ACM International Conference on
Automated Software Engineering, ASE 2019, San Diego, CA, USA,
November 11-15, 2019, 2019, pp. 415‚Äì426.
[2] Y . Shen, Y . Jiang, C. Xu, P. Yu, X. Ma, and J. Lu, ‚ÄúReScue: crafting
regular expression DoS attacks,‚Äù in Proceedings of the 33rd ACM/IEEE
International Conference on Automated Software Engineering, ASE
2018, Montpellier, France, September 3-7, 2018, 2018, pp. 225‚Äì235.
[3] C. Chapman, P. Wang, and K. T. Stolee, ‚ÄúExploring regular expression
comprehension,‚Äù in Proceedings of the 32nd IEEE/ACM International
Conference on Automated Software Engineering, ASE 2017, Urbana, IL,
USA, October 30 - November 03, 2017, 2017, pp. 405‚Äì416.
[4] A. Bartoli, A. D. Lorenzo, E. Medvet, and F. Tarlao, ‚ÄúInference of
Regular Expressions for Text Extraction from Examples,‚Äù IEEE Trans.
Knowl. Data Eng., vol. 28, no. 5, pp. 1217‚Äì1230, 2016.
[5] J. C. Davis, C. A. Coghlan, F. Servant, and D. Lee, ‚ÄúThe impact of
regular expression denial of service (ReDoS) in practice: an empirical
study at the ecosystem scale,‚Äù in Proceedings of the 2018 ACM Joint
Meeting on European Software Engineering Conference and Symposium
on the Foundations of Software Engineering, ESEC/SIGSOFT FSE 2018,
Lake Buena Vista, FL, USA, November 04-09, 2018, 2018, pp. 246‚Äì256.
[6] J. C. Davis, L. G. M. IV , C. A. Coghlan, F. Servant, and D. Lee, ‚ÄúWhy
aren‚Äôt regular expressions a lingua franca? an empirical study on the re-
use and portability of regular expressions,‚Äù in Proceedings of the ACM
Joint Meeting on European Software Engineering Conference and Sym-
posium on the Foundations of Software Engineering, ESEC/SIGSOFT
FSE 2019, Tallinn, Estonia, August 26-30, 2019, 2019, pp. 443‚Äì454.
[7] E. Spishak, W. Dietl, and M. D. Ernst, ‚ÄúA type system for regular
expressions,‚Äù in Proceedings of the 14th Workshop on Formal Techniques
for Java-like Programs, FTfJP 2012, Beijing, China, June 12, 2012,
2012, pp. 20‚Äì26.
[8] X. Liu, Y . Jiang, and D. Wu, ‚ÄúA Lightweight Framework for Regular
Expression VeriÔ¨Åcation,‚Äù in 19th IEEE International Symposium on High
Assurance Systems Engineering, HASE 2019, Hangzhou, China, January
3-5, 2019, 2019, pp. 1‚Äì8.
[9] B. Luo, Y . Feng, Z. Wang, S. Huang, R. Yan, and D. Zhao, ‚ÄúMarrying Up
Regular Expressions with Neural Networks: A Case Study for Spoken
Language Understanding,‚Äù in Proceedings of the 56th Annual Meeting
of the Association for Computational Linguistics, ACL 2018, Melbourne,
Australia, July 15-20, 2018, Volume 1: Long Papers, 2018, pp. 2083‚Äì
2093.
[10] N. Kushman and R. Barzilay, ‚ÄúUsing Semantic UniÔ¨Åcation to Generate
Regular Expressions from Natural Language,‚Äù in Human Language Tech-
nologies: Conference of the North American Chapter of the Association
of Computational Linguistics, Proceedings, June 9-14, 2013, Westin
Peachtree Plaza Hotel, Atlanta, Georgia, USA, 2013, pp. 826‚Äì836.
[11] N. Locascio, K. Narasimhan, E. DeLeon, N. Kushman, and R. Barzilay,
‚ÄúNeural Generation of Regular Expressions from Natural Language with
Minimal Domain Knowledge,‚Äù in Proceedings of the 2016 Conference
on Empirical Methods in Natural Language Processing, EMNLP 2016,
Austin, Texas, USA, November 1-4, 2016, 2016, pp. 1918‚Äì1923.
[12] Z. Zhong, J. Guo, W. Yang, J. Peng, T. Xie, J. Lou, T. Liu, and
D. Zhang, ‚ÄúSemRegex: A Semantics-Based Approach for Generating
Regular Expressions from Natural Language SpeciÔ¨Åcations,‚Äù in Proceed-
ings of the 2018 Conference on Empirical Methods in Natural Language
Processing, Brussels, Belgium, October 31 - November 4, 2018, 2018,
pp. 1608‚Äì1618.
[13] J. Park, S. Ko, M. Cognetta, and Y . Han, ‚ÄúSoftRegex: Generating Regex
from Natural Language Descriptions using Softened Regex Equiva-
lence,‚Äù in Proceedings of the 2019 Conference on Empirical Methods in
Natural Language Processing and the 9th International Joint Conference
on Natural Language Processing, EMNLP-IJCNLP 2019, Hong Kong,
China, November 3-7, 2019, 2019, pp. 6424‚Äì6430.
[14] X. Wang, S. Gulwani, and R. Singh, ‚ÄúFIDEX: Ô¨Åltering spreadsheet
data using examples,‚Äù in Proceedings of the 2016 ACM SIGPLAN
International Conference on Object-Oriented Programming, Systems,
Languages, and Applications, OOPSLA 2016, part of SPLASH 2016,
Amsterdam, The Netherlands, October 30 - November 4, 2016, 2016,
pp. 195‚Äì213.
[15] S. Gulwani, S. Jha, A. Tiwari, and R. Venkatesan, ‚ÄúSynthesis of loop-
free programs,‚Äù in Proceedings of the 32nd ACM SIGPLAN Conferenceon Programming Language Design and Implementation, PLDI 2011,
San Jose, CA, USA, June 4-8, 2011, 2011, pp. 62‚Äì73.
[16] G. J. Bex, F. Neven, T. Schwentick, and S. Vansummeren, ‚ÄúInference
of concise regular expressions and DTDs,‚Äù ACM Trans. Database Syst.,
vol. 35, no. 2, pp. 11:1‚Äì11:47, 2010.
[17] G. J. Bex, W. Gelade, F. Neven, and S. Vansummeren, ‚ÄúLearning
Deterministic Regular Expressions for the Inference of Schemas from
XML Data,‚Äù ACM Trans. Web, vol. 4, no. 4, pp. 14:1‚Äì14:32, 2010.
[18] D. D. Freydenberger and T. K ¬®otzing, ‚ÄúFast Learning of Restricted
Regular Expressions and DTDs,‚Äù Theory Comput. Syst., vol. 57, no. 4,
pp. 1114‚Äì1158, 2015.
[19] M. Lee, S. So, and H. Oh, ‚ÄúSynthesizing regular expressions from exam-
ples for introductory automata assignments,‚Äù in Proceedings of the 2016
ACM SIGPLAN International Conference on Generative Programming:
Concepts and Experiences, GPCE 2016, Amsterdam, The Netherlands,
October 31 - November 1, 2016, 2016, pp. 70‚Äì80.
[20] Y . Li, Z. Xu, J. Cao, H. Chen, T. Ge, S.-C. Cheung, and H. Zhao,
‚ÄúFlashRegex: deducing anti-ReDoS regexes from examples,‚Äù in 35th
IEEE/ACM International Conference on Automated Software Engineer-
ing, ASE 2020, Melbourne, Australia, September 21-25, 2020, 2020, p.
To appear.
[21] Q. Chen, X. Wang, X. Ye, G. Durrett, and I. Dillig, ‚ÄúMulti-modal
synthesis of regular expressions,‚Äù in Proceedings of the 41st ACM
SIGPLAN International Conference on Programming Language Design
and Implementation, PLDI 2020, London, UK, June 15-20, 2020, 2020,
pp. 487‚Äì502.
[22] Z. Zhong, J. Guo, W. Yang, T. Xie, J. Lou, T. Liu, and D. Zhang,
‚ÄúGenerating Regular Expressions from Natural Language SpeciÔ¨Åcations:
Are We There Yet?‚Äù in The Workshops of the The Thirty-Second AAAI
Conference on ArtiÔ¨Åcial Intelligence, New Orleans, Louisiana, USA,
February 2-7, 2018, 2018, pp. 791‚Äì794.
[23] M. H. Manshadi, D. Gildea, and J. F. Allen, ‚ÄúIntegrating Programming
by Example and Natural Language Programming,‚Äù in Proceedings of the
Twenty-Seventh AAAI Conference on ArtiÔ¨Åcial Intelligence, July 14-18,
2013, Bellevue, Washington, USA, 2013.
[24] X. Ye, Q. Chen, X. Wang, I. Dillig, and G. Durrett, ‚ÄúSketch-Driven
Regular Expression Generation from Natural Language and Examples,‚Äù
Trans. Assoc. Comput. Linguistics, vol. To appear, 2020.
[25] X. Ye, Q. Chen, I. Dillig, and G. Durrett, ‚ÄúBenchmarking Multimodal
Regex Synthesis with Complex Structures,‚Äù in Proceedings of the 58th
Annual Meeting of the Association for Computational Linguistics, ACL
2020, Online, July 5-10, 2020, 2020, pp. 6081‚Äì6094.
[26] Y . Li, R. Krishnamurthy, S. Raghavan, S. Vaithyanathan, and H. V .
Jagadish, ‚ÄúRegular Expression Learning for Information Extraction,‚Äù in
2008 Conference on Empirical Methods in Natural Language Process-
ing, EMNLP 2008, Proceedings of the Conference, 25-27 October 2008,
Honolulu, Hawaii, USA, A meeting of SIGDAT, a Special Interest Group
of the ACL, 2008, pp. 21‚Äì30.
[27] T. Rebele, K. Tzompanaki, and F. M. Suchanek, ‚ÄúAdding Missing Words
to Regular Expressions,‚Äù in Advances in Knowledge Discovery and Data
Mining - 22nd PaciÔ¨Åc-Asia Conference, PAKDD 2018, Melbourne, VIC,
Australia, June 3-6, 2018, Proceedings, Part II, 2018, pp. 67‚Äì79.
[28] R. Pan, Q. Hu, G. Xu, and L. D‚ÄôAntoni, ‚ÄúAutomatic repair of regular
expressions,‚Äù Proc. ACM Program. Lang., vol. 3, no. OOPSLA, pp.
139:1‚Äì139:29, 2019.
[29] R. J. Williams, ‚ÄúSimple Statistical Gradient-Following Algorithms for
Connectionist Reinforcement Learning,‚Äù Mach. Learn., vol. 8, pp. 229‚Äì
256, 1992.
[30] A. Bartoli, G. Davanzo, A. D. Lorenzo, E. Medvet, and E. Sorio,
‚ÄúAutomatic Synthesis of Regular Expressions from Examples,‚Äù IEEE
Computer, vol. 47, no. 12, pp. 72‚Äì80, 2014.
[31] Y . Li, X. Zhang, J. Cao, H. Chen, and C. Gao, ‚ÄúLearning k-Occurrence
Regular Expressions with Interleaving,‚Äù in Database Systems for Ad-
vanced Applications - 24th International Conference, DASFAA 2019,
Chiang Mai, Thailand, April 22-25, 2019, Proceedings, Part II, 2019,
pp. 70‚Äì85.
[32] D. E. Shaw, W. R. Swartout, and C. C. Green, ‚ÄúInferring LISP Programs
From Examples,‚Äù in Advance Papers of the Fourth International Joint
Conference on ArtiÔ¨Åcial Intelligence, Tbilisi, Georgia, USSR, September
3-8, 1975, 1975, pp. 260‚Äì267.
[33] K. Ellis and S. Gulwani, ‚ÄúLearning to Learn Programs from Examples:
Going Beyond Program Structure,‚Äù in Proceedings of the Twenty-Sixth
International Joint Conference on ArtiÔ¨Åcial Intelligence, IJCAI 2017,
Melbourne, Australia, August 19-25, 2017, 2017, pp. 1638‚Äì1645.REFERENCES
[1] L. G. M. IV , J. Donohue, J. C. Davis, D. Lee, and F. Servant, ‚ÄúRegexes
are Hard: Decision-Making, DifÔ¨Åculties, and Risks in Programming
Regular Expressions,‚Äù in 34th IEEE/ACM International Conference on
Automated Software Engineering, ASE 2019, San Diego, CA, USA,
November 11-15, 2019, 2019, pp. 415‚Äì426.
[2] Y . Shen, Y . Jiang, C. Xu, P. Yu, X. Ma, and J. Lu, ‚ÄúReScue: crafting
regular expression DoS attacks,‚Äù in Proceedings of the 33rd ACM/IEEE
International Conference on Automated Software Engineering, ASE
2018, Montpellier, France, September 3-7, 2018, 2018, pp. 225‚Äì235.
[3] C. Chapman, P. Wang, and K. T. Stolee, ‚ÄúExploring regular expression
comprehension,‚Äù in Proceedings of the 32nd IEEE/ACM International
Conference on Automated Software Engineering, ASE 2017, Urbana, IL,
USA, October 30 - November 03, 2017, 2017, pp. 405‚Äì416.
[4] A. Bartoli, A. D. Lorenzo, E. Medvet, and F. Tarlao, ‚ÄúInference of
Regular Expressions for Text Extraction from Examples,‚Äù IEEE Trans.
Knowl. Data Eng., vol. 28, no. 5, pp. 1217‚Äì1230, 2016.
[5] J. C. Davis, C. A. Coghlan, F. Servant, and D. Lee, ‚ÄúThe impact of
regular expression denial of service (ReDoS) in practice: an empirical
study at the ecosystem scale,‚Äù in Proceedings of the 2018 ACM Joint
Meeting on European Software Engineering Conference and Symposium
on the Foundations of Software Engineering, ESEC/SIGSOFT FSE 2018,
Lake Buena Vista, FL, USA, November 04-09, 2018, 2018, pp. 246‚Äì256.
[6] J. C. Davis, L. G. M. IV , C. A. Coghlan, F. Servant, and D. Lee, ‚ÄúWhy
aren‚Äôt regular expressions a lingua franca? an empirical study on the re-
use and portability of regular expressions,‚Äù in Proceedings of the ACM
Joint Meeting on European Software Engineering Conference and Sym-
posium on the Foundations of Software Engineering, ESEC/SIGSOFT
FSE 2019, Tallinn, Estonia, August 26-30, 2019, 2019, pp. 443‚Äì454.
[7] E. Spishak, W. Dietl, and M. D. Ernst, ‚ÄúA type system for regular
expressions,‚Äù in Proceedings of the 14th Workshop on Formal Techniques
for Java-like Programs, FTfJP 2012, Beijing, China, June 12, 2012,
2012, pp. 20‚Äì26.
[8] X. Liu, Y . Jiang, and D. Wu, ‚ÄúA Lightweight Framework for Regular
Expression VeriÔ¨Åcation,‚Äù in 19th IEEE International Symposium on High
Assurance Systems Engineering, HASE 2019, Hangzhou, China, January
3-5, 2019, 2019, pp. 1‚Äì8.
[9] B. Luo, Y . Feng, Z. Wang, S. Huang, R. Yan, and D. Zhao, ‚ÄúMarrying Up
Regular Expressions with Neural Networks: A Case Study for Spoken
Language Understanding,‚Äù in Proceedings of the 56th Annual Meeting
of the Association for Computational Linguistics, ACL 2018, Melbourne,
Australia, July 15-20, 2018, Volume 1: Long Papers, 2018, pp. 2083‚Äì
2093.
[10] N. Kushman and R. Barzilay, ‚ÄúUsing Semantic UniÔ¨Åcation to Generate
Regular Expressions from Natural Language,‚Äù in Human Language Tech-
nologies: Conference of the North American Chapter of the Association
of Computational Linguistics, Proceedings, June 9-14, 2013, Westin
Peachtree Plaza Hotel, Atlanta, Georgia, USA, 2013, pp. 826‚Äì836.
[11] N. Locascio, K. Narasimhan, E. DeLeon, N. Kushman, and R. Barzilay,
‚ÄúNeural Generation of Regular Expressions from Natural Language with
Minimal Domain Knowledge,‚Äù in Proceedings of the 2016 Conference
on Empirical Methods in Natural Language Processing, EMNLP 2016,
Austin, Texas, USA, November 1-4, 2016, 2016, pp. 1918‚Äì1923.
[12] Z. Zhong, J. Guo, W. Yang, J. Peng, T. Xie, J. Lou, T. Liu, and
D. Zhang, ‚ÄúSemRegex: A Semantics-Based Approach for Generating
Regular Expressions from Natural Language SpeciÔ¨Åcations,‚Äù in Proceed-
ings of the 2018 Conference on Empirical Methods in Natural Language
Processing, Brussels, Belgium, October 31 - November 4, 2018, 2018,
pp. 1608‚Äì1618.
[13] J. Park, S. Ko, M. Cognetta, and Y . Han, ‚ÄúSoftRegex: Generating Regex
from Natural Language Descriptions using Softened Regex Equiva-
lence,‚Äù in Proceedings of the 2019 Conference on Empirical Methods in
Natural Language Processing and the 9th International Joint Conference
on Natural Language Processing, EMNLP-IJCNLP 2019, Hong Kong,
China, November 3-7, 2019, 2019, pp. 6424‚Äì6430.
[14] X. Wang, S. Gulwani, and R. Singh, ‚ÄúFIDEX: Ô¨Åltering spreadsheet
data using examples,‚Äù in Proceedings of the 2016 ACM SIGPLAN
International Conference on Object-Oriented Programming, Systems,
Languages, and Applications, OOPSLA 2016, part of SPLASH 2016,
Amsterdam, The Netherlands, October 30 - November 4, 2016, 2016,
pp. 195‚Äì213.
[15] S. Gulwani, S. Jha, A. Tiwari, and R. Venkatesan, ‚ÄúSynthesis of loop-
free programs,‚Äù in Proceedings of the 32nd ACM SIGPLAN Conferenceon Programming Language Design and Implementation, PLDI 2011,
San Jose, CA, USA, June 4-8, 2011, 2011, pp. 62‚Äì73.
[16] G. J. Bex, F. Neven, T. Schwentick, and S. Vansummeren, ‚ÄúInference
of concise regular expressions and DTDs,‚Äù ACM Trans. Database Syst.,
vol. 35, no. 2, pp. 11:1‚Äì11:47, 2010.
[17] G. J. Bex, W. Gelade, F. Neven, and S. Vansummeren, ‚ÄúLearning
Deterministic Regular Expressions for the Inference of Schemas from
XML Data,‚Äù ACM Trans. Web, vol. 4, no. 4, pp. 14:1‚Äì14:32, 2010.
[18] D. D. Freydenberger and T. K ¬®otzing, ‚ÄúFast Learning of Restricted
Regular Expressions and DTDs,‚Äù Theory Comput. Syst., vol. 57, no. 4,
pp. 1114‚Äì1158, 2015.
[19] M. Lee, S. So, and H. Oh, ‚ÄúSynthesizing regular expressions from exam-
ples for introductory automata assignments,‚Äù in Proceedings of the 2016
ACM SIGPLAN International Conference on Generative Programming:
Concepts and Experiences, GPCE 2016, Amsterdam, The Netherlands,
October 31 - November 1, 2016, 2016, pp. 70‚Äì80.
[20] Y . Li, Z. Xu, J. Cao, H. Chen, T. Ge, S.-C. Cheung, and H. Zhao,
‚ÄúFlashRegex: deducing anti-ReDoS regexes from examples,‚Äù in 35th
IEEE/ACM International Conference on Automated Software Engineer-
ing, ASE 2020, Melbourne, Australia, September 21-25, 2020, 2020, p.
To appear.
[21] Q. Chen, X. Wang, X. Ye, G. Durrett, and I. Dillig, ‚ÄúMulti-modal
synthesis of regular expressions,‚Äù in Proceedings of the 41st ACM
SIGPLAN International Conference on Programming Language Design
and Implementation, PLDI 2020, London, UK, June 15-20, 2020, 2020,
pp. 487‚Äì502.
[22] Z. Zhong, J. Guo, W. Yang, T. Xie, J. Lou, T. Liu, and D. Zhang,
‚ÄúGenerating Regular Expressions from Natural Language SpeciÔ¨Åcations:
Are We There Yet?‚Äù in The Workshops of the The Thirty-Second AAAI
Conference on ArtiÔ¨Åcial Intelligence, New Orleans, Louisiana, USA,
February 2-7, 2018, 2018, pp. 791‚Äì794.
[23] M. H. Manshadi, D. Gildea, and J. F. Allen, ‚ÄúIntegrating Programming
by Example and Natural Language Programming,‚Äù in Proceedings of the
Twenty-Seventh AAAI Conference on ArtiÔ¨Åcial Intelligence, July 14-18,
2013, Bellevue, Washington, USA, 2013.
[24] X. Ye, Q. Chen, X. Wang, I. Dillig, and G. Durrett, ‚ÄúSketch-Driven
Regular Expression Generation from Natural Language and Examples,‚Äù
Trans. Assoc. Comput. Linguistics, vol. To appear, 2020.
[25] X. Ye, Q. Chen, I. Dillig, and G. Durrett, ‚ÄúBenchmarking Multimodal
Regex Synthesis with Complex Structures,‚Äù in Proceedings of the 58th
Annual Meeting of the Association for Computational Linguistics, ACL
2020, Online, July 5-10, 2020, 2020, pp. 6081‚Äì6094.
[26] Y . Li, R. Krishnamurthy, S. Raghavan, S. Vaithyanathan, and H. V .
Jagadish, ‚ÄúRegular Expression Learning for Information Extraction,‚Äù in
2008 Conference on Empirical Methods in Natural Language Process-
ing, EMNLP 2008, Proceedings of the Conference, 25-27 October 2008,
Honolulu, Hawaii, USA, A meeting of SIGDAT, a Special Interest Group
of the ACL, 2008, pp. 21‚Äì30.
[27] T. Rebele, K. Tzompanaki, and F. M. Suchanek, ‚ÄúAdding Missing Words
to Regular Expressions,‚Äù in Advances in Knowledge Discovery and Data
Mining - 22nd PaciÔ¨Åc-Asia Conference, PAKDD 2018, Melbourne, VIC,
Australia, June 3-6, 2018, Proceedings, Part II, 2018, pp. 67‚Äì79.
[28] R. Pan, Q. Hu, G. Xu, and L. D‚ÄôAntoni, ‚ÄúAutomatic repair of regular
expressions,‚Äù Proc. ACM Program. Lang., vol. 3, no. OOPSLA, pp.
139:1‚Äì139:29, 2019.
[29] R. J. Williams, ‚ÄúSimple Statistical Gradient-Following Algorithms for
Connectionist Reinforcement Learning,‚Äù Mach. Learn., vol. 8, pp. 229‚Äì
256, 1992.
[30] A. Bartoli, G. Davanzo, A. D. Lorenzo, E. Medvet, and E. Sorio,
‚ÄúAutomatic Synthesis of Regular Expressions from Examples,‚Äù IEEE
Computer, vol. 47, no. 12, pp. 72‚Äì80, 2014.
[31] Y . Li, X. Zhang, J. Cao, H. Chen, and C. Gao, ‚ÄúLearning k-Occurrence
Regular Expressions with Interleaving,‚Äù in Database Systems for Ad-
vanced Applications - 24th International Conference, DASFAA 2019,
Chiang Mai, Thailand, April 22-25, 2019, Proceedings, Part II, 2019,
pp. 70‚Äì85.
[32] D. E. Shaw, W. R. Swartout, and C. C. Green, ‚ÄúInferring LISP Programs
From Examples,‚Äù in Advance Papers of the Fourth International Joint
Conference on ArtiÔ¨Åcial Intelligence, Tbilisi, Georgia, USSR, September
3-8, 1975, 1975, pp. 260‚Äì267.
[33] K. Ellis and S. Gulwani, ‚ÄúLearning to Learn Programs from Examples:
Going Beyond Program Structure,‚Äù in Proceedings of the Twenty-Sixth
International Joint Conference on ArtiÔ¨Åcial Intelligence, IJCAI 2017,
Melbourne, Australia, August 19-25, 2017, 2017, pp. 1638‚Äì1645.
1221[34] S. Gulwani, ‚ÄúAutomating string processing in spreadsheets using input-
output examples,‚Äù in Proceedings of the 38th ACM SIGPLAN-SIGACT
Symposium on Principles of Programming Languages, POPL 2011,
Austin, TX, USA, January 26-28, 2011, 2011, pp. 317‚Äì330.
[35] R. Singh, ‚ÄúBlinkFill: Semi-supervised Programming By Example for
Syntactic String Transformations,‚Äù Proc. VLDB Endow., vol. 9, no. 10,
pp. 816‚Äì827, 2016.
[36] R. Singh and S. Gulwani, ‚ÄúLearning Semantic String Transformations
from Examples,‚Äù Proc. VLDB Endow., vol. 5, no. 8, pp. 740‚Äì751, 2012.
[37] N. Yaghmazadeh, C. Klinger, I. Dillig, and S. Chaudhuri, ‚ÄúSynthesizing
transformations on hierarchically structured data,‚Äù in Proceedings of the
37th ACM SIGPLAN Conference on Programming Language Design
and Implementation, PLDI 2016, Santa Barbara, CA, USA, June 13-17,
2016, 2016, pp. 508‚Äì521.
[38] J. K. Feser, S. Chaudhuri, and I. Dillig, ‚ÄúSynthesizing data structure
transformations from input-output examples,‚Äù in Proceedings of the 36th
ACM SIGPLAN Conference on Programming Language Design and
Implementation, Portland, OR, USA, June 15-17, 2015, 2015, pp. 229‚Äì
239.
[39] Y . Feng, R. Martins, J. V . Geffen, I. Dillig, and S. Chaudhuri,
‚ÄúComponent-based synthesis of table consolidation and transformation
tasks from examples,‚Äù in Proceedings of the 38th ACM SIGPLAN
Conference on Programming Language Design and Implementation,
PLDI 2017, Barcelona, Spain, June 18-23, 2017, 2017, pp. 422‚Äì436.
[40] W. R. Harris and S. Gulwani, ‚ÄúSpreadsheet table transformations from
examples,‚Äù in Proceedings of the 32nd ACM SIGPLAN Conference on
Programming Language Design and Implementation, PLDI 2011, San
Jose, CA, USA, June 4-8, 2011, 2011, pp. 317‚Äì328.
[41] C. Wang, A. Cheung, and R. Bod ¬¥ƒ±k, ‚ÄúSynthesizing highly expressive
SQL queries from input-output examples,‚Äù in Proceedings of the 38th
ACM SIGPLAN Conference on Programming Language Design and
Implementation, PLDI 2017, Barcelona, Spain, June 18-23, 2017, 2017,
pp. 452‚Äì466.
[42] S. Zhang and Y . Sun, ‚ÄúAutomatically synthesizing SQL queries from
input-output examples,‚Äù in 2013 28th IEEE/ACM International Confer-
ence on Automated Software Engineering, ASE 2013, Silicon Valley, CA,
USA, November 11-15, 2013, 2013, pp. 224‚Äì234.
[43] C. Smith and A. Albarghouthi, ‚ÄúMapReduce program synthesis,‚Äù in
Proceedings of the 37th ACM SIGPLAN Conference on Programming
Language Design and Implementation, PLDI 2016, Santa Barbara, CA,
USA, June 13-17, 2016, 2016, pp. 326‚Äì340.
[44] M. B. S. Ahmad and A. Cheung, ‚ÄúLeveraging Parallel Data Processing
Frameworks with VeriÔ¨Åed Lifting,‚Äù in Proceedings Fifth Workshop on
Synthesis, SYNT@CAV 2016, Toronto, Canada, July 17-18, 2016, 2016,
pp. 67‚Äì83.
[45] S. Gulwani, O. Polozov, and R. Singh, ‚ÄúProgram Synthesis,‚Äù Found.
Trends Program. Lang., vol. 4, no. 1-2, pp. 1‚Äì119, 2017.
[46] P. Yin and G. Neubig, ‚ÄúA Syntactic Neural Model for General-Purpose
Code Generation,‚Äù in Proceedings of the 55th Annual Meeting of
the Association for Computational Linguistics, ACL 2017, Vancouver,
Canada, July 30 - August 4, Volume 1: Long Papers, 2017, pp. 440‚Äì
450.
[47] M. Rabinovich, M. Stern, and D. Klein, ‚ÄúAbstract Syntax Networks
for Code Generation and Semantic Parsing,‚Äù in Proceedings of the 55th
Annual Meeting of the Association for Computational Linguistics, ACL
2017, Vancouver, Canada, July 30 - August 4, Volume 1: Long Papers,
2017, pp. 1139‚Äì1149.
[48] P. Huang, C. Wang, R. Singh, W. Yih, and X. He, ‚ÄúNatural Language
to Structured Query Generation via Meta-Learning,‚Äù in Proceedings of
the 2018 Conference of the North American Chapter of the Association
for Computational Linguistics: Human Language Technologies, NAACL-
HLT, New Orleans, Louisiana, USA, June 1-6, 2018, Volume 2 (Short
Papers), 2018, pp. 732‚Äì738.
[49] N. Yaghmazadeh, Y . Wang, I. Dillig, and T. Dillig, ‚ÄúSQLizer: query
synthesis from natural language,‚Äù Proc. ACM Program. Lang., vol. 1,
no. OOPSLA, pp. 63:1‚Äì63:26, 2017.
[50] Y . Zeng, Y . Gao, J. Guo, B. Chen, Q. Liu, J. Lou, F. Teng, and D. Zhang,
‚ÄúRECPARSER: A Recursive Semantic Parsing Framework for Text-
to-SQL Task,‚Äù in Proceedings of the Twenty-Ninth International Joint
Conference on ArtiÔ¨Åcial Intelligence, IJCAI 2020, 2020, pp. 3644‚Äì3650.
[51] H. Li, Y . Wang, J. Yin, and G. Tan, ‚ÄúSmartShell: Automated Shell
Scripts Synthesis from Natural Language,‚Äù Int. J. Softw. Eng. Knowl.
Eng., vol. 29, no. 2, pp. 197‚Äì220, 2019.[52] X. V . Lin, C. Wang, L. Zettlemoyer, and M. D. Ernst, ‚ÄúNL2Bash: A
Corpus and Semantic Parser for Natural Language Interface to the Linux
Operating System,‚Äù in Proceedings of the Eleventh International Con-
ference on Language Resources and Evaluation, LREC 2018, Miyazaki,
Japan, May 7-12, 2018, 2018.
[53] S. Gulwani and M. Marron, ‚ÄúNLyze: interactive programming by natural
language for spreadsheet data analysis and manipulation,‚Äù in Interna-
tional Conference on Management of Data, SIGMOD 2014, Snowbird,
UT, USA, June 22-27, 2014, 2014, pp. 803‚Äì814.
[54] M. Motwani and Y . Brun, ‚ÄúAutomatically generating precise Oracles
from structured natural language speciÔ¨Åcations,‚Äù in Proceedings of the
41st International Conference on Software Engineering, ICSE 2019,
Montreal, QC, Canada, May 25-31, 2019, 2019, pp. 188‚Äì199.
[55] R. S. Malik, J. Patra, and M. Pradel, ‚ÄúNL2Type: inferring JavaScript
function types from natural language information,‚Äù in Proceedings of
the 41st International Conference on Software Engineering, ICSE 2019,
Montreal, QC, Canada, May 25-31, 2019, 2019, pp. 304‚Äì315.
[56] T. Gvero and V . Kuncak, ‚ÄúSynthesizing Java expressions from free-
form queries,‚Äù in Proceedings of the 2015 ACM SIGPLAN International
Conference on Object-Oriented Programming, Systems, Languages, and
Applications, OOPSLA 2015, part of SPLASH 2015, Pittsburgh, PA,
USA, October 25-30, 2015, 2015, pp. 416‚Äì432.
[57] M. Raza, S. Gulwani, and N. Milic-Frayling, ‚ÄúCompositional Program
Synthesis from Natural Language and Examples,‚Äù in Proceedings of the
Twenty-Fourth International Joint Conference on ArtiÔ¨Åcial Intelligence,
IJCAI 2015, Buenos Aires, Argentina, July 25-31, 2015, 2015, pp. 792‚Äì
800.
[58] M. I. Nye, L. B. Hewitt, J. B. Tenenbaum, and A. Solar-Lezama,
‚ÄúLearning to Infer Program Sketches,‚Äù in Proceedings of the 36th
International Conference on Machine Learning, ICML 2019, 9-15 June
2019, Long Beach, California, USA, 2019, pp. 4861‚Äì4870.
1222