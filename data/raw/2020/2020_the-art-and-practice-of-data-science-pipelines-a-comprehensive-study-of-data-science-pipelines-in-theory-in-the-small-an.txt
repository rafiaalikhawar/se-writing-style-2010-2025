The Art and Practice of Data Science Pipelines
A Comprehensive Study of Data Science Pipelines In Theory, In-The-Small, and In-The-Large
Sumon Biswas
Iowa State University
Ames, IA, USA
sumon@iastate.eduMohammad Wardat
Iowa State University
Ames, IA, USA
wardat@iastate.eduHridesh Rajan
Iowa State University
Ames, IA, USA
hridesh@iastate.edu
ABSTRACT
Increasinglylargernumberofsoftwaresystemstodayareincluding
datasciencecomponentsfordescriptive,predictive,andprescriptive
analytics. The collection of data science stages from acquisition, to
cleaning/curation, to modeling, and so on are referred to as data
science pipelines. To facilitate research and practice on data science
pipelines, it is essential to understand their nature. What are the
typical stages of a data science pipeline? How are they connected?
Dothepipelinesdifferinthetheoreticalrepresentationsandthatin
the practice? Today we do not fully understand these architecturalcharacteristics of data science pipelines. In this work, we present a
three-pronged comprehensive study to answer this for the state-
of-the-art, datascience in-the-small, and datascience in-the-large.
Ourstudyanalyzesthreedatasets:acollectionof71proposalsfor
datasciencepipelinesandrelatedconceptsin theory,acollection
of over 105implementations of curated data science pipelines from
Kagglecompetitionstounderstanddatascience in-the-small,and
a collection of 21 mature data science projects from GitHub tounderstand data science in-the-large. Our study has led to three
representationsofdatasciencepipelinesthatcapturetheessence
of our subjects in theory, in-the-small, and in-the-large.
CCSCONCEPTS
‚Ä¢Softwareanditsengineering ‚ÜíSoftwarecreationandman-
agement;‚Ä¢Computingmethodologies ‚ÜíMachinelearning.
KEYWORDS
data science pipelines, data science processes, descriptive, predictive
ACM Reference Format:
Sumon Biswas, Mohammad Wardat, and Hridesh Rajan. 2022. The Art
and Practice of Data Science Pipelines: A Comprehensive Study of Data 
Science Pipelines In Theory, In-The-Small, and In-The-Large. In 44th Inter-
national Conference on Software Engineering (ICSE ‚Äô22), May 21‚Äì29, 2022,
Pittsburgh, PA, USA. ACM, New York, NY, USA, 13 pages. https://doi.org/10.
1145/3510003.3510057
1 INTRODUCTION
Data science processes, also called data science stages as in stages of
a pipeline, for descriptive, predictive, and prescriptive analytics are
becoming integral components of many software systems today.
This work is licensed under a Creative Commons Attribution International 4.0 
License.
ICSE ‚Äô22, May 21‚Äì29, 2022, Pittsburgh, PA, USA
¬© 2022 Copyright held by the owner/author(s).
ACM ISBN 978-1-4503-9221-1/22/05.
https://doi.org/10.1145/3510003.3510057Thedatasciencestagesareorganizedintoa datasciencepipeline,
wheredatamightflowfromonestageinthepipelinetothenext.
These data science stages generally perform different tasks such
asdataacquisition,data preparation,storage,featureengineering,
modeling,training, evaluationof themachinelearning model,etc.
In order to design and build software systems with data science
stages effectively, we must understand the structure of the data
sciencepipelines.Previousworkhasshownthatunderstandingthe
structureandpatternsusedinexistingsystemsandliteraturecan
help build better systems [ 23,78]. In this work, we have taken the
first stepto understand the structure and patterns of DS pipelines.
Fortunately, we have a number of instances in both the state-of-
the-artandpracticetodrawobservations.Intheliterature,there
havebeenanumberofproposalstoorganizedatasciencepipelines.
We call such proposals DS Pipelines in theory. Another source of
information is Kaggle, a widely known platform for data scientists
tohostandparticipateinDScompetitions,sharedatasets,machine
learning models, and code. Kaggle contains a large number of data
sciencepipelines,butthesepipelinesaretypicallydevelopedbya
single data scientist as small standalone programs. We call such
instances DSPipelinesin-the-small.ThethirdsourceofDSpipelines
are mature data science projects on GitHub developed by teams,
suitablefor reuse. We call such instances DS Pipelines in-the-large.
This work presents a study of DS pipelines in theory, in-the-
small, and in-the-large. We studied 71 different proposals for DS
pipelines and related concepts from the literature. We also studied
105instancesofDSpipelinesfromKaggle.Finally,westudied21
matured open-source data science projects from GitHub. For both
KaggleandGitHub,weselectedprojectsthatmakeuseofPythontoeasecomparativeanalysis.Ineachsetting,weanswerthefollowing
overarching questions.
(1)Representative pipeline: What are the stages in DS pipeline
and how frequently they appear?
(2)Organization: How are the pipeline stages organized?
(3)Characteristics: What are the characteristics of the pipelines
in a setting and how does that compare with the others?
Thisworkattemptstoinformtheterminologyandpracticefor
designing DS pipeline. We found that DS pipelines differ signifi-
cantlyintermsofdetailedstructuresandpatternsamongtheory,
in-the-small,andin-the-large.Specifically,anumberofstagesare
absent in-the-small, and the pipelines have a more linear structure
with an emphasis on data exploration. Out of the eleven stages
seen in theory, only six stages are present in pipeline in-the-small,
namelydata collection, data preparation, modeling, training,eval-
uation, and prediction. In addition, pipelines in-the-small do not
haveclearseparationbetweenstageswhichmakesthemaintenance
harder. On the other hand, the DS pipelines in-the-large have a
20912022 IEEE/ACM 44th International Conference on Software Engineering (ICSE)
ICSE‚Äô22,May21‚Äì29,2022,Pittsburgh,PA, USA Sumon Biswas, Mohammad Wardat, and Hridesh Rajan
more complex structure with feedback loops and sub-pipelines.
We identified different pipeline patterns followed in specific phase
(development/post-development) of the large DS projects. The ab-
stractionofstagesarestricterin-the-largehavingbothloosely-and
tightly-coupled structure.
Our investigation also suggest that DS pipeline is a well used
software architecture but often built in ad hoc manner. We demon-
strated the importance of standardization and analysis framework
forDSpipelinefollowingthetraditionalsoftwareengineeringre-
search on software architecture and design patterns [ 48,61,78].
WecontributedthreerepresentationsofDSpipelinesthatcapture
the essence of our subjects in theory, in-the-small, and in-the-large
thatwouldfacilitatebuildingnewDSsystems.Weanticipateour
results to inform design decisions made by the pipeline architects,
practitioners,andsoftwareengineeringteams.Ourresultswillalso
help the DS researchers and developers to identify whether the
pipeline is missing any important stage or feedback loops (e.g.,
storageandevaluation are missed in many pipelines).
The restof thispaper isorganized asfollows: insection ¬ß2,we
present our study of DS pipelines in theory. Section ¬ß3 describes
ourstudyofDSpipelinesin-the-small.Insection¬ß4,wedescribe
our study of DS pipelines in-the-large. Section ¬ß5 discusses the
implications,section¬ß6describesthethreatstothevalidity,section
¬ß7 describes related work, and section ¬ß8 concludes.
2 DS PIPELINE IN THEORY
DataScience .DataScience(DS)isabroadareathatbringstogether
computational understanding, inferential thinking, and the knowl-
edge of the application area. Wing [ 91] argues that DS studies how
to extract value out of data. However, the value of data and extrac-
tion process depends on the application and context. DS includes a
broadsetoftraditionaldisciplinessuchasdatamanagement,data
infrastructurebuilding,data-intensivealgorithmdevelopment,AI
(machine learning and deep learning), etc., that covers both the
fundamental and practical perspectives from computer science,
mathematics, statistics, and domain-specific knowledge [ 9,81]. DS
also incorporates the business, organization, policy and privacy
issues of data and data-related processes. Any DS project involves
three main stages: data collection and preparation, analysis and
modeling, and finally deployment [ 90]. DS is also more than statis-
tics or data mining since it incorporates understanding of data and
itspattern,developingimportantquestionsandansweringthem,
and communicatingresults [81].
Data Science Pipeline . The term pipelinewas introduced by
Garlanwith box-and-line diagramsandexplanatoryprosethatassist
softwaredeveloperstodesignanddescribecomplexsystemssothat
the software becomes intelligible [ 26]. Shaw and Garlan have pro-
videdthe pipes-and-filter designpatternthatinvolvesstageswith
processingunits(filters)andorderedconnections(pipes)[ 78].They
also argued that pipeline gives proper semantics and vocabulary
whichhelpstodescribetheconcerns,constraints,relationshipbe-
tweenthesub-systems,andoverallcomputationalparadigm[ 26,78].
Bydatasciencepipeline (DSpipeline),wearereferringtoaseriesof
processing stages that interact with data, usually acquisition, man-
agement,analysis,andreasoning[ 54,56].ThesequentialDSstages
from acquisition, to cleaning/curation, to modeling, and so on arereferredtoas datasciencepipeline .ADSpipelinemayconsistofsev-
eral stages and connections between them. The stages are defined
to perform particular tasks and connected to other stage(s) with
input-outputrelations[ 4].However,thedefinitionsofthestages
are not consistent across studies in the literature. The terminology
vary depending on the application context and focus.
Different study in the literature presented DS pipeline based
on their context and desiderata. No study has been conducted to
unify the notions DS pipeline and collect the concepts [ 76]. While
designinganew DSpipeline [ 92], dividingroles inDS teams[ 46],
definingsoftwareprocessindata-intensivesetting[ 86],identifying
best practices in AI and modularizing DS components [ 4], it is
important to understand the current state of the DS pipeline, its
variations and different stages. To understand the DS pipelines
and compare them, we collected the available pipelines from the
literatureandconductedanempiricalstudytounifythestageswith
theirsubtasks.Then we created a representative DS pipeline with
thedefinitionsofthestages.Next,wepresentthemethodologyand
results of our analysis of DS pipelines in theory.
2.1 Methodology
2.1.1 Collecting Data Science Pipelines. We searched for the stud-
iespublished intheliteratureand popularpressthat describesDS
pipelines.Weconsideredthestudiesthatdescribedbothend-to-end
DS pipeline or a partial DS pipeline specific to a context. First, we
searched for peer-reviewed papers published in the last decade i.e.,
from 2010 to 2020. We searched the terms ‚Äú data science pipeline ‚Äù,
‚Äúmachinelearningpipeline ‚Äù, ‚Äúbigdatalifecycle ‚Äù, ‚Äúdeeplearningwork-
flow‚Äù,andthepermutationofthesekeywordsinIEEEXplore,ACM
Digital Library and Google Scholar. From a large pool, we selected
1,566 papers that fall broadly in the area of computer science, soft-
ware engineering and data science. Then we analyzed each article
inthispooltoselecttheonesthatproposeordescribeaDSpipeline.
We found many papers in this collection use the terms (e.g., ML
lifecycle),butdonotcontainaDSpipeline.Weselectedtheones
thatcontainDSpipelineandextractedthepipelines(screenshot/de-
scription) as evidence from the article. The extracted raw pipelines
are available in the artifact accompanied by this paper [ 5]. Thus,
we found 46 DS pipelines that were published in the last decade.
Besides peer-reviewed papers, by searching the keywords on
web, we collectedthe DS pipelines from USpatent, industry blogs
(e.g.,Microsoft,GoogleCloud,IBMblogs),andpopularpresspub-
lished between 2010 and 2020. After manual inspection, we found
25 DS pipelines from this grey literature. Thus, we collected 71
subjects (46 from peer reviewed articles and 25 from grey litera-
ture) that contain DS pipeline. We used an open-coding method to
analyzethese DS pipelines in theory [5] .
2.1.2 Labeling Data Science Pipelines. Inthecollectedreferences,
DS pipeline is defined with a set of stages ( data acquisition ,data
preparation ,modeling, etc.) and connections among them. Each
stageinthepipelineisdefinedforperformingaspecifictaskand
connectedtootherstages.However,notallthestudiesdepictDS
pipelines with the same set of stages and connections. The studies
usedifferentterminologiesfordefiningthestagesdependingonthe
context. To be able to compare the pipelines, we had to understand
the definitions and transform them into a canonical form. For a
2092TheArtandPracticeof Data Science Pipelines ICSE‚Äô22,May21‚Äì29,2022,Pittsburgh,PA, USA
EndTraining Independent Labeling
Train the raters
Label one subject
Discussion meeting
Reconciling
disagreements
Finished
trainingLabel next pipeline
Perfect
agreement
Independent
re-labelingNo
YesFinishedYes
No
Reconcile all
disagreements ¬¨No Yes
Figure 1: Labeling method for DS pipelines in theory
given DS pipeline, identifying their stages and mapping them to a
canonical form is often challenging. The sub-tasks, overall goal of
the project, utilities affect the understanding of the pipeline stages.
To counter these challenges, we used an open-coding method to
label the stages of the pipelines.
Two authors labeled the collected DS pipelines into different
criteria. Each author read the article, understood the pipeline, iden-
tified the stages, and labeled them. In each iteration, the raters
labeled 10% of the subjects (7-8 pipelines). The first 8 subjects were
usedfortrainingandformingtheinitiallabels.Aftereachiteration,
wecalculatedtheCohen‚ÄôsKappacoefficient[ 84],identifiedthemis-
matches,andresolvedtheminthepresenceofamoderator,who
isanotherauthor.Thus,wefoundtherepresentativeDSpipeline
afterrigorousdiscussionsamongtheratersandthemoderator.The
methodologyofthisopen-codingprocedureisshowninFigure1.
Theentirelabelingprocesswasdividedintotwophases:1)training,
and 2) independent labeling.
Training: Thetworatersweretrainedonthegoalofthisproject
andtheirroles.Werandomlyselectedeightsubjectsfortraining.
First, the raters and the moderator had discussions on three sub-
jectsandidentifiedthestagesintheirDSpipeline.Thus,weformed
the commonlyoccurred stages and their definitions, whichwere
updatedthroughtheentirelabelingandreconciliationprocesslater.
Aftertheinitialdiscussionandtraining,theratersweregiven the
already created definitions of the stages and one pipeline from the
remainingfivefortraining.Theraterslabeledthispipelineindepen-
dently.Afterlabelingthepipeline,wecalculatedtheagreementand
conductedadiscussionsessionamongtheratersandthemoderator.
Inthissession,wereconciledthedisagreementsandupdatedthe
labels with the definitions. We continued the training session until
wegotperfectagreementindependently.Theinter-rateragreement
wascalculatedusingCohen‚ÄôsKappacoefficient[ 84].Ahigher Œ∫([0,
1])indicatesabetteragreement.Theinterpretationofof Œ∫isshown
in Figure 2a. In the discussion meetings, the raters discussed each
label (both agreed and disagreed ones) with the other rater and
moderator,arguedforthedisagreedonesandreconciledthem.In
this way, we came up with most of the stages and a representative
terminologyforeach stage includingthe sub-tasks.
Independentlabeling: Aftercompletingthetrainingsession,
the rest of the subjects were labeled independently by the raters.
The raters labeled the remaining 63 labels: 7 subjects (10%) in each
of the 9 iterations. The distribution of Œ∫after each independentRange( Œ∫)Agreement level
0.00- 0.20 Slightagreement
0.21- 0.40 Fairagreement
0.41- 0.60 Moderate agreement
0.61- 0.80 Substantialagreement
0.81- 1.00 Perfect agreement
(a) Interpretation of Kappa ( Œ∫)Iteration# Œ∫Iteration# Œ∫
1 0.676 0.91
2 0.747 0.87
3 0.828 0.90
4 0.849 0.94
5 0.8410 0.91
(b) Agreement in different stages
Figure 2: Labeling agreement calculation
labeling iteration is shown in Figure 2b. In each iteration, first, the
raters hadthe labeling session,and then the raters and moderator
had the reconciliation session.
Labeling. The raters labeled separately so that their labels were
private,andtheydidnotdiscusswhilelabeling.Theratersidenti-
fied the stages and connections between them, and finally labeled
whether the DS pipeline involves processes related to cyber, physi-
calorhumancomponentinit.Inindependentlabeling,wefound
almost perfect agreement ( Œ∫= 0.83) on average. Even after high
agreement, there were very few disagreements in the labels, which
were reconciled after each iteration.
Reconciling. Reconciliation happened for each label for the sub-
ject studies in the training session, and the disagreed labels for the
studies in independent labeling session. In training session, the
reconciliationwasdoneindiscussionmeetingsamongtheraters
and the moderator, whereas for the independent labels, reconcilia-
tionwasdonebythemoderatorafterseparatemeetingswiththe
two raters. For reconciliation, the raters described their arguments
for the mislabeled stages. Fora few cases, we had straightforward
solution to go for one label. For others, both the raters had good
arguments for their labels, and we had to decide on that label by
updating the stages in the definition of the pipeline. All the labeled
pipelinesfrom the subjects are shared in our paper artifact [5].
Furthermore, after finishing labeling the pipelines stages, we
also classified the subject references into four classes based on the
overall purpose of the article. First, after a few discussions, the
raters and moderator came up with the classes. Then, the raters
classifiedeachpipelineintooneclass.Wefounddisagreementsin6
out of 71 references, which the moderator reconciled with separate
meetings with the two raters. Based on our labeling, the literature
that we collected are divided into four classes: describe or propose
DS pipeline, survey or review, DS optimization, and introduce new
methodorapplication.Next,wearegoingtodiscusstheresultof
analyzingthe DS pipelines in theory.
2.2 Representative Pipeline in Theory
The labeled pipelines1with their stages are visually illustrated
in the artifact [ 5]. We found that pipelines in theory can be both
software architecture and team processes unlike pipelines in-the-
small and in-the-large. Through the labeling process, we separated
those team processes (25 out of 71), which are discussed in ¬ß2.4.
RQ1a: What is a representative definition of the DS pipe-
line in theory? Fromtheempiricalstudy,wecreatedarepresen-
tative rendition of DS pipeline with 3 layers, 11 stages and possible
connections between stages as shown in Figure 3. Each shaded box
represents a DS stage that performs certain sub-tasks (listed under
the box). In the preprocessing layer, the stages are data acquisition ,
preparation , andstorage. The preprocessing stage study design only
1https://github.com/sumonbis/DS-Pipeline/blob/main/pipelines.pdf
2093ICSE‚Äô22,May21‚Äì29,2022,Pittsburgh,PA, USA Sumon Biswas, Mohammad Wardat, and Hridesh Rajan
'DWD
SUHSDUDWLRQ)HDWXUH
HQJLQHHULQJ&RPPXQLFDWLRQ ,QWHUSUHWDWLRQ3RVWSURFHVVLQJ/D\HU 0RGHO%XLOGLQJ/D\HU
'DWD
DFTXLVLWLRQ6WRUDJH 0RGHOLQJ 7UDLQLQJ (YDOXDWLRQ 3UHGLFWLRQT
/RDG
&ROOHFW
2EWDLQ
&DSWXUH
6XUYH\SS
([SORUH
:UDQJOH
&OHDQ
)LOWHU
2UJDQL]H3UHVHUYH
$UFKLYH
:DUHKRXVH
/RJ
5HF\FOHJJ
)HDWXUH
VHOHFW
FRQVWUXFW
/DEHO
$QQRWDWH&ODVVLI\
&OXVWHU
0LQH
$QDO\]H
3URFHVV7XQH
2SWLPL]H9DOLGDWH
7HVW
9HULI\
5HYLHZ'LVFRYHU
'HULYH
'HWHUPLQH7UDQVIRUP
9LVXDOL]H
5HQGHU
7UDQVODWH
([SODLQ7UDQVIHU
6KDUH
'LVWULEXWH
7UDQVPLW
3XEOLVK,QVWDOO
([SORLW
6HUYH
0RQLWRU'LVFRYHU
'HULYH
'HWHUPLQH'HSOR\PHQW6WXG\GHVLJQ LJQ3UHSURFHVVLQJ/D\HU
Figure3:Conceptsinadatasciencepipeline.Thesub-tasksarelistedbeloweachstage.Thestagesareconnectedwithfeedback
loops denoted with arrows. Solid arrows are always present in the lifecycle, while the dashed arrows are optional. Distant
feedback loops (e.g., from deployment to data acquisition ) are also possible through intermediate stage(s).
Stagesof Data Science Pipeline
Data Acquisition (ACQ): In the beginning of DS pipeline, data are collected
fromappropriatesources.Datacanbeacquiredmanuallyorautomatically.Data
acquisition also involves understanding the nature of the data, collecting relevant
data,andintegratingavailabledatasets.
DataPreparation(PRP): Data are generally acquired in a raw format that needs
certainpreprocessingsteps.Thisinvolvesexplorationandfiltering,whichhelps
identify the correct data for further processing. Well prepared data reduces the
timerequired for data analysis and contributes to the success of the DS pipeline.
Storage(STR): It is important to find an appropriate hardware-software combina-
tion to preserve data so that it can be processed efficiently. For example, Miao et al .
usedgraphdatabasesystemNeo4j[ 52]tobuildacollaborativeanalysispipeline
[49],sinceNeo4J supports querying graph data properties.
Feature Engineering (FTR): Theentiredatasetmightnotcontributeequallyto
decision making. In this stage, appropriate features that are useful to build the
modelareidentifiedorconstructed.Featuresthatarenotreadilyavailableinthe
dataset,require engineering to create them from raw data.
Modeling(MDL): Whendataarepreprocessedandfeaturesareextracted,amodel
isbuilttoanalyzethedata.Modelbuildingincludesmodelplanning,modelselection,
mining and deriving important properties of data. Appropriate data processing
strategiesandalgorithmsare selected to create a good model.
Training (TRN): Foraspecificmodel,weneedtotrainthemodelwithavailable
labeled data. By each training iteration, we optimize the model and try to make it
better.Thequalityofthetrainingdatasetcontributestothetrainingaccuracyof
themodel.
Evaluation(EVL): Aftertrainingthemodel,itistestedwithanewdatasetwhich
has not been used as training data. Also, the model can be evaluated in real-life
scenariosandcomparedwithothercompetingmodels.Existingmetricsareused
or new metrics are created to evaluate the model.
Prediction (PRD): Thesuccessofthemodeldependsonhowgoodamodelcan
predictinanunknownsetup.Afterasatisfactoryevaluation,weemploythemodel
tosolvetheproblemandseehowitworks.Therearemanypredictionmetricssuch
as classification accuracy, log-loss, F1-score, to measure the success of the model.
Interpretation (INT): The prediction result might not be enough to make a deci-
sion. We often need a transformation of the prediction result and post-processing
to translate predictions into knowledge. For example, only numerical results do
nothelpmuchbut a good visualization can help to make a decision.
Communication (CMN): Different components of the DS system might reside
in a distributed environment. So, we might need to communicate with the in-
volvedparties(e.g.,devices,persons,systems)toshareandaccumulateinformation.
Communicationmighttakeplacein different geographical locations or the same.
Deployment (DPL): The built DS solution is installed in its problem domain to
servetheapplication.Overtime,theperformanceofthemodelismonitoredsothat
the model can be improved to handle new situations. Deployment also includes
model maintenance and sending feedback to the model building layer.
Table 1: Description of the stages in DS pipeline
appeared in team process pipelines that comprise requirement for-
mulation, specification, and planning, which are often challenging
indatascience.Thealgorithmicstepsanddataprocessingaredone
inthemodelbuildinglayer. Modeling doesnotnecessarilyimplythe
existence of an ML component,since DS can involve custom data
processingorstatisticalmodeling.Post-processinglayerincludes
the tasks that take place after the results have been generated. The
DS pipeline stages are described in Table 1.
RQ1b: What are the frequent and rare stages of the DS
pipeline in theory? The frequency of stage can depend on the
Figure 4: Frequency of pipeline stages in theory
focusof thepipeline oritsimportance incertaincontext (ML,big-
data management). Among 46 DS pipelines (which are not team
processes), Figure 4 shows the number of times each stage appears.
A few pipelines present stages with broad terminology that fit mul-
tiple stage-definitions. In those cases, the pipelines were labeled
withthefittedstagesandcountedmultipletimes. Modeling,data
preparation, and feature engineering appear most frequently in the
literature. While modeling is present in 93% of the pipelines, other
model relatedstages ( feature engineering,training, evaluation,pre-
diction) are not used consistently. Often trainingis not considered
asaseparatestageandincludedinsidethe modeling stage.Similarly,
wefoundthat evaluation andprediction areoftennotdepictedas
separatestages. However,by separating thestages andmodulariz-
ing the tasks, the DS process can be maintained better [ 4,76]. The
pipeline created with the most number of stages (11) is provided
byAshmoreetal .[7].Ontheotherhand,about15%ofthepipelines
fromtheliteraturearecreatedwithaminimalnumber(3)ofstages.
Among them, 80% are ML processes and falls in the category of
DSoptimizations.Wefoundthatthesepipelinesareveryspecific
to particular applications, which include context-specific stages
likedatasampling,querying,visualization,etc.,butdonotcover
most of the representative stages. A pipeline in theory may not
requireallrepresentativestages,sinceitcanhavenoveltyincertain
stagesandexcludetheothers.However,therepresentativepipeline
provides common terminology and facilitate comparative analysis.
Finding1 :Post-processinglayersareincludedinfrequently(52%)
compared to pre-processing (96%)and model building (96%) layers
of pipelines in theory.
Clearly, preprocessing and model building layers are considered
in almost all of the studies. In most of the cases, the pipelines
do not consider the post-processing activities ( interpretation, com-
munication,deployment ).Thesepipelinesoftenendwiththepre-
dictive process and thus do not follow up with the later stages
which entails how the result is interpreted, communicated and de-
ployed to the external environment. Miao et al .argued that overall
lifecycle management tasks (e.g., model versioning, sharing) are
largely ignored for deep learning systems [ 50]. Previous studies
2094TheArtandPracticeof Data Science Pipelines ICSE‚Äô22,May21‚Äì29,2022,Pittsburgh,PA, USA
alsoshowedthatsignificantamountofcostandeffortisspentinthe
post-developmentphasesintraditionalsoftwarelifecycle[ 48,66].
Indata-intensivesoftware,themaintenancecostcangoevenhigher
withthehigh-interesttechnicaldebtinthepipeline[ 75].Therefore,
post-processingstagesshouldbeincorporatedforabetterunder-
standingoftheimpactoftheproposedapproachonmaintenance
of the DS pipeline.
2.3 Organizationof Pipeline Stages in Theory
RQ2:Howarepipelinestagesconnectedtoeachother? InFig-
ure 3, for simplicity, we depicted the DS pipeline as a mostly linear
chain. However, our subject DS pipelines often have non-linear
behavior. In any stage, the system might have to return to the pre-
vious stage for refinement and upgrade, e.g., if a system faces a
real-world challenge in modeling, it has to update the algorithm
whichmightaffectthedatapre-processingandfeatureengineering
as well. Furthermore, the stages do not have strict boundaries in
the DS lifecycle. In Figure 3, two backward arrows, from feature
engineering andevaluation ,indicatefeedbacktoanyoftheprevi-
ous stages. Although in traditional software engineering processes
(e.g., waterfall model, agile development, etc.), feedback loop is not
uncommon, in DS lifecycle, there are multiple stakeholders and
models in a distributed environment which makes the feedback
loops more frequent and complex. Sculley et al .pointed that DS
taskssuchassampling,learning,hyperparameterchoice,etc.are
entangledtogethersothatChangingAnythingChangesEverything
(CACEprinciple)[ 76],whichinturncreatesimplicitfeedbackloops
that are not depicted in the pipelines [ 15,24,67,83]. The feedback
loops inside any specific layer are more frequent than the feedback
loops from one layer to another. Also, a feedback loop to a distant
previous stage is expensive. For example, if we do data preparation
afterevaluation then the intermediate stagesalso require updates.
2.4 Characteristicsof the Pipelines in Theory
RQ3: What are the different types of pipelines available in
theory?Thecontextandrequirementsoftheprojectcaninfluence
pipelinedesignandarchitecture[ 25].Here,wepresentthetypesof
pipelines with different characteristics that are available in theory.
Weclassifiedeachsubjectinourstudyintofourclassesbasedonthe
overallgoalofthearticle.Themostofthepipelinesintheory(39%)
aredescribing or proposing new pipelines to solve a new or existing
problem. About 31% of the pipelines are on reviewing or comparing
the existing pipelines. The third group of DS pipelines (14%) are
intendedto optimizeacertainpartofthepipeline.Forexample,Van
DerWeideetal .proposesapipelineformanagingmultipleversions
ofpipelinesandoptimizeperformance[ 83].Mostofthepipelines
inthiscategoryareapplicationspecificandincludeveryfewstages
that are necessary for the optimization. Fourth, some research
introducenewapplication ormethodandpresentwithinthepipeline.
We observed that there is no standard methodology to develop
comparable and inter-operable DS pipelines. Using the labeling
methodologyshowninFigure1,welabeledeachpipelineandfound
threetypesofDSpipelinesintheliterature:1)MLprocess,2)big
data managementprocess, and 3) team process.
ML process: 46% of all the pipelines we found in the literature
are describing machine learning processes. The recent advent ofartificial intelligence, supervised learning and deep learning has
ledtomoreDSsystemsthatinvolveMLcomponents.Thepipelines
inthiscategoryemphasizethealgorithmicprocess,learningpat-
terns,andbuildingpredictivemodels.However,thepost-processing
stagesarerareinthesetypeofpipelines.TheMLpipelinesareoften
thoughtofasalgorithmicprocessinthelaboratoryscenario.Butas
mentioned in[ 7], incorporatingthe post-processingstages would
be desiredto ensuresafe real-worlddeployment of suchpipelines.
Bigdatamanagement :Thereferencesinthiscategorypresent
DS pipelines that manage a large amount of data or describes a
framework (software-hardware infrastructure) for data processing
butdonotcontainmachinelearningcomponentsinthepipeline.
Processing large amount data often requires specific algorithms
and engineering methods for efficiency and further processing. We
foundthat 18% of all the subject studies fall in this category.
Team process: We also found some DS pipelines that are not
describingDSsoftwarearchitecture.Thesepipelinesdescribework-
flow of human activities that needs to be followed in a DS pipeline.
Thesestudiespresentahigh-levelviewforbuildingDScomponent
in a team environment. The data science teams require specific
expertise and management to build successful DS pipelines [ 4,46].
In this paper, in ¬ß3 and ¬ß4, we are only focusing on DS pipeline as
software architecture, and therefore, we did not compare the team
process pipelines in the rest of this section.
Finding 2 :Most of the pipelines in-theory involve cyber and phys-
ical components,only a few with human processes in the loop.
Weidentifiedwhetherthepipelinesinvolvecyber,physicalorhu-
man process, using our labeling process described in section ¬ß2.1.2.
Cyberprocesses refer to activities that involve automated systems
andmachinerycomputations.SincemodernDSsystemsinvolves
large amount of data and requires extensive computation, all of
the pipelines include cyber component in it. Physical processes
includetheactivitieswhichrequirereal-worldconnectionswiththe
system. Forexample, collecting data using mobile sensors or cam-
erasisaphysicalprocess.Although23%ofthebigdatapipelines
include physical processes, only 9% of the ML pipelines include
thatinthepipeline.InmanyDSsystems,developersorresearchers
participate in the pipelines actively to make decisions that need
humaninterventions[ 81,83].Forexample,inmanyDSsystems,
analytical model validation, troubleshooting, data interpretation is
necessary which requires human involvement. However, only 13%
ofthepipelinesacknowledgedhumaninvolvementinthepipeline.
3 DS PIPELINE IN-THE-SMALL
Similar to the DS pipelines in large systems and frameworks, for
averyspecificdatasciencetask(e.g.,objectrecognition,weather
forecasting,etc.),programmersbuildpipeline.Differentstagesof
theprogramperformaspecificsub-taskandconnectwiththeother
stages using data-flow or control-flow relations. In this section, we
described such DS pipelines in-the-small .
3.1 Methodology
We collected 105 DS programs from Kaggle competition notebooks
[35].Kaggleisoneofthemostpopularcrowd-sourcedplatforms
for DS competitions, owned by Google. Besides participating in
2095ICSE‚Äô22,May21‚Äì29,2022,Pittsburgh,PA, USA Sumon Biswas, Mohammad Wardat, and Hridesh Rajan
1. Featured
2. Research
3. Recruitment
4. Masters
5. Analytics
6. Playground
7. Getting started
105 top-rated
DS programsAPI:Stage
DictionaryParse and
extract APIs
Create
Pipeline of
Stages
Competitions
PRP PRD MDL TRN MDL PRP ACQFor each
competition, select
most voted solution
Filter out solutions
¬¨ - votes < 10
¬¨ - not end-to-end ¬¨ ¬¨
¬¨ ¬¨ pipeline
Figure5:ThepipelinecreationprocessforKaggleprograms
competitions, data scientists, researchers, developers collaborate to
learnandshareDSknowledgeinvarietyofdomains.Theusersand
organizations can host a DS competition in Kaggle to solve real-
world problems. A competition is accompanied by a dataset and
prize money. Many Kaggle solutions have resulted in impactful DS
algorithmsandresearchsuchasneuralnetworksusedbyHinton
and Dahl [ 17], improving the search for the Higgs Boson at CERN
[34],etc.WechoseKagglesolutionstoanalyzeDSpipelineforthree
reasons: 1) all programs perform a DS task and provide solution to
awellspecifiedproblemassociatedwithadataset,2)solutionswith
thehighestnumberofvotesarewellacceptedsolutionsforaspecific
problem, and 3) the problems cover a wide range of domains.
Thereare331completedcompetitionsinKaggletodate.They
categorized the competitions into Featured,Research,Recruitment ,
Masters,Analytics ,Playground andGetting started . We collected
solutionsofallthecompetitionsfrom eachcategoryexcept Getting
startedandPlayground (these two categories are intended to serve
as DS tutorials and toy projects). First, we filtered the competitions
for which there are solutions available (many old competitions do
not contain any public solution). We found 138 such competitions.
For a given competition problem, we selected the most voted so-
lutionwhichhasatleast10votes.Thus,wegot105top-ratedDS
solutionsforanalyzingpipelinesin-the-small.Thisselectionand
pipeline creation process is shown in Figure 5.
All of the DS programs are written in Python using ML libraries
like Keras, Scikit-learn, Tensorflow, etc. These packages provide
high-level Application Programming Interfaces (APIs) for perform-
ingaspecific task on dataor model. We parsedthe programs into
Abstract Syntax Tree (AST) and collected all the API calls from the
programs. Thenthe functionalityof an APIis usedto identify the
stage of the pipeline. We extracted the temporal order of API calls
to identify the stages. Standard static analysis of the Python pro-
grams facilitate the extraction process. Our analysis suggests that
theDSprogramsfollowalinearstructurewithlessthan4%AST
nodesbeingconditionalorloops.Wangetal .proposedasimilarap-
proach for extracting external dependencies in Jupyter Notebooks
by creating an API database and analyzing AST [87].
WecreatedadictionarybymappingeachAPIcollectedfromthe
programs, to one of the 11 stages of the DS pipeline described in
section¬ß2.Duringthemapping,weexcludedthegenericAPIsfrom
the dictionary. For example, model.summary() is used to print the
model parameters and does not represent any stage of the pipeline.
For creating the dictionary, we taken a two-fold approach. First,
we understand the context of the program and API usage. Second,'DWD
$FTXLVLWLRQ'DWD
3UHSDUDWLRQ0RGHOLQJ 7UDLQLQJ (YDOXDWLRQ 3UHGLFWLRQ
Figure 6: Pipeline in-the-small extracted from API usages

 

	 
  
  

Figure 7: Frequency of pipeline stages in-the-small
we look at the API documentation to confirm the corresponding
pipelinestage.WefoundthatDSAPIsaredefinitive intheiroper-
ationsandwell-categorizedbythelibrary.Forexample,theAPIs
in Keras [ 42] and Scikit-learn [ 43] are grouped into preprocess-
ing,models,etc.OurAPI-dictionarywasmanuallyvalidatedbya
second-rater and moderator who labeled DS pipelines in section
¬ß2. Then, we built a tool which takes the API dictionary and DS
program,andautomaticallycreatestheDSpipeline.Forasequence
ofAPIswiththesamestage,weabstractedthemintoasinglestage.
Asanexample,Figure5showsaDSpipelinecreatedfromaKaggle
solution[ 36].Eachstageinthepipeline(e.g.,ACQ,PRP)represents
oneormoreAPIusages.Thearrowsinthepipelinedenotethetem-
poral sequence of stages. Note that, one stage can appear multiple
times in a pipeline. The API dictionary, Kaggle programs, and tool
to generate the pipelines is shared in the paper artifact [5].
3.2 Representative Pipeline in-the-Small
RQ4:Whatarethestagesof DS pipeline in-the-small ?Among
the11pipelinestagesdescribedinFigure3,wefoundonly6stages
in the DS programs that are depicted in Figure 6. Other stages (e.g.,
storage,featureengineering,interpretation,communication,deploy-
ment) are not found in these programs because these stages occur
while building a production-scale large DS system and often not
presentintheDSnotebooks.Therefore,thepipelineinDSprograms
consistsof the subset of pipeline stages in theory.
We summarized the frequency of each stage of the DS programs
in Figure 7. Among 105 programs, data acquisition anddata prepa-
rationarepresentinalmostallofthem.Surprisingly, modeling is
present inonly 70%of theprograms. We foundthat, inmany pro-
grams,nomodelingAPIshadbeenusedbecausedevelopersdidnot
useanybuilt-inMLalgorithmfromlibraries,e.g.,LogisticRegres-
sion, LSTM, etc. In these cases, the developers use data-processing
APIs on the training data to build custom model, e.g., this note-
book [37] usesdata preparation APIs to produce results. To enable
more abstraction ofthe stages in these pipelines,further modular-
izationis necessary, which has been investigated in RQ8.
Finding 3 :Evaluationstageisinfrequent,appearingonlyin36%
of the pipelines in-the-small.
Evaluation isa trickystageof theDSpipeline. Developershaveto
choosetheappropriatemetricandmethodologytoevaluatetheir
model.Basedontheevaluationresult,themodelisupdatedover
multipleiterations.Wefoundthat, besidesusingmetrics,inmany
2096TheArtandPracticeof Data Science Pipelines ICSE‚Äô22,May21‚Äì29,2022,Pittsburgh,PA, USA

            

 		    	

			
Figure 8: Stages occurring before and after each stage
cases,evaluationrequireshumanunderstandingandcomparison
of the result produced by the model. The reason for having less
numberof evaluation stageinthepipelineisthatoftenthedevelop-
ers evaluate the performance by plotting and visualizing the result.
Since the visualization APIs are not considered as evaluation stage,
we found this stage less frequently in pipelines. Also, many pro-
gramsdirectlygotothe prediction stagewithoutgoingto evaluation
stage at all. Furthermore, notebooks are often used for experimen-
tationpurposessothatmanycomputationsareperformedduring
developmentbuteliminatedwhenthenotebooksareshared[ 44].
Forexample,onedevelopermighttryanumberofclassifiersand
evaluate their accuracy. After finding the best performing classi-
fier,itcanbetheonlyonesharedinthenotebook.Therefore,we
experienced many missing stages in the pipeline in-the-small. The
complexDStasksrequireseveralcomputationswhichmightnot
beusedinproducingthefinalprediction,butdefinitelyshouldbe
considered as part of the pipeline.
3.3 Pipeline Organizationin the Small
RQ5: How are the stages connected with each other in pipe-
linein-the-small? ToanswerRQ5,weconsideredeachoccurrence
ofthestagesinaDSprogramandlookedatitspreviousandnext
stage.InFigure8,weshowedwhichstagesarefollowedorpreceded
by each stage. We found that datapreparation can occur before or
afterall otherstages.Apartfrom that, dataacquisition is followed
bydata preparation most of the time, which in turn is followed
bymodeling.Modeling is followed mostly by training, which in
turn is followed by prediction .Evaluation is mostly surrounded
byprediction anddata preparation . From Figure 8, we can also
find some most occurring feedback loop: evaluation topreparation ,
evaluation tomodeling andprediction tomodeling.
Data preparation tasks (e.g., formatting, reshaping, sorting) are
not limited to just before the modeling stage, rather it is done on a
whenever-needed basis. For example, in the following code snippet
from a Kaggle competition [ 38], while creating model-layers, data
preprocessing API has been called in line 2.
1x = Conv2D(mid, (4 , 1) , activation= 'relu ', padding= 'valid ' )(x)
2x = Reshape((branch_model.output_shape[1], mid, 1))(x)
3x = Conv2D(1 , (1 , mid) , activation= 'linear ' , padding= 'valid ' )(x)
4x = Flatten(name= 'flatten ' )(x)
5head_model = Model([xa_inp , x b_inp], x, name='head'
Themodeling stage is always surrounded by other stages of the
pipeline. However, there is often a loop around modeling, training,
evaluation, and prediction .Modeling often repeats many times to
improve the model over multiple iterations. For example, in thefollowingKagglecodesnippet[ 39],themodeliscreatedandtrained
multipletimesto find the best one.
1random_forest = RandomForestClassifier(n_estimators=100,
random_state=50, verbose=1, n_jobs= ‚àí1)# Modeling
2random_forest. fit( train , tra in_labels) # Train
3...
4poly_features = sca ler.fit_transform (poly_features)
5poly_features_test = scaler.transform(poly_features_test)
6random_forest_poly = RandomForestClassifier(n_estimators=100,
random_state=50, verbose=1, n_jobs= ‚àí1)# Modeling
7random_forest_poly.fit(poly_features , train_labels) # Training
8pred = random_forest_poly.predict_proba( poly_features_test)[:,1]
Finding4 :Stagesof pipelinesin-the-smallare oftentangledwith
eachother.
AlloftheDSprogramsfailtomaintainagoodseparationofcon-
cerns[19]betweenstages.Strongabstractionboundarieshelpto
make the program modular and easy-to-maintain [ 58,59,61]. In
addition,agoodDSsolutionshouldnotonlycomputebetterpre-
dictive result, but also facilitate software engineering activities e.g.,
debugging,testing,monitoring[ 28].However, wefoundthatstages
are often tangledwith other stages [ 14,45,64] across the pipelines.
Thecodeforonestageisinterspersedwiththecodeforotherstages.
For example, while building the deep learning network ( modeling),
the developers often switch to different data preparation tasks, e.g.,
reshaping,resizing[ 32,33],whichtanglesdatapreparationconcern
with the modeling concern. We observed some early attempts to
adopt modular design practices. For instance, this notebook [ 40]
separated code into different high-level stages, namely, prepara-
tion,feature extraction ,exploratory data analysis (EDA) ,topic model ,
etc. These high-level pipelines can improve the abstraction, which
further enable the maintainability, and reusability [ 71]. In some
scenarios, reuse or maintenance might not be desired for pipelines
in-the-small.However, to enhance readability [ 44] and repeatabil-
ity[28]andeaseoftesting,debuggingorrepairing[ 88,89],more
attentionon modular design practices is needed for DS pipelines.
Finding5 :Data preparation stage is occurring significant number
of times between any two stages of pipelines in-the-small, which is
causingpipeline jungles.
We found that new data sources are added, new features are identi-
fied, and new values are calculated incrementally in the pipeline
whichevolvesorganically.Thisresultsinalargenumberofdata
preprocessing tasks like sampling, joining, resizing along with ran-
dom file input-output. This is called pipeline jungles [76], which
causestechnical debtfor DS systems in thelong run. Pipelinejun-
glesare hard to test and any small change in the pipeline will
take a lot of effort to integrate. The situation gets worse in case of
largerDSpipelines,whereseveraldatamanagementactivities(e.g.,
clean, serve, validate) are necessary through the pipeline in differ-
ent stages [ 62,63]. The recommended way is to think about the
pipelineholisticallyandscrapethepipelinejunglebyredesigningit,
which inturn takes furtherengineering effort[ 76]. We found that
the large DS projects, which are discussed in ¬ß4, isolate the data
preparation tasks into separate files and modules [ 13,73,82,94],
whichalleviatesthepipelinejunglesproblem.So,DSpipelinein-
the‚ÄìsmallneedsfurtherIDE(e.g.,JupyterNotebook,etc.)support
and methodologiesfor code isolation and modularization.
2097ICSE‚Äô22,May21‚Äì29,2022,Pittsburgh,PA, USA Sumon Biswas, Mohammad Wardat, and Hridesh Rajan
/RDG'DWD
DQG/LEUDU\)HDWXUH
(QJLQHHULQJ0RGHOLQJ 7UDLQLQJ(YDOXDWLRQ
	9LVXDOL]DWLRQ3UHGLFWLRQ('$
	9LVXDOL]DWLRQ'DWD
3UHSDUDWLRQ
Figure 9: Representative data science pipeline in-the-small
3.4 Characteristicsof Pipelines in-the-Small
RQ6:Whatarethepatternsinpipelinein-the-smallandhow
it compares to pipeline in theory? We have not found many
stages from Figure 3, e.g., feature engineering, interpretation, com-
munication, in pipeline in-the-small. One reason is that the low-
level pipeline extractedfrom theAPI usagescannot capturesome
stages. For example, even if a developer is conducting feature engi-
neering, the used APIs might be from the data preparation stage.
Fortunately,wefoundmanyKagglenotebooksthatareorganizedby
the pipeline stages. We visited all the 105 Kaggle notebooks in our
collectionandextractedthesehigh-levelpipelinesmanually.Unlike
thelow-levelpipelines(extractedusingAPIusages),ahigh-level
pipeline consistsof the stages abstracted by the developers.
TheKagglenotebooksfollowliterateprogrammingparadigm[ 71,
85],whichallowsthedeveloperstodescribethecodeusingrichtext
and separate them into sections. We found that 34 out of 105 note-
books divided the code into stages. We collected those stages from
the Kaggle notebooks. Furthermore, we labeled these notebooks
into the 11 stages from DS pipeline in theory by two raters, and
extractedthestagesthatarenotpresentintheory.Theextracted
high-levelpipelinesandlabelsareavailableinthepaperartifact[ 5].
We observed that no notebooks specify these stages: storage,
interpretation, communication, anddeployment . These DS programs
arenotproduction-scaleprojects.Therefore,theydonotincludethe
post-processingstagesinthepipeline.Themostcommonstagesare
modeling (79%),data preparation (62%),data acquisition (53%), and
featureengineering (35%),whichisalignedwiththefindingofDS
pipelineintheory.Inaddition,wefoundthesestageswhicharenot
presentintheory: libraryloading,exploratorydataanalysis(EDA),
visualization .Amongthem EDAhasbeenusedmostofthetimes
(43%) and covered the most part of those pipeline. Before going
to the modeling and successive stages, a lot of effort is given on
understanding the data, compute feature importance, and visualize
the patterns, which help to build models quickly in later stages [ 7].
Furthermore,somenotebookspresent libraryloading asseparate
stage.Weobservedthatchoosingappropriatelibrary/framework
andsettinguptheenvironmentisanimportantstepwhiledevel-
opingpipelinein-the-small.Wealsofoundthatdatavisualization
is an recurring stage mentioned by the developers. Visualization
canbe done for EDA or feature engineering (before modeling), or
for evaluation (after modeling). Based on these observations we
updatedtherepresentativepipelinein-the-smallinFigure9.The
high-level pipeline provides an overall representation of the sys-
tem,whichcanbeleveragedtodesignsoftwareprocess.Itwouldbe
beneficialforthedeveloperstoclosethegapbetweenthelow-level
and the high-level pipeline by identifying the tangled stages.
4 DS PIPELINE IN-THE-LARGE
The DS solutions described in the previous section are specific to a
givendatasetandawell-definedproblem.However,therearemanyTable2:GitHubprojectsforanalyzingpipelinein-the-large
Project Name Purpose #Files#ASTLOC
Autopilot [3] Pilot a car using computer vision 36 11185 348
CNN-Text-Classification[13] Sentenceclassification 694779711.4K
Darkflow [82] Real-time object detection and classification 1025 655670 8.6K
Deep ANPR [22] Automatic number plate recognition 647046410.8K
Deep Text Corrector [57] Correct input errors in short text 47 50770 3.0K
FaceClassification[6] Real-timefaceandemotion/genderdetection 29211790135.3K
FaceNet[73] Facerecognition 1352 1889529 18.2K
KittiSeg[80] Roadsegmentation 276187143 4.8K
LSTM-Neural-Network[8] Predict time series steps and sequences 24 11434 1.2K
MaskR-CNN[2] Object detection and instance segmentation 2561567786 15.6K
MobileNetSSD[65] Object detection network 28 21272 25.6K
MTCNN [16] Jointfacedetectionandalignment 153121138219.7K
Object-Detector-App[18] Real-timeobject recognition 215 318534 47.9K
Password-Analysis [72] Analyzea large corpus of text passwords 148678703.6K
PersonBlocker[93] Block people in images 12 44517 977
QANet [94] Machinereading comprehension 83107669 2K
Speech-to-Text-WaveNet Sentenceleve lenglishspeec hrecognition 32 18626 5.1K
Tacotron [60] Text-to-speec h synthesis 114588451.4K
Text-Detection-CTPN [70] Text detection 640 257083 18.4K
TF-Recomm[79] Recommendation systems 177789535
XLNet[95] Languageunderstanding 36 143172 11.5K
DS projects which are large, not limited to a single source file, and
containsmultiplemodules.Thesesolutionsareintendedtosolve
moregeneralproblemswhichmightnotbespecifictoadataset.For
example,theobjectiveofthe FaceClassification projectinGitHub
[6] is to detect face from images or videos and classify them based
on gender and emotion. This problem is not specific to a particular
dataset and the scope is broader compared to the Kaggle solutions.
We collected such top-rated DS projects from GitHub to analyze
DS pipeline in-the-large.
4.1 Methodology
Biswasetal .publishedadatasetcontainingtopratedDSprojects
fromGitHub[ 10].Fromthelistofprojectsinthisdataset,wefiltered
matureDSprojectshavingmorethan1000stars.Thus,wefound
269 mature GitHubprojects. However, thereare many projects in
this list which are DS libraries, frameworks or utilities. Since we
want to analyze the pipeline of data science software, we removed
those projects. Finally, we also removed the repositories which
serve educational purposes. Thus, we found a list of 21 mature
open-source DS projects. The list of projects, and their purpose are
shown in Table 2.
Foreachproject,wecreatedtwopipelines:high-levelpipeline
and low-level pipeline. For creating the high-level pipeline, we
manuallycheckedtheprojectarchitecture,modulestructureand
executionprocess.Thisgaveusagoodunderstandingofthesource
fileorganizationandlinkagebetweenmodules.Afteridentifyingthe
high-level pipeline and execution sequences of the source files, we
used the same API based method used to analyze Kaggle programs
intheprevioussection,tocreatelow-levelpipelineoftheseGitHub
projects. The methodology of selecting and extracting pipelines
from the GitHub projects is shown in Figure 10.
For example, theproject QANet[94] is intended to do machine
readingcomprehension.Here,Pythonhasbeenusedastheprimary
2098TheArtandPracticeof Data Science Pipelines ICSE‚Äô22,May21‚Äì29,2022,Pittsburgh,PA, USA
projects for data
science
High level
pipeline
21 Mature
Github
ProjectsLow level
pipelineFilter out projects
¬¨ ¬¨ - stars < 1000
¬¨ ¬¨ - libraries ¬¨
¬¨ ¬¨ -¬¨utility
¬¨ ¬¨ - tutorialsAnalyze
pipeline
architecture
Source code
Figure 10: Pipeline creation process for GitHub projects
language, and shell script has been used for data downloading and
projectsetup.Thehigh-levelpipelinefor QANetincludesthestages:
dataacquisition ,datapreparation ,modeling,training,evaluation and
prediction . In the beginning, config.py file integrates the modules
(preparation, modeling, and training) and provides an interface
toconfigureamodelbyspecifyingdatasetandotherparameters.
Then,thefile evaluate.py isexecutedtoperformtheevaluation
and prediction. For the low-level pipeline, for a specific file, we
used the API based analysis to generate the pipeline, which was
usedtoanalyzepipelinein-the-small.Forinstance,intheproject
QANet, although model.py serves modeling at a high level, it also
does data preparation, training, and evaluation, when APIs are
considered. In addition to the pipeline stages, we also identified
afewotherpropertiesofeachproject:1)numberofcontributors,
2) AST count, 2) technology/language used, 3) entry points and 4)
executionsequence.WeleveragedtheBoainfrastructure[ 20,21]
to analyze the different properties of the projects. These properties
helped us to categorize and analyze the pipeline in-the-large. The
detailsof the projects are available in the paper artifact [5].
The projects are from various domains: object detection, face
classification,automateddriving,s peechsynthesis,numberplate
recognition,predicttimeseriessequence,etc.Thenumberofdevel-
opers in each projectranges between 1 and 40 with anaverageof
8. Among 21 projects, 16 of them are developed by teams and 5 of
themaredeveloped byindividuals. The primarylanguage usedto
develop these projects is Python.
4.2 Representative Pipeline in-the-Large
Compared to the Kaggle programs, we found a significant differ-
ence in the pipeline of large DS projects. Because of the larger
size of the projects, the pipeline architecture is different. All the
projectscontainmultiplesourcefilesforhandlingdifferenttasks
(e.g.,modeling,training)andabout50%oftheprojectsorganizethe
source files into modules (e.g., utils,preprocessing ,model, etc.).
RQ7:WhatistherepresentativeDSpipelinein-the-large?
EachoftheprojectscontainssixstagesdescribedinFigure6: ac-
quisition, preparation, modeling, training, evaluation , andprediction .
However, since the projects are not coupled to a specific dataset
and they solve a more general problem, the projects are not lim-
ited to one single pipeline. We found that the pipeline of each
project is divided into two phases: 1) development phase and 2)
post-development phase, which is depicted in Figure 11.
Indevelopment phase ,themaingoalistobuildamodelthat
solvestheproblemingeneral.Abasedatasetisusedtobuildthe
modelthatwouldbeusedforotherfuturedatasets.Aftercompleting
amodeling, training, evaluation loop, the final model is created and
saved as an artifact. Afterwards, the projects also create model
interfaces, which lets the user modify and exploit the model in
the post-development phase. Finally, the model artifact is saved as
a source file or some model archiving formats. For example, the0RGLI\PRGHO$FTXLVLWLRQ7UDLQLQJ 0RGHOLQJ
0RGHODUWLIDFW
7UDLQLQJ3UHSDUDWLRQ
3UHGLFWLRQ(YDOXDWLRQ
(YDOXDWLRQ
7UDLQLQJ (YDOXDWLRQ
7UDLQHGPRGHO'HYHORSPHQWSKDVH
3RVWGHYHORSPHQWSKDVH
Figure11:DSpipelinein-the-large.Developmentphase(top)
runs during model building and post-development phase
(bottom) runs for making prediction.
projectPerson-Blocker [93] andSpeech-to-Text-WaveNet [47] saved
the model in the source file ( model.py ) and lets the users train the
model in the next phase. On the other hand, the project KittiSeg
[80]andAutopilot [3]savedthebuiltmodelartifactinJSONformat
(.json) and checkpoint format ( .ckpt) respectively. We observed
that the evaluation and prediction is often not the main goal in
this phase; rather, building an appropriate model and making it
availablefor further usageis the central activity.
Inpost-development phase , the users access the pre-built
model and use that for prediction. After acquiring data, a few
preprocessing steps are needed to feed the model. In all of the
projects under this study, we found that the development phase
is similar. Howev er, we identified three different patterns in the
post-developmentphasewhichareshowninFigure11.First,the
userscanmodifythemodelbysettingitshyperparametersanduse
that to make prediction on a new dataset. Second, the users can
use the model as-it-is and train the model on the new dataset to
makeprediction.Third,theuserscanalsodownloadthepre-trained
model and directly leverage that for prediction. Finally, at the end
of this phase, the prediction result is obtained.
The post-development phase in the pipeline enabled software
reusability of the models. All of these projects have instructions in
their readmeordocumentationexplainingtheusageandcustomiza-
tion.Forexample,theprojectDeepANPR[ 22]providesinstructions
forobtaininglargetrainingdata,retrainingthemodels,andbuild
it for prediction. However, not all the projects enable reusability
in the development pipelines. Only a few ofthem provides access
to the modules by importing in new development scenario. For
instance, Darkflow [ 82] let users access the darkflow.net.build
module and use it in new application development. To increase the
reusability of DS programs, it would be desired to consider similar
accessto the development pipeline of these large projects.
4.3 Organizationof DS Pipeline in-the-Large
RQ8:Howarethestagesconnectedinpipelinein-the-large?
The abstraction in DS projects is stricter than the DS programs
described in ¬ß3. The projects are built in a modular fashion, i.e.,
one source file for a broad task (e.g., train.py, model.py ). How-
ever,insideonespecificfile,therearemanyotherpossiblestages,
especiallydata preprocessing appearsinsideallthesourcefiles.In
addition, the module connectivity is not linear. All of the modules
use external libraries for performing different tasks. As a result,
2099ICSE‚Äô22,May21‚Äì29,2022,Pittsburgh,PA, USA Sumon Biswas, Mohammad Wardat, and Hridesh Rajan
there are a lot of interdependencies (both internal and external) in
the DS pipeline. One immediate difference of these pipelines with
traditionalsoftwareisDSpipelinesareheavilydependantonthe
data.Forexample,theproject Speech-to-Text-WaveNet [47]requires
a certain format of data. When we want to use that in a new situa-
tion, the data properties might be different. So, the usage pipelines
would have a few additional stages. In some cases, the original
pipelineismodified.Here,therearemanysub-pipelinesworkto-
gethertobuildalargepipeline.However,wehavenotfoundany
framework or common methodology these software are using. The
differentpatternsofDSpipelinesseekmoreadvancedmethodology
or framework to build DS pipeline and release for production.
4.4 Characteristicsof Pipelines in-the-Large
RQ9:Whatarethepatternsfoundinthepipelines? Thepipe-
linesfoundinthissettingcanbecategorizedinto1)looselycoupled
and2)tightlycoupled,basedontheirmodularity.Ahighnumberof
contributorsintheprojectresultedinlooselycoupledpipelines.We
foundtheloosely coupledonesaredesigned inamodularfashion
and one module (e.g., data cleaning, modeling) is designed to be
usedbyothermodules.Usually,therearemultipleentry-pointsina
looselycoupledpipelineanduserhasmoreflexibility.Ontheother
hand, in a tightly coupled pipeline, the modules are stricter and
integrated tightly with other modules. There is only one or two
entry-points to the pipeline, which automatically calls the other
modules.Wefoundthattheprojectswith6ormorecontributors
(‚àº75%) followed a loosely coupled architecture and projects with 1
to 5 contributors followed a tightly coupled architecture.
Finding 6 :Thereisneedforintegrationanddeploymenttoolsfor
pipelinesin-the-largebutnocommonframeworkisusedinpractice.
Although all the project under this study are written using
Python, no project is using any common tool that integrates the
DSmodulesandprovidesinterfacetothepipeline.Today,contin-
uous integration and deployment (CI/CD) tools are widely used
in traditional software lifecycle to automate compilation, build-
ing, and testing [ 29,41]. Additionally, from our subject studies
of pipelines in theory, we found some CI/CD tools designed for
ML pipelines available [ 30,51,53]. Surprisingly, here we found no
projectsinpipelinein-the-largeareusinganyCI/CDtools.How-
ever,theprojectsdemonstratetheneedofCI/CDintherepositories.
Inmostoftheprojects,theenvironmentsetupandaccesstofunc-
tionalities are configured through command lines scripts [ 3,70].
Some projects used docker container [6,47,73,93] to set up the
environment and run the pipeline. A few others used Python note-
books that call different modules to integrate the pipeline stages
[2,18,57]. 7 out of 21 projects used shell script for integration
(e.g., sending HTTP request to download data, model reuse, etc.)
[65,94]. Although CI/CD frameworks e.g., TravisCI, GitHub Ac-
tions, Microsoft Azure DevOps are well established for traditional
softwaresuchaswebapplications,severalchallengesremainfor
DSpipelines.Karla≈°etal .outlinedtheprobabilisticnatureofML
testingasamajorCI/CDchallengeandpointedoutthegapbetween
recent theoretical development of CI/CD in DS and their usage in
practice[ 41]. Hence,further researchis needed toinvestigate the
practicalchallengesof using CI/CD in data science projects.5 DISCUSSION
Throughoursurvey,empiricalstudy,andanalysis,wepresented
thestateofdatasciencepipelinethatdescribesitssemantics,design
concerns, and the overall computational paradigm. Furthermore,
thefindingsshowtheimportanceofstudyingthepipelinestructure
reminiscing the traditional software engineering works on design
patternsand architecture.
In Theory: We presented all the representative stages and sub-
tasks that inform the terminology of DS pipelines to be used in
future works. By comparing with the available pipeline categories
e.g.,MLprocess,bigdata,andteamprocesses,similaritiesanddiver-
gences can be directly identified. The presence of implicit feedback
loops and lack of post-processing stages suggest ad hoc pipeline
construction at the present time. This paper takes the first step
towards comparable and reusable pipeline construction.
In-The-Small: ThenovelAPI-basedanalysiscanbeutilizedfor
mining, extracting, and statically analyzing pipelines. We also
elicited the notion of high-level and low-level pipelines, where
thehigh-levelabstractionhasmoresimilaritywiththatintheory.
However,low-levelpipelinesexhibitmanydifferencessuchasmiss-
ing some stages, sparse data preparation, lack of modularization.
The gap between low-level pipeline and its presentation in high-
level can be reduced by making pipeline specific features available
indevelopmentenvironmente.g.,pipelinetemplateinJupyterNote-
book. Additionally, the low-level pipelines often had an important
stageexploratory data analysis missing which incurs much time
andeffort.Pipelineversioningtechniquesthatconsiderdata,model,
and source code will facilitate storing such intermediate stages.
In-The-Large: Different pipeline patterns emerged in develop-
ment and post-development phase of the large projects, which
suggestcreatingseparate developer-centric anduser-centric pipeline
structure. Intightly-coupled projects, the abstractionof stages are
contingent upon the project-specific requirements and internal/ex-
ternaldependencies,whereas,inloosely-coupledprojects,opportu-
nities remain to build reusable sub-pipelines that span over project
boundaries.Finally,thereisaneedforbuildingautomatedCI/CD
toolsfordatasciencespecifictesting,deployment,andmaintenance.
To researchers and tool builders. (1) Modularization of DS
pipeline into stages is challenging over all three representations.
Furtherworksare needed forstandardization of pipelinearchitec-
ture e.g., defining the interfaces of stages, enumerating externally
visible properties, identifying domain-specific constraints, to de-
velopreusableandinteroperableDSpipelines.(2)Weshowedpo-
tentialsfor automaticpipeline analysisframework basedon static
analysis and API specifications. A few future directions would
be mining (sub-)pipelines patterns, build AutoML pipelines [ 55],
andanalyzing evolution. (3) We confirmed several antipatterns of
pipelinesthatcallforactionse.g.,CACEprinciple,pipelinejungles,
scarcepost-processing,implicitfeedbackloop,CI/CDchallenges.
(4) Pipeline specific tool support is needed such as version control
for dataand models, storing intermediate results between stages.
To datascientists and engineers. (1) Pipelines are often built
foraprototypein-the-small,whichmightnotscaletoaproduction
level system. A well-designed pipeline inthe early stage will help
toidentifykeycomponents,estimatecost,optimize,andmanage
risksbetterinthelifecycle.(2)Therepresentativeviewsofpipelines
2100TheArtandPracticeof Data Science Pipelines ICSE‚Äô22,May21‚Äì29,2022,Pittsburgh,PA, USA
will serve as a checklist of stages and their connections. (3) Data
andalgorithmsbeingthefocusofDSpipeline,preprocessingand
modelingactivitiesarewellunderstoodandpracticedbydatascien-
tists. However, they should emphasize more on including rigorous
evaluation beyondaccuracysuchasrobustnessandfairness[ 11,12].
(4) Many people with diverse backgrounds are involved in a DS
pipeline. A pipeline with human-in-the-loop approach will benefit
identifyingcollaborationpoints,decomposingtasks,andmanage
transdisciplinary teams. For example, a pipeline can encourage
data scientists to choose a modeling technique that is maintain-
able. (5) Future work is necessary to identify the interactions of DS
pipeline with the real world i.e., which stages receive inputs, when
a checkpoint is saved, how results are disseminated, etc.
6 THREAT TO VALIDITY
ForbuildingthepipelinesfromDSprograms,wereliedontheAPIs.
Onethreatmightbe,whathappensifthedeveloperdoesnotuse
anyAPIforcompletingastageintheprogram.Weexaminedthis
possibilityandfoundthatDSprogramsareheavilydependenton
libraries and external APIs and ML tasks are always performed
using library APIs. Additionally, we validated the API-to-stage
dictionary with the API documentation and manual verification.
AnotherpossiblethreatisthattheKagglesolutionsmightnot
be representative. We adopted a two-fold strategy to mitigate that
threat. First, we selected the solutions with the most number of
votes and at least 10 votes. Second, we manually verified each
program whether it is an end-to-end DS solution. Since some most
voted solutions are only for introduction and exploratory analysis
ofthedataset,bymanualverification,weexcludedthoseprograms.
The GitHub projects are also taken from a previously published
dataset containing DS repositories. We further filtered them based
on the number of stars and whether they perform a DS task.
Moreover,sincethechosenDSprogramsfromKaggleandGitHub
are using Python as the primary language, another question might
be on the generalization of them as DS programs. According to
GitHub and Stack Overflow , Python has become the most growing
language in recent times [ 31,68]. In data science, Python is the
mostusedlanguagebecauseoftheavailabilityofnumerousML,DL
and data analysis packages such as Pandas, NumPy, TensorFlow,
Keras, Caffe, Theano, Scikit-Learn and many more.
7 RELATED WORK
ManystudiespresentedMLpipelineintheirowncontext,which
can not be generalized for all DS systems. Garcia et al .focused
onbuildinganiterativeprocesswiththreemainphases:develop-
ment, training and inference. They described the interpretation of
data and code while integrating the whole lifecycle [ 25]. Polyzotis
et al.presented the challenges of data management in building
production-levelMLpipelineinGooglearoundthreebroadthemes:
dataunderstanding,datavalidationandcleaning,anddataprepa-
ration[62,63].Theyalsoprovidedanoverviewofanend-to-end
large-scaleMLpipelinewithadatapointofview.CarltonE.Sapp
defined ML concepts, business challenges, stages in the lifecycle,
roles of DS teams with comprehensive end-to-end ML architec-
ture [74]. This gives us a holistic understanding of the business
processes (e.g., acquire, organize, analyze, deliver) of a DS project.A few other studies try to capture the DS process by surveying
andinterviewingdevelopers.Rohetal .surveyedthedatacollection
techniques in the field of big data. They presented the workflow
of data collection answering how to improve data or models in an
MLsystem[ 69].Anotherstudyidentifiedthesoftwareengineering
practicesandchallengesinbuildingAIapplicationsinsideMicrosoft
development teams [ 4]. They found some key differences in AI
software process compared to other domains. They considered a 9-
stageworkflowforDSsoftwaredevelopment.Hilletal .interviewed
experienced AI developers and identified problems they face in
eachstage[ 28].Theyalsotriedtocomparethetraditionalsoftware
processandtheAIprocess.Zhoupresentedherownviewtobuilda
betterMLpipeline[ 96].Theypresentedthreechallengesinbuilding
ML pipelines: data quality, reliability and accessibility.
Some articles described ML applications and frameworks which
presentDSpipelinesfromindustry.Forexample, Databricks pro-
vides high-level APIs for programming languages [ 30]. Team Data
Science Process (TDSP) is an agile and iterative process to build
intelligentapplicationsinsideMicrosoftcorporation[ 77].InaUS
patent, the authors compared two data analytic lifecycles [ 81], and
presentedthedifferenceinthesetofparameterswithrespecttotime
andcost.CRossIndustryStandardProcessforDataMining(CRISP-
DM) is a 6-stage comprehensive process model for data mining
projects across any industry [ 92]. Google Cloud Blog described the
workflowofanAIplatform[ 27].Theyexplainedtaskscompleted
in each stage with respect to Google Cloud and TensorFlow[ 1].
Although there are many papers in the literature presenting DS
pipeline, thereis nocomprehensive studythat triesto understand
and compare DS pipelines in theory and practice.
8 CONCLUSION
Many software systems today are incorporating a data science
pipelineastheirintegralpart.Inthiswork,wearguedthattofacili-
tateresearchandpracticeondatasciencepipelines,itisessentialto
understandtheirnature.Tothatend,wepresentedathree-pronged
comprehensivestudyofdatasciencepipelinesintheory,datasci-
ence pipelines in-the-small, and data science pipelines in-the-large.
Our study analyzedthree datasets: a collectionof 71 proposals for
datasciencepipelinesandrelatedconceptsintheory,acollection
of 105 implementations of data science pipelines from Kaggle com-
petitions to understand data science in-the-small, and a collection
of 21 mature data science projects from GitHub to understand data
science in-the-large. We have found that DS pipelines differ signifi-
cantly between these settings. Specifically, a number of stages are
absent in-the-small, and the DS pipelines have a more linear struc-
ture. The DS pipelines in-the-large have a more complex structure
andfeedbackloopscomparedtothetheoreticalrepresentations.We
also contribute threerepresentations of DS pipelines thatcapture
theessenceofoursubjectsintheory,in-the-small,andin-the-large.
ACKNOWLEDGMENTS
Thiswork wassupported in part byUS NSF grants CNS-21-20448
and CCF-19-34884. We also thank the reviewers for their insightful
comments. All opinions are of the authors and do not reflect the
view of sponsors.
2101ICSE‚Äô22,May21‚Äì29,2022,Pittsburgh,PA, USA Sumon Biswas, Mohammad Wardat, and Hridesh Rajan
REFERENCES
[1]Mart√≠nAbadi,PaulBarham,JianminChen,ZhifengChen,AndyDavis,Jeffrey
Dean,MatthieuDevin,SanjayGhemawat,GeoffreyIrving,MichaelIsard,etal .
2016. Tensorflow: A system for large-scale machine learning. In 12th USENIX
SymposiumonOperatingSystemsDesignandImplementation(OSDI16) .265‚Äì283.
[2]Waleed Abdulla. 2017. Mask R-CNN for object detection and instance segmenta-
tionon Keras and TensorFlow. https://github.com/matterport/Mask_RCNN.
[3]Jesse Hu Alexis Chan, Octavio Arriaga. 2017. Autopilot-TensorFlow. https:
//github.com/SullyChen/Autopilot-TensorFlow.
[4]SaleemaAmershi,AndrewBegel,ChristianBird,RobertDeLine,HaraldGall,Ece
Kamar, Nachiappan Nagappan, Besmira Nushi, and Thomas Zimmermann. 2019.
Software Engineering for Machine Learning: A Case Study. In Proceedings of the
41stInternationalConferenceon Software Engineering . ACM.
[5]Anonymous.2021. DataSciencePiplineArtifact. https://github.com/anonymous-
authorss/DS-Pipeline.
[6]Octavio Arriaga. 2018. Face classification and detectionn. https://github.com/
oarriaga/face_classification.
[7]Rob Ashmore, Radu Calinescu, and Colin Paterson. 2021. Assuring the Machine
LearningLifecycle:Desiderata,Methods,andChallenges. ACMComput.Surv. 54,
5, Article 111 (may 2021). https://doi.org/10.1145/3453444
[8]JakobAungiers. 2019. LSTMNeuralNetwork forTimeSeries Prediction. https:
//github.com/jaungiers/LSTM-Neural-Network-for-Time-Series-Prediction.
[9]Francine Berman, Rob Rutenbar, Brent Hailpern, Henrik Christensen, Susan
Davidson, Deborah Estrin, Michael Franklin, Margaret Martonosi, Padma Ragha-
van,VictoriaStodden,etal .2018. Realizingthepotentialofdatascience. Commun.
ACM61,4 (2018), 67‚Äì72.
[10]Sumon Biswas, Md Johirul Islam, Yijia Huang, and Hridesh Rajan. 2019. Boa
meets Python: a Boa dataset of data science software in Python language. In
Proceedings of the 16th International Conference on MiningSoftwareRepositories .
IEEEPress, 577‚Äì581.
[11]Sumon Biswas and Hridesh Rajan. 2020. Do the Machine Learning Models on a
CrowdSourcedPlatformExhibitBias?AnEmpiricalStudyonModelFairness.
InProceedingsofthe28thACMJointMeetingonEuropeanSoftwareEngineering
ConferenceandSymposiumontheFoundationsofSoftwareEngineering (Virtual
Event, USA). 642‚Äì653. https://doi.org/10.1145/3368089.3409704
[12]Sumon Biswas and Hridesh Rajan. 2021. Fair Preprocessing: Towards Under-
standing Compositional Fairness of Data Transformers in Machine Learning
Pipeline.In ESEC/FSE‚Äô2021:The29thACMJointEuropeanSoftwareEngineering
ConferenceandSymposiumontheFoundationsofSoftwareEngineering (Athens,
Greece).
[13]Denny Britz. 2018. Convolutional Neural Network for Text Classification in
Tensorflow. https://github.com/dennybritz/cnn-text-classification-tf.
[14]MuffyCalder,MarioKolberg,EvanH.Magill,andStephanReiff-Marganiec.2003.
FeatureInteraction:ACriticalReviewandConsideredForecast. Comput.Netw.
41,1 (Jan. 2003), 115‚Äì141. https://doi.org/10.1016/S1389-1286(02)00352-3
[15]CLPhilipChenandChun-YangZhang.2014. Data-intensiveapplications,chal-
lenges, techniques and technologies: A survey on Big Data. Information sciences
275(2014),314‚Äì347.
[16]Z Ming Chen Mengda. 2018. reproduce MTCNN,a Joint Face Detection and
AlignmentusingMulti-taskCascadedConvolutionalNetworks. https://github.
com/AITTSMD/MTCNN-Tensorflow.
[17]GeorgeEDahl,NavdeepJaitly,andRuslanSalakhutdinov.2014. Multi-taskneural
networksfor QSARpredictions. arXivpreprint arXiv:1406.1231 (2014).
[18]Sam Crane Dat Tran. 2018. Real-Time Object Recognition App with Tensorflow
andOpenCV. https://github.com/datitran/object_detector_app.
[19]EdsgerWDijkstra.1982. Ontheroleofscientificthought. In Selectedwritingson
computing:a personal perspective . Springer, 60‚Äì66.
[20]Robert Dyer, Hoan Anh Nguyen, Hridesh Rajan, and Tien N. Nguyen. 2013.
Boa:ALanguageandInfrastructureforAnalyzingUltra-Large-ScaleSoftware
Repositories. In Proceedings of the 35th International Conference on Software
Engineering (SanFrancisco, CA) (ICSE‚Äô13). 422‚Äì431.
[21]Robert Dyer, Hoan Anh Nguyen, Hridesh Rajan, and Tien N. Nguyen. 2015. Boa:
Ultra-Large-Scale Software Repository and Source-Code Mining. ACM Trans.
Softw. Eng. Methodol. 25, 1, Article 7 (Dec. 2015), 34 pages. https://doi.org/10.
1145/2803171
[22]Matthew Earl. 2016. Using neural networks to build an automatic number plate
recognition system. https://github.com/matthewearl/deep-anpr.
[23]Erich Gamma, Richard Helm, Ralph E. Johnson, and John M. Vlissides. 1993.
DesignPatterns:AbstractionandReuseofObject-OrientedDesign.In Proceedings
of the 7th European Conference on Object-Oriented Programming (ECOOP ‚Äô93) .
Springer-Verlag, Berlin, Heidelberg, 406‚Äì431.
[24]AmirGandomiandMurtazaHaider.2015. Beyondthehype:Bigdataconcepts,
methods,andanalytics. Internationaljournalofinformationmanagement 35,2
(2015),137‚Äì144.
[25]Rolando Garcia, Vikram Sreekanti, Neeraja Yadwadkar, Daniel Crankshaw,
Joseph E Gonzalez, and Joseph M Hellerstein. 2018. Context: The missing piece
in the machine learning lifecycle. In KDDCMIWorkshop , Vol. 114.[26]David Garlan. 2000. Software architecture: a roadmap. In Proceedings of the
Conferenceon the Future of Software Engineering . 91‚Äì101.
[27]Google Cloud Blog. 2019. Machine Learning Workflow. https://cloud.google.
com/ml-engine/docs/tensorflow/ml-solutions-overview.
[28]Charles Hill, Rachel Bellamy, Thomas Erickson, and Margaret Burnett. 2016.
Trials and tribulations of developers of intelligent systems: A field study. In 2016
IEEESymposiumonVisualLanguagesandHuman-CentricComputing(VL/HCC) .
IEEE,162‚Äì170.
[29]MichaelHilton,NicholasNelson,TimothyTunnell,DarkoMarinov,and Danny
Dig. 2017. Trade-Offs in Continuous Integration: Assurance, Security, and Flexi-
bility. InProceedings of the 2017 11th JointMeetingon Foundations of Software En-
gineering (Paderborn,Germany) (ESEC/FSE2017) .AssociationforComputingMa-
chinery, New York, NY, USA, 197‚Äì207. https://doi.org/10.1145/3106237.3106270
[30]SueAnnHongandTimHunter.2017. Build,Scale,andDeployDeepLearning
Pipelines with Ease. https://databricks.com/blog/2017/09/06/build-scale-deploy-
deep-learning-pipelines-ease.html.
[31] GitHubInc.2019. Octoverse 2018. https://octoverse.github.com/projects.
[32]Md Johirul Islam, Giang Nguyen, Rangeet Pan, and Hridesh Rajan. 2019. A
Comprehensive Study on Deep Learning Bug Characteristics. In ESEC/FSE‚Äô19:
The ACM Joint European Software Engineering Conference and Symposium on the
Foundationsof Software Engineering (ESEC/FSE) .
[33]Md Johirul Islam, Rangeet Pan, Giang Nguyen, and Hridesh Rajan. 2020. Repair-
ing Deep Neural Networks: Fix Patterns and Challenges. In ICSE‚Äô20: The 42nd
InternationalConferenceon Software Engineering (Seoul,SouthKorea).
[34]Kathryn Jepsen. 2014. The machine learning community takes on the
Higgs. https://www.symmetrymagazine.org/article/july-2014/the-machine-
learning-community-takes-on-the-higgs.
[35] Kaggle. 2021. Kaggle Notebook. www.kaggle.com/competitions.
[36]Kaggle. 2021. Kaggle Notebook. www.kaggle.com/thousandvoices/simple-lstm.
[37]Kaggle. 2021. Kaggle Notebook. https://www.kaggle.com/zfturbo/simple-ru-
baseline-lb-0-9627.
[38]Kaggle. 2021. Kaggle Notebook. www.kaggle.com/seesee/siamese-pretrained-0-
822.
[39]Kaggle.2021. KaggleNotebook. www.kaggle.com/willkoehrsen/start-here-a-
gentle-introduction.
[40]Kaggle. 2021. Kaggle Notebook. https://www.kaggle.com/danielbecker/
careervillage-org-recommendation-engine.
[41]Bojan Karla≈°, Matteo Interlandi, Cedric Renggli, Wentao Wu, Ce Zhang, Deepak
Mukunthu IyappanBabu, JordanEdwards, Chris Lauren, Andy Xu,and Markus
Weimer. 2020. Building continuous integration services for machine learning.
InProceedingsofthe26thACMSIGKDDInternationalConferenceonKnowledge
Discovery & Data Mining . 2407‚Äì2415. https://doi.org/10.1145/3394486.3403290
[42] Keras. 2021. Keras API Reference. https://keras.io/api/.
[43]Keras.2021. Scikit-LearnAPIReference. https://scikit-learn.org/stable/modules/
classes.html.
[44]MaryBethKery,MarissaRadensky,MahimaArya,BonnieEJohn,andBradA
Myers.2018. Thestoryinthenotebook:Exploratorydatascienceusingaliterate
programmingtool.In Proceedingsofthe2018CHIConferenceonHumanFactors
in Computing Systems . 1‚Äì11.
[45]GregorKiczales,JohnLamping,AnuragMendhekar,ChrisMaeda,CristinaLopes,
Jean-Marc Loingtier, and John Irwin. 1997. Aspect-oriented programming. In
ECOOP‚Äô97 ‚Äî Object-Oriented Programming , Mehmet Ak≈üit and Satoshi Matsuoka
(Eds.).SpringerBerlinHeidelberg,Berlin,Heidelberg,220‚Äì242.
[46]MiryungKim,ThomasZimmermann,RobertDeLine,andAndrewBegel.2016.
The emerging role of data scientists on software development teams. In Proceed-
ings of the 38th International Conference on Software Engineering . ACM, 96‚Äì107.
[47]Namju Kim.2018. Speech-to-Text-WaveNet : End-to-end sentencelevel English
speechrecognitionbasedonDe epMind‚ÄôsWaveNetandtensorflow. https://github.
com/buriburisuri/speech-to-text-wavenet.
[48]Bennet P Lientz, E. Burton Swanson, and Gail E Tompkins. 1978. Characteristics
of application software maintenance. Commun.ACM 21,6 (1978), 466‚Äì471.
[49]Hui Miao, Amit Chavan, and Amol Deshpande. 2017. Provdb: Lifecycle manage-
ment of collaborative analysis workflows. In Proceedings of the 2nd Workshop on
Human-In-the-Loop Data Analytics . ACM, 7.
[50]HuiMiao,AngLi,LarrySDavis,andAmolDeshpande.2017. Towardsunified
data and lifecycle management for deep learning. In 2017 IEEE 33rd International
Conferenceon Data Engineering (ICDE) . IEEE, 571‚Äì582.
[51]Microsoft Blog. 2019. What are ML pipelines in Azure Machine Learning ser-
vice? https://docs.microsoft.com/en-us/azure/machine-learning/service/concept-
ml-pipelines.
[52]Justin J Miller. 2013. Graph database applications and concepts with Neo4j.
InProceedings of the Southern Association for Information Systems Conference,
Atlanta,GA,USA , Vol. 2324.
[53]Valohai MLOps. 2020. What Is a Machine Learning Pipeline? https://valohai.
com/machine-learning-pipeline/.
[54]GiangNguyen,StefanDlugolinsky,MartinBob√°k,VietTran,√ÅlvaroL√≥pezGarc√≠a,
Ignacio Heredia, Peter Mal√≠k, and Ladislav Hluch `y. 2019. Machine Learning and
DeepLearningframeworksandlibrariesforlarge-scaledatamining:asurvey.
Artificial Intelligence Review (2019),1‚Äì48.
2102TheArtandPracticeof Data Science Pipelines ICSE‚Äô22,May21‚Äì29,2022,Pittsburgh,PA, USA
[55]Giang Nguyen, Johir Islam, Rangeet Pan, and Hridesh Rajan. 2022. Manas:
MiningSoftwareRepositoriestoAssistAutoML.In ICSE‚Äô22:The44thInternational
Conferenceon Software Engineering (Pittsburgh,PA, USA).
[56]RandalSOlson,NathanBartley,RyanJUrbanowicz,andJasonHMoore.2016.
Evaluationofatree-basedpipelineoptimizationtoolforautomatingdatascience.
InProceedingsoftheGeneticandEvolutionaryComputationConference2016 .ACM,
485‚Äì492.
[57]Alex Paino. 2017. Deep learning models trained to correct input errorsin short,
message-liketext. https://github.com/atpaino/deep-text-corrector.
[58]Rangeet Pan and Hridesh Rajan. 2020. On Decomposing a Deep Neural Network
into Modules. In ESEC/FSE‚Äô2020: The 28th ACM Joint European Software Engi-
neeringConferenceandSymposiumontheFoundationsofSoftwareEngineering
(Sacramento, California, United States).
[59]Rangeet Pan and Hridesh Rajan. 2022. Decomposing Convolutional Neural Net-
works into Reusable and Replaceable Modules. In ICSE‚Äô22: The 44th International
Conferenceon Software Engineering (Pittsburgh,PA, USA).
[60]Kyubyong Park. 2018. A TensorFlow Implementation of Tacotron: A Fully End-
to-EndText-T o-Speech SynthesisModel. https://github.com/Kyubyong/tacotron.
[61]DavidLorgeParnas,PaulCClements,andDavidMWeiss.1985. Themodular
structureofcomplexsystems. IEEETransactionsonsoftwareEngineering 3(1985),
259‚Äì266.
[62]NeoklisPolyzotis,SudipRoy,StevenEuijongWhang,andMartinZinkevich.2017.
Data management challenges in production machine learning. In Proceedings of
the2017ACMInternationalConferenceonManagementofData .ACM,1723‚Äì1726.
[63]NeoklisPolyzotis,SudipRoy,StevenEuijongWhang,andMartinZinkevich.2018.
Data Lifecycle Challenges in Production Machine Learning: A Survey. ACM
SIGMODRecord 47,2 (2018), 17‚Äì28.
[64]Christian Prehofer. 1997. Feature-oriented programming: A fresh look at ob-
jects.InECOOP‚Äô97‚ÄîObject-OrientedProgramming ,MehmetAk≈üitandSatoshi
Matsuoka(Eds.).SpringerBerlinHeidelberg,Berlin,Heidelberg,419‚Äì443.
[65]Chuan Qi. 2019. Caffe implementation of Google MobileNet SSD detection
network. https://github.com/chuanqi305/MobileNet-SSD.
[66]V√°clav Rajlich. 2014. Software evolution and maintenance. In Future of Software
EngineeringProceedings . 133‚Äì144.
[67]Muhammad Habib Rehman, Victor Chang, Aisha Batool, and Teh Ying Wah.
2016. Big data reduction framework for value creation in sustainable enterprises.
InternationalJournalof Information Management 36,6 (2016), 917‚Äì928.
[68] David Robinson.2017. The Incredible Growth of Python. https://stackoverflow.
blog/2017/09/06/incredible-growth-python/.
[69]Yuji Roh, Geon Heo, and Steven Euijong Whang. 2019. A Survey on Data Collec-
tion for Machine Learning: a Big Data-AI Integration Perspective. IEEE Transac-
tionson Knowledge and Data Engineering (2019).
[70]Eragon Ruan. 2019. Scene text detection based on ctpn (connectionist text
proposal network). https://github.com/eragonruan/text-detection-ctpn.
[71] Adam Rule, Aur√©lien Tabard, and James D Hollan. 2018. Exploration and expla-
nation in computational notebooks. In Proceedings of the 2018 CHI Conference on
HumanFactorsin Computing Systems . 1‚Äì12.
[72]PhilippeR√©my.2018. DeepLearningmodeltoanalyzealargecorpusofcleartext
passwords. https://github.com/philipperemy/tensorflow-1.4-billion-password-
analysis.
[73]DavidSandberg.2018. FaceRecognitionusingTensorflow. https://github.com/
davidsandberg/facenet.
[74]Carlton E Sapp. 2017. Preparing and architecting machine learning. Gartner
Technical Professional Advice (2017),1‚Äì37.
[75]David Sculley, Gary Holt, Daniel Golovin, Eugene Davydov, Todd Phillips, Diet-
mar Ebner, Vinay Chaudhary, and MichaelYoung.2014. Machine learning:Thehighinterestcredit card of technical debt. (2014).
[76]David Sculley, Gary Holt, Daniel Golovin, Eugene Davydov, Todd Phillips, Diet-
mar Ebner, Vinay Chaudhary, Michael Young, Jean-Francois Crespo, and Dan
Dennison.2015. Hiddentechnicaldebtinmachinelearningsystems.In Advances
in neural information processing systems . 2503‚Äì2511.
[77]Roald Bradley Severtson. 2017. What is the Team Data Science Pro-
cess? https://docs.microsoft.com/en-us/azure/machine-learning/team-data-
science-process/overview.
[78]Mary Shaw and David Garlan. 1996. Software Architecture: Perspectives on an
EmergingDiscipline . Prentice-Hall, Inc., USA.
[79]GuocongSong.2017. Tensorflow-basedRecommendationsystems. https://github.
com/songgc/TF-recomm.
[80]Marvin Teichmann. 2018. A Kitti Road Segmentation Model Implemented in
Tensorflow. https://github.com/MarvinTeichmann/KittiSeg.
[81]Stephen Todd and David Dietrich. 2017. Computing resource re-provisioning
duringdataanalyticlifecycle. US Patent 9,619,550.
[82]Andrew Bagshaw Trieu. 2018. Real-time object detection and classification.
https://github.com/thtrieu/darkflow.
[83]TomVanDerWeide,DimitrisPapadopoulos,OlegSmirnov,MichalZielinski,and
TimVanKasteren.2017. Versioningforend-to-endmachinelearningpipelines.
InProceedings of the 1st Workshop on Data Management for End-to-End Machine
Learning. ACM, 2.
[84]Anthony J Viera, Joanne M Garrett, et al .2005. Understanding interobserver
agreement: the kappa statistic. Fammed 37,5 (2005), 360‚Äì363.
[85]Ben Wagner. 2020. Accountability by design in technology research. Computer
Law& Security Review 37 (2020), 105398.
[86]Zhiyuan Wan, Xin Xia, David Lo, and Gail C Murphy. 2019. How does machine
learning change software development practices? IEEE Transactions on Software
Engineering (2019).
[87]Jiawei Wang, Li Li, and Andreas Zeller. 2021. Restoring Execution Environments
ofJupyterNotebooks.In 2021IEEE/ACM43rdInternationalConferenceonSoftware
Engineering(ICSE) . IEEE, 1622‚Äì1633.
[88]Mohammad Wardat, Breno Dantas Cruz, Wei Le, and Hridesh Rajan. 2022. Deep-
Diagnosis:AutomaticallyDiagnosingFaultsandRecommendingActionableFixes
inDeepLearningPrograms.In ICSE‚Äô22:The44thInternationalConferenceonSoft-
ware Engineering (Pittsburgh,PA, USA).
[89]MohammadWardat,WeiLe,andHrideshRajan.2021. DeepLocalize:FaultLocal-
ization for Deep Neural Networks. In ICSE‚Äô21: The 43nd International Conference
on Software Engineering (VirtualConference).
[90]HadleyWickham.2019. Datascience:howisitdifferenttostatistics? IMSBulletin
48 (2019).
[91]Jeannette M Wing. 2019. The Data Life Cycle. Harvard Data Science Review
(2019).
[92]R√ºdigerWirthandJochenHipp.2000. CRISP-DM:Towardsastandardprocess
modelfordatamining.In Proceedingsofthe4thinternationalconferenceonthe
practicalapplicationsof knowledge discovery and data mining . Citeseer, 29‚Äì39.
[93]MaxWoolf.2018.Automatically"block"peopleinimages(likeBlackMirror)using
a pretrained neural network. https://github.com/minimaxir/person-blocker.
[94]Adams Wei Yu, David Dohan, Minh-Thang Luong, Rui Zhao, Kai Chen, Moham-
mad Norouzi, and Quoc V Le. 2018. A Tensorflow implementation of QANet for
machinereading comprehension. https://github.com/NLPLearn/QANet.
[95]Charlie Bickerton Zhilin Yang, Zihang Dai. 2019. XLNet: Generalized Autore-
gressive Pretraining for Language Understanding. https://github.com/zihangdai/
xlnet.
[96]LindaZhou.2019. HowtoBuildaBetterMachineLearningPipeline. https://www.
datanami.com/2018/09/05/how-to-build-a-better-machine-learning-pipeline.
2103