Machine Translation Testing via Pathological Invariance
Shashij Gupta
Department of Computer Science and Engineering
IIT Bombay
India
shashijgupta@cse.iitb.ac.inPinjia He
Department of Computer Science
ETH Zurich
Switzerland
pinjia.he@inf.ethz.ch
Clara Meister
Department of Computer Science
ETH Zurich
Switzerland
clara.meister@inf.ethz.chZhendong Su
Department of Computer Science
ETH Zurich
Switzerland
zhendong.su@inf.ethz.ch
ABSTRACT
Machine translation software has become heavily integrated into
our daily lives due to the recent improvement in the performance
of deep neural networks. However, machine translation software
has been shown to regularly return erroneous translations, which
canleadtoharmfulconsequencessuchaseconomiclossandpoliti-
cal conflicts. Additionally, due to the complexity of the underlying
neural models, testing machine translation systems presents new
challenges. To address this problem, we introduce a novel method-
ology called PatInv. The main intuition behind PatInvis that sen-
tences with different meanings should not have the same transla-
tion.Underthisgeneralidea,weprovidetworealizationsofPatInv
thatgivenanarbitrarysentence,generatesyntacticallysimilarbut
semantically different sentences by: (1) replacing one word in the
sentenceusingamaskedlanguagemodelor(2)removingoneword
or phrase from the sentence based on its constituency structure.
We then test whether the returned translations are the same for
theoriginalandmodifiedsentences.WehaveappliedPatInvtotest
Google Translate and Bing Microsoft Translator using 200 English
sentences. Two language settings are considered: English !Hindi
(En-Hi) and English !Chinese (En-Zh). The results show that Pat-
Invcanaccuratelyfind308erroneoustranslationsinGoogleTrans-
late and 223 erroneous translations in Bing Microsoft Translator,
most of which cannot be found by the state-of-the-art approaches.
CCS CONCEPTS
‚Ä¢Software and its engineering ‚ÜíSoftware verification and
validation ; ‚Ä¢Computing methodologies ‚ÜíMachine transla-
tion.
KEYWORDS
Testing, Machine translation, Pathological Invariance
Permission to make digital or hard copies of all or part of this work for personal or
classroomuseisgrantedwithoutfeeprovidedthatcopiesarenotmadeordistributed
for profit or commercial advantage and that copies bear this notice and the full cita-
tion on the first page. Copyrights for components of this work owned by others than
ACMmustbehonored.Abstractingwithcreditispermitted.Tocopyotherwise,orre-
publish,topostonserversortoredistributetolists,requirespriorspecificpermission
and/or a fee. Request permissions from permissions@acm.org.
ESEC/FSE ‚Äô20, November 8‚Äì13, 2020, Virtual Event, USA
¬© 2020 Association for Computing Machinery.
ACM ISBN 978-1-4503-7043-1/20/11‚Ä¶$15.00
https://doi.org/10.1145/3368089.3409756ACM Reference Format:
Shashij Gupta, Pinjia He, Clara Meister, and Zhendong Su. 2020. Machine
Translation Testing via Pathological Invariance. In Proceedings of the 28th
ACM Joint European Software Engineering Conference and Symposium on the
Foundations of Software Engineering (ESEC/FSE ‚Äô20), November 8‚Äì13, 2020,
Virtual Event, USA. ACM, New York, NY, USA, 13pages.https://doi.org/10.
1145/3368089.3409756
1 INTRODUCTION
Duetorecentimprovementsinthequalityoftranslationsfromma-
chinetranslationsoftware,manypeoplehavestartedtorelyonthe
technologyintheirdailylives.Forexample,peopleoftenreadpolit-
ical news or articles from other countries and visit websites with
content in various languages. According to [ 73], in 2016, Google
Translate [ 2] had 500 million users and translated more than 100
billion words per day. The improvements that have lead to such
widespread usage of machine translation systems are largely due
to the advent of deep neural network, which are now frequently
thecorecomponentofmodernmachinetranslationsoftware.Neu-
ral machine translation (NMT) models are becoming as good as
a human translator. Many recent NMT systems are approaching
human-level performance in terms of ‚Äúquality score‚Äù (defined by
Google) [ 77] and ‚Äúhuman parity‚Äù (defined by Microsoft) [ 29].
Despite these recent improvements, NMT systems are not as
reliable as one might expect. Deep neural networks are brittle; of-
ten, when a neural network is evaluated on examples that differ
intrinsically from the examples it was trained to model, it does
not perform well [ 6]. NMT models are no exceptions; they can
produce erroneous outputs when inputs are adversarially manip-
ulated, such as upper casing some of the letters of the sentence
or injecting grammatical errors (e.g., ‚ÄúI are studying‚Äù). However,
it is not vital for the sentence to be syntactically wrong to fool
anNMTmodel.TherearenumerouscasesfoundwhereNMTmod-
elsreturnerroneoustranslationsforsyntacticallyandsemantically
correct inputs, e.g., in WeChat, a messenger app with over one bil-
lion monthly active users [ 88]. When encountered by users, incor-
rect translations can have severe and harmful consequences such
asfinancialloss,politicalconflicts,medicalmisdiagnoses,socialis-
sues, or personal safety threats [ 19,47,54,55]. These side-effects
motivatetheneedtocreatesystemsforensuringtherobustnessof
machine translation software.
863ESEC/FSE ‚Äô20, November 8‚Äì13, 2020, Virtual Event, USA Shashij Gupta, Pinjia He, Clara Meister, and Zhendong Su
Table 1: Examples of the errors detected by PatInv. The first two are from En->Hi and the last two are from En->Zh. The
translation is erroneous for both sentences in the first and the third examples; the translation is erroneous for the source
sentence in the second example; the translation is erroneous for the modified sentence in the fourth example.
Source Sentence Modified Sentence Translation (for both) Translation Meaning
The situation at the southern border, The situation at the southern border, ‡§¶»¢W‡§£‡•Ä ‡§∏‡•Ä‡§Æ‡§æ ‡§™‡§∞ ‡§øƒ•‡§•»†‡§§, ‡§ú‡•ã ‡§è‡§ï ‡§∏‡§Ç‡§ï‡§ü ‡§ï ‡•á The situation at the southern border,
which started as a crisis, is now a which started as a crisis, is now a …¥‡§™ ‡§Æ»Ö ‡§∂‡•Å…¥ …∑‡§à ‡§•‡•Ä, ‡§Ö‡§¨ ‡§è‡§ï »†‡§®‡§ï‡§ü‡§µ‡§§»É which started as a crisis, is now a
near system wide meltdown . near system wide development . ≈†‡§£‡§æ‡§≤‡•Ä ‡§π ‡•à ‡•§ nearby system.
I had a story to tell and I wanted I had a story to tell and I wanted ‡§Æ‡•á‡§∞ ‡•á ‡§™‡§æ‡§∏ ‡§¨‡§§‡§æ‡§®‡•á ‡§ï ‡•á »¢‡§≤‡§è ‡§è‡§ï ‡§ï‡§π‡§æ‡§®‡•Ä ‡§•‡•Ä I had a story to tell and I wanted
to finish it, Drapersays. to finish it, says. ‡§î‡§∞ ‡§Æ»à ‡§á‡§∏‡•á ‡§ñƒï‡§Æ ‡§ï‡§∞‡§®‡§æ ‡§ö‡§æ‡§π‡§§‡§æ ‡§•‡§æ‡•§ to finish it.
The South has emerged as a hub of The South has diedas a hub of Áî±‰∫éËæÉ‰ΩéÁöÑÂà∂ÈÄ†ÊàêÊú¨ÂíåËæÉÂº±ÁöÑÔºå The South has became a hub of
new auto manufacturing by foreign new auto manufacturing by foreign Â∑•‰ºöÂçóÊñπÂ∑≤Áªè Êàê‰∏∫Â§ñÂõΩÂà∂ÈÄ†ÂïÜÊñ∞ new auto manufacturing by foreign
makers thanks to lower manufact- makers thanks to lower manufact- Ê±ΩËΩ¶Âà∂ÈÄ†ÁöÑÊû¢Á∫Ω„ÄÇ makers thanks to lower manufact-
uring costs and less powerful unions. uring costs and less powerful unions. uring costs and less powerful unions.
The threatened tariffs led to the The threatened tariffs led to the Â®ÅËÉÅÁöÑÂÖ≥Á®éÂØºËá¥Ê¨ßÁõüÊâøËØ∫Âèç The threatened tariffs led to the
European Union pledging counter- Union pledging countertariffs. ÂÖ≥Á®é„ÄÇ European Union pledging counter-
tariffs. tariffs.
However, it can be very difficult to test NMT models. First, test-
ing deep learning models in general is quite different from testing
traditionalsoftware,inwhichsystems‚Äôcoreconceptsoralgorithms
manifest in source code. Rather, the output of neural networks de-
pends largely on the millions of parameters it has optimized over
during training, making these models essentially black boxes. Sec-
ond, recent testing approaches for artificial intelligence (AI) soft-
ware, of which machine translation software is a subset, primarily
target models with a small number of potential outputs, such as
classifiers. In contrast, simply enumerating the possible outputs of
mostNMTmodelsisanintractableproblem[ 56],makingmachine
translation systems incredibly difficult to test.
The current standard for automatic evaluation is BLEU [ 58],
which is calculated by comparing the word sequences1in the sys-
tem‚Äôs output with a set of reference quality translations. One of
BLEU‚Äôs major weaknesses is that it does not truly understand the
sentence meanings. Furthermore, it is necessary to provide accu-
rate reference translations to determine such a metric for machine
translationsystems,whichisprohibitiveinmanysituationswhere
such resources are limited.
Clearly, there is need for effective automated systems for test-
ingmachinetranslationsoftware.Thispaperproposesanoveltest-
ingmethodology,namelyPatInv,whosemainintuitionis thatsen-
tences of different meanings should not have the same transla-
tions. We use this intuition to formulate approaches which auto-
matically produce syntactically similar sentences with different
meanings. In particular, PatInv generates sentences of different
meaning through two approaches: 1) replacing one word in a sen-
tencewithanon-synonymouswordand2)removingameaningful
word or phrase from the sentence. The original and the newly gen-
eratedsentencesarefedtothetranslationsystemundertest;ifthe
translations are exactly the same, we report them as a suspicious
issue. Our practical implementation of PatInv uses a masked lan-
guage model based on BERT [ 20] to perturb words in a sentence
and a constituency parser to identify core words and phrases. The-
saurus [ 3], WordsAPI [ 4] and the NLTK library [ 10] are then used
to filter out synonyms and syntactically incorrect sentences.
1A maximum length of four words is common.To evaluate the effectiveness of PatInv, we use it to test Google
Translate and Bing Microsoft Translator on 200 English sentences
fromtwoarticlecategories(politicsandbusiness)asinputreleased
by He et al. [ 30]. Without using the optional filtering step (i.e., fil-
tering by sentence embeddings, Section 3.2.1), PatInv successfully
reports 452 pathological invariants with 56.6% average precision.
Whenthisoptionalfilteringmechanismisemployed,PatInvcanre-
port 28 pathological invariants with 100% accuracy. Table 1show
some of the erroneous translations found by PatInv. We find that,
due to its conceptual novelty, PatInv detects a unique set of er-
rors not found by other approaches. All the reported pathological
invariants and source code have been released [ 65]. The main con-
tributions of this paper are as follows:
We introduce a novel, widely applicable black-box method-
ology to validate machine translation software.
Wedescribeapracticalimplementationtogeneratesyntacti-
callysimilarbutsemanticallydifferentsentencesusingBERT,
using various filters to avoid generating invalid test cases.
We evaluate the model on 200 sentences for Google Trans-
late and Bing Microsoft Translator with two translation set-
tings.
We successfully find 308 erroneous translations in Google
translate and 223 in Bing Microsoft Translator with high ac-
curacy, most of which cannot be found by the state-of-the-
art techniques.
2 A MOTIVATING EXAMPLE
During the 2018 Winter Olympic Games, the Norwegian team‚Äôs
cookingfacilitiesintendedtoorder1500eggs.Thegameswereheld
in South Korea and thus they need to place an order in Korean
in a local grocery. The chefs turned to Google Translate for help
translating their order. To their surprise, a truck load of eggs fell
upon their kitchen: Google Translate mistakenly translated 1500
eggs into 15000 eggs in Korean [ 5].2
2In theory, this erroneous translation can be detected if PatInv replaces ‚Äù1500‚Äù with
‚Äù15000.‚ÄùAtthemoment,thiserrorhasalreadybeenfixed.Inpractice,itispossiblethat
BERTdoesnotrecommend‚Äù15000‚Äùasafillwordandthusmaynotfindthetranslation
error.
864Machine Translation Testing via Pathological Invariance ESEC/FSE ‚Äô20, November 8‚Äì13, 2020, Virtual Event, USA
This is real life translation error which caused inconvenience
and could have lead to a huge financial loss. Still, translation er-
rors can have much more serious and harmful consequence [ 19,
47,54,55]. For example, in 2018, due to a machine translation
error, Israel‚Äôs prime minister‚Äôs compliment to Israel Eurovision
winner Netta went from ‚Äúyou are a real darling‚Äù to ‚Äúyou are a
realcow‚Äù[ 66],leadingtoembarrassmentandmisunderstanding.In
2017,aPalestinianmanwasarrestedbypoliceafterposting‚ÄúGood
morning‚Äù on Facebook in Arabic; the post was wrongly translated
to ‚Äúattack them‚Äù in Hebrew and ‚Äúhurt them‚Äù in English [ 19]. As
more and more people have started to rely on machine transla-
tion, building robust machine translation software is of significant
importance.Toenhancetherobustnessofmachinetranslationsoft-
ware, this paper introduces PatInv, a novel and widely-applicable
methodology for testing machine translation.
3 APPROACH AND IMPLEMENTATION
This section discusses the high-level idea of PatInv and provides
two implementations (PatInv-Replace and PatInv-Remove). Recall
that the main intuition behind PatInv is that sentences with dif-
ferent meanings should not have the same translations. Hence, we
find issues such that both sentences have different meanings but
result in the same translation by the model under test. The input
for PatInv is a list of unlabeled, monolingual sentences, while its
output is a list of suspicious issues. For each input sentence, ei-
ther no issue is detected or a list of suspicious issues is returned.
A suspicious issue consists of 1) the original sentence 2) a gener-
ated (semantically different) sentence and 3) their shared transla-
tion. Three types of translation errors can cause an issue: (1) The
original sentence has an erroneous translation (2) the generated
sentence has an erroneous translation (3) both the sentences have
erroneous translations.
Figure1shows the overview of PatInv for both implementa-
tions; we use a source sentence from our dataset as the example
input. In summary, PatInv carries out the following four steps:
(1)Generating syntactically-similar sentences. Foreachunlabelled
sentence,wegeneratealistofsyntacticallysimilarsentences
by modifying a word or a phrase in the sentence.
(2)Filtering via syntactic and semantic information. Wefilterout
those test cases in which the newly generated sentence has
the same meaning as the original sentence.
(3)Collecting target sentences. Wefeedtheoriginalandthenewly
generated sentences to the machine translation system un-
der test and collect their target sentences (i.e., translations).
(4)Detecting translation errors. Translations of the newly gen-
erated sentences are compared with the translation of the
originalsentence.Ifanytranslationmatchesthatoftheorig-
inal sentence, the pair is reported as a potential issue which
may contain an erroneous translation.
3.1 Step 1: Generating Syntactically-Similar
Sentences
Givenasentenceinasourcelanguage(i.e.alanguagewetranslate
from),weneedtogeneratestructurallysimilarsentencesthathave
a different meaning as input for PatInv. Specifically, we want to
generate syntactically similarbut semantically differentsentences.Several approaches can be taken to produce a list of variations
fromasinglesentence.Forexample,wecanrandomlyinterchange
words. However, sentences generated using this method are not
necessarily syntactically similar to the original sentence. We have
found the following two approaches effective for the task:
(1)Replace a word in the original sentence with a word of the
same part-of-speech (e.g., noun or adjective) but different
meanings (PatInv-Replace)
(2)Remove a word or phrase from the original sentence sen-
tence. (PatInv-Remove)
3.1.1 PatInv-Replace. In this approach, we replace a single word
in our original sentence with another word of the same part-of-
speech to generate a new sentence with different meaning. For ex-
ample, in Fig. 1we replace the word ‚Äúcompletely‚Äù with the word
‚Äúslightly‚Äù which leads to a syntactically similar but semantically
different sentence. In practice, we replace a given word with k
new words to generate knew sentences. If there are mwords in
a sentence that can be replaced, our method will produce a list of
kmnew sentences. We narrow the scope of words that can be re-
placed to avoid strange linguistic phenomenon, which can lead to
falsepositiveslaterinthetestingprocess.Specifically,onlynouns,
verbs, adverbs, adjectives, and possessive pronouns are taken as
candidates for replacement. Additionally, we elect not to replace
stopwords (commonly used words, e.g., has, were) as we empiri-
callyfindthatreplacingthosewordsoftenleadstosyntacticallyor
semantically incorrect sentences. In our approach, we use the set
of stopwords provided by NLTK [ 10]3.
Now we discuss the problem of selecting candidates to replace
a given word in the original sentence. We note that it is impor-
tant to find words which fit well in the context to ensure that
our generated sentence makes sense. This criterion eliminates the
method of choosing words just by high word-embedding [ 60] vec-
tor similarity, as standard word-embeddings do not have any con-
textual information. For example, ‚Äúback‚Äù and ‚Äúfront‚Äù have high
word-embeddingvectorsimilaritybutifwereplace‚Äúback‚Äùin‚ÄúLook-
ing back at the experience‚Äù with ‚Äúfront‚Äù in ‚ÄúLooking front at the
experience,‚Äù we end up with a sentence unlikely to occur in the
English language.
AmodelthatiswellsuitedforthistaskistheMaskedLanguage
Model (MLM) [ 51], inspired by the Cloze Task [ 71]. Specifically,
givenastringcontainingasingle‚Äúmasked‚Äùwordasinput,anMLM
predicts what word belongs in the masked position. The MLM re-
turns a set of words as potential replacements. The key benefit of
using an MLM is that it takes into account the context of the sen-
tencewhenpredictingthemaskedtoken.IftheMLMisgood,then
it is highly likely that the predicted words will lead to meaningful
sentences. Note that we also check whether the words predicted
by the MLM belong to NLTK stopwords and likewise elect to not
use these words as candidate replacements.
ForMLMs,thereareseveralout-of-boxoptionsavailable,which
are built on top of language representation models such as ELMo
[61], GPT-2 [ 64], and BERT [ 20]. While training one‚Äôs own MLM
is also possible, it would require huge amounts of data and vast
computationalresourcestoevenmatchthecaliberofasystembuilt
3There is no universally-used list of stopwords available
865ESEC/FSE ‚Äô20, November 8‚Äì13, 2020, Virtual Event, USA Shashij Gupta, Pinjia He, Clara Meister, and Zhendong Su
Figure 1: Overview of PatInv (English !Hindi in Google Translate)
on the aforementioned state-of-the-art models, with no guarantee
ofproducingsomethingbetter.SinceagoodMLMiscriticalforour
approach, we elect to use the state-of-the-art options. Specifically,
in our implementation, we use BERT, a language representation
modeldevelopedbyGoogleAI.Interestingly,themaskedlanguage
task was one of the two main tasks used to train the base model,
givingusfurtherreasontobelieveBERTiswell-suitedforthistask.
3.1.2 PatInv-Remove. In this approach, we remove a word or a
phrase in our original sentence to generate new sentences. Our
maingoalistoremovesomethingmeaningfulfromasentence.For
example, Fig. 1shows the complete removal of a word from the
sentence. However, choosing words or phrases to remove is not
a trivial problem. A naive implementation of this approach may
remove all pairs of consecutive words, then triples of consecutive
words, etc. While this implementation surely will not miss any er-
ror that can be detected with this approach, it also leads to many
false positives.
Identifying meaningful phrases in a sentence can be aided by
using its constituency structure, i.e. the syntactic structure of a
sentence. Constituency structures show how a word or a group of
words form different units in a sentence, such as noun phrases or
verb phrases. This structure can be identified using a constituency
parser, which derives a representative parse tree by splitting a sen-
tence according to a set of phrase structure rules [ 17] defined by a
context-freegrammar.Thereareseveralapproachesforconstituency
parsing;currently,thestate-of-the-artmodelusesshift-reducepars-
ing [91]. This model parses grammar non-terminals from left to
right in a stack-like manner to produce a complete set of relations.
An implementation of [ 91], which we use, is available through
Stanford‚Äôs CoreNLP library [ 49].
Once a sentence is parsed, we identify all noun phrases (NP),
verb phrase (VB), prepositional phrase (PP), and adverb phrase
(ADVP) as candidates for removal, which can be done by recur-
sively moving through the constituency parse tree. We then form
a set of new sentences by removing each word/phrase in this set
from the original sentence. Specifically, each removal corresponds
to a new sentence.3.2 Step 2: Filtering by Syntactic and Semantic
Information
For our two sentence generation approaches, we apply filters, i.e.
remove some of the generated sentences, to reduce potential false
positives. Here we explain the two sets of filters.
3.2.1 PatInv-Replace. We find that some sentences generated us-
ing the masked language model (MLM) can be too semantically
similar to the original sentence or are syntactically incorrect. To
address this issue, we introduce three filtering mechanisms:
1)Filtering out synonyms. An MLM can predict words that are
synonyms of the original word. As we want to generate sentences
with meanings different from that of the original sentence, hav-
ing a word replaced by its synonym (e.g., ‚Äúgood talk‚Äù and ‚Äúnice
talk‚Äù) may lead to false positives. Therefore, we filter out those
caseswherethepredictedwordisasynonymoftheoriginalword.
ThisisdoneusingThesaurus.com[ 3]andWordsAPI[ 4]dictionary.
Specifically, we crawl Thesaurus.com to generate a list of syn-
onyms for each of the predicted words. We likewise used the api
provided by WordsAPI to find synonyms. We note that WordsAPI
alsoincludeshierarchicalinformation,suchasknowingthatahatch-
back is a type of car, a finger is a part of a hand, etc. In our imple-
mentation, we check whether the predicted word is a synonym or
type of the original word. We filter out these cases.
Interestingly, there are a few cases where similar words can-
not be detected by either of the services mentioned. For exam-
ple, ‚Äúconfirmed‚Äù and ‚Äúaffirmed‚Äù can have the same meaning in cer-
taincontextsbutneitherthesaurusnorWordsAPItagthemassyn-
onyms. Similarly, ‚Äúsay‚Äù and ‚Äúsaid‚Äù are conjugations of the same
verb but are not identified as synonyms. In general, we want to
eliminate cases where the base of the predicted word is a syn-
onym of or equivalent to the base of the original word. As a so-
lutiontothisproblem,weuseNLTK‚ÄôsWordNetLemmatizertoget
the base form of the verb. WordNet Lemmatizer lemmatizes us-
ing WordNet‚Äôs built-in Morphy function. It returns the input word
unchanged if it cannot be found in WordNet. A similar situation
occurs for words with the same stem, i.e. ‚Äúselect‚Äù and ‚Äúselective‚Äù;
when both are used as adjectives, replacing one with the other in
a sentence will generally not lead to a different semantic meaning.
866Machine Translation Testing via Pathological Invariance ESEC/FSE ‚Äô20, November 8‚Äì13, 2020, Virtual Event, USA
We used NLTK‚Äôs snowball stemmer to identify the word stem and
elimante cases where the stem of the predicted and original words
are the same.
2)Filtering by constituency structure. Inpractice,replacingaword
in a sentence with a word of a different part-of-speech can lead
to semantically or syntactically incorrect sentences. For example,
in figure 1when ‚Äúcompletely‚Äù is replaced by ‚Äúaround‚Äù we end up
withagrammaticallyincorrectsentence.Suchcasescanbeavoided
by ensuring the original and generated sentence have the same
constituency structure. For this task, we again use the implemen-
tation of [ 91] available in NLTK‚Äôs CoreNLPParser to generate a
constituency parse. We then calculate the distance between the
constituency parse trees of the original and generated sentence.
We use tree-edit distance as our distance measurement, as imple-
mentedinpython‚ÄôsAPTEDlibrary.Betweeneachoriginalandgen-
erated sentence, we ensure tree edit distance is equal to 1, i.e. only
one leaf node, the replaced word, has changed. Note that a dis-
tance of 1 means the part-of-speech of the replaced word has not
changed since part-of-speech tags correspond to nonterminals in
the tree.
3)Filtering by sentence embeddings.4Asanadditionalprecaution
to ensure generated sentences do not have the same meaning as
the original sentence, we ensure the ‚Äúdistance‚Äù between sentences
is sufficiently large. To mimic a linguistic notion of distance, we
take the vector distance between the embedding of the original
and generated sentences. There are several out-of-the-box options
available for generating sentence embeddings, such as the Univer-
sal Sentence Encoder [ 13] and BERT. While training a sentence
encoder would also be possible, we note that these state-of-the-art
models are readily available and have proven to perform well on
the task.
In PatInv, we used the Universal Sentence Encoder, a model
for encoding sentences into vectors based on encoders parameter-
ized by deep neural networks. The Universal Sentence Encoder is
freely available on TensorFlow Hub. The similarity between two
sentences is then calculated as the cosine similarity of the two
sentence embeddings. We filter out the generated sentences with
similarity (to its original sentence) greater than a given threshold,
which is a tunable parameter.
While there is some redundancy in the above filtering steps, us-
ing all three ensures a minimal number of false positives, which is
critical for reducing manual effort.
3.2.2 PatInv-Remove. Sentences generated using PatInv-Remove
may face similar problems, i.e. they may be nonsensical or retain
their original meaning. Interestingly, we find that removing small
words in the original sentence often leads to the latter problem.
This is the case even when the word is not explicitly a stopword.
For example, in our experiments, we remove the word ‚Äúfound‚Äù
from the sentence, ‚ÄúWhether there is a political gain to be found
in dishonoring a servant‚Äù and the translation system returns the
same results. While it can be argued that there is an erroneous
translation here, the sentence after word removal does not have
a clearly different meaning. We empirically find that removing
words of character length only greater than 6 alleviates this issue.
4optional step to increase precision as discussed in section 4Tofind a suitable lowerbound, wetune this parameter on the Poli-
tics dataset with En-Hi language setting from ‚Äú0‚Äù to ‚Äú10‚Äù with step
size‚Äú1.‚Äù‚Äú6‚Äùwasfoundtobethelowerboundthatledtodecentpre-
cision and number of pathological invariants. In our experiments,
we found that this lower bound also led to high-quality results in
other experimental settings.
3.3 Step 3: Collecting Target Sentences
Once modified sentences have been generated, we must then feed
them to the translation system under test and retrieve their trans-
lations. In the case of Google Translate and Bing Microsoft Trans-
lator, which we evaluate PatInv on, we use the Google API and
BingAPI,whichreturnthesameresultsastheirrespectivewebin-
terfaces. Specifically, sentences in a source language are fed to the
translation system and their translations, in a chosen target lan-
guage, are returned. We do this for each of our original sentences
and all of the generated sentences.
3.4 Step 4: Detecting Translation Errors
Finally, once all translations are collected, we must search for er-
roneous translations. This component of our methodology is quite
straightforward;aswehavegeneratedsentencesthatdifferinmean-
ing from their respective original sentences, if the translations of
the original and generated sentences are the same, we have good
reasontosuspectanerroneoustranslation.Thischeckcanbedone
via a simple string comparison which checks for equality. Fig. 1
shows an example of such an erroneous translation pair.
4 EVALUATION
In this section, we evaluate PatInv by applying both of its imple-
mentations, PatInv-Replace and PatInv-Remove, to Google Trans-
late and Bing Microsoft Translator with real-world sentences as
input. Our main research questions are:
RQ1: How effective is PatInv at finding erroneous transla-
tions?
RQ2: Where do the false positives and false negatives come
from?
RQ3: Can PatInv find erroneous translations that existing
approaches cannot find?
RQ4: How efficient is PatInv?
RQ5: How does one tune the parameters of PatInv in prac-
tice?
Inansweringthesequestions,weshowthatbothimplementations
areeffectiveinfindingerroneoustranslationsandmostoftheerro-
neoustranslationsfoundcannotbedetectedbyexistingapproaches.
Additionally,theapproachisefficientandiseasytouseinpractice.
4.1 Experimental Setup
Environment. AllexperimentsarerunonDALCOr2264l6gRack-
mount Server with 2x12 Core Intel Xeon E5-2650 v4 2.2GHz Pro-
cessor, 256GB DDR4 2400MHz ECC REG Server Memory and 8x
nVidia RTX 2080TI 11GB GPU Processor.
Dataset. To evaluate the performance of PatInv and align with
the SOTA, we use the dataset released in our previous paper [ 30].
Specifically, this dataset contains 200 English sentences crawled
867ESEC/FSE ‚Äô20, November 8‚Äì13, 2020, Virtual Event, USA Shashij Gupta, Pinjia He, Clara Meister, and Zhendong Su
Table 2: Statistics of input sentences for evaluation. Each cor-
pus contains 100 sentences.
Corpus# of words/ Average # of # of words
sentence words/sentence Total Distinct
Politics 4-32 19.2 1,918 933
Business 4-33 19.5 1,949 944
from CNN (Cable News Network).5The dataset consists of articles
fromtwocategories:PoliticsandBusiness,whereeachdatasetcon-
tains 100 sentences. More statistics of the released dataset is illus-
trated in Table 2.
Labeling. The output of our approach is a list of suspicious is-
sues, each of which consists of a pair of sentences where the first
is the original sentence and the second is our generated one. We
manually label each issue reported by PatInv. Two of the authors
inspect all the results separately and discuss the labels for all the
reportedissuesuntilconvergence.Specifically,forPatInv-Replace,
we first check whether the generated sentence in the issue con-
tains any syntax or semantic errors. If so, the issue will be labeled
as a false positive (examples are in Table 7). Otherwise, we check
whether the original sentence and generated sentence are seman-
tically different. If not, the issue will be labeled as a false positive
(examples are in Table 6). Otherwise, we label the issue as a true
positive. Then, we decide which sentence(s) in this issue contains
translationerror(s).ForPatInv-Remove,thelabelingprocessissim-
ilartoPatInvexceptthatwewillnotlabelanissueasfalsepositive
because of syntax/semantic errors in the generated sentence.
Comparison. We compared PatInv with two state-of-the-art
approaches: SIT [ 30] and TransRepair [ 69]. Both approaches are
based on the intuition that similar source sentences should have
similar translations. For SIT, we directly used the source code re-
leased by the authors. For TransRepair, the authors did not release
their source code due to industrial confidentiality. Thus, we imple-
ment TransRepair by carefully following the explanations in their
paper and consulting the work‚Äôs main author for key implemena-
tion details. We re-tune the parameters for both SIT and TransRe-
pairfollowingthesamestrategiesintroducedinthepapers.In[ 69],
0.9 is used as the minimum cosine similarity of word embeddings
to generate word pairs. In our experiment, we lower the threshold
to 0.8 because otherwise, we were unable to reproduce the quan-
tity of pairs reported in the original paper, i.e., using 0.9 as the
threshold yielded two magnitudes fewer word pairs than reported
by TransRepair. The implementation of PatInv, SIT, and TransRe-
pair are all released for reuse [ 65].
4.2 The Effectiveness of PatInv
Both of our approaches aim to automatically find erroneous trans-
lationsinmachinetranslationsoftwareusingunlabelledsentences.
Thus the effectiveness of both of our approaches will be deter-
mined by (1) How many erroneous translations an approach can
find (2) With what precision an approach can find translation er-
rors. In this section, we show the effectiveness of our model by
testing Google translate and Bing Microsoft Translator for two
5https://edition.cnn.com/
(a) Politics Dataset
(b) Business Dataset
Figure 2: Precision v/s Recall trade-off threshold curve for
En-Hi in Google Translate
translation settings (English !Hindi and English !Chinese). We
calculatedourresultsseparatelyforeachdomainintheevaluation
dataset.
4.2.1 Effectiveness of PatInv-Replace. RecallthatforPatInv-Replace,
weusethesimilaritybetweenthesentenceembeddingvectorsgiven
by the Universal Sentence Encoder to check whether the original
sentence and newly generated have same meaning. For different
thresholds we filter out issues in which the pair of sentences have
similarity less than the chosen threshold; we calculate the follow-
ing metrics for the remaining issues:
precision : the number of erroneous issues divided by total
number of reported issues.
recall: the number of erroneous issues with similarity less
than threshold divided by total number of erroneous issues
when there is no threshold.
F1-score : the harmonic mean of precision and recall.
868Machine Translation Testing via Pathological Invariance ESEC/FSE ‚Äô20, November 8‚Äì13, 2020, Virtual Event, USA
Table 3: Precision, recall and F1-score of PatInv-Replace.
Google Translate
Setting Dataset TP FP TN FN Prec. Rec. F1
En-Hi Politics 28 16 0 0 0.64 1.0 0.78
En-Hi Business 50 18 13 6 0.74 0.89 0.81
En-Zh Politics 92 67 0 0 0.58 1.0 0.73
En-Zh Business 90 54 0 0 0.63 1.0 0.77
Bing Microsoft Translator
Setting Dataset TP FP TN FN Prec. Rec. F1
En-Hi Politics 1 0 12 1 1 0.5 0.67
En-Hi Business 1 0 12 0 1 1 1
En-Zh Politics 92 81 0 0 0.53 1 0.69
En-Zh Business 91 74 0 0 0.55 1 0.71
Table 4: The number of remaining sentences after each step.
(a) PatInv-Replace
Step No. Dataset No. of sentences
1 Politics 15675
1 Business 16091
2 Politics 11639
2 Business 12604
(b) PatInv-Remove
Step No. Dataset No. of sentences
1 Politics 2047
1 Business 2091
2 Politics 1619
2 Business 1660
(c) State-of-the-art approaches
Toolname Dataset No. of sentences
SIT Politics 2,068
SIT Business 1,957
TransRepair Politics 209
TransRepair Business 196
Results. Table4(a) shows the number of total sentences (orig-
inal sentences + newly generated sentences) after step 1 and step
2 of PatInv-Replace. For each translation setting, we count the to-
tal number of issues reported for each dataset. We manually check
andverifyeachissue,reportinghowmanyoftheseissuesareerro-
neous. Ultimately, we were able to achieve precision comparable
with those of the state-of-the-art methods.
We categorize erroneous issues by which of the two sentences
(original or generated) contain a translation error. We categorize
non-erroneous issues into two categories: (1) Issues in which both
sentences have the same meaning. (2) Issues in which the second
sentence is semantically or syntactically incorrect.
For each dataset (business and politics), we tune PatInv‚Äôs avail-
able parameter, i.e., the threshold similarity for sentence embed-
dings, and report the precision, recall and F1-score corresponding
each threshold. For the threshold with the best F1-score, we re-
port True Positives (TP), False Positives (FP), True Negatives (TN),Table 5: Result of PatInv-Remove for En-Hi.
Translator Dataset TP FP Precision
Google Politics 3 2 0.60
Google Business 3 1 0.75
Bing Politics 0 1 0
Bing Business 2 3 0.4
False Negatives (FN). Table 3shows results for Google Translate
and Bing Microsoft Translator. The results shows that we were
able to achieve high recall with precision comparable to the state-
of-the-art approaches as discussed in section 4.4. We note that re-
ducingthethresholddoesnotnecessarilyincreasetheprecisionas
a lower threshold does not eliminate semantic or syntactic errors;
this was specifically apparent in the En-Zh results for the Bing-
Business dataset, as shown in Table 10. We additionally find that
we can achieve 100% precision given a low enough threshold. We
were able to report 28 erroneous issues in total in this case. The
precision-recall trade-off curve is illustrated in Fig. 2.
4.2.2 Effectiveness of PatInv-Remove. Evaluation Metric. Like-
wise, the output of this approach consists of a list of issues where
the first sentence is the original sentence and the second sentence
is a generated one. The generated sentence is obtained by remov-
ing something meaningful (a word or a phrase) from the sentence.
As in our first approach, an issue is only reported when both the
original and generated sentences have the same translations. We
notethattherearetwopotentialcasesforeachissue:(1)something
meaningfulwasremovedfromtheoriginalsentence,inwhichcase
the issue is a true positive (TP) (2) nothing important or meaning-
ful was removed from the sentence, in which case the issue is a
false positive (FP).
Results. Table4(b) shows the number of total sentences (orig-
inal sentences + generated sentences) after step 1 and step 2 of
PatInv-Remove. For each translation pair, we count the total num-
ber of issues reported for each dataset and manually verified how
many of these issues were erroneous. Table 5shows results for
GoogleTranslateandBingMicrosoftTranslatorforEnglish !Hindi.
The results for English !Chinese are shown in Table 10, where
they are compared with state-of-the-art approaches in section 4.4.
The results shows that we were able to achieve precision compa-
rable to state-of-the-art approaches. For each erroneous issue, we
labelwhichsentenceamongthepairhasanerroneoustranslation.
4.3 False Positives and False Negatives
Despite applying multiple filters, there were still some false posi-
tives in both of our approaches. Specifically, we encountered five
sources of false positives. 1) The generated sentence is syntactically
or semantically incorrect as shown in Table 7. Note that our ap-
proach minimizes these false positives; we have used a masked
language model built on top of BERT, a high-performing language
representation model. Employing this strategy should lead to very
few syntactically or semantically incorrect sentences. 2) The re-
placed word is a synonym of the original word. Despite combining
multiple large online databases to filter out synonyms, such cases
still occurred. The main source of such false positives was when
869ESEC/FSE ‚Äô20, November 8‚Äì13, 2020, Virtual Event, USA Shashij Gupta, Pinjia He, Clara Meister, and Zhendong Su
Table 6: False Positives for PatInv-Replace due to same
meaning.
SourceAnd this would takeus closer to our
goal of a truly European banking sector.
Newly GeneratedAnd this would allowus closer to our
goal of a truly European banking sector.
Translation (En !Hi)‡§î‡§∞ ‡§Ø‡§π ‡§π‡§Æ»Ö ‡§µ‡§æƒ•‡§§‡§µ ‡§Æ»Ö ‡§Ø‡•Ç‡§∞‡•ã‡§™‡•Ä‡§Ø ‡§¨»à»´‡§ï ‡§ó W‡•á≈õ ‡§ï ‡•á ‡§π‡§Æ‡§æ‡§∞ ‡•á
‡§≤ƒ®‡§Ø ‡§ï ‡•á ‡§ï‡§∞…ç‡§¨ ‡§≤‡•á ‡§ú‡§æ‡§è‡§ó‡§æ‡•§
Translation MeaningAnd this would takeus closer to our
goal of a truly European banking sector.
SourceNielsen said the most serious cyber threats
are those aimedat the heart of democracy.
Newly GeneratedNielsen said the most serious cyber threats
are those posedat the heart of democracy.
Translation (En !Hi)‡§®‡•Ä‡§≤‡§∏‡§® ‡§®‡•á ‡§ï‡§π‡§æ »ü‡§ï ‡§∏‡§¨‡§∏‡•á ‡§ó‡§Ç‡§≠‡•Ä‡§∞ ‡§∏‡§æ‡§á‡§¨‡§∞ ‡§ñ‡§§‡§∞ ‡•á ‡§≤‡•ã‡§ï‡§§‡§Ç≈õ
‡§ï ‡•á ‡§ï »Ö ≈ù ‡§Æ»Ö ‡§π »à ‡•§
Translation MeaningNielsen said that the most serious cyber threats
are at the heart of democracy.
Table 7: Syntactically/semantically wrong sentences gener-
ated by PatInv-Replace.
SourceTheyleftout that the pilots were not trained
to handle it.
Newly GeneratedTheyleavesout that the pilots were not trained
to handle it.
Translation (En !Hi)‡§âƒô‡§π»ã‡§®‡•á ‡§ï‡§π‡§æ »ü‡§ï ‡§™‡§æ‡§Ø‡§≤‡§ü»ã ‡§ï‡•ã ‡§á‡§∏‡•á ‡§∏‡§Ç‡§≠‡§æ‡§≤‡§®‡•á ‡§ï ‡•á »¢‡§≤‡§è
≈†»°‡§∂»¢W‡§§ ‡§®‡§π»Ç »ü‡§ï‡§Ø‡§æ ‡§ó‡§Ø‡§æ ‡§•‡§æ‡•§
Translation MeaningTheysaidthat the pilots were not trained
to handle it.
SourceThe key to accepting praise at work is to show
you received it and appreciate it.
Newly GeneratedThe key to accepting praise at work is to take
you received it and appreciate it.
Translation (En !Hi)‡§ï‡§æ‡§Æ ‡§™‡§∞ ≈†‡§∂‡§Ç‡§∏‡§æ ƒ•‡§µ‡•Ä‡§ï‡§æ‡§∞ ‡§ï‡§∞‡§®‡•á ‡§ï…è ‡§ï ‡•Å‡§Ç ‡§ú‡•Ä ‡§Ø‡§π ‡§π ‡•à »ü‡§ï
‡§Ü‡§™ ‡§á‡§∏‡•á ≈†‡§æƒö‡§§ ‡§ï‡§∞ »Ö ‡§î‡§∞ ‡§á‡§∏‡§ï…è ‡§∏‡§∞‡§æ‡§π‡§®‡§æ ‡§ï‡§∞ »Ö ‡•§
Translation MeaningThe key to accepting praise at work is to
receive it and appreciate it.
Table 8: False Positives generated by PatInv-Remove.
SourceFirst, proper training for pilots who are flying
new aircraft is crucially important .
Newly GeneratedFirst, proper training for pilots who are flying
new aircraft is crucially .
Translation (En !Hi)‡§™‡§π‡§≤‡§æ, ‡§®‡§è »ü‡§µ‡§Æ‡§æ‡§® ‡§â‡•ú‡§æ‡§®‡•á ‡§µ‡§æ‡§≤‡•á ‡§™‡§æ‡§Ø‡§≤‡§ü»ã ‡§ï ‡•á »¢‡§≤‡§è ‡§â»°‡§ö‡§§
≈†»°‡§∂W‡§£ ‡§Æ‡§πƒï‡§µ‡§™‡•Ç‡§£ƒÉ ‡§π ‡•à ‡•§
Translation MeaningFirst, proper training for pilots who are flying
new aircraft is important .
SourceThreemonths later, Holmes was indicted on federal
wire fraud charges and stepped down from Theranos.
Newly GeneratedThree later, Holmes was indicted on federal wire
fraud charges and stepped down from Theranos.
Translation (En !Zh)‰∏â‰∏™ÊúàÂêéÔºåÈúçÂßÜÊñØÂõ†ËÅîÈÇ¶ÁîµÊ±áÊ¨∫ËØàÊåáÊéßË¢´Ëµ∑ËØâÔºå
Âπ∂ËæûÂéª‰∫Ü Theranos ÁöÑËæûËÅå„ÄÇ
Translation MeaningThreemonths later, Holmes was indicted on federal
wire fraud charges and stepped down from Theranos.
our databases did not identify words as synonymous due to differ-
encesintenseorpluralityfromtherelevantwordregisteredintheTable 9: False Negatives generated by PatInv.
SourceThe key to accepting praise at work is to show
you received it and appreciate it.
Newly GeneratedThe key to accepting praise at work is to think
you received it and appreciate it.
similarity 0.993
SourceActress Jennifer Lawrence is also expected to
star as Holmes in a movie basedon Bad Blood.
Newly GeneratedActress Jennifer Lawrence is also expected to
star as Holmes in a movie titledon Bad Blood.
similarity 0.990
database. While we identified word stems with available tools, we
note that this approach does not always work as expected. For ex-
ample, NLTK‚Äôs snowball stemmer returns the stems ‚Äúunivers‚Äù for
‚Äúuniversities‚Äùand‚Äúcolleg‚Äùfor‚Äúcollege,‚Äùwhichthencannotbeused
with a synonym database as they are not standalone words. 3) The
replaced word in a sentence does not carry much meaning. We find
thatreplacingsomewordsdoesnotchangethemeaningofthesen-
tence because the replaced wordwas not vital for the meanning of
thesentence.ExamplesofsuchcaseshasbeenprovidedinTable 6.
To reduce such cases, we remove generated sentences where sen-
tence embedding similarity is high, implying semantic similarity
between the original and generated sentences. 4) Something mean-
ingful is not removed from the sentence. We find that only remov-
ing words and phrases with length > 6 helps to minimize these
cases. 5) The translation system successfully ‚Äúpredicts‚Äù the removed
word/phrase. In particular, it is possible that something meaning-
ful is removed from the original sentence and a syntactically- or
semantically-incorrect sentence is generated, but the translation
system still returns the removed word translation for the gener-
ated sentence. Examples are illustrated in Table. 8.
These false positives can be further reduced to make our ap-
proaches more efficient. For example, syntactically incorrect sen-
tences can be eliminated by using a good grammar checker. To
eliminate cases of semantically incorrect sentences, we can train a
corresponding classifier. We note that there were also some false
negatives produced in ‚Äúfiltering by sentence embeddings‚Äù step of
PatInv-Replace, which are shown in Table 9.
4.4 Errors Reported by Different Approaches
In this section, we compare PatInv with SIT and TransRepair in
termsofprecisionandoverlapoferroneoustranslationsfound.The
comparison is done with En-Zh language setting because SIT only
provides En-Zh implementation.
Precision. First, we compare the precision of PatInv with SIT
[30] and TransRepair [ 69] because precision is used as a core eval-
uationmetricinbothpapers.TheresultsarepresentedinTable 10.
PatInv-Remove achieves comparable precision compared with ex-
isting approaches. When the threshold for filtering by sentence
semantics is 1.0 (i.e., no filtering in this step), PatInv-Replace ob-
tains more erroneous issues with slightly lower precision. When
we decrease the threshold (more generated sentences are filtered
out),theprecisionofPatInv-Replacewillincreaseaccordingly.For
870Machine Translation Testing via Pathological Invariance ESEC/FSE ‚Äô20, November 8‚Äì13, 2020, Virtual Event, USA
Table 10: Precision and the number of erroneous issues using different threshold values (En-Zh)
PatInv-ReplacePatInv-Remove SIT TransRepair1.00 0.99 0.95 0.90
Google-Politics 57.9% (92) 68.5% (61) 72.2% (13) 75.0% (3) 42.9% (6) 65.3% (34) 64.2% (45)
Google-Business 62.5% (90) 70.0% (56) 67.4% (31) 76.7% (23) 76.9% (10) 64.7% (33) 61.1% (22)
Bing-Politics 53.1% (92) 60.0% (63) 61.1% (22) 66.7% (6) 61.5% (16) 70.5% (36) 70.5% (24)
Bing-Business 55.2% (91) 61.8% (60) 53.5% (23) 65.4% (17) 58.4% (14) 62.7% (32) 55.0% (22)
GoogleTranslatePatInvBingMicrosoftTranslator4TransRepairSIT62126280195
PatInv6TransRepairSIT5394483200
Figure 3: Erroneous Translations reported by PatInv, SIT
and TransRepair (En-Zh)
example, when the threshold is 0.99, PatInv-Replace finds more er-
roneous issues with comparable precision. Thus, consider the per-
formance of PatInv-Replace and PatInv-Remove, the effectiveness
of PatInv on accurately finding erroneous translations is compara-
ble to the state-of-the-art approaches.
Overlap of erroneous translations. Fig.3presents the over-
lapoftheerroneoustranslationsreportedbyPatInv,SITandTran-
sRepair.ThisVenndiagramshowsthatmostoftheerroneoustrans-
lationsreportedbyPatInvcannotbefoundbyexistingapproaches.
Specifically, only 8.4% (18/213) and 8.2% (18/218) erroneous trans-
lations reported by PatInv can be found by either of the existing
approaches. Thus, we believe PatInv complements with SIT and
TransRepair.
4.5 Running Time of Our Approach
In this section, we present the running time of PatInv. For each
experimental setting, we run PatInv 10 times and report the aver-
age timing. Results are shown in Table 11. Note that the running
time of PatInv-Replace‚Äôs ‚Äúfiltering by sentence embeddings‚Äù step
is not counted in the total time of PatInv-Replace because it is an
optional step. Compared with other steps, this optional step is rel-
atively time-consuming. For example, our implementation takes
around 27 seconds for calculating the similarity of one sentence
pair due to expensive API calls.
As we can observe from Table 11, most of the time is consumed
in the filtering step and the translation step for PatInv-Replace.The filtering step of PatInv-Replace takes time because of the con-
stituency parser used (8.1 seconds per hundred sentences). The
translation step takes time because for each sentence in our ex-
periments, we invoke the translators‚Äô API, which include both the
translation time and the network communication. Compared with
existing approaches, PatInv-Replace takes more time because Pat-
Inv-Replace generates much more sentences (i.e., 31,766 for Pat-
Inv-Replace, 4,025 for SIT, and 405 for TransRepair) and thus Pat-
Inv-Replace needs to run the constituency parser and the transla-
tor‚Äôs API much more times, leading to longer running time. How-
ever, we believe PatInv is efficient for practical usage because it
is designed to be an offline approach. Moreover, with this longer
running time, PatInv-Replace has reported more erroneous trans-
lations (in Table 10). PatInv-Remove and TransRepair are much
faster because they generate much less sentences (Table 4).
4.6 Fine-Tuning with Errors Reported by
PatInv
In this section, we explore whether the reported erroneous trans-
lations can be utilized as a fine-tuning set for the NMT model to
quickly fix translation errors. Fine-tuning is a common practice in
NMT [18,68]. It is often used when developers intend to adapt a
trainedNMTmodeltoanewdomainofdata(e.g.,fromtexton‚Äúpol-
itics‚Äù to text on ‚Äúbusiness‚Äù). To simulate this situation, we train a
transformernetworkwithglobalattention[ 74]ontheWMT‚Äô18Zh-
En (Chinese-to-English) corpus [ 11], which contains 20M sen-
tence pairs. We use the fairseq framework [ 1] to create the model.
We note that transformers are currently the state-of-the-art mod-
elsforNMT.Wereversetheoriginaldirectionoftranslationsothat
wecanproperlycomparewithourexperimentsontheGoogleand
Bing translation systems.
WetestthismodelwithPatInv-ReplaceusingthePoliticsdataset
in Table 2. PatInv-Replace successfully finds 26 erroneous transla-
tions. We manually label them with correct translations and use
them as the dataset to fine-tune our NMT model. Specifically, we
train the model for another 6 epochs with only the fixed transla-
tions. As the dataset was small, training took <10mins. After
this fine-tuning, all the 26 erroneous translations are fixed. Mean-
while, the BLEU score on a held out test set remains the same.
Thus,theerroneoustranslationsreportedbyPatInvcanbeusedto
improvetherobustnessofmachinetranslationsoftware.Notethat
although error fixing for machine translation is an interesting and
important topic, it is not the focus of this paper. Thus, we regard it
as a promising future direction.
871ESEC/FSE ‚Äô20, November 8‚Äì13, 2020, Virtual Event, USA Shashij Gupta, Pinjia He, Clara Meister, and Zhendong Su
Table 11: Running time of PatInv
Google
Politics
HindiGoogle
Business
HindiGoogle
Politics
ChineseGoogle
Business
ChineseBing
Politics
HindiBing
Business
HindiBing
Politics
ChineseBing
Business
Chinese
Initialization
PatInv-Replace29.62 29.44 27.49 27.58 29.62 29.44 27.49 27.58
Pair Generation 129.94 120.92 129.94 120.92 129.94 120.92 129.94 120.92
Filtering 2231.55 2313.04 2231.55 2313.04 2231.55 2313.04 2231.55 2313.04
Translation 1600.47 1726.74 2530.22 2735.07 1775.17 1915.8 3068.59 3310.73
Detection 0.002 0.0022 0.0019 0.0023 0.0017 0.0025 0.0019 0.0022
Total 3991.29 4190.14 4919.2 5196.61 4166.29 4379.2 5457.57 5772.27
Initialization
PatInv-Remove3.03 3.06 3.03 3.06 3.03 3.06 3.03 3.06
Pair Generation 15.20 16.14 15.20 16.14 15.20 16.14 15.20 16.14
Filtering 0.00002 0.00002 0.00002 0.00002 0.00002 0.00002 0.00002 0.00002
Translation 220.78 228.95 246.15 252.98 344.34 360.11 425.14 435.76
Detection 0.0019 0.002 0.0019 0.0022 0.0023 0.0021 0.0018 0.0021
Total 239.01 248.15 264.38 272.18 362.57 379.31 443.37 454.96
SIT NA NA 1360.91 1285.73 NA NA 2303.93 2214.46
TransRepair NA NA 18.37 15.11 NA NA 69.92 66.01
5 RELATED WORK
5.1 Robust AI Software
Throughout recent years, there has been much work and develop-
ment in the area of artificial intelligence (AI). AI is used every-
where today, such as in autonomous cars and face recognition.
However, modern AI models are not as robust as we might hope.
In particular, they can return incorrect results (e.g., wrong clas-
sification labels) that lead to fatal accidents [ 37,39,92]. Recent
research has reported a variety of adversarial examples that can
foolAIsoftware,suchasautonomouscars[ 8,21,27,83],3Dobject
classifiers[ 80,82],andspeechrecognitionservices[ 12,22].Topro-
mote robustness, lines of research have focused on testing, [ 23,26,
31,36,44,59,62,72,79,85,86], debugging [ 45], detecting adver-
sarial examples [ 46,70,75,81], or training networks in a robust
way [35,42,48,57]. However, none of these papers explore ma-
chine translation, which is the main focus of our paper.
5.2 Robust NLP Systems
The recent advances in the robustness of computer vision systems
have encouraged researchers to investigate the robustness of NLP
systemsperformingtaskssuchassentimentanalysis[ 7,32,40,63],
textual entailment [ 32] and toxic content detection [ 40]. However,
all of these studies work on classification problems which have a
much smaller output space than machine translation. Robustness
of more complex NLP system has also been explored in a a few
recent studies. In particular, Jia and Liang [ 34] proposed an ad-
versarial evaluation scheme for the Stanford Question Answering
Dataset (SQuAD), which has been widely used for the evaluation
of reading comprehension systems. They found that appending a
specially designed sentence to the paragraph can mislead the well-
trainedreadingcomprehensionsystems.Additionally,Mudrakarta
etal.[52]successfullyattackedquestionansweringmodels.Inpar-
ticular, they leverage the finding that deep networks often ignore
important question terms and thus they perturb the questions ac-
cordingly. Compared with machine translation, these systems areeasier to verify because the ground truth is unique. For example,
the output of reading comprehension could be a specific person
name. For the task of translation, there could be multiple correct
translationsforagivensentence,whichmakesdetectingincorrect
outputs incredibly nuanced.
5.3 Robust Machine Translation
Machine translation strives to translate text in a source language
to text in a target language automatically. There are primarily two
linesofworkontherobustnessofmachinetranslationsoftware:(1)
adversarial machine learning and (2) machine translation testing.
Adversarial machine learning aims at fooling machine transla-
tion models with adversarial examples. Most of the existing tech-
niques generate such examples by white-box methods [ 14,50,87],
in which complete knowledge of network structure and parame-
tersofthemachinetranslationmodelisavailable.Unlikethem,Pat-
Inv takes a black-box approach. Existing black-box techniques [ 9,
24,33] perturb or paraphrase sentences that can easily lead to
invalid sentences (e.g., syntactic or semantic errors). In compari-
son, this paper aims at generating syntactically- and semantically-
correct test cases (i.e., source sentences).
Machine translation testing aims at finding syntactically- and
semantically-correct sentences that trigger translation errors. [ 76,
89]proposedtwospecializedmethodstodetecttwokindsoftrans-
lationerrorsrespectively(under-translationandover-translation).
In contrast, PatInv is a general methodology that targets general
translationerrors.Heetal.[ 30]andSunetal.[ 69]developedmeta-
morphictestingtechniquesforgeneraltranslationerrorsbasedon
the assumption that similar sentences should have similar trans-
lations (evaluated by sentence structures [ 30] or existing distance
metrics [ 69]). Sun et al. [ 69] also further proposed an automated
repair approach. PatInvcomplements these approaches because it
is based on a brand new concept named pathological invariance:
sentences of different meanings should not have the same transla-
tion. Our evaluation has also shown that PatInv can report many
872Machine Translation Testing via Pathological Invariance ESEC/FSE ‚Äô20, November 8‚Äì13, 2020, Virtual Event, USA
erroneoustranslationsthatthestate-of-the-artapproaches[ 30,69]
cannot report. Thus, we believe PatInv can complement these ap-
proaches.Inaddition,allexistingworkswereevaluatedbyonelan-
guage setting (En-Zh), while PatInv is evaluated by two language
settings (En-Hi and En-Zh).
5.4 Metamorphic Testing
Metamorphic testing techniques find software bugs by detecting
the violation of necessary functionalities (i.e., metamorphic rela-
tions) of software under test [ 15,16,67]. As an effective approach
for addressing the test oracle and test case generation problems,
metamorphic testing has been adopted in the testing phase of a
variety of software systems, such as compilers [ 38,41], scientific
libraries[ 84],anddatabasesystems[ 43].Inaddition,metamorphic
testinghasalsobeenusedinAIsoftwaretestingbecauseofitsabil-
ity to test ‚Äúnon-testable‚Äù programmings, such as statistical classi-
fiers [53,78], search engines [ 90], and autonomous cars [ 72,86].
PatInvisanovelmetamorphictestingapproachformachinetrans-
lation software.
6 CONCLUSION
In this paper, we present PatInv, a novel, effective, and widely
applicable methodology for finding translation errors in machine
translationsystems.Incontrasttoexistingapproachesthatrelyon
invariances between translations (e.g., structural invariance [ 30]),
PatInv is based on pathological invariance: sentences of different
meaningshaveidenticaltranslation.Becauseofthissignificantcon-
ceptual difference, our implementations of PatInv have reported
many erroneous translations that cannot be found by existing ap-
proaches.Inourevaluation,PatInvsuccessfullyfinds100erroneous
translations for En-Hi and 431 erroneous translations for En-Zh
respectively in Google Translate and Bing Microsoft Translator. In
particular,among the errors reported for En-Zh, 395 out of 431 are
uniquetoPatInv.Inaddition,PatInvperformsaccuratelyandises-
peciallyeffectiveinfindingword/phrasemistranslationerrorsand
under-translation errors. Thus, PatInv complements with existing
approaches. In the future, we will provide more implementations
ontopofPatInv‚Äôscoreidea,forexample,generatingnewsentences
by inserting words to detect over-translation errors. We will also
applythegeneralideatovalidateothertextgenerationtasks,such
as speech recognition and code summarization.
ACKNOWLEDGMENTS
We thank the anonymous ESEC/FSE reviewers for their valuable
feedback on the earlier draft of this paper. In addition, the tool im-
plementation benefited tremendously from Stanford NLP Group‚Äôs
language parsers [ 28] and Hugging Face‚Äôs BERT implementation
in PyTorch [ 25]. Pinjia He is the corresponding author.
REFERENCES
[1][n.d.]. fairseq:AFast,ExtensibleToolkitforSequenceModeling. https://github.
com//pytorch/fairseq
[2][n.d.]. Google Translate. https://translate.google.com
[3][n.d.]. Thesaurus. https://www.thesaurus.com/
[4][n.d.]. WordsAPI. https://www.wordsapi.com/
[5]2018. 15,000EggsDeliveredtoNorwegianOlympicTeamAfterGoogleTranslate
Error. https://www.nbcwashington.com/news/national-international/google-
translate-fail-norway-olympic-team-gets-15k-eggs-delivered/2034392/[6]2018. Greedy, Brittle, Opaque, and Shallow: The Downsides to Deep Learn-
ing. https://www.wired.com/story/greedy-brittle-opaque-and-shallow-the-
downsides-to-deep-learning/
[7]Moustafa Alzantot, Yash Sharma, Ahmed Elgohary, Bo-Jhang Ho, Mani Srivas-
tava, and Kai-Wei Chang. 2018. Generating Natural Language Adversarial Ex-
amples. In Proceedings of the 2018 Conference on Empirical Methods in Natural
Language Processing .
[8]Anish Athalye, Nicholas Carlini, and David Wagner. 2018. Obfuscated Gradi-
ents Give a False Sense of Security: Circumventing Defenses to Adversarial Ex-
amples. In Proceedings of the 35th International Conference on Machine Learning
(ICML).
[9]Yonatan Belinkov and Yonatan Bisk. 2018. Synthetic and Natural Noise Both
Break Neural Machine Translation. In Proceedings of the 6th International Con-
ference on Learning Representations (ICLR) .
[10]Edward Loper Bird, Steven and Ewan Klein. 2009. Natural Language Processing
with Python. O‚ÄôReilly Media Inc.
[11]Ond rej Bojar, Christian Federmann, Mark Fishel, Yvette Graham, Barry Had-
dow, Matthias Huck, Philipp Koehn, and Christof Monz. 2018. Findings of the
2018 Conference on Machine Translation (WMT18). In Proceedings of the Third
Conference on Machine Translation, Volume 2: Shared Task Papers . Association
forComputationalLinguistics,Belgium,Brussels,272‚Äì307. http://www.aclweb.
org/anthology/W18-6401
[12]Nicholas Carlini, Pratyush Mishra, Tavish Vaidya, Yuankai Zhang, Micah Sherr,
Clay Shields, David Wagner, and Wenchao Zhou. 2016. Hidden Voice Com-
mands.In Proceedings of the 25th USENIX Security Symposium (USENIX Security) .
[13]Daniel Cer, Yinfei Yang, Sheng-yi Kong, Nan Hua, Nicole Limtiaco, Rhomni St
John, Noah Constant, Mario Guajardo-Cespedes, Steve Yuan, Chris Tar, et al.
2018. Universal sentence encoder. arXiv preprint arXiv:1803.11175 (2018).
[14]Akshay Chaturvedi, Abijith KP, and Utpal Garain. 2019. Exploring the Robust-
ness of NMT Systems to Nonsensical Inputs. arXiv preprint arXiv:1908.01165
(2019).
[15]TsongY. Chen, Shing C. Cheung, and Shiu Ming Yiu. 1998. Metamorphic testing:
a new approach for generating next test cases . TechnicalReport.TechnicalReport
HKUST-CS98-01, Department of Computer Science, Hong Kong University of
Science and Technology, Hong Kong.
[16]Tsong Yueh Chen, Fei-Ching Kuo, Huai Liu, Pak-Lok Poon, Dave Towey, T. H.
Tse, and Zhi Quan Zhou. 2018. Metamorphic Testing: A Review of Challenges
and Opportunities. ACM Computing Surveys (CSUR) 51 (2018). Issue 1.
[17]N. Chomsky. 1957. Syntactic Structures . Mouton, The Hague.
[18]Chenhui Chu, Raj Dabre, and Sadao Kurohashi. 2017. An empirical comparison
ofsimpledomainadaptationmethodsforneuralmachinetranslation.In Proceed-
ings of the 55th Annual Meeting of the Association for Computational Linguistics
(ACL).
[19]Gareth Davies. 2017. Palestinian man is arrested by police after posting
‚ÄôGood morning‚Äô in Arabic on Facebook which was wrongly translated as ‚Äôat-
tackthem‚Äô. https://www.dailymail.co.uk/news/article-5005489/Good-morning-
Facebook-post-leads-arrest-Palestinian.html
[20]JacobDevlin,Ming-WeiChang,KentonLee,andKristinaToutanova.2018. Bert:
Pre-training of Deep Bidirectional Transformers for Language Understanding.
arXiv preprint arXiv:1810.04805 (2018).
[21]Yinpeng Dong, Qi-An Fu, Xiao Yang, Tianyu Pang, Hang Su, Zihao Xiao,
and Jun Zhu. 2019. Benchmarking Adversarial Robustness. arXiv preprint
arXiv:1912.11852 (2019).
[22]TianyuDu,ShoulingJi,JinfengLi,QinchenGu,TingWang,andRaheemBeyah.
2019. SirenAttack: Generating Adversarial Audio for End-to-End Acoustic Sys-
tems. arXiv preprint arXiv:1901.07846 (2019).
[23]Xiaoning Du, Xiaofei Xie, Yi Li, Lei Ma, Yang Liu, and Jianjun Zhao. 2019. Deep-
Stellar: Model-based quantitative analysis of stateful deep learning systems. In
ESEC/FSE 2019 - Proceedings of the 2019 27th ACM Joint Meeting European Soft-
ware Engineering Conference and Symposium on the Foundations of Software En-
gineering .
[24]Javid Ebrahimi, Daniel Lowd, and Dejing Dou. 2018. On Adversarial Examples
forCharacter-LevelNeuralMachineTranslation.In Proceedings of the 27th Inter-
national Conference on Computational Linguistics (COLING) .
[25]Hugging Face. [n.d.]. Transformers: State-of-the-art Natural Language Pro-
cessing for TensorFlow 2.0 and PyTorch. https://github.com/huggingface/
transformers
[26]Alessio Gambi, Marc Mueller, and Gordon Fraser. 2019. Automatically testing
self-drivingcarswithsearch-basedproceduralcontentgeneration.In Proc. of the
28th ACM SIGSOFT International Symposium on Software Testing and Analysis
(ISSTA).
[27]Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. 2015. Explaining and
Harnessing Adversarial Examples. Proceedings of the 3rd International Confer-
ence on Learning Representations (ICLR) .
[28]Stanford NLP Group. [n.d.]. Stanford CoreNLP ‚ÄìNatural language software.
https://stanfordnlp.github.io/CoreNLP/
[29]Hany Hassan, Anthony Aue, Chang Chen, Vishal Chowdhary, Jonathan Clark,
Christian Federmann, Xuedong Huang, Marcin Junczys-Dowmunt, William
873ESEC/FSE ‚Äô20, November 8‚Äì13, 2020, Virtual Event, USA Shashij Gupta, Pinjia He, Clara Meister, and Zhendong Su
Lewis, Mu Li, et al. 2018. Achieving Human Parity on Automatic Chinese to
English News Translation. arXiv preprint arXiv:1803.05567 (2018).
[30]PinjiaHe,ClaraMeister,andZhendongSu.2020. Structure-InvariantTestingfor
Machine Translation. In Proc. of the 42nd International Conference on Software
Engineering (ICSE) .
[31]J.Henriksson,C.Berger,M.Borg,L.Tornberg,C.Englund,S.R.Sathyamoorthy,
andS.Ursing.2019. TowardsStructuredEvaluationofDeepNeuralNetworkSu-
pervisors. In 2019 IEEE International Conference On Artificial Intelligence Testing
(AITest).
[32]Mohit Iyyer, John Wieting, Kevin Gimpel, and Luke Zettlemoyer. 2018. Adver-
sarial Example Generation with Syntactically Controlled Paraphrase Networks.
InProceedings of the 2018 Conference of the North American Chapter of the Asso-
ciation for Computational Linguistics: Human Language Technologies, Volume 1
(Long Papers) .
[33]Mohit Iyyer, John Wieting, Kevin Gimpel, and Luke Zettlemoyer. 2018. Adver-
sarial Example Generation with Syntactically Controlled Paraphrase Networks.
InProceedings of the 16th Annual Conference of the North American Chapter
of the Association for Computational Linguistics: Human Language Technologies
(NAACL-HLT) .
[34]Robin Jia and Percy Liang. 2017. Adversarial Examples for Evaluating Read-
ing Comprehension Systems. In Proceedings of the 2017 Conference on Empirical
Methods in Natural Language Processing (EMNLP) .
[35]Harini Kannan, Alexey Kurakin, and Ian Goodfellow. 2018. Adversarial Logit
Pairing. arXiv preprint arXiv:1803.06373 (2018).
[36]Jinhan Kim, Robert Feldt, and Shin Yoo. 2019. Guiding Deep Learning System
Testing using Surprise Adequacy. In Proceedings of the 41st International Confer-
ence on Software Engineering (ICSE) .
[37]Fred. Lambert. 2016. Understanding the fatal Tesla accident on Autopilot and
the NHTSA probe. https://electrek.co/2016/07/01/understanding-fatal-tesla-
accident-autopilot-nhtsa-probe/
[38]VuLe,MehrdadAfshari,andZhendongSu.2014. CompilerValidationviaEquiv-
alence Modulo Inputs. In ACM SIGPLAN Conference on Programming Language
Design and Implementation (PLDI) .
[39]Sam Levin. 2018. Tesla fatal crash: ‚Äôautopilot‚Äô mode sped up car before dri-
ver killed, report finds. https://www.theguardian.com/technology/2018/jun/
07/tesla-fatal-crash-silicon-valley-autopilot-mode-report
[40]Jinfeng Li, Shouling Ji, Tianyu Du, Bo Li, and Ting Wang. 2019. TextBugger:
Generating Adversarial Text Against Real-world Applications. In Proceedings of
the 26th Annual Network and Distributed System Security Symposium (NDSS) .
[41]Christopher Lidbury, Andrei Lascu, Nathan Chong, and Alastair F. Donaldson.
2015. Many-Core Compiler Fuzzing. In ACM SIGPLAN Conference on Program-
ming Language Design and Implementation (PLDI) .
[42]Ji Lin, Chuang Gan, and Song Han. 2019. Defensive Quantization: When Effi-
ciency Meets Robustness. In Proceedings of the 7th International Conference on
Learning Representations (ICLR) .
[43]MikaelLindvall,DharmalingamGanesan,Ragnar√Årdal,andRobertE.Wiegand.
2015. Metamorphic Model-based Testing Applied on NASA DAT-an experience
report. In Proceedings of the 37th International Conference on Software Engineer-
ing (ICSE) .
[44]Lei Ma, Felix Juefei-Xu, Fuyuan Zhang, Jiyuan Sun, Minhui Xue, Bo Li, Chun-
yang Chen, Ting Su, Li Li, Yang Liu, et al. 2018. Deepgauge: Multi-Granularity
TestingCriteriaforDeepLearningSystems.In Proceedings of the 33rd ACM/IEEE
International Conference on Automated Software Engineering (ASE) .
[45]Shiqing Ma, Yingqi Liu, Wen-Chuan Lee, Xiangyu Zhang, and Ananth Grama.
2018. MODE: Automated Neural Network Model Debugging via State Differen-
tial Analysis and Input Selection. In Proceedings of the 26th ACM Joint Meeting
on European Software Engineering Conference and Symposium on the Foundations
of Software Engineering (ESEC/FSE) .
[46]Shiqing Ma, Yingqi Liu, Guanhong Tao, Wen-Chuan Lee, and Xiangyu Zhang.
2019. NIC: Detecting Adversarial Samples with Neural Network Invariant
Checking. In Proceedings of the 26th Annual Network and Distributed System Se-
curity Symposium (NDSS) .
[47]Fiona Macdonald. 2015. The Greatest Mistranslations Ever. http://www.bbc.
com/culture/story/20150202-the-greatest-mistranslations-ever
[48]AleksanderMadry,AleksandarMakelov,LudwigSchmidt,DimitrisTsipras,and
Adrian Vladu. 2018. Towards Deep Learning Models Resistant to Adversarial
Attacks. In Proceedings of the 6th International Conference on Learning Represen-
tations (ICLR) .
[49]Christopher D. Manning, Mihai Surdeanu, John Bauer, Jenny Finkel, Steven J.
Bethard, and David McClosky. 2014. The Stanford CoreNLP Natural Language
Processing Toolkit. In Association for Computational Linguistics (ACL) System
Demonstrations .
[50]Paul Michel, Xian Li, Graham Neubig, and Juan Miguel Pino. 2019. On Eval-
uation of Adversarial Perturbations for Sequence-to-Sequence Models. In Pro-
ceedings of the 17th Annual Conference of the North American Chapter of the As-
sociation for Computational Linguistics: Human Language Technologies (NAACL-
HLT).[51]Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. Efficient Esti-
mation of Word Representations in Vector Space. arXiv e-prints (2013).
[52]Pramod K. Mudrakarta, Ankur Taly, Mukund Sundararajan, and Kedar Dhamd-
here. 2018. Did the Model Understand the Question?. In Proceedings of the 56th
Annual Meeting of the Association for Computational Linguistics (ACL) .
[53]Christian Murphy, Gail E. Kaiser, Lifeng Hu, and Leon Wu. 2008. Properties
of Machine Learning Applications for Use in Metamorphic Testing. In Proceed-
ings of the 20th International Conference on Software Engineering and Knowledge
Engineering (SEKE) .
[54]Arika Okrent. 2016. 9 Little Translation Mistakes That Caused Big Prob-
lems. http://mentalfloss.com/article/48795/9-little-translation-mistakes-
caused-big-problems
[55]Thuy Ong. 2017. Facebook apologizes after wrong translation sees Palestinian
man arrested for posting ‚Äôgood morning‚Äô. https://www.theverge.com/us-
world/2017/10/24/16533496/facebook-apology-wrong-translation-palestinian-
arrested-post-good-morning
[56]Myle Ott, Michael Auli, David Grangier, and Marc‚ÄôAurelio Ranzato. 2018. Ana-
lyzing Uncertainty in Neural Machine Translation. arXiv: cs.CL/1803.00047
[57]NicolasPapernot,PatrickMcDaniel,XiWu,SomeshJha,andAnanthramSwami.
2016. DistillationasaDefensetoAdversarialPerturbationsagainstDeepNeural
Networks. In IEEE Symposium on Security and Privacy .
[58]Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. BLEU: A
Method for Automatic Evaluation of Machine Translation. In Proceedings of the
40th Annual Meeting on Association for Computational Linguistics (ACL) .
[59]Kexin Pei, Yinzhi Cao, Junfeng Yang, and Suman Jana. 2017. Deepxplore: Auto-
mated Whitebox Testing of Deep Learning Systems. In Proceedings of the 26th
Symposium on Operating Systems Principles (SOSP) .
[60]Jeffrey Pennington, Richard Socher, and Christopher Manning. 2014. Glove:
Global vectors for word representation. In Proceedings of the 2014 Conference on
Empirical Methods in Natural Language Processing (EMNLP) .
[61]Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher
Clark, Kenton Lee, and Luke Zettlemoyer. 2018. Deep contextualized word rep-
resentations. arXiv e-prints (2018).
[62]Hung Viet Pham, Thibaud Lutellier, Weizhen Qi, and Lin Tan. 2019. CRADLE:
cross-backend validation to detect and localize bugs in deep learning libraries.
In2019 IEEE/ACM 41st International Conference on Software Engineering (ICSE) .
IEEE.
[63]Danish Pruthi, Bhuwan Dhingra, and Zachary C. Lipton. 2019. Combating Ad-
versarialMisspellingswithRobustWordRecognition.In Proc. of the 57th Annual
Meeting of the Association for Computational Linguistics (ACL) .
[64]Alec Radford. 2018. Improving Language Understanding by Generative Pre-
Training.
[65]RobustNLP.2020. Atoolkitfortestingmachinetranslation. https://github.com/
RobustNLP/TestTranslation
[66]Benny Royston. 2018. Israel Eurovision winner Netta called ‚Äòa real cow ‚Äôby
Prime Minister in auto-translate fail. https://metro.co.uk/2018/05/13/israel-
eurovision-winner-netta-called-a-real-cow-by-prime-minister-in-auto-
translate-fail-7541925/
[67]Sergio Segura, Gordon Fraser, Ana B. Sanchez, and Antonio Ruiz-Cort√©s. 2016.
A Survey on Metamorphic Testing. IEEE Transactions on Software Engineering
(TSE)42 (2016). Issue 9.
[68]Rico Sennrich, Barry Haddow, and Alexandra Birch. 2016. Improving neural
machine translation models with monolingual data. In Proceedings of the 54th
Annual Meeting of the Association for Computational Linguistics (ACL) .
[69]Zeyu Sun, Jie M Zhang, Mark Harman, Mike Papadakis, and Lu Zhang. 2020.
AutomaticTestingandImprovementofMachineTranslation.In Proc. of the 42nd
International Conference on Software Engineering (ICSE) .
[70]GuanhongTao,ShiqingMa,YingqiLiu,andXiangyuZhang.2018. AttacksMeet
Interpretability: Attribute-steered Detection of Adversarial Samples. In Proceed-
ings of the 34th Conference on Neural Information Processing Systems (NeurIPS) .
[71]Wilson L. Taylor. 1953. ‚ÄùCloze Procedure‚Äù: A New Tool for Measuring Readabil-
ity.Journalism Bulletin 30, 4 (1953), 415‚Äì433.
[72]Yuchi Tian, Kexin Pei, Suman Jana, and Baishakhi Ray. 2018. Deeptest: Auto-
mated Testing of Deep-Neural-Network-Driven Autonomous Cars. In Proceed-
ings of the 40th International Conference on Software Engineering (ICSE) .
[73]Barak Turovsky. 2016. Ten years of Google Translate. https://blog.google/
products/translate/ten-years-of-google-translate/
[74]Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
Aidan N. Gomez, and Illia Kaiser, Lukasz abd Polosukhin. 2017. Attention is All
youNeed.In Proceedings of the 33rd Conference on Neural Information Processing
Systems (NeurIPS) .
[75]JingyiWang,GuoliangDong,JunSun,XinyuWang,andPeixinZhang.2019. Ad-
versarial Sample Detection for Deep Neural Network through Model Mutation
Testing. In Proceedings of the 41st International Conference on Software Engineer-
ing (ICSE) .
[76]Wenyu Wang, Wujie Zheng, Dian Liu, Changrong Zhang, Qinsong Zeng, Yue-
tang Deng, Wei Yang, Pinjia He, and Tao Xie. 2019. Detecting Failures of Neural
Machine Translation in the Absence of Reference Translations. In Proc. of the
874Machine Translation Testing via Pathological Invariance ESEC/FSE ‚Äô20, November 8‚Äì13, 2020, Virtual Event, USA
49th IEEE/IFIP International Conference on Dependable Systems and Networks (in-
dustry track) .
[77]Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi,
Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al.
2016. Google‚Äôs Neural Machine Translation System: Bridging the Gap Between
Human and Machine Translation. arXiv preprint arXiv:1609.08144 (2016).
[78]Xiaoyuan Xie, Joshua WK Ho, Christian Murphy, Gail Kaiser, Baowen Xu, and
TsongYuehChen.2011. TestingandValidatingMachineLearningClassifiersby
Metamorphic Testing. Journal of Systems and Software (JSS) 84 (2011). Issue 4.
[79]Xiaofei Xie, Lei Ma, Felix Juefei-Xu, Minhui Xue, Hongxu Chen, Yang Liu, Jian-
jun Zhao, Bo Li, Jianxiong Yin, and Simon See. 2019. Deephunter: A coverage-
guided fuzz testing framework for deep neural networks. In ISSTA 2019 - Pro-
ceedings of the 28th ACM SIGSOFT International Symposium on Software Testing
and Analysis .
[80]Chong Xiong, Charles R. Qi, and Bo Li. 2019. Generating 3D Adversarial Point
Clouds.In Proceedings of the 2019 IEEE Conference on Computer Vision and Pattern
Recognition (CVPR) .
[81]Weilin Xu, David Evans, and Yanjun Qi. 2018. Feature Squeezing: Detecting Ad-
versarial Examples in Deep Neural Networks. In Proceedings of the 25th Annual
Network and Distributed System Security Symposium (NDSS) .
[82]Dawei Yang, Chaowei Xiao, Bo Li, Jia Deng, and Mingyan Liu. 2019. Realistic
Adversarial Examples in 3D Meshes. In Proceedings of the 2019 IEEE Conference
on Computer Vision and Pattern Recognition (CVPR) .
[83]Fuyuan Zhang, Sankalan Pal Chowdhury, and Maria Christakis. 2019.
DeepSearch: Simple and Effective Blackbox Fuzzing of Deep Neural Networks.
arXiv preprint arXiv:1910.06296 (2019).
[84]Jie Zhang, Junjie Chen, Dan Hao, Yingfei Xiong, Bing Xie, Lu Zhang, and Hong
Mei.2014. Search-BasedInferenceofPolynomialMetamorphicRelations.In Pro-
ceedings of the 29th ACM/IEEE International Conference on Automated SoftwareEngineering (ASE) .
[85]Jie M. Zhang, Mark Harman, Lei Ma, and Yang Liu. 2019. Machine Learning
Testing: Survey, Landscapes and Horizons. IEEE Transactions on Software Engi-
neering(2019).
[86]Mengshi Zhang, Yuqun Zhang, Lingming Zhang, Cong Liu, and Sarfraz Khur-
shid. 2018. Deeproad: Gan-Based Metamorphic Autonomous Driving System
Testing. In Proceedings of the 33rd ACM/IEEE International Conference on Auto-
mated Software Engineering (ASE) .
[87]Zhengli Zhao, Dheeru Dua, and Sameer Singh. 2018. Generating natural adver-
sarial examples. In Proceedings of the 6th International Conference on Learning
Representations (ICLR) .
[88]Wujie Zheng, Wenyu Wang, Dian Liu, Changrong Zhang, Qinsong Zeng, Yue-
tang Deng, Wei Yang, Pinjia He, and Tao Xie. 2018. Testing Untestable Neural
MachineTranslation:AnIndustrialCase. arXiv preprint arXiv:1807.02340 (2018).
[89]Wujie Zheng, Wenyu Wang, Dian Liu, Changrong Zhang, Qinsong Zeng, Yue-
tang Deng, Wei Yang, Pinjia He, and Tao Xie. 2019. Testing untestable neural
machine translation: an industrial case. In Proc. of the 41st International Confer-
ence on Software Engineering: Companion Proceedings .
[90]Zhi Quan Zhou, Shaowen Xiang, and Tsong Yueh Chen. 2016. Metamorphic
TestingforSoftwareQualityAssessment:AStudyofSearchEngines. IEEE Trans-
actions on Software Engineering (TSE) 42 (2016). Issue 3.
[91]MuhuaZhu,YueZhang,WenliangChen,MinZhang,andJingboZhu.2013. Fast
andAccurateShift-ReduceConstituentParsing.In Proceedings of the 51st Annual
Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) .
Association for Computational Linguistics.
[92]Chris. Ziegler. 2016. A Google self-driving car caused a crash for the first
time.https://www.theverge.com/2016/2/29/11134344/google-self-driving-car-
crash-report
875