Identifying and Describing Information Seeking Tasks
Chris Satterfield
University of British Columbia
Vancouver, CanadaThomas Fritz
University of Zurich
Zurich, SwitzerlandGail C. Murphy
University of British Columbia
Vancouver, Canada
ABSTRACT
A software developer works on many tasks per day, frequently
switching between these tasks back and forth. This constant churn
oftasksmakesitdifficultforadevelopertoknowthespecificsof
when they worked on what task, complicating task resumption,
planning,retrospection,andreportingactivities.Inafirststepto-
wardsanautomatedaidtothisissue,weintroduceanewapproach
to help identify the topic of work during an information seeking
task‚Äîoneofthemostcommontypesoftasksthatsoftwaredevelop-
ersface‚Äîthatisbasedoncapturingthecontentsofthedeveloper‚Äôs
active window at regular intervals and creating a vector represen-
tation of key information the developer viewed. To evaluate our
approach, we created a data set with multiple developers working
on the same set of six information seeking tasks that we also make
available for other researchers to investigate similar approaches.Our analysis shows that our approach enables: 1) segments of a
developer‚Äôs work to be automatically associated with a task from a
knownsetoftaskswithaverageaccuracyof70.6%,and2)aword
cloud describing a segment of work that a developer can use to
recognize a task with average accuracy of 67.9%.
CCS CONCEPTS
‚Ä¢Human-centered computing ‚ÜíEmpirical studies in HCI.
KEYWORDS
software development productivity, information seeking tasks
ACM Reference Format:
ChrisSatterfield,ThomasFritz,andGailC.Murphy.2020.Identifyingand
Describing Information Seeking Tasks. In 35th IEEE/ACM International
Conference on Automated Software Engineering (ASE ‚Äô20), September 21‚Äì
25, 2020, Virtual Event, Australia. ACM, New York, NY, USA, 12 pages.
https://doi.org/10.1145/3324884.3416537
1 INTRODUCTION
Developers work on many tasks in a day: some of of these tasks
arecode-relatedandothersinvolveinformationseeking[ 20].As
developersworktheyswitchbetweentasksconstantly[ 7,17].This
constantswitching,andthevarietyandhighnumberoftasks,makeitdifficultfordeveloperstoknowwhichtasktheyworkedonwhen.
Asaresult,developersspendsignificanttimeandeffortrecalling
what information is needed when a task is resumed [ 12,26,27].
Additionally, developers are unable to accurately record how much
ASE ‚Äô20, September 21‚Äì25, 2020, Virtual Event, Australia
¬© 2020 Copyright held by the owner/author(s).
ACM ISBN 978-1-4503-6768-4/20/09.
https://doi.org/10.1145/3324884.3416537time is spent on tasks, impacting personal planning and retrospec-
tion activities (e.g., [ 16]), as well as impacting effort estimation for
the entire team.
To ease this problem, some developers, manually track and note
whichinformationtheyaccesswhileperformingataskasaform
ofexternalizationof theworkingstateofa task[27].Thismanual
approach is time consuming and requires substantial effort from
thedeveloper.Sometoolshavebeenintroducedtoalleviateparts
of this burden from the developer. For instance, the Mylyn tool
enablesadevelopertoindicatewhenworkonaparticulartaskis
started and stopped, and the tool then tracks relevant information
forthetask[ 12].Alloftheseapproachesrequirethedeveloperto
explicitly indicate whenthey start working on a task, and which
task they are working on, which is cumbersome at best.
Recently,researchershavemadeincreasingprogressonautomat-
icallyidentifying whendevelopersswitchtasks[ 13,18,21,25,31,
32]. These advances mean it is becoming possible to automatically
split a developer‚Äôs past work into segments associated with differ-
ent tasks. An open problem is to determine whichtask a developer
is working on and associating each segment with the task.
Inthispaper,weexplorethisopenproblem,focusingonwhether
the topic of work‚Äîa task‚Äîcan be identified automatically based
on the information that a developer accesses as part of a task. Our
initial focus is on information seeking tasks. As it is common in
softwaredevelopmenttorecordtaskstobeperformedineithera
sharedorprivateissuerepository,wefirstassumethatdescriptions
of what work is or has been performed are available and examine
the following research question:
RQ1:Can we automatically associate existing task descriptions
withinformationdevelopersaccessas theyworkonthese
tasks?
Approaches that address this question can help a developer to
locate when they had performed work on a particular task. In a
secondstep,weexaminewhetheritisalsopossibletogeneratetask
descriptions from scratch based solely on the performed work:
RQ2:Can we automatically create a word cloud representation of
work performed that enables developers to identify the task
on which work was occurring?
To explore these questions, we developed an approach that gen-
eratesrepresentationsofadeveloper‚Äôsworkforagiventimeperiod.
The approach, as depicted in Figure 2, takes in work being per-
formed by a developer. Specifically, our approach continuously
recordsscreenshotsofthedeveloper‚Äôsactivewindowandutilizes
opticalcharacterrecognition(OCR)toextracttheinformationfrom
it.Foragiventimeperiod,theapproachthenappliesnaturallan-
guage processing and information retrieval techniques to generate
a vector representation of the segment. This representation can
thenbematchedtoexistingtaskdescriptionstodeterminethetask
the developer worked on (for RQ1) or can be used to generate a
7972020 35th IEEE/ACM International Conference on Automated Software Engineering (ASE)
This work is licensed under a Creative Commons Attribution International 4.0 License.
word cloud that highlights the most relevant words to describe the
task(forRQ2).Anadvantageofourapproachusingscreenshots
andOCRisthatitisagnostictotheapplicationsadeveloperuses
to perform their work.
Weperformedtwoevaluationstoassessourapproach:onefor
eachresearchquestion.Theseevaluationsarebasedonadataset
that we created consisting of work streams from 17 participants
experiencedinsoftwaredevelopmentperformingsixinformation
seekingdevelopment-orientedtasks inaninterleavedfashion ina
controlled lab setting (Section 3). We designed the tasks tobe rep-
resentative of information seeking tasks commonly performed by
softwaredevelopers[ 6].Basedonmanuallyidentifiedtaskswitches,
wethenapplyourapproachoneachsegmentofwork(betweentask
switches)togeneratevectorrepresentationsandwordcloudsfor
eachtasksegment(Section2).AsweshowinFigure1,weevaluate
RQ1byexaminingwhetherourapproachcancorrectlyassociate
task descriptions written by variousdevelopers with the task seg-
mentsusingthegeneratedvectorrepresentations.Wegatheredtask
descriptions for each task from 20 people experienced in software
development and found that our approach is able to correctly asso-
ciate the descriptions with the correct segment of work in 70.6%
ofcases(Section4).WeperformapreliminaryevaluationofRQ2
byexaminingwhethersoftwaredevelopersareabletomatchthe
generated word clouds to the corresponding tasks. We surveyed 28
experienced software developers and found that they were able to
matchthewordcloudstothesixoriginaltaskdescriptionscorrectly
in 67.9% of cases (Section 5).
This paper makes four contributions:
‚Ä¢A data set from a controlled lab setting involving 17 par-
ticipants working onsix information seekingdevelopment
tasks; other researchers can build on this data set to investi-
gate other approaches.
‚Ä¢An application-agnostic approach to generate representa-
tionsofadeveloper‚Äôsworkforagiventimeperiodtohelp
determineanddescribe thetaskthatisbeingperformedand
anevaluationofavarietyoftechniquesforgeneratingthese
representations.
‚Ä¢Anevaluationoftheapproach‚Äôsaccuracyfordetermining
thetaskadeveloperwasworkingonforagiventimeperiod,
based on the collected data set and task descriptions from
20 participants.
‚Ä¢An evaluation of the approach‚Äôs ability to generate wordclouds for task segments that can be used to identify the
tasks developers were working on.
Whileourevaluationonlyfocusesoninformationseekingdevelop-
ment-orientedtasksandusesrecordedtaskswitchinformation,the
results show promise for our approach‚Äôs ability to automatically
identifyanddescribethetasksadeveloperisworkingonandfor
further automating task support.
2 GENERATING TASK REPRESENTATIONS
Our goal is to create representations of a developer‚Äôs work that
allow the developer to determine the tasks worked on. Specifically,
weconsiderthecreationoftworepresentationsofworkperformed:
a vector space representation (vectors) that can be used to auto-
matically match it to existing task descriptions and thus determinethetaskworkedon;andawordcloudrepresentationthatdescribes
the task and allows the developer to identify the task worked on
withoutpre-existingtaskdescriptions.Previousworkhasshown
that word clouds are useful aids for helping users determine the
relevance of a document to a topic [8].
Wedescribeourapproachthatcontinuouslymonitorsadevel-
oper‚Äôsworkby recordingscreenshotsofthe activewindows,pro-
cesses and extracts relevant information, and is able to generate
vectors and word clouds for specified time periods of work. By
recording and processing screenshots, our approach is agnostic to
the applications developers use for their work. Figure 2 depicts the
main steps involved in our approach.
For this research, we focus on generating representations for
tasksegments ‚Äîtimeperiodsofworkinwhichadeveloperworks
on one task before switching to another one‚Äîand assume thatthese switches can automatically be determined using emerging
techniques (e.g., [13, 18, 21, 25, 31, 32]).
2.1 Screenshot Pre-processing
Ourapproachfirstpreparestherecordedscreenshotsofdevelopers‚Äô
active windows for the optical character recognition (OCR) with
Tesseract [ 34].Specifically,we convertthe coloredscreenshotsto
grayscale and scale the resolution down to 300 DPI. These steps
areconsideredbestpracticeasTesseractwasoriginallyintended
forreadingblackonwhitepaperdocuments.Inaddition,wecrop
apercentage ofthe topofthe screenshotas mostapplicationwin-
dows have menu or bookmark bars at the top that generally do
notcontaininformationspecifictothetaskathand.Throughex-
perimentation,wefoundthatremovingthetop15%ofthescreen
across all application window screenshots provides a good balance
between removing noise without much loss of meaningful con-tent.
1We automate all screenshot pre-processing steps with the
ImageMagick tool [10].
2.2 Extracting Bags of Words
Afterpre-processing,ourapproachappliestheTesseractOCRen-
gine to extract the textual content of each screenshot. As Tesseract
tries to preserve the format of the text, it produces a structuredstring for each screenshot. We store these strings in a document,
one for each screenshot. These structured strings contain substan-
tial noise even after the pre-processing. For instance, an ‚ÄòI‚Äô is often
misinterpreted as the number ‚Äò1‚Äô or the letter ‚Äòl‚Äô. As well, many
nonsensical artifacts can be produced due to noise from items like
images and menu bars on the screenshot that remain after pre-
processing.
To break these documents into usable pieces of information
(words/tokens) and further reduce noise, our approach supports
the application of one of two techniques, either (a) tokenization,or (b) keyword extraction. For tokenization, we use the NaturalLanguage Toolkit (NLTK) [
24] Version 3.2.5 and apply standard
wordtokenizationtechniquesbasedonwhitespaceandpunctuation
to generate lists of all words in a screenshot. We further remove
allstop-wordsfromthelists.Forthekeywordextraction,weuse
an open source implementation [ 29] (version 1.0.4) of the RAKE
algorithm [ 30]. Based on an input string, RAKE produces a set
1This percentage might have to be adjusted for different screen resolutions and sizes.
798Approach
Generating task representations (V)
for task segments (TS)Data Set on Developers‚Äô Task Work
17 developers working on 6 diÔ¨Äerent tasks 
1 2 3 4 5 6 7 8 9
S1
S2
S17
Tasks
          Duplicate Bug Detection (BugD) 
          Viz Library Selection (Viz)               Blockchain Expertise (BlC)       T1
       T2
       T61 2 3 4 5 6VTS1
VTS2
VTS3
VTS4
VTS5
VTS6Word cloud 
generation
WCTS1
WCTS2
WCTS3Evaluation RQ1
Automatic matching of task representations 
to task descriptions
120 task 
descriptions
20 participants 
x 6 tasks VTS1
VTS2
VTS370.6%
accuracy
Evaluation RQ2
Matching word clouds to original tasks 
67.9%
accuracy
T1
T2
T628 developers
x 12 word clouds
7VTS7BugD
Viz
BlC
Figure 1: Overview of the process used to evaluate our approach.
Table 1: Techniques used in the vectorization process of a task segment.
Technique Description
TF Uses the frequency of a term/word in a task segment as vector entry, and 0 if the word does not occur in the task segment.
The vector dimensions are the unique words that occur in any of the task segments for a developer.
TF-IDF Has the same vector dimensions as TF, but uses TF-IDF for calculating entries. TF-IDF for a word/term ùë°is defined as
ùëìùë°‚àóùëñùëëùëìùë°,whereùëìùë°isthe termfrequency of ùë°,andùëñùëëùëìùë°iscalculatedas ùëôùëúùëîùëÅ
ùëëùëìùë°,whereùëÅisthe totalnumberoftask segments
andùëëùëìùë°is the number of task segments in which ùë°occurs.
W2V Usesaword2vec[ 19]modelpretrainedonacorpusextractedfromWikipedia.Eachwordwithinatasksegmentisassigned
a 300 dimensional embedding vector. These vectors are then averaged to create one embedding vector for the entire task
segment. This technique has been shown to be an effective baseline in many NLP tasks [11].
of keywords with a size equal to 1/3 of the number of original
words (not counting duplicates). After breaking up each document
intoa setofwordsusing tokenizationorkeywordextraction,our
approachstemsallwordsusingthePorterstemmerimplementation
from NLTK.
Finally, the approach creates a bag of words‚Äîa record of the
frequency of each word‚Äîfor each task segment by aggregating all
words extracted from all screenshots of a task segment.
2.3 Generating Task Representations
A bag of words is itself a primitive representation of a task with
thefrequencyofeachwordinthebagindicatingtheimportance
of a word to the task. However, this r epresentation only reflects
importanceofawordwithrespecttothecurrentdocument(task
segment). To also take into account the relevance of the wordin context of the overall work of the developer and further helpfilter noise from the screenshots and the OCR, our approach is
designed to enable experimentation with several natural language
processing (NLP) and information retrieval techniques to generate
more advanced representations.
Table 1 summarizes the three techniques we experimented with
in this work to produce (a) a vector space representation V, andsubsequently (b) a word cloud representation WC. Each of these
techniques takes the words from the bag of words produced in theprevious step as input, and produces a vector representation of the
task segment. In the case of TFandTF-IDF, the dimension of the
vectors is the number of unique words in the set of all words from
all task segments of a developer. For W2V, we chose the dimension
of the vectors to be 300 based on our training of the word2vecmodel.Allofthesevectorrepresentationscanthenbecompared
usingcosinesimilarityagainstvectorswhichcouldbegenerated
based on other task segments or, for example, task descriptions.
Based on these vector representations, our approach can be
usedtogeneratewordclouds.However,sincethemeaningofthe
dimensions in the W2V vectors are difficult to interpret, we did
notusethe W2Vtechniqueforcreatingwordclouds.Togenerate
word clouds for the TFandTF-IDFtechnique, our approach selects
the 100 largest entries in a vector, corresponding to the highest
ranked words in a task segment, and use the score of the words to
determine the proportional size of the words in the cloud. Figure 3
shows two examples of such word clouds.
3 DATA SET CREATION
To support the investigation of the two research questions, we
createdadatasetfrom17developersworkinginacontrolledlab-
oratory setting on a set of six information seeking tasks over a 2
hourtimeperiod.Wechosealaboratorysettingtobeabletogather
799... ... ...
t1 t3 t2
OCR (Tesseract)
Structured
Strings of
Text
Tokenization (TK) OR Keyword Extraction (RAKE)
Stemming & Aggregation
TF / TF-IDF / W2VBag of Words
WCTS1¬¨+ VTS1TimeTask 1 Task 2 Task 1 Task 3
WCTS2¬¨+ VTS2 WCTS3¬¨+ VTS3 WCTS4¬¨+ VTS4Task Switch
Figure 2: Main steps of the approach to generate task repre-
sentations (the light blue boxes represent task segments).
(a)WCforaDeepLearningPresentation(DeepL)tasksegmentofD1
(b) WC for a Duplicate Bug (BugD) task segment of D4
Figure 3: Word Clouds (WCs) generated for task segmentsfrom different tasks and developers.
data from multiple developers working on the same tasks. The full
data set will be made available in the supplementary material2[1].
2Thedatasetistemporarilywithheldtoprotectdoubleblindduringthereviewprocess.3.1 Developers
We recruited 17 participants‚Äîwho we refer to as developers in
the following‚Äîthrough advertising at our university and personal
contacts.Alldevelopershadseveralyearsofexperienceinsoftware
development, with an average of 6.4 ( ¬±2.4) years per developer.
10 of the developers were female, and 7 were male. At the time
ofthedatasetcreation,10weregraduatestudents,4wereupperyear undergraduates, and 3 were interns at a mid-sized software
company. All developers were residents of Canada.
3.2 Tasks
Developers workon manydifferent kindsof taskseach day,some
of which focus on code (23.4% [ 17]) and some of which focus on
informationseeking(31.9%[ 6]).Incollectingthisdataset,wechose
to focus on information seeking tasks. We made this choice given
the significant, and higher,fraction oftheir daydevelopers spend
on these kinds of tasks. This choice also enabled developers to
attemptmoretasksinthelimitedtwohoursavailableperdeveloper;codingtaskswouldhaverequiredmoretimeperdevelopertoenabledeveloperstogainsufficientfamiliaritywithacodebase.Wediscuss
theimplicationsof ourchoiceinfocusingoninformation seeking
tasks in Section 6.1.
Wecreatedsixtasksthatarerepresentativeofcommoninforma-
tionseekingtasksbasedontheauthors‚Äôknowledgeofindustrial
development.Thetasksweselectedweredesignedtoberealistic,
yet simple enough for it to be possiblefor developers to make sig-
nificant progress in the limited time available. The tasks were also
chosentoenableadevelopertomakeprogresswithoutpriorknowl-
edge.Developerswere notconstrainedinhowtheyapproacheda
task.
Table 2 provides a short description of each of the six tasks,
includingashortnamethatweuseinthispapertorefertoaspecific
task; the short task name and description was not presented to
developers. An example of one of the actual task descriptions used
in this study is presented in Table 3. We intentionally designed the
App Market Research Task andRecommend Tool Task as tasks
which were likely to have very similar information accessed as
part of working on the task to allow us to assess the discriminativepowerofourapproach.Fulldescriptionsofthetasksthedevelopers
worked on can be found in the supplementary material [1].
3.3 Session
Beforethestartofasession,wegaveeachdeveloperabriefoverview
oftheproceduretheywouldbeaskedtofollow.Developerswere
told that they would be asked to work on a number of information
seeking tasks, and that they could accomplish these tasks in what-
ever manner they chose. However, the quantity of tasks and the
contentofeachtaskwaswithhelduntilthesessioncommenced.De-
velopers were also told that their screen would be recorded by our
monitoring tool,and that theywould be observed bythe observer
as they worked.
Atthestartofasession,developerswerepresentedwithalist
of6taskstoperformwithina2hourtimeperiod.Thetaskswere
presented in the form of unread emails sitting in an inbox accessed
byawebmailclient.Theorderinwhichthetasksappearedinthe
inbox for a developer was randomized. We asked a developer to
800Table 2: Overview of Controlled Lab Tasks.
Abbrev. Short Task Name Short Task Description (by us)
BugD Duplicate Bug ExamineacollectionofbugreportsfromaBugzillarepositorytodetermineifanywereduplicates.
(Eachdeveloperwasaskedtoexaminefourbugreports,twobeingduplicatereportsandtwonot.
Foreachparticipant,bugreportswererandomlyselectedfromthesetofallresolvedbugreports
from the Mozilla projects (e.g., Firefox, Thunderbird, etc.) [23] over the course of a month.
Viz Viz Library Selection Researchvisualization librariesand identifyonewhich issuitable foroutlining thebenefitsof
your companies tool, for creating a presentation to clients.
PrMR App Market Research Performmarketresearchonthreeproductivityapps.Identifycommonfunctionalities,similarities
and differences, and report on your findings.
PrRec Recommend Tool Examine app store reviews for three productivity apps (the same ones as above) in order to
recommend one to your coworker.
DeepL Deep Leaning Presentation Prepareinadvanceanswerstolikelyquestionsforahypotheticalpresentationyouaregiving
about potential deep learning applications.
BlC Blockchain Expert Answer your coworkers follow-up questions about a hypothetical presentation you gave about
the different ways your company could make use of blockchain.
Table 3: Full Task Description for the App Market Research Task (PrMR) as Presented to Developers.
The software company you work for is considering expanding into the productivity tool sphere. Your manager has asked you to do some
market research on 3 of the most popular already existing apps in this domain: Microsoft To-do, Wunderlist, and Todoist. Provide a short
written summary of the similarities and differences between these 3 apps.
workonthetasksonalaptopwitha 13.3inch,1440x900 sizedscreen
runningmacOSwhichwas instrumentedwithourrecordingtool
(reference omitted for double blind). As a developer worked on
the tasks, the tool recorded screenshots of the developer‚Äôs active
windowat1secondintervals.Applicationnamesandwindowtitles
were also recorded whenever they changed.
To simulate interruptions, we also installed a tool on the laptop
that produced a popup in random intervals lasting from 6.5 to 16.5
minutes. The average time between popups was selected as 11.5
minutes,inaccordancewithGonz√°lezandMark‚Äôsfindingsonthe
average amount of time knowledge workers spend in a working
sphere segment before switching [ 7]. To simulate the disruptive
effectsofarealexternalinterruption,thepopupprompteddevelop-
ers to solve an arithmetic question before switching to a new task.
These popups were excluded from our tools recordings to avoid
biasing our results.
Asadeveloperworkedonthetasks,aresearchermanuallyanno-
tatedthetimesatwhichthedeveloperswitchedtasks,alsokeeping
trackofthetaskthedeveloperwasworkingon.Afterthesession
was complete, the times at which switches happened were veri-
fied and adjusted by reviewing a screen capture that ran in the
backgroundoftheprovidedlaptop,toensuretaskswitcheswere
recorded accurately.
3.4 Data Collected
In total, we were able to collect screenshot data for all 17 develop-
ers and on all six tasks for each developer. All but one developer
completed the 6 tasks within the allowed time period. On average,
developerstook91.2 ¬±17.5minutestocompletethesixtasksand
we collected an average of 5131 screenshots per developer. Due to
Figure 4: An example of a developer working on severaltasks over time, revisiting task 3 in two task segments.
a technical issue, we were able to gather window titles for only 12
of the 17 developers. The full data set will be made available in the
supplementary material3[1].
3.5 Data Annotation
Using the task annotations collected by the researcher during each
session, we annotated the collected data with the task switches
and the task the developer was working on. Each session resulted
in the developers working in an interleaved fashion on the six
tasks. Figure 4 depicts a portion of a developer‚Äôs work, showing an
exampleoftheinterleaving.Wedefinea tasksegment astheperiod
of time between two task switches, during which a developer was
working on a specific task. We define a task segment grouping as
the collection of all task segments that collectively represent work
onaspecifictask.Weuse tasksegmentgroupings asabaselinefor
evaluatingourapproach,asitmimicsthesimplestcaseinwhicheach task is completed in one contiguous segment and we have
3Thedatasetistemporarilywithheldtoprotectdoubleblindduringthereviewprocess.
801the entirety of the information accessed for the work on a task
available.
4 RQ1: IDENTIFYING TASKS
Our first research question asks whether we can automatically
associate descriptionsof adeveloper‚Äôs tasks withthe information
thedeveloperaccessesassheworks.Performingthisassociation
automaticallyischallengingbecausetherearemanywaysinwhich
adevelopercancompleteataskandtherearemanywaysinwhich
a developer can describe the task on which they are working.
Thedatawecollectedinthelabsetting(Section3)includesanum-
ber of ways in which the tasks assigned could be completed. While
participants in the lab setting had some overlap in the resourcesthey accessed as part of a task, no two participants completed a
task in exactly the same manner.
Similarly,developersarelikelytotailortheirtaskdescriptions
towards the ways they might approach a task. To study the first
researchquestion,wethereforealsoneededarangeofdescriptions
of thetasks on whichthe developers hadworked. To gatherthese
descriptions, we employed Amazon Mechanical Turk (MT). Given
a range ofdescriptions collected in thisway, we are able toassess
how the range of techniques we developed for generating task
representations (Section 2) can address the first research question.
4.1 Gathering Task Descriptions
To capture a range of task descriptions, we distributed a survey via
MechanicalTurk.Asarequirementforrespondingtooursurvey,
we asked that respondents be currently or previously employed
in the software industry. In total we received responses from 29respondents. These respondents represented a range of fluency
withEnglishandarangeofexperienceinsoftwaredevelopment.
Onaverage, respondentshad 6.2( ¬±5.4)years ofsoftware develop-
ment experience, and 3.8 ( ¬±3.5) years of professional development
experience.Oftheserespondents,24reportedthattheywerenativeEnglishspeakers,while3reportedbeingfullyfluentand2reported
being proficient.
Respondents of this survey were presented with the same set of
six full task descriptions that we also used for the data set creation
inthecontrolledlabsetting.AnexamplecanbeseeninTable3.We
asked respondents to ‚ÄúPlease summarize the task described below
inyourownwords,asyoumightwriteitforyourownreference
inato-dolistorsimilar.Pleaselimityourresponsetoatmost15
words.‚Äù.Thereby,werandomizedtheorderinwhichthefulltask
descriptions were presented.
Tofilteroutirrelevantorlowqualityresponses,weaskedtwo
externalexperts,whowerebothresearchersinthesoftwareengi-
neeringdomainandexperiencedsoftwaredevelopers,toratethe
quality of every task description generated by each respondent.Each rater was instructed to use a scale from 1-3 to indicate therelevancy and quality of the responses, with a score of 1 indicat-ing an irrelevant response, 2 indicating relevant but low quality
responses, and 3 representing relevant and high quality responses.
Wefoundthatthedistinctionbetweenresponsesrated2or3var-
ied greatly between our two experts, but that there was a strong
consensus with regard to the responses which were rated 1 / irrele-
vant(Cohen‚ÄôsKappa:0.74,indicatingstrongagreement[ 15]).Theseirrelevant responses tended to come in multiples from the same
participants.Weremovedallparticipantswithirrelevantresponsesandconsideredonlythoseresponseswhichbothauthorsratedwith
a score of 2 or higher, leaving us with 20 participants and a total
of 120 task descriptions. A sample of 3 responses for one task with
the ratings by one expert rater is depicted in Table 4.
4.2 Evaluation
From the controlled lab setting, we have 189 task segments and
102tasksegmentgroupings.FromtheMTsurvey,wehave20de-
scriptionsforeachtask,resultinginatotalof120taskdescriptions.
We wish to determine if the approach we developed for generating
task representations, and which choice of techniques within theapproach, can be used to determine which task segment (or task
segment group) maps to which task description with sufficient pre-
cision and recall, even when these task descriptions might vary.
Recallthatweknowthegroundtruthofwhichtask,andthuswhichtaskdescription,eachtasksegmentrepresentsbasedonnotestaken
by a researcher during the controlled lab setting.
Ourevaluationconsistsofconsideringeachtasksegmentfrom
a lab developer‚Äôs work and mapping it to one of the six task de-scriptions produced by a MT respondent. We use this evaluation
methodthat assumesa completesetof descriptionsas wewishto
assesshowwellourapproachmightworkinasituationwhereadeveloper may be trying to determine, from a given set of tasks,
when they performed work on each task. For the mapping, we gen-
erateavectorspacerepresentationofthetasksegmentV ùëáùëÜaswell
as one for each of the six task descriptions V 1to V6produced by a
respondent and then calculate the cosine similarities between V ùëáùëÜ
and each of V 1to V6. We choose the task description most similar
to our generated task representation and evaluate it by comparing
it to the ground truth to determine if it is correct.
Forgeneratingtaskrepresentationsfromtasksegmentsinvec-
torspaceformat,weexperimentedwithandcomparedsix(3x2)
different combinations of techniques: 3 different techniques for
vectorization (term frequency, TF-IDF, and word2vec word em-
bedding),and2differenttechniquesforextractingbagsofwords
(tokenization using NLTK, and keyword extraction using RAKE as
described in Section 2.2). The vectorization techniques applied are
described in Section 2.3.
TogeneratevectorsfromthetaskdescriptionsofMTworkers,
wetokenizedthetaskdescriptionsusingNLTK(keywordextraction
is not useful in this case given the brevity of the descriptions), and
thenappliedtheexactsamevectorizationtechniqueasusedforthe
task segments, i.e. either TF, TF-IDF, or W2V.
4.3 Results
Figure5illustratestheresultsofthecomparisonbetweenthesix
differentcombinationsofvectorizationandwordextractiontech-
niques.Overall,thecombinationofTF-IDFwithsimplewordtok-
enizationperformedthebest,howeverthedifferencesaresmallcom-
paredtothecombinationwithRAKEorusingjustTF.Ultimately,
word2vec performed the worst for the generation of task represen-
tations and mapping to the task descriptions. Since word2vec is
also the most computing intensive, it was the least appropriate for
thisscenario.Basedontheseresults,weselectedthecombination
802Table 4: Examples of descriptions received for the Viz Library Selection (Viz) task, together with one of the expert‚Äôs ratings.
Rating Task Description (Survey Response)
Irrelevant (1) I would suggest SIMILE Exhibit or InfoVis Toolkit for Javascript libraries to create a visualization.
Low Quality (2) Visualize workers work pattern.High Quality (3) Create visualizations for product benefits. Select libraries and give existing work examples.

	 		 	 		 
 	
	
	

Figure 5: Accuracy comparison for the 6 different combina-
tions of techniques used to generate task representations
(TK = tokenziation).
of TF-IDF with word tokenization (NLTK) as the approach that we
use for the remainder of the paper.
Table5presentstheresultsoftheevaluationwhenmappingtask
representations of task segments (or task segment groupings) to
the task descriptions written by the 20 MT workers. We report the
precision and recall for each of the 6 tasks, calculated on a per task
segment basis (or with the baseline of the per task segment group-
ing). Overall, using only task segments, our approach achieved
highaccuracyacrossalltasks(70.6%)incomparisontoarandom
classifier (16.7%). Accuracy for task segment groupings was moder-
atelybetter(75.5%).Thisisapromisingresultasitindicatesthat
thereisoftenalreadysufficientinformationinanindividualtask
segmenttopredictthetaskthatisbeingperformed;addingmore
information helps some but does not make a dramatic difference.
Our approach performed well at predicting tasks with a distinct
focus, such as DeepL and BlC, with precision values over 80%. This
result is unsurprising, as in order to perform these tasks, the devel-
opers in the lab setting tended to turn to resources that contained
a dense amount of highly specific information related to these top-
ics, such as the Wikipedia pages for blockchain and deep learning.
The presence of dense, consistent information eases the produc-
tion of accurate representations for the tasks. Our approach also
performedwellatrecognizingtheBugDtaskwithprecisionover
92%. We found this result surprising as we expected this task tobe one of the more difficult tasks to predict, especially since oursummary authors were given no information about the contentof this task, beyond that it involved finding duplicate bugs in a
Bugzilla repository. As expected, the most difficult to predict tasks
werethePrMRandPrRectasks.Thesetasksareverysimilarandas
such,resultedinverysimilartaskrepresentationsaswellasveryFigure 6: Accuracy distributions by MT respondent and bylab developer for mapping task representations to the cor-rect task descriptions.
similar task descriptions and in turn, in a high confusion between
the two tasks.
Figure 6 illustrates the accuracy distributions of the results on a
perdeveloperandaperMTrespondentlevel.Despitedifferencesin
the way each developer performed each task, the results are fairly
consistent across developers, ranging from a minimum accuracy
of62.9%toamaximumof77.3%.AcrosstheMTrespondentsthat
authored the task descriptions, the results are mostly consistent,
however, there is a significant variation for a few respondents. The
respondents for which the accuracy of mapping task representa-
tionstotheirtaskdescriptionswereratherlowtendedtobeones
whoauthoredmultipledescriptionsthatwerealsoratedlowerby
the experts (e.g., item 2 in table 4 was written by author S6). These
resultdemonstratethatthetaskrepresentationsarerelativelyro-
bust across developers and different ways of performing the tasks,
and that writing precise and somewhat detailed descriptions of the
tasksbeing performedclearly impactsthe resultsof ourapproach.
5 RQ2: DESCRIBING TASKS
To address the question of whether we can automatically generate
word cloud representations of information accessed by a developer
which would help the developer to identify what task she worked
on during a specific period of time, for a task which would helpdevelopers identify what task they worked on during a specificperiod of time, we evaluated the word clouds we generated as
describedinSection2.3.Weasked28participantsexperiencedin
softwaredevelopmenttomatchourgeneratedwordcloudstothe
original full task descriptions of the tasks that were performed
during the data set creation.
803Table 5: Results of mapping task representations to task descriptions written by MT workers.
Task Segments Task Segment Groupings
BugD Viz PrMR PrRec DeepL BlC BugD Viz PrMR PrRec DeepL BlC
Precision 92.6% 72.7% 53.3% 44.7% 83.5% 80.9% 97.4% 78.1% 55.0% 55.7% 85.3% 86.4%
Recall 82.3% 81.5% 47.1% 65.4% 75.3% 70.0% 89.4% 89.4% 45.0% 70.9% 82.1% 76.5%
5.1 Survey
Toevaluatethequalityofourautomaticallygeneratedwordclouds
asavisualrepresentationofatask,weconductedasurveywithex-
periencedsoftwaredevelopers.Participantswererecruitedthroughpersonalandprofessionalcontacts,andasanincentiveforrespond-
ingwereenteredintoadrawforoneoftwo$25giftcardsifthey
desired. In total, we received survey responses from 28 individuals,
withanaverageof8.0( ¬±3.9)yearsofsoftwaredevelopmentexperi-
ence.20participantsweremale,while8werefemale.9participants
reportedthey werenative Englishspeakers, 12reportedthat they
were fully fluent in English, and the remainder (7) reported that
they were proficient in their understanding of English.
We asked our participants to match word clouds to correspond-
ingtasksbypresentingthemwiththelistofthesixfulltaskdescrip-tionsthatwealsousedforthedatasetcreation.Anexampleofone
of the descriptions can be seen in Table 3. The word clouds used
in the survey were generated following the procedure described in
Section2.3.Usingthedatathatwecollectedacrossall17developers
in the data set creation (Section 3), we randomly selected 4 tasksegments and 4 task segment groupings for each of the six task
andgeneratedwordcloudsforthese,resultinginatotalof48word
clouds. Since asking survey participants to examine a total of 48
wordcloudswouldbetoomuchandimpractical,werandomlyse-
lectedandaskedeachparticipantabout12ofthe48,endingupwith2wordclouds(1foratasksegment,1foratasksegmentgrouping)
for each of the six tasks. Examples of two of these word clouds canbe found in Figure 3. We asked participants to read the six full task
descriptions and to then identify which task the presented wordclouds describe best. Participants also had the option to indicate
that the word cloud does not match any task.
5.2 Results
Weaggregatedtheresultsofthesurveyresponsestoobtainaccu-
racyratingsforthewordcloudswegenerated.Overall,theaverage
accuracy ofmapping word clouds tothe corresponding taskswas
67.9%forthewordcloudsgeneratedfromtasksegments,and69.6%
for the word clouds generated from groupings. Figure 7 shows the
breakdown of the accuracy on a per task level. The success rates of
our participants varied widely between tasks. For example, for the
blockchain expert task BlC, our participants were able to correctly
identify the task for the generated word cloud 100% of the time.
Conversely, participants struggled to properly identify the task for
the word clouds generated for the duplicate bug task BugD (35.7%).
Thistaskwasbyfarthemostdifficultforparticipantstoidentify,
and many participants reported that the word clouds generated
bythistaskwerenotdescriptive.Unsurprisingly,participantsfre-
quently confused the word clouds generated for the app market
research task PrMR and for the recommend tool task PrRec. These !"#$%
 
    
	
	

Figure7:Accuracyforidentifyingthetaskbasedonourgen-
erated word clouds. The dotted red line indicates the accu-racy for a random classifier.
word clouds tended to have very similar key words, as both full
task descriptions mentioned the same three productivity tools.
Comparingtheresultsofthewordcloudsgeneratedfromseg-
mentstotheonesgeneratedfromgroupingsdidnotrevealasub-
stantial difference. This is a promising result, as it indicates that
enoughdatacanbegeneratedfromwithintheboundsofmosttask
segments to create word clouds that accurately represent the topic
of a task as a whole.
6 DISCUSSION
Decisionswehavemadeindesigningtheapproachweintroduce
are impacted by the evaluations we undertook. We discuss threats
tothevalidityoftheseevaluationsandconsideralternativesthat
could make it easier to apply our approach.
6.1 Threats to Validity
The evaluations of the approach we conducted rely on a data set
that focused on six tasks. Although we chose these tasks to beexamples of information finding tasks performed by developers,the range of tasks explored is small. By focusing on informationfinding tasks, we also exclude a significant category of tasks on
which developers commonly work, namely coding related tasks.
We believe that with minor adaptions, such as tokenizing camel
casewordsorparsingtheOCRresultstoextractincodecomments,
our approach could be made to work with coding tasks. If we took
this approach to coding tasks, the quality of the code base in terms
ofdocumentation,namingconvention,andsoon,couldplayalarge
role in the ability of our approach to make accurate predictions.It would be impossible to associate a developer description, or
generate a meaningful visual representation, if the code base doesnot contain descriptive names and lacks documentation. We leave
804theinvestigationofthegeneralizabilityofourapproachacrossa
wider range of tasks to future study.
Anotherthreattothefindingsisthesizeofthetasksstudiedand
theinterleavingofworkondifferenttasks.Tofitwithinareasonable
time frame for a lab setting, the tasks worked on were relatively
smallinscope.Inreality,developersworkoncomplextasksthatcan
have a huge scopeand span multiple topics. In addition, although
we caused developers to switch tasks, it is not possible to replicate
themanytaskswitchesadeveloperundertakesasheworks[ 17].
It is also unlikely that in reality any single developer would be
assigned all six of the tasks we selected at the same time. A field
study is likely needed to mitigate these threats.
Wealsonotethatthetaskswedesignedmaybemorespecificin
their wording than those that might occur in a developer‚Äôs normal
work pattern. For example, a developer might work on a task in
response to some relatively vague verbal request for help from a
colleague.In suchcases,it isunlikelythat thesummariesthat the
developer would write for these tasks are highly descriptive. Wemitigate this threat by including a wide variety of low and highquality task summaries written by a group of MT workers with
diverse demographic in our evaluation.
Whileallthedeveloperswhosedatawecollectedtocreateour
data set had significant development experience and were actively
developingsoftware,allwerestudentdevelopers,andnonewere
employedinapermanent,professionalsoftwaredevelopmentpo-
sition.Assuch,itispossiblethattheirworkinghabitsmaydiffer
to some degree from those of professional software developers,
effecting the generalizability of our results.
6.2 Limitations of the Approach
A prerequisite for the application of our approach is that the times
at which task switches occur must be indicated. To achieve a fully
automatic application of our approach, it is necessary that thesetask switches be detected automatically and with high accuracy.
Automaticdetectionoftasksegments(i.e.,taskswitches)isadif-
ficultproblem(e.g.,[ 18,21,32]).Whileweareoptimisticthatthe
techniques to detect task switches will continue to improve, future
work should explore the performance of our approach in the ab-
sence of knowing task segment boundaries. It may be that missing
or erroneously predicting a task switch could lead to degraded
performance in our approach in practice.
It is also possible that in practice the vocabulary a developer
usesto describetheirtaskdoes notmatchexactly withthewords
commonly found within the content of the task. For example, adeveloper might use the word ‚Äúchart‚Äù in their task description,yet in the window content of the task the word ‚Äúgraph‚Äù might
appearprominentlyinstead.ApplicationsofTF-IDFwouldmissthis
connectiongivenitsfocusonexactwordmatches.Incorporating
some notion of semantic similarity into our approach, for example
adding word2vec or another model for word embedding, we might
be able to enhance task descriptions to also include semantically
similarwords.Moreexperimentationinamorerealisticsettingis
needed to investigate the impact and need for semantic similarity.6.3 Artifact Access
Using OCR and capturing a developer‚Äôs screen content has several
benefits. First, it is an application agnostic approach that does not
require any instrumentation of applications. As well, a screenshot
showsusthe exactcontentadeveloperislookingatinthemoment.
While OCR performed well for the purposes of our analysis, there
aremanydrawbacksthatcouldlimititsusabilityinpractice.Forone,
OCRisanextremelyCPUintensivetask.Processingscreenshots
in real time in the background while a developer works may beimpractical for this reason. An obvious alternative might be to
send screenshots to the cloud for processing, but privacy concerns,
both from the developer‚Äôs and company‚Äôs perspective, limit the
applicability of this approach. Another issue is the noise generated
when using OCR. This may be alleviated to some extent by using a
commercialoptionratherthantheopensourceTesseractengine.
However, we can not guarantee that the product of a screenshot
processed with OCR is exactly the same as the content a developer
saw on their screen when the screenshot was taken.
An alternative which we will investigate in future work is to
track all file accesses and edits made within the scope of a task
segment.Ifweknowwhichfilesadeveloperisinteractingwith,we
can extract the contents of the file directly. The benefit of knowing
exactlywhichinformationina documentis beingviewedwould
be lost in such an approach. However, this loss may be outweighed
by the ability to produce cleaner data, and the much lower CPUusage. The contents of web page visits could also be extracted
relatively easilywith the help ofa browser extension.However, it
couldbedifficulttoobtaininformationfromapplicationssuchas
instantmessagingandemailclients,asthereisamuchwiderrange
of choices for a developer to use in these cases. For this reason,
producingasuiteofinstrumentationsforallthemostcommonly
used applications is impractical. Further investigation is needed to
determinehowmuchpredictivepowerislostbytheexclusionof
these categories of applications.
6.4 Representations from Window Titles
Asmentionedinsection3,inadditiontorecordingscreenshotsofa
developer‚Äôs active window, our tool also recorded the window title
ofeverywindowthedeveloperaccessed.Unfortunately,duetoa
recordingerrorwindowtitledatawaslostfor5ofthe17developers
in our data collection session.
Toinvestigatewhethertheeasiertocollectinformationaboutap-
plication window titles might suffice for supporting our approach,
we evaluated RQ1 with the window title data from the 12 develop-
ers,inplaceoftheinformationextractedusingOCR.Comparing
the results of this evaluation with the results from the same 12developersusingscreencontent,wefoundthatwhiletheresults
wereloweroverall,thedifferencewasmodest(64.4%accuracyusingwindowtitlesvs70.3%accuracyusingscreencontent).Whilescreen
content proved to be a superior choice of data source in almost all
cases, window titles seem like a viable alternative especially given
the savings in CPU resources. Worth investigating is whether acombination of our approach using window titles and the other
dataextractiontechniquesmentionedabovecanrivaltheresults
we achieved using screen content.
8057 RELATED WORK
Our approach aids in determining the task driving a segment of
workperformedbyadeveloper.Theassociationofataskwithwork
providescluestowhatandwhyasoftwaredeveloperisperform-
ing the work and is thus related to the intentof the developer in
undertakingthework.Determiningtheintentofadeveloperisa
growingareaofresearch.Themoreweknowaboutadeveloper‚Äôs
intention, such as the task she is working on, the better we can
support the developer, for example by providing better code rec-
ommendations(e.g.,[ 5,13]).Approacheshavebeendevelopedto
determineintentfromhowadeveloperinteractswiththecomputer,
from the documents produced by a developer and from a mix of
both.Wedescribeapproaches ineach ofthese categoriesandalso
describe related work in finding meaning in artifacts.
Intent from Interactions. For some research systems, intent is
specified through specific interactions a developer takes within
the environment in which they work. As mentioned, in the Mylyn
system, a developer can indicate through an explicit click of thebutton on which issue they are currently working: the text in an
issueprovidesinformationaboutthedeveloper‚Äôsintent[ 12].Inthe
Jaspersystem,adevelopercancreatespecialworkingareasoftheir
development environment into which fragments of work can be
placed forlater recall [ 3]. Theapproachwe consider in this paper
relievesthedeveloperfromaprioriindicatingworkonaspecific
task.
Otherresearchershaveattemptedtodetermineautomaticallythe
higher-levelactivitiesdevelopersperformbasedontheirinterac-
tionwiththecomputer.Forexample,Mirzaetal.usedtemporaland
semanticfeaturesbasedonwindowinteractionsandthewindow
titlesover5minutetimewindowstopredictoneofsixworkactivitycategories:writing,reading,communicating,systembrowsing,web
browsing, and miscellaneous [ 22]. In a controlled lab study and
fieldstudywith5participants,theyachievedanaccuracyof81%.
Koldijk et al. investigated the predictive power of keyboard and
mouseinput,aswellasapplicationswitchesandthetimeofday,for
predicting a larger set of 12 high-level task types‚Äîsuch as reading
email, programming, creating a visualization‚Äîfor a given 5 minute
period of time [ 14]. Using classifiers trained on an individual basis,
theywereabletoachieveupto80%accuracy.However,theyfound
thataclassifiertrainedononepersonishighlyindividualtothat
personanddoesnotgeneralizewelltootherpeople.Inanapproach
morespecifictosoftwaredevelopers,Baoetal.exploretheuseof
conditional random fields (CRFs) to predict one of six development
activities: coding,debugging, testing, navigation, search,or docu-
mentation[ 2].Applyingtheirapproachtodatacollectedfrom10
software developers over a week, the authors found they were abletoclassifyanactivitywithanaccuracyof73%.TheresultsofBaoetal.pointtothedifficultyofdeterminingatafinegranularitywhatadeveloper is working on at a specific moment. In our work, we aimtodeterminethecontentofadeveloper‚Äôstaskratherthanthekindof
activity being undertaken, which we see as a complementary goal.
Intent from Documents. Researchers have also looked into the
extractionofintentfromnaturallanguagedocumentsassociated
withasoftwaredevelopment.Earlyon,researchershavetriedtodetect the coarse intent of sentences in emails and tried to sum-
marizethem,forexampletoaddthemtoatodolist(e.g.,[ 4]).Di
Sorbo et al. introduced the concept of intention mining in the con-
textofemailsinsoftwaredevelopment[ 33].TheyusedaNatural
Language Processing (NLP) approach to classify the content of de-
velopmentemailsaccordingtothepurposeoftheemails,suchas
feature request or information seeking. The researchers definedsix categories that describe the intent of a developer‚Äôs sentence
andreporteda90%precisionand70%recallfortheirapproachin
the context of email intent classification. Huang et al. attempted to
generalize the approach of Di Sorbo et al. to developer discussions
in other mediums, for example those contained in issue reports [ 9].
They found that the NLP patterns used did not adapt well to other
mediums,achievinganaccuracyofonly0.31.Byrefiningthetax-
onomy of intentions defined by Di Sorbo et al., and applying a
convolutionalneuralnetwork(CNN)basedapproach,theauthors
wereabletoimproveontheresultsoftheoriginalpaperby171%.
Theseapproachesaimtoclassifywhatthecontentofadocument
is attempting to state as compared to our approach in this paper
which aims to determine what the developer is attempting to do.
IntentfromaCombinationofInteractionsandDocuments. Shen
et al. [31] use a combination of information about how a user
interactswithwindowsontheirscreenandemailmessagestheuser
handlesintheirTaskPredictorsystem.Usingsupervisedmachine
learning,theypredictonwhichtaskauserisworking.However,
thistechniquesrequirestheusertopre-definethetasksonwhich
theyworksothattheycanbepredictedandtheclassifierneedstobetrainedonsomeoftheuser‚Äôsdatabeforehand.Ourapproachdiffers
inassessingmethodsforrepresentingtheworkbeingperformed
based on information that a developer works on through screen
scraping;theserepresentationscanbeusedforpredictingwhichof
a known set of tasks the work represents and for generating word
cloudrepresentationsoftheworkthatadevelopercanrecognize
irrespective of having a set of known tasks.
Finding Meaning in Artifacts. The content of artifacts created as
partof,orabout,softwaredevelopmentcontainsignificantmeaning.
Software engineering researchers have developed techniques to
findparticularmeaninginartifactsthathavesimilarcharacteristicstotheapproachwedevelopinthispaper.Forexample,Ponzanelliet
al. present CodeTube, an approach that mines video tutorials from
thewebtoenabledeveloperstoquerythecontentsofthetutorialtoretrieverelevantfragments[
28].Theauthorsused OCRandspeech
recognition in order to extract text from the videos and evaluate
the relevancy of fragments to the user‚Äôs query. The determination
of what a segment of video is about is similar to the problem we
tackle of what a segment of a developer‚Äôs work is about.
8 SUMMARY
Haveyoueverwonderedwhatyouworkedonthroughoutaday,
possibly to record time spent on different projects? Have you ever
wanted to look back and find where you worked on a particular
task to find what resources you consulted as part of the task?
Thispaperintroducesandevaluatesanapproachtohelpsupport
thesegoals.Givenknowledgeoftaskboundaries,whichispossi-ble from using automated task switch detection techniques, our
806approach extracts the contents of the active window being worked
with on a regular basis, uses optical character recognition (OCR)
andwordtokenizationtotransformthecontentsintotokensand
words,andappliesTF-IDFtoformavectorrepresentationofthe
tasksegment.Oninformationseekingtasks,weshowedthatthis
vectorrepresentationcanhelpidentifywhichtaskasegmentrepre-
sents for a known set of tasks with an averaged accuracy of 70.6%.
Wealsoinvestigatedtheproductionofwordcloudrepresenta-
tions of a task segment using TF-IDF scores for each word in a bag
of words formed from the screen content of the task segment as
adeveloper worked.Througha preliminary evaluation,wefound
that participants could determine which task a word cloud for a
segmentofworkrepresentedwithreasonableaccuracy(67.9%on
average)forseveralinformation-seekingtasks.Interestingly,the
accuracyroseonlymodestlywhenconsideringidentifyingthetask
based on a word cloud formed from all segments comprising work
on a task.
This approach shows promise for helping to determine automat-
ically the task on which a developer is working during different
time periods. When the task can be identified, various tools canbe improved that a developer relies upon and new tools can be
introduced to help support such activities as time tracking. Future
workcaninvestigatehowtheapproachweintroduceappliestoa
broader set of kinds of tasks performed by a developer.
ACKNOWLEDGEMENTS
We thank all our study participants and the reviewers for their
helpful feedback. This work was funded, in part, through a collabo-
rativegrantwithABBInc.(IndustrialResearchGrantF17-05273-
22R77358), supported by NSERC (CRDPJ 530226-18).
REFERENCES
[1]Anonymous.2020. SupplementalMaterialforthepaper"IdentifyingandDescrib-
ing a Software Developer‚Äôs Tasks". https://doi.org/10.5281/zenodo.3764485
[2]Lingfeng Bao, Zhenchang Xing, Xin Xia, David Lo, and Ahmed E. Hassan.
2018. Inference of development activities from interaction with uninstrumented
applications. Empirical Software Engineering 23, 3 (June 2018), 1313‚Äì1351.
https://doi.org/10.1007/s10664-017-9547-8
[3]Michael J. Coblenz, Andrew J. Ko, and Brad A. Myers. 2006. JASPER: An Eclipse
Plug-in to Facilitate Software Maintenance Tasks. In Proceedings of the 2006
OOPSLA Workshopon Eclipse TechnologyExchange. Associationfor Computing
Machinery, 65‚Äì69. https://doi.org/10.1145/1188835.1188849
[4]Simon Corston-Oliver, Eric Ringger, Michael Gamon, and Richard Campbell.
2004. Task-focusedsummarizationofemail.In TextSummarizationBranchesOut .
Association of Computational Linguistics, 43‚Äì50.
[5]K.Damevski,H.Chen,D.C.Shepherd,N.A.Kraft,andL.Pollock.2018.Predicting
Future Developer Behavior in the IDE Using Topic Models. IEEE Transactions on
Software Engineering 44, 11 (2018), 1100‚Äì1111. https://doi.org/10.1109/TSE.2017.
2748134
[6]M√°rcioKurokiGon√ßalves,CleidsonRBdeSouza,andVictorMGonzalez.2011.
Collaboration,InformationSeekingandCommunication:AnObservationalStudy
of Software Developers‚Äô Work Practices. J. UCS17, 14 (2011), 1913‚Äì1930.
[7]VictorMGonz√°lezandGloriaMark.2004. ‚ÄúConstant,Constant,Multi-tasking
Craziness‚Äù: Managing Multiple Working Spheres. In Proceedings of the 2004
Conference on Human Factors in Computing Systems, CHI 2004. 113‚Äì120.
[8]Thomas Gottron. 2009. Document Word Clouds: Visualising Web Documents
as Tag Clouds to Aid Users in Relevance Decisions. In Research and Advanced
Technology for Digital Libraries, Maristella Agosti, Jos√© Borbinha, Sarantos Kap-
idakis, Christos Papatheodorou, and Giannis Tsakonas (Eds.). Springer Berlin
Heidelberg, 94‚Äì105.
[9]QiaoHuang,XinXia,DavidLo,andGailC.Murphy.2018. AutomatingIntention
Mining.IEEE Transactions on Software Engineering (2018), 1‚Äì1. https://doi.org/
10.1109/TSE.2018.2876340 Early access.
[10]ImageMagick. 2020. ImageMagick. https://imagemagick.org/index.php. [Ac-
cessed March 5, 2020].[11]Tom Kenter, Alexey Borisov, and Maarten De Rijke. 2016. Siamese cbow:Optimizing word embeddings for sentence representations. arXiv preprint
arXiv:1606.04640 (2016).
[12]Mik Kersten and Gail C. Murphy. 2006. Using Task Context to Improve Pro-grammer Productivity. In Proceedings of the 14th ACM SIGSOFT International
SymposiumonFoundationsofSoftwareEngineering (SIGSOFT‚Äô06/FSE-14).ACM,
1‚Äì11. https://doi.org/10.1145/1181775.1181777
[13]KatjaKevicandThomasFritz.2017. TowardsActivity-AwareToolSupportfor
Change Tasks. In 2017 IEEE International Conference on Software Maintenance
and Evolution (ICSME). IEEE, 171‚Äì182.
[14]SaskiaKoldijk,MarkvanStaalduinen,MarkNeerincx,andWesselKraaij.2012.
Real-time task recognition based on knowledge workers‚Äô computer activities.
InProceedingsofthe30thEuropeanConferenceonCognitiveErgonomics (ECCE
‚Äô12). Association for Computing Machinery, 152‚Äì159. https://doi.org/10.1145/
2448136.2448170
[15]Mary L. McHugh. 2012. Interrater reliability: the kappa statistic. Biochemia
Medica22, 3 (Oct. 2012), 276‚Äì282.
[16]Andr√© Meyer, Gail C Murphy, Thomas Zimmermann, and Thomas Fritz. 2017.
Design Recommendations for Self-Monitoring in the Workplace: Studies in Soft-
wareDevelopment. PACMonHuman-ComputerInteraction 1,CSCW(2017),1‚Äì24.
https://doi.org/10.1145/3134714
[17]A. N. Meyer, L. E. Barton, G. C. Murphy, T. Zimmermann, and T. Fritz. 2017.
The Work Life of Developers: Activities, Switches and Perceived Productivity.
IEEE Transactions on Software Engineering 43, 12 (2017), 1178‚Äì1193. https:
//doi.org/10.1109/TSE.2017.2656886
[18]A.N.Meyer,C.Satterfield,M.Z√ºger,K.Kevic,G.C.Murphy,T.Zimmermann,and
T.Fritz.2020. DetectingDevelopers‚ÄôTaskSwitchesandTypes. IEEETransactions
on Software Engineering (2020). Early access.
[19]TomasMikolov,IlyaSutskever,KaiChen,GregSCorrado,andJeffDean.2013.
Distributed representations of words and phrases and their compositionality. In
Advances in neural information processing systems. 3111‚Äì3119.
[20]AllenE.Milewski.2007. Globalandtaskeffectsininformation-seekingamong
softwareengineers. Empir.Softw.Eng. 12,3 (2007),311‚Äì326. https://doi.org/10.
1007/s10664-007-9036-6
[21]HamidTurabMirza,LingChen,GencaiChen,IbrarHussain,andXufengHe.2011.
Switch detector: an activity spotting system for desktop. In Proceedings of the
20th ACMInternational Conferenceon Information andKnowledge Management
(CIKM‚Äô11).AssociationforComputingMachinery,2285‚Äì2288. https://doi.org/
10.1145/2063576.2063947
[22]HamidTurabMirza,LingChen,IbrarHussain,AbdulMajid,andGencaiChen.
2015. AStudyonAutomaticClassificationofUsers‚ÄôDesktopInteractions. Cy-
bernetics and Systems 46, 5 (2015), 320‚Äì341. https://doi.org/10.1080/01969722.
2015.1012372
[23]Mozilla. 2020. Bugzilla. https://bugzilla.mozilla.org/home. [Accessed August 31,
2020].
[24]NLTK.2020. NaturalLanguageToolkit(NLTK). https://www.nltk.org/. [Accessed
March 5, 2020].
[25]NuriaOliver,GregSmith,ChintanThakkar,andArunCSurendran.2006. SWISH:
semanticanalysisofwindowtitlesandswitchinghistory.In Proceedingsofthe
11th International Conference on Intelligent User Interfaces. 194‚Äì201.
[26]Chris Parnin and Robert DeLine. 2010. Evaluating cues for resuming interrupted
programmingtasks.In ProceedingsoftheSIGCHIConferenceonHumanFactors
in Computing Systems (CHI ‚Äô10). Association for Computing Machinery, 93‚Äì102.
https://doi.org/10.1145/1753326.1753342
[27]Chris Parnin and Spencer Rugaber. 2011. Resumption strategies for interrupted
programmingtasks. SoftwareQualityJournal 19,1(March2011),5‚Äì34. https:
//doi.org/10.1007/s11219-010-9104-9
[28] Luca Ponzanelli,GabrieleBavota,AndreaMocci,Massimiliano DiPenta,Rocco
Oliveto, Barbara Russo, Sonia Haiduc, and Michele Lanza. 2016. CodeTube:
Extracting Relevant Fragments from Software Development Video Tutorials. In
2016IEEE/ACM38thInternationalConferenceonSoftwareEngineeringCompanion
(ICSE-C). 645‚Äì648.
[29]RAKE.2020. PythonimplementationoftheRapidAutomaticKeywordExtraction
algorithm using NLTK. https://pypi.org/project/rake-nltk/. [Accessed March 5,
2020].
[30]Stuart Rose, Dave Engel, Nick Cramer, and Wendy Cowley. 2010. Automatic
KeywordExtractionfromIndividualDocuments. In TextMining:Applicationsand
Theory. 1‚Äì20. https://doi.org/10.1002/9780470689646.ch1 Journal Abbreviation:
Text Mining: Applications and Theory.
[31]Jianqiang Shen, Werner Geyer, Michael Muller, Casey Dugan, Beth Brownholtz,
andDavidRMillen.2008. Automaticallyfindingandrecommendingresources
to support knowledge workers‚Äô activities. In Proceedings of the 13th International
Conference on Intelligent User Interfaces (IUI ‚Äô08). Association for Computing
Machinery, 207‚Äì216. https://doi.org/10.1145/1378773.1378801
[32]Jianqiang Shen, Lida Li, and Thomas G. Dietterich. 2007. Real-time detectionoftaskswitchesofdesktopusers.In Proceedingsofthe20thInternationalJoint
Conference on Artifical Intelligence (IJCAI‚Äô07). Morgan Kaufmann Publishers Inc.,
2868‚Äì2873.
807[33]AndreaDi Sorbo,SebastianoPanichella,CorradoA. Visaggio,MassimilianoDi
Penta, Gerardo Canfora, and Harald C. Gall. 2015. Development Emails Content
Analyzer:IntentionMininginDeveloperDiscussions(T).In 201530thIEEE/ACM
International Conference on Automated Software Engineering (ASE). 12‚Äì23. https://doi.org/10.1109/ASE.2015.12
[34]Tesseract.2020.TesseractOpenSourceOCREngine.https://github.com/tesseract-
ocr. [Accessed March 5, 2020].
808