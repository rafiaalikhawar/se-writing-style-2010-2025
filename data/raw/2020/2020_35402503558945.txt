Understanding Why We Cannot Model How Long a Code Review
Will Take: An Industrial Case Study
Lawrence Chen
Meta Platforms, Inc.
Menlo Park, CA, USA
lawrencechen@fb.comPeter C. Rigby‚àó
Meta Platforms, Inc.
New York, NY, USA
pcr@fb.comNachiappan Nagappan
Meta Platforms, Inc.
Bellevue, WA, USA
nnachi@fb.com
ABSTRACT
Code review is an effective practice for finding defects, but because
it is manually intensive it can slow down the continuous integration
of changes. Our goal was to understand the factors that influenced
the time a change, i.e.a diff at Meta, would spend in review. A
developer survey showed that diff reviews start to feel slow after
they have been waiting for around 24 hour review. We built a
review time predictor model to identify potential factors that may
be causing reviews to take longer, which we could use to predict
when would be the best time to nudge reviewers or to identify
diff-related factors that we may need to address.
The strongest feature of the time spent in review model we built
was the day of the week because diffs submitted near the weekend
may have to wait for Monday for review. After removing time on
weekends, the remaining features, including size of diff and the
number of meetings the reviewers have did not provide substantial
predictive power, thereby not being able to predict how long a code
review would take.
We contributed to the effort to reduce stale diffs by suggesting
that diffs be nudged near the start of the workday and that diffs
published near the weekend be nudged sooner on Friday to avoid
waiting the entire weekend. We use a nudging threshold rather
than a model because we showed that hours in review cannot be
accurately modelled. The NudgeBot has been rolled to over 30k
developers at Meta.
CCS CONCEPTS
‚Ä¢Software and its engineering ‚ÜíSoftware development pro-
cess management .
KEYWORDS
Code Review, Statistical Modelling
‚àóRigby is an associate professor at Concordia University in Montreal, QC, Canada.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
ESEC/FSE ‚Äô22, November 14‚Äì18, 2022, Singapore, Singapore
¬©2022 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 978-1-4503-9413-0/22/11. . . $15.00
https://doi.org/10.1145/3540250.3558945ACM Reference Format:
Lawrence Chen, Peter C. Rigby, and Nachiappan Nagappan. 2022. Under-
standing Why We Cannot Model How Long a Code Review Will Take: An
Industrial Case Study. In Proceedings of the 30th ACM Joint European Soft-
ware Engineering Conference and Symposium on the Foundations of Software
Engineering (ESEC/FSE ‚Äô22), November 14‚Äì18, 2022, Singapore, Singapore.
ACM, New York, NY, USA, 6 pages. https://doi.org/10.1145/3540250.3558945
1 INTRODUCTION
Best engineering practices allow developers to take risk and cele-
brate failures as they allow companies to avoid wasting resources
on something that does not and cannot be made to work. It is im-
portant to report these results to the broader software engineering
community, and in this paper, we report our attempt to model how
long a code review will take, and why we were unable to create a
successful model.
Before we discuss the model, why is it important to model the
amount of time something will be under review? Code review has
become increasingly incremental with Fagan [ 5] inspection taking
weeks to months in the late 70‚Äôs to the present day where code
reviews take on the order of hours to a day [ 8,9,14]. However,
code review is a manual process that can limit the productivity
and the pace at which code can be continuously integrated and
delivered. Our goal is to model the review interval using traditional
mining software repository measures, such as churn, as well as
new measures, such as how many meetings a reviewer has and the
amount of time the files under review have taken to review in the
past.
The original practical goal for our hours in review model was
to predict how long a diff will be in-review and then to nudge re-
viewers of a diff that has taken longer than was predicted, i.e.the
diff review is ‚Äústale‚Äù. This model is related to the Nudge system
deployed at Microsoft [ 6] to nudge developers on overdue pull re-
quest. However, there are significant differences, Microsoft reports
a mean absolute error of 32.60 hours. Given that 75% of reviews at
Meta take less than 24 hours, a model with this level of inaccuracy
would be unusable at Meta. The goal of this paper is to report our
attempt to investigate a review time prediction model in a new
industrial context.
The overall research question for this work is how long will this
diff be in review? We create a random forest model that includes
multiple factors such as expected review time for the directories
under review, the workload of the developers reviewing the files,
and the size of the change. We found that the strongest predictor
was the day of week, with diffs published nearer to the weekend
waiting until Monday for review. Removing weekend hours, the
remaining features had poor predictive power: they reduced the
median absolute error by only 30 minutes over the baseline model.
1314
ESEC/FSE ‚Äô22, November 14‚Äì18, 2022, Singapore, Singapore Lawrence Chen, Peter C. Rigby, and Nachiappan Nagappan
This indicated that a prediction model does not work at Meta. Our
work guided the development of a threshold for nudging developers
with a bot that has been rolled out to all developers at Meta.
2 BACKGROUND AND METHODOLOGY
Meta is a large online services company that works across the spec-
trum in the communications, retail, entertainment industries. Meta
has tens of thousands of employees with offices spread across the
globe (North America, Europe, Middle East and Asia). Meta has its
own dedicated infrastructure teams where the tools used are a mix
of commercial and in-house developed systems. We study reviews
from all the projects using Phabricator including Facebook and
WhatsApp.
Code review at Meta follows a similar structure to the contem-
porary practices used at Google [ 14] and Microsoft [ 1], so we only
briefly summarize the practice here. Code review is conducted in
thePhabricator tool. A developer makes a change, called a diff,
selects potential reviews and teams of reviews and publishes it
for review. Reviewers provide feedback and comments which are
addressed by the author. There may be multiple rounds of feedback
and changes, ultimately, the diff will be accepted or rejected. Meta
uses a single truck development model, so if the diff is accepted it
will be integrated for further testing and then ultimately ‚Äúshipped‚Äù
into production. For this paper, our data set includes all diff re-
view sessions between June 8, 2021 and August 17, 2021 (70 days)
covering over 150k reviews.
DevEx Survey. The motivation for modeling hours in review
came from the Developer Experience (DevEx) survey that is con-
ducted each half at Meta. In the second quarter of 2020, around 1.9k
developers responded to the DevEx survey. Here we focus on the
question related to hours in review: "How satisfied are you with the
time it takes to get your diffs reviewed by peers?" From the survey
alone, we learned that 84.7% of developers are satisfied with the
time their diffs spend in review, i.e. 15.3% are dissatisfied. While the
vast majority of developers are satisfied, when we anonymously
associate the time it takes for a developers slowest diffs to be re-
viewed, we found that developers that had longer review times
were less satisfied and less likely to respond that they were sat-
isfied. In discussions with the Phabricator team, that maintains
the diff review tool, we decided to model the slowest 25% of diffs,
which on the Meta dataset roughly corresponds to diffs that have
been in-review for 24 or more hours. More details can be found in
Shan et al. [15].
2.1 Model Features
We created a model to predict how long a diff will be in review
using the features described in Table 1. We suspected information
about the files contained in the diff under review would influence
the time a diff is in review. For example, larger changes may take
longer to review. We included the repo name for each project to
factor in project-level differences in the time in review, the historical
median time taken by diffs that modified the files in the directories
to understand if review time varies by directory, and the length
of words in the diff summary to understand whether the size of
the explanatory text for the change affects time in review. We
differentiated between code generated by human developers andautomated refactoring scripts ( i.e.rule-based changes). We also
included the day of week and time of day to understand whether
reviews submitted later in the week or in the day effect time in
review, the diff author‚Äôs timezone, and the number of reviewers
directly added to the diff as well as the number of reviewers added
through review groups. Finally, we factored in the average number
of meetings that the assigned reviewers have scheduled over the
next 48 hours as an indicator of meeting workload.
2.2 Model and Evaluation Method
To evaluate the model of time in review, the TimeInReview Model ,
we compared the predicted review session duration with the actual
review session time for each diff. We narrowed our data to diffs
with review session times between 24 hours and 140 hours, roughly
between P75 and P95 of review time. This helped us reduce the
skew of the distribution: a large proportion of diff review times are
under 24 hours, and they were not the target diffs of our Overdue
Diff Nudging initiative.
We built a random forest model using the features described in
Table 1. Our model was trained on 120k review sessions and tested
on 30k review sessions between June 8, 2021 and August 17, 2021
(70 days). To evaluate the importance of each feature, we use the
feature importance package from scikit-learn.1This determines the
decrease in the score when each feature is removed and indicates
its importance.
For comparison, we also built a baseline model that predicted that
every review would take the median time in review of our review
session data set, to compare our new random forest with a trivial
modeling approach. This trivial baseline represents a unsophisti-
cated method of predicting review time, without any consideration
of diff or reviewer features.
Our outcome measure is the absolute error in hours, with the
following metrics: mean, median (MAE), p75, and max. We contrast
this with our baseline to evaluate how much of an improvement
our more sophisticated model has compared to a trivial model.
3 RESULTS
Can we predict how long a diff will be in review?
In Table 2, we see that the TimeInReview Model accurately
predicts the length of reviews with a MAE of 14.89 hours compared
to the baseline model that always predicts the median delay and
has a MAE of 25.91 hours, an improvement in MAE of 11.02 hours.
In Table 3, we see the permutation importance of each feature.
We discuss each by feature importance: (1) the most important
feature is the day of the week. Intuitively, this makes sense as diffs
submitted for review on Fridays will often have to wait the entire
weekend for a review. (2) the hour of day is intuitively important
with diffs submitted later in the day waiting until the following
morning for review. (3) Geography, as measured by the timezone
in which the author resides, influences review time. A developer
who makes a change in London, will have to wait for a teammate
in California to wake up before the review can happen. (4) the
average number of meetings the reviewers have scheduled in the
next 48 hours impacts time in review. Meetings take time away
from time available for reviewing and other development activities.
1https://scikit-learn.org/stable/modules/permutation_importance.html
1315Understanding Why We Cannot Model How Long a Code Review Will Take: An Industrial Case Study ESEC/FSE ‚Äô22, November 14‚Äì18, 2022, Singapore, Singapore
Table 1: The Features for the TimeInReview Model
Feature Description
list of file features Lines added, lines removed, number of files modified, number of different file extentions,
line count ratio, total line count of files in the diff
repository name The name of the project repo that the diff modifies (one-hot encoding)
median directory review
timeThe median time that the files in the directory in the diff take to review in the past
summary count words ex-
cluding code blockThe summary of the code change without any code markdown.
rule based change Is this diff created using a rule? An example, if a human wants to rename a widely used
class across the entire code base, they would apply a tool-based rule to implement the diff.
author is bot Is the code author a bot or developer?
review iteration How many times has the diff been updated during the review?
day of week The day of week when the author published the diff for review
hour of day The hour of day when the author published the diff for review
author timezone The author‚Äôs timezone. Captures geographical work office location
num direct reviewers Direct reviewer is a reviewer that is explicitly added by name
reviewer is same team per-
centagePercentage of reviewers that are on the same team as the author
avg direct reviewer meet-
ings hoursAverage number of scheduled meetings hours across assigned reviewers in the 48 hour
period after the diff is published.
num group reviewers Number of reviewers added as part of a team/group
group reviewer is same team
percentagePercentage of group reviewers that are on the same team as the author
avg group reviewer meet-
ings hoursAverage number of scheduled meetings hours across assigned reviewers in the 48 hour
period after the diff is published.
Table 2: Model quality for TimeInReview Model and
TimeInReview ModelNoWeekends vs the baseline models. The
baseline models simply predict the median value for each
distribution. After removing weekends, the model poorly
predicts time in review. All values in absolute hours.
Mean AE Median AE P75 AE Max AE
Median Base-
line26.46 25.91 37.34 76.39
TimeInReview
Model19.74 14.89 27.13 99.85
Median
NoWeekends
Baseline19.89 17.00 24.00 75.00
TimeInReview
Model
NoWeekends19.08 16.54 26.22 91.63
(5) the length of the summary of the diff is likely related to the
complexity of the change with more explanation around complex
or difficult changes. (6) code changes that are done using a rule,
such as renaming a class, are easier to review (7) the time to review
changes in the past in a directory predicts how long they will take
to review in the future. (8, 9) the more reviewers in a review group
and the more reviews on from the same team, the more likelyTable 3: Importance of TimeInReview Model Features
Feature Importance
1. day of week 0.56
2. hour of day 0.12
3. author timezone 0.06
4. avg group reviewer meetings hours 0.05
5. summary count words excluding code block 0.03
6. is rule based changed 0.03
7. median directory review time 0.02
8. num group reviewers 0.02
9. group reviewer is same team percentage 0.01
10. avg direct reviewer meetings hours 0.01
11. lines added 0.01
12. line count 0.01
13. repository 0.01
14. file extensions 0.01
more people will see the diff. (10) the number of meeting hours
that directly assigned reviewer has only a weak influence on time
in review likely because they prioritize assigned reviews. (11-14)
surprisingly, features related to the actual source code have lower
importance. The lines added and total count of changed lines have
a weak influence on interval. The time in review appears to hold
1316ESEC/FSE ‚Äô22, November 14‚Äì18, 2022, Singapore, Singapore Lawrence Chen, Peter C. Rigby, and Nachiappan Nagappan
across Meta repositories and across file extensions with both having
a relatively low impact on time in review.
The model appears to learn clear patterns regarding weekend
delays, but this pattern is obvious and one that can be accounted
for using rules about the weekend rather than statistical models.
We were more interested in observing whether the model can find
other patterns related to the diff features or reviewer character-
istics, besides the impact of weekends. Our next step was to re-
move the weekend hours from the hours in review and create an
TimeInReview ModelNoWeekends . This updated model is identical
toTimeInReview Model in terms of model architecture and fea-
tures, but uses a modified hours in review that excludes weekend
hours.
Table 2 shows that the baseline model which simply predicts the
median wait time has a MAE of 17.00 hours, while TimeInReview
ModelNoWeekends has a MAE of 16.54 hours. The TimeInReview
ModelNoWeekends improves median time predictions by less than
30 minutes. We do not report individual feature importance because
the model performs so poorly. It is clear that once weekends are
removed the other factors (such as diff size, number of meetings,
and project repository) are not strong enough indicators of review
time to enable our model to outperform the baseline. We believe this
is because the review time at Meta is already short, so the natural
variance of review times will diminish any correlation effects of
the remaining features.
Our model‚Äôs best feature was whether the review spans
the weekend. Once we used a rule to account for week-
ends, the resulting model improved the MAE by only 30
minutes relative to predicting the median review time for
all reviews. Given the poor performance of the model, we
cannot recommend it for use in review time prediction
tasks, such as nudging on overdue diffs.
Retrospective Correlation Analysis. Models that predict time in re-
view naturally cannot contain features that are only known after the
review is complete. However, a retrospective analysis can highlight
factors that influence hours in review. One potentially important
factor is the degree of participation and how long it takes to get
reviewer attention. When we conducted a Spearman correlation
between hours in review and "the time to first action or comment,"
we found an ùëü=0.90with ùëù<0.001. While correlation is not cau-
sation, a correlation as high as 0.90 indicates that hours in review
is strongly related to how long it takes to get a reviewer to take
a look at the diff and provide a comment or accept or reject the
diff. Clearly a model that nudges developers on diffs that are taking
longer than the median time for review deserve to be nudged to
get the attention of a reviewer.
4 IMPACT AT META
As part of the team responsible for reducing hours in review our
initial model that included weekends was used to determine when
to nudge reviewers on diffs. Our analysis had important impacts on
the tool and final rollout. First, our original model had been used
to determine when a diff was beyond the expected amount of time
for review, i.e.overdue, and nudges needed to be sent to reviewers.
Figure 1: NudgeBot sends up to three diffs that are stale to a
developer who is likely to review them one hour after the
start of the workday. The chat message is sent with #silent,
which will not push notify the developer, but will allow them
to view the message between blocks of focus. The time the
diff has been waiting is shown along with the username of
the author and a clickable link to the diff. If the reviewer
is busy, they can select ‚ÄúRemind Me Later." NudgeBot then
examines the calendar and selects the next fragmented time,
e.g.,between two meetings, to remind the developer later in
the day. Emails are sent with the same information and same
ability to be reminded later. The diff numbers, log message,
etc are mocked to preserve confidentiality, but they are rep-
resentative of real diffs at Meta.
However, we convinced the team that no model was necessary and
that a threshold should be used to nudge reviewers because our
model could not outperform a simple median review time model.
This drastically reduced the engineering effort. Second, we showed
the enormous impact of weekends and time of day on how long a
diff will take to review. As a result, the nudges are sent first thing
in the morning to developers, and the diffs submitted on Thursday
are nudged with a shorter threshold so that reviewers take a look
at them before the end of day on Friday.
The final NudgeBot, which is in use by over 30k developers at
Meta and sends over 4000 nudges per day on overdue diffs is shown
in Figure 1, and the full design can be found in Shan et al. [15]. Any
diff that has been inactive for 24 hours has the reviewer that is
most likely to take action on the review nudged via chat. Reviewers
can also ‚Äúsnooze‚Äù notification, which will resend the nudge to the
reviewer when the are not in deep focus, e.g.,after a meeting ends
or after lunch. The tool has been successful in reducing hours in
review by around 10% in an A/B test. Fewer than 1% of developers
have opted out of nudging indicating that developers find these
reminders useful.
5 THREATS TO VALIDITY
This paper focuses on our industrial experience and it unclear that
our results would generalize to other contexts. Like the Microsoft
Nudge paper [ 6], we have very low predictive power of review
interval despite a very different development context: Meta uses
a mono repo (single repository for code control) rather than the
Microsoft branch structure [ 3]. Also the systems built are different,
1317Understanding Why We Cannot Model How Long a Code Review Will Take: An Industrial Case Study ESEC/FSE ‚Äô22, November 14‚Äì18, 2022, Singapore, Singapore
the domain, the programming language are all different. This also
shows the value of replicating empirical studies in different contexts
to evaluate the efficacy of results to build an empirical body of
knowledge [2].
We only look at reviews that take 24 or more hours in our model.
This decision was made because nudging a reviewer on a diff that
is less than a day old will distract and annoy developers. Other
software companies may decide to look at the entire review interval,
but that was not desirable at Meta.
The outcome of time in review was highly susceptible to week-
end delays. We corrected this metric to remove weekends. However,
some reviews occur on the weekend and these were ignored. The
impact of the weekend also suggests interesting future work to cor-
rect for ‚Äúworking hours" in the metric. This is in-line with recently
published results [ 6] which found that two-thirds of engineers did
not work weekends or nights. Irrespective of this, the data used in
this study is real live data from development at Meta and hence, the
vagaries that exist in the data reflect the reality without ignoring
or removing true events that happened in the data.
We did not perform any hyperparameter tuning for our
TimeInReview Model , which leaves open the possibility that these
models can be further improved through a rigorous hyperparameter
optimization. By using unoptimized defaults, the hyperparameters
were fixed before looking at the test set to ensure no contamination.
In addition, there is no reason that a hyperparameter search on
random forest would result in dramatic changes in which features
are considered the most important, which is what we focused on in
the end. The first hyperparameter of ‚Äòdepth‚Äô would mostly impact
features that are less important, since most major features would
already be accounted for in the first few layers of the model‚Äôs trees.
In addition, the other hyperparamer of ‚Äònumber of trees‚Äô would be
expected to have negligible impact on feature importances given
that each tree in the random forest is independent.
6 RELATED WORK
Code review has been a best practice in software development for
over 45 years [ 5]. In the 1970‚Äôs, Fagan reported that inspections took
weeks or months to complete. Fagan‚Äôs inspection process relied on
formal roles and synchronous meetings. Votta [ 16] showed in the
1990‚Äôs that scheduling inspection meetings added 20% to the review
interval. Eick et al. [4] found that over 90% of the defects found
during inspection could be found in the without a meeting, i.e.in
the individual preparation phase. In a controlled experiment, Perry
et al. [7] showed that synchronous meetings were unnecessary and
that asynchronous inspections were equally effective at finding
defects. Building on these results, Porter et al. [8] simplified inspec-
tion and found that it could be reduced to around one week. Theses
gains in shortening the review interval by reducing the complexity
of the process has continued with Rigby et al. [10,12] showing that
the contemporary lightweight review process used by open source
projects had a review interval of approximately one day, which was
confirmed to be similar to the review time at Microsoft [ 11] and
Google [ 14]. Meta is no exception with short code review intervals
on the order of a few hours and with over 75% of reviews being
completed in less than a day.Although it is simple to report how long code reviews take, we
are unaware of any works that have successfully modelled the
time in review. In the 1990‚Äôs Porter et al. [8] used a regression
model of review interval with the factors of the team size, the
number of review sessions, and categorical variables for author and
reviewers. Their model could only explain 25% of the variance in
the hours in review. Rigby et al. [12] modelled the review interval
of large open source projects including Apache and Linux. Their
model included author and reviewer experience, the churn, and
the number of reviews involved in the review. Like Porter et al.
their regression model was only able to explain 29% of the variance.
Microsoft recently created a model of review interval with the
goal of nudging developers on pull requests that had taken longer
than the model predicted [ 6]. Unfortunately, they report very low
accuracy with a mean average error of 32.60 hours and a mean
relative error of .58. Our random forest model of hours in review
was also inaccurate, with a mean average error of 19.08 hours and
only a 30 minute improvement over the baseline model that always
predicts the median time.
7 CONCLUDING REMARKS
How long will this diff be in review? Prior works on modeling the
hours in review have shown low predictive power [ 6,8,13]. At
Meta, the model shows that review times are consistent regardless
of factors such as diff size, the number of meetings reviewers have,
and the project repository. The largest contributing factor is wait
time. This is seen in the importance of reviews that span weekends,
e.g.,they usually wait an extra 48 hours, and diffs submitted later in
the day will wait overnight. The key is to get the diff in front of the
reviewer before the review starts to feel too slow to the author. From
the developer experience survey this roughly corresponds to diffs
that have had no actions for 24 or more hours. After removing the
weekend hours from the TimeInReview Model , the model did not
outperform the baseline model which always predicts the median
hours in review from the training set. As a result, the overdue diff
nudging tool shown in Figure 1 nudges developers after 24 hours
and pings them at the start of the work day.
ACKNOWLEDGEMENTS
We would like to thank Celeste Barnaby, Dave Sukhdeo, Tobi Ako-
molede, and Rui Abreu for feedback on our model, data, and features.
We also thank Akin Olugbade, Katherine Zak, Louise Huang, and
Seth Rogers for getting NudgeBot into production.
REFERENCES
[1]Alberto Bacchelli and Christian Bird. 2013. Expectations, outcomes, and chal-
lenges of modern code review. In Proceedings of the 2013 international conference
on software engineering . IEEE Press, 712‚Äì721.
[2]V.R. Basili, F. Shull, and F. Lanubile. 1999. Building knowledge through families
of experiments. IEEE Transactions on Software Engineering 25, 4 (1999), 456‚Äì473.
https://doi.org/10.1109/32.799939
[3]Christian Bird and Thomas Zimmermann. 2012. Assessing the Value of Branches
with What-If Analysis. In Proceedings of the ACM SIGSOFT 20th International
Symposium on the Foundations of Software Engineering (Cary, North Carolina)
(FSE ‚Äô12) . Article 45, 11 pages.
[4]Stephen G Eick, Clive R Loader, M David Long, Lawrence G Votta, and Scott
Vander Wiel. 1992. Estimating software fault content before coding. In Proceedings
of the 14th international conference on Software engineering . 59‚Äì65.
[5]M. E. Fagan. 1976. Design and Code Inspections to Reduce Errors in Program
Development. IBM Systems Journal 15, 3 (1976), 182‚Äì211.
1318ESEC/FSE ‚Äô22, November 14‚Äì18, 2022, Singapore, Singapore Lawrence Chen, Peter C. Rigby, and Nachiappan Nagappan
[6]Chandra Maddila, Sai Surya Upadrasta, Chetan Bansal, Nachiappan Nagappan,
Georgios Gousios, and Arie van Deursen. 2022. Nudge: Accelerating Overdue
Pull Requests Towards Completion. ACM Trans. Softw. Eng. Methodol. (2022).
https://doi.org/10.1145/3544791
[7]D.E. Perry, A. Porter, M.W. Wade, L.G. Votta, and J. Perpich. 2002. Reducing
inspection interval in large-scale software development. IEEE Transactions on
Software Engineering 28, 7 (2002), 695‚Äì705. https://doi.org/10.1109/TSE.2002.
1019483
[8]Adam Porter, Harvey Siy, Audris Mockus, and Lawrence Votta. 1998. Under-
standing the sources of variation in software inspections. ACM Transactions on
Software Engineering and Methodology (TOSEM) 7, 1 (1998), 41‚Äì79.
[9]Peter Rigby, Brendan Cleary, Frederic Painchaud, Margaret-Anne Storey, and
Daniel German. 2012. Contemporary peer review in action: Lessons from open
source development. IEEE software 29, 6 (2012), 56‚Äì61.
[10] Peter Rigby, Daniel German, and Margaret-Anne Storey. 2008. Open source
software peer review practices. In 2008 ACM/IEEE 30th International Conference
on Software Engineering . 541‚Äì550. https://doi.org/10.1145/1368088.1368162
[11] Peter C Rigby and Christian Bird. 2013. Convergent contemporary software peer
review practices. In Proceedings of the 2013 9th Joint Meeting on Foundations ofSoftware Engineering . ACM, 202‚Äì212.
[12] Peter C Rigby, Daniel M German, Laura Cowen, and Margaret-Anne Storey. 2014.
Peer review on open-source software projects: Parameters, statistical models,
and theory. ACM Transactions on Software Engineering and Methodology (TOSEM)
23, 4 (2014), 35.
[13] Peter C Rigby and Margaret-Anne Storey. 2011. Understanding broadcast based
peer review on open source software projects. In 2011 33rd International Confer-
ence on Software Engineering (ICSE) . IEEE, 541‚Äì550.
[14] Caitlin Sadowski, Emma S√∂derberg, Luke Church, Michal Sipko, and Alberto
Bacchelli. 2018. Modern code review: a case study at google. In Proceedings of
the 40th International Conference on Software Engineering: Software Engineering
in Practice . ACM, 181‚Äì190.
[15] Qianhua Shan, David Sukhdeo, Qianying Huang, Seth Rogers, Lawrence Chen,
Elise Paradis, Peter C. Rigby, and Nachiappan Nagappan. 2022. Using Nudges
to Accelerate Code Reviews at Scale. In Proceedings of the ACM SIGSOFT 30th
International Symposium on the Foundations of Software Engineering . 11 pages.
https://doi.org/10.1145/3540250.3549104
[16] Lawrence G. Votta. 1993. Does Every Inspection Need a Meeting? SIGSOFT Softw.
Eng. Notes 18, 5 (Dec. 1993), 107‚Äì114. https://doi.org/10.1145/167049.167070
1319