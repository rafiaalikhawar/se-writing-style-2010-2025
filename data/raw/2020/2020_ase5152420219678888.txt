SMARTIAN : Enhancing Smart Contract Fuzzing
with Static and Dynamic Data-Flow Analyses
Jaeseung Choi∗
KAIST
jschoi17@kaist.ac.kr
Gustavo Grieco
Trail of Bits
gustavo.grieco@trailofbits.comDoyeon Kim∗†
LINE Plus Corporation
doyeon1017@linecorp.com
Alex Groce
Northern Arizona University
alex.groce@nau.eduSoomin Kim
KAIST
soomink@kaist.ac.kr
Sang Kil Cha
KAIST
sangkilc@kaist.ac.kr
Abstract —Unlike traditional software, smart contracts have the
unique organization in which a sequence of transactions shares
persistent states. Unfortunately, such a characteristic makesit difﬁcult for existing fuzzers to ﬁnd out critical transactionsequences. To tackle this challenge, we employ both static anddynamic analyses for fuzzing smart contracts. First, we staticallyanalyze smart contract bytecodes to predict which transactionsequences will lead to effective testing, and ﬁgure out if there is acertain constraint that each transaction should satisfy. Such infor-mation is then passed to the fuzzing phase and used to constructan initial seed corpus. During a fuzzing campaign, we perform alightweight dynamic data-ﬂow analysis to collect data-ﬂow-basedfeedback to effectively guide fuzzing. We implement our ideas ona practical open-source fuzzer, named S
MARTIAN .S MARTIAN
can discover bugs in real-world smart contracts without theneed for the source code. Our experimental results show that
S
MARTIAN is more effective than existing state-of-the-art tools
in ﬁnding known CVEs from real-world contracts. S MARTIAN
also outperforms other tools in terms of code coverage.
I. I NTRODUCTION
Bugs in smart contracts can cause catastrophic failures
because smart contracts often handle digital assets worth
millions of dollars. In the notorious DAO attack in 2016 [23],for example, the attacker exploited a reentrancy bug in a smartcontract to steal 3.6 million ether, which was worth 70 millionUSD at that time.
Understandably, there has been surging research interest in
automatically ﬁnding bugs in smart contracts [24], [32], butto our knowledge, all the existing tools we found suffer fromone or more of the following issues.
First, the tools neglect to emit test cases needed for coverage
measurement. Many tools do not produce replayable test
cases, and output incomplete information about transactions.Moreover, some tools focus only on bug-triggering test casesand ignore test cases that increase coverage. This makes ithard to quantitatively compare the coverage achievement oftesting tools (see §II-C). Second, research papers in the ﬁelddo not always provide their implementation or publish datasets used in the evaluation. This problem has been noted in
*Co-ﬁrst authors.
†This work was done when the author was at KAIST.another recent study [25]. Third, many of the tools focus onlyon a small set of bug classes, which signiﬁcantly limits theirusability. For instance, Echidna [31] can only detect assertionfailures and check custom properties.
All these observations suggest a need for a practical testing
tool that is (1) able to produce replayable test cases, (2)publicly available, and (3) able to ﬁnd a set of various bugclasses. Although fuzzing is a plausible technique to achievethese requirements, none of the current smart contract fuzzerssatisﬁes them all.
Nevertheless, those are not the only requirements; there is
a critical technical challenge in current fuzzers in handlingstateful transactions. Smart contracts differ from traditionalapplications in that they take in a sequence of transactions asinput while maintaining a persistent state. The main challenge
in smart contract testing is to ﬁnd a transaction sequence thatcan change the persistent state of the target contract in acritical way. Unfortunately, traditional code coverage feedbackmay not be effective enough for identifying such importanttransaction sequences. That is, two transaction sequences mayachieve exactly the same branch coverage, although only oneof them can change the persistent state in a meaningful way.
Previous fuzzers partly handle this problem either by ran-
domly varying transaction orders [31], [53], [70] or by resort-ing to machine learning [35]. However, none of the approachesis deterministic, and thus, all of them are prone to potentialfailure in detecting crucial transaction sequences.
In this paper, we address this challenge by leveraging both
static and dynamic analyses on EVM bytecode. The keyintuition is that the signiﬁcance of transaction sequences canbe determined by the data dependencies between functionsand persistent state variables. Therefore, analyzing data ﬂowsof persistent state variables can help in identifying criticaltransaction sequences.
In particular, we use both static and dynamic analyses for
(1) generating an initial seed pool; and (2) evolving the seedpool at runtime. First, we statically analyze the target smartcontracts (in the form of raw EVM bytecode) to ﬁgure outmeaningful transaction orders, which can effectively modifythe persistent states, as well as their sender constraints. Each
2272021 36th IEEE/ACM International Conference on Automated Software Engineering (ASE)
DOI 10.1109/ASE51524.2021.000302021 36th IEEE/ACM International Conference on Automated Software Engineering (ASE) | 978-1-6654-0337-5/21/$31.00 ©2021 IEEE | DOI: 10.1109/ASE51524.2021.9678888
978-1-6654-0337-5/21/$31.00  ©2021  IEEE
transaction sequence obtained in this step is deemed to be a
useful seed for fuzzing. Note this is a preprocessing step thatruns only once per smart contract.
Next, we run fuzzing with the initial seeds obtained by the
preprocessing step. However, we note that our fuzzer alsoneeds to be able to discern useful transaction sequences atruntime to effectively update the seed pool. Thus, we introducedata-ﬂow-based feedback, a novel feedback mechanism thatcarefully monitors dynamic data ﬂows between state variablesduring a fuzzing campaign.
We design and implement S
MARTIAN , an open-sourced
smart contract fuzzer that can systematically generate criticaltransaction sequences for the smart contract under test withboth static and dynamic analyses. We evaluated S
MARTIAN
on a benchmark including 500 real-world Ethereum smartcontracts we collected based on their popularity and size.
S
MARTIAN outperformed existing tools in terms of both code
coverage and bug-ﬁnding ability. Furthermore, we found 211bugs in real-world smart contracts. All these results suggestthat our analyses enable S
MARTIAN to ﬁnd bugs in smart
contracts effectively.
In summary, we make the following contributions.
1) We propose a novel static analysis technique for gener-
ating initial seed pools, which is complementary to any
existing smart contract fuzzers.
2) We present data-ﬂow-based feedback, a novel and sys-
tematic feedback mechanism for fuzzing smart contracts.
3) We present S MARTIAN , a grey-box fuzzer for smart
contracts, which is (1) able to generate replayable testcases; (2) open-sourced; (3) able to detect a superset ofbug classes handled by existing fuzzers; and (4) ableto systematically generate critical transaction sequenceswith the help of both static and dynamic analyses.
4) We make our benchmark public, which includes 500
non-trivial, real-world smart contracts.
II. B
ACKGROUND
In this section, we ﬁrst introduce basic terms to understand
the rest of the paper. Next, we summarize classes of well-known smart contract bugs. Finally, we present a comparativestudy on existing bug-ﬁnding tools for smart contracts.
A. Basic Terminologies
Ethereum [26] is the most popular blockchain-based dis-
tributed computing platform. A smart contract is essentially a
collection of code and data that is located on the Ethereumblockchain. Ethereum Virtual Machine (EVM) is an executionenvironment for running smart contracts. Generally, contractcode is ﬁrst written in a high-level language like Solidity [10],but eventually, it must be compiled into bytecode to run onEVM. A smart contract maintains storage, which is essentially
a key-value store for holding persistent state variables . Storage
is different than memory orstack as its contents are non-
volatile. To execute a function deﬁned in the contract, a userneeds to make a transaction to the contract. A transaction
contains information about a function call, such as parameterT ABLE I
BUG CLASSES SUPPORTED BY SMARTIAN .
ID Bug Name Description
AF Assertion
FailureThe condition of an assert statement is not satis-
ﬁed [2].
AW ArbitraryWriteAn attacker can overwrite arbitrary storage data byaccessing a mismanaged array object [12].
BD Block StateDependencyBlock states (e.g. timestamp, number) decide ethertransfer of a contract [36], [44].
CH Control-ﬂowHijackAn attacker can arbitrarily control the destination ofaJUMP orDELEGATECALL instruction [1], [36].
EL Ether Leak A contract allows an arbitrary user to freely retrieve
ether from the contract [54].
FE FreezingEther
†A contract can receive ether but does not have anymeans to send out ether [36], [54].
IB Integer Bug Integer overﬂows or underﬂows occur, and the result
becomes an unexpected value.
ME MishandledExceptionA contract does not check for an exception whencalling external functions or sending ether [36], [44].
MS MultipleSendA contract sends out ether multiple times within onetransaction. This is a speciﬁc case of DoS [5].
RE Reentrancy A function in a victim contract is re-entered and leads
to a race condition on state variables [44].
RV RequirementViolation
‡The condition of a require statement is not satis-
ﬁed [8].
SC SuicidalContractAn arbitrary user can destroy a victim contract byrunning a SELFDESTRUCT instruction [54].
TO TranasactionOrigin UseA contract relies on the origin of a transaction (i.e.tx.origin) for user authorization [3].
†While other bugs deal with safety properties, FE concerns a liveness property. As
it is unnatural to ﬁnd the absence of behavior with testing, we make this oracle
optional, and provide a command-line option to enable it.
‡Since the ofﬁcial document [11] recommends to use require for validating
program inputs, it is debatable whether this is a bug. Thus, we make this optional.
values. Both a contract and a user are assigned a unique
address, and can have a certain amount of digital cash calledether. A transaction is also used to transfer ether betweencontracts and users.
A deployer is a special user who initially publishes a smart
contract on the blockchain network. Typically, the address ofthe deployer is saved in the storage during the initialization
phase (see the example in Figure 1). The stored address canthen be used to discern between the deployer and regular users.Although it is desirable for testing tools to be able to sendtransactions from both deployers and normal users, we arenot aware of any existing fuzzer that can systematically select
proper users during a fuzzing campaign.
B. Smart Contract Bug Classes
Previous research deﬁnes their own bug classes with differ-
ent terminologies, and there is no general consensus among
them. Thus, we study and summarize them in Table I. S
MAR -
TIAN supports the detection of all these bugs (see §IV -C3).
First, we investigated bug classes handled by existing state-
of-the-art fuzzers [31], [35], [36], [43], [53], [66], [70]. Weincluded all the bugs from these fuzzers. In addition, weexamined more previous work on smart contracts [4], [9],[44], [52], [54] and selected bugs that can be detected withoutexcessive false positives.
As a result, we identiﬁed 13 types of bugs listed in Table I.
While some papers make ﬁner classiﬁcations of bugs, we tried
228T ABLE II
COMP ARISON OF EXISTING BUG -FINDING TOOLS FOR SMART CONTRACTS .
Tool KindReplayable Public Available Byte Bug Oracle
Test CaseaTool Benchmark Code AF A W BD CH EL FE IB ME MS RE RV SC TO
MadMax [30] Static analyzer         
Remix [57] Static analyzer           
SASC [73] Static analyzer         
Securify [64], [65] Static analyzer           
Slither [27] Static analyzer          
SmartCheck [62] Static analyzer             
V andal [15] Static analyzer            
V eriSmart [60] Static analyzer             
Zeus [38] Static analyzer           
Maian [54] Symbolic executor          
Manticore [50] Symbolic executor           
Mythril [52] Symbolic executor /triangleb           
Osiris [63] Symbolic executor           
Oyente [44] Symbolic executor           
sCompile [16] Symbolic executor ?c         
teEther [39] Symbolic executor /triangle            
ContractFuzzer [36] Fuzzer            
ContraMaster [66], [67] Fuzzer            
Echidna [31] Fuzzer      
Harvey [70], [71] Fuzzer ?       
ILF [35] Fuzzer            
Reguard [43] Fuzzer ?      
sFuzz [53] Fuzzer d          
⋆SMARTIAN Fuzzer      
aDoes the tool generate test cases that contain complete information needed to reproduce the transactions, enabling the measurement of coverage achievement?
bPrints to the terminal only about test cases that trigger bugs, and ignore test cases that increase coverage.cWe cannot identify the fact as the tool is not publicly available.
dOlder version of sFuzz could generate replayable test cases, but this functionality disappeared in the latest version.
to merge closely related bug classes into one. For instance,
ContractFuzzer [36] distinguishes Timestamp Dependency and
Block Number Dependency, but we merge them into Block
State Dependency. Also, we consider Gasless Send [36] as a
speciﬁc case of Mishandled Exception.
We note that existing tools implement different bug oracles.
Thus, in our evaluation, we carefully consider this differenceto make fair comparisons (see §V -C).
C. Existing Tools
We studied 23 different tools for ﬁnding smart contract bugs,
and summarized them in Table II.
The third column indicates whether a tool can generate
test cases that contain complete information to reproduce the
transactions. While it is natural for static analyzers to notgenerate test cases, we found that surprisingly many testingtools do not generate replayable test cases. Some tools, such asOyente, simply emit terse information about inputs and do notoutput complete data needed to reproduce transactions. Thismakes it infeasible to reproduce bugs found or to measurecoverage achievements. As a result, one can evaluate thetools only by looking at their textual reports, which are oftenprone to errors. For example, [53] states that Oyente reportsinfeasible paths as feasible.
The fourth and ﬁfth columns respectively represent whether
a tool and a benchmark are publicly available. Although it iscrucial to publicize benchmarks for reproducing research [25],only a few of those tools make their benchmarks public.
The sixth column shows whether a tool runs at a bytecode
level. As Ethereum deploys smart contracts in the form ofbytecode, this enables testing smart contracts in the blockchaineven if the source code is not available.
Finally, the rest of the columns present bug oracles em-
ployed by each tool. While all the other tools focus on aspeciﬁc set of bug oracles, S
MARTIAN handles everything as
shown in the table.1contract C{
2 // State variables in the storage.
3 address owner =0;
4 uint private stateA =0;
5 uint private stateB =0;
6 uint CONST =32;
7
8 function C() { // Constructor
9 owner =msg.sender;
10 }
11 function f(uint x) {
12 if(msg.sender ==owner) { stateA =x; }
13 }
14 function g(uint y) {
15 if(stateA %CONST ==1){
16 stateB =y-10;
17 }
18 }
19 function h() {
20 if(stateB ==62) { bug(); }
21 }
22 }
Fig. 1. Example smart contract.
III. O VERVIEW
In this section, we ﬁrst present a motivating example to
describe a unique challenge in smart contract fuzzing. We then
brieﬂy describe how S MARTIAN addresses this challenge by
employing both static and dynamic analyses.
A. Motivating Example
Smart contracts impose a unique challenge to fuzzing due
to their intrinsic structure where multiple transactions are
interconnected to each other with persistent state variables.
Consider our motivating example in Figure 1, which has
the constructor C, along with the three functions f,g, and h.
While our system operates on EVM bytecode, the examplecode is written in Solidity for ease of explanation.
The constructor C, which simply stores the address of
the deployer in the storage, runs once when the deployerinstantiates the contract. This is indeed a commonly found
229pattern as it provides means to distinguish the deployer from
regular users. Note that msg.sender is an expression that
evaluates to the current sender’s address at runtime.
The contract has a bug in h, which can be triggered only if
stateB is 62. For example, one can trigger the bug with a
transaction sequence [f(33),g(72),h()]. Note that the
three transactions should be made in the exact order to triggerthe bug. Moreover, fshould be sent by the deployer.
At a ﬁrst glance, the conditions in each function may
not seem so hard to satisfy. For example, one can penetratethe condition in Line 15 with the probability of 1/32 by
randomly mutating stateA. The condition in Line 20 is
relatively harder to solve, but recent advances in grey-boxfuzzing provide practical solutions to it [13], [18], [20], [53].In our implementation, we adopt grey-box concolic testingtechnique from Eclipser [20].
However, ﬁnding this bug is still challenging as we need
to generate the transactions in the correct order. For instance,let us assume that we have a sequence [f(*),h(),g( *)]
as a seed
1, where *can be any value. Any mutation attempt
on the function arguments will not trigger the bug becausehcannot observe any difference for stateB. Therefore,
our fuzzer needs to have a transaction sequence such as[f(*),g( *),h()] in the seed pool to ﬁnd the bug.
One may argue that grey-box fuzzers are able to dis-
cover such a critical transaction sequence by randomlymutating transaction orders. However, it is not as trivial
as it seems. Even if we manage to generate a sequence[f(*),g( *),h()] by randomly trying different transaction
orders, we cannot realize that this is indeed a meaningful testcase because traditional code coverage is not sensitive enough.
For instance, consider two transaction sequences S
A=
[f(33),g(0),h()] and SB=[f(33),h(),g(0)],
which achieve the same branch coverage. If SBwas already
in our test case pool, our fuzzer would have no chance to addS
Ato the seed pool, even though it is the critical one.
Preliminary experiments: Despite the simplicity of the
example contract in Figure 1, none of the existing fuzzersthat we tested was able to ﬁnd the bug. Speciﬁcally, we ranthree open-sourced smart contract fuzzers, Echidna, ILF, andsFuzz, for one hour each. On the other hand, S
MARTIAN was
able to ﬁnd the bug within just a few seconds.
B. Our Approach
To address the aforementioned challenges, S MARTIAN
leverages both static and dynamic analyses. Figure 2 outlines
the overall architecture of S MARTIAN . At a high level, our
system runs in three major steps: (1) I NFO GA THER , (2)
SEED POOL INIT , and (3) D ATA FLOW FUZZ .
1)INFO GAT H E R :First, S MARTIAN takes in the EVM
bytecode under test as input and runs a static analysis tocollect useful data-ﬂow facts to guide both S
EED POOL INIT
and D ATA FLOW FUZZ . Speciﬁcally, for each function in the
contract, S MARTIAN ﬁgures out which state variables are
1In this paper, we interchangeably use the terms “transaction sequence”,
“test case”, and “seed”.EVM Bytecode
010110110011110101001111000010101Static Analysis
Dynamic AnalysisINFOGA THER
(§ IV-A)SEEDPOOL INIT
(§ IV-B)
DATAFLOW FUZZ
(§ IV-C)
BugsFacts
Sender info.Seeds
Data-ﬂo w-based feedback
Fig. 2. S MARTIAN architecture.
deﬁned and used by the function, and whether the functioncompares the transaction sender address against the deployeraddress. From our example contract, our static analysis willgather the following facts.
•stateA is deﬁned by fand used by g.
•stateB is deﬁned by gand used by h.
•owner is deﬁned by Cto be the deployer’s address.
•owner is used by fto check the transaction sender.
Since our analysis directly runs on the low-level EVM byte-code, gathering such information is not trivial (see §IV -A).
2)S
EED POOL INIT:Based on the gathered information,
SMARTIAN predicts which transaction sequences are likely to
lead to meaningful exploration, and consider them as initialseeds. Speciﬁcally, S
MARTIAN realizes that fmust precede
gto explore execution paths affected by stateA. Similarly,
it infers that gmust be called before hto change the value
ofstateB and explore more paths in h. Finally, it ﬁgures
out that fmust be executed by the deployer in order to pass
the sender check we identiﬁed in the ﬁrst step. Consequently,
SMARTIAN creates S0=[f(0),g(0),h()] as the initial
seed for fuzzing, while making sure that the sender of fis the
deployer. The transaction argument is set to 0 by default. See§IV -B for the detailed algorithm for seed initialization.
3)D
ATA FLOW FUZZ :While initializing the seed pool with
meaningful transaction sequences can increase the probabilityto ﬁnd the bug, this does not immediately solve the wholechallenge. Ideally, we will ﬁrst randomly mutate the givenseed S
0and obtain a new seed S1=[f(33),g(0),h()],
which can reach Line 16. If we can identify S1as a critical
test case, we will add it to the test case pool and later applythe aforementioned grey-box concolic testing to ﬁgure out theproper argument value of gthat triggers the bug (see §IV -C).
Unfortunately, as we emphasized in §III-A, we may fail to
discern such critical intermediate seeds if we merely employexisting code coverage feedback. Assume that we accidentallygenerated S
2=[f(33),g(0)] prior to S1, and added S2
to the test case pool. This is possible because our fuzzer canrandomly add, remove, or reorder transaction(s) from a givenseed. Once S
2is added to the test case pool, S1will no longer
be considered interesting because it provides no coverage gainover the existing seeds {S
0,S2}.
To mitigate this problem, we employ a dynamic data-ﬂow
analysis to collect data-ﬂow-based feedback. At a high level,
230our approach considers the data-ﬂow coverage as fuzzing
feedback along with the branch coverage used by Eclipser [20]That is, we adopt dynamic instrumentation to observe dataﬂows that occur in the given transaction sequence at runtime,and use them as feedback too. With S
1, there is a data ﬂow
from the deﬁnition of stateB in Line 16 to the use of
stateB in Line 20. However, neither S0norS2have this
data ﬂow. Based on this, we can conclude that S1discovers
an interesting program behavior that is not observed in S0nor
S2. We detail our approach in §IV -C.
4) Impact of Data-ﬂow Analyses: With the help of both
static and dynamic analyses, S MARTIAN can ﬁnd the bug
from the above example within just ﬁve seconds on ourmachine. Meanwhile, when we disabled our analyses (see§V -B), S
MARTIAN failed to ﬁnd the bug in one hour.
C. Our Contribution over Previous Work
Our technical contribution is twofold: (1) we are the ﬁrst
in systematically generating seeds for smart contract fuzzing;and (2) we use data-ﬂow-based coverage to effectively guidesmart contract fuzzing.
1) Seed Generation: Previous fuzzers suffer from system-
atically generating proper transaction sequences. For instance,Echidna and sFuzz generate sequences at random.
Harvey [70] partly addresses this challenge with runtime
heuristics. First, Harvey forcefully mutates state variables toﬁnd which functions are affected by them. It then randomlyprepends other transactions to those functions. However, thismethod is not scalable to complex contracts with a largenumber of functions. Moreover, Harvey may fail to distinguishconstants, e.g., CONST in our motivating example, from vari-
ables, and spend its fuzzing budget to change such constants.
ILF [35] generates transaction sequences based on a
machine-learning model obtained by symbolically executingsmart contracts. While ILF can potentially ﬁnd meaningfultransaction sequences via learned models, the result is notdeterministic as it is based on statistical reasoning. Moreover,our approach is complementary to ILF as we directly analyzethe semantics of the contract to generate seeds.
2) Data-ﬂow-based feedback: The use of data-ﬂow graph
coverage has been previously studied in data-ﬂow testing [61].However, none of the existing fuzzers except ContraMas-ter [67] had employed data-ﬂow coverage as fuzzing feedback.While data-ﬂow-based feedback shares the same key intuitionwith ContraMaster, ContraMaster uses data-ﬂow coverage todecide whether to perform mutation on transaction orders.Meanwhile, we use the feedback to evaluate the generatedseeds and decide whether to put them in the seed pool.
IV . D
ESIGN
This section presents the design details of S MARTIAN . Re-
call from §III, S MARTIAN operates in three major steps: I NFO -
GAT H E R (§IV -A), S EED POOL INIT (§IV -B), and D ATA FLOW -
FUZZ (§IV -C).A. Information Gathering ( INFO GA THER )
INFO GA THER analyzes the given EVM bytecode and
returns a 4-tuple /angbracketleftFuncs, Defs, Uses, SenderChecks/angbracketright
where:
•Funcs is a set of identiﬁed functions.
•Defs is a map from each identiﬁed function to the state
variables deﬁned by the function.
•Uses is a map from each identiﬁed function to the state
variables used by the function.
•SenderChecks is a set of functions that includes a
sender-checking routine.
It starts by constructing a Control-Flow Graph (CFG) from
the given EVM bytecode. It internally disassembles EVMinstructions, and lifts them into an Intermediate Representation(IR). It then runs a constant propagation analysis on the IR toﬁgure out the destinations of control-ﬂow transfer instructions,e.g.,JUMP, and identiﬁes functions including constructors. We
note that this step also includes resolving call edges within thecontract, to enable an inter-procedural analysis. Finally, it runsthe main analysis based on abstract interpretation [22].
Our main static analysis computes abstract values stored
in the stack and the memory in a ﬂow- and context-sensitivemanner [49]. Tracking stack values is important because EVMis a stack-based machine that pushes instruction operands tothe stack [26], [69]. Following memory values is also criticalbecause these operands are often loaded from the memory, too.Our ultimate goal here is to ﬁgure out which state variablesare deﬁned and used by each function. To distinguish whichstate variable is used (or deﬁned), we check which value isused as a key for SLOAD (orSSTORE) instruction.
To approximate values, we use a product domain [58] that
entails three different domains. First, we employ the liftedinteger domain [58] to trace constants. This is because smartcontracts use a hard-coded unique constant as a key to accessstate variables of primitive data types, e.g., uint. Second,
we use a variation of the lifted integer domain to abstractthe output of hash instruction SHA3. This is because smart
contracts access state variables of compound data types, e.g.,mapping, through a computed hash value as a key. With bothdomains, we can track which speciﬁc state variable is accessedfor every program point, and thus can update both Defs and
Uses accordingly.
The last component of the product domain is the taint
domain for tracking the ﬂow of the deployer’s address (recallfrom §III-B). With this domain, we compute SenderChecks
by analyzing the following two conditions. First, we check ifthe constructor of a smart contract saves the deployer’s addressinto the storage. Second, we see if a sender’s address, whichis returned by a CALLER instruction, ﬂows into a conditional
branch, and gets compared with the deployer’s address. Noteboth ﬂows can be easily tracked with traditional static taintanalysis. If both conditions hold, then we put the functioncontaining the conditional into SenderChecks.
231Algorithm 1: Deriving Function Call Orders.
1function GenSequences(Funcs, Defs, Uses)
2 seqs←∅
3 works←InitWorks({[f ]|f∈Funcs, Defs(f )/negationslash=∅})
4 while works/negationslash=∅do
5 s←works.pop()
6 nogain ←true
7 for finFuncs do
8 ifDataFlowGain(s ||[f],Defs, Uses) then
9 works.push(s ||[f])
10 nogain ←false
11 ifnogain then
12 seqs←seqs∪{s}
13 return seqs
B. Seed Pool Initialization ( SEED POOL INIT)
To generate initial seeds, S MARTIAN ﬁrst derives useful
function call orders, based on the information gathered from
INFO GAT H E R , and then generates concrete transaction se-
quences based on the orders.
1) Deriving Useful Call Orders: Algorithm 1 illustrates the
decision of function call orders. It takes in as input Funcs,
Defs, and Uses obtained from I NFO GAT H E R , and outputs a
set of function sequences.
In Line 3, we initialize the worklist (works) with singleton
sequences containing each function in Funcs. We ignore
functions that do not deﬁne any state variable as they cannot
affect the persistent state. Next, we pull a sequence sout of
the worklist (Line 5), and creates new sequences by appendingeach function in Funcs tos. We then examine each of the
generated sequences with DataFlowGain to decide which
sequence covers previously unseen data ﬂows (Line 7–8).Line 9 pushes such sequences to the worklist, and internallyremoves redundant entries for greater efﬁciency.
Speciﬁcally, DataFlowGain statically approximates
function-level data ﬂows by collecting triples /angbracketleftf
1,v ,f 2/angbracketrightfrom
a given sequence, where (1) f1and f2are functions that
appear in the sequence, (2) f1deﬁnes v, and (3) f2uses that
v. It returns true if a previously unseen triple is found from
the sequence.
We repeatedly extend sequences in the worklist as long as
a new data ﬂow is observed. If a sequence produces no gainby extending it, we ﬁnalize the sequence by adding it to theoutput set (Line 11–12).
2) Generating Seeds: We now turn the generated function
sequences into transaction sequences by concretizing theircontents, which works mainly in two steps.
First, for every function in each transaction, we decide
whether each function belongs to SenderChecks. If so, we
set the sender of the transaction as the deployer. Otherwise,we randomly choose the sender (either deployer or a user).
Second, we need to initialize the function arguments of
each transaction. Here, we consider the amount of ether totransfer as an additional argument, too. S
MARTIAN internally
represents each argument as a byte stream. When the targetcontract ships with its ABI speciﬁcation, we leverage it toset the argument types as well as the length of the bytestreams accordingly. When the ABI speciﬁcation does notexist, S
MARTIAN will simply set the length of each byte
stream to a predeﬁned maximum value. Correctly inferringdata types of function arguments in the absence of ABI isbeyond the scope of this paper, and we leave it as future work.
C. Data-Flow-Based Fuzzing ( D
ATA FLOW FUZZ )
With the generated initial seed pool, S MARTIAN iteratively
selects one and mutates it to generate new test cases (§IV -C1).
SMARTIAN then evaluates the usefulness of the newly gen-
erated test cases by running the smart contract under test
with each test case (§IV -C2). During each execution, our bugoracles check whether it is buggy (§IV -C3).
1) Mutation Methodologies: S
MARTIAN employs two com-
plementary strategies for mutating seeds. One is randommutation, and the other is grey-box concolic testing from [20].
S
MARTIAN alternates between them to achieve synergy.
First, our random mutation strategy runs at both the se-
quence level and the transaction level. Sequence-level mutationconsists of the following operations: (1) inserting a newtransaction for a random function; (2) removing a randomtransaction; and (3) swapping two random transactions. Wheninserting a transaction, we refer to SenderChecks gathered
from the static analysis and use it to decide the sender, as in§IV -B. Transaction-level mutation mainly modiﬁes argumentsof each transaction. We leverage classic mutation operatorswidely used in grey-box fuzzers, such as bit-ﬂipping muta-tion [72]. Besides, we randomly mutate the sender of thetransaction, too.
However, it is well known that random mutation can eas-
ily get stuck on conditional branches such as magic valuechecks [13], [18], [41]. Eclipser [20] addresses this challengeby introducing the grey-box concolic testing technique, which
operates similarly to traditional concolic testing [29], [59], butwithout SMT solving [51] or expensive extra instrumentation.
2) Data-ﬂow-based Feedback: Recall from §III-A, previous
code coverage feedback is not enough to discern interest-ing seeds during a fuzzing campaign. To overcome this,
S
MARTIAN introduces data-ﬂow-based feedback in addition
to the traditional code coverage feedback. That is, S MARTIAN
considers a seed as interesting when it exhibits a previouslyunseen data ﬂow or covers previously unvisited code.
To collect data ﬂows, we dynamically instrument the EVM
bytecode by modifying an EVM emulator in order to monitorthe storage accesses during the execution. Particularly, we
capture a dynamic data ﬂow with a def-use chain. Let p
1v− →p2
be a def-use chain over a state variable vdeﬁned in a program
point p1, and used in a program point p2. We can then
represent def-use chains from the example in Figure 1 as
follows. SAyields def-use chains 12stateA− − → 15and16stateB− − → 20,
while SBonly yields 12stateA− − → 15. Therefore, S MARTIAN can
recognize that SAexhibits an interesting program behavior
not presented by SB. In the actual implementation, we use an
232instruction address as a program point pi, and use the key of
the storage as a state variable v.
Recall that we also check for data ﬂows during the seed pool
initialization in §IV -B. Note that in Algorithm 1 we statically
approximated the data ﬂows at the function level. Meanwhile,in fuzzing, we trace data ﬂows that actually take place, andcalculate them at instruction-level granularity. That is, we ﬁrststatically analyze data ﬂows to decide promising transactionsequences that are likely to reveal more dynamic data ﬂowsduring the fuzzing. Then, we employ concrete and more ﬁne-grained data ﬂows as feedback at the fuzzing phase.
3) Bug Oracles: Here, we summarize our bug oracle im-
plementation for 13 classes of bugs supported by S
MARTIAN .
Again, we modify the EVM emulator to implement these bugoracles. Thus, our runtime instrumentation is responsible forboth collecting data-ﬂow-based feedback and detecting bugsduring the execution.
AF At the bytecode level, an assertion failure correspondsto the execution of an INVALID instruction. Therefore,
we can precisely detect AF by checking if an INVALID
instruction is executed. We note that compilers alsoautomatically insert assert statements to prevent errors
such as division by zero. We consider the failures fromthese compiler-inserted assertions as AF, too.
BD We leverage dynamic taint analysis to check if a blockstate can affect an ether transfer. We trace both directand indirect taint ﬂows for this. We ﬁrst taint the returnsof instructions that acquire the state of a block (e.g.TIMESTAMP, NUMBER). Then, we monitor if the tainted
value ﬂows into the operands of a CALL orJUMPI.
CH First, we raise an alarm if a normal user can set thedestination contract of a DELEGATECALL into an arbi-
trary user contract. Second, we also report an alarm ifthe destination of a JUMP instruction is manipulatable.
EL We employ an oracle similar to that of Mythril [52],which checks if a normal user can gain ether by sendingtransactions to the contract. However, this is prone to falsepositives, because some contracts allow the deployer tohand over the ownership of the contract to another user.In such contracts, it is an intended behavior that a usercan withdraw the contract’s balance when the deployerallows to. To avoid such false positives, we report alarmsonly when the transaction sequence does not have anypreceding transaction from the deployer.
IB We monitor ADD, SUB, MUL instructions to check if
they cause an integer over/underﬂow. If so, we taint theresulting value, and perform a dynamic taint analysis tocheck whether the tainted value is used to determine theamount of ether to transfer, or is used to update the statevariables. This is to avoid raising alarms on benign integerover/underﬂows. For example, without this taint analysis,we will raise an alarm on a safe code snippet ‘if(x +y < x) revert();’.
ME We run a taint analysis to make sure that the return valueof aCALL instruction ﬂows into a predicate of a JUMPI
instruction. If there is a return value that is not used byaJUMPI, we report an alarm.
MS We detect multiple ether transfers taking place in a singletransaction.
RE We ﬁrst monitor if there is a cyclic call chain duringan ether transfer, as ContractFuzzer [36] or sFuzz [53]does. In addition, we use taint analysis to identify statevariables that affect this ether transfer, similarly to as wedid for BD. Then, we report RE if such variables are
updated after the transfer takes place.
RV We check for the execution of a REVERT instruction,
which corresponds to a requirement violation.
SC We check if a normal user can execute SELFDESTRUCT
instruction and destroy the contract. Similarly to EL,
we reduce false positives by ﬁltering out test cases thathave any preceding transaction from the deployer in thesequence.
TO We taint the return value of ORIGIN instruction, and
check if it ﬂows into the predicate of a JUMPI instruction.
For the rest of the bug classes, we implemented the same
bug oracle as ContractFuzzer [36] (FE) and Harvey [70] (AW ).
D. Implementation
To implement our static analyzer, we used B2R2 [37] as
a front-end to parse and disassemble EVM bytecode. Themain logic of static analysis (§IV -A) is written in 1,053
source lines of F# code. The fuzzing component of S
MAR -
TIAN (§IV -C1) is implemented by extending Eclipser [19] to
operate with EVM and transaction sequences, and is composedof 3,112 source lines of F# code. We used NethermindEVM [7] for deploying contracts and emulating transactionswith dynamic instrumentation. Speciﬁcally, we added 979lines of C# code to Nethermind to implement data-ﬂow-based feedback (§IV -C2) as well as our bug oracles (§IV -C3).We make all our source code and benchmarks public at:https://github.com/SoftSec-KAIST/Smartian.
V. E
V ALUA TION
In this section, we answer the following research questions.
RQ1. Can our analyses improve the fuzzing effectiveness
of S MARTIAN ? (§V -B)
RQ2. Can S MARTIAN ﬁnd known bugs more effectively
compared to existing state-of-the-art tools? (§V -C)
RQ3. How does S MARTIAN perform on a large-scale
benchmark? (§V -D)
A. Experimental Setup
1) Our Environment: We ran all our experiments on an
Ubuntu 18.04 server machine equipped with two Intel E5-2699
v4 (2.2 GHz) CPUs and 512 GB of main memory. We usedDocker 20.10.3 for our experiments, and used one containerto run a tool on a single contract. We spawned at most 72containers in parallel, and assigned a single CPU core and 6GB of memory to each container. To compile contracts, weusedsolc-0.4.25.
233T ABLE III
BENCHMARKS USED .
ID Source Used ForAvg. SD†Num. of
SLoC SLoC Contracts
B1 V erismart [60] RQ1, RQ2 136 48 58
B2 SmartBug [25] RQ2 51 75 72
B3 Etherscan RQ3 331 277 500
†Standard Deviation.
2) Comparison T argets: We selected two fuzzers and
two symbolic executors as our comparison targets. To select
fuzzers, we ﬁrst ﬁltered open-sourced fuzzers that are pub-lished in top conferences, and obtained ContractFuzzer, ILF,and sFuzz. We chose ILF and sFuzz over ContractFuzzer,since the two tools respectively outperformed ContractFuzzerin their experiments [35], [53]. To select symbolic executors,we initially applied the same criteria and obtained Oyenteand teEther as a result. However, we found that Oyentereports unfeasible paths as executable, according to [53]. Also,teEther supports only a small set of bug classes, making thecomparison against sFuzz difﬁcult. Thus, we chose Mythriland Manticore instead, as they support various bug classes.
For each of the selected tools, we added functionality to
emit replayable test cases if the tool does not already haveit. Also, we modiﬁed their code to save all the test cases thatincrease code coverage. We publicize the modiﬁed versions ofthe tools in GitHub.
3) Benchmarks: We used three distinct benchmarks for
our experiments. Table III summarizes them. We make thesebenchmarks public as well, in order to support open science.
B1. First, we used the benchmark from V eriSmart [60], which
consists of 58 real-world contracts. Each of the contracts
is assigned a CVE for an integer bug (IB). Note that theauthors of V eriSmart originally collected 60 contracts, butthey conﬁrmed that two of the CVEs were not real bugs.
B2. While B1 is a realistic benchmark with known vul-
nerabilities, all the assigned CVEs are for IB. Thus,
we constructed B2 that contains more bug classes, by
extracting contracts from SmartBug [25]. In particular,we imported contracts that have block state dependency(BD), mishandled exception (ME), or reentrancy (RE), asthese bug classes are supported by all of our comparisontargets (§V -A2). Then, we performed preprocessing suchas ﬁltering out contracts that have any argument in theconstructor. This is because some of the tools we selectedfor comparison did not run properly on such contracts. Asa result, we obtained a total of 72 contracts that contain13BD,5 0 ME, and 19 RE. Note that a single contract
can have multiple classes of bugs here.
B3. This benchmark comprises 500 popular and complex
smart contracts obtained from Etherscan [6], which isan online platform that provides code and statistics ofEthereum smart contracts. We ﬁrst downloaded contractsthat have more than 30,000 transactions, and ﬁltered outcontracts that do not compile with the solc version01020304050
0 1 02 03 04 05 06 0
Time (min.)Total # of CVEs foundw/ static & dynamic analyses
w/ static analysis onlyw/ dynamic analysis onlyw/o any analysis
Fig. 3. Impact of our data-ﬂow analyses on B1.
we used. Then, we ﬁltered out contracts that have any
constructor argument, to make the benchmark usableby as many tools as possible. Finally, we sorted theremaining contracts based on their bytecode size, andselected the 500 largest contracts in order to gather bothpopular and complex contracts.
B. Impact of Our Analyses
Do our analyses help S
MARTIAN ﬁnd bugs more effec-
tively? How much overhead do they introduce? We answerthese questions by comparing the effectiveness of S
MARTIAN
onB1 with and without our analyses. We used B1 here because
each contract in the benchmark contains a previously knownbug, which serves as ground truth for our evaluation. Also, allthe contracts in B1 are real-world contracts, whereas some of
the contracts in B2 are artiﬁcially created toy programs.
To assess the impact of both static (§IV -A) and dynamic
(§IV -C) analyses, we ran S
MARTIAN in four different modes:
(1) with both of the analyses, (2) only with the static analysis,(3) only with the dynamic analysis, and (4) without anyanalysis. We ran with each mode for one hour on B1, and
repeated the experiment for ﬁve times.
1) Impact on Bug Finding: We ﬁrst measured the impact of
our analyses in terms of bug ﬁnding. We say our tool found theground truth bug in each contract if it can pinpoint the exactprogram point for the assigned CVE. This is important becauseeach contract may also have integer overﬂows or underﬂowsthat are irrelevant to the assigned CVE. Moreover, some ofthem can be benign, as we discussed in §IV -C3.
Figure 3 compares the number of unique bugs found over
time with the four different modes. We note that there is asigniﬁcant difference in ﬁnding deep bugs at the later stage offuzzing. On average, we were able to ﬁnd about 22% moreunique bugs with our analyses than without them (p-value <
0.05 from Mann-Whitney U-Test). This result conﬁrms that
there are real-world smart contract bugs, which can only befound by considering stateful transaction sequences, and ouranalyses indeed help in ﬁnding them.
2) Impact on Code Coverage: We also measured the
number of executed instructions with and without using ouranalyses. As a result, we found that turning on both static anddynamic analyses helped in covering 1% more instructions
234Smartian sFuzz Manticore Mythril
0102030
0 1 02 03 04 05 06 0
Time (min.)Total # of CVEs found
0K20K40K60K80K100K
0 1 02 03 04 05 06 0
Time (min)Instruction Coverage
Fig. 4. Comparison against state-of-the-art tools on the subset of B1.
on average. While the coverage gap is not signiﬁcant, recall
that there was a signiﬁcant difference in the number of bugsfound. This observation aligns with the key motivation of ourwork: while the traditional code coverage feedback can beeffective in guiding fuzzers to increase coverage, it may notbe enough to trigger bugs when a stateful transaction sequenceis required.
3) Overhead of Our Analyses: We also evaluated how
much overhead is imposed by applying our analyses. Forthe contracts in B1, the static analysis took less than two
seconds on average, and took ﬁve seconds in the worst case.This is indeed a negligible overhead for fuzzing. We alsomeasured the overhead of our dynamic data-ﬂow analysis, i.e.,data-ﬂow-based feedback. Speciﬁcally, we collected test casesgenerated from our previous experiments, and replayed themwith and without the data-ﬂow-based feedback computation.As a result, we observed that computing data-ﬂow-based feed-back incurred only 2.7% overhead in terms of execution time.Thus, we conclude that our analyses incur reasonably smalloverhead, and despite the overhead, they allow S
MARTIAN to
ﬁnd more bugs and to achieve more coverage.
Answer to RQ1. Both static and dynamic analyses can
effectively guide fuzzing to ﬁnd more bugs. When usedtogether, the analyses enable S
MARTIAN to ﬁnd 22% more
bugs in our benchmark.
C. Comparison against Existing Tools
Next, we compare S MARTIAN against state-of-the-art tools
that we selected in §V -A2. For this, we measured the bug-ﬁnding effectiveness as well as coverage achievement of eachtool. To measure coverage achievement, we replayed the testcases generated by each tool. Recall from §V -A2 that wemodiﬁed the tools to emit all the test cases that can increasecoverage, in a replayable format.
1) Comparison on B1: We ﬁrst compare S
MARTIAN with
other testing tools on B1. As we mentioned in §V -A, some of
our comparison targets did not properly operate on contractswith constructor arguments. Therefore, we excluded suchcontracts and obtained a subset of B1, which comprises 32
contracts. We ran the tools for one hour on each contract, andrepeated the experiment ﬁve times to compute the average. Tomeasure the bug-ﬁnding effectiveness, we counted the numberof ground truth bugs found by each tool. As in §V -B, wechecked whether the tool can report the exact program pointthat corresponds to the CVE assigned for an integer bug (IB).
Figure 4 shows the comparison result between S
MARTIAN
and other tools. The left-hand-side plot presents the numberof CVEs found over time, whereas the right-hand-side plotpresents the instruction coverage over time. Note that ILF isnot included in this comparison, because it does not supportthe detection of IB (see Table II).
As the ﬁgure indicates, S
MARTIAN constantly outperformed
other tools in terms of bug-ﬁnding effectiveness. S MARTIAN
found 5.8× , 4.8× , and 2.1× more ground truth bugs (CVEs)
than sFuzz, Manticore, and Mythril, respectively. This resultwas consistent over the ﬁve times of repeated experiments (p-value <0.01 from Mann-Whitney U-Test for all the tools).
Moreover, there was only one bug that S
MARTIAN missed but
one of the other tools could ﬁnd.
Also, S MARTIAN covered more instructions than other
tools throughout the whole fuzzing campaign. S MARTIAN
covered 1.8× , 3.9× , and 1.1× more instructions than sFuzz,
Manticore, and Mythril, respectively. While Mythril was theclosest to S
MARTIAN in terms of code coverage, it still found
signiﬁcantly fewer bugs than S MARTIAN . This implies that
Mythril failed to modify the state variables in a critical waywhile it was able to cover enough code.
Difference in Bug Oracles. As we discussed in §II-B,
each tool implements its own oracle for the same bug class.This may affect the bug-ﬁnding effectiveness of each tool.For instance, sFuzz only monitors additions and subtractionsto detect integer overﬂows, and ignores multiplications. Thismakes sFuzz prone to false negatives.
To tackle this problem, we ran additional experiments by
modifying S
MARTIAN to have the same oracle logic with the
comparison target. For example, we replaced our IB oracle
with the oracle of sFuzz, and then compared the modiﬁed
SMARTIAN against sFuzz. This way, we can compare the bug-
ﬁnding effectiveness of each tool without being affected bythe inconsistency of oracles. It turned out that S
MARTIAN
outperforms other tools even after aligning the IBoracle with
them. S MARTIAN still found 4.0× , 3.8× , and 2.1× more
CVEs than sFuzz, Manticore, and Mythril, respectively.
We further investigated the result to compare the bug
oracles. First, when we replaced our bug oracle with the oracleof sFuzz and Manticore, S
MARTIAN found 31% and 21%
fewer CVEs. When we modiﬁed S MARTIAN ’s oracle to match
with that of Mythril, S MARTIAN found 1% more CVEs, but it
raised 46% more alarms instead. Thus, we conclude that ourIB oracle in §IV -C3 is most appropriate for this benchmark.
2) Comparison on B2: We now compare our system against
other tools on B2, which we constructed from SmartBug
benchmark [25]. We selected contracts that were labeled withblock state dependency (BD), mishandled exception (ME),or reentrancy (RE). However, we found that the labels wereincomplete for some of the contracts. For instance, a contract
235Smartian sFuzz Manticore Mythril ILF
020406080
0 1 02 03 04 05 06 0
Time (min.)Total # of Bugs found
0K10K20K30K40K50K60K
0 1 02 03 04 05 06 0
Time (min)Instruction Coverage
Fig. 5. Comparison against state-of-the-art tools on B2.
T ABLE IV
NUMBER OF TP AND FP ALARMS RAISED BY EACH TOOL ON B2.
Bug IDSMARTIAN ILF sFuzz Manticore Mythril
TP FP TP FP TP FP TP FP TP FP
BD 1 1 0 0 01 00 6 5 8 0
ME 48 0 10 0 29 6 18 0 46 0
RE 19 0 15 2 5 20 19 3 19 38
that was classiﬁed to have ME often contained RE, as well.
Thus, we manually investigated the contracts and labeled them
again. Then, we ran each tool for one hour on each of thecontracts in B2. To measure the bug-ﬁnding effectiveness, we
checked whether the tool can report the bugs labeled on eachcontract.
Figure 5 illustrates the result of this comparison. We present
the number of bugs found over time on the left side, andinstruction coverage over time on the right side. The resultsare averaged over ﬁve times of repeated experiments.
The ﬁgure shows that S
MARTIAN is able to ﬁnd more bugs
than other tools. When compared to fuzzers, S MARTIAN found
3.1× and 1.8× more bugs than ILF and sFuzz, respectively.
SMARTIAN also outperformed symbolic executors, by ﬁnding
1.8× and 1.1× more bugs than Manticore and Mythril, re-
spectively. The result was consistent over the ﬁve repetitions(p-value <0.01from Mann-Whitney U-Test for all the tools).
In addition, there were only two bugs that were missed by
S
MARTIAN but found by any other tool.
Moreover, S MARTIAN constantly covered more instructions
than other tools, too. In particular, S MARTIAN covered respec-
tively 1.4× and 1.7× more instructions than ILF and sFuzz.
Also, it covered 3.1× more instructions than Manticore and
1.2× more instructions than Mythril.
We also count the number of false positive alarms raised
by each tool and summarize them in Table IV, along with thenumber of found bugs (i.e. true positives). The table showsthat S
MARTIAN not only has the highest recall, but it also
has the highest precision for this benchmark. Especially, othertools suffered from a high false positive rate for RE bugs.
This is mainly because these tools do not properly considerthe storage access pattern of the target contract, and simplyT ABLE V
NUMBER OF BUGS FOUND BY SMARTIAN ON B3.
Bug ID Description # of Bugs Reported # TP # FP
AW Arbitrary Write 0 0 0
BD Block State Dependency 26 20 6
CH Control Hijack 0 0 0
EL Ether Leak 5 3 2
IB Integer Bug 170 170 0
ME Mishandled Exception 2 2 0
MS Multiple Send 5 5 0
RE Reentrancy 0 0 0
SC Suicidal Contract 0 0 0
TO Transaction Origin Use 11 11 0
Total - 219 211 8
check whether there is a call to an external contract.
Difference in Bug Oracles. Again, we reimplemented
the bug oracles of S MARTIAN to match with other tools’
oracles, to compare the bug-ﬁnding effectiveness under thesame condition. In particular, we modiﬁed our BD, ME, and
RE oracles to align with those of other tools. S
MARTIAN still
prevailed other tools after the alignment of the oracles. Themodiﬁed S
MARTIAN found 2.8× more bugs than ILF and
1.7× more bugs than sFuzz. Also, it found 1.9× more bugs
than Manticore and 1.1× more bugs than Mythril.
Answer to RQ2. SMARTIAN is more effective in ﬁnding
bugs compared to existing state-of-the-art tools. S MARTIAN
ﬁnds 1.1–5.8× more bugs than our comparison targets.
D. Large-Scale Study
Now that we have evaluated the comparative performance of
SMARTIAN , we now turn our attention to the scalability of our
system. Speciﬁcally, we ran S MARTIAN onB3, which consists
of 500 popular and large contracts we gathered from Etherscan(see §V -A3). We ran S
MARTIAN on each contract for one hour,
and manually investigated the reported alarms to classify theminto true and false alarms. We did not include other tools in thisexperiment because implementing other tools’ bug oracles in
S
MARTIAN as we did in §V -C, requires signiﬁcant engineering
effort. Instead, we focus on the scalability of S MARTIAN here.
Table V summarizes the result. Note that we do not report
FE and RV here, for the reasons we discussed in Table I.
In addition, we also omit AF found by S MARTIAN . While
SMARTIAN found hundreds of true positive AF, it is debatable
whether they can be considered as serious bugs, so we do notinclude AF in the table. After excluding these bug classes,
S
MARTIAN reported a total of 219 bug alarms. Out of the 500
contracts, 72 contracts were ﬂagged to have at least one ofthese alarms. We manually inspected the alarms and conﬁrmedthat 96.3% of them were true positives. Recall from §IV -C3,this was possible because S
MARTIAN employs precise bug
oracles to reduce false alarms.
We conﬁrmed that some of the bugs found by S MARTIAN
had similar patterns to the CVEs in B1. Some of the bugs
could even cause contract users or owners to unexpectedly losetheir assets. We also found that most of the bugs were causedby poor software engineering practices. Thus, we conclude
236that S MARTIAN can indeed ﬁnd meaningful bugs in real-world
smart contracts.
Answer to RQ3. SMARTIAN is effective in ﬁnding various
kinds of bugs from a large-scale benchmark. S MARTIAN
could ﬁnd 211 bugs from 500 complex real-world contracts
we collected.
E. Threats to V alidity
First, we performed our experiments on a limited set of
benchmarks. We used two benchmarks (B1 and B2) containing
known bugs, and another benchmark (B3) that consists of largeand popular real-world contracts. We showed the effectivenessof our system on these benchmarks, but other benchmarksmay yield different results. We open-source our code so that
S
MARTIAN can be further evaluated in other work.
Another threat to validity is related to the manual processes
included in our evaluation. For instance, we manually labeledbugs to each contract in B2, and manually classiﬁed the alarms
that S
MARTIAN reported from the contracts in B3. Although
we tried our best to carefully inspect the contract code, wemight have erroneously concluded whether the bug indeedexists or not. We also make our dataset public, to enable cross-checks from other researchers.
VI. D
ISCUSSION
Due to the over-approximating nature of static analysis,
data-ﬂow facts gathered from our I NFO GA THER step (§IV -A)
may contain spurious data-ﬂows that cannot actually occur inruntime. If such false positives are prevalent in the analysisresult, our static analysis may even degrade the fuzzing per-formance. However, the evaluation in §V -B empirically showsthat our analysis is precise enough to guide fuzzing effectively.In the future, we may further improve the static analysisprecision and study how it affects the fuzzing effectiveness.
S
MARTIAN currently inherits the limitations of Eclipser,
such as lack of handling non-linear branch conditions. Adopt-ing other grey-box technologies and recent advances in theﬁeld, such as [17], [42], [46], [47], to complement S
MARTIAN
would be a promising direction for future research.
Although S MARTIAN is speciﬁcally designed for fuzzing
smart contracts, we believe the idea of leveraging data-ﬂowanalyses in fuzzing can be applied to other areas as well. Forinstance, generating the sequence of system calls is a criticalproblem in kernel fuzzing [21], [33], [55]. We leave it as futurework to apply our idea to other domains.
VII. R
ELA TED WORK
Fuzzing has become a de facto standard technique for
ﬁnding security bugs [14], [34], [40], [45], and there has beensigniﬁcant research effort on adopting fuzzing in the domainof smart contracts, too. Recall from §III-C, our contributionis unique in that we are the ﬁrst in adopting both staticand dynamic analyses to systematically deal with statefultransactions of smart contracts for fuzzing.
ContractFuzzer [36] is the ﬁrst academically developed
fuzzer for smart contracts. Since it is a black-box fuzzer, ithas difﬁculties achieving high code coverage. Echidna [31]checks a set of user-deﬁned invariant rules to detect bugs. Ananalyst should embed these rules within the contract sourcecode itself. On the other hand, S
MARTIAN does not require
any human intervention.
Harvey [70] is a commercial (closed-source) fuzzer. It
employs a heuristic referred to as the aggressive mode, whichdirectly mutates state variables to ﬁgure out the dependenciesbetween functions. In contrast, S
MARTIAN systematically ad-
dresses this by statically analyzing the semantics of code. Thesame authors recently enhanced Harvey by employing a staticanalyzer called Bran [71], which can guide grey-box fuzzingtowards target locations. Bran is orthogonal to our work, and
S
MARTIAN can also beneﬁt from it.
sFuzz [53] incorporates AFL [72] with the idea of branch
distance feedback used in search-based testing [48] in orderto explore hard-to-reach branches. However, sFuzz does notdirectly handle the stateful transaction problem we address inthis paper. ILF leverages machine learning to effectively gen-erate transaction sequences. It is orthogonal to our technique,and our analysis can complement ILF, too.
There are several fuzzers outside the domain of smart
contracts, which utilize data ﬂow analysis [18], [28], [56],[68] to ﬁgure out which input bytes need to be mutated orwhich values should be used for the mutation. Our work isorthogonal to them as we are using data-ﬂow information toﬁnd meaningful transaction sequence orders.
VIII. C
ONCLUSION
We studied the current limitation of smart contract test-
ing tools, and identiﬁed several design and technical issues.Speciﬁcally, we tackled the problem of effectively handlingmultiple stateful transactions of smart contracts, which leads tothe introduction of combined static and dynamic analysis tech-niques for generating seeds and updating the seed pool duringa fuzzing campaign. Our study showed that the proposedtechniques incur negligible overhead while enabling effectivefuzzing in terms of both code coverage and bug ﬁnding.We also compared S
MARTIAN against the various state-of-
the-art testing tools on a carefully designed benchmark, andconﬁrmed the effectiveness of it. We publicize both our tooland benchmarks to boost future research.
A
CKNOWLEDGEMENT
We thank the anonymous reviewers for their valuable com-
ments and suggestions. We also thank Josselin Feist andFelipe Manzano for their helpful advice on smart contracttesting. This work was supported by Institute of Information& communications Technology Planning & Evaluation (IITP)grant funded by the Korea government (MSIT) (No.2021-0-01332, Developing Next-Generation Binary Decompiler).
R
EFERENCES
[1] “Arbitrary jump with function type variable,” https://swcregistry.io/docs/
SWC-127.
[2] “Assertion failure,” https://swcregistry.io/docs/SWC-110.
[3] “Authorization through tx.origin,” https://swcregistry.io/docs/SWC-115.
237[4] “Decentralized application security project,” https://dasp.co/.
[5] “Dos with failed call,” https://swcregistry.io/docs/SWC-113.[6] “Etherscan,” https://etherscan.io/.[7] “Nethermind,” https://github.com/NethermindEth/nethermind.[8] “Requirement violation,” https://swcregistry.io/docs/SWC-123.[9] “Smart contract weakness classiﬁcation registry,” https://swcregistry.io/.
[10] “Solidity documentation,” https://docs.soliditylang.org.[11] “Solidity expressions and control structures,” https://docs.soliditylang.
org/en/v0.4.25/control-structures.html.
[12] “Write to arbitrary storage location,” https://swcregistry.io/docs/
SWC-124.
[13] C. Aschermann, S. Schumilo, T. Blazytko, R. Gawlik, and T. Holz,
“REDQUEEN: Fuzzing with input-to-state correspondence,” in Proceed-
ings of the Network and Distributed System Security Symposium, 2019.
[14] M. B ¨ohme, V . J. M. Man `es, and S. K. Cha, “Boosting fuzzer efﬁciency:
An information theoretic perspective,” in Proceedings of the Interna-
tional Symposium on F oundations of Software Engineering, 2020, pp.
678–689.
[15] L. Brent, A. Jurisevic, M. Kong, E. Liu, F. Gauthier, V . Gramoli, R. Holz,
and B. Scholz, “V andal: A scalable security analysis framework for smartcontracts,” 2018.
[16] J. Chang, B. Gao, H. Xiao, J. Sun, Y . Cai, and Z. Y ang, “sCompile: Crit-
ical path identiﬁcation and analysis for smart contracts,” in Proceedings
of the International Conference on F ormal Engineering Methods, 2019,pp. 286–304.
[17] H. Chen, Y . Xue, Y . Li, B. Chen, X. Xie, X. Wu, and Y . Liu, “Hawkeye:
Towards a desired directed grey-box fuzzer,” in Proceedings of the
ACM Conference on Computer and Communications Security, 2018, pp.2095–2108.
[18] P . Chen and H. Chen, “Angora: Efﬁcient fuzzing by principled search,”
inProceedings of the IEEE Symposium on Security and Privacy, 2018,
pp. 855–869.
[19] J. Choi, J. Jang, C. Han, and S. K. Cha, “Eclipser,” https://github.com/
SoftSec-KAIST/Eclipser, 2019.
[20] ——, “Grey-box concolic testing on binary code,” in Proceedings of the
International Conference on Software Engineering, 2019, pp. 736–747.
[21] J. Choi, K. Kim, D. Lee, and S. K. Cha, “NTFuzz: Enabling type-aware
kernel fuzzing on windows with static binary analysis,” in Proceedings
of the IEEE Symposium on Security and Privacy, 2021, pp. 1973–1989.
[22] P . Cousot and R. Cousot, “Abstract interpretation: A uniﬁed lattice
model for static analysis of programs by construction or approximationof ﬁxpoints,” in Proceedings of the ACM Symposium on Principles of
Programming Languages, 1977, pp. 238–252.
[23] P . Daian, “Analysis of the dao exploit,” https://hackingdistributed.com/
2016/06/18/analysis-of-the-dao-exploit/, 2016.
[24] M. di Angelo and G. Salzer, “A survey of tools for analyzing ethereum
smart contracts,” in Proceedings of the IEEE International Conference
on Decentralized Applications and Infrastructures, 2019, pp. 69–78.
[25] T. Durieux, J. F. Ferreira, R. Abreu, and P . Cruz, “Empirical review
of automated analysis tools on 47,587 ethereum smart contracts,” inProceedings of the International Conference on Software Engineering,2020, pp. 530–541.
[26] Ethereum, “Ethereum whitepaper,” https://ethereum.org/en/whitepaper/.
[27] J. Feist, G. Grieco, and A. Groce, “Slither: A static analysis framework
for smart contracts,” in Proceedings of the International Workshop on
Emerging Trends in Software Engineering for Blockchain, 2019, pp. 8–
15.
[28] S. Gan, C. Zhang, P . Chen, B. Zhao, X. Qin, D. Wu, and Z. Chen,
“GREYONE: Data ﬂow sensitive fuzzing,” in Proceedings of the
USENIX Security Symposium, 2020, pp. 2577–2594.
[29] P . Godefroid, N. Klarlund, and K. Sen, “DART: Directed automated ran-
dom testing,” in Proceedings of the ACM Conference on Programming
Language Design and Implementation, 2005, pp. 213–223.
[30] N. Grech, M. Kong, A. Jurisevic, L. Brent, B. Scholz, and Y . Smarag-
dakis, “MadMax: Surviving out-of-gas conditions in ethereum smartcontracts,” in Proceedings of the ACM SIGPLAN International Con-
ference on Object Oriented Programming Systems Languages & Appli-cations, 2018, pp. 116:1–116:27.
[31] G. Grieco, W . Song, A. Cygan, J. Feist, and A. Groce, “Echidna:
Effective, usable, and fast fuzzing for smart contracts,” in Proceedings
of the International Symposium on Software Testing and Analysis, 2020,pp. 557–560.[32] A. Groce, J. Feist, G. Grieco, and M. Colburn, “What are the actual
ﬂaws in important smart contracts (and how can we ﬁnd them)?” inInternational Conference on Financial Cryptography and Data Security,
2020.
[33] H. Han and S. K. Cha, “IMF: Inferred model-based fuzzer,” in Pro-
ceedings of the ACM Conference on Computer and Communications
Security, 2017, pp. 2345–2358.
[34] H. Han, D. Oh, and S. K. Cha, “CodeAlchemist: Semantics-aware code
generation to ﬁnd vulnerabilities in javascript engines,” in Proceedings
of the Network and Distributed System Security Symposium, 2019.
[35] J. He, M. Balunovi ´c, N. Ambroladze, P . Tsankov, and M. V echev,
“Learning to fuzz from symbolic execution with application to smartcontracts,” in Proceedings of the ACM Conference on Computer and
Communications Security, 2019, pp. 531–548.
[36] B. Jiang, Y . Liu, and W . K. Chan, “ContractFuzzer: Fuzzing smart
contracts for vulnerability detection,” in Proceedings of the International
Conference on Automated Software Engineering, 2018, pp. 259–269.
[37] M. Jung, S. Kim, H. Han, J. Choi, and S. K. Cha, “B2R2: Building
an efﬁcient front-end for binary analysis,” in Proceedings of the NDSS
Workshop on Binary Analysis Research, 2019.
[38] S. Kalra, S. Goel, M. Dhawan, and S. Sharma, “Zeus: Analyzing safety
of smart contracts,” in Proceedings of the Network and Distributed
System Security Symposium, 2018.
[39] J. Krupp and C. Rossow, “teether: Gnawing at ethereum to automati-
cally exploit smart contracts,” in Proceedings of the USENIX Security
Symposium, 2018, pp. 1317–1333.
[40] C. Lemieux and K. Sen, “FairFuzz: A targeted mutation strategy
for increasing greybox fuzz testing coverage,” in Proceedings of the
International Conference on Automated Software Engineering, 2018, pp.475–485.
[41] Y . Li, B. Chen, M. Chandramohan, S.-W . Lin, Y . Liu, and A. Tiu,
“Steelix: Program-state based binary fuzzing,” in Proceedings of the
International Symposium on F oundations of Software Engineering, 2017,pp. 627–637.
[42] Y . Li, Y . Xue, H. Chen, X. Wu, C. Zhang, X. Xie, H. Wang, and Y . Liu,
“Cerebro: Context-aware adaptive fuzzing for effective vulnerability de-tection,” in Proceedings of the International Symposium on F oundations
of Software Engineering, 2019, pp. 533–544.
[43] C. Liu, H. Liu, Z. Cao, Z. Chen, B. Chen, and A. W . Roscoe, “ReGuard:
Finding reentrancy bugs in smart contracts,” in Proceedings of the
International Conference on Software Engineering: Companion (ICSE-Companion), 2018, pp. 65–68.
[44] L. Luu, D.-H. Chu, H. Olickel, P . Saxena, and A. Hobor, “Making smart
contracts smarter,” in Proceedings of the ACM Conference on Computer
and Communications Security, 2016, pp. 254–269.
[45] V . J. M. Man `es, H. Han, C. Han, S. K. Cha, M. Egele, E. J. Schwartz,
and M. Woo, “The art, science, and engineering of fuzzing: A survey,”IEEE Transactions on Software Engineering, 2019.
[46] V . J. M. Man `es, S. Kim, and S. K. Cha, “Ankou: Guiding grey-
box fuzzing towards combinatorial difference,” in Proceedings of the
International Conference on Software Engineering, 2020, pp. 1024–1036.
[47] B. Mathis, R. Gopinath, and A. Zeller, “Learning input tokens for
effective fuzzing,” in Proceedings of the International Symposium on
Software Testing and Analysis, 2020, pp. 27–37.
[48] P . McMinn, “Search-based software test data generation: A survey,”
Software Testing, V eriﬁcation and Reliability, vol. 14, no. 2, pp. 105–156, 2004.
[49] A. Møller and M. I. Schwartzbach, “Static program analysis,”
https://cs.au.dk/ amoeller/spa/, 2019.
[50] M. Mossberg, F. Manzano, E. Hennenfent, A. Groce, G. Grieco, J. Feist,
T. Brunson, and A. Dinaburg, “Manticore: A user-friendly symbolicexecution framework for binaries and smart contracts,” in Proceedings
of the International Conference on Automated Software Engineering,2019, pp. 1186–1189.
[51] L. D. Moura and N. Bjørner, “Satisﬁability modulo theories: Introduction
and applications,” Communications of the ACM, vol. 54, no. 9, pp. 69–
77, 2011.
[52] B. Mueller, “Smashing ethereum smart contracts for fun and actual
proﬁt,” in Proceedings of the HITB Security Conference, 2018.
[53] T. D. Nguyen, L. H. Pham, J. Sun, Y . Lin, and Q. T. Minh, “sFuzz: An
efﬁcient adaptive fuzzer for solidity smart contracts,” in Proceedings of
the International Conference on Software Engineering, 2020, pp. 778–788.
238[54] I. Nikoliundeﬁned, A. Kolluri, I. Sergey, P . Saxena, and A. Hobor,
“Finding the greedy, prodigal, and suicidal contracts at scale,” in
Proceedings of the Annual Computer Security Applications Conference ,
2018, pp. 653–663.
[55] S. Pailoor, A. Aday, and S. Jana, “MoonShine: Optimizing OS fuzzer
seed selection with trace distillation,” in Proceedings of the USENIX
Security Symposium, 2018, pp. 729–743.
[56] S. Rawat, V . Jain, A. Kumar, L. Cojocar, C. Giuffrida, and H. Bos,
“VUzzer: Application-aware evolutionary fuzzing,” in Proceedings of
the Network and Distributed System Security Symposium, 2017.
[57] Remix, “Ethereum ide and tools for the web,” https://github.com/
ethereum/remix, 2017.
[58] X. Rival and K. Yi, Introduction to Static Analysis: An Abstract
Interpretation Perspective. MIT Press, 2020.
[59] K. Sen, D. Marinov, and G. Agha, “CUTE: A concolic unit testing
engine for C,” in Proceedings of the International Symposium on
F oundations of Software Engineering, 2005, pp. 263–272.
[60] S. So, M. Lee, J. Park, H. Lee, and H. Oh, “V eriSmart: A highly precise
safety veriﬁer for ethereum smart contracts,” in Proceedings of the IEEE
Symposium on Security and Privacy, 2020, pp. 1678–1694.
[61] T. Su, K. Wu, W . Miao, G. Pu, J. He, Y . Chen, and Z. Su, “A survey on
data-ﬂow testing,” ACM Computing Surveys, vol. 50, no. 1, pp. 1–35,
2017.
[62] S. Tikhomirov, E. V oskresenskaya, I. Ivanitskiy, R. Takhaviev,
E. Marchenko, and Y . Alexandrov, “SmartCheck: Static analysis ofethereum smart contracts,” in Proceedings of the IEEE/ACM Inter-
national Workshop on Emerging Trends in Software Engineering forBlockchain, 2018, pp. 9–16.
[63] C. F. Torres, J. Sch ¨utte, and R. State, “Osiris: Hunting for integer bugs in
ethereum smart contracts,” in Proceedings of the 34th Annual Computer
Security Applications Conference, 2018, pp. 664–676.[64] P . Tsankov, A. Dan, D. Drachsler-Cohen, A. Gervais, F. B ¨unzli, and
M. V echev, “Securify2,” https://github.com/eth-sri/securify2.
[65] ——, “Securify: Practical security analysis of smart contracts,” in
Proceedings of the ACM Conference on Computer and CommunicationsSecurity, 2018, pp. 67–82.
[66] H. Wang, Y . Li, S.-W . Lin, L. Ma, and Y . Liu, “Vultron: Catching
vulnerable smart contracts once and for all,” in Proceedings of the
International Conference on Software Engineering: New Ideas andEmerging Results, 2019, pp. 1–4.
[67] H. Wang, Y . Liu, Y . Li, S.-W . Lin, C. Artho, L. Ma, and Y . Liu,
“Oracle-supported dynamic exploit generation for smart contracts,”IEEE Transactions on Dependable and Secure Computing, 2020.
[68] T. Wang, T. Wei, G. Gu, and W . Zou, “TaintScope: A checksum-aware
directed fuzzing tool for automatic software vulnerability detection,” inProceedings of the IEEE Symposium on Security and Privacy, 2010, pp.497–512.
[69] G. Wood, “Ethereum: A secure decentralised generalised transaction
ledger,” https://ethereum.github.io/yellowpaper/paper.pdf.
[70] V . W ¨ustholz and M. Christakis, “Harvey: A greybox fuzzer for smart
contracts,” in Proceedings of the International Symposium on F ounda-
tions of Software Engineering: Industry Papers, 2020, pp. 1398–1409.
[71] ——, “Targeted greybox fuzzing with static lookahead analysis,” in
Proceedings of the International Conference on Software Engineering,2020, pp. 789–800.
[72] M. Zalewski, “American Fuzzy Lop,” http://lcamtuf.coredump.cx/aﬂ/.[73] E. Zhou, S. Hua, B. Pi, J. Sun, Y . Nomura, K. Y amashita, and
H. Kurihara, “Security assurance for smart contract,” in Proceedings of
the IFIP International Conference on New Technologies, Mobility andSecurity, 2018, pp. 1–5.
239