ReassessingAutomatic EvaluationMetricsforCode
Summarization
Tasks
DevjeetRoy
devjeet.roy@wsu.edu
WashingtonStateUniversity
Pullman, WA,USASarah Fakhoury
sarah.fakhoury@wsu.edu
WashingtonStateUniversity
Pullman, WA,USAVeneraArnaoudova
venera.arnaoudova@wsu.edu
WashingtonStateUniversity
Pullman, WA,USA
ABSTRACT
In recent years, research in the domain of source code summa-
rization has adopted data-driven techniques pioneered in machine
translation (MT). Automatic evaluation metrics such as BLEU, ME-
TEOR,andROUGE,arefundamentaltotheevaluationofMTsys-
tems and have been adopted as proxies of human evaluation in the
codesummarizationdomain.However,theextenttowhichauto-
maticmetricsagreewiththegoldstandardofhumanevaluationhas
notbeenevaluatedoncodesummarizationtasks.Despitethis,mar-
ginal improvements in metric scores are often used to discriminate
between the performance of competing summarization models.
Inthispaper,wepresentacriticalexplorationoftheapplicability
andinterpretationofautomaticmetricsasevaluationtechniques
for code summarization tasks. We conduct an empirical study with
226 human annotators to assess the degree to which automatic
metricsreflecthumanevaluation.Resultsindicatethatmetricim-
provementsoflessthan2pointsdonotguaranteesystematicim-
provementsinsummarizationquality,andareunreliableasproxies
ofhumanevaluation.Whenthedifferencebetweenmetricscores
for two summarization approaches increases but remains within
5points,somemetricssuchasMETEORandchrFbecomehighly
reliableproxies,whereasothers,suchascorpusBLEU,remainunre-
liable. Based on these findings, we make several recommendations
fortheuseofautomaticmetricstodiscriminatemodelperformance
in code summarization.
CCS CONCEPTS
¬∑Softwareanditsengineering ‚ÜíSoftwaremaintenancetools ;
¬∑General andreference ‚ÜíMetrics; Evaluation.
KEYWORDS
automatic evaluation metrics, code summarization, machine trans-
lation
ACM Reference Format:
Devjeet Roy, Sarah Fakhoury, and Venera Arnaoudova. 2021. Reassess-
ing Automatic Evaluation Metrics for Code Summarization Tasks. In Pro-
ceedings of the 29th ACM Joint European Software Engineering Conference
and Symposium on the Foundations of Software Engineering (ESEC/FSE ‚Äô21), 
August 23≈õ28, 2021, Athens, Greece. ACM, New York, NY, USA, 12 pages.
https://doi.org/10.1145/3468264.3468588
ESEC/FSE ‚Äô21, August 23≈õ28, 2021, Athens, Greece
¬© 2021 Copyright held by the owner/author(s).
ACM ISBN 978-1-4503-8562-6/21/08.
https://doi.org/10.1145/3468264.34685881 INTRODUCTION
Useful source code comments play a vital role in program compre-
hensionandothersoftwaremaintenanceactivities[ 45,58].How-
ever, proper documentation comes at a cost and producing well-
written comments requires a substantial amount of effort on the
partofsoftwaredevelopers.Thisisoneofthemotivatingfactors
behind why source code summarization is a rapidly growing re-
search area√êatleast 18 papersproposing orevaluating automated
summarization approaches are published in 2020 [ 1,2,10,16,18,
20,22,24,27,32,47,52,55,56,60,61,63,66].
Theearliestcodesummarizationapproachesarebasedonstrong
syntactic theories of comment structure, information retrieval, and
textualtemplates.Theseapproachestypicallyevaluatethequalityof
ageneratedsummaryusinghumanannotatorswhoratesummaries
on metrics such as as: content adequacy [ 36],conciseness[ 23,37],
andfluency[37, 38,50].
In the last 5 years, the solution space for code summarization
has significantly shiftedtowardsthe widespread adoptionof tech-
niques and evaluation metrics from the Machine Translation (MT)
domain.Priorto2015,virtuallyallsummarizationapproachesin-
volved template or information retrieval based techniques. In 2015,
thefirstpaperusinganMTapproachwaspublishedatanSEcon-
ference [39], and, to the best of our knowledge, there are 35 pa-
pers thus far that propose an approach, an evaluation, or a cri-
tiqueofMTsummarizationapproaches[ 2≈õ5,8≈õ10,12,16≈õ21,23≈õ
25,27,28,31,35,39,47,48,53≈õ57,59≈õ62,65,66]. Note that 7 of
thoseare publishedin 2019 and13 in 2020 alone.
Oneoftheprimaryreasonsforthisshiftistheideathattrans-
latingsourcecodetoitsnaturallanguageequivalentholdsmany
parallels with the concept of translating one natural language to
another [ 23]. A side effect of this shift is a substantial change in
the amount of data needed to evaluate code summarization ap-
proaches; training generative models requires a large amount of
data.Naturally,theevaluationtechniquesusedmustalsoscale,and
automated metrics provide an efficient way to evaluate the quality
of generatedsummariesen mass.
Automatedmetricsdesignedtoevaluatenaturallanguagetransla-
tionapproaches,suchasBLEU[ 40],METEOR[ 6],andROUGE[ 30],
have been adopted by code summarization researchers where a
generated summary is compared to a ‚Äògold standard‚Äô or ‚Äòreference‚Äô
summary. In the context of leading comment summaries, the refer-
encesummaryisoftentheoriginalaccompanyingcommentwritten
by adeveloper.
IntheMTdomain,BLEUhaslongbeenacceptedasade-facto
best-practice metric. Recently, however, prominent machine trans-
lation researchers have raised concern over the use of BLEU [ 34,
42,43], warning the MT community that the way BLEU is used
1105This work is licensed under a Creative Commons Attribution International 4.0 License.
ESEC/FSE ‚Äô21, August 23‚Äì28, 2021,Athens,Greece Devjeet Roy,SarahFakhoury,andVenera Arnaoudova
and interpreted can greatly affect its reliability. Moreover, while
evidence supports BLEU for diagnostic evaluationof MT systems,
itdoes not support using BLEUoutsideofMT[ 43].
Alarmingly, although critiques of BLEU have been published
foroverthreeyearsnow,thereliabilityandvalidityofminorim-
provements for those automatic metrics with respect to human
evaluationshavenotbeenre-assessedinthedomainofSoftware
Engineeringfor the purpose ofcode summarization.
Inthispaper,wepresentacriticalexplorationoftheapplicability
andinterpretationofautomatedmetrics,forcodesummarization
tasks.First,asamotivationalresearchquestion,wecomparethedis-
tributions of automatic metric scores for 5 existing summarization
approaches,andweshowthatsimilartoMT,whentheautomatic
metric difference is marginal (within 2 points) the distributions are
not statistically different,meaningthat none of the approaches can
be declared as a state-of-the-art based solely on automatic metrics.
Next,weinvestigatewhetherautomaticmetricsarereliableprox-
ies for human evaluations, i.e., whether automatic metrics are able
to reflect perceived quality improvements in generated summaries,
as indicated by human annotators. We do that both at corpus- and
summary-levels using results from 226 human annotators. Corpus-
level metrics have the advantage to provide an overview of the
performanceofanapproachonacorpus.Summary-levelmetricsal-
low traceability when trying to understand the situations in which
an approachdoes not performwell andthusallow totarget those
for improvements.
We show that, at corpus-level, when the difference between
automaticmetric scoresfortwosummarizationapproachesis ‚â§2
points,all automaticmetrics areveryunreliable (i.e.,they makean
errorinmorethan70%ofthecasesonaverage,rangingfrom62.5%
to 83.7%). We also show that the minimum threshold for reliability
for automatic metrics varies significantly across the metrics we
evaluated. Notably, METEOR and chrF are extremely reliable for
differencesgreaterthan2points ;exhibiting1.3%and2.6%errorrates,
respectively.Finally,weshowthatsummary-levelmetricsdonot
havesufficientdiscriminativepowertoreflecthumanevaluation
andthus cannotbe usedas reliable proxies.
Implications. Our goal is not to identify the current state-of-
the-art summarization approach, but rather to provide a critical
evaluation of automatic evaluation metrics as they are currently
used in code summarization research. For instance, from the 30
papersthat declared state-of-the-artsummarization techniques in
the past5 years, 15of themreportimprovedperformance base on
metric improvements within the range of 0-2 points, for which,
asweshow,allmetricsareunreliable.OurfindingsenabletheSE
communitytocontextualizetheseresultsinordertobetterinterpret
evaluations ofcode summarizationapproaches.
The maincontributions ofthisworkare as follows:
(1)Weshowthatsmalldifferencesinmetricscoresmightnot
guaranteesystematicdifferencesinsummarizationquality
between twoapproaches.
(2)We demonstrate that automatic evaluation metrics com-
monlyusedinsourcecodesummarizationarenotreliable
indicatorsofhumanassessmentwhenmetricdifferencesare
small (<2points).(3) We provideconcrete recommendations for practitioners re-
gardingtheinterpretationofevaluationmetricsaswellas
the designofhuman evaluation studies.
(4)A replication package containing human annotations and
therepresentativerandomsampleofsourcecodesnippets
usedinthis work [ 46].
Vocabulary Throughoutthispaperwewillbedrawingparallels
between the domain of Machine Translation (MT) and the Code
Summarization sub-domain of Software Engineering. To remain
consistent,weadaptthevocabularyoftheformertothelatter.In
MTasystemisanalogoustoacodesummarization approach.System-
levelmetrics inMTcorrespondstometricsthatarecalculatedfor
the entire corpus and will be referred to as corpus-level metrics
here. In MT a sentence or translation-level metric isreferred to as
asegment-level metric , whereas in code summarization this will
bereferredtoasa summary-levelmetric .Terminologydenotinga
groundtruthtranslation,orsummary,is referredtoas a reference
inboth domains.
2 RELATED WORK
2.1 EvaluationofAutomaticMetricsin
Machine Translation
In the past few years, a number of researchers in the machine
translationcommunityhavecalledforareevaluationofthegold
standardmetrics andprocedures being used.
In 2018, Reiter [ 43] presented a structured review of the validity
ofBLEUandfoundthatwhilethereisevidencethatsupportsusing
BLEU for diagnostic evaluation of MT systems, it does not support
using BLEU outside of the MT domain, for evaluation of individual
texts,orfor scientifichypothesistesting.
Most recently, in their 2020 paper Mathur et al. [ 34] highlight
seriousissueswiththecurrentmethodsforevaluatingmetricsin
MT.TheuseofPearson‚Äôs (ùëü)todeterminehowmetricscorrelate
with human assessment is highly sensitive to the translations used
for assessment, particularlythe presenceof outliers. They demon-
stratehowthismethodoftenleadstoinflatedconclusionsaboutthe
efficiencyofametric.Mathuretal.alsoshowthatsmallchanges
in evaluation metrics (e.g., 1≈õ2 points BLEU on a 100 scale) are
not enough to draw empirical conclusions, and they should be
supportedwithmanual evaluations.
Our work investigates how reliable automatic metrics are as
proxiesforhumanevaluation,andhowmetricthresholdsthresholds
translate inthe domainof sourcecode summarization.
2.2 EvaluationofAutomaticMetricsin Code
Summarization
LeClair and McMillan [ 26] point out the lack of suitable datasets
and community standards to create those datasets in the code sum-
marizationdomain,whichresultsinconfusingandun-reproducible
researchresults.
Stapletonetal.[ 52]explorehowautomaticallygeneratedsum-
maries effect code comprehension and show that participants per-
formsignificantlybetteronsnippetsthatcontainhumanwritten
summaries, as opposed to generated summaries. However, they
1106ReassessingAutomatic Evaluation Metrics forCode SummarizationTasks ESEC/FSE ‚Äô21, August 23‚Äì28, 2021,Athens,Greece
werenotabletoshowacorrelationbetweenhumanperformance
andautomaticmetric scores.
Gros et al. [ 16] show that, as compared to natural language
datasets,sourcecodecommentsaremuchmorerepetitivewhich
can have a significant and exaggerated impact on the performance
of an approach. They also demonstrate that BLEU scores vary con-
siderably between differentwaysofcalculation.
Building upon previous work we investigate the reliability of
corpus-andsummary-levelautomaticmetricswithrespecttohu-
man assessments scores, and devise guidelines for minimum im-
provements inautomatic metricscores that mustexistinorder to
systematically demonstrate perceivable improvement by human
evaluation.
3 STUDYDESIGN AND SETUP
In this section, we formulate the research questions that guide this
study,providedetailsregarding thedatasetand automatic metrics
chosen for this work, and the survey designed to collect human
evaluations.
3.1 Research Questions
RQ0Isthereasignificantdifferenceinthecorpuslevelmet-
rics of different models? Automatic evaluation metrics
help discriminate between two approaches based on their
ability to summarize source code. However, these metrics
arenotdesignedtobeusedinisolation,andsimplyattaining
ahighermetricscoreisinsufficienttoestablishwhetherone
code summarization approach is better than another. This
researchquestioninvestigatesifincreasesinmetricscores
result from systematic improvements in the quality of the
summariesgeneratedbyanapproach,ratherthanbychance.
RQ1Docommonlyusedcorpus-levelmetricsreflecthuman
qualityassessmentsofgeneratedsummaries? Automatic
evaluationmetricsaredesignedtoserveasanapproximation
ofhumanassessments.Thedevelopmentandperformance
ofthesemetricshave been extensively documentedinthe
natural language domain, however, it is unclear how they
perform in the context of source code summarization. In
this research question, we measure the degree of agreement
betweencorpus-levelmetricproxiesandactualhumanas-
sessmentscores.Specifically,wemeasuretheiragreementin
thecontextofdiscriminatingtwosummarizationapproaches
using pairwisetests ofsignificance.
RQ2Aresummary-levelmetricsabletoreflecthumanqual-
ity assessments of generated summaries? Corpus-level
metricscannotexplainnuanceinanapproach‚Äôsperformance
atasummarylevelbecausetheyprovideasinglescoreforan
approach, acrossall of itsgenerated summaries, thustrace-
abilitytoindividualsummariesisimpossible.Summary-level
metrics score individual summaries and enable a direct com-
parison of a human assessment score to a metric score for
each generated summary. However, summary level metrics
have been shown to have a lower correlation on natural lan-
guagedatasets[ 49,64].Weinvestigatewhetherthisisalso
the casefor code summarizationtasks.Graduate
StudentProfessional
DeveloperWorking in
AcademiaUndergraduate
StudentOther
0 25 50 75Gender
Man
Non‚àíBinary
Prefer not to disclose
Prefer to self describe
Woman
Figure 1:Demographics forHumanAnnotators.
3.2 Dataset
Haque et al. [ 18] recently proposed an improvement to existing
summarization techniques by leveraging the file context of Java
methods.Intheirwork,theyapplytheproposedimprovementto
severalexistingtechniquesonapubliclyavailabledataset[ 26],and
provide a replication package containing all requisite materials
requiredforeasyreplication.Duetotherecencyofthework,the
ease of replication, and variety of approaches presented, we select
fivesummarizationapproachesfromtheirreplicationpackageto
use in our study as follows: ast-attendgru ,ast-attend-gru-fc ,
code2seq ,graph2seq andtransformer ,whichwewillreferto
asM1≈õM5.Thereplicationpackageincludesapproachsnapshots
at various epochs.We usethesnapshots thathave thebest corpus
level performance for eachof the selectedapproaches.
3.3 SurveyDesign
To evaluate the degree of agreement between automatic metrics
and human assessments, we ask annotators to rate individual sum-
maries1.Eachannotatorisshown6differentcodesnippets(i.e.,Java
methodimplementation).Foreachsnippet,participantsevaluate
5 summaries (generated by the 5 summarization approaches), as
well as the reference summary. The order in which the snippets
andsummariesarepresentedisrandomized.Thesurveytakeson
average15 minutesto complete.
3.4 Human Annotators
We recruit annotators through social media and direct emails to
researchersandpractitionersintheSEcommunity.Participantsare
asked demographic information, including the number of years of
academic and professional experience. Figure 1displays the distri-
butions of the human annotators participating in the study. A total
of 226 people submitted annotations: 48 professional developers,
61workinginacademia,87graduatestudents,17undergraduate
students, and 13 others. 92% of the participants have experience
withJava.
3.5 AutomaticMetrics
An automatic metric compares generated summaries with manual
referencesummariestoproduce(1)acorpus-levelscore,i.e.,asingle
overall score for thegivenapproach, (2) summary-levelscores for
eachofthegeneratedsummaries,or(3)both.Weselectmetricsthat
are most commonlyused by code summarizationapproaches[ 50]
1This study has been certified as exempt from the need for review by the Washington
State UniversityInstitutionalReviewBoard.
1107ESEC/FSE ‚Äô21, August 23‚Äì28, 2021,Athens,Greece Devjeet Roy,SarahFakhoury,andVenera Arnaoudova
or that show the best performance from the most recent WMT
2019 findings [ 33] (results for the metrics task for WMT-2020 have
not been published yet). One notable mention here is that we re-
placeYISI(best performingsegmentlevelmetric inWMT19) with
BERTScore[ 64],sincethewordembeddingsrequiredtorunYISIare
no longer available on the author‚Äôs website. We use the official im-
plementationsofeachmetricusedbyWMTortheauthorprovided
implementation,whenavailable.Detailsabouttheimplementations
usedare available inthe replication package.
(1)Corpus-LevelOnly
(a)BLEU[40]isatextualsimilaritymetricthatcalculatesthe
precisionofn-gramsinatranslatedsentenceascompared
toareferencesentence,withaweightedbrevitypenalty
to punish short translations. We use the standard BLEU
scorewhichprovidesacumulativescoreofuni-,bi-,tri-,
and4-grams.
(b)ROUGE[30]isapopularautomaticevaluationmetricthat
is recall oriented. It computes the count of several over-
lapping units such as n-grams, word pairs, and sequences.
ROUGE has several different variants from which we con-
sider the most popular ones: ROUGE-N(1-4), ROUGE-L,
andROUGE-W.
(2)Summary-LevelOnly
(a)sentBLEU is a smoothed version of BLEU, designed to
scoretranslationsatasegmentlevel.Inouranalysis,we
use the script provided by NLTK with smoothing method
5, as reportedin[ 7]2.
(3)BothSummary-andCorpus-Level
(a)METEOR[ 6]isametricbasedonthegeneralconceptof
unigram matching, and itcombinesprecision, recall,and
a custom score determining the degree to which words
are orderedcorrectlyinthe translation.
(b)BERTScore[ 64]isanewertypeofautomaticevaluation
metricthatemployscontextualwordembeddingsusing
the widely popular BERT language model, to compute
the semantic and lexical similarities between a model‚Äôs
predictions andreference tokens.
(c)chrF [41] is another recent automatic evaluation metric
that works solely on character n-grams rather than word
n-grams.Itcan be seen as acharacter n-gramF-score.
3.6 Sampling andData Preprocessing
3.6.1 Sampling. The test set in the dataset described above con-
tains90,908sourcecodesnippetsandtheircorrespondingreference
summary.ForRQ1andRQ2,werandomlysample383snippets(95%
confidencelevelandaconfidenceintervalof ¬±5%).Foreachmethod,
wegenerate5summaries(usingthe5selectedsummarizationap-
proaches). Including the reference summary, this results in a total
of2,298(=383*(5+1))uniquesummariesneedingevaluation.Weaim
for3annotatorsforeachsummary,i.e.,atotalof6,894(=2,298*3)
unique human evaluations are needed.
3.6.2 Data Preprocessing. Each of the 383 snippets has summaries
writtenby3participantsthatareusedforqualitycontrol(RQ1and
2Chen and Cherry [ 7] show that smoothing method 7 performs the best, but we
encountered unstable results (NANs) with it. Method 5 is the second best performing
smoothingmethod,with a small performance difference.RQ2) and for the analysis of multiple references (RQ2). This is a to-
talof1,322(=383*3)collectedsummarieswrittenbytheannotators.
After collectingthedata, we discarded answers thateitherdid not
contain summaries (as in such cases it is impossible to know if the
annotator understood the method) or contained summaries that in-
dicatedaclearmisunderstandingofthemethod‚Äôsfunctionality.We
also excluded annotators who clearly misunderstood the questions.
Examples include annotators who provided rankings from 1 to 6
insteadofscoresfrom1to100,andannotatorswhoprovidedthe
same scores for all snippetsand all summaries. We used standard
outlier detection techniques to identify potential anomalies and
validatedthemmanually.Afterremovingoutliers,somesnippets
had 2 evaluations (instead of 3). We added a third evaluation if the
existingtwoevaluationsdidnotconverge(i.e., if thedifferencein
human scores is greater than 25 points out of 100). We kept the
survey open until we gatheredthe necessary annotations.
Finally, after cleaning the data, we retained 6,253 unique human
evaluations ofsummariesfor the analysis.
4 ANALYSISMETHOD
4.1 Human Assessment oftheQuality of
Summaries
4.1.1 Collection of human assessments. Human assessments for
each approach are collected using two different scales: Likert Scale
andDirectAssessment (DA) scores.
LikertScale .Annotatorsratedifferentaspectsofeachsummary,
namely:conciseness,fluency,andcontentadequacyona5-point
Likertscale[ 29].Thoseaspectsarestandardandhavebeenusedin
both MT [ 43] and in code summarization [ 38,50] evaluations with
human annotators. To calculate a golden truth score, the values
from the Likert scale are averaged across all human evaluations to
create asentence-level oracorpus-level score,per approach.
DirectAssessment (DA) Scores . Direct Assessment (DA) [ 14]
is a standard technique used in MT to collect summary evalua-
tionsusingavisualanalogscalefrom 0to 100.DAscoresprovide
reference-free human assessments of each approach for a given
snippet,i.e.,anassessmentindependentoftheoriginalreference
summary for that snippet. We modify the DA process to fit our
needsandaskannotatorstofirstsummarizethesnippetandthen
evaluate a set of summaries for that snippet. One of the summaries
is the original summary and the rest are the generated summaries;
annotators are unaware of whether a summary is automatically
generatedorwritten bydevelopers.
4.1.2 Interpretinghumanassessmentsatsummary-andcorpus-level.
Interpretinghumanscoresatcorpus-levelisdifferentfrominter-
preting themat summary-level.
Corpus-Level Golden Truth . Collecting enough human as-
sessmentsforcorpuslevelanalysisisfeasiblesincetheassessments
foreachindividualsummarycontributetowardsthecorpuslevel
humanassessment.Thecorpuslevelhumanassessmentscorefor
an approach is the mean of all its human assessment scores. Mean
scorescalculatedthiswayhavebeenshowntobeconsistentand
reproduciblefor MTtasks[ 14].
1108ReassessingAutomatic Evaluation Metrics forCode SummarizationTasks ESEC/FSE ‚Äô21, August 23‚Äì28, 2021,Athens,Greece
Summary-LevelGolden Truth(DARR) .However,atasum-
marylevel,onlyassessmentsforthatsummaryaretakenintoac-
count. Graham et al. [ 14] found that 15 human assessments per
summaryarerequiredinordertoprovideastableandreproducible
mean human assessment score for a summary. Collecting such a
large number of assessments per summary drastically raises the
numberofhumanannotatorsrequired.Tomitigatethisissue,we
followtheMaetal.[ 33]andconvertDirectAssessmentscoresinto
pairwiserelativerankings,knownasDirectAssessmentRelative
Rankings (DaRR). One human assessment for each of the 5 ap-
proaches for a single snippet we consider yields 10 pairs of relative
ranking assessments, wherein each pair is a human assessment of
whether one approach isbetterthanthe other,orofequal quality.
Inthefollowing,wedescribetheanalysismethodwefollowto
evaluate automaticmetrics at summary-andcorpus-level.
4.2 RQ Analysis Method
4.2.1 RQ 0.Is there a significant difference in the corpus level metrics
ofdifferentmodels? Forsummary-levelmetricsorcorpus-levelmet-
ricsthataredefinedasanaggregationofindividualsummary-level
metric scores, we can compute the significance of the difference of
metric scores between two summarization approaches using tradi-
tionalstatisticaltechniques,suchasapairedt-testoraWilcoxon
Sign-Ranktest.However,thesetestscannotbeappliedtocorpus-
level only metrics, such as BLEU, wherein the corpus-level score
is not composed of individual summary level scores. Hence, it is
common practice in the MT community to instead use randomized
significance testing for comparing corpus level scores [ 15]. A com-
monlyusedtestispairedbootstrapresampling.However,bootstrap
resamplingissensitive to the sample being tested;itassumesthat
the sample is representative of the larger population, a tenuous
assumptionforsmallsamplesizes[ 44].WhileGrahametal.[ 15]
found that this concern might be exaggerated, we nonetheless fol-
low a conservative approach and utilize an alternative randomized
significant test, known as approximate randomization (a variant of
permutationtesting),whichcanprovidegreaterstatisticalpower
whilesimultaneouslymakingnoassumptionsabouttheunderlying
sampling distribution.
To understand how approximate randomization works, consider
two summarization approaches, ùê¥andùêµ, with metric scores ùëÜùê¥
andùëÜùêµfor a given metric. Our null hypothesis ( ‚Ñé0) is that there
isnodifferenceinthemetricscoresbetweenthetwoapproaches
(ùëÜùê¥=ùëÜùêµ).Theapproximaterandomizationtestapproximatesthe
samplingdistributionoftheteststatisticbyrandomlyshuffling50%
ofthesummariesbetweenthetwoapproachesseveraltimes(the
number of shuffles is defined by the parameter ùëÖ). For each shuffle,
we calculate a pseudo-statistic ( ùëÜùëü
ùê¥&ùëÜùëü
ùêµ), which is the same as the
test statistic but computed for the shuffle. Then, for a one-tailed
test, we can simply count the number of shuffles for which the
test statistic is smaller than the pseudo-statistic for each shuffle,
whichinturnisusedtocomputethe ùëù‚àíùë£ùëéùëôùë¢ùëíforthetest.Inother
words, the test assumes that each summary is equally likely to
come from approach ùê¥orùêµ, and if the two approaches are not
significantly different, the shuffling should not affect the difference
intheirmetric scores.4.2.2 RQ 1.Do commonly used corpus-level metrics reflect human
qualityassessmentsofgeneratedsummaries? Tomeasurethedegree
towhichautomaticmetricscoresagreewithhumanassessments,
wefirstbuildasetofsyntheticapproachesfromexistingapproaches
in our dataset. Then, we compute the overall corpus-level corre-
lation of automatic metric scores with human assessments and
pairwise corpus-level significance tests to determine the degree to
which human assessments agree with automatic evaluation met-
rics. The rationale behind building synthetic approaches is that
collecting human assessments of a large number of approaches
isextremelyresourceintensive.Moreover,thelackofacommon
benchmarkdatasetwithpubliclyavailablepredictionlogs andthe
computationalcostoftrainingtheseapproachesonanewcommon
datasetcanbeprohibitive.Syntheticapproachescreatedthisway
havetheaddedbenefitofprovidingdiversityinmetricscores.Thus,
we collect human assessments for five approaches, and then we
usetheseassessmentstocreateasetofsyntheticsummarization
approachesbysystematicallyimprovingordegradingeachofthe
original approaches to bolster the initial set of approaches. We de-
scribedetailsforthecreationofthesyntheticapproachesandthe
analysisbelow.
Synthetic Models .Tocreateasyntheticmodel,westartwith
one of the five approaches and replace a varying proportion of the
predictionswithpredictionsfromotherapproachesthatreceivea
higher or lower human assessment score. We parameterize the cre-
ationoftheseapproachesbytheproportionofpredictionsreplaced,
andtheminimumthresholdforwhichthehumanassessmentscore
for a prediction is deemed better/worse. We replace predictions
fromamodelat8differentproportions,inparticular:1%,2%,5%,
10%,15%,20%,25%,and30%of the predictions.
To improve 1% of a model‚Äôs predictions, for example, we choose
its1%worstperformingpredictionsandreplacethemwiththebest
existingpredictionfromtheremaining models.We use5different
thresholds for minimum human score improvement: 1, 5, 10, 15,
and20(outof100).Thisistosaythatiftheminimumhumanscore
improvementissetto15,thenifbetweenamodel‚Äôsworstprediction
andthebestpredictionofothermodelsthedifferenceislessthan
15, we discard this data point. This combination of proportions
and thresholds gives us 40 ( =8‚àó5) different configurations for
generatingsyntheticmodels.Foreachconfiguration,webothim-
proveanddegradethescore.Aswestartoutwith5summarization
approaches,we end upwith400( =40‚àó2‚àó5) syntheticmodels.
After de-duplicating, we are left with 267 unique synthetic mod-
elsthathaveuniquecorpus-levelhumanscores.Tokeepthecom-
putation time for our pairwise approach comparison, we randomly
sub-sample 100 from the set of synthetic models (100 models yield
4,950 pairwise combinations, whereas 267 models yield 35,511
pairwisecombinations).Combinedwiththe5summarizationap-
proaches, we end up with a total of 105 models that we use for our
analysisinRQ1.
Pairwisemodelcomparisonofhumanassessmentandcor-
pus level metrics . We conduct pairwise model comparisons be-
tweenhumanassessmentsandmetricscoresusingthemethodology
adoptedbyMathuretal.[ 34],which webrieflydescribehere.We
startbyenumeratingallpairsofsummarizationmodelsinoursam-
ple(includingbothoriginalandsyntheticmodels)andcomputing
pairwise differences in corpus-level metric scores. We then divide
1109ESEC/FSE ‚Äô21, August 23‚Äì28, 2021,Athens,Greece Devjeet Roy,SarahFakhoury,andVenera Arnaoudova
these pairs into several different buckets based on the statistical
significance of themetric score differenceas well as onthe magni-
tude.
For example, a bucket can be defined for statistically significant
metricdifferencesbetween2and5.Foreachofthesepairs,wealso
calculatethesignificanceofthedifferenceintheircorresponding
human assessment scores (DA). The effectiveness of a corpus-level
metriccanthenbedeterminedbylookingattheagreementbetween
themetricscoreandhumanassessmentscore.Inotherwords,given
two models with significantly different metric scores, we aim to
understand if this difference is also determined as significant by
human annotators. For a reliable automatic evaluation metric, one
expectstofindaone-to-onecorrespondencebetweensignificant
differences inmetric scores andhuman assessment scores.
4.2.3 RQ 2.Are summary-level metrics able to reflect human quality
assessmentsofgeneratedsummaries? ToanswerthisRQ,wecalcu-
late the summary level correlation of automatic metric scores with
human assessment scores. While DA scores are continuous and
amenabletotraditionalcorrelationcoefficientssuchasPearson‚Äôs
and Spearman‚Äôs Correlation, it has been shown empirically that at
least15humanassessmentsare requiredpersummaryforresults
to bestable [ 13].Hence,we follow[ 33], andconvert DAscores to
pairwise DARR scores as explained in Section 4.1.2. To compute a
correlationbetweentheDARRscoresandthecorrespondingmetric
scoresforourdataset,weemployamodifiedversionofKendall‚Äôs
ùúè3to compute this correlation:
ùúè=|ùê∂ùëúùëõùëêùëúùëüùëëùëéùëõùë° ‚àíùê∑ùëñùë†ùëêùëúùëüùëëùëéùëõùë° |
|ùê∂ùëúùëõùëêùëúùëüùëëùëéùëõùë° +ùê∑ùëñùë†ùëêùëúùëüùëëùëéùëõùë° +ùëáùëñùëíùë†|(1)
where concordant, discordant, and tied pairs for two summaries
ùë†1 andùë†2 in a DARR pair are calculated according to Table 1. By
comparing how humans (rows) and metrics (columns) relatively
rank the two summaries, we mark the pair as either concordant
(humanandmetricrelativeranksagree),discordant(humanand
metric relative ranks disagree) or tied (metric relative ranks are
the same).This specificformulationpenalizesmetricsthathave a
largenumber ofties, as opposed to thevariantused most recently
in WMT [ 33], which discards ties in human or metric scores. In
WMT‚Äôsformulation,tiesinmetricdifferencesandhumanscoresare
discarded. As a consequence, metrics that produce a large number
oftiescanexhibitahighcorrelationwithhumanjudgment,despite
having a large proportion of ties. For example, a metric with 5
concordant,1discordantand500tieswouldattainahighcorrelation
of0.8,withthe500tiesnotbeing accountedfor inthecorrelation.
Ourformulationexplicitlypenalizestiesandpreventsthisscenario.
WedirectthereadertoStanchevetal.[ 51]foradetailedtreatmentof
the different formulations of Kendall‚Äôs Tau for machine translation
evaluation. We follow WMT protocol, and consider a difference of
lessthan25inDAscoresasanon-significantdifferencei.e.,thetwo
summariesare deemed to be ofequal quality [ 33]. This isbecause
pairs of assessment‚Äôs in a DARR score are comprised of individual
3An adaptation is needed since the original Kendall‚Äôs ùúèis used to measure the cor-
relationofasinglepairofjointrandomvariables,i.e.,wewouldneedthecomplete
ranking of all summaries by both human scores (by each human annotator) and auto-
maticmetrics.However, DARRpairscanonly providedisjointrankingsof 5model‚Äôs
summaries,andnotacompleterankingofallsummariesinthedataset.Hence,weuse
a version of Kendall‚Äôs ùúèadaptationoriginally proposed by Grahametal. [ 13].assessmentscollectedonavisualanalogscaleratherthanaprecise
numeric scale, and therefore small differences in individual DA
scores can be discarded. On a corpus level, such thresholding is
not needed, as the analysis there involves several DA scores and
statistical tests can be used to determine the significance of the
difference between twosystems.
Table 1: Modified Kendall‚Äôs ùúè: Concordant and Discordant
PairFormulations.
Metric
ùë†1<ùë†2ùë†1=ùë†2ùë†1>ùë†2
DAùë†1<ùë†2 Concordant Tie Discordant
‚à•ùë†1‚àíùë†2‚à•‚â§25- - -
ùë†1>ùë†2 Discordant Tie Concordant
5 RESULTS
In this section, we answer the research questions defined in Sec-
tion3.
5.1 RQ 0:Is thereasignificantdifferenceinthe
corpuslevel metricsofdifferent models?
ThecorpuslevelscoresforeachofthemodelsareshowninTable 2,
for the entire test set of the dataset by LeClair et al. [ 26] (see the
leftsideof Table 2,highlighted ingray).We also report themetric
scoresforthesampleweuseforRQ1andRQ2toseehowthetrends
inmetricscoreschangefromthecorpustothesample(rightside
Table2). For both splits, M1 tends to be a top performer across
all but 2 metrics, chrF and METEOR for the entire test set, and
ROUGE-1andROUGE-4forthesample.M5isconsistentlytheworst
performing model across both splits, with its scores often being
half the score of the closest performing model. We also observe
thatthepairsofmodels(M1,M2)and(M3,M4)tendtohavesimilar
metric scores ( <1 point) within the pair. Across the pairs, the
metric difference ranges within 2points. Both thewithin pair and
acrosspairdifferencesinmetricscoresareplausiblescenarioswhen
comparing a newly proposed approach to the existing state of the
art,asitisuncommontoimproveonthestateoftheartbymore
than2metric points.
The results of the approximate randomization test for each com-
bination of 2 approaches shows that there is no statistically signifi-
cant (ùëù>0.05) difference between the metric scores of the top 4
Table 2:Corpus-Level Automatic Metric Scores.
EntireTestSet Sample Only
M1M2M3M4M5M1 M2 M3 M4 M5
BERTScore 35.735.134.034.619.439.337.1 37.0 38.3 21.9
BLEU 19.919.618.618.65.422.020.8 21.2 21.6 6.7
chrF 38.338.637.237.621.141.440.5 40.0 40.6 22.5
METEOR 19.519.618.918.88.221.421.0 20.4 20.9 8.8
ROUGE-1 46.546.445.845.723.549.048.2 47.6 48.1 24.8
ROUGE-2 26.125.925.525.38.529.726.7 27.5 28.3 9.8
ROUGE-3 17.116.916.316.12.720.317.1 18.1 19.0 4.0
ROUGE-4 12.312.011.211.01.713.6 11.8 13.6 13.82.4
ROUGE-L 44.544.443.943.822.947.146.1 45.9 46.2 24.1
ROUGE-W 44.544.443.943.822.947.146.1 45.9 46.2 24.1
1110ReassessingAutomatic Evaluation Metrics forCode SummarizationTasks ESEC/FSE ‚Äô21, August 23‚Äì28, 2021,Athens,Greece
Table 3:Corpus-levelHumanAssessmentScores.
Overall Content
DA Score Conciseness Adequacy Fluency
reference 54.40 3.27 3.14 3.46
M1 48.22 3.37 2.89 3.49
M2 49.15 3.32 2.97 3.38
M3 49.63 3.38 2.99 3.47
M4 49.44 3.39 2.93 3.46
M5 16.38 2.31 1.49 2.92
performingapproaches.Thismeans that interms ofallautomatic
evaluationmetricsconsideredinthispaper,thetop4approaches
performequallywell,despitesmalldifferencesintheirscores.How-
ever, each of M1, M2, M3 and M4 is significantly better than M5,
the worst performingapproach.
RQ0takeaway: When looking at the distribution of automatic
metric scores at corpus-level, there is no statistically significant
difference in performance between models whose performance
iswithina1.5pointdifference.Thismeansthatsuchadifference
in scores does not guarantee that an approach with a higher
metricscoreoffersasystematicimprovementovertheapproach
with the lower score. We do however observe a statistically
significant difference in performance between models whose
performance difference isgreater than10 points.
5.2 RQ 1:Docommonly usedcorpus-level metrics
reflect humanqualityassessments of
generatedsummaries?
HumanAssessments ofSummarization Approaches . Table3
contains the human assessment scores at corpus-level for each
of the approaches. We observe that the reference summaries are
rated with the highest score in terms of DA scores, which is a
5 point difference from the best DA score for a summarization
approach.Thisindicatesthatthereferencesummariesareperceived
as having higherqualitycomparedto the automaticallygenerated
summaries in terms of DA; the difference is statistically significant.
Thereferencesummariesalsoscorethehighestintermsofcontent
adequacy.References,however,performworsethanM1-4,interms
ofconciseness,andworse thanM1andM3interms offluency.
Overall, all approaches perform similarly to each other in terms
ofDA,contentadequacy,fluency,andconciseness.Theexceptionis
M5,whichrankssignificantlyloweracrossallmetrics.TheDAscore
for M5 is more than 26 points lower than the closest performing
approach.WealsoconductapairwiseWilcoxonSigned-Ranktest
tocheckwhetherthedifferencesinDAscorefortheapproachesare
significant.Wefindthatthehumanwrittensummariesarerated
significantly based on DA scores when compared to automatic
summariesgeneratedbyM1≈õM5.Whenweconsideronlymodel
generated summaries, there is no significant difference between
thescoresofthetop4models.ThecorpuslevelDAscoresforallof
these models lie between 2 points of each other. There is, however,
a significant differencewhen comparing any of the top 4 modelsto
the worst performingmodel, M5.Pairwise Model Comparisons. Given two summarization ap-
proaches ùê¥andùêµ, and an automatic evaluation metric, we are
interestedinwhetheradifferenceintheirrespectivehumanassess-
ment scores is reflected in their metric scores ùëÜùê¥andùëÜùêµ. We can
characterize this relationbystudyinginstances 1) where ametric
detectsachangeinqualitywhennoneisperceivedbyhumanasses-
sors(Type-IError),and2)whereametricfailstodetectachange
inqualitywhenthereisahumanperceivedqualitychange(Type-II
Error). To conduct this analysis, we enumerate all pairwise com-
binations of models (original and synthetic), and conduct paired
difference tests on their metric and human quality assessments. By
comparing the results of these tests, we can quantify the Type-I
and Type-II errors of the metric with respect to human quality
assessments.
Figure2and Figure 3represents all pairwise summarization
approach comparisons for BLEU and METEOR, respectively. Each
point on the plot represents a pair of summarization approaches
ùê¥andùêµ.They-axisrepresentsthemetricdifferencebetweenthe
approaches for a pair ( ùëÜùê¥‚àíùëÜùêµ) and is binned into 5 groups. The
firstbinlabelled≈ÇNS≈ærepresentsdifferencesinmetricscoresthat
arenotsignificant,whiletherestrepresentsignificantdifferences
inmetricscores.Thex-axisrepresentsthecorrespondingchange
in the human overall DA score. Each pair is colored to indicate
whether the change in DA score is significant. Each pair is ordered
to represent an improvement from ùê¥toùêµ, i.e.,ùëÜùêµ>ùëÜùê¥. For all
significantdifferencesinmetricscores(allbinsexcluding≈ÇNS≈æ),we
willseeapositiveandsignificantchangeinhumanassessmentscore
whenthemetricdecisionalignswithhumandecision.Anypairs
herethataredeemedtobeofequalquality(darkbluepairs)inthese
binsrepresentType-Ierrors.Forallnon-significantdifferencesin
metricscores(firstbin)weexpecttoseenon-significantdifferences
in human DA scores (dark blue pairs) when the metric aligns with
humanassessment.Anypairswithasignificantdifferenceinhuman
scores (green or red ) here represent Type-II errors. We also report
precise numbers of Type-I and Type-II errors for all metrics in
Table4.
WeobservethatforbothBLEUandMETEOR,thereexistseveral
pairs that are assessed as better ( ùëÜùêµ>ùëÜùê¥) or worse ( ùëÜùê¥<ùëÜùêµ) by
human assessors but insignificant by the evaluation metric (first
bin).Thisisalsotrueforalloftheothermetricsweconsider(figures
omitted for space). From Table 4, we see that the majority of these
Type-II errors occurs when the metric difference is less than 2
points, and gradually diminishes as the metric difference increases.
Within the bin (0, 2], all metrics have a Type-II error range from
a minimum of 69% (BERTScore) to a maximum of 83.9% (BLEU).
BLEU is in fact notably worse than the rest of metrics here; out
ofthe6,025metricpairsitratesasbeingofequalquality,human
assessorsrate5,075ofthesepairstobesignificantlydifferent.All
theothermetricsmakethiserroronlessthan2,000pairsinthisbin.
Overall,we see thatwhen differences inmetric scores are small (<
2 points),all automatic evaluation metricsare highlysusceptible to
Type-IIerrors.Whenwemovetothenextbin(2,5],weseethatthe
Type-IIerrordiminishesforallmetrics.WhiletheoverallType-II
errorrateincreasesformostmetrics,i.e.,thepercentageofthetimes
that these metrics align with human assessment when they report
a non-significant difference, the total number of times that they
makeaType-IIerrorisstrikinglylowcomparedtothepreviousbin
1111ESEC/FSE ‚Äô21, August 23‚Äì28, 2021,Athens,Greece Devjeet Roy,SarahFakhoury,andVenera Arnaoudova
(< 80 for all metrics besides BLEU). Notably, METEOR commits no
Type-II errors in this bin. The rest of the metrics, excluding BLEU,
commitnoType-IIerrorswhen metricdifferencesare largerthan
5.Lastly,BLEUcontinuestosufferfromahighincidenceType-II
errorsas we move across thebins.Theincidence ofType-IIerrors
seems to be unaffectedbythe magnitude ofits metric difference.
(20.0, 100.0](15.0, 20.0](10.0, 15.0](5.0, 10.0](2.0, 5.0](‚àí0.001, 2.0]NS
‚àí50 ‚àí25 0 25 50
DA DifferenceMetric Difference
Better
Worse
NS
Figure 2:Pairwise BLEUvs DAScores.
(10.0, 15.0](5.0, 10.0](2.0, 5.0](‚àí0.001, 2.0]NS
0 20 40
DA DifferenceMetric Difference
Better
Worse
NS
Figure 3:Pairwise METEOR vs DAScores.
WenowturnourattentiontoType-1errors.FromFigures 2and3,
weseethatbothBLEUandMETEORshowadecreasingtrendof
Type-Ierrorasthemetricdifferencesgetlarger.Inthistendency,
BLEUdiffersfrom the restofthe metrics(whichare verysimilar
to METEOR), having a relatively low rate of Type-I error when
metricdifferencesaresmall.Infact,forthebin(2,5],weobserve
that it makes no errors at all. If we refer to Table 4, the reason
behind this becomes apparent; BLEU only reports 14/1,245 pairs to
have a significant difference. This is consistent with the other bins,
wherein BLEU marks pairs of approaches as being significantly
differentveryconservatively,andconsequentlyhasahighType-I
errorinstead.Alltheothermetricsshowamoderatebutnon-trivial
incidence of Type-I error when the metric difference is less than 2
points, ranging from 8% for chrF to 27.6% for ROUGE-4. This error
ratedropssharplyasmetricdifferencesgetlargerthan2points,and
disappearscompletelyabove10points.BERTScore,chrF,METEOR,
ROUGE-3andROUGE-4ceasetocommitType-Ierrorswhenthe
metricdifferencebecomeslargerthan10points,whereasROUGE-1,
ROUGE-L and ROUGE-W continue to have the same errors, but at
muchlower rates.
BeyondType-IandType-IIerrorrates,wealsowanttolookatthe
overallagreementbetweenmetricdecisionsandhumandecisions
atdifferentbins.Itispossibleforbothmetricsandhumanassessors
toassesstwosystemstobesignificantlydifferent,butinoppositedirections, i.e., metrics might rate ùëÜùê¥to be better than ùëÜùêµ, while
humans rate ùëÜùêµto be better than ùëÜùê¥. Table5reports disagreement
rates,whichconsistsofType-IandType-IIerrors,alongwithany
instanceswherethemetricscoresdisagreewithhumanassessment
regardless of statistical significance. Here, we observe a similar
trend as the Type-I and -II errors. When metric differences are less
than 2 points, all metrics have an overall disagreement rate of at
least62.5%.Whenwemoveontohigherbins,allmetricsbesides
BLEU experience significant drops in their overall disagreement
rates. In the third bin, (10, 15], BLEU is the only metric with an
overalldisagreementrateofover5%.Forthebins(10,15]and(20,
100], these metrics largelyagree with humanassessment. Overall,
METEOR shows the lowest levels of disagreements across the bins.
BLEUdeviatessignificantlyfrom theothermetrics considered;its
overall disagreement rate is erratic. It drops from 97.5% to 31.6% as
wemovefromthefirstbintothethird,beforeincreasingagainand
maintainingat greater than75%for the remaining bins.
From the patterns of Type-I and Type-II errors, as well as the
overalldisagreementrate,aclearpictureemerges:whenthemet-
ric difference between two approaches are less than 2 points, no
metric is able to discriminate between two systems in a manner
consistent with human ratings. At this level of metric difference,
metrics are 1) not sensitive to changes in summarization quality
that are otherwise perceived by human assessors and 2) falsely re-
portsignificantdifferencesinqualitywhennoneexists.Inaddition,
evenwhenthemetricscorrectlyreportasignificantdifferencein
quality,thedirectionofthisdifferencemightnotalignwithhuman
assessors. It is only when metric differences get larger than 2-5
points(dependingonthemetric),thatmetricsareabletoreliably
discriminatebetweentwosummarizationapproachesinagreement
withhuman assessors.
RQ1takeaway: Automaticevaluationmetricsarenotableto
accuratelycapturedifferencesinsummarizationqualitybetween
twoapproacheswhenthemetricdifferenceissmall( ‚â§2points).
METEOR, BERTScore and chrF perform the best in terms of
Type-IandType-IIerrorrate.BLEUhasthehighestType-Ierror
rateregardless ofthe magnitude of the difference.
5.3 RQ 2:Aresummary-level metricsableto
reflect humanqualityassessments of
generated summaries?
Results of the calculation of the correlation between human assess-
ment and summarylevel metrics using Kendall‚ÄôsTauformulation
are reported in Table 6. Similar to the results for MT tasks [ 33],
the correlation between human assessment scores and sentence-
level metrics are low for all metrics, between 0.1 for ROUGE-4 and
0.47 for BERTScore. This indicates that at a sentence level, none of
theconsideredmetricsare suitableproxiesforhumanassessment,
includingratingsoffluency,conciseness,andcontentadequacy.
NotethattheKendall‚ÄôsTauformulationusedinWMT2019[ 33],
disregards ties in both metric scores and human assessment scores.
Forourdataset,theseveralmetricshavearight-skeweddistribution.
Forexample,ROUGE-4rates89%ofsummarieswithascoreof0.
While this is not a problem for large scale datasets, for smaller
datasets, such as the one we use here, this can potentially bias the
correlationtowardsmetricsthatproducealotofties,sincetheyare
1112ReassessingAutomatic Evaluation Metrics forCode SummarizationTasks ESEC/FSE ‚Äô21, August 23‚Äì28, 2021,Athens,Greece
Table 4:Corpus-levelErrorRates forAutomatic Metrics.
Delta range: [0.0, 2.0] Delta range: (2.0, 5.0] Delta range: (5.0, 10.0] Delta range: (10.0,15.0]
TypeIError TypeIIError TypeIError TypeIIError TypeIError TypeIIError TypeIError TypeIIError
Rate Gr. Size Rate Gr. Size Rate Gr. Size Rate Gr. Size Rate Gr. Size Rate Gr. Size Rate Gr. Size Rate Gr. Size
BERTScore 14.5% 392 69.4% 1164 8.2% 1152 77.8% 18 1.5% 1303 0.0% 741
BLEU 1.9% 1613 83.9% 6025 0.0% 14 98.0% 1231 2.7% 1599 81.3% 476 1.3% 1510 85.6% 90
chrF 8.0% 513 71.4% 1503 2.2% 1098 100.0% 1 4.5% 778 0.0% 718
METEOR 14.8% 899 76.2% 1494 1.3% 1341 2.7% 859 0.0% 763
ROUGE-1 12.5% 375 72.3% 1477 4.5% 793 57.1% 7 1.6% 1108 7.1% 241
ROUGE-2 13.6% 221 78.4% 1582 9.7% 1237 86.7% 30 3.7% 845 0.4% 744
ROUGE-3 18.4% 174 80.3% 1699 8.5% 1374 82.9% 70 3.3% 910 0.0% 1010
ROUGE-4 27.6% 445 82.5% 1833 3.5% 1493 100.0% 3 1.7% 1151 0.0% 404
ROUGE-L 11.4% 395 72.7% 1548 3.2% 877 100.0% 2 2.4% 959 4.0% 300
ROUGE-W 11.4% 395 72.7% 1548 3.2% 877 100.0% 2 2.4% 959 4.0% 300
Table 5:Corpus-levelDisagreementRateforAutomatic Metrics.
Delta: [0.0, 2.0] Delta: (2.0, 5.0] Delta: (5.0, 10.0] Delta: (10.0,15.0] Delta: (15.0,20.0] Delta: (20.0,100.0]
D. Rate Gr. Size D. Rate Gr. Size D. Rate Gr. Size D. Rate Gr. Size D. Rate Gr. Size D. Rate Gr. Size
BERTScore 66.6% 1556 9.4% 1170 1.5% 1303 741 586
BLEU 72.7% 7638 97.5% 1245 79.7% 2075 31.6% 1600 76.6% 2882 75.6% 5972
chrF 65.7% 2016 2.6% 1099 4.5% 778 718 745
METEOR 62.5% 2393 1.3% 1341 2.7% 859 763
ROUGE-1 72.1% 1852 5.1% 800 1.6% 1108 7.1% 241 696 659
ROUGE-2 80.7% 1804 17.8% 1267 3.7% 845 0.4% 744 696
ROUGE-3 83.7% 1878 18.4% 1444 3.3% 910 1010 114
ROUGE-4 80.8% 2305 5.1% 1496 1.7% 1151 404
ROUGE-L 71.5% 1943 3.5% 879 2.4% 959 4.0% 300 783 492
ROUGE-W 71.5% 1943 3.5% 879 2.4% 959 4.0% 300 783 492
Figure 4:Sentence-levelcomparisons:BERTScore vs DA.
evaluated on a smaller subset of the data than metrics that don‚Äôt
producemanyties.Hence,weuseaversionoftheformulationthat
penalizesties.
FollowingtheresultsfromRQ1,correlationsalonedonotprovide
enough information intothenature of the relationshipbetween a
metric scores and human quality assessments. We looked at the
agreementbetweenDARRscoresandsummary-levelmetrics,utiliz-
ingthebinsusedinRQ1,butnowwithmetricdifferencescalculatedTable 6:Kendall‚Äôs ùúèforsummary-level metrics.
Overal DA Conciseness ContentAdequacy Fluency
BERTScore 0.475 0.362 0.387 0.294
chrF 0.451 0.312 0.401 0.210
METEOR 0.467 0.336 0.398 0.201
ROUGE-1 0.446 0.334 0.376 0.196
ROUGE-2 0.302 0.206 0.245 0.123
ROUGE-3 0.191 0.127 0.161 0.075
ROUGE-4 0.110 0.064 0.092 0.034
ROUGE-L 0.435 0.327 0.362 0.197
ROUGE-W 0.441 0.327 0.371 0.198
sentBLEU 0.240 0.302 0.167 0.210
atthesummary-level.Weplot theseresultsforBERTScore inFig-
ure4.
WeobservethatBERTScorescoredistributionisheavilyright-
skewed, with the majority of summaries with scores of 0. For sum-
marylevelmetricswithhighcorrelationstoDARRscores,wewould
expecttoseetheproportionofconcordanttodiscordantpairsin-
crease as the metric score increases. For BERTScore, there is no
clear trend. This explains the lowKendall‚ÄôsTau correlation.
Summary level metrics can potentially provide an advantage
overcorpuslevelmetricsthroughfine-grainedtraceabilityofthe
performance of a summarization approach. However, results show
thatcurrentsummary-levelmetricscannotbeusedasreliableprox-
ies for human evaluations at the level of individualsummaries.
1113ESEC/FSE ‚Äô21, August 23‚Äì28, 2021,Athens,Greece Devjeet Roy,SarahFakhoury,andVenera Arnaoudova
RQ2takeaway: Summary-levelmetricsdonotcorrelatewith
human assessment for source code summaries and cannot be
usedas reliable proxies for human evaluations.
6 IMPLICATIONS
The useof MTtechniques for thepurpose of codesummarization
is relatively new but growing rapidly, with the first paper being
published in2015 and at least30 novel approaches have been pub-
lishedsincethen.Resultsofthispaperreflectthattherapidgrowth
inthedomainmightbeattheexpenseofthereliabilityofmetrics
used to evaluate novel approaches. For instance, from the 30 pa-
persthatdeclaredstate-of-the-artsummarizationtechniquesinthe
past 5 years [ 2≈õ5,8≈õ10,12,17≈õ21,23≈õ25,28,31,35,39,48,54≈õ
57,59≈õ61,65,66], 15 declare metrics improvements in the 0-2
range [2,3,9,10,12,20,24,25,28,31,48,57,60,65,66]. At these
levels of differences, we show that evaluation metrics are not re-
liable, and do not guarantee an actual improvement in summa-
rizationqualityasjudgedbyhumanassessors.Outofthese,only
5[20,23,39,55,56]performhumanevaluations,noneofwhichfol-
lowasystematicmethodtocomparethecompetingsummarization
approaches:humanevaluationsareeitherconductedonabiased
sample(methodschosenamongthebestperformingsummaries)
or summaries are compared only to the reference and not to a
competing model. Based on our findings, we provide the following
recommendations for practitioners:
Automatic Metric Recommendations:
‚Ä¢Usesignificanceteststoestablishwhetherchangesinmetric
scores represent systematic improvements inmodelperfor-
mance.Formetricsthatprovidesummarylevelscoresthat
can be aggregated to produce a corpus level score, standard
paired t-test, or a Wilcoxon Sign-Rank test can be utilized
to check for differences in performance. When this is not
possible, randomized significance tests such as approximate
randomizationand paired-bootstrap resampling can be used
to determine significance. A detailed methodology is out-
linedinDror[ 11]andGraham etal.[ 15].
‚Ä¢Differences in automatic metric scores within 2 points do
not guarantee a perceptible difference in human assessment,
andthisholdsforallmetricsevaluatedinthiswork.Small
changes in evaluation metrics should not be used as the
sole basis to draw important empirical conclusions, when
possible, human evaluation should be used to strengthen
performance claims.
‚Ä¢METEOR and chrF are more reliable indicators of human
judgement than corpus level BLEU, especially for metric
differenceslargerthan2points.Thecommunityshouldre-
consider the useof BLEUas thestandard evaluationmetric
for code summarization.
‚Ä¢Theautomaticevaluationmetricsweconsiderarenotreli-
able at a summary level. This result is consistent with ma-
chine translation literature. While these metrics might be
useful at a summary level when supplemented with manual
analysis,theyarenotareliableproxyforhumanjudgment
forindividualexamplesontheirown.However,thesemet-
rics can still be used if they are aggregated across the entire
corpus.HumanEvaluation:
‚Ä¢When human evaluation is performed, it must be conducted
onarepresentativesampleofsummariesgeneratedbythe
approachbeingevaluated.Thisensuresthattheevaluation
willprovideanunbiasedestimateofhowtheapproachwould
perform onthe restof the dataset.
‚Ä¢For each summary, we recommend collecting 1) a summary
ofthecodesnippetwrittenbyeachhumanannotatorthat
will serve as quality control, 2) human evaluations on a
continuousscale(DAscores)fortheautomaticsummaries
generated by the compared approaches, and 3) at least 3
evaluations per snippet.
6.1 Threatsto Validity
Results in the paper are dependent on the considered datasets; a
differentdatasetmightyielddifferentresults.Wechosethisdataset
provided by Haque et al. [ 18] as 1) this is the only dataset that
providesalargesetofpre-trainedmodelswhich uniformlytrains
several models on the same dataset, and 2) several of these models
haveshownstate-of-the-artperformancewhenoriginallyproposed.
We acknowledge the existence of other models that might perform
better, however, the focus of this paper is to evaluate automatic
evaluation metrics and not to establish the current state-of-the-art.
Wecreatesyntheticapproachesbydegradingandimprovingthe
summaries of the original approaches in our dataset, and conse-
quently, our results could vary were we to perform a larger human
study with more models. However, gathering assessments for such
alargenumberofrealapproacheswouldbeprohibitive.Tomitigate
thisthreat,weparameterizethecreationofsyntheticapproaches
to represent different levels of improvements over the original ap-
proach as describedearlier.
Another threat to the validity of our results comes from the
evaluationsofthehumanannotators.Weaskparticipantstoprovide
summariesofthesnippetsthattheyareevaluatingandusethose
summaries for quality control. We also perform standard outlier
detection to identify and potentially remove abnormal data points.
7 CONCLUSION
This work provides a critical evaluation of the applicability and
interpretation of automatic metrics as evaluation techniques for
codesummarizationtasks.Tothebestofourknowledge,thisisthe
first empirical studytoinvestigatethe degree to which automatic
evaluationmetricsreflecthumanevaluationinthedomainofsource
code summarization.
Automatic metrics are commonly used to compare the perfor-
mance of two approaches, and in this work we investigate to what
extent the difference in metric scores reflect human judgments.
Resultsshowthatmarginaldifferences(0-2points)inmetricscores
between two approaches do not guarantee human perceivable im-
provement, and make an error in up to 70% of cases on average. As
a result, small changes in evaluation metrics should not be used
as the sole basis to draw important empirical conclusions about
performance improvements, and should be supported with human
evaluation.Ourfindingsindicatethecommunityshouldreconsider
BLEU as the standard metric for evaluation, in lieu of more reliable
metrics such as METEOR andchrF.
1114ReassessingAutomatic Evaluation Metrics forCode SummarizationTasks ESEC/FSE ‚Äô21, August 23‚Äì28, 2021,Athens,Greece
REFERENCES
[1]Alireza Aghamohammadi,Maliheh Izadi,and Abbas Heydarnoori.2020. Gener-
ating summaries for methods of event-driven programs: An Android case study.
Journal ofSystemsand Software 170(2020), 110800.
[2]WasiAhmad,SaikatChakraborty,BaishakhiRay,andKai-WeiChang.2020. A
Transformer-based Approach for Source CodeSummarization. In ACL(short) .
[3]Abdulaziz Alhefdhi, Hoa Khanh Dam, Hideaki Hata, and Aditya Ghose. 2018.
Generating pseudo-code from source code using deep learning. In 2018 25th
Australasian SoftwareEngineering Conference (ASWEC) . IEEE,21≈õ25.
[4]Miltiadis Allamanis, Hao Peng, and Charles Sutton. 2016. A convolutional at-
tention network for extreme summarization of source code. In International
Conference onMachineLearning . 2091≈õ2100.
[5]Uri Alon, Shaked Brody, Omer Levy, and Eran Yahav. 2018. code2seq: Gen-
erating sequences from structured representations of code. arXiv preprint
arXiv:1808.01400 (2018).
[6]Satanjeev Banerjee and Alon Lavie. 2005. METEOR: An automatic metric for
MTevaluation withimproved correlation withhumanjudgments.In Proceedings
of the acl workshop on intrinsic and extrinsic evaluation measures for machine
translation and/or summarization . 65≈õ72.
[7]Boxing Chen and Colin Cherry. 2014. A systematic comparison of smoothing
techniques for sentence-level bleu. In Proceedings of the Ninth Workshop on
Statistical MachineTranslation . 362≈õ367.
[8]Minghao Chen and Xiaojun Wan. 2019. Neural Comment Generation for Source
Code with Auxiliary Code Classification Task. In 2019 26th Asia-Pacific Software
Engineering Conference (APSEC) . IEEE,522≈õ529.
[9]Qingying Chenand Minghui Zhou.2018. Aneural frameworkforretrieval and
summarizationofsourcecode.In ProceedingsoftheInternationalConferenceon
AutomatedSoftwareEngineering (ASE) . IEEE,826≈õ831.
[10]YunSeok Choi, Suah Kim, and Jee-Hyong Lee. 2020. Source Code Summariza-
tionUsingAttention-BasedKeywordMemoryNetworks.In Proceedingsofthe
International Conference on Big Data and Smart Computing (BigComp) . IEEE,
564≈õ570.
[11]Rotem Dror, Gili Baumer, Segev Shlomov, and Roi Reichart. 2018. The hitch-
hiker‚Äôs guide to testing statistical significance in natural language processing.
InProceedingsof the 56thAnnual Meetingof theAssociationfor Computational
Linguistics (Volume 1: Long Papers) . 1383≈õ1392.
[12]PatrickFernandes,MiltiadisAllamanis,andMarcBrockschmidt.2018. Structured
Neural Summarization. In International Conferenceon Learning Representations .
[13]Yvette Graham, Timothy Baldwin, and Nitika Mathur. 2015. Accurate evalu-
ation of segment-level machine translation metrics. In Proceedings of the 2015
ConferenceoftheNorthAmericanChapteroftheAssociationforComputational
Linguistics: Human Language Technologies . 1183≈õ1191.
[14]YvetteGraham, TimothyBaldwin,AlistairMoffat,andJustinZobel.2013. Con-
tinuous measurement scales in human evaluation of machine translation. In
Proceedingsofthe7thLinguisticAnnotationWorkshopandInteroperabilitywith
Discourse . 33≈õ41.
[15]Yvette Graham, Nitika Mathur, and Timothy Baldwin. 2014. Randomized sig-
nificancetestsinmachinetranslation.In ProceedingsoftheNinthWorkshopon
Statistical MachineTranslation . 266≈õ274.
[16]David Gros, Hariharan Sezhiyan, Prem Devanbu, and Zhou Yu. 2020. Code to
Comment≈ÇTranslation≈æ:Data,Metrics,Baselining&Evaluation.In Proceedingsof
the International Conference on Automated Software Engineering (ASE) . 746≈õ757.
[17]TjallingHaije,BachelorOpleidingKunstmatigeIntelligentie,EGavves,andH
Heuer.2016. Automaticcommentgenerationusinganeuraltranslationmodel.
Inf.Softw.Technol. 55,3 (2016), 258≈õ268.
[18]Sakib Haque, Alexander LeClair, Lingfei Wu, and Collin McMillan. 2020. Im-
provedAutomaticSummarizationofSubroutinesviaAttentiontoFileContext.
InProceedingsoftheWorkingConferenceonMiningSoftwareRepositories(MSR) .
300≈õ310.
[19]Xing Hu, Ge Li, Xin Xia, David Lo, and Zhi Jin. 2018. Deep code comment gener-
ation.InProceedingsoftheInternationalConferenceonProgramComprehension
(ICPC). IEEE,200≈õ20010.
[20]Xing Hu, Ge Li, Xin Xia, David Lo, and Zhi Jin. 2020. Deep code comment
generationwithhybridlexicalandsyntacticalinformation. EmpiricalSoftware
Engineering Journal (EMSE) 25,3 (2020), 2179≈õ2217.
[21]Xing Hu, Ge Li, Xin Xia, David Lo, Shuai Lu, and Zhi Jin. 2018. Summarizing
source code with transferred api knowledge.(2018). In Proceedings of the Twenty-
Seventh International Joint Conference on Artificial Intelligence (IJCAI) , Vol. 19.
2269≈õ2275.
[22]YuanHuang,ShaohaoHuang,HuanchaoChen,XiangpingChen,ZibinZheng,
Xiapu Luo, Nan Jia, Xinyu Hu, and Xiaocong Zhou. 2020. Towards automati-
cally generating block comments for code snippets. Information and Software
Technology 127(2020), 106373.
[23]Srinivasan Iyer, Ioannis Konstas, Alvin Cheung, and Luke Zettlemoyer. 2016.
Summarizingsourcecodeusinganeuralattentionmodel.In Proceedingsofthe
54thAnnualMeetingoftheAssociationforComputationalLinguistics(Volume1:
Long Papers) . 2073≈õ2083.[24]AlexanderLeClair,SakibHaque,LinfgeiWu,andCollinMcMillan.2020.Improved
codesummarizationviaagraphneuralnetwork.In ProceedingsoftheInternational
Conference onProgramComprehension(ICPC) .
[25]Alexander LeClair, Siyuan Jiang, and Collin McMillan. 2019. A neural model
for generating natural language summaries of program subroutines. In 2019
IEEE/ACM 41st International Conference on Software Engineering (ICSE) . IEEE,
795≈õ806.
[26]AlexanderLeClairandCollinMcMillan.2019. Recommendationsfordatasetsfor
source codesummarization. arXiv preprint arXiv:1904.02660 (2019).
[27]Boao Li, Meng Yan, Xin Xia, Xing Hu, Ge Li, and David Lo. 2020. DeepCom-
menter:adeepcodecommentgenerationtoolwithhybridlexicalandsyntactical
information. In Proceedings of the 28th ACM Joint Meeting on European Software
EngineeringConferenceandSymposiumontheFoundationsofSoftwareEngineering
(ESEC/FSE) . 1571≈õ1575.
[28]Yuding Liang and Kenny Zhu. 2018. Automatic generation of text descriptive
comments for code blocks. In Proceedings of the AAAI Conference on Artificial
Intelligence , Vol. 32.
[29]R. Likert. 1932. A Technique for the Measurement of Attitudes. Archives of
Psychology 140(1932), 44≈õ53.
[30]Chin-YewLin.2004. Rouge:Apackageforautomaticevaluationofsummaries.
InTextsummarization branchesout . 74≈õ81.
[31]BohongLiu,TaoWang,XunhuiZhang,QiangFan,GangYin,andJinshengDeng.
2019. A Neural-Network based Code Summarization Approach by Using Source
CodeanditsCallDependencies.In Proceedingsofthe11thAsia-PacificSymposium
onInternetware . 1≈õ10.
[32]Mingwei Liu, Xin Peng, Xiujie Meng, Huanjun Xu, Shuangshuang Xing, Xin
Wang, Yang Liu, and Gang Lv. 2020. Source Code based On-demand Class
Documentation Generation. In Proceedings of the International Conference on
SoftwareMaintenance and Evolution(ICSME) . IEEE,864≈õ865.
[33]QingsongMa,JohnnyWei,Ond≈ôejBojar,andYvetteGraham.2019. Resultsof
the WMT19 metrics shared task: Segment-level and strong MT systems pose
big challenges. In Proceedings of the Fourth Conference on Machine Translation
(Volume 2: SharedTaskPapers, Day 1) . 62≈õ90.
[34]Nitika Mathur, Tim Baldwin, and Trevor Cohn. 2020. Tangled up in BLEU:
Reevaluating the Evaluation of Automatic Machine Translation Evaluation Met-
rics.arXiv preprint arXiv:2006.06264 (2020).
[35]SergeyMatskevichandColinSGordon.2018. Generatingcommentsfromsource
code with CCGs. In Proceedings of the 4th ACM SIGSOFT International Workshop
onNLP for SoftwareEngineering . 26≈õ29.
[36]PaulWMcBurneyandCollinMcMillan.2014. Automaticdocumentationgen-
erationvia sourcecodesummarizationof methodcontext.In Proceedingsofthe
22nd InternationalConference onProgramComprehension . 279≈õ290.
[37]L. Moreno, A. Marcus, L. Pollock, and K. Vijay-Shanker. 2013. JSummarizer: An
automatic generator of natural language summaries for Java classes. In Interna-
tional Conference onProgramComprehension(ICPC) . 230≈õ232.
[38]Najam Nazar, Yan Hu, and He Jiang. 2016. Summarizing software artifacts:
A literature review. Journal of Computer Science and Technology 31, 5 (2016),
883≈õ909.
[39]Yusuke Oda, Hiroyuki Fudaba, Graham Neubig, Hideaki Hata, Sakriani Sakti,
TomokiToda,andSatoshiNakamura.2015. Learningtogeneratepseudo-code
fromsourcecodeusingstatisticalmachinetranslation(t).In Proceedingsofthe
InternationalConferenceonAutomatedSoftwareEngineering(ASE) .IEEE,574≈õ584.
[40]KishorePapineni,SalimRoukos,ToddWard,andWei-JingZhu.2002. BLEU:a
method for automatic evaluation of machine translation. In Proceedings of the
40thannualmeetingofthe Associationfor ComputationalLinguistics . 311≈õ318.
[41]MajaPopoviƒá.2015. chrF:charactern-gramF-scoreforautomaticMTevaluation.
InProceedingsoftheTenthWorkshoponStatisticalMachineTranslation .392≈õ395.
[42]Matt Post. 2018. A call for clarity in reporting BLEU scores. arXiv preprint
arXiv:1804.08771 (2018).
[43]Ehud Reiter.2018. A Structured Review of theValidityof BLEU. Computational
Linguistics 44,3 (2018), 393≈õ401.
[44]Stefan Riezler and John T Maxwell III. 2005. On some pitfalls in automatic
evaluationandsignificancetestingforMT.In ProceedingsoftheACLworkshop
on intrinsic and extrinsic evaluation measures for machine translation and/or
summarization . 57≈õ64.
[45]Peter C. Rigby, Daniel M German, Laura Cowen, and Margaret-Anne Storey.
2014. Peer Review on Open Source Software Projects: Parameters, Statistical
Models,andTheory. ACMTransactionsonSoftwareEngineeringandMethodology
(TOSEM) (2014), Toappear.
[46]Devjeet Roy, Sarah Fakhoury, and Venera Arnaoudova. 2021. Online Replica-
tionPackage .https://github.com/devjeetr/Re-assessing-automatic-evaluation-
metrics-for-source-code-summarization-tasks
[47]DevjeetRoy,ZiyiZhang,VeneraArnaoudova,APanichella,SebastianoPanichella,
DanielleGonzalez,andMehdiMirakhorli.2020. DeepTC-Enhancer:Improving
the Readabilityof AutomaticallyGenerated Tests. (2020).
[48]YusukeShido,YasuakiKobayashi,AkihiroYamamoto,AtsushiMiyamoto,and
TadayukiMatsumura.2019. Automaticsourcecodesummarizationwithextended
tree-lstm.In 2019InternationalJointConferenceonNeuralNetworks(IJCNN) .IEEE,
1115ESEC/FSE ‚Äô21, August 23‚Äì28, 2021,Athens,Greece Devjeet Roy,SarahFakhoury,andVenera Arnaoudova
1≈õ8.
[49]Xingyi Song, Trevor Cohn, and Lucia Specia. 2013. BLEU deconstructed: De-
signingabetterMTevaluationmetric. InternationalJournalofComputational
Linguistics and Applications 4,2 (2013), 29≈õ44.
[50]XiaotaoSong,HailongSun,XuWang,andJiafeiYan.2019. Asurveyofautomatic
generation of source code comments: Algorithms and techniques. IEEE Access 7
(2019), 111411≈õ111428.
[51]PeterStanchev,WeiyueWang,andHermannNey.2020. TowardsaBetterEvalua-
tion of Metrics for Machine Translation. In Proceedings of the Fifth Conference on
MachineTranslation .AssociationforComputationalLinguistics,Online,928≈õ933.
[52]SeanStapleton,YashmeetGambhir,AlexanderLeClair,ZacharyEberhart,Westley
Weimer,KevinLeach,andYuHuang. 2020. A Human Study of Comprehension
andCodeSummarization.In ProceedingsoftheInternationalConferenceonPro-
gram Comprehension(ICPC) . 2≈õ13.
[53]Akiyoshi Takahashi, Hiromitsu Shiina, and Nobuyuki Kobayashi. 2019. Au-
tomatic Generation of Program Comments Based on Problem Statements for
Computational Thinking. In 2019 8th International Congress on Advanced Applied
Informatics (IIAI-AAI) . IEEE,629≈õ634.
[54]Yao Wan, Zhou Zhao, Min Yang, Guandong Xu, Haochao Ying, Jian Wu, and
PhilipSYu.2018. Improvingautomaticsourcecodesummarizationviadeeprein-
forcement learning. In Proceedings ofthe International Conference onAutomated
SoftwareEngineering (ASE) . 397≈õ407.
[55]RuyunWang,HanwenZhang,GuoliangLu,LeiLyu,andChenLyu.2020. Fret:
FunctionalReinforcedTransformerWithBERTforCodeSummarization. IEEE
Access8 (2020), 135591≈õ135604.
[56]WenhuaWang,YuqunZhang,YuleiSui,YaoWan,ZhouZhao,JianWu,Philip
Yu, and Guandong Xu. 2020. Reinforcement-Learning-Guided Source Code Sum-
marizationviaHierarchicalAttention. IEEETransactionsonSoftwareEngineering
(TSE)(2020).
[57]Bolin Wei, Ge Li, Xin Xia, Zhiyi Fu, and Zhi Jin. 2019. Code generation as a
dualtaskofcodesummarization.In AdvancesinNeuralInformationProcessingSystems. 6563≈õ6573.
[58]Kurt D. Welker, Paul W. Oman, and Gerald G. Atkinson. 1997. Development
and Application of an Automated Source Code Maintainability Index. Journal of
SoftwareMaintenance:Research and Practice 9,3 (May1997),127≈õ159.
[59]ShaofengXuandYunXiong.2018. AutomaticGenerationofPseudocodewithAt-
tention Seq2seq Model. In 2018 25th Asia-Pacific Software Engineering Conference
(APSEC). IEEE,711≈õ712.
[60]Wei Ye, Rui Xie, Jinglei Zhang, Tianxiang Hu, Xiaoyin Wang, and Shikun Zhang.
2020. LeveragingCodeGenerationtoImproveCodeRetrievalandSummarization
viaDual Learning. In ProceedingsofThe Web Conference (WWW) . 2309≈õ2319.
[61]Xiaohan Yu, Quzhe Huang, Zheng Wang, Yansong Feng, and Dongyan Zhao.
2020. Towards Context-Aware Code Comment Generation. In Proceedings of the
2020ConferenceonEmpiricalMethodsinNaturalLanguageProcessing:Findings .
3938≈õ3947.
[62]Lingbin Zeng, Xunhui Zhang, Tao Wang, Xiao Li, Jie Yu, and Huaimin Wang.
2018. Improving code summarization by combining deep learning and empir-
ical knowledge (S).. In Proceedings of the International Conference on Software
Engineering and KnowledgeEngineering (SEKE) . 566≈õ565.
[63]Jian Zhang, Xu Wang, Hongyu Zhang, Hailong Sun, and Xudong Liu. 2020.
Retrieval-based neural source code summarization. In Proceedings of the Interna-
tional Conference onSoftwareEngineering (ICSE) .
[64]Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, and Yoav
Artzi. 2019. Bertscore: Evaluating text generation with bert. arXiv preprint
arXiv:1904.09675 (2019).
[65]YuZhou,XinYan,WenhuaYang,TaolueChen,andZhiqiuHuang.2019. Aug-
menting Javamethod commentsgenerationwith context informationbased on
neural networks. Journal ofSystemsand Software 156(2019), 328≈õ340.
[66]ZiyiZhou,HuiqunYu,andGuishengFan.2020.Effectiveapproachestocombining
lexicalandsyntacticalinformationforcodesummarization. Software:Practice
and Experience 50,12(2020), 2313≈õ2336.
1116