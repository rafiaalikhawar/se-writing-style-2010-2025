An Evolutionary Study of Conﬁguration Design
and Implementation in Cloud Systems
Yuanliang Zhangy, Haochen He, Owolabi Legunsenz, Shanshan Li, Wei Dong, Tianyin Xuy
National University of Defense Technology, Changsha, Hunan, China
yUniversity of Illinois at Urbana-Champaign, Urbana, IL 61801, USA
zCornell University, Ithaca, NY 14850, USA
{zhangyuanliang13, hehaochen13, shanshanli, wdong}@nudt.edu.cn, legunsen@cornell.edu, tyxu@illinois.edu
Abstract —Many techniques were proposed for detecting soft-
ware misconﬁgurations in cloud systems and for diagnosing
unintended behavior caused by such misconﬁgurations. Detection
and diagnosis are steps in the right direction: misconﬁgurations
cause many costly failures and severe performance issues. But,
we argue that continued focus on detection and diagnosis is
symptomatic of a more serious problem: conﬁguration design
and implementation are not yet ﬁrst-class software engineering
endeavors in cloud systems. Little is known about how and why
developers evolve conﬁguration design and implementation, and
the challenges that they face in doing so.
This paper presents a source-code level study of the evolution
of conﬁguration design and implementation in cloud systems.
Our goal is to understand the rationale and developer practices
for revising initial conﬁguration design/implementation decisions,
especially in response to consequences of misconﬁgurations. To
this end, we studied 1178 conﬁguration-related commits from
a 2.5 year version-control history of four large-scale, actively-
maintained open-source cloud systems (HDFS, HBase, Spark,
and Cassandra). We derive new insights into the software
conﬁguration engineering process. Our results motivate new tech-
niques for proactively reducing misconﬁgurations by improving
the conﬁguration design and implementation process in cloud
systems. We highlight a number of future research directions.
I. I NTRODUCTION
Software conﬁguration design and implementation have
signiﬁcant impact on the functionality, reliability, and per-
formance of large-scale cloud systems. The idea behind
conﬁguration is to expose conﬁguration parameters which
enable deployment-time system customization. Using different
parameter values, system users (e.g., operators, sysadmins, and
DevOps engineers) can port a software system to different
environments, accommodate different workloads, or satisfy
new user requirements. In cloud systems, conﬁguration pa-
rameters are changed constantly. For example, at Facebook,
conﬁguration changes are committed thousands of times a day,
signiﬁcantly outpacing source-code changes [1].
With the high velocity of conﬁguration changes, misconﬁg-
urations (in the form of erroneous parameter values) inevitably
become a major cause of system failures, severe service
outages, and downtime. For example, misconﬁgurations were
the second largest cause of service-level disruptions in one of
Google’s main production services [2]. Misconﬁgurations also
contribute to 16% of production incidents at Facebook [1], in-
cluding the worst-ever outage of Facebook and Instagram thatoccurred in March 2019 [3]. Similar statistics and incidents
were reported in other systems [4]–[11].
Software conﬁgurations also impose signiﬁcant total cost of
ownership on software vendors, who need to diagnose user-
reported failures or performance issues caused by miscon-
ﬁgurations. Vendors may even have to compensate users, if
the failures lead to outages and downtime. Software vendors
also need to support and help users with conﬁguration-related
questions, e.g., how to ﬁnd the right parameter(s) and set
the right value(s) [12]. Note that system users are often not
developers; they may not understand implementation details
or they may not be able to debug code [13]–[15].
Unfortunately, conﬁguration design and implementation
have been largely overlooked as ﬁrst-class software engi-
neering endeavors in cloud systems, except for few recent
studies (Section VIII). The focus has been on detecting
misconﬁgurations and diagnosing their consequences [13],
[16]–[30]. These efforts tremendously improve system-level
defenses against misconﬁgurations, but they do not address
the fundamental need for better software conﬁguration design
and implementation. Yet, better conﬁguration design can ef-
fectively reduce user difﬁculties, reduce conﬁguration com-
plexity while maintaining ﬂexibility, and proactively reduce
misconﬁgurations [12], [31]–[33]. Also, better conﬁguration
implementation can help detect and correct misconﬁgurations
earlier to prevent failure damage [13], [16].
The understanding of what constitutes software conﬁgu-
ration engineering in cloud systems is preliminary in the
literature, compared with other aspects of engineering these
software systems (e.g., software architecture, modeling, API
design, and testing) which are well studied. Meanwhile, we
observed that developers struggle to design and implement
conﬁgurations. For example, we found that developers raise
many conﬁguration-related concerns and questions—“is the
conﬁguration helpful?” (Spark-25676), “can we reuse an
existing parameter?” (HDFS-13735), “what is a reasonable
default value?” (HBase-19148). Furthermore, we found that
developers frequently revise conﬁguration design/implemen-
tation decisions, usually after observing severe consequences
(e.g., failures and performance issues) induced by the initial
decisions (Sections IV-A1 and IV-A2).
This paper presents a source-code level study of the evo-
lution of conﬁguration design and implementation in cloud
1882021 IEEE/ACM 43rd International Conference on Software Engineering (ICSE)
1558-1225/21/$31.00 ©2021 IEEE
DOI 10.1109/ICSE43902.2021.00029
TABLE I: Summary of our ﬁndings on conﬁguration design and implementation, and their implications.
FINDINGS ABOUT CONFIGURATION INTERFACE IMPLICATIONS
F.1Software developers often parameterize constant values into I.1Conﬁguration auto-tuning techniques that consider reliability and func-
conﬁguration parameters. Performance and reliability tuning are common tionality are needed, in addition to performance-only optimization. Timing
rationales for such parameterization. parameters are an example (critical to both performance and reliability).
F.2Over 50% of parameterizations were driven by severe consequences I.2Techniques for identifying pathological conﬁguration use cases through
of deﬁciencies in constant values. Unfortunately, use cases that drove the testing and analysis are desired. Tools that can identify and categorize use
parameterization were often poorly discussed or documented. cases could help proactively parameterize deﬁcient constants.
F.3Only 28.1% of default-value changes mentioned systematic testing; I.3Many default values in existing software systems may not be optimal.
31.3% of default changes chose values that work around reported issues. Research on how to better select default values is needed.
FINDINGS ABOUT CONFIGURATION USAGE IMPLICATIONS
F.4Most conﬁguration-checking code were added as afterthoughts, I.4Proactive parameter value checking and validation can prevent many
postmortem to system failures and performance issues in production. severe consequences (but they are still not a common engineering practice).
F.5Over 50% of checks added as afterthoughts are basic (non-emptiness I.5Automated solutions for generating basic checking code and applying
and value-range checks); other commits invoked checking code earlier. them in program early execution phases are useful and feasible.
F.6Throwing exceptions is common for handling misconﬁgurations; I.6Automatically correcting conﬁguration errors is feasible and should be
auto-correction is not, missing opportunities to help users handle errors. explored in future research.
F.7Developers often enhance conﬁguration-related log/exception mess- I.7Techniques on automated enhancement of conﬁguration-related log and
ages by including related parameters and providing guidance. exception messages to improve misconﬁguration diagnosis are needed.
F.8Reusing existing parameters in different program locations is a com- I.8Tools are needed for identifying and ﬁxing various inconsistencies
mon practice. However, parameter reuse leads to various inconsistencies. among conﬁguration parameters and their code implementations.
FINDINGS ABOUT CONFIGURATION DOCUMENTATION IMPLICATIONS
F.9Inadequate and outdated information are major reasons behind I.9Enforcing complete, up-to-date documentation of conﬁguration info-
the changes that enhance conﬁguration documents. mation is still a challenge (despite a lot of research effort).
F.10 Conﬁguration use cases, parameter constraints and dependencies I.10 Conﬁguration documentation should be systematically augmented
between parameters are commonly added to documents. to include critical, user-facing information.
systems, towards ﬁlling the knowledge gap and better un-
derstanding the needs that conﬁguration engineering must
meet. Speciﬁcally, we study 1178 conﬁguration-related com-
mits spanning 2.5 years (2017.6–2019.12) in four large-scale,
widely-used, and actively-maintained open-source cloud sys-
tems (HDFS, HBase, Spark, and Cassandra). Each commit that
we study is associated with a JIRA/GitHub issue or a Pull
Request link which provides more context about the change
and the discussions among developers. (Section III describes
our methodology for selecting conﬁguration-related commits).
Our goal is to understand current conﬁguration engineering
practices, identify developer pain points, and highlight future
research opportunities. We focus on analyzing commits that
revise or reﬁne initial conﬁguration design or implementation
decisions, instead of commits that add or remove parameters
as code evolves. These revisions or reﬁnements were driven
by consequences of misconﬁgurations. Our analysis helps to
1) understand the rationale for the changes, 2) learn design
lessons and engineering principles, and 3) motivate future
automated solutions that can prevent such consequences.
To systematically analyze conﬁguration-related commits,
we propose a taxonomy of conﬁguration design and imple-
mentation changes in cloud systems along three dimensions:
1) interface: why and how developers change the conﬁgu-
ration interface (parameters, default values and constraints).
2) usage: how developers change and improve parameter value
checking, error-handling, and uses. 3) documentation: how
developers improve conﬁguration documentation.
Note that this paper focuses on cloud systems, instead
of desktop software or mobile apps, because misconﬁguring
cloud systems results in more far-reaching impact. Moreover,
we focus on runtime conﬁgurations [34] whose values can be
changed post-deployment without re-compiling the software.Runtime conﬁgurations fundamentally differ from compile-
time conﬁgurations such as #ifdef -based feature ﬂags [35].
But runtime and compile-time conﬁgurations have similar
problems. So, there are opportunities to extend techniques that
solve problems for one to the other.
This paper makes the following contributions:
?Study and Insights. We study code changes to understand
the evolution of conﬁguration design and implementation
in cloud systems. We ﬁnd insights that motivate future
research on reducing misconﬁgurations in these systems.
?Taxonomy. We develop a taxonomy of cloud system
conﬁguration design and implementation evolution.
?Data. We release our dataset and scripts at “https://github.
com/xlab-uiuc/open-cevo” to help followup research. A
detailed replication package can be found in [36].
Table I summarizes our ﬁndings and their implications.
II. T AXONOMY
Figure 1 shows the three parts of our taxonomy of cloud-
system conﬁguration engineering: interface ,usage , and docu-
mentation . We focus on aspects that affect how system users
interact with conﬁgurations, and not on developer-focused
UseHandlingFeedbackPASSCheckFAILCORRECTIONTHROWEXCEPTIONParseUSAGEDOCUMENTATIONINTERFACEValue TypeValue RangeDependence…NameDefault ValueConstraintsP1P2P3Pn…User manualsCode CommentsSTRUCTUREDENTRIESTUTORIALS /GUIDES/*...*/
Fig. 1: Three parts of our taxonomy of software conﬁguration design
and implementation, and their components.
189TABLE II: Our taxonomy of conﬁguration engineering evolution.
INTERFACE (S ECTION IV)
AddParam Add new conﬁguration parameters
Add NewCode Add new parameters when introducing new modules
Add CodeChange Add new parameters due to changes in existing code
Add Parameterization Convert constant values to conﬁguration parameters
RemoveParam Remove existing conﬁguration parameters
Rmv RmvModule Remove parameters when removing existing modules
Rmv Replace Replace parameters with constants or automation
ModifyParam Modify existing conﬁguration parameters
Mod Naming Change the name of a conﬁguration parameter
Mod DefaultValue Change the default value of a conﬁguration parameter
Mod Constraint Change the constraints of a conﬁguration parameter
USAGE (S ECTION V)
Parse Change conﬁguration parsing code
Check Change conﬁguration checking code
Handle Change conﬁguration error handling code
Handle Action Handling actions (correction and exceptions)
Handle Message Feedback messages (log and exception messages)
Use Change how conﬁguration values are used
Use Change Change existing code that uses parameters
Use Add Add code to reuse a conﬁguration parameter
DOCUMENTATION (S ECTION VI)
User manual Change conﬁguration-related user manual content
Code comment Change conﬁguration-related source code comments
aspects like variability and testability. We organize our study
along the categories shown in Table II.
Interface. The conﬁguration interface that a system exposes to
users consists primarily of conﬁguration parameters (parame-
ters, for short). As shown in Fig. 1, a parameter is identiﬁed
by a name and it typically has a default value . Users can cus-
tomize system conﬁguration by changing parameter values in
conﬁguration ﬁles or by using command line interfaces (CLIs).
Each parameter places constraints (correctness rules) on its
values, e.g., type, range, dependency on other parameters.
Values that violate the constraints lead to misconﬁgurations.
In Table II, changes that contribute to conﬁguration interface
evolution include adding parameters, removing parameters,
and modifying parameters.
Usage. Fig. 1 presents the conﬁguration usage model. To use
a parameter, the software program ﬁrst reads its value from a
conﬁguration ﬁle or CLI, parses the value and stores it in a
program variable. The variable is then used when the program
executes. In principle, the program checks the value against
the parameter’s constraints before using it. If checks fail, the
program needs to handle the error and provide user with
feedback messages . In Table II, conﬁguration usage evolution
consists of changes to all parts of the usage model.
Documentation. These are natural language descriptions re-
lated to conﬁgurations. We consider changes to user manuals
and code comments—the former are written for system users
while the latter are written for developers.
III. S TUDY SETUP
To understand how conﬁguration design/implementation
evolve, we identiﬁed and analyzed conﬁguration-related com-
mits that modify conﬁguration design and implementation.
Following [34], we refer to the design, implementation, andTABLE III: Software and their commits that we study.
SUBJECT #DESCRIPTION #PARAMS #ALLC #S TUDIED C
HDFS File system 560 1618 221
HBase Database 218 3516 268
Spark Data processing 442 6194 602
Cassandra Database 220 1868 87
maintenance of software conﬁguration as conﬁguration en-
gineering . We start from commits instead of bug databases
(e.g., JIRA and GitHub issues) because conﬁguration design
and implementation evolution is not limited to bug ﬁxing.
All cloud systems that we study record related issue or Pull
Request ID(s) in commit messages (Section III-A). We found
detailed context about changes in the conﬁguration-related
commits through developer discussions. Moreover, commits
allow us to analyze the “ diffs”—the actual changes.
A. Target Software and Version Histories
We study conﬁguration design and implementation in four
open-source cloud systems, shown in Table III: HDFS, HBase,
Spark, and Cassandra. These projects 1) have many conﬁgura-
tion parameters and conﬁguration-related commits, 2) are ma-
ture, actively-developed and widely-used, with well-organized
GitHub repositories and bug databases, (3) link to issue IDs
in commit messages, and 4) are commonly used subjects in
cloud and datacenter systems research.
In these subjects, we studied conﬁguration-related commits
from June 2017 to December 2019, a 2.5–year time span.
In Table III, “ #PARAMS ” is the total number of documented
parameters in the most recent version, “ #ALLC” is the total
number of commits in the 2.5–year span, and “ #STUDIED C”
is the number of conﬁguration-related commits that we stud-
ied. We excluded conﬁguration-related commits that only
added or modiﬁed test cases; we expected such commits
to yield less insights on design/implementation evolution. In
total, we studied 1178 conﬁguration-related commits.
B. Data Collection and Analysis
To ﬁnd conﬁguration-related commits within our chosen
time span, we wrote scripts to automate the analysis of
commit messages and diffs, ﬁlter out irrelevant commits,
and select likely conﬁguration-related commits. Then, we
manually inspected each resulting commit and its associated
issue. Overall, we collected 384 commits by analyzing commit
messages and 794 commits by analyzing the commit diffs,
yielding a total of 1178 conﬁguration-related commits.
1) Analysis of Commit Messages: Keyword search on com-
mit messages is commonly used to ﬁnd related commits,
e.g., [37]–[42]. We manually performed a formative study with
hundreds of commit messages and found that three strings
commonly occur in conﬁguration-related commits: “conﬁg”,
“parameter” and “option”. These strings were previously used
in keyword searches [34], [37], and matched 525 times in all
four subjects. We manually inspected these 525 commits and
removed commits that did not change conﬁgurations, yielding
384 conﬁguration-related commits.
1902) Analysis of Commit Diffs: Many commit messages do
not match during keyword search, even though the diffs
show conﬁguration-related changes. So, we further analyzed
diffs to ﬁnd more conﬁguration-related commits, and found
additional 794 conﬁguration-related commits. Our diff analysis
determines whether diffs modify how parameters are deﬁned,
loaded, used, or described. Accurate automated diff analysis
requires applying precise taint tracking—treating parameter
values as initial taints that are propagated along control- and
data-ﬂow paths [13], [16]–[18], [20], [26], [37], [43]–[45]—
to each commit and its predecessor and comparing the taint
results in both commits. Such pairwise analysis does not scale
well to the 13196 commits in all four projects (Table III).
To scale diff analysis, we used a simple text-based search
of conﬁguration metadata, including the 1) conﬁguration in-
terface (including how conﬁgurations are deﬁned and loaded),
2) default conﬁguration ﬁle, and 3) message that contains con-
ﬁguration information. Metadata are expected to be stable in
the mature cloud systems that we study; commits that modify
them may yield good insights on conﬁguration evolution.
Finding commits that change parameter deﬁnitions. We
start from commits that change default conﬁguration ﬁles or
parameter descriptions in those ﬁles. These two locations are
key user-facing parts of conﬁguration design [13], [46]. Thus,
modiﬁcation of parameters (introduction, deprecation, changes
to default values, etc.) likely requires changes to either. This
heuristic was effective: it found 272 additional conﬁguration-
related commits with an average false positive rate of 3.2%.
Finding commits that change parameter loading or setting.
Here, we leverage knowledge of conﬁguration APIs. As re-
ported in prior studies [13], [16], [43]–[45], [47] and validated
in our study, mature software projects have uniﬁed, well-
deﬁned APIs for retrieving and assigning parameter values.
For instance, HDFS has getter or setter methods (e.g., getInt ,
getBoolean , declared in a Java class; each of which has a
corresponding setter method (e.g., setInt ,setBoolean ). The
other evaluation subjects follow this pattern.1So, identifying
commits that changed code containing getter or setter method
usage requires a few lines of code using regular expressions.
This heuristic found 457 additional conﬁguration-related com-
mits with a 19.9% average false positive rate.
Finding commits that change parameter value data ﬂow.
If a commit changes code with variables that store parameter
values, then that commit is likely related to the data ﬂow
of parameter values. We implemented a simple text-based
taint tracking to track such variables as follows. Once a
conﬁguration value is stored in a variable, we add the variable
name to a global taint set. We perform the tracking for every
commit in the time span that we studied. We do not remove
variables from our taint set. We output candidate commits
where a modiﬁed statement contains a variable name in the
1This is common in Java and Scala projects: the conﬁguration interface
typically wraps around core library APIs such as java.util.Properties
to provide conﬁguration getter and setter methods.TABLE IV: Conﬁguration-related commits by category. Some com-
mits contain changes in multiple categories.
INTERFACE BEHAVIOR DOCUMENT COMMIT
HDFS 139 (62.9%) 58 (26.2%) 27 (12.2%) 221
HBase 171 (63.8%) 87 (32.5%) 21 (7.8%) 268
Spark 367 (61.0%) 182 (30.2%) 61 (10.1%) 602
Cassandra 54 (62.1%) 32 (36.8%) 5 (5.7%) 87
Total 731 (62.1%) 359 (30.5%) 114 (9.7%) 1178
taint set. Taint tracking found 31 additional conﬁguration-
related commits with an average false positive rate of 26.2%.
Identifying other conﬁguration-related commits We applied
the same keyword search on commit messages (Section III-B1)
to messages that occur in diffs, to capture commits that change
related exception or log messages without modifying any other
code. We found 34 additional conﬁguration-related commits
with an average false positive rate of 29.2%.
3) Inspection and Categorization: At least two authors
independently studied each conﬁguration-related commit and
its corresponding issue. They independently categorized each
commit based on the taxonomy in Section II, and then met
to compare their categorization. When they diverged, a third
author was consulted for additional discussion until consensus
was reached. Further, in twice-weekly project meetings, the
inspectors met with a fourth author to review their catego-
rization of 15% of commits inspected during the week. These
meetings helped check that understanding of the taxonomy is
consistent. Our experience shows that consistently checking a
taxonomy like Figure 1 with concrete examples signiﬁcantly
improves inter-rater reliability and categorization efﬁciency.
Note that we categorized each commit based on how it
revised the original conﬁguration design/implementation. If
a commit adds a new parameter and also a manual entry
to document this new parameter, we treat this commit as
Add Param (Table II)—the commit revises the conﬁguration
interface instead of documentation. Some commits modify
multiple (sub-)parts in our taxonomy.
4) Data Collection Results: Table IV shows the studied
conﬁguration-related commits along the three parts of our
taxonomy. There is a signiﬁcant number of commits in each
part. The rest of this paper summarizes our analysis and pro-
vides insights on how conﬁguration design and implementation
evolve along these three parts.
IV. C ONFIGURATION INTERFACE EVOLUTION
Changes to the conﬁguration interface were the most
common, compared with behavior or documentation changes
(Table IV). We focus on analyzing changes to conﬁg-
urability —the level of user-facing conﬁguration ﬂexibility—
(Section IV-A) and default values (Section IV-B). We omit
other kinds of conﬁguration interface changes which are often
routine and cannot directly lead to misconﬁgurations.
A. Evolution of User-Facing Conﬁguration
Table V shows our categorization of changes to conﬁgura-
bility. Most changes add or remove parameters; per project,
191TABLE V: Statistics on conﬁguration interface changes.
HDFS HB ASE SPARK CASSANDRA TOTAL
AddP aram 106 122 277 42 547
Add NewCode 54 55 143 23 275
Add CodeChange 16 34 72 8 130
Add Parameterization 36 33 62 11 142
Remo veParam 5 24 30 6 65
Rmv RmvModule 3 16 25 4 48
Rmv Replace 2 8 5 2 17
ModifyP aram 28 25 60 6 119
Mod Naming 5 8 30 1 44
Mod DefaultValue 19 14 20 3 56
Mod Constraint 4 3 10 2 19
removal is 5.1 to 21.2less frequent than addition (with an
average of 8.4 ). We ﬁnd that adding or removing parameters
occur naturally during software evolution—parameters are
added with new code, and removed with code deletion. We
do not focus on co-addition or co-removal of parameters with
code. Rather, we focus on changes that revise previous conﬁg-
uration engineering decisions by 1) parameterizing constants
and 2) eliminating parameters or converting them to constants.
Our data corroborates a prior ﬁnding [12] that conﬁguration
interface complexity increases rapidly over time, as more
parameters are added than are removed. Complexity measures
the size of the conﬁguration space (number of parameters
multiplied by the number of all their possible values). Ap-
proaches for dealing with the rapid growth rate are desired.
Variability modeling [48]–[52] which is extensively researched
for compile-time conﬁgurations, can potentially be extended
to understand and manage runtime conﬁguration complexity.
1) Parameterization: Developers often convert constants
into parameters after discovering that one constant cannot sat-
isfy all use cases. We ﬁnd 142 commits that parameterize 169
constants (169 parameterizations). We report on 1) rationales
for the parameterizations, 2) how developers identify constants
to parameterize, 3) use cases that made constants insufﬁcient,
and 4) how developers balance increase in conﬁguration
complexity (caused by adding new parameters) with the need
for ﬂexibility (which necessitates parameterization). Our re-
sults have ramiﬁcations for conﬁguration interface design: we
provide understanding for managing the conﬁgurability versus
simplicity tradeoff. The rationales for parameterization also
motivate conﬁguration parameter auto-tuning.
Rationales for parameterization. These include: perfor-
mance tuning, reliability, environment setup, manageability,
debugging, compatibility, testability, and security. Table VI
shows, for each rationale, the number of commits and param-
eters, an example parameter, and a description. We discuss the
top two rationales, due to space limits. Performance tuning was
the top rationale for parameterizing constants, involving 39.6%
(67/169) of parameters in 56 commits. Different workloads
need different values, so it is hard to ﬁnd one-size-ﬁts-all con-
stants. Resource-related (e.g., buffer size and thread number),
feature selection (turning on/off features with performance
impact, e.g., monitoring), and timing logic (mostly timeouts
and intervals) were the main resulting parameter types, with20.9% (14/67), 37.3% (25/67), and 14.9% (10/67) new param-
eters, respectively. Others 26.9% (18/67) set algorithm-speciﬁc
parameters (e.g., weights and sample sizes).
Reliability, with 37 of 169 of the parameterizations, was
the second most common rationale. Of these, 17 were caused
by hardcoded timeout values that led to constant request
failures in the reported deployments, so developers made them
conﬁgurable. Note that new timing parameters were created
for both reliability and performance tuning. For example,
in HDFS, a new timing parameter was created to improve
performance. The previous constant was causing a “ delete
ﬁle task to wait for... too long ” (HBase-20401). But, another
HDFS timing parameter was created to improve reliability.
The previous constant was too small, causing “ timeouts while
creating 3TB volume ” (HDFS-12210).
DISCUSSION :Conﬁguration auto-tuning techniques
that consider reliability and functionality are needed, in
addition to performance-only optimization [53]–[71].
Speciﬁcally, timing parameters have important impli-
cations to both reliability and performance; however,
not much work has been done on auto-tuning timing
parameters (e.g., timeouts and intervals).
How de velopers ﬁnd constants to parameterize. 54.4%
(92/169) of parameterizations were postmortem to se vere
consequences, e.g., system f ailures, performance degradation,
resource o veruse, and incorrect results. Among previous con-
stants for these, 40.2% (37/92) led to performance degradation;
35.9% (33/92) caused se vere failures; 19.6% (18/92) led to
incorrect or unexpected results ( e.g., data loss and wrong
output); and 4.3% (4/92) resulted in resource o veruse.
DISCUSSION :Despite the efforts in parameterization,
developers still overlook deﬁcient constants that may
lead to severe consequences (e.g., failures and perfor-
mance issues). Proactive techniques for detecting de-
ﬁcient constants and for automating parameterization
are needed; the latter could assist performance testing
of cloud systems.
Describing use cases that pr ompt parameterization. Use
cases where constants were deﬁcient should be described fully
to help users set correct v alues. But, developers described
theconcrete use cases that prompt parameterization for only
37.9% (64/169) parameters. Others discussed use cases either
vaguely (45.0% or 76/169 parame ters) or provided no informa-
tion(17.1% or 29/169 parameters) . Table VII shows examples.
DISCUSSION :Future work should identify and docu-
ment use cases and workloads, including which param-
eters can be tuned, and suggest beneﬁcial conﬁguration
values that are designed for concrete use cases.
192TABLE VI: Statistics and examples of developers’ rationales for parameterization (excluding two commits that lacks information).
RATIONALE #COMMIT #PARAM EXAMPLE NEWPARAMETER LIMITATION OF PREVIOUS CONSTANT
Performance 56 67 spark.sql.codegen.cache.maxEntries The cache size does not work for online stream processing (Spark-24727)
Reliability 28 37 spark.sql.broadcastExchange.maxThreadThreshold Out of memory if thread-object garbage collection is too slow (Spark-26601)
Manageability 20 20 dfs.federation.router.default.nameservice.enable Enable the default name service to store ﬁles (HDFS-13857)
Debugging 8 9 spark.kubernetes.deleteExecutors Disable auto-deletion of pods for debugging and diagnosis (Spark-25515)
Environment 8 13 dfs.cblock.iscsi.advertised.ip Allows server and target addresses to be different (HDFS-13018)
Compatibility 13 13 spark.network.remoteReadNioBufferConversion Add the parameter to fall back to an old code path (Spark-24307)
Testability 3 4 spark.security.credentials.renewalRatio May not need to be set in production but can make testing easier (Spark-23361)
Security 4 4 spark.sql.redaction.string.regex The output of query explanation can contain sensitive information (Spark-22791)
TABLE VII: Use-case description of parameterization changes.
LEVEL EXAMPLE
Concrete “Volume creation times out while creating 3TB volume ” (HDFS-12210).
Vague “ Ifmany regions on a RegionServer, the default will be not enough ” (HBase-21764).
NoInfo “ Itwould be better if the user has the option instead of a constant ” (Spark-25233).
Balancing ﬂexibility and simplicity Conﬁguration interface
design must balance ﬂexibility (i.e., conﬁgurability) with
simplicity [12]. New parameters increase ﬂexibility (by han-
dling additional use cases), but increase interface complexity
(thus reducing usability). 16.2% (23/142) of parameterization
commits contained developer discussions on the ﬂexibility-
simplicity tradeoff. Most discussed estimated prevalence of use
cases for parameterization—it is not worth increasing interface
complexity for rare use cases—and typically involve advanced
users, e.g., “ admittedly, this...is an expert-level setting, useful
in some cases ” (Cassandra-14580). We also found developers’
debates on whether to parameterize (Cassandra-12526, HDFS-
12496, Spark-26118).
Amiddle-ground solution is to parameterize without docu-
menting or exposing the parameter, e.g., “ although...not widely
used, I could see allowing control...via an undocumented
parameter ” (Spark-23820). With this practice, not all but the
most advanced users know of such parameters. We found
that 58.0% (98/169) of the newly added parameters were not
documented in the parameterization commit, indicating that
these parameters were ﬁrst added as middle-ground solutions.
DISCUSSION :Further studies are needed on 1) if and
why undocumented parameters are eventually docu-
mented, and 2) how often and why (expert) users
modify un-exposed parameters, in order to understand
the intent and utility of such parameters.
Speciﬁcally , visibility conditions from variability model-
ing [50], [51], [72] can be extended to manage the tradeof f
of ﬂe xibility v ersus simplicity , which can beneﬁt navig ation
support and user guidance [73]–[76]. Currently , visibility
conditions are mainly designed for Boolean feature ﬂags based
on dependenc y speciﬁcations (e.g., in CDL and KConﬁg);
complexity metrics and variability analysis for other parameter
types (e.g., numeric and strings) are needed.2) Remo ving P arameter s:Understanding parameter re-
moval can yield insights on reducing conﬁguration interf ace
complexity [12], [35]. We examined all 17 conﬁguration-
related commits that remo ved a parameter (not co-remo val
with code). All 17 remov ed parameters were con verted to
constants or code logic was added that obviated them. 14
remov ed parameters were con verted to constants: 11 to their
default values and 3 to safe v alues. De velopers mentioned that
8 ofthe 17 parameters had no clear use case (e.g., HBase-8518,
HBase-18786), or required users to understand implementation
details (e.g., Cassandra-14108). Three parameters confused
users or might lead to se vere errors (e.g., Spark-26362).
Three of 17 remo ved parameters were obviated by ne w
automation logic. For example, in HBase-21228, hbase.r
egionserver.handler.countwhich speciﬁed the number
of concurrently updating threads to be garbage collected in
a JavaConcurrentHashMap , was remo ved after de velopers
switched to ThreadLocal<SyncFuture>which automatically
garbage collects terminated threads. This example shows how
implementation choices could af fect conﬁguration complexity .
DISCUSSION :Future studies can evaluate the utility
and impact of each parameter (e.g., by analyzing if
and how often deployed values are equal or similar to
the default values). Conﬁgurations with low utility can
be replaced with constants (e.g., default values).
B.Evolution of Default V alues
Parameter default values are important to the usability of
conﬁgurable systems; they provide users with good starting
points for setting parameters without needing to understand
the entire conﬁguration space. Thus, de velopers usually choose
default values that satisfy common use cases. Ideally , ade-
fault value applies under most common workloads, without
causing f ailures (HBase-16417, HBase-20390, HDFS-11998).
The “Mod DefaultValue ” row in Table V sho ws 56 commits that
changed 81 default values. Wediscuss wh y def ault v alues
changed and how new default values were chosen.
Reasons f or changing default v alues. Weobserve proactiv e
and reacti vedefault value changes. 38.3% (31/81) def ault-
value changes were proactiv e, incl uding 1) enabling a pre-
viously disabled feature ﬂag (32.3% (10/31) ), e.g., “running
193the feature in production for a while with no issues, so
enabled the feature by default ” (HDFS-7964), 2) performance
reasons (35.4%, 11/31), e.g., “ sets properties at values yielding
optimal performance ” (HBase-16417), and 3) supporting new
use cases (32.3%, 10/31), e.g., “it may be a common use case
to ...list queries on these values ” (Cassandra-14498).
The remaining 61.7% (50/81) of default value changes
were reactive to user-reported issues, including 1) system
failures and performance anomalies due to not supporting new
workloads, deployment scale, hardware, etc (50.0%, 25/50),
2) inconsistencies with user manual (38.0%, 19/50), and 3)
working around software bugs (12.0%, 6/50), e.g., “we set
the parameter to false by default for Spark 2.3 and re-enable
it after addressing the lock congestion issue” (Spark-23310).
Choosing new values. It is straightforward to change new
default values for Boolean and enumerative parameters, given
their small value ranges. So, we describe how new default
values of 32 numeric parameters were chosen (excluding those
that ﬁx default value inconsistency (e.g., HBase-18662). Only
28.1% (9/32) numeric parameters had systematic performance
testing and benchmarking mentioned in the JIRA/GitHub
issues. Later commits reset these new default values, despite
the initial testing and benchmarking. For example, HBase
developers performed “ write-only workload evaluation...read
performance in read-write workloads. We investigate several
settings... ” (HBase-16417). Yet, we found three later commits
that changed the default value of the same parameter to differ-
ent numbers. For 31.3% (10/32) of numeric parameters, new
default values were chosen by adjusting the previous default
values to resolve production failures. In many of these cases,
usually without high conﬁdence in the new default values,
developers simply chose values that resolve the problem(s).
Examples: “ It probably makes sense to set it to something
lower ” (Spark-24297), or “ I’m thinking something like 3000
or 5000 would be safer ” (HBase-18023). We found no infor-
mation on the remaining 40.6% (13/32) numeric parameters.
We observe that backward compatibility and safety are
common considerations in selecting new default values. New
default values that radically change system behavior are often
considered inappropriate (e.g., HBase-18662).
DISCUSSION :Default value changes are often reactive
to the reported issues, without systematic assessment.
Systematic testing and evaluation of new (and existing)
default values are needed.
Dynamic workloads and heterogenous deployments neces-
sitate continuous and incremental changes to default values.
Future w ork could maintain a set of default values (instead of
one) for typical w orkloads, hardw are, and scale.
C. Summary
There is an unmet need for practical conﬁguration automa-
tion te chniques and tools for choosing and testing parameter
values—wh y do cloud system developers still change param-
eter v alues statically rather than using parameter automation?TABLE VIII: Statistics on conﬁguration usage e volution.
HDFS HB ASESPARK CASSANDRA TOTAL
Parse 5 14 59 7 85
Check 7 20 29 11 67
Handle 12 18 20 2 52
Handle Action 8 6 4 1 19
Handle Message 4 12 16 1 33
Use 34 35 74 12 155
Use Change 7 10 25 3 45
Use Add 27 25 49 9 110
There is also need for automatic ways of identifying workloads
or use cases for which def ault v alues (and even constants) are
ill-suited. Such automatic w orkload identiﬁcation approaches
can help developers to better 1) decide which constant v alues
need to be parameterized, 2) understand when their current
default values will lead to sys tem f ailures, and 3) come up
with better tests and benchmarks for default values.
V.CONFIGURATIONUSAGEEVOLUTION
Wepresent results on conﬁguration usage e volution (recall
the conﬁguration usage model described in Fig. 1 and Sec-
tion II). Across the four cloud systems, 26.2%–36.8% of
commits changed parameter usage (Table VIII). W e describe
changes to checking, error handing, and use of parameters. W e
omit changes to parsing APIs (e.g., Spark-23207).
A. Evolution of Parameter Checking Code
Proactiv ely checking parameter v alues is key to preventing
misconﬁgurations [16]. Howe ver, we ﬁnd that many parame-
ters had nochecking code when the y were introduced. Check-
ing code was added reactively : 1)74.6% (50/67) of commits
that changed checking code occurred after users reported
runtime f ailures, service unav ailability, incorrect/unexpected
results, startup failures, etc. (T able IX shows examples).
2) In 14.9% (10/67) commits, de velopers proactively added
or impro vedthe checking code; 2 of them applied reacti vely-
added checking code to other parameters with similar types
(e.g., Cassandra-13622). 3) W e did not ﬁnd sufﬁcient infor-
mation of the other 7 commits .
1) Adding new checking code: 79 ne w checks were added
in 83.6% (56/67) of checking-code related commits. 87.3%
(69/79) of these ne w checks were for speciﬁc parameters,
while the others were applied to groups of conﬁguration pa-
rameters (e.g., read-only parameters ). Surprisingly , for speciﬁc
parameter checks (69 checks in 46 commits), 58.0% (40/69)
were basic checks: NOT-NULL (20/69), v alue range (15/69) and
deprecation checks (5/69). An example is in Fig. 2(a). Majority
of ne w checking code were added reacti vely, corroborating
that simple checks can prevent man y severe failures [16],
[77]. More of such checks could be automatically added and
invoked at system startup. The other 29 checks were more
complex: 9 value semantic checks (e.g., ﬁle/URI properties
and data alignment, Fig. 2(b)), 2) 13 checks for parameter
dependencies (e.g., Fig. 2(c)), and 3) 7 checks for e xecution
context (e.g., Fig. 2(d)).
194TABLE IX: Examples of consequences that can be prevented by adding conﬁguration checking code.
CONSEQUENCE EXAMPLE PARAMETER DESCRIPTION
Runtime Error hbase.bucketcache.bucket.sizes If value is not aligned with 256, instantiating a bucket cache throws IOException (HBase-16993)
Early Termination commitlog_segment_size_in_mb If value2048, Cassandra throws an exception when creating commit logs (Cassandra-13565)
Service unavailability spark.dynamicAllocation.enabled Running barrier stage with dynamic resource allocation may cause deadlocks (Spark-24954)
Unexpected Results spark.sql.shufﬂe.partitions If the value is 0, the result of a table join will be an empty table (Spark-24783)
1+if( writeTables == null || writeTables . isEmpty ()) {
2+ throw new IllegalArgumentException (
3+ " Configurtion parameter " +
4+ OUTPUT_TABLE_NAME_CONF_KEY + " cannot be empty " )}
(a) Add a NOT-NULL check (HBase-18161)
1+if ( bucketSize % 256 != 0) {
2+ throw new IllegalArgumentException (
3+ " Illegal value :" + bucketSize +
4+ " configured for " + BUCKET_CACHE_BUCKETS_KEY +
5+ " All bucket sizes to be multiples of 256 " )}
(b) Add a semantic check (HBase-16993)
1+require ( conf . getOption ( authKey ). isEmpty ()
2+ || ! restServerEnabled ,
3+ s " The RestSubmissionServer does not " +
4+ " support authentication via ${ authKey }." +
5+ " Either turn off spark . master . rest . enabled " +
6+ "or do not use authentication ." )
(c) Add a check for parameters dependency (Spark-25088)
1+if( rdd . isBarrier () &&
2+ Utils . isDynamicAllocationEnabled (sc. getConf )) {
3+ throw new SparkException (
4+ " Barrier execution mode does not support "
5+ " dynamic resource allocation for now . You can "
6+ " disable dynamic resource allocation : setting "
7+ " spark . dynamicAllocation . enabled to false " )}
(d) Add a check for execution context (Spark-24954)
Fig. 2: Examples of conﬁguration checking code.
2) Improving existing checking code: 11 commits improved
existing checking code: eight made checks more strict, e.g., a
NOT-NULL check was improved to “ only allow table replication
for sync replication ” (HBase-19935), and three moved check-
ing code to be invoked earlier instead of during subsequent
execution, e.g., “ when starting task scheduler, spark.task.cpus
should be checked ” (Spark-27192).
DISCUSSION :Checks for parameter values are often
added as afterthoughts. Proactively generating check-
ing code can help prevent failures due to misconﬁgu-
rations.
Two poss ible directions are automatically learning checking
code (we ﬁnd that newly-added checking code is often simple)
and automatically applying checking code for one parame-
ter to other parameters both in the same software (which
developers are already doing manually) and across softw are
projects. A direction is to co-learning checking code and
usage code. Techniques for extracting complex constraints and
speciﬁcations can reduce manual effort for reasoni ng about
and implementing checking code. A fe w recent works show
promise for inferring parameter constraints through analysis ofsource code and documentation [13], [78]–[80]. Techniques for
extracting feature constraints could be extended and applied
to runtime conﬁgurations [81]–[84].
B. Evolution of Err or-Handling Code
Wediscuss changes to misconﬁguration-related exception-
handling code and to messages that provide user feedback.
1) Chang es to conﬁguration error handling: 19 commits
dealt with error handling: 10 added new handling code
totry-catch blocks or throw new exceptions; 9 commits
changed handling code. Among the 9 commits, (1) four
changed misconﬁguration-correction code: three of these
added logic to handle a misconﬁguration, e.g. “ if secr et ﬁle
speciﬁed in httpfs.authentication.signature .secret.ﬁle does not
exist, random secret is generated ” (HDFS-13654) and one
changed b uggy misconﬁguration-correction code to simply log
errors (HDFS-14193) (showing that auto-correcting misconﬁg-
urations is not al ways easy), (2) three changed the e xception
type as it was “danger ous to throw statements whose exception
class does notaccurately describe why they arethrown...since
it mak es corr ectly handling them c hallenging ” (HDFS-14486),
and (3) two replaced e xception thro wing with logging the
errors and resuming the execution.
Wealso studied the ne wly added handling code in the 79
commits that added ne w checking code in Section V -A1. In
73.4% (58/79) of the cases, the handling code threw runtime
exceptions or logged error messages. The e xpectation is that
users should handle the errors. In the remaining 26.6% (21/79)
cases, de velopers attempted to correct the misconﬁgurations,
e.g., “ it’sdeveloper s’ responsibility to make sure the conﬁgu-
ration don’t break code. ” (Spark-24610). Developers corrected
misconﬁgurations by changing to the closest value in the valid
range ( 11/21), re verting to the default value (3/21), a nd using
the v alue of another parameter with similar semantics ( 7/21).
DISCUSSION :Developers want to make code more
robust in the presence of misconﬁgurations, but their
manual efforts are often ad hoc. There is need for new
techniques for generating misconﬁguration correction
code and improving existing handling code.
Techniques for ﬁxing compile-time conﬁguration errors,
such as range ﬁx es [85], [86], may be applicable for generating
correction strate gies for some types of runtime parameters. A
key challenge is to attribute runtime errors (e.g., e xceptions)
to misconﬁgurations and to rerun the related e xecution with
the corrected conﬁgurations.
195TABLE X: Four levels of message feedback quality in commits that changed exception or logging messages.
LEVEL DESCRIPTION EXAMPLE
L4Contain parameter names and “Barrier execution mode does not support dynamic resource allocation... You can disable dynamic
provide guidance for ﬁxing resource allocation by setting... spark.dynamicAllocation.enabled to false.” (Spark-24954)
L3 Contain parameter names “Failed to create SSL context using server_encryption_options .” (Cassandra-14991)
L2 Do not contain parameter names “This is commonly a result of insufﬁcient YARN conﬁguration.” (HBase-18679)
L1 No mention of conﬁguration “Could not modify concurrent moves thread count.” (HDFS-14258)
2) Changes to feedback messages: Feedback (error log
or exception) messages are important for users to diagnose
and repair misconﬁgurations. We investigated commits that
modiﬁed feedback messages and categorize the level of feed-
back that they provided in Table X, where L4 messages
provide the highest-quality feedback and L1 messages pro-
vide the lowest-quality feedback. Among 33 commits that
modiﬁed messages, 18 enhanced feedback quality by adding
conﬁguration-speciﬁc information. After enhancement, 8 mes-
sages became L3, and 7 became L4. Changes in the other
15 commits improved 1) correctness (9/15)—half changed
imprecise parameter boundary values, e.g., from “no less” to
“greater” (Spark-26564), 2) readability (3/15), such as ﬁxing
typographic errors, 3) the log level (2/15), and 4) security
(1/15), i.e., removing potentially sensitive value.
DISCUSSION :Future work could study the feedback
level in allmessages related to misconﬁguration han-
dling code. If most messages are not L4, then future
work should automatically detect deﬁcient messages
and automatically enhance them to L4.
Moreov er,conﬁguration-related logging isnot as mature
as logging for deb ugging [87]–[92]. Improving conﬁguration-
related l ogging requires logging related parameters, erroneous
values, and, where feasible, possible ﬁxes. Poor-quality feed-
back from tools hinders developers [93] and techniques exist
for dealing with message errors in other domains [94], [95].
C. Evolution of Parameter Value Usag e
Software developers change how existing parameters are
used (“Use Change ” inTable VIII) and reuse e xistin g parameters
for dif ferent purposes (“Use Add” in Table VIII).
1) Changing how e xisting par ameters areused: 45 com-
mits changed parameter usage for the following reasons:
Fine-grained contr ol. In 12/45 commits, de velopers pre-
viously used one parameter for multiple purposes, due to
poor design—“e.g., CompactionCheck er and PeriodicMem-
StoreFlusher execution period ar e bound together ” (HBase-
22596)—or for reuse—e.g., “ arrow .enabled was added... wit h
PySpark... Later , SparkR... was added... controlled by the same
parameter . Suppose users want to share some JVM between
PySpark and SparkR... The y use the optimization for all or
none. ” (Spark-27834). Developers res olved both cate gories by
creating separate parameters for ﬁne-grained control.
Domain/scope. 8/45 commits changed the usage domain
or scope of a parameter. For example, HDFS de veloperschanged a parameter, which w as pre viously only used in the
decommission phase to also be used in the maintenance phase,
so “lots of code can be shar ed” (HDFS-9388).
Parameter overriding 9/45 commits changed parameter over-
ride priority , e.g., “Weneed to support both table-level pa-
rameter s. User s might also use session-level parameter ... the
precedence would be ...” (Spark-21786).
Semantics 6/45 commits changed what a parameter is used
for,e.g., in Spark-21871, de velopers started using spark.sql.
codegen.hugeMethodLimit as the maximum compiled function
size instead ofspark.sql.codegen.maxLinesPerFunction .
Parameter replacement 6/45 commits swapped one parame-
ter for another because the pre vious one wasoutdated or wrong
, e.g., in Spark-24367, a use ofparquet.enable.summary-me
tadatawas replaced with a use ofparquet.summary.metada
ta.level because the former w as deprecated.
Buggy parameter values 4/45 commits changed parameter
values that were buggy , e.g., the v alue of a parameter changed
because, “ user speciﬁed ﬁlters are not applied in Y ARN
mode...we need... user provided ﬁlter s” (Spark-26255).
2) Reusing existing parameter s:Toavoid gro wing the
conﬁguration space unnecessarily, developers som etimes reuse
existing parameters that are similar to their new use case, in-
stead of introducing a new parameter. 110 commits reused 151
existing parameters for different purposes. Ho wever,parameter
reuse comes at a cost. W e ﬁnd two main problems. First,
reusing a parameter and code that it controls can result in
subtle inconsistencies that can lead to bugs or user confusion.
19.2% (29/151) parameter reuses had such inconsistencies.
Second, de velopers often clone existing code to enable reuse.
Wefocus on inconsistencies. Problems of code cloning are the
subjects of other research.
Wemanually check ed for inconsistencies by comparing the
newly-added code in a tar get commit with code that used
the parameter in existing code base. W e found 29 inconsis-
tencies in HDFS (9/29) , HBase (9/29) and Spark (11/29) .
Inconsistencies manifest in di fferent ways. Wecategorized
them based on the sources of inconsistencies during reuse:
1) feedback message (9/29) , e.g., Spark-18061; 2) checking
code (4/29) e.g., HBase-20590; 3) ne w uses of deprecated
parameters (6/29) , e.g., HDFS-12895; 4) default values (3/29),
e.g., HBase-21809; and 5) use statements (7/29), e.g., HBase-
20586. Fig. 3 sho ws e xamples of inconsistencies in reuse of
checking code and use statements, where added lines start with
+. In Fig. 3(a), the ne w parameter usage did not check for
parameter v alue emptiness as the old usage did. In Fig. 3(b),
1961+ String principal = conf . get (
2+ Constants . REST_KERBEROS_PRINCIPAL );
3+ if( principal != null ) {...}
4
5Preconditions . checkArgument ( principalConfig != null
6 && ! principalConfig . isEmpty () ,
7 REST_KERBEROS_PRINCIPAL +
8 " should be set if security is enabled " );
(a) Inconsistent checking (HBase-20590)
1+if( peerConf . get ( " hbase . security . authentication " )
2+ . equals ( " kerberos " )) {...}
3
4isSecurityEnabled = " kerberos " . equalsIgnoreCase (
5 conf . get ( " hbase . security . authentication " ));
6if( isSecurityEnabled ) {...}
(b) Inconsistent parameter usage (HBase-20586)
Fig. 3: Examples of conﬁguration inconsistent reuse.
the new usage of hbase.security.authentication checked
case-insensitive equality; the old usage was case-sensitive.
DISCUSSION :Inconsistencies in parameter usage can
confuse users (the same values are used in different
ways) or lead to bugs. Ideas for detecting bugs as
deviations from similar program behavior [96], [97]
could be starting points for addressing this problem.
D.Summary
Weadvocate that impro ving softw are qualities—resilience,
diagnosability, and consistenc y—should be ﬁrst-class princi-
ples in softw are conﬁguration engineering. W e ﬁnd that e ven
in mature, production-quality cloud systems, checking, error
handling, feedback, and parameter usage are often not de-
signed or implemented in aprincipled manner . More research
effort should be put on enhancing these essential qualities of
conﬁgurable softw are to defend ag ainst misconﬁgurations, in
addition to detection and diagnosis tools that are external to
the cloud system [1], [17]–[28], [30].
VI. C ONFIGURATIONDOCUMENTEVOLUTION
Wevery brieﬂy discuss conﬁguration document evolution:
114 comm its made 149 changes to user manuals or code com-
ments. 100 of these commits changed user manuals and the
rest changed code comments. Wediscuss why conﬁguration
documents were changed and the changed content.
Reasons f or changing conﬁguration documents. The 149
changes to conﬁguration documents resolved ﬁve types of
problems: 1) 63 were inadequate for users to understand pa-
rameters or to set values correctly, e.g., “ users wondered why
spark.sql.shufﬂe .partitions...unchang ed when they chang ed the
conﬁg...worth to explain it in guide doc” (Spark-25245); 2) 29
were outdated after conﬁguration design and implementation
changed (Section IV and Section V); 3) 21 were incorrect,
e.g., “ LazyPer sistFileScrubber will be disabled if ... conﬁgur ed
to zer o. But the document was incorr ect” (HDFS-12987);
4) 17 had readability issues, e.g., “Client rpc timeouts ar e not
easy to understand from documentation ” (HBase-21727); and5) 19 improv ed content, e.g., “ Add thrift scheduling ... conﬁg
to sc heduling docs ” (Spark-20220).
DISCUSSION :Document-as-code techniques can be
applied to eliminate inconsistencies between conﬁgu-
ration documents and conﬁguration design/implemen-
tation.
Content added to enhance documents .Inadequate infor -
mation w as the most c ommon problem resolved by con-
ﬁguration document changes. W e put the 63 changes that
enhanced inadequate documents in six cate gories based
on the content added: 1) 16 changed constraints on pa-
rameter v alues, e.g., “This should be positive and less
than 2048 ” (Cassandra-13622); 2) 10 explained depen-
dence on other parameters, e.g., “ This pr operty works
with dfs.namenode .invalidate .work.pct.per.iter ation ” (HDFS-
12079); 3) 6 changed parameter v alue types and units; 4) 6
changed parameter scope, e.g., “Timeout... is contr olled dif fer-
ently. Use hbase .client.scanner.timeout.period property to set
this timeout ” (HBase-21727), 5) 22 provided use cases and
guidance, e.g., “enabling this will be very helpful if dfs imag e
is lar ge” (HDFS-13884); and 6) 3 warned about deprecation,
e.g., “ this conﬁg will be remo ved i n Spark 3.0” (Spark-25384).
DISCUSSION :Ethnographic studies could help un-
derstand the gaps between documented conﬁguration
information and conﬁguration obstacles faced by users.
Summary Correctness and ef fectiv eness of technical doc-
umentation is a long-lasting problem in softw are engineer -
ing. Conﬁguration documentation is no e xception. Special-
ized techniques for maintaining and impro ving conﬁguration
documentation are needed. F or example, checking for incon-
sistencies between documents and source code [98]–[101]
could help detect defects in conﬁguration-related code or
documents. Also, techniques for auto-generating documents,
especially using structured data, can be applied to generating
per-parameter comments and manual entries [102]–[104].
VII.THREATSTOVALIDITY
Westudied cloud systems. Some of our ﬁndings may not
generalize to other kinds of softw are. W e chose these projects
because the y are widely used, highly conﬁgurable with lots
of parameters, mature, and well maintained. They also ha ve
issue-tracking systems that help us unders tand the context of
conﬁguration-related commits.
Though we selected candidate commits from version con-
trol history , we may ha vemissed some conﬁguration-related
commits due to tw o limitati ons. First, our re gular e xpressions
assume standard coding con ventions and will not match if de-
velopers do not follow these con ventions. Second, our simple,
text-based tainting may miss some changes to the data ﬂow
of variables that store parameter values. Howe ver, aswe men-
tioned in Section III, precise pairwise tainting does not scale
197to all the commits in the range that we studied—we traded off
precision for scalability. All commits selected were manually
inspected and categorized through a rigorous quality-assurance
process (Section III-B3). False positives came mainly from
commits that touched lines containing conﬁguration-related
variables but did not change the conﬁguration.
VIII. R ELATED WORK
A prior study [37] found that software evolution necessi-
tates resetting parameter values and built ConfigSuggester
to identify parameters whose values need to be changed
after a software updates. We study how the conﬁguration
interface and parameter usage change across (a portion of)
version control history to draw insights for better conﬁguration
design and implementation. Sayagh et al. [34] studied software
conﬁguration engineering in practice using interviews, user
surveys, and a literature review. Our work is complementary:
we perform a code -level study of conﬁguration evolution,
which yields new insights.
There have been many studies on misconﬁgurations in a
wide variety of software systems [1], [2], [4]–[11]. Our work
does not focus on detecting misconﬁgurations or diagnosing
failures caused by misconﬁgurations. We focus on current con-
ﬁguration engineering practices, with the goal to understand
how to improve conﬁguration design and implementation.
Recently, a few studies investigated automated techniques
or engineering practices to enhance conﬁguration checking
code [16], diagnosability [105], interface [46], security [14],
[106], [107], conﬁguration data analysis [108], conﬁguration
libraries [109], [110], and correlations or coupling in con-
ﬁguration and code [111]–[113]. Our work corroborates and
complements the aforementioned work from the perspective of
software evolution. Speciﬁcally, our work studies the practices
ofsoftware developers and reveals how software conﬁguration
design and implementation are revised and evolved.
Despite the differences (Appendix A), runtime conﬁgura-
tions share commonalities with compile-time conﬁgurations or
SPL conﬁgurations, such as #ifdef -based feature ﬂags [35].
It is possible that techniques and methodologies designed for
compile-time conﬁgurations, especially feature and variability
modeling [48], [50]–[52], [72], [114], could be adapted for use
with runtime conﬁgurations. Such adaptation needs to address
unique challenges of runtime conﬁguration parameters, such
as dependencies on deployment environments, as well as their
complex data types and misconﬁguration patterns.
Conﬁguration design and implementation have signiﬁcant
implications on software testing and debugging [115]–[120].
For example, introducing new parameters enlarges the conﬁgu-
ration space and thus makes it more costly to comprehensively
test software. In this paper, we focus on understanding how
to improve conﬁguration design and implementation so that
fewer misconﬁgurations occur, and not on software bugs that
can occur under different parameter value combinations.
IX. C ONCLUSIONS
We presented present an evolutionary study of conﬁguration
design and implementation in cloud systems. To the bestof our knowledge, ours is the ﬁrst evolutionary study on
code-level runtime conﬁguration design and implementation
in these systems. We analyze rationales and practices for
revising conﬁguration design and implementation decisions,
especially in response to consequences of misconﬁgurations.
Our study yields several new insights into the conﬁguration
engineering process, and research opportunities for reducing
misconﬁgurations. Our hope is to inspire researchers and
developers to treat conﬁguration engineering as a ﬁrst-class
software engineering endeavor.
APPENDIX A: R UNTIME VERSUS SPL C ONFIGURATION
A very frequent request is to compare runtime conﬁguration
(the type of conﬁguration studied in this paper) with software
product lines (SPL) conﬁguration (often referred to as “feature
ﬂags” or “feature toggles”) and to position the work in the area
of SPL and variability modeling. We explicitly discuss a few
fundamental differences:
First, runtime conﬁgurations are changed by software users
(operators/sysadmins in our context); SPL conﬁgurations are
managed by developers. Since users are unfamiliar with code,
the conﬁguration speciﬁcations become the interfaces (Sec-
tion IV). Moreover, as users are prone to misconﬁgurations,
checking and providing feedback are critical (Section V).
Second, runtime conﬁgurations are implemented differently
than SPL conﬁgurations. Runtime conﬁgurations are mostly
in the form of conﬁguration parameters that load values from
ﬁles or command lines; SPL conﬁgurations are typically in the
form of preprocessors that determine modules to be included
in the released binary.
Third, runtime conﬁgurations of cloud software are changed
frequently (hundreds to thousands of times a day [1], [9],
[113]); SPL conﬁgurations are typically changed with product
release cycles. This higher velocity of runtime conﬁguration
changes increases misconﬁguration occurrences and makes
checking, error handling, and logging critical.
Fourth, runtime conﬁgurations depend on the deployment
environment, including machine resources (e.g., CPU, mem-
ory, and storage), operating systems (e.g., ﬁles, IP addresses,
and ports), and workloads (data size and requests per seconds).
In contrast, SPL conﬁgurations are often determined before
software release or system deployment.
Lastly, runtime conﬁgurations have more complex data
types (e.g., string and numeric) with different error patterns;
SPL conﬁgurations are mostly boolean or enumerative types.
Certainly, ideas in SPL and variability modeling can be
extended and applied to runtime conﬁguration. We have dis-
cussed them in context of our analysis throughout the paper.
ACKNOWLEDGEMENT
We thank Xiangbing Huang, Xudong Sun, Sam Cheng, Jack
Chen, and Darko Marinov for discussions. The research was
mainly conducted when Zhang was a visiting student at UIUC,
supported by China Scholarship Council. Zhang, He, Li, and
Dong were supported in part of National Key R&D Program
of China No. 2017YFB1001802; NSFC No. 61872373 and
61872375. Xu was supported in part of NSF 1816615.
198REFERENCES
[1] C. Tang, T. Kooburat, P. Venkatachalam, A. Chander, Z. Wen,
A. Narayanan, P. Dowell, and R. Karl, “Holistic Conﬁguration Manage-
ment at Facebook,” in SOSP , 2015.
[2] L. A. Barroso, U. Hölzle, and P. Ranganathan, The Datacenter as a
Computer: Designing Warehouse-Scale Machines . 2018.
[3] J. Shieber, “Facebook blames a server conﬁguration change
for yesterday’s outage.” https://techcrunch.com/2019/03/14/
facebook-blames-a-misconﬁgured-server-for-yesterdays-outage/,
2019.
[4] G. Amvrosiadis and M. Bhadkamkar, “Getting Back Up: Understanding
How Enterprise Data Backups Fail,” in USENIX ATC , 2016.
[5] Z. Yin, X. Ma, J. Zheng, Y . Zhou, L. N. Bairavasundaram, and S. Pasu-
pathy, “An Empirical Study on Conﬁguration Errors in Commercial and
Open Source Systems,” in SOSP , 2011.
[6] S. Kendrick, “What takes us down?,” USENIX ;login: , vol. 37, no. 5,
2012.
[7] A. Rabkin and R. Katz, “How Hadoop Clusters Break,” IEEE Software ,
vol. 30, no. 4, 2013.
[8] H. S. Gunawi, M. Hao, R. O. Suminto, A. Laksono, A. D. Satria,
J. Adityatama, and K. J. Eliazar, “Why Does the Cloud Stop Computing?
Lessons from Hundreds of Service Outages,” in SoCC , 2016.
[9] B. Maurer, “Fail at Scale: Reliability in the Face of Rapid Change,”
CACM , vol. 58, no. 11, 2015.
[10] D. Oppenheimer, A. Ganapathi, and D. A. Patterson, “Why Do Internet
Services Fail, and What Can Be Done About It?,” in USITS , 2003.
[11] K. Nagaraja, F. Oliveira, R. Bianchini, R. Martin, and T. Nguyen, “Un-
derstanding and dealing with operator mistakes in internet services,” in
OSDI , 2004.
[12] T. Xu, L. Jin, X. Fan, Y . Zhou, S. Pasupathy, and R. Talwadker, “Hey,
You Have Given Me Too Many Knobs! Understanding and Dealing with
Over-Designed Conﬁguration in System Software,” in FSE, 2015.
[13] T. Xu, J. Zhang, P. Huang, J. Zheng, T. Sheng, D. Yuan, Y . Zhou, and
S. Pasupathy, “Do not blame users for misconﬁgurations,” in SOSP , 2013.
[14] T. Xu, H. M. Naing, L. Lu, and Y . Zhou, “How Do System Administrators
Resolve Access-Denied Issues in the Real World?,” in CHI, 2017.
[15] T. Xu, V . Pandey, and S. Klemmer, “An HCI View of Conﬁguration
Problems,” arXiv:1601.01747 , 2016.
[16] T. Xu, X. Jin, P. Huang, Y . Zhou, S. Lu, L. Jin, and S. Pasupathy, “Early
detection of conﬁguration errors to reduce failure damage,” in OSDI ,
2016.
[17] M. Attariyan and J. Flinn, “Automating Conﬁguration Troubleshooting
with Dynamic Information Flow Analysis,” in OSDI , 2010.
[18] M. Attariyan, M. Chow, and J. Flinn, “X-ray: Automating root-cause
diagnosis of performance anomalies in production software,” in OSDI ,
2012.
[19] M. Attariyan and J. Flinn, “Using Causality to Diagnose Conﬁguration
Bugs,” in USENIX ATC , 2008.
[20] S. Zhang and M. D. Ernst, “Automated Diagnosis of Software Conﬁgu-
ration Errors,” in ICSE , 2013.
[21] J. Zhang, L. Renganarayana, X. Zhang, N. Ge, V . Bala, T. Xu, and
Y . Zhou, “EnCore: Exploiting System Environment and Correlation
Information for Misconﬁguration Detection,” in ASPLOS , 2014.
[22] H. J. Wang, J. C. Platt, Y . Chen, R. Zhang, and Y .-M. Wang, “Automatic
Misconﬁguration Troubleshooting with PeerPressure,” in OSDI , 2004.
[23] Y .-M. Wang, C. Verbowski, J. Dunagan, Y . Chen, H. J. Wang, C. Yuan,
and Z. Zhang, “STRIDER: A Black-box, State-based Approach to
Change and Conﬁguration Management and Support,” in LISA , 2003.
[24] M. Santolucito, E. Zhai, and R. Piskac, “Probabilistic Automated Lan-
guage Learning for Conﬁguration Files,” in CAV, 2016.
[25] M. Santolucito, E. Zhai, R. Dhodapkar, A. Shim, and R. Piskac, “Synthe-
sizing conﬁguration ﬁle speciﬁcations with association rule learning,” in
OOPSLA , 2017.
[26] Z. Dong, A. Andrzejak, and K. Shao, “Practical and Accurate Pinpointing
of Conﬁguration Errors using Static Analysis,” in ICSME , 2015.
[27] P. Huang, W. Bolosky, A. Sigh, and Y . Zhou, “ConfValley: A systematic
conﬁguration validation framework for cloud services,” in EuroSys , 2015.
[28] S. Baset, S. Suneja, N. Bila, O. Tuncer, and C. Isci, “Usable Declarative
Conﬁguration Speciﬁcation and Validation for Applications, Systems,
and Cloud,” in Middleware , 2017.
[29] M. Sayagh, N. Kerzazi, and B. Adams, “On Cross-stack Conﬁguration
Errors,” in ICSE , 2017.
[30] X. Sun, R. Cheng, J. Chen, E. Ang, O. Legunsen, and T. Xu, “Testing
Conﬁguration Changes in Context to Prevent Production Failures,” in
OSDI , 2020.
[31] D. Norman, “Design Rules Based on Analyses of Human Error,” CACM ,
vol. 26, no. 4, 1983.[32] D. Norman, “Design principles for human-computer interfaces,” in CHI,
1983.
[33] R. A. Maxion and R. W. Reeder, “Improving User-Interface Dependabil-
ity through Mitigation of Human Error,” JHCS , vol. 63, no. 1-2, 2005.
[34] M. Sayagh, N. Kerzazi, B. Adams, and F. Petrillo, “Software Con-
ﬁguration Engineering in Practice Interviews, Survey, and Systematic
Literature Review,” TSE, vol. 46, no. 6, 2018.
[35] J. Meinicke, C.-P. Wong, B. Vasilescu, and C. Kästner, “Exploring
Differences and Commonalities between Feature Flags and Conﬁguration
Options,” in ICSE SEIP , 2020.
[36] Y . Zhang, H. He, O. Legunsen, S. Li, W. Dong, and T. Xu, “An Evo-
lutionary Study of Conﬁguration Design and Implementation in Cloud
Systems (with Replication Package),” arXiv:submit/3601478 , 2020.
[37] S. Zhang and M. D. Ernst, “Which Conﬁguration Option Should I
Change?,” in ICSE , 2014.
[38] Q. Luo, F. Hariri, L. Eloussi, and D. Marinov, “An Empirical Analysis of
Flaky Tests,” in FSE, 2014.
[39] J. Bernardo, D. da Costa, and U. Kulesza, “Studying the impact of
adopting continuous integration on the delivery time of pull requests,”
inMSR , 2018.
[40] M. Rigger, S. Marr, B. Adams, and H. Mössenböck, “Understanding
GCC Builtins to Develop Better Tools,” in FSE, 2019.
[41] L. P. Hattori and M. Lanza, “On the nature of commits,” in ASE, 2008.
[42] S. Dutta, O. Legunsen, Z. Huang, and S. Misailovic, “Testing Probabilis-
tic Programming Systems,” in FSE, 2018.
[43] A. Rabkin and R. Katz, “Static Extraction of Program Conﬁguration
Options,” in ICSE , 2011.
[44] A. Rabkin and R. Katz, “Precomputing Possible Conﬁguration Error
Diagnosis,” in ASE, 2011.
[45] M. Lillack, C. Kästner, and E. Bodden, “Tracking Load-time Conﬁgura-
tion Options,” in ASE, 2014.
[46] T. Xu and Y . Zhou, “Systems Approaches to Tackling Conﬁguration
Errors: A Survey,” ACM Surveys , vol. 47, no. 4, 2015.
[47] F. Behrang, M. B. Cohen, and A. Orso, “Users Beware: Preference
Inconsistencies Ahead,” in FSE, 2015.
[48] R. Lotufo, S. She, T. Berger, K. Czarnecki, and A. W ˛ asowski, “Evolution
of the Linux Kernel Variability Model,” in SPLC , 2010.
[49] S. She, R. Lotufo, T. Berger, A. Wasowski, and K. Czarnecki, “The
Variability Model of the Linux Kernel,” in VaMoS , 2010.
[50] D. Nesic, J. Krüger, S. Stanciulescu, and T. Berger, “Principles of Feature
Modeling,” in ESEC/FSE , 2019.
[51] T. Berger, S. She, R. Lotufo, A. Wasowski, and K. Czarnecki, “A Study
of Variability Models and Languages in the Systems Software Domain,”
TSE, vol. 39, no. 12, 2013.
[52] J. Liebig, S. Apel, C. Lengauer, C. Kästner, and M. Schulze, “An
Analysis of the Variability in Forty Preprocessor-based Software Product
Lines,” in ICSE , 2010.
[53] D. V . Aken, A. Pavlo, G. J. Gordon, and B. Zhang, “Automatic database
management system tuning through large-scale machine learning,” in
SIGMOD , 2017.
[54] S. Wang, C. Li, H. Hoffmann, S. Lu, W. Sentosa, and A. I. Kistijantoro,
“Understanding and Auto-Adjusting Performance-Sensitive Conﬁgura-
tions,” in ASPLOS , 2018.
[55] W. Zheng, R. Bianchini, and T. D. Nguyen, “Automatic Conﬁguration of
Internet Services,” in EuroSys , 2007.
[56] Z. Yu, Z. Bei, and X. Qian, “Datasize-aware high dimensional conﬁgura-
tions auto-tuning of in-memory cluster computing,” in ASPLOS , 2018.
[57] V . Nair, T. Menzies, N. Siegmund, and S. Apel, “Using Bad Learners to
Find Good Conﬁgurations,” in FSE, 2017.
[58] N. Siegmund, A. Grebhahn, S. Apel, and C. Kästner, “Performance-
Inﬂuence Models for Highly Conﬁgurable Systems,” in FSE, 2015.
[59] V . Nair, Z. Yu, T. Menzies, N. Siegmund, and S. Apel, “Finding faster
conﬁgurations using ﬂash,” TSE, vol. 46, no. 7, 2018.
[60] C.-J. Hsu, V . Nair, T. Menzies, and V . W. Freeh, “Scout: An Experienced
Guide to Find the Best Cloud Conﬁguration,” arXiv:1803.01296 , 2018.
[61] S. Duan, V . Thummala, and S. Babu, “Tuning Database Conﬁguration
Parameters with iTuned,” in VLDB , 2009.
[62] Y . Zhu, J. Liu, M. Guo, Y . Bao, K. Song, and Z. Liu, “BestConﬁg:
Tapping the Performance Potential of Systems via Conﬁguration Adjust-
ment,” in SoCC , 2017.
[63] P. Jamshidi, N. Siegmund, M. Velez, C. Kästner, A. Patel, and Y . Agarwal,
“Transfer Learning for Performance Modeling of Conﬁgurable Systems:
An Exploratory Analysis,” in ASE, 2017.
[64] P. Jamshidi, M. Velez, C. Kästner, and N. Siegmund, “Learning to Sam-
ple: Exploiting Similarities Across Environments to Learn Performance
Models for Conﬁgurable Systems,” in FSE, 2018.
[65] B. Xi, Z. Liu, M. Raghavachari, C. H. Xia, and L. Zhang, “A smart hill-
climbing algorithm for application server conﬁguration,” in WWW , 2004.
199[66] T. Ye and S. Kalyanaraman, “A Recursive Random Search Algorithm for
Large-Scale Network Parameter Conﬁguration,” in SIGMETRICS , 2003.
[67] H. Herodotou, F. Dong, and S. Babu, “No One (Cluster) Size Fits All:
Automatic Cluster Sizing for Data-intensive Analytics,” in SoCC , 2011.
[68] T. Osogami and T. Itoko, “Finding Probably Better System Conﬁgura-
tions Quickly,” in SIGMETRICS , 2006.
[69] R. Krishna, V . Nair, P. Jamshidi, and T. Menzies, “Whence to Learn?
Transferring Knowledge in Conﬁgurable Systems using BEETLE,”
arXiv:1911.01817 , 2019.
[70] H. Hoffmann, S. Sidiroglou, M. Carbin, S. Misailovic, A. Agarwal, and
M. Rinard, “Dynamic Knobs for Responsive Power-Aware Computing,”
inASPLOS , 2011.
[71] H. Herodotou, H. Lim, G. Luo, N. Borisov, L. Dong, F. B. Cetin, and
S. Babu, “Towards Automatic Optimization of MapReduce Programs,”
inCIDR , 2011.
[72] T. Berger, S. She, R. Lotufo, A. Wasowski, and K. Czarnecki, “Variability
Modeling in the Real: A Perspective from the Operating Systems Do-
main,” in ASE, 2010.
[73] R. Barrett, E. Kandogan, P. P. Maglio, E. Haber, L. A. Takayama,
and M. Prabaker, “Field Studies of Computer System Administrators:
Analysis of System Management Tools and Practices,” in CSCW , 2004.
[74] L. Takayama and E. Kandogan, “Trust as an Underlying Factor of System
Administrator Interface Choice,” in CHI, 2006.
[75] E. M. Haber and J. Bailey, “Design Guidelines for System Administration
Tools Developed through Ethnographic Field Study,” in CHI, 2007.
[76] D. Jin, M. B. Cohen, X. Qu, and B. Robinson, “PrefFinder: Getting the
Right Preference in Conﬁgurable Software Systems,” in ASE, 2014.
[77] D. Yuan, Y . Luo, X. Zhuang, G. Rodrigues, X. Zhao, Y . Zhang, P. Jain,
and M. Stumm, “Simple testing can prevent most critical failures: An
analysis of production failures in distributed data-intensive systems,” in
OSDI , 2014.
[78] Q. Chen, T. Wang, O. Legunsen, S. Li, and T. Xu, “Understanding
and Discovering Software Conﬁguration Dependencies in Cloud and
Datacenter Systems,” in FSE, 2020.
[79] C. Xiang, H. Huang, A. Yoo, Y . Zhou, and S. Pasupathy, “PracExtractor:
Extracting Conﬁguration Good Practices from Manuals to Detect Server
Misconﬁgurations,” in USENIX ATC , 2020.
[80] C. Li, S. Wang, H. Hoffmann, and S. Lu, “Statically Inferring Perfor-
mance Properties of Software Conﬁgurations,” in EuroSys , 2020.
[81] S. Nadi, T. Berger, C. Kästner, and K. Czarnecki, “Where Do Conﬁgura-
tion Constraints Stem from? An Extraction Approach and An Empirical
Study,” TSE, vol. 99, 2015.
[82] S. Nadi, T. Berger, C. Kästner, and K. Czarnecki, “Mining Conﬁguration
Constraints: Static Analyses and Empirical Results,” in ICSE , 2014.
[83] K. C. Kang, S. G. Cohen, J. A. Hess, W. E. Novak, and A. S. Peterson,
“Feature-Oriented Domain Analysis (FODA) Feasibility Study,” Tech.
Rep. CMU/SEI-90-TR-021, SEI, CMU, 1990.
[84] S. She, R. Lotufo, T. Berger, A. W ˛ asowski, and K. Czarnecki, “Reverse
Engineering Feature Models,” in ICSE , 2011.
[85] Y . Xiong, A. Hubaux, S. She, and K. Czarnecki, “Generating Range Fixes
for Software Conﬁguration,” in ICSE , 2012.
[86] Y . Xiong, H. Zhang, A. Hubaux, S. She, J. Wang, and K. Czarnecki,
“Range Fixes: Interactive Error Resolution for Software Conﬁguration,”
TSE, vol. 41, no. 6, 2015.
[87] D. Yuan, S. Park, P. Huang, Y . Liu, M. M. Lee, X. Tang, Y . Zhou,
and S. Savage, “Be Conservative: Enhancing Failure Diagnosis with
Proactive Logging,” in OSDI , 2012.
[88] D. Yuan, S. Park, and Y . Zhou, “Characterising Logging Practices in
Open-Source Software,” in ICSE , 2012.
[89] D. Yuan, J. Zheng, S. Park, Y . Zhou, and S. Savage, “Improving Software
Diagnosability via Log Enhancement,” in ASPLOS , 2011.
[90] D. Yuan, H. Mai, W. Xiong, L. Tan, Y . Zhou, and S. Pasupathy, “SherLog:
Error diagnosis by connecting clues from run-time logs,” in ASPLOS ,
2010.
[91] R. Ding, H. Zhou, J.-G. Lou, H. Zhang, Q. Lin, Q. Fu, D. Zhang,
and T. Xie, “Log2: A Cost-Aware Logging Mechanism for Performance
Diagnosis,” in USENIX ATC , 2015.
[92] Q. Fu, J. Zhu, W. Hu, J.-G. Lou, R. Ding, Q. Lin, D. Zhang, and T. Xie,
“Where Do Developers Log? An Empirical Study on Logging Practices
in Industry,” in ICSE , 2015.
[95] T. Barik, J. Witschey, B. Johnson, and E. R. Murphy-Hill, “Compiler
Error Notiﬁcations Revisited: An Interaction-ﬁrst Approach for Helping
Developers More Effectively Comprehend and Resolve Error Notiﬁca-
tions,” in ICSE , 2014.[93] B. Johnson, Y . Song, E. Murphy-Hill, and R. Bowdidge, “Why don’t
software developers use static analysis tools to ﬁnd bugs?,” in ICSE ,
2013.
[94] C. Sun, V . Le, and Z. Su, “Finding and Analyzing Compiler Warning
Defects,” in ICSE , 2016.
[96] D. Engler, D. Y . Chen, S. Hallem, A. Chou, and B. Chelf, “Bugs as
Deviant Behavior: A General Approach to Inferring Errors in Systems
Code,” in SOSP , 2001.
[97] L. Tan, X. Zhang, X. Ma, W. Xiong, and Y . Zhou, “AutoISES: Auto-
matically inferring security speciﬁcations and detecting violations,” in
USENIX Security , 2008.
[98] L. Tan, D. Yuan, G. Krishna, and Y . Zhou, “/* iComment: Bugs or Bad
Comments? */,” in SOSP , 2007.
[99] S. H. Tan, D. Marinov, L. Tan, and G. Leavens, “@tComment: Testing
Javadoc comments to detect comment-code inconsistencies,” in ICST ,
2012.
[100] Y . Zhou, C. Wang, X. Yan, T. Chen, S. Panichella, and H. Gall, “Au-
tomatic Detection and Repair Recommendation of Directive Defects in
Java API Documentation,” TSE, 2018.
[101] H. Zhong and Z. Su, “Detecting API Documentation Errors,” in OOP-
SLA, 2013.
[102] E. Wong, J. Yang, and L. Tan, “AutoComment: Mining Question and
Answer Sites for Automatic Comment Generation,” in ASE, 2013.
[103] J. Zhai, X. Xu, Y . Shi, G. Tao, M. Pan, S. Ma, L. Xu, W. Zhang, L. Tan,
and X. Zhang, “CPC: Automatically Classifying and Propagating Natural
Language Comments via Program Analysis,” in ICSE , 2020.
[104] G. Sridhara, E. Hill, D. Muppaneni, L. Pollock, and K. Vijay-Shanker,
“Towards Automatically Generating Summary Comments for Java Meth-
ods,” in ASE, 2010.
[105] S. Zhang and M. D. Ernst, “Proactive Detection of Inadequate Diagnostic
Messages for Software Conﬁguration Errors,” in ISSTA , 2015.
[106] N. Meng, S. Nagy, D. D. Yao, W. Zhuang, and G. A. Argoty, “Secure
Coding Practices in Java: Challenges and Vulnerabilities,” in ICSE , 2018.
[107] C. Xiang, Y . Wu, B. Shen, M. Shen, H. Huang, T. Xu, Y . Zhou, C. Moore,
X. Jin, and T. Sheng, “Towards Continuous Access Control Validation
and Forensics,” in CCS, 2019.
[108] T. Xu and D. Marinov, “Mining Container Image Repositories for Soft-
ware Conﬁgurations and Beyond,” in ICSE NIER , 2018.
[109] M. Sayagh, Z. Dong, A. Andrzejak, and B. Adams, “Does the Choice of
Conﬁguration Framework Matter for Developers? Empirical Study on 11
Java Conﬁguration Frameworks,” in SCAM , 2017.
[110] M. Raab and G. Barany, “Challenges in Validating FLOSS Conﬁgura-
tion,” in OSS, 2017.
[111] E. Horton and C. Parnin, “V2: Fast Detection of Conﬁguration Drift in
Python,” in ASE, 2019.
[112] C. Wen, Y . Zhang, X. He, and N. Meng, “Inferring and applying def-use
like conﬁguration couplings in deployment descriptors,” in ASE, 2020.
[113] S. Mehta, R. Bhagwan, R. Kumar, B. Ashok, C. Bansal, C. Maddila,
C. Bird, S. Asthana, and A. Kumar, “Rex: Preventing bugs and miscon-
ﬁguration in large services using correlated change analysis,” in NSDI ,
2020.
[114] L. Passos, R. Queiroz, M. Mukelabai, T. Berger, S. Apel, K. Czarnecki,
and J. Padilla, “A study of feature scattering in the Linux kernel,” TSE,
2018.
[115] C. Yilmaz, M. B. Cohen, and A. A. Porter, “Covering Arrays for Efﬁcient
Fault Characterization in Complex Conﬁguration Spaces,” IEEE Trans-
actions on Software Engineering (TSE) , vol. 32, no. 1, 2006.
[116] D. Jin, X. Qu, M. B. Cohen, and B. Robinson, “Conﬁgurations every-
where: Implications for testing and debugging in practice,” in ICSE , 2014.
[117] S. M. Fouché, M. B. Cohen, and A. Porter, “Incremental Covering Array
Failure Characterization in Large Conﬁguration Spaces,” in ISSTA , 2009.
[118] X. Qu, M. B. Cohen, and G. Rothermel, “Conﬁguration-aware regression
testing: An empirical study of sampling and prioritization,” in ISSTA ,
2008.
[119] E. Reisner, C. Song, K.-K. Ma, J. S. Foster, and A. Porter, “Using
Symbolic Evaluation to Understand Behavior in Conﬁgurable Software
Systems,” in ICSE , 2010.
[120] C. Song, A. Porter, and J. S. Foster, “iTree: Efﬁciently Discovering High-
Coverage Conﬁgurations Using Interaction Trees,” in ICSE , 2012.
200