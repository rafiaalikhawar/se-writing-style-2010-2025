On the Importance of Building High-quality Training Datasets
for Neural Code Search
Zhensu Sun
zhensuuu@gmail.com
Monash University
Melbourne, Victoria, AustraliaLi Li
1853549@tongji.edu.cn
Tongji University
Shanghai, ChinaYan Liu
yanliu.sse@tongji.edu.cn
Tongji University
Shanghai, China
Xiaoning Duâˆ—
xiaoning.du@monash.edu
Monash University
Melbourne, Victoria, AustraliaLi Liâˆ—
li.li@monash.edu
Monash University
Melbourne, Victoria, Australia
ABSTRACT
Theperformanceofneuralcodesearch issignificantlyinfluenced
bythequalityofthetrainingdatafromwhichtheneuralmodels
are derived. A large corpus of high-quality query and code pairs is
demandedtoestablishaprecisemappingfromthenaturallanguage
to the programming language. Due to the limited availability, most
widely-used code search datasets are established with compromise,
such as using code comments as a replacement of queries. Our
empiricalstudyonafamouscodesearchdatasetrevealsthatover
one-thirdofitsqueriescontainnoisesthatmakethemdeviatefrom
naturaluserqueries.Modelstrainedthroughnoisydataarefaced
withsevereperformancedegradationwhenappliedinreal-world
scenarios.Toimprovethedatasetqualityandmakethequeriesofits
samplessemanticallyidenticaltorealuserqueriesiscriticalforthe
practical usability of neural code search. In this paper, we propose
adatacleaningframeworkconsistingoftwosubsequentfilters:arule-basedsyntacticfilterandamodel-basedsemanticfilter.This
is the first framework that applies semantic query cleaning to code
search datasets. Experimentally, we evaluated the effectivenessof our framework on two widely-used code search models and
three manually-annotated code retrieval benchmarks. Training the
popularDeepCSmodelwiththefiltereddatasetfromourframework
improves its performance by 19.2% MRR and 21.3% Answer@1, on
average with the three validation benchmarks.
CCS CONCEPTS
â€¢Software and its engineering â†’Reusability.
KEYWORDS
Code search, dataset, data cleaning, deep learning
âˆ—Xiaoning Du and Li Li are co-corresponding authors.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation
on the first page. Copyrights for components of this work owned by others than ACMmustbehonored.Abstractingwithcreditispermitted.Tocopyotherwise,orrepublish,topostonserversortoredistributetolists,requirespriorspecificpermissionand/ora
fee. Request permissions from permissions@acm.org.
ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
Â© 2022 Association for Computing Machinery.
ACM ISBN 978-1-4503-9221-1/22/05...$15.00
https://doi.org/10.1145/3510003.3510160ACM Reference Format:
ZhensuSun,LiLi,YanLiu,XiaoningDu,andLiLi.2022.OntheImportance
of Building High-quality Training Datasets for Neural Code Search. In
44thInternationalConferenceonSoftwareEngineering(ICSEâ€™22),May21â€“
29, 2022, Pittsburgh, PA, USA. ACM, New York, NY, USA, 12 pages. https:
//doi.org/10.1145/3510003.3510160
1 INTRODUCTION
A semantic code search engine is a vital software development
assistant, which significantly improves the development efficiency
andquality.Withadescriptionoftheintendedcodefunctionality
innaturallanguage,asearchenginecanretrievealistofsemanti-
cally best-matched code snippets from its codebase. Recently, deep
learning (DL) has been widely applied in this area in view of its
advantages in semantic modeling and understanding of languages.
Inthetaskofcodesearch,DLmodelslearnandrepresenttheseman-
ticmappingsbetweenthenaturallanguageandtheprogramminglanguage from query-code pairs.
Like many other DL tasks, code search models are data-hungry
andrequirelarge-scaleandhigh-qualitytrainingdatasets.Never-
theless, collecting a large set of query-code pairs is challenging,
where the queries are supposed to be natural expressions from
developers and the code to be a valid semantic match. Instead, con-
sidering the scale and availability, code comments are popularly
usedasanalternativetothequeries,manyofwhichdescribethe
corefunctionalitiesandwiththecorrespondingcodeimplementa-
tionrightlyavailable.Tobetterunderstandthequalityofdatasets
henceconstructed,weinvestigatedaGithubdataset,CodeSearch-
Net (Java) [ 19], which is popularly used in current code search
research. Surprisingly, we found a considerable amount of noiseand unnaturalness in the queries of its data samples, which can
hinderthetrainingofhigh-qualitymodelsforpracticalusage.As
showninFig.1,one-thirdofitsqueriescontaintextfeatures(see
Table1forexamplesofdifferentfeatures)thathardlyexistinactual
userqueries.Thefeaturesaresummarizedbasedonourobserva-
tionsofthedataset,andmaynotbesufficient.Commentsmayalso
be used for other purposes, such as copyright and to-do, instead of
describingthecorefunctionalities,thusshallnotbeseenasqueries.
The proportion of noise data can be higher than one-third.
Code search models trained with noisy queries will face severe
performance degradation when dealing with actual user queries.
Thegapbetweenthecollectedcomment-codepairsandthenatural
userqueriesviolatesthebasicassumptionoflearningalgorithms
16092022 IEEE/ACM 44th International Conference on Software Engineering (ICSE)
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:32:16 UTC from IEEE Xplore.  Restrictions apply. ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA Zhensu Sun, Li Li, Yan Liu, Xiaoning Du, and Li Li
Non-English Languages
3.4%Interrogation0.3%Punctuation10.4%HTML tags16.6%URLs0.3%Short Sentence
35.5%
Parentheses
9.7%
Javadoc tags
23.8%Ja
P33.1% 66.9%
Others
Noise
Figure 1: Statistics of 394,471 code comments used in Code-
SearchNet (Java). The feature definitions are presented inSection 3.1.
that the training data and the evaluation data share a similar distri-
bution.Itisalsonoteworthythatevaluatingthemodelwithanoisycomment-code benchmark can hardly reflect how useful the model
would be in practice, and, even worse, may bring non-negligible
biastothemodeldesign,evaluationandapplication.Manyotherre-
searchers [ 7,30,44] also point out the misalignment between code
comments and natural user queries, and report it as a threat to the
validityoftheirapproaches.Asmentionedin[ 31,57],improving
the quality of the training data is still a research opportunity for
machine learning, including DL-based code search models. Consid-
ering that there are still plenty of comments close to actual userqueries and naturally paired with high-quality code snippets, apromising solution is to filter out the noisy ones. Manual filter-ing can produce the most accurate results but is hardly practical
forlarge-scaledatasets.Automateddatacleaningmethodsareof
demanding needs.
Queriesforcoderetrievalpossessspecific syntactic andsemantic
characteristics, which can be utilized as key features to distinguishgenuine user queries from noise. Typical syntactic features include
text attributes such as keywords, sentence length, and language
type. Semantic features are related to the intention underneath the
textexpression,whichusuallydescribesthecomputationalfunc-tionality of code snippets and might be influenced by the design
conventionofcommonprogramAPIs.Comparedwithsyntacticfea-
tures, semantic features are more abstract, implicit, and hard to be
matchedbysimplerules.Recently,someinitialeffortshaveemerged
on query quality enhancement, but primarily focusing on the regu-
larization ofsyntactic features.Simple filtering heuristicsare pro-
posed, based on the appearance of verb and noun phrases [ 30],
keywords uncommonly used in queries [ 7], and constraints on the
query length [19, 30].
However,theimprovementindataqualityislimited.Asdeclared
in[19],thecollecteddatasetisstillnoisydespitetheirdatacleaning
efforts.Theproposedrulesarenotsufficienttocoverthevarious
syntacticviolations,letalonethesemanticmisalignment.Forexam-
ple, warning messages such as â€œUse of this property requires Java
6â€ widely exist in the code comments, but few code queries would
request thisway. Hence, aremaining challengeis recognizing the
codecommentsthataresyntacticallyvalidbutencodesemantics
rarely seen in natural user queries.
To tackle this challenge, we propose an automated and effective
data cleaning framework that distills high-quality queries fromTable 1: Examples of syntactic rules.
Syntax Feature Rule Action Example
HTML tags Partly Remove <p>parse line</p>
Parentheses Partly Remove (TODO) Send requests
Javadoc tags Fully Remove Returns a {@link Support}
URLs Fully Remove See https://github.com/
Non-EnglishLanguagesFully Remove åˆ›å»ºä¸´æ—¶æ–‡ä»¶
Punctuation Fully Remove ==============
Interrogation Fully Remove Is this a name declaration?
Short Sentence Fully Remove DEPRECATED
generally collected code comments on a large scale. The frame-
work is orthogonal to the design of code search algorithms and
could be integrated with any of them to improve the quality of the
trainingdataset.Basically,itencompassestwosubsequentfilters:arule-basedsyntacticfilter andamodel-basedsemanticfilter.The
rule-based filter includes a set of systematically designed heuristic
rulesandweedsoutdatawithanomaloussyntacticfeatures,e.g.,
HTML tags and Javadoc tags. It is developed to cover a diverse
range of syntactic violations, and each member inside is validated
to reduce the noises effectively. It is also extensible to fulfill the
specificrequirementsforthedatasetbasedontheapplications.The
model-based semantic filter further refines the dataset produced
bytherule-basedfilterandretainsthecommentsthatareseman-
ticallyclosetothenaturalqueries.Thefilterreliesonabootstrap
query corpus, a set of high-quality queries, which represents how
semanticallythequeriesshouldlook.Itlearnsthesemanticfeatures
of the corpus, such as the expression style and topic, with a DL
model, and leverages it to identify samples with similar semantics.
The bootstrap query corpus could be constructed with any trusted
sources of natural user queries, and we formulate it with question
titles from StackOverflow in this work. These titles are an ideal
approximationofnaturalqueriesandcouldbere-usedbyrelated
studies.Then,aVariationalAuto-Encoder[ 22]istrainedwiththe
bootstrap query corpus, which maps the inputs into a latent space
and attempts to reconstruct the original inputs solely based on the
latent features. The reconstruction loss reflects â€œhow far awayâ€ an
input is from the training data distribution, i.e., the distribution of
queriesinthebootstrapquerycorpus.Thelowerthereconstruction
loss, the more qualified an input is as a natural query. We compute
thereconstructionlossforeachcodecommentintherawdataset
and cluster them into two groups. The group of qualified queries is
retained for training, and the group of noises is discarded.
To evaluate the effectiveness of our data cleaning framework,
wecomparetheperformanceofcodesearchmodelstrainedwith
datasets before and after the filtering. One training dataset, two
neuralmodels,andthreemanuallyannotatedvalidationdatasets
areusedintheexperiments,andourframeworkbringsasignificant
performance improvement under all settings. In particular, the per-
formanceofthepopularDeepCS [ 15]modelisimprovedby 19.2%
MRRand21.3%Answer@1,onaveragewiththethreevalidationdatasets.Moreimportantly,withlesstrainingdatausedafterthe
filtering, we also save the training time and computation resources.
1610
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:32:16 UTC from IEEE Xplore.  Restrictions apply. On the Importance of Building High-quality Training Datasets for Neural Code Search ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
Further,wecarryoutacomprehensiveablationstudytovalidate
theusefulnessofeachfiltercomponentandeachruleandmanually
inspect the quality of the rejected and retained data. Finally, we
releasetheimplementationofourframework, NLQF,andacleaned
codesearchdataset, COFIC,tofacilitatefutureresearch.Thesource
code and datasets are available at https://github.com/v587su/NLQF.
To the best of our knowledge, this is the first systematic data
cleaning framework for comment-based code search datasets. Our
main contributions include:
â€¢A two-step data cleaning framework for code search datasets,
which bridges the gap between code comments and natural user
queries, both syntactically and semantically.
â€¢Implementation of the framework as a Python library for the
code search task in academia and industry.
â€¢A comprehensive evaluation of our frameworkâ€™s effectiveness,
whichdemonstratessignificantmodelperformanceimprovement
on three manually-annotated validation benchmarks.
â€¢ThefirstsystematicallydistilledGithubdatasetforneuralcode
search, containing over one million comment-code pairs.
2 PRELIMINARIES
We prepare readers with the primary sources for collecting query-
codepairsandtheVariationalAuto-Encoder,amajorbuildingblock
of our framework.
2.1 Data Source
A query for neural code search describes, in natural language, the
functionality of the code snippets desired by users, e.g., â€œconvert
string to JSON objectâ€. The ideal data source for genuine codequeries is the production data from existing neural code search
engines. However, these queries are not publicly accessible due to
privacy and business sensitivity. In academia, researchers use texts
withsimilarintentions(e.g.,codecomments)asareplacement.The
primary alternative data sources for semantic code search research
include GitHub and StackOverflow.
2.1.1 Github. Github[1] is an open-source community, hosting
more than 100 million repositories. It is the most popular platform
for developers to share and manage their projects. The large-scale
well-maintained repositories on Github are a treasury for code
reuse during development, thus naturally becoming the main re-trieval source for code search tasks. Moreover, mature projectsare usually accompanied by canonical development documents.
According to Javadoc[ 2], a code comment style guide, the first sen-
tence of doc comments should be a summary sentence. Therefore,
it is convenient to construct a code search dataset by collecting the
code snippets paired with the first sentence of comments, forming
the comment-code pairs. Javadoc-generated comments have hence
been widely used in practice for various software engineering pur-
poses [25â€“27,33] due to their large scale, ease of obtaining, and
being close to actual use scenarios.
However,developerswritecommentsfortheirsoftwareprojects
withoutconsideringtheretrievalpurposes.Notallthecomments
properly map to queries. As mentioned in Section 1, the Code-
SearchNet collected from Github contains plenty of anomalies that
rarely exist in natural user queries. It is not appropriate to includethesecommentsinthedataset,andwecallformoreattentiontobe
drawn to this problem.
2.1.2 StackOverflow. StackOverflow[ 4]servesasaQ&Acommu-
nity specialized for software developers. It is a rich resource of
software-related questions and answers. When asking about codes
or APIs for implementing a specific functionality, users would pro-
poseaquestiontitletoexpresstheirintention.Thesearenatural
userquerieswithvalidsyntaxandsemantics.Hence,researchers[ 52,
54] also collect the titles of StackOverflow questions paired with
proper answers containing sample code snippets, which also form
thequery-codepairs.Othersalsoevaluatetheircodesearchmodels
with queriesmanually selected from StackOverflow [ 7,15,32], and
additionalpublicevaluationbenchmarkscouldbefoundin[ 24,50].
Compared with the Github data source, queries from StackOver-
flow have a significant advantage of being closer to natural user
queries,butthequalityofcodesamplesishardtoguarantee.Hence,
the dataset collected from StackOverflow is still not as desired.
Nevertheless,thequerycorpusisvaluable.Itisworthinvestigat-
ing whether and how it could be leveraged to improve the other
query-code datasets.
2.2 Variational Auto-Encoder
VariationalAuto-Encoder(VAE)[ 5]isaneuralmodelthatlearns
the distribution of a set of data. A VAE model consists of an en-coder and a decoder. The encoder learns to map an input data
ğ‘¥
into a prior distribution ğ‘ğœƒ(ğ‘§), from which a latent variable ğ‘§is
sampled,andthedecodermaps ğ‘§backtoË†ğ‘¥,areconstructionof ğ‘¥.
It is expensive to calculate ğ‘ğœƒ(ğ‘§)directly, so VAE introduces an
approximate posterior ğ‘ğœ™(ğ‘§|ğ‘¥).ğœƒandğœ™are parameters of the prior
and the approximate posterior.
Thelossfunction,EvidenceLowerBound (ELBO),whichseeks
to maximize the likelihood of reconstructing the original data and
minimize the Kullback-Leibler (KL) divergence between the actual
and estimated posterior distributions, is represented as:
L=Eğ‘ğœ™(ğ‘§|ğ‘¥)[âˆ’ğ‘™ğ‘œğ‘”ğ‘ ğœƒ(ğ‘¥|ğ‘§)]+ğ¾ğ¿(ğ‘ğœ™(ğ‘§|ğ‘¥)||ğ‘ğœƒ(ğ‘§)),(1)
whereğ¾ğ¿represents the KL divergence. Theoretically, the distribu-
tionsofğ‘ğœ™(ğ‘§|ğ‘¥)andğ‘ğœƒ(ğ‘§)canbearbitrary.Inpractice,theGaussian
distribution is mostly adopted.
3 THE DATA CLEANING FRAMEWORK
This section introduces our automated and effective data cleaning
frameworkforcodesearchdatasets,mainlytofilteroutquery-code
pairswithinappropriatequeries.Theframeworkconsistsoftwo
subsequentfilters,the rule-basedsyntacticfilter andmodel-basedse-
manticfilter.Anoverviewoftheframework,whenappliedtoclean
thecomment-codepairscollectedfromGithub,isshowninFig.2.
Therawcomment-codepairsarefirstlycleanedbytherule-based
filter, where a ruleset is appliedto detect the existence of invalid
querysyntax.Next,forthemodel-basedfilter,leveragingasmall
bootstrap query corpus as the semantics reference, a VAE modelis trained to model its characteristics and further used to reject
commentsviolatingthenaturalquerysemantics.Herewecollect
thebootstrapquerycorpus(noneedofthepairedcodesnippets)
from StackOverflow,and more detailscan befound inSection 4.1.
Such, we take advantage of both the Github and StackOverflow
1611
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:32:16 UTC from IEEE Xplore.  Restrictions apply. ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA Zhensu Sun, Li Li, Yan Liu, Xiaoning Du, and Li Li
Github
StackOverflowHeuristic Rule
Variational  
Auto-EncoderBootstrap 
Query Corpus
EM-GMM 
ClusteringRaw Comment-
Code PairsJavadoc tags
HTML tags
Short Sentence
â€¦
Losstrain
Model-based Semantic Filter Comment-Code 
Pairs
Query-Code PairsRule-based Syntactic Filter
Figure 2: An overview of our data cleaning framework.
data sources and produce a large set of high-quality query-code
pairs.Inthefollowing,weelaborateonthedesignofthetwofilters.
3.1 The Rule-based Syntactic Filter
Codecommentscontainricherinformationthanjustdescriptions
on code functionalities and manifest various syntactic features
rarely existing in actual code search quires. For example, URLs are
usedforexternalreferences,andHTMLtagsareusedincomments
for documentation autogeneration. To reduce such deviations from
naturalqueries, wesampled 1%codecomments fromCodeSearch-
Net, manually inspected and summarized noises in these 3,949instances. We establish a black list of invalid syntax features toreject unqualified code comments. If a comment matches any of
thesefeatures,weremovetheinvalidpartsiftheyaredetachable;
otherwise, we abandon this comment-code pair.
Based on a comparative observation of code comments and user
queries,wedevelopasetofrulestopreciselyidentify synthetically
inappropriate queries and leave the fine-grained semantic check
tothe model-basedfilter. Tofacilitate themanagement, wedefine
three criteria that the ruleset must comply with: 1) any rule shoulddefineauniqueandspecificconstructionpattern,2)therulesshould
be conservative and limit the preclusion of valid queries within an
acceptable range,and 3)any rule isnot asubrule ofother rules in
theset. Asa plug-inframework, therulesetis extensible,and any
rules that meet these criteria can be appended to the set.
Weintroducethesyntaxfeaturescoveredbyourrulesetinthefol-
lowing,andtheirexamplescanbefoundinTable1.Weempirically
decide whether to keep the content enclosed by a feature structure
or not and validate the decisions with experiments (see our web-site [
3] for more details) and manual inspection (see Section 5.2).
Fromtheresults,ourdecisionshelpimprovethenaturalnessand
bring greater improvement to the model performance.HTMLtags
HTMLtagsareusedfordocumentationautogenera-
tion in comments and should not appear in user queries. However,
the content wrapped by the tags can still be informative. There-
fore,weremovetheHTMLtagsfromthecommentsbutkeepthe
wrapped content.
Parentheses Parentheses in comments are for adding supplemen-
tary information and do not appear in user queries. Due to suchpurpose, the removal of the content inside the parentheses doesnothavemuchinfluenceonthecompletenessofthecomments.We
only retain the content outside of the parentheses.
Javadoc tags Javadoc tags starting with an â€œ@â€ sign are special
tagsindicatinga Javadoccomment.Suchcommentsare onlycon-
sumed by the Javadoc project for autogenerating well-formatted
documentation.Consideringthatthespecialsyntaxofthetagsmay
mislead code search models on natural language understanding,
we reject all comments containing Javadoc tags.
URLsURLsincommentsprovideexternalreferencestorelevant
code snippets, but natural language queries do not contain any
URLs. We reject all comments containing URLs.Non-English Languages
Non-English expressions exist as devel-
opers from different countries may write comments in their first
languages.However, currentcodesearch modelsarenot designed
to handle multi-languages. We reject all non-English comments.Punctuation
Sometimes,punctuationsymbolsareusedforsection
partitioninginthecomments.Forexample,developersusearow
of equal signs ( =) or asterisks signs ( âˆ—) (see examples in Table 1)
to indicate a new section. For effectiveness, we reject comments
containing no English letters in our implementation.Interrogation
Basedonourobservation,someofthecomments
in the dataset are interrogative. Developers seem to use comments
to communicate with their collaborators during the code reviewprocess. There may be some sparse information about the code
functionality,butthequalityishardtocontrol.Werejectcomments
ending with a question mark.ShortSentence
Thesentencelengthisacommonlyusedcriterion
for comment filtering. Extremely short comments are not informa-
tiveenoughforcodesearchmodelstoestablishtheirmappingto
thecorresponding codesnippets.Werejectcommentscontaining
no more than two words.
3.2 The Model-based Semantic Filter
Thissectionintroducesthemodel-basedsemanticfilter,whichtakes
the initially cleaned comment-code pairs from the rule-based filter
as input and further selects the pairs with comments semantically
closetothequeriesinapre-collectedbootstrapquerycorpus.We
present the detailed design of the VAE model and discuss how it is
used for filtering.
3.2.1 The VAE Model. The two main components of a VAE model
aretheencoderanddecoder,whicharegenerallycomposedofdeepneuralnetworks.Here,weuseGatedRecurrentUnit(GRU)[
11]for
both the encoder and decoder in our VAE model. GRU is a variant
of Recurrent Neural Network (RNN), which enables the modeltocaptureinformationfromsequentialtexts.Fig.3illustratesan
overviewofthedesignofthemodelstructure.Detailsabouteach
layer are as follows.
Embedding Given a query ğ‘¤0ğ‘¤1...ğ‘¤ ğ‘›of length ğ‘›, theğ‘–-th to-
ken isğ‘¤ğ‘–. The embedding layer is responsible for mapping each
tokenintoanembeddingvector.Itconsistsofanembeddingmatrix
EâˆˆRğ‘œğ‘¤Ã—ğ‘‘, whereğ‘œğ‘¤is the vocabulary size of the query language
andğ‘‘is the dimension of embedding vectors. The matrix is initial-
ized with random values and updated during training.
GRUEncoder WedesigntheencoderofVAEasabi-directional
GRU.Sequentially,itdealswiththeinputtokens,andpropagates
theupstreamanddownstreamcontextthroughthehiddenstates,
1612
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:32:16 UTC from IEEE Xplore.  Restrictions apply. On the Importance of Building High-quality Training Datasets for Neural Code Search ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
â€œConvert int to stringâ€
Token
Embeddingconvert int string to
Cell Cell Cell Cell GRU (Encoder)
Cell Cell Cell Cell GRU (Decoder)
convert int string to[BOS] Cell
[EOS] Reconstructed TokenLatent Variable ( ) zLoss
Figure 3: The structure of the Variational Auto-Encoder in
themodel-basedfilter.Thedashedlinesdenotethepropaga-tion of hidden states in neural cells.
respectively in the forward and backward directions, as shownin Eq. (2) and Eq. (3).
ğ‘’ğ‘šğ‘maps a token to its embedding vector.
Finally,wesumupthelasthiddenstatesofbothdirectionstoget
the final hidden state, as in Eq. (4), and pass it to the next layer.
âˆ’ â†’hğ‘–=âˆ’âˆ’âˆ’â†’ğºğ‘…ğ‘ˆ(ğ‘’ğ‘šğ‘(ğ‘¤ğ‘–),âˆ’âˆ’âˆ’â†’hğ‘–âˆ’1) (2)
â† âˆ’h
ğ‘–=â†âˆ’âˆ’âˆ’ğºğ‘…ğ‘ˆ(ğ‘’ğ‘šğ‘(ğ‘¤ğ‘–),â†âˆ’âˆ’âˆ’hğ‘–+1) (3)
h=âˆ’ â†’h
ğ‘›+â† âˆ’h
ğ‘› (4)
Latent Variable Based on the hidden state hfrom the encoder,
we estimate the parameters of a Gaussian distribution with a fully-
connected layer, which are the mean vector ğand variance vector
ğˆ2.Thelatentvariable zisrandomlysampledfromthisdistribution.
The equations are as follows:
ğ;ğˆ2=ğ¹ğ¶(h)
z=ğ+rÂ·eğˆ2/2
whereğ¹ğ¶is a fully-connected layer and ris a random vector from
the standard normal distribution.
GRUDecoder Thelatentvariablerepresentsthekeyfeatures
of the original input in a highly abstract and compact way. The
decoderworkstoreconstructtheinputsolelybasedonthelatent
variable. Iteratively, the decoder computes the hidden state ğ‘ ğ‘–at
each step ğ‘–and reconstructstoken ğ‘¤/prime
ğ‘–, basedon the previousstate
ğ‘ ğ‘–âˆ’1(orzat step 0) and ğ‘¤/prime
ğ‘–âˆ’1generated in the previous step. The
equations are as follows:
sğ‘–=/braceleftBigg
ğºğ‘…ğ‘ˆ(ğ‘’ğ‘šğ‘(ğ‘ğ‘œğ‘ ),z)) i=0
ğºğ‘…ğ‘ˆ(ğ‘’ğ‘šğ‘(ğ‘¤/prime
ğ‘–âˆ’1),sğ‘–âˆ’1))i>0
ğ‘ğ‘–=ğ¹ğ¶(sğ‘–)
ğ‘¤/prime
ğ‘–=ğ‘ğ‘Ÿğ‘”ğ‘šğ‘ğ‘¥(ğ‘ğ‘–)
whereğ‘ğ‘œğ‘ is a special token indicating the start of a sentence, and
ğ‘ğ‘–âˆˆRğ‘œğ‘¤represents the probability of ğ‘–-th token to be generated.
LossWemeasurethelikelihoodofreconstructingtheoriginal
inputwiththeCross-Entropy(CE)loss.Hence,theELBOlossin-
troduced in Section 2.2 can be computed as:
L=âˆ’1
ğ‘›ğ‘›/summationdisplay.1
ğ‘–=1ğ¶ğ¸ğ¿ğ‘œğ‘ ğ‘ (ğ‘¤ğ‘–,ğ‘¤/prime
ğ‘–)+ğ¾ğ¿ğ·ğ‘–ğ‘£ğ‘’ğ‘Ÿğ‘”ğ‘’ğ‘›ğ‘ğ‘’ (ğ,ğˆ2)
Performance
0% 100%
 best dividing point
Figure 4: An illustration of the relation between the por-
tionofretaineddataandtheperformanceofthecodesearchmodel trained with it.
whereğ¶ğ¸ğ¿ğ‘œğ‘ ğ‘ andğ¾ğ¿ğ·ğ‘–ğ‘£ğ‘’ğ‘Ÿğ‘”ğ‘’ğ‘›ğ‘ğ‘’ represents the calculation of the
CE loss and KL divergence.
3.2.2 The Filtering Algorithm. We train the VAE model with a set
ofhigh-qualitycodesearchqueriescollectedfromnear-realscenar-
ios,whichwecallthebootstrapquerycorpus.Afterthetraining,
theVAEmodelisabletorecognizewhetheraquerysemantically
resembles those in the corpus. We measure the reconstruction loss,
i.e.,theCEloss,ofaninputwhenfedtotheVAEmodel,whichjustreflectshowwellitiswithinthetrainingsetdistribution.Intuitively,
the loss value is the anomaly score gauging how far an input stays
away from the queries in bootstrap query corpus. Comments with
smaller losses are more likely to be query-appropriate.
Toselectcommentsresemblingqueriesinbootstrapquerycor-
pus, we sort the comments based on their reconstruction losses,inascendingorder,andretainthetop-rankedones.Itistrickyto
decide an appropriate dividing point for retaining the portion with
better quality and discarding the remaining. The less data we keep
fromthetop,thehigherthedataset quality.However ,asharpre-
duction in the data size hinders the performance of the trained
codesearchmodel.Fig.4showsatheoreticalmodelillustratingthe
relation between the dividing point and the model performance.
Astheamountofretaineddataincreases,themodelperformance
firstly increases and then decreases after reaching the peak. There
is a trade-off between the quality and quantity of the dataset.
We leverage an unsupervised clustering algorithm, EM-GMM
(Expectation-MaximizationforGaussianMixtureModel)[ 14],to
decide the partition automatically. It is widely used to model themixed distributions of a dataset. For our task, EM-GMM dividesa set of comments into the qualified and the unqualified groups
basedonthereconstructionloss.Foreachgroup,GMMfitsaGauss-
ian probability density function and mixes them together as the
distribution of the whole dataset, which can be represented as:
ğ‘ƒ(ğ‘¥)=ğœ‹ğ‘(ğ‘¥|ğœ‡ğ‘,ğœğ‘)+(1âˆ’ğœ‹)ğ‘(ğ‘¥|ğœ‡ğ‘¢ğ‘,ğœğ‘¢ğ‘)
whereğœ‹isthemixturecoefficientforthequalifiedgroup,( ğœ‡ğ‘,ğœğ‘)
and (ğœ‡ğ‘¢ğ‘,ğœğ‘¢ğ‘) are the parameters for the Gaussian probability den-
sity functions of the qualified and unqualified groups, respectively.
Finally,theEMalgorithm[ 14]isappliedtoestimateasetofoptimal
values for all the parameters.
Note that, to establish a high-quality code search dataset, all
commentsareprocessedtogetherwiththeirpairedcodesnippets.
1613
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:32:16 UTC from IEEE Xplore.  Restrictions apply. ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA Zhensu Sun, Li Li, Yan Liu, Xiaoning Du, and Li Li
Hence,weobtainasetofcomment-codepairsafterapplyingthis
semantic filter, where the comments are syntactically and semanti-
cally close to natural user queries.
4 EXPERIMENT SETUP
We introduce the research questions, the basic experimental setup
about the datasets and models, and the evaluation metrics used
throughouttheevaluation.Theresearchquestionsweaimtoanswer
include:
RQ1:How effective is our data cleaning framework?
RQ2:What is the impact of each filter component and each rule
on the effectiveness of our framework?RQ3:
How is the dividing point determined by the clustering algo-
rithm during the model-based filtering?
4.1 Datasets
Three types of datasets are involved in our evaluation, including
the training and validation datasets used to train and assess the
performanceofcodesearchmodelsandthebootstrapquerycorpus
used to develop our model-based filter. To make the best use ofexisting resources, we focus on the Java programming languagein this work, for which there have been the most public datasetsand models in the field of neural code search. Theoretically, ourframework is language-independent and applicable to other pro-
gramminglanguageswithaproperadaptationofthefilteringrules.
4.1.1 TrainingDatasets. WeusethepopularCodeSearchNet(CSN)[ 19]
datasettotrainallthecodesearchmodels.CSNisacollectionofdatasets and benchmarks for semantic code retrieval. It extracts
functionsandtheirpairedcommentsfromGithubrepositories.It
coverssixprogramminglanguages,andwetakethetrainingdataset
for Java, which contains 394,471 data points. We took the first sen-
tence of each comment. In what follows, we denote it as CSN-t.
AnotherwidelyuseddatasetinDeepCS[ 15]isnotincludedbecause
theauthorsonlyreleasedtheprocesseddata,butourframework
cannot work without accessing the raw data.
4.1.2 Validation Datasets. We utilize human-annotated validation
datasets to evaluate how well a code search model performs in a
real-world scenario, and three widely used datasets are adopted. Itis noteworthy that the validation datasets are neverfiltered by our
frameworkinordertoensurethefairnessofourexperiments.They
are listed as follows:
â€¢CSN-vCSN also offers a validation benchmark for Java, contain-
ingquery-codepairscollectedfromBingandStackOverflow.Hu-
manannotatorsarehiredtoratetherelevancebetweenthequeryandthecode.Pairswithascoregreaterthan0aredeemedasrele-
vant,andthereare434relevantpairsintotal.Inthedataset,each
pair is accompanied by 999 distractor code snippets. It means,
givenaquery,thecodesearchmodelneedstoretrievetheground
truth among 1000 candidates.
â€¢CBCosBench (CB)[ 50] isa validationdataset consistingof 52
selectivequeriesfromStackOverflow.Foreachquery,theauthors
preparedaroundtenparingcodesnippetsasitsgroundtruths,including its best answer on StackOverflow and several other
matched code snippets selected from GitHub. Additionally, there
is a pool of 4,199,769 distractor code snippets. The model needsto search the ground truths from a mixture with the complete
code pool given a query.
â€¢NCSEDProposed in [ 24], the NCSED dataset contains 287 ques-
tion queries manually collected from StackOverflow. For each
query,therearearoundthreepairingcodesnippetsselectedfrom
GitHub. The ground truths are mixed with other 4,716,814 dis-
tractorcodesnippetscollectedfromGitHub.Thesearchmodel
is required to retrieve the ground truths from the large corpus
for a query.
The extremely large search space in NCSED and CB makes it
extremely hard for code search models to achieve a good perfor-
mance, and the performances variations brought by data cleaning
canalsobetoomarginaltocompare.Withoutlossofgenerality,foreachqueryinNCSEDandCB,weconstruct999distractorsnippets,
following a similar fashion as CSN-v.
4.1.3 BootstrapQueryCorpus. StackOverflowisanidealsourcefor
collecting resemblers of actual user queries, though the quality of
thepairingcodesnippetsishardtoguarantee.Itbecomesanoptimalchoicetoestablishthebootstrapquerycorpus.WesurveyedexistingStackOverflowdatasetsinthecodesearchfield,andfoundthatthey
were of severely limited size. With their aim to collect high-quality
question-code pairs, numerous questions were discarded due tothe lack of qualified code answers. Hence, to better facilitate the
training of our VAE model, we determined to construct a question-
only corpus from StackOverflow instead of using existing ones.
Accordingtoastudy[ 36],theStackOverflowquestionscanbedi-
vided into four types: â€œDebug/Correctiveâ€, â€œNeed-To-Knowâ€, â€œHow-
To-Do-Itâ€andâ€œSeeking-Different-Solutionâ€.Amongthem,questions
of the â€œHow-To-Do-Itâ€ type are most relevant to queries for the
codesearchtask.Aimingtoselectthemostqualifiedresemblers,werequirethequestiontitlesto1)startwithâ€œhowtoâ€,2)betaggedwith
â€œJavaâ€,and3)passtherule-basedsyntacticfilterproposedinSec-
tion3.1 (exceptfor the Interrogationrule). Intheend, 168,779out
of 1,709,703 Java-related question titles were retained. Afterwards,
wetransformedthemintodeclarativesentencesbyremovingthe
startingâ€œhow toâ€and thequestionmarks ifany, thusformingthe
bootstrap query corpus, which is used to train the VAE model later.
4.2 Code Search Model
Two code search models, DeepCS [ 15] and CARLCS [ 41], are used
inourexperiments.Theyaredesignedwithrepresentativearchi-
tectures among most neural code search models. DeepCS is based
onthe Siamesearchitecture,and CARLCSisanInteraction-based
network [ 35]. The Siamese architecture consists of two DL models
to represent the query and code, respectively, with independent
embeddingvectors,andthesimilaritybetweenthesevectorsisusedtomeasuretherelevancebetweenqueryandcode.TheInteraction-basednetworkcomparesthequeryandcodedirectlybygenerating
an interaction matrix to reflect their relevance.
Whentrainingthemodelswithourtrainingdataset,weadopted
the recommended settings for all the hyper-parameters, except for
thetrainingepochoftheDeepCSmodel.Inordertosavesometime
and computation resources, we set the maximum training epochof DeepCS to 100 instead of the recommended 500. Without lossof fairness, the same setting has been used for training with the
dataset either before or after the data cleaning. This change should
1614
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:32:16 UTC from IEEE Xplore.  Restrictions apply. On the Importance of Building High-quality Training Datasets for Neural Code Search ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
Table 2: The Answered@k and MRR scores of the DeepCS and CARLCS models trained over different datasets.
Model Test Set #Query Train Set #Pairs Train Hours A@1 A@5 A@10 MRR
DeepCSCSN-v 434CSN-t (all) 394,471 8h 123 248 306 0.407
CSN-t (controlled) 192,031 4h 107 235 275 0.376
CSN-t (filtered) 192,031 4h 16836.6%â†‘29920.6%â†‘34813.7%â†‘0.51226.0%â†‘
CB 52CSN-t (all) 394,471 8h 25 31 38 0.522
CSN-t (controlled) 192,031 4h 20 26 28 0.438
CSN-t (filtered) 192,031 4h 2916.0%â†‘3822.6%â†‘405.3%â†‘0.64423.3%â†‘
NCSED 287CSN-t (all) 394,471 8h 44 101 136 0.250
CSN-t (controlled) 192,031 4h 31 90 135 0.210
CSN-t (filtered) 192,031 4h 4911.4%â†‘1108.9%â†‘1424.4%â†‘0.2718.4%â†‘
CARLCSCSN-v 434CSN-t (all) 394,471 6h 54 210 292 0.283
CSN-t (controlled) 192,031 3h 54 202 284 0.281
CSN-t (filtered) 192,031 3h 6214.8%â†‘2215.2%â†‘2961.4%â†‘0.3026.7%â†‘
CB 52CSN-t (all) 394,471 6h 1 2 6 0.038
CSN-t (controlled) 192,031 3h 0 1 3 0.012
CSN-t (filtered) 192,031 3h 2100.0%â†‘4100.0%â†‘716.7%â†‘0.05649.4%â†‘
NCSED 287CSN-t (all) 394,471 6h 20 34 57 0.105
CSN-t (controlled) 192,031 3h 15 33 49 0.097
CSN-t (filtered) 192,031 3h 3575.0%â†‘5870.6%â†‘7429.8%â†‘0.16859.9%â†‘
not affect the evaluation conclusion on the effectiveness of our
framework,whichfocusesmoreonwhetherthemodelperformance
improves after removing the noises instead of its absolute level.
4.3 Evaluation Metrics
Twowidelyusedmetricsareadoptedinourexperimentstoevaluate
the code retrieval performance.
â€¢Answered@k : Answered@k (abbrev. A@k) is the number of
queries answered by snippets in the top-k results.
â€¢Mean Reciprocal Rank (MRR) :MRRistheaverageofthere-
ciprocal ranks of the ground truth in the result list.
5 RESULTS
Inthissection,weshowtheexperimentalresultsandanswerthere-
searchquestions.Measuresforbothevaluationmetricsarereported
as the medium over five independent runs.
5.1 RQ1: Effectiveness
This experiment evaluates the effectiveness of our data cleaning
solution as a pre-processing step when training neural code search
models. Specifically, one training dataset (CSN-t), two code search
models (DeepCS and CARLCS), and three validation datasets (CSN-
v, CB, and NCSED) are used in the evaluation. Thus, we have six
(1Ã—2Ã—3)experimental settings in total. During experiments, a
relatively smaller filtered training set will be derived from CSN-t after our framework is applied for the data cleaning. To also
benchmarktheperformancevariationbroughtbythesizeshrinking,
wefurtherderiveacontrolledtrainingsetbyrandomlyselecting
from CSN-t an equivalent number of data as the filtered set. We
observethe modelperformance resultedfrom trainingwiththese
three datasets respectively.
Themodelperformanceismeasuredwith fourevaluationmet-
rics,namely,A@1,A@5,A@10,andMRR,andtheresultsareshowninTable2.Underallthesixexperimentalsettings,ourdataclean-
ing framework demonstrates a positive influence on the modelâ€™s
searchingabilityandhelpsithitthebestscore.Onaverageofthe
threevalidationdatasets,DeepCStrainedoverthefiltereddataout-
performstheonetrainedoveroriginaldataby21.3%A@1,17.4%
A@5, 7.8% A@10, and 19.2% MRR. Correspondingly, the improve-
ments of CARLCS are 63.3% A@1, 58.6% A@5, 16.0% A@10, and38.6% MRR. Regarding the MRR on the three validation datasets,CSN-v,CB, and NCSED, DeepCS achieves 0.512, 0.644 and 0.271,
andCARLCSachieves0.302,0.056and0.168,respectively.Basically,
DeepCS and CARLCS are boosted to their new best records, and
CARLCS sees agreater improvement. Note that theA@1 score of
CARLCS over NCSED is increased by 75.0% (from 20 to 35), which
is an extraordinary improvement.
Overall, with around half of the data quantity and half of the
training time, models trained over the filtered data achieve a sig-
nificant improvementon the number of answeredqueries and the
rank of ground truth in search results.
Answer to RQ1: Our filtering framework produces a high-
quality query-codedataset, which shortensthe trainingtime
by reducing the training data and effectively improves theperformance of the code search model under a real-world
application scenario.
5.2 RQ2: The impact of each filter component
and each rule
Weevaluatetheeffectivenessofeachfiltercomponentwithabla-
tion experiments and conduct manual inspection on the queries
accepted/rejected by each syntactic rule and the model-based filter
to study their precision in identifying noises.
Each time, one of the two filter components is muted for the
ablation experiments. We observe the model performance after
1615
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:32:16 UTC from IEEE Xplore.  Restrictions apply. ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA Zhensu Sun, Li Li, Yan Liu, Xiaoning Du, and Li Li
Table 3: Results of the ablation experiments on the filter components.
Model Test Set #Query Train Set #Pairs A@1 A@5 A@10 MRR
DeepCSCSN-v 434CSN-t (all) 394,471 123 248 306 0.407
CSN-t (filtered) 192,031 168 299 348 0.512
Rule Filter only 285,372 157 6.5% â†“283 5.4% â†“335 3.7% â†“0.490 4.3% â†“
Model filter only 286,306 158 6.0% â†“276 7.7% â†“323 7.2% â†“0.491 4.2% â†“
CB 52CSN-t (all) 394,471 25 31 38 0.522
CSN-t (filtered) 192,031 29 38 40 0.644
Rule Filter only 285,372 28 3.4% â†“35 7.9% â†“38 5.0% â†“0.598 7.2% â†“
Model filter only 286,306 24 17.2% â†“35 7.9% â†“38 5.0% â†“0.539 16.3% â†“
NCSED 287CSN-t (all) 394,471 44 101 136 0.250
CSN-t (filtered) 192,031 49 110 142 0.271
Rule Filter only 285,372 46 6.1% â†“106 3.6% â†“139 2.1% â†“0.265 2.3% â†“
Model filter only 286,306 48 2.0% â†“106 3.6% â†“137 3.5% â†“0.264 2.4% â†“
CARLCSCSN-v 434CSN-t (all) 394,471 54 210 292 0.283
CSN-t (filtered) 192,031 62 221 296 0.302
Rule Filter only 285,372 57 8.1% â†“211 4.5% â†“288 2.7% â†“0.293 3.0% â†“
Model filter only 286,306 57 8.1% â†“219 0.9% â†“294 0.7% â†“0.300 0.6% â†“
CB 52CSN-t (all) 394,471 1 2 6 0.038
CSN-t (filtered) 192,031 2 4 7 0.056
Rule Filter only 285,372 1 50.0% â†“2 50.0% â†“5 28.6% â†“0.039 31.2% â†“
Model filter only 286,306 1 50.0% â†“3 25.0% â†“6 14.3% â†“0.049 14.0% â†“
NCSED 287CSN-t (all) 394,471 20 34 57 0.105
CSN-t (filtered) 192,031 35 58 74 0.168
Rule Filter only 285,372 18 48.6% â†“57 1.7% â†“740.0%â†“0.122 27.6% â†“
Model filter only 286,306 21 40.0% â†“46 20.7% â†“63 14.9% â†“0.120 28.4% â†“
trainingwithsuchderivedfiltereddatasetandcompareitwiththeir
previous performance (in Section 5.1). If the performance declines
comparedwithwhenbothfiltersareenabled,wecaninferapositive
impact of the muted component on the framework effectiveness.
WeevaluatetheperformanceofDeepCSandCARLCStrainedunder
ablation and report the results in Table 3. The removal of any filter
leads to worse performance scores. Without the model-based filter,
the A@1, A@5, A@10, and MRR scores of DeepCS on the three
validation sets reduce by 5.4%, 5.6%, 3.6%, and 4.6% on average.
CARLCSseesamuchmoreseverededuction,andonaverage,A@1,
A@5, A@10, and MRR decrease by 35.5%, 18.7%, 10.4%, and 20.6%.
After removing the rule-based filter, the performance of DeepCS
averagely drops by 8.4% A@1, 6.4% A@5, 5.2% A@10, and 7.6%
MRR.Meanwhile,theaveragereductionpercentagesofCARLCS
onallthevalidationsetsare32.7%A@1,15.5%A@5,9.9%A@10,
and14.3%MRR.ItisnoteworthythattheA@1scoreofCARLCSon
NCSEDdropsfrom35to18whentherulefilterismuted,indicatingthattherulesetplaysaveryinfluentialpartduringthedatacleaning.
For the manual inspection, two annotators, with over two yearsâ€™
development experience, are hired to rate how likely a sentence is
tobeusedasacodesearchquery.Theratingscorerangesfrom0to
2,where0meansworstand2best.Thereare11groupsofdatato
annotate,includingeightgroupsofcommentsrejectedbyeachrule,
the group of comments discarded by the model filer, the original
CSN-t dataset, and the filtered dataset after the two-filter cleaning.
Thelasttwogroupsareforcomparisonpurposes.Forrulesfocusing
on detachable features, i.e., the Parentheses and HTML tags, we let
the annotators judge how well the removed part can help with aqueryexpression.Wesampleasubsetofdatafromitsfullsetfor
each group. The sample size ğ‘ ğ‘ of each group is computed by a sta-
tisticalformulawhichisextractedfrom[ 12],ğ‘ ğ‘ =ğ‘§2âˆ—ğ‘âˆ—(1âˆ’ğ‘)/ğ‘2
1+ğ‘§2âˆ—ğ‘âˆ—(1âˆ’ğ‘)/(ğ‘2âˆ’1)
ğ‘ğ‘œğ‘ğ‘¢ğ‘™ğ‘ğ‘¡ğ‘–ğ‘œğ‘›,
whereğ‘ğ‘œğ‘ğ‘¢ğ‘™ğ‘ğ‘¡ğ‘–ğ‘œğ‘› is the size of the entire dataset, ğ‘is the standard
deviation of the population, ğ‘is the confidence interval (margin
of error), ğ‘§is the Z-Score determined by the confidence level. In
this experiment, we choose to work with a 95% confidence level
(i.e.,1.96Z-Scoreaccordingto[ 20]),astandarddeviationof0.5(0.5
is the maximum standard deviation, and a safe choice given theexact figure unknown), and a confidence interval of 5%. We also
measure the agreement between the two annotators with Cohenâ€™s
Kappa [13], which is 0.69 and within the range of fair to good.
For each data, we finalize its score as the average of scores from
the two annotators and display the statistics in Table 4. We report
the number of data examined in each group, the respective portion
of data scored as 0 or no less than 1, and the groupâ€™s average score,
inthelastfourcolumns.Ingeneral,thecommentsrejectedbyeither
the rule-based filter or the model-based filter poorly resemble real
userqueries,with96.9%and85.9%ofthemreceivingascoreof0
andtheaveragescoresbeingaslowas0.04and0.20,respectively.
Still, it comes at an acceptable cost of losing a small set of good
qualitydata,where3.1%and14.1%ofthediscardeddatabythetwofiltersscoreatleast1.Eachoftheeightrulesrejectscodecommentsinaneffectiveway,withfourofthemrejectingnon-query-likedataat100%precision.TheprecisionoftheParenthesesruleisrelativelylow,where11%ofthediscardeddataisofhighquality.Inthefuture,
when deciding whether the content inside the parentheses should
1616
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:32:16 UTC from IEEE Xplore.  Restrictions apply. On the Importance of Building High-quality Training Datasets for Neural Code Search ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
Table 4: Results of the manual inspection.
Type Rule #Likeness score
=0â‰¥1 Avg.
Origin -394,47179.0% 21.0% 0.27
Discarded by
Rule FilerHTML Tag 32,989100.0% 0.0% 0.00
Parentheses 19,30589.0% 11.0% 0.15
Javadoc Tags 47,10694.9% 5.1% 0.08
URLs 64097.3% 2.7% 0.05
Non-Eng. Lan. 6,503100.0% 0.0% 0.00
Punctuation 39,032100.0% 0.0% 0.00
Interrogation 516100.0% 0.0% 0.00
Short Sentence 15,18697.3% 2.7% 0.04
In total 161,27796.9% 3.1% 0.04
Discarded by
Model Filer- 93,45785.9% 14.1% 0.20
Retained -192,03159.2% 40.8% 0.61
be removed, a more refined rule can be derived. Also, the model-
basedsemanticfilterisaccompaniedbyalargersacrifice,indicating
it as a more challenging task.
Overall, through the two-phase filtering, the average likeness
scoreincreasesfrom0.27to0.61.Inparticular,theportionofnon-
query-likedatadropsfrom79.0%to59.2%,andtheportionofhighly
query-likedatascoringatleast1improvesfrom21%to40.8%.There
arestillmanycommentsinappropriatetobeseenascodesearch
queries, but our data cleaning framework makes a substantial con-
tribution to alleviating the situation. We call for more attention to
be drawn to overcoming related challenges.
AnswertoRQ2: Eachfilterandruleinourframeworkdemon-
strates a positive contribution to the effectiveness. The full
settingboostsittothebestperformance.However,therere-
mainmanyunqualifiedcommentsevenafterthefiltering,and
it calls for more attention to be paid from the community.
5.3 RQ3: Quality of dividing point determined
in the model-based filtering
In themodel-based filter, we useEM-GMM to decidethe dividing
pointbetweenthequalifiedandtheunqualifiedgroups.Toassess
thequalityofthedividingpoint,weobservethemodelperformanceresultingfromalternativedividingpoints,includingfixproportions
andtheonedecidedbyK-means,anotherwidelyusedclustering
algorithm.Forthefixedproportions,weseta25%stepandselect
25%, 50%, 75%, and 100% top-ranked comments, respectively.
TheresultsonDeepCSandCARLCSarereportedinTable5.For
DeepCS,EM-GMMoutperformsK-meansandthefixedproportionson all the validation sets. Compared with the second-best partition,
75%, EM-GMM still achieves higher average performances by 4.3%
A@1,4.2%A@5,4.3%A@10,and3.3%MRR.Thesuperiorityisalso
observed on CARLCS at every metric, and EM-GMM outperforms
K-means on average of CSN-v and NCSED by 30.4% A@1, 5.9%
A@5, 2.8% A@10, and 15.0% MRR.
EM-GMM ultimately retains 192,031 data points, accounting
for 67.3% of the original dataset, which locates between 57.5%, thedividingpointsetbyK-meansand75%.AsdiscussedinSection3.2.2,
the relation between the data quantity and the model performance
should be a convex function. According to the property of theconvex function, if there exists another optimal dividing point,it would locate between 57.5% and 75.0%. Therefore, EM-GMMsuccessfully identifies an optimal solution of the dividing point
with an error less than 9.8% (calculated by 67 .3%âˆ’57.5%).
Answer to RQ3: EM-GMM produces a better approximation
ofthebestdividingpointforthedatasetsandisadequateto
be used in the framework.
6 APPLICATION
Thissectionpresentstheapplicationsofourfilteringframework,
including a proof-of-concept data cleaning toolbox and a high-
quality code search dataset.
6.1 NLQF: Natural Language Query Filter
Wereleasetheimplementationofourfilteringframeworkasathird-
partyPythonlibrary, NaturalLanguageQueryFilter(NLQF ),which
is designed to systemically filter queries for neural code search
models. As alightweight library with convenient APIs, NLQFcan
be easily integrated into the development pipeline of any codesearch model. Besides, NLQFis extensible at several features to
ensure its applicability in a wide range of contexts:Extensible Ruleset
The ruleset in NLQFis configurable, which
enablesuserstospecifytherulesbasedonthecharacteristicsoftheirowndata.Besides, NLQFacceptsuser-definedfunctionsasapartof
rule-basedfiltering.Onecaneasilyextendthefilterimplementation
by creating the filtering function for any new rule.Open-sourceFilteringModel
NLQFrequiresatrainedVAEmodel
inthemodel-basedfilter.Wereleasethesourcecodefortrainingthe
VAEmodelusedinthispaper.Followingtheinstructions,userscan
easily train a new model with their own bootstrap query corpus,
which may boost the filtering performance further.
TunableDividingProportion Besidestherecommendedcluster-
ing method, EM-GMM, NLQFalso provides an interface accepting
user-defineddividingpoints.Userscancreatetheirownmethodfor
finding the dividing point and configure NLQFto adopt it easily.
6.2 COFIC: Codebase Paired with Filtered
Comments
We build and release a Codebase paired with FilteredComments
(COFIC) for Java programming language.
6.2.1 DatasetBuilding. WecollectthesourcecodeofJavareposito-
riesfromGithubaccordingtothelistmaintainedbyLibraries.io[ 37],
Fromthesefiles,weextractthemethodsandcorrespondingcom-
ments using the scripts provided by CodeSearchNet [ 19]. In the
end, 2,475,692 raw comment-code pairs are obtained. Through the
processing with NLQF, there are 1,048,519 data points left in the
cleanedquery-codedataset.Detailedstatisticsofthedatasetduring
filtering are reported in Table 6.
6.2.2 DatasetComparison. Wecompare COFIC,onthequeryqual-
ity,withseveralotherdatasetscurrentlyusedinneuralcodesearch
research. Following the same manual inspection convention as
1617
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:32:16 UTC from IEEE Xplore.  Restrictions apply. ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA Zhensu Sun, Li Li, Yan Liu, Xiaoning Du, and Li Li
Table 5: Results of changing the EM-GMM to other methods.
Model Dividing Point #CSN-v (434) CB (52) NCSED (287)
A@1 A@5 A@10 MRR A@1 A@5 A@10 MRR A@1 A@5 A@10 MRR
DeepCSPercentile (25%) 71,343134 240 285 0.420 22 31 34 0.500 40 80 115 0.215
Percentile (50%) 142,686 138 277 324 0.459 23 33 36 0.528 34 91 129 0.212
KMeans (57.5%) 164,194 146 274 321 0.471 23 34 39 0.530 39 99 127 0.229
EM-GMM (67.3%) 192,031 168 299 348 0.512 29 38 40 0.644 49 110 142 0.271
Percentile (75%) 214,029 160 284 332 0.505 28 36 38 0.604 47 108 138 0.266
Percentile (100%) 285,372 157 283 335 0.490 28 35 38 0.598 46 106 139 0.265
CARLCSPercentile (25%) 71,343 51 190 270 0.264 0 2 6 0.031 6 23 35 0.053
Percentile (50%) 142,686 57 217 290 0.295 1 2 5 0.031 8 32 48 0.070
KMeans (57.5%) 164,194 58 220 292 0.299 1 2 5 0.032 24 44 59 0.119
EM-GMM (67.3%) 192,031 62 221 296 0.302 2 4 7 0.056 35 58 74 0.168
Percentile (75%) 214,029 61 216 288 0.298 1 3 5 0.035 22 53 72 0.130
Percentile (100%) 285,372 57 211 288 0.293 1 2 5 0.039 18 57 74 0.122
Table 6: The statistics during the data filtering.
Step Rule #Discarded #Retained
Rule-basedHTML tags 189,250 2,475,692
Parentheses 129,130 2,475,692
Javadoc tags 423,313 2,052,379
URLs 3,119 2,049,260
Non-English Languages 67,943 1,981,317
Punctuation 201,881 1,779,436
Interrogation 3,300 1,776,136
Short Sentence 112,133 1,664,003
Model-based - 615,484 1,048,519
Table7:Acomparisonbetweenthetrainingdatasetsforcode
search tasks.
Dataset Source Language Likeness #
COFIC Github Java 0.52 1 M
CSN (Java)[19] Github Java 0.27 543 K
Hu et al.[18] Github Java 0.48 69 K
StaQC[52] StackOverflowPython 0.80 148 K
SQL 0.80 120 K
Barone et al.[6] Github Python 0.43 150 K
inSection5.2,theannotatorsratethequeriessampledfromeach
dataset, reported in Table 7. Again, we measure the agreement
level between the two annotators with Cohenâ€™s Kappa, which is0.73andwithintherangeoffairtogood.Amongallthedatasetscollected from Github, COFICreceives the highest score on data
quality, but there is still a gap compared with the StackOverflow
dataset, StaQC. Indeed, the datasetscollected from StackOverflow
havehigh-qualityqueries,buttheysufferfromtheunstablecode
quality in answers[ 45,55]. With our filtering framework, a Github
dataset with better quality is established.
Besidestheuser study,wealsoexperimentallycompare COFIC
with CSN-t. We train the DeepCS and CARLCS models with threedatasets: CSN-t, COFIC, and a controlled COFIC(same size as CSN-
t). The model trained with COFICoutperforms other experimental
settingsonthethreevalidationdatasets(CSV-v,CB,andNCSED).
The detailed results are reported in Table 8.
7 THREATS TO VALIDITY
Rule Design Though our experiments have evaluated the use-
fulness of each rule in the ruleset; the rule-based filter may still
introduceafewfalsepositivesorfalsenegativesduetoitsdesign
and implementation. For example, the widely used query â€œquick
sortâ€ can be filtered out by the rule Short Sentences. Besides, some
rules are tricky to be implemented exactly in line with our aim.For example, non-English letters in the comments are identified
based on ASCII encoding. It may leave out several other languages
also using English letters. But no English sentences will be falsely
filtered out. Overall, it requires further exploration on balancing
the trade-off between precision and recall better.BootstrapQueryCorpus
Thebootstrapquerycorpusinthiswork
is built based on the questions on StackOverflow. Only titles start-
ing with â€œhow toâ€ are collected into the corpus, which limits the
sentence pattern. The VAE model trained over this corpus may not
have a good tolerance to other patterns. Besides, StackOverflow
titlesarealsonotfullyquery-appropriate.Althoughwefilterthe
titles by rules, there are still semantically irrelevant texts left.Generalization
Limited by the accessibility of models and evalua-
tion benchmarks for code search tasks, we evaluate our solutiononly on Java datasets. In theory, our approach is capable of any
comment-based code search dataset. Yet, the generalization of our
filtering framework in different programming languages has not
beenexperimentallyverified.Besides,weonlyevaluateourfilter-
ing framework on two code search models, DeepCS and CARLCS,
which is also a threat to the generalizability of our approach.
8 RELATED WORK
CodeSearch Dataset Recentyearshavewitnessedagrowingin-
terest in the semantic search for code snippets [ 21]. DL models are
appliedtoestablishlinksbetweennaturallanguageandprogram-
minglanguage.Totrainthesemodels[ 7,9,15â€“17,28,30,40,41,46,
1618
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:32:16 UTC from IEEE Xplore.  Restrictions apply. On the Importance of Building High-quality Training Datasets for Neural Code Search ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
Table 8: The results of the experimental comparison between COFIC and CSN-t.
ModelTest Set #Query Train Set #Pairs A@1 A@5 A@10 MRR
DeepCSCSN-v 434CSN-t 394,471 123 248 306 0.407
COFIC (controlled) 394,471 188 52.8% â†‘297 19.8% â†‘344 12.4% â†‘0.555 36.3% â†‘
COFIC 1,048,519 19155.3%â†‘31426.6%â†‘35415.7%â†‘0.57741.9%â†‘
NCSED 287CSN-t 394,471 44 101 136 0.250
COFIC (controlled) 394,471 58 31.8% â†‘109 7.9% â†‘137 0.7% â†‘0.281 12.4% â†‘
COFIC 1,048,519 7263.6%â†‘11816.8%â†‘1488.8%â†‘0.32731.0%â†‘
CB 52CSN-t 394,471 25 31 38 0.522
COFIC (controlled) 394,471 34 36.0% â†‘38 22.6% â†‘39 2.6% â†‘0.693 32.6% â†‘
COFIC 1,048,519 3852.0%â†‘3925.8%â†‘417.9%â†‘0.74442.3%â†‘
CARLCSCSN-v 434CSN-t 394,471 54 210 292 0.283
COFIC (controlled) 394,471 66 22.2% â†‘237 12.9% â†‘319 9.2% â†‘0.328 15.8% â†‘
COFIC 1,048,519 6927.8%â†‘247 17.6% â†‘32210.3%â†‘0.33919.8%â†‘
NCSED 287CSN-t 394,471 20 34 57 0.105
COFIC (controlled) 394,471 34 70.0% â†‘68 100.0% â†‘87 52.6% â†‘0.172 64.1% â†‘
COFIC 1,048,519 47135.0%â†‘75120.6%â†‘9261.4%â†‘0.211101.1%â†‘
CB 52CSN-t 394,471 1 2 6 0.038
COFIC (controlled) 394,471 1-3 50.0% â†‘7 16.7% â†‘0.043 12.9% â†‘
COFIC 1,048,519 1-4100.0%â†‘716.7%â†‘0.05956.3%â†‘
47,51,53], code snippets paired with comments are collected from
Github [6,15,18,19,46]. According to a manual investigation[ 39],
there are 16 categories of comments in source code, most of which,
e.g., TODO, License, and Exception, are not appropriate to serve as
queries.Howev er,tothebestofourknowledge,thecommentsin
codesearchdatasetshaveneverbeenfullycleaned.Forexample,Barone et al. [
6] remove empty or non-alphanumeric lines from
the docstrings. CodeSearchNet [ 19] filters each comment-code pair
with its comment length. Ling et al. [ 30] use heuristic rules (e.g.,
theexistenceofverbandnounphrases)tofiltercomments.Cam-
bronero et al.[ 7] filterout queries that containspecific keywords.
These simple and scattered efforts are not enough to filter out the
various noises, especially the texts that are semantically unrelated
to real queries. Liu et al. [ 31] also mention that improving the data
quality is still a research opportunity for deep-learning-based code
search models, which well motivates our work.
There are two evaluation methods for neural code search re-
search: train-test split and actual user query evaluation. A lot of
works [7,41,47,53] split their datasets into train and test sets. The
queries of their test set contain the same defects as the train set so
thattheresultsfailtoreflectthemodelperformanceinanactual
environment.Manuallyreviewedqueries[ 19,24,50]canovercome
thisproblembuttheyareusuallyonasmallscaleandcannotserve
as the training dataset.UnsupervisedAnomalyDetection
Commentscleaningisanap-
plication of the unsupervised anomaly detection algorithm as la-
beledcommentsarenon-trivialtoobtain.Unsupervisedanomaly
detection algorithms identify the outliers solely based on the in-trinsic properties of the data instances. Various techniques canbe applied, such as Principal Component Analysis [
48], Genera-
tive Adversarial Network [ 23], Spatio Temporal Networks [ 10] and
LSTM[42].Amongthem,Auto-Encoder(AE)isthefundamental
architecture for unsupervised anomaly detection [ 38]. It has been
applied in many tasks. For example, Zhang et al. [ 56] detect therumorsinsocialmediausingmulti-layerAE.Castellinietal.[ 8]ap-
plyAEtodetectfalsefollowersonTwitter.LuoandNagarajan[ 34]
useAEtoidentifytheerroreventsofinterestsuchasequipment
faults and undiscovered phenomena in wireless sensor networks.
The encoder of AE maps an input to a point in the latent space,
while VAE maps an input to a region. In this way, VAE can extract
moreabstractsemanticfeatures.Ithasbeenappliedtounsupervisedanomalydetectionwithpromisingevaluationscores[
5,29,43,49].
9 CONCLUSION
Weproposethefirstdatacleaningframeworkforcodesearchtasks,
which improves the quality and naturalness of the queries. The
framework leverages two subsequent filters, the rule-based syntac-
tic filter, and the model-based semantic filter. The rule-based filteruses configurable heuristics rules to filter out comments with syn-
tactic anomalies. The model-based filter aims to refine the dataset
semantically. It trainsa VAE model over apre-collected bootstrap
querycorpus,andexploitsittoselectcommentswithsmallerre-
constructionlosses.Experimentsshowthatourfilteringframework
can significantly save computing resources and improve the model
accuracy. Finally, we release our framework as a Python library
NLQFand make public a high-quality cleaned code search dataset
COFIC, to facilitate relevant research in academia and industry.
REFERENCES
[1] 2021. Github. Retrieved Sep 1, 2021 from https://github.com/
[2]2021.How to Write Doc Comments for the Javadoc Tool . Retrieved Sep 1,
2021 from https://www.oracle.com/technical-resources/articles/java/javadoc-
tool.html#styleguid
[3]2021.On the Importance of Building High-quality Training Datasets for Neural
Code Search. Retrieved Sep 1, 2021 from https://sites.google.com/view/hqtd
[4] 2021. StackOverflow. Retrieved Sep 1, 2021 from https://stackoverflow.com/
[5]Jinwon An and S. Cho. 2015. Variational Autoencoder based Anomaly Detection
using Reconstruction Probability.
1619
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:32:16 UTC from IEEE Xplore.  Restrictions apply. ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA Zhensu Sun, Li Li, Yan Liu, Xiaoning Du, and Li Li
[6]Antonio Valerio Miceli Barone and Rico Sennrich. 2017. A Parallel Corpus of
Python Functions and Documentation Strings for Automated Code Documenta-
tion and Code Generation. ArXivabs/1707.02275 (2017).
[7]JosÃ© Cambronero, Hongyu Li, S. Kim, K. Sen, and S. Chandra. 2019. When deep
learning met code search. Proceedings of the 2019 27th ACM Joint Meeting on
European Software Engineering Conference and Symposium on the Foundations of
Software Engineering (2019).
[8]Jacopo Castellini, V. Poggioni, and Giulia Sorbi. 2017. Fake Twitter followers
detection by denoising autoencoder. Proceedings of the International Conference
on Web Intelligence (2017).
[9]Q.ChenandMinghuiZhou.2018. ANeuralFrameworkforRetrievalandSum-
marization of Source Code. 2018 33rd IEEE/ACM International Conference on
Automated Software Engineering (ASE) (2018), 826â€“831.
[10]DanChianucciandA.Savakis.2016. UnsupervisedchangedetectionusingSpatial
Transformer Networks. 2016 IEEE Western New York Image and Signal Processing
Workshop (WNYISPW) (2016), 1â€“5.
[11]KyunghyunCho,B.V.Merrienboer,Ã‡aglarGÃ¼lÃ§ehre,DzmitryBahdanau,Fethi
Bougares,HolgerSchwenk,andYoshuaBengio.2014. LearningPhraseRepresen-
tationsusingRNNEncoder-DecoderforStatisticalMachineTranslation. ArXiv
abs/1406.1078 (2014).
[12] William G Cochran. 1977. Sampling techniques. Wiley Eastern Limited.
[13]Cohen and J. 1960. A Coefficient of Agreement for Nominal Scales. Educational
&Psychological Measurement 20, 1 (1960), 37â€“46.
[14]A.Dempster,N.Laird,andD.Rubin.1977. Maximumlikelihoodfromincomplete
data via the EM - algorithm plus discussions on the paper.
[15]XiaodongGu,H.Zhang,andS.Kim.2018. DeepCodeSearch. 2018IEEE/ACM
40th International Conference on Software Engineering (ICSE) (2018), 933â€“944.
[16]Rajarshi Haldar, L. Wu, Jinjun Xiong, and J. Hockenmaier. 2020. A Multi-
PerspectiveArchitectureforSemanticCodeSearch. ArXivabs/2005.06980(2020).
[17]Gang Hu, Min Peng, Yihan Zhang, Qianqian Xie, and Mengting Yuan. 2020.
Neural joint attention code search over structure embeddings for software Q &A
sites.J. Syst. Softw. 170 (2020), 110773.
[18]X. Hu, G. Li, Xin Xia, D. Lo, and Zhi Jin. 2018. Deep Code Comment Generation.
2018 IEEE/ACM 26th International Conference on Program Comprehension (ICPC)
(2018), 200â€“20010.
[19]H. Husain, Ho-Hsiang Wu, Tiferet Gazit, Miltiadis Allamanis, and Marc
Brockschmidt.2019. CodeSearchNetChallenge:EvaluatingtheStateofSemantic
Code Search. ArXivabs/1909.09436 (2019).
[20] Glenn D Israel. 1992. Determining sample size. (1992).
[21]Kisub Kim, Dongsun Kim, TegawendÃ© F BissyandÃ©, Eunjong Choi, Li Li, Jacques
Klein, and Yves Le Traon. 2018. FaCoY - A Code-to-Code Search Engine. In The
40th International Conference on Software Engineering (ICSE 2018).
[22]Diederik P. Kingma and M. Welling. 2014. Auto-Encoding Variational Bayes.
CoRRabs/1312.6114 (2014).
[23]W. Lawson, Esube Bekele, and K. Sullivan. 2017. Finding Anomalies with Gener-
ativeAdversarialNetworksforaPatrolbot. 2017IEEEConferenceonComputer
Vision and Pattern Recognition Workshops (CVPRW) (2017), 484â€“485.
[24]HongyuLi,S.Kim,andS.Chandra.2019. NeuralCodeSearchEvaluationDataset.
ArXivabs/1908.09804 (2019).
[25]LiLi,TegawendÃ©FBissyandÃ©,YvesLeTraon,andJacquesKlein.2016. Access-
ing Inaccessible Android APIs: An Empirical Study. In The 32nd International
Conference on Software Maintenance and Evolution (ICSME 2016).
[26]Li Li, Jun Gao, TegawendÃ© F BissyandÃ©, Lei Ma, Xin Xia, and Jacques Klein. 2018.
CharacterisingDeprecatedAndroidAPIs.In The15thInternationalConferenceon
Mining Software Repositories (MSR 2018).
[27]Li Li, Jun Gao, TegawendÃ© F BissyandÃ©, Lei Ma, Xin Xia, and Jacques Klein. 2020.
CDA:Characterising DeprecatedAndroidAPIs. EmpiricalSoftware Engineering
(EMSE)(2020).
[28]W. Li, Haozhe Qin, Shuhan Yan, Beijun Shen, and Y. Chen. 2020. Learning
Code-QueryInteractionforEnhancingCodeSearches. 2020IEEEInternational
Conference on Software Maintenance and Evolution (ICSME) (2020), 115â€“126.
[29]Shuyu Lin, R. Clark, R. Birke, Sandro SchÃ¶nborn, Niki Trigoni, and S. Roberts.2020. Anomaly Detection for Time Series Using VAE-LSTM Hybrid Model.
ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal
Processing (ICASSP) (2020), 4322â€“4326.
[30]ChunyangLing,ZeqiLin,YanzhenZou,andBingXie.2020. AdaptiveDeepCodeSearch.Proceedingsofthe28thInternationalConferenceonProgramComprehension
(2020).
[31]C.Liu,XinXia,DavidLo,CuiyunGao,XiaohuYang,andJ.Grundy.2020. Op-
portunities and Challenges in Code Search Tools. ArXivabs/2011.02297 (2020).
[32]Chao Liu, Xin Xia, David Lo, Zhiwei Liu, A. Hassan, and Shanping Li. 2020.
Simplifying Deep-Learning-Based Model for Code Search. ArXivabs/2005.14373
(2020).
[33]Pei Liu, Li Li, Yichun Yan, Mattia Fazzini, and John Grundy. 2021. Identifyingand Characterizing Silently-Evolved Methods in the Android API. In The 43rd
ACM/IEEEInternationalConferenceonSoftwareEngineering,SEIPTrack(ICSE-SEIP
2021).[34]Tie Luo and Sai Ganesh Nagarajan. 2018. Distributed Anomaly Detection Using
AutoencoderNeuralNetworksinWSNforIoT. 2018IEEEInternationalConference
on Communications (ICC) (2018), 1â€“6.
[35]BhaskarMitra,NickCraswell,etal .2018.Anintroductiontoneuralinformation
retrieval. Now Foundations and Trends.
[36]SeyedMehdiNasehi,JonathanSillito,F.Maurer,andC.Burns.2012. Whatmakes
a good code example?: A study of programming Q &A in StackOverflow. 2012
28thIEEEInternationalConferenceonSoftwareMaintenance(ICSM) (2012),25â€“34.
[37]A.NesbittandBenjaminNickolls.2017. Libraries.ioOpenSourceRepositoryand
Dependency Metadata.
[38]Raghavendra Chalapathy University of Sydney, Capital Markets Cooperative Re-
searchCentre,SanjayChawlaQatarComputingResearchInstitute,andHbku.
2019. Deep Learning for Anomaly Detection: A Survey.
[39]Luca Pascarella,Magiel Bruntink, and AlbertoBacchelli. 2019. Classifying code
comments in Java software systems. Empirical Software Engineering 24, 3 (June
2019), 1499â€“1537. https://doi.org/10.1007/s10664-019-09694-w
[40]ZhuQihao,SunZe-yu,LiangXiran,XiongYingfei,andZ.Lu.2020. OCoR:An
Overlapping-Aware Code Retriever. arXiv: Computation and Language (2020).
[41]JianhangShuai,LingXu,ChaoLiu,MengYan,XinXia,andYanLei.2020. Im-
proving Code Search with Co-Attentive Representation Learning. Proceedings of
the 28th International Conference on Program Comprehension (2020).
[42]A.Singh.2017. AnomalyDetectionforTemporalDatausingLongShort-Term
Memory (LSTM).
[43]Suwon Suh, Daniel H. Chae, Hyon-Goo Kang, and S. Choi. 2016. Echo-stateconditionalvariationalautoencoderforanomalydetection. 2016International
Joint Conference on Neural Networks (IJCNN) (2016), 1015â€“1022.
[44]Zhensu Sun, Yan Liu, Chen Yang, and Yu Qian. 2020. PSCS: A Path-based Neural
Model for Semantic Code Search. arXiv preprint arXiv:2008.03042 (2020).
[45]Valerio Terragni, Yepang Liu, and S. C. Cheung. 2016. CSNIPPEX: automatedsynthesisofcompilablecodesnippetsfromQ&Asites. Proceedingsofthe25th
International Symposium on Software Testing and Analysis (2016).
[46]Yao Wan, Jingdong Shu, Yulei Sui, Guandong Xu, Zhou Zhao, Jian Wu, and
PhilipS.Yu.2019. Multi-modalAttentionNetworkLearningforSemanticSource
Code Retrieval. 2019 34th IEEE/ACM International Conference on Automated
Software Engineering (ASE) (2019), 13â€“25.
[47]W. Wang, Y. Zhang, Zhengran Zeng, and Guandong Xu. 2020. TranS Ë†3: A
Transformer-based Framework for Unifying Code Summarization and Code
Search.ArXivabs/2003.03238 (2020).
[48]S. Wold, K. Esbensen, and P. Geladi. 1987. Principal component analysis. Chemo-
metrics and Intelligent Laboratory Systems 2 (1987), 37â€“52.
[49]Haowen Xu, Wenxiao Chen, N. Zhao, Z. Li, Jiahao Bu, Zhihan Li, Y. Liu, Y.
Zhao, D. Pei, Y. Feng, Jian Jhen Chen, Zhaogang Wang, and Honglin Qiao. 2018.
Unsupervised Anomaly Detection via Variational Auto-Encoder for SeasonalKPIsinWebApplications. Proceedingsofthe2018WorldWideWebConference
(2018).
[50]Shuhan Yan, H. Yu, Y. Chen, Beijun Shen, and L. Jiang. 2020. Are the CodeSnippets What We Are Searching for? A Benchmark and an Empirical Studyon Code Search with Natural-Language Queries. 2020 IEEE 27th International
Conference on Software Analysis, Evolution and Reengineering (SANER) (2020),
344â€“354.
[51]Ziyu Yao, Jayavardhan Reddy Peddamail, and Huan Sun. 2019. CoaCor: Code
AnnotationforCodeRetrievalwithReinforcementLearning. TheWorldWide
Web Conference (2019).
[52]ZiyuYao,DanielS.Weld,W.Chen,andHuanSun.2018. StaQC:ASystematicallyMinedQuestion-CodeDatasetfromStackOverflow. Proceedingsofthe2018World
Wide Web Conference (2018).
[53]WeiYe,RuiXie,JingleiZhang,TianxiangHu,XiaoyinWang,andShikunZhang.2020. LeveragingCodeGenerationtoImproveCodeRetrievalandSummarization
via Dual Learning. Proceedings of The Web Conference 2020 (2020).
[54]PengchengYin,BowenDeng,EdgarChen,BogdanVasilescu,andGrahamNeubig.
2018. Learning toMine Aligned Code and Natural Language Pairs from StackOverflow. 2018 IEEE/ACM 15th International Conference on Mining Software
Repositories (MSR) (2018), 476â€“486.
[55]Tianyi Zhang, Ganesha Upadhyaya, Anastasia Reinhardt, Hridesh Rajan, andMiryung Kim. 2018. Are Code Examples on an Online Q&A Forum Reliable?:
A Study of API Misuse on Stack Overflow. 2018 IEEE/ACM 40th International
Conference on Software Engineering (ICSE) (2018), 886â€“896.
[56]Y.Zhang,WeilingChen,C.Yeo,C.Lau,andB.Lee.2017. Detectingrumorson
Online SocialNetworks using multi-layerautoencoder. 2017 IEEETechnology &
Engineering Management Conference (TEMSCON) (2017), 437â€“441.
[57]Yanjie Zhao, Li Li, Haoyu Wang, Haipeng Cai, Tegawende Bissyande, Jacques
Klein, and John Grundy. 2021. On the Impact of Sample Duplication in Machine
Learning based Android Malware Detection. ACM Transactions on Software
Engineering and Methodology (TOSEM) (2021).
1620
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:32:16 UTC from IEEE Xplore.  Restrictions apply. 