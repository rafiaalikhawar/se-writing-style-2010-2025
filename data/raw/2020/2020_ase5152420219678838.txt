Nekara: Generalized Concurrency Testing
Udit Agarwal*, Pantazis Deligiannis+, Cheng Huang++, Kumseok Jung**, Akash Lal+, Immad Naseer++
Matthew Parkinson+, Arun Thangamani+, Jyothi Vedurada#, Yunpeng Xiao++
*IIIT Delhi,+Microsoft Research,++Microsoft,**University of British Columbia,#IIT Hyderabad
Abstract —Testing concurrent systems remains an uncomfort-
able problem for developers. The common industrial practice is
to stress-test a system against large workloads, with the hopeof triggering enough corner-case interleavings that reveal bugs.However, stress testing is often inefﬁcient and its ability to getcoverage of interleavings is unclear. In reaction, the researchcommunity has proposed the idea of systematic testing, where a
tool takes over the scheduling of concurrent actions so that it canperform an algorithmic search over the space of interleavings.
We present an experience paper on the application of systematic
testing to several case studies. We separate the algorithmicadvancements in prior work (on searching the large space ofinterleavings) from the engineering of their tools. The latter wasunsatisfactory; often the tools were limited to a small domain,hard to maintain, and hard to extend to other domains. Wedesigned Nekara, an open-source cross-platform library for easilybuilding custom systematic testing solutions.
We show that (1) Nekara can effectively encapsulate state-
of-the-art exploration algorithms by evaluating on prior bench-marks, and (2) Nekara can be applied to a wide variety ofscenarios, including existing open-source systems as well asproduction distributed services of Microsoft Azure. Nekara waseasy to use, improved testing, and found multiple new bugs.
I. I NTRODUCTION
Exploiting concurrency is fundamental to building scalable
systems. It can range from desktop applications with multiple
threads that exploit a multi-core CPU or giant distributedsystems with multiple processes spanning many VMs. In eithercase, getting the concurrency right is challenging. The combi-natorial explosion of possible interleavings between concurrentactions makes it hard to ﬁnd bugs, or even reproduce knownones [1], [2]. Prior work argues for systematic testing, which
hooks on to the concurrency in a program to reliably controlthe interleaving of concurrent actions, and then orchestratesa search (exhaustive or randomized) within the space of allinterleavings. This idea manifests in several tools, such asCHESS [2], [3] and Cuzz [4] for multi-threaded applications,dBug [5], Modist [6] and SAMC [7] for distributed message-passing systems, and many others [8], [9], [10], [11], [12].
This paper summarizes our experience in applying system-
atic testing in practice. Concurrency comes in many forms; itchanges with the programming language (C#, Go, Rust, etc.),programming model (e.g., threads, tasks, actors, async-await,etc.), framework (e.g., Service Fabric [13], libevent [14], Trio[15], etc.) and so on. Our immediate learning in this space wasthat each of the tools mentioned above only address a speciﬁcclass of programs, and are difﬁcult or impossible to apply toother classes of programs.
To understand this shortcoming, we can examine the design
of a typical solution. It consists of three parts. The ﬁrst isalgorithmic search: a collection of heuristics that guide thesearch to interesting parts where bugs can be found, forinstance, fewer context switches ﬁrst [3], or priority-basedscheduling [4], etc. Second is the modeling of supported con-
currency primitives that specify their semantics, for instance,the behavior of spawning a thread or acquiring a semaphore,etc. Last is the injection of these models into a program so
that calls to the original primitives can be replaced by callsto their corresponding models, helping take over schedulingdecisions at runtime.
Prior work attempts to offer integrated solutions that address
all three aspects for a particular domain. For instance, theCHESS tool supports a subset of C# threading APIs (from theSystem.Threading namespace). Their models are build
into the tool itself, and they are injected automatically into theprogram binary via binary instrumentation. This offers a seam-less experience for supported programs. However, there is noeasy way to extend the tool to programs outside this supportedclass. Even C# Task-based or async-await programs, which
are very commonplace, are not supported. Furthermore, use ofcomplex technology like binary instrumentation makes the toolhard to maintain. Other tools do not fare any better. SAMC, forinstance, only supports Sockets-based communication that
too only for ZooKeeper-like distributed systems. Supporting
other systems was arguably never a goal of that work.
This paper aims to democratize systematic testing by al-
lowing users (i.e., developers) to build their own systematictesting solutions. We propose Nekara, a cross-platform open-source library that provides a simple, yet expressive API thata developer can use to model the set of concurrency primitivesthat they have used in their program. Nekara encapsulates var-ious search heuristics from prior work, freeing the developerfrom having to design these themselves. Nekara can recordscheduling decisions and provide full repro for bugs.
Unlike an integrated solution, Nekara delegates modeling, as
well as injection of these models, to the developer. We argue,through experience presented in this paper, that the burden ondevelopers is small, and is out-weighted by many engineeringbeneﬁts. In most cases, applications depend on a well-deﬁnedframework or library to provide concurrency support (e.g.,pthreads). The user can then only design models for theseframework APIs (only ones they used) with Nekara hooks,and the rest of the system remains unchanged.
Importantly, these Nekara-enabled models are just code;
they live and evolve alongside the rest of the system, andbeneﬁt from standard engineering practices. The models canbe easily injected for testing, say through macros for C code or
6792021 36th IEEE/ACM International Conference on Automated Software Engineering (ASE)
DOI 10.1109/ASE51524.2021.000662021 36th IEEE/ACM International Conference on Automated Software Engineering (ASE) | 978-1-6654-0337-5/21/$31.00 ©2021 IEEE | DOI: 10.1109/ASE51524.2021.9678838
978-1-6654-0337-5/21/$31.00  ©2021  IEEE
mocking utilities for higher-level languages like C#. (Binary
instrumentation is an overkill.) They can be shared betweenmultiple teams that use the same form of concurrency, or acommunity can contribute to support models for a popularframework like pthreads. Developers can even choose to
limit their code to only those APIs for which they have thesemodels, given the clarity that Nekara provides.
Nekara also helps with other forms of non-determinism as
well (such as failure injection), and provides the same beneﬁtsof reproducibility. In all, with Nekara, systematic testingof non-determinism is just another programming exercise,completely in the hands of the developer.
We show the effectiveness of Nekara by documenting
several case studies, each time showcasing that modelingconcurrency requires minimal effort (less than a few weeks,easy to share) and provides signiﬁcant value for testing (morebugs found). Nekara is just 3K lines of C++ code.
1We believe
that a solution like Nekara unlocks the potential of systematictesting that has so far been siloed in individual tools.
The main contributions of this paper are as follows.
•The design of the Nekara library that allows buildingcustom systematic-testing solutions (Sections II and III).
•Experience from several case studies that cover a rangeof different forms of concurrency and non-determinism(Sections IV to VI). Nekara has been adopted by Mi-crosoft Azure to test distributed services (Sections VI-Aand VI-B).
•The search algorithms in Nekara are inspired from pre-vious work; we show that the generality of Nekara doesnot limit its bug-hunting abilities (Section VII).
Section IX discusses related work and Section X concludes.
II. N
EKARA LIBRARY
This section illustrates how Nekara can be used to build a
systematic testing solution. Nekara is only meant for testing.Production code need not take a dependency on Nekara.
The core Nekara APIs are shown in Figure 1. Nekara op-
erations generalize over threads and resources generalize over
synchronization. Operations and resources are uninterpreted,each is just an integer. Nekara understands that each operation,once started and before it ends, executes concurrently withrespect to other operations, unless it blocks on a resource.
Operations while executing can additionally signal a resource,
which unblocks alloperations that may be blocked on it.
Nekara implements a co-operative scheduler (§III). It en-
sures that at most one operation executes at any pointin time. The current operation continues running until itcallsschedule_next orwait_resource. At that point,
Nekara can switch to execute some other enabled operation.
It is up to the programmer to map the concurrency in
their program to Nekara operations and resources. We explainthis exercise using the simple concurrent program shown inFigure 2. The program contains a test case, which executesthe method foo using multiple threads and does an assertion
1Available open source at https://github.com/microsoft/coyote-scheduler// Starting/stopping the schedulervoid attach();
void detach();
// Operationsvoid create_operation();
void start_operation( int op);
void end_operation( int op);// Resourcesvoid wait_resource( int rid);
void signal_resource( int rid);
// Co-operative context switchvoid schedule_next();
// Nondeterministic choicesint next_integer( int min, int max);
Fig. 1: Core APIs of the Nekara library.
at the end. Clearly, the assertion can fail, but only in a speciﬁcinterleaving where thread t1updates the shared variable x
last. For this program, it makes sense to map threads tooperations and locks to resources. Our goal will be to simplymock the concurrency-related APIs and leave the rest of theprogram unchanged as much as possible.
Figure 3 shows the mocks. The
create_thread_wrapper function calls
create_operation from the parent thread and then
start_operation in the child thread. The former informs
Nekara that a new operation is about to be created, whereasthe latter allows Nekara to take control of the child thread.The actual thread is still spawned via create_thread;
Nekara does not provide any runtime of its own.
Next step is to implement the synchronization via resources.
We map each lock to a unique resource, and then acquire
andrelease methods can be mocked as shown in Figure 3.
For ease of explanation, we have assumed that the resourceid corresponding to a lock object xis stored in the ﬁeld
x->id, and a Boolean variable representing the status of thelock (locked/unlocked) is stored in the ﬁeld x->acquired.
Because Nekara performs co-operative scheduling, imple-menting synchronization is typically easy. The procedureacquire_wrapper ﬁrst does a context switch to give other
operations the chance to acquire the lock, and then goesahead to grab the lock if it is available. When the lock isnot available, then the operation blocks on the correspondingresource. In release_wrapper, we signal the resource, so
that blocked operations can try to grab the lock again. (Notethat signalling a resource unblocks all waiting operations, butNekara ensures that only one unblocked operation executes atany point in time.) Any other way of implementing a lockis acceptable, as long as it ensures that the only blockingoperation is wait_resource.
The ﬁnal step is to run the test in a loop for a desired number
of iterations, under the control of Nekara’s scheduler, as shownin Figure 4. Nekara uses search heuristics to explore thespace of interleavings of the underlying test. Nekara recordsthe set of scheduling decisions that it makes. When a testiteration fails, the sequence of scheduling decisions can besupplied back to the Nekara scheduler to directly reproduce thebuggy iteration. The default set of search heuristics in Nekaraare randomized, so we measure the effectiveness of Nekaraas the percentage of buggy iterations, i.e., what fraction of
680Modeling
Project LoC Operations Resources LoC #PW
Memcached (§IV) 21K pthread_t pthread_mutex_t, pthread_cond_t, libevent::event_loop 1K 2
Verona (§V) 8K std::thread std::condition_variable, std::atomic 302 n/a
CSCS (§VI-A) 56K Task TaskCompletionSource<T> 3K 4
ECSS (§VI-B) 44K Task TaskCompletionSource<T>, lock 3K 4
Coyote (§VII) 27K Actor EventQueue 16 <1
TABLE I: Systems integrated with Nekara.
int x; // Shared variable
LCK lock; // Mutex
void test() {
  // Initialization
  x = 0; lock = new LCK();
  // Run workload  t1 = create_thread(&foo, 1);  t2 = create_thread(&foo, 2);  t3 = create_thread(&foo, 3);  // Continues in right column.  thread_ join(t1);  thread_ join(t2);  thread_ join(t3);  assert(x != 1);   
}
void foo(int arg) {
  acquire(lock);
  x = arg;  release(lock);}
Fig. 2: A simple concurrent program.
invocations of the underlying test fail.
Completeness, Limitations: A natural question is if
Nekara’s exploration is complete, i.e., if it is possible to
explore all behaviors of a given test in the limit. With Nekara,
we leave this decision to the developer. Completeness can beachieved by inserting a context switch just before each non-
commuting action [16]. For message-passing programs, non-
commuting actions are typically just message transfers, thusinstrumenting them is enough to get completeness.
In shared-memory programs, an access to shared memory
is potentially non-commuting. One option is for a developerto implement their own methodology for inserting contextswitches at memory accesses in their code. A second simpler(and common [2]) solution is to only insert at synchronizationAPIs. Then the instrumentation remains limited to mocks ofthe synchronization APIs, and does not pollute the code. Onecase study (§V) uses the former strategy whereas rest usethe latter strategy. With Nekara, the aim is not necessarilyto ﬁnd all bugs (it’s a testing solution after all), but rather tosigniﬁcantly improve existing practice. Behaviors induced byweak memory models [17] are also outside the scope of thispaper (although interesting future work).
Nondeterminism: The call to next_integer returns
a randomly-generated integer within the supplied range. Thereturned value is recorded to allow for replay. This is handy formodeling non-determinism such as failure injection (§VI-A) orabstracting branches (§V).
Case Studies: We demonstrate Nekara on various sys-
tems (Table I). Memcached [18] is a popular open-sourcein-memory key-value store with cache management. Verona[19] is a language runtime written in C++. CSCS and ECSSare production cloud services of Microsoft Azure, writtenin C#. Coyote [20] provides a C# actor-based programmingframework for building distributed systems. The third andint create_thread_wrapper(
    FUNC_PTR foo,    ARGS_PTR args) {  int op = create_new_op();
  scheduler. create_operation (op);
  create_thread(starter, (op,    foo, args));}
void starter( int op, FUNC_PTR foo,
    ARGS_PTR args) {
  scheduler. start_operation (op);
  foo(args);  scheduler. end_operation (op);
}void acquire_wrapper(LCK lock) {
  scheduler. schedule_next ();
  while (true) {
    if (lock.acquired == true) {
      scheduler. wait_resource (lock->id);
    } else {      lock.acquired = true; break ;
    }} }
void release_wrapper(LCK lock) {
   assert(lock.acquired);
   lock.acquired = false ;
   scheduler. signal_resource (lock->id);
}
Fig. 3: Mocks for thread creation, and lock acquire-release.
void nekara_test() {
  Scheduler scheduler(options);  for (int i = 0; i < 100; i++) {
    scheduler. attach (); // Start the scheduler
    test(); // Run the test for iteration i
    scheduler. detach (); // Stop the scheduler
} }  
Fig. 4: A typical Nekara test running for 100iterations.
fourth columns of Table I list what is mapped to Nekaraoperations and resources, respectively. The last two columnsshow the modeling effort: lines of code (LoC) as well as thenumber of person weeks (#PW) spent for modeling. Veronaused custom Nekara modeling from the outset, so we cannotquantify the effort. Each of CSCS and ECSS use the samemocks, which were developed once in 4person weeks. It is
worth noting that Nekara testing is an integral part of theengineering process for Verona, CSCS and ECSS.
III. N
EKARA IMPLEMENTATION
The Nekara scheduler must be attached before it takes
any action; once detached, all Nekara APIs are no-ops. A
simpliﬁed implementation of the core Nekara APIs is shownin Algorithm 1. Nekara also includes APIs for joining onan operation, blocking on a conjunction or a disjunction ofresources, as well as signalling a particular operation. Theseadditional APIs are not discussed here.
Nekara must ensure that only one operation executes at
any point in time, and it must block the rest, to give itprecise control of the program’s execution. Nekara maintains:the current operation o
cur, a map M:O→Rthat maps
681Algorithm 1: Nekara Scheduling APIs
State: int cnt pending ,ocur;M:O→R
1Procedure create_operation(o)
2 O←O∪{o}; M[o]←∅ ;atomic{cnt pending ++}
3Procedure start_operation(o)
4 atomic{cnt pending−− }
5 o.cv.wait() // cv is a condition variable
6Procedure schedule_next()
7 while cnt pending >0dowait()
8E←{ o|M[o]=∅}// collect enabled operations
9 ifE=∅then raise deadlock
10 onxt←S .next(E )// choose next operation
11 ifocur=onxt then return
12 oprev←ocur;ocur←onxt
13 onxt.cv.notify() // resume next operation
14 ifoprev∈E then
15 oprev.cv.wait() // pause previous operation
16Procedure wait_resource(r)
17 M[ocur]←{r};schedule_next()
18Procedure signal_resource(r)
19 foreach o∈Odo
20 M[o]←M[o]\{r}
an operation to a set of resources that it’s currently blocked
on, and a counter cnt pending that is initialized to zero. An
operation oisenabled ifM[o]is empty, and is disabled
otherwise. Nekara creates a condition variable o.cvfor each
operation othat it uses to block the operation.
Calling create_operation atomically increments
cnt pending to keep track of operations that Nekara should
expect to be spawned. No other action is taken, whichmeans that the current operation o
curwill continue executing.
start_operation decrements cnt pending and immediately
blocks the caller. Having these two calls decorating the spawnof a new operation was important to keep Nekara independentof the underlying runtime. A call to start_operation
can happen concurrently with respect to other Nekara API.
The executing operation o
curcontinues until it calls either
schedule_next orwait_resource. The latter just calls
the former, so we just describe schedule_next. It ﬁrst
waits until cnt pending goes to zero. (Once this happens, it
implies that all operations must be inside Nekara.) Nekarathen uses a search heuristic Sto decide the next operation
to schedule from the set of all enabled operations (or raise“Deadlock” is none is enabled). If the next operation isthe same as the current, no action is necessary. Otherwise,the current operation is blocked and the next operation isscheduled.
Search heuristics: Several search heuristics have been
proposed in prior work; Thomson et al. [12] provides a survey.These heuristics are based around empirical observations ofwhere most bugs lie in practice: for instance, prioritize fewcontext switches [3], few delays [21], few priority-exchangepoints [4], [9], etc.; even selecting the next operation uniformlyat random works well in practice [12]. Nekara, by default, hasseveral heuristics implemented and uses all of them in a round-robin fashion (across test iterations).Network
Slabs...Application Programming Interface (API)
Slab RebalancerLRU 
Crawler
Slab 
classes
LRU...
...
...LRU 
MaintainerWorker
ThreadsWorker
ThreadsMemcached
lab 
asses
Threads
 Crawler
s
 Maintainer
 wler
 M
 Cra
 Threads
 s
KV Hash TableAssoc
MaintainerWorker
Threads
 Maintainer
 r
er
 Main
 Thread
 M
ds
Fig. 5: Architecture of Memcached.
Nekara captures the crux of systematic testing in a small,
simple C++ library. It separates search heuristics from themodeling and specialization of testing to a particular system.
IV . C
ASE STUDY :M EMCACHED
Memcached [18] (MC) is an open-source, in-memory key-
value store commonly used as a cache between an applicationand a database. It is mostly written in C with approximately21 KLOC. Being a popular multi-threaded application, it hasbeen used for benchmarking bug-detection tools in prior work[22], [23], [24], [25].
Figure 5 illustrates the high-level architecture of MC, show-
ing the key data structures at the bottom and the various kindsof threads that access them. MC maintains an in-memory,chained hash table for indexing key-value (KV) pairs. Worker
threads update the hash table and the assoc maintainer thread
is responsible for expanding or shrinking the hash table whenits load crosses above or below certain threshold.
To reduce internal fragmentation, MC uses a slab-based
allocator for storing KV pairs. KV pairs of different sizesare mapped to different slab classes, and the slab rebalancer
thread redistributes memory among slab classes as needed.
For every slab class, MC maintains three linked lists, namedhot, warm and cold LRUs, to keep track of the least recently
used (LRU) KV pairs. When a slab class runs out of memory,some memory is reclaimed by evicting the least recently usedKV pair. Two background threads, called LRU maintainer and
LRU crawler, update these linked lists and performs evictionswhenever required.
The dispatcher thread is the main thread that starts and stops
all other threads. Its primary function is to look for incom-ing network connections and dispatch them to the availableworker threads, which then serve the network client’s requests.MC relies on an event-driven, asynchronous model based onlibevent [14]; worker threads do not block on network I/O,
but instead switch to processing another request.
Integrating Nekara: We ﬁrst needed to make MC
more modular and unit-testable. We mocked system callslike socket, sendmsg, recvmsg, getpeername,
poll, read, write, etc. so that we could imi-
682KnownId Bug Type Description Reference
1 Misuse of pthread API Double initialization of mutex and conditional variables PR#566
2 Data Race Dispatcher and worker thread race on a shared ﬁeld named stats PR#573
3 Misuse of pthread API Deadlock due to recursive locking of a non-recursive mutex PR#560
4 Misuse of pthread API Deadlock due to an attempt to pthread_join a non-existing thread Issue#685
5 Atomicity Violation Two worker threads simultaneously increment the value of the same KV pair Issue#127New6 Atomicity violationNull pointer dereference when one worker thread deletes a global variable
while another worker thread was updating itIssue#728
7 Misuse of pthread API Attempt to join a non-existant thread Issue#733
8 DeadlockSlab rebalancer and the dispatcher thread deadlock due to simultaneous invocation
ofpthread_cond_signal by a worker and the dispatcher thread.Issue#738
9 Misuse of pthread APIThis bug can lead to either unlocking an unlocked mutex or unlocking a
mutex that is held by some other thread (resulting in data races).Issue#741
TABLE II: List of previously known bugs, as well as new bugs, that Nekara found in Memcached.
Bug#2 Bug#5 Bug#6 Bug#8
Uncontrolled  0.12% 
Nekara 4% 20% 0.8% 0.01%
TABLE III: Percentage of buggy iterations, for Memcached
tests. Each test was executed for 10K iterations.
tate network trafﬁc coming from one or multiple MC
clients. Nekara-speciﬁc work was limited to the mocking oflibevent andpthread APIs. These totalled around 1
KLOC and took a nominal effort of around two weeks.
Existing MC tests did not exercise concurrency, so we wrote
a test ourselves. It concurrently invoked several MC oper-ations; the workload itself was a combination of workloadsfrom existing tests. We then compared the response returnedby MC with the expected response. Because Nekara tests runin a loop, we needed to ensure that every iteration of the testwas independent of the previous one. This required resettingof all the global, static variables and clearing the cache afterevery iteration.
Bugs: We picked ﬁve previously-known concurrency-
related bugs, listed in Table II, and injected them back in thelatest version. We picked MC’s latest stable version, 1.6.8 forexperimentation. Some of these bugs (Bugs 1, 2, 5) were eitherused or found by previous work [25], [23], and others wereobtained from GitHub. In the course of testing, we also foundfour previously unknown bugs (Bugs 6 to 9), which have beenconﬁrmed by MC developers.
Bugs related to misuse of pthread APIs (Bug 1, 3, 4, 7, 9)
were easy: they were caught in the ﬁrst test iteration. The onlyreason they escaped MC’s existing tests is that pthreads doesnot fail on invalid invocations; we found them simply becauseour pthread API mocks checked for them.
Table III shows results for the remaining bugs where using
Nekara was crucial. Most of these bugs could not be caughtwithout Nekara despite running the test several (10K) times.All of previously-unknown bugs were present in MC from atleast the past four years, and even prior tools [23], [26] didnot ﬁnd them.
State coverage: In addition to ﬁnding bugs, we wanted to
check if Nekara indeed provides more coverage of concurrent1000 5000 10000 15000
#Iterations0100200300400500600700800#Unique statesUncontrolled
Nekara
Fig. 6: State coverage for Memcached.
behaviors. We took a hash of all key data-structures of MC(LRU linked lists, slab classes, and the KV table) at the endof a test iteration, and counted the number of unique hashesseen over all test iterations. We ran the Nekara test for 15Kiterations and the results are shown in Figure 6.
The results show a roughly four-fold increase in the total
number of hashes with Nekara, clearly indicating that itwas able to exercise many more behaviors of MC. Deeperinspection revealed that Nekara was able to trigger corner-case behaviors more often, for instance, a slab class runningout of memory, or a getoperation observing a cache miss.
Overall, our evaluation demonstrates that a system like MCcan beneﬁt greatly from systematic testing, and using Nekararequires little effort.
V. C
ASE STUDY :VERONA
Project Verona [19] is a prototype implementation of a new
programming language that explores the interaction betweenconcurrency and ownership to guarantee race freedom. Theruntime of the language has several complex concurrent pro-tocols that use shared memory. The runtime has schedulingincluding work stealing, back pressure (to slow message queuegrowth), fairness, memory management using atomic referencecounting, and global leak detection. These concepts interact insubtle ways that are difﬁcult to debug.
The designers of Verona decided to use systematic testing
from the start, motivated by prior struggle with concurrencybugs that took hours to days to resolve, and that had been asigniﬁcant barrier to making quick progress.
683Uncontrolled Nekara Commit Hash
Bug#1  0.049% 25bb324
Bug#2  0.091% c087803
TABLE IV: Percentage of buggy iterations, for Verona tests.
Each test was executed repeatedly for 5 mins.
During development of the Verona runtime, the scheduler,
concurrent queues, and memory management protocols werecarefully reviewed for uses of shared memory concurrency.Threads were mapped to operations (like in Section II).Synchronization happened in two forms. The ﬁrst was the useofstd:atomic; these remain unchanged except that before
every store, and after every load, a call to schedule_next
is inserted, because they mark a potentially racy access toshared memory. The second was the use of condition variables;these directly map to Nekara resources.
Verona uses Nekara’s next_integer to control other
sources of non-determinism. For instance, object identity, insystematic testing builds, was made deterministic, so featuresthat sort based on identity can be tested reproducibly. Theruntime has numerous heuristics to postpone expensive op-erations until there is sufﬁcient work to justify their cost.This implies that certain bugs, which require the expensiveoperation to execute, can take a long time to manifest. Thepostponement-heuristic was replaced with systematic choice,i.e.,if next_integer(0,1) == 0. This had two ben-
eﬁts: (1) it shortened the trace length to ﬁnd a bug, and (2) itremoved any dependency on a speciﬁc heuristic’s behaviour.
Verona uses systematic testing as part of its code-review
and CI process. Any change to the Verona runtime has anadditional twenty minutes of CI time for running systematictesting. There is a small amount of stress testing, but mostruntime bugs are found using systematic testing. The project’sethos is such that any failure found which did not exhibitduring systematic testing is treated as two bugs: the underlyingbug, and a bug in the use of systematic testing. The latteris ﬁxed ﬁrst by writing a systematic test that reveals thebug. Moreover, users of Verona get systematic testing for freebecause the runtime is already instrumented with Nekara.
Anecdotal evidence from the Verona team has said that the
use of systematic testing has given greater conﬁdence for newmembers of the team to modify the runtime primarily dueto the debugging experience of replayable crashes. Detailedlogging and replayable crashes provide a way to understandthe subtle interactions in concurrent systems that would nor-mally be a signiﬁcant barrier to entry for new programmers.Most runtime bugs do not make it off the developer’s machineas the local systematic testing catches them before CI. Tworecent bugs that we investigated are listed in Table IV. Bothbugs would not have been found without systematic testing.Bug #1 was a failure to correctly protect memory frombeing deallocated by another thread. The window for this tooccur was a few hundred cycles, hence almost impossible toreproduce reliably without systematic testing. The second bugwas due to not considering a corner case of a new feature.class Task{  static Task Run(Func<Task> func);
  static Task Delay(TimeSpan delay);
  static Task WhenAll( params
    Task[] tasks);  static Task<Task> WhenAny(
    params Task[] tasks);     ...  TaskAwaiter GetAwaiter();}class TaskCompletionSource<T> {  Task<T> Task { get; }
     ...  void SetResult(T result);
  void SetException(Exception ex);
  void SetCanceled(
    CancellationToken ct);}
Fig. 7: Task andTaskCompletionSource<T> APIs.
VI. C ASE STUDY :TASK PARALLEL LIBRARY
The Task Parallel Library [27] (TPL) is a popular, open-
source, cross-platform library provided by .NET for buildingconcurrent applications. TPL exposes several key types, suchasTask andTaskCompletionSource<T> that interoper-
ate with the async andawait keywords in the C# language,
enabling writing asynchronous code without the complexity ofmanaging callbacks. A developer writes code using these high-level APIs and TPL takes care of the hard job of partitioningthe work, scheduling tasks to execute on the thread pool,canceling and timing out tasks, managing state and invokingasynchronous callbacks.
TPL is pervasive in the .NET ecosystem. We designed
TPL
Nas a drop-in-replacement library for TPL. TPLNpro-
vides stubs that replace the original TPL APIs, as well assubclasses that override original TPL types. These stubs andsubclasses call into Nekara via a C++ to C# foreign-functioninterface. Any C# application that uses TPL for its concurrencysimply needs to replace it with TPL
Nto get systematic testing.
We now explain core TPLNdesign.
TheTask type (Figure 7) provides several public-facing
APIs including: Task.Run for queueing a function to execute
on the thread pool and returning a task that can be usedas a handle to asynchronously await for the function tocomplete; Task.WhenAll andTask.WhenAny for waiting
one or more tasks to complete; Task.Delay for creating
an awaitable task that completes after some time passes; andcompiler-only APIs, such as Task.GetAwaiter, which
the C# compiler uses in conjunction with the async and
await keywords to generate state machines that manage
asynchronous callbacks [28].
Figure 8 shows the Nekara-instrumented version of
Task.Run. For simplicity, we have omitted low-level de-tails such as task cancellation and exception handling.Task.Run of TPL
Ncreates a new Nekara operation
viacreate_operation, and then invokes the origi-
nalTask.Run to spawn a task. This new task ﬁrst
calls start_operation, then invokes the user func-
tion, followed by end_operation. The parent task calls
schedule_next to give the child task a chance to execute.
TaskCompletionSource<T> (TCS), shown in Fig-
ure 7, allows developers to asynchronously produce and con-sume results. This type exposes a Task get-only property
684using SystemTask = System.Threading.Tasks.Task;
static Task Run(Func<Task> func) {
  // Create a new Nekara operation id for the new task.
  int op = GetUniqueOperationId();
  Nekara. create_operation (op);
  var task = SystemTask.Run( async () => {
    Nekara. start_operation (op);
    // Execute the user-specified asynchronous function.    // The await logic is instrumented with Nekara.    await  func();
    Nekara. end_operation (op);
  });
  Nekara. schedule_next ();
  return task;
}
Fig. 8: Instrumentation of Task.Run with Nekara.
// TCS context used for Nekara testing.
class TCSContext<T> {  // Nekara resource id for the TCS.  int Id = GetUniqueResourceId(); 
  // The result set by the TCS.  T Result = default ;
  bool IsCompleted = false ;
}
void SetResult(T result) {
  var context = GetCurrentContext();
  if (!context.IsCompleted) {
    // Set the TCS result.    context.Result = result;    context.IsCompleted = true;
    // Signal the TCS consumer.    Nekara. signal_resource (context.Id);
  }}Task<T> Task => {  // Get the current TCS context.  var context = GetCurrentContext();  if (context.IsCompleted) {
    // If TCS is completed, return the result.    return Task.FromResult(context.Result);  }
  // Return a Nekara-controlled task
  // that will be completed by the producer.  return Task.Run(() => {    // Wait the producer to set the result.    Nekara. wait_resource (context.Id);
    // Exception/cancellation logic.    ...    return context.Result;  });};
Fig. 9: Instrumentation of TCS APIs with Nekara.
that a consumer can await to receive a result of type Tasyn-
chronously. This task remains uncompleted until the producer
completes it with a result by invoking SetResult.
Instrumentation of TCS Task andSetResult is shown in
Figure 9. We designed TCSContext<T>, a simple test-only
data structure in TPLNthat contains information needed to
model a TCS. TCSContext<T> contains a Nekara resource
id associated with the TCS, the result of the TCS, and aBoolean value that is set to true once the TCS completes.TCSContext<T> is set to the TCS state upon initialization
by TPL
N, so that it can be accessed when invoking one of the
stub TCS APIs. The producer (SetResult) is modeled asfollows: it ﬁrst accesses the context, then checks if the TCS hascompleted and, if not, it sets the result and IsCompleted
totrue and signals the resource associated with the TCS
to unblock any task that might be waiting on the TCS Task
getter property. The consumer (Task getter) is modeled as
follows: it ﬁrst accesses the context, then checks if the TCShas completed and, if it has, it simply returns a completed taskwith the result. If the TCS has not completed yet, it will createa new task by invoking the Task.Run TPL
NAPI (which we
described above). This asynchronous task immediately waitson the resource, and once the producer signals, it returns the(Mock) Client
Control Plane
ASP.NET
MicroserviceData Plane
ASP.NET
MicroserviceWorker
ASP.NET
MicroserviceCSCS Kubernetes Cluster
(Mock)
Cosmos DB(Mock) Storage Queue
Fig. 10: The high-level architecture of CSCS.
completed result.
We also modeled two other TPL types. First is the
Monitor type that implements a reentrant lock in C#
(along similar lines to Figure 3). Second is the typeAsyncTaskMethodBuilder that the C# compiler uses to
generate state machines to manage asynchronous callbacks inmethods that use async andawait.
Creating TPL
Ntook roughly one person month, with most
time spent in ensuring that we support each TPL API andmaintain details such as exception propagation, cancellation,etc. This is largely a one-time effort (unless TPL itself changessigniﬁcantly). Several engineering teams in Microsoft wereable to use TPL
Nto test their services without needing
any changes to their production code. Two such systemsare summarized next in Sections VI-A and VI-B. They useconditional compilation to automatically replace the originalTPL library with TPL
Nduring testing.
A. Testing CSCS with TPLN
Cloud Supply Chain Service (CSCS) exposes a set of
HTTP REST APIs that clients can invoke to create supply-chain entities and orchestrate supply-chain processes. CSCSis designed with a typical microservice-based architecture(Figure 10) where multiple stateless microservices coordinatewith each other through shared state maintained in backendstorage systems. CSCS consists of roughly 56K lines of C#
code. It is built using ASP.NET [29] and achieves horizontalscalability through Kubernetes [30].
The CSCS microservices, although stateless, concurrently
access the backend storage, including Cosmos DB [31] (aglobally-distributed database service) and Azure Queue Stor-age [32] (a durable distributed queue). Some requests inCSCS follow a simple request/response pattern, while otherstrigger background jobs, making the client periodically pollto check on the completion of the job. This requires complexsynchronization logic between the microservices.
CSCS is written against storage interfaces, so they can
be easily mocked during testing using techniques such asdependency injection. Multiple concurrent client calls are sim-ulated by spawning concurrent tasks that invoke the relevantASP.NET controller actions.
685// Instantiates a Cosmos DB Mock, an instance of a CSCS
// ASP.NET microservice, and a CSCS client.var cosmosDbMock = new CosmosDb_Mock(...);var factory = new ServiceFactory(cosmosDbMock, ...);
var client = factory.CreateClient(...);
// Invokes a concurrent Create and Update client request.
Task req1 = Task.Run(() => client.Create("id",  { Content: "payload1", Timestamp: 7 }));Task req2 = Task.Run(() => client.Update("id",  { Content: "payload2", Timestamp: 8 }));
await Task.WhenAll(req1, req2);
// Gets the resource (stored in Cosmos DB) and asserts
// it contains the expected payload and timestamp.var resource = await client.Get("id");
Assert.IsTrue(resource.Content == "payload2" &&  resource.Timestamp == 8);
Fig. 11: A simple Nekara concurrent test in CSCS.
Bug#1 Bug#2 Bug#3 Bug#4
Uncontrolled 
Nekara 11% 7% 1% 1%
TABLE V: Percentage of buggy iterations on CSCS tests.
CSCS concurrency unit tests range from simple concurrency
patterns where the test calls an API with different inputs but
the same key (to exercise interference in Cosmos DB) to morecomplex tests that randomly fail certain mock invocations(using the next_integer API) to simulate intermittent
network failures. Figure 11 shows a simple CSCS test. Thesetests, when exercised by Nekara, were able to uncover subtlebugs. Table V lists some of the bugs, comparing testing withand without Nekara. Each test was run multiple times for atotal of 5 minutes and the table shows percentage of buggyruns. Without Nekara, none of these bugs would have beenfound. Following is a description of these bugs.
a) Data loss due to concurrent requests (Bug#1): CSCS
requires that upon two concurrent Create orUpdate re-
quests, only the request with the latest modiﬁed timestampsucceeds. To achieve this, CSCS uses Cosmos DB ETagsfunctionality for optimistic concurrency control, but a bugin the handling of ETags led to a stale Create overwriting
fresher data. This bug was missed by both stress testing andmanual code review, but found quickly with Nekara (with thetest shown in Figure 11). This bug could have lead to customerdata loss.
b) Inconsistent entity state (Bug#2): CSCS manages two
sets of related entities, which are stored in different tables andpartitions of Cosmos DB. Rejecting an update on one of theentities, must lead to rejection of updating the other entity too.However, Cosmos DB does not support transactions betweenentities stored in different partitions, and the developers hadto implement custom synchronization logic to get around thislimitation. When the team wrote a concurrent test that tries tocancel and reject an entity at the same time, Nekara uncoveredan issue where the system got into an inconsistent state. Thisbug had escaped stress testing and manual code review.
c) Resource creation liveness issue (Bug#3): Certain
requests trigger a background task requiring the client to peri-odically poll its completion. This functionality is implementedby storing a record indicating the request status in Cosmos DBand then submitting the job to a worker queue to trigger theasynchronous work. There was a bug where the submission tothe worker queue could fail due to network connectivity issues.However, as the record was created in the database, the userwould ﬁnd the status to be pending-creation upon a
GET request and would erroneously assume the resource will
be eventually created. This liveness bug was caught with a test
that simulated potential network failures.
d) Race condition in test logic (Bug#4): Interestingly,
Nekara found a bug in the CSCS test code itself. The buggytest performed two concurrent PUT requests to provision a
resource, then waited for the provisioning to complete andthen deleted the resource. The test was failing because twoconcurrent Create requests led to two asynchronous workers
for the same resource. The test then deleted the resource assoon as one of the workers transitioned the resource state tocreated. However, there was a race between the secondasynchronous worker and the Delete request, which caused
the test to fail. The developers were initially not sure if thisbug was in their production logic or test logic, but due toNekara’s reproducible traces they were able to understand theissue and ﬁx it.
B. Testing ECSS with TPL
N
Cloud storage uses geo-redundancy to protect against catas-
trophic data center failures. The Erasure Coding Storage Ser-
vice (ECSS) offers an economical solution to this problem byapplying erasure coding to blob objects across geographicallydistributed regions. Figure 12 shows a partial view of thehigh-level architecture of ECSS. The system consists of a dataservice and a metadata service. The data service is responsiblefor striping data and generating parities across regions, as wellas reconstructing data in the event of failures. The metadataservice manages erasure coding stripes and handles dynamicupdates to the stripes (due to object creation, update, anddeletion). ECSS consists of roughly 44K lines of C# code.
To achieve high-throughput, ECSS was designed to be
highly-concurrent. The data service implements a componentcalled Syncer that periodically synchronizes metadata betweenindividual regions and the metadata service using AzureQueue Storage. Syncer is sharded and the ECSS developersimplemented a lease-based mechanism to assign differentSyncer partitions to different metadata service nodes. Themetadata service executes two long running TPL tasks: atable updater and a table scanner. The updater asynchronouslydequeues Syncer messages from Azure Queue Storage anduses their contents to update Azure Table Storage. The scannerperiodically scans Azure Table Storage to check different typesof metadata. Based on the metadata state, the scanner will
686`
Data ServiceMetadata Service
Syncer ...
(Mock) Storage TableTable Scanner Table UpdaterIn-memory Action Queue
In-memory Action Queue...
Action
Engine
(Mock) Storage QueueData Block
Fig. 12: Parts of the high-level architecture of ECSS.
Syncer_1
Syncer_2(Mock)
Storage Tableupdate
metadata
to v2init and read
metadata from v1
update
metadata
to v3enqueue
msg_Benqueue
msg_A
(Mock) Storage Queue
1. msg_A (metadata_v2)
2. msg_B (metadata_v3)
init and read
metadata from v2Data Block
read
data at
time 1read
data at
time 2
Fig. 13: Race condition in the ECSS data service.
enqueue actions on a set of in-memory queues. Long-running
tasks execute action engines that drain these queues, performactions on the state and update the metadata in Azure TableStorage, as well as sending action messages to the data servicethrough Azure Queue Storage to do erasure coding amongother operations.
Eventually, the state of the data service and Azure Ta-
ble Storage must be consistent. ECSS manages exabytes ofcustomer data, and correctness is absolutely vital, which iswhy the team used TPL
Nfor thorough testing. No changes
were required to the production code. The main investmentwas in writing the tests: the tests instantiate the Syncer andthe metadata service in-memory, use dependency injection forinserting their mocks of Azure Table Storage and Azure QueueStorage, write input data and ﬁnally assert that the Azure TableStorage state is consistent.
Nekara helped ﬁnd several critical bugs that could have
resulted in data loss. Figure 13 illustrates one of these bugs,a race condition between two Syncer instances from the samepartition. The ﬁrst Syncer instance (Syncer_1) starts andreads metadata with version 1from Azure Table Storage,
updates it and writes it back to the table with version 2.
On the same partition, a new Syncer instance (Syncer_2)starts, reads the same metadata (which now has version 2),
and updates it to version 3. Immediately after, Syncer_2
reads from the data block, but just before it continues itsexecution, Syncer_1 reads the latest data from the same
block and generates msg
A containing version 2of theTPLNCoyoteNCoyote
Benchmarks BI% Time BI% Time BI% Time
Ch. Replication  594 0.01% 197 0.01% 224
Fail. Detector  39 0.08% 19 0.07% 23
Paxos 0.05% 254 0.07% 56 0.06% 104
Raft 0.35% 789 0.29% 151 0.38% 156
TABLE VI: Comparing systematic testing with Nekara againstthe original Coyote. Tests run for 10k iterations. BI% denotespercentage of buggy iterations. Time is in seconds.
metadata and enqueues this message to Azure Queue Storage.
Next, Syncer_2 continues executing and generates msg
B
containing version 3of the metadata and also enqueues this
message. Finally, the metadata service dequeues conﬂictingmessages from the same partition, which could result in alater version of the metadata being overwritten by an outdatedversion. To ﬁx the bug, the developers had to write logic justbefore enqueueing the messages to check if the metadata andthe version are consistent, and if not then discard the message.
This bug had escaped continuous unit and stress tests.
Nekara was able to ﬁnd it in under 10 seconds and just 8
test runs (on average). The ECSS team (as well as the CSCSteam) routinely run Nekara tests as part of their CI.
VII. R
EPRODUCING KNOWN BUGS
We evaluate Nekara against three prior tools on their own
set of benchmarks. The purpose of this evaluation is to showthat one can get state-of-the-art systematic testing with Nekara.
a) Coyote: The ﬁrst comparison is against Coyote [20]
that provides an open-source .NET actor library, used by teamsin Azure [33]. Coyote provides a runtime for executing actors.All actors execute concurrently and communicate by sendingmessages to each other. Coyote also provides a systematictesting solution for testing these actor-based programs [11],[34].
We directly instrumented the Coyote runtime using Nekara
to build our own systematic testing solution. We had twooptions. In the ﬁrst approach, we took advantage of the Coyoteruntime being implemented entirely on top of TPL, whichwe simply replaced with TPL
N. In the second approach, we
instead instrumented at the level of actor semantics: an actoris mapped to an operation, and the actor’s inbox is mappedto a resource. We skip the details of this instrumentation forlack of space; the high-level summary is that it only required16lines of changes to the Coyote runtime. We refer to the
second approach as Coyote
N. Note that in both approaches, all
changes were limited to the Coyote runtime; the user programremains unchanged.
Table VI shows results on prior Coyote benchmarks, which
consist of buggy protocol implementations. The two Nekaraapproaches have different advantages. TPL
Nhas the beneﬁt of
being able to test the Coyote runtime itself (that it correctlyimplements actor semantics). Moreover, the instrumentationwas mechanical and required no knowledge of Coyote itself.
687class MockDictionary<K, V> : Dictionary<K, V> {
  int SharedVar; bool IsWrite = false ; ...
  bool override ContainsKey(K key) {
    var tid = Task.CurrentId;    SharedVar = tid;    Nekara. schedule_next ();
    // Check for race.    assert(!(SharedVar != tid && IsWrite));    return base .ContainsKey(key);
  }}  void override Add(K key, V value) {
    var tid = Task.CurrentId;    SharedVar = tid; IsWrite = true;
    Nekara. schedule_next ();
    // Check for race.    assert(SharedVar == tid);    IsWrite = false ;
    base.Add(key, value);  }
Fig. 14: Mock dictionary instrumented to detect races.
However, bugs in the Coyote runtime was not in question
here, and we found this approach to have worse performancethan the other systematic testing solutions. Instrumenting atthe TPL level signiﬁcantly increased the number of operationsand scheduling points in the program, which decreased bug-ﬁnding ability and increased test time. Coyote
N, however, was
comparable to Coyote because both directly leverage actorsemantics. We also compared against uncontrolled testing,which unsurprisingly, could not ﬁnd any of the bugs.
We make a note on the relationship of Coyote to Nekara
because both projects have inﬂuenced each other. Coyote hadearlier only supported an actor-based programming model.Nekara inherited many of the search techniques used inCoyote, but applied them to a generalized setting. Giventhe success of Nekara’s TPL
Nmodel in testing Task-based
programs, the Coyote team has since incorporated that worknatively to support Task-based programs as well. Coyoteuses bytecode-level instrumentation to automatically injecthooks into an unmodiﬁed C# program, making the end-userexperience more seamless than with using TPL
N.
b) TSVD: In the second experiment, we reproduce bugs
found by TSVD [35], a tool that looks for thread-safety viola-
tions (TSVs), which are concurrent invocations of operations
on a non-thread-safe data structure (e.g., a Dictionary).
TSVD’s open-source evaluation [35] covered nine .NET appli-cations; here we consider six of those (one was removed fromGitHub and in two others we were unable to locate a TSV bugfrom the cited GitHub issue). In each application, we replacedTPL with TPL
Nand made one additional change. To capture
TSV as an assertion, we implemented a MockDictionary
type as a subclass of Dictionary, and modiﬁed the applica-
tion to use this type. Figure 14 shows this mock: an assertionfailure in the mock corresponds to a TSV . Table VII showsthe results: we were able to ﬁnd all TSVs, taking at most 8
iterations on average. In the DataTimeExtension benchmark,we found four additional TSVs (not reported by TSVD) byrunning the test for 100 iterations.
c) Maple: The third comparison is against Maple [36], a
systematic-testing tool that uses dynamic instrumentation andprovides coverage-driven testing through online proﬁling anddynamic analyses. We picked a set of real-world benchmarksfrom SCTBench [12]. These include: an older version of Mem-cached, SpiderMonkey (a JavaScript runtime engine), Stream-Nekara
Applications LoC #BF BR? BI%TSVDDataTimeExtention 3.2K 3  89.3%
FluentAssertion 78.3K 2  51.3%
K8s-client 332.3K 1  11.7%
Radical 96.9K 3  28.3%
System.Linq.Dynamic 1.2K 1  99.0%
Thunderstruck 1.1K 2  48.3%MapleSpiderMonkey∗200K 2  0.5%
Memcached-1.4.4 10K 1  20.0%
StreamCluster∗2.5K 3  41.7%
Pbzip2 1.5K 1  50.0%
TABLE VII: Comparison with TSVD and Maple. #BF isnumber of bugs found by these tools. The (under BR?)
denotes that Nekara reproduced all these previously foundbugs. BI% denotes percentage of buggy iterations.
Cluster (online clustering of input streams), and pbzip2 (a
parallel implementation of bzip2). Bugs in these applicationsinclude atomicity violations, order-violation, deadlocks andlivelocks. All these benchmarks use pthreads so we reused
the models from Section IV. Table VII shows the results.Nekara found all the reported bugs; most in a small numberof iterations. These numbers are either comparable or smallerthan those reported by Maple, although a direct comparisonis not possible because of the different set of techniques andinstrumentation used. For some benchmarks, marked in thetable with a
∗, we also inserted schedule_next just before
global variables accesses, without which we were unableto ﬁnd some of the bugs (due to the incompleteness issuementioned in Section II).
VIII. E
XPERIENCE SUMMARY
This section summarizes our experience in integration
Nekara with the various case studies presented in this paper.
Integration: We were able to integrate Nekara testing
with several systems with no changes to the production code inmost cases. Only in the case of Memcached, we had to modifythe code in order to mock system calls to sockets so that wecould write unit tests with multiple concurrent clients. The factthat production code need not take a dependency on Nekarawas important was acceptance with development teams.
Modeling effort: Table I shows that the modeling effort
was small to moderate. For systems like Verona, systematictesting was always an integral part of its development process.Systems like ECSS and CSCS fully shared their models(TPL
N) demonstrating amortization of effort across teams.
Development of the models does require some expertise in
reasoning about concurrency, especially for understanding thesemantics of synchronization APIs and encoding it correctlyusing Nekara resources. Correctness of these models is aconcern, because mistakes can lead to deadlocks when runningNekara tests. We leave the problem of validating models asfuture work. In general, it would be interesting to explore acommunity-driven effort that maintains models of commonconcurrency frameworks or APIs.
688Bugs: Nekara helped catch several bugs, including live-
ness bugs, deadlocks, memory leaks as well as functionality
bugs like data corruption. Many of these would not have beencaught with existing practices like stress testing or code review.
Writing test cases: Developing good tests that exercise
concurrency in the system, and assert something meaningful,are crucial to get beneﬁts from systematic testing. For Mem-cached, we wrote a generic test by simply combining existingtest cases. For other systems, the developers of those systemswere able to write tests themselves without our help.
Debugging: Repro capabilities of Nekara was well ap-
preciated by developers. Furthermore, instrumentation of aruntime (like Verona or Coyote) provides systematic testingto users of the runtime for free.
IX. R
ELATED WORK
The systematic testing approach has its roots in stateless
model checking, popularized ﬁrst by VeriSoft [10]. Stateful
approaches require capturing the entire state of a program inorder to avoid visiting the same state again, because of whichthese techniques are typically applied on an abstract modelof an implementation [37], [38]. Stateless approaches, on theother hand, only control program actions. They do not inspectprogram state at all, consequently are directly applicable totesting the implementation itself. All these techniques wereinitially focussed on veriﬁcation. CHESS [39] shifted thefocus to bug ﬁnding; it prioritized exploration of a subsetof behaviors, one with a few number of context switches,and found many bugs. Several randomized search techniques[4], [9], [12], [40], [21] have followed since, showcasing veryeffective bug-ﬁnding abilities. Our focus has been on makingsystematic testing easy to use, to that end we capture allthese search techniques inside Nekara. One class of techniquesthat we are unable to capture in Nekara’s interface currentlyis partial-order-reduction (POR) based techniques [10], [41].POR requires knowing if the next steps of two different
operations are independent of each other or not. Supporting
POR require more information to be supplied to Nekara, whichis interesting future work.
Instantiation of systematic testing exists for multi-threaded
[42], [2], [3], [4], [41] as well as message-passing programs[5], [6], [7], [8], [9], however their combined reach is stillsmall. For instance, most C# applications that we consideredare built on TPL Tasks. These Tasks eventually execute onthe .NET threadpool, so one can consider them to be multi-threaded applications, but instrumenting the .NET runtime ischallenging, and we did not ﬁnd any readily applicable tool.Moreover, even if this was possible, instrumenting at a higher-level is typically much easier (§VI) and more efﬁcient (§VII).Our goal is to unlock systematic testing; modeling is nothidden inside a tool, but available as code that can be readilyadopted by others.
Complementary to systematic testing is the work on ﬁnding
low-level concurrency bugs such as data races and atomic-ity violations [43], [44], [45], [46], [47]. These techniquesexamine a given concurrent execution and evaluate if therewere potentially racy accesses. Data race detection typicallyrequires monitoring memory accesses at runtime. Nekara canhelp generate a diverse set of concurrent executions for thesetools, or be used directly for speciﬁc races (§VII).
Another related problem is deterministic replay of pro-
cesses, even virtual machines [48], [49], [50], [51], [52]. Thesetechniques seek generality to support arbitrary systems, andrequire very detailed tracing. The problem is simpler in oursetting, as we are focussed on a single test written by adeveloper aware of the concurrency used in their application.
X. C
ONCLUSIONS
Systematic testing holds promise in changing the way we
test concurrent systems. In this paper, we present Nekara,a simple library that allows developers to build their ownsystematic testing solutions. Integration of Nekara is a simpleprogramming exercise that only requires modeling of keyconcurrency APIs. The model code is easy to share to furtheramortize the cost across multiple projects. We report severalcase studies where the use of Nekara made considerableimpact, both in existing systems, as well as systems designedfrom scratch with systematic testing.
R
EFERENCES
[1] J. Gray, “Why do computers stop and what can be done about it?” in
Proceedings of the 5th Symposium on Reliability in Distributed Software
and Database Systems. IEEE, 1986, pp. 3–12.
[2] M. Musuvathi, S. Qadeer, T. Ball, G. Basler, P. A. Nainar, and
I. Neamtiu, “Finding and reproducing heisenbugs in concurrent pro-grams,” in Proceedings of the 8th USENIX Symposium on Operating
Systems Design and Implementation, 2008, pp. 267–280.
[3] M. Musuvathi and S. Qadeer, “Fair stateless model checking,” in
Proceedings of the ACM SIGPLAN 2008 Conference on ProgrammingLanguage Design and Implementation, Tucson, AZ, USA, June 7-13,2008, R. Gupta and S. P. Amarasinghe, Eds. ACM, 2008, pp.362–371. [Online]. Available: https://doi.org/10.1145/1375581.1375625
[4] S. Burckhardt, P. Kothari, M. Musuvathi, and S. Nagarakatte, “A
randomized scheduler with probabilistic guarantees of ﬁnding bugs,”inProceedings of the 15th International Conference on Architectural
Support for Programming Languages and Operating Systems, ASPLOS2010, Pittsburgh, Pennsylvania, USA, March 13-17, 2010, 2010, pp.167–178. [Online]. Available: https://doi.org/10.1145/1736020.1736040
[5] J. Simsa, R. Bryant, and G. A. Gibson, “dbug: Systematic
testing of unmodiﬁed distributed and multi-threaded systems,” inModel Checking Software - 18th International SPIN Workshop,Snowbird, UT, USA, July 14-15, 2011. Proceedings, ser. LectureNotes in Computer Science, A. Groce and M. Musuvathi, Eds.,vol. 6823. Springer, 2011, pp. 188–193. [Online]. Available:https://doi.org/10.1007/978-3-642-22306-8\
14
[6] J. Yang, T. Chen, M. Wu, Z. Xu, X. Liu, H. Lin, M. Yang, F. Long,
L. Zhang, and L. Zhou, “MODIST: transparent model checking ofunmodiﬁed distributed systems,” in Proceedings of the 6th USENIX
Symposium on Networked Systems Design and Implementation, 2009,pp. 213–228.
[7] T. Leesatapornwongsa, M. Hao, P. Joshi, J. F. Lukman, and H. S.
Gunawi, “SAMC: semantic-aware model checking for fast discoveryof deep bugs in cloud systems,” in Proceedings of the 11th USENIX
Symposium on Operating Systems Design and Implementation, 2014,pp. 399–414.
[8] J. F. Lukman, H. Ke, C. A. Stuardo, R. O. Suminto, D. H. Kurni-
awan, D. Simon, S. Priambada, C. Tian, F. Ye, T. Leesatapornwongsa,A. Gupta, S. Lu, and H. S. Gunawi, “Flymc: Highly scalable testingof complex interleavings in distributed systems,” in Proceedings of the
F ourteenth EuroSys Conference 2019, Dresden, Germany, March 25-28,2019, 2019, pp. 20:1–20:16.
689[9] B. K. Ozkan, R. Majumdar, F. Niksic, M. T. Befrouei, and G. Weis-
senbacher, “Randomized testing of distributed systems with probabilistic
guarantees,” PACMPL, vol. 2, no. OOPSLA, pp. 160:1–160:28, 2018.
[10] P. Godefroid, “Software model checking: The verisoft approach,”
F ormal Methods in System Design, vol. 26, no. 2, pp. 77–101, 2005.[Online]. Available: https://doi.org/10.1007/s10703-005-1489-x
[11] P. Deligiannis, M. McCutchen, P. Thomson, S. Chen, A. F. Donaldson,
J. Erickson, C. Huang, A. Lal, R. Mudduluru, S. Qadeer, and W. Schulte,“Uncovering bugs in distributed storage systems during testing (notin production!),” in 14th USENIX Conference on File and Storage
Technologies, F AST 2016, Santa Clara, CA, USA, February 22-25, 2016.,2016, pp. 249–262.
[12] P. Thomson, A. F. Donaldson, and A. Betts, “Concurrency testing
using controlled schedulers: An empirical study,” ACM Transactions on
Parallel Computing, vol. 2, no. 4, pp. 1–37, 2016.
[13] Microsoft, “Azure Service Fabric,” https://azure.microsoft.com/services/
service-fabric/.
[14] libevent, “An event notiﬁcation library,” https://libevent.org/.
[15] Trio, “A friendly Python library for async concurrency and I/O,” https:
//trio.readthedocs.io/en/stable/.
[16] R. J. Lipton, “Reduction: A method of proving properties of parallel
programs,” Commun. ACM, vol. 18, no. 12, pp. 717–721, Dec. 1975.
[17] B. Norris and B. Demsky, “Cdschecker: checking concurrent data
structures written with C/C++ atomics,” in Proceedings of the 2013 ACM
SIGPLAN International Conference on Object Oriented Programming
Systems Languages & Applications, OOPSLA 2013, part of SPLASH2013, Indianapolis, IN, USA, October 26-31, 2013, 2013, pp. 131–150.
[18] Memcached, “An in-memory key-value store,” https://www.memcached.
org/, 2020.
[19] Microsoft Research, “Verona: Research programming language for con-
current ownership,” https://github.com/microsoft/verona, 2021.
[20] Microsoft Coyote, “Fearless coding for reliable asynchronous software,”
https://github.com/microsoft/coyote, 2020.
[21] M. Emmi, S. Qadeer, and Z. Rakamaric, “Delay-bounded scheduling,”
inProceedings of the 38th ACM SIGPLAN-SIGACT Symposium on
Principles of Programming Languages, POPL 2011, Austin, TX, USA,January 26-28, 2011, 2011, pp. 411–422.
[22] T. Elmas, J. Burnim, G. Necula, and K. Sen, “CONCURRIT: a domain
speciﬁc language for reproducing concurrency bugs,” in Proceedings of
the 34th ACM SIGPLAN conference on Programming language designand implementation, 2013, pp. 153–164.
[23] D. Schemmel, J. B ¨uning, C. Rodr ´ıguez, D. Laprell, and K. Wehrle,
“Symbolic partial-order execution for testing multi-threaded programs,”arXiv preprint arXiv:2005.06688, 2020.
[24] S. Nagarakatte, S. Burckhardt, M. M. Martin, and M. Musuvathi,
“Multicore acceleration of priority-based schedulers for concurrency bugdetection,” in Proceedings of the 33rd ACM SIGPLAN conference on
Programming Language Design and Implementation, 2012, pp. 543–554.
[25] N. Jalbert, C. Pereira, G. Pokam, and K. Sen, “RADBench: A concur-
rency bug benchmark suite.” HotPar, vol. 11, pp. 2–2, 2011.
[26] K. Serebryany and T. Iskhodzhanov, “ThreadSanitizer: data race detec-
tion in practice,” in Proceedings of the workshop on binary instrumen-
tation and applications, 2009, pp. 62–71.
[27] D. Leijen, W. Schulte, and S. Burckhardt, “The design of a task parallel
library,” in Proceedings of the 24th ACM SIGPLAN Conference on
Object Oriented Programming Systems Languages and Applications.ACM, 2009, pp. 227–242.
[28] Microsoft, “Asynchronous programming with async and await,” https://
docs.microsoft.com/dotnet/csharp/programming-guide/concepts/async/,2020.
[29] ——, “ASP.NET: A framework for building web apps and services with
.NET and C#,” https://dotnet.microsoft.com/apps/aspnet, 2020.
[30] The Kubernetes Authors, “Kubernetes,” https://kubernetes.io/, 2020.
[31] Microsoft, “Cosmos DB: Fast NoSQL database with open APIs for any
scale,” https://azure.microsoft.com/en-us/services/cosmos-db/, 2020.
[32] ——, “Queue Storage: Durable queues for large-volume cloud services,”
https://azure.microsoft.com/en-us/services/storage/queues/, 2020.
[33] P. Deligiannis, N. Ganapathy, A. Lal, and S. Qadeer, “Building
reliable cloud services using P# (experience report),” CoRR, vol.
abs/2002.04903, 2020. [Online]. Available: https://arxiv.org/abs/2002.
04903
[34] P. Deligiannis, A. F. Donaldson, J. Ketema, A. Lal, and P. Thomson,
“Asynchronous programming, analysis and testing with state machines,”inProceedings of the 36th ACM SIGPLAN Conference on Programming
Language Design and Implementation , 2015, pp. 154–164.
[35] G. Li, S. Lu, M. Musuvathi, S. Nath, and R. Padhye, “Efﬁcient scal-able thread-safety-violation detection: Finding thousands of concurrencybugs during testing,” in Proceedings of the 27th ACM Symposium
on Operating Systems Principles, ser. SOSP ’19. New York, NY ,USA: Association for Computing Machinery, 2019, p. 162180, https://doi.org/10.1145/3341301.3359638.
[36] J. Yu, S. Narayanasamy, C. Pereira, and G. Pokam, “Maple: A coverage-
driven testing tool for multithreaded programs,” in Proceedings of
the ACM International Conference on Object Oriented ProgrammingSystems Languages and Applications, ser. OOPSLA ’12. New York,NY , USA: Association for Computing Machinery, 2012, p. 485502.[Online]. Available: https://doi.org/10.1145/2384616.2384651
[37] G. Holzmann, The SPIN Model Checker: Primer and Reference Manual ,
1st ed. Addison-Wesley Professional, 2011.
[38] T. Andrews, S. Qadeer, S. K. Rajamani, J. Rehof, and Y . Xie, “Zing: A
model checker for concurrent software,” in Computer Aided V eriﬁcation,
16th International Conference, CA V 2004, Boston, MA, USA, July 13-17,2004, Proceedings, 2004, pp. 484–487.
[39] M. Musuvathi and S. Qadeer, “Iterative context bounding for system-
atic testing of multithreaded programs,” in Proceedings of the ACM
SIGPLAN 2007 Conference on Programming Language Design andImplementation, San Diego, California, USA, June 10-13, 2007, 2007,pp. 446–455.
[40] A. Desai, S. Qadeer, and S. A. Seshia, “Systematic testing of asyn-
chronous reactive systems,” in Proceedings of the 2015 10th Joint
Meeting on F oundations of Software Engineering, ESEC/FSE 2015,Bergamo, Italy, August 30 - September 4, 2015, 2015, pp. 73–83.
[41] J. Huang, “Stateless model checking concurrent programs with maximal
causality reduction,” in Proceedings of the 36th ACM SIGPLAN Confer-
ence on Programming Language Design and Implementation, Portland,OR, USA, June 15-17, 2015, 2015, pp. 165–174.
[42] M. Musuvathi, D. Y . W. Park, A. Chou, D. R. Engler, and D. L. Dill,
“CMC: A pragmatic approach to model checking real code,” in 5th
Symposium on Operating System Design and Implementation (OSDI2002), Boston, Massachusetts, USA, December 9-11, 2002, 2002.
[43] C. Flanagan and S. N. Freund, “Atomizer: a dynamic atomicity checker
for multithreaded programs,” in Proceedings of the 31st ACM SIGPLAN-
SIGACT Symposium on Principles of Programming Languages, POPL2004, V enice, Italy, January 14-16, 2004, 2004, pp. 256–267.
[44] S. Park, S. Lu, and Y . Zhou, “Ctrigger: exposing atomicity violation
bugs from their hiding places,” in Proceedings of the 14th International
Conference on Architectural Support for Programming Languages andOperating Systems, ASPLOS 2009, Washington, DC, USA, March 7-11,2009, 2009, pp. 25–36.
[45] C. Flanagan and S. N. Freund, “Fasttrack: efﬁcient and precise dynamic
race detection,” in Proceedings of the 2009 ACM SIGPLAN Conference
on Programming Language Design and Implementation, PLDI 2009,Dublin, Ireland, June 15-21, 2009, 2009, pp. 121–133.
[46] K. Sen, “Race directed random testing of concurrent programs,” in
Proceedings of the ACM SIGPLAN 2008 Conference on ProgrammingLanguage Design and Implementation, Tucson, AZ, USA, June 7-13,2008, 2008, pp. 11–21.
[47] S. Savage, M. Burrows, G. Nelson, P. Sobalvarro, and T. E. Anderson,
“Eraser: A dynamic data race detector for multi-threaded programs,”inProceedings of the Sixteenth ACM Symposium on Operating System
Principles, SOSP 1997, St. Malo, France, October 5-8, 1997, 1997, pp.27–37.
[48] S. Bhansali, W.-K. Chen, S. de Jong, A. Edwards, R. Murray, M. Drini ´c,
D. Miho ˇcka, and J. Chau, “Framework for instruction-level tracing and
analysis of program executions,” in Proceedings of the 2nd International
Conference on Virtual Execution Environments, ser. VEE ’06. NewYork, NY , USA: Association for Computing Machinery, 2006, pp.154–163. [Online]. Available: https://doi.org/10.1145/1134760.1220164
[49] G. W. Dunlap, S. T. King, S. Cinar, M. A. Basrai, and P. M. Chen,
“Revirt: Enabling intrusion analysis through virtual-machine loggingand replay,” SIGOPS Oper . Syst. Rev., vol. 36, no. SI, pp. 211–224,
Dec. 2003. [Online]. Available: https://doi.org/10.1145/844128.844148
[50] G. W. Dunlap, D. G. Lucchetti, M. A. Fetterman, and P. M. Chen,
“Execution replay of multiprocessor virtual machines,” in Proceedings
of the F ourth ACM SIGPLAN/SIGOPS International Conference onVirtual Execution Environments, ser. VEE ’08. New York, NY , USA:
690Association for Computing Machinery, 2008, pp. 121–130. [Online].
Available: https://doi.org/10.1145/1346256.1346273
[51] J.-D. Choi and H. Srinivasan, “Deterministic replay of java multithreaded
applications,” in Proceedings of the SIGMETRICS Symposium on
Parallel and Distributed Tools, ser. SPDT ’98. New York, NY , USA:Association for Computing Machinery, 1998, pp. 48–59. [Online].Available: https://doi.org/10.1145/281035.281041
[52] M. Xu, R. Bodik, and M. D. Hill, “A ”ﬂight data recorder”
for enabling full-system multiprocessor deterministic replay,” inProceedings of the 30th Annual International Symposium on ComputerArchitecture, ser. ISCA ’03. New York, NY , USA: Associationfor Computing Machinery, 2003, pp. 122–135. [Online]. Available:https://doi.org/10.1145/859618.859633
691