Zurich Open Repository and
Archive
University of Zurich
University Library
Strickhofstrasse 39
CH-8057 Zurich
www.zora.uzh.ch
Year: 2020
Dynamically reconfiguring software microbenchmarks: reducing execution time
without sacrificing result quality
Laaber, Christoph ; Würsten, Stefan ; Gall, Harald C ; Leitner, Philipp
DOI: https://doi.org/10.1145/3368089.3409683
Posted at the Zurich Open Repository and Archive, University of Zurich
ZORA URL: https://doi.org/10.5167/uzh-193476
Conference or Workshop Item
Published Version
Originally published at:
Laaber, Christoph; Würsten, Stefan; Gall, Harald C; Leitner, Philipp (2020). Dynamically reconfiguring software
microbenchmarks: reducing execution time without sacrificing result quality. In: 28th ACM Joint European
Software Engineering Conference and Symposium on the Foundations of Software Engineering, Virtual Event,
USA, 8 December 2020 - 13 December 2020. ACM, 989-1001.
DOI: https://doi.org/10.1145/3368089.3409683Dynamically Reconfiguring SoftwareMicrobenchmarks:
ReducingExecution Timewithout Sacrificing ResultQuality
ChristophLaaber
Universityof Zurich
Zurich, Switzerland
laaber@ifi.uzh.chStefanWürsten
Universityof Zurich
Zurich, Switzerland
stefan.wuersten@uzh.ch
Harald C. Gall
Universityof Zurich
Zurich, Switzerland
gall@ifi.uzh.chPhilippLeitner
Chalmers |Universityof Gothenburg
Gothenburg, Sweden
philipp.leitner@chalmers.se
ABSTRACT
Executingsoftwaremicrobenchmarks,aformofsmall-scaleperfor-
mance testspredominantly used forlibraries and frameworks, is a
costlyendeavor.Fullbenchmarksuitestakeuptomultiplehours
ordaystoexecute,renderingfrequentchecks,e.g.,aspartofcon-
tinuous integration (CI),infeasible. However,altering benchmark
configurationstoreduceexecutiontimewithoutconsideringthe
impact on result quality can lead to benchmark results that are not
representative ofthe software’strue performance.
We propose the first technique to dynamically stop software mi-
crobenchmark executions when their results are sufficiently stable.
Our approach implements three statistical stoppage criteria and
iscapableofreducingJavaMicrobenchmarkHarness(JMH)suite
executiontimesby 48.4%to86.0%.Atthesametimeitretainsthe
sameresultqualityfor 78.8%to87.6%ofthebenchmarks,compared
to executingthe suite for the defaultduration.
Theproposedapproachdoesnotrequiredeveloperstomanually
craftcustombenchmarkconfigurations;instead,itprovidesauto-
mated mechanisms for dynamic reconfiguration. Hence, making
dynamic reconfiguration highly effective and efficient, potentially
pavingthe wayto inclusionofJMHmicrobenchmarks inCI.
CCS CONCEPTS
·Generalandreference →Measurement ;Performance ;·Soft-
wareanditsengineering →Softwareperformance ;Software
testinganddebugging .
KEYWORDS
performance testing,software benchmarking, JMH,configuration
ACMReference Format:
Christoph Laaber, Stefan Würsten, Harald C. Gall, and Philipp Leitner.
2020. Dynamically Reconfiguring Software Microbenchmarks: Reducing
Execution Time without Sacrificing Result Quality. In Proceedings of the
28th ACM Joint European Software Engineering Conference and Symposium
ESEC/FSE ’20, November 8ś13, 2020, Virtual Event, USA
©2020 Copyright heldby the owner/author(s). Publicationrightslicensed to ACM.
Thisistheauthor’sversionofthework.Itispostedhereforyourpersonaluse.Not
for redistribution. The definitive Version of Record was published in Proceedings of
the28thACMJointEuropeanSoftwareEngineeringConferenceandSymposiumonthe
FoundationsofSoftwareEngineering(ESEC/FSE’20),November8ś13,2020,VirtualEvent,
USA,https://doi.org/10.1145/3368089.3409683 .on the Foundations of Software Engineering (ESEC/FSE ’20), November 8ś
13, 2020, Virtual Event, USA. ACM, New York, NY, USA, 13pages.https:
//doi.org/10.1145/3368089.3409683
1 INTRODUCTION
Performancetestingenablesautomatedassessmentofsoftwareper-
formanceinthehopeofcatchingdegradations,suchasslowdowns,
inatimelymanner.Avarietyoftechniquesexist,spanningfrom
system-scale (e.g., load testing) to method or statement level, such
assoftwaremicrobenchmarking.Forfunctionaltesting,CIhasbeen
arevelation,where(unit)testsareregularlyexecutedtodetectfunc-
tional regressions as early as possible [ 22]. However, performance
testingisnotyetstandardCIpractice,althoughtherewouldbea
needforit[ 6,36].Amajorreasonfornotrunningperformancetests
on every commit is their long runtimes, often consuming multiple
hoursto days [ 24,26,32].
To lower the time spent in performance testing activities, previ-
ousresearchappliedtechniquestoselectwhichcommitstotest[ 24,
45] or which tests to run [ 3,14], to prioritize tests that are more
likelytoexposeslowdowns[ 39],andtostoploadtestsoncethey
become repetitive [ 1,2] or do not improve result accuracy [ 20].
However, none of these approaches are tailored to and consider
characteristicsofsoftwaremicrobenchmarks andenablerunning
full benchmark suites, reduce the overall runtime, while still main-
tainingthe same result quality.
Inthispaper,wepresentthefirstapproachtodynamically,i.e.,
during execution, decide when to stop the execution of software
microbenchmarks. Our approach ÐdynamicreconfigurationÐ de-
termines at different checkpoints whether a benchmark execution
is stable and if more executions are unlikely to improve theresult
accuracy. It builds on the concepts introduced by He et al . [20],
applies them to software microbenchmarks, and generalizes the
approach for any kindofstoppagecriteria.
To evaluate whether dynamic reconfiguration enables reducing
executiontimewithoutsacrificingquality,weperformanexperi-
mental evaluation on ten Java open-source software (OSS) projects
withbenchmarksuitesizesbetween 16and995individualbench-
marks,rangingfrom 4.31to191.81hours.Ourempiricalevaluation
comprises of three different stoppage criteria, including the one
from He et al .[20]. It assesses whether benchmarks executed with
dynamicreconfigurationincontrolled,bare-metalenvironmentsESEC/FSE ’20, November8ś13,2020,VirtualEvent, USA ChristophLaaber,Stefan Würsten, Harald C.Gall, andPhilippLeitner
(1)maintaintheirresultqualityand(2)haveshorterexecutiontimes,
comparedto being executedwiththe defaultJMHconfiguration.
Wefindthatforthemajorityofstudiedbenchmarkstheresult
quality remains the same after applying our approach. Depending
on the stoppage criteria, between 78.8% and87.6% of the bench-
marksdonotproducedifferentresults,withanaverageperformance
changeratebetween 1.4%and3.1%.Eventhoughcomputationof
the stoppage criteria introduces an overhead between < 1% and
~11%,dynamicreconfigurationstillenables savingatotalof66.2%
to82% of the execution time across all projects. For individual
projects, benchmark suites take 48.4% to86.0% less time to execute.
Our empirical results support that dynamic reconfiguration of soft-
ware microbenchmarks is highly effective and efficient in reducing
executiontime withoutsacrificingresult quality.
Contributions. The main contributionsofour study are:
•We present the first approach to dynamically stop the ex-
ecutionofsoftwaremicrobenchmarkusingthreedifferent
stoppagecriteria.
•We provideempiricalevidencethat demonstratestheeffec-
tivenessandefficiencyofdynamicreconfigurationforten
OSS applications.
•We provide a fork of JMH that implements our approach on
Github[34]andas part ofour replication package [ 35].
•Toinvestigatewhetherreal-worldbenchmarksuitescould
benefitfromourapproachtosavetime,wecollectthelargest
datasetofJMHOSSprojects( 753projectswith 13,387bench-
marks)includingextractedsourcecodepropertiessuchas
benchmarkconfigurationsandparameters.
2 JAVA MICROBENCHMARKHARNESS(JMH)
JMH1isthede-factostandardframeworkforwritingandexecuting
softwaremicrobenchmarks(inthefollowingsimplycalledbench-
marks)forJava.Benchmarksoperateonthesamelevelofgranu-
larityasunittests,i.e.,statement/methodlevel,andaresimilarly
definedincodeandconfiguredthroughannotations.Differentfrom
unit tests where the outcome is binary, i.e., a test passes or fails
(disregarding flakiness), benchmarks produce outputs for a cer-
tain performance metric, such as execution time or throughput.
Astheseperformancemetricsareeasilyaffectedbyconfounding
factors,suchasthecomputer’shardwareandsoftware,background
process, or even temperature, one must execute benchmarks re-
peatedly to obtain rigorous results that are representative of the
software’strue performance [ 18].
Figure1depicts a standard execution of a JMH benchmark suite
B,wherebenchmarks baresequentiallyscheduled.Everybench-
markexecutionstartswithanumberofwarmupforks wf,tobring
thesystemintoasteadystate,whoseresultsarediscarded.Aforkis
JMHparlanceforrunningasetofmeasurementsinafreshJavaVir-
tual Machine (JVM). The warmup forks are followed bya number
ofmeasurementforks f(oftensimplycalledforks).Duetodynamic
compilation,everyforkisbroughtintosteadystatebyrunninga
seriesofwarmupiterations wi,afterwhichaseriesofmeasurement
iterations miare executed. An iteration has a specific duration ś
wtormtfor warmup time and measurement time, respectivelyś
1https://openjdk.java.net/projects/code-tools/jmh(incl. source codeexamples)microbenchmark suite
b1
fnf1
warmup
wi1wi2…winmeasurement
mi1mi2…min… wfn wf1…bn
…
mt wtinvocation samplesoccurrences
performance metricbbenchmark
wf warmup fork
wiwarmup iteration
ffork
mi measurement iteration
mt measurement timewt warmup time
Figure 1:JMHExecution
for which the benchmark isexecuted as often as possible, and the
performance metrics for a sample of the invocations is reported.
Performance metrics from warmup iterations are discarded, and
theunionofthemeasurementiterationsacrossallforksformthe
benchmark’sresult.Allthesevaluescanbeconfiguredbythede-
veloperthroughJMHannotationsorthecommandlineinterface
(CLI), otherwisedefaultvaluesare used.
JMHsupportsbenchmarkfixtures,i.e.,setupandteardownmeth-
ods,aswellasparameterizationofbenchmarks.Aparameterized
benchmark has a number of parameters with (potentially) multiple
values; JMH then runs the benchmark once for every parameter
combination, which are formed as the cartesian product of the
individualparameters.
JMH usesdifferent setsofdefaultconfiguration values, depend-
ing on the version: ≤1.20and≥1.21. Versions until 1.20use10
forks(f)running 40iterations( 20wiandmieach)withaniteration
time (wtandmt) of1s; starting from 1.21, defaults are 5forks (f),
5iterations(both wiandmi),and10siterationtime(both wtand
mt) [47,48]. JMHdoes not use warmupforks( wf) bydefault.
Consequently, and as Fig. 1depicts, we can define the overall
warmuptimeas tbw=wf∗(wi∗wt+mi∗mt)+f∗wi∗wt,the
overall measurement time as tbm=f∗mi∗mt, and the benchmark
executiontimeas tb=tbw+tbm+tb
fix,wheretb
fixisthetimespent
inbenchmarkfixtures.Finally,the full microbenchmarksuite exe-
cutiontime Tisthesumofallbenchmarkparametercombinations,
defined as T=/summationtext
b∈B/summationtext
p∈Pbtbp, wherePbthe set of parameter
combinations for a benchmark b. These definitions will be used in
the remainderofthe paper.
3 PRE-STUDY
Tomotivateourwork,weconductapre-studyinvestigatingwhether
benchmarkexecutiontimesareinfactaprobleminreal-worldOSS
projectsusing JMH.
3.1 Data Collection
We create, to the best of our knowledge, the most extensive OSS
JMH data set to date from Github, by querying and combiningDynamicallyReconfiguringSoftware Microbenchmarks ESEC/FSE ’20, November8ś13,2020,VirtualEvent, USA
three sources: (1) Google BigQuery’s most recent Github snap-
shot2, queried for org.openjdk.jmh import statements [ 12,32];
(2)Github’ssearchapplicationprogramminginterface(API)with
anapproachasoutlinedbyStefanetal .[49];and(3)MavenCentral
searchingfor projectswithJMHas dependency.Our finaldataset
consists of 753pre-study subjects after removing duplicate entries,
repositories that do not exist anymore, projects without bench-
marksinthe mostrecent commit, andforkedprojects.
For each project, we apply the tool bencher[30] to construct
abstractsyntaxtrees(ASTs)forJavasource-codefilesandextract
informationrelatedto(1)executionconfiguration( @Fork,@Warmup,
@Measurement , and@BenchmarkMode ) and (2) benchmark param-
eterization ( @Param). In addition, (3) we extract the JMH version
from the buildscript ( Mavenandgradle).
3.2 SummaryofPre-Study Subjects
The753projectshaveintotal 13,387benchmarkmethodsand 48,107
parametercombinations. 400(53.1%)projectsfeaturefewerthan 10
benchmarks,and 52(6.9%)projectscontain 50benchmarksormore.
On average, a project has 19.7±44.9benchmarks, with a median of
7. The project with the largest benchmark suite is eclipse/eclipse-
collections with515benchmarks.
Benchmarkparameterizationisquitecommonwithprojectshav-
ing, on average, 70.6±303.3parameterized benchmarks, with a me-
dianof9.76.9%ofthebenchmarkshave 10parametercombinations
or fewer. We find the largest number of parameter combinations
in the project msteindorfer/criterion , with4,132combinations; and
the project withthe the most parameter combinations fora single
benchmark is apache/hive , which contains an individual bench-
mark3withan astounding 2,304 different combinations. However,
themajorityofthebenchmarksarenotparameterized,i.e., 10,394
(77.6%).
Extracting the JMH version is crucial for our analysis, as the
defaultvaluesoftheexecutionconfigurationshavebeenchanged
with version 1.21(see also Section 2). However, automatically
extracting the JMH version is not possible for each project. We
are able to successfully extract the JMH version from build scripts
for573(76%) of our pre-study subjects, containing 10,816(80.8%)
benchmarks. About 20% of the projects (containing 4,115(38.0%)
benchmarks) already use the mostrecent JMHversion.
3.3 Results
We use this data to analyze how much time benchmark suites in
the wild take to execute. Figure 2adepicts a summary of bench-
mark suite execution times Tfor the573studied projects where
JMH version extraction was successful. The runtimes vary greatly,
ranging from 143millisecondsfor protobufel/protobuf-el tonoless
than7.4years for kiegroup/kie-benchmarks (clearly, this project
does not intend to execute all benchmarks at once), with a median
of26.7minutes.364(49%)benchmarksuitesrunforanhourorless,
whichisprobablyacceptable,eveninCIenvironments.However,
110(15%) suites take longer than 3hours, with 22projects ( 3%)
2https://console.cloud.google.com/bigquery?project=fh-bigquery&page=
dataset&d=github_extracts&p=fh-bigquery
3VectorGroupByOperatorBench.testAggCount
0100200300400500600
0%10%20%30%40%50%60%70%76%
0 1 2 3 4 5 6 7 8 9 10 11 12
Execution Time [h]# Projects (cum)(a) Benchmark suite execution
timesT
050010001500200025003000350040004500
0%5%10%15%20%25%30%34%
1 2 3 5 7 10 15 25 50 100
Decrease Factor (log 10)# Benchmarks (cum)(b) Decreased tbcompared to
JMH defaults
Figure 2: Impact of custom configurations on the execution
timesof(a) benchmarksuites and (b)benchmarks.
exceeding 12hoursruntime.Forexample,thepopularcollectionsli-
braryeclipse/eclipse-collections hasatotalbenchmarksuiteruntime
of over16days, executing 515benchmarks with 2,575parameter
combinations.Weconcludethatatleast 15%ofthepre-studysub-
jects would greatly benefit from an approach to reduce benchmark
executiontimes given their currentconfiguration.
Thebenchmarksuiteexecutiontimeisbasedontheextracted
JMH configurations from the projects. We speculate that devel-
opers specifically apply custom configurations to reduce the de-
faultsettingsof JMH.Indeed, 4,836(36%)benchmarkshaveacon-
figuration change that affects its runtime, of which 4,576(34%)
have a decreased benchmark time tbwith respect to JMH de-
faults(seeFig. 2b).Weobservethatforthemajorityofthebench-
marks the execution time is in fact drastically reduced: for 3,735
(28%) and2,379(18%) by a factor≥5 and≥10, respectively. Still
374(3%) benchmarks are reduced by a factor ≥50. While only a
minority of 250(2%) of the benchmarks belonging to just 17(3.0%)
ofprojectsareconfiguredtoincreaseexecutiontimecomparedto
the defaults.
Pre-Study Summary. OSS developers extensively customize
benchmark configurations, often setting their values consider-
ably lower than the JMH default. Despite these changes, 15% of
the projects still have a benchmark suite execution time of over 3
hours.Thesefindingsindicatethatdevelopersofmanyprojects
could be supported by a data-driven way to reduce the execution
time ofJMH benchmarks.
4 DYNAMICRECONFIGURATION
In Section 3, we found that real-world OSS benchmark suites of-
ten are configured to considerably reduce runtime, with respect to
JMH’sdefaults;stillmanyrunformultiplehours,makingiteffec-
tively impossible toassessperformance on every software change.
We hypothesize that this time reductionis an effort by developers
tokeepbenchmarksuiteruntimesreasonablewithoutconfirming
that benchmarkresults remainstable (accurate).
This section introduces an approach to dynamically stop bench-
markswhentheirresultisstable,withthegoalofsavingexecution
time withoutsacrificingquality.ESEC/FSE ’20, November8ś13,2020,VirtualEvent, USA ChristophLaaber,Stefan Würsten, Harald C.Gall, andPhilippLeitner
i1i2i3i4i5i6i7i8i9i10i11i12i13i14i15i16i17i18i19i20
f1
f2
f3
f4
f5
(a)Staticconfiguration
i1i2i3i4i5i6i7i8i9i10i11i12i13i14i15i16i17i18i19i20
f1
f2
f3
f4
f5
t
(b) Dynamic reconfiguration
Figure 3: Standard JMH execution with static configuration
vs. dynamic reconfiguration approach. A yellow box is a
warmup iteration, a blue box is a measurement iteration,
and a dashed box is a skipped iteration. A solid line indi-
cates that the stoppage criterion is met, and a dashed line
indicates theopposite.
4.1 Approach
JMH allows developers to define benchmark configurations before
execution, either through annotations or CLI parameters, and then
executesallbenchmarksaccordingtothisconfiguration(seeSec-
tion2).Wecall thisthełstaticconfigurationžofa benchmarkexe-
cution.Figure 3ashowsthestaticconfigurationwhereeveryrow
indicatesaJMHfork(f 1śf5)andeverycolumn/rectangleanitera-
tion (i1śi20) of the corresponding fork. Yellow rectangles (i 1śi10)
indicatewarmupiterations,andbluerectangles(i 11śi20)indicate
measurementiterations.Thisstaticconfigurationbearstheproblem
thatallforksareexecutedwiththesameconfiguration,irrespective
oftheaccuracyoftheresults,potentiallywastingpreciousruntime.
Inordertostopbenchmarkexecutionswhentheirresultisac-
curateenough,weproposedynamicbenchmarkreconfiguration,
i.e.,an approachthatdynamicallydecides,at certain checkpoints,
when the benchmark results are unlikely to change with more
executions. This happens at two points: (1) within a fork, when
the execution reaches a steady state, i.e., the warmup phase was
executedlongenough,and(2)afterafork’sexecution,whenitis
unlikely that more forks will lead to different results. Figure 3b
illustrates dynamic reconfiguration. Vertical bars indicate check-
points after iterations (line 7), horizontal bars indicate checkpoints
afterforks(line 10),andwhite,dashedboxesrepresentiterations
that are skipped.
Algorithm 1depicts the pseudo code for our dynamic reconfigu-
ration approach. The algorithm takes the benchmark bto execute,
itsextendedJMHexecutionconfiguration Cb,astabilityfunction
stablethat is executed at every checkpoint, and a threshold tfor
decidingwhatisconsideredstable. Cbisatupleofconfigurationval-
ues defined as Cb=⟨wimin,wimax,mi,fmin,fmax,wf,wt,mt⟩
(seealsoSection 2).Notethatcheckpointsonlyhappenafteri 5andAlgorithm1: Dynamicreconfigurationalgorithm
Input:b∈B: the benchmarkto execute
Cb=⟨wimin,wimax,mi,fmin,fmax,···⟩: execution
configurationfor b
stable:M×T/mapsto→{true,false}: stability function at thresh-
oldt∈Tfor a set of measurements M′∈M
t: stability threshold (specificfor stable)
Data:execute :B/mapsto→M: executes a benchmarkiteration
Result:Measurements Mbof the benchmark b
1begin
2Mb←∅
3forf←1tofmaxdo
4 Mw←∅
// dynamic warmup phase
5 forwi←1towimaxdo
6 Mw←Mw∪execute (b,Cb)
// warmup stoppage
7 ifwi≥wimin∧stable(Mw,t)thenbreak
// measurement phase
8 for1tomido
9 Mb←Mb∪execute (b,Cb)
// fork stoppage
10 iff≥fmin∧stable(Mb,t)thenbreak
11returnMb
f2in the example, defined as wiminandfmin. If a benchmark is
notstableatacheckpoint,thebarisdashed(solidotherwise)and
the warmupphasecontinues oranotherforkisspawned.
Tocircumventthesituationwhereabenchmark’swarmupphase
never reaches a steady state or the overall measurements are never
accurateenough,ourapproachtakesamaximumnumberofwarmup
iterations ( wimax) and forks ( fmax), e.g., f3has a dashed bar after
thelastwarmupiteration.Thisguaranteesthatasinglebenchmark
executionneverexceedsaconfigurabletimebudget,whichdefaults
to JMH’swarmupiterations ( wi) andforks( f).
Benchmarks often exhibit multiple steady states resulting in
multi-modaldistributions,andoutliers,duetonon-deterministic
behavior, might still occur even after stableconsidered a fork to
be in a steady state [ 19]. Therefore, our approach uses a fixed
number of multiple measurement iterations mi(lines8ś9), as a
single measurement iteration would not accurately represent a
fork’sperformance.
4.2 StoppageCriteria
Todecidewhetheraforkreachedasteadystate(line 7)orthegath-
eredmeasurementsarestable(line 10),ourapproachneedstodecide
whether more measurements providesignificantlymore accurate
results. For this, we rely on statistical procedures on the perfor-
mance measurement distributions. Thatis, if more measurements
(i.e., data points) are unlikely to change the result distribution, we
consider the measurement stable. There are three key aspects to
consider: (1) a stability criteria sc:M/mapsto→R+that assigns a stability
values∈Rto a set of measurements M′∈M; (2) a threshold
t∈Tthat indicates whether a stability value sis considered stable;
and (3) a stability function stable:M×T/mapsto→{true,false}that,
basedonasetofstabilityvalues (extractedfromasetof measure-
mentsM′∈M) and a threshold t∈T, decides whether a set of
performance measurements isstable ornot.DynamicallyReconfiguringSoftware Microbenchmarks ESEC/FSE ’20, November8ś13,2020,VirtualEvent, USA
4.2.1 Stopping Warmup Phase. The first stoppage point (line 7)
decides whether a fork is in a steady state, which indicates the end
ofthewarmupphaseand thestartofthemeasurementphase.For
this, the dynamic reconfiguration approach uses a sliding-window
technique where the measurement distributions at the last itera-
tionsarecomparedalongastabilitycriteria.Letusconsidertheset
of warmup measurements Mw(across multiple warmup iterations)
suchthat mi∈Mwisthemeasurementatthe ithiteration.Wethen
definethesliding-windowwarmupvector Wi′′afteracurrentitera-
tioni′′, a sliding-window size sW, and the resulting start iteration
ofthe window i′=i′′−sW:i′≥1inEq. (1).
Wi′′=/angbracketleftBig
sc/parenleftBigx/uniondisplay
i=i′mi/parenrightBig/barex/barex/barexi′≤x≤i′′/angbracketrightBig
(1)
4.2.2 Stopping Forks. The second stoppage point (line 10) decides
whetherthebenchmarkmeasurementresults Mbaresufficiently
stable andnoadditionalforkneedsto be spawned,therefore stop-
ping the execution of benchmark b. Let us consider the set of mea-
surements Mb(across multiple forks) such that Mb
f⊆Mbis the
subset of measurements at fork number f. We then define the fork
vectorFf′′after acurrentfork f′′inEq. (2).
Ff′′=/angbracketleftBig
sc/parenleftBigx/uniondisplay
f=1Mb
f/parenrightBig/barex/barex/barex1≤x≤f′′/angbracketrightBig
(2)
4.2.3 StabilityCriteriaandFunction. Thedynamicreconfiguration
approach allows for different stability criteria ( sc) and functions
(stable), andwe identifiedandevaluatedthree:
Coefficientofvariation(CV): coefficient of variation (CV) is a
measure of variability under the assumption that the distribu-
tionisnormal.However,performancedistributionsareusually
non-normal, e.g., multi-modal or long-tailed [ 11,37]. As a sta-
bility criteria sc, CV might still be a łgood enoughž proxy to
estimateabenchmark’sstability,especiallyduetoitslowcom-
putational overhead. Depending on the benchmark, the stability
values in the vector v∈{Wi′′,Ff′′}converge towards different
values,makinga globalthreshold tforallbenchmarksunrealis-
tic. Instead, we compare all stability values from vsuch that the
delta between the largest and the smallest is at most t. Formally,
stablevar(M′,t)=true⇐⇒max(v)−min(v)≤t.
Relative confidenceinterval width(RCIW): Thesecondstabil-
ity criteria scÐrelative confidence interval width (RCIW)Ð is
similar to CV, as it estimates a benchmark’s variability, hence
stablevaralsoapplieshere.DifferentfromCV,weemployatech-
nique based on Monte Carlo simulation called bootstrap [ 13,21]
toestimatetheRCIWforthemean.Forthis,weutilizethetool
pa[31]thatimplementsatechniquebyKaliberaandJones [27].
It uses hierarchical random resampling [ 43] with replacement,
which is tailored to performance evaluation. The hierarchical
levels are (1) invocations, (2) iterations, and (3) forks (we refer to
pa[31]andKalibera andJones[ 27]for details).
Kullback-Leiblerdivergence (KLD): The third stability criteria
scuses a technique outlined by He et al . [20]that constructs a
probability that two distributions d1andd2are similar based
on the Kullback-Leibler divergence (KLD)[ 29].sccomputes thisprobability(foreveryelementofthevector v)whered1isthemea-
surement distribution excluding the last measurement (warmup
iterationiorforkf)andd2isthemeasurementdistribution in-
cludingthelastmeasurement.Consequentlyanddifferentfroma
variability-basedstabilitycriteria,thevector vconsistsofprob-
abilities rather than variabilities. The stability function stable
checkswhetherthemeanprobabilityofthestabilityvaluesfrom v
areabovethethreshold t.Formally, stableprob(M′,t)=true⇐⇒
mean(v)>t.
4.3 Modified JMH Implementation
Weimplementedthedynamicreconfigurationapproachwiththe
three stoppage criteria for JMH version 1.21, by adding a recon-
figurationbenchmarkmodewithstoppagecriteria( scandstable)
andthreshold( t)properties,annotationpropertiesfor wiminand
fmin,andcorrespondingCLIflags.Additionally,weadaptedJMH’s
console and JavaScript Object Notation (JSON) result file output to
include the newconfigurationoptionsandaddedawarningif the
stability criterion has not been met for a benchmark. The modified
fork of JMH is available on Github [ 34] and part of our replication
package [ 35].
5 EMPIRICAL EVALUATION
Toassesswhetherdynamicreconfigurationiseffectiveandefficient,
weconductanexperimentalevaluationonasubsetoftheJavaOSS
projects identified in our pre-study (see Section 2). Our evaluation
comparesthreedynamicreconfigurationapproaches(oneforevery
stoppage criterion). As a baseline for comparison, we use standard
JMHwithstaticconfigurationandthe defaultvalues.
To support open science, we provide all evaluation data and
scripts inareplication package [ 35].
5.1 Research Questions
First, we want to ensure that dynamic reconfiguration does not
changetheresultscomparedtostaticconfiguration.Iftheresults
of the same benchmark executed with static configuration and
withdynamicreconfigurationareequal,weconcludethatdynamic
reconfigurationiseffectiveinpreservingresultquality.Forthis,we
formulate RQ1:
RQ 1How does dynamic reconfiguration of software microbench-
marksaffecttheirexecutionresult?
Second,wewanttoevaluateifdynamicreconfigurationimproves
theoverallruntimeofabenchmarksuite,comparedtostaticcon-
figuration,includingtheoverheadimposedbythestoppagecriteria
computation.For this, we formulate RQ2:
RQ 2How much time can be saved by dynamically reconfiguring
software microbenchmarks?
Asabenchmark’sresultquality(accuracy)andruntimearecom-
peting objectives, the combination of the results from RQ 1 and
RQ2validateswhetherdynamicreconfigurationenablesłreducing
executiontime withoutsacrificingresult qualityž.
5.2 StudySubjects
Evaluating the dynamic reconfiguration approach on all 753pre-
study subjects (see Section 3) is infeasible as executing benchmarkESEC/FSE ’20, November8ś13,2020,VirtualEvent, USA ChristophLaaber,Stefan Würsten, Harald C.Gall, andPhilippLeitner
Table 1:Selected studysubjects.All projectsare hosted on Github except the ones indicated
Name Project Version #Benchs.1#Param.
Benchs.1Exec.
TimeDomain
byte-buddy raphw/byte-buddy c24319a 39 39 5.42h Bytecode manipulation
JCTools JCTools/JCTools 19cbaae 60 148 20.56h Concurrentdata structures
jdk jmh-jdk-microbenchmarks2d0fab23 994 1,381 191.81h Benchmarksof the JDK
jenetics jenetics/jenetics 002f969 40 40 5.56h Geneticalgorithms
jmh-core jmh-core-benchmarks3a07e914 110 110 15.28h Benchmarksof JMH
log4j2 apache/logging-log4j2 ac121e2 358 510 70.83h Logging
protostuff protostuff/protostuff 2865bb4 16 31 4.31h Serialization
RxJava ReactiveX/RxJava 17a8eef 217 1,282 178.06h Asynchronous programming
SquidLib SquidPony/SquidLib 055f041 269 367 50.97h Visualization
zipkin openzipkin/zipkin 43f633d 61 61 8.47h Distributedtracing
1The numberscorrespond to succeeding benchmarks and excludes 38failing parameterizations. Seeourreplicationpackage for a list [ 35]
2Repository: http://hg.openjdk.java.net/code-tools/jmh-jdk-microbenchmarks
3Moduledirectoryin repository: https://hg.openjdk.java.net/code-tools/jmh
suitespotentiallytakesalongtime.Hence,we performpurposive
sampling [ 5] to select a subset of ten, non-trivial projects from a
widevarietyofdomainswithsmall( 16)tolarge( 994)benchmark
suites. Our evaluation executes all 3,969benchmark parameter
combinationsofthetenstudysubjects,whichare 8.2%ofthe48,107
parametercombinationsfrom the pre-study.
Table1liststhestudysubjectswiththeirnumberofbenchmarks
(ł# Benchs.ž) and benchmark parameter combinations (ł# Param.
Benchs.ž), gitversionusedfortheevaluation(łVersionž),andexe-
cutiontime when using JMHdefaultvalues(łExec. Timež).
5.3 StudySetup
We execute all benchmarks, retrieve the benchmark results, and
afterwards apply dynamic reconfiguration and the stoppage cri-
teria to the obtained data set. This allows us to experiment with
thresholdsand parameterswithout having to rerunthe fullbench-
marksuiteswithourmodifiedJMHimplementation(withdynamic
reconfiguration).
5.3.1 Execution and Data Gathering. As performance measure-
ments are prone to confounding factors [ 11,15,18,37,40], we
apply the subsequent steps to follow a rigorous methodology in
order to increaseresult reliability.
(1) Allbenchmarksuites are patchedwithJMH 1.21.
(2)WecompileandexecuteallbenchmarkswithAdoptOpenJDK
andJavaHotSpotvirtualmachine(VM)version 1.8.0_222-b10 ,
exceptlog4j2whichrequiresaJavaDevelopmentKit(JDK)
version≥9,hence we employ version 13+33.
(3)We run the benchmarks on a bare-metal machine [ 4,46]
with a12-core Intel Xeon X5670 @2.93GHz CPU, 70GiB
memory,andaSamsungSSD 860PROSATAIIIdisk,running
ArchLinuxwithakernel version 5.2.9-1-1-ARCH .
(4)All non-mandatory background processes except ssh are
disabled, without explicitly disabling software/hardware op-
timizations.
(5)Regardingbenchmarksuiteexecution,weconfigureandexe-
cuteallbenchmarkswithfiveforks f,100measurementiter-
ationsmi,1smeasurementtime mt,andJMH’s samplemode,setthroughJMH’sCLI.Thisconfigurationcorrespondstothe
JMH1.21defaults,only mtchangesfrom 10sto1sbut,atthe
sametime, miincreasesbyafactorof 10,whichgrantsour
approach more checkpoints. Note that warmup iterations
wiare settozerobut miis doubled(from 50to100), which
is required to obtain results for every iteration to dynami-
cally decide whentostop the warmupphase. The resulting
execution configuration is then Cb=⟨0,0,100,5,5,0,0s,1s⟩.
(6)We remove outliers that are a magnitude larger than the
median.
5.3.2 Approach. Withtheobtainedperformance resultsfromthe
suite executions, we evaluate dynamic reconfiguration with the
followingconfigurationparameters.Recalltheconfigurationdefi-
nitionCb=⟨wimin,wimax,mi,fmin,fmax,wf,wt,mt⟩(see Sec-
tion4.1).
StaticConfiguration(Baseline). Thebaseline,i.e.,JMHwithstatic
configuration, uses the JMH 1.21default configuration for all
benchmarks.Forthis,weremovefromthegathereddatathefirst
50iterations(correspondingto wi)fromeachforkandusethe 50
remainingiterationsas mi.Hence,thebaselinehasthefollowing
configuration: Cb=⟨50,50,50,5,5,0,1s,1s⟩.
We consciously decided for the JMH default configuration as
baseline and against the developers’ custom benchmark configura-
tionsforthefollowingreasons:(1) 36%ofthepre-studybenchmarks
change the benchmark execution time through custom configu-
rations, hence, 64% of the benchmarks still use the JMH default
configuration; (2) the majority of these benchmarks ( 28% of all
pre-studybenchmarks)onlyuseasinglefork f,whichisconsid-
eredbadpracticeasinter-JVM-variabilityiscommon[ 32],basically
invalidating developers’ custom configurations for rigorous bench-
marking;and(3)aunifiedbenchmarkconfigurationasthebaseline
enables comparabilityacrossour study subjects.
DynamicReconfiguration. Forthedynamicreconfigurationap-
proaches,weemploytheconfiguration Cb=⟨5,50,10,2,5,0,1s,1s⟩
for all benchmarks, which changes the minimum warmup itera-
tions (wimin=5) and minimum forks ( fmin=2) compared to theDynamicallyReconfiguringSoftware Microbenchmarks ESEC/FSE ’20, November8ś13,2020,VirtualEvent, USA
baseline.Notethatwealsoreduce mito10insteadof 50,whichthe
baseline uses. Initialexperiments showed that an increasein mea-
surementiterations, afterasteadystateisreached,hasonlyaminor
effectonresult accuracy butwithconsiderably longer runtimes.
Weusethefollowingparametersforthethreedynamicrecon-
figuration approaches(one per stoppagecriterion).
(1)Wedrawaweightedsampleof 1,000invocationsperiteration
to reduce computationaloverheadat checkpoints.
(2) The sliding-windowsize issetto sW=5.
(3)CVuses a threshold t=0.01, which corresponds to a maxi-
mumvariability difference inthe slidingwindowof1%.
(4)RCIWusesa99%confidencelevel, 1,000bootstrapiterations
(which is a good tradeoff between runtime overhead and
estimation accuracy), and a threshold t=0.03 following
bestpractice[ 18].
(5)KLDpartitionsthedistributions d1andd2into1,000strips
for the KLD calculation [ 20]; removes outliers that are more
than 1.5×IQRaway from the median; and uses a threshold
t=0.99, which corresponds to a mean probability within
theslidingwindowof 99%orlarger.Morestripswouldresult
in longer calculation times for the kernel density estimation
and,consequently,inahigherruntimeoverhead.Without
theoutlierremoval,KLDwouldnotconvergeabovetheprob-
ability threshold t, and, hence, our approach would not stop
thebenchmarkexecution.Notethattheoutlierremovalis
onlyperformedaspartofthestoppagecriteriacalculation
ofourapproach;fortheevaluation,weconsiderallmeasure-
mentsand donotremove any outliers(see Section 5.4).
5.4 Results andAnalysis
We now present the results of our empirical evaluation by compar-
ing the benchmark results of the static configuration to the ones of
ourdynamicreconfigurationapproacheswiththethreestoppage
criteria.
5.4.1 RQ1:ResultQuality. Toassesswhetherapplyingdynamic
reconfiguration changes benchmark results and to answer RQ 1,
we perform two analyses between the execution results coming
from the baseline with static configuration and each ofthe three
dynamicreconfigurationapproaches:(1)statisticalA/Atestsand
(2) mean performance changerate.
A/ATests. AnA/Atestcheckswhetherresultsfromtwodistribu-
tionsarenotsignificantlydifferent,wherenodifferenceisexpected.
In our context, this means if an A/A test between static configu-
rationanddynamicreconfiguration(foreachstoppagecriterion)
doesnotreportadifference,weconcludethatdynamicreconfigura-
tiondoesnotchangethebenchmarkresult.Followingperformance
engineeringbestpractice[ 9,10,27,33],weestimatetheconfidence
interval for the ratio of means with bootstrap [ 13], using10,000it-
erations [ 21], and employing hierarchical random resampling with
replacement on(1) invocation,(2) iteration, and (3) forklevel [ 27]
(againrelyingon pa[31]).Iftheconfidenceinterval(oftheratio)
straddles 1, there is no statistically significant difference. Note that
thisprocedureisdifferentfromthestoppagecriteriaRCIW(seeSec-
tion4);herewecomparetheresults(allmeasurementiterations miTable 2: Result quality differences between static configura-
tion approachanddynamic reconfiguration approaches
CV RCIW KLD
A/A tests notdifferent 78.8% 87.6% 79.6%
Meanchangerate 3.1% ±8.1% 1.4%±3.8% 2.4%±7.4%
#benchs<1% 57.4% 73.2% 62.3%
#benchs<2% 72.4% 87.0% 78.2%
#benchs<3% 79.6% 91.9% 84.6%
fromallforks f)oftwotechniques,whereasRCIWusesconfidence
intervalwidthsas avariability measure of asingletechnique.
The first row of Table 2shows the A/A results. For a majority of
the3,969benchmarkparametercombinations,applyingdynamic
reconfiguration does notresult in significantly different distribu-
tions. About 80% or more of the benchmarks have similar result
distributions comparedtothestatic configuration.RCIWachieves
thebestresultwith 87.6%,whileCVandKLDperformsimilarlywell
with78.8% and79.6%, respectively. Note that the static approach
uses50measurementiterations( mi)whilethedynamicapproach
łonlyž runs 10, indicating that if a steady state is reached (which is
onegoalofdynamicreconfiguration)moremeasurementiterations
have anegligibleimpact onthe overallresult.
ChangeRate. InadditiontoA/Atests,weassesstheperformance
change rate between the static configuration approach and each
of the dynamic reconfiguration approaches, i.e., by how much the
means of the performance result distributions differ. The change
rate augments the A/A tests’ binary decision, by showing how
differentthebenchmarkresultsbecomewhenapplyingdynamic
reconfiguration.
ThesecondrowofTable 2showsthemeanchangerateacross
all benchmarks in percent and its standard deviation. The mean
changeratebetweenthethreestoppagecriteriaandthestaticap-
proach is ~ 3% or lower for all three. Note that, following a rigorous
measurement methodology, ~ 3% could still be caused by JVM in-
stabilities unrelated to our approach [ 19]. Again, RCIW is the best
criterion with 1.4%±3.8%. Finally, the last three rows show how
many benchmarks have a change rate below 1%,2%, and3% for all
stoppage criteria. We observe that RCIW outperforms the other
twosignificantly,followed byKLD.~ 73%ofthebenchmarkshave
achangeratebelow 1%,~87%below2%,and~92%below3%.This
suggests that RCIW is a highly effective technique for stopping
benchmarkexecutions.
Figure4depicts the change rate distributions per project and
stoppage criterion, where every data point corresponds to a bench-
mark’s mean performance change. Considering the median change
rateofaproject’sbenchmarks,RCIWperformsbestforallprojects
exceptjenetics,jmh-core, andSquidLib where KLD is slightly su-
perior. CV consistently has the largest change rates of the three
stoppagecriteria;nonetheless,itperformsonlyslightlyworsein
mostcases.Consideringthemeanchangerate,RCIWisthemostac-
curate stoppage criteria for 9/10projects, with only jmh-corebeing
morestablewhenKLDisemployed.Notethatfortheprojectswhere
RCIW is not the best stoppage criterion, both mean and medianESEC/FSE ’20, November8ś13,2020,VirtualEvent, USA ChristophLaaber,Stefan Würsten, Harald C.Gall, andPhilippLeitner
0%2%5%8%10%12%
byte-buddy JCTooolsjdkjenetics jmh-corelog4j2protostuff RxJava SquidLibzipkin
Study SubjectChange RateStoppage Criteria
 CV KLD RCIW
Figure 4: Mean change rate per study subject and stoppage
criteria. The bar indicates the median, the diamond the
mean,theboxtheIQR,andthewhiskers [Q1|Q3]+1.5∗IQR.
changeratesarebelow 1%.Theprojectswiththemostdiverging
benchmarks between static configuration and dynamic reconfig-
uration execution are byte-buddy ,JCTools,log4j2, andSquidLib.
Thebenchmarksoftheseprojectsarelessstablecomparedtothe
other projects, likely dueto executing non-deterministicbehaviour
such as concurrency and input/output (I/O). Results from bench-
marksthatarelessstablewillpotentiallyhavestatisticallydifferent
distributionsand,therefore,not maintain the same result quality.
UnreachbleStabilityCriteria. Ifthestabilityfunction stablenever
evaluatesthemeasurementsafterawarmupiterationoraforkas
stable,themaximumnumberofwarmupiterations( wimax)orforks
(fmax)areexecuted.Thiscorrespondstothestaticconfiguration
of JMH. We analyzed how often stability is not achieved according
to the three stoppage criteria across all study subjects. CV is the
most lenient criterion with only 1.0% of the benchmarks’ forks
not considered stable after 50warmup iterations and 12% of the
benchmarksinsufficientlyaccurateafterfiveforks.KLDachieves
similar numbers ( 0.8%) for warmup iterations, however 46.4% of
the benchmarks were not considered stable after five forks. RCIW
is even more restrictive where 46.7% and37.9% of the benchmarks
do not reach the stability criteria after wimaxandfmax, respec-
tively.ThisrestrictivenessimpactstheA/Atestandmeanchange
rate results, leading to benchmark results with higher quality. Not
reachingthestabilitycriteriacaneitherhappenifthethreshold t
is too restrictive or the benchmark is inherently variable, whichis
acommon phenomenon[ 32,33].
RQ 1 Summary. Applying dynamic reconfiguration does not
changetheresultqualityofthemajorityofthebenchmarks,when
compared to the static configuration. The RCIW stoppage criteria
outperforms KLD and CV, with 87.6% of the benchmarks main-
tainingtheirresultqualityandameanperformancechangerate
of1.4%.
5.4.2 RQ2:TimeSaving. Themaingoalofdynamicreconfiguration
is to save time executing benchmark suites. For this, and to answer
RQ2,we(1)measuretheruntimeoverheadofthethreestoppage
criteria,(2)estimatethetimesavingforallprojectscomparedtothestatic configuration, and (3) show at which checkpoint (warmup or
fork) more time can be saved.
RuntimeOverhead. Tomeasuretheruntimeoverheadofthethree
stoppagecriteria,weexecutethebenchmarksuiteof log4j2once
with standard JMH 1.21(i.e., static configuration) and once for
eachstoppagecriteriawithourJMHforkimplementingdynamic
reconfiguration. To ensure a valid comparison between the four
measurements(staticconfiguration+dynamicreconfigurationof
threestoppagecriteria),weusethesameconfigurationforthestatic
andthedynamicapproachesof Cb=⟨5,90,10,2,5,0,1s,1s⟩,butdo
notstopatthestoppagecheckpoints.Wemeasuretheend-to-end
executiontime tb′ofeverybenchmark bwhenexecutedthrough
JMH’s CLI. This time includes JVM startup, benchmark fixtures,
benchmark execution, and stoppage criteria computation, which is
negligible comparedto the duration of the measurement. Note that
thenumberofdatapointsusedforthestoppagecriteriacalculation
isindependentofthestudysubjectbyconstructionofJMHandour
approach;thereforeitissufficienttomeasuretheoverheadbased
onone project(see adiscussiononthis inSection 7).
The overheads o∈Oof allbenchmarksfor astoppagecriteria is
O=/uniontext
b∈Btb′
dyn/tb′
sta−1, where tb′
dynis the execution time of the
dynamic reconfiguration with a specific stoppage criteria, and tb′
sta
istheexecutiontimeofthestaticconfiguration.Theoverheads o
are independent of the number of iterations and forks executed,
becausetheyarefactorsoftheruntimedifferencebetweendynamic
reconfiguration with one stoppage criterion and the static configu-
ration (i.e., standard JMH), and all our overhead measurements use
the same configuration Cb.
The overheads we measure are oCV=0.88%±0.34% for CV,
oRCIW=10.92%±0.63%forRCIW, oKLD=4.32%±0.65%for
KLD. Note that changing the iteration time of 1s and executing
benchmarks on different hardware might affect the overhead. The
considerable difference in overhead is explained by the complexity
ofthestoppagecriteriacalculations.WhereasCViscomputationally
cheap (it only needs to compute standard deviation, mean, and
their difference), RCIW is computationally intensive due to the
simulations required for bootstrap. Because there is hardly any
overheadvariability( <1%)amongallbenchmarks,weconsiderthe
overheadconstantandusethemeanvaluefortheremainderofthe
experiments.
Time Saving Estimation. To estimate the overall time that can
be saved with dynamic reconfiguration, we adapt the execution
timeequation tb(seeSection 2)toincorporatethestoppagecriteria.
The dynamic reconfiguration benchmark execution time is then
tb
dyn=/summationtext
f∈forks[(1+o)∗wif∗wt+mi∗mt].forkscorresponds
to the number of executed forks fof a benchmark according to
the stoppage criterion, wifto the number of warmup iterations
in this fork f, and the rest according to Cbfrom Section 4.1. For
simplicityandbecauseofthelowvariabilitybetweenbenchmark
overheads, we disregard benchmark fixture times. The total bench-
mark suite execution time when using dynamic reconfiguration is
thenTdyn=/summationtext
b∈B′tb
dyn, whereB′is the set of benchmark param-
eter combinations.DynamicallyReconfiguringSoftware Microbenchmarks ESEC/FSE ’20, November8ś13,2020,VirtualEvent, USA
Table 3:Time savingperprojectandstoppage criteria
Project Time Saving
CV RCIW KLD
byte-buddy 4.42h (81.7%) 2.62h (48.4%) 4.22h (77.8%)
JCTools 17.42h (84.8%) 11.45h (55.7%) 17.13h (83.3%)
jdk 157.32h (82.0%) 135.57h (70.7%) 154.41h (80.5%)
jenetics 4.78h (86.0%) 3.37h (60.7%) 4.52h (81.4%)
jmh 12.76h (83.5%) 12.69h (83.1%) 12.42h (81.3%)
log4j2 54.56h (77.0%) 39.12h (55.2%) 55.96h (79.0%)
protostuff 3.43h (79.6%) 2.91h (67.7%) 3.44h (79.8%)
RxJava 147.91h (83.1%) 121.55h (68.3%) 138.68h (77.9%)
SquidLib 43.07h (84.5%) 30.70h (60.2%) 41.11h (80.7%)
zipkin 6.17h (72.8%) 4.93h (58.2%) 6.59h (77.8%)
Total 451.84h (82.0%) 364.92h (66.2%) 438.48h (79.5%)
Table3shows the time saving perprojectand stoppage criteria
inabsolutenumbers(hours)andrelativetothestaticconfiguration.
Weobservethatdynamicreconfigurationwithallthreestoppage
criteriaenablesdrastictimereductionscomparedtostaticconfig-
uration. In total, CV and KLD save ~ 80% and RCIW ~ 66% of the
benchmarksuiteexecutiontimesofallprojectscombined.Forindi-
vidualprojects,thetimesavingrangesbetween 72.8%and86.0%for
CV,48.4%and83.1%forRCIW,and 77.8%and83.3%forKLD.Even
withthecomputationallymostexpensivetechnique,i.e.,RCIW,we
can save at least 48.4% of time. In total numbers, the savings are
between 3.43h and157.32h for CV, 2.62h and135.57h for RCIW,
and3.44hand154.41h for KLD.
Stoppage Criteria Checkpoints. Dynamic reconfiguration defines
two points during benchmark execution when to stop: (1) after the
warmup phaseif measurements are stablewithin a fork and (2) af-
tera forkifmeasurementsacross forksarestable.Inour analysis,
the range of warmup iterations is from five ( wimin) to50(wimax),
andforksarebetweentwo( fmin)andfive( fmax)(seeCbinSec-
tion4.1).AlthoughCVandKLDsaveasimilaramountoftime,they
havedifferentstoppagebehavior.WhereCVrequiresmorewarmup
iterations( 18.5±9.4)thanKLD( 14.1±6.9),theoppositeisthecasefor
forkswith 3.1±1.2vs.4.1±1.2,respectively.RCIW,whichsavescon-
siderablylesstime,demandsmorewarmupiterations( 34.6±16.6)
to consider a fork stable but lies between CV and KLD in terms
of forks (3.3±1.4). The reported numbers are arithmetic means (of
warmupiterationsandforks)withstandarddeviationsacrossall
benchmarks of all study subjects. Generally, warmup iterations
aremorereducedthanforksinoursetup,indicatingthatfork-to-
forkvariabilityismorepresentthanwithin-forkvariance,thatis
variability across multiple JVMs compared to within a JVM, re-
spectively.Dynamicreconfigurationenablesfindingthesweetspot
betweenshorteningwarmupiterationsandforksincombination
withacertainstoppagecriteria.
RQ2Summary. Withruntimeoverheadsbetween< 1%and~11%,
dynamicreconfigurationenablesreducingbenchmarksuiterun-
timesby 48.4%to 86.0%compared to JMH’sdefaultruntime.6 DISCUSSION AND RECOMMENDATIONS
Our pre-study (see Section 3) shows that developers often dras-
tically reduce benchmark execution times. We see two potential
reasonsforthis:(1)thebenchmarksuiteruntimesaretoolong,and,
consequently, developers trade shorter runtimes for inaccurate re-
sults; or (2) JMH defaults are overly conservative, and benchmarks
withshorterruntimesoftenstillproduceresultsthatareconsidered
sufficientlyaccurate.Wehypothesizethattheformerismorelikely,
butleavethedeveloperperspectiveforconfigurationchoicesfor
futurework.Inanycase,theproposeddynamicreconfigurationap-
proachenablesreducingtimewhilemaintainingsimilarbenchmark
results, as our empirical evaluation shows.
RecommendationsforDevelopers. Developersareadvisedtoei-
ther assess their benchmark accuracies when executed in their
environment and adjust configurations accordingly, or employ dy-
namicreconfigurationwhichisabletoadjusttodifferentexecution
environments.Thechoiceofstoppagecriteriadependsonthere-
quiredresult qualityand,therefore,the performancechangesizes
desiredtobedetected.Forslightlylessaccurateresultsbutmore
time reduction, we recommend using KLD, otherwise RCIW is
preferred. The exact threshold tdepends on the stability of the
execution environments the benchmarks that are run in it. If a
controlled, bare-metal environment is available, we suggest the
thresholds of our study. In a virtualizedor cloud environment, the
thresholds needtobeadjusted(see alsoHeetal . [20]).The effec-
tiveness of our technique in non-bare-metal environments, such
as in the cloud, is subject to future research. Moreover, whether
a combination of different stoppage criteria, e.g., stopping when
both KLD and RCIW deem a benchmark run to be stable, improves
result accuracy also requires further research. Such a combination
would,however,negativelyaffecttheruntimeoverheadofdynamic
reconfiguration.
Microbenchmarks in CI. The long benchmark execution times
(see Section 3and [24,32,45]) are a major obstacle for including
microbenchmarks in CI [ 6]. To overcome this hurdle, a combina-
tionofourtechniquewithbenchmarkselection[ 14],benchmark
prioritization[ 39],and riskanalysisoncommits[ 24]would reduce
therequiredtimeformicrobenchmarkingandpotentiallyenableCI
integration.Continuouslyassessingsoftwareperformancewould
increaseconfidencethatachangedoesnotdegradeperformance
andlikely be beneficialfor performance bugroot cause analysis.
ChoosingJMHConfigurationParameters. ChoosingJMHconfigu-
rationparametersthatkeepexecutiontimelowandresultaccuracy
highis non-trivial,and developers decrease configurations drasti-
cally.Ourresultsshowtheimportanceofsettingthewarmupphase
correctlyandutilizingmultipleforksforbenchmarkaccuracy.With
a large number of benchmarks, expecting developers to pick the
łrightž values becomes unrealistic. Our dynamic reconfiguration
approach helps in this regard by deciding based on data and per
benchmarkwhen the results are accurateenough.
IterationTimeandForks. Thewarmupandmeasurementtimes
affectbenchmarkresultaccuracyandcontrolthefrequencywith
which stability checkpoints occur. JMH 1.21changed the iteration
timefrom1sto10s,andreducedthenumberofforksfromtentoESEC/FSE ’20, November8ś13,2020,VirtualEvent, USA ChristophLaaber,Stefan Würsten, Harald C.Gall, andPhilippLeitner
five[47,48].TheOpenJDKteamarguedthat1sistooshortforlarge
workloads[ 48].Weperformedanadditionalanalysiswhetherresult
accuracy changes when switching from 10s to 1s but did not ob-
serve differences in most cases. Hence, we decided for 1s iterations
to give the dynamic reconfiguration approach more checkpoints
toassessabenchmark’sstability.Whereas10sisasafechoicefor
staticconfigurations,webelievethat1sprovidesmoreflexibility
andworksbetterwithdynamicreconfiguration.Ourresultssup-
port reducing to five forks, which indicates that most fork-to-fork
variability iscaptured.
Choosing Stability Criteria Parameters. Choosing optimalmeta-
parameters for the stability criteria can affect the effectiveness
and efficiency of the overall approach. Dynamic reconfiguration
supportsthesliding-windowsize sW,thethreshold twhenasta-
bility criterion value (CV, RCIW, or KLD) is considered stable, and
stability-criterion-dependent parameters (see Section 5.3). We base
our parameters on common statistical practice and previous re-
search[18,20](seeSection 5.3).Onlytheslidingwindowsize sW
is manually set by us. Our empirical evaluation shows that the
employedparametersworkwellacrossallstudysubjects.However,
futureresearchshouldexplorethemeta-parameterspacethrough
experimentation.It isimportant toemphasize thatchoosing these
meta-parametersisanofflineactivity,whichisdoneonceandbe-
fore executing the benchmarks; hence, the cost for choosing these
parametersisnot part ofthe overheadestimationsinRQ2.
UnreachableStabilityCriteria. Althoughthestabilitycriteriais
frequently not met for warmup iterations or forks of individual
benchmarks,atleastwhenusingKLDandRCIW,theoverallrun-
time of the full benchmarksuites is considerably reduced (see Sec-
tion5.4). Dynamic reconfiguration uses upper bounds for warmup
iterations ( wimax) and forks ( fmax); therefore, it doesnot exceed
theruntimeofstandardJMHwithstaticconfiguration.Incaseof
anunreachablestabilitycriteria,ourJMHimplementationwarns
thedeveloper,whocanthenadjustthisbenchmark’supperbounds
to obtain better results. Our approach could also automatically lift
thecapsifthedesiredresultqualityisnotreached,whichshould
be exploredbyfuture research.
7 THREATS TO VALIDITY
Construct Validity. Our pre-study (see Section 3) relies on infor-
mation extracted from source code, i.e., configurations based on
JMH annotations.Wedonotconsider overwrittenconfigurations
through CLI arguments, which might be present in build scripts
or documentation in the repositories. Reported runtimes do not
consider fixture (setup and teardown) times, JVM startup, and time
spent in the benchmark harness of JMH; and they assume iteration
timesareasconfigured,whileinrealitytheyareminimumtimes.
Therefore, reported times might slightly underestimate the real
executiontimes.
The results and implications from RQ 1 are based on the notion
of benchmark result similarity. We assess this through statistical
A/A tests (based on bootstrap confidence intervals for the ratio
of means) and mean performance changerate, similar to previous
work[10,33].Othertestsforthesimilarityofbenchmarkresults,such as non-parametric hypothesis tests and effect sizes [ 12,33],
mightleadto differentoutcomes.
We base the time savings from RQ 2 on overhead calculations
fromasingleprojectandassumethisoverheadisconstantforall
stoppage points and benchmarks. There is hardly any reason to
believethatoverheadschangebetweenstudysubjects,benchmarks,
and stoppage points, because the number ofdatapoints usedfor
stoppage criteria computation are similar. This is due to how JMH
and our approach work (see Sections 2and4), and how our experi-
ment is designed (see Section 5): (1) the measurement time mtis
fixed,irrespectiveofthebenchmarkworkload;(2)thenumberof
iterations miandforks fisfixed;(3)benchmarkfixtures,i.e.,setup
and teardown, are constant and of negligible duration compared to
themeasurementduration;and(4)thestoppagecriteriacalculation
usesasliding-windowapproach( sW)and,therefore,thenumber
ofiterations usedfor the calculationisconstant.
Further, we perform post-hoc analysis on a single benchmark
execution data set for all stoppage criteria. That is,we execute the
benchmarksuiteswithfiveforksand 100measurementiterationsà
1s and then compute the stoppage points. Computing the stoppage
pointswhileexecutingtestsuitesmightleadtoslightlydifferent
results.
Finally,weuseasliding-windowapproachfordeterminingthe
endofthewarmupphasewithawindowsize( sW)offive.Different
window sizes might impose a larger runtime overhead and change
the stoppagepointoutcomes.
Internal Validity. Internal validity is mostly concerned with our
performance measurement methodology and the employed thresh-
olds. We follow measurement best practice [ 18] and run experi-
mentsonabare-metalmachine[ 49]toreducemeasurementbias[ 11,
15,37,40].Wedidnotexplicitlyturnoffsoftwareandhardwareopti-
mizations,whichmightaffectbenchmarkvariabilityand,therefore,
our results.
Regardingthethresholds,westartedfrompreviousworks[ 18,
20]andadaptedthemtofitthecontextofmicrobenchmarks.Aswe
usedthesamethresholdsforallbenchmarksandprojects,weare
confidentthattheyaregenerallyapplicableforJavamicrobench-
marksexecutedonasimilar machine to ours.
Further, the times reported in Section 3rely on the JMH version
ofabenchmark;weappliedsimpleheuristicstoextracttheversion,
which might not be fully accurate in case of, for instance, multi-
moduleprojectsordynamic JMHversiondeclarations.
ExternalValidity. Generalizabilitymightbeaffectedwithrespect
tothestudiedprojects.WeonlyfocusonOSSprojectsfromGithub,
and it is unclear whether our findings are equally valid in the
context of industrial software or projects hosted on other plat-
forms. Especially, the ten selected projectsfor our empirical evalu-
ation (see Section 5) might not be a representative sample for all
JMH projects. Due to the long benchmark suite execution times,
more projects would not have been feasible to study. We aimed for
adiversesetofprojects,spanningmultipledomains(seeTable 1),
covering ~ 8% of the benchmarks from the pre-study (see Section 3).
The effectiveness and efficiency results of dynamic reconfigu-
rationdependsontheenvironmentusedforexecutingthebench-
marks.Ourexperimentalevaluationfavorsinternalvalidityover
external validity by using a controlled, bare-metal environment.DynamicallyReconfiguringSoftware Microbenchmarks ESEC/FSE ’20, November8ś13,2020,VirtualEvent, USA
Therefore,wecanbemoreconfidentthatthereportedcomparisons
betweenstudysubjectsandstoppagecriteriaareindeedcorrectand
notduetouncontrollablefactorspresentinvirtualizedandclouden-
vironments. Executing benchmarks with dynamic reconfiguration
insuch otherenvironmentsmightleadto differentresults.
Moreover, our focus has been on Java projects that use JMH
as their benchmarking framework. Although the concepts from
Section4also translate to other frameworks and languages, the
exact results might be different. We opted for Java/JMH because
(1) it is a dynamically compiled language where warmup phases
andmultipleforksareessential,(2)JMHbenchmarksuitesarelong
running[ 32]andcanbenefitgreatlyfromdynamicreconfiguration,
and (3) JMH is a mature framework with many features offering
greatopportunitiesfor our approach.
Finally, switching to different Java virtual machines, such as
EclipseOpenJ9orGraal,might change the results duetodifferent
performance characteristics.
8 RELATED WORK
Performance testing is a form of measurement-based performance
engineering[ 52],whichcomesintwomainflavors:system-level
tests and method/statement-level tests. Historically, research fo-
cussedonsystem-leveltests[ 26,38,51],suchasloadandstresstest-
ing,withmorerecentadvancestargetingindustrialapplicabilityand
practice [ 17,41]. The otherflavor,i.e.,software microbenchmarks
and performance unit tests, has only recently gained popularity in
research. Studies on OSS projects [ 36,49] found that adoption lags
behindtheirfunctionalcounter-parts,i.e., unittests. One problem
isthathandlingperformancetestsiscomplexandrequiresin-depth
knowledgefromdevelopers.Toreducethisfriction,Dingetal .[16]
studied utilizing unit tests for assessing performance properties.
Bulejetal .[9]proposedaframeworkthatletsdevelopersspecify
performance assertions and handles rigorous statistical evaluation.
Horký et al . [23]compose performance unit test outcomes into
codedocumentationtoraiseperformanceawareness,andDama-
scenoCostaetal . [12]uncoverbadpracticesinmicrobenchmark
code through static analyses. Generating tests removes the need
towritetestsbyhand:AutoJMHhelpsavoidingpitfallsrootedin
compileroptimization[ 44],Pradeletal .[42]generateperformance
regression tests for concurrent classes, and PerfSyn synthesizes
inputsthroughmutationthatexposeworst-caseperformancebe-
haviour[50].Ourworkisorthogonaltotheaforementionedworks:
itdynamicallyadaptssoftware microbenchmarkconfigurations to
stop theirexecutiononcetheirresult isstable.
Longexecutiontimes[ 17,24,32]anduncertainresults[ 33,37]
are well-known to complicate the usage of performance tests in
general, including software microbenchmarks. There are a few
approachesthatreducethetimespentinperformancetestingac-
tivitieswithoutconsideringresultquality:(1)predictingcommits
that are likely toimpact performance [ 24,45], (2) prioritizing [ 39]
and(3)selecting[ 3,14]thetestsinasuitethataremorelikelyto
expose performance changes. Our approach pursues the same goal
of reducing benchmarking time, but with a focus on running all
benchmarks(similartoprioritization)aslongasnecessarywhile
maintainingthe same result quality.Resultqualityisimpairedbynotrunningenoughmeasurements
as well as measurement bias, which requires careful experiment
planningandexecution[ 7,11,15,18,19,28,40].Tomitigatemea-
surement bias, Georges et al .[18]outlined a rigorous methodology
how to asses performance of Java programs, which we base our
measurementtechniqueon.Usingthecorrectstatisticaltechniques
to assess performance is paramount, with estimated confidence in-
tervals using bootstrap being the state-of-the-art [ 8,9,27,33]. One
ofourstoppingcriteriaisbasedonandourresultqualityevaluation
usesconfidenceintervalswithbootstrap.Todecidehowmanymea-
surementsareenough,approachesusingstatisticaltechniqueshave
beenproposed,employingCV[ 18,37],confidenceintervals[ 25,37],
andtheKullback-Leiblerdivergence(KLD)[ 20].Withthese,perfor-
manceexperimentssuchasbenchmarkexecutionsrununtiltheir
resultsareaccurate/stableenoughandthenabortexecution,ideally
reducingexecutiontime.Ourstoppagecriteriausethesethreetech-
niquesandapplytheminthecontextofsoftwaremicrobenchmarks
after the warmupphaseandafter every fork.
Closest to our approach are the ones by Maricq et al . [37]and
Heetal.[20].Maricqetal .[37]estimatethenumberoftrialsand
iterations using a bootstrap technique. While they perform this
estimationbeforeexecutingbenchmarks,weevaluateresultquality
during execution. He et al . [20]stop system-level performance
tests executed in cloud environments, once they reach a certain
stabilitycriteria.Differentfromthebenchmarksusedintheirstudy,
microbenchmarks are much shorter, with runtimes in the order
of seconds instead of multiple hours. Our work builds on top of
their statistics-based approach using KLD for system benchmarks,
adapts it for microbenchmarks and extends it to other stoppage
criteria.
9 CONCLUSIONS
This paper introduced a dynamic reconfiguration approach for
softwaremicrobenchmarks,whichreducesbenchmarkexecution
time andmaintains the same result quality.
Inapre-studybasedonreal-worldconfigurationsof 13,387mi-
crobenchmarks comingfrom 753projects,we find thatdevelopers
make extensive use of custom configurations to considerably re-
duce runtimes for 34% of the benchmarks. Still, about 15% of the
projectshave benchmarksuite runtimesof more than3hours.
Our dynamic reconfiguration approach implements data-driven
decisions to stop microbenchmark executions, assisting developers
withtheintricatetaskofcorrectlyconfiguringmicrobenchmarks.
Withoverheadsbetween 1%and11%,itachievesatimereduction
of48.4%to86.0%,withbetween 78.8%and87.6%ofthemicrobench-
markspreservingtheirresult quality.
These results show that dynamic reconfiguration is highly effec-
tiveandefficient,andweenvisionittoenableregularperformance
microbenchmarkingactivities, such as part of CI.
ACKNOWLEDGMENTS
Theresearchleadingtotheseresultshasreceivedfundingfromthe
SwissNationalScienceFoundation(SNSF)underprojectnumber
165546andtheSwedishResearchCouncil(VR)undergrantnumber
2018-04127 .ESEC/FSE ’20, November8ś13,2020,VirtualEvent, USA ChristophLaaber,Stefan Würsten, Harald C.Gall, andPhilippLeitner
REFERENCES
[1]HammamM.AlGhamdi, Cor-PaulBezemer,Weiyi Shang, Ahmed E.Hassan, and
ParminderFlora.2020.Towardsreducingthetimeneededforloadtesting. Journal
ofSoftware:Evolutionand Process (July2020). https://doi.org/10.1002/smr.2276
[2]HammamM.AlGhamdi,MarkD.Syer,WeiyiShang,andAhmedE.Hassan.2016.
AnAutomatedApproachforRecommendingWhentoStopPerformanceTests.In
2016IEEEInternationalConferenceonSoftwareMaintenanceandEvolution(ICSME
2016). 279ś289. https://doi.org/10.1109/ICSME.2016.46
[3]Deema Alshoaibi, Kevin Hannigan, Hiten Gupta, and Mohamed Wiem Mkaouer.
2019. PRICE:DetectionofPerformanceRegressionIntroducingCodeChanges
UsingStaticandDynamicMetrics.In Proceedingsofthe11thInternationalSym-
posium on Search Based Software Engineering (Tallinn, Estonia) (SSBSE 2019) .
SpringerNature, 75ś88. https://doi.org/10.1007/978-3-030-27455-9_6
[4]Eytan Bakshy and Eitan Frachtenberg. 2015. Design and Analysis of Bench-
markingExperimentsforDistributedInternetServices.In Proceedingsofthe24th
International Conference on World Wide Web (Florence, Italy) (WWW 2015) . Inter-
nationalWorldWideWebConferencesSteeringCommittee,RepublicandCanton
of Geneva, Switzerland,108ś118. https://doi.org/10.1145/2736277.2741082
[5]Sebastian Baltes and Paul Ralph. 2020. Sampling in Software Engineering
Research: A Critical Review and Guidelines. CoRRabs/2002.07764 (2020).
arXiv:2002.07764 https://arxiv.org/abs/2002.07764
[6]Cor-PaulBezemer,SimonEismann,VincenzoFerme,JohannesGrohmann,Robert
Heinrich,PooyanJamshidi,WeiyiShang,AndrévanHoorn,MonicaVillavicencio,
Jürgen Walter, and Felix Willnecker. 2019. How is Performance Addressed
in DevOps?. In Proceedings of the 2019 ACM/SPEC International Conference on
Performance Engineering (Mumbai, India) (ICPE 2019) . ACM, New York, NY, USA,
45ś50.https://doi.org/10.1145/3297663.3309672
[7]Stephen M. Blackburn, Amer Diwan, Matthias Hauswirth, Peter F. Sweeney,
José Nelson Amaral, Tim Brecht, Lubomír Bulej, Cliff Click, Lieven Eeckhout,
SebastianFischmeister,andetal.2016. TheTruth,TheWholeTruth,andNothing
But the Truth: A Pragmatic Guide to Assessing Empirical Evaluations. ACM
Transactions on Programming Languages and Systems 38, 4, Article 15 (Oct. 2016),
20pages. https://doi.org/10.1145/2983574
[8]LubomírBulej,,Vojt ˘echHorký,PetrTůma,FrançoisFarquet,andAleksandar
Prokopec. 2020. Duet Benchmarking: Improving Measurement Accuracy in
the Cloud. In Proceedings of the 2020 ACM/SPEC International Conference on
Performance Engineering (ICPE 2020) . ACM, New York, NY, USA. https://doi.org/
10.1145/3358960.3379132
[9]LubomírBulej,TomášBureš,Vojt ˘echHorký,JaroslavKotrč,LukášMarek,Tomáš
Trojánek, and Petr Tůma. 2017. Unit testing performance with Stochastic Perfor-
mance Logic. AutomatedSoftware Engineering 24, 1(01March2017), 139ś187.
https://doi.org/10.1007/s10515-015-0188-0
[10]LubomírBulej,Vojt ˘echHorký,andPetrTůma.2019. InitialExperimentswith
DuetBenchmarking:PerformanceTestingInterferenceintheCloud.In 2019IEEE
27th International Symposium on Modeling, Analysis, and Simulation of Computer
andTelecommunicationSystems(MASCOTS) .249ś255. https://doi.org/10.1109/
MASCOTS.2019.00035
[11]Charlie Curtsinger and Emery D. Berger. 2013. STABILIZER: Statistically Sound
PerformanceEvaluation.In ProceedingsoftheEighteenthInternationalConference
on Architectural Support for Programming Languages and Operating Systems
(Houston,Texas,USA) (ASPLOS2013) .ACM,NewYork,NY,USA,219ś228. https:
//doi.org/10.1145/2451116.2451141
[12]Diego Elias Damasceno Costa, Cor-Paul Bezemer, Philipp Leitner, and Artur
Andrzejak. 2019. What’s Wrong With My Benchmark Results? Studying Bad
Practices in JMH Benchmarks. IEEE Transactions on Software Engineering (2019),
1ś1.https://doi.org/10.1109/TSE.2019.2925345
[13]AnthonyC. DavisonandDHinkley.1997. BootstrapMethodsandTheir Appli-
cation.J.Amer. Statist. Assoc. 94(Jan. 1997).
[14]Augusto Born de Oliveira, Sebastian Fischmeister, Amer Diwan, Matthias
Hauswirth, and Peter F. Sweeney. 2017. Perphecy: Performance Regression
Test Selection Made Simple but Effective. In 2017 IEEE International Confer-
ence on Software Testing, Verification and Validation (ICST) . 103ś113. https:
//doi.org/10.1109/ICST.2017.17
[15]AugustoBorndeOliveira,Jean-ChristophePetkovich,ThomasReidemeister,and
Sebastian Fischmeister. 2013. DataMill: Rigorous Performance Evaluation Made
Easy.InProceedingsofthe4thACM/SPECInternationalConferenceonPerformance
Engineering (Prague, Czech Republic) (ICPE 2013) . ACM, New York, NY, USA,
137ś148. https://doi.org/10.1145/2479871.2479892
[16]ZishuoDing,JinfuChen,andWeiyiShang.2020. TowardstheUseoftheReadily
AvailableTestsfromtheReleasePipelineasPerformanceTests.AreWeThere
Yet?. InProceedings of the 42nd International Conference on Software Engineering
(Seoul, SouthKorea) (ICSE2020) . ACM,NewYork, NY, USA,12.
[17]KingChunFoo,ZhenMing(Jack)Jiang,BramAdams,AhmedE.Hassan,Ying
Zou, and Parminder Flora. 2015. An Industrial Case Study on the Automated
Detection of Performance Regressionsin HeterogeneousEnvironments.In Pro-
ceedings of the 37th International Conference on Software Engineering - Vol-
ume 2(Florence, Italy) (ICSE 2015) . IEEE Press, Piscataway, NJ, USA, 159ś168.https://doi.org/10.1109/icse.2015.144
[18]Andy Georges,DriesBuytaert,andLievenEeckhout. 2007. StatisticallyRigorous
Java Performance Evaluation.In Proceedings of the22Nd Annual ACM SIGPLAN
Conference on Object-oriented Programming Systems and Applications (Montreal,
Quebec, Canada) (OOPSLA 2007) . ACM, New York, NY, USA, 57ś76. https:
//doi.org/10.1145/1297027.1297033
[19]Joseph Yossi Gil, Keren Lenz, and Yuval Shimron. 2011. A Microbenchmark Case
StudyandLessonsLearned.In ProceedingsoftheCompilationoftheCo-located
WorkshopsonDSM’11,TMC’11,AGERE!2011,AOOPES’11,NEAT’11,&VMIL’11
(Portland,Oregon,USA) (SPLASH2011Workshops) .ACM,NewYork,NY,USA,
297ś308. https://doi.org/10.1145/2095050.2095100
[20]SenHe,GlennaManns,JohnSaunders,WeiWang,LoriPollock,andMaryLou
Soffa. 2019. A Statistics-based Performance Testing Methodology for Cloud
Applications. In Proceedings of the 2019 27th ACM Joint Meeting on European
SoftwareEngineeringConferenceandSymposiumontheFoundationsofSoftware
Engineering (Tallinn, Estonia) (ESEC/FSE 2019) . ACM,New York, NY, USA, 188ś
199.https://doi.org/10.1145/3338906.3338912
[21]Tim C. Hesterberg. 2015. What Teachers Should Know About the Bootstrap:
ResamplingintheUndergraduateStatisticsCurriculum. TheAmericanStatistician
69,4 (2015), 371ś386. https://doi.org/10.1080/00031305.2015.1089789
[22]MichaelHilton,TimothyTunnell,KaiHuang,DarkoMarinov,andDannyDig.
2016. Usage, Costs, and Benefits of Continuous Integration in Open-Source
Projects.In Proceedingsofthe31stIEEE/ACMInternationalConferenceonAuto-
mated Software Engineering (Singapore, Singapore) (ASE 2016) . ACM, New York,
NY, USA,426ś437. https://doi.org/10.1145/2970276.2970358
[23]Vojt˘echHorký,PeterLibič,LukášMarek,AntonínSteinhauser,andPetrTůma.
2015. Utilizing Performance Unit Tests To Increase Performance Awareness.
InProceedings of the 6th ACM/SPEC International Conference on Performance
Engineering (Austin,Texas,USA) (ICPE2015) .ACM,NewYork,NY,USA,289ś300.
https://doi.org/10.1145/2668930.2688051
[24]Peng Huang, Xiao Ma, Dongcai Shen, and Yuanyuan Zhou. 2014. Perfor-
mance Regression Testing Target Prioritization via Performance Risk Analy-
sis. InProceedings of the 36th International Conference on Software Engineer-
ing(Hyderabad, India) (ICSE 2014) . ACM, New York, NY, USA, 60ś71. https:
//doi.org/10.1145/2568225.2568232
[25] Raj Jain. 1991. The Art ofComputer SystemsPerformance Analysis . Wiley.
[26]Zhen Ming Jiang and Ahmed E. Hassan. 2015. A Survey on Load Testing of
Large-Scale SoftwareSystems. IEEE Transactions on SoftwareEngineering 41,11
(Nov. 2015),1091ś1118. https://doi.org/10.1109/TSE.2015.2445340
[27]Tomas Kalibera and Richard Jones. 2012. Quantifying Performance Changes with
Effect Size Confidence Intervals . Technical Report 4ś12. University of Kent. 55
pages.http://www.cs.kent.ac.uk/pubs/2012/3233
[28]Tomas Kalibera and Richard Jones. 2013. Rigorous Benchmarking in Reasonable
Time.InProceedingsofthe2013InternationalSymposiumonMemoryManagement
(Seattle,Washington,USA) (ISMM2013) .ACM,NewYork,NY,USA,63ś74. https:
//doi.org/10.1145/2464157.2464160
[29]Solomon Kullback and Richard A. Leibler. 1951. On Information and Sufficiency.
AnnalsofMathematicalStatistics 22,1(March1951),79ś86. https://doi.org/10.
1214/aoms/1177729694
[30]ChristophLaaber. 2020. bencher - JMH Benchmark Analysis and Prioritization.
https://github.com/chrstphlbr/bencher
[31]Christoph Laaber. 2020. pa - Performance (Change) Analysis using Bootstrap.
https://github.com/chrstphlbr/pa
[32]Christoph Laaber and Philipp Leitner. 2018. An Evaluation of Open-Source
SoftwareMicrobenchmarkSuitesforContinuousPerformanceAssessment.In
Proceedingsofthe15thInternationalConferenceonMiningSoftwareRepositories
(Gothenburg,Sweden) (MSR2018) .ACM,NewYork,NY,USA,119ś130. https:
//doi.org/10.1145/3196398.3196407
[33]ChristophLaaber,JoelScheuner,andPhilippLeitner.2019. SoftwareMicrobench-
marking in the Cloud. How Bad is it Really? Empirical Software Engineering (17
April2019),40. https://doi.org/10.1007/s10664-019-09681-1
[34]ChristophLaaber,StefanWürsten,HaraldC.Gall,andPhilippLeitner.2020. JMH
with Dynamic Reconfiguration. https://github.com/sealuzh/jmh
[35]Christoph Laaber, Stefan Würsten, Harald C. Gall, and Philipp Leitner. 2020.
ReplicationPackage"DynamicallyReconfiguringSoftwareMicrobenchmarks:
ReducingExecutionTimeWithoutSacrificingResultQuality". https://doi.org/
10.6084/m9.figshare.11944875
[36]Philipp Leitner and Cor-Paul Bezemer. 2017. An Exploratory Study of the
State of Practice of Performance Testing in Java-Based Open Source Projects.
InProceedingsofthe8thACM/SPEConInternationalConferenceonPerformance
Engineering (L’Aquila, Italy) (ICPE 2017) . ACM, New York, NY, USA, 373ś384.
https://doi.org/10.1145/3030207.3030213
[37]Aleksander Maricq, Dmitry Duplyakin, Ivo Jimenez, Carlos Maltzahn, Ryan
Stutsman,andRobertRicci.2018. TamingPerformanceVariability.In Proceedings
of the13th USENIX Conference on Operating Systems Design andImplementation
(Carlsbad, CA, USA) (OSDI 2018) . USENIX Association, USA, 409ś425. https:
//www.usenix.org/conference/osdi18/presentation/maricqDynamicallyReconfiguringSoftware Microbenchmarks ESEC/FSE ’20, November8ś13,2020,VirtualEvent, USA
[38]Daniel A. Menascé.2002. Loadtestingof Websites. IEEEInternet Computing 6, 4
(July2002),70ś74. https://doi.org/10.1109/MIC.2002.1020328
[39]Shaikh Mostafa, Xiaoyin Wang, and Tao Xie. 2017. PerfRanker: Prioritization of
PerformanceRegressionTestsforCollection-IntensiveSoftware.In Proceedingsof
the 26th ACM SIGSOFT International Symposium on Software Testing and Analysis
(Santa Barbara, CA, USA) (ISSTA 2017) . ACM, New York, NY, USA, 23ś34. https:
//doi.org/10.1145/3092703.3092725
[40]Todd Mytkowicz, Amer Diwan, Matthias Hauswirth, and Peter F. Sweeney. 2009.
Producing Wrong Data Without Doing Anything Obviously Wrong!. In Proceed-
ingsofthe14thInternationalConferenceonArchitecturalSupportforProgramming
Languagesand Operating Systems (Washington, DC, USA) (ASPLOS XIV) . ACM,
NewYork, NY, USA,265ś276. https://doi.org/10.1145/1508244.1508275
[41]ThanhH.D.Nguyen,MeiyappanNagappan,AhmedE.Hassan,MohamedNasser,
andParminderFlora.2014. AnIndustrialCaseStudyofAutomaticallyIdentifying
PerformanceRegression-Causes. In Proceedingsofthe 11thWorkingConference
on Mining Software Repositories (Hyderabad, India) (MSR 2014) . ACM, New York,
NY, USA,232ś241. https://doi.org/10.1145/2597073.2597092
[42]Michael Pradel, Markus Huggler, and Thomas R. Gross. 2014. Performance
RegressionTestingofConcurrentClasses.In Proceedingsofthe2014International
SymposiumonSoftwareTestingandAnalysis (SanJose,CA,USA) (ISSTA2014) .
ACM,NewYork, NY, USA,13ś25. https://doi.org/10.1145/2610384.2610393
[43]Shiquan Ren, Hong Lai, Wenjing Tong, Mostafa Aminzadeh, Xuezhang Hou,
and Shenghan Lai. 2010. Nonparametric bootstrapping for hierarchical data.
Journal of Applied Statistics 37, 9 (2010), 1487ś1498. https://doi.org/10.1080/
02664760903046102
[44]Marcelino Rodriguez-Cancio, Benoit Combemale, and Benoit Baudry. 2016. Au-
tomatic Microbenchmark Generation to Prevent Dead Code Elimination and
Constant Folding. In Proceedings of the 31st IEEE/ACM International Confer-
ence on Automated Software Engineering (Singapore, Singapore) (ASE 2016) .
Association for Computing Machinery, New York, NY, USA, 132ś143. https:
//doi.org/10.1145/2970276.2970346[45]JuanPabloSandovalAlcocer,AlexandreBergel,andMarcoTulioValente.2016.
Learning from Source Code History to Identify Performance Failures. In Pro-
ceedingsofthe7thACM/SPEConInternationalConferenceonPerformanceEngi-
neering(Delft,TheNetherlands) (ICPE2016) .ACM,NewYork,NY,USA,37ś48.
https://doi.org/10.1145/2851553.2851571
[46]MarijaSelakovicandMichaelPradel.2016. PerformanceIssuesandOptimiza-
tionsinJavaScript:AnEmpiricalStudy.In Proceedingsofthe38thInternational
Conference on Software Engineering (Austin, Texas) (ICSE 2016) . ACM, New York,
NY, USA,61ś72. https://doi.org/10.1145/2884781.2884829
[47]AlekseyShipilev.2018. Reconsiderdefaultsforforkcount .https://bugs.openjdk.
java.net/browse/CODETOOLS-7902170
[48]AlekseyShipilev.2018. Reconsiderdefaultsforwarmupandmeasurementiteration
counts, durations .https://bugs.openjdk.java.net/browse/CODETOOLS-7902165
[49]Petr Stefan, Vojt ˘ech Horký, Lubomír Bulej, and Petr Tůma. 2017. Unit Test-
ingPerformanceinJavaProjects:AreWeThereYet?.In Proceedingsofthe8th
ACM/SPEC on International Conference on Performance Engineering (L’Aquila,
Italy)(ICPE2017) .ACM, New York,NY,USA, 401ś412. https://doi.org/10.1145/
3030207.3030226
[50]Luca Della Toffola, Michael Pradel, and Thomas R. Gross. 2018. Synthesizing
ProgramsThatExposePerformanceBottlenecks.In Proceedingsofthe2018In-
ternationalSymposiumonCodeGenerationandOptimization (Vienna,Austria)
(CGO2018) .AssociationforComputingMachinery,NewYork,NY,USA,314ś326.
https://doi.org/10.1145/3168830
[51]ElaineJ.WeyukerandFilipposI.Vokolos.2000. ExperiencewithPerformance
Testing of Software Systems: Issues, an Approach, and Case Study. IEEE
Transactions on Software Engineering 26, 12 (Dec. 2000), 1147ś1156. https:
//doi.org/10.1109/32.888628
[52]Murray Woodside, Greg Franks, and Dorina C. Petriu. 2007. The Future of
Software Performance Engineering. In 2007 Future of Software Engineering (FOSE
2007). IEEE Computer Society, Washington, DC,USA, 171ś187. https://doi.org/
10.1109/FOSE.2007.32