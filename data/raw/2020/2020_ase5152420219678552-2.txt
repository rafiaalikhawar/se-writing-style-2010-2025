Automating User Notice Generation for Smart
Contract Functions
Xing Hu‚àó/bardbl, Zhipeng Gao‚Ä†, Xin Xia‚Ä†¬∂, David Lo‚Ä°and Xiaohu Yang¬ß
‚àóSchool of Software Technology, Zhejiang University, Ningbo, China
‚Ä†Faculty of Information Technology, Monash University, Melbourne, Australia
‚Ä°School of Information Systems, Singapore Management University, Singapore, davidlo@smu.edu.sg
¬ßCollege of Computer Science and Technology, Zhejiang University, Hangzhou, China
{xinghu,yangxh}@zju.edu.cn, {zhipeng.gao,Xin.Xia }@monash.edu, davidlo@smu.edu.sg
Abstract ‚ÄîSmart contracts have obtained much attention and
are crucial for automatic Ô¨Ånancial and business transactions. For
end-users who have never seen the source code, they can readthe user notice shown in end-user client to understand whata transaction does of a smart contract function. However, dueto time constraints or lack of motivation, user notice is oftenmissing during the development of smart contracts. For end-users who lack the information of the user notices, there is noeasy way for them to check the code semantics of the smartcontracts. Thus, in this paper, we propose a new approach
S
MART DOCto generate user notice for smart contract functions
automatically. Our tool can help end-users better understandthe smart contract and aware of the Ô¨Ånancial risks, improvingthe users‚Äô conÔ¨Ådence on the reliability of the smart contracts.
S
MART DOCexploits the Transformer to learn the representation
of source code and generates natural language descriptionsfrom the learned representation. We also integrate the Pointermechanism to copy words from the input source code insteadof generating words during the prediction process. We extract7,878 /angbracketleftfunction, notice /angbracketrightpairs from 54,739 smart contracts
written in Solidity. Due to the limited amount of collectedsmart contract functions (i.e., 7,878 functions), we exploit atransfer learning technique to utilize the learned knowledge toimprove the performance of S
MART DOC. The learned knowledge
obtained by the pre-training on a corpus of Java code, thathas similar characteristics as Solidity code. The experimentalresults show that our approach can effectively generate usernotice given the source code and signiÔ¨Åcantly outperform thestate-of-the-art approaches. To investigate human perspectives onour generated user notice, we also conduct a human evaluationand ask participants to score user notice generated by differentapproaches. Results show that S
MART DOCoutperforms baselines
from three aspects, naturalness, informativeness, and similarity.
Index T erms‚ÄîSmart Contract, User Notice Generation, Deep
Learning
I. I NTRODUCTION
Recent years have seen an emerging interest in cryptocur-
rencies (e.g., Bitcoin and Ethereum) on distributed ledgers
(a.k.a., Blockchains [1]) from both industry and academia.As one of the largest cryptocurrency platform [2], Ethereumhas become a widely used platform to enable Ô¨Ånancial andbusiness transactions. As the core of Ethereum [3], smartcontracts [4][5] are Turing-complete programs and executed
/bardblAlso with PengCheng Laboratory.¬∂Corresponding author.on the Ethereum Blockchain. After the deployment, the end-users can interact with a smart contract by sending transactionsto its functions. Each transaction consumes a certain amount of‚Äúgas‚Äù whose price is given in Ethereum cryptocurrency namedEther ($737.15 per unit of Ether as of Dec 2020 [6]). Dueto the high stakes of smart contracts and the potential riskof Ô¨Ånancial loss for users, it is necessary to assist end-usersbetter understand the functionality of smart contracts.
Two groups of people interact with smart contracts: de-
velopers and end-users. When developers implement a smartcontract, they need to translate Ô¨Ånancial operations (e.g.,transfer) into one or more contract transactions; then the end-users start the transaction which triggers the execution of afunction deÔ¨Åned within the smart contract. In this study, weargue that the end-users are often non-tech-savvy consumersof the contracts. To assist these users who cannot read thesource code, Solidity (one of the most popular programminglanguage for smart contracts) provides a mechanism that canprovide notices for end-users. An example of a smart contractfunction and how it is used by an end-user are illustratedin Figure 1 and Figure 2. Consider Alice is an end-user ofsmart contracts who knows nothing about programming; whenshe submits a transaction to the above function with a targetaddress of 0x83 ***Cc and ‚ÄúmintedAmount‚Äù of 100, then
the user notice will be rendered to Alice as: ‚Äú Create 100
tokens and send it to 0x83 ***Cc‚Äù. After reading
the user notice, Alice can better understand the contract andthus can better make informed decision, i.e., Reject or ConÔ¨Årmthe transaction.
Unfortunately, user notices are often lacking for a large
number of smart contracts. Even though the ofÔ¨Åcial guideline of Solidity recommends that the smart contracts shouldbe annotated with user notice for all public interfaces, thispractice is often neglected or ignored by developers duringsmart contract development. This will make the end-userscompletely clueless and uninformed, which may discouragethe participation of end-users and the usability of the smartcontract. Moreover, due to the unalterable feature of theblockchain system, unlike traditional software, user notice cannot be added once the smart contract is deployed. Therefore, itis desirable to have a tool that can automatically generate usernotices for smart contract developers whenever they forget to
52021 36th IEEE/ACM International Conference on Automated Software Engineering (ASE)
DOI 10.1109/ASE51524.2021.000122021 36th IEEE/ACM International Conference on Automated Software Engineering (ASE) | 978-1-6654-0337-5/21/$31.00 ¬©2021 IEEE | DOI: 10.1109/ASE51524.2021.9678552
978-1-6654-0337-5/21/$31.00  ¬©2021  IEEE
 	
	 	
	

 	 	 	"#$ 		 
	
	 %&	 
 %&	 


 !	 


 		 
 
Fig. 1. An example of a smart contract function
do so.
Existing approaches mainly focus on generating comments
for common programming languages, e.g., Java and Python.
These comments are provided to developers and help them tounderstand the source code. However, generating commentsespecially user-oriented comments (i.e., user notices) has notgained much attention yet. Making such a tool for smart con-tracts is a non-trivial task considering the following challenges:(i) Dynamic Expressions Mechanism. The user notice in
smart contracts supports dynamic expressions. Different fromthe general code comments of other programming languages(e.g., Java comments), the Solidity compiler produces theuser notice dynamically from the source code. The dynamicexpression mechanism requires that certain words in the usernotice should be identical with the corresponding tokens insource code. For the example shown in Figure 1, the word‚ÄúmintedAmount‚Äù and ‚Äútarget‚Äù are both copied from thesource code. This mechanism causes the user notice to beclosely related to the source code. Even though the existingdocumentation generation approaches have achieved a hugesuccess for general code comments generation (e.g., commentsfor Java method [7][8][9][10][11]), it is not clear whetherthey can be successfully applied to user notice generationfor smart contracts. How to copy variable names correctlyfrom the smart contracts is still challenging for these deeplearning models. (ii) Data Hungry. Compared with other
popular programming languages (i.e., Java), it is more difÔ¨Åcultto collect large-scale datasets for smart contracts. Even thoughthe Ethereum blockchain has accumulated a great numberof smart contracts, data hungry problem still exists. Thatis, only a small proportion of smart contracts have usernotices. According to our preliminary study, only 11,409 outof 54,739 smart contracts contain user notices; if we look atthe functions, the proportion is even smaller. How to utilizethe limited labeled data for generating accurate user notice ischallenging for this work.
In this paper, we propose a new approach named S
MART -
DOC to address the aforementioned challenges. We aim to
understand functions in smart contracts and automaticallygenerate user notices (i.e., @notice) for functions in a smart
contracts. The main idea of our approach is two-folds: (1)while generating user notice, S
MART DOCcan predict a word
or copy a token from source code. It exploits Transformer [12]equipped with Pointer mechanism to predict user notice. Exist-ing code comment generation approaches usually use Recur-rent Neural Network (e.g., LSTM and GRU) to generate codecomments. However, these techniques are difÔ¨Åcult to capturelong-range dependencies between code tokens. In this paper,we exploit the Transformer architecture that can generate user
#M 
!&$&M &$&!M

&%M &M
'&!M 0LQW7RNHQKM+5;MC3/@/E:M /DG<L7
 I0M 
	M
&UHDWHWRNHQVDQGVHQGLWWR[FHHH$HH(E%&%D'IG(&&F
0LQW7RNHQ 
(M&MM*&%M
[HIDEEGFEIEFEIIHHG
FFIEGEGEEI
7DUJHW [FHHH$HH(E%&
%D'IG(&&F
#M 
!&$&M &$&!M

&%M &M
'&!& 1RW)RXQGKM+5;MC3/@/E:M /DG<L7
 I0M 
	M
(M&MM*&%M
[HIDEEGFEIEFEIIHHG
FFIEGEGEEI

RXQG
EEIIEEEIEII
Fig. 2. An example of an end-user submitting a transaction with (on the
right) and without (on the left) user notice
notices to reinforce the capability of capturing the long-range
dependencies between code tokens. Considering the dynamicexpression mechanism, many words in the user notice canbe copied from the smart contract functions. Therefore, weintegrate the Pointer generator in our approach to overcomethe Ô¨Årst challenge. The Pointer mechanism can copy a wordby pointing tokens in source code. (2) In order to alleviate thelimitation of minimal labeled data, we propose to use transferlearning [13] that transfers the knowledge of general commentgeneration for Java methods into user notice generation forsmart contract functions. Solidity and Java languages aresomewhat similar in that both are object-oriented and high-level programming languages. Models that have learnt how toconvert Java methods into comments can be a good start tothe user notice generation.
To evaluate our proposed model, we extract 7,878
/angbracketleftfunction,notice/angbracketright pairs from 54,739 veriÔ¨Åed smart con-
tract. The automatic evaluation results show that S
MART DOC
achieves the best performance when compared with baselinesincluding attendgru [11], ast-attendgru [11], and Re
2Com [10]
regarding the he BLEU score and ROUGE-L score. To explorethe practitioners‚Äô perspective on the generated notice, we alsoconduct a human evaluation. Each practitioner is asked toevaluate user notice generated by various approaches fromthree aspects, the similarity of generated notice and human-
written notice, naturalness (grammatical correctness and Ô¨Çu-
ency) of the generated notices, and their informativeness (the
amount of content carried over from the input code to thegenerated notices, ignoring Ô¨Çuency of the text). Experimentsshow that our approach can achieve the best performance whencompared to the baseline techniques.
The main contributions of this paper are as follows:
‚Ä¢We are the Ô¨Årst to investigate characteristics of smartcontract user notices. Our study highlights a problemthat has been neglected in the literature but has practicalimplications.
‚Ä¢We propose a novel approach S MART DOC for smart
contract user notice generation, which aims to help end-users understand smart contracts when they are executedin Blockchain platforms.
‚Ä¢We integrate the Pointer mechanism into Transformer for
6better user notice prediction. The approach can generate
words or copy tokens from source code. We exploittransfer learning to alleviate the effect of minimal labeleddata on training a deep learning model. The experimentalresults show that our approach outperforms the state-of-the-art techniques.
‚Ä¢We build the Ô¨Årst dataset with respect to user notice whichcontains 7,878 /angbracketleftfunction,notice/angbracketright pairs. To the best of
our knowledge, this is the Ô¨Årst dataset of user notices forsmart contract functions.
This paper is organized as follows. In Section II, we provide
the preliminaries of smart contracts. Section III presents ourapproach for smart contract user notice generation. SectionIV evaluates our approach on actual contracts collected fromthe Ethereum blockchain. Section V and Section VI illustrateexperimental results and practitioners‚Äô perspectives on thegenerated user notice. Section VII discusses our proposedapproach. Section IX presents the related works. Section Xconcludes the paper.
II. P
RELIMINARIES
In this section, we present the data hungry issue related
to user notices. Then, we show the correlation between usernotice and transactions .
A. User notice hungry
Ethereum [14] has attracted increasing attention as
a blockchain platform and smart contracts deployed on
Ethereum have been applied to many business domains toenable efÔ¨Åcient and trustable transactions [15], [16]. Whendeveloping smart contracts, Solidity provides a special formof comments, named the Ethereum Natural Language Spec-iÔ¨Åcation Format (NatSpec), to document contracts and func-tions [17]. The @notice tag is the main NatSpec tag and
its audience is end-users. Considering that smart contract end-users are often non-tech-savvy consumers, the user notices canbridge the information gap between smart contract developersand end-users. By interacting with the user notices, the end-users can better assess the Ô¨Ånancial risks and make betterinformed decisions. However, according to our preliminarystudy, only a small proportion of smart contracts include usernotices. For example, among the 54,739 contracts that we havecollected, only 11,409 of them contain user notices; moreover,only 4% of the functions in smart contracts have user notices.
B. User notice & Transactions
In smart contracts, functions are the executable units of
code and can be called by end-users. Intuitively, user notice of
functions can help end-users understand the smart contracts,and thus improve the probability of smart contracts‚Äô trans-actions. To verify this conjecture, we collect the transactioninformation of smart contracts and investigate whether smartcontracts with user notice have more transactions. Figure 3presents the function distribution in smart contracts and thethe distribution of the average amount of transactions of smart(a) Function distribution in Smart
Contracts(b) The average amount of transac-tions of Smart Contracts with differ-ent amount of user notice
Fig. 3. Function Distribution and the Transaction Distribution of SmartContracts.
Contract Name: SMT
Transactions: 58,785
contract Token{
...
/// @notice send `_value` token to `_to` from `msg.sender`
/// ...
function transfer( address _to,uint256 _value) public returns (boolsuccess) ;
/// @notice send `_value` token to `_to` from `_from` on the condition it is approved by `_from`
/// ...function transferFrom( address _from,address _to,uint256 _value) public returns (boolsuccess);
...
}
Contract Name: LooksCoin
Transactions: 25
contract ERC20{
...
function transfer (address _to,uint256 _value) public returns (boolsuccess) ;
function transferFrom( address _from,address _to,uint256 _value) public returns (boolsuccess);
...
}
Fig. 4. Motivating Examples
contracts with different amount of user notice. From Figure
3(a), we can observe that almost all smart contracts haveless than 40 functions. Thus, we analyze the transactions ofthem and Ô¨Ånd that smart contracts with more user noticestend to have more transactions (shown in Figure 3(b)). Thisis reasonable because detailed user notices can make end-users well aware of the Ô¨Ånancial risks and improve the users‚ÄôconÔ¨Ådence in the reliability of the smart contracts, thereforeend-users prefer to start a transaction through smart contractswith more high-quality user notices.
Figure 4 shows the source code of two smart contracts, i.e.,
SMT [18] and LooksCoin [19], in which SMT has 58,785transactions and LooksCoin has 25 transactions. Althoughthese two smart contracts implemented the same functions,such as transfer andtransferFrom in this example, the
SMT contract provided adequate user notices while LooksCoindid not make any notice for end-users. The detailed user noticecan help end-users better understand of their operations andthus make the smart contract more popular among end-users.
III. A
PPROACH
Figure 5 illustrates the overall framework of our approach
SMART DOC. It mainly consists of three phases: pre-training,
Ô¨Åne-tuning, and application. In the pre-training phase, we
exploit the Java dataset prepared by Hu et al. [9] to pre-trainthe model of S
MART DOC. Similar to Java, Solidity is also an
object-oriented programming language. The two programminglanguages share similar coding conventions and syntax. There-fore, the knowledge (such as variable naming conventions,sequential information among code tokens) learned from Javacan be reused into Solidity. To better exploit the existing
7




	


	





		
 	 	
	 



	


	








	
	
	
	
	





Fig. 5. Overview of our Approach
knowledge of source code, we transfer the pre-trained weights
of Java encoder into Solidity encoder.
In the Ô¨Åne-tuning phase, we further train the user notice
generation model on the corpus of annotated /angbracketleftfun,doc/angbracketrightpairs
extracted from smart contracts. The encoder is initialized bythe learned encoder from the pre-trained model. Except forthe source code encoder, parameters of other components aretrained from scratch. After training, we can get a trainedneural network. Then, given a new function of smart contract,corresponding user notice can be generated by the trainedmodel.
Figure 6 is an overview of the network architecture of our
proposed deep learning based model. The architecture of ourmodel follows the Transformer framework [12], [20], [21],which has been successfully adopted in machine translationtasks. The architecture mainly consists of three submodules:(1) Source code encoder. This module aims to representthe source code and exploits the multi-head self-attentionto learn the sequential information of the source code. (2)Notice generation decoder. This module aims to generatenotice through the self-attention layer and the encoder-decoderattention layer. The encoder-decoder attention layer helps thedecoder focus on appropriate places in the input sequence. (3)Pointer generator. The pointer generator [22] is used to copyvariables from source code.
We will elaborate on each component in this framework in
the following subsections.
A. Encoder
The encoder aims to learn representations for a smart
contract function X=x
1,x2,...,x m. Each token is embed-
ded into a vector (i.e., X=(x1,...,xm)) before fed into
the encoder. To help S MART DOCfocuses on the important
information of the function X, the encoder adopts a multi-
head self-attention layer to capture important parts of the input.
Then the output of the multi-head self-attention layer is fedinto a feed-forward neural network.
The multi-head self-attention layer depicted in Figure 6
exploits scaled dot-product attention to calculate attentionweights. Given an input vector I
i‚ààRd(in this paper, Ii
represents the embedding of each token), the Ô¨Årst step is tocreate three vectors, i.e., a query vector q
i, a key vector ki,
and a value vector vi.
	
	
   


 
   
 
 
 
 


 
 
 

	
	

	

	

	

 
  
 
  

  

	


Fig. 6. The structure of our neural network
Then, we use the query vector qiof theith input and the
key vector kjof each word of the input sentence to calculate
the attention scores through dot products. The attention scoreagainst the ith input is computed as follows:
Œ±
i,j=qi¬∑kj‚àö
d(1)
wheredis the dimension of qiandkj. The score determines
how much focus to place on the jth input as we encode the
ith input. Then, we get the normalized scores by a softmax
function:
ÀÜŒ±i,j= softmax( Œ±i)=exp(Œ± i,j)/summationtext
texp(Œ± i,t)(2)
To keep the values of the tokens we want to focus on intact,and drown-out irrelevant tokens, we multiply each value vectorby the softmax score and sum up the weighted value vectors:
z
i=/summationdisplay
jÀÜŒ±i,jvj (3)
For faster processing, the calculation can be done in matrix
form, shown as follows:
Attention( Q,K,V)=s o f t m a x (QKT
‚àödk)V (4)
In addition, S MART DOC adopts the multi-head attention
withhheads to focus on different channels of the input
vectors. The outputs of hheads self-attention are concatenated
into one matrix and then are linearly projected by the linearlayer:A=c o n c a t ( Attention
i(Qi,Ki,Vi))W , whereWis
the parameter matrix of the linear layer. Then, the outputs ofthe multi-head self-attention layer are fed into a feed-forwardneural network.
B. Decoder
The decoder component mainly consists of two parts,
namely, the self-attention layer and the encoder-decoder at-
tention layer. The self-attention layer is similar to that in theencoder component except that it only deals with generatedwords in the output sequence. Different from the self-attentionlayer, the encoder-decoder attention layer learns the relation-ship between the source code and the target user notice. Thecalculation of attention is similar to self-attention. The Queriesmatrix Qcomes from the output of the self-attention layer and
the Key Kand V alues matrix Vfrom the output of the encoder
component. For each step, the decoder outputs a state vectorvwhich can be turned into a word of a sequence.
8C. Pointer Generator
As described in Section II, the Dynamic Expressions Mech-
anism of smart contracts user notice causes that words in
user notices can be copied from the source code. So we
integrate the pointer generator [22] in our approach to solvethis problem.
The pointer generator is calculated from the vocabulary
distribution P
vocab , the copy distribution Pcopy , and the gen-
eration probability pgen.
1) V ocabulary Distribution: According to the Transformer
architecture, the vocabulary distribution pvocab is calculated
by a softmax layer which follows a linear layer:
Pvocab= softmax(W ‚àóv) (5)
W is a learnable parameter that projects the vector binto a
larger vector (i.e., has the same size as the vocabulary size).
2) Copy Distribution: Copy distribution Pcopy is the proba-
bility of copying a word from the input sequence, i.e., sourcecode in this work. It is computed from the attention distri-bution, namely, the output of the encoder-decoder attentionlayer in Decoder. The calculation of attention distributionbetween encoder and decoder is similar to the self-attentionlayer excepted that the Key vector Kis from the outputs
of the encoder R=(r
1,...,rm). The attention distribution
Œ±jbetween the target words yjand the source code tokens
w1,...,w mis:
Œ±j= softmax(QjKT
‚àödk) (6)
Then, the copy distribution Pcopy is calculated as follows:
Pcopy=/summationdisplay
i:w i=yjŒ±j
i (7)
3) Final Distribution: At last, the model uses a soft switch
pgen to choose between generating a word from vocabulary
Pvocab or copying a token from the input source code Pcopy .
Similar to See et al. [22], the generation probability pgen is
for predicting yjcalculated from the concatenation of decoder
input ÀÜyj, decoder state vj, and the attention distribution Œ±j:
pgen=œÉ(Wgen[ÀÜyj‚àí1;vj;Œ±j]+b) (8)
whereWgen andbare learnable parameters. œÉis the Sig-
moid function and pgen‚àà[0,1]. At last, the Ô¨Ånal distribution
foryjis:
P(yj)=pgenPvocab+(1‚àípgen)Pcopy (9)
D. Transfer Learning
During the training process, deep learning models need
large amounts of labeled data. However, the parallel dataof smart contract function and notice is limited. To betterlearn the latent knowledge in smart contracts, we exploitthe transfer learning technique to reuse learned knowledge.Transfer learning is an effective technique to alleviate the datahungry issue. Transfer learning goes beyond speciÔ¨Åc tasksand domains (in this paper, comment generation), and triesto leverage knowledge from pre-trained models and use it tosolve target problems (in this paper, user notice generation).Considering the features of smart contract functions, we selectcomment generation for Java methods as the source task T
S
to learning features of programming language.
The learned knowledge of the Java method, namely, source
domainDS, is then transferred into the target task TT.
1) Pre-training Procedure: To get a good code represen-
tation model for Solidity functions, we Ô¨Årst pre-train ourmodel on the Java corpus D
S. This is reasonable due to
the following reasons. First, the Java corpus is much largerthan the size of the Solidity functions (i.e, 11,409 ), whichhas provided sufÔ¨Åcient data sets for training a comprehensivemodel. Second, the Solidity language is similar to the Javalanguage with respect to their grammars and syntax; thesesimilar features learned from Java corpus may also be effectivefor Solidity functions. To pre-train the model, we exploit theencoder to learn the semantic representation of Java methodsand use the decoder to generate Java comments accordingto the learned representation. The encoder and the decoderare introduced above. The pre-trained encoder contain theknowledge that convert a source code in Java language tothe semantic representation. Thus, we can obtain the Javamethod knowledge D
Sby accessing the pre-trained encoder
parameters.
2) Fine-tuning Procedure: When the model is pre-trained,
we then Ô¨Åne-tune it on the user notice generation task. TheÔ¨Åne-tuning process can quickly adapt the knowledge from theJava pre-trained model to learn the code semantics and struc-tures of Solidity functions. During the Ô¨Åne-tuning procedure,we reused the pre-trained encoder to learn the representation ofsmart contract functions. The downstream task of generatinguser notices can be implemented by a decoder which receivesuser notice representations from the pre-trained encoder. Inthis way, we can reuse the pre-trained knowledge from theJava programming language.
IV . E
V ALUA TION
In this section, we Ô¨Årstly describe the evaluation corpus
of the task. We then introduce the baselines to compareand evaluation metrics. Lastly, we explain our experimentalsettings. The replication package is available
1.
A. Dataset
We use the raw smart contract dataset provided by Chen
et al. [23], which contains 54,739 veriÔ¨Åed Solidity Ô¨Åles (eachÔ¨Åle may contain multiple smart contracts) crawled from Ether-scan [24]. We describe how we prepared the dataset for usernotice generation as follows.
Preprocessing: First, we exploit the Solidity-parser [25] to
parse smart contracts and extract functions. We exclude the
Constructor functions since they are trivial to generate noticefor such functions. As Solidity-parser does not support Nat-Spec comments extraction, we deÔ¨Åne regular expressions to
1https://github.com/xing-hu/SmartDoc
9TABLE I
THE STA TISTICS OF OUR COLLECTED SMART CONTRACTS AND FUNCTIONS
Contract FunctionFunctions with
User noticeAverage Function LOC
28,2793 1,296,556 51,567 6.34
(a) Code length distribution (b) User notice length distribution
Fig. 7. Length distribution of the training data
extract NatSpec comments that are tagged with @notice for
functions. Table I provides the statistics of the preprocessed
dataset. We extract 1,296,556 functions from smart contractsand get 51,567 functions with user notices. The averagenumber of Lines of Code (LOC) of smart contract functionsis 6.34.
Filtering: Then, we extract the user notices (NatSpec com-
ment labeled @notice) from smart contract functions and
Ô¨Åltered out non-English samples. Considering that duplicate
code has negative impacts on neural networks and introducesbias in evaluation, we remove duplicate functions and usernotices. Finally, we get 7,878 /angbracketleftfunction,notice/angbracketright pairs.
Generating Training/Test sets: We split dataset into training
set and test set. We randomly select 1k pairs for testing andthe rest for training. Figure 7 illustrates the length distributionof functions and user notice on the training data. We Ô¨Ånd thatmore than 96% code snippets have less than 200 tokens anduser notice has less than 50 words. In addition, the mode oftheir lengths are 30 and 10, respectively.
Tokenization: To convert functions into sequential text, we
tokenize the source code via Solidity-parser. Then, we tokenize
the user notice by Natural Language Toolkit (NLTK) [26].The vocabulary size of code and notice is 17,844 and 4,692,respectively.
B. Baselines
We compare our model with the following baselines:
1) attendgru: attendgru [11] exploits the attentional
seq2seq model to generate code comment. It includes an
encoder and a decoder that are both gated recurrent unit(GRU) [27]. The encoder aims to learn the representationfrom the source code and the decoder generates commentsfrom learned representation. They propose to use an attentionmechanism to attend words in the output summary sentence towords in the code word representation. During the predictionphase, they use a greedy search algorithm for inference thatminimizes the number of experimental variables and compu-tation cost.2) ast-attendgru: ast-attendgru [11] integrates struc-
tural information on the basis of attendgru. The structural
information comes from the abstract syntax tree (AST). Inaddition to the code encoder, it also contains an encoder to pro-cess ASTs. They traverse ASTs into sequences by Structure-based Traversal (SBT) proposed by Hu et al. [8] before fedinto neural networks. A separate attention mechanism is usedto attend the words to parts of the AST. Then, they concatenatethe vectors from each attention mechanism to create a contextvector. Finally, they predict the comment one word at a timefrom the context vector, following what is typical in seq2seqmodels.
3) Re
2Com: Re2Com [10] is the state-of-the-art code
comment generation approach that integrates three kinds oftechniques, namely, IR, template, and neural networks. Themodel consists of two modules: a Retrieve module and aReÔ¨Åne module. In the Retrieve module, Re
2Com exploits IR
techniques to retrieve the most similar code snippet from alarge parallel corpus of code snippets and their correspondingcomments, and treat the comment of the similar code snippetas an exemplar. In the ReÔ¨Åne module, it applies a novelseq2seq neural network whose encoder takes the given codesnippet, the similar code snippet, and the exemplar as inputand the decoder generates the token sequence of a comment.
C. Evaluation Metrics
1) BLEU: Following Wei et al. [10], we evaluate different
approaches using the metric BLEU [28]. It calculates the
similarity between the generated notice and references. Thesimilarity is computed as the geometric mean of n-grammatching precision scores multiplied by a brevity penalty toprevent very short generated sentences:
BLEU=BP¬∑exp(
N/summationdisplay
n=1wnlogpn) (10)
wherepnis the precision scores of matched n-grams. In this
paper, N is set to 4 that is the same as previous studies.It is widely used in various tasks of automatic softwareengineering, such as API sequence generation [29], commentgeneration [8], [9], [10], and commit message generation [30].We reuse the evaluation script provided by Wei et al. [10] tocompute the BLEU scores.
2) ROUGE-L: ROUGE-L [31] is the other widely used
metric that takes into account sentence level structure similar-ity naturally and identiÔ¨Åes longest co-occurring in sequencen-grams automatically. For two sentences X(with length m)
andY(with length n), in which Xis a reference and Yis
a candidate generated sentence. ROUGE-L Ô¨Årst calculates theprecision and recall the longest common subsequence of them,i.e.,
P
lcs=LCS(X,Y)
n(11)
Rlcs=LCS(X,Y)
m(12)
10TABLE II
COMPARISONS OF SMART DOC WITH EACH BASELINE IN TERMS OF BLEU
AND ROUGE-L (* P<0.05)
Approaches BLEU B1 B2 B3 B4 R-L
attendgru 29.01 37.39 28.52 26.41 25.15 38.48
ast-attendgru 26.01 33.75 25.71 23.61 22.34 34.51
Re2Com 29.37 41.39 28.99 25.77 24.07 34.55
SMART DOC 47.39* 56.51* 46.78* 44.27* 43.08* 51.86*
Then, the Ô¨Ånal score is calculated according to PlcsandRlcs:
Flcs=(1+Œ≤2)RlcsPlcs
Rlcs+Œ≤2Plcs(13)
We follow the default value of Œ≤provided in [31] and set
it as 1.
D. Training Details
We implement S MART DOCon top of TensorFlow [32]. Both
token embeddings and hidden size are set to 256 dimensions.
In addition, the number of attention heads and blocks are setto 4 and 3, respectively. All parameters are optimized usingAdam [33] with the initial learning rate of 0.0005. FollowingV aswani et al. [12], we increase the learning rate linearlyfor the Ô¨Årst 4000 steps (i.e., warmup steps) and decrease itthereafter proportionally to the inverse square root of the stepnumber. During the training, the batch size is set to 32. Tomitigate overÔ¨Åtting, we exploit dropout mechanism and setthe dropout rate as 0.1. We set the maximum length of theencoder to 200 and the maximum length of the decoder to 50.Training runs for 50 epochs. We conduct our experiments ona Linux server with an NVIDIA GeForce RTX 2080Ti GPUhaving 10 GB memory.
V.
RESULTS
To gain a deeper understanding of the performance of our
approach, we conduct analysis on our evaluation results in thissection. SpeciÔ¨Åcally, we focus on three research questions:
‚Ä¢How effective is our S MART DOC for generating user
notice given smart contract functions?
‚Ä¢How effective is each component of S MART DOC?
‚Ä¢How efÔ¨Åcient is S MART DOC?
A. RQ1: SMART DOCOverall Effectiveness (vs. Baselines)
In this RQ, we want to investigate how effective our
approach is and how much performance improvement ourapproach can achieve over the baselines.
1) Experimental Setup: We apply our approach and the
baseline methods (i.e., attendgru, ast-attendgru, and
Re
2Com) on the collected dataset, and compare their perfor-
mance in terms of BLEU and ROUGE-L. To check whetherthe performance differences between S
MART DOCand baseline
approaches are signiÔ¨Åcant, we run Wilcoxon signed-rank tests[34] at the conÔ¨Ådence level of 95%. For each approach, wecollect 1,000 scores (one for each case) considering everyevaluation metric. Then, we conduct the Wilcoxon signed-ranktest for each pair of competing approaches considering thesescores.TABLE III
EFFECTIVENESS OF EACH INCOMPLETE V ARIANT OF OUR APPROACH IN
TERMS OF BLEU AND ROUGE-L.(* P<0.05)
Approaches BLEU B1 B2 B3 B4 R-L
Transformer 43.52 52.02 43.2 40.56 39.34 49.91
Transformer+P 45.12 53.23 44.59 42.34 41.25 48.55
SMART DOC 47.39* 56.51* 46.78* 44.27* 43.08* 51.86*
2) Results: Table II shows the BLEU and ROUGE-L scores
for our approach S MART DOC and the baseline techniques.
We can observe that ast-attendgru has the worst perfor-
mance and our approach S MART DOCoutperforms all baseline
models signiÔ¨Åcantly. Compared to baselines, the improvementsof our proposed approach S
MART DOCare more than 60% and
35% in terms of BLEU and ROUGE-L, respectively. All thep-values are substantially smaller than 0.05, which means ourapproach signiÔ¨Åcantly improves over the baseline approaches.
B. RQ2: Ablation Analysis
In this paper, we use the Transformer to model the source
code of functions in smart contracts. Then, we exploit the
pointer mechanism to copy words from the source code duringthe user notice generation process. To alleviate the limitationof minimal labeled data, we also leverage the transfer learningto utilize the pre-trained knowledge before predicting the usernotice. We want to investigate the impacts of these componentson the performance of our approach.
1) Experimental Setup: To illustrate the importance of
each component, we compare our approach with two of itsincomplete variants:
‚Ä¢Transformer removes both the pointer mechanism and
the pre-trained knowledge.
‚Ä¢Transformer+P is transformer with pointer and only
removes the pre-trained knowledge.
We can observe the effectiveness of pointer mechanismand transfer learning technique by comparing S
MART DOC
andTransformer. Then, we compare S MART DOC and
Transformer+P to measure the improvements from the
pre-trained knowledge. Similar to RQ1, BLEU and ROUGE-Lare used to evaluate our approach and the two variants. TheWilcoxon signed-rank test is also computed.
2) Results: The effectiveness of the two variants are
demonstrated in Table III. We can observe that our approach
S
MART DOCoutperforms the other two variants on each met-
ric. Compared to experimental results of baselines shownin Table II, both Transformer andTransformer+P
outperforms baselines. SpeciÔ¨Åcally, the Transformer booststhe performance by a large margin (more than 40% in termsof BLEU and 30% in terms of ROUGE-L) on user noticegeneration compared to baselines that are based on recurrentneural network. The integration of Pointer mechanism im-proves the performance further (with improvement of about4% in terms of BLEU) ‚Äì as compared to Transformer.
11Source Code:
function vest() external { 
uintnumEntries =numVestingEntries (msg.sender);
uinttotal;
for(uinti=0;i<numEntries ;i++) { 
uinttime = getVestingTime (msg.sender,i);
if(time >now) { break ;} 
uintqty =getVestingQuantity (msg.sender, i);
if(qty == 0) { continue ;} 
vestingSchedules[ msg.sender][i] =[0,0];
total =total.add(qty);
} 
if(total ! =0) { 
totalVestedBalance =totalVestedBalance .sub(total);
totalVestedAccountBalance[msg .sender] =
totalVestedAccountBalance[msg .sender].sub(total);
synthetix .transfer (msg.sender,total);
emit Vested(msg.sender,now,total);
} 
}
 

	


   

	

		



	   	
 

 	


Fig. 8. A test example with copied words
It helps copy words from smart contract instead of gen-
erating words during the prediction phase. However, theROUGE-L of Transformer+P is slightly lower than that
ofTransformer. The reason of the slight drop is that
the Pointer usually copies one word instead of N-grams,thus, the longest co-occurring n-grams are lower than se-quences generated by the Transformer. Compared to the
Transformer+P,S
MART DOCexploits the transfer learning
technique to alleviate the limitation of the amount of thedataset. We can observe that it improves about 4% and 7%in terms of BLEU and ROUGE-L, respectively. In addition,the P-value of the improvements is less than 0.05 that indicatesout approach S
MART DOCsigniÔ¨Åcantly outperforms variants.
3) Effectiveness of Pointer mechanism: In this paper, we
integrate Pointer mechanism to copy words from source codeduring the user notice prediction. It further improves theaccuracy of generated notice. To Ô¨Ågure out which tokenswill be copied into the notice, we manually inspect the testresults of the predictions. We Ô¨Ånd that our approach usuallycopies tokens with natural features, such as function namesand parameter names. Figure 8 shows an example whosegenerated notice contains words copied from source code.Word ‚Äúvested‚Äù is directly copied from the function namein the emit statement. In addition, some words are copiedfrom the sub-words in speciÔ¨Åc parameter names. For example,‚Äúschedule‚Äù is derived from parameter ‚ÄúvestingSchedules‚Äù.
C. RQ3: Time Costs of our Approach
Neural network models need to be trained before being
adapted to generate user notice for smart contracts. The
training process is conducted ofÔ¨Çine and can be used tomake prediction online. In this research question, we want toinvestigate the training time cost and the test time cost of ourapproach to better understand the practicality of our approach
S
MART DOC.
1) Experimental Setting: To measure the time complexity
of our approach and other baselines, we record the starttime and the end time of their training process and the testprocess. For fair comparison, all models are trained on theTABLE IV
TIME COSTS OF DIFFERENT APPROACHES
Approaches Train Test Test One Params
attendgru 0.5h 3.04s 0.004s 16.9 M
ast-attendgru 0.9h 4.67s 0.005s 17.4 M
Re2Com 8.2h 16s 0.02s 18.1 M
SMART DOC 0.59h 72s 0.07s 25.0M
same machine containing a NVIDIA GeForce RTX 2080TiGPU with 10 GB memory.
2) Results: Table IV illustrates the time costs of our ap-
proach and baselines. Compared to baselines, our approach
S
MART DOChas the most parameters with about 25M trainable
parameters. Re2Com incurs the highest cost (about 8.2 hours)
to train the model well. During the test phase, approachattendgru is the most efÔ¨Åcient approach; on average, it takes0.004 second to recommend user notice. However, the qualityof user notice generated by it is limited. Our approach,
S
MART DOCtakes about 0.59 hour to be trained well and takes
about 70ms to recommend a user notice. The experimentalresults demonstrate that our approach is efÔ¨Åcient for practicaluses.
VI. H
UMAN EV ALUA TION
Although automatic metrics, such as BLEU and ROUGE-
L, can evaluate the gap between the generated user doc-umentation and reference texts written by humans, it cannot reÔ¨Çect the human perceptions on the generated userdocumentation. We follow Wei et al. [10] to conduct humanevaluation. The human evaluation measures three aspects,including the Similarity of generated user documentation
and references, Naturalness (grammaticality and Ô¨Çuency of
the generated user documentation), and Informativeness (the
amount of content carried over from the input code to thegenerated user documentation, ignoring Ô¨Çuency of the text).The scores range from 0 to 4 (the higher the better). Weinvite 10 volunteers with 1-3 years of blockchain or smartcontract experience and have good English proÔ¨Åciency for30 minutes each to evaluate the generated user documen-tation in the form of a questionnaire. We randomly select100 smart contract functions and provide references (human-written user documentation) for each function. In addition, wealso provide four machine-generated user documentation thatis generated by attendgru, ast-attendgru, Re
2Com,
and S MART DOC, respectively. Each participant is asked to
score each sample considering similarity, naturalness, and
informativeness aspects. All these scores are integers, ranging
from 0 to 4. During the annotation, participants are allowedto search the Internet for related information and unfamiliarconcepts. Participants do not know which approach produceswhich texts.
Table V presents the human evaluation results of our ap-
proach and baselines. S
MART DOC outperforms other tech-
niques in three aspects, especially in Informativeness (im-
provements more than 30%) and Similarity (improvements
12TABLE V
THE RESULTS (STANDARD DEVIA TION œÉIN PARENTHESES )OF HUMAN
EV ALUA TION (*P<0.05)
Approaches Informativeness Naturalness Similarity
attendgru 1.80 (1.44) 3.03 (1.04) 1.71 (1.44)
ast-attendgru 1.85 (1.43) 3.06 (1.08) 1.74 (1.35)
Re2Com 2.07 (1.37) 3.24 (0.86) 2.06 (1.30)
SMART DOC 2.69* (1.31) 3.45* (0.73) 2.63* (1.34)
Fig. 9. The count of similarity scores of the generated user notices by
SMART DOCcompared with human-written user notices.
more than 28%). Naturalness scores of the four approaches
(all more than 3) are much higher than Informativeness and
Similarity scores, indicating that almost all generated user
documentation is grammatical and Ô¨Çuent. Similar to Wei
et al [10], the difference in standard deviation of the fourmethods is also very small, indicating that their scores areabout the same degree of concentration. All the p-values aresubstantially smaller than 0.05, which shows the improvementsof our proposed model are statistically signiÔ¨Åcant.
Figure 9 shows the count of similarity scores of user notices
generated by S
MART DOC. We can Ô¨Ånd that 34% generated
user notices are almost the same as human-written ones (i.e.,generated user notices with score 4). 29% generated usernotices are scored as 3 that means 29% of them share manysimilar words and express similar meanings. In other words,63% of user notices generated by S
MART DOCare similar to
human-written ones and can be added to the source code.
VII. DISCUSSION
In this section, we discuss the performance of the generated
user notice and analyze the effectiveness of our approach.
A. Cross-Smart contract validation
Different from mature smart contracts with many transac-
tions, new smart contracts often lack sufÔ¨Åcient training data.
Thus it is difÔ¨Åcult to directly apply our approach to generateuser notice for a new smart contract. This problem may beovercome through the cross-project prediction, which uses thedata collected from mature smart contracts to train a model,and applies the trained model to make predictions for newsmart contracts. We want to investigate whether our approachis still effective for the cross-project setting.TABLE VI
THE A VERAGE BLEU AND ROUGE-L SCORES OF SMART DOC AND
BASELINES IN CROSS -PROJECT SETTING .
Approaches BLEU B1 B2 B3 B4 R-L
attendgru 30.83 38.62 30.34 28.4 27.15 39.67
ast-attendgru 28.5 35.99 28.03 26.18 25 36.32
Re2Com 30.63 40.15 30.13 27.68 26.31 36.10
SMART DOC 43.44 50.26 42.81 41.11 40.24 47.79
1) Experimental Setting: We conduct a cross-project vali-
dation experiment for our collect smart contracts. This processrepeats 5 times and we train various models on functions from80% smart contracts and test these models on functions fromthe rest 20% smart contracts each time. BLEU scores andROUGE-L are used to measure the effectiveness of cross-project predictions.
2) Results: Table VI illustrates the cross-project validation
for user notice generation. It shows the average scores fromthe 5 times experiments. We can observe that our approach
S
MART DOC still outperforms baselines signiÔ¨Åcantly. Com-
pared to results in RQ1, the performance of baselines (i.e.,attendgru, ast-attendgru, and S
MART DOC) improves
whereas S MART DOCdecreases in cross-project setting. The
smart contracts come from different domains, which intro-duces the decrease of performance of our approach S
MART -
DOC. For baselines, they have more samples to train the
models and can improve the performance.
B. Investigating the effectiveness of Transfer Learning
To address the user notice hungry problem, we employ
the Transfer Learning technique to transfer the knowledge of
comment generation for Java into user document generationfor smart contracts. We would like to investigate whether theTransfer Learning is effective for general neural models. Todo this, we Ô¨Årst analyze the convergence time of S
MART DOC
with/without transfer learning, after that, we further explorethe performance of baseline models after adding TransferLearning techniques.
1) Experimental Settings.: First, we record the results of
S
MART DOC andTransformer+P for every 2,000 steps
(about 10 epochs) during the training process, and comparetheir convergence. Then, we equip each baseline with transferlearning technique to explore their performance with trans-ferred knowledge. Similar to S
MART DOC, we Ô¨Årst pre-train
various models with Java dataset. As Leclair et al [11] providetrained models in their replication, we reuse their trainedmodels before training on the smart contract dataset. Then,we reuse the weights of source code encoder and Ô¨Åne tunethem on the smart contract dataset. Finally, we evaluate themin terms of BLEU scores and ROUGE-L.
2) Results.: Figure 10 presents the convergence of S
MART -
DOC andTransformer+P, in which S MART DOC uses
transfer learning whereas Transformer+P not. We can ob-
serve that S MART DOCoutperforms Transformer+P about
15% in terms of ROUGE-L after training 2,000 steps. In
13Fig. 10. The convergence of S MART DOC(with transfer learning technique)
andTransformer+P
TABLE VII
COMPARISONS OF SMART DOC WITH EACH BASELINE IN TERMS OF BLEU
AND ROUGE-L WHEN INTEGRA TED TRANSFER LEARNING .
Approaches BLEU B1 B2 B3 B4 R-L
attendgru + T 29.05 38.92 28.58 26.07 24.57 40.15
ast-attendgru + T 29.01 38.43 28.66 26.13 24.6 38.92
Re2Com + T 25.66 39.59 25.46 21.62 19.89 31.32
SMART DOC 47.39 56.51 46.78 44.27 43.08 51.86
addition, we Ô¨Ånd that the convergence of Transformer+P
is slow by 2,000 steps compared to SMARTDOC. In addition,
we can observe that the S MART DOCconvergence curve has
higher start, higher slope, and higher asymptote. The higherstart means that our model has the better initial ability (beforereÔ¨Åning the model) to generate user notice. The higher slopeindicates that rate of improvement of our model during train-ing is steeper. Finally, the higher asymptote shows a betterconverged skill of S
MART DOC.
Table VII shows the results of all models integrated the
transfer learning technique. We can Ô¨Ånd that attendgru and
ast-attendgru perform better when integrated transfer
learning and ast-attendgru has biggest improvement
(11.8% in terms of BLEU) among them. However, Re2Com
performs worse when integrated transfer learning technique.The effectiveness of Re
2Com comes from two parts, template
notice and the source code. As the pre-trained knowledgeof source code is affected by the retrieved template notice,thus lead to worse performance of Re
2Com. In summary, the
transfer learning helps models leverage existing knowledgewhen training on a new task. It can alleviate the limit of theamount of dataset and improve performance of neural models.
C. Investigating the User Notice with Low Scores
We are also curious about why our approach fail to generate
accurate user notice. The reasons are demonstrated in the
following subsections.
1) Abbreviations: Different from other software, smart con-
tracts development is related to currencies and Ô¨Ånance. Thus,there are many abbreviations in human-written notice. For ex-ample, Figure 11 illustrates an example human-written noticecontaining many abbreviations, including ‚ÄúDOL‚Äù, ‚ÄúV AULT‚Äù,and ‚ÄúETH‚Äù. However, in the user notice generated by S
MART -Source Code:
function buy() payable public {
require (!frozenAccount[msg .sender]);
require (msg.value >0);
buyToken ();
}
 
		
   	
  

   
 
  
"!
Fig. 11. A test example whose reference contains abbreviations
Source Code:
function approveAndCall (address _spender ,uint256 _value,
bytes_extraData) returns (boolsuccess) { 
mimonedarecipiente spender =mimonedarecipiente (_spender) ;
if(approve(_spender, _value)) { 
spender.receiveApproval (msg.sender,_value,this,_extraData);
returntrue;
} 
}
 



	

   	



	

   	


		

	
Fig. 12. A test example with missed details
DOC, these abbreviations are replaced by the full version of the
word or phrase. ‚ÄúDOL‚Äù is the symbol for ‚Äúmetadollar‚Äù [35]
that is generated by S MART DOC. Both ‚Äúether‚Äù and ‚ÄúETH‚Äù
represent the native cryptocurrency token of the Ethereumplatform. Although these words are regarded as error tokensduring evaluation, they describe the same meaning.
2) Less detailed information: Some details are missed in
the generated notice. For example, Figure 12 illustrates anexample whose details are missing in the generated user notice.We can Ô¨Ånd that information ‚Äúno more than ‚Äò
value‚Äô tokens‚Äù
is missing in the notice generated by S MART DOC. This
missed information causes the low score during evaluation.In addition, we Ô¨Ånd that the missed information is hard to belearned from the given input source code. It is usually impliedin API invocations, such as ‚Äúno more than ‚Äò
value‚Äô tokens‚Äù
can be obtained in the invocation of API ‚Äúapprove‚Äù.
D. User notice generation VS. Comment generation
Problem Difference: The audience of code comments is
developers who can understand the source code, and code
comments often contain many details such as the imple-mentation details. Considering the audiences of smart con-tracts include developers and end-users, comments of smartcontracts are divided into two types, i.e., the comment fordevelopers (tagged with @dev) and the comment for end-users(tagged with @notice). Comments for developers (tagged
with@dev) are similar with general code comments, while
comments for end-users (tagged with @notice) are a special
type of comment, whose audience is contract end users whoare unable to read the source code. We refer to this kind ofspecial comment as user notice in this study.
Technique Difference: Compared to code comments, user
notices support dynamic expression mechanisms that usu-ally copy variable names from source code. These variables
14will dynamically be replaced by corresponding values when
end users interact with the contract. Compared to commentgeneration, user notice generation is more dependent on thecopy mechanism. In addition, the dataset of code commentgeneration is large enough to train a model well. However, thedataset for user notice generation is limited, thus, we need toexploit pre-trained models to better avoid overÔ¨Åtting on smalldata.
VIII. T
HREA TS TO VALIDITY
We have identiÔ¨Åed the following threats to validity among
our study:
Internal Validity In this paper, we exploit Java dataset to pre-
train the models and then use the learned knowledge by using
the trained parameters. Pre-trained knowledge learned fromdifferent programming languages may bias the effectiveness ofour approach on user notice generation. We will try to employdatasets in different programming languages (e.g., Python andJavascript) to pre-train the code knowledge in the future.
Data Validity We use the smart contracts provided by Chen et
al. [23] that contains 54,739 smart contracts. However, because
a very large number of smart contracts are copied from othersmart contracts [36], [37], [38], [39], the number of collected/angbracketleftfunction,notice/angbracketright pairs is limited after deduplication. The
selected contracts may not be sufÔ¨Åciently diverse or repre-sentative of all contracts. To mitigate the threat, we conductexperiments on cross-project setting. The trained models areapplied to generate user notice for new smart contracts. Webelieve that our approach is effective for new smart contractsif it performs well on cross-project validity.
External Validity We validate our approach by comparing
the generated user notice and human-written user notice. We
assume that human-written user notice is correct. However,human-written user notice may also not correct sometimes andwe can not ensure the quality of it. For example, some human-written user notice may be outdated during the development.Therefore, the results may be biased and incomprehensive.
IX. R
ELA TED WORK
Code comment generation is the most relevant task
which aims to generate natural descriptions for code snip-pets. Manually-crafted templates [40], [41], [42], IR tech-niques [43], [7], [44], [45], and neural models [11], [8], [10],[46], [47] are widely used in automatic comment generation.
Approaches based on manually-crafted templates usually
leverage stereotype identiÔ¨Åcation techniques to generate com-ments for code snippets. Sridhara et al. [41] propose to con-struct Software Word Usage Model (SWUM) to select relevantkeywords from source code and then leverage them to con-struct natural language descriptions from deÔ¨Åned templates.Mcburney et al. [42] exploit SWUM to extract keywords fromJava methods and use PageRank to select the most importantmethods from a given context.
Information Retrieval (IR) techniques are widely used in
comment generation task. Generally, these approaches Ô¨Årstretrieve similar code snippets with comments and take theircomments as the output. Latent Semantic Indexing (LSI),V ector Space Model (VSM), and Latent Dirichlet Allocation(LDA) are widely used in comment generation. Kuhn etal. [43] propose to use the Latent Semantic Indexing (LSI)technique to extract topics that reÔ¨Çect the intention of sourcecode. Haiduc et al. [7] exploit two IR techniques, V ectorSpace Model (VSM) and LSI, to analyze methods and classesin Java projects and generate short descriptions for them.Different from these works, Wong et al. [44], [45] exploitclone detection techniques to retrieve similar code snippetsand use corresponding comments for comment generation.
In recent years, considerable attention has been paid to
neural networks on comment generation. Iyer et al. [48] Ô¨Årstpropose to utilize the encoder-decoder framework to generatecomments, in which the encoder is token embeddings of sourcecode and the decoder is an LSTM. The experimental resultson C# and SQL comment generation illustrate that neuralnetworks perform better than traditional techniques. Soonafter, Hu et al. [8] propose to integrate structural informationwhile generating comments for Java methods. They propose anew approach to traverse an AST into a sequence and encodethe sequence by an LSTM. Some studies [49], [10] combinethe IR-based techniques and deep-learning-based techniques togenerate code comments. Wei et al. [10] propose an approachthat takes the advantages of manually-crafted templates, IR,and neural networks techniques. It Ô¨Årst retrieves a similar codesnippet from the training set and uses its comment as theexemplar to guide the neural model for comment generation.
X. C
ONCLUSION AND FUTURE WORK
In this paper, we propose a new approach, S MART DOC,
based on Transformer, Pointer mechanism, and transfer learn-ing technique for smart contract user notice generation. Wehave evaluated our approach with 1,000 pairs of smart contractfunctions and their user notice. Experimental results showthat it can effectively generate user notice for smart contractsand outperforms the state-of-the-art approaches signiÔ¨Åcantly.In addition, we conduct human evaluation to investigate thehuman perspectives on the generated user notice. The resultsshow that our approach can generate natural and informativeuser notice, and the generated user notice is much more similarto the reference text than baselines.
XI. A
CKNOWLEDGMENTS
This research was partially supported by the National
Science Foundation of China (No. U20A20173), Key Re-search and Development Program of Zhejiang Province(No.2021C01014), and the National Research Foundation,Singapore under its Industry Alignment Fund ‚Äì Preposition-ing (IAF-PP) Funding Initiative. Any opinions, Ô¨Åndings, andconclusions, or recommendations expressed in this materialare those of the author(s) and do not reÔ¨Çect the views of theNational Research Foundation, Singapore.
15REFERENCES
[1] N. Radziwill, ‚ÄúBlockchain revolution: How the technology behind
bitcoin is changing money, business, and the world,‚Äù The Quality
Management Journal, vol. 25, no. 1, pp. 64‚Äì65, 2018.
[2] W. Chen, M. Ma, Y . Ye, Z. Zheng, and Y . Zhou, ‚ÄúIot service based on
jointcloud blockchain: The case study of smart traveling,‚Äù in 2018 IEEE
Symposium on service-oriented system engineering (SOSE). IEEE,
2018, pp. 216‚Äì221.
[3] T. Chen, Z. Li, Y . Zhu, J. Chen, X. Luo, J. C.-S. Lui, X. Lin,
and X. Zhang, ‚ÄúUnderstanding ethereum via graph analysis,‚Äù ACM
Transactions on Internet Technology (TOIT), vol. 20, no. 2, pp. 1‚Äì32,2020.
[4] N. Szabo, ‚ÄúSmart contracts,‚Äù 1994.
[5] M. Alharby and A. V an Moorsel, ‚ÄúBlockchain-based smart contracts: A
systematic mapping study,‚Äù arXiv preprint arXiv:1710.06372, 2017.
[6] https://etherscan.io/chart/etherprice.[7] S. Haiduc, J. Aponte, L. Moreno, and A. Marcus, ‚ÄúOn the use of
automated text summarization techniques for summarizing source code,‚Äù
in2010 17th Working Conference on Reverse Engineering. IEEE, 2010,
pp. 35‚Äì44.
[8] X. Hu, G. Li, X. Xia, D. Lo, and Z. Jin, ‚ÄúDeep code comment gener-
ation,‚Äù in 2018 IEEE/ACM 26th International Conference on Program
Comprehension (ICPC). IEEE, 2018, pp. 200‚Äì20 010.
[9] ‚Äî‚Äî, ‚ÄúDeep code comment generation with hybrid lexical and syntac-
tical information,‚Äù Empirical Software Engineering, vol. 25, no. 3, pp.
2179‚Äì2217, 2020.
[10] B. Wei, Y . Li, G. Li, X. Xia, D. Lo, and Z. Jin, ‚ÄúRetrieve and
reÔ¨Åne: Exemplar-based neural comment generation,‚Äù in 2020 IEEE/ACM
International Conference on Automated Software Engineering (ASE).IEEE, 2020.
[11] A. LeClair, S. Jiang, and C. McMillan, ‚ÄúA neural model for gener-
ating natural language summaries of program subroutines,‚Äù in 2019
IEEE/ACM 41st International Conference on Software Engineering(ICSE). IEEE, 2019, pp. 795‚Äì806.
[12] A. V aswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,
≈Å. Kaiser, and I. Polosukhin, ‚ÄúAttention is all you need,‚Äù in Advances
in neural information processing systems, 2017, pp. 5998‚Äì6008.
[13] S. J. Pan and Q. Yang, ‚ÄúA survey on transfer learning,‚Äù IEEE Trans-
actions on knowledge and data engineering, vol. 22, no. 10, pp. 1345‚Äì1359, 2009.
[14] L. Luu, D.-H. Chu, H. Olickel, P . Saxena, and A. Hobor, ‚ÄúMaking smart
contracts smarter,‚Äù in Proceedings of the 2016 ACM SIGSAC conference
on computer and communications security, 2016, pp. 254‚Äì269.
[15] Z. Wan, X. Xia, and A. Hassan, ‚ÄúWhat do programmers discuss about
blockchain? a case study on the use of balanced lda and the referencearchitecture of a domain to capture online discussions about blockchainplatforms across stack exchange communities,‚Äù IEEE Trans Softw Eng,
vol. 2019, pp. 1‚Äì1, 2019.
[16] Z. Wan, X. Xia, D. Lo, J. Chen, X. Luo, and X. Yang, ‚ÄúSmart contract
security: a practitioners‚Äô perspective,‚Äù arXiv preprint arXiv:2102.10963,
2021.
[17] https://solidity.readthedocs.io/en/v0.7.0/natspec-format.html.
[18] https://etherscan.io/address/0x55F93985431Fc9304077687a35A1BA103
dC1e081#code.
[19] https://etherscan.io/address/0x8b5fe260f2b7bdd03efc833f78557a349600
ae46#code.
[20] Q. Wang, B. Li, T. Xiao, J. Zhu, C. Li, D. F. Wong, and L. S.
Chao, ‚ÄúLearning deep transformer models for machine translation,‚Äù
inProceedings of the 57th Annual Meeting of the Association for
Computational Linguistics, 2019, pp. 1810‚Äì1822.
[21] Y . Y ou, W. Jia, T. Liu, and W. Yang, ‚ÄúImproving abstractive document
summarization with salient information modeling,‚Äù in Proceedings of the
57th Annual Meeting of the Association for Computational Linguistics,2019, pp. 2132‚Äì2141.
[22] A. See, P . J. Liu, and C. D. Manning, ‚ÄúGet to the point: Summarization
with pointer-generator networks,‚Äù in Proceedings of the 55th Annual
Meeting of the Association for Computational Linguistics (V olume 1:Long Papers), 2017, pp. 1073‚Äì1083.
[23] J. Chen, X. Xia, D. Lo, and J. Grundy, ‚ÄúWhy do smart contracts self-
destruct? investigating the selfdestruct function on ethereum,‚Äù arXiv
preprint arXiv:2005.07908, 2020.
[24] https://etherscan.io/.
[25] https://pypi.org/project/solidity-parser/.[26] E. Loper and S. Bird, ‚ÄúNltk: the natural language toolkit,‚Äù in Proceedings
of the ACL-02 Workshop on Effective tools and methodologies for
teaching natural language processing and computational linguistics-V olume 1, 2002, pp. 63‚Äì70.
[27] J. Chung, C. Gulcehre, K. Cho, and Y . Bengio, ‚ÄúEmpirical evaluation of
gated recurrent neural networks on sequence modeling,‚Äù arXiv preprint
arXiv:1412.3555, 2014.
[28] K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu, ‚ÄúBleu: a method for
automatic evaluation of machine translation,‚Äù in Proceedings of the 40th
annual meeting of the Association for Computational Linguistics , 2002,
pp. 311‚Äì318.
[29] X. Gu, H. Zhang, D. Zhang, and S. Kim, ‚ÄúDeep api learning,‚Äù in
Proceedings of the 2016 24th ACM SIGSOFT International Symposiumon F oundations of Software Engineering, 2016, pp. 631‚Äì642.
[30] S. Jiang, A. Armaly, and C. McMillan, ‚ÄúAutomatically generating
commit messages from diffs using neural machine translation,‚Äù in2017 32nd IEEE/ACM International Conference on Automated SoftwareEngineering (ASE). IEEE, 2017, pp. 135‚Äì146.
[31] C.-Y . Lin, ‚ÄúROUGE: A package for automatic evaluation of summaries,‚Äù
inText Summarization Branches Out. Barcelona, Spain: Association for
Computational Linguistics, Jul. 2004, pp. 74‚Äì81. [Online]. Available:https://www.aclweb.org/anthology/W04-1013
[32] https://www.tensorÔ¨Çow.org/.
[33] D. P . Kingma and J. Ba, ‚ÄúAdam: A method for stochastic optimization,‚Äù
arXiv preprint arXiv:1412.6980, 2014.
[34] F. Wilcoxon, ‚ÄúIndividual comparisons by ranking methods,‚Äù in Break-
throughs in statistics. Springer, 1992, pp. 196‚Äì202.
[35] https://etherscan.io/address/0x4414a8c55fcbd70a6957e3cb9561e60f0b4
e742d.
[36] J. Chen, X. Xia, D. Lo, J. Grundy, and X. Yang, ‚ÄúMaintaining smart
contracts on ethereum: Issues, techniques, and future challenges,‚Äù arXiv
preprint arXiv:2007.00286, 2020.
[37] N. He, L. Wu, H. Wang, Y . Guo, and X. Jiang, ‚ÄúCharacterizing code
clones in the ethereum smart contract ecosystem,‚Äù in International
Conference on Financial Cryptography and Data Security. Springer,
2020, pp. 654‚Äì675.
[38] Z. Gao, L. Jiang, X. Xia, D. Lo, and J. Grundy, ‚ÄúChecking smart con-
tracts with structural code embedding,‚Äù IEEE Transactions on Software
Engineering, 2020.
[39] Z. Gao, V . Jayasundara, L. Jiang, X. Xia, D. Lo, and J. Grundy,
‚ÄúSmartembed: A tool for clone and bug detection in smart contractsthrough structural code embedding,‚Äù in 2019 IEEE International Con-
ference on Software Maintenance and Evolution (ICSME). IEEE, 2019,pp. 394‚Äì397.
[40] L. Moreno, J. Aponte, G. Sridhara, A. Marcus, L. Pollock, and K. Vijay-
Shanker, ‚ÄúAutomatic generation of natural language summaries for javaclasses,‚Äù in 2013 21st International Conference on Program Compre-
hension (ICPC). IEEE, 2013, pp. 23‚Äì32.
[41] G. Sridhara, E. Hill, D. Muppaneni, L. Pollock, and K. Vijay-Shanker,
‚ÄúTowards automatically generating summary comments for java meth-ods,‚Äù in Proceedings of the IEEE/ACM international conference on
Automated software engineering, 2010, pp. 43‚Äì52.
[42] P . W. McBurney and C. McMillan, ‚ÄúAutomatic source code summa-
rization of context for java methods,‚Äù IEEE Transactions on Software
Engineering, vol. 42, no. 2, pp. 103‚Äì119, 2015.
[43] A. Kuhn, S. Ducasse, and T. G ¬¥ƒ±rba, ‚ÄúSemantic clustering: Identifying
topics in source code,‚Äù Information and software technology, vol. 49,
no. 3, pp. 230‚Äì243, 2007.
[44] E. Wong, J. Yang, and L. Tan, ‚ÄúAutocomment: Mining question and an-
swer sites for automatic comment generation,‚Äù in 2013 28th IEEE/ACM
International Conference on Automated Software Engineering (ASE).IEEE, 2013, pp. 562‚Äì567.
[45] E. Wong, T. Liu, and L. Tan, ‚ÄúClocom: Mining existing source code for
automatic comment generation,‚Äù in 2015 IEEE 22nd International Con-
ference on Software Analysis, Evolution, and Reengineering (SANER).IEEE, 2015, pp. 380‚Äì389.
[46] U. Alon, S. Brody, O. Levy, and E. Yahav, ‚Äúcode2seq: Generating
sequences from structured representations of code,‚Äù in International
Conference on Learning Representations, 2018.
[47] Y . Wan, Z. Zhao, M. Yang, G. Xu, H. Ying, J. Wu, and P . S. Y u,
‚ÄúImproving automatic source code summarization via deep reinforce-ment learning,‚Äù in Proceedings of the 33rd ACM/IEEE International
Conference on Automated Software Engineering, 2018, pp. 397‚Äì407.
16[48] S. Iyer, I. Konstas, A. Cheung, and L. Zettlemoyer, ‚ÄúSummarizing source
code using a neural attention model,‚Äù in Proceedings of the 54th Annual
Meeting of the Association for Computational Linguistics (V olume 1:
Long Papers), 2016, pp. 2073‚Äì2083.
[49] J. Zhang, X. Wang, H. Zhang, H. Sun, and X. Liu, ‚ÄúRetrieval-based
neural source code summarization,‚Äù in 2020 IEEE/ACM 42nd Interna-
tional Conference on Software Engineering (ICSE) . IEEE, 2020, pp.
1385‚Äì1397.
17