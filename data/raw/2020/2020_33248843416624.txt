How Incidental are the Incidents? Characterizing and
Prioritizing Incidents for Large-Scale Online Service Systems
Junjie Chenâˆ—
College of Intelligence and
Computing, Tianjin University
Tianjin, China
junjiechen@tju.edu.cnShu Zhang
Microsoft Research
Beijing, China
v-shuzh@microsoft.comXiaoting He
Microsoft Research
Beijing, China
v-xiah@microsoft.com
Qingwei Linâ€ 
Microsoft Research
Beijing, China
qlin@microsoft.comHongyu Zhang
The University of Newcastle
Callaghan, Australia
Hongyu.Zhang@newcastle.edu.auDan Hao
Peking University
Beijing, China
haodan@pku.edu.cn
Yu Kang
Microsoft Research
Beijing, China
kay@microsoft.comFeng Gao
Microsoft Azure
Redmond, USA
fgao@microsoft.comZhangwei Xu
Microsoft Azure
Redmond, USA
zhangxu@microsoft.com
Yingnong Dang
Microsoft Azure
Redmond, USA
yidang@microsoft.comDongmei Zhang
Microsoft Research
Beijing, China
dongmeiz@microsoft.com
ABSTRACT
Although tremendous effortshave been devoted to thequality as-
surance of online service systems, in reality, these systems still
come across many incidents (i.e., unplanned interruptions and out-
ages), which can decrease user satisfaction or cause economic loss.
Tobetterunderstandthecharacteristicsofincidentsandimprove
the incident management process, we perform the first large-scale
empirical analysis of incidents collected from 18 real-world online
service systems in Microsoft. Surprisingly, we find that although a
largenumberofincidentscouldoccuroverashortperiodoftime,
many of them actually do not matter, i.e., engineers will not fix
them with a high priority after manually identifying their root
cause.Wecalltheseincidents incidentalincidents.Ourqualitative
andquantitativeanalysesshowthatincidentalincidentsaresignifi-cantintermsofbothnumberandcost.Therefore,itisimportantto
prioritize incidents by identifying incidental incidents in advance
to optimize incident management efforts. In particular, we pro-pose an approach, called
DeepIP(Deeplearning based Incident
âˆ—This work was mainly done when he was visiting Microsoft Research.
â€ Corresponding author.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation
on the first page. Copyrights for components of this work owned by others than ACM
mustbehonored.Abstractingwithcreditispermitted.Tocopyotherwise,orrepublish,topostonserversortoredistributetolists,requirespriorspecificpermissionand/ora
fee. Request permissions from permissions@acm.org.
ASE â€™20, September 21â€“25, 2020, Virtual Event, Australia
Â© 2020 Association for Computing Machinery.
ACM ISBN 978-1-4503-6768-4/20/09...$15.00
https://doi.org/10.1145/3324884.3416624Prioritization), to prioritizing incidents based on a large amount of
historical incident data. More specifically, we design an attention-
basedConvolutionalNeuralNetwork(CNN)tolearnaprediction
model to identify incidental incidents. We then prioritize all in-
cidents by ranking the predicted probabilities of incidents being
incidental.WeevaluatetheperformanceofDeepIPusingreal-worldincidentdata.TheexperimentalresultsshowthatDeepIPeffectively
prioritizes incidents by identifying incidental incidents and signifi-
cantly outperforms all the compared approaches. For example, the
AUC of DeepIP achieves 0.808, while that of the best compared
approach is only 0.624 on average.
CCS CONCEPTS
â€¢Software and its engineering â†’Maintaining software.
KEYWORDS
Incidents, Online Service Systems, Prioritization
ACM Reference Format:
Junjie Chen, Shu Zhang, Xiaoting He, Qingwei Lin, Hongyu Zhang, Dan
Hao, Yu Kang, Feng Gao, Zhangwei Xu, Yingnong Dang, and Dongmei
Zhang. 2020. How Incidentalare the Incidents? Characterizing and Priori-
tizing Incidents for Large-Scale Online Service Systems. In 35th IEEE/ACM
International Conference on Automated Software Engineering (ASE â€™20), Sep-
tember 21â€“25, 2020, Virtual Event, Australia. ACM, New York, NY, USA,
12 pages. https://doi.org/10.1145/3324884.3416624
1 INTRODUCTION
In recent years, online service systems, such as Microsoft Azure
and Office 365, have been widely used by millions of users around
3732020 35th IEEE/ACM International Conference on Automated Software Engineering (ASE)
ASE â€™20, September 21â€“25, 2020, Virtual Event, Australia J. Chen et al.
the world. To assure their quality, practitioners put dedicated ef-
forts[10â€“12,26,29,35,36,46,53â€“55],butsuchonlineservicesys-
tems still encounter many incidents (i.e., unplanned interruptions
and outages). These incidents can decrease user satisfaction or
cause serious economic loss. For example, the one-hour downtime
forAmazon.com on Prime Day in 2018 (its biggest sale event of
the year) caused the loss of up to $100 million [ 2]. Therefore, high
availability and reliability are essential to online service systems.
Once an incident occurs to an online service system, it needs to
be mitigated as soon as possible so as to reduce the loss caused by
the incident [ 10,29]. However, an online service system is quite
complex,i.e.,involvingmanycomponentssuchashardware,virtual
machines,network,anddatabase,andinthemeanwhileallthese
components can lead to incidents in the daily operation of the
system. Therefore, incidents tend to occur frequently in practice.
Moreover, the number of engineers, who are responsible to deal
with incidents, and computing resources is limited, and the cost
spent on incident management is considerable. Therefore, it is
scarcelypossibletomitigateeveryincidenttimely.Toreducethe
impactsofincidentsasmuchaspossible,intuitively,oneofthemost
cost-effective solutions is to deal with more important incidents
earlier. That is, it is necessary to prioritize incidents for engineersto optimize the incident management process.
To achieve the goal of incident prioritization, we first need to
understandwhathigh-priorityincidentsandlow-priorityincidents
are. However, to date, there lack extensive studies on incidents.
Therefore, we perform the first empirical analysis for incidents of
18 real-world online service systems in Microsoft, including many
worldwidepopularsystems.Foreachsystem,wecollectincident
dataoverasix-monthperiod.Indeed,therearealargenumberof
incidents reported over a short period. For example, for Service
Xtherearenearly3,000incidentspertimeunit1.However, many
of them actually are not important, i.e., they do not really affectcustomers and engineers will not fixthem with a high priority
afterthey manually find the reasons why the incidents occur. In
thispaper,wecalltheseincidents incidentalincidents.Incontrast,
we call the remaining incidents essential incidents.
More specifically, since essential incidents can be caused by a
variety of factors (e.g., various source code bugs and hardware fail-
ures),itisdifficulttocharacterizethemthoroughly.Here,weaim
toprioritizeincidentsfromtheoppositedirection,i.e.,identifying
incidentalincidentsandthenputtingthemintheend.Therefore,
we conduct a qualitative analysis to characterize incidental inci-
dents.Wefindthattheincidentsfallintoseveralcategories.Also,
we conduct a quantitative analysis to investigate the impacts of
incidental incidents. We find that for 15 out of 18 studied systems,
the percentage of incidental incidents is more than 30% and thepercentage of maintenance time spent on incidental incidents is
alsomorethan30%!Theresultsdemonstratethatalargenumber
of engineersâ€™ efforts were spent on incidental incidents, which can
largely delay the mitigation of really important incidents. Those
also empirically motivate the necessity of incident prioritization.
Further,inthispaperweproposeadeep-learningbasedapproach,
calledDeepIP(Deeplearningbased IncidentPrioritization),topri-
oritizing incidents by identifying incidental incidents based on a
1Due to the company policy, we hide the time unit and service name.large amount of historical incident data. In particular, there aretwo main challenges in the problem: 1) which features are help-ful to identify incidental incidents; 2) how to effectively utilizethese features to identify incidental incidents. For the first chal-
lenge, our empirical study provides some guidelines on effective
featuresandhelpsusidentifythreetypesoffeatures,i.e.,textual
descriptions (i.e.,title and summaryof an incidentreport), special
terms (e.g., API names and component names occurring in an inci-
dentreport),andincident-occurringenvironmentinformation(e.g.,
incident-occurring device). To overcome the second challenge, we
drawsupportfromdeeplearning.Sincethefeaturesweusedaremainly textual information, deep learning can achieve semanticunderstanding of natural-language descriptions and outperformtraditional machine learning algorithms, as demonstrated by ex-
isting studies [
10,16]. More specifically, we design a CNN-based
deepneuralnetworkandincorporateanattentionmechanism.The
incidentalincidentscanbepredictedbytheattention-basedCNN
model. Then, we prioritize all the incidents based on the ascend-
ingorder ofthepredicted probabilitiesofbeing incidental.Inthis
way,engineers canoptimizetheincidentmanagement processby
handling the incidents ranked higher first.
ToinvestigatetheeffectivenessofourapproachDeepIP,wecon-
ductanextensiveevaluationusingreal-worldincidentdatafrom
Microsoft (the same data as the one used in the empirical study).
TheresultsdemonstratethatDeepIPisabletoeffectivelyandeffi-
cientlyprioritizeincidentsforlarge-scaleonlineservicesystems,
andsignificantlyoutperformallthecomparedapproachesinthe
areaofsoftwarebugseverityprediction2.Forexample,theaverage
AUC(measuringtheaccuracythatessentialincidentsareranked
higher than incidental incidents) of DeepIP is 0.808, while that
of the best compared approach is just 0.624. The results demon-strate that as the first attempt to solve the practical problem of
incidentprioritization,ourdeep-learningbasedapproachDeepIP
is indeed promising. Our study results also show that each type of
features(i.e.,specialtermsandincident-occurringenvironments)
cansignificantlyimprovetheeffectivenessofDeepIP,confirming
the contributions of each of them. In particular, the practical value
of DeepIP has been appreciated by engineers in Microsoft.
The major contributions of the paper are as follows:
â€¢Weperformthefirstlarge-scaleempiricalstudyonincidents
of 18real-world onlineservice systems,characterizing inci-
dents qualitatively and quantitatively.
â€¢We propose the first approach to prioritizing incidents byidentifying incidental incidents, in order to optimize the
incident management process.
â€¢We conduct an extensive study to evaluate the performance
of DeepIP based on real-world incident data of 18 large-
scaleonlineservicesystems.OurresultsshowthatDeepIP
significantly outperforms all the compared approaches.
2 BACKGROUND
In this section, we introduce the background of incidents and inci-
dent management (IcM) for online service systems in practice.
2Since there is no existing incident prioritization approach for online service systems,
we adapt the typical approaches of traditional software bug severity prediction for
comparison in our study.
374How Incidental are the Incidents? ASE â€™20, September 21â€“25, 2020, Virtual Event, Australia
Online  Service 
SystemAuto 
alert
Manual
reportIcM System
TeamTeamIncident 
Reporting
Calling
Incident 
TriageIncident Mitigation
Incident ResolutionIncident status 
update
OCEs
Figure 1: Workflow of Incident Management
Figure1showstheworkflowofIcMforlarge-scaleonlineservice
systemsinMicrosoft.TheIcMconsistsoffourstages.Thefirststage
isincident reporting. In an online service system, a large number of
monitorsareusedtowatchforsomekeyperformanceindicators
(e.g., latency and network status). Most incidents are actually re-
portedbyvariousmonitorsautomatically,whichisdifferentfrom
the manualreporting of traditionalsoftware bugs. Inparticular,a
monitor automatically reports an incident to the IcM system when
some predefined anomalous conditions are met. Besides, engineers
could observe incidents during their daily operation, and thus they
couldalsoreportincidentsmanuallytotheIcMsystem.Afteran
incident is reported to the IcM system, the IcM system first makes
a phone call to a set of On-Call Engineers (OCEs) to trigger the
investigation process of the incident. Ideally, the OCEs can directly
identify the root cause and then mitigate the incident based on the
information in the incident report. However, in most cases they
cannotfindtherootcausewithinashorttime.Therefore,theOCEs
have to assign the incident to a team that they think is the most
suitable to handle it, which is the second stage incident triage. The
third stage is incident mitigation. When the incident is assigned
to the correct team, the engineers in the team begin to diagnose
the problem and then take all necessary actions to mitigate the
incident (e.g., reboot the server or replace failure hardware). After
mitigation,theengineersfurtheranalyzetheunderlyingrootcause
of the incident through offline postmortem analysis, and finally
completely resolve the incident, which is the last stage incident res-
olution. Moreover,the engineersupdate thestatus (e.g.,mitigated
or resolved) of an incident in the IcM system.
Incident mitigation should be timely since long Time To Miti-
gate (TTM) could leadto poor serviceavailability andcause huge
financial loss. However, it is often costly for engineers to manu-ally mitigate an incident due to the complexity and scale of thesystem.Consideringthelargenumberofincidentsaswellasthe
limited number of engineers and computing resources, it is essen-
tial to handle more important incidents earlier. However, to our
best knowledge, none of existing work has studied the priority and
influence of incidents before. Therefore, in this paper, we conduct
the first extensive study to characterize incidents (presented in
Section3),andfurtherproposeaneffectiveapproachtoprioritizing
incidents to optimize the incident management process (presented
in Section 4).3 AN EMPIRICAL STUDY OF INCIDENTS
Tobetterunderstandthepriorityandinfluenceofincidentsforlarge-
scaleonlineservicesystems,weconductthefirstextensivestudy
onreal-worldincidents.AspresentedinSection1,inthisstudy,weaim to understand the characteristics of incidental incidents. Based
ontheidentificationoftheincidentalincidents,wecanprioritize
incidentsbyputtingtheincidentalincidentsintheend.Here,we
target at the following research questions:
â€¢RQ1: Which incidents are incidental in large-scale online ser-
vice systems?
â€¢RQ2: What is the percentage of incidental incidents?
â€¢RQ3: What is the cost spent on incidental incidents?
â€¢RQ4:Isthecurrentincidentmanagementpracticegoodenough?
RQ1 is qualitative analysis to investigate â€œwhat are incidental
incidentsâ€,whiletheotherRQsarequantitativeanalysistostudy
incidentalincidentsintermsofnumberandcost.Inparticular,RQ4
aimstoexplorewhetherthecurrentpracticeofincidentmanage-
ment is sufficiently good from the view of incident priority.
3.1 Subjects
We used 18 large-scale online service systems in Microsoft as sub-
jects. These subjects include many worldwide popular products
andareusedbymillionsofusersworldwide.Allthe18systemsare
in different application areas and developed by different product
groups, indicating the diversity of subjects. For each online service
system, we collected real incident data over a six-month period.
The size of all the collected incident reports is up to 4.2GB. The
total number of monitors used to monitor these systems is more
than 80K. Due to the policy of Microsoft, we hid some details such
asthespecifictimeperiodforincidentcollectionandthespecific
number of collected incidents.
3.2 RQ1: Qualitative Analysis
After diagnosing an incident, engineers tend to manually record
whether the incident needs to be fixed with a high priority, and
give a simple explanation for the incident in the IcM system. If the
incidentindeedneedstobefixedwithahighpriority(i.e.,itisan
essential incident), engineers also record the fixing steps. That alsomeans,althoughanincidentisanincidentalincident,engineersstill
have to spend time and resources on diagnosing the reason why it
occurs,and finallyfind thatit actuallyis an incidentalone. With
the help (i.e., manual recording) of engineers, incidental incidents
can bedivided into sixcategories: by design, customer error, wonâ€™t
fix,unable to reproduce, transient, and false alarm. We analyze each
category of incidental incidents in the following.
3.2.1 By Design. The incidents belonging to this category are pro-
ducedintentionally,anddonotneedtobedealtwith.Thiscategory
of incidents tends to have the purpose of testing. One of such in-cidents is shown in Example 1. According to the title (one line
description)oftheincidentreportandtheexplanationgivenbythe
engineerswhodiagnosedtheincident,obviously,thisincidentis
anintentionalincidentfortesting ODataAPI.Theoccurrencesof
this category of incidents are as expected, and thus they should be
assigned low priority in practice.
375ASE â€™20, September 21â€“25, 2020, Virtual Event, Australia J. Chen et al.
Example 1:
Incident: TestIncident2018-10-23T00:06:36Z.
Explanation: OData API Test.
3.2.2 Customer Error. This category of incidents is caused by cus-
tomer errors rather than the problems of online service systems.
Inotherwords,customersmisusethesystemsorincorrectlycon-
figure the systems, causing the occurrences of the incidents. For
example, as shown in Example 2, the incident is that sending emailnotificationfailed.However,thecauseforthisincidentwasthattheinboxofthecustomerwasfull.Similarly,inExample3,thecausefor
the incident â€œbadge preview missing at DSMâ€ was that the DYMO
driverwasnotcorrectlyinstalledbycustomers.Thiscategoryof
incidents canbe directly handledby the technicalsupporters, and
thus do not need to notify engineers.
Example 2:
Incident: Failed to send email notification on Tenant:
022d9fca-60a3-4aac-9a90-c18e51ac527e at Frequency: Daily.
Explanation: Inbox is full - insufficient storage error.
Example 3:
Incident: Badge Preview Missing at DSM.
Explanation :CustomerinstalledDYMOdriverandresolved
the issue.
3.2.3 Wonâ€™t Fix. The incidents belonging to this category are real
incidents,buttheytendtooccuratthepartsthatareout-of-date
(i.e.,notmaintainedanymore).Therefore,itisnotworthtakingthe
engineersâ€™ efforts to solve them. As shownin Example 4, engineers
think that it is not necessary to solve the incident â€œnode service
stuck in crash loopâ€, since the occurring part of the incident has
been deprecated.
Example 4:
Incident: Node service stuck in crash loop on BR1-NNS207.
Explanation: Deprecated fabric.
3.2.4 UnabletoReproduce. Theincidentsbelongingtothiscate-
gory are not able to be reproduced during diagnosis. In this case, it
ishardtosaywhethertheyarerealincidents,anditisscarcelypos-
sibleto checkwhether wecan solvethemsuccessfully.Therefore,
even if the incidents are reported, they have to be ignored.
3.2.5 Transient. This category of incidents is real things but they
can be automatically recovered. These incidents tend to be caused
by other operations/factors. When these operations/factors are
corrected/eliminated, the incidents can be automatically resolved.
Therefore, it should be low-priority for engineers to look into
these incidents. For example, as shown in Example 5, the test
EndToEndDataPushAndPull was not executed during a period of
time. Actually, it was caused by another factor, which is that the
correspondingmachineshavenotbeencompletelyrebootedatthat
time. When the machines finished rebooting, the test would run.
Similarly,inExample6,theincidentoccurred,sincetheâ€œmsfForestâ€was still at the stage of deployment. After the deployment finished,
the incident was automatically resolved accordingly.
Example 5:
Incident: WestCentralUS: GIP2 test EndToEndDataPushAnd-
Pull didnâ€™t execute at least once during the last 14:00:00 min-
utes.
Explanation :Theargowasdownbecausethemachinestook
longtimetoreboot(ntdevtexaspasswordwaschanged).Argo
is up now and tests are running.
Example 6:
Incident: RED ALERT: System Level Issue Detected in msf
Forest.
Explanation : Transient issue due to deployments. There are
no usersin thisforest andserver is notmember ofany DAG.
3.2.6 False Alarm. The incidents belonging to this category are
actually not real incidents. They tend to be caused by the problems
of monitors. For example, as shown in Example 73, this incident
is a false alarm, and the real cause lay in the monitor reportingtheincident.Morespecifically,thedatawasupdatedeverythree
minutesbutthemonitorcheckedeveryoneminute,andthusthe
monitor reported the incident â€œrefresh time exceeded thresholdâ€.However, the incident was actually due to the sensitive monitor
(improper threshold). That is, this category of incidents should not
be investigated by engineers, since they are not real incidents.
Example 7:
Incident: DS002 (MDM): Refresh time of delta store "Stor-
ageAccountName":"xtlcsuse", " âˆ—âˆ—TableName":"DeltaStore",
"Name":"envindex" exceeded threshold.
Explanation : MDM was configured for this at a threshold
of 1 minute but the new code is at 3 minutes. We did not
getenough3minuteoutagestotriggerthis,sothisisafalse
alarm.
3.3 RQ2: Percentage of Incidental Incidents
We investigated the percentage of incidental incidents in online
service systems, whose results are shown in Figure 2. In this figure,thevalueaboveeachbarrepresentsthepercentageofincidentalin-cidentsamongalltheincidentsforthecorrespondingsubject.From
thisfigure,wefindthatthepercentageofincidentalincidentsfor
all the 18 studied systems is significant, which ranges from 11.92%
to 71.43%. The average percentage of them is up to 50.32%. That is,
morethan halfof incidentsare actuallyincidental, indicatingthat
a great deal of engineersâ€™ efforts were spent on these low-priority
incidentsduring historical diagnosis.Therefore,itisquite neces-
sary tounderstand andthen prioritizeincidents foronline service
systems.Inparticular,weinvestigatedwhy S9hasthesmallestrate,
and found that the number of monitors used for checking S9is the
smallest,whichmayleadtomanyincidentsthatcannotbereported
and hard to capture complex interactions among components.
3We useâˆ—âˆ—to replace some words due to the company policy.
376How Incidental are the Incidents? ASE â€™20, September 21â€“25, 2020, Virtual Event, Australia
34.0338.0971.05
61.61
16.955.9162.35
38.17
11.9229.345.359.5662.4 62.4165.8370.25
49.2771.43
0204060
S1 S2 S3 S4 S5 S6 S7 S8 S9 S10 S11 S12 S13 S14 S15 S16 S17 S18Percentage of trivial incidents (%)/g5/g9/g16/g7/g9/g14/g18/g6/g11/g9/g1/g15/g10/g1/g12/g14/g7/g12/g8/g9/g14/g18/g6/g13/g1/g12/g14/g7/g12/g8/g9/g14/g18/g17/g1/g3/g2/g4
Figure 2: Percentage of incidental incidents
/g23/g34/g33/g3/g38/g1/g19/g30/g40
/g6/g6/g4/g5/g8/g2
/g21/g36/g24/g33/g37/g30/g28/g33/g38
/g10/g9/g4/g9/g11/g2/g19/g24/g31/g37/g28/g1/g14/g31/g24/g36/g32
/g6/g11/g4/g6/g13/g2/g15/g41/g1/g17/g28/g37/g30/g29/g33
/g6/g7/g4/g12/g11/g2/g22/g33/g24/g25/g31/g28/g1/g21/g34/g1/g20/g28/g35/g36/g34/g27/g39/g26/g28
/g8/g4/g6/g12/g2/g16/g39/g37/g38/g34/g32/g28/g36/g1/g18/g36/g36/g34/g36
/g7/g4/g7/g12/g2
Figure3: Percentageofeachcategoryofincidentalincidents
We further investigated the percentage of each category of inci-
dentalincidents,whoseresultsareshowninFigure3.Thenumbers
inthisfigurerepresenttheaveragepercentageofthecorrespond-
ingcategoryofincidentalincidentsonthe18studiedsystems.In
thisfigure,wefindthatthepercentagesofâ€œCustomerErrorâ€and
â€œUnable to Reproduceâ€ are small, while the percentage of â€œTran-
sientâ€ is large. The large percentage (i.e., 54.46%) for â€œTransientâ€
is asexpected, since thereexist many interactionsamong various
components in a large-scale online service system. When one com-
ponentis abnormal,the componentsinteractingwith itare likely
toreportincidentsthatarecausedbythefirstabnormalone,which
may lead to many â€œTransientâ€ incidents.
3.4 RQ3: Effort Spent on Incidental Incidents
We explored the effort spent on incidental incidents in terms of
TTR(TimetoResolve).TTRreferstothetimeperiodfromincident
creationtoincidentresolution.Figure4showsthepercentageof
TTRspentonincidentalincidentsforeachonlineservicesystem.
Fromthisfigure,wecanseethatthepercentageofTTRspenton
incidentalincidentsissignificant,rangingfrom10.57%to76.72%.
Theaveragepercentageisupto55.05%.Thatis,thecostspenton
incidentalincidentsisalmostthesameasthatspentonessential
incidentsintermsofTTR,whichmaydelaytheresolutionofessen-
tial incidents and thus result in greater economic loss. We also find
that for 15 out of18 studied systems, the percentage of resolution
time spent on incidental incidents is more than 30%. These results
further motivate the necessity of understanding and prioritizing
incidents for large-scale online service systems.
WefurtherexploredtheTTRspentoneachcategoryofincidental
incidents. We first calculated the average TTR of each category53.4655.9273.8
69.64
10.5738.97
26.0670.66
19.5643.81
34.2166.9671.45
63.3374.07 74.15
67.5976.72
020406080
S1 S2 S3 S4 S5 S6 S7 S8 S9 S10 S11 S12 S13 S14 S15 S16 S17 S18Percentage of TTM spent on trivial incidents (%)/g5/g11/g19/g9/g11/g16/g21/g8/g13/g11/g1/g17/g12/g1/g7/g7/g6 /g1/g20/g18/g11/g16/g21/g1/g17/g16/g1/g14/g16/g9/g14/g10/g11/g16 /g21/g8/g15/g1/g14/g16/g9/g14/g10/g11/g16 /g21/g20/g1/g3/g2/g4
Figure 4: Percentage of TTR spent on incidental incidents
G
GG
G
GG
GGGG
By Design Customer Error Won't Fix Unable To Reproduce Transient False AlarmAverage TTM/g2/g9/g6/g8/g5/g7/g6/g1 /g4/g4/g3
Figure 5: Distribution of TTR
ofincidentalincidentsforeachstudiedsystem,andthenshowed
the average TTR distribution across all the subjects in Figure 5.
Due to the policy of Microsoft, we hid the specific time and its
unit. In Figure 5, the violin plots show the density of average TTR
at different values, and the box plots show the median and inter-
quartile ranges. Among all these categories of incidental incidents,
themedianofaverageTTRforâ€œUnabletoreproduceâ€isthelargest.
The reason could be that developers spent long time trying to
reproduce these incidents but failed.
3.5 RQ4: Investigation of Incident
Management Practice
The current incident management practice in Microsoft is that
engineersinvestigatethereportedincidentsbasedonthenumber
of potentially impacted customers, which is estimated accordingto the region/cluster where the incidents occurred. Based on the
number of potentially impacted customers, there are five levels of
incidentsinMicrosoft,i.e.,0 âˆ¼4,where0referstothehighestlevel
and4referstothelowestlevels.However,inthecurrentpractice
engineers may still investigate some incidental incidents first since
these incidents have larger estimated impacts, which is harmfulto incident management of systems. Therefore, we explored thedistribution of incidents at each level to investigate whether the
current practice is good enough.
Table 1 shows the distribution of incidents at each level. In this
table, Rows 2-7 present the distribution of each category of inci-
dental incidents at each level across all the studied systems. Rows
8and9summarizetheoveralldistributionofincidentalincidents
and essential incidents at each level. From this table, we can see
377ASE â€™20, September 21â€“25, 2020, Virtual Event, Australia J. Chen et al.
Table 1: Distribution of incidents at each level
Severity 0 (%) 1 (%) 2 (%) 3 (%) 4 (%)
By Design 13.52 5.06 3.54 4.70 6.77
Customer Error 0.00 0.45 0.41 0.67 1.73
Wonâ€™t Fix 2.96 6.43 6.74 3.20 6.41
Unable To Reproduce 0.00 0.83 2.28 0.64 2.47
Transient 30.37 9.25 23.04 21.62 21.36
False Alarm 11.11 3.75 6.22 5.55 8.84
Incidental Incidents 57.96 25.77 42.23 36.38 45.57
Essential Incidents 42.04 74.23 57.77 63.62 52.43
that at each level, there are both incidental incidents and essential
incidents, and their rates are relatively similar. That is, there are
indeedmanyincidentalincidentsthatareassignedwithahigher
levelthanactualessentialincidents.Evenatthehighestlevel,the
percentageofincidentalincidents(i.e.,57.96%)islargerthanthat
of essential incidents (i.e., 42.04%). That indicates, many inciden-
tal incidents wereactually investigated preferentially,causing the
waste of engineersâ€™ efforts. Therefore, the current incident man-
agementpracticeshouldbefurtherimproved,andonepromising
directionistoprioritizeincidentsbyidentifyingessentialincidents
and incidental incidents.
4 INCIDENT PRIORITIZATION
Duringtheincidentmanagementprocess,ahugeamountoflabeled
incident data is accumulated. Each incident is reported along with
variousinformationsuchasthesymptomdescriptionandoccurring
environment. The abundance of data provides an opportunity to
automaticallyidentifywhetheranincidentisincidentaloressential.
Here, we treat the problem of identifying incidental incidents as a
supervised classification problem, which can produce a probability
ofanincidentbeingincidental.Then,allincidentscanbeprioritized
based on the ascending order of the predicted probabilities. In this
way,engineerscanhandletheincidentsbasedontheprioritiesand
the incident management process could be improved.
Here,weproposeadeep-learningbasedapproach,called DeepIP,
toprioritizingincidentsbyidentifyingincidentalincidents.Figure6
shows the overview of DeepIP. We identify three types of features
tohelppredictincidentalincidentsbasedontheguidelinesacquired
fromtheempiricalstudy(Section4.1).Also,wedesignaCNN-based
deepneuralnetworkandincorporateanattentionmechanismto
effectively utilize these features for the identification of incidental
incidents (Section 4.2). Here, we draw support from deep learning,
sincethefeaturesweusedaremainlytextualinformation,anddeep
learningcan achievesemantic understandingof natural-language
descriptions and has been demonstrated to outperform traditional
machine learning algorithms [10, 16].
4.1 Feature Identification
Whenanincidentisreported,itisprovidedwiththetextualdescrip-
tion about the symptom, i.e., the title and summary of the incident
report. The textual description is the core information about an
incident,whichcandirectlyreflecttheincidenttosomedegree,andthusitcouldbehelpfultodistinguishwhethertheincidentisessen-tialorincidental.Besides,fromSection3andtheexistingstudy[
10],
many incidents are actually correlated in a system. For example,
anincidentinonecomponentofasystemcouldcauseaseriesofincidentsinothercomponentsofthesystem.Also,anincidentmay
becontinuouslyreportedseveraltimes,sincemonitorscheckthe
statusofasystemregularly.â€œTransientâ€incidentsarealsorelatedto
incident correlations. For ease of presentation, we call the incident
to bepredicted target incident and theincidents correlated with it
relevant incidents. The relevant incidents are helpful to predict the
target incident. Therefore, we consider the textual descriptions of
both the target incident and its relevant incidents in DeepIP. Astherelevantincidentstendtobereportedatclosetime[
10]with
the target incident, we identify them by setting atime window andcollectingtheincidentswhosereportingtimeiswithinthewindowandbeforethetimeofthetargetincident.Insummary, thefirsttype
offeaturesusedinDeepIPisthetextualdescriptionsinboththetarget
incident report and its relevant incident reports.
Based on the observations in Section 3, most of incident reports
includespecialterms,suchasAPInamesandcomponentnames,
andmanyofthemarehelpfultoidentifyincidentalincidents.For
example,asshowninExample4,â€œBR1-NNS207â€wasdeprecated,
and thus this incident did not need to resolve, belonging to â€œWonâ€™t
Fixâ€.Here,â€œBR1-NNS207â€isaspecialterm.AsshowninExample
6,â€œmsfForestâ€isalsoaspecialterm.Thisincidentisâ€œTransientâ€,
whichoccurred sinceâ€œmsfForestâ€ wasstillat thestageof deploy-
ment. Therefore, the second type of features used in DeepIP is special
termsinthetargetincidentreport.Actually,specialtermshavebeenincludedintextualdescriptionsinincidentreports(thefirsttypeof
features). However, during the learning of textual descriptions, the
knowledgeofspecialtermsishardtolearnsincethefrequencies
of special terms are much smaller than those of other words in
textualdescriptions. Therefore, toeffectivelylearn theknowledge
ofspecialterms,weextractspecialtermsasakindoffeaturesbased
on their frequencies.
According to Section 3, the incident-occurring environments
are also related to the identification of incidental incidents. For
example,theincidentsbelongingtoâ€œFalseAlarmâ€tendtobecaused
by the problems of monitors, and thus the monitor ID reporting an
incidentis helpfultodistinguish whethertheincident isessential
or incidental. Therefore, the third type of features used in DeepIP
is the incident-occurring environmental information. In particular,
we consider the following environmental information: monitor ID,
incident-occurring device, and incident-reporting type.
4.2 Design of Deep Neural Network
Toeffectivelyutilizethethreetypesoffeaturestoidentifyincidental
incidents, we design a deep neural network for DeepIP. In the
following, we first present feature embedding in Section 4.2.1, thenintroduceattention-basedtextencodinginSection4.2.2,andfinally
present incidental incident prediction in Section 4.2.3.
4.2.1 Feature Embedding. Since the values of the second and third
typesoffeaturesusedinDeepIPareafinitesetofdiscretevalues,
weconductfeatureembeddingforthem.Onesimplemethodisto
express all these discrete values as one-hot vectors [25]. However,
in this way, the dimension of an one-hot vector may be very high,
and it ignores the possible relations among different feature values.
To overcome these problems, we adopt representation learning [ 7]
to embed each feature value into a vector. Representation learning
is able to embed a value to a fixed-dimension vector, and gradually
378How Incidental are the Incidents? ASE â€™20, September 21â€“25, 2020, Virtual Event, Australia
/g38/g59/g49/g55/g50/g51/g59/g64/g1/g37/g55/g63/g64/g60/g62/g69/g1
/g44/g30/g42/g70/g27/g1/g1/g44/g47/g62/g53/g51/g64/g1/g38/g59/g49/g55/g50/g51/g59/g64
/g1/g1
/g42/g71/g27/g1/g42/g51/g57/g51/g66/g47/g59/g64/g1/g38/g59/g49/g55/g50/g51/g59/g64/g42/g72/g27/g1/g42/g51/g57/g51/g66/g47/g59/g64/g1/g38/g59/g49/g55/g50/g51/g59/g64/g42/g73/g27/g1/g42/g51/g57/g51/g66/g47/g59/g64/g1/g38/g59/g49/g55/g50/g51/g59/g64/g42/g74/g27/g1/g42/g51/g57/g51/g66/g47/g59/g64/g1/g38/g59/g49/g55/g50/g51/g59/g64/g44/g51/g68/g65/g47/g57/g1/g34/g51/g63/g49/g62/g55/g61/g64/g55/g60/g59/g63
/g44/g47/g62/g53/g51/g64/g1/g38/g59/g49/g55/g50/g51/g59/g64
/g35/g59/g66/g55/g62/g60/g59/g58/g51/g59/g64/g63
/g39/g60/g59/g55/g64/g60/g62/g38/g50/g1/g1
/g34/g51/g66/g55/g49/g51/g40/g47/g58/g51/g1
/g38/g59/g49/g55/g50/g51/g59/g64/g44/g69/g61/g51/g1
/g28/g42/g51/g57/g51/g66/g47/g59/g64/g1/g38/g59/g49/g55/g50/g51/g59/g64/g1/g45/g51/g49/g64/g60/g62/g63
/g44/g47/g62/g53/g51/g64/g1/g38/g59/g49/g55/g50/g51/g59/g64/g1/g45/g51/g49/g64/g60/g62
/g35/g59/g66/g55/g62/g60/g59/g58/g51/g59/g64/g1/g45/g51/g49/g64/g60/g62/g1/g35
/g39/g60/g59/g55/g64/g60/g62/g38/g50/g31/g1/g45/g51/g49/g64/g60/g62
/g34/g51/g66/g55/g49/g51/g40/g47/g58/g51/g27/g1/g45/g51/g49/g64/g60/g62
/g38/g59/g49/g55/g50/g51/g59/g64/g44/g69/g61/g51/g27/g1/g45/g51/g49/g64/g60/g62
/g28/g44/g30/g42/g70/g27/g6/g12/g17/g8/g18/g23/g15/g10/g1/g7/g12/g10/g23/g19/g21/g1
/g42/g71/g27/g6/g12/g17/g8/g18/g23/g15/g10/g1/g7/g12/g10/g23/g19/g21/g42/g72/g27/g6/g12/g17/g8/g18/g23/g15/g10/g1/g7/g12/g10/g23/g19/g21/g42/g74/g27/g6/g12/g17/g8/g18/g23/g15/g10/g1/g7/g12/g10/g23/g19/g21
/g44/g51/g68/g64/g1/g35/g59/g49/g60/g50/g55/g59/g53/g32/g64/g64/g51/g59/g64/g55/g60/g59
/g46/g51/g55/g53/g54/g64
/g46/g74
/g46/g72
/g46/g71
/g46/g70/g1/g2/g23/g23/g12/g18/g23/g15/g19/g18/g29/g9/g8/g22/g12/g11/g1/g1
/g5/g12/g16/g12/g25/g8/g18/g23/g1
/g7/g12/g10/g23/g19/g21/g1/g3
/g36/g55/g59/g47/g57/g1/g37/g55/g50/g50/g51/g59/g1/g43/g64/g47/g64/g51
/g33/g57/g47/g63/g63/g55/g52/g55/g49/g47/g64/g55/g60/g59/g1
/g41/g62/g60/g48/g47/g48/g55/g57/g55/g64/g55/g51/g63/g43/g61/g51/g49/g55/g47/g57/g1/g44/g51/g62/g58/g63 /g43/g61/g51/g49/g55/g47/g57/g1/g44/g51/g62/g58/g1/g45/g51/g49/g64/g60/g62/g1/g41/g42/g51/g57/g51/g66/g47/g59/g64/g1/g38/g59/g49/g55/g50/g51/g59/g64/g63
/g36/g51/g47/g64/g65/g62/g51/g1/g35/g58/g48/g51/g50/g50/g55/g59/g53/g44/g51/g68/g65/g47/g57/g1/g34/g51/g63/g49/g62/g55/g61/g64/g55/g60/g59/g1/g45/g51/g49/g64/g60/g62/g63 /g33/g40/g40/g76/g48/g47/g63/g51/g50/g1/g39/g60/g50/g51/g57
/g33/g60/g59/g66/g60/g57/g65/g64/g55/g60/g59/g47/g57/g1/g57/g47/g69/g51/g62/g1/g67/g55/g64/g54/g1
/g58/g65/g57/g64/g55/g61/g57/g51/g1/g52/g55/g57/g64/g51/g62/g1/g67/g55/g50/g64/g54/g63/g1
/g47/g59/g50/g1/g52/g51/g47/g64/g65/g62/g51/g1/g58/g47/g61/g63/g39/g47/g68/g1
/g60/g66/g51/g62/g76/g64/g55/g58/g51/g1
/g20/g19/g19/g16/g15/g18/g14/g75
/g33/g60/g60/g57/g55/g59/g53
/g6/g26/g22/g23/g12/g17
/g15/g22
/g4/g8/g16/g13/g24/g18/g10/g23/g15/g19/g18
/g8/g18/g11
/g15/g17/g20/g8/g10/g23/g12/g11
/g4/g24/g16/g23/g15/g20/g16/g12
/g28
/g42/g47/g59/g56/g55/g59/g53/g1
/g38/g59/g49/g55/g50/g51/g59/g64/g63
Figure 6: Overview of DeepIP
updates the vector during the training process. In this way, each
of these features is embedded into a fixed-dimension vector. More
specifically, we denote the ğ‘—ğ‘¡â„feature vector in the second types of
features as ğ‘ƒğ‘—={ğ‘ğ‘—1,ğ‘ğ‘—2,...,ğ‘ğ‘—ğ‘›}, and denote the ğ‘˜ğ‘¡â„feature vec-
torinthethird typesoffeaturesas ğ¸ğ‘˜={ğ‘’ğ‘˜1,ğ‘’ğ‘˜2,...,ğ‘’ğ‘˜ğ‘ },where
ğ‘›andğ‘ refer to the pre-defined fixed dimensions in representation
learning for the second and third types of features, respectively.
Here, we concatenate all the second types of feature vectors into a
vectorğ‘ƒ=ğ‘ƒ1âŠ•ğ‘ƒ2âŠ•...âŠ•ğ‘ƒğ‘¡,andconcatenateallthethirdtypesof
feature vectors into a vector ğ¸=ğ¸1âŠ•ğ¸2âŠ•...âŠ•ğ¸ğ‘Ÿ, whereğ‘¡is the
numberofthesecondtypeoffeaturevectors, ğ‘Ÿisthenumberofthe
third type of feature vectors, and âŠ•is the concatenation operator.
4.2.2 Attention-based Text Encoding. After acquiring the target in-
cidentanditsrelevantincidents,foreachoftheseincidents,DeepIP
first applies standard text mining method [ 8] to process the tex-
tual description (including tokenization, removing stop words, and
splitting).DeepIPthenembedseachwordinthetextualdescription
ofanincidentreportintoavectorbyusingawordvector,which
ispre-trained onhistoricalincident datausingthe FastTextalgo-
rithm [28]. In this way, the textual description of an incident is
transformed into a matrix, in which thenumber ofrows is equal to
the number of words in the textual description.
Then,DeepIPencodesthematrixforanincidentintoavector.
WeuseaCNNbasedneural-languagemodelratherthantraditional
languagemodelstoconductencoding,sincetheformerhasbeen
demonstratedtobeabletoencodemorecomplexpatternsandfocusonword-levelknowledgetoachievebetterperformance[
27,31].In
particular, we adopt the simple single-layer CNN-based model [ 31]
forencoding,sinceithasbeendemonstratedthatsuchamodelcan
achieve better or comparable results and is easy for training andprediction [
31]. An overview of the CNN-based model is shown
in the left figure of Figure 6. More specifically, the CNN-based
model contains multiple 1D (1-dimension) convolution kernels andmax-over-timepooling.Byusingmultipleconvolutionkernelswithseveraldifferentwidths,severalfeaturemapsareproducedfromthe
matrix,wheremultipleconvolutionkernelswithdifferentwidths
cancapturethecorrelationamongdifferentnumbersofadjacent
words. Then, the max-over-time pooling is applied to produce a
vector for each feature map in order to extract the most important
words. Finally, all produced vectors are concatenated to generate avectorforanincident.Inthisway,thetextualdescriptionofthetar-
getincidentoreachoftherelevantincidentsisencodedintoavector.
Wedenotethevectorofthetargetincidentas ğ‘‡={ğ‘¡1,ğ‘¡2,...,ğ‘¡ğ‘š},
andthevectorofthe ğ‘–ğ‘¡â„relevantincidentas ğ‘…ğ‘–={ğ‘Ÿğ‘–1,ğ‘Ÿğ‘–2,...,ğ‘Ÿğ‘–ğ‘š},
whereğ‘šis the total number of convolution kernels.
Since different relevant incidents may have different degrees
ofcorrelationwiththetargetincident,itisnecessaryforDeepIPto learn these different degrees. The relevant incident that has astronger correlation with the target incident should be assigned
withalargerweight.Here,weintroduceanattentionmechanismto
automatically learn theweights of relevant incidents. Actually, it ispossiblethatalltheidentifiedrelevantincidentshavenocorrelation
with the target incident, and thus we also consider the correlation
of the target incident with itself when learning weights, so as to
avoid assigning weights to the irrelevant incidents in this case. For
ease of description, we call the vector of the target incident the0ğ‘¡â„
relevantincident anddenote ğ‘‡asğ‘…0={ğ‘Ÿ01,ğ‘Ÿ02,...,ğ‘Ÿ0ğ‘š}.Theinput
totheattentionmechanismisthevectorsofthetargetincidentand
its relevant incidents. The output is the attention-based relevant
vector integrated bythese vectors, denoted as ğ¶={ğ‘1,ğ‘2,...,ğ‘ğ‘š},
whereğ‘ğ‘–=/summationtext.1ğ‘
ğ‘—=0(ğ‘¤ğ‘—âˆ—ğ‘Ÿğ‘—ğ‘–)andğ‘¤ğ‘—isthelearnedweightofthe ğ‘—ğ‘¡â„
relevant incident. The calculation of ğ‘¤ğ‘—is shown as Formula 1.
ğ‘¤ğ‘—=ğ‘’ğ‘“(ğ‘…0,ğ‘…ğ‘—))
/summationtext.1ğ‘
ğ‘˜=0ğ‘’ğ‘“(ğ‘…0,ğ‘…ğ‘˜)(1)
where,ğ‘“(ğ´,ğµ)=ğ‘£Tğ‘¡ğ‘ğ‘›â„(ğ‘Šğ´ğ´+ğ‘Šğµğµ),ğ´andğµare two vectors,
andğ‘£,ğ‘Šğ´, andğ‘Šğµare parameters learned in the MLP [6].
4.2.3 Predicting Incidental Incidents. After acquiring the four vec-
tors (i.e., ğ‘‡,ğ¶,ğ‘ƒ, andğ¸), DeepIP first utilizes the four vectors to
constructthefinalhiddenstate.Here,DeepIPconcatenatesthefour
producedvectors ğ‘‡,ğ¶,ğ‘ƒ,andğ¸,i.e.,ğ‘‡âŠ•ğ¶âŠ•ğ‘ƒâŠ•ğ¸.Then,DeepIP
converts the final hidden state into a probability distribution of
labels by the last layer, i.e., the softmax layer. In particular, DeepIP
minimizesthelossbygradientdescentandgraduallyupdatesthe
weight of the network. The classification cross entropy loss is used
fortraining. Inthisway, theincidentalincidents canbepredicted,
andeachincident isassignedwitha probability ofbeingincidental.
At last, all the incidents are prioritized based on the ascend-
ing order of the predicted probabilities. The higher the incidents
379ASE â€™20, September 21â€“25, 2020, Virtual Event, Australia J. Chen et al.
are ranked, the larger the probabilities that the incidents are es-
sential incidents are. Engineers can then optimize their incident
management process by handling the incidents ranked higher first.
Moreover, since our attention mechanism is able to learn the corre-
lationdegreesofrelevantincidentswiththetargetincident,DeepIP
also recommends the most relevant incident together with the tar-
get incident, which could facilitate engineers to understand the
nature of the incident and diagnose it.
5 EVALUATION
ToinvestigatetheperformanceofDeepIP,weconductanextensive
studyusingreal-worldincidentdata.Weusethe18industrialonline
servicesystemsstudiedinSection3assubjects.Inparticular,we
usetheincidentdatafromthefirstfourmonthsasthetrainingdata,
and the incident data from the last two months as the testing data.
In the study, we address the following research questions:
â€¢RQ5:HowdoesDeepIPperformforreal-worldlarge-scale
online service systems?
â€¢RQ6:Does each type of features contribute to DeepIP?
5.1 Evaluation Design
5.1.1 Compared Approaches. Since our work is the first to pri-
oritizeincidentsbypredictingincidentalincidentsforonlineser-
vice systems, there is no direct comparative approach. For tra-
ditional software systems, there are several work on predicting
severity of bug reports, which can be adapted to prioritize inci-
dents [32,37,43,52]. Therefore, we select two typical bug-severity
prediction approaches as the comparative approaches in this study.
Both approaches are also based on textual descriptions of reports.
â€¢Menzies and Marcus [ 37], which applies the standard text
miningmethod[ 8]toprocesstextualdescriptionsinreports,
andthenusestf-idf(termfrequencyandinversedocument
frequency) [ 24] to transform the textual descriptions in a
bugreporttoavector.Finally,theirapproachusestherule
classifierbased onentropy andinformationgain topredict
bug severity [39].
â€¢Lamkanfi et al. [ 32], which applies the standard text mining
method to process textual descriptions. Then, this approach
counts token frequency and uses the Naive Bayes algorithm
to predict bug severity.
Whenadaptingtheseapproaches,theincidentseverityisinci-
dentalandessential,andalltheincidentsareprioritizedbasedon
the ascendant order of the predicted probabilities that incidents
areincidental,whicharegivenbythecorrespondingapproach.For
ease of presentation, in this paper we call the two approaches Rule
andBayesrespectively,based onthe way theyperform prediction.
Actually,we alsotriedtouse anotherstate-of-the-artapproach
to bug severity prediction [ 52] as a comparative approach. This
approach calculates the similarities between a new bug report and
historical bug reports using BM25F [ 41] and LDA [ 9]. However,
it cannot work well on incidental incident prediction since it is
very timecostly. Itstime complexity is ğ‘‚(ğ‘šğ‘›), whereğ‘šandğ‘›are
thenumberofincidentsintrainingandtestingdata,respectively.
For each instance in testing data, it has to calculate the similarities
with all incidents in training data, thus its cost is very considerable
duetothelargescaleoftheincidentdataforasysteminpractice.For example, we applied it to the system with the smallest number
of incidents in our study (i.e., S7). The time spent on prioritizing
incidentsintestingdataisupto3,330seconds,whilethetimere-
quiredbyDeepIP, Rule,and Bayesisonly2.54seconds,0.01seconds,
and 0.31 seconds as shown in Table 2. For larger datasets, the re-quired time could be much longer. Therefore, we did not include
this approach as a comparative approach in our study.
In RQ6, we evaluate the contribution of each type of features.
Here, we always keep the first type of features (i.e., textual descrip-
tions)since itis thecore informationaboutincidents. We remove
the second or third type of features and produce two variants ofDeepIP, denoted as DeepIP
ğ‘›ğ‘œğ‘ƒand DeepIP ğ‘›ğ‘œğ¸, respectively. We
then compare the performance of the three versions.
5.1.2 Implementations and Parameters. Since the implementations
of compared approaches are unavailable, we re-implemented them
following descriptions in the papers. For the involved machinelearning algorithms, we adopted the implementations providedby scikit-learn [
3]. For DeepIP, we implemented CNN based on
Apache MXNet [ 1], a scalable deep learning framework. For the
compared approaches, we used the same values of the parameters
as given in the corresponding papers. If a parameterâ€™s value is not
explicitly given in the paper, we used the default value provided
by the adopted tools. For the parameters in DeepIP, we determined
themthroughgridsearchandsetthesameparametersforallthe
studiedsystems.Morespecifically,wesettheparametersasfollows:theCNNusesthreesetsofconvolutionkernelswithdifferentwidths
(i.e., 3, 4, 5), each of which has 100 kernels, and the used epoch
is 20. The time window is set to be the time interval backwards
10incidentsfromthetargetincident.InSection6,wediscussthe
impact of the time window on DeepIP. Our study is conductedon Windows Server 2016 with 28-core Dual-Intel Xeon E5-2690CPU(2.60GHz), 512 GB memory, 64-bit operating system, and a
singleNVIDIATeslaK80GPUaccelerator.Wecannotreleasethe
incident data used in our study due to the policy of Microsoft, but
we release the source code for these approaches in the project
homepage: https://github.com/JunjieChen/DeepIP.
5.1.3 Metrics. In this study, we consider both effectiveness and
efficiency to measure the performance of DeepIP. We first measure
theeffectivenessofDeepIPinincidentprioritizationbyadopting
the widely-used AUCmetric [18], which measures the accuracy
that essential incidents are ranked higher than incidental incidents
in our context. That is, AUC can be viewed as a metric based on
pairwisecomparisonsbetweenclassificationsofthetwoclasses.Fol-
lowingexistingwork[ 13,14],supposingtheoutputprobabilitiesof
an approach on the essential incidents are {ğ‘¥1,ğ‘¥2,...,ğ‘¥ğ‘š}and the
output probabilities on the incidental incidents are {ğ‘¦1,ğ‘¦2,...,ğ‘¦ğ‘›},
the AUC is computed as Formula 2. Larger is better.
ğ´ğ‘ˆğ¶ =/summationtext.1ğ‘š
ğ‘–=1/summationtext.1ğ‘›
ğ‘—=11ğ‘¥ğ‘–>ğ‘¦ğ‘—
ğ‘šğ‘›(2)
Besides,sincetheidentificationofincidentalincidentsisthecore
ofDeepIP,wealsomeasuretheeffectivenessoftheclassification
of incidental and essential incidents. Here, we adopted the widely-
usedPrecision andRecallmetrics.Precision is computed byğ‘‡ğ‘ƒ
ğ‘‡ğ‘ƒ+ğ¹ğ‘ƒ,
whileRecalliscomputedbyğ‘‡ğ‘ƒ
ğ‘‡ğ‘ƒ+ğ¹ğ‘,whereğ‘‡ğ‘ƒ,ğ¹ğ‘ƒ,andğ¹ğ‘refer
tothenumberoftruepositives(TP),falsepositives(FP),andfalse
380How Incidental are the Incidents? ASE â€™20, September 21â€“25, 2020, Virtual Event, Australia
Table 2: Performance comparison among the DeepIP, Rule, and Bayesapproaches
SubAUC Precision Recall Training Time (s) Predicting Time (s)
DeepIP Rule Bayes DeepIP Rule Bayes DeepIP Rule Bayes DeepIP Rule Bayes DeepIP Rule Bayes
S1 0.9150.698 0.737 0.8080.705 0.738 0.8000.698 0.724 5586.71 1681.51 62.76 55.481.8219.05
S2 0.7650.605 0.510 0.6870.601 0.570 0.6790.601 0.510 16743.19 97.12756.16 130.51 0.12468.29
S3 0.8790.688 0.439 0.7940.693 0.445 0.6810.688 0.438 2482.99 42.80 30.59 14.920.0512.84
S4 0.8240.675 0.449 0.7210.686 0.408 0.7450.675 0.449 2254.65 29.28 26.19 17.040.0827.37
S5 0.8800.621 0.691 0.6420.535 0.549 0.7780.621 0.691 5482.05 4.77 3.20122.05 0.10 8.93
S6 0.8480.669 0.652 0.7920.675 0.657 0.7870.669 0.653 2597.75 9.78 6.06 7.860.02 2.82
S7 0.7210.584 0.516 0.7000.584 0.539 0.6930.584 0.515 657.72 3.37 0.84 2.540.01 0.31
S8 0.8090.630 0.577 0.7550.629 0.588 0.7560.630 0.565 3150.25 62.68122.30 7.070.0535.00
S9 0.8240.537 0.522 0.6560.524 0.510 0.6660.537 0.521 1509.49 9.87 4.77 6.920.01 3.37
S10 0.9250.750 0.763 0.8480.753 0.719 0.8460.750 0.562 68792.16 489.68 9.21139.52 0.35 4.30
S11 0.6860.581 0.536 0.6570.582 0.580 0.6540.581 0.534 1563.38 48.01 35.37 7.980.0527.17
S12 0.7830.616 0.584 0.7730.645 0.583 0.7470.616 0.579 5518.76 118.45 149.06 24.740.1478.69
S13 0.8500.619 0.616 0.7540.616 0.673 0.7720.617 0.609 14627.12 258.57 269.80 87.920.29159.02
S14 0.7510.588 0.671 0.6800.591 0.620 0.6730.588 0.616 4839.38 140.64 2.54 29.530.11 1.64
S15 0.7040.570 0.496 0.6510.579 0.494 0.6330.570 0.497 1402.75 28.11 13.25 12.620.0410.36
S16 0.8040.602 0.530 0.6910.583 0.543 0.7080.602 0.529 982.64 20.01 23.94 2.180.02 6.15
S17 0.7900.660 0.710 0.7410.662 0.740 0.7400.662 0.710 9709.73 61.36 71.97 54.300.0429.77
S18 0.7920.534 0.546 0.7150.534 0.554 0.7140.534 0.546 1116.51 11.51 7.35 1.350.01 1.04
Avg 0.8080.624 0.586 0.7260.621 0.584 0.7260.624 0.569 8278.74 173.20 88.63 40.250.1849.78
negatives(FN),respectively.Inparticular,asdemonstratedinthe
existing work [ 19], we calculate the mean of all the classes for the
two metrics. Larger is better.
For efficiency, we record the time spent on the training process
andthetimespentonprioritizingalltheincidentsintestingdata.
We call the former training time and the latter predicting time.
5.2 Results and Analysis
5.2.1 Overall Effectiveness of DeepIP. Table 2 shows the perfor-
mancecomparisonamongthethreeapproaches,i.e., DeepIP,Rule,
andBayes, where bold values represent the best results among
the three approaches. From Columns 2-4 in Table 2, DeepIP out-
performs the two compared approaches (i.e., RuleandBayes)o n
all the studied systems in terms of AUC, showing the significant
advantageofDeepIPforincidentprioritization.Inparticular,the
averageAUCofDeepIPis0.808,whiletheothertwoapproaches
are just 0.624 and 0.586, respectively. Also, the range of AUC for
DeepIP is from 0.686 to 0.925, demonstrating that DeepIP is able
to achieve stably good prioritization effectiveness. Furthermore, in
terms ofPrecision andRecallshown in Columns 5-10 in Table 2,
DeepIPalsoperformsmuchbetterthan RuleandBayesonallthe
subjects. The average Precision andRecallof DeepIP are 0.726 and
0.726, while those of Ruleare 0.621 and 0.624 and those of Bayes
are0.584and0.569.Theseresultsalsoreflectthattheadaptedap-
proaches fromtraditional bug-severityprediction arenot suitable
to solvethe problemof incidentsof onlineservice systemsdue to
theirdifferences.Wefurtheranalyzethereasonisthat,incidents
aremainlyautomaticallyreportedbymonitorsandhavesignificanttimeandlocationcorrelations[
10],whilebugreportsaremanually
reportedbyusersandthusthecorrelationsarenotassignificantas
in incident reports. Therefore, existing approaches for bug reports
ignoring correlations, cannot perform well for incidents. That is, it
isquitenecessarytoproposenewapproachesbyconsideringthe
characteristicsofincidents.Forincidentprioritization,DeepIPisthe first attempt to solve this problem, and our results have shown
that it is a promising approach.
Columns11-16inTable2showtheefficiencycomparisonamong
DeepIP,Rule, and Bayes. For the offlinetraining time, Bayesspends
the shortest time (i.e., 0.02 hours), while our approach DeepIPspends the longest time (i.e., 2.30 hours) on average. However,
the training is offline and thus several hours are acceptable in prac-
tice. For the onlinepredicting time, Ruleis the most efficient one
(i.e.,0.18seconds)while Bayesspendsthelongesttime(i.e.,49.78
seconds) and DeepIP is the medium (i.e., 40.25 seconds) on average.
Actually,theonlinepredictingtimeofallthethreeapproachesis
short,and eventhe timespent onpredicting oneincident isnegli-
gible. Therefore, our approach DeepIP is practical due to the short
online predicting time and acceptable offline training time.
5.2.2 Contributions of Different Types of Features. Figure 7 shows
the comparison among DeepIP, DeepIP ğ‘›ğ‘œğ‘ƒ, and DeepIP ğ‘›ğ‘œğ¸, where
the Y-axis represents the average metric value of the 18 subjects.
From the left figure, DeepIP performs better than both DeepIP ğ‘›ğ‘œğ‘ƒ
andDeepIP ğ‘›ğ‘œğ¸inallthemetrics.WefurtherconductedaWilcoxon
signed-ranktest[ 47]atthesignificantlevelof0.05betweenDeepIP
andDeepIP ğ‘›ğ‘œğ‘ƒ/DeepIP ğ‘›ğ‘œğ¸toinvestigatewhethertheformersig-
nificantly outperforms the latter two. From the right of Figure 7,
we find that all the p-values are much smaller than 0.05, demon-
stratingthatDeepIPindeedsignificantlyoutperformsDeepIP ğ‘›ğ‘œğ‘ƒ
and DeepIP ğ‘›ğ‘œğ¸in terms of AUC, Precision, and Recall. The results
indicate that both special term features and incident-occurring en-
vironmentfeaturescansignificantlyimprovetheeffectivenessof
DeepIP, confirming the contributions of these features.
6 DISCUSSION
CommunicationwithEngineersinMicrosoft. Wehavereported
ourresultstotheresponsibleengineersinMicrosoftandcommu-
nicated with them by emails. They expressed their troubles on
incidental incidents, and also experienced and largely appreciated
381ASE â€™20, September 21â€“25, 2020, Virtual Event, Australia J. Chen et al.
/g2/g8/g10/g4/g12/g19/g12/g11/g1/g15/g8/g7/g11/g6/g5/g21/g14/g3/g11/g9/g1/g16/g6/g15/g16/g1
/g13/g21/g18/g3/g10/g17/g6/g20
/g2/g10/g3/g23
/g4/g14/g14/g19/g9/g6/g7 /g22/g24/g21/g24/g1/g4/g14/g14/g19/g9/g6/g7 /g17/g18/g11/g23/g1/g25/g24/g25/g25/g27/g26
/g4/g14/g14/g19/g9/g6/g7 /g22/g24/g21/g24/g1/g4/g14/g14/g19/g9/g6/g7 /g17/g18/g5/g23/g1/g25/g24/g25/g25/g25/g30
/g7/g20/g14/g13/g15/g21/g15/g18/g17/g23/g4/g14/g14/g19/g9/g6/g7 /g22/g24/g21/g24/g1/g4/g14/g14/g19/g9/g6/g7
/g17/g18/g11/g23/g1/g25/g24/g25/g25/g27/g29
/g4/g14/g14/g19/g9/g6/g7 /g22/g24/g21/g24/g1/g4/g14/g14/g19/g9/g6/g7 /g17/g18/g5/g23/g1/g25/g24/g25/g25/g25/g28
/g8/g14/g13/g12/g16/g16/g23/g4/g14/g14/g19/g9/g6/g7 /g22/g24/g21/g24/g1/g4/g14/g14/g19/g9/g6/g7
/g17/g18/g11/g23/g1/g25/g24/g25/g25/g29/g25
/g4/g14/g14/g19/g9/g6/g7 /g22/g24/g21/g24/g1/g4/g14/g14/g19/g9/g6/g7 /g17/g18/g5/g23/g1/g25/g24/g25/g25/g26/g250.00.20.40.60.8
AUC Precision RecallAverage metric valueDeepTIP DeepTIP(noV) DeepTIP(noE)/g3/g8/g8/g11/g5/g6 /g3/g8/g8/g11/g5/g6/g1/g9/g10/g7/g2 /g3/g8/g8/g11/g5/g6/g1/g9/g10/g4/g2
/g1/g6/g6/g9/g3/g4
/g1/g6/g6/g9/g3/g4
/g1/g6/g6/g9/g3/g4/g1/g6/g6/g9/g3/g4
/g1/g6/g6/g9/g3/g4/g1/g6/g6/g9/g3/g4/g1/g6/g6/g9/g3/g4
/g7/g8/g5
/g1/g6/g6/g9/g3/g4 /g7/g8/g2
/g1/g6/g6/g9/g3/g4 /g7/g8/g5
/g1/g6/g6/g9/g3/g4 /g7/g8/g2
/g1/g6/g6/g9/g3/g4 /g7/g8/g5
/g1/g6/g6/g9/g3/g4 /g7/g8/g2
Figure 7: Contribution of each type of features
the functionality (i.e., prioritizing incidents by identifying inciden-
talincidents)providedbyus.Forexample,oneengineercomplained
that she/he spent too much time on investigating false-alarm inci-
dents, and noticed the real issue much later. Another engineer also
pointedoutthatthecurrentIcMsystemisoverloadedandhasmany
incidental incidents. In particular, they believed that the current
performanceofDeepIPisusefulandgaveussomesuggestionsto
make our tool more user-friendly, e.g., it would be better if there is
a â€œprioritized viewâ€ of incidents in the IcM system.
Generality of DeepIP. Although DeepIP is proposed and eval-
uated for incidents of online service systems, the framework of
DeepIP is actually general. To investigate the generality of DeepIP,
weappliedittopredicttheseverityoftraditionalbugreports.In
particular,wecomparedDeepIPwiththestate-of-the-artbugsever-
ity prediction approach [ 52] (introduced in Section 5.1.1). Here, we
used the same Mozilladataset released by the compared work, and
used the results reported in their paper (the precision of 0.439 and
therecallof0.486).ByapplyingDeepIPtothesamedataset,DeepIP
achievestheprecisionof0.610andtherecallof0.536,improvingthe
state-of-the-artapproachby41.00%and10.29%respectively.There-sultconfirmsthegeneralityofDeepIP.ThereasonwhyDeepIPcan
performwellforbugreportsisthat,itsattentionmechanismcan
determine whether there are relevant bugs for the target bug, and
it can understand semantics of textual descriptions, outperforming
traditionaltext-similarity-based approaches.Wealso releasedthe
Mozilla dataset on our project webpage.
ImpactoftheSettingofTimeWindow .Weinvestigatedtheim-
pact of the time window on DeepIP. The default setting of the time
windowinDeepIPis10.Wealsoexperimenteddifferentwindow
sizes such as 0, 5, and 15. The results show that the default setting
performsthebestingeneral.Thesmallsettings(0 and5)perform
relatively worse than the large settings (10 and 15), indicating the
necessity of considering a relatively large number of relevant in-cidents of a target incident. In addition, we conducted Wilcoxon
signed-ranktestatthesignificantlevelof0.05betweenthesettings
of10and0toinvestigatewhetherconsideringrelevantincidents
cansignificantlyimprovetheeffectivenessofDeepIPintermsof
AUC,Precision, and Recall. The resulting p-values are all less than
0.05, confirming the contribution of relevant incidents in DeepIP.
Threats to Validity. The internalthreat to validity mainly lies in
theimplementationsofourapproachandthecomparedapproaches.
To reduce this threat, two authors have carefully checked the code.For the various machine learning and information retrieval algo-
rithmsusedinourwork,weadoptedtheimplementationsprovided
by mature tools, which has been presented in Section 5.1.2.
Theexternalthreats to validity mainly lie in the subjects and
comparedapproaches.Inthiswork,westudied18real-worldonline
service systems in Microsoft. All the used data are real in indus-try. To our best knowledge, this is the first large-scale study inthis area. Even so, the used subjects may not represent systemsin other companies. In the future, we will investigate more sys-
temsfromdifferentcompanies.SincetheframeworkofDeepIPis
general,itiseasytoapplyittoothercompaniesaslongasthecom-panieshavehistoricaldatafortraining.Also,somespecificfeaturesmaybedifferentfordifferentcompanies.Inparticular,wehavecon-ductedastudyabovebyapplyingDeepIPtoanopen-source Mozilla
dataset, demonstrating the generality of DeepIP. For the compared
approaches, we selected two typical bug severity prediction ap-
proachesandalsodiscussedthestate-of-the-artapproach[ 52]in
Section5.1.1.However,theymaynotrepresentotherapproaches.
In the future, we will consider more approaches for comparison.
Theconstruct threatstovaliditymainlylieintheusedmetrics,
used parameters, and labeled data. To evaluate the performance of
DeepIP,weusedwidely-usedAUC,precision,andrecallformea-
suring effectiveness, and used training time and predicting timefor measuring efficiency. In the future, we will use more metrics
to more sufficiently measure the performance of these approaches.
Fortheparametersinthecomparedapproaches,wesetthemusingthevaluesgiveninthepapersorprovidedbytheusedmaturetools.
For theparameters inDeepIP, we setthem viagrid search, whose
specific settings have been presented in Section 5.1.2. Also, we dis-
cussedtheimpactofthemainparameter timewindowsize above.
Inthefuture,wewillfurtherinvestigatetheimpactofotherparam-
eters.Theincidentdatawerelabeledmanuallybyengineers,and
theremaybenoise.However,theseengineershaverichexperiences
and domain knowledge, thus this threat may not be serious.
7 RELATED WORK
Incident Management . The most related work to ours is inci-
dent management. For example, Lou et al. [ 35,36] presented an
experience report on applying software analytics to incident man-
agement of online service systems, including incident diagnosisand mitigation. Chen et al. [
10] conducted an extensive study to
investigateincidenttriageforonlineservicesystemsandevaluatedtheperformanceoftraditionalsoftwarebugtriagetechniquesintheincident-triagecontext.Someworkaimstoassociateanewincident
with a previous known incident [ 17,34]. For example, Duan and
Babu[17] proposedanapproachtoimproving theaccuracybased
on active learning, which maximizes the benefits gained from new
unknowninstancestofacilitatemanuallabelingefforts.Different
fromthem,ourworkaimstocharacterizeincidentsofonlineser-
vice systems and then prioritize incidents by identifying incidental
incidents to improve the incident management process.
Bug Report Management . There are many common character-
isticsbetweenincidentreportsofonlineservicesystemsandbug
reports of traditional software systems. Over the years, there have
been many empirical studies on bug reports and bug-fixing perfor-
mance[22,30,33,38].Forexample,Lietal.[ 33]manuallycollected
382How Incidental are the Incidents? ASE â€™20, September 21â€“25, 2020, Virtual Event, Australia
709bugsfromMozillaandApacheWebServer,andanalyzedthe
bugcharacteristics.Mockusetal.[ 38]proposedqualitymetrics(e.g.,
the percentage of defective files) to understand software mainte-
nance efforts quantitatively. Guo et al. [ 22] performed an empirical
studytocharacterizefactorsthatdeterminewhichbugsgetfixed
inWindows 7.Some researchers alsostudied defectlife cycles[ 15],
bugdistributions[ 4,51],bugtriage[ 5,23],andbug-reportbased
fault localization [ 48,57]. Furthermore, there are also many pa-
pers on automatic assessment of the severity and priority of bug
reports [32,49,52]. For example, Menzies and Marcus [ 37]p r o -
posed a tool named SEVERIS, which utilized standard information
retrieval techniques and a rule learner to infer the connections be-
tweenthemostinformativetokensinabugreportandtheseverity
level. Tian et al. [ 44] proposed a machine learning based approach
thatcanrecommendaprioritylevelbasedoninformationavailableinbugreports.Differentfromthem,weperformanempiricalstudy
ofincidents usingindustrialdataandproposeanapproachto pri-
oritizingincidents.Althoughincidentandbugreportshavemuch
in common,they havesome differentcharacteristics. Forexample,
bugreportsareoftenreportedandtreatedindividually,whilemany
incident reports tend to be correlated. One major reason is thatan incident can lead to a series of other incidents, which can be
detected by different monitors.
Empirical Analysis of Failures of Cloud Systems . Over the
years,therehavebeenmanyempiricalstudiesonthecharacteris-
ticsoffailuresofdatacentersandcloudsystems[ 20,42,45,56].For
example,SchroederandGibson[ 42]describedalarge-scalestudy
offailuresinhigh-performancecomputingsystems.Zhouetal.[ 56]
performed an empirical study on quality issues of a real-world big
data platform. Researchers also investigated the root causes of the
systemfailures[ 21,40,50].Forexample,Gray[ 21]foundthatad-
ministrator errors were responsible for 42% of system failures in
high-end mainframes. Yin et al. [ 50] studied 546 real-world mis-
configurations and found that a large portion of misconfigurations
can cause hard-to-diagnose failures. Different from them, we focus
on incidents rather than failures. Our study shows that not all inci-
dents can lead to system failures. Many incidents (i.e., incidentalincidents) are not important and will not get fixed with a high
priority. In this work, we characterize the incidental incidents and
proposeadeep-learningbasedapproachtoprioritizingincidents.
Ourworkallowsengineerstomoreefficientlyspendtheirefforts
on incident management.
8 CONCLUSION
Tobetterunderstandreal-worldincidents,weconductalarge-scale
empirical study on incidents of 18 online service systems in Mi-
crosoft.Wefindthatalargenumberofincidentsarereportedwithin
ashortperiod,butmanyincidentsareincidental.Ourqualitative
and quantitativeanalysis showthat onaverage, morethan halfof
incidents are incidental and the percentage of maintenance timespent on them is up to 55.05%. Therefore, it is quite necessary to
prioritize incidents by identifying incidental incidents in advance
soastooptimizetheincidentmanagementprocess.Towardsthis
direction,weproposeDeepIP,adeeplearningbasedapproachto
prioritizing incidents by predicting the probabilities of incidents
beingincidental.OurexperimentalresultsshowthatDeepIPcanachieve the AUC value of 0.808 on average with acceptable cost,
whichsignificantlyoutperformsallthecomparedapproaches.In
the future, we will further improve DeepIP to predict multiplecategories of incidents instead of directly identifying incidental
incidents and essential incidents.
ACKNOWLEDGEMENTS
This work was partially supported by the National Natural Science
Foundation of China 62002256 and ARC DP200102940.
REFERENCES
[1] Accessed: 2019. MXNet. https://mxnet.incubator.apache.org/
[2]Accessed:2019. news. https://www.businessinsider.com/amazon-prime-day-
website-issues-cost-it-millions-in-lost-sales-2018-7
[3] Accessed: 2019. scikit-learn. http://scikit-learn.org/stable/
[4]Carina Andersson and Per Runeson. 2007. A Replicated Quantitative Analysis of
FaultDistributionsinComplexSoftwareSystems. IEEETrans.Softw.Eng. 33,5
(May 2007), 273â€“286. https://doi.org/10.1109/TSE.2007.1005
[5]JohnAnvikandGailCMurphy.2011.Reducingtheeffortofbugreporttriage:Rec-
ommenders for development-oriented decisions. ACM Transactions on Software
Engineering and Methodology (TOSEM) 20, 3 (2011), 10.
[6]Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2015. Neural Machine
Translation by Jointly Learning to Align and Translate. international conference
on learning representations (2015).
[7]Yoshua Bengio, Aaron Courville, and Pascal Vincent. 2013. Representation
learning:Areviewandnewperspectives. IEEEtransactionsonpatternanalysis
and machine intelligence 35, 8 (2013), 1798â€“1828.
[8]Michael W Berry and Malu Castellanos. 2004. Survey of text mining. Computing
Reviews45, 9 (2004), 548.
[9]DavidMBlei,AndrewYNg,andMichaelIJordan.2003.Latentdirichletallocation.
Journal of machine Learning research 3, Jan (2003), 993â€“1022.
[10]JunjieChen,XiaotingHe,QingweiLin,YongXu,HongyuZhang,DanHao,Feng
Gao,ZhangweiXu,YingnongDang,andDongmeiZhang.2019. AnEmpirical
Investigation of Incident Triage for Online Service Systems. In Proceedings of the
41st ACM/IEEE International Conference on Software Engineering. to appear.
[11]Junjie Chen, Xiaoting He, Qingwei Lin, Hongyu Zhang, Dan Hao, Feng Gao,
Zhangwei Xu, Yingnong Dang, and Dongmei Zhang. 2019. Continuous incident
triageforlarge-scaleonlineservicesystems.In 201934thIEEE/ACMInternational
Conference on Automated Software Engineering (ASE). IEEE, 364â€“375.
[12]Yujun Chen, Xian Yang, Hang Dong, Xiaoting He, Hongyu Zhang, Qingwei Lin,
JunjieChen,PuZhao,YuKang,FengGao,ZhangweiXu,andDongmeiZhang.
2020. Identifying Linked Incidents in Large-scale Online Service Systems. In The
28th ACM Joint European Software Engineering Conference and Symposium on the
Foundations of Software Engineering. to appear.
[13]StÃ©phan ClÃ©menÃ§on and Nicolas Vayatis. 2007. Ranking the best instances.
Journal of Machine Learning Research 8, Dec (2007), 2671â€“2699.
[14]CorinnaCortesandMehryarMohri.2003. AUCOptimizationvs.ErrorRateMini-mization.In Proceedingsofthe16thInternationalConferenceonNeuralInformation
Processing Systems. 313â€“320.
[15]Marco Dâ€™Ambros, Michele Lanza, and Martin Pinzger. 2007. " A Bugâ€™s Life" Visu-
alizingaBugDatabase.In VisualizingSoftwareforUnderstandingandAnalysis,
2007. VISSOFT 2007. 4th IEEE International Workshop on. IEEE, 113â€“120.
[16]Min Du, Feifei Li, Guineng Zheng, and Vivek Srikumar. 2017. Deeplog: Anomaly
detection and diagnosis from system logs through deep learning. In Proceedings
ofthe2017ACMSIGSACConferenceonComputerandCommunicationsSecurity.
ACM, 1285â€“1298.
[17]Songyun Duan and Shivnath Babu. 2008. Guided problem diagnosis through
active learning. In International Conference on Autonomic Computing. 45â€“54.
[18]Tom Fawcett. 2006. An introduction to ROC analysis. Pattern Recognition Letters
27, 8 (2006), 861â€“874.
[19]GeorgeFormanandMartinScholz.2010. Apples-to-applesincross-validation
studies: pitfalls in classifier performance measurement. ACM SIGKDD Explo-
rations Newsletter 12, 1 (2010), 49â€“57.
[20]Phillipa Gill, Navendu Jain, and Nachiappan Nagappan. 2011. Understanding
network failures in data centers: measurement, analysis, and implications. In
ACM SIGCOMM Computer Communication Review, Vol. 41. ACM, 350â€“361.
[21]Jim Gray. 1986. Why do computers stop and what can be done about it?. In
Symposiumonreliabilityindistributedsoftwareanddatabasesystems.LosAngeles,
CA, USA, 3â€“12.
[22]PhilipJGuo,ThomasZimmermann,NachiappanNagappan,andBrendanMurphy.
2010. Characterizing and predicting which bugs get fixed: an empirical study of
MicrosoftWindows.In Proceedingsofthe32ndACM/IEEEInternationalConference
on Software Engineering-Volume 1. ACM, 495â€“504.
383ASE â€™20, September 21â€“25, 2020, Virtual Event, Australia J. Chen et al.
[23]PhilipJGuo,ThomasZimmermann,NachiappanNagappan,andBrendanMurphy.
2011. Not my bug! and other reasons for software bug report reassignments. In
Proceedings of the ACM 2011 conference on Computer supported cooperative work.
ACM, 395â€“404.
[24]DavidJHand.2007. Principlesofdatamining. Drugsafety 30,7(2007),621â€“622.
[25]David Harris and Sarah Harris. 2012. Digital Design and Computer Architecture,
Second Edition (2nd ed.). Morgan Kaufmann Publishers Inc.
[26]JiajunJiang,WeihaiLu,JunjieChen,QingweiLin,PuZhao,YuKang,Hongyu
Zhang,YingfeiXiong,FengGao,ZhangweiXu,YingnongDang,andDongmei
Zhang. 2020. How to Mitigate the Incident? An Effective Troubleshooting Guide
Recommendation Technique for Online Service Systems. In The 28th ACM Joint
European Software Engineering Conference and Symposium on the Foundations of
Software Engineering, Industry track. to appear.
[27]RieJohnsonandTongZhang.2017. Deeppyramidconvolutionalneuralnetworks
fortextcategorization.In Proceedingsofthe55thAnnualMeetingoftheAssociation
for Computational Linguistics (Volume 1: Long Papers), Vol. 1. 562â€“570.
[28]Armand Joulin, Edouard Grave, Piotr Bojanowski, Matthijs Douze, HÃ©rve JÃ©gou,
andTomasMikolov.2016. FastText.zip:Compressingtextclassificationmodels.
arXiv preprint arXiv:1612.03651 (2016).
[29]YujunChen; XianYang;QingweiLin; HongyuZhang;Feng Gao;ZhangweiXu;
Yingnong Dang; Dongmei Zhang; Hang Dong; Yong Xu; Hao Li; Yu Kang;. 2019.
Outage Prediction and Diagnosis for Cloud Service Systems. In Proceedings of
the 2018 World Wide Web Conference on World Wide Web, WWW 2018. to appear.
[30]Sunghun Kim and E James Whitehead Jr. 2006. How long did it take to fix bugs?.
InProceedings of the 2006 international workshop on Mining software repositories.
ACM, 173â€“174.
[31]Yoon Kim. 2014. Convolutional Neural Networks for Sentence Classification.
InProceedings of the 2014 Conference on Empirical Methods in Natural Language
Processing, EMNLP2014, October 25-29,2014, Doha, Qatar, A meetingof SIGDAT,a
Special Interest Group of the ACL. 1746â€“1751.
[32]Ahmed Lamkanfi, Serge Demeyer, Emanuel Giger, and Bart Goethals. 2010. Pre-
dictingtheseverityofareportedbug.In MiningSoftwareRepositories(MSR),2010
7th IEEE Working Conference on. IEEE, 1â€“10.
[33]ZhenminLi,LinTan,XuanhuiWang,ShanLu,YuanyuanZhou,andChengxiangZhai. 2006. Have things changed now?: an empirical study of bug characteristics
inmodernopensourcesoftware.In Proceedingsofthe1stworkshoponArchitec-
tural and system support for improving software dependability. ACM, 25â€“33.
[34]MengHuiLim,JianGuangLou,HongyuZhang,FuQiang,AndrewTeoh,QingweiLin,JustinDing,andDongmeiZhang.2015. IdentifyingRecurrentandUnknown
Performance Issues. In IEEE International Conference on Data Mining.
[35]Jianguang Lou, Qingwei Lin, Rui Ding, Qiang Fu, Dongmei Zhang, and Tao
Xie. 2013. Software analytics for incident management of online services: an
experience report. automated software engineering (2013), 475â€“485.
[36]Jianguang Lou, Qingwei Lin, Rui Ding, Qiang Fu, Dongmei Zhang, and Tao Xie.
2017. Experience report on applying software analytics in incident management
of online service. automated software engineering 24, 4 (2017), 905â€“941.
[37]Tim Menzies and Andrian Marcus. 2008. Automated severity assessment of soft-
ware defect reports. In Software Maintenance, 2008. ICSM 2008. IEEE International
Conference on. IEEE, 346â€“355.
[38]AudrisMockus,RoyTFielding,andJamesDHerbsleb.2002. Twocasestudiesof
opensourcesoftwaredevelopment:ApacheandMozilla. ACMTransactionson
Software Engineering and Methodology (TOSEM) 11, 3 (2002), 309â€“346.
[39]Nasser M Nasrabadi. 2007. Pattern recognition and machine learning. Journal of
electronic imaging 16, 4 (2007), 049901.
[40]David Patterson, Aaron Brown, Pete Broadwell, George Candea, Mike Chen,
James Cutler, Patricia Enriquez, Armando Fox, Emre Kiciman, Matthew
Merzbacher,etal .2002.Recovery-orientedcomputing(ROC):Motivation,definition,
techniques,andcasestudies. TechnicalReport.TechnicalReportUCB//CSD-02-
1175, UC Berkeley Computer Science.
[41]Stephen Robertson, Hugo Zaragoza, et al .2009. The probabilistic relevance
framework: BM25 and beyond. Foundations and Trends Â®in Information Retrieval3, 4 (2009), 333â€“389.
[42]Bianca Schroeder and Garth Gibson. 2010. A large-scale study of failures in
high-performance computing systems. IEEE Transactions on Dependable and
Secure Computing 7, 4 (2010), 337â€“350.
[43]Yuan Tian, David Lo, and Chengnian Sun. 2013. Drone: Predicting priority of
reported bugsby multi-factoranalysis. In 2013 IEEEInternational Conferenceon
Software Maintenance. IEEE, 200â€“209.
[44]Yuan Tian, David Lo, Xin Xia, and Chengnian Sun. 2015. Automated prediction
ofbugreportpriorityusingmulti-factoranalysis. EmpiricalSoftwareEngineering
20, 5 (2015), 1354â€“1383.
[45]KashiVenkateshVishwanathandNachiappanNagappan.2010. Characterizing
cloud computing hardware reliability. In Proceedings of the 1st ACM symposium
on Cloud computing. ACM, 193â€“204.
[46]Lingzhi Wang, Nengwen Zhao, Junjie Chen, Pinnong Li, Wenchi Zhang, and
Kaixin Sui. 2020. Root-Cause Metric Location for Microservice Systems via Log
AnomalyDetection.In The2020IEEEInternationalConferenceonWebServices.
to appear.
[47]Frank Wilcoxon. 1945. Individual Comparisons by Ranking Methods. Biometrics
Bulletin1, 6 (1945), 80â€“83.
[48]Chu-Pan Wong, Yingfei Xiong, HongyuZhang, Dan Hao, Lu Zhang,and Hong
Mei. 2014. Boosting bug-report-oriented fault localization with segmentation
and stack-trace analysis. In 2014 IEEE International Conference on Software Main-
tenance and Evolution (ICSME). IEEE, 181â€“190.
[49]Cheng-Zen Yang, Chun-Chi Hou, Wei-Chen Kao, and Xiang Chen. 2012. An
empirical study on improving severity prediction of defect reports using feature
selection. In Software Engineering Conference (APSEC), 2012 19th Asia-Pacific,
Vol. 1. IEEE, 240â€“249.
[50]ZuoningYin,XiaoMa,JingZheng,YuanyuanZhou,LakshmiNBairavasundaram,
and Shankar Pasupathy. 2011. An empirical study on configuration errors in
commercialandopensourcesystems.In ProceedingsoftheTwenty-ThirdACM
Symposium on Operating Systems Principles. ACM, 159â€“172.
[51]H. Zhang. 2008. On the Distribution of Software Faults. IEEE Transactions on
Software Engineering 34, 2 (March 2008), 301â€“302. https://doi.org/10.1109/TSE.
2007.70771
[52]Tao Zhang, Jiachi Chen, Geunseok Yang, Byungjeong Lee, and Xiapu Luo. 2016.
Towardsmoreaccurateseveritypredictionandfixerrecommendationofsoftware
bugs.Journal of Systems and Software 117 (2016), 166â€“184.
[53]Xu Zhang, Yong Xu, Qingwei Lin, Bo Qiao, Hongyu Zhang, Yingnong Dang,Chunyu Xie, Xinsheng Yang, Qian Cheng, Ze Li, et al
.2019. Robust log-based
anomaly detection on unstable log data. In Proceedings of the 2019 27th ACM
JointMeetingonEuropeanSoftwareEngineeringConferenceandSymposiumon
the Foundations of Software Engineering. 807â€“817.
[54]NengwenZhao,JunjieChen,XiaoPeng,HonglinWang,XinyaWu,Yuanzong
Zhang,ZikaiChen,XiangzhongZheng,XiaohuiNie,GangWang,YongWu,Fang
Zhou,WenchiZhang,KaixinSui,andDanPei.2020. UnderstandingandHandling
Alert Storm for Online Service Systems. In The 42nd International Conference on
Software Engineering, SEIP track. to appear.
[55]NengwenZhao,JunjieChen,ZhouWang,XiaoPeng,GangWang,YongWu,Fang
Zhou, Zhen Feng, Xiaohui Nie, Wenchi Zhang, Kaixin Sui, and Dan Pei. 2020.
Real-time Incident Prediction for Online Service Systems. In The 28th ACM Joint
European Software Engineering Conference and Symposium on the Foundations of
Software Engineering. to appear.
[56]Hucheng Zhou, Jian-Guang Lou, Hongyu Zhang, Haibo Lin, Haoxiang Lin, and
Tingting Qin. 2015. An Empirical Study on Quality Issues of Production Big
Data Platform. In Proceedings of the 37th International Conference on Software
Engineering - Volume 2 (Florence, Italy) (ICSE â€™15). IEEE Press, Piscataway, NJ,
USA, 17â€“26. http://dl.acm.org/citation.cfm?id=2819009.2819014
[57]Jian Zhou, Hongyu Zhang, and David Lo. 2012. Where should the bugs be fixed?
more accurate information retrieval-based bug localization based on bug reports.
InSoftwareEngineering(ICSE),201234thInternationalConferenceon.IEEE,14â€“24.
384