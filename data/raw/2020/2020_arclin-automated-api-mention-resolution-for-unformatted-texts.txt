ARCLIN: Automated API Mention Resolution for Unformatted
Texts
Yintong Huo
The Chinese University of Hong Kong
Hong Kong, China
ythuo@cse.cuhk.edu.hkYuxin Suâˆ—
School of Software Engineering
Sun Yat-sen University
Zhuhai, China
suyx35@mail.sysu.edu.cn
Hongming Zhang
The Hong Kong University of Science and Technology
Hong Kong, China
hzhangal@cse.ust.hkMichael R. Lyu
The Chinese University of Hong Kong
Hong Kong, China
lyu@cse.cuhk.edu.hk
ABSTRACT
Onlinetechnicalforums(e.g.,StackOverflow)arepopularplatforms
for developers to discuss technical problems such as how to use
aspecificApplicationProgrammingInterface(API),howtosolve
the programming tasks, or how to fix bugs in their code. These
discussionscanoftenprovideauxiliaryknowledgeofhowtouse
the software that is not covered by the official documents. The
automaticextractionofsuchknowledgemaysupportasetofdown-
streamtaskslikeAPIsearchingorindexing.However,unlikeofficial
documentation written by experts, discussions in open forums aremade by regular developers who write in short and informal texts,
including spelling errors or abbreviations. There are three major
challengesfortheaccurateAPIsrecognitionandlinkingmentioned
APIs from unstructured natural language documents to an entry in
the API repository: (1) distinguishing API mentions from common
words;(2)identifyingAPI mentionswithoutafully qualifiedname;
and(3)disambiguatingAPImentionswithsimilarmethodnames
butinadifferentlibrary.Inthispaper,totacklethesechallenges,
we propose an ARCLIN tool, which can effectively distinguish and
linkAPIswithoutusinghumanannotations.Specifically,wefirst
design an API recognizer to automatically extract API mentionsfrom natural language sentences by a Conditional Random Field
(CRF) on the top of a Bi-directional Long Short-Term Memory (Bi-
LSTM) module, then we apply a context-aware scoring mechanism
to computethe mention-entry similarityfor eachentry in anAPI
repository. Compared to previous approaches with heuristic rules,
ourproposedtoolwithoutmanualinspectionoutperformsby8%
inahigh-qualitydatasetPy-mention,whichcontains558mentionsand 2,830 sentences from five popular Python libraries. To our bestknowledge,ARCLINisthefirstapproachtoachievefullautomation
âˆ—Corresponding author (suyx35@mail.sysu.edu.cn).
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation
on the first page. Copyrights for components of this work owned by others than ACMmustbehonored.Abstractingwithcreditispermitted.Tocopyotherwise,orrepublish,topostonserversortoredistributetolists,requirespriorspecificpermissionand/ora
fee. Request permissions from permissions@acm.org.
ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
Â© 2022 Association for Computing Machinery.
ACM ISBN 978-1-4503-9221-1/22/05...$15.00
https://doi.org/10.1145/3510003.3510158ofAPImentionresolutionfromunformattedtextwithoutmanually
collected labels.
KEYWORDS
API, API disambiguation, text mining
ACM Reference Format:
YintongHuo,YuxinSu,HongmingZhang,andMichaelR.Lyu.2022.AR-
CLIN:AutomatedAPIMentionResolutionforUnformattedTexts.In 44th
International Conference on Software Engineering (ICSE â€™22), May 21â€“29,
2022, Pittsburgh, PA, USA. ACM, New York, NY, USA, 12pages.https:
//doi.org/10.1145/3510003.3510158
1 INTRODUCTION
ApplicationProgrammingInterface(API)isanessentialcomponent
for programming. Developers use APIs to interact with a program-
minglanguageorasoftwarelibrary. However,as alibrarycontains
thousandsofAPIs(e.g.,PyTorchv1.8hasover2,400APIs)andthere
are hundreds of popular libraries in a language, it is impossible for
developerstobefamiliarwithallAPIs.Therefore,developersare
used to discussing programming-related questions in the online
technical forum when they face troubles in programming tasks.
Oneofthe mostpopularforums,StackOverflow,containsover20
million questions and 14 million users1. It motivates researchers
toexplorehowtoidentifyknowledgeinopenforumstoassistde-
velopersinmanyaspects,suchasAPIrecommendation[ 37],API
misuse detection [29, 30], and document augmentation [35].
Thefoundationoftheabovetasksisrecognizingandidentifying
API mentions from an unstructured natural language. Convention-
ally,researcherstriedtouserule-basedmethodstosolvethetask.
Forexample,Bacchellietal .[2],TreudeandRobillard [35]identified
APIelementsintextsbyasetofregularexpressions.Huangetal .
[18]choseahyperlinkineachStackOverflowpostandusedregular
expressions to detect API entities. They also analyzed whether the
text in HTML <code>tag can match the API names in the API
repositories.Lietal .[21]detectedAPIsbycheckingwhetherthe
tokenofasentencecanmatchorpartiallymatchthenameofan
API by conducting minor modifications. Ren et al .[30]kept API
mentions only in HTML <code>elements.
However,theserule-basedmethodsdonotconsidertheshortand
informal nature of forum discussions, falling short in mining APIs
1The data dump is retrieved in September 1ğ‘ ğ‘¡, 2021.
1382022 IEEE/ACM 44th International Conference on Software Engineering (ICSE)
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 07:51:54 UTC from IEEE Xplore.  Restrictions apply. ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA Yintong Huo, Yuxin Su, Hongming Zhang, and Michael R. Lyu
in certain scenario. Typically, a forum may contain a large number
of unprofessional developers with different technical backgrounds,
who share the knowledge and information in their own writing
styles.Asaresult, theAPI mentionscould bein differentformats.
For example, previous study [ 33] concluded from StackOverflow
poststhat47%oftheAPIelementsarenotincludedwiththeHTML
<code>tag.Suchinconsistencycausesdifferentkindsofambiguity
when we recognize and identify APIs. In this paper, we categorize
these ambiguities into the following three types.
Thefirstoneis common-wordambiguity,referringtotheambigu-
itybetweencommonwordsandAPImentions[ 40].Traditionally,
APInameiscomposedofpunctuations,brackets,anduppercase
letters;however,sometimesdevelopersonlywritetheAPIâ€™smethod
name in their answers, causing the difficulty of distinguishing it
fromcommonwords.ThefirstgroupintheTable 1illustratesexam-
plesofthisproblem.Eveniftwosentencesareallmentioningthe
wordview,thefirstsentenceuse viewtorefertotheAPI torch.view()
whereas the second one use viewas a common verb. Regular ex-
pressionsfailindiscriminatingsuchAPImentionswithcommon
words.Previouswork[ 40]revealedthat35.1%ofthetoken ğ‘ğ‘ğ‘ğ‘™ğ‘¦
in StackOverflow posts tagged with Pandas actually referred to an
API mention.
Thesecondoneis morphologicalambiguity,whichisbecausede-
velopers rarely write down the full API name that can be perfectly
matchedwithanAPInameinthelibrary.ResearchonStackOver-
flow [5] concludes that morphological mentions, which include ab-
breviations,synonyms,andmisspellings,arequiteoftenininformal
discussions. Fourexamples in the secondgroup of Table 1demon-
stratethemorphologicalvariations.Inthefirstthreesentences,the
APInumpy.reshape() wasmentionedbyreplacing numpywithits
abbreviation np,omittinglibraryname,andusingthecustomized
variable name, respectively. The fourth sentence talks about the
torch.nn.Conv2d andtorch.nn.Conv3d APIs,butincludesneitherthe
library/module/class name nor the correct case (i.e., use conv2d
instead of Conv2d).
The third type is reference ambiguity, which happens if the API
lists contain various third-party libraries. The third group in Ta-
ble1provides two instances of this problem. Even if both PyTorch
library and Tensorflow library contain the API method flatten(),
we could characterize what the mentions refer to based on their
sentence contexts (i.e., the first sentence mentions â€œkerasâ€module
whereas the second one mentions â€œPyTorchâ€ ). It is often the case
that developers do not explicitly point out the specific library in
their mentions, but such information can be derived from other
words in the context.
Dueto theabove ambiguities,traditional informationretrieval
techniques cannot be effectively employed. Dagenais and Robillard
[7]appliedasetoffilteringheuristicstotacklethesecondchallenge,
buttheyfailedinresolvingcommonwordsambiguityduetothe
shortcoming of regular expressions. The above challenges become
more difficult if we apply the API mining task into unformattedsentences. Such free text does not contain any
<code>tags, so
detecting API in this scenario is even harder. However, it is a non-
negligibleproblem,since,inotherscenarios(e.g.,emails),wecannot
use HTML tags. To make our research applicable for a broaderapplication, we focus on mining APIs from free text. Althoughthe most recent work [
40] claimed to distinguish API mentionsfromcommonwords,theystored <code>tagsandcodesnippetsin
<pre><code>tagsfromStackOverflowposts,insteadofmining
fromfreetexts.Thus,theirapproachcannotbeextendedtogeneral
scenarios.
Inthispaper,toovercometheaforementionedambiguitychal-
lenges, we propose a new API mining approach named ARCLIN
(APIRecognition and Contextual LINking), which recognizes and
identifies API mentions from natural language descriptions to a set
of APIs without any human-annotated labels or handcrafted rules.
OurmodelismadeupofanAPIrecognizerthatfindsAPImentionsinfreetexts,andacontextualAPIlinkerthatlinksAPImentionstothecorrectAPItheyreferto.Specifically,ourAPIrecognizerexten-
sively deals with the first common word ambiguity by considering
the contextinformation insentence-level aroundan APImention.
For the words that are predicted to be an API mention, a librarypredictor inside the API linker predicts the related library to the
sentence, restricting ARCLIN to link APIs in the predicted library,
which resolves the reference ambiguity. The similarity function in
the API linker compares API mention with every entry in the API
repository, considering both spelling similarity and lexical simi-
larity,sominormorphologicalchangeswillnotaffectthelinking
result. To the best of our knowledge, ARCLIN is the first approach
that can automatically cope with these challenges above.
Considering the numerous number of APIs in the real world, it
isimpracticaltoaskannotatorstolabelsuchalargescaleofdata.
To avoid this labor-intensive process, we design ARCLIN to be free
fromanyhumanannotationinthetrainingprocessbyexploiting
naturallabelsinthetrainingset.Unlikehuman-labeleddata,the
automatedlabelsmaycontainerrors,butourAPIlinkerinthenext
step provides a strict selection to address this problem. To evaluate
theeffectivenessofARCLIN,weannotateatestset,whichcontains
2,948sentenceswith563mentionsunderfivepopularthird-party
libraries. On average, ARCLIN achieves 78.26%, 73.53% and 75.82%
inprecision,recallandF1score,respectively.Thepromisingresultsindicate that, even though our approach does not need any human-annotatedlabels,itoutperformsthecurrentstate-of-the-artbaseline
trained with labeled data.
To sum up, the main contributions of this paper are threefold:
â€¢To our best knowledge, we are the first to design an unla-
beled approach focusing on API recognizing and linking in
unformatted text corpora.
â€¢WebuildanAPIcontextuallinker,makingthemodelauto-
maticallylinkAPImentionsto anAPIrepository,takingthe
sentence context into account.
â€¢TheexperimentresultsshowARCLINcandiscovertraceabil-
itylinksbetweenAPIsandtherepositorymoreaccurately,
comparingwithstate-of-the-artbaselinemodels.Thecode
and dataset are released2.
2 PROBLEM STATEMENT
In this section, we first introduce the main concepts used in this
paperinSection 2.1andthenprovideaformaldefinitionofthetask
in Section 2.2.
2Please find the resources in https://github.com/YintongHuo/ARCLIN.
139
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 07:51:54 UTC from IEEE Xplore.  Restrictions apply. ARCLIN: Automated API Mention Resolution for Unformatted Texts ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
Table1:ThreemainchallengesforAPImininginunformattedtexts, BluewordsreferstoAPImentionsand Redwordsrefers
to common words.
Question ID Sentence API
#66952125 So far I managed to use viewonce in my first very simple project... torch.view()
#59905234 You can use .numpy() to viewthe internal data... None
#42233297 Simply reshaping the by np.reshape(data,(5000,3,32,32)) would not work. numpy.reshape()
#41518351 The.reshape() method (of ndarray) returns the reshaped array. numpy.reshape()
#47477945 I have tried a.reshape(3,4) in for the numpy array but nothing is producing what I want. numpy.reshape()
#65103822 I would like to understand the difference between conv2dandconv3din PyTorch. torch.nn.Conv2d, torch.nn.Conv3d
#47532162 I want to use the keras layer Flatten() or Reshape((-1,)) at the end of my model... tensorflow.keras.layers.Flatten()
#60115633 But in PyTorch, flatten() is an operation on the tensor. torch.Tensor.flatten()
Figure 1: A screenshot of one StackOverflow post.
2.1 Terminology
API mining is the task of recognizing API mentions from free texts
and linking the recognized API mentions to the corresponding API
repositories. Figure 1is a screenshot of a StackOverflow post3,w e
use this screenshot to illustrate the concepts used in this paper.
Here,anAPIcouldbeaclassname,amethodname,oranattribute
of a class. The term free text(also called unformatted text ) refers to
thetextwithoutanyHTMLtags(e.g., <code>).Orangedashbox
in Figure 1shows an example of <code>usage. An API mention in
textsisatokenappearinginthefreetextthatreferstoaspecificAPI
intherepository.BlueboxinthefigureshowstwoAPImentions
(i.e.,nn.Linear() andnn.Module )4.AnAPIrepository isacollection
3The entire page is in https://stackoverflow.com/questions/50463975/pytorch-how-to-
properly-create-a-list-of-nn-linear.
4nn.ModuleList in Orange box is also an API mention after removing the <code>tag.of all entire qualified API names. Entire qualified name is the ex-
act APIâ€™sname shown in its officialwebsite (e.g., torch.nn.Linear,
torch.nn.Module ). Each APIâ€™s name in this repository is called an
entry. An entry is composed of sub-fields, splitted by â€œ.â€, which are
called entities, the entities of nn.Linear() andtorch.nn.Linear are
shown in nearby red circles.
2.2 Task Description
Given a natural language sentence ğ‘†in free text and an API reposi-
toryğ·={ğ·1,ğ·2,...,ğ· ğ‘›},whereğ·ğ‘–referstoanentryinthereposi-
tory,theAPI miningtaskistolink APImentionstoanentryinthe
API repository. Thetask involves two phases:(1) recognizing API
mentions in the sentence; (2) linking API mentions to the corre-
spondingentriesintheAPIrepository.Inpractice,wefirsttokenize
ğ‘†into a token list [ğ‘¡ğ‘œğ‘˜ğ‘’ğ‘›0,ğ‘¡ğ‘œğ‘˜ğ‘’ğ‘›1,...,ğ‘¡ğ‘œğ‘˜ğ‘’ğ‘› ğ‘›], thenâˆ€0â‰¤ğ‘–â‰¤ğ‘›,w e
determine whether ğ‘¡ğ‘œğ‘˜ğ‘’ğ‘› ğ‘–refers to the element ğ·ğ‘—âˆˆğ·.
3 APPROACH
In this section, we introduce our approach, including data prepara-
tion,anAPIrecognizer,andacontextualAPIlinker.Figure 2shows
theoverallframeworkofARCLIN.Tobeginwith,sentencesarefed
intotheAPIrecognizertouncoverAPImentions.Specifically,acon-
text encoder is applied to acquire contextual embeddings of tokens
byabidirectionalLong-ShortTermMemory(LSTM)network,then
these representations are decoded via a Conditional Random Field
(CRF). The tokenswhich decoded asAPI mentions are sent tothe
APIlinker.Next,anAPIlinkerisdesignedfordiscoveringthemost
possiblematchedentryintherepository.Todoso,wefirstgenerate
a series of candidates by heuristic rules, then a library predictornarrows down the candidates by specifying a library. After that,
we use anintegrated scoringfunction to rank
<mention, entry >
pairs.Finally,thecandidatewiththehighestsimilarityabovethe
threshold will be chosen as a link.
3.1 Data Preparation
3.1.1 Text Corpus. Given some libraries, we crawl all questions
taggedwithatleastoneofthegivenlibrariesfromanonlinetechni-calforum.Besidesquestionsandanswers,Zhangetal
.[43]revealed
thatthemajorityofcommentswerealsoinformativeastheypro-
vided a supplementary view to the answer. Therefore, for each
question-answeringthread,wecrawlthequestion,allanswers,and
their comments. We discard code snippets in <pre><code>but
140
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 07:51:54 UTC from IEEE Xplore.  Restrictions apply. ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA Yintong Huo, Yuxin Su, Hongming Zhang, and Michael R. Lyu
Figure 2: The framework of ARCLIN.
keep contents in <code>when it appears in a natural language
sentence.
StackOverflow users highlight API mentions in a natural lan-
guage sentence by <code>tags. However, Tabassum et al .[33]
shows that 47% of the code mentions are not indicated with this
tag. If we only rely on the tags to do the mention detection, we
will miss a large number of mentions. Moreover, it is observed that
contentsincodetagscanbenoisy,manynon-codeelements(e.g.,
variablesname,keypoints,orusername)arealsohighlightedby
codetags[ 33].Toaddressthisnoisinessissue,aswellastogeneral-
izetootherscenariosthatcannotuse <code>tags(e.g.,emails),we
removeall <code>tagsinsentencesandthemarkdownmarkersto
makethis taskmoresimilarto therealapplication. Aftercollecting
the data, we tokenize all sentences with the NLTK [ 3] sentence
parser. As a result, we obtain a set of parsed sentences in free text.
3.1.2 Repository Construction. Foreachgivenlibrary,wecrawlall
APIswiththeirentirequalifiednamesfromofficialdocumentations.
For instance, the APIs in the PyTorch library include methods (e.g.,
torch.Tensor.dim() ),functions(e.g., torch.nn.functional.avg_pool1d() ),
classes (e.g., torch.nn.AdaptiveAvgPool1d ), and attributes such as
torch.backends.cudnn.enabled. The API repository is made up of all
crawled APIsâ€™ names.
3.1.3 Tokenizer. Weadaptasoftware-specifictokenizerusedinYe
et al.[41]and Ye et al .[42], which preserves the integrity of an
API mention. Current popular tokenizers such as SpaCy [ 17], Stan-
ford Parser [ 8], and NLTK all parse numpy.shape() into a token
listof[â€œnumpy.shapeâ€,â€œ(â€,â€œ)â€],butthedeployedsoftware-specific
tokenizer will treat numpy.shape() as a single token.
3.1.4 Inverse Document Frequency (IDF). IDF is a way to measure
theimportanceofawordinacorpus.Awordâ€™sIDFisdispropor-tionate to the wordâ€™s frequency. Given the assumption that if a
word frequently occurs in a document, it may contain relativelyless information, the formula for computing IDF for a word
ğ‘¤is
shown in Equation 1:
ğ¼ğ·ğ¹(ğ‘¤)=ğ‘™ğ‘œğ‘”(#Documents_Number
#Document_with_w +1) (1)
Inthispaper,wecomputetwotypesofIDF, ğ¼ğ·ğ¹ ğ‘¡ğ‘œğ‘˜ğ‘’ğ‘›andğ¼ğ·ğ¹ ğ‘’ğ‘›ğ‘¡ğ‘–ğ‘¡ğ‘¦.
We useğ¼ğ·ğ¹ ğ‘¡ğ‘œğ‘˜ğ‘’ğ‘›to measure the tokenâ€™s importance in a corpus,
whereweregardeachsentenceasadocument.For ğ¼ğ·ğ¹ ğ‘’ğ‘›ğ‘¡ğ‘–ğ‘¡ğ‘¦,w e
computetheentityâ€™simportanceintherepository.Weconsidereach
entry is a document and its entities are words. For example, the
document numpy.reshape() hastwowords: â€œnumpyâ€ andâ€œreshape()â€.
Intuitively, since all Numpy APIs contain the entity â€œnumpyâ€, its
IDF value is relatively low.
3.2 Recognizer
The first step of mining APIs from the sentences is extracting API
mentions without specifying which APIs they refer to. At this step,
weproposeanautomaticAPImentionrecognizerthatprefersrecall
over precision to cover as many API mentions as possible. We first
introduceanautomaticapproachtominenatural(butnoisy)labels,
thenelaborateonarchitecturesoftherecognizerinthefollowing
subsections.
3.2.1 Automatic labeling. Traditionalmachinelearningapproaches
label a large-scale training set to train classification models for the
task. However, considering that there are enormous APIs even
inoneprogramminglanguage,itisinfeasibletoobtainsufficient
human-labeled data for all of them. There is a need to devise an
algorithm that escapes from any human annotation.
Previousstudies[ 27,34]showthatpriorexternalknowledge(i.e.,
API repository) was critical for good performance in identifying
namedentityinasentence.Motivatedbythis,weusethefollowingcriteria(i.e.,domainknowledge)toautomaticallyannotatepotential
API mentions:
141
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 07:51:54 UTC from IEEE Xplore.  Restrictions apply. ARCLIN: Automated API Mention Resolution for Unformatted Texts ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
â€¢Ifatokenisexactlytheentirequalifiedname(i.e.,sameas
anentryinAPIrepository),weregardthetokenasanAPI
mention.
â€¢Inspiredfromthatusersusuallyuseâ€œ() â€attheendofatoken
torepresentanAPImethodnameorfunctionname,wetreat
the token as an API mention if the token contains â€œ() â€.
â€¢Users also use â€œ.â€ (e.g., numpy.shape(),o r x.shape()) when
they mention an API; thus, we consider the token is an API
mention if it contains â€œ.â€. To distinguish such mentions from
emoticon or punctuation, we require the token to consist of
more than three characters.
Moreover,toaddressthecommon-wordpolysemyproblemin-
troducedinSection 1,weemployadataaugmentationtechnique
for each sentence with at least one API mention being detected.
Specifically, we randomly replace the originally detected API men-
tion with a new one only containing the last part of the name (e.g.,
x.viewwill be replaced with view). This data augmentation process
forces the recognizer to learn contextual information of an API
mention.
The self-labeling process inevitably introduces some noisy la-
bels. For instance, even if the token â€œpython2.7â€ consists â€œ.â€, it is
not an API mention; Besides that, a missing space between two
sentences (e.g., ... plot 500 ellipses on a single graph.If you do ...)
will generate the wrong label for the token â€œgraph.Ifâ€. However,
theproposedcontextuallinkerisabletomitigatethesenoisylabels.
After automatic labeling, we feed the self-labeled data as well as
the augmented ones into our context encoder.
3.2.2 Context Encoder. Contextencoderisresponsibleforacquir-
ingcontextualwordembeddingsinasentence.Thelongshort-term
memory (LSTM) network has shown promising results in sequen-
tial labeling tasks [ 32], due to its strong ability to capture long-
distancecontextinformation.ThememoryunitinLSTMenables
it to generate the representation based on both the short-distance
and long-distance context. In this work, we design an LSTM net-
workto achieve thegoal. Abi-directional LSTM(Bi-LSTM)[ 16]is
specificallyusedforpreservingbothpastandfutureinformation
within a sentence.
ThearchitectureoftheconstructedencoderisshowninFigure 2,
where two granular-level features are considered. By doing so,
theBi-LSTM encodersimultaneouslygrasps word-level semantics
and character-level details. Firstly, word embedding techniques are
used to extract word-level semantics. Word embedding represents
words as distributed vectors in a low-dimensional space so that
words with similar semantic or syntactic meaning tend to be close
in their vector space. Assuming that words present in a similar
contexthavesimilarmeanings,thecommonapproachSkip-gram
(Word2Vec)[ 25]learnswordembeddingsbypredictingsurrounding
words given the central word. Similar to previous research [ 15,18,
38], we train domain-specific word embeddings by Skip-gram on a
domain corpus.
Secondly, as previous study [ 22] has shown that character-level
representation is crucial to extract morphological evidence, we use
this feature to alleviate the second morphological problem men-
tionedinSection 1.Besides,sincedeveloperswriteAPImentions
with customized variable names under different scenarios, deploy-
ingcharacter-levelembeddingallowsustocopewithunseenwords,namedtheout-of-vocabulary(OOV)problem.Inparticular,weelicitcharacter-levelfeaturesfromthearchitectureshowninblue-dotted
rectanglesinFigure 2,whichincorporatesonemaxpoolinglayer
after a Convolutional Neural Network (CNN) is applied.
3.2.3 Tag Decoder. Given contextual word representations in a
sentence,thetagdecoderisusedtodeterminewhethertheword
is an API mention or just a common word. Inspired by previoussequence labeling works [
20] in the natural language processing
domain, we adopt a Conditional Random Field (CRF) to conductthe tag decoder on top of the text encoder (i.e., Bi-LSTM layer).
Byaccuratelyobtainingstructuraldependenciesamongadjacent
words in a sentence, the CRF module jointly predicts the tag of
each word sequentially instead of predicting tags independently.
InordertobalancebetweenAPImentioncoverageandprecision
in predictions, we select ğ‘‡ğ‘œğ‘_ğ‘ƒpaths with the highest confidence
scoreastheresultoftheCRFlayer.Ifthetokenin ğ¾(0â‰¤ğ¾â‰¤ğ‘ƒ)
paths is predicted as an API mention, we treat the token as an API
mention and feed it to the contextual linker.
3.3 Contextual Linker
Once we obtain the API mentions in the text, ARCLIN links the
correctAPImentionstoanentryintherepository.Thecoreidea
behindthislinkerisaseriesofdisambiguationmethods.Specifically,
wefirstlyselectentriesascandidatesintherepository,thenrank
thesimilarityscoreofevery <mention,entry >pairwiththehelp
of the mentionâ€™s context information. Although the predicted API
mentionsmaycontainerrors,thewrongmentionwillbehardto
find an entry with a high similarity score. From this aspect, the
noise introduced by the last step will not affect the final results.
3.3.1 Candidate Selection. To reduce the time complexity of com-
paringallentriesintherepositorywiththeAPImention,wenarrow
the scope by listing a set of candidates. Inspired by the fact that,
even though humans can make errors in spelling words, such mis-
spelling is hardly seen at the beginning of the word. So do the
developers. Given a mention, we directly compare its last part (i.e.,
lastentity)andthelastpartoftheentriesintherepository.Ifthe
firsttwocharactersofthelastentitiesarecase-insensitivematching,
we add the entry to a candidate list.
3.3.2 Library Predictor. As the third challenge aforementioned,
similar API entries in different libraries bring difficulties to disam-
biguate the mention. An intuitive way is to take sentence-level
semanticsintoconsideration.Tocapturerichcontextualinforma-
tionfromsentences,wefirsttrainthemostpopularlanguagemodelBERT[
9]withalltrainingsentencesforeachlibrary.Then,onefully
connectedlayerfollowedbyasoft-maxoutputlayerisfine-tuned
to predict the library of input sentences based on the semantic
embedding produced by BERT.
3.3.3 Similarity Computation. Given an API mention ğ‘šand its
candidates ğ‘’, we calculate the similarity score between the API
mention and each candidate. Finally, we rank all candidates based
ontheirsimilarityandselectthemostrelevantcandidateabovethe
threshold. Basically, we compute similarity based on bag similarity.
Given two bags of entities, ğ‘€,ğ¸ğ‘–being split by â€œ.â€ from the API
142
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 07:51:54 UTC from IEEE Xplore.  Restrictions apply. ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA Yintong Huo, Yuxin Su, Hongming Zhang, and Michael R. Lyu
mentionğ‘šand a candidate ğ‘’ğ‘–âˆˆğ‘’, respectively, we compute the
similarity from two aspects, lexical-similarity and entity-similarity.
LexicalSimilarity. Thisstepismotivatedbythefactthatsome-
timesdevelopersmakespellingerrorsinsentences,especiallywhen
the mentioned API name is long. However, even if we make a typo
insomewords,itslexicalmeaning(i.e.,wordrepresentationlearned
from corpus) will not change.
Inspired by Huang et al .[18], we use the Equation 2to calculate
lexical-based similarity between mention entities ğ‘€and entities of
one candidate ğ¸ğ‘–.
ğ‘†ğ‘–ğ‘š ğ¿(ğ‘€â†’ğ¸ğ‘–)=/summationtext.1
ğ‘¤âˆˆğ‘€ğ‘ ğ‘–ğ‘š(ğ‘¤,ğ¸ ğ‘–)âˆ—ğ¼ğ·ğ¹ ğ‘¡ğ‘œğ‘˜ğ‘’ğ‘›(ğ‘¤)/summationtext.1
ğ‘¤âˆˆğ‘€ğ¼ğ·ğ¹ ğ‘¡ğ‘œğ‘˜ğ‘’ğ‘›(ğ‘¤),(2)
whereğ¼ğ·ğ¹ ğ‘¡ğ‘œğ‘˜ğ‘’ğ‘›(ğ‘¤)represents the IDF value of token ğ‘¤in the
trainingdata. ğ‘ ğ‘–ğ‘š(ğ‘¤,ğ¸ ğ‘–)referstothemaximumlexicalsimilarity
score between the element ğ‘¤âˆˆğ‘€and elements in set ğ¸ğ‘–.W e
calculate lexical similarity for pairs of entities by another word
embeddingmodelFastText[ 4].UnlikeWord2Vec,FastTextisthe
embeddingmodelthatincorporatesn-gramfeaturesofatoken,soit
solves the OOV problem. Inversely, we also compute the similarity
ğ‘†ğ‘–ğ‘š ğ¿(ğ¸ğ‘–â†’ğ‘€)by exchanging ğ‘€andğ¸ğ‘–in Equation 3. In the end,
theoveralllexicalsimilarityisformulatedthroughanarithmetic
mean operation:
ğ‘†ğ‘–ğ‘š ğ¿(ğ‘€,ğ¸ ğ‘–)=ğ‘†ğ‘–ğ‘š ğ¿(ğ‘€â†’ğ¸ğ‘–)+ğ‘†ğ‘–ğ‘š ğ¿(ğ¸ğ‘–â†’ğ‘€)
2.(3)
EntitySimilarity. Jaccardsimilaritycoefficient[ 19]iswidely
used in gauging how similar the two sets are. Given two bags of
entitiesğ‘€,ğ¸ğ‘–, we formulate our weighted Jaccard similarity as
Equation 4:
ğ‘†ğ‘–ğ‘š ğ½(ğ‘€,ğ¸ ğ‘–)=/summationtext.1
ğ‘¤âˆˆ(ğ‘€âˆ©ğ¸ğ‘–)ğ¼ğ·ğ¹ ğ‘’ğ‘›ğ‘¡ğ‘–ğ‘¡ğ‘¦(ğ‘¤)
/summationtext.1
ğ‘¤âˆˆğ¸ğ‘–ğ¼ğ·ğ¹ ğ‘’ğ‘›ğ‘¡ğ‘–ğ‘¡ğ‘¦(ğ‘¤), (4)
whereğ¼ğ·ğ¹ ğ‘’ğ‘›ğ‘¡ğ‘–ğ‘¡ğ‘¦(ğ‘¤)represents the IDF value of entity ğ‘¤in the
API repository. IDF provides a standard to measure the salience
ofatoken.a higherIDFvaluerepresentsthatitappearsmorefre-
quently,carryinglowerinformationentropy.Forinstance,inour
repository,tokenssuchas nn,torch,numpycontainalowIDFvalue
sinceit isalmostpresent ineveryentry,buttokens suchas Adap-
tiveMaxPool1d andbinary_cross_entropy deserve more attention,
thusahighIDFvalue.Intuitively,insteadofclassormodulenames,
we always use method names to clarify the mentioned API, which
contains a higher IDF value. Such discriminative tokens contribute
significantlytothis ğ‘†ğ‘–ğ‘š ğ½function,whilemissingamatchin nnjust
makes a minor effect on the entity similarity score.
Overall Similarity. Tosumup,thescoringfunctionforcalcu-
lating similarity is composed of a lexical similarity function and an
entitysimilarityfunction.GivenanAPImention ğ‘šandanentry
ğ‘’ğ‘–, the overall similarity is calculated by Equation 5:
ğ‘†ğ‘–ğ‘š(ğ‘š,ğ‘’ ğ‘–)=ğ‘†ğ‘–ğ‘š ğ¿(ğ‘š,ğ¸ ğ‘–)+ğ‘†ğ‘–ğ‘š ğ½(ğ‘š,ğ¸ ğ‘–), (5)
whereğ‘†ğ‘–ğ‘š ğ¿(ğ‘š,ğ¸ ğ‘–)andğ‘†ğ‘–ğ‘š ğ½(ğ‘š,ğ¸ ğ‘–)are defined above. Toexclude
theAPImentionsthatarewrongpredictionsintroducedfromthe
recognizer, and the API mentions that refer to an API out of our
repository,weeliminatethecandidates ğ‘’ğ‘–âˆˆğ‘’withlower ğ‘†ğ‘–ğ‘š(ğ‘š,ğ‘’ ğ‘–)
valuethanthesimilaritythreshold ğ‘†.Finally,werankallremaining
candidates and choose the ğ‘’ğ‘—âˆˆğ‘’with the highest ğ‘†ğ‘–ğ‘š(ğ‘š,ğ‘’ ğ‘—)as
output.Table 2: Statistics of API repository and Py-mention set.
Library Version #API#Mention #Sentence
PyTorch [12] 1.8.0 2,472 133 562
Tensorflow [14] 2.4.110,361 87 532
Pandas [13] 1.2.4 2,174 117 573
Numpy [11] 1.20 1,913 116 580
Matplotlib [10] 3.4.1 6,937 105 583
Sum -23,857 558 2,830
Table 3: Statistics of training set.
Library #Sentence #Autolabel #Augmentation
PyTorch 150,000 11,057 27,077
Tensorflow 150,000 8,925 21,899
Pandas 150,000 10,537 25450
Numpy 150,000 11,501 27,941
Matplotlib 150,000 9,769 23,564
Sum 750,000 51,789 125,931
4 EXPERIMENTAL SETUP
In this section, we introduce the experimental setup details, includ-
ing data collection, implementation details, and evaluation metrics.
4.1 Data Collection
4.1.1 Text Preparation. Inthispaper,wefocusonfivewidely-used
third-partylibrariesinPython: Pytorch,Pandas,Tensorflow, Numpy,
Matplotlib.Wecrawlallquestionstaggedwithatleastoneofthe
abovelibrariesusingScrapyinStackOverflow.For eachquestion-
answer thread, we collect questions, all answers and their com-
ments.Detailsofthedatapreprocessingmethodaredescribedin
Section3.1.1.
4.1.2 API Repository. WeconstructanAPIrepositorycontaining
allAPIinfivechosenthird-partylibrarieswiththeirentirequalified
names. We use Scrapy to crawl all APIs from their official websites.
Informationsuchastheversionofeachlibrary,thenumberofAPIsineachlibraryislistedinTable 2.Consideringthatparentheses â€œ()â€
arenotthesigntodifferAPIsfromeachother,weremoveall â€œ()â€at
the end of the API entire qualified names (e.g., store numpy.einsum
instead of numpy.einsum() ).
4.1.3 Dataset. Consideringalltextscrawledfromtextpreparation
are too large to cope with, we randomly sample 150,000 sentences
foreachofthelibrariesandtreatthemasunlabeledtrainingdata.Af-terapplyingself-labelinganddataaugmentation,weobtain125,931
sentences for training the recognizer. The distributions of training
data,automaticallyAPIlabels,andaugmentationresultsareshown
in Table3.
For the testing data, we randomly select 600 sentences from
eachlibrary(withoutoverlappingwiththetrainingdata)andask
expertstoannotatethem.Toensureannotationquality,twoinvited
experts both have more than four years of experience in Pythondevelopment and are all familiar with five libraries. Consideringthat a long sentence is more likely to contain API mentions, weselect the testing data sentences longer than ten tokens. During
143
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 07:51:54 UTC from IEEE Xplore.  Restrictions apply. ARCLIN: Automated API Mention Resolution for Unformatted Texts ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
the annotation, given the whole API repository, experts are asked
to annotate whether each token in a sentence is referring to an
API in the repository or not. If yes, they need to write down the
entirequalifiednameofanAPImention.Wealsoaskannotatorsto
throw away the sentence if they are not confident at what it refers
to.Inthisway,wecollect2,830sentenceswith558APImentions
from five libraries in total, where their distributions along with the
repositoryâ€™s distribution are shown in Table 2. Typical examples
are below:
â€¢If you donâ€™t want to export, please uncomment plt.show()
[matplotlib.pyplot.show()] and remove ...
â€¢Iâ€™veusuallygottengoodperformanceoutofnumpyâ€™s einsum
[numpy.einsum()] function and I like ...
â€¢Here is a way to do it using stack[torch.stack()] orunbind
[torch.unbind()].
Here,black italic fonts indicates API mentions and blue italic
fontsin brackets are the linked APIs in the repository (with entire
qualified names).
4.2 Implementation Details
Inthedatapreprocessingperiod,wetrainaskip-gramWord2Vec
model based on our corpus with gensim [ 28]. We also train a
FastText word embedding model with gensim [ 28] for comput-
ing<mention, entry >pair lexical-based similarity. The embedding
size for Word2Vec and FastText models are set to 300. Two models
are trained for ten epochs5.ğ¼ğ·ğ¹ ğ‘¡ğ‘œğ‘˜ğ‘’ğ‘›andğ¼ğ·ğ¹ ğ‘’ğ‘›ğ‘¡ğ‘–ğ‘¡ğ‘¦are trained on
the training data.
For the API recognition part, we use an open-source natural
sequencelabelingtoolfrom[ 39]astheimplementationandtrain
therecognizerontheaugmenteddata.Thecharacterembedding
size is set to 30, and the layer number of Bi-LSTM is set to one. We
traintherecognizerwiththelearningrateas0.001forfiveiterations.
We choose five paths with the highest confidence score in the CRF
layer,andtreatatokenasanAPImentionifandonlyifitispredicted
so in at least two out of five paths ( ğ‘‡ğ‘œğ‘_ğ‘ƒ=5,ğ¾=2). For the API
linker, we train our library predictor with Transformer [ 36] for ten
iterationswiththelearningrateof0.001.Thedefaultthreshold ğ‘†
for the scoring function is 1.1 unless we specify them with other
values.
4.3 Baselines
To the best of our knowledge, there is no existing work focusing
onextractingAPIlinksfromunformattedtexts.Wecompareour
method with the following baselines: APIReal is the most relevant
work to ours but they mine APIs from StackOverflow posts, and
the other two baselines are rule-based.
4.3.1 APIReal. Ye et al.[40]proposed the model named APIReal,
which predicted API recognition and linking in a StackOverflow
post.APIRealcontainstwostagessimilartoours:arecognizerto
extract API mentions and a linker to link API to the repository. In
therecognizer,theymanuallylabeledthetrainingdatatolearnAPI
mentions by feeding human-crafted features into a CRF model. In
the linker, they utilized external information, such as the question
5Word2Vec and FastText models converge before ten epochs.title,contentsinthecodeblock, <code>tags,and URLsinapost
to predict what an API mention links to.
Whenimplementingthisbaseline,weâ€œcounterfeitâ€afilecrawled
from StackOverflow in the same input format, where each line is a
sentencefromourtestset.Inthisway,APIRealwilltreatourfile
as a post from StackOverflow and continuously processes them.
Moreover,asthedatabaseofAPIRealincludesthreeoffivelibraries
comparingtoours(i.e.,Pandas,Numpy,Matplotlib),wecompute
Precision, Recall, and F1 scores on the three libraries.
4.3.2 RuleBase-Pure. We also include a pure Rule-based approach
as the baseline. Specifically, we check whether each token in the
sentenceisthesameasanentryintheAPIrepository.Thisbaseline
provides us with insights into the quality of written API mentions
in StackOverflow.
4.3.3 RuleBase-Knowl. We also include a Rule-based approach
withpriorknowledgeasabaseline.Here,priorknowledgerefersto
the common writing behaviors for API mentions in StackOverflow.
Specifically,wereplace â€œnpâ€withâ€œnumpyâ€, â€œpdâ€withâ€œpandasâ€, â€œtfâ€
withâ€œtensorflowâ€ for each token, respectively.
4.4 Evaluation Metrics
For fair comparison, we use Precision, Recall, and F1scores to eval-
uateARCLINâ€™sperformanceinourtestset,whichisalsousedby
all previous works [ 2,7,40]. Specifically, precision means what
percentage of API linking predictions are correct, recall means
what percentage of the real API mentions are covered, and F1 is
the harmonic mean of precision and recall.
5 EXPERIMENTAL RESULTS
Inthissection,wediscusstheperformanceofARCLINmodelby
divingintothreeresearchquestionsfromSection 5.1toSection 5.3:
(1)HoweffectiveisARCLIN? WecompareARCLINtothree
baselinesintheproposedtestset.TheresultshowsthatARCLIN
outperformsbaselinesbylargemargins,eventhoughitisfreefrom
any labor-intensive annotations and handcrafted rules.
(2) How effective are the components of ARCLIN? The
devised framework is made up of an API recognizer and an API
linker. The latter one includes a library predictor and a scoring
functionbalancebetweenthelexicalsimilarityandentitysimilarity.
Toevaluatethecontributionofeachcomponent,wediscardeach
element at one time and implementthe remaining part in our test
set. Details of analysis are provided along with the experiment
results.
(3) What is the generalization ability of ARCLIN? Con-
sidering the large number of libraries in the real world, we are
interestedinhowARCLINperformsinminingAPIsinsideanun-
seenlibrary.Toexploreitsgeneralizationability,wetrainthemodel
in one library and test it in another library.
5.1 RQ1: How effective is ARCLIN?
ARCLIN aims to automatically extract API mentions from free text
sentences and link them to an entry in the repository. Thus, to
prove its effectiveness, we evaluate ARCLIN in sentences selected
fromStackOverflowposts.WefeedtestsentencesintotheARCLIN
model and examine whether it could mine correct APIs.
144
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 07:51:54 UTC from IEEE Xplore.  Restrictions apply. ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA Yintong Huo, Yuxin Su, Hongming Zhang, and Michael R. Lyu
Table 4: Experimental Results.
Approach Precision Recall F1
RuleBase-Pure 1.00 0.0700.131
RuleBase-Knowl 1.00 0.3140.478
APIReal 0.787 0.6040.683
- w/o rules 0.823 0.4770.599
ARCLIN Ensemble 0.784 0.7420.762
- PyTorch 0.861 0.8010.830
- Tensorflow 0.576 0.8400.683
- Pandas 0.717 0.7780.746
- Numpy 0.865 0.7410.798
- Matplotlib 0.825 0.7620.792
To answer this question, we compare ARCLIN with a current
state-of-the-artbaselinenamedAPIReal[ 40],apurelyrule-based
approachRuleBased-Pure,andarule-basedapproachincorporating
priorknowledgenamedRuleBased-Knowl.Experimentalresultsare
shownin Table 4. Apartfrom theoverallperformance ofARCLIN
inour wholetestset (in ARCLINEnsemble),we alsoexaminethe
performance in every single library, shown in the following five
rows.TheresultsindicateourARCLINsignificantlyoutperforms
allotherbaselines.Fromtheresults,weseethatourARCLINmodel
canachieve78.41%,74.19%,and76.24%inprecision,recall,andF1
score, respectively.
It is worthy to notice that RuleBase-Pure only retrieves 6.99% of
all API mentions, reflecting that developers rarely write the entire
qualifiednamewhentheymentionsomeAPIs.Thisisalsooneof
the motivations of this work. The RuleBase-Knowl model provides
better performance with the help of prior knowledge. However,if we want to extend the model to a large number of libraries, itis implausible for researchers to enumerate all possible abbrevi-ations for each library. Although the model gives an acceptable
performance,itcanhardlybeusedextensively.Anotherbaseline
APIRealreachestheF1scoreof0.683,whichislowerthantheperfor-
manceintheirdataset.Weattributetheunfavorableperformance
toseveralreasons:(1)Theconstraintsofhandcraftedpatternsin
resolvingcustomizedvariables.Asillustratinginthesecondmor-
phologicalchallenge,theunprofessionaldevelopersusuallywrite
downAPImentionswithcustomizedvariables(a.reshape )oraliases
(np.reshape ). Since APIReal leverages a collection of pre-defined
rules to solve the problem (e.g., npfornumpy), the customized
variablesoruncommonaliasesoutsidethescopeleadtomistakes.
The impact of such handcrafted rules is quantitated in the w/o rule
lineinTable 4.(2)DifficultyinminingAPIsinfreetexts.APIReal
leverages <code>tagsinitsrecognizer;thus,oncesomeonewrites
down API mentions in such tags, APIReals can easily extract them.
But our dataset does not contain such signs to help the recognizer
find out API mentions. (3) Insufficient information. APIReal uti-
lizesinformationfromsourceStackOverflowposts,suchas URLs,
question titles, and code snippets. However, mining APIs from sen-
tences in our task requires the model to capture a richer semantic
meaning.
ARCLIN reaches the highest performance among the three base-
lines. We conclude the reasons as follows: (1) ARCLIN owns the
Figure 3: P-R curve of different hyper-parameters ğ¾andğ‘†.
Table 5: Effectiveness of components in ARCLIN.
Precision Recall F1
ARCLIN (ensemble) 0.784 0.7420.762
- w/o recognizer 0.112 0.7830.195
- w/o lib_pred 0.649 0.7150.680
- w/o lexical_sim 0.814 0.4390.570
- w/o entity_sim 0.645 0.7190.680
recognizerthatkeepsallpossibleAPImentionsbyselectingthetopfivepathsinCRFandconductvoting.Inthisway,ARCLINwillnot
misstoomanyAPImentions;(2)ARCLINâ€™slibraryindicatorpro-
videsscopeforlibraryselection,preventingitfrom linkingtothe
entryfromwronglibraries;(3)ARCLINâ€™sscoringfunctionbalances
the lexical similarity and spelling similarity, so small variations of
an APIâ€™s name will not affect its final prediction.
Inaddition tothe goodperformance, anotheradvantage ofAR-
CLIN is its flexibility. Figure 3provides a precision-recall curve to
showhowtheperformanceisaffectedbythehyperparameters ğ¾
andğ‘†. Each curve CRF-Kin the figure represents a token will be
considered as an API mention if ğ¾(0â‰¤ğ¾â‰¤5)out of five paths
predict it so. Each point in a curve is ARCLINâ€™s performance under
a similarity threshold ğ‘†(0â‰¤ğ‘†â‰¤2). A higher-scoring threshold
means a matched <mention, entry >requires a higher similarity.
Generally,ahigherprecisionoccurssimultaneouslywithalower
recallrate.ARCLINisabletoachieve100%precisionunderalow
recall rate. Therefore, we can customize the threshold under differ-
entscenarios.Thefigurealsoshowsthatwecannotachieve100%
recalleventheprecisiongetsdowntozero.Weascribethesituationintothefollowingreason:Comparedwithcharacter-disorder,word-disorderistoocomplexforARCLINtodealwith.Forexample,when
torch.nn.BCEWithLogitsLoss is written as BCELosswithlogits,e v e ni f
ARCLINnarrowsdowncandidatesintothecorrectlibrary,itishard
forARCLINtoconductAPIlinkingwitheachother.Toconclude,
afterevaluatingARCLINinourtestsettheexperimentresultshows
that it outperforms baselines by large margins, even though it is
free from any labor-intensive annotations and handcrafted rules.
145
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 07:51:54 UTC from IEEE Xplore.  Restrictions apply. ARCLIN: Automated API Mention Resolution for Unformatted Texts ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
Table 6: The generalization ability of ARCLIN. P, R and F1 refers to precision, recall and F1 score respectively.
TrainingTesting
PyTorch Tensorflow Pandas Numpy Matplotlib
PRF 1 PRF 1 PRF 1 PRF 1 PRF 1
PyTorch --- 0.2890.753 0.418 0.4550.650 0.535 0.4720.741 0.576 0.8520.657 0.742
Tensorflow 0.4270.710 0.533 --- 0.4200.701 0.526 0.4220.768 0.544 0.7380.752 0.745
Pandas 0.4720.634 0.541 0.2840.753 0.412 --- 0.4690.741 0.574 0.8330.667 0.741
Numpy 0.4220.664 0.516 0.2220.803 0.348 0.3650.727 0.486 --- 0.8170.724 0.768
Matplotlib 0.4490.641 0.528 0.2680.765 0.397 0.4150.684 0.516 0.4540.741 0.563 ---
5.2 RQ2: How effective are the components of
ARCLIN?
ARCLINiscomprisedofan APIrecognizer andanAPIlinkerwitha
librarypredictor andascoringfunctionbalancebetweenthe lexical
similarity andentitysimilarity.Toinvestigatethecontributionof
each module, we discard each component at a time, implement the
modelinourtestset,andanalyzeitsperformance.Experimental
results of this ablation study are shown in Table 5, where each
rowbelowARCLIN(ensemble)representstheresultofamissing
component. In the w/o lexical_sim and w/o entity_sim setting, we
set the scoring threshold to 0.6. Generally, the missing module
negativelyaffectsthemodelâ€™sperformancemoreorless.Wewill
discuss the effects in the following paragraphs respectively.
5.2.1 NO Recognizer. In this setting, the model tries to link an
entry in the API repository for each token. The precision perfor-manceisdramaticallydecreasingbecausethemajorityoftokensin a sentence are not API mentions, but they can still be linkedto an API in the repository because of their high similarity. Forinstance, the common word wherehas a high similarity with the
APInumpy.where() becausebothofthemcontainâ€œwhereâ€within
the token, but it is not an API mention. ARCLIN made lots of such
mistakes, causing low precision.
5.2.2 NO Library Predictor. In this setting, the model tries to gen-
eratecandidatesfromallfivelibraries,neglectingthesentencecon-
text information. The failure occurs when different libraries have a
method with similar names. For instance, PyTorch has the method
torch.stack() while Numpy also contains the method numpy.stack(),
if a developer only writes â€œstackâ€as the API mention, the model
cannot disambiguate the token.
5.2.3 NO Lexical Similarity. Withoutthelexicalsimilarity,thescor-
ingfunctionfullyreliesontheentitysimilarity.A <mention,entry >
pairwillbelinkedifandonlyifsomeentitieswithinthemareex-
actly the same. This approach provides a high precision rate, since
it issimilar toan advanced rule-basedalgorithm. However, itcan-
not deal with spelling errors. For example, np.zeros() will be linked
withnumpy.zeros() because both of them has the entity â€œzerosâ€,
butnumpy.zeros() cannotmatchedwith np.zero(),eveniftheAPI
mention contains only one missing character.
5.2.4 NO Entity Similarity. In this case, the lexical similarity is de-
terminativetothescoringfunction.Thisfunctionworksfineinmost
cases,butfallingshortwhenanAPImentionreferstoalongAPIname. For instance, the API mention tf.layers.batch_normalization
hasahighersimilarityscorewith tf.keras.layers.BatchNormalization()
ratherthan tf.compat.v1.layers.batch_normalization().Fromalexical
perspective, batch_normalization is not far away from BatchNor-
malization,sothefinalscoringfunctionwilleasilybeaffectedby
other factors (i.e., missing module name in this example).
5.3 RQ3: What is the generalization ability of
ARCLIN?
Considering the large number of libraries even for one program-ming language, we are interested in the generalization ability of
ARCLIN. A promising API mining model should have the ability to
mine APIs without training on the library-specific corpus.
Toanswerthequestion,wetraintherecognizerandlinkerinone
librarycorpus,thenthemodelattemptstorecognizeandidentify
APIs of another library in our test set. In this setting, the model
neverseesthenewlibrarybefore,sothelibrarypredictorisremoved
from ARCLIN.
Table6shows the generalization ability of each pair of libraries.
The experimental results show ARCLIN gains the generalization
abilityto someextent.The experimentsfurtherindicates thatthe
transferred model evaluated with Matplotlib achieves a higherperformance.Forinstance,themodelthathasbeentrainedfrom
Numpy,isabletocorrectlyrecognize81.7%MatplotlibAPIs,accord-
ingtotheTable 6.Weascribethereasontothedistinctivenessof
APIâ€™snameinMatplotlib.Specifically,APInamesinMatplotlib(e.g.,
matplotlib.pyplot.pcolormesh() are rather different from APIs in sci-
entificcomputinglibraries(e.g., numpy.zeros() ortorch.zeros() ),so
the model is free from mistakenly linking to APIs in other libraries.
Generally, the transferred models contain a better recall rate
ratherthanprecision,andwediscussthereasonasfollows.Without
library predictor, ARCLIN may link API mentions to the wrong
library if they contain similar method names. For example, given a
sentence â€œI have trouble with concatenating a list of tensors using
PyTorchâ€™s stack.â€ where â€œstackâ€here is labeled as torch.stack() in
ground truth during the testing phase. If we train the model inPandas and evaluate its generalization ability in Numpy library,
ARCLINwilllink â€œstackâ€totheAPI numpy.stack().Insummary,the
experimentresultsdemonstratetheeffectivenessandrobustnessof
generalization ability. Such library-transferred experiment mimics
the real-world scenario of applying ARCLIN to mine APIs from
unseen libraries.
146
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 07:51:54 UTC from IEEE Xplore.  Restrictions apply. ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA Yintong Huo, Yuxin Su, Hongming Zhang, and Michael R. Lyu
Table 7: Case study.
Approach Rule-K APIReal ARCLIN
API Mention -Recog Link Recog Link
figure.add_suplot   
ax.set_major_locator   
ticker.MultipleLocator   
plt.show()   
6 CASE STUDY
In this section, we dive into three cases to specify why ARCLIN
outperforms APIReal and Rule-K (i.e., RuleBase-Knowl), where
ground-truth is shown in blue italic font. The experimental results
offourAPImentions(inblue)arepresentedinTable 7withrespect
to the two phases (i.e., API Recognizer and API Linker6). One API
mention is successfully identified and resolved if and only if the
â€œLinkâ€ phase gives the correct answer ( /check).
â€¢Isthereamoreconvenientalternativeto figure.add_suplot
[matplotlib.figure.Figure.add_subplot()] if I have multiple fig-
ures ...
â€¢Youmaytrytickingthemajoraxisusing ax.set_major_locator
[matplotlib.axis.Axis.set_major_locator()] calledwith ticker.
MultipleLocator() [matplotlib.ticker.MultipleLocator] .
â€¢If you donâ€™t want to export, please uncomment plt.show()
[matplotlib.pyplot.show()] and remove ...
Compared ARCLIN with Rule-K and APIReal, we categorize the
characteristicsforthreeapproaches.Firstly,Rule-Kcanonlyresolve
the API mention with the qualified name or a collection of specific
abbreviations, depending on the handcrafted rules. For instance,if we add the common writing behavior that a developer usuallycallsmatplotlib.pyplot by its alias plt, Rule-K will try to replace
the alias with its original name for each token, then find if thenew token matches a fully qualified API name in the repository.
Secondly, we observe that APIReal is more flexible than rule-based
matching algorithm, by uncovering some API mentions by therecognizer (e.g., ax.set_major_locator andticker.MultipleLocator ),
allowing it to address the first common-word ambiguity challenge.
Nevertheless, its API Linker is not perfect to resolve the ambiguity
introduced by morphological mentions, mainly comes from the
customizedname,suchas axorticker.APIRealdetectsaliasesby
handcrafted patterns (e.g., pdforpandas), thus the alias that is
notcoveredbyruleswillbeinappropriatelycopedwith.Lastbut
notleast,thecasesdemonstratetheeffectivenessofARCLIN.The
carefully devised API recognizer enables it to detect API mentions
in unformattedtext. Besides, theAPI Linker withentity similarity
forcesthemodeltopayattentiontotheinformativeentities(e.g.,
set_major_locator ),andthelexicalsimilarityallowsittoaddressthe
misspelling in API mentions. Therefore, ARCLIN can even resolve
thefigure.add_suplot tomatplotlib.figure.Figure.add_subplot() even
if the mention leaves out the letter â€œbâ€.
6API Recognizer is denoted as Recog and API Linker is denoted as Link for space
limitation.7 THREAT TO VALIDITY
In this section, we discuss three potential threats to the validity of
ARCLINandprovideoursolutionstoalleviatethesethreats.The
first one is the potential bias brought by manual annotation of the
data. We evaluate ARCLIN the Py-mention dataset, which is anno-
tatedbytwodifferentannotators.Toovercomethehumanbiasandensurethedataquality,wenotonlyemploydomainexpertsinsteadofcrowd-sourcingworkers,butalsothrowawaythesentenceswith
uncertainty.Annotationexamplesandguidelinesareprovidedat
first.Asaresult,theannotatorsfullyunderstandwhattheyneed
to do and keep confidence in their annotation.
Thesecondoneis thelimitedrecallrate.As showninFigure 3,
the recall cannot achieve 100% regardless of the threshold. In other
words, ARCLIN cannot cover all ground-truth labels. We owe this
recalllimitation tothe reasonsofobserved worddisorder inmen-
tions.ARCLINcomputessimilaritybasedonlexical-levelandentity-
level, but it fails in comparing <mention, entry >pairs in word
disorder. For example, if we use BCELosswithlogits to represent
torch.nn.BCEWithLogitsLoss,thesimilarityscorefromARCLINis
close totorch.nn.BCELoss, therefore, the final output gets pertur-
bation by other factors. To alleviate the issue, an n-gram based
similarity can be used to extend our ARCLIN model.
Thethirdthreatisstyleconstraint.Currently,weevaluateAR-
CLINwithfivePythonlibrariesandachievepromisingperformance,
but if we migrate the model to other programming languages, theinconsistencyof functioncallingformatwill introducethisthreat.
For instance, in C++ language, we use double colon â€œ::â€ to call a
static function or declare the namespace identification. Besides, to
call a function in a class, one may use â€œ- >â€ from a pointer or use
the node â€œ.â€ from a C++ entity. ARCLIN uses â€œ.â€ to split the APIâ€™s
entirequalifiednameintoabagofpackageentitiesforsimilarity
computation.IfweimplementARCLINinanotherlanguage(e.g.,
C++), it is necessary to implement new split marks.
8 RELATED WORKS
APIRecognition. If we want to link an API to some other source,
thefirststepistorecognizeAPIs.Inthispaper,weusearecognizertorecognizeAPIsinfreetextsentences.DagenaisandRobillard
[7]
adopted partial program analysis (PPA) to parse Java snippets and
thenextractscode-liketermsininformaldiscussions.Thedifference
between theirs and ours is, our paper targets extracting APIs from
naturallanguagesentences,buttheabovestudieswereaboutex-
tractingAPIsfromcodeblocks(writteninfreetexts).Bacchellietal .
[2]employed arule-based approach to extractAPI mentions from
e-mails by designing different regular expressions applicable to
different languages.Treude and Robillard [35]suggested different
regular expressions for question and body to extract API mentions
from StackOverflow posts. Rigby and Robillard [31]used island
grammars to identify code elements from free text with the help of
compoundcamelcasedtermswhileignoringthecommon-wordam-
biguity. The most relevant research to us is APIReal [ 40], but their
approachwasapplicabletorecognizingAPIsfromStackOverflow
postswith <code>tags,whichwasmucheasierthanoursetting.
Besides,insteadoflinkingsuchfinegranularityAPIs,researchers
alsoexploredlinkingbetweentextualdocumentsandcodeartifacts
for maintenance. Some works Antoniol et al .[1], Chen[6], Marcus
147
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 07:51:54 UTC from IEEE Xplore.  Restrictions apply. ARCLIN: Automated API Mention Resolution for Unformatted Texts ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
and Maletic [23], Marcus et al .[24]used information retrieval (IR)
techniquesorleverageLatentSemanticIndexing(LSI)torecover
traceabilitylinksbetweenelementsinnaturallanguagedocumen-
tation and source code in software systems. These studies were
different from this paper since they performed a coarse granularity
linking.
API linking. â€œLinkingâ€ can refer to linking code artifacts to
documents, or linking APIs from free-texts to its entire qualified
namein therepository.Regarding tolinkingcode artifactstodoc-
uments,Bacchellietal .[2]usedtwostring-matchinformationre-
trievaltechniques(i.e.,vectorspacemodelandLSI)tolinkdetected
APIs from e-mails to source code artifacts. The latter category is
whatwehavedoneinthispaper,themainideaofmatchingAPIs
with their entire qualified name is how to conduct the disambigua-
tion. Dagenais and Robillard [7]suggested a set of filtering heuris-
tics to disambiguate the API mentions. Ye et al .[40]disambiguated
APImentionsinaStackOverflowpostbyutilizinginformationin
code blocks, question titles, and the location where URLspoints
to. The first paper did not address the common word polysemy,
whilethesecondresearchmitigatedthemorphologicalchallenge
by labor-intensive rules, which was different from ours.
Mining Technical Forums. Nowadays,manyresearchersde-
vote themselves to mining knowledge from technical forums (e.g.,
StackOverflow)tofacilitatedevelopersintheirprogrammingissues.
Forexample,apopularscenarioisAPIrecommendation[ 18,26,37],
thesepaperssuggestedalistof APIclassesforanaturallanguage
query by mining StackOverflow posts. Specifically, given a natural
languagequery,Huangetal .[18]firstlysearchedthemostrelevant
50questionsandextractingAPIsfromposts.Then,itrankedallcan-
didate APIs by considering the query-title similarity and title-APIs
similarity. Li et al .[21]proposed another application that explores
API caveat in such a technical forum and presented a system to
help developers to tackle the problem of negative usage of APIs. It
isnoticedthatmanyworksstudiedinStackOverflowtalksabout
APIs,our workserves asafoundation ofthis workforfacilitating
themtorecognizeandidentifytheAPIswithouttheentirequalified
name.
9 CONCLUSION
In this paper, we propose a novel framework ARCLIN for recogniz-
ingAPImentionsfromfreetextandlinkingtoanAPIrepository.
ARCLINiscomposedoftwocomponents,anAPIrecognizerand
an API linker. The API recognizer extracts API mentions from free
textsandtheAPIlinkerdisambiguatestheAPImentionsbyalibrary
predictor to address reference ambiguity, and a scoring function
incorporating lexical similarity and entity similarity. After training
the model in an unlabeled StackOverflow corpus, we implement
ARCLIN in a human-annotated dataset named Py-mention, the
experimental results demonstrate that it significantly outperforms
allbaselines.Moreover,theexperimentaboutgeneralizationability
demonstrates that ARCLIN can extract APIs from a new library
even though ARCLIN is trained from another libraries.
10 ACKNOWLEDGEMENT
TheworkwassupportedbytheGuangdongKeyResearchProgram(No.2020B010165002)andtheResearchGrantsCounciloftheHong
Kong Special Administrative Region, China (CUHK 14210920).REFERENCES
[1]Giuliano Antoniol, Gerardo Canfora, Gerardo Casazza, Andrea De Lucia, and Et-
toreMerlo.2002. Recoveringtraceabilitylinksbetweencodeanddocumentation.
IEEE transactions on software engineering (TSE) 28, 10 (2002), 970â€“983.
[2]Alberto Bacchelli, Michele Lanza, and Romain Robbes. 2010. Linking e-mails
and source code artifacts. In Proceedings of the 32nd ACM/IEEE International
Conference on Software Engineering-Volume 1 (ICSE). 375â€“384.
[3]StevenBird,EwanKlein,andEdwardLoper.2009. Naturallanguageprocessing
withPython:analyzingtextwiththenaturallanguagetoolkit. "Oâ€™ReillyMedia,
Inc.".
[4]Piotr Bojanowski, Edouard Grave, Armand Joulin, and Tomas Mikolov. 2017.
Enrichingwordvectorswithsubwordinformation. TransactionsoftheAssociation
for Computational Linguistics (TACL) 5 (2017), 135â€“146.
[5]Chunyang Chen, Zhenchang Xing, and Ximing Wang. 2017. Unsupervised
software-specificmorphologicalformsinferencefrominformaldiscussions.In
Proceedingsofthe39thIEEE/ACMInternationalConferenceonSoftwareEngineering
(ICSE). IEEE, 450â€“461.
[6]Xiaofan Chen. 2010. Extraction and visualization of traceability relationships
betweendocumentsandsourcecode.In ProceedingsoftheIEEE/ACMinternational
conference on Automated software engineering (ASE). 505â€“510.
[7]BarthÃ©lÃ©my Dagenais and Martin P Robillard. 2012. Recovering traceability linksbetweenanAPIanditslearningresources.In Proceedingsofthe34thInternational
Conference on Software Engineering (ICSE). IEEE, 47â€“57.
[8]Marie-CatherineDeMarneffe,BillMacCartney,ChristopherDManning,etal .
2006. Generating typed dependency parses from phrase structure parses.. In
ProceedingsoftheEleventhInternationalConferenceonLanguageResourcesand
Evaluation (LREC), Vol. 6. 449â€“454.
[9]JacobDevlin,Ming-WeiChang,KentonLee,andKristinaToutanova.2019. BERT:Pre-trainingofDeepBidirectionalTransformersforLanguageUnderstanding.InProceedingsofthe2019ConferenceoftheNorthAmericanChapteroftheAssociation
for Computational Linguistics: Human Language Technologies, (NAACL-HLT).
Association for Computational Linguistics, 4171â€“4186.
[10]Matplotlib Documentation. Retrieved in 2021. https://matplotlib.org/stable/
contents.html
[11]NumpyDocumentation.Retrievedin2021. https://numpy.org/devdocs/reference/
index.html
[12]PyTorch Documentation. Retrieved in 2021. https://pytorch.org/docs/stable/
index.html
[13]Pandas Documentation. Retrieved in 2021. https://pandas.pydata.org/docs/
reference/index.html#api
[14]Tensorflow Documentation. Retrieved in 2021. https://www.tensorflow.org/api_
docs/python/tf
[15]WeiFuandTimMenzies.2017. Easyoverhard:Acasestudyondeeplearning.In
Proceedings of the 11th the ACM Joint European Software Engineering Conference
and Symposium on the Foundations of Software Engineering (ESEC/FSE). 49â€“60.
[16]Alex Graves, Abdel-rahman Mohamed, and Geoffrey Hinton. 2013. Speech
recognition with deep recurrent neural networks. In International Conference on
Acoustics,Speec h andSignalProcessing, Vancouver, BC,Canada,May 26-31,2013 .
IEEE, 6645â€“6649. https://doi.org/10.1109/ICASSP.2013.6638947
[17]MatthewHonnibal,InesMontani,SofieVanLandeghem,andAdrianeBoyd.2020.
spaCy: Industrial-strength Natural Language Processing in Python.
[18]QiaoHuang,XinXia,ZhenchangXing,DavidLo,andXinyuWang.2018. API
methodrecommendationwithoutworryingaboutthetask-APIknowledgegap.InProceedingsofthe33rdIEEE/ACMInternationalConferenceonAutomatedSoftware
Engineering (ASE). IEEE, 293â€“304.
[19]Paul Jaccard. 1912. The distribution of the flora in the alpine zone. 1. New
phytologist 11, 2 (1912), 37â€“50.
[20]JohnD.Lafferty,AndrewMcCallum,andFernandoC.N.Pereira.2001. Condi-
tionalRandomFields:ProbabilisticModelsforSegmentingandLabelingSequence
Data. InProceedings of the Eighteenth International Conference on Machine Learn-
ing (ICML). Morgan Kaufmann, 282â€“289.
[21]JingLi,AixinSun,ZhenchangXing,andLeiHan.2018. APICaveatExplorerâ€“
Surfacing Negative Usages from Practice: An API-oriented Interactive Ex-
ploratorySearchSystemforProgrammers.In Proceedingsofthe41stInternational
ACMSIGIRConferenceonResearch&DevelopmentinInformationRetrieval(SIGIR).
1293â€“1296.
[22]Xuezhe Ma and Eduard Hovy. 2016. End-to-end Sequence Labeling via Bi-directionalLSTM-CNNs-CRF.In Proceedingsofthe54thAnnualMeetingofthe
AssociationforComputationalLinguistics(ACL).AssociationforComputationalLinguistics, 1064â€“1074.
[23]
Andrian Marcus and Jonathan I Maletic. 2003. Recovering documentation-to-
source-codetraceabilitylinksusinglatentsemanticindexing.In Proceedingsof
the 25th International Conference on Software Engineering (ICSE). IEEE, 125â€“135.
[24]Andrian Marcus, Jonathan I Maletic, and Andrey Sergeyev. 2005. Recovery of
traceabilitylinksbetweensoftwaredocumentationandsourcecode. International
JournalofSoftwareEngineeringandKnowledgeEngineering 15,05(2005),811â€“836.
148
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 07:51:54 UTC from IEEE Xplore.  Restrictions apply. ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA Yintong Huo, Yuxin Su, Hongming Zhang, and Michael R. Lyu
[25]Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013.
Distributed representations of words and phrases and their compositionality.
arXiv preprint arXiv:1310.4546 (2013).
[26]Mohammad Masudur Rahman, Chanchal K Roy, and David Lo. 2016. Rack:
Automatic api recommendation using crowdsourced knowledge. In Proceedings
of the 23rd IEEE International Conference on Software Analysis, Evolution, and
Reengineering (SANER), Vol. 1. IEEE, 349â€“359.
[27]LevRatinovandDanRoth.2009. Designchallengesandmisconceptionsinnamed
entityrecognition.In Proceedingsofthe13thConferenceonComputationalNatural
Language Learning (CoNLL). 147â€“155.
[28]Radim Å˜ehÅ¯Å™ek and Petr Sojka. 2010. Software Framework for Topic Modelling
withLargeCorpora.In ProceedingsoftheLREC2010WorkshoponNewChallenges
for NLP Frameworks. ELRA, 45â€“50.
[29]Xiaoxue Ren, Jiamou Sun, Zhenchang Xing, Xin Xia, and Jianling Sun. 2020.
Demystify official API usage directiveswith crowdsourced API misuse scenarios,
erroneous code examples and patches. In Proceedings of the ACM/IEEE 42nd
International Conference on Software Engineering (ICSE). 925â€“936.
[30]Xiaoxue Ren, Xinyuan Ye, Zhenchang Xing, Xin Xia, Xiwei Xu, Liming Zhu, and
JianlingSun.2020. API-MisuseDetectionDrivenbyFine-GrainedAPI-Constraint
KnowledgeGraph.In 202035thIEEE/ACMInternationalConferenceonAutomated
Software Engineering (ASE). IEEE, 461â€“472.
[31]Peter C Rigby and Martin P Robillard. 2013. Discovering essential code elements
in informal documentation. In Proceedings of the 35th International Conference on
Software Engineering (ICSE). IEEE, 832â€“841.
[32]Martin Sundermeyer, Ralf SchlÃ¼ter, and Hermann Ney. 2012. LSTM neural
networksforlanguagemodeling.In AnnualConferenceoftheInternationalSpeech
CommunicationAssociation,Portland,Oregon,USA,September9-13,2012.ISCA,
194â€“197. http://www.isca-speech.org/archive/interspeech_2012/i12_0194.html
[33]Jeniya Tabassum, Mounica Maddela, Wei Xu, and Alan Ritter. 2020. Code and
NamedEntityRecognitioninStackOverflow.In Proceedingsofthe58thAnnual
MeetingoftheAssociationforComputationalLinguistics(ACL).Associationfor
Computational Linguistics, 4913â€“4926.
[34]ErikF.TjongKimSangandFienDeMeulder.2003. IntroductiontotheCoNLL-
2003 Shared Task: Language-Independent Named Entity Recognition. In Pro-
ceedings of the 7th Conference on Natural Language Learning at HLT-NAACL.
142â€“147.[35]Christoph Treude and Martin P Robillard. 2016. Augmenting api documenta-tion with insights from stack overflow. In Proceedings of the 38th IEEE/ACM
International Conference on Software Engineering (ICSE). IEEE, 392â€“403.
[36]ThomasWolf,LysandreDebut,VictorSanh,JulienChaumond,ClementDelangue,
Anthony Moi, Pierric Cistac, Tim Rault, RÃ©mi Louf, Morgan Funtowicz, Joe
Davison,SamShleifer,PatrickvonPlaten,ClaraMa,YacineJernite,JulienPlu,
CanwenXu,TevenLeScao,SylvainGugger,MariamaDrame,QuentinLhoest,
and Alexander M. Rush. 2020. Transformers: State-of-the-Art Natural Language
Processing.In Proceedingsofthe2020ConferenceonEmpiricalMethodsinNatural
Language Processing: System Demonstrations. Association for Computational
Linguistics, 38â€“45.
[37]WenkaiXie,XinPeng,MingweiLiu,ChristophTreude,ZhenchangXing,Xiaoxin
Zhang, and Wenyun Zhao. 2020. API method recommendation via explicitmatching of functionality verb phrases. In Proceedings of the 28th ACM Joint
Meeting on European Software Engineering Conference and Symposium on the
Foundations of Software Engineering. 1015â€“1026.
[38]Bowen Xu, Deheng Ye, Zhenchang Xing, Xin Xia, Guibin Chen, and Shanping Li.2016. Predictingsemanticallylinkableknowledgeindeveloperonlineforumsvia
convolutional neural network. In Proceedings of the 31st IEEE/ACM International
Conference on Automated Software Engineering (ASE). IEEE, 51â€“62.
[39]Jie Yang and Yue Zhang. 2018. NCRF++: An Open-source Neural Sequence
Labeling Toolkit. In Proceedings of the 56th Annual Meeting of the Association for
Computational Linguistics (ACL).
[40]DehengYe,LingfengBao,ZhenchangXing,andShang-WeiLin.2018. APIReal:
an API recognition and linking approach for online developer forums. Empirical
Software Engineering (ESE) 23, 6 (2018), 3129â€“3160.
[41]Deheng Ye, Zhenchang Xing, Chee Yong Foo, Zi Qun Ang, Jing Li, and Nachiket
Kapre.2016. Software-specificnamedentityrecognitioninsoftwareengineeringsocialcontent.In Proceedingsofthe23rdIEEEInternationalConferenceonSoftware
Analysis, Evolution, and Reengineering (SANER), Vol. 1. IEEE, 90â€“101.
[42]Deheng Ye, Zhenchang Xing, Chee Yong Foo, Jing Li, and Nachiket Kapre. 2016.
Learning to extract api mentions from informal natural language discussions. In
Proceedingsofthe32ndIEEEInternationalConferenceonSoftwareMaintenance
and Evolution (ICSME). IEEE, 389â€“399.
[43]HaoxiangZhang,ShaoweiWang,Tse-HsunChen,andAhmedEHassan.2019.
Reading answers on stack overflow: Not enough! IEEE Transactions on Software
Engineering (TSE) (2019).
149
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 07:51:54 UTC from IEEE Xplore.  Restrictions apply. 