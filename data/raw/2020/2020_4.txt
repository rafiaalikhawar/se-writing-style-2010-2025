Mobile Application Coverage: The 30% Curse and Ways
Forward
by
Faridah Akinotcho
B.Sc., Paris Diderot University, 2019
A THESIS SUBMITTED IN PARTIAL FULFILLMENT
OF THE REQUIREMENTS FOR THE DEGREE OF
Master of Applied Science
in
THE FACULTY OF GRADUATE AND POSTDOCTORAL STUDIES
(Electrical and Computer Engineering)
The University of British Columbia
(Vancouver)
March 2024
© Faridah Akinotcho, 2024The following individuals certify that they have read, and recommend to the Faculty
of Graduate and Postdoctoral Studies for acceptance, the thesis entitled:
Mobile Application Coverage: The 30% Curse and Ways Forward
submitted by Faridah Akinotcho in partial fulfillment of the requirements for
the degree of Master of Applied Science
inElectrical and Computer Engineering
Examining Committee:
Julia Rubin, Associate Professor, Electrical and Computer Engineering, UBC
Supervisor
Ali Mesbah, Professor, Electrical and Computer Engineering, UBC
Supervisory Committee Member
Karthik Pattabiraman, Professor, Electrical and Computer Engineering, UBC
Supervisory Committee Member
iiAbstract
Testing, security analysis, and other dynamic quality assurance approaches rely on
mechanisms that invoke software under test, aiming to achieve high code coverage.
A large number of invocation mechanisms proposed in the literature, in particular
for Android mobile applications, employ Graphical User Interface ( GUI)-driven
application exploration. However, studies show that even the most advanced GUI
exploration techniques can cover only around 30% of a real-world application. With
extensive effort being invested into GUI exploration strategies, in this thesis, we
factored out GUI-related issues by having skilled human analysts perform a thorough
manual exploration on a number of large and popular Google Play applications. To
our surprise, coverage achieved by humans exceeded that of the automated tools by
only a small fraction.
The main reasons preventing human analysts from covering the entire appli-
cation include application dependencies on device characteristics, remote servers,
and external resources. We further explore characteristics of these dependencies
and discuss the applicability of dynamic techniques that extend beyond GUI explo-
ration, i.e., that involve various forms of alterations to the application’s ecosystem,
to bypass these dependencies. Based on our analysis, we outline possible future
research directions for improving application coverage, such as developing adaptive
exploration strategies that determine when and how to bypass dependencies.
iiiLay Summary
Having an efficient strategy to explore applications is a major requirement to ensure
the quality of mobile applications, especially those built for millions of users.
Detecting bugs or security risks, for example, requires interacting with as much
of the application as possible. Unfortunately, most approaches to automatically
explore applications only focus on user actions like button clicks, and fall short for
real-word commercial applications.
In this thesis, we aim to understand and characterize parts of applications which
are hard to reach, through a case study on a large number of Android applications.
By relating these challenges to specialized, more targeted techniques for addressing
them, we pinpoint gaps in knowledge and directions for future work. We hope our
findings will help create better and more tailored application exploration tools.
ivPreface
This thesis presents an in-depth characterization of reasons leading to low coverage
of top Google Play applications. The presented work was conducted by myself,
under the guidelines of my advisor, Professor Julia Rubin. I collaborated with
Professor Lili Wei from McGill University, Siddharth Gupta, an undergraduate
research intern from the Indian Institute of Technology (IIT), Patna., and Louie
Tang, an undergraduate intern from UBC.
I was responsible for the collection, exploration, analysis, and characterization
of applications, and for the selection, experimentation, analysis and categorization
of tools. I was also responsible for the manuscript writing and revision.
Professor Rubin helped with the concept formation and discussion, and the
manuscript write-up and revisions. Professor Wei also helped with concept discus-
sion and manuscript revisions. Siddharth Gupta and Louie Tang helped with the
cross-validation of manual exploration coverage and analysis results (Section 3.1.3
and 3.2.3).
vTable of Contents
Abstract . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . iii
Lay Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . iv
Preface . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . v
Table of Contents . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . vi
List of Tables . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ix
List of Figures . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . x
List of Abbreviations . . . . . . . . . . . . . . . . . . . . . . . . . . . . xi
Acknowledgments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . xii
1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1
1.1 Problem Overview . . . . . . . . . . . . . . . . . . . . . . . . . 1
1.2 Main Research Contributions . . . . . . . . . . . . . . . . . . . . 3
1.3 Thesis Structure . . . . . . . . . . . . . . . . . . . . . . . . . . . 4
2 Background . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5
2.1 Android Platform . . . . . . . . . . . . . . . . . . . . . . . . . . 5
2.1.1 Android Layered Architecture . . . . . . . . . . . . . . . 5
2.1.2 Android Framework APIs . . . . . . . . . . . . . . . . . 6
2.1.3 Android App Components . . . . . . . . . . . . . . . . . 6
vi2.2 Automated App Exploration . . . . . . . . . . . . . . . . . . . . 7
2.2.1 GUI-based Exploration . . . . . . . . . . . . . . . . . . . 7
2.2.2 Ecosystem Manipulation . . . . . . . . . . . . . . . . . . 8
3 Investigating Application Coverage: A Case Study . . . . . . . . . . 9
3.1 App Exploration Experiments . . . . . . . . . . . . . . . . . . . 9
3.1.1 Application Selection . . . . . . . . . . . . . . . . . . . . 9
3.1.2 Experimental Setup . . . . . . . . . . . . . . . . . . . . . 11
3.1.3 Application Coverage Results . . . . . . . . . . . . . . . 14
3.2 Reasons for Low Coverage . . . . . . . . . . . . . . . . . . . . . 15
3.2.1 Application Selection . . . . . . . . . . . . . . . . . . . . 15
3.2.2 Analysis Methodology . . . . . . . . . . . . . . . . . . . 16
3.2.3 Results: Reasons for Unreachability . . . . . . . . . . . . 19
3.2.4 Results: Properties of Conditions . . . . . . . . . . . . . 25
4 Investigating Ecosystem Manipulation Techniques . . . . . . . . . . 28
4.1 Relevant Techniques . . . . . . . . . . . . . . . . . . . . . . . . 29
4.2 Categorization of Exploration Mechanisms . . . . . . . . . . . . 29
4.2.1 Results: Invocation & Navigation Strategy . . . . . . . . 30
4.2.2 Results: Value Generation Strategy . . . . . . . . . . . . 34
5 Discussion and Future Work . . . . . . . . . . . . . . . . . . . . . . 36
5.1 Threats to Validity . . . . . . . . . . . . . . . . . . . . . . . . . . 36
5.2 Discussion and Implications . . . . . . . . . . . . . . . . . . . . 37
5.3 Additional Future Directions . . . . . . . . . . . . . . . . . . . . 38
6 Related Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39
6.1 Comparison of GUI-based Exploration Techniques . . . . . . . . 39
6.2 Insights on Reasons for Low Coverage . . . . . . . . . . . . . . . 40
7 Summary and Conclusions . . . . . . . . . . . . . . . . . . . . . . . 42
7.1 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42
7.2 Conclusions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43
Bibliography . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44
viiA Supporting Materials . . . . . . . . . . . . . . . . . . . . . . . . . . 52
A.1 Additional Data . . . . . . . . . . . . . . . . . . . . . . . . . . . 52
A.2 Additional Research Work . . . . . . . . . . . . . . . . . . . . . 52
viiiList of Tables
Table 3.1 Coverage for BenchNotGP andBenchGP . . . . . . . . . . . . 12
Table 3.2 Coverage for Google Play apps ( TopGP ) . . . . . . . . . . . . 13
Table 3.3 Reasons for unreachability (11 apps, 363 activities) . . . . . . 18
Table 3.4 Unreachability conditions (out of 139 activities) . . . . . . . . 25
Table 4.1 Ecosystem manipulation papers . . . . . . . . . . . . . . . . . 28
Table 4.2 Invocation & navigation strategy of ecosystem manipulation tools 30
Table 4.3 Value generation strategy of ecosystem manipulation tools . . . 34
ixList of Figures
Figure 2.1 Android stack. . . . . . . . . . . . . . . . . . . . . . . . . . 5
Figure 3.1 An example path from target (B) to caller (A) . . . . . . . . . 16
Figure 3.2 Device software properties in Pinterest. . . . . . . . . . . . . 19
Figure 3.3 Device hardware properties in Samsung Smart Switch. . . . . 20
Figure 3.4 Server push-based dependencies in Pluto TV. . . . . . . . . . 20
Figure 3.5 Usage pattern check in PC Health. . . . . . . . . . . . . . . . 22
Figure 3.6 Alternate entry in McDonald’s. . . . . . . . . . . . . . . . . . 23
Figure 3.7 Disabled features in Pinterest. . . . . . . . . . . . . . . . . . 24
xList of Abbreviations
ADB Android Debug Bridge
API Application Programming Interface
APK Android Package
DEX Dalvik Executable
dpi dots per inch
GUI Graphical User Interface
HAL Hardware Abstraction Layer
HTTP HyperText Transfer Protocol
ICC Inter Component Communication
iOS iPhone Operating System
OS Operating System
RQ Research Question
SDK Software Development Kit
SMS Short Message Service
xiAcknowledgments
I would first like to thank my supervisor, Prof. Julia Rubin. I am extremely grateful
for the constant encouragement, constructive feedback, patience, and precious time
Prof. Rubin has given me over the course of my studies. I got from her the best
introduction to research one could ask for, and learned a great deal from both the
academic and human standpoints, while working together.
I would also like to thank my committee members, Prof. Ali Mesbah and Prof.
Karthik Pattabiraman, for carefully reviewing the thesis and the insightful feedback
they provided.
Prof. Lili Wei, Siddharth Gupta, Louie Tang have been a great help and valuable
collaborators. The discussions with Prof. Wei throughout this process and the ideas
we shared have enriched this research, and I learned a lot from her.
I am also very grateful to all my fellow lab mates and friends from the ReSeSS
lab who have been a constant source of support, advice, and inspiration over the
past few years. They have made our lab such a positive, uplifting, and stimulating
environment that I am proud to be a part of.
Last but not least, I would like to thank my family and friends for their uncondi-
tional support and love throughout this journey. To my parents and siblings, thank
you for always being listening ears and rooting for me every step of the way.
xiiChapter 1
Introduction
1.1 Problem Overview
Efficiently driving the exploration of a software system is essential for various
types of dynamic analyses, e.g., those that aim to improve quality, security, and
performance of software. In particular, the analysis of third-party mobile applica-
tions requires exploration techniques which can easily and reliably operate, even on
closed-source applications. For example, a security analyst or compliance auditor
for the Google Play Store would need to review a high number of applications to
ensure they meet the standards required by the platform.
A number of automated application exploration techniques have been recently
proposed in particular for Android software. The most basic yet most prominent
of these techniques is Android Monkey [ 6], which is part of the standard Android
distribution. Monkey simply generates pseudo-random streams of user events, such
as clicks, touches, or gestures, to drive the execution of the application (app). Like
Monkey, the majority of app exploration techniques proposed in the literature are
Graphical User Interface (GUI)-based. That is, they generate user interface events,
such as clicks, and system-level events, such as screen rotation and phone volume
change, simulating the ways users interact with the application and device.
More advanced approaches propose search-, model-, statistical- , and heuristic-
based techniques [ 9,11,19,22–24,30,31,33,37–40,44,45,48,53,57,71] to
achieve higher exploration coverage and better bug discovery capabilities. With
1the continuous emergence of new GUI-based exploration strategies, a number of
studies also compare the capabilities of the emerging tools on a set of benchmark
applications [ 20,54,58–60,69,74]. The results of these studies show that even
the most advanced tools are able to cover only around 30% of an industrial-scale
application.
This thesis deals with the question of why application exploration techniques “hit
a ceiling” of 30% and what the possible ways forward are. To answer these questions,
we first run two prominent and available GUI exploration tools: APE [ 23], which
was identified as one of the most performant tools in several earlier studies [ 54,59],
and the most recent publicly available tool – FastBot2 [ 36]. As case studies, we
used popular Android benchmark applications [ 20] and a set of large and popular
Google Play applications we collected from Google Play.
To factor out known difficult-to-solve issues for the GUI-based exploration
techniques, such as ordering of events (e.g., the need to add items to the shopping
cart before checking out) and app-specific semantic inputs (e.g., only zip codes in
country-specific formats are accepted), another analyst and I performed a thorough
manual exploration of the applications and compared the coverage results obtained
via automated vs. manual exploration.
To our surprise, while the average activity coverage achieved by the two tools
combined on our case study applications is 22.4%, the coverage achieved by human
analysts does not exceed this number significantly and is only 28.4%, on average.
Moreover, the combined coverage of all tool and human runs is 29.6%, on average.
This implies that just improving GUI-based exploration is unlikely to result in a
substantial increase in application coverage.
To facilitate advancement in automated app exploration, we further reverse-
engineered and manually analyzed a subset of our case study applications, identify-
ing reasons behind the low app coverage. That is, unlike the majority of compar-
ative studies that report on the limited abilities of GUI-based exploration tools to
achieve high coverage on both benchmark and realistic industrial-scale apps [ 20,58–
60,69,74], we perform a detailed code analysis to map out the reasons and code-
level properties preventing GUI exploration from reaching that code.
21.2 Main Research Contributions
The results of our analysis show that dependencies on the device’s software/hardware
properties, the server, and the environment, as well as disabled features, alternate
entry points, and error-handling code, are among the main reasons preventing even
human analysts from exploring a large portion of an application. We also observe
that a large fraction of the unreached activities start from difficult-to-trigger non-
GUI Android- and application-specific events and rely on data that is passed to the
activity as a parameter and/or retrieved from the app and device global state.
Our findings imply that approaches extending beyond GUI-based exploration
are necessary to achieve a substantial breakthrough in the efficiency of automated
app exploration, e.g., by simulating the necessary app execution dependencies for
the app to be executed successfully. Inspired by this observation, we collect and
categorize dynamic analysis techniques that manipulate application ecosystem , i.e.,
code, framework, device, and external dependencies [ 14,26,46,63,65,66]. These
techniques are often used to drive security and performance testing of mobile apps,
and we refer to such techniques as ecosystem manipulation techniques. With our
analysis, we aim to consolidate these methods into a unified categorization schema,
assessing the capabilities of existing work to be applied for addressing application
exploration challenges.
We conclude that the majority of ecosystem manipulation techniques focus
on defining particular strategies for directly invoking or navigating through any
given app entry point, as well as strategies for generating mocked data to facilitate
exploration. While such techniques can be useful to invoke and cover the unreached
activities, they are limited by their ability to generate the necessary values of the
right type and complexity. We discuss possible ways forward, including app
exploration techniques which adaptively utilize the described strategies, leveraging
the advantages and mitigating the disadvantages of each, and delegate/inform the
analysts when needed. We aim to bridge the gap between large-scale third-party
app exploration, often lacking app-specific domain knowledge, and an ‘informed’
exploration; one in which the necessary domain knowledge is provided or bypassed
to get closer to 100% coverage.
3To present our findings, we organize the analysis results around three main
research questions (RQs):
RQ1 : What are the main reasons for unreachability in real-world applications?
RQ2: What are the properties of conditions that guard unreached application
activities?
RQ3: Which mechanisms are employed by dynamic analysis techniques that ma-
nipulate app ecosystem and how do these mechanisms map to the properties of
unreached activities?
In summary, this thesis makes the following contributions:
1. It compares the performance of a state-of-the-art GUI-based automated app
exploration tools, APE [ 23] and FastBot2 [ 36], with that of skilled human analysts
on a number of large and popular Google Play applications.
2. It categorizes reasons leading to tool and manual unreachability, as well as the
code-level properties of unreached activities. To the best of our knowledge, this
is the first work which provides such a detailed categorization on a number of
large-scale Google Play applications.
3. It systematically collects exploration mechanisms that manipulate app ecosys-
tem, categorizes their capabilities, and puts them in context of the unreachability
characteristics we observed in our work.
4. It discusses gaps and future research directions for improving application cover-
age.
To support further work in this area, our data and analysis results are available
online [10].
1.3 Thesis Structure
The remainder of this thesis is structured as follows. Chapter 2 outlines the neces-
sary background on Android and automated app exploration. Chapter 3 describes
our methodology to select aapps and tools, experiments, observed results, and
categorization. Chapter 4 discusses our selection and categorization of ecosystem
manipulation tools and how they map to properties of unreachability. Chapter 5
discusses our findings and their implications for future work. Chapter 6 outlines the
related work, Chapter 7 summarizes the thesis and outlines its main conclusions.
4Chapter 2
Background
In this chapter, we provide the necessary background information on the Android
platform and automated GUI-based exploration.
2.1 Android Platform
2.1.1 Android Layered Architecture
Figure 2.1: Android stack.Android is an open-source Linux-based soft-
ware stack created for a wide range of de-
vices [ 21]. Android applications are written
in Java or Kotlin and compiled into Dalvik Ex-
ecutable ( DEX ) bytecode. The Android Soft-
ware Development Kit ( SDK ) tools package
compiled code alongside libraries and resource
files to form an Android Package ( APK ), which
is used for the distribution and installation of
applications. The Android Java Application
Programming Interface ( API) Framework (or,
simply, the framework) serves as an interface
between applications and lower-level layers of the Android stack, which is shown
in Figure 2.1.
5The lower-level layers are the Android Runtime which translates bytecode to
native instructions; Native C/C++ libraries which can be used for core functionalities,
e.g., related to performance or graphics enhancements; the Hardware Abstraction
Layer ( HAL ), which exposes device hardware capabilities such as bluetooth or
microphone; and the foundational Linux Kernel. The framework exposes the entire
feature-set of the Android Operating System ( OS) to apps through APIs written
in Java. These APIs form the building blocks applications use to generate user
interfaces, manage data and resources on the device, and manage the application
lifecycle.
2.1.2 Android Framework APIs
Framework APIs can largely be divided into callbacks andmethods . Callbacks
are overridden by an application to implement custom responses to events. They
serve as entry points into the applications and are invoked by the framework when a
particular event occurs. For example, a Button::onClick callback is invoked when
a button is clicked. The framework also invokes callbacks when the state of the
device changes, e.g., the network is disconnected, or when the application lifecycle
state changes, e.g., on creation or destruction. Unlike callbacks, APImethods are
invoked by the apps directly, to access functionalities exposed by the framework. For
example, TelephonyManager::getDeviceId is a framework method used to obtain
the unique identifier of the underlying device.
2.1.3 Android App Components
Android apps are built from four main types of components: activities, services,
broadcast receivers, and content providers. Activities are the main app building
blocks that facilitate the interactions between the app and the user. Each application
contains a main activity that is declared in the app’s manifest file and serves as
the main entry point into the application when started by a user. In the manifest
file, the exported attribute defines whether a component can be launched by other
applications and thus, be an additional entry point into the app.
Services execute long-running background tasks, e.g., playing music while the
user is interacting with another activity. Broadcast receivers are components that
respond to notifications from the system or another app component, e.g., when
6the battery is low or a file download is completed successfully. Content providers
manage access to data, e.g., for storing and retrieving contacts.
The transition between Android components, also known as Inter Component
Communication ( ICC), relies on Intents , which specify, either explicitly or implicitly,
the target component to invoke and the data to be passed to that component. For an
explicit intent, the name of the target activity is explicitly provided. Implicit intents
rely on intent-filters , to specify the type of interactions they handle. An intent filter
must contain one or more action elements, which can be standard Android actions
or developer-defined ones. Intents and intent filters can also contain other fields
such as data andextras , which carry additional information needed for the requested
action.
Android Debug Bridge ( ADB ) is a command-line tool that is included in the
Android development toolkit to facilitate communication with a device. It can be
used to install and debug apps, send inputs/intents to the device and the app, and
more.
2.2 Automated App Exploration
2.2.1 GUI-based Exploration
The vast majority of automated app exploration techniques proposed in the
literature are GUI-based, and stimulate apps’ graphical interface to explore various
app behaviors. On top of their strategy to explore the GUI, these techniques can
also integrate device- or system-level events, such as screen rotations or network
connection status changes, to further emulate user actions.
Monkey [ 6] is a fully random exploration tool developed by Google. It is part
of the Android SDK and one of the earliest yet most prominent techniques, despite
its simple exploration strategy. Similarly, other app exploration techniques are also
guided by randomness [ 32,37,71]. More sophisticated approaches use app mod-
els [11,19,23,24,30,36,53,57], evolutionary algorithms [ 22,38,39], symbolic
execution [ 9,40], or even deep learning [ 33,36,44]. APE [ 23] and FastBot2 [ 36],
are prominent, model-based and reinforcement-learning-based approaches, which
we evaluate in Chapter 3.
72.2.2 Ecosystem Manipulation
A number of approaches explore the application space through means com-
plementary to GUI exploration. These approaches perform alterations to the
app [ 15,16,34,46,49,52,64,68,75], the framework [ 63,65], the device
state [ 25,26,42,47,65], or external dependencies [ 50], to execute applications.
They often aim to trigger highly-constrained behavior of an application, e.g., an
action takes place at a specific time and location. Common applications of these
techniques are in the security domain, e.g., to expose malicious payloads or perfor-
mance testing domain, e.g., to reach specific APIs known to be energy-consuming.
In the majority of cases, tools implementing such techniques rely only on alter-
ations as standalone drivers of an application but some tools also combine them
with GUI- or system-level events (often reusing existing tools such as Monkey).
In the remaining of this thesis, we refer to this type of techniques as ecosystem
manipulation techniques and further characterize the alteration mechanisms they
employ in Chapter 4.
8Chapter 3
Investigating Application
Coverage: A Case Study
In this chapter, we discuss the results of our experiments running automated and
manual app exploration. We first describe our app exploration experiments (Section
3.1). We then outline the identified reasons for low coverage and the associated
code-level properties (Section 3.2).
3.1 App Exploration Experiments
In this section, we describe our app selection strategy, experimental setup, and
evaluation results.
3.1.1 Application Selection
We started our analysis from the 68 open-source applications in the AndroTest [ 20]
dataset, which has become a de facto standard benchmark used to evaluate numerous
existing approaches, e.g., [ 23,30,33,39,42,45,48,53,56]. As this benchmark was
created in 2015, newer versions of the apps have become available throughout the
years; these apps are commonly used to evaluate more recent tools [ 16,22]. We thus
collected the most recent version of each app in the dataset from the open-source
Android app repository, F-Droid [ 2] (37 apps), from GitHub [ 3] (19 apps), and from
Google Code archives [ 5] (6 apps). We could not find source code for four apps and
thus excluded them from our analysis. We further excluded three apps which we
9could not run as they crashed on startup. At the end of this process, we obtained a
set of 61 applications which span the years from 2007 to 2023.
We observed that only 14 of these benchmark applications (23%) are currently
available on the official Google Play app store and none of them is in the list of the
200 most popular apps. Furthermore, only four of the apps (6.5%), all from the
Google Play store, are from 2023. The number and distribution of the applications
per year is summarized in the first two columns of Table 3.1. The full list of all
applications we considered can be found online [10].
With the goal of collecting insights applicable to popular contemporary applica-
tions, we further collected our own benchmark consisting of top popular apps on
Google Play. Specifically, in early 2023, we traversed the ordered list of the top
100 free applications from the Google Play store in our country. We filtered out
newly added apps with less than 500,000 downloads and then selected one (most
popular) app from each of the Google Play categories in the dataset, to ensure a
representative sample that covers a wide range of behaviors. This process resulted
in the selection of 20 apps from the list of 100 top free applications on Google Play,
which belong to 20 distinct categories.
After initial experiments with this dataset, we further extended it to include at
least one app from each of the 32 Google Play categories. To this end, we sampled
the most popular app from each category. As the popularity of 10 apps did not
change and they were already included in our earlier sample, we extended the
dataset with 22 additional apps, producing a benchmark of 42 applications in total.
The list and info about these apps are given in Table 3.2.
In summary, our selected apps can be divided into three groups:
1.47 open-source apps from the AndroTest benchmark, which range between
2007 and 2022 and do not appear on Google Play. We refer to these apps as
BenchNotGP .
2.14 open-source apps from the AndroTest benchmark, which range between
2012 and 2022 and are present on Google Play. We refer to these apps as
BenchGP .
103.42 closed-source popular apps from Google Play in 2023. We refer to these
apps as TopGP .
3.1.2 Experimental Setup
In all our experiments, we measured activity coverage by calculating the fraction
of activities reached during app exploration out of the total number of activities
implemented by an app, as was done in earlier work [58, 74].
Manual Exploration. For the manual exploration, two analysts, i.e., myself and
another student with mobile app development experience, independently ran each
application, aiming to systematically explore all visible user interface choices
while making an effort to provide semantically-meaningful application inputs. The
analysts had no restrictions on changing device settings, or on interacting with
other applications, if prompted by the application under test. The exploration was
performed in a black-box style, i.e., without prior knowledge of the application’s
code, as it is the most common testing strategy for exploring third-party applications
as a whole [ 29].During this exploration, the analysts also took notes of any “blockers”
they observed preventing them from proceeding with the exploration. In these cases,
they backtracked and made an effort to explore other parts of the application.
Analysts were also provided with a script which logged the current activity
and achieved coverage during the exploration, as an indicator of progress. The
exploration stopped when an analyst spent five minutes being unable to trigger any
new application activity.
As the majority of TopGP and some BenchGP applications require user authen-
tication, each analyst used a distinct set of credentials and created a new account
for each of the available login options in the application (e.g., Google, Facebook,
and custom login). The analysts first explored the app without logging in and then
created the needed credentials to log in in each of the available ways and further
explore the app. Both analysts were unable to create new credentials and log in into
only one of the apps: Microsoft Teams (app #14), due to a documented bug with
the mobile version of the app [ 1]. We thus report the observed coverage without
logging in for this app, but exclude it from further discussion.
11Table 3.1: Coverage for BenchNotGP andBenchGP
Benchmark (Year) #Apps# Reached Activities (%)
Tools Manual Total
BenchNotGP (2007-2022) 47 81.5 88.7 88.7
BenchGP (2012-2022) 10 72.9 78.9 80.3
BenchGP (2023) 4 47.6 67.7 67.7
BenchGP (Total) 14 65.7 75.7 76.7
Exploring an app took around 20 minutes, on average (min: 5, max: 55). We
provide detailed plots of the achieved application coverage over time for each app
and for each analyst in our online appendix [10].
Automated Exploration. For the automated exploration, we focused on tools that
are available, able to process large-scale applications, and can run on a real device
(close to half of the TopGP apps can not be installed on emulators as they do not
support x86 architecture).We selected APE [ 23] – an automated exploration tool
which was shown to outperform related techniques in a number of studies [54, 59]
and that was recently updated to a newer version as part of the Themis project [ 54].
We further selected FastBot2 [ 36] – a recent publicly available tool designed explic-
itly for industrial-scale applications. We run the single-device version of the tool as
the multi-device version is not part of the open-source implementation, as confirmed
by the tool authors. Other tools, e.g., [ 22,33,44,57], were excluded as we either
could not successfully run them in our setup or the coverage results reported in
comparative studies [54, 59] were lower than those of APE and FastBot2.
Consistently with the manual evaluation, we run each tool twice: once without
any manual intervention and once after we logged in into each app manually. Each
run was configured with a dynamic timeout, i.e., we stop exploration when no
improvement in coverage is achieved for one hour [ 30,54]. On average, the tools
spend around 95 minutes on each app (min: 2, max: 261).
The experiments were conducted on Google Pixel XL phone running Android
10 (APIlevel 29). We selected this model after ensuring compatibility with all the
TopGP applications, i.e., APIlevel 29 is within the minimum SDK and target SDK
version of each app. For 23 older benchmark apps that were not compatible with
the current architecture, we used an emulator configured to support the version
compatible with the app.
12Table 3.2: Coverage for Google Play apps ( TopGP )
IDApp Name#Down-
loadsPopu-
larityCategory#Acti-
vities#Reached Activities (%)
Tools Manual Total
1WhatsAppMessenger 5B+ 4 Communication 387 117 (30.2%) 131 (33.9%) 152 (39.3%)
2SpotifyMusicandPodcasts 1B+ 36 Music & Audio 98 13 (13.3%) 31 (31.6%) 31 (31.6%)
3Instagram 1B+ 25 Social Media 264 12 (4.5%) 17 (6.4%) 17 (6.4%)
4TikTok 1B+ 6 Social Media 325 49 (15.1%) 79 (24.3%) 83 (25.5%)
5Pinterest 500M+ 53 Lifestyle 30 13 (43.3%) 15 (50.0%) 15 (50.0%)
6Uber-Requestaride 500M+ 23 Maps & Navigation 128 2 (1.6%) 2 (1.6%) 2 (1.6%)
7AmazonPrimeVideo 500M+ 17 Entertainment 80 23 (28.8%) 35 (43.8%) 35 (43.8%)
8Google Wallet 500M+ 8 Finance 69 3 (4.3%) 3 (4.3%) 4 (5.8%)
9CapCut-VideoEditor 500M+ 8 Video Players 178 48 (27.0%) 54 (30.3%) 61 (34.3%)
10SamsungSmartSwitchMobile 100M+ 20 Tools 51 10 (19.6%) 10 (19.6%) 10 (19.6%)
11Duolingo 100M+ 51 Education 128 25 (20.3%) 26 (21.1%) 31 (24.2%)
12AudibleAudioEntertainment 100M+ 32 Books & Reference 63 12 (19.0%) 18 (28.6%) 18 (28.6%)
13CanvaDesignPhotoVideo 100M+ 31 Art & Design 29 7 (24.1%) 9 (31.0%) 10 (34.5%)
14MicrosoftTeams 100M+ 15 Business 299 7 (2.3%) 8 (2.7%) 8 (2.7%)
15Pluto TV 100M+ 2 Entertainment 13 1 (7.7%) 1 (7.7%) 1 (7.7%)
16BumbleDatingFriendsapp 50M+ 73 Dating 199 11 (5.5%) 38 (19.1%) 42 (21.1%)
17BeCloserShareyourlocation 10M+ >200 Parenting 16 5 (31.3%) 10 (62.5%) 10 (62.5%)
18CardBoard 10M+ >200 Librairies & Demo 17 7 (41.2%) 8 (47.1%) 8 (47.1%)
19FIFA+ - Your Home for Football 10M+ 70 Sports 35 7 (20.0%) 6 (25.7%) 8 (22.9%)
20Expedia: Hotels, Flights & Car 10M+ 68 Travel & Local 84 26 (31.0%) 25 (39.3%) 29 (34.5%)
21TemuShopLikeaBillionaire 10M+ 1 Shopping 31 9 (29.0%) 13 (41.9%) 13 (41.9%)
22TheWeatherNetwork 10M+ 64 Weather 63 19 (30.2%) 25 (38.1%) 26 (41.3%)
23PictureThis-PlantIdentifier 10M+ 25 Education 197 41 (20.8%) 37 (18.8%) 45 (22.8%)
24Ticketmaster 10M+ 22 Events 110 9 (8.2%) 16 (14.5%) 18 (16.4%)
25ShopAllYourFavoriteBrands 10M+ 19 Shopping 10 3 (30.0%) 3 (30.0%) 4 (40.0%)
26Lensa 10M+ 1 Photography 46 27 (58.7%) 27 (63.0%) 32 (69.6%)
27McDonaldsCanada 5M+ 14 Food & Drink 157 33 (21.0%) 35 (22.3%) 35 (22.3%)
28FeverLocalEventsTickets 5M+ >200 Events 125 27 (21.6%) 36 (28.8%) 36 (28.8%)
29SephoraBuyMakeupSkincare 5M+ 75 Beauty 130 42 (32.3%) 52 (40.0%) 56 (43.1%)
30Wonder - AI Art Generator 5M+ 3 Art & Design 76 9 (11.8%) 8 (10.5%) 10 (13.2%)
31AutoTrader-ShopCarDeals 1M+ >200 Auto & Vehicles 60 17 (28.3%) 24 (40.0%) 25 (41.7%)
32MessengerLite-SMSLauncher 1M+ 93 Personalization 65 11 (16.9%) 18 (27.7%) 19 (29.2%)
33LocalNews 1M+ 74 News & Magaz. 75 32 (42.7%) 36 (48.0%) 37 (49.3%)
34AIMirrorAIArtPhotoEditor 1M+ 55 Photography 63 1 (1.6%) 5 (7.9%) 5 (7.9%)
35AirCanada+Aeroplan 1M+ 53 Travel & Local 29 3 (10.3%) 4 (13.8%) 4 (13.8%)
36FeelsyStressAnxietyRelief 1M+ 27 Health & Fitness 34 4 (11.8%) 7 (20.6%) 7 (20.6%)
37ChatGPTpoweredChat-NovaAI 1M+ 5 Productivity 19 2 (10.5%) 3 (15.8%) 3 (15.8%)
38REALTOR.caRealEstateHomes 500K+ >200 House & Home 15 8 (53.3%) 9 (60.0%) 9 (60.0%)
39ShoppersDrugMart 500K+ >200 Medical 17 1 (5.9%) 2 (11.8%) 2 (11.8%)
40VIZManga 500K+ >200 Comics 32 17 (53.1%) 17 (53.1%) 17 (53.1%)
41CBCSportsScoresNews 500K+ 102 Sports 35 11 (31.4%) 11 (31.4%) 11 (31.4%)
42PC Health 500K+ 14 Health & Fitness 29 6 (20.7%) 8 (27.6%) 8 (27.6%)
Average 93.1 22.4% 28.4% 29.6%
133.1.3 Application Coverage Results
Table 3.1 shows the coverage results for benchmark applications, grouped by market
types and years. The last three columns of Table 3.2 show the coverage results for
each of the TopGP apps. We report the coverage achieved by both tools, combined,
and by both human analysts, combined. We refer to these as Tools andManual ,
respectively. The difference between the coverage results achieved by the two tools
is below 0.3%; the difference between the results achieved by the human analysts is
also very low (<0.1%), confirming the reliability of manual inspection. We refer to
the total coverage achieved by all tools and humans collectively as Total ; we use this
as a baseline for the more detailed analysis in Section 3.2. Detailed coverage results
for each tool and each human analyst are available in our online appendix [10].
Overall, the tools were unable to outperform the human analysts. For example,
the average coverage achieved by both tools combined is 22.4% for TopGP apps,
while humans could reach 28.4% coverage. As expected, we noted several issues
preventing the tools from achieving the same results as human analysts, including
being unable to provide semantic inputs (zip code in app #3 and a selfie image in
app #16) The performance of the tools also decreases as the “complexity” of the
dataset increases: they can cover 81.5%, 65.7%, and 22.4% of activities, on average,
forBenchNotGP ,BenchGP , and TopGP , respectively.
Yet, to our surprise, the human analysts only exceed the tools coverage result by
a relatively small fraction, covering 88.7% and 75.7% of activities in the benchmarks
apps, on average, and only 28.4% of activities in popular Google Play apps. While
humans can generally reach more activities than tools, as they can bypass “semantic
barriers” outlined above, we also observed a few cases reached by tools and not
humans (5.6% of all reached activities). Such cases are caused by non-deterministic
app behaviors, error humans did not trigger, and human “exhaustion”. For example,
in app #13, actions that are visible after scrolling for more than 10 sec were missed
by humans, who most likely assumed that such a long scrolling action is redundant.
When combining the results of tools and humans, we observe the total coverage
of 29.6%, on average, for the Google Play apps. These motivate the need to
better understand reasons preventing both automated and manual exploration from
reaching the remaining >70% portion of the application. We explore this next,
14focusing explicitly on a more “difficult” subset of apps in our dataset: recent apps
from BenchGP and apps from TopGP .
3.2 Reasons for Low Coverage
To better understand the reasons for low app coverage and to answer RQ1and
RQ2, we analyzed open-source apps from BenchGP . To obtain more complete and
realistic results, we further reverse-engineered a subset of apps from TopGP and
manually analyzed their code. The goal of our analysis was to extract patterns
corresponding to the uncovered app activities. In what follows, we discuss our
application selection strategy, analysis methodology, and our analysis results.
3.2.1 Application Selection
We included in our app selection the four most recent open-source apps from
theBenchGP dataset (all from 2023): WordPress, MyExpenses, K9-Mail, and
Wikipedia. For TopGP , due to the extensive manual effort required to analyze
partially obfuscated closed-source applications, we focus our manual analysis on
seven of our case study applications. To select these applications, we use a bin
sorting technique: we sorted our 42 applications by the total number of downloads,
divided them into seven bins of equal range, and randomly selected one app as
a representative for each bin. The resulting set of applications is bold-faced in
Table 3.2 and includes apps #5 (Pinterest) – a platform to create and share digital
content; #10 (Samsung SmartSwitch) – a data transfer application; #15 (Pluto TV) –
a free streaming app with access to hundreds of TV channels and movies; #21
(Temu) – a shopping application which is also the most popular app on the Play
Store; #27 (McDonald’s Canada) – a menu ordering application which also provides
access to promotional content; #33 (Local News) – an app which delivers daily
personalized news to users; and #42 (PC Health) – an app that provides a variety
of health-related services, such as tips, contents, prescriptions, communication and
booking appointments with health care providers. We believe our selection of apps
is diverse and representative. The selected apps have 621 activities in total, out of
which 363 are reached by neither humans nor tools. (58.4%).
1566onCreate (…){…}
onClick (…){
m()
}
method m(){
if(getTime ().equals(“ 2PM”))
startActivity (B,…)
}
}//Caller Activity A
//class initialization
onCreate (…){…}
onStart (…){…}//Target Activity B
cb
2
1a3Figure 3.1: An example path from target (B) to caller (A)
3.2.2 Analysis Methodology
We decompiled the closed-source APK s using JADX [ 51] – a decompiler for the
Java programming language and used JADX’s built-in deobfuscation functionality
to map obfuscated names to more readable unique identifiers, which simplified the
analysis of the code. We then performed a manual reachability analysis to identify
paths leading to each activity not reached during manual exploration, examplified in
Figure 3.1.
More specifically, for each unreached activity, we searched for statements
launching the activity, i.e., invoking an ICC API such as startActivity (1on the
figure). Once the launching statements were located, we did a backward traversal of
the paths to this activity ( 2) until we reached a lifecycle method of another activity
(3), i.e., we found the most immediate caller activities. We sorted the obtained
paths by size, picked the shortest one, and only tried the next path if the analysis on
the selected path was inconclusive (until there were no paths left to analyze).
When identifying paths to an unreached activity, to account for activities started
through implicit intents, we analyzed the manifest file and packed resource files,
and searched for the correct intent action name to locate the launching statement.
Similarly, we accounted for cases with no caller activities and tracked entry points
in the manifest file, e.g., an activity started by an exported service or broadcast
16receiver. We also leveraged knowledge from the app’s manual exploration to guide
the search, e.g., searching for text displayed on a screen where the analyst could not
proceed forward.
We then collected conditional checks ( a) within the identified paths, e.g.,
scheduling an event for a specific time, and summarized them into in a human-
readable description. Given these descriptions, we used a card sorting technique,
which is typically employed to organize information into logical groups [ 18,43], to
extract a uniform categorization of reasons for unreachability.
To extract properties for the identified conditions in code, we recorded call-
backs ( b) on the analyzed paths and the source of information for the variables
checked on the path (e.g., in code, in resource files, etc.). Additionally, we analyzed
class constructors and default lifecycle methods of the unreached activities ( c),
e.g., onCreate ,onStart ,onResume , to extract static variables and check how/if the
conditions on the path are used by the activity. We followed the same methodology
for the analyzed open-source apps.
On average, the process of analyzing an application, and extracting reasons and
properties of unreachable code took around 3-4 days per app.
17Table 3.3: Reasons for unreachability (11 apps, 363 activities)
IDReasons# of activities (% out of all app activities)
All appsWord
PressMy
ExpensesK9-Mail Wiki PinterestSmart
SwitchPluto
TVTemu McDoLocal
NewsPC
Health
1Device
Software properties 9 (2.1%) 2 (6.1%) 1 (3.3%) 4 (7.8%) 1 (3.4%)
Hardware properties 10 (1.7%) 6 (11.8%) 1 (3.2%) 3 (4%)
2Server 56 (10.6%) 10 (9%) 1 (3%) 5 (38.5%) 9 (5.7%) 22 (29.3%) 9 (31%)
3Environment 10 (1.4%) 6 (3.8%) 1 (1.3%) 3 (10.3%)
4External resources
Equipment 15 (2.7%) 13 (25.5%) 1 (3.2%) 1 (0.6%)
Information 32 (2.8%) 4 (12.9%) 28 (17.8%)
5Usage patterns 3 (1.1%) 1 (7.7%) 1 (1.3%) 1 (3.4%)
6Error handling 20 (4.4%) 3 (2.67%) 3 (10%) 2 (3.9%) 2 (15.4%) 1 (3.2%) 4 (2.5%) 1 (1.3%) 2 (6.9%)
7Disabled
For app version 29 (1.7%) 29 (18.5%)
For all end-users 8 (1.4%) 1 (0.9%) 1 (2.3%) 1 (2.1%) 1 (3.3%) 1 (2%) 1 (3.2%) 2 (1.3%)
8Alternate entry 49 (5.8%) 8 (7.2%) 4 (9.3%) 3 (9.1%) 2 (6.7%) 2 (3.9%) 24 (15.3%) 4 (5.3%) 2 (6.9%)
9Transitive 55 (5.4%) 2 (1.8%) 1 (3%) 19 (37.3%) 23 (14.7%) 2 (8%)
10No caller 43 (6.0%) 30 (27%) 1 (2.3%) 1 (3%) 1 (2.1%) 1 (6.7%) 1 (1.9%) 2 (15.4%) 2 (1.3%) 2 (2.6%) 2 (6.9%)
11Unknown 74 (12.5%) 19 (17.1%) 4 (12.1%) 2 (4.2%) 7 (23.3%) 2 (15.4%) 8 (25.8%) 23 (14.7%) 3 (4%) 6 (20.7%)
Total 363 (55.4%) 72 (64.9%) 7 (16.3%) 12 (36.4%) 5 (10.4%) 15 (50%) 41 (80.4%) 12 (92.3%) 18 (58.1%) 122 (77.7%) 38 (50.7%) 21 (72.4%)
183.2.3 Results: Reasons for Unreachability
Table 3.3 shows the results of this analysis. We identified nine main reasons for
unreachability, which are listed in the second column of the table and are described
below in detail. Several of these reasons were only observed in the closed-source
TopGP applications that we analyzed. The and notations in the second column
indicate reasons not reported or partially reported by previous work; they will be
discussed in Chapter 6. The third column of the table shows the average percentage
of activities that are not reached due to a particular reason across all apps, while
the remaining columns show that percentage for each app individually. The last
row of the table shows the total number of unreached activities per app and across
all apps, and the average percentage across all apps. The selected 11 apps have
621 activities in total, out of which 363 are reached by neither humans nor tools
(58.4%). As one activity can be dynamically unreachable due to multiple reasons,
the percentages for individual reasons do not sum up to the reported total in the last
row of the table.
1.Device refers to dependencies on certain software or hardware specifications.
Software properties correspond to content, configurations, or applications that must
be present on the device for an activity to be reached. While a human tester can
generally deduce such properties by interacting with the app, we were unable to do
so in around 2% of the cases. For example, two activities of the Pinterest app can
only be reached by sharing content with another app, Line. However, this option
only becomes visible if Line is already installed on the device. Figure 3.2 shows a
snippet of the manifest file of the application which defines this dependency.
Device hardware properties are immutable characteristics specific to the device
type. Apps display particular content depending on such device characteristics. For
example, Samsung Smart Switch displays different activities depending on whether
it is running on a Samsung phone, another Android phone, or an iPhone Operating
<manifest package="com.pinterest">
<queries>
...
<package android:name="jp.naver.line.android"/>
...
</queries>
</manifest>
Figure 3.2: Device software properties in Pinterest.
191public void onClick(View v) {
2Intent i;
3if(SystemInfoUtil.isSamsungDevice()){
4 i = new Intent(this, IntroduceSamsungActivity.class);
5}
6else if { ... }
7startActivity(i);
8}
Figure 3.3: Device hardware properties in Samsung Smart Switch.
System ( iOS) device, as shown in Figure 3.3. It also checks for the device’s locale
to deal with access restrictions, e.g., for certain countries where the Google Play
store is unavailable. Temu contains activities that are only displayed on devices
with at least 600 dots per inch (dpi) (i.e., large-screen devices, like tablets).
Checks conditioned on software and hardware properties are typically imple-
mented by retrieving info from Android API methods and evaluating it against a set
of pre-defined values. In practice, an analyst would need knowledge of the correct
device specifications in advance to design the appropriate test setup. As these
specifications can be defined and checked virtually anywhere in code, it constitutes
a challenge for black-box exploration of third-party applications.
2.Server dependencies correspond to cases where app behaviors depend on results
of communication with a third-party server, via HyperText Transfer Protocol ( HTTP )
or a socket, and constitute the largest reason for unreachability in our case studies
(10.6%).
The majority of server dependencies that we observed are pushed-based , i.e.,
initiated by a server as a notification rather than initiated by the app via an explicit
(pull) request. For example, Pluto TV sends catalog updates and displays in-app
advertisement when specific events occur on the server side. Such server-side
updates introduce non-determinism when running an application and result in large
portions of an application that cannot be explored unless specific data is received
from the server.
1var brazeBridge = {
2 logCustomEvent: function (name, properties) {
3 // invokes a Java method logCustomEvent
4}};
Figure 3.4: Server push-based dependencies in Pluto TV.
20Our analysis shows that applications often use third-party libraries to receive
server-side notifications. Moreover, these libraries combine code written in Java
with code written in JavaScript. For example, Pluto TV, McDonald’s, and Samsung
Smart Switch use the com.braze marketing automation library that manages push
notifications, in-app messages, and other forms of dynamic content. The library
uses a JavaScript bridge to define APIs invoked by a remote server and delegates
the invocations to Java methods, as shown in Figure 3.4.
For pull-based scenarios, the application typically fetches structured data from
the server to display it to the end user, e.g., a list of available pharmacies in PC
Health, or uses it as a feature toggle that enables/disables features on demand. The
latter is actually another reason for the very low reachability we observed in Pluto
TV: the code of the app contains a toggle controlled by a server, which guards the
functionality related to user authentication, profile management, etc. As the server
did not enable this behavior, we could not reach the corresponding functionality
during our exploration. Similarly, both WordPress and Temu load configurations
from the server at runtime to select which version of an activity to display.
Enforcing server behavior falls outside of the capabilities of third-party analysts
but remains an important challenge due to its prevalence.
3.Environment describes the situation of the user and its surroundings, such as
being at a certain location or at a specific date or time interval. For example, the user
must be at the pickup location and retrieve their order, to trigger the McDonald’s
activity that changes the status of the order to “completed”. Similarly, one can
only chat with a PC Health agent during pre-defined operating hours, which fell
outside of our testing time. In addition, apps use internally-timed promotional
campaigns to redirect users to certain activities, e.g., for periodically claiming gifts
in McDonald’s.
To facilitate environment dependencies, apps retrieve information, e.g., the
current time and location, through Android framework methods, as well as register
for callbacks, e.g., alarms and location updates. Enforcing the proper environment
in testing is challenging as testing budgets are typically limited and one cannot
simply wait long enough or move to the right location. Moreover, satisfying
environment dependencies might require domain-specific knowledge, such as what
the operating hours are and where the pickup location is.
211public boolean completedEnoughHealthJourneys(){
2return SharedPreferences.getInt("numJourneysCompleted", -1) >= 5;
3}
4public void promptUserForStoreRating(Activity a){
5if(completedEnoughHealthJourneys()){
6 startActivity(new Intent(a, PlayCoreDialogWrapperActivity.
class))
7}
8}
Figure 3.5: Usage pattern check in PC Health.
4.External resources are dependencies on equipment andinformation not readily
available at testing time. For example, Samsung Smart Switch requires a secondary
phone to enable an activity that transfers data; several activities in PC Health are
enabled only when the user scans a physical ID card; McDonald’s ordering features
all require valid payment information, which restricts access to close to 20% of the
application.
The interactions with external resources themselves are typically implemented
using GUI-, hardware- and sensors-related framework methods, often in third-party
libraries.While providing semantically meaningful inputs can often be handled
by a human tester, such sensitive information cannot be fabricated, limiting the
exploration of the app.
5.Usage patterns refer to dependencies on the type and frequency of app usage.
For example, PC Health keeps track of the user’s completed health journey activities
in persistent local storage; it then checks if the user has completed at least five
activities before asking the user to rate the app. Figure 3.5 shows an example of
such a check. Similarly, Pluto TV checks for the number of times the user switched
channels prior to asking for feedback. On average, 1.1% of activities depend on
usage patterns, which were not reached during the exploration due to the limited
time budget and lack of domain knowledge.
6.Alternate entry refers to activities started through entry points other than the
default/main activity. This includes exported activities visible to other apps only,
activities started through external links, and activities started through shortcuts or
widgets. For example, Figure 3.6 shows a snippet from the manifest file of the
McDonald’s app, which exports the UberDeepLinkHandlerActivity . This activity
is called, via an implicit intent, when the user interacts with McDonald’s through
Uber’s app delivery service.
221<activity android:name="com.mcdonalds.delivery.activity.
UberDeepLinkHandlerActivity" android:exported="true">
2 <intent-filter>
3 <action android:name="android.intent.action.VIEW"/>
4 <category android:name="android.intent.category.
DEFAULT"/>
5 <category android:name="android.intent.category.
BROWSABLE"/>
6 <data android:scheme="mcdmobileapp.uberauth" android:
host="uberredirect"/>
7 </intent-filter>
8</activity>
Figure 3.6: Alternate entry in McDonald’s.
Similarly, apps can expose activities via deep links – a mechanism that takes the
user directly to a specific destination within an app. For example, the
mcdmobileapp://aetcalendar?campaign=sigcrafted-launcher link is used in Mc-
Donald’s to start a promotional campaign activity. For validation purposes, we
confirmed that the activity is indeed started through this link; however, it dis-
played an error as a result, indicating that the correct data associated with the
link was missing.
Identifying all different ways to invoke an app, with their proper data types,
requires extensive data tracking through app and configuration files, which prevented
both automated and manual exploration from reaching these parts of the applications
(5.8% of cases, on average).
7.Error handling refers to activities that handle unexpected or erroneous cases
that never occurred at testing time. For example, all apps in our dataset include the
PlayCoreMissingSplitsActivity activity from Google’s com.google.android.play.core
library, which deals with incomplete/corrupted app installations. This activity is
automatically added as a library dependency and only enabled at runtime, if missing
app splits are detected, to prompt the user to reinstall the application through the
official store. As an example of a custom error recovery, Pinterest contains an
activity that is started only if a crash occurred in the previous launch. Such errors
are hard to predict and simulate at testing time.
8.Disabled are cases where an activity cannot be reached for the version of
the application under test. We distinguish between cases which are disabled
for the app version we are running and for all users . For the first case, we observe
231 if(isEnabled("android_dev_menu", y.map, user_status)){
2 setupDeveloperView(R.layout.dialog_developer);
3}
Figure 3.7: Disabled features in Pinterest.
that application developers reuse the same code to create different versions of an ap-
plication. For example, the McDonald’s app contains a number of country-specific
configuration files which determine the features that will be available for a specific
app version. Over 18% of unreached activities were disabled in the Canadian version
of this app, primarily for location-specific campaigns and promotions. For example,
as the Canadian JSON configuration file does not include the surveyEnabled key,
none of survey related activities can be reached. This key is included in other
configuration files, e.g., for Slovakia.
We also observed apps that include debugging activities and other non-user-
facing functionality, which are disabled for all end users. For example, the Pinterest
app includes a view only available to app developers, as shown in Figure 3.7. Testing
such activities requires enabling the corresponding flags in configuration files and/or
code and cannot be “expected” from GUI exploration tools.
9.Transitive activities are activities which would be reached if their caller activity
was successfully reached during exploration. On average, we observe that 5.4% of
the activities of an app belong to this category.
10.No caller refers to cases where we could not identify a caller entrypoint for
the activity under test. It mainly corresponds to activities which are only used as
interfaces or superclasses for other activities and activities which are likely dead.
For example, over 25% of the WordPress app could not be mapped to a caller
context. This portion of the application mainly consists of features which have
been migrated to an auxiliary app, JetPack, and which will be removed in future
versions of Wordpress according to the documentation. We also note that around
3% of activities are outdated (e.g., annotated with @Deprecated) and originate from
common libraries, which were not removed by developers although they already
include newer versions of the activities.
11.Unknown are cases where we could not confirm by analyzing the code why a
certain app activity cannot be reached (12.5%). We observe that around two-thirds
of such cases are likely to originate from third-party libraries, and correspond to
24code not supported on the device or in an app, e.g., both McDonald’s and Pinterest
usecom.google.ar.code – Google’s augmented reality SDK , likely because of
scanning features they implement.
Answer to RQ1: The main reasons preventing even human testers to cover a large
fraction of an application include application dependencies on device software/hard-
ware properties, remote servers, and external resources, as well as application usage
patterns, alternate entry points, and disabled features.
3.2.4 Results: Properties of Conditions
Table 3.4: Unreachability conditions (out of 139 activities)
Activation Location %
Code 71.1
GUI & Lifecycle 42.2
Other Android 20
App-specific 8.9
Manifest 28.9Activation Guards %
With Guards 55.6
Strategy
Code-enforced 49
Platform-enforced 6.6
Restriction
Mandatory 35.6
Discretionary 20
No Guards 44.4Execution Dependencies %
With Dependencies 82.2
Intent Fields 71.7
Primitive 39.1
Object 26.1
Primitive + Object 6.5
Global state 24.4
Primitive 4.4
Object 17.8
Primitive + Object 2.2
No Dependencies 17.8
Unreached activities are guarded by several code-level conditions preventing
their execution. We observed that these conditions have different manifestations
in code, and some can be more easily satisfied or “bypassed” than others. To
inform automated techniques that can increase application coverage by extending
beyond GUI exploration (Chapter 4), we systematically study and characterize
code properties of conditions guarding the execution of unreached activities. Our
categorization has three main dimensions described below: activation location,
activation guards, and execution dependencies. We exclude activities from the
Unknown (74 activities), No Caller (43 activities) and Transitive (55) categories.
We further exclude 48 activities for which we could not reliably obtain a complete
path to a caller activity in code, e.g., GoogleApiActivity library activity for error
handling. Table 3.4 summarizes the percentage of unreached activities that exhibit
properties pertained to each dimension, out of the remaining 139 activities.
1.Activation Location describes where the launching of an activity takes place
within the app, e.g., bin Figure 3.1. In the majority of the cases (71.1%), the
25activity is activated in code: within GUI or activity lifecycle callbacks, such as
onClick oronResume (42.2%); Other Android callbacks, such as onReceive for
system-wide broadcasts (20%); or App-specific custom events, such as socket event
handlers or cross-language event handlers, like in Figure 3.4 (8.9%). App-specific
events are mainly observed in Server cases to build a communication channel
between the app and the server.
In another 28.9% of the cases, the activity is triggered from an alternate entry
point defined in the Manifest file; these cases correspond to the Alternate Entry case
discussed in Section 3.2.
2.Activation Guards refer to conditions ( if,while , and similar statements or
Android constructs) that constrain the launch of an activity, e.g., ain Figure 3.1.
We distinguish between Code-enforced guards which correspond to conditional
statements in code ( if,switch ,while , and forloops, try/catch blocks and similar
statements) and Platform-enforced guards which are implicitly applied by the An-
droid platform. Here, we mainly note cases of app components which can not
be triggered due to properties defined in resource files, e.g., the ‘android:enabled’
property of a button is set to false in the Pinterest app to disable non-user facing
components. In most of the cases, guards correspond to activities started from Code.
Yet, in two of the cases,we observe guards for Manifest cases; the activity is disabled
by default and can be later enabled programmatically once a particular condition in
code is satisfied.
In 44.4% of the cases, activities do not have any activation guards. These cases
correspond to the Other Android callbacks and App-specific events we could not
trigger in our exploration.
For the remaining 55.6% cases with an activation guard (35.6% + 20%), we
characterize how restrictive the enforcement of the guard, for the proper execution
of the activity. We deem a guard mandatory (35.6% of the cases) if it corresponds
to a “hard constraint” and the activity indeed cannot be triggered if the condition is
not met. For example, in the Samsung Smart Switch app, a secondary connected
device must be present for the majority of activities to be activated. We deem a
guard as discretionary (20% of the cases) if it corresponds to a “soft constraint” – a
choice made by an application to show an activity only under specific circumstances.
For example, in McDonald’s, an advertisement displaying activity is only shown
26for users located in California, as part of a promotional campaign. However, the
actual content of the activity is independent from the user’s location and would still
be displayed correctly if shown at a different location, i.e., the constraint on the
location does not impact the behavior of the activity when launched.
3.Execution Dependencies refer to information an activity relies on once it
is launched, e.g., cin Figure 3.1. This information can be passed in Intent Fields ,
which we further divide into Primitive fields (39.1% of cases), Object fields (26.1%
of cases) and compound intents containing Primitive+Object fields (6.5% of cases).
For example, in PC Health, messages from the server are passed as part of the intent
viaParcelable extra values as Objects . Intent Fields can be present in both activities
triggered in code and through a manifest file. This also explains why the fraction
of activities having execution dependencies is higher than the fraction of activities
with mandatory conditions.
In addition to information passed through intents, activities can also rely on
Global State , i.e., information that is accessible from any point in the app through
APIs or shared variables. Such information can be Android- or application-specific,
e.g., device properties or particular data stored by the app in Shared Preferences . We
note that 65% of Global State dependencies correspond to Android-specific de-
pendencies on device properties or external resources, while 35% are application-
specific values created by the app. Again, we divide this information into Primitive
fields, Object fields and compound Primitive+Object (4.4%, 17.8% and 2.2% of
cases, respectively).
In 17.8% of the cases, activities have no execution dependencies.
Answer to RQ2: While the majority of unreached activities are activated through
GUI clicks and lifecycle events, slightly less than third are activated through more-
difficult-to-trigger non- GUI Android and custom events; an equivalent number of
activities are only reachable through alternate entry points defined in the manifest
file. Moreover, activation of more than half of the activities are guarded by a
condition. Yet, some of these conditions do not have to be satisfied for an activity to
execute. Finally, the majority of activities rely on primitive and object data to run,
which is passed via Intent fields or retrieved from the app and device global state.
27Chapter 4
Investigating Ecosystem
Manipulation Techniques
In this chapter, we discuss the applicability of ecosystem manipulation to address
the exploration challenges we identified in our work.
Table 4.1: Ecosystem manipulation papers
Objective Tool Name Year # Cit. Objective Tool Name Year # Cit.
FunctionalAppDoctor [25] 2014 141
SecurityConDroid [49] 2014 65
Crashscope [42] 2016 213 Brahmastra [15] 2014 155
EHBDroid [52] 2017 84 GroddDroid [7] 2015 54
SnowDrop [72] 2017 18 IntelliDroid [65] 2016 292
FAX [68] 2020 26 Harvester [46] 2016 183
DALT [34] 2022 1 FuzzDroid [47] 2017 66
COLUMBUS [16] 2023 1 MALTON [67] 2017 86
Concurrency SIEVE [66] 2020 5 Droid-ANTIRM [62] 2017 35
Energy COBWEB [26] 2019 41 SMARTGEN [75] 2017 49
DirectDroid [63] 2018 33
ARES [14] 2018 21
MoSSot [50] 2019 16
CAR [64] 2022 3
284.1 Relevant Techniques
Our findings imply that solutions extending beyond GUI-based exploration are
needed to cover the majority of currently uncovered activities. Moreover, such
solutions should provide the necessary app execution dependencies, for the triggered
activities to be executed successfully. In practice, satisfying all dependencies is
a tedious and time-consuming task, especially in the presence of discretionary
behavior, e.g., waiting five hours for an ad to display or impossible for GUI-based
exploration alone, e.g., app-specific callbacks, as discussed above.
Inspired by these observations and to answer RQ3, we collected and categorized
ecosytem manipulation techniques from existing work. Such techniques, through
modifications to the application itself or what it interacts with, i.e., the Android
framework, the device or external components like servers, have the potential to
‘unlock’ the unreached activities for subsequent dynamic analyses.
To identify such techniques, we started from existing surveys on testing ap-
proaches for mobile software [ 28,29,35] and selected all approaches that propose
mechanisms which extend GUI exploration with different forms of ecosystem
adaptation.
We further excluded approaches which require additional human input to guide
the exploration, such as bug reports [ 41,73] or pre-computed app models [ 8,27,55].
We also excluded approaches with an exploration logic that is only applicable to
a specific goal, e.g., RacerDroid [ 55] manipulates threads for data race detection,
which would not map to generic app exploration. Next, we performed forward
snowballing, to augment our list with papers published after the surveys’ timeline,
using the same inclusion/exclusion criteria. At the end of this process, we identified
22 relevant papers listed in Table 4.1. The number of citations reported are as of
March 2024.
4.2 Categorization of Exploration Mechanisms
To better classify, discuss and compare the papers, we construct a categorization
schema by following an iterative process where we first sampled five papers and
two of the authors independently proposed categorization attributes to describe
these papers. We then jointly discussed the proposed attributes while unifying
29related attributes, removing redundant ones, and updating labels. We verified the
applicability of the constructed schema on another set of ten papers and adjusted it
based on a joint discussion. We continued extending the schema after reading the
remaining papers, to ensure its inclusiveness.
The resulting categorization schema, which we further use to analyze and
describe the papers, contains two high-level areas described below. The detailed
categorization of each paper along the attributes of this schema is available in the
online appendix [10].
Table 4.2: Invocation & navigation strategy of ecosystem manipulation tools
Strategy #
Direct Activity Invocation 2
ICC API Code Injection 2
Invocation Through Path Navigation 13
(Invocation) Of Enclosing Callback 11
Callback Code Injection 2
Code Extraction 1
Framework Event Generation 2
(Navigation) Within Enclosing Callback 13
Forced Branching 2
Variable Manipulation 2
Device Manipulation 2
Server Simulation 1
4.2.1 Results: Invocation & Navigation Strategy
Invocation & Navigation Strategy are mechanisms tools employ to launch a given
activity. At high-level, a tool can either directly jump start an activity of interest,
(Direct Activity Invocation) or trigger the relevant activation location in the caller
component (Invocation through Path Navigation) , as shown in Table 4.2.
1. Direct Activity Invocation takes place by sending intents to launch an activity
of interest and is performed at app-level by our surveyed techniques.
ICC API Code Injection. By default, activity transitions are defined in code and
utilities like ADB can only trigger activities that are exposed to other processes
(i.e., exported in the Manifest file). In order to trigger any activity, even those
which are not exposed, tools directly invoke ICC API s which launch activities (e.g.,
30startActivity ) [34,68]. FAX [ 68] and DALT [ 34] modify the Manifest file of the
original application to expose all activities in the app and build an intermediary app
to send the necessary ICC calls for each newly exposed activity. This strategy is
particularly useful when sending intents containing data types for which ADB has
limited support for, e.g., Intent Objects fields (observed in 26.1+ 6.5= 32.6% of
cases).
It is not only designed/well-suited for activities started in the Manifest file
(28.9% of the analyzed cases); it is, in fact, irrespective of activation locations or
guards, thus suitable for any activity. However, these tools only focus on sending
intents to an activity and do not account for global state used after launching the
activity, which constitutes 24.4 (2.2+4.4+ 17.8)% of cases.
2. Invocation through Path Navigation focuses on triggering the activity through
its original call path and requires two complementary steps:
2.1. (Invocation) Of Enclosing Callback. Some tools focus on triggering call-
backs within caller components at app- or framework-level, i.e., invoking activation
locations. We identify three ways to implement this strategy:
Callback Code Injection. These tools directly invoke callbacks which makes it pos-
sible for them to trigger a variety of Code activation locations. Invocation is injected
in the app code through through static instrumentation [ 15,49,52,75], e.g., in
theonCreate method of the main activity. Some tools rather employ dynamic
instrumentation [ 16,64] to inject code in the app memory at runtime. One tool [ 64]
creates a path driving method and instruments the Android framework to trigger
that method at runtime. Tools typically support a predefined (and limited) set of
GUI, Lifecycle or Other Android-specific callbacks. COLUMBUS [ 16] is the only
tool that implements a fully automated strategy to identify callbacks.
Thus, it has a chance to trigger application-specific callbacks (8.9% of all cases)
and other callbacks missed during exploration.
Code Extraction. One tool, Harvester [ 46], also operates at app-level and generates
a new application containing only one activity and none of the original application
entry points/callbacks. The tool targets code statements specified by an analyst,
extracting a slice of statements from the target and executing these statements in the
new app, without retaining the original entry points and their inputs; it uses default
constructors to declare and initialize any undefined variable.
31Such a strategy is not suitable for continuous app exploration, since it alters the
app behavior and prevents the exploration from reaching any activity other than the
targeted one.
Framework Event Generation. Tools in this category [ 63,65] customize the Android
OSto inject events at the framework layer, i.e., invoke the same methods within the
framework that are triggered by the hardware layer. This is particularly important
for certain events that span all Android platform layers, to maintain consistency
between an app and the framework state. For example, when an SMS is received,
the framework records the SMS history in a database, in addition to invoking the
onReceive callback.
Like the Callback Injection tools discussed above, tools in this category can
trigger Lifecycle, and other Android-specific callbacks, but both have limited sup-
port for GUI callbacks. While they generally are only designed for triggering
callbacks/activation locations, a more consistent device state could satisfy some of
the Global State dependencies, e.g., having the SMS history recorded. Yet, we did
not observe such cases in our dataset.
2.2. (Navigation) Within Enclosing Callback. Assuming that the enclosing
callback is triggered, tools will implement logic to explore paths within a callback,
i.e., to navigate past activations guards. Similarly to other categories, tools operate
at app-level but also at device- or external-dependency-level. We identify four main
implementation paradigms:
Forced Branching. Several tools modify the control flows of a program to steer
execution towards a desired point of interest, aiming to reach a specific code
location for various objectives, such as security monitoring and race detection [ 7,
14,46,61,66]. For example, ARES [ 14] rewrites app code to flip conditional
checks which depend on the execution environment, e.g., device build version,
while SIEVE [ 66] does so to bypass complex event sequences. Forced branching
only applies to conditional statements, with the remaining original code statements
executed “as-is”.
Thus, depending on the path to the target it extracts, there is a potential for at
least some of the necessary application dependencies (Intent Fields and Global State)
to be set in the flow of execution. However, this strategy will fail for mandatory
conditions (35.6% of all unreached cases) where the dependencies must be satisfied
32and not simply bypassed. It also can not bypass platform-enforced guards, which
do not map to conditional statements in code (6.6% of cases).
Variable Manipulation. Other tools directly manipulate values for code variables, to
ensure execution takes the desired path. For example, several tools intercept and
modify return values of Android framework APImethods [ 47,49,63,72], through
static or dynamic app instrumentation. ConDroid [ 49] also modifies values of frame-
work class fields. In addition, Malton [ 67] can modify values of arbitrary variables,
when such variables are explicitly specified by the user; CAR [ 64] automatically
selects and mocks any method return values and class fields, when needed.
Similar to force branching, tools using this approach have the potential to set
at least some of the necessary application dependencies. They could also generate
some required values as discussed in the Value Generation Strategy section below.
However, these tools are unlikely to support the full range and diversity of the
complex Objects we observed in our dataset, which are, in fact, necessary in more
that 30% of our observed cases (32.6% and 20%, for Intent Fields and Global State,
respectively).
Device Manipulation. Tools also interact with configurable and/or stateful aspects
of devices without modifying code [ 25,26,42,47,65]. For example, Intel-
liDroid [ 65] adjusts the device’s local time and location to values obtained in
code, FuzzDroid [ 47] preloads user contacts, and AppDoctor [ 25] uses various emu-
lator screen sizes and densities. Two tools, CrashScope [ 42] and COBWEB [ 26]
simulate different values for hardware sensors (e.g., motion, position, etc.), aiming
to explore the application under different contexts.
Tools in this category have the potential to generate some of the global state
dependencies, in particular those related to device properties (65% of global state
dependencies). However, while the existing tools focus on time, location, contacts,
and screen sizes, we observed other reasons for the lack of reachability in our
case, such as apps and files. Additionally, while it might efficiently trigger crashes,
in practice, manipulating individual sensors is unlikely to fully model complex
interactions with external equipment or environment.
Server Simulation. Finally, one tool in our collection, MoSSOT [ 50], focuses on ex-
ternal servers to identify authentication-related vulnerabilities. It monitors network
33traffic to extract app-specific authentication profiles and uses this information to
mock server interactions that expose authentication-related vulnerabilities.
While the idea to mock server-related traffic could be valuable, this tool only
focuses on fuzzing traffic in different authentication scenarios; reasoning about
types of traffic a tool never observed could be challenging or even an impossible
task.
Table 4.3: Value generation strategy of ecosystem manipulation tools
Type Fixed (8) Symbolic (10) Heuristic (15)
Generic Primitives (12) 6 8 6
Generic Objects (2) - 1 2
Specialized (14) 6 4 9
4.2.2 Results: Value Generation Strategy
Value Generation Strategy provides inputs necessary for triggering callbacks and
activities, and for generating data to steer navigation. We identified three types of
such strategies, shown in Table 4.3:
Fixed. Tools in this category use random [ 47,67] or default/manually-crafted val-
ues [ 25,26,34,47,49,50,52]. This strategy is mainly applied to generate Generic
Primitives (including Strings) or Specialized value types; Specialized referring to
specific and proprietary types of values generated by the majority of tools in our
study [ 15,25,26,34,42,47,49,50,52,63,65,68,72,75]. For example, Crash-
Scope [ 42] generates random values for sensor inputs (which could or could not
happen in practice); EHBDroid uses default values for callback inputs extracted
from the Android API documentation.
In general, such techniques require customized solutions for each of the needed
values (Primitives and Objects), which makes the value generation for real-world
apps impractical.
Heuristic. Tools also implement a variety of heuristics to integrate information
specific to the application or domain [ 15,16,25,26,34,42,47,49,50,52,63,64,
72,75]. In general, these mostly apply for Specialized value types. For example,
DroidFuzzer [ 70] computes models of different media types (e.g., MP3, MP4,
A VI) and mutates chunks to generate ill-formatted data; SnowDrop [ 72] deduces
34expected values from variable names using natural language processing. Two tools,
COLUMBUS [ 16] and CAR [ 64] aim at generating values for any object type
automatically by retrieving objects of matching type from the heap at runtime.
Again, such heuristics have to be customized for a variety of data types employed
by applications.
Symbolic. Tools applying this strategy use a constraint solver to generate semanti-
cally meaningful values [ 16,34,46,47,49,64,65,67,68,75]. While this strategy
is primilarily applied for Generic Primitive values, tools also obtain Specialized
values symbolically. IntelliDroid [ 65] and DirectDroid [ 63] only reason about
values related to SMS , location, and certain GUI classes, while ConDroid [ 49],
for example, combines symbolic and concrete execution, i.e., performs concolic
execution, to obtain values. CAR further uses constraint solving for creating mock
objects and their fields.
While symbolic techniques can generate application-specific values, they mostly
work for Primitive types. CAR [ 64] makes an effort to use symbolic execution to
construct Object variables. However, scaling symbolic execution to process complex
app-wide dependencies is challenging.
Answer to RQ3: We identified a number of techniques which trigger activities, by
directly invoking application APIs and callbacks, or sending events at the framework
layer. While such techniques can be useful to directly invoke and cover the necessary
activities, these techniques are limited by their value generation strategies and ability
to satisfy Global State execution dependencies. Unfortunately, such cases amount
for the majority of unreached activities in our dataset. Path navigation has the
potential to set at least some of the necessary application dependencies. These
techniques can also successfully bypass discretionary application guards. Yet, they
are also limited in their ability to generate the necessary values of the right type and
complexity, likely causing many executions to fail.
35Chapter 5
Discussion and Future Work
In this chapter, we outline the main threats to the validity of our findings. We then
discuss the implications of our findings on improving application exploration and
suggest additional avenues for future work.
5.1 Threats to Validity
The main threat to the internal validity of our results stems from the manual
analysis we performed: when identifying reasons or properties for unreachability in
applications, we could have missed or misinterpreted some relevant dependencies
or implementation patterns. Likewise, we could have misclassified some of the
analyzed tools. To mitigate these threats, we extensively discussed the categorization
and cross-validated the findings between the authors. We also referred to API
documentation of all identified library code for app analysis, and work which cite
the papers included in our tool analysis, when available.
As an external validity threat, our findings might not generalize beyond the
dataset we considered. Yet, as we carefully designed the data collection process
to include a diverse set of real-world applications with large user bases as well
as popular benchmarks for testing tool evaluation, and evaluated reachability of
those applications through both tool and manual exploration, we believe that our
results are reliable. Our experiments support the findings of existing work for GUI
tool performance and our detailed analysis of reasons for unreachability with code
indicators, further corroborates our results. Additionally, while we cannot extract a
36complete set of reasons for unreachability, we believe the diversity of conditions
identified with our dataset is a good indication of the need to go beyond GUI-based
exploration and further invest in coverage explainability.
Finally, our discussion of implications for dynamic tools is based on paper
analysis rather than concrete evaluation. Among the 22 surveyed techniques, 16 are
outdated: they are implemented for old Android versions and cannot run on the latest
ones. Out of the remaining six techniques, none is publicly available. We contacted
the authors of the two latest techniques, namely CAR [ 64] and COLUMBUS [ 16] to
obtain the tools. However, the authors indicate that the tools are not ready for public
release yet. The necessary effort to update each of these tools to the necessary
versions and add support for newer Android constructs is substantial, thus we can not
experiment with the tools. Instead, our empirical evaluation focuses on fundamental
aspects of these tools while taking into account additional practical considerations
for tooling in order to better orientate further work.
5.2 Discussion and Implications
We envision future work for third-party app analysis to focus on developing explo-
ration techniques that combine GUI-based exploration with direct activity invocation
and invocation through path navigation, interactively deciding which strategy to
use based on the properties of the app. Specifically, such techniques would utilize
callback or ICC code injection when launching activities in non- GUI callbacks, and
in the Manifest. For GUI cases, when GUI exploration is blocked due to one of the
described reasons, navigation-based strategies would take a higher priority as they
have the potential to retain some of the original execution context: forced branch-
ing for cases with discretionary guards as it is the simplest and the least invasive
technique; device manipulation for cases with dependencies on Android-specific
global state; and variable manipulation as a last resort. Techniques should also be
extended to handle platform-enforced guards. None of the surveyed techniques
can fully reason about server dependencies, while it is one of the main reasons for
unreachability. Building specialized approaches to produce an isolated or mock
server for app exploration would be a useful future direction for future work.
37There is still a tradeoff to be considered between navigation which needs to a
complete (sound) path to the targeted activity and directly jumping to the activity,
which lacks context but could limit the search space. Alternative ways of combining
these strategies, e.g., manipulating values forward after jumping to an activity, could
be investigated. Tools could also integrate logic to prioritize unreached code that is
most valuable to patch. Estimating the relevance of a condition (e.g., based on the
transitive activities it guards), could be an interesting heuristic for a tool to decide
whether to bypass or satisfy the identified condition.
Finally, for cases that can not be resolved automatically or those with low
priority, techniques would alert human analysts and delegate the decision.
5.3 Additional Future Directions
We envision additional avenues of possible future work such as:
1.Automatically extracting coverage expectations. We invested extensive man-
ual effort for the analysis described in this thesis, which is needed to better
understand the applications. Automatically creating this characterization of
reasons would be valuable, not only as insights to analysts, as noted above, but
also for GUI-based tool evaluation to define a notion of ‘expected coverage’.
2.Extending the analysis to malware applications. There is an overlap with
common reasons for unexercised malware payloads, e.g., those described in
previous work [ 17]. It would be interesting to study how conditions differ
between malware and benign apps and assess how well they can be reused as
an indicator of maliciousness (or lack thereof).
3.An important, complementary dimension is coverage measurement. Design-
ing techniques which can reliably measure coverage at higher granularity
than activity coverage for industrial closed-source applications would be
useful. In our experiments, user actions do not always lead to new activities
but rather to dynamic changes in the GUI. Creating GUI models reflecting
these dynamic changes and measuring coverage of GUI states could be an
interesting direction.
38Chapter 6
Related Work
In this chapter, we discuss existing work that empirically compares app exploration
techniques and work that, through application and tool analysis, provides insights
into reasons for low coverage.
6.1 Comparison of GUI-based Exploration Techniques
Choudhary et al. [ 20] propose the first comparative study of existing GUI-based
event generation tools on 60 open-source applications, along several dimensions,
such as ease of use, code coverage, and fault detection.
The authors explicitly exclude tools with purposes other than increasing app
coverage, e.g., tools focusing on triggering crashes. They report that Monkey
performed the best, with around 45% coverage, despite its simple exploration
strategy. The benchmarks the authors contributed have become a de facto evaluation
dataset for new testing techniques. Wang et al. [ 58] argue that the effectiveness
of the techniques has yet to be proven in industrial cases, extending the study by
Choudhary et al. with additional tools collected in 2017 and a new set of applications
from the Google Play Store. Furthermore, the authors measure both coverage and
bug-finding abilities of the tools. They report a significant decrease in the state-of-
the-art tools’ performance when applied to industrial applications, with around 30%
coverage, on average. The authors also provide additional insights related to the
ease of use of each tool under study and ways to combine different GUI-based tools
to achieve better coverage or fault detection.
39While these studies focus on tool performance comparison, our work rather
provides insights into the reasons leading to the observed low coverage in industrial
cases. Additionally, we categorize and map capabilities of tools outside of GUI-
based event generation, which these studies exclude.
6.2 Insights on Reasons for Low Coverage
Behrang and Orso [ 13] identify seven reasons why random exploration achieves
limited coverage, when running Monkey on 68 open-source applications from F-
Droid, such as unsupported UI actions, system events, and missing inputs files.
The authors also manually patch those issues to validate the expected coverage
improvement.
Su et al. [ 54] use bug-finding ability rather than coverage as their metric, evaluat-
ing recent tools on a benchmark of documented bugs from open-source applications,
which the authors collect and make available as a ground truth for follow-up work.
The authors identify issues which automated tools cannot deal with, such as complex
event sequences and application settings.
Similarly, Wang et al. [ 60], identify exploration tarpits of existing GUI-based
tools such as logging out too early or restarting the application too often; they
further build VET, a platform to prevent such issues.
The majority of the challenges reported by these papers correspond to problems
that an informed human analyst would not face, e.g., complex event sequence,
app settings, and semantic inputs. Our work differs from them as it factors out
challenges of inefficient GUI exploration, and focuses on the remaining reasons for
low application coverage.
Wang et al. [ 59] discuss inefficiencies of underlying infrastructure used by
existing GUI-based tools. Similarly to us, this paper motivates the need to analyze
the current state of exploration tools and look beyond algorithmic changes of GUI
exploration strategy. However, their contributions are orthogonal to ours: the authors
propose a tool named Toller that decreases the time needed to interact with GUI
components and thus facilitates more efficient app exploration. Improvements made
in this paper are still bounded by the capabilities of human analysts.
40Azim and Neamtiu [ 11] leverage human input for app exploration through a
study with seven users who achieve around 30% coverage on average. However,
the authors report that the incomplete exploration by the users is mainly due to the
lack of knowledge of application features or lack of interest. In contrast, our work
involves skilled human analysts (motivated by the desire to explore a large fraction
of a mobile application). Furthermore, we perform a detailed code-level analysis
for the reasons of limited coverage.
The work closest to ours is probably by Zheng et al. [ 74], who propose a GUI-
based tool built on top of Monkey and manually analyze activities not covered by
the tool on one industrial app, WeChat. The authors identify reasons such as lack of
login or financial information, historical data, dead activity, etc., and tool-specific
reasons (e.g., text inputs, system events). They also provide insights into testing
efforts that must be applied moving forward, such as developer-provided seed data
and rules to guide testing. While our findings are similar, we provide a more
fine-grained, code-level classification to better understand properties of identified
reasons for unreachability (Section 3.2.4). A number of issues reported by Zheng
et al., would be reachable through GUI exploration alone by human analysts such
as enabling features in app settings, valid text inputs or long event sequences. In
constrast, we specifically focus on extracting exploration challenges for humans. We
extend our analysis to more than one app, and thus identify challenges not reported
by Zheng et al., i.e., hardware properties, environmental properties, error handling,
usage patterns, cases disabled for app version and transitive cases (marked with
in Table 3.3). We also report partially new findings such as interface-only cases
in the No caller category or deep link routing cases in the Alternate entry category
(marked with ). Finally, we propose a characterization of dynamic approaches
which could help address those challenges.
41Chapter 7
Summary and Conclusions
Lastly, we summarize our study and outline the main conclusions of this thesis.
7.1 Summary
In this thesis, we compared the activity-level coverage achieved by state-of-the-art
Android GUI-based application exploration techniques, with that of skilled human
analysts. To our surprise, the tool and the human analysts reached a similar level
of coverage: 22.4% and 28.4%, respectively. This result indicates that improving
application coverage by investing in GUI-based exploration strategies might not
bring substantial improvements and future work should rather focus on identify-
ing reasons for not reaching the remaining (>70%) of the applications. To make
progress in this direction, we performed a detailed code-level manual analysis of 11
applications from our dataset, extracting reasons for the lack of reachability. We
further analyzed existing dynamic techniques outside of the GUI-based exploration
domain, i.e., those that contribute proprietary exploration strategies for security,
performance, and functional application testing. We discussed the applicability of
these techniques to address coverage challenges in our dataset and possible research
directions.
427.2 Conclusions
The work described in this thesis describes the high number of dependencies for
application exploration which must be satisfied to perform meaningful dynamic
analyses. To reach as close to 100% coverage as possible, GUI-based exploration
should be complemented with approaches which can extract and manipulate these
dependencies, or at the very least, inform analysts in a semi-automated fashion.
The work aims to close the gap between app developers and app analysts’ knowl-
edge of applications, with the latter still focusing on obtaining sytem-level view of
applications. It is also important to scope the expectations from automated explo-
ration approaches, which only follows from better app explainability. For example,
whether the responsibility of exploring disabled code falls under the responsibility
of an exploration technique, a dynamic analysis or not, can only be determined
once clear identicators of disabling are extracted in the app and similarly for other
patterns discussed in this thesis. Thus, identifying and classifying dependencies of
an application is needed to gauge (1) the relevance of satisfying said dependency
based on the exploration goal and (2) the applicable strategies to satisfy or bypass it.
Additionally, this work highlights inherent limitations of GUI-based exploration
which are wrongly attributed to tools, in the absence of additional qualitative analy-
sis. With coverage or bug finding ability as the main measures of tool performance
or test suite effectiveness employed in the literature, more research effort should be
invested into building and maintaining ground truths. This would help to (1) focus
efforts on parts of an application that are truly reachable/relevant and (2) define
appropriate testing budgets given domain knowledge about the application under
test. De-facto testing benchmarks are outdated, lack annotations and might not be
reflective of the variety of issues observed in commercial applications (as seen in
our experiments).
This work aims to serve as a precursor through an extensive manual analysis, for
the much needed larger-scale research effort that must be invested into app explain-
ability, prior to designing suitable exploration techniques. We hope our detailed
mapping of exploration challenges and related approaches can help researchers
and practitioners develop more efficient, appropriate app exploration techniques,
consequently dynamic analysis tools.
43Bibliography
[1] Teams App Bug Report. https://techcommunity.microsoft.com/t5/
microsoft-teams/cannot-access-microsoft-teams/m-p/1212804#M46636 ,
2020.→page 11
[2] F-Droid. https://f-droid.org/ , 2023. →page 9
[3] Github. https://github.com , 2023. →page 9
[4] GoalExplorer. https://github.com/resess/GoalExplorer , 2023. →page 52
[5] Google Code Archives. https://code.google.com/archive/ , 2023. →page 9
[6] Monkey.
https://developer.android.com/studio/test/other-testing-tools/monkey , 2023.
→pages 1, 7
[7] A. Abraham, R. Andriatsimandefitra, A. Brunelat, J.-F. Lalande, and V . V . T.
Tong. GroddDroid: a gorilla for triggering malicious behaviors. In Proc of the
International Conference on Malicious and Unwanted Software (MALWARE) ,
pages 119–127, 2015. →pages 28, 32
[8] D. Amalfitano, A. R. Fasolino, P. Tramontana, S. De Carmine, and A. M.
Memon. Using GUI ripping for automated testing of Android applications. In
Proc. of International Conference on Automated Software Engineering (ASE) ,
pages 258–261, 2012. →page 29
[9] S. Anand, M. Naik, M. J. Harrold, and H. Yang. Automated concolic testing
of smartphone apps. In Proc. of the International Symposium on the
Foundations of Software Engineering (ESEC/FSE) , pages 1–11, 2012. →
pages 1, 7
[10] Anonymous. Supplementary Materials. https://anon-self.github.io/website/ ,
2023.→pages 4, 10, 12, 14, 30, 52
44[11] T. Azim and I. Neamtiu. Targeted and depth-first exploration for systematic
testing of android apps. In Proc. of the International Conference on
Object-Oriented Programming Systems Languages & Applications
(OOPSLA) , pages 641–660, 2013. →pages 1, 7, 41
[12] S. Badihi, F. Akinotcho, Y . Li, and J. Rubin. Ardiff: scaling program
equivalence checking via iterative abstraction and refinement of common
code. In Proceedings of the Joint Meeting on Foundations of Software
Engineering (ESEC/FSE) , pages 13–24, 2020. →page 52
[13] F. Behrang and A. Orso. Seven Reasons Why: an In-depth Study of the
Limitations of Random Test Input Generation for Android. In Proc.of the
International Conference on Automated Software Engineering (ASE) , pages
1066–1077, 2020. →page 40
[14] L. Bello and M. Pistoia. Ares: triggering payload of evasive android malware.
InProc. of the International Conference on Mobile Software Engineering and
Systems (MOBILESoft) , pages 2–12, 2018. →pages 3, 28, 32
[15] R. Bhoraskar, S. Han, J. Jeon, T. Azim, S. Chen, J. Jung, S. Nath, R. Wang,
and D. Wetherall. Brahmastra: Driving apps to test the security of third-party
components. In Proc. of the USENIX Security Symposium (USENIX) , pages
1021–1036, 2014. →pages 8, 28, 31, 34
[16] P. Bose, D. Das, S. Vasan, S. Mariani, I. Grishchenko, A. Continella,
A. Bianchi, C. Kruegel, and G. Vigna. COLUMBUS: Android App Testing
Through Systematic Callback Exploration. In Proc. of the International
Conference on Software Engineering (ICSE) , 2023. →pages
8, 9, 28, 31, 34, 35, 37
[17] M. Cao, K. Ahmed, and J. Rubin. Rotten apples spoil the bunch: An anatomy
of Google Play malware. In Proc. of the International Conference on
Software Engineering (ICSE) , 2022. →page 38
[18] E. F. Cataldo, R. M. Johnson, L. A. Kellstedt, and L. W. Milbrath. Card
sorting as a technique for survey interviewing. Public Opinion Quarterly , 34
(2):202–215, 1970. →page 17
[19] W. Choi, G. Necula, and K. Sen. Guided gui testing of android apps with
minimal restart and approximate learning. Acm Sigplan Notices , 48(10):
623–640, 2013. →pages 1, 7
45[20] S. R. Choudhary, A. Gorla, and A. Orso. Automated Test Input Generation for
Android: Are We There Yet? In Proc. of the International Conference on
Automated Software Engineering (ASE) , pages 429–440, 2015. →pages
2, 9, 39
[21] G. Developers. Platform Architecture.
https://developer.android.com/guide/platform , 2023. →page 5
[22] Z. Dong, M. Böhme, L. Cojocaru, and A. Roychoudhury. Time-travel testing
of android apps. In Proc. of the International Conference on Software
Engineering (ICSE) , pages 481–492, 2020. →pages 1, 7, 9, 12
[23] T. Gu, C. Sun, X. Ma, C. Cao, C. Xu, Y . Yao, Q. Zhang, J. Lu, and Z. Su.
Practical GUI Testing of Android Applications via Model Abstraction and
Refinement. In Proc. of the International Conference on Software
Engineering (ICSE) , pages 269–280, 2019. →pages 2, 4, 7, 9, 12
[24] W. Guo, L. Shen, T. Su, X. Peng, and W. Xie. Improving Automated GUI
Exploration of Android Apps via Static Dependency Analysis. In Proc. of the
International Conference on Software Maintenance and Evolution (ICSME) ,
pages 557–568, 2020. →pages 1, 7
[25] G. Hu, X. Yuan, Y . Tang, and J. Yang. Efficiently, effectively detecting mobile
app bugs with appdoctor. In Proc. of the Ninth European Conference on
Computer Systems , pages 1–15, 2014. →pages 8, 28, 33, 34
[26] R. Jabbarvand, J.-W. Lin, and S. Malek. Search-based energy testing of
android. In Proc. of the International Conference on Software Engineering
(ICSE) , pages 1119–1130, 2019. →pages 3, 8, 28, 33, 34
[27] C. S. Jensen, M. R. Prasad, and A. Møller. Automated testing with targeted
event sequence generation. In Proc. of the International Symposium on
Software Testing and Analysis (ISSTA) , pages 67–77, 2013. →page 29
[28] M. C. Júnior, D. Amalfitano, L. Garcés, A. R. Fasolino, S. a. A. Andrade, and
M. Delamaro. Dynamic Testing Techniques of Non-Functional Requirements
in Mobile Apps: A Systematic Mapping Study. ACM Computing Surveys , 54
(10s), 2022. →page 29
[29] P. Kong, L. Li, J. Gao, K. Liu, T. F. Bissyandé, and J. Klein. Automated
testing of android apps: A systematic literature review. IEEE Transactions on
Reliability (TR) , 68(1):45–66, 2018. →pages 11, 29
46[30] D. Lai and J. Rubin. Goal-driven exploration for android applications. In
Proc. of the International Conference on Automated Software Engineering
(ASE) , pages 115–127. IEEE, 2019. →pages 1, 7, 9, 12, 52
[31] Y . Lan, Y . Lu, Z. Li, M. Pan, W. Yang, T. Zhang, and X. Li. Deeply
Reinforcing Android GUI Testing with Deep Reinforcement Learning. In
Proc. of the International Conference on Software Engineering (ICSE) , 2024.
→page 1
[32] Y . Li, Z. Yang, Y . Guo, and X. Chen. Droidbot: a lightweight ui-guided test
input generator for android. In Proc. of the International Conference on
Software Engineering Companion (ICSE-C) , pages 23–26, 2017. →page 7
[33] Y . Li, Z. Yang, Y . Guo, and X. Chen. Humanoid: A deep learning-based
approach to automated black-box android app testing. In Proc. of the
International Conference on Automated Software Engineering (ASE) , pages
1070–1073, 2019. →pages 1, 7, 9, 12
[34] A. Liu, C. Guo, N. Dong, Y . Wang, and J. Xu. DALT: Deep Activity
Launching Test via Intent-Constraint Extraction. In Proc. of the International
Symposium on Software Reliability Engineering (ISSRE) , pages 482–493,
2022.→pages 8, 28, 31, 34, 35
[35] C. Luo, J. Goncalves, E. Velloso, and V . Kostakos. A survey of context
simulation for testing mobile context-aware applications. ACM Computing
Surveys (CSUR) , 53(1):1–39, 2020. →page 29
[36] Z. Lv, C. Peng, Z. Zhang, T. Su, K. Liu, and P. Yang. Fastbot2: Reusable
Automated Model-based GUI Testing for Android Enhanced by
Reinforcement Learning. In Proc. of the International Conference on
Automated Software Engineering (ASE) , pages 1–5, 2022. →pages 2, 4, 7, 12
[37] A. Machiry, R. Tahiliani, and M. Naik. Dynodroid: An input generation
system for android apps. In Proc. of the Joint Meeting on Foundations of
Software Engineering (ESEC/FSE) , pages 224–234, 2013. →pages 1, 7
[38] R. Mahmood, N. Mirzaei, and S. Malek. Evodroid: Segmented evolutionary
testing of android apps. In Proc. of the International Symposium on
Foundations of Software Engineering (ESEC/FSE) , pages 599–609, 2014. →
page 7
[39] K. Mao, M. Harman, and Y . Jia. Sapienz: Multi-objective automated testing
for android applications. In Proc. of the International Symposium on Software
Testing and Analysis (ISSTA) , pages 94–105, 2016. →pages 7, 9
47[40] N. Mirzaei, H. Bagheri, R. Mahmood, and S. Malek. Sig-droid: Automated
system input generation for android applications. In Proc. of the International
Symposium on Software Reliability Engineering (ISSRE) , pages 461–471,
2015.→pages 1, 7
[41] K. Moran, M. Linares-Vásquez, C. Bernal-Cárdenas, and D. Poshyvanyk.
Auto-completing bug reports for android applications. In Proc. of the Joint
Meeting on Foundations of Software Engineering (ESEC/FSE) , pages
673–686, 2015. →page 29
[42] K. Moran, M. Linares-Vásquez, C. Bernal-Cárdenas, C. Vendome, and
D. Poshyvanyk. Automatically discovering, reporting and reproducing
android application crashes. In Proc. of the International Conference on
Software Testing, verification and validation (ICST) , pages 33–44. IEEE,
2016.→pages 8, 9, 28, 33, 34
[43] N. Nurmuliani, D. Zowghi, and S. P. Williams. Using card sorting technique
to classify requirements change. In Proc. of the International Requirements
Engineering Conference (RE) , pages 240–248, 2004. →page 17
[44] M. Pan, A. Huang, G. Wang, T. Zhang, and X. Li. Reinforcement learning
based curiosity-driven testing of Android applications. In Proc. of the
International Symposium on Software Testing and Analysis (ISSTA) , pages
153–164, 2020. →pages 1, 7, 12
[45] J. Qin, H. Zhang, S. Wang, Z. Geng, and T. Chen. Acteve++: An improved
android application automatic tester based on acteve. IEEE Access , 7:
31358–31363, 2019. →pages 1, 9
[46] S. Rasthofer, S. Arzt, M. Miltenberger, and E. Bodden. Harvesting runtime
values in android applications that feature anti-analysis techniques. In Proc. of
the Network and Distributed System Security Symposium (NDSS) , 2016. →
pages 3, 8, 28, 31, 32, 35
[47] S. Rasthofer, S. Arzt, S. Triller, and M. Pradel. Making Malory Behave
Maliciously: Targeted Fuzzing of Android Execution Environments. In Proc.
of the International Conference on Software Engineering (ICSE) , pages
300–311, 2017. →pages 8, 28, 33, 34, 35
[48] A. Romdhana, A. Merlo, M. Ceccato, and P. Tonella. Deep reinforcement
learning for black-box testing of android apps. ACM Transactions on Software
Engineering and Methodology (TOSEM) , 31(4):1–29, 2022. →pages 1, 9
48[49] J. Schütte, R. Fedler, and D. Titze. Condroid: Targeted dynamic analysis of
android applications. In Proc. of the International Conference on Advanced
Information Networking and Applications (AINA) , pages 571–578, 2015. →
pages 8, 28, 31, 33, 34, 35
[50] S. Shi, X. Wang, and W. C. Lau. MoSSOT: An automated blackbox tester for
single sign-on vulnerabilities in mobile applications. In Proc. of the ACM
Asia Conference on Computer and Communications Security (ASIACCS) ,
pages 269–282, 2019. →pages 8, 28, 33, 34
[51] skylot. JADX. https://github.com/skylot/jadx, 2021. →page 16
[52] W. Song, X. Qian, and J. Huang. EHBDroid: Beyond GUI testing for Android
applications. In Proc. of the 32nd Conference on Automated Software
Engineering (ASE) , pages 27–37, 2017. →pages 8, 28, 31, 34
[53] T. Su, G. Meng, Y . Chen, K. Wu, W. Yang, Y . Yao, G. Pu, Y . Liu, and Z. Su.
Guided, Stochastic Model-based GUI Testing of Android Apps. In Proc. of
the Joint Meeting on Foundations of Software Engineering (ESEC/FSE) ,
pages 245–256, 2017. →pages 1, 7, 9
[54] T. Su, J. Wang, and Z. Su. Benchmarking Automated GUI Testing for
Android against Real-World Bugs. In Proc. of the Joint European Software
Engineering Conference and Symposium on the Foundations of Software
Engineering (ESEC/FSE) , pages 119–130, 2021. →pages 2, 12, 40
[55] H. Tang, G. Wu, J. Wei, and H. Zhong. Generating test cases to expose
concurrency bugs in android applications. In Proc. of the International
Conference on Automated Software Engineering (ASE) , pages 648–653, 2016.
→page 29
[56] T. A. T. Vuong and S. Takada. A reinforcement learning based approach to
automated testing of android applications. In Proc. of the International
Workshop on Automating TEST Case Design, Selection, and Evaluation
(A-TEST) , pages 31–37, 2018. →page 9
[57] J. Wang, Y . Jiang, C. Xu, C. Cao, X. Ma, and J. Lu. ComboDroid: generating
high-quality test inputs for Android apps via use case combinations. In Proc.
of the International Conference on Software Engineering (ICSE) , pages
469–480, 2020. →pages 1, 7, 12
[58] W. Wang, D. Li, W. Yang, Y . Cao, Z. Zhang, Y . Deng, and T. Xie. An
Empirical Study of Android Test Generation Tools in Industrial Cases. In
49Proc. of the International Conference on Automated Software Engineering
(ASE) , pages 738–748, 2018. →pages 2, 11, 39
[59] W. Wang, W. Lam, and T. Xie. An Infrastructure Approach to Improving
Effectiveness of Android UI Testing Tools. In Proc. of the International
Symposium on Software Testing and Analysis (ISSTA) , pages 165–176, 2021.
→pages 2, 12, 40
[60] W. Wang, W. Yang, T. Xu, and T. Xie. Vet: Identifying and Avoiding UI
Exploration Tarpits. In Proc. of the Joint Meeting on European Software
Engineering Conference and Symposium on the Foundations of Software
Engineering (ESEC/FSE) , pages 83–94, 2021. →pages 2, 40
[61] X. Wang, S. Zhu, D. Zhou, and Y . Yang. Droid-AntiRM: Taming control flow
anti-analysis to support automated dynamic analysis of android malware. In
Proc. of the Annual Computer Security Applications Conference (ACSAC) ,
pages 350–361, 2017. →page 32
[62] X. Wang, S. Zhu, D. Zhou, and Y . Yang. Droid-AntiRM: Taming control flow
anti-analysis to support automated dynamic analysis of android malware. In
Proc. of the Annual Computer Security Applications Conference (ACSAC) ,
pages 350–361, 2017. →page 28
[63] X. Wang, Y . Yang, and S. Zhu. Automated hybrid analysis of android
malware through augmenting fuzzing with forced execution. IEEE
Transactions on Mobile Computing (TMC) , 18(12):2768–2782, 2018. →
pages 3, 8, 28, 32, 33, 34, 35
[64] M. Y . Wong and D. Lie. Driving Execution of Target Paths in Android
Applications with (a) CAR. In Proc. of the ACM Asia Conference on
Computer and Communications Security (ASIACCS) , pages 888–902, 2022.
→pages 8, 28, 31, 33, 34, 35, 37
[65] Wong, Michelle Y and Lie, David. IntelliDroid: A Targeted Input Generator
for the Dynamic Analysis of Android Malware. In Proc. of the Network and
Distributed System Security Symposium (NDSS) , pages 1–15, 2016. →pages
3, 8, 28, 32, 33, 34, 35
[66] D. Wu, D. He, S. Chen, and J. Xue. Exposing android event-based races by
selective branch instrumentation. In Proc. of the International Symposium on
Software Reliability Engineering (ISSRE) , pages 265–276, 2020. →pages
3, 28, 32
50[67] L. Xue, Y . Zhou, T. Chen, X. Luo, and G. Gu. Malton: Towards On-Device
Non-Invasive Mobile Malware Analysis for ART. In Proc of the USENIX
Security Symposium (USENIX) , pages 289–306, 2017. →pages 28, 33, 34, 35
[68] J. Yan, H. Liu, L. Pan, J. Yan, J. Zhang, and B. Liang. Multiple-entry testing
of android applications by constructing activity launching contexts. In Proc.
of the International Conference on Software Engineering (ICSE) , pages
457–468, 2020. →pages 8, 28, 31, 34, 35
[69] H. N. Yasin, S. H. A. Hamid, R. J. R. Yusof, and M. Hamzah. An empirical
analysis of test input generation tools for android apps through a sequence of
events. Symmetry , 12(11):1894, 2020. →page 2
[70] H. Ye, S. Cheng, L. Zhang, and F. Jiang. Droidfuzzer: Fuzzing the android
apps with intent-filter tag. In Proc. of the International Conference on
Advances in Mobile Computing & Multimedia (MoMM) , pages 68–74, 2013.
→page 34
[71] X. Zeng, D. Li, W. Zheng, F. Xia, Y . Deng, W. Lam, W. Yang, and T. Xie.
Automated Test Input Generation for Android: Are We Really There Yet in an
Industrial Case? In Proc. of the International Symposium on Foundations of
Software Engineering (ESEC/FSE) , pages 987–992, 2016. →pages 1, 7
[72] L. L. Zhang, C.-J. M. Liang, Y . Liu, and E. Chen. Systematically testing
background services of mobile apps. In Proc. of the International Conference
on Automated Software Engineering (ASE) , pages 4–15, 2017. →pages
28, 33, 34
[73] Y . Zhao, T. Yu, T. Su, Y . Liu, W. Zheng, J. Zhang, and W. G. Halfond.
Recdroid: automatically reproducing android application crashes from bug
reports. In Proc. of International Conference on Software Engineering (ICSE) ,
pages 128–139, 2019. →page 29
[74] H. Zheng, D. Li, B. Liang, X. Zeng, W. Zheng, Y . Deng, W. Lam, W. Yang,
and T. Xie. Automated test input generation for android: Towards getting
there in an industrial case. In Proc. of the International Conference on
Software Engineering: Software Engineering in Practice Track (ICSE-SEIP) ,
pages 253–262, 2017. →pages 2, 11, 41
[75] C. Zuo and Z. Lin. Smartgen: Exposing server urls of mobile apps with
selective symbolic execution. In Proc. of the International Conference on
World Wide Web (WWW) , pages 867–876, 2017. →pages 8, 28, 31, 34, 35
51Appendix A
Supporting Materials
A.1 Additional Data
All additional data for this thesis can be found in our online appendix [10].
A.2 Additional Research Work
During my degree, I also contributed to ARDiff [ 12] - a novel symbolic execution
based technique for checking the equivalence of Java programs. This work has been
published as a conference paper but it is not included in this thesis:
•Sahar Badihi, Faridah Akinotcho , Julia Rubin, Yi Li, ARDiff: Scaling
Program Equivalence Checking via Iterative Abstraction and Refinement
of Common Code, ACM Joint Meeting on European Software Engineering
Conference and Symposium on the Foundations of Software Engineering
(ESEC/FSE), 2020.
I also worked on extending GoalExplorer [ 30] - aGUI exploration tool, with better
API support, improved memory-efficiency, and made my contributions publicly
available [4].
52