Data-Oriented Differential Testing of
Object-Relational Mapping Systems
Thodoris Sotiropoulos
Athens University of Economics and Business
Athens, Greece
theosotr@aueb.grStefanos Chaliasos
Athens University of Economics and Business
Athens, Greece
schaliasos@aueb.grVaggelis Atlidakis
Columbia University
New York, USA
vatlidak@cs.columbia.edu
Dimitris Mitropoulos
Athens University of Economics and Business
National Infrastructures for Research and Technology - GRNET
Athens, Greeece
dimitro@aueb.grDiomidis Spinellis
Athens University of Economics and Business
Athens, Greece
dds@aueb.gr
Abstract ‚ÄîWe introduce, what is to the best of our knowledge,
the Ô¨Årst approach for systematically testing Object-Relational
Mapping (ORM) systems. Our approach leverages differential
testing to establish a test oracle for ORM-speciÔ¨Åc bugs. SpeciÔ¨Å-
cally, we Ô¨Årst generate random relational database schemas, set
up the respective databases, and then, we query these databases
using the APIs of the ORM systems under test. To tackle the
challenge that ORMs lack a common input language, we generate
queries written in an abstract query language. These abstract
queries are translated into concrete, executable ORM queries,
which are ultimately used to differentially test the correctness
of target implementations. The effectiveness of our method
heavily relies on the data inserted to the underlying databases.
Therefore, we employ a solver-based approach for producing
targeted database records with respect to the constraints of the
generated queries. We implement our approach as a tool, called
CYNTHIA, which found 28 bugs in Ô¨Åve popular ORM systems.
The vast majority of these bugs are conÔ¨Årmed (25 / 28), more
than half were Ô¨Åxed (20 / 28), and three were marked as release
blockers by the corresponding developers.
Index Terms‚ÄîObject-Relational Mapping, Differential Testing,
Automated Testing
I. I NTRODUCTION
Object-Relational Mapping (ORM) is an established pro-
gramming technique [1], [2], [3] that has emerged as a solution
to the Object-Relational Impedance Mismatch problem [4],
[5]. ORM provides an object-oriented interface atop relational
databases. Through that, the objects of a program can be
easily saved and retrieved from the secondary storage without
requiring boilerplate code for mapping application data to
database records. ORM not only boosts developer productivity
and reduces maintenance costs [4], [6], but also promotes
portability because it abstracts away differences of Database
Management Systems (DBMS) [4], [6].
Currently, there is a plethora of ORM implementations:
through a simple Github search, one runs into more than 50
ORM frameworks, written for almost every language. Indica-
tive examples include Django and SQLAlchemy for Python,
Hibernate for Java, ActiveRecord for Ruby, and Sequelize forJavaScript. These systems are used by millions of applica-
tions [7] and are adopted by many popular organizations, such
as Dropbox, Gitlab, and OpenStack [8], [9], [10].
Despite their wide industrial adoption, the automated testing
of ORM systems is an overlooked problem. Current testing
efforts mainly use manually-written test suites, which, as we
demonstrate, are often insufÔ¨Åcient for ensuring the correctness
of ORM implementations. Yet, ORM implementations are
complex [4] (typically consist of thousands lines of code) and,
unfortunately, involve a high density of bugs. For example,
the ORM implementation in the Django web framework is
the component that suffers from the most bugs [11]: 23% of
the reported bugs in Django are related to the ORM compo-
nent, and they are signiÔ¨Åcantly more than the reported bugs
associated with the secondly affected component (8%). Such
ORM bugs lead to incorrect interactions with the underlying
database and cause frustrating crashes [12], wrong store and
retrieval of data [13], and even security vulnerabilities [14],
[15].
To detect bugs in ORM implementations, we propose a
differential testing approach. At a high-level, our approach
exercises ORM systems by constructing equivalent queries
written in the target ORM implementations, and then compares
query results for mismatches. We begin by generating a ran-
dom database schema used to set up databases across multiple
DBMSs. We test the functionality of ORMs by querying
the databases using each ORM‚Äôs API. However, since ORM
systems do not share a common input format, we design an
abstract query language which is close to ORM APIs. This
allows us to build expressive queries that exercise diverse func-
tionality combinations across ORM implementations. Finally,
we use ORM-speciÔ¨Åc translators to convert abstract queries
into concrete ones, which are executable in the corresponding
ORM implementations.
Our differential testing approach is data-oriented : beyond
queries, it is the data inserted to the underlying databases
that affect the effectiveness of the testing efforts. We em-
15352021 IEEE/ACM 43rd International Conference on Software Engineering (ICSE)
1558-1225/21/$31.00 ¬©2021 IEEE
DOI 10.1109/ICSE43902.2021.00137
1from django.db import models
2class Person(models.Model):
3 age = models.IntegerField()
4 name = models.CharField(max_length=20)
5...
6p1 = Person(age=31, name= "John" )
7p1.save()
8p2 = Person.objects.get(age=32)
9p2.delete()
Fig. 1: Example CRUD operations using the Django ORM.
ploy a solver-based approach for generating targeted database
records with respect to the constraints of the generated ab-
stract queries. This improves the effectiveness of differential
testing because it minimizes the number of queries where
ORMs return empty results. Our approach goes beyond the
existing body of work in compiler and programming language
testing [16], [17], [18], [19], [20], and addresses several
challenges speciÔ¨Åc to ORM systems, such as lack of a common
input, data generation, database schema generation, or DBMS
setup. SpeciÔ¨Åcally, we make the following contributions:
We introduce the Ô¨Årst automatic, data-oriented differential
testing approach for ORM system implementations.
We implement CYNTHIA , an extensible open-source frame-
work for systematically testing well-established ORM im-
plementations.
We provide experimental evidence showing that our solver-
based approach is an effective technique to generate data
that are useful for differential testing.
We use CYNTHIA to test Ô¨Åve popular ORM systems on four
widely-used database engines, and Ô¨Ånd 28unique bugs. The
vast majority of these bugs are conÔ¨Årmed ( 25/28), more
than half were Ô¨Åxed ( 20/28), and three were marked as
release blockers by the corresponding developers.
Availability. Our system, CYNTHIA , is available as open-
source software under the GNU General Public License v3.0
at https://github.com/theosotr/cynthia. The research artifact is
available at https://doi.org/10.5281/zenodo.4455486.
II. B ACKGROUND & M OTIVATION
We provide a brief overview of object-relational mapping
and an illustrative example of bugs that our approach can
detect in the related tools and frameworks. Then, we brieÔ¨Çy
explain why we adopt differential testing for detecting these
bugs. Finally we enumerate the main challenges associated
with differential testing of object-relational mapping systems.
A. Object-Relational Mapping Systems
Object-Relational Mapping provides an abstraction over
relational data that enables programmers to interact with their
databases through the object-oriented programming paradigm.
In this context, a database schema (tables and their inter-
relationships) is abstracted through classes, called models , and
the associated database records are represented via objects
of these classes. ORM systems then provide a rich API for
basic Create, Read, Update, and Delete (CRUD) operations
on database records as well as more advanced features, such
as transaction management or query caching.1q1 = T1.objects.using( "mysql" )
2q2 = T2.objects.using( "mysql" )
3q3 = T3.objects.using( "mysql" )
4//ProgrammingError: "You have an error in your
SQL syntax"
5q1.union(q2).union(q3)
6// Generated SQL
7(SELECT ‚Äòt1‚Äò.‚Äòid‚Äò FROM ‚Äòt1‚Äò)
8UNION (
9 (SELECT ‚Äòt2‚Äò.‚Äòid‚Äò FROM ‚Äòt2‚Äò)
10 UNION
11 (SELECT ‚Äòt3‚Äò.‚Äòid‚Äò FROM ‚Äòt3‚Äò))
Fig. 2: Django generates MySQL query with invalid syntax.
Figure 1 shows an example of database interactions using
the Django ORM system [9]. The code Ô¨Årst declares a class
that maps to a table and to its associated columns in the
underlying database (lines 2‚Äì4). Using this class, the code
then runs simple queries. SpeciÔ¨Åcally, the code creates a class
object (line 6), and based on this object, creates a new database
record by calling the save() method (line 7). Then, the code
fetches a single record from the database matching certain
criteria (line 8), and then deletes this record (line 9).
ORM system APIs provide a higher level of abstraction that
hides the mechanics of SQL queries from the programmer. For
example, the save() method results in an SQL INSERT
statement, which remains transparent to the programmer.
B. Bugs in Object-Relational Mapping Systems
To motivate the design of our testing approach, we discuss
two indicative bugs found in well-established ORM systems.
Bug in Django. Consider the Django query shown in
Figure 2 (lines 1‚Äì5). This query Ô¨Årst fetches the records
of tables t1,t2, and t3(lines 1‚Äì3), and it then per-
forms a chain of unions (line 5) in order to combine the
results of the individual queries. When we run this Django
code on MySQL (version 8.0.4), Django produces and runs
the SQL query shown on lines 7‚Äì11. This SQL query is
invalid on MySQL and the Django program crashes with
adjango.db.utils.ProgrammingError: (1064, ‚ÄúError in SQL
syntax; check the manual that corresponds to your MySQL
server version for the right syntax to use near ‚ÄôUNION‚Äô‚Äù) .
This bug was detected by our approach, and was conÔ¨Årmed
by the Django developers.
When the Django code shown in Figure 2 is run on another
DBMS, such as SQLite or PostgreSQL, Django produces a
valid SQL query. Such inconsistencies indicate that ORM bugs
may appear (or not) depending on the underlying DBMS.
Although DBMSs share common functionality, they differ
signiÔ¨Åcantly from each other [21]. Therefore, an ORM needs
to abstract away such differences and take care of running the
same ORM code on different DBMSs reliably. Unfortunately,
this complicates the design of ORMs: bugs may occur when
an ORM fails to produce a valid SQL query with respect to a
certain DBMS.
Bug in peewee. Figure 3 shows another ORM bug detected
by our approach. On lines 1‚Äì3, the code creates a simple
query using the peewee ORM. The query deÔ¨Ånes a simple
15361expr = (1 + T.col)
2squared = (expr *expr)
3T.select(fn.sum(expr), fn.avg(squared)).all()
4// Generated SQL
5SELECT SUM(1 + "t"."col" ),
6 AVG(1 + "t" ."col" *1 +"t" ."col" )
7FROM "t" AS"t"
Fig. 3: Logic error detected in peewee ORM.
expression expr given by the addition of a table‚Äôs column
with 1 (line 1). The code then forms a simple query that
applies the function SUM toexpr , and AVG to the square of
expr (lines 2, 3). The peewee ORM translates this high-level
query into the incorrect SQL query shown on lines 5‚Äì7. In this
SQL query, the expression passed to the aggregate function
AVG is not in the expected format because the sub-expressions
are not wrapped in parentheses: peewee incorrectly produces
AVG(1 + col1 +col)instead of AVG((1 + col)(1 + col)).
This bug was conÔ¨Årmed and Ô¨Åxed by the peewee developers
immediately after our report.
Unlike the Django bug discussed earlier, this peewee bug is
more subtle: Although peewee generates a grammatically and
semantically valid SQL query, this query produces incorrect
results. Unlike crashes, such subtle bugs cannot be detected
through a naive fuzzing approach. This explains our primary
design choice to adopt differential testing.
C. Differential Testing of ORM Systems
To Ô¨Ånd bugs similar to the ones discussed above, we need
to systematically determine whether the SQL query generated
by an ORM system is correct or not. To do so, we need to
deÔ¨Åne a test oracle. Nevertheless, establishing a test oracle for
ORM-speciÔ¨Åc bugs is not straightforward. For example, we are
unable to decide whether the SQL query generated by Django
(Figure 2) is incorrect, unless we have domain knowledge
that nested unions are indeed supported by MySQL, and
therefore, it is a bug from Django which failed to produce
a grammatically correct SQL query involving nested unions.
Worse, there is no an easy way to tell that the peewee bug of
Figure 3 is buggy. Although this query runs successfully on
all DBMSs, we cannot be sure that this query indeed fetches
the expected results from the database.
To address the test oracle problem, we employ differential
testing [22], a generally-applicable method for testing equiv-
alent implementations. Differential testing provides us with
an oracle as follows. We feed the same test input (e.g.,
query) to two equivalent implementations (e.g., Django and
peewee), and then compare their results. A mismatch found
in the results of the implementations under test indicates a
potential bug in at least one of them. For example, through
differential testing, we run the query associated with nested
unions (Figure 2) on MySQL, this time using the API of
peewee. Peewee executes the given query on MySQL without
errors. This helps us to identify that there is a bug in Django
implementation. Similarly, for the peewee query shown in
Figure 3, we construct its counterpart written in Django, only
to see that Django and peewee produce different results.D. Challenges
Our approach is inspired by prior work on compiler and
programming language testing [16], [17], [18], [19], [20], a
domain where differential testing has been successfully used
in the past. However, applying differential testing on ORM
systems is not straightforward, and it involves several new
challenges.
Challenge 1: Lack of a common speciÔ¨Åcation and input
language. ORM systems do not implement a common spec-
iÔ¨Åcation or standard. Therefore, differences in ORM results
may be due to valid but inconsistent implementations and not
due to actual bugs. Furthermore, each ORM offers its own
APIs and, to make matters worse, these APIs may even be
exposed through different programming languages. As a result,
differential testing cannot be uniformly applied to test ORM
systems in a straightforward manner.
Challenge 2: Non-deterministic query results. In some
ORM systems, it is possible to write a query that leads to an
SQL statement that produces a non-deterministic result, i.e.,
the result depends on the implementation of the underlying
DBMS. An example of such query is when the results are not
ordered. In this case, the DBMS is free to return results in any
order. Another example is when the resulting SQL query has
a columnaand an aggregate function in the SELECT part,
but the query does not deÔ¨Åne a GROUP BY clause on the
columna. According to the SQL standard, selecting a column
and an aggregate function, without specifying a GROUP BY
clause leads to an ambiguous query whose results are not
deterministic. To compare the results of the ORM systems
under test in a meaningful way, we have to deal with this
non-determinism.
Challenge 3: DBMS-dependent results. As shown previ-
ously (Figure 2), there are ORM bugs that are DBMS-speciÔ¨Åc,
i.e., the bugs are triggered only when the ORM code works
on a certain DBMS. To effectively capture such bugs, we need
to differentially test the ORM systems on multiple DBMSs.
At the same time, though, differences between the underlying
DBMSs (e.g., two DBMSs may have different semantics on
arithmetic expressions) must not affect the comparisons of
ORMs. Finally, for performing safe comparisons, the ORM
code needs to run on a common reference, i.e., the ORM
queries need to run on the same database.
Challenge 4: Data generation. Beyond ORM queries, we
have to generate appropriate data to populate the databases
so that ORM systems produce non-trivial results in response
to given queries. In this way, we can reveal logic errors that
cause ORMs to fetch the wrong data from the database. For
example, it is impossible to detect the peewee bug of Figure 3
when the underlying database contains no records.
III. T ESTING APPROACH
Our approach for testing ORMs is automated as shown
in Figure 4. It takes as input the ORM systems to test,
and the DBMSs where the ORM queries will run. Schema
Generation is an initial phase where we generate a number of
relational database schemas. During the Setup phase, we build
1537Iterate per SchemaIterate per Abstract Query   Bug Detection  SetupSchema GenerationSchema1Build Database2Generate ORM Models   Concretization of Abstract Queries4DBModelsDBModelsGenerate RecordsTranslate
SchemanAbstract Query GenerationAbstract Query3ORM QueryExecuteCompare5BugAbstract Queryn1. Random generation of a speciÔ¨Åed number of schemas2. Building the databases (e.g. SQLite) with an SQL script based on Scheman .Then we generate corresponding models (e.g. Django) by introspecting the database3. Abstracting SQL- and ORM-speciÔ¨Åc details through our Abstract Query Language (AQL)4. Populating the databases via a solver-based approach and generating ORM queries based on a speciÔ¨Åc Abstract Query (Abstract Queryn )5. Executing ORM queries and comparing the results per database to check for potential bugsFig. 4: Overview of our approach for automatically testing ORMs.
the different databases‚Äîone for each provided DBMS‚Äîwith
respect to the schema generated during the Ô¨Årst step. Then,
we proceed to the Abstract Query Generation phase which in-
volves the generation of queries written in the Abstract Query
Language (AQL). We design this language to abstract ORM-
and SQL-speciÔ¨Åc details and provide a common reference for
testing ORMs, thus addressing ‚ÄúChallenge 1‚Äù. By design, AQL
queries never lead to ambiguous ORM queries (‚ÄúChallenge
2‚Äù). However, AQL queries may be unordered. In this case, we
interpret query results as a set of rows rather than a sequence
(see also Section III-D). In the Concretization of Abstract
Query phase we use ORM-speciÔ¨Åc translators to translate each
query into a concrete one. To deal with ‚ÄúChallenge 4‚Äù and
minimize the number of cases where ORMs produce empty
results, we synthesize database records using a solver-based
approach. In the last step, i.e. Bug Detection , we execute the
ORM queries on diverse DBMSs, and compare their results. A
mismatch in the outputs indicates a potential bug in at least one
ORM. Notably, testing the ORM code across different DBMSs
enables us to Ô¨Ånd DBMS-dependent bugs (‚ÄúChallenge 3‚Äù).
A. Schema Generation & Setup
We generate a number of schemas that capture the structure
of the databases on which each ORM under test operates. Each
schemasis a collection of tables and their associated columns.
Each column has a type that can be a serial (primary key of
the table), a number (i.e., integer orreal), a string, or foreignt
which indicates a table‚Äôs relationship with another table tof
the schema. We omit schema details such as indexes, views or
column constraints (e.g., unique), as these constructs do not
affect the querying and translation mechanisms of ORMs, and
therefore, are beyond the scope of this paper.
Our method randomly generates a user-deÔ¨Åned number of
schemas. For each table, the schema generation algorithm
creates a serial column named ‚Äúid‚Äù that stands for the primary
key of the table, to guarantee that each record in the table
is unique and that there is no ambiguity in the data inserted
into the table. The remaining Ô¨Åelds of the table are randomly
generated (optionally based on a deterministic procedure).
We use the schemas generated in the previous step to set up
and instantiate the respective DBMSs and ORMs. To set up
DBMSs, we automatically construct an SQL script containing
allCREATE TABLE statements for creating the tables deÔ¨Åned
in a provided schema along with their columns. Then, weq2Query ::=evalqsjqs[i]jqs[i:i]jfoldf(l: e)+gqs
qs2QuerySet ::=newtjapply qsjqs[qs
jqs\qs
2Func ::=Ô¨Ålterpjmapdjunique
jsort(asc)jsort(desc)
d2FieldDecl ::=l:ejhiddenl:ejd;d
p2Pred::=ejp^pjp_pj:p
e2Expr::=cjj eje+eje ejeeje=e
2Field ::=t:cjlj:c
2AggrFunc ::=countjsumjavgjmaxjmin
2 BinaryOp ::= =j>jj<j
jcontainsjstartswithjendswith
Fig. 5: The syntax of the Abstract Query Language (AQL).
automatically generate the models for each ORM under test
by examining the structure of the newly-created databases.
To this end, we leverage tools used to ease ORM porting
to existing databases. These tools make a connection to an
existing database, introspect its structure, and automatically
construct the respective ORM model classes. An example of
such tool is the command manage.py inspectdb found
in the Django project [23].
B. Abstract Query Generation
Following the Schema Generation & Setup phases, we
start a testing session for each individual schema. A testing
session involves the generation of multiple valid queries (with
respect to the provided schema) that are likely to reveal bugs
in the ORMs under test. These queries are represented in
theAbstract Query Language (AQL) , which is close to the
APIs and the functionality of ORMs, and provides a wide
range of operations (through a functional notation) that are
commonly supported by the querying mechanism of ORMs.
AQL operations include Ô¨Åltering, sorting, aggregate functions,
creation of compound expressions, Ô¨Åeld aliasing, or union
and intersection of queries. By contrast, raw SQL dialect
is too low-level and many ORMs are not aware of SQL
constructs. Also, the SQL language is not rich enough to
express and capture the different API calls of ORMs. For
example, the same SQL query can be produced by calling
different combinations of ORM‚Äôs API methods. Since our
focus is on detecting bugs in ORMs by exercising different
combinations of their API calls, we design AQL.
1) Abstract Query Language: Figure 5 shows the syntax of
AQL. A query in AQL is the evaluation of a query set ( evalqs).
15381apply (filter "addCol" > 5
2 apply (map "addCol" : t1.colA + t1.t2.colB
3 new t1))
4
5SELECT t2.colA + t2.colB AS"addCol"
6FROM t1 as "t1"
7JOIN t2AS"t2" ON (t1.t2_id = t2.id)
8WHERE (t2.colA + t2.colB > 5)
Fig. 6: Example AQL query and its equivalent SQL query.
Conceptually, a query set evaluates to a set or to a sequence of
records (in case the query set is ordered). Operations such as
indexing or slicing, can be applied to the result of a query set,
while AQL also supports folding. The function foldaggregates
the result of a query set into labeled scalar values by applying
one or more aggregate functions.
The simplest form of a query set is newt, which creates a
new query set from the speciÔ¨Åed table t. When this query is
evaluated, it returns all records of the table t. Then, various
operations can be applied to a query set through the apply
construct. In particular, AQL provides the Ô¨Ålterpfunction
that returns all records of the query set that satisfy the given
predicatep. The map function is used to create new compound
Ô¨Åelds using existing Ô¨Åelds found in the given query set.
SpeciÔ¨Åcally, map expects a sequence of Ô¨Åeld declarations of
the forml:e. This declaration creates a new Ô¨Åeld in the
current query set by binding the expression eto the label
l. Optionally, a Ô¨Åeld can be marked as hidden meaning that
it is not part of the query set, but it is used for creating
other Ô¨Åelds (hidden Ô¨Åelds are similar to temporary variables).
The function sort, sorts the provided query set according to
the Ô¨Åeldin an ascending or a descending order, while
theunique primitive removes duplicate records with respect to
the provided Ô¨Åeld . Finally, AQL supports the combination of
two query sets through the union and intersection operations.
A predicate consists of comparison operators (i.e., e)
which are used to compare the value of a Ô¨Åeld with the
result of an expression e. A predicate may also contain the
usual logical operators. An expression can be a constant c,
a Ô¨Åeld reference , an application of an aggregate function,
or an expression derived from the usual arithmetic operators.
Finally, a Ô¨Åeld 2Field may be a reference to a column of
a table, i.e., t:c, a labellcreated by the map function, or a
reference to a column of a table‚Äôs relationship (e.g., t1:t2:c).
Figure 6 shows an example query written in AQL and its
equivalent query written in SQL. In this AQL query, we apply
two functions. First, we apply map to the query set given
bynew t1 (lines 2‚Äì3) in order to create a new Ô¨Åeld named
‚ÄúaddCol‚Äù given by the addition between the t1.colA and
t1.t2.colB columns. Notice that since the latter column
refers to a column of the table t2, which has a relationship
with the original table t1, in SQL this is interpreted as a JOIN
between t1andt2(line 7). Finally, we apply filter to get
the records satisfying addCol > 5 (line 1).
Remark. AQL currently supports only read operations. The
implementations of ORM API methods associated with read
operations are much more complex than those related to writeAlgorithm 1: Generating Abstract Queries
1fungenQuerySet( , min, max )=
2 stopCond [depth ]>min^([depth ]>max_
randBool ())
3 ifstopCond then[qs]
4 else
5 match chooseFrom( [qsNodes]) with
6 case NewNode)
7 t chooseTable ([schema ])
8 2 [qs!New(t),t!t]
9 genQuerySet( 2++, min, max )
10 case FilterNode)
11 p genPred(++, min, max )
12 2 [qs!Apply(Ô¨Ålter, p,[qs])]
13 genQuerySet( 2++, min, max )
14 case ...)
15
16fungenPred(, min, max )=
17 match chooseFrom( [predNodes]) with
18 case EqPredNode)
19f choosseField ()
20 Eq(f,genExpr(++, min, max ))
21 case ...)
22
23fungenExpr(, min, max )=
24 match chooseFrom(exprNodes) with
25 case FieldRefNode)
26 Field (chooseField( ))
27 case ...)
operations (a write operation is straightforwardly translated
intoINSERT ,DELETE orUPDATE queries). Thus, examining
read operations for Ô¨Ånding bugs is more promising. Note
though that AQL can be easily extended for supporting write
queries. Also, supporting write operations would not require
to take into account schema properties that we are currently
ignoring (e.g., column constraints), because such properties
affect the conÔ¨Åguration of ORM models and not the way an
ORM translates a write query into an SQL statement.
2) Generating AQL Queries: Algorithm 1 shows how
we generate random AQL queries. Our algorithm generates
queries that exercise all of the features supported by AQL, as
well as different combinations of them. The main component
of Algorithm 1 is the genQuerySet function (lines 1‚Äì14).
This function generates an AQL query set by recursively
constructing a valid AST node based on the syntax of Figure 5.
The algorithm ensures that the depth of the resulting query set
ranges within speciÔ¨Åc limits speciÔ¨Åed by the user-provided pa-
rameters minandmax (see stopCond , line 2). The parameter 
keeps track of the state of the query set that is being generated.
The initial state contains the schema ( [schema ]) based on
which the algorithm creates table and column references. For
what follows, the operation ++results in a new state where
the value of [depth ]is incremented.
Our algorithm Ô¨Årst constructs a new query set ( newt) that
queries a certain table (lines 6‚Äì9). To do so, we randomly
choose a table to query from the underlying schema (line 7).
Then, the algorithm updates the state in order to properly
build the next available AST node in the next iteration. In
particular, it initializes the AST of the current query set to
1539P(c;i)!c
P(f;i)!fi
P(e1e2;i)!P(e1;i)P(e2;i)
A(c;g)!c
A(f;g)!P(f;i)i2g
A(counte;g)!len(g)
A(sume;g)!X
i2gP(e;i)
A(avge;g)!(X
i2gP(e;i))=len(g)
A(maxe;g)!max(e, g)
A(mine;g)!min(e, g)
A(e1e2;g)!A(e1;g)A(e2;g)
max(e;g) =8
><
>:P(e;i) g=fig
ite(P(e;i)>P(e;j);P(e;i);P(e;j))g=fi;jg
ite(P(e;i)>max(e;g0);P(e;i);max(e;g0))g=ig0
min(e;g) =:::
Fig. 7: Translating AQL expressions into SMT formulae.
New(t), while it sets the queried table to t(line 8). Then, it re-
cursively calls genQuerySet to construct the next available
AST nodes (line 9). For example, on lines 10‚Äì13, the algorithm
applies Ô¨Ålter to the current query set given by [qs]. To achieve
this, the algorithm randomly generates a predicate pusing the
function genPred (lines 11, 16‚Äì21), and then extends the
AST of the current query set to Apply(Ô¨Ålterp;[qs])(line 12).
The AQL predicates and expressions are generated in a similar
manner (see lines 16‚Äì21, 23‚Äì27). Finally, after producing a
valid query set qs, we randomly decide for any operations
applied toqs, i.e., slicing, indexing, or folding.
C. Concretization of Abstract Queries
During this phase, our approach derives multiple, concrete
ORM queries (one for each target ORM) using ORM-speciÔ¨Åc
translators (Section III-C2). Before producing these queries,
our method populates the underlying databases with targeted
data in order to enable differential testing (Section III-C1).
1) Generating Database Records: We follow a solver-based
approach for generating a small number of targeted database
records that satisfy the constraints of a given AQL query.
SpeciÔ¨Åcally, we model an AQL query and its constraints
into SatisÔ¨Åability Modulo Theories (SMT) formulae which we
pass to a theorem prover. The theorem prover then solves the
given SMT formulae and generates assignments that stand for
the records inserted into the database. This approach improves
the effectiveness of differential testing, as the corresponding
ORMs will likely return non-empty results which in turn, can
be used for detecting discrepancies in ORM outputs. In the
following, we explain how we model an AQL query to SMT
formulae.
Modeling table columns. We introduce a sequence of
variables for every column of the queried table. Each variable
in this sequence, namely xi, represents the value of the column
xin theithrecord of the table, where 1in, andn
is a speciÔ¨Åed number of records inserted into the database.
After declaring these variables, we model the uniqueness of thetable‚Äôs id. To this end, we introduce the following constraint:
idi6=idjfor1in, whereidirefers to the id of the ith
record. Now, for what follows, F(t1;t2)iis the value of the
foreign key deÔ¨Åned in t1and refers to the table t2, in theith
record oft1, whileV(t)gives the set of columns deÔ¨Åned in
tablet, except for its id column.
Modeling joins. An AQL query may refer to columns
deÔ¨Åned in tables joined with the initial one. We traverse
the AST of the given AQL query to identify such column
references and compute the set of joins. For example, when
encountering the t1:t2:creference, we know that there is join
from table t1tot2. After computing the set of joins, we
introduce new variables for the columns of every joined table
as we did for the root table. Then, for a join between two tables
t1,t2, we create the following constraints, for 1i<jn:
F(t1;t2)i=id(t2)i
F(t1;t2)i=F(t1;t2)j)V
v2V(t2)vi=vj
The Ô¨Årst constraint indicates that the foreign key of the source
tablet1must be the same with the id of the target table t2for
all the records of t1. The second constraint denotes that when
there are two records in t1, namelyiandj, where the values
of the foreign keys for t2are equal, all column values of the
joined table t2must be also equal in the respective rows (e.g.,
vi=vjforv2V(t2)). The last constraint ensures that two
records oft2with the same id are identical.
Modeling AQL predicates. We model AQL predicates
using two different ways, depending on whether the given
predicate contains expressions consisting of an aggregate
function (e.g., sum) or not. The simplest case is when a
predicate does not contain an aggregate function. Such a
predicate operates on all the records of the table. Converting
a non-aggregate predicate is straightforward. For example, we
convert the AQL equality predicate t:c=einto:
9i: t:c i=P(e;i)for1in
In the above formula, t:ciis the SMT variable that represents
the value of the column t:cin theithrecord of the table, while
the function P(e;i)encodes the given AQL expression einto
a logical formula as shown in Figure 7. The above logical
formula encodes the constraint that there must be at least one
record in the table where the value of the column t:cis equal
with the value of the expression e.
An AQL predicate containing an aggregate function works
on aggregated data formed by groups of records, and is
conceptually similar to a condition that appears in the HAVING
clause of an SQL query. To model such predicates as logical
formulae, we Ô¨Årst create a set G, consisting of a speciÔ¨Åed
number of groups of records. Each group g2Gincludes all
records that are identical based on a set of grouping Ô¨Åelds
GF. To compute the set of grouping Ô¨Åelds GF, we traverse
the AST of the given AQL query and add all column references
that are not passed to an aggregate function. We then generate
constraints so that the records of the same group are identical
with respect to each Ô¨Åeld found in GF. Finally, we model
aggregate predicates and their AQL expressions using the
15401import os, django
2from django.db.models import *
3os.environ.setdefault( "DJANGO_SETTINGS_MODULE" ,
4 "djangoproject.settings" )
5django.setup()
6from project.models import *
7
8addCol = F( "colA" ) + F( "t2__colB" )
9q = T1.objects.using( "sqlite" )\
10 .annotate(addCol=addCol). filter (addCol__gt=5)\
11 .values( "addCol" )
12 for rinq:
13 print ("addCol" , r[ "addCol" ])
Fig. 8: The Django code related to the AQL query of Figure 6.
functionA(e;g)as deÔ¨Åned in Figure 7. For example, the AQL
predicatet:a=sumt:bis translated into:
9g2G: A(t:a;g) =A(sumt:b;g)
In the example above, A(t:a;g)gives the SMT variable of the
columnt:aassociated with a random record of the group g.
This is because t:ais a grouping Ô¨Åeld (it is not part of an
aggregate function) and all the records of gare the same with
respect to the value of t:a. On the other hand, A(sumt:b;g)
aggregates all records of the group gbased on the column t:b,
i.e.,P
i2gP(t:b;i). As Figure 7 indicates, the main difference
between the functions P(e;i)andA(e;g)is that the former
encodes the expression eas an SMT formula with regards to
the recordi, while the latter reasons about a group of records.
Modeling Unions & Intersections. Modeling unions and
intersections is straightforward. Each sub-query of such an
operation (e.g., qs1[qs2) is translated into an SMT formula
separately. Then the individual formulae are combined through
logical operators. For unions, we use the disjunction operator
(_), while we use^in case of intersection.
2) From Abstract Queries to Concrete ORM Queries: A
translator takes an AQL query, converts it into an ORM query,
and produces an executable Ô¨Åle that runs the ORM query
on a speciÔ¨Åed DBMS. Hence, a translator produces multiple
executable Ô¨Åles, one for each provided DBMS.
Every translator consists of three components. The Ô¨Årst
component adds the necessary boilerplate code for running
the ORM query (e.g., imports, creating the connection with the
database, etc.). The second component performs the transla-
tion. SpeciÔ¨Åcally, it uses the API of the corresponding ORM to
generate the actual ORM query. The last component dumps the
results of the query to standard output, again by using the API
of the speciÔ¨Åed ORM. When the query produces a sequence
of records, the translator produces code that iterates over each
element of the sequence and prints this element to standard
output. To properly dump a record, the translator emits code
that prints the value of every Ô¨Åeld deÔ¨Åned in the AQL query.
For example, when the query contains an application of map,
the translator produces code that prints the value of every non-
hidden Ô¨Åeld deÔ¨Åned in map. When the given query does not
apply map, then the id of the fetched records is printed. Finally,
for queries returning scalar values (i.e., fold), the translator
emits code that prints these scalar values.Figure 8 shows the executable Ô¨Åle that corresponds to the
AQL query of Figure 6 and is produced by the Django trans-
lator. Notice that this Ô¨Åle runs the Django query on SQLite.
Lines 1‚Äì6 contain the necessary setup code for running the
query, the actual Django query is on lines 8‚Äì11, while on
lines 12‚Äì13, we print the results of the query.
D. Bug Detection
The last step of our testing approach is to run the exe-
cutables produced by the translators and compare the output
of these executables for mismatches. To do so, we run every
executable and capture its standard output and standard error.
Our approach makes DBMS-speciÔ¨Åc comparisons: the out-
put of a query qwritten in ORM o1and run on DBMS
xis compared against the same query qwritten in another
ORMo2and run on the same DBMSx. We do this because
certain query features may be unsupported by some DBMS
(e.g., MySQL does not support intersection queries.) Based
on the above, our approach identiÔ¨Åes mismatches and Ô¨Çags
them as bugs, when one of the following conditions holds:
(1) the same query written in two different ORMs produces
different results on the same DBMS, or (2) a query written in
a certain ORM runs successfully on a speciÔ¨Åc DBMS, but the
same query written in another ORM fails on the same DBMS.
The second condition allows us to detect cases where an ORM
produced either a grammatically or semantically invalid SQL
query with regards to a certain DBMS.
Remark. To make safe comparisons between unordered
queries, our approach Ô¨Årst sorts the outputs of these queries,
and then compares them.
E. Implementation Details
We have implemented our data-oriented testing approach as
a Scala command-line tool called CYNTHIA .1The interface
ofCYNTHIA takes as input the names of the ORMs to test
along with a set of DBMS on which CYNTHIA runs the ORM
code. The tool implements the steps described in Figure 4.
For efÔ¨Åciency, CYNTHIA processes testing sessions and ORM
queries in parallel using Scala futures [24]. Optionally, CYN-
THIA may also receive a random seed (i.e., a number) from
the user to make the testing procedure deterministic.
CYNTHIA also provides a replay mode, which is used to
replay a testing session (i.e., repeat the execution of existing
AQL queries) for either debugging purposes or experiment-
ing with different settings (e.g., running existing queries on
different DBMSs). Finally, to generate database records, our
tool uses the Z3 theorem prover [25], conÔ¨Ågured with a user-
speciÔ¨Åed timeout.
Regarding the implementation effort of ORM translators,
each translator consists of roughly 300‚Äì400 lines of Scala
code. Every translator traverses the AST of AQL queries and
emits code that uses the API of the corresponding ORM.
Adding a new translator is guided by extending and imple-
menting an abstract Scala class.
1In Greek mythology, Cynthia was the epithet of Artemis, the goddess of
the hunt.
1541TABLE I: The ORM systems examined in our evaluation.
ORM Language LoC(k) Stars(k) Used By(k)
ActiveRecord (Rails) Ruby 49.2 46.2 1400
Django Python 37.7 51.3 466
Sequelize JavaScript 25.3 22.6 211
SQLAlchemy Python 150 2.6 182
peewee Python 7.6 7.7 10
IV. E VALUATION
We seek answers to the following research questions:
RQ1 IsCYNTHIA effective in Ô¨Ånding new bugs in established
ORM systems? (Section IV-B)
RQ2 What are the characteristics of the bugs discovered by
CYNTHIA ? (Section IV-C)
RQ3 Is solver-based approach effective in generating appro-
priate data for differential testing? (Section IV-D)
A. Experimental Setup
Target ORM systems. We applied CYNTHIA to the Ô¨Åve
ORM systems listed in Table I. We selected these ORMs based
on the following criteria:
Usage : the ORM should be established and widely-used.
High-level Logic : the ORM should expose a high-level API
that abstracts SQL-speciÔ¨Åc details.
Automation : the ORM must provide tools for easy setup and
utilities for generating model classes (recall Section III-A).
According to the Github‚Äôs statistics, all ORMs incorporated
in our evaluation are used by millions of applications. For
example, ActiveRecord, which is part of the Rails web frame-
work, is employed by more than 1400k Github repositories.
Further, many popular applications and services rely on them.
For example, ‚ÄúNova‚Äù, OpenStack‚Äôs cloud computing service,
uses SQLAlchemy for interacting with the database. Finally,
exposing high-level APIs from programmers can be prone to
bugs / errors [26]. Notably, Django, which provides the most
expressive API has the most bugs as we will see later.
DBMS. We ran the ORM queries on four DBMSs: SQLite,
MySQL, PostgreSQL, and Microsoft‚Äôs SQL Server (MSSQL).
The Ô¨Årst three DBMSs are extensively used by the open-
source community and are supported by all the examined
ORMs. Although MSSQL is supported by a subset of ORMs
(i.e., Django, SQLAlchemy, Sequelize), we selected it because
is one of the most popular proprietary DBMSs.
Cynthia ConÔ¨Åguration. We ran CYNTHIA on a regular
basis, and tested the ‚Äúmaster‚Äù version of the selected ORMs.
In each run, CYNTHIA generated Ô¨Åve random schemas. After
setting up the databases, CYNTHIA spawned a testing ses-
sion, and processed each testing session separately until a
speciÔ¨Åc timeout was reached (eight hours). For every query,
Z3 produced 5 records, while we set the solver timeout to 5
seconds. After each run, we manually inspected the reported
mismatches for new bugs, and report them to the developers.
B. RQ1: New Bugs Found
CYNTHIA found 28bugs in total, out of which, 20were
Ô¨Åxed by the developers, 5were conÔ¨Årmed but are not yet Ô¨Åxed,
3are still unconÔ¨Årmed, while one conÔ¨Årmed bug in DjangoTABLE II: Bugs detected by CYNTHIA .
ORM Total Fixed ConÔ¨Årmed UnconÔ¨Årmed
Django 10 6 3 1
SQLAlchemy 8 8 0 0
Sequelize 5 2 1 2
peewee 4 4 0 0
ActiveRecord 1 0 1 0
Total 28 20 5 3
TABLE III: The types of the detected bugs and the DBMSs
where the bugs manifest themselves.
Type #Bugs AllDBMS SQLite MySQL PostgreSQL MSSQL
Logic Error 12 11 0 0 0 1
Invalid SQL 11 3 1 3 2 3
Crash 5 3 0 0 2 0
Total 28 17 1 3 4 4
was previously known and marked as duplicate. Table II sum-
marizes the bug detection results. Django is the system where
we detected the most bugs ( 10), followed by SQLAlchemy
(8), Sequelize ( 5), peewee ( 4), and Ô¨Ånally ActiveRecord ( 1).
71% (20/28) of the reported bugs have already been Ô¨Åxed
by the developers demonstrating the correctness and impor-
tance of the reported issues. We were particularly impressed
by the prompt Ô¨Åxes of SQLAlchemy and peewee developers:
they Ô¨Åxed most of the bugs within six hours after our bug
report. Furthermore, three Django bugs were marked as release
blockers by the corresponding developers.
C. RQ2: Characteristics of Discovered Bugs
We classify the detected bugs into three categories. The
Ô¨Årst category ( logic errors ) contains cases where an ORM
produced a grammatically and semantically valid SQL query,
but this query did not fetch the right data from the database.
The second category ( invalid SQL ) contains cases where an
ORM yielded either a grammatically or semantically invalid
SQL query. The third category ( crashes ) contains cases where
an ORM crashed unexpectedly, without even producing an
SQL query. Most of the discovered bugs ( 12) were logic
ones (Table III). Unlike differential testing, a naive fuzzing
technique is unable to identify such bugs. In a signiÔ¨Åcant
number of cases ( 11), the ORM generated an invalid SQL
query, while the remaining cases ( 5) are related to crashes.
Table III also presents how many bugs are DBMS-
dependent. Almost all logic errors ( 11/12) are DBMS-
independent, i.e., they appear regardless of the underlying
DBMS. By constrast, the majority of ‚ÄúInvalid SQL‚Äù bugs
are DBMS-dependent. For example, two instances of ‚ÄúIn-
valid SQL‚Äù bugs happen when the code operates on Post-
greSQL. Overall, 17/28of the reported bugs are DBMS-
independent. Yet, there is a large number of DBMS-dependent
bugs ( 11/28). This validates our intuition to test ORMs across
multiple database engines.
Based on the feature that ORMs fail to handle correctly, we
further classify the discovered bugs into six categories.
Expression-related bugs. Expression-related bugs are the
most common ones (7/28). This category involves cases where
ORMs fail to produce an SQL expression that respects the
15421Comment.new(:rating => 4)
2Comment.new(:rating => 4)
3# It incorrectly applies AVG
to duplicate records.
4Comment.select( "comments.
rating" ).distinct.average
("comments.rating" )
(a) A bug in ActiveRecord associated with
DISTINCT .1// WHERE Comment.text LIKE
‚Äô%_%‚Äô
2Comment.findAll({
3where: {
4 text: {[Op.substring]: "_"}
5})
6})
(b) A buggy Sequelize query associated
with incorrect string comparison.1cons = ExpressionWrapper(
2 Value(3),...)
3# GROUP BY Comment.text, 3
4Comment.objects\
5 .annotate(cons=cons)\
6 .values( "cons" ,"text" )\
7 .annotate( sum=Sum( "rating" ))
(c) A buggy Django query associated with
GROUP BY .
Fig. 9: A collection of bugs discovered by CYNTHIA .
original ORM query. As an example of this category, consider
the peewee bug (Figure 3) discussed in Section III-D. In this
bug, peewee produces an SQL expression (i.e., 1+col1+col)
that is notequivalent with the high-level peewee expression
written by the programmer (see Figure 3, lines 1, 2).
Distinct-related bugs. DISTINCT is a keyword in SQL
that when present, it removes all duplicate records from the
result set. ORM systems expose this functionality through a
simple method call (typically called distinct() ). Although
the use of this feature looks simple, we detected six bugs
related to this functionality. Figure 9a shows a buggy query
in ActiveRecord associated with DISTINCT . The intended
functionality of this query is to fetch all the records of the table
‚ÄúComments‚Äù, remove the duplicates, and then apply AVG to a
column named ‚Äúrating‚Äù. However, ActiveRecord produces an
SQL query that ignores the call of distinct , and therefore,
it applies AVG to the entire set of records.
Combined-query-related bugs. SQL supports the com-
bination of individual queries using the UNION and
INTERSECT keywords. ORM systems support this feature
by implementing the union andintersect methods. Five
bugs discovered by CYNTHIA are associated with this func-
tionality of ORMs. An example of this category of bugs has
been already discussed in Section III-D (Recall Figure 2). In
particular, Django is unable to produce a valid sequence of
UNION operations when using MySQL as the database engine.
String-comparison-related bugs. String comparisons in
SQL are typically done via the LIKE operator. These op-
erators expect a pattern which SQL matches the value of
a string against. There are two characters (namely ‚Äò %‚Äô and
‚Äò_‚Äô) that have special semantics when used as part of a
LIKE pattern. For example, ‚Äò %‚Äô is a wildcard character that
matches any sequence of characters. ORMs typically abstract
LIKE with high-level methods, such as contains() . ORMs
must escape the aforementioned characters when passed as
an argument to these methods. We found four cases where
ORMs fail to escape these characters leading to wrong string
comparisons in the SQL part.
Consider Figure 9b that presents a bug in Sequelize. The
Sequelize query shown in this Ô¨Ågure attempts to fetch the
records of ‚ÄúComments‚Äù where the column ‚Äútext‚Äù contains the
character "_". Sequelize produces the SQL condition shown
on line 1. Although the character "_" has a special meaning
(it matches every single character), Sequelize does not escape
it. As a result, the generated SQL query incorrectly retrievesall the records of the table.
Aliasing-related bugs. SQL allows column aliasing through
theASconstruct. ORM systems also support aliasing. CYN-
THIA uncovered four bugs where the corresponding ORMs
either do not construct the alias correctly, or do not make a
reference to a legal alias.
As an example of an aliasing-related bug,
consider the SQLAlchemy query: session.query(
Model.column.label("exists")) . When running
this query on SQLite, SQLAlchemy generates the following
SQL code: SELECT "model"."column" AS exists
FROM model . Unfortunately, this SQL query is invalid
because ‚Äú exists ‚Äù is a reserved keyword in SQLite.
As a result, the execution of this query throws an
‚Äúsqlite3.OperationalError: near ‚Äùexists‚Äù: syntax error‚Äù
message. To Ô¨Åx this bug, the developers of SQLAlchemy
wrapped the reserved word with quotes (i.e., AS "exists" ).
Group-by-related bugs. TheGROUP BY clause is used
when selecting or referencing a table‚Äôs column together with
aggregated data. GROUP BY comes with some caveats that
ORMs need to consider in order to properly handle this SQL
feature. We ran into three bugs caused by incorrect handling
of the GROUP BY functionality.
Consider the Django query shown in Figure 9c. Django
builds three expressions: the constant 3 (lines 1,5), a reference
to the column ‚Äútext‚Äù, and an aggregate function SUM applied
to the column ‚Äúrating‚Äù. Django places all non-aggregate ex-
pressions on GROUP BY as shown on line 3. Integer constants
have special semantics when they are part of GROUP BY . For
example, GROUP BY 3 means to group by the third expression
of the SELECT clause of the query (i.e., SUM("rating") ).
This makes the generated SQL query invalid, leading to
‚ÄúProgrammingError: aggregate functions are not allowed in
GROUP BY‚Äù . The developers Ô¨Åxed this by ignoring constant
expressions from the set of grouping Ô¨Åelds.
D. RQ3: Effectiveness of Solver-Based Data Generation
For effectively identifying mismatches between the out-
puts of ORMs, it is important that ORMs return non-empty
results for the given queries. Empty results indicate that
the corresponding query was unsatisÔ¨Åed with respect to the
data inserted to the database. Empty results can potentially
hide logic errors that otherwise would be uncovered if the
corresponding ORMs could get some data from the database
and we were able to notice differences in their results.
1543Solver-basedNaive (50 recs) Naive (100 recs) Naive (300 recs) Naive (500 recs) Naive (1000 recs)
Data Generation Strategy01020304050Unsatisfied Queries (%)Fig. 10: Percentage of the unsatisÔ¨Åed queries per data gener-
ation strategy using a sample of 20 testing sessions.
To demonstrate the effectiveness of our solver-based data
generation approach and its suitability for differential testing,
we compare it against a simplistic approach that populates
the database with random records a-priori [21], [27], i.e., it
inserts data while setting up the tables, without considering
the constraints of the generated queries.
We used CYNTHIA to spawn 20testing sessions. For each
testing session, we generated 100 queries and compared the
results of ORMs as usual. At the end of each testing session,
we measured in how many queries the ORMs returned empty
results. We then replayed each testing session, using a naive
data generation strategy, and tried out different settings: gen-
erating 50random records, 100,300,500and Ô¨Ånally 1000 .
Figure 10 illustrates the comparison results. The y-axis
shows the percentage of the unsatisÔ¨Åed queries. Every box plot
contains the observations taken from the 20testing sessions,
along with the median (horizontal line), the mean (black
triangle), and the maximum and minimum values. The solver-
based approach leads to signiÔ¨Åcantly fewer unsatisÔ¨Åed queries
(median: 7:5%, mean: 8:9%) than the naive approach (mean
and median values are roughly 38% for all the different
settings). The reason why there is still a number of unsatisÔ¨Åed
queries even with the solver-based approach is because either
the corresponding AQL query was unsatisÔ¨Åable or the solver
timed out. Regarding the naive data generation, increasing
the number of the records inserted to the database does not
improve the effectiveness of this method at all, i.e., generating
50records is almost identical to generating 1000 records.
We also tried to reproduce the discovered bugs using the
naive data generation strategy. This strategy missed 3out of
the12logic errors previously detected by CYNTHIA , because
it failed to generate appropriate data for the database. In these
cases, the differential testing was meaningless, as the ORMs
returned empty results. We did not consider the rest categories
(e.g., invalid SQL), as in these cases the corresponding ORMs
produce an error message regardless of the data stored in the
database. Overall, our Ô¨Åndings suggest that it is the quality of
the inserted data that matters, and not the quantity: it is better
to produce 5targeted records than 1000 random records.
E. Discussion & Threats to Validity
Regression Bugs. Running CYNTHIA on the master versionof ORMs enabled us to Ô¨Ånd a couple of interesting regression
bugs. Regression bugs indicate that a feature that worked
properly in previous versions, is broken in the current im-
plementation. These bugs were of paramount importance for
the developers. For example, Django developers marked our
regression bugs as release blockers. Also, SQLAlchemy de-
velopers commented: ‚Äúit‚Äôs very useful if you are in fact alpha
testing it. ‚Äù (i.e., master branch). We also noticed that some
bugs that were allegedly Ô¨Åxed were triggered again by new
queries. This observation was conÔ¨Årmed by the developers,
who, indeed reopened and Ô¨Åxed old bugs reported by us.
ORMs. Although it is the de-facto framework for Python,
the Django ORM is the system where our approach detected
the most bugs. One may wonder why we detected so many
bugs in Django, while we uncovered only one bug in Ac-
tiveRecord. The reason is that Django is a more high-level
ORM than ActiveRecord: it hides every single SQL-detail
via its API. On the other hand, ActiveRecord‚Äôs API provides
some functionalities that are closer to SQL. For example,
ActiveRecord supports arithmetic operations and aliasing by
writing plain SQL. Thus, ActiveRecord does not employ any
sophisticated translation mechanism and in many cases, the
input of the programmer is passed directly to the SQL code.
DBMSs. CYNTHIA identiÔ¨Åed four PostgreSQL- and
MSSQL-related bugs. These ORM bugs are triggered only
when the DBMS is switched to PostgreSQL or MSSQL. On
the other hand, only one ORM bug is related to SQLite. This
happens because PostgreSQL and MSSQL are much stricter
than SQLite (and even MySQL). For example, unlike MySQL
and SQLite, PostgreSQL has a strict type system, and comes
with many restrictions that ORMs need to take into account
when producing SQL code. Also, we note that during our
testing efforts, we discovered one bug in SQLite. The bug was
already known and Ô¨Åxed in a later version of SQLite though.
This implies that with some tuning, our approach may be also
useful for testing DBMSs.
Threats to Validity. A threat to the internal validity of our
approach, involves correctness bugs in the implementation of
our translators. In this case, a mismatch in ORM results may
be caused due to a bug in our translators and not in ORMs
themselves. To mitigate this threat, before reporting a bug to
the developers, the Ô¨Årst two authors carefully examined each
mismatch to verify that it was not generated by an error in the
translators.
A threat to external validity is related to the representa-
tiveness of the examined ORMs. All the selected ORMs are
popular and used by millions of applications. There is no
fundamental limitation on supporting other ORM implemen-
tations. As our prototype gains developer traction, we will
implement more translators (e.g., for JPA implementations).
Finally, a threat to construct validity concerns the general-
izability of our approach. Our approach targets to Ô¨Ånd bugs
associated with the translation of ORM API calls into SQL
queries. An ORM though, may suffer from other kinds of
bugs, such as performance issues, transaction management,
or conÔ¨Åguration of model classes.
1544V. R ELATED WORK
Quality in ORM-based Applications. A number of tools
and studies have been proposed to improve the quality of
ORM-based applications. Chen et al. [2] introduced a static
analysis framework for identifying ORM queries in Java ap-
plications that degrade the response times of database engines.
Their approach Ô¨Årst explores the paths of the program to
identify database accesses, and then detects performance anti-
patterns through a rule-based approach. Furthermore, their
technique provides an assessment mechanism for prioritizing
the Ô¨Åxes of the detected performance issues. Subsequent
work [28], [29] focused on Ô¨Åxing performance issues through
automated means. In particular, Singh et al. [29] introduced
a genetic algorithm for tuning the conÔ¨Åguration of ORM
systems to achieve better performance. Davar et al. [28]
proposed a refactoring framework by applying a set of known
transformation rules to inefÔ¨Åcient ORM-based code. Unlike
prior work that Ô¨Ånds issues in the ORM-based applications, our
work is the Ô¨Årst to Ô¨Ånd issues in the ORM implementations.
Testing of DBMSs. The work of Slutz [30] is the Ô¨Årst to un-
cover bugs in DBMSs using a differential testing approach. To
safely compare results, his method generates random queries
on a small subset of the SQL language that is common across
DBMSs. Over the past decade, there have been numerous
approaches for generating (targeted) SQL queries in order to
effectively test DBMSs [31], [32], [33], [34]. The most recent
approaches are SQLsmith [35] and SQLfuzz [36], two SQL
query generators that respectively target crashes and regression
bugs in popular DBMSs. Our approach differs from all these
query generators because it produces queries in a higher-level
query language (AQL) and adopts differential testing to detect
logic errors beyond crashes or regression bugs. Khalek et
al. [37], [34] followed a solver-based approach for testing
DBMSs. Their work employs a relational constraint solver to
generate valid database records with respect to a given SQL
query and database schema. Besides populating the database,
their method also determines the expected results of an SQL
query and the authors use this oracle to Ô¨Ånd bugs. We also
use an SMT-solver to populate the database, but we specify
the test oracle by adopting a differential testing approach.
More recently, Rigger et al. [21] proposed the Pivoted Query
Synthesis (PQS) technique for testing database engines. PQS
generates SQL queries so that they fetch a speciÔ¨Åc record from
the database. In this way, PQS forms the test oracle: failing
to fetch the expected record reveals a potential bug in DBMS.
Unlike this work, our approach adopts differential testing for
determining the oracle. Also, beyond reasoning about a single
record, our approach is able to detect bugs involving operations
on result sets (e.g., aggregate functions, sorting, distinct). In
an attempt to Ô¨Ånd optimization bugs in database systems,
their subsequent work introduced a metamorphic testing tech-
nique called Non-Optimizing Reference Engine Construction
(NoREC) [27]. At a high-level, NoREC applies a semantics-
preserving transformation to a given SQL query in way that the
various optimizations performed by the DBMS are disabled.Finally, NoREC compares the results of the original and the
resulting queries for mismatches. In their most recent work,
they propose Ternary Logic Partitioning (TLP) [38]. Given
an SQL query, TLP derives multiple queries that compute
a partial result of the initial query, and then combines the
results of each individual query using a UNION operation. If
the result of the combined query does not match that of the
initial one, then a bug is found. TLP is suitable for testing the
implementation of the WHERE ,HAVING ,DISTINCT clauses,
or aggregate functions.
All these previous approaches are tailored to testing
DBMSs, i.e., they aim to Ô¨Ånd DBMS-speciÔ¨Åc bugs (e.g., opti-
mization bugs, bugs associated with the evaluation of WHERE
clauses). ORM systems differ from database engines, and
suffer from other types of bugs.
Differential Testing. Differential testing [22], [39] is a
generally-applicable testing technique that aims to Ô¨Ånd bugs
in software implementations by addressing the oracle prob-
lem [40]. Differential testing has been successfully applied
to various domains, most notably compilers and runtime
systems [16], [19], [17], [18], [20]. Following this success,
differential testing has been applied to many other domains,
from program analyzers, such as model checkers [41], debug-
gers [42], and symbolic execution engines [43], to probabilistic
programming languages [44], and software libraries and ser-
vices [45], [46], [47], [48]. Inspired by this work, we also
employ differential testing for Ô¨Ånding bugs in ORM systems.
VI. C ONCLUSION
A fundamental requirement for differential testing is that the
implementations under test must be equivalent. By introducing
an appropriate layer of abstraction that hides the implemen-
tation differences (AQL), we showed that differential testing
can be also applicable in systems with (seemingly) dissimilar
interfaces, such as ORMs.
Further, we addressed an ORM-speciÔ¨Åc challenge: the gen-
eration of data that are likely to produce non-trivial results in
response to given queries. To do so, we employed an SMT
solver to synthesize targeted records, dependant on the con-
straints of the generated inputs. Our Ô¨Åndings showed that when
compared to other simplistic data generation strategies, the
solver-based approach enhances the bug detection capability.
We demonstrated the importance and practicality of our
approach by systematically testing Ô¨Åve popular open-source
ORM systems. We discovered 28bugs, most of which have
been Ô¨Åxed by the developers. The effectiveness of our method
can be further improved by considering other forms of queries
and functionalities, such as insert or update operations, and
transaction management.
ACKNOWLEDGMENTS
We thank the anonymous reviewers for their constructive
feedback. We also thank the developers of the examined ORM
systems for addressing our bug reports. This work has received
funding from the European Union‚Äôs Horizon 2020 research and
innovation programme under grant agreement No. 825328.
1545REFERENCES
[1] A. Torres, R. Galante, M. S. Pimenta, and A. J. B. Martins,
‚ÄúTwenty years of object-relational mapping: A survey on patterns,
solutions, and their implications on application design,‚Äù Information
and Software Technology , vol. 82, pp. 1 ‚Äì 18, 2017. [Online]. Available:
http://www.sciencedirect.com/science/article/pii/S0950584916301859
[2] T.-H. Chen, W. Shang, Z. M. Jiang, A. E. Hassan, M. Nasser,
and P. Flora, ‚ÄúDetecting performance anti-patterns for applications
developed using object-relational mapping,‚Äù in Proceedings of the 36th
International Conference on Software Engineering , ser. ICSE 2014.
New York, NY , USA: Association for Computing Machinery, 2014,
p. 1001‚Äì1012. [Online]. Available: https://doi.org/10.1145/2568225.
2568259
[3] K. Roebuck, Object-Relational Mapping (ORM): High-Impact
Strategies-What You Need to Know: DeÔ¨Ånitions, Adoptions, Impact,
BeneÔ¨Åts, Maturity, Vendors . Emereo Publishing, 2012.
[4] C. Bauer, G. King, and G. Gregory, Java Persistence with Hibernate ,
2nd ed. USA: Manning Publications Co., 2015.
[5] D. Maier, Representing Database Programs as Objects . New York,
NY , USA: Association for Computing Machinery, 1990, p. 377‚Äì386.
[Online]. Available: https://doi.org/10.1145/101620.101642
[6] M. EltsuÔ¨Ån, ‚ÄúBringing Hibernate ORM to cloud Spanner for
database adoption,‚Äù https://cloud.google.com/blog/products/databases/
bringing-hibernate-orm-cloud-spanner-database-adoption, 2019.
[7] T. Chen, W. Shang, J. Yang, A. E. Hassan, M. W. Godfrey, M. Nasser,
and P. Flora, ‚ÄúAn empirical study on the practice of maintaining Object-
Relational Mapping code in Java systems,‚Äù in 2016 IEEE/ACM 13th
Working Conference on Mining Software Repositories (MSR) , 2016, pp.
165‚Äì176.
[8] M. Bayer, ‚ÄúSQLAlchemy - the database toolkit for Python,‚Äù https://www.
sqlalchemy.org/, 2020, [Online; accessed 29-July-2020].
[9] D. S. Foundation, ‚ÄúThe web framework for perfectionists with dead-
lines,‚Äù https://www.djangoproject.com/, 2020, [Online; accessed 29-July-
2020].
[10] J. Yang, P. Subramaniam, S. Lu, C. Yan, and A. Cheung, ‚ÄúHow
¬°i¬ønot¬°/i¬ø to structure your database-backed web applications: A study
of performance bugs in the wild,‚Äù in Proceedings of the 40th
International Conference on Software Engineering , ser. ICSE ‚Äô18.
New York, NY , USA: Association for Computing Machinery, 2018, p.
800‚Äì810. [Online]. Available: https://doi.org/10.1145/3180155.3180194
[11] D. S. Foundation, ‚ÄúDjango issues,‚Äù https://code.djangoproject.com/
query, 2020, [Online; accessed 29-July-2020].
[12] A. Viswa, ‚Äúselect forupdate() with ‚Äùof‚Äù uses wrong tables from (multi-
level) model inheritance,‚Äù https://code.djangoproject.com/ticket/31246,
2020, [Online; accessed 29-July-2020].
[13] S. Bank, ‚Äúnegated EXISTS result type not bool with SQLite dialect,‚Äù
https://github.com/sqlalchemy/sqlalchemy/issues/3682, 2016, [Online;
accessed 29-July-2020].
[14] ‚ÄúCVE-2019-7164,‚Äù https://nvd.nist.gov/vuln/detail/CVE-2019-7164,
2019, [Online; accessed 29-July-2020].
[15] ‚ÄúCVE-2020-9402,‚Äù https://nvd.nist.gov/vuln/detail/CVE-2020-9402,
2020, [Online; accessed 29-July-2020].
[16] X. Yang, Y . Chen, E. Eide, and J. Regehr, ‚ÄúFinding and understanding
bugs in C compilers,‚Äù in Proceedings of the 32nd ACM SIGPLAN
Conference on Programming Language Design and Implementation ,
ser. PLDI ‚Äô11. New York, NY , USA: Association for Computing
Machinery, 2011, p. 283‚Äì294. [Online]. Available: https://doi.org/10.
1145/1993498.1993532
[17] Y . Chen, T. Su, C. Sun, Z. Su, and J. Zhao, ‚ÄúCoverage-
directed differential testing of JVM implementations,‚Äù SIGPLAN
Not., vol. 51, no. 6, p. 85‚Äì99, Jun. 2016. [Online]. Available:
https://doi.org/10.1145/2980983.2908095
[18] Y . Chen, T. Su, and Z. Su, ‚ÄúDeep differential testing of JVM
implementations,‚Äù in Proceedings of the 41st International Conference
on Software Engineering , ser. ICSE ‚Äô19. IEEE Press, 2019,
p. 1257‚Äì1268. [Online]. Available: https://doi.org/10.1109/ICSE.2019.
00127
[19] C. Lidbury, A. Lascu, N. Chong, and A. F. Donaldson, ‚ÄúMany-core
compiler fuzzing,‚Äù SIGPLAN Not. , vol. 50, no. 6, p. 65‚Äì76, Jun. 2015.
[Online]. Available: https://doi.org/10.1145/2813885.2737986
[20] C. Sun, V . Le, and Z. Su, ‚ÄúFinding and analyzing compiler
warning defects,‚Äù in Proceedings of the 38th International Conference
on Software Engineering , ser. ICSE ‚Äô16. New York, NY , USA:Association for Computing Machinery, 2016, p. 203‚Äì213. [Online].
Available: https://doi.org/10.1145/2884781.2884879
[21] M. Rigger and Z. Su, ‚ÄúTesting database engines via pivoted query
synthesis,‚Äù in 14th USENIX Symposium on Operating Systems Design
and Implementation (OSDI 20) . USENIX Association, Nov. 2020,
pp. 667‚Äì682. [Online]. Available: https://www.usenix.org/conference/
osdi20/presentation/rigger
[22] W. M. McKeeman, ‚ÄúDifferential testing for software,‚Äù Digital Technical
Journal , vol. 10, no. 1, pp. 100‚Äì107, 1998.
[23] D. S. Foundation, ‚ÄúIntegrating Django with a legacy database,‚Äù https://
docs.djangoproject.com/en/3.0/howto/legacy-databases/, 2020, [Online;
accessed 29-July-2020].
[24] P. Haller, A. Prokopec, H. Miller, V . Klang, R. Kuhn, and V . Jo-
vanovic, ‚ÄúFutures and promises,‚Äù https://docs.scala-lang.org/overviews/
core/futures.html, 2020.
[25] L. de Moura and N. Bj√∏rner, ‚ÄúZ3: An efÔ¨Åcient SMT solver,‚Äù in Tools
and Algorithms for the Construction and Analysis of Systems , C. R.
Ramakrishnan and J. Rehof, Eds. Berlin, Heidelberg: Springer Berlin
Heidelberg, 2008, pp. 337‚Äì340.
[26] V . Atlidakis, J. Andrus, R. Geambasu, D. Mitropoulos, and J. Nieh,
‚ÄúPOSIX abstractions in modern operating systems: The old, the new, and
the missing,‚Äù in Proceedings of the Eleventh European Conference on
Computer Systems , ser. EuroSys ‚Äô16. New York, NY , USA: Association
for Computing Machinery, 2016.
[27] M. Rigger and Z. Su, ‚ÄúDetecting optimization bugs in database engines
via non-optimizing reference engine construction,‚Äù in Proceedings of the
28th ACM Joint Meeting on European Software Engineering Conference
and Symposium on the Foundations of Software Engineering . New
York, NY , USA: Association for Computing Machinery, 2020,
p. 1140‚Äì1152. [Online]. Available: https://doi.org/10.1145/3368089.
3409710
[28] Z. Davar and Handoko, ‚ÄúRefactoring object-relational database
applications by applying transformation rules to develop better
performance,‚Äù in Proceedings of the 16th International Conference
on Information Integration and Web-Based Applications & Services ,
ser. iiWAS ‚Äô14. New York, NY , USA: Association for Computing
Machinery, 2014, p. 283‚Äì288. [Online]. Available: https://doi.org/10.
1145/2684200.2684304
[29] R. Singh, C.-P. Bezemer, W. Shang, and A. E. Hassan, ‚ÄúOptimizing
the performance-related conÔ¨Ågurations of object-relational mapping
frameworks using a multi-objective genetic algorithm,‚Äù in Proceedings
of the 7th ACM/SPEC on International Conference on Performance
Engineering , ser. ICPE ‚Äô16. New York, NY , USA: Association
for Computing Machinery, 2016, p. 309‚Äì320. [Online]. Available:
https://doi.org/10.1145/2851553.2851576
[30] D. R. Slutz, ‚ÄúMassive stochastic testing of SQL,‚Äù in Proceedings of the
24rd International Conference on Very Large Data Bases , ser. VLDB
‚Äô98. San Francisco, CA, USA: Morgan Kaufmann Publishers Inc.,
1998, p. 618‚Äì622.
[31] N. Bruno, S. Chaudhuri, and D. Thomas, ‚ÄúGenerating queries with cardi-
nality constraints for DBMS testing,‚Äù IEEE Transactions on Knowledge
and Data Engineering , vol. 18, no. 12, pp. 1721‚Äì1725, 2006.
[32] H. Bati, L. Giakoumakis, S. Herbert, and A. Surna, ‚ÄúA genetic approach
for random testing of database systems,‚Äù in Proceedings of the 33rd
International Conference on Very Large Data Bases , ser. VLDB ‚Äô07.
VLDB Endowment, 2007, p. 1243‚Äì1251.
[33] C. Mishra, N. Koudas, and C. Zuzarte, ‚ÄúGenerating targeted queries
for database testing,‚Äù in Proceedings of the 2008 ACM SIGMOD
International Conference on Management of Data , ser. SIGMOD ‚Äô08.
New York, NY , USA: Association for Computing Machinery, 2008, p.
499‚Äì510. [Online]. Available: https://doi.org/10.1145/1376616.1376668
[34] S. Abdul Khalek and S. Khurshid, ‚ÄúAutomated SQL query generation
for systematic testing of database engines,‚Äù in Proceedings of
the IEEE/ACM International Conference on Automated Software
Engineering , ser. ASE ‚Äô10. New York, NY , USA: Association
for Computing Machinery, 2010, p. 329‚Äì332. [Online]. Available:
https://doi.org/10.1145/1858996.1859063
[35] A. Seltenreich, ‚ÄúSQLsmith: A random SQL query generator,‚Äù https://
github.com/anse1/sqlsmith, 2020, [Online; accessed 29-July-2020].
[36] J. Jung, H. Hu, J. Arulraj, T. Kim, and W. Kang, ‚ÄúAPOLLO:
Automatic detection and diagnosis of performance regressions in
database systems,‚Äù Proc. VLDB Endow. , vol. 13, no. 1, p. 57‚Äì70, Sep.
2019. [Online]. Available: https://doi.org/10.14778/3357377.3357382
1546[37] S. Abdul Khalek, B. Elkarablieh, Y . O. Laleye, and S. Khurshid,
‚ÄúQuery-aware test generation using a relational constraint solver,‚Äù in
2008 23rd IEEE/ACM International Conference on Automated Software
Engineering , 2008, pp. 238‚Äì247.
[38] M. Rigger and Z. Su, ‚ÄúFinding bugs in database systems via query
partitioning,‚Äù Proc. ACM Program. Lang. , vol. 4, no. OOPSLA, Nov.
2020. [Online]. Available: https://doi.org/10.1145/3428279
[39] T. Petsios, A. Tang, S. Stolfo, A. D. Keromytis, and S. Jana, ‚ÄúNEZHA:
EfÔ¨Åcient domain-independent differential testing,‚Äù in 2017 IEEE Sym-
posium on Security and Privacy (SP) , 2017, pp. 615‚Äì632.
[40] E. J. Weyuker, ‚ÄúOn testing non-testable programs,‚Äù The Computer
Journal , vol. 25, no. 4, pp. 465‚Äì470, 1982.
[41] C. Klinger, M. Christakis, and V . W ¬®ustholz, ‚ÄúDifferentially testing
soundness and precision of program analyzers,‚Äù in Proceedings of the
28th ACM SIGSOFT International Symposium on Software Testing
and Analysis , ser. ISSTA 2019. New York, NY , USA: Association
for Computing Machinery, 2019, p. 239‚Äì250. [Online]. Available:
https://doi.org/10.1145/3293882.3330553
[42] D. Lehmann and M. Pradel, ‚ÄúFeedback-directed differential testing of
interactive debuggers,‚Äù in Proceedings of the 2018 26th ACM Joint
Meeting on European Software Engineering Conference and Symposium
on the Foundations of Software Engineering , ser. ESEC/FSE 2018.
New York, NY , USA: Association for Computing Machinery, 2018, p.
610‚Äì620. [Online]. Available: https://doi.org/10.1145/3236024.3236037
[43] T. Kapus and C. Cadar, ‚ÄúAutomatic testing of symbolic execution
engines via program generation and differential testing,‚Äù in Proceedings
of the 32nd IEEE/ACM International Conference on Automated Software
Engineering , ser. ASE 2017. IEEE Press, 2017, p. 590‚Äì600.
[44] S. Dutta, O. Legunsen, Z. Huang, and S. Misailovic, ‚ÄúTesting
probabilistic programming systems,‚Äù in Proceedings of the 2018 26th
ACM Joint Meeting on European Software Engineering Conference
and Symposium on the Foundations of Software Engineering ,
ser. ESEC/FSE 2018. New York, NY , USA: Association for
Computing Machinery, 2018, p. 574‚Äì586. [Online]. Available: https:
//doi.org/10.1145/3236024.3236057
[45] S. Srisakaokul, Z. Wu, A. Astorga, O. Alebiosu, and T. Xie, ‚ÄúMultiple-
implementation testing of supervised learning software,‚Äù in Workshops
at the Thirty-Second AAAI Conference on ArtiÔ¨Åcial Intelligence , 2018.
[46] M. Selakovic, M. Pradel, R. Karim, and F. Tip, ‚ÄúTest generation
for higher-order functions in dynamic languages,‚Äù Proc. ACM
Program. Lang. , vol. 2, no. OOPSLA, Oct. 2018. [Online]. Available:
https://doi.org/10.1145/3276531
[47] Y . Chen and Z. Su, ‚ÄúGuided differential testing of certiÔ¨Åcate validation
in SSL/TLS implementations,‚Äù ser. ESEC/FSE 2015. New York,
NY , USA: Association for Computing Machinery, 2015, p. 793‚Äì804.
[Online]. Available: https://doi.org/10.1145/2786805.2786835
[48] P. Godefroid, D. Lehmann, and M. Polishchuk, ‚ÄúDifferential regression
testing for REST APIs,‚Äù in Proceedings of the 29th ACM SIGSOFT
International Symposium on Software Testing and Analysis , ser. ISSTA
2020. New York, NY , USA: Association for Computing Machinery,
2020, p. 312‚Äì323. [Online]. Available: https://doi.org/10.1145/3395363.
3397374
1547