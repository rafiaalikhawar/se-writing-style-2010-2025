Pipelining Bottom-up Data Flow Analysis
Qingkai Shi
The Hong Kong University of Science and Technology
Hong Kong, China
qshiaa@cse.ust.hkCharles Zhang
TheHong Kong University of Science and Technology
Hong Kong, China
charlesz@cse.ust.hk
ABSTRACT
Bottom-up program analysis has been traditionally easy to par-
allelize because functions without caller-callee relations can be
analyzed independently. However, such function-level parallelism
issignificantlylimitedbythecallingdependence-functionswith
caller-callee relations have to be analyzed sequentially becausethe analysis of a function depends on the analysis results, a.k.a.,
functionsummaries,ofitscallees.Weobservethatthecallingdepen-dence can be relaxed in many cases and, as a result, the parallelism
can be improved. In this paper, we present Coyote, a framework of
bottom-up data flow analysis, in which the analysis task of each
function is elaborately partitioned into multiple sub-tasks to gener-ate pipelineable function summaries. These sub-tasks are pipelined
and run in parallel, even though the calling dependence exists. We
formalize our idea under the IFDS/IDE framework and have im-plemented an application to checking null-dereference bugs andtaint issues in C/C++ programs. We evaluate Coyoteon a series
of standard benchmark programs and open-source software sys-
tems, which demonstrates significant speedup over a conventional
paralleldesign.
CCSCONCEPTS
•Software and its engineering →Software verification and
validation.
KEYWORDS
Compositionalprogramanalysis,modularprogramanalysis,bottom-
up analysis, data flow analysis, IFDS/IDE.
ACM Reference Format:
Qingkai Shi and Charles Zhang. 2020. Pipelining Bottom-up Data Flow
Analysis. In 42nd International Conference on Software Engineering (ICSE
’20),May23–29,2020,Seoul,RepublicofKorea. ACM,NewYork,NY,USA,
13pages.https://doi.org/10.1145/3377811.3380425
1 INTRODUCTION
Bottom-upanalysesworkbyprocessingthecallgraphofaprogramupwardsfromtheleaves–beforeanalyzingafunction,allitscallee
functionsareanalyzedandsummarizedasfunctionsummaries[
4,8,
9,11,15,16,54,63,64].Theseanalyseshavetwokeystrengths:the
Permissionto make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation
on the first page. Copyrights for components of this work owned by others than ACM
mustbehonored.Abstractingwithcreditispermitted.Tocopyotherwise,orrepublish,topostonserversortoredistributetolists,requirespriorspecificpermissionand/ora
fee. Request permissions from permissions@acm.org.
ICSE’20,May23–29,2020,Seoul, Republic of Korea
© 2020Association for Computing Machinery.
ACM ISBN 978-1-4503-7121-6/20/05...$15.00
https://doi.org/10.1145/3377811.3380425f
h
Call Graphtimefh
gg
Figure 1: Conventional parallel design of bottom-up pro-
gram analysis. Each rectangle represents the analysis taskfora function.
timef0h0
f1h1
f2h2g0 g1 g2
Figure 2: The analysis task of each function is partitionedinto multiplesub-tasks.All sub-tasksare pipelined.
functionsummariesthey computeare highly reusable and they are
easy to parallelize because the analyses of functions are decoupled.
While almost all existing bottom-up analyses take advantage of
suchfunction-levelparallelization,thereislittleprogressinimprov-
ing its parallelism. As reported by recent studies, it still needs to
takeafewhours,eventensofhours,topreciselyanalyzelarge-scale
software. For example, it takes 6 to 11 hours for Saturn[64] and
Calysto[4]toanalyzeprogramsof685KLoC[ 4].Ittakesabout5
hours for Pinpoint [54] to analyze about 8 million lines of code.
With regard to the performance issues, McPeak et al .[35]pointed
out that the parallelism often drops off at runtime and, thus, theCPU resources are usually not well utilized. Specifically, this is
becausetheparallelismissignificantlylimitedbythe callingdepen-
dence–functionswithcaller-calleerelationshavetobeanalyzed
sequentiallybecausetheanalysisofacallerfunctiondependsonthe
analysis results, i.e., function summaries, of its callee functions. To
illustratethisphenomenon,letusconsiderthecallgraphinFigure 1
where the function fcalls the functions, gandh. In a conventional
bottom-up analysis, only functions without caller-callee relations,e.g.,thefunction
gandthefunction h,canbeanalyzedin parallel.
The analysis of the function fcannot start until the analyses of the
functions, gandh, complete. Otherwise, when analyzing a call site
of the function, gorh, in the function f, we may miss some effects
of the callees due to the incomplete analysis.1
1Thisisdifferentfromatop-downmethodthatcanlettheanalysisofthefunction f
runfirstbutstoptowaitfortheanalysisresultsofthefunction gwhenanalyzinga
callstatementcallingthefunction g.
*&&&"$.OE*OUFSOBUJPOBM$POGFSFODFPO4PGUXBSF&OHJOFFSJOH	*$4&
ICSE’20,May23–29,2020,Seoul, Republic of Korea QingkaiShiandCharlesZhang
In this paper, we present Coyote, a framework of bottom-up
data flow analysis that breaks the limits of function boundaries,
so that functions having calling dependence can be analyzed in
parallel. As a result, we can achieve much higher parallelism than
the conventional parallel design of bottom-up analysis. Our key
insight is that many analysis tasks of a caller function only depend
on partial analysis results of its callee functions. Thus, the analysis
of the caller function can start before the analyses of its calleefunctions complete. Therefore, our basic idea is to partition the
analysistaskofafunctionintomultiplesub-tasks,sothatwecan
pipeline the sub-tasks to generate function summaries. The key to
the partition is a soundness criterion, which requires a sub-task
onlydependsonthesummariesproducedbythesub-tasksfinished
in the callees. Violating this criterion will cause the analysis to
neglect certain function effects and make the analysis unsound.
Toillustrate,assumethattheanalysistaskofeachfunctionin
Figure1, e.g., the function f, is partitioned into three sub-tasks,
f0,f1, andf2, each of which generates one kind of function sum-
maries. These sub-tasks satisfy the constraints that the sub-task
fionlydependsonthefunctionsummariesproducedbythesub-
taskgjand the sub-task hj(j≤i). As a result, these sub-tasks can
be pipelined as illustrated in Figure 2, where the analysis of the
function fstartsimmediatelyafterthesub-tasks g0andh0finish.
Clearly, the parallelism in Figure 2is much higher than that in
Figure1, providing a significant speedup over the conventional
paralleldesign of bottom-up analysis.
Inthispaper,weformalizeourideaundertheIFDS/IDEframe-
work for a wide range of data flow problems known as the inter-
proceduralfinitedistributivesubsetorinter-proceduraldistributive
environment problems [ 45,50]. In both problems, the data flow
functionsarerequiredtobedistributiveoverthemergeoperator.
Although this is a limitation in some cases, the IFDS/IDE frame-
work has been widely used for many practical problems such as
secureinformationflow[ 3,23,42],typestate[ 19,39],aliassets[ 40],
specification inference [ 55], and shape analysis [ 47,65]. Given any
of those IFDS/IDE problems, conventional solutions compute func-
tion summaries either in a bottom-up fashion (e.g., [ 49,67]) or in a
top-downmanner(e.g.,[ 45,50]),dependingontheirspecificdesign
goals.Inthispaper,wefocusonthebottom-upsolutionsandaimto
improve their performance via the pipeline parallelization strategy.
We implemented Coyoteto path-sensitively check null derefer-
encesandtaintissuesinC/C++programs.Ourevaluationof Coyote
is based on standard benchmark programs and many large-scale
software systems, which demonstrates that the calling dependence
significantly limits the parallelism of bottom-up data flow analy-sis. By relaxing this dependence, our pipeline strategy achieves2
×-3×speedup over the conventional parallel design of bottom-
up analysis. Such speedup is significant enough to make many
overlylengthyanalysesusefulinpractice.Insummary,themain
contributionsof this paper include the following:
•Weproposethedesignofpipelineablefunctionsummaries,
whichenablesthepipelineparallelizationstrategyforbottom-
up data flow analysis.
•Weformallyprovethecorrectnessofourapproachandapply
it to a null analysis and a taint analysis to show its general-
izability.id: S.S f: S.{a} g: S. if a S:  ( S {b}) 
else (S – {b})
0ab
0ab.
.. .
..0ab
0ab.
.. .
..0ab
0ab.
.. .
..
Figure3:Dataflowfunctionsandtheirrepresentationinthe
exploded super-graph [45].
•We conduct a systematic evaluation to demonstrate that our
approach can achieve much higher parallelism and, thus,
runs faster thanthe state of the arts.
2 BACKGROUND AND OVERVIEW
Inthissection,weintroducethebackgroundoftheIFDS/IDEframe-
work (Section 2.1) and provide an example to illustrate how we
improvetheparallelismofabottom-upanalysisbypartitioningthe
analysisof a function (Section 2.2).
2.1 TheIFDS/IDEFramework
TheIFDS/IDEframeworkaimstosolveawiderangeofdataflow
problemsknownasInter-proceduralFiniteDistributiveSubsetor
Inter-procedural Distributive Environment problems [ 45,50]. Its
basicideaistotransformadataflowproblemtoagraphreachability
problemonthe explodedsuper-graph,whichisbuiltbasedonthe
inter-procedural control flow graph of a program.
The IFDS Framework. In the IFDS framework, every vertex
(si,d)in the exploded super-graph stands for a statically decidable
dataflowfact,orsimply,fact, dataprogrampoint si.Everyedge
models the data flow functions between data flow facts. In thepaper, to ease the explanation, we use
sito denote the program
point at Line iin the code. For example, in an analysis to check
null dereference, the vertex (si,d)could denote that the variable d
isanullpointeratLine i.Asfortheedgesordataflowfunctions,
Figure3illustratesthree examples that show how thecommonly-
useddata flowfunctionsare representedas edgesinthe exploded
super-graph.Theverticesatthetoparethedataflowfactsbeforea
program point and the vertices at the bottom represent the facts
after theprogram point.
The first data flow function idis the identity function which
mapseachdataflowfactbeforeaprogrampointtoitself.Itindicatesthatthestatementattheprogrampointhasnoimpactsonthedata
flow analysis.
Thespecialvertexforthefact 0isassociatedwitheveryprogram
pointintheprogram.Itdenotesatautology,adataflowfactthat
always holds. An edge from the fact 0to a non- 0fact indicates
that the non- 0fact is freshly created. For example, in the second
functionin Figure 3,thefact aiscreated,which isrepresentedby
anedgefromthefact 0tothefact a.Atthesametime,since aisthe
onlyfactafterthedataflowfunction,thereisnoedgeconnecting
the factbbefore and after the program point.
PipeliningBottom-upDataFlow Analysis ICSE’20,May23–29,2020,Seoul, Republic of Korea
14.bool y = …;
15.
16.int* bar( int* a) {
17.18.
int* b = null;
19.20.
…
21.
22. int* c = y ? a : b;
23.24.
return c;
25.
26.}1.…
2.3.
int* foo() {
4.5.
int* p = bar(null);
6.7.
int* q = p;
8.
9. int* r = q;
10.11.
return r;
12.
13.}....
....
....
....
....0q r p
............
....
....0b c a
normal flow function call flow function return flow functionto callers
Figure 4: An example of the exploded super-graph for a null-dereference analysis.
The third data flow function is a typical function that models
theassignment b=a.Intheexplodedsuper-graph,thevariable a
has the same value as before. Thus, there is an edge from the data
flow fact ato itself. The variable bgets the value from the variable
a, which is modeled by the edge from the fact ato the fact b.
Itisnoteworthythatthedataflowfactsarenotlimitedtosimple
values like the local variables in the examples of the paper. For
example, in alias analysis, the facts can be sets of access paths [ 59].
In typestate analysis, the facts can be the combination of different
typestates[39].
Figure4illustrates the exploded super-graph for a data flow
analysis that tracks the propagation of null pointers. Since Line 18
assigns a null pointer to the variable b, we have the edge from the
vertex(s17,0)to the vertex (s19,b), meaning that we have the data
flow fact b=nullat Line 19. Since Line 19 does not change the
value of the variable a, we have the edge from the vertex (s17,a)
to the vertex (s19,a), which means the data flow fact about the
variableadoes not change.
Assuming that smainis the program entry point, the IFDS frame-
work aims to find paths, or determine the reachability relations,
between the vertex (smain,0)and the vertices of interests. Each of
such paths represents that some data flow fact holds at a program
point. For instance, the path from the vertex (s4,0)to the vertex
(s12,r)in Figure 4impliesthat the fact r=null holds at Line 12.
The IFDS method is efficient because it computes function sum-
maries only once for each function. Each summary is a path on the
explodedsuper-graphconnectingapairofverticesattheentryandtheexitofafunction.Thepathfromthevertex
(s17,a)tothevertex
(s25,c)in Figure 4is such a summary of the function bar. When
analyzing the callers of the function bar, e.g., the function foo,w e
can directly jump from the vertex (s4,0)to the vertex (s6,p)using
the summary without analyzing the function baragain.
The IDE Framework. TheIDEframeworkisageneralization
of the IFDS framework [ 50]. Similar to the IFDS framework, it also
works as a graph traversal on an exploded super-graph. There are
threemajordifferences.First,eachvertexontheexplodedsuper-
graph is no longer associated with a simple data flow fact d, but anenvironmentmappingafact dtoavalue vfromaseparatevalue
domain,denotedas {d/mapsto→v}.Second,duetothefirstdifference,the
data flow functions, i.e., the edges on the exploded super-graph,
transform an environment {d/mapsto→v}to the other{d/prime/mapsto→v/prime}. The
third important difference is that each edge on the exploded super-
graphislabeledwithanenvironmenttransformfunction,which
makes IDE no longer only a simple graph reachability problem.Instead,ithastofindthepathsbetweentwoverticesofinterestsand, meanwhile, compose the environment transform functions
labeledontheedgesalongthepaths.Thesedifferenceswidenthe
class of problems that can be expressed in the IFDS framework.
Inthispaper,forsimplicity,wedescribeourworkundertheIFDS
framework. This does not lose the generality for the IDE problems
because, intuitively, both problems are solved by a graph traversal
on the exploded super-graph.
2.2Coyotein a Nutshell
Let us briefly explain our approach using the code and its corre-sponding exploded super-graph in Figure 4, where the analysis
aimsto track the propagation of null pointers.
Bottom-up Analysis. For the example in Figure 4, a conven-
tional bottom-up analysis firstly analyzes the function barand
produces function summaries to summarize its behavior. With the
functionsummariesin hand, the function foothen is analyzed.
Using the symbol /leadstoto denote a path between two vertices, a
common IFDS/IDE solution will generate the following two intra-
procedural paths as the summaries of the function bar:
•The path (s17,a)/leadsto(s25,c)summarizes the function behav-
ior that a null pointer created in a caller of the function bar,
i.e.,a=null, maybe returned back to the caller.
•The path (s17,0)/leadsto(s25,c)summarizes the function behav-
ior that a null pointer created in the function barmay be
returned to the caller functions.
Note that we do not need to summarize the path (s17,0)/leadsto
(s25,0)forthefunction bar,becausethefact 0isatautologyand
always holds.
ICSE’20,May23–29,2020,Seoul, Republic of Korea QingkaiShiandCharlesZhang
(s17, 0)(s25, c)
time(s17, a)(s25, c)
(s4, 0)(s17, a)(s25, c)
(s6, p)(s10, r)(s4, 0)(s17, 0)(s25, c)
(s6, p)(s10, r)analyzing the function bar
analyzing the function foo
foo 1 foo 2bar 0 bar 1
Figure 5: The pipeline parallelization strategy.
Next, we analyze the function fooby a graph traversal from the
vertex(s4,0), which aims to track the propagation of null pointers
and produce function summaries of the function foo. During the
graphtraversal,whenthecallflowfunctions(i.e.,thedashededges)
arevisited,weapplythesummariesofthefunction barandproduce
two summaries of the function fooas following ( /llbracket·/rrbracketbarare the
summariesof the function bar):
•The path (s4,0)/leadsto/llbracket(s17,a)/leadsto(s25,c)/rrbracketbar/leadsto(s6,p)/leadsto
(s12,r)summarizesthefunctionbehaviorthatanullpointer
in the function foowill be returned to its callers.
•The path (s4,0)/leadsto/llbracket(s17,0)/leadsto(s25,c)/rrbracketbar/leadsto(s6,p)/leadsto
(s12,r)summarizesthefunctionbehaviorthatanullpointer
in the callees of the function foowill be returned to the
callers of the function foo.
Our Approach. As discussed before, in a conventional bottom-
upanalysis,theanalysisofacallerfunctionneedstowaitforthe
analysisofitscalleestocomplete.Differently, Coyoteaimstoim-
prove the parallelism by starting the analysis of the function foo
beforecompletingtheanalysisofthefunction bar.Tothisend, Coy-
otepartitions the analysis of each function finto three parts based
on where a data flow fact is created. Such a partition categorizes
the function summaries into three groups, f0,f1, andf2, which we
refer to as the pipelineable summaries:
•f0summarizes the behavior that some data flow facts cre-
ated in the caller functions will be propagated back to the
callersthroughthecurrentfunction.Thefirstsummaryof
the function baris an example.
•f1summarizesthebehaviorthatsomedataflowfactscreated
in the current function will be propagated back to the caller
functions. The second summary of the function barand the
firstsummary of the function fooare two examples.
•f2summarizesthebehaviorthatsomedataflowfactscreated
inthecalleesarepropagatedtothecurrentfunctionandwillcontinuetobepropagatedtothecallerfunctions.Thesecond
summary of the function foois an example.
Accordingtothepartitionmethod,thesummariesofthefunction
foois partitioned into two sets, foo1andfoo2, just as illustrated
in Figure 5. Since the function foodoes not have any function
parameters, the set foo0is empty and, thus, omitted. Similarly, thesummariesofthefunction barispartitionedintotwosets, bar0and
bar1. Since the function bardoes not have any callees, the set bar2
is empty and, thus, omitted. As detailed later, the above partition is
sound because it satisfies the constraint that summaries in the set
fooionly depends on the summaries in the set barj(j≤i). Thus,
we can safely pipeline the analyses of the function fooand the
functionbar- we can start analyzing the function fooimmediately
after summariesin the set bar0are generated.
Intheremainderofthispaper,undertheIFDSframework,we
formally present how to partition the analysis of a function to
generatepipelineablefunctionsummaries,sothattheparallelism
of bottom-up analysis can be improved in a sound manner.
3COYOTE: PIPELINED BOTTOM-UP
ANALYSIS
To explain our method in detail, we first define the basic notations
andterminologiesinSection 3.1andthenexplainthecriteriathat
guideourpartitionmethodinSection 3.2.Basedonthecriteria,we
present the technical details of our pipeline parallelization strategy
from Section 3.3to Section 3.5.
3.1 Preliminaries
To clearly present our approach, we introduce the following nota-
tions and terminologies.
Program Model. Given an IFDS problem, a program is mod-
eled as an exploded super graph Gthat consists of a set of intra-
procedural graphs {Gf,Gg,Gh,...}of the functions{f,g,h,...}.
Given a function f, its local graph Gfis a tuple (Lf,ef,xf,Df,Ef):
•Lfis the set of program locations in the function.
•ef,xf∈Lfare the entry and exit points of the function.
•Dfis the set of data flow facts in the function.
•Lf×Dfis the set of vertices of the graph.
•Ef⊆(Lf×Df)×(Lf×Df)is the edge set (see Figure 3).
AsillustratedinFigure 4,thelocalgraphsofdifferentfunctionsare
connected by call and return flow functions, respectively.
Function Summaries. For any function f, its function sum-
maries are a set of paths between data flow facts at the entry point
anddataflowfactsatitsexitpoint[ 45],denotedas Sf={(ef,a)/leadsto
(xf,b):a,b∈Df}.Apparently,wecangeneratethesesummaries
by traversing the graph Gffrom every vertex at the function entry.
Owingtofunctioncallsinaprogram,thesummariesofafunction
oftendependonthesummariesofitscallees.Wesayasummaryset
Sdepends on the other summary set S/primeif and only if there exists a
pathintheset Sthatsubsumesapathintheset S/prime.Asillustrated
in Section 2.2, the summaries of the function foodepend on the
summariesof the function bar.
Summary Dependence Graph. To describe the dependence
between summary sets, we define the summary dependence graph,
whereavertexisasetoffunctionsummariesandadirectededge
indicates the source summary set depends on the destination sum-
mary set.
The summarydependencegraph isbuilt based onthe callgraph.
Conventionally, vertices of the summary dependence graph are
thesummarysets {Sf,Sg,Sh...},andanedgefromthesummary
PipeliningBottom-upDataFlow Analysis ICSE’20,May23–29,2020,Seoul, Republic of Korea
setSfto the summary set Sgexists if and only if the function f
calls the function g. A bottom-up analysis works by processing
thesummary dependence graph upwards from the leaves. It starts
generatingsummariesin a summary set if it does not depend on
other summary sets or the summary sets it depends on have been
generated. Summary sets that do not have dependence relations
can be generated in parallel.
Problem Definition. In this paper, we aim to find a partition
forthesummarysetofeachfunction,say Π(Sf)={S0
f,S1
f,S2
f,...},2
suchthat a vertex of the summary dependence graph is no longer
a complete summary set Sfbut a subset Si
f(i≥0). Meanwhile,
to improve the parallelism, the bottom-up analysis based on the
dependence graph should be able to generate summaries for a pair
ofcallerandcalleefunctionsatthesametime.Indetail,thepartition
needs to satisfy the criteria discussed in the next subsection.
3.2 PartitionCriteria
Given a pair of functions where the function fcalls the function g,
weusetheset Ω(Sf,Sg)⊆Π(Sf)×Π(Sg)todenotethedependence
relationsbetweensummarysets.Generally,aneffectivepartition
methodmustmeetthefollowingcriteriatoimprovetheparallelism
of a bottom-up analysis.
TheEffectivenessCriterion. Thiscriterionconcernswhether
thedependencebetweensummarysetsintheconventionalbottom-
up analysis is actually relaxed, so that the parallelism can be im-
proved. We say the partition is effective if and only if |Ω(Sf,Sg)|<
|Π(Sf)×Π(Sg)|. Intuitively, this means that some summaries in
thecallerfunctiondonotdependonallsummariesincalleefunc-
tions.Thus,thedependencerelationintheconventionalbottom-up
analysisis relaxed.
TheSoundnessCriterion. Thiscriterionconcernsthecorrect-
nessafterthedependencebetweensummarysetsisrelaxed.Wesay
thepartitionissoundifandonlyifthefollowingconditionissat-
isfied:iftheset Si
fdependsontheset Sj
g,then(Si
f,Sj
g)∈Ω(Sf,Sg).
Violating this criterion will cause the analysis to neglect certain
functionsummariesand make the analysisunsound.
The Efficiency Criterion. Thiscriterionconcernshowmany
computationalresourcesweneedtoconsumeinordertodetermine
how to partition a summary set. Since summaries in the summary
sets,SfandSg,areunknownbeforeananalysiscompletes,theexact
dependencerelationsbetweensummariesinthetwosetsarealso
undiscovered. This fact makes it difficult to perform a fine-grained
partition, unless the analysis has been completed and we have
known what summaries are generated for each function.
Asatrade-off,conventionalbottom-upanalysisdoesnotparti-
tionthesummarysets(orequivalently, Π(Sf)={Sf}andΠ(Sg)=
{Sg}). It conservatively utilizes the observation that all summaries
in the set Sfmay depend on certain summaries in the set Sg, i.e.,
Ω(Sf,Sg)={(Sf,Sg)}. Such a conservative method satisfies the
soundnesscriterionanddoesnotpartitionthesummarysets.How-
ever,apparently,itdoesnotmeettheeffectivenesscriterionbecause
|Ω(Sf,Sg)|=|Π(Sf)×Π(Sg)|=1.
2A set partition needs to satisfy ∪i≥0Si
f=Sfand∀i,j≥0:Si
f∩Sj
f=∅.3.3 PipelineableSummary-Set Partition
Generally, it is challenging to partition a summary set satisfyingthe above criteria because the exact dependence between sum-
maries are unknown before the summaries are generated. We now
present a coarse-grained partition method that requires few pre-
computations, and thus, meets the efficiency criterion. Meanwhile,
italsomeetstheeffectivenessandsoundnesscriteriaand,thus,can
soundlyimprovetheparallelismofabottom-upanalysis.Wealso
establisha few lemmas to prove the correctness of our approach.
Intuitively, given a summary set Sf, we partition it according to
where a data flow fact is created: in a caller of the function f,i n
the current function f, and in a callee of the function f. Formally,
Π(Sf)={S0
f,S1
f,S2
f}, where
S0
f={(ef,a)/leadsto(xf,b):a/nequal0}
S1
f={(ef,0)/leadsto(eg,a)/leadsto(xf,b):f=g∨a/nequal0}
S2
f={(ef,0)/leadsto(eg,0)/leadsto(xf,b):f/nequalg}
By definition, there is no edge from a non- 0data flow fact to
the fact0on the exploded super-graph. An edge from the fact 0
to a non- 0fact means that the non- 0fact is freshly created [ 45].
Thus, any summary path in the set S0
fdoes not go through the
fact0,meaningthatthedataflowfactiscreatedinacallerofthe
functionf.Ontheotherhand,sinceasummarypathintheset S1
f
or the set S2
fstarts with the fact 0, it means that the non- 0data
flow fact on the summary path must be created in the function for
a callee of the function f. Specifically, since a summary path in the
setS1
fdoes not go through the fact 0in callee functions, the non- 0
data flow fact on the summary path is created in the function f.
Similarly,thenon- 0dataflowfactonapathfromtheset S2
fmust
be created in a callee of the function f.
The following lemma states that generating summaries in the
sets,S0
f,S1
f,andS2
f,doesnotmissanysummaryintheset Sfand,
meanwhile, does not repetitively generate a summary in the set Sf.
Lemma3.1./uniontext
i≥0Si
f=Sfand∀i,j≥0:Si
f∩Sj
f=∅.
Proof.Thisfollowsthedefinitionsofthesets S0
f,S1
f,andS2
f./square
Next, we study whether such a partition method follows the
effectivenessandsoundnesscriteria.Thekeytotheproblemisto
computetheset Ω(Sf,Sg)ofdependencerelationsbetweenapair
ofsummarysets, Si
fandSj
g,givenanypairofcaller-calleefunctions,
fandg.
Lemma3.2. ThesetsS0
f,S1
f, andS2
fdepend on the set S0g.
Proof.This follows the fact that any summary path in a caller
function may go through a callee’s summary path and the set S0gis
a part of the callee’s summaries. /square
Lemma 3.3. ThesetS2
fdepends on the sets S1gandS2g.
Proof.By definition, a summary path in the set S2
fneedstogo
through the vertex (eg,0). Given the function g, summary paths
in both the set S1gand the set S2gstart with the vertex (eg,0). Thus,
the setS2
fdepends on the sets S1gandS2g. /square
Todemonstratethattheabovelemmasdonotmissanydepen-
dence relations, we establish the following two lemmas.
ICSE’20,May23–29,2020,Seoul, Republic of Korea QingkaiShiandCharlesZhang
S0
f S1
f S2f
S0
g S1
g S2g
Figure6:Thesummarydependencegraphforacaller-callee
functionpair, fandg.
Lemma3.4. ThesetS0
fdoes not depend on the sets S1gandS2g.
Proof.This follows the fact that a non- 0data flow fact cannot
be connected back to the fact 0[45], but a summary path in the
setsS1gandS2gmuststart withthe fact 0. /square
Lemma3.5. ThesetS1
fdoes not depend on the sets S1gandS2g.
Proof.By definition, a summary path in the set S1
fdoes not go
throughthefact 0inacalleefunction.However,asummarypath
in the sets S1gandS2gmust start with the fact 0. Thus, the set S1
f
does not depend on the sets S1gandS2g. /square
PuttingLemma 3.2toLemma 3.5together,wehavethedepen-
dence set Ω(Sf,Sg)={(S0
f,S0g),(S1
f,S0g),(S2
f,S0g),(S2
f,S1g),(S2
f,S2g)},
whichdoesnotmissanydependencerelationbetweentheset Si
f
andtheset Sj
g.Thus,thepartitionmethodsatisfiesthesoundness
criterion.Meanwhile, |Ω(Sf,Sg)|=5<|Π(Sf)×Π(Sg)|=9.Thus,
theeffectivenesscriterionissatisfied,meaningthatthedependence
betweenthesummarysetsisrelaxedand,basedonthepartition,
the parallelism of a bottom-up analysis can be improved.
Figure6illustrates the summary dependence graph for a pair
ofcaller-calleefunctions, fandg.Apparently,basedonthegraph,
when the summaries in the set S0gare generated, a bottom-up anal-
ysis does not need to wait for summaries in the sets S1gandS2g, but
can immediatelystart generatingsummariesin the sets S0
fandS1
f.
3.4 Pipeline Scheduling
AsillustratedinFigure 6,givenacaller-calleefunctionpair, fand
g,wehaveanalyzedthedependencerelationsbetweentheset Si
f
and the set Sj
gand shown that the relaxed dependence provides
an opportunity to improve the parallelism of a bottom-up analysis.
However,weobservethatakeyproblemhereisthatthereareno
dependence relations between the sets Si
fandSj
ffor a function f,
and scheduling the summary-generation tasks for Si
fandSj
fin a
randomorder significantly affects the parallelism.
Figure7(a)illustratestheworstschedulingmethodwhenonly
one thread is available for each function, respectively. In the sched-ulingmethod,thesets
S0
fandS0ghavethelowestschedulingpriority
compared to other summary sets. Since all summary sets of the
functionfdepend on the set S0g, they have to wait for all summary
sets of thefunction gto generate, whichis essentially the same as
a conventional bottom-up analysis.
Thus,tomaximizetheparallelperformance,givenanyfunction g,
weneedtodeterminetheschedulingpriorityofthesets S0g,S1g,andtimeS1
g S2
g S0g
S1
f S2
f S0f
timeS0
g S2
g S1
g
S0
f S2
f S1f
timeS0
g S1
g S2g
S0
f S1
f S2f(a)
(b)
(c)
Figure 7: Different scheduling methods when one thread
availableforeach function.
S2g. First, as shown in Figure 6, since more summary sets depend
on the set S0gthan the sets S1gandS2g, scheduling the summary-
generation task for the set S0gin a higher priority will release more
tasksfor othersummary sets.
Figures7(b)and7(c)illustratethetwopossibleschedulingmeth-
odswhenforanyfunction g,thesetS0gisinthehighestpriority.In
Figure7(b),the set S2ghasahigher priority than theset S1g. Since
thesetS2
fdependsonthesets S0g,S1g,andS2g,ithastowaitforall
summaries ofthe function gto generate,leading to asub-optimal
scheduling method. In contrast, Figure 7(c) illustrates the best case
where the summary-generation tasks are adequately pipelined.
To conclude, the scheduling priority for any given function g
should be S0g>S1g>S2g, so that the parallelism of a bottom-up
analysis can be effectively improved when a limited number ofidle threads are available. Such prioritization does not affect the
parallelismwhenthere are enough idle threads available.
3.5ϵ-Bounded Partition and Scheduling
Ideally, the aforementioned partition method evenly partitions a
summarysetsothattheanalysistasksforgeneratingsummariesareadequatelypipelined,asshowninFigure 7(c).However,inpractice,
itisusuallynotthecasebutworksasFigure 8(a),wherethesets
S0g
andS1gare much larger than other summary sets.
Apparently,ifthereareextrathreadsavailableandwecanfurther
partitionthesummarysets S0gandS1gintotwosubsets,theanalysis
performancethenwillbe improvedby generatingsummariesinthe
subsetsinparallel,justasillustratedinFigure 8(b).Unfortunately,
beforea bottom-upanalysisfinishes, wecannot knowthe actual
sizeofeachsummarysetand,thus,cannotevenlypartitionaset.As
an alternative, what we can do is to approximate an even partition.
PipeliningBottom-upDataFlow Analysis ICSE’20,May23–29,2020,Seoul, Republic of Korea
timeS0
g S1
g S2g
S0
f S1
f S2f
timeS0
g S1
g S2g
S0
f S1
f S2fS0
g S1
g(a)
(b)
Figure 8: Bounded partition and its scheduling method.
Considering that the analysis task of summary generation is
actuallytoperformagraphtraversalfromavertex,wetrytofurther
partitionasummaryset Si
fbasedonthenumberofstartingvertices
of the graph traversal. To this end, we introduce a client-definedconstant
ϵ,3so that, after the approximately even partition, the
graphtraversalforgeneratingfunctionsummariesinasummary
set startsfrom no more than ϵvertices.
Forexample,togeneratesummariesintheset S0
f,theanalysis
needs to traverse the graph Gffrom each non- 0data flow fact at
thefunctionentry.Supposethefunction fhasfournon- 0dataflow
facts,{w,x,y,z}andϵ=2.Then,theset S0
fisfurtherpartitioned
into two subsets{(ef,a)/leadsto(xf,b):a∈{w,x}}and{(ef,a)/leadsto
(xf,b):a∈{y,z}}.Afterthepartition,thegraphtraversalforboth
summary sets starts from two vertices.
Similar partition can be performed on the sets S1
fandS2
fbut
the following explanation needs to be considered. By definition, it
seemsdifficulttofurtherpartitionsets S1
fandS2
fbasedontheabove
method, because all summary paths in them start with a single
vertex(ef,0). The key is that, since the fact 0is a tautology and
vertices with the fact 0are always reachable from each other [ 45],
thegraphtraversaltogeneratesummariesinthesets S1
fandS2
fare
not necessary to start from the vertex (ef,0). For instance, since
the setS1
fcontains the summary paths where data flow facts are
created in the function f, we can traverse the graph Gffrom every
vertex that has an immediate predecessor (s∈Lf,0).4Similarly,
consideringthattheset S2
fcontainsthesummarypathswheredata
flowfactsare createdinacalleeofthe function f,wecan traverse
thegraph Gffromeveryvertexthathasanincomingedgefromthe
callees. With multiple startingvertices for the graph traversal, we
then can partitionthe sets S1
fandS2
fsimilarlyas the set S0
f.
Itisnoteworthythatsuchaboundedpartitionaimstoparallelize
the analysis in a single function and, thus, is applicable to both
ourpipeliningapproachandtheconventionalbottom-upapproach.
Nevertheless, it is particularly useful to improve the pipeline ap-
proach as discussed above.
3We useϵ=5in our implementation.
4Recall that anedge from the fact 0to anon-0dataflow factmeans the non- 0factis
freshly created.Master Process CycleThread Process 
Cycles
Queue of TasksNew Task
to Generate
Summaries
Completed TaskThread Pool
Summary Dep. Graph
Figure 9: Pipelining bottom-up data flow analysis using athreadpool.
4 IMPLEMENTATION
We have implemented Coyoteon top of LLVM5to path-sensitively
analyzeC/C++programs.Thissectiondiscussestheimplementa-
tion details. In the evaluation, for a fair comparison, except for the
parallelstrategywestudyinthepaper,allotherimplementation
details are the same in both Coyoteand the baselineapproaches.
4.1 Parallelization
As illustrated in Figure 9, we implement a thread pool to drive our
pipelineparallelizationstrategy.Inthefigure,themasterprocess
cyclemaintainsthesummarydependencegraphforallfunctions.
Each vertex in the graph represents a task to generate certain func-
tion summaries. Whenever all of the dependent tasks have been
completed, it pushes the current task, referred to as the active task,
into a queue and waits for an idle thread to consume it. When a
task is completed, the master process cycle is notified so that it can
continueto find more active tasks on the dependence graph.
Inourimplementation,insteadofrandomlyschedulingthetasks
in thethread pool, wealso seek todesign a systematicscheduling
method so that we can well-utilize CPU resources. However, itis known that generating an optimal schedule to parallelize thecomputations in a dependence graph is a variant of precedent-constraintscheduling, whichis NP-complete [
30]. Therefore, we
employagreedycriticalpathscheduler[ 35].Acriticalpathisthe
longest remaining path from a vertex to the root vertex on the
dependencegraph.WethenreplacethetaskqueueinFigure 9with
a priority queue and prioritize tasks based on the length of critical
paths. It is noteworthy that this heuristic scheduling method does
not conflict with the pipeline scheduler presented in Section 3.4.
The pipeline scheduler prioritizes the analysis tasks in the same
function,whilethecritical-pathscheduleronlyprioritizesthetasks
from different functions.
4.2 Taint Analysis
To demonstrate that our approach is applicable to a broad range
ofdataflowanalysis,inadditiontothenullanalysisdiscussedin
thepaper,wealsoimplementataintanalysistochecktwokinds
of taint issues. First, we check relative path traversal, which allows
5LLVM:https://llvm.org/.
ICSE’20,May23–29,2020,Seoul, Republic of Korea QingkaiShiandCharlesZhang
an attacker to access files outside of a restricted directory.6It is
modeledasapathontheexplodedsuper-graphfromanexternal
input to a file operation. A typical example is a path from a user
inputinput=gets(...) to a file operation fopen(...). Second, we check
transmission of private resources, which may leak private data to
attackers.7Itismodeledasapathontheexplodedsuper-graphfrom
sensitivedatatoI/Ooperations.Atypicalexampleisapathfromthe
password password=getpass(...) to an I/O operation sendmsg(...).
4.3 PointersandPath-Sensitivity
The null analysis and the taint analysis in Coyoterequire highly
precisepointerinformationsothattheycandeterminehowdata
flowfactspropagatethroughpointer(loadandstore)operations.To
resolvethepointerrelations,wefollowthepreviouswork[ 54]to
perform a path-sensitive points-to analysis. The points-to analysis
isefficientbecauseitdoesnotexhaustivelysolvepathconditions
butrecordstheconditionsonthegraphedges.Whentraversingthe
graphforananalysis,wecollectandsolveconditionsonapathinademand-drivenmanner.In Coyote,weuseZ3 [
13]astheconstraint
solvertodeterminepathfeasibility.Accordingtoourexperience
andmanyexistingworks[ 4,15,54,64],path-sensitivityisacritical
factortomakeananalysispracticalandmaketheevaluationcloser
to a real application scenario. For instance, a path-insensitive nullanalysisreports >90% false positives and, thus, is impractical.
Afterbuildingtheexplodedsuper-graphwiththepoints-toanal-
ysis, we simplify the graph via a program slicing procedure, which
removesirrelevantedgesandvertices,therebyimprovingtheperfor-manceofthesubsequentnullandtaintanalyses.Thissimplificationprocessisalmostlineartothegraphsizeand,thus,isveryfast[
46].
As an example, Figure 10(a) is a program where a null pointer is
propagated to the variable cthrough the store and load operations
atLine5andLine9.Weusethepoints-toanalysistoidentifythe
propagationandbuildtheexplodedsuper-graphasillustratedin
Figure10(b).Inthisgraph,theconditionofthepropagation yand¬y
arelabeledontheedges.Figure 10(c)illustratesthesimplifiedform
of the original graph, where unnecessary edges like (s10,ob)/leadsto
(s12,ob)and unnecessary vertices like (s8,ob)are removed.
4.4 Soundness
Our implementation of Coyoteis soundy [ 32], meaning that it
handlesmostlanguagefeaturesinasoundmannerwhilewealso
make some well-identified unsound choices following the previous
work[4,10,54,58,64].Notethat Coyoteaimstofindasmanybugs
as possible rather than rigorously verifying the correctness of a
program.Inthiscontext,theunsoundchoiceshavelimitednegativeimpactsasdemonstratedinthepreviousworks.Inourimplementa-
tion, like the previous work [ 24], we use a flow-insensitive pointer
analysis [ 56] to resolve function pointers. We unroll each cycle
twiceonboththecallgraphandthecontrolflowgraph[ 4].Follow-
ingtheworkof Saturn[64],arepresentativestaticbugdetection
tool, wedo notmodel inlineassembly and libraryutilities suchas
std::vector, std::set, and std::map from the C++ standard template
library.
6CWE-23: https://cwe.mitre.org/data/definitions/23.html.
7CWE-402: https://cwe.mitre.org/data/definitions/402.html.1.bool y = …;
2.
3.int* bar( int** a, int** b) {
4.5.
*b = null;
6.7.
…
8.
9. int* c = y ? *a : *b;
10.
11. *c = 1;
12.
13.}..
.
.
..0o b co a
....
........
....
....0o bco a
yy yy
(a)                                       (b)                                       (c)
Figure 10: (a) A code snippet. (b) The exploded super-graph
built basedon a points-toanalysis. (c) Thesimplified graph.o
aand obrepresentthememoryobjectpointedtoby aandb,
respectively. yand¬yon the edges are the path conditions.
5 EVALUATION
Wenowpresenttheexperimentalsetupandtheexperimentalre-
sults to demonstrate the effectiveness of our new parallel data flowanalysis.Wealsodiscussthefactorsaffectingtheevaluationresults
at the end of this section.
5.1 ExperimentalSetup
Our goal is to study the scalability of Coyote, a pipeline paral-
lelization strategy for bottom-up data flow analysis. We did thisby measuring the CPU utilization rates and the speedup over a
conventional parallel implementation. More specifically, a conven-
tional parallel implementation only analyzes functions withoutcalling dependence in parallel, just as illustrated in Figure 1.T o
precisely measure and study the scalability of our approach, we
introduce an artificial throttle that allows us to switch between
our pipeline strategy and the conventional parallel strategy. In this
manner,wecanguaranteethat,exceptfortheparallelstrategies,
allotherimplementationdetailsdiscussedinSection 4arethesame
forbothourapproachandthebaselineapproach.Forinstance,both
approachesacceptthesameexplodedsuper-graphastheinput.Par-
ticularly, as discussed in Section 3.5, since the ϵ-bounded partition
aims to parallelize the analysis in a single function, it is adopted in
bothourapproachand thebaselineapproachforafaircomparison.
Therefore,thespeedupofourapproachdemonstratedinthissec-
tionisachievedbythepipelinestrategy,i.e.,thekeycontribution
ofthispaper,inisolation.Likethepreviouswork[ 1],wedidnot
compare our implementation with other tools like Saturn[64] and
Calysto[4].Thisisbecausethecomparisonresultswillnotmake
any sense due to a lot of different implementation details that may
affect the runtime performance.
Ourevaluationof CoyotewasoverthestandardSPECCINT2000
benchmarks,8whichiscommonlyusedintheliteratureonstatic
analysis[ 54,58].Wealsoincludeeightindustrial-sizedopen-source
C/C++ projects such as Python, OpenSSL, and MySQL. These real-
world subjects are the monthly trending projects on Github that
we are able to set up. Table 1lists the evaluation subjects. The size
8SPECCINT2000benchmarks: https://www.spec.org/cpu2000/CINT2000/.
PipeliningBottom-upDataFlow Analysis ICSE’20,May23–29,2020,Seoul, Republic of Korea
Table 1: Subjects for evaluation.
Origin ID Program Size (KLoC) # Functions
SPEC
CINT20001 mcf 2 26
2 bzip2 3 74
3 gzip 6 89
4 parser 8 324
5 vpr 11 272
6 crafty 13 108
7 twolf 18 191
8 eon 22 3,367
9 gap 36 843
10 vortex 49 923
11 perlbmk 73 1,069
12 gcc 135 2,220
Open
Source13 bftpd 5 260
14 shadowsocks 32 574
15 webassembly 75 7,842
16 redis 101 1,527
17 python 434 3,619
18 icu 537 27,046
19 openssl 791 11,759
20 mysql 2,030 79,263
Total4,381 Avg.7,070
of these subjects is more than four million lines of code in total,
ranging from a few thousand to two million lines of code. Thenumberoffunctionsofthesesubjectsrangesfromtenstonearly
eighty thousand functions, with about seven thousand on average.
Weranourexperimentsonaserverwitheighty“Intel(R)Xeon(R)
CPU E5-2698 v4 @ 2.20GHz” processors and 256GB of memoryrunning Ubuntu-16.04. We set our initial number of threads tobe twenty and added twenty for every subsequent run until themaximum number of available processors, i.e., eighty. All the ex-
periments were run with the resource limitation of twelve hours.
5.2 Study of the Null Analysis
We first present the experimental results of the null analysis in
detail, followed by a brief discussion on the taint analysis in the
next subsection.
5.2.1 Speedup. Table2lists the comparison results of the con-
ventional parallel mechanism (Conv) and our pipeline strategy
(Pipeline) for the bottom-up program analysis. Each row of the
table represents the results of a benchmark program, including the
timecostinsecondsandthespeedupforthesetwokindsofparallel
mechanisms. The speedup is calculated as the ratio of the time
taken byCoyoteto that of the conventional parallel approach with
the same number of threads.
We observe that the speedup achieved with 20 threads is 1.5 ×
on average. However, a sthenumberofthreadsisincreasedto80,
the observed speedup also increases, up to 3 ×faster. Using several
typical examples, Figure 11illustrates the relation between the
numberofthreadsandthespeedup.Thegrowingcurvesshowthat
thespeedupincreaseswiththegrowthofthenumberofthreads,
demonstratingthatwecanalwaysachievespeedupandhavehigher
parallelismthan the conventionalparallel approach.1. 01. 52. 02. 53. 03. 5
0. 5 1 1. 5 2 2. 5 3 3. 5 4 4. 5



ID = 2
ID = 3
ID = 6
ID = 7
ID = 13
20                                     40 60                                   80                   
# Thread Speedup
Figure11:Speedupvs.Thenumberofthreads.
Itisnoteworthythatsuch2 ×-3×speedupissignificantenoughto
makemanyoverlylengthyanalysesusefulinpractice.Forexample,
originally, it takes more than 10 hours to analyze MySQL (ID =
20,Size=2MLoC,typicalsizeinindustry).Thetimecostcannot
satisfy the industrial requirement of finishing analysis in 5 to 10
hours[35].Withthepipelinestrategy,itsavesmorethan6hours,
makingthe bug findingtaskacceptablein the industrial setting.
5.2.2 CPU Utilization Rate. The speedup over the conventional
parallel design is due to the higher parallelism achieved by the
pipelinestrategy.Toquantifythiseffect,weprofiletheCPUutiliza-
tionratesforboththeconventionalparalleldesignandthepipeline
method.Figure 12demonstrates theCPUutilizationrates against
the elapsed running time. Due to the page limit, we only show sev-
eraltypicalonesforsomeoftheprogramsrunningwith80threads.
In the figure, the solid line represents the CPU utilization rate ofour pipeline method while the dashed line represents that of the
conventionalparallel design.
We can observe that, for each project, in the initial phase of
the analysis, the CPU utilization rates for both parallel designs are
similar, almost occupying all available CPUs. This is because the
call graph of a program is usually a tree-like data structure. In the
bottom half of the call graph, it usually has enough independent
functionsthatwecananalyzeinparallel.Thus,bothparalleldesigns
can sufficientlyutilize the CPUs.
Our pipeline strategy unleashes its power in the remaining part
oftheanalysis,where itapparentlyhasmuchhigherCPUutiliza-
tion rates, thus finishing the analysis much earlier. This is because
thetophalfofacallgraphismuchdenser,wheretherearemore
callingrelationsthanthebottomhalf.Sincetheconventionalparal-leldesigncannotanalyzefunctionswithcallingrelationsinparallel,
it cannot sufficiently utilize the CPUs. In contrast, our approach
splitstheanalysisofafunctionintomultiplepartsandallowsusto
analyze functions with calling relations in parallel, thus being able
to utilize more CPUs.
5.3 Study of the Taint Analysis
In order to demonstrate that our approach is generalizable to other
analyses, we also conducted an experiment to see whether the
pipeline approach can improve the scalability of taint analysis.
ICSE’20,May23–29,2020,Seoul, Republic of Korea QingkaiShiandCharlesZhang
Table2:Runningtime(seconds)andthespee dup over the conventional parallel design of bottom-up analysis.
ID# Thread = 20 # Thread = 40 # Thread = 60 # Thread = 80
Conv Pipeline Speedup Conv Pipeline Speedup Conv Pipeline Speedup Conv Pipeline Speedup
160 28 2.1× 60 24 2.5× 60 20 3.0× 60 20 3.0×
2108 64 1.7× 96 40 2.4× 96 36 2.7× 96 32 3.0×
3168 76 2.2× 168 61 2.8× 168 56 3.0× 168 56 3.0×
4252 215 1.2× 168 120 1.4× 132 92 1.4× 132 72 1.8×
5264 192 1.4× 180 116 1.6× 156 88 1.8× 144 76 1.9×
6192 104 1.8× 168 76 2.2× 168 64 2.6× 168 60 2.8×
7168 132 1.3× 133 80 1.7× 121 64 1.9× 122 56 2.2×
82568 2148 1.2 × 1620 1192 1.4 × 1296 865 1.5 ×1128 708 1.6×
91728 860 2.0 × 1524 648 2.4 × 1500 576 2.6 ×1476 545 2.7×
10843 648 1.3× 698 374 1.9× 674 280 2.4× 662 252 2.6×
111530 913 1.7 × 1325 604 2.2 × 1232 528 2.3 ×1217 500 2.4×
121978 1573 1.3 × 1486 926 1.6 × 1306 729 1.8 ×1235 613 2.0×
13156 109 1.4× 132 68 1.9× 132 52 2.5× 132 44 3.0×
14876 468 1.9× 780 340 2.3× 768 296 2.6× 768 288 2.7×
152940 1990 1.5 × 2292 1248 1.8 × 2076 1012 2.1 ×1980 908 2.2×
161332 1060 1.3 × 984 628 1.6× 900 488 1.8× 864 416 2.1×
175162 3022 1.7 × 4276 2036 2.1 × 4035 1738 2.3 ×3895 1605 2.4×
187.8hr 5.5hr 1.4 ×5.8hr 3.4hr 1.7 ×5.2hr 2.6hr 2.0 ×4.9hr 2.3hr 2.1×
192.8hr 2.2hr 1.2 ×1.9hr 1.2hr 1.6 ×1.7hr 0.9hr 1.9 ×1.6hr 0.8hr 2.0×
20Time Out 9.6hr - TimeOut 7.8hr - TimeOut 6.4hr - 11.8hr 5.6hr 2.1×
	
 bftpd
	

icu
timetimeCPU Utilization Rate (%) CPU Utilization Rate (%)
	
 webassembly
	
 openssltime
timeCPU Utilization Rate (%) CPU Utilization Rate (%)
	
 python
	

mysql
timetimeCPU Utilization Rate (%) CPU Utilization Rate (%)
Figure12:CPUutilizationratevs.Theelapsedtime.ThesolidlinesrepresenttheCPUutilizationrateofourpipelinemethod
whilethe dashed lines represent that of the conventional parallel design.
Sincetheresultof taintanalysisarequitesimilartothat ofthenull
analysis, we briefly summarize the experimental results in Table 3,
wheretheresultsofourlargestbenchmarkprogram,MySQL,are
presented.Theresultsdemonstratethat,withtheincreaseofthe
number of available threads, the speedup of our approach overthe conventional approach also grows to >2
×in analyzing both
therelativepathtraversal (RPT)bugorthe transmissionofprivate
resources (TPR) bug.5.4 Discussion
There are two main factors affecting the evaluation results: the
densityof the call graph and the number of available threads.
As discussed above, when the call graph is very sparse, the
advantage of our approach is not very obvious. For instance, if
functionsareallindependentoneachother,allfunctionscanberun
inparallel.Thus,bothapproachescanalwayssufficientlyutilizethe
availablethreadsand,thus,havesimilartimeefficiency.Inpractice,asdemonstratedinourevaluation,thecallgraphisusuallytree-like.
PipeliningBottom-upDataFlow Analysis ICSE’20,May23–29,2020,Seoul, Republic of Korea
Table 3: Results of the taint analysis on MySQL.
Taint Issues# Thread = 20 # Thread = 40 # Thread = 60 # Thread = 80
Conv Pipeline Speedup Conv Pipeline Speedup Conv Pipeline Speedup Conv Pipeline Speedup
RPT Time Out 10.2hr - Time Out 8.7hr - Time Out 7.1hr - 10.9hr 4.7hr 2.3×
TPR 9.3hr 6.6hr 1.4× 8.1hr 5.0hr 1.6× 7.4hr 3.9hr 1.9 ×6.1hr 2.8hr 2.2×
Thus, our approach can present its power in the second half of the
analysisandachieves up to 3× speedup in practice.
The number of threads is also a key factor affecting the ob-
served speedup of our approach. For instance, if we only have one
threadavailable,althoughourapproachcanprovidemoreindepen-
dent tasks, these tasks cannot be run in parallel. Thus, both of our
approach and the conventional one will emit similar results. As
illustratedbytheevaluation,ourapproachcanworkbetterwhen
we have more available threads. In the cloud era, we can expect
that we have unlimited CPU resources and, thus, can expect more
benefits from our approach in practice.
6 RELATED WORK
Parallel and distributed algorithms for data flow analysis is an
active area of research. In this section, we survey existing parallel
or distributed techniques and compare them with Coyote.
Inordertoutilizethemodularstructureofaprogramtoparal-
lelize the analyses in different functions, developers usually imple-
ment a data flow analysis in a top-down fashion or a bottom-up
manner.Albarghouthietal .[1]presentedagenericframeworkto
distributetop-downalgorithmsusingamap-reducestrategy.Par-
allel worklist approaches, a kind of top-down analysis, also canaddress the IFDS/IDE problems. They operate by processing the
elementsonananalysisworklistinparallel[ 14,21,48].Theseap-
proaches are different from ours because this paper focuses on
bottom-upanalysis.Inouropinion,thetop-downapproachandthe
bottom-up approach are two separate schools of methodologies to
implement program analysis.Bottom-up approaches analyze each
function onlyonce and generate summaries reusable at all calling
contexts.Top-downapproachesgeneratesummariesthatarespe-
cific to individual calling contexts and, thus, may need to repeatanalyzing a function. For analyses that need high precision likepath-sensitivity, repetitively analyzing a function is costly. Thus,
we may expect better performance from bottom-up analysis when
highprecision is required.
Comparedtotop-downanalysis,bottom-upanalysishasbeen
traditionallyeasier toparallelize. Existingstatic analyses,suchas
Saturn[64],Calysto[4],Pinpoint [54], andInfer[8], have utilized
the function-level parallelization to improve their scalability. How-
ever, none of them presented any techniques to further improve itsparallelism. McPeak et al
.[35]pointed out that the CPU utilization
ratemaydropinthedensepartofthecallgraphwheretheparal-
lelism is significantly limited by the calling dependence. Although
they presented an optimized scheduling method to mitigate theperformance issue, the calling dependence was not relaxed and
thefunction-levelparallelismwasnotimproved.Webelievethat
their scheduling method is complementary to Coyoteand their
combinationhas the potential for the greater scalability.In contrast to top-down and bottom-up approaches, partition-
based approaches [ 6,12,17,22,26,29,33,38] do not utilize the
modularstructureofaprogrambutpartitionthestatespaceanddistributethestate-spacesearchtoseveralthreadsorprocessors.
Anothercategoryofdataflowanalyses(e.g.,[ 2,7,25])aremodeled
as Datalog queries rather than the graph reachability queries in
theIFDS/IDEframework.TheycanbenefitfromparallelDatalog
engines to improve the scalability [ 20,27,28,34,51–53,61,62,66].
Recently, some other parallel techniques have been proposed.
Many of them focus on pointer analysis [ 18,31,37,41,44,57]
rather than general data flow analysis. Mendez-Lojo et al. [ 36]
proposed a GPU-based implementation for inclusion-based pointer
analysis. EigenCFA [43] is a GPU-based flow analysis for higher-
order programs. Graspan[60] andGrapple[68] turn sophisticated
code analysis into big data analytics. They utilize recent advances
onsolid-statediskstoparallelizeandscaleprogramanalysis.These
techniquesarenotdesignedforcompositionaldataflowanalysis
and, thus,are different from our approach.
Inadditiontoautomatictechniques,Balletal .[5]usedmanually
createdharnessestospecifyindependentdevicedriverentrypoints
so that an embarrassingly parallel workload can be created.
7 CONCLUSION
Wehave presented Coyote,a pipelineparallelization strategythat
enablestoperformbottom-updataflowanalysisinafasterway.Thepipelinestrategyrelaxesthecallingdependence,whichconvention-
ally limits the parallelism of bottom-up analysis. The evaluation of
our approach demonstrates higher CPU utilization rates and signif-icantspeedupoveraconventionalparalleldesign.Inthemulti-core
era,webelievethatimprovingthe parallelismisanimportantap-
proach to scaling static program analysis.
ACKNOWLEDGMENTS
The authors would like to thank the anonymous reviewers for
their insightful comments. This work is partially funded by anMSRA grant, as well as Hong Kong GRF16230716, GRF16206517,
ITS/215/16FP, and ITS/440/18FP grants.
REFERENCES
[1]AwsAlbarghouthi,RahulKumar,AdityaVNori,andSriramKRajamani.2012.
Parallelizing top-down interprocedural analyses. In Proceedings of the 33rd ACM
SIGPLANConferenceonProgrammingLanguageDesignandImplementation(PLDI
’12). ACM, 217–228.
[2]Nicholas Allen, Padmanabhan Krishnan, and Bernhard Scholz. 2015. Combining
type-analysis with points-to analysis for analyzing Java library source-code. In
Proceedings of the 4th ACM SIGPLAN International Workshop on State Of the Art
in Program Analysis (SOAP ’15) . ACM, 13–18.
[3]Steven Arzt, Siegfried Rasthofer, Christian Fritz, Eric Bodden, Alexandre Bar-
tel,JacquesKlein,YvesLeTraon,DamienOcteau,andPatrickMcDaniel.2014.
Flowdroid: Precise context, flow, field, object-sensitive and lifecycle-aware taint
analysisforandroidapps.In Proceedingsofthe35thACMSIGPLANConferenceon
Programming Language Design and Implementation (PLDI ’14). ACM, 259–269.
ICSE’20,May23–29,2020,Seoul, Republic of Korea QingkaiShiandCharlesZhang
[4]Domagoj Babic and Alan J. Hu. 2008. Calysto: Scalable and precise extended
staticchecking.In Proceedingsofthe30thInternationalConferenceonSoftware
Engineering (ICSE’08). IEEE, 211–220.
[5]ThomasBall,VladimirLevin,andSriramKRajamani.2011. Adecadeofsoftware
model checking with SLAM. Commun.ACM 54,7 (2011), 68–76.
[6]JiriBarnat,LubosBrim,andJitkaStříbrná.2001. DistributedLTLmodel-checking
in SPIN. In International SPIN Workshop on Model Checking of Software . Springer,
200–216.
[7]Martin Bravenboer and Yannis Smaragdakis. 2009. Strictly declarative specifica-
tionofsophisticatedpoints-toanalyses.In Proceedingsofthe24thACMSIGPLAN
Conference on Object Oriented Programming Systems Languages and Applications
(OOPSLA ’09). ACM, 243–262.
[8]CristianoCalcagno,DinoDistefano,PeterW.O’Hearn,andHongseokYang.2011.
Compositional shape analysis by means of bi-abduction. J. ACM58, 6 (2011),
26:1–26:66.
[9]SagarChaki,EdmundMClarke,AlexGroce,SomeshJha,andHelmutVeith.2004.ModularverificationofsoftwarecomponentsinC. IEEETransactionsonSoftware
Engineering 30,6 (2004), 388–402.
[10]SigmundCherem,LonniePrincehouse,andRaduRugina.2007. Practicalmemory
leak detection using guarded value-flow analysis. In Proceedings of the 28th ACM
SIGPLANConferenceonProgrammingLanguageDesignandImplementation(PLDI
’07). ACM, 480–491.
[11]Chia Yuan Cho, Vijay D’Silva, and Dawn Song. 2013. BLITZ: Compositional
bounded model checking for real-world programs. In Proceedings of the 28th
IEEE/ACM International Conference on Automated Software Engineering (ASE ’13).
IEEE,136–146.
[12]Liviu Ciortea, Cristian Zamfir, Stefan Bucur, Vitaly Chipounov, and GeorgeCandea. 2010. Cloud9: A software testing service. ACM SIGOPS Operating
SystemsReview 43,4 (2010), 5–10.
[13]LeonardoDeMouraandNikolajBjørner.2008. Z3:AnefficientSMTsolver.In
InternationalconferenceonToolsandAlgorithmsfortheConstructionandAnalysis
of Systems. Springer, 337–340.
[14]Kyle Dewey, Vineeth Kashyap, and Ben Hardekopf. 2015. A parallel abstract
interpreterforJavaScript.In 2015IEEE/ACMInternationalSymposiumonCode
GenerationandOptimization(CGO ’15) . IEEE, 34–45.
[15]IsilDillig,ThomasDillig,andAlexAiken.2008. Sound,completeandscalable
path-sensitiveanalysis. In Proceedingsof the29thACM SIGPLANConferenceon
Programming Language Design and Implementation (PLDI ’08). ACM, 270–280.
[16]IsilDillig,ThomasDillig,AlexAiken,andMoolySagiv.2011. Preciseandcompact
modular proceduresummaries for heapmanipulating programs. In Proceedings
of the 32nd ACM SIGPLAN Conference on Programming Language Design and
Implementation (PLDI’11). ACM, 567–577.
[17]MatthewBDwyer,SebastianElbaum,SuzettePerson,andRahulPurandare.2007.
Parallelrandomizedstate-spacesearch.In Proceedingsofthe29thInternational
Conferenceon Software Engineering (ICSE ’07). IEEE, 3–12.
[18]Marcus Edvinsson, Jonas Lundberg, and Welf Löwe. 2011. Parallel points-to
analysisformulti-coremachines.In Proceedingsofthe6thInternationalConference
on High Performance and Embedded Architectures and Compilers. ACM, 45–54.
[19]StephenJFink,EranYahav,NuritDor,GRamalingam,andEmmanuelGeay.2008.
Effective typestate verification in the presence of aliasing. ACM Transactions on
Software Engineering and Methodology (TOSEM) 17,2 (2008), 9.
[20]SumitGanguly,AviSilberschatz,andShalomTsur.1990. AFrameworkforthe
Parallel Processing of Datalog Queries. In Proceedings of the 1990 ACM SIGMOD
International Conference onManagement of Data (SIGMOD ’90). ACM,143–152.
[21]DiegoGarbervetsky,EdgardoZoppi,andBenjaminLivshits.2017. Towardfull
elasticity in distributed static analysis: the case of callgraph analysis. In Proceed-
ingsofthe201711thJointMeetingonFoundationsofSoftwareEngineering (FSE
’17). ACM, 442–453.
[22]OrnaGrumberg,TamirHeyman,NiliIfergan,andAssafSchuster.2005. Achieving
speedups in distributed symbolic reachability analysis through asynchronous
computation. In Advanced Research Working Conference on Correct Hardware
DesignandVerification Methods. Springer, 129–145.
[23]SalvatoreGuarnieri,MarcoPistoia,OmerTripp,JulianDolby,StephenTeilhet,
and Ryan Berg. 2011. Saving the world wide web from vulnerable JavaScript. In
Proceedings of the 2011 International Symposium on Software Testing and Analysis
(ISSTA ’11). ACM, 177–187.
[24]BenHardekopfandCalvinLin.2011. Flow-sensitivepointeranalysisformillions
of lines of code. In Code Generation and Optimization (CGO), 2011 9th Annual
IEEE/ACM International Symposium on. IEEE, 289–298.
[25]Behnaz Hassanshahi, Raghavendra Kagalavadi Ramesh, Padmanabhan Krishnan,
BernhardScholz,andYiLu.2017. Anefficienttunableselectivepoints-toanal-
ysis for large codebases. In Proceedings of the 6th ACM SIGPLAN International
Workshop on State Of the Art in Program Analysis (SOAP ’17). ACM, 13–18.
[26]Gerard J Holzmann and Dragan Bosnacki. 2007. The design of a multicore
extension of the SPIN model checker. IEEE Transactions on Software Engineering
33,10 (2007), 659–674.
[27]G. Hulin. 1989. Parallel Processing of Recursive Queries in Distributed Archi-
tectures.In Proceedingsofthe15thInternationalConferenceonVeryLargeDataBases (VLDB’89). Morgan Kaufmann Publishers Inc., 87–96.
[28]HerbertJordan,PavleSubotić,DavidZhao,andBernhardScholz.2019. Aspe-
cialized B-tree for concurrent datalog evaluation. In Proceedings of the 24th Sym-
posium on Principles and Practice of Parallel Programming (PPoPP ’19). ACM,
327–339.
[29]Yong-fong Lee and Barbara G Ryder. 1992. A comprehensive approach to par-allel data flow analysis. In Proceedings of the 6th International Conference on
Supercomputing. ACM, 236–247.
[30]Jan Karel Lenstra and AHG Rinnooy Kan. 1978. Complexity of scheduling under
precedence constraints. Operations Research 26,1 (1978), 22–35.
[31]BozhenLiu,JeffHuang,andLawrenceRauchwerger.2019. RethinkingIncremen-
tal and Parallel PointerAnalysis. ACM Transactions on Programming Languages
and Systems (TOPLAS) 41,1 (2019), 6.
[32]BenjaminLivshits,ManuSridharan,YannisSmaragdakis,OndřejLhoták,JNelson
Amaral, Bor-Yuh Evan Chang, Samuel Z Guyer, Uday P Khedker, Anders Møller,
andDimitriosVardoulakis.2015. Indefenseofsoundiness:amanifesto. Commun.
ACM58,2 (2015), 44–46.
[33]Nuno P Lopes and Andrey Rybalchenko. 2011. Distributed and predictable soft-
ware model checking. In International Workshop on Verification, Model Checking,
andAbstractInterpretation . Springer, 340–355.
[34]Carlos Alberto Martínez-Angeles, Inês Dutra, Vítor Santos Costa, and Jorge
Buenabad-Chávez. 2013. A datalog engine for gpus. In Declarative Programming
andKnowledge Management. Springer, 152–168.
[35]Scott McPeak, Charles-Henri Gros, and Murali Krishna Ramanathan. 2013. Scal-
able and incremental software bug detection. In Proceedings of the 2013 9th Joint
Meetingon Foundationsof SoftwareEngineering (ESEC/FSE’13).ACM, 554–564.
[36] MarioMendez-Lojo,Martin Burtscher,and KeshavPingali. 2012. AGPU imple-
mentationof inclusion-basedpoints-to analysis.In Proceedingsof the17th ACM
SIGPLANSymposiumonPrinciplesandPracticeofParallelProgramming (PPoPP
’12). ACM, 107–116.
[37]Mario Méndez-Lojo, Augustine Mathew, and Keshav Pingali. 2010. Parallel
inclusion-based points-to analysis. In Proceedings of the ACM International Con-
ference on Object Oriented Programming Systems Languages and Applications
(OOPSLA ’10). ACM, 428–443.
[38]DavidMonniaux.2005. TheparallelimplementationoftheAstréestaticanalyzer.
InAsian Symposium on Programming Languages and Systems. Springer, 86–96.
[39]NomairANaeemandOndrejLhotak.2008. Typestate-likeanalysisofmultiple
interactingobjects.In Proceedingsofthe23rdACMSIGPLANConferenceonObject-
oriented Programming Systems Languages and Applications (OOPSLA ’08). ACM,
347–366.
[40]Nomair A Naeem and Ondrej Lhoták. 2009. Efficient alias set analysis using SSA
form.InProceedingsofthe2009InternationalSymposiumonMemoryManagement
(ISMM’09). ACM, 79–88.
[41]VaivaswathaNagarajandRGovindarajan.2013. Parallelflow-sensitivepointer
analysis by graph-rewriting.In Proceedings of the22nd International Conference
on Parallel Architectures and Compilation Techniques. IEEE, 19–28.
[42]DamienOcteau,PatrickMcDaniel,SomeshJha,AlexandreBartel,EricBodden,
JacquesKlein, andYves Le Traon. 2013. Effective inter-component communica-
tion mapping in android: An essential step towards holistic security analysis. In
Presented aspart of the22nd USENIX SecuritySymposium (USENIX Security’13).
USENIXAssociation, 543–558.
[43]Tarun Prabhu, Shreyas Ramalingam, Matthew Might, and Mary Hall. 2011.
EigenCFA: Accelerating flow analysis with GPUs. In Proceedings of the 38th
AnnualACMSIGPLAN-SIGACTSymposiumonPrinciplesofProgrammingLan-
guages (POPL’11). ACM, 511–522.
[44]Sandeep Putta and Rupesh Nasre. 2012. Parallel replication-based points-to
analysis. In International Conference on Compiler Construction (CC ’12). Springer,
61–80.
[45]ThomasReps,SusanHorwitz,andMoolySagiv.1995. Preciseinterprocedural
dataflowanalysisviagraphreachability.In Proceedingsofthe22ndACMSIGPLAN-
SIGACTSymposiumonPrinciplesofProgrammingLanguages (POPL’95).ACM,
49–61.
[46]ThomasReps,SusanHorwitz,MoolySagiv,andGenevieveRosay.1994. Speeding
up slicing. In Proceedings of the 2nd ACM SIGSOFT Symposium on Foundations of
Software Engineering (FSE ’94). ACM, 11–20.
[47]Noam Rinetzky, Mooly Sagiv, and Eran Yahav. 2005. Interprocedural shape
analysisforcutpoint-freeprograms.In InternationalStaticAnalysisSymposium.
Springer, 284–302.
[48]Jonathan Rodriguez and Ondřej Lhoták. 2011. Actor-based parallel dataflow
analysis. In International Conference on Compiler Construction (CC ’11). Springer,
179–197.
[49]AtanasRountev,MarianaSharp,andGuoqingXu.2008. IDEdataflowanalysis
inthepresenceoflargeobject-orientedlibraries.In InternationalConferenceon
CompilerConstruction (CC ’08). Springer, 53–68.
[50]MoolySagiv,ThomasReps,andSusanHorwitz.1996. Preciseinterprocedural
dataflowanalysiswithapplicationstoconstantpropagation. TheoreticalComputer
Science167,1 (1996), 131–170.
PipeliningBottom-upDataFlow Analysis ICSE’20,May23–29,2020,Seoul, Republic of Korea
[51]BernhardScholz,HerbertJordan,PavleSubotić,andTillWestmann.2016. Onfast
large-scale program analysis in datalog. In International Conference on Compiler
Construction (CC’16). ACM, 196–206.
[52]Jürgen Seib and Georg Lausen. 1991. Parallelizing Datalog programs by gen-
eralized pivoting. In Proceedings of the tenth ACM SIGACT-SIGMOD-SIGART
symposiumon Principles of database systems. ACM, 241–251.
[53]Marianne Shaw, Paraschos Koutris, Bill Howe, and Dan Suciu. 2012. Optimizing
large-scale Semi-Naïvedatalog evaluationin hadoop.In International Datalog2.0
Workshop. Springer, 165–176.
[54]QingkaiShi,XiaoXiao,RongxinWu,JinguoZhou,GangFan,andCharlesZhang.
2018. Pinpoint: Fast and precise sparse value flow analysis for million linesof code. In Proceedings of the 39th ACM SIGPLAN Conference on Programming
LanguageDesignandImplementation (PLDI’18) . ACM, 693–706.
[55]Sharon Shoham, Eran Yahav, Stephen J Fink, and Marco Pistoia. 2008. Staticspecificationminingusingautomata-basedabstractions. IEEETransactionson
Software Engineering 34,5 (2008), 651–666.
[56]Bjarne Steensgaard. 1996. Points-to analysis in almost linear time. In Proceedings
of the 23rd ACM SIGPLAN-SIGACT symposium on Principles of programming
languages. ACM, 32–41.
[57]Yu Su, Ding Ye, and Jingling Xue. 2014. Parallel pointer analysis with CFL-reachability. In 2014 43rd International Conference on Parallel Processing. IEEE,
451–460.
[58]Yulei Sui, Ding Ye, and Jingling Xue. 2014. Detecting memory leaks statically
withfull-sparsevalue-flowanalysis. IEEETransactionsonSoftwareEngineering
40,2 (2014), 107–122.
[59]Omer Tripp, Marco Pistoia, Patrick Cousot, Radhia Cousot, and SalvatoreGuarnieri. 2013. Andromeda: Accurate and scalable security analysis of web
applications. In International Conference on Fundamental Approaches to Software
Engineering. Springer, 210–225.
[60]KaiWang,AftabHussain,ZhiqiangZuo,GuoqingXu,andArdalanAmiriSani.
2017. Graspan:Asingle-machinedisk-basedgraphsystemforinterproceduralstatic analyses of large-scale systems code. ACM SIGOPS Operating Systems
Review51,2 (2017), 389–404.
[61]Ouri Wolfson and Aya Ozeri. 1990. A New Paradigm for Parallel and DistributedRule-processing.In Proceedingsofthe1990ACMSIGMODInternationalConference
on Management of Data (SIGMOD ’90). ACM, 133–142.
[62]OuriWolfsonandAviSilberschatz.1988. DistributedProcessingof LogicPro-
grams. In Proceedings of the 1988 ACM SIGMOD International Conference on
Managementof Data (SIGMOD ’88). ACM, 329–336.
[63]Yichen Xie and Alex Aiken. 2005. Context- and path-sensitive memory leak
detection.In Proceedingsofthe10thEuropeanSoftwareEngineeringConference
Held Jointly with 13th ACM SIGSOFT International Symposium on Foundations of
Software Engineering (ESEC/FSE ’05). ACM, 115–125.
[64]Yichen Xie and Alex Aiken. 2005. Scalable error detection using Boolean satisfia-
bility. InProceedings of the 32nd ACM SIGPLAN-SIGACT Symposium on Principles
of Programming Languages (POPL ’05). ACM, 351–363.
[65]HongseokYang,OuksehLee,JoshBerdine,CristianoCalcagno,ByronCook,Dino
Distefano, and Peter O’Hearn. 2008. Scalable shape analysis for systems code. In
InternationalConferenceon Computer Aided Verification . Springer, 385–398.
[66]Mohan Yang, Alexander Shkapsky, and Carlo Zaniolo. 2017. Scaling up theperformance of more powerful Datalog systems on multicore machines. The
VLDBJournal-TheInternationalJournalonVeryLargeDataBases 26,2(2017),
229–248.
[67]Greta Yorsh, Eran Yahav, and Satish Chandra. 2008. Generating precise and
concise procedure summaries. In Proceedings of the 35th Annual ACM SIGPLAN-
SIGACTSymposiumonPrinciplesofProgrammingLanguages (POPL’08).ACM,
221–234.
[68]Zhiqiang Zuo, John Thorpe, Yifei Wang, Qiuhong Pan, Shenming Lu, Kai Wang,
GuoqingHarry Xu,Linzhang Wang,and XuandongLi. 2019. Grapple: Agraph
systemforstaticfinite-statepropertycheckingoflarge-scalesystemscode.In
Proceedings of the Fourteenth EuroSys Conference 2019 (EuroSys ’19). ACM, 38.
