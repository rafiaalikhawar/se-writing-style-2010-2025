Semantic Test Repair for Web Applications
Xiaofang Qi
School of Computer Science and
Engineering, Southeast University
China
xfqi@seu.edu.cnXiang Qian
School of Computer Science and
Engineering, Southeast University
China
3165446389@qq.comYanhui Li
State Key Laboratory for Novel
Software Technology, Nanjing
University
China
yanhuili@nju.edu.cn
ABSTRACT
Automation testing is widely used in the functional testing of web
applications. However, during the evolution of web applications,
such web test scripts tend to break. It is essential to repair such
broken test scripts to make regression testing run successfully. As
manual repairing is time-consuming and expensive, researchers fo-
cus on automatic repairing techniques. Empirical study shows that
the web element locator is the leading cause of web test breakages.
Most existing repair techniques utilize Document Object Model
attributes or visual appearances of elements to find their location
but neglect their semantic information.
This paper proposes a novel semantic repair technique called
Sem antic TestRepair ( Semter ) for web test repair. Our approach
captures relevant semantic information from test executions on
the applicationâ€™s basic version and locates target elements by cal-
culating semantic similarity between elements to repair tests. Our
approach can also repair test workflow due to web page additions
or deletions by a local exploration in the updated version. We
evaluated the efficacy of our technique on six real-world web appli-
cations compared with three baselines. Experimental results show
thatSemter achieves an 84% average repair ratio within an accept-
able time cost, significantly outperforming the state-of-the-art web
test repair techniques.
CCS CONCEPTS
â€¢Software and its engineering â†’Software testing and debug-
ging .
KEYWORDS
Web Testing, Test Repair, GUI Testing, Semantic Similarity
ACM Reference Format:
Xiaofang Qi, Xiang Qian, and Yanhui Li. 2023. Semantic Test Repair for
Web Applications . In Proceedings of the 31st ACM Joint European Software
Engineering Conference and Symposium on the Foundations of Software Engi-
neering (ESEC/FSE â€™23), December 3â€“9, 2023, San Francisco, CA, USA. ACM,
New York, NY, USA, 13 pages. https://doi.org/10.1145/3611643.3616324
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
ESEC/FSE â€™23, December 3â€“9, 2023, San Francisco, CA, USA
Â©2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-0327-0/23/12. . . $15.00
https://doi.org/10.1145/3611643.36163241 INTRODUCTION
Web applications frequently evolve to meet the constantly chang-
ing needs of users [ 33]. Regression testing is conducted on the
updated versions to ensure that such evolution does not break the
functionality of the applications [ 38]. In such testing scenarios, soft-
ware engineers commonly employ record/replay test techniques
to automate the testing of web applications [ 34,38]. Typically, a
record/replay tool for web applications like Selenium test frame-
work captures and records manual operations (e.g., mouse clicks
and filling in forms) that testers conducted on Graphic User In-
terface (GUI) to generate a test script, which testers redeliver to
browsers for replaying and testing automatically.
Even though record/replay techniques improve the ability of
test reuse and automation dramatically, they suffer from the well-
known â€œtest fragilityâ€ problem [ 45]. A test script would usually
become fragile as the application evolves, e.g., a modification in
GUI as minor as adding or deleting a User Interface (UI) element
would make the test script break (i.e., test breakage1) [16]. Empir-
ical studies demonstrate at least one breakage occurs in 12% of
web application tests, and nearly 80% of these tests exhibit multiple
breakages within the same test [ 17,40]. Besides, the manual repair
for test breakage is time-consuming, expensive, and highly tedious
[40]. Currently, web developers tend to release new versions fre-
quently to attract more users, implying a shorter time available for
test repairs and regression testing. In this context, automated test
repair technique is urgently demanded to ensure the high quality
of web applications.
Researchers have aimed to analyze potential causes of test break-
ages, including UI element locator, value/action exception, and page
reloading. Their empirical studies [ 17,23,24] show that the UI el-
ement locator is the leading cause. As a result, researchers have
paid much attention to improving the accuracy of UI element loca-
tors to repair broken test scripts [ 16,18,22,25]. Most existing web
test repair techniques essentially leverage Document Object Model
(DOM) attributes [ 11] and visual appearances [ 40] of UI elements as
critical factors to match appropriate UI elements. The assumption
behind these repair techniques is that UI elementsâ€™ DOM attributes
and visual appearances are relatively stable, i.e., unchanged or only
slightly changed as a web application evolves. Unfortunately, this
assumption would be unrealistic in some real-world cases. Fig. 1
illustrates the change of UI elements between two versions of Claro-
line2, where two UI elements (My User Account on Claroline 1.10.7
1A test breakage indicates that a test case that functions well on the previous web
application version is no longer applicable to its updated version, not due to bugs but
rather due to a change that makes the test function incorrectly.
2Claroline is one of the six web applications studied in our paper, whose detailed
information will be presented in Section 4.1.
1190
ESEC/FSE â€™23, December 3â€“9, 2023, San Francisco, CA, USA Xiaofang Qi, Xiang Qian, and Yanhui Li
(a) A snapshot with UI element My User Account on Claroline 1.10.7
(b) A snapshot with UI element Manage my account on Claroline 1.11.9
Figure 1: An example of two Claroline versions to show how
UI elements change as a web app evolves. Two elements re-
quired to be matched are highlighted with dashed red boxes.
and Manage my account on Claroline 1.11.9 with dashed red boxes)
are required to be matched when fixing test breakages. From Fig. 1,
we have the following two observations:
â€¢DOM attributes are insufficient to match these two elements .
The structure of DOM trees on current web pages has greatly
changed between the two versions, which causes the DOM
attributes (e.g., linkText and XPath) of the latter to be utterly
distant from that of the former.
â€¢Visual appearances are too different to match these two el-
ements. Fig. 1 shows the obvious differences between the
visual appearances of these two UI elements, e.g., the text
content, text fonts, and background colors are all changed.
Consequently, methods based on visual appearances would
underestimate the similarity between these two elements
and fail to match them.
This example reveals the limitations of current techniques and
drives us to rethink how testers could match these two elements.
For testers, the text content of these two elements contains se-
mantic information: their text â€œMy User Accountâ€ and â€œManage
my accountâ€ indicate that these two elements are highly related
to operations (e.g., manage, modify, and search) on the current
user account. Previous studies have shown that UI elements carry
rich semantic information (e.g., textual descriptions and icons),
which greatly facilitate human understanding of their functionali-
ties [ 8,10,26,29,31,36,41]. The above observations indicate that
semantic information would be helpful for achieving more accurate
UI element locators.
In this paper, we propose a novel approach called Sem antic Test
Repair ( Semter ) to repairing tests for web applications. The key
insight of Semter is that it simulates human repair behaviors andenables automated regression testing by semantic matching tech-
niques. Specifically, Semter initially runs a test on the base version
of a web application and collects relevant semantic information
of UI elements and other requisite information as the reference of
the test execution. Then, when the test is replayed on the updated
version, it utilizes collected semantic information to decide which
UI elements from the base version have been changed and identify
possible counterpart UI elements from the updated version. Semter
captures semantic meanings contained in texts and images of UI
elements, i.e., computes the semantic similarity by a hybrid deep
learning-based semantic model, which transforms both text and
image information of UI elements into the semantic information
in a unified framework. Furthermore, the contexts of UI elements
are also incorporated to augment the semantic information of UI
elements. Besides, Semter adopts the local exploration algorithm in
Vista [40], to handle the test breakage scenarios due to the broken
workflow of test execution, e.g., the target UI elements are not on
the same page but removed or moved to the neighboring web page.
The evaluation of our approach Semter is conducted on six
real-world and open-source web applications. We compare Semter
with three representative web test repair methods, namely Water
[11],Vista [40], and WebEvo [39]. The results corroborate the
effectiveness of Semter .
This paper makes the following contributions:
â€¢Strategy. We propose a novel semantic test repair approach
Semter for web applications, which leverages semantic in-
formation of UI elements to yield effective repairs.
â€¢Implementation. We implement a prototype tool (also
called Semter , https://github.com/Reoke/SEMTER) to sup-
port the application of our semantic test repair approach.
â€¢Study. We empirically evaluate the efficacy of Semter on
six real-world web applications. Results demonstrate that
Semter achieves an 84% average repair ratio within an ac-
ceptable time, significantly surpassing the three state-of-the-
art web test repair techniques.
2 BACKGROUND AND MOTIVATION
EXAMPLE
This section introduces motivation examples and the background
of our studies, including web testing, test breakage, and semantics
required for test repair.
2.1 Web Testing and Test Breakage
A web test script (i.e., a test case) that runs on a given web app is
a set of statements {ğ‘ ğ‘¡1,ğ‘ ğ‘¡2,...,ğ‘ ğ‘¡ ğ‘›}, whereğ‘ ğ‘¡ğ‘–=âŸ¨ğ‘’ğ‘–,ğ‘œğ‘–âŸ©(1â‰¤ğ‘–â‰¤ğ‘›)
describes an UI element event containing the UI element ğ‘’ğ‘–and the
operationğ‘œğ‘–performed on ğ‘’ğ‘–. Fig. 2(a) and 2(c) shows the UI element
event sequence of a test on the basic version 1.10.7 of Claroline
and the accompanying test script tcrespectively. As can be seen in
Fig. 2(c),ğ‘¡ğ‘has 8 statements ( ğ‘ ğ‘¡1,...,ğ‘ ğ‘¡ 8), which run successfully on
the version 1.10.7. To illustrate, ğ‘ ğ‘¡3indicates a successful operation
(â€œclickâ€) on an element (the â€œEnterâ€ button).
As mentioned in Section 1, test scripts are brittle, and test break-
ages occur easily as the app evolves. From a repair-oriented per-
spective, researchers categorized locator-related test breakages into
the following classes [40]:
1191Semantic Test Repair for Web Applications ESEC/FSE â€™23, December 3â€“9, 2023, San Francisco, CA, USA
2
4
(a) Version 3.0
2
6
3
7
1
5
82
3
64
(a) UI elements on version 1.10.7 of Claroline
7 8
1
2
3
4
6
5 (b) UI elements on version 1.11.9 of Claroline
1
2
3
4
5
6
7 8
78
1
2
3
4
5
6
(a) Version 3.0(a) Version 4.0
1. driver .findElement (By.id("login ")).sendKeys ("Smith ");
2. driver .findElement (By.id("password ")).sendKeys ("pass");
3. driver .findElement (By.name ("submitAuth ")).click ();   ï‚´
4. driver .findElement (By.linkText ("My User Account ")).click ();  ï‚´  
5. driver .findElement (By.linkText ("View my statistics ")).click ();
6. new Select (driver .findElement (By.id("cidReq "))).selectByVisibleText ("Math ");
7. assertEquals ("Exercise 1", driver .findElement (By.xpath (".//*[ @id='leftContent ']/div[3]/div[2]/table /
tbody /tr[1]/td[1]/a")).getText ());  ï‚´
8. assertEquals ("9", driver .findElement (By.xpath (".//*[ @id='leftContent ']/div[3]/div[2]/table /tbody /tr[1]/
td[3]")).getText ());  ï‚´1. driver .findElement (By.id("login ")).sendKeys ("Smith ");
2. driver .findElement (By.id("password ")).sendKeys ("pass");
3. driver .findElement (By.cssSelector ("button [type='submit ']")).click ();  ïƒ– 
4. driver .findElement (By.linkText ("Manage my account ")).click (); ïƒ– 
5. driver .findElement (By.linkText ("View my statistics ")).click ();
6. new Select (driver .findElement (By.id("cidReq "))).selectByVisibleText ("Math ");
7. assertEquals ("Exercise 1", driver .findElement (By.xpath (".//*[ @id='leftContent ']/div[4]/div[1]/table /
tbody /tr[1]/td[1]/a")).getText ());  ïƒ–  
8. assertEquals ("9", driver .findElement (By.xpath (".//*[ @id='leftContent ']/div[4]/div[1]/table /tbody /tr[1]/
td[3]")).getText ());  ïƒ– 
(c) A successful test script ğ‘¡ğ‘containing 8 statements on Version 1.10.7 with four test
breakages (highlighted by Ã—) on the updated version 1.11.9
1
2
3
4
5
6
7 8
78
1
2
3
4
5
6
(a) Version 3.0(a) Version 4.0
1. driver .findElement (By.id("login ")).sendKeys ("Smith ");
2. driver .findElement (By.id("password ")).sendKeys ("pass");
3. driver .findElement (By.name ("submitAuth ")).click ();   ï‚´
4. driver .findElement (By.linkText ("My User Account ")).click ();  ï‚´  
5. driver .findElement (By.linkText ("View my statistics ")).click ();
6. new Select (driver .findElement (By.id("cidReq "))).selectByVisibleText ("Math ");
7. assertEquals ("Exercise 1", driver .findElement (By.xpath (".//*[ @id='leftContent ']/div[3]/div[2]/table /
tbody /tr[1]/td[1]/a")).getText ());  ï‚´
8. assertEquals ("9", driver .findElement (By.xpath (".//*[ @id='leftContent ']/div[3]/div[2]/table /tbody /tr[1]/
td[3]")).getText ());  ï‚´1. driver .findElement (By.id("login ")).sendKeys ("Smith ");
2. driver .findElement (By.id("password ")).sendKeys ("pass");
3. driver .findElement (By.cssSelector ("button [type='submit ']")).click ();  ïƒ– 
4. driver .findElement (By.linkText ("Manage my account ")).click (); ïƒ– 
5. driver .findElement (By.linkText ("View my statistics ")).click ();
6. new Select (driver .findElement (By.id("cidReq "))).selectByVisibleText ("Math ");
7. assertEquals ("Exercise 1", driver .findElement (By.xpath (".//*[ @id='leftContent ']/div[4]/div[1]/table /
tbody /tr[1]/td[1]/a")).getText ());  ïƒ–  
8. assertEquals ("9", driver .findElement (By.xpath (".//*[ @id='leftContent ']/div[4]/div[1]/table /tbody /tr[1]/
td[3]")).getText ());  ïƒ– (d) A revised and successful test script ğ‘¡ğ‘â€²containing 8 statements on version 1.11.9
with changes (highlighted by âœ“) from ğ‘¡ğ‘
Figure 2: An example of test breakages occurring as web apps evolve and the need for semantics during test breakage repair.
Non-SelectionÂ·Same Page (NSSP). When a test script state-
ment is executed, no UI element is found and returned. However,
the target UI element is present on the current page (i.e., where the
test stops).
Non-SelectionÂ·Neighbouring Pages (NSNP). The target ele-
ment is not found on the current page, but it is on its neighboring
page (e.g., due to the insertion of a web page in the updated version).
Non-SelectionÂ·Removed (NSR). The target element is not
found since it is removed from the page.
Mis-SelectionÂ·Directed and Propagated (MSDP). When a
test script statement is executed, an incorrect UI element is selected
as the target element and returned. This case tends to cause a test
execution different from the intended behavior.
As shown in Fig. 2(b), in the subsequent version 1.11.9 of Claro-
line, changes come to pages, e.g., the text of the â€œMy User Accountâ€
menu item (marked with â‘£) is changed to â€œManage my accountâ€,
and the background color of the â€œEnterâ€ (marked with â‘¢) is also
changed. When the test case tcis executed on version 1.10.7, it
breaks at Lines 3, 4, 7, and 8. Based on the above classification, the
manual check shows that all test breakages in Fig. 2(c) are NSSPs
due to significant changes in the DOM attributes of the UI elements.2.2 Semantics Required for Test Repair
The breakage at Line 4 (see Fig. 2(c)) on Version 1.11.9 shows that
semantic information is required to locate target UI elements accu-
rately in test repair. Comparing the source and target elements, i.e.,
â€œMy User Accountâ€ menu item marked with â‘£on version 1.10.7 in
Fig. 2(a) and â€œManage my accountâ€ menu item on version 1.11.9
in Fig. 2(b), we observe that large changes in DOM attributes and
visual appearances occur. Such changes cause methods based on
DOM attributes [ 11], visual appearances [ 39,40] to fail to repair
this breakage.
In addition, the semantic information about surrounding ele-
ments would help locate the UI elements. As shown in Fig. 3, there
are two similar input boxes (marked with â‘ andâ‘¡on two versions),
which are hard to differentiate based on their local information. We
find that the text elements in the right hand of source and target
input boxes marked with â‘ /â‘¡, namely â€œAdd Categoryâ€/â€œAdd Ver-
sionâ€, identical on the two versions regardless of case, supplement
the semantic information and strongly indicate that these two input
boxes are matched on the two versions.
1192ESEC/FSE â€™23, December 3â€“9, 2023, San Francisco, CA, USA Xiaofang Qi, Xiang Qian, and Yanhui Li
(a) A page snapshot with the input boxes â‘ andâ‘¡on MantisBT 1.2.0
(b) A page snapshot with the input boxes â‘ andâ‘¡on MantisBT 1.3.0
Figure 3: An example to show the requirement for the seman-
tic information supplemented by surrounding elements.
3 APPROACH
Fig. 4 provides an overview of our Semter approach with three
main modules, i.e., Semantic Test Tracer ,Test Repair , and Semantic
Similarity Calculator . Given a base version ğ‘‰of a web app, a test
caseğ‘¡ğ‘that runs successfully on ğ‘‰, and an updated version ğ‘‰â€²,
Semter first runsğ‘¡ğ‘onğ‘‰, records its execution trace, and collects
a variety of semantic information (e.g., text) by the Semantic Test
Tracer module; then it runs ğ‘¡ğ‘onğ‘‰â€², and the previous trace is
utilized to validate the correctness of each test script statement.
Once a test breakage occurs, the Test Repair module attempts to
exploit the previously generated semantic information and the
page information to find the possible target UI element by calling
Semantic Similarity Calculator and then fix the breakage. Afterward,
the next test statement continues its execution. Each statement in ğ‘¡ğ‘
proceeds sequentially until all statements in ğ‘¡ğ‘are executed. Finally,
Semter outputs a repaired test case ğ‘¡ğ‘â€²that runs successfully on ğ‘‰â€²,
along with report information. The tester can analyze the report
and manually check whether the repairs for breakages are correct
or not.
3.1 Semantic Test Tracer
For a test case ğ‘¡ğ‘,Semantic Test Tracer runsğ‘¡ğ‘on the basic version
ğ‘‰, records the execution trace of ğ‘¡ğ‘, and generates a semantic-
augmented test case, which contains semantic information of the
UI elements that are executed by ğ‘¡ğ‘. As described in Section 2.1,
we formulate a test case ğ‘¡ğ‘as a set of UI element events with their
operations{âŸ¨ğ‘’1,ğ‘œ1âŸ©,âŸ¨ğ‘’2,ğ‘œ2âŸ©,...,âŸ¨ğ‘’ğ‘›,ğ‘œğ‘›âŸ©}, whereğ‘’ğ‘–denotes a UI
element (e.g., a menu item), and ğ‘œğ‘–is the operation performed on ğ‘’ğ‘–.
As shown in Fig. 2(c), the operation can be a simple action like click
or a method call with arguments, such as sendKeys (â€œpassâ€ ) on Line
Figure 4: Overview of Semter
2. Besides, an oracle is also taken as a special type of event where
the operation is an assertion, e.g., assertEquals()on Line 8. For
eachâŸ¨ğ‘’ğ‘–,ğ‘œğ‘–âŸ©,Semantic Test Tracer analyzes the recorded trace and
retrieves semantic information of ğ‘’ğ‘–, which consists of the following
two kinds of semantic information.
Theindividual semantic information of ğ‘’ğ‘–(denoted as ğ‘’ğ‘–.ğ‘–ğ‘ )
is extracted entirely from ğ‘’ğ‘–itself, which consists of its text and
image, denoted as ğ‘’ğ‘–.ğ‘¡ğ‘’ğ‘¥ğ‘¡ andğ‘’ğ‘–.ğ‘–ğ‘šğ‘ğ‘”ğ‘’ , respectively. Considering
the fact that the semantic information of a UI element that a human
captures mostly depends on visible texts or images rather than those
invisible DOM attributes while he observes it, we extract only the
text contents that are displayed on the page. Specifically, Semter
retrieves the visible text contents of ğ‘’ğ‘–.ğ‘¡ğ‘’ğ‘¥ğ‘¡ by calling Selenium
methodğ‘¤ğ‘’ğ‘ğ¸ğ‘™ğ‘’ğ‘šğ‘’ğ‘›ğ‘¡. getText(), or extracting the text information
fromğ‘¡ğ‘’ğ‘¥ğ‘¡ğ¶ğ‘œğ‘›ğ‘¡ğ‘’ğ‘›ğ‘¡ attribute orğ‘£ğ‘ğ‘™ğ‘¢ğ‘’ attribute of a form, etc. Images
are captured from the screenshot on the page. If no visible text
content/image is retrieved, ğ‘’ğ‘–.ğ‘¡ğ‘’ğ‘¥ğ‘¡ /ğ‘’ğ‘–.ğ‘–ğ‘šğ‘ğ‘”ğ‘’ isğ‘›ğ‘¢ğ‘™ğ‘™.
Thecontextual semantic information of ğ‘’ğ‘–(denoted as ğ‘’ğ‘–.ğ‘ğ‘ )
is extracted from the context of ğ‘’ğ‘–, i.e.,ğ‘’ğ‘–.ğ‘ğ‘ is a set of UI elements
surrounding ğ‘’ğ‘–. As UI elements in ğ‘’ğ‘–.ğ‘ğ‘ tend to be close to ğ‘’ğ‘–in
the DOM tree, Semantic Test Tracer retrievesğ‘’ğ‘–.ğ‘ğ‘ by analyzing the
DOM tree. It first visits the node ğ‘’ğ‘–and selects all text-contained
UI elements in the parent of ğ‘’ğ‘–and the descendants3of the siblings
ofğ‘’ğ‘–as candidate elements. If no candidate element is captured,
it recursively ascends to visit the parent of the node just visited
and again selects candidate elements in its parent and descendants
of its siblings, the process continues until candidate elements are
captured, or the root (i.e., the element with XPath: html/body ) is
reached. Here, text-contained UI elements are referred to as ele-
ments on the leaf nodes or those whose texts are not ğ‘›ğ‘¢ğ‘™ğ‘™, but the
texts of all their descendants (not including themselves) are ğ‘›ğ‘¢ğ‘™ğ‘™.
Eventually, those candidate elements that are far from ğ‘’ğ‘–on the page
are filtered. Specifically, ğ‘’ğ‘–.ğ‘ğ‘ only contains those UI elements that
have coordinate overlap with ğ‘’ğ‘–or are closest in terms of Euclidean
distance toğ‘’ğ‘–in the up, down, left, and right directions.
3.2 Test Repair
Algorithm 1 illustrates how the test repair module works. It takes a
test caseğ‘¡ğ‘that runs successfully on the basic version ğ‘‰and the
updated version ğ‘‰â€²as inputs, and outputs the repaired test case ğ‘¡ğ‘â€²
that works well on ğ‘‰â€²and the relevant repair actions ğ‘Ÿğ‘.
3Here, we consider that the descendants of a node include the node itself.
1193Semantic Test Repair for Web Applications ESEC/FSE â€™23, December 3â€“9, 2023, San Francisco, CA, USA
The algorithm starts by initializing ğ‘¡ğ‘â€²andğ‘Ÿğ‘, and opening a
web driver to load ğ‘‰â€²(Lines 1â€“2). For each test script statement ğ‘ ğ‘¡ğ‘–
inğ‘¡ğ‘, it first retrieves the UI element ğ‘’executed by ğ‘ ğ‘¡ğ‘–onğ‘‰, the
operationğ‘œonğ‘’, and the DOM-based locator ğ‘™used byğ‘ ğ‘¡ğ‘–, then
searches for a candidate target element ğ‘’â€²on the current web page
by calling searchElement()function (Lines 4â€“7). If it is found, ğ‘’is
substituted by ğ‘’â€², and the corresponding repair action is added to
ğ‘Ÿğ‘(Lines 8â€“9). Otherwise, the algorithm attempts to find it on the
neighbouring pages (Lines 11â€“21). The event ğ‘’ğ‘£that is clickable
on the current page ğ‘ is triggered to reach the neighbouring page
(function localExplore()in Line 13) to find the element. Once a
candidate target element ğ‘’â€²is found on some neighbouring page, the
elementğ‘’ğ‘£and the click operation that triggers the page transition,
and the found element ğ‘’â€²and its operation are added to ğ‘Ÿğ‘, and the
test continues (Line 16â€“18). If no candidate element is found on
any neighbouring page, our algorithm considers the target element
asğ‘Ÿğ‘’ğ‘šğ‘œğ‘£ğ‘’ğ‘‘ fromğ‘‰â€². Then the repair action of removal is added
toğ‘Ÿğ‘(Lines 20â€“21). Considering that an exhaustive exploration
often causes a state explosion, our algorithm adopts the same local
exploration strategy as Vista , namely the exploration is limited
to one step from the current page. Specifically, only the clickable
events on the current page ğ‘ are selected and triggered to reach
the neighbouring page (Lines 12). During the local exploration,
when not finding the target element on some neighbouring page,
our algorithm would backtrack to the current page ğ‘ (Line 19).
Before executing the next test script statement, the tester checks
the correctness of the repair actions in ğ‘Ÿğ‘onğ‘ ğ‘¡ğ‘–(Line 22). If the
automatic repair is inappropriate, it is substituted by a manual
fix. Subsequently, the repaired statement ğ‘ ğ‘¡ğ‘–is executed (Line 23),
and the repair process proceeds. Finally, a repaired test case ğ‘¡ğ‘â€²is
generated by performing the repair actions in ğ‘Ÿğ‘onğ‘¡ğ‘(Line 24).
The function searchElement attempts to search for a candidate
target element on the current page. It takes a web driver ğ‘‘ğ‘Ÿğ‘–ğ‘£ğ‘’ğ‘Ÿ , a UI
elementğ‘’, and its DOM-based locator ğ‘™inğ‘‰as the input and returns
a UI element in ğ‘‰â€²that matches ğ‘’semantically. It first utilizes the
original locator ğ‘™to look up the DOM of ğ‘‰â€²to return a UI element
ğ‘’â€²(Line 27). If ğ‘’â€²is notğ‘›ğ‘¢ğ‘™ğ‘™, and the semantic similarity between
ğ‘’andğ‘’â€²(ğ‘ ğ‘–ğ‘š(ğ‘’,ğ‘’â€²)) is greater than or equal to a threshold ğ‘¡,ğ‘’â€²is
returned as the target element (Lines 28â€“29). Otherwise, i.e., ğ‘’â€²is
ğ‘›ğ‘¢ğ‘™ğ‘™ orğ‘ ğ‘–ğ‘š(ğ‘’,ğ‘’â€²)is less thanğ‘¡, the function tries to find the element
ğ‘’ğ‘šğ‘ğ‘¥ as the candidate target element on the current page (Lines
30â€“34).ğ‘’ğ‘šğ‘ğ‘¥ has the maximum semantic similarity with ğ‘’and the
value is greater than or equal to ğ‘¡. Note thatğ‘’ğ‘šğ‘ğ‘¥ could beğ‘›ğ‘¢ğ‘™ğ‘™,
which implies that no target element is returned. To improve the
search efficiency, in the first step (Lines 28â€“29), the function does
not search for ğ‘’ğ‘šğ‘ğ‘¥ but uses a semantic similarity threshold ğ‘¡for
sifting out those impossible candidate elements. In this work, the
thresholdğ‘¡is set to 0.6(detailed explanation will be provided in
Section 5).
3.3 Semantic Similarity Calculator
The two motivating examples (see Figures 2 and 3) show that com-
prehensive semantic information, namely combining individual
and contextual semantic information, is required and useful to lo-
cate target elements. Thus, in our approach, the semantic similarity
between two UI elements is calculated from the individual andAlgorithm 1: Semantic Test Repair
Input : ğ‘¡ğ‘={ğ‘ ğ‘¡1, ğ‘ ğ‘¡2, ..., ğ‘ ğ‘¡ ğ‘–, ..., ğ‘ ğ‘¡ ğ‘›}: a test case that runs successfully on
the basic version ğ‘‰,ğ‘ˆ: the URL of the updated version ğ‘‰â€².
Output: ğ‘¡ğ‘â€²: A repaired test case that works on ğ‘‰â€²,ğ‘Ÿğ‘: a set of repair actions.
1ğ‘¡ğ‘â€²â†âˆ… ,ğ‘Ÿğ‘â†âˆ…
2ğ‘‘ğ‘Ÿğ‘–ğ‘£ğ‘’ğ‘Ÿâ†loadApp(ğ‘ˆ)
3forğ‘–â†1toğ‘›do
4 ğ‘’â†getElement(ğ‘ ğ‘¡ğ‘–)
5 ğ‘œâ†getAction(ğ‘ ğ‘¡ğ‘–)
6 ğ‘™â†getLocator(ğ‘ ğ‘¡ğ‘–)
7 ğ‘’â€²â†searchElement(ğ‘‘ğ‘Ÿğ‘–ğ‘£ğ‘’ğ‘Ÿ, ğ‘’,ğ‘™)
8 ifğ‘’â€²â‰ ğ‘›ğ‘¢ğ‘™ğ‘™ then
9 ğ‘Ÿğ‘.add(ğ‘ ğ‘¡ğ‘–,ğ‘¢ğ‘ğ‘‘ğ‘ğ‘¡ğ‘’,âŸ¨ğ‘’â€², ğ‘œâŸ©)
10 else
11 ğ‘ â†ğ‘‘ğ‘Ÿğ‘–ğ‘£ğ‘’ğ‘Ÿ. getState()
12 forğ‘’ğ‘£âˆˆğ‘ .getClickable()do
13 ğ‘ .localExplore(âŸ¨ğ‘’ğ‘£, ğ‘ğ‘™ğ‘–ğ‘ğ‘˜âŸ©)
14 ğ‘’â€²â†searchElement(ğ‘‘ğ‘Ÿğ‘–ğ‘£ğ‘’ğ‘Ÿ, ğ‘’,ğ‘™)
15 ifğ‘’â€²â‰ ğ‘›ğ‘¢ğ‘™ğ‘™ then
16 ğ‘Ÿğ‘.add(ğ‘ ğ‘¡ğ‘–, ğ‘–ğ‘›ğ‘ ğ‘’ğ‘Ÿğ‘¡,âŸ¨ğ‘’ğ‘£, ğ‘ğ‘™ğ‘–ğ‘ğ‘˜âŸ©)
17 ğ‘Ÿğ‘.add(ğ‘ ğ‘¡ğ‘–,ğ‘¢ğ‘ğ‘‘ğ‘ğ‘¡ğ‘’,âŸ¨ğ‘’â€², ğ‘œâŸ©)
18 break
19 ğ‘‘ğ‘Ÿğ‘–ğ‘£ğ‘’ğ‘Ÿ. updateState(ğ‘ )
20 ifğ‘’â€²=ğ‘›ğ‘¢ğ‘™ğ‘™ then
21 ğ‘Ÿğ‘.add(ğ‘ ğ‘¡ğ‘–, ğ‘Ÿğ‘’ğ‘šğ‘œğ‘£ğ‘’, ğ‘›ğ‘¢ğ‘™ğ‘™)
22 checkRepair(ğ‘ ğ‘¡ğ‘–, ğ‘Ÿğ‘)
23 ğ‘‘ğ‘Ÿğ‘–ğ‘£ğ‘’ğ‘Ÿâ†executeUpdate(ğ‘ ğ‘¡ğ‘–,ğ‘‰â€²)
24ğ‘¡ğ‘â€²â†performRepair(ğ‘¡ğ‘, ğ‘Ÿğ‘)
25return ğ‘¡ğ‘â€²,ğ‘Ÿğ‘
26function searchElement( ğ‘‘ğ‘Ÿğ‘–ğ‘£ğ‘’ğ‘Ÿ ,ğ‘’,ğ‘™):
27 ğ‘’â€²â†ğ‘‘ğ‘Ÿğ‘–ğ‘£ğ‘’ğ‘Ÿ . findElement(ğ‘™)
28 ifğ‘’â€²â‰ ğ‘›ğ‘¢ğ‘™ğ‘™âˆ§ğ‘ ğ‘–ğ‘š(ğ‘’, ğ‘’â€²)â‰¥ğ‘¡then
29 return ğ‘’â€²
30 ğ‘’ğ‘šğ‘ğ‘¥â†ğ‘›ğ‘¢ğ‘™ğ‘™ ,ğ‘ ğ‘–ğ‘š ğ‘šğ‘ğ‘¥â†0
31 foreach ğ‘’â€²âˆˆğ‘‘ğ‘Ÿğ‘–ğ‘£ğ‘’ğ‘Ÿ . getElement()do
32 ifğ‘ ğ‘–ğ‘š(ğ‘’, ğ‘’â€²)>ğ‘ ğ‘–ğ‘š ğ‘šğ‘ğ‘¥âˆ§ğ‘ ğ‘–ğ‘š(ğ‘’, ğ‘’â€²)â‰¥ğ‘¡then
33 ğ‘ ğ‘–ğ‘š ğ‘šğ‘ğ‘¥â†ğ‘ ğ‘–ğ‘š(ğ‘’, ğ‘’â€²)
34 ğ‘’ğ‘šğ‘ğ‘¥â†ğ‘’â€²
35 return ğ‘’ğ‘šğ‘ğ‘¥
contextual perspectives. Given two UI elements ğ‘’andğ‘’â€²,Similarity
Calculator calculates the comprehensive semantic similarity be-
tweenğ‘’andğ‘’â€², namelyğ‘ ğ‘–ğ‘š(ğ‘’,ğ‘’â€²). In Algorithm 1, ğ‘ ğ‘–ğ‘š(ğ‘’,ğ‘’â€²)is the
crucial criterion for selecting the target element. In our approach,
it is calculated as:
ğ‘ ğ‘–ğ‘š(ğ‘’,ğ‘’â€²)=ğ‘ ğ‘–ğ‘šğ‘–(ğ‘’,ğ‘’â€²)+ğ›¼Ã—ğ‘ ğ‘–ğ‘šğ‘(ğ‘’,ğ‘’â€²) (1)
whereğ‘ ğ‘–ğ‘šğ‘–(ğ‘’,ğ‘’â€²)andğ‘ ğ‘–ğ‘šğ‘(ğ‘’,ğ‘’â€²)represent the individual semantic
similarity and the contextual semantic similarity between ğ‘’andğ‘’â€²,
respectively, and ğ›¼is a weighting parameter (set to 0.75, which will
be discussed in Section 4).
3.3.1 Individul Semantic Similarity. Specifically, the individual se-
mantic similarity ğ‘ ğ‘–ğ‘šğ‘–(ğ‘’,ğ‘’â€²)betweenğ‘’andğ‘’â€²is calculated based
on the individual semantic information of ğ‘’andğ‘’â€², denoted as ğ‘’.ğ‘–ğ‘ 
andğ‘’â€².ğ‘–ğ‘ respectively. Considering that ğ‘’.ğ‘–ğ‘ consists ofğ‘’.ğ‘¡ğ‘’ğ‘¥ğ‘¡ and
ğ‘’.ğ‘–ğ‘šğ‘ğ‘”ğ‘’ , our semantic model provides a hybrid semantic similarity
1194ESEC/FSE â€™23, December 3â€“9, 2023, San Francisco, CA, USA Xiaofang Qi, Xiang Qian, and Yanhui Li
calculation between text&text, text&image, or image&image:
ğ‘ğ‘Ÿğ‘–(ğ‘¥.ğ‘–ğ‘ )=(
ğ‘¥.ğ‘¡ğ‘’ğ‘¥ğ‘¡ ifğ‘¥.ğ‘¡ğ‘’ğ‘¥ğ‘¡ is not null,
ğ‘¥.ğ‘–ğ‘šğ‘ğ‘”ğ‘’ ifğ‘¥.ğ‘¡ğ‘’ğ‘¥ğ‘¡ is null.
ğ‘ ğ‘–ğ‘šğ‘–(ğ‘’,ğ‘’â€²)=Mğ‘ (ğ‘ğ‘Ÿğ‘–(ğ‘’.ğ‘–ğ‘ ),ğ‘ğ‘Ÿğ‘–(ğ‘’â€².ğ‘–ğ‘ ))
whereğ‘ğ‘Ÿğ‘–(ğ‘¥.ğ‘–ğ‘ )indicates the prioritized one in individual seman-
tic information (text has a higher priority over image), and Mğ‘ ()
presents the running results of our semantic model.
Our semantic model consists of two parts: an NLP model and an
image model. They take texts and images as inputs, respectively,
and output their semantic encodings.
(a) We have chosen Sentence-BERT [ 37] as our NLP model, which
is the modification of the pretrained BERT network [ 13] that uses
siamese and triplet network structures to derive sentence embed-
dings with dimensions of 384. It handles semantic information at the
sentence level rather than the word level (e.g., Word2vec), making it
have a strong capability of natural language processing. Compared
with BERT, the model has high efficiency in calculating semantic
similarity while maintaining accuracy.
(b) We have chosen ResNet-50 [ 19] as our image model and
revised it to produce encodings of the same dimension (i.e., 384) as
encodings produced by the NLP model. ResNet-50 has demonstrated
excellent performance on large-scale image classification tasks and
often outperforms previous state-of-the-art architectures. It has
become a widely-used and preferred choice as a backbone network
in many image processing applications. We have used 80% of the
data for training, 10% for early stopping, and the remainder as the
test set to determine the value of threshold ğ‘¡. The model has been
trained in such a way that it maps images to text encoding space
specified by the NLP model. More specifically, the goal is to make
the semantic similarity between icons and the corresponding labels
as high as possible while the semantic similarity between icons and
the corresponding negatively sampled labels as low as possible on
average.
(c) As mentioned above, the NLP and image models output 384-
dimensional encodings. The semantic similarity between texts and
texts, images and images, or texts and images is measured by the co-
sine similarity of their semantic encodings. Due to space limitation,
the details of our semantic model are presented in the README.md
file at https://github.com/Reoke/SEMTER.
To illustrate, consider the menu item (with the text â€œMy User
Accountâ€) marked with â‘£in Fig. 2(a) on version 1.10.7 (denoted as
ğ‘’) and the counterpart one with the text â€œManage My Accountâ€ on
version 1.11.9 in Fig. 2(b) (denoted as ğ‘’â€²) from the motivating exam-
ple. Using the individual texts of ğ‘’andğ‘’â€², i.e., â€œMy User Accountâ€
and â€œManage My Accountâ€, the individual similarity between ğ‘’and
ğ‘’â€²is computed as follows:
ğ‘ ğ‘–ğ‘šğ‘–(ğ‘’,ğ‘’â€²)=Mğ‘ (â€œMy User Accountâ€ ,â€œManage My Accountâ€ )=0.65
3.3.2 Contextual Semantic Similarity. The contextual semantic sim-
ilarity between ğ‘’andğ‘’â€²is calculated based on the contextual se-
mantic information of elements. It is formulated as follows:
ğ‘ ğ‘–ğ‘šğ‘(ğ‘’,ğ‘’â€²)=âˆ‘ï¸
ğ‘ğ‘˜âˆˆğ‘’.ğ‘ğ‘ max((ğ‘ ğ‘–ğ‘šğ‘–(ğ‘ğ‘˜,ğ‘â€²
ğ‘—)âˆ’ğ‘¡),0)
whereğ‘â€²
ğ‘—is the best matched element of ğ‘ğ‘˜inğ‘’â€².ğ‘ğ‘ , andğ‘¡is the
semantic similarity threshold. In detail, the matching procedure ontwo contexts of ğ‘’andğ‘’â€²is conducted as follows: (a) pick the pair
âŸ¨ğ‘ğ‘˜,ğ‘â€²
ğ‘—âŸ©with the max similarity, where ğ‘ğ‘˜âˆˆğ‘’.ğ‘ğ‘ andğ‘â€²
ğ‘—âˆˆğ‘’â€².ğ‘ğ‘ , and
considerğ‘â€²
ğ‘—the best-matched element of ğ‘ğ‘˜; (b) remove ğ‘ğ‘˜andğ‘â€²
ğ‘—
from the two sets and goto (a) until ğ‘’.ğ‘ğ‘ =âˆ…orğ‘’â€².ğ‘ğ‘ =âˆ….
To illustrate, we also consider the matching between â€œMy User
Accountâ€ (ğ‘’) and â€œManage My Accountâ€ ( ğ‘’â€²). The context of ğ‘’has
two UI elements labelled with â€œMy Desktopâ€ and â€œMy messageâ€
(see Fig. 2(a)), denoted as ğ‘1andğ‘2, respectively. The context of
ğ‘’â€²also has two elements labelled with â€œBert Smithâ€ and â€œLogoutâ€,
denoted asğ‘â€²
1andğ‘â€²
2, respectively (see Fig. 2(b)). Thus, there are
two matching pairs from the two contexts, i.e., ( ğ‘1,ğ‘â€²
1) and (ğ‘2,ğ‘â€²
2).
The context similarity between ğ‘’andğ‘’â€²is calculated as follows:
ğ‘ ğ‘–ğ‘šğ‘(ğ‘’,ğ‘’â€²)=max((ğ‘ ğ‘–ğ‘šğ‘–(ğ‘1,ğ‘â€²
1)âˆ’ğ‘¡),0)
+max((ğ‘ ğ‘–ğ‘šğ‘–(ğ‘2,ğ‘â€²
2)âˆ’ğ‘¡),0)
=max(0.3âˆ’0.6,0)+max(0.2âˆ’0.6,0)=0
Finally, the comprehensive similarity between ğ‘’andğ‘’â€²is computed
as follows:
ğ‘ ğ‘–ğ‘š(ğ‘’,ğ‘’â€²)=ğ‘ ğ‘–ğ‘šğ‘–(ğ‘’,ğ‘’â€²)+ğ›¼Ã—ğ‘ ğ‘–ğ‘šğ‘(ğ‘’,ğ‘’â€²)=0.65+0.75Ã—0=0.65
4 EXPERIMENTAL RESULTS
To evaluate the efficacy of our Semter in supporting web test
repair, we conducted an empirical study on six real-world web
applications compared to Water ,Vista , and WebEvo . Our three
research questions (RQs) are summarized as follows:
RQ1: Effectiveness. How effective is Semter in terms of repair
ratio of test breakages compared to three baselines?
RQ2: Efficiency. How efficient is Semter in terms of running time
compared to three baselines?
RQ3: Parameter Setting. What is the impact of parameter setting
on test repair effectiveness?
4.1 Experimental Setup
Subject Applications. In our experiment, we selected six real-
world web applications.
(1) AddressBook [ 1]: a contact manager that manages personal
information in groups.
(2) Claroline [ 2]: an online collaborative learning environment
that allows teachers or education institutions to create and admin-
ister courses.
(3) Collabtive [ 3]: a project management application that pro-
vides the functionality of creating, tracking, and managing projects.
(4) Password-Manager (PWMA) [ 6]: an application used to gen-
erate and manage the passwords of users.
(5) MantisBT [4]: a bug tracker that facilitates tracking bugs.
(6) Meeting Room Booking System (MRBS) [ 5]: a system used
for multi-site booking of meeting rooms or any other resources
such as computers, planes, etc.
Table 1 shows their properties, including the number of releases,
the average number of lines of code (LOC) per release, and the
number of test cases across all releases. For each application, we
selected those representative releases between which non-trivial
GUI differences occur instead of adjacent versions to find out the
repair capability of Semter in the face of challenges (details on se-
lected releases are available at https://github.com/Reoke/SEMTER).
1195Semantic Test Repair for Web Applications ESEC/FSE â€™23, December 3â€“9, 2023, San Francisco, CA, USA
Table 1: Subject applications and their test suites
Applications #Releases LOCs #Test Cases
AddressBook 7 4,354 148
Claroline 5 258,262 155
Collabtive 8 89,048 279
PWMA 9 4,631 122
MantisBT 6 215,103 205
MRBS 8 68,320 174
We reused the test cases designed by [ 21]4and extended them to
all releases (except for the last release).
Baselines. Three representative state-of-the-art web test repair
tools, namely Water [11],Vista [40], and WebEvo [39], were se-
lected as baselines. All three repair tools are based on differential
testing that compares the executions of the test case on the basic
and updated versions of a web application. The test case runs suc-
cessfully on the basic version but may break on the updated version.
The difference among the three techniques is the method of map-
ping UI elements. Water collects DOM attributes of UI elements
and uses them to find the target element. Instead, Vista captures
the visual information of UI elements from the test execution and
exploits image processing techniques to map UI elements while
attempting to repair NSNP breakages using the local exploring
strategy (the exploration step is one). WebEvo combines the string
similarity computed by Levenshtein distance and image similarity
of UI elements to identify the target element.
4.2 RQ1 Effectiveness
Motivation and Approach. The goal of RQ1 is to evaluate the ef-
fectiveness of Semter in repairing test breakages. For each subject
application and each test case, we applied Water ,Vista ,Semter ,
andWebEvo to each test breakage that occurred on the subsequent
updated version. For each tool and each breakage, the correctness
of each repair was examined by manual inspection. Only those
repairs passed by the manual inspection were counted as correct
repairs. Specifically, (a) we invited two graduate students majoring
in computer science to judge whether each breakage was correctly
repaired by studied tools (the inter-rater agreement between the
first two students was measured using the Cohenâ€™s Kappa coeffi-
cient [ 43]). (b) The first two students discussed the disagreements
with the third student to form a unified judgment.
Results. The Cohenâ€™s Kappa coefficients to measure the inter-rater
agreement in Step (a) are 99.7%, 99.9%, 99.7%, and 99.3% for Water ,
Vista ,Semter , and WebEvo , respectively. The four Cohenâ€™s Kappa
coefficients are more than 99%, indicating the very high consistency
of labeling results from manual inspection.
Table 2 reports the repair effectiveness. For each application,
the table presents the number of breakages and the number of cor-
rect repairs made by Water ,Vista ,WebEvo , and Semter . The
results are further divided into four test breakage classes: NSSP,
MSDP, NSNP, and NSR. On the whole, Semter is capable of re-
pairing 1293/1540 (84%) breakages, whereas Water ,Vista , and
WebEvo are capable of repairing 620/1540 (40%), 743/1540 (48%),
and 837/1540 (54%) breakages, respectively. As a result, Semter
can correct more breakages than Water ,Vista , and WebEvo , with
4Test cases are available at https://sepl.dibris.unige.it/2014-Visual-DOM.php.Table 2: Repaired results ( ğ‘¡=0.6,ğ›¼=0.75)
Applications Breakages Water Vista WebEvo Semter
AddressBookNSSP 183 120 72 69 168
MSDP 12 12 8 12 12
NSNP 4 0 0 0 4
NSR 0 0 0 0 0
Total 199 132 80 81 184
ClarolineNSSP 290 79 176 151 253
MSDP 37 23 22 26 30
NSNP 0 0 0 0 0
NSR 3 3 2 0 3
Total 330 105 200 177 286
CollabtiveNSSP 164 90 117 83 143
MSDP 13 3 2 10 3
NSNP 9 0 0 0 0
NSR 0 0 0 0 0
Total 186 93 119 93 146
PWMANSSP 145 49 52 100 139
MSDP 35 27 11 27 9
NSNP 14 0 0 0 1
NSR 13 9 4 0 0
Total 207 85 67 127 149
MantisBTNSSP 223 86 96 187 202
MSDP 27 13 10 20 22
NSNP 5 0 0 0 2
NSR 0 0 0 0 0
Total 255 99 106 207 226
MRBSNSSP 305 101 147 113 263
MSDP 28 4 24 24 24
NSNP 14 0 0 0 0
NSR 16 1 0 15 15
Total 363 106 171 152 302
All AppsNSSP 1310 525 660 703 1168
MSDP 152 82 77 119 100
NSNP 46 0 0 0 7
NSR 32 13 6 15 18
Total 1540 620 743 837 1293
Ratio (%) / 40 48 54 84
109%, 74%, and 54% increments, respectively. Concerning specific
applications, Semter repairs 39%-185% breakages more than Water ,
23%-130% more than Vista , and 9%-127% more than WebEvo .
Observe the repair effectiveness of different classes of breakages.
Semter repairs 89% of NSSP breakages, whereas Water ,Vista , and
WebEvo repair only 40%, 50%, and 54% respectively. Even though
Semter repairs only 7 NSNP breakages, it prevails over Water ,
Vista , and WebEvo .WebEvo repairs 19% MSDP breakages more
than Semter . The overall superiority of Semter over WebEvo is,
however, still great as MSDP breakages occupy only 9.9% out of all
breakages. There is no great significant difference between the four
tools with respect to NSR breakages.
Fig. 5 further compares Semter ,Water ,Vista , and WebEvo . In
each Venn diagram, the red, green, blue, and yellow circles denote
the number of breakages repaired by Semter ,Water ,Vista , and
WebEvo , respectively. The six left Venn diagrams show the number
of overlaps of repaired breakages for each application. For exam-
ple, for AddressBook , 40 breakages are repaired by four tools, 51
breakages are repaired only by Water ,Semter andWebEvo , 131
breakages are repaired only by Water andSemter , and 10 break-
ages are repaired only by Semter . The rightmost diagram provides
the total across all applications. As shown in the rightmost diagram,
187/1540 (12%) breakages are correctly repaired by all four tools.
However, there are 155 breakages that are repaired only by Semter .
In contrast, only 29 breakages are repaired only by Water whereas
42 and 23 by Vista andWebEvo , respectively. It is noted that no
technique is capable of repairing 97/1540 (6.3%) test breakages.
1196ESEC/FSE â€™23, December 3â€“9, 2023, San Francisco, CA, USA Xiaofang Qi, Xiang Qian, and Yanhui Li
Figure 5: Partition of breakages based on whether they can
be repaired by each approach
Answer to RQ1. Semter is able to repair 84% of breakages
on average, significantly outperforming Water ,Vista , and
WebEvo .
4.3 RQ2: Efficiency
Motivation and Approach. RQ2 aims to evaluate the running
time of Semter . Our experiment was performed on Windows 10
and Firefox web browser 100.0, running on a 1.1 GHz Intel Core i7
CPU with 16 GB memory. We measured the execution time required
to run the test suites without and with the trace module, and repair
breakages.
Results. Table 3 reports the average time per test case that is re-
quired to run the test case set without (Column 2) and with (Column
3) the trace module, and repair breakages (Column 4). Vista takes
the most trace time (57.51s), then Semter (25.00s) and WebEvo
(14.04s), finally Water (12.83s). The reason is that Water traces
only the DOM information of UI elements, and WebEvo requires
capturing additional page images. VISTA requires capturing the
page image and other necessary information to generate visual
locators. The process of image capturing and processing seriously
slows down the efficiency of Vista . Compared with Water and
WebEvo , the increase in time for Semter mainly comes from the
process of extracting the contexts of elements, which is positively
correlated with the number of elements in the unfiltered contexts.
In most cases, the number of elements in the unfiltered contexts is
relatively small, so Semter will not spend much more time than
Water andWebEvo .
Concerning the repair time, Water still takes the least repair
time due to its rather simple repair process. WebEvo takes the
most time as its repair requires the simultaneous computation of
text similarity and image similarity of UI elements. Vista is fasterTable 3: Execution Time
Applications Run(s)Trace(s) Repair(s)
Water Vista WebEvoSemter Water Vista WebEvoSemter
AddressBook 6.57 7.39 49.88 9.18 29.05 7.67 18.73 22.60 33.79
Claroline 9.92 10.83 54.76 12.37 20.44 9.48 16.55 27.56 20.04
Collabtive 13.71 14.45 60.51 16.16 21.33 10.52 24.02 61.94 27.31
PWMA 13.85 14.70 58.46 15.80 18.87 9.39 29.18 41.04 20.67
MantisBT 8.09 8.72 56.30 9.01 20.67 9.62 28.06 41.41 29.32
MRBS 19.35 20.21 62.37 20.97 40.93 13.05 25.03 103.98 57.50
Average 12.05 12.83 57.51 14.04 25.00 10.09 23.74 52.16 33.29
Figure 6: Difference of repair ratio with different ğ›¼
than Semter , which is much faster than WebEvo . The context
processing is a key factor that greatly influenced the repair time
forSemter .
On the whole, Vista takes the most time (81.25s), then WebEvo
(66.20s) and Semter (58.29s), finally Water (22.92s). The total time
spent by Semter is less than 1 minute, which is an acceptable time.
Answer to RQ2. The efficiency of Semter is lower than
Water , but higher than Vista andWebEvo . The total running
time is acceptable.
4.4 RQ3: Parameter Settings
Motivation and Approach. The weighting parameter ğ›¼in For-
mula 1 is a key parameter that impacts the repair effectiveness of
Semter . To evaluate the impact, we selected 5 different values for
ğ›¼, and for each ğ›¼, we conducted a repair effectiveness experiment
and counted the repair ratios.
Results. Fig. 6 reports the impact of ğ›¼on the repair effectiveness.
Whenğ›¼is selected as 0.75, the highest repair ratio (84%) is achieved
across all applications. However, when ğ›¼is 0, i.e., the contextual
semantic information is not taken into account, the repair ratio
reduces by 8.7% on average. The reduction ranges from 1.2%-15.2%
across different applications, as compared with ğ›¼=0.75. Such
a result demonstrates that contextual semantic information can
facilitate test repairs, significantly improving the repair ratio. Ad-
ditionally, when ğ›¼changes from 0.25 to 1.5, the repair rate varies
slightly. The difference between the maximum and the minimum
repair ratios is only 1.9%.
Answer to RQ3. The contextual semantic information im-
proves the repair ratio significantly. Variation of ğ›¼in some
range yields no big fluctuation on the repair ratio.
1197Semantic Test Repair for Web Applications ESEC/FSE â€™23, December 3â€“9, 2023, San Francisco, CA, USA
Table 4: Causes of Breakages not repaired by any approach
Breakages Causes Numbers
NSSPUI elements are changed from text to icon and the
changes are great.12
Both appearances and visible text contents of UI
elements are changed significantly.11
Multiple similar UI elements on the page are difficult
to differentiate.16
UI Elements change essentially, e.g., from drop
down list to radio button.8
MSDPMis-selected elements are similar to original ele-
ments in DOM attributes and appearances.6
NSNPToo many neighbouring pages and the limitation of
exploration strategy make the repair difficult.14
5 DISCUSSION
This section further discusses the factors that may impact the effec-
tiveness of the repair and presents tool implementation.
5.1 Explanation of Effectiveness Results
Breakages not repaired by Semter .As shown in Table 2, Semter
achieves the best repair ratio of NSSP breakages (89%). The critical
factor is the relatively reliable semantics of most UI elements dur-
ing the evolution of web applications (compared with their DOM
attributes or GUI appearances). However, the ratios of NSNP, MSDP,
and NSR repaired by Semter are comparatively low. To find out
the cause, we conducted an additional experiment.
Considering that when repairing MSDP breakages, Algorithm
1 does not guarantee that the most similar UI element is selected
as the target element since the code in Lines 28â€“29 is executed if
the original locator ğ‘™returns a non-null element. Then we modified
the algorithm and forced it to select the most similar UI element on
the current page to repair the 52 ( 152âˆ’100) breakages. Similarly,
for NSNP breakages, we forced it also to select the most similar
UI element from all neighbouring pages. The experimental results
show that our approach is capable of repairing 13 more MSDP
breakages and 25 more NSNP breakages. Accordingly, the repair
ratio of MSDP increases from 66% to 74% whereas NSNP from
15% to 70%, achieving a significant increase. The results further
corroborate the superiority of Semter . As for NSR, increasing the
thresholdğ‘¡would improve the repair ratio, yet at the expense of
influencing repairing other classes of test breakages.
Breakages not repaired by any approach. Fig. 5 shows that
97 breakages are not repaired by any approach. If we use Semter
and select the most similar element as the target element and then
remove such repaired breakages, there are still 47 NSSPs, 6 NSDPs,
and 14 NSNP breakages. Table 4 describes the number of breakages
and the corresponding causes.
5.2 Selection of the Threshold of Semantic
Similarity
In Algorithm 1, the threshold ğ‘¡is utilized to filter candidate se-
mantically dissimilar elements. A too large threshold may lead to
missing the target elements, whereas a too small threshold may
produce excessive candidate elements. To select an appropriate
threshold value, we conducted an empirical study. The subjects of
our experiment consist of a set of text-text pairs, a set of text-image
Figure 7: Precision/recall of different pairs
pairs, and a set of image-image pairs. For each subject, we used our
semantic model to predict a semantic similarity score. When the
score is greater than or equal to the threshold ğ‘¡, we say the pair is
semantically similar. Otherwise, it is semantically dissimilar. We
selected the following data sets:
(1)Text-text pairs. SNLI corpus is a collection of English sen-
tence pairs with labels entailment ,contradiction , and neutral [9].
We selected it as the data set of text-text pairs. Those text-text pairs
with the label entailment in the set were taken as semantically sim-
ilar pairs, and others as semantically dissimilar pairs. Eventually,
we generated a data set with 3,368 semantically similar pairs and
6,632 semantically dissimilar pairs.
(2)Text-image and image-image pairs. We selected the test
set of the data set used for training our semantic model (as described
in Section 3). Those icon images and their labels were taken as
semantically similar image-text pairs, whereas those icon images
and their negatively sampled labels were taken as semantically
dissimilar pairs. Those two icons with the same labels were taken as
a semantically similar image-image pair. For each icon, we randomly
sampled an icon image from the test set to form a semantically
dissimilar image-image pair. Finally, we generated a data set with
44,134 semantically similar image-text pairs and dissimilar pairs
and a data set with 30,950 semantically similar image-image pairs
and dissimilar pairs, respectively.
As shown in Fig. 7, the solid and dotted lines represent the
precision and recall of similar elements respectively. The precision
and recall is calculated as:
ğ‘ğ‘Ÿğ‘’ğ‘ğ‘–ğ‘ ğ‘–ğ‘œğ‘› =ğ‘‡ğ‘ƒ
ğ‘‡ğ‘ƒ+ğ¹ğ‘ƒ, ğ‘Ÿğ‘’ğ‘ğ‘ğ‘™ğ‘™ =ğ‘‡ğ‘ƒ
ğ‘‡ğ‘ƒ+ğ¹ğ‘
whereğ‘‡ğ‘ƒdenotes the number of true similar pairs which are also
predicted as similar ones (i.e., true positive), ğ¹ğ‘ƒdenotes the number
of false similar pairs which are yet predicted as similar ones (i.e.,
false positive), ğ¹ğ‘denotes the number of true similar pairs which
are yet predicted as dissimilar ones (i.e., false negative).
Asğ‘¡increases from 0to0.6, the precision of text-image and
image-image pairs rises quickly to a peak (nearly 1.0). Then the
precision has no change if ğ‘¡increases continuously. The precision
of text-text pairs rises steadily as ğ‘¡increases from 0, which might
be due to the different quantity of positive and negative samples.
Instead, the three recall curves reduce steadily as the threshold ğ‘¡
increases from 0to0.6, then they decline rapidly. Overall, 0.6 is an
ideal threshold of semantic similarity.
1198ESEC/FSE â€™23, December 3â€“9, 2023, San Francisco, CA, USA Xiaofang Qi, Xiang Qian, and Yanhui Li
Figure 8: Precision of similar elements
5.3 Effectiveness of Semantic Model
As mentioned in Section 3.3, our semantic model supports the cal-
culation of semantic similarity, which is crucial for their matching.
We conducted an experiment to observe the effectiveness of our
semantic model in calculating semantic similarity.
We first collected those UI elements exercised by the test state-
ments at which NSSP and MSDP test breakages (a total of 1462)
occurred (NSR and NSNP were not chosen as the target elements
were not on the current page). Then, we invited 3 graduate stu-
dents majoring in computer science to construct the standard set of
semantically similar UI elements for such UI elements. Specifically,
the first 2 students independently found semantically similar UI
elements on the current page for each UI element. The inter-rater
agreement among them was measured using Cohenâ€™s Kappa coeffi-
cient [ 43]. In our experiment, Cohenâ€™s Kappa coefficient was nearly
88%. Afterwards, the first 2 students discussed the disagreements
with the third student, and eventually, all the 2,587 similar elements
in the standard set were unanimously passed.
Subsequently, we used Semter ,Vista , and WebEvo to auto-
matically search for similar elements on the corresponding pages
(Water has not been considered here, as DOM attributes of UI ele-
ments are often invisible for testers, and they are volatile in many
cases during the evolution of web apps). To measure the effective-
ness of our semantic model in essence, Semter only leveraged
individual semantic information to compute the semantic similarity
between UI elements and find similar elements. The contextual
semantic information has not been considered as it only makes
up for the inadequacy of semantic information of UI elements. We
checked whether the element returned by Semter ,Vista , orWe-
bEvo was in the standard set. If the returned element was in the
standard set, it was considered to be accurate. Otherwise, it was
not. Fig. 8 reports the precision of the returned elements captured
bySemter ,Vista , and WebEvo across applications. The precision
ofSemter is 0.92. It ranges from 0.90 to 0.95 for each application,
whereas Vista from 0.44 to 0.72 and WebEvo from 0.47 to 0.86.
The results corroborate that our semantic model is very effective
in computing semantic similarity between UI elements.
5.4 Comparison of Different Model
Components
To evaluate the key component of our semantic repair approach, we
conducted an empirical study on six real-world web applications
and compared the repair effects with multiple different natural
language processing and image processing components.Table 5: Repaired results of different model components
Applications Breakages WV+TL SB+TL SB+RN
AddressBook 199 184 188 184
Claroline 330 251 264 286
Collabtive 186 168 161 146
PWMA 207 81 124 149
MantisBT 255 237 219 226
MRBS 363 195 302 302
All Apps 1540 1116 1258 1293
Ratio (%) / 72 82 84
We replaced Sentence-Bert with Word2Vec, which was often
used by most semantic GUI test tools mentioned in the related
work. Specifically, we selected word2vec-google-news-3005. The
model was trained using a corpus of approximately 100 billion
words from the entire Google News and covers approximately 3
million words and phrases. Regarding image components, we chose
a transformer-based model (like LabelDroid [ 10], also mentioned in
the related work) and the data set that Semter was used to train a
label predicting model for image-based UI elements. The structure,
loss function, other training details of the model and corresponding
codes were similar to LabelDroid.
Table 5 reports the repaired results of three different configura-
tions, where WV, SB, RN, and TL represent Word2Vec, Sentence-
Bert, ResNet and transformer-based label model respectively. The
last column (â€œSB+RNâ€) is the configuration of Semter whereas the
two left columns (marked as â€œWV+TLâ€ and â€œSB+TLâ€) are taken as
comparisons. The results show that Semter (â€œSB+RNâ€) is capable
of repairing 1293/1540 (84%) breakages, whereas â€œWV+TLâ€ and
â€œSB+TLâ€ are capable of repairing 1116/1540 (72%) and 1258/1540
(82%) breakages respectively. Our hybrid model used in Semter
is confirmed to be the best configuration of natural language pro-
cessing and image processing components that we have found.
â€œSB+TLâ€ is capable of repairing 10% breakages more than â€œWV+TLâ€,
indicating that Sentence-Bert is a much stronger natural language
processing component than Word2Vec. Instead, there is no big dif-
ference between â€œSB+TLâ€ and â€œSB+RNâ€, demonstrating that the
two image-based processing components have similar semantic
processing capabilities.
Moreover, we find that even though different natural language
processing and image-based processing components achieve differ-
ent repair effects, the lowest repair ratio using our semantic-based
approach is still 72%, which is much higher than those DOM-based
or image-based methods (e.g., Water (40%), Vista (48%), and We-
bEvo (54%)). This comparison further corroborates the superiority
of our semantic-based approach over the state-of-the-art repair
techniques.
5.5 Implementation
We have implemented our approach into a tool called Semter to au-
tomate the repair of GUI test scripts for web applications. Semter ,
written in Java, supports Selenium test cases written in Java. As
shown in Fig. 4, Semter mainly consists of Semantic Test Tracer ,
Semantic Similarity Calculator , and Test Repair modules. Seman-
tic Test Tracer , which is responsible for recording and collecting
5https://radimrehurek.com/gensim/
1199Semantic Test Repair for Web Applications ESEC/FSE â€™23, December 3â€“9, 2023, San Francisco, CA, USA
the execution trace of test cases, is realized using AOP (Aspect
Oriented Programming) technique. Semantic Similarity Calcula-
toradopts PyTorch to implement our semantic model in Python,
which interacts with Java code through Jep6.Semter is available
at https://github.com/Reoke/SEMTER. Note that our approach is
general and easily adapted to repair test cases developed by other
testing frameworks or programming languages.
6 THREATS TO VALIDITY
The main internal threat is the possible faults in implementing our
approach. To mitigate this threat, we have carefully reviewed our
code to ensure its correctness. Besides, the data set of icon images
may not be quite sufficient and representative, which influences
the precision of the semantic similarity predicted by our semantic
model, especially for icon-related UI elements. More icon image
data are required to be collected for retraining our semantic model.
The external threat is the ability to generalize our results to
other web applications. However, in our experiment, we select six
real-world web applications that have been used in previous studies
and reuse the test suite to ensure the objectivity and fairness of the
results. In the future, we plan to conduct empirical studies on more
web applications to mitigate this threat further.
7 RELATED WORK
GUI Test Repair. Choudhary et al. propose Water that uses DOM
attributes of UI elements to search for the target elements and
provides repair suggestions [ 11]. Hammoudi et al. further improve
Water by implementing an incremental strategy that repairs the
breakages across intermediate fined-grained successive versions
[16]. Stocco et al. present Vista that exploits visual appearances of
UI elements to match UI elements [ 40]. Shao et al. combine the text
similarity (based on Levenshtein distance) with image similarity to
match UI elements, and present a framework WebEvo for repairing
breakages [ 39]. Recently, DOM-based and vision-based test repair
techniques have been extended to regression testing of mobile
applications [ 35,44]. Yoon et al. also present a machine learning-
based Android GUI test case repair technique [ 46]. Different from
these web test repair techniques, Semter simulates human repair
behaviors. It transforms both text and image information of UI
elements into semantic information in a unified framework and
then uses it to match them. Instead, existing test repair techniques
repair web tests by comparing either DOM attributes or images.
In addition, Grechnaik et al. propose an approach that reports
repair suggestions by comparing the differences in GUI models [ 15].
Huang et al. use a genetic algorithm to repair GUI test suites [ 20].
Memon et al. present event flow graphs as the GUI model of desktop
applications for path repairs [ 32]. Gao et al. further implement an
event flow graph-based repair algorithm [ 14]. Harman et al. present
an automated session-based test repair approach [ 18]. Daniel et al.
use a white box method to integrate the functionality of recording
GUI code modification on IDE and then repair tests [12].
Semantic GUI Testing. Rau et al. present a semantic-based
method to transfer tests across web applications by using Word2Vec
technique to compute semantic similarity between web UI elements
[36]. These techniques are then extended to mobile applications
6https://github.com/ninia/jep/[7,8,26]. Marriani et al. conduct an empirical study on different se-
mantic mapping of GUI events for test reuses [ 29]. Thummalapenta
et al. propose a guided test generation technique, which implements
higher coverage of specified business rules by extracting semantic
information about widgets [ 42].Semter differs from these semantic
GUI testing techniques in the computation method of semantic sim-
ilarity by training a hybrid semantic model that can handle text and
image information in a unified semantic framework and incorporate
the contextual semantic information. Moreover, Semter extracts
only visible texts or images as the semantic information source
like humans while most of these semantic GUI testing techniques
often also extract many invisible texts, which may actually reduce
the effectiveness of key semantic information due to conflicting or
dispersing impacts between texts. Finally, Semter selects Sentence-
BERT as the NLP model, which processes semantic information in
the sentence level instead of the word level, e.g., Word2Vec, which
is used by most of these above semantic GUI testing techniques.
Besides, Mariani et al. present an automated technique to gen-
erate semantic GUI test cases for independent application func-
tionalities [ 30]. Talebipour et al. exploit CV and NLP techniques
to evaluate the similarity of GUI events for test migration across
mobile platforms [ 41]. Zhang et al. adopt deep learning techniques
and the word2vec model to generate text input for mobile applica-
tions automatically [ 27]. Recently, deep learning techniques have
been utilized to generate labels for icons automatically [10, 31].
Web Element Locator. Leotta et al. propose a robust XPath
locator generator tool ROBULA by converting absolute XPaths
to relative XPaths [ 22,24]. Subsequently, they integrate the five
most commonly used locators into one robust multi-locator tool
that gains all advantages of the five locators [ 23]. Long et al. also
present a self-replay enhanced robust record/replay tool WebRR
for web testing, which is capable of deriving multiple locators [ 28].
8 CONCLUSION
In this paper, we propose a novel semantic test repair technique and
a prototype tool Semter for repairing web tests, which retrieves
relevant semantic information from test executions to facilitate test
repair. We evaluated Semter on six real-world web applications.
Our experimental results show that Semter is capable of correctly
repairing on average 84% of web test breakages within an acceptable
time, outperforming the state-of-the-art repair techniques signifi-
cantly. In future work, we will conduct additional empirical studies
further to corroborate the effectiveness of our semantic test re-
pair technique. Moreover, we will extend our semantic test repair
approach to mobile applications.
Data Availability
We provide data and source code used to conduct this study at
https://github.com/Reoke/SEMTER.
ACKNOWLEDGEMENT
The authors thank the anonymous reviewers for their valuable
feedback. This work is supported by the National Science Founda-
tion of China under Grant No. 61972082 and 62172202. Yanhui Li
(yanhuili@nju.edu.cn) is the corresponding author.
1200ESEC/FSE â€™23, December 3â€“9, 2023, San Francisco, CA, USA Xiaofang Qi, Xiang Qian, and Yanhui Li
REFERENCES
[1] [n. d.]. AddressBook. https://sourceforge.net/projects/php-addressbook/.
[2] [n. d.]. Claroline. https://sourceforge.net/projects/claroline/.
[3] [n. d.]. Collabtive. https://sourceforge.net/projects/collabtive/.
[4] [n. d.]. MantisBT. https://sourceforge.net/projects/mantisbt/.
[5][n. d.]. Meeting Room Booking System. https://sourceforge.net/projects/mrbs/.
[6][n. d.]. Password-Manager. https://sourceforge.net/projects/
phppasswordmanager/.
[7]Farnaz Behrang and Alessandro Orso. 2018. Test migration for efficient large-
scale assessment of mobile app coding assignments. In Proceedings of the 27th
ACM SIGSOFT International Symposium on Software Testing and Analysis, ISSTA
2018, Amsterdam, The Netherlands, July 16-21, 2018 , Frank Tip and Eric Bodden
(Eds.). ACM, 164â€“175. https://doi.org/10.1145/3213846.3213854
[8]Farnaz Behrang and Alessandro Orso. 2019. Test Migration Between Mobile
Apps with Similar Functionality. In 34th IEEE/ACM International Conference on
Automated Software Engineering, ASE 2019, San Diego, CA, USA, November 11-15,
2019. IEEE, 54â€“65. https://doi.org/10.1109/ASE.2019.00016
[9]Samuel R. Bowman, Gabor Angeli, Christopher Potts, and Christopher D. Man-
ning. 2015. A large annotated corpus for learning natural language inference.
InProceedings of the 2015 Conference on Empirical Methods in Natural Language
Processing (EMNLP) . Association for Computational Linguistics.
[10] Jieshan Chen, Chunyang Chen, Zhenchang Xing, Xiwei Xu, Liming Zhu, Guo-
qiang Li, and Jinshui Wang. 2020. Unblind your apps: predicting natural-
language labels for mobile GUI components by deep learning. In ICSE â€™20: 42nd
International Conference on Software Engineering, Seoul, South Korea, 27 June
- 19 July, 2020 , Gregg Rothermel and Doo-Hwan Bae (Eds.). ACM, 322â€“334.
https://doi.org/10.1145/3377811.3380327
[11] Shauvik Roy Choudhary, Dan Zhao, Husayn Versee, and Alessandro Orso. 2011.
Water: Web application test repair. In Proceedings of the First International Work-
shop on End-to-End Test Script Engineering . 24â€“29.
[12] B. Daniel, Q. Luo, M. Mirzaaghaei, D. Marinov, and M PezzÃ¨. 2011. Automated
GUI refactoring and test script repair. ACM (2011).
[13] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT:
Pre-training of Deep Bidirectional Transformers for Language Understanding. In
Proceedings of the 2019 Conference of the North American Chapter of the Associa-
tion for Computational Linguistics: Human Language Technologies, NAACL-HLT
2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers) , Jill
Burstein, Christy Doran, and Thamar Solorio (Eds.). Association for Computa-
tional Linguistics, 4171â€“4186. https://doi.org/10.18653/v1/n19-1423
[14] Zebao Gao, Zhenyu Chen, Yunxiao Zou, and Atif M. Memon. 2016. SITAR:
GUI Test Script Repair. IEEE Trans. Software Eng. 42, 2 (2016), 170â€“186. https:
//doi.org/10.1109/TSE.2015.2454510
[15] Mark Grechanik, Qing Xie, and Chen Fu. 2009. Maintaining and evolving GUI-
directed test scripts. In 31st International Conference on Software Engineering,
ICSE 2009, May 16-24, 2009, Vancouver, Canada, Proceedings . IEEE, 408â€“418. https:
//doi.org/10.1109/ICSE.2009.5070540
[16] Mouna Hammoudi, Gregg Rothermel, and Andrea Stocco. 2016. WATERFALL:
an incremental approach for repairing record-replay tests of web applications. In
Proceedings of the 24th ACM SIGSOFT International Symposium on Foundations of
Software Engineering, FSE 2016, Seattle, WA, USA, November 13-18, 2016 , Thomas
Zimmermann, Jane Cleland-Huang, and Zhendong Su (Eds.). ACM, 751â€“762.
https://doi.org/10.1145/2950290.2950294
[17] Mouna Hammoudi, Gregg Rothermel, and Paolo Tonella. 2016. Why do
Record/Replay Tests of Web Applications Break?. In 2016 IEEE International
Conference on Software Testing, Verification and Validation, ICST 2016, Chicago, IL,
USA, April 11-15, 2016 . IEEE Computer Society, 180â€“190. https://doi.org/10.1109/
ICST.2016.16
[18] Mark Harman and Nadia Alshahwan. 2008. Automated Session Data Repair for
Web Application Regression Testing. In First International Conference on Software
Testing, Verification, and Validation, ICST 2008, Lillehammer, Norway, April 9-11,
2008. IEEE Computer Society, 298â€“307. https://doi.org/10.1109/ICST.2008.56
[19] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep Residual
Learning for Image Recognition. In 2016 IEEE Conference on Computer Vision
and Pattern Recognition, CVPR 2016, Las Vegas, NV, USA, June 27-30, 2016 . IEEE
Computer Society, 770â€“778. https://doi.org/10.1109/CVPR.2016.90
[20] Si Huang, Myra B. Cohen, and Atif M. Memon. 2010. Repairing GUI Test Suites
Using a Genetic Algorithm. In Third International Conference on Software Testing,
Verification and Validation, ICST 2010, Paris, France, April 7-9, 2010 . IEEE Computer
Society, 245â€“254. https://doi.org/10.1109/ICST.2010.39
[21] Maurizio Leotta, Diego Clerissi, Filippo Ricca, and Paolo Tonella. 2014. Visual
vs. DOM-Based Web Locators: An Empirical Study. In Web Engineering, 14th
International Conference, ICWE 2014, Toulouse, France, July 1-4, 2014. Proceedings
(Lecture Notes in Computer Science, Vol. 8541) , Sven Casteleyn, Gustavo Rossi, and
Marco Winckler (Eds.). Springer, 322â€“340. https://doi.org/10.1007/978-3-319-
08245-5_19
[22] Maurizio Leotta, Andrea Stocco, Filippo Ricca, and Paolo Tonella. 2014. Reduc-
ing Web Test Cases Aging by Means of Robust XPath Locators. In 25th IEEEInternational Symposium on Software Reliability Engineering Workshops, ISSRE
Workshops, Naples, Italy, November 3-6, 2014 . IEEE Computer Society, 449â€“454.
https://doi.org/10.1109/ISSREW.2014.17
[23] Maurizio Leotta, Andrea Stocco, Filippo Ricca, and Paolo Tonella. 2015. Us-
ing Multi-Locators to Increase the Robustness of Web Test Cases. In 8th IEEE
International Conference on Software Testing, Verification and Validation, ICST
2015, Graz, Austria, April 13-17, 2015 . IEEE Computer Society, 1â€“10. https:
//doi.org/10.1109/ICST.2015.7102611
[24] Maurizio Leotta, Andrea Stocco, Filippo Ricca, and Paolo Tonella. 2016. Robula+:
an algorithm for generating robust XPath locators for web testing. J. Softw. Evol.
Process. 28, 3 (2016), 177â€“204. https://doi.org/10.1002/smr.1771
[25] Maurizio Leotta, Andrea Stocco, Filippo Ricca, and Paolo Tonella. 2018. Pesto:
Automated migration of DOM-based Web tests towards the visual approach.
Softw. Test. Verification Reliab. 28, 4 (2018). https://doi.org/10.1002/stvr.1665
[26] Jun-Wei Lin, Reyhaneh Jabbarvand, and Sam Malek. 2019. Test Transfer Across
Mobile Apps Through Semantic Mapping. In 34th IEEE/ACM International Confer-
ence on Automated Software Engineering, ASE 2019, San Diego, CA, USA, November
11-15, 2019 . IEEE, 42â€“53. https://doi.org/10.1109/ASE.2019.00015
[27] Peng Liu, Xiangyu Zhang, Marco Pistoia, Yunhui Zheng, Manoel Marques, and
Lingfei Zeng. 2017. Automatic text input generation for mobile testing. In Proceed-
ings of the 39th International Conference on Software Engineering, ICSE 2017, Buenos
Aires, Argentina, May 20-28, 2017 , SebastiÃ¡n Uchitel, Alessandro Orso, and Mar-
tin P. Robillard (Eds.). IEEE / ACM, 643â€“653. https://doi.org/10.1109/ICSE.2017.65
[28] Zhenyue Long, Guoquan Wu, Xiaojiang Chen, Wei Chen, and Jun Wei. 2020.
WebRR: self-replay enhanced robust record/replay for web application testing.
InESEC/FSE â€™20: 28th ACM Joint European Software Engineering Conference and
Symposium on the Foundations of Software Engineering, Virtual Event, USA, No-
vember 8-13, 2020 , Prem Devanbu, Myra B. Cohen, and Thomas Zimmermann
(Eds.). ACM, 1498â€“1508. https://doi.org/10.1145/3368089.3417069
[29] Leonardo Mariani, Ali Mohebbi, Mauro PezzÃ¨, and Valerio Terragni. 2021. Se-
mantic matching of GUI events for test reuse: are we there yet?. In ISSTA â€™21: 30th
ACM SIGSOFT International Symposium on Software Testing and Analysis, Virtual
Event, Denmark, July 11-17, 2021 , Cristian Cadar and Xiangyu Zhang (Eds.). ACM,
177â€“190. https://doi.org/10.1145/3460319.3464827
[30] Leonardo Mariani, Mauro PezzÃ¨, and Daniele Zuddas. 2018. Augusto: exploiting
popular functionalities for the generation of semantic GUI tests with Oracles. In
Proceedings of the 40th International Conference on Software Engineering, ICSE 2018,
Gothenburg, Sweden, May 27 - June 03, 2018 , Michel Chaudron, Ivica Crnkovic,
Marsha Chechik, and Mark Harman (Eds.). ACM, 280â€“290. https://doi.org/10.
1145/3180155.3180162
[31] Forough Mehralian, Navid Salehnamadi, and Sam Malek. 2021. Data-driven
accessibility repair revisited: on the effectiveness of generating labels for icons
in Android apps. In ESEC/FSE â€™21: 29th ACM Joint European Software Engineering
Conference and Symposium on the Foundations of Software Engineering, Athens,
Greece, August 23-28, 2021 , Diomidis Spinellis, Georgios Gousios, Marsha Chechik,
and Massimiliano Di Penta (Eds.). ACM, 107â€“118. https://doi.org/10.1145/3468264.
3468604
[32] Atif M. Memon. 2008. Automatically repairing event sequence-based GUI test
suites for regression testing. ACM Trans. Softw. Eng. Methodol. 18, 2 (2008),
4:1â€“4:36. https://doi.org/10.1145/1416563.1416564
[33] Ali Mesbah. 2015. Advances in Testing JavaScript-Based Web Applications. Adv.
Comput. 97 (2015), 201â€“235. https://doi.org/10.1016/bs.adcom.2014.12.003
[34] Shabnam Mirshokraie, Ali Mesbah, and Karthik Pattabiraman. 2015. JSEFT:
Automated Javascript Unit Test Generation. In 8th IEEE International Conference
on Software Testing, Verification and Validation, ICST 2015, Graz, Austria, April 13-
17, 2015 . IEEE Computer Society, 1â€“10. https://doi.org/10.1109/ICST.2015.7102595
[35] Minxue Pan, Tongtong Xu, Yu Pei, Zhong Li, Tian Zhang, and Xuandong Li. 2022.
GUI-Guided Test Script Repair for Mobile Apps. IEEE Trans. Software Eng. 48, 3
(2022), 910â€“929. https://doi.org/10.1109/TSE.2020.3007664
[36] Andreas Rau, Jenny Hotzkow, and Andreas Zeller. 2018. Transferring Tests
Across Web Applications. In Web Engineering - 18th International Conference,
ICWE 2018, CÃ¡ceres, Spain, June 5-8, 2018, Proceedings (Lecture Notes in Computer
Science, Vol. 10845) , Tommi Mikkonen, Ralf Klamma, and Juan HernÃ¡ndez (Eds.).
Springer, 50â€“64. https://doi.org/10.1007/978-3-319-91662-0_4
[37] Nils Reimers and Iryna Gurevych. 2019. Sentence-BERT: Sentence Embed-
dings using Siamese BERT-Networks. In Proceedings of the 2019 Conference
on Empirical Methods in Natural Language Processing and the 9th International
Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019, Hong
Kong, China, November 3-7, 2019 , Kentaro Inui, Jing Jiang, Vincent Ng, and
Xiaojun Wan (Eds.). Association for Computational Linguistics, 3980â€“3990.
https://doi.org/10.18653/v1/D19-1410
[38] Danny Roest, Ali Mesbah, and Arie van Deursen. 2010. Regression Testing
Ajax Applications: Coping with Dynamism. In Third International Conference on
Software Testing, Verification and Validation, ICST 2010, Paris, France, April 7-9,
2010. IEEE Computer Society, 127â€“136. https://doi.org/10.1109/ICST.2010.59
[39] Fei Shao, Rui Xu, Wasif Arman Haque, Jingwei Xu, Ying Zhang, Wei Yang,
Yanfang Ye, and Xusheng Xiao. 2021. WebEvo: taming web application evolu-
tion via detecting semantic structure changes. In ISSTA â€™21: 30th ACM SIGSOFT
1201Semantic Test Repair for Web Applications ESEC/FSE â€™23, December 3â€“9, 2023, San Francisco, CA, USA
International Symposium on Software Testing and Analysis, Virtual Event, Den-
mark, July 11-17, 2021 , Cristian Cadar and Xiangyu Zhang (Eds.). ACM, 16â€“28.
https://doi.org/10.1145/3460319.3464800
[40] Andrea Stocco, Rahulkrishna Yandrapally, and Ali Mesbah. 2018. Visual web
test repair. In Proceedings of the 2018 ACM Joint Meeting on European Software
Engineering Conference and Symposium on the Foundations of Software Engineering,
ESEC/SIGSOFT FSE 2018, Lake Buena Vista, FL, USA, November 04-09, 2018 , Gary T.
Leavens, Alessandro Garcia, and Corina S. Pasareanu (Eds.). ACM, 503â€“514.
https://doi.org/10.1145/3236024.3236063
[41] Saghar Talebipour, Yixue Zhao, Luka Dojcilovic, Chenggang Li, and Nenad Med-
vidovic. 2021. UI Test Migration Across Mobile Platforms. In 36th IEEE/ACM
International Conference on Automated Software Engineering, ASE 2021, Melbourne,
Australia, November 15-19, 2021 . IEEE, 756â€“767. https://doi.org/10.1109/ASE51524.
2021.9678643
[42] Suresh Thummalapenta, K. Vasanta Lakshmi, Saurabh Sinha, Nishant Sinha,
and Satish Chandra. 2013. Guided test generation for web applications. In 35th
International Conference on Software Engineering, ICSE â€™13, San Francisco, CA,
USA, May 18-26, 2013 , David Notkin, Betty H. C. Cheng, and Klaus Pohl (Eds.).
IEEE Computer Society, 162â€“171. https://doi.org/10.1109/ICSE.2013.6606562[43] Susana M. Vieira, Uzay Kaymak, and JoÃ£o M. C. Sousa. 2010. Cohenâ€™s kappa
coefficient as a performance measure for feature selection. In FUZZ-IEEE 2010,
IEEE International Conference on Fuzzy Systems, Barcelona, Spain, 18-23 July, 2010,
Proceedings . IEEE, 1â€“8. https://doi.org/10.1109/FUZZY.2010.5584447
[44] Tongtong Xu, Minxue Pan, Yu Pei, Guiyin Li, Xia Zeng, Tian Zhang, Yuetang
Deng, and Xuandong Li. 2021. GUIDER: GUI structure and vision co-guided test
script repair for Android apps. In ISSTA â€™21: 30th ACM SIGSOFT International
Symposium on Software Testing and Analysis, Virtual Event, Denmark, July 11-17,
2021, Cristian Cadar and Xiangyu Zhang (Eds.). ACM, 191â€“203. https://doi.org/
10.1145/3460319.3464830
[45] Rahulkrishna Yandrapally, Suresh Thummalapenta, Saurabh Sinha, and Satish
Chandra. 2014. Robust test automation using contextual clues. In International
Symposium on Software Testing and Analysis, ISSTA â€™14, San Jose, CA, USA - July
21 - 26, 2014 , Corina S. Pasareanu and Darko Marinov (Eds.). ACM, 304â€“314.
https://doi.org/10.1145/2610384.2610390
[46] Juyeon Yoon, Seungjoon Chung, Kihyuck Shin, Jinhan Kim, Shin Hong, and Shin
Yoo. 2022. Repairing Fragile GUI Test Cases Using Word and Layout Embedding.
In15th IEEE Conference on Software Testing, Verification and Validation, ICST
2022, Valencia, Spain, April 4-14, 2022 . IEEE, 291â€“301. https://doi.org/10.1109/
ICST53961.2022.00038
1202