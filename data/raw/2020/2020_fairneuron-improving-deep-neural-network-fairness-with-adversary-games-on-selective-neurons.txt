FairNeuron: Improving Deep Neural Network Fairness with
Adversary Games on Selective Neurons
Xuanqi Gao
Xiâ€™an Jiaotong University
Xiâ€™an, China
gxq2000@stu.xjtu.edu.cnJuan Zhai
Rutgers University
United States
juan.zhai@rutgers.eduShiqing Ma
Rutgers University
United States
shiqing.ma@rutgers.com
Chao Shen
Xiâ€™an Jiaotong University
Xiâ€™an, China
chaoshen@mail.xjtu.edu.cnYufei Chen
Xiâ€™an Jiaotong University
Xiâ€™an, China
yfchen@sei.xjtu.edu.cnQian Wang
Wuhan University
Wuhan, China
qianwang@whu.edu.cn
ABSTRACT
WithDeepNeuralNetwork(DNN)beingintegratedintoagrowing
number of critical systems with far-reaching impacts on society,
there are increasing concerns on their ethical performance, suchas fairness. Unfortunately, model fairness and accuracy in manycases are contradictory goals to optimize during model training.
To solve this issue, there has been a number of works trying to
improvemodel fairnessby formalizingan adversarialgame inthe
model level. This approach introduces an adversary that evaluates
the fairness of a model besides its prediction accuracy on the main
task, and performs joint-optimization to achieve a balanced result.
In this paper, we noticed that when performing backward prop-agationbasedtraining,suchcontradictoryphenomenonarealso
observableonindividualneuronlevel.Basedonthisobservation,
wepropose FairNeuron ,aDNNmodelautomaticrepairingtool,
to mitigate fairness concerns and balance the accuracy-fairness
trade-offwithoutintroducinganothermodel.Itworksondetecting
neurons with contradictory optimization directions from accuracy
andfairnesstraininggoals,andachievingatrade-offbyselective
dropout.Comparingwithstate-of-the-artmethods,ourapproach
is lightweight, scaling to large models and more efficient. Our eval-
uation on three datasets shows that FairNeuron can effectively
improve all modelsâ€™ fairness while maintaining a stable utility.
KEYWORDS
fairness, path analysis, neural networks
ACM Reference Format:
Xuanqi Gao, Juan Zhai, Shiqing Ma, Chao Shen, Yufei Chen, and Qian
Wang.2022.FairNeuron:ImprovingDeepNeuralNetworkFairnesswith
Adversary Games on Selective Neurons. In 44th International Conference on
Software Engineering (ICSE â€™22), May 21â€“29, 2022, Pittsburgh, PA, USA. ACM,
New York, NY, USA, 13 pages. https://doi.org/10.1145/3510003.3510087
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation
onthe firstpage.Copyrights forcomponentsof thisworkowned byothersthan the
author(s)mustbehonored.Abstractingwithcreditispermitted.Tocopyotherwise,or
republish,topostonserversortoredistributetolists,requirespriorspecificpermission
and/or a fee. Request permissions from permissions@acm.org.
ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
Â© 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 978-1-4503-9221-1/22/05...$15.00
https://doi.org/10.1145/3510003.35100871 INTRODUCTION
Deepneuralnetworks(DNNs)aregraduallyadoptedinawiderange
of applications, including image recognition [ 37], self-driving [ 19],
andnaturallanguageprocessing[ 38,39].Oneofthemosttrendy
applications is decision-making systems, which requires a high
utility DNN with fairness. As examples, artificial intelligent (AI)judge [
8] or human resource (HR) [ 9] try to judge who should
get a loan or interview. These systems should provide objective,
supposedly consistentdecision basedon the givendata, although
thereareoftensocietalbiasinthesedata[ 59].Wewishthesesys-
temscouldcounteractunfairdecisionmadebyhumans,butthey
still exhibit unfair behavior which affects individuals belonging to
specific social subgroups. The COMPAS system is an example. It
predictsrecidivismofpretrialoffenders[ 23],andcontinuestomake
decisions that favor Caucasians compared to African-Americans.Such bias has made very negative societal impacts. Therefore, itis crucial to have systematical methods for automatically fixing
fairness problems in a given DNN model.
Intuitively, fairness problems happen when a model tends to
make different decision for different instances which only differ-
entiated by some sensitive attributes, such as age, race and gender.
Dependingonspecifictasks,theprotectedorsensitiveattributes
can vary. Similarly, there are different fairness notations defined
in existing DNN literature, e.g., group fairness [ 28], individual fair-
ness [26], and max-min fairness [ 36,45]. According to existing
study [23,43], thesefairness definitions arecorrelated with each
other.Inpractice,weusuallyconsiderafewrepresentativeones,i.e.,demographicparity,demographicparityrateandequalopportunity.
In this paper, we also consider these.
Existing DNN training frameworks, e.g., TensorFlow and Py-
Torch [10,52], have provided no support for fairness problems
detectionandfixing.Someotherworkstrytofixothermodelprob-
lems [47,48,72]. There are existing fairness fixing frameworks,
suchasFAD[ 11]andEthicalAdversaries[ 25]thattrytoprovide
suchfunctionality.Basedontheobservationsthatoptimizingac-
curacy and fairness can be contradictory goals in training, these
frameworksintroduceanadversarythatmonitorsthefairnessof
the current training. When fairness issues are detected, they solve
itbyvariousmethods,e.g.,dataaugmentation,thatis,leveraging
theadversarymodeltogenerateadversaryexampleswhichhelpfix the unfair problem and using them as part of the new train-
ingdata.Similartogenerativeadversarynetworks,trainingsuch
9212022 IEEE/ACM 44th International Conference on Software Engineering (ICSE)
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:19:36 UTC from IEEE Xplore.  Restrictions apply. ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA Xuanqi Gao, Juan Zhai, Shiqing Ma, Chao Shen, Yufei Chen, and Qian Wang
anadversarycanbetime-consumingandchallenging.Ithasalot
of practical problems such as mode collapse [ 22], which is hard
to solve. Moreover, such methods usually require using a more
complex model training protocol, which is heavyweight.
We observe that the essential challenge of fixing model fairness
isthatoptimizationonaccuracyonlycanleadtotheselectionofthe
usageofsensitiveattributes.Forexample,anAIHRthatusesthe
sensitiveattributesgenderasanimportantfeaturewillbebiased.
Moreover,suchfeatureselectionhappenincertainneuronsorpaths,
whichisdifferentformtheonesusingallfeaturesordistinguishable
features.Andsuchpaths/neuronstakeasmallportionofthewhole
network, otherwise, the network will have low accuracy for all
samples.Basedonourobservations,weproposed FairNeuron ,a
fairness fixing algorithm that detects and repairs potential DNN
fairness problems. It works by first identify conflict paths with a
neural network slicing technique. Conflict paths refer to the paths
thatcontainalotofneuronsthatselectsensitiveattributestomake
predictions rather than distinguishable ones. Then, we leverage
such paths to cluster samples by measuring if they can trigger the
selection of sensitive attributes. Lastly, we retrain the model by
selectiveretraining.Thatis,forsamplesthatcancausethemodelto
select sensitive attributes as main features to make predictions, we
enforcetheDNNtoreconsiderthisbymutingotherneuronsthat
are not in the conflict paths. By doing so, the conflict path neurons
havetoconsiderallfeatures,otherwise,itwillverylowaccuracy
onothersamples.Thishelpsremovetheimpactsofbiasedsamples,
and fix the fairness problem.
FairNeuron hasbeenimplementedasaself-containedtoolkit.
Our experiments on three popular fairness datasets show that
FairNeuron improvestwicefairnessperformanceandtakesone-
fifthusageoftrainingtimeonaveragethanstate-of-the-artsolution,EthicalAdversaries[
25].Notethat FairNeuron onlyreliesonlight-
weightprocedureslikepathanalysisand dropout,whichmakesit
much more effective and scalable than existing methods.
In summary, we make the following main contributions:
â€¢We propose a novel model fairness fixing frameworks. Itavoids training an adversary model, and does not requiremodifying model training protocol or architecture. It also
features lightweight analysis and fixing, leading to high effi-
ciency repairing.
â€¢Wedevelopaprototype FairNeuron basedontheproposed
idea, and evaluate it with 3 popular public datasets. The
evaluationresultsdemonstratethat FairNeuron caneffec-
tivelyandefficientlyimprovefairnessperformanceofmodels
whilemaintainingastableutility.Onaverage,thefairness
performanceDPcanbeimprovedby57.65%,whichis20%
higher than that of state-of-the-art adversary training based
method, Ethical Adversaries.
â€¢Ourimplementation,configurationsandcollecteddatasets
are available at [32].
Roadmap . This paper is organized as follows. Section Â§2 presents
thenecessarybackgroundonfairnessnotionsandfixingalgorithms.
InSectionÂ§3, wediscuss FairNeuron indetail.Section Â§4shows
ourexperimentsetupandresults.WereviewrelatedworksinÂ§5
and conclude this paper in Section Â§6.ThreattoValidity .FairNeuron iscurrentlyevaluatedon3datasets,
which may be limited. Similarly, there are configurable parameters
used inFairNeuron , and even though our experiments show that
theyaregoodenoughtoachievehighfixingresults,thismaynot
holdwhenthesizeofmodelissignificantlylargerorsmaller.Be-
sides,weassumethatmostsamplesactivatealimitednumberofpaths, and most paths are activated by samples with certain fea-
tures.Thishasbeenobservedbyexistingworks[ 56,63].Wealso
empiricallyvalidatethisassumptioninÂ§4.2.However,itispossible
that this assumption may not hold for some models. To mitigate
thesethreats,alltheoriginalandrepairedtrainingscripts,model
architecture and training configuration details, implementation in-
cluding dependencies, and evaluation data are publicly available
at [32] for reproduction.
2 BACKGROUND AND MOTIVATION
2.1 Fairness
Depending on concrete task specifications, fairness can have dif-
ferent notations [ 21]. These notions can be categorized into two
groups:individualfairness[ 26,67],whichmeasuresif individuals
inthedatasetistreatedequallybythelearnedmodel;andgroupfairness [
28,35], which concerns about whether subpopulation
with different sensitive attributes are treated equally. For example,
foranonlineshoppingrecommendationsystem,allcustomersin
thedatasetshouldbetreatedequally,whichasksforindividualfair-
ness.ForanAIpoweredhiringsystem,applicantswithsensitive
attributes (e.g., different genders) should be treated equally, which
is a typical case of group fairness.
Beforediscussingdifferentfairnessnotations,wefirstdefinea
set of notations. We denote the sensitive attribute as ğ‘†and other
observable insensitive attributes as ğ´. We assume that the subpop-
ulationwith ğ‘†=1isthedisadvantagedgroup,andtheprivileged
group is the subpopulation with ğ‘†=0. Also, we represent the true
label as ğ‘Œ, and the predicted output, i.e., positive/negative as Ë†ğ‘Œ
which is a random variable depending on attributes ğ‘†andğ´.Ë†ğ‘Œ=1
andË†ğ‘Œ=0 are the positive and negative outcomes, respectively.
Followingsuchnotations,wecandefinecommonlyuseddifferent
fairness notations as follows:
Demographicparity(DP) .Demographicparity,orstatisticalpar-
ity,isoneoftheearliestdefinitionsoffairness[ 26].Itviewsfairness
as different subpopulations (i.e., ğ‘†=0 and ğ‘†=0) should have an
equal probability of being classified to the positive label. Formally,
demographic parity measures the probability differences between
different groups:
ğ·ğ‘ƒ=/barex/barexğ‘ƒ(Ë†ğ‘Œ=1|ğ‘†=0)âˆ’ğ‘ƒ(Ë†ğ‘Œ=1|ğ‘†=1)/barex/barex(1)
In an ideal case, we say that a model is when ğ·ğ‘ƒ=0, which
indicates that the prediction output Ë†ğ‘Œand sensitive attribute ğ‘†are
statisticallyindependent.Ifso,theoutputisnotaffectedbythesen-
sitive attribute, and hence the model is not biased towards certain
valuesof thesensitive attributeshowing fairnessinprediction. In
practice, ğ·ğ‘ƒ=0ishardtogetandweviewamodelasfairwhen
ğ·ğ‘ƒâ‰¤ğœ–where ğœ–is a threshold value that is determined by real
world tasks and requirements.
922
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:19:36 UTC from IEEE Xplore.  Restrictions apply. FairNeuron: Improving Deep Neural Network Fairness with Adversary Games on Selective Neurons ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
Demographicparity ratio (DPR) .Demographicparityratio,or
disparateimpact,issimilartodemographicparity.Thekeydiffer-
ence is that it represents the equality or similarly of prediction on
different groups as a ratio (instead of a substitution). Formally, it is
defined as:
ğ·ğ‘ƒğ‘…=ğ‘ƒ(Ë†ğ‘Œ=1|ğ‘†=1)
ğ‘ƒ(Ë†ğ‘Œ=1|ğ‘†=0)(2)
Likedemographicparity, ğ·ğ‘ƒğ‘…=1indicatesafairmodelinthe
ideal case, and in practice, we say a model is fair when ğ·ğ‘ƒğ‘…â‰¥ğœ
where ğœisthefairnessthreshold. Moreover,italsofocuseson the
probability of different groups being classified to the positive label.
ThekeydifferenceisthatDPRmeasuresthedifferencesinaratio.
This is because its origins are in legal fairness considerations for
selectionprocedureswhichtheParetoprinciple,a.k.a.,the80%rule,
iscommonlyused[ 28].Tomakeadirectcomparisonwith80%,DPR
calculated the ratio instead of substitution.
Equalopportunity(EO) .AlimitationofDPandDPRisthatthey
donotconsiderpotentialdifferencesincomparedsubgroups.Equal
opportunity(EO)overcomesthisbymakinguseoftheFPR(false
positive rate) and TPR (true positive rate) between subgroups [ 35].
Formally, EO is defined as:
ğ¸ğ‘‚=/barex/barexğ‘ƒ(Ë†ğ‘Œ=1|ğ‘†=0,ğ‘Œ=1)âˆ’ğ‘ƒ(Ë†ğ‘Œ=1|ğ‘†=1,ğ‘Œ=1)/barex/barex(3)
A model achieves EO fairness when ğ¸ğ‘‚=0, namely, the pre-
diction is (conditional) independent of the sensitive attribute ğ‘†.I n
practice, we say an model is EO fair when ğ¸ğ‘‚â‰¤ğœˆand here, ğœˆis
the fairness threshold value.
Besidesthesediscussednotions,therearemanyotherfairness
definitions,suchasfairnessthroughunawareness(FTU)[ 44],dis-
parate treatment[ 15], disparatemistreatment [ 65], counterfactual
fairness[44],ex-antefairnessandex-postfairness[ 29],etc.Friedler
et al. [30] compareddifferent notations and measuredtheir correla-
tionsontheRicciandAdultdatasets.Resultsshowthatdifferent
notationshavestrongcorrelationswitheachother.Asaresult,most
work usually pick a few representative ones. Following existing
related work, we choose three most common notations, i.e., DP,
DPR [26], and EO [35] in our study.
2.2 Improving DNN Fairness
Many machine learning algorithms including DNNs suffer from
the bias problem. Namely, the model can make a decision based
on wrong attributes. For example, a biased hiring AI may make ad-
missions based on applicantsâ€™ gender information. Such issues can
be caused by the biased training data or the algorithm itself. DNN
hasshowntobeabiasedalgorithm,andpotentiallytrainedDNN
models can make unfair predictions despite its high accuracy. This
canleadtosevereproblemsespeciallywhenDNNsarebecoming
more and more popular including applications like AI judge [ 8],
AIbasedauthentication,AIHR[ 9],etc.Forexample,COMPAS,a
popular system that predicts the risk of recidivism, claimed that
â€œblack people re-offend moreâ€ [ 3]; the first beauty contest robot,
Beauty.AI[ 2],â€œfounddarkskinunattractiveâ€[ 7];andtheMicrosoft
chatbot Tay became a racist and sex-crazed neo-Nazi [ 4]. Biased
AIs in such systems can lead to severe ethical concerns, potentially
threatening our daily life and economy. As a response to this issue,existingworkhasproposedmethodstoimproveDNNfairnessby
removing such bias.
FAD.Adeletal.[ 11]proposedafairadversarialframeworkFAD,
which leverages gradient reversal [ 31] (which acts as an identity
function during forward propagation and multiplies its input by
-1 during back propagation) to fix model fairness problems. The
authors introduced an adversarial network to encode fairness into
themodel:apredictornetwork Fğ‘ƒandanadversarynetwork Fğ´.
The goal of the predictor is to maximize accuracy in Ë†ğ‘Œwhile the
adversarynetworktriestomaximizefairnessinprotectedattribute
ğ‘†. For fairness fixing, we need a new model architecture which
can: (i) predict the true label ğ‘Œ, and (ii) not be able to predict the
sensitive attribute ğ‘†:
ğ¿Fğ‘ƒ=ğ¿ğ¶ğ¸âˆ’ğ›¼ğ¿Fğ´(4)
where ğ¿Fğ‘ƒ,ğ¿ğ¶ğ¸,ğ¿Fğ´denote the predictor loss, predictor logistic
loss (a.k.a., CE loss) and the adversary logistic loss, respectively.
The hyperparameter ğ›¼regulates the accuracy-fairness trade off.
After that, it uses a post-training process to align TP (true positive)
and FP (false positive) across all classes by adjusting class-specific
thresholdvaluesoflogitswithaROCanalysisintroducedbyHardt
et al. [35].Ethical Adversaries
. Delobelle et al. [ 25] proposed the ethical ad-
versaries framework to solve the fairness problem. The framework
hastwoparties,theexternaladversary,a.k.a.,thefeeder,andthe
reader, which represents the protected attribute ğ‘†. It is an iterative
trainingprocedures,duringwhicheachpartyinteractswitheach
other. The readeris trained with thetarget label atthe same time,
and each time, it evaluates if the training has bias or not. If so, it
propagatestherelatedgradientbacktothenetwork.Thefeedercan
beviewedas adataaugmenterwhichperforms evasionattacksto
find adversarial examples that can be used in the adversarial train-
ing.Duringthisadversarialtraining,thetargetlabel(i.e.,maintask
ofthemodel)andthefairnessgoalisadjustedbyahyperparameter
ğœ†, which is similar to the FAD framework.
Pre-/Post-processingMethods .FADandEthicalAdversariesare
online methods which solves the fairness issue during training.
There are other methods that leverages pre-processing or post-
processing to solve this problem, e.g., reweighing [ 41], and re-
ject optionclassification (ROC[42]). Reweighing assignsdifferent
weights to input samples in different group to make the dataset
discrimination-free(pre-processing).ROCgivesfavorableoutcomes
to unprivileged groups and unfavorable outcomes to privilegedgroups in a confidence band around the decision boundary with
the highest uncertainty (post-processing).
2.3 Motivation and Basic Idea
Limitations of existing work. Existing work has a few limita-
tions. Firstly, they introduce another model as the adversary in the
trainingprocedure.Inheritingfromexistingadversarialnetworks,
training such models is not easy. Problems like mode collapse [ 22],
failingtoconverge[ 50],andvanishinggradientsarequitecommon
in such a model structure. This will require extra efforts in solv-ing such problems. Secondly, there is no guarantee that trainingsuch adversary networks willalways converge fornow. There is
atheoretical guaranteethatGAN (generativeadversarynetwork)
923
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:19:36 UTC from IEEE Xplore.  Restrictions apply. ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA Xuanqi Gao, Juan Zhai, Shiqing Ma, Chao Shen, Yufei Chen, and Qian Wang
willconverge,despiteitspracticaldifficulty.Asaminimaxgame,
trainingsuchGANmodelswillconvergewhenitachievestheNash
equilibrium [ 33]. However, FAD and Ethical Adversaries empiri-
callyobservethatmodelaccuracyandfairnessmayconflictwith
each other in some cases and may not conflict with each other
in other cases. On one hand, this shows that there exist models
that are both accurate and fair. On the other hand, it also indicates
thatthedesignedadversarytrainingisnotazero-sumgame,and
there is no guarantee to show the existence of Nash equilibrium
in thisgame. Asa result,existing workcan failto convergewhen
trainingthemodelbecausethegamehasnosolutions.Empiricalre-sultsconfirmthisconclusion.Â§4.2reportsthatFADmayexacerbate
fairnessproblem.AsTable4shown,FADresultsindecreasingof
DPRonCensusandincreasingofEOonCOMPAS,whichmeanthe
fairnessproblemshasnotbeenmitigatedfromtheseperspectives.
Elazar and Goldberg made an empirical observation on leakage of
protectedattributesforneuralnetworkstrainedontextdata,which
can also demonstrate this conclusion [27].
WhybiashappensinaDNNtraining? Based on existing litera-
tures and our experiences, we make a few key observations that
are important for us to develop our method.
Observation I:Optimizing accuracy anddifferentfairnessobjectives
canbecontradictory toeachother,butnotalways.Existingwork[ 11,
25]hasshownthataccuracyanddifferentfairnessgoals(e.g.,DP,
DPR and EO), including different fairness goals themselves, can be
contradictorytoeachother.Thisisthereasonwhysomemodels
with high accuracy are highly biased: when optimizing during
training,directionswithhigheraccuracygainmaybecontradictory
todirectionswithhigherfairness.Thegoodnewsisthatexisting
workhasempiricallydemonstratethatitispossibletotrainamodel
with high accuracy and fairness at the same time [25].
Observation II:Aneuronrepresents acombination ofdifferentfeatures,
andmodelbiasindicates thatthemodelfocusesoncertainfeatures
thatitshouldnot.AsageneralunderstandingofDNNs,eachneuron
inthenetworkisextractingfeaturesfromtheinput.Formtheinputlayer to the final prediction layer, the extracted features are becom-
ingmoreandmoreabstract.Eachneuronisrepresentingasetof
featuresitreceivesfromthepreviouslayer,andweightscanhelp
it determine which set of features are more important compared
withothers.Amodelisbiasedindicatesthatamodelisfocusingon
thewrongfeatures,e.g,AIjudegesshouldbeaffectedbysenstive
attributeslikegenders.Forexample,ahiringAIisbiasedongender
whenitselectsgenderfeatureratherthanothersasanimportant
factortodecideifacandidatecangetaninterview.Noticethatsuch
importance is represneted by weights in the DNN.
Now, we can use our observations to explain why bias happens
in training a deep neural network. When training a DNN, the opti-
mizertriestopickimportantfeaturesbasedongradientinformation.
When updating individual neurons, it may encounter cases where
the fairness and accuracy optimization subjects are pointing to
differentdirections.Ifitonlyconsidersaccuracyasitstraininggoal,
it will select the direction that optimizes the accuracy the most
which can lead to low fairness. Furthermore, we know that the
gradient information is calculated based on given samples. If we
are able to detect such samples, we can potentially fix the problem
by enforcing the optimizer to pick the correct set of features.Ouridea. Basedonourobservations,wearguethatitisnotneces-
sarytointroduceanadversarythatdetectsthepotentialconflictsof
optimizing the accuracy and fairness. Instead, we first monitor the
training process to detect neurons whose accuracy and fairness op-
timization get conflicts with each other. Then, we identify samples
that causes such contradictory optimizations. Lastly, we enforce
the optimizer to decide a balanced optimal direction that optimizes
both accuracy and fairness. By doing so, we remove the need of
introducinganadversary.Itsimplifiesthetrainingprocedureand
is more lightweight compared with existing solutions.
3 DESIGN
3.1 Overview
Workflow. Figure1presentstheworkflowof FairNeuron .Ittakes
abiasedmodel anditstrainingdata asinputs,andoutputsa fixed
model.Firstly, FairNeuron performs neuralnetworkslicing,which
detects neurons and paths that have contradictory optimization
directions.NoticethatbecauseofthedenseconnectionsofDNN,
suchneuronsaretypicallyconnectedwitheach,passingthebiased
features from one layer to the next. As such, we do this in the path
granularity. In this step, we leverage a neuron slicing technique
whichperformsadifferentialanalysistoidentifythetargetpaths.
Next, we leverage such paths to identify the samples that cause
such effects, known as the sample clustering. After this step, we
canseparatethesamplesintotwoclusters,biaseddatasamplesand
benignsamples.Lastly,weperform selectiveretraining toenforce
the model to learn unbiased features. Essentially, for samples in
differentclusters,wehavedifferenttrainingstrategies.Forsamples
in the benign data cluster, we do not change anything, while for
samples in the biased cluster, we enforce detected neurons to con-
sideralargersetoffeaturesandweighthemtolearn allfeatures
that are important for prediction rather than the biased ones.
Algorithm. The overall algorithm of FairNeuron is presented
inAlgorithm1,denotedasProcedure FairNeuron .Asmentioned
before, it takes a biased model BiasedModel and training dataset
TrainDataset as inputs, and outputs a fixed model, referred to as
NewModel in the algorithm. In the main algorithm, FairNeuron
analyze the relationship between dataset and model by gettingactivation paths of each input sample (line 1-5). After acquiring
pathinformation, FairNeuron groupsthetrainingdatasetintotwo
parts(line6).Thefirstoneisconsistsofbenignsampleswhoseacti-
vationpathsareclusteredbysamples,andthesecondoneisconsists
ofbiasedsamplesandcorrespondingpaths.Then FairNeuron per-
formsdifferent trainingstrategies onthem, itdeactivates dropout
layers when training benign samples and activated them when
training biased samples.
3.2 Neural Network Slicing
InNeuralNetworkSlicing,wetrytofindpathsandneuronsthat
containtheoptimizerfindscontradictoryoptimizationdirections
foraccuracyandfairness.Figure2showstheneuralnetworkslicingmethodof
FairNeuron .Theinputofthisalgorithmisthetraining
dataset and the biased model to fix, a neural network which hasalready learned the weights based on a training dataset. We will
use this example to demonstrate how it works in this section.
924
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:19:36 UTC from IEEE Xplore.  Restrictions apply. FairNeuron: Improving Deep Neural Network Fairness with Adversary Games on Selective Neurons ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA

	



	

	















		

		

	

	




		



Figure 1: Overview of FairNeuron.
First,FairNeuron getstheaveragebehaviorofaneuron,bylever-
aging its activation values. The behavior of a neuron can be repre-
sented in many ways, and in FairNeuron , we use the most simple
andnaiverepresentation,itsaveragedactivationvalue.Specifically,
FairNeuron calculatestheaverageactivationvaluesoftheneuron
for a given dataset, which is usually the training dataset.
Then,FairNeuron performsa forwardanalysis tounderstand
the diversity of a neuron behavior. Similar to the first step, we also
use the activation values of a neuron to represent the behavior
oftheneuron.Inthisstep,wefeedindividualinputstotheDNN,
and record the activation value differences between the average
activation and the value for this concrete input. By doing so, we
canestimatethecontributionsofeachneurontotheoutputfora
given sample. This helps us to identify neurons that contain biased
features.
Afterwards,weobtainpathsthatcontainbiasedfeatures.Notice
that a DNN is a highly connected network, and as a layered struc-
ture, behaviors of a single layer will be passed to the next layer.
Becauseofthis,biasedfeatureswillbeaccumulatedinthisnetwork,
and asa result,neurons inthe last fewlayers willcontribute alot
tothebiasedprediction.Ontheotherhand,theseneuronsdonot
denote the root cause of such bias. To completely fix the neuron
network,itisimportanttoidentifythewholechainofsuchprop-
agation. So we comprehensively consider neurons and synapses
andcalculatetheircontributions,andbacktrackthesecontributions
in the network. We show the detail of this phase in the proce-dureGetActivationPath in Algorithm 1. Starting from the output
neuron,weiterativelycomputethecontributionsoftheprevious
neurons (line 19-22), which is similar to the backward propagation.
Then,wesortthemindescendingorder(line23),andaddthekey
synapses and corresponding neurons into the path set (line 24-29).
Todetermineifasynapseisakeysynapseornot,weneedtocalcu-
late whether the sum of all the synapses that are connected to the
samesuccessorneuronisstilllessthanthethreshold.Thethreshold
isdeterminedbytheactivationvalueofsubsequentneuronsand
the hyperparameter ğ›¾.
Lastly, we identify conflict paths, namely paths that contain fea-
tures causing the biased prediction. Based on our observations, weknowthatwhenmakingpredictions,themodelusesthebenignfea-
ture set to make predictions for benign samples and use the biased
feature set to make predictions for biased samples. Consideringthat a neuron represents a set of features, we know that biasedsamples are activating neurons/paths that are different from theothers. Notice that biased paths/neurons takes a relatively small
portion of the whole neural network. Otherwise, the network will
makepredictionsonalotofbiasedfeatures,leadingtolowaccuracy.
Based on this intuition, we obtain such conflict paths by analyzing
the frequencyof the activatedpaths. More specifically,weset the
activationfrequencyofthemostfrequentlyactivatedpathasthe
standard, and compare the activation frequency of each path with
it. If the activation frequency of a certain path is less than a cer-tain percentage of the standard (determined by
ğœƒ), then it can be
considered as a conflict path.
Example. Assume the biased model is a simple neural network
showninFigure2.Theweightvalueshavebeenlabeledonthecor-respondingsynapsesinFigure2(a).Firstweperformpathprofiling,
and the results are set to 0 for simplicity. Then we feed a sample
(3,1)intothemodel,andcalculateitsrelativeactivationvalueon
eachneuron.Takethetopneuronofthesecondlayerasanexample,
its relative activation value is 3 Ã—2+1Ã—(âˆ’1)âˆ’0=5, as shown
inFigure2(c).Finallywebacktrackthecontributionsofsynapses
to get the activation path. Figure 2(d)-(f) shows how we get a path
iteratively.Letusdenotethe ğ‘˜-thneuroninthe ğ‘š-thlayeras ğ‘›ğ‘š
ğ‘˜.
At first ğ‘„=ğ‘›4
2, assuming ğ›¾=0.8, we add ğ‘›3
2intoğ‘„/primeand do not
addtheothersbecause |6Ã—2|>|0.8Ã—9|.Thenwelet ğ‘„=ğ‘„/prime=ğ‘›32,
andadd ğ‘›2
3since|6Ã—2|>|0.8Ã—6|.Lastweadd ğ‘›11andğ‘›12intoğ‘„/prime
because|3Ã—1â‰¤0.8Ã—6|and|3Ã—1|+|1Ã—3|>|0.8Ã—6|.Ultimately
we get all the paths iteratively. The conflict paths detection can
be regarded as the preceding step of sample clustering, and the
example in Â§3.3 shows its process. /square
3.3 Sample Clustering
Thesampleclusteringaimstomeasuretheimpactofinputsamples
onfairness.Afterdetectingconflictpaths,wecandistinguishthese
neurons exhibiting biased behavior. We handle the corresponding
925
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:19:36 UTC from IEEE Xplore.  Restrictions apply. ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA Xuanqi Gao, Juan Zhai, Shiqing Ma, Chao Shen, Yufei Chen, and Qian Wang































	
















	















	















	












  
 


  
! 
"#$%
	
	

	
 &%'
	




(&%'

	


		

	


(&%'



		


(&%'



		

	
	


	

Figure 2: Abnormal path detection example.
samples of these neurons (denoted as biased sample ) to improve
their fairnessperformance. Since werecorded the relationshipbe-
tweenpathsandsamples(line4inAlgorithm1),wecaneasilyfind
thesecorrespondingsamplesandgetthetrainingdatasetdivided
into two groups.
As shown in Algorithm 1, ğ‘ƒğ‘ğ‘¡â„ğ¿ğ‘–ğ‘ ğ‘¡ is a list which contains each
path and its corresponding activation samples. First, we count the
totalnumberofpathâ€™scorrespondingactivationsamplesonebyone,
andthenumberisdenotedas activationfrequency (line34).Second,
wesorttheactivationfrequencylistwegetabove,andrecordits
maximum as ğ‘€(line 35). Third, we check whether these pathsâ€™
activation frequencies are greater than the threshold ğœƒÃ—ğ‘€.W e
denote the paths which donot meet the above condition as biased
path, and denote their corresponding samples as biased sample.
After we detecting the biased paths, these biased samples can be
separated from ordinary samples (line 36-40).
Example. Suppose our training dataset has 40 samples, as shown
in Figure 3. We feed the training dataset into the biased model,
and obtain 4 different paths A, B, C and D based on the procedure
GetActivationPath in Algorithm 1. Then we count the number of
samplesactivatingthese4paths,andget25,10,3,and2forA,B,
C, and D, respectively. Weassume that ğœƒ=0.3, so the threshold is
25Ã—0.3=7.5 since the maximum of path activation statistics is 25.
Then,pathCandDwillbothbeclassifiedasbiasedpaths,which
resultsin3samplesactivatingpathCand2samplesactivatingpath
D being grouped in biased samples. /square

 

	

	






	

	

	


		
Figure 3: Sample clustering example.
3.4 Selective Training
Finally, we perform ordinary training on the ordinary samples and
dropouttrainingonbiasedsamplesobtainedabove.Wedonotneed
tochangethe modelstructure,onlyneedto changetheactivation
stateofthedropoutlayers.Ordinarytrainingmeansdeactivating
thedropoutlayersfortraining.Withthecurrenttrainingsystem,we
can activate dropout layers when training on these biased samples
and vice versa. By performing dropout training on these biased
neurons,weenforcethemtolearnmoreunbiasedfeaturesrather
than biased ones to mitigate the fairness problems.
926
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:19:36 UTC from IEEE Xplore.  Restrictions apply. FairNeuron: Improving Deep Neural Network Fairness with Adversary Games on Selective Neurons ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
Algorithm 1 FairNeuron Algorithm
Input: ğµğ‘–ğ‘ğ‘ ğ‘’ğ‘‘ğ‘€ğ‘œğ‘‘ğ‘’ğ‘™ : a biased model to fix
Input: ğ‘‡ğ‘Ÿğ‘ğ‘–ğ‘›ğ·ğ‘ğ‘¡ğ‘ğ‘ ğ‘’ğ‘¡ : training dataset
Output: ğ‘ğ‘’ğ‘¤ğ‘€ğ‘œğ‘‘ğ‘’ğ‘™ : trained model after fixing
1:procedure FairNeuron
2: ğ‘ƒğ‘ğ‘¡â„ğ¿ğ‘–ğ‘ ğ‘¡ â†[ ]
3:forğ‘ ğ‘ğ‘šğ‘ğ‘™ğ‘’âˆˆğ‘‡ğ‘Ÿğ‘ğ‘–ğ‘›ğ·ğ‘ğ‘¡ğ‘ğ‘ ğ‘’ğ‘¡ do
4: ğ‘ƒâ†ğºğ‘’ğ‘¡ğ´ğ‘ğ‘¡ğ‘–ğ‘£ğ‘ğ‘¡ğ‘–ğ‘œğ‘›ğ‘ƒğ‘ğ‘¡â„ (ğµğ‘–ğ‘ğ‘ ğ‘’ğ‘‘ğ‘€ğ‘œğ‘‘ğ‘’ğ‘™,ğ‘ ğ‘ğ‘šğ‘ğ‘™ğ‘’,ğ›¾ )
5: ğ‘ƒ. ğ‘ ğ‘ ğ‘šğ‘ğ‘™ğ‘’ â†ğ‘ ğ‘ğ‘šğ‘ğ‘™ğ‘’
6: ğ´ğ‘ğ‘ğ‘’ğ‘›ğ‘‘(ğ‘ƒğ‘ğ‘¡â„ğ¿ğ‘–ğ‘ ğ‘¡,ğ‘ƒ )
7: ğ‘‚,ğ‘†â†ğºğ‘’ğ‘¡ğ‘†ğ‘ğ‘šğ‘ğ‘™ğ‘’ğ‘ ğ·ğ‘–ğ‘£ğ‘–ğ‘‘ğ‘’ğ‘‘ (ğ‘ƒğ‘ğ‘¡â„ğ¿ğ‘–ğ‘ ğ‘¡,ğœƒ )
8: ğ‘ğ‘’ğ‘¤ğ‘€ğ‘œğ‘‘ğ‘’ğ‘™ â†ğµğ‘–ğ‘ğ‘ ğ‘’ğ‘‘ğ‘€ğ‘œğ‘‘ğ‘’ğ‘™
9:forğ‘œâˆˆğ‘‚do
10: ğ‘‚ğ‘Ÿğ‘‘ğ‘–ğ‘›ğ‘ğ‘Ÿğ‘¦ğ‘‡ğ‘Ÿğ‘ğ‘–ğ‘›ğ‘–ğ‘›ğ‘” (ğ‘ğ‘’ğ‘¤ğ‘€ğ‘œğ‘‘ğ‘’ğ‘™,ğ‘œ )
11:forğ‘ âˆˆğ‘†do
12: ğ·ğ‘Ÿğ‘œğ‘ğ‘œğ‘¢ğ‘¡ğ‘‡ğ‘Ÿğ‘ğ‘–ğ‘›ğ‘–ğ‘›ğ‘” (ğ‘ğ‘’ğ‘¤ğ‘€ğ‘œğ‘‘ğ‘’ğ‘™,ğ‘  )
return ğ‘ğ‘’ğ‘¤ğ‘€ğ‘œğ‘‘ğ‘’ğ‘™
Input: ğ‘€ğ‘œğ‘‘ğ‘’ğ‘™: model to analyze
Input: ğ‘†ğ‘ğ‘šğ‘ğ‘™ğ‘’: samples used in analyze
Input: ğ›¾: hyperparameter to determine the activation of neurons
Output: ğ‘ƒ: path activated
13:procedure GetActivationPath
14: ğ‘ƒâ†âˆ…
15: ğ‘„â†âˆ…
16: ğ‘„â†ğ‘‚ğ‘¢ğ‘¡ğ‘ğ‘¢ğ‘¡ğ‘ğ‘’ğ‘¢ğ‘Ÿğ‘œğ‘›
17:while ğ‘„â‰ âˆ…do
18: ğ‘„/primeâ†âˆ…
19: forğ‘âˆˆğ‘„do
20: ğ‘â†ğºğ‘’ğ‘¡ğ‘ƒğ‘Ÿğ‘’ğ‘ğ‘’ğ‘¢ğ‘Ÿğ‘œğ‘› (ğ‘)
21: forğ‘›âˆˆğ‘do
22: ğ¶ğ‘œğ‘›ğ‘¡ğ‘Ÿğ‘–ğ‘ğ¿ğ‘–ğ‘ ğ‘¡ [ğ‘›]â†ğ¶ğ‘œğ‘šğ‘ğ‘¢ğ‘¡ğ‘’ğ¶ğ‘œğ‘›ğ‘¡ğ‘Ÿğ‘–ğ‘ (ğ‘›)
23: ğ‘†ğ‘œğ‘Ÿğ‘¡ğ‘’ğ‘‘ğ¿ğ‘–ğ‘ ğ‘¡ â†ğ‘†ğ‘œğ‘Ÿğ‘¡(ğ¶ğ‘œğ‘›ğ‘¡ğ‘Ÿğ‘–ğ‘ğ¿ğ‘–ğ‘ ğ‘¡ )
24: ğ‘†ğ‘¢ğ‘šâ†0
25: forğ‘–â†0ğ‘¡ğ‘œ ğ‘™ğ‘’ğ‘›(ğ¶ğ‘œğ‘›ğ‘¡ğ‘Ÿğ‘–ğ‘ğ¿ğ‘–ğ‘ ğ‘¡ )do
26: ifğ‘†ğ‘¢ğ‘šâ‰¤ğ›¾Ã—ğ‘.ğ‘£ğ‘ğ‘™ğ‘¢ğ‘’then
27: ğ‘†ğ‘¢ğ‘šâ†ğ‘†ğ‘¢ğ‘š+ğ‘†ğ‘œğ‘Ÿğ‘¡ğ‘’ğ‘‘ğ¿ğ‘–ğ‘ ğ‘¡ [ğ‘–].ğ‘£ğ‘ğ‘™ğ‘¢ğ‘’
28: ğ‘„/primeâ†ğ‘„/primeâˆ©ğ‘†ğ‘œğ‘Ÿğ‘¡ğ‘’ğ‘‘ğ¿ğ‘–ğ‘ ğ‘¡ [ğ‘–].ğ‘–ğ‘›ğ‘‘ğ‘’ğ‘¥
29: ğ‘ƒâ†ğ‘ƒâˆ©(ğ‘†ğ‘œğ‘Ÿğ‘¡ğ‘’ğ‘‘ğ¿ğ‘–ğ‘ ğ‘¡ [ğ‘–].ğ‘–ğ‘›ğ‘‘ğ‘’ğ‘¥, ğ‘.ğ‘–ğ‘›ğ‘‘ğ‘’ğ‘¥ )
30: ğ‘„â†ğ‘„/prime
return ğ‘ƒ
Input: ğ‘ƒğ‘ğ‘¡â„ğ¿ğ‘–ğ‘ ğ‘¡: a list of paths used to cluster samples
Input: ğœƒ: hyperparameters used to find conflict paths
Output: ğ‘‚ğ¿,ğ´ğ¿: benign and biased samples, respectively.
31:procedure GetSamplesDivided
32: ğ‘‚ğ¿â†[ ]
33: ğ´ğ¿â†[ ]
34: ğ‘ƒğ‘ğ‘¡â„ğ¿ğ‘–ğ‘ ğ‘¡.ğ‘ğ‘œğ‘¢ğ‘›ğ‘¡ â†ğ¶ğ‘œğ‘¢ğ‘›ğ‘¡(ğ‘ƒğ‘ğ‘¡â„ğ¿ğ‘–ğ‘ ğ‘¡.ğ‘ ğ‘ğ‘šğ‘ğ‘™ğ‘’ğ‘  )
35: ğ‘€â†ğ‘€ğ‘ğ‘¥(ğ‘ƒğ‘ğ‘¡â„ğ¿ğ‘–ğ‘ ğ‘¡.ğ‘ğ‘œğ‘¢ğ‘›ğ‘¡ )
36:forğ‘–â†0ğ‘¡ğ‘œ ğ‘™ğ‘’ğ‘›(ğ‘ƒğ‘ğ‘¡â„ğ¿ğ‘–ğ‘ ğ‘¡)do
37: ifğ‘ƒğ‘ğ‘¡â„ğ¿ğ‘–ğ‘ ğ‘¡[ğ‘–].ğ‘ğ‘œğ‘¢ğ‘›ğ‘¡â‰¤ğœƒÃ—ğ‘€then
38: ğ´ğ‘ğ‘ğ‘’ğ‘›ğ‘‘(ğ‘‚ğ¿, ğ‘ƒğ‘ğ‘¡â„ğ¿ğ‘–ğ‘ ğ‘¡ [ğ‘–].ğ‘ ğ‘ğ‘šğ‘ğ‘™ğ‘’ğ‘ )
39: else
40: ğ´ğ‘ğ‘ğ‘’ğ‘›ğ‘‘(ğ´ğ¿, ğ‘ƒğ‘ğ‘¡â„ğ¿ğ‘–ğ‘ ğ‘¡ [ğ‘–].ğ‘ ğ‘ğ‘šğ‘ğ‘™ğ‘’ğ‘ )
return ğ‘‚ğ‘Ÿğ‘‘ğ‘–ğ‘›ğ‘ğ‘Ÿğ‘¦ğ¿ğ‘–ğ‘ ğ‘¡,ğ´ğ‘ğ‘›ğ‘œğ‘Ÿğ‘šğ‘ğ‘™ğ¿ğ‘–ğ‘ ğ‘¡Table 1: Experimented DNN models.
Dataset Model Accuracy
Census 3 Hidden-layer Fully-connected NN 83.9%
Credit 3 Hidden-layer Fully-connected NN 73.4%COMPAS 3 Hidden-layer Fully-connected NN 62.1%
4 EVALUATION
4.1 Experiment Setup
4.1.1 Hardware andsoftware. We conducted ourexperiments ona
GPU server with 32 cores Intel Xeon 2.10GHz CPU, 256 GB system
memoryand1NVIDIATITANVGPUrunningtheUbuntu16.04
operating system.
4.1.2 Datasets. Weevaluatedourmethodonthreepopulardatasets:
the UCI Adult Census, COMPAS, and German Credit.
â€¢UCI Adult Census. The UCI Adult Census was extracted
from the 1994 Census bureau database, gathering 32,561
instancesrepresentedby9featuressuchasage,education
and occupation. The gender is considered as the sensitive
attribute.
â€¢COMPAS. The COMPAS system is a popular commercial
algorithmusedbyjudgesforpredictingtheriskofrecidivism,
and the COMPAS dataset is a sample outcome from the
COMPASsystem.Theraceofeachdefendantisthesensitive
attribute.
â€¢German Credit. This is a small dataset with 600 records
and20attributes.Theoriginalaimofthedatasetistogive
anassessmentofindividualâ€™screditbasedonpersonaland
financial records. The gender is the sensitive attribute.
4.1.3 Models. Inourexperiment,webuiltafully-connectedneural
network with three hidden layers for each dataset respectively. For
theCOMPASandtheGerman Creditdataset,eachhiddenlayeris
composedof32neurons,whilefortheUCIAdultCensusdataset,
each hidden layer is composed of 128 neurons due to its larger
encodedinput.Thedetailsofthemodelsusedintheexperiments
are shown in Table 1.
We use the softmax activation function for Census and German
Credit to achieve binary classification, and the linear activation
functionforCOMPAStogetrecidivismscores.Werandomlysep-
arate the dataset into the training, validation, and test sets, by a
ratio of 7:1:2, respectively. The neural network is trained by the
Adam optimizer with ğ›½1=0.9,ğ›½2=0.9999, and initial learning
rateğ‘™ğ‘Ÿ=0.01, which is scheduled by a factor of 0.1 when reaching
a plateau.
4.1.4 Hyperparametertuning. Toobtainthesuitablehyperparam-
eters ğœƒandğ›¾, we run a parallel grid search for hyperparameters
to optimize training loss function. We sample ğœƒbetween the in-
terval[10âˆ’4,1]proportionally, and sample ğ›¾between the interval
[0.5,1].Followingthestandardpracticeinmachinelearning,the
gridsearchisperformedonasmallsubsetdrawnfromthetraining
set in a certain proportion (e.g., 10%), and we utilize the ray tune
toolto perform it automatically [6].
927
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:19:36 UTC from IEEE Xplore.  Restrictions apply. ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA Xuanqi Gao, Juan Zhai, Shiqing Ma, Chao Shen, Yufei Chen, and Qian Wang
4.1.5 Metrics and baseline methods. We compare our algorithm
with other popular in-processing state-of the-art fixing algorithms,
such as FAD [ 11] and Ethical Adversaries [ 25]. Besides, we also
comparedwiththerepresentativealgorithmsoftheothertwokinds
of fixing algorithms, reweighing in pre-processing [ 41] and Reject
Option Classification in post-processing [42].
We aim to answer the following research questions through our
experiments:
RQ1: How effective is our algorithm in fixing bias model?RQ2: How efficient is our algorithm in fixing bias model?RQ3: How parameters affect model performance?RQ4: How our algorithm perform in image datasets?
4.2 Effectiveness of FairNeuron
ExperimentDesign: Toevaluatetheeffectivenessof FairNeuron ,
wetestthefollowingmodels:thenaivebaselinemodel,modelsfixed
by FAD, by Ethical Adversaries, and by FairNeuron . Due to the
randomness in these experiments, we ran the training 10 times to
ensurethereliabilityofresultsandenforcedthesefixingalgorithms
tosharethesameoriginaltrainingdataset.Tomeasuretheeffec-tiveness of
FairNeuron , we compare the performance between
FairNeuron and the other algorithms in terms of both utility and
fairness. To demonstrate the effectiveness of the three components
ofFairNeuron (i.e. neural network slicing, sample clustering and
selectivetraining),weconductedadetailedcomparisonbetween
our algorithm and other popular works.Results:
The details of the comparison results are presented in
Table4.Thefirstcolumnliststhethreedatasets.Thesecondcolumn
shows the different algorithm. The third column lists the utility,and the remaining columns list the fairness criteria. The model
utilitiesareevaluatedbybinaryclassificationaccuracy(Acc),and
thefairnessperformancearemeasuredbydemographicparity(DP),
demographicparityratio(DPR),andEqualopportunity(EO).The
best results are shown in bold.
Analysis: The experimental results demonstrate the effectiveness
of our algorithm. Firstly, FairNeuron can effectively fix the fair-
ness bias of all models trained on different datasets. Secondly,
FairNeuron achieves the highest utility among all models with
fairness constraints, and even surpasses the naive model on COM-
PAS and Credit.
Table4showsthefairnessimprovementofnaivemodelsonCen-
sus,CreditandCOMPAS,respectively. FairNeuron improvesDPR
by 98.47%, 157.23%, and 3895.23%, mitigates fairness problem by
69.69%,21.12%and38.95%intermsofEO,and74.68%,2.08%,96.19%
in terms of DP. Compared with the other algorithms, FairNeuron
achieves the best fairness performance on Census and COMPAS.However, the EO and DP results of
FairNeuron on Credit is not
satisfactory.Afterourcarefulanalysis,wefoundthatourneuron
network slicing is not fully functional since Credit only has 600
instances.Thus,howtoimprovetheutilityofmodelstrainingon
such small datasets will be one of our future works.
Besides,Table4demonstratesthat FairNeuron haslittleimpact
on model utility after a successful fairness fixing, and even has
theadvantageofincreasingaccuracybyfixingfairnessproblems.
Theaverageutilityof FairNeuron isthehighestamongallmodels
withfairnessconstraints,whichexceedsROCby27.9%,ReweighingTable 2: Random clustering vs. our clustering
Method Acc DP DPR EO
Random 0.749 0.325 1.89 0.159Ours 0.799 0.013 1.02 0.058
by 17.5% , Ethical Adversaries by 3.85% and FAD by 27.22%, and
evensurpassesthenaivemodelontheGermanCreditandCOMPASdatasets.Thedetailedaverageaccuracychangeis-0.83%,1.36%,and
28.66%. We found that it is mainly because FairNeuron improves
the utility by mitigating the overfitting problem in model training
procedures, and the size of Census dataset is relatively large, so its
overfitting problem is unobvious.
In summary, FairNeuron can effectively fix the bias training
procedures, and has little impact on the model utility while im-
proving the fairness performance significantly. Then we prove the
effectiveness of each step in FairNeuron separately.
4.2.1 Effectiveness of neuron network slicing. Figure 4 shows a
example of the distribution of abnormal paths. Here, the maximum
of path activation statistics is 47, and we assume ğœƒ=0.03, so the
threshold is 1.41,as the green lineshows. We can seethat most of
non-zero paths are concentrated near 1, but the proportion of their
correspondingsamplesisnothigh.Thesepathsaretheabnormal
paths detected by our approach.
Figure 4: Result of neuron network slicing. The blue step line
presents the probability density function of path activation statis-tics,theredlinepresentstheaccumulationofsampleratio,andthegreen line presents the threshold.
4.2.2 Effectivenessof sampleclustering. Todemonstrate theeffec-
tiveness of sample clustering, we compare the fixing performance
betweenoursampleseparationandtherandomclusteringmethods.
We set the number of randomly-obtained abnormal samples to be
the same as that of FairNeuron.
Table 2 reports the performance of different clustering methods.
With our method, the average accuracy is improved by 6.68%, and
the fairness performance has also been greatly improved, which
are 96.19%, 97.67% and 38.95% of DP, DPR and EO, respectively.
928
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:19:36 UTC from IEEE Xplore.  Restrictions apply. FairNeuron: Improving Deep Neural Network Fairness with Adversary Games on Selective Neurons ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
Table 3: Comparison among dropout, ordinary and selective train-
ing.
Training approach Acc DP DPR EO
Ordinary 0.575 0.733 0.183 0.683
Dropout 0.621 0.341 1.860 0.095
Selective 0.799 0.013 1.021 0.058
Table 4: Results on the three datasets. Best results are in bold.
Dataset Model Acc DP EO DPR
CensusNaive model 0.839 0.079 0.102 0.609
ROC 0.597 0.044 0.051 0.773
Reweighing 0.719 0.059 0.0141 1.497
FAD 0.612 0.059 0.061 0.518
Ethical Adversaries 0.814 0.031 0.179 0.784
FairNeuron 0.832 0.020 0.031 0.869
CreditNaive model 0.734 0.048 0.142 0.407
ROC 0.646 0.041 0.073 1.273
Reweighing 0.632 0.067 0.066 0.828
FAD 0.710 0.000 0.000 inf
Ethical Adversaries 0.715 0.041 0.031 2.442
FairNeuron 0.7440.047 0.112 0.834
COMPASNaive model 0.621 0.341 0.095 1.860
ROC 0.618 0.083 0.069 0.890
Reweighing 0.671 0.193 0.176 1.406
FAD 0.567 0.057 0.114 0.926
Ethical Adversaries 0.759 0.095 0.095 1.203
FairNeuron 0.799 0.013 0.058 1.021
4.2.3 Effectiveness of selective training. To show the effectiveness
ofselectivetraining,weprovideacomparisonamongpuredropout,
pure ordinary and selective training.
Table 3 presents the results of different training approaches.
Selectivetrainingsurpassestheordinarytrainingby38.96%,98.22%,
97.43% and 91.50%, while surpassing the pure dropout trainingby 22.27%, 96.19%, 97.55% and 38.95% in Acc, DP, DPR and EO,
respectively. It confirms that the selective training in FairNeuron
can achieve high accuracy and fairness.
4.3 Efficiency of FairNeuron
Experiment Design: To evaluate the efficiency of FairNeuron ,
we measured the time usage of ordinary training, Ethical Adver-
sariesand FairNeuron trainingonallthreedatasets.Weperformed
10trialswhichusesrandomtraining/testdatasplitting,naivemodel
training,hyperparametertuningandmodelrepairing(forEthical
Adversariesand FairNeuron )andcomputedtheaverageoverhead
toavoidrandomness.Table5presentshowmuchtimeittakesto
complete its fixing for each method. For Ethical Adversaries, it
shows the time for per iteration in 50 iterations. For FairNeuron ,
it shows the time usage per trial. We also recorded the time usage
of each step in FairNeuron . Results and analysis are presented
below.Table 5: Time to train a model.
Dataset Naive EA (/iteration) FairNeuron (/trial)
Census 115.74s 1439.96s 254.41s
Credit 3.07s 33.24s 31.49s
COMPAS 11.92s 81.93s 44.31s
Table 6: Time used in each step.
Dataset Para selection Slicing Clustering Training
Census 115.41s 25.37s 43.70s 74.37sCredit 30.98s 0.20s 6.73e-4s 0.30s
COMPAS 40.76s 2.09s 0.06s 1.40s
Results: Table 5 shows the comparison of time usage among ordi-
narytraining,EthicalAdversariesand FairNeuron training.The
firstcolumnliststhethreedatasetsandtheremainingcolumnsshowthedifferenttrainingmethods.Onaverage,
FairNeuron takes5.39
times of ordinary training and 55.49% of Ethical Adversaries.
Table 6 reports the time costs of each step. The first column also
liststhethreedatasetsandtheremainingcolumnsshowthetime
costsofhyperparametersselection,neuronnetworkslicing,sample
clustering and selective training respectively.
Analysis: Forordinarytraining,theruntimeoverheadallcomes
fromthetrainingprocedure,butfor FairNeuron ,thehyperparam-
eterstuningaccountsforalargerratioofthetotaltimeusage,as
showninTable6.So FairNeuron takesonlylessthantwiceofthe
timeusageofordinarytrainingonlargedatasetslikeCensus,but
on small datasets like the German Credit dataset, it takes relatively
a long time. If FairNeuron tries more times, the average time will
be reduced because the hyperparameters tuning is only conducted
once.
Overall,FairNeuron is more efficient than Ethical Adversaries
infixingmodels,withanaveragespeedupof180%.
4.4 Effects of Configurable Hyperparameters
FairNeuron leveragestwoconfigurablehyperparameters, ğœƒand
ğ›¾,tofixfairnessproblems.Thehyperparameters ğ›¾representsthe
threshold of neuron activation, and its value affects the complexity
of the path. As its value decreases, more neurons and synapses
areincludedinthe path,resultinginamorecomplex path.And ğœƒ
represents the threshold of neuron network slicing. The lower is
theğœƒ, the fewer paths are classified as abnormal.
We conduct a comparison experiment of these hyperparameters.
ğœƒvaries between the interval [10âˆ’4,1]andğ›¾varies between the
interval[0.5,1]. Note that we use logarithmic coordinates for ğœƒ
since its value is sampled proportionally.
Figure 5 shows how hyperparameters assignments will affect
the performance. Based on our results in comparison with thenaive model and Ethical Adversaries, we can conclude that our
algorithmdoesperformbetteronthistaskandisnotsensitiveto
hyperparameters assignments except for EO (Figure 5(c) & (g)). By
increasingtheweightofEOinhyperparametertuninglossfunction,
we can constrain its fluctuations.
929
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:19:36 UTC from IEEE Xplore.  Restrictions apply. ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA Xuanqi Gao, Juan Zhai, Shiqing Ma, Chao Shen, Yufei Chen, and Qian Wang
(a)ğœƒ-accuracy (b)ğœƒ-DP (c)ğœƒ-EO
(d)ğœƒ-DPR (e)ğ›¾-accuracy (f)ğ›¾-DP
(g)ğ›¾-EO (h)ğ›¾-DPR
Figure 5: Effect of hyperparameters ğœƒandğ›¾.ğœƒis sampled proportionally, so we take the logarithm of ğœƒas x axis.
4.5 Performance on Image Datasets
ExperimentDesign: Wealsoexploredthepossibilityofusingour
method in fixing models on image datasets, which has not done
by baseline methods due to their inefficiency. In our experiment,
we leverage a 4-layer fully-connected NN trained on MNIST [ 5]
andResNet-18[ 37]trainedonCIFAR-10[ 1],compare FairNeuron
with the naive model and random dropout. We use Class-wise
Variance (CV) and Maximum Class-wise Discrepancy (MCD) as
fairness metrics.Results:
Table 7 summarizes our results. The first column lists the
datasets. The second column shows the different model, and the
remainingcolumnslisttheperformance.Thebestresultsareshown
in bold. As we can see from the table, FairNeuron can effectively
improvethemodelfairnessby20%forMCDand80%forCV.We
discuss it further in Â§6.Table 7: Results on image datasets. Best results are in bold.
Dataset Model Acc CV MCD
MNISTNaive model 0.957 6.66e-5 0.057Random dropout 0.949 3.58e-5 0.052FairNeuron 0.961 9.54e-6 0.046
CIFAR-10Naive model 0.8146.04e-4 0.236
Random dropout 0.798 8.55e-4 0.464FairNeuron 0.808 1.16e-4 0.187
5 RELATED WORK
Neural Network Slicing . Pathanalysis ordataflow analysis[ 13]
is a fundamental technique in traditional software engineering
tasksliketesting,debuggingandoptimization.Itoffersawindow
to study programâ€™s dynamic behavior. In recent years, with the de-
velopment of AI security, especially adversarial attack and defense,
930
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:19:36 UTC from IEEE Xplore.  Restrictions apply. FairNeuron: Improving Deep Neural Network Fairness with Adversary Games on Selective Neurons ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
conflict path detection has been used for interpretability. Wang et
al. [63] proposed a method to interpret neural networks by extract-
ing the critical data routing paths (CDRPs), and they demonstrated
its effectiveness on adversarial sample detection problem. Qiu et
al.[56]treataneuralnetworkasadataflowgraph,whichcanbe
appliedtheprofilingtechniquetoextractitsexecutionpath.Zhang
et al. [73] apply the dynamic slicing on deep neural networks.Fairness of ML
. With the increasing use of automated decision-
making approaches and systems, fairness considerations in ML
havegainedsignificantattention.Researchersfoundmanyfairness
problems with high social impact, such as standardized tests inhigher education [
24], employment [ 34,57,61], and re-offence
judgement[ 16,17,20,51].Besides,governments(e.g.theEU[ 62]
and the US [ 54,55]), organizations [ 49], and the media have called
for more societal accountability and social understanding of ML.
To address the concern above, numerous fairness notions are
proposed.Inhighlevel,thesefairnessnotionscanbesplitintothree
categories: (i)individual fairness, whichrequires that similarindi-
vidualsshouldbetreatedsimilarly[ 26,46,67];(ii)groupfairness,
whichconcernsaboutwhethersubpopulationwithdifferentsen-
sitive characteristics are treated equally [28, 35, 70]; (iii) Max-Minfairness, which try to improve the per-group fairness [36, 45, 69].
Fairnesstestingisalsoanimportantresearchdirection,andits
approachesmostlybasedongenerationtechniques.THEMIS[ 14]
considersgroupfairnessusingcausalanalysisandusesrandomtest
generation to evaluate fairness. AEQUITAS inherits and improves
THEMIS, and focuses on the individual discriminatory instances
generation[ 60].Later,ADFcombinesglobalsearchandlocalsearch
tosystematicallysearchtheinputspacewiththeguidanceofgradi-
ent[71].SymbolicGeneration(SG)integratessymbolicexecution
and local model explanation techniques to craft individual discrim-
inatory instances [12].
The ML model needs to be repaired after the fairness problem is
found.Theseapproachescanbegenerallysplitintothreecategories:
(i)Pre-processingapproaches,whichfixthetrainingdatatoreduce
the latent discrimination in dataset. For example, the bias could be
mitigatedbycorrectinglabels[ 40,70],revisingattributes[ 28,41],
generatingnon-discriminationdata[ 58,64],andobtainingfairdata
representations[ 18].(ii)In-processingapproaches,whichrevisethe
training of the bias model to achieve fairness [ 66,68]. More specifi-
cally, these approaches apply fairness constraints [ 26,66], propose
an objective function considering the fairness of prediction [ 68],
or design a new training frameworks [ 11,64]. (iii) post-processing
approaches, which directly change the predictive labels of bias
modelsâ€™ output to obtain fairness [35, 53].
6 CONCLUSION
Inthispaper,weproposedalightweightalgorithm FairNeuron to
effectivelyfixingfairnessproblemsfordeepneuralnetworkthrough
pathanalysis.Ouralgorithmcombinesapathanalysisprocedure
andadropoutproceduretosystematicallyimprovemodelperfor-
mance.FairNeuron searches bias instanceswith the guidance of
path analysis and mitigates fairness problems by dropout training.
Ourevaluationresultsshowthat FairNeuron hassignificantlybet-
terperformancebothintermsofeffectivelyandefficientlyinfixing
bias models. For CNN model, we can only perform FairNeuronon the last full-connected layer, so its performance is not ideal. We
will improve FairNeuron on CNN in the future.
7 ACKNOWLEDGEMENT
We thank the anonymous reviewers for their constructive com-
ments.ThisresearchwaspartiallysupportedbyNationalKeyR&D
Program(2020YFB1406900),NationalNaturalScienceFoundation
ofChina(U21B2018,62161160337,61822309,U20B2049,61773310,
U1736205,61802166)andShaanxiProvinceKeyIndustryInnova-
tion Program (2021ZDLGY01-02). Chao Shen is the corresponding
author. The views, opinions and/or findings expressed are only
those of the authors.
REFERENCES
[1][n.d.]. CIFAR-10 and CIFAR-100 datasets. https://www.cs.toronto.edu/~kriz/ci
far.html
[2][n.d.]. TheFirstInternationalBeautyContestJudgedbyArtificialIntelligence.
http://beauty.ai
[3][n.d.]. MachineBiasâ€”ProPublica. https://www.propublica.org/article/machine-
bias-risk-assessments-in-criminal-sentencing
[4][n.d.]. Microsoftâ€™s neo-Nazi sexbot was a great lesson for makers of AI assis-
tants. https://www.technologyreview.com/2018/03/27/144290/microsofts-neo-
nazi-sexbot-was-a-great-lesson-for-makers-of-ai-assistants/
[5][n.d.]. MNISThandwrittendigitdatabase,YannLeCun,CorinnaCortesandChris
Burges. http://yann.lecun.com/exdb/mnist/
[6][n.d.]. Tune: Scalable Hyperparameter Tuning â€” Ray v1.9.0. https://docs.ray.i
o/en/latest/tune/index.html
[7]2016. A beauty contest was judged by AI and the robots didnâ€™t like dark
skin. http://www.theguardian.com/technology/2016/sep/08/artificial-intell
igence-beauty-contest-doesnt-like-black-people Section: Technology.
[8]2020. NAB turns to AI to decide on small business loans. https:
//www.afr.com/companies/financial-services/nab-turns-to-artificial-
intelligence-to-assess-small-business-loans-20201204-p56kmk Section:
financialservices.
[9]2021. How AI will change the HR industry | HRExecutive.com. http://hrexecut
ive.com/ai-will-make-traditional-hr-extinct-how-to-prepare-for-whats-next/
[10]MartinAbadi,PaulBarham,JianminChen,ZhifengChen,AndyDavis,Jeffrey
Dean,MatthieuDevin,SanjayGhemawat,GeoffreyIrving,MichaelIsard,Man-
junathKudlur,JoshLevenberg,RajatMonga,SherryMoore,DerekG.Murray,
Benoit Steiner, Paul Tucker, Vijay Vasudevan, Pete Warden, Martin Wicke, Yuan
Yu, and Xiaoqiang Zheng. 2016. TensorFlow: A System for Large-Scale Ma-
chine Learning. 265â€“283. https://www.usenix.org/conference/osdi16/technical-
sessions/presentation/abadi
[11]Tameem Adel, Isabel Valera, Zoubin Ghahramani, and Adrian Weller. 2019. One-
NetworkAdversarialFairness. ProceedingsoftheAAAIConferenceonArtificialIn-
telligence 33(July2019),2412â€“2420. https://doi.org/10.1609/aaai.v33i01.33012412
[12]Aniya Agarwal,Pranay Lohia,Seema Nagar, KuntalDey, andDiptikalyan Saha.
2018. AutomatedtestgenerationtodetectindividualdiscriminationinAImodels.
arXiv preprint arXiv:1809.03260 (2018).
[13]Glenn Ammons and James R Larust. [n.d.]. Improving Data-flow Analysis with
Path Profiles. ([n.d.]), 13.
[14]Rico Angell, Brittany Johnson, Yuriy Brun, and Alexandra Meliou. 2018. Themis:
automatically testing software for discrimination. In Proceedings of the 2018 26th
ACM Joint Meeting on European Software Engineering Conference and Symposium
on the Foundations of Software Engineering - ESEC/FSE 2018. ACM Press, Lake
Buena Vista, FL, USA, 871â€“875. https://doi.org/10.1145/3236024.3264590
[15]Solon Barocas and Andrew D. Selbst. 2016. Big dataâ€™s disparate impact. Calif. L.
Rev.104 (2016), 671. Publisher: HeinOnline.
[16]Richard Berk. 2019. Accuracy and fairness for juvenile justice risk assessments.
Journal of Empirical Legal Studies 16, 1 (2019), 175â€“194. Publisher: Wiley Online
Library.
[17]Richard Berk, Hoda Heidari, Shahin Jabbari, Michael Kearns, and Aaron Roth.
2021.Fairnessincriminaljusticeriskassessments:Thestateoftheart. Sociological
Methods&Research 50,1(2021),3â€“44. Publisher:SagePublicationsSageCA:Los
Angeles, CA.
[18]Alex Beutel, Jilin Chen, Zhe Zhao, and Ed H. Chi. 2017. Data Decisions and
Theoretical Implications when Adversarially Learning Fair Representations.
arXiv:1707.00075 [cs] (July 2017). http://arxiv.org/abs/1707.00075 arXiv:
1707.00075.
[19]Mariusz Bojarski, Davide Del Testa, Daniel Dworakowski, Bernhard Firner, Beat
Flepp, Prasoon Goyal, Lawrence D. Jackel, Mathew Monfort, Urs Muller, and
931
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:19:36 UTC from IEEE Xplore.  Restrictions apply. ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA Xuanqi Gao, Juan Zhai, Shiqing Ma, Chao Shen, Yufei Chen, and Qian Wang
Jiakai Zhang. 2016. End to end learning for self-driving cars. arXiv preprint
arXiv:1604.07316 (2016).
[20]Tim Brennan and William L. Oliver. 2013. Emergence of machine learning
techniquesincriminology:implicationsofcomplexityinourdataandinresearch
questions. Criminology & Pub. Polâ€™y 12 (2013), 551. Publisher: HeinOnline.
[21]Simon Caton and Christian Haas. 2020. Fairness in Machine Learning: A Survey.
arXiv:2010.04053 [cs, stat] (Oct. 2020). http://arxiv.org/abs/2010.04053 arXiv:
2010.04053.
[22]TongChe,YanranLi,AthulPaulJacob,YoshuaBengio,andWenjieLi.2017. Mode
RegularizedGenerativeAdversarialNetworks. arXiv:1612.02136[cs] (March2017).
http://arxiv.org/abs/1612.02136 arXiv: 1612.02136.
[23]AlexandraChouldechova.2017. Fairpredictionwithdisparateimpact:Astudyofbiasinrecidivismpredictioninstruments. Bigdata5,2(2017),153â€“163. Publisher:
Mary Ann Liebert, Inc. 140 Huguenot Street, 3rd Floor New Rochelle, NY 10801
USA.
[24]T.AnneCleary.1966. Testbias:ValidityoftheScholasticAptitudeTestforNegro
andWhitestudentsinintegratedcolleges. ETSResearchBulletinSeries 1966,2
(1966), iâ€“23. Publisher: Wiley Online Library.
[25]Pieter Delobelle, Paul Temple, Gilles Perrouin, BenoÃ®t FrÃ©nay, Patrick Heymans,
andBettinaBerendt.2021. Ethicaladversaries:Towardsmitigatingunfairness
withadversarialmachinelearning. ACMSIGKDDExplorationsNewsletter 23,1
(2021), 32â€“41. Publisher: ACM New York, NY, USA.
[26]Cynthia Dwork, Moritz Hardt, Toniann Pitassi, Omer Reingold, and Richard
Zemel.2012. Fairnessthroughawareness.In Proceedingsofthe3rdInnovationsin
TheoreticalComputerScienceConferenceon-ITCSâ€™12.ACMPress,Cambridge,
Massachusetts, 214â€“226. https://doi.org/10.1145/2090236.2090255
[27]Yanai Elazar and Yoav Goldberg. 2018. Adversarial removal of demographic
attributes from text data. arXiv preprint arXiv:1808.06640 (2018).
[28]Michael Feldman, Sorelle A. Friedler, John Moeller, Carlos Scheidegger, and
SureshVenkatasubramanian.2015. CertifyingandRemovingDisparateImpact.
InProceedingsofthe21thACMSIGKDDInternationalConferenceonKnowledge
Discovery and Data Mining - KDD â€™15. ACM Press, Sydney, NSW, Australia, 259â€“
268. https://doi.org/10.1145/2783258.2783311
[29]RupertFreeman,NisargShah,andRohitVaish.2020. BestofBothWorlds:Ex-
Ante and Ex-Post Fairness in Resource Allocation. arXiv:2005.14122 [cs] (May
2020). http://arxiv.org/abs/2005.14122 arXiv: 2005.14122.
[30]Sorelle A. Friedler, Carlos Scheidegger, Suresh Venkatasubramanian, SonamChoudhary, Evan P. Hamilton, and Derek Roth. 2019. A comparative studyoffairness-enhancinginterventionsinmachinelearning.In Proceedingsofthe
Conference on Fairness, Accountability, and Transparency (FAT* â€™19). Association
for Computing Machinery, New York, NY, USA, 329â€“338. https://doi.org/10.
1145/3287560.3287589
[31]Yaroslav Ganin and Victor Lempitsky. [n.d.]. Unsupervised Domain Adaptation
by Backpropagation. ([n.d.]), 10.
[32]Xuanqi Gao. 2022. FairNeuron. https://github.com/Antimony5292/FairNeuron
original-date: 2021-09-01T12:52:43Z.
[33]IanGoodfellow,JeanPouget-Abadie,MehdiMirza,BingXu,DavidWarde-Farley,
Sherjil Ozair, Aaron Courville, and Yoshua Bengio. 2014. Generative Adversarial
Nets. InAdvances in Neural Information Processing Systems, Vol. 27. Curran
Associates,Inc. https://papers.nips.cc/paper/2014/hash/5ca3e9b122f61f8f06494c
97b1afccf3-Abstract.html
[34]RobertM.Guion.1966. Employmenttestsanddiscriminatoryhiring. Industrial
Relations: A Journal of Economy and Society 5, 2 (1966), 20â€“37. Publisher: Wiley
Online Library.
[35]Moritz Hardt, Eric Price, and Nati Srebro. 2016. Equality of opportunity in
supervised learning. Advances in neural information processing systems 29 (2016),
3315â€“3323.
[36]Tatsunori Hashimoto, Megha Srivastava, Hongseok Namkoong, and Percy Liang.
2018. Fairness without demographics in repeated loss minimization. In Interna-
tional Conference on Machine Learning. PMLR, 1929â€“1938.
[37]KaimingHe,XiangyuZhang,ShaoqingRen,andJianSun.2016. Deepresidual
learning for image recognition. In Proceedings of the IEEE conference on computer
vision and pattern recognition. 770â€“778.
[38]BaotianHu,ZhengdongLu,HangLi,andQingcaiChen.2014. Convolutionalneu-
ral network architectures for matching natural language sentences. In Advances
in neural information processing systems. 2042â€“2050.
[39]NalKalchbrenner,EdwardGrefenstette,andPhilBlunsom.2014. Aconvolutional
neural network for modelling sentences. arXiv preprint arXiv:1404.2188 (2014).
[40]FaisalKamiranandToonCalders.2009. Classifyingwithoutdiscriminating.In
2009 2ndinternational conferenceon computer, controland communication. IEEE,
1â€“6.
[41]Faisal Kamiran and Toon Calders. 2012. Data preprocessing techniques for
classificationwithout discrimination. KnowledgeandInformation Systems 33,1
(Oct. 2012), 1â€“33. https://doi.org/10.1007/s10115-011-0463-8
[42]Faisal Kamiran, Asim Karim, and Xiangliang Zhang. 2012. Decision Theory for
Discrimination-Aware Classification. In 2012 IEEE 12th International Conference
onDataMining.IEEE,Brussels,Belgium,924â€“929. https://doi.org/10.1109/IC
DM.2012.45[43]JonKleinberg,JensLudwig,SendhilMullainathan,andAsheshRambachan.2018.
Algorithmic fairness. In Aea papers and proceedings, Vol. 108. 22â€“27.
[44]MattJKusner,JoshuaLoftus,ChrisRussell,andRicardoSilva.[n.d.]. Counterfac-
tual Fairness. NIPS 2017 ([n.d.]), 11.
[45]PreethiLahoti,AlexBeutel,JilinChen,KangLee,FlavienProst,NithumThain,
Xuezhi Wang, and Ed H. Chi. 2020. Fairness without Demographics through
AdversariallyReweightedLearning. arXiv:2006.13114[cs,stat] (Nov.2020). http:
//arxiv.org/abs/2006.13114 arXiv: 2006.13114.
[46]PreethiLahoti,KrishnaP.Gummadi,andGerhardWeikum.2019. Operational-
izing individual fairness with pairwise fair representations. arXiv preprint
arXiv:1907.01439 (2019).
[47]ShiqingMa,YousraAafer,ZhaoguiXu,Wen-ChuanLee,JuanZhai,YingqiLiu,
and Xiangyu Zhang. 2017. LAMP: data provenance for graph based machine
learningalgorithmsthroughderivativecomputation.In Proceedingsofthe2017
11th Joint Meeting on Foundations of Software Engineering. 786â€“797.
[48]ShiqingMa,YingqiLiu,Wen-ChuanLee,XiangyuZhang,andAnanthGrama.
2018. MODE: automatedneuralnetworkmodeldebuggingvia statedifferential
analysisandinputselection.In Proceedingsofthe201826thACMJointMeeting
on European Software Engineering Conference and Symposium on the Foundationsof Software Engineering - ESEC/FSE 2018. ACM Press, Lake Buena Vista, FL, USA,
175â€“186. https://doi.org/10.1145/3236024.3236082
[49]AnnetteMarkhamandElizabethBuchanan.2012. Ethicaldecision-makingand
internetresearch:Version2.0.recommendationsfromtheAoIRethicsworking
committee. Available online: aoir. org/reports/ethics2. pdf (2012).
[50]Panayotis Mertikopoulos, Christos Papadimitriou, and Georgios Piliouras. 2018.
CyclesinAdversarialRegularizedLearning. In Proceedingsofthe2018Annual
ACM-SIAM Symposium on Discrete Algorithms (SODA). Society for Industrial and
Applied Mathematics, 2703â€“2717. https://doi.org/10.1137/1.9781611975031.172
[51]CathyOâ€™neil.2016. Weaponsofmathdestruction:Howbigdataincreasesinequality
and threatens democracy. Crown.
[52]AdamPaszke,SamGross,FranciscoMassa,AdamLerer,JamesBradbury,GregoryChanan,TrevorKilleen,ZemingLin,NataliaGimelshein,LucaAntiga,AlbanDes-
maison,AndreasKopf,EdwardYang,ZacharyDeVito,MartinRaison,Alykhan
Tejani,SasankChilamkurthy,BenoitSteiner,LuFang,JunjieBai,andSoumith
Chintala.[n.d.]. PyTorch:AnImperativeStyle,High-PerformanceDeepLearning
Library. ([n.d.]), 12.
[53]GeoffPleiss,ManishRaghavan,FelixWu,JonKleinberg,andKilianQWeinberger.
[n.d.]. On Fairness and Calibration. ([n.d.]), 10.
[54]Executive Office of the President, Cecilia Munoz, Domestic Policy Council Direc-
tor,Megan(USChiefTechnologyOfficerSmith(OfficeofScience,Technology
Policy)), DJ (Deputy Chief Technology Officer for Data Policy, Chief Data Sci-
entistPatil(OfficeofScience,andTechnologyPolicy)).2016. Bigdata:Areport
on algorithmic systems, opportunity, and civil rights. Executive Office of the
President.
[55]United States Executive Office of the President and John Podesta. 2014. Big data:
Seizing opportunities, preserving values. White House, Executive Office of the
President.
[56]YuxianQiu,JingwenLeng,CongGuo,QuanChen,ChaoLi,MinyiGuo,andYuhaoZhu.2019.AdversarialDefenseThroughNetworkProfilingBasedPathExtraction.
In2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR).
IEEE, Long Beach, CA, USA, 4772â€“4781. https://doi.org/10.1109/CVPR.2019.
00491
[57]ManishRaghavan,SolonBarocas,JonKleinberg,andKarenLevy.2020.Mitigating
bias in algorithmic hiring: Evaluating claims and practices. In Proceedings of the
2020 conference on fairness, accountability, and transparency. 469â€“481.
[58]Prasanna Sattigeri, Samuel C. Hoffman, Vijil Chenthamarakshan, and Kush R.
Varshney. 2019. Fairness GAN: Generating datasets with fairness properties
usingagenerativeadversarialnetwork. IBMJournalofResearchandDevelopment
63, 4/5 (2019), 3â€“1. Publisher: IBM.
[59]FlorianTramer,VaggelisAtlidakis,RoxanaGeambasu,DanielHsu,Jean-Pierre
Hubaux,MathiasHumbert,AriJuels,andHuangLin.2017. FairTest:Discovering
UnwarrantedAssociationsinData-DrivenApplications.In 2017IEEEEuropean
Symposium on Security and Privacy (EuroS&P). IEEE, Paris, 401â€“416. https:
//doi.org/10.1109/EuroSP.2017.29
[60]SakshiUdeshi,PryanshuArora,andSudiptaChattopadhyay.2018. Automated
directed fairness testing.In Proceedings of the33rd ACM/IEEE International Con-
ference on Automated Software Engineering - ASE 2018. ACM Press, Montpellier,
France, 98â€“108. https://doi.org/10.1145/3238147.3238165
[61]Elmira van den Broek, Anastasia Sergeeva, and Marleen Huysman. 2019. Hiring
algorithms: An ethnography of fairness in practice. (2019).
[62]Paul Voigt and Axel Von dem Bussche. 2017. The eu general data protection reg-ulation (gdpr). A Practical Guide, 1st Ed., Cham: Springer International Publishing
10 (2017), 3152676. Publisher: Springer.
[63]Yulong Wang, Hang Su, Bo Zhang, and Xiaolin Hu. 2018. Interpret Neural
NetworksbyIdentifyingCriticalDataRoutingPaths.In 2018IEEE/CVFConference
on Computer Vision and Pattern Recognition. IEEE, Salt Lake City, UT, 8906â€“8914.
https://doi.org/10.1109/CVPR.2018.00928
932
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:19:36 UTC from IEEE Xplore.  Restrictions apply. FairNeuron: Improving Deep Neural Network Fairness with Adversary Games on Selective Neurons ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
[64]Depeng Xu, Shuhan Yuan, Lu Zhang, and Xintao Wu. 2018. Fairgan: Fairness-
aware generative adversarial networks. In 2018 IEEE International Conference on
Big Data (Big Data). IEEE, 570â€“575.
[65]Muhammad Bilal Zafar, Isabel Valera, Manuel Gomez Rodriguez, and Krishna P.
Gummadi. 2017. Fairness Beyond Disparate Treatment &amp; Disparate Impact:
LearningClassificationwithoutDisparateMistreatment.In Proceedingsofthe26th
International Conference on World Wide Web (WWW â€™17). International World
Wide Web Conferences Steering Committee, Republic and Canton of Geneva,
CHE, 1171â€“1180. https://doi.org/10.1145/3038912.3052660
[66]Muhammad Bilal Zafar, Isabel Valera, Manuel Gomez Rogriguez, and Krishna P.
Gummadi. 2017. Fairness constraints: Mechanisms for fair classification. In
Artificial Intelligence and Statistics. PMLR, 962â€“970.
[67] Richard Zemel. [n.d.]. Learning Fair Representations. ([n.d.]), 9.
[68]Brian Hu Zhang, Blake Lemoine, and Margaret Mitchell. 2018. Mitigating Un-
wanted Biases with AdversarialLearning. In Proceedings of the 2018AAAI/ACM
Conference on AI, Ethics, and Society. ACM, New Orleans LA USA, 335â€“340.
https://doi.org/10.1145/3278721.3278779
[69]Chongjie Zhang and Julie A. Shah. 2014. Fairness in multi-agent sequentialdecision-making. In Advances in Neural Information Processing Systems. 2636â€“
2644.
[70]Lu Zhang, Yongkai Wu, and Xintao Wu. 2017. Achieving Non-Discrimination in
DataRelease.In Proceedingsofthe23rdACMSIGKDDInternationalConference
on Knowledge Discovery and Data Mining. ACM, Halifax NS Canada, 1335â€“1344.
https://doi.org/10.1145/3097983.3098167
[71]Peixin Zhang, Jingyi Wang, Jun Sun, Guoliang Dong, Xinyu Wang, Xingen
Wang,JinSongDong,andTingDai.2020. White-boxfairnesstestingthrough
adversarial sampling. In Proceedings of the ACM/IEEE 42nd International Con-
ference on Software Engineering. ACM, Seoul South Korea, 949â€“960. https:
//doi.org/10.1145/3377811.3380331
[72]XiaoyuZhang,JuanZhai,ShiqingMa,andChaoShen.2021. AUTOTRAINER:
An Automatic DNN Training Problem Detection and Repair System. In 2021
IEEE/ACM 43rd International Conference on Software Engineering (ICSE). IEEE,
Madrid, ES, 359â€“371. https://doi.org/10.1109/ICSE43902.2021.00043
[73]ZiqiZhang,YuanchunLi,YaoGuo,XiangqunChen,andYunxinLiu.2020. Dy-
namic Slicing for Deep Neural Networks. (2020), 13.
933
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:19:36 UTC from IEEE Xplore.  Restrictions apply. 