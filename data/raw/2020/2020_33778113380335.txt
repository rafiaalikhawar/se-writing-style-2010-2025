Mitigating Turnover with Code Review Recommendation:
Balancing Expertise, Workload, and Knowledge Distribution
Ehsan Mirsaeedi
Department of Computer Science and
Software Engineering
Concordia University, Montréal, Québec, Canada
s_irsaee@encs.concordia.caPeter C. Rigby
Department of Computer Science and
Software Engineering
Concordia University, Montréal, Québec, Canada
peter.rigby@concordia.ca
ABSTRACT
Developer turnover is inevitable on software projects and leads to
knowledge loss, a reduction in productivity, and an increase in de-
fects.Mitigationstrategiestodealwithturnovertendtodisruptand
increase workloads for developers. In this work, we suggest that
throughcodereviewrecommendationwecandistributeknowledge
andmitigateturnoverwithminimalimpactonthedevelopmentpro-
cess. We evaluate review recommenders in the context of ensuring
expertiseduringreview, Expertise,reducingthereviewworkload
ofthecoreteam, CoreWorkload ,andreducingtheFilesatRiskto
turnover, FaR.Wefindthatpriorworkthatassignsreviewersbased
onfileownershipconcentratesknowledgeonasmallgroupofcore
developers increasing risk of knowledge loss from turnover by up
to 65%. We propose learning and retention aware review recom-
mendersthat whencombined areeffectiveat reducingtherisk of
turnoverby-29%buttheyunacceptablyreducetheoverallexpertise
during reviews by-26%. Wedevelopthe Sofiarecommender that
suggests experts when none of the files under review are hoardedbydevelopers,butdistributesknowledgewhenfilesareatrisk.In
thisway,weareabletosimultaneouslyincreaseexpertiseduring
review with a ΔExpertise of 6%, with a negligible impact on work-
load of ΔCoreWorkload of 0.09%, and reduce the files at risk by
ΔFaR-28%. SofiaisintegratedintoGitHubpullrequestsallowing
developerstoselectanappropriateexpertor“learner"basedonthe
contextofthereview.Wereleasethe Sofiabotaswellasthecode
and data for replication purposes.
KEYWORDS
Turnover,KnowledgeDistribution,CodeReview,Recommenders,
Tool Support
ACM Reference Format:
Ehsan Mirsaeedi and Peter C. Rigby. 2020. Mitigating Turnover with Code
ReviewRecommendation:BalancingExpertise,Workload,andKnowledge
Distribution.In 42ndInternationalConferenceonSoftwareEngineering(ICSE
’20),May23–29,2020,Seoul,RepublicofKorea. ACM,NewYork,NY,USA,
13 pages. https://doi.org/10.1145/3377811.3380335
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation
onthe firstpage.Copyrights forcomponentsof thisworkowned byothersthan the
author(s)mustbehonored.Abstractingwithcreditispermitted.Tocopyotherwise,or
republish,topostonserversortoredistributetolists,requirespriorspecificpermission
and/or a fee. Request permissions from permissions@acm.org.
ICSE ’20, May 23–29, 2020, Seoul, Republic of Korea
© 2020 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 978-1-4503-7121-6/20/05...$15.00
https://doi.org/10.1145/3377811.33803351 INTRODUCTION
Turnover on software projects is frequent and inevitable and leads
to the loss of knowledge when developers leave a project [ 3,45].
Turnoverincurssubstantialeconomiccostinrecruitingandtraining
newemployees[ 32,36],itreducestheproductivityofdevelopment
teams [20,32], it leads to the loss of critical tacit knowledge [ 19,
31,32], and has been shown to increase the number of defects in a
product and reduce overall product quality [12, 31, 32].
Recentworkshavetriedtomitigatetheadverseimpactofturnover
throughincreasingknowledgeretentionbypredictingleavers[ 3,9,
24], planning for succession [ 31,35,45,49], documenting knowl-
edge, and persisting knowledge on StackOverflow and other inter-
nal QA forums [35, 40]. However, these mitigation practices often
require organizational changesand additional developer effort es-
peciallybythosewhoareexpertenoughtoanswerquestionsand
write documentation [40].
Inthiswork,weshowthatcodereviewcanmitigateturnoverrisk
because it naturally distributes knowledge by exposing developers
to new code during reviews. Prior work interviewed developers
and showed that code review is an opportunity for learning anditplaysavitalroleindistributingknowledge[
1,6,17,42,47,50].
Furthermore, studies have quantified the knowledge gained during
code review [ 41,47] and shown that developers review code in
modules they have not modified [ 50]. In contrast to other turnover
mitigationstrategies,codereviewisacommonandwell-established
practice in teams that does not require teams and individuals to
alter their current workflow.
In this work, we enhance code review’s inherent knowledge
sharingpotentialbydevelopingreviewrecommenderstodistributeknowledgeandusesimulationstoshowthattheymitigateturnoverrisk.Incontrast,existingreviewrecommenders[
2,18,21,39,52,54–
56]aresolelyfocusedonfindingexpertreviewersanddisregardthe
roleofcodereviewindistributingknowledgeamongdevelopers.
These recommenders result in expertise concentration because the
evaluationbenchmarkishowmanyoftheactualdeveloperswho
performed the review were recommended. Interviewed developers
state thatthese recommenderssuggest obvious candidatesand do
not provide additional value [23].
Toevaluaterecommendersfromotherperspectives,weintroduce
three outcome measures that interviews with developers indicated
asimportantaspectsofcodereview[ 1,17]:Expertise,CoreWorkload ,
andFaR. The first outcome ensures that expertise remains high for
finding defects during review. The second, ensures that the core
developersare notunreasonably overworkeddue toalways being
the top recommendation. The third outcome measures the number
offilesthatareatrisktoturnover, FaR,toensurethatknowledge
11832020 IEEE/ACM 42nd International Conference on Software Engineering (ICSE)
is adequately distributed during review. We run simulations on
the historical reviews of five large projects to understand howrecommenders affect each outcome. For completeness, we alsocalculate Mean Reciprocal Rank, MRR, to understand how well
eachrecommenderpredictsthedeveloperswhoactuallyperformed
the review.
Research Questions
RQ1,ReviewandTurnover:Whatisthereductioninfilesatrisk to turnover when both authors and reviewers are con-sidered knowledgeable?
Recentstudieshavequantifiedknowledgelossfromturnoveron
the basis of the commits that each developer has authored [ 34,45].
However,theknowledgetransferthatoccursduringcodereviewis
widelydocumentedwithpriorworkshowingthatreviewpromotes
teamawareness,transparency,andsharedcodeownership[ 1,42,
47,50]. We modify the previous turnover measure to consider both
authorsofcodeaswellasreviewerstobeknowledgeableandrecal-
culatethenumberoffilesthatareatrisk, FaR.Withonlyauthors
beingconsideredknowledgeableonaverage79%ofthetotalfiles
areatrisktoturnover.Whenweconsiderbothauthorsandreview-
erstobeknowledgeable FaRdropsto32%.Codereviewnaturally
distributes knowledge.
RQ2,Ownership:Doesrecommendingreviewersbasedon
codeownershipreducethenumberoffilesatrisktoturnover?
Studies show that teams tend to assign reviews to the owners
of files under review [ 17,47] and experts who have modified or
reviewed the files in the past [ 2,23]. We implement simple owner-
shiprecommendersthatsuggestreviewersbasedonthefilesthat
developers have modified or reviewed in the past.
Weshowthatassigningreviewersbasedonpriorcommits, Au-
thorshipRec , or prior reviews, RevOwnRec , increases expertise by
11.29% and 15.17%, respectively, while increases turnover risk, FaR,
by 25.25% and 65.19%. We conclude that concentrating expertise
on the top developers make projects susceptible to knowledge loss
from turnover.
RQ3,cHRev:Doesastate-of-the-artrecommenderreduce
the number of files at risk to turnover?
We review the literature on review recommenders and find that
most mine historical review information. Unfortunately, we did
not find working implementations or replication packages for any
of the existing recommenders. For comparison purposes, we re-
implement cHRev which has been shown to outperform other rec-
ommenders [ 56]. When re-evaluate cHRevon our outcome mea-
sures, we find that like the ownership recommenders, cHRev in-
creases thelevel of expertiseby 11.11%, andhas the addedbenefit
of reducing CoreWorkload by -3.49%. Unfortunately, cHRev concen-
trates knowledge and increases the risk of knowledge loss through
turnover by 4.15%.
RQ4,LearningandRetention:Canwereducethenumber
offilesatrisktoturnoverbydevelopinglearningandreten-tion aware review recommenders?
Weproposetwoknowledgeawareproxiesforestimatingknowl-
edge distribution and retention. LearnRec ensures that a developer
who has not reviewed or modified all of the files currently un-
der review will be proposed. RetentionRec recommender ensuresthat non-transient developer who have commitment to the project
arerecommended.Assigninglearnersthrough LearnRec substan-
tiallyreduces Expertise,-35.13%,butcounter-intuitivelyitmakes
the project drastically more susceptible to knowledge loss from
turnover as less committed developers are recommended, ΔFaR of
63.04%.Suggestingcommitteddevelopersthrough RetentionRec is
the most successful strategy in ensuring experts, 16.59%, during
review, but has the greatest increase in CoreWorkload , 29.42%.
RQ5,Sofia:Canwecombinerecommenderstobalance Ex-
pertise,CoreWorkload , andFaR?Each of the previous recom-
menders has a focus and cannot simultaneously balance the out-comes. Our final recommender, Sofia, assigns either experts or
learnersbasedonthefilesunderreview.ItusescHRevwhenthe
filesunderreviewarenotatriskanduses TurnoverRec whenfew
developers know about the files under review. This multi focus
strategy improves all outcomes simultaneously. Sofiaincreases the
level of expertise during review by 6.27%, while having a minor
impact of 0.09% on CoreWorkload and reduces turnover risk with a
ΔFaR of -28.27%.
Weintegrated SofiatomakerecommendationsforGitHubpull
requests and to recommend both expert and “learning" developers.
TheSofiasource code [ 30] is publicly available along with the data
in a replication package [29]
This paper is organized as follows. In Section 2, we provide
the study background as well as defining our measures, review
recommender,scoringfunctions,andsimulationmethodology.In
Section 3, we describe the projects under study. In Section 4, we
present results for each of our research questions. In Section 5, we
describe the Sofiabot which integrates into GitHub pull request.
In Section 6, we discuss threats to validity. In Sections 7 and 8, we
discuss our findings in the context of the existing literature and
conclude the paper.
2 BACKGROUND AND DEFINITIONS
In this section we introduce the background on ownership, review
recommenders,andknowledgelossandshowthemannerinwhicheachhasbeenquantifiedinthepast.Wewillsubsequentlyusethesemeasuresasthebasisonwhichtoexpandreviewerrecommendation
in a scoring function that will also be knowledge aware.
2.1 The Ownership Recommenders
Theinfluenceofcodeownershiponcodequalityhasbeenexten-
sively investigated in the literature[ 5,12,38,50]. Ownership is a
human factor that helps with finding knowledgeable developersthat can be accountable for a particular part of code or task [
33].
Developer Recommenders use ownership to automatically assign
tasks to experts [ 22]. Researchers have used a wide range of granu-
larity, from lines [ 13,14,38] to modules [ 5], to estimate ownership
of developers. Studies on code review find that code owners are
usuallyselectedtoreviewchanges[ 1,17,47].Inthisworkwede-
velop two simple scoring functions for review recommendation
based on ownership.
AuthorshipRec .Birdet al.[5]definesthecodeownershipfora
developer in a module as the ratio of commits the developer hasmaderelativeto thetotalcommits madeto thatcomponent.Our
AuthorshipRec scores a developer, D, as a candidate reviewer based
1184on the number of commits he or she has made to the files under
review,R,dividedbythetotalnumberofcommitsmadetothese
files.
AuthorshipRec( D,R)=CommitsForFilesUnderReview( D,R)
/summationtext.1Devs
dCommitsForFilesUnderReview( d,R)
(1)
RevOwnRec .Thongtanunam et al.[50]deviseareviewaware
ownership metric based on the files that a developer has reviewed.
Intuitively, reviewers who have reviewed the changed files or mod-
ulesinthepast,willbegoodcandidatereviewers.Torecommend
reviewers, we score the number of times a candidate has reviewed
thefilesinthepastdividedbythetotalnumberoftimesthefiles
have been reviewed.
RevOwnRec( D,R)=ReviewsOfFilesUnderReview( D,R)
/summationtext.1Devs
dReviewsOfFilesUnderReview( d,R)
(2)
2.2 The cHRevRecommender
There is a large literature on review recommendation [ 2,18,39,
52,54–56].Wenotethatwedidnotfindareplicationpackageor
recommenderimplementationforanyoftheseworks[ 25].Weonly
re-implementcHRev[ 56]becauseitincludesawiderangeoffactors
in its recommendation and has a higher accuracy than the other
review recommenders such as RevFinder [52].
cHRevscorescandidatereviewersbytheexpertise,frequency,
andrecencyoftheirpastreviews.First,cHRevtakesthenumberof
comments made by a candidate on a file as a proxy for expertise.Second, cHRev considers the number of work days a developer
has worked on a file as a proxy for measuring effort. Third, cHRev
weights recent reviews more highly.
cHRevdefinesthe xFactor(D,F)asthemeasureoftheexpertise
for a developer Don a fileF.Cf,Wf, andTfrespectively show the
numberofreviewcommentscontributedby DforF,thenumberof
workdays Dhasdedicatedoncontributingcommentson F,andthe
last day that Dworked on F. To provide a denominator, Cf/prime,Wf/prime,
andTf/primeindicatethetotalnumberofcommentsmadeon F,thetotal
numberofworkdaysspentoncommentingon F,andthetimeof
the most recent comment on F, respectively.
xFactor(D,F)=Cf
Cf/prime+Wf
Wf/prime+1
|Tf−Tf/prime|+1(3)
Tocomputethescoreofacandidatereviewerforagivencode
review, they sum up the xFactor(D,F)that the candidate, D, has
on the files in the change, F.
2.3 The Turnover Mitigating Recommenders
Thefocusofexistingrecommendersonexpertsdisregardstheother
benefits of code review such as knowledge sharing. Rigby and
Bird [42] reportthat codereview increasesthe numberof files de-
velopersseebybetween100%and150%.Inthiswork,wespeculate
thatcodereviewcanbeeffectiveinmitigatingtheturnover-induced
knowledgeloss.Baseduponthisidea,wedesignreviewerrecom-
menders that either distribute or retain knowledge.2.3.1 Distributing Knowledge. Wethendefineacandidate’sknowl-
edgeofreviewrequestasthenumberoffilesunderreviewthata
candidate has modified or reviewed in the past divided by the total
number of files under review.
ReviewerKnows( D,R)=NumCommitOrReviewedFiles( D,R)
NumFilesUnderReview( R)
(4)
Equation4,assignsdeveloperswithknowledgeofthecodeunder
reviewandensuresexpertopinionsbutconcentratestheknowledge
of these files exacerbating the risk from turnover.
LearnRec .Todistributeknowledgeamongthedevelopers,wein-
verse the ReviewerKnows( D,R)function to understand how many
newfilesadeveloperwillgainknowledgeofifheorsheisassigned
thereview.Welimittherecommendertoonlydisplaycandidates
thatknowaboutatleastonefileunderreview.Wethenscorethe
remainingreviewersusingthe LearnRec recommendertomaximize
learning through the scoring function:
LearnRec(D,R)=1−Knowledge( D,R) (5)
2.3.2 Developer Retention. Developerswhohavemadesubstantial
recent contributions to a project have demonstrated a high degree
ofcommitmenttotheproject[ 9,48].Incontrast,assigningareview
toadeveloperwhoistransientandwilllikelyleavetheprojectis
antitheticaltothegoalofretainingprojectknowledge.Wedefine
commitmentandcontributionconsistencymeasurestorecommendreviewerswithahighpotentialofremainingontheproject,
i.e.high
retention potential. In contrast to the previous measures which are
atthepullrequestorreviewlevel,theretentionisdoneataproject-
wide level.
ContributionRatio. We measure the contribution of potential
of a developer, D, by the number of reviews and commits he or she
hasmadeinthelastyeardividedbyallthecommitsandreviews
on the project.
ContributionRatio 365(D)=TotalCommitReview 365(D)
/summationtext.1Devs
dTotalCommitReview 365(d)
(6)
ConsistencyRatio. It is common for developers to make sub-
stantial contributions to a feature and leave the project after the
featureis complete.Toavoid assigningreviews totransientdevel-
opers,wedefinethe ConsistencyRatio365(D)astheproportionof
months a developer has been active in the last year.
ConsistencyRatio365(D)=ActiveMonths 365(D)
12(7)
RetentionRec .Wedevelop RetentionRec thatsuggestsreviewers
whowhoareunlikelytoleavetheproject.Thescoringfunctionfor
a candidate review, Dis
RetentionRec (D)=ConsistencyRatio365(D)∗ContributionRatio 365(D)
(8)
2.3.3 Distribution and Retention Combined. TurnoverRec .To en-
surethatknowledgeisdistributedamongdeveloperswhoarelikely
1185to remain on the project, we define the TurnoverRec recommender
scoring function for a developer and review as
TurnoverRec( D,R)=LearnRec( D,R)∗RetentionRec( D)(9)
Sofia:TurnoverRec andcHRevCombined When the files
under review have many developers who know about them, it
is best to suggest an expert. In contrast, when the number of
knowledgeable developers is low, knowledge should be distributed
among the development team. Our final recommender, Sofia, com-
bines the cHRev, which is designed to find recent experts and
TurnoverRec ,whichisdefinedtodistributeknowledgeamongde-
velopers who have high retention potential. Given the function
Knowledдeable (f)that returns the set of developers who have
modifiedorreviewedfile f,Sofia(D,R)selectseithera cHRev(D,R)
score or a TurnoverRec (D,R)score as defined in the cases below:
/braceleftBigg
cHRev(D,R),if|Knowledдeable (f)| ≤d, anyf|f∈R
TurnoverRec (D,R),otherwise
(10)
Weconsiderfilesthathavenoknowledgeabledevelopersorthat
arehoardedbyasingledevelopertobeatrisk.Asresult,weconsider
a review that has a file with 0, 1, or 2 knowledgeable developers to
haveapotentialforknowledgelossfromturnoverandsodistribute
knowledge and set d=2.
2.4 Simulation and Evaluation
To evaluate reviewer recommenders, prior works made recommen-
dationsforeachexitingreviewandcomparedtheirresultagainst
theactualreviewerswhoperformedthereview[ 2,18,39,52,54–56].
To compare with the actual reviewers, we use the Mean Reciprocal
Rank (MRR)and evaluate eachrecommender.MRR is theaverage
of the inverse rank of the highest ranked correct recommendation.
Forexample,ifacorrectrecommendationisonaveragethethird
recommendation, the score would be 1/3.
AcriticismofpriorworkscanbefoundinKovalenco et al.’s[23]
interviewswithdeveloperswhostatethattherecommendersrarely
provide additional value because they suggest obvious expert can-
didatereviewers.Thisproblemisinherentintheoutcomemeasure,
whichassumesthattheactualreviewerswerethebest, i.e.“correct"
reviewers.Kovalenco et al.[23]suggeststhatweneedtoaccount
forother perspectivesand outcomesbeyond simplyattempting to
predict the actual reviewers.
To evaluate the impact of reviewer recommendation on diverse
outcomes, we perform simulations. Simulation requires us to re-place the actual reviewer with a recommended reviewer and toevaluate the outcomes over a period of time. The simulation in-
volves sequentially making recommendations for each review on a
project.Totraineachrecommender,weusetheentirehistoryprior
tothereview.Therecommendersconsiderthefilesunderreview
and accordingto the formulas definedin Sections 2.1,2.2, and 2.3,
they randomly replace one of the actual reviewers with the toprecommended reviewer. For example, if DevA actually reviewedthe files, but is replaced with top recommended DevB, then the
knowledgefromthereviewwillbeattributedtoDevB,notDevA,
forfuturerecommendationandforoutcomemeasurement.Weonlyrandomlyreplaceonedevelopertoavoiddisruptingthepeerreview
process and becauseKovalenco et al.[23] showed thatdevelopers
usually already know at least one expert review candidate.
To evaluate how each recommender changes the project, we
measurethreeoutcomes:thedegreeofreviewer Expertise,reviewer
CoreWorkload , and the number of files at risk to turnover, FaR.
These measures incorporate the reasons interviewed developers
conduct review [ 1,17]. We measure the change in the outcomes
over the standard quarterly period [ 34,45]. Each measure is calcu-
latedasapercentagechangerelativetotheactualreviewerswho
performedthereview.Forexample,ifarecommenderreplacesan
expertreviewerwithanon-expert“learner,”wewouldexpectthe
measures to report a percentage decrease in expertise and a per-
centage increase in the knowledge distribution of the development
team. We define each outcome measure below.
Expertise .Having high expertise ensures having high quality
codereview[ 1,7,11].Wemeasurethe Expertise forareviewasthe
proportionoffilesunderreviewthattheselectedreviewershave
modified or reviewed in the past, i.e.the union of the files that the
reviewers know about. We sum the expertise across the reviews
per quarter, Q.
Expertise (Q)=Reviews (Q)/summationdisplay.1
RFilesReviewersKnow(R)
FilesUnderReview(R)(11)
CoreWorkload .To ensure high retention potential of reviewed
files,anaiverecommendercouldsuggestonlycoredeveloperswho
arebothexpertsandarecommittedtotheproject.Sucharecom-
menderwouldleadtoadrasticincreaseinthecoredeveloperwork-
load. To measure the workload, we find the 10 reviewers who have
performed the most reviews in a quarter, Top10Reviewers( Q), and
sum the total number of reviews that this top 10 group performed:
CoreWorkload (Q)=Top10Reviewers( Q)/summationdisplay.1
DNumReviews( D,Q)(12)
FaR.We need to quantify the project’s exposure to turnover
fromknowledgeloss.BuildingonRigby et al.’s[45]definitionof
knowledge loss we define the quarterly Files at Risk, FaR,a st h e
number of files that are known by zero or one active developers.
Giventhefunction ActiveDevs( Q,f)thatreturnsthesetofdevel-
opers who have modified or reviewed the file, f, and have not left
the project at the end of the quarter, Q, we define FaR(Q):
FaR(Q)={f|f∈Files ,|ActiveDevs( Q,f)| ≤1}(13)
The raw outcome measures do not facilitate easy interpretation
orcomparison.Wereportthepercentagechangeforarecommender
relative to the actual reviewers.
Sincepercentagechangeisatrivialformula,weillustrateitonly
forΔCoreWorkload:
ΔCoreWorkload( Q)=(SimulatedCoreWorkload (Q)
ActualCoreWorkload (Q)−1)∗100 (14)
The simulation results for an ideal reviewer recommender in-
creasesExpertise during review with a positive percentage change
inΔExpertise, reduces CoreWorkload with a negative percentage
1186Table1:Sizeofprojectsunderstudy.Weexplicitlyselectfor
large, long-lived projects.
Name Total Files Reviewed PRs YearsDevelopers
CoreFX 16,015 13,499 5 985
CoreCLR 15,199 10,250 4 698
Roslyn 12,313 8,646 5 469
Rust 12,472 17,499 9 2,720
Kubernetes 12,792 32,400 5 2,617
change in ΔCoreWorkload, and reduces the number of files at risk,
FaR, with a negative percentage change in ΔFaR.
3 PROJECT SELECTION AND DATA
We explicitly select well-established large projects with many com-
pletedcodereviews.Onsmallerprojects,reviewerrecommendation
islessmeaningfulasthepotentialsetofreviewsissmallandthe
developers are often aware of the entire team. To select projects,
wefirstquerytheGitHubtorrentdatasettofindprojectswithmore
than10Kpullrequests[ 15,29].Wethenapplythefollowingmanual
selection criteria:
(1)We need existing reviews, so 25% or more of the commits
must be reviewed.
(2)We need to simulate across time, so the project must be 4 or
more years old.
(3)Weneeddiverseknowledgeandmodules,soweensurethere
are at least 10K files.
Fiveprojectsmetourselectioncriteria.Oftheseprojects,CoreFX,
CoreCLR,andRoslynareledbyindustrybutareavailableunder
anopensourcelicenseandaredevelopedintheopenonGitHub.
Rust andKubernetes are community drivenOSS projects. Table 1,
provides the summary statistics including the number of files, pull
requests,andcommits.Ourreplicationpackagecontainsalinkto
the project data [29].
3.1 Gathering Data
We gatherauthorshipcommitdata fromgitand reviewdatafrom
GitHub.Weclonetherepositoriestoextractallcommitsandcor-
responding changes. On GitHub, reviews are conducted in pull-
requests that allow the authors and reviewers to discuss eachchange [
16]. In this study, we consider an individual to be a re-
viewer of a pull-request if he or she writes a review comment on a
file,asksforfurtherchangesfromthe author,orapprov es/rejectsthe
pull request.To gatherand cleanthe requireddata, wedeveloped a
post-processing pipeline which we make publicly available [29].
Unifying Developer Names. When a developer makes com-
mitsusinghisorherGitHubusernamewecanlinkthiswiththe
email address they use in the git commit. In some cases, the author
commits without using a GitHub username and we use a nameunifying approach that employs edit distances to match the git
emailnameswithGitHubusernames.Thisapproachissimilarto
Bird’set al.’s [4] and Canfora et al.’s [8].
Leavers. Robillard et al.[46]showsthatusingthelastcommitas
anindicatorfordepartureofdevelopersdrawssomerisks.Basedon
this finding, at the end of each quarter, we consider the knowledge
of a developer to be inaccessible if he or she has no contributionin the subsequent four quarters. We exclude the last quarter ofprojects from analysis to ensure that we do not mistakenly label
adeveloperasaleaveriftheyhavegoneonvacationforamonth
more.
Excludingmegacommits. Rigbyet al.[45]arguethatcommits
withhundredsoffilechangesaretoolargetobefullycomprehended
by the author. In manual analysis of mega commits and review
requests, we find that they tend to be superficial changes includingrenamingafolder,renamingafunctionthroughoutthesourcecode,
changing commented trademarks of files, or importing a large
chunk of code from a different source control system to git. We do
not associate any knowledge to the author or reviewer of changes
with 100 or more files.
In this work, we limit our study of knowledge to code files,
including .cs,.java,and .scala.Ourreplicationpackagecontainsthe
full list of file types [ 29]. We also exclude changes made by bots,
review comments that are made after the code has been merged,unmerged pull-requests, and files that were committed without
review.
4 RESULTS
In this Section, we discuss the results for our research questions
relating to (1) an empirical study of knowledge distribution during
review,(2)recommendationsbasedonownership,(3)recommenda-tionsbasedonthestate-of-the-art, cHRev,(4)learningandretention
aware recommenders, and (5) Sofiawhich combines the best rec-
ommenders.Wemakethreenotes:First,wenotethatRQ1doesnot
involve simulation and is an empirical result based on the actualreviews and commits. Second, we note that the MRR outcomes
doesnotinvolvesimulationandinsteadreportshowaccuratelythe
recommender predicts the actual reviewers. Third, simulations are
run for each recommender and we note the changes in ΔExpertise,
ΔCoreWorkload, and ΔFaR as a percentage difference relative to
the actual values for each project. Table 3 shows the average for
each outcome across all projects.
4.1 RQ1: Review and Turnover
Whatisthereductioninfilesatrisktoturnoverwhenbothauthorsandreviewersareconsideredknowledgeable? Recentstudieshavequanti-
fiedknowledgelossfromturnoveronthebasisofthecommitsthat
each developer has made [ 34,45]. The assumption in these works,
is that knowledge is only attained through writing code. However,
the knowledgetransfer that occursduring codereview iswidely
documented with prior work showing that review promotes team
awareness,transparency,andsharedcodeownership[ 1,42,47,50].
RigbyandBird[ 42]quantifiedtheadditionalknowledgeattained
duringreviewandreportedthatcodereviewexposesdevelopersto
between100%and150%morefilesthantheyedit.Thongtanunam
et al.[50]addedthatdeveloperswhohavenotmadeanychangesto
a module contributed by reviewing 21% to 39% of the code changes
in the module. In this section, we consider both authors of code as
well as reviewers to be knowledgeable and calculate the number of
files that are at risk when turnover occurs.
To assess the extent that the project is at risk to knowledge loss
fromturnover,wemeasure FaR,seeEquation13,whichmeasures
thenumberoffilesthathavezerooroneactivedevelopersattheend
1187Table 2: The proportion of total files that are at risk to
turnover.Whenonlyauthorsareconsideredknowledgeable
theproportionoffilesatriskisdrasticallyhigherthanwhenboth authors and reviewers are considered knowledgeable.
FaR Authors Authors + Reviewers
CoreFX 89.46% 24.74%
CoreCLR 86.65% 45.56%
Roslyn 68.00% 22.14%
Rust 78.10% 44.51%
Kubernetes 76.78% 26.04%
Average 79.79% 32.59%
of each quarter. To mirror prior works, we calculate the FaRauthor
which only considers authors to be knowledgeable [ 34,45]. We
then calculate FaR, which considers both authors and reviewers as
knowledgeable.
Table 2 reports the proportion of files at risk relative to the total
filesontheproject.Themedianrawvalueperquarterof FaRauthor
is 7,648, 3,704, 5,602, 2,932, and 5,448 files for CoreFX, CoreCLR,
Roslyn, Rust, and Kubernetes, respectively. As a percentage of the
codebase, between 68% and 89% of the files are at risk of aban-
donment.Incontrast,whenboththeauthorandthereviewerare
considered knowledgeable, the median raw value per quarter of
FaRis 1,988, 2,000, 1,918, 1,958, 1,877, respectively. As a percentage
ofthecodebase,between22%and45%ofthefilesareatriskofaban-donment.Asapercentageincreaseinfilesatriskfor
FaRrelativeto
FaRauthorweseethat74.00%,46.00%,65.76%,33.21%,and65.54%
fewerfilesareatriskofabandonmentforCoreFX,CoreCLR,Roslyn,
Rust,andKubernetes,respectively.Weconcludethatconsidering
reviewersto be knowledgeableofthefilestheyreviewdrastically
reducesFaRand gives a clearer picture of the risk a project is at
to turnover than prior works that only considered authors to be
knowledgeable [34, 45].
Whenonlyauthorsareconsideredknowledgeableanaver-
age of 79.79% of files are at risk to turnover. When review-
ersarealsoconsideredknowledgeablethe FaRaverageis
32.59%.Thereissubstantialknowledgedistributionduring
code review.
4.2 RQ2 Ownership
Doesrecommendingreviewersbasedoncodeownershipre-duce the number of files at risk to turnover?
Studies show that teams tend to assign reviews to the owners
of files under review [ 17,47] and experts who have modified or
reviewedthefilesinthepast[ 2,23].Inthisresearchquestion,we
runsimulationstoshowhowrecommendingreviewersbasedon
ownership affects project outcome measures.
AuthorshipRec .Priorworkshaveadapteddevelopertaskrec-
ommenders[ 22,26,33]thatusehistoricalauthorshipdatatorec-
ommendreviewers[ 18,56].Wepartiallyreproducetheseauthor-
ship recommendations by using the scoring function defined in
Equation 1. We use the simulation method described in Section 2.4
and evaluate the impact of AuthorshipRec on MRR, ΔExpertise,ΔCoreWorkload, and ΔFaR. The average values are shown in Ta-
ble 3.
AuthorshipRec is successful in predicting the reviewers who
actually performed the review with an MRR of 0.59, 0.54, 0.48,
0.44, and 0.41 for CoreFX, CoreCLR, Roslyn, Rust, and Kubernetes,
respectively. The average across all projects is 0.49. This implies
that on average the actual reviewer is ranked 2.04.
From the simulations, we see that assigning reviewers based on
their commit ownership, i.e.authorship, increases the Expertise in
reviewsby7.26%,5.97%,19.57%,10.89%,and12.77%,respectively,
with an average of 11.29% across the projects. The CoreWorkload
increases forRust by 7.50%, whileit is reducedby -11.30%, -4.74%,
-6.91%, and -2.95% for the other projects, with an average of -3.68%.
Although Expertise is high for each review, FaRhas risen across
allprojects by28.05%,12.00%, 36.23%,33.51%,and 14.48%,withanaverage of 25.25%.
Developerswhohaveauthoredthefilesunderreviewareclearly
experts. However, suggesting past authors as reviewers concen-
tratestheknowledgeofthesefilesandputstheprojectatgreater
risk to turnover as non-authors are not suggested as reviewers.
RevOwnRec .The majorityof reviewrecommenders haveused
historicalreviewdata, i.e.whohasreviewedwhichfilesormodules
inthepast,torecommendreviewers[ 2,21,52,54,55].Wepartially
reproduce these review ownership results by using the scoring
functiondefinedinEquation2.Weusethesimulationmethodology
and outcome measures as described above.
RevOwnRec isslightlylesssuccessfulatpredictingthereviewers
who actually performed the review with an MRR of 0.53, 0.50, 0.42,
0.46, and 0.37 for CoreFX, CoreCLR, Roslyn, Rust, and Kubernetes,
respectively.Theaverageacrossallprojectsis0.45.whichmeans
the actual reviewer rank is averaged to 2.22.
From the simulations, we see that assigning reviewers based on
the files theyhave reviewed in the past increasesreview Expertise
by 12.99%, 10.14%, 22.12%, 13.33%, and 17.31% respectively, with an
average of 15.17% across projects. These individuals tend to be top
reviewers and we see a corresponding increase in CoreWorkload
of 11.81%, 21.62%, 10.97%, 16.14%, and 40.93%, with an averageof 20.29%. Despite the high utilization of expert reviewers, thisrecommender has the largest increase in files at risk with
ΔFaR
valuesof9.29%,51.24%,159.42%,105.98%,and0.04%,withanaverage
of 65.19%.
Recommending reviewers based on the files they have
reviewedinthepastensuresexpertiseduringreview(av-
erage increase of 15.17%), but increases the workload of
thetopreviewersbyonaverage20.29%anddifferfromthe
set of actual reviewers with an average MRR of 0.45. Con-
centrating expertise on the top developers substantially
increasestheriskofknowledgelosswhenturnoveroccurs
on average by 65.19%.
11884.3 RQ3 cHRevRecommender
Does a state-of-the-art recommender reduce the number of
files at risk to turnover?
cHRevbuildsuponpriorworkthatleveragesinformationinpast
reviews [22], but also accounts for the number of days a candidate
reviewer has worked on a file, and the recency of this work (See
Section2.2forfurtherdetails). cHRevhasbeenshowtooutperform
theotherreview-basedrecommenders,includingRevFinder[ 50].
In this research question, we re-implement this state-of-the-artrecommender and re-evaluate it. We use the simulation method
described in Section 2.4 and evaluate the impact of cHRevon MRR,
ΔExpertise, ΔCoreWorkload, and ΔFaR.
In the original cHRev paper, the authors report an average MRR
of.67acrossfourprojects[ 56].Onourprojects,cHRevhasanMRR
of0.64,0.59,0.49,0.50,and0.42,forCoreFX,CoreCLR,Roslyn,Rust,and Kubernetes, respectively. The average is 0.52. This implies that
onaveragetheactualreviewerisranked1.92.AlthoughtheMRR
is lower in our reproduction than in the original study, we notethat for MRR cHRev outperforms all of the other recommenders
we consider.
From the simulations, we see that like the ownership recom-
menders, cHRevincreases the Expertise in reviews by 9.84%, 7.27%,
16.45%, 8.22%, and13.81%, respectively, with an averageof 11.11%
across projects. However, unlike RevOwnRec , it reduces the load
on top reviewers. The corresponding values for ΔCoreWorkload
are -5.93%, -2.35%, -0.51%, -2.19%, and -6.47% with an average of
-3.49%.cHRevconcentrates knowledgeand increases the project’s
risk to turnover with a FaRincrease of 6.46%, 13.85%, 4.43%, 10.28%
in CoreFX, CoreCLR, Roslyn, and Rust, respectively and for Kuber-
netesthe ΔFaRisreducedat-14.24%.Theaverageof ΔFaRacross
all projects is 4.15%.
cHRevremains accurate in suggesting actual reviewers
with an MRR of 0.52. It increases the degree of Expertise
during review by 11.11%, while reducing the CoreWork-
loadon the top reviewers by -3.49%. However, the risk of
turnover increases with an average ΔFaR of 4.15%.
4.4 RQ4: Learning and Retention
Canwereducethenumberoffilesatrisktoturnoverbyde-
velopinglearningandretentionawarereviewrecommenders?
The previous research questions have demonstrated that ex-
isting review recommenders concentrate knowledge on experts
increasing the riskof knowledge loss fromturnover. Furthermore,
intwolargeindustrialsettings,Kovalenco et al.[23]interviewed
developersand foundthat suggestingprior review expertstends torecommend reviewers that are obvious to the author of the change.
Theystatethatmakingobviousrecommendationsleadstoalack
of use of recommenders. They envision a new research path fornext generation of recommenders that go beyond suggesting ex-
perts.Inthisresearchquestion,weinvestigatehowwecanmitigate
turnover-inducedlossanddisseminateknowledgeusinglearning
and retention measures.Table 3: The average of outcome measures across the
projects.MRRisshownforreplicationpurposes.Individualproject outcomes are discussed in the paper text. The idealrecommender increases expertise (positive ΔExpertise), re-
duces workload (negative ΔCoreWorkload), and reduces
files at risk to turnover (negative ΔFaR).
Recommender Average Across Projects
MRR ΔExpertise ΔCoreWorkload ΔFaR
AuthorshipRec 0.49 11.29% -3.68% 25.25%
RevOwnRec 0.45 15.17% 20.29% 65.19%
cHRev 0.52 11.11% -3.49% 4.15%
LearnRec 0.12 -35.13 -39.51% 63.04%
RetentionRec 0.39 16.59% 29.42% -15.91%
TurnoverRec 0.19 -26.55% 1.07% -29.54%
Sofia 0.43 6.27% 0.09% -28.27%
LearnRec .Without review recommenders, development teams
naturally distribute knowledge during review by assigning review-
ers who would benefit by learning about the files under review [ 1,
6,47].Buildingonthisidea,inSection2.3.1,wedefinedascoring
function that determines how many files a candidate reviewer will
learnabout.Weensurethatthecandidateknowsatleastoneofthe
files that is under review. In this way, we spread knowledge, but
ensure that the reviewer has some relevant knowledge. We use the
simulationmethoddescribedinSection2.4andevaluatetheimpact
ofLearnRec on MRR, ΔExpertise, ΔCoreWorkload, and ΔFaR with
the average outcomes shown in Table 3.
LearnRec does a poor job of predicting the reviewers who ac-
tuallyperformedthereviewwithanMRRof0.18,0.14,0.12,0.11,
and 0.09 for CoreFX, CoreCLR, Roslyn, Rust, and Kubernetes, re-
spectively. The average across all projects is 0.12. This implies that
onaveragetheactualreviewerisranked8.33.However,thegoal
ofthisrecommender wastoensurethatdevelopers learnandthis
shows that it suggests unexpected reviewers.
From the simulations, we see a substantial decrease in Exper-
tise: -34.91%, -32.76%, -24.35%, -50.34%, and -33.33%, respectively,
with an average of -35.13% across all projects. The CoreWorkload is
drastically reduced as fewer expert reviewers are assigned reviews:
-38.07%, -38.53%, -35.68%, -49.86%, and -35.45%, with an average of
-39.51%. The goal of this measure is to distribute knowledge and
reduce turnover. Counter-intuitively we see an increase in the files
atriskwith ΔFaRvaluesof16.26%,22.31%,119.32%,108.72%,48.61%
with an average of 63.04%. By selecting non-experts, LearnRec rec-
ommendstransientdeveloperswhohavelesscommitmenttothe
project.
Therecommendationssubstantiallydifferfromactualre-
viewers, MRR 0.12. LearnRec substantially reduces Ex-
pertise,-35.13%,butsuggestslearnersreducingthe Core-
Workload by -39.51%. Counter-intuitively it makes the
projectdrasticallymoresusceptibletoknowledgelossfrom
turnover because it assigns reviews to learners who are
less committed to the project, ΔFaR of 63.04%.
1189RetentionRec .Assigningreviewstotransientdevelopersmay
distributeknowledge,butdoesnotreduceturnover.InSection2.3.2,
we define a measure that captures how frequently developers con-
tribute to the project and the number of months in the last year
thattheyareactive.Weensurethatthecandidateknowsatleast
oneofthefilesthatisunderreview.Ourgoalistoassignreviewsto
committeddevelopers.Weusethesamesimulationmethodology
and outcome measures.
RetentionRec doessimilarlyto RevOwnRec atpredictingthere-
viewerswhoactuallyperformedthereviewwithanMRRof0.57,
0.44, 0.31, 0.42, and 0.25 for CoreFX, CoreCLR, Roslyn, Rust, andKubernetes, respectively. The average across all projects is 0.39.
This implies that on average the actual reviewer is ranked 2.56.
From the simulations, we see an increase in Expertise of 13.84%,
10.94%,24.80%,24.13%,and19.24%,respectively,withanaverageof
16.59%.Thesepercentagesarehighestforanyrecommenderout-
performing ownership recommenders at ensuring expertise during
review.Weseeacorrespondingincreasein CoreWorkload of23.03%,
35.34%,20.73%,20.18%,and47.82%withanaverageof29.42%.How-
ever, unlike the ownership and cHRev recommenders, we see areduction in the files at risk with
ΔFaR values of -28.45%, -4.60%,
-22.73%, -7.33%, and -16.47% with an average of -15.91%. Clearly Re-
tentionRec selects committed developers who are unlikely to leave
the project.
RetentionRec is the most successful in ensuring experts,
16.59%, during review, while reducing the risk of knowl-
edge loss from turnover, -15.91%. However, by focusing
on the most committed developers it also has the greatest
increase in CoreWorkload , 29.42%. The MRR of 0.39 indi-
catesthattheactualreviewersaremorediversethanthe
recommendations.
TurnoverRec .We showed that distributing knowledge through
LearnRec does not alleviate knowledge loss and RetentionRec in-
creases the CoreWorkload . We combine these approaches to distrib-
uteknowledgebuttodistributeitamongindividualswhohavea
higherretentionpotential.ThroughEquation9,wedefined Turnover-
Recthat multiplies the learning measure by the retention measure.
Again we ensure that each candidate knows about at least one file.
We use the same simulation methodology and outcomes.
TurnoverRec does a poor job of predicting the reviewers who
actually performed the review with an MRR of 0.29, 0.20, 0.18,
0.19, and 0.12 for CoreFX, CoreCLR, Roslyn, Rust, and Kubernetes,
respectively. The average across all projects is 0.19. This implies
that on average the actual reviewer is ranked 5.26.
From the simulations, we see that similar to LearnRec recom-
mender,the Expertise hasdecreasedby-27.41%,-24.91%,-14.05%,
-34.22%,and-25.93%,respectively,withanaverageof-26.55%.How-
ever, in terms of CoreWorkload there is only a slight increase of
5.98%,5.52%,and0.50%inCoreFX,CoreCLR,andKubernetesanda
reduction in Roslyn and Rust by -0.12% and -6.52% with an aver-
ageof1.07%.Thefilesatriskarereducedwitha ΔFaRof-34.95%,
-14.20%, -41.70%, -24.32%, and -32.53% with an average of -29.54%.TurnoverRec combines learning and retention recom-
mendersandhasthegreatestreductioninturnoverrisk,
ΔFaR-29.54. However, there is a substantial cost in the
reduction of Expertise, -26.55%, and a minor increase in
CoreWorkload ,1.07.ThelowMRRvalueof0.19indicates
that developersnaturally focus on reviewers withgreater
expertise than TurnoverRec .
4.5 RQ5 Sofia
Can we combine recommenders to balance Expertise ,Core-
Workload , andFaR?
Not all reviews contain files that are at risk of abandonment. As
aresult, wedonotneed todistributeknowledgeon thesefilesbe-
cause there is already a sufficient number of developers to mitigateknowledge loss from developer turnover. In Equation 10, we define
Sofiathat distributes knowledge during review using TurnoverRec
whentherearefilesatriskofabandonment.Incontrast,whenall
the files have active developers, Sofiauses thecHRevscoring func-
tion to suggest recent experts. Of the 13,690, 10,256, 10,388, 17,810,
and 32,260 reviewed pull request on CoreFX, CoreCLR, Roslyn,
Rust,andKubernetesaround1 /4,25.18%,26.13%,29.82%,29.41%,
and17.17%,containfilesatrisk.Theremainingpullrequestsuse
cHRevrecommendations to ensure concentrated expertise. We use
the simulation method described in Section 2.4 and evaluate theimpact of Sofiaon MRR,
ΔExpertise, ΔCoreWorkload, and ΔFaR
with average outcomes shown in Table 3.
Sofiadoesagoodjobofpredictingthereviewerswhoactually
performedthereviewwithanMRRof0.54,0.48,0.39,0.39,and0.36
forCoreFX,CoreCLR,Roslyn,Rust,andKubernetes,respectively.
The average across all projects is 0.43. This implies that on average
the actual reviewer is ranked 2.32.
From the simulations, we see that by only distributing knowl-
edge when files are at risk and otherwise suggesting experts, Sofia
inherits the best characteristics of TurnoverRec andcHRev. The
Expertise goes upby 4.69%,3.32%, 8.04%,5.82%, and 9.58%, respec-
tively,withanaverageof6.27%.Intermsof CoreWorkload ,weseea
reductionof-0.27%and-5.89%inCoreFXandCoreCLR,anincreaseinRoslynof 5.09%andaslight increaseof0.43%and1.12% forRust
and Kubernetes. The average of ΔCoreWorkload is minor at 0.09%.
Sofiadistributesknowledgetodeveloperswhohaveahighreten-
tion potential and reduces the risk of turnover as measured by a
decrease in ΔFaR of -34.46%, 12.42%, -41.56%, -19.92%, and -33.02%,
with an average of -28.27%.
TheSofiarecommenderdistributesknowledgewhenthere
are files under review that are at risk of abandonment
andsuggestsexpertswhenallfilesalreadyhavemultiple
knowledgeabledevelopers.Thisstrategyallowsustoin-
crease the level of Expertise during review, 6.27%, while
havingaminorimpacton CoreWorkload ,0.09%,andsub-
stantiallyreducingthenumberoffilesatriskby-28.27%.
Sofiaalso does a reasonable job of predicting the actual
reviewers with an MRR of 0.43.
1190Figure 1: An example of Sofiarecommending both learners and experts for the CoreFX project.
5 THE SOFIAB O TO NG IT HU B
Codereviewisknowntohavemultiplepurposeandoutcomesfrom
findingdefectstodistributingknowledge[ 1,6,17,42,47].Ourtool
design allows developers to make an informed selection balancing
the need for experts and learners. We created a GitHub applica-
tion [30] that will recommend reviewers based on the combination
ofcHRevwithTurnoverRec astheSofiabot.Feedbackfromdevelop-
ers showed that the rationale behind a review recommendation is
required[ 23].Forthe Sofiabotwedisplaysimplifiedmeasuresto
complementadeveloper’sintuitionanddomainexpertiseonwho
should review the pull request.
Implementation. TheSofiarepository with the source code
and the straightforward installation instructions are publicly avail-
able [28]. Once installed Sofiaprocesses the entire history of the
project to be able to recommend reviewers. Sofiauses GitHub web-
hooks to scan submitted commits and reviewed pull requests to
keep recommendations up-to-date. Sofiacan operate in two modes:
fullyautomatedorlistcandidates.Inthefullyautomatedmode,foreachpullrequest, Sofiaassignsthetopscoringcandidatetoperform
the review.
In Figure 1 Sofiadisplays a list of candidates when the pull
request is created or when the Sofia suggest command is issued
(Box A in figure). The Sofiabot displays the ranked list of potential
reviewers (Box B). In Box C in the figure, the author can select the
person with the highest expertise. Or if learning is more importantthey can select the developer who would learn about the most files
(BoxD).Theauthorcanalsoissuethe Sofia suggest learners
orSofia suggest experts if he or she is only interested in a
particular type of candidate.
Tohelpwithtooladoption,thedisplayedmeasuresaredesigned
to be quick and easy to interpret by pull request authors and are
majorsimplificationsofthescoringfunctionsdefinedinSection2.3.
The ownership dimension maps to the “Files Authored" and the
“Files Reviewed" fields which are simplified to show the proportion
offilesunderreviewthatthecandidatehasauthoredorreviewed
in the past, respectively. Learning maps to the “New Files" field
which is simplified to the number of files that the candidate would
1191learn about, i.e.they have not modified or reviewed. Retention
potentialmapstothe“ActiveMonths"fieldwhichissimplifiedto
the proportion of months that the developer has been active in the
previous year.
Thegoalofourtoolistocomplimentadeveloper’sintuition.For
example,ifadeveloperfeelsthathighexpertiseisrequired,heor
she might choose the top candidate in Figure 1 Box C, “stephen-
toub,” who has in the past modified 3 /4 of the files under review,
has reviewed all of the files under review, and has been active in 5
months in the last year. Sofiawill warn developers that in a review
thereisatleastonefileatrisk(topofBoxB).Thedevelopermay
thenselectthebest“learner”reviewcandidatefromBoxD,“dan-
mosemsft.” Although he has never modified the files under review,
he has reviewed 2 of 4 and has also been been active in 5 of thelast 12 months. Finally, “hughbe” has both expertise and wouldalso learn about new files. He has authored 2 of 4 of the files un-der review, reviewed 1 of the files, and would learn about 2 new
files.Hehasalsobeenactive5ofthelast12months. Sofiamakes
recommendations, but provides a simple rationale for each review
candidate allowing the developer to select the best reviewer given
their intuition and the review context.
6 THREATS TO VALIDITY
Generalizability. We selected large and successful open source
software projects that were led by either industry or a community.
Onsmallerprojects,thereisnoneedforreviewerrecommendation
because the list of candidates is small and obvious to all devel-opers. Future work is necessary to validate our results in other
development contexts.
Construct Validity. Following prior works on review recom-
mendation [ 52,56], ownership [ 13,17,42], and turnover [ 34,45],
weusethesourcecodefileastheunitofknowledge.Knowledge
is contained in other documents and at other unit levels. We leave
these investigations to future work. We have also provided for-
mulasforeachofourmeasuresandscoringfunctionstofacilitate
replication.
Theknowledgeacquiredbyareviewerwillbedifferentfromthe
knowledge of the author. The author will usually know more of
thedetails,whileanexpertreviewermayknowmoreaboutother
modules and dependencies. In this work, we consider both authors
and reviewers to be knowledgeable and able to work on the files
whenturnoveroccurs.Futureworkisrequiredtounderstandthe
different types of knowledge that authors and reviewers have.
Randomlyreplacementofareviewer. Inoursimulation,we
randomly select one of the reviewers in each review to be replaced
with the top recommended reviewer for each recommender (see
Section 2.4). Table 1 showed that we examine over 80k reviews
across5projects,makingitunlikelythatthisrandomselectionwill
lead to systematic bias. As a further check, over a period of four
months,were-ranourtoptwotechniques, cHRevandSofia,amini-
mumof215timesforeachproject.For cHRev,weseeachangeof
-0.04, -0.75, -0.52, and -0.86 percentage points for MRR, ΔExpertise,
ΔCoreWorkload, and ΔFaR, respectively. Thecorresponding values
forSofia, are 0.00, -0.10, 2.24, and 1.21 percentage points, respec-
tively.Theresultsremainconsistentwith Sofiaincreasing Expertisewithaminorincreasein CoreWorkload ,whiledrasticallydecreasing
FaR.
CoreWorkload Threshold Wedefine CoreWorkload tomeasure
the reviews performed by the top 10 reviewers on a project (see
Equation12).Whilefutureworkcoulduseproportionbasedcore
teams,weusedthisvaluebecauseitsimplifiedourfunctionsand
representedareasonablyconsistentpercentageofreviewsacross
thestudiedprojects:58%,53%,61%,52%,and37%forCoreFX,Core-
CLR, Roslyn, Rust, and Kubernetes, respectively.
ReplicationandReproducability. Existingrecommendersin-
cluding ReviewBot [ 2], RevFinder [ 50], and cHRev [ 56] do not pro-
vide a replication package or sourcecode for their recommenders.
Asaresult,were-implementedcHRevforcomparisonbecauseit
outperform other state-of-the-art recommenders. We also imple-mented simple authorship and ownership recommenders. Com-paring each recommender with existing baseline recommenders
reducesthethreatofinternalvalidity.Wemakeallofourcode,data,
and GitHub Sofiabot available for future researchers as well as for
use on software projects [29].
7 DISCUSSION AND LITERATURE
We position our findings in the research literature. We discuss how
we advance our understanding of code review practice, mitigation
of turnover risk through FaR, and evaluate reviewer recommender
systems on diverse outcome measures.
7.1 Understanding Code Review Practice
Fagan [11] introduced software inspections in 1976 with a detailed
experiment that conclusively showed that inspection found defects
earlierin thedesignprocessand thatunrevieweddesign artifacts
leadtodefectsthatslippedthroughtolatterstagesincreasedoveralleffort.Inthesubsequent40years,codereviewhasbeenextensively
studied. Early works focused on examining the process [ 10,11].
However, Porter et al.[37] demonstrated that process was much
less of a factor than ensuring expertise during review. Current
codereviewpracticefavorsalightweightprocessthatfocuseson
expert discussion of changes to the system [ 1,6,7,42,44] that still
improves software quality [ 27,43]. We show that RetentionRec has
the highest ΔExpertise among all expert recommenders with an
average of 16.59%. RevOwnRec andAuthorshipRec that focus on
ownership have an average of 15.17% and 11.29%, respectively. We
also found that focusing on learners will reduce Expertise by up to
-26.55%.
Recent works that interview reviewers, find that experts tend to
be overloaded with their review workloads [ 17,47] and that it is
oftendifficulttofindanavailableexpertreviewer[ 17,44,52].More-
over, it has shown that high overall workload could lead to poor
review participation[ 51] and requesting feedback from experts can
leadtodelaysfromlackofavailabilityandalsofeweropportunities
forknowledgedissemination[ 17].Weshowthattherelationship
between Expertise andCoreWorkload is not straightforward. For
instance, cHRevandAuthorshipRec improvethe Expertise whileat
the same time reduce the CoreWorkload by -3.49% and -3.68% on
average,respectively.Ontheotherhand, TurnoverRec drastically
reducesExpertise by-26.55% whileincreases the CoreWorkload by
11921.07% and Sofiaimproves the Expertise with a negligible change of
0.09% inCoreWorkload .
7.2 Turnover-Induced Knowledge Loss and
Mitigation
Turnoverdeprivestheprojectoftheleaver’sexperienceandknowl-
edge [19,53] and has been shown to increase the number of de-
fects [32]. Previous research has quantified the knowledge loss
from turnover and shown that projects with very high turnover
aresusceptibletoasmuchasfivetimestheexpectedloss[ 34,45].
However, these works considered authorship as the only way of
gaining knowledge about files.
Incontrastwithpriorwork,weincludetheknowledgegained
from conducting reviews into the turnover risk calculations be-
cause interviews with developers show that code review is anopportunity for learning and it plays a vital role in distributingknowledge[
1,6,17,42,47]. Two separate studies quantified the
knowledge gained during code review and showed that at both
Google [47] and Microsoft [ 42] code review doubles the number of
files that developers know. Furthermore, Thongtanunam et al.[50]
showed that reviewers of modules are often not authors of the
module [50]. In Section 4.1, our empirical results show that review
naturally reducesturnover risk. We show thatwhen only authors
areconsideredknowledgeableanaverageof79%ofthetotalfiles
areatrisk.Whenbothauthorsandreviewersareconsideredknowl-
edgeabletheaverage FaRis32%.Thisreductioninfarshowsthat
substantial knowledge is attained during code review.
In this work, we design recommenders that explicitly distribute
knowledge by suggesting reviewers who would learn about the
filesunderreview.Weshowthatbydistributingknowledgeamong
developers who have a higher retention potential, there is a FaR
reduction of -29.54% and -28.27% for TurnoverRec andSofia, which
outperformscHRevwhichincreases FaRby4.15%.Theadvantageof
using code review in mitigating knowledge loss is that it adds littleadditionaleffortbecausecodereviewisalreadyacommonpractice
on software teams. In contrast, prior works on turnover mitiga-
tion suggest increasing documentation with blogs, formalizing the
process of documenting bugs in issue trackers, and participatinginStackOverflowandinternalQAforums[
35,40].Eachstrategy
requires additional developer effort especially for developers who
are expert enough to answer questions and write documentation.
7.3 Recommenders
Identifying the right reviewers for a given change is a challenging
and critical step in the code review process [ 1,2,11,17,52,56].
Inappropriate selection of reviewers can slow down the review
process [52] or lower the quality of inspection [ 1,7]. The research
on reviewer recommendersfocus onthe problemof automatically
assigningreviewrequeststotheexpertdeveloperswhoaremost
likely to provide better feedback [2, 18, 21, 52, 54–56].
Advanced recommenders have been proposed which are built
uponmachinelearning[ 21],textmining[ 54],andsocialrelation
graphs [55]. However, these papers do not provide public imple-
mentation of their recommenders. Re-implementing and testing
these recommenders against our outcome measures is beyond the
scopewesetforthispaper.Wehopefutureworkwillexaminetheserecommenders, and we release all our code and data to facilitate
replication and advancement of review recommenders [29].
The existingrecommenders have been evaluated using accuracy
metricssuchas Top-KandMRRthatmeasurehowaccuratelythe
recommendationsmatchtheactualdevelopersthatwereinvolvedin
a review. This evaluation is based upon the assumption that actual
reviewers were among the best candidates to review a change[ 23].
However,itisreportedthatthefocusonaccuracyrarelyprovides
additionalvaluefordevelopersbecausetherecommendationsare
obvious[23]. Furthermore, in teams with strong code ownership,
findingrelevantexpertsisnotproblematic[ 47].Forreplicationcom-
pletenesswecalculatedMRR.OurresultsconfirmKovalenco et al.’s
findings that a broader perspective is needed when evaluating rec-
ommenders. We showed that recommenders with similar MRR
valuesmayhaveentirelydifferentimpacton Expertise,CoreWork-
load, andFaR. For instance, RevOwnRec andRetentionRec have a
difference of 0.06 in MRR while the difference between their ΔFaR
is 81.10%. LearnRec andTurnoverRec have a difference of 0.07 in
MRR while the difference between their ΔCoreWorkload and ΔFaR
is 40.58% and 92.58%.
8 CONCLUDING REMARKS
Inthisstudy,weprovideanovelevaluationframeworkforreviewer
recommenders based their impact on Expertise,CoreWorkload , and
FilesatRisktoturnover(FaR ).Weshowthatselectingreviewers
solelybasedonownership,expertise,orlearningproxymeasures
does not balance all three outcomes and leads to a knowledge
concentration, low knowledge retention, or low expertise.
Theoutcomeofthisworkis Sofiathatcombinesthestate-of-the-
art expert recommender, cHRev, with the learning and retention
recommender, TurnoverRec .Thisbi-functionalrecommenderadapts
itselftothecontextofthereview.Itdistributesknowledgewhen
therearefilesunderreviewthat areatrisktoturnover,butother-
wise suggests experts. Through simulation we show that Sofiais
theonlyrecommenderthatbalancesthethreeoutcomessimulta-
neously.Thisstrategyallowsustoincreasethelevelof Expertise
during review by 6.27%, while having a minor impact on workload,
ΔCoreWorkload0.09%,andreducingthenumberoffilesatriskwith
aΔFaR of -28.27%.
Werelease Sofiabotasanopensourcesoftwarethatfullyinte-
grateswithGitHubpullrequestsandprovidesreviewerrecommen-dations. The recommendations complement a developer’s intuitionandexperiencebyprovidingsimplerationaleforeachreviewcandi-
date, such as showing how active a candidate has been, how many
files he or she would learn about if they performed the review,
and howmany of the files underreview they havemodified orre-
viewed in the past. To the best of our knowledge, existing reviewer
recommendersincludingMicrosoft’sCodeFlow[ 42]andGoogle’s
Gerrit [47] do not explicitly recommend reviewers based on dis-
tributingknowledgetoreduceturnover.Futureworkisnecessary
tofullyevaluate Sofiaandtounderstandthecostsandbenefitsof
recommending “learner” reviewers in practice.
REFERENCES
[1]Alberto Bacchelli and Christian Bird. 2013. Expectations, outcomes, and chal-
lenges of modern code review. In Proceedings of the 2013 international conference
on software engineering. IEEE Press, 712–721.
1193[2]Vipin Balachandran. 2013. Reducing human effort and improving quality in peer
code reviews using automatic static analysis and reviewer recommendation. In
Proceedings of the 2013 International Conference on Software Engineering . IEEE
Press, 931–940.
[3]Lingfeng Bao, Zhenchang Xing, Xin Xia, David Lo, and Shanping Li. 2017. Who
willleavethecompany?:alarge-scaleindustrystudyofdeveloperturnoverby
mining monthly work report. In 2017 IEEE/ACM 14th International Conference on
Mining Software Repositories (MSR). IEEE, 170–181.
[4]Christian Bird, Alex Gourley, Prem Devanbu, Michael Gertz, and Anand Swami-
nathan.2006. Miningemailsocialnetworks.In Proceedingsofthe2006interna-
tional workshop on Mining software repositories. ACM, 137–143.
[5]Christian Bird, Nachiappan Nagappan, Brendan Murphy, Harald Gall, and
Premkumar Devanbu. 2011. Don’t touch my code!: examining the effects of
ownership on software quality. In Proceedings of the 19th ACM SIGSOFT sympo-
sium and the 13th European conference on Foundations of software engineering .
ACM, 4–14.
[6]AmiangshuBosu,JeffreyCCarver,ChristianBird,JonathanOrbeck,andChristo-
pher Chockley. 2016. Process aspects and social dynamics of contemporary
codereview:Insightsfromopensourcedevelopmentandindustrialpracticeat
microsoft. IEEE Transactions on Software Engineering 43, 1 (2016), 56–75.
[7]Amiangshu Bosu, Michaela Greiler, and Christian Bird. 2015. Characteristics of
useful code reviews: An empirical study at microsoft. In 2015 IEEE/ACM 12th
Working Conference on Mining Software Repositories. IEEE, 146–156.
[8]Gerardo Canfora, Massimiliano Di Penta, Rocco Oliveto, and Sebastiano
Panichella.2012. Whoisgoingtomentornewcomersinopensourceprojects?.InProceedingsoftheACMSIGSOFT20thInternationalSymposiumontheFoundations
of Software Engineering. ACM, 44.
[9]Eleni Constantinou and Tom Mens. 2017. An empirical comparison of developer
retentionintheRubyGemsandnpmsoftwareecosystems. InnovationsinSystems
and Software Engineering 13, 2-3 (2017), 101–115.
[10]MichaelFagan.2002. Designandcodeinspectionstoreduceerrorsinprogram
development. In Software pioneers. Springer, 575–607.
[11]M. E. Fagan. 1976. Design and Code Inspections to Reduce Errors in Program
Development. IBM Systems Journal 15, 3 (1976), 182–211.
[12]MatthieuFoucault,MarcPalyart,XavierBlanc,GailCMurphy,andJean-Rémy
Falleri.2015. Impactofdeveloperturnoveronqualityinopen-sourcesoftware.In
Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering.
ACM, 829–841.
[13]ThomasFritz,GailCMurphy,andEmilyHill.2007. Doesaprogrammer’sactivity
indicate knowledge of code?. In Proceedings of the the 6th joint meeting of the
EuropeansoftwareengineeringconferenceandtheACMSIGSOFTsymposiumon
The foundations of software engineering. ACM, 341–350.
[14]Tudor Girba, Adrian Kuhn, Mauricio Seeberger, and Stéphane Ducasse. 2005.
Howdevelopersdrivesoftwareevolution.In EighthInternationalWorkshopon
Principles of Software Evolution (IWPSE’05). IEEE, 113–122.
[15] Georgios Gousios. 2013. The GHTorent dataset and tool suite. In Proceedings of
the 10th working conference on mining software repositories. IEEE Press, 233–236.
[16]Georgios Gousios, Martin Pinzger, andArie van Deursen. 2014. An exploratory
studyofthepull-basedsoftwaredevelopmentmodel.In Proceedingsofthe36th
International Conference on Software Engineering. ACM, 345–355.
[17]Michaela Greiler, Christian Bird, Margaret-Anne Storey, Laura MacLeod, and
JacekCzerwonka.2016. CodeReviewinginthe Trenches:UnderstandingChal-
lenges, Best Practices and Tool Needs. (2016).
[18]ChristophHannebauer,MichaelPatalas,SebastianStünkel,andVolkerGruhn.
2016. Automatically recommending code reviewers based on their expertise:
An empirical comparison. In Proceedings of the 31st IEEE/ACM International
Conference on Automated Software Engineering. ACM, 99–110.
[19]Mark A Huselid. 1995. The impact of human resource management practiceson turnover, productivity, and corporate financial performance. Academy of
management journal 38, 3 (1995), 635–672.
[20]DanielIzquierdo-Cortazar,GregorioRobles,FelipeOrtega,andJesusMGonzalez-
Barahona. 2009. Using software archaeology to measure knowledge loss in
softwareprojectsduetodeveloperturnover.In 200942ndHawaiiInternational
Conference on System Sciences. IEEE, 1–10.
[21]GaeulJeong,SunghunKim,ThomasZimmermann,andKwangkeunYi.2009. Im-provingcodereviewbypredictingreviewersandacceptanceofpatches. Research
on software analysis for error-free computing center Tech-Memo (ROSAEC MEMO
2009-006) (2009), 1–18.
[22]HuzefaKagdi,MaenHammad,andJonathanIMaletic.2008. Whocanhelpme
with this source code change?. In 2008 IEEE International Conference on Software
Maintenance. IEEE, 157–166.
[23]VladimirKovalenko,NavaTintarev,EvgenyPasynkov,ChristianBird,andAl-
berto Bacchelli. 2018. Does reviewer recommendation help developers? IEEE
Transactions on Software Engineering (2018).
[24]BinLin,GregorioRobles,andAlexanderSerebrenik.2017. Developerturnoverin
global, industrial open source projects: Insights from applying survival analysis.
In2017IEEE12thInternationalConferenceonGlobalSoftwareEngineering(ICGSE).
IEEE, 66–75.[25]Jakub Lipcak and Bruno Rossi. 2018. A Large-Scale Study on Source Code
Reviewer Recommendation. In 2018 44th Euromicro Conference on Software Engi-
neering and Advanced Applications (SEAA). IEEE, 378–387.
[26]David W McDonald and Mark S Ackerman. 2000. Expertise recommender: a
flexiblerecommendationsystemandarchitecture.In Proceedingsofthe2000ACM
conference on Computer supported cooperative work. ACM, 231–240.
[27]Shane McIntosh, Yasutaka Kamei, Bram Adams, and Ahmed E. Hassan. 2016. An
Empirical Study of the Impact of Modern Code Review Practices on Software
Quality.Empirical Software Engineering 21, 5 (2016), 2146–2189.
[28]Ehsan Mirsaeedi and Peter C. Rigby. 2020. GitHub App: Sofia Bot. https://github.
com/apps/sofiarec. (2020).
[29]EhsanMirsaeediandPeterC.Rigby.2020. ReplicationPackageandRelationalGit.
https://github.com/cesel/relationalgit. (2020).
[30]Ehsan Mirsaeedi and Peter C. Rigby. 2020. Sofia Bot Source code. https://github.
com/cesel/Sofia. (2020).
[31]Audris Mockus. 2009. Succession: Measuring transfer of code and developer
productivity. In Proceedings of the 31st International Conference on Software Engi-
neering. IEEE Computer Society, 67–77.
[32]Audris Mockus. 2010. Organizational volatility and its effects on software de-
fects. InProceedings of the eighteenth ACM SIGSOFT international symposium on
Foundations of software engineering. ACM, 117–126.
[33]AudrisMockusandJamesDHerbsleb.2002. Expertisebrowser:aquantitativeap-proachtoidentifyingexpertise.In Proceedingsofthe24thInternationalConference
on Software Engineering. ICSE 2002. IEEE, 503–512.
[34]MathieuNassifandMartinPRobillard.2017. Revisitingturnover-inducedknowl-
edge loss in software projects. In 2017 IEEE International Conference on Software
Maintenance and Evolution (ICSME). IEEE, 261–272.
[35]LooGeokPee,AtreyiKankanhalli,GekWooTan,andGZTham.2014. Mitigating
the impact of member turnover in information systems development projects.
IEEE Transactions on Engineering Management 61, 4 (2014), 702–716.
[36]NancyPekala.2001. Holdingontotoptalent. JournalofPropertymanagement
66, 5 (2001), 22–22.
[37]Adam Porter, Harvey Siy, Audris Mockus, and Lawrence Votta. 1998. Under-
standing the sources of variation in software inspections. ACM Transactions on
Software Engineering and Methodology (TOSEM) 7, 1 (1998), 41–79.
[38]Foyzur Rahman and Premkumar Devanbu. 2011. Ownership, experience and
defects:afine-grainedstudyofauthorship.In Proceedingsofthe33rdInternational
Conference on Software Engineering. ACM, 491–500.
[39]Mohammad Masudur Rahman, Chanchal K Roy, and Jason A Collins. 2016. Cor-
rect: code reviewer recommendation in github based on cross-project and tech-nology experience. In 2016 IEEE/ACM 38th International Conference on Software
Engineering Companion (ICSE-C). IEEE, 222–231.
[40]Mehvish Rashid, Paul M Clarke, and Rory V OâĂŹConnor. 2017. Exploring
knowledgelossinopensourcesoftware(OSS)projects.In Internationalconference
onsoftwareprocessimprovementandcapabilitydetermination.Springer,481–495.
[41]Peter Rigby, Brendan Cleary, Frederic Painchaud, Margaret-Anne Storey, and
Daniel German. 2012. Contemporary peer review in action: Lessons from open
source development. IEEE software 29, 6 (2012), 56–61.
[42]PeterCRigbyandChristianBird.2013. Convergentcontemporarysoftwarepeer
reviewpractices.In Proceedingsofthe20139thJointMeetingonFoundationsof
Software Engineering. ACM, 202–212.
[43]PeterCRigby,DanielMGerman,LauraCowen,andMargaret-AnneStorey.2014.
Peer review on open-source software projects: Parameters, statistical models,
andtheory. ACMTransactionsonSoftwareEngineeringandMethodology(TOSEM)
23, 4 (2014), 35.
[44]Peter C Rigby and Margaret-Anne Storey. 2011. Understanding broadcast based
peer review on open source software projects. In 2011 33rd International Confer-
ence on Software Engineering (ICSE). IEEE, 541–550.
[45]P.C.Rigby,Y.C.Zhu,S.M.Donadelli,andA.Mockus.2016. QuantifyingandMitigating Turnover-Induced Knowledge Loss: Case Studies of Chrome and aProject at Avaya. In 2016 IEEE/ACM 38th International Conference on Software
Engineering (ICSE). 1006–1016. https://doi.org/10.1145/2884781.2884851
[46]MartinPRobillard,MathieuNassif,andShaneMcIntosh.2018. ThreatsofAg-gregating Software Repository Data. In 2018 IEEE International Conference on
Software Maintenance and Evolution (ICSME). IEEE, 508–518.
[47]Caitlin Sadowski, Emma Söderberg, Luke Church, Michal Sipko, and AlbertoBacchelli. 2018. Modern code review: a case study at google. In Proceedings of
the 40th International Conference on Software Engineering: Software Engineering
in Practice. ACM, 181–190.
[48]PratyushNSharma,JohnHulland,andSheraeDaniel.2012. Examiningturnover
in open source software projects using logistic hierarchical linear modelingapproach. In IFIP International Conference on Open Source Systems. Springer,
331–337.
[49]Meaghan Stovel and Nick Bontis. 2002. Voluntary turnover: knowledge
management–friend or foe? Journal of intellectual Capital 3, 3 (2002), 303–322.
[50]Patanamon Thongtanunam, Shane McIntosh, Ahmed E Hassan, and Hajimu Iida.
2016. Revisitingcodeownershipanditsrelationshipwithsoftwarequalityinthe
scope of modern code review. In Proceedings of the 38th international conference
1194on software engineering. ACM, 1039–1050.
[51]PatanamonThongtanunam,ShaneMcIntosh,AhmedE.Hassan,andHajimuIida.
2017. Review ParticipationinModern CodeReview: AnEmpiricalStudy ofthe
Android,Qt,andOpenStackProjects. EmpiricalSoftwareEngineering 22,2(2017),
768–817.
[52]Patanamon Thongtanunam, Chakkrit Tantithamthavorn, Raula Gaikovina Kula,
Norihiro Yoshida, Hajimu Iida, and Ken-ichi Matsumoto. 2015. Who should
review my code? a file location-based code-reviewer recommendation approach
formoderncodereview.In 2015IEEE22ndInternationalConferenceonSoftware
Analysis, Evolution, and Reengineering (SANER). IEEE, 141–150.
[53]Zeynep Ton and Robert S Huckman. 2008. Managing the impact of employee
turnoveronperformance:Theroleofprocessconformance. OrganizationScience19, 1 (2008), 56–68.
[54]Xin Xia, David Lo, Xinyu Wang, and Xiaohu Yang. 2015. Who should review
thischange?:Puttingtextandfilelocationanalysestogetherformoreaccurate
recommendations.In 2015IEEEInternationalConferenceonSoftwareMaintenance
and Evolution (ICSME). IEEE, 261–270.
[55]Yue Yu, Huaimin Wang, Gang Yin, and Tao Wang. 2016. Reviewer recommenda-
tionforpull-requestsinGitHub:Whatcanwelearnfromcodereviewandbug
assignment? Information and Software Technology 74 (2016), 204–218.
[56]Motahareh Bahrami Zanjani, Huzefa Kagdi, and Christian Bird. 2016. Automati-
cally Recommending Peer Reviewers in Modern Code Review. IEEE Trans. Softw.
Eng.42, 6 (June 2016), 530–543. https://doi.org/10.1109/TSE.2015.2500238
1195