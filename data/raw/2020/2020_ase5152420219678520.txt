Subtle Bugs Everywhere: Generating
Documentation for Data Wrangling Code
Chenyang Y ang
Peking UniversityShurui Zhou
University of TorontoJin L.C. Guo
McGill UniversityChristian K ¨astner
Carnegie Mellon University
Abstract —Data scientists reportedly spend a signiﬁcant amount
of their time in their daily routines on data wrangling, i.e.
cleaning data and extracting features. However , data wranglingcode is often repetitive and error-prone to write. Moreover , it iseasy to introduce subtle bugs when reusing and adopting existingcode, which results in reduced model quality. T o support datascientists with data wrangling, we present a technique to generatedocumentation for data wrangling code. We use (1) programsynthesis techniques to automatically summarize data transfor-mations and (2) test case selection techniques to purposefullyselect representative examples from the data based on executioninformation collected with tailored dynamic program analysis.We demonstrate that a JupyterLab extension with our techniquecan provide on-demand documentation for many cells in popularnotebooks and ﬁnd in a user study that users with our pluginare faster and more effective at ﬁnding realistic bugs in datawrangling code.
Index T erms—computational notebook, data wrangling, code
comprehension, code summarization
I. I NTRODUCTION
It has been reported that data scientists spend a signiﬁcant
amount of time and effort on data cleaning and feature
engineering [1], the early stages in data science pipelines,collectively called data wrangling [2] in the literature. Typical
data wrangling steps include removing irrelevant columns,converting types, ﬁlling missing values, and extracting andnormalizing important features from raw data. Data wranglingcode is often dense, repetitive, error-prone, and generally notwell-supported in the commonly used computational-notebookenvironments.
Importantly, data wrangling code often contains subtle prob-
lems that may not be exposed until later stages, if at all. Inour evaluation, we found dozens of instances of suspiciousbehavior, such as computations that use the wrong source, thatare not persisted, or that inconsistently transform part of thedata. Although they do not crash the program, they clearly vio-late the code’s apparent intention (often speciﬁed in commentsand markdown cells), thus we consider them as bugs. Unfor-tunately, as tests are very rare in data science code in note-books [3], these bugs remain undetected even for many highly“upvoted” notebooks on popular data science sites like Kaggle.
In this work, we propose to automatically generate concise
summaries for the data wrangling code and purposefully
select representative examples to help users understand the
impact of the code on their data. This form of automated
documentation is useful for multiple scenarios that require
code understanding:•Debugging: Data wrangling code is often concise, se-
quencing multiple nontrivial data transformations, asin our example in Fig. 1a, but also usually not welltested [3]. Data scientists currently mostly rely on codereading and inserting print statements to look for potentialproblems.
•Reuse: Data scientists heavily reuse code through copy-
ing and editing code snippets, often within a notebook,from other notebooks, from tutorials, or from StackOver-ﬂow [4]. At the same time reusing data wrangling codecan be challenging and error-prone [5], especially if thereused code needs to be adapted for the data scientist’sown data.
•Maintenance: Data science code in notebooks is often
not well-documented [3], [6], [7], yet data science codeneeds to be maintained and evolve with changes indata and models, especially when adopted in productionsettings [8]. To avoid mistakes in maintenance tasksand degrading model quality over time, understandingexisting data wrangling code and assumptions it makesis essential.
Our work is inspired by past work on code summarization to
automatically create summaries of code fragments that couldserve as documentation for various tasks. However, whileexisting code summarization work [9] tries to characterizewhat a code fragment does generally (for all possible inputs),our approach summarizes what effect code has on speciﬁc
input data in the form of a dataframe, highlighting represen-tative changes to rows and columns of tabular data. Giventhe data-centric nature of data wrangling code, understandingthe effect that data wrangling code has on the data often isthe immediate concern for data scientists. To the best of ourknowledge, this is a novel view on summarization, tailored fordebugging, reuse, and maintenance tasks of data scientists.
Moreover, our approach generates the documentation on
demand for the code and data at hand to help with program
comprehension. This is achieved by instrumenting data sciencecode to collect runtime data and select data-access pathsand branches executed at runtime, using program synthesis
techniques to generate short descriptive summaries, and using
techniques inspired by test-suite minimization to select and
organize examples. We integrate our tool, W
RANGLE DOC,i n
JupyterLab, a commonly used notebook environment.
We evaluated our approach and tool in two ways. First, we
conducted a case study with 100 Kaggle notebooks to evaluate
3042021 36th IEEE/ACM International Conference on Automated Software Engineering (ASE)
DOI 10.1109/ASE51524.2021.000362021 36th IEEE/ACM International Conference on Automated Software Engineering (ASE) | 978-1-6654-0337-5/21/$31.00 ©2021 IEEE | DOI: 10.1109/ASE51524.2021.9678520
978-1-6654-0337-5/21/$31.00  ©2021  IEEE
1 data = pd.read_csv(’./data.csv’)
2# x = load some other data that’s not relevant for the
next cell
3# first change ’Varies with device’ to nan
4def to_nan(item):
5 ifitem == ’Varies with device’:
6 return np.nan
7 else:
8 return item
9
10 data[’Size’] = data[’Size’].map(to_nan)1112# convert Size
13 num = data.Size.replace(r’[kM]+$’, ’’, regex=True).
astype(float)
14 factor = data.Size.str.extract(r’[\d\.]+([KM]+)’, expand
=False)
15 factor = factor.replace([’k’,’M’], [10 **3, 10 **6]).
fillna(1)
16 data[’Size’] = num *factor.astype(int)
1718# fill nan
19 data[’Size’].fillna(data[’Size’].mean(), inplace = True)
20# some training code reading combined
21 targets = data[’Target’]22 data.drop(’Target’, inplace=True, axis=1)2324 clf = RandomForestClassifier(n_estimators=50,
max_features=’sqrt’)
25 clf = clf.fit(data, targets)
(a) Three notebook cells, loading tabular data, transforming the ‘Size’
column (converting k and M to numbers and replacing ‘varies withdevice’ by mean value), and learning a model from the data. Whilethis code is fairly linear and relies heavily on common APIs, itencodes nontrivial transformations compactly, that are not alwayseasy to understand.
(b) W RANGLE DOC Interface: documentation of the second
cell;: data ﬂow into or out of the cell, : concise sum-
mary of changes, : highlighting changed columns, : meta
information (type, unique, range) for columns, : selected
examples.
Fig. 1: Excerpt of real data wrangling code from a Kaggle competition and corresponding generated documentation with
WRANGLE DOC. Due to case sensitivity in regular expressions, values with a ‘k’ are not transformed correctly, as easily visible
in the generated summary.
correctness and runtime overhead and additionally explore
the kind of documentation we can generate for commonnotebooks. Second, we conducted a human-subject study toevaluate whether W
RANGLE DOC improves data scientists’ ef-
ﬁciency in tasks to debug notebooks. Through the two studies,we provide evidence that our approach is both practical andeffective for common data wrangling code.
Overall, we make the following contributions:
•An approach to summarize data transformations for datawrangling code, based on program synthesis.
•An approach to purposefully select rows to illustratechanges in data wrangling code, inspired by test suiteminimization techniques.
•A prototype implementation as a JupyterLab plugin.
•Empirical evidence showing that our approach can accu-rately generate summaries for nontrivial, real-world datascience code with acceptable overhead.
•A user study ﬁnding that our approach improves data sci-entists’ efﬁciency when debugging data wrangling code.
We share the tool and our supplementary material on GitHub.
1
1https://github.com/malusamayo/notebooks-analysisII. D ESIGN MOTIV A TIONS
Many prior studies explored practices of data scientists in
notebooks and challenges that they face. With the surginginterest in machine learning, notebooks are a very populartool for learning data science and for production data scienceprojects [6], [7], [10], [11], used by data scientists withwidely varying programming skills and software engineeringbackground. Data science work is highly exploratory and iter-ative [11]–[13] with heavy use of copy-and-paste from othernotebooks and online examples [14]. While researchers foundwide range of challenges, including reproducibility [3], [15],[16], collaborative editing [17], [18], and reliability [5], wefocus on challenges regarding comprehension and debugging.
Data wrangling code can be challenging to understand:
Although it is typically linear and structured in short cells,data wrangling code can be dense and make nontrivial trans-formations with powerful APIs, as in our example (Fig. 1).
To better capture how data scientists approach understand-
ing data wrangling code, we conducted a small informal ex-periment, in which we gave four volunteers with data scienceexperience a notebook and two tasks that required programcomprehension. Speciﬁcally, we asked them to modify the
305notebook to accommodate changes to the input dataframe and
to look for possible improvements of model performance, allwhile thinking aloud [19].
We observed two main strategies that our participants used
to understand data wrangling code. On the one hand, theyfrequently reasoned statically about the code, inspecting the
code line by line without running it. In this process, theyoften left the notebook to look up the API documentationand code examples as needed for the numerous functions inthe used data science libraries, such as extract and replace
and their various arguments in our example. On the otherhand, they also reasoned dynamically by observing executions.
Our participants frequently injected print statements at the
beginning and the end of cells, or in a new cell, to inspect datasamples (typically the ﬁrst few rows) and manually comparethem before and after the data wrangling steps. We sawthat dynamic reasoning quickly became overwhelming andtedious with large amounts of data, especially if data triggeringproblematic behavior is not part of the ﬁrst few rows. Inour example (Fig. 1a), the ﬁrst ﬁve rows of the 9360 rowscontained sizes ending with the letter ‘M’ and containing thevalue of ‘V aried with device’, but not sizes ending in ‘k’,
which makes the incorrect transformations of ‘k’ ending rowsdifﬁcult to spot.
Existing tools are limited: Notebook environments are
evolving and various new tools are proposed by practitionersand researchers [20]. For example, more recent notebookenvironments now provide code completion features and canshow API documentation in tooltips; the IDE PyCharm andJupyterLab extensions integrate a traditional debugger—allstandard features in IDEs. Several extensions, like pandas
proﬁling [21], help inspect data stored in variables.
Y et tool support for understanding data wrangling code
is still limited and does not well support the activities weobserved. Classic tools like debuggers, if available at all,do not provide a good match for data-centric, linear, andoften exploratory notebook code, where a single line canapply transformations to thousands of rows at once and actualcomputations are performed deep in libraries (often in nativecode). Tools for exploring data in variables are useful forunderstanding data at one point in time, but do not help inunderstanding complex transformations within a cell.
Data wrangling code is frequently buggy: Several re-
searchers have pointed out code quality problems in note-books [11], [22], [23]. Notebooks almost never include anytesting code [3] and practitioners report testing as a commonpain point [5]. The commonly used data wrangling APIsare large and can be easily misunderstood [24]. Due to thedynamic and error-forgiving nature of Python and the Pandaslibrary design, buggy code often does not crash with anexception but continues to execute with wrong values, whichcould subsequently reduce model accuracy.
It is generally easy to introduce mistakes in data wrangling
code, which became very obvious when we inspected exam-ples of documentation generated with our tool on popularnotebooks (some among the most upvoted notebooks onAPI misuse
1# attempting to remove na values from column, not table
2 df[’Join_year’] = df.Joined.dropna().map(lambda x: x.
split(’,’)[1].split(’’ )[1])
3
4# loc[] called twice, resulting in assignment
5# to temporary column only
6 df.loc[idx_nan_age,’Age’].loc[idx_nan_age] = df[’Title’
].loc[idx_nan_age].map(map_means)
78# astype() is not an in-place operation
9 df["Weight"].astype(str).astype(int)
Typos
10# reading from wrong table (should be df2)
11 df2[’Reviews_count’] = df1[’Reviews’].apply(int)
Data modelling problems
12# converting money to numbers, e.g., ’10k’ -> 10000.0
13# ignoring decimals, thus converting ’3.4k’ to 3.4000
14 df["Release Clause"]= df["Release Clause"].replace(regex
=[’k’], value=’000’)
15 df["Release Clause"]= df["Release Clause"].astype(str).
astype(float)
Fig. 2: Examples of subtle bugs in data wrangling code,
ranging from data cleaning stage (e.g., normalizing the col-umn ‘Reviews’ to integers) to feature engineering stage (e.g.,extracting new feature ‘Join
year’ from the ‘Joined’ column).
Kaggle). Without actively looking for bugs (it is not alwaysclear what the code intends to do), we found many exampleswith subtle problems in data wrangling code.
For example, there is a subtle bug in our example in
Fig. 1a where the code tries to convert ‘k’ to 1000 and ‘M’to 1,000,000 in download counts: A capitalized ‘K’ in Line14 results in converting ‘k’ to 1 instead of 1000. The codeexecutes without exception, but produces wrong results, e.g.,670.0 for ‘670.0k’ rather than the intended 670000.0. Theproblem could have been found easily if one could observeexample transformations with ‘k’.
In Fig. 2, we illustrate three kinds of problems in data
wrangling code that we found repeatedly across 100 popular
notebooks in our evaluation (described later in Section V -A):
•API misuse is common where a function call looks
plausible, but does not have the intended effect on theinput data (e.g., dropna does not remove the entire row
of a table if applied to a single column). This commonlyresults in computations that are not persisted and have noeffect on the data used later.
•Simple typos in variable names, column names, and reg-
ular expressions are the source of many other problems,often leading to wrong computations.
•Finally, multiple problems relate to incorrect modeling
of data, often stemming from wrong assumptions aboutthe different kinds of data in the dataset, thus missingrare cases.
All the above problems can be difﬁcult to locate without aclear and thorough understanding of the API speciﬁcations,how they are used in the data wrangling code, and the impact
306on the speciﬁc instances from the input dataset.
III. S OLUTION OVERVIEW
Before we describe the technical details of how we generate
documentation, let us illustrate the kind of documentation
we generate with W RANGLE DOC from a notebook user’s
perspective. In a nutshell, we summarize the changes a codefragment (typically a notebook cell) performs on dataframesand show them in a side panel through a JupyterLab extension,as illustrated in Fig. 1b for our running example. Ourdocumentation includes the following pieces of information:
: We identify the dataframes (tabular variables) that ﬂow
in and out of the code fragment to identify which importantvariables are read and written in the code fragment. To avoidinformation overload, we deliberately include only variablesthat are later used in the notebook again, but not temporaryvariables. In our running example, the dataframe data is
changed for subsequent cells, whereas we omit temporaryvariables num and factor from our documentation.
: We provide a concise summary of the transformations
for each changed dataframe using a domain speciﬁc language(DSL) we designed. The summary describes which columnsof the dataframe were added, removed, or changed, howcolumns were changed, and whether rows were removed. Thesummary intentionally uses somewhat generic function nameslike str
transform to indicate that strings were manipulated
without describing the details of that transformation, whichcan be found in the code. These summaries provide a quickoverview of what the code does, helping to ensure thatthe (static) understanding of APIs aligns with the observedexecution. It is particularly effective at highlighting “data notwritten” bugs, where the summary would clearly indicate thatno data was changed. For example, data scientists can easilyspot all API misuse bugs in Fig. 2 when they encounter the
unexpected summary of “no changes” for their transformationcode. Similarly, the typos bug in Fig. 2 can also be surfaced as
the summary “Review
count = int(merge(Reviews))” would
show that different items are merged, whereas the code intendsto convert strings to integers without merging.
–: We show sample data from the modiﬁed dataframes,
speciﬁcally comparing a dataframe’s values before andafter the cell: The summary highlights which columns havebeen modiﬁed ( ), highlights changes to column data and
metadata, including types, cardinality, and range of values(). This direct before-after comparison highlights thechanges that would usually require manual comparison of twodataframes, hence reducing the manual efforts of comparingthe output of print statements in dynamic debugging.
: Finally, where classic print statements would simply
show the ﬁrst few rows of long dataframes, our documentationpurposefully groups rows that take the same path at branchingdecisions in transformation code, showing one example eachand highlighting the number of other rows that take thesame path. Grouping rows by transformation decisions drawsattention to paths that may not occur in the ﬁrst few rows,making it easier to spot potential problems. For example, this
Input
notebookPython
scriptDef-use
Analysis
Run-time
informationRun
Instrumented
script
Patterng
Synthesis
Clustering
examplesDocumentation
Instrumenting
tracing code
Summary 
Synthesizer
Example 
SelectorExecution pathsDynamic values
Fig. 3: Approach overview.
makes the bug in Fig. 1 that does not transform ‘k’ to 1000obvious, even though it occurs only in 3 percent of all rows andnot in any early ones. Our approach enables the data scientiststo examine those rare examples and corner cases effectively.
As we will show, the above forms of documentation support
effective static and dynamic reasoning, which is the foundationof various debugging, reuse, and maintenance tasks, and theyhelp surface subtle bugs in data wrangling code.
IV . W
RANGLE DOC:S YNTHESIZING SUMMARIES AND
SELECTING EXAMPLES
WRANGLE DOC generates documentation on demand with
two components: Summary Synthesizer and Example Selector.
Both collect information by analyzing and instrumenting anotebook’s code and observing it during its execution—seeFig. 3 for an overview. The Summary Synthesizer gathers
static and run-time information about access to dataframesand columns and runtime values of dataframes before andafter a cell to synthesize summary patterns (Fig. 1b, –).
The Example Selector traces branching decisions during data
transformations to cluster rows in a dataframe that share thesame execution paths (Fig. 1b, ).
A. Summary synthesis
The goal of synthesizing summaries is to derive a concise
description of how data is transformed by a fragment of datawrangling code, typically a notebook cell. To avoid distractingusers with implementation details, which may use nontrivialAPI sequences, external libraries, and custom code, we syn-thesize summaries that describe the relationship between databefore and after the code fragment. Through instrumentation,
we collect data of all variables (with emphasis on tabular datain dataframes) before and after the target code, from whichwe synthesize summaries that explain the differences, such asadded columns, removed rows, or changed values.
a) Synthesis approach: As in all summary generation,
there is an obvious tradeoff between providing concise sum-maries (e.g., ‘dataframe X was changed’) and detailed sum-
maries (e.g., “column Y was added with values computed by
removing all dashes from column Z, replacing ‘K’ at the endof the string by 1000, and then converting the result into anumber”). Summaries at either extreme are rarely useful: Too
307<df> := createCol(<df>, COL, <col>) 0
|modifyCol(<df>, COL, <col>) 0// modify column
|removeCol(<df>, COL) 0
|removeDuplicateRows(<df>, ROW+, COL*)0
|removeNullRows(<df>, ROW+, COL*)0
|removeRows(<df>, ROW+, COL*)1// remove some rows
|rearrangeCols(<df>, COL*,COL*)0// change col. order
|rearrangeRows(<df>, ROW*,ROW*)0// change row order
|concatRows(<df>, <df>) 0// concat dataframes by rows
|DATAFRAME 0
|compute() 15// unspecified dataframe computation
<col> := fillna(<col>) 2// fill null values
|merge(<col>) 2// merge items to reduce cardinality
|category(<col>) 2// convert columns to category type
|float(<col>) 2|str(<col>) 2|int(<col>) 2
|bool(<col>) 2|datetime64(<col>) 2// type conversion
|encode(<col>) 2// encode columns in consecutive ints
|one_hot_encoding(<col>) 2// encode columns in 0/1 ints
|type_convert(<col>) 3// other type conversion
|str_transform(<col>) 3// unspecified string transf.
|num_transform(<col>) 3// unspecified numerical transf.
|compute(<col_ref> *)15// unspecified col. computation
| <col_ref> *0
<col_ref> := DATAFRAME.COL 0
Fig. 4: The DSL for data transformation. Cost per expression
is indicated as subscript.
concise summaries do not convey much information, whereas
too detailed summaries might just paraphrase the code andprovide little beneﬁt over reading the code directly.
Our summary synthesis aims to ﬁnd a balance by describing
data wrangling code as expressions formed from an extensiblegrammar of transformation patterns, which describe the im-
pact of the transformations concisely and unambiguously. Forexample, pattern ﬁllna describes that null values are ﬁlled, but
not any details about how the new values are computed; patternstr
transform describes that a column with string values has
been manipulated, but not how; pattern one hot encoding
describes that a binary column was created for distinct valuesin a source column. We include generic catch-all summariesfor transformations we cannot further explain, such as com-
pute for generating data from unknown processes. While
the pattern language is easily extensible, we start with themost used patterns shown in Fig. 4, which we derived bymanually summarizing and grouping common transformationpatterns in dozens of sampled notebooks from GitHub. OurDSL supports abstracting multiple transformation patterns inone combined expression. The operation of applying ﬂoat to
the ‘Size’ column in data after str
transform in the running
exampling in Fig. 1a will be synthesized as “modifyCol(data,
Size, ﬂoat(str transform(data.Size)))” (cf. Fig. 1b, ).
During the summary synthesis, the synthesis engine
searches for the expressions in the pattern language that matchthe input-output examples created by the target code. Becausethere could exist multiple matches for the same input-outputpair, we design a cost function to select the best matched ex-pression. Concretely, we assign a higher cost to more genericpatterns (see Fig. 4) and consider the cost of an expression asthe sum of the cost of all patterns used in that expression. Thisway, our synthesis engine favors concise but concrete patterns.1.decl apply(depth:number, pattern:Pattern)
2.decl var(depth:number, type:ColType, nafilled:boolean,
carddrop:boolean, onehot:boolean, encode:boolean)
3
4var(depth + 1, type, true, carddrop, any, any): -
5 apply(depth, "fillna"),
6 var(depth, type, false, carddrop, _, _)
Fig. 5: We describe propagation of knowledge about rules in
terms of Datalog relations and illustrate the propagation rulefor “ﬁllna”. In line 1-2, there are two Datalog facts: apply and
var . We use var to track a column’s attributes (e.g., whether
its missing values are ﬁlled), and we use apply to represent
application of a pattern. The rule in line 4-6 propagates var
through a pattern “ﬁllna”. The attribute ‘naﬁlled’ is set to true,while ‘onehot’ and ‘encode’ are set to any, because any priorinformation about these is lost in the possible transformations.
While this kind of search is standard for program synthesis, the
novelty of our approach is in encoding summary generationas a synthesis problem, not in the synthesis algorithm itself.
b) Synthesis implementation: For the synthesis, we fol-
low a standard top-down enumerative search to explore ex-pressions of a given grammar, as shown in Algorithm 1.The synthesis engine incrementally enumerates expressionsallowed by the grammar by substituting non-terminals withnew expressions in order of increasing costs; the synthesisprunes the search space for partial expressions (i.e., containholes for some nonterminals) that cannot explain the input-output difference; it returns the expression that explains theinput-output difference with the lowest cost as all expressionsare maintained in a priority queue. The synthesis problem canbe encoded in standard synthesis engines like Rosette [25],but for simplicity, we implemented our own.
To validate whether a (partial) expression can explain the
input-output pair, we compute and check the result of anexpression against the output. Aside from parts that can beeasily checked concretely, such as the removal of columnsand rows, many patterns express generic computations thatcan match many input-output examples (e.g., ﬁllna). Here,
we reject infeasible expressions similar to how static analysesidentify problematic computations [26]: We track abstractfacts across patterns and each pattern has a transfer functionthat can generate or kill facts. Throughout these patterns, wetrack each column’s type (if known), whether it has missingvalues, its length, its cardinality, and whether it looks like acommon encoding pattern. This way we can check that ﬁllna
actually ﬁlled missing values without having to know concreteﬁlled values or having to worry about interactions with otherpatterns. In Fig. 5, we show an example rule for propagatingfacts when applying “ﬁllna”. Details of the other validationrules can be found in the supplementary material.
c) Data gathering and optimization: To collect the val-
ues of all variables before and after the target data wranglingcode, we instrument the user’s code to store values into ﬁles atruntime. To target the instrumentation, we use a simple static
308Algorithm 1 Enumeration-based synthesis algorithm
Input:
e⊿ Input-Output example pair
C⊿ Cost function for transformation patterns
s⊿ A set of accessed columns
1:function SYNTHESIS (e,C,s)
2:q← [/angbracketleftdf/angbracketright] ⊿Priority queue of patterns, ordered by C
3:top← “compute()”
4: whileqis not empty do
5: cur←q.dequeue ()
6: ifC(cur )>C (top) then
7: break ⊿Prune because elements left have higher cost
8: ifcur is fully resolved then
9: ifvalidate(e,cur )then
10: top←cur
11: else
12: newpatterns ←extend(cur,s)
13: forpinnewpatterns do
14: ifisfeasible (e,p) then
15: q.add (p)
16: returntop
def-use analysis (adopted from prior work [23]) to record only
values of variables that ﬂow into or out of the target code.
In addition, we collect the sequence of access paths for read
and write access to columns of dataframes by dynamicallyintercepting such access, to limit the columns over which tosearch as part of the synthesized expressions. Furthermore,when synthesizing column expressions, we consider onlysource columns for which we observed read access beforewrite access to the target column.
In our current implementation, we consider each cell as
a separate segment for which we gather data and synthesizesummaries, but it is also possible to consider each line asa segment if more granular documentation is desired. Forexample, users could interactively indicate what to consideras a segment, or a tool could automatically cluster lines orcells [27]–[29].
d) Presenting results: In the user interface, we present
transformations for all changed dataframes. As exempliﬁedin Fig. 1b, we translate the synthesized expressions into astructured format , separated by affected column. In addition,
we highlight parts of the transformation patterns in a tableof example data, showing but crossing out removed columns,highlighting changed columns , and a few patterns, such as
type conversions, as part of the table metadata .
B. Example selection
As discussed, data scientists often inspect the data before
and after a cell with temporary print statements, but they maymiss issues that are not visible in the ﬁrst few rows. Our insightis to group rows that are affected by the same transformationpaths, highlighting examples from each path to reveal data thatﬁt unusual processing conditions, which may be of particularinterest in understanding and debugging tasks.
Inspired by test-suite minimization techniques that select a
small number of test cases from a larger pool to trigger differ-ent execution paths [30], [31], we group rows in a dataframe bythe branching decisions taken in the data wrangling code, thusidentifying a minimum number of rows that trigger differentexecution paths. Conceptually, our strategy can be consideredas inspecting path coverage within data wrangling code whereeach row is treated as a test input to the code.
If a function is applied to every row separately, we can treat
each row as separate test input and track branching decisionswithin the supplied function. However, most transformationsin data science code are applied to entire tables or vectors atonce, through functions like replace, ﬁllna, and map, possibly
parallelized. Also many decisions in common functions likesplit and replace happen deep in libraries or native code. For
simplicity and manageable overhead, we instrument commonlibrary functions on tables and vectors, each producing a vectorof decision outcomes for every instrumented function thathas internal branching decisions. For example, for replace we
record whether the search term has been replaced and for split
we record the number of splits. Only for user-deﬁned functionssupplied to map orapply functions we collect branching
decisions with a tracer as a traditional test coverage tool does.
Using the vector that represents the branching decision of
every decision point for each row, we group all rows that
took the same path through all decisions. For our motivatingexample, we have branching decisions corresponding to the if
statement in the function supplied to map and in the calls to
replace, extract, and ﬁllna. In this case, the input data takes
three distinct paths, shown in Fig. 1b .
Presenting results: When displaying the examples for the
target data wrangling code, we show only the ﬁrst row fromeach group, while indicating how many additional rows thereare in each group, and provide a button to show more examples(Fig. 1b, ). In a tooltip, we also show the branching decisions
taken in each group. This presentation highlights the mostcommon transformations and also directs the user’s attentionto uncommon transformations, where the subtle bugs oftenoccur in data science code.
C. Implementation
We implemented our synthesis and example selection engine
as an extension to the popular JupyterLab environment, called
W
RANGLE DOC. The goal of this prototype implementation
is to demonstrate feasibility and allow experimentation with
a best-effort approach, though a fully-featured industry-readyimplementation would likely need to extend this.
Speciﬁcally, our DSL patterns are derived transformations
observed in a convenience sample of notebooks on GitHub.While all transformations can be explained with the generic“compute” pattern, we are likely missing more speciﬁc pat-terns that may be useful in some settings. For example
selection, we instrumented 24 common library functions thatwe observed in the same sample, including str .replace, pan-
das.Series.ﬁllna, and pandas.DataFrame.apply. We also in-
strumented if expression that may lead to different brancheswith the same line number. We do not claim that instrumen-tation is exhaustive in our prototype, but the implementationcan be easily extended with support for more functions.
309TABLE I: Characteristics of our subject notebooks, 25 per
dataset, reporting averages for code length, number of codeand text cells, and number of named and lambda functions.
Dataset LoC code cells text cells funct. lambda
Titanic 516 54 49 5.2 2.2
Google Play 195 42 24 1.5 5.2FIFA19 328 44 25 4.0 1.0Airbnb 267 47 33 0.9 0.4
All 327 47 33 2.9 2.2
V. E MPIRICAL STUDY OF POPULAR NOTEBOOKS
We evaluate our approach both regarding technical capabili-
ties on popular notebooks (this section) and how it helps users
debug notebooks in action (next section).
We start by exploring to what degree W RANGLE DOC can
generate documentation in data wrangling cells of a set ofnotebooks and how accurate that documentation is: How often
are the synthesized summaries correct for data wranglingcells? (RQ1)
Next, we report statistics about patterns and branching char-
acteristics observed to capture to what degree data wranglingcode performs nontrivial transformations for which documen-tation is likely useful: What are typical characteristics of
explanations for data wrangling code? (RQ2)
Finally, we measure the overhead introduced by our instru-
mentation and the computational effort required to synthesizethe documentation to capture to what degree W
RANGLE DOC
can be used in an interactive setting: How much overhead does
WRANGLE DOC introduce? (RQ3)
A. Notebook selection
To answer the research questions, we apply our approach
to a set of notebooks that were not used during our tool’sdevelopment. There are several challenges with assembling acorpus for our study: First, while millions of public notebooksare shared on sites like GitHub, they are often from classprojects, of low quality, and challenging to reproduce, e.g., dueto missing data or library dependencies [3]. In addition, manynotebooks start with an already cleaned dataset and focus moreon modeling than data cleaning and feature engineering. Hencewe decided to curate a small but diverse corpus of reproduciblepopular notebooks with signiﬁcant data wrangling code.
We decided to sample high-quality notebooks from popular
Kaggle competitions that provide tabular data in a raw format.Kaggle is the largest social platform for data science com-petitions, where users can upload datasets and correspondingchallenges and others can submit and rate solutions in theform of notebooks. Popular challenges often have thousands ofsubmitted solutions. We select multiple solutions per competi-tion, as they share the same setup. Speciﬁcally, we select fourpopular competitions: Titanic, Google Play Store, FIFA 19 andAirbnb [32]–[35]. From each competition, we choose the top25 notebooks based on “most votes” after ﬁltering non-Pythonnotebooks, task-irrelevant notebooks (not solving the modelingtask, typically tutorials for Python libraries), and notebooks wecould not reproduce due to missing or outdated dependencies.We sampled each competition with careful consideration: theTitanic dataset is usually used for educational purposes; manysolutions are written as educational notebooks for data sciencelearners. Google Play and FIFA 19 are selected based on theirpopularity, representative of trending notebooks on Kaggle.Finally, we selected Airbnb as a challenge that uses multiplelarger datasets, which provides a better approximation ofproduction setting.
Notebooks in our corpus have many characteristics that are
similar to those found in other large scale studies of publiclyavailable notebooks [3], [6]: As shown in Tab. I, they aretypically long and split into many cells, rarely abstract codeinto functions or lambda expressions, though they containmore text cells than most public notebooks.
B. Summary correctness (RQ1)
First, we analyze correctness of the synthesized summaries
for the sampled notebooks. We are not aware of an automated
procedure that could test correctness, other than how wevalidate patterns during the synthesis process in the ﬁrst place,hence we manually judge whether the summary corresponds
to the actual transformation in the cell. To gain conﬁdence inthe manual judgement, multiple authors independently analyzea subset of cells to establish inter-rater reliability.
Research design: We proceed in four steps. First, we
identiﬁed all cells from the 100 sampled notebooks that createor modify any dataframes, by monitoring state changes ofscript execution. We found 1401 such cells writing to 1998dataframes. Second, we synthesized documentation for allthe 1401 cells and prepared a user interface to show thatdocumentation, including tool tips explaining each involvedpattern. Third, to establish a reliable judgement process, wecreated a rubric on how to evaluate correctness of the syn-thesized summary (shared in supplementary material). Usingthe rubric, four authors independently judged the correctnessof 20 randomly selected dataframes and their evaluationachieved excellent inter-rater agreement (0.88 according tofree-marginal kappa [36]). Finally, having established relia-bility of judgement, another 80 randomly sampled dataframeswere judged by a single author. Analyzing 100 out of 1998dataframes gives us a margin of error of less than 10 percentat a 95 % conﬁdence level.
Results: W
RANGLE DOC created correct summaries for
92 of the 100 inspected dataframes that were created orchanged in sample code. In six out of the eight incorrectcases, W
RANGLE DOC did not create a summary even though
data was changed due to limitations of our current def-useanalysis — it can not catch modiﬁcations to a dataframethrough indirect references. The remaining two cases areoperations not currently covered by our patterns, involvingreindexing rows and performing a join operation, for which
W
RANGLE DOC synthesized a wrong pattern expression. The
results provide conﬁdence that W RANGLE DOC’s summaries
are indeed mostly correct.
310Fig. 6: Top 12 patterns observed in our dataset.
C. Wrangling characteristics (RQ2)
After establishing the accuracy of our generated summaries,
we collect statistics on the summaries and examples generated
for all 1998 modiﬁed or created dataframes in the cells of the100 sampled notebooks, demonstrating that W
RANGLE DOC
provides summaries for nontrivial transformations.
Summary characteristics: Among the 1998 dataframes
that are created or modiﬁed in cells, W RANGLE DOC does not
create documentation for 971 dataframes, almost always be-cause those variables are not used outside the cell (temporaryvariables, such as num, factor in Fig. 1a) and in a few cases be-
cause of limitations of the static analysis (see RQ1). For the re-maining 1027 created or changed dataframes, W
RANGLE DOC
creates summaries: 39% of them involve modifying columns,26% adding columns, 34% removing columns, 25% removingrows, and 14% rearranging columns or rows. None of themare summarized with a generic dataframe-level compute.
As part of these summaries, W
RANGLE DOC generated
expressions for 3754 created or modiﬁed columns. Ofthose column-level expressions, 79.5% consist of a sin-gle pattern and 19.1% are expressions with 2–3 patterns;only for 1.4% of columns we cannot provide a more de-scriptive pattern than a generic compute. The most com-
mon patterns in column-level summaries are ﬁllna, merge,
one
hot encoding, num transform, and encode (see Fig. 6
for details), which matches typical intuition about data wran-gling code: Handling missing values is a common and im-portant cleaning task, reducing variability in data by mergingor grouping data is common for feature engineering, as arevarious (often numerical or binary) encodings.
Although the studied notebooks differ considerably in size,
most of those notebooks contain a substantial amount ofdata wrangling code that uses many different patterns acrossdifferent cells. More than half of the notebooks use over 5patterns throughout their explanations.
The summaries expressed through our patterns are naturally
very concise and often not immediately obvious from thenotebook’s code. For example, when inspecting 10 cells withencode pattern, we found that they follow radically different
strategies to implement similar encodings, as exempliﬁed inFig. 7. Such level of diversity challenges simple template-based code summarization strategies, while our synthesisapproach can identify more general patterns.1 data[’Initial’].replace([’Mr’,’Mrs’,’Miss’,’Master’,’
Other’],[0,1,2,3,4],inplace=True)
2 title_mapping = {"Mr":1 ,"Miss":2 ,"Mrs":3 ,"Master" :
4,"Rare":5 }
3 dataset[’Title’] = dataset[’Title’].map(title_mapping)
4 combine["Deck"] = combine["Deck"].astype("category")
5 combine["Deck"].cat.categories = [0,1,2,3,4,5,6,7,8]6 combine["Deck"] = combine["Deck"].astype("int")
7 le = preprocessing.LabelEncoder()8 le = le.fit(df_combined[feature])9 df_train[feature] = le.transform(df_train[feature])
Fig. 7: Widely different implementations can be summarized
asencode.
Example selection: Our example selection technique is
also used in the explanations of many cells. Out of the 1027dataframes for which W
RANGLE DOC generates documenta-
tion, 54% have at least one branching decision in their com-putation, resulting in grouping rows into at least two groups.We observed a total of 1240 executions of statements thatintroduce branching decisions, of which 96.3% are commonAPI functions applied to entire columns (most commonlyﬁllna, replace, and loc), and only 3.7% are from ifexpressions
within user-deﬁned functions. This conﬁrms our observationthat, instead of writing their own functions, data scientistsheavily rely on common APIs for data wrangling code.
We also found a few cells with large numbers of branching
decisions, resulting in identifying more than 20 groups, thoughmost outliers represent long cells that contain multiple datawrangling steps (likely from copied code) that could reason-ably be split into smaller more focused steps. For a few cases,the large number results from a large number of new one-hot-encoding columns, each of which executes a different path.
D. Analysis Overhead (RQ3)
To analyze the overhead of W
RANGLE DOC, we measured
wallclock time for executing the sampled notebooks with
and without our tool and the time for static analysis andsynthesizing summaries on a Linux machine with a 4-coreIntel(R) Xeon(R) CPU (E5-2686 v4) and 16GB memory. Thestatic def-use analysis is near instantaneous, and the synthesisis also usually very fast, on average 2 seconds per targetdataframe, but the instrumentation does slow down executionby 309 percent on average (see Tab. II). In a typical usagescenario where one analyzes one cell at a time, most cellsexecute fairly quickly and the imposed overhead would addonly a few seconds, which is acceptable for interactive use.
E. Threats to validity
Regarding external validity, while we carefully selected a
sample of notebooks, readers should be careful in general-
izing the results to other notebooks, such as lower-qualitynotebooks, educational notebooks, production notebooks, ornotebooks using different libraries or datasets (e.g., images).
311TABLE II: Tool overhead: all metrics are average of all
notebooks in the dataset. Execution time refers to the timeto run the entire notebook (wallclock time) without and withour tool’s instrumentation.
Dataset Original/Instr. Exec. Time (s) Slowdown (%)
Titanic 33.1/60.5 182.7%
Google Play 25.3/59.1 233.5%
FIFA19 35.9/121.8 339.1%
Airbnb 90.3/328.3 363.5%
All 46.3/143.2 309.3%
Regarding internal validity, results must be interpreted
within the limitations of the used research design. First, despitecareful evaluation of inter-rater reliability we cannot entirelyexclude bias and subjectivity that may arise from humanjudgment in our evaluation. Second, we analyze only the cor-
rectness of transformations, but not their quality: While every
transformation can be expressed, worst case, with pattern com-
pute, the synthesized pattern may not be the most speciﬁc andinformative ones that could be expressed with our language orwith patterns not in our language. We chose to evaluate cor-rectness and completeness instead of quality because it is un-clear how to deﬁne and measure quality reliably. Third, we donot evaluate whether we miss execution paths during exampleselection due to the difﬁculty of establishing ﬁrm ground truth.
VI. C
ONTROLLED EXPERIMENT ON DEBUGGING
NOTEBOOKS
In addition to the technical evaluation in Section V, we
conducted a human-subject controlled experiment to evaluateto what degree our JupyterLab extension actually supportsdata scientists in their work, asking: How well does W
RAN -
GLE DOC improve data scientists’ efﬁciency at ﬁnding data
wrangling bugs? (RQ4)
Speciﬁcally, we asked participants to ﬁnd four bugs in
notebooks, with and without W RANGLE DOC. Our approach
is designed for program comprehension, which is the essentialactivity in bug ﬁnding. By studying the users performingthese tasks, we can understand how our approach can supportprogram comprehension both qualitatively and quantitatively.
A. Experiment design
We design our user study as a conventional within-subject
controlled experiment, in which each participant solves four
tasks, two with our tool and two without. We use a Latin-square design [37] with four groups, varying (1) which tasksare conducted using our tool and (2) task order (see Fig. 8).Compared to a between-subject design, our design gives usmore statistical power for the same number of participants, isless inﬂuenced by individual differences between participants,and controls learning effects.
Tasks: We prepare two notebooks with two data wran-
gling bugs each (shared in our supplementary material). Bothnotebooks are modiﬁed versions of notebooks analyzed inSec. V, and all bugs are based on real bugs we found. Tasks
Fig. 8: Latin square design varying treatment and task order.
1 and 2 in the ﬁrst notebook correspond to incorrect datamodelling (missing cases) and API misuse; Tasks 3 and 4 inthe second notebook correspond to a typo in a regular expres-sion and API misuse. Tasks 1 and 3 have similar symptoms(incorrect transformations for a subset of the data) and roughlyhave the same level of difﬁculty. Similarly, Tasks 2 and 4have similar symptoms (code does not have a persistent effect)and roughly have the same level of difﬁculty. Participants areinstructed to look for a bug in a speciﬁc subset of cells (distinctfor each task) and report the location of the bug or showan example of incorrect data wrangling results. Participantscan modify and run notebooks code to obtain any dynamicinformation and are free to search for information online.
We tested the difﬁculty of the tasks and the clarity of
instructions in a pilot study with ﬁve participants, replacingone bug where participants questioned intended behavior andreﬁning the wording of our instructions.
Participants: For our study, we recruited 20 participants.
We estimated the number of needed participants conservativelybased on the expected large effect size from pilot experiments.
We recruited students with data-science experience, who are
common target users for notebook environments and likelyshare traits with early-career data scientists. We recruitedthrough university-wide social media by advertising for a one-hour study for students with data-science experience at PekingUniversity, compensating participants for their time.
The 20 participants (15 male, 5female; 18-24 years old)
all have Python programming experience (from half a year toseven years; 18 with more than 2 years of experience). Overhalf of participants self-rated their familiarity with Jupyternotebook as ‘familiar’ or ‘very familiar’. Most participantshave a computer science background or study data science,while four learn or use data science for their domains (infor-mation systems, economics, geography, and physics). We ran-domly assigned participants to the four experimental groups.The four groups are comparable in terms of Python experienceand Jupyter familiarity.
Procedure: After participants ﬁlled out a background sur-
vey to gather demographic information, we invited participantsfor a one-hour experiment session in the lab, one participantat a time. We provided a laptop with the notebooks preparedin a Chrome web browser.
We started the session by introducing the W
RANGLE DOC
extension with a tutorial. Through a dedicated notebook, wedescribed the tool’s features in text cells between code cellson which they could directly try it for about 10 minutes. After
312(a) Number of bugs found and not found.
(b) Average completion time and standard variance per task.
Fig. 9: User study results.
the tutorial, we gave them a warm-up bug hunting task, to
make sure they understand how to use W RANGLE DOC.
We then asked participants to work on the four tasks.
Depending on their assigned experimental group, participantsstarted either with tasks 1 and 2 or tasks 3 and 4 and eitherused W
RANGLE DOC for the ﬁrst two or the last two tasks. We
enforced a time limit of 12 minutes per bug. When participantssuggested an incorrect solution, we rejected it and asked themto continue searching until they either identiﬁed the bug orexhausted the time limit. Participants only continued to thenext task once they ﬁnished or timed out on the previous one.
After completing all four tasks, we conducted an optional
interview, where we asked participants about their experiencewith understanding data wrangling code and using our tool.
Analysis: We analyze both whether participants ﬁnd the
bugs within the time limit and how long it takes them if theydo. We analyze completion times with an ANOV A analysis,testing to what degree our tool, the tasks, the order (tool ﬁrst ortool last), and the participants’ self-reported experience explainvariance in completion times. We use the McFadden’s pseudo-R
2measure to evaluate the goodness-of-ﬁt of our model.
In Tab. 9, we report the exponentiated coefﬁcient, standarderror, p-value, and effect size (see column “LR Chisq” for the
absolute amounts of deviance explained).
For tasks where participants did not ﬁnd the bug within the
time limit, we report the maximum time of 12 minutes, but ourresults are also robust to analyzing competition time varianceonly among tasks that were ﬁnished within the time limit.
B. Results
Overall, our experiment demonstrates that W
RANGLE DOC
could signiﬁcantly improve participants’ performance, as
it helps the understanding process in general as well asexpose unusual cases in a clear form. First, we ﬁnd thatparticipants are much more likely to ﬁnd the bugs if they use
our tool (37 out of 40) than if not (24 out of 40), as shownin Fig. 9b. Second, we ﬁnd that participants complete theTABLE III: User study result model (R
2= 33%).
Coeffs (Errors) LR Chisq
(Intercept) 37013.17 (1.55)∗∗∗
Interv.: Used W RANGLE DOC? 0.02 (0.68)∗∗∗34.53∗∗∗
Order: Tool in ﬁrst notebook? 1.47 (0.68) 0.32
Y ears of Python experience 1.07 (0.25) 0.07
Jupyter experience (1–5) 0.83 (0.34) 0.32
Task number 0.69 (0.30) 1.48
∗∗∗p<0.001 N=8 0
tasks on average 44% faster when using our tool (stat. sign.,p<0. 001), as shown in Fig. 9b and Tab. III. Lastly, we ﬁnd
that all other factors (task order, Python experience, notebookfamiliarity) have little effect on how fast participants completethe tasks (see Tab. III).
Beyond the quantitative results, we observed that partici-
pants use similar strategies to understand notebook when theydo the tasks without W
RANGLE DOC as observed in Sec. II.
We found that participants learned W RANGLE DOC quickly.
In our post-experiment interviews, participants universallyexpressed that the tool was helpful, but differed in how
W
RANGLE DOC helped them: Some commented that synthe-
sized summary helps their overall code understanding, e.g.“Summary is very straightforward. Before I read the code, Ijust looked at the summary to get a rough idea of what the codedoes, ” but others emphasized the usefulness of table visualiza-
tion and examples, e.g., “Example rows are the most helpful
part. It saves me lots of time of manually checking all possiblecases for the data, ” and “Visualization helps a lot. Everything
I need to know (for a given cell) is placed on the left and inbold, and data changes are shown directly in one table. [...]When I was doing tasks without the tool, I have to scroll andcompare data before and after , and this is really annoying. ”
C. Threats to validity
Readers should be careful when generalizing our results
beyond the studied participant population and tasks. The par-
ticipants in our user study are not professional data scientists,but their background is comparable with early-carrier datascientists. While the tasks mirror many practical debuggingand reuse settings that require understanding existing code, thebug ﬁnding experience for the participants in our study mightbe different from practical settings where data scientists areworking with their own code, with different data, and withoutthe artiﬁcial, time constrained setting of an experiment.
VII. R
ELA TED WORK
Studies on computational notebooks: Researchers have
conducted empirical studies to understand the practices andchallenges of using computational notebook [3], [5]–[7], [11],[15], [16], [38]. For example, Pimentel et al. [3] studied thereproducibility issues of over one million notebooks and found
that only 24% of the notebooks could be executed withoutexceptions, and only 4% produced the same results. Koenzenet al. [14] found that code duplication is common in notebooks
and presented the needs for supporting code reuse in notebook
313interfaces. Chattopadhyay et al. [5] studied the pain points
for data scientists when working with notebooks, includinglarge challenges due to poor tool support and scalabilitychallenges. Wang et al. [17], [18] studied the challenges forcollaborative editing using computational notebooks.
While some praise notebooks as a literate programming
environment that provides opportunities for integrating doc-umentation with code to publish code with a narrative andrationale [10], [39]–[42], large-scale empirical studies ofnotebooks have found that most public notebooks containrather sparse documentation [3], [6], [7]. Such studies alsohighlight how most notebooks use only a small set of commonlibraries [3], [6], which makes analysis strategies like oursfeasible that require static or dynamic analyses for commonlyused functions.
Tooling support for computational notebooks: A large
number of tools has been proposed and developed to helpdata scientists on various tasks, including ﬁne-grained ver-sion control [11], [43], code cleaning through slicing [23],tracking provenance information [44]–[46], synthesizing datawrangling code from examples [47], facilitating collaborativeexploratory programming using notebooks [18], and assuringreproducibility [48], among many others. Lau et al. [20]summarized the design space of notebooks covering differentstages of a computational workﬂow and provided an excellentoverview of tools for notebooks in academia and practice.
With regard to ﬁnding bugs, researchers have proposed the
use of static analysis tools on data science code [22], [49]to look for certain patterns of mistakes. For several kinds ofbugs we found, e.g., API misuse and not persisted changes,specialized analyzers could likely be developed.
Regarding documentation, JupyterLab can show API docu-
mentation in a tooltip on demand. Closest to our work, Wanget al. [50] developed three strategies for documentation gen-eration in notebooks: neural-network-based automated codesummarization, linking to external API documentation basedon keywords in a cell, and prompting users to manually writedocumentation. Their work is intended to support users inwriting markdown cells when sharing the notebook, rather thanproviding on-demand summaries of data transformations forprogram comprehension as in our approach; they do not relyon runtime information and do not consider the data processed.
Other related ﬁelds: Gulzar et al. [51]–[53] explored
strategies to test and debug big data applications in Apache
Sparks, which often have a similar ﬂavor to the data wranglingcode in notebooks. They analyze the executions of the runninganalysis, using row-based provenance tracking and symbolicexecution to identify and generate inputs that crash the datawrangling code or violate user-provided test cases. In contrast,our summaries help surface wrong behavior with existing datathat did not result in crashes or fail tests.
Outside of notebooks, various code summarization
techniques have been developed to provide short summariesof code fragments automatically [9], for example forgenerating descriptive summaries for methods and classes,commit messages, or even “extreme summarization” asmethod names; they use various forms of informationretrieval [54]–[56], code structure analysis [57], machinelearning [50], [58]–[61], and natural language processing [62].They usually either consider the code as individual tokens,a sequence of tokens, or use static analysis of the code toprovide more effective inference of code properties. Since theydo not analyze concrete executions dynamically, none of thesemethods can summarize the code’s impact on speciﬁc data.We, on the other hand, speciﬁcally use code synthesis and testsuite minimization techniques based on runtime information.Our tool can generate abstract patterns and examples derivedfrom observing executions which is more accurate and relevantto the data wrangling task, but speciﬁc to a given dataset.
Program synthesis is a well-established ﬁeld to generate
programs from input-output samples [63], which has alsobeen suggested to synthesize data science code from providedbefore-and-after samples of dataframes [47]. In our work,we use synthesis in an unusual way: We execute code todynamically collect before and after values and then applysynthesis to those values but synthesize only general patternsfor documentation rather than concrete code. Similarly, weapply test suite minimization , which has been broadly studied
for removing redundancy from large test suites [30], [31], inan unusual way to select input rows for data science code.In both cases, we build on existing techniques but use themin novel contexts. We are not aware of other work that usessynthesis or test suite minimization in a similar context.
VIII. C
ONCLUSION
We propose to adapt program synthesis to generate sum-
maries for data transformations performed in notebooks anduse test suite minimization techniques to group rows byshared execution paths through data wrangling code. Bothforms of documentation are based on observing a notebook’sexecution at runtime with concrete data, implemented in aJupyterLab extension W
RANGLE DOC. Our evaluation found
that our approach can generate correct documentation withacceptable runtime overhead and that the documentation helpsdata scientists ﬁnd subtle bugs in data wrangling code.
A
CKNOWLEDGMENT
We thank Anjiang Wei and Yiqian Wu for their feedback on
this work and Jingjing Liang for her help with the user study.We acknowledge the support of the Natural Sciences andEngineering Research Council of Canada (NSERC), RGPIN-2021-03538, the NSF award 1813598, and Fonds de recherchedu Qu ´ebec (FRQNT) 2021-NC-284820.
R
EFERENCES
[1] G. Press, “Cleaning big data: Most time-consuming, least
enjoyable data science task, survey says,” F orbes, Mar 2016,
https://www.forbes.com/sites/gilpress/2016/03/23/data-preparation-
most-time-consuming-least-enjoyable-data-science-task-survey-says/.
[2] P . J. Guo, S. Kandel, J. M. Hellerstein, and J. Heer, “Proactive wrangling:
Mixed-initiative end-user programming of data transformation scripts,”inProceedings of the 24th annual ACM symposium on User interface
software and technology, 2011, pp. 65–74.
314[3] J. F. Pimentel, L. Murta, V . Braganholo, and J. Freire, “A large-scale
study about quality and reproducibility of Jupyter notebooks,” in Proc.
Conf. Mining Software Repositories (MSR) . IEEE Computer Society,
2019, pp. 507–517.
[4] A. Koenzen, N. A. Ernst, and M. Storey, “Code duplication and reuse
in Jupyter notebooks,” Proc. Int’l Symp. Visual Languages and Human-
Centric Computing (VLHCC), pp. 1–9, 2020.
[5] S. Chattopadhyay, I. Prasad, A. Z. Henley, A. Sarma, and T. Barik,
“What’s wrong with computational notebooks? pain points, needs, and
design opportunities,” in Proc. Conf. Human Factors in Computing
Systems (CHI). ACM Press, 2020, p. 1–12.
[6] F. Psallidas, Y . Zhu, B. Karlas, M. Interlandi, A. Floratou, K. Karanasos,
W. Wu, C. Zhang, S. Krishnan, C. Curino et al., “Data science
through the looking glass and what we found there,” arXiv preprint
arXiv:1912.09536, 2019.
[7] A. Rule, A. Tabard, and J. D. Hollan, “Exploration and explanation in
computational notebooks,” in Proc. Conf. Human Factors in Computing
Systems (CHI), 2018, pp. 1–12.
[8] S. Amershi, A. Begel, C. Bird, R. DeLine, H. Gall, E. Kamar,
N. Nagappan, B. Nushi, and T. Zimmermann, “Software engineeringfor machine learning: A case study,” in Proc. Int’l Conf. Software
Engineering: Software Engineering in Practice (ICSE-SEIP). IEEEComputer Society, 2019, pp. 291–300.
[9] Y . Zhu and M. Pan, “Automatic code summarization: A systematic
literature review,” arXiv preprint arXiv:1909.04352, 2019.
[10] J. M. Perkel, “Why Jupyter is data scientists’ computational notebook
of choice,” Nature, vol. 563, no. 7732, pp. 145–147, 2018.
[11] M. B. Kery, M. Radensky, M. Arya, B. E. John, and B. A. Myers,
“The story in the notebook: Exploratory data science using a literateprogramming tool,” in Proc. Conf. Human Factors in Computing Systems
(CHI), 2018, pp. 1–11.
[12] S. Kandel, A. Paepcke, J. M. Hellerstein, and J. Heer, “Enterprise data
analysis and visualization: An interview study,” IEEE Transactions on
Visualization and Computer Graphics, vol. 18, no. 12, pp. 2917–2926,2012.
[13] K. Patel, J. Fogarty, J. A. Landay, and B. Harrison, “Investigating
statistical machine learning as a tool for software development,” in Proc.
Conf. Human Factors in Computing Systems (CHI), 2008, pp. 667–676.
[14] A. P . Koenzen, N. A. Ernst, and M.-A. D. Storey, “Code duplication and
reuse in Jupyter notebooks,” in Proc. Int’l Symp. Visual Languages and
Human-Centric Computing (VLHCC). IEEE Computer Society, 2020,pp. 1–9.
[15] M. Bussonnier, J. Forde, J. Freeman, B. Granger, T. Head, C. Holdgraf,
K. Kelley, G. Nalvarte, A. Osheroff et al., “Binder 2.0-reproducible,
interactive, sharable environments for science at scale,” in Proceedings
of the 17th Python in Science Conference, vol. 113, 2018, p. 120.
[16] J. Wang, K. Tzu-Y ang, L. Li, and A. Zeller, “Assessing and restoring
reproducibility of Jupyter notebooks,” in Proc. Int’l Conf. Automated
Software Engineering (ASE). IEEE Computer Society, 2020, pp. 138–149.
[17] A. Y . Wang, A. Mittal, C. Brooks, and S. Oney, “How data scientists
use computational notebooks for real-time collaboration,” Proceedings
of the ACM on Human-Computer Interaction, vol. 3, no. CSCW, pp.1–30, 2019.
[18] A. Y . Wang, Z. Wu, C. Brooks, and S. Oney, “Callisto: Capturing the”
why” by connecting conversations with computational narratives,” inProc. Conf. Human Factors in Computing Systems (CHI), 2020, pp.1–13.
[19] K. Holtzblatt and H. Beyer, Contextual design: deﬁning customer-
centered systems. Morgan Kaufmann, 1997.
[20] S. Lau, I. Drosos, J. M. Markel, and P . J. Guo, “The design space of
computational notebooks: An analysis of 60 systems in academia andindustry,” in Proc. Int’l Symp. Visual Languages and Human-Centric
Computing (VLHCC). IEEE Computer Society, 2020, pp. 1–11.
[21] “pandas-proﬁling,” 2021. [Online]. Available: https://github.com/
pandas- proﬁling/pandas-proﬁling
[22] J. Wang, L. Li, and A. Zeller, “Better code, better sharing: On the need of
analyzing Jupyter notebooks,” in Proc. Int’l Conf. Software Engineering:
New Ideas and Emerging Results (ICSE-NIER). ACM Press, 2020, p.53–56.
[23] A. Head, F. Hohman, T. Barik, S. M. Drucker, and R. DeLine, “Manag-
ing messes in computational notebooks,” in Proc. Conf. Human Factors
in Computing Systems (CHI), 2019, pp. 1–12.[24] M. J. Islam, H. A. Nguyen, R. Pan, and H. Rajan, “What do developers
ask about ml libraries? a large-scale study using stack overﬂow,” arXiv
preprint arXiv:1906.11940, 2019.
[25] E. Torlak and R. Bodik, “A lightweight symbolic virtual machine for
solver-aided host languages,” in Proc. Conf. Programming Language
Design and Implementation (PLDI). ACM Press, 2014, p. 530–541.
[26] F. Nielson, H. R. Nielson, and C. Hankin, Principles of program
analysis. Springer Science & Business Media, 2004.
[27] M. Barnett, C. Bird, J. Brunet, and S. K. Lahiri, “Helping developers
help themselves: Automatic decomposition of code review changesets,”inProc. Int’l Conf. Software Engineering (ICSE), vol. 1. IEEE
Computer Society, 2015, pp. 134–144.
[28] S. Zhou, S. Stanciulescu, O. Leßenich, Y . Xiong, A. Wasowski, and
C. K ¨astner, “Identifying features in forks,” in Proc. Int’l Conf. Software
Engineering (ICSE). IEEE Computer Society, 2018, pp. 105–116.
[29] G. Zhang, M. A. Merrill, Y . Liu, J. Heer, and T. Althoff, “Coral:
Code representation learning with weakly-supervised transformers foranalyzing data analysis,” 2020.
[30] H.-Y . Hsu and A. Orso, “Mints: A general framework and tool for
supporting test-suite minimization,” in Proc. Int’l Conf. Software En-
gineering (ICSE). IEEE Computer Society, 2009, pp. 419–429.
[31] S. Y oo and M. Harman, “Regression testing minimization, selection and
prioritization: a survey,” Software testing, veriﬁcation and reliability,
vol. 22, no. 2, pp. 67–120, 2012.
[32] “Titanic - Machine Learning from Disasters.” [Online]. Available:
https://www.kaggle.com/c/titanic/code?competitionId=3136
[33] L. Gupta, “Google Play Store Apps,” Feb 2019. [Online]. Available:
https://www.kaggle.com/lava18/google-play-store-apps
[34] K. Gadiya, “FIFA 19 complete player dataset,” Dec 2018. [Online].
Available: https://www.kaggle.com/karangadiya/ﬁfa19/
[35] Dgomonov, “New Y ork City Airbnb Open Data,” Aug 2019.
[Online]. Available: https://www.kaggle.com/dgomonov/new-york-city-airbnb-open-data
[36] J. J. Randolph, “Free-marginal multirater kappa (multirater k [free]): An
alternative to ﬂeiss’ ﬁxed-marginal multirater kappa.” Online submission,
2005.
[37] G. E. P . Box, Statistics for experimenters: design, innovation, and
discovery. Wiley-Blackwell, 2009.
[38] K. Subramanian, I. Zubarev, S. V ¨olker, and J. Borchers, “Supporting data
workers to perform exploratory programming,” in Extended Abstracts
of the 2019 CHI Conference on Human Factors in Computing Systems,2019, pp. 1–6.
[39] J. Somers, “The scientiﬁc paper is obsolete,” The Atlantic, vol. 4, 2018.
[40] A. Rule, A. Birmingham, C. Zuniga, I. Altintas, S.-C. Huang, R. Knight,
N. Moshiri, M. H. Nguyen, S. B. Rosenthal, F. P ´erez et al.,“ T e n
simple rules for writing and sharing computational analyses in Jupyternotebooks,” 2019.
[41] J. Singer, “Notes on notebooks: Is Jupyter the bringer of jollity?”
inProceedings of the 2020 ACM SIGPLAN International Symposium
on New Ideas, New Paradigms, and Reﬂections on Programming andSoftware, 2020, pp. 180–186.
[42] T. Kluyver, B. Ragan-Kelley, F. P ´erez, B. E. Granger, M. Bussonnier,
J. Frederic, K. Kelley, J. B. Hamrick, J. Grout, S. Corlay et al.,
“Jupyter notebooks – a publishing format for reproducible computationalworkﬂows,” in Proceedings of the 20th International Conference on
Electr onic Publishing, vol. 2016, 2016.
[43] M. B. Kery and B. A. Myers, “Interactions for untangling messy history
in a computational notebook,” in Proc. Int’l Symp. Visual Languages and
Human-Centric Computing (VLHCC). IEEE Computer Society, 2018,pp. 147–155.
[44] J. Hu, J. Joung, M. Jacobs, K. Z. Gajos, and M. I. Seltzer, “Improving
data scientist efﬁciency with provenance,” in Proc. Int’l Conf. Software
Engineering (ICSE). IEEE Computer Society, 2020, pp. 1086–1097.
[45] J. F. Pimentel, L. Murta, V . Braganholo, and J. Freire, “noworkﬂow: A
tool for collecting, analyzing, and managing provenance from pythonscripts,” Proceedings of the VLDB Endowment, vol. 10, no. 12, 2017.
[46] M. H. Namaki, A. Floratou, F. Psallidas, S. Krishnan, A. Agrawal, and
Y . Wu, “V amsa: Tracking provenance in data science scripts,” CoRR,
vol. abs/2001.01861, 2020.
[47] I. Drosos, T. Barik, P . J. Guo, R. DeLine, and S. Gulwani, “Wrex: A
uniﬁed programming-by-example interaction for synthesizing readablecode for data scientists,” in Proc. Conf. Human Factors in Computing
Systems (CHI). ACM Press, 2020, p. 1–12.
315[48] J. Wang, L. Li, and A. Zeller, “Restoring execution environments of
Jupyter notebooks,” 2021.
[49] N. Hynes, D. Sculley, and M. Terry, “The data linter: Lightweight,
automated sanity checking for ml data sets,” in NIPS MLSys Workshop,
2017.
[50] A. Y . Wang, D. Wang, J. Drozdal, M. Muller, S. Park, J. D. Weisz,
X. Liu, L. Wu, and C. Dugan, “Themisto: Towards automated doc-
umentation generation in computational notebooks,” arXiv preprint
arXiv:2102.12592, 2021.
[51] M. A. Gulzar, M. Interlandi, S. Y oo, S. D. Tetali, T. Condie, T. Millstein,
and M. Kim, “Bigdebug: Debugging primitives for interactive big dataprocessing in spark,” in Proc. Int’l Conf. Software Engineering (ICSE).
IEEE Computer Society, 2016, pp. 784–795.
[52] M. A. Gulzar, S. Mardani, M. Musuvathi, and M. Kim, “White-box
testing of big data analytics with complex user-deﬁned functions,”inProc. Europ. Software Engineering Conf./F oundations of Software
Engineering (ESEC/FSE), 2019, pp. 290–301.
[53] Q. Zhang, J. Wang, M. A. Gulzar, R. Padhye, and M. Kim, “Bigfuzz:
Efﬁcient fuzz testing for data analytics using framework abstraction,”inProc. Int’l Conf. Automated Software Engineering (ASE). IEEE
Computer Society, 2020, pp. 722–733.
[54] S. Haiduc, J. Aponte, L. Moreno, and A. Marcus, “On the use of
automated text summarization techniques for summarizing source code,”in2010 17th Working Conference on Reverse Engineering. IEEE
Computer Society, 2010, pp. 35–44.
[55] J. Fowkes, P . Chanthirasegaran, R. Ranca, M. Allamanis, M. Lapata, and
C. Sutton, “Autofolding for source code summarization,” IEEE Trans.
Softw. Eng. (TSE), vol. 43, no. 12, pp. 1095–1109, 2017.
[56] M. Liu, X. Peng, A. Marcus, Z. Xing, W. Xie, S. Xing, and Y . Liu, “Gen-
erating query-speciﬁc class api summaries,” in Proc. Europ. Software
Engineering Conf./F oundations of Software Engineering (ESEC/FSE),2019, pp. 120–130.
[57] L. Moreno, J. Aponte, G. Sridhara, A. Marcus, L. Pollock, and K. Vijay-
Shanker, “Automatic generation of natural language summaries for javaclasses,” in Proc. Int’l Conf. Program Comprehension (ICPC). IEEE
Computer Society, 2013, pp. 23–32.
[58] N. Nazar, H. Jiang, G. Gao, T. Zhang, X. Li, and Z. Ren, “Source code
fragment summarization with small-scale crowdsourcing based features,”Frontiers of Computer Science, vol. 10, no. 3, pp. 504–517, 2016.
[59] X. Hu, G. Li, X. Xia, D. Lo, and Z. Jin, “Deep code comment
generation,” in Proc. Int’l Conf. Program Comprehension (ICPC). IEEE
Computer Society, 2018, pp. 200–20 010.
[60] M. Allamanis, H. Peng, and C. Sutton, “A convolutional attention
network for extreme summarization of source code,” in International
conference on machine learning. PMLR, 2016, pp. 2091–2100.
[61] A. LeClair, S. Haque, L. Wu, and C. McMillan, “Improved code
summarization via a graph neural network,” in Proc. Int’l Conf. Program
Comprehension (ICPC), 2020, pp. 184–195.
[62] X. Wang, L. Pollock, and K. Vijay-Shanker, “Automatically generating
natural language descriptions for object-related statement sequences,”inProc. Int’l Conf. Software Analysis, Evolution, and Reengineering
(SANER). IEEE Computer Society, 2017, pp. 205–216.
[63] S. Gulwani, O. Polozov, R. Singh et al., “Program synthesis,” F oun-
dations and Trends® in Programming Languages , vol. 4, no. 1-2, pp.
1–119, 2017.
316