DeepState: Selecting Test Suites to Enhance the Robustness of
Recurrent Neural Networks
Zixi Liu
zxliu@smail.nju.edu.cn
State Key Laboratory for Novel Software Technology
Nanjing University
Nanjing 210023, ChinaYang Fengâˆ—
fengyang@nju.edu.cn
State Key Laboratory for Novel Software Technology
Nanjing University
Nanjing 210023, China
Yining Yin
ynyin@smail.nju.edu.cn
State Key Laboratory for Novel Software Technology
Nanjing University
Nanjing 210023, ChinaZhenyu Chen
zychen@nju.edu.cn
State Key Laboratory for Novel Software Technology
Nanjing University
Nanjing 210023, China
ABSTRACT
DeepNeuralNetworks(DNN)haveachievedtremendoussuccess
in various software applications. However, accompanied by out-
standing effectiveness, DNN-driven software systems could alsoexhibit incorrect behaviors and result in some critical accidents
andlosses.ThetestingandoptimizationofDNN-drivensoftware
systemsrelyonalargenumberoflabeleddatathatoftenrequire
many human efforts, resulting in high test costs and low efficiency.
Although plenty of coverage-based criteria have been proposed to
assistinthedataselectionofconvolutionalneuralnetworks,itis
difficulttoapplythemonRecurrentNeuralNetwork(RNN)models
due to the difference between the working nature.
Inthispaper,weproposeatestsuiteselectiontool DeepState
towardstheparticularneuralnetworkstructuresofRNNmodels
for reducing the data labeling and computation cost. DeepState
selectsdatabasedonastatefulperspectiveofRNN,whichidentifies
the possibly misclassified test by capturing the state changes of
neurons in RNN models. We further design a test selection method
toenabletesterstoobtainatestsuitewithstrongfaultdetection
andmodelimprovementcapabilityfromalargedataset.Toevalu-
ateDeepState , we conduct an extensive empirical study on pop-
ular datasets and prevalent RNN models containing image and
textprocessingtasks.Theexperimentalresultsdemonstratethat
DeepState outperforms existingcoverage-based techniques inse-
lectingtestsregardingeffectivenessandtheinclusivenessofbug
cases.Meanwhile,weobservethattheselecteddatacanimprove
the robustness of RNN models effectively.
âˆ—Yang Feng is the corresponding author.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation
onthe firstpage.Copyrights forcomponentsof thisworkowned byothersthan the
author(s)mustbehonored.Abstractingwithcreditispermitted.Tocopyotherwise,or
republish,topostonserversortoredistributetolists,requirespriorspecificpermission
and/or a fee. Request permissions from permissions@acm.org.
ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
Â© 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 978-1-4503-9221-1/22/05...$15.00
https://doi.org/10.1145/3510003.3510231CCS CONCEPTS
â€¢Software and its engineering â†’Software testing and debug-
ging.
KEYWORDS
deep learning testing, deep neural networks, recurrent neural net-
works, test selection
ACM Reference Format:
Zixi Liu, Yang Feng, Yining Yin, and Zhenyu Chen. 2022. DeepState: Se-
lecting Test Suites to Enhance the Robustness of Recurrent Neural Net-
works.In 44thInternationalConferenceonSoftwareEngineering(ICSEâ€™22),
May 21â€“29, 2022, Pittsburgh, PA, USA. ACM, New York, NY, USA, 12 pages.
https://doi.org/10.1145/3510003.3510231
1 INTRODUCTION
Deep Neural Networks (DNN) has been widely adopted in many
fields to assist in solving various tasks, such as image classifica-tion [
32], speech r ecognition [ 59], and natural language process-
ing [7,8,13], etc. Although the DNN-driven systems have made
remarkable progress in many aspects, they suffer from quality and
reliabilityissues.TheerroneousbehaviorsproducedbyDNN-drivensystemscouldcausesignificantlosses.Forexample,theRNN-drivendialoguesystem,Amazonâ€™ssmartspeakerAlexacouldbefooledby
somecornerinputcasesastomakescreepylaughs,whichscares
theelderlyandchildrenandaffectstheirlives[ 2].Theerroneous
behaviorsproducedbyDNN-drivensystemscouldleadtomisun-
derstanding, threats to personal safety [ 3], or even political con-
flicts[12].Therefore,thetestingandoptimizationofDNN-driven
systems have become an urgent yet challenging task.
The DNN-driven systems are constructed upon the data-driven
programming paradigm [ 29], which requires plenty of data with
ground truth(i.e., labeled data)for modeltraining and evaluation.
Unfortunately, collecting a high-quality data set for building DNN-
drivensystemsrequiresplentyofhumaneffortstolabelthedata,
whichmakestheprocessexpensiveandtime-consuming.Itisin-
efficientandimpracticaltoemploythedatacollectedfromusage
scenariosforthemodelâ€™stestingandoptimizationdirectly.Becauseinamassivedataset,onlyasmallamountofcases[
34,43,56]which
cantriggerthesystemâ€™spotentialerrorsareespeciallycrucialfor
testing the DNN-driven systems. Furthermore, due to the nature of
5982022 IEEE/ACM 44th International Conference on Software Engineering (ICSE)
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 07:56:15 UTC from IEEE Xplore.  Restrictions apply. ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA Zixi Liu, Yang Feng, Yining Yin, and Zhenyu Chen
DNNmodels,optimizingthemvia retrainingwithenormousdata
often costs plenty of time and computational resources.
Inspiredbytheeffectivenessofcodecoverageinconventional
software testing, researchers have proposed several neuron cover-
age (NC) criteria [ 23,34,35,43] to evaluate the testing adequacy
andguidetestselection,therebyreducingthelabelingcostandsav-
ing computational resources. Recently, the confidence-based Deep-
Gini [16] and the feature-based PRIMA [ 57] have been proposed
toprioritize tests.However, mostofthe existingneuroncoverage
criteriaaremainlydesignedforthefeedforwardneuralnetworks
(FNN) [55], such as convolution neural networks (CNN) [ 6] and
fullyconnectedneuralnetworks(FC)[ 44].Thegeneralrationaleof
thesecriteriaistocalculatethecoverageratebyseparatelycalculat-
ing the activation of each layer and neuron. Howeve r, it is difficult
totransferthemtoRNNwhichisatypicalfeedbackneuralnetwork,
due to the differences in working nature and network structures
between RNN and FNN. Only a few test criteria [ 15,24,25] have
been proposed for RNN models, which are calculated by analyzing
the neuron states when the model is processing different test cases.
Theexistingresearchoftenregardsthehiddenstateateachitera-
tion as the neuron layer in the FNN. Yet, such criteria are specially
designed for detecting adversarial examples [ 63] that are manually
generated with unrealistic transformations to attack the trained
models. Moreover, the current research on neuron coverage is still
in the controversial stage, and there are plenty of discussions on
their effectiveness and usage scenarios [16, 20, 31, 60].
Different from these criteria, in this paper, we propose a test se-
lectiontool,namely DeepState ,especiallyforidentifyingtestcases
thatmaytriggererrorsintheRNN-drivensystems. DeepState is
designed upon a stateful perspective rather than the neuron cover-
age of RNN. We measure RNNâ€™s uncertainty for a given test and
analyzetheoutputbehaviorsbyexpandingthehiddeninternalstate
of the RNN according to time steps. Specifically, we first capture
thehiddenstatechangesofRNNneuronsovertimesteps.Then,we
design two metrics, i.e., the changing rate and changing trend of
the hidden states, to analyze the output behaviors of RNN models.
To select test suites with a high bug detection rate from massive
data, we design a test selection method based on the changing rate
and changing trend of hidden states.
We implement DeepState and evaluate it on four RNN-based
systemscoveringimageclassificationandtextclassificationtasks.
Besides,wevalidateitseffectivenessonavarietyofRNNmodels,
includingthemostcommonlyusedLSTM,BiLSTM,andGRU.To
evaluate the effectiveness of DeepState , we compared with ran-
dom and existing neuron coverage-based selection methodologies.
The experiment results show that DeepState can effectively se-
lect test cases with a high bug detection rate, which can help totestRNNmodelsandfindpotentialdefects.Besides,weevaluate
DeepState â€™seffectivenessbycalculatingtheinclusivenessofthese-
lected testsuites. The resultsunder multiple selectionratios show
that DeepState can filter out the tests that can reveal potential
flaws in RNN models. Further, we evaluate its capability of improv-
ing the quality of RNN. We employ the selected data to retrain the
originalRNNmodelandrecordtheaccuracyimprovements.The
experiment resultsindicate that DeepState can improvethe RNN
models by retraining the model with the selected data.
In summary, the main contributions of this paper are as follows.â€¢Approach. We present an approach to model the internal
states of RNN into a stateful perspective that can assist usin analyzing the behaviors of RNN models. Based on the
statefulperspective,wefurtherproposethemetricsofthe
changing rate and changing trend to reflect the behaviors of
RNNâ€™s hidden states.
â€¢Tool.Basedonthestatefulperspective,wedesignandim-
plementatestselectiontool,namely DeepState ,toassistin
identifying a smallportion of tests withhigh bug detection
rate from a massive dataset efficiently. The source code of
DeepState is publicly available1.
â€¢Study.Weevaluate DeepState onthetestselectionofimage
andtextclassification.The experimentresultsdemonstrate
that the test suite selected by DeepState has a high bug
detectionrateandcanbeappliedtoretrainRNNstoimprove
the robustness of the models.
2 BACKGROUND
Inthissection,weintroducethepreliminaryknowledgeofRNN,ex-istingneuralnetworktestcriteria,especiallythoseforRNNmodels,
and conventional test selection techniques.
2.1 Recurrent Neural Networks
Theneuralnetworkscanbedividedintofeedforwardneuralnet-
works and feedback neural networks based on their information
propagation methods [ 49]. The feedforward neural networks [ 55],
suchasConvolutionalNeuralNetwork(CNN)[ 6]andFullyCon-
nected neural networks (FC) [ 50], are composed of multiple layers
of neurons. The neurons in each layer receive the output of the
previouslayerandoutputtothenext.Onthecontrary,theneurons
inthefeedbackneuralnetworkcannotonlyreceivesignalsfrom
other neurons but also receive their own feedback signals.
Therecurrentneuralnetwork(RNN)isatypicalkindoffeedback
neuralnetworks[55]. Fig.1illustratesa generalstructureofRNN.
WiththeRNNunfolded,eachneuroninRNNâ€™shiddenlayerupdates
its state and weight iteratively over time steps [ 53]. The hidden
states output â„ğ‘¡at time step ğ‘¡is calculated upon current input
ğ‘¥ğ‘¡as well as â„ğ‘¡âˆ’1from the previous time step, and then passed
tothesoftmaxlayertooutputtheprediction ğ‘¦ğ‘¡[64].WhileRNN
can output the prediction result at each time step, it can regard the
outputatthelasttimestepasthefinalpredictionofRNN.TheLSTM
(Long Short-Term Memory) model [ 21] is the most widely-used
optimized RNN, which is primarily designed to solve the problem
of gradient disappearance and gradient explosion in the training
processoflongsequences.TheLSTMmodeladdsaforgetgateto
thehiddenlayertodiscardunimportantinformation,soitcanbetterexpressthelong-termandshort-termdependencelocally[
36].RNN
is particularly effective in processing time-series data (i.e., a string
ofinterrelateddata)[ 37],suchasimagedescription,textgeneration,
text classification, and other tasks.
2.2 Neuron Coverage Criteria
Neuron Coverage (NC) was first proposed by Pei et al. [ 43]t o
findinputsforDLsystemsthatcantriggerdifferentialbehaviors.
Analogoustothesourcecodecoverageofconventionalsoftware
1https://github.com/SATE-Lab/DeepState
599
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 07:56:15 UTC from IEEE Xplore.  Restrictions apply. DeepState: Selecting Test Suites to Enhance the Robustness of Recurrent Neural Networks ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
844State 
vectorOutput
Inputsoftmax
hà¯§à¬¿à¬µà¯Ÿhà¯§à¯Ÿhà¯§à¬¾à¬µà¯Ÿ
hà¯§à¬¿à¬µà¯Ÿ hà¯§à¯Ÿ
İ”à¯§à¬¿à¬µ İ”à¯§ İ”à¯§à¬¾à¬µà·œ İ•à¯§à¬¿à¬µ à·œ İ•à¯§à·œ İ•à¯§à¬¾à¬µ
Unfoldsoftmax softmax
Time step
Figure 1: A general RNN structure.
testing,theneuroncoverageisusedtojudgewhethertheoutput
value of each neuron after passing the activation function exceeds
a certain threshold ğ‘˜, and consider the neurons that exceed the
threshold as activated, i.e., covered. The rate of ğ‘ğ¶(ğ‘˜)for a test is
defined as:
ğ‘ğ¶(ğ‘˜)=|ğ‘ğ‘œğ‘£ğ‘’ğ‘Ÿğ‘’ğ‘‘ ğ‘›ğ‘’ğ‘¢ğ‘Ÿğ‘œğ‘›ğ‘  |
|ğ‘¡ğ‘œğ‘¡ğ‘ğ‘™ ğ‘›ğ‘’ğ‘¢ğ‘Ÿğ‘œğ‘›ğ‘  |
Recently, some coverage criteria have been proposed especially
for RNN, such as DeepStellar [ 15], testRNN [ 24,25], and RNN-
Test[19].DeepStellar[ 15]calculatesthebasicstatecoverage(BSCov)
and the basic transition coverage (BTCov) for quantitative analysis
of RNNs. BSCovis designed tomeasurehow thoroughly the test
inputs ğ‘‡coverthemajorfunctionregionvisitedwhiletraining.The
abstract states visited by the training inputs ğ‘€and the test inputs
ğ‘‡are denoted by Ë†ğ‘†ğ‘€andË†ğ‘†ğ‘‡, and the BSCov is defined as:
ğµğ‘†ğ¶ğ‘œğ‘£(ğ‘‡,ğ‘€)=|Ë†ğ‘†ğ‘‡âˆªË†ğ‘†ğ‘€|
|Ë†ğ‘†ğ‘€|
BTCovtargetstheabstracttransitionsactivatedbyvariousinput
sequences. BTCov compares the abstract transformation of the
hidden state of the RNN model during training and testing phases,
whichare denotedas Ë†ğ›¿ğ‘‡andË†ğ›¿ğ‘€, respectively.Thus, theBTCov is
defined as:
ğµğ‘‡ğ¶ğ‘œğ‘£(ğ‘‡,ğ‘€)=|Ë†ğ›¿ğ‘‡âˆªË†ğ›¿ğ‘€|
|Ë†ğ›¿ğ‘€|
testRNN [ 24,25] proposed Step-wise Coverage (SC) for evaluat-
ingthechangesintheoutputofthehiddenlayersinRNNsoverthe
timestep.Thechangeofthehiddenvectorinadjacenttimesteps
is defined as follows:
Î”ğœ‰â„
ğ‘¡=|ğœ‰â„,+
ğ‘¡âˆ’ğœ‰â„,+
ğ‘¡âˆ’1|+|ğœ‰â„,âˆ’
ğ‘¡âˆ’ğœ‰â„,âˆ’
ğ‘¡âˆ’1|
where Î”ğœ‰â„,+
ğ‘¡represents the sum of all positive components in
thehiddenstate â„attimestep ğ‘¡,andÎ”ğœ‰â„,âˆ’
ğ‘¡representsthesumof
all negative components in the hidden state â„at time step ğ‘¡. Then,
SC is defined as:
ğ‘†ğ¶=|{Î”ğœ‰â„
ğ‘¡â‰¥ğ‘£ğ‘†ğ¶|ğ‘¡âˆˆ{1,...,ğ‘›}|
|{Î”ğœ‰â„
ğ‘¡|ğ‘¡âˆˆ{1,...,ğ‘›}|
In the formula, ğ‘£ğ‘†ğ¶is a customizable threshold. In general, the
maximumvalue thatcan bereached duringtraining isselectedas
the threshold for testing.
RNN-Test [ 19] defines the hidden state coverage (HSCov) as
the ratio of the hidden states that achieve the maximum value
during testing. Assume ğ»denotes all the hidden states of an RNNmodelofgiveninputs,whichisafour-dimensionalmatrixofshape
(ğ‘‡,ğ¿,ğµ,ğ¸), where ğ‘‡,ğ¿,ğµ,ğ¸ are the number of time steps, layers,
batches, and hidden units, respectively. The HSCov is defined as:
ğ»ğ‘†ğ¶ğ‘œğ‘£=|{ğ‘’|âˆ€ğ‘’âˆˆâ„,âˆ€â„âˆˆğ»,ğ‘’=ğ‘šğ‘ğ‘¥(â„)}|
|ğ»|
2.3 Test Data Selection
Thetestdataselectiontechniqueisoriginallydesignedtoselecttest
casesintheregressiontestingscenarioandsavetheexecutiontime
cost and resources. It selects the tests that are deemed necessary to
verifythemodifiedsoftwarefromtheexistingtestset.Suppose ğ‘ƒ
is a procedure or program, ğ‘ƒ/primeis a modified version of ğ‘ƒ, andğ‘‡is a
set of tests (a test suite) created to test ğ‘ƒ. Tests that are valid for ğ‘ƒ
mayberedundantfor ğ‘ƒ/prime,becausetheirexecutiontrajectorydoes
not go through the modified code in ğ‘ƒ/prime. The process of identifying
themodifiedpartoftheexecutiontrajectorythrough ğ‘ƒ/primeiscalled
test selection.
Twoprimarycoverage-basedtestcasesprioritizationtechniques
areknownastheCoverage-TotalMethod(CTM)andtheCoverage-
Additional Method (CAM) [62].
(1)Coverage-Total Method (CTM) : it is regarded as the â€œnext
bestâ€strategy.CTMtakesnoconsiderationontheselectedtests.
It always selects the test with the highest coverage from the
candidate test suite.
(2)Coverage-AdditionalMethod (CAM) : it is a selection strat-
egydrivenbytheadditionalgreedyalgorithm.CAMdynami-
callyadjuststhenextselectedtestbasedontheselectedtests.
Italwaysselectsthetestthatcancovermostuncoveredcode
structures from the candidate test suite.
Inspired by the test selection in regression testing, in this paper,
we propose DeepState to select a subset from massive data for
RNN models.
3 APPROACH
WemodeltheRNNinternalstatechangesintoastatefulview.Based
on this stateful view, we design and implement a tool, namely
DeepState ,toselectasubsetofdatafromalarge unlabeled dataset
fortheRNNmodel.AsshowninFig.2, DeepState firstunfoldsthe
RNN model according to the time steps and captures the predicted
label sequence based on a stateful view as discussed in Section 3.1.
The label sequence can represent the internal state of the RNN hid-
den layers when predicting a given input data. Then, DeepState
calculates the changing rate and the changing trend, which are
presentedinSection3.2.Thesemetricsareemployedtoevaluate
the uncertainty and internal state transition of the RNN when pro-
cessing theinput data. We introduce the detailedimplementation
ofDeepState selectingtestcasesinSection3.3.Finally,Section3.4
discusses how to enhance RNN quality with DeepState.
3.1 The Stateful View of RNN
RNNs are often employed to process the sequence data, i.e., a se-
ries of interdependent data streams, because the neurons in RNNs
can dynamically update the hidden states based on the received
information over time. Suppose an RNN is designed for processing
atextclassificationtaskandtheinputisasequencedata xâˆˆXğ‘,
whereXistheinputdomain,and ğ‘denotesthelengthoftheinput
600
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 07:56:15 UTC from IEEE Xplore.  Restrictions apply. ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA Zixi Liu, Yang Feng, Yining Yin, and Zhenyu Chen
Label 
sequence
Selected 
DataMassive 
Test SetA Stateful View of RNN Selecting Test Suites
Changing rate (CR) 
Changing trend (CT)Uncertainty Similarity
Original 
test data
Unfolded by 
time stepsTest 
caseÜ¬Üµà¬µ,Üµà¬¶=|Üµà¬µÜµ×ªà¬¶|
|Üµà¬µÜµ×«à¬¶|
RNN units1332213
223à¡¾à¡¯=à«›
à«
1 3 2
2 33
2 3t1 t3
t2 t4
LowHigh
SortingCT Distance
>ß¬?
Test1  
& Test2Test1  
or Test2Yes Nohà¯§à¬¿à¬µà¯Ÿhà¯§à¯Ÿhà¯§à¬¾à¬µà¯Ÿ
hà¯§à¬¿à¬µà¯Ÿhà¯§à¯Ÿ
İ”à¯§à¬¿à¬µ İ”à¯§İ”à¯§à¬¾à¬µà·œ İ•à¯§à¬¿à¬µ à·œ İ•à¯§à·œ İ•à¯§à¬¾à¬µ
softmax softmax softmaxSame 
CR Test1
Test2
â€¦Generated  
test data
Figure 2: Overview of DeepState.
sequence.The ğ‘–-thwordintheinputsentence xisrepresentedas
ğ‘¥ğ‘–âˆˆX. For a given input sentence x, the RNN model iteratively
receives each element in x(i.e.,ğ‘¥ğ‘¡is received at time step ğ‘¡) and
maintainstheinternationalhiddenstatevector sâˆˆSğ‘.Specifically,
the time step represents the number of iterations and is equal to ğ‘.
At time step ğ‘¡, the hidden state output is denoted as ğ‘ ğ‘¡âˆˆS, andğ‘ ğ‘¡
isupdateduponboth ğ‘¥ğ‘¡andğ‘ ğ‘¡âˆ’1.Then,theRNNmodeloutputthe
classification prediction result ğ‘¦ğ‘¡through an activation function
ğ‘“(Â·).Generally,inmulti-classificationtasks,theSoftmaxfunction
isappliedtoreflectthehigh-dimensionalhiddenstatevectorinto
the probability distribution vector for each label.
From the stateful view of RNN, we analyze the RNN modelâ€™s
predictionuncertaintyandbehaviorfeaturesuponthehiddenstate
ateachtimestep.Becausethehigh-dimensionalhiddenstatevector
sisdifficulttoanalyze,weapplythecorrespondingpredictedlabel ğ‘¦
toreflecttheinformationofthehiddenstate.Fortwoadjacenttime
stepsğ‘¡andğ‘¡+1, the change of hidden state can be regarded as the
corresponding label ğ‘¦ğ‘¡andğ‘¦ğ‘¡+1. Note that in the earlier time steps,
whentheRNNmodelhasnotyetbeenwelltrained,itisnaturalfor
it to produce different prediction results and has low confidencefor the output. As the received information increases, the RNN
modelâ€™sconfidenceforthepredictionresultsalsoincreases,andthe
prediction results at later time steps are supposed to be consistent.
Table1:Theexampleofhiddenstateoutputsoftwoimages
in the MNIST dataset.
ID Figure Label sequence Changes Output
1
 [1, 1, 3, 0, 0, 0, 9, 9, 9, 9, 9, 9, 9, 9]1â†’3;
3â†’0;
0â†’99
2
 [1, 1, 2, 2, 2, 2, 2, 2, 3, 3]1â†’2;
2â†’33
Toselectteststhatmaytriggerthepotentialerroneousbehaviors
of RNN models, we weigh the cases that cause a high changing
rateoftheRNNmodelâ€™spredictionoveralltimesteps.Forexample,
Table 1 shows a LSTM modelâ€™s prediction process of two similar
imagesfromtheMNIST[ 61]dataset.Thecorrectlabelofthesetwo
images is nine, but the RNN mispredicted the second image (ID=2)
to three, and the first picture (ID=1) is predicted correctly. Sincethe input data is a 28x28 image, the data is divided into 28 time
steps (i.e., each time step inputs a row of pixels) and input into theRNN model. We capture the hidden state output at each time step,
and the corresponding predicted label. We list the prediction labels
with confidence greater than 0 .5 at each time step, which is shown
in the label sequence column in Table 1. The label sequence for thefirstfigurehas3changes(1
â†’3,3â†’0,0â†’9)overthetimesteps.
Similarly, the label sequence corresponding to the second figure
hastwochanges(1 â†’2,2â†’3).Althoughthechangesâ€™numberof
the second label sequence is one less than that of the first one, the
firstsequenceislonger,andthesecondsequencehasachangeat
the back of the sequence, which indicates the uncertainty of RNN.
3.2 Metric Computation
Basedontheabovestatefulview,weemploythepredictionlabelsequencetorepresentthehiddenstateoutputstatusofRNNpro-
cessing a given input data. Then we propose two analysis metrics,
i.e., the changing rate and changing trend of hidden states. The
changingratedescribes theuncertaintydegreeoftheRNN model
for agiven input testcase, while thechanging trend describesthe
similarityoftheoutputstateofRNNprocessingdifferenttestcases.
Definition 3.1 (Label sequence). For a given input ğ‘¥, a label se-
quence is a list ğ‘†ğ‘’ğ‘(ğ‘¥,ğ‘)={ğ‘¦1,ğ‘¦2,...,ğ‘¦ğ‘}, where ğ‘is the confi-
dence threshold. At the ğ‘–-time step, the RNN modelâ€™s hidden state
vectorcanberepresentedasthecorrespondingpredictedlabel ğ‘¦ğ‘–
throughRNNâ€™sactivationfunction.Wemaintainthepredictedla-
bel with confidence greater than ğ‘in the label sequence. And ğ‘is
thenumberofoutputlabelswithconfidencegreaterthan ğ‘inthe
output of all time steps, i.e., |ğ‘|â‰¤|ğ‘¡ğ‘–ğ‘šğ‘’ ğ‘ ğ‘¡ğ‘’ğ‘ğ‘  |.
Based on the label sequence, we measure the uncertainty of
theRNNforagiveninputbycalculatingthechangingrateofthe
predicted labels.
Definition3.2(Changingrate). Thechangingrateindicatesthe
ratio of thenumber of adjacent labelsthat are not thesame as the
total sequence length in a label sequence ğ‘†ğ‘’ğ‘(ğ‘¥,ğ‘).
The changing rate ğ¶ğ‘…(ğ‘†ğ‘’ğ‘(ğ‘¥,ğ‘))can be defined as:
ğ¶ğ‘…(ğ‘†ğ‘’ğ‘(ğ‘¥,ğ‘))=ğ¶ğ‘œğ‘¢ğ‘›ğ‘¡(ğ‘¦ğ‘¡+1â‰ ğ‘¦ğ‘¡)
|ğ‘âˆ’1|(1)
Becausethepredictionsateachtimestepmaynothaveahigh
degreeofconfidence,especiallyinthefirstperiodoftime,wecal-
culate the changing rate on the label sequence, where each label is
predicted with a confidence higher than a given threshold.
Wecalculatetheweightedchangingrate,consideringthatasthe
time step increases, RNN can receive more information, and the
basisofthepredictionresultissufficient.Thus,thelaterchanges
601
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 07:56:15 UTC from IEEE Xplore.  Restrictions apply. DeepState: Selecting Test Suites to Enhance the Robustness of Recurrent Neural Networks ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
in the ğ‘†ğ‘’ğ‘(ğ‘¥,ğ‘)should be more weighted. Therefore, the changing
rate we applied to select tests is defined as follows:
ğ¶ğ‘…(ğ‘†ğ‘’ğ‘(ğ‘¥,ğ‘))=ğ¶ğ‘œğ‘¢ğ‘›ğ‘¡(ğ‘¦ğ‘¡+1â‰ ğ‘¦ğ‘¡)âˆ—ğ‘Šğ‘’ğ‘–ğ‘” â„ğ‘¡( ğ‘¡)
Î£ğ‘Šğ‘’ğ‘–ğ‘” â„ğ‘¡( ğ‘¡)(2a)
ğ‘Šğ‘’ğ‘–ğ‘” â„ğ‘¡( ğ‘¡)=ğ‘¡2,where ğ‘¡={1,2,...,ğ‘}. (2b)
We calculate the changing trend of the internal state of the
RNN by mapping the specific transition between two states to the
predicted labelâ€™s transition. Specifically, the changing trend can
be expressed as a set of conversions ğ¶ğ‘‡of two adjacent labels
(ğ‘¦ğ‘¡,ğ‘¦ğ‘¡+1)in the label sequence ğ‘†ğ‘’ğ‘(ğ‘¥,ğ‘).
Definition3.3(Changingtrend). Foralabelsequence ğ‘†ğ‘’ğ‘(ğ‘¥,ğ‘),
the changing trend is a set ğ¶ğ‘‡(ğ‘†ğ‘’ğ‘(ğ‘¥,ğ‘))of tuples composed of
adjacent labels (ğ‘¦ğ‘¡âˆ’1,ğ‘¦ğ‘¡)in chronological order.
For example, the label sequence corresponding to the test ID=1
in Table 1 is [1,1,3,0,0,0,9,9,9,9,9,9,9,9], and the corresponding
changing trend is {(1,1),(1,3),(3,0),(0,0),(0,9),(9,9)}. Similarly,
the change sequence of the test ID=2 is [1,1,2,2,2,2,2,2,3,3], and
the changing trend is {(1,1),(1,2),(2,2),(2,3),(3,3)}.
To evaluate the similarity of two changing trends, we apply the
Jaccardsimilaritycoefficient[ 39]forevaluation.Thesimilarityis
calculated as follows:
ğ½(ğ¶ğ‘‡1,ğ¶ğ‘‡2)=|ğ¶ğ‘‡1âˆ©ğ¶ğ‘‡2|
|ğ¶ğ‘‡1âˆªğ¶ğ‘‡2|(3)
where ğ¶ğ‘‡1andğ¶ğ‘‡2representthechangingtrendoftheRNNcor-
respondingtotwodifferenttestcases.Thevaluerangeof ğ½(ğ¶ğ‘‡1,ğ¶ğ‘‡2)
is[0,1].When ğ½(ğ¶ğ‘‡1,ğ¶ğ‘‡2)isgreaterthanagiventhreshold,itin-
dicates these two tests are similar to each other, and then we only
keep one of them in the selection.
3.3 RNN Test Suite Selection Algorithm
DeepState weightsthetestcaseswithahighchangingrateoflabel
sequence, especially the predicted label changes at a later time
step.Specifically, DeepState firstsortseachtestcaseinthedataset
according to the changing rate, which indicates the uncertainty
degree of RNN for a given test. We believe the uncertainty degree
indicates the probability of the RNN model triggering bugs. Yet, it
is not suitable to select many tests with the same changing rate
directly because the selected test suites may not be representative.
Thus, DeepState selectstestswithdifferentstatesâ€™changingtrends
as the representative from the tests with the same changing rate.
Specifically, Algorithm 1 presents the process of DeepState
selecting test suites for testing and optimizing the RNN models.For a given RNN model
ğ‘€to be tested and a massive data set ğ·,
DeepState firstmakestheRNNexecutealltestdatainturnandcal-
culatesthechangingrate ğ¶ğ‘…andchangingtrend ğ¶ğ‘‡corresponding
toeachdata(Line1-7).Second, DeepState sortsalltestsinreverse
orderaccordingtothechangingrate(Line8)andsetsasimilarity
threshold ğœfor the changing trend (Line 9). Third, for the test case
set in ğ·sorted by changing rate, DeepState traverses each test
case in turn (Line 10). If the changing rates of the two test cases
are different, both of them are selected (Line 11-13). For tests with
the same changing rate, we calculate the changing trend similarity
between them (Line 14-17). Both tests are selected if the distance is
smaller than the threshold ğœ(Line 18-20). Otherwise, only one testisselected.Finally,wecheckthelasttest ğ‘‘ğ‘›âˆˆğ·.Ifthechanging
rateğ¶ğ‘…is different from that of ğ‘‘ğ‘›âˆ’1, and it is not selected, then
we add it to ğ·ğ‘ğ‘¡ğ‘ğ‘†ğ‘’ğ‘™ğ‘’ğ‘ğ‘¡ğ‘’ğ‘‘ (Line 26-28).
Algorithm 1: Selecting test suites
Input:ğ·=[ğ‘‘0,ğ‘‘1,...,ğ‘‘ ğ‘›]: The original data set
Input:ğ‘€: The tested RNN model
Output:ğ·ğ‘ğ‘¡ğ‘ğ‘†ğ‘’ğ‘™ğ‘’ğ‘ğ‘¡ğ‘’ğ‘‘ : The selected data
1ğ‘–=0;
2ğ‘—=0;
3whileğ‘–<ğ‘›+1do
4ğ‘‘ğ‘–.ğ¶ğ‘…= getChangeRate( ğ‘‘ğ‘–,ğ‘€);
5ğ‘‘ğ‘–.ğ¶ğ‘‡= getChangeTrend( ğ‘‘ğ‘–,ğ‘€);
6ğ‘–=ğ‘–+1;
7end
8ğ·= reverse_sortBy( ğ·.ğ¶ğ‘…);
9ğœ= setThreshold() ; // If the distance between two CTs is
greater than ğœ, then we keep one of them.
10whileğ‘—<ğ‘›do
11ğ·ğ‘ğ‘¡ğ‘ğ‘†ğ‘’ğ‘™ğ‘’ğ‘ğ‘¡ğ‘’ğ‘‘ .add(ğ‘‘ğ‘—);
12ifğ‘‘ğ‘—+1.ğ¶ğ‘…â‰ ğ‘‘ğ‘—.ğ¶ğ‘…then
13 ğ‘—=ğ‘—+1;
14else
15 ğ‘˜=ğ‘—+1;
16 whileğ‘‘ğ‘˜.ğ¶ğ‘…=ğ‘‘ğ‘—.ğ¶ğ‘…andğ‘˜<ğ‘›+1do
17 ğ‘‘ğ‘–ğ‘ = JaccardDis( ğ‘‘ğ‘—.ğ¶ğ‘‡,ğ‘‘ğ‘˜.ğ¶ğ‘‡);
18 ifğ‘‘ğ‘–ğ‘ <ğœthen
19 ğ·ğ‘ğ‘¡ğ‘ğ‘†ğ‘’ğ‘™ğ‘’ğ‘ğ‘¡ğ‘’ğ‘‘ .add(ğ‘‘ğ‘˜);
20 end
21 ğ‘˜=ğ‘˜+1;
22 end
23 ğ‘—=ğ‘˜;
24end
25end
26ifğ‘‘ğ‘›.ğ¶ğ‘…â‰ ğ‘‘ğ‘›âˆ’1.ğ¶ğ‘…andğ‘‘ğ‘›âˆ‰ğ·ğ‘ğ‘¡ğ‘ğ‘†ğ‘’ğ‘™ğ‘’ğ‘ğ‘¡ğ‘’ğ‘‘ then
27ğ·ğ‘ğ‘¡ğ‘ğ‘†ğ‘’ğ‘™ğ‘’ğ‘ğ‘¡ğ‘’ğ‘‘ .add(ğ‘‘ğ‘›);
28end
29returnğ·ğ‘ğ‘¡ğ‘ğ‘†ğ‘’ğ‘™ğ‘’ğ‘ğ‘¡ğ‘’ğ‘‘
Arunningexample. Assumethatwehavesixtests ğ´, ğµ,ğ¶, ğ·, ğ¸
andğ¹as well as an RNN with five time-steps. The RNN modelis
designedtoclassifytestsintofourclasses,i.e.,thelabelrangesfrom
1to4.Table2showspredictionlabelsequences,changingrates,and
changing trends of some tests. Assume that the distance threshold
for judging whether the changing trends of two tests are similar is
0.5, i.e., ğœ=0.5. These test cases are sorted into a descending order
upon the value of the changing rate. DeepState first selects test
caseğ´because it has the highest changing rate, and no other case
has the same changing rate value. Next, because the changing rate
valuesoftestcases ğµ,ğ¶,andğ·arethesame, DeepState firstselects
caseğµ,andthencalculatetheJaccarddistanceofchangingtrend
between ğµandğ¶,whichis1 /7(<0.5).Because1 /7islessthan ğœ,we
expectthat althoughthesetwo testcaseshave thesamechanging
rate, they trigger different states of the RNN, so DeepState selects
bothğµandğ¶.However,becausetheJaccarddistanceofchanging
trend between ğ¶andğ·is 3/5(>0.5), which is greater than ğœ,
DeepState passes ğ·. Similarly, DeepState only selects one case
between ğ¸andğ¹because the Jaccard distance of changing trend
between them is 3/5 (>0.5).
602
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 07:56:15 UTC from IEEE Xplore.  Restrictions apply. ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA Zixi Liu, Yang Feng, Yining Yin, and Zhenyu Chen
Insummary,forthisexampleinTable2, DeepState selecttest
cases{ ğ´, ğµ,ğ¶, ğ¸}asitsoutput.Iftherequirementistoselectmore
than4testcases,then DeepState selectsthetestcase ğ·andğ¹in
turn.BecausetheinternalstatebehaviorsofRNNwhenprocessing
testsğ¶andğ·(orğ¸andğ¹)areverysimilar,thereisnoneedtoapply
both tests for testing or optimization.
Table 2: An example to show how DeepState selects tests.
TestLabel Seq. Changing rate Changing trend
A[1, 2, 3, 2, 1] 4/4 { (1, 2), (2, 3), (3, 2), (2, 1) }
B[1, 2, 3, 3, 1] 3/4 { (1, 2), (2, 3), (3, 3), (3, 1) }
C[2, 1, 3, 1, 1] 3/4 { (2, 1), (1, 3), (3, 1), (1, 1) }
D[3, 1, 2, 1, 1] 3/4 { (3, 1), (1, 2), (2, 1), (1, 1) }
E[2, 3, 3, 1, 1] 2/4 { (2, 3), (3, 3), (3, 1), (1, 1) }
F[2, 2, 3, 3, 1] 2/4 { (2, 2), (2, 3), (3, 3), (3, 1) }
ğ½(ğµ,ğ¶)=|{(3,1)}|
|{(1,1),(1,2),(1,3),(2,1),(2,3),(3,3),(3,1)}|=1
7
ğ½(ğ¶,ğ·)=|{(2,1),(3,1),(1,1)}|
|{(1,1),(1,2),(1,3),(2,1),(3,1)}|=3
5
ğ½(ğ¸,ğ¹)=|{(2,3),(3,1),(3,3)}|
|{(1,1),(2,2),(2,3),(3,1),(3,3)}|=3
5
3.4 Enhancing RNN with DeepState
To improve the practical performance of modern RNN-driven soft-
ware applications, a general approach is to retrain the RNN model
periodicallywiththedatacollectedintheusagescenarios.Thepro-
cessoftenrequiresthedatatobelabeledproperly;however,data
labeling is an expensive and time-consuming task. Moreover, re-
trainingtheRNNmodelswithalargeamountofdataisalsoawaste
oftimeandresources. DeepState providesanautomaticsolution
to this problem. It is designed for selecting a subset from a large
unlabeled dataset.Theselecteddatahastheabilitytodiscoverpo-
tential defects of the RNN, which can activate the state transitions
of different RNNs, ensuring the adequacy of the retraining data.
DeepState allows us to find and label as many tests that may trig-
gerRNNâ€™serroneousbehaviorsaspossibleinalimitedtimebudget.
We observe that the tests selected by DeepState are more effec-
tiveinenhancingRNNthanthetestsselectedbycoverage-based
selection techniques.
4 EXPERIMENT DESIGN
Wehaveconductedextensiveexperimentstoevaluatetheperfor-
manceof DeepState .Thissectionintroducestheexperimentset-
tingsandtheresearchquestions.Toconducttheexperiments,we
implement DeepState upon Python 3.6.0 [ 4] and Keras [ 9] with
TensorFlow 1.14.0 [ 5]. All experiments are performed on a Ubuntu
18.04.3LTSserverwithTeslaV100-SXM2,one10-coreprocessor
with 2.50GHz, and 32GB physical memory.
4.1 Datasets and RNN Models
AsshowninTable3,weexperiment DeepState withimageclassifi-
cationandtextclassificationmodels.Foreachdatatype,weemploytwowidely-useddatasets,andweappliedeachdatasetontwoRNNmodelstoensurethegeneralityoftheexperimentresults.Thus,we
haveeightcombinationsofdatasetandmodelthatcoverpopular
RNN variants.The MNIST [ 61] dataset is for handwritten digits recognition,
containing60 ,000trainingimagesand 10 ,000testingimages. The
Fashion[58]datasetisadatasetofZalandoâ€™sarticleimagesconsist-
ingof60 ,000trainingimagesand10 ,000testingimages.Snips[ 11]
natural language understanding benchmark is a dataset of over
16,000 crowdsourced queries distributed among 7 different user
intents. AgNews [ 1,66] dataset is collected by more than 1 mil-
lionnewsarticles,whicharedividedintofourclasses,i.e.,world,
sports, business, and sci/tec. The total number of training samples
is 120,000, and the testing is 7 ,600.
The LSTM (Long Short-Term Memory) [ 21] model is the most
widely-used optimized RNN variant. It employs forget gates to
recordthelong-termmemoryandshort-termmemoryoutputofthe RNN [
41]. LSTM can effectively solve the problem of gradi-
entdisappearanceandgradientexplosionoftheRNNmodel.The
BiLSTM (Bidirectional LongShort-Term Memory) [ 17] is thebidi-
rectionalLSTMmodel,combiningtheforwardLSTMandbackward
LSTM [41]. The GRU (Gated Recurrent Unit) [ 14] model is another
variantofLSTM.ItcombinestheforgetgateandinputgateofLSTM
into a single gate, which is regarded as the reset gate [10].
Table 3: The details of Subject Datasets and RNN models.
DatasetRNN
ModelsState Vec.
Shape# Trainable
ParametersDataset
Description
MNISTLSTM (128, 28) 81 ,674 Handwritten
numbers 0~9 BiLSTM (256, 28) 177 ,866
FashionLSTM (128, 28) 69 ,194 Zalandoâ€™s article
images (10 classes) GRU (128, 28) 98 ,186
SnipsBiLSTM (256, 16) 3 ,194,119 Intent recognition
(7 classes) GRU (128, 16) 2 ,918,279
AgNewsLSTM (128, 35) 214 ,148 News classification
(4 classes) BiLSTM (256, 35) 427 ,652
4.2 Test Data Collection
To ensure that the experiment data can reflect the realistic data
mutation in the usage scenarios, we employ widely-used benigndatamutationoperatorsratherthanadversarialexamplegenera-tion techniques to augment test data. Specifically, we adjust the
parametersoftheseoperatorsandlimitthemutationextenttobe
lessthan5%.Thus,theaugmenteddatamaintainsthesamelabel
as the original data. For each image classification model, we gener-
ate the data by adding perturbations with existing transformation
techniques[ 38,54],includingcontrast,brightness,shift,rotation,
scaling, and shearing. All the implementation of image transforma-
tion is based on the Keras ImageDataGenerator tool [ 42]. For each
figureintheoriginaltestset,wegeneratethecorrespondingfigure
with random parameters of multiple operators. For the text data,
wegeneratetheaugmenteddatabyaddingperturbationswithex-
isting transformation techniques, including synonym replacement,
back translation [ 51], word insertion [ 27], and abstract summariza-
tion [45], which is a data synthesis method for paragraphs. The
textual transitions are implemented based on an open-source tool
called nlpaug [33].
603
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 07:56:15 UTC from IEEE Xplore.  Restrictions apply. DeepState: Selecting Test Suites to Enhance the Robustness of Recurrent Neural Networks ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
4.3 Baseline Approaches
Wecompare DeepState withothermethodsbasedonneuroncover-
ageasintroducedinSection2.2.Thenwesortthetestsbyapplying
CTM and CAM as introduced in Section 2.3 and then select the
tests to satisfy the selection ratio requirement.
We apply both CAM and CTM strategies to sort the NC and
SC (testRNN) and select the corresponding test cases. We apply
HSCov(RNNTest)withtheCAMselectionstrategy.HSCOVcan
onlyindicatewhichRNNhiddenneuronisactivatedforeachgiven
input, so it cannot be selected after sorting by CTM. We apply
BSCovandBTCov(DeepStellar)withtheCTMselectionstrategy.
ThecriteriaproposedinDeepStellarcannotbeselectedbasedon
CAM because it converts the output state of the hidden layer into
an abstract model and calculates the coverage. In addition, we also
employarandomselectionmethodasthebaseline,i.e.,randomly
selecting a fixed proportion of test cases from the candidate set.
4.4 Research Questions
DeepState is designed for facilitating testers of RNN-driven sys-
tems to quickly select tests that can trigger the potential erroneous
behaviors andeffectively enhance therobustness. To this end,we
empirically evaluate its performance based on the following three
research questions (RQ).
RQ1. Effectiveness: Can the dataset selected by DeepState
detect more defects than neuron-coverage-based methods?
Similartothetestcaseselectiontechnologyintraditionalsoft-
waretesting,thegoaloftestcaseselectionfordeeplearningmodels
is also to filter out cases that can trigger potential defects. We pro-
vide answers to RQ1 by evaluating the bug detection rate of the
test data set selected by DeepState and other baseline approaches.
To generate the candidate dataset, we apply the benign augmen-
tation methods described in Section 4.2 on the original test set
ğ·ğ‘¡ğ‘’ğ‘ ğ‘¡.Theaugmenteddataisdenotedas ğ·/prime
ğ‘¡ğ‘’ğ‘ ğ‘¡,whichhasthesame
sizeas ğ·ğ‘¡ğ‘’ğ‘ ğ‘¡.Werandomlyselect30%sizeofthedatafrom ğ·ğ‘¡ğ‘’ğ‘ ğ‘¡
andğ·/prime
ğ‘¡ğ‘’ğ‘ ğ‘¡, respectively, and then merge them together to form a
candidate test set ğ·ğ‘ ğ‘’ğ‘™ğ‘’ğ‘ğ‘¡. Thus, The size of ğ·ğ‘ ğ‘’ğ‘™ğ‘’ğ‘ğ‘¡is 60% of the
size of ğ·ğ‘¡ğ‘’ğ‘ ğ‘¡. To alleviate the potential bias, we follow the above
step and repeat 30 times to randomly sample 30 candidate sets,
denotedas ğ‘‡1,ğ‘‡2,...,ğ‘‡30.Foreachmodelandtestsuite,weapply
all the selection techniques to select test suites from ğ·ğ‘ ğ‘’ğ‘™ğ‘’ğ‘ğ‘¡, and
we record the corresponding bug detection rate when the selection
ratioğ‘Ÿ={10%,20%}.Weemploy ğ‘‡ğ‘–todenotethecandidateset,and
ğ‘‡ğ‘–,ğ‘ğ‘¢ğ‘”to denote the bug cases in ğ‘‡ğ‘–, then the bug detection rate can
be calculated as follows:
ğµğ‘¢ğ‘” ğ·ğ‘’ğ‘¡ğ‘’ğ‘ğ‘¡ğ‘–ğ‘œğ‘› ğ‘…ğ‘ğ‘¡ğ‘’ (ğ‘‡ğ‘–)=|ğ‘‡ğ‘–,ğ‘ğ‘¢ğ‘”|
|ğ‘‡ğ‘–|(4)
RQ2.Inclusiveness: CanDeepState filteroutthebugtestcases
in the candidate test data set?
In RQ2, we investigate the extent to which DeepState selects
tests that can reveal potential flaws in RNN models. Referring tothe evaluation metric of inclusiveness proposed by Rothermel et
al. [47], inclusiveness can measure the capability of regression test
selectionmethodsinchoosingmodification-revealingtestsfromthecandidateset.Considering
DeepState isdesignedtoselectasubset
for manual labeling rather than for regression testing, we adapt
thenotionofinclusivenesstofittheperformanceevaluationofourapplication scenario, which measures the capability of selecting
bug-revealingtestsfromthe candidate set.Specifically,wedenote
the original test set as ğ·ğ‘¡ğ‘’ğ‘ ğ‘¡, the selected test set as ğ·ğ‘ ğ‘’ğ‘™ğ‘’ğ‘ğ‘¡, and
denotetheteststhatcandetecterroneousbehaviorsinthesetwo
sets as ğ·ğ‘¡ğ‘’ğ‘ ğ‘¡,ğ‘ğ‘¢ğ‘”, andğ·ğ‘ ğ‘’ğ‘™ğ‘’ğ‘ğ‘¡,ğ‘ğ‘¢ğ‘”, respectively. The inclusiveness
can be calculated as follows:
ğ¼ğ‘›ğ‘ğ‘™ğ‘¢ğ‘ ğ‘–ğ‘£ğ‘’ğ‘›ğ‘’ğ‘ ğ‘  =|ğ·ğ‘ ğ‘’ğ‘™ğ‘’ğ‘ğ‘¡,ğ‘ğ‘¢ğ‘”|
|ğ·ğ‘¡ğ‘’ğ‘ ğ‘¡,ğ‘ğ‘¢ğ‘”|âˆ—100% (5)
RQ3.Guidance: CanDeepState guidetheretrainingofanRNN
to improve its accuracy?
Theoretically,the selectedtest suiteswith ahigh bugdetection
rate can be applied to optimize the RNN models. Thus, we pro-
pose RQ3 to evaluate whether the selected data can be applied for
retraining and further enhance the robustness of RNN models.
ToanswerRQ3,wegenerateacandidatedataset ğ·ğ‘ ğ‘’ğ‘™ğ‘’ğ‘ğ‘¡contain-
ingaugmentedtrainingdataandtheoriginaltrainingdata.Tokeep
ğ·ğ‘ ğ‘’ğ‘™ğ‘’ğ‘ğ‘¡diverse,werandomlyselect10% ğ·ğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘›asthedataset ğ·ğ‘ğ‘ğ‘Ÿğ‘¡
andkeepitseparatefromtheoriginaltrainingprocess.Then,we
generatetheaugmenteddata ğ·/prime
ğ‘ğ‘ğ‘Ÿğ‘¡basedon ğ·ğ‘ğ‘ğ‘Ÿğ‘¡.Finally, ğ·ğ‘ ğ‘’ğ‘™ğ‘’ğ‘ğ‘¡
iscomposedof ğ·/prime
ğ‘ğ‘ğ‘Ÿğ‘¡,ğ·ğ‘ğ‘ğ‘Ÿğ‘¡andpartof ğ·ğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘›,andtheaugmented
datağ·/prime
ğ‘ğ‘ğ‘Ÿğ‘¡is kept to account for 25% of ğ·ğ‘ ğ‘’ğ‘™ğ‘’ğ‘ğ‘¡.
We employ each selection technique to select 15% data from
ğ·ğ‘ ğ‘’ğ‘™ğ‘’ğ‘ğ‘¡to retrain the original model. To demonstrate that improve-
ments in the accuracy are not influenced by inconsistencies in the
data, we verify the modelâ€™s accuracy improvement on both aug-
mented test set ğ·/prime
ğ‘¡ğ‘’ğ‘ ğ‘¡and the mixed test set ğ·ğ‘šğ‘–ğ‘¥(augmented test
datağ·/prime
ğ‘¡ğ‘’ğ‘ ğ‘¡+originaltestdata ğ·ğ‘¡ğ‘’ğ‘ ğ‘¡)respectively.Weconsiderthat
the robustness of an RNN model is improved when the accuracy of
the retrained model on the original test set and the augmented test
set both improved compared to the original model.
5 RESULT ANALYSIS
This section presents the experiment result and then analyzes the
performance of our approach.
5.1 Answer to RQ1: Effectiveness
The average bug detection rate of 30 time experiments correspond-
ing to 10% and 20% of selection rates is shown in Table 4. Fig. 3
showsthebugdetectionrateofeachselectionmethodwhenthese-lectionratioissetto10%.Comparedwithbaselines,
DeepState has
achieved thehighest bugdetectionrate underdifferent selectionratios, which indicates
DeepState can effectively can help RNN
models to detect more potential defects under given resource con-
straints. Among these four data sets, MNIST and Fashion are both
imagedata,whileSnipsandAgNewsaretextdata.Duetotheshort
originaltextlengthandlimitedtransformationmethods,thetext
classificationmodelshavearelativelyhighaccuracyscoreonthe
augmented text data set, so the bug detection rate is lower than
that of the other two models.
Compared with random selection strategies, HSCOV (CAM), SC
(CAM), NC (CAM), and DeepState can select tests with a higher
bug detection rate, which indicates that these methods are moreeffective than random selection. Among them, the bug detectionrateofthetestcasesselectedby
DeepState achievesthehighest.
604
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 07:56:15 UTC from IEEE Xplore.  Restrictions apply. ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA Zixi Liu, Yang Feng, Yining Yin, and Zhenyu Chen
20 40 60BTCov(CTM)BSCov(CTM)NC(CTM)NC(CAM)SC(CTM)SC(CAM)HSCov(CAM)RandomDeepStateMnist-LSTM
20 40 60Fashion-LSTM
10 20 30Snips-BiLSTM
10 20 30 40 50AgNews-LSTM
20 40 60BTCov(CTM)BSCov(CTM)NC(CTM)NC(CAM)SC(CTM)SC(CAM)HSCov(CAM)RandomDeepStateMnist-BiLSTM
30 40 50 60Fashion-GRU
0 10 20 30Snips-GRU
10 20 30 40 50AgNews-BiLSTM
Figure 3: The bug detection rate of different selection methods with 10% selected tests.
ThisisbecausetheCAMstrategycanachievethemaximumcov-
eragewithasmallnumberoftests,leadingtosomeselectedtests
being invalid. Meanwhile, in fact, it is difficult to activate neurons
attheearliertimestep.Therefore,basedontheselectionstrategyof
CAMandcoverage,onlyasmallpartofthetestswithdiversitycan
be selected. From the perspective of selection strategy, the CAM
generally can select more bug cases than the CTM. This is because
theCAMcandeterminethedatatobeselectednextbasedonthe
selectedresultstoensurethatthenumberofactivatedneuronsis
as large as possible. However, CTM directly sorts and selects ac-
cordingtothecoveragerate.Itislikelythatarelativelysimilartestcaseisselected,whichcausesthebugdetectionratetoberelatively
low.Therefore,comparedwiththerandomselectionstrategy,the
selectedtestsetbasedonCTMhasarelativelylowerbugdetection
rate, while the tests selected based on CAM generally achieve a
higher bug detection rate.
Table4:Thebugdetectionrateof10%and20%selectedtests.
Model Sel Ran.HSCov SC SC NC NC BSCov BTCov Deep
(CAM) (CAM) (CTM) (CAM) (CTM) (CTM) (CTM) StateMNISTLSTM10% 0.33 0.55 0.32 0.18 0.33 0.1 0.14 0.23 0.63
20% 0.32 0.44 0.33 0.18 0.33 0.09 0.13 0.19 0.58
BiLSTM10% 0.33 0.61 0.33 0.32 0.39 0.23 0.08 0.08 0.69
20% 0.33 0.49 0.33 0.32 0.36 0.23 0.1 0.1 0.63FashionLSTM10% 0.32 0.46 0.32 0.21 0.32 0.22 0.21 0.25 0.6
20% 0.32 0.39 0.32 0.22 0.33 0.24 0.22 0.26 0.55
GRU10% 0.32 0.42 0.32 0.31 0.32 0.32 0.3 0.29 0.61
20% 0.32 0.37 0.32 0.31 0.32 0.3 0.3 0.24 0.56SnipsBiLSTM10% 0.12 0.12 0.13 0.11 0.13 0.09 0.1 0.14 0.31
20% 0.12 0.12 0.12 0.12 0.13 0.1 0.11 0.13 0.27
GRU10% 0.1 0.11 0.1 0.11 0.1 0.12 0.03 0.05 0.3
20% 0.1 0.11 0.11 0.11 0.11 0.12 0.07 0.08 0.25AgNewsLSTM10% 0.18 0.24 0.18 0.18 0.18 0.15 0.13 0.14 0.48
20% 0.18 0.22 0.18 0.18 0.19 0.16 0.12 0.13 0.42
BiLSTM10% 0.21 0.26 0.2 0.2 0.2 0.17 0.12 0.14 0.46
20% 0.2 0.25 0.2 0.2 0.2 0.18 0.13 0.14 0.41
5.2 Answer to RQ2: Inclusiveness
Foreachmodelanddataset,weevaluatetheinclusivenessofthe
selecteddatasetwhentheselectionratio ğ‘˜={1%,2%,3%,...,40%}.AsshowninFig.4,thex-axisofeachlinechartrepresentstheselec-
tionratio ğ‘˜fromthecandidatedataset,andthey-axisrepresents
theinclusivenessoftheselecteddataset.Withtheselectionratio ğ‘˜
increasing,theinclusivenessofthetestdataselectedbyallmethodsis improved. Among them,
DeepState outperforms other selection
methods, which indicates that DeepState can select more bug test
cases under the same select proportion.
In most combinations of models and data sets, the inclusiveness
of the test suite selected by HSCOV (CAM) is better than that of
random. We can safely conclude that HSCov is more effective than
randomasaselectionstrategy,butitisnotaseffectiveas DeepState .
The effects of SC (CAM) and NC (CAM) are similar to those of
randomselection. Thismay be becausethe maximumcoverageis
reachedquicklywhenapplyingtheCAMselectstrategy,andthe
remaining partkeeps thesame effect asrandom selection. Forthe
Snips dataset, it seems that the inclusiveness of all the methodsexcept
DeepState are similar to random selection. This may be
due to the small size of the Snips data set and the short sentencelength. Meanwhile, input sequence lengths in the Snips datasetare inconsistent, which may cause some methods that calculate
coverage based on the output of the RNN hidden layer to be valid.
5.3 Answer to RQ3: Guidance
Table 5 shows the accuracy of all models after retraining with the
data selected by all methods. Each grid represents the accuracy
score of the model on the mixed test set afterretraining. The data
inparenthesesrepresentstheaccuracyimprovementonthetestsetcomparedtotheoriginalmodel.Additionally,wecalculatetheaccu-racyimprovementafterretrainingwithallthedatainthecandidate
set as reference results. As can be seen from Table 5, in all models
anddata sets,compared torandomly selectingdata forretraining,
thedataselectedby DeepState cangreatlyimprovetheaccuracy
of the RNN models. In most combinations of models and data sets,
the accuracy improvement of DeepState â€™s selection method ex-
ceeds 50% of selecting all the data for retraining. We can safely
concludethattherobustnessofRNNcanbegreatlyimprovedby
retrainingwithasmallamountofdataselectedthrough DeepState .
The effectof DeepState on MNIST-BLSTMis slightly worsethan
HSCov (CAM). This is because the bidirectional neural network
hasmoreneuronsthantheotherRNNmodelsandrequiresmore
605
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 07:56:15 UTC from IEEE Xplore.  Restrictions apply. DeepState: Selecting Test Suites to Enhance the Robustness of Recurrent Neural Networks ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
0 5 10 15 20 25 30 35 40
Selection Rate0102030405060InclusivenessRandom
DeepState
RNNTest-HSCov(CAM)
DeepStellar-BSCov(CTM)
DeepStellar-BTCov(CTM)
testRNN-SC(CTM)
testRNN-SC(CAM)
NC(CTM)
NC(CAM)
(a) MNIST-LSTM0 5 10 15 20 25 30 35 40
Selection Rate01020304050InclusivenessRandom
DeepState
RNNTest-HSCov(CAM)
DeepStellar-BSCov(CTM)
DeepStellar-BTCov(CTM)
testRNN-SC(CTM)
testRNN-SC(CAM)
NC(CTM)
NC(CAM)
(b) Fashion-LSTM0 5 10 15 20 25 30 35 40
Selection Rate0102030405060InclusivenessRandom
DeepState
RNNTest-HSCov(CAM)
DeepStellar-BSCov(CTM)
DeepStellar-BTCov(CTM)
testRNN-SC(CTM)
testRNN-SC(CAM)
NC(CTM)
NC(CAM)
(c) Snips-BiLSTM0 5 10 15 20 25 30 35 40
Selection Rate010203040506070InclusivenessRandom
DeepState
RNNTest-HSCov(CAM)
DeepStellar-BSCov(CTM)
DeepStellar-BTCov(CTM)
testRNN-SC(CTM)
testRNN-SC(CAM)
NC(CTM)
NC(CAM)
(d) AgNews-LSTM
0 5 10 15 20 25 30 35 40
Selection Rate01020304050InclusivenessRandom
DeepState
RNNTest-HSCov(CAM)
DeepStellar-BSCov(CTM)
DeepStellar-BTCov(CTM)
testRNN-SC(CTM)
testRNN-SC(CAM)
NC(CTM)
NC(CAM)
(e) MNIST-BiLSTM0 5 10 15 20 25 30 35 40
Selection Rate01020304050InclusivenessRandom
DeepState
RNNTest-HSCov(CAM)
DeepStellar-BSCov(CTM)
DeepStellar-BTCov(CTM)
testRNN-SC(CTM)
testRNN-SC(CAM)
NC(CTM)
NC(CAM)
(f) Fashion-GRU0 5 10 15 20 25 30 35 40
Selection Rate0102030405060InclusivenessRandom
DeepState
RNNTest-HSCov(CAM)
DeepStellar-BSCov(CTM)
DeepStellar-BTCov(CTM)
testRNN-SC(CTM)
testRNN-SC(CAM)
NC(CTM)
NC(CAM)
(g) Snips-GRU0 5 10 15 20 25 30 35 40
Selection Rate0102030405060InclusivenessRandom
DeepState
RNNTest-HSCov(CAM)
DeepStellar-BSCov(CTM)
DeepStellar-BTCov(CTM)
testRNN-SC(CTM)
testRNN-SC(CAM)
NC(CTM)
NC(CAM)
(h) AgNews-BiLSTM
Figure 4: The inclusiveness of different selection methods.
testcasestomaximizetheHSCovcoverage.Theexperimentresults
showthatastheproportionofdataselectionincreases,theeffectof
DeepState cangradually outperform theHSCov (CAM)selection
method. Besides, although the retraining effect of SC (CTM) onSnips-BLSTM is slightly higher than that of
DeepState , it is less
effectivethan DeepState onothermodels,whichindicatesthatthe
strategy of SC (CTM) is not stable and effective.
For the text classification tasks, the accuracy of the original
model on the augmented test set is relatively high, and thus theaccuracy improvement after retraining is also limited compared
withtheimageclassificationmodels.Theaccuracyimprovement
of retraining with all candidate data may be less than selectingpart of the data for retraining. This may be because there are anamount of original data in the candidate set, which leads to the
model not learning the features of the augmented data well during
retraining. In addition, the features of text augmentation are not
as obvious as image transformation, which may affect the learning
and optimization of RNN models.
1 4 8 12 16 20
Selection rate0.050.100.150.200.25Acc Imp (aug)Random
DeepState
RNNTest-HSCov(CAM)
DeepStellar-BSCov(CTM)
DeepStellar-BTCov(CTM)
testRNN-SC(CTM)
testRNN-SC(CAM)
NC(CTM)
NC(CAM)
(a) The augmented test set.1 4 8 12 16 20
Selection rate0.020.040.060.080.100.12Acc Imp (aug)Random
DeepState
RNNTest-HSCov(CAM)
DeepStellar-BSCov(CTM)
DeepStellar-BTCov(CTM)
testRNN-SC(CTM)
testRNN-SC(CAM)
NC(CTM)
NC(CAM)
(b) The mix test set.
Figure5:TheMNIST-LSTMâ€™saccuracyimprovementonaug-mented and mixed test set with different selection rate.
In our experiments, we evaluated the accuracy improvement af-
ter retraining under the selection ratio from 1% to 20%. Since most
modelshave thesame trendofaccuracy afterretraining, weshow
theaccuracyofonemodel(MNIST-LSTM)underdifferentselectionratios,asdepictedinFig.5.Todemonstratetheimprovementsin
theaccuracyarenotinfluencedbyinconsistenciesinthedata,we
evaluated the retrained model on both the augmented test set and
themixtestset.Theaccuracyimprovementsontheaugmentedtest
set are greater than it on the mix test set. The accuracy improve-
mentsonthemixedtestsethaveshownthattheimprovementof
the retrained RNN model does not result from data inconsistency.
6 DISCUSSION
This section discusses the comparison with existing NC-guided
test selection techniques, application scenarios of DeepState , and
threats to validity.
6.1 Comparison with NC-guided Methods
By comparing DeepState with other existing test case selection
methodsbasedoncoveragecriterion,wedemonstratethat DeepState
is more effective than other existing methods. The selected testshavethecapabilityofdetectingbugsandcanbeemployedtoen-hance the robustness of the RNN models. For a given test case,
DeepState evaluates the uncertainty degree of the RNN model
basedonthechangesofthehiddenstateâ€™soutputovertimesteps.
Comparedwithothercoveragecriteriathatjudgetheneuronâ€™sacti-
vationbasedonthevalueofthehiddenneuronâ€™soutput, DeepState
can better capture the state transition of RNN based on time steps.
Further,differentfromthebaselinemethodsapplyingCTMorCAM
for test case selection, the selection algorithm of DeepState incor-
porates the advantage of both CTM and CAM. We evaluate the
uncertaintyofRNNforagiventestcasebasedonthechangingrate,
the process of sorting tests based on the changing rate is similar to
CTM. Meanwhile, we select based on the similarity of their chang-
ing trends for cases with the same changing rate. This method can
beregardedasCAMâ€™sselectionstrategy,whichaimsatselecting
the cases that trigger different behaviors of the RNN. The selection
strategy of DeepState makes the selected test set more diverse,
and we conjecture this fundamentally benefits its performance.
606
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 07:56:15 UTC from IEEE Xplore.  Restrictions apply. ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA Zixi Liu, Yang Feng, Yining Yin, and Zhenyu Chen
Table 5: The RNNsâ€™ accuracy score on mix test data set after retraining with 15% selected tests.
MethodDataset MNIST Fashion Snips AgNews
LSTM BLSTM LSTM GRU BLSTM GRU LSTM BLSTM
Random 76.25 (+9.86%) 74.76 (+7.84%) 75.26 (+4.29%) 75.20 (+3.75%) 90.15 (+2.22%) 89.25 (+0.06%) 80.05 (+0.17%) 81.66 (+0.28%)
HSCov (CAM) 74.78 (+8.39%) 78.02 (+11.09%) 75.73 (+4.77%) 75.08 (+3.62%) 88.99 (+1.06%) 89.45 (0.26%) 79.60 (-0.28%) 81.70 (+0.32%)
BSCov (CTM) 69.21 (+2.82%) 68.88 (+1.95%) 73.46 (+2.50%) 73.17 (+1.72%) 90.01 (+2.08%) 89.39 (+0.20%) 79.27 (-0.61%) 81.16 (-0.22%)
BTCov (CTM) 71.65 (+5.26%) 68.90 (+1.97%) 72.72 (+1.76%) 74.08 (+2.63%) 89.25 (+1.32%) 89.31 (+0.12%) 79.35 (-0.53%) 81.60 (+0.22%)
SC (CTM) 71.53 (+5.13%) 72.17 (+5.25%) 72.61 (+1.64%) 73.47 (+2.02%) 90.39 (+2.46%) 88.67 (-0.52%) 79.35 (-0.53%) 81.41 (+0.04%)
SC (CAM) 75.35 (+8.96%) 75.76 (+8.83%) 75.13 (+4.16%) 75.50 (+4.05%) 89.05 (+1.12%) 88.71 (-0.48%) 79.90 (+0.03%) 81.64 (+0.26%)
NC (CTM) 71.93 (+5.54%) 74.79 (+7.86%) 74.42 (+3.46%) 72.86 (+1.41%) 89.83 (+1.90%) 88.27 (-0.92%) 80.13 (+0.26%) 81.23 (-0.14%)
NC (CAM) 75.96 (+9.57%) 75.58 (+8.65%) 74.69 (+3.73%) 73.62 (+2.17%) 89.77 (+1.84%) 89.43 (+0.24%) 80.14 (+0.26%) 81.15 (-0.22%)
DeepState 78.58 (+12.19%) 77.40 (+10.47%) 76.90 (+5.94%) 76.80 (+5.35%) 90.03 (+2.10%) 90.05 (+0.86%) 80.24 (+0.37%) 81.79 (+0.41%)
100% tests 85.54 (+19.15%) 85.42 (+18.49%) 80.13 (+9.17%) 79.78 (+8.33%) 89.87 (+1.94%) 90.01 (+0.82%) 80.82 (+0.95%) 80.44 (+1.28%)
6.2 Application Scenarios
RNNisakindoffeedbackneuralnetworkthatspecializesinprocess-
ing time-seriesdata. It isdifferent from conventionalfeedforward
neuralnetworks,andmanyexistinganalysiscriteriafortraditional
neuralnetworksarenotsuitableforRNNs. DeepState canbeap-
plied to select tests from massive unlabeled data collected from the
usagescenarios.Itcanautomaticallyandefficientlyidentifytests
thathaveahighprobabilityoftriggeringtheincorrectbehaviorsof
RNN-driven systems from plenty of unlabelled data and thus help
reducethecostofmanuallylabeling.Further,theexperimentresults
also show that the accuracy of the model can be improved after
retrainingwiththeselecteddata.Applyingsmall-scaleselecteddata
forretrainingthemodelcanalsoenhancetheefficiencyofmodel
optimization and save computational resources. DeepState can be
potentially applied to detect malicious attack samples against RNN
models, such as backdoor attack data. It is capable of analyzing
thestatesâ€™changingtrendsofRNNafterreceivingdifferenttests,
because attack samples and ordinary samples are likely to cause
significant changes in RNN models. We will explore the related
study in our future work.
6.3 Threats to Validity
Testsubjectselection. TheselectionofdatasetsandRNNmodels
selection is one of the primary threats to validity. There are many
variantsoftheRNNmodel,includingLSTMandmultiplestructures,
and the corresponding effects can be different. On the other hand,
thetrainingoftheRNNsreliesonthedataset,andthequalityofthe
datamayhaveaninfluenceonthemodelâ€™saccuracy.Wealleviate
thisthreatbyemployingfourcommonlyuseddatasets,including
image and text data types. Further, for each studied dataset, we
employed two RNN models with different numbers of neurons and
architecture to evaluate the performance of DeepState.
Parameters settings. Anotherthreatcouldbe theparameterset-
tingsin neuroncoverages.To comparewithother coverage-based
testsetselectionmethods,wereproducedotherexistingcoverage
methodsforRNN,whichmayincludeparameters.Withfine-tuning
the parameters settings, the selected data could be different. To al-
leviatethepotentialbias,wefollowtheauthorsâ€™suggestedsettings
or employ the default settings of the original papers.
Test data simulation. Thelastthreattovaliditycomesfromthe
augmented test input generation. Although the augmented op-erators are common data noises in the actual environment, it isimpossible to guarantee that the distribution of the real unseeninput is the same as our simulation. To ensure the reliability of
the augmented data, we refer to the existing image and text trans-
formationmethodsbasedonsomeopen-sourcetools.Webelieve
that only some minor fine-tuning or adjustments are needed to
supplement and simulate other transformations.
7 RELATED WORK
Thissectiondiscussestherelatedworkintwogroups:(1)testselec-
tion methods for conventional software and (2) testing techniques
for RNN models.
7.1 Test Selection
The test selection technique aims to find the test suite so that
software testerscan getthe most benefitswithin limitedresource
budgets. The test selection technique was first proposed in conven-
tionalregressiontesting[ 18,47,48,65],whichaimsatimproving
the testing efficiency of a modified software system.
Rothermel et al. [ 47] outlined the issues related to regression
testing selection and proposed a series of metrics for evaluating
regression testing selection techniques. Based on the relevant met-
rics,wemadesomefine-tuningontheinclusivenesstoadaptfor
the testing of deep learning models. Rothermel et al. [ 48] proposed
a safe test selection algorithm that constructs control flow graphs
for the program and its modified versions. Then, the tests are se-
lectedbasedonthesegraphsandappliedtoexecutethemodifiedcodefromtheoriginaltestsuite.HyRTS[
65]wasthefirsthybrid
regression test selection technique which performs analysis onmultiple granularities to combine the advantages of traditionalselection techniques with different granularities. Different from
theapproachesmentionedabove, DeepState â€™stestcaseselection
technique is mainly aimed at testing RNNs, rather than traditional
software testing based on data flow and control flow.
InthefieldofDNNtesting,activelearning[ 46],asaspecialcase
ofmachinelearning,caninteractivelyquerytheinformationsource
tolabelnewdatapointsfortrainingDNNmodels[ 52].Themost
commonly used query framework is uncertainty sampling [ 30],
which aims to select unlabeled examples that the DNN finds hard-
est to classify. These uncertainty degree can be calculated basedon conditional random fields [
28], margin [ 26], and entropy [ 22].
Different from the active learning methods, DeepState is designed
607
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 07:56:15 UTC from IEEE Xplore.  Restrictions apply. DeepState: Selecting Test Suites to Enhance the Robustness of Recurrent Neural Networks ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
toidentifyasubsetoftestdatawiththehighestprobabilityofre-
vealing bugs in pre-trained RNN models. It is effective in testing
and optimization scenarios, different from active learning focusing
on the training process. Meanwhile, while most active learning
techniques leveragethe outputof DNNmodels, such asboundary
andposteriorprobability,asguidanceforselectingdata, DeepState
employsstructuralcharacteristicsofRNNmodelsandcapturestheir
internal state to reach the goal.
7.2 Testing Recurrent Neural Networks
For the quality assurance tasks of RNN-driven software systems,
some coverage-based testing approaches have been proposed in
recentyears.DeepStellar[ 15]adaptsfivecoveragecriteriaofDeep-
Gauge [34] to test and analyze RNN models, which is first trans-
formed into a Discrete-Time Markov Chain (DTMC) [ 40]a sa n
abstraction. TestRNN [ 24,25] proposes a series of neuron cover-
age metrics of the LSTM network and develops a coverage-guided
fuzzing approach for deep learning models with LSTM network
structure. Then, some random mutation enhanced with the cover-
age is designed to generate test cases. RNN-Test [ 19] defines the
hidden state coverage as the ratio of the hidden states that achieve
the maximum value of all the hidden states during testing.
Differentfromthesecoverage-guidedadversarialexamplegen-
eration methods, DeepState explores another solution to guide
the test cases selection. DeepState analyzes the changing rate and
changing trend of the hidden states in RNN models, and selects
tests effectively among the unlabeled large-scale data sets. On the
other hand, we focus on selecting test suites from massive dataset for testing and optimizing the RNN models. Thus, the crite-
rion of DeepState relies on the relationship among different cases
ratherthanevaluatingforasingletest.Ourstudydemonstratedthat
DeepState couldmoreeffectivelyselectdatasetforRNNtesting
and optimization, and the oracle problem is alleviated.
8 CONCLUSION
In this paper, we propose DeepState for selecting massive test
suitestoenhancetherobustnessofRNNs.BasedonastatisticalviewofRNN,weexpandtheoutputoftheRNNmodelaccordingtotime
steps and analyze the uncertainty degree of the RNN model as the
time step changes. We design and implement a test suite selection
tool DeepState by calculating the changing rate and changing
trend of the RNN output sequence over time. The experimental
resultsdemonstrate that DeepState caneffectively helptestersto
choose test cases with a high ability to detect bugs and improve
thequalityoftheRNNmodels. DeepState caneffectivelyensure
that the test data set has a high bug detection rate while greatly
reducingthecostofdatalabelingandimprovingtheefficiencyof
RNN model testing and optimization.
ACKNOWLEDGEMENT
Wewouldliketothankanonymousreviewersfortheirinsightful
andconstructivecomments.Thisprojectwaspartiallyfundedby
the National Natural Science Foundation of China under Grant
Nos. 62002158, 61832009, and 61932012, andthe Science, Technol-
ogy, and Innovation Commission of Shenzhen Municipality (No.
CJGJZD20200617103001003).REFERENCES
[1][n.d.]. AGâ€™s corpus of news articles. http://groups.di.unipi.it/~gulli/AG_corpus_
of_news_articles.html. (Accessed on 08/21/2021).
[2][n.d.]. AmazonpromisesfixforcreepyAlexalaugh-BBCNews. https://www.
bbc.com/news/technology-43325230. (Accessed on 08/23/2021).
[3][n.d.]. AmazonpromisesfixforcreepyAlexalaugh-BBCNews. https://www.
bbc.com/news/technology-43325230. (Accessed on 08/29/2021).
[4][n.d.]. Python Release Python 3.6.0 | Python.org. https://www.python.org/
downloads/release/python-360/. (Accessed on 07/10/2021).
[5] [n.d.]. TensorFlow. https://www.tensorflow.org/. (Accessed on 08/23/2021).
[6]SaadAlbawi,TareqAbedMohammed,andSaadAl-Zawi.2017. Understandingof
a convolutional neural network. In 2017 International Conference on Engineering
and Technology (ICET). 1â€“6. https://doi.org/10.1109/ICEngTechnol.2017.8308186
[7]Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2014. Neural ma-chine translation by jointly learning to align and translate. arXiv preprint
arXiv:1409.0473 (2014).
[8]Qian Chen, Xiaodan Zhu, Zhenhua Ling, Si Wei, Hui Jiang, and DianaInkpen. 2016. Enhanced lstm for natural language inference. arXiv preprint
arXiv:1609.06038 (2016).
[9] FranÃ§ois Chollet et al. 2015. Keras. https://keras.io.
[10]JunyoungChung,CaglarGulcehre,KyungHyunCho,andYoshuaBengio.2014.
Empirical evaluation of gated recurrent neural networks on sequence modeling.
arXiv preprint arXiv:1412.3555 (2014).
[11]Alice Coucke, Alaa Saade, Adrien Ball, ThÃ©odore Bluche, Alexandre Caulier,
David Leroy, ClÃ©ment Doumouro, Thibault Gisselbrecht, Francesco Caltagirone,
Thibaut Lavril, et al .2018. Snips voice platform: an embedded spoken language
understanding system for private-by-design voice interfaces. arXiv preprint
arXiv:1805.10190 (2018).
[12]Jan Deriu, Alvaro Rodrigo, Arantxa Otegi, Guillermo Echegoyen, Sophie Rosset,
Eneko Agirre, and Mark Cieliebak. 2020. Survey on evaluation methods for
dialoguesystems. ArtificialIntelligenceReview (2020),1â€“56. https://doi.org/10.
1007/s10462-020-09866-x
[13]JacobDevlin,Ming-WeiChang,KentonLee,andKristinaToutanova.2018. BERT:
Pre-trainingofDeepBidirectionalTransformersforLanguageUnderstanding.
arXiv preprint arXiv:1810.04805 (2018).
[14]Rahul Dey and Fathi M. Salem. 2017. Gate-variants of Gated Recurrent Unit
(GRU)neuralnetworks.In 2017IEEE60thInternationalMidwestSymposiumon
Circuits and Systems (MWSCAS). 1597â€“1600. https://doi.org/10.1109/MWSCAS.2017.8053243
[15]
Xiaoning Du, Xiaofei Xie, Yi Li, Lei Ma, Yang Liu, and Jianjun Zhao. 2019. Deep-
Stellar:Model-BasedQuantitativeAnalysisofStatefulDeepLearningSystems.InProceedingsofthe201927thACMJointMeetingonEuropeanSoftwareEngineering
Conferenceand SymposiumontheFoundationsof SoftwareEngineering (Tallinn,
Estonia)(ESEC/FSE 2019). Association for Computing Machinery, New York, NY,
USA, 477â€“487. https://doi.org/10.1145/3338906.3338954
[16]Yang Feng, Qingkai Shi, Xinyu Gao, Jun Wan, Chunrong Fang, and Zhenyu
Chen. 2020. DeepGini: Prioritizing Massive Tests to Enhance the Robustness of
Deep Neural Networks. In Proceedings of the 29th ACM SIGSOFT International
SymposiumonSoftwareTestingandAnalysis (VirtualEvent,USA) (ISSTA2020).
Association for Computing Machinery, New York, NY, USA, 177â€“188. https:
//doi.org/10.1145/3395363.3397357
[17]A. Graves and J. Schmidhuber. 2005. Framewise phoneme classification with
bidirectional LSTM networks. In Proceedings. 2005 IEEE International Joint Con-
ference on Neural Networks, 2005., Vol. 4. 2047â€“2052 vol. 4. https://doi.org/10.
1109/IJCNN.2005.1556215
[18]Todd L. Graves, Mary Jean Harrold, Jung-Min Kim, Adam Porter, and Gregg
Rothermel.2001. AnEmpiricalStudyofRegressionTestSelectionTechniques.
ACM Trans. Softw. Eng. Methodol. 10, 2 (April 2001), 184â€“208. https://doi.org/10.
1145/367008.367020
[19]JianminGuo,YueZhao,XueyingHan,YuJiang,andJiaguangSun.2019. Rnn-
test: Adversarial testing framework for recurrent neural network systems. arXiv
preprint arXiv:1911.06155 (2019).
[20]FabriceHarel-Canada,LingxiaoWang,MuhammadAliGulzar,QuanquanGu,
and Miryung Kim. 2020. Is Neuron Coverage a Meaningful Measure for Testing
DeepNeuralNetworks?.In Proceedingsofthe28thACMJointMeetingonEuropean
SoftwareEngineeringConferenceandSymposiumontheFoundationsofSoftware
Engineering (VirtualEvent,USA) (ESEC/FSE2020) .AssociationforComputingMa-
chinery, New York, NY, USA, 851â€“862. https://doi.org/10.1145/3368089.3409754
[21]Sepp Hochreiter and JÃ¼rgen Schmidhuber. 1997. Long Short-Term Memory.
Neural Computation 9, 8 (1997), 1735â€“1780. https://doi.org/10.1162/neco.1997.9.
8.1735
[22]Alex Holub, Pietro Perona, and Michael C Burl. 2008. Entropy-based activelearning for object recognition. In 2008 IEEE Computer Society Conference on
Computer Vision and Pattern Recognition Workshops. IEEE, 1â€“8.
[23]Qiang Hu, Lei Ma, Xiaofei Xie, Bing Yu, Yang Liu, and Jianjun Zhao. 2019. Deep-
Mutation++: A Mutation Testing Framework for Deep Learning Systems. In 2019
34thIEEE/ACMInternationalConferenceonAutomatedSoftwareEngineering(ASE).
608
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 07:56:15 UTC from IEEE Xplore.  Restrictions apply. ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA Zixi Liu, Yang Feng, Yining Yin, and Zhenyu Chen
1158â€“1161. https://doi.org/10.1109/ASE.2019.00126
[24]Wei Huang, Youcheng Sun, Xiaowei Huang, and James Sharp. 2019. testrnn:
Coverage-guided testing on recurrent neural networks. arXiv preprint
arXiv:1906.08557 (2019).
[25]Wei Huang, Youcheng Sun, XingyuZhao,James Sharp, Wenjie Ruan, Jie Meng,
andXiaoweiHuang.2021. Coverage-GuidedTestingforRecurrentNeuralNet-
works.IEEE Transactions on Reliability (2021), 1â€“16. https://doi.org/10.1109/TR.
2021.3080664
[26]Ajay J Joshi, Fatih Porikli, and Nikolaos Papanikolopoulos. 2009. Multi-classactive learning for image classification. In 2009 IEEE Conference on Computer
Vision and Pattern Recognition. IEEE, 2372â€“2379.
[27]SosukeKobayashi.2018. Contextualaugmentation:Dataaugmentationbywords
with paradigmatic relations. arXiv preprint arXiv:1805.06201 (2018).
[28]John Lafferty, Andrew McCallum, and Fernando CN Pereira. 2001. Conditional
randomfields:Probabilisticmodelsforsegmentingandlabelingsequencedata.
(2001).
[29] YannLeCun,YoshuaBengio,andGeoffreyHinton.2015. Deeplearning. nature
521, 7553 (2015), 436â€“444.
[30]David D Lewis and William A Gale. 1994. A sequential algorithm for training
text classifiers. In SIGIRâ€™94. Springer, 3â€“12.
[31]Zenan Li, Xiaoxing Ma, Chang Xu, and Chun Cao. 2019. Structural CoverageCriteria for Neural Networks Could Be Misleading. In Proceedings of the 41st
InternationalConferenceonSoftwareEngineering:NewIdeasandEmergingResults
(Montreal, Quebec, Canada) (ICSE-NIER â€™19). IEEE Press, 89â€“92. https://doi.org/
10.1109/ICSE-NIER.2019.00031
[32]D.LuandQ.Weng.2007. ASurveyofImageClassificationMethodsandTech-
niquesforImproving ClassificationPerformance. Int.J.RemoteSens. 28,5(Jan.
2007), 823â€“870. https://doi.org/10.1080/01431160600746456
[33]Edward Ma. 2019. NLP Augmentation. https://github.com/makcedward/nlpaug.
[34]LeiMa,FelixJuefei-Xu,FuyuanZhang,JiyuanSun,MinhuiXue,BoLi,ChunyangChen,TingSu,LiLi,YangLiu,JianjunZhao,andYadongWang.2018. DeepGauge:
Multi-Granularity Testing Criteria for Deep Learning Systems. In Proceedings of
the33rdACM/IEEEInternationalConferenceonAutomatedSoftwareEngineering
(Montpellier, France) (ASE 2018). Association for Computing Machinery, New
York, NY, USA, 120â€“131. https://doi.org/10.1145/3238147.3238202
[35]Lei Ma, Fuyuan Zhang, Jiyuan Sun, Minhui Xue, Bo Li, Felix Juefei-Xu, Chao
Xie, Li Li, Yang Liu, Jianjun Zhao, et al .2018. Deepmutation: Mutation testing of
deeplearningsystems.In 2018IEEE29thInternationalSymposiumonSoftware
Reliability Engineering (ISSRE). IEEE, 100â€“111.
[36]PankajMalhotra,LovekeshVig,GautamShroff,andPuneetAgarwal.2015. Longshorttermmemorynetworksforanomalydetectionintimeseries.In Proceedings,
Vol. 89. Presses universitaires de Louvain, 89â€“94.
[37]TomÃ¡Å¡ Mikolov, Stefan Kombrink, LukÃ¡Å¡ Burget, Jan ÄŒernock `y, and Sanjeev
Khudanpur. 2011. Extensions of recurrent neural network language model.In2011 IEEE international conference on acoustics, speech and signal processing
(ICASSP). IEEE, 5528â€“5531.
[38]AgnieszkaMikoÅ‚ajczykandMichaÅ‚Grochowski.2018. Dataaugmentationfor
improvingdeeplearninginimageclassificationproblem.In 2018International
Interdisciplinary PhD Workshop (IIPhDW). 117â€“122. https://doi.org/10.1109/
IIPHDW.2018.8388338
[39]Suphakit Niwattanakul, Jatsada Singthongchai, Ekkachai Naenudorn, and Su-pachanun Wanapu. 2013. Using of Jaccard coefficient for keywords similarity.InProceedings of the international multiconference of engineers and computer
scientists, Vol. 1. 380â€“384.
[40]JamesRNorrisandJohnRobertNorris.1998. Markovchains. Number2.Cam-
bridge university press.
[41] Christopher Olah. 2015. Understanding lstm networks. (2015).[42]
TomOâ€™Malley,ElieBursztein,JamesLong,FranÃ§oisChollet,HaifengJin,Luca
Invernizzi, et al .2019. Keras Tuner. https://github.com/keras-team/keras-tuner.
[43] KexinPei,YinzhiCao,JunfengYang,andSumanJana.2019. DeepXplore:Auto-
mated Whitebox Testing of Deep Learning Systems. Commun. ACM 62, 11 (Oct.
2019), 137â€“145. https://doi.org/10.1145/3361566
[44]Philipp Petersen and Felix Voigtlaender. 2020. Equivalence of approximation by
convolutional neural networks and fully-connected networks. Proc. Amer. Math.
Soc.148, 4 (2020), 1567â€“1581. https://doi.org/10.1090/proc/14789
[45]Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang,Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2019. Exploring the lim-its of transfer learning with a unified text-to-text transformer. arXiv preprint
arXiv:1910.10683 (2019).
[46]Pengzhen Ren, Yun Xiao, Xiaojun Chang, Po-Yao Huang, Zhihui Li, Brij B Gupta,
Xiaojiang Chen, and Xin Wang. 2021. A survey of deep active learning. ACM
Computing Surveys (CSUR) 54, 9 (2021), 1â€“40.
[47]Gregg Rothermel and Mary Jean Harrold. 1996. Analyzing regression test selec-
tion techniques. IEEE Transactions on software engineering 22, 8 (1996), 529â€“551.
[48]Gregg Rothermel and Mary Jean Harrold. 1997. A Safe, Efficient Regression Test
Selection Technique. ACM Trans.Softw. Eng. Methodol. 6, 2 (April1997), 173â€“210.
https://doi.org/10.1145/248233.248262[49]TaraNSainath,OriolVinyals,AndrewSenior,andHaÅŸimSak.2015. Convolu-
tional,long short-termmemory,fully connecteddeepneuralnetworks. In 2015
IEEE international conference on acoustics, speech and signal processing (ICASSP).
IEEE, 4580â€“4584.
[50]AlexanderGSchwingandRaquelUrtasun.2015. Fullyconnecteddeepstructured
networks. arXiv preprint arXiv:1503.02351 (2015).
[51]Rico Sennrich, Barry Haddow, and Alexandra Birch. 2015. Improving neural ma-
chine translation models with monolingual data. arXiv preprint arXiv:1511.06709
(2015).
[52] Burr Settles. 2009. Active learning literature survey. (2009).[53]
AlexSherstinsky.2020. Fundamentalsofrecurrentneuralnetwork(RNN)and
long short-term memory (LSTM) network. Physica D: Nonlinear Phenomena 404
(2020), 132306.
[54]Connor Shorten and Taghi M Khoshgoftaar. 2019. A survey on image data
augmentation for deep learning. Journal of Big Data 6, 1 (2019), 1â€“48.
[55]DanielSvozil,VladimirKvasnicka,andJiriPospichal.1997. Introductiontomulti-
layer feed-forward neural networks. Chemometrics and intelligent laboratory
systems39, 1 (1997), 43â€“62.
[56]YuchiTian,KexinPei,SumanJana,andBaishakhiRay.2018.DeepTest:Automated
Testing of Deep-Neural-Network-Driven Autonomous Cars. In Proceedings of
the 40th International Conference on Software Engineering (Gothenburg, Sweden)
(ICSE â€™18). Association for Computing Machinery, New York, NY, USA, 303â€“314.
https://doi.org/10.1145/3180155.3180220
[57]Zan Wang, Hanmo You, Junjie Chen, Yingyi Zhang, Xuyuan Dong, and Wenbin
Zhang. 2021. Prioritizing Test Inputs for Deep Neural Networks via Mutation
Analysis.In 2021IEEE/ACM43rdInternationalConferenceonSoftwareEngineering
(ICSE). 397â€“409. https://doi.org/10.1109/ICSE43902.2021.00046
[58]Kashif Rasul & Han Xiao. 2017. Fashion. https://research.zalando.com/welcome/
mission/research-projects/fashion-mnist/.
[59]Wayne Xiong, Jasha Droppo, Xuedong Huang, Frank Seide, Mike Seltzer, An-
dreasStolcke,DongYu,andGeoffreyZweig.2016. Achievinghumanparityin
conversational speechr ecognition. arXiv preprint arXiv:1610.05256 (2016).
[60]Shenao Yan, Guanhong Tao, Xuwei Liu, Juan Zhai, Shiqing Ma, Lei Xu, and
Xiangyu Zhang. 2020. Correlations between Deep Neural Network Model
Coverage Criteria and Model Quality. In Proceedings of the 28th ACM Joint
Meeting on European Software Engineering Conference and Symposium on theFoundationsofSoftwareEngineering (VirtualEvent,USA) (ESEC/FSE2020).A s-
sociation for Computing Machinery, New York, NY, USA, 775â€“787. https:
//doi.org/10.1145/3368089.3409671
[61]Christopher J.C. Burges Yann LeCun, Corinna Cortes. 1998. MNIST. http://yann.
lecun.com/exdb/mnist/.
[62]S. Yoo and M. Harman. 2012. Regression Testing Minimization, Selection andPrioritization: A Survey. Softw. Test. Verif. Reliab. 22, 2 (March 2012), 67â€“120.
https://doi.org/10.1002/stv.430
[63]XiaoyongYuan,PanHe,QileZhu,andXiaolinLi.2019. AdversarialExamples:
Attacks and Defenses for Deep Learning. IEEE Transactions on Neural Networks
andLearningSystems 30,9(2019),2805â€“2824. https://doi.org/10.1109/TNNLS.
2018.2886017
[64]Wojciech Zaremba, Ilya Sutskever, and Oriol Vinyals. 2014. Recurrent neural
network regularization. arXiv preprint arXiv:1409.2329 (2014).
[65]Lingming Zhang. 2018. Hybrid Regression Test Selection. In 2018 IEEE/ACM
40th International Conference on Software Engineering (ICSE). 199â€“209. https:
//doi.org/10.1145/3180155.3180198
[66]XiangZhang,JunboZhao,andYannLeCun.2015. Character-LevelConvolutional
Networks for Text Classification. In Proceedings of the 28th International Con-
ference on Neural Information Processing Systems - Volume 1 (Montreal, Canada)
(NIPSâ€™15). MIT Press, Cambridge, MA, USA, 649â€“657.
609
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 07:56:15 UTC from IEEE Xplore.  Restrictions apply. 