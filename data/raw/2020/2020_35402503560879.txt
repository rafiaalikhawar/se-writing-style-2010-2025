Reflections on SoftwareFailureAnalysis
PaschalC.Amusuo
PurdueUniversity, USA
pamusuo@purdue.eduAishwarya Sharma
PurdueUniversity, USA
sharm234@purdue.eduSiddharth R.Rao
PurdueUniversity, USA
rao147@purdue.edu
AbbeyVincent
PurdueUniversity, USA
vincen17@purdue.eduJamesC.Davis
PurdueUniversity, USA
davisjam@purdue.edu
ABSTRACT
Failurestudiesareimportantinrevealingtherootcauses,behaviors,
andlifecycleofdefectsinsoftwaresystems.Thesestudieseither
focus on understanding the characteristics of defects in specific
classesofsystemsorthecharacteristicsofaspecifictypeofdefectin
the systems it manifests in. Failure studies have influenced various
software engineeringresearch directions, especially in the areaof
software evolution,defect detection,andprogramrepair.
In this paper, we reflect on the conduct of failure studies in soft-
wareengineering.Wereviewedasampleof52failurestudypapers.
We identifiedseveral recurringproblemsinthese studies,some of
which hinder the ability of the engineering community to trust
or replicate the results. Based on our findings, we suggest future
research directions, including identifying and analyzing failure
causalchains,standardizingtheconductoffailurestudies,andtool
support forfasterdefect analysis.
CCSCONCEPTS
·Software and its engineering →Software defect analysis .
KEYWORDS
Failure analysis,software defects,empiricalsoftware engineering
ACM ReferenceFormat:
PaschalC.Amusuo, AishwaryaSharma, SiddharthR. Rao, AbbeyVincent,
and James C. Davis. 2022. Reflections on Software Failure Analysis. In
Proceedingsofthe30thACMJointEuropeanSoftwareEngineeringConference
andSymposiumontheFoundationsofSoftwareEngineering(ESEC/FSE’22),
November 14ś18, 2022, Singapore, Singapore. ACM, New York, NY, USA,
6pages.https://doi.org/10.1145/3540250.3560879
1 INTRODUCTION
The study of failures is integral to the success of engineered sys-
tems[27].Insoftwareengineering,failurestudiesdescribethechar-
acteristics of defects in software systems. These studies, otherwise
knownasbugstudies,areeithertailoredtowardunderstandingthe
characteristics of defects in specific classes of systems ( e.g.,web
systems[ 5],Androidapps[ 17],orembeddedsystems[ 19])orthe
characteristics of specific classes of defects ( e.g.,performance [ 15],
ESEC/FSE ’22,November 14ś18, 2022, Singapore, Singapore
©2022 Copyright held bytheowner/author(s).
ACM ISBN978-1-4503-9413-0/22/11.
https://doi.org/10.1145/3540250.3560879Figure1:Thedistributionoffailurestudiesbyyearpublished.
concurrency [ 7], or security [ 20]). These studies are designed to
reveal the root causes of these defects, their manifestation, impact,
fix characteristics,andlife-cycle.
Overthelastdecade,thenumberoffailurestudieshassteadilyin-
creased(Figure 1).Thesestudieshaveinfluencedresearchintosoft-
ware testing[ 12],defect detection [ 6],andrepairtechniques [ 24].
In thispaper, we reflecton the conduct of software failure anal-
ysisresearchoverthelast20years.Usingasystematicliterature
review,weidentifiedseveralflawsandchallengesthataffectthis
research direction. Following the flaws and challenges we iden-
tified, we discussed future research directions that the software
engineeringcommunitycanembarkon,toaidtheconductofthese
failurestudies. Ourresearch directionsarefocusedonattempting
toanswervariousquestionsrelevanttotheefficientconductand
impact offailure studies.
2 IDEALIZEDFAILURE STUDYMODEL
Failurestudiesareresearchfocusedonunderstandingthecharac-
teristicsandcausesoffailuresinengineeredsystems[ 16][39].In
software engineering,these studies commonly considerdefects.
Thissectionpresentsanidealizedmodelofthefailurestudypro-
cess in software engineering. We derived this model by reviewing
stepscurrentlytakentoconductsoftwarefailurestudies,comple-
mentedwithfailurestudiesconductedinotherengineeringdisci-
plines[8].Weusedthismodeltoanalyzeandreviewvariousfailure
studies reportedin the software engineeringliterature.
Figure2shows the various stages of this idealized model, which
is applied across engineering disciplines. First, the project scope is
defined.Thisusuallyinvolvesidentifyingwhatclassofdefectsto
Thiswork islicensedunderaCreativeCommonsAttribution4.0Interna-
tional License.
1615
ESEC/FSE ’22, November14ś18, 2022,Singapore, Singapore PaschalC.Amusuo, AishwaryaSharma,SiddharthR. Rao,Abbey Vincent, andJames C.Davis
1. Define ProblemScope
- Understudiedclassesofsystems
- Bias towardsopen-source
software systems
2. CollectDefectReports&
Supplementary Data
- Difficultyidentifying target
defectreports
3. Analyze Bug Characteristics
- Inconsistenttaxonomies &
term definitons
- Absenceof quality measures
4. PerformRoot Cause Analysis
- Root causes are arbitrarilydefined
- Absenceof causal-chain
data to analyze
5. Report Results
- Missingreplication data
6. Impact& Recommendations
forIndustry
- Not tailoredtowards
improvingengineeringpractice
Figure 2:Idealized modelofsoftware engineeringfailure studythat our studyidentified flaws in.
study,thesystemtostudy,andhowthetargetdefectsandsystem
wouldbeidentified.Thenthedefectreportsandotherrelevantdata
are collectedandreviewed.Theinvestigatorsusetheinformation
extracted to analyze the characteristics of the various defects, such
as how they manifest, their impact, their life cycle, etc. In addition
to this, the investigators can also perform a root cause analysis
to determine the probable root cause and contributing causes of
the defects. Once the study is completed, investigators report their
results and discuss their implications. This report should also con-
taintheiranalyzeddatatoaidreplicabilitybyotherinvestigators.
Toensurethatpractitionerslearnfromtheresultsofthestudy,it
behoovestheinvestigatorstoproviderecommendationstothese
practitioners while also working with them to validate the impact
oftheirresults andrecommendations.
The figure also depicts common shortcomings of the existing
studies in software engineering literature at various stages. We
discuss theseshortcomingsinthe nextsection.
3 FLAWSINFAILURE STUDYMETHODS
This section presents the flaws we identified in this research direc-
tion,as practicedinsoftware engineering.
3.1 Methodology
We first searched the proceedings of prominent software engineer-
ing conferences (ICSE, ESEC/FSE, ASE) and journals (IEEE TSE,
ESEM,JSS)andmanuallyidentifiedfailurestudypapers.Theresults
helped us define our search phrase.1We used this phrase to search
scholarly databases (Google Scholar, IEEE Xplore, ACM Digital Li-
brary). This search yielded 92 candidate papers. Working in teams
of 2, we manually reviewed the abstract of these papers, identified
and selected 52 papers that studied and characterized defects in
software,andwere publishedinpeer reviewvenues.
Wereviewedtheselectedpapersandcollecteddatarelatedtothe
various stages outlined in Figure 2. We analyzed the data extracted
andidentifiedthe flawsdiscussedinthe nextsubsection.
To ensure the quality of our results, we had multiple authors
independently perform data extraction on a sample of 20 papers.
WecomputedtheCohenkappascoreonthissampleas0.763,which
1Our final search query was "(empirical OR comprehensive OR taxonomy OR char-
acteristics)AND(bugORbugsORfaultsORdefectsORfailuresORvulnerabilities)
AND(studyOR review)"showssubstantialagreement[ 13].Subsequently,theauthorscontin-
ued the data extraction independently while one more experienced
authorreviewedthe data extractedbythe otherauthors.
Threat tovalidity :Wesampledonly52 failurestudies, which
maynothaveincludedallrelevantfailurestudies.Butwebelieve
this sample is representative, and our findings are valid and rele-
vant.Thesamplewasselectedthroughamethodologicalprocess,as
discussedabove.Wealsoincludedrecentpaperspublishedinpromi-
nent venues to ensure our findings were relevant to the current
peer-reviewedconduct.Also,eachoftheflawsweidentifiedwas
prevalentinoverhalfofthesampleofpapersstudied.Finally,while
some of the flaws identified may seem obvious, we are the first
topresentempiricalevidenceoftheirexistencewhilesuggesting
researchdirections to manage them.
3.2 Recurring Flaws
3.2.1 BiastowardsOpen-sourceSoftware: Investigatorsconduct-
ingfailurestudiesarebiasedtowardstudyingdefectsinopen-source
software (first row of Table 1). This is usually because open-source
software has publicly available code, documentation, and complete
evolutionhistory.Unfortunately,focusingononlyopen-sourcesoft-
waremaybeinconsistentwiththeinvestigator’sgoal,ultimately
aidingsoftware engineeringpracticebeyondopen-source.
Priorresearchhasinvestigatedandreporteddifferencesbetween
open-source and commercial software [ 22] [26] [3]. Mockus et
al.[22]showedthatthepost-releasedefectdensityforApachewas
significantly different compared to 4 commercial projects. Paul-
sonet al.[26] reported that more defects are being found and fixed
in open-sourcesoftware, whichmay havecontributed tothe high
defect density reported in [ 22]. Boulanger [ 3] identified differences
betweenthesoftwaredevelopmentpracticesfor open-sourceand
commercial software projects. In open-source software, defects are
usuallyreportedbycustomers,unlikeincommercialsoftware.This
could also affect the kinds of defects analyzed by failure studies.
As a result, the results from these failure studies that studied open-
sourcesoftware maynot generalize to commercialenvironments.
3.2.2 Root Causes are Subjectively Identified: Root cause analysis
is the most common aspect of defects considered by failure studies
(Figure3).However,onlyonepaper[ 19]reportedusingarootcause
analysismethodologytoidentifytheserootcauses.Accordingto
Paradiesetal.[25],rootcausesshouldbebasiccausesthatarewithin
1616Reflections onSoftware Failure Analysis ESEC/FSE ’22, November14ś18, 2022,Singapore, Singapore
Table 1:Table showing furtherfailure studyanalysis.
Analysis Yes No
Papers thatstudied defects inproprietary software 3 49
Papers thatreusedtaxonomiesfromliterature 10 42
Papers thatreported theuse of anytool 12 40
Papers thatmade practitioner-relevantcontributions 14 38
Figure 3: Research questions investigated by failure studies.
theambitofmanagementto fix.Gangidi[ 9]alsoexplainedthata
systematicrootcauseanalysismethodologyshouldrevealdeeper
systemiccauses ( e.g.,policies, practices,managementdecisions).
The root causes identified by the failure studies we reviewed
mostlyrepresenttechnicalflawsanddonotcorrespondwithany
ofthesedefinitions.Wang[ 40]identifiedrootcausessuchasmis-
use of mathematical formulas, inconsistency between hardware
andsoftware,andimproperhandlingofparameters.Whilethese
aretheimmediatecausesofthereporteddefects,theyareneither
‘basic’norsystemic.Deeperinvestigationsintodefectscausedby
hardware/software inconsistency may reveal underlying causes
such as poor documentation, which may also have been attributed
totheabsenceofdocumentationguidelines.Asanotherexample,
Gunawiet al.[10] identified data races as one of the root causes
ofdatainconsistencyincloudsystems,butdeeperanalysismight
havealsorevealedotherunderlyingfactorsthatledtothesedata
races.Ifpapersconductedadeeperrootcauseanalysis,theirresults
could be more helpfulto practitioners andengineeringteams.
3.2.3 InconsistentDefectTaxonomies: Failurestudiesattemptto
characterizethedefectsinsoftwaresystemstoaidtheiranalysis.
Ourresults,asshowninthesecondrowofTable 1,showthatmost
failurestudiesinventthetaxonomiestheyuseforthischaracteriza-
tion, even when they study the same class of defects. For example,
Caoet al.[4] characterized performance bugs in deep learning
systemsusingaself-generatedtaxonomybutcouldhaveadapted
taxonomies from prior research on performance bugs [ 18] [21]
[41]. As a result, it becomes difficult to compare the distribution of
performance defectsin[ 4]andearlierworks such as [ 21].
We also found disagreement in the interpretation of terms in
thetaxonomywheninvestigatorschoosetoreusetaxonomiesfrom
Figure 4:Distribution offailure studiesby system type.
earlierstudies.Forexample,Tan etal.[38]reportedtheyreusedthe
taxonomydefinedbySullivan etal.[36]butacknowledgedthatthe
definitionof semanticbugs betweenthetwostudiesmaybedifferent,
accounting for the huge discrepancy between the percentage of
semantic bugsfoundbythe twopapers.
3.2.4 Non-integrationofPracticingSoftwareEngineersintheStudy:
Our review of failure study papers shows that practitioners are
not included during the conduct of these studies. Investigating
the perspectives of the software engineers who create or fix these
defects can be helpful in providing insights into the causes and
characteristics ofthesedefects.
Furthermore,failurestudy papersarefocusedonenablingsoft-
wareengineeringresearchbutfailtomakecontributionsthatare
relevant to software engineers. According to the fourth row of
Table1,only27%ofreviewedpapersproposedrecommendations
pertinenttocurrentsoftwareengineeringpractices.Mantyla[ 23]
provided guidelines for conducting code and documentation re-
views.Sun[ 37]maderecommendationsforgeneratingtestcases
forcompilers.Othersonlydiscussedtheresearchimplicationsof
theirwork.Thisiscontrarytofailurestudiesinotherdisciplines
whose results recommended changes in practitioners’ practices
[8] [27] [31] [32]. With an increased focus on improving engineer-
ingpractice,theresultsandrecommendationsfromthesestudies
could reducethe occurrence of defects, which would significantly
increasesoftware engineers’ productivity..
3.2.5 DefectsinEmbedded/IoTSystemsareUnderstudied: Fromour
results,weobservedthatthesoftwareengineeringcommunityis
biasedtowardsfailurestudiesonweb-basedanddesktop-basedsys-
tems,whileembedded/IoTsystemsarestillunderstudied.Asshown
in Figure 4, embedded/IoT systems accounted for onlytwo papers,
whileweb-basedsystems( e.g.,browsers)had16anddesktop-based
systems ( e.g.,compilers) had 12. Embedded systems power our
airplanes, vehicles, and industries and deserve additional attention.
3.2.6 MiscellaneousFlaws: Inadditiontotheprimaryflawsdis-
cussedabove,we summarize three more issues.
Inconsistent quality measures: Defect analysis is subjective, and
single-author investigationmethodsareuntrustworthy. Of the 52
papers reviewed, only 19 studies had multiple authors indepen-
dently analyze the data. Hence, the results of most studies are
untrustworthywithoutthe use of quality controlmeasures.
Absenceofreplicabilitydata:Only11papersincludedlinksto
theirreplication package; 3of thesewere inaccessible.
1617ESEC/FSE ’22, November14ś18, 2022,Singapore, Singapore PaschalC.Amusuo, AishwaryaSharma,SiddharthR. Rao,Abbey Vincent, andJames C.Davis
Missing tool support: Failure studies are time-consuming and
lacktoolsupport.Leesatapornwongsa etal.[14]andShen etal.[35]
reportedthatittookthem15and24monthstoconducttheirstudy.
Yet, according to the third row of Table 1, only 23% of failure stud-
ies reported using any tool in their study. These studies require
investigators to analyze and categorize hundreds of defect reports
manually.Whenstudyingaspecificclassofdefects,theseinvesti-
gatorsrelyononlykeywordmatchingtofilterprospectivedefect
reports and need to go through each filtered report to identify and
remove false positives. Mazuera-rozo et al.[21] identified 1,010
commits using keyword matching, and after manual analysis by
twoauthors,only 20%(204 commits)were true positives.
4 A RESEARCH AGENDA
4.1 Defect Causal Chains
To effectively identify the root causes of defects, as discussed in
ğ3.2.2, we suggest investigators use additional sources that pro-
vide more information about the causal chain of the defect. It is
uncertainifanalysisofpullrequestcomments,meetinglogs,design
documents, or other artifacts will be helpful. Still, these documents
canprovidemoreinsightsintothereasonbehindthecodeswritten
by the developers. The research community can conduct further
researchtodeterminewhichartifactswouldbemorehelpfuland
howinvestigatorscanadequatelyanalyzethemtoidentifytheroot
causes ofdefects.
In addition, software engineers have no standard approach to
documenting design or implementation decisions or efforts. While
standardssuchasISO/IEC/IEE12207requiredetaileddocumenta-
tion by the software engineers, Agile methodologies [ 1] [2] rec-
ommend less comprehensive documentation. Hence, this presents
anotherchallengeasthereisnoguaranteethatthesedocuments
will be available for analysis. The research results can also inform
engineering teams what documentation needs to be maintained if
they want to learn from theirfailures.
4.2StandardizingtheConductofFailureStudies
As we discussed in ğ 3.2.3, there are inconsistencies in the con-
duct of failure studies. We suggest two ways to standardize the
conductofthesestudies.First,addastandardforfailureanalysis
totheSIGSOFTempiricalstandards[ 28]tonotethequalitymea-
sures, replication packages, and expected general guidelines for
conducting a failure study. Second, we suggest the development
of a defect-type taxonomy map for software defects, similar to the
CommonWeaknessEnumeration(CWE)usedforcategorizingse-
curity vulnerabilities. Such a map would contain a taxonomy of
commondefecttypes.Itcanbeextensiblethatinvestigatorscon-
ducting failure studies for a specific system or defect classes can
builduponexistingtaxonomieswithdefecttypecategoriesparticu-
lartotheclassofsystembeinginvestigatedratherthaninventinga
newtaxonomy.Thismapwouldensurethattheresultsofallfailure
studiesarecomparable,whichwillimprovethe generalizabilityof
researchinfluencedbythe results.4.3 Increased Impacton EngineeringPractices
Followingthebiasreportedinğ 3.2.1,weproposeincreasedresearch
emphasis on replicability studies to verify if failure studies con-
ducted on open-source software also hold for commercial software.
Wealsosuggestincreasedcollaborationbetweeninvestigatorsof
failure studies and software engineering companies, which would
provide these investigators access to defect reports of commer-
cial software. This collaboration would ensure that failure studies’
resultsinfluence research,which wouldalsoberelevant topracti-
tioners inthesecompanies.
Wealsorecommendthat,inadditiontoprovidingresearchdi-
rections,softwarefailurestudiesproviderecommendationstoen-
gineering teams that will reduce the occurrence of defects and the
timetodebugandfixreporteddefects.Thisisakintofailureanalysis
inotherengineeringdisciplines,such asinthe NTSB,wheresuch
studieshaveledtovariouschangesinengineering,management,
andregulatory practices [ 8].
4.4 ToolSupport forFaster Defect Analysis
Withthechallengeofmissingtool supportdiscussedinğ 3.2.6,we
recommend the research and development of tools that would aid
theconductofthesestudies.NaturalLanguageProcessing(NLP)
techniques have become increasingly helpful in understanding the
semantic meaning of documents, summarizing, and extracting use-
fulinformationfromdocuments.Theyhavesuccessfullybeenused
toidentifydefectsinrequirementdocuments[ 33],identifydupli-
catedefectreports[ 34],extracttasksanduserstoriesfromappstore
reviews [ 11], and summarize defect reports [ 30] [29]. Hence, the
researchcommunitycaneasilyexploretheuseofNLPtoidentify
targetdefectreports,characterizethedefectsinthemandextract
otherrelevantinformationaboutthedefect( e.g.,consequence,man-
ifestation behavior, component affected) from these reports. While
usingNLPcannotreplacetheneedforexpertise-basedhumananal-
ysis, automatingtheabove-listed taskswouldsignificantlyreduce
the time the investigatorsspend conductingmanual analysis.
5 CONCLUSION
Inthispaper,wereflectontheconductoffailurestudiesinsoftware
engineering by surveying 52 published failure study papers. We
identified eight recurring flaws that have marred the conduct of
failure studies. These flaws impede the correctness, reliability, and
impact ofthe reportedresults of thesestudies.
Motivated by these challenges, we identify various ways the
researchcommunitycansupporttheconductofthesefailurestudies.
Weencouragefurtherresearchonidentifyingandanalyzingcausal
chainsfordefectsandtoolsupporttosimplifydefectanalysiswhile
recommending efforts to standardize the conduct of failure studies.
With these steps, software failure studies may improve software
engineeringquality.
DATA AVAILABILITY
Ourartifactcanbefoundat https://doi.org/10.5281/zenodo.7041931 .
This spreadsheetcontains our analysisofthefailure studypapers
we surveyed.
1618Reflections onSoftware Failure Analysis ESEC/FSE ’22, November14ś18, 2022,Singapore, Singapore
REFERENCES
[1]Kent Beck. 2001. Manifesto for Agile Software Development. https://
agilemanifesto.org/
[2] KentBeck. 2005. ExtremeProgrammingexplained . John Wait.
[3]A. Boulanger. 2005. Open-source versus proprietary software: Is one more
reliable and secure than the other? IBM Systems Journal 44, 2 (2005), 239ś248.
https://doi.org/10.1147/sj.442.0239 ConferenceName: IBM SystemsJournal.
[4]Junming Cao, Bihuan Chen, Chao Sun, Longjie Hu, and Xin Peng. 2021. Charac-
terizingPerformance Bugsin Deep Learning Systems. arXiv:2112.01771 [cs] (Dec.
2021).http://arxiv.org/abs/2112.01771 arXiv: 2112.01771.
[5]Haicheng Chen, Wensheng Dou, Yanyan Jiang, and Feng Qin. 2019. Under-
standing Exception-Related Bugs in Large-Scale Cloud Systems. In 2019 34th
IEEE/ACM International Conference on Automated Software Engineering (ASE) .
339ś351. https://doi.org/10.1109/ASE.2019.00040 ISSN:2643-1572.
[6]Nicolas Dilley and Julien Lange. 2020. Bounded verification of message-passing
concurrency in Go using Promela and Spin. Electronic Proceedings in Theoretical
ComputerScience 314(April2020),34ś45. https://doi.org/10.4204/EPTCS.314.4
arXiv: 2004.01323.
[7]Pedro Fonseca, Cheng Li, Vishal Singhal, and Rodrigo Rodrigues. 2010. A
study of the internal and external effects of concurrency bugs. In 2010 IEEE/I-
FIP International Conference on Dependable Systems Networks (DSN) . 221ś230.
https://doi.org/10.1109/DSN.2010.5544315 ISSN:2158-3927.
[8]Matthew R. Fox. 2001. Failure analysis at the National Transportation Safety
Board-JournalofFailureAnalysisandPrevention. https://link.springer.com/
article/10.1007/s11668-006-5004-5
[9]Prashant Gangidi.2018. A systematic approach toroot causeanalysis using3 ×
5 why’s technique. International Journal of Lean Six Sigma 10, 1 (Jan. 2018), 295ś
310.https://doi.org/10.1108/IJLSS-10-2017-0114 Publisher: Emerald Publishing
Limited.
[10]HaryadiS.Gunawi,MingzheHao,TanakornLeesatapornwongsa,TiratatPatana-
anake, Thanh Do, Jeffry Adityatama, Kurnia J. Eliazar, Agung Laksono, Jeffrey F.
Lukman, Vincentius Martin, and Anang D. Satria. 2014. What Bugs Live in
the Cloud? A Study of 3000+ Issues in Cloud Systems. In Proceedings of the
ACMSymposiumonCloudComputing (SOCC’14) .AssociationforComputing
Machinery, New York, NY, USA, 1ś14. https://doi.org/10.1145/2670979.2670986
[11]HuiGuoandMunindarP.Singh.2020. Caspar:ExtractingandSynthesizingUser
Stories of Problems from App Reviews. In 2020 IEEE/ACM 42nd International
Conference onSoftwareEngineering (ICSE) . 628ś640. ISSN:1558-1225.
[12]Nargiz Humbatova, Gunel Jahangirova, and Paolo Tonella. 2021. DeepCrime:
mutation testing of deep learning systems based on real faults. In Proceedings of
the 30th ACM SIGSOFT International Symposium on Software Testing and Analysis
(ISSTA 2021) . Association for Computing Machinery, New York, NY, USA, 67ś78.
https://doi.org/10.1145/3460319.3464825
[13]J. Richard Landis and Gary G. Koch. 1977. The Measurement of Observer
Agreement for Categorical Data. Biometrics 33, 1 (1977), 159ś174. https:
//doi.org/10.2307/2529310 Publisher: [Wiley, International Biometric Society].
[14]TanakornLeesatapornwongsa,JeffreyF.Lukman,ShanLu,andHaryadiS.Gu-
nawi.2016.TaxDC:ATaxonomyofNon-DeterministicConcurrencyBugsinData-
centerDistributedSystems.In ProceedingsoftheTwenty-FirstInternationalConfer-
ence on Architectural Support for Programming Languages and Operating Systems .
ACM,AtlantaGeorgia USA,517ś530. https://doi.org/10.1145/2872362.2872374
[15]Penghui Li, Yinxi Liu, and Wei Meng. 2021. Understanding and Detecting
Performance Bugs in Markdown Compilers. In 2021 36th IEEE/ACM Interna-
tional Conference on Automated Software Engineering (ASE) . 892ś904. https:
//doi.org/10.1109/ASE51524.2021.9678611 ISSN:2643-1572.
[16]BenjaminLiblitandAlexanderAiken.2002. Buildingabetterbacktrace:Techniques
for postmortem program analysis . Computer Science Division, University of
California.
[17]MarioLinares-Vásquez,GabrieleBavota,andCamiloEscobar-Velásquez.2017.
An Empirical Study on Android-Related Vulnerabilities. In 2017 IEEE/ACM 14th
International Conference on Mining Software Repositories (MSR) . 2ś13.https:
//doi.org/10.1109/MSR.2017.60
[18]Yepang Liu, Chang Xu, and Shing-Chi Cheung. 2014. Characterizing and
detecting performance bugs for smartphone applications. In Proceedings of
the 36th International Conference on Software Engineering (ICSE 2014) . Asso-
ciation for Computing Machinery, New York, NY, USA, 1013ś1024. https:
//doi.org/10.1145/2568225.2568229
[19]Amir Makhshari andAli Mesbah. 2021. IoT Bugs andDevelopment Challenges.
In2021IEEE/ACM43rdInternationalConferenceonSoftwareEngineering(ICSE) .
IEEE,Madrid, ES, 460ś472. https://doi.org/10.1109/ICSE43902.2021.00051
[20]AlejandroMazuera-Rozo,JairoBautista-Mora,MarioLinares-Vásquez,Sandra
Rueda, and Gabriele Bavota.2019. The Android OS stack and its vulnerabilities:
an empirical study. Empirical Software Engineering 24, 4 (Aug. 2019), 2056ś2101.
https://doi.org/10.1007/s10664-019-09689-7
[21]AlejandroMazuera-Rozo, CatiaTrubiani, MarioLinares-Vásquez,andGabriele
Bavota. 2020. Investigating types and survivability of performance bugs in
mobile apps. Empirical Software Engineering 25, 3 (May 2020), 1644ś1686. https://doi.org/10.1007/s10664-019-09795-6
[22]Audris Mockus, Roy T. Fielding, and James Herbsleb. 2000. A case study of
open source software development: the Apache server. In Proceedings of the
22nd international conference on Software engineering (ICSE ’00) . Association for
ComputingMachinery,NewYork,NY,USA,263ś272. https://doi.org/10.1145/
337180.337209
[23]Mika V. Mäntylä and Casper Lassenius. 2009. What Types of Defects Are Really
Discovered in Code Reviews? IEEE Transactions on Software Engineering 35, 3
(May 2009), 430ś448. https://doi.org/10.1109/TSE.2008.71 Conference Name:
IEEE Transactions onSoftwareEngineering.
[24]FrolinS.Ocariza,Jr.,KarthikPattabiraman,andAliMesbah.2014.Vejovis:suggest-
ing fixes for JavaScript faults. In Proceedings of the 36th International Conference
onSoftwareEngineering (ICSE2014) .AssociationforComputingMachinery,New
York, NY, USA,837ś847. https://doi.org/10.1145/2568225.2568257
[25]M. Paradies and D. Busch. 1988. Root cause analysis at Savannah River plant
(nuclear power station). In Conference Record for 1988 IEEE Fourth Conference on
HumanFactorsandPowerPlants, .479ś483. https://doi.org/10.1109/HFPP.1988.
27547
[26]JamesWPaulson,GiancarloSucci,andArminEberlein.2004. Anempiricalstudy
ofopen-sourceandclosed-sourcesoftwareproducts. IEEEtransactionsonsoftware
engineering 30,4 (2004), 246ś256. https://doi.org/10.1109/TSE.2004.1274044
[27]Henry Petroski. 1994. Design Paradigms: Case Histories of Error and Judgment in
Engineering . CambridgeUniversityPress. Google-Books-ID:C_ZroS6rY54C.
[28]Paul Ralph, Sebastian Baltes, Domenico Bianculli, Yvonne Dittrich, Michael
Felderer, Robert Feldt, Antonio Filieri, Carlo Alberto Furia, Daniel Graziotin,
PinjiaHe,RashinaHoda,NataliaJuristo,BarbaraKitchenham,RomainRobbes,
DanielMendez,JeffersonMolleri,DiomidisSpinellis,MiroslawStaron,KlaasStol,
Damian Tamburri, Marco Torchiano, Christoph Treude, Burak Turhan, and Sira
Vegas.2020. ACMSIGSOFTEmpiricalStandards. https://onikle.com/articles/
288927
[29]SarahRastkar,GailC.Murphy,andGabrielMurray.2010. Summarizingsoftware
artifacts:acasestudyofbugreports.In 2010ACM/IEEE32ndInternationalCon-
ference on Software Engineering , Vol. 1. 505ś514. https://doi.org/10.1145/1806799.
1806872ISSN:1558-1225.
[30]Sarah Rastkar, Gail C. Murphy, and Gabriel Murray. 2014. Automatic Summa-
rizationofBugReports. IEEETransactionsonSoftwareEngineering 40,4(April
2014), 366ś380. https://doi.org/10.1109/TSE.2013.2297712 Conference Name:
IEEE Transactions onSoftwareEngineering.
[31]James Reason. 1990. Human Error . Cambridge University Press. Google-Books-
ID: WJL8NZc8lZ8C.
[32]J Reason. 1997. Organizational accidents: the management of human and organi-
zational factors in hazardous technologies. England: Cambridge University Press,
Cambridge (1997).
[33]BenedettaRosadini,AlessioFerrari,GloriaGori,AlessandroFantechi,Stefania
Gnesi, Iacopo Trotta, and Stefano Bacherini. 2017. Using NLP to Detect Require-
mentsDefects:AnIndustrialExperienceintheRailwayDomain.In Requirements
Engineering: Foundation for Software Quality (Lecture Notes in Computer Science) ,
PaulGrünbacherandAnnaPerini(Eds.).SpringerInternationalPublishing,Cham,
344ś360. https://doi.org/10.1007/978-3-319-54045-0_24
[34]Per Runeson, Magnus Alexandersson, and Oskar Nyholm. 2007. Detection
of Duplicate Defect Reports Using Natural Language Processing. In 29th In-
ternational Conference on Software Engineering (ICSE’07) . 499ś510. https:
//doi.org/10.1109/ICSE.2007.32 ISSN:1558-1225.
[35]Qingchao Shen, Haoyang Ma, JunjieChen,Yongqiang Tian, Shing-Chi Cheung,
andXiangChen.2021. Acomprehensivestudyofdeep learningcompilerbugs.
InProceedingsofthe29thACMJointMeetingonEuropeanSoftwareEngineering
Conference and Symposium on the Foundations of Software Engineering (ESEC/FSE
2021). Association for Computing Machinery, New York, NY, USA, 968ś980.
https://doi.org/10.1145/3468264.3468591
[36]M.SullivanandR.Chillarege.1992. Acomparisonofsoftwaredefectsindatabase
management systems and operating systems. In [1992] Digest of Papers. FTCS-
22: The Twenty-Second International Symposium on Fault-Tolerant Computing .
475ś484. https://doi.org/10.1109/FTCS.1992.243586
[37]Chengnian Sun, Vu Le, Qirun Zhang, and Zhendong Su. 2016. Toward under-
standingcompilerbugsinGCCandLLVM.In Proceedingsofthe25thInternational
Symposium on Software Testing and Analysis (ISSTA 2016) . Association for Com-
putingMachinery,NewYork,NY,USA,294ś305. https://doi.org/10.1145/2931037.
2931074
[38]LinTan,ChenLiu,ZhenminLi,XuanhuiWang,YuanyuanZhou,andChengxiang
Zhai. 2014. Bug characteristics in open source software. Empirical Software
Engineering 19,6(Dec.2014),1665ś1705. https://doi.org/10.1007/s10664-013-
9258-8
[39]E.UbaniandC.Ononuju.2013. Astudyoffailureandabandonmentofpublic
sector-drivencivilengineeringprojectsinNigeria:Anempiricalreview. American
JournalofScientificandIndustrialResearch 4,1(Feb.2013),75ś82. https://doi.
org/10.5251/ajsir.2013.4.1.75.82
[40]Dinghua Wang, Shuqing Li, Guanping Xiao, Yepang Liu, and Yulei Sui. 2021.
An exploratory study of autopilot software bugs in unmanned aerial vehicles.
1619ESEC/FSE ’22, November14ś18, 2022,Singapore, Singapore PaschalC.Amusuo, AishwaryaSharma,SiddharthR. Rao,Abbey Vincent, andJames C.Davis
InProceedingsofthe29thACMJointMeetingonEuropeanSoftwareEngineering
Conference and Symposium on the Foundations of Software Engineering (ESEC/FSE
2021). Association for Computing Machinery, New York, NY, USA, 20ś31. https:
//doi.org/10.1145/3468264.3468559[41]ShahedZaman,BramAdams,andAhmedE.Hassan.2012. Aqualitativestudy
onperformancebugs.In 20129thIEEEWorkingConferenceonMiningSoftware
Repositories (MSR) . 199ś208. https://doi.org/10.1109/MSR.2012.6224281 ISSN:
2160-1860.
1620