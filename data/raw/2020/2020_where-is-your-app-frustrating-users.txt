Where is Your App Frustrating Users?Yawen Wang1,2, Junjie Wang1,2,3,∗, Hongyu Zhang5, Xuran Ming1,2, Lin Shi1,2, Qing Wang1,2,3,4,∗1Laboratory for Internet Software Technologies,3State Key Laboratory of Computer Sciences,4Science & Technologyon Integrated Infomation System Laboratory, Institute of Software Chinese Academy of Sciences, Beijing, China2University of Chinese Academy of Sciences, Beijing, China5The University of Newcastle, Callaghan, Australia{yawen2018, junjie, xuran2020, shilin, wq}@iscas.ac.cn, hongyu.zhang@newcastle.edu.auABSTRACTUser reviews of mobile apps provide a communication channel fordevelopers to perceive user satisfaction. Many app features thatusers have problems with are usually expressed by key phrasessuch as “upload pictures”, which could be buried in the review texts.The lack of!ne-grained view about problematic features couldobscure the developers’ understanding of where the app is frus-trating users, and postpone the improvement of the apps. Existingpattern-based approaches to extract target phrases su"er from lowaccuracy due to insuﬃcient semantic understanding of the reviews,thus can only summarize the high-level topics/aspects of the re-views. This paper proposes a semantic-aware,!ne-grained appreview analysis approach (SIRA) to extract, cluster, and visualizethe problematic features of apps. The main component of SIRA isa novel BERT+Attr-CRF model for!ne-grained problematic fea-ture extraction, which combines textual descriptions and reviewattributes to better model the semantics of reviews and boost theperformance of the traditional BERT-CRF model. SIRA also clustersthe extracted phrases based on their semantic relations and presentsa visualization of the summaries. Our evaluation on 3,426 reviewsfrom six apps con!rms the e"ectiveness of SIRA in problematicfeature extraction and clustering. We further conduct an empiricalstudy with SIRA on 318,534 reviews of 18 popular apps to exploreits potential application and examine its usefulness in real-worldpractice.CCS CONCEPTS•Software and its engineering→Requirements analysis;Soft-ware maintenance tools.KEYWORDSApp Review, Information Extraction, Deep LearningACM Reference Format:Yawen Wang1,2, Junjie Wang1,2,3,∗, Hongyu Zhang5, Xuran Ming1,2, LinShi1,2, Qing Wang1,2,3,4,∗. 2022. Where is Your App Frustrating Users?.In44th International Conference on Software Engineering (ICSE ’22), May∗Corresponding author.Permission to make digital or hard copies of all or part of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor pro!t or commercial advantage and that copies bear this notice and the full citationon the!rst page. Copyrights for components of this work owned by others than ACMmust be honored. Abstracting with credit is permitted. To copy otherwise, or republish,to post on servers or to redistribute to lists, requires prior speci!c permission and/or afee. Request permissions from permissions@acm.org.ICSE ’22, May 21–29, 2022, Pittsburgh, PA, USA© 2022 Association for Computing Machinery.ACM ISBN 978-1-4503-9221-1/22/05. . . $15.00https://doi.org/10.1145/3510003.351018921–29, 2022, Pittsburgh, PA, USA.ACM, New York, NY, USA, 13 pages. https://doi.org/10.1145/3510003.35101891 INTRODUCTIONMobile app development has been active for over a decade, generat-ing millions of apps for a wide variety of application domains suchas shopping, banking, and social interactions. They have now be-come indispensable in our daily life. The importance of mobile appsurges the development team to make every endeavor to understandusers’ concerns and improve app quality.Users often write reviews of the mobile apps they are using ondistribution platforms such as Apple Store and Google Play Store.These reviews are short texts that can provide valuable informa-tion to app developers, such as user experience, bug reports, andenhancement requests [16,27,41,52]. A good understanding ofthese reviews can help developers improve app quality and usersatisfaction [15,30,48]. However, popular apps may receive a largenumber of reviews every day. Therefore, manually reading andanalyzing each user review to extract useful information is verytime-consuming,̵ǤǤǤǡ͸ǤǤǤǡǤǨͲͶȀͳͻȀʹͲʹͲFigure 1: An example app review and problematic feature.In recent years, automated techniques for mining app reviewshave attracted much attention [21,45,47]. These techniques canhelp reduce the e"ort required to understand and analyze app re-views in many ways, such as topic discovery [6,40,48], and keyphrase extraction [12,15,27,52,57]. However, existing work abouttopic discovery can only identifyWHATthe users complain about[30,48,57], such as the high-level topics/aspects of the reviews (e.g.,compatibility,update,connection, etc). Taken the review ofInsta-gramin Figure 1 as an example, existing approaches would captureterms such asupdate, cache, uninstall, yet missing its core intent.Developers still could not have a concrete understanding aboutwhich speci!c features of the app the users are complaining about.Furthermore, existing work about key phrase extraction mainlyutilizes heuristic-based techniques (such as Part-of-Speech patterns,24272022 IEEE/ACM 44th International Conference on Software Engineering (ICSE)
ICSE ’22, May 21–29, 2022, Pi/t_tsburgh, PA, USAWang, et al.parsing tree, and semantic dependence graph) to extract the targetphrases, which could have insuﬃcient semantic understanding ofthe reviews. As a result, their accuracy is less satisfactory and canbe further improved.In comparison, we aim at exploiting theWHEREaspect of the appreviews, and providing an accurate!ne-grained landscape aboutwhere an app frustrates the users, i.e., which speci!c app features1the users have problems with. As an example in Figure 1, the reviewis about a crashing problem, and the problematic feature the usercomplained about isupload to my story. The!ne-grained knowledgeabout problematic features could facilitate app developers in un-derstanding the user concerns, localizing the problematic modules,and conducting follow-up problem-solving activities.To overcome the drawbacks of existing work and better exploitthe app reviews, this paper proposes a Semantic-aware, fIne-grainedapp Review Analysis approach (SIRA), which can extract, cluster,and visualize the problematic features of apps. More speci!cally,SIRA includes a novel BERT+Attr-CRF model to automatically ex-tract the!ne-grained phrases (i.e., problematic features). It com-bines the review descriptions and review attributes (i.e., app cat-egory and review description sentiment) to better model the se-mantics of reviews and boost the performance of the traditionalBERT-CRF model [63]. With the extracted phrases, SIRA then de-signs a graph-based clustering method to summarize the commonaspects of problematic features based on their semantic relations. Fi-nally, SIRA presents a visualization of the summarized problematicfeatures.We evaluate SIRA on 3,426 reviews involving 8,788 textual sen-tences from six apps spanning three categories. For problematicfeature extraction, the overall precision and recall achieved by SIRAis 84.27% and 85.06% respectively, signi!cantly outperforming thestate-of-the-art methods. SIRA can also achieve high performancein problematic feature clustering, outperforming two commonly-used baselines. We further conduct an empirical study with SIRA on318,534 reviews of 18 popular apps (reviews spanning 10 months)to explore its potential application and examine its usefulness inreal-world practice. We!nd that di"erent apps have their uniqueproblematic features and problematic feature distributions. Theresults also reveal that di"erent apps can share some commonproblematic features. This observation can facilitate mobile apptesting, e.g., recommending bug-prone features to similar apps fortest prioritization.The main contributions of this paper are as follows:•A semantic-aware,!ne-grained app review analysis approach(SIRA) to extracting, clustering, and visualizing the problem-atic features of apps. In SIRA, we design a BERT+Attr-CRFmodel to automatically extract the!ne-grained phrases (i.e.,problematic features), and a graph-based clustering methodto summarize the common aspects of problematic features.•The evaluation of the proposed SIRA on 3,426 reviews in-volving 8,788 textual sentences from six apps spanning threecategories, with aﬃrmative results.1We refer to a feature as a distinctive, user-visible characteristic of a mobile app[29][65], e.g., sending videos, viewing messages, etc.•A large-scale empirical study on 318,534 reviews of 18 popu-lar apps, to explore its potential application and usefulnessin real-world practice.•Public accessible source code and experimental data at https://github.com/MeloFancy/SIRA.2 BACKGROUND AND RELATED WORKNamed Entity Recognition (NER).NER is a classic Natural Lan-guage Processing (NLP) task of sequence tagging [25,66]. Given asequence of words, NER aims to predict whether a word belongsto named entities, e.g., names of people, organizations, locations,etc. NER task can be solved by linear statistical models, e.g., Max-imum Entropy Markov models [43,53], Hidden Markov Models[11] and Conditional Random Fields (CRF) [34]. Deep learning-based techniques would use a deep neural network to capturesentence semantics and a CRF layer to learn sentence-level tagrules. Typical network structures include convolutional neural net-work with CRF (Conv-CRF) [7], Long Short-Term Memory networkwith CRF (LSTM-CRF) and bidirectional LSTM network with CRF(BiLSTM-CRF) [25]. By taking advantage of the bidirectional struc-ture, BiLSTM-CRF model can use the past and future input infor-mation and can usually obtain better performance than Conv-CRFand LSTM-CRF.Language model pre-training techniques have been shown tobe e"ective for improving many NLP tasks [10,22]. BERT (Bidi-rectional Encoder Representations from Transformers) [10] is aTransformer-based [55] representation model that uses pre-trainingto learn from the raw corpus, and!ne-tuning on downstream taskssuch as the NER task. Employing BERT to replace BiLSTM (shortfor BERT-CRF) could lead to further performance boosts [63]. BERT-CRF model bene!ts from the pre-trained representations on largegeneral corpora combined with!ne-tuning techniques.Mining user reviews.Harman et al. introduced the concept ofapp store mining by identifying correlations between the customerratings and the download rank of a mobile app [21,42]. Palombaet al. found that developers implementing user reviews would berewarded in terms of app ratings [47]. Noei et al. investigated theevolution of app ranks and identi!ed the variables that share astrong relationship with ranks, e.g., number of releases [45].Previous studies on mining user reviews emphasized the topicdiscovery/classi!cation and summarization of reviews as a wayof aggregating a large amount of text and reducing the e"ort re-quired for analysis [6,40,46,48,52]. These classi!cations are fromdi"erent points of view, e.g., whether or not the reviews includebug information, requests for new features [40], whether they areinformative [6], whether reviews across di"erent languages andplatforms are similar [46], or based on a taxonomy relevant tosoftware maintenance and evolution [48], etc. Other studies fo-cused on the information extraction from app reviews consider-ing the fact that reading through the entire reviews is impractical[12,15,16,30,33,57]. For example, the types of complains [30], theapp aspects loved by users [15], user rationale [33] and summariesfor guiding release planning [56] are extracted and summarized forfacilitating the review understanding.There are some studies on mining API-related opinions frominformal discussions, such as Q&A websites (e.g., Stack Over$ow)2428Where is Your App Frustrating Users?ICSE ’22, May 21–29, 2022, Pi/t_tsburgh, PA, USA
Figure 2: The overview of SIRA.to alleviate developers’ burden in performing manual searches[38,54]. These methods mainly depend on fuzzy matching withpre-built API databases, which cannot work in our context. Thereare also some studies on mining social media data (e.g., Twitterdata) [18]. The app reviews mainly convey users’ feedback aboutan app, while the Twitter data is more general and contains dailymessages. Therefore, general-purpose techniques for Twitter datarequire customizations to better understand app reviews.Some studies are similar to our work, such as topic discov-ery/classi!cation, sentiment analysis, etc. However, they do notsupport the extraction of!ne-grained features well. For example,INFAR [12] mines insights from app reviews and generates sum-marizes after classifying sentences into pre-de!ned topics. Thediscovered topics from INFAR are more coarse-grained (e.g., GUI,crash, etc.). Our method can highlight the!ne-grained features (e.g.,"push noti!cation") that users complained about; SUR-Miner [15]and Caspar [16] uses techniques, such as dependency parsing andPart-of-Speech pattern, to extract some aspects from app reviews.Guzman et al. [19] proposed a method, which can only extract fea-tures consisting of two words (i.e., collocations) from the reviewsbased on word co-occurrence patterns, which is not applicable inour context, because the problematic features might contain mul-tiple words; Opiner [54] is a method to mining aspects from APIreviews. It extracts API mentions from API reviews through exactand fuzzy name matching with pre-built API databases, which isdiﬃcult to work in our context because we do not have a databaseof feature phrases in advance. These studies utilized pattern-basedmethod to extract the target phrases, which did not consider thereview semantics suﬃciently, and had bad tolerance to noise; bycomparison, our proposed approach is a semantic-aware approach.Mining open source bug reports.Previous studies have pro-posed various methods to automatically classify bug reports [28,39],detect the duplicate reports [8,60,67], summarize the reports [20],and triage the reports [23,36,62], etc. The bug reports in opensource or crowd testing environment are often submitted by soft-ware practitioners, and often described with detailed bug expla-nation and in relatively longer length. Yet the app reviews aresubmitted by the end users and in much fewer words, thus theabove mentioned approaches could not be easily adopted in thiscontext.Semantic-aware approaches in SE.Researchers have utilizeddeep learning based techniques to capture the semantics of softwareartifacts and facilitate the follow-up software engineering tasks.Such kinds of studies include neural source code summarizationwith attentional encoder-decoder model based on code snippets andsummaries [64], requirement traceability by incorporating require-ments artifact semantics and domain knowledge into the tracingsolutions [17], knowledge mining of informal discussions on socialplatforms [59], etc. This paper focuses on a di"erent type of soft-ware artifact (i.e., app reviews) and incorporates a state-of-the-arttechnique (i.e., BERT) for the semantic-aware learning, and theresults show its e"ectiveness.3 APPROACHThis paper proposes a Semantic-aware, fIne-grained app ReviewAnalysis approach SIRA to extract, cluster, and visualize the prob-lematic features of apps (i.e., the phrases in app reviews depictingthe feature which users have problems with, see the examples inFigure 1.)Figure 2 presents the overview of SIRA, which consists of foursteps.First, it preprocesses the app reviews crawled from onlineapp marketplace, to obtain the cleaned review descriptions andthe review attributes (i.e., the category of the belonged appcandthe review description sentiments).Second, it builds and trains aBERT+Attr-CRF model to automatically extract the!ne-grainedphrases about problematic features. BERT+Attr-CRF combines thereview descriptions and two review attributes as input to bettermodel the semantics of reviews and boost the phrase extractionperformance of the traditional BERT-CRF model.Third, SIRA clus-ters the extracted phrases with a graph-based clutering method to2429ICSE ’22, May 21–29, 2022, Pi/t_tsburgh, PA, USAWang, et al.summarize the common aspects of problematic features based ontheir semantic relations. And/f_inally, it presents a visualizationview to illustrate the summaries and compare the problematic fea-tures among apps, in order to acquire a better understanding ofwhere users complain about across apps.3.1 Data PreprocessingData preprocessing mainly includes two steps: textual data cleaningand review attribute collection.3.1.1Textual Data Cleaning.The raw app reviews are often submitted via mobile devices andtyped using limited keyboards. This situation leads to the frequentoccurrences of massive noisy words, such as repetitive words, mis-spelled words, acronyms and abbreviations [13, 15, 57, 58].Following other CRF-based practices [25], we treat each sen-tence as an input unit. We!rst split each review into sentences bymatching punctuations through regular expressions. Then we!lterall non-English sentences with Langid2. We tackle the noisy wordsproblem with the following steps:•Lowercase: we convert all the words in the review descrip-tions into lowercase.•Lemmatization: we perform lemmatization with Spacy3toalleviate the in$uence of word morphology.•Formatting: we replace all numbers with a special symbol“<number>” to help the BERT model unify its understanding.Besides, we build a list containing all the app names crawledfrom Google Play Store, and replace them with a uniformspecial symbol “<appname>”.3.1.2Review A/t_tribute Collection.Some attributes related to the review or the app can facilitate theextraction of problematic features in Section 3.2. This subsectioncollects these attributes, i.e., the category of the belonged appcandthe review description sentimentsas shown in Figure 2 and Figure3. The reason why we include the app category is that apps fromdi"erent categories would exert unique nature in terms of func-tionalities and topics [14]. Furthermore, review descriptions withnegative sentiment would be more likely to contain problematicfeatures, compared with the description with positive sentiment.Hence, we include review description sentiment as the second at-tribute in our model.App categories can be directly collected when crawling datafrom Google Play Store. To obtain the sentiment for each reviewsentence, we employ SentiStrength-SE [26], a domain-speci!c senti-ment analysis tool especially designed for software engineering text.SentiStrength-SE would assign a positive integer score in the rangeof 1 (not positive) to 5 (extremely positive) and a negative integerscore in the range of -1 (not negative) to -5 (extremely negative) toeach sentence. Employing two scores is because previous researchfrom psychology [2] has revealed that human beings process thepositive and negative sentiment in parallel. Following previouswork [14,19], if the absolute value of the negative score multipliedby 1.5 is larger than the positive score, we assign the sentence the2https://github.com/sa"sd/langid.py3https://spacy.io
Figure 3: Detailed structure of BERT+Attr-CRF.negative sentiment score; otherwise, the sentence is assigned withthe positive sentiment score.3.2 Problematic Feature ExtractionWe model the problematic feature extraction problem as a NamedEntity Recognition (NER) task, where we treat problematic featuresas named entities, and solve the problem with the commonly-usedCRF technique. To better capture the semantics of the app reviews,we employ the BERT model to encode the review descriptions. Fur-thermore, we incorporate the review attributes in the CRF modelto further boost the recognition of problematic features. Two at-tributes, i.e., category of the belonged appcand review descriptionsentiments(see Section 3.1.2), are utilized in our model.Following other NER tasks, we use the BIO tag format [9,50] totag each review sentence, where•B-label (Beginning): The word is the beginning of the targetphrase.•I-label (Inside): The word is inside the target phrase but notits beginning.•O-label (Outside): The word is outside the target phrase.The BIO-tagged review sentence is input into the BERT+Attr-CRFmodel for further processing.Figure 3 presents the detailed structure of our proposed BERT+Attr-CRF model. Since app reviews are short texts, and the involved vo-cabulary is relatively small, we use the pre-trained modelBERTBASE4,which has 12 layers, 768 hidden dimensions and 12 attention heads.It has been pre-trained on the BooksCorpus (800M words) and Eng-lish Wikipedia (2,500M words), and will be!ne-tuned using ourown data. Each input sentence is represented by 128 word tokenswith a special starting symbol[CLS]. For those not long enough,we use a special symbol[PAD]to pad them to the length of 128,following the common practice. The outputs of BERT are fed into adropout layer to avoid over-!tting. Finally, we obtainn(the lengthof the input sentence) vectors, with each vector (denoted as/v.alti)having 768 dimensions and corresponding to each input word.4https://huggingface.co/bert-base-uncased2430Where is Your App Frustrating Users?ICSE ’22, May 21–29, 2022, Pi/t_tsburgh, PA, USAWe incorporate the review attributes into the textual vectors (/v.alt)to jointly capture the underlying meaning of the review sentence.The review attributes (cands) extracted in Section 3.1.2 are discretevalues. We!rst convert them into continuous vectors (denotedashcandhs) by feeding them into the embedding layers. Takingattributesas an example, it can take ten values (-5 to -1 and 1 to 5).The embedding layer could represent each value with a continuousvector, which can be trained jointly with the whole model. Wethen concatenatehc,hsand/v.alt(hc/circleplustext.1hs/circleplustext.1/v.alt) to obtain a vector(denoted as/v.alt/primei) for each input word. The concatenated vectors!rstgo through a Multi-layer Perceptron (MLP), which computes theprobability vector (denotedp) of BIO tags for each word:p=f(W[hc;hs;/v.alt])(1)wheref(·)is the activation function, andWis trainable parame-ters in MLP.[hc;hs;/v.alt]is the concatenation of these three vectors.Finally,pis input into the CRF layer to determine the most likelytag sequence based on Viterbi Algorithm [1].Based on the derived tag sequence, we can obtain the phrasesabout problematic features. For example, if our input review sen-tence is “whenever I go to send a video it freezes up”, and the outputtag sequence is “<O><O><O><O><B><I><I><O><O><O>”, we can determine the extracted problematicfeature as “send a video” based on the BIO format.The loss function of the model should measure the likelihood ofthe whole true tag sequence, instead of the likelihood of the truetag for each word in the sequence. Therefore, the commonly-usedCross Entropy is not suitable in this context. Following existingstudies [25], the loss function contains two parts: the emission scoreand the transition score. It is computed as:s([x]T1,[l]T1,/tildewideθ)=T/summationdisplay.1t=1([A][l]t−1,[l]t+[fθ][l]t,t)(2)where[x]T1is the sentence sequence of lengthT, and[l]T1is thetag sequence.fθ([x]T1)is the emission score, which is the output ofMLP with parametersθ, and[A]i,jis the transition score, which isobtained with the parameters from the CRF layer. The transitionscore[A]i,jmodels the transition from thei-th state to thej-thstate in the CRF layer./tildewideθ=θ∪/braceleftbig[A]i,j∀i,j/bracerightbigis the new parametersfor the whole network. The loss of a sentence[x]T1along with asequence of tags[l]T1is derived by the sum of emission scores andtransition scores.Model Training:The hyper-parameters in SIRA are tuned care-fully with a greedy strategy to obtain the best performance. Given ahyper-parameterPand its candidate values{/v.alt1,/v.alt2,. . . ,/v.altn}, we per-form automated tuning forniterations, and choose the values whichleads to the best performance as the tuned value ofP. After tuning,the learning rate is set as 10−4. The optimizer is Adam algorithm[31]. We use the mini-batch technique for speeding up the trainingprocess with batch size 32. The drop rate is 0.1, which means 10%of neuron cells will be randomly masked to avoid over-!tting.We implement this BERT+Attr-CRF model using Transformers5,which is an open-source Pytorch library for Natural Language Un-derstanding and Natural Language Generation. Our implementationand experimental data are available online6.3.3 Problematic Feature ClusteringThe extracted problematic features might be linguistically di"er-ent yet semantically similar. To provide a summarized view of theproblematic features, this step clusters the extracted problematicfeatures based on the topics derived from their semantic relations.Conventional topic models use statistical techniques (e.g., Gibbssampling) based on word co-occurrence patterns [49]. They are notsuitable for the short texts (i.e., problematic features in our context),because the co-occurrence patterns can hardly be captured fromthe short text, instead the semantic information should be takeninto consideration. Additionally, these models need to specify thenumber of clusters/topics, which is hardly determined in our con-text. To tackle these challenges, we design a graph-based clusteringmethod, which employs semantic relations of problematic features.First, we convert problematic feature phrases into 512 dimen-sional vectors using Universal Sentence Encoder (USE) [5]. It is atransformer-based sentence embedding model that captures richsemantic information, and has been proven more e"ective thantraditionally-used word embedding models [16].Second, we con-struct a weighted, undirected graph, where each problematic featureis taken as a node, and the cosine similarity score between USE vec-tors of two problematic features is taken as the weight between thenodes. If the score is over a certain ratio, we add an edge betweentwo nodes. The ratio is an input hyper-parameter, which measuresthe semantic correlations between problematic features. The higherratio leads to higher cluster cohesion. We set it as 0.5 after tuningin the training data.Third, we perform Chinese Whispers (CW)[3], which is an eﬃcient graph clustering algorithm, on this graphto cluster problematic features.With this graph-based clustering method, SIRA can group theproblematic features that are semantically similar into the sametopic. We implement our clustering method in python, based onthe open-source implementation of USE7and CW8.3.4 VisualizationIn order to display the clustering results of multiple apps moreintuitively, we provide a visualized view in the form of bubblecharts (an example is shown in Figure 4). The y-axis demonstratesthe names of investigated apps, and the x-axis represents the id ofeach cluster. The size of the bubble (denoted assa,c) of appainclustercis de!ned as the ratio between the number of problematicfeatures of appain clustercand the total number of problematicfeatures in appa.When the cursor hovers over the bubble, it would display detailedinformation of this cluster, including the cluster name, the numberof problematic features, and example reviews with correspondingproblematic features. For the cluster name, we!rst!nd the most5https://github.com/huggingface/transformers6https://github.com/MeloFancy/SIRA7https://github.com/MartinoMensio/spacy-universal-sentence-encoder8https://github.com/nlpub/chinese-whispers-python2431ICSE ’22, May 21–29, 2022, Pi/t_tsburgh, PA, USAWang, et al.Table 1: Experimental dataset.CategoryApp# Reviews# SentencesSocialInstagram5821,402Snapchat5851,388CommunicationGmail5861,525Yahoo Mail5421,511FinanceBPI Mobile5881,488Chase Mobile5431,474Overall3,4268,788frequent noun or verb (denoted asw) among all problematic featuresin the cluster. We then count the number of problematic featurescontainingw, and treat the most frequent phrase as the cluster name(i.e., the representative problematic feature). By comparing therelative sizes of bubbles, one can intuitively acquire the distributionof problematic features across apps.4 EXPERIMENTAL DESIGN4.1 Research QuestionsWe answer the following three research questions:•RQ1:What is the performance of SIRA in extracting prob-lematic features?•RQ2:Is each type of the review attributes employed in SIRAnecessary?•RQ3:What is the performance of SIRA in clustering prob-lematic features?RQ1investigates the performance of SIRA in problematic featureextraction, and we also compare the performance with four state-of-the-art baselines (see Section 4.3) to further demonstrate itsadvantage.RQ2conducts comparison with SIRA’s three variantsto demonstrate the necessity of the employed review attributes inBERT+Attr-CRF model.RQ3investigates the performance of SIRAin problematic feature clustering, and we also compare SIRA withtwo commonly-used baselines (see Section 4.3).4.2 Data PreparationWe use the reviews of six apps from three categories (two in eachcategory) in our experiments. All six apps are popular and widely-used by a large number of users. We!rst crawl the app reviewsfrom Google Play Store submitted during August 2019 to January2020, with the tool google-play-scraper9. For each app, we thenrandomly sample around 550 reviews (about 1500 sentences) andlabel them for further experiments. Table 1 elaborates the statisticsof the experimental dataset in detail. It contains 3,426 reviews and8,788 sentences in total.Three authors then manually label the app reviews to serve asthe ground-truth in verifying the performance of SIRA. To guar-antee the accuracy of the labeling outcomes, the!rst two authors!rstly label the app reviews of an app independently, i.e., mark thebeginning and ending position of the problematic features in eachreview sentence. Second, the fourth author compares the labelingresults,!nds the di"erence, and organizes a face-to-face discussionamong them three to determine the!nal label. All the six appsfollow the same process. For the!rst labeled app (Instagram), the9https://github.com/facundoolano/google-play-scraperCohen’s Kappa is 0.78 between the two participants, while for thelast labeled app (Chase Mobile), the Cohen’s Kappa is 0.86. Aftertwo rounds of labeling, a common consensus is reached for everyreview sentence.4.3 Baselines4.3.1Baselines for Problematic Feature Extraction.We select methods that can extract target phrases from app re-views as baselines for problematic feature extraction. To the best ofour knowledge, existing methods are mainly pattern-based, whichcan be classi!ed into three types based on the techniques: 1) Part-of-Speech (PoS) Pattern: SAFE [27] and PUMA [58]; 2) Depen-dency Parsing plus PoS Pattern: Caspar [16] and SUR-Miner [15];3) Pattern-based Filter plus Text Classi!cation: KEFE [61]. We se-lect the representative method from each type as baselines, i.e.,KEFE, Caspar, and SAFE. In addition, since we model the featureextraction as an NER task, we also include BiLSTM-CRF [25], acommonly-used technique in NER tasks, as a baseline. We intro-duce four baselines in detail below:BiLSTM-CRF[25]: A commonly-used algorithm in sequencetagging tasks such as NER. Being a deep learning-based technique,it utilizes a BiLSTM to capture sentence semantics and a CRF layerto learn sentence-level tags.KEFE[61]: A state-of-the-art approach for identifying key fea-tures from app reviews. A key feature is referred as the featuresthat are highly correlated to app ratings. It!rstly employs a pattern-based!lter to obtain candidate phrases, and then a BERT-basedclassi!er to identify the features. Since its patterns are designed forChinese language, we replace them with the patterns in SAFE [27]to handle English reviews.Caspar[16]: A method for extracting and synthesizing user-reported mini stories regarding app problems from reviews. Wetreat its!rst step, i.e., events extraction, as a baseline. An event isreferred as a phrase that is rooted in a verb and includes other at-tributes related to the verb. It employed pattern-based and grammat-ical NLP techniques such as PoS tagging and dependency parsing onreview sentences to address this task. We use the implementationprovided by the original paper10.SAFE[27]: A method for extracting feature-related phrases fromreviews by 18 PoS patterns. For example, the patternVerb-Adjective-Nouncan extract features like “delete old emails”. We implement all18 patterns to extract the phrases based on the NLP toolkit NLTK11.4.3.2Baselines for problematic feature Clustering.We employ the following two baselines for problematic featureclustering, which are commonly used for mining topics of appreviews:K-Means: It is a commonly-used clustering algorithm, and wasemployed to cluster the keywords of app reviews [57]. In this work,we!rst encode each problematic feature with TF-IDF [51] vectors,then run K-Means to cluster all problematic features into topics,following previous work [57]. We apply the implementation in thelibrary scikit-learn12.10https://hguo5.github.io/Caspar11https://github.com/nltk/nltk12https://scikit-learn.org2432Where is Your App Frustrating Users?ICSE ’22, May 21–29, 2022, Pi/t_tsburgh, PA, USALDA[4]: It is a commonly-used topic clustering algorithm, andwas utilized to group the app features [19]. In this work, we treatthe extracted problematic features as documents and run LDA fortopic modeling, following previous work [19]. We employ the im-plementation in the library Gensim13.4.4 Experimental SetupTo answer RQ1, we conduct nested cross-validation [32] on theexperimental dataset. The inner loop is for selecting optimal hyper-parameters, which are used for evaluating performance in the outerloop. In the outer loop, we randomly divide the dataset into ten folds,use nine of them for training, and utilize the remaining one fold fortesting the performance. The process is repeated for ten times, andthe average performance is treated as the!nal performance. In theinner loop, we use eight folds for training and one fold for validation.We run each baseline (see Section 4.3) to obtain its performancefollowing the same experimental setup, and present the evaluationresults on each app and on the overall dataset, respectively.For RQ2, we design three variants of BERT+Attr-CRF modelto demonstrate the necessity of employed review attributes inour model architecture. In detail, BERT-CRF, BERT+Cat-CRF, andBERT+SEN-CRF respectively represent the model without reviewattributes (i.e., only with text), the model without review descriptionsentiment (i.e., with text and app category), and the model withoutapp category (i.e., with text and review description sentiment). Wereuse other experimental setups as RQ1.For RQ3, we manually build the ground-truth clustering resultsto evaluate the problematic feature clustering performance. Thecriteria for labeling are to group the features that represent thesame functionality into one cluster. More speci!cally, we randomlysample 100 problematic features for each app (600 in total) derivedfrom the results of RQ1. The two authors independently label theseproblematic features into clusters in the!rst round, where the Co-hen’s Kappa between two authors reaches 0.81 (i.e., a satisfactorydegree of agreement). Then follow-up discussions are conducteduntil common consensus is reached. Finally, the 600 problematicfeatures were labeled into 20 groups. Note that we do not specifythe number of clusters in advance, because it is hard to decidethe number in our context. Our proposed clustering method doesnot need to specify this parameter as well. Meanwhile, we runour approach and each baseline (see Section 4.3) to cluster theseproblematic features, and obtain each approach’s clustering per-formance by comparing the predicted and ground-truth clusteringresults for each app and the overall dataset, respectively.The experimental environment is a desktop computer equippedwith an NVIDIA GeForce RTX 2060 GPU, intel core i7 CPU, 16GBRAM, running on Windows 10, and training the model takes about2.5 hours for each fold nested cross-validation.4.5 Evaluation Metrics4.5.1Metrics for Problematic Feature Extraction.We use precision, recall, and F1-Score, which are commonly-usedmetrics, to evaluate the performance of SIRA for problematic fea-ture extraction. We treat a problematic feature is correctly predicted13https://radimrehurek.com/gensimif the predicted phrase from SIRA for a review sentence of an app isthe same as the ground-truth one. Three metrics are computed as:•Precisionis the ratio of the number of correctly predictedphrases to the total number of predicted phrases.•Recallis the ratio of the number of correctly predictedphrases to the total number of ground-truth phrases.•F1-Scoreis the harmonic mean of precision and recall.4.5.2Metrics for Problematic Feature Clustering.Following previous work [24], we use the commonly-used AdjustedRand Index (ARI) [35] and Normalized Mutual Information (NMI)[44] to evaluate the clustering performance by comparing with theground-truth clustering results. Higher metric values indicate betterclustering performance. For clarity, we denoteGas the ground-truthclustering result, andCas the predicted clustering result.Adjusted Rand Index (ARI): It takes values in[−1, 1],r e$ect-ing the degree of overlap between the two clusters. The raw RandIndex (RI) is computed byRI=a+b(n2), whereais the number of pairsthat are assigned in the same cluster inGand also in the same clus-ter inC, andbis the number of pairs that are assigned in di"erentclusters both inGandC./parenleftbign2/parenrightbigis the total number of unordered pairsin a set ofnphrases. The raw RI score is then “adjusted for chance”into the ARI score using the following scheme:ARI=RI−E(RI)max(RI)−E(RI)(3)whereE(RI)is the expected value ofRI. In this way, the ARI canbe ensured to have a value close to 0.0 for random labeling inde-pendently of the number of clusters and samples.Normalized Mutual information (NMI): It measures the sim-ilarity degree of the two sets of clustering results between 0 (nomutual information) and 1 (perfect correlation).NMI(G,C)=MI(G,C)/radicalbigH(G)H(C)(4)whereH(G)=−/summationtext.1|G|i=1P(i)lo/afii10069.ital(P(i))is the entropy of setG, andP(i)=GiNis the probability that a phrase picked randomly fallsinto clusterGi. TheMI(G,C)is the mutual information ofGandC, i.e.,MI(G,C)=/summationtext.1|G|i=1/summationtext.1|C|j=1P(i,j)lo/afii10069.ital/parenleftBigP(i,j)P(i)P(j)/parenrightBig.5 RESULTS AND ANALYSIS5.1 Answering RQ1The last column of Table 2 presents the performance of SIRA inproblematic feature extraction. The overall precision, recall and F1are 84.27%, 85.06% and 84.64% respectively, which indicates that84.27% of problematic features extracted by SIRA are correct, and85.06% problematic features are correctly extracted from the ground-truth ones. The results con!rm that our proposed approach canaccurately extract the problematic features.More speci!cally, SIRA reaches the highest precision of 90.27%onGmailand the highest recall of 87.37% onYahoo Mail. Its lowestprecision is 79.18% onYahoo Mailand the lowest recall is 84.15%onSnapchat. We can see that even with its worst performance, anacceptable precision and recall can be achieved.We then examine the extracted problematic features in detail,and!nd that there are indeed some observable patterns associated2433ICSE ’22, May 21–29, 2022, Pi/t_tsburgh, PA, USAWang, et al.Table 2: Evaluation on problematic feature extraction (RQ1).AppMetric MethodKEFECasparSAFEBiLSTM-CRFSIRAInstagramP40.32%16.26%14.17%80.24%83.59%R60.76%10.49%70.61%71.79%85.70%F148.29%12.46%23.55%75.58%84.53%SnapchatP42.08%18.87%12.95%78.49%82.63%R58.71%13.81%65.60%74.71%84.15%F148.70%15.74%21.59%76.47%83.30%GmailP53.79%25.60%22.25%87.58%90.27%R78.54%9.88%88.21%71.74%84.16%F163.46%14.12%35.49%78.81%87.09%YahooMailP12.57%18.26%12.57%74.45%79.18%R70.10%11.85%70.10%74.69%87.37%F121.25%14.19%21.25%74.26%83.00%BPIMobileP41.92%20.98%18.22%82.58%87.37%R62.75%9.24%77.05%73.53%85.07%F150.13%12.51%29.44%77.63%86.13%ChaseMobileP36.98%17.53%12.17%77.23%80.32%R52.85%13.38%64.85%68.43%84.59%F143.16%15.03%20.44%72.31%82.26%OverallP42.79%∗19.14%∗15.51%∗80.40%84.27%R63.50%∗11.27%∗73.94%∗∗72.48%∗85.06%F151.05%∗14.13%∗25.62%∗76.15%∗84.64%Compared to SIRA, statistical signi!cancep−/v.altalue<0.05is denoted by∗∗, andp−/v.altalue<0.01is denoted by∗.with the problematic features. For example, users would use somenegative words (e.g., “cannot”, “hardly”) or temporal conjunctions(e.g., “as soon as”, “when”) before mentioning the problematic fea-tures. This could probably explain why the pattern-based technique[12,16,27] could work sometimes. Taking the review in Figure1 as an example, extracting the phrases after the negative word“can’t” would obtain the correct phrase. However, the pattern-basedtechniques highly rely on the manually de!ned patterns and havepoor scalability in a di"erent dataset. Furthermore, there are manycircumstances when the pattern-based approach can hardly work.For example, it is quite demanding to design patterns for the fol-lowing review sentence: “this update takes away my ability to viewtransactions”, where the problematic feature is “view transaction”.These circumstances further prove the advantages and$exibilityof our approach.We also examine the bad cases where SIRA fails to work. In somecases, SIRA can extract the core nouns and verbs of the target phrase,but misses or additionally extracts some trivial words, especiallysome adverbs/adverbials before or after the core phrase. For exam-ple, SIRA might wrongly extract “received emails for 10 days” from“I have not received emails for 10 days”, where the ground-truthphrase is “received emails”. Such results pull down the performance.This could be improved by considering PoS patterns of words whenvectorizing review sentences in future work.Comparison with baselines.Table 2 presents the performanceof SIRA and four baselines in extracting problematic features. SIRAoutperforms all baselines on all metrics. This indicates that thesepattern-based baselines (i.e., KEFE, Caspar and SAFE) are far frome"ective in extracting problematic features, while the deep learning-based baseline (i.e., BiLSTM-CRF) is a bit worse than SIRA becauseof the inferior semantic understanding and neglect of review at-tributes. To further intuitively demonstrate the advantages of SIRA,Table 3 presents two example reviews and the corresponding prob-lematic features extracted by SIRA and four baselines.Among the three pattern-based baselines, SAFE achieves 15.51%precision and 73.94% recall. This is because it de!nes 18 PoS pat-terns for feature-related phrases, and can retrieve a large numberof possible problematic features (i.e., high recall). For example, inthe!rst example of Table 3, SAFE would return two phrases. Bycomparison, Caspar only extracts events from reviews containingtemporal conjunctions and key phrases, including “when”, “ev-ery time”, which can hardly work well in this context. Taking the!rst review in Table 3 as an example, Caspar can only extract thetwo phrases/clauses. KEFE achieves the promising performance,indicating that it can!lter away many low-quality phrases withthe BERT classi!er; yet the classi!cation is still conducted basedon candidate phrases extracted by a pattern-based method, whichlimits its performance. In the!rst example of Table 3, KEFE can!lter the wrong phrase “keeps crashing”, but the reserved phrase“take a picture” is still not accurate enough due to the drawback ofpattern-based candidate phrases. BiLSTM-CRF can achieve promis-ing performance but still not as accurate as our proposed SIRA, e.g.,“view story” in Table 3. SIRA can be regarded as an improved ver-sion of BiLSTM-CRF, which employs BERT!ne-tuning techniqueand two customized review attributes. The features extracted bySIRA is the superset of BiLSTM-CRF, which can be also re$ectedby the results in Table 2. SIRA outperforms BiLSTM-CRF in bothrecall and precision, indicating that SIRA can extract features moreaccurately and retrieve more problematic features.5.2 Answering RQ2Table 4 presents the performance of SIRA and its three variants,respectively. The overall performance of SIRA is higher than all thethree variants. Compared with the base BERT-CRF model, addingthe app category and the sentiment attributes noticeably increasethe precision (2.03%) and recall (6.74%). This indicates that the twoattributes are helpful in identifying the problematic features. For theperformance on each app, adding the two attributes (i.e., BERT+Attr-CRF) obtains the best performance on most apps, and adding oneof the two attributes (i.e., BERT+CAT-CRF or BERT+SEN-CRF)occasionally achieves the best performances on some apps (e.g.,BERT+SEN-CRF onSnapchat). Moreover, even the performance ofthe base BERT-CRF model outperforms the best baseline in RQ1 (i.e.,BiLSTM-CRF), which veri!es the advantage of our model design.Among the two added review attributes, the review descriptionsentiment attribute contributes slightly more to performance im-provement (1.64% in precision and 5.80% in recall) than the appcategory attribute (1.38% in precision and 5.26% in recall). Further-more, we also observe that the contribution of these two attributesoverlaps to some extent, i.e., the increased performance by eachattribute is not simply added up to the performance of the wholemodel. This is reasonable considering the fact that words express-ing the user sentiment could be encoded semantically in the textualdescriptions and captured by the BERT model. Nevertheless, theoverall performance achieved by adding both of the attributes isthe highest, further indicating the necessity of our model design.5.3 Answering RQ3Table 5 presents the performance of SIRA in clustering problematicfeatures, as well as the two baselines. SIRA outperforms the two2434Where is Your App Frustrating Users?ICSE ’22, May 21–29, 2022, Pi/t_tsburgh, PA, USATable 3: Examples on extracted problematic features by diﬀerent approaches (RQ1).#ReviewKEFECasparSAFEBiLSTM-CRFSIRA#1Keeps crashingwhen I try to take a picture of a check.take a picturekeeps crashing, I try to takea picture of a checkkeeps crashing,take a picturetake a pictureof a checktake a pictureof a check#2When I try to view story of friend, the majority ofthe time it get stuck on a wheel and never load.view storyI try to view story of friend,the majority of the time it getstuck on a wheel,never loadview storyview storyview story of friendTable 4: Ablation experiment on attributes (RQ2).AppMetric MethodBERT-CRFBERT+CAT-CRFBERT+SEN-CRFBERT+Attr-CRFInstagramP82.46%84.08%83.78%83.59%R80.39%85.60%85.50%85.70%F181.34%84.73%84.56%84.53%SnapchatP84.58%83.82%83.38%82.63%R81.49%83.31%85.31%84.15%F182.89%83.48%84.23%83.30%GmailP88.33%89.30%90.59%90.27%R78.37%83.43%83.50%84.16%F182.99%86.16%86.86%87.09%YahooMailP75.92%76.67%78.23%79.18%R83.72%83.72%86.09%87.37%F179.54%79.94%81.86%83.00%BPIMobileP84.87%85.92%85.52%87.37%R78.09%84.94%82.60%85.07%F181.25%85.32%83.96%86.13%ChaseMobileP78.24%80.26%80.05%80.32%R77.59%82.19%83.74%84.59%F177.73%81.11%81.76%82.26%OverallP82.59%83.73%83.95%84.27%R79.69%83.88%∗84.31%∗85.06%∗F181.10%83.78%∗∗84.10%∗∗84.64%∗Compared to BERT-CRF, statistical signi!cancep−/v.altalue<0.05is denotedby∗∗, andp−/v.altalue<0.01is denoted by∗.Table 5: Evaluation on problematic feature clustering (RQ3).AppMetricMethodLDAK-MeansSIRAInstagramARI0.100.300.29NMI0.720.780.84SnapchatARI0.190.130.32NMI0.800.720.85GmailARI0.180.070.45NMI0.730.580.82Yahoo MailARI0.420.470.41NMI0.810.830.82BPI MobileARI0.440.100.59NMI0.830.580.89Chase MobileARI0.380.210.26NMI0.810.790.82OverallARI0.210.140.38NMI0.570.620.77baselines on the overall performance, where ARI and NMI reach0.38 and 0.77, respectively, which is higher than that of LDA (0.21and 0.57) and K-Means (0.14 and 0.62).Furthermore, the improvement of SIRA on ARI is greater thanthe improvement on NMI. ARI is a pair-wise metric, which is moresensitive when two phrases that should belong to the same clusterare wrongly assigned into di"erent clusters, or when two phraseswhich should belong to di"erent clusters are wrongly placed intothe same cluster. The ARI results we obtained indicate that SIRA canTable 6: Experimental dataset for investigating “where theapps frustrating users”.CategoryApp# Reviews# SentencesSocialFacebook64,559147,156Instagram63,124153,852TikTok61,178104,094Snapchat18,26841,278Twitter15,58336,386Sina Weibo10,77237,372CommunicationFacebook-Messenger27,12159,303Gmail9,65524,520Telegram7,70417,672Yahoo Mail7,09020,124Skype3,2668,139Tencent QQ3,1947,326FinancePaytm18,31647,836Chase Mobile3,7329,952Alipay3,1539,359BPI Mobile1,3753,638BCA Mobile386960WavePay58124Overall318,534729,091e"ectively avoid generating new clusters or breaking up the originalclusters. NMI is an entropy-based metric, which mainly focuseson the changes of two distributions based on information entropytheory. The NMI results we obtained indicate that the distributionof the entire cluster (e.g., the number of problematic features ineach cluster) derived from SIRA are closer to the ground-truth.The baseline approaches use the word statistics or co-occurrencerelations to cluster the problematic features. The performance of ourproposed graph-based clustering method indicates that it can betterunderstand the semantic relations among problematic features.6 WHERE THE APPS FRUSTRATE USERS - ANEMPIRICAL STUDY WITH SIRAThis section describes a large-scale empirical study with SIRA onpopular apps. First, we apply SIRA to 18 apps of three categories(6 in each category) to demonstrate: 1) how SIRA can be utilizedin real-world practice; 2) the distribution of problematic featuresacross these popular apps. We also select 3 apps (1 in each category)and conduct a user survey to verify the usefulness of SIRA.SIRA in the Large.We crawl the app reviews of 18 apps fromthree categories (6 in each category) submitted during February2020 to December 2020 (note that this is di"erent from the timeperiod in Section 4.2). Table 6 lists the statistics of this dataset,which contains 318,534 reviews and 729,091 sentences. We runSIRA on this large-scale dataset to obtain the visualization of theclustered problematic features (see Section 3.4). In total, we obtain113 clusters for social apps, 78 clusters for communication apps2435ICSE ’22, May 21–29, 2022, Pi/t_tsburgh, PA, USAWang, et al.
(a) Social
(b) Communication
(c) FinanceFigure 4: The distribution of problematic features of diﬀerent categories.Table 7: Cluster name (i.e., representative problematic fea-ture) of each cluster in Figure 4.#SocialCommunicationFinanceC1the reel optiondelete emailsend messageC2like a postopen applog inC3search optionreceive noti!cationreceive otp codeC4load tweetsend and receive emailload the pageC5use!lterdark modecheck depositC6follow peopleload inboxget noti!cationC7the front camerasign into accountuse!nger printC8click on photosend picture and videoclick buttonC9send snapvideo calldo transactionC10receive noti!cationsee storytransfer moneyC11get live optionclick on call buttonget cash backC12post storysync accountscan qr codeC13access accountchange the emojiand nicknamerecharge mobilenumberC14open snapshare photochange phone numberC15send messageregister useropen passbookC16watch videochat with friendbook ticketC17dark modeget otp for loginselect optionC18scroll the feedreceive veri!cation codecheck balanceC19retrieve tweetquiz botmake paymentC20get veri!cation codechange phone numberreceive the refundand 90 clusters for!nance apps. Figure 4 presents the visualizationresults of clusters for each category with the bubble size denotingthe ratio of corresponding problematic features. For clarity, we onlypresent the clusters whose number of problematic features is intop 20, by the order of cluster id. Table 7 shows the name of eachcluster in Figure 4. The following observations can be obtained.First, our visualization can provide a summarized view of theproblematic features for each app and the comparison across apps.This enables the developers to acquire where the app is prone toproblems, and where other apps are also likely to have issues, witha single glance. One can also derive the detailed content of eachcluster, and example app reviews of the cluster by hovering thecursor over the bubble in the!gure (see examples in Figure 4(c)).Second, di"erent apps can share similar problematic features,which can facilitate app testing and re!ne the testing techniques.Take Figure 4(a) as an example, although the problematic featuresare observed distributing di"erently across apps, all the six in-vestigated apps would have a noticeable number of problematicfeatures in certain clusters (i.e.,C12. post storyandC13. access ac-count). These information can warn the developers of similar appsto notice potential problems, especially which have not yet beenreported or only mentioned in a few reviews. Further, developerscan leverage reviews from similar apps for quality assurance activi-ties, rather than only focus on the limited set of reviews of its ownapp. This is especially the case for the less popular apps which onlyhave few reviews regarding app problems.Third, di"erent apps can have their unique problematic featuresand problematic feature distributions, which further indicates thenecessity of review mining and analysis in a!ne-grained way. Forexample, from Figure 4(b), we can see that, based on the user re-ported problems, 63% reviews of theFacebook Messengerapp relatewith featureC8. send picture and video. By comparison, its competi-torGmailapp is mainly prone to bugs for quite di"erent featureC4. send and receive email. In addition, for its another competitorTelegramapp, the problematic features are distributed more evenly,i.e., the number of user submitted reviews do not exert big di"er-ence acrossC4,C7andC8, and the largest cluster (i.e.,C7. signinto account) occupies a mere of 33% reviews. From these insightsprovided by our approach, the developers can obtain a clear under-standing of an app about the features that are prone to problems, soas to arrange the follow-up problem solving and allocate the testingactivity for subsequent versions. More than that, these informationcan also assist the developers in the competitive analysis of apps,e.g., acquire the weakness of their app compared with similar apps.Furthermore, a series of attempts can be made to re!ne the apptesting techniques. For example, one can recommend problematicfeatures to similar apps in order to prioritize the testing e"ort, orrecommend related descriptions (mined from app reviews) to similarapps to help bug detection. In addition, the automated graphical userinterface (GUI) testing techniques can be customized and the testingcontents can be prioritized. Current automated GUI testing toolstend to dynamically explore di"erent pages of a mobile app throughrandom actions (e.g., clicking, scrolling, etc) to trigger the crash orexplicit exceptions [37]. If one could know the detailed problematicfeatures of other similar apps in advance, the explored pages canbe re-ranked so that the bug-prone features can be explored earlierto facilitate the bugs being revealed earlier. We will further exploreproblematic features based app testing in our future work.A User Survey.In order to assess the usefulness of SIRA, weconduct a user survey on three popular apps:Weibo,QQandAli-pay. We invite 15 respondents (5 from each company) in total,including 2 product managers, 5 requirement analysts, and 8 devel-opers, who are familiar with the app reviews of their own company.2436Where is Your App Frustrating Users?ICSE ’22, May 21–29, 2022, Pi/t_tsburgh, PA, USA
Figure 5: Feedback of user study.More speci!cally, we conduct SIRA on the reviews obtained in the!rst week of May 2021, which contains 177 reviews fromWeibo,149 fromQQ, and 177 fromAlipayafter preprocessing. Each re-spondent examines the extracted problematic features, clusters andvisualization results obtained by SIRA, and answer the followingthree questions: 1) (Usefulness) Can SIRA help understand userrequirements from app reviews? 2) (Extraction) Can SIRA extractedproblematic features accurately? 3) (Clustering) Can SIRA clus-ter problematic features accurately? We provide!ve options foreach question from 1 (strongly disagree) to 5 (strongly agree). The!rst question concerns the usefulness of SIRA, i.e., whether SIRAcan save e"ort for analyzing large-scale app reviews. The last twoquestions concern the performance of SIRA on problematic featureextraction and clustering respectively, when analyzing app reviewsin real-world practice.Figure 5 shows the box plot statistics of respondents’ feedback.There are respectively 11, 13 and 10 (out of 15) respondents givethe score over 3 for Q1, Q2, and Q3. Most of them (over 73%) aresatis!ed (score over 3) with the usefulness of SIRA, and think SIRAcan help them obtain a!ne-grained understanding on problematicfeatures. The average score of Q1, Q2, and Q3 are 3.93, 4.13, and 3.93respectively. Besides, three of them heard about or tried existingreview analysis tools such as INFAR [12] and SUR-Miner [15], andthey admit the advantages of SIRA as its extracted features andderived clusters are!ner-grained and more meaningful. We alsointerviewed the respondents about the possible enhancement ofSIRA. They said there were still some cases where SIRA doesn’twork well, such as some extracted phrases contain two or morefeatures, which leads to poor performance of clustering. This canbe solved in future work by exploring the patterns of such tangledfeatures and deconstructing them into separate ones. In addition, wereceived some suggestions from developers for better visualizations(e.g., supporting interactive visual analytics).7 DISCUSSIONAdvantage Over Topic Discovery Approaches.There are sev-eral previous approaches which involve topic discovery [12,15,52,57,58]. Yet, their discovered topics are more coarse-grainedthan our proposed approach. For example, based on 95 mobile appslikeFacebookandTwitterfrom Google Play, MARK [57] can onlydiscover such topics ascrash,compatibility, andconnection, andPUMA [58] generates topics likebattery consumption.Similarly,SUR-Miner [15] generates topics such aspredictions,auto-correct,andwords. SURF [52] can discover topics such asGUI,app, andcompany, while INFAR [12] can generate topics likeupdate,radar,download. With these discovered topics, the developers can acquirea general view about the problems the app undergoes, yet could notget a clear understanding about where it is wrong. By comparison,as demonstrated in Figure 4 and Table 7, our proposed approachcan generate more!ner-grained topics asopen message, get crashback, which helps developers achieve a deeper and more accurateunderstanding about where the app is wrong.Threats to Validity.Theexternal threatsconcern the gener-ality of the proposed approach. We train and evaluate SIRA on thedataset consisting of six apps from three categories. The selectedapps and their belonging categories are all the commonly-usedones with rich reviews in practice, which relatively reduces thisthreat. In addition, we demonstrate the usage of SIRA on a muchbigger dataset derived from 18 apps. The results are promising,which veri!es its generality further. Regardinginternal threats,SIRA is a pipeline method, where the problematic feature clusteringdepends on the accuracy of extracting problematic features. Sincewe have seen a relatively high performance of SIRA on problematicfeature extraction, we believe SIRA can alleviate the error accu-mulation to some extent. In addition, we reuse the source codefrom the original paper (i.e., forCasparandKEFE), or the opensource implementation (i.e., forSAFE,K-Means, andLDA) for thebaselines, which help ensure the accuracy of the experiments. Theconstruct validityof this study mainly questions the evaluationmetrics. We utilize precision, recall and F1-Score to evaluate theperformance of problematic feature extraction. We consider that aproblematic feature is correctly extracted when it is the same as theground-truth, which is a rather strict measure. The metrics used toevaluate clustering results are also commonly used [24].8 CONCLUSIONTo help acquire a concrete understanding about where the app isfrustrating the users, this paper proposes a semantic-aware,!ne-grained app review analysis approach SIRA, which can extract,cluster, and visualize the problematic features of app reviews. SIRAdesigns a novel BERT+Attr-CRF model to extract!ne-grained prob-lematic features, and employs a graph-based clustering method tocluster them. We evaluate SIRA on 3,426 reviews from six apps,and the results con!rm the e"ectiveness of the proposed approach.We further conduct an empirical study on 318,534 reviews from18 popular apps to explore its potential application and usefulnessin real-world practice. Our source code and experimental data arepublicly available at: https://github.com/MeloFancy/SIRA.ACKNOWLEDGMENTSThis work is supported by the National Key Research and De-velopment Program of China under grant No.2018YFB1403400,the National Natural Science Foundation of China under grantNo.62072442, the Youth Innovation Promotion Association ChineseAcademy of Sciences, and Australian Research Council DiscoveryProject DP220103044.2437ICSE ’22, May 21–29, 2022, Pi/t_tsburgh, PA, USAWang, et al.REFERENCES[1] 2014. Viterbi Algorithm. https://en.wikipedia.org/wiki/Viterbi_algorithm.[2]Raul Berrios, Peter Totterdell, and Stephen Kellett. 2015. Eliciting mixed emotions:a meta-analysis comparing models, types, and measures.Frontiers in psychology6 (2015), 428.[3]C. Biemann. 2006. Chinese whispers: An eﬃcient graph clustering algorithmand its application to natural language processing problems.Association forComputational Linguistics(2006).[4]David M. Blei, Andrew Y. Ng, and Michael I. Jordan. 2001. Latent DirichletAllocation. InAdvances in Neural Information Processing Systems 14 [NeuralInformation Processing Systems: Natural and Synthetic, NIPS 2001, December 3-8, 2001, Vancouver, British Columbia, Canada], Thomas G. Dietterich, SuzannaBecker, and Zoubin Ghahramani (Eds.). MIT Press, 601–608.[5]Daniel Cer, Yinfei Yang, Sheng-yi Kong, Nan Hua, Nicole Limtiaco, Rhomni St.John, Noah Constant, Mario Guajardo-Cespedes, Steve Yuan, Chris Tar, BrianStrope, and Ray Kurzweil. 2018. Universal Sentence Encoder for English. InProceedings of the 2018 Conference on Empirical Methods in Natural LanguageProcessing, EMNLP 2018: System Demonstrations, Brussels, Belgium, October 31 - No-vember 4, 2018, Eduardo Blanco and Wei Lu (Eds.). Association for ComputationalLinguistics, 169–174.[6]Ning Chen, Jialiu Lin, Steven C. H. Hoi, Xiaokui Xiao, and Boshen Zhang. 2014.AR-miner: mining informative reviews for developers from mobile app market-place. In36th International Conference on Software Engineering, ICSE ’14, Hyder-abad, India - May 31 - June 07, 2014. 767–778.[7]Ronan Collobert, Jason Weston, Léon Bottou, Michael Karlen, Koray Kavukcuoglu,and Pavel P. Kuksa. 2011. Natural Language Processing (Almost) from Scratch.J.Mach. Learn. Res.12 (2011), 2493–2537.[8]Nathan Cooper, Carlos Bernal-Cárdenas, Oscar Chaparro, Kevin Moran, andDenys Poshyvanyk. 2021. It Takes Two to Tango: Combining Visual and TextualInformation for Detecting Duplicate Video-Based Bug Reports. InInternationalConference on Software Engineering (ICSE).[9]Hong-Jie Dai, Po-Ting Lai, Yung-Chun Chang, and Richard Tzong-Han Tsai. 2015.Enhancing of chemical compound and drug name recognition using representa-tive tag scheme and!ne-grained tokenization.J. Cheminformatics7, S-1 (2015),S14.[10]Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT:Pre-training of Deep Bidirectional Transformers for Language Understanding. InProceedings of the 2019 Conference of the North American Chapter of the Associa-tion for Computational Linguistics: Human Language Technologies, NAACL-HLT2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers), JillBurstein, Christy Doran, and Thamar Solorio (Eds.). Association for Computa-tional Linguistics, 4171–4186.[11]J Felsenstein and G A Churchill. 1996. A Hidden Markov Model approach tovariation among sites in rate of evolution.Molecular Biology and Evolution13, 1(01 1996), 93–104.[12]Cuiyun Gao, Jichuan Zeng, David Lo, Chin-Yew Lin, Michael R. Lyu, and IrwinKing. 2018. INFAR: insight extraction from app reviews. InProceedings of the 2018ACM Joint Meeting on European Software Engineering Conference and Symposiumon the Foundations of Software Engineering, ESEC/SIGSOFT FSE 2018, Lake BuenaVista, FL, USA, November 04-09, 2018. 904–907.[13]Cuiyun Gao, Jichuan Zeng, Michael R. Lyu, and Irwin King. 2018. Online appreview analysis for identifying emerging issues. InProceedings of the 40th Inter-national Conference on Software Engineering, ICSE 2018, Gothenburg, Sweden, May27 - June 03, 2018, Michel Chaudron, Ivica Crnkovic, Marsha Chechik, and MarkHarman (Eds.). ACM, 48–58.[14]Cuiyun Gao, Jichuan Zeng, Xin Xia, David Lo, Michael R. Lyu, and Irwin King.2019. Automating App Review Response Generation. In34th IEEE/ACM Interna-tional Conference on Automated Software Engineering, ASE 2019, San Diego, CA,USA, November 11-15, 2019. IEEE, 163–175.[15]Xiaodong Gu and Sunghun Kim. 2015. "What Parts of Your Apps are Lovedby Users?". In30th IEEE/ACM International Conference on Automated SoftwareEngineering, ASE 2015, Lincoln, NE, USA, November 9-13, 2015. 760–770.[16]Hui Guo and Munindar P. Singh. 2020. Caspar: extracting and synthesizing userstories of problems from app reviews. InICSE ’20: 42nd International Conferenceon Software Engineering, Seoul, South Korea, 27 June - 19 July, 2020. 628–640.[17] Jin Guo, Jinghui Cheng, and Jane Cleland-Huang. 2017. Semantically enhancedsoftware traceability using deep learning techniques. InProceedings of the 39th In-ternational Conference on Software Engineering, ICSE 2017, Buenos Aires, Argentina,May 20-28, 2017. 3–14.[18]Emitza Guzman, Rana Alkadhi, and Norbert Sey". 2017. An exploratory study ofTwitter messages about software applications.Requir. Eng.22, 3 (2017), 387–412.[19]Emitza Guzman and Walid Maalej. 2014. How Do Users Like This Feature? AFine Grained Sentiment Analysis of App Reviews. InIEEE 22nd InternationalRequirements Engineering Conference, RE 2014, Karlskrona, Sweden, August 25-29,2014, Tony Gorschek and Robyn R. Lutz (Eds.). IEEE Computer Society, 153–162.[20]Rui Hao, Yang Feng, James Jones, Yuying Li, and Zhenyu Chen. 2019. CTRAS:Crowdsourced test report aggregation and summarization. InICSE’2019. 921–932.[21]Mark Harman, Yue Jia, and Yuanyuan Zhang. 2012. App store mining andanalysis: MSR for app stores. In9th IEEE Working Conference of Mining SoftwareRepositories, MSR 2012, June 2-3, 2012, Zurich, Switzerland. 108–111.[22]Jeremy Howard and Sebastian Ruder. 2018. Universal Language Model Fine-tuning for Text Classi!cation. InProceedings of the 56th Annual Meeting of theAssociation for Computational Linguistics, ACL 2018, Melbourne, Australia, July15-20, 2018, Volume 1: Long Papers, Iryna Gurevych and Yusuke Miyao (Eds.).Association for Computational Linguistics, 328–339.[23]Hao Hu, Hongyu Zhang, Jifeng Xuan, and Weigang Sun. 2014. E"ective BugTriage Based on Historical Bug-Fix Information. In2014 IEEE 25th InternationalSymposium on Software Reliability Engineering. 122–132.[24]Yi Huang, Chunyang Chen, Zhenchang Xing, Tian Lin, and Yang Liu. 2018.Tell them apart: distilling technology di"erences from crowd-scale comparisondiscussions. InProceedings of the 33rd ACM/IEEE International Conference onAutomated Software Engineering, ASE 2018, Montpellier, France, September 3-7,2018, Marianne Huchard, Christian Kästner, and Gordon Fraser (Eds.). ACM,214–224.[25]Zhiheng Huang, Wei Xu, and Kai Yu. 2015. Bidirectional LSTM-CRF Models forSequence Tagging.CoRRabs/1508.01991 (2015).[26]Md Rakibul Islam and Minhaz F. Zibran. 2018. SentiStrength-SE: Exploitingdomain speci!city for improved sentiment analysis in software engineering text.J. Syst. Softw.145 (2018), 125–146.[27]Timo Johann, Christoph Stanik, Alireza M. Alizadeh B., and Walid Maalej. 2017.SAFE: A Simple Approach for Feature Extraction from App Descriptions andApp Reviews. In25th IEEE International Requirements Engineering Conference, RE2017, Lisbon, Portugal, September 4-8, 2017. 21–30.[28]Wang Junjie, Cui Qiang, Wang Song, and Wang Qing. 2017. Domain Adaptationfor Test Report Classi!cation in Crowdsourced Testing. InICSE’17. 83–92.[29]Kyo Kang, Sholom Cohen, James Hess, William Novak, and A. Peterson. 1990.Feature-Oriented Domain Analysis (FODA) Feasibility Study. Technical ReportCMU/SEI-90-TR-021. Software Engineering Institute, Carnegie Mellon University,Pittsburgh, PA.[30]Hammad Khalid, Emad Shihab, Meiyappan Nagappan, and Ahmed E. Hassan.2015. What Do Mobile App Users Complain About?IEEE Softw.32, 3 (2015),70–77.[31]Diederik P. Kingma and Jimmy Ba. 2015. Adam: A Method for Stochastic Opti-mization. In3rd International Conference on Learning Representations, ICLR 2015,San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings.[32]Ron Kohavi. 1995. A Study of Cross-Validation and Bootstrap for AccuracyEstimation and Model Selection. InProceedings of the Fourteenth InternationalJoint Conference on Arti/f_icial Intelligence, IJCAI 95, Montréal Québec, Canada,August 20-25 1995, 2 Volumes. 1137–1145.[33]Zijad Kurtanovic and Walid Maalej. 2018. On user rationale in software engi-neering.Requir. Eng.23, 3 (2018), 357–379.[34]John D. La"erty, Andrew McCallum, and Fernando C. N. Pereira. 2001. Condi-tional Random Fields: Probabilistic Models for Segmenting and Labeling SequenceData. InProceedings of the Eighteenth International Conference on Machine Learn-ing (ICML 2001), Williams College, Williamstown, MA, USA, June 28 - July 1, 2001,Carla E. Brodley and Andrea Pohoreckyj Danyluk (Eds.). Morgan Kaufmann,282–289.[35]Lawrence, HubertPhipps, and Arabie. 1985. Comparing partitions.Journal ofClassi/f_ication(1985).[36]Sun-Ro Lee, Min-Jae Heo, Chan-Gun Lee, Milhan Kim, and Gaeul Jeong. 2017.Applying deep learning based automatic bug triager to industrial projects. InProceedings of the 2017 11th Joint Meeting on Foundations of Software Engineering,ESEC/FSE 2017, Paderborn, Germany, September 4-8, 2017. 926–931.[37]Yuanchun Li, Ziyue Yang, Yao Guo, and Xiangqun Chen. 2017. DroidBot: alightweight UI-guided test input generator for Android. InProceedings of the39th International Conference on Software Engineering, ICSE 2017, Buenos Aires,Argentina, May 20-28, 2017 - Companion Volume. 23–26.[38]Bin Lin, Fiorella Zampetti, Gabriele Bavota, Massimiliano Di Penta, and MicheleLanza. 2019. Pattern-based mining of opinions in Q&A websites. InProceedingsof the 41st International Conference on Software Engineering, ICSE 2019, Montreal,QC, Canada, May 25-31, 2019, Joanne M. Atlee, Tev!k Bultan, and Jon Whittle(Eds.). IEEE / ACM, 548–559.[39]Hui Liu, Mingzhu Shen, Jiahao Jin, and Yanjie Jiang. 2020. Automated classi!ca-tion of actions in bug reports of mobile apps. InISSTA ’20: 29th ACM SIGSOFTInternational Symposium on Software Testing and Analysis, Virtual Event, USA,July 18-22, 2020. 128–140.[40]Walid Maalej and Hadeer Nabil. 2015. Bug report, feature request, or simplypraise? On automatically classifying app reviews. In23rd IEEE InternationalRequirements Engineering Conference, RE 2015, Ottawa, ON, Canada, August 24-28,2015. 116–125.[41]Yichuan Man, Cuiyun Gao, Michael R. Lyu, and Jiuchun Jiang. 2016. ExperienceReport: Understanding Cross-Platform App Issues from User Reviews. In27thIEEE International Symposium on Software Reliability Engineering, ISSRE 2016,Ottawa, ON, Canada, October 23-27, 2016. 138–149.[42] William J. Martin, Federica Sarro, Yue Jia, Yuanyuan Zhang, and Mark Harman.2017. A Survey of App Store Analysis for Software Engineering.IEEE Trans.2438Where is Your App Frustrating Users?ICSE ’22, May 21–29, 2022, Pi/t_tsburgh, PA, USASoftware Eng.43, 9 (2017), 817–847.[43]Andrew McCallum, Dayne Freitag, and Fernando C. N. Pereira. 2000. MaximumEntropy Markov Models for Information Extraction and Segmentation. InPro-ceedings of the Seventeenth International Conference on Machine Learning (ICML2000), Stanford University, Stanford, CA, USA, June 29 - July 2, 2000, Pat Langley(Ed.). Morgan Kaufmann, 591–598.[44]Xuan Vinh Nguyen, Julien Epps, and James Bailey. 2010. Information TheoreticMeasures for Clusterings Comparison: Variants, Properties, Normalization andCorrection for Chance.J. Mach. Learn. Res.11 (2010), 2837–2854.[45]Ehsan Noei, Daniel Alencar da Costa, and Ying Zou. 2018. Winning the appproduction rally. InProceedings of the 2018 ACM Joint Meeting on EuropeanSoftware Engineering Conference and Symposium on the Foundations of SoftwareEngineering, ESEC/SIGSOFT FSE 2018, Lake Buena Vista, FL, USA, November 04-09,2018. 283–294.[46]Emanuel Oehri and Emitza Guzman. 2020. Same Same but Di"erent: FindingSimilar User Feedback Across Multiple Platforms and Languages. In28th IEEEInternational Requirements Engineering Conference, RE 2020, Zurich, Switzerland,August 31 - September 4, 2020, Travis D. Breaux, Andrea Zisman, Samuel Fricker,and Martin Glinz (Eds.). IEEE, 44–54.[47]Fabio Palomba, Mario Linares Vásquez, Gabriele Bavota, Rocco Oliveto, Massim-iliano Di Penta, Denys Poshyvanyk, and Andrea De Lucia. 2015. User reviewsmatter! Tracking crowdsourced reviews tosupport evolution of successful apps.In2015 IEEE International Conference on Software Maintenance and Evolution,ICSME 2015, Bremen, Germany, September 29 - October 1, 2015. 291–300.[48]Sebastiano Panichella, Andrea Di Sorbo, Emitza Guzman, Corrado Aaron Vis-aggio, Gerardo Canfora, and Harald C. Gall. 2015. How can i improve my app?Classifying user reviews for software maintenance and evolution. In2015 IEEEInternational Conference on Software Maintenance and Evolution, ICSME 2015,Bremen, Germany, September 29 - October 1, 2015. 281–290.[49]Ian Porteous, David Newman, Alexander T. Ihler, Arthur U. Asuncion, PadhraicSmyth, and Max Welling. 2008. Fast collapsed gibbs sampling for latent dirichletallocation. InProceedings of the 14th ACM SIGKDD International Conference onKnowledge Discovery and Data Mining, Las Vegas, Nevada, USA, August 24-27,2008, Ying Li, Bing Liu, and Sunita Sarawagi (Eds.). ACM, 569–577.[50]Lev-Arie Ratinov and Dan Roth. 2009. Design Challenges and Misconceptionsin Named Entity Recognition. InProceedings of the Thirteenth Conference onComputational Natural Language Learning, CoNLL 2009, Boulder, Colorado, USA,June 4-5, 2009, Suzanne Stevenson and Xavier Carreras (Eds.). ACL, 147–155.[51]Gerard Salton and Michael McGill. 1984.Introduction to Modern InformationRetrieval. McGraw-Hill Book Company.[52]Andrea Di Sorbo, Sebastiano Panichella, Carol V. Alexandru, Junji Shimagaki,Corrado Aaron Visaggio, Gerardo Canfora, and Harald C. Gall. 2016. What wouldusers change in my app? summarizing app reviews for recommending softwarechanges. InProceedings of the 24th ACM SIGSOFT International Symposium onFoundations of Software Engineering, FSE 2016, Seattle, WA, USA, November 13-18,2016. 499–510.[53]Aonan Tang, David Jackson, Jon Hobbs, Wei Chen, Jodi L. Smith, Hema Pa-tel, Anita Prieto, Dumitru Petrusca, Matthew I. Grivich, Alexander Sher, PawelHottowy, Wladyslaw Dabrowski, Alan M. Litke, and John M. Beggs. 2008. AMaximum Entropy Model Applied to Spatial and Temporal Correlations fromCortical Networks In Vitro.Journal of Neuroscience28, 2 (2008), 505–518.[54]Gias Uddin and Foutse Khomh. 2017. Opiner: an opinion search and summariza-tion engine for APIs. InProceedings of the 32nd IEEE/ACM International Conferenceon Automated Software Engineering, ASE 2017, Urbana, IL, USA, October 30 - No-vember 03, 2017, Grigore Rosu, Massimiliano Di Penta, and Tien N. Nguyen (Eds.).IEEE Computer Society, 978–983.[55]Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is All youNeed. InAdvances in Neural Information Processing Systems 30: Annual Conferenceon Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach,CA, USA, Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach,Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett (Eds.). 5998–6008.[56]Lorenzo Villarroel, Gabriele Bavota, Barbara Russo, Rocco Oliveto, and Massim-iliano Di Penta. 2016. Release planning of mobile apps based on user reviews.InProceedings of the 38th International Conference on Software Engineering, ICSE2016, Austin, TX, USA, May 14-22, 2016. 14–24.[57]Phong Minh Vu, Tam The Nguyen, Hung Viet Pham, and Tung Thanh Nguyen.2015. Mining User Opinions in Mobile App Reviews: A Keyword-Based Approach.In30th IEEE/ACM International Conference on Automated Software Engineering,ASE 2015, Lincoln, NE, USA, November 9-13, 2015. 749–759.[58]Phong Minh Vu, Hung Viet Pham, Tam The Nguyen, and Tung Thanh Nguyen.2016. Phrase-based extraction of user opinions in mobile app reviews. InPro-ceedings of the 31st IEEE/ACM International Conference on Automated SoftwareEngineering, ASE 2016, Singapore, September 3-7, 2016, David Lo, Sven Apel, andSarfraz Khurshid (Eds.). ACM, 726–731.[59]Han Wang, Chunyang Chen, Zhenchang Xing, and John C. Grundy. 2020.Di"Tech: a tool for di"erencing similar technologies from question-and-answerdiscussions. InESEC/FSE ’20: 28th ACM Joint European Software Engineering Con-ference and Symposium on the Foundations of Software Engineering, Virtual Event,USA, November 8-13, 2020. 1576–1580.[60]Junjie Wang, Mingyang Li, Song Wang, Tim Menzies, and Qing Wang. 2019.Images don’t lie: Duplicate crowdtesting reports detection with screenshot infor-mation.Inf. Softw. Technol.110 (2019), 139–155.[61]Huayao Wu, Wenjun Deng, Xintao Niu, and Changhai Nie. 2021. Identifying KeyFeatures from App User Reviews. In43rd IEEE/ACM International Conference onSoftware Engineering, ICSE 2021, Madrid, Spain, 22-30 May 2021. IEEE, 922–932.[62]Xin Xia, David Lo, Ying Ding, Jafar M. Al-Kofahi, Tien N. Nguyen, and XinyuWang. 2017. Improving Automated Bug Triaging with Specialized Topic Model.IEEE Trans. Software Eng.43, 3 (2017), 272–297.[63]Liang Xu, Qianqian Dong, Cong Yu, Yin Tian, Weitang Liu, Lu Li, and XuanweiZhang. 2020. CLUENER2020: Fine-grained Name Entity Recognition for Chinese.arXiv preprint arXiv:2001.04351(2020).[64]Jian Zhang, Xu Wang, Hongyu Zhang, Hailong Sun, and Xudong Liu. 2020.Retrieval-based neural source code summarization. InICSE ’20: 42nd InternationalConference on Software Engineering, Seoul, South Korea, 27 June - 19 July, 2020.1385–1397.[65]Wei Zhang, Hong Mei, and Haiyan Zhao. 2006. Feature-driven requirementdependency analysis and high-level software design.Requir. Eng.11, 3 (2006),205–220.[66]Yuan Zhang, Hongshen Chen, Yihong Zhao, Qun Liu, and Dawei Yin. 2018.Learning Tag Dependencies for Sequence Tagging. InProceedings of the Twenty-Seventh International Joint Conference on Arti/f_icial Intelligence, IJCAI 2018, July13-19, 2018, Stockholm, Sweden, Jérôme Lang (Ed.). ijcai.org, 4581–4587.[67]Jian Zhou and Hongyu Zhang. 2012. Learning to Rank Duplicate Bug Reports. InProceedings of the 21st ACM International Conference on Information and Knowl-edge Management (CIKM ’12). Association for Computing Machinery, New York,NY, USA, 852–861.
2439