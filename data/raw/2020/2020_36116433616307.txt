A PracticalHuman Labeling Method for Online Just-in-Time
So/f_twareDefect Prediction
LiyanSong
songly@sustech.edu.cn
SouthernUniversity ofScienceand Technology
Shenzhen,ChinaLeandroLeiMinku∗
l.l.minku@bham.ac.uk
TheUniversity ofBirmingham
Birmingham,UK
CongTeng
12132358@mail.sustech.edu.cn
SouthernUniversity ofScienceand Technology
Shenzhen,ChinaXinYao∗
xiny@sustech.edu.cn
SouthernUniversity ofScienceand Technology
Shenzhen,China
ABSTRACT
Just-in-TimeSoftwareDefectPrediction(JIT-SDP)canbeseenas
an online learning problem where additional software changes
produced over time may be labeled and used to create training
examples. These training examples form a data stream that can
be used to update JIT-SDP models in an attempt to avoid mod-
elsbecomingobsoleteandpoorlyperforming.However,labeling
procedures adopted in existing online JIT-SDP studies implicitly
assumethatpractitionerswouldnotinspectsoftwarechangesupon
adefect-inducingprediction,delayingtheproductionoftraining
examples. This is inconsistent with a real-world scenario where
practitioners would adopt JIT-SDP models and inspect certain soft-
warechangespredictedasdefect-inducingtocheckwhetherthey
really induce defects. Such inspection means that some software
changes would be labeled much earlier than assumed in existing
work,potentiallyleadingtodiﬀerentJIT-SDPmodelsandperfor-
mance results. This paper aims at formulating a more practical
humanlabelingprocedurethat takesintoaccounttheadoptionof
JIT-SDPmodelsduringthesoftwaredevelopmentprocess.Itthen
analyseswhetherandtowhatextentitwouldimpactthepredictive
performance of JIT-SDP models. We also propose a new method
totargetthelabelingofsoftwarechangeswiththeaimofsaving
human inspection eﬀort. Experiments based on 14 GitHub projects
revealed that adopting a more realistic labeling procedure led to
signiﬁcantlyhigherpredictiveperformancethanwhendelayingthe
labelingprocess,meaningthatexistingworkmayhavebeenunder-
estimatingtheperformanceofJIT-SDP.Inaddition,ourproposed
method to target the labeling process was able to reduce human
eﬀortwhilemaintainingpredictiveperformancebyrecommending
∗Liyan Song, Cong Teng, and Xin Yao (Corresponding Author) are with Research
InstituteofTrustworthyAutonomousSystems,SouthernUniversityofScienceand
Technology, Shenzhen, China and Guangdong Provincial Key Laboratory of Brain-
inspired Intelligent Computation, Department of Computer Science and Engineering,
Southern Universityof Scienceand Technology, Shenzhen,China.
Leandro Lei Minku (Corresponding Author) is with School of Computer Science,the
Universityof Birmingham, Edgbaston, Birmingham, UK
ESEC/FSE ’23, December 3–9, 2023, San Francisco, CA,USA
©2023 Copyright heldby theowner/author(s).
ACM ISBN 979-8-4007-0327-0/23/12.
https://doi.org/10.1145/3611643.3616307practitioners to inspect software changes that are more likely to
inducedefects.Weencouragetheadoptionofmorerealistichuman
labeling methods in research studies to obtain an evaluation of
JIT-SDP predictive performance that is closerto reality.
CCSCONCEPTS
•Softwareanditsengineering →Riskmanagement ;Software
defect analysis ;•Computing methodologies →Online learn-
ing settings ;Classiﬁcation and regressiontrees ;Bagging.
KEYWORDS
Just-in-timesoftwaredefectprediction,onlinelearning,veriﬁcation
latency,waitingtime,human labeling,human inspection
ACM Reference Format:
Liyan Song, Leandro Lei Minku, Cong Teng, and Xin Yao. 2023. A Practical
HumanLabelingMethodforOnlineJust-in-TimeSoftwareDefectPrediction.
InProceedings of the 31st ACM Joint European Software Engineering Confer-
ence and Symposium on the Foundations of Software Engineering (ESEC/FSE
’23), December 3–9, 2023, San Francisco, CA, USA. ACM, New York, NY, USA,
13pages.https://doi.org/10.1145/3611643.3616307
1 INTRODUCTION
Just-in-Time Software Defect Prediction (JIT-SDP) is a type of SDP
specialized at the code change level with the aim of predicting
whether or not a software change is defect-inducing at commit
time (just-in-time) [ 19,20]. It is of practical relevance as a decision
supporting tool for improving software quality by automatically
alerting developers of potential defects at a very early stage, as
soon as a software change is produced, and at a relatively ﬁne
granularitycomparedtomodule-basedSDP.Assuch,ithasbeen
attractingincreasinginterestfrombothacademia[ 54]andindustry
[29,38,48]. From the machine learning viewpoint, JIT-SDP can
be seen as a binary classiﬁcation problem for which models are
constructedbasedontrainingexampleslabeledas defect-inducing
(class 1) or clean(class 0) thatcan then be used topredict whether
ornot newsoftware changes wouldinducedefects.
Inpractice,trainingexamplescorrespondingtosoftwarechanges
arrive sequentiallyinorder overtimeandthus JIT-SDP shouldbe
takenasanonlinelearningtask,whereJIT-SDPmodelsareupdated
withnew incoming training examples [ 4,27,45].Existing litera-
ture has revealed that updating JIT-SDP models with such training
examples(sothatthemodelscancapturethelatestdatageneration
Thiswork islicensedunderaCreativeCommonsAttribution4.0Interna-
tional License.
605
ESEC/FSE ’23, December3–9, 2023,San Francisco, CA, USA Liyan Song,Leandro Lei Minku,CongTeng, XinYao
status) canleadtobetterpredictiveperformancecomparedtomod-
els built with obsolete training examples [ 27]. Updating JIT-SDP
modelsovertimeisparticularlyimportantgiventheexistenceof
conceptdrift ,whicharechangestakingplaceinthecodedefectgen-
eration process. Concept drift has been shown to occur in JIT-SDP
[3,4] and may signiﬁcantlydeteriorate predictive performanceof
JIT-SDPmodels if they are not updatedover time [ 27].
DealingwithconceptdriftinJIT-SDPisneverthelessparticularly
challengingastheactuallabelsofsoftwarechangesonlybecome
available long after their commit time, an issue referred to as veri-
ﬁcationlatency inmachinelearning[ 9,11,24,43].Thiscandelay
the model updates, potentially aﬀecting the ability of such models
to react to concept drift. In particular, a training example can only
belabeledasacleansoftwarechangeafterenoughtimehaspassed
from its commit time for one to be conﬁdent on its clean status, or
as a defect-inducing training example when a defect is found to be
associatedtothischange,whicheverisshorter[ 4].Suchlabeling
process is referred to as the waiting time method and has been
adoptedinrecent onlineJIT-SDPstudies[ 3,4,42].
However,suchlabelingprocessimplicitlyassumesthatpracti-
tionerswouldnotinspectsoftwarechangesuponadefect-inducing
prediction.Thisisinconsistentwithareal-worldscenariowhere
developersadoptJIT-SDPmodelsduringthesoftwaredevelopment
process. Speciﬁcally, when a JIT-SDP model predicts a new soft-
ware change to be defect-inducing, the developer who has just
completed the change may inspect the change to check whether
or not it really induces any defect. Ignoring such inspection means
assuming that practitioners would always completely ignore the
warnings given by the JIT-SDP model and wait until defects are
foundmuchlater,whenthedefectismorediﬃculttobeﬁxed.In
other words, it means completely ignoring the main point of using
JIT-SDPmodelsinpractice,whichistoinspectsoftwarechanges
predicted as defect-inducing at (or close to) commit time, in an
attempt to eliminate their defects when the code change is still
fresh in the developers’ minds. This may in turn negatively impact
theevaluationofpredictiveperformanceofJIT-SDPmodels,asit
resultsinadelayinthelabelingprocessthatislargerthanthedelay
that would likely occur in practice, potentially preventing JIT-SDP
models from reactingto concept drift inamore timely manner.
Therefore, a more realistic labeling procedure that takes into
account the adoption of JIT-SDP models during the software de-
velopment process and the resulting human inspection of software
changespredictedasdefect-inducingshouldalsobetakenasanim-
portantpartoftheJIT-SDPprocess.Thispaperaimsatformulating
such labeling procedure and investigating whether and to what ex-
tentitwouldimpactthepredictiveperformanceofJIT-SDPmodels.
We refer to this labeling procedure as Immediate HumanLabeling
forSoftwareChangesPredictedasDefect-Inducing(HumLa).To-
getherwiththedelayedwaitingtimemethod,itformsalabeling
processthatisclosertotherealitythatwouldbeadoptedinpractice
whenJIT-SDPmodelsareusedduringthesoftwaredevelopment
process. It is noteworthy that the term “immediate" in our paper
does not mean that the developers has to give up all other work
that they had planned in their schedule to immediately inspect the
softwarechange.Whatwemeanisthatthedeveloperwilllabelthe
change as partof the process of inspecting thischange atan early
stage(which is inherent fromthe adoptionof JIT-SDP), and this isa more immediate labeling than the waiting time procedure. More-
over,theprocessofinspectingachangepredictedasdefect-inducing
isthesameasthelabelingprocess.Inotherwords,ifadeveloper
opts for inspecting a change predicted as defect inducing as part
oftheadoptionofJIT-SDPintheirproject/organization,labeling
this software change does not increase this eﬀort further. That
said,inspectingallsoftwarechangesthatarepredictedasdefect-
inducing as part of the adoption of JIT-SDP could result in high
inspection/labeling eﬀort. So, we also propose a human labeling
methodtosavehumanlabelingcostsbyhelpingpractitionerstotar-
gettheirinspectioneﬀorttowardsspeciﬁcdefect-predictedchanges,
calledEﬀort-Conservative HumanLabeling(ECo-Humla).
Our study answers the following Research Questions(RQs):
RQ1HowtoformulateaJIT-SDPlabelingmethodwithimmediate
human labelingofchanges predictedas defect-inducing?
RQ1.1How does this labeling method aﬀect the JIT-SDP predic-
tiveperformancecomparedto thedelayedwaitingtime
methodfor JIT-SDP?
RQ1.2To what extent the quality of human labels impacts the
predictive performance when using this method?
RQ1.3Howdoestheamountofhumaneﬀortaﬀectthepredictive
performance when using this method?
RQ2How can we target the labeling process towards speciﬁc
softwarechangestoreducetheamountofrequiredhuman
inspection eﬀort?
RQ2.1How does this labeling method aﬀect the JIT-SDP predic-
tive performance?
RQ2.2 Howmuchhuman eﬀortcan this methodsave?
RQ2.3Howhelpfulisthismethodinencouragingdefectstobe
foundwhen savingeﬀort?
The main contributionsof this paper are listedbelow:
•We are the ﬁrst to formulate the immediate human labeling
method (HumLa) into JIT-SDP, being closer to the reality
that wouldbe adoptedinpractice(RQ1).
•We show based on experiments with 14 datasets that this
practicallabelingmethodcansigniﬁcantlybeneﬁtpredictive
performanceofJIT-SDP when thehuman label quality and
quantity are above a given threshold (RQ1.1-1.3). Studies
thatdonottakesuchlabelingprocessintoaccountmaythus
beunderestimatingthepredictiveperformanceofJIT-SDP
models that wouldbe obtainedinpractice.
•We propose an eﬀort-conservative human labeling method
(ECo-HumLa)toprioritizethemostconﬁdentlydefect-pre-
dictedsoftware changes for practitioners to inspect(RQ2).
•We show experimentally that ECo-HumLa can substantially
reducehumaneﬀortbyaround50%whilemaintainingJIT-
SDP predictive performance (RQ2.1-2.2). ECo-HumLa en-
courages a higher number of defects to be found through
inspection thanabaselineeﬀortreduction method(RQ2.3).
2 RELATED WORK
2.1 JIT-SDP
EarlystudiesusuallymodeledJIT-SDPasanoﬄinelearningtask,
assumingthatalltrainingexamplesareavailablebeforehandand
nofurtheradjustmentorevaluationoftheJIT-SDPmodelswould
606A Practical Human Labeling MethodforOnline Just-in-TimeSo/f_tware DefectPrediction ESEC/FSE ’23, December3–9, 2023,San Francisco, CA, USA
happen over time. A landmark study is that of Kamei et al., who
summarized14featuresextractedfromcommitsandbugreports
andshowedthemtobegoodindicatorsforyieldinghighpredictive
performanceinJIT-SDP[ 19].Manylaterstudieswereconducted
basedonthesefeatures[ 17,28].Variouslearningmachineshave
been investigated for (JIT-)SDP, among which tree-based methods
wereshowntohavepotentialinyieldinggoodperformance[ 13,17,
18,25,46,51].Techniquessuchasoversamplingandundersampling
[33] have alsobeen adoptedto deal with the class imbalance issue
usuallysuﬀeredbyJIT-SDP,wherethedefect-inducing(clean)class
istypicallythe minority (majority).
However, studies on oﬄine JIT-SDP disregard the chronology of
softwarechangesthatarrivesequentiallyovertimeinpractice.Tan
etal.showedthatoverlookingchronologycanresultindeceptively
higher predictive performance than the prediction models could
achieve in practice, posing serious threats to validity in JIT-SDP
studies [45]. Later studies [ 3,4,27] further found that predictive
performance of JIT-SDP models can deteriorate over time as a
resultofconceptdrift.Therefore,JIT-SDPapproachesbeingableto
learnovertimeandreducedropsinpredictiveperformancethatare
potentiallycausedbyconceptdrifthavebeenproposed[ 3,4,44,45].
2.2Chronology-PreservingLabelingProcedures
WhentakingchronologyintoaccountinJIT-SDP,onealsoneeds
to consider the issue of veriﬁcation latency when labeling training
examples, as explained in Section 1. In particular, Tan et al. [ 45]
adoptedalabelingprocedurewhereafullbatchofsoftwarechanges
is labeled after a pre-deﬁned amount of time, so that there is an
increasedchancethatdefectsassociatedtosoftwarechangeswould
have been found to produce defect-inducing training examples.
However, this procedure does not consider that defect-inducing
trainingexamplescouldbeproducedassoonasdefectsarefoundto
be associated to them, which potentially happens before the end of
this pre-deﬁnedperiodoftime.Therefore,this labelingprocedure
can unnecessarily delay the training process of JIT-SDP models,
potentiallyslowing downreactionto concept drift.
Cabral et al. [ 4] proposed a method that overcomes this issue.
It makes use of a pre-deﬁned parameter called waiting time that
deﬁneshowlongonewouldwaitafterasoftwarechangeiscommit-
tedtolabelitasclean.Ifnodefectisfoundwithinthiswaitingtime,
acleantrainingexampleisproducedattheendofthewaitingtime.
If a defect is found to be associated to this software change within
thewaitingtime,adefect-inducingtrainingexampleisproducedat
themomentwhenthisisfound.Ifadefectisfoundafterthewaiting
time for a change that had been previously labeled as clean, a new
defect-inducingtraining example isproducedfor this change.
Suchwaitingtimestrategyhasbeenusedasalabelingmethod
foranonlineJIT-SDPlearningprocedureasillustratedinFigure 1(a).
ConsideraninitialJIT-SDPmodel M0(·)thathasbeenproduced
withexistingdata.Whenanewsoftwarechange /u1D44B/u1D461iscommitted
at test time step /u1D461, where/u1D44B/u1D461∈R/u1D451denotes a /u1D451-dimensional feature
vector representing the software change, the latest model M/u1D461−1(·)
is adopted to predict whether /u1D44B/u1D461would be defect-inducing (class 1)
orclean(class 0), formulated as /hatwide/u1D466.alt/u1D461=M/u1D461−1(/u1D44B/u1D461). Once that is done,
new training examples are produced based on the waiting time
methoduntilanewsoftwarechangearrivestobepredicted.Such
(a) JIT-SDP with the waiting time method adopted in recentonlineJIT-SDP studies.
(b) JIT-SDP with HumLa proposed in thispaper.
Figure 1:JIT-SDP with diﬀerentlabeling methods.
trainingexamplesare usedto producean updatedJIT-SDPmodel
M/u1D461(·).Whenthenewchangearrivestobepredicted,theprocedure
isrepeatedfortesttimestep /u1D461←/u1D461+1.Thisiterativetest-then-train
processbasedonthewaitingtimemethodcontinuesthroughout
the process of JIT-SDP. Several other studies have adopted this
procedure withthe waiting time method[ 3,40,42].
However,allpreviousstudieshaveoverlookedthefactthatprac-
titioners would be inspecting software changes predicted as defect-
inducingatcommittimewhenJIT-SDPisadoptedinpractice,to
checkiftheyarereallyassociatedtoadefect.Eventhoughawaiting
time isnecessary for labelingsoftware changes predicted asclean
byaJIT-SDPmodel,changespredictedasdefect-inducingwould
likely be labeled much earlier. Therefore, this more immediate hu-
man labeling procedure should be taken as an important part of
theJIT-SDPprocesstoformulatealabelingprocedurethatiscloser
to the realitythat wouldbe adoptedinpractice.
3 IMMEDIATEHUMAN LABELING INJIT-SDP
3.1 HumLa
Figure1(b)illustratestheprocedureofImmediate HumanLabeling
forSoftwareChangesPredictedasDefect-Inducing(HumLa),for-
mulatedtoanswerRQ1.InHumLa,ifasoftwarechangeispredicted
asdefect-inducing,thischangewillbemoreimmediatelylabeledby
humans;whereasachangepredictedascleanislabeledfollowing
the waiting time labelingprocedure [ 4]explainedinSection 2.2.
In particular, when a test example /u1D44B/u1D461∈R/u1D451is predicted as
defect-inducing (class 1) at test time step /u1D461, formulated as[/hatwide/u1D466.alt/u1D461=
M/u1D461−1(/u1D44B/u1D461)]==1,the developer who has just produced the code
change is requested to immediately inspect the code while it is
stillfreshintheirmind,andtodecidewhetherornotthischange
reallycontainsanydefect.Thisprocessproducesatraininglabel
H(/u1D44B/u1D461)immediately after /u1D461, whereH(·)represents the practical
humanlabelingprocedure.Meanwhile,thewaitingtimelabeling
procedureisstillusedtolabelanysoftwarechangesthathavebeen
previously predicted as clean and that are now ready to be labeled
due to the end of their waiting time. When /u1D44B/u1D461is predicted as clean
(class0),formulatedas [/hatwide/u1D466.alt/u1D461=M/u1D461−1(/u1D44B/u1D461)]==0,thereisnoneedfor
developers to immediately inspect this change. Therefore, only the
waiting time procedure isused.
607ESEC/FSE ’23, December3–9, 2023,San Francisco, CA, USA Liyan Song,Leandro Lei Minku,CongTeng, XinYao
The immediate training example (/u1D44B/u1D461,H(/u1D44B/u1D461))that is possibly
created by HumLa,and any delayed training example(s) that were
possiblycreatedbythewaitingtimemethodatteststep /u1D461areallused
to update the JIT-SDP model following their labelingchronological
order. The latest model available before a new software change
needstobepredictedisdenotedas M/u1D461(·).Itisworthnotingthat
whennosoftwarechangehasbeenlabeledbeforeanewsoftware
change arrives to be predicted, the model M/u1D461(·)remains the same
as the one in the prior test step as M/u1D461−1(·), i.e.,M/u1D461(·)=M/u1D461−1(·).
When a new software change arrives to be predicted, the time step
/u1D461is incremented as /u1D461←/u1D461+1, such that the most up-to-date model
usedfor its prediction isnowreferredto as M/u1D461−1(·).
The HumLa procedure makes the modeling of JIT-SDP closer
tothereality.Asmoreimmediatehumanlabeledexamplescould
better capture recent concepts ofthe defect-generation processin
JIT-SDP,thislabelingprocedurecouldbepotentiallybeneﬁcialto
predictiveperformanceespeciallywhenconceptdriftoccursand
as long as the quality of human labeling is not poor. The beneﬁt of
HumLatopredictiveperformanceisinvestigatedinRQ1.1.How-
ever, two issues need to be considered when investigating HumLa.
Theﬁrstisthatdevelopersmaymakemistakeswhenlabelingthe
defect-predicted examples. Therefore, it is important to investigate
theimpactofsuchhumanlabelnoiseonpredictiveperformanceof
JIT-SDPwhenadoptingHumLa.ThisisdoneinRQ1.2andisfur-
ther explained in Section 3.1.1. The second issue is that developers
mayconsidertheeﬀorttoinspect andlabel allchangespredicted
as defect-inducing as too high. To save resources and reduce hu-
man eﬀort, developers may want to inspect only part of, not all,
defect-predicted examples.Therefore, itisof particularinterestto
investigate the extent to which the amount of human eﬀort would
aﬀect predictive performance when adopting HumLa. This is done
inRQ1.3andisfurther explainedinSection 3.1.2.
It is worth noting that, since only defect-predicted software
changeswouldbeimmediatelyinspectedbyhumans,thecurrent
underlying data generating process would likely be represented
mostlybyexamplesofthedefect-inducingclass.However,sincethe
defect-inducing class is usually the minority in JIT-SDP [ 45], such
distributionbiastowardsthisminorityclasswouldprobablynotbe
detrimentaltopredictiveperformanceofJIT-SDPmodelscompared
to the beneﬁts gained by the more recent training examples pro-
duced by HumLa [ 3]. Indeed, our experimental studies in Section 6
show that HumLa can typically lead to signiﬁcant improvement in
predictive performance.
3.1.1 Human Label Noise. When adopting HumLa, defect-pre-
dicted examplesmaybe mislabeled ascleanbyhumans, whomay
fail to ﬁnd the defect induced by the software change. When devel-
opersdoﬁndadefect-predictedsoftwarechangetoinduceadefect,
itisassuredthatthechangeshouldreallybedefect-inducing,i.e.,
such label would be unlikely to be noisy. Therefore, mislabeling
ofdefect-predictedexamplesisone-sided[ 40].Thatbeingsaid,a
changethatistrulydefect-inducingmaybemanuallymislabeled
asclean,butachangethatistrulycleanwouldbehighlyunlikely
to be manually mislabeledas defect-inducing.
Therefore, the practical human labeling process H(/u1D44B)of a soft-
ware change described by the feature vector /u1D44Band predicted as
defect-inducingbythemostup-to-datemodelcanbeformulatedasH(/u1D44B)= 
0,if/u1D466.alt=0
0,if/u1D466.alt=1&withthe probability /u1D6FC
1,if/u1D466.alt=1&withthe probability 1−/u1D6FC(1)
where/u1D466.altis the true label of /u1D44B,/u1D6FC∈ [0,1]denotes a pre-deﬁned
humanlabelnoisecorrespondingtotheprobabilitythat /u1D44Bwould
be mislabeledby human. This formulation of H(·)will be used to
analyze the impact of diﬀerentamountsof one-sided human label
noise/u1D6FC∈{0,0.1,···,0.9,1.0}on the predictive performance of
JIT-SDPwhenadoptingHumLatoanswerRQ1.2inSection 6.1.2.
Inthispaperhereafter,werefertosuchformulationas HumLaat
/u1D6FC-humannoise ,andfor the sakeof simplicity,HumLa at 0-human
noiseisadoptedas the defaultsettingunless otherwisespeciﬁed.
3.1.2 HumanEﬀortofHumLa. Inthissection,weformulateHumLa
consideringthatsomesoftwarechangespredictedasdefect-inducing
wouldnotbeinspectedbydeveloperstosaveeﬀort.Givenaran-
dom variable following the uniform distribution /u1D704∼/u1D448[0,1], we
decide whether humans would inspect and label a software change
/u1D44Bpredictedas defect-inducingbasedonthe following
1(/u1D44B)=/braceleftbigg1,if/u1D704≤/u1D6FD
0,if/u1D704>/u1D6FD(2)
where/u1D6FD∈ [0,1]denotes a pre-deﬁned human labeling percent-
agecorrespondingtothepercentageofdefect-predictedexamples
that developersopt forinspectingover the total number of defect-
predicted examples, and 1(·)is the indicator function deciding
whether (value 1) or not (value 0) the developer opts for inspecting
thischangetoproduceatrainingexamplewithhumanlabel H(/u1D44B)
asinEq.(1).Larger/u1D6FDallowsformoredefect-predictedexamplesto
be inspectedbydevelopers, usually requiring more human eﬀort.
Wewillanalyzetheimpactofdiﬀerentamountsofhumaneﬀort
intermsofthelabelingpercentage /u1D6FD∈{1,0.9,···,0.1}onpredic-
tiveperformanceofJIT-SDPforansweringRQ1.3inSection 6.1.3.In
thispaperhereafter,werefertoitas HumLaat /u1D6FD-humaneﬀort ,and
for the sake of simplicity, HumLa at 100%-human eﬀort is adopted
as the default setting, unless otherwise speciﬁed. It is also worth
notingthatJIT-SDPat0%-humaneﬀortisequivalenttothewaiting
time method.
3.2 ECo-HumLa
This section proposes an Eﬀort-Conservative HumanLabeling
(ECo-Humla) method for RQ2 to save human eﬀort in labeling
defect-predicted examples while maintaining predictive perfor-
manceofJIT-SDP.Assoftwarechangestobelabeledcorrespondto
thosethatareinspectedbypractitionersatcommittimeinanat-
tempttoﬁndpotentialdefectsatanearlystage,itisalsoimportant
for ECo-HumLa to prioritize the labeling of software changes that
are more likely to contain defects.
Givenasoftwarechange /u1D44B,JIT-SDPpredictswhetherornotthis
softwarechangewouldbedefect-inducing(class1)orclean(class
0)basedonthelatestmodel M(·).Besidestheclasspredictionas
/hatwide/u1D466.alt=M(/u1D44B), many predictive models can also provide prediction
probabilities. In the case of ensembles of models, the prediction
probabilities can be computed as the mean predicted class prob-
abilities of the base learners in the ensemble [ 30]. We use /u1D4501(/u1D44B)
and/u1D4500(/u1D44B)to denote the prediction probability that /u1D44Bbelongs to
608A Practical Human Labeling MethodforOnline Just-in-TimeSo/f_tware DefectPrediction ESEC/FSE ’23, December3–9, 2023,San Francisco, CA, USA
class 1 and class 0, respectively, where 0≤/u1D4500(/u1D44B),/u1D4501(/u1D44B)≤1and
/u1D4501(/u1D44B)+/u1D4500(/u1D44B)=1.A predictedlabel istypicallydeterminedas
/hatwide/u1D466.alt=/braceleftbigg1if/u1D4501(/u1D44B)>/u1D4500(/u1D44B)
0if/u1D4501(/u1D44B)≤/u1D4500(/u1D44B).
Based onthese notations, we deﬁne the predictionconﬁdence of
test example /u1D44Bbasedonthe JIT-SDPmodel M(·)as
/u1D70C(/u1D44B)=|/u1D4501(/u1D44B)−/u1D4500(/u1D44B)|, (3)
where/u1D70C(·)∈[0,1]and|·|denotes the absolute value. This metric
can measure how much conﬁdence the model has in predicting
the software change /u1D44B, and larger /u1D70Cindicates higher conﬁdence
upon this prediction. For JIT-SDP, a defect-predicted example with
larger/u1D70Cmeans that this change has higher chances of inducing
adefectandthusthedeveloperwouldbestronglyrecommended
toinspectthischangeatcommittime,producinganimmediately
labeledexample.
Given a random variable uniformly distributed as /u1D704∼/u1D448[0,1],
practitionerscan heuristically decide whetherto inspect andlabel
a defect-predicted example /u1D44Baccording to a probability equal to
thepredictionconﬁdence /u1D70C(/u1D44B).ThisECo-HumLaprocesscanbe
formulatedas
1(/u1D70C(/u1D44B))=/braceleftbigg1,if/u1D704≤/u1D70C(/u1D44B)
0,if/u1D704>/u1D70C(/u1D44B)(4)
where 1(·)istheindicatorfunctiondecidingwhether(value1)or
not(value0)practitionersoptforinspectingthechangetoproduce
an immediately labeledtrainingexample.Itisworth notingthat,
similar to HumLa, ECo-HumLa only deals with defect-predicted
examples. There is no need for humans to immediately inspect
clean-predicted examples,asthiswouldleadto a largeamountof
eﬀortfor inspecting changes that are unlikely to inducedefects.
Based on this formulation, the higher the conﬁdence /u1D70C(·), the
more strongly practitioners are encouraged to inspect and label
the software change, and the more likely practitioners are to really
label it. However, as this procedure is stochastic, there is still some
chancethatpractitionerswould(wouldnot)labelagivenchange
thatwaspredictedasdefect-inducingwithlow(high)conﬁdence.
ECo-HumLacouldalsobeusedinadeterministicwaybysetting
a ﬁxed threshold to replace /u1D70Fin Eq.(4). However, we encourage
the use of this stochastic process to avoid the strict assumption
thatpractitionerswouldhavetolabelallchangesaboveacertain
threshold and cannot label any of the changes below the threshold.
We illustrate how to conductthe ECo-HumLaprocedure as fol-
lows.Giventestexample /u1D44B1and/u1D44B2,supposethatthemodelpro-
duces prediction probability /u1D4500(/u1D44B1)=0.68and/u1D4501(/u1D44B1)=0.32for
/u1D44B1and/u1D4500(/u1D44B2)=0.32and/u1D4501(/u1D44B2)=0.68for/u1D44B2, individually. As
/u1D4501(/u1D44B1)</u1D4500(/u1D44B1), we have /hatwide/u1D466.alt1=0and it is unnecessary for humans
to inspect /u1D44B1. As/u1D4501(/u1D44B2)>/u1D4500(/u1D44B2), we have /hatwide/u1D466.alt2=1and its predic-
tionconﬁdenceisfurthercomputedas /u1D70C(/u1D44B2)=|0.68−0.32|=0.36.
This means that /u1D44B2has the probability of 36% to be inspected by
human to immediately obtain its training label.
Theprocedure of ECo-HumLais consistent withthe real-world
processwherepractitionerswouldfavorinspectingthe mostcon-
ﬁdent predictions as defect-inducing to ﬁnd as many defects as
possible while saving inspection eﬀort. However, it relies on the
assumption that the mostconﬁdent defect predictions correspond
tosoftwarechangesthataremorelikelytobedefect-inducing,sothat the number of defects thatpractitioners would miss to ﬁnd at
committimeisnotlarge.Itisalsounclearhowmuchthereduction
ofeﬀort isobtainedthroughthis procedureandhow itwouldaf-
fect the predictive performance of the resulting JIT-SDP models.
Thesethreepointswillbeinvestigatedaspartoftheexperiments
to answer RQ2.1,RQ2.2andRQ2.3,respectively.
AnotherpossiblesetupforECo-HumLacouldbetheopposite,
wherepractitionerswouldberecommendedtoprioritizeinspect-
ingthose leastconﬁdent predictions, so thatthemostinformative
training samples [ 1,26] for the JIT-SDP model can be produced
during the human labeling process. This could possibly contribute
themosttotheperformanceimprovementoftheJIT-SDPmodel.
We have conﬁrmed this conjecture through experiments which
have shown that human labeling the least conﬁdent predictions
did outperform the proposed ECo-HumLa in terms of achieving
signiﬁcantlybetterpredictiveperformanceandconservingmuch
morehumaneﬀort.However,theleastconﬁdentdefect-inducing
predictions may be more likely to correspond to software changes
that are actually clean than the more conﬁdent defect-inducing
predictions.Ifwerecommenddeveloperstoprioritizeinspecting
(and thus labeling) these changes, they would waste eﬀort in in-
specting changes that may be clean, and miss several defects by
not inspecting the changes that are more likely defect-inducing.
Therefore, while such alternative setup makes sense from a model
predictive performance perspective, it would be unsuitable as a
practical approach to JIT-SDP.
4 DATASETS
Thispaperuses14GitHubopensourceprojectsasinpreviouswork
[40,42] to investigate the proposed human labeling methods for
JIT-SDP, assummarized in Table 1 of thesupplementarymaterial.
Twelvemetricshavebeenusedasinputfeaturesfollowingprevious
work [19], as explained in Section 1 of the supplementary material.
TheCommit Guru [37] tool was used to collect the data with
input features and labels of software changes. The tool is based on
the SZZ algorithm [ 39] to decide actual labels of software changes,
which are defect-inducing (class 1) or clean (class 0). As SZZ is
knowntoleadtolabelnoise[ 16,20,34–36],wehaveconducteda
manualinspectionofarandomsampleofchangesofeachproject
to investigate its data quality. Four experts with at least 4 years
of programming experience have been asked to work in pairs to
label these changes. Each pair was asked to discuss each of the
softwarechangestodecideontheirlabels, leadingtotwosetsof
human-generatedlabels(Pair1andPair2).Followingexistingwork
[16,27],humanannotatorswereaskedtolabelsoftwarechanges
asdefect-ﬁxingornon-defect-ﬁxing,insteadoflabelingsoftware
changes as defect-inducing or clean directly. This is because it
wouldbeextremelytime-consuming,ifevenpossibleatall,fora
softwaredeveloperwhohadnotworkedonagivenprojecttoman-
ually check whether a software change is defect-inducing or clean
directly on this project based on (e.g.) codes and/or commit mes-
sages.FixesareusedbySZZtoidentifydefect-inducingsoftware
changes.Therefore,ahighlevelofnoiseintheSZZidentiﬁcationof
changes as defect-ﬁxing and non-defect-ﬁxing means a high level
ofnoiseintheSZZlabelsofdefect-inducingandclean.Notethat
noise arising from git blame withinCommit Guru is not included
inthismanualinspectionprocessandcouldleadtoadditionalnoise
609ESEC/FSE ’23, December3–9, 2023,San Francisco, CA, USA Liyan Song,Leandro Lei Minku,CongTeng, XinYao
Table 1:Kappascores
Dataset(1) SZZ (2) SZZ Average of (3) Pair 1
vs Pair 1 vs Pair2 (1) and(2) vs Pair2
Brackets 0.5384 0.4722 0.5053 0.7083
Broadleaf 0.7101 0.6522 0.6812 0.7867
Camel 0.4376 0.5604 0.4990 0.6330
Fabric8 0.6418 0.8056 0.7237 0.6620
jGroups 0.5070 0.5303 0.5187 0.9045
Nova 0.5550 0.5639 0.5594 0.7085
Tomcat 0.5740 0.6840 0.6290 0.7451
Corefx 0.7573 0.6291 0.6932 0.6835
Django 0.7791 0.8739 0.8265 0.7387
Rails 0.4324 0.5556 0.4940 0.7258
Rust 0.5935 0.4804 0.5370 0.4804
Tensorﬂow 0.8584 0.8052 0.8318 0.7973
VScode 0.8750 0.8095 0.8423 0.8750
wp-Calypso 0.8751 0.7746 0.8249 0.7101
Median 0.6176 0.6407 0.6551 0.7180
[2,35].Asampleof100changes(50defect-ﬁxingand50non-defect-
ﬁxing)wasdrawnfromeachprojectandsortedinrandomorder
to circumvent bias during the human annotation process. If the
commit message did not contain an issue ID within it, then only
thecommitmessageitselfwascheckedtodeterminewhetherthe
softwarechangeisdefect-ﬁxingornot.Otherwise,boththecommit
messageandtheissuewereused.Inparticular,ifacommitmessage
isaddressinganissueidentiﬁedbyagivenIDcorrespondingtoa
bug,this changewaslabeledas adefect-ﬁxing change.
The Kappa scores [ 7] (1) between SZZ and Pair 1, (2) between
SZZ and Pair 2, and (3) between Pair 1 and Pair 2 are shown in Ta-
ble1.FollowingHalletal.[ 15],weinterpretKappascoreasfollows:
[−1,0]lessthanchanceagreement;[0.01,0.20]slightagreement;
[0.21, 0.40] fair agreement;[0.41, 0.60] moderate agreement;[0.61,
0.80]substantialagreement;and[0.81,0.99]almostperfectagree-
ment. We can see that the Kappa scores (1) and (2) indicate at least
moderate agreement for all datasets. In four datasets (Broadleaf,
Fabric8,Tomcat, and Corefx),the average of (1) and (2) indicates
a substantial agreement and in other four (Django, Tensorﬂow,
VScode, and wp-Calypso) it indicates almost perfect agreement.
Moreover,themedianKappascoresacrossdatasetsbetweenhuman
annotatorsandSZZ(averageof(1)and(2)),andbetweenhuman
annotators themselves (3) both indicate a substantial agreement,
showing that the level of agreement between human annotators
and SZZ is in line with the level of agreement between humans
themselves. This suggeststhatthelabels providedbySZZ were in
generalunlikely to be worse thanthe labels given byhumans.
Kappascoresindicatetheleveloflabelnoiseassociatedtocor-
rectlydistinguishingdefect-ﬁxes fromnon-defect-ﬁxes,which are
in turn used to identify defect-inducing changes by SZZ. However,
if agiven ﬁxhas not yetbeen implemented,SZZ would be unable
to link this ﬁx to a defect-inducing change, even if its ability to
distinguish defect-ﬁxes from non-defect-ﬁxes is perfect. A previ-
ousstudy[ 42]showedthat,ifweusetheﬁrst10kchangesofthe
projects in our study, there is at least an estimated 99% conﬁdence
level that the ﬁxes corresponding to these changes have already
been reported. Therefore, we use the ﬁrst 10k software changes of
eachprojectinthe experiments to increasedata quality.5 EXPERIMENTALSETUP
Toinvestigatetheimpactofthelabelingprocessesthattakeinto
accountthehumanlabelingconductedthroughinspectionofdefect-
predicted software changes, the predictive performance of JIT-SDP
modelswithandwithoutimmediatehumanlabelingwillbecom-
pared.Ourlabelingapproachescanbeadoptedwithdiﬀerentma-
chinelearningalgorithms.Inourexperiments, Oversampling-based
DataStreamingbaggingwith Conﬁdence(ODaSC)usingHoeﬀding
trees [40] are adopted whenever JIT-SDP models need to be cre-
ated/updated.ThisisarecentonlinelearningalgorithmforJIT-SDP.
Beinganonlinealgorithm,itupdatesJIT-SDPmodelsusingeach
training example individually upon arrival, and then discards it,
withouttheneedforretrainingonpastexamples.ODaSCischosen
forbeingthestate-of-the-artonlineJIT-SDPmodel.Itdealswith
label noise resulting from veriﬁcation latency by estimating the
conﬁdence in the labels assigned to training examples [ 40]. It was
shown to be more robust to noise than OOB [ 40], which in turn
wasshownto be betterthanslidingwindowapproaches[ 4].
We use Geometric Meanof Recall0andRecall 1(G-Mean) [ 23]
to evaluate predictive performance of JIT-SDP models with and
without immediate human labeling. Diﬀerent from accuracy or
precision,G-meanisknowntoberobustagainstclassimbalance,
which is particularly important for studies suﬀering from class
imbalance evolution such as JIT-SDP [ 4,42,49]. We have also
adoptedMatthewsCorrelationCoeﬃcient(MCC)[ 6,55]asithas
become popular in the area of JIT-SDP [ 21,22,56]. We use /u1D461/u1D45D
todenotetruepositives(thenumberofdefect-inducingsoftware
changes that are predicted correctly), /u1D453/u1D45Bto denote false negatives
(the number of defect-inducing software changes that are erro-
neouslypredictedasclean), /u1D461/u1D45Btodenotetruenegatives(thenum-
berof clean software changes thatare predictedcorrectly) and /u1D453/u1D45D
to denote false positives (the number of clean software changes
that are erroneously predicted as defect-inducing). Based on them,
G-Mean=/radicalBig/u1D461/u1D45D
/u1D461/u1D45D+/u1D453/u1D45B·/u1D461/u1D45B
/u1D461/u1D45B+/u1D453/u1D45D∈[0,1]. As the false positive rate is de-
ﬁnedas1−/u1D461/u1D45B/(/u1D461/u1D45B+/u1D453/u1D45D),G-meantakesintoaccountthetrade-oﬀbe-
tweentruepositivesandfalsepositives;largerG-Meanmeansbetter
performance. MCC=/u1D461/u1D45D·/u1D461/u1D45B−/u1D453/u1D45D·/u1D453/u1D45B√
(/u1D461/u1D45D+/u1D453/u1D45D)·(/u1D461/u1D45D+/u1D453/u1D45B)·(/u1D461/u1D45B+/u1D453/u1D45D)·(/u1D461/u1D45B+/u1D453/u1D45B)∈[−1,1]
takes all 4 elements of the confusion matrix into consideration and
thus provides high scores only if the predictions return good rates
forall4entriesoftheconfusionmatrix[ 5,6].G-MeanandMCCare
computed in a sequential way based on a fading factor to track the
changesinpredictiveperformanceovertimeasrecommendedin
theonlinelearning scenario[ 14].Fadingfactor /u1D703∈[0,1]controls
how much emphasis one would like to place on past evaluation
examplescomparedtothenewoneandlarger/smallervaluesfor
/u1D703places more emphasis on the past/present model performance
status. The fading factor /u1D703=0.99is used in this paper following
previousJIT-SDPstudies[ 4,40,44],enablingagoodtrade-oﬀbe-
tweenrapidlytracking performancechangesand preventingwild
variations.Whencomputingtheaveragepredictiveperformance,
an averageofthe sequentialperformance wasused.
Weusedagridsearchtochoosethebestparametersettingfor
eachproject.ODaSChasthreeparameters:theensemblesize(#Ho-
eﬀdingtrees)∈{5,10,20,30,40},thedecayfactorofclassimbalance
∈{0.9,0.95,0.99,0.999}andtheresamplingthreshold ∈{0.8,0.9}.
610A Practical Human Labeling MethodforOnline Just-in-TimeSo/f_tware DefectPrediction ESEC/FSE ’23, December3–9, 2023,San Francisco, CA, USA
We adopt the parameter settings that can achieve the best average
G-Means across 30 runs based on the ﬁrst 500 software changes
inthedatastreamofeachproject.Hoeﬀdingtreesusethedefault
parametersprovidedinthepythonpackage scikit-multiﬂow [30],
following previous JIT-SDPstudies[ 3,40,42].
To investigate to what extent the proposed ECo-HumLa can
encourage defects to be found when saving human eﬀort, we need
to formulate two additional metrics. The ﬁrst metric is human
recall-1that is the ratio of defect-inducing changes ( /u1D466.alt=1) that
were predicted as defect-inducing ( /hatwide/u1D466.alt=1) and that developers were
askedtolabel(H(/u1D44B)).Givenahumanlabelingprocess,thismetric
can be formulatedas
/u1D4451H=#[/hatwide/u1D466.alt=1,H(/u1D44B),/u1D466.alt=1]
#[/u1D466.alt=1], (5)
where#(·)denotesthenumber ofsoftwarechangessatisfyingthe
inside condition(s). We can see that /u1D4451His the ratio of defect-
inducing changes that we asked developers to inspect / label. A
highervaluemeansthatdevelopersarebeingaskedtoinspectmore
softwarechangesthatarereallydefect-inducing,potentiallyuncov-
ering more defects. The second metric is human false alarm that is
theratioofcleansoftwarechanges( /u1D466.alt=0)thatwerepredictedas
defect-inducingandthatdeveloperswereaskedtolabel.Givena
human labelingprocess H(·),this metric can be formulatedas
/u1D439/u1D44E/u1D459/u1D460/u1D452/u1D434/u1D459/u1D44E/u1D45F/u1D45AH=#[/hatwide/u1D466.alt=1,H(/u1D44B),/u1D466.alt=0]
#[/u1D466.alt=0]. (6)
Wecanseethat /u1D439/u1D44E/u1D459/u1D460/u1D452/u1D434/u1D459/u1D44E/u1D45F/u1D45AHistheratioofcleanchangesthatwe
asked human to inspect / label. A higher value means that more
human inspection eﬀortiswasted.
Predictiveperformanceofonlinelearningmethodswiththebest
parametersettingsisevaluatedbasedontherest500 ∼10,000soft-
warechangesoftheproject.ComparisonsbetweenJIT-SDPwithvs
without the practical human labeling method are conducted based
on the mean predictive performance across 100 runs to account of
ODaSC’s stochasticity. We report the results corresponding to the
waiting time of 15 days, following previous related studies [ 40]. In
particular,thiswaitingtimewasfoundtoleadtoJIT-SDPmodels
with better predictive performance thanothervaluesas they oﬀer
a better trade-oﬀ between one-sided label noise and the ability
to tackle concept drift [ 40,42]. Friedman tests [ 10] will be per-
formedforstatisticalcomparisonsbetweenmorethantwoJIT-SDP
methods across datasets. The null hypothesis (H0) states that all
methodsperformsimilar;thealternativehypothesisstatesthatthey
havestatisticallysigniﬁcantdiﬀerence.GiventherejectionofH0,
wewillfurtherperformpairwisecomparisonsusingtheConover
posthoctests.Two-tailedpairwiseWilcoxonsignedranktestsat
signiﬁcancelevel0.05[ 10]willbeperformed wheneverstatistical
comparisons between twomethodsacrossdatasets are needed.
To analyze human eﬀort, in addition to consideringthe human
labelingpercentagementionedinSection 3.1.2,wewillalsoanalyze
its corresponding code churn, which is a popular metric of human
inspectioneﬀortinthedefectpredictionliterature[ 19,31,52,53].
The code churn of a software change is deﬁned as (/u1D43F/u1D434+/u1D43F/u1D437)/2,
where/u1D43F/u1D434is the number of lines added and /u1D43F/u1D437is the number of
linesdeletedbythe software change.Table 2: RQ1.1 – Performance comparisons between HumLa
at0%-humannoiseand100%-humaneﬀort(thedefaultsetup)
and the waiting time method in terms of G-Mean and MCC.
DatasetG-Mean MCC
Waiting time HumLa Imp% Waiting time HumLa Imp%
Bracket 0.643 0.639 [-b] -0.48 0.300 0.297 [-m] -0.97
Broadleaf 0.607 0.663 [b] 9.34 0.292 0.347 [b] 18.63
Camel 0.669 0.681 [b] 1.82 0.356 0.378 [b] 6.30
Fabric8 0.653 0.661 [b] 1.13 0.320 0.333 [b] 4.25
jGroup 0.568 0.600 [b] 5.68 0.193 0.242 [b] 25.68
Nova 0.682 0.688 [b] 0.85 0.380 0.391 [b] 3.01
Tomcat 0.613 0.638 [b] 4.05 0.282 0.309 [b] 9.36
Corefx 0.639 0.636 [-s] -0.50 0.362 0.360 [-*] -0.41
Django 0.690 0.698 [b] 1.21 0.413 0.430 [b] 3.93
Rails 0.562 0.623 [b] 10.77 0.214 0.296 [b] 38.54
Rust 0.584 0.586 [*] 0.32 0.250 0.248 [-*] -0.75
Tensorﬂow 0.691 0.678 [-b] -1.87 0.389 0.394 [b] 1.27
VScode 0.527 0.527 [*] 0.12 0.276 0.302 [b] 9.43
wp-Calypso 0.551 0.622 [b] 12.98 0.256 0.293 [b] 14.68
ave-rank 1.75 1.25 - 1.79 1.2 -
“Imp%" denotes the improvementratio of HumLa over the waiting time (control)
method.Symbols [*],[s],[m],and[b] denoteinsigniﬁcant,small,medium,and large
A12[47],respectively.Presence/absenceofthesign“-"inA12meansthatHumLawas
worse/betterthan the waiting time method.The A12eﬀect size of performance
diﬀerencesforeachdatasetwastypicallylarge.Thelastrowreportstheaverageranks
across datasets for the two methods, which were found to be statistically signiﬁcantly
diﬀerentboth in terms of G-Mean and MCC.Smallerranks arebetterranks.
6 EXPERIMENTALRESULTS
6.1 RQ1: JIT-SDPwith HumLa
WecompleteouranswertoRQ1inthissectionbyinvestigatingthe
impact of HumLa on predictive performance of JIT-SDP compared
tothewaitingtimemethod,andwithrespecttodiﬀerenthuman
label qualityandlevels of human eﬀort.
6.1.1 RQ1.1 – Predictive Performance. Table2shows the perfor-
mance comparison between HumLa and the waiting time method.
Performance tables using other metrics are reported in the supple-
mentary material for space consideration. We can see that HumLa
leadstosigniﬁcantdiﬀerenceinpredictiveperformanceinterms
of both G-Mean and MCC across datasets. Two-tailed pairwise
Wilcoxon signed rank tests found signiﬁcant diﬀerence with /u1D45D-
values0.0208and0.0016intermsofG-MeanandMCC,respectively.
This means that a more practical labeling procedure would lead to
signiﬁcant diﬀerencesin performance comparedtothe lesspracti-
cal waiting time method. Thus, it is important to evaluate JIT-SDP
methodswithalabelingprocedure that iscloser to the reality.
We can also see from Table 2that HumLa usually improves
predictiveperformanceofJIT-SDPcomparedtothewaitingtime
methodinmostdatasetsexceptforBracketwithanegativeimprove-
mentratio-0.48%,Corefxwith-0.50%andTensorﬂowwith-1.87%
intermsofG-Mean,andBracketwith-0.97%,Corefxwith-0.41%
and Rust with -0.75% in terms of MCC, respectively. While such
negative eﬀects are of a small magnitude, the beneﬁt to predictive
performanceofJIT-SDPcanbesubstantial:theimprovementratios
are 9.34% (18.63%) in Broadleaf, 10.77% (38.54%) in Rails and 12.98%
(14.68) in wp-Calypso in terms of G-Mean (MCC). This is in line
with thestatistical test, which found signiﬁcant beneﬁtto theper-
formance.Suchimprovementsalsomeanthatexiting studiesmay
be underestimating predictive performance of JIT-SDPmethods.
611ESEC/FSE ’23, December3–9, 2023,San Francisco, CA, USA Liyan Song,Leandro Lei Minku,CongTeng, XinYao
(a) Averageranks in G-Mean.
 (b) Averageranks in MCC
Figure 2: RQ1.2 – Performance of HumLa at diﬀerent hu-
man noise. Bar plots report average ranks of methods across
datasets.Thewaitingtimemethodischosenasthecontrol
method (framed in red). Methods signiﬁcantly better (worse)
than thecontrolmethod are ﬁlled inlight orange(green).
6.1.2 RQ1.2 – HumLa at Diﬀerent Human Noise. When labeling
software changes predicted as defect-inducing through HumLa,
humans may mislabel some software changes, leading to noisy
trainingexamples.Diﬀerentpersonsandorganizationsmayhave
diﬀerent human annotation error rates. Determining the typical
human annotation error rate associated to software engineers is
not possible as the ground truth labels are unknown. In particular,
neitherhumansnoralgorithmssuchasSZZarecurrentlyableto
providefullyreliablegroundtruthlabelsforthispurpose.Therefore,
we provide a detailed analysis to investigate theimpactof various
amounts of human annotation error rates, which we refer to as
human noiseinthis section for brevity.
Figure2showsaverageFriedmanranksofHumLaintermsof
G-Mean and MCC at diﬀerent amounts of human noise against
the waiting time method across datasets. Plots of the performance
overtimeandtablesofoverallperformanceusingseveralmetrics
are reported in the supplementary material for space considera-
tions.Friedmantestswiththesigniﬁcancelevel0.05rejectH0with
/u1D45D-values8.61E-19and2.12E-17intermsofG-MeanandMCC,re-
spectively, meaning that diﬀerent human noise leads to signiﬁcant
diﬀerence in predictive performance of JIT-SDP. The waiting time
method isthen chosenasthecontrol methodto conductConover
post-hoc tests, whose results are alsoillustratedinFigure 2.
We can see that HumLa at 0%-human noise can signiﬁcantly
improvepredictiveperformanceofJIT-SDPcomparedtothewaiting
time method; when the human label noise is no worse than 10%
(40%) in terms of G-Mean (MCC), such beneﬁcial impact produced
by HumLa would be signiﬁcant. Therefore, even when there is
humanlabelnoise,adoptingalabelingprocedureclosertoreality
can have positive impact on predictive performance of JIT-SDP
comparedtothewaitingtimemethod.Inotherwords,existingwork
adopting waiting time may be underestimating the performance
thatcanbeachievedinpracticeespeciallywhenadoptingMCCasa
performancemetric.Inaddition,HumLacanobtainsimilarorbetter
rankingthanthewaitingtimemethodsolongasthehumanlabel
noiseisnohigherthan80%(90%)inG-Mean(MCC).Thismeans
that producing labels earlier leads to similar or better predictive
performance, unless the amount of noise in the human labels is
extremely high. Therefore, delaying the production of labels for
changes predictedas defect-inducingisnot recommended.
6.1.3 RQ1.3–HumLaatDiﬀerentHumanEﬀort. Figure3shows
performance comparisons between HumLa at diﬀerent amounts of
(a) Averageranks acrossdatasets in G-Mean.
 (b) G-Mean comparison.
(c) Averageranks acrossdatasets in MCC.
 (d) MCC comparison.
Figure3:RQ1.3–PredictiveperformanceofHumLaatdiﬀer-
entamountsofhumaneﬀort.Barplotsreportaverageranks
of each method across datasets. The waiting time method
isequivalenttoHumLaat0-humaneﬀortandischosenas
thecontrolmethod(framedinred).Methodsthatperform
signiﬁcantlybetterthanthecontrolmethodareﬁlledinlight
orange.Nomethod was worsethanthe control. Radarplots
show performance comparisons of HumLa at a particular
humaneﬀort against the controlmethod.
human eﬀort in terms of G-Mean and MCC, respectively. Perfor-
mancetablesusingothermetricsarereportedinthesupplementary
material for space consideration. Friedman tests at the signiﬁcance
level0.05runacrossdatasetsrejectH0with /u1D45D-values1.576E-05and
3.159E-12 in terms of G-Mean and MCC, respectively. Therefore,
diﬀerentamounts ofhuman eﬀort lead tosigniﬁcant diﬀerencein
predictiveperformanceofJIT-SDP.GiventherejectionofH0,the
waiting time method (HumLa at 0%-human eﬀort) is chosen as the
controlmethodtoconductConoverpost-hoctests,whoseresults
are illustratedinFigures 3(a)and3(c).
We can see from these ﬁgures that larger human eﬀort is in
general beneﬁcial to the predictive performance. In addition, when
practitioners randomly label 60% (40%) defect-predicted test ex-
amples,HumLasigniﬁcantlyimprovespredictiveperformanceof
JIT-SDP compared to the waiting time method in terms of G-Mean
(MCC). Indeed, so long as the amount of human eﬀort is no less
than60%(40%)intermsofG-Mean(MCC),HumLawouldhavesig-
niﬁcant beneﬁt to predictive performance compared to the control
method. Moreover, HumLa achieves similar ranking as the waiting
time method when using only 10% of the eﬀort in terms of human
labelingpercentage. AsthekeydiﬀerencebetweenHumLaandthe
waitingtimemethodistheearlierlabelingoftrainingexamples,the
positive impact of HumLa on predictive performance is due to the
abilitytoupdateJIT-SDPmodelsearlier.Thisinturnmayenable
JIT-SDPtoreacttoconceptdriftfaster,evenwhentheamountof
immediately labeled data is not large. Figures 3(b)and3(d)show
that HumLa at 60%(40%)-human eﬀort can indeed usually attain
performanceimprovementcomparedtothecontrolmethodinmost
612A Practical Human Labeling MethodforOnline Just-in-TimeSo/f_tware DefectPrediction ESEC/FSE ’23, December3–9, 2023,San Francisco, CA, USA
Figure 4: RQ1.3 – Relationship between the human label-
ing percentage and the cumulative code churn on random
datasets for HumLa at varying amounts of human eﬀort. Re-
sults of other datasets showed the same pattern and were
omitted forspacereasons.
datasets,whereasfor thedatasets wherethere isno improvement,
the magnitude of the diﬀerences in performance is very small (-
0.02%forBracket,-0.35%forCorefxand-0.70%forTensorﬂowin
terms ofG-Mean,and-2.95%for Corefxinterms ofMCC).
Figure4shows the relationship between cumulative code churn
and the human labeling percentage on 6 random datasets. Plots
of other datasets showed the same pattern and were reported in
the supplementary material for space restrictions. We can see that
higherhumanlabelingpercentagealmostalwaysaccountsforlarger
value of the cumulative code churn, showing a good correlation
between these two metrics. Therefore, given a project for which
practitioners may be interested in creating a JIT-SDP model, reduc-
ingtheinspectionrateisreallylikelytocorrelatewithadecrease
inchurnforthisproject.Table 3containsthecodechurnvaluesfor
all datasets. We can see that, for example, HumLa at 40%- and 60%-
human eﬀort would reduce around up 40% and 60% code churns,
respectively,being consistent to the human labelingpercentages.
These results are particularly relevant and of practical signiﬁ-
canceastheyindicatethatwhenpractitionersallowforarelatively
low cost of human eﬀort in randomly labeling a small portion
(e.g., 10%) of defect-predicted test examples, HumLa already begins
to have beneﬁcial impact; when HumLa allows for a moderately
large human eﬀort such as 60%, it would perform signiﬁcant better
performance comparedto the waiting time method.
AnswertoRQ1: HumLawouldnotonlymodelthepracticalJIT-
SDP process tobe closer totherealitybutalsoleadstosigniﬁcant
diﬀerenceinpredictiveperformancecomparedtothelesspracti-
cal waiting time method. Even when there is human label noise,
adopting HumLa can usually have positive impact on predictive
performance, indicating that existing work adopting the waiting
time method is likely underestimating the predictive performance
thatcanbeachievedinpracticeespeciallyintermsofMCC.HumLa
allowingforhigheramountsofhumaneﬀortgenerallyattainbetter
performance and when the allowance for human eﬀort is beyond a
moderately large value such as 60%, HumLa would produce signiﬁ-
cantlybetterperformance comparedto the waiting time method.
6.2 RQ2: JIT-SDPwith ECo-HumLa
ThissectioncompletesouranswertoRQ2toevaluatetheproposed
ECo-HumLawithrespecttohowwellitcansavehumaneﬀortwhile
maintainingpredictiveperformanceandavoidingalargenumber
of defect-inducing software changes to be missed by practitioners.
(a) Averageranks acrossdatasets in G-Mean.
 (b) G-Mean comparison.
(c) Averageranks acrossdatasets in MCC.
 (d) MCC comparison.
Figure 5: RQ2.1 – Performance comparisons between ECo-
HumLavs HumLaatdiﬀerent amountsof humaneﬀort.Bar
plots report average ranks of each method across datasets.
HumLaat100%-humaneﬀortischosenasthecontrolmethod
(framedinred)andmethodsthatperformsigniﬁcantlyin-
ferior to the control method are ﬁlled in light green. ECo-
HumLa is framed in orange to facilitate visualization. Radar
plots showperformance comparisonsbetween ECo-HumLa
andHumLa at 100%-humaneﬀort foreachdataset.
6.2.1 RQ2.1 – Retained Performance. Figure5shows performance
comparisonsbetweenECo-HumLaandHumLaatdiﬀerentamounts
of human eﬀort in terms of G-Mean and MCC. Performance values
usingseveralmetricsarereportedinthesupplementarymaterial
for space consideration. Friedman tests at signiﬁcance level 0.05
reject H0with /u1D45D-values 2.65E-05and 5.10E-10in terms of G-Mean
andMCC,respectively,meaningthatECo-HumLaandHumLawith
diﬀerent amounts of eﬀort achieve signiﬁcantly diﬀerent perfor-
mance. Given the rejection of H0, HumLa at 100%-human eﬀort is
chosen as the control method to conduct Conover post-hoc test,
whose results are illustratedinFigures 5(a)and5(c).
We can see from these ﬁgures that statistical tests found no
signiﬁcant diﬀerence between the performance of ECo-HumLa
and HumLa at 100%-human eﬀort across datasets, in terms of both
G-Mean and MCC. This indicates the capability of ECo-HumLa
inretaining predictiveperformancecomparedtoHumLaat100%-
humaneﬀort,despitelabelinglesssoftwarechanges.Wecanalso
seethat ECo-HumLa’sranking isinbetweenHumLaat40% ∼50%-
human eﬀort in terms of both G-Mean and MCC. Further post-hoc
comparisonusingECo-HumLa asthecontrolmethod cannotﬁnd
signiﬁcantdiﬀerencebetweenthesethreemethods,indicatingthem
to have similar performance when comparedacrossdatasets.
Figures5(b)and5(d)show that ECo-HumLa’s performance is
usually below that achieved by HumLa at 100%-human eﬀort in
mostdatasetsasonewouldexpect,buttheinferiorityratioisusually
of small magnitude: in terms of G-Mean, the inferiority ratio is
613ESEC/FSE ’23, December3–9, 2023,San Francisco, CA, USA Liyan Song,Leandro Lei Minku,CongTeng, XinYao
Table 3:RQ2.2 –Savedhumaneﬀortof ECo-HumLa against
HumLaatdiﬀerenthumaneﬀortintermsofhumanlabeling
percentageandcumulative codechurn(in kilo).
DatasetHuman eﬀort (in kilo)ofHumLa Eco-HumLa Human eﬀort (in kilo)ofHumLa
100% 90% 80% 70% 60% autohuman% 50% 40% 30% 20% 10%
Bracket 664.8[b] 593.2 530.4 466.0 403.9 371.055.51%331.5 261.1 223.2 137.9 74.3
Broadleaf 2715.4[b]2418.22171.91884.01612.6 2162.646.60%1326.41002.0 777.2 506.9 258.2
Camel 913.3[b] 818.0 727.8 633.0 553.6 578.254.57%455.0 360.5 271.8 177.9 87.6
Fabric8 2099.6[b]1941.01733.71528.71297.5 1165.246.85%1071.2 844.6 651.0 403.9 214.5
jGroup 302.8[b] 276.2 255.3 224.4 203.8 200.045.55%174.8 147.8 118.6 83.4 35.6
Nova 612.7[b] 555.1 488.5 434.7 380.4 332.762.09%338.5 267.1 199.3 134.2 73.1
Tomcat 564.9[b] 510.0 450.8 386.8 329.3 373.258.01%273.5 214.7 156.0 104.1 50.5
Corefx 4486.7[b]4129.23631.03382.02904.6 3305.151.95%2384.01913.41450.3985.9 512.4
Django 867.6[b] 793.3 711.8 605.3 509.1 499.871.28%426.5 321.5 255.6 171.3 79.6
Rails 940.6[b] 834.8 733.1 619.4 500.5 359.147.07%416.2 317.9 195.5 93.6 38.5
Rust 547.8[b] 479.8 418.9 356.6 323.1 234.240.73%289.7 241.7 179.0 130.7 55.0
Tensorﬂow 1140.3[b]1106.21058.1 979.3 902.2 900.455.94%796.3 655.1 506.2 328.5 164.5
VScode 286.1[b] 262.6 232.6 206.7 170.2 203.448.64%144.4 123.8 83.4 57.5 35.6
wp-Calypso 301.9[b] 273.5 245.8 211.6 178.9 187.851.41%145.3 114.4 82.6 52.1 24.0
median 766.2 693.3 621.1 535.7 452.2 372.151.68%377.3 292.5 211.3 136.1 73.7
ForHumLa at 100%-humaneﬀort,the A12eﬀect size [ 47] of diﬀerences in saved
human eﬀort for eachdatasetwas always large([b]) than Eco-HumLa. Eﬀect sizes
for otherlevelsof human eﬀort arein the supplementarymaterial.
below3.6%inBroadleafandwp-Calypso;intermsofMCC,most
of the inferiority ratios are around below 5% except for 9.21% in
jGroup and 14.31% in Rails. This is in line with the results of the
abovementionedConovertests,whichfoundnodiﬀerencebetween
ECo-HumLa andHumLa at 100% human eﬀortacrossdatasets.
AnimportantquestionisthenhowmucheﬀortECo-HumLacan
savecomparedtoothermethodsthatobtainedsimilarperformance.
This isinvestigatedinSection 6.2.2.
6.2.2 RQ2.2 – Saved Human Eﬀort. Table3shows that the median
humaneﬀortofECo-HumLaacrossdatasetsis372.1kintermsof
the cumulative code churn. Compared to the median cumulative
code churn766.2k ofHumLaat 100%-human eﬀort, ECo-HumLa
cangenerallyconservearound50%-humaneﬀortincheckingthe
linesofcodebeingchanged,demonstratingtheeﬀectivenessofECo-
HumLa in saving human eﬀort. We can also see that the median
humanlabelingpercentageofECo-HumLaacrossdatasetsis51.68%,
in-betweenHumLaat50% ∼60%-humaneﬀort,alsoshowingthat
ECo-HumLa saves around halfofhuman eﬀort.
Combining with the observation in Section 6.2.1, we can con-
clude that ECo-HumLa achieved similar performance of HumLa at
100%-human eﬀort while requiring only around half of the inspec-
tioneﬀortintermsofhumanlabelingpercentageandcodechurn.
Therefore, we would recommend practitioners to inspect and label
alldefect-predictedsoftwarechangeswhenthehumancostisal-
lowable, astheinspection used tolabel defect-predicted examples
isthesameastheinspectionneededtoﬁndandﬁxanydefectsthat
thechangemayinduce.Therefore,inspectingthesechangesmay
helppractitionerstoﬁndmore defectsatanearlystage.However,
ifthecostofinspectingalldefect-predictedsoftwarechangesistoo
high, inspecting only around half of the defect-predicted software
changesthroughECo-HumLawillnotleadtoworsepredictiveper-
formanceoftheresultingJIT-SDPmodels.Inthissense,itwould
be acceptable to reduce labelingeﬀortthroughECo-HumLa.
6.2.3 RQ2.3 – Finding Defects. Based on the previous sections,
ECo-HumLa requires similar eﬀortand leads tosimilar predictive
performanceasHumLaat50%-humaneﬀort.However,ECo-HumLa
was designed to direct practitioners’ inspection eﬀort towards soft-
ware changes that are more likely to contain defects, so that theTable4:RQ2.3–Recall1andFalsealarmforhumansbetween
ECo-HumLa vs HumLa at 50%-human eﬀort that perform
similarly to ECo-HumLa at the cost of similar amount of
humaneﬀort as foundinRQ2.2.
DatasetRecall 1forhumans Falsealarm forhumans
ECo-HumLa HumLa-50% ECo-HumLa HumLa-50%
Bracket 0.4039 0.3338 [-b] 0.1631 0.1787 [-b]
Broadleaf 0.2854 0.2793 [-s] 0.0885 0.1168 [-b]
Camel 0.4253 0.3612 [-b] 0.1815 0.1900 [-b]
Fabric8 0.3488 0.3380 [-m] 0.1434 0.1704 [-b]
jGroup 0.2995 0.2544 [-b] 0.1159 0.1527 [-b]
Nova 0.4246 0.3330 [-b] 0.1573 0.1366 [b]
Tomcat 0.3684 0.3044 [-b] 0.1639 0.1567 [b]
Corefx 0.2932 0.2322 [-b] 0.0734 0.0788 [-m]
Django 0.4745 0.3116 [-b] 0.1026 0.1016 [s]
Rails 0.3038 0.3203 [b] 0.1417 0.1906 [-b]
Rust 0.2208 0.2489 [b] 0.0851 0.1190 [-b]
Tensorﬂow 0.4446 0.3886 [-b] 0.1627 0.1902 [-b]
VScode 0.1924 0.1538 [-b] 0.0551 0.0511 [b]
wp-Calypso 0.2555 0.2531 [-*] 0.0910 0.1075 [-b]
ave-rank 1.14 1.86 1.29 1.71
Symbols[*],[s],[m]and[b]denoteinsigniﬁcant,small,medium,andlargeA12,resp-
ectively. Presence/absenceof the sign “-" in A12means that HumLa at 50%-human
eﬀort was worse/betterthan ECo-HumLa. Eﬀect size of diﬀerences in performance
for eachdatasetweretypicallylarge. The lastrowreports the averageranks across
datasets for the twomethods,whichwerefoundto bestatistically signiﬁcantly
diﬀerentboth in terms of Recall1 and False Alarms.Smallerranks arebetterranks.
reductionineﬀortdoesnotcomeatthecostofpractitionersmissing
atoolargenumberofdefectsinthecode.So,aquestionremains
onwhetherECo-HumLacanreally encouragealargernumberof
defectsto be foundthanHumLa at 50%-humaneﬀort(RQ2.3).
Table4reportsthehumanrecall1thatisformulatedinEq. (5)
and the human false alarm that is formulated in Eq. (6)between
Eco-HumLaandHumLaat50%-humaneﬀort.Wecanseethatin
terms of the human recall 1, practitioners can usually detect more
defects (with the improvement ratio of up to 52.27% for Django
andaround26%forNova,Corefx,andVScode)insoftwarechanges
when they opt for ECo-HumLa compared to HumLa at 50%-human
eﬀort, and such superiority is statistically signiﬁcant accordingto
two-tailedpairwiseWilcoxonsignedranktestsatsigniﬁcancelevel
0.05 (/u1D45D-value 4.03E-03). In the meantime, practitioners can have
lower(better)humanfalsealarm(withtheimprovementratioof
around 25% for Broadleaf, jGroup, Rails and Rust) compared to
HumLa at 50%-human eﬀort, and such superiority is signiﬁcant
according to two-tailed pairwise Wilcoxon signed rank tests at
signiﬁcance level 0.05 ( /u1D45D-value 0.0166). Such results are of practical
signiﬁcance as it means that although ECo-HumLa can get similar
predictive performance at similar human cost compared to HumLa
at 50%-human eﬀort, this targeted human labeling approach can
helppractitionersinspectchangesthataremorelikelytrulydefect-
inducing,encouragingthemto ﬁnd defectsat an early stage.
Answer to RQ2: The proposed ECo-HumLa can save around 50%-
humaneﬀortinterms ofboth thehumanlabelingpercentage and
thecumulativecodechurnwhilestillretainingpredictiveperfor-
manceofJIT-SDPcomparedtoHumLaat100%-humaneﬀort.More-
over,whenadoptingECo-HumLa,practitionerswouldbeencour-
aged to ﬁnd more detects and achieve a lower false alarm rate than
whenrandomlydecidingwhichchangestoinspectbasedonHumLa
at 50%-humaneﬀort.
614A Practical Human Labeling MethodforOnline Just-in-TimeSo/f_tware DefectPrediction ESEC/FSE ’23, December3–9, 2023,San Francisco, CA, USA
7 THREATS TO VALIDITY
Internal Validity. One potential issue for HumLa is that developers
maytakeawhiletodecideactuallabelsofdefect-predictedexam-
ples, resulting in veriﬁcation latency of the “immediate” human
labeling procedure. However, such time delay would be negligi-
blecomparedtothatinthewaitingtimelabelingprocedure.Also,
as such human delay is disregarded, the HumLa labeling proce-
durewillnotproducetrainingexamplesexactlyinchronological
order. Nevertheless, it still leads to a more realistic labeling and
modeltrainingproceduresthantheonesadoptedinexistingliter-
aturewhenthetruetimetakentolabeldefect-inducingsoftware
changes is unknown. Future work could further investigate the
lengthandimpactofsuchtimedelay.Similartopreviousworkthat
has adoptedthe same datasets [ 4,40,42], otherthreatsto internal
validityincludevarioustypesofnoisearisingfromSZZ[ 16,20,34–
36] including noise stemming from the git blame command used
inCommit Guru thatwerenotcapturedbyourmanualKappaanal-
ysis in Section 4. The data were collected based on the original
SZZalgorithm[ 39],whichwasshowntoleadtoJIT-SDPmodelsof
similarpredictiveperformancetomodelscreatedbasedonthemost
recent SZZ algorithm [ 12], which applies the largest number of
noise ﬁlters in comparison to several other SZZ variants [ 8,32,50].
Wealsoadoptedthewaitingtimethatwasshowntoleadtoagood
trade-oﬀbetweenlabelnoiseandtheobsolescenceofthetrained
models[40,42].Tomitigatethreatsrelatedtotherandomnessof
ODaSC,our results are basedon100runsoneachdataset.
Construct Validity. G-Mean, Recall 0 and Recall 1 are unbiased
metrics suitable for class imbalanced problems such as JIT-SDP.
We have also adopted MCC, a popular metric in the SDP literature,
which uses all entries of the confusion matrix. A fading factor is
adopted to enable tracking the ﬂuctuations in predictive perfor-
mance over time,as recommendedfor onlinelearning[ 14].
ExternalValidity. We haveinvestigated14open sourceprojects,
coveringawiderangeofdatacharacteristicsasexplainedinSec-
tion4. However, as with any study involving machine learning,
experimental results may not generalize well to other projects. We
investigate the proposed methods based on ODaSC with Hoeﬀding
trees,whichhavebeenpreviouslyoptedforonlineJIT-SDP[ 40,42].
Other online machine learning models could lead to diﬀerent re-
sults.However,itisworthnotingthatmachinelearningapproaches
are in general expected to struggle more to adapt to a concept drift
if there is no labeled data coming from the new underlying data
distribution than if they had access to such labeled data. Our label-
ing procedures can thus act as enablers for learning approaches to
adapttoconceptdriftmorepromptly,possiblybeneﬁtingpredictive
performance of other learning algorithms. When investigating the
impactofhumaneﬀortwithin(Eco-)HumLa,zerohumannoisewas
assumed,facilitatingafocusedanalysisofhumaneﬀortwithoutbe-
ing aﬀected by human noise. In practice, a non-zero and unknown
levelofhumannoisewouldbeassociatedwith(Eco-)HumLa,which
could leadtodiﬀerentconclusions.Following standardpracticein
theJIT-SDPliterature,wehaveusedthemainbranchofopensource
repositories to collect software changes and their labels. Therefore,
only software changes that have been accepted in the main branch
(possibly after code review) have been used in our evaluation. The
results maynot generalize to predictingrejectedchanges.8 CONCLUSIONS
We have conducted the ﬁrst study on how to consider the eﬀect of
adopting JIT-SDP during the software development process in the
labeling procedure of software changes for online JIT-SDP, leading
totheHumLa procedure.TheimpactofHumLaon thepredictive
performanceofJIT-SDPwasinvestigatedatdiﬀerentlevelsofnoise
and human eﬀort. We have also proposed ECo-HumLa to save
human eﬀort by targeting the inspection process towards software
changes predictedas defect-inducingwithhigher conﬁdence.
Our experiments showed that adopting a labeling procedure
closer to reality leads to a signiﬁcant impact on the predictive per-
formanceofJIT-SDP,withgenerallybetterperformancethanthe
delayedlabelingmethodwithwaitingtimeevenwhenhumanlabel-
ingcontainedacertainlevelofnoise.Asanimplicationtoresearch,
this shows the importance of adopting labeling methods such as
HumLa that are closer to what would be adopted in practice, when
conductingstudies to evaluate JIT-SDP models. As an implication
to practice, it shows the importance of not delaying the inspection
of software changes toachieve better performing JIT-SDPmodels.
We also showed that it is possible to save around 50% of in-
spection eﬀort through ECo-HumLa while maintaining predictive
performancecomparedtoHumLaat100%-humaneﬀortandencour-
agingalargernumberofdefectstobefoundthanwhensavingeﬀort
throughHumLaat50%-humaneﬀort.Asanimplicationtoresearch,
this shows thateﬀort-aware strategies can be designed to work in
anonlinemanner,encouragingfurtherresearchononlineeﬀort-
awareJIT-SDP.Researchersmayalsoanalysetheperformanceof
theirmodelsunderdiﬀerentlabeling eﬀortsthroughECo-HumLa.
Asanimplicationtopractice,theseresultsshowthatifpractitioners
are unable to inspectall defect-predicted software changes due to
the inspection eﬀort, we recommend to target the inspection eﬀort
based on the conﬁdence of the predictions through ECo-HumLa
ratherthanrandomlydecidingwhichchangestoinspect.Iftheycan
aﬀordthehighereﬀortoraredealingwithsafety-criticalsystems,
itisstillrecommendedto use HumLa to ﬁnd more defects.
FutureworkincludesinvestigatingHumLaandECo-HumLawith
other JIT-SDP models, datasets, and input features such as high-
level latent representations of a deep neural network; proposing
noveleﬀort-awareonlineJIT-SDPapproachestofurtherimprove
onECo-HumLa;analyzingtheveriﬁcationlatencyofthehumanin-
spection process; investigating (ECo-)HumLa with JIT-SDP models
forpredictingpre-codereviewchanges;investigatingtheimpact
ofgit blame usedby SZZin thelabel qualityoftheinvestigated
datasets;andinvestigatingtheimpactofvariouslevelsofhuman
eﬀortundervariouslevels of human noisefor (Eco-)HumLa.
DATA AVAILABILITYSTATEMENT
A replication package is available in[ 41]. Thesource code is avail-
ableunderaGNUGPL v3.0 license.
ACKNOWLEDGMENTS
ThisworkwassupportedbyNationalNaturalScienceFoundationof
China(NSFC)underGrantNos.62002148and62250710682,thePro-
gramfor Guangdong Introducing InnovativeandEnterpreneurial
Teams under Grant No. 2017ZT07X386, Guangdong Provincial Key
LaboratoryunderGrantNo.2020B121201001andResearchInstitute
ofTrustworthyAutonomousSystems(RITAS).
615ESEC/FSE ’23, December3–9, 2023,San Francisco, CA, USA Liyan Song,Leandro Lei Minku,CongTeng, XinYao
REFERENCES
[1]MoloudAbdar,FarhadPourpanah,SadiqHussain,DanaRezazadegan,LiLiu,Mo-
hammadGhavamzadeh,PaulFieguth,XiaochunCao,AbbasKhosravi,U.Rajendra
Acharya, Vladimir Makarenkov, and Saeid Nahavandi. 2021. A Review of Uncer-
taintyQuantiﬁcationinDeepLearning:Techniques,ApplicationsandChallenges.
InformationFusion 76(2021),243–297. https://doi.org/10.1016/j.inﬀus.2021.05.008
[2]PeterBludauandAlexanderPretschner.2022. PR-SZZ:Howpullrequestscan
support the tracing of defects in software repositories. In IEEE International
Conference on Software Analysis, Evolution and Reengineering (SANER) . IEEE,
IEEE, Munich, Germany, 1–12. https://doi.org/10.1109/SANER53432.2022.00012
[3]GeorgeGomesCabralandLeandroL.Minku.2023. TowardsReliableOnlineJust-
in-timeSoftwareDefectPrediction. IEEETransactionsonSoftwareEngineering
49,3 (2023), 1342–1358. https://doi.org/10.1109/TSE.2022.3175789
[4]GeorgeG.Cabral,LeandroL.Minku,EmadShihab,andSuhaibMujahid.2019.
Class Imbalance Evolution and Veriﬁcation Latency in Just-in-Time Software
Defect Prediction. In International Conference on Software Engineering . Monteal,
Canada,666–676. https://doi.org/10.1109/ICSE.2019.00076
[5]DavideChiccoandGiuseppeJurman.2020. TheAdvantagesoftheMatthewsCor-
relation Coeﬃcient (MCC) over F1 Score and Accuracy in BinaryClassiﬁcation
Evaluation. BMCGenomics 21(2020). https://doi.org/10.1186/s12864-019-6413-7
[6]Davide Chicco, Matthijs J. Warrens, andGiuseppe Jurman. 2021. TheMatthews
CorrelationCoeﬃcient(MCC)isMoreInformativeThanCohen’sKappaandBrier
Score in Binary Classiﬁcation Assessment. IEEE Access 9 (2021), 78368–78381.
https://doi.org/10.1109/ACCESS.2021.3084050
[7]Jacob Cohen. 1960. A Coeﬃcient of Agreement for Nominal Scales. Educational
and Psychological Measurement 20, 1 (1960), 37–46. https://doi.org/10.1177/
001316446002000104
[8]DanielAlencardaCosta,ShaneMcIntosh,WeiyiShang,UiraKulesza,Roberta
Coelho, and Ahmed E. Hassan. 2017. A framework for evaluating the resultsof
theSZZapproachforidentifyingbug-introducingchanges. IEEETransactions
on Software Engineering 43, 7 (2017), 641–657. https://doi.org/10.1109/TSE.2016.
2616306
[9]AndreaDalPozzolo,GiacomoBoracchi,OlivierCaelen,CesareAlippi,andGi-
anluca Bontempi. 2018. CreditCard FraudDetection:ARealisticModelingand
aNovelLearningStrategy. IEEETransactionsonNeuralNetworksandLearning
Systems29,8 (2018), 3784–3797. https://doi.org/10.1109/TNNLS.2017.2736643
[10]Janez Demšar. 2006. Statistical Comparisons of Classiﬁers over Multiple Data
Sets.The Journal ofMachineLearning Research 7 (2006), 1–30.
[11]KarlDyer,RobertCapo,andRobiPolikar.2014. COMPOSE:ASemisupervised
LearningFrameworkforInitiallyLabeledNonstationaryStreamingData. IEEE
TransactionsonNeuralNetworksandLearningSystems 25(2014),12–26. https:
//doi.org/10.1109/TNNLS.2013.2277712
[12]Yuanrui Fan, Xin Xia, Daniel Alencar da Costa, David Lo, Ahmed E. Hassan,
and Shanping Li. 2021. The Impact of Mislabeled Changes by SZZ on Just-in-
TimeDefectPrediction. IEEETransactionsonSoftwareEngineering 47,8(2021),
1559–1586. https://doi.org/10.1109/TSE.2019.2929761
[13]TakafumiFukushima,YasutakaKamei,ShaneMcIntosh,KazuhiroYamashita,and
NaoyasuUbayashi.2014. AnEmpiricalStudy ofJust-in-TimeDefectPrediction
Using Cross-Project Models. In Working Conference on Mining Software Reposito-
ries(Hyderabad, India).172–181. https://doi.org/10.1145/2597073.2597075
[14]Joao Gama, Raquel Sebastiao, and Pedro Pereira Rodrigues. 2013. On Evaluating
Stream Learning Algorithms. Journal of Machine Learning 90, 3 (2013), 317–346.
https://doi.org/10.1007/s10994-012-5320-9
[15]TracyHall,MinZhang,DavidBowes,andYiSun.2014. SomeCodeSmellsHavea
Signiﬁcant but Small Eﬀect on Faults. ACM Transactions on Software Engineering
and Methodology 23,4 (2014). https://doi.org/10.1145/2629648
[16]SteﬀenHerbold,AlexanderTrautsch,FabianTrautsch,andBenjaminLedel.2022.
Problemswith SZZand Features: An Empirical Study of the State of Practice of
DefectPredictionDataCollection. EmpiricalSoftwareEngineering 27,2(2022).
https://doi.org/10.1007/s10664-021-10092-4
[17]Yasutaka Kamei, Takafumi Fukushima, Shane Mcintosh, Kazuhiro Yamashita,
NaoyasuUbayashi,andAhmedE.Hassan.2016. StudyingJust-in-TimeDefect
Prediction Using Cross-project Models. Empirical Software Engineering 21, 5
(2016), 2072–2106. https://doi.org/10.1007/s10664-015-9400-x
[18]Yasutaka Kamei, Shinsuke Matsumoto, Akito Monden, Ken-ichi Matsumoto,
BramAdams,andAhmedEHassan.2010. RevisitingCommonBugPrediction
FindingsUsingEﬀort-awareModels.In IEEEInternationalConferenceonSoftware
Maintenance . 1–10.https://doi.org/10.1109/ICSM.2010.5609530
[19]Yasutaka Kamei, Emad Shihab, Bram Adams, Ahmed E. Hassan, Audris Mockus,
Anand Sinha, and Naoyasu Ubayashi. 2013. A Large-Scale Empirical Study of
Just-in-Time Quality Assurance. IEEE Transactions on Software Engineering 39, 6
(2013), 757–773. https://doi.org/10.1109/TSE.2012.70
[20]Sunghun Kim, E. James Whitehead, and Yi Zhang. 2008. Classifying Software
Changes:CleanorBuggy? IEEETransactionsonSoftwareEngineering 34,2(2008),
181–196. https://doi.org/10.1109/TSE.2007.70773
[21]MasanariKondo,DanielMGerman,OsamuMizuno,andEun-HyeChoi.2020.The
Impact of Context Metrics on Just-In-Time Defect Prediction. Empirical Software
Engineering 25(2020), 890–939. https://doi.org/10.1007/s10664-019-09736-3[22]Masanari Kondo, Yutaro Kashiwa, Yasutaka Kamei, and Osamu Mizuno. 2022
(in press). An Empirical Study of Issue-Link Algorithms: Which Issue-Link
AlgorithmsShouldWeUse? EmpiricalSoftwareEngineering 27,6(2022(inpress)).
https://doi.org/10.1007/s10664-022-10120-x
[23]Miroslav Kubat, Robert Holte, and Stan Matwin. 1997. Learning When Negative
Examples Abound. In European Conference on Machine Learning . 146–153. https:
//doi.org/10.1007/3-540-62858-4_79
[24]LudmilaI.KunchevaandJ.SalvadorSánchez.2008. NearestNeighbourClassiﬁers
forStreamingDatawithDelayed Labelling. In IEEEInternationalConference on
DataMining . 869–874. https://doi.org/10.1109/ICDM.2008.33
[25]Stefan Lessmann, Bart Baesens, Christophe Mues, and Swantje Pietsch. 2008.
Benchmarking Classiﬁcation Models for Software Defect Prediction: A Proposed
Framework and Novel Findings. IEEE Transactions on Software Engineering 34, 4
(2008), 485–496. https://doi.org/10.1109/TSE.2008.35
[26]Minlong Lin, Ke Tang, and Xin Yao. 2013. Dynamic Sampling Approach
to Training Neural Networks for Multiclass Imbalance Classiﬁcation. IEEE
Transactions on Neural Networks and Learning Systems 24, 4 (2013), 647–660.
https://doi.org/10.1109/TNNLS.2012.2228231
[27]Shane McIntosh and Yasutaka Kamei. 2018. Are Fix-Inducing Changes a Moving
Target? A Longitudinal Case Study of Just-In-Time Defect Prediction. IEEE
Transactions on Software Engineering 44, 5(2018), 412–428. https://doi.org/10.
1145/3180155.3182514
[28]Ayse Tosun Misirli, Emad Shihab, and Yasukata Kamei. 2016. Studying High
ImpactFix-InducingChanges. EmpiricalSoftwareEngineeringJournal 21,2(2016),
605–641. https://doi.org/10.1007/s10664-015-9370-z
[29]AudrisMockus andDavidM.Weiss.2000. Predicting RiskofSoftwareChange.
Bell Labs Technical Journal 5, 2 (2000), 169–180. https://doi.org/10.1002/bltj.2229
[30]Jacob Montiel, Jesse Read, Albert Bifet, and Talel Abdessalem. 2018. Scikit-
Multiﬂow:AMulti-outputStreamingFramework. JournalofMachineLearning
Research19,72(2018), 1–5.
[31]Nachiappan Nagappan and Thomas Ball. 2005. Use of Relative Code Churn Mea-
surestoPredictSystemDefectDensity.In InternationalConferenceonSoftware
Engineering (ICSE) . 284–292. https://doi.org/10.1145/1062455.1062514
[32]EdmilsonCamposNeto,DanielAlencardaCosta,andUiráKulesza.2018. The
impact of refactoring changes on the SZZ algorithm: An empirical study. In
InternationalConference onSoftware Analysis,Evolutionand Reengineering . 380–
390.https://doi.org/10.1109/SANER.2018.8330225
[33]Hien M. Nguyen, Eric W. Cooper, and Katsuari Kamei. 2011. Online Learning
from Imbalanced Data Streams. In International Conference of Soft Computing
and Pattern Recognition . 347–352. https://doi.org/10.1109/SoCPaR.2011.6089268
[34]Yusuf Sulistyo Nugroho, Hideaki Hata, and Kenichi Matsumoto. 2020. How
diﬀerentarediﬀerentdiﬀalgorithmsinGit?Use–histogramforcodechanges.
Empirical Software Engineering 25 (2020), 790–823. https://doi.org/10.1007/
s10664-019-09772-z
[35]ChristopheRezk,YasutakaKamei,andShaneMcIntosh.2022. TheGhostCommit
ProblemWhenIdentifyingFix-InducingChanges:AnEmpiricalStudyofApache
Projects. IEEE Transactions on Software Engineering 48, 9 (2022), 3297–3309.
https://doi.org/10.1109/TSE.2021.3087419
[36]GemaRodríguez-Pérez,MeiyappanNagappan,andGregorioRobles.2022. Watch
OutforExtrinsicBugs!ACaseStudyofTheirImpactinJust-In-TimeBugPredic-
tionModelsontheOpenStackProject. IEEETransactionsonSoftwareEngineering
48,4 (2022), 1400–1416. https://doi.org/10.1109/TSE.2020.3021380
[37]Christoﬀer Rosen, Ben Grawi, and Emad Shihab. 2015. Commit Guru: analytics
and risk prediction of software commits. In International Symposium on the
Foundationsof SoftwareEngineering .966–969. https://doi.org/10.1145/2786805.
2803183
[38]EmadShihab,AhmedE.Hassan,BramAdams,andZhenMingJiang.2012. An
IndustrialStudyontheRiskofSoftwareChanges.In InternationalSymposiumon
theFoundationsofSoftwareEngineering .1–11.https://doi.org/10.1145/2393596.
2393670
[39]Jacek Śliwerski, Thomas Zimmermann, and Andreas Zeller. 2005. When Do
Changes Induce Fixes? ACM SIGSOFT Software Engineering Notes 30, 4 (2005),
1–5.https://doi.org/10.1145/1082983.1083147
[40]Liyan Song, Shuxian Li, Leandro L. Minku, and Xin Yao. 2022. A Novel Data
StreamLearningApproachtoTackleOne-SidedLabelNoiseFromVeriﬁcation
Latency.In InternationalJointConferenceonNeuralNetworks(IJCNN) .1–8.https:
//doi.org/10.1109/IJCNN55064.2022.9891911
[41]LiyanSong,LeandroMinku,CongTeng,andYaoXin.2023. ArtifactandData
for“APracticalHumanLabelingMethodforOnlineJust-in-TimeSoftwareDefect
Prediction" .https://doi.org/10.5281/zenodo.8272293
[42]Liyan Song and Leandro L. Minku. 2023. A Procedure to Continuously Evalu-
atePredictivePerformanceofJust-In-TimeSoftwareDefectPredictionModels
DuringSoftwareDevelopment. IEEETransactionsonSoftwareEngineering 49,2
(2023), 646–666. https://doi.org/10.1109/TSE.2022.3158831
[43]Vinicius M.A. Souza, Diego F. Silva, Gustavo E.A.P.A. Batista, and João Gama.
2015. Classiﬁcation of Evolving Data Streams with Inﬁnitely Delayed Labels. In
InternationalConferenceonMachineLearningandApplications(ICMLA) .214–219.
https://doi.org/10.1109/ICMLA.2015.174
616A Practical Human Labeling MethodforOnline Just-in-TimeSo/f_tware DefectPrediction ESEC/FSE ’23, December3–9, 2023,San Francisco, CA, USA
[44]Sadia Tabassum, Leandro L. Minku, Danyi Feng, George G. Cabral, and Liyan
Song.2020. AnInvestigationofCross-ProjectLearninginOnlineJust-in-Time
Software Defect Prediction. In International Conference on Software Engineering .
554–565. https://doi.org/10.1145/3377811.3380403
[45]MingTan,LinTan,SashankDara,andCalebMayeux.2015. OnlineDefectPre-
diction for Imbalanced Data. In International Conference on Software Engineering .
99–108.https://doi.org/10.1109/ICSE.2015.139
[46]AlexanderTarvo,NachiappanNagappan,ThomasZimmermann,Thirumalesh
Bhat, and Jacek Czerwonka. 2013. Predicting Risk of Pre-Release Code Changes
with Checkinmentor. In International Symposium on Software Reliability Engi-
neering. 128–137. https://doi.org/10.1109/ISSRE.2013.6698912
[47]Andras Vargha and Harold D. Delaney. 2000. A Critique and Improvement
of the “CL" Common Language Eﬀect Size Statistics of McGraw and Wong.
Journal of Educational and Behavioral Statistics 25, 2 (2000), 101–132. https:
//doi.org/10.3102/10769986025002101
[48]Zhiyuan Wan, Xin Xia, Ahmed E. Hassan, David Lo, Jianwei Yin, and Xiaohu
Yang. 2020. Perceptions, Expectations, and Challenges in Defect Prediction.
IEEE Transactions on Software Engineering 46, 11 (2020), 1241–1266. https:
//doi.org/10.1109/TSE.2018.2877678
[49]Shuo Wang, Leandro L. Minku, and Xin Yao. 2018. A Systematic Study of Online
ClassImbalanceLearningWithConceptDrift. IEEETransactiononNeuralNet-
works and Learning Systems 29, 10 (2018), 4802–4821. https://doi.org/10.1109/
TNNLS.2017.2771290
[50]ChaddWilliamsandJaimeSpacco.2008. SZZRevisited:VerifyingWhenChanges
Induce Fixes. In Proceedings of the 2008 Workshop on Defects in Large SoftwareSystems. 32–36.https://doi.org/10.1145/1390817.1390826
[51] Limin Yang, Xiangxue Li, and Yu Yu. 2017. Vuldigger: A Just-In-Time and Cost-
Aware Tool for Digging Vulnerability-Contributing Changes. In IEEE Global
CommunicationsConference .1–7.https://doi.org/10.1109/GLOCOM.2017.8254428
[52]XingguangYang,HuiqunYu,GuishengFan,KaiShi,LiqiongChen,andEmiliano
Tramontana. 2019. Local versus Global Models for Just-In-Time Software Defect
Prediction. Scientiﬁc Programming 2019(2019),1–13. https://doi.org/10.1155/
2019/2384706
[53]YibiaoYang,YumingZhou,JinpingLiu,YangyangZhao,HongminLu,LeiXu,
Baowen Xu, and Hareton Leung. 2016. Eﬀort-Aware Just-in-Time Defect Predic-
tion:SimpleUnsupervisedModelsCouldBeBetterthanSupervisedModels.In
ACMSIGSOFTInternationalSymposiumonFoundationsofSoftwareEngineering
(FSE). 157–168. https://doi.org/10.1145/2950290.2950353
[54]Y. Zhao, K. Damevski, and H. Chen. 2023. A Systematic Survey of Just-in-
Time Software Defect Prediction. Comput. Surveys 55, 10 (2023), 201.1–201.35.
https://doi.org/10.1145/3567550
[55]QiumingZhu.2020. OnthePerformanceofMatthewsCorrelationCoeﬃcient
(MCC) for Imbalanced Dataset. Pattern Recognition Letters 136 (2020), 71–80.
https://doi.org/10.1016/j.patrec.2020.03.030
[56]XiaoyanZhu,ChenyuYan,EJamesWhiteheadJr,BinbinNiu,LeiZhu,andLong
Pan. 2022. Just-In-Time Defect Prediction for Software Hunks. Software: Practice
and Experience 52,1 (2022), 130–153. https://doi.org/10.1002/spe.3001
Received 2023-02-02; accepted 2023-07-27
617