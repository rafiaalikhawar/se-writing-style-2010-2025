Unblind Your Apps: Predicting Natural-Language Labels for
Mobile GUI Components by Deep Learning
Jieshan Chen
Jieshan.Chen@anu.edu.au
Australian National University
AustraliaChunyang Chenâˆ—
Chunyang.Chen@monash.edu
Monash University
AustraliaZhenchang Xingâ€ 
Zhenchang.Xing@anu.edu.au
Australian National University
Australia
Xiwei Xu
Xiwei.Xu@data61.csiro.au
Data61, CSIRO
AustraliaLiming Zhuâ€ â€¡
Liming.Zhu@data61.csiro.au
Australian National University
AustraliaGuoqiang Liâˆ—
Li.G@sjtu.edu.cn
Shanghai Jiao Tong University
China
Jinshui Wangâˆ—
ymkscom@gmail.com
Fujian University of Technology
China
ABSTRACT
According to the World Health Organization(WHO), it is estimated
thatapproximately1.3billionpeoplelivewithsomeformsofvision
impairment globally, of whom 36 million are blind. Due to their
disability, engaging these minority into the society is a challenging
problem. The recent rise of smart mobile phones provides a new
solution by enabling blind usersâ€™ convenient access to the infor-
mation and service for understanding the world. Users with vision
impairmentcanadoptthescreenreaderembeddedinthemobileoperating systems to read the content of each screen within theapp, and use gestures to interact with the phone. However, the
prerequisite of using screen readers is that developers have to add
natural-language labels to the image-based components when they
aredevelopingtheapp.Unfortunately,morethan77%appshaveissues of missing labels, according to our analysis of 10,408 An-
droidapps.Mostoftheseissuesarecausedbydevelopersâ€™lackof
awareness and knowledge in considering the minority. And even if
developers want to add the labels to UI components, they may not
come up with concise and clear description as most of them are of
no visual issues. To overcome these challenges, we develop a deep-
learning based model, called LabelDroid, to automatically predict
thelabelsofimage-basedbuttonsbylearningfromlarge-scalecom-
mercialapps inGooglePlay.Theexperimentalresultsshowthat
âˆ—Corresponding author.
â€ Also with Data61, CSIRO.
â€¡Also with University of New South Wales.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation
on the first page. Copyrights for components of this work owned by others than ACM
mustbehonored.Abstractingwithcreditispermitted.Tocopyotherwise,orrepublish,topostonserversortoredistributetolists,requirespriorspecificpermissionand/ora
fee. Request permissions from permissions@acm.org.
ICSE â€™20, May 23â€“29, 2020, Seoul, Republic of Korea
Â© 2020 Association for Computing Machinery.
ACM ISBN 978-1-4503-7121-6/20/05...$15.00
https://doi.org/10.1145/3377811.3380327our model can make accurate predictions and the generated labels
are of higher quality than that from real Android developers.
CCS CONCEPTS
â€¢Human-centered computing â†’Accessibility systems and
tools;Empirical studies in accessibility ;â€¢Software and its engi-
neeringâ†’Software usability.
KEYWORDS
Accessibility, neural networks, user interface, image-based buttons,
content description
ACM Reference Format:
Jieshan Chen, Chunyang Chen, Zhenchang Xing, Xiwei Xu, Liming Zhu,
GuoqiangLi,andJinshuiWang.2020.UnblindYourApps:PredictingNatural-
LanguageLabelsforMobileGUIComponentsbyDeepLearning.In 42nd
International Conference on Software Engineering (ICSE â€™20), May 23â€“29,
2020, Seoul, Republic of Korea. ACM, New York, NY, USA, 13 pages. https:
//doi.org/10.1145/3377811.3380327
1 INTRODUCTION
GivenmillionsofmobileappsinGooglePlay[ 11]andAppstore[ 8],
the smart phones are playing increasingly important roles in daily
life.Theyareconvenientlyusedtoaccessawidevarietyofservices
such as reading, shopping, chatting, etc. Unfortunately, many apps
remain difficult or impossible to access for people with disabili-
ties. For example, a well-designed user interface (UI) in Figure 1
oftenhaselementsthatdonâ€™trequireanexplicitlabeltoindicate
their purpose to the user. A checkbox next to an item in a task
list application has a fairly obvious purpose for normal users, asdoes a trash can in a file manager application. However, to users
withvisionimpairment,especially fortheblind,otherUIcuesare
needed. According to the World Health Organization(WHO) [ 4], it
isestimatedthatapproximately1.3billionpeoplelivewithsome
formofvisionimpairmentglobally,ofwhom36millionareblind.
Comparedwiththenormalusers,theymaybemoreeagertouse
the mobile apps to enrich their lives, as they need those apps to
3222020 IEEE/ACM 42nd International Conference on Software Engineering (ICSE)
323324325326327Unblind Your Apps: Predicting Natural-Language Labels for Mobile GUI Components by Deep Learning ICSE â€™20, May 23â€“29, 2020, Seoul, Republic of Korea
Table 2: Details of our accessibility dataset.
#App #Screenshot #Element
Train 6,175 10,566 15,595
Validation 714 1,204 1,759
Test 705 1,375 1,879
Total 7,594 13,145 19,233
tokenstothestartandtheendofthesequence.Wealsoreplacethe
low-frequency words (less than five) with an <ğ‘¢ğ‘›ğ‘˜>token. To
enable the mini-batch training, we need to add a <ğ‘ğ‘ğ‘‘>token to
padthewordsequenceoflabelsintoafixedlength.Notethatthe
maximum number of words for one label is 15 in this work.
After the data cleaning, we finally collect totally 19,233 pairs of
image-basedbuttonsandcontentdescriptionsfrom7,594apps.Note
thattheappnumberissmallerthanthatinSection3astheappswith
nooruninformativelabelsareremoved.Wesplitcleaneddataset
into training, validation3and testing set. For each app category,
werandomlyselect80%appsfortraining,10%forvalidationand
the rest 10% for testing. Table 2 shows that, there are 15,595 image-
based buttons from 6,175 apps as the training set, 1,759 buttons
from 714 apps as validation set and 1,879 buttons from 705 apps as
testing set. The dataset can also be downloaded from our site.
5.2 Model Implementation
We use ResNet-101 architecture [ 46] pretrained on MS COCO
dataset [57] as our CNN module. As you can see in the leftmost
ofFigure6,itconsistsofaconvolutionlayer,amaxpoolinglayer,
four types of blocks with different numbers of block (denoted indifferent colors). Each type of block is comprised of three convo-
lutionallayerswithdifferentsettingsandimplementsanidentity
shortcut connection which is the core idea of ResNet. Instead of
approximating the target output of current block, it approximates
the residual between current input and target output, and then the
target output can be computed by adding the predicted residual
andtheoriginalinputvector.Thistechniquenotonlysimplifiesthe
trainingtask,butalsoreducesthenumberoffilters.Inourmodel,
weremovethelastglobalaveragepoolinglayerofResNet-101to
computea sequence ofinputforthe consequentencoder-decoder
model.
For transformer encoder-decoder, we take ğ‘=3,ğ‘‘ğ‘’ğ‘šğ‘ğ‘’ğ‘‘ =512,
ğ‘‘ğ‘˜=ğ‘‘ğ‘£=ğ‘‘ğ‘šğ‘œğ‘‘ğ‘’ğ‘™ =64,ğ‘‘ğ‘“ğ‘“=2048,â„=8andthevocabularysize
as 633. We train the CNN and the encoder-decoder model in anend-to-end manner using KL divergence loss [
53]. We use Adam
optimizer [ 51] withğ›½1=0.9,ğ›½2=0.98 andğœ–=10âˆ’9and change
thelearningrateaccordingtotheformula ğ‘™ğ‘’ğ‘ğ‘Ÿğ‘›ğ‘–ğ‘›ğ‘”_ğ‘Ÿğ‘ğ‘¡ğ‘’=ğ‘‘âˆ’0.5
ğ‘šğ‘œğ‘‘ğ‘’ğ‘™Ã—
ğ‘šğ‘–ğ‘›(ğ‘ ğ‘¡ğ‘’ğ‘_ğ‘›ğ‘¢ğ‘šâˆ’0.5,ğ‘ ğ‘¡ğ‘’ğ‘_ğ‘›ğ‘¢ğ‘š Ã—ğ‘¤ğ‘ğ‘Ÿğ‘šğ‘¢ğ‘_ğ‘ ğ‘¡ğ‘’ğ‘ğ‘ âˆ’1.5)totrainthemodel,
whereğ‘ ğ‘¡ğ‘’ğ‘_ğ‘›ğ‘¢ğ‘šisthecurrentiterationnumberoftrainingbatch
andthefirst ğ‘¤ğ‘ğ‘Ÿğ‘š_ğ‘¢ğ‘trainingstepisusedtoacceleratetraining
processbyincreasingthelearningrateattheearlystageoftraining.
Our implementation uses PyTorch [ 17] on a machine with Intel
i7-7800XCPU,64GRAMandNVIDIAGeForceGTX1080TiGPU.
3for tuning the hyperparameters and preventing the overfitting6 EVALUATION
WeevaluateourLabelDroidinthreeaspects,i.e.,accuracywith
automated testing, generality and usefulness with user study.
6.1 Evaluation Metric
Toevaluatetheperformanceofourmodel,weadoptfivewidely-
used evaluation metrics including exact match, BLEU [ 62], ME-
TEOR [26], ROUGE [ 56], CIDEr [ 69] inspired by related works
about image captioning. The first metric we use is exact match
rate,i.e.,thepercentageoftestingpairswhosepredictedcontentdescription exactly matches the ground truth. Exact match is a
binarymetric,i.e.,0ifanydifference,otherwise1.Itcannottellthe
extent to which a generated content description differs from theground-truth.Forexample,thegroundtruthcontentdescription
maycontain4words,butnomatteroneor4differencesbetween
thepredictionandgroundtruth,exactmatchwillregardthemas
0. Therefore, we also adopt other metrics. BLEU is an automaticevaluation metric widely used in machine translation studies. Itcalculates the similarity of machine-generated translations andhuman-created reference translations (i.e., ground truth). BLEU
isdefinedastheproductof ğ‘›-gramprecisionandbrevitypenalty.
As most content descriptions for image-based buttons are short,
we measure BLEU value by setting ğ‘›as 1, 2, 3, 4, represented as
BLEU@1, BLEU@2, BLEU@3, BLEU@4.
METEOR[ 26](MetricforEvaluationofTranslationwithExplicit
ORdering)isanothermetricusedformachinetranslationevalua-
tion.ItisproposedtofixsomedisadvantagesofBLEUwhichignores
theexistenceofsynonymsandrecallratio.ROUGE(Recall-Oriented
Understudy for Gisting Evaluation) [ 56] is a set of metric based on
recallrate,andweuseROUGE-L,whichcalculatesthesimilaritybetween predicted sentence and reference based on the longestcommon subsequence (short for LCS). CIDEr (Consensus-BasedImage Description Evaluation) [
69] uses term frequency inverse
documentfrequency(tf-idf)[ 64]tocalculatetheweightsinrefer-
ence sentence ğ‘ ğ‘–ğ‘—for different n-gram ğ‘¤ğ‘˜because it is intuitive to
believethatararen-gramswouldcontainmoreinformationthana
commonone.WeuseCIDEr-D,whichadditionallyimplementsalength-basedgaussianpenaltyandismorerobusttogaming.We
then divide CIDEr-D by 10 to normalize the score into the range
between 0 and 1. We still refer CIDEr-D/10 to CIDEr for brevity.
All of these metrics give a real value with range [0,1] and are
usually expressed as a percentage. The higher the metric score, the
moresimilarthemachine-generatedcontentdescriptionistothe
ground truth. If the predicted results exactly match the groundtruth, the score of these metrics is 1 (100%). We compute these
metrics using coco-caption code [39].
6.2 Baselines
We set up two state-of-the-art methods which are widely used for
image captioning as the baselines to compare with our contentdescription generation method. The first baseline is to adopt the
CNNmodeltoencodethevisualfeaturesastheencoderandadoptaLSTM(long-shorttermmemoryunit)asthedecoderforgeneratingthecontentdescription[
48,70].Thesecondbaselinealsoadoptthe
encoder-decoderframework.AlthoughitadoptstheCNNmodelas
328329330ICSE â€™20, May 23â€“29, 2020, Seoul, Republic of Korea Jieshan Chen, Chunyang Chen, Zhenchang Xing, et al.
Table 6: The acceptability score (AS) and the standard deviation for 12 completely unseen apps. * denotes ğ‘<0.05.
ID Package name Category #Installation #Image-based button AS-M AS-A1 AS-A2 AS-A3
1com.handmark.sportcaster sports 5M - 10M 8 4.63(0.48) 3.13(0.78) 3.75(1.20) 4.38(0.99)
2com.hola.launcher personalization 100M - 500M 10 4.40(0.92) 3.20(1.08) 3.50(1.75) 3.40(1.56)
3com.realbyteapps.moneymanagerfree finance 1 M-5 M 24 4.29(1.10) 3.42(1.29) 3.75(1.45) 3.83(1.55)
4com.jiubang.browser communication 5M - 10M 11 4.18(1.34) 3.27(1.21) 3.73(1.54) 3.91(1.38)
5audio.mp3.music.player media_and_video 5M - 10M 26 4.08(1.24) 2.85(1.06) 2.81(1.62) 3.50(1.62)
6audio.mp3.mp3player music_and_audio 1 M-5 M 16 4.00(1.27) 2.75(1.15) 3.31(1.53) 3.25(1.39)
7com.locon.housing lifestyle 1 M-5 M 10 4.00(0.77) 3.50(1.12) 3.60(1.28) 4.40(0.80)
8com.gau.go.launcherex.gowidget.weatherwidget weather 50M - 100M 12 3.42(1.66) 2.92(1.38) 3.00(1.78) 3.42(1.80)
9com.appxy.tinyscanner business 1 M-5 M 13 3.85(1.23) 3.31(1.20) 3.08(1.59) 3.38(1.44)
10com.jobkorea.app business 1 M-5 M 15 3.60(1.67) 3.27(1.57) 3.13(1.67) 3.60(1.54)
11browser4g.fast.internetwebexplorer communication 1 M-5 M 4 3.25(1.79) 2.00(0.71) 2.50(1.12) 2.50(1.66)
12com.rcplus social 1 M-5 M 7 3.14(1.55) 2.00(1.20) 2.71(1.58) 3.57(1.29)
AVERAGE 13 3.97*(1.33) 3.06(1.26) 3.27(1.60) 3.62(1.52)
Table 7: Examples of generalization.
ID E1 E2 E3 E4 E5
Button
M next song add to favorites open ad previous song clear query
A1 change tothenext songinplaylist addthemp3as favorite showmore details about SVIP palythe former one cleancontent
A2 playthenext song like check playthe last song close
A3 next like enter last close
whichsignificantlyoutperformsthreedevelopersby30.0%,21.6%,
9.7%. The evaluator rates 51.3% of labels generated from Label-
Droid as highly acceptable (5 point), as opposed to 18.59%, 33.33%,
44.23% from three developers. Figure 9 shows that our model be-
havesbettersinmostappscomparedwiththreehumanannotators.
Theseresultsshowthatthequalityofcontentdescriptionfromour
modelishigherthanthatfromjuniorAndroidappdevelopers.Note
thattheevaluatorisreliableasboth2intentionalinsertedwrong
labels and 2 good labels get 1 and 5 acceptability score as expected.
Tounderstandthesignificanceofthedifferencesbetweenfour
kindsofcontentdescription,wecarryouttheWilcoxonsigned-rank
test [74] between the scores of our model and each annotator and
between the scores of any two annotators. It is the non-parametric
version of the paired T-test and widely used to evaluate the differ-
ence between two related paired sample from the same probability
distribution. The test resultssuggest that the generated labels from
our model are significantlybetter than that of developers ( ğ‘-value
<0.01 for A1, A2, and <0.05 for A3)6.
For some buttons, the evaluator gives very low acceptability
scoreto thelabelsfrom developers.According toourobservation,
we summarise four reasons accounting for those bad cases and
give some examples in Table 7. (1) Some developers are prone to
write long labels for image-based buttons like the developer A1.
Although the long label can fully describe the button (E1, E2), it is
too verbose for blind users especially when there are many image-
based buttons within one page. (2) Some developers give too short
labelswhichmaynotbeinformativeenoughforusers.Forexample,
A2 and A3 annotate the â€œadd to favoriteâ€ button as â€œlikeâ€ (E2).
Since this button will trigger an additional action (add this song to
favoritelist),â€œlikeâ€couldnotexpressthismeaning.Thesamereason
6Thep-valuesareadjustedbyBenjamin&Hochbergmethod[ 27].Alldetailedp-values
are listed in https://github.com/chenjshnn/LabelDroidappliestoA2/A3â€™slabelsforE2andsuchshortlabelsdonotcontain
enough information. (3) Some manual labels may be ambiguous
which may confuse users. For example, A2 and A3 annotate â€œplay
thelastsongâ€orâ€œlastâ€toâ€œprevioussongâ€button(E4)whichmay
misleadusersthatclickingthisbuttonwillcometothefinalsong
in the playlist. (4) Developers may make mistakes especially when
theyareaddingcontentdescriptionstomanybuttons.Forexample,
A2/A3 use â€œcloseâ€ to label a â€œclear queryâ€ buttons (E5). We further
manuallycheck135low-quality(acceptabilityscore=1)labelsfrom
annotatorsintothesefourcategories.18casesareverboselabels,21
ofthemareuninformative,sixcasesareambiguouswhichwould
confuse users, and the majority, 90 cases are wrong.
We also receive some informal feedback from the developers
and the evaluator. Some developers mention that one image-based
buttonmayhavedifferentlabelsindifferentcontext,buttheyare
not very sure if the created labels from them are suitable or not.
MostofthemneverconsideraddingthelabelstoUIcomponents
duringtheirappdevelopmentandcurioushowthescreenreader
works for the app. All of them are interested in our LabelDroid
andtellthattheautomaticgenerationofcontentdescriptionsfor
iconswilldefinitelyimprovetheuserexperienceinusingthescreen
reader. All of these feedbacks indicate their unawareness of app
accessibility and also confirm the value of our tool.
7 RELATED WORK
Mobile devices are ubiquitous, and mobile apps are widely usedfor different tasks in peopleâ€™s daily life. Consequently, there are
manyresearchworksforensuringthequalityofmobileapps[ 29,
43,47]. Most of them are investigating the appsâ€™ functional and
non-functional properties like compatibility [ 73], performance [ 58,
81], energy-efficiency [ 24,25], GUI design [ 31,36], GUI animation
linting[80],localization[ 72]andprivacyandsecurity[ 37,38,40,
331Unblind Your Apps: Predicting Natural-Language Labels for Mobile GUI Components by Deep Learning ICSE â€™20, May 23â€“29, 2020, Seoul, Republic of Korea
42,76]. However, few of them are studying the accessibility issues,
especiallyforuserswithvisionimpairmentwhichisfocusedinour
work.
7.1 App Accessibility Guideline
Google and Apple are the primary organizations that facilitate
mobiletechnologyandtheappmarketplacebyAndroidandIOS
platforms.Withtheawarenessoftheneedtocreatemoreaccessibleapps,bothofthemhavereleaseddeveloperanddesignerguidelines
for accessibility [ 6,15] which include not only the accessibility
designprinciples, butalso thedocumentsfor usingassistive tech-
nologies embedding in the operating system [ 15], and testing tools
or suits for ensuring the app accessibility. The World Wide Web
Consortium(W3C)hasreleasedtheirwebaccessibilityguideline
long time ago [ 21] And now they are working towards adapting
theirwebaccessibilityguideline[ 21]byaddingmobilecharacter-
istics into mobile platforms. Although it is highly encouraged to
followtheseguidelines,theyareoftenignoredbydevelopers.Differ-
ent from these guidelines, our work is specific to users with vision
impairmentandpredictsthelabelduringthedevelopingprocess
without requiring developers to fully understand long guidelines.
7.2 App Accessibility Studies for Blind Users
Many works in Human-Computer Interactionarea have explored
the accessibility issues of small-scale mobile apps [ 63,75] in differ-
ent categories such as in health [ 71], smart cities [ 60] and govern-
mentengagement[ 50].Althoughtheyexploredifferentaccessibility
issues, the lack of descriptions for image-based components has
beencommonlyexplicitlynotedasasignificantprobleminthese
works. Park et al [ 63] rated the severity of errors as well as fre-
quency, and missing labels is rated as the highest severity of tenkinds of accessibility issues. Kane et al [
49] carry out a study of
mobiledeviceadoptionandaccessibilityforpeoplewithvisualand
motor disabilities. Ross et al [ 66] examine the image-based button
labeling in a relative large-scale android apps, and they specify
some common labeling issues within the app. Different from their
works, our study includes not only the largest-scale analysis of
image-basedbuttonlabelingissues,butalsoasolutionforsolving
those issues by a model to predict the label of the image.
There are also some works targeting at locating and solving the
accessibility issues, especially for users with vision impairment.
Eleretal[ 41]developanautomatedtestgenerationmodeltody-
namicallytestthemobileapps.Zhangetal[ 78]leveragethecrowd
sourcemethodtoannotatetheGUIelementwithouttheoriginal
content description. For other accessibility issues, they further de-
velop an approach to deploy the interaction proxies for runtime
repair and enhancement of mobile application accessibility [ 77]
withoutreferringtothesourcecode.Althoughtheseworkscanalso
helpensurethequalityofmobileaccessibility,theystillneedmucheffortfromdevelopers.Instead,themodelproposedinourworkcan
automaticallyrecommendthelabelforimage-basedcomponents
and developers can directly use it or modify it for their own apps.
7.3 App Accessibility Testing Tools
It is also worth mentioning some related non-academic projects.
There are mainly two strategies for testing app accessibility (forusers with vision impairment) such as manual testing, and auto-mated testing with analysis tools. First, for manual testing, the
developerscanusethebuilt-inscreenreaders(e.g.,TalkBack[ 12]
for Android, VoiceOver [ 20] for IOS) to interact with their Android
device without seeing the screen. During that process, developers
can find out if the spoken feedback for each element conveys its
purpose.Similarly,theAccessibilityScannerapp[ 5]scansthespec-
ifiedscreenandprovidessuggestionstoimprovetheaccessibility
ofyourappincludingcontentlabels,clickableitems,colorcontrast,
etc. The shortcoming of this tool is that the developers must run it
ineachscreenoftheapptogettheresults.Suchmanualexplorationoftheapplicationmightnotscaleforlargerappsorfrequenttesting,
anddevelopersmaymisssomefunctionalitiesorelementsduring
the manual testing.
Second,developerscanalsoautomateaccessibilitytasksbyre-
sortingtestingframeworkslikeAndroidLint,EspressoandRobolec-tric,etc.TheAndroidLint[
1]isastatictoolforcheckingallfilesof
an Android project, showing lint warnings for various accessibility
issuesincludingmissingcontentdescriptionsandprovidinglinks
totheplacesinthesourcecodecontainingtheseissues.Apartfrom
the static-analysis tools, there are also testing frameworks such as
Espresso [ 10] and Robolectric [ 18] which can also check accessi-
bilityissuesdynamicallyduringthetestingexecution.Andthere
are counterparts for IOS apps like Earl-Grey [ 9] and KIF [ 16]. Note
thatallofthesetoolsarebasedonofficialtestingframework.For
example, Espresso, Robolectric and Accessibility Scanner are based
on Androidâ€™s Accessibility Testing Framework [7].
Although all of these tools are beneficial for the accessibility
testing,therearestillthreeproblemswiththem.First,itrequires
developersâ€™ well awareness or knowledge of those tools, and un-
derstanding the necessity of accessibility testing. Second, all these
testing are reactive to existing accessibility issues which may have
alreadyharmedtheusersoftheappbeforeissuesfixed.Inadditiontothesereactivetesting,wealsoneedamoreproactivemechanism
ofaccessibilityassurancewhichcouldautomaticallypredictsthe
contentlabelingandreminds thedeveloperstofillthem intothe
app.Thegoalofourworkistodevelopaproactivecontentlabeling
model which can complement the reactive testing mechanism.
8 CONCLUSION AND FUTURE WORK
More than 77% apps have at least one image-based button without
natural-language label which can be read for users with vision im-
pairment. Considering that most app designers and developers are
of no vision issues, they may not understand how to write suitable
labels.Toovercomethisproblem,weproposeadeeplearningmodel
based on CNN and transformer encoder-decoder for learning to
predict the label of given image-based buttons.
We hope thatthis work caninvoking the communityattention
inappaccessibility.Inthefuture,wewillfirstimproveourmodel
for achieving better quality by taking the app metadata into the
consideration.Second,wewillalsotrytotestthequalityofexisting
labels by checking if the description is concise and informative.
REFERENCES
[1] 2011. Android Lint - Android Studio Project Site. http://tools.android.com/tips/
lint.
[2]2017. Screen Reader Survey. https://webaim.org/projects/screenreadersurvey7/.
332ICSE â€™20, May 23â€“29, 2020, Seoul, Republic of Korea Jieshan Chen, Chunyang Chen, Zhenchang Xing, et al.
[3]2018. android.widget | Android Developers. https://developer.android.com/
reference/android/widget/package-summary.
[4]2018.Blindnessandvisionimpairment.https://www.who.int/en/news-room/fact-
sheets/detail/blindness-and-visual-impairment.
[5]2019. Accessibility Scanner. https://play.google.com/store/apps/details?id=com.
google.android.apps.accessibility.auditor.
[6]2019. Android Accessibility Guideline. https://developer.android.com/guide/
topics/ui/accessibility/apps.
[7]2019. Androidâ€™s Accessibility Testing Framework. https://github.com/google/
Accessibility-Test-Framework-for-Android.
[8] 2019. Apple App Store. https://www.apple.com/au/ios/app-store/.
[9] 2019. Earl-Grey. https://github.com/google/EarlGrey.
[10]2019. Espresso | Android Developers. https://developer.android.com/training/
testing/espresso.
[11] 2019. Google Play Store. https://play.google.com.[12] 2019. Google TalkBack source code. https://github.com/google/talkback.
[13]
2019. ImageButton. https://developer.android.com/reference/android/widget/
ImageButton.
[14]2019. ImageView. https://developer.android.com/reference/android/widget/
ImageView.
[15]2019. iOSAccessibiliyuGuideline. https://developer.apple.com/accessibility/ios/.
[16] 2019. KIF. https://github.com/kif-framework/KIF.
[17] 2019. PyTorch. https://pytorch.org/s.[18] 2019. Robolectric. http://robolectric.org/.
[19]
2019. Talkback Guideline. https://support.google.com/accessibility/android/
answer/6283677?hl=en.
[20] 2019. VoiceOver. https://cloud.google.com/translate/docs/.
[21]2019. WorldWideWebConsortiumAccessibility. https://www.w3.org/standards/
webdesign/accessibility.
[22]Jyoti Aneja, Aditya Deshpande, and Alexander G Schwing. 2018. Convolutional
image captioning. In Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition. 5561â€“5570.
[23]Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. 2016. Layer normaliza-
tion.arXiv preprint arXiv:1607.06450 (2016).
[24]AbhijeetBanerjee,Hai-FengGuo,andAbhikRoychoudhury.2016. Debugging
energy-efficiency related field failures in mobile apps. In Proceedings of the Inter-
national Conference on Mobile Software Engineering and Systems. ACM, 127â€“138.
[25]Abhijeet Banerjee and Abhik Roychoudhury. 2016. Automated re-factoringof android apps to enhance energy-efficiency. In 2016 IEEE/ACM International
Conference on Mobile Software Engineering and Systems (MOBILESoft). IEEE, 139â€“
150.
[26]Satanjeev Banerjee and Alon Lavie. 2005. METEOR: An automatic metric for
MT evaluation withimproved correlation with humanjudgments. In Proceedings
of the acl workshop on intrinsic and extrinsic evaluation measures for machine
translation and/or summarization. 65â€“72.
[27]YoavBenjaminiandYosefHochberg.1995. Controllingthefalsediscoveryrate:apracticalandpowerfulapproachtomultipletesting. JournaloftheRoyalstatistical
society: series B (Methodological) 57, 1 (1995), 289â€“300.
[28]JohnBrookeetal .1996. SUS-Aquickanddirtyusabilityscale. Usabilityevaluation
in industry 189, 194 (1996), 4â€“7.
[29]Margaret Butler. 2010. Android: Changing the mobile landscape. IEEE Pervasive
Computing 10, 1 (2010), 4â€“7.
[30]Chunyang Chen, Xi Chen, Jiamou Sun, Zhenchang Xing, and Guoqiang Li. 2018.
Data-driven proactive policy assurance of post quality in community q&a sites.
Proceedings of the ACM on human-computer interaction 2, CSCW (2018), 1â€“22.
[31]Chunyang Chen, Sidong Feng, Zhenchang Xing, Linda Liu, Shengdong Zhao,
andJinshuiWang.2019. GalleryDC:DesignSearchandKnowledgeDiscovery
through Auto-created GUI Component Gallery. Proceedings of the ACM on
Human-Computer Interaction 3, CSCW (2019), 1â€“22.
[32]Chunyang Chen, Ting Su, Guozhu Meng, Zhenchang Xing, and Yang Liu. 2018.
Fromuidesignimageto guiskeleton:aneuralmachinetranslatortobootstrap
mobileguiimplementation.In Proceedingsofthe40thInternationalConferenceon
Software Engineering. ACM, 665â€“676.
[33]Chunyang Chen, Zhenchang Xing, and Yang Liu. 2017. By the community & for
thecommunity:adeeplearningapproachtoassistcollaborativeeditinginq&a
sites.ProceedingsoftheACMonHuman-ComputerInteraction 1,CSCW(2017),
1â€“21.
[34]ChunyangChen,ZhenchangXing,YangLiu,andKentLongXiongOng.2019.Mining likely analogical apis across third-party libraries via large-scale unsu-
pervisedapisemanticsembedding. IEEETransactionsonSoftwareEngineering
(2019).
[35]GuibinChen,ChunyangChen,ZhenchangXing,andBowenXu.2016. Learninga
dual-languagevectorspacefordomain-specificcross-lingualquestionretrieval.In
2016 31st IEEE/ACM International Conference on Automated Software Engineering
(ASE). IEEE, 744â€“755.
[36]SenChen,LinglingFan,ChunyangChen,TingSu,WenheLi,YangLiu,andLihua
Xu. 2019. Storydroid: Automated generation of storyboard for Android apps.
In2019IEEE/ACM41stInternationalConferenceonSoftwareEngineering(ICSE).IEEE, 596â€“607.
[37]Sen Chen,Lingling Fan,Chunyang Chen, MinhuiXue, Yang Liu,and Lihua Xu.
2019. GUI-Squatting Attack: Automated Generation of AndroidPhishing Apps.
IEEE Transactions on Dependable and Secure Computing (2019).
[38]SenChen,MinhuiXue,LinglingFan,ShuangHao,LihuaXu,HaojinZhu,and
Bo Li. 2018. Automated poisoning attacks and defenses in malware detection
systems: An adversarial machine learning approach. computers & security 73
(2018), 326â€“344.
[39]XinleiChen,HaoFang,Tsung-YiLin,RamakrishnaVedantam,SaurabhGupta,Pi-
otrDollÃ¡r,andCLawrenceZitnick.2015. Microsoftcococaptions:Datacollection
and evaluation server. arXiv preprint arXiv:1504.00325 (2015).
[40]Tobias Dehling, Fangjian Gao, Stephan Schneider, and Ali Sunyaev. 2015. Ex-
ploring the far side of mobile health: information security and privacy of mobile
health apps on iOS and Android. JMIR mHealth and uHealth 3,1 (2015),e8.
[41]Marcelo Medeiros Eler, JosÃ© Miguel Rojas, Yan Ge, and Gordon Fraser. 2018.
Automatedaccessibilitytestingofmobileapps.In 2018IEEE11thInternational
ConferenceonSoftware Testing,Verification andValidation(ICST) .IEEE, 116â€“126.
[42]Ruitao Feng, Sen Chen, Xiaofei Xie, Lei Ma, Guozhu Meng, Yang Liu, and Shang-
WeiLin.2019.MobiDroid:APerformance-SensitiveMalwareDetectionSystemon
MobilePlatform.In 201924thInternational Conference onEngineeringofComplex
Computer Systems (ICECCS).IEEE,61â€“70.
[43]Bin Fu, Jialiu Lin, Lei Li, Christos Faloutsos, Jason Hong, and Norman Sadeh.2013. Why people hate your app: Making sense of user feedback in a mobileapp store. In Proceedings of the 19th ACM SIGKDD international conference on
Knowledge discovery and data mining.ACM,1276â€“1284.
[44]Sa Gao, Chunyang Chen, Zhenchang Xing, Yukun Ma, Wen Song, and Shang-
Wei Lin. 2019. A neural model for method name generation from functional
description. In 2019 IEEE 26th International Conference on Software Analysis,
Evolution and Reengineering (SANER).IEEE,414â€“421.
[45]Isao Goto, Ka-Po Chow, Bin Lu, Eiichiro Sumita, and Benjamin K Tsou. 2013.
OverviewofthePatentMachineTranslationTaskattheNTCIR-10Workshop..
InNTCIR.
[46]KaimingHe,XiangyuZhang,ShaoqingRen,andJianSun.2016. Deepresidual
learning for image recognition. In Proceedings of the IEEE conference on computer
vision and pattern recognition. 770â€“778.
[47]GeoffreyHecht,OmarBenomar,RomainRouvoy,NaouelMoha,andLaurence
Duchien. 2015. Tracking the software quality of android applications along
their evolution (t). In 2015 30th IEEE/ACM International Conference on Automated
Software Engineering (ASE).IEEE,236â€“247.
[48]SeppHochreiterandJÃ¼rgenSchmidhuber.1997. Longshort-termmemory. Neural
computation 9, 8 (1997), 1735â€“1780.
[49]ShaunKKane,ChandrikaJayant,JacobOWobbrock,andRichardELadner.2009.
Freedom to roam: a study of mobile device adoption and accessibility for people
withvisualandmotordisabilities.In Proceedingsofthe11thinternationalACM
SIGACCESS conference on Computers and accessibility.ACM,115â€“122.
[50]Bridgett A King and Norman E Youngblood. 2016. E-government in Alabama:
Ananalysisofcountyvotingandelectionwebsitecontent,usability,accessibility,
and mobile readiness. Government Information Quarterly 33,4(2016),715â€“726.
[51]Diederik P Kingma and Jimmy Ba. 2014. Adam: A method for stochastic opti-
mization. arXiv preprint arXiv:1412.6980 (2014).
[52]AlexKrizhevsky,IlyaSutskever,andGeoffreyEHinton.2012. Imagenetclassifica-
tion with deep convolutional neural networks. In Advances in neural information
processing systems. 1097â€“1105.
[53]SolomonKullbackandRichardALeibler.1951. Oninformationandsufficiency.
The annals of mathematical statistics 22, 1 (1951), 79â€“86.
[54]Richard E Ladner. 2015. Design for user empowerment. interactions 22, 2 (2015),
24â€“29.
[55]Yann LeCun, Yoshua Bengio, et al .1995. Convolutional networks for images,
speech,an d time series. The handbook of brain theory and neural networks 3361,
10 (1995), 1995.
[56]Chin-Yew Lin and Eduard Hovy. 2003. Automatic evaluation of summaries
using n-gram co-occurrence statistics. In Proceedings of the 2003 Human Lan-
guage Technology Conference of the North American Chapter of the Association for
Computational Linguistics. 150â€“157.
[57]Tsung-YiLin,MichaelMaire,SergeBelongie,JamesHays,PietroPerona,Deva
Ramanan, Piotr DollÃ¡r, and C Lawrence Zitnick. 2014. Microsoft coco: Common
objects in context. In European conference on computer vision . Springer, 740â€“755.
[58]Mario Linares-Vasquez, Christopher Vendome, Qi Luo, and Denys Poshyvanyk.
2015. HowdevelopersdetectandfixperformancebottlenecksinAndroidapps.
In2015 IEEE International Conference on Software Maintenance and Evolution
(ICSME). IEEE, 352â€“361.
[59]Henry B Mann and Donald R Whitney. 1947. On a test of whether one of
two random variables is stochastically larger than the other. The annals of
mathematical statistics (1947), 50â€“60.
[60]HiginioMora,VirgilioGilart-Iglesias,RaquelPÃ©rez-delHoyo,andMarÃ­aAndÃºjar-
Montoya.2017. Acomprehensivesystemformonitoringurbanaccessibilityin
smart cities. Sensors17, 8 (2017), 1834.
333Unblind Your Apps: Predicting Natural-Language Labels for Mobile GUI Components by Deep Learning ICSE â€™20, May 23â€“29, 2020, Seoul, Republic of Korea
[61]Yusuke Oda, Hiroyuki Fudaba, Graham Neubig, Hideaki Hata, Sakriani Sakti,
Tomoki Toda, and SatoshiNakamura. 2015. Learning toGenerate Pseudo-Code
from SourceCodeUsing StatisticalMachine Translation(T). In AutomatedSoft-
wareEngineering(ASE),201530thIEEE/ACMInternationalConferenceon.IEEE,
574â€“584.
[62]KishorePapineni,SalimRoukos,ToddWard,andWei-JingZhu.2002. BLEU:a
method for automatic evaluation of machine translation. In Proceedings of the
40th annualmeeting onassociationfor computationallinguistics.Association for
Computational Linguistics, 311â€“318.
[63]KyudongPark,TaedongGoh,andHyo-JeongSo.2014. Towardaccessiblemobile
application design: developing mobile application accessibility guidelines for
people with visual impairment. In Proceedings of HCI Korea . Hanbit Media, Inc.,
31â€“38.
[64]JuanRamosetal .2003. Usingtf-idftodeterminewordrelevanceindocument
queries. In Proceedings of the firstinstructionalconference onmachine learning ,
Vol. 242. Piscataway, NJ, 133â€“142.
[65]Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. 2015. Faster r-cnn:
Towards real-time object detection with region proposal networks. In Advances
in neural information processing systems. 91â€“99.
[66]Anne Spencer Ross, Xiaoyi Zhang, James Fogarty, and Jacob O Wobbrock. 2018.
Examiningimage-basedbuttonlabelingforaccessibilityinAndroidappsthrough
large-scale analysis. In Proceedings of the 20th International ACM SIGACCESS
Conference on Computers and Accessibility. ACM, 119â€“130.
[67]Ch Spearman. 2010. The proof and measurement of association between two
things.International journal of epidemiology 39, 5 (2010), 1137â€“1150.
[68]Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
Aidan N Gomez, Åukasz Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In Advances in neural information processing systems. 5998â€“6008.
[69]Ramakrishna Vedantam, C Lawrence Zitnick, and Devi Parikh. 2015. Cider:
Consensus-based image description evaluation. In Proceedings of the IEEE confer-
ence on computer vision and pattern recognition. 4566â€“4575.
[70]Oriol Vinyals, Alexander Toshev, Samy Bengio, and Dumitru Erhan. 2015. Show
and tell: A neural image caption generator. In Proceedings of the IEEE conference
on computer vision and pattern recognition. 3156â€“3164.
[71]FahuiWang.2012. Measurement,optimization,andimpactofhealthcareaccessi-bility:amethodologicalreview. AnnalsoftheAssociationofAmericanGeographers102, 5 (2012), 1104â€“1112.
[72]XuWang,ChunyangChen,andZhenchangXing.2019. Domain-specificmachine
translationwithrecurrentneuralnetworkforsoftwarelocalization. Empirical
Software Engineering 24, 6 (2019), 3514â€“3545.
[73]Lili Wei, Yepang Liu, and Shing-Chi Cheung. 2016. Taming Android fragmen-tation: Characterizing and detecting compatibility issues for Android apps. In
2016 31st IEEE/ACM International Conference on Automated Software Engineering
(ASE). IEEE, 226â€“237.
[74]FrankWilcoxon.1992. Individualcomparisonsbyrankingmethods. In Break-
throughs in statistics. Springer, 196â€“202.
[75]Shunguo Yan and PG Ramachandran. 2019. The current status of accessibility in
mobile apps. ACM Transactions on Accessible Computing (TACCESS) 12, 1 (2019),
3.
[76]Zhenlong Yuan, Yongqiang Lu, and Yibo Xue. 2016. Droiddetector: android
malwarecharacterizationanddetectionusingdeeplearning. TsinghuaScience
and Technology 21, 1 (2016), 114â€“123.
[77]Xiaoyi Zhang, Anne Spencer Ross, Anat Caspi, James Fogarty, and Jacob O
Wobbrock. 2017. Interaction proxies for runtime repair and enhancement of
mobile application accessibility. In Proceedings of the 2017 CHI Conference on
Human Factors in Computing Systems.ACM,6024â€“6037.
[78]Xiaoyi Zhang, Anne Spencer Ross, and James Fogarty. 2018. Robust Annota-tion of Mobile Application Interfaces in Methods for Accessibility Repair andEnhancement. In The 31st Annual ACM Symposium on User Interface Software
and Technology. ACM, 609â€“621.
[79]Dehai Zhao, Zhenchang Xing, Chunyang Chen, Xin Xia, and Guoqiang Li. 2019.
ActionNet: vision-based workflow action recognition from programming screen-
casts. In2019 IEEE/ACM 41st International Conference on Software Engineering
(ICSE). IEEE, 350â€“361.
[80]DehaiZhao,ZhenchangXing,ChunyangChen,XiweiXu,LimingZhu,Guoqiang
Li, and Jinshui Wang. 2020. Seenomaly: Vision-Based Linting of GUI Animation
Effects Against Design-Donâ€™t Guidelines. In 42nd International Conference on
Software Engineering (ICSE â€™20). ACM, New York, NY, 12 pages. https://doi.org/
10.1145/3377811.3380411
[81]Hui Zhao, Min Chen, Meikang Qiu, Keke Gai, and Meiqin Liu. 2016. A novel
pre-cache schema for high performance Android system. Future Generation
Computer Systems 56 (2016), 766â€“772.
334