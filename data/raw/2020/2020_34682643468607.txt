StateFormer: Fine-Grained TypeRecoveryfrom Binaries using
GenerativeStateModeling
KexinPei
kpei@cs.columbia .edu
Columbia University
New York, USAJonasGuan
jonas@cs .toronto.edu
Universityof Toronto
Toronto, CanadaMatthewBroughton
mb4207@columbia .edu
Columbia University
New York, USAZhongtianChen
zc2399@columbia .edu
Columbia University
New York, USA
SongchenYao
sy2743@columbia .edu
Columbia University
New York, USADavidWilliams-King
dwk@cs.columbia .edu
Columbia University
New York, USAVikas Ummadisetty
ummadisettyvikas@gmail .com
Dublin HighSchool
Dublin, USAJunfeng Yang
junfeng@cs .columbia .edu
Columbia University
New York, USA
Baishakhi Ray
rayb@cs.columbia .edu
Columbia University
New York, USASumanJana
suman@cs .columbia .edu
Columbia University
New York, USA
ABSTRACT
Binarytypeinferenceisacriticalreverseengineeringtasksupport-
ing many security applications, including vulnerability analysis,
forensics, and decompilation. Itis adifficult taskbecause source-
leveltypeinformationisoftenstrippedduringcompilation,leaving
only binaries with untyped memory and register accesses. Existing
approaches rely on hand-coded type inference rules defined by
domainexperts,whicharebrittleandrequirenontrivialeffortto
maintainandupdate.Eventhoughmachinelearningapproaches
haveshownpromiseatautomaticallylearningtheinferencerules,
theiraccuracyisstilllow,especiallyfor optimizedbinaries.
Wepresent StateFormer ,anewneuralarchitecturethatisadept
at accurate and robust type inference. StateFormer follows a two-
step transfer learning paradigm. In the pretraining step, the model
is trained with Generative State Modeling ( GSM), a novel task that
wedesigntoteachthemodeltostaticallyapproximateexecution
effects of assembly instructions in both forward and backward
directions. In the finetuning step, the pretrained model learns to
use its knowledge ofoperationalsemantics to infertypes.
We evaluate StateFormer â€™s performance on a corpus of 33 pop-
ular open-source software projects containing over 1.67 billion
variables of different types. The programs are compiled with GCC
and LLVM over 4 optimization levels O0-O3, and 3 obfuscation
passes based on LLVM. Our model significantly outperforms state-
of-the-art ML-based tools by 14.6% in recovering types for both
functionargumentsandvariables.Ourablationstudiesshowthat
GSMimproves type inference accuracyby33%.
Permissionto make digitalor hard copies of allorpart ofthis work for personalor
classroom use is granted without fee provided that copies are not made or distributed
forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation
onthefirstpage.Copyrights forcomponentsofthisworkownedbyothersthanthe
author(s)mustbehonored.Abstractingwithcreditispermitted.Tocopyotherwise,or
republish,topostonserversortoredistributetolists,requirespriorspecificpermission
and/or a fee. Request permissions from permissions@acm.org.
ESEC/FSE â€™21, August 23Å›28,2021, Athens,Greece
Â©2021 Copyright heldby the owner/author(s). Publicationrightslicensed to ACM.
ACM ISBN 978-1-4503-8562-6/21/08...$15.00
https://doi .org/10.1145/3468264 .3468607CCS CONCEPTS
Â·Security and privacy â†’Software reverse engineering ;Â·
Computingmethodologies â†’Machine learning .
KEYWORDS
TypeInference,ReverseEngineering,Transfer Learning,Machine
Learningfor Program Analysis
ACMReference Format:
Kexin Pei, Jonas Guan, Matthew Broughton, Zhongtian Chen, Songchen
Yao, David Williams-King, Vikas Ummadisetty, Junfeng Yang, Baishakhi
Ray, and Suman Jana. 2021. StateFormer : Fine-Grained Type Recovery
fromBinariesusingGenerativeStateModeling.In Proceedingsofthe29th
ACM Joint European Software Engineering Conference and Symposium on
theFoundations ofSoftware Engineering(ESEC/FSEâ€™21), August 23Å›28, 2021,
Athens,Greece. ACM,NewYork,NY,USA, 13pages.https://doi .org/10.1145/
3468264.3468607
1 INTRODUCTION
Recovering source-level data types from binaries is very useful for
many security-criticalsoftware engineering tasks, such as vulner-
ability analysis [ 18,37,45], binary hardening [ 30,53,57,74,96,
99,101], memory introspection [ 43,89], and decompilation [ 4,26].
Type inference in binaries involves reconstructing source-level
constructs, such as local function variables and data types, from
untypedbyte-addressedmemoryandregisters.Thisprocessischal-
lengingbecausethereconstructionisbasedonincompleteinforma-
tionÅ›mostsource-levelinformationisstrippedduringcompilation
for optimization andto deterreverseengineering.
Traditionalapproachestotypeinferencerelyextensivelyonhand-
coded rules defined by domain experts. These rules facilitate (1)
recognizing types directly from specified patterns ( e.g.,consecutive
printablecharactersfordetectingstrings);and(2) propagating types
from known type sinks ( e.g.,known string manipulation functions)
to registers and memory regions storing the source-level variables.
Unfortunately,theserulesarebrittle[ 16,66]andrequirecontinuous
efforttoadapttonewinstructionsequencesintroducedbycompiler
andarchitecture evolution [ 10].
690
ESEC/FSE â€™21, August 23Å›28, 2021,Athens,Greece Pei, Guan,Broughton,Chen, Yao, Williams-King,Ummadisetty, Yang,Ray, Jana
As a result, recent years have witnessed a growing interest in
data-drivenapproachesleveragingMachineLearning(ML)forbi-
narytypeinference[ 40,59].Theseapproachesmitigatethereliance
on hand-coded heuristics by learning from a rich training set of
diversebinaries.Moreover,theirlearnedrepresentationshavebeen
showntogeneralizeacrossvariouscompilers,operatingsystems,
andarchitecturesandarehighlyefficienttocompute( i.e.,theun-
derlying learning algorithms are amenable to GPU parallelization).
Whilepromising,existingML-basedapproachesstillcannotre-
cover data types with high accuracy or robustness, especially in
thepresenceofcompileroptimizations[ 20,40].Thetypesarees-
sentially abstractionsdescribing how a data object isexpected to be
manipulatedandusedduringexecution .Therefore,theinherentchal-
lengefacedbyallML-basedapproachesistounderstandtheeffects
oftheruntimeexecutionofinstructionsinthetargetbinary[ 55,83],
i.e.,theoperational semantics of codeblocks[ 64]. For example, on
x64,theruntimeeffectofiterativeincrementsofthe rcxregisterby
1togetherwiththeinstruction mov rax,[rdx+rcx*4] isindicative
oftraversingan intarray.
Unfortunately, existing ML-based approaches are agnostic to
theexecutioneffectsofcodeastheylearnthedirectmappingbe-
tweenstaticcodetokensandcorrespondingtypesinanend-to-end
fashion. A model trained this way often learns spurious correla-
tions [7], taking shortcuts to leverage simple yet brittle patterns
for inferring types. For example, Chua et al.[20] showed that their
model,EKLAVYA,mispredictsthetypeoftheargumenttothefunc-
tionck_fopen from Diffutils to be integer instead of pointer. A
completely unrelatedinstructionwithin ck_fopen (namelycallq
0x3fc)contributesthemosttothemisprediction.Withoutunder-
standinghowanintegerisaccessedandmanipulatedduringexe-
cutionandtheeffectsof callq,EKLAVYAestablishesaspurious
correlation that the internal call instruction implies an intargu-
menttock_fopen .
Inthispaper,wepresent StateFormer ,anewneuralarchitecture
thatexplicitlylearnstheoperationalsemanticsofassemblyfortype
inference. Specifically, we design a novel pretraining task to teach
theStateFormer model the operational semantics of both data
and control flow behavior of diverse code blocks, and then finetune
the pretrainedmodelfor type inference.
Learning operationalsemantics. A human reverse engineer of-
ten makes sense of a target binary by following its assembly in-
structions through mental simulation of their execution. While the
reverseengineermightnotaccuratelyresolveallinvokedbranches
byfollowingcontrolfloworcomputetheprecisevaluesofallstates
by following data flow duringsimulation, she can still get a rough
idea of what the code does by approximately following the oper-
ational semantics of code blocks. Our key insight is to teach the
StateFormer model, via a novel pretraining task, the approximate
operationalsemantics of assembly by forcing the modelto predict
howdifferentsequencesofinstructions transformtheunderlying
program states . Specifically, the pretraining task asksthe modelto
predictthechangedvaluesofregistersandmemoryafterexecut-
ingeach instruction,whichcaptures theoperationalsemantics of
assembly code [ 29,64,70]. Thisgives themodel an understanding
of the execution effects of code, which helps the model to infer
thetypesoflow-levelregistersandmemoryregionsbasedontheinstructions used to manipulate them without executing any parts
ofthe code duringinference.
Generative State Modeling. Wedesignanovelpretraining task,
GenerativeStateModeling( GSM),wherewetrainaneuralnetwork
to reconstruct the complete set of itsexecution states whiletaking
the assembly code and a very small subset of its execution states
(e.g.,register values at specific program points) as input. For ex-
ample, given the instruction sequence: inc ecx;add ecx,3;xor
ecx,ecx;mov ebx,ecx; and its corresponding execution states
ecx=0;ecx=1;ecx=4;ecx=0;(ebx=0,ecx=0) , we feed the model
with all the instructions and only the executionstate after the sec-
ondinstruction, i.e.,ecx=4.Ourtrainingprocessforcesthemodel
to compute all the preceding and succeeding states: ecx=0;ecx=1;
andecx=0;(ebx=0,ecx=0) . Therefore,toachievelow losson the
GSMtask,themodelneedstounderstandtheoperationalsemantics
ofinc,add,xor,andmov.
GSMdynamicallyselectsrandomsubsetsofstatesasinputsacross
differenttrainingsamplesand iterations.Moreover, GSMisfully
self-supervised [ 24], implying that we can collect data from an
unrestricted number of binaries found in the wild. As a result,
GSMcreates diverse prediction tasks that compel the model to
approximately reason about the effects of both dataandcontrol
instructions,inboth forwardandbackward directionsÅ›acritical
capability for type recognition and propagation [ 55,83]. During
pretraining with GSM,StateFormer encodes such a reasoning
capabilityaspartofitsnetworkparameters,knownasembeddings.
Such embeddings can then be finetuned for type inference as a
finetuning taskwithafewbinarieswithlabeledtypes.
Consideragaintheexampleofinferringthetraversalofan int
basedoniterativeincrementsofthe rcxregisterby 1togetherwith
the instruction mov rax,[rdx+rcx*4] . The output of pretrained
StateFormer willbeasequenceofembeddingsencodingtheeffects
ofinc,movon other registers and memory locations. Therefore,
insteadoftrainingonrawcodesequencesfromscratch,thefine-
tuningprocesscaneasilyexploitthelearnedexecutioneffectsof
code compressed in these embeddings to predict that rdxcontains
the baseaddressofan intarray.
StateFormer neuralarchitecture. To efficiently pretrain with
GSM, we develop a novel neuralarchitecture specifically designed
forlearningoperationalsemanticsofassemblyinstructions.First,as
themodeltakesasinputboththeprogramcodeandprogramstates,
we developa multi-modalencodingmodule thatcanbetrained on
heterogeneous inputsindifferentformats.
Second,weconstructtwoexplicitobjectivefunctionstojointly
optimize the model to understand the operational semantics of
bothdataflow andcontrolflow .Specifically,tohelpthemodellearn
about control-flow, which requires learning operational semantics
ofcomparisoninstructions( e.g.,cmp),weannotatethenon-executed
pathswithdummyprogramstatestoincorporatepredictingnon-
executed paths as a part of the StateFormer â€™s pretraining task. To
helpthemodeltobetterunderstandtheoperationalsemanticsof
dataflow,whichofteninvolvesassignment( e.g.,mov)andarithmetic
instructions( e.g.,add)onnumericalvalues,weexplicitlymodelthe
numerical values in both decimal and hexadecimal formats with
a trainable numerical representation module based on the neural
arithmetic unit(NAU) [ 58].
691StateFormer : Fine-GrainedType Recovery from BinariesusingGenerative StateModeling ESEC/FSE â€™21, August 23Å›28, 2021,Athens,Greece
Finally, as the composite execution effects of a piece of code can
resultfromtheinteractionsbetweenfarawayinstructions,welever-
ageself-attentionlayersfromTransformer[ 93],whichisamenable
tolearninglong-rangedependencieswithoutmanually construct-
ingthedependencies( e.g.,graphneuralnet[ 63,97]).Weshowin
Section5.5thatsuchadesignindeedachieveshightestingaccuracy
inGSMfor unseen program state traces.
Result summary. We evaluate StateFormer on a corpus of 33
popularopen-sourcesoftwareprojectswith1.67billionsourcevari-
ablesofdifferenttypes.Theprogramsarecompiledfor4instruction
set architectures (x86, x64, ARM, and MIPS), by 2 compilers (GCC
andLLVM),andwith4optimizationlevels( O0-O3)and3obfusca-
tionpassesbasedonLLVM[ 104].Bytrainingwith GSM,ourmodel
outperformsthe state-of-the-artML-based toolsbyup to14.6% in
recoveringtypesforbothfunctionargumentsandvariables.Our
extensive ablation studies show that StateFormer trained with
GSMsubstantially boosts thetype inferenceaccuracy by 33%. We
make the following contributions.
â€¢Weproposeanewpretrainingtask,GenerativeStateModeling
(GSM), toexplicitly learn the operationalsemanticsof assembly
code for accurateandrobust type inference.
â€¢We develop a novel neural architecture, StateFormer , with spe-
ciallydesignedsub-modulestolearntheoperationalsemantics
ofboth data flowandcontrolflowinstructions.
â€¢Weevaluate StateFormer onanextensivecollectionof33open-
sourcesoftwareprojectsacrossdifferentarchitectures,compilers,
optimizations,andobfuscations.Aftertrainingwith GSM,State-
Formeroutperforms the state-of-the-art learning-based tools
by14.6%.Ourablationstudiesunveilthattrainingwith State-
Formerbooststhetypeinferenceaccuracyby33%.Werelease
the code and datasets of StateFormer athttps://github .com/
CUMLSec/stateformer .
2 OVERVIEW
Thehigh-levelworkflowof StateFormer followsthegeneraltrans-
ferlearningparadigm.AsshowninFigure 1,wefirstpretrain State-
FormerwithGSMbytrainingittoreconstructthemaskedstates
(grayed-out) in the trace of program states of various assembly
instructions (Section 2.3). We train StateFormer to reconstruct
both the data and control states (Section 2.4). After pretraining
StateFormer withGSM, we transfer its learned knowledge by
finetuning onthe type inference task(definedinSection 3.4).
2.1 ProblemDefinition
Weconsidertheproblemofmappinguntypedlow-levelregistersor
memoryregions(specifiedbymemoryoffsets)tothecorrespond-
ingsource-leveltypes. The source-leveltypes are associatedwith
functionarguments,local,static,andglobalvariables.Thegranu-
larity of recovered source-level types varies widely across existing
works[16],rangingfromprimitive( e.g.,int,float)andaggregate
(e.g.,struct,array)typestoclassesinobject-orientedprograms
andrecursive types such as trees andlists.
WefocusontheCprimitive,aggregate,andpointertypes.Our
supportedtypesaremore fine-grained thanpriorworks[ 20,40,59],
whichonlysupportastrictsubsetofours(seeSection 3.4forthe
complete list of types). Predicting fine-grained types, while helpfulto the reverse engineer to better understand the target binary, is
achallenginglearningtaskthatmustdistinguishbetweensubtly
differentaccess patterns of differenttypes [ 83].
We formulate type inference asa classificationtask. Specifically,
givena sequenceofassembly instructions, StateFormer predicts
thetypelabelsforeachoperandintheinstructions.Notethat State-
Formerperforms the type prediction in one shot(see Section 3for
design specifics), as opposed to the traditional type propagation
approaches that infer the types one-by-one in a sequence of in-
structions.AsweshowinSection 5.3,thisdesignbringssignificant
performance gainsduringinference.
2.2 Understanding Operational Semantics
Helps TypeInference
While reverse engineering types from binaries, human analysts
needtounderstandwhatatargetfunctioncomputeswithoutexe-
cutingthebinary.Oftentheanalystfollowstheassemblyinstruc-
tions by simulating the execution in their mind. Without knowing
theexactprogramstates duringthefunctioncall,theanalystcan-
notaccuratelyresolvethetakenbranchesorcomputetheprecise
valuesduringthesimulation.Still,theycangetaroughideaofwhat
the code does. This loose approximation of the operational seman-
ticsofthecodeallowstheunderstandingofitsruntimebehavior,
providingstronghints aboutthe underlying data types [ 83].
For instance, given a pointer a, observing a dereference like
âˆ—(a+4)in the execution behavior might imply a 4-byte read of
the object at a, indicating an intor a pointer type on 32-bit sys-
tems.Similarly,contiguousdereferencingofsequentialaddresses
likeâˆ—(a),âˆ—(a+1), ... suggests that ais likely an array of chars.
Examining precise runtime behaviors of a binary over many in-
putswithhigh-coveragedynamicanalysisisprohibitivelyexpen-
sive[51,83,87],Therefore,inthispaper,weuseMLmodelstolearn
approximateoperationalsemanticsofbinariesinadata-drivenman-
ner anduse this knowledge to statically infertypes.
2.3 Learning Operational Semantics withGSM
Ourkeymotivationfordeveloping GSMistoteachanMLmodel
to approximate operational semantics of code, i.e.,its execution
effects,which wethenexploit fortypeinference. Teachingan ML
model the code execution effects is challenging due to many possi-
blecombinationsofinstructionsthatintroducecomplexdataand
controlflowdependencies.Therefore,itisnotpracticaltomanually
engineer input features or target labels to represent the execution
effectsand train the modelto understandthem.To thisend, GSM
exploresaself-supervisedapproachthatexploitsalargenumber
of traces that can be cheaply generated from many code blocks
using under-constrained dynamicexecution, e.g.,micro-execution
(detailed in Section 3.1), toautomate learning diverse instructionsâ€™
executioneffectwithacarefully-designedtraining task.
Predictingmaskedstates. Thetrainingtaskperformedin GSM
requiresaneuralnetworktoreconstructthewholemicro-execution
traces(i.e.,allrecordedprogramstates)inthetrainingdatabasedon
thecorrespondingcodeblocks.Tolearnonahugenumberoftraces,
we exploit stochasticity to efficiently train a network for GSM.
Specifically,for eachtrainingsampleineachepoch, werandomly
masksomestatesinthe traces.Suchrandomnessensuresthatthe
692ESEC/FSE â€™21, August 23Å›28, 2021,Athens,Greece Pei, Guan,Broughton,Chen, Yao, Williams-King,Ummadisetty, Yang,Ray, Jana
...
sub	ecx,1
add	ecx,3
......
ecx=2
ecx=1
ecx=4
...code	1partial
states	1
.........
mov	eax,2
add	eax,1
......
eax=0
eax=2
eax=3
...code	npartial
states	nPretrained	Model.........
ecx=2
ecx=??
ecx=??
...Data
...
exec=??
exec=??
...Control
...
eax=??
eax=??
eax=3
...Data
...
exec=??
exec=??
...Control(i)	GSM	Pretraining	Task
Pretrained	ModelTransfer
...
mov	rax,1
push	rax
...code	1
.........
cmp	eax,3
ja	0x3c
...code	n...... Type	Prediction	HeadPredict	types
Type	Prediction	HeadPredict	types(ii)	Type	Inference	Finetuning	Task
Pretrained	Model
...
mov	rax,1
push	rax
...codeType	Prediction	HeadPredict	types(iii)	Type	Prediction
Predict	complete	states	1 Predict	complete	states	n
Figure 1: StateFormer workflow. We first pretrain StateFormer withGSM. We then stack type prediction heads on top of
thepretrainedmodelandfinetuneboththepretrainedmodelandthestackedheadsfortypeinference.Finally,thefinetuned
modelwill onlytaketheprogram codeas input (we donotexecute thecodeduringtype inference) and predictthe type.
modelcannotconsistentlyachievelowlossbytakingshortcutsthat
only work well for afewstates, traces,orcode blocks.
Whiledecidingwhichstatestomask, GSMdoesnotfollowthe
sequential execution order of states as recorded in the traces. This
designchoiceensuresthatthemodellearnstoreasonaboutboth
forward and backward execution effects of a diverse set of code
blocks.Understandingtheseforwardandbackwarddependencies
isknownto be crucial for accuratetype inference [ 55,83].
Differencewith masked language models. Whileourstochas-
tic masking setup is inspired by the Masked Language Modeling
(MLM)usedinlearningnaturallanguagesemantics[ 24],thekeydif-
ference is that natural languages are not stateful, i.e.,they have no
notion similar to programexecution.Therefore, the model trained
byMLM only uses words inthe neighboring context to predict the
maskedwords,exploitingthe localwordphrasepatterns .Whilein
GSM, the unmasked states alone provide little observable patterns
due to high masking rate Å› the model has to also look at the cor-
respondinginstructions ,understandtheirexecutioneffectsonthe
unmaskedstates, inorder to correctlypredict the maskedstates.
2.4StateFormer Architecture
Learning instruction-state dependencies. Achieving low loss
onGSM, by design, requires a neural network to understand the
long-range dependencies between instructions and unmasked pro-
gramstates.However,standardfully-connectedorrecurrentnet-
works are inefficient at learning long-range dependencies between
differentparts ofthe network inputs[ 61,93,98].
To avoid these issues, we develop a hierarchical input embed-
ding module tolearn theinteractions betweenprogramstates and
instructions. Specifically, we design two input sub-networks for
learningtwoembeddingsofthebinarycodeandtracesÅ›onefor
the registers and instruction opcodes and another for the concrete
data values.We combine theserepresentations by aggregating the
embeddingswithavectoradditionoperationandfeedingtheminto
self-attention layers that facilitate capturing long-range dependen-
cies[93](Section 3.2).Finally,weusetwooutputsub-networkstodecodetheoutputofself-attentionlayersfortwodifferentobjec-
tives: (1) regressionfor predicting the program datastate and (2)
classification for predicting the program control state (Section 3.3).
Learning representations for numerical values. Typical em-
beddings for numerical tokens ( i.e.,register values) Å› just like how
anydiscretetokenisembeddedÅ›areknowntofailtoextrapolateto
unseen values even on the outputs of simple arithmetic operations
like addition [ 88]. As understanding data and control flow often
requiresreasoningtheexecutioneffectofarithmeticinstructions,
we use Neural Arithmetic Units (NAUs) [ 58] as part of the subnet-
work for data value embeddings. Note that our NAU layers, unlike
the original NAU model that directly takes numerical values as
input, learn to represent the numerical values (both decimal and
hexadecimal formats) as embeddings. We have done a thorough
study and refer interested readers to our supplementary material.
3 METHODOLOGY
We now provide the details of our methodology, including how we
collectruntimestatesofbinaryprograms,thearchitectureof State-
Former,andhowwedistillthelearnedknowledgein StateFormer
from training GSMfor type inference.
3.1 Collecting ProgramStates
TotrainStateFormer withGSM,weneedtoobtainruntimeexecu-
tiontracesofbinaryprograms.Ideally,wewanttocollectdiverse
traceswithdifferentinstructionsandcontrolflowtolearnmiscella-
neousoperationalsemanticsfortypeinferenceinvariousscenarios.
However, the typical dynamic analysis approach is often limited
by path coverage, resulting in potentially restricted sets of covered
instructions.Therefore, weadoptmicro-execution[ 33]to support
tracing arbitrary parts of a binary program without having to find
concrete program inputsthat maximizecoverage.
Without executing the program from its entry point, our exe-
cutionengineneedstoinitializeintermediateprogramstates( i.e.,
registers and memory content) with randomized values, which can
be under-constrained ( i.e.,infeasible when executing the program
693StateFormer : Fine-GrainedType Recovery from BinariesusingGenerative StateModeling ESEC/FSE â€™21, August 23Å›28, 2021,Athens,Greece
add	[ebp+0x8],0x3
cmp	[ebp+0x8],0x2
jle	0x6
sub	[ebp+0x8],0x1
mov	eax,0Code
add	[0x4+0x8],0x3
cmp	[0x4+0x8],0x2
jle	0x6
sub	[0x4+0x8],0x1
mov	0x0,0
State	Trace
mov	ebp,esp mov	0x1c,0x4
Figure 2: Sample ğœ‡State trace consisting of both the data
statesassociatedwitheachinstructionandthecontrolstates
(indicated by âœ“andâœ—) generated by themicro-execution.
normally).Inaddition,wefocusonprogramstatesthatare explic-
itly manipulated ininstructions( e.g.,we onlylogthevalueof eax
insub eax,1 ,insteadofloggingallregisters,flags,andvaluesin
memory).Therefore,wecallourcollectedprogramstatesas partial
states(ğœ‡State), implying that they might differ from the genuine
program states from actual program executions.
ğœ‡Statecollection. Wecollect ğœ‡Statetracesandmaskarandomsub-
setofthemtotrainthemodeltoreconstructthecomplete ğœ‡State
trace.ğœ‡Stateconsistsoftwosourcesofinformation.(1)Theconcrete
values of all registers, memory addresses, and hardcoded offsets
that appear in the instruction, dubbed ğœ‡DataState. (2) The boolean
annotationindicatingwhethereachinstructioninthecodeisexe-
cuted in a given ğœ‡State, dubbed ğœ‡ControlState. The former appears
in bothStateFormer â€™s input and output ( i.e.,subset of ğœ‡DataState
as input, complete set of ğœ‡DataState as output). The latter appears
onlyinStateFormer â€™soutput.Section 3.2elaboratesonhowthese
twoparts of ğœ‡Stateare usedto train StateFormer .
Figure2showsanexampleof ğœ‡Statetracegeneratedbymicro-
executing a simple code block, e.g.,the concrete values of registers
areğœ‡DataStateandthe âœ“andâœ—besideseachinstructionindicates
ğœ‡ControlState. We assign dummy values ( $$) to all opcode, as they
do not hold anyvalue during micro-execution. This helps to align
ğœ‡Stateandtheassemblycodesequence,whichmakesitconvenient
forStateFormer toaggregatethemasnetworkinputs(Section 3.2).
To construct ğœ‡ControlState, we annotate each instruction with a
binary indicator to denotewhether itisexecutedornot.
3.2StateFormer Input andOutput
Weconstruct5sequencesfor StateFormer input,namely(1)static
assemblycodesequence,(2) ğœ‡DataStatesequence,(3)instruction
positionsequence,(4)opcode/operandpositionsequence,and(5)ar-
chitecture sequence. Each token of the 5 sequences are aligned and
embedded into an embedding (a low-dimensional vector) with the
samedimensions,suchthattheycanbeeasilyaggregatedasasingle
sequenceof ğ‘›embeddings: ğ‘¥={ğ‘¥1,...,ğ‘¥ğ‘›}.Figure3illustratesan
example inputof StateFormer when training on GSM.
Encoding assembly code. The assembly sequence with length
ğ‘›:ğ‘={add,ebp, ...}ğ‘›is constructed by tokenizing the assembly
instructions.Besidestreatingbothopcodesandoperandsastokens,
we keep punctuations as they provide crucial contextual hints,e.g.,thecommadelimitsthesourceanddestination,andbrackets
indicateadereference ofamemory address.
Assemblycodecan have concretenumerical values hardcodedin
instructions,whichleadstoaprohibitivelylargevocabularysize
(e.g.,232possiblevaluesinx86),makingitchallengingtoembedall
tokens in ğ‘. Therefore, we place the concrete value into ğœ‡DataState
and replace all numerical values (in both hexadecimal and decimal
forms)withaspecialtoken hex.Thisreducesthevocabularysize
ofğ‘acrossallinstructionsetarchitecturestoonly648.Wedescribe
howwe encode the numerical valuesinthe following.
Encoding ğœ‡DataState. Wenormalize ğœ‡DataStatesequence ğ‘£asa
two-dimensionalarray ğ‘£=Vğ‘›Ã—8,whereV={0x00,...,0xff}âˆª{$$}
(theunionof256bytesandadummytoken $$).Eachğœ‡DataState
ğ‘£ğ‘–can thus be viewed as a sequence of 8-byte tokens V8, where
wetransformallthenumericalvaluesintoan8-bytehexadecimal
representation. For example, Figure 3shows that a ğœ‡DataState 0x6
is padded to ( 00,00,00,00,00,00,00,06). As each ğ‘£ğ‘–is aligned with
each token ğ‘ğ‘–in code sequence, we put 8 $$s for those ğ‘ğ‘–that do
not have dynamic values( e.g.,opcode).
Such a setting reduces the vocabulary size used to encode all
possible numerical values from 264(assume 64-bit architectures)
toonly257.Moreover,representinganumericalvaluewithfixed
dimensionsmakesiteasytostackasinglelearnablemodule(see
Section3.3)tocomputeinter-dependenciesbetweendigits,learning
useful hierarchical knowledge ( i.e.,an address 0x104cmight be
decomposedas asection baseat 0x1000withthe offset 0x4c).
Encoding spatial information and syntactic hint. As we flat-
tenandconcatenatealltheassemblyinstructionsasaplaincode
tokensequence,theinstructionboundariesandtherelativelocation
of tokens within each instruction become ambiguous. To this end,
weintroducetwopositionalencodings[ 93],namelytheinstruction
positional encoding and opcode/operand positional encoding. The
resultinginstructionpositionsequence ğ‘=Zğ‘›+andopcode/operand
position sequence ğ‘œ=Zğ‘›+annotate each token in ğ‘with their in-
struction position and the opcode/operand position within each
position, respectively.Figure 3showsthe example of ğ‘andğ‘œ.
When training with GSM, we mix the training samples from
differentinstructionsetarchitectures,whichintroducedisparate
syntax in their assembly code. We thus append the architecture
sequence ğ‘to indicate the architecture, which assists the model to
transferthelearnedinstructionsemanticsusefulononearchitec-
ture to another ( e.g.,push eax in x86 hasthe similar semantics to
addi $sp,$sp,-4;sw $t0,($sp) inMIPS)[ 49].
StateFormer output.StateFormer havedifferentoutputsde-
pendingonthetrainingtasks.Whenitisinthepretrainingstage
withGSM, its output consists of complete ğœ‡State trace including
bothğœ‡DataState trace and ğœ‡ControlState trace. We describe how
these outputs participate in the computation of loss functions in
Section3.3. When we finetune StateFormer for type inference, its
outputisthe prediction of type labels definedinSection 3.4.
3.3 Pretrainingwith GSM
Numerical representation module. We treat each value ğ‘£ğ‘–in
ğœ‡DataState trace ğ‘£as an 8-byte sequence. To learn the inter-
dependencies between high and low bytes in ğ‘£ğ‘–, we develop a
694ESEC/FSE â€™21, August 23Å›28, 2021,Athens,Greece Pei, Guan,Broughton,Chen, Yao, Williams-King,Ummadisetty, Yang,Ray, Jana
push rbp jmp hexv
1 1 2 2 Inst POS
1 2 1 2 OP POSv1 v2 v3 v4
x64 x64 x64 x64 ArchCode
 
 DataStatex1 x2 x3 x4
x1 x2 x3 x4E1 E2 E3 E4
Self-attention Layers
Input
EmbeddingOutput
Embedding
E1 E2 E3 E4Aggregate    DataState as Embedding
v4
0000000000000006Neural Arithmetic UnitInput
Embedding
DataState 0x6Input encoding sub-networks
1. Regression on    DataState 
00000000013bc4a700000000033bc4a7
PredictedGround-truth
Mean Squared Error
Feedforward Network2. Classification on    ControlState
PredictedGround-truth
Cross-entropy
Feedforward Network10Output sub-networks
Vector Addition
Figure 3: StateFormer â€™s architecture and input-output when training with GSM(the color consistent with that of Figure 1).
StateFormer takes as input the code sequence and a subset of ğœ‡DataState ( e.g.,ğ‘£4in the figure). The other input sequences
are described in Section 3.2. The loss functions measure (1) Mean Squared Error (MSE) between the reconstructed ğœ‡DataState
andthegroundtruth, and(2) Binary Cross-Entropy (BCE) between thepredicted ğœ‡ControlStateand the groundtruth.
learnableneuralmodulewithNeuralArithmeticUnit(NAU)[ 58],
which is shown beneficial to capture the semantics of numerical
values involved in arithmetic operations (Section 2.4). Formally, let
ğ‘£ğ‘–=(ğ‘£ğ‘–1,...ğ‘£ğ‘–8)denote the 8-byte sequence of ğ‘£ğ‘–, we denote the ag-
gregatedembedding ğ¸ğ‘£ğ‘–astherepresentationofeach ğœ‡DataState:
ğ¸ğ‘£ğ‘–=ğ‘ğ´ğ‘ˆ(ğ¸ğ‘šğ‘(ğ‘£ğ‘–1),...,ğ¸ğ‘šğ‘(ğ‘£ğ‘–8)),e.g.,ğ¸ğ‘šğ‘(ğ‘£ğ‘–1)denote apply-
ing the embedding to the first byte token of ğ‘£ğ‘–. Figure3briefly
illustrateshowa ğœ‡DataState 0x6getsencodedbyNAU.Notethat
ğ‘£ğ‘–inFigure 3indicates the embedding ğ¸ğ‘£ğ‘–.
Sampling subset of ğœ‡DataState. Wesamplearandomsubsetof
ğœ‡DataState and replace them with <MASK>(e.g.,the grayed-out
tokens as shown in Figure 3) tokens in the model input so that the
model is trained to reconstruct the removed ğœ‡DataState. We define
ğ‘ƒğ‘šğ‘ğ‘ ğ‘˜asthepercentageofthemasked ğœ‡DataStateandstudythe
effectofdifferent ğ‘ƒğ‘šğ‘ğ‘ ğ‘˜ontype inference inSection 5.4.
Multimodal encoding module. We only apply NAU to each
ğœ‡DataState sequence ğ‘£. For other sequences, we apply regular
embeddings. We end up with 5 embeddings for each token in
each sequence: ğ¸ğ‘ğ‘–,ğ¸ğ‘£ğ‘–,ğ¸ğ‘ğ‘–,ğ¸ğ‘œğ‘–,ğ¸ğ‘ğ‘–. We then compute the vec-
tor sum of 5 embeddings and output a single embedding ğ‘¥ğ‘–:ğ‘¥ğ‘–=
ğ‘ ğ‘¢ğ‘š(ğ¸ğ‘ğ‘–,ğ¸ğ‘£ğ‘–,ğ¸ğ‘ğ‘–,ğ¸ğ‘œğ‘–,ğ¸ğ‘ğ‘–). The vector sum operation aggregates
themultiplemodalities( e.g.,instructionandstate)ofeachtokeninto
asingleembedding.Whenwecomputeattentionsbetweenthese
embeddings, i.e.,dot product [ 93], the cross-modality (instruction-
state) dependencies are naturally computed, following the distribu-
tive property ofmultiplication: ğ‘¥ğ‘–Â·ğ‘¥ğ‘—=ğ¸ğ‘ğ‘–Â·ğ¸ğ‘ğ‘—+...+ğ¸ğ‘ğ‘–Â·ğ¸ğ‘ğ‘—.
Lossfunctions. Afterencodingalltheinputsequencesasasingle
sequence of embeddings ğ‘¥=(ğ‘¥1,..,ğ‘¥ğ‘›), we feed ğ‘¥to self-attention
layers. The output of self-attention layers are known as the con-
textual embeddings ğ‘’=(ğ‘’1,..,ğ‘’ğ‘›). We stack two independent 2-
layer feedforward networks â„ğ‘£andâ„ğ‘“, that takes ğ‘’as input and
outputthepredicted ğœ‡DataStateand ğœ‡ControlState.Formally,let
ğ‘“={0,1}ğ‘›denotethe ğœ‡ControlStatelabels,and ğ‘€asetoflocations
in the masked ğœ‡DataState ğ‘£. We define the pretraining objective as:
ğ‘šğ‘–ğ‘›/summationdisplay.1
ğ‘–âˆˆğ‘€ğ‘€ğ‘†ğ¸(ğ‘£ğ‘–,â„ğ‘£(ğ‘’ğ‘–)) +ğ›¼ğ‘›/summationdisplay.1
ğ‘–=1ğµğ¶ğ¸(ğ‘“ğ‘–,â„ğ‘–(ğ‘’ğ‘–))(1)The first part of the objective function aims to minimize the
MeanSquaredError(MSE)betweenthepredicted8-byteandthe
groundtruth8-bytefortheonlymasked ğœ‡DataState.NotethatMSE
treats the output byte tokens as numerical values (as opposed to
categoricalastreatedintheinput).Suchasettingencouragesthe
loss to penalize predictions far from the groundtruth ( e.g.,predicts
0x00butthegroundtruthis 0xff).Thesecondpartoftheobjective
functionaimstominimizetheBinaryCross-Entropy(BCE)between
thepredicted ğœ‡ControlStateandthegroundtruth,forallinputto-
kens.ğ›¼is the weighting hyperparameter that keeps the scale of
bothlossesatroughlythesamemagnitude.Asallthemodulesof
StateFormer aredifferentiable, i.e.,NAU,FFNusedforaggregating
input sequences, self-attention layers, and â„ğ‘£andâ„ğ‘–, optimizing
Equation 1can be efficiently solvedbygradient descent.
3.4 Transfer Learning TypeInference
Afterpretrainingwith GSM,wetransfer StateFormer â€™slearned
knowledgebyfinetuningitfortypeinference.Wedefineourconsid-
eredtypesinFigure 4,whichservesasthelabelsfor StateFormer to
predict. Notably, our considered types are much more fine-grained
thantheexistingML-basedtypeinferenceapproaches.Forexample,
EKLAVYA [ 20] does not distinguish signedness of the primitive
types.Debin[ 40]doesnothandlefloatingpoint.Andbothworks
treat the pointer as a single type ( ptr), without inferring what the
pointer refers( e.g.,predicting charâˆ—orstructâˆ—).
As discussed in Section 2.3, we do not collect ğœ‡State by micro-
executing the code during finetuning. Specifically, we replace each
token inğ‘£with the dummy token $$(described in Section 3.2) and
still follow thesamesteps tocompute theembeddings ğ‘¥.We then
stack a new prediction head â„ğ‘¡ğ‘¦ğ‘ğ‘’, a 2-layer feedforward network,
that takes as input ğ‘’(the output of the last self-attention layers),
and predicts the type labels defined in Figure 4for each input
code token. Formally, let ğ‘¡ğ‘–denote the groundtruth type of code
tokenğ‘ğ‘–, the objective function of finetuning task is defined as the
Cross-Entropybetweenthepredictedtype â„ğ‘¡ğ‘¦ğ‘ğ‘’(ğ‘’ğ‘–)andğ‘¡ğ‘–foreach
tokeninaninputsequencewithlength ğ‘›:ğ‘šğ‘–ğ‘›ğ‘›/summationtext.1
ğ‘–=1ğ¶ğ¸(ğ‘¡ğ‘–,â„ğ‘¡ğ‘¦ğ‘ğ‘’(ğ‘’ğ‘–)).
695StateFormer : Fine-GrainedType Recovery from BinariesusingGenerative StateModeling ESEC/FSE â€™21, August 23Å›28, 2021,Athens,Greece
âŸ¨typeâŸ©::=âŸ¨accessâŸ©|â€˜no-access â€™
âŸ¨accessâŸ©::=âŸ¨primâŸ©|âŸ¨aggâŸ©|âŸ¨ptrâŸ©
âŸ¨ptrâŸ©::=âŸ¨primâŸ©â€˜*â€™ |âŸ¨aggâŸ©â€˜*â€™ |â€˜void*â€™
âŸ¨primâŸ©::= â€˜floatâ€™|â€˜doubleâ€™|â€˜long double â€™|âŸ¨signâŸ©â€˜charâ€™|âŸ¨signâŸ©â€˜shortâ€™|
âŸ¨signâŸ©â€˜intâ€™ |âŸ¨signâŸ©â€˜longâ€™ |âŸ¨signâŸ©â€˜long long â€™
âŸ¨aggâŸ©::= â€˜structâ€™ |â€˜unionâ€™ |â€˜enumâ€™ |â€˜arrayâ€™
âŸ¨signâŸ©::= â€˜signedâ€™ |â€˜unsigned â€™
Figure4:Thetypes(total35typesafterconcretizingthepro-
duction rule) that StateFormer predicts. âŸ¨ğ‘ğ‘Ÿğ‘–ğ‘šâŸ©,âŸ¨ğ‘ğ‘”ğ‘”âŸ©, and
âŸ¨ğ‘ğ‘¡ğ‘ŸâŸ©stand forprimitive,aggregate,andpointertypes.
During finetuning, both â„ğ‘¡ğ‘¦ğ‘ğ‘’and the pretrained model weights
willbe updatedbygradient descent.
4 IMPLEMENTATION AND SETUP
We implement StateFormer using the Fairseq toolkit [65] based
on PyTorch 1.6.0. All the experiments are run on a Linux server
withUbuntu18.04,IntelXeon4214at2.20GHzwith48virtualcores,
188GBRAM,and4NvidiaRTX2080-TiGPUs.Toobtainground-
truth types for training and testing, we compile all the software
projectswithdebugginginformationandparsetheDWARFsections
usingpyelftools [12]andGhidra[1].
ğœ‡State collection. To log the program states ( ğœ‡State) for pretrain-
ingStateFormer onGSMtask, we implement micro-execution
using Unicorn [ 76], a cross-architecture CPU emulator based on
QEMU[11].Specifically,wemicro-executeeachfunctionbinaries
(collected from the datasets described below) 9 times with different
randomizedinitialvaluesforregistersandmemory,generating9
sets ofğœ‡State for each function binary. To align the ğœ‡State with
the corresponding assembly instructions (Section 3.2), we leverage
Capstone [75]to disassemble the function binaries.
Metrics. As described in Section 2, we treat type inference as a
classificationtask.Asthedatasetshavehighly imbalancedlabels ,
wherethemajorityoftokensdonotpossessanytype,weusepreci-
sion(ğ‘ƒ),recall(ğ‘…),andğ¹1scoretomeasuretheactualperformance
ofStateFormer andallothertools.Let ğ‘‡ğ‘ƒdenotethenumberof
correctlypredictedtypes, ğ¹ğ‘ƒdenotethatofincorrectlypredicted
types,ğ‘‡ğ‘denote the number of correctly predicted no-access ,
andğ¹ğ‘denote the number of incorrectly predicted no-access .
ğ‘ƒ=ğ‘‡ğ‘ƒ/(ğ‘‡ğ‘ƒ+ğ¹ğ‘ƒ),ğ‘…=ğ‘‡ğ‘ƒ/(ğ‘‡ğ‘ƒ+ğ¹ğ‘),andğ¹1=2Â·ğ‘ƒÂ·ğ‘…/(ğ‘ƒ+ğ‘…).
Baseline tools. We compare StateFormer with 3 state-of-the-art
ML-based type inference prototypes: EKLAVYA [ 20], Debin [ 40],
andTypeMiner[ 59].Thesetoolshavebeendemonstratedtooutper-
formtraditionaltypeinferencetechniques.Forexample,EKLAVYA
has been shown to outperform TypeArmor [ 91], which is based on
principleddataflowanalysissuch as def-use andliveness analysis.
EKLAVYA implements the function signature recovery task. The
authorsdefinethetaskaspredictingthetypeoffunctionarguments.
Since EKLAVYA does not release their trained model, we use their
reported numbers and use the same datasets to evaluate State-
Formerâ€™saccuracyinrecoveringtypes for function argument.Table 1: The statistics of our datasets, categorized by archi-
tecture (Arch),optimization (OPT), and obfuscation (OBF).
ARCH OPT/OBF # Variables # Instructions # Functions
O0 26,173,242 12,511,100 821,191
O1 27,845,108 9,346,292 874,595
O2 27,829,459 9,279,857 898,930
O3 28,143,646 10,114,915 942,138ARM
Total 109,991,455 41,252,164 3,536,854
O0 13,474,083 14,096,871 602,699
O1 15,081,503 10,559,297 652,769
O2 15,146,769 10,170,866 678,577
O3 15,457,561 11,021,417 721,519MIPS
Total 59,159,916 45,848,451 2,655,564
O0 187,621,379 53,057,850 6,735,347
O1 189,217,168 51,024,118 6,787,678
O2 189,220,382 51,410,490 6,810,321
O3 189,554,035 52,275,998 6,853,561x86
Total 755,612,964 207,768,456 27,186,907
O0 184,390,034 40,286,578 6,599,662
O1 186,140,724 38,196,269 6,656,821
O2 186,114,113 38,355,719 6,679,632
O3 186,425,557 39,179,302 6,723,296
bcf 714,892 12,960,798 119,706
cff 644,018 11,604,224 90,740
sub 714,310 6,960,835 119,481x64
Total 745,143,648 187,543,725 26,989,338
Total 1,669,907,983 482,412,796 60,368,663
Debin recovers both variable types and names. As we do not
studyrecoveringsource-level variablenamesbutfocusonobtain-
ingvariabletypes,wecomparewithDebinâ€™stypepredictiononly.
Since Debin has released their trained model, we run Debin on our
datasets directlyandcompare against its attainedaccuracy.
TypeMinerconsidersmuchfiner-grainedtypelabelsthanthepre-
vious twoworks. For example,it further distinguishes the pointer
type tostructandchar, while the former two do not. As TypeM-
iner isnot open-sourced,we have contacted the authors to obtain
their reported F1 scores and compare them to StateFormer by
runningStateFormer ontheir dataset.
These tools vary in their definition of the target types ( e.g.,
EKLAVYAislimitedtopredictingonlyfunctionargumenttypes)
and the evaluated architectures ( e.g.,TypeMiner only handles x64,
EKLAVYAhandlesx86andx64).Hence,weadjustoursetupaccord-
inglywhen comparing withthe baselines.
Dataset. We collect 33 open-sourced software projects in their lat-
estversions,includingpopularandlargeprojectssuchasOpenSSL,
ImageMagic,andCoreutils.Duetothepageconstraints,weputthe
detailsofthedatasetsinour supplementarymaterial.Wecompile
these software projects to 4 instruction set architectures including
x86, x64, MIPS, and ARM, each with 4 optimizations, i.e.,O0-O3, us-
ingGCC-7.5, and 3 obfuscation strategies, including bogus control
flow(bcf),controlflowflattening( cff),andinstructionsubstitution
(sub), using Hikari [ 104] based on Clang-8. Table1summarizes
the statisticsofthe datasets.
Pretraining and finetuning setup. We pretrain StateFormer
(withGSM) on all datasets inTable 1. We sample a random 10% of
the functions from the pretraining datasets as the validation set.
Wethenpretrainthemodelin10epochsandcheckpointthemodel
weights that achieve the lowest validation loss for finetuning. Note
thatGSMpretraining task does not have any access to ground truth
696ESEC/FSE â€™21, August 23Å›28, 2021,Athens,Greece Pei, Guan,Broughton,Chen, Yao, Williams-King,Ummadisetty, Yang,Ray, Jana
Table 2: StateFormer â€™s precision/recall/F1 for each archi-
tecture(ARCH),optimization(OPT),andobfuscation(OBF).
ARCH OPT/OBF Precision Recall F1 score
O0 77.1 79.2 78.1
O1 78 76.2 77.1
O2 77.3 73.7 75.4ARM
O3 90.9 89.9 90.4
MIPSO0 98.9 91.7 95.2
O1 86.1 67.6 75.7
O2 80 68 73.4
O3 81.3 71.2 75.8
O0 85.3 83.8 84.5
O1 72.4 70.9 71.6
O2 74.8 70.9 72.8x86
O3 83.6 79.8 81.6
O0 81.5 81.4 81.4
O1 75.8 74 74.9
O2 71.1 69.2 70.1
O3 72.3 70.4 71.3
bcf 73.5 70.5 72
cff 73.2 71.1 72.1x64
sub 75.6 69.1 72.2
type labels . Therefore, we canalways collectarbitrary binariesfor
pretraining, includingthoseused infinetuningfor type inference.
This isacommon practiceintransfer learning[ 24,67].
We finetune on StateFormer our dataset categorized by the
architectureandoptimization/obfuscation(Section 5.1).Weparti-
tionthetrainingandtestingsetbyrandomlyselecting90%ofthe
functionsfor training andthe remainderfor testing.
Hyperparameters. We pretrain and finetune StateFormer for
10epochsand50epochs,respectively. We set thedefaultmasking
percentage ğ‘ƒğ‘šğ‘ğ‘ ğ‘˜=0.8(Section3.3) and study different choices
ofğ‘ƒğ‘šğ‘ğ‘ ğ‘˜in Section 5.4. We choose ğ›¼=4in Equation 1such
that the MSE of predicting ğœ‡DataState and the BCE of predict-
ingğœ‡ControlStatearescaledtothesamemagnitude.Weperform
an extensive evaluation of the properties of NAU to understand its
capabilityinencodingnumericalvaluesandlearningarithmetics.
Duetothespaceconstraints,weputthedetailsofthisstudyandthe
complete hyperparameter settings inour supplementary material.
5 EVALUATION
We aim to answer the following researchquestions.
â€¢RQ1: Howaccurateis StateFormer intype inference?
â€¢RQ2: How does StateFormer compare to the state-of-the-art
ML-basedsystems?
â€¢RQ3: Howfast is StateFormer comparedto othertools?
â€¢RQ4: How effective is pretraining with GSMin improving the
type inference accuracy?
â€¢RQ5: How well does StateFormer approximate the operational
semantics bytraining with GSM?
5.1 RQ1: Accuracy
We first study the accuracy of StateFormer on all binaries. Fol-
lowing the setup described in Section 4, we report the results in
Table2.StateFormer achievesanaverage77.9%F1scoreacross
allarchitecture,optimization, andobfuscation.
Onx86andx64,Weobservethat StateFormer remainsrelatively
robustforbinarieswithhigheroptimizationandobfuscation.For
EKLAVYA
 StateFormer
O0
 O1
 O2
 O3
0.00
0.25
0.50
0.75
1.00Accuracy(a)x86
O0
 O1
 O2
 O3
0.00
0.25
0.50
0.75
1.00Accuracy (b)x64
Figure 5: Accuracy of EKLAVYA and StateFormer on bina-
ries ofdifferentarchitecturesand optimizations.
example,theF1scoreforx86 O3isonly2.9%lowerthanthatofx86
O0.TheF1scoreforx64 O3isonly3.6%lowerthanthatofx64 O1.
Regarding the performance across differentarchitectures (with all
optimizations/obfuscations), we notice no significant difference on
average.Theseobservationsindicatethat StateFormer isrobust
across architectures and optimizations with disparate operational
semantics oftheirinstructions.
StateFormer achievesanaverage77.9%F1scoreacrossallar-
chitecture,optimization,andobfuscationandremainsrobustfor
binarieswithhigher optimization levels andobfuscations.
5.2 RQ2: Comparisonto Baseline
Baselinecomparison. Wecompare StateFormer with3state-of-
the-art type inference tools, namely EKLAVYA, Debin, and TypeM-
iner,asdescribedinSection 4.TocomparewithEKLAVYA,weeval-
uateStateFormer onthesame8projectsconsideredintheirpaper:
Binutils,Coreutils,Findutils,sg3-utils,util-linux,Inetutils,Diffutils,
andusbutils.Weevaluate StateFormer on7typesconsideredin
EKLAVYA.EKLAVYAtreatstypeinferenceforeachargument(of
multiple function arguments) as an independent classification task
and reportsthe accuracy(insteadofF1score). Wethusalsoeval-
uateStateFormer â€™saccuracy,definedasthenumberof correctly
predictedtypes dividedbyallthe number of tokens.
Figure5compares StateFormer to EKLAVYA side-by-side on
two architectures ( i.e.,x86 and x64) and 4 optimizations ( O0-O3)
asEKLAVYAisevaluatedwiththesesettings.Onaverage, State-
FormeroutperformsEKLAVYAby13.3%.Notably, StateFormer
remainsrobustacrossdifferentoptimizationlevels,whileEKLAVYA
has aclear dropwhen the optimization level isincreased.
TocomparewithDebin,weruntheirreleasedmodelonOpenSSL,
whichwehaveconfirmedisnotincludedintheirtrainingset.We
compileOpenSSLinto3architectures(x86,x64,andARM)with4
optimizations( O0-O3).AsDebinconsidersonly17types,wealso
restrictthepredictionof StateFormer tothesame17types.Fig-
ure6shows that StateFormer consistently outperforms Debin
onallarchitecturesandoptimizations,achieving14.6%higherF1
scores onaverage.We observe Debin hasan apparentdrop inF1
scoreswithhigheroptimizations(downto46.1%forARM),while
StateFormer remainsrobust withat least70%F1 scores.
Finally, we compare StateFormer to TypeMiner on the same
datasets they have considered. We restrict our test on x64 with
O3,as TypeMiner is evaluated onlyonx64.TypeMiner treatstype
inference as a multi-stage classification task,trainingindependent
697StateFormer : Fine-GrainedType Recovery from BinariesusingGenerative StateModeling ESEC/FSE â€™21, August 23Å›28, 2021,Athens,Greece
Debin-x86
StateFormer-x86
Debin-x64
StateFormer-x64
Debin-ARM
StateFormer-ARM
O0
 O1
 O2
 O3
 O0
 O1
 O2
 O3
 O0
 O1
 O2
 O3
0.00
0.25
0.50
0.75F1 score
Figure6:F1ofDebinand StateFormer inrecoveringtypes
forbinariesofdifferentarchitecturesandoptimizations.
<ptr>
 <prim>
0.00
0.25
0.50
0.75
1.00F1 score
TypeMiner
 StateFormer
(a)Task 1
array*
 struct*
 char*
 other ptr
0.0
0.2
0.4
0.6
0.8F1 score
TypeMiner
 StateFormer (b)Task 2
int
 long
 char
 double
0.00
0.25
0.50
0.75
1.00F1 score
TypeMiner
 StateFormer
(c) Task 3
signed
 unsigned
0.0
0.2
0.4
0.6
0.8F1 score
TypeMiner
 StateFormer (d)Task 4
Figure 7: StateFormer â€™s and TypeMinerâ€™s F1 scores in 4
type inferencetasksdefined inSection 5.2.
classifierstopredicttypesatdifferentlevels.Forexample,itfirst
trainsabinaryclassifiertopredictwhetheravariableisapointer
andthentrainsasecondclassifiertopredictthepointertype.Since
they do not make complete predictions in one-shot, we compare
StateFormer on4sub-tasksonwhichTypeMinerhasbeeneval-
uated. Specifically, TypeMinerâ€™s first prediction task is a binary
classificationtaskdecidingwhetheravariablehasapointertype
(<ptr>)oraprimitivetype( <prim>).Itssecondtaskistopredictthe
pointertypes,including arrayâˆ—,structâˆ—,charâˆ—,andother ptr .
Itsthirdtaskistopredict the primitivetypes, including int,long
int,char, anddouble. Its fourth task is to predict the signedness,
including signedandunsigned .Welabelthese4tasksasTask1-4.
Figure7demonstratesthat StateFormer outperformsTypeMiner
in4tasksbyanaverage8.2%.Inparticular,TypeMinersignificantly
fluctuates when predicting primitive types (Task 3) and pointer
types (Task 2),but StateFormer ismore robust.
StateFormer outperforms EKLAVYA, Debin, and TypeMiner
by13.3%,14.6%, and8.2%, respectively, and ismore robust than
allbaselinesfor differentoptimizations andtype granularity.
5.3 RQ3: Inference Speed
Weevaluate StateFormer â€™sinferencespeedonbinaryprograms
and compare it to Debin and Ghidra. Specifically, we consider 4
software projectswithdifferentsizesonx64compiledwith O0.Table3:Executiontime(seconds)of StateFormer (onboth
CPUandGPU),Debin,andGhidraon4ofourdatasets,with
diversenumberofinstructions(measured inthousand).
Project # Inst (k)RuntimeStateFormer
SpeedupStateFormerDebinGhidraCPU GPU
ImageMagic 1,252187.87.3N/Aâˆ—664.3 91Ã—
PuTTY 969 146.0 5.6 5239.8514.2 91.8Ã—
Findutils 15723.70.9849.083.3 92.6Ã—
zlib 22 3.3 0.1 119.011.7 117Ã—
âˆ—Debinterminates abruptly afterrunningone ofthebinariesfor 138 minutes.
10
 20
 30
 40
 50
Epochs
âˆ’0.4
âˆ’0.2
0.0
0.2
0.4
0.6F1 score
 w/ GSM
w/ GSM (only Î¼DataState)
w/ GSM (only Î¼ControlState)
w/o GSM
(a)Pretraining GSMeffect
10
 20
 30
 40
 50
Epochs
0.1
0.2
0.3
0.4
0.5
0.6
0.7F1 score
 Masking 80% Pmask=0.8
Masking 60% Pmask=0.6
Masking 40% Pmask=0.4
Masking 20% Pmask=0.2 (b)Maskingpercentage effect
Figure 8: (Left) StateFormer â€™s testing F1 scores when
it is (1) pretrained with GSM, (2) pretrained with only
predicting ğœ‡DataState, (3) pretrained with only predicting
ğœ‡ControlState,or(4)notpretrained.(Right) StateFormer â€™s
validationF1scoreateachfinetuningepochwhenthemask-
ingpercentages ğ‘ƒğ‘šğ‘ğ‘ ğ‘˜inGSMare 0.8, 0.6, 0.4, or0.2.
Table3shows the runtime performance of StateFormer , Debin,
andGhidra. StateFormer (basedonGPUs)achieves98.1 Ã—speedup
onaveragethanthesecond-besttool.Notably,whiletheauthorsof
Debin have tried to optimize their underlying learning algorithms
(conditionalrandomfield)withparallelizedimplementation[ 78],
itperforms1023 Ã—and35.8Ã—slowerthan StateFormer GPUand
CPU, respectively. We attribute the speedup of StateFormer to its
underlying neural architecture, which is amenable to GPU acceler-
ation, while neither Debinâ€™s nor Ghidraâ€™s underlying algorithms
can be implementedusing GPUefficiently.
StateFormer is98.1Ã—faster thanthe second-besttool.
5.4 RQ4: Effectiveness of GSM
Inthissection,wedigdeeperintotheeffectivenessof GSMpretrain-
ingtaskbyquantifyinghowmuchimprovementthat StateFormer
achieves when pretrainedwith GSM.
Effectiveness of GSM.We compare StateFormer â€™s finetuning
accuracywhenitis(1)pretrainedwith GSM,(2)pretrainedwith par-
tialGSMby only predicting ğœ‡DataState, (3) pretrained with partial
GSMbyonly predicting ğœ‡ControlState, and(4) not pretrained.
Figure8ashowsStateFormer â€™sF1ateachfinetuningepoch. It
demonstratesthat StateFormer pretrainedwith GSMachievesthe
bestfinetuningF1scores:itreaches71.3%F1scorewithinthe 50
epochs.Withoutpretrainingwith GSM,itonlyachieves38.3%F1
698ESEC/FSE â€™21, August 23Å›28, 2021,Athens,Greece Pei, Guan,Broughton,Chen, Yao, Williams-King,Ummadisetty, Yang,Ray, Jana
score.Wealsonotethat StateFormer pretrainedwithonlypredict-
ingğœ‡DataStateoutperformsthatwithonlypredicting ğœ‡ControlState.
Thisisintuitiveaspredicting ğœ‡DataStaterequiresunderstanding
instructionsâ€™ actual execution effect and computing the concrete
values,whilepredicting ğœ‡ControlStateisonlyabinaryclassification
task encoding approximate control flow. Nevertheless, we observe
even pretraining with predicting only ğœ‡DataState or ğœ‡ControlState
is still beneficial for type inference, as StateFormer pretrained on
eitherofthemobtains 69%and62%F1 scores, respectively.
Masking percentage. Recall in GSM, we train StateFormer to
reconstruct the masked ğœ‡DataState, and we use default masking
percentage ğ‘ƒğ‘šğ‘ğ‘ ğ‘˜=0.8throughout our experiments (Section 4).
Asmaskinglesspercentageof ğœ‡DataStatemakesiteasiertotrain
onGSM, we study how varying ğ‘ƒğ‘šğ‘ğ‘ ğ‘˜affects the type inference
performance. Figure 8bshows the validation F1 scores achieved
byStateFormer whenwevary ğ‘ƒğ‘šğ‘ğ‘ ğ‘˜.Weobservethatthemore
we mask in GSM, the better it boosts the type inference perfor-
mance, but the gap of improvement is not significant. One possible
explanation is that even in one example, the masked states are less,
manypretrainingsamplesandthedynamicmaskingstillintroduce
diverseenough casesfor learningoperationalsemantics.
StateFormer pretrainedwith GSMoutperformsthatwithout
pretrainingby 33% inF1 score.Masking percentageinpretrain-
ingGSMdoes not significantly affect the finetuning results: pre-
training with 20% masking rate results in <2% decrease in F1
score comparedto pretrainingwith80%maskingrate.
5.5 RQ5: StateFormer Performance on GSM
Pretraining losses with GSM.We also study the losses of pre-
trainingStateFormer withGSM.Suchastudydirectlyvalidates
whether pretraining with GSMindeed helps StateFormer to learn
operational semantics. Low losses on unseen testing ğœ‡State and
functionbinariesindicatesthat StateFormer highlylikelylearnsto
generalizebasedonitslearnedknowledgeofoperationalsemantics.
Figure9shows the training and validation losses in 10 epochs
ofpretraining StateFormer .Thevalidationsetisconstructedby
sampling a random 10% functions from the projects used in pre-
training (as described in Section 4). Specifically, Figure 9ashows
theMSElossofpredicting ğœ‡DataStateandFigure 9bshowstheBCE
loss of predicting ğœ‡ControlState. We observe that the validation
MSEdropsto0.00011,whichtranslatestoaverageabsolutedistance
(by taking the square root) between prediction and groundtruth
as 0.011 (âˆš
0.00011=0.011). As we normalize the byte values from
[0,256]into[0,1](seeSection 3.2),0.011Ã—256=2.8istheactual
absoluteerrorbetweenthepredictedbyteandthegroundtruth.The
average error within the deviation of only 3-byte indicates that
StateFormer learns to approximate the executioneffect.
Effectsofcontrolanddataflowpretraining. Concurrenttoour
work,Trex[ 68]alsoleveragestransferlearningtolearnprogram
execution semantics. However, Trex completely ignores control
and data flow modeling as it focuses on binary similarity detection.
Incontrast, StateFormer focusesontypeinference;therefore,it
requiresprecisedataflow(typeofoutputofaninstructiondepends
on types of operands) and control flow (it must also infer types of
valuesinthe unexecutedportionofthe code).
1
2
3
4
5
6
7
8
9
10
Pretrain Epoch
0.00025
0.00050
0.00075
0.00100
0.00125MSE loss
Training
Validation(a)MSEinpredicting ğœ‡DataState
1
2
3
4
5
6
7
8
9
10
Pretrain Epoch
0.000
0.002
0.004
0.006BCE Loss
Training
Validation (b)BCE inpredicting ğœ‡ControlState
Figure 9: MSE and BCE of predicting ğœ‡DataState and
ğœ‡ControlState,respectively,duringpretrainingwith GSM.
10
 20
 30
 40
 50
Epochs
0.1
0.2
0.3
0.4
0.5
0.6
0.7F1 score
StateFormer
Trex
Figure 10: Type inference F1 score between models pre-
trained by GSMandTrexâ€™spretrainingobjective.
Because of these differences in the high-level requirements of
thedownstreamtasks, StateFormer and Trexadoptsignificantly
differentpretrainingapproaches, i.e.,generatingcontrolanddata
flowstate( GSM)vs.codeandtracetokenclassification.Ingeneral,it
remains an open challenge in transfer learning to determine which
pretrainingtaskisthemosteffectiveforwhichdownstreamtask.
Part of our contribution in StateFormer is to design a pretraining
task that makes the downstream task of type inference precise. For
example, Figure 10shows that StateFormer outperforms Trex by
around 10.9percentagepointsinF1 score for type inference.
Probing StateFormer on real-world code. Besides quantify-
ingthepretraininglosses,weprobethepretrained StateFormer
using aconcrete binary example to study howitpredicts ğœ‡State.
Consider the example in Figure 11. We examine how State-
Formerpredicts registers esp,edi,ebp, andesifrom input
ğœ‡DataState, in which we mask all registers except for their first
appearances. The accurate prediction of espat line 5 suggests that
StateFormer is able to associate 0x0886644e withespat line 1
andline2andunderstandtheexecutioneffectof sub.Further,to
predictesiat line 6,StateFormer needs to understand xorâ€™s exe-
cution effects at line 3. Since there is no other occurrence of esi
in this code block, we can conclude that the prediction of esiis
basedsolely on StateFormer â€™sunderstanding of xor.
GSMiseffectiveinassisting StateFormer tolearnvariousin-
structionsâ€™operationalsemantics. StateFormer â€™sabsoluteerror
in predicting ğœ‡DataState during pretraining GSMis very low
(within 3onaveragefor eachbyte).
699StateFormer : Fine-GrainedType Recovery from BinariesusingGenerative StateModeling ESEC/FSE â€™21, August 23Å›28, 2021,Athens,Greece
Line	Number 		Register Ground	Truth Prediction
2 esp 0x0886644e 0x0886644e
5 esp 0x0886644e 0x0886644e
5 edi 0x00000000 0x00000000
6 ebp 0x0886644e 0x0886644e
6 esi 0x00000000 0x00000000<remove_quoted_ifs>:
Code: 																		 Input			DataState:
...																				...
1		mov	ebp,esp									mov	0x0885544e,0x0886644e
2		sub	esp,hexv								sub	<mask>,0x00000000
...																				...
3		xor	esi,esi									xor	0x0886644f,0x0886644f
4		mov	edi,eax									mov	0x02222883,0x00000000
5		mov	[esp],edi							mov	[<mask>],<mask>
...																				...
6		mov	[ebp-hexv],esi		mov	[<mask>],<mask>
Figure11:Thecodeand ğœ‡DataStateof remove_quoted_ifs in
bash.Wehighlightwithsamecolorsthemaskedvaluesthat
StateFormer relieson to make theprediction.
6 THREATS TO VALIDITY
Targetbinaries. We focus on the binary ready to be disassembled
anddonotconsidermaliciouslypackedbinariesasitrequiresan
entirelydifferenttoolchaintounpack. StateFormer canbeapplied
oncethe binariesare unpackedordecrypted.
Datasets. Weaimtocollectdiversedatasetsofsoftwareprojects
toexposevariousinstancesofoperationalsemantics.Tothisend,
we ensure our datasets have different implemented functionalities
(e.g.,utilityfunctions, imageprocessing functions, etc.).
Hyperparameters. Wekeepmosthyperparametersfixedthrough-
outtheevaluation,consideringthefact thatthereisnoprincipled
methodfor tuningthemto date[ 50].Nevertheless, weensureour
hyperparameter choiceisempirically reasonable(Section 4).
7 RELATED WORK
There are two main lines of prior works that are related to our
work Å› type inference from binaries ( e.g.,for binary hardening and
decompilation)andtypeinferencefromsourcecodeofdynamically-
typed languages ( e.g.,for software debugging, IDE support, and
API understanding for developers) [ 42,73,95]. In this paper, we
focusontypeinferenceforbinaries.Binaryanalysisisknowntobe
more challenging asrecoveringstrippedsource-levelconstructsis
anundecidableproblem[ 62,66].Moreover,manysource-leveltype
hints such as the variable name and the computation that operates
on the variable are absent at binary-level. We summarize common
approachesusedfor differenttype inference tasksbelow.
Traditionalapproaches. Static analysis hasbeenwidely adopted
inoff-the-shelf reverseengineeringtools for type inference[ 1,14,
81,82,85,86].Astandardstaticanalysisapproachfortypeinference
uses domain-expert-provided rules for different instructions/state-
ments to either directly specify the operand types [ 6,23,31,32,39,
44], or define how types should be propagated from instructions
with known types to other instructions [ 9,19,21,25,28,51,52,55,103].Totrackthetypepropagation,theseworksoftenrelyonexpen-
sivedata/controldependencyanalysis[ 8,9,46,47,51,52,59,72,82].
By contrast,dynamic analysisuses accurateprogram states and
memoryaccesspatternsobservedduringprogramexecution[ 27]
to define precise rules for type inference [ 5,15,22,38,48,77,79,
84] and propagation [ 17,34Å›36,54,83,102]. However, dynamic
approachessufferfrom lowcodecoverage,leading toa highfalse
negativerate[ 51].Increasingcodecoveragerequirescollectingand
combiningdynamictracesfrommultipleprogramexecutions[ 15,
83], whichincurs prohibitivelyhigh overhead.
StateFormer enjoysthebenefitsofbothstaticanddynamicanal-
ysis as itautomates learning instructionsâ€™ approximate operational
semanticsfrom cheapmicro-execution andusessuchsemanticsto
learn type inference rules withoutdynamic execution .
ML-based approaches. Recently, machine learning has been in-
creasingly applied to type inference. Examples include inferring
the type of function argument [ 20], recovering general variable
type [41,59,60,71,97,100], and other metadata ( e.g.,variable
names) [2,40,56,78,80,90,92,94]. However, existing ML-based
binary type inference approaches use only static code without any
tracesandsufferfromsimilarlimitationsasstaticanalysis.Concur-
renttoourwork,Trex[ 68]alsoleveragestransferlearningtolearn
program execution semantics. However, Trex is not control/data
flow aware, resulting in a significant performance drop in the type
inference task,as showninSection 5.5.
Morebroadly,machinelearninghasshownsignificantsuccessin
learninggeneralizablerepresentationthatappliestomanyprogram
analysistasks [ 3,13,69].StateFormer contributes a new generic
framework to learn programsâ€™ operational semantics. Therefore,
we believe StateFormer has a great potential to apply to other
downstream program analysistasksbeyondtype inference.
8 CONCLUSION
We presented StateFormer , a neural architecture that uses the
operational semantics of assembly code to recover type informa-
tionfromstrippedbinaries.Wedesignedanovelpretrainingtask,
Generative State Modeling, to help StateFormer to learn code
operationalsemanticsandtransfersthisknowledgetolearntype
inference rules. We showed that StateFormer is 14.6% more ac-
curate than state-of-the-art tools, and our ablation studies showed
thatGSMimproves type inference accuracyby33%.
ACKNOWLEDGMENT
Wethanktheanonymousreviewersfortheirconstructiveandvalu-
able feedback. This work is sponsored in part by NSF grants CNS-
1842456, CNS-1801426, CNS-1617670, CCF-1822965, CCF-1845893,
CNS-1563843,CNS-1564055,and IIS-2040961;ONR grants N00014-
17-1-2010,N00014-16-1-2263,andN00014-17-1-2788;anNSFCA-
REER award; an ARL Young Investigator (YIP) award; a Google
FacultyFellowship;aJ.P.MorganFacultyresearchaward;aDiDi
Faculty research award; a Google Cloud grant; a Capital One re-
search grant; and an Amazon Web Services grant. Any opinions,
findings, conclusions, or recommendations expressed herein are
those of the authors, and do not necessarily reflect those of the US
Government,ONR,ARL,NSF,CaptitalOne,Google,J.P.Morgan,
DiDi, orAmazon.
700ESEC/FSE â€™21, August 23Å›28, 2021,Athens,Greece Pei, Guan,Broughton,Chen, Yao, Williams-King,Ummadisetty, Yang,Ray, Jana
REFERENCES
[1]National Security Agency. 2019. Ghidra Disassembler. https://ghidra-sre .org/.
[2]Miltiadis Allamanis, Earl T Barr, Christian Bird, and Charles Sutton. 2015. Sug-
gesting accurate method and class names. In 2015 10th Joint Meeting on Founda-
tions ofSoftwareEngineering .
[3]MiltiadisAllamanis,EarlTBarr,PremkumarDevanbu,andCharlesSutton.2018.
Asurveyofmachinelearningforbigcodeandnaturalness. Comput.Surveys
(2018).
[4]Anil Altinay, Joseph Nash, Taddeus Kroes, Prabhu Rajasekaran, Dixin Zhou,
AdrianDabrowski,DavidGens,YeoulNa,StijnVolckaert,CristianoGiuffrida,
et al.2020. BinRec: dynamic binary lifting and recompilation. In Fifteenth
European Conference onComputer Systems .
[5]Jong-hoon An,AvikChaudhuri, JeffreySFoster,andMichaelHicks.2011. Dy-
namicinferenceof static typesfor Ruby. ACMSIGPLAN Notices (2011).
[6]Christopher Anderson, Paola Giannini, and Sophia Drossopoulou. 2005. To-
wardstypeinferenceforJavaScript.In EuropeanconferenceonObject-oriented
programming .
[7]DennisAndriesse,AsiaSlowinska,andHerbertBos.2017. Compiler-agnostic
function detection in binaries. In 2017 IEEE European Symposium on Security
and Privacy .
[8]Gogul Balakrishnan and Thomas Reps. 2004. Analyzing memory accesses in
x86executables. In Internationalconference oncompiler construction .
[9]Gogul Balakrishnan and Thomas Reps. 2007. Divine: Discovering variables
inexecutables.In InternationalWorkshoponVerification,ModelChecking,and
AbstractInterpretation .
[10]TiffanyBao,JonathanBurket,MaverickWoo,RafaelTurner,andDavidBrumley.
2014. BYTEWEIGHT: Learning to Recognize Functions in Binary Code. In 23rd
USENIXSecuritySymposium .
[11]FabriceBellard.2005. QEMU,afastandportabledynamictranslator.In USENIX
Annual TechnicalConference, FREENIXTrack .
[12] Eli Bendersky. [n.d.]. PYEFLTOOLS. https://github .com/eliben/pyelftools .
[13]Pavol Bielik, Veselin Raychev, and Martin Vechev. 2015. Programming with"
bigcode":Lessons,techniquesandapplications.In 1stSummitonAdvancesin
ProgrammingLanguages .
[14]David Brumley, IvanJager, Thanassis Avgerinos, andEdward J Schwartz. 2011.
BAP:Abinaryanalysisplatform.In InternationalConferenceonComputerAided
Verification .
[15]JuanCaballero,NoahMJohnson,StephenMcCamant,andDawnSong.2010.
BinaryCodeExtractionandInterfaceIdentificationforSecurityApplications.
In2010Network and DistributedSystemSecuritySymposium .
[16]Juan Caballero and Zhiqiang Lin. 2016. Type inference on executables. Comput.
Surveys(2016).
[17]Juan Caballero, Pongsin Poosankam, ChristianKreibich, and DawnSong. 2009.
Dispatcher:Enablingactivebotnetinfiltrationusingautomaticprotocolreverse-
engineering.In 16thACMconferenceonComputerandcommunicationssecurity .
[18]XiChen,AsiaSlowinska,DennisAndriesse,HerbertBos,andCristianoGiuffrida.
2015. StackArmor:Comprehensive protectionfromstack-based memory error
vulnerabilities for binaries. In 2015 Network and Distributed System Security
Symposium .
[19]Mihai Christodorescu, Nicholas Kidd, and Wen-Han Goh. 2005. String analysis
forx86binaries.In 6thACMSIGPLAN-SIGSOFTworkshoponProgramanalysis
for softwaretoolsand engineering .
[20]ZhengLeongChua,ShiqiShen,PrateekSaxena,andZhenkaiLiang.2017.Neural
nets canlearn function type signatures from binaries. In 26th USENIX Security
Symposium .
[21]Ezgi Ã‡iÃ§ek, Weihao Qu, Gilles Barthe, Marco Gaboardi, and Deepak Garg. 2019.
Bidirectional type checking for relational properties. In 40th ACM SIGPLAN
Conference onProgrammingLanguage Designand Implementation .
[22]AnthonyCozzie,FrankStratton,HuiXue,andSamuelTKing.2008. Digging
forDataStructures.In 2008USENIXSymposiumonOperatingSystemsDesign
and Implementation .
[23]Lorisdâ€™Antoni,MarcoGaboardi,EmilioJesÃºsGallegoArias,AndreasHaeberlen,
andBenjaminPierce.2013. Sensitivityanalysisusingtype-basedconstraints.
In1stannualworkshop onFunctionalprogrammingconcepts indomain-specific
languages .
[24]JacobDevlin,Ming-WeiChang,KentonLee,andKristinaToutanova.2019.BERT:
Pre-training of deep bidirectional transformers for language understanding. In
2019 Annual Conference of the North American Chapter of the Association for
ComputationalLinguistics: Human Language Technologies .
[25]DavidDeweyandJonathonTGiffin.2012. StaticdetectionofC++vtableescape
vulnerabilities in binarycode.In 2012 NetworkandDistributedSystem Security
Symposium .
[26]EN Dolgova and AV Chernov. 2009. Automatic reconstruction of data types in
the decompilation problem. Programmingand Computer Software (2009).
[27]Khaled ElWazeer, Kapil Anand, Aparna Kotha, Matthew Smithson, and Rajeev
Barua.2013.Scalablevariableanddatatypedetectioninabinaryrewriter.In 34th
ACMSIGPLANconferenceonProgramminglanguagedesignandimplementation .[28]MV EmmerikandTrentWaddington.2004. Usingadecompilerforreal-world
source recovery.In 11thWorking Conference onReverse Engineering .
[29]Michael D Ernst. 2003. Static and dynamic analysis: Synergy and duality. In
2003 International Conference on Software Engineering Workshop on Dynamic
Analysis. 24Å›27.
[30]RezaMirzazadeFarkhani,SamanJafari,SajjadArshad,WilliamRobertson,Engin
Kirda, and Hamed Okhravi. 2018. On the effectiveness of type-based control
flowintegrity. In 34thAnnual Computer SecurityApplicationsConference .
[31]Alexander Fokin, Katerina Troshina, and Alexander Chernov. 2010. Recon-
structionof classhierarchies fordecompilation of C++ programs.In 201014th
European Conference onSoftwareMaintenance and Reengineering .
[32]MichaelFurr,Jong-hoonAn,JeffreySFoster,andMichaelHicks.2009. Static
typeinferencefor Ruby. In 2009 ACMsymposiumonAppliedComputing .
[33]PatriceGodefroid.2014. Microexecution.In 36thInternationalConferenceon
SoftwareEngineering .
[34]NevilleGrech,BerndFischer,andJulianRathke.2018. Preemptivetypechecking.
Journal oflogical and algebraicmethodsinprogramming (2018).
[35]NevilleGrech,JulianRathke,andBerndFischer.2013. Preemptivetypechecking
in dynamically typed languages. In International Colloquium on Theoretical
Aspects ofComputing .
[36]Philip J Guo, Jeff H Perkins, Stephen McCamant, and Michael D Ernst. 2006.
Dynamicinferenceofabstracttypes.In 2006internationalsymposiumonSoftware
testingand analysis .
[37]IstvanHaller,YuseokJeon,HuiPeng,MathiasPayer,CristianoGiuffrida,Herbert
Bos,andErikVanDerKouwe.2016. TypeSan:Practicaltypeconfusiondetection.
In2016ACMSIGSAC Conference onComputer and Communications Security .
[38]Istvan Haller, Asia Slowinska, and Herbert Bos. 2013. Mempick: High-level
data structure detection in C/C++ binaries. In 2013 20th Working Conference on
Reverse Engineering .
[39]MostafaHassan,CaterinaUrban,MarcoEilers,andPeterMÃ¼ller.2018. MaxSMT-
basedtypeinferenceforPython3.In InternationalConferenceonComputerAided
Verification .
[40]JingxuanHe,PeshoIvanov,PetarTsankov,VeselinRaychev,andMartinVechev.
2018. DEBIN: Predicting debuginformation in strippedbinaries.In 2018 ACM
SIGSAC Conference onComputer and Communications Security .
[41]Vincent J Hellendoorn, Christian Bird, Earl T Barr, and Miltiadis Allamanis.
2018. Deep learning type inference. In 2018 26th acm joint meeting on european
softwareengineeringconferenceandsymposiumonthefoundationsofsoftware
engineering .
[42]Christian Holler, Kim Herzig, and Andreas Zeller. 2012. Fuzzing with code
fragments.In 21st USENIXSecuritySymposium .
[43]MdNahidHossain, JunaoWang,OfirWeisse,RSekar,DanielGenkin, Boyuan
He, Scott D Stoller, Gan Fang, Frank Piessens, Evan Downing, et al .2018.
Dependence-preservingdata compactionfor scalable forensic analysis.In 27th
USENIXSecuritySymposium .
[44]SimonHolmJensen,AndersMÃ¹ller,andPeterThiemann.2009. Typeanalysis
for JavaScript.In InternationalStatic AnalysisSymposium .
[45]Jiajun Jiang, Yingfei Xiong, Hongyu Zhang, Qing Gao, and Xiangqun Chen.
2018. Shaping program repair space with existing patches and similar code. In
27thACMSIGSOFTinternational symposiumonsoftwaretestingand analysis .
[46]WuxiaJin, Yuanfang Cai, Rick Kazman, Gang Zhang, Qinghua Zheng, and Ting
Liu. 2020. Exploring the Architectural Impact of Possible Dependencies in
PythonSoftware.In 202035thIEEE/ACMInternationalConferenceonAutomated
SoftwareEngineering .
[47]Wesley Jin, Cory Cohen, Jeffrey Gennari, Charles Hines, Sagar Chaki, Arie
Gurfinkel,JeffreyHavrilla,andPriyaNarasimhan.2014. RecoveringC++objects
frombinaries using inter-procedural data-flowanalysis. In Proceedings of ACM
SIGPLAN onProgramProtectionand Reverse Engineering Workshop 2014 .
[48]Changhee Jung and Nathan Clark. 2009. DDT: design and evaluation of a
dynamic program analysis for optimizing data structure usage. In 42nd Annual
IEEE/ACMInternationalSymposiumonMicroarchitecture .
[49]Guillaume Lample and Alexis Conneau. 2019. Cross-lingual language model
pretraining.In 33rdConference onNeural InformationProcessingSystems .
[50]YannLeCun,PatriceYSimard,andBarakPearlmutter.1993. Automaticlearning
ratemaximizationbyon-lineestimationofthehessianâ€™seigenvectors.In 1993
AdvancesinNeural InformationProcessingSystem .
[51]JongHyup Lee, Thanassis Avgerinos, and David Brumley. 2011. TIE: Principled
reverseengineeringoftypesinbinaryprograms.In 2011NetworkandDistributed
SystemSecuritySymposium .
[52]Junghee Lim, Thomas Reps, and Ben Liblit. 2006. Extracting output formats
from executables. In 200613thWorking Conference onReverse Engineering .
[53]Yan Lin and Debin Gao. 2021. When Function Signature Recovery Meets Com-
piler Optimization.In 2021 IEEE SymposiumonSecurityand Privacy .
[54]Zhiqiang Lin, Xuxian Jiang, Dongyan Xu, and Xiangyu Zhang. 2008. Auto-
matic Protocol Format Reverse Engineering through Context-Aware Monitored
Execution. In 2008Network and DistributedSystemSecuritySymposium .
[55]Zhiqiang Lin, Xiangyu Zhang, and Dongyan Xu. 2010. Automatic reverse
engineering of data structures from binary execution. In 2010 Network and
DistributedSystemSecuritySymposium .
701StateFormer : Fine-GrainedType Recovery from BinariesusingGenerative StateModeling ESEC/FSE â€™21, August 23Å›28, 2021,Athens,Greece
[56]KuiLiu,DongsunKim,TegawendÃ©FBissyandÃ©,TaeyoungKim,KisubKim,Anil
Koyuncu, Suntae Kim,and YvesLeTraon. 2019. Learningto spot and refactor
inconsistent method names. In 2019 IEEE/ACM 41st International Conference on
SoftwareEngineering .
[57]Kangjie Lu and Hong Hu. 2019. Where does it go? refining indirect-call targets
withmulti-layertypeanalysis.In 2019ACMSIGSACConferenceonComputer
and Communications Security .
[58]Andreas Madsen and Alexander RosenbergJohansen. 2020. NeuralArithmetic
Units.InInternationalConference onLearning Representations .
[59]Alwin Maier, Hugo Gascon, Christian Wressnegger, and Konrad Rieck. 2019.
TypeMiner:Recoveringtypesinbinaryprogramsusingmachinelearning.In
InternationalConferenceonDetectionofIntrusionsandMalware,andVulnerability
Assessment .
[60]Rabee Sohail Malik, Jibesh Patra, and Michael Pradel. 2019. NL2Type: inferring
JavaScriptfunctiontypesfromnaturallanguageinformation.In 2019IEEE/ACM
41st InternationalConference onSoftwareEngineering .
[61]JamesMartensandIlyaSutskever.2011. Learningrecurrentneuralnetworks
with hessian-free optimization. In 28th international conference on machine
learning.
[62]Kenneth Miller, Yonghwi Kwon, Yi Sun, Zhuo Zhang, Xiangyu Zhang, and
ZhiqiangLin.2019. Probabilisticdisassembly.In 41stInternationalConference
onSoftwareEngineering .
[63] LiliMou,Ge Li,LuZhang,TaoWang,andZhiJin.2016. Convolutional neural
networks over tree structures forprogramming language processing. In AAAI
Conference onArtificial Intelligence .
[64]HanneRiisNielsonandFlemmingNielson.1992. Semanticswithapplications .
Vol. 104. Springer.
[65]MyleOtt, Sergey Edunov, Alexei Baevski,Angela Fan,SamGross, Nathan Ng,
DavidGrangier,andMichaelAuli.2019. Fairseq:Afast,extensibletoolkitfor
sequencemodeling.In 2019AnnualConferenceoftheNorthAmericanChapterof
the Association for Computational Linguistics: Human Language Technologies:
Demonstrations .
[66]ChengbinPang,RuotongYu,YaohuiChen,EricKoskinen,GeorgiosPortokalidis,
BingMao,andJunXu.2021. SoK:AllYouEverWantedtoKnowAboutx86/x64
BinaryDisassemblyButWereAfraidtoAsk.In 2021IEEESymposiumonSecurity
and Privacy .
[67]KexinPei,JonasGuan,DavidWilliams-King,JunfengYang,andSumanJana.
2021. XDA: Accurate, Robust Disassembly with Transfer Learning. In 2021
Network and DistributedSystemSecuritySymposium .
[68]Kexin Pei, Zhou Xuan, Junfeng Yang, Suman Jana, and Baishakhi Ray. 2020.
TREX:LearningExecutionSemanticsfromMicro-TracesforBinarySimilarity.
arXiv preprint arXiv:2012.08680 (2020).
[69]HaoPeng,LiliMou,GeLi,YuxuanLiu,LuZhang,andZhiJin.2015. Building
programvectorrepresentationsfordeeplearning.In InternationalConference
onKnowledgeScience, Engineering and Management .
[70]G.D.Plotkin.1981. AStructuralApproachtoOperationalSemantics. University
ofAarhus (1981).
[71]MichaelPradel, GeorgiosGousios,Jason Liu,andSatish Chandra.2020. Type-
writer: Neural type prediction with search-based validation. In 28th ACM Joint
Meeting on European Software Engineering Conference and Symposium on the
FoundationsofSoftwareEngineering .
[72]MichaelPradel,ParkerSchuh,andKoushikSen.2015. TypeDevil:Dynamictype
inconsistency analysis for JavaScript. In 2015 IEEE/ACM 37th IEEE International
Conference onSoftwareEngineering .
[73]Michael Pradel and Koushik Sen. 2015. The good, the bad, and the ugly: An
empirical study of implicit type conversions in JavaScript. In 29th European
Conference onObject-OrientedProgramming .
[74]AravindPrakash,HengYin,andZhenkaiLiang.2013. Enforcingsystem-wide
controlflowintegrityforexploitdetectionanddiagnosis.In 8thACMSIGSAC
symposiumonInformation, computer and communicationssecurity .
[75]Nguyen AnhQuynh. 2014. Capstone: Next-gen disassembly framework. Black
Hat USA(2014).
[76]NGUYEN Anh QuynhandDANG HoangVu.2015. Unicorn:NextGeneration
CPU Emulator Framework. BlackHatUSA (2015).
[77]Easwaran Raman and David I August. 2005. Recursive data structure profiling.
In2005workshop onMemorysystemperformance .
[78]VeselinRaychev,MartinVechev,andAndreasKrause.2015. Predictingprogram
propertiesfrom Å‚bigcodeÅ¾. ACMSIGPLAN Notices (2015).
[79]BriannaMRen,JohnToman,TStephenStrickland,andJeffreySFoster.2013.
The ruby type checker. In 28th Annual ACM Symposium on Applied Computing .
[80]MartinPRobillard,EricBodden,DavidKawrykow,MiraMezini,andTristan
Ratchford.2012. AutomatedAPI property inferencetechniques. IEEETransac-
tions onSoftwareEngineering (2012).[81] Hex-Rays SA.2008. IDAProDisassembler.
[82]Yan Shoshitaishvili, Ruoyu Wang, Christopher Salls, Nick Stephens, Mario
Polino,AudreyDutcher,JohnGrosen,SijiFeng,ChristopheHauser,Christopher
Kruegel,andGiovanniVigna.2016. SoK:(Stateof)TheArtofWar:Offensive
TechniquesinBinaryAnalysis.In 2016IEEESymposiumonSecurityandPrivacy .
[83]Asia Slowinska, Traian Stancescu, and Herbert Bos. 2011. Howard: A Dynamic
ExcavatorforReverseEngineeringDataStructures.In 2011NetworkandDis-
tributedSystemSecuritySymposium .
[84]Venkatesh Srinivasan and Thomas Reps. 2014. Recovery of class hierarchies
andcomposition relationshipsfrommachine code.In InternationalConference
onCompiler Construction .
[85]Binary Ninja Team. 2015. Binary Ninja Å› A new type of reversing platform.
https://binary .ninja/.
[86]Radare2 Team. 2017. Radare2 GitHub repository. https://github .com/radare/
radare2.
[87]DavidTrabish,TimotejKapus,Noam Rinetzky,andCristian Cadar.2020. Past-
sensitive pointer analysis for symbolic execution. In 28th ACM Joint Meeting on
EuropeanSoftwareEngineeringConferenceandSymposiumontheFoundationsof
SoftwareEngineering .
[88]Andrew Trask, Felix Hill, Scott E Reed, Jack Rae, Chris Dyer, and Phil Blunsom.
2018. Neuralarithmeticlogicunits.In AdvancesinNeuralInformationProcessing
Systems. 8035Å›8044.
[89]David Urbina, Yufei Gu, Juan Caballero, and Zhiqiang Lin. 2014. Sigpath: A
memorygraphbasedapproachforprogramdataintrospectionandmodification.
InEuropean SymposiumonResearch inComputer Security .
[90]Muhammad Usman, Wenxi Wang, Kaiyuan Wang, Cagdas Yelen, Nima Dini,
and Sarfraz Khurshid. 2020. A study of learning likely data structure properties
using machine learning models. International Journal on Software Tools for
Technology Transfer (2020).
[91]VictorVan DerVeen,Enes GÃ¶ktas,MoritzContag,Andre Pawoloski,XiChen,
SanjayRawat,HerbertBos,ThorstenHolz,EliasAthanasopoulos,andCristiano
Giuffrida. 2016. A tough call: Mitigating advanced code-reuse attacks at the
binarylevel.In 2016IEEE SymposiumonSecurityand Privacy .
[92]Bogdan Vasilescu, Casey Casalnuovo, and Premkumar Devanbu. 2017. Recover-
ingclear,naturalidentifiersfromobfuscatedJSnames.In 201711thjointmeeting
onfoundations ofsoftwareengineering .
[93]Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
Aidan N Gomez, Åukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you
need.In2017AdvancesinNeural InformationProcessingSystems .
[94]Yaza Wainakh, Moiz Rauf, and Michael Pradel. 2021. IdBench: Evaluating
SemanticRepresentationsofIdentifierNamesinSourceCode.In 2021IEEE/ACM
43rdInternationalConference onSoftwareEngineering .
[95]Xiaoyin Wang, Lu Zhang, Tao Xie, Hong Mei, and Jiasu Sun. 2009. Locating
need-to-translate constant strings for software internationalization. In 2009
IEEE 31st InternationalConference onSoftwareEngineering .
[96]RichardWartell,VishwathMohan,KevinWHamlen,andZhiqiangLin.2012.
Securinguntrustedcodeviacompiler-agnosticbinaryrewriting.In 28thAnnual
Computer SecurityApplicationsConference .
[97]Jiayi Wei, Maruth Goyal, Greg Durrett, and Isil Dillig. 2020. Lambdanet: Proba-
bilistic type inferenceusing graph neuralnetworks. In 2020 InternationalCon-
ference onLearning Representations .
[98]PaulJWerbos.1990. Backpropagationthroughtime:whatitdoesandhowto
do it.IEEE78,10(1990), 1550Å›1560.
[99]DavidWilliams-King,HidenoriKobayashi,KentWilliams-King,GrahamPatter-
son, Frank Spano, Yu Jian Wu, Junfeng Yang, and Vasileios P. Kemerlis. 2020.
Egalito:Layout-AgnosticBinaryRecompilation.In 25thInternationalConference
onArchitecturalSupport for ProgrammingLanguages and OperatingSystems .
[100]ZhaoguiXu,XiangyuZhang,LinChen,KexinPei,andBaowenXu.2016. Python
probabilistictypeinferencewithnaturallanguagesupport.In 24thACMSIGSOFT
international symposiumonfoundations ofsoftwareengineering .
[101]DongruiZengandGangTan.2018. FromDebugging-InformationBasedBinary-
Level Type Inference to CFG Generation. In Eighth ACM Conference on Data
and ApplicationSecurityand Privacy .
[102] JunyuanZeng,YangchunFu,KennethAMiller, ZhiqiangLin,Xiangyu Zhang,
andDongyanXu.2013. Obfuscation resilientbinarycodereuse throughtrace-
oriented programming. In 2013 ACM SIGSAC conference on Computer & commu-
nicationssecurity .
[103]Chao Zhang, Chengyu Song, Kevin Zhijie Chen, Zhaofeng Chen, and Dawn
Song.2015. VTint:ProtectingVirtualFunctionTablesâ€™Integrity.In 2015Network
and DistributedSystemSecuritySymposium .
[104]NavilleZhang.2017. HikariÅ›animprovementoverObfuscator-LLVM. https:
//github.com/HikariObfuscator/Hikari .
702