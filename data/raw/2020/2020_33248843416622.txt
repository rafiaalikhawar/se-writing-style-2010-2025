DeepTC-Enhancer: Improving the Readability of Automatically
Generated Tests
Devjeet Roy
devjeet.roy@wsu.edu
Washington State
UniversityZiyi Zhang
ziyi.zhang2@wsu.edu
Washington State
UniversityMaggie Ma
yuzhanm@amazon.com
Infra Supply Chain &
Automation, AmazonVenera Arnaoudova
venera.arnaoudova@wsu.edu
Washington State
University
Annibale Panichella
a.panichella@tudelft.nl
Delft University of
TechnologySebastiano Panichella
panc@zhaw.ch
Zurich University of
Applied ScienceDanielle Gonzalez
dng2551@rit.edu
Rochester Institute of
TechnologyMehdi Mirakhorli
mehdi@se.rit.edu
Rochester Institute of
Technology
ABSTRACT
Automatedtestcasegenerationtoolshavebeensuccessfullypro-
posed to reduce the amount of human and infrastructure resources
requiredtowriteandruntestcases.However,recentstudiesdemon-
stratethatthereadabilityofgeneratedtestsisverylimiteddueto
(i) uninformativeidentifiers and (ii)lack of properdocumentation.
Prior studies proposed techniques to improve test readability by
eithergeneratingnaturallanguagesummariesormeaningfulmeth-
ods names. While these approaches are shown to improve test
readability, they are also affected by two limitations: (1) generated
summariesareoftenperceivedastooverboseandredundantbyde-
velopers, and (2) readable tests require both proper method names
but also meaningful identifiers (within-method readability).
In this work, we combine template based methods and Deep
Learning(DL)approachestoautomaticallygeneratetestcasesce-
narios (elicited from natural language patterns of test case state-
ments)aswellastotrainDLmodelsonpath-basedrepresentations
of source code to generate meaningful identifier names. Our ap-
proach,called DeepTC-Enhancer ,recommendsdocumentationand
identifier names with the ultimate goal of enhancing readability of
automatically generated test cases.
Anempiricalevaluationwith36externalandinternaldevelop-
ers shows that (1) DeepTC-Enhancer outperforms significantly the
baseline approach for generating summaries and performs equally
with the baseline approach for test case renaming, (2) the trans-
formation proposed by DeepTC-Enhancer results in a significant
increaseinreadabilityofautomaticallygeneratedtestcases,and(3)
there is a significant difference in the feature preferences between
external and internal developers.
ACM Reference Format:
DevjeetRoy,ZiyiZhang,MaggieMa,VeneraArnaoudova,AnnibalePanichella,
Sebastiano Panichella, Danielle Gonzalez, and Mehdi Mirakhorli. 2020.
DeepTC-Enhancer : Improving the Readability of Automatically Generated
Tests. In35th IEEE/ACM International Conference on Automated Software
ASE ‚Äô20, September 21‚Äì25, 2020, Virtual Event, Australia
¬© 2020 Copyright held by the owner/author(s).
ACM ISBN 978-1-4503-6768-4/20/09.
https://doi.org/10.1145/3324884.3416622Engineering (ASE ‚Äô20), September 21‚Äì25, 2020, Virtual Event, Australia. ACM,
New York, NY, USA, 12 pages. https://doi.org/10.1145/3324884.3416622
1 INTRODUCTION
Softwaretestingisacrucialpartofthesoftwaredevelopmentlife
cyclethatensuressoftwaresystemqualityandreliabilityproper-
ties [9]. However, writing tests is a resource-intensive endeavor;
developers often spend 25% of their development time on software
testing[11].Tohelpreducethecostoftesting,softwareengineering
researchers have developed several approaches to generate testsautomatically. A significant amount of progress has been made
primarilyinthecaseofunittests.Today,thereexistseveraltools,
such as EvoSuite [ 16] and Randoop [ 35], that can automatically
generate an entire test suite given a project‚Äôs source code (or byte-
code). The maturity of the field also resulted in surveys [ 10,31]
and several editions of tool competitions [ 15,25,32]. Furthermore,
empiricalstudiesshowedthatthetestssynthesizedbythesetools
areeffective[ 5,18]atdetectingfaults,andaresubstantiallycheaper
to produce [37].
Despitetheseadvances,generatedunittestsposeasignificant
maintenanceburdenwhenincludingtheminaproject[ 12].This
is because developers have to manually validate the generate as-
sertions(oracleproblem)andanalyzethethrownexceptions[ 24]
(potential crashes). These automatically written tests have poor
readabilitycomparedtotheirhuman-writtencounterpartsdueto
thelackofdocumentationandtheuseofobfuscatedvariablenames.
Consider for example, the test case in Figure 1, which is automati-
callygeneratedusingEvoSuite[ 16]fortheclass KeycloakUriBuilder
fromtheKeycloakopen-sourceproject.Whilethetestmethodis
concise,itspurposeisnotimmediatelyobvious.Thevariablenames
have no clear purpose and just tell us the types of the instantiated
objects and primitive types and their counts. Besides, the name of
thetestinitselfisgeneric,andtherearenocommentstoprovide
any hints about the scenario under test.
Inrecentyears,researchershaveproposedvariousapproaches
topartiallymitigatetheseissues.Therelatedworkcanbeclassified
into two main categories: (1) generating natural language sum-
maries to support comprehension, and (2) improving the test code
for better readability. Panichella et al. [ 38] proposed a template-
based summary generator for automatically generated tests. Their
empiricalstudyshowedthattestsummarieshelpdevelopersduring
debugging, i.e., finding more bugs and in less time. Daka et al. [ 14]
2872020 35th IEEE/ACM International Conference on Automated Software Engineering (ASE)
This work is licensed under a Creative Commons Attribution International 4.0 License.
Figure 1: A unit test generated by EvoSuite.
proposed a technique to generate descriptive test names based
on code coverage heuristics. Their empirical study showed that
humansperceivethesynthesizednamesasdescriptiveasmanually-
writtentestnames[ 14].Whilethesetwoapproachesaddressthe
testreadabilityproblemindifferentandcomplementaryways,theybothhavesignificantlimitations.First,developersoftenperceivethe
generatedsummariesastoodetailed(statement-levelcomments)
and redundant [ 38]; they also do not solve the nondescript naming
convention(developers‚Äôfeedbackreportedin[ 38]).Furthermore,
the approach from Daka et al. [ 14] considers only test method
names, while the actual content of the test methods remains un-
changed.Inotherwords,thereadabilityofthegeneratedtestsisstillaffectedbymeaninglessidentifiernames(e.g.,
string0 inFigure1).
To address these open challenges, we propose a two-stage ap-
proach,called DeepTC-Enhancer ,thatcomprehensivelyimproves
the readability of automatically generated unit tests. First, DeepTC-
Enhancer automaticallygenerates testcasescenarios usingatem-
platebasedapproach.Thesescenariosaremethod-levelsummaries,
i.e., leading comments, that aim to summarize the steps, i.e., the
scenariobeingsetupandtestedbyagiventestcase.Ourtestsce-
nariosdifferfromthosegeneratedbyexistingapproaches[ 38]in
the level of abstraction‚Äîthey are more high-level (method-levelas opposed to statement-level) and, therefore, more concise. Therationale is that higher-level summaries will quickly provide de-
velopers with enough information to decide whether the given
test case needs to be further investigated for the task at hand. Sec-
ond, DeepTC-Enhancer reliesonextremecodesummarizationtech-
niques based on Deep Learning to rename all identifiers in the test
casewithmeaningfulnames.Wehypothesizethatsuchrenaming
can significantly increase the readability of these test cases and
ease program comprehension and test maintenance activities.
Specifically, our contributions can be summarized as follows:
‚Ä¢A novel approach for generating natural language scenarios
ofJUnittestcases. DeepTC-Enhancer generatestestmethod-
level summaries that describe the test case scenarios.
‚Ä¢Anoveladaptationofanexistingidentifierrenamingtech-
niqueappliedinthecontextofunittestsinJavaandadapted
to remove its reliance on existing identifier names.
‚Ä¢An empirical evaluation of DeepTC-Enhancer using 6 inter-
nal and 30 external developers, including a comparison to
existing approaches [14, 38].
Figure 2: Overview of DeepTC-Enhancer.
‚Ä¢Areplicationpackage1thatincludes(1)aprototypeimple-
mentationoftheproposedapproach,(2)internalandexternal
developer surveys and (3) the data used for the evaluation.
Paper Structure. Section 2 details of the implementation of
DeepTC-Enhancer . Section 3 provides an overview of the study
designandresearchquestions.Section4discussestheresults,while
threats to its validity are discussed in Section 5. Section 6 provides
an overview of the related work and contrasts the work proposed
heretothestate-of-the-artapproaches.Finally,Section7concludes
the paper and outlines directions for the future work.
2 THE DEEPTC-ENHANCER APPROACH
Figure2depictstheproposed DeepTC-Enhancer ,whichisdesigned
toautomaticallygeneratemethod-levelsummariesandautomati-
cally rename identifiers contained within leveraging (i) existing ap-
proaches on code summarization and (ii) deep learning techniques.
Inthissection,weelaboratethesesteps,detailing DeepTC-Enhancer ‚Äôs
approach,whichconsistsoftwomainphases:(1)TestCaseScenario-
based Summaries and (2) Test Case Identifier Renaming.
2.1 Test Case Scenario-based Summaries
At a high level, the summary generation phase starts by analyzing
each line of code to filter out redundant information. Then, the
remainingstatementsareaggregatedtoallowtheactualgeneration
of the method-level summary. Following is an in-depth description
of each step in the summary generation phase.
2.1.1 Statement Analysis. We analyze all statements in the gen-
erated test case, looking for opportunities to reduce redundancy.For this, we employ intra- and inter-statement heuristics. Intra-
statement heuristics determine whether details can be removed
from within a single statement. For example, if a statement con-
tainsamethod callwithmorethanoneargument, weremovethe
details about the parameters of the method call. Inter-statement
heuristics determine which statements should be summarized. For
example, if a temporary variable is created in a test case as a place-
holder, then its creation does not need to be summarized as shown
1https://github.com/devjeetr/DeepTC-Enhancer-Improving-the-Readability-of-
Automatically-Generated-Tests
288inFigure3wherethevariableinline21isnotpartofthesummary.
Anotherexample,alsoshowninFigure3,iswhenmultiplevariable
declarations (l.12‚Äì17) can be summarised in one sentence (l.2‚Äì3).
The rationale here is that providing detailed information about
every statement brings the summary too close to the actual source
code.Instead,wehypothesisethatifdevelopersneedmoreinfor-
mation,theywillrefertothesourcecodetoprocureit.Thus,the
summaries that DeepTC-Enhancer generates are intended to guide
developersinidentifyingrelevanttestsfortheirmaintenanceactiv-
ity, rather than to convey detailed information about the tests.
Figure3:Exampleoftestcasescenario-basedsummarygen-
erated by DeepTC-Enhancer.
2.1.2 Statement Aggregation. Once redundancy has been reduced,
DeepTC-Enhancer aggregatestheremainingstatementsintoafinal
summary. To this end, DeepTC-Enhancer takes a template-based
approach as templates have been shown to be successful for the
generation of summaries [ 28,38,41].DeepTC-Enhancer uses a set
ofheuristicsthatwecraftedbymanuallyinvestigating293JUnit
testsfilesfromover31open-sourceprojects.Duringthismanual
investigation process, we identified several common sequences of
statements that appear in unit tests and that can be summarized
by a single statement. For example, a common pattern is the use of
severalassertionsverifyingdifferentpropertiesofatargetobjectasshown in Figure 1. In this example, the last two assertions (l.13‚Äì14)
can be aggregated into a single phrase ‚Äúchecks if port is -1 and
host is not null‚Äù. Another common pattern is a method invocation
and an assert statement on the same object as shown in Figure 3
(l.20 and 22) which are summarized as one step (step 3)in the test
case scenario. The comment in Figure 3 represents the summary
generated by DeepTC-Enhancer.
Theaggregationisperformediteratively,resultinginaggregated
statementsthemselvesbeingcombinedwithotherswhenapplicable.
Weuseasimpleabstractiontoenablethisbehavior:eachindividual
or aggregated statement is assigned an objectand anaction. When
testcasestatementsdonotfallundertheseaggregationpatterns,
we simply provide the statement level scenarios. Our templatescover 97% of the statements in the automatically generated tests
from the projects described in Section 3.3.1.2.
2.2 Identifier Renaming
For the identifier renaming phase of the proposed approach, we
leverage and adapt existing deep learning approaches for extreme
source code summarizations [ 2,4,6,8]. The identifier renaming
process consistsof twoseparate prediction tasks:test caserenam-
ing and variable renaming. There exist several techniques that
haveachievedsubstantialsuccessinbothofthesetasks[ 4,6,40].
Recently,deeplearningtechniqueshaveachievedstate-of-the-art
performance forpredicting the methodname from the body of amethod. Conversely, structured prediction has been successfully
appliedtothetaskofclarifyingvariablenamesfromtheobfuscatedcode[
40].Motivatedbythesuccessofmachinelearningmodelsfor
thesewellknowntasks,wedividetheidentifierrenamingaspect
of our approach into two learning tasks: test case name prediction
and variablename prediction.Wetrain themodel onopen source
projects, asdetailed inSection 2.2.4,and then useit topredict test
case and variable names for automatically written tests. The ra-tionale is that if the model can learn to predict identifier names
usedinhumanwrittentestcases,itwouldbeabletopredictsimilar
names for automatically written tests.
One key difference between this and prior works for these tasks
isthatourmodeldoesnotrelyonvariablenamesalreadypresented
in the source code. This is important as our approach is being
applied to automatically generated test cases that lack meaningful
variablenames.Forboththevariableandthetestcaseprediction
tasks,thisisdonebymaskingallvariablenamesduringthetraining
of the model; we mask the variable that needs to be renamed with
a special token.
2.2.1 Source Code Representation. One of the most important con-
siderationswhendesigningamachinelearningsystemforsoftware
systems is source code representation. In practice, this can varyfrom simplistic representations of source code as a stream of to-
kens [21] to more structured, graph-based representations [ 3]. Our
rationale for the selection process is two-fold:
(1)Cost: The cost of generating the representation should be
minimal, in order to incorporate this tool as an IDE plugin.
(2)Dependencies: The source code representation must be gen-
erated solely from the raw text of the given test case. We
imposethisconstrainttominimizetheconfigurationburden
on the developer end.
Basedonthesecriteria,weuseapath-basedrepresentationwhich
Alonetal.proposed[ 7]andappliedforvarioussourcecodesum-
marization tasks [ 6,8]. In this approach, a section of source code is
represented as an unordered set of abstract syntax tree (AST) paths.
Each path represents a walk between two leaves in the AST of aprogram. We direct readers to [
6‚Äì8] for detailed treatments and
formalized definitions.
At the time it was proposed, this path-based representation pro-
ducedstate-of-the-artperformanceforseveralsummarizationtasks.
2Detailsregardingtheaggregationsandtheirassociatedtemplatescanbefoundinthe
replication package
289Since then, other approaches have been proposed that utilize al-
ternativeabstractsyntaxtreerepresentations[ 27]oruseahybrid
approach that combines an AST representation with a textual rep-
resentation [ 44], with varying levels of performance. We chose the
path-based representation because it is very cheap to compute and
offersalevelofgeneralizationthatenablesittobeappliedunmodi-
fied to our two learning tasks: variable name prediction and test
case name prediction.
2.2.2 Machine Learning Model. Many of the advances made in
codesummarizationoverthepast5yearsframetheproblemasa
translationproblem,i.e.,translatingsourcecodetonaturallanguage.
This formulation allows for the use of a vast variety of approaches
dealing with sequence transduction developed for neural machine
translation(NMT)systems.Forbothofourlearningtasks,weadapt
the model proposed by Alon et al. [ 6]. This model follows the stan-
dard encoder-decoder architecture that, until recently, has been
primarilyutilizedinNMTsystems.Themaindifferencebetween
themodelproposedbyAlonetal.andthestandardencoder-decoder
architecture is that it is designed to use the path based representa-
tiondiscussedintheprevioussection.Themodelusesaspecialized
encoder to create a distributed representation of each AST path,
which is then merged with token embeddings for the two terminal
nodes of the path. This is then used by the decoder to sequentially
generate the target prediction.
2.2.3 Token Representation. Originally, code2seq utilized subto-
ken embeddings to represent source code tokens. In this approach,
tokensinthesourcecodearesplitintosubtokens.Recently,Karam-
patsis et al. show that the use of subword embeddings such as
byte pair encoding (BPE) can significantly decrease the size of the
vocabularyandimp rovethep erformanceofmachinelearningmod-
els inthe contextof sourcecode tasks[ 23]. Hence,for thiswork,
weutilizeSentencePieceBPE[ 26]basedvocabulary.Forthelabel
(methodorvariablenames)subtokens,welimitthevocabularysize
to16,000,andfortheterminalnodesubtokens(anyidentifierthatis
not a variable or method name), we limit the vocabulary to 32,000.
2.2.4 Dataset. Deep learning approaches are typically data ineffi-
cient,requiringalotoftrainingdata.However,wemustensurethat
our predicted identifier names would be of high quality. Manual
validation of the dataset is not feasible, due to the scale of data we
neededto collect.Toensureonly engineered softwareprojectsare
consideredforthiswork,weusedthedatasetMunaiahetal.gen-
erated using REAPER[34]. This dataset includes quality metrics for
eachproject.Weusedthistofilterdatasetforprojectswithatest-to-
sourcecoderatiogreaterthan0.01.Weselectedthisthresholdusing
descriptivestatisticstofindacompromisebetweenthequalityof
theunittestsandthesizeoftheresultingdataset.Afterfiltering,
weextractedallJavafilesthatstartorendwith‚ÄúTest",whichisa
common naming convention for unit test files. The final dataset
consists of 274 engineered projects containing 96,534 unit test files
for a total of 678,860 unit test cases. We divided the dataset into
training (70%), validation (10%), and test set (20%) for our model.
2.2.5 Examples of suggested names. Fortheautomaticallygener-
ated test shown in Figure 1, DeepTC-Enhancer suggests testUri
as test name and primaryKeyUri ,uriBuilder , and hostfor variables
uRI0(l.3), keycloakUriBuilder0 (l.4), and string0(l.6), respectively.
Figure 4: Test case from Figure 1 enhanced usingDeepTC-Enhancer.
For the example shown in Figure 3, DeepTC-Enhancer suggests
thetesttoberenamedto testGetGroup andvariables loggerGroups0
(l.12), hashMap0 (l.14), linkedList0 (l.16), and loggerGroup0 (l.21) to
be renamed to logger,expected,string, and result, respectively.
Figure4andFigure5showthetestcasesfromFigure1andFigure3
with suggested method name and variable names generated by
DeepTC-Enhancer.
Figure 5: Test case from Figure 3 enhanced usingDeepTC-Enhancer.
3 STUDY DEFINITION AND DESIGN
3.1 Research Questions
Thegoalofthisstudyistoevaluatetheabilityof DeepTC-Enhancer to
improve the readability of automatically generated test cases using
test case scenarios and identifier renaming. The quality focus is the
evaluationoftool‚Äôsperformancefromtheperspectiveofdevelopers.
Theperspective of the study is developers who are interested in
using automatically generated test cases but struggle with their
readability.Hence,thestudyisdesignedtoanswerthefollowing
research questions (RQs):
RQ1:Howdoes DeepTC-Enhancer performcomparedtoexisting
techniques? Severalotherapproachesaimtoimprovethereadability
ofautomaticallygeneratedtests,bycreatingtestcasesummaries or
byprovidingmoremeaningfultestnames.As DeepTC-Enhancer en-
hancesboththedocumentation(byaddingmethod-levelsummaries)
290and code (by renaming identifiers) of generated tests, we compare
itsperformanceagainsttheexistingapproachesforthatgenerate
both.ForthisRQ,weusethedatasetsusedintheoriginalevalua-
tions of existing approaches to survey external developers.
RQ2:Towhatextentdoes DeepTC-Enhancer increasethereadabil-
ityofthegeneratedtests? Automatictestcasegenerationrequires
fewerresourcesthanthestandardmanualefforts.However,they
incur a higher maintenance effort due to their poor readability. For
this RQ, in addition to external developers, we also perform an
evaluation with internaldevelopers to gain insighton how useful
developers might find DeepTC-Enhancer in their day to day work.
For this RQ, we use a dataset that we collected consisting of top
starred Java projects on GitHub.
RQ3:What aspects of DeepTC-Enhancer do developers find most
useful?We aim to assess whether developers perceive some en-
hancements applied by our approach more useful than others. For
this RQ, we use the same developer pool and dataset as in RQ 2.
3.2 Baselines
To the best of our knowledge, DeepTC-Enhancer is the first ap-
proachtoapply bothautomateddocumentationandtestcodeen-
hancement(viaidentifierrenaming)towardsimprovingtheread-
abilityofautomaticallygeneratedtestcases.However,thereareap-
proachesthatimprovedindividuallyoneortheotheraspects.Hence,
wecomparethetestscenariosgeneratedby DeepTC-Enhancer with
thetestcasesummariesgeneratedby TestDescriber ,pr oposedby
Panichella et al. [ 38].TestDescriber automatically generates test
case summaries of the portion of code exercised by each test to
provideadynamicviewoftheclassundertest.Thegeneratedsum-
marieshavebeenshowntohelpdeveloperstobetterunderstand
the code under test and improve their bug fixing performance [ 38].
Wecomparethetestcasenamesgeneratedby DeepTC-Enhancer with
the names generated by Daka et al.‚Äôs approach [14]. The later syn-
thesizesdescriptivenamesforautomaticallygeneratedunittests
in terms of their observable behavior at a test code level. This
technique has been implemented as an extension to EvoSuite.
3.3 Experiment Design
Toanswerourresearchquestions,weconducttwoonlineempirical
studies involving internal and external developers. Section 3.3.2
provides detailed information about the participants and their pro-
gramming experience. We recruited participants via e-mail and
socialmedia.Foreachinternaldeveloper,wecreateaspecificver-
sionofthesurveyfortheopen-sourceprojectforwhichtheyare
listed as a contributor. The use of an online questionnaire was pre-
ferredoverin-personinterviews,asitismoreconvenientforthe
participants.Eachsurveystartsbygatheringdemographicinforma-tion.Thenparticipantsareaskedtorespondtoaseriesofquestionsregardingthegeneratedsummaries,methodrenaming,andvariable
renaming.Thesurveyalsocontainedopen-endedfeedbackforms
aftereachquestionto allowthedeveloperstoprovideadditional
insight into their responses.
3.3.1 Objects. Weusethreedifferentdatasetstoevaluatedifferent
aspects of DeepTC-Enhancer:
1)Testsummaries: WeusedthedatasetusedbyPanichellaetal.[ 38]
tocomparethetestcasesummariesgeneratedby DeepTC-EnhancerTable 1: Experience of Participants.
External Internal
Experience # (%) #
0-2 years 6 (20%) 0
3-6 years 14 (47%) 17-10 years 7 (23%) 0>10 years 3 (10%) 5
Total 30 (100%) 6
and TestDescriber in the survey with external developers. The
datasetconsistsoftwoJavaclassesextractedfromtwoopen-source
projects. We use this dataset to answer RQ 1,R Q2, and RQ 3.
2)Methodrenaming: Dakaetal.[ 14]followedasystematicproto-
col[17]toselectobjectsfromtheSF110corpus3ofopen-sourceJava
projects.Theyselectedtentargetmethodsfromdifferentclasses.
We use data from this dataset to answer RQ 1,R Q2, and RQ 3.
3) Variable renaming and evaluation of the overall approach: We
use the 30 most-starred open-source Java projects from GitHub.
We use the same dataset to recruit internal developers for our
evaluation. This dataset was used to evaluate the quality of the
suggestedvariablenamesandtheoverallapproachthatincludes
the generated summaries, method names, and variable names. It is
used to answer RQ 2and RQ 3.
3.3.2 Participants. Werecruitedbothexternaldevelopers,i.e.,peo-
plethathavenotdevelopedthecodeunderinvestigation,andinter-naldevelopers,i.e.,contributorsoftheprojectsweareanalyzing.Weinvitedexternaldevelopersfromindustry,students,andresearchers
from the authors‚Äô institutions as well as from other institutions by
e-mail and social media. For our study with internal developers we
invited 198 top contributors from the 30 most starred open-source
JavaprojectsfromGitHub.Attheend,30externaland6internal
developers responded to our surveys. Table 1 details the partici-
pants‚Äô programming experience. All participants have a Computer
Science background. They were all volunteers and did not receiveany reward for participation in the study.
3.3.3 Surveys. We performedtwo differentsurveys: onewith ex-
ternalandonewithinternaldevelopers.Herewebrieflydescribe
the surveys; more details can be found in our replication package.
1) External developers survey: The purpose of this survey is to eval-
uate several research tools developed to enhance the readability of
generated tests. We create two versions of the survey to contain
differentcodesnippetswhicharerandomlyselectedfromthecorre-
sponding datasets. Participants are randomly assigned to a survey
and asked to evaluate the enhancements using different criteria.
Thesurveyconsistsof17questionsdividedintofoursections;there
is also optional feedback forms intended to allow participants to
elaborateontheiranswers.InSection1,participantsareaskedto
evaluate the quality of the test case summary generated by ourapproach and the baseline TestDescriber [
38]. In Section 2, they
evaluate the quality of the test names suggested by our tool and
bytheworkofDakaetal.[ 14].Sections3containsanevaluation
of the variable renaming in isolation, and Section 4 contains an
evaluationoftheoverallapproach.Tominimizeorderandsequence
3http://www.evosuite.org/experimental-data/sf110/
291effectsaswellasotherbias,forSections1and2,theidentityofthe
tools is not revealed, and the order in which the summaries and
method names are presented is randomized.
2)Internal developerssurvey: Thegoal ofthis surveyisto evaluate
the usefulness of DeepTC-Enhancer for software developers. For
alldevelopersthatindicateaninterestinparticipating,wecreate
a unique survey, even if there is more than one developer for a
particularproject.Thesurveyconsistsof16questionsandoptional
feedbackformsdividedintofoursections.Threeofthesesections
evaluate the test case scenario, method renaming, and variable
renaming inisolation, while the last section containsan evaluation
for the complete approach. The same automatically generated test
case is used for each of these sections.
3.4 Evaluation metrics
Similartorelatedwork[ 33,38,42],thequalityofgeneratedsum-
maries is evaluated according to three dimensions: conciseness,
content,andreadability. Concisesummariesdonotincludeextra-
neous or irrelevant information. Contentmeasures whether the
summary correctly reflects the content of the test case. Readability
measures to what extent a test case (including the generated en-
hancements) is perceived as readable and understandable by the
participants.Inaddition,participantsevaluatehowthe intentofthe
test case are captured by the suggested test case name and variable
names. Participantsalso ratethe naturalness of thesuggested test
casenamewhichindicateshoweasyitistoreadandtounderstand
it.Finally,participantsareaskedtoratethe improvementinread-
abilityof the code snippet enhanced using DeepTC-Enhancer over
the original automatically generated test and their likelihood toutilize
DeepTC-Enhancer if they were to use automatically writ-
ten tests. Depending on the question, participants express their
opinions using a 3-, 4-, or 5-point Likert scale.
3.5 Analysis Method
We used statistical tests to assess the significance of the difference
betweenthescoresachievedbydifferenttools.WeusetheWilcoxon
RankSumtestwithasignificancelevelof ùõº=0.05.Weoptedfor
non-parametrictestsbecausetheShapiro-Wilktestrevealedthat
our data (scores) does not follow a normal distribution ( ùëù-value
<0.01). Besides, we use Cliff‚Äôs ùëëeffect size [ 20] to measure the
magnitudeofthedifference,whichcanbeinterprettedasfollows:
smallforùëë<0.33,mediumfor 0.33‚â§ùëë<0.474 andlargefor
ùëë‚â•0.474 [20].
4 RESULTS
4.1 RQ1 : How does DeepTC-Enhancerperform
compared to existing techniques?
4.1.1 Test Case Summaries. For the reader to get a sense of the
summaries generated by different tools, we present examples of
summariesgeneratedby DeepTC-Enhancer andTestDescriberin
Figure 8 and Figure 7, respectively, for the same unit test. Figure 6
showstheresultsoftheevaluationoftheproposedtestcasescenar-ioswhencomparedwithTestDescriber[
38].Intermsof conciseness,
19 (64%) respondents rated test case scenarios as containing no
unnecessaryinformation,whileontheotherhand,11(37%)rated
Figure 6: Results from the comparison ofDeepTC-Enhancer and TestDescriber for method summaries
on a 3-point Likert scale (higher numbers are better).
Figure 7: Example summary generated by TestDescriber.
Figure 8: Example summary generated by DeepTC-Enhancer.
TestDescriber‚Äôs summaries as having no unnecessary information.
Foreitherapproach,7-9respondentsfoundthatthesummariescon-
tain some unnecessary information. Only 3 respondents found test
case scenarios to contain mostly unnecessary information, as com-
paredto3(10%)respondentsforTestDescribersummaries.Interms
ofcontent adequacy, test case scenarios scored better overall, with
17(57%)respondentsreportingthemtocontainallimportantinfor-
mation, while 7 (23%) and 4 (13%) respondents reported them to be
missingsomeimportantinformationandmissingsomeveryimpor-
tant information, respectively. On the other hand, TestDescriber‚Äôs
292summaries werereported tocontain allimportant information by
10(33%)respondents,missingsomeimportantinformationby11
(37%) and missing some very important information by 7 (23%)
respondents.Lastly,onthecriteriaof readability,mostrespondents
found testcase scenarios to be eithereasy (19 (63%))or somewhat
easy to read (9 (30%)), while only one respondent rated it as being
difficult to read. TestDescriber‚Äôs summaries were reported by 8
(26%) to be easy to read, somewhat easy to read by 12 (40%) and
difficult to read by 8 (27%). Overall, 20 (67%) respondents preferred
test case scenarios to TestDescriber‚Äôs summaries, while 4 (13%)
preferred neither.
From a statistical point of view, the results of the Wilcoxon test
revealedthatthetestscenariosgeneratedby DeepTC-Enhancer are
perceivedbyparticipantsassignificantlymoreconcise( ùëù-value=0.02)
andmorereadable( ùëù-value‚â§0.01)comparedtothesummariesby
TestDescriber.Theeffectsizeis mediuminbothcases,being0.32
and0.44,respectively.Instead,thereisnosignificantdifferencein
terms of quality of the content(ùëù-value=0.07).
Discussion: Acrossallthreecriteriaevaluatedinthesurveyon
external developers, test case scenarios performed better than Test-
Describer‚Äôs summaries. Based on our qualitative analysis of the
respondents,weattributethesehigherratingstotestcasescenarios
being shorter and less detailed than the summaries provided by
TestDescriber.While DeepTC-Enhancer provideshigh-levelsum-
maroesoftestcasesatthemethod-level,thelatterinsteadprovides
detailed, line by line summaries. Some respondents indicated that
detailed comments is redundant, as reading the source code that
follows each statement summary would provide them with the
sameinformation.Thissentimentwasechoedbyanotherpartici-
pantstatedthatthetestcasescenariowas‚Äúconcise,easytoread‚Äù
whereastheforTestDescribersummarytheycould‚Äútellfromthe
code what it does‚Äù. However, while overall DeepTC-Enhancer is
receivedbetterthanTestDescriber,tworespondentsdidnotseethe
value in the summaries.
Figure 9: Results from the comparison ofDeepTC-Enhancer and Daka et al. ‚Äôs approach for test
case names on a 5 and 3 points Likert scales for intentand
naturalness respectively (higher numbers are better).
Figure 10: Example test case that needs renaming.
4.1.2 Test Case Names. ResultsforthecomparisonofDakaetal.‚Äôs
method naming approach to DeepTC-Enhancer ‚Äôs method renam-
ing are shown in Figure 9. More survey respondents found the
method names provided by Daka et al. ‚Äôs approach to fully capture
the intent of the test case (13 (43%)) as compared to those sug-
gestedby DeepTC-Enhancer (4(13%)).7(23%)respondentsreported
that the names suggested by DeepTC-Enhancer either mostly cap-
turetheintentofthetestcaseascomparedto5(17%)forDakaet
al.‚Äôsapproach.Ontheotherendofthespectrum,4(13%)respon-
dents reported the name generated by DeepTC-Enhancer as not
capturing the intent of the test case while 3 (10%) reported the
sameforthebaselineapproach.Asimilarnumberofrespondents
(6 (20%) versus 8 (27%)) reported that the test case names proposed
by either approach were misleading in conveying the intent ofthe test case. With regard to readability, the names suggested by
DeepTC-Enhancer wereperceivedasslightlymorereadablethan
thosesuggestedbythebaseline.14(47%),8(27%),and6(20%)re-
spondentsreportedtheproposedapproachasbeingeasy,somewhateasy and difficult to read, respectively, while 11 (37%), 12 (47%) and
3 (10%) reported the same for Data et al. ‚Äôs approach. Lastly, 11
(37%)respondentseachindicatedapreferenceforeithertool,while
8 (26%) indicated they preferred neither.
However, the Wilcoxon test revealed that there is not signifi-
cant difference in quality between the test names generated by
DeepTC-Enhancer and the approach by Daka et al. [ 14]. More
precisely, there is no significant difference in terms of intent(ùëù-
value=0.07) nor in terms of readability ( ùëù-value=0.84).
Discussion: Overall, Daka et al. ‚Äôs approach performs slightly
better in terms of capturing the intent of the test case, and slightly
worseintermsofreadability,albeitthedifferencesarenotstatis-
tically significant. Despite that, participants were equally likelytoprefernamesgeneratedbyeitherapproach.Wepositthatthis
preferencearoseoutoftheconcisenessof DeepTC-Enhancer ‚Äôssug-
gested names, and conversely the excessive verbosity of Daka et
al.‚Äôsapproach.Indeed, severalrespondentsindicated thatthey pre-ferconcisenessoververbosityfortestcasenames,unlessincreased
verbosity is required to address naming conflicts. For example,
for the test case in Figure 10, Daka et al. ‚Äôs approach suggeststest name
testVisitAnnotation -WithNonEmptyStringAndFalse and
DeepTC-Enhancer suggests testVisitAnnotation . We also observe
from the responses that names suggested by DeepTC-Enhancer
might be perceived as too general. One respondent stated thatthis can be a problem when multiple test cases are similar toone another. Lastly, our results reveal that a significant portion
(8, i.e., 26%) of the respondents did not prefer the method names
fromeithertool.Thisindicatesthatthemethodnamesgeneratedby
both DeepTC-Enhancer andDakaetal.havequitesomeroomfor
293improvements. We surmise that respondents would prefer method
names that combine some aspects of the verbosityof Dakaet al. ‚Äôs
names,whilealsoretainingsomeoftheconcisenessofthenames
suggested by DeepTC-Enhancer.
RQ1Externaldevelopersfindtheproposedtestcasescenarios
generated by DeepTC-Enhancer to be more concise, content
adequate, and readable than the summaries generated by the
baseline. The difference is statistically significant for concise-
ness and readability with a medium effect size. They also found
the method names suggested by DeepTC-Enhancer to be more
concisethanthoseofthebaseline.Intermsofreadabilityandin-
tent, the method names by DeepTC-Enhancer and the baseline
are statistically equivalent.
4.2 RQ 2: To what extent does
DeepTC-Enhancerincrease the readability of
the generated tests?
4.2.1 Overall Approach. We observe that respondents provided fa-
vorableratingsoftheproposedapproach‚Äôsimpactonthereadability
ofautomaticallygeneratedunittests.13respondents(43%)reported
thattheenhancementappliedby DeepTC-Enhancer resultedina
significant improvement in the readability of the generated test
cases,while11(37%)reportedthat theincreaseinreadabilitywas
minor.Threerespondents(10%)indicatedthattherewasnochange
inreadabilitywhile2(7%)reportedthattherewasaminordecrease
in readability. No participant indicated there to be a significantdecrease in readability as a result of the enhancements providedby the proposed approach. As to the likelihood of respondents
using DeepTC-Enhancer toenhanceautomaticallygeneratedtest
cases,wefoundthat6participants(20%)reportedthattheywere
extremelylikelytouseit,while16(53%)indicatedthattheywere
somewhatlikelytouseit.Onerespondentindicatedthattheywere
somewhat unlikely to use the proposed approach while two in-dicated that they were highly unlikely to use
DeepTC-Enhancer .
Lastly, mostparticipants (20(67%)) indicatedthatthe existenceof
the proposed tool would make them more likely to utilize auto-matedtestgenerationtools intheirprojects.7participants (23%)
indicatedthattheexistenceofthetoolwouldhavenoeffectontheir
choiceofusingautomatictestgenerationtools;tworespondents
indicated that DeepTC-Enhancer was unlikely to have an effect on
their choice to use these tools.
Internal developers, much like external developers, positively
ratedtheimpactof DeepTC-Enhancer onreadability:4/6indicatea
significantincreaseand2/6indicateaminorincrease.However,they
werenotaslikelyasexternaldeveloperstoutilize DeepTC-Enhancer
should theyuse automatictest generation; with3/6 indicating they
were highly or somewhat likely to use it, 1/6 neutral and 2/6 either
highly or somewhat unlikely. Lastly, all 6 participants indicated
thatthepresenceof DeepTC-Enhancer wouldnotmakethemmore
likely to use automatic test case generation. This is mostly because
the generated tests do not match the code styles of their organiza-
tions.
4.2.2 Test Case Scenarios. We report the results of the evalua-
tionoftheproposedtestcasescenariosbyinternaldevelopersin
Table 2. Respondents rated the test case scenarios generated byTable2:Internaldevelopers‚Äôfeedbackontestcasescenarios
generated by DeepTC-Enhancer.
Criteria Rating # Resp.
Conciseness No unnecessary information 2/6
Mostly unnecessary information 4/6
Content Not missing important information 4/6
Missing some very important information 2/6
Readability Easy to read/understand 4/6
Somewhat easy to read/understand 2/6
Table 3: External and Internal developer feedback on vari-
able names suggested by DeepTC-Enhancer.
RatingExternal Internal
#(%) #
Fullyconveys intent 167 (48%) 7/18
Somewhat conveys intent 121 (35%) 8/18Does not convey intent 33 (10%) 2/18
Misleading with regard to intent 24 (7 %) 1/18
DeepTC-Enhancer favorablyintermsof readability andcontentad-
equacy,butnotintermsof conciseness.Thisissomewhatconsistent
with the external developer responses reported in Section 4.1.1.
However,internaldevelopersratedtheconcisenessofthescenarios
much lower than the external counterparts. Their concerns mir-
roredthoseofexternaldevelopers;1participantindicatedthatthey
preferredreadingtheactualsourcecodeinsteadofthesummary
commenttofigureoutwhatthetestcaseexactlydoes.Theyalso
were more specific about code quality standards for unit tests; one
developer indicated that a good test case should make it evident
whatisbeingtested;theywouldrathertheintentofthetestcase
becontainedinthecodethanthecomments.Ingeneral,internal
developerswerenotinfavorofhavecommentsthatdescribethe
functionality of the test cases. Part of the reasons for that might
be due to the fact that for the evaluation respondents were shown
single test cases as opposed to the entire test suite generated byEvoSuite. We hypothesize that developers will see more value inthe test case scenarios in the context of test case navigation. We
plan to perform such evaluation as part of future work.
4.2.3 Test Case Names. Overall, internal developers found the
methodnamessuggestedby DeepTC-Enhancer donotsuccessfully
capturetheintentofthetestcasespresentedtothem.Fourdevelop-
ersindicatedthatthesuggestednameeitherdidn‚Äôtcaptureintent
(2)orwasmisleading(2).However,theyratedthesuggestednames
favorably in terms of readability, with 5 indicating that it was easy
to read (3) or somewhat easy to read (2), with the remaining devel-
oper indicating the name to be not easy to read. This is once again
similartotheresultsoftheexternaldevelopersurveyreportedin
Section 4.1.2.
4.2.4 Variable Names. Thesurveyresultswithexternaldevelopers
for the variable names suggested by DeepTC-Enhancer are shown
inTable3.Overall,participantsreportedthatthenamescapturetheintentofthevariableusagetosomeextent83%ofthetimes,withthe
remaining 17% reporting that the variable renaming either did not
294Table4:Externalandinternaldeveloperfeedbackforuseful-
ness of features
Internal External
UsefulNot Useful Useful Not Useful
Summary 3/6 2 / 6 20 (67%) 9 (30%)
Variable 5/6 0 / 6 23 (77%) 5 (17%)
Method 5/6 0 / 6 22 (73%) 6 (20%)
capturetheintentorwasmisleading.48%ofthetimes,respondentsindicatedthatthesuggestedvariablenamefullycapturedtheintent
of its usage.
Table 3 also shows the internal developer evaluation of the vari-
ablenameson18differentinstances.Overall,theyratedthevariable
names‚Äô ability to convey the intent of their usage fairly positively:
seven variablenamesbeingratedas fullycapturingtheintent oftheir usage and eight being rated as somewhat capturing the in-
tent of the usage. Two variable names were rated as not capturing
theintentoftheirusagewhileonewasratedisbeingmisleading
withregardtointent.Ingeneral,developerswereveryenthusiastic
about the variable renaming feature. One developer indicated that
theimpactoftherenamingofthevariableswouldbemuchmore
apparent in larger test cases.
RQ2Overall,externalandinternaldevelopersreportthatthe
transformations of DeepTC-Enhancer result in a significant in-
creaseinreadabilityofautomatictestcases.Internaldevelop-ers are particularly enthusiastic about the variable renaming.
Moreover, internal developers also find test case scenarios to be
readable and content adequate, but lacking conciseness.
4.3 RQ3: What aspects of DeepTC-Enhancerdo
developers find most useful?
Table 4 shows the results from the evaluation with external de-
velopers. Respondents were asked to place the three features of
DeepTC-Enhancer (scenarios,testcasenames,andvariablenames)
in two buckets: useful and not useful. 20 (67%) respondents placed
test case scenarios in the useful bucket, while nine (30%) placed
them in the not useful bucket. Variable renaming were considered
useful by 22 (73%) respondents and not useful by 6 (20%). Lastly, 23
(77%) respondents found the test case renaming to be useful, while
5 (17%) found it to not useful.
In terms of the relative importance of these three aspects of the
proposed approach, the largest number of participants reported
test case scenarios to be the most important feature, with 12 (40%)
participantsratingitasthemostimportant,whereas6(20%)and
8 (27%) participants ranked variable and method renaming respec-
tively as the most important feature. Test case scenarios were also
the most frequently top-ranked feature rated by respondents (3) as
not being useful.
Theresultsforinternaldevelopersareincontrastwiththeresults
fromexternaldevelopers.3/6developersrankvariablerenaming
as the feature they found the most useful while another rankedit second. Method name renaming is ranked as the second most
useful feature by 3/6 developers. Lastly, internal developers did notfindtestcasescenariostobeasusefulastheexternaldevelopers;
in fact, 2/6 rated summaries as not being useful at all.
The results of the Wilcoxon test revealed that the test scenarios
are significantly ranked higher than the other features in our tool
in terms of usefulness as indicated in participants‚Äô answers ( ùëù-
value=0.05). The effect size is medium(0.39) compared to variables
names and small(0.283) compared to method names. Instead, there
is no statistical difference between the other two features, i.e., , the
usefulness (ranks) of methods and variables names.
Discussion: Weobservethatforexternaldevelopers,testcasesce-
nariosarethemostusefulenhancementofferedby DeepTC-Enhancer .
Thisiscontrarytoourfindingsforinternaldevelopers.Fromaqual-
itativeanalysisofthecommentsleftbyrespondents,wegatherthatinternaldevelopersoftenpreferself-documentingcodeoverexplicit
documentation. While this is true even for external developers, the
section of respondents that expressed this preference was in the
minority as evidenced by the results. In addition, internal develop-
ers tended to perceive the test scenarios from the perspective ofthe project being evaluated; one of them indicated that both the
coding style and thetest scenarios of the automatically generated
test presented to them did not fit their current coding quality stan-
dards.Wealsopositthatthedifferenceintheperceptionoftestcase
scenarios for internal and external developers could be, at the very
least, attributed to the familiarity the developers had with these
projects.Somesnippetsusedforinternaldeveloperswerealsoused
for the external evaluation, and given the external developers‚Äô lack
of familiarity with the code, they found test case scenarios to be
more helpful.
RQ3External developers consider the automatically gen-
erated test case scenarios as the most useful aspect of
DeepTC-Enhancer , whereas internal developers prefer the vari-
able renaming feature. We attribute the main reasons for the
different opinions to the familiarity (or lack of it) with the code
and the coding standards followed by different projects and
developers.
5 THREATS TO VALIDITY
In this section, we outline possible threats to the validity of our
study and show how we mitigated them.
Threats to construct validity concern the way in which we
set up our study. Due to the fact that our study was performed in aremotesettinginwhichparticipantscouldworkonthetasksattheirowndiscretion,wecouldnotoverseetheirbehaviours.Tominimize
potentialbiasintheparticipants‚Äôbehaviours,wehavesharedthe
experimental data with the participants using an online survey
platform,which supportthe participants(1) toperformtasks and
(2) facilitate the filling of the questionnaires. In additional, to limit
this threat, we also involved both external and internal developers,
so that the final reported results are more reliable.
Threats to internal validity concern factors that might af-
fect the casual relationship. To reduce biasing developers to thebaselines evaluated, the name of the tools used to generate the
summariesandidentifiersnameswerenotrevealedinthesurvey.
To avoid bias in the task assignment, we randomly assigned the
tasks to the participants in order to have a balanced number of
datapointsforalltreatments.Specifically,forexternaldeveloper
295surveys, participants were randomly assigned to one off the two
surveys. For internal developers surveys, we created a unique sur-
vey, even if there was more than one developer for a particular
project, we then randomly selected both test method and test class
from the automatically generated test suite. Another factor that
can influence our results is the order of assignments. However, our
results suggest similar results among participants, thus, presenting
no interaction between the treatments and the final outcome.
Threats to external validity concern the generalizability of
ourfindings,andinparticularontheevaluationof DeepTC-Enhancer .
To limit this threat, we considered different dataset to evaluate our
approach. To evaluate the quality of generated test summaries and
suggested method names, we considered the original datasets used
inthestudiesinvolvingtwobaselines[ 14,38].Wealsouseanew
dataset containing 30 top starred projects on Github to evaluate
DeepTC-Enhancer .Weplantoevaluate DeepTC-Enhancer witha
larger dataset with more complex test cases. We will also work on
improving the suggestions for test case names. Future work will
also focus on investigating how the approach helps developers fix
potential bugs [ 38], which deserve future investigations. Finally,
evenifourpopulationincludedasubstantialpartofprofessionalin-
ternal and external developers, we plan to replicate this study with
more participants in the future in order to increase the confidence
in the generalizability of our results.
Threats to conclusion validity concern the degree to which
ourconclusions about DeepTC-Enhancer arereasonable basedon
thedata. DeepTC-Enhancer generatestestsummariesandsuggests
identifier names for automatically generated test cases by Evosuite.UsingdifferentautomatictestgenerationtoolssuchasRandoop[
35]
mightleadtodifferentresults.However,weobservethatthetest
cases generated by Evosuite are not significantly different from
thosegeneratedbyotherexistingtoolsintermsofsize,structure
and coverage. We support our findings by using appropriate statis-
tical tests, i.e. the non-parametric Wilcoxon test. We also used the
Wilk-Shapiro normality test to verify whether the non-parametric
testcouldbeappliedtoourdata.Finally,weusedtheVarghaand
Delaney √Ç 12statistical test to measure the magnitude of the differ-
ences between different approaches.
6 RELATED WORK
SourceCodeSummarization .Researchershaveproposedseveral
approaches that generate summarization of software artifacts atdifferent levelsof granularityto reduceprogram comprehension
effortduringsoftwaredevelopmentandmaintenance.Atstatement
level, Gonzalezet al. develop anautomated technique to convert
JUnit assertion statements into natural language sentences [ 19]. At
method level, Sridhara et al. [ 42] propose an approach that auto-
matically generates natural language summary comments for Java
methods.Atclasslevel,Morenoetal.[ 33]presentanapproachto
generate human-readable summaries for Java classes so that devel-
opers can understand the main goal and structure of a class easily.
McBurney and McMillan [ 30] propose an approach to generate au-
tomatic source codesummaries with contextual meaning inthem
by analyzing how those methods are invoked to show why the
method exists or what role it plays in the software. To locate cross-
cutting concern code, Rastkar et al. [ 39] introduce an automatedapproach that produces a natural language summary describing
bothwhattheconcernisandhowtheconcernisimplementedso
thatdeveloperscanperformchangetasksmoreefficiently.Allthese
approaches focus on generating summaries for source code; we
targetautomaticallygeneratedtestcasesandgeneratetestscenario
rather than statement-level descriptions as in [19].
Method and Variable Renaming . Allamanis et al. [ 2] intro-
duce a neural probabilistic language model for source code thatcan suggest method names. In addition, Yonai et al. [
45] propose
anapproach Mercemtorecommendmethodnamesinsourcecode
by applying graph embedding techniques to the call graph. Both
approaches target methods whereas DeepTC-Enhancer focuses on
suggesting names for automatically generated test cases.
Raychev [ 40] build an engine called JSNICEto predict variable
names and type annotations of JavaScript programs. In addition,
Vasilescuetal.[ 43]presentedanapproach JSNAUGHTY torecoverthe
original names from minified JavaScript variable names. However,
both JSNICEand JSNAUGHTY are provided for JavaScript software
system but not automatically generated Java test cases. To the best
ofourknowledge,nootherworkfocusonenhancingthereadability
of variable names in automatically generated test cases in Java.
Improving the Readability of Test Cases . Kamimura and
Murphy [ 22] propose generating human-oriented summaries of
testcasesbasedonstaticsourcecodeanalysis.Lietal.[ 28]present
an approach UnitTestScribe that combines static analysis, natural
language processing, backward slicing, and code summarizationtechniques to generate descriptions documenting the purpose ofmethods within unit tests.
UnitTestScribe works for C# project.
Panichellaetal. [ 38]introducean approach, TestDescriber ,toau-
tomatically generates test case summaries of the portion of code
exercisedbyeachindividualtesttoimprove.Wecompareourap-
proach with the work of Panichella et al.
TogeneratedescriptivemethodnamesforJavaunittestcases,
Zhangetal.[ 46]presentanapproach, NameAssist ,thatcombines
natural-language program analysis and text generation, which can
createtestmethodnamesthatsummarizethetest‚Äôsscenarioand
the expected outcome. We considered NameAssist as a candidate
baseline but neither the dataset nor the tool is publicly available,
henceweexcludeitfromthiscomparison.Dakaetal.[ 14]introduce
an approach for automatically generated tests that can generatedescriptive names by summarizing API-level coverage goals. We
use the approach by Daka et al. as a baseline for our comparison.
Afshanetal.[ 1]usedalinguisticmodeltogenerate‚ÄòEnglish-like‚Äù
inputstrings,whicharemoreunderstandablethatrandomlygener-atedones.Ourworkcomplementsthislineofresearchasweaimat
improvingreadabilityindifferentdimensions,i.e.,documentation
and method/variable names. Our paper shows that these factorsare perceived as very important by developers. Daka et al. [
13]
proposed a domain-specific model to measure and improve the
readability of generated units test and based on human judgments.
EvoSuitealreadyincorporatesheuristicstoreducethesizeofthe
generatedtestsandthenumberofassertions[ 16].Onecouldargue
that,atthesamelevelofcoverage,shortertestsareeasiertovalidateandinspectmanually,reducingtheoraclecost[
36].However,these
studiesfocusonthestructure,size,andcomplexityofthegenerated
tests. Instead, we focus on natural language documentation (not
296included in the model by Daka et al. [ 13]) and the quality of the
identifiers (that remain obfuscated).
As indicated by Lin et al. [ 29], the poor quality of the identifiers
in test code is widespread, and it is more severe in generated tests
compared to the manually-written counterpart. Our tool applies
multiple strategies to address, among others, this open issue.
7 CONCLUSIONS AND FUTURE WORK
We propose DeepTC-Enhancer : a hybrid deep learning and tem-
platebasedapproachtoimprovethereadabilityofautomatically
generatedtestcases. DeepTC-Enhancer isthefirstapproachthaten-
hancesboththedocumentationandcodeaspectsofthetests.Tothis
end, DeepTC-Enhancer generatesleading testscase summariesin
theformoftestcasescenarioswhichoutlinetheactionsperformed
inthetest.Toenhancetheautomaticallygeneratedidentifiersof
tests, DeepTC-Enhancer adaptsadeeplearningbasedextremecode
summarizationapproachtogeneratetestcasenamesandvariable
names.
With two empirical evaluations involving 30 external and six
internal developers, we evaluate all aspects of the transformations
proposed by DeepTC-Enhancer and compare it to existing base-
lines.Results showthat summariesgenerated usingtheproposed
approachsignificantlyoutperformthebaseline,andthetestcase
renaming performs similar to the baseline. The variable renaming
feature in the context of automatic tests is first of its kind and thus
isnotcomparedtoabaseline.Thisisthefeaturethatwaspreferred
byinternaldevelopers,whileexternaldevelopersrankedthetest
casescenariosasamoreimportantfeature.Overall,bothinternal
and external developers report a significant improvement in the
readability of generated tests after the enhancement applied by
DeepTC-Enhancer .Lastly,themajorityofparticipantsindicatethat
theexistenceof DeepTC-Enhancer increasesthelikelihoodofusing
tools for automatic test case generation in their projects.
In the future, we plan to evaluate DeepTC-Enhancer in the con-
textofdifferentmaintenancetasksthatinvolvetestcasenavigation.
Moreover,fromtheevaluationwithdevelopers,weseethatthere
is a need to improve the suggestions for test case names.
REFERENCES
[1]S.Afshan,P.McMinn,andM.Stevenson.2013. EvolvingReadableStringTest
InputsUsingaNaturalLanguageModeltoReduceHumanOracleCost.In Pro-
ceedings International Conference on Software Testing, Verification and Validation
(ICST). IEEE, 352‚Äì361.
[2]MiltiadisAllamanis,EarlTBarr,ChristianBird,andCharlesSutton.2015. Sug-
gestingaccuratemethodandclassnames.In Proceedingsofthe201510thJoint
Meeting on Foundations of Software Engineering. 38‚Äì49.
[3]MiltiadisAllamanis,MarcBrockschmidt,andMahmoudKhademi.2017. Learning
to represent programs with graphs. arXiv preprint arXiv:1711.00740 (2017).
[4]Miltiadis Allamanis, Hao Peng, and Charles Sutton. 2016. A convolutional at-
tention network for extreme summarization of source code. In International
conference on machine learning. 2091‚Äì2100.
[5]M Moein Almasi, Hadi Hemmati, Gordon Fraser, Andrea Arcuri, and Janis Bene-
felds.2017. Anindustrialevaluationofunittestgeneration:Findingrealfaults
in a financial application. In 2017 IEEE/ACM 39th International Conference on
SoftwareEngineering:SoftwareEngineeringinPracticeTrack(ICSE-SEIP).IEEE,
263‚Äì272.
[6]Uri Alon, Shaked Brody, Omer Levy, and Eran Yahav. 2018. code2seq: Gen-erating sequences from structured representations of code. arXiv preprint
arXiv:1808.01400 (2018).
[7]Uri Alon,Meital Zilberstein, OmerLevy, and EranYahav.2018. Ageneral path-basedrepresentationforpredictingprogramproperties. ACMSIGPLANNotices
53, 4 (2018), 404‚Äì419.[8]Uri Alon, Meital Zilberstein, Omer Levy, and Eran Yahav. 2019. code2vec: Learn-
ing distributed representations of code. Proceedings of the ACM on Programming
Languages 3, POPL (2019), 1‚Äì29.
[9] Paul Ammann and Jeff Offutt. 2016. Introduction to software testing. Cambridge
University Press.
[10]Saswat Anand, Edmund K Burke, Tsong Yueh Chen, John Clark, Myra B Cohen,
Wolfgang Grieskamp, Mark Harman, Mary Jean Harrold, Phil Mcminn, Antonia
Bertolino,etal .2013. Anorchestratedsurveyofmethodologiesforautomated
software test case generation. Journal of Systems and Software 86, 8 (2013),
1978‚Äì2001.
[11]Moritz Beller, Georgios Gousios,Annibale Panichella, andAndy Zaidman. 2015.
When, how, and why developers (do not) test in their IDEs. In Proceedings of the
2015 10th Joint Meeting on Foundations of Software Engineering. 179‚Äì190.
[12] Mariano Ceccato, Alessandro Marchetto, Leonardo Mariani, Cu D Nguyen, and
Paolo Tonella. 2015. Do automatically generated test cases make debuggingeasier? an experimental assessment of debugging effectiveness and efficiency.
ACMTransactionsonSoftwareEngineeringandMethodology(TOSEM) 25,1(2015),
1‚Äì38.
[13]Ermira Daka, Jos√© Campos, Gordon Fraser, Jonathan Dorn, and Westley Weimer.
2015. Modeling readability to improve unittests. In Proceedings of the2015 10th
Joint Meeting on Foundations of Software Engineering. 107‚Äì118.
[14]Ermira Daka, Jos√© Miguel Rojas, and Gordon Fraser. 2017. Generating unit tests
withdescriptivenamesor:Wouldyounameyourchildrenthing1andthing2?.
InProceedings of the 26th ACM SIGSOFT International Symposium on Software
Testing and Analysis. 57‚Äì67.
[15]XavierDevroey,SebastianoPanichella,andAlessioGambi.2020.JavaUnitTesting
Tool Competition - Eighth Round. In IEEE/ACM 42nd International Conference
on Software Engineering Workshops (ICSEW‚Äô20). Seoul, Republic of Korea. https:
//doi.org/10.1145/3387940.3392265
[16]GordonFraserandAndreaArcuri.2011. Evosuite:automatictestsuitegenerationfor object-oriented software. In Proceedings of the19th ACM SIGSOFT symposium
andthe13thEuropeanconferenceonFoundationsofsoftwareengineering.416‚Äì419.
[17]Gordon Fraser and Andrea Arcuri. 2014. A large-scale evaluation of automated
unittestgenerationusingevosuite. ACMTransactionsonSoftwareEngineering
and Methodology (TOSEM) 24, 2 (2014), 1‚Äì42.
[18]GordonFraserandAndreaArcuri.2015. 1600faultsin100projects:automatically
findingfaultswhileachievinghighcoveragewithevosuite. Empiricalsoftware
engineering 20, 3 (2015), 611‚Äì639.
[19]DanielleGonzalez,SuzannePrentice,andMehdiMirakhorli.2018. Afine-grainedapproach for automated conversion of JUnit assertions to English. In Proceedings
of the 4th ACM SIGSOFT International Workshop on NLP for Software Engineering.
14‚Äì17.
[20]RobertJ.GrissomandJohnJ.Kim.2005. Effectsizesforresearch:Abroadpractical
approach (2nd edition ed.). Lawrence Earlbaum Associates.
[21]Srinivasan Iyer, Ioannis Konstas, Alvin Cheung, and Luke Zettlemoyer. 2016.
Summarizingsourcecodeusinganeuralattentionmodel.In Proceedingsofthe
54thAnnualMeetingoftheAssociationforComputationalLinguistics(Volume1:
Long Papers). 2073‚Äì2083.
[22]Manabu Kamimura and Gail C Murphy. 2013. Towards generating human-oriented summaries of unit test cases. In 2013 21st International Conference on
Program Comprehension (ICPC). IEEE, 215‚Äì218.
[23]Rafael-Michael Karampatsis, Hlib Babii, Romain Robbes, Charles Sutton, and
Andrea Janes. 2020. Big Code!= Big Vocabulary: Open-Vocabulary Models for
Source Code. arXiv preprint arXiv:2003.07914 (2020).
[24]Maria Kechagia, Xavier Devroey, Annibale Panichella, Georgios Gousios, and
ArievanDeursen.2019. EffectiveandefficientAPImisusedetectionviaexception
propagationandsearch-based testing.In Proceedingsofthe28th ACMSIGSOFT
International Symposium on Software Testing and Analysis. 192‚Äì203.
[25]Fitsum Kifetew, Xavier Devroey, and Urko Rueda. 2019. Java unit testing tool
competition-seventhround.In 2019IEEE/ACM12thInternationalWorkshopon
Search-Based Software Testing (SBST). IEEE, 15‚Äì20.
[26]TakuKudoandJohnRichardson.2018. Sentencepiece:Asimpleandlanguage
independentsubwordtokenizeranddetokenizerforneuraltextprocessing. arXiv
preprint arXiv:1808.06226 (2018).
[27]Alexander LeClair, Siyuan Jiang, and Collin McMillan. 2019. A neural modelfor generating natural language summaries of program subroutines. In 2019
IEEE/ACM 41st International Conference on Software Engineering (ICSE). IEEE,
795‚Äì806.
[28]BoyangLi,ChristopherVendome,MarioLinares-V√°squez,DenysPoshyvanyk,
and Nicholas A Kraft. 2016. Automatically documenting unit test cases.In 2016
IEEEinternationalconferenceonsoftwaretesting,verificationandvalidation(ICST).
IEEE, 341‚Äì352.
[29]BinLin,CsabaNagy,GabrieleBavota,AndrianMarcus,andMicheleLanza.[n.d.].
On the Quality of Identifiers in Test Code. In 2019 19th International Working
Conference on Source Code Analysis and Manipulation (SCAM). IEEE, 204‚Äì215.
[30]P. W. McBurney and C. McMillan. 2016. Automatic Source Code Summarization
of Context for Java Methods. IEEE Transactions on Software Engineering 42, 2
(2016), 103‚Äì119.
297[31]PhilMcMinn.2004.Search-basedsoftwaretestdatageneration:asurvey. Software
testing, Verification and reliability 14, 2 (2004), 105‚Äì156.
[32]UrkoRuedaMolina,FitsumKifetew,andAnnibalePanichella.2018. Javaunittest-
ing tool competition-sixth round. In 2018 IEEE/ACM 11th International Workshop
on Search-Based Software Testing (SBST). IEEE, 22‚Äì29.
[33]Laura Moreno, Jairo Aponte, Giriprasad Sridhara, Andrian Marcus, Lori Pollock,
andKVijay-Shanker.2013. Automaticgenerationofnaturallanguagesummaries
for java classes. In 2013 21st International Conference on Program Comprehension
(ICPC). IEEE, 23‚Äì32.
[34]NuthanMunaiah,StevenKroh,CraigCabrey,andMeiyappanNagappan.2017.
CuratingGitHubforengineeredsoftwareprojects. EmpiricalSoftwareEngineering
22, 6 (2017), 3219‚Äì3253.
[35]Carlos Pacheco and Michael D Ernst. 2007. Randoop: feedback-directed random
testingforJava.In Companiontothe22ndACMSIGPLANconferenceonObject-
oriented programming systems and applications companion. 815‚Äì816.
[36]Annibale Panichella, Fitsum Meshesha Kifetew, and Paolo Tonella. 2017. Au-
tomated test case generation as a many-objective optimisation problem with
dynamic selection of the targets. IEEE Transactions on Software Engineering 44, 2
(2017), 122‚Äì158.
[37]Annibale Panichella and Urko Rueda Molina. 2017. Java unit testing tool
competition-fifthround.In 2017IEEE/ACM10thInternationalWorkshoponSearch-
Based Software Testing (SBST). IEEE, 32‚Äì38.
[38]SebastianoPanichella,AnnibalePanichella,MoritzBeller,AndyZaidman,and
HaraldCGall.2016.Theimpactoftestcasesummariesonbugfixingperformance:
An empirical investigation. In Proceedings of the 38th International Conference onSoftware Engineering. 547‚Äì558.
[39]Sarah Rastkar, Gail C Murphy, and Alexander WJ Bradley. 2011. Generating
natural language summaries for crosscutting source code concerns. In 2011 27th
IEEE International Conference on Software Maintenance (ICSM). IEEE, 103‚Äì112.
[40]Veselin Raychev, Martin Vechev, and Andreas Krause. 2015. Predicting program
properties from" big code". ACM SIGPLAN Notices 50, 1 (2015), 111‚Äì124.
[41]GiriprasadSridhara.2012. AutomaticGenerationofDescriptiveSummaryCom-
ments for Methods in Object-oriented Programs. University of Delaware.
[42]Giriprasad Sridhara, Emily Hill, Divya Muppaneni, Lori Pollock, and K Vijay-
Shanker.2010. Towardsautomaticallygeneratingsummarycommentsforjava
methods. In Proceedings of the IEEE/ACM international conference on Automated
software engineering. 43‚Äì52.
[43]BogdanVasilescu,CaseyCasalnuovo,andPremkumarDevanbu.2017.Recoveringclear,naturalidentifiersfromobfuscatedJSnames.In Proceedingsofthe201711th
Joint Meeting on Foundations of Software Engineering. 683‚Äì693.
[44]Yao Wan, Zhou Zhao, Min Yang, Guandong Xu, Haochao Ying, Jian Wu, and
PhilipSYu.2018. Improvingautomaticsourcecodesummarizationviadeeprein-forcementlearning.In Proceedingsofthe33rdACM/IEEEInternationalConference
on Automated Software Engineering. 397‚Äì407.
[45]H. Yonai, Y. Hayase, and H. Kitagawa. 2019. Mercem: Method Name Recom-
mendationBasedonCallGraphEmbedding.In 201926thAsia-PacificSoftware
Engineering Conference (APSEC). 134‚Äì141.
[46]Benwen Zhang, Emily Hill, and James Clause. 2016. Towards automatically
generating descriptive names for unit tests. In Proceedings of the 31st IEEE/ACM
International Conference on Automated Software Engineering. 625‚Äì636.
298