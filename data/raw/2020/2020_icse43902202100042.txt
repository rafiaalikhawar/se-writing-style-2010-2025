Operation is the hardest teacher: estimating
DNN accuracy looking for mispredictions
Antonio Guerriero, Roberto Pietrantuono, Stefano Russo
DIETI, Universit `a degli Studi di Napoli Federico II
Via Claudio 21, 80125 - Napoli, Italy
fantonio.guerriero, roberto.pietrantuono, stefano.russo g@unina.it
Abstract ‚ÄîDeep Neural Networks (DNN) are typically tested
for accuracy relying on a set of unlabelled real world data
(operational dataset), from which a subset is selected, manually
labelled and used as test suite. This subset is required to be
small (due to manual labelling cost) yet to faithfully represent
the operational context, with the resulting test suite containing
roughly the same proportion of examples causing misprediction
(i.e., failing test cases) as the operational dataset.
However, while testing to estimate accuracy, it is desirable
to also learn as much as possible from the failing tests in the
operational dataset, since they inform about possible bugs of the
DNN. A smart sampling strategy may allow to intentionally in-
clude in the test suite many examples causing misprediction, thus
providing this way more valuable inputs for DNN improvement
while preserving the ability to get trustworthy unbiased estimates.
This paper presents a test selection technique (DeepEST) that
actively looks for failing test cases in the operational dataset of a
DNN, with the goal of assessing the DNN expected accuracy by a
small and ‚Äúinformative‚Äù test suite (namely with a high number of
mispredictions) for subsequent DNN improvement. Experiments
with Ô¨Åve subjects, combining four DNN models and three
datasets, are described. The results show that DeepEST provides
DNN accuracy estimates with precision close to (and often better
than) those of existing sampling-based DNN testing techniques,
while detecting from 5 to 30 times more mispredictions, with the
same test suite size.
Index Terms ‚ÄîSoftware testing, ArtiÔ¨Åcial neural networks
I. I NTRODUCTION
Deep Neural Networks (DNN) are today integral part of
many applications, including safety-critical software systems
such as in the medical [1] and autonomous driving domains
[2]. Testing is a crucial activity in the development of such
systems, for both quality/safety reasons to avoid DNN-caused
catastrophic failures [3], [4], and for cost as well ‚Äì very large
samples may be needed to reliably test a DNN [5], [6].
A signiÔ¨Åcant research effort is currently being put on DNN
testing. A primary goal is to Ô¨Ånd adversarial examples causing
mispredictions , namely to expose as many failing behaviours
as possible [7]‚Äì[11]. Several structural coverage criteria have
been proposed to drive the automated generation of test
inputs and assess the test adequacy ‚Äì neuron coverage [7], k-
multisection neuron coverage, neuron boundary coverage [8],
combinatorial coverage [10]. It has been argued that these
criteria may be misleading, because of the low correlation
between the number of misclassiÔ¨Åed inputs in a test set and its
This work has been supported by the project COSMIC of UNINA DIETI.coverage [12]. Wu et al. [13] and Kim et al. [14] recently con-
sidered discrepancy measures between the training/validation
data and test data, to improve fault detection and to have
coverage criteria better correlated to failure-inducing inputs.
The output of this type of failure-Ô¨Ånding testing (and
then debugging) process is an improved DNN, with higher
accuracy. This resembles what is called debug testing in
the traditional testing literature [15]. Beside differences in
testing DNN-based and conventional software (e.g., the oracle
deÔ¨Ånition), which make DNN testing problematic, a further
issue is that the so-obtained testing results are not necessarily
related to the accuracy actually experienced in operation. In
fact, testing data may be not representative of the actual
operational context . This may happen when test data are
generated artiÔ¨Åcially (like in adversarial examples generation)
or they differ signiÔ¨Åcantly from input observed in the Ô¨Åeld. The
number of mispredictions and/or the coverage achieved give
only an ‚Äúindirect‚Äù (and, for what said, inaccurate) measure
of the expected accuracy in operation, and ultimately of the
conÔ¨Ådence that can be placed in the DNN.
A less investigated research path is testing a DNN with the
explicit goal of providing a statistical estimate of its expected
accuracy in the operational context. In software reliability
engineering, this is a well established practice known as
operational testing [16]. The objective is to assess how well
a DNN will perform in the intended context using a small yet
effective amount of test data. With a cost-effective accuracy
estimate, testers can establish a threshold-based release crite-
rion and correct or tune their artiÔ¨Åcial networks (e.g., adjust
the DNN structure or hyper-parameters) until the criterion is
met. The reference scenario is the following: a DNN model
is trained with a set of data ( training dataset ) and is meant
to operate in a given context ( operational context ). To test the
model, an arbitrarily large set of operational data is available
(operational dataset ) containing examples whose correct label
is unknown: the goal of the tester is to select a small subset
of the operational data, to be manually labelled and used as
test cases to estimate the accuracy of the model in operation.
Due to manual labelling, testers typically look for a trade-off
between a good estimate and the labelling cost. This problem
has been recently faced by Li et al. [17]. Their selection
criterion is to minimise cross-entropy between selected test
data and the whole set of unlabelled operational data, so as to
have a sample of tests representative of the operational context.
3482021 IEEE/ACM 43rd International Conference on Software Engineering (ICSE)
1558-1225/21/$31.00 ¬©2021 IEEE
DOI 10.1109/ICSE43902.2021.00042
As this approach does not explicitly look for failing examples,
it does not give much room for improving the DNN accuracy.
From this viewpoint, many tests selected are useless, as they
are non-failing examples that serve only the purpose of having
highly-conÔ¨Ådent estimates.
Given the above, we either have a test suite exposing failures
but not representative (adversarial-like testing), or representa-
tive tests but few failures exposed (operational testing). This
paper targets the drawbacks of both strategies together. We
present DeepEST (Deep neural networks Enhanced Sampler
for operational Testing), an operational testing method for
DNN that builds test suites from an operational dataset so
as to provide a close and efÔ¨Åcient estimate of the expected
accuracy and to Ô¨Ånd, at the same time, a high number of
failing examples. Paraphrasing a famous aphorism by O. Wilde
(‚ÄúExperience is the hardest kind of teacher. It gives you the
testÔ¨Årst and the lesson afterward.‚Äù), DeepEST aims to sample,
from operational data, many failing tests to learn from while
building a set of tests suited for the DNN accuracy estimate.
With respect to pure operational testing, DeepEST is ex-
pected to provide DNN accuracy estimates that are more
precise and more efÔ¨Åcient (for number of tests required),
plus the practical advantage of enabling debug-testing-like
scenarios (improving accuracy through debugging/re-training).
With respect to adversarial-like testing, DeepEST is able to
provide accuracy estimates, enabling evaluation of alternative
designs or DNN Ô¨Åne tuning, and establishing a release crite-
rion directly related to what will be observed in operation.
Experiments with various DNN and datasets are presented,
which assess DeepEST effectiveness, sensitivity to the number
of tests to select from the operational dataset, and dependency
on the dataset. As DeepEST can exploit various types of
auxiliary information to select tests, the experimental study
considers four variants. The results show that all the variants
produce accuracy estimates similar to those of the state-of-
the-art technique, while detecting from 5 to 30 times more
mispredictions, with the same sample size.
The paper is structured as follows. Section II introdcues
sampling-based operational testing concepts used by Deep-
EST. Section III describes the DeepEST algorithm. Sections
IV and V report the experimentation. Section VI discusses
related work. Section VII concludes the paper.
II. S AMPLING -BASED OPERATIONAL TESTING OF DNN S
A. Operational testing of DNNs
In the traditional testing literature, operational testing refers
to the family of techniques that use an operational proÔ¨Åle
to test a system to estimate its expected reliability (i.e.,
probability of not failing) in operation. Likewise, the primary
goal of DNN operational testing is to estimate the expected
accuracy (i.e., probability of not having mispredictions) in a
given operational context [17].1Two main challenges arise.
Data skew . The idea of operational testing is that testing
should not just care about exposing possible failures, but
1We use the terms misprediction and failure interchangeably for DNN.should be able to spot those failure-causing inputs that are
more likely to occur in operation. In case of a relevant
mismatch between the pre-release test data and the post-release
context, the system could be stimulated in operation with
inputs never seen during testing, with unexpected failures.
Data skew is a concern for DNN, more than for traditional
software systems. These are expected to work on a range of
input data given to functionalities, and can be tested on a
small carefully-selected sample of input data (e.g., obtained
by input space partitioning). DNN are data-driven by nature;
they are constructed around a training dataset, and generalising
beyond what observed during training is hard. This data-driven
approach is governed by a statistical process and, due also of
the black-box nature of DNN, it is tricky to identify classes of
inputs that homogeneously represent the expected behaviour
in operation. For instance, white-box partitioning has been
shown to be not clearly correlated to the failing behaviour [12].
Thus, a drift of the post-release operational context from the
pre-release testing context is more likely to cause unexpected
failures compared to traditional software.
The imitation bias of operational testing. The operational
context drift is just a triggering condition for unexpected
failures at runtime. The problem occurs because of the way
in which operational testing is conducted. Operational testing
selects a small data sample that can accurately represent the
population; however, the mere imitation of the expected input
can be inefÔ¨Åcient, especially in highly reliable systems, be-
cause many failure-free tests are executed to get an acceptable
estimate. A representative sample would roughly contain the
same proportion of examples causing misprediction as the
operational dataset. There are two problems with this: Ô¨Årst,
in highly accurate DNNs, the number of examples causing
mispredictions is low, thus requiring other types of testing
activities dedicated to detect mispredictions (e.g., through
adversarial-like techniques) to possibly improve the accuracy.
Second, just mimicking the expected usage is Ô¨Åne from the
estimation point of view as long as the imitation is faithful.
If this is not the case, the risk of overestimation increases: if
we only aim at having a representative sample of tests, the
actual experienced accuracy may be signiÔ¨Åcantly smaller than
the estimated one if the operational context drifts, because of
the occurrence of unexpected mispredictions.
Thus, a conventional approach for DNN operational testing
allows to obtain the desired accuracy estimate but, since it
reveals few mispredictions, can be ineffective (leading to
wrong estimates) and inefÔ¨Åcient (requiring further separate
testing activities to detect mispredictions). Recent results in
operational testing for traditional software show that exposing
failures and estimating reliability are not contrasting objectives
[18]. A strategy that actively looks for failures (rather than just
mimicking the expected usage) can lead to accurate and stable
estimates and, at the same time, expose many failures. Our
aim is exactly to spot failing examples in a DNN operational
dataset, while preserving the ability to yield effective (small
error) and efÔ¨Åcient (low variance) estimates.
349B. Sampling-based testing
DeepEST uses statistical sampling, a natural way to cope
with estimation problems: it serves to design sampling plans
tailored for a population under study, providing effective and
efÔ¨Åcient estimators. In sampling-based testing [19], the sample
is the set of ntest casesT=ft1;:::;tng, having a binary
outcome (pass/fail). Test outcomes are a series of independent
Bernoulli random variables ztisuch thatzti=1if the DNN
predicts the correct label for ti,zti=0otherwise. The parameter
of our interest is the DNN accuracy ; we aim to an estimate ^
with two desirable properties: unbiasedness ‚Äì i.e., the expec-
tation of the estimate E[^]should be equal to the true value 
- and efÔ¨Åciency ‚Äì for the given the sample size, the variance
of^should be as low as possible (for a highly conÔ¨Ådent,
stable estimate). The probability that zti= 1 corresponds to
the true (unknown) proportion: =PN
t=1zti
N, withNbeing
the population size (i.e., the size of the operational dataset).
Simple random sampling with replacement (SRSWR) is the
baseline approach: an unbiased estimator of is the observed
proportion of correct predictions over the number of trials n:
^SRSWR =Pn
t=1zti
n: (1)
Having assumed independent variables, the variance of ^is:
V(^SRSWR ) =(1 )
n: (2)
An improvement is represented by simple random sampling
without replacement (SRSWOR), namely, the same test case
is not selected twice: this reduces the variance to:
V(^SRSWOR ) =N n
N 1(1 )
n: (3)
While SRS keeps the mathematical treatment simple, it is
unable to exploit additional information a tester might have.
Exploiting auxiliary information to modify the sampling
scheme is what is done in sampling theory to get more efÔ¨Åcient
estimators [20]. The sampling is made proportionally to an
auxiliary observable variable assumed to be related to the
(unknown) quantity to estimate; the estimator is then adjusted
to account for the non-uniform sampling, so as to preserve
unbiasedness. For instance, stratiÔ¨Åed sampling is a strategy
which uses knowledge about which sample units are expected
to have homogeneous values, and selects units contributing
more to lower the estimate‚Äôs variance.
Liet al. [17] present two sampling strategies for DNN,
which are, to our knowledge, the only attempt to DNN op-
erational testing: ConÔ¨Ådence-based StratiÔ¨Åed Sampling (CSS)
and Cross Entropy-based Sampling (CES) ‚Äì the latter being the
authors‚Äô proposal. Both strategies exploit auxiliary information
to drive the sampling task.
In CSS, sampling is proportional to the conÔ¨Ådence value
provided by classiÔ¨Åers when predicting a label: examples with
higher conÔ¨Ådence are more likely to be selected as part of
the test suite. This works well when the classiÔ¨Åer is reliable,
namely if examples with higher conÔ¨Ådence are actually thosefor which the prediction is more likely to be correct (in other
words, the model is perfectly trained for the operation context).
Whenever the operation context drifts from the training one,
CSS exhibits poor performance.
CES attempts to overcome this limitation by using the
output of the mneurons in the last hidden layer in the last
hidden layer, assumed to be more robust to the operation
context drift. It builds the test suite trying to minimize the
average cross-entropy between the probability distribution of
them-dimensional representation of the output of neurons
computed on the operational dataset and on the selected tests.
To pursue the double objective of sampling cases causing
mispredictions and estimating accuracy efÔ¨Åciently, DeepEST
adopts an adaptive and without-replacement sampling algo-
rithm, described in Section III.
C. Auxiliary information
To look for mispredictions, the auxiliary information lever-
aged by DeepEST adaptive sampling should represent the
belief about some factor(s) related to the model‚Äôs (in)accuracy.
We consider two auxiliary variables, related to somehow
opposite sources of information: the conÔ¨Ådence value and
thedistance between the operational dataset example and the
training dataset. The latter is based on the result by Kim
et al. [14] that inputs more ‚Äúdistant‚Äù from training data are
more likely to cause misprediction ‚Äì they show that distance
is correlated to mispredictions, which is what we look for.
We borrow their distance metrics Distance-based Surprise
Adequacy (DSA) and Likelihood-based Surprise Adequacy
(LSA). These are computed by the Activation Trace (AT),
namely a vector of activation values of each neuron of a certain
layer corresponding to a certain input; we compute AT with
reference to the last DNN activation layer. LSA is deÔ¨Åned
as the negative log of density (computed via Kernel Density
Estimation). DSA is deÔ¨Åned as the Euclidean distance between
the AT of a new input and ATs observed during training. The
variant using conÔ¨Ådence is DeepEST CS; the variants using the
distance metrics are DeepEST DSA and DeepEST LSA.
Performance of an auxiliary variable can strongly depend on
the DNN and on the training/operation dataset: for instance,
for a distance metric the belief that examples far from the one
in the training set are more likely to cause a misprediction
is not an absolute truth. In particular, if an example is very
similar to many others in the training set according to the
distance metric, but has a different label, distance will be not
a good metric to select it. Similarly, the conÔ¨Ådence is a good
proxy ifthe DNN is well trained for the operational context. In
general, relying on a single auxiliary variable may work well
in some settings and bad in others. Combining multiple beliefs
is a choice that is expected to improve the stability of results
across multiple settings. Based on this, we deÔ¨Åne a further
variant of our algorithm, named DeepEST C, considering as
auxiliary variable the combination of conÔ¨Ådence and distance:
P=Pc(1 Pd) (4)
350wherePcis the conÔ¨Ådence value, Pdis the DSA value nor-
malized [0, 1]2andPis probability of correct prediction. The
intuition behind is that the probability of a correct prediction is
related to both the conÔ¨Ådence of the DNN and the drift of the
example from what seen during training. In fact, Pcis related
to the probability that the DNN does a correct prediction
according to what seen during training (in other words: it
is the probability of correct prediction with perfect training);
Pdis a proxy for the probability of wrong prediction related
to how far the example is from what seen during training ‚Äì
hence due to the imperfection of training. If conÔ¨Ådence Pcis
high andthe example is close to the training dataset (i.e., Pd
is small), there is a high chance of correct prediction.
III. T HEDEEPEST ALGORITHM
Both CES and CSS select a sample of tests representative of
the operational context. DeepEST takes a different approach,
favouring the sampling of failing (namely, misprediction-
causing ) tests, then used to estimate the DNN accuracy.
While the estimation ability demands for probabilistic (hence
‚Äúsampling-based‚Äù) selection of examples, the very idea of
DeepEST is that, since mispredictions are usually rare com-
pared to correct examples, looking for failing tests is well
handled by adaptive sampling [21]: the examples progressively
selected may give hints about the probability of Ô¨Ånding other
failing examples, so as to adapt the sampling process ac-
cordingly. Given a samples size, adaptive sampling implicitly
assumes that inputs of interest are not uniformly spread across
the input space, and adopts a disproportional selection to spot
them, counterbalanced by the estimator to preserve unbiased-
ness. In software testing, this is expected to give smaller
variance compared to conventional sampling (e.g., SRS), since
the failing inputs are usually not uniformly distributed. The
DeepEST sampling algorithm is inspired to the Adaptive Web
Sampling [22], a Ô¨Çexible design for sampling rare populations.
The above auxiliary variables are used to deÔ¨Åne a weight
wi;jbetween any pair of examples iandjof the operational
dataset, used to explore the example space adaptively. For
instance, let us assume to use the normalised DSA distance :
if the example ihas distance Pdi(representing the belief that
icauses a misprediction), and a pre-deÔ¨Åned threshold is
exceeded (i.e., ihas a sufÔ¨Åciently high distance compared to
the others), then all the wi;jvalues (8jof the operational
dataset) are set to their distance Pdj; otherwise wi;j= 0.
This way, a strong-enough belief about example icausing
misprediction entails the activation of all the weights toward
i. The latter are used, as explained hereafter, for sampling,
and makes the algorithm follow the distance criterion to spot
potential clusters of failing examples.
The DeepEST sampling strategy is sketched in Algorithm
1. Assuming nexamples to be selected from the operational
dataset as test cases, the algorithm selects and executes one
test case per step. The Ô¨Årst input is selected by simple random
2In DeepEST C, DSA is preferred to LSA since it has been shown to
have better performance for the deeper layer [14].Algorithm 1: DeepEST adaptive sampling
Data:OD: operational dataset; i: current sample; T: test
suite;n: number of tests; r: probability of WBS
1T=;;
2i=SRS sampling(OD) ;// first sample by SRS
3OD=ODni;// remove sample from dataset
4T=T[i;// add to suite
5y1=labelling and test execution(i) ;
6fork= 2;k<=n;k++ do
7rs=random (0;1);
8 ifrs<r then
9i=WBS sampling(OD); OD=ODni;
10 else
11i=SRS sampling(OD); OD=ODni;
12T=T[i;
13yi=labelling and test execution(i) ;
14zk=Equation 6;
15^=Equation 7;// compute final estimate
sampling, namely, initially all examples have equal probability
of being selected. Then, one of two sampling schemes is
used to select next test: weight-based sampling (WBS), or
simple random sampling (SRS). Example iis selected from
the operational dataset at step kwith probability qk;igiven by:
qk;i=rP
j2skwi;jP
h=2sk;j2skwh;j+ (1 r)1
N nsk(5)
where:
r: probability of using WBS (hence, probability of using
SRS = 1-r;ris set to 0.8 in our implementation);
sk: current sample (all examples selected up to step k);
wi;j: weight relating example jinskto examplei;
nsk: size of the current sample sk;
N: size of the operational dataset.
For both WBS and SRS, the selection is without replacement .3
WBS selects an example iproportionally to the sum of weights
wi;jof already selected examples toward i‚Äì the chance
of takingidepends on the current sample, favouring the
identiÔ¨Åcation of clusters of failing examples ifthe auxiliary
variable (hence, wi;j) is well-correlated with mispredictions.
As this is not always the case, the WBS ‚Äúdepth‚Äù exploration is
balanced by SRS, chosen with probability ( 1-r), for a breadth
exploration of the example space. This diversiÔ¨Åcation in the
search is useful to escape from unproductive cluster searches.
The steps are repeated until the testing budget nNis over.
At step 1, the probability that a randomly selected example
will cause a misprediction is estimated as the outcome y1of
the Ô¨Årst test (1 in case of misprediction, 0 otherwise).
3Without replacement sampling schemes generally give smaller variance
than their with replacement counterpart on the same sample size [20].
351TABLE I: Experimental DNN and datasets
Subject Training Test True
DNN Dataset Classes set size set size accuracy
S1 CN5MNIST 10 60,000 10,0000.9905
S2 LeNet5 0.9868
S3 CN12CIFAR10 10 50,000 10,0000.8066
S4 VGG16 0.9359
S5 VGG16 CIFAR100 100 50,000 10,000 0.7048
At stepk>1, examplei, whose outcome is yi, is selected
with probability qk;iaccording to Eq. 5, and the estimator of
the misprediction probability is that by Hansen-Hurwitz [23]:
zk=1
N(X
j2skyj+yi
qk;i) (6)
where theyjvalues are the outcome of the tests already
selected.zkis an unbiased estimator of the expected mispre-
diction probability at step k; the Ô¨Ånal estimator of the expected
accuracy of the DNN is 1 minus the average of the zkvalues:
^= 1 1
n(y1+nX
k=2zk) (7)
wherenis the number of tests run.
IV. E VALUATION
A. Experimental subjects
The four variants of DeepEST are evaluated against the
SRSWR scheme as baseline and the mentioned state-of-the-
art technique CES. Five experiments are conducted with four
DNN models and three popular datasets.
The datasets are MNIST, a dataset of handwritten digits
[24]; CIFAR10, for image processing systems; and CIFAR100,
similar to the previous one but with 100 classes [25]. The cho-
sen DNN models are ConvNet5 (here simply CN5) and LeNet5
for MNIST classiÔ¨Åcation; ConvNet12 (simply, CN12) and
VGG16 for CIFAR10 classiÔ¨Åcation; VGG16 for CIFAR100.4
Table I lists the Ô¨Åve subjects (DNN-dataset pairs); the true
accuracy is in the last column.
B. Research questions and experiment design
The evaluation answers the following research questions.
RQ1: Effectiveness .How does DeepEST perform in Ô¨Ånding
inputs causing misprediction (i.e., failing examples) and si-
multaneously estimating a DNN operational accuracy?
To gauge DeepEST ability to provide effective DNN ac-
curacy estimates with few examples, while spotting a high
number of failing inputs, we set to 200 the number of tests
to select (then varied to answer RQ2) and repeat 30 times the
execution of the 6 compared techniques on the 5 subjects.
As for evaluation metrics, we compute:
4CN5 and CN12 are calibrated in the same way as Li et al. [17]; LeNet5
is calibrated as Kim et al. [14]; for the VGG16 network we considered the
weights at https://github.com/geifmany/cifar-vgg.The accuracy ^iat thei-th repetition, and then com-
pute the Mean Squared Error (MSE) as MSE (^) =
1
30P30
i=1(^i )2, whereis the true operational accu-
racy. Note that for unbiased estimators, MSE and variance
can be considered indistinguishable. In fact, MSE =
Variance +Bias2andBias(^) =E[^] = 0.
The precision of the estimator is: (^)=1
MSE (^), and the
relative precision (or relative efÔ¨Åciency) of estimator A
with respect to Bis:A;B=MSE (^B)
MSE (^A)(A;B>1means
thatAis better than B).
The average number of failures ( '=Mean ('i)) with'i
being the number of failures in repetition i.
For comparison purpose, we consider the relative number
of failures of technique Awith respect to B:A;B='A
'B(A;B>1means thatAis better than B).
RQ2: Sensitivity to sample size .How does the performance
of DeepEST vary with the sample size?
It is important to Ô¨Ågure out how performance varies with
the number of test cases to select from the operational dataset,
namely with sample size. Indeed, DeepEST aims to perform
well especially with a small sample size, so as to yield precise
estimates with relatively few examples to be manually labelled.
RQ3: Dataset inÔ¨Çuence .How is DeepEST performance af-
fected by the datasets?
In DNN testing, results are often heavily dependent on the
(training and operational) datasets. This RQ aims to Ô¨Ågure out
how these may inÔ¨Çuence the ability of the auxiliary variables
(conÔ¨Ådence, DSA, LSA) to discriminate failing examples,
affecting the performance of DeepEST. To answer RQ3, the
test set is completely labeled, so as to identify all failures.
C. Implementation
DeepEST is implemented mostly in Java.5The implementa-
tion of the distance metric in DeepEST DSA and DeepEST LSA
is the same used by Kim et al. [14]; we used their Python
scripts to compute DSA and LSA values. These are computed
considering the last activation layer of each DNN. The thresh-
oldneeded for the weights deÔ¨Ånition is set as follows:
DeepESTDSA:=mean (DSA ) + 2Std(DSA );
DeepESTLSA:=mean (LSA) +Var(LSA).
The threshold for conÔ¨Ådence , used by DeepEST CS and
DeepESTC, is set to 0:7, assuming that lower conÔ¨Ådence
values are more related to misprediction (i.e., the weights are
activated when the conÔ¨Ådence is less than = 0:7).
In CES, the selection exploits the output of the last hidden
layer. We use the same conÔ¨Åguration as the original article
[17]. The size of the initial sample is p=30, enlarged by a
groupQofq=5examples at each step. The number of random
groups from which Qis selected is L= 300 . For CES and
SRS, we used the Python scripts provided by Li et al. [17].
352(a) Subject S1 (CN5, MNIST)
 (b) Subject S2 (LeNet5, MNIST)
(c) Subject S3 (CN12, CIFAR10)
 (d) Subject S4 (VGG16, CIFAR10)
 (e) Subject S5 (VGG16, CIFAR100)
Fig. 1: RQ1 (effectiveness): Mean Squared Error of estimates
TABLE II: RQ1 (effectiveness): Mean and standard deviation ( ) of the number of failing examples detected
SubjectSRS CES DeepEST CS DeepEST DSA DeepEST LSA DeepEST C Total
failing
examplesMean #
%Mean #
%Mean #
%Mean #
%Mean #
%Mean #
%
S1
(CN5, MNIST)1.901.522.231.5744.570.6830.174.0322.433.2911.933.06 95
S2
(LeNet5, MNIST)2.772.062.031.1665.130.8637.575.1224.103.7424.074.15 132
S3
(CN12, CIFAR10)37.936.4733.773.92106.107.2698.334.6238.076.6170.777.09 1,934
S4
(VGG16, CIFAR10)12.603.3313.033.4985.004.8616.803.0214.773.1424.904.97 641
S5
(VGG16, CIFAR100)57.336.5355.776.62131.177.3578.206.6267.473.68108.704.45 2,952
V. R ESULTS
A. RQ1: effectiveness
Figure 1 plots the MSE of the estimated accuracy. The
techniques exhibit comparable performances, with DeepEST C
being the best one for 3 of the 5 subjects. CES has good
performance in terms of MSE, it is the second technique in
3 cases. Considering the single variables: conÔ¨Ådence, DSA or
LSA lead to results slightly more variable over the subjects ‚Äì
an aspect explored in RQ3. The SRS case is interesting, too:
it is never the worst approach and is the best in one case.
Table II reports the average number and the standard devia-
tion of the failing examples detected. All variants of DeepEST
identify many more failing examples than SRS and CES,
even up to a factor of 30x (DeepEST CSvsCES for subject
S2) and reaching in some cases (S1 and S2, DeepEST CS)
5A replication package is at: https://github.com/dessertlab/DeepESTalmost 50% of the total number of failing examples in the
datasets (last column). The DeepEST algorithm leverages the
adaptive sampling to spot clusters of failing examples with
relatively few tests (set to 200 for RQ1). Its performance varies
depending on the auxiliary information used, but it is always
remarkably better than SRS and CES.
Among the DeepEST variants, conÔ¨Ådence (DeepEST CS)
turns out to be the most effective auxiliary variable in detecting
failures, showing the best performance for all datasets and
models, followed by DSA. CES and SRS select the lowest
number of mispredicted examples, and are close to each other.
Considering both the failure detection ability and the esti-
mate accuracy, DeepEST C‚Äì that combines conÔ¨Ådence and
DSA, the two best auxiliary variables for failing examples
detection - gives a good trade-off, since it provides stable
(across subjects) and close-to-true estimates of the accuracy,
with many more detected failing examples than CES and SRS.
353TABLE III: Pairwise comparison of techniques. A value of R;C>1means the technique on the row has a greater precision
than that on the column. Similarly for the relative number of detected failures R;C
(a) Subject S1 (CN5, MNIST)
DeepEST
row vs col CSDSALSAC SRS
CESR;C 0.0501 0.0740 0.0996 0.1872 1.1754
R;C 1.0987 0.6790 0.3564 0.1136 0.8929DeepESTCSR;C-1.4773 1.9866 3.7346 23.4561
R;C 0.6180 0.3244 0.1034 0.8127
DSAR;C- -1.3447 2.5279 15.8772
R;C 0.5249 0.1673 1.3150
LSAR;C- - -1.8799 11.8070
R;C 0.3187 2.5054
CR;C- - - -6.2807
R;C 7.8624(b) Subject S2 (LeNet5, MNIST)
DeepEST
row vs col CSDSALSAC SRS
CESR;C 0.0312 0.0541 0.0844 0.0845 0.7349
R;C 3.9040 2.9238 1.4523 0.1473 2.4766DeepESTCSR;C-1.7338 2.7026 2.7064 23.5422
R;C 0.7489 0.3720 0.0377 0.6344
DSAR;C- -1.5588 1.5609 13.5783
R;C 0.4967 0.0504 0.8470
LSAR;C- - -1.0014 8.7108
R;C 0.1014 1.7053
CR;C- - - -8.6988
R;C 16.8140
(c) Subject S3 (CN12, CIFAR10)
DeepEST
row vs col CSDSALSAC SRS
CESR;C 0.3183 0.3434 0.8870 0.4772 0.8902
R;C 1.6669 4.9728 1.0967 0.3741 1.0522DeepESTCSR;C-1.0790 2.7872 1.4993 2.7970
R;C 2.9833 0.6579 0.2244 0.6312
DSAR;C- -2.5832 1.3895 2.5923
R;C 0.2205 0.0752 0.2116
LSAR;C- - -0.5379 1.0035
R;C 0.3411 0.9594
CR;C- - - -1.8656
R;C 2.8126(d) Subject S4 (VGG16, CIFAR10)
DeepEST
row vs col CSDSALSAC SRS
CESR;C 0.1533 0.7758 0.8826 0.5234 1.0344
R;C 6.5937 4.1503 0.8772 1.7910 0.9106DeepESTCSR;C-5.0595 5.7562 3.4137 6.7460
R;C 0.6294 0.1330 0.2716 0.1381
DSAR;C- -1.1377 0.6747 1.3333
R;C 0.2114 0.4315 0.2194
LSAR;C- - -0.5930 1.1720
R;C 2.0417 1.0380
CR;C- - - -1.9762
R;C 0.5084
(e) Subject S5 (VGG16, CIFAR100)
DeepEST
row vs col CSDSALSAC SRS
CESR;C 0.4252 0.7131 0.8266 0.5130 0.9727
R;C 2.7761 3.4999 3.5473 1.2076 0.8323DeepESTCSR;C-1.6773 1.9442 1.2067 2.2878
R;C 1.2607 1.2778 0.4350 0.2998
DSAR;C- -1.1591 0.7194 1.3640
R;C 1.0136 0.3450 0.2378
LSAR;C- - -0.6207 1.1767
R;C 0.3404 0.2346
CR;C- - - -1.8959
R;C 0.6892(f) Number of wins and losses
wins DeepEST Total
(r vs c) CES CS DSA LSAC SRS wins
CES - 0 0 0 0 0 0/25DeepESTCS 0 - 2 1 0 0 3/25
DSA 1 0 - 1 0 1 3/25
LSA 2 0 0 - 0 3 5/25
C 3 0 2 2 - 3 10/25
SRS 1 0 0 0 0 - 1/25
Total CESCSDSA LSAC SRS
losses 7/25 0/25 4/25 4/25 0/25 7/25
Tables III(a)‚Äì(e) show the results of the pairwise comparison
of the techniques. For the four DeepEST variants, rows and
columns headings list (in blue) the name of the auxiliary
variables. The evaluation metrics are the ratio of the failing
examples and the relative precision of the estimators. Values
oforgreater (lower) than 1 mean the technique on the
row (column) has better performance. If a technique is better
than the other in a pair for both metrics (values coloured in
the table), we say that it wins.
Table III(f) summarizes the number of wins (and losses).
CES never wins over other approaches, while DeepEST Cwins
against CES 3 out of 5 times. SRS never wins over DeepEST,
while it wins vsCES considering VGG16 on CIFAR100.
DeepESTCnever looses and collects the highest number
of wins (10), showing the best trade-off among accuracy
of the estimation and number of detected failing examples.The choice of the DeepEST variant may be determined by
which auxiliary information can be collected. We see that
DeepESTC, exploiting a combination of two variables, has
good and more stable results in terms of MSE than the other
variants, at the expense of a slight decrease of detected failures.
Single auxiliary variables are more sensitive to the speciÔ¨Åc
dataset/model pair (e.g., conÔ¨Ådence works well if the DNN
is reliable), but expose more mispredictions. ConÔ¨Ådence has
the advantage of not requiring knowledge of the hidden layers
and is easier to compute. When no information is available or
easily computable, SRS could be a good low cost solution.
B. RQ2: sensitivity to sample size
To answer this RQ, experiments are run with the sample
sizes 50, 100, 200, 400, 800, considering the subject with the
highest accuracy (S1), and the one with the lowest accuracy
354(a) Subject S1 (CN5, MNIST)
 (b) Subject S5 (VGG16, CIFAR100)
Fig. 2: RQ2 (sensitivity to sample size): Accuracy for the most (a) and the least (b) accurate subjects
(a) Subject S1 (CN5, MNIST)
 (b) Subject S5 (VGG16, CIFAR100)
Fig. 3: RQ2 (sensitivity to sample size): MSE for the most (a) and the least (b) accurate subjects
(a) Subject S1 (CN5, MNIST)
 (b) Subject S5 (VGG16, CIFAR100)
Fig. 4: RQ2 (sensitivity to sample size): Number of failing examples for the most (a) and the least (b) accurate subjects
(S5), so as to analyze how DeepEST performs when there
are very few and many failing examples in the dataset,
respectively. Figure 2 shows the mean values of the estimates‚Äô
accuracy over repetitions. Figures 3 and 4 plot the MSE and
the mean number of detected failures, respectively. Expectedly,
increasing the sample size all techniques exhibit a decreasing
trend in MSE and an increasing trend in failing examples.
For the subject with highest accuracy (S1), we observe the
following. DeepEST Cshows very good performance for MSE
(Fig. 3(a), Fig. 1), and it detects on average about six times the
failures of SRS and CES (Table II). The advantage for MSE is
more pronounced with the smallest sample sizes, which make
DeepESTCparticularly suited when the number of examplesto select and label is very small. For larger sample sizes, the
MSE is similar but the advantage of DeepEST Cover SRS
and CES is very pronounced for detected failures (Fig. 4(a)).
DeepESTCSis the best among all techniques to detect failures
for small sample sizes; for sizes 400 and800, the best is
DeepESTDSA (Fig. 4(a)). Although the estimates are unbiased
(hence, they tend to the true value), if we look at the mean
estimates over the repetitions (Figure 2(a)), CES shows bad
performance with up to 200test cases. SRS works well with
low budget; its good results may be inÔ¨Çuenced by the very
low number of failures: 18/30 repetitions show 0failures and
100% accuracy. It is interesting to note that in most cases
CES and SRS overestimate the true accuracy ‚Äì an undesired
355property, especially for critical systems. This is related to the
low number of failures detected, as discussed in Section II.
For the subject with lowest accuracy (S5), CES and SRS
outperform DeepEST Cfor small sample sizes ( 50and100);
from 200to800, the estimation by CES starts diverging, while
SRS and DeepEST keep good performance. The tendency to
overestimate the accuracy by CES and SRS is conÔ¨Årmed.
Performance in failing examples detection is always clearly
in favour of all DeepEST variants. Performance in estimation
accuracy is almost specular to what observed with the most
accurate model. A reason is that DeepEST is a sampling
techniques particularly suited for rare populations, which is
not the case of S5. As for the ability to detect failing examples,
conÔ¨Ådence is the best auxiliary variable for DeepEST for
subject S5: it presents the best values in all conÔ¨Ågurations.
C. RQ3: dataset inÔ¨Çuence
We have seen in the experiments for RQ1 and RQ2 that
no single auxiliary variable performs best in all situations.
For instance, we can consider the conÔ¨Ådence a good auxiliary
information for subject S1, and a bad choice for S5. This
may depend on several factors: assuming a perfect training,
the conÔ¨Ådence could be affected by a bias in the training
set; or, with a perfect training set, a wrong training phase
(e.g., due to overÔ¨Åtting) could generate mispredictions with
high conÔ¨Ådence. In some cases the operational dataset could
contain examples very similar (i.e., small distance) to those in
the training set but with a different label, affecting the discrim-
inative power of the DSA and LSA metrics. To analyze how
the three datasets inÔ¨Çuence the ability of auxiliary variables
to discriminate failing examples (impacting the performance
observed in RQ1-RQ2), we consider the subjects S1 and S5,
as for RQ2, plus the VGG16 DNN for CIFAR10.
Figures 5, 6, and 7 show the logistic regression for the
three datasets. The curves Ô¨Åt the probabilities for the outcome
fail/pass to the three predictors: conÔ¨Ådence, DSA, and LSA.
Consider MNIST, for which CN5 reaches the highest accuracy
among all subjects; the probability for a test to fail is very low
for values of conÔ¨Ådence between 0.7 and 1 (Figure 5). This is
clearly not the case for CIFAR10 (Figure 6) and, especially,
CIFAR 100 (Figure 7). In the latter, there is a high chance of
misprediction even with high values of conÔ¨Ådence; this could
be due to a high skew between training and test data.
The discriminating power of DSA and LSA is clearly
greater for MNIST, as the high slope of the S-shaped curves
in Figures 5(b)-(c) suggests, compared to corresponding ones
in Figures 6 and 7. In this case, it actually happens that the
farthest examples have highest probability to be related to a
failure, with a sharp increase after DSA 0:9and LSA300.
This means that if in operation there are (a lot of) examples
far from what observed in training, re-training with a more
representative dataset can be useful to improve the accuracy.
This behaviour is not observed for CIFAR10 and CIFAR100,
and DSA and LSA do not seem to be effective in dividing
the two sets of examples. In CIFAR10, the DSA line is more
horizontal, meaning that the DSA value does not reÔ¨Çect wellthe failure probability. The scarcely discriminative power of
the auxiliary variables in CIFAR10 and CIFAR100 partially
explains the smaller gain of DeepEST over CES and SRS (es-
pecially in terms of MSE); nevertheless, its adaptivity allows
spotting many failing examples even in these conditions.
Finally, it is interesting to highlight the performance of
DeepESTCS(based on conÔ¨Ådence) on MNIST in RQ2. The
saturation in its failure detection ability (Figure 4(a)) can
be explained observing that, after a number of tests able to
spot failures looking at low-conÔ¨Ådence examples, the few
remaining ones with high conÔ¨Ådence are selected with low
probability; the sharper discrimination made by DSA and LSA
determines a high detection ability even when few failing
examples remain. In summary, whenever a tester has good
belief/evidence about the appropriateness of one of the above
auxiliary variables, it is a good choice to select the speciÔ¨Åc
DeepEST variant; if not, the combined variant DeepEST Chas
shown to give the best trade-offs in all Ô¨Åve experimented cases.
D. Threats to validity
A threat to the internal validity comes from the selection
of the experimental subjects. To favor the repeatability of
the experiments under different possibly inÔ¨Çuencing factors,
we have used publicly available networks and pre-trained
models, to avoid incorrect implementation. The conÔ¨Åguration
of parameter rin DeepEST and a different setting of thresholds
may also affect the results (in terms of efÔ¨Åciency of the
estimator), hence a Ô¨Åne-tuning is suggested before applying the
method to other dataset-DNNs. Although the code developed
was carefully inspected, a common threat is the correctness of
the scripts to collect data and compute the results.
The choice of the sample size inÔ¨Çuences the effectiveness
too. We ran a sensitivity analysis to show that DeepEST is
still effective (compared to both CES and SRS) under Ô¨Åve
(from 50 to 800) values of the sample size, but different
values could yield different results. Threats to external validity
depend on both the number of models and datasets considered
for experimentation. We strived to control this threat con-
sidering different widely used DNN and datasets. Although
the results may change with different subjects, the diversity
and signiÔ¨Åcance of the chosen subjects give conÔ¨Ådence to
the general considerations. Replicability of the experiments
on other subjects is to further mitigate this threat.
VI. R ELATED WORK
Testing of DNN is a hot research topic. Zhang et al. [26]
present a survey on Machine Learning testing, identifying
three main families of techniques: mutation testing [27],
metamorphic testing [9], [28], and cross referencing [7], [29].
The former two are for test generation: they generate adver-
sarial examples causing mispredictions. Cross-referencing can
highlight the most interesting test cases when different imple-
mentations of a system disagree. Most of these techniques are
meant for fault detection, rather than for accuracy estimate.
Testing DNN for accuracy is the focus of Li et al.[17], who
presented CES ‚Äì which DeepEST has been compared against
356(a) ConÔ¨Ådence
 (b) DSA
 (c) LSA
Fig. 5: RQ3 (dataset inÔ¨Çuence): MNIST samples distribution
(a) ConÔ¨Ådence
 (b) DSA
 (c) LSA
Fig. 6: RQ3 (dataset inÔ¨Çuence): CIFAR10 samples distribution
(a) ConÔ¨Ådence
 (b) DSA
 (c) LSA
Fig. 7: RQ3 (dataset inÔ¨Çuence): CIFAR100 samples distribution
- as the Ô¨Årst approach using operational testing to this aim.
DeepEST differs from CES in several aspects, the key ones
being the sampling algorithm and the used auxiliary variables,
which are conceived to improve failing examples detection
while preserving the accuracy estimation power.
Operational testing, where tests are derived according to the
operational proÔ¨Åle, is an established practice to estimate soft-
ware reliability [30]. It was the core technique of Cleanroom
software engineering [31]‚Äì[34] and of the Software Reliability
Engineering Test process [30]. Cai et al.developed Adaptive
Testing, still based on the operational proÔ¨Åle, but foreseeing
adaptation in assigning tests to partitions [35]‚Äì[38]. Recently,
Pietrantuno et al.stressed the use of unequal probability
sampling to improve the estimation efÔ¨Åciency [39], to this
aim formalizing several sampling schemes [19]. Our proposal
goes along this direction: we do not sample tests representative
of the operational dataset, but we ‚Äúalter‚Äù the selection and
counterbalance the uneven selection in the estimator. Unequal
probability, without-replacement and adaptive sampling are the
key concepts we borrowed for operational testing of DNNs.VII. C ONCLUSION
Testing the accuracy of a DNN with operational data aims
at precise estimates with small test suites, due to the cost for
manually labelling the selected test cases. This effort motivates
to pursue also the goal of exposing many mispredictions with
the same test suite, so as to improve the DNN after testing.
With these two concurrent goals, we presented DeepEST, a
technique to select failing tests from an operational dataset,
while ultimately yielding faithful estimates of the accuracy of
a DNN under test.
We evaluated experimentally four variants of DeepEST,
based on various types of auxiliary information that its
adaptive sampling strategy can leverage. The results with
four DNN models and three popular datasets show that all
DeepEST variants provide accurate estimates, compared to
existing sampling-based DNN testing techniques, while gen-
erally much outperforming them in exposing mispredictions.
Practitioners may choose the appropriate variant, depending
on the characteristics of their operational dataset and on which
auxiliary information is available or they can collect.
357REFERENCES
[1] Ziad Obermeyer and Ezekiel J. Emanuel. Predicting the future ‚Äî big
data, machine learning, and clinical medicine. New England Journal of
Medicine , 375(13):1216‚Äì1219, 2016. PMID: 27682033.
[2] Mariusz Bojarski et al. End to End Learning for Self-Driving Cars.
arXiv:1604.07316, 2016.
[3] Amir Efrati. Uber Ô¨Ånds deadly accident likely caused by software set
to ignore objects on road. The Information , May 7, 2018.
[4] Jack Stewart. Tesla‚Äôs autopilot was involved in another deadly car
crash. [Online]. Available: https://www.wired.com/story/tesla-autopilot-
self-driving-crash-california/, 2018.
[5] Yuchi Tian, Kexin Pei, Suman Jana, and Baishakhi Ray. DeepTest:
Automated testing of deep-neural-network-driven autonomous cars. In
40th International Conference on Software Engineering , ICSE, pages
303‚Äì314. ACM, 2018.
[6] Yaniv Taigman, Ming Yang, Marc‚ÄôAurelio Ranzato, and Lior Wolf.
DeepFace: Closing the gap to human-level performance in face veriÔ¨Åca-
tion. In IEEE Conference on Computer Vision and Pattern Recognition ,
CVPR, pages 1701‚Äì1708. IEEE, 2014.
[7] Kexin Pei, Yinzhi Cao, Junfeng Yang, and Suman Jana. Deepxplore:
Automated whitebox testing of deep learning systems. Communications
of the ACM , 62(11):137‚Äì145, 2019.
[8] Lei Ma, Felix Juefei-Xu, Fuyuan Zhang, Jiyuan Sun, Minhui Xue,
Bo Li, Chunyang Chen, Ting Su, Li Li, Yang Liu, Jianjun Zhao, and
Yadong Wang. DeepGauge: Multi-Granularity Testing Criteria for Deep
Learning Systems. In 33rd ACM/IEEE International Conference on
Automated Software Engineering , ASE, pages 120‚Äì131. ACM, 2018.
[9] Mengshi Zhang, Yuqun Zhang, Lingming Zhang, Cong Liu, and Sarfraz
Khurshid. DeepRoad: GAN-Based Metamorphic Testing and Input Vali-
dation Framework for Autonomous Driving Systems. In 33rd ACM/IEEE
International Conference on Automated Software Engineering , ASE,
pages 132‚Äì142. ACM, 2018.
[10] Lei Ma, Fuyuan Zhang, Minhui Xue, Bo Li, Yang Liu, Jianjun Zhao,
and Yadong Wang. Combinatorial testing for deep learning systems.
arxiv.org/abs/1806.07723, 2018.
[11] Augustus Odena and Ian Goodfellow. TensorFuzz: Debugging neural
networks with coverage-guided fuzzing. In 36th International Confer-
ence on Machine Learning , volume 97 of Proc. of Machine Learning
Research , 2019.
[12] Zenan Li, Xiaoxing Ma, Chang Xu, and Chun Cao. Structural coverage
criteria for neural networks could be misleading. In 41st International
Conference on Software Engineering: New Ideas and Emerging Results ,
ICSE-NIER, pages 89‚Äì92. IEEE, 2019.
[13] Weibin Wu, Hui Xu, Sanqiang Zhong, Michael R. Lyu, and Irwin King.
Deep Validation: Toward Detecting Real-World Corner Cases for Deep
Neural Networks. In 49th Annual IEEE/IFIP International Conference
on Dependable Systems and Networks , DSN, pages 125‚Äì137. IEEE,
2019.
[14] Jinhan Kim, Robert Feldt, and Shin Yoo. Guiding Deep Learning System
Testing Using Surprise Adequacy. In 41st International Conference on
Software Engineering , ICSE, pages 1039‚Äì1049. IEEE, 2019.
[15] Phyllis G. Frankl, Richard G. Hamlet, Bev Littlewood, and Lorenzo
Strigini. Evaluating testing methods by delivered reliability. IEEE
Transactions on Software Engineering , 24(8):586‚Äì601, 1998.
[16] Michael R. Lyu, editor. Handbook of Software Reliability Engineering .
McGraw-Hill, Inc., Hightstown, NJ, USA, 1996.
[17] Zenan Li, Xiaoxing Ma, Chang Xu, Chun Cao, Jingwei Xu, and Jian
L¬®u. Boosting Operational DNN Testing EfÔ¨Åciency through Conditioning.
InProc. of the 2019 27th ACM Joint Meeting on European Software
Engineering Conference and Symposium on the Foundations of Software
Engineering , ESEC/FSE, pages 499‚Äì509. ACM, 2019.
[18] Domenico Cotroneo, Roberto Pietrantuono, and Stefano Russo. RELAI
testing: a technique to assess and improve software reliability. IEEE
Transactions on Software Engineering , 42(5):452‚Äì475, 2016.
[19] Roberto Pietrantuono and Stefano Russo. On adaptive sampling-
based testing for software reliability assessment. In 27th International
Symposium on Software Reliability Engineering , ISSRE, pages 1‚Äì11.
IEEE, 2016.
[20] Sharon L. Lohr. Sampling: Design and Analysis . Duxbury Press, 2009.
[21] Steven K. Thompson. Sampling, Third Edition . John Wiley & Sons,
Inc., 2012.[22] Daniel G. Horvitz and Donovan J. Thompson. A generalization of
sampling without replacement from a Ô¨Ånite universe. Journal of the
American Statistical Association , 47(260):pp. 663‚Äì685, 1952.
[23] Morris H. Hansen and William N. Hurwitz. On the Theory of Sam-
pling from Finite Populations. The Annals of Mathematical Statistics ,
14(4):333‚Äì362, 1943.
[24] Yann Lecun, L ¬¥eon Bottou, Yoshua Bengio, and Patrick Haffner.
Gradient-based learning applied to document recognition. Proceedings
of the IEEE , 86(11):2278‚Äì2324, 1998.
[25] Alex Krizhevsky. Learning multiple layers of features from tiny images.
Technical Report TR-2009, University of Toronto, 2009.
[26] Jie M. Zhang, Mark Harman, Lei Ma, and Yang Liu. Machine learning
testing: Survey, landscapes and horizons. IEEE Transactions on Software
Engineering , pages 1‚Äì37, 2020.
[27] Lei Ma, Fuyuan Zhang, Jiyuan Sun, Minhui Xue, Bo Li, Felix Juefei-
Xu, Chao Xie, Li Li, Yang Liu, Jianjun Zhao, and et al. DeepMutation:
Mutation Testing of Deep Learning Systems. In 29th International
Symposium on Software Reliability Engineering , ISSRE, pages 100‚Äì111.
IEEE, 2018.
[28] Xiaoyuan Xie, Joshua W. K. Ho, Christian Murphy, Gail Kaiser, Baowen
Xu, and Tsong Yueh Chen. Testing and validating machine learning
classiÔ¨Åers by metamorphic testing. Journal of Systems and Software ,
84(4):544‚Äì558, 2011.
[29] Siwakorn Srisakaokul, Zhengkai Wu, Angello Astorga, Oreoluwa
Alebiosu, and Tao Xie. Multiple-implementation testing of supervised
learning software. In AAAI Workshops . Association for the Advancement
of ArtiÔ¨Åcial Intelligence, 2018.
[30] John D. Musa. Software reliability-engineered testing. Computer ,
29(11):61‚Äì68, 1996.
[31] Harlan D. Mills, Michael Dyer, and Richard C. Linger. Cleanroom
software engineering. IEEE Software , 4(55):19‚Äì24, 1987.
[32] P. Allen Currit, Michael Dyer, and Harlan D. Mills. Certifying the
reliability of software. IEEE Transactions on Software Engineering ,
SE-12(1):3‚Äì11, 1986.
[33] Richard H. Cobb and Harlan D. Mills. Engineering software under
statistical quality control. IEEE Software , 7(6):45‚Äì54, 1990.
[34] Richard C. Linger and Harlan D. Mills. A case study in cleanroom
software engineering: the IBM COBOL Structuring Facility. In 12th
International Computer Software and Applications Conference , COMP-
SAC, pages 10‚Äì17. IEEE, 1988.
[35] Junpeng Lv, Bei-Bei Yin, and Kai-Yuan Cai. On the asymptotic behavior
of adaptive testing strategy for software reliability assessment. IEEE
Transactions on Software Engineering , 40(4):396‚Äì412, 2014.
[36] Junpeng Lv, Bei-Bei Yin, and Kai-Yuan Cai. Estimating conÔ¨Ådence
interval of software reliability with adaptive testing strategy. Journal of
Systems and Software , 97:192‚Äì206, 2014.
[37] Kai-Yuan Cai, Chang-Hai. Jiang, Hai Hu, and Cheng-Gang Bai. An ex-
perimental study of adaptive testing for software reliability assessment.
Journal of Systems and Software , 81(8):1406‚Äì1429, 2008.
[38] Kai-Yuan Cai, Yong-Chao Li, and Ke Liu. Optimal and adaptive
testing for software reliability assessment. Information and Software
Technology , 46(15):989‚Äì1000, 2004.
[39] Roberto Pietrantuono and Stefano Russo. Probabilistic sampling-based
testing for accelerated reliability assessment. In International Confer-
ence on Software Quality, Reliability and Security , QRS, pages 35‚Äì46.
IEEE, 2018.
358