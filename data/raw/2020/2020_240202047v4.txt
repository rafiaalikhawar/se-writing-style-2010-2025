Calibration and Correctness of
Language Models for Code
Claudio Spiess∗
UC Davis
USA
cvspiess@ucdavis.eduDavid Gros∗
UC Davis
USA
dgros@ucdavis.eduKunal Suresh Pai
UC Davis
USA
kunpai@ucdavis.eduMichael Pradel
Univ. of Stuttgart
Germany
michael@binaervarianz.deMd Rafiqul Islam Rabin
Univ. of Houston
USA
mrabin@central.uh.edu
Amin Alipour
Univ. of Houston
USA
maalipou@central.uh.eduSusmit Jha
SRI
USA
susmit.jha@sri.comPrem Devanbu
UC Davis
USA
ptdevanbu@ucdavis.eduToufique Ahmed
UC Davis
USA
tfahmed@ucdavis.edu
Abstract —Machine learning models are widely used, but can
also often be wrong. Users would benefit from a reliable indi-
cation of whether a given output from a given model should
be trusted, so a rational decision can be made whether to use
the output or not. For example, outputs can be associated with
aconfidence measure ; if this confidence measure is strongly
associated with likelihood of correctness , then the model is said
to be well-calibrated .
A well-calibrated confidence measure can serve as a basis for
rational, graduated decision-making on how much review and
care is needed when using generated code. Calibration has so
far been studied in mostly non-generative ( e.g., classification)
settings, especially in software engineering. However, generated
code can quite often be wrong: Given generated code, developers
must decide whether to use directly, use after varying intensity
of careful review, or discard model-generated code. Thus, cali-
bration is vital in generative settings.
We make several contributions. We develop a framework for
evaluating the calibration of code-generating models. We consider
several tasks, correctness criteria, datasets, and approaches, and
find that, by and large, generative code models we test are not
well-calibrated out of the box. We then show how calibration
can be improved using standard methods, such as Platt scaling.
Since Platt scaling relies on the prior availability of correctness
data, we evaluate the applicability and generalizability of Platt
scaling in software engineering, discuss settings where it has good
potential for practical use, and settings where it does not. Our
contributions will lead to better-calibrated decision-making in the
current use of code generated by language models, and offers a
framework for future research to further improve calibration
methods for generative models in software engineering.
Index Terms —LLMs, Calibration, Confidence Measure
I. I NTRODUCTION
Generative large language models (LLMs) are now widely-
used for code completion in IDEs. However, LLMs make
mistakes: they can generate known buggy code [1], or code
with risky vulnerabilities [2], [3]. Despite these risks, LLM
“copilots” [4] are growing in popularity—thus there is a grow-
ing concern that bad LLM-generated code could be integrated
into widely-used software. Given that LLMs might generate
*Equal contribution. Order determined by random coin flip.buggy code, how should a developer decide whether generated
code is correct or not?
One possibility is to use the confidence , or probability
assigned to the generated code by the LLM itself. Consider
a developer who asks G PT-3.5 to complete some unfinished
code. For example, given the prefix
def clear(self, tag: Optional[Hashable] = None) -> None:
the model generates the completion
self.jobs[:] = [job for job in self.jobs if job.tag != tag]
with an average per-token confidence of 91%, suggesting
high confidence (based on its training) that the code is a
likely completion for the given prefix. However, this code
is known to be buggy! In fact, when we test thousands of
line completions, in cases where the average probability was
greater than 90%, only 52% actually passed test cases. One
can also find reverse examples, where the LLM has very little
confidence, but the generated code is actually correct.
We make two observations. First, since LLMs can make
mistakes, users would benefit from a reliable indication of
confidence that the generated code is actually correct . Second,
such indications of confidence, when well-aligned with actual
correctness, can support well-justified software quality control
measures, and thus improve the reliability of the AI4SE ecosys-
tem. When an LLM-offered code suggestion is accompanied
by a numerical “confidence” signal e.g., a probability measure,
then this signal should be well-aligned with the likelihood that
the code is actually correct. Such a measure is said to be well -
calibrated .
Calibration has been studied in other settings e.g., classi-
cally in weather prediction and recently for software-related
classification tasks [5]. In this paper, we study the calibration
ofgenerative1large language models, when used in practical
software engineering settings, such as line-level code comple-
tion, function synthesis, and code-repair.
1We note that the notion of correctness for generative tasks is quite different
than for classification tasks, where the output is a label, rather than a sequence
of tokens.arXiv:2402.02047v4  [cs.SE]  21 Aug 2024A well-calibrated confidence measure would support ratio-
nal risk-management in the development process, and help
quality-improvement processes. For example, a development
team might reasonably adopt a policy that: a) generated code
associated with high confidence could be reviewed lightly and
quickly accepted; b) suggestions with a medium confidence
value should be reviewed more carefully before acceptance;
and c) suggestions with a low confidence value should be
simply rejected or the prompt should be adjusted.
Despite its importance and widespread use of LLMs in
software engineering, the correctness and calibration of code-
generating models currently is not well understood. In par-
ticular, it is currently unknown whether confidence measures
provided by the LLMs themselves align well with actual code
correctness.
This paper does an empirical study of the calibration of
code-generating models using several prior techniques and
explores approaches for improving calibration further. To this
end, we describe an evaluation framework for the calibration
of code-generating language models. We instantiate the frame-
work for different tasks, e.g., code synthesis, code completion,
and program repair, using different correctness criteria (exact-
match w.r.t. a reference solution and correctness-modulo-
testing), and by applying the framework to different models.
Based on this framework, we evaluate well-established tech-
niques for estimating how a model’s confidence align with
actual correctness; then, based on our findings, we present
improvements over existing techniques.
Our work yields several findings:
•The alignment of confidence measures provided by LLMs
with standard notions of code correctness is poor, when
evaluated on realistic datasets across different tasks, in-
cluding completion, synthesis, and automated program
repair. We observe generally high ECE (Expected Cal-
ibration Error, described in Section III-C) across all
settings, ranging from 0.09to0.73, suggesting intrinsic
LLM confidences are poor predictors of code correctness.
•We evaluate several reflective approaches to improve this
alignment, and also confidence rescaling using known
correctness labels. While rescaling generally improves
calibration, reflective methods are rather inconsistent,
working better in some settings than others.
•Finally, we focus on the most widely-used SE task for
LLMs, viz. code completion, and use the instructable
GPT-3.5 model, and few-shotting, in a reflective setting,
and show that calibration improves substantially from the
skill score2of0to a much higher level of 0.15.
Our work considers the important problem of providing de-
velopers with a reliable indication of whether generated code
is correct, and (especially for the widely-used task of code
completion) offers an approach, using BM25-aided [6] few-
shotting, that has potential practical value.
2Brier Skill score, which we explain below in § III-C.A. Research Agenda
Code LLMs are perhaps mostly widely-used for code sug-
gestion/completion ; other tasks include code synthesis and
program repair .
RQ 1 . How well is the confidence of language models in their output
aligned with the empirical correctness of the output, specifically
for common generative tasks, viz.function synthesis, line-level code
completion, and program repair?
We evaluate the output for two different notions of correctness,
viz., exact-match with known correct code, and second, passing
all given tests.
In general, the levels of alignment between the intrinsic
confidence ( viz., directly provided by the LLM) and correct-
ness is poor. This indicates need for better approaches to
calibration. We then explore several engineering responses to
the problem, listed in the following research questions. First,
we consider the standard approach of confidence rescaling ,
using Platt scaling.
RQ 2 . Can alignment between LLM confidence in generated code,
and its correctness, be improved by confidence rescaling?
While confidence rescaling can help remedy over- and
under-confidence, it does require some data to determine
the parameters of the scaling function. We also analyze and
discuss some considerations in obtaining this data, specifically
for code-generation tasks.
Next, we investigate the possibility that the model is able
to better calibrate upon reflection . We ask the model (using a
separate reflective prompt) to consider its own generated code
and judge its confidence in the quality.
RQ 3 . Is confidence obtained by reflection better aligned with correct-
ness?
Finally we investigate few-shotting, to see whether it helps
calibration for the widely used task of code completion.
RQ 4 . Can we use few-shot techniques to achieve better calibrated
confidence for code completion, using an instruction-tuned model
with in-context learning?
II. B ACKGROUND
A. Calibration
This concept originates in prediction problems like weather
forecasting. Consider a weather model predicting a 70%
confidence (probability) of rain the next day. If we ran this
model for a while, and observed rain in 70% of the days where
a forecast with 70% confidence was made, then we call it a
well-calibrated model. A well-calibrated model’s confidence in
a given output, is quite close to the empirical relative frequency
(likelihood) with which the output is actually correct.
With well-calibrated rain-forecasting, a user has options for
arational response : at 20% confidence of rain, one might take
a hat; at higher confidences, one might take an umbrella; if
even higher, one might take the bus rather than walk, etc.
From an earlier work by Jiang et al. [7]: given a model M,an input Xand true, expected correct output Y, a model
output M(X) =ˆY, (note that we won’t always have Y=ˆY)
and a output probability PM(ˆY|X)provided by the model, a
perfectly calibrated model satisfies the following condition
P(ˆY=Y|PM(ˆY|X) =p) = p,∀p∈[0,1]. (1)
In other words, if we have perfectly-calibrated confidence
p(the model’s calculated probability of its prediction that
the output is ˆY), then this value equals the empirical fraction
pof the cases where the actual output Ycorrectly matches
the prediction ˆY. Usually these probabilities don’t perfectly
match; there are various measures of the deviation, including
Brier Score [8] and ECE (Expected Calibration Error) [9]
B. Why Calibration Matters for Code
Even powerful LLMs can make mistakes [1], potentially
leading users to accept incorrect code. A well-calibrated
confidence signal could help developers manage [4], [10] this
risk. Consider the confidence passociated with generated
code. A well-calibrated high-value of pwould indicate a high
empirical probabilty pthat the code is correct, and so it could
be simply accepted; a low value would indicate higher risk
that the code is incorrect, and so should be rejected. A poorly
calibrated model may lead to either unnecessary rejection
of likely correct code, or ill-advised acceptance of likely
incorrect code. We note that good calibration allows more
nuanced, effective quality-control (Q-C) decisions, beyond
simple binary decisions e.g., carefully review each token of
generated code, perhaps by several people, vs.just use it.
Such a well-calibrated quality-control process has been used in
medicine e.g., for elder-care [11], and for decision-making in
cancer-care [12]. Given the cost & consequences of properly
addressing software quality, and the potential benefits of LLM-
generated code, a well-calibrated confidence signal is highly
desirable.
III. R ESEARCH METHODOLOGY
We consider three generative tasks i.e., function synthesis,
line-level code completion, and program repair, where gen-
erative LLMs are directly applicable and widely-used ( e.g.,
completion in Copilot). In this section, we will discuss the
tasks, datasets, models, and methodology of our approach.
A. Code Correctness
When evaluating calibration, we need a notion of correct-
ness. For (non-generative) models outputting labels, classes,
True/False, etc. correctness is simply an exact-match with
ground-truth correct label. Exact-match could also be viewed
as a notion of correctness for code, e.g., with defect repair,
where there is a known, incorrect “buggy” version, and a
known “fixed” version. Generated code is correct only if it
matches exactly the fixed version, given an appropriate prompt.
However, this approach is overly strict ; the generated code
might match exactly, but still pass all tests. Other notions of
correctness exist: code-review, formal verification, etc.We use
test cases provided with the code as our preferred indication of
correctness, as tests are widely used, and are easily automated.While test-passing correctness offers the advantage of ad-
mitting different semantically identical forms, test cases maybe
insufficient or incorrect3; tests can also be “flaky” [14]: the
same test, on the same code, might pass, or fail.
B. Confidence Measures
We usually calculate a confidence measure (or probability)
p, associated with generated output code C. We consider two
categories of measures: intrinsic probability , which is calcu-
lated by the generative LLM per se , and reflective probability ,
obtained by re-invoking the model, instructing it to estimate
its confidence in the correctness of the code just generated
(see Appendix figure A1 for prompts). Our measures include:
Average Token Probability (Intrinsic, pavg)For an output
sequence Tof tokens τi, i= 1. . . n , we collect the associ-
ated model probabilities p(τi), and then compute the mean
pavg(T) =1
nnX
i=1p(τi).
Generated Sequence Probability (Intrinsic, ptot)The full
generated sequence confidence is calculated as the product of
probabilities, ptot(T) =nY
i=1p(τi).
Verbalized Self-Ask [15]–[17] (Reflective, pv)We instruct
the model to reflect jointly on the prompt, and the model-
generated code, and then output a numeric value of its confi-
dence in the generated code.
Extra logic is implemented for when the model fails to output
a probability (discussed further in Appendix A).
Question Answering Logit [18] (Reflective, pBandpNB)
In this case, rather than prompting for a numerical score
(as above), we ask for a TRUE orFALSE answer. The
probability associated with the TRUE token is taken to be
the confidence measure. Additionally, we extend this approach
using normalization : the model can assign probability mass to
multiple possible expressions of TRUE orFALSE or even
other variations (e.g, “ True”, “ true”, “ ”, ). Thus when
extending to the normalized form (Ask T/F N), we take the
fraction of probability mass “True” assigned between only
“True” and “False”.
Our experiments consider the four above: average token
probability ,generated sequence probability ,verbalized self-
evaluation , and question answering logit . In addition, as a
baseline, we also used the length of the generated sequence.
The length baseline is calculated based on the number of
characters in the generated sequence, scaled such that 0is
the shortest value in the dataset, and 1is the longest.
For reflective measures, we expected that a code generation
model should perform well (and provide well-calibrated con-
fidence scores for correctness): first, for synthesis , given just
3e.g., Liu et al. [13] report gaps in the test sets of HumanEval.a good natural language description (without tests), of the de-
sired function; second, for completion & bug-fixing , given the
surrounding context. In both settings, we measure correctness
using available hidden test cases. We also note that including
hidden or failing tests in an LLM prompt is not common
experimental practice for the tasks we analyze. [19]–[21].
C. Measures of Calibration
Using model’s confidence in its generated output, and a
way of determining correctness , one can compute measures
of calibration. Calibration measures conceptually arise from
theReliability Plot , which plots correctness vs.confidence
Two reliability plots illustrating this method are shown
in Figure 1. Figure 1a is for token-level code completions
from C ODEX ; it shows the observed proportion of exact-
match correct tokens (y-axis) vs.the predicted probability i.e.,
confidence as per the language model, based on bucketing
observations into subsets S1, S2, . . . S n. Here, there are n= 10
buckets, equally spaced by confidence measure. Each bucket
has an associated bar whose height indicates the proportion
(value ∈[0,1]) of correct samples in the bucket. The closer
the bars in each Siare to the diagonal line, the better the
calibration.
0.0 0.5 1.0
P(estimate)0.00.20.40.60.81.0P(correct): 0.13
Codex T oken Level Reliability
(a) Well Calibrated
0.0 0.5 1.0
P(estimate)0.00.20.40.60.81.0P(correct): 0.45
GPT3.5 Verbalize Reliability (b) Poorly Calibrated
Fig. 1: Sample calibration plots demonstrating well- vs.poorly- calibrated.
We note here that C ODEX is a large model, well-trained
on the task of token-level completion; thus, it is both well-
calibrated and generally correct on the simple token-level
completion task. However, for notions of correctness farther
from the training objective, such as line-level code completion
and test-passing, C ODEX ’sintrinsic probability may not be as
well-calibrated. An example (Figure 1b) where the confidence
is not well-calibrated is G PT-3.5 for line completion using
verbalized confidence ( i.e., asking it to write its confidence;
see Section III-B).
We study two measures of calibration: Brier Score [8] and
ECE [9]. Both measure the deviation from perfect calibration.
As before (following [22]), we assume a model M, input X,
actual desired output Y, and model prediction M(X) = ˆY.
In our case, both Yand ˆYare code, rather than a single
label. Calibration measures indicate the extent to which the
deviations of ˆYfrom the desired Yare actually aligned with
the model’s confidence in its output, ˆY.
From the calibration plot, with the evaluation set Tbucketed
into subsets Si, i= 1. . . m s.t.S
iSi=T,∀i∈1. . . m . We estimate correctness in each bucket as the fraction
of predictions in that bucket which are “correct” (as discussed
in § III-A); confidence is the average estimated probability
from the model in that bucket:
corr(Si) =1
|Si|X
xi∈Si1(✓M(xi)) (2)
conf(Si) =1
|Si|X
xi∈Siˆpi (3)
where the model generates the code M(xi)with confidence
(probability) ˆpi, and✓M(xi)indicates that the generated
code is correct, as per the operative experimental definition.
For a perfectly calibrated M, we would have corr (Si) =
conf(Si)∀i∈1. . . m . In practice, we observe deviations
from this ideal. Expected Calibration Error ( ECE ) [9] is
a typical measure of calibration, calculated as the weighted
average of these deviations.
ECE =mX
i=1|Si|
|T||corr(Si)−conf(Si)| (4)
ECE is intuitive, but can mislead; as seen below, a na ¨ıve
predictor whose confidence is always the base rate would yield
a deceptively low ECE value. An alternative measure, the
Brier score, B[8] avoids this issue; it is calculated as follows:
B=1
|T||T|X
i=1(ˆpi−1(✓M(xi)))2(5)
One can achieve an optimal Brier of B ≈0when confidently
estimating ˆpi≈1when the code is correct, and estimating
ˆpi≈0when the code is incorrect for each sample.
For comparison, consider using the Unskilled Reference
Brier Score, Bref, attainable by a na ¨ıve, “unskilled” model,
which simply assigns the base-rate pras its confidence for
every prediction. Here, all prediction confidence values are
in one bin, the value pr; and the empirical correctness in this
single bin isthe base rate pr; soECE≈0which is misleading
(thus exemplifying one of the weaknesses of ECE ). The
closed-form Brier Score for this unskilled predictor is:
Bref=pr(1−pr) (6)
In a 50-50 coinflip scenario (assuming heads iscorrect ),
a na ¨ıve predictor that randomly guesses correct with 50%
confidence receives Bref= 0.5∗(1−0.5)≈0.25. Higher
base rates yield lower Bref;e.g., for the MBPP dataset [20],
GPT-3.5 generates test-passing solutions for about 72% of the
programming problems; here always guessing correct with
72% confidence results in Bref= 0.72∗0.28≈0.20.
With well-calibrated confidence scores, a “skilled” model can
achieve Brier Scores lower than this unskilled Brefvalue; if
a model does worse, it is indicative of poor calibration. Thus,
one commonly reports a Skill Score (SS), calculated thus:
SS=Bref− Bactual
Bref(7)
Positive SS(perfect score = 1.0) indicates improvement
over baseline Bref; negative indicates worse calibration than
the baseline. Small positive values of SS can sometimes
indicate good skill. For example, the Deutsche Wetterdienst(German weather forecasting service) considers 0.05 Skill
Score to be a minimum threshold for a good forecast quality4.
As another data point, the American data journalism site
538 reports a skill of around 0.13 in forecasting World Cup
games5, which is in the range of what we observe in our
experiments for best case code generation by LLMs; but these
LLM performance numbers are just a starting point, and can
be expected to improve in the future. ECE (Equation 4) and
Brier Score (Equation 5) serve slightly different purposes:
the Brier Score is calculated for each sample, and measures
both the ability to correctly discriminate output categories, and
calibration of the output probability. The ECE measures just
calibration; but it can be misleadingly low, as noted above
for the unskilled predictor. Additionally binning must be done
carefully, since it can affect ECE scores [23].
D. Rescaling Approaches
Machine learning models are not always calibrated. Guo
et al. [22] discuss ways of rescaling probability estimates
to better match observations. A common approach is Platt
scaling [24], where a logistic regression is fit to the logit
values of the prediction i.e., thelnof the measured confidence
probability. This optimizes two parameters, a linear scaling
multiplier and a bias i.e., intercept, that shifts the value.
To reduce the likelihood that the scaling overfits & skews
our results, we rescale over five folds; i.e., we fit a logistic
regression on a random 4/5of data and apply it to 1/5of data,
before sliding over and doing each combination of 4/5.
Besides Platt scaling, temperature rescaling has also been
used [18], [22], [25]: this approach applies a scalar multiplier
on the logits representing each class e.g., a multiclass image
classifier. In our binary confidence case, this has similar
expressivity to Platt scaling without an intercept. Other ap-
proaches include histogram binning [26], isotonic regression
[27], inter alia [22], [28]. These approaches are more param-
eterized; given the data limitations in our experimental setup
e.g., a few hundred examples in function synthesis, they pose
higher risk of overfitting.
As we discuss in Section IV, Platt scaling does improve
calibration, with some caveats.
E. Tasks & Dataset
Task Dataset Dataset SizeCorrectness
MeasureConfidence MeasureCalibration
Metric
Function synthesisHumanEval 164 Test-passing
CorrectnessAverage Token
Probability, Generated
Sequence Probability,
Verbalized Self-Evaluation,
Question Answering LogitBrier Score,
ECEMBBP Func 880
Line-level Completion DyPyBench 1,988 Test-passing
Correctness, EMProgram RepairDefects4J 1-line 120
ManySStubs4j 3,000 Exact-Match (EM)
TABLE I: List of tasks with associated datasets and measures.
1) Function Synthesis: This task aims to generate Python
functions from “Docstrings”.6Correctness is determined by
functional testing.
4www.dwd.de/EN/ourservices/seasonals forecasts/forecast reliability.htm
5projects.fivethirtyeight.com/checking-our-work/
6Docstrings are code comments that explain the code’s purpose and usage.
Further discussed in § III-E2.We use H UMAN EVAL [19] and MBPP [20] datasets
(see Appendix figure A2 for sample prompts and model
output). One caveat: the samples in these datasets largely
constitute artificial problems, specifically assembled to test the
code-synthesis capacity of LLMs; measurements (both accu-
racy and calibration) over these datasets may not generalize
to real-world software development. Even so, these datasets
provide a valuable datapoint for assessing model calibration.
We restructure all MBPP problems into a function synthesis
task by placing the prompt inside the tested method as a Doc-
string, making it comparable to H UMAN EVAL. Additionally
we exclude approximately 75 problems where the reference
solution fails to pass the provided test cases7.
2) Line-level Code Completion: Code completion is cur-
rently the most important and widely-deployed generative task,
with tools like GitHub Copilot [19]. Completion performance
has been studied at both the token and line levels [29]–[32].
However, calibration for this vital, widely-deployed task has
so far not been evaluated in detail
The current decoder-only GPT models [19], [33] are already
trained to generate the next token at low average cross-
entropy given all the prior tokens, following the condition
p(token |prior tokens ). Unsurprisingly, we found such
autoregressively-trained models are per se well-calibrated at
the token level. In this work, we will primarily focus on
line-level completion. While several datasets exist for this
problem [34], [35], we use D YPYBENCH [36], a new dataset
consisting of 50 popular open-source Python projects, includ-
ing test suites for these projects. The test suites allow a test-
correctness measure, in addition to the highly restrictive exact-
match ( viz., the original line).
DYPYBENCH consists of complex real-world projects, each
with hundreds of thousands of lines of Python code, and
totaling over 2.2 million lines of Python code. We ran all
test suites for each project with coverage reporting enabled,
extracted all functions from the projects, following [37], and
selected 1,988 functions with at least 3 lines in the body, 100%
test coverage, and at least one line in the “Docstring”.
3) Program Repair: Program repair is a well-studied prob-
lem in software engineering [38]. Several studies report that
LLMs are effective at this task [21], [39], [40]. However,
LLM calibration for program repair is not well-understood.
This paper focuses on small, pre-localized single-line bugs.
We leverage the widely-used Defects4J dataset [41], which
includes real-world examples of buggy programs, with fixes
and test-sets. We extract 120 single-line bugs from D E-
FECTS 4J dataset. However, with only 120 samples, we may
not obtain a comprehensive view of calibration. Therefore, we
included another dataset, ManySStubs4J [42] (abbr. SS TUBS ),
which consists of single-line repairs. Following the setup of
the SS TUBS dataset, the bug might be localized to a sub-
7Due to either buggy reference code/tests, or possibly missing
environment/networking/compute-time requirements.expression of the line8. We sample (uniformly at random)
3,000 examples from this dataset. SS TUBS does not provide
test-sets; so the only evaluation metric available is the exact-
matching of the generated text to the ground truth bug-free
text.
F . The Models
We explore confidence calibration for three code generation
models. These include OpenAI G PT-3.59, OpenAI C ODEX
[19], and C ODEGEN2-16B [43]. We sample from the models
with temperature of 0, consistent with the reality that busy
developers typically look at just the first suggestion in the
completion [44]. For function synthesis task, temperature zero
is most accurate and is fairly standard practice when doing
pass@1 with only one solution to generate and present [13],
[19].
IV. R ESULTS
We begin with a brief overview of the findings on the
correctness- & confidence- measures of LLMs on the various
tasks, and then provide detailed results on the calibration-
related research questions.
All Pass@1 Exact-Match
CodeGen2 Codex GPT-3.5 CodeGen2 Codex GPT-3.5
SStubs - - - 0.73% 27.77% 20.27%
DyPyBench 28.84% 32.96% 33.22% 19.68% 23.60% 23.96%
Defects4J 0.00% 23.33% 19.17% 0.00% 19.17% 15.00%
HumanEval 23.17% 47.24% 64.60% - - -
MBPP 29.08% 61.79% 72.04% - - -
TABLE II: Performance comparison of models on tasks. Metrics are All Pass at
Rank 1 ( All Pass@1 ), meaning all project test cases passed with the line completion on
first and only sample (at t= 0), and Exact-Match, meaning the line completion was an
exact string match with the original project line. Exact-Match is not commonly used for
function synthesis tasks, since the generated output is longer and less likely to match.
SStubs dataset does not have test cases. Boldface signifies high performing model for
task and metric.
A. RQ 1: How well are language models’ confidence in
their output aligned with the empirical correctness of their
output, specifically for common generative tasks, viz.function
synthesis, line-level code completion, and program repair?
1) Overall Correctness: Correctness performance rate of
the various models on the various tasks and datasets, are
presented in Table II. Specifically, we report the fraction of
samples passing all test cases for a given model and dataset,
and the percentage of exact-matches. We found that G PT-3.5
worked well for both function synthesis (H UMAN EVAL and
MBPP), and line-level code completion, whilst C ODEX gen-
erally performed well on program repair. The D YPYBENCH
benchmark reflects the most popular use of LLMs, viz., for
code completion.
8Note, due to data processing errors, 3 Defects4J examples have slightly
mis-localized bugs. We leave these as-is, reasoning that model confidence
should be robustly well-calibrated even with slight localization noise, ideally
giving a lower confidence of a fix if the location is noisy.
9The gpt-3.5-turbo-instruct model, https://platform.openai.com/docs/
models/gpt-3-5-turbo2) Correctness: Test-passing vs. Exact-match: As
per § III-A, we evaluate correctness both on test-passing
and exact-match. Our experiment included two datasets
(DEFECTS 4J and D YPYBENCH ), where both methods of
measuring correctness were available. Since D EFECTS 4J
consists of only 120 samples, we present the results for
DYPYBENCH ; in this case, as per Table II, G PT-3.5
performed best, with approximately 33% of generated code
passing all available tests, and approximately 24% matching
exactly.
In this setting (D YPYBENCH /GPT-3.5), we cross-tabulate
performance across the two correctness-measuring methods.
We note that approximately half the test-passing generations
didnotmatch the original code exactly; furthermore 6.89% of
the cases where the code matched exactly, did not pass all the
test cases. Upon careful study we found that these tests were
“flaky”, depending on network conditions, execution order,
and other variable execution environment conditions. This
aligns with [36], the author of this dataset, who observed an
overall 7%failure rate, but noted that 31 out of 50 projects had
zero failed tests. This illustrates the relative merits/demerits of
each correctness-evaluating approach, in practical SE settings.
Since the correctness performance is different with these two
notions of correctness, the calibration is also different, as we
see below.
3) Confidence Measures: As might be expected, the two
intrinsic measures pavg&ptotare usually somewhat, and
sometimes strongly, positively correlated with each other
within the same model, dataset, and task.
4) Calibration without Rescaling: Table III presents the
results for Line Completion, Function Synthesis, and Program
Repair for each model and the raw confidence measure, with-
out any rescaling method. We find raw confidence measures
are poorly calibrated, with inconsistent exceptions. In fact, the
raw baseline rate (using the average fraction correct without
considering the individual generation) is hard to beat; the best
skill-score is around 0.05.
For line completion, the ptotconfidence measure is slightly
worse than the baseline rate; calibration error is modest
(ECE ∼0.15). The total probability improves on the av-
erage probability, which is overconfident: the average token
probability exceeds the ∼30% overall success rate.
For function synthesis with raw measures, ptotexhibits
very poor calibration for G PT-3.5 and C ODEX , but not for
CODEGEN2 on H UMAN EVAL, while the best intrinsic mea-
sure for MBPP is pavgfor G PT-3.5 and C ODEX . The intrinsic
measures are inconsistent; with average probability showing
indicators of better calibration for G PT-3.5 and C ODEX , but
not for C ODEGEN2.
For program repair, intrinsic measures are consistently
below the base rate for both models and are as such,
poorly calibrated. There are several caveats here. First,
DEFECTS 4J is a small dataset, so findings may not
generalize. Second, C ODEGEN2 performs poorly on
DEFECTS 4J. Since C ODEGEN2 is a smaller model without
instruction tuning and relatively more limited reasoningLine Completion Function Synthesis Program Repair
DyPyBench HumanEval MBPP Defects4J SStubs
Model Metric B SS ECE B SS ECE B SS ECE B SS ECE B SS ECE
GPT-3.5 Total Prob 0.23 -0.03 0.15 0.62 -1.70 0.63 0.71 -2.50 0.71 0.25 -0.63 0.28 0.24 -0.50 0.25
Avg Prob 0.41 -0.87 0.46 0.27 -0.18 0.23 0.22 -0.09 0.14 0.68 -3.39 0.73 0.64 -2.94 0.69
Ask T/F 0.25 -0.13 0.16 0.34 -0.47 0.37 0.33 -0.64 0.38 0.15 +0.05 0.04 0.16 -0.01 0.04
Ask T/F N 0.25 -0.15 0.15 0.23 +0.01 0.19 0.22 -0.11 0.16 0.20 -0.30 0.24 0.22 -0.34 0.23
Verbalize 0.43 -0.92 0.42 0.28 -0.24 0.22 0.24 -0.17 0.17 0.58 -2.72 0.60 0.50 -2.09 0.53
Length 0.44 -0.99 0.46 0.23 -0.03 0.15 0.22 -0.10 0.16 0.53 -2.43 0.60 0.53 -2.26 0.60
Unskilled 0.22 0.00 0.00 0.23 0.00 0.00 0.20 0.00 0.00 0.15 0.00 0.00 0.16 0.00 0.00
Codex Total Prob 0.23 -0.02 0.16 0.44 -0.77 0.45 0.60 -1.52 0.60 0.25 -0.39 0.24 0.20 0.00 0.09
Avg Prob 0.46 -1.07 0.50 0.34 -0.38 0.35 0.24 -0.03 0.19 0.66 -2.68 0.69 0.58 -1.90 0.62
Ask T/F 0.24 -0.09 0.12 0.37 -0.47 0.36 0.49 -1.06 0.50 0.18 +0.01 0.07 0.19 +0.03 0.02
Ask T/F N 0.23 -0.06 0.07 0.32 -0.29 0.30 0.42 -0.79 0.43 0.25 -0.41 0.27 0.23 -0.14 0.18
Verbalize 0.38 -0.74 0.35 0.42 -0.67 0.40 0.38 -0.61 0.33 0.47 -1.65 0.50 0.43 -1.14 0.42
Length 0.43 -0.95 0.45 0.44 -0.77 0.44 0.56 -1.35 0.56 0.50 -1.79 0.55 0.56 -1.78 0.59
Unskilled 0.22 0.00 0.00 0.25 0.00 0.00 0.24 0.00 0.00 0.18 0.00 0.00 0.20 0.00 0.00
CodeGen2 Total Prob 0.21 -0.02 0.15 0.23 -0.30 0.23 0.29 -0.41 0.29 - - - - - -
Avg Prob 0.44 -1.16 0.50 0.60 -2.39 0.66 0.58 -1.80 0.61 - - - - - -
Ask T/F 0.23 -0.10 0.14 0.25 -0.38 0.24 0.25 -0.19 0.21 - - - - - -
Ask T/F N 0.33 -0.59 0.35 0.39 -1.19 0.45 0.39 -0.88 0.43 - - - - - -
Verbalize 0.42 -1.04 0.41 0.43 -1.39 0.42 0.40 -0.94 0.38 - - - - - -
Length 0.47 -1.28 0.51 0.38 -1.14 0.42 0.33 -0.58 0.28 - - - - - -
Unskilled 0.21 0.00 0.00 0.18 0.00 0.00 0.21 0.00 0.00 - - - - - -
TABLE III: Calibration measured as raw, non-scaled Brier Score ( B,↓lower better), Skill Score ( SS,↑higher better), and Expected Calibration Error ( ECE ,↓lower
better), with respect to “all passed” notion of correctness, except SStubs which is “exact-match”. CodeGen2 repair values are omitted as it does not perform the task with greater
than 1% accuracy. The “Unskilled” row corresponds to a naive approach where the confidence is always returned as the base correctness rate, with Skill Score ( SS) always zero
by definition.
capabilities it gets “distracted” by the buggy version shown
in the prompt: it often just repeats the buggy lines. With
very few correct outputs, the estimation of the confidence
measure becomes unreliable. Therefore, we have removed
the C ODEGEN2 results from Table III. A final caveat
is that SS TUBS uses only exact-match as a correctness
measure, which is quite different from a test passing measure.
In general, non-scaled confidence measures are only well calibrated
for exact match on code completion; for test-passing correctness, they
are poorly calibrated.
B. RQ 2: Can alignment between LLM confidence in gen-
erated code, and its correctness, be improved by confidence
rescaling?
Table IV shows the results after applying Platt scaling to
all measures (See § III-D). Figure 2a shows a reliability plot
before rescaling, and its equivalent after rescaling in Figure 2b.
Rescaling can improve calibration. Considering all values,
ECE improves from an average of 0.32 to 0.03. For just
those measures with a post-scaling SSof at least 0.05, ECE
improves from 0.38 to 0.05.
a) Understanding “Bucket Collapse”: Platt scaling can
lead to deceptively low ECE . If a confidence measure is
poorly aligned with correctness, Platt-scaling can rescale
(squash) all the confidence values to the baseline rate; this
places all samples in a single confidence value bucket where
probability exactly matches the baseline rate of correctness,
resulting in an ECE near 0. This indicates the problem of only
considering ECE .SSand Brier, on the other hand, would
reveal the poor utility of the confidence measure. Thus when
applying rescaling, it is important to consider Skill Score,
rather than only Brier and ECE .
P(estimate)0.00.20.40.60.81.0P(correct)
Rescaling: None
All Pass @1: 33%
ref: 0.22
ECE: 0.46
: 0.41
SS: -0.87DyPyBench GPT-3.5 Reliability Plot
0.00.10.20.30.40.50.60.70.80.91.0
P(estimate)05001000Count(a) DyPyBench, nonscaled relia-
bility plot
P(estimate)0.00.20.40.60.81.0P(correct)
Rescaling: Platt
All Pass @1: 33%
ref: 0.22
ECE: 0.04
: 0.20
SS: 0.08DyPyBench GPT-3.5 Reliability Plot
0.00.10.20.30.40.50.60.70.80.91.0
P(estimate)05001000Count(b) DyPyBench, Platt scaled reli-
ability plot
Fig. 2: Reliability plots for D YPYBENCH line-level code completion tasks, with
respect to All Pass @1 correctness measure and Average Token Probability confidence
measure. G PT-3.5 was used for both experiments. Bottom histogram represents number
of samples in each bin. Bref refers to the unskilled predictor Brier, ECE to Expected
Calibration Error, Bto Brier Score, and SSto Skill Score. Red & purple lines represent
scaled & non-scaled quantile bins rather than evenly spaced bins with 1/5 of the data at
each point. The left nonscaled plot shows over-confidence, as the confidence estimate is
high, but the actual correctness is low. The scaled plot (right) improves calibration.
b) Results: After rescaling, only the intrinsic measures
show skill improvement over the baseline rate for line com-
pletion. pavgandptotare similarly calibrated. The calibra-
tion and skill appears roughly consistent between all three
models in this case. Rescaling improves calibration results
for function synthesis. The ptotmeasure reaches a SSof
0.15 for H UMAN EVAL. Rescaling useful improvement for
reflective prompts as well bringing SSandECE to similar
values (discussed further in Section IV-C). For program repair,
rescaling doesn’t improve skill score, for any measure.Line Completion Function Synthesis Program Repair
DyPyBench HumanEval MBPP Defects4J SStubs
Model Metric B SS ECE B SS ECE B SS ECE B SS ECE B SS ECE
GPT-3.5 Total Prob 0.21 +0.07 0.03 0.20 +0.15 0.09 0.19 +0.07 0.05 0.16 -0.05 0.16 +0.03
Avg Prob 0.20 +0.08 0.04 0.23 -0.02 0.20 0.00 0.16 -0.03 0.16 +0.02
Ask T/F 0.22 0.00 0.20 +0.12 0.11 0.18 +0.09 0.06 0.15 +0.05 0.16 0.00
Ask T/F N 0.22 0.00 0.20 +0.14 0.07 0.18 +0.11 0.04 0.15 +0.04 0.16 +0.01
Verbalize 0.22 0.00 0.24 -0.05 0.20 +0.02 0.17 -0.09 0.16 0.00
Length 0.22 0.00 0.24 -0.03 0.20 +0.01 0.16 -0.06 0.16 0.00
Unskilled 0.22 0.00 0.23 0.00 0.20 0.00 0.16 0.00 0.16 0.00
Codex Total Prob 0.20 +0.09 0.03 0.22 +0.11 0.08 0.22 +0.06 0.04 0.18 -0.01 0.19 +0.05 0.02
Avg Prob 0.20 +0.09 0.04 0.22 +0.14 0.07 0.21 +0.12 0.06 0.18 -0.02 0.19 +0.05 0.02
Ask T/F 0.22 0.00 0.24 +0.03 0.24 0.00 0.18 0.00 0.19 +0.04
Ask T/F N 0.22 0.00 0.24 +0.03 0.24 0.00 0.18 -0.01 0.20 +0.02
Verbalize 0.22 0.00 0.26 -0.02 0.24 -0.01 0.18 0.00 0.20 0.00
Length 0.22 +0.01 0.26 -0.03 0.24 -0.01 0.20 -0.10 0.20 0.00
Unskilled 0.22 0.00 0.25 0.00 0.24 0.00 0.18 0.00 0.20 0.00
CodeGen2 Total Prob 0.19 +0.08 0.04 0.18 0.00 0.21 0.00 - - - -
Avg Prob 0.19 +0.07 0.02 0.17 +0.03 0.21 0.00 - - - -
Ask T/F 0.21 0.00 0.18 -0.01 0.20 +0.01 - - - -
Ask T/F N 0.21 0.00 0.17 +0.04 0.21 0.00 - - - -
Verbalize 0.21 0.00 0.18 -0.01 0.21 0.00 - - - -
Length 0.21 0.00 0.18 -0.02 0.21 0.00 - - - -
Unskilled 0.21 0.00 0.18 0.00 0.21 0.00 - - - -
TABLE IV: Calibration measured as Platt-scaled Brier Score ( B,↓lower better), Skill Score ( SS,↑higher better), and Expected Calibration Error ( ECE ,↓lower better),
with respect to “all passed” notion of correctness, except SStubs which is “exact-match”. In cases where the SSis less than 0.05, the ECE is omitted. This is because an estimate
without any signal will become Platt-scaled to approximately the base rate. This will appear as one well calibrated bin, resulting in an ECE near zero, but does not provide
information. CodeGen2 repair values are omitted as it does not perform the task with greater than 1% accuracy.
c) Is Rescaling a Panacea for Calibration?: Rescaling
typically improves calibration; it has been used in settings
other than generative models of code, with other notions
of correctness [22], [25], [45]–[49]. However, there are
disadvantages. First, “bucket collapse” (see § IV-B0a) can
mislead with deceptively low ECE . Second, some correctness
data is needed to fit rescaling parameters. When sweeping
through various sized bootstrapped subsets of the data, we
find that it can take over 64 data points for the rescaling to
result in positive skill and lower ECE , with improvements
continuing into 100s of data points (see Appendix figure A8
for bootstrapping analysis). When using the full data, the
rescaling between tasks can vary dramatically.10Ideally, we
want confidence measures which are reliable and allow trust-
worthy auditing even when applying language models to new
software engineering tasks. To study how close we are to
this, we fit rescaling parameters to one task, and then apply
it to the other tasks (see Appendix figure A7). We find it
is viable to use rescaling between tasks of the same domain
with similar base rates, such as within the program synthesis
tasks. For example, for G PT-3.5 when fitting pNB(results in
next section) rescaling to each of two function synthesis tasks,
and then applying it to the other, we observe an average drop
ofSSfrom 0.14 →0.12, and average drop of ECE from
0.07→0.05. However, applying the ptotal rescaling fit on
DYPYBENCH to the function synthesis tasks, results in an
average SSchange of 0.12 →-1.28 and ECE change of
0.08→0.54, indicating a lack of robustness. These reasons
suggest one must be careful when analyzing and reporting
calibration results based on rescaling, and highlights the need
10As seen in Appendix figure A6, which shows the curves learned between
measure-task pairs.for further work on confidence measures that might be more
directly calibrated.
Without rescaling, total probability ptotalshows hints of
calibration. With rescaling, there is a possible 10-20% im-
provement over baseline rates and good calibration, but it
is inconsistent as skill is poor for C ODEGEN2 on Function
Synthesis.
Rescaling is an effective technique for improving calibration, but
metric improvements ( BandECE ) may be misleading by matching
the base rate in a “bucket collapse” scenario or can lack generalization.
C. RQ 3: Is confidence obtained by reflection better aligned
with correctness?
The two logit-based reflective measures pB&pNBare usu-
ally strongly positively correlated with one another, since they
are calculated from similar numbers. The reflective verbalized
self-ask confidence measure pv, and the two logit-based re-
flective confidence measures have no consistent relationships.
For function synthesis with raw measures, pNBshows best
calibration for G PT-3.5, slightly better than Unskilled, for
HUMAN EVAL. For program repair, we observe the strongest
best-case performance with regards to the metrics. Both G PT-
3.5 and C ODEX show positive SSand low ECE forpB,
but they are inconsistent after normalization ( pNB). These
metrics suggest with reflection, these models’ confidence is
calibrated regarding repair correctness; however, further analy-
sis (see Appendix figure A3) does not indicate good calibration
on this task, from any confidence measure. We find that in
general, the intrinsic vs.reflective measure values have no
consistent relationship, even for a given model, dataset and
task.
This lack of relationship may not necessarily be negative:
e.g., perhaps the model’s reflective, prompted confidence maybe better calibrated, as suggested by prior work [20], [50].
Without rescaling or few-shot prompting, reflective results are
inconsistent. In some cases, such as G PT-3.5 H UMAN EVAL
and D EFECTS 4J, there are signs of calibration with slightly
positive SSvalues and ECE values less than 0.2. Normal-
izing the T/F values induces some difference; but there are
inconsistencies vis-`a-vis tasks and models. For nonscaled G PT-
3.5 results, pNBimproves calibration in line completion and
function synthesis by an average of -0.34 SSand 0.19 ECE ,
but not for program repair or for C ODEGEN2. Rescaling
generally removes any sign of a normalization trend. For the
alternative reflective approach of verbalization, the probability
is not well calibrated for these models on the studied SE tasks.
In some cases, the reflective approaches are best calibrated
without rescaling (see Table III), and show signs of being more
robust when reusing learned rescaling parameters on unseen
tasks (see Appendix figure A7).
Reflective approaches may be best calibrated “out-of-the-box” and in
some settings when rescaled. However, in the tested settings, they do
not improve significantly over intrinsic measures.
D. RQ 4: Can we use few-shot techniques to achieve bet-
ter calibrated confidence for code completion, using an
instruction-tuned model with in-context learning?
We investigated the impact of few-shotting viz.providing a
model completion and correctness as part of the pNBprompt,
on calibration [18]. To effectively perform few-shotting, we
need a model that is instruction tuned and sufficiently large,
which is best matched by G PT-3.5. We explore few-shotting
for the widely-used line completion task.
We perform the experiment with 5-shots consisting of prior
completions from the same experiments presented in Table IV,
the reflective question, and the ground truth True/False. We
try two variants of this experiment, one where the examples
are randomly selected, and one where they are chosen based
on the similarity to the unanswered prompt. In both cases, we
exclude the ground truth result for the unanswered prompt. We
focus on Line Completion for this experiment as it represents
widespread use and has a large number of examples available.
Confidence Measure B ↓ SS↑ECE↓
0-Shot Reflect 0.25 -0.15 0.15
0-Shot Reflect (Rescaled) 0.22 0.00
FS Random 0.29 -0.29 0.21
FS Random (Rescaled) 0.22 0.0
FS BM25 0.20 0.08 0.10
FS BM25 (Rescaled) 0.19 0.15 0.02
TABLE V: Few-shot reflective prompting using G PT-3.5 for line completion. ‘FS
Random’ refers to selecting random few-shot examples. ‘FS BM25’ retrieves more
relevant known completions. ECE values when rescaled values SS are close to zero
are omitted (to avoid confusion with “bucket collapse”, Section IV-B0a)
For line completion, the non-scaled results using random
examples did not result in improved calibration over the
baseline pNB, however using BM25 [6] to select similar
examples yielded a positive SS of 0.08, which could beimproved further by rescaling, up to 0.15. This result notably
exceeds any other measure for D YPYBENCH , and significantly
improves over the baseline pNBSSof 0.
While random few-shotting requires limited extra data,
BM25 is more similar to rescaling, in that it is dependent
on a larger set of ground truths. This could be actualized by
logging user completions, and building up ground truths (on
if the completion was correct) based off the test case runs or
acceptance of completions.
Providing a few examples selected via BM25 when asking G PT-3.5 to
reflect on its own completion output significantly improves reflective
calibration.
Alternative and improved ways of prompting ( e.g., different
verbalization formats [15], [17], fine-tuning [15], [18], chain-
of-thought [17], [51], etc.) may alter these findings and are
areas for future work.
V. D ISCUSSION
Language models are now widely-integrated into Software
Engineering practice, via tools like Copilot [32] and Didact11.
We raise here the importance of calibration when integrating
generative LLMs in coding practice. We evaluate the cali-
bration of generative LLM use (especially code completion)
with large samples of realistic data (D YPYBENCH , SS TUBS ),
using widely adopted models, as well as some more academic
datasets (H UMAN EVAL, MBPP).
a) Using a well-calibrated model–beyond simple defect
prediction: To clarify how a well-calibrated model enables
more well-grounded decision-making concerning generated
outputs, as compared to as compared to a traditional pro-
cess choosing a binary decision point—we consider G PT-3.5
working on code completion, where correctness is determined
by test-passing, and confidence is assigned by few-shotting,
average token probability. The base correctness (test-passing)
rate of completions is about 33%. With few-shotting, we get
a very high skill score of 0.15 (Section IV-D, Table V).
If we didn’t have a well-calibrated model, we might very
cautiously accept only those completions that are generated
at a very high-confidence threshold; here, the FP rate could
be low (of course TP rate would be low as well). While this
may lower the risk of bad code, it also regrettably reduces
the available help from the LLM. However, a well-calibrated
confidence measure allows a more rational, graduated set of
decisions. Such a well-calibrated measure is visualized in
Figure 3. In this setting, for much of the confidence scale,
a user could look at the confidence level, and get a very
good idea of how likely the code is to be correct, and
make a well-reasoned, situation-specific set of decisions to
manage risk, and allocate reviewing resources, based on the
model’s confidence. This provides an illustration of the greater
benefit provided a well-calibrated measure (high Skill level,
low ECE) over one that is just providing good precision-
recall trade-off (or, ROC curve); the latter does not allow
such graduated deployment of quality-control effort. However,
11blog.research.google/2023/05/large-sequence-models-for-software.htmlP(estimate)0.00.20.40.60.81.0P(correct)
Rescaling: Platt
All Pass @1: 33%
ref: 0.22
ECE: 0.02
: 0.19
SS: 0.15
105465424359266180119 3729 3Few-shot Reflective Reliability Plot
0.00.10.20.30.40.50.60.70.80.91.0
P(estimate)05001000CountFig. 3: Few-shot reflective reliability plot, based on “FS BM25” row of
Table V
developers would need to learn to use calibrated probabilities
in decision-making.
b) Beyond simple “correctness”: In addition to the above
uses, which considered a single notion of “correctness”, one
could consider a multi-class correctness prediction task, where
the model could indicate the confidence in correctness (the
absence of defects) from multiple perspectives: severity of pos-
sible defect, the kind of defect (relating to security, integrity,
privacy, fairness, etc.) and defect complexity (indicating the
cost or schedule impact of repairs). Drawing an analogy
to classical forecasting, this is analogous to not just the
probability it will rain, but probability it will be a drizzle or
be a drenching thunderstorm.
c) Why calibration now?: We’ve always had bugs; poor-
quality code isn’t new. Our push for calibration, however,
arises from the increasing amount of code generated by LLM.
GitHub claims that up to 61% of code12in some systems
is generated by LLMs. It is also known that LLMs make a
lot of mistakes. A recent paper has reported [1] that LLMs,
even when trained on properly fixed code, tends to recapitulate
thethe old unfixed, buggy code when prompted in context.
However, LLMs do have very high capacity, and a demon-
strated ability to usefully reflect [50], [52] on their generated
text. Thus, we have both a high risk (of buggy code), and
a chance to improve productivity. We believe that improved
calibration could lead to better management of the risk-benefit
of LLM-generated code. The studied correctness calibration is
a stepping stone for more complex notions of confidence (like
severity and localized confidence). Additionally, by studying
code LLMs, we might make progress on the general safe
deployment of capable generative models [10].
d) Summarizing per-token probabilities: To produce a
summary confidence for generated token sequences in Ta-
bles III and IV, in Tables III & IV, we used (arithmetic mean)
average & product to summarize the per-token probabilities.
12github.blog/2023-02-14-github-copilot-now-has-a-better-ai-model-and-
new-capabilitiesIn this setting, it might be more reasonable to use geometric
mean (as in [53]) to get a product value normalized for length;
indeed, when we tried that, we found that Brier and skill scores
improved marginally, but consistently so; future research could
indicate whether these findings generalize.
VI. T HREATS TO VALIDITY
a) Sample Size & Generalizability: While three of our
datasets contain more than 800+ samples each, H UMAN EVAL
and D EFECTS 4J datasets consist of only 164 and 120 samples,
respectively. Results on these datasets may not generalize.
However, we note that our study has a large and natural dataset
for the line-level code completion task, which has current
practical importance. Given the noise and variance we observe,
we recommend future work push towards larger and more
natural datasets, in particular for Function Synthesis & Repair.
While some of our treatments ( e.g., few-shotting) suggest
substantial improvements in skill score (Table V), in other
cases such as the different approaches to summarize per-token
confidence differences, the differences are less clear. In future
work, these differences could be judged more robustly using
bootstrapped p-values and effect sizes.
b) Artificial vs.real world data: For function synthesis,
we used the popular H UMAN EVAL and MBPP function syn-
thesis datasets. These datasets contain small-ish Python pro-
grams that may not represent real-world software development
functions. However, our other datasets, such as D YPYBENCH
and SS TUBS are more representative of real-world, open-
source GitHub projects.
c) Model Selection: Results might not generalize to all
models, especially those with greatly differing training/fine-
tuning or different architectures.
d) Experimental Design: Our exploration is not exhaus-
tive; other SE tasks and datasets also could benefit from
calibration studies. Additionally, the specific prompts we used
for this paper surely played a role in our findings. Other
prompts or problem phrasings (such as different forms of con-
text for line-level code completion) may yield different results.
Regarding test flakiness: the test “flake” rate of DyPyBench
is not zero, but is quite low and not unrealistic [14].
Despite these caveats, our study, which includes three tasks
and five datasets, provides a good starting point for further
studies.
VII. R ELATED WORK
LLMs for code are extensively studied [54], [55]. While
calibration has a long history in modeling [8], [56], it is not
a frequently studied topic in the SE community. Early work
moving into modern machine learning studied the calibration
of smaller neural models performing classification tasks on
text and images; while these early models were poorly cali-
brated per se , their performance could be improved by simple
scaling [22] of their output probabilities. As models became
larger, calibration was found to improve [57]. Pre-training was
also found to improve calibration [25], [58]; however, these
findings have been disputed [47].More recent works evaluated LLM calibration on a wide
variety of settings [7], [18], [25], [59]. Desai et al. [25]
studied non-code (natural language) tasks such as inference
or paraphrasing, with only intrinsic measures using older-
generation models (BERT and RoBERTA). Jiang et al. [7]
studied calibration for natural language question-answering
using just intrinsic measures. In contrast, we study calibration
for three coding-related tasks, using both artificial and natural
code datasets, and both intrinsic and reflective confidence
measures, to evaluate calibration in the SE domain.
Other prior work has investigated tokens that might be
edited. Vasconcelos et al. [60] discusses code model uncer-
tainty for function-synthesis-style problems, and ran human
evaluation of the usefullness of colored highlighting of uncer-
tain tokens. They found highlighting a human-derived ground-
truth of which tokens might be edited was helpful, and more
useful than raw token probabilities from the model. Johnson
et al. [61] developed method of highlighting likely edit tokens
via a utility optimization algorithm comparing different file
completions. We find exploring more on calibrated uncertainty
for local areas be a interesting area for additional work.
Liet al. [49] investigate the calibration of Computer vision
(CV) models from an operational perspective i.e., the shift
between training input and production inputs, presenting it
as a software quality problem that can be addressed us-
ing Bayesian approaches. Minderer et al. [45] evaluate the
calibration of at the time, state of the art CV models and
find improved calibration with more recent models, notably
those not using convolutions. Park et al. [46] study the
effect of the mixup technique [62] on calibration in a natural
language understanding (NLU) setting using older generation
models (BERT and RoBERTa). Chen et al. [47] investigate
the calibration of pretrained language models on various NLP
tasks, also using older generation models (RoBERTa and T5).
Bommasani et al. [48] introduce the HELM benchmark, which
includes calibration as one of its seven metrics to evaluate
language models in a natural language context. Huang et al.
[63] explored LM uncertainty with a range of techniques and
tasks, including both NLP and function synthesis tasks. They
evaluated using correlation measures, rather than focusing
on calibration. They explore interesting sample-based and
perturbation techniques which could be explored more for
calibration on diverse SE tasks. Other work [64] has explored
training an ML model that sees code and execution results to
estimate correctness probabilities for solution reranking. For
natural language question answering tasks, work has explored
improving calibration by training a model to adjust token logits
[53], and training a model from LLM hidden states specifically
around the ECE metric [65].
When suitably prompted, Kadavath et al. [18] found that
LLMs can output well-calibrated scores on whether their own
answers are correct or not, viz., larger models “know what
they know”. While this work did investigate some function
synthesis tasks (H UMAN EVAL & an unpublished Python func-
tion dataset), they did so using only their private models, and
ultimately focused on natural language tasks. Key et al. [59]developed an approach that given a natural language problem
description, produces a confidence score for a sampled candi-
date solution based on generated specifications, allowing them
to judge whether the LLM can solve the problem at all. Their
metrics include calibration. Recent work has also explored
calibration of software topics such as root cause analysis [66].
VIII. C ONCLUSION
In this paper, we begin with the observation that while
LLMs are often helpful (for example producing code-
completions for developers) they often produce buggy code.
We argue that a well-calibrated confidence score, could pro-
vide a reliable indication of whether the generated code was
correct, and help more rational, graduated quality-control of
of LLM-generated code We studied the calibration of intrinsic
and reflective confidence measures in several practical settings
(completion and repair) and a widely-used competitive setting
(synthesis), across several LLMs. We find that LLMs are
generally poorly calibrated out of the box, across a variety
of confidence measures (both intrinsic and reflective) We then
found that Platt scaling generally results in somewhat better
calibrated confidence measures.
Finally, we focused in on a) coding task where LLMs
are most widely-deployed, viz. code completion, and b) a
very widely used instruction-tuned model, viz.GPT-3.5, and
investigated whether a reflective, in-context learning approach
(few-shotting) could provide better calibrated confidence mea-
sures. In this setting, we found that calibration improves
substantially, reaching a skill score of 0.15, particularly with
retrieval augmented few-shotting.
To our knowledge, our paper is the first to consider the
problem of calibration in a real-world code generation setting.
We do find that most models, both out-of-the-box and with
simple reflection, don’t provide reliable confidence measures.
However, our results with retrieval-augmented few-shotting are
very encouraging, and point towards a future where Language
Models could provide developers with guidance on how to
quality-control the code they generate.
IX. A CKNOWLEDGMENTS
We acknowledge partial support for this work by the Intel-
ligence Advanced Research Projects Agency (IARPA) under
contract W911NF20C0038, the National Science Foundation
under CISE SHF MEDIUM 2107592, the European Research
Council (ERC grant agreement 851895), and the German Re-
search Foundation (ConcSys, DeMoCo, and QPTest projects).
Devanbu was supported by a Humboldt Research Award13.
Our conclusions do not necessarily reflect the position or the
policy of our sponsors and no official endorsement should be
inferred.
13https://www.humboldt-foundation.de/en/connect/
explore-the-humboldt-network/singleview/1226147/
prof-dr-premkumar-t-devanbuREFERENCES
[1] K. Jesse, T. Ahmed, P. T. Devanbu, and E. Morgan,
“Large Language Models and Simple, Stupid Bugs,” in
2023 IEEE/ACM 20th International Conference on Min-
ing Software Repositories (MSR) , May 2023, pp. 563–
575.
[2] O. Asare, M. Nagappan, and N. Asokan, “Is GitHub’s
Copilot as bad as humans at introducing vulnerabilities
in code?” Empirical Software Engineering , vol. 28,
no. 6, p. 129, Sep. 23, 2023, ISSN : 1573-7616.
[3] R. Schuster, C. Song, E. Tromer, and V . Shmatikov,
“You Autocomplete Me: Poisoning Vulnerabilities in
Neural Code Completion,” presented at the 30th
USENIX Security Symposium (USENIX Security 21),
2021, pp. 1559–1575, ISBN : 978-1-939133-24-3.
[4] D. Lo. “Trustworthy and Synergistic Artificial In-
telligence for Software Engineering: Vision and
Roadmaps.” arXiv: 2309.04142 [cs] . (Oct. 4, 2023),
preprint.
[5] Z. Zhou, C. Sha, and X. Peng, “On calibration of pre-
trained code models,” in 2024 IEEE/ACM 46th Inter-
national Conference on Software Engineering (ICSE) ,
IEEE Computer Society, 2024, pp. 861–861.
[6] N. Nashid, M. Sintaha, and A. Mesbah, “Retrieval-
based prompt selection for code-related few-shot learn-
ing,” in 2023 IEEE/ACM 45th International Conference
on Software Engineering (ICSE) , 2023, pp. 2450–2462.
DOI: 10.1109/ICSE48619.2023.00205.
[7] Z. Jiang, J. Araki, H. Ding, and G. Neubig, “How
Can We Know When Language Models Know? On the
Calibration of Language Models for Question Answer-
ing,” Transactions of the Association for Computational
Linguistics , vol. 9, pp. 962–977, Sep. 8, 2021, ISSN :
2307-387X.
[8] G. W. Brier, “Verification of forecasts expressed in
terms of probability,” Monthly Weather Review , vol. 78,
no. 1, pp. 1–3, Jan. 1, 1950, ISSN : 1520-0493, 0027-
0644.
[9] M. P. Naeini, G. Cooper, and M. Hauskrecht, “Obtain-
ing Well Calibrated Probabilities Using Bayesian Bin-
ning,” Proceedings of the AAAI Conference on Artificial
Intelligence , vol. 29, no. 1, 1 Feb. 21, 2015, ISSN : 2374-
3468.
[10] D. Gros, P. Devanbu, and Z. Yu. “AI Safety Subprob-
lems for Software Engineering Researchers.” arXiv:
2304.14597 [cs] . (Aug. 31, 2023), preprint.
[11] D. M. Nierman, C. B. Schechter, L. M. Cannon, and
D. E. Meier, “Outcome prediction model for very
elderly critically ill patients,” Critical Care Medicine ,
vol. 29, no. 10, p. 1853, Oct. 2001, ISSN : 0090=3493.
[12] J. Schwarz and D. Heider, “GUESS: Projecting machine
learning scores to well-calibrated probability estimates
for clinical decision-making,” Bioinformatics , vol. 35,
no. 14, pp. 2458–2465, Jul. 15, 2019, ISSN : 1367-4803.[13] J. Liu, C. S. Xia, Y . Wang, and L. Zhang, “Is Your
Code Generated by ChatGPT Really Correct? Rigorous
Evaluation of Large Language Models for Code Gen-
eration,” Advances in Neural Information Processing
Systems , vol. 36, pp. 21 558–21 572, Dec. 15, 2023.
[14] Q. Luo, F. Hariri, L. Eloussi, and D. Marinov, “An
empirical analysis of flaky tests,” in Proceedings of
the 22nd ACM SIGSOFT International Symposium on
Foundations of Software Engineering , ser. FSE 2014,
New York, NY , USA: Association for Computing Ma-
chinery, Nov. 11, 2014, pp. 643–653, ISBN : 978-1-4503-
3056-5.
[15] S. Lin, J. Hilton, and O. Evans, “Teaching Models to
Express Their Uncertainty in Words,” Transactions on
Machine Learning Research , Jun. 19, 2022, ISSN : 2835-
8856.
[16] K. Zhou, D. Jurafsky, and T. Hashimoto, “Navi-
gating the Grey Area: How Expressions of Uncer-
tainty and Overconfidence Affect Language Models,”
inEMNLP’23 , H. Bouamor, J. Pino, and K. Bali, Eds.,
Singapore: Association for Computational Linguistics,
Dec. 2023, pp. 5506–5524.
[17] K. Tian et al. , “Just Ask for Calibration,” in Proceed-
ings of the 2023 Conference on Empirical Methods in
Natural Language Processing , H. Bouamor, J. Pino, and
K. Bali, Eds., Singapore: Association for Computational
Linguistics, Dec. 2023, pp. 5433–5442.
[18] S. Kadavath et al. “Language Models (Mostly) Know
What They Know.” arXiv: 2207.05221 [cs] . (Nov. 21,
2022), preprint.
[19] M. Chen et al. “Evaluating Large Language Models
Trained on Code.” arXiv: 2107.03374 [cs] . (Jul. 14,
2021), preprint.
[20] J. Austin et al. “Program Synthesis with Large Lan-
guage Models.” arXiv: 2108.07732 [cs] . (Aug. 15,
2021), preprint.
[21] N. Jiang, K. Liu, T. Lutellier, and L. Tan, “Impact
of Code Language Models on Automated Program
Repair,” in ICSE’23’ , May 2023, pp. 1430–1442.
[22] C. Guo, G. Pleiss, Y . Sun, and K. Q. Weinberger, “On
Calibration of Modern Neural Networks,” in Proceed-
ings of the 34th International Conference on Machine
Learning , PMLR, Jul. 17, 2017, pp. 1321–1330.
[23] J. Nixon et al. , “Measuring Calibration in Deep Learn-
ing,” presented at the Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recogni-
tion Workshops, 2019, pp. 38–41.
[24] J. Platt et al. , “Probabilistic outputs for support vector
machines and comparisons to regularized likelihood
methods,” Advances in large margin classifiers , vol. 10,
no. 3, pp. 61–74, 1999.
[25] S. Desai and G. Durrett, “Calibration of Pre-trained
Transformers,” in Proceedings of the 2020 Conference
on Empirical Methods in Natural Language Processing
(EMNLP) , B. Webber, T. Cohn, Y . He, and Y . Liu,Eds., Online: Association for Computational Linguis-
tics, Nov. 2020, pp. 295–302.
[26] B. Zadrozny and C. Elkan, “Obtaining calibrated prob-
ability estimates from decision trees and naive Bayesian
classifiers,” in ICML’01 , C. E. Brodley and A. P. Dany-
luk, Eds., Morgan Kaufmann, 2001, pp. 609–616.
[27] B. Zadrozny and C. Elkan, “Transforming classifier
scores into accurate multiclass probability estimates,”
inKDD ’02 , New York, NY , USA: Association for
Computing Machinery, Jul. 23, 2002, pp. 694–699,
ISBN : 978-1-58113-567-1.
[28] M. Kull et al. , “Beyond temperature scaling: Obtaining
well-calibrated multi-class probabilities with Dirichlet
calibration,” in Advances in Neural Information Pro-
cessing Systems , vol. 32, Curran Associates, Inc., 2019.
[29] M. Izadi, R. Gismondi, and G. Gousios, “CodeFill:
Multi-token code completion by jointly learning from
structure and naming sequences,” in ICSE’22 , ser. ICSE
’22, New York, NY , USA: Association for Computing
Machinery, Jul. 5, 2022, pp. 401–412, ISBN : 978-1-
4503-9221-1.
[30] S. Kim, J. Zhao, Y . Tian, and S. Chandra, “Code Pre-
diction by Feeding Trees to Transformers,” in ICSE’21 ,
May 2021, pp. 150–162.
[31] S. Lu et al. , “CodeXGLUE: A Machine Learning
Benchmark Dataset for Code Understanding and Gen-
eration,” NeurIPS , vol. 1, Dec. 6, 2021.
[32] A. Ziegler et al. , “Productivity assessment of neural
code completion,” in MAPS 2022 , New York, NY , USA:
Association for Computing Machinery, Jun. 13, 2022,
pp. 21–29, ISBN : 978-1-4503-9273-0.
[33] T. Brown et al. , “Language Models are Few-Shot
Learners,” in Advances in Neural Information Process-
ing Systems , vol. 33, Curran Associates, Inc., 2020,
pp. 1877–1901.
[34] M. Allamanis and C. Sutton, “Mining source code
repositories at massive scale using language modeling,”
in2013 10th Working Conference on Mining Software
Repositories (MSR) , May 2013, pp. 207–216.
[35] V . Raychev, P. Bielik, and M. Vechev, “Probabilistic
model for code with decision trees,” in OOPSLA 2016 ,
New York, NY , USA: Association for Computing Ma-
chinery, Oct. 19, 2016, pp. 731–747, ISBN : 978-1-4503-
4444-9.
[36] I. Bouzenia, B. P. Krishan, and M. Pradel, “DyPyBench:
A benchmark of executable python software,” in Foun-
dations of Software Engineering FSE , 2024.
[37] H. Husain et al. “CodeSearchNet Challenge: Evaluating
the State of Semantic Code Search.” arXiv: 1909.09436.
(Jun. 8, 2020), preprint.
[38] C. L. Goues, M. Pradel, and A. Roychoudhury, “Au-
tomated program repair,” Communications of the ACM ,
vol. 62, no. 12, pp. 56–65, Nov. 21, 2019, ISSN : 0001-
0782.
[39] Z. Fan et al. , “Automated Repair of Programs from
Large Language Models,” in 2023 IEEE/ACM 45thInternational Conference on Software Engineering
(ICSE) , May 2023, pp. 1469–1481.
[40] T. Ahmed and P. Devanbu, “Better Patching Using
LLM Prompting, via Self-Consistency,” in 2023 38th
IEEE/ACM International Conference on Automated
Software Engineering (ASE) , Sep. 2023, pp. 1742–1746.
[41] R. Just, D. Jalali, and M. D. Ernst, “Defects4J: A
database of existing faults to enable controlled test-
ing studies for Java programs,” in ISSTA 2014 , New
York, NY , USA: Association for Computing Machinery,
Jul. 21, 2014, pp. 437–440, ISBN : 978-1-4503-2645-2.
[42] R.-M. Karampatsis and C. Sutton, “How Often Do
Single-Statement Bugs Occur? The ManySStuBs4J
Dataset,” in MSR ’20 , New York, NY , USA: Association
for Computing Machinery, Sep. 18, 2020, pp. 573–577,
ISBN : 978-1-4503-7517-7.
[43] E. Nijkamp et al. “CodeGen2: Lessons for Training
LLMs on Programming and Natural Languages.” arXiv:
2305.02309 [cs] . (Jul. 11, 2023), preprint.
[44] S. Barke, M. B. James, and N. Polikarpova, “Grounded
Copilot: How Programmers Interact with Code-
Generating Models,” Proceedings of the ACM on Pro-
gramming Languages , vol. 7, 78:85–78:111, OOPSLA1
Apr. 6, 2023.
[45] M. Minderer et al. , “Revisiting the Calibration of Mod-
ern Neural Networks,” in NeurIPS , vol. 34, Curran
Associates, Inc., 2021, pp. 15 682–15 694.
[46] S. Y . Park and C. Caragea, “On the Calibration of Pre-
trained Language Models using Mixup Guided by Area
Under the Margin and Saliency,” in Proceedings of the
60th Annual Meeting of the Association for Computa-
tional Linguistics (Volume 1: Long Papers) , S. Muresan,
P. Nakov, and A. Villavicencio, Eds., Dublin, Ireland:
Association for Computational Linguistics, May 2022,
pp. 5364–5374.
[47] Y . Chen et al. , “A Close Look into the Calibration of
Pre-trained Language Models,” in ACL 2023 , A. Rogers,
J. Boyd-Graber, and N. Okazaki, Eds., Toronto, Canada:
Association for Computational Linguistics, Jul. 2023,
pp. 1343–1367.
[48] R. Bommasani, P. Liang, and T. Lee, “Holistic Eval-
uation of Language Models,” Annals of the New York
Academy of Sciences , vol. 1525, no. 1, pp. 140–146,
2023, ISSN : 1749-6632.
[49] Z. Li et al. , “Operational calibration: Debugging con-
fidence errors for DNNs in the field,” in Proceedings
of the 28th ACM Joint Meeting on European Soft-
ware Engineering Conference and Symposium on the
Foundations of Software Engineering , ser. ESEC/FSE
2020, New York, NY , USA: Association for Computing
Machinery, Nov. 8, 2020, pp. 901–913, ISBN : 978-1-
4503-7043-1.
[50] Y . Bai et al. “Constitutional AI: Harmlessness from AI
Feedback.” arXiv: 2212.08073 [cs] . (Dec. 15, 2022),
preprint.[51] J. Wei et al. , “Chain-of-Thought Prompting Elicits Rea-
soning in Large Language Models,” Advances in Neural
Information Processing Systems , vol. 35, pp. 24 824–
24 837, Dec. 6, 2022.
[52] N. Shinn et al. “Reflexion: Language Agents with
Verbal Reinforcement Learning.” arXiv: 2303 . 11366
[cs] . (Oct. 10, 2023), preprint.
[53] X. Liu et al. , “Litcab: Lightweight language model
calibration over short- and long-form responses,” in
ICML , 2023.
[54] Q. Zhang et al. , “A Survey of Learning-based Auto-
mated Program Repair,” ACM Transactions on Software
Engineering and Methodology , vol. 33, no. 2, 55:1–
55:69, Dec. 23, 2023, ISSN : 1049-331X.
[55] Z. Zheng et al. “A Survey of Large Language Mod-
els for Code: Evolution, Benchmarking, and Future
Trends.” arXiv: 2311 . 10372 [cs] . (Jan. 8, 2024),
preprint.
[56] E. W. Steyerberg et al. , “Assessing the performance
of prediction models: A framework for some tradi-
tional and novel measures,” Epidemiology (Cambridge,
Mass.) , vol. 21, no. 1, pp. 128–138, Jan. 2010. pmid:
20010215.
[57] A. Srivastava et al. “Beyond the Imitation Game: Quan-
tifying and extrapolating the capabilities of language
models.” arXiv: 2206.04615 [cs, stat] . (Jun. 12,
2023), preprint.
[58] D. Hendrycks, K. Lee, and M. Mazeika, “Using Pre-
Training Can Improve Model Robustness and Uncer-
tainty,” in Proceedings of the 36th International Con-
ference on Machine Learning , PMLR, May 24, 2019,
pp. 2712–2721.
[59] D. Key, W.-D. Li, and K. Ellis. “Toward Trustworthy
Neural Program Synthesis.” arXiv: 2210.00848 [cs] .
(Oct. 9, 2023), preprint.
[60] H. Vasconcelos et al. , “Generation probabilities are
not enough: Improving error highlighting for ai code
suggestions,” in NeurIPS Workshop on Human-Centered
AI, Oct. 2022. [Online]. Available: https : / / www .
microsoft.com/en-us/research/publication/generation-
probabilities - are - not - enough - improving - error -
highlighting-for-ai-code-suggestions/.
[61] D. D. Johnson, D. Tarlow, and C. Walder, “R-u-sure?
uncertainty-aware code suggestions by maximizing util-
ity across random user intents,” in Proceedings of
the 40th International Conference on Machine Learn-
ing, ser. ICML’23, Honolulu, Hawaii, USA: JMLR.org,
2023.
[62] H. Zhang, M. Cisse, Y . N. Dauphin, and D. Lopez-
Paz, “Mixup: Beyond Empirical Risk Minimization,”
presented at the International Conference on Learning
Representations, Feb. 15, 2018.
[63] Y . Huang et al. “Look before you leap: An exploratory
study of uncertainty measurement for large language
models.” arXiv: 2307.10236 [cs.SE] . (2023).[64] A. Ni et al. , “Lever: Learning to verify language-
to-code generation with execution,” in Proceedings of
the 40th International Conference on Machine Learn-
ing, ser. ICML’23, Honolulu, Hawaii, USA: JMLR.org,
2023.
[65] X. Liu et al. , “Enhancing language model factuality
via activation-based confidence calibration and guided
decoding,” ArXiv , vol. abs/2406.13230, 2024.
[66] D. Zhang et al. “PACE-LM: Prompting and Augmenta-
tion for Calibrated Confidence Estimation with GPT-4
in Cloud Incident Root Cause Analysis.” arXiv: 2309.
05833 [cs] . (Sep. 29, 2023), preprint.APPENDIX
We have the following python code implementing a method.from typing import Listdef below_zero(operations: List[int]) -> bool:”””You're given a list of deposit and withdrawal operations on a bank account that starts with zero balance. Your task is to detect if at any point the balance of account falllsbelow zero, and at that point function should return True. Otherwiseit should return False.>>> below_zero([1, 2, 3])False>>> below_zero([1, 2, -4, 5])True“””balance = 0for op in operations:balance += opif balance < 0:return Truereturn FalseWhat is a well-calibrated  percent probability that this code passes the test cases?Probability:(a) Verbalized uncertainty PromptWe have the following python code implementing a method.from typing import Listdef below_zero(operations: List[int]) -> bool:””” You're given a list of deposit and withdrawal operations on a bank account that starts with zero balance. Your task is to detect if at any point the balance of account falllsbelow zero, and at that point function should return True. Otherwiseit should return False.>>> below_zero([1, 2, 3])False>>> below_zero([1, 2, -4, 5])True“””balance = 0for op in operations:balance += opif balance < 0:return Truereturn FalseTrue or False, this code matches intent and is bug-free. Answer: TruePrompt
(a) Question answering 
Fig. A1: Prompts for Verbalized Self-Ask and Question Answering logit.
from typing import Listdef below_zero(operations: List[int]) -> bool:”””You're given a list of deposit and withdrawal operations on a bank account that starts with zero balance. Your task is to detect if at any point the balance of account falllsbelow zero, and at that point function should return True. Otherwiseit should return False.>>> below_zero([1, 2, 3])False>>> below_zero([1, 2, -4, 5])True“””balance = 0for op in operations:balance += opif balance < 0:return Truereturn False(a) Function SynthesisPrompt
Modeloutputprivate SimpleTextAttributesgetAttributes() {if (isSynthetic() || isInLibraryContent()) {return SimpleTextAttributes. GRAYED_ATTRIBUTES;}
return SimpleTextAttributes. REGULAR_ATTRIBUTES;(b) Line CompletionPrompt
Modeloutput---------------lines omitted for space contraint---------------else if (myVariables!= null) {children = <buggy> new XValueChildrenList() </buggy>;myVariables.forEach(children::add);}node.addChildren(children, true);---------------lines omitted for space contraint---------------Question: There is a bug in the above code snippet tagged by <buggy> and </buggy>. Please generate the correct version.Answer:---------------lines omitted for space contraint---------------else if (myVariables!= null) {children = <fixed>new XValueChildrenList( myVariables.  size ( ) )(c) Program repairPrompt
Modeloutput
Fig. A2: Prompt and model output for the tasks while calculating confidence measure based on Average Token Probability and
Generated Sequence Probability.Line Completion Program Repair
DyPyBench Defects4J SStubs
Model Metric B SS ECE B SS ECE B SS ECE
GPT-3.5 Total Prob 0.11 +0.38 0.03 0.13 +0.01 0.16 +0.03
Avg Prob 0.13 +0.31 0.03 0.13 +0.01 0.16 +0.02
Ask T/F 0.18 0.00 0.12 +0.05 0.16 0.00
Ask T/F N 0.18 0.00 0.13 +0.02 0.16 +0.01
Verbalize 0.18 0.00 0.13 -0.03 0.16 0.00
Length 0.17 +0.06 0.03 0.14 -0.06 0.16 0.00
Unskilled 0.18 0.00 0.13 0.00 0.16 0.00
Codex Total Prob 0.10 +0.43 0.02 0.15 +0.01 0.19 +0.05 0.02
Avg Prob 0.12 +0.34 0.02 0.16 -0.01 0.19 +0.05 0.02
Ask T/F 0.18 +0.01 0.16 -0.01 0.19 +0.04
Ask T/F N 0.18 +0.02 0.16 -0.01 0.20 +0.02
Verbalize 0.18 0.00 0.16 -0.01 0.20 0.00
Length 0.16 +0.09 0.02 0.17 -0.11 0.20 0.00
Unskilled 0.18 0.00 0.16 0.00 0.20 0.00
CodeGen2 Total Prob 0.09 +0.41 0.01 - - - -
Avg Prob 0.11 +0.30 0.01 - - - -
Ask T/F 0.16 0.00 - - - -
Ask T/F N 0.16 0.00 - - - -
Verbalize 0.16 0.00 - - - -
Length 0.15 +0.06 0.03 - - - -
Unskilled 0.16 0.00 - - - -
TABLE A1: Calibration measured as Platt-scaled Brier Score ( B), Skill Score ( SS), and Expected Calibration Error ( ECE ),
with respect to “exact-match” (EM) notion of correctness, excluding function synthesis tasks as EM is not a useful or commonly
used notion of correctness. In cases where the SSis less than 0.05, the ECE is omitted. This is because an estimate without
any signal will become Platt-scaled to approximately the base rate. This will appear as one well calibrated bin, resulting in an
ECE near zero, but does not provide information. CodeGen2 repair values are omitted as it does not perform the task with
greater than 1% accuracy.
Line Completion Function Synthesis Program Repair
Model Metric DyPyBench HumanEval MBPP Defects4J SStubs
GPT-3.5 Total Prob 0.67 0.77 0.70 0.54 0.61
Avg Prob 0.68 0.61 0.56 0.57 0.60
Ask T/F 0.54 0.73 0.71 0.67 0.54
Ask T/F N 0.53 0.74 0.73 0.67 0.57
Verbalize 0.53 0.54 0.60 0.43 0.51
Length 0.53 0.64 0.61 0.48 0.53
Unskilled 0.50 0.50 0.50 0.50 0.50
Codex Total Prob 0.68 0.70 0.66 0.52 0.66
Avg Prob 0.68 0.71 0.69 0.57 0.65
Ask T/F 0.49 0.65 0.56 0.62 0.61
Ask T/F N 0.49 0.61 0.53 0.54 0.59
Verbalize 0.52 0.46 0.50 0.45 0.49
Length 0.55 0.51 0.50 0.43 0.49
Unskilled 0.50 0.50 0.50 0.50 0.50
CodeGen2 Total Prob 0.68 0.44 0.52 - -
Avg Prob 0.66 0.67 0.52 - -
Ask T/F 0.52 0.39 0.59 - -
Ask T/F N 0.54 0.34 0.57 - -
Verbalize 0.51 0.49 0.51 - -
Length 0.55 0.39 0.47 - -
Unskilled 0.50 0.50 0.50 - -
TABLE A2: AUC-ROC score of each techniqueGPT-3.5
T otal Prob
DyPyBench
 HumanEval
 MBPP
 Defects4J
 SStubsAvg Prob
 Ask T/F N
 Verbalize
 Length
CodeGen2
T otal Prob
DyPyBench
 HumanEval
 MBPP
 SStubsAvg Prob
 Ask T/F N
 Verbalize
 Length
Codex
T otal Prob
DyPyBench
 HumanEval
 MBPP
 Defects4J
 SStubsAvg Prob
 Ask T/F N
 Verbalize
 Length
Fig. A3: Calibration plots per model, confidence measure, and task. The blue bars represent nonscaled, evenly spaced bins.
The orange bars are Platt scaled bins. The red lines represent five points of equal count quantiles (an equivalent number of
problems in each bin).0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0
P(estimate)0.00.20.40.60.81.0P(correct)
Rescaling: Platt
All Pass @1: 65%
ref: 0.23
ECE: 0.11
: 0.20
SS: 0.12
1 2 7 12 10 17 37 41 30 4GPT-3.5 HumanEval Ask T/F Reliability n=161
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0
P(estimate)0.00.20.40.60.81.0P(correct)
Rescaling: Platt
All Pass @1: 72%
ref: 0.20
ECE: 0.06
: 0.18
SS: 0.09
1 5 10 17 49 85 145 288 321 9GPT-3.5 MBPP Ask T/F Reliability n=930
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0
P(estimate)0.00.20.40.60.81.0P(correct)
Rescaling: None
All Pass @1: 65%
ref: 0.23
ECE: 0.37
: 0.34
SS: -0.47
27 32 38 24 22 11 6 1 0 0GPT-3.5 HumanEval Ask T/F Reliability n=161Fig. A4: Reliability plots for GPT-3.5, from left to right: HumanEval Ask T/F (Scaled), MBPP Ask T/F (Scaled), and HumanEval
Ask T/F (Nonscaled). Red line denotes five quantiles. All three examples have similar AUC (0.73, 0.71, 0.73) but vastly different
ECE (0.11, 0.06, 0.37).
0.45 0.50 0.55 0.60 0.65 0.70 0.75
AUC-ROC (Raw)0.000.020.040.060.080.10ECE (Scaled)GPT-3.5 AUC vs. Scaled ECE
0.45 0.50 0.55 0.60 0.65 0.70 0.75
AUC-ROC (Raw)0.10
0.05
0.000.050.100.15Skill Score (Scaled)GPT-3.5 AUC vs. Scaled Skill Score
Colors
Defects4J
DyPyBench
HumanEval
MBPP
SStubs
Marks
T otal T oken Probability
Average T oken Probability
Ask T/F
Ask T/F N
Verbalize
Length
Unskilled
Fig. A5: AUC vs Scaled ECE (left) and AUC vs Scaled Skill Score (right) for GPT-3.5 confidence measures on all tasks.
Shows a limited relationship between AUC and ECE, but a strong relationship between AUC and scaled SS.0.0 0.2 0.4 0.6 0.8 1.00.00.20.40.60.81.0est_token_prob_total
intercept=-0.34
coef=0.19SS=0.07DyPyBench
0.0 0.2 0.4 0.6 0.8 1.00.00.20.40.60.81.0
intercept=2.21
coef=0.18SS=0.14HumanEval
0.0 0.2 0.4 0.6 0.8 1.00.00.20.40.60.81.0
intercept=1.89
coef=0.09SS=0.07MBPP
0.0 0.2 0.4 0.6 0.8 1.00.00.20.40.60.81.0
intercept=-1.40
coef=0.12SS=-0.06Defects4J
0.0 0.2 0.4 0.6 0.8 1.00.00.20.40.60.81.0
intercept=-1.33
coef=0.41SS=0.03SStubs
0.0 0.2 0.4 0.6 0.8 1.00.00.20.40.60.81.0est_token_prob_avg
intercept=-1.62
coef=0.55SS=0.08
0.0 0.2 0.4 0.6 0.8 1.00.00.20.40.60.81.0
intercept=-0.95
coef=0.79SS=-0.02
0.0 0.2 0.4 0.6 0.8 1.00.00.20.40.60.81.0
intercept=0.31
coef=0.34SS=0.00
0.0 0.2 0.4 0.6 0.8 1.00.00.20.40.60.81.0
intercept=-2.07
coef=0.24SS=-0.03
0.0 0.2 0.4 0.6 0.8 1.00.00.20.40.60.81.0
intercept=-2.69
coef=0.56SS=0.02
0.0 0.2 0.4 0.6 0.8 1.00.00.20.40.60.81.0est_ask_correct_true
intercept=-0.46
coef=0.14SS=-0.00
0.0 0.2 0.4 0.6 0.8 1.00.00.20.40.60.81.0
intercept=1.64
coef=0.78SS=0.12
0.0 0.2 0.4 0.6 0.8 1.00.00.20.40.60.81.0
intercept=1.55
coef=0.56SS=0.09
0.0 0.2 0.4 0.6 0.8 1.00.00.20.40.60.81.0
intercept=0.06
coef=1.21SS=0.04
0.0 0.2 0.4 0.6 0.8 1.00.00.20.40.60.81.0
intercept=-1.01
coef=0.26SS=0.00
0.0 0.2 0.4 0.6 0.8 1.00.00.20.40.60.81.0est_ask_correct_true_false_normalizedintercept=-0.63
coef=0.08SS=-0.00
0.0 0.2 0.4 0.6 0.8 1.00.00.20.40.60.81.0
intercept=0.78
coef=0.61SS=0.14
0.0 0.2 0.4 0.6 0.8 1.00.00.20.40.60.81.0
intercept=0.90
coef=0.44SS=0.11
0.0 0.2 0.4 0.6 0.8 1.00.00.20.40.60.81.0
intercept=-1.23
coef=0.91SS=0.03
0.0 0.2 0.4 0.6 0.8 1.00.00.20.40.60.81.0
intercept=-1.25
coef=0.47SS=0.01
0.0 0.2 0.4 0.6 0.8 1.00.00.20.40.60.81.0est_self_question_calibrateintercept=-0.72
coef=0.01SS=-0.00
0.0 0.2 0.4 0.6 0.8 1.00.00.20.40.60.81.0
intercept=0.50
coef=0.02SS=-0.05
0.0 0.2 0.4 0.6 0.8 1.00.00.20.40.60.81.0
intercept=0.70
coef=0.03SS=0.02
0.0 0.2 0.4 0.6 0.8 1.00.00.20.40.60.81.0
intercept=-1.45
coef=0.01SS=-0.10
0.0 0.2 0.4 0.6 0.8 1.00.00.20.40.60.81.0
intercept=-1.36
coef=-0.00SS=-0.00
0.0 0.2 0.4 0.6 0.8 1.00.00.20.40.60.81.0est_length
intercept=-0.80
coef=0.07SS=0.00
0.0 0.2 0.4 0.6 0.8 1.00.00.20.40.60.81.0
intercept=0.51
coef=0.06SS=-0.03
0.0 0.2 0.4 0.6 0.8 1.00.00.20.40.60.81.0
intercept=0.84
coef=0.04SS=0.01
0.0 0.2 0.4 0.6 0.8 1.00.00.20.40.60.81.0
intercept=-1.43
coef=-0.00SS=-0.06
0.0 0.2 0.4 0.6 0.8 1.00.00.20.40.60.81.0
intercept=-1.50
coef=0.09SS=-0.00Fig. A6: A comparison of the rescaling curves across tasks and measures for GPT-3.5. Logistic regression (Platt scaling)
functions rescale the measurement (x-axis) to a new confidence (y-axis). The ■represents a median measured value, ▶the
lower quartile, and ◀the upper quartile. The five curves from the different folds are shown in light gray, with the main line
being the curve from fitting to all data. Dataset-measure pairs with less data or highly concentrated values have greater curve
variance across folds. Scaled SSis shown along with the logistic regression parameters.033% 65% 72% 19% 20%Base Rate (% Correct)
00.15 0.63 0.71 0.28 0.25 0.41ECE RawDyPyBench
HumanEval
MBPP
Defects4J
SStubs
Rescale ApplyDyPyBench
HumanEval
MBPP
Defects4J
SStubsRescale Fitting0.07 -0.95 -1.61 -0.28 -0.23
-1.17 0.16 -0.04 -3.17 -2.97
-1.14 0.08 0.07 -2.91 -2.71
-0.08 -1.28 -1.97 0.00 0.01
-0.08 -1.64 -2.41 -0.00 0.03Skill Score 
DyPyBench
HumanEval
MBPP
Defects4J
SStubs
Mean
Rescale ApplyDyPyBench
HumanEval
MBPP
Defects4J
SStubsRescale Fitting0.04 0.50 0.58 0.21 0.20 0.31
0.52 0.10 0.11 0.70 0.69 0.43
0.51 0.18 0.07 0.67 0.66 0.42
0.17 0.56 0.64 0.02 0.03 0.28
0.19 0.62 0.70 0.05 0.01 0.31ECE 
Rescaleing Reuse Plots: GPT-3.5 T otal T oken Probability
033% 65% 72% 19% 20%Base Rate (% Correct)
00.46 0.23 0.14 0.73 0.69 0.45ECE RawDyPyBench
HumanEval
MBPP
Defects4J
SStubs
Rescale ApplyDyPyBench
HumanEval
MBPP
Defects4J
SStubsRescale Fitting0.08 -0.28 -0.63 -0.46 -0.26
-0.15 0.04 -0.06 -1.94 -1.47
-0.53 -0.00 0.01 -2.11 -1.81
-0.09 -0.97 -1.51 0.01 0.01
-0.07 -0.93 -1.50 -0.00 0.03Skill Score 
DyPyBench
HumanEval
MBPP
Defects4J
SStubs
Mean
Rescale ApplyDyPyBench
HumanEval
MBPP
Defects4J
SStubsRescale Fitting0.04 0.27 0.36 0.27 0.21 0.23
0.23 0.05 0.10 0.55 0.49 0.28
0.36 0.09 0.01 0.58 0.54 0.32
0.17 0.48 0.55 0.02 0.02 0.25
0.18 0.47 0.55 0.06 0.01 0.25ECE 
Rescaleing Reuse Plots: GPT-3.5 Average T oken Probability
033% 65% 72% 19% 20%Base Rate (% Correct)
00.15 0.19 0.16 0.24 0.23 0.20ECE RawDyPyBench
HumanEval
MBPP
Defects4J
SStubs
Rescale ApplyDyPyBench
HumanEval
MBPP
Defects4J
SStubsRescale Fitting0.00 -0.35 -0.61 -0.14 -0.12
-0.28 0.16 0.10 -1.25 -1.18
-0.39 0.14 0.11 -1.50 -1.40
-0.21 -0.49 -0.60 0.06 0.01
-0.13 -0.61 -0.82 0.04 0.02Skill Score 
DyPyBench
HumanEval
MBPP
Defects4J
SStubs
Mean
Rescale ApplyDyPyBench
HumanEval
MBPP
Defects4J
SStubsRescale Fitting0.00 0.31 0.36 0.15 0.14 0.19
0.22 0.10 0.04 0.46 0.44 0.25
0.28 0.06 0.04 0.50 0.48 0.27
0.21 0.37 0.36 0.05 0.03 0.20
0.17 0.41 0.43 0.07 0.01 0.22ECE 
Rescaleing Reuse Plots: GPT-3.5 Ask T/F N
033% 65% 72% 19% 20%Base Rate (% Correct)
00.42 0.22 0.17 0.60 0.53 0.39ECE RawDyPyBench
HumanEval
MBPP
Defects4J
SStubs
Rescale ApplyDyPyBench
HumanEval
MBPP
Defects4J
SStubsRescale Fitting0.00 -0.39 -0.64 -0.13 -0.12
-0.39 0.01 0.01 -1.26 -1.16
-0.53 -0.01 0.03 -1.56 -1.44
-0.09 -0.89 -1.34 0.00 -0.00
-0.08 -0.87 -1.35 -0.00 0.00Skill Score 
DyPyBench
HumanEval
MBPP
Defects4J
SStubs
Mean
Rescale ApplyDyPyBench
HumanEval
MBPP
Defects4J
SStubsRescale Fitting0.00 0.30 0.36 0.15 0.13 0.19
0.30 0.01 0.06 0.44 0.43 0.25
0.34 0.08 0.01 0.49 0.48 0.28
0.14 0.45 0.52 0.03 0.02 0.23
0.13 0.44 0.52 0.05 0.00 0.23ECE 
Rescaleing Reuse Plots: GPT-3.5 VerbalizeFig. A7: Exploration of GPT-3.5 confidences when fitting a rescaling for one the datasets, and then reusing it on another. In
green is Skill Score, and in blue is the ECE . Above, we plot the raw (nonscaled) ECE for each task. This informs whether
a measure would be better calibrated if one uses it as-is, or one reuses the rescaling. Cells where there is an improvement in
ECE are shown in a coral outline. Datasets within similar task & base rate exhibit most potential for reuse, but still liable to
sizable changes in SSorECE . This analysis suggests that reflective measures may be more robust across rescalings. Note
values might differ slightly from results tables as the full data is used for training the rescaler, rather than folds.8 16 32 64 128 256 512
Sample Size For Rescale Fitting0.8
0.6
0.4
0.2
0.0Skill Score on Rescaled Non-sampled Data
Skill Score
Estimate
T otal T oken Probability
Ask T/F N
8 16 32 64 128 256 512
Sample Size For Rescale Fitting0.00.10.20.30.4ECE on Rescaled Non-sampled Data
ECE
Estimate
T otal T oken Probability
Ask T/F NGPT-3.5 on DyPyBench Docstring Line CompletionFig. A8: Bootstrapped resampling of varying sample for both an intrinsic and reflective measure. During each of 500 bootstrap
simulations, a given number of data points is sampled. This is used to fit a Platt rescaling. We then apply that rescaling to the
remaining non-sampled data points. We show the median simulation, and a 90% interval. We observe that as the number of
examples used for the rescaling increases, there are improvements in SSandECE .UNDERSTANDING EFFECTS OF VERBALIZED RETRY FAILURES
Our implementation for verbalized confidence prompts the model to output the probability its generation is correct at
temperature 1.0. If it does not contain a probability, then we resample up to 3 times. If that loop fails, then a confidence of
0.5 is returned.
This implementation seemed reasonable at the time (if the model won’t tell you its confidence, just go with the
maximum uncertainty of 50-50), but after collecting the data and analyzing results we reconsidered, as 50% values might
be overrepresented in the data.
To try to estimate how this might have influenced our results and conclusions, we searched for instances where the verbalized
confidence was 50% (this provides a upper bound on how often this happens. There can also be cases where the model actually
verbalizes a 50% confidence). This is relatively rare for GPT-3.5 (with the mean dataset having 0.04 of instances as 50%,
range 0.02-0.08). It is more common for Codex (mean 0.14, range 0.09-0.17) and for CodeGen2 (mean 0.10, range 0.09-0.12).
This evidence of how the instruction tuned models are more likely to actually perform the prompted task.
We reran our analysis excluding all these instances. We do not believe a different handling of these fail-retry values would
have greatly changed our conclusions. In the scaled case, the Skill Score on average did not change (mean diff of 0.00) with
extreme change of -0.04 SSwhen already low skill. In the nonscaled case there were some drops in calibration (mean SS
change of -0.11 and mean ECE change of 0.02). The more extreme changes areas of already poor calibration.
It is not clear what is the best default is in the situation where the model fails to verbalize a probability. It is not particularly
valid to exclude these instance. More exploration is needed on this and the effects.
HumanEval MBPP
Confidence Measure B ↓ SS↑ECE↓ B ↓ SS↑ECE↓
0-Shot Reflect 0.23 0.01 0.19 0.22 -0.11 0.16
0-Shot Reflect (Scaled) 0.20 0.14 0.07 0.18 0.11 0.04
FS Random 0.20 0.12 0.11 0.20 -0.03 0.15
FS Random (Scaled) 0.19 0.16 0.04 0.16 0.16 0.04
FS BM25 0.19 0.19 0.08 0.19 0.00 0.11
FS BM25 (Scaled) 0.19 0.18 0.04 0.17 0.14 0.04
TABLE A3: Few-shot reflective prompting using G PT-3.5. We observe the the unscaled skill score and ECE both improve.
The raw SS improves 0.08-0.11 unscaled and further when scaled. The improvement from BM25 was more modest if doing
rescaling, but appears useful if using raw values.