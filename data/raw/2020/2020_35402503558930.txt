JSIMutate: Understanding PerformanceResults
throughMutations
ThomasLaurent
Lero& University CollegeDublin
Dublin, Ireland
thomas.laurent@ucd.iePaoloArcaini
NationalInstituteofInformatics
Tokyo, Japan
arcaini@nii.ac.jp
Catia Trubiani
GranSassoScienceInstitute
L’Aquila, Italy
catia.trubiani@gssi.itAnthonyVentresque
Lero& University CollegeDublin
Dublin, Ireland
anthony.ventresque@ucd.ie
ABSTRACT
Understandingtheperformancecharacteristicsofsoftwaresystems
is particular relevant when looking at design alternatives. How-
ever, it is a very challenging problem, due to the complexity of
interpretingtheroleandincidenceofthedifferentsystemelements
onperformancemetricsofinterest,suchassystemresponsetime
or resources utilisation. This work introduces JSIMutate , a tool
thatmakesuseofqueueingnetworkperformancemodelsanden-
ablestheanalysisofmutationsofamodelreflectingpossibledesign
changestosupportdesignersinidentifyingthemodelelementsthat
contribute toimprovingorworseningthe system’s performance.
CCSCONCEPTS
·Software and its engineering →Software performance ;·
General and reference →Performance; Metrics.
KEYWORDS
performance analysis,queueingnetworks,mutation operators
ACM ReferenceFormat:
ThomasLaurent,PaoloArcaini,CatiaTrubiani,andAnthonyVentresque.
2022.JSIMutate:UnderstandingPerformanceResultsthroughMutations.In
Proceedingsofthe30thACMJointEuropeanSoftwareEngineeringConference
andSymposiumontheFoundationsofSoftwareEngineering(ESEC/FSE’22),
November 14ś18, 2022, Singapore, Singapore. ACM, New York, NY, USA,
5pages.https://doi.org/10.1145/3540250.3558930
1 INTRODUCTION
Thereisagrowinginterestinthesoftwareengineeringcommunity
for performance characteristics of software systems [ 7,24], and
recently many techniques have been developed to identify root
causesofperformanceanomalies[ 3,4,8,18,19,26].Whenfocusing
on developer needs [ 25], it becomes clear that current state-of-the-
artmethodologiesdonotprovideenoughsupportto(i)understand
the performance characteristics ofsoftware systems, inparticular
ESEC/FSE ’22,November 14ś18, 2022, Singapore, Singapore
©2022 Copyright held bytheowner/author(s).
ACM ISBN978-1-4503-9413-0/22/11.
https://doi.org/10.1145/3540250.3558930when there is a large set of design alternatives, and (ii) trace the
cause-effect chain by identifying which components most likely
(in)directly affect the system’s performance.
Understandingperformanceresultsischallenging,sinceanaly-
sisresultsonlygiveinformationofthesystem’sperformancebut
do not provide any hint of whythese results are obtained, and
their root causes. However, this type of information would be very
useful. Indeed, system designers are also interested in knowing
which portions of the software system contribute to improving
orworseningperformanceresults.Theactualbenefitcomesfrom
identifying which system components are responsible for bad sys-
temperformance,orwhichdesignchoicesleadtoexpensivesoft-
ware/hardwaresettings,increasingtheoverallcostofthesystem.
Therefore, to understand performance results, system designers
usuallyneedtorunexpensivesoftwareperformancetestingcam-
paigns[3,6,17]toverifywhetherdesignchangesactuallyaffectthe
system’sperformance.Suchanapproachshowsseveralweaknesses:
itrequiresperformance-basedtestingexpertise,itiscomputation-
allyexpensive,itismanual,anditmayalsobeerror-prone.Thus,
automation is desirable toaddress these challenges.
Thispaperintroducesatool,namely JSIMutate ,thatcansup-
port designers in understanding the performance characteristics
ofasoftwaresystemandinsizingitsresourcesinasuitableway,
forinstancebyidentifyingperformancebottlenecks,challenging
workloads,andanalysingdesignchanges.Thetheoreticalelements
underlying JSIMutate wereintroducedinpreviouswork[ 13],how-
ever they were not implemented in an easily applicable way for
designers. This work describes how JSIMutate was designed to
automatetheapproachandprovideauser-friendlyinterfacesothat
it can be easily used.
Forperformanceanalysis,weleverage softwareperformanceengi-
neeringtechniques, in particular Queueing Networks (QNs) [12,16]
performancemodels, sincetheyarerecognisedasprovidingaccu-
ratepredictionswhenanalysingrealsystems[ 1,5,9,23],andthere
exist well-assessed frameworks (e.g., JMT [ 2]) to simulate these
modelsandcollecttheperformancemetricsofinterest.Toeasily
implement design alternatives, we take inspiration from mutation
analysis[11,20]: we systematically apply changes toperformance
modelsbasedondefined mutationoperators ,andautomaticallyeval-
uatetheirperformancetosupportacosttrade-offanalysis.Inother
terms, our tool embeds the specification of mutation operators and
itgeneratesasetofmutatedqueueingnetworkmodels,withthe
Thiswork islicensedunderaCreativeCommonsAttribution4.0Interna-
tional License.
1721
ESEC/FSE ’22, November14ś18, 2022,Singapore, Singapore Thomas Laurent, PaoloArcaini,Catia Trubiani,andAnthonyVentresque
goaltobetterunderstandtheperformanceoftheoriginalsystem
andofalternativedesigns.Thisway,wecanautomaticallycompare
theperformanceresultsproducedbythese mutantswith thoseof
the original model, when executed with typical system workloads.
Tosummarise, this paperpresents JSIMutate ,a toolthathelps
designers understand performance results through mutations. This
means to evaluatedesign alternativesand selectthe ones thatare
beneficial (from a performance-based perspective) as possible sys-
temrefactoringstobesuggestedtosoftwaredevelopers. JSIMutate
manipulates performance models and simulates typical workloads
definedbythedesigners. JSIMutate wasdesignedtoberobustand
easy to use, and implements a set of changes to the QN models
thathavebeenproposedintheliterature[ 21,27]andthatareoften
usedbysystemdesignerswhentuningthecostandperformance
ofasystem.Thisshouldmake JSIMutate easilyadoptablebyprac-
titioners. JSIMutate is available at [ 14], and a screencast showing
ituse isavailable at [ 15].
2 BACKGROUND
This section provides background information on queueing net-
works (i.e., the used performance modelling formalism) and model-
basedanalysisto getsystemperformance metrics ofinterest.
QueueingNetworks. ThemainconstituentelementsofaQN
model are: (i) processing nodes representing system resources, (ii) a
set ofcustomer classes that refer to the external users requests of
services,and(iii) structuralnodes denotingtheflowofrequests[ 12].
Processingnodescanbedistinguishedaccordingtotheirroleinthe
system.First, servicecentres arecomposedofa serverandaqueue.
Serversaremeanttoprocessajob fromtheirqueue and routethe
processed request to another node. Queues can be characterised by
afiniteoraninfinitesize.Second, delaycentres differentiatefrom
the service centres since they have no queue. Service and delay
centresareconnectedthrough linksthatcontributetothenetwork’s
topology.A routingnode canbeintroducedtoroutecustomerclasses
to differentnodes according to somegiven probabilities.
Inotherwords,aQNmodelcanbeexpressedasadirectedgraph
whosenodesareservice/delaycentresandwhoseedgesrepresent
theirconnections. Jobscirculatethroughthegraph’sedgeseton
thebasisofthecustomers’servicerequests.Afurthertypeofnodes
is theforkthat expresses parallelism, i.e., one task is split into
multiple jobs that can be executed in parallel and synchronised
throughthe joinnode.Toregulatetheparallelism,thereare finite
capacity regions that explicitly specify the maximum number of
requests allowedto run inparallel.
Model-based performance analysis requires to parameterise the
QNmodelwithtiminginformation.Thetimespentbyeachrequest
inprocessingnodesismodelledbyprobabilitydistributions,e.g.,
exponential or deterministic distributions. Delay centres are reg-
ulatedbyadeterministicservicetimethatdenoteshowlongjobs
are delayed before proceeding in further delay or queueing centres.
Service centres also include the service times needed to process
thedifferenttypesofrequests,alongwiththepolicytomanagethe
requestswaitinginthequeue,e.g.,first-come-first-served(FCFS).
The workload can be modelled as open(i.e., specified by an arrival
rate𝜆) orclosed(i.e., a constant number 𝑁of requests specified
asthepopulationsize).Incaseofanopenworkload,requestsare
Figure 1:Example ofaqueuingnetwork(takenfrom[ 22])
generatedby sourcenodes connectedwiththerestoftheQN,and
terminatein sinknodes denotingthatallrequeststoresourceshave
been completed.
Fig.1providesanillustrativeexampleofapubliclyavailableQN
model[22]thatwillserveasrunningexampleinthepaper.Asource
node,namely Requests,representsanopenworkloadofusers’re-
quests.Twodifferenttypesofrequestshavebeendefined:(i) Class0
followsanhyperexponentialdistribution,and(ii) Class1presents
anexponentialdistribution.Bothclassesareroutedtothe RAID0
fork node and the finite capacity region called Apache - MaxClients
=100.Each requestroutedtothe RAID0fork nodeistransformed
to three tasks that are forwarded to the next queueing network
centres,i.e., Disk1,Disk2,andDisk3,eachwithacertainservice
time distribution. The completionof forkedtasks is regulatedby
the join node Join0. This node is connected to the Replysink node
thatdeterminesthecompletionofsuchrequests.Eachrequestin
thefinitecapacityregionfollowsitsrules;specifically,theregion
capacityissetto100,thatmeansnomorethatonehundredrequests
are allowed to reach the LoadBalancer routing node at a time. A
strategyofroutingissetforeachjobclass;forinstance, Class0joins
theshortestqueue,and Class1followstheshortestresponsetime
criterion. Two queueing centres (i.e., WebServer 1 andWebServer
2) are connected to the routing station to balance the load that
terminatesinthe Replysink node.
Model-basedperformanceanalysis. QNsareadoptedtopre-
dict the performance characteristics of software systems. When
using QNsto perform OperationalProfileAssessment ,thesystem’s
designers defines the system’s typical workloads, and some per-
formancerequirements,e.g.,systemresponsetimeshouldnotbe
larger than 10 seconds. JSIMutate simulates these workloads and
measuresthesystem’sperformance,whichcanbecheckedbythe
designer against the requirements, and can help find possible mod-
ificationsofthe system, eitherto meet violated requirements (e.g.,
by adding additional resources), or to minimise costs of the system
(e.g.,byremovingunnecessaryresources). JSIMutate usesJMT[ 2]
astheback-endtoolforQNmodellingandsimulationtocollectthe
performance metrics of interest. For each performance metric, it
ispossible tospecifythemaximumrelativeerror thatdenotesthe
precision requiredinthesimulation.Followingtheliterature[ 10],
JSIMutate relies on three performance metrics. System response
time(SRT) is defined as the time interval between a user’s request
ofaserviceandtheresponseofthesystem;STRiscomputedforthe
wholenetwork,i.e.,atthe systemlevel .Utilisation (UT)isdefinedas
theratioofbusytimeofaresourceandthetotalelapsedtimeofthe
measurementperiod;UTiscomputedatthe componentlevel ,i.e.,
thereisanutilisationvalueforeachnodeofthenetwork. System
1722JSIMutate: UnderstandingPerformance Results through Mutations ESEC/FSE ’22, November14ś18, 2022,Singapore, Singapore
S
S S Ss
i
m
u
l
a
t
i
o
nOriginal model
MutantsPerformance resultsJSIMutate
?Compare 
resultsMetric 1 
Metric 2 
Metric 3
Performance 
expert{
    "jsimgFile": "model.jsimg",
    "workloads": [
        {
            "workloadID": “worload1”,
            "openClassesExponential": [
             {
                   "name": "Class1",
        ...
    ]
Input JSON file
Performance resultsResult tablesS S S
Mutant jsimg 
files
Mutation 
operators
Figure 2: JSIMutate ’sworkflow
drop rate (SDR) is defined as the rate at which customers’ requests
are dropped from a station or a region due to a maximum capacity;
SDRiscomputedbothatthe componentlevel andatthe systemlevel .
3JSIMUTATE
This section describes JSIMutate , its use case and workflow, the
waysitmutatesqueueing networks, andhowto use it.
3.1 Use Case
Queueing networks can be used to model and evaluate the per-
formance of a system under certain expected workloads when
designing the system. This activity is referred to as Operational
Profile Assessment .Inthissetting, asystem designermightrealise
that the systemdoes not meetthe performance requirements that
havebeenset,orthatthesystemistoocostlytobuild.Thedesigner
then wants to refactor the system in order to achieve the same
functionalitywithbetterperformance orusing fewer resources.
Exploringandunderstandingtheperformanceeffectsofpossible
refactorings is a tedious and error-prone task when donemanually,
asitismostlydoneinthecurrentdevelopmentpractice. JSIMutate
allows to automate this task by presenting the performance effects
ofdifferent changes in the modelto the designer in asummarised
way. The designer can then easily find refactorings that might help
achievetheirperformancegoals,andcanbetterunderstandtherole
andimportanceofeachofthe elements intheirnetwork.
3.2 Workflow
Given a queueing network and a set of workloads, JSIMutate sys-
tematicallyappliesa setofcommonlyusedrefactoringsśreferred
to asmutations in the toolś to the network, and evaluates the ef-
fect of these mutations on the system’s performance using a set of
performance metrics .
Fig.2illustrates this workflow. The designer (i.e., the perfor-
mance expert) inputs the model and the workloads they want to
study performanceunder, asaJSON file (described further in Sec-
tion3.5.1).JSIMutate simulates these workloads on the system,
measuringasetoforiginalperformancemetricsvalues. JSIMutate
then applies mutations to the network, defined by the mutation
operatorsdescribedinSection 3.3,simulatestheworkloadsonthese
mutatedsystems,measuringtheperformancemetricsofinterest.
Then,JSIMutate compares thevalues of the performance metrics
achievedbytheoriginalsystemandeachofthemutatedsystems,
asdescribedinSection 3.4,todeterminetheeffectofthedifferentchangesonthesystem’sperformance.Finally,itoutputsasummary
ofthis comparison inasetof outputfilesdescribedinSection 3.5.
3.3 Mutation Operators
JSIMutate implementsthefollowingoperators,mimickingpossible
refactorings ofthe network:
•Change Queue Size : creates a mutant for each node in the
model, modifying the size of its queue. It creates multiple
mutants per node, dividing the queue size by 2, removing
oneunit,doublingthequeuesize,andaddingoneunit.For
infinite queues, only one mutant is created and the queue is
given an arbitrary finitesize of 10.
•Change Queue Strategy : creates a mutant for each node in
themodel,modifyingthequeueingstrategyofitsqueue.The
FCFSandLCFS strategiesareswapped,giving theopposite
strategyfrom the originalto the mutatedqueues.
•Change Number of Servers : creates a mutant for each pro-
cessing node inthe model, modifyingits numberof servers
(processingunits).Itcreatesmultiplemutantspernode,di-
viding the number of servers by 2, removing one server,
doublingthe number of servers, andaddingone server.
3.4 MeasuringtheImpactoftheMutations
WhenJSIMutate simulatesthedifferentworkloadsontheoriginal
networkandthemutatednetworks,itcollectsasetofperformance
metrics consisting of the system response time (including the re-
sponsetimeofeachnodetothedifferenttypesofrequestsreceived),
theutilisationofeachnode,andthesystemdroprate.Thesemetrics
are then used to compare the performance of the original network
with that of the mutated network, to measure the impact of the
mutationsto the system’sperformance.
Asallmetricsareestimatedwithacertainprecision,theoriginal
andmutatednetworks’performanceresultswouldalmostalways
differ. Hence, JSIMutate filters differences that are deemed in-
significant. To evaluate if a mutation had a significant impact on a
particularmetric, JSIMutate comparestheresultsoftheoriginal
network for this metric, represented by [𝑚𝑙,𝑚𝑚,𝑚𝑢], where𝑚𝑙
and𝑚𝑢arethelowerandupperboundsmeasuredforthemetricin
thesimulationand 𝑚𝑚themeanvalue;andtheresultsofthemu-
tated network, represented by [𝑚′
𝑙,𝑚′𝑚,𝑚′𝑢].JSIMutate considers
whetheramutationhadasignificantimpactontheperformance
ofthe system withregardsto ametric. Specifically, if themetric’s
meanvalue changed by more than 5%ofits original value andthe
range ofvaluesmeasuredforthe originaland mutatednetworks
do not overlap,i.e.,:
|𝑚′𝑚−𝑚𝑚|
𝑚𝑚>0.05∧ (𝑚𝑢<𝑚′
𝑙∨𝑚′
𝑢<𝑚𝑙)(1)
In this case,we say that the mutant has been triggered.
3.5 Running JSIMutate
JSIMutate isdistributedasaJARfileandcanberunonthecom-
mand line. Its main input is a JSON file that points to the queueing
networktosimulateandmutate(asa jsimgfile),andtheworkloads
under which the system must be simulated. JSIMutate can also
take in some options, such as the list of mutation operators to use
1723ESEC/FSE ’22, November14ś18, 2022,Singapore, Singapore Thomas Laurent, PaoloArcaini,Catia Trubiani,andAnthonyVentresque
{
"jsimgFile" :"./2cl_10stat_fcr_fork_whatif.jsimg" ,
"workloads" : [
{
"workloadID" :"w1",
"openClassesExponential" : [
{
"name":"Class1" ,
"referenceStation" :"Requests" ,
"interarrivaldistributionLambda" :0.2
}
],
"openClassesHyperExponential" : [
{
"name":"Class0" ,
"referenceStation" :"Requests" ,
"p":0.2,
"lambda1" :0.2,
"lambda2" :0.45
}
]
},{
"workloadID" :"w2",
"openClassesExponential" : [
{
"name":"Class1" ,
"referenceStation" :"Requests" ,
"interarrivaldistributionLambda" :0.4
}
],
"openClassesHyperExponential" : [
{
"name":"Class0" ,
"referenceStation" :"Requests" ,
"p":0.8,
"lambda1" :0.2,
"lambda2" :0.45
}
]
}
]
}
Figure 3:Example of JSIMutate ’sinput file (JSONformat)
Table 1: Example of JSIMutate ’s output ś Raw results (ex-
cerpt)
w/load location operator class MeasureTypeorig
_lowerLimitorig_
meanValueorig_
upperLimitmut_
lowerLimitmut_
meanValuemut_
upperLimittimeoutMutsignificant
_difference...
w1 WebServer1 CNS SRT 1.074 1.093 1.112 1.099 1.123 1.147 false false ...
w1 WebServer1 CNS Class0 Utilization 0.0869 0.0892 0.0914 0.1751 0.1797 0.1843 false true ...
w1 WebServer1 CNS SystemDR 0.0 0.0 0.0 0.0 0.0 0.0 false false ...
w1 WebServer1 CNS DR 0.0 0.0 0.0 0.0 0.0 0.0 false false ...
w2 WebServer1 CNS SRT 1.142 1.176 1.209 1.176 1.205 1.234 false false ...
w2 WebServer1 CNS Class0 SRT 0.9770 0.9960 1.014 1.005 1.027 1.049 false false ...
w2 WebServer1 CNS Class1 SRT 1.250 1.289 1.327 1.275 1.292 1.309 false false ...
w2 WebServer1 CNS Utilization 0.1012 0.1042 0.1072 0.2077 0.2132 0.2187 false true ...
w2 WebServer1 CNS Class0 Utilization 0.05540 0.05682 0.05824 0.1082 0.1112 0.1143 false false ...
w2 WebServer1 CNS Class1 Utilization 0.04846 0.04986 0.05125 0.09743 0.1000 0.1027 false true ...
w2 Disk3 CNSP Class1 DR 0.0 0.0 0.0 0.0 0.0 0.0 false false ...
w1 WebServer1 CQSize SRT 1.074 1.093 1.112 1.074 1.093 1.112 false false ...
w1 WebServer1 CQSizeM1 SRT 1.074 1.093 1.112 1.074 1.093 1.112 false false ...
w1 WebServer2 CQStrategy Class0 Utilization 0.08906 0.09135 0.09363 0.08834 0.09073 0.09313 false false ...
...
fortheanalysis,orthetimeoutsetwhensimulatingthemutated
networks, and used in case one of the mutations prevented the
simulation from converging.
3.5.1 Input file. The main input file is a JSON file that contains
thepathtothejsimgfiledescribingthenetworktoanalyseinthe
jsimgFile attribute,andadescriptionoftheworkloadstosimulate
in theworkloads attribute. The workloads are represented as a
JSONobjecteach,whichcontainsadescriptionofthedifferentuser
defined classes (i.e., typesof requestssent to the server). Different
typesofuserclassessuchasopenorclosedclasses,deterministic
or exponential loads can be defined in this format. Fig. 3shows an
example input file for the network shown in Fig. 1, defining two
workloads,eachcontainingtwouserclasses,oneexponentialand
the otherone hyper exponential.
3.5.2 Outputfiles. JSIMutate ’smainoutputconsistsofthreeta-
bles that summarise the comparison between the original and mu-
tated models’ performance, in the form of .tsvfiles. Optionally,
JSIMutate can also save the mutated models as .jsimgfiles so
thattheusercaneasilyreuseamutatedmodelthattheypreferto
comparewiththeoriginalmodel.Thissectionillustratesthesefiles
byusingtheresultsproducedbytheinputinFig. 3onthemodel
inFig.1as an illustrative example.
The first output file contains the raw results of the performance
comparison between the original and mutated models. Table 1
showsanexcerptoftherawresults(thewholeresultscontain2952
rows and some additional columns with details on the collected
metrics). For each mutated model, each workload, and each per-
formance metric, it contains a row displaying the original model’s
values for the metric under analysis, the mutated model’s valuesTable2:Example of JSIMutate ’s outputś Mutant-workload
matrix (excerpt)
w/loadChange
NumberServers
_Disk 1Change
NumberServersM1
_Disk 1Change
NumberServersP
_Disk 1Change
NumberServers
_Disk 2Change
NumberServers
_WebServer1Change
QueueingStrategy
_WebServer2...
w1 true true false false true false ...
w2 false false false false true false ...
Table 3: Example of JSIMutate ’s output ś Mutant-metric ma-
trix (excerpt)
Measure DescriptionChange
NumberServers
_Disk1Change
QueueSizeM1
_Disk1Change
NumberServers
_Disk3Change
NumberServers
_WebServer1Change
QueueingStrategy
_WebServer2...
Drop_Rate_Class0_Disk1 false false false false false ...
Drop_Rate_Class0_WebServer 1 false false false false false ...
System Drop Rate_NA_NA false false false false false ...
System Response Time_Class0_NA false false false false false ...
System Response Time_Class1_NA false false false false false ...
System Response Time_NA_NA false false false false false ...
Utilization_Class0_WebServer 1 false false false false false ...
Utilization_Class1_Disk 3 false false true false false ...
Utilization_Class1_WebServer 1 false false false true false ...
...
for such a metric, whether the mutated model timed out (the simu-
lationdidnotconverge),andwhetherthetwosetsofvalueswere
significantly different(as specifiedbyEq. 1).
The two other files summarise the information contained in the
raw results as matrices. The first table shows, for each mutant-
workload pair, whether the mutant produced a significantly differ-
entresultfor any ofthemetrics ortimedoutwiththismetric; see
Table2for an excerpt of this matrix (the whole matrix contains
46 columns, correspondingto all themutants).The second matrix
shows, for each mutant-metric pair, whether the mutant produced
asignificantlydifferentresultforthismetricunderanyofthework-
loads;seeTable 3foranexcerptofthismatrix(thewholematrix
contains 46 columns for all the mutants, and 36 rows for all the
performance metrics measuredonthe differentnodes).
Thesetablesprovidequantitativeinformationtothedesigners
whocandecidehowtomodifythesystembasedonthemutants’
impactontheperformance. JSIMutate automaticallyevaluatesthe
differentmutantsandpointsoutsthoseimpactingtheperformance
metrics ofinterest.
4 CONCLUSIONS
Queueingnetworksarewidelyusedtoassesstheperformancere-
quirements of a system. A typical analysis technique consists in
checking whether the system is properly equipped for meeting
performancerequirements,orif itiseitherover-equipped(unnec-
essarilycostly)orunder-equipped(notmeetingtheperformance
requirements).Inthispaper,weproposed JSIMutate ,atoolthat
automatisesthisapproachbycheckingtheperformanceofmutated
versionsofthe originalnetwork undertypical workloads.
ACKNOWLEDGMENTS
Thisworkwassupported,inpart,byScienceFoundationIreland
grants20/FFP-P/8818and13/RC/2094_P2.P.Arcainiissupported
by Engineerable AI Project (No. JPMJMI20B8), JST; and ERATO
HASUO Metamathematics for Systems Design Project (No. JPM-
JER1603), JST, Funding Reference number: 10.13039/501100009024
ERATO.ThisworkhasbeenpartiallyfundedbyMIURprojectPRIN
2017TWRCNB SEDUCE (Designing Spatially Distributed Cyber-
Physical SystemsunderUncertainty).
1724JSIMutate: UnderstandingPerformance Results through Mutations ESEC/FSE ’22, November14ś18, 2022,Singapore, Singapore
REFERENCES
[1]Giuliano Casale, Vittoria De Nitto Personé, and Evgenia Smirni. 2016. QRF:
An Optimization-Based Framework for Evaluating Complex Stochastic Net-
works.ACMTrans.Model.Comput.Simul. 26,3,Article15(Jan.2016),24pages.
https://doi.org/10.1145/2724709
[2]Giuliano Casale, Giuseppe Serazzi, and Lulai Zhu. 2018. Performance Evaluation
withJavaModellingTools:AHands-onIntroduction. SIGMETRICSPerform.Eval.
Rev.45,3 (March2018),246ś247. https://doi.org/10.1145/3199524.3199567
[3]JinfuChen,WeiyiShang,andEmadShihab.2022. PerfJIT:Test-LevelJust-in-Time
Prediction for Performance Regression Introducing Commits. IEEE Transactions
on Software Engineering 48, 5 (2022), 1529ś1544. https://doi.org/10.1109/TSE.
2020.3023955
[4]Zhuangbin Chen, JinyangLiu, YuxinSu, Hongyu Zhang, Xiao Ling, Yongqiang
Yang, and Michael R. Lyu. 2022. Adaptive Performance Anomaly Detection
for Online Service Systems via Pattern Sketching. In Proceedings of the 44th
International Conference on Software Engineering (Pittsburgh, Pennsylvania)
(ICSE ’22) . Association for Computing Machinery, New York, NY, USA, 61ś72.
https://doi.org/10.1145/3510003.3510085
[5]Salvatore Dipietro, Giuliano Casale, and Giuseppe Serazzi. 2017. A Queueing
Network Model for Performance Prediction of Apache Cassandra. In Proceedings
of the 10th EAI International Conference on Performance Evaluation Methodolo-
gies and Tools on 10th EAI International Conference on Performance Evaluation
Methodologies andTools (Taormina, Italy) (VALUETOOLS’16) . ICST(Institute for
ComputerSciences,Social-InformaticsandTelecommunicationsEngineering),
Brussels, BEL, 186ś193. https://doi.org/10.4108/eai.25-10-2016.2266606
[6]SimonEismann,DiegoEliasCosta,LizhiLiao,Cor-PaulBezemer,WeiyiShang,
André van Hoorn, and Samuel Kounev. 2022. A case study on the stability of
performancetestsforserverlessapplications. JournalofSystemsandSoftware
189(2022), 111294. https://doi.org/10.1016/j.jss.2022.111294
[7]HaochenHe,ZhouyangJia,ShanshanLi,YueYu,ChenglongZhou,QingLiao,
Ji Wang, and Xiangke Liao. 2022. Multi-Intention-Aware Configuration Se-
lection for Performance Tuning. In Proceedings of the 44th International Con-
ference on Software Engineering (Pittsburgh, Pennsylvania) (ICSE ’22) . Asso-
ciation for Computing Machinery, New York, NY, USA, 1431ś1442. https:
//doi.org/10.1145/3510003.3510094
[8]Jingzhu He, Yuhang Lin, Xiaohui Gu, Chin-Chia Michael Yeh, and Zhongfang
Zhuang.2022.PerfSig:ExtractingPerformanceBugSignaturesviaMulti-Modality
CausalAnalysis.In Proceedingsof the44thInternationalConferenceon Software
Engineering (Pittsburgh, Pennsylvania) (ICSE ’22) . Association for Computing
Machinery, New York, NY, USA, 1669ś1680. https://doi.org/10.1145/3510003.
3510110
[9]Emilio Incerto, Annalisa Napolitano, and Mirco Tribastone. 2021. Learning
Queuing Networks via Linear Optimization. In Proceedings of the ACM/SPEC
International Conference on Performance Engineering (Virtual Event, France)
(ICPE ’21) . Association for Computing Machinery, New York, NY, USA, 51ś60.
https://doi.org/10.1145/3427921.3450245
[10]RajJain.2008. Theartofcomputersystemsperformanceanalysis . JohnWiley&
Sons.
[11]Yue Jia and Mark Harman. 2011. An Analysis and Survey of the Develop-
ment of Mutation Testing. IEEE Trans. Softw. Eng. 37, 5 (Sept. 2011), 649ś678.
https://doi.org/10.1109/TSE.2010.62
[12]LeonardKleinrock.1975. Theory,Volume1,QueueingSystems . Wiley-Interscience,
NewYork, NY, USA.
[13]ThomasLaurent,PaoloArcaini,CatiaTrubiani,andAnthonyVentresque.2022.
Mutation-basedanalysisofqueueingnetworkperformancemodels. Journalof
SystemsandSoftware 191(2022),111385. https://doi.org/10.1016/j.jss.2022.111385[14]ThomasLaurent,PaoloArcaini,CatiaTrubiani,andAnthonyVentresque.2022.
Repositoryof JSIMutate .https://github.com/ucd-csl/JSIMutate .
[15]ThomasLaurent,PaoloArcaini,CatiaTrubiani,andAnthonyVentresque.2022.
Screencastof JSIMutate .https://drive.google.com/file/d/13Kca-AOXYmU4Ol_
agFdsZaAmALQ-ZrVR/view?usp=sharing .
[16]E. D. Lazowska, J. Zahorjan, G. Scott Graham, and K. C. Sevcik. 1984. Computer
System Analysis Using Queueing Network Models . Prentice-Hall, Inc., Englewood
Cliffs.
[17]PhilippLeitnerandCor-PaulBezemer.2017. AnExploratoryStudyoftheStateof
Practice of Performance Testing in Java-Based Open Source Projects. In Proceed-
ingsofthe8thACM/SPEConInternationalConferenceonPerformanceEngineering
(L’Aquila,Italy) (ICPE’17) .AssociationforComputingMachinery,NewYork,NY,
USA,373ś384. https://doi.org/10.1145/3030207.3030213
[18]LizhiLiao,JinfuChen,HengLi,YiZeng,WeiyiShang,CatalinSporea,Andrei
Toma, and Sarah Sajedi. 2021. Locating Performance Regression Root Causes
in the Field Operations of Web-based Systems: An Experience Report. IEEE
TransactionsonSoftwareEngineering (2021),1ś1. https://doi.org/10.1109/TSE.
2021.3131529
[19]XinyuLiu,QiZhou,JoyArulraj,andAlessandroOrso.2022. AutomaticDetection
of Performance Bugs in Database Systems Using Equivalent Queries. In Proceed-
ings of the 44th International Conference on Software Engineering (Pittsburgh,
Pennsylvania) (ICSE’22) . Association for ComputingMachinery, NewYork, NY,
USA,225ś236. https://doi.org/10.1145/3510003.3510093
[20]Mike Papadakis, Marinos Kintis, Jie Zhang, Yue Jia, Yves Le Traon, and Mark
Harman.2019. ChapterSix-MutationTestingAdvances:AnAnalysisandSurvey.
Advances in Computers, Vol. 112. Elsevier, 275ś378. https://doi.org/10.1016/bs.
adcom.2018.03.015
[21]Dorina C Petriu. 2021. Integrating the analysis of multiple non-functional prop-
erties in model-driven engineering. Software and Systems Modeling 20, 6 (2021),
1777ś1791. https://doi.org/10.1007/s10270-021-00953-3
[22]ShicaoUW.2017. ExamplesforłQN-ACTRcognitivearchitecturež. https://github.
com/HOMlab/QN-ACTR-Release/tree/master/QN-ACTR%20Java/examples .
[23]Bhuvan Urgaonkar, Giovanni Pacifici, Prashant Shenoy, Mike Spreitzer, and
Asser Tantawi. 2005. An Analytical Model for Multi-Tier Internet Services
and Its Applications. In Proceedings of the 2005 ACM SIGMETRICS Interna-
tional Conference on Measurement and Modeling of Computer Systems (Banff,
Alberta, Canada) (SIGMETRICS ’05) . ACM, New York, NY, USA, 291ś302. https:
//doi.org/10.1145/1064212.1064252
[24]Miguel Velez, Pooyan Jamshidi, Norbert Siegmund, Sven Apel, and Christian
Kästner. 2021. White-Box Analysis over Machine Learning: Modeling Perfor-
manceofConfigurableSystems.In Proceedingsofthe43rdInternationalConfer-
enceonSoftwareEngineering (Madrid,Spain) (ICSE’21) .IEEEPress,1072ś1084.
https://doi.org/10.1109/ICSE43902.2021.00100
[25]Miguel Velez, Pooyan Jamshidi, Norbert Siegmund, Sven Apel, and Christian
Kästner.2022. OnDebuggingthePerformanceofConfigurableSoftwareSystems:
Developer Needs and Tailored Tool Support. In Proceedings of the 44th Inter-
national Conference on Software Engineering (Pittsburgh, Pennsylvania) (ICSE
’22). Association for Computing Machinery, New York, NY, USA, 1571ś1583.
https://doi.org/10.1145/3510003.3510043
[26]Jin Wu, Jian Dong, Ruili Fang, Wen Zhang, Wenwen Wang, and Decheng
Zuo. 2022. FADATest: Fast and Adaptive Performance Regression Testing of
Dynamic Binary Translation Systems. In Proceedings of the 44th International
Conference on Software Engineering (Pittsburgh, Pennsylvania) (ICSE ’22) . As-
sociation for Computing Machinery, New York, NY, USA, 896ś908. https:
//doi.org/10.1145/3510003.3510169
[27]Jing Xu. 2012. Rule-based automatic software performance diagnosis and im-
provement. PerformanceEvaluation 69,11(2012),525ś550. https://doi.org/10.
1016/j.peva.2009.11.003
1725