On the Effectiveness of Unified Debugging:
An Extensive Study on 16 Program Repair Systems
Samuel Benton
The University of Texas at Dallas
Samuel.Benton1@utdallas.eduXia Li‚àó
Kennesaw State University
xli37@kennesaw.edu
Yiling Lou‚àó
Peking University
louyiling@pku.edu.cnLingming Zhang
University of Illinois at Urbana-Champaign
lingming@illinois.edu
ABSTRACT
Automated debuggingtechniques, including faultlocalization and
program repair, have been studied for over a decade. However, the
onlyexistingconnectionbetweenfaultlocalizationandprogramre-
pairisthatfaultlocalizationcomputesthepotentialbuggyelements
for program repair to patch. Recently, a pioneering work, ProFL,explored the idea of unified debugging to unify fault localization
andprogramrepairintheotherdirectionforthefirsttimetoboost
both areas. More specifically, ProFL utilizes the patch execution
resultsfromonestate-of-the-artrepairsystem,PraPR,tohelpim-
provestate-of-the-artfaultlocalization.Inthisway,ProFLnotonly
improves fault localization for manual repair, but also extends the
application scope of automated repair to all possible bugs (not only
thesmallratioofbugsthatcanbeautomaticallyfixed).However,
ProFL only considers one APR system (i.e., PraPR), and it is not
clearhowotherexistingAPRsystemsbasedondifferentdesigns
contributetounifieddebugging.Inthiswork,weperformanexten-sivestudyoftheunified-debuggingapproachon16state-of-the-art
program repair systems for thefi rst time. Our experimental results
on the widely studied Defects4J benchmark suite reveal variouspractical guidelines for unified debugging, such as (1) nearly all
thestudied16repairsystemscanpositivelycontributetounified
debugging despite their varying repairing capabilities, (2) repairsystems targeting multi-edit patches can bring extraneous noise
intounifieddebugging,(3)repairsystemswithmoreexecuted/plau-
sible patchestend toperform better forunified debugging,and (4)
unified debugging effectiveness does not rely on the availability of
correctpatchesinautomatedrepair.Basedonourresults,wefurther
proposeanadvancedunifieddebuggingtechnique,UniDebug++,
whichcanlocalizeover20%morebugswithinTop-1positionsthan
state-of-the-art unified debugging technique, ProFL.
‚àóThis work was mainly done when they are (visiting) PhD students at UT Dallas.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation
on thefi rst page. Copyrights for components of this work owned by others than ACMmustbehonored.Abstractingwithcreditispermitted.Tocopyotherwise,orrepublish,topostonserversortoredistributetolists,requirespriorspecificpermissionand/ora
fee. Request permissions from permissions@acm.org.
ASE ‚Äô20, September 21‚Äì25, 2020, Virtual Event, Australia
¬© 2020 Association for Computing Machinery.
ACM ISBN 978-1-4503-6768-4/20/09...$15.00
https://doi.org/10.1145/3324884.3416566CCS CONCEPTS
‚Ä¢Software and its engineering ‚ÜíSoftware testing and de-
bugging.
KEYWORDS
Unified debugging, Program repair, Fault localization
ACM Reference Format:
SamuelBenton,XiaLi,YilingLou,andLingmingZhang.2020.OntheEffec-
tiveness ofUnified Debugging:An ExtensiveStudy on16 ProgramRepair
Systems. In 35th IEEE/ACM International Conference on Automated Software
Engineering (ASE ‚Äô20), September 21‚Äì25, 2020, Virtual Event, Australia. ACM,
New York, NY, USA, 12 pages. https://doi.org/10.1145/3324884.3416566
1 INTRODUCTION
Withtherapiddevelopmentofinformationtechnology,software
systems have been widely adopted in almost all aspects of modern
society. However, software bugs (also called software faults in this
paper)areinevitablebecauseofthecomplexityofmodernsoftware
systems. Software faults can cause software systems to crash or
perform unexpected behaviors, both scenarios resulting in disas-ter, e.g., costing trillions of dollars infi nancial loss and affecting
billionsofpeople[ 1].Inpractice,softwaredebuggingisessential
forremovingbugsfromexistingfaultysoftwaresystems.Manual
debugging, however, can be extremely challenging, tedious, andcostly. Such impediments consume over 50% of the development
time/effort [ 48] and cost the global economy billions of dollars [ 7].
To date, a huge body of research effort has been dedicated to
automateddebuggingtorelievedeveloperburdens,investigating
both fault localization [ 4,15,21,23,38,40,54,59,62,64] and auto-
matedprogramrepair[ 9,13,14,25,26,28‚Äì32,35,36,43,51,57,61]
techniques. Fault localization aims to precisely localize buggy el-
ements within a buggy system based on dynamic and/or static
programanalysis,andcanautomaticallyproducearankedlistof
suspiciouscodeelementsfordevelopers,reducingtheireffortfor
manual bug checking. Classic spectrum-based fault localization
(SBFL) techniques [ 4,15,23] mainly analyze the statistical correla-
tion between code coverage and test outcomes to infer potential
buggylocations.Forexample,acodeelementprimarilyexecuted
byfailedtestsarelikelytobemoresuspicious.However,usingonly
coverage information may not be precise enough. Therefore re-
searchersfurtherproposemutation-basedfaultlocalization(MBFL)
techniques [ 22,38,40,63] by further considering the impact infor-
mation between mutated code elements and tests (simulated via
mutations). Recently, machine learning techniques have been used
9072020 35th IEEE/ACM International Conference on Automated Software Engineering (ASE)
to combine various dimensions of debugging information for more
powerful fault localization [6, 21, 22, 60].
Whilefaultlocalizationstillrequiresmanualrepair, automated
programrepair (APR)aimstodirectlyfixsoftwarebugsautomati-
cally with minimal human intervention. A typical test-driven APR
techniquetakesafaultyprogramanditstestsuiteasinputandgen-
eratesprogrampatcheswiththeendgoaltofindapatchpassingalltests.Duetoitspromisingfuture,variousAPRtechniqueshavebeen
proposed, including search-based, semantics-driven, and learning-
basedtechniques[ 14,20,35,59,61].Formoredetails,pleaserefer
to recent surveys on fault localization [56] and APR [37].
Despiteextensiveresearchonautomateddebuggingoverthepast
decades, we still lack practical automated debugging techniques.Current fault localization techniques has limited effectiveness in
practice‚Äìtheyeitherrequiremassivetrainingdatathatmaynot
always be available [ 21,49] or are ineffective for debugging real-
world systems [ 18,41]. Furthermore, it is rather challenging for
APR techniques tofi x all possible bugs ‚Äì even state-of-the-art APR
techniques [ 10,14,46] can onlyfi x a small ratio of real bugs (i.e.,
<20% for Defects4J [16]) automatically.
Toenablemorepracticaldebugging,theunifieddebuggingap-
proach,ProFL,wasrecentlyproposedtounifyfaultlocalizationand
APR to boost both areas [ 33,34]. While both fault localization and
APR havebeenstudied forover adecade, theironly priorconnec-
tion is that fault localization is leveraged as a supplier for pointing
out potentially buggy locations for APR tofi x. The unified debug-
ging approach ProFL unifies the two areas in the other direction
forthefirsttime,i.e.leveraginglargenumberofpatchexecutionre-sults generated during APR (even when APR fails tofi x the bug) tofurtherboostfaultlocalization.Thebasicintuitionisthatifapatch
passessomeoriginallyfailingtest(s),thepatchedlocationisvery
likely to have some close relationship with the real buggy location
(e.g., sharing the same method or even same line), since otherwise
thepatchcannotmutethebugimpactandpasstheoriginallyfail-
ing test(s). Using the recent PraPR [ 10] APR system, ProFL is able
to substantially boost/outperform state-of-the-art SBFL [ 3,15,23],
MBFL [22,38,40,63], and unsupervised/supervised learning based
fault localization [21, 49, 64].
In this way, given any buggy project, ProFL not only directly
returnsthepatcheswhen automatedrepair works,butalsoprovides
improved fault localization hints for manual repair for all other
cases. That is, ProFL not only significantly improves fault localiza-
tion formanual repair, but also extends the application scope of
automated repair to all possible bugs (not only the small portion of
bugs that can be automaticallyfi xed).
Despite this promising direction, the ProFL work only considers
one APR system (i.e., PraPR), while there are many other available
APR systems based on different designs and it is not clear how
other APR systems contribute to unified debugging. Therefore,
tobridgethisgap,weconductthefirstextensivestudyofunified
debuggingon16state-of-the-artAPRsystems.These16systems
represent recent public Java APR systems that execute without
requiringspecializeddataorinfrastructure.Theseselectedsystems
utilize constraint-based [ 9,36,58], heuristic-based [ 14,35,61], and
template-based [ 19,24,25] repair approaches seen in recent repair
literature. Furthermore,we use theDefects4J benchmark suitefor
ourevaluationsinceitisthemostwidelyusedbenchmarkinrecentfaultlocalizationandAPRwork(includingtheunifieddebugging
work[33]).Ourexperimentalresultsrevealvariouspracticalguide-
linesforfurtheradvancingunifieddebuggingandevensoftware
debugging in general. To summarize, this paper makes the follow-
ing main contributions:
‚Ä¢Study.Thispaperpresentsthefirstextensivestudyofuni-
fied debugging using 16 state-of-the-art APR systems.
‚Ä¢Dataset. Our detailed experimental data (including patch
executioninformation,experimentalscript,andresultanaly-sisforfaultlocalization,APR,andunifieddebugging)onthe
studied Defects4J subjects are publicly available online [ 2].
‚Ä¢Guidelines. Our study reveals various practical guidelines,
including: (1) nearly all the studied 16 APR tools can pos-itively contribute to unified debugging despite their vary-
ing repairing capabilities, (2) APR tools targeting multi-edit
patches bring noise and degrade performance for unified de-bugging,(3)APRtoolswithmoreexecuted/plausiblepatches
tend to perform better for unified debugging, and (4) uni-
fieddebuggingeffectivenessdoesnotexclusivelyrelyonthe
availability of correct patches from APR.
‚Ä¢Technique. Based on our study results, we further pro-
poseanadvancedunifieddebuggingtechnique,UniDebug++,
whichcanlocalize21%morebugswithinTop-1thanstate-
of-the-art unified debugging technique, ProFL.
2 STUDIED APPROACH
In this section, wefi rst briefly discuss the traditional fault localiza-
tionandprogramrepairprocess(Section2.1)tomotivateunified
debugging. Then, we present the basic process for the studied uni-
fieddebuggingprocess(Section2.2).Lastly,wepresentareal-world
example to further motivate our study in this paper (Section 2.3).
2.1 Fault Localization and Program Repair
Given a buggy program and its failing test suite, test-based fault
localization computes each code element‚Äôs probability to be buggy
based on various techniques [ 4,5,21,40,65]. For example, the
widely studied spectrum-based fault localization (SBFL) [ 4,15,23]
willcollectthedynamiccoverageinformationforeachfailing/pass-
ingtesttocomputeeachcodeelement‚Äôssuspiciousnessvalue.In
thisway,developerscanchoosetodirectlystart manualrepair with
the help of such suspiciousness information.
Alternatively,developerscanalsochoosetodirectlyperform au-
tomated program repair (APR) [14,35,61]. Typical APR techniques
leverage fault localization techniques to compute the potential
buggylocationsforpatching,e.g.,theOchiaiSBFLtechniquehas
been widely used in recent APR work, such as PraPR [ 10], Sim-
Fix[14],andCapGen[ 55].Afterthepatchgenerationandvalidation,
all theplausible patches (i.e., the patches that can pass all tests) are
returned for manual inspection tofi nd thefinal correct patches (i.e.,
thepatchessemanticallyequivalenttodeveloperpatches).Inthis
way,thefinalcorrectpatchesaretheonlyusefuloutcomefromAPR;
in fact, even plausible but incorrect patches are treated as harmful
intraditionalAPRwork[ 58],sincetheyrequiretime-consuming
and tedious manual inspection. However, to date, even state-of-
the-artAPRcanonlyproducecorrectpatchesforasmallratioof
real-world bugs, making APR a waste of resources for all the other
908Figure 1: Unified debugging
cases.Forexample,thecurrentmosteffectiveAPRwork[ 10,26,46]
cannotevenfix20%ofbugsfromthewidelystudiedDefects4J[ 16].
2.2 Unified Debugging
To further boost both the fault localization and APR areas, unified
debugging [ 33,34] aims to unify these two areas from the other
directionforthefirsttime.Thebasicinsightofunifieddebuggingis
thatthemassivepatchexecutioninformationfromAPR(evenones
thatdonotleadtocorrectpatches)canfurtherhelpsubstantially
improvefaultlocalizationtofacilitatemanualrepair.Inthisway,
unifieddebuggingcanreportcorrectpatcheswhenpossible,and
moreimportantlycanalsoreturnrefinedfaultlocalizationforall
cases (even the cases without correct patches). Unified debugging
not onlyextends theapplication scopeof APRto allpossible bugs
(notonlythebugsthatcanbedirectlyautomaticallyfixed),butalso
providesmoreprecisefaultlocalization.Forexample,ProFL[ 33],
thefi rst unified debugging technique based on the recent PraPR
APRsystem, significantlyimproves/outperforms variousstate-of-
the-artfaultlocalizationtechniques(e.g.,SBFL[ 4,15,23],MBFL[22,
38, 40, 63], and even learning-based techniques [21, 64]).
The basic assumption of unified debugging is that if a patch can
pass some originally failing tests, its patch location may be closely
related to the actually buggy locations (e.g., sharing the same code
element, such as method). Similarly, if a patch fails some originally
passing tests, its patch location may be closely related to correct
locations since otherwise the passing tests would have been failed
before patching [ 34]. In this way, all the generated patches can
be categorized into the following categories according to execu-tion information automatically collected during patch validation
forunifieddebugging:(1) CleanFixPatches :patchespassingonat
least one originally failed test and not failing on any originallypassed test, (2) NoisyFix Patches : patches passing on at least one
originally failed tests but also failing on some originally passed
tests,(3)NoneFixPatches :patchesnotimpactingtheoutcomefor
any originally failed or passed test, (4) NegFix Patches : patches not
passing any originally failed test but failing on some originallypassed tests. Note that all such patch validation information can
be directly obtained from the studied test-based APR tools. Unified
debuggingsimplyleveragessuchexistinginformationtoclassify
each patch into the aforementioned categories.Table 1: Example of Math-32
Suspicious Method SBFL PraPR Kali-A TBar UniDebug+
PolyhedronsSet.<init> 1.0NoneFix Unmodified Unmodified NoneFix
PolygonsSet.compute... 1.0NoneFix CleanFix NoneFix CleanFix
PolygonsSet.followLoop 1.0NoneFix NoneFix Unmodified NoneFix
AVLTree.getNotSmaller 1.0NoneFix NoneFix NoneFix NoneFix
TheoverallapproachofunifieddebuggingispresentedinFig-
ure1.Givenanybuggyprogramanditstestsuite,unifieddebugging
first applies off-the-shelf APR systems to generate and execute var-
ious possible patches. Then, while existing APR work only returns
the correct patches to the developers, unified debugging further
utilizestheexecutioninformationforallpatchesandcategorizes
them into relevant categories discussed in the previous paragraph.
Then, for each code element (e.g., method), unified debugging then
adopts the best category from its corresponding patches according
tothispredefinedorder(i.e.,CleanFix>NoisyFix>NoneFix>Neg-
Fix)[34].Finally,alltheelementsarere-rankedfirstaccordingto
their patch categories, e.g., all elements with the CleanFix category
are ranked higher than all elements with the NoisyFix category;
after that, the elements within the samecategory are then further
re-ranked in the descending order by their initial suspiciousness
scorescomputedbyanyexistingfaultlocalizationtechnique(i.e.,
Ochiai[4]bydefault).Inthisway,thedeveloperswillobtainlargely
refinedfaultlocalizationforallpossiblebugs(evenincludingthe
case where no correct or plausible patch is found).
2.3 Motivating Example
While the existing unified debugging technique ProFL has demon-
strated promising results, in this section, we use Math-32 from De-
fects4J (V1.0.0) [ 16], a widely used real-world Java bug benchmark,
to motivate our study. Math-32 denotes the 32nd buggy version
ofApacheCommonsMathproject.Thebugislocatedinmethod
computeGeometricalProperties of Class PolygonsSet.
Table1shows4examplesuspiciousmethodsincludingtheac-
tual buggy method shown in gray. Please note that we disregard
theargumentssincetheclassandmethodnamescansufficiently
distinguish them.
Inthetable,Column‚ÄúSBFL‚Äùindicatesthesuspiciousnessscore
of each method according to the state-of-the-art SBFL techniqueOchiai [
4] with aggregation strategy [ 49], which aggregates the
maximumsuspiciousnessvaluesfromstatementstomethodsand
909protected void computeGeometricalProperties() {
...
if(v.length == 0) {
final BSPTree <Euclidean2D > tree = getTree( false );
- if((Boolean) tree.getAttribute()) {
+ if(false ){
// the instance covers the whole space
setSize(Double.POSITIVE_INFINITY);
setBarycenter(Vector2D.NaN);
...
}protected void computeGeometricalProperties() {
...
if(v.length == 0) {
final BSPTree <Euclidean2D > tree = getTree( false );
- if((Boolean) tree.getAttribute()) {
+ if(tree.getCut() == null && (Boolean) tree.getAttribute()) {
// the instance covers the whole space
setSize(Double.POSITIVE_INFINITY);
setBarycenter(Vector2D.NaN);
...
}
Figure 2: Generated patch of Kali-A and developer patch for Math-32
has been demonstrated to substantially outperform raw method-
level SBFL. Columns "PraPR", "Kali-A", and "TBar" represent the
unifieddebuggingapproaches usingthepatch-executioninforma-
tionfrom APRsystems PraPR,Kali-A, andTBar, respectively. The
patchcategoryinformationforeachmethodisincludedinthetable.
Unmodified,herebyreferredtoasNon-Modify,representsanew
patchcategoryimplyingthatthesecodeelementsareneverpatched
by an APR tool. Lastly, Column "UniDebug+" presents the tech-
niquesimplyusingallpatchesfromthepriorthreeAPRsystems.
From the motivating example, we have the following interesting
findings:
First, unified debugging using other APR systems can have
promisingfaultlocalizationresultsevenwhenthePraPRsystem
used by ProFL cannot help improve the performance. For example,
Kali-Acandirectlyrankthebuggymethodatthe1stlocation,while
bothSBFLandProFLwithPraPRrankthebugatthe4thlocation.
Figure2representsthepatchgeneratedbyKali-A(leftside)andthecorrectpatchprovidedbydevelopers(rightside).Fromthepatches,
we found that Kali-A generates a patch by changing the buggy
conditional statement into if (false) which is useless forfixing
the real bug; however, this patch does help pinpoint the actual bug
location, demonstrating the generality of unified debugging for
all possible APR systems. Thisfi nding motivates us to perform an
extensive study to investigate the effectiveness of different APR
systemsforunifieddebugging.Second,differentAPRsystemshavedifferentunifieddebuggingperformancesandcombiningthemmaypotentiallyresultinevenmorepowerfulunifieddebugging.Shown
in the last column of Table 1, simply combining all patches gen-erated by different APR systems can also localize the bug within
Top-1.
3 STUDY DESIGN
3.1 Research Questions
Inthisstudy,weaimtoinvestigatethefollowingresearchquestions:
‚Ä¢RQ1:How does unified debugging perform with all studied
APR systems?
‚Ä¢RQ2:HowdounmodifiedcodeelementsduringAPRimpact
unified debugging?
‚Ä¢RQ3:Howdoes unified debuggingcorrelatewith program
repair effectiveness?
‚Ä¢RQ4:How do we further advance state-of-the-art unified
debugging with all studied APR systems?Tool Category Tools
Constraint-based ACS, Cardumen, Dynamoth
Heuristic-based Arja, GenProg-A, jGenProg, jKali,
jMutRepair,Kali-A,RSRepair-A, Simfix
Template-based AVATAR, FixMiner, kPar, PraPR, TBar
Table 2: Repair systems studied
Project Name # Bugs # Tests LOC
Chart JFreeChart 262,205 96K
Lang Apache Lang 652,245 22K
Math Apache Math 1063,602 85K
Time Joda-Time 274,130 28K
Total 22412,362 231K
Table 3: Studied bugs from Defects4J v1.0.0
3.2 Experimental Setup
For this study, we considered all the 16 program repair systems
accessiblefromarecentstudy[ 27].Furthermore,wealsoconsid-
eredtherecentPraPRrepairsystem[ 10]whichtheinitialunified
debuggingworkisbasedon.Table2showsthebreakdownofall
the APR systems studied, including: heuristic-based - Arja [ 61],
GenProg-A [ 61], jGenProg [ 35], jKali [35], jMutRepair [ 35], Kali-A
[61],RSRepair-A[ 61],andSimfix[ 14];constraint-based-ACS[ 58],
Cardumen [ 36], and Dynamoth [ 9]; and template-based - AVATAR
[25], FixMiner [ 19], kPar [24], TBar [26], and PraPR [ 10]. We man-
uallymodifiedallstudiedAPRsystemstocollectthedetailedpatch
execution information required by unified debugging, and ensured
thatourmodifiedversiondidnotimpactthetoolfunctionality.1
Eachsystemusedoriginaltimesettingssuggestedbytheoriginal
papers.
Most of the studied APR systems have been implemented to
target version 1.0.0 (and older) of the widely used Defects4J bench-
mark[16],whichincludes357real-worldbugsfromfivereal-world
software systems. Meanwhile, in our evaluation process, we found
thatmanyofthestudiedtools(e.g.,CapGen,ACS,Arja,GenProg-A,
etc.) do not support or cannot successfully execute Closure from
Defects4J1.0.0(accordingtotheiroriginalpublications). Therefore,
for fair comparison, all of our experiments are performed on the
remainingfoursubjectsfromDefects4J1.0.0;theChart,Time,Lang,
and Math projects. Detailed statistics are shown in Table 3.
Each tool was executed using each the same JDK version found
inthetool‚Äôsoriginalpublication,allowingustoobtainrepairexe-
cutionresultsascloseaspossibletothetool‚Äôsoriginalresults.Thus,
1Note that we fail to get the our modified version of Nopol functional due to the
specificdesignofNopol.Nevertheless,webelievethatexcludingonespecifictooldoes
not impact the generalfi ndings and contributions of this study (especially we also
have many other constraint-based APR systems studied).
910inourexperiments,weultimatelyutilizedtwoJDKversions,JDK
1.8.0.242(herebyreferredtoasJDK1.8)andJDK1.7.0.80(hereby
referred to as JDK 1.7). Systems Simfix and Dynamoth executed
using JDK 1.8 exclusively. Systems Cardumen, jGenProg, jKali, and
jGenProg executed usingJDK 1.8 and validatedsystem test suites
with JDK 1.7. All other systems executed using JDK 1.7 exclusively.
All our experiments were conducted within the following en-
vironment: 36 3.0GHz Intel Xeon Platinum Processors, 60GBs of
memory, and Ubuntu 18.04.4 LTS operating system.
3.3 Implementation Details
3.3.1 ProFL Configuration. Although unified debugging can be
used to refine any existing fault localization technique, by default,
the original unified debugging work, ProFL, utilizes APR to re-
finestate-of-the-artSBFLtechnique,Ochiai[ 3]withaggregation
strategy [ 49]. Actually, the original ProFL work demonstrates that
unifieddebugginghasconsistentperformanceforrefiningdifferent
state-of-the-art faultlocalization techniques.Therefore, in thispa-
per,wealsofocusonusingOchiai(withaggregation)toinvestigate
the impact of different repair systems. Furthermore, following the
originalProFLwork,thisstudyalsofocusesonmethod-levelfaultlo-
calization (i.e., localizing potential buggy methods), as researchers
have demonstrated that class-level fault localization can be too
coarse-grained [ 18] while statement-level fault localization can be
toofi ne-grained and miss necessary contextual information [41].
3.3.2 Non-ModifyCategory. FortheoriginalProFLwork,theused
PraPRrepairsystem[ 10]isextremelyfastduetothebytecode-level
manipulationand cangeneratepatchesfor almostallthe possible
suspicious methods, i.e., methods executed by failed tests (since
methods not executed by failed tests should not be responsible for
the current test failures). However, for all other APR tools, there
may exist many suspicious methods without any patch, since itisexpensiveforAPRtoolstogeneratepatchesforeverymethod.
Therefore, besides the four categories of methods mentioned in
Section2.2,wecreateanewcategory,Non-Modify,torepresentthe
methodsthatdonotreceiveanypatchforaspecificAPRsystem.
Itisunclearhowthisnewcategorycompareswiththeotherfour
categoriesstudiedintheoriginalunifieddebuggingwork.There-
fore, we explore the impact of the ranking this new Non-ModifycategorywithintheexistingfourProFLcategoriesinSection4.2.
Note that as the default setting, we put Non-Modify alongside the
NegFixcategorysincethepluralityof allpatches fallintotheNegFix
category(thus themajorityofNon-Modify methodsmayalsofall
intotheNegFixcategoryiftheyhadbeengeneratedwithpatches).
3.3.3 RepairToolIntegrationwithProFL. TheoriginalProFLtool
hasbeenimplementedasapubliclyavailableMavenplugin.Weob-tainedtheoriginalProFLsourcecodefromtheauthorsandanalyzed
the interface between ProFL and its underlying APR system. Then,
we modified all the 16 studied APR systems to produce detailed
patch execution information consistent with the original ProFLinterface (e.g., regarding the patch location, failing and passing
tests for each patch). In this way, we can safely replace the original
PraPRsystemwithanyotherstudiedAPRsystemsforourstudy.
PleasenotethatwealsoaugmenttheoriginalProFLcodetohandle
the new Non-Modify method category.3.4 Evaluation Metrics
Following prior work [ 21,49,64], we measure the number of bugs
localized within Top-1, Top-3, and Top-5 positions as the primary
metricsforthisstudy.Thereasonisthatresearchershaveobserved
that most developers will abort automated debugging tools if they
cannot return the actual buggy elements within the Top-5 posi-tions [
18]. Specifically, given a set of methods which tie for the
same rank, each method is assigned the worstrank of the tied
methods,followingpriorwork[ 21,22,33].Furthermore,wealso
presentthemeanfirstrank(MFR)andmeanaveragerank(MAR)
resultswidelyusedinpriorfaultlocalization[ 21,22]andunified
debugging[ 33]work.Morespecifically,forpreciselocalizationof
allbuggyelementsofeachbug,wecomputetheaverageranking
of all the buggy elements for each bug; MAR is simply the mean of
theaveragerankingof allbugs. Similarly, forabug withmultiple
buggyelements,thelocalizationofthefirstbuggyelementiscrit-
icalsincetherestbuggyelementsmaybedirectlylocalizedafter
that;therefore,weuseMFRtocomputethemeanofthefirstbuggy
element‚Äôs rank for each bug.
4 RESULT ANALYSIS
4.1 RQ1 - Performance of Unified Debugging
with Different APR systems
Inthisresearchquestion,wefirstinvestigatetheeffectivenessof
unifieddebuggingonallthe16studiedAPRsystems.Figure3shows
thefaultlocalizationresultsonallthestudiedsubjects(i.e.,Lang,
Chart, Time, and Math from the Defects4J benchmark) in terms of
the Top-1, Top-3, Top-5, MFR, and MAR metrics. The upper sub-
figurerepresentstheTop-Nresultsandbottomsub-figureindicates
theMFR/MARresults.Eachbarinbothsub-figuresrepresentsdif-
ferent APR systems. Note that we use the default treatment for
the Non-Modify category which inserts such methods alongside
the NegFix category, i.e., CleanFix > NoisyFix > NoneFix > NegFix
= Non-Modify (discussed in Section 3.3.2). Also note that the 16repair systems in thisfi gure are ordered chronologically with re-
specttothedateforeachpublication,followingtheexistingAPR
study [27]. We also include the result of state-of-the-art SBFL (i.e.,
Ochiai with aggregation) and thefi rst unified debugging technique
(i.e., ProFL with PraPR) in the last for comparison (Note that ProFL
has been demonstrated to outperform/improve all state-of-the-art
fault localization [ 33,34]). From thefi gure, we have the following
observations. First, unified debugging with most APR systems per-
formsbetterthanstate-of-the-artSBFL!Forexample,intermsof
Top-1, 15 out of 16 tools can help improve SBFL and only Arja fails
to meet the initial SBFL results. That said, Arja can still localize 73
faults within Top-1 which is fairly close to the SBFL result. This
findingindicatesthebroadapplicabilityoftheunifieddebugging
approach. Second, even though existing APR study [ 27] has ob-
servedthatmorerecentAPRsystemscanfixmorebugsthanearlier
systems, there is no obvious trend showing that unified debugging
with more recent (i.e., chronologically later) APR systems can help
localizemorebugsthanearlierones.Thisfindingdemonstratesthat
APR systems‚Äô capability to produce correct patches is not highly
correlatedtotheunifieddebuggingeffectivenessinfaultlocaliza-
tion. Lastly, different results of the 16 APR systems indicate that
911.+'*/'-'
--'+*'-*
.*'*+',/
/,'+('-(
.,'+('-+
./'+''-(
/('*/'-(
..'**',/
-,'*)',.
.*'+&'-&
-)'+('-)
'&&'+('-&
/*'+'',/
/,'+('-)
/,'+)'-)
'&)',+'.)
-+'**',/
&(&*&,&.&'&&'(&'*&',&'.&(&&
#' #) #+#
 # 	  # # "   ! !	   
,"+/-".0
,",*-"/(
,",,-"/,
,"*0-".)
,"+)-".+
,"+*-".'
,",*-"/)
,",.-"/+
,"-,-"0)
,"+/-"./
,",)-".0
,",,-"0.
,"+(-".,
,")(-",,
,"))-",-
+"-.,"0/,"-*-"0'
'"''("'')"''*"''+"'',"''-"''."''/"''
	 		#	

 
$ 	 
$ $ !    	  

 
Figure 3: Unified debugging results with all studied APR systems
each APR system has its own advantages and disadvantages for
unified debugging. The potential reason is that some tools haveexclusive abilities to repair various classes of bugs by leveraging
differentalgorithmstogeneratepatches,incurringvariouslevelsof
effectivenessforfaultlocalization.Thisfindingfurthermotivates
us to combine multiple APR systems to advance state-of-the-art
unified debugging (studied in Section 4.4).
Finding 1: Despite their varying repair capabilities, almost
all the studied 16 APR systems individually boost state-of-
the-art SBFL and contribute to unified debugging.
4.1.1Qu alitative Analysis. Now we perform a detailed qualitative
analysis to investigate the different performanceof different APR
systems. Table 4 shows a subset of the unified debugging results
withdifferentAPRsystemsfromMath-77.Notethatwealsoinclude
the results for UniDebug+, which simply uses all patches fromdifferent APR systems. Column ‚ÄúEID‚Äù describes the ID for eachmethod. Column ‚ÄúCategory‚Äù represents the category computedfor each method based on each tool. Column ‚ÄúRank‚Äù describes
howeachmethodranksinthefinalresults. Math-77failsontwo
tests, testBasicFunctions within class ArrayRealVectorTest
and testBasicFunctions within class SparseRealVectorTest ,
both from the org.apache.commons.math.linear package. The
buggymethodsforMath-77involvemodificationsofmethods e4
and e5, according to developer patch in Figure 4. According to
traditional SBFL, allfi ve methods tie and are ranked 5th (according
to theworstranking). We next discuss the performance of three
example APR systems for unified debugging:
TBarisabletogenerateaCleanFixpatchbyexclusivelymodifying
method e4, shown in Figure 5. This CleanFix patch successfully
passes one of the originally failed tests, and passes every othertest.Eventhoughthispatchisnotacorrectpatch,itdoeshelpto
boosttherankofonebuggymethodtoTop-1.Thereasonisthat
the patch shares the same location with the bug and thus is able to
mutethebugimpactviamodifyingthereturnvalue.Thisfurther
demonstrates the effectiveness of unified debugging.RSRepair-A
generates 43 NegFix and 355 NoneFix patches across
14 unique methods for this bug. Allfi ve methods in Table 4 are
NoneFix based on RSRepair-A. From this categorization, e1-e5
are ranked the same as the SBFL results. Note that, in this case,although RSRepair-A was not able to improve SBFL, it will not
deteriorate the fault localization results when combining with the
more effective TBar. The reason is that when putting all patches
together, methods with higher patch categories will still be ranked
higher.
Arjais a very interesting case. It actually produces many incorrect
but CleanFix patches for Math-77, including for allfi ve suspicious
methods shown in Table 4. We were surprised by the fact that Arja
canproducesomanyCleanFixsincetheyareusuallyhardtogen-
erate.Diggingintovarioussuchpatches,wefoundthereasonto
be that Arja specifically targets multi-edit patches (i.e., each patch
modifiesmultipleprogramlocations).Forexample,onesuchClean-
FixpatchisshowninFigure6.Inthisway,aslongasone/partof
the multiple edits within a multi-edit patch can make some failing
teststopass,thepatchcanpotentiallybeCleanFix,makingallmod-
ified methods of this patch to be highly ranked. Furthermore, such
noiseincurredbymulti-editAPRsystemscanalsobeharmfulwhen
combining different APR systems for unified debugging. For exam-
ple,showninthelastcolumnofTable4,UniDebug+alsocannot
distinguish thefive suspiciousmethods. Therefore,we excludeall
such multi-edit APR toolswhen combining different APR systems
for unified debugging (Section 4.4) to remove unnecessary noise.
912EID Suspicious MethodSBFL TBar RSRepair-A Arja UniDebug+
Susp. Rank Category Rank Category Rank Category Rank Category Rank
e1 AbstractRealVector:getL1Norm()D 0.707 5 Non-Modify 5 NoneFix 5 CleanFix 5 CleanFix 5
e2 AbstractRealVector:getNorm()D 0.707 5 Non-Modify 5 NoneFix 5 CleanFix 5 CleanFix 5
e3 ArrayRealVector:getL1Norm()D 0.707 5 Non-Modify 5 NoneFix 5 CleanFix 5 CleanFix 5
e4 ArrayRealVector:getLInfNorm()D 0.707 5CleanFix 1NoneFix 5CleanFix 5CleanFix 5
e5 OpenMapRealVector:getLInfNorm()D 0.707 5 Non-Modify 5 NoneFix 5 CleanFix 5 CleanFix 5
Table 4: Unified debugging with different APR systems for Math-77
org.apache.commons .math.linear.Ar rayRealVector. java
public double getLInfNorm() {
double max = 0;
for (double a : data) {
- max += Math.max(max, Math.abs(a));
+ max = Math.max(max, Math.abs(a));
}
return max;
}org.apache.commons.math.linear.OpenMapRealVector.java
-public double getLInfNorm() {
- double max = 0;
- Iterator iter = entrie s.iterator();
- while (iter.hasNext()) {
- iter.advance();
- max += iter.value();-}- return max;
-}
Figure 4: Correct developer patch for Math-77
org.apache.commons.math.linear.Ar rayRealVector. java
public double getLInfNorm() {
double max = 0;
for (double a : data) {
max += Math.max(max, Math.abs(a));
}
- return max;
+ return getDimension();
}
Figure 5: TBar‚Äôs incorrect CleanFix patch for Math-77
Finding2: APRtoolsspecificallytargetingmulti-editpatches
can bring noise into unified debugging, as each multi-edit
patch involves multiple modifications and many modifica-tions are not helpful in muting the bug impacts even the
patch can pass some originally failing test(s).
4.1.2Qu antitativeAnalysis. Sincethecapabilitytoproducecorrect
patchesisnothighlycorrelatedwithunifieddebuggingeffective-
ness, we further performs detailed quantitative analysis to explore
what factors of APR systems are highly correlated to the effec-
tivenessofunifieddebugging.Figure7representsthecorrelation
analysisbetweendifferentfactors ofAPRsystemsandrepresenta-
tivefaultlocalizationmetrics(i.e.,Top-1andMFR).Notethatwe
also excluded APR tools targeting multi-edit patches. In thisfig-
ure, ‚ÄúTotalPatch‚Äù represents the number of all executed compilable
patchesgeneratedfromeachAPRtool,‚ÄúMethodByTotal‚Äùrepresents
thenumberofuniquemethodsmodifiedbyallexecutedpatches,
‚ÄúPlausiblePatch‚Äù represents the number of plausible patches gener-
atedbyeachtool,and‚ÄúMethodsByPlausible‚Äùrepresentsthenumber
ofuniquemethodscoveredbytheplausiblepatches.Withineach
sub-figure, each data point represents one studied APR system,and we perform Pearson Correlation Coefficient analysis [
42]a t
significancelevelof0.05.Fromthisfigure,wecanobservethatAPR
systems tend to perform significantly better for unified debugging
when executing more patches for more methods and/or producing
more plausible patches for more methods. Thefi nding is statis-tically significant for all the sub-figures at the significance levelof 0.05. Although thefi nding is surprisingly uniform, this makesintuitive sense, since APR systems patching more code elements
tendtoaccumulatemoreinformationfordebugging.Thisfinding
suggests future APR systems to explore more patches for morepowerfulunifieddebugging,andalsocallsforresearchforfasterpatch execution (otherwise APR systems cannot afford massive
patch executions).
Finding3: APRsystemsexecutingmorepatchesacrosscode
elements and/or producing more plausible patches tend toperform better for unified debugging, calling for future re-
search on fast & exhaustive patch exploration .
4.2 RQ2 - Impacts of Non-Modify Code
Elements on Unified Debugging
As discussed in Section 3.3.2, we add one new patch category, Non-
Modify, which represents suspicious methods that some APR tools
do not afford to modify. Thefi rst research question has demon-
strated the effectiveness of default unified debugging setting on 16
APR systems by treating Non-Modify equivalently as the fourth
patch category NegFix. In this research question, we further eval-
uate the unified debugging effectiveness when casting the Non-Modify category into each of the four different ProFL categories.Figure 8 represents the representative Top-1 and MFR results ofall the 16 APR systems with such four settings represented withlines in different colors. From thefi gure, we can observe that for
mostsystems,castingNon-ModifycodeelementsintothelastNeg-
Fix category performs better than casting them into other threecategories in terms of both Top-1 and MFR. For example, Simfix
can localize 100 bugs within Top-1 when casting Non-Modify into
NegFix category, and can only localize 95/93/90 bugs within Top-1
when casting Non-Modify into NoneFix/NoisyFix/CleanFix cate-
gory.Also,intermsofMFR,Kali-Acanachieve5.53withNegFix,
which is also significantly better than Kali-A with other three cate-
gories (6.76/6.25/6.45). The reason is that NegFix patches are more
prevalent for most code elements (including Non-Modify ones),
while other patch categories can be harder to generate.
Finding4: Non-Modifycodeelements(i.e.,elementswithno
patches)canbetreatedinthesamewayaselementswithonly
NegFixpatches(i.e.,thepatchesthatcannotfixanyfailing
testbut cancauseoriginally passingtests tofail)for preciseunified debugging.
913// ArrayRealVector. java
public double getLInfNorm() {
double max = 0;
for (double a : data) {
max+=Math.max(max,Math.abs(a));
}
- return max;
+ return data.length;
}
(a)Modifications for ArrayRealVector.java// OpenMapRealVector.java
public double getLInfNorm() {
double max = 0;
Iterator iter=entries .iterator();
while (iter.hasNext()) {
iter.advance();
max += iter.value();
}
- return max;
+ return virtualSize;
}
(b)Modifications for OpenMapRealVector.java// AbstractRealVector. java
public double getL1Norm() {
double norm = 0;
Iterator <Entry> it = sparseIterator();
Entry e;
+ while (it.hasNext()&&(e=it.next())!= null ){
+ norm += Math. abs(e.getVal ue());
+}
while (it.hasNext()&&(e=it.next())!= null ){
norm += Math.abs (e.getVa lue());
}
return norm;
}
(c)Modifications for AbstractRealVector.java
Figure 6: An incorrect Arja CleanFix patch (with three modified methods) for Math-77







R = 0.78
p = 0.0016
8090100110
102103104105106
TotalPatchTop1






R = 0.66
p = 0.015
8090100110
101.5102102.5103103.5104
MethodByTotalTop1







R = 0.83
p = 0.00041
8090100110
100.5101101.5102102.5103
PlausiblePatchTop1







R = 0.9
p = 2.5e‚àí05
8090100110
100100.5101101.5102
MethodsByPlausibleTop1







R = ‚àí 0.69
p = 0.0092
5.05.5
102103104105106
TotalPatchMFR






R = ‚àí 0.67
p = 0.012
5.05.56.0
101.5102102.5103103.5104
MethodByTotalMFR









R = ‚àí 0.86
p = 0.00017
5.05.5
100.5101101.5102102.5103
PlausiblePatchMFR








R = ‚àí 0.72
p = 0.0059
5.05.56.0
100100.5101101.5102
MethodsByPlausibleMFR
Figure 7: Correlation analysis






 
8090100
ACS Arja AVATAR Cardumen Dynamoth FixMiner GenProg ‚àíA jGenProg jKali jMutRepair Kali ‚àíA kPar PraPR RSRepair ‚àíA Simfix TBar
ToolTop1
 
 


 
5.05.56.06.57.07.5
ACS Arja AVATAR Cardumen Dynamoth FixMiner GenProg ‚àíA jGenProg jKali jMutRepair Kali ‚àíA kPar PraPR RSRepair ‚àíA Simfix TBar
ToolMFRNegFix
NoneFixNoisyFixCleanFix
Figure 8: Impact of casting Non-Modify code elements into different categories
4.3 RQ3 - How Does Unified Debugging
Correlate with APR Effectiveness?
APRsystemsallaim toproducecorrectpatchesforas manybugs
as possible. However, this is a rather challenging goal, and even
state-of-the-art APR tools can onlyfi x less than 20% of the studied
bugs[10].Therefore,inthisresearchquestion,weempiricallystudy
whether unified debugging is also limited by the APR effectiveness
(i.e., in producing correct patches). The three sub-figures in Fig-
ure 9 show the representative Top-1 metric for SBFL and unifieddebuggingusingeachAPRsystemon(1)buggyversionswherethe
corresponding APR system has correct patches, (2) buggy versions
withincorrectbutplausiblepatches,and(3)buggyversionswith-
outevenplausiblepatches,respectively.Pleasenotethatweomit
the APR systems that did not present detailed correct patch IDs
in their original publications in thisfi gure. From thefi gures, we
canobservethatdifferentAPRsystemsperformdifferentlyinall
three different bug sets, and almost all APR systems can contribute
to unified debugging to outperform SBFL. More importantly, we
914  



4812
ACS Arja AVATAR Dynamoth FixMiner jKali jMutRepair kPar Simfix TBar
ToolTop1SBFL
ProFL

 


246810
ACS Arja AVATAR Dynamoth FixMiner jKali jMutRepair kPar Simfix TBar
ToolTop1SBFL
ProFL
 
 


 707580859095
ACS Arja AVATAR Dynamoth FixMiner jKali jMutRepair kPar Simfix TBar
ToolTop1SBFL
ProFL
Figure 9: Unified debugging on buggy versions with (1) cor-
rect, (2) incorrect but plausible, and (3) implausible patches
Table 5: Effectiveness of UniDebug+ and UniDebug++
Tech Name Top-1Top-3Top-5MFRMAR
SBFL 751441695.636.90
ProFL 1031651834.675.98
UniDebug+ ùëéùëôùëô951601774.846.16
UniDebug++ ùëéùëôùëô1191681804.485.86
UniDebug+ 1101681824.495.88
UniDebug++ 1251721844.275.71
observe that almost all APR systems consistently outperform SBFL
in all the three bug sets. One potential reason is that as long asapatchcanpasssomeoriginallyfailingtest(s),itspatchlocation
maybecloselyrelatedtotheactualbuggylocationsinceotherwise
it cannot mute the bug impact to pass failing tests. In this way,patchesdonotneedbecorrectoreven plausibletocontributeto
unifieddebugging.Furthermore,eventhepatchesthatonlymake
originally passing tests turn to fail can help eliminate the poten-
tiallycorrect/benignlocationstoalsoboostunifieddebugging. Thisfurtherdemonstratesthegeneralapplicabilityandpromisingfuture
for unified debugging.
Finding 5: Unified debugging effectiveness does not rely
on the availabilityof correct or even plausiblepatches from
APR. Similar as when conducting manual program repair,
APR patch execution results from even incorrect/implausible
patches can still reveal actual buggy locations (when they
pass some failing test(s)) or eliminate correct locations (even
when they only fail on originally passing tests).
4.4 RQ4 - More Advanced Unified Debugging
TocombinethestrengthsofdifferentAPRsystems,onenaivewayis
to simply combine the patches of different APR systems for unifieddebugging,i.e.,theUniDebug+techniquethatwehavetalkedabout.Table 6: Example for UniDebug++
SBFL Tool1 Tool2 Tool3 UniDebug+ UniDebug++
e10.8CleanFix CleanFix CleanFix CleanFix CleanFix(3)
e20.8CleanFix NegFix CleanFix CleanFix CleanFix(2)
e30.8CleanFix NoneFix NoneFix CleanFix CleanFix(1)
Inthisway,thefinalcategoryinformationforacodeelementcan
bedeterminedbythebestcategoryinformationofallpatchesofthe
code element from the combined APR systems. In this section, we
furtherproposeamoreadvancedtechnique,UniDebug++,which
further distinguishes code elements with the same suspiciousness
values in thesame category. More specifically, after assigning the
patch groupcategory(for patches generated byall combined APR
systems)toacodeelementinthecategoryaggregationstep(showninFigure1),wefurthercountthetotalnumberofAPRsystemsthat
generate patches in the same category as this code element.
The intuition is that if more APR systems can assign the best
categoryinformationtoacodeelement,thiselementshouldhave
higherpriorityintherankedlistcomparedtoitstiedpeers.Table6showsansimpleexampletoillustrateUniDebug++.Inthisexample,
elements e1,e2ande3havethesameSBFLsuspiciousnessvalue
0.8andareallintheCleanFixcategoryaccordingtoUniDebug+,therefore they cannot be distinguished when using UniDebug+.In contrast, UniDebug++ further considers the number pf APR
systemsproducingCleanFixpatchesforeachelement.Forexample,
e1has CleanFix patches when using all three APR systems and
shouldberankedhigherthanotherelements.Inthisway,wecan
leverage more precise APR information for more powerful unified
debugging.
Table 5 shows the results of original SBFL, ProFL, UniDebug+
andUniDebug++intermsofTop-1,Top-3,Top-5,MFRandMAR.
Note that as discussed in Section 4.1.1, APR systems specifically
targetingmulti-editpatchescanintroduceextranoiseforunified
debugging and have been excluded for UniDebug+ and UniDe-bug++. Meanwhile, we also include, as references, their variants
which consider all studied APR systems, denoted as UniDebug+ ùëéùëôùëô
and UniDebug++ ùëéùëôùëôin the table. From the results, we have the
following observations. First, UniDebug+ ùëéùëôùëôand UniDebug++ ùëéùëôùëô
perform worse than UniDebug+ and UniDebug++, respectively. In
fact, UniDebug+ ùëéùëôùëôeven performs worse than ProFL which only
uses the PraPR APR system. Thisfi nding further confirms our ear-
lierqualitativeanalysisand Finding2 thatAPRsystemstargeting
multi-editpatchesarenotsuitableforunifieddebugging.Second,
both UniDebug+ and UniDebug++ can significantly outperform
SBFLandProFLinallmetrics.Forexample,UniDebug+canlocalize
110 bugs within Top-1, i.e., 35/7 more than SBFL/ProFL. Third,
UniDebug++canachievethebestresult(evencomparingagainst
our own UniDebug+), localizing 125 faults within Top-1, i.e., 50/22
more than state-of-the-art SBFL/ProFL.
Shown in Section 4.1, APR systems with more plausible patches
tend to perform better in unified debugging. Therefore, we further
study the impact of having different subsets of APR systems forUniDebug+ and UniDebug++. To that end, we rank all APR sys-tems in descending order of the number of additional bugs that
each APR system can come up with plausible patches. In this way,
wecanobservetheeffectivenesstrendofUniDebug+andUniDe-
bug++ with more and more APR systems. Figure 10 presents Top-1
915

105110115120125
1 2 3 4 5 6 7 8 9 10 11 12 13
CombTop1UniDebug+ UniDebug++
Figure 10: UniDebug+ and UniDebug++ with increasing
number of APR systems
results when including more and more APR systems under UniDe-
bug+ and UniDebug++. From thefi gure, we can observe several
interestingfi ndings. First, both UniDebug+ and UniDebug++ over-
all have increasing effectiveness when including more and more
APRsystems.Second,wealsoobservethatbothtechniquestend
to saturate when new systems cannot provide much additional
plausible patches. This actually indicates that a small subset of the
studiedAPRsystems(e.g.,6ofthem)canbecombinedtoachieve
same effectiveness as the whole set. Third, UniDebug++ has much
better effectiveness compared to UniDebug+ which simply puts all
patches from different APR systems together. The reason is that
mostAPRsystemsarehelpfulforfaultlocalization,andthecode
elements ranked high by multiple APR systems are indeed more
likely to be buggy.
PleasenotethatrunningmultipleAPRsystemsforUniDebug+
or UniDebug++ can be costly. Although the efficiency issue is out
ofscopeforthispaper,ourabovefindingdemonstratesthatasmallsubset(e.g.,6)ofthemcanalreadybesufficientforeffectiveunified
debugging. Also,Chen etal. [ 8] haverecently proposeda general
on-the-fly patch-validation framework to substantially speed up
existing APR systems. Furthermore, such APR systems can be run
in parallel to further increase the potential for improved unified
debugging with minimal delay; this not only 1) improves the prob-
ability for thebuggy project to be directlyautomaticallyfi xed via
multiple APR systems, but also 2) further boosts fault localization
for manual repair even when none of the APR systems can directly
fix the bug.
Finding 6: Our new unified debugging technique UniDe-
bug++consideringcommonbehaviorsbetweenmultipleAPR
systems can localize 125 bugs within Top-1, which signifi-
cantly improves the state-of-the-art ProFL by over 20%.
4.5 Threats to Validity
4.5.1 Internal Validity. All of our study results are directly depen-
dent on the correctness of our implementation of all the studiedtechniques. Faulty implementations in any aspect will yield mis-leading / inaccurate results. To mitigate this threat, we reuse the
implementationoftheSBFLandProFLtechniquesobtainedfrom
the ProFL authors. We also obtained the source code for all the
studiedAPRsystemsfromtheauthorstoinvestigatetheirimpact
on unified debugging. Furthermore, we execute both APR systems
withourmodifications(torecorddetailedpatchexecutioninforma-
tion required by unified debugging) and the original APR systemsto ensure that they produce the same results (i.e., our modification
does not change the APR behavior).
4.5.2 Construct Validity. This threatmainly liesin thedependent
variablesormetricsusedinthisstudy.Toreducesuchthreats,we
adoptedthemostwidelyusedmetricsinrecentfaultlocalization[ 21,
22, 49] and unified debugging [33, 34] studies.
4.5.3 ExternalValidity. Toevaluateonreal-worldbugs,wechoose
theDefects4Jv1.0.0datasetwithhundredsofreal-worldbugs,which
isthemost widelyusedbenchmarksuiteforrecent APRandfault
localization work. However, the study results may still not gen-eralizetoallpossiblesystemsinthewild,andweplantofurther
enlarge our benchmark selection in the near future.
Also, since the study aims to investigate the impact of APR
systems for unified debugging, different APR systems may yield
totallydifferentresults. Therefore,wesimplystudied allthestate-
of-the-art APR systems that (1) have publicly available source code
and (2) are applicable to the used Defects4J benchmark [27].
5 RELATED WORK
As the studied unified debugging approach unifies traditional fault
localization and automated program repair (APR) to boost both
areas, in this section, we talk about the related work in both areas.
5.1 Fault Localization
The basic idea of fault localization is to automatically produce a
rankinglistofcodeelements(e.g.,programmethodsorstatements)
basedonthedescendingorderoftheirsuspiciousnessvalues(i.e.,
the probability of being buggy) to help developers in manual de-
bugging or serve as the supplier for APR. Various fault localization
techniqueshavebeenproposedoverthepastdecades.Spectrum-
basedfaultlocalization(SBFL)[ 39,47],oneofthemostclassicfault
localization approaches, has been intensively studied due to its
effectiveness and scalability. Its basic insight is that code elements
primarilyexecutedbyfailedtestsaremoresuspiciousthanelements
primarily executed by passed tests.
To date, various formulae (e.g., based on statistical analysis or
other heuristics) have been proposed to compute code element sus-
piciousness, such as Tarantula [ 15], Ochiai [ 3], SBI [23], and so on.
OnemainlimitationforsuchtraditionalSBFListhatfaultycodeele-mentsmaybecoincidentallyexecutedbypassedtestsandelements
executed by failed tests do not always have real impacts on the
programfailure.Tobridgethegapbetweencoverageandimpactin-formation,mutation-basedfaultlocalization(MBFL)[
22,38,40,63]
has been proposed to transform program source code based onmutation testing [
12] to check the impact of each code element
on test outcomes. The basic idea of MBFL is that if one mutant
incursdifferent failureoutputsof failedtestsbeforeand aftermu-
tation, the corresponding code element of this mutant may havea high impact on program failures, and thus may be the buggy.
MUSE [38] and Metallaxis [ 40] are two widely studied MBFL tech-
niquestargetingtraditionalapplicationscenarios,whileFIFL[ 63]is
a MBFL technique specifically targeting evolving software systems.
ComparedwithMBFL,unifieddebuggingutilizesprogramrepair
informationthataimstofixsoftwarebugsto passmoretestsrather
thanmutationtestingthatwasoriginallyproposedtocreatenew
916artificial bugs to failmore tests; furthermore, unified debugging
hasalsobeenshowntosubstantiallyoutperformstate-of-the-art
MBFL[33,34].Intheliterature,researchershavealsoproposedvar-
iousotherfaultlocalizationtechniques,includingtechniquesbased
on program slicing [ 5,44], development history [ 17], and infor-
mation retrieval[65], as wellas techniques for combiningvarious
dimensions of information via machine learning [21, 49, 60].
5.2 Automated Program Repair
Automatedprogramrepair(APR)aimstodirectlyfixprogrambugs
without human intervention. Given a buggy project, APR tech-
niquesutilizevariousstrategiestoautomaticallygeneratepotential
patches and then validate those patches to check their correct-ness, e.g., based on regression tests [
10], static analysis [ 50], or
formal specifications [ 52]. To date, test-driven APR has been ex-
tensively studied due to the wide adoption of testing in practice. A
typical test-driven APR techniquefi rst applies off-the-shelf faultlocalization techniques (e.g., Ochiai [
3] has been widely used for
APR[10,14,55])topinpointpotentialbuggylocationsforpatching.
Then,anypatchesthatcanpassalltheoriginallyfailingandpassing
tests are called plausible patches, while plausible patches seman-
tically equivalent to corresponding developer patches are called
correctpatches (whicharethefinaloutcomeforAPR).Dependingon
how the patchesare generated, APR [ 10,14,31,58] can becatego-
rized into the following categories [ 11,27]: (1)heuristic-based APR,
whichinvestigatespossiblecodemodificationsforpatchingbyiter-
ating a search space, e.g., GenProg [ 53] uses genetic programming
algorithm to search donor code from existing code for generating
patches;(2) constraint-basedAPR,whichtypicallytransformsthe
APR problem intoa satisfiability problem byconstructing a repair
constraint that the patches should satisfy, e.g., Nopol [ 59] lever-
ages an SMT solver to solve the condition synthesis problem; (3)
template-basedAPR,whichperformsAPRviapredefinedfixingpat-terns,e.g.,FixMiner[
19]automaticallyminesbug-fixpatternsfrom
existingcoderepositories;(4) learning-basedAPR,whichusesma-
chine learning techniques to learn correct code locations/snippets
from a training code corpus, e.g., Prophet [31] and ELIXIR [45].
Recently, ProFL [ 33] initializes the idea of unified debugging
to investigate the effectiveness of APR for fault localization. The
experimentalresultsshowthatProFLisabletoboost/outperform
state-of-the-artSBFL[ 3,15,23],MBFL[22,38,40,63],andunsuper-
vised/supervised learning based fault localization [ 21,49,64] using
therecentPraPRAPRsystem[ 10],andalsoextendstheapplication
scopeofAPRtoallpossiblebugs.Ho wever,itisnotclearhowother
state-of-the-art APR techniques contribute to unified debuggingand how to further advance unified debugging, while this papermoves one step forward to that end, particularly with regards to
assessing the impact of other APR tools in unified debugging.
6 CONCLUSION
Inthispaper,wehaveperformedanextensivestudyoftheimpacts
of different automated program repair systems on the recently pro-
posedunifieddebuggingapproach[ 33,34].Ourstudyresultsonthe
popular Defects4J benchmark suite have revealed various practical
guidelinesforfurtheradvancingunifieddebugging,including:(1)
nearlyallthestudied16repairsystemscancontributetounifieddebugging despite their varying repairing capabilities, (2) repair
systems targetingmulti-edit patches can bringnoise and degrade
performanceforunifieddebugging,(3)repairsystemswithmore
executed/plausible patches tend to perform better for unified de-
bugging,(4)unifieddebuggingeffectivenessdoesnotexclusively
rely on the availability of correct patches from automated repair.Based on ourfi ndings, we further proposed an advanced unifieddebugging technique, UniDebug++, which can localize over 20%more bugs within Top-1 than state-of-the-art unified debugging
technique,ProFL[ 33,34].Inthenearfuture,wewillworkon tenta-
tive program repair, a new direction enabled by unified debugging
toallowfaultlocalizationandprogramrepairtoboosteachother
for more powerful debugging, e.g., patch execution results from aninitial set of repair systems can enable precise fault localization for
applying more advanced repair systems for cost-effective repair.
ACKNOWLEDGEMENTS
This work was partially supported by National Science Foundation
under Grant Nos. CCF-1763906 and CCF-1942430, and Alibaba.
REFERENCES
[1]2020. Tricentis reports. https://www.tricentis.com/resources/software-fail-
watch-5th-edition/
[2]2020. Unified Debugging Website. https://github.com/ProdigyXable/
UnifiedDebuggingStudy
[3]Rui Abreu, Peter Zoeteweij, and Arjan JC Van Gemund. 2006. An evaluation
of similarity coefficients for software fault localization. In 2006 12th Pacific Rim
International Symposium on Dependable Computing (PRDC‚Äô06). IEEE, 39‚Äì46.
[4]RuiAbreu,PeterZoeteweij,andArjanJCVanGemund.2007. Ontheaccuracyof
spectrum-based fault localization. In Testing: Academic and Industrial Conference
PracticeandResearchTechniques-MUTATION(TAICPART-MUTATION2007).IEEE,
89‚Äì98.
[5]Hiralal Agrawal, Joseph R Horgan, Saul London, and W Eric Wong. 1995. Fault
localization using execution slices and dataflow tests. In Proceedings of Sixth
InternationalSymposiumonSoftwareReliabilityEngineering.ISSRE‚Äô95.IEEE,143‚Äì
151.
[6]Tien-Duy B. Le, David Lo, Claire Le Goues, and Lars Grunske. 2016. A learning-
to-rank based fault localization approach using likely invariants. In Proceedings
of the 25th International Symposium on Software Testing and Analysis. 177‚Äì188.
[7]CO Boulder. 2013. University of Cambridge Study: Failure to Adopt
Reverse Debugging Costs Global Economy $41 Billion Annually.https://www.roguewave.com/company/news/2013/university-of-cambridge-
reverse-debugging-study. Accessed: Jan. 8, 2019.
[8]Lingchao Chen and Lingming Zhang. 2020. Fast and Precise On-the-fly Patch
Validation for All. arXiv preprint arXiv:2007.11449 (2020).
[9]Thomas Durieux and Martin Monperrus. 2016. DynaMoth: Dynamic Code Syn-
thesis for Automatic Program Repair. In Proceedings of the 11th International
WorkshoponAutomationofSoftwareTest (AST‚Äô16).AssociationforComputing
Machinery, 85‚Äì91.
[10]Ali Ghanbari, Samuel Benton, and Lingming Zhang. 2019. Practical program re-
pairviabytecodemutation.In Proceedingsofthe28thACMSIGSOFTInternational
Symposium on Software Testing and Analysis. 19‚Äì30.
[11]Claire Le Goues, Michael Pradel, and Abhik Roychoudhury. 2019. Automated
program repair. Commun. ACM 62, 12 (2019), 56‚Äì65.
[12] Yue Jia and Mark Harman. 2011. An analysis and survey of the development of
mutation testing. IEEE TSE 37, 5 (2011), 649‚Äì678.
[13]Jiajun Jiang, Luyao Ren, Yingfei Xiong, and Lingming Zhang. 2019. Inferringprogram transformations from singular examples via big code. In 2019 34th
IEEE/ACM International Conference on Automated Software Engineering (ASE).
IEEE, 255‚Äì266.
[14]Jiajun Jiang, Yingfei Xiong, Hongyu Zhang, Qing Gao, and Xiangqun Chen.
2018. Shapingprogramrepairspacewithexistingpatchesandsimilarcode.In
Proceedingsofthe27thACMSIGSOFTInternationalSymposiumonSoftwareTesting
and Analysis. 298‚Äì309.
[15]JamesAJonesand Mary JeanHarrold.2005. Empiricalevaluationofthetaran-
tula automatic fault-localization technique. In Proceedings of the 20th IEEE/ACM
international Conference on Automated software engineering. 273‚Äì282.
[16]Ren√©Just,DarioushJalali,andMichaelDErnst.2014. Defects4J:Adatabaseofex-istingfaultstoenablecontrolledtestingstudiesforJavaprograms.In Proceedings
of the 2014 International Symposium on Software Testing and Analysis. 437‚Äì440.
917[17]Sunghun Kim, Thomas Zimmermann, E James Whitehead Jr, and Andreas Zeller.
2007. Predictingfaults fromcachedhistory.In 29thInternationalConference on
Software Engineering (ICSE‚Äô07). IEEE, 489‚Äì498.
[18]Pavneet Singh Kochhar, Xin Xia, David Lo, and Shanping Li. 2016. Practitioners‚Äô
expectations onautomated fault localization.In Proceedings ofthe 25th Interna-
tional Symposium on Software Testing and Analysis. 165‚Äì176.
[19]Anil Koyuncu, Kui Liu, Tegawend√© F Bissyand√©, Dongsun Kim, Jacques Klein,
Martin Monperrus, and Yves Le Traon. 2020. Fixminer: Mining relevantfix
patternsforautomatedprogramrepair. EmpiricalSoftwareEngineering (2020),
1‚Äì45.
[20]ClaireLeGoues,ThanhVuNguyen,StephanieForrest,andWestleyWeimer.2011.
Genprog:Agenericmethodforautomaticsoftwarerepair. Ieeetransactionson
software engineering 38, 1 (2011), 54‚Äì72.
[21]Xia Li, Wei Li, Yuqun Zhang, and Lingming Zhang. 2019. Deepfl: Integrating
multiple fault diagnosis dimensions for deep fault localization. In Proceedings of
the28thACMSIGSOFTInternationalSymposiumonSoftwareTestingandAnalysis .
169‚Äì180.
[22]Xia Li and Lingming Zhang. 2017. Transforming Programs and Tests in Tandem
for Fault Localization. Proc. ACM Program. Lang. 1, OOPSLA, Article 92 (Oct.
2017), 30 pages. https://doi.org/10.1145/3133916
[23]Ben Liblit, MayurNaik, Alice X Zheng, Alex Aiken,and Michael I Jordan. 2005.
Scalable statistical bug isolation. ACM Sigplan Notices 40, 6 (2005), 15‚Äì26.
[24]K. Liu, A. Koyuncu, T. F. Bissyand√©, D. Kim, J. Klein, and Y. Le Traon. 2019. You
CannotFixWhatYouCannotFind!AnInvestigationofFaultLocalizationBiasinBenchmarkingAutomatedProgramRepairSystems.In 201912thIEEEConference
on Software Testing, Validation and Verification (ICST). 102‚Äì113.
[25]Kui Liu, Anil Koyuncu, Dongsun Kim, and Tegawend√© F. Bissyand√©. 2019.
AVATAR:FixingSemanticBugswithFixPatternsofStaticAnalysisViolations.
InSANER. 456‚Äì467.
[26]Kui Liu, Anil Koyuncu, Dongsun Kim, and Tegawend√© F. Bissyand√©. 2019. TBar:
Revisiting Template-Based Automated Program Repair. In Proceedings of the 28th
ACM SIGSOFT International Symposium on Software Testing and Analysis. 31‚Äì42.
[27]Kui Liu, Shangwen Wang, Anil Koyuncu, Kisub Kim, Tegawend√© Fran√ßois D As-
sise Bissyande, Dongsun Kim, Peng Wu, Jacques Klein, Xiaoguang Mao, and
Yves Le Traon. 2020. On the Efficiency of Test Suite based Program Repair: A
SystematicAssessmentof16AutomatedRepairSystemsforJavaPrograms.In
42nd ACM/IEEE International Conference on Software Engineering (ICSE).
[28]Fan Long, Peter Amidon, and Martin Rinard. 2017. Automatic inference of code
transforms for patch generation. In Proceedings of the 2017 11th Joint Meeting on
Foundations of Software Engineering. 727‚Äì739.
[29]FanLongandMartinRinard.2015. Stagedprogramrepairwithconditionsyn-
thesis.In Proceedingsofthe201510thJointMeetingonFoundationsofSoftware
Engineering. 166‚Äì178.
[30]Fan Long and Martin Rinard. 2016. An analysis of the search spaces for generate
and validate patch generation systems. In 2016 IEEE/ACM 38th International
Conference on Software Engineering (ICSE). IEEE, 702‚Äì713.
[31]Fan Long and Martin Rinard. 2016. Automatic patch generation by learning
correctcode.In Proceedingsofthe43rdAnnualACMSIGPLAN-SIGACTSymposium
on Principles of Programming Languages. 298‚Äì312.
[32]YilingLou,JunjieChen,LingmingZhang,DanHao,andLuZhang.2019. History-
driven build failurefi xing: how far are we?. In Proceedings of the 28th ACM
SIGSOFT International Symposium on Software Testing and Analysis. 43‚Äì54.
[33]Yiling Lou, Ali Ghanbari, Xia Li, Lingming Zhang, Dan Hao, and Lu Zhang.
2019. Can Automated Program Repair Refine Fault Localization? arXiv preprint
arXiv:1910.01270 (2019).
[34]Yiling Lou, Ali Ghanbari, Xia Li, Lingming Zhang, Haotian Zhang, Dan Hao,
andLu Zhang.2020. Canautomated programrepairrefine faultlocalization?a
unifieddebuggingapproach.In Proceedingsofthe29thACMSIGSOFTInternational
Symposium on Software Testing and Analysis. 75‚Äì87.
[35]MatiasMartinezandMartinMonperrus.2016. Astor:Aprogramrepairlibraryforjava.In Proceedingsofthe25thInternationalSymposiumonSoftwareTesting
and Analysis. 441‚Äì444.
[36]Matias Martinez and MartinMonperrus. 2018. Ultra-LargeRepair Search Space
withAutomaticallyMinedTemplates:TheCardumenModeofAstor.In SSBSE.
65‚Äì86.
[37]Martin Monperrus. 2018. Automatic software repair: a bibliography. ACM
Computing Surveys (CSUR) 51, 1 (2018), 1‚Äì24.
[38]Seokhyeon Moon, Yunho Kim, Moonzoo Kim, and Shin Yoo. 2014. Ask themutants:Mutatingfaultyprogramsforfaultlocalization.In 2014IEEESeventh
International Conference on Software Testing, Verification and Validation. IEEE,
153‚Äì162.
[39]LeeNaish,HuaJieLee,andKotagiriRamamohanarao.2011. Amodelforspectra-basedsoftwarediagnosis. ACMTransactionsonsoftwareengineeringandmethod-
ology (TOSEM) 20, 3 (2011), 1‚Äì32.
[40]MikePapadakisandYvesLeTraon.2015. Metallaxis-FL:mutation-basedfault
localization. Software Testing, Verification and Reliability 25, 5-7 (2015), 605‚Äì628.
[41]Chris Parnin and Alessandro Orso. 2011. Are automated debugging techniques
actuallyhelpingprogrammers?.In Proceedingsofthe2011internationalsymposiumon software testing and analysis. 199‚Äì209.
[42]KPearson.1895. NotesonRegressionandInheritanceintheCaseofTwoParents
Proceedings of the Royal Society of London, 58, 240-242.
[43]Zichao Qi, Fan Long, Sara Achour, and Martin Rinard. 2015. An Analysis of
PatchPlausibilityandCorrectnessforGenerate-and-ValidatePatchGeneration
Systems. In Proceedings of the 2015 International Symposium on Software Testing
and Analysis. 24‚Äì36.
[44]ManosRenieresandStevenPReiss.2003. Faultlocalizationwithnearestneighborqueries.In 18thIEEEInternationalConferenceonAutomatedSoftwareEngineering,
2003. Proceedings. IEEE, 30‚Äì39.
[45]RiponKSaha,YingjunLyu,HiroakiYoshida,andMukulRPrasad.2017. Elixir:
Effectiveobject-orientedprogramrepair.In 201732ndIEEE/ACMInternational
Conference on Automated Software Engineering (ASE). IEEE, 648‚Äì659.
[46]Seemanta Saha et al .2019. Harnessing evolution for multi-hunk program repair.
In2019IEEE/ACM41stInternationalConferenceonSoftwareEngineering(ICSE).
IEEE, 13‚Äì24.
[47]Raul Santelices, James A Jones, Yanbing Yu, and Mary Jean Harrold. 2009. Light-
weightfault-localizationusingmultiplecoveragetypes.In 2009IEEE31stInter-
national Conference on Software Engineering. IEEE, 56‚Äì66.
[48]Undo Software. 2016. Increasing software development productivity
with reversible debugging. https://undo.io/media/uploads/files/Undo_
ReversibleDebugging_Whitepaper.pdf. Accessed: Jan. 21, 2019.
[49]Jeongju Sohn and Shin Yoo. 2017. FLUCCS: using code and change metrics to
improve fault localization. In Proceedings of the 26th ACM SIGSOFT International
Symposium on Software Testing and Analysis. ACM, 273‚Äì283.
[50]Rijnard van Tonder and Claire Le Goues. 2018. Static automated program repair
forheapproperties.In Proceedingsofthe40thInternationalConferenceonSoftware
Engineering. 151‚Äì162.
[51]Shangwen Wang, Ming Wen, Bo Lin, Hongjun Wu, Yihao Qin, Deqing Zou,
XiaoguangMao,andHaiJin.2020. AutomatedPatchCorrectnessAssessment:
HowFarare We?.In the35thIEEE/ACM InternationalConferenceonAutomated
Software Engineering (ASE 2020).
[52]YiWei,YuPei,CarloAFuria,LucasSSilva,StefanBuchholz,BertrandMeyer,and
AndreasZeller.2010. Automatedfixingofprogramswithcontracts.In Proceedings
of the 19th international symposium on Software testing and analysis. 61‚Äì72.
[53]WestleyWeimer,ThanhVuNguyen,ClaireLeGoues,andStephanieForrest.2009.
Automaticallyfi nding patches using genetic programming. In 2009 IEEE 31st
International Conference on Software Engineering. IEEE, 364‚Äì374.
[54]MingWen,JunjieChen,YongqiangTian,RongxinWu,DanHao,ShiHan,and
Shing-Chi Cheung. 2020. Historical Spectrum based Fault Localization. IEEE
Transactions on Software Engineering (TSE) (2020).
[55]M.Wen,J.Chen,R.Wu,D.Hao,andS.Cheung.2018.Context-AwarePatchGener-ation for Better Automated Program Repair. In 2018 IEEE/ACM 40th International
Conference on Software Engineering (ICSE). 1‚Äì11.
[56]W Eric Wong, Ruizhi Gao, Yihao Li, Rui Abreu, and Franz Wotawa. 2016. A
survey on software fault localization. IEEE Transactions on Software Engineering
42, 8 (2016), 707‚Äì740.
[57]MingyuanWu,LingmingZhang,CongLiu,ShinHweiTan,andYuqunZhang.
2019. Automatingcudasynchronizationviaprogramtransformation.In 201934th
IEEE/ACM International Conference on Automated Software Engineering (ASE).
748‚Äì759.
[58]YingfeiXiong,JieWang,RunfaYan,JiachenZhang,ShiHan,GangHuang,andLu
Zhang. 2017. Precise condition synthesis for program repair. In 2017 IEEE/ACM
39th International Conference on Software Engineering (ICSE). IEEE, 416‚Äì426.
[59]JifengXuan,MatiasMartinez,FavioDemarco,MaximeClement,SebastianLame-
las Marcote, Thomas Durieux, Daniel Le Berre, and Martin Monperrus. 2016.
Nopol:Automaticrepairofconditionalstatementbugsinjavaprograms. IEEE
Transactions on Software Engineering 43, 1 (2016), 34‚Äì55.
[60]Jifeng Xuan and Martin Monperrus. 2014. Learning to combine multiple ranking
metricsforfaultlocalization.In 2014IEEEInternationalConferenceonSoftware
Maintenance and Evolution. IEEE, 191‚Äì200.
[61]YuanYuanandWolfgangBanzhaf.2018. ARJA:Automatedrepairofjavapro-
gramsviamulti-objectivegeneticprogramming. IEEETransactionsonSoftware
Engineering (2018).
[62]Lingming Zhang,Miryung Kim, andSarfraz Khurshid. 2011. Localizingfailure-
inducing program edits based on spectrum information. In 2011 27th IEEE Inter-
national Conference on Software Maintenance (ICSM). 23‚Äì32.
[63]LingmingZhang,LuZhang,andSarfrazKhurshid.2013. InjectingMechanical
Faults to Localize Developer Faults for Evolving Software. In OOPSLA. 765‚Äì784.
[64]Mengshi Zhang, Xia Li, Lingming Zhang, and Sarfraz Khurshid. 2017. Boosting
spectrum-basedfaultlocalizationusingPageRank.In Proceedingsofthe26thACM
SIGSOFT International Symposium on Software Testing and Analysis. 261‚Äì272.
[65]Jian Zhou, Hongyu Zhang, and David Lo. 2012. Where should the bugs befi xed?
more accurate information retrieval-based bug localization based on bug reports.
In201234thInternationalConferenceonSoftwareEngineering(ICSE).IEEE,14‚Äì24.
918