API-Knowledge Aware Search-Based Software Testing: Where,
What, and How
Xiaoxue Ren
Zhejiang University
China
xxren@zju.edu.cnXinyuan Ye
Australian National University
Australia
xinyuan.ye@anu.edu.auYun Lin
Shanghai Jiao Tong University
China
lin_yun@sjtu.edu.cn
Zhenchang Xing
CSIROâ€™s Data61 & Australian
National University
Australia
Zhenchang.Xing@anu.edu.auShuqing Li
Chinese University of Hong Kong
China
sqli21@cse.cuhk.edu.hkMichael R. Lyu
Chinese University of Hong Kong
China
lyu@cse.cuhk.edu.hk
ABSTRACT
Search-based software testing (SBST) has proved its effectiveness in
generating test cases to achieve its defined test goals, such as branch
and data-dependency coverage. However, to detect more program
faults in an effective way, pre-defined goals can hardly be adap-
tive in diversified projects. In this work, we propose Kat, a novel
knowledge-aware SBST approach to generate on-demand asser-
tions in the program under test (PUT) based on its used APIs. Kat
constructs an API knowledge graph from the API documentation to
derive the constraints that the client codes need to satisfy. Each con-
straint is instrumented into the PUT as a program branch, serving
as a test goal to guide SBST to detect faults. We evaluate Kat with
two baselines (i.e., EvoSuite and Catcher) with a close-world and
an open-world experiment to detect API bugs. The close-world ex-
periment shows that Kat outperforms the baselines in the F1-score
(0.55 vs. 0.24 and 0.30) to detect API-related bugs. The open-world
experiment shows that Kat can detect 59.64% and 9.05% more bugs
than the baselines in practice.
CCS CONCEPTS
â€¢Security and privacy â†’Software and application security .
KEYWORDS
Software Testing, Test Case Generation, Knowledge Graph
ACM Reference Format:
Xiaoxue Ren, Xinyuan Ye, Yun Lin, Zhenchang Xing, Shuqing Li, and Michael
R. Lyu. 2023. API-Knowledge Aware Search-Based Software Testing: Where,
What, and How. In Proceedings of the 31st ACM Joint European Software
Engineering Conference and Symposium on the Foundations of Software Engi-
neering (ESEC/FSE â€™23), December 3â€“9, 2023, San Francisco, CA, USA. ACM,
New York, NY, USA, 13 pages. https://doi.org/10.1145/3611643.3616269
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
ESEC/FSE â€™23, December 3â€“9, 2023, San Francisco, CA, USA
Â©2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-0327-0/23/12. . . $15.00
https://doi.org/10.1145/3611643.36162691 INTRODUCTION
Search-based software testing (SBST), as the most practical software
testing solution, has proved its success in effectively covering pre-
defined test goals (e.g., branches, data dependencies, lines, etc.)
[20,22,52]. By measuring the distance on how far a test case is
away from achieving a specific goal, SBST can evolve the test case
to minimize such a distance. State-of-the-art SBST tools such as
EvoSuite [19] and Randoop [44] are highly effective in generating
diversified test cases regarding branch coverage, line coverage, and
data-dependency coverage in practice [ 22,28,34,52,62]. They are
widely used in detecting software bugs in the industry [ 3,5,11,21,
46], and serve as popular baselines to evaluate many fault detection
techniques in academia [33, 47, 53].
However, to detect real-world software faults in practice, a test
generator needs to be enhanced with the specified oracle to be effec-
tive. To this end, many researchers propose approaches to enhance
test generation with oracles for fault detection [ 2,15,22,28]. Gen-
erally, the oracles are derived to validate either the final test results
[8,66] or the internal execution states [ 23,37]. The approaches
targeting the final test results usually derive project-specific oracle
from the comments of the target method [ 9,26] and the machine
learning (or deep learning) based models [ 1,57]. In contrast, the
approaches targeting the internal execution states focus on general
oracles such as the manifestation of runtime exceptions [ 37] and
the violation of pre-defined heuristics (e.g., memory access and null
value access) [23].
Generally, existing solutions that infer the internal execution
states often rely on heuristics to derive a general oracle. While these
solutions are practical for locating pre-defined types of faults (e.g.,
null value access and invalid array access), their rules are not easily
expandable. Additionally, defining the granularity of these rules
involves a tradeoff. The more general the rules (e.g., the program
terminates with runtime exception have faults), the more likely
that the rule is imprecise for a specific project (e.g., some programs
may expect a runtime exception on some invalid inputs). The less
general the rules, the more likely we miss detecting some program
faults. Inspired by previous work [ 51], developers can effectively
resolve the Oracle problem, by adhering to API usage specifications.
The API usage specifications provide a standardized framework that
enables accurate and reliable inference of internal execution states.
This framework presents a notable advancement, overcoming theESEC/FSE â€™23, December 3â€“9, 2023, San Francisco, CA, USA Xiaoxue Ren, Xinyuan Ye, Yun Lin, Zhenchang Xing, Shuqing Li, and Michael R. Lyu
limitations associated with heuristic-based approaches and offering
a more adaptable and scalable solution.
In this work, we propose Knowledge- Aware search-based soft-
ware Testing ( Kat), a knowledge extraction-based approach to
generate on-demand assertions in the PUT (program under test),
validating the internal execution states when interacting with API
frameworks. Our rationale is based on the understanding that
well-documented descriptions, particularly the exception handling
knowledge, of APIs serve as informative oracles/specifications
when APIs are utilized in the PUT. Moreover, the (in)consistencies
between API specification and their usage in the code can be trans-
formed into test goals to further facilitate SBST to discover the
faults. Specifically, given a target program, by parsing the descrip-
tion of API and library documentation, Kat aims to identify (1)
where in the code to test based on the (in)consistencies between
API specification in the documentation and the API usage in the
code, (2) what API specification in the code to test, and (3) how to
test the specification (i.e., manipulate the source code to facilitate
state-of-the-art test generators).
Technically, Kat is designed to (1) construct an API exception
handling knowledge graph, which is done by extracting knowl-
edge triples from API exception triggers in the documentation
(e.g., [ String.charAt(index) , throw, IndexOutOfBoundsException ],
[index ,<,0] and [ index ,>,String.length() ] will be extracted from
â€œString.charAt(index) throws IndexOutOfBoundsException - if the in-
dex argument is negative or not less than the length of this string â€,
see Part I in Fig. 1), (2) transform the knowledge graph into pro-
gram constraints instrumented as additional program branches (e.g.,
if(startOffset < 0){throw new IndexOutOfBoundsException();} ,
andif(startOffset >= signature.length()){throw new IndexOutOf-
BoundsException();} see Part III in Fig. 1 for more details), and (3)
convert those instrumented branches as test goals for SBST tools
(e.g., EvoSuite) to generate test cases to cover. The instrumented
program constraints and their covering test cases serve as both the
fault detection results and explanations. To further improve the per-
formance, Kat also schedules the testing budget for different test
goals regarding our measured ROI (return on investment) metrics
for each goal.
We conduct extensive experiments to evaluate both the qual-
ity of API exception handling knowledge graph construction and
the performance of Kat to detect software faults in experimental
settings and in practice. To evaluate the constructed knowledge
graph, we manually check the ğ‘€ğ¼ğ‘ (computed by a statistical
sampling method [ 58]) samples from the total 39,132 API usage
constraints, which are extracted from the exceptions (also called
exception triggers) of 1,821 Java SE&JDK API classes. The results
demonstrate that Kat achieves an impressive average accuracy of
96.46% in extracting exception triggers and successfully transform-
ing them into knowledge triples. To evaluate the performance of
Kat to detect software faults, we compare Kat with two baselines
(i.e.,Catcher [33] and EvoSuite [19]) in a close-world experiment
and an open-world experiment. In the close-world experiment, we
would like to assess the effectiveness of Katin detecting API-related
bugs, as it is designed to be API-knowledge-aware. We gathered a
close-world dataset comprising 12 real-world bugs that violated API
specifications. These bugs were reported, fixed, and accompanied by
corresponding patches in the GitHub repository. This dataset allowsus to compare Kat with other baselines in detecting API-related
bugs using evaluation metrics (i.e., precision, recall and F1-score).
By executing these baselines on the close-world dataset, we can
assess their performance. The results indicate that Kat achieves an
impressive overall F1-score of 0.55, surpassing the state-of-the-art
baseline (Catcher) by an outstanding margin of 83.33% in terms of
F1-score. This comparison demonstrates the superior performance
ofKat in accurately detecting API-related bugs when compared
to existing approaches. In the open-world experiment, we run Kat
and baselines on 21 Apache projects, which have been used in other
related work [ 33]. As the open-world dataset does not have the
ground truth reported by developers, we compare Kat to baselines
by counting the number of bugs found. The results show Kat dis-
covers 265 real-world bugs in total within two days, outperforming
the baselines by finding 9.05% and 59.64% more bugs, respectively.
We summarize our contributions as follows:
â€¢We propose Kat, an SBST approach to detect bugs by translating
the API specifications into on-demand assertions in PUT. These
generated assertions serve as oracles for evaluating the internal
execution states.
â€¢We conduct comprehensive experiments to evaluate the effec-
tiveness of Kat. The results demonstrate that Kat outperforms
the state-of-the-art approaches in detecting bugs. Specifically,
it achieves an impressive 83.33% improvement in F1-score and
identifies 9.05% more bugs compared to Catcher.
â€¢We report 22 new bugs in the Apache open-source projects. In
addition, the dataset and tutorial to run Kat are available at
https://github.com/XiaoxueRenS/KAT.
2 MOTIVATION
Fig. 1 shows a buggy code example from the EasyMock project
where line 63 (Part II) can throw an IndexOutOfBoundException
when the string length of signature is smaller than the value of
startOffset . To detect such a bug, existing SBST approaches (e.g.,
EvoSuite [ 19] and Catcher [ 33]) define test goals, guiding the search-
based approach to synthesize a test case to trigger such an exception.
However, we run each approach on SignatureReader class of the
EasyMock project for 30 minutes, failing to report the bug. Gener-
ally, both approaches suffer from the limits of brute-force search or
lack of knowledge.
Challenge of EvoSuite. It is a big challenge for EvoSuite to dis-
cover the fault because it has abundant test goals (e.g., line coverage,
branch coverage, etc.) to diverge its computation resource from the
goals with actual bugs. In this example, SignatureReader class has
102 branches, 164 lines, and four methods. Treating all test goals
with equality is computationally exhaustive. More specifically, Evo-
Suite lacks the knowledge of where to test .
Challenge of Catcher. Catcher is an SBST approach, built upon
EvoSuite, to detect exception-triggered faults by localizing the call
site with the potential to trigger unexpected exceptions. By in-
vestigating how the approach can work on the example, we find
Catcher can effectively identify that line 63 is a potential call site
with an unexpected exception. However, Catcher is challenging
to synthesize a test case to confirm the exception. Since there is
no guidance (e.g., branch distance) to synthesize the two specific
Java objects into SignatureReader and SignatureWriter with fieldsAPI-Knowledge Aware Search-Based Software Testing: Where, What, and How ESEC/FSE â€™23, December 3â€“9, 2023, San Francisco, CA, USA
public void acceptType(SignatureVisitor signatureVisitor) {
    parseType(this.signatureValue, 0, signatureVisitor);
}
private static int parseType(String signature, int startOffset, SignatureVisitor 
signatureVisitor) {
    int offset = startOffset + 1;
    char currentChar = signature.charAt(startOffset);//target line
    switch(currentChar) {
    case 'B':
    ...
    }
}56
57
58
59
60
61
62
63
64
65
66
67
68Part II: Program Under Test
Part III: Program After Instrumentation
public void acceptType(SignatureVisitor signatureVisitor) {
    parseType(this.signatureValue, 0, signatureVisitor);
}
private static int parseType(String signature, int startOffset, SignatureVisitor 
        signatureVisitor) {
    if (startOffset < 0) {
        throw new IndexOutOfBoundsException("Error message: if the index argument is" 
+ "negative.");
    } else if (startOffset >= signature.length()) {
        throw new IndexOutOfBoundsException("Error message:" 
+ "if the index argument is not less than the length of this string.");
    } else {
        int offset = startOffset + 1;
        char currentChar = signature.charAt(startOffset);
        switch(currentChar) {
        case 'B':
...
    }
}56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
Part IV: Test Case By KAT
@Test(timeout = 4000)
public void test9()  throws Throwable  {
    SignatureReader signatureReader0 = new SignatureReader("");
    SignatureWriter signatureWriter0 = new SignatureWriter();
    // Undeclared exception!
    try { 
      signatureReader0.acceptType(signatureWriter0);
    } catch(IndexOutOfBoundsException e) {
       //
       // Error message: if the index argument is not less than the length of this string.
       //
       verifyException("org.easymock.asm.signature.SignatureReader", e);
    }
}142
143
144
145
146
147
148
149
150
151
152
153
154
155Exception: String.charAt(index) throws IndexOutOfBoundsException
Trigger: if  â‘  the index argument is negative or â‘¡ not less than the length of this string.Part I: Exception of String.charAt(index)
[ index, is, negative ] [ index, is not less than, the length of this string ]
[ index, <, 0 ] [ index, >=, String.length() ][ String.charAt (index), throw,  
  IndexOutOfBoundsException ]
extracting extracting
linking linkingextracting
Figure 1: Example of a Bug in SignatureReader class of Easy-
Mock Project
of specific value for test inputs (see Part IV). As a result, Catcher has
to randomly generate test cases regarding the method calls of the
SignatureReader class, with a slim chance of satisfying the specific
constraints when synthesizing the Java objects. More specifically,
Catcher lacks the knowledge of how to test.
To address the aforementioned challenges, we propose Kat,
which is a knowledge-aware approach to (1) extract constraints
from the API documentation and construct an exception handling
knowledge graph to validate call sites of APIs in the PUT, (2) trans-
late the constraints as program conditions instrumented in the PUT,
and (3) guide SBST to synthesize the test cases covering test goals
of the instrumented program conditions.
Specifically, Kat statically detects the (in)consistencies between
API exception handling knowledge and the PUT. In Fig. 1, the API in
the target line (i.e., line 63 of Part II) is String.charAt(index) , which
has two constraints that may trigger IndexOutOfBoundsException ,
i.e., [ index ,<,0] and [ index ,>=,String.length() ]. With static
analysis, Kat transforms the above two constraints into triples
[startOffset ,<,0] and [ startOffset ,>=,signature.length() ] (seePart I in Fig. 1). Considering what and how to test, Kat trans-
forms the inconsistencies into programs, and instruments addi-
tional branches into the PUT (i.e., lines 62-67 in Part III). With
the instrumented branches, Kat searches with branch coverage to
find bugs. Also, Kat computes the confidence score of the code
instrumented by considering both knowledge confidence and in-
strumentation confidence (see section 3.2.2 and 3.3.2). With the
confidence score, Kat can tell SBST how to allocate time budget
to detect bugs. In this case, Kat is with a pretty high score and is
confident in finding bugs. Part IV is the test cases generated by Kat,
which can catch IndexOutOfBoundsException in the PUT by setting
signatureReader0 as a null object, which existing SBST methods
cannot simulate.
3 APPROACH
Fig. 2 presents the overview of Kat, which consists of three main
phases, i.e., construction of API exception handling knowledge graph
(Phase-I), static detection of potential bugs (Phase-II), and specifica-
tion violation as test goals (Phase-III). In Phase-I, we first extract
API exception handling knowledge from Java SE&JDK API docu-
mentation [ 30], and then construct them into a knowledge graph.
In Phase-II, Kat performs static analysis to detect potential bugs by
checking whether the PUT violates the constraints in API handling
knowledge (for where ). In Phase-III, Kat transforms the violations
into test goals to generate and validate test cases via code instrumen-
tation (for what ). Besides, Kat also schedules the testing budget for
different test goals (for how). More details are described as follows.
3.1 Knowledge Graph Schema
Kat aims to generate on-demand assertions in the PUT according
to the knowledge defined in the official API documentation. Thus,
we construct an API exception handling knowledge graph based
on the official Java SE&JDK API documentation [ 30], which with
relatively complete contents and high quality [29, 50, 68].
The entities of the API exception handling knowledge graph in-
clude API components (module ,package ,class,method ,parameter ,
return-value , and exception ) declared in Java official documenta-
tion and value literals (-1,null,true and range like negative or
[0-9] ) extracted from textual usage directives in documentation.
The relations between entities include declaration relation and
constraint relation . The declaration relations are between API
components, including ( hasClass ,hasMethod ,hasParameter ,return
andthrow ). The constraint relations include constraint-enriched re-
turn andthrow , which are attached with conditions extracted from
return contents and exception triggers in the documentation. For
instance, Part I in Fig. 1 shows an exception handling sentences of
String.indexOf(index) , which has the following two exception trig-
gers (i.e., â€œ the index argument is negative â€, and â€œ the index argument
not less than the length of this string â€).
3.2 Construction of API Exception Handling
Knowledge Graph
The official Java SE&JDK API documentation contains semi struc-
tured API specifications, including API hierarchies (i.e., API decla-
ration graph in this paper) and textual usage directives. To detect
bugs violating API specifications, we focus on exception triggersESEC/FSE â€™23, December 3â€“9, 2023, San Francisco, CA, USA Xiaoxue Ren, Xinyuan Ye, Yun Lin, Zhenchang Xing, Shuqing Li, and Michael R. Lyu
API Documentatiaon
API Usage DirectivesAPI Declaration GraphAPI Exception Handling 
Knowledge GrpahProgram Under Test
Program After Instrumentation Time Budget Allocating
Real BugCFG+DFGStatic Analyzing
API Constraint SolverFormulating
Searching & Optimizing Phase-II: Static Detection of Potential Bugs
Phase-I: Construction of API Exception Handling Knowledge Graph Phase-III: Specification Violation as Test Goals
Potential Bug
Code Formula KG FormulaFormulating
 Return TripleExtracting 
Extracting 
 Entity-linked Triple
Entity Linking (in)Consistencies Checking Code Instrumenting 
 Exception TripleFunctionality/Parameter 
description
Exception triggerReturn content
API KnowledgeAligning
Aligning
Knowledge TriplePreparing
Figure 2: The Overall Framework of Kat
and build an API exception handling knowledge graph to represent
the semantics of API usage in the PUT. The API exception han-
dling knowledge graph consists of two parts: the API declaration
graph , which can be accessed directly from the documentation, and
knowledge triples extracted from textual usage directives. By align-
ing triples into the API declaration graph, we can finally obtain
the API exception handling knowledge graph. The API declara-
tion graph needed by us is the same as the generic API knowledge
graph in [ 36,39], and we follow their treatments to obtain the API
declaration graph. Therefore, the central part of knowledge graph
construction lies in knowledge triple extraction and alignment.
Details are as follows:
3.2.1 API Knowledge Preparation. We first do some preliminary
work for constructing the knowledge graph, including knowledge
collection and pre-processing.
Knowledge collection. We collect three types of textual API usage
directives: descriptions (both functionality and parameter descrip-
tion), return contents and exceptions . Descriptions are the basic
knowledge of APIs, which are usually composed of several descrip-
tive sentences. We focus on the first sentence, commonly recognized
as the most helpful knowledge [ 36]. It is usually a functionality
summary or overall description. Return contents and exceptions are
critical knowledge to triple extraction, which may contain essential
conditions/triggers causing potential bugs.
Knowledge pre-processing. We pre-process the collected knowl-
edge as follows. (1) First, we resolve coreferences, including both
general coreferences (e.g., itandthem ) and domain-specific corefer-
ences (e.g., the arguments ). We use neuralcoref [ 43] for general
coreferences, which is a state-of-the-art coreference resolution
method. We use self-defined patterns to resolve domain-specific
coreferences. Our observation is that conditions/triggers in re-
turn contents/exceptions often use â€œ any/either/all (of the) param-
eter(s)/argument(s) â€ to represent their parameters. For example,
the trigger of NullPointerException inEnumSet.of(e1, e2) , â€œif any
parameters are null â€, means either e1ore2are null. Thus, this trig-
ger should be resolved into â€œ ife1ore2are null â€. (2) Then, we
split clauses by parsing the syntax dependency tree of trigger sen-
tences with Spacy [ 61]. We detect conjunctions of the sentencesand complete the omitted parts in the subordinate clause based on
the elements in the main clause. In the example above, the trigger
after resolution can be further split into two triggers (i.e., â€œ ife1is
nullâ€ and â€œ ife2is null â€), and the relation between triggers is ğ‘‚ğ‘….
3.2.2 Knowledge triple Extraction & Alignment. To construct the
API exception handling knowledge graph, we first represent the
knowledge in the format of triples via entity-relation extraction
approaches, and then align the triples into the declaration graph
by linking entities.
Triple Extraction. Return contents and exceptions we collected
from API documentation often come with constraints, which are
called return conditions and exception triggers, respectively. Return
contents are helpful when analyzing the data flow, and exceptions
are closely related to bugs violating API specifications. Hence, we
need to extract knowledge triples from return contents and excep-
tions, as well as their constraints.
To extract knowledge triples from textual sentences, we employ
Spacy [ 61] to parse sentences into semantic dependency trees, from
which we identify entities and relations. Since official documen-
tations usually have a relatively fixed expression, we summarize
three common sentence structures of exception triggers and return
conditions, including subject-verb (S-V) (e.g.,â€œ if the character does
not occur â€), subject-verb-predicative (S-V-P) (e.g., â€œ the beginIndex
is negative â€), and subjectâ€“verbâ€“object (S-V-O) (e.g., â€œ beginIndex is
larger than endIndex â€). When extracting knowledge triples, we first
detect the main verbs and categorize the sentences into the corre-
sponding structures. Then, by detecting nouns linked by the verbs,
which are the entities, we can obtain the entity-relation triples from
sentences. Note that compound nouns and noun phrases often ap-
pear in the documentation. We need to merge them into one noun
with Spacy [ 61] before extracting triples. For example, â€œ the length
of the string â€ should be merged into one entity.
Inspired by [ 51,70], to make the knowledge graph better serve
bug detection with specific bug types, we further classify exception
triggers into four types, i.e., T1: nullness prohibition ,T2: range limita-
tion,T3: call dependency , and T4: other restriction . They correspond
to four types of bugs violating API specifications, i.e., B1: missing
null check ,B2: missing range check ,B3: missing call , and B4: missingAPI-Knowledge Aware Search-Based Software Testing: Where, What, and How ESEC/FSE â€™23, December 3â€“9, 2023, San Francisco, CA, USA
try-catch . After observation, T2andT3triggers are consistent in
expression, which can be identified by detecting key patterns. We
compare the two pattern sets defined in [ 42,69] and obtain 15 key
patterns (available in https://kat4cs.com/) related to our categories
for trigger classification (e.g., â€œ nullâ€, â€œnegative â€, â€œ>â€ and â€œ less than â€).
T3andT4cannot be identified through pattern matching due to the
absence of templated expressions. However, they can be resolved
by linking entities with semantic similarities, referring to the triple
alignment part for details. More specifically, the knowledge triples
of triggers in Fig. 1 both belong to ğ‘‡2(i.e., [ index, is, negative ]
and [ index, is not less than, the length of this string ]), which
may cause missing range check bugs.
Triple Alignment. As API usage directives are organized in un-
structured natural language, which has different formats from struc-
tured knowledge in the API declaration graph, we perform entity
linking over API usage directives to align the extracted triples to
the entities in the API declaration graph. For preparation, we train
a domain-specific word embedding model to evaluate the similar-
ity between entities/sentences. Specifically, we use gensim [ 25] to
fine-tune Googleâ€™s pre-trained Word2vec model [ 65] with all texts
from official Java documentation.
For alignment, we compute a confidence score for a triple with
respect to the entity to judge whether to link. The confidence score
is calculated with the following steps: (1) Direct URL linking: check
whether there is a direct URL link with the entity. If any, match the
entity to the object pointed by the link and denote the knowledge
confidence as 1.0; (2) Intra-method linking: compute the similarity
between the entity in the triple and the parameters in the same
method, considering parameter names first and then descriptions.
Once the similarity score is larger than 0.8, stop matching, and the
similarity score is denoted as the knowledge confidence. (3) Intra-
class linking: compute the similarity between the entity in the triple
and methods in the same class similar to strategy (2). Stop matching
when similarity is more than 0.8and denote its knowledge confi-
dence score. (4) Inter-class linking: compute the similarity between
the entity in triple with related methods in other classes. The re-
lated methods include inherited methods declared in other classes
or the parameterâ€™s functionality methods. Selecting the matched
entity with the highest score ( >0.5), which is also denoted as its
knowledge confidence score. The threshold here is different from
0.8, because the expressions in the same class are more similar,
although they may not be related, while the expressions in different
classes are more different, although they may be related. To avoid
mismatching, we choose different thresholds. We also identify T3
andT4triggers in this step. After detecting T1andT2triggers by
matching key patterns, we continue to identify types of the rest
triggers by linking entities after extracting triples. If the entity in
a triple can be linked with another method, and the trigger is to
check the state of the linked method, this triple can be identified
asT3, otherwise T4. For example the triple [ Iterator.hasnext(),
==, true ] extracted from â€œ Iterator.next() throws NoSuchElementEx-
ception if the iteration has no more elements â€ is identified as T3,
because â€œ has no more elements â€ is linked with the false state of
Iterator.hasnext() .
we finally align all knowledge triples into the API declaration
graph by the linked entities, which forms our API exception han-
dling knowledge graph. To better display the knowledge graph, weuse heuristic rules to symbolize relations. Take the trigger â€œ the index
argument is negative â€ of String.charAt(index) as an example, the
entities â€œ the index argument â€ are ideally linked with the parameter
of a 1.0confidence score. Thus, the knowledge triple is [ index , is,
negative ], which can be further represented as [ index ,<,0].
3.3 Static Detection of Potential Bugs
After constructing the API exception handling knowledge graph,
we use static analysis to detect the (in)consistencies between the
knowledge graph and PUT to find potential bugs (referring to where
to test ). Specifically, it includes the following two steps: programs
parsing the PUT to obtain flow information, and (in)consistencies
checking to detect potential bugs.
3.3.1 Programs parsing. To obtain flow information, we adopt
Soot [ 60], a popular framework for analyzing and transforming
Java programs, to parse the PUT. First, we use soot to transform
the PUT into an intermediate representation as Jimple, which can
essentially simplify flow analysis [ 63]. Then, we construct control
flow and data flow graphs from Jimple files. With the flow graphs,
we can traverse and record various flow information about the PUT,
e.g., value assignment, variable type, handled exceptions, method
invocations, and conditional branch, which can help transform the
knowledge graph into program constraints.
3.3.2 (In)Consistencies Checking. With the flow graphs, we first
use the knowledge graph to detect target APIs in the PUT. The target
APIs refer to the APIs with exception triples in the knowledge graph.
Then, we collect corresponding arguments and constraints of target
APIs in the PUT according to entities and relations in exception
triples, which form the code formula (i.e., ğ‘“ğ¶ğ‘œğ‘‘ğ‘’ ). After that, we also
transform constraints of target APIs in the knowledge graph into
program constraints by replacing entities with arguments in the
PUT, which forms the KG formula (i.e., ğ‘“ğ¾ğº). Take the PUT in Fig. 1
as an example, it shows a process of acceptType calls parseType . The
target API is String.charAt(index) in line 63, and we can traverse
the flow graphs and obtain its code formula and KG formula (i.e.,
ğ‘“ğ¾ğº) as follows:
ğ‘“ğ¶ğ‘œğ‘‘ğ‘’=ï£±ï£´ï£´ ï£²
ï£´ï£´ï£³ğ‘–ğ‘›ğ‘‘ğ‘’ğ‘¥ =ğ‘ ğ‘’ğ‘¡ğ‘‚ğ‘“ğ‘“ğ‘ ğ‘’ğ‘¡ =0
ğ‘†ğ‘¡ğ‘Ÿğ‘–ğ‘›ğ‘”.ğ‘™ğ‘’ğ‘›ğ‘”ğ‘¡â„()=ğ‘ ğ‘–ğ‘”ğ‘›ğ‘ğ‘¡ğ‘¢ğ‘Ÿğ‘’.ğ‘™ğ‘’ğ‘›ğ‘”ğ‘¡â„()
=ğ‘¡â„ğ‘–ğ‘ .ğ‘ ğ‘–ğ‘”ğ‘›ğ‘ğ‘¡ğ‘¢ğ‘Ÿğ‘’.ğ‘™ğ‘’ğ‘›ğ‘”ğ‘¡â„ ()
ğ‘“ğ¾ğº=ğ‘‚ğ‘…(ğ‘–ğ‘›ğ‘‘ğ‘’ğ‘¥ <0,ğ‘–ğ‘›ğ‘‘ğ‘’ğ‘¥ >=ğ‘†ğ‘¡ğ‘Ÿğ‘–ğ‘›ğ‘”.ğ‘™ğ‘’ğ‘›ğ‘”ğ‘¡â„())
Then, to check the (in)consistencies of the two formulas, we
adopt an SMT solver (i.e., Z3 [ 49]) to check whether there is a
solver that can trigger the exception, namely, whether there is
a solver inğ‘“ğ¶ğ‘œğ‘‘ğ‘’ can meet one of the constraints in ğ‘“ğ¾ğº. There-
fore, we feed the arguments in ğ‘“ğ¶ğ‘œğ‘‘ğ‘’ intoğ‘“ğ¾ğº, obtaining a series
of formulas under (in)consistencies checking. For example, when
feedingğ‘“ğ¶ğ‘œğ‘‘ğ‘’ of the PUT in Fig. 1 into its corresponding ğ‘“ğ¾ğº, we
can get the (in)consistencies checking formula: ğ‘‚ğ‘…(0<0,0>=
this.signature.length() ). By evaluating the (in)consistencies check-
ing formula with Z3, we can find whether there is a potential bug
or not. The output of Z3 should be ğ‘ ğ‘ğ‘¡orğ‘¢ğ‘›ğ‘ ğ‘ğ‘¡ .ğ‘¢ğ‘›ğ‘ ğ‘ğ‘¡ means the
API usage in the PUT meets the corresponding constraints in API
specifications and does not trigger any exceptions, while ğ‘ ğ‘ğ‘¡means
potential bugs may happen and will be marked as where to test.
Specifically, in the PUT of Fig. 1, if this.signature.length() <=0,ESEC/FSE â€™23, December 3â€“9, 2023, San Francisco, CA, USA Xiaoxue Ren, Xinyuan Ye, Yun Lin, Zhenchang Xing, Shuqing Li, and Michael R. Lyu
IndexOutOfBoundsException might be thrown in line 63, see the test
case by Kat (Part IV). Hence, the solver of the (in)consistencies
checking formula should be ğ‘ ğ‘ğ‘¡, marking a potential bug in line 63.
Note that, we also compute a confidence score for detecting
potential bugs (called bug confidence), which has three levels: 0.0
means no potential bugs because no target APIs are found; 1.0
means perfect confidence, it is when the determined/inferred ar-
gument value can meet the constraints; 0.5is borderline confi-
dence, it is when the argument is an uncertain input, and its value
cannot be inferred from the knowledge graph. Specifically, the
(in)consistencies checking formula of PUT in Fig. 1 is with bor-
derline confidence ( 0.5) because the occurrence of the exception
largely depends on the input of signature , which may be possible
to trigger an exception.
3.4 Specification Violation as Test Goals
After finding potential violations by checking (in)consistencies be-
tween the PUT and knowledge triples, we transform the violations
into code snippets for instrumentation, which serve as additional
branches to cover.
3.4.1 Code Instrumentation. Considering the different types of
triggers we extracted from the documentation, we first summarize
the general expression of human patches and design different in-
strumentation patterns for each, which reflect what to test. Then,
we use Javassist [ 10], a Java bytecode engineering toolkit, to instru-
ment code snippets into the class files of projects. The instrumented
code snippets serve as additional branches for SBST to generate
test cases covering test goals.
Specifically, the instrumentation patterns are as follows: (1) For
nullness prohibition and range limitation, we translate the violation
into if ([condition]) {throw new [Exception];} , e.g., the violation
of triple [ index ,<,0] in Fig. 1 is converted into if(index < 0){throw
new IndexOutOfBoundsException;} . (2) For call dependency, differ-
ent from specific range and null, we add a state checking to check
the state of the dependent API call, i.e., if([API] == [State]){throw
new [Exception];} . For example, if the violate knowledge triple
[Iterator.hasNext(), false, Iterator.next()] can be converted
into if(hasNext() == false){throw new NoSuchElementException;} .
(3) For other restrictions, we wrap the target line with try{[Target]}
catch([Exception] e) . Note that the ğ‘‚ğ‘…/ğ´ğ‘ğ· relationship exists
among different violations, so the positional relationship between
instrumented code snippets should be considered. For example, the
instrumented code snippets in Fig. 1 (see lines 62-68 in Part III)
check two potential violations with the ğ‘‚ğ‘…relationship.
3.4.2 Time Budget Allocation. According to the previous study [ 47],
time budget is important. We design a strategy to allocate different
time budgets concerning the confidence of the additional branch
instrumented, which is how to test. This confidence is an average
of knowledge confidence in section 3.2.2 and bug confidence in
section 3.3.2. For each class under test, we allocate a total time, and
each method in the class is allocated with a minimum time. Then,
we continue to allocate the remaining time for methods, according
to their confidence in the additional branch instrumented. As a re-
sult, the time budget of method ğ‘šisğ‘¡ğ‘š=ğ‘ğ‘šÃğ‘›
ğ‘–=0ğ‘ğ‘–(ğ‘‡âˆ’ğ‘¡ğ‘šğ‘–ğ‘›Ã—ğ‘›)+ğ‘¡ğ‘šğ‘–ğ‘›,
whereğ‘‡is the total time budget allocated to each class under testand the default value is 200 seconds, ğ‘¡ğ‘šğ‘–ğ‘›is the minimum time
allocated to each method in the class (2 seconds by default), ğ‘›is
the number of methods in a class, and ğ‘ğ‘šrefers to the confidence
score of method ğ‘š.
3.5 Implementation
We implemented Katon top of EvoSuite [ 19], a state-of-the-art Java
testing framework. Many search algorithms have been proposed
for EvoSuite, and we choose DynaMOSA as the search algorithm
ofKat, which optimizes multiple coverage targets, simultaneously.
DynaMOSA stands as the current state of the art and has been
integrated into the EvoSuite tool [33, 38, 47].
Search algorithms have various parameters to set, but previous
work [ 6] shows that parameter tuning SBST is extremely expensive
and not necessary compared to default parameter values. Thus, we
use the default settings of DynaMOSA. Moreover, considering Kat
works by instrumenting additional branches, we choose branch
coverage as the optimization objective.
4 EXPERIMENT SETUP
4.1 Baseline Methods
To investigate the overall performance of Kat, we select two state-
of-the-art SBST approaches as baselines:
â€¢Catcher [ 33]: Similar to Kat, Catcher also combines static ex-
ception propagation analysis with automatic search-based test
case generation to detect crash-prone Java API misuses. The pri-
mary focus of Catcher is to narrow down the search space for
automatic test case generation by honing in on API-call locations
that are prone to triggering exceptions at runtime. To enhance its
effectiveness, Catcher optimizes the search objective function by
considering various coverage metrics, including branch coverage,
line coverage, input coverage, and output coverage.
â€¢EvoSuite [ 46]: It is a cutting-edge Search-Based Software Testing
(SBST) approach designed specifically for generating test cases
for Java classes. With its advanced techniques, EvoSuite has been
shown to achieve remarkable code coverage and improved bug
detection capability.
To illustrate the effectiveness of Katâ€™s different design choices,
we design two variants as follows:
â€¢Katğ‘ ğ‘¡ğ‘ğ‘¡ğ‘–ğ‘ : This variant of Kat keeps only the static analysis
technique to detect bugs, i.e., Phase-I and Phase-II in Fig. 2. By
comparing Kat toKatğ‘ ğ‘¡ğ‘ğ‘¡ğ‘–ğ‘ , we evaluate the effectiveness of
dynamic checking by generating a series of test cases (i.e., Phase-
III in Fig. 2) in finding real bugs.
â€¢Katğ‘‘ğ‘¦ğ‘›ğ‘ğ‘šğ‘–ğ‘ : This variant of Kat keeps only the test case gener-
ation part of Kat (i.e., removing static detection with knowledge
graph and specification violation as test goals). Thus, this variant
is the same as EvoSuite. By comparing Kat toKatğ‘‘ğ‘¦ğ‘›ğ‘ğ‘šğ‘–ğ‘ , we
evaluate the effectiveness of detecting potential bugs and con-
verting them into additional branches (i.e., Phase-I and Phase-II
in Fig. 2) in finding API-related bugs.
4.2 Experimental Datasets
To evaluate the performance of Kat, Both close-world and open-
world datasets are used to compare different evaluation metrics.API-Knowledge Aware Search-Based Software Testing: Where, What, and How ESEC/FSE â€™23, December 3â€“9, 2023, San Francisco, CA, USA
4.2.1 Close-world Dataset. The close-world dataset refers to some
API-related bugs with fixed patches, which means they have the
ground truth and can be used to compute precision, recall and
F1-score for evaluation. Given that Kat is designed as an API-
knowledge-aware approach, we leverage the close-world dataset
to conduct experiments specifically focused on evaluating the per-
formance of API-related bug detection. However, upon manual
inspection of Defects4j [ 32,59], a popular benchmark for bug detec-
tion, only three bugs are related to Java SE&JDK API. Consequently,
we followed the bug collection procedure outlined in Defects4j [ 32],
searching for fixed bugs from historical commits on GitHub. This
process resulted in the discovery of an additional nine fixed API-
related bugs. Overall, our close-world dataset comprises a total
of 12 reported bugs that have corresponding fixed patches. This
dataset enables us to evaluate Katâ€™s performance accurately and
reliably in detecting API-related bugs.
4.2.2 Open-world Dataset. The open-world dataset refers to open-
source projects that may contain unreported bugs. We reuse the
dataset released by Catcher [ 33], which consists of 21 large-scale
projects from Apache. With this dataset, we can investigate the
generalizability of Kat in the large-scale dataset by comparing the
total number of detected bugs with Catcher and EvoSuite.
4.3 Research Questions
We investigate the following research questions:
â€¢RQ1: Whatâ€™s the quality of the knowledge graph? We assess
the accuracy of the triples extracted from the API documentation
and evaluate the quality of the knowledge graph constructed.
â€¢RQ2: Whatâ€™s the overall performance of Kat?We utilize a
close-world dataset to compare the overall performance of Kat
with other SBST approaches in finding API-related bugs.
â€¢RQ3: Whatâ€™s the effectiveness of Katâ€™s different design
choices? We compare the performance of Kat with its variants
on the close-world dataset to evaluate the effectiveness of our
design choices.
â€¢RQ4: Whatâ€™s the generalizability of Kat on an open-world
dataset? We quantitatively and qualitatively analyze the perfor-
mance of Kat and other baselines on an open-world dataset.
5 EVALUATION
5.1 Quality of Knowledge Graph (RQ1)
5.1.1 Evaluation Strategy & Metric. The API exception handling
knowledge graph is used to guide generating test oracles in this
work, so its quality can directly affect the performance of Kat
in detecting bugs violating API specifications. We need to evalu-
ate whether the knowledge graph is highly qualified to be used.
As we consider four types of exception triggers in the knowledge
graph, we adopt a statistical sampling method [ 58] to examine the
quality of the minimum number ( ğ‘€ğ¼ğ‘ ) of triggers in each type
(see section 3.2.2), concerning their classified types and extracted
triples. This sampling method ensures that the estimated accuracy
is in a certain error margin at a certain confidence level. This ğ‘€ğ¼ğ‘
is calculated by ğ‘€ğ¼ğ‘ =ğ‘›0/(1+(ğ‘›0âˆ’1)/ğ‘ğ‘œğ‘ğ‘¢ğ‘™ğ‘ğ‘¡ğ‘–ğ‘œğ‘›ğ‘ ğ‘–ğ‘§ğ‘’). In the
equation,ğ‘›0depends on the selected confidence level and the de-
sired error margin: ğ‘›0=(ğ‘2âˆ—0.25)/ğ‘’2, whereğ‘is a confidencelevelâ€™s Z-score and ğ‘’is the error margin. We use the error margin
0.05at the 95% confidence level in our evaluation. Given a large
number of triggers, ğ‘€ğ¼ğ‘ is approximately 384 in this statistical
setting. Two authors are invited to independently evaluate the accu-
racy (i.e.,ğ‘›ğ‘¢ğ‘šğ‘ğ‘’ğ‘Ÿ ğ‘œğ‘“ ğ‘ğ‘œğ‘Ÿğ‘Ÿğ‘’ğ‘ğ‘¡ ğ‘¡ğ‘Ÿğ‘–ğ‘ğ‘™ğ‘’ğ‘ 
ğ‘›ğ‘¢ğ‘šğ‘ğ‘’ğ‘Ÿ ğ‘œğ‘“ ğ‘ ğ‘ğ‘šğ‘ğ‘™ğ‘’ğ‘‘ ğ‘¡ğ‘Ÿğ‘–ğ‘ğ‘™ğ‘’ğ‘ ) for the sampled triggers. They
judge two aspects: whether or not the triggers are classified into
the appropriate types, and whether the triples extracted from cor-
responding triggers are correct. We compute Cohenâ€™s Kappa [ 35]
to evaluate the inter-rater agreement. If there is a disagreement
between the above two judgments, authors must discuss and come
to a consensus. Based on the consensus annotations, we evaluate
the quality of each type of extracted information.
5.1.2 Results of RQ1 - Quality of knowledge graph. Table 1 shows
the statistics of triggers and their corresponding triples extracted.
Table 2 reports the accuracy of all sampled triggers evaluated by
us. The columns ğ´ğ‘ğ‘. 1andğ´ğ‘ğ‘. 2show the accuracy determined by
two annotators independently, and the column ğ´ğ‘ğ‘.ğ¹ is the final
accuracy after resolving disagreements. Column Kappa shows the
inter-rater agreement.
The statistics of triggers show that we collect a total of 40,132
triggers with four types, covering 17,039 Java SE&JDK APIs. For
the accuracy of different trigger types in Table 2, the final accuracy
values are all above 94.00%, and the Cohenâ€™s Kappa values are over
0.77. These results demonstrate substantial agreement between the
annotators and highlight the high quality of the extracted triples.
This, in turn, reflects the quality of our API exception handling
knowledge graph. Note that T4has the most significant number
of triggers and T3has the least, and both of them show relevant
low accuracy and Kappa. The reason is different from T1andT2,
which can be identified by explicit patterns (e.g., â€œnullâ€, â€œless thanâ€
etc.), we cannot extract patterns to identify T3andT4. We identify
them by computing sentence similarity when linking entities in
triples (see section 3.2.2 for details). Thus, we sacrifice the quantity
ofT3for the high quality of the knowledge graph. Although the
accuracy of T3is the lowest, it still achieves high performance with
a final accuracy of 94%.
Answer to RQ1: Kat is supported by sufficient high-quality
knowledge, covering a wide range of APIs, which can be reliably
used in detecting potential bugs, generating and validating test
cases.
5.2 Performance of Kat (RQ2/3/4)
5.2.1 Evaluation Strategy & Metric. ForRQ2 andRQ3 , we apply
Kat and other baselines to generate test cases and validate them
with ground truth for all the buggy classes in the close-world dataset.
ForRQ4 , we apply Kat, Catcher and EvoSuite in the open-world
dataset and compare how many bugs they can detect, respectively.
On the close-world experiments, we compute precision, recall,
and F1-score to evaluate the performance of finding bugs among our
Kat, the other two baselines (i.e., Catcher and EvoSuite to answer
RQ2 ), and the variants of Kat (i.e.,Katğ‘ ğ‘¡ğ‘ğ‘¡ğ‘–ğ‘ andKatğ‘‘ğ‘¦ğ‘›ğ‘ğ‘šğ‘–ğ‘ to
answer RQ3 ). Precision (ğ‘‡ğ‘ƒ
ğ‘‡ğ‘ƒ+ğ¹ğ‘ƒ) represents the proportion of bugs
that are correctly classified as bugs among all bugs detected by
the methods. Recall (ğ‘‡ğ‘ƒ
ğ‘‡ğ‘ƒ+ğ¹ğ‘) represents the proportion of all bugsESEC/FSE â€™23, December 3â€“9, 2023, San Francisco, CA, USA Xiaoxue Ren, Xinyuan Ye, Yun Lin, Zhenchang Xing, Shuqing Li, and Michael R. Lyu
Table 1: Four types of exception triples in Knowledge Graph
Trigger Type # of triggers # of covered API Trigger Example triple
T1 3,525 526 Method.getAnnotation(annotationClass) throw NullPointerEx-
ception if the given annotation class is null.NullPointerException: [annotationClass, ==, null]
T2 6,488 772 List.remove(index) throw IndexOutOfBoundsException if the
index is out of range (index < 0 || index >= size())IndexOutOfBoundsException: ğ‘‚ğ‘…([index, <, 0], [index, >=, List.size()])
T3 1,357 1,188 SortedSet.first() throw NoSuchElementException if this set is
emptyNoSuchElementException: [util.Set.isEmpty(), ==, true]
T4 28,762 14,553 Certificate.getEncoded() throw CertificateEncodingException if
an encoding error occursCertificateEncodingException: [Certificate.getEncoded(), try, catch]
Table 2: Quality of API Exception Handling Knowledge
Graph
Trigger Type MIN Acc.1 Acc.2 Acc.F Kappa
T1 346 99.72% 98.97% 99.25% 0.86
T2 363 97.40% 96.81% 97.15% 0.84
T3 300 95.28% 94.40% 94.00% 0.77
T4 379 96.33% 95.26% 95.42% 0.81
Average 347 97.18% 96.36% 96.46% 0.82
that are correctly classified as bugs. F1-score (2Ã—ğ‘ğ‘Ÿğ‘’ğ‘ğ‘–ğ‘ ğ‘–ğ‘œğ‘›Ã—ğ‘Ÿğ‘’ğ‘ğ‘ğ‘™ğ‘™
ğ‘ğ‘Ÿğ‘’ğ‘ğ‘–ğ‘ ğ‘–ğ‘œğ‘›+ğ‘Ÿğ‘’ğ‘ğ‘ğ‘™ğ‘™)
is the harmonic mean of precision and recall, which is a balanced
metric of them. The four statistics appear above are as follows: FP
(false positive) represents the number of bug-free programs that
are classified as bugs; FN (false negative) represents the number
of bugs that are classified as bug-free programs; TP (true positive)
represents the number of bugs that are correctly classified as bugs;
and TN (true negative) represents the number of bug-free programs
classified as bug-free. On the open-world experiments to answer
RQ4 , we follow the evaluation metrics of Catcher (i.e., counting the
total number of real bugs detected by Kat, Catcher and EvoSuite)
to demonstrate the generalizability of Kat.
For experiments on the two datasets with SBST methods (i.e.,
Kat, Catcher, EvoSuite, Katğ‘‘ğ‘¦ğ‘›ğ‘ğ‘šğ‘–ğ‘ ), we repeat each of the experi-
ments 25 times (the repeat times is determined by [ 33]) since such
approaches employ optimization strategies to search. We allocate
200 seconds for each class to generate test cases in individual ex-
ecution, and run each execution on gnu/Linux system (Ubuntu
18.04.6 LTS) with 5.4.0-128-generic Linux kernel, 28-core 2.60GHz
Intel(R) Xeon(R) Gold 6348 CPU and 1TB RAM. Note that the vari-
antKatğ‘ ğ‘¡ğ‘ğ‘¡ğ‘–ğ‘ does not require repeated experiments as it is based
on static code analysis.
5.2.2 Results of RQ2 - Overall Performance of Kat.Table 3 presents
the comprehensive results obtained from the close-world dataset,
highlighting the effectiveness of different approaches in identifying
API-related bugs. The table consists of three columns: Catcher,
EvoSuite, and Kat, representing the outcomes of two baselines
and our proposed method, respectively. Each column indicates the
ability of the corresponding method to detect bugs and the number
of times it correctly identified the ground truth out of 25 executions.
For instance, 7/25 (see the first row of column Catcher) signifies
that Catcher accurately detected the bug on seven occasions during
25 executions. The Total row represents the total number of bugs
identified among all the bugs present in the close-world dataset.
For example, 5/12 (see the last row of column Catcher) means that
Catcher successfully identified five bugs out of the total 12 bugs in
the close-world dataset.Table 3: Overall Performance of Bug Detection on the Close-
world Dataset
Source Type Bug ID Project Name CatcherEvoSuite
(Katğ‘‘ğ‘¦ğ‘›ğ‘ğ‘šğ‘–ğ‘ )Katğ‘ ğ‘¡ğ‘ğ‘¡ğ‘–ğ‘ Kat
Defects4jB1 CSV5 Commons-csv 7/25 6/25 8/1 21/25
B4Lang13 Commons-lang - - 1/1 -
Lang37 Commons-lang - - 12/1 7/25
GithubB1#02f519e Commons-cli - - 3/1 25/25
#48f0e90 Hibernate-ORM 6/25 - 8/1 25/25
B2#899f2f4 Rumble - - 6/1 25/25
#2d0436e sphinx4 3/25 15/25 5/1 25/25
#d88d7a1 jMeter - 13/25 8/1 25/25
B3#5787fe0 jStyleParser 3/25 - 4/1 25/25
#7314516 jMeter - - 8/1 25/25
#9ed8de3 msgraph-sdk 10/25 12/25 4/1 25/25
#586d1ed tabula - - 5/1 25/25
Total - - - 5/12 4/12 72/12 11/12
From the total row of Catcher, EvoSuite and Kat, we can observe
an overall better performance of Kat in detecting bugs violating
API specifications. Specifically, Katcan find six ( 11âˆ’5) more bugs
than Catcher andseven ( 11âˆ’4) more bugs than EvoSuite after
repeating 25 times experiments. Such a significant achievement can
be attributed to the fact that Kat can specifically point out where
to test based on the (in)consistencies between the API knowledge
graph and the API usage in the PUT. Kat then transforms the
(in)consistencies into programs and instruments them into the PUT
as test goals. Both Catcher and EvoSuite show poor performance,
especially when no test goals are found because they can only
search randomly for test goals in this case.
Such randomness is also shown by the hit times in 25 executions.
The bugs detected by Catcher and EvoSuite often have a few hit
times in all 25 executions, such as three times hitting the bug for
sphinx4. However, as long as the bug is a little more complicated,
it can be detected by neither Catcher nor EvoSuite. For an example
of the bug #586d1ed in tabula, which is a B3bug about checking
Iterator.hasNext() before Iterator.next() , it can be detected by
neither Catcher nor EvoSuite. Unlike Catcher and EvoSuite, Kat
can add test goals to the PUT by adding additional branches. In this
case, Kat can avoid randomness and always hit the bugs, and more
complex bugs can be detected. The PUT in Fig. 1 is a complicated
example of missing test goals. Both Catcher and EvoSuite cannot
detect it, because it is difficult for them to generate a null object to
trigger the bug randomly.
Moreover, Table 4 shows the performance of each method from
the aspects of different bug types by measuring precision, recall, and
F1-score. The bug types are corresponding to the types of exception
triggers. The results illustrate that B2/B3/B4 (i.e., missing null check,
missing range checking and missing call) show better performances
than B4(i.e., missing try/catch ). It is because the correspondingAPI-Knowledge Aware Search-Based Software Testing: Where, What, and How ESEC/FSE â€™23, December 3â€“9, 2023, San Francisco, CA, USA
Table 4: Results of Precision, Recall and F1-score on the Close-World Dataset
Bug TypeCatcher EvoSuite ( Katğ‘‘ğ‘¦ğ‘›ğ‘ğ‘šğ‘–ğ‘ ) Katğ‘ ğ‘¡ğ‘ğ‘¡ğ‘–ğ‘ Kat F1 Improvement
Prec. Rec. F1 Prec. Rec. F1 Prec. Rec. F1 Prec. Rec. F1 Catcher EvoSuite Katğ‘ ğ‘¡ğ‘ğ‘¡ğ‘–ğ‘
B1 0.40 0.67 0.50 0.33 0.33 0.33 0.16 1.00 0.27 0.50 0.67 0.57 14.00% 72.72% 111.11%
B2 0.20 0.33 0.25 0.20 0.33 0.25 0.16 1.00 0.27 0.38 1.00 0.55 120.00% 120.00% 103.70%
B3 0.22 0.50 0.31 0.13 0.25 0.17 0.19 1.00 0.32 0.40 1.00 0.57 83.87% 235.29% 78.13%
B4 - - - - - - 0.15 1.00 0.27 0.50 0.50 0.50 - - 85.18%
Total 0.26 0.36 0.30 0.20 0.29 0.24 0.16 1.00 0.28 0.42 0.79 0.55 83.33% 129.17% 96.43%
triggers, (i.e., T1/T2/T3 ) have more specific API constraints, which
can be easily converted into test goals to help search algorithms
optimize with branch coverage. In contrast, T4provides limited
knowledge to guide test case generation because it is vague. Kat
just instruments try{[Target]} catch([Exception] e) into the PUT,
considering T4 triggers.
Answer to RQ2: Kat can find six and seven more API-related
bugs than Catcher and EvoSuite, respectively. Moreover, Kat
shows great performance in detecting missing null check /missing
range /missing call bugs.
5.2.3 Results of RQ3 - Effectiveness of Kat.To evaluate the effec-
tiveness of Kat, we compare it with its two variants (i.e., Katğ‘ ğ‘¡ğ‘ğ‘¡ğ‘–ğ‘
andKatğ‘‘ğ‘¦ğ‘›ğ‘ğ‘šğ‘–ğ‘ ). Table 3 and Table 4 report the performance of
Kat and its variants (i.e., Katğ‘ ğ‘¡ğ‘ğ‘¡ğ‘–ğ‘ andKatğ‘‘ğ‘¦ğ‘›ğ‘ğ‘šğ‘–ğ‘ ) in the close-
world dataset.
In Table 3, we present the performance of Kat and its variants,
Katğ‘ ğ‘¡ğ‘ğ‘¡ğ‘–ğ‘ andKatğ‘‘ğ‘¦ğ‘›ğ‘ğ‘šğ‘–ğ‘ , on the close-world dataset. The column
labeled Katğ‘‘ğ‘¦ğ‘›ğ‘ğ‘šğ‘–ğ‘ (EvoSuite) provides information on whether
the bug can be detected by Katğ‘‘ğ‘¦ğ‘›ğ‘ğ‘šğ‘–ğ‘ and how many times it
successfully detects bugs out of 25 executions. Column Katğ‘ ğ‘¡ğ‘ğ‘¡ğ‘–ğ‘
reports whether the bug can be detected by Katğ‘ ğ‘¡ğ‘ğ‘¡ğ‘–ğ‘ and how
many bugs in total it can detect in the programs. For example,
8/1 (see the first row of column Katğ‘ ğ‘¡ğ‘ğ‘¡ğ‘–ğ‘ ) means the static ap-
proach (i.e., Katğ‘ ğ‘¡ğ‘ğ‘¡ğ‘–ğ‘ ) reports eight potential bugs and contains
the ground truth bug. 72/12 (see the last row of column Katğ‘ ğ‘¡ğ‘ğ‘¡ğ‘–ğ‘ )
means Katğ‘ ğ‘¡ğ‘ğ‘¡ğ‘–ğ‘ reports a total of 72 potential bugs, which covers
all the 12 ground truth bugs. In Table 4, we measure the precision,
recall, and F1-score of Kat and its variants. Note that once the
bug is detected among 25 executions, the result is denoted as pos-
itive; otherwise negative. Results of the close-world experiment
in Table 3 show, although Katğ‘ ğ‘¡ğ‘ğ‘¡ğ‘–ğ‘ can find all bugs in the close-
world dataset, it suffers from an extremely high false positive rate.
Because Katğ‘ ğ‘¡ğ‘ğ‘¡ğ‘–ğ‘ uses static analysis to detect bugs, and naively as-
sumes that programs violating constraints in knowledge graphs are
bugs. Thus, bugs detected by Katğ‘ ğ‘¡ğ‘ğ‘¡ğ‘–ğ‘ require manual verification,
which cannot effectively help in bug detection, and even increases
the burden of verification. For Katğ‘‘ğ‘¦ğ‘›ğ‘ğ‘šğ‘–ğ‘ , as discussed in sec-
tion 5.2.2, it uses a random search strategy to generate test cases for
bug detection automatically. The strategy makes it with acceptable
results in the quality of the bug detected, but poor performance in
the number of bugs in the close-world dataset.
Table 4 shows that the design of Kat is very effective, outper-
forming both variants with the improvement of F1-score by 129.17
% and 96.43%, respectively. Kat is able to achieve better perfor-
mance because it considers the characteristics of both variants.Table 5: Performance on the Open-world Dataset (Total re-
ports the number of all unique bugs detected from the open-
world dataset.)
ID Project Name Version Kat Catcher EvoSuite
BCEL bcel 6.2 8 9 7
CLI commons-cli 1.4 0 0 0
CODEC commons-codec 1.12 12 9 9
COLL commons-collections 4.2 15 10 9
COMP commons-compress 1.17 33 34 18
LANG commons-lang 3.7 28 32 23
MATH commons-math 3.6.1 16 12 9
EASY easymock 3.6 18 16 11
GSON gson 2.8.5 16 16 8
HAMC hamcrest-core 1.3 0 0 0
JACK jackson-databind 2.9.6 7 6 5
JAVS javassist 3.23.1 6 4 2
JCOM jcommander 1.71 2 2 1
JFCH jfreechart 1.5.0 28 27 23
JODA joda-time 2.10 23 25 11
JOPT jopt-simple 5.0.4 5 3 3
NATT natty 0.13 5 4 4
NEO4 neo4j-java-driver 1.6.2 21 17 8
SHIRO shiro-core 1.3.2 12 7 6
XJOB xwiki-commons-job 10.6 10 10 9
XTEX xwiki-commons-text 10.6 0 0 0
Total - - 265 243 166
Kat designs an effective strategy to integrate both variantsâ€™ advan-
tages by converting constraints in the knowledge graph to program
constraints and instrumenting them into the PUT as additional
branches.
Answer to RQ3: The design choices can contribute to the effec-
tiveness of Kat, which improve the the F1-score from 96.43% âˆ¼
129.17%.
5.2.4 Results of RQ4 - Generalizability of Kat.Table 5 shows the
total number of bugs detected by Kat and the other two baselines
(i.e., Catcher and EvoSuite) with 25 executions of experiments in the
open-world dataset. Overall, Kat exhibits remarkable performance
in bug detection, surpassing both Catcher and EvoSuite. With a
total of 265 unique bugs identified across all 25 executions, Kat
showcases superior effectiveness and remarkable generalizability
in the large-scale open-world dataset. In more specific terms, Kat
outperforms Catcher by discovering 22 additional bugs ( 265âˆ’243)
and surpasses EvoSuite by detecting 99 additional bugs ( 265âˆ’166).
These findings further emphasize the significant advantage and
robustness of Kat over the baselines.ESEC/FSE â€™23, December 3â€“9, 2023, San Francisco, CA, USA Xiaoxue Ren, Xinyuan Ye, Yun Lin, Zhenchang Xing, Shuqing Li, and Michael R. Lyu
Table 6: Overlap Between Kat and Catcher Regarding the
Unique Bugs Detected Across 25 Executions.
Porject ID Katâˆ©Catcher Kat\Catcher Catcher\Kat
BCEL 8 - 1
CLI - - -
CODEC 9 3 -
COLL 10 5 -
COMP 33 - 1
LANG 28 - 4
MATH 11 5 1
EASY 16 2 -
GSON 16 - -
HAMC - - -
JACK 6 1 -
JAVS 4 2 -
JCOM 2 - -
JFCH 27 1 -
JODA 23 - 2
JOPT 3 2 -
NATT 4 1 -
NEO4 17 4 -
SHIRO 7 5 -
XJOB 10 - -
XTEX - - -
Total 234 31 9
Table 6 presents a comprehensive summary of bug detection
results for both Kat and Catcher. The table highlights the number
of bugs detected by both approaches, as well as the distinct set
of bugs identified exclusively by each tool. The analysis reveals
that 234 unique bugs were detected by both Kat and Catcher, in-
dicating a considerable overlap in their bug detection capabilities.
Additionally, Kat identified 31 unique bugs that were not detected
by Catcher, demonstrating its ability to uncover specific bugs that
might have been overlooked by the other tool. Conversely, Catcher
discovered nine unique bugs that were not detected by Kat, show-
casing its effectiveness in identifying a subset of bugs missed by
the alternative approach. Compared with EvoSuite, both Kat and
Catcher take absolute advantages, because both methods utilize
external exception information from documentation/source code
to identify bugs. Catcher first detects potential bugs and then opti-
mizes searching objectives by combining four coverage (i.e., branch,
line, input, and output). Differently, Kat transforms API excep-
tion handling knowledge into program constraints and instruments
them into PUT as additional branches. As a result, it can guide
search algorithms to cover more branches and detect more bugs.
For example, Fig. 1 is a bug from the class SignatureReader in
EASY project that can only be detected by Kat. The bug happens
in the method acceptType in line 57 of PUT, because it can trigger
the exception of String.charAt(index) in line 63 of the method
parseType . This case shows a bug without test goals, and Catcher
cannot easily synthesize a test case to trigger the exception because
there is no guidance such as branch distance to synthesize the two
specific Java objects.
When considering the bugs exclusively detected by Catcher, our
observations indicate that these cases primarily fall into the cat-
egory of B4bugs, which are triggered by T4. The reason behindCatcherâ€™s success in detecting these bugs lies in its utilization of var-
ious coverage techniques, allowing it to address the challenge posed
by such transformations effectively. On the other hand, Kat han-
dles this transformation by introducing try/catch as an additional
branch. However, covering this branch within a reasonable branch
distance can pose challenges for search algorithms. Nonetheless,
Catcherâ€™s approach proves effective in overcoming these challenges.
Answer to RQ4: Kat can detect 22 more and 99 more bugs
than Catcher and EvoSuite in the open-world dataset, which
indicates better generalizability.
6 DISCUSSION
6.1 Effectiveness and Efficiency of Kat
6.1.1 Effectiveness. With extensive experiments, our Kat can out-
perform static analysis and other dynamic SBST approaches in
finding bugs. Compared to static analysis (i.e., Katğ‘ ğ‘¡ğ‘ğ‘¡ğ‘–ğ‘ ),Kat
demonstrates significant improvements in addressing false pos-
itive API-related bugs and achieving automated bug detection. By
incorporating search strategies in Phase III (as depicted in Fig. 2),
Kat effectively generates test cases to verify the existence of bugs
reported by static analysis. This capability leads to a considerable
reduction of false positive instances, for example, a decrease of
61 cases ( 72âˆ’11). Furthermore, when compared to existing SBST
approaches (i.e., Catcher and EvoSuite/ Katğ‘‘ğ‘¦ğ‘›ğ‘ğ‘šğ‘–ğ‘ ),Kat exhibits
the ability to discover a greater number of bugs within a limited
search time. Specifically, Kat can detect 22 more and 99 more bugs
than Catcher and EvoSuite, respectively. This advantage stems
from the utilization of an API exception handling knowledge graph,
which enables Kat to identify potential bugs and transform the
corresponding violation constraints into instrumented branches.
Consequently, Kat effectively improves branch coverage and en-
hances the bug detection process. While the advancements achieved
byKat may be considered relatively modest compared to the state-
of-the-art SBST approaches, it is important to recognize that even
modest progress can have significant implications. Such advance-
ments contribute to the cumulative progress within the SBST field
and lay the groundwork for further developments and improve-
ments.
6.1.2 Efficiency. When optimizing searching strategies, SBST re-
quires setting a time budget. Kat allocates a maximum of 200 sec-
onds for each class under test, which is the same time set in Catcher.
For the same class under test, all of Catcher, EvoSuite and Kat are
allocated the same time (i.e., 200s), but Kat can find more bugs.
Note, for Kat, we do not consider the time consumption of con-
structing the API exception handling knowledge graph, static bug
detection and instrumentation. First, the construction of the knowl-
edge graph occurs offline, meaning it is not performed in real time
during the bug detection. As a result, any time consumed during
this process does not impact the overall efficiency of Kat. Second,
regarding the static bug detection and instrumentation for each
class, it is crucial to note that these operations are highly efficient
and completed within a very short duration, typically within one
second. When considering the overall time budget allocated for
the evaluation or execution of Kat, which is around 200 seconds,API-Knowledge Aware Search-Based Software Testing: Where, What, and How ESEC/FSE â€™23, December 3â€“9, 2023, San Francisco, CA, USA
the negligible time consumed by static bug detection and instru-
mentation becomes inconsequential. Therefore, it is appropriate
to disregard this time when comparing the performance of Kat
within the allocated time budget.
6.2 Threats to Validity
6.2.1 Internal Validity. The internal threats to validity refer to the
quality of the API exception handling knowledge graph, which
plays a pivotal role in our bug detection approach. To reduce per-
sonal bias when evaluating the knowledge graph quality, we em-
ployed a method that involved two authors independently anno-
tating the data instances. To assess the agreement between the
annotators and ensure reliability, we computed Cohenâ€™s kappa co-
efficient, which indicates a substantial or almost perfect agreement
between the two annotators.
6.2.2 External Validity. The external threats to validity arise from
two aspects: limited knowledge from the API documentation and
the small-scale close-world dataset. First, considering the quality
of API knowledge is critical to the performance of Kat, we prefer
selecting the high-quality Java official documentation to construct
a knowledge graph inspired by the previous studies [ 36,50]. Al-
though the official documentation provides a limited number of
APIs, we design our API exception handling knowledge graph and
open information extraction pipeline to be adaptable and extend-
able, allowing us to incorporate additional knowledge from diverse
sources. By continuously expanding our knowledge graph, we aim
to enhance the comprehensiveness of the information available.
Second, the small scale of our close-world dataset can be attributed
to two key factors: 1) our research primarily focuses on a specific
subset of API knowledge (i.e., Java SE&JDK API knowledge). Con-
sequently, the number of bugs directly associated with these APIs
is naturally limited. 2) Our adherence to the rigorous strategy em-
ployed by Defects4j [ 32] has led us to conduct an exhaustive search
for high-quality bugs. This meticulous approach is time-consuming
and demands significant effort and resources. Nevertheless, we
remain dedicated to ongoing efforts aimed at expanding and en-
riching our close-world dataset. This commitment entails actively
collecting more bugs in the future. Hence, we aim to broaden the
scope of our dataset, enhance its representativeness, and strengthen
the validity and reliability of our research findings.
7 RELATED WORK
7.1 Search-based Software Testing
Search-based software testing (SBST) is a powerful technique for
automating test generation, showing promising results [ 2,27,38,
40]. SBST relies on specific heuristics for different coverage criteria,
guiding the generation of test cases to achieve higher coverage.
Initially, the single-target approach aimed to satisfy one coverage
target (e.g., one branch) at a time through multiple search iterations
(e.g., genetic algorithms) [ 12,17,18,54]. However, multi-target
approaches have emerged as superior, using many-objective search
techniques to achieve higher code coverage [31, 41, 45, 56, 71].
Catcher [ 33], related to our work, employs a two-step approach.
It first uses exception propagation to identify potential API misuses
in the program under test statically (PUT), narrowing the searchspace. Then, it uses coverage-based heuristics to guide a many-
objective search to cover the identified API call sites and expose
propagated exceptions, primarily focusing on API-related bugs.
In contrast, our approach, called Kat, goes beyond static detec-
tion of API misuses. We leverage API exception-handling knowl-
edge gathered from API usage specifications to create test oracles,
enhancing Katâ€™s effectiveness by incorporating domain-specific
API exception handling into the process.
7.2 Test Oracle Automation
To effectively detect real-world software faults, we must improve
generated tests with accurate test oracles [ 4]. These oracles signifi-
cantly impact testing quality and software system reliability [ 22,28].
Constructing test oracles automatically is a challenging task, often
considered a bottleneck in automated software testing [8]. Recent
efforts have approached this problem from three angles: implicit
oracles, derived oracles, and specified oracles [7, 13, 55, 67].
Implicit oracles rely on domain expert knowledge to determine
test pass/fail. For example, segment faults are typical errors, and
frame rate drops in game applications may indicate performance
issues [ 8]. Derived oracles use existing information to distinguish
correct from incorrect software behavior. Techniques include meta-
morphic relations, version comparisons, and metadata analysis [ 7,
13,55,67]. Specified oracles require formal specifications or con-
tracts to define expected behavior [ 16,48] or contracts [ 14,64]. The
quality and completeness of these specifications are critical [ 8,24].
In contrast, our approach focuses on converting exception han-
dling documentation knowledge into test oracles.
8 CONCLUSION
Search-based software testing (SBST) has proved its effectiveness
in generating test cases to achieve its defined test goals such as
branch and data-dependency coverage. However, to detect more
program faults in an effective way, pre-defined goals can hardly
be adaptive in diversified projects. We propose Kat, a knowledge
extraction-based approach to generate on-demand assertions in
the PUT, validating the internal execution states when interacting
with API frameworks. With evaluations on both close-world and
open-world datasets, we illustrate Kat performs better than other
baselines (i.e., Catcher and EvoSuite) in finding more bugs violating
API specifications. Additionally, Kat also shows good generalizabil-
ity in a large-scale dataset. In the future, we will extend our work
in the following two aspects. First, we will extend the knowledge
graph to include more knowledge to guide test case generation.
Second, we will further develop Kat and apply the idea to program
repair.
9 DATA AVAILABILITY
All Experimental data are available at our GitHub repository.
ACKNOWLEDGEMENTS
This research was partially supported by the Fundamental Research
Funds for the Central Universities (226-2022-00064) and Research
Grants Council of the Hong Kong Special Administrative Region,
China (No. CUHK 14206921 of the General Research Fund).ESEC/FSE â€™23, December 3â€“9, 2023, San Francisco, CA, USA Xiaoxue Ren, Xinyuan Ye, Yun Lin, Zhenchang Xing, Shuqing Li, and Michael R. Lyu
REFERENCES
[1]KK Aggarwal, Yogesh Singh, Arvinder Kaur, and OP Sangwan. 2004. A neural
net based approach to test oracle. ACM SIGSOFT Software Engineering Notes 29,
3 (2004), 1â€“6.
[2]Shaukat Ali, Lionel C Briand, Hadi Hemmati, and Rajwinder Kaur Panesar-
Walawege. 2009. A systematic review of the application and empirical inves-
tigation of search-based test case generation. IEEE Transactions on Software
Engineering 36, 6 (2009), 742â€“762.
[3]M Moein Almasi, Hadi Hemmati, Gordon Fraser, Andrea Arcuri, and Janis Bene-
felds. 2017. An industrial evaluation of unit test generation: Finding real faults
in a financial application. In 2017 IEEE/ACM 39th International Conference on
Software Engineering: Software Engineering in Practice Track (ICSE-SEIP) . IEEE,
263â€“272.
[4]Saswat Anand, Edmund K Burke, Tsong Yueh Chen, John Clark, Myra B Cohen,
Wolfgang Grieskamp, Mark Harman, Mary Jean Harrold, Phil McMinn, Antonia
Bertolino, et al .2013. An orchestrated survey of methodologies for automated
software test case generation. Journal of Systems and Software 86, 8 (2013),
1978â€“2001.
[5]Andrea Arcuri, JosÃ© Campos, and Gordon Fraser. 2016. Unit test generation
during software development: Evosuite plugins for maven, intellij and jenkins. In
2016 IEEE International Conference on Software Testing, Verification and Validation
(ICST) . IEEE, 401â€“408.
[6]Andrea Arcuri and Gordon Fraser. 2013. Parameter tuning or default values? An
empirical investigation in search-based software engineering. Empirical Software
Engineering 18, 3 (2013), 594â€“623.
[7]Jon Ayerdi, Pablo Valle, Sergio Segura, Aitor Arrieta, Goiuria Sagardui, and Maite
Arratibel. 2022. Performance-driven metamorphic testing of cyber-physical
systems. IEEE Transactions on Reliability (2022).
[8]Earl T Barr, Mark Harman, Phil McMinn, Muzammil Shahbaz, and Shin Yoo. 2014.
The oracle problem in software testing: A survey. IEEE transactions on software
engineering 41, 5 (2014), 507â€“525.
[9]Arianna Blasi, Alberto Goffi, Konstantin Kuznetsov, Alessandra Gorla, Michael D
Ernst, Mauro PezzÃ¨, and Sergio Delgado Castellanos. 2018. Translating code
comments to procedure specifications. In Proceedings of the 27th ACM SIGSOFT
International Symposium on Software Testing and Analysis . 242â€“253.
[10] Javassist: Java bytecode engineering toolkit. [n. d.]. https://www.javassist.org/.
([n. d.]).
[11] JosÃ© Campos, Andrea Arcuri, Gordon Fraser, and Rui Abreu. 2014. Continuous test
generation: Enhancing continuous integration with automated test generation. In
Proceedings of the 29th ACM/IEEE international conference on Automated software
engineering . 55â€“66.
[12] Kai H Chang, JAMES H CROSS II, W Homer Carlisle, and Shih-Sung Liao. 1996.
A performance evaluation of heuristics-based test case generation methods for
software branch coverage. International Journal of Software Engineering and
Knowledge Engineering 6, 04 (1996), 585â€“608.
[13] Tsong Y Chen, Shing C Cheung, and Shiu Ming Yiu. 2020. Metamorphic testing:
a new approach for generating next test cases. arXiv preprint arXiv:2002.12543
(2020).
[14] Yoonsik Cheon and Gary T Leavens. 2002. A simple and practical approach to unit
testing: The JML and JUnit way. In ECOOP 2002â€”Object-Oriented Programming:
16th European Conference MÃ¡laga, Spain, June 10â€“14, 2002 Proceedings 16 . Springer,
231â€“255.
[15] Myra B Cohen. 2019. The maturation of search-based software testing: successes
and challenges. In 2019 IEEE/ACM 12th International Workshop on Search-Based
Software Testing (SBST) . IEEE, 13â€“14.
[16] Xin Feng, David Lorge Parnas, TH Tse, and Tony Oâ€™Callaghan. 2011. A compari-
son of tabular expression-based testing strategies. IEEE Transactions on Software
Engineering 37, 5 (2011), 616â€“634.
[17] Roger Ferguson and Bogdan Korel. 1996. The chaining approach for software
test data generation. ACM Transactions on Software Engineering and Methodology
(TOSEM) 5, 1 (1996), 63â€“86.
[18] Gordon Fraser and Andrea Arcuri. 2011. Evolutionary generation of whole test
suites. In 2011 11th International Conference on Quality Software . IEEE, 31â€“40.
[19] Gordon Fraser and Andrea Arcuri. 2011. Evosuite: automatic test suite generation
for object-oriented software. In Proceedings of the 19th ACM SIGSOFT symposium
and the 13th European conference on Foundations of software engineering . 416â€“419.
[20] Gordon Fraser and Andrea Arcuri. 2012. Whole test suite generation. IEEE
Transactions on Software Engineering 39, 2 (2012), 276â€“291.
[21] Gordon Fraser and Andrea Arcuri. 2014. A large-scale evaluation of automated
unit test generation using evosuite. ACM Transactions on Software Engineering
and Methodology (TOSEM) 24, 2 (2014), 1â€“42.
[22] Gordon Fraser and Andrea Arcuri. 2015. 1600 faults in 100 projects: automatically
finding faults while achieving high coverage with evosuite. Empirical software
engineering 20, 3 (2015), 611â€“639.
[23] Gordon Fraser and Andreas Zeller. 2011. Mutation-driven generation of unit tests
and oracles. IEEE Transactions on Software Engineering 38, 2 (2011), 278â€“292.
[24] Marie-Claude Gaudel. 2001. Testing from formal specifications, a generic ap-
proach. In Reliable SoftwareTechnologiesâ€”Ada-Europe 2001: 6th Ada-Europe In-
ternational Conference on Reliable Software Technologies Leuven, Belgium, May14â€“18, 2001 Proceedings . Springer, 35â€“48.
[25] Gensim. [n. d.]. https://pypi.org/project/gensim/. ([n. d.]).
[26] Alberto Goffi, Alessandra Gorla, Michael D Ernst, and Mauro PezzÃ¨. 2016. Auto-
matic generation of oracles for exceptional behaviors. In Proceedings of the 25th
international symposium on software testing and analysis . 213â€“224.
[27] Mark Harman, Yue Jia, and Yuanyuan Zhang. 2015. Achievements, open problems
and challenges for search based software testing. In 2015 IEEE 8th International
Conference on Software Testing, Verification and Validation (ICST) . IEEE, 1â€“12.
[28] Mark Harman, Sung Gon Kim, Kiran Lakhotia, Phil McMinn, and Shin Yoo.
2010. Optimizing for the number of tests generated in search based test data
generation with an application to the oracle cost problem. In 2010 Third Inter-
national Conference on Software Testing, Verification, and Validation Workshops .
IEEE, 182â€“191.
[29] Johannes Henkel, Christoph Reichenbach, and Amer Diwan. 2007. Discover-
ing documentation for Java container classes. IEEE Transactions on Software
Engineering 33, 8 (2007), 526â€“543.
[30] Standard Edition & Java Development Kit Version 17 API Specification Java Plat-
form. [n. d.]. https://docs.oracle.com/en/java/javase/17/docs/api. ([n. d.]).
[31] Yue Jia. 2015. Hyperheuristic search for sbst. In 2015 IEEE/ACM 8th International
Workshop on Search-Based Software Testing . IEEE, 15â€“16.
[32] RenÃ© Just, Darioush Jalali, and Michael D Ernst. 2014. Defects4J: A database of ex-
isting faults to enable controlled testing studies for Java programs. In Proceedings
of the 2014 International Symposium on Software Testing and Analysis . 437â€“440.
[33] Maria Kechagia, Xavier Devroey, Annibale Panichella, Georgios Gousios, and
Arie van Deursen. 2019. Effective and efficient API misuse detection via exception
propagation and search-based testing. In Proceedings of the 28th ACM SIGSOFT
international symposium on software testing and analysis . 192â€“203.
[34] Pavneet Singh Kochhar, Ferdian Thung, and David Lo. 2015. Code coverage
and test suite effectiveness: Empirical study with real bugs in large systems.
In2015 IEEE 22nd international conference on software analysis, evolution, and
reengineering (SANER) . IEEE, 560â€“564.
[35] J Richard Landis and Gary G Koch. 1977. An application of hierarchical kappa-
type statistics in the assessment of majority agreement among multiple observers.
Biometrics (1977), 363â€“374.
[36] Hongwei Li, Sirui Li, Jiamou Sun, Zhenchang Xing, Xin Peng, Mingwei Liu, and
Xuejiao Zhao. 2018. Improving api caveats accessibility by mining api caveats
knowledge graph. In 2018 IEEE International Conference on Software Maintenance
and Evolution (ICSME) . IEEE, 183â€“193.
[37] Nan Li and Jeff Offutt. 2016. Test oracle strategies for model-based testing. IEEE
Transactions on Software Engineering 43, 4 (2016), 372â€“395.
[38] Yun Lin, You Sheng Ong, Jun Sun, Gordon Fraser, and Jin Song Dong. 2021.
Graph-based seed object synthesis for search-based unit testing. In Proceedings
of the 29th ACM Joint Meeting on European Software Engineering Conference and
Symposium on the Foundations of Software Engineering . 1068â€“1080.
[39] Mingwei Liu, Xin Peng, Andrian Marcus, Zhenchang Xing, Wenkai Xie, Shuang-
shuang Xing, and Yang Liu. 2019. Generating query-specific class API summaries.
InProceedings of the 2019 27th ACM joint meeting on European software engineering
conference and symposium on the foundations of software engineering . 120â€“130.
[40] Phil McMinn. 2011. Search-based software testing: Past, present and future. In
2011 IEEE Fourth International Conference on Software Testing, Verification and
Validation Workshops . IEEE, 153â€“163.
[41] Mahshid Helali Moghadam, Markus Borg, and Seyed Jalaleddin Mousavirad. 2021.
Deeper at the sbst 2021 tool competition: ADAS testing using multi-objective
search. In 2021 IEEE/ACM 14th International Workshop on Search-Based Software
Testing (SBST) . IEEE, 40â€“41.
[42] Martin Monperrus, Michael Eichberg, Elif Tekes, and Mira Mezini. 2012. What
should developers be aware of? An empirical study on the directives of API
documentation. Empirical Software Engineering 17, 6 (2012), 703â€“737.
[43] neuralcoref. [n. d.]. https://spacy.io/universe/project/neuralcoref. ([n. d.]).
[44] Carlos Pacheco and Michael D Ernst. 2007. Randoop: feedback-directed random
testing for Java. In Companion to the 22nd ACM SIGPLAN conference on Object-
oriented programming systems and applications companion . 815â€“816.
[45] Annibale Panichella, Fitsum Meshesha Kifetew, and Paolo Tonella. 2015. Refor-
mulating branch coverage as a many-objective optimization problem. In 2015
IEEE 8th international conference on software testing, verification and validation
(ICST) . IEEE, 1â€“10.
[46] Annibale Panichella, Fitsum Meshesha Kifetew, and Paolo Tonella. 2017. Au-
tomated test case generation as a many-objective optimisation problem with
dynamic selection of the targets. IEEE Transactions on Software Engineering 44, 2
(2017), 122â€“158.
[47] Anjana Perera, Aldeida Aleti, Marcel BÃ¶hme, and Burak Turhan. 2020. Defect pre-
diction guided search-based software testing. In 2020 35th IEEE/ACM International
Conference on Automated Software Engineering (ASE) . IEEE, 448â€“460.
[48] Dennis K Peters and David Lorge Parnas. 2002. Requirements-based monitors
for real-time systems. IEEE Transactions on Software Engineering 28, 2 (2002),
146â€“158.
[49] Z3 Prover. [n. d.]. https://github.com/Z3Prover/z3. ([n. d.]).API-Knowledge Aware Search-Based Software Testing: Where, What, and How ESEC/FSE â€™23, December 3â€“9, 2023, San Francisco, CA, USA
[50] Xiaoxue Ren, Zhenchang Xing, Xin Xia, Guoqiang Li, and Jianling Sun. 2019.
Discovering, explaining and summarizing controversial discussions in community
q&a sites. In 2019 34th IEEE/ACM International Conference on Automated Software
Engineering (ASE) . IEEE, 151â€“162.
[51] Xiaoxue Ren, Xinyuan Ye, Zhenchang Xing, Xin Xia, Xiwei Xu, Liming Zhu, and
Jianling Sun. 2020. API-misuse detection driven by fine-grained API-constraint
knowledge graph. In 2020 35th IEEE/ACM International Conference on Automated
Software Engineering (ASE) . IEEE, 461â€“472.
[52] JosÃ© Miguel Rojas, Gordon Fraser, and Andrea Arcuri. 2016. Seeding strategies in
search-based unit test generation. Software Testing, Verification and Reliability
26, 5 (2016), 366â€“401.
[53] JosÃ© Miguel Rojas, Mattia Vivanti, Andrea Arcuri, and Gordon Fraser. 2017. A
detailed investigation of the effectiveness of whole test suite generation. Empirical
Software Engineering 22, 2 (2017), 852â€“893.
[54] Simone Scalabrino, Giovanni Grano, Dario Di Nucci, Rocco Oliveto, and An-
drea De Lucia. 2016. Search-based testing of procedural programs: Iterative
single-target or multi-target approach?. In International symposium on search
based software engineering . Springer, 64â€“79.
[55] Sergio Segura, Gordon Fraser, Ana B Sanchez, and Antonio Ruiz-CortÃ©s. 2016. A
survey on metamorphic testing. IEEE Transactions on software engineering 42, 9
(2016), 805â€“824.
[56] Mohammad Mehdi Dejam Shahabi, S Parsa Badiei, S Ehsan Beheshtian, Reza
Akbari, and S Mohammad Reza Moosavi. 2017. EVOTLBO: A TLBO based Method
for Automatic Test Data Generation in EvoSuite. International Journal of Advanced
Computer Science and Applications 8, 6 (2017).
[57] Seyed Reza Shahamiri, Wan Wan-Kadir, Suhaimi Ibrahim, and Siti Zaiton Mohd
Hashim. 2012. Artificial neural networks as multi-networks automated test oracle.
Automated Software Engineering 19, 3 (2012), 303â€“334.
[58] Ravindra Singh and Naurang Singh Mangat. 2013. Elements of survey sampling .
Vol. 15. Springer Science & Business Media.
[59] Victor Sobreira, Thomas Durieux, Fernanda Madeiral, Martin Monperrus, and
Marcelo A. Maia. 2018. Dissection of a Bug Dataset: Anatomy of 395 Patches
from Defects4J. In Proceedings of SANER .
[60] Soot. [n. d.]. http://soot-oss.github.io/soot/. ([n. d.]).[61] Spacy. [n. d.]. https://spacy.io/. ([n. d.]).
[62] Suresh Thummalapenta, Tao Xie, Nikolai Tillmann, Jonathan De Halleux, and
Zhendong Su. 2011. Synthesizing method sequences for high-coverage testing.
ACM SIGPLAN Notices 46, 10 (2011), 189â€“206.
[63] Raja Vallee-Rai and Laurie J Hendren. 1998. Jimple: Simplifying Java bytecode
for analyses and transformations. (1998).
[64] Yi Wei, Carlo A Furia, Nikolay Kazmin, and Bertrand Meyer. 2011. Inferring
better contracts. In Proceedings of the 33rd International Conference on Software
Engineering . 191â€“200.
[65] Word2vec. [n. d.]. https://code.google.com/archive/p/word2vec/. ([n. d.]).
[66] Tao Xie. 2006. Augmenting automatically generated unit-test suites with re-
gression oracle checking. In ECOOP 2006â€“Object-Oriented Programming: 20th
European Conference, Nantes, France, July 3-7, 2006. Proceedings 20 . Springer,
380â€“403.
[67] Xiaoyuan Xie, Joshua WK Ho, Christian Murphy, Gail Kaiser, Baowen Xu, and
Tsong Yueh Chen. 2011. Testing and validating machine learning classifiers by
metamorphic testing. Journal of Systems and Software 84, 4 (2011), 544â€“558.
[68] Juan Zhai, Jianjun Huang, Shiqing Ma, Xiangyu Zhang, Lin Tan, Jianhua Zhao,
and Feng Qin. 2016. Automatic model generation from documentation for Java
API functions. In Proceedings of the 38th International Conference on Software
Engineering . 380â€“391.
[69] Hao Zhong, Lu Zhang, Tao Xie, and Hong Mei. 2009. Inferring resource specifica-
tions from natural language API documentation. In 2009 IEEE/ACM International
Conference on Automated Software Engineering . IEEE, 307â€“318.
[70] Yu Zhou, Changzhi Wang, Xin Yan, Taolue Chen, Sebastiano Panichella, and
Harald Gall. 2018. Automatic detection and repair recommendation of directive
defects in Java API documentation. IEEE Transactions on Software Engineering
46, 9 (2018), 1004â€“1023.
[71] Ziming Zhu, Xiong Xu, and Li Jiao. 2017. Improved evolutionary generation
of test data for multiple paths in search-based software testing. In 2017 IEEE
Congress on Evolutionary Computation (CEC) . IEEE, 612â€“620.
Received 2023-02-02; accepted 2023-07-27