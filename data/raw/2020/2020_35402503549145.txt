AreWeBuilding on theRock? On theImportanceof Data
Preprocessing forCodeSummarization
LinShiâˆ—23
shilin@iscas.ac.cn
Institute of Software, Chinese
Academy of Sciences
Beijing, ChinaFangwenMuâˆ—23
fangwen2020@iscas.ac.cn
Institute of Software, Chinese
Academy of Sciences
Beijing, ChinaXiao Chenâˆ—2
chenxiao2021@iscas.ac.cn
Institute of Software, Chinese
Academy of Sciences
Beijing, China
Song Wang
wangsong@eecs.yorku.ca
Lassonde School of Engineering, York
University
Toronto, CanadaJunjie Wangâˆ—2
junjie@iscas.ac.cn
Institute of Software, Chinese
Academy of Sciences
Beijing, ChinaYe Yang
yangye@gmail.com
Schoolof Systems andEnterprises,
StevensInstitute of Technology
Hoboken, NJ, USA
Ge Li
lige@pku.edu.cn
Key Lab of High Confidence Software
Technology, Peking University
Beijing, ChinaXinXia
xin.xia@acm.org
SoftwareEngineeringApplication
Technology Lab, Huawei
ChinaQing Wangâˆ—2ÄŸ/pilcrow
wq@iscas.ac.cn
Institute of Software, Chinese
Academy of Sciences
Beijing, China
ABSTRACT
Codesummarization,thetaskofgeneratingusefulcommentsgiven
the code,has longbeen ofinterest. Mostof theexistingcodesum-
marization modelsare trainedandvalidatedon widely-usedcode
comment benchmarkdatasets. However,little isknown aboutthe
quality of the benchmark datasets built from real-world projects.
Arethebenchmarkdatasetsasgoodasexpected?Tobridgethegap,
weconductasystematicresearchtoassessandimprovethequality
of four benchmark datasets widely used for code summarization
tasks. First, we propose an automated code-comment cleaning tool
that can accurately detect noisy data caused by inappropriate data
preprocessing operations from existing benchmark datasets. Then,
weapplythetooltofurtherassessthedataqualityofthefourbench-
mark datasets, based on the detected noises. Finally, we conduct
comparative experiments to investigate the impact of noisy data
on the performance of code summarization models. The results
showthatthesedatapreprocessingnoiseswidelyexistinallfour
benchmarkdatasets, and removingthese noisydataleads toa sig-
nificant improvement on the performance ofcode summarization.
âˆ—Also With Laboratory for Internet Software Technologies, Institute of Software, CAS
2AlsoWithUniversityof Chinese Academy of Sciences
3Bothauthors contributed equally tothisresearch
ÄŸAlso With Science & Technology on Integrated Information System Laboratory,
Instituteof Software, CAS
/pilcrowCorresponding author
ESEC/FSE â€™22,November 14Å›18, 2022, Singapore, Singapore
Â©2022 Copyright held by the owner/author(s).
ACM ISBN978-1-4503-9413-0/22/11.
https://doi.org/10.1145/3540250.3549145We believe that the findings and insights will enable a better un-
derstanding of data quality in code summarization tasks, and pave
the wayforrelevant research andpractice.
CCS CONCEPTS
â€¢Software and its engineering â†’Open source model ;â€¢Gen-
eraland reference â†’Empirical studies .
KEYWORDS
Code Summarization, Data Quality,Empirical Study
ACMReferenceFormat:
Lin Shi, Fangwen Mu, Xiao Chen, Song Wang, Junjie Wang, Ye Yang, Ge
Li, Xin Xia, and Qing Wang. 2022. Are We Building on the Rock? On the
Importance of Data Preprocessingfor Code Summarization.In Proceedings
ofthe30thACMJointEuropeanSoftwareEngineeringConferenceandSympo-
siumonthe FoundationsofSoftwareEngineering(ESEC/FSEâ€™22),November
14Å›18, 2022, Singapore, Singapore. ACM, New York, NY, USA, 13pages.
https://doi.org/10.1145/3540250.3549145
1 INTRODUCTION
Codesummarizationconcernstheproductionofanatural-language
description of source code that facilitates software development
and maintenance by enabling developers to comprehend, ideate,
anddocumentcodeeffectively.Learning-basedmodelshavebeen
widely leveraged for the advantages in semantic modeling and
understandingoflanguages. Similarto manyotherlearningtasks,
code summarization models require large-scale and high-quality
training datasets. To that end, multiple benchmark datasets for
code summarizationtasks have been constructed from real-world
projectrepositories, e.g.,GitHub,andarepopularlyusedinmany
codesummarizationstudies.Forexample, Funcom[41]wasreleased
with over 2.1M code-comment pairs from over 29K Java projects in
Thiswork islicensedunderaCreativeCommonsAttribution4.0Interna-
tional License.
107
ESEC/FSE â€™22, November14â€“18, 2022,Singapore, Singapore Lin Shi,FangwenMu, XiaoChen, Song Wang,JunjieWang,Ye Yang,GeLi, XinXia,andQingWang
Benchmark
Datasets
(Origin ) Clean DataClean 
DataOrigin Data
Controlled DataTraining Set Test Set Models
Sec. 4 The Code-Comment 
Cleaning ToolSec. 6 Impacts on the Performance of 
Code SummarizationSec. 5 Quality Assessment of 
Benchmark Datasets
Benchmark
Datasets
(Clean)Sec.3 Taxonomy of Noisy Data
Non-Literal
Over-splittingAuto Code
Duplication
â€¦â€¦Comment Code
â€¦â€¦
Heuristic 
Rules
Figure 1:Overview ofour researchmethodology.
theSourcererrepository.Manycodesummarizationmodels,such
asRe2Com[73],DeepSumm[ 26],andEditSum[ 42],aretrainedand
evaluatedtoberelativelyeffectiveonit.Similarpopulardatasets
includeTLC[33],CSN[35], andPCSD[67].
Althoughthebenchmarkdatasetsareexpectedtobeofgoodqual-
ity, noise is inevitable due to the differences in coding conventions
and assumptions employed in modern programming languages
andIDEs,aswellasadhocnatureofdevelopmentprocessesand
practices[ 54].Forexample,sourcecodeinGitHubiscontributed
by developers all around the world, thus their comments are likely
to contain multiple natural languages that can lead to increases
in complexity regarding the understanding and maintenance of
source code. Existing studies also have confirmed the existence
of many different types of noise in various benchmark datasets,
such as auto-generated code [ 30], Å‚TODOÅ¾ comments [ 14], and
incomplete comments [ 60], despite their data cleaning efforts. Par-
ticularly,Steidl etal.[63]analyzedfiveopensourceprojects,and
reported that nearly one third of the comments do not promote
systemunderstanding.
Toinvestigatetheaforementionedconcernsofdataqualityfor
code summarization,we conducta systematic study to assess and
improvethequalityoffourwidely-usedbenchmarkdatasets, i.e.,
Funcom,TLC,CSN,andPCSD.Theresearchmethodologyoverview
consists of four main steps, as illustrated in Figure 1. First, we
propose a taxonomy of 12 different types of data noises due to
inappropriate or insufficient data preprocessing in code summa-
rization,derivedfromobservationsontheselectedfourbenchmark
datasets.Second,webuildarule-basedcleaningtool,namedCAT
(Code-comment cleAning Tool), for automatically scanning and
detectingtheoccurrencesanddistributionofdatanoisesforagiven
dataset,basedontheproposedtaxonomy.Themanualvalidation
resultsshowthatthetoolcanaccuratelydetectnoisydata.Third,
we conduct an evaluation study to assess the data quality of the
four widely-used benchmark datasets. The results show that noisy
dataextensivelyexistinthefourbenchmarkdatasets(rangingfrom
31% to 66%). Finally, we investigate the impacts of noises on three
typical code summarization models ( i.e.NNGen [45], NCS [8], and
Rencos [78]) by comparing their performance trained on the same
datasetsbeforeandafterdatacleaning.Theabovefourstepswill
be elaborated in later sections, i.e., sec. 3 to sec. 6, respectively.
Theresultsshowthat,removingnoisydatahaveapositiveinflu-
enceonmodelsummarizationability.Trainingthreemodelswith
the filtered datasets improves the BLEU-4 by 27%, 21%, and 24%,
respectively.The majorcontributionsofthis paper are as follows.
â€¢To the best of our knowledge, it is the first to systematically
study the patterns and impact of the noises in various code
summarizationdatasets.â€¢Wedevelopanautomateddatacleaningtool,namedCAT,for
codesummarizationdatasets,whichcanhelpdistillhigh-quality
code-commentdata.
â€¢We perform a comprehensive assessment on data quality of
benchmarkdatasests,whichprovidespracticalinsightsforfu-
ture code summarizationresearch.
â€¢We conduct a comparative analysis on the performance of code
summarizationmodelstrainedontheoriginanddistilledbench-
mark datasets, our results demonstrate that removing noises
yieldssignificant modelperformance improvement.
â€¢We release CAT and the distilled benchmark datasets [ 7] to the
general public, in order to facilitate the replication of our study
andits extensive applicationinothercontexts.
Intheremainderofthepaper,Section 2illustratestheprelimi-
naries. Section 3introduces the taxonomy of noisydata. Section 4
presentsthecode-commentcleaningtool.Section 5demonstrates
the quality assessment of benchmark datasets. Section 6shows the
impact of noisy data on the performance of code summarization.
Section7discussesresultsandthreatstovalidity.Section 8surveys
the relatedwork to our study.Section 9concludes this paper.
2 PRELIMINARIES
Thissectionbrieflyintroducestheliteratureofcodesummarization,
as well as fourwidely-usedbenchmarkdatasets.
2.1 CodeSummarization
Codesummarization[ 49,62]aimsatgeneratingacommentfora
givenblockofsourcecodethatcanhelpdevelopersbetterunder-
standandmaintainsourcecode.Theessentialtaskistotranslate
thecodewritteninprogramminglanguagesintocommentswrit-
ten in naturallanguages.Meanwhile, comments maydescribe not
only the functions, but also the design intents, program logic, and
functionalities of programs behind the source code. The existing
code summarization models can be categorized into three different
types based on the techniques used, i.e., Information Retrieval (IR)
based approaches [ 19,27,74], Neural Machine Translation (NMT)
basedapproaches[ 8,10,13,15,29,36,40,67,72,73,76],andhybrid
approaches[ 31,32,41,77]that combine IR andNMTtechniques.
Specifically, IR-based code summarization models use IR tech-
niquestoextractkeywordsfromthesourcecodeandcomposethem
intoterm-basedsummarizationforagivencodesnippet[ 19,27,74].
For example, Edmund et al.[74] generated code summarization
for agiven codesnippetbyretrievingthereplicatedcodesamples
from the corpus with clone detection techniques. Recently, with
theboomingofdeeplearningtechniques,manyNMTbasedcode
summarization approaches have been proposed, which train the
108Are We Building onthe Rock? On the Importance of DataPreprocessingforCode Summarization ESEC/FSE â€™22, November14â€“18, 2022,Singapore, Singapore
neuralmodelsfromalarge-scalecode-commentcorpustoautomat-
icallygeneratesummaries[ 8,10,13,15,29,31,36,40,67,72,73,76].
Forexample,Iyer etal.[36]treatedthecodesummarizationtaskas
an end-to-endtranslationproblem andfirstintroducedNMTinto
codecommentgeneration.Thehybridapproaches[ 32,41,77,78]
leveragetheadvantagesofIRandNMTtechniquesforimproving
codesummarization. For example, Zhang et al.[78] first retrieved
topsimilarcodeinthetrainingdataforagivenpieceofcodeand
theninputthemintoanNMTmodelforsummarizationgeneration.
2.2 Benchmark Datasets
Asintroducedearlier,thisstudyconductsvariousexperimentson
fourwidely-usedcodesummarizationdatasets,including Funcom[41],
TLC[33],CSN[35],andPCSD[67].Thedataformatofthesedatasets
isprimarilyrepresentedusing code-commentpairs ,wherethe
codedataisatthegranularityof method-level .Eachdatasetap-
pliesitsownoperationswhenextractingandpreprocessingtheraw
data.Table 1summarizestheinformation ofdescriptive metadata
and associated studies where each dataset has been employed in
existing literature.
More specifically, Funcomis a collection of 2.1M code-comment
pairsfrom29Kprojects.Foreachmethod,itextracteditsJavadoc
comment and treated the first sentence in the Javadoc of each
methodasitssummary. TLChas87Kcode-commentpairscollected
from more than 9K open-source Java projects created from 2015 to
2016 with at least 20 stars. It extracted the Java methods and their
corresponding Javadoc comments. These comments are considered
ascodesummaries. CSNcontainsabout2Mmethodandcomment
pairsmined frompubliclyavailableopen-source non-forkGitHub
repositories spanning six programming languages, i.e., Go, Java,
JavaScript,PHP,Python,andRuby.Inthisstudy,weconductthe
experimentson the Javaportion ofthe CSNdataset.PCSDcontains
105K pairs of Python functions and their comments from open
sourcerepositoriesinGitHub.Specifically,itusesdocstrings( i.e.,
thestringliteralsthatappearrightafterthedefinitionoffunctions)
as summariesfor Pythonfunctions.
3 THE TAXONOMYOFNOISYDATA
An essential and effective starting point is a systematic and robust
categorization of data noises. This section presents details on how
thenoisydatataxonomyisbuilt,andthedescriptionsandexamples
for every 12 categories.
3.1 Taxonomy Construction
We employ an open cardsort [56] processby involving ninepartici-
pants.ParticipantsincludetwoPhDstudents,fourmasterstudents,
andthreeseniorresearchers.Allofthemhavedoneeitherintensive
Table 1:BenchmarkDatasetsInformation
Name Funcom TLC CSN PCSD
Year 2019 2018 2019 2017
Source Sourcerer Github Github Github
Download [4] [2] [3] [1]
Language Java Java Java Python
#Pairs 2,149,121 87,136 496,688 105,540
Train/Val/Test 9/0.5/0.5by project 8/1/1by function 8/1/1by project 6/2/2by function
Trained-on Models[23,41,42]
[29,40,73]
[12,28,61]
[26,39,47][33,72,81]
[8,61,80]
[16,22,60]
[78,79][17,35,52]
[59,60,69]
[21,46,70]
[25,43,48][22,67,75]
[8,23,72]
[22,75,80]
[16,68,78]research work with software development or have been actively
contributing to open-source projects. The sorting process is con-
ducted in multiple rounds. For each round, we randomly sample
160 code-comment pairs without replacement from the four bench-
mark datasets (40 pairs for each). In the first round, all participants
labelthesamesampleddata,withanintensivediscussionsessionto
achieve conceptual coherence about noisy categories. The average
Cohenâ€™sKappais0.86,whichindicatessubstantialagreement.Then,
asharedpoolofcategoriesisutilizedandcarefullymaintained,and
eachparticipantcouldselectexistingcategoriesfromand/oradd
newcategorynamesintothesharedpool.Thesortingprocessends
when there isno new categoryaddedfor twoconsecutiverounds.
Intotal, weconducted10roundsandlabeled1,600pairs ofsource
code and the corresponding comments (400 pairs for each of the
fourbenchmarkdatasets).Thedetailedannotationresultscanbe
foundinSection 4.2.3.
3.2 Comment-RelatedNoisy Data
PartialSentence. Sinceitisacommonpracticetoplaceamethodâ€™s
summaryatthefirstsentenceofitscomment[ 50],mostresearchers
use the first sentences of the code comments as the target sum-
maries. While, we have observed that some inappropriate process-
ingcanleadtopartialfirstsentencescollected.Forexample, Funcom
onlycollectsthefirstlinefromthefollowingjavadocasthecom-
ment,i.e., Å‚Returns the high valueÅ¾, where the next line that should
be part of the first sentence is missing. This is primarily due to
automaticsplitting using newlinecharacters such as Å‚ \nÅ¾.
/* Returns the high-value
* for an item within a series. */
Comment (Funcom): returns the high value
VerboseSentence. Whencollectingthefirstsentenceasthetarget
comment, some inappropriate processing will lead to verbose first
sentences collected. For example, PCSDexcessively includes thear-
gument description Å‚arguments course dataÅ¾ into the functionality
summary.
"""
Generate a CSV file containing a summary of the xBlock usage
Arguments:course_data
"""
Comment (PCSD): generate a csv file containing a summary of
the xblock usage arguments course data
Content Tampering. Developers may use HTML tags for doc-
umentation auto-generation or URLs for external references in
comments. We observe that some inappropriate processing will
keepthetagsorURLcontentstogetherwiththecomments,thus
contaminatingthebenchmarkdatawithmeaningless text.Forex-
ample,CSNreserves the HTML tag Å‚pÅ¾ at the beginning and end of
the comment.
/* <p> Builds the JASPIC application context.</p> */
Comment (CSN): pbuilds the jaspic application context p
Over-SplittingofVariableIdentifiers. Codecommentsarelikely
tocontainvariableidentifiersorAPItermswhendescribingcode
functionalities.SplittingcodebycamelCaseorsnake_caseisacom-
mon operation for code understanding [ 30,41,59]. However, we
109ESEC/FSE â€™22, November14â€“18, 2022,Singapore, Singapore Lin Shi,FangwenMu, XiaoChen, Song Wang,JunjieWang,Ye Yang,GeLi, XinXia,andQingWang
observethatsomestudiesperformthisoperationoneverymatched
token in the comments including the predefined variable identi-
fiers or API terms. For example, Funcomsplits a variable named
Å‚jTextFieldÅ¾intoÅ‚jtextfieldÅ¾whencollectingcomments.Wecon-
sidersuchanoperationcanchangetheoriginalmeaningofcode
comments.
/* This method initializes jTextField. */
Comment (Funcom): this method initializes j text field
Non-Literal. Developers from different countries may write com-
ments in their first languages, mixing with the English language
inthecommentssometimes.We observethat existingbenchmark
datasetsoccasionallydiscardtheNon-Englishtextbutremainthe
English text as code comments. For example, CSNonly extracts the
Englishwords, i.e.,Å‚jsonarraybeanlistarraylistÅ¾fromthefollowing
mixedcomment that contains both Chinese andEnglish wordsas
the summarization for the corresponding source code. Since the
remainingcommentdataaretypicallyincompleteandmeaningless,
we considerthemas noises.
/*å°†JSONArrayè½¬æ¢ä¸ºBeançš„Listï¼Œé»˜è®¤ä¸ºArrayList */
Comment (CSN): jsonarray bean list arraylist
Interrogation. Basedonourobservation,someofthecomments
inthebenchmarkdatasetareinterrogations.Forexample,in CSN,
the comment for the isDue()method is Å‚do we need to show the
upgradewizardpromptÅ¾.Suchinterrogationsaremainlyusedfor
communication,ratherthansummarizing functionalities.
/* Do we need to show the upgrade wizard prompt? */
public boolean isDue() {
if(isUpToDate)
return false ; ...
Comment (CSN): do we need to show the upgrade wizard prompt ?
Under-DevelopmentComments. Basedonourobservation,some
ofthecommentsarerelatedtoongoingandfuturedevelopment,in-
cludingtemporarytips,notes, etc.Forexample, TLChasacomment
Å‚description of the methodÅ¾ for the openFilemethod, which is of
littleworthforunderstandingcode.Sincetheunder-development
commentsaretypicallyinappropriateforthescenarioofautomated
code summarization, we considerthemas noises.
/* Description of the Method */
protected void openFile(File f) {
if(f ==null) { ...
Comment (TLC): description of the method
3.3 Code-RelatedNoisy Data
EmptyFunction. Developersoftentakeontechnicaldebttospeed
up software development [ 71]. It has been widely observed that
emptyfunctionisacommontypeoftechnicaldebt.However,the
code-comment pairs extracted from these empty functions can
introduce non-trivial noises, this is because an unimplemented
empty function and its comment do not match either syntactically
or semantically. For example, Funcomincludes an empty method
end()witha10-wordcomment./*Specifies the behaviour of the automaton in its end state*/
protected void end(){}
Code (Funcom): protected void end
Commented-OutMethod. Developers oftencommentouta
whole method for deprecating a specific functionality [ 20]. We ob-
servethat,inthestudiedbenchmarkdatasets,somecommented-out
methods are collected as the comments for the sequential methods.
For example,the commented-outmethod transformTypeID andits
comments are stillincludedin Funcom.
/* for now try mappig full type URI */
// public String transformTypeID(URI typeuri){
// return typeuri.toString();}
Code (Funcom): public string transform type id ...
Block-CommentCode. Wehaveobservedthatsomecodein
the benchmark datasets contains block comments inside their bod-
ies.Theblockedcommentscouldbenatural-languagecomments
or commented-out code. For example, the block comment Å‚TODO:
WhyisheusingMath.roundÅ¾isconsideredasapieceofcodefor
thegetFixQuality method in Funcom. If keeping these blocked com-
mentsinthesourcecode,theoriginallogicsofthecodearelikely
to be distorted when tokenizing it for code summarization models.
/* Get GPS Quality Data */
public int getFixQuality(){
checkRefresh();
// TODO: Why is he using Math.round?
Return Math.round(quality);}
Code (Funcom): public int get fix quality check refresh todo
why is he using math round return math round quality
AutoCode. DevelopersoftenusemodernIDEslikeEclipseor
Visual Studio to generate auxiliary functions such as getter,setter,
toString, ortesterfor some predefined variables. The comments
fortheseautogeneratedmethodsareoftensimilartoorthesame
asthemethodnames,whichmakesthecode-commentpairsless
informative. For example, in Funcom, the comment for the auto-
generatedtestmethod( i.e.,testConstructor )isÅ‚TesttheconstructorÅ¾,
whichisalmostthe same as the methodname after splitting.
/* Test the constructor */
public void testConstructor() {
System TestResult str;
System TestID testID1; ...
Comment (Funcom): test the constructor
Code (Funcom): public void test constructor ...
Duplicated Code. Developers often reuse code by copying,
pastingandmodifyingtospeedupsoftwaredevelopment[ 9,57].
These code snippets often have similar or the same comments.
Sharing identical code and summarization pairs in the training and
test sets is inappropriate and would make the model learn these
caseseasily.
4 THE CODE-COMMENTCLEANING TOOL
Tosupportautomaticdetectionofnoisesintheproposedtaxonomy,
wedevelopacode-commentcleaningtool,namedCAT,basedon
110Are We Building onthe Rock? On the Importance of DataPreprocessingforCode Summarization ESEC/FSE â€™22, November14â€“18, 2022,Singapore, Singapore
Raw 
commentResult
FS = null ;
For each line L  in 
the raw comments
Is FS empty ?
List T = splitted 
setences in L
If Num( T ) > 0
FS += T[0]FS += LIs L capitalized ?No
Yes
YesNoNo
Yes
Return  FS
Figure2:If-elserulesofcollectingthefirstsentenceinthe
raw comments forpartialandverbose sentencenoises.
a set of heuristic rules. This section introduces the design of the
rule-based cleaning tool, and presents the analysis results of its
effectiveness.
4.1 The HeuristicRules
Construction criteria. The heuristic rules conform to the fol-
lowing criteria: (1) Each rule should define a unique and specific
category without overlap; (2) Rules should limit the exclusion of
valid data within an acceptable range, i.e., all the F1 scores should
be larger than90%; and(3) Any ruleisnot asubruleofthe others.
Construction process. For each category of noisy data, we
developasetofif-elserulestodetectthembythefollowingsteps.
(1) Based on the manually annotated noises produced in Section 3,
Table 2:Syntax featuresandour actions inheuristic rules
Category Syntax Feature ActionCommentPartial SentenceShorter thanthe corrected
first sentenceUPDATE:Replacewith
the corrected
first sentence
Verbose SentenceLonger thanthe corrected
first sentenceUPDATE:Replacewith
the corrected
first sentence
ContentTamperingHTMLtags,Doctags,
and URL formatUPDATE:Cleanthe tags
from comment data
Over-SplittingSplitcommentsoncamel
caseand underscoreUPDATE:Replacethe
over-splitting variables
withthe original ones
Non-Literal non-ASCII REMOVE
Interrogation Å‚?Å¾, Å‚whatÅ¾, Å‚howÅ¾, etc. REMOVE
Under-DevelopmentÅ‚todoÅ¾, Å‚deprecateÅ¾,
Å‚copyrightÅ¾,Å‚FIXME:Å¾, etc.REMOVECodeEmpty Function Themethod body isempty REMOVE
Commented-Out MethodThewhole method
iscommented out.REMOVE
Block-Comment CodeThemethod containsthe
block comment.UPDATE:Cleanthe
blocked comments
from the codebody
Auto code setter, getter, tester, etc. REMOVE
Duplicated
CodeExact Match REMOVEwe carefully identify syntax features for eachcategory from80% of
the manual data; (2) We design a set of if-else rules to detect the
noisy data from the raw data (note that, the raw data refers to the
sourcedatathathasnotbeenprocessedforuse,andtheorigindata
refers to the processed raw data in the benchmark datasets); (3) To
avoid overfitting, we test the correctness of the rules on the rest
20% of the manually annotated noises. We iteratively adjust the
rulesuntil the performance isacceptable, i.e.,over 90%F1 score.
ExampleRules. Figure2illustratestheif-elserulesofcollecting
the first sentence in the raw comments for partial and verbose sen-
tencenoises.Thekeyideaistosequentiallydeterminewhethereach
lineofcommentinarawcommentcontainsacompletesentence.
If so, return the first complete sentence; if not, save the content
of the line and continueto determine the next line. By comparing
thefirstsentenceweextractedfromrawdatawiththeprocessed
sentence provided in the benchmark datasets, we can determine
the verbose orpartialsentence category.
Table2demonstratesthesyntaxfeatureofheuristicrulesandour
actions to resolve noises detected. Details of the implementation of
eachcategory can be foundonour website [ 7].
4.2 Effectiveness Evaluation
4.2.1 DataPreparation. AsintroducedinSection 3.1,welabeled
12categoriesofnoisydatafrom1,600code-commentpairssampled
from the four benchmark datasets. These manually annotated data
areusedtoevaluatetheperformanceofCAT.Webuildtheheuristic
rules based on observing 80% of the annotated noisy data, and
evaluateontherest20%.TheÅ‚DatasetÅ¾columninTable 3shows
the detail.
4.2.2 Evaluation Metrics. We use three commonly-used metrics to
evaluatetheperformanceofCAT, i.e.,Precision,Recall,andF1.(1)
Precisionreferstotheratioofcorrectpredictionstothetotalnumber
of predictions; (2) Recallrefers to the ratio of correct predictions to
the total number of samples in the golden test set; and (3) F1is the
harmonic mean ofprecision andrecall.
4.2.3 Results. Table3demonstrates the performance of CAT.We
can see that, it can accurately detect noises on the four benchmark
datasets. The F1 scores of detecting comment-related noises are
rangingfrom93.0%to100.0%,and95.5%onaverage.Theaverage
Table 3:Effectiveness ofnoise detection rules
CategoryDataset Performance(%)
#Anno-
tations
(100%)Rule-
Build
(80%)Rule-
Test
(20%)PRF1CommentPartial Sentence 176 135 4197.595.196.3
Verbose Sentence 129 111 1894.7100.097.3
ContentTampering 147 120 2792.996.394.6
Over-Splitting 84 632190.995.293.0
Non-Literal 38 30 8100.0100.0100.0
Interrogation 16 79100.088.994.1
Under-Development 57 925791.594.793.1
Total 647 55818195.495.895.5CodeEmptyFunction 21 14 7100.0100.0100.0
Commented-Out Method 4 22100.0100.0100.0
Block-CommentCode 44 3113100.092.396.0
AutoCode 179 133 4697.793.595.6
DuplicatedCode 22 16 6100.0100.0100.0
Total 270 196 7499.697.298.3
111ESEC/FSE â€™22, November14â€“18, 2022,Singapore, Singapore Lin Shi,FangwenMu, XiaoChen, Song Wang,JunjieWang,Ye Yang,GeLi, XinXia,andQingWang
F1scoresofdetectingcode-relatednoisesarerangingfrom95.6%
to100.0%,and98.3%onaverage.Theresultsshowthat,CATcan
achievehighlysatisfactoryperformanceonfilteringnoisydatafrom
code-comment datasets. In summary, our code-comment cleaning
toolcanaccuratelyfilternoisydata,withalltheF1scoresofover
90.0%,whichcanhelpbuildahigh-qualitydatasetforthefollow-up
code summarizationtasks.
5 QUALITYASSESSMENTOFBENCHMARKS
In this step, the code-comment cleaning tool is applied to detect
and correct noises through comment removal or update actions
as listed in Table 2. Based on the noisy data output by the tool,
we further analyze the quality of the four benchmark datasets.
Table4illustrates the distribution of each noise category on the
four benchmark datasets. The number on each cell presents the
percentageofthenoisesinthecorrespondingbenchmarkdataset,
directlygeneratedfromthecleaningtool.Notethat,sinceonecode-
comment pair may involve multiple noises, the tool repeatedly
countsthosethatinvolvemultiplenoisecategorieswhencalculating
frequencyforeachcategory,andcountsonceforthetotalfrequency.
Thus, the sum of individual category percentages is slightly higher
thanthe percentageoftotalnoises.
Overall,Funcomhasthehighestproportionofnoisydata(65.8%),
followed by TLC(41.9%),CSN(37.2%), and PCSD(31.2%). We can
alsoobservethat,thebenchmarkdatasetsoftencontainmultiple
categories of noises. Funcomcontains the most noise categories.
Except for the verbose sentence noises, every other category is
included. The other three benchmark datasets contain seven or
eightcategories.
Noise distribution in comments . It is observed that 40.9%
comments in Funcomcontain noises, followed by CSN,TLC, and
PCSD. Specifically, all the four benchmark datasets have content-
tampering, interrogation, and under-development noisy comments.
24.4% comments in CSNare contaminated by the meaningless text
such asHTMLtags,Javadoctags, orURLs. 24.1% comments in
Funcomareover-splitingbycamelCase.22.8%commentsin TLCare
verbose sentences.
Noise distribution in source code. It is also observed that
40.7%sourcecodein Funcomcontainnoises,followedby TLC,PCSD,
Table 4:Distribution ofnoisy data inbenchmarkdatasets.
Category of NoisyData Funcom(%)TLC(%)CSN(%)PCSD(%)
Total65.841.9 37.2 31.2CommentPartialSentence 17.1 0.0 7.8 15.9
VerboseSentence 0.0 22.8 0.0 7.8
Content Tampering 9.7 3.2 24.4 0.5
Over-Splitting 24.1 0.0 0.0 0.0
Non-Literal 0.5 0.0 7.8 0.2
Interrogation 0.7 0.9 0.7 0.3
Under-Development 3.7 1.2 1.2 2.3
Total 40.9 25.4 36.1 26.5CodeEmpty Function 1.6 1.1 0.0 0.0
Commented-OutMethod 0.2 0.0 0.0 0.0
Block-CommentCode 11.1 0.0 0.0 0.0
Auto Code 29.8 4.6 1.6 4.3
DuplicatedCode 0.6 18.4 0.0 1.5
Total 40.7 22.6 1.6 5.8
Removednoisy data 38.7 21.1 29.2 9.3
Updatednoisy data 27.1 20.8 8.0 21.9andCSN. Specifically, all the four benchmark datasets have auto-
code noises. In Funcom, 29.8% code is auto-generated such as setter,
getter,tester,andtoStringmethods.Indeed,previous research [ 30]
usedto complainaboutsimilarissuesthat Funcomcontainsmuch
auto-generatedcode.In TLC,18.4%codeisexactlyduplicatedwhile
the other three benchmark datasets are nearly none. This phenom-
enonindicatesthatpreprocessingoperationsappliedonexisting
benchmark datasets are not coincident all the time in that, some
benchmarkdatasetsapplythededuppreprocessingoperationwhile
somedo not.
Distribution of updates and removals. The bottom part of
Table4showsthefrequencyofdifferenttypesofnoisethatwere
removed or updated from the four benchmark datasets based on
the corrective actions introduced in Table 2.FuncomandTLChave
highproportionsofbothremovalsandupdates, i.e.,38.7%and27.1%
noisy data in Funcomare removed and updated respectively. The
majorcorrectionfor CSNisremovals.Whilethemajorcorrection
forPCSDis updating noises. This might be causedby thedifferent
noisedistributionsinthesetwobenchmarkdatasets.
Finding 1 : Noisy data extensively exist in the four widely-
usedbenchmarkdatasets,rangingfrom 31.2%to65.8%. 29.8%
ofthecodein Funcomisauto-generated;22.8%commentsin
TLCare verbose first sentences; 24.4% comments in CSNare
contaminatedbythemeaninglesstext;and15.9%comments
inPCSDare the partialfirstsentences.
6IMPACTS ON THE PERFORMANCE OF CODE
SUMMARIZATION
In this section, we investigate the impact of noisy data on the
performance of code summarization. Specifically, we choose three
state-of-the-artcodesummarizationmodelsandtrainthesemodels
on three versions ( i.e., original, controlled, and filtered) of each
benchmark dataset. Thus, we have 3 (the number of models) Ã—4
(thenumberofbenchmarkdatasets) Ã—3(thenumberofversionsper
benchmarkdataset)=36experimentalmodelsintotal.Weevaluate
theperformanceofallthemodelsbasedoncommonly-usedmetrics
for code summarizationtasks.
6.1 ExperimentalDesign
6.1.1 Data Preparation. We use three versions of each benchmark
dataset as training sets, as shown in Table 5. The Å‚TotalÅ¾ rows
illustrate the overall data before and after being distilled by our
tool. The Å‚ExperimentalÅ¾ rows show the data that are used for our
experiments. The Å‚Origin" refers to the original training dataset
splitbythebenchmarkdataset.TheÅ‚Filtered"referstothetrain/test
Table 5: Total and experimental datasets for impact analysis
.Funcom TLC CSN PCSD
TotalOrigin 2,149,121 87,136496,688 105,540
Filtered 1,316,532 68,743351,394 95,793
ExperimentalTrainOrigin 1,937,136 69,708454,451 63,324
Controlled 1,184,438 53,597323,226 57,849
Filtered 1,184,438 53,597323,226 57,849
TestFiltered 69,392 7,58419,319 19,028
112Are We Building onthe Rock? On the Importance of DataPreprocessingforCode Summarization ESEC/FSE â€™22, November14â€“18, 2022,Singapore, Singapore
Table 6:Performanceofexisting models trained overdifferentdatasets
Benchmark Model Train set Training Hours BLEU-1 BLEU-2 BLEU-3 BLEU-4 ROUGE METEOR CIDEr
FuncomNNGenOrigin 5h 23.87 14.28 11.4 10.05 26.88 12.59 1.26
Controlled 5h 21.93 12.19 9.34 8.09 24.84 11.39 1.05
Filtered 5h 24.583.0%â†‘15.266.9%â†‘12.499.6%â†‘11.210.3%â†‘27.080.7%â†‘13.245.2%â†‘1.389.5%â†‘
NCSOrigin 20h 29.95 17.79 10.2 6.42 34.84 16.14 1.38
Controlled 20h 29.33 17.06 9.78 5.31 34.05 15.65 1.42
Filtered 20h 30.531.9%â†‘18.795.6%â†‘11.4712.5%â†‘7.6416.0%â†‘35.421.7%â†‘16.321.1%â†‘1.465.8%â†‘
RencosOrigin 9h 27.23 15.97 9.62 6.43 31.97 14.32 1.25
Controlled 9h 26.90 15.71 9.48 6.42 31.79 14.16 1.23
Filtered 9h 27.922.5%â†‘16.85.2%â†‘10.6110.3%â†‘7.4413.6%â†‘32.511.7%â†‘14.501.3%â†‘1.314.8%â†‘
TLCNNGenOrigin <1h 32.58 24.16 21.92 20.74 36.07 18.14 2.01
Controlled <1h 39.84 32.01 29.24 27.51 43.57 23.22 2.64
Filtered <1h 46.8843.9%â†‘39.2762.5%â†‘36.8167.9%â†‘35.1941.1%â†‘49.0836.1%â†‘25.5340.7%â†‘3.6280.5%â†‘
NCSOrigin 6h 42.09 32.95 29.09 27.09 46.30 24.18 2.65
Controlled 6h 39.28 29.61 25.83 23.89 43.49 22.11 2.37
Filtered 6h 46.5210.5%â†‘37.1912.9%â†‘33.4114.9%â†‘31.3813.7%â†‘49.406.7%â†‘24.672.0%â†‘3.3024.5%â†‘
RencosOrigin 6h 43.66 34.82 31.29 29.19 47.87 24.95 2.81
Controlled 6h 43.71 34.89 31.21 28.93 47.85 25.37 2.84
Filtered 6h 51.5418.0%â†‘42.9023.2%â†‘39.2225.3%â†‘37.0021.1%â†‘54.2513.3%â†‘28.2113.1%â†‘3.8838.1%â†‘
CSNNNGenOrigin <1h 14.86 6.08 4.07 3.42 18.04 8.54 0.40
Controlled <1h 13.95 5.09 3.21 2.62 17.08 7.97 0.34
Filtered <1h 19.8933.8%â†‘8.2836.2%â†‘5.7240.5%â†‘4.9631.0%â†‘23.1728.4%â†‘9.6713.2%â†‘0.6562.5%â†‘
NCSOrigin 15h 25.47 12.34 5.81 3.02 30.47 12.48 0.80
Controlled 15h 25.45 12.29 5.68 2.88 31.17 12.30 0.82
Filtered 15h 28.6812.6%â†‘14.0113.5%â†‘6.9619.8%â†‘3.8722.0%â†‘34.2912.5%â†‘13.8410.9%â†‘0.9518.8%â†‘
RencosOrigin 11h 16.99 7.65 4.09 2.64 20.91 8.33 0.49
Controlled 11h 16.30 7.09 3.75 2.43 20.00 8.13 0.44
Filtered 11h 24.7245.5%â†‘11.3648.5%â†‘6.5159.2%â†‘4.5642.1%â†‘29.3540.4%â†‘11.5238.3%â†‘0.8267.3%â†‘
PCSDNNGenOrigin <1h 22.52 15.48 12.63 10.45 24.90 12.97 1.24
Controlled <1h 21.81 14.77 11.99 9.91 24.16 12.49 1.18
Filtered <1h 25.9615.3%â†‘18.9122.2%â†‘16.2728.8%â†‘14.0025.4%â†‘27.6811.2%â†‘15.0916.3%â†‘1.6331.9%â†‘
NCSOrigin 6h 28.14 18.69 14.28 11.36 32.95 16.30 1.61
Controlled 6h 26.85 17.42 13.05 10.17 31.77 15.42 1.49
Filtered 6h 37.3332.7%â†‘24.7432.4%â†‘19.4936.5%â†‘16.4831.1%â†‘40.9324.2%â†‘18.6714.5%â†‘2.0728.6%â†‘
RencosOrigin 5h 30.37 21.27 16.42 12.93 33.66 17.40 1.65
Controlled 5h 29.73 20.55 15.71 12.37 33.05 16.96 1.59
Filtered 5h 33.5910.6%â†‘24.1413.5%â†‘19.6319.5%â†‘16.1019.7%â†‘36.157.4%â†‘19.1810.2%â†‘2.0222.4%â†‘
datasetcleanedbyourtoolfromtheÅ‚Origin"train/testdatasets.To
benchmarktheperformancevariationbroughtbythesizeshrinking,
wefurtherbuildtheÅ‚Controlled"setbyrandomlysamplingfrom
the Å‚Origin" set, which has an equal amount of data instances as
the Å‚Filtered" dataset.
6.1.2 CodeSummarizationModels. AsintroducedinSection 2.1,
existing code summarization models can be divided into three cat-
egories:InformationRetrieval(IR)basedapproaches,NeuralMa-
chineTranslation(NMT)basedapproaches,andhybridapproaches
thatcombineIRandNMTtechniques.Weselectonestate-of-the-art
method from each category to explore the impact of noisy data on
modelperformance.
NNGen[ 45]is anIR-based model for generating commitmes-
sages by utilizing the nearest neighbors algorithm. It first embeds
code changes intovectors based on the bag of words and the term
frequency. Then, NNGen retrieves the nearest neighbors of code
changes by calculating the cosine similarity of vectors and the
BLEU-4score.Finally,itdirectlychoosesthemessageofthecode
changewiththe highestBLEUscore as the final result.
NCS [8]is an NMT-based model which replaces the previous
RNNunitswiththemoreadvancedTransformer[ 65]model.NCS
extends the vanilla Transformer in two aspects. Firstly, it incor-
poratesthecopyingmechanism[ 58]intheTransformertoallow
bothgeneratingwordsfromvocabularyandcopyingfromtheinput
source code. Secondly, NCS utilizes relative positional embeddingrather than absolute positional embedding to capture the semantic
representation ofthe code better.
Rencos [ 78]is a state-of-the-art model that combines the ad-
vantages of both IR-based and NMT-based techniques. Specifically,
given an input code snippet, Rencos first retrieves its two most
similar code snippets in the training set from the aspects of syntax
and semantics, respectively. Then it encodes the input and two re-
trievedcode-snippets, and generates the summaryby fusing them
duringdecoding.
6.1.3 ExperimentalSettings. Theexperimentalenvironmentisa
desktop computer equipped with an NVIDIA GeForce RTX 3060
GPU,IntelCorei5CPU,12GBRAM,runningonUbuntuOS.When
training the three codesummarizationmodelswiththe benchmark
datasets,wefollowtheimplementationprovidedintheiroriginal
papers,andadopttherecommendedhyperparametersettings,ex-
cept for the training epoch of NCS and Rencos. To save training
time and computation resources, we set max_epoch =50 for NCS
andmax_iteration =100kfor Rencos.
6.1.4 EvaluationMetrics. Weevaluatetheperformanceofthethree
models using four metrics including BLEU [ 51], METEOR [ 11],
ROUGE-L [ 44], and CIDEr [ 66].BLEUmeasures the ð‘›-gram pre-
cision by computing the overlap ratios of ð‘›-grams and applying
brevity penalty on short translation hypotheses. BLEU-1/2/3/4 cor-
respond to the scores ofunigram,2-grams,3-grams,and 4-grams,
respectively. ROUGE-L isdefinedasthelengthofthelongestcom-
monsubsequencebetweengeneratedsentenceandreference,and
113ESEC/FSE â€™22, November14â€“18, 2022,Singapore, Singapore Lin Shi,FangwenMu, XiaoChen, Song Wang,JunjieWang,Ye Yang,GeLi, XinXia,andQingWang
basedonrecallscores. METEOR isbasedontheharmonicmeanof
unigram precision and recall, with recall weighted higher than pre-
cision.CIDErconsiders the frequency of ð‘›-grams in the reference
sentences by computing the TF-IDF weighting for each ð‘›-gram.
ð¶ð¼ð·ð¸ð‘Ÿð‘›score for ð‘›-gram is computed using the average cosine
similaritybetween the candidate sentence andthe reference.
6.2 QuantitativeResults
Table6shows the performance of the three models trained over
different experimental datasets. Overall, removing noisy data from
the training set in the four benchmark datasets produces a positive
effect on improving the performance of the three models. Training
threeexistingmodelswiththefilteredbenchmarkdatasetsimproves
theBLEU-4by26.9%,20.7%,and24.1%,ROUGEby19.1%,11.3%,and
15.7%,METEORby18.9%,7.1%,and15.7%,CIDErby46.1%,19.4%,
and33.2%, respectively.
Amongthefourbenchmarkdatasets,theeffectonthe CSNdataset
is the most significant, which leads to the three models (NNGen,
NCS,andRencos)increasingby31.0%,22.0%and42.1%onBLEU-
4, 28.4%, 12.5% and 40.4% on ROUGE, 13.2%, 10.9%, and 38.3% on
METEOR, and 62.5%, 18.8%, and 67.3% on CIDEr, respectively. This
is followed by TLCandPCSD. Considering the fact that even the
least effect obtained in Funcomstill contributes to an increase of
10.3%, 16.0% and 13.6% on BLEU-4, 0.7%, 1.7% and 1.7% on ROUGE,
5.2%, 1.1%, and 1.3% on METEOR, and 9.5%, 5.8%, and 4.8% on
CIDEr,respectively.Themainreasonthatthethreemodelsexert
thebiggestperformancedifferenceonCSNisthat,theprimarynoisy
dataonCSNarecontenttamperingbyHTMLtags,andremoving
these noises will make the generated comments more accurate. We
willillustratethisinthefollowingqualitativeanalysis.Weargue
that the existing models used for code summarization tasks in the
literature have a significant scope of improvement given a large,
good-qualitydataset.
By observing the performance of the three models trained on
different filtered datasets,we find that therelative rankingamong
the three types of models is not consistent. For the filtered TLC,
Rencosachievesthebestperformanceonallmetricscomparedto
theothertwomodels.WhileNCSperformsbestwhentrainedon
filteredFuncom,CSN, andPCSD. This result implies that, to more
comprehensively evaluate different code summarization models, it
isbettertousemultipledatasets,astherankingofthemodelcan
be inconsistentondifferentdatasets.
Finding2 :Removingnoisydatafromthetrainingsetinthe
fourbenchmarkdatasetshasapositiveinfluenceontheper-
formanceofthemodels.Trainingthreeexistingmodelswith
thefilteredbenchmarkdatasetsimprovestheBLEU-4by26.9%,
20.7%, and24.1%, respectively.
6.3 QualitativeAnalysis
To qualitatively illustrate the impact of the noises on code summa-
rizationmodels,wepresenttwocasesgeneratedbythethreemodels
trained on different datasets, as shown in Figure 3.Overall, the
commentsgeneratedbythemodelstrainedonthedistilleddatasets
tend to be more accurate and more readable than the comments
generatedbythe models trainedonthe origindatasets.
Generated by models trained on original dataset (BLEU-4=53.32) : 
NNGen: returns the value for the cell at code column index code and
NCS: returns the value for the cell at code column index code and
Rencos : returns the value for the cell at code column index code andpublic boolean isEnumberatedTagValueReferenceAttribute (
String nodeName ,String attributeName ){
boolean isEnumberatedTagValueReferenceAttribute =false;
if(nodeName !=null&& !nodeName .equals("")
&&attributeName !=null 
&& !attributeName .equals("")){
isEnumberatedTagValueReferenceAttribute =
refAttributeToEnumeratedTag .containsKey (
nodeName +separator +attributeName );
}
returnisEnumberatedTagValueReferenceAttribute ;}
Human-written Comment: returns the value for the cell at columnind exand rowindex
Generated by models trained on distilled dataset (BLEU-4=100.00, Inc.= 87.5%) : 
NNGen: returns the value for the cell at columnindex and rowindex
NCS: returns the value for the cell at columnindex and rowindex
Rencos: returns the value for the cell at columnindex and rowindexCaused by unromoved html 
tag <code>
Caused by over-splitting
Caused by incorrect first 
sentence(a) Case1:Anexampleofgeneratedcommentsthatcontainsover-splittingvariable,
HTMLtags,and unfinished sentence
public boolean validateBPELVariableName_Pattern (
String bpelVariableName ,
DiagnosticChain diagnostics ,
Map<Object,Object>context){
boolean result =ExecutablePackage .Literals .BPEL_VARIABLE_NAME ,
bpelVariableName ,
BPEL_VARIABLE_NAME__PATTERN__VALUES ,
diagnostics ,context;
returnresult;}
Generated by models trained on original dataset (BLEU-4=52.54) : 
NNGen: validates the pattern constraint of embpel variable name em
NCS : validates the pattern constraint of embpel variable name em
Rencos: validates the pattern constraint of embpel variable name emHuman-written Comment: validates the pattern constraint of bpel v ariable name
Generated by models trained on distilled dataset (BLEU-4=100.00, Inc.= 90.3%) : 
NNGen: validates the pattern constraint of bpel variable name
NCS: validates the pattern constraint of bpel variable name
Rencos: validates the pattern constraint of bpel variable nameCaused by 
unromoved
tag < em>
(b) Case 2:Anexample of generated commentsthat contains HTMLtags
Figure 3: Inaccurate comment generation affected by noises
Case1.Giventhecode,thecommentsgeneratedbythethree
modelstrainedonoriginbenchmarkdatasetsareÅ‚returnsthevalue
for the cell at code column index code andÅ¾. Compared with the
human-writtencomment,weconsiderthefollowingthreeerrors
arelikelyrelatedtonoisydata:(1)Theover-splittingÅ‚columnindexÅ¾
as Å‚column indexÅ¾. This error is likely to be caused by the over-
splitting of variable identifiers in the comment; (2) The redundant
Å‚codeÅ¾aroundÅ‚ columnindex Å¾.Itismainlyduetothefactthatthe
origindatasetscontainmanyunremovedHTMLtags,therebyin-
creasing the probability of HTML tags, e.g., Å‚<code>Å¾, appearing in
thecontext,makingiteasierforthemodeltogeneratetheseHTML
wordswhenencounteringsomespecificpatterns;(3)Theredundant
Å‚andÅ¾ in the end. This is mainly because the widely-existing partial
orverbosesentencesintrainingsetswouldhindertheabilityofthe
modeltolearntodeterminewhenthegenerationprocessshould
end. After being retrained on the distilled data, the three models
canaccuratelygeneratethecomments,wheretheBLEU-4hasan
87.5% increase,from 53.32 to 100.00.
Case2.Giventhecode,thecommentsgeneratedbythethree
models trained on origin benchmark datasets have two redundant
Å‚emÅ¾,whicharecausedbytheunremovedHTMLtagÅ‚<em>Å¾thatis
usedtodefineemphasizedtext.Afterbeingretrainedonthedistilled
data,thethreemodelscanaccuratelygeneratethecomments,where
the BLEU-4has a90.3% increase,from 52.54 to 100.00.
114Are We Building onthe Rock? On the Importance of DataPreprocessingforCode Summarization ESEC/FSE â€™22, November14â€“18, 2022,Singapore, Singapore
7 DISCUSSION
In this section, we discuss several interesting implications that are
derived fromthe results ofthis study,aiming to facilitate the code
summarizationresearchandthe SE community.
7.1 ImpactofNoises on CodeSummarization
DatasetsandModels
7.1.1 Impact of Noises on Datasets. (1) Removing the noises in
Funcomleadstoaslightimprovementinmodelperformance
(i.e.,theBLEU-1scoreincreases2.46%onaverage).Itmightbebe-
causethatFuncomistheonewiththemostauto-generatedcode,
andautocodeoffersÅ‚easygainÅ¾inperformancethatisnotavail-
able anymore. Therefore, thebaselineperformancecould actually
decreaseif removingthemfrom testset,thusmaking theimprove-
ments of other rules look smaller in comparison. (2) Removing
the noises in TLC, CSN, and PCSD leads to a considerable
improvement in performance (i.e.,the BLEU-1 score increases
24.1%,30.6%,and19.5%onaveragerespectively.Itmightbebecause
the major noises of these three datasets are â€˜Verbose Sentenceâ€™,
â€˜Content Tamperingâ€™, and â€˜Partial Sentenceâ€™, respectively, and re-
movingthemwillbenefitthe models.
7.1.2 ImpactofNoisesonModels. Basedontheresultsshownin
Table4andTable6,wecanobservethatthenoisesaffectcodesum-
marization models differently. (1) Removing â€˜Verbose Sentenceâ€™
noises might largely benefit the IR-based model NNGen. The
major noises in TLC are â€˜Verbose Sentenceâ€™, and removing them
leads to the performance of NNGen increases 53.25% on average
of all the seven metrics, followed by Rencos (21.74%) and NCS
(12.17%). It might be because the NNGen model directly outputs
theretrievedresults.Takingtheretrievedverbosecommentsasthe
output leads to a substantial decline in the scores of the evaluation
metrics, since these N-gram based metrics are less beneficial for
longercomments. (2)Removingâ€˜ContentTamperingâ€™noises
might largely benefit the hybrid model Rencos. The major
noisesin CSNare â€˜ContentTamperingâ€™,and removingthem leads
to the performance of Rencos increasing 48.75% on average, fol-
lowedbyNNGen(35.11%)andNCS(15.73%).Itmightbebecause
the hybrid model Rencos employs a more complex input including
thetestcodeandtheretrieveddata.Suchmodelsâ€™effectivetraining
typicallyrequirestheground-truthcommentstobesemantically
correct.However,theâ€˜ContentTamperingâ€™noisescausetheground-
truth comments to mingle with irrelevant text such as HTML tags
or URLs, which alter the semantics of the ground-truth comments.
Therefore, when removing the â€˜Content Tamperingâ€™ noises, the
hybridmodelRencosincreasesthemost. (3)Removingâ€˜Partial
Sentenceâ€™ noises might largely benefit the NMT-based model
NCS.Themajor noises in PCSDare â€˜Partial Sentenceâ€™, and remov-
ing them leads to the performance of NCS increasing 28.56% on
average, followed by NNGen (21.58%) and Rencos (14.77%). The
mainreasonisthatthecommentsbelongingtotheâ€˜PartialSentenceâ€™
noisearenotcompletesentences,andthuslackintegrityforboth
syntacticandsemantic,whichhinderslanguagemodelslikeNCS
fromlearningthesyntacticandsemanticinformationcorrectly.We
willfurther illustrate the impact ofnoisesinqualitative cases.7.2 LessonsLearnedofData Preprocessing for
CodeSummarization
Data quality has been a growing concern, especially since deep
learning (DL) is widely applied for massive SE tasks. As DL models
typicallyrequirehigh-volumedata,ensuringdataqualityatalarge
scale has become a compelling need. Most existing studies focus
onadvancesinmodelingbuttypicallyovershadowthedataquality.
When investigating the current code summarization models (as
shown in Table 1), we notice that a substantial amount of data pre-
processingoperationsare cursoryor lackconsistency.Inaddition,
thereisalackofprinciplesormethodstoguideandreinforcethe
datapreprocessingassociatedeffortwhileconductingcodesumma-
rization research, regarding how to soundly preprocess benchmark
datasetsfordifferenttasks.Forinstance,ourresultsshowthatnoisy
data extensively exist in the four widely-used benchmark datasets
for code summarization tasks. Shi et al.[60] reported that different
codepreprocessing operationscanaffectthe overallperformance
ofcodesummarizationmodelsbyanoticeablemargin.Therefore,
paying more attention to data quality while training code sum-
marization models is recommended, rather than directly reusing
existing datasets withoutquality inspection.
Specifically, the study and its results lead to the identification
of the following lessons learned from the quality assessment on
benchmarkdatasets,for future code summarizationresearchers.
â€¢When reusing existing datasets, check the quality of pro-
cesseddata bycomparing withtheir raw data.
â€¢Extracting the firstsentences of comments iserror-prone.
â€¢Avoid including under-development or obsolete code, e.g.,
TODOs, commented-outmethods.
â€¢Avoid over-splittingof variable identifiers incomments.
â€¢Be careful of Å‚what to commentÅ¾, check whether the fol-
lowing types of code-comment data are suitable for your
scenarios: interrogative comments, auto code, duplicated
code,andblock-comment code.
â€¢Remembertodealwithabnormality, e.g.,HTMLtags,URL,
code path,andnon-literal naturallanguages.
7.3 ToolSupport andPotential Applications
We releasetheimplementationofthe CATcode-commentcleaning
tool as a third-party Python library [ 6], which can be easily inte-
grated into the development pipeline of most code summarization
models.ThefeaturesofCATare:(1) ConfigurableandExtend-
ableRules .TherulesetinCATisconfigurable,whichallowsusers
tocustomizetheexistingrulesbasedonthedifferentdatacharac-
teristics. Besides, CAT provides interfaces enabling users to design
new rules or clean functions to extend the feature of CAT. (2) Sup-
portforMultipleProgrammingLanguage .CATisatoolthat
cansupportmultipleprogramminglanguagessuchasJava,Python,
andC#.Similarly,forapplyingCATtootherprogramminglanguage
datasets,userscanreplacetheexistinglanguage-specificruleswith
thenewrules.Wealsoreleasethedistilledfourbenchmarkdatasets
onourwebsite[ 7]tofacilitatefuturecodesummarizationresearch.
This study applies the CAT tool for exploring the data quality
issues for code summarization exclusively. Similar to code sum-
marization, there exist other tasks on the intersection of Natural
LanguageProcessingandSoftwareEngineering,suchascommit
115ESEC/FSE â€™22, November14â€“18, 2022,Singapore, Singapore Lin Shi,FangwenMu, XiaoChen, Song Wang,JunjieWang,Ye Yang,GeLi, XinXia,andQingWang
message generation [ 37] (generates a natural language summariza-
tionfor eachsubmitted code change), codesearch [ 24] (generates
API usage sequences for a given natural language query), and code
synthesis[ 72](synthesizescodebasedonnaturallanguageintents).
These tasks also require datasets that contain a large number of
code and natural language description pairs to train their mod-
els, the quality oftheirdatasets can have a critical impact onthe
performanceofmodels.Thus,werecommendfutureresearchon
thesetopicsshouldalsoapplyCATtoremovepotentialdataprepro-
cessingnoises.Inaddition,webelieveCATcanalsofacilitatethe
downstreamtaskswithbuildinglargepre-trainedcodemodels[ 38]
to learn code representations, which require a large corpus of code.
CATcan helpremove code noisesas listedinTable 2.
7.4 Implications forResearch andPractice
Need for collaborative community effort on principles of
datapreprocessing. Improvingdataqualitytakescollaborative,
community effort to establish and maintain principles, methods,
andtoolstogovernthedataextractionandpreprocessingpipelines.
Thisstudytakesaninitialsteptowardsaddressingthechallengesof
building high-quality datasets forcode summarization exclusively.
Althoughwehaveproposedqualitycriteriatomeasuredataquality,
methodstohelpcollectdata,andfilterstoremovepreprocessing
noises, our solutions might be not sufficient for other research
tasks in software engineering or data science areas that require
massive data as inputs because of the diversity of data sources, the
complexityofdifferentdatastructures,andthescaleofdatavolume.
Thus, we urge for collaboration and effort from the whole research
communitytohelpbuildacomprehensiveandreliableprincipleset
fordatacollection,preprocessing,andqualityassessment,which
we believe can benefitour researchcommunity.
Needforresearchoncomprehensivenoisycode-comment
detection. In this study, we define 12 categories of noisy data
in code summarization datasets, and apply our cleaning tool to
filterthemout.Forthefiltereddata,weobservedseveraladditional
qualityissuesthatrequireadeeperunderstandingofthecontent
of the comments and the corresponding code. (1) Inconsistent
code-comment pairs. The following example shows a spotted
inconsistency between the comment and its code. The comment
explicitly states the return fact, but the code does not. Since only a
fewapproachesare proposed
/* Read information object and return pointer */
public void readInformationObject(...){
try{objectDecoder.checkResolved(infoObj);
}catch(finalException e) {
LogWriter.writeLog( "Exception: " + e.getMessage());
...
Comment (TLC): read information object and return pointer
Code (TLC): public voidread information object...
for detecting inconsistent Java code-comment pairs [ 18,53], and
there is a lack of inconsistent code-comment detection for other
programming languages, such as Python and C#, it is quite chal-
lenging to assess and clean such noises in a parallel corpus. (2)
Low-readabilitycomments . We notedthat somecomments are
notfluencyorhavesyntacticerrors.E.g.,acommentinthePCSDdataset Å‚transforms a doc in content in one document in presen-
tationÅ¾. If trained on datasets with such comments, the end-to-
endcodesummarizationmodelsare alsoatrisk ofproducinglow-
readabilitycomments. (3)Less-informativecomments .Wefound
that many methodsin benchmark datasets do not need comments
as methods are self-explainable with their names. For example,
two methods in Funcom dataset are named "renderImgTag" and
"createTextPane",andtheircorrespondingcommentsare"render
img tag" or "create the text pane". Since such comments are highly
similar to the method names, they can hardly convey additional
information for better understanding of the source code. If trained
ondatasetswithsuchcomments,thecodesummarizationmodels
are also likely to produce less-informative comments. Therefore,
thereisaneedforresearchoncomprehensivenoisycode-comment
detection, which can further benefit the quality improvement of
code-commentdatasets.
Integrating tool support to aid publication peer review.
Following the open science policies [ 5], most existing research
makestheirrawandtransformeddatapubliclyaccessibleduringthe
peerreviewprocess.Forthosedatasetsthatareintheformatofcode-
commentpairs,theCATcleaningtoolcanbeusedtoautomatically
detect their noise data in a reasonable time. The output statistic
couldobjectivelyreflecttheinsidequalityoftheopendatasetsto
some extent, thus can help professional peer reviewers to infer the
qualityandreliabilityofthe under-reviewresearch.
7.5 Threatsto Validity
Onethreattovalidityrelatestotherandomsamplingprocess.Sam-
pling may lead to incomplete results, e.g., noise taxonomy, we plan
toenlargetheanalyzeddatasetandinspectwhethernewtypesof
noisesareemerging.Moreover,ourheuristicrulesfordatacleaning
are elaborated from the four popular benchmark datasets, cover-
ing Java and Python. Although we generally believe all similar
code-comment datasets may benefit from our cleaning tool, future
studies are needed to focus on datasets with other programming
languages.
Thesecondthreatmightcomefromtheprocessofmanualan-
notation and card sorting. We understand that such a process is
subject to introducing mistakes. To reduce that threat, we establish
alabelingteam,andperformmultipleroundsoflabelingtomake
surethatallparticipantsachieveconceptualcoherenceaboutnoisy
categories.
The third threatrelates to the BLEU that is used to evaluate the
performance of code summarization models. Recent researchers
haveraisedconcernovertheuseofBLEU[ 55],warningthecom-
munitythatthewayBLEUisusedandinterpretedcangreatlyaffect
itsreliability.To mitigatethatthreat,wealsoadoptothermetrics,
i.e.,ROUGE,METEOR, andCIDEr,when evaluating performance.
Another threat to validity is the replication of each model. To
ensurethattheexperimentalresultsareconsistentwiththeirpapers,
weretrainthemodelsusingthesourcecodeprovidedbytheauthors,
andreusetheparametersprovidedbytheauthors.Ourexperiments
showthat the performance of ourretrainedmodels iscomparable
to the performance of models reported in the papers. For example,
the METEOR score of Rencos is 21.1 on PCSDin their paper, and
our retrainedRencosmodelis20.2.
116Are We Building onthe Rock? On the Importance of DataPreprocessingforCode Summarization ESEC/FSE â€™22, November14â€“18, 2022,Singapore, Singapore
8 RELATED WORK
Recently,moreandmoreresearchershaverealizedthatthereare
some underlying threats to the validity of existing code summa-
rization research. These empirical studiesmainly focused ondata,
evaluation metrics, andmodeleffectiveness.
Biases in data. Existing research of data biases in the code
summarizationrelatedtasksmainlyfocusedondataquality,data
representativeness,codepreprocessing,anddataselection.Sun et
al.[64] applied syntacticandsemanticquery cleaningto improve
the data quality for code search tasks. Their experiment results
show that, training the popular code-search model with the fil-
tered dataset improves its performance significantly. Gro et al.[23]
examined the underlying assumption about data representative-
nessthat:thetaskofgeneratingcommentssufficientlyresembles
thetaskoftranslatingbetweennaturallanguages,andsosimilar
modelsandevaluationmetricscouldbeused.Bycomparingfour
code-commentdatasets, i.e.,CodeNN,DeepCom,FunCom,andDoc-
String,withastandardnaturallanguagetranslatordatasetWMT19,
theyreportedthatcommentsarefarmoresaturatedwithrepeating
trigramsthanEnglishtranslationdatasets,andtherepetitiveness
has a very strong effect on measured performance. Shi et al.[60]
analyzedtheinfluenceofcodepreprocessingoperationsanddataset
sizeoncodesummarizationmodelperformance.Theyfoundthat
differentcodepreprocessingoperationscanaffecttheoverallper-
formance by a noticeable margin, and the code summarization
approaches perform inconsistently on different datasets. Huang et
al.[34] reported the biases in data selection that, not all code is
necessarilycommented.They analyzed136well-known projects
in GitHub, and reported that only a small part (4.4%) of methods
have header comments in real software projects. Theyproposed a
machine learning technique to automatically identify commenting
necessity,basedonthestructuralfeatures,syntacticfeatures,and
textual features of code. There is a lack of in-depth analysis of the
benchmark datasets. Our study bridges that gapwith a large-scale
analysis of data preprocessing errors and low-quality comments in
the benchmark datasets, and investigates performance variation of
existing models onthe distilleddataset.
Biases in evaluation metrics. Royet al.[55] conducted an
empirical study with 226 human annotators to assess the degree to
which automatic metrics reflect human evaluation for code sum-
marization tasks. Their results indicated that metric improvements
of less than 2 points do not guarantee systematic improvements
insummarizationquality,andareunreliableasproxiesofhuman
evaluation.Gro etal.[23]measured5,000code-commentpairs,and
found that the variants of BLEU chosen can cause substantial vari-
ation in the measured performance. Shi et al.[60] also examined
the BLEU variants. They concluded that BLEU variants used in
prior work on code summarization are different from each other
and the differences can carry some risks such as the validity of
theirclaimedresults. Mahmud et al.[47] observedthat someauto-
generated comments provide a semantic meaning similar to the
ground truth, despite exhibiting fewer n-gram matches. Therefore,
theyarguedthefeasibilityofn-grammetricssuchasBLEU.Mostof
theseworkfocusonvalidatingtheevaluationprocedureforcode
summarization, while our work targets to validate the benchmarkdatasets,whichwouldbeimportantandvaluableforbuildingsound
code summarizationmodels.
Analysis on Model Effectiveness. Mahmud et al.[47] com-
pared three recently proposed code summarization models, and
performed a manual open-coding of the most common errors com-
mitted by the models. They reported that missing information and
incorrect constructionare the mostprevalent error types. Chen et
al.[14]classifiedcodecommentsintosixcategories(Å‚whatÅ¾,Å‚whyÅ¾,
Å‚how-to-useÅ¾, Å‚how-it-is-doneÅ¾, Å‚propertyÅ¾, and Å‚othersÅ¾) accord-
ingtotheintention,andconductedanexperimenttoperformsix
code summarizationapproaches on them toexplore the impactof
comment categories on code summarization. They reported that
no models perform the best for Å‚whyÅ¾ and Å‚propertyÅ¾ comments
among the six categories. Most of the previous work focused on
assessing the model effectiveness in terms of error types, comment
intentions, preprocessing operations, and dataset size, while our
work aims to investigate the model effectiveness on difficult levels
ofthecodesummarizationtask,complementingtheexistingstudies.
In addition, we report data preprocessing errors and low-quality
comments in the code-comment dataset, which could provide a
sounder foundationfor existing work.
9 CONCLUSION
Weproposeataxonomyofdatapreprocessingnoisesinfourpop-
ularly used benchmark datasets for code summarization, which
contains12differenttypesofnoise.Wefurtherbuildarule-based
cleaningtoolfordetectingnoisydataofeachcategory.Experiments
show that, the tool can accurately detect noises in our manually
annotated data. We then apply the cleaning tool to the four bench-
mark datasets, and assess their data quality. The results show that
noisy data extensively exist in the four widely-used benchmark
datasets (ranging from31% to66%). Finally, weinvestigate the im-
pactsofnoisydataonthreetypesofcodesummarizationmodels
(i.e., NNGen, NCS, and Rencos) by comparing their performance
trained with datasets before and after the cleaning. The results
showthattheperformanceofthreeexistingmodelstrainedwith
the filtered benchmark datasets improves BLEU-4 by 27%, 21%, and
24%, ROUGE by 19%, 11%, and 16%, METEOR by 19%, 7%, and 16%,
CIDEr by 46%, 19%, and 33%, respectively. We release our tool as a
pythonlibrary,namedCAT,tofacilitaterelevantresearchinboth
academiaandindustry.
Inourfuturework,weplantoextendourresearchmethodology
to other text generation tasks in software engineering such as
commit messagegenerationandcode synthesis.
ACKNOWLEDGMENTS
Wesincerelyappreciateanonymousreviewersfortheirconstruc-
tive and insightful suggestions for improving this manuscript. This
work is supported by the National Key Research and Development
ProgramofChinaunderGrantNo.2018YFB1403400,theNational
Science Foundation of ChinaunderGrant No.61802374,62002348,
62072442, 614220920020 and Youth Innovation Promotion Associa-
tionChinese Academy of Sciences.
117ESEC/FSE â€™22, November14â€“18, 2022,Singapore, Singapore Lin Shi,FangwenMu, XiaoChen, Song Wang,JunjieWang,Ye Yang,GeLi, XinXia,andQingWang
REFERENCES
[1]2017. PCSD Dataset Download. https://github.com/wanyao1992/code_
summarization_public/tree/master/dataset/original .
[2] 2018. TLC Dataset Download. https://github.com/xing-hu/TL-CodeSum .
[3] 2019. CSNDataset Download. https://github.com/github/CodeSearchNet .
[4] 2019. Funcom Dataset. http://leclair.tech/data/funcom/ .
[5]2020. SIGSOFT Open Science Policies. https://github.com/acmsigsoft/open-
science-policies .
[6] 2022. CATPythonLibrary. https://pypi.org/project/FSE22-CAT/0.0.1/ .
[7]2022. Project Website. https://github.com/BuiltOntheRock/FSE22_
BuiltOntheRock
[8]Wasi Uddin Ahmad, Saikat Chakraborty, Baishakhi Ray, and Kai-Wei Chang.
[n.d.]. A Transformer-based Approach for Source Code Summarization. In
Proceedings of the 58th Annual Meeting of the Association for Computational
Linguistics, ACL2020 . 4998Å›5007.
[9]MiltiadisAllamanis.[n.d.]. Theadverseeffectsofcodeduplicationinmachine
learningmodelsofcode.In Proceedingsofthe2019ACMSIGPLANInternational
SymposiumonNewIdeas,NewParadigms,andReflectionsonProgrammingand
Software,Onward! 2019 . 143Å›153.
[10]UriAlon,ShakedBrody,OmerLevy,andEranYahav.[n.d.]. code2seq:Gener-
atingSequencesfromStructuredRepresentationsofCode.In 7thInternational
Conference onLearning Representations, ICLR2019 .
[11]SatanjeevBanerjeeandAlonLavie.[n.d.]. METEOR:AnAutomaticMetricfor
MTEvaluationwithImproved CorrelationwithHumanJudgments. In Proceed-
ingsoftheWorkshoponIntrinsicandExtrinsicEvaluationMeasuresforMachine
Translation and/or Summarization@ACL2005 . 65Å›72.
[12]AakashBansal,SakibHaque,andCollinMcMillan.[n.d.]. Project-LevelEncod-
ing for Neural Source Code Summarization of Subroutines. In 29th IEEE/ACM
InternationalConference onProgramComprehension,ICPC2021 . 253Å›264.
[13]Ruichu Cai, Zhihao Liang, Boyan Xu, Zijian Li, Yuexing Hao, and Yao Chen.
2020. TAG: Type auxiliary guiding for code comment generation. arXiv preprint
arXiv:2005.02835 (2020).
[14]Qiuyuan Chen, Xin Xia, Han Hu, David Lo, and Shanping Li. 2021. Why My
Code Summarization Model Does Not Work: Code Comment Improvement with
Category Prediction. ACMTrans. Softw. Eng. Methodol. 30,2 (2021), 25:1Å›25:29.
[15]Qingying Chenand Minghui Zhou.2018. Aneural frameworkforretrieval and
summarizationofsourcecode.In 201833rdIEEE/ACMInternationalConference
onAutomatedSoftwareEngineering (ASE) . IEEE,826Å›831.
[16]Junyan Cheng, Iordanis Fostiropoulos, and Barry W. Boehm. 2021. GN-
Transformer: Fusing Sequence and Graph Representation for Improved Code
Summarization. CoRRabs/2111.08874 (2021). arXiv: 2111.08874 https://arxiv.org/
abs/2111.08874
[17]MatteoCiniselli,NathanCooper,LucaPascarella,AntonioMastropaolo,Emad
Aghajani, Denys Poshyvanyk, Massimiliano Di Penta, and Gabriele Bavota. 2021.
AnEmpirical Study on the UsageofTransformerModelsforCode Completion.
CoRRabs/2108.01585 (2021). arXiv: 2108.01585 https://arxiv.org/abs/2108.01585
[18]Anna Corazza, Valerio Maggio, and Giuseppe Scanniello. 2018. Coherence of
commentsandmethodimplementations:adatasetandanempiricalinvestigation.
Softw. Qual. J. 26,2 (2018),751Å›777. https://doi.org/10.1007/s11219-016-9347-1
[19]BrianPEddy,JeffreyARobinson,NicholasAKraft,andJeffreyCCarver.2013.
Evaluatingsourcecodesummarizationtechniques:Replicationandexpansion.
In201321st InternationalConference onProgramComprehension(ICPC) . 13Å›22.
[20]Davide Falessi and Philippe Kruchten. 2015. Five reasons for including technical
debt in the software engineering curriculum. In Proceedings of the 2015 European
Conference onSoftwareArchitectureWorkshops . 1Å›4.
[21]Zhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xiaocheng Feng, Ming Gong,
LinjunShou,BingQin,TingLiu,DaxinJiang,andMingZhou.[n.d.]. CodeBERT:
A Pre-Trained Model for Programming and Natural Languages. In Findings of
theAssociationforComputationalLinguistics:EMNLP2020,OnlineEvent,16-20
November 2020 (FindingsofACL,Vol.EMNLP 2020) . 1536Å›1547.
[22]ShuzhengGao,CuiyunGao,YulanHe,JichuanZeng,LunYiuNie,andXinXia.
2021. CodeStructureGuidedTransformerforSourceCodeSummarization. CoRR
abs/2104.09340 (2021). arXiv: 2104.09340 https://arxiv.org/abs/2104.09340
[23]David Gros, Hariharan Sezhiyan, Prem Devanbu, and Zhou Yu. [n.d.]. Code
to Comment "Translation": Data, Metrics, Baselining & Evaluation. In 35th
IEEE/ACM International Conference on Automated Software Engineering, ASE
2020, Melbourne, Australia,September 21-25, 2020 . 746Å›757.
[24]XiaodongGu,HongyuZhang,DongmeiZhang,andSunghunKim.2016. Deep
API learning. In Proceedings of the 2016 24th ACM SIGSOFT international sympo-
sium onfoundations ofsoftwareengineering . 631Å›642.
[25]Daya Guo, Shuo Ren, Shuai Lu, Zhangyin Feng, Duyu Tang, Shujie Liu, Long
Zhou, Nan Duan, Alexey Svyatkovskiy,Shengyu Fu,Michele Tufano, Shao Kun
Deng, Colin B. Clement, Dawn Drain, Neel Sundaresan, Jian Yin, Daxin Jiang,
and Ming Zhou. [n.d.]. GraphCodeBERT: Pre-training Code Representations
withDataFlow.In 9thInternationalConferenceonLearningRepresentations,ICLR
2021.
[26]VivekGupta.2020.DeepSumm-DeepCodeSummariesusingNeuralTransformer
Architecture. CoRRabs/2004.00998(2020). arXiv: 2004.00998 https://arxiv.org/abs/2004.00998
[27]SoniaHaiduc,JairoAponte,andAndrianMarcus.2010. Supportingprogramcom-
prehension with source code summarization. In 2010 acm/ieee 32nd international
conference onsoftwareengineering , Vol. 2.IEEE,223Å›226.
[28]SakibHaque,AakashBansal,LingfeiWu,andCollinMcMillan.[n.d.]. Action
WordPredictionforNeuralSourceCodeSummarization.In 28thIEEEInterna-
tional Conference on Software Analysis, Evolution and Reengineering, SANER 2021 .
330Å›341.
[29]Sakib Haque, Alexander LeClair, Lingfei Wu, and Collin McMillan. 2020. Im-
proved automatic summarization of subroutines viaattention to file context.In
Proceedingsof the17thInternationalConference onMiningSoftwareRepositories .
300Å›310.
[30]MasumHasan,TanveerMuttaqueen,AbdullahAlIshtiaq,KaziSajeedMehrab,
Md.MahimAnjumHaque,TahmidHasan,WasiUddinAhmad,AnindyaIqbal,
and RifatShahriyar. [n.d.]. CoDesc:ALarge Code-Description Parallel Dataset.
InFindings of the Association for Computational Linguistics: ACL/IJCNLP 2021
(FindingsofACL,Vol.ACL/IJCNLP2021) . 210Å›218.
[31]Xing Hu, Ge Li, Xin Xia, David Lo, and Zhi Jin. 2018. Deep code comment gener-
ation.In2018IEEE/ACM26thInternationalConferenceonProgramComprehension
(ICPC). IEEE,200Å›20010.
[32]Xing Hu, Ge Li, Xin Xia, David Lo, and Zhi Jin. 2020. Deep code comment
generationwithhybridlexicalandsyntacticalinformation. EmpiricalSoftware
Engineering 25,3 (2020), 2179Å›2217.
[33]Xing Hu, Ge Li, Xin Xia, David Lo, Shuai Lu, and Zhi Jin. 2018. Summarizing
source codewith transferred api knowledge. (2018).
[34]Yuan Huang, Nan Jia, Junhuai Shu, Xinyu Hu, Xiangping Chen, and Qiang Zhou.
2020. Does your code need comment? Softw. Pract. Exp. 50, 3 (2020), 227Å›245.
https://doi.org/10.1002/spe.2772
[35]Hamel Husain, Ho-Hsiang Wu, Tiferet Gazit, Miltiadis Allamanis, and Marc
Brockschmidt. 2019. Codesearchnet challenge: Evaluating the state of semantic
codesearch. arXiv preprint arXiv:1909.09436 (2019).
[36]Srinivasan Iyer, Ioannis Konstas, Alvin Cheung, and Luke Zettlemoyer. 2016.
Summarizingsourcecodeusinganeuralattentionmodel.In Proceedingsofthe
54thAnnualMeetingoftheAssociationforComputationalLinguistics(Volume1:
Long Papers) . 2073Å›2083.
[37]Siyuan Jiang, Ameer Armaly, and Collin McMillan. 2017. Automatically generat-
ing commitmessages fromdiffs usingneuralmachinetranslation.In 201732nd
IEEE/ACM International Conference on Automated Software Engineering (ASE) .
IEEE,135Å›146.
[38]AnjanKarmakarandRomainRobbes.2021. Whatdopre-trainedcodemodels
knowaboutcode?.In 202136thIEEE/ACMInternationalConferenceonAutomated
SoftwareEngineering (ASE) . IEEE,1332Å›1336.
[39]AlexanderLeClair,AakashBansal,andCollinMcMillan.[n.d.]. EnsembleModels
for Neural Source Code Summarization of Subroutines. In IEEE International
Conference onSoftwareMaintenance and Evolution, ICSME2021 . 286Å›297.
[40]Alexander LeClair, Sakib Haque, Lingfei Wu, and Collin McMillan. 2020. Im-
proved code summarization via a graph neural network. In Proceedings of the
28thinternational conference onprogramcomprehension . 184Å›195.
[41]AlexanderLeClair,SiyuanJiang,andCollinMcMillan.[n.d.]. Aneuralmodel
for generating natural language summaries of program subroutines. In 2019
IEEE/ACM41stInternationalConferenceonSoftwareEngineering(ICSE) .795Å›806.
[42]Jia Li, Yongmin Li, Ge Li, Xing Hu, Xin Xia, and Zhi Jin. [n.d.]. EditSum: A
Retrieve-and-EditFrameworkforSourceCodeSummarization.In 36thIEEE/ACM
International Conference on Automated Software Engineering, ASE 2021 . 155Å›166.
[43]ChenLin,ZhichaoOuyang,JunqingZhuang,JianqiangChen,HuiLi,andRongxin
Wu.[n.d.].ImprovingCodeSummarizationwithBlock-wiseAbstractSyntaxTree
Splitting. In 29th IEEE/ACM International Conference on Program Comprehension,
ICPC2021 . 184Å›195.
[44]Chin-Yew Lin. 2004. ROUGE: A package for automatic evaluation of summaries.
InTextsummarization branchesout . 74Å›81.
[45]Zhongxin Liu, Xin Xia, Ahmed E Hassan, David Lo, Zhenchang Xing, and Xinyu
Wang. 2018. Neural-machine-translation-based commit message generation:
how far are we?. In Proceedings of the 33rd ACM/IEEE International Conference on
AutomatedSoftwareEngineering . 373Å›384.
[46]ShuaiLu,DayaGuo,ShuoRen,JunjieHuang,AlexeySvyatkovskiy,Ambrosio
Blanco,ColinB.Clement,DawnDrain,DaxinJiang,DuyuTang,GeLi,Lidong
Zhou,LinjunShou,LongZhou,MicheleTufano,MingGong,MingZhou,Nan
Duan, Neel Sundaresan, Shao Kun Deng, Shengyu Fu, and Shujie Liu. 2021.
CodeXGLUE: A Machine Learning BenchmarkDataset for Code Understanding
andGeneration. CoRRabs/2102.04664(2021). arXiv: 2102.04664 https://arxiv.org/
abs/2102.04664
[47]Junayed Mahmud, Fahim Faisal, Raihan Islam Arnob, Antonios Anastasopoulos,
andKevinMoran.2021. CodetoCommentTranslation:AComparativeStudy
on Model Effectiveness & Errors. CoRRabs/2106.08415 (2021). arXiv: 2106.08415
https://arxiv.org/abs/2106.08415
[48]Antonio Mastropaolo, Simone Scalabrino, Nathan Cooper, David Nader-Palacio,
Denys Poshyvanyk, Rocco Oliveto, and Gabriele Bavota. 2021. Studying the
118Are We Building onthe Rock? On the Importance of DataPreprocessingforCode Summarization ESEC/FSE â€™22, November14â€“18, 2022,Singapore, Singapore
Usage of Text-To-Text Transfer Transformer to Support Code-Related Tasks.
In43rdIEEE/ACMInternationalConferenceonSoftwareEngineering,ICSE2021,
Madrid,Spain,22-30May2021 .IEEE,336Å›347. https://doi.org/10.1109/ICSE43902.
2021.00041
[49]Laura Moreno, Jairo Aponte, Giriprasad Sridhara, Andrian Marcus, Lori Pollock,
andKVijay-Shanker.2013. Automaticgenerationofnaturallanguagesummaries
for java classes. In 2013 21st International Conference on Program Comprehension
(ICPC). IEEE,23Å›32.
[50]Oracle.[n.d.]. http://www.oracle.com/technetwork/articles/java/index-137868.
html.
[51]KishorePapineni,SalimRoukos,ToddWard,andWei-JingZhu.[n.d.]. Bleu:a
MethodforAutomatic Evaluation ofMachine Translation.In Proceedings ofthe
40thAnnualMeetingoftheAssociationforComputationalLinguistics,July6-12,
2002, Philadelphia,PA, USA . 311Å›318.
[52]MaryamVahdatPour,ZhuoLi,LeiMa,andHadiHemmati.2021. ASearch-Based
Testing Framework for Deep Neural Networks of Source Code Embedding. In
14thIEEEConferenceonSoftwareTesting,VerificationandValidation,ICST2021,
Porto de Galinhas, Brazil, April 12-16, 2021 . IEEE, 36Å›46. https://doi.org/10.1109/
ICST49551.2021.00016
[53]FazleRabbiandMd.SaeedSiddik.2020. DetectingCodeCommentInconsistency
using Siamese Recurrent Network.In ICPC â€™20: 28th International Conference on
ProgramComprehension,Seoul,RepublicofKorea,July13-15,2020 .ACM,371Å›375.
https://doi.org/10.1145/3387904.3389286
[54]PoojaRani,SuadaAbukar,NataliiaStulova,AlexandreBergel,andOscarNier-
strasz. 2021. Do Comments follow Commenting Conventions? A Case Study in
JavaandPython.In 21stIEEEInternationalWorkingConferenceonSourceCode
AnalysisandManipulation,SCAM2021,Luxembourg,September27-28,2021 .IEEE,
165Å›169. https://doi.org/10.1109/SCAM52516.2021.00028
[55]Devjeet Roy, Sarah Fakhoury, and Venera Arnaoudova. 2021. Reassessing au-
tomaticevaluationmetricsforcodesummarizationtasks.In ESEC/FSEâ€™21:29th
ACM Joint European Software Engineering Conference and Symposium on the
Foundations ofSoftware Engineering,Athens, Greece,August23-28,2021 ,Diomidis
Spinellis, Georgios Gousios, Marsha Chechik, and Massimiliano Di Penta (Eds.).
ACM,1105Å›1116. https://doi.org/10.1145/3468264.3468588
[56]Gordon Rugg and Peter McGeorge. 2005. The Sorting Techniques: A Tutorial
Paper on Card Sorts, Picture Sorts and Item Sorts. Expert Syst. J. Knowl. Eng. 22,
3 (2005), 94Å›107. https://doi.org/10.1111/j.1468-0394.2005.00300.x
[57]HiteshSajnani,VaibhavSaini,JeffreySvajlenko,ChanchalK.Roy,andCristinaV.
Lopes. [n.d.]. SourcererCC: scaling code clone detection to big-code. In Proceed-
ingsofthe38thInternationalConferenceonSoftwareEngineering,ICSE2016,Austin,
TX, USA, May 14-22, 2016 . 1157Å›1168. https://doi.org/10.1145/2884781.2884877
[58]Abigail See, Peter J Liu, and Christopher D Manning. 2017. Get to the point:
Summarizationwithpointer-generatornetworks. arXivpreprintarXiv:1704.04368
(2017).
[59]RaminShahbazi, RishabSharma, andFatemeh H.Fard.2021. API2Com:Onthe
Improvement of Automatically Generated Code Comments Using API Docu-
mentations. In 29th IEEE/ACM International Conference on Program Compre-
hension, ICPC 2021, Madrid, Spain, May 20-21, 2021 . IEEE, 411Å›421. https:
//doi.org/10.1109/ICPC52881.2021.00049
[60]EnshengShi,YanlinWang,Lun Du, JunjieChen, Shi Han,HongyuZhang, Dong-
mei Zhang,and Hongbin Sun. 2021. Neural CodeSummarization: How Far Are
We?CoRRabs/2107.07112(2021). arXiv: 2107.07112 https://arxiv.org/abs/2107.
07112
[61]EnshengShi,YanlinWang,LunDu,HongyuZhang,ShiHan,DongmeiZhang,and
Hongbin Sun. [n.d.]. CAST: Enhancing Code Summarization with Hierarchical
Splitting and Reconstruction of Abstract Syntax Trees. In Proceedings of the 2021
ConferenceonEmpiricalMethodsinNaturalLanguageProcessing,EMNLP2021 .
4053Å›4062.
[62]Giriprasad Sridhara, Emily Hill, Divya Muppaneni, Lori Pollock, and K Vijay-
Shanker.2010. Towardsautomaticallygeneratingsummarycommentsforjava
methods. In Proceedings of the IEEE/ACM international conference on Automated
softwareengineering . 43Å›52.
[63]Daniela Steidl, Benjamin Hummel, and Elmar JÃ¼rgens. 2013. Quality analysis of
sourcecodecomments.In IEEE21stInternationalConferenceonProgramCom-
prehension, ICPC 2013, San Francisco, CA, USA, 20-21 May, 2013 . IEEE ComputerSociety, 83Å›92. https://doi.org/10.1109/ICPC.2013.6613836
[64]Zhensu Sun, Li Li, Yan Liu, Xiaoning Du, and Li Li. 2022. On the Importance
of Building High-quality Training Datasets for Neural Code Search. CoRR
abs/2202.06649 (2022). arXiv: 2202.06649 https://arxiv.org/abs/2202.06649
[65]Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
Aidan N Gomez, Åukasz Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. Advancesinneuralinformation processingsystems 30(2017).
[66]Ramakrishna Vedantam, C. Lawrence Zitnick, and Devi Parikh. 2015. CIDEr:
Consensus-based image description evaluation. In IEEE Conference on Computer
VisionandPatternRecognition,CVPR2015,Boston,MA,USA,June7-12,2015 .IEEE
Computer Society, 4566Å›4575. https://doi.org/10.1109/CVPR.2015.7299087
[67]Yao Wan, Zhou Zhao, Min Yang, Guandong Xu, Haochao Ying, Jian Wu, and
PhilipSYu.2018. Improvingautomaticsourcecodesummarizationviadeeprein-
forcementlearning.In Proceedingsofthe33rdACM/IEEEInternationalConference
onAutomatedSoftwareEngineering . 397Å›407.
[68]Wenhua Wang, Yuqun Zhang, Yulei Sui, Yao Wan, Zhou Zhao, Jian Wu, Philip S.
Yu, and Guandong Xu. 2022. Reinforcement-Learning-Guided Source Code
Summarization Using Hierarchical Attention. IEEE Trans. Software Eng. 48, 2
(2022), 102Å›119. https://doi.org/10.1109/TSE.2020.2979701
[69]YanlinWang, EnshengShi, Lun Du, Xiaodi Yang, Yuxuan Hu, Shi Han, Hongyu
Zhang, and Dongmei Zhang. 2021. CoCoSum: Contextual Code Summariza-
tionwithMulti-RelationalGraphNeuralNetwork. CoRRabs/2107.01933(2021).
arXiv:2107.01933 https://arxiv.org/abs/2107.01933
[70]YueWang,WeishiWang,ShafiqR.Joty,andStevenC.H.Hoi.[n.d.]. CodeT5:
Identifier-awareUnifiedPre-trainedEncoder-DecoderModelsforCodeUnder-
standing and Generation. In Proceedings of the 2021 Conference on Empirical
MethodsinNatural Language Processing,EMNLP 2021, pages = 8696Å›8708, .
[71]SultanWehaibi,EmadShihab,andLatifaGuerrouj.2016. Examiningtheimpactof
self-admitted technical debt on software quality. In 2016 IEEE 23Rd international
conferenceonsoftwareanalysis,evolution,andreengineering(SANER) ,Vol.1.IEEE,
179Å›188.
[72]Bolin Wei, Ge Li, Xin Xia, Zhiyi Fu, and Zhi Jin. 2019. Code generation as a dual
taskofcodesummarization. Advancesinneuralinformationprocessingsystems
32(2019).
[73]Bolin Wei, Yongmin Li, Ge Li, Xin Xia, and Zhi Jin. 2020. Retrieve and refine:
exemplar-basedneuralcommentgeneration.In 202035thIEEE/ACMInternational
Conference onAutomatedSoftwareEngineering (ASE) . IEEE,349Å›360.
[74]Edmund Wong,Taiyue Liu, and Lin Tan.2015. Clocom:Mining existing source
code for automatic comment generation. In 2015 IEEE 22nd International Confer-
ence on Software Analysis, Evolution, and Reengineering (SANER) . IEEE, 380Å›389.
[75]Hongqiu Wu, Hai Zhao, and Min Zhang. 2021. Code Summarization with
Structure-induced Transformer. In Findings of the Association for Computa-
tional Linguistics: ACL/IJCNLP 2021, Online Event, August 1-6, 2021 (Findings
of ACL, Vol. ACL/IJCNLP 2021) , Chengqing Zong, Fei Xia, Wenjie Li, and Roberto
Navigli (Eds.). Association for Computational Linguistics, 1078Å›1090. https:
//doi.org/10.18653/v1/2021.findings-acl.93
[76]Wei Ye, Rui Xie, Jinglei Zhang, Tianxiang Hu, Xiaoyin Wang, and Shikun Zhang.
2020. Leveraging code generation to improve code retrieval and summarization
viadual learning. In ProceedingsofThe Web Conference 2020 . 2309Å›2319.
[77]Huang Yuchao, Wei Moshi, Wang Song, Wang Junjie, and Wang Qing. 2021.
YetAnother CombinationofIR-and Neural-based Comment Generation. arXiv
preprint arXiv:2107.12938 (2021).
[78]Jian Zhang, Xu Wang, Hongyu Zhang, Hailong Sun, and Xudong Liu. 2020.
Retrieval-based neural source code summarization. In 2020 IEEE/ACM 42nd Inter-
nationalConference onSoftwareEngineering (ICSE) . IEEE,1385Å›1397.
[79]Xiaoqing Zhang, Yu Zhou, Tingting Han, and Taolue Chen. 2020. Training Deep
CodeCommentGeneration Models via Data Augmentation. In Internetwareâ€™20:
12thAsia-PacificSymposiumonInternetware,Singapore,November1-3,2020 .ACM,
185Å›188. https://doi.org/10.1145/3457913.3457937
[80]Yu Zhou, Xiaoqing Zhang, Juanjuan Shen, Tingting Han, Taolue Chen, and
HaraldC.Gall.2021. AdversarialRobustnessofDeepCodeCommentGeneration.
CoRRabs/2108.00213 (2021). arXiv: 2108.00213 https://arxiv.org/abs/2108.00213
[81]ZiyiZhou,HuiqunYu,andGuishengFan.2021.Adversarialtrainingandensemble
learning for automaticcode summarization. Neural Comput. Appl. 33, 19 (2021),
12571Å›12589. https://doi.org/10.1007/s00521-021-05907-w
119