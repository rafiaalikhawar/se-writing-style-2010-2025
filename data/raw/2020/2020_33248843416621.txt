Multiple-Boundary Clustering and Prioritization to Promote
Neural Network Retraining
Weijun Shen
State Key Laboratory for Novel
Software Technology, Nanjing
University, China
shenweijun@smail.nju.edu.cnYanhui Liâˆ—
State Key Laboratory for Novel
Software Technology, Nanjing
University, China
yanhuili@nju.edu.cnLin Chen
State Key Laboratory for Novel
Software Technology, Nanjing
University, China
lchen@nju.edu.cn
Yuanlei Han
State Key Laboratory for Novel
Software Technology, Nanjing
University, China
mg1833022@smail.nju.edu.cnYuming Zhou
State Key Laboratory for Novel
Software Technology, Nanjing
University, China
zhouyuming@nju.edu.cnBaowen Xu
State Key Laboratory for Novel
Software Technology, Nanjing
University, China
bwxu@nju.edu.cn
ABSTRACT
With the increasing application of deep learning (DL) models in
many safety-critical scenarios, effective and efficient DL testing
techniquesaremuchindemandtoimprovethequalityofDLmodels.
Oneofthemajorchallengesisthedatagapbetweenthetraining
data to construct the models and the testing data to evaluate them.
Tobridgethegap,testersaimtocollectaneffectivesubsetofinputs
fromthetestingcontexts,withlimitedlabelingeffort,forretraining
DL models.
To assist the subset selection, we propose Multiple-Boundary
Clusteringand Prioritization( MCP),atechniquetoclustertestsam-
ples into the boundary areas of multiple boundaries for DL models
and specify the priority to select samples evenly from all bound-
aryareas, to make sure enoughuseful samplesfor eachboundary
reconstruction. To evaluate MCP, we conduct an extensive empiri-
calstudywiththreepopularDLmodelsand33simulatedtesting
contexts.Theexperimentresultsshowthat,comparedwithstate-
of-the-artbaselinemethods,oneffectiveness,ourapproachMCP
has a significantly better performance by evaluating the improved
quality of retrained DL models; on efficiency, MCP also has the
advantages in time costs.
KEYWORDS
Software testing, Deep learning, Multiple-Boundary, Neural net-
work, Retraining
âˆ—Yanhui Li is the corresponding author.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation
on thefi rst page. Copyrights for components of this work owned by others than ACMmustbehonored.Abstractingwithcreditispermitted.Tocopyotherwise,orrepublish,topostonserversortoredistributetolists,requirespriorspecificpermissionand/ora
fee. Request permissions from permissions@acm.org.
ASE â€™20, September 21â€“25, 2020, Virtual Event, Australia
Â© 2020 Association for Computing Machinery.
ACM ISBN 978-1-4503-6768-4/20/09...$15.00
https://doi.org/10.1145/3324884.3416621ACM Reference Format:
WeijunShen,YanhuiLi,LinChen,YuanleiHan,YumingZhou,andBaowen
Xu. 2020. Multiple-Boundary Clustering and Prioritization to Promote Neu-
ralNetworkRetraining.In 35thIEEE/ACMInternationalConferenceonAuto-
mated Software Engineering (ASE â€™20), September 21â€“25, 2020, Virtual Event,
Australia. ACM, New York, NY, USA, 13 pages. https://doi.org/10.1145/
3324884.3416621
1 INTRODUCTION
Deeplearning(DL)aimstoconstructaformofadvancedpattern
classification by the use of artificial neural networks (NNs), which
learnstodistinguishbetweensamplesofdifferentcategoriesand
recognizes new samples into the right categories [ 18]. Along with
large-scale training data sets (e.g., YFCC100M [ 43]) and improved
processing capabilities (e.g., GPUs [ 48]), DL models have achieved
more â€œdeeperâ€ layers of data transformation, and surpassed the
levels of human experts in manyfi elds [ 37], including natural lan-
guageprocessing[ 42],speechr ecognition[ 3],autonomousdriving
[6], and so on. At the same time, concern has been raised at the
qualityofDLmodelsinthesafety-criticalapplications,especially
after a serial of self-driving car fatalities reported to the public1,
which calls for more testing effort to deal with issues on reliability
of DL models.
In the angle of statistics on DL models, testers evaluate the
quality of DL models on the whole performance of all test samples
collected from a specific application context, where the faults of
DL models are denoted as the wrong prediction behaviors, i.e., the
predicated labels are not equal to the actual ones [ 34,51]. The
different distributions between the training data used to construct
the models and the testing data collected from real applications
may result in the lower-than-expected performance of DL models.
For example, a medical diagnostic DL model, trained by the data of
foreign patients, is introduced to the local hospital. This DL model
maynotworkwellforthelocalpatients,justsincetheyaretrained
with different distribution of patient data in some aspects, e.g., the
race.
Asthedifferentdistributionbetweenthetrainingdataandthe
testingdataisaninherentproblemintheapplicationofDLmodels,
1https://en.wikipedia.org/wiki/List_of_self-driving_car_fatalities
4102020 35th IEEE/ACM International Conference on Automated Software Engineering (ASE)
oneofthemaintasksfortestersistodealwithitsadverseaffects.
In Artificial Intelligence (AI) area, a common approach to adjust-
ing DL models to deal with new data of different distributions is
constructingretrainedmodelsfromcurrentmodels,basedonthe
theory of active learning [ 20,39,46]. Specifically, the neural net-
workarchitecturesofretrained models areinitializedasthesame
asthecurrentmodel,andupdatedbytrainingonadditionallabeled
samples ofnew data.Inspired bythis idea,testers forDL models
couldcollectusefulinputsfromthetestingcontextandreportthem
for retraining current models. Intuitively, more additional samples
with label information will help retrain DL models to adjust to the
testing context with more reliability.
However,intherealapplication,labelingthesesamplesrequires
a lot of manpower, which is prohibitively time-consuming andexpensive [
21]. This fact greatly weakens the possibility of em-
ployingenoughlabeledsamplesfromthetestingdata.Asaresult,
researchers of DL testing are setting their eyes on a new goal: to
design and implement effective test sampling methods to save the
labeling effort[ 25,40]. Liet al.[ 21] proposedan efficienttest selec-
tion method to estimate the accuracy of DL models on operational
environments.Kimetal.proposedanoveltestadequacycriterion
called SADL (Surprise Adequacy for Deep Learning Systems) [17]
fortestingDLsystems,whichcanguildthetestselectionfortesting
or retraining DL models with higher accuracy.
Based on the above considerations, we focus on the problem
â€œguiding the retraining of DL modelsâ€: with limitedlabeling effort,
the testers aim to select an effective subset of testing data, labelthem, test them, and report them, which are most helpful to re-
trainDLmodels.Toaddressthis,wepropose Multiple-Boundary
Clustering and Prioritization ( MCP) as the affirmative answer
andincarnationofthisproblem.Afterrevisitingthefunctionality
of DL models, we observe that as a form of advanced multiple-classification, DL models have multiple boundaries according to
differentclasses,andthereliabilityofDLmodelslargelydepends
on the correctness of the decision boundaries of patterns [ 18]. The
differencebetweentrainingandtestingdatacouldbeconsidered
as the difference of boundaries between original DL models and
retrained DL models. As a result, retraining DL models converts to
thereconstructionofmultipleboundaries.Themainideasofour
MCP algorithm are (a) clustering: we cluster the samples of test-
ingdataintodifferentboundaryareasaccordingtotheirfirstand
second predicted classes; (b) prioritization: we specify the priority
to select samples, evenly from all boundary areas, to make sure
enough samples for each boundary reconstruction.
MCP has been evaluated on three widely-used open datasets
MNIST [19], CIFAR10 [ 32] and SVHN [ 31], and three DL Mod-
els with advanced DL Networks [ 15,17,22]. In each dataset, we
leverage two strategies image transformation [ 44] and adversarial
attack [17] (with 11 synthetic operators) on generating realistic
synthetic images to simulate the testing contexts with different
distributions.Asmentionedabove,wefocusonthescenariowith
thelimitedlabelingeffort,i.e.,onlytheverysmallpercentoftest
datacouldbelabeled.Intheexperiment,wefocusonfourrepresen-tative labeling ratios 1%, 3%, 5%, and 10% to check the performance
of MCP with labeling percent not larger than 10%. To assess theperformance of MCP, we introducefi ve test sampling methodsas the baselines: three state-of-the-art methods from DL testing(LSA&DSA at ICSEâ€™2019 [
17], CES at FSEâ€™2019 [ 21]), one active
learningmethod(AAL[ 20])andthecommonbaselinesimpleran-
dom selection (SRS).
Our experimental results support the claim that MCP performs
well: (a) on effectiveness, we observe that MCP performs signifi-
cantly better than the other baseline methods when considering
the accuracy improvementsof retraining DL models,i.e., with the
highest values of accuracy improvement after retraining the DLmodels; (b) on efficiency, we observe that with the exception of
randomselection,otherbaselinemethodscostseveraltimesoreven
hundredsoftimesmoreruntimethanMCP.ThatmeansMCPhas
anobviousadvantageintimecosts.Tosumup,MCPisaneffective
and efficient test sampling method for â€œguiding the retraining of
DL modelsâ€.
Our study makes the following contributions:
â€¢Strategy. This paper proposes a novel technique Multiple-
Boundary Clusteringand Prioritization(MCP)asanefficient
strategy to measure the usefulness of the samples by con-sidering boundary areas of multiple-boundaries w.r.t. the
distribution of data in the testing context.
â€¢Study.Thisstudyincludesanextensiveempiricalstudyof
threeDLmodelsand33simulatedtestingdatasetswithmore
than 10000 testing inputs. The results of our experimentshow that MCP has a significant better performance than
otherfi ve baseline methods, evaluated on effectiveness and
efficiency of six studied methods.
Therestofthispaperisorganizedasfollows.InSection2,we
introducethebackgroundofourpaper,includingtheprincipleof
DL models and two DL testing properties. In Section 3, we present
a technical description of our algorithm. In Section 4, we introduce
ourexperimentalsettings,includingsubjectdatasets,DLmodels,
data simulation, baseline methods, research questions and so on.Section 5 explains experimental results and discoveries. Section6furtherdiscussessomeimportantexperimentaldetails.Section
7 and 8 are threats to validity and related works. Section 9 is the
summary of the paper.
2 BACKGROUND
In this section, wefi rst introduce the principle of DL models, then
present two properties of DL testing.
2.1 The Principle of DL Models
For a DL model M, it can be considered as a neural networks
implementation of a mapping function from the feature domain ğ¹ğ‘‘
to the classification domain ğ¶ğ‘‘:
M:ğ¹ğ‘‘â†’ğ¶ğ‘‘
wheretheinput ğ‘¥=[ğ‘¥1,...,ğ‘¥ğ‘›]âˆˆğ¹ğ‘‘isamatrix(vector)contain-
ing high dimensional features (e.g., a gray value matrix to denote a
picture), and the output ğ‘¦âˆˆğ¶ğ‘‘is one predictive label in a given
set of target classification labels ğ¶ğ‘‘={ğ¶1,ğ¶2,...ğ¶ğ‘š}.
The standard architecture of a deep learning software gener-
ally consists of three parts: input layer, hidden layer, and output
layer. The input layer connects the input matrix ğ‘¥with the hidden
layer, and its dimension is the same as the number of elementsin the input matrix. The hidden layer can be composed of many
411layers, and the structure of each layer can be freely chosen. The
outputlayerconnects thehiddenlayerand thesoftmaxlayer,and
its dimension is the same as the size of the target classification
label set ğ¶ğ‘‘. Layers are connected by weight values, which are
obtained by training. The softmax function [ 2] takes a sequence of
ğ‘›valuesğ‘¦1,ğ‘¦2,...,ğ‘¦ğ‘›fromoutputlayer,andnormalizestheminto
ğ‘›output probabilities ( ğ‘¦âˆ—
1,ğ‘¦âˆ—
2,...,ğ‘¦âˆ—ğ‘›for target classification labels
ğ¶1,ğ¶2,...,ğ¶ğ‘›, respectively):
ğ‘¦âˆ—
ğ‘–=ğ‘’ğ‘¦ğ‘–
/summationtext.1ğ‘›
ğ‘—=1ğ‘’ğ‘¦ğ‘—
The probability values ğ‘¦âˆ—
1,ğ‘¦âˆ—
2,...ğ‘¦âˆ—ğ‘›satisfy
ğ‘¦âˆ—
1+ğ‘¦âˆ—
2+...+ğ‘¦âˆ—
ğ‘›=1
Thefinaloutput ğ‘¦istheindexofthelargestprobabilityvalue ğ‘¦=ğ¶ğ‘–,
whereğ‘–satisfiesğ‘–=argmaxğ‘–(ğ‘¦âˆ—
ğ‘–).
2.2 DL Testing properties
DLTestingpropertiesrefertothatforwhatconditions,aDLmodel
offers some guarantees under DL testing [ 50]. In this subsection,
we introduce two properties we focus on in the rest of the paper.
Definition 1 (Accuracy). LetMbe the trained DLmodel that
we need to test. Let Dğ‘¡be the testing data to evaluate M. Let
ğ‘¥be an input of the model ğ‘¥âˆˆDğ‘¡.M(ğ‘¥)is the predicted label
ofğ‘¥generated by M, andL(ğ‘¥)is the actual label. The accuracy
ğ´ğ‘ğ‘(M,Dğ‘¡)ofMw.r.t.Dğ‘¡is defined as the probability that the
predicted labels are equal to the actual labels:
1
|Dğ‘¡|/summationdisplay.1
ğ‘¥âˆˆDğ‘¡I(M(ğ‘¥)=L(ğ‘¥))
where the indicator function I(Â·)returns 1 if the logical expression
Â·issatisfied,andreturns0ifitisunsatisfied.â€œAccuracyâ€isoneof
the most widely used statistical indictor to measure the quality of
DL models in previous studies for DL testing [21, 27].
Definition 2 (Accuracy Improvement). LetMğ‘œbetheorigi-
nal DL model, which is with lower accuracy w.r.t. the testing data
Dğ‘¡.LetMğ‘Ÿbetheretrainedmodelsfrom Mğ‘œbyretrainingwith
the useful subset Dğ‘¢of samples selected from the test data Dğ‘¡.
The accuracy improv ement is defined as the delta of the accuracies
between two DL models:
ğ´ğ‘ğ‘(Mğ‘Ÿ,Dğ‘¡)âˆ’ğ´ğ‘ğ‘(Mğ‘œ,Dğ‘¡)
3 METHODOLOGY
In this section, we present a technical description of our algorithm.
First,weformallypresentthestudiedproblem.Next,wedescribe
the basic idea of our algorithm. Finally, we describe our algorithm
in detail.
3.1 The Studied Problem
Basedonabovedefinitions,wepresenttheproblemâ€œguidingthe
retraining of DL models with limited labeling effortâ€ specifically:
Problem. Mğ‘œis tested with Dğ‘¡, whereDğ‘¡exhibits the dif-
ferent distribution compared with the training data of Mğ‘œ.
All samples ğ‘ (ğ‘ âˆˆDğ‘¡)o fDğ‘¡are unlabeled. Given labelingeffortğ¸(i.e.,ğ¸samplesoftestingdatacouldbelabeledwith
ğ¸/lessmuch| Dğ‘¡|),ouraimistoselectthethesubset Dğ‘¢containing
ğ¸inputs from Dğ‘¡, label them, and return them to retrain Mğ‘œ,
with an as high accuracy improvement as possible.
Obviously, the subset Dğ‘¢with more accuracy improvement
would be considered as more useful. To solve this problem, we try
to construct an effective and efficient algorithm for selecting Dğ‘¢.
3.2 The Basic Idea
For a classification task, a good DL model obtains as accurate as
possibleboundaryinformationbylearningthehiddenrulesfrom
large-scale training data. In the research of AI and DL testing,samples near the boundary have attracted widespread attention.
Hamidzadeh et al. [ 14] proposed an instance reduction method for
k-NNclassificationbasedonhyperrectangleclustering,inwhich
theykeeponlythosenearboundary.Maetal.proposedamutation
testingframework DeepMutationfor DLmodels, andthey argued
thatthesamplesneartheboundaryofDLmodelsaremorelikelytobeaffectedbythemutationoperators[
26].Kimetal.[ 17]proposed
twometricsLSA(Likelihood-basedSurpriseAdequacy)andDSA
(Distance-basedSurpriseAdequacy)tomeasurehowclosetothe
boundary between different classes the test samples are.
Althoughthe mostboundary-basedtechniquesare effectiveon
selectinginformativesamplesinabovescenarios,theyonlycapturethe distance relationship of test samples with the boundaries of DL
models and fail to take the boundary distribution information (e.g.,
multipleboundariesfor multipleclassification)inthe wholeunla-
beledtestsetintoaccount.Thismaygiverisetoimbalanceselection
among boundaries. For example, Figure 1 shows an example of DL
models with multiple-classification (i.e., three classes, Class 1: â˜…,
Class2: /diamondsolid,andClass3: /trianglesolid).Thesolidlinesshowdecisionboundaries
of the original model Mğ‘œ, while the dashed lines show boundaries
of the retrained model Mğ‘Ÿ. The shapes ( â˜…,/diamondsolid, and /trianglesolid) of samples
denotetheactuallabels,andthecolors(red,blue,andgreen)denote
thepredictedlabelsby Mğ‘œ.Thesamples â˜…,/diamondsolidand/trianglesolidarecorrectly
predicted2, while the others (e.g., â˜…andâ˜…) are wrong predicted. As
shown in Figure 1, we obtain the following observations:
â€¢For DL models with multiple-classification, they have multi-
ple boundaries, each of which may have the corresponding
boundary area (i.e., the light grey ellipse areas I, II, and IIIrepresent the boundary areas of
Mğ‘œ, between Class 1 and
2, Class 2 and 3, and Class 1 and 3, respectively.) with the
different distribution of samples near it. Obviously, the sam-
ples in III are more far (i.e., with longer distances) from thethe original boundary than the samples in I and II.
â€¢
Generally, samples near the boundary are more sensitive to
thechangeoftheboundary(asshowninthisexample,the
retrained boundary only crosses the boundary area of theoriginal boundary), and hence more useful to reconstruct
the retrained boundary.
â€¢However,thedifferentdistributionofmultipleboundaries
can not be ignored. If we only consider the distance metric,
2â˜…is correctly predicted as red by Mğ‘œ,/diamondsolidblue, and /trianglesolidgreen, respectively
412Original Boundary
Between Class 1&3Retrained Boundary
Between Class 1&3
Original  Boundary
Between Class 1&2Retrained BoundaryBetween Class 1&2OriginalBoundaryBetweenClass 2&3
RetrainedBoundary 
Between 
Class &3Class 1Class 3
Class 2III
III
Figure 1: An example of multiple-classification DL models
with multiple-boundaries. The samples from test data are
classified into three classes by the original model Mğ‘œwith
originalboundaries(solidlines)andbytheretrainedmodel
Mğ‘Ÿwith retrained boundaries (dashed lines).
wemayoverselectsamplesfromIandII(withshorterdis-
tance), and lose sight of the samples from III (with longer
distance), which are also essential to reconstruct the bound-
ary between Class 2 and 3.
Based on the above three points, our basic idea is considering
retraining DL models as the reconstruction of multiple boundaries
basedonmultipleclassificationandtryingtoselectsamplesevenly
from multiple boundary areas to make sure enough samples for
each boundary reconstruction.
3.3 Multiple-Boundary Clustering and
Prioritization
Basedonthebasicideamentionedabove,weproposeouralgorithm
namedMultiple-Boundary Clustering and Prioritization ( MCP
for short), detailed presented in Algorithm 1. Given the originalDL model
Mğ‘œ, an unlabeled test dataset Dğ‘¡, the label set ğ¶ğ‘‘=
{ğ¶1,ğ¶2,...,ğ¶ğ‘š}and the labeling effort ğ¸/lessmuch| Dğ‘¡|, MCP comprises
the following three steps:
Step 1:Output Probabilities Extraction. In this step, we run the
modelMğ‘œagainst the whole test dataset Dğ‘¡without know-
ing the actual labels of samples (line 3-4). For each sam-ple
ğ‘ inDğ‘¡, we obtain a sequence of output probabilities
ğ‘¦ğ‘ 
1,ğ‘¦ğ‘ 
2,Â·Â·Â·,ğ‘¦ğ‘ ğ‘š, which represents the probability distribution
ofğ‘ for theğ‘šclasses in the given label set ğ¶ğ‘‘. These output
probabilities are the basis of the following clustering and
prioritization.
Step 2:BoundaryAreaClustering.DLmodelswithmultipleclassifi-cationhavemultipleboundaries.Inthisstep,wetrytocluster
thesamplesof Dğ‘¡intodifferentboundaryareas,whichwe
consider as the confusion areas of two classes. Based on out-
putprobabilities,foreachsample ğ‘ inDğ‘¡,wecalculatetheAlgorithm 1: M ultiple-Boundary Clustering and Prioriti-
zation MCP (Mğ‘œ,Dğ‘¡,ğ¶ğ‘‘,ğ¸)
Input:The DL model Mğ‘œ, an unlabeled test dataset Dğ‘¡, the label
setğ¶ğ‘‘={ğ¶1,ğ¶2,...,ğ¶ğ‘š}, the labeling effort ğ¸/lessmuch| Dğ‘¡|
Output:The selected subset Dğ¸âŠ†Dğ‘¡with|Dğ¸|=ğ¸
1initialize Dğ¸=âˆ…,ğ¸ğ‘¢ğ‘ ğ‘’ğ‘‘=0;
2initialize priority queues ğ‘„ğ‘–ğ‘—=âˆ…(1â‰¤ğ‘–,ğ‘—â‰¤ğ‘š,ğ‘–â‰ ğ‘—), whose
elements /angbracketleftğ‘ ,ğ‘ğ‘ /angbracketrighthave twofi elds the sample ğ‘ and the priority ğ‘ğ‘ ,
ğ‘„={ğ‘„ğ‘–ğ‘—|1â‰¤ğ‘–,ğ‘—â‰¤ğ‘š,ğ‘–â‰ ğ‘—};
3forğ‘ âˆˆDğ‘¡do//Step 1: Output Probabilities Extraction
4runMğ‘œwithğ‘ and get output probabilities: ğ‘¦ğ‘ 
1,ğ‘¦ğ‘ 
2,Â·Â·Â·,ğ‘¦ğ‘ ğ‘š;
5forğ‘ âˆˆDğ‘¡do //Step 2: Boundary Area Clustering
6determine which boundary area ğ‘ belongs to by calculating the
first/second predicted class ğ‘–/ğ‘—ofğ‘ with the max/secondmax
probabilities: ğ‘–=argmax1â‰¤ğ‘˜â‰¤ğ‘š{ğ‘¦ğ‘ 
ğ‘˜},
ğ‘—=argmax1â‰¤ğ‘˜â‰¤ğ‘š,ğ‘˜â‰ ğ‘–{ğ‘¦ğ‘ 
ğ‘˜};
7calculate the boundary priority ğ‘ğ‘ :ğ‘ğ‘ =ğ‘¦ğ‘ 
ğ‘–/ğ‘¦ğ‘ 
ğ‘—;
8insertğ‘ intoğ‘„ğ‘–ğ‘—with its priority ğ‘ğ‘ :insert(ğ‘„ğ‘–ğ‘—,/angbracketleftğ‘ ,ğ‘ğ‘ /angbracketright);
9whileğ¸ğ‘¢ğ‘ ğ‘’ğ‘‘â‰¤ğ¸do//Step 3: Uniform Priority Selection
10initial a temporary priority queue ğ‘„ğ‘¡=âˆ…;
11forğ‘„ğ‘–ğ‘—âˆˆğ‘„do
12 pop the sample with minimal priority from ğ‘„ğ‘–ğ‘—:
/angbracketleftğ‘ âˆ—,ğ‘ğ‘ âˆ—/angbracketright=extractMin (ğ‘„ğ‘–ğ‘—);
13 insert the sample into ğ‘„ğ‘¡:insert(ğ‘„ğ‘¡,/angbracketleftğ‘ âˆ—,ğ‘ğ‘ âˆ—/angbracketright);
14if|ğ‘„ğ‘¡|â‰¤(ğ¸âˆ’ğ¸ğ‘¢ğ‘ ğ‘’ğ‘‘)then
15 add all sample in ğ‘„ğ‘¡intoDğ¸:
Dğ¸=Dğ¸âˆª{ğ‘ âˆ—|/angbracketleftğ‘ âˆ—,ğ‘ğ‘ âˆ—/angbracketrightâˆˆğ‘„ğ‘¡};
16 ğ¸ğ‘¢ğ‘ ğ‘’ğ‘‘=ğ¸ğ‘¢ğ‘ ğ‘’ğ‘‘+|ğ‘„ğ‘¡|;
17else
18 whileğ¸ğ‘¢ğ‘ ğ‘’ğ‘‘â‰¤ğ¸do
19 pop the sample with minimal priority from ğ‘„ğ‘¡:
/angbracketleftğ‘ âˆ—,ğ‘ğ‘ âˆ—/angbracketright=extractMin (ğ‘„ğ‘¡);
20 add the sample into Dğ¸:Dğ¸=Dğ¸âˆª{ğ‘ âˆ—};
21 ğ¸ğ‘¢ğ‘ ğ‘’ğ‘‘=ğ¸ğ‘¢ğ‘ ğ‘’ğ‘‘+1;
22returnDğ¸;
first(second)predictedclass ğ‘–(ğ‘—)withthemax(secondmax)
outputprobabilities,andclusterthesample ğ‘ intothebound-
ary area between class ğ‘–andğ‘—(line 6-8). Our assumption
isthat,moreclosethesetwoprobabilitiesare,moreclose ğ‘ 
istotheboundaryofclass ğ‘–andğ‘—.Therefore,wecalculate
the priority ğ‘as the ratio of the probability ğ‘¦ğ‘ 
ğ‘–of thefirst
classğ‘–to the probability ğ‘¦ğ‘ 
ğ‘—of the second class ğ‘—(line 7).
Forconvenienceofthefollowingprioritization,weusethe
priorityqueues ğ‘„ğ‘–ğ‘—tosorttheresultsofclustering(line8),
which contains twofi elds: the sample ğ‘ and the priority ğ‘ğ‘ .
Step 3:UniformPrioritySelection.Afterobtainingthepriorityqueues
ğ‘„ğ‘–ğ‘—for multiple boundaryareas, with the given labelingef-
fortğ¸,wetrytoevenlyselectsamplesfromeach ğ‘„ğ‘–ğ‘—tomake
sure that for each boundary, there are enough selected sam-
ples to reconstruct the boundary. Specifically, our approach
comprisesroundsofselection.Innormalrounds,weinitialatemporarypriorityqueue
ğ‘„ğ‘¡tosortthesamplewithminimal
prioritypoppedofffromnon-empty ğ‘„ğ‘–ğ‘—(i.e.,evenlyselect
the most suitable sample from each non-empty boundary
413Table 1: Datasets and DNN models
Dataset DNN Model Layers Parameters Training Acc. Testing Acc.
MNIST ConvNet 12 1,218,634 99.91% 99.32%
CIFAR10 DenseNet 158 1,037,818 95.28% 94.84%
SVHN RCNN 53 5,340,298 99.03% 96.54%
area,line12-13)andaddthemintotheselectedsubset Dğ¸,
when the labeling effort is enough (line 14-16). In the last
round, we will add the samples with the minimal priority
in the temporary priority queue ğ‘„ğ‘¡, one by one, until the
labeling effort is used up (line 18-21).
To sum up, MCP selects almost equal sizes of ranked samples
from each boundary area into the selected subset Dğ¸, which totals
to the given labeling effort |Dğ¸|=ğ¸; andfi nally returns Dğ¸as the
subset to be labeled.
4 EXPERIMENT SETUP
Inthissection,wepresenttheexperimentalsetuptoevaluatethe
performanceofMCP.OurexperimentrunsonaUbuntu18.04server
with8GPUcoresâ€œTeslaV100SXM232GBâ€.Therelatedframeworks
ofdeeplearningincludepython3.6.3,numpy1.18.1,tensorflow-gpu
1.15.0,Keras2.1.6,cleverhans3.0.1,andOpenCV2.0.Weprovide
the replication package including detailed description of studied
methods and source code online (see Section 10).
4.1 Studied Datasets and Models
We conduct the experiments on three widely used datasets MNIST,
CIFAR10,andSVHN.MNISTisadatasetof28 Ã—28grayscaleimages
forhandwrittendigit(from0to9)imagerecognition,whichcon-
tains60,000trainingdataand10,000testdata[ 19].CIFAR10dataset
consistsof60,000(50,000trainingimagesand10,000testimages)
32x32 color images in 10 classes, e.g., airplane, bird, and ship, with
6,000 images per class [ 32]. SVHN (Street View House Numbers)
isobtainedfromhousenumbersextractedinGoogleStreetView
images [31]. It can be seen as similar infl avor to MNIST, i.e., the
images are of small cropped digits from 0 to 9, but comes from a
significantly harder and real world problem (i.e., recognizing digits
and numbers in natural scene images). In detail, it contains 73,257
digits for training, and 26,032 digits for testing.
Forabovethreedatasets,weapplyDLmodelstructuresandtrain
DL models with competitive accuracies [ 1], as shown in Table 1.
Specifically, for MNIST, we train afi ve layer Convolutional Neural
Network (ConvNet) model which is applied in [ 17] with 99.91%
training accuracy and 99.32% testing accuracy; for CIFAR10, we
apply the advanced DL Network named DenseNet [ 15], and train a
modelwith95.28%trainingaccuracyand94.84%testingaccuracy;
for SVHN, we apply the advanced DL Network named RCNN [ 22],
andtrainamodelwith99.03%trainingaccuracyand96.54%testing
accuracy.
4.2 Data Simulation
We recall the studied testing scenario: the distributions of train-
ing dataset and testing dataset share common components buthave non-ignorable differences. In the experiment, we leverageTable2:Thedescriptionsofsyntheticoperatorsandthesim-
ulated accuracy of 33 simulated datasets
Operator DescriptionSimulated Accuracy (%)
MNIST CIFAR10 SVHN
Rotate Rotate with a certain angle 91.12 90.57 91.55
Translate Translate with several pixels 87.84 89.97 89.89
Shear Horizontal Shearing 88.59 90.01 89.58
Brightness Changing brightness 94.19 90.99 91.71
Contrast Changing Contrast 87.91 91.15 90.95
Scale Zoom out or Zoom in 87.47 90.61 91.01
FGSM Fast Gradient Sign Method 82.3 83.98 81.85
JSMA Jacobian-based Saliency Map Attack 82.42 83.84 90.78
C&W Carlini&Wagner 79.84 81.8 81.81
BIM-A Basic Iterative Method A 80.64 85.31 90.98
BIM-B Basic Iterative Method B 80.63 86.12 91.51
(a) Original
 (b) Rotate
 (c) Translate
 (d) Shear
 (e)Brightness
 (f) Contrast
(g) Scale
 (h) FGSM
 (i) JSMA
 (j) C&W
 (k) BIM-A
 (l) BIM-B
Figure 2: One original image from MNIST and 11 syntheticimages by our image synthetic operators
two strategies image transformation [ 44] and adversarial attack
[17]ongeneratingrealisticsyntheticimagestosimulatethetest-
ing context with different distribution. Table 2 shows the short
descriptionsofthesesyntheticoperatorsusedinourexperiment.
First,weapplysixdifferentrealisticimagetransformations(Rotate,
Translate,Shear,Brightness,Contrast,andScale)tomimicdifferent
real-worldphenomena.Rotate,Translate,Shear,andScaleareall
different types of affine transformations3, which are often applied
inimageprocessingtofixdistortionsduetocameraanglevariations
[35].Besides,BrightnessandContrastarebothlineartransforma-
tions. Our implementation of these transformations is based on
OpenCV4.Second,weapplyfivewidelystudiedattackstrategies:
FGSM,JSMA,C&W,BIM-A,andBIM-B[ 17],whoseimplementa-
tion is based on cleverhans [ 33]. Figure 2 shows an example of the
applicationofsyntheticoperators:oneoriginalimagelabelledas
â€œ2â€ and 11 synthetic images generated by 11 synthetic operators,
respectively.
We apply synthetic operators on original testing samples to
generate synthetic testing samples. We control the distribution di-
versitybyadding20%realisticsyntheticsamplesgeneratedfrom
each synthetic operator to replace the original samples. For exam-
ple, for MNIST and CIFAR10 with 10000 testing samples and the
syntheticoperatorFGSM,werandomlyextract2000(10000 Ã—20%)
originalsamples,generate2000syntheticsamplesbyFGSM,and
3https://www.mathworks.com/discovery/affine-transformation.html
4https://opencv.org
414addthemintoMNISTandCIFAR10toreplace2000originalones.
For clarity, the hybrid testing datasets are named FGSM_MNIST
andFGSM_CIFAR10,respectively.Tosumup,weconstruct33(11
synthetic operators Ã—3 original testing datasets) hybrid testing
dataset to simulate the testing context, whose accuracies of the
original models are shown in the last three columns of Table 2.
4.3 Baseline Methods
Givenanunlabeledtestingdatasetandthelimitedlabelingeffort,
weaimtoselectasubsettolabelandretraintheoriginalDLmodel.
We want to study whether our proposed method MCP, which is
based on multiple boundary areas of DL models, can perform well
intheretrainingofDLmodels.Wechoosethefollowingbaseline
methodsforcomparison,whicharefromthefieldofAIsoftware
testing and traditional AI research.
â€¢LSA/DSA.Recently,Kimetal.proposedanoveltestadequacy
criterioncalledSADL(SurpriseAdequacyforDeepLearning
Systems) [ 17] for testing DL systems. In their study, LSA
(Likelihood-based SurpriseAdequacy)andDSA( Distance-
basedSurpriseAdequacy) aretwo different metrics tomea-
sure SA (Surprise Adequacy), which support â€œa sense ofhow close to the class boundary the new inputs areâ€. Thehigher the value of SA is, the corresponding test case is
moresurprisetotheDLsystemsundertesting.Weintroduce
LSA/DSA as the baseline selection method, which select the
subset of test cases with higher corresponding SAs.
â€¢AAL.Activelearningisaspecialcaseofmachinelearningto
deal with new data of different distribution, which is widely
appliedinmanyapplicationdomains,suchascomputervi-
sion. Li et al. presented a novel approach AAL ( Adaptive
ActiveLearning) [ 20] for computer vision that combines an
informationdensitymeasureandamostuncertaintymea-
suretogethertoselecteffectiveinstancestolabelfortraining
image classifications ceaselessly.
â€¢CES. Li et al. proposed an efficient test selection methodbased on conditioning [
21] to estimate new modelsâ€™ preci-
siononoperationalenvironments.Theirresultsshowthat
CES (CrossEntropy-based Sampling) estimator consistently
outperformsothercomparedmethodsinallthestudiedcases.
SowechoosethemethodCES, whichisalsoappliedinthe
circumstance where the testing dataset is unlabeled, as a
baseline for comparison.
â€¢SRS.Duringthesubsetselection,the SimpleRandomSampling
(SRS)isnaturallyconsideredasabaselinemethod.SRSdraws
samples randomly from the testing dataset.
Tosumup,wecomparetheperformancesofourmethodMCP
with following baseline methods: LSA, DSA, AAL, CES, and SRS in
retrainingDLmodels.Thankstothegenerosityoftheauthorsof
theabovebaselinemethods[ 17,20,21],thesecodesareavailable
online and can be obtained easily. In order to ensure the accuracy
of experimental repetition, we directly invoke these original codes.
Duetothespacelimitation,weomitthedetailsoftheimplemen-
tationofbaselinemethodshere,andgivethedetaileddescription
and source code in the replication package (see Section 10).
Figure 3: The framework of retraining settings
4.4 Retraining Settings
Figure3showstheframeworkofretrainingsettings,whichcom-
prises the following three steps.
Sampling. AsmentionedinSection3.1,wefocusonthescenario
withthelimitedlabelingeffort ğ¸,i.e.,onlytheverysmallpartoftest
dataDğ‘¡5couldbelabelled( ğ¸/lessmuch| Dğ‘¡|).Intheexperiment,wefocus
onfourrepresentativesamplingratios1%,3%,5%,and10%tocheckthe performance of MCP with labeling percent not larger than 10%.
Specifically, for each simulated test dataset, we apply MCP and the
5 baseline methods to sample 1% ,3%, 5%, and 10% inputs from it.
For example, when focusing on the ratio 1% of MINIST, we select a
subsetwiththe1%size(10000 Ã—1%=100)oftestingdatasetaccording
to MCP and other baseline methods.
Retraining. Following the retraining process in SADL [ 17], we
retraintheDLmodelsbasedonthenewsampledsubsetwithan-other 5 epochs. Mao et al. [
29] have observed that retraining on
the combination of the training set and the new sampled subsetmay performs badly, because a few new sampled data is easy tobe overwhelmed by other old training data. Besides, in order to
eliminatetherandomnessoftheselectionmethods(e.g.,CESand
SRS)andthetrainingprocess,werepeattheprocessofretraining5
times and report 5 retrained accuracy improvements.
Evaluation. As stated in the problem in Section 3.1, to evaluate
theperformanceofourMCPmethodandotherbaselinemethods,
we focus on the accuracy improvement of the retrained model Mğ‘Ÿ
againsttheoriginalmodel Mğ‘œonthe33simulatedtestingdatasets
Dğ‘¡. Thehigher improved accuracymeans the betterperformance
of the test sample selection methods.
4.5 Research Questions
WetrytoevaluatetheperformanceofMCPoneffectivenessand
efficiency. Three research questions are listed as follows.
â€¢RQ1(OverallEffectiveness) :WhetherMCPexceedsstate-
of-the-art test sample selection techniques in the overall
datasets?
â€¢RQ2(EffectivenessunderDifferentTestingContexts) :
Whether MCP exceeds state-of-the-art test sample selection
techniques under different testing contexts generated by
different synthetic operators?
â€¢RQ3(Efficiency) :IsMCPmoreefficientthanstate-of-the-
art test sample selection techniques?
5Usually,whenpeoplesaytestdata,thesamplesarealreadylabeled.Inthispaper, Dğ‘¡
indicates the whole set of â€œunseen dataâ€ or â€œfuture dataâ€ in the testing environment.
4154.6 Analysis Methods
To quantify the differences of accuracy performance, wefi rst apply
ğ‘-valuesoftheWilcoxonrank-sumtest[ 5]tocheckwhetherthe
differences between MCP and baseline methods are statistically
significant at the significance level of 0.05.
Second, we introduce another statistical analysis method Cliffâ€™s
deltağ›¿[36].Themagnitudeofdifferencesisusuallyassessedbythe
thresholds: |ğ›¿|<0.147asnegligible, 0 .147â‰¤|ğ›¿|<0.330assmall,
0.330â‰¤|ğ›¿|<0.474 as medium, and |ğ›¿|â‰¥0.474 as large.
Third, we introduce â€œW/T/Lâ€ [ 30] to denote our method per-
forming better than, equal to, or worse than the baseline methods.
For our method, we mark it as a â€œWinâ€, if (1) our method performs
significantly better according to the Wilcoxon rank-sum test (p -
value is less than 0.05), and (2) the magnitude of the difference
betweentwomethodsisnon-negligibleaccordingtoCliffâ€™sdelta
ğ›¿(ğ›¿â‰¥0.147). In contrast, we mark it as a â€œLoseâ€. Otherwise, we
mark it as a â€œTieâ€.
5 EXPERIMENT RESULTS
In this section, we specify the results of our RQs, along with our
motivations andfindings.
5.1 RQ1: Overall Effectiveness
Motivation and Approach. Our scenario is to sample a subset
from the unlabeled testing set by a certain ratio and then manually
labeltheselectedsamplesforretraining.Wewanttocheckwhether
MCP can work well, when the sampling ratio is very small, to save
label costs, which is the core concern of our method. To achieve
this aim, we compare the performance of MCP with the 5 baseline
methods in the overall33 (11Ã—3) simulated datasets under four
sampling ratios 1%, 3%, 5%, and 10%. Specifically, in each simulated
dataset, we use these six studied methods to sample the simulated
datasetsaccordingtothecertainsamplingratios,andemploythe
sampled subset to retrain the original model. We repeat the pro-
cess of retraining5 times and report 5 accuracyimprovements by
comparing original models and retrained models.
Results. Figure 4 gives an overview of MCP compared with
five baseline methods. The boxplots present the distribution of
average accuracy improvements on all simulated datasets grouped
by six methods under four sampling ratios 1%, 3%, 5%, and 10%.
The green points in the boxplots represent the mean values, while
the black lines represent the median values. In each subfigure, the
sixstudiedmethodsarerankedbythemedianvalues.Thecolors
of boxplots indicate the comparison results based on ğ‘-values and
Cliffâ€™s delta(see Section 4.6for detail): takingMCP (denoted with
red) as the benchmark, blue indicates that MCP is considerably
better than this method (i.e., ğ‘<0.05 andğ›¿â‰¥0.147), red indicates
that there is no considerably difference between MCP and thismethod (i.e.,
ğ‘>0.05 or|ğ›¿|<0.147); and purple indicates that
MCP is considerably worse than this method (i.e., ğ‘<0.05 and
ğ›¿â‰¤âˆ’0.147). Figure 4 shows that under four sampling ratios, MCP
has the best performance of accuracy improvements, i.e., with the
highestmedianandmeanvalues.Besides,under3(1%,3%and10%)out of 4 comparisons, MCP is considerably better than all the other
baseline methods.


âˆ’50510
MCP SRS CES DSA AAL LSAACC Improvement (%)
(a) Performance with 1% sampling





051015
MCP DSA CES AALSRS LSAACC Improvement (%)
(b) Performance with 3% sampling


051015
MCP AALDSA CES SRS LSAACC Improvement (%)
(c) Performance with 5% sampling


051015
MCP AALDSA CES SRS LSAACC Improvement (%)
(d) Performance with 10% sampling
Figure 4: The boxplots of accuracy improvements of MCP
and5baselinemethodsunder1%,3%,5%,and10%sampling.Blue color indicates that MCP is considerably better thanthis method when considering ğ‘-values and Cliffâ€™s delta,
while red color indicates that there is no considerably dif-
ference between MCP and this method.
Table 3 lists the detailed results of accuracy improvements with
thetop4methodsrankedbymedianvalues.InTable3,numbers
are means of accuracy improvements, and the best numbers are
highlighted in bold. In particular, an entry with a gray background
indicatesthatMCPwinswhencomparedwiththebaselinemodel
according to ğ‘-values (ğ‘<0.05) and Cliffâ€™s delta ( ğ›¿>0.147). In
addition,Table3providesaveragerank(AR)[ 23]ofeachmethod
overallsimulateddatasets,whichwellreflectshowthecorrespond-
ing method outperforms others by eliminating the influence of
extremevalues.The lastlineW/T/Lreportsthenumbers ofsimu-
lated datasets, on which MCP wins/ties/loses when compared with
baseline models.
From Table 3, we obtain the following observations: (a) Consid-
eringtheaveragevalues,MCPhasthebestaccuracyimprovementsamongthetop4methodsunderallthelabelingefforts.The
â†‘(%)line
shows the relative improvement rates (A-B)/B of average accuracy
improvements whencomparing MCP(A) with theother baselines
(B). As can be seen, the â†‘(%) values range from 10% to 150%; (b)
Highlightedbyboldfont,MCPachievesthebestperformanceon
nolessthan20(outof33)datasetsunderallfourlabellingefforts;
(c) In the view of AR, MCP has 1.758, 1.545, 1.727, and 1.864 under
1%,3%,5%,and10%labellingefforts,respectively,whicharealso
the best among the top 4 methods. The lower AR values show abetter applicability of MCP on different simulated datasets with
416Table 3: The Overall Results of Average Accuracy Improvements(%) in the TOP 4 Methods
Sample Ratio 1% 3% 5% 10%
DataSetMethodMCP DSA CES SRS MCP DSA CES AAL MCP DSA CES AAL MCP DSA CES AAL
Rotate_MNIST 3.182 1.511.922 1.836 4.994 4.853.246 3.766 5.696 5.562 4.084.752 6.396.652 6.352 5.124
Translate_MNIST 3.702 4.262 2.54 2.962 6.042 6.326 4.7587.152 7.388 7.632 6.6147.954 9.4 9.27 7.974 9.218
Shear_MNIST 4.458 1.768 1.371.198 6.886 4.736 4.388 5.302 8.276 7.076 5.766 7.546 9.65 9.374 7.624 9.478
Brightness_MNIST 3.39 2.292.064 1.876 4.254 3.582 3.198 3.634.548 4.284 3.466 3.936 4.962 4.652 4.088 4.764
Contrast_MNIST 4.246 4.352 2.9942.822 7.184 6.246 5.014 7.1 8.104 7.988 6.376 7.798 9.514 9.088 7.556 9.116
Scale_MNIST 4.388 1.721.662 2.588 7.028 5.394 5.714.782 8.522 8.438 7.446.982 10.4710.596 9.452 9.752
Rotate_CIFAR10 3.79 3.11 3.163.128 4.862 4.694 4.158 5.158 5.446 5.632 4.2485.902 6.196 6.574 5.114 6.468
Translate_CIFAR10 4.33 3.966 4.036 3.812 5.208 5.158 4.652 5.132 5.764 6.164.882 6.04 6.494 6.852 5.62 6.496
Shear_CIFAR10 3.022 3.044 2.513.152 3.778 4.528 3.466 4.06 4.392 5.314 3.99 4.972 5.952 6.122 4.946 6.004
Brightness_CIFAR10 2.718 2.114 0.952 2.12 3.908 4.172 2.806 2.922 4.282 4.958 3.446 4.156 5.866 5.75 4.176 5.774
Contrast_CIFAR10 3.338 2.69 3.21 2.982 4.484.296 3.8544.488 5.036 5.038 3.9425.322 5.922 5.956 4.46.114
Scale_CIFAR10 32.858 2.738 2.9 4.25 4.23.372 3.95 4.842 4.998 3.978 4.62 5.764 6.014 4.406 5.912
Rotate_SVHN 1.268 -0.32-0.466 -0.048 3.014 2.221.738-0.576 3.796 3.02 2.652.236 5.34 4.208 3.344 4.534
Translate_SVHN 0.914 1.086 0.304 0.902 3.31.018 2.89 0.248 5.008 2.032 3.946 0.804 6.636 3.816 4.71 4.302
Shear_SVHN 2.926 -1.524 2.302 1.844.708 1.108 4.332 1.908 5.696 0.494 4.782 2.27 7.02 2.468 6.042 4.322
Brightness_SVHN 1.038 -0.59 0.43-0.422 2.902 2.171.812-1.034 3.912 3.216 1.902 2.514 5.284 4.178 2.998 3.994
Contrast_SVHN 0.592 -0.406 -1.658 0.408 1.974 1.77 1.522 -0.952 3.348 2.62.191.698 5.434 3.96 3.492 3.49
Scale_SVHN 1.01-0.028 -0.048 -0.166 3.472 1.666 1.84-0.262 4.184 1.972 2.944 1.315.858 3.944 4.034 4.162
FGSM_MNIST 3.844.878 3.918 4.206 6.388 7.66 6.826 7.74 7.159.868 8.924 8.9 8.51212.692 10.442 12.062
JSMA_MNIST 10.994 9.554 8.336 7.9513.646 12.974 11.476 12.834 14.35 14.066 12.792 14.302 15.314 15.474 14.236 15.318
BIM-A_MNIST 17.606 16.1916.244 17.272 17.836 17.394 17.034 17.238 18.182 17.796 17.556 17.126 18.596 18.396 17.864 17.668
BIM-B_MNIST 0.192 6.744 5.674 5.11 0.236 6.692 6.688 6.756 0.662 6.62 6.782 6.89 6.332 9.74 6.79 7.436
C&W_MNIST 17.946 17.814 17.008 16.8818.376 18.216 17.9717.926 18.496 18.422 18.114 18.248 18.63 18.69 18.232 18.548
FGSM_CIFAR10 4.712 2.356 4.846 5.042 7.028 6.108 6.514 4.772 8.498.618 7.966 7.74210.536 10.536 9.142 10.386
JSMA_CIFAR10 1.812 -0.278 3.352 2.7 4.242.9064.276 3.516 5.196 4.814 4.854 5.91 7.712 7.766 5.788.216
BIM-A_CIFAR10 7.204 7.174 7.892 8.488 9.102 8.4389.468 8.64810.392 10.02 9.864 8.744 11.18 11.104 10.332 10.524
BIM-B-CIFAR10 4.498 1.721.086 4.116.712 5.533.302 3.064 7.467.616 5.306 6.94 9.27 9.19 7.898 9.034
C&W-CIFAR10 2.902-59.472 3.644 3.058 4.68-60.11 3.588-69.116 6.234 -51.28 4.21-48.754 7.762 1.926 6.072 4.35
FGSM_SVHN 1.544 0.360.292 0.742 5.444 3.152 2.514-11.678 6.92 5.204 3.784-5.384 9.424 7.224 5.612 -5.638
JSMA_SVHN 4.366 2.446 3.136 3.218 5.392 4.406 4.206 4.336 5.722 4.986 4.808 5.422 6.736 5.746 5.226 6.604
BIM-A_SVHN 4.104 2.406 3.558 2.464 5.266 4.466 4.336 4.086 5.694 4.866 4.558 5.112 6.532 5.778 5.236 6.53
BIM-B_SVHN 1.694 0.95-0.11 0.446 3.414 2.241.064 2.194 4.14 3.198 2.222.934 5.54 4.368 3.498 5.178
C&W_SVHN 0.484-62.552 0.55 0.804 6.354-20.942 2.648 6.072 9.25-17.516 8.976 8.424 12.826 2.9913.054 12.388
Average 4.097 -0.540 3.317 3.527 5.950 2.644 4.990 2.378 6.866 4.052 5.980 4.344 8.396 7.600 7.106 7.516
â†‘(%)/ - 24 16 / 125 19 150 / 6 91 55 8 /1 0 1 8 1 2
AR1.758 3.121 3.000 2.697 1.545 2.818 3.515 3.273 1.727 2.424 3.636 3.000 1.864 2.409 3.818 2.697
W/T/L / 26/3/4 24/4/5 20/11/2 / 24/5/4 27/4/2 24/4/5 / 18/9/6 30/1/2 20/6/7 / 13/17/3 30/2/1 16/14/3
the baseline methods; (d) The W/T/L result also reports that MCP
performsconsiderablybetterthanalltheotherbaselinemethodson
the most of simulated datasets. All above results indicate that MCP
can significantly increase the overall effectiveness of retraining DL
models.
Answer to RQ1: We observe that MCP performs significantly
better than the other baseline methods when considering the
overall effectiveness of retraining DL models.
5.2 RQ2: Effectiveness under Different Testing
Contexts
Motivation and Approach. As mentioned in Section 4.2, we em-
ploy synthetic operators to simulate the testing contexts. Image
transformation can mimic the real-world phenomena, e.g., the im-
age with cameralens distortions could besimulated by the Rotate
operator,andtheimagewithdifferentbackgroundconditionscould
be simulated by the Brightness operator.
Similarly,adversarialattacksmimicadeliberateviciousattack
forthepurposeofattackingthesystembyelaboratelyconstructing
the input which is only slightly different from the original input.
In this RQ, we want to observe the performance of all six methodsunder different kind of synthetic samples generated from syntheticoperators,andcheckwhetherMCPismoreeffectivethantheother
methods.
Results. Figure 5 shows the detailed results of accuracy im-
provements under 11 synthetic operators. Take Rotate as an ex-
ample,wegatheralltheresultsofthreedatasetâ€œRotate_MNISTâ€,
â€œRotate_CIFAR10â€ and â€œRotate_SVHNâ€ generated by the Rotate op-
erator, then compare all the average accuracy improvements of6 compared methods. Similar to Figure 4, the white points in the
boxplotsrepresentthemeanvalues,whiletheblacklinesrepresent
themedianvalues;blueindicatesourmethodisconsiderablybet-
terthanthecomparedmethodwhenconsideringon ğ‘-valuesand
Cliffâ€™sdelta,redmeans therearenoconsiderably differences,and
purple indicates that MCP is considerably worse.
As can be seen in Figure 5, we observe that (a) MCP has the
best mean values and the median values in most (i.e., 10 out of
11) testing contexts; (b) The color of boxplots shows that MCP
performssignificantlybetterthanallthefivebaselinemethodsin
most(i.e.,6outof11)testingcontexts,i.e.,alltheothermethodare
coloredasblue;(c)ThoughMCPisnotthebestmethodwiththe
BIM-Boperators,MCPisstillcomparabletotheothermethods,i.e.,
therearenoconsiderablydifferencesbetweenMCPandtheother
baseline methods with red colors.
417







Rotate Translate Shear Brightness Contrast Scale
MCPLSADSAAALCESSRSMCPLSADSAAALCESSRSMCPLSADSAAALCESSRSMCPLSADSAAALCESSRSMCPLSADSAAALCESSRSMCPLSADSAAALCESSRSâˆ’50510ACC Improvement
(a) performance under image transformation


FGSM JSMA C&W BIMâˆ’A BIMâˆ’B
MCPLSADSAAALCESSRS MCPLSADSAAALCESSRS MCPLSADSAAALCESSRS MCPLSADSAAALCESSRS MCPLSADSAAALCESSRSâˆ’50âˆ’250ACC Improvement
(b) performance under adversarial attack
Figure 5: Results of accuracy improvements under different simulated testing contexts generated by 11 synthetic operators.
Answer to RQ2: We observe that in most testing contexts,
MCP has the best performance of mean and median values,
and is considerably better than all other methods.
5.3 RQ3: Efficiency
Motivation and Approach. In the previous RQs, we have ob-
served that the subset selected by MCP outperforms all the com-
pared baseline methods considering the accuracy improvementsafter retraining. As selecting the subset of testing dataset is time
consuming, in this RQ we want to check whether our method has
obvious advantages in efficiency (i.e., with lower time cost).
Result.Table4showsthetotaltimecostsofall6methodswhen
sampling10%6imagesfrom11simulateddatasetsgeneratedfrom
three original testing dataset from MNIST, CIFAR10 and SVHN, re-
spectively.Forexample,â€œ28â€showsthatthetimecostis28seconds,
whenwesample10%inputsfromallthe11MNISTsimulatedtest
datasetsbyourproposedmethodMCP.FromTable4,weobserve
that with the exception of the random method SRS, other methods
cost several times (at least 2 times just as 1796/836=2.15) or even
hundreds of times more time than our method.
AnswertoRQ3:Ourmethodisnotonlyeffectiveinretraining
DLmodelswithhigheraccuracyimprovements,butalsohasadvantages in time costs.
6As10%isthemaxsamplingratioweconsiderinthispaper,wemeasuretheefficiency
with 10% sampling.Table 4: The time costs (second) of all 6 methods when sam-
pling 10% from all simulated Datasets
DATASET MCPLSADSAAALCES SRS
MNIST 284853,629NAâˆ—702 0.053
CIFAR10 6132,21214783NA170,132 0.207
SVHN 8362,6526,102NA1,796 0.142
*When the time cost is larger than 24 hours, we denote it as NA.
6 DISCUSSION
We further discuss the aims and results of our method in the fol-
lowing three aspects.
6.1 Performance under different original
accuracies
Asshown inTable 2,ourstudied modelsperform on33simulated
testing dataset with different original accuracy from 79.84% to
94.19%. In this subsection, we try to evaluate the effects of original
accuracies on the performance of our studied methods. We sort
all the simulated datasets according to the original accuracies, and
group theminto four parts thefirst quarter, thesecondquarter, the
third quarter, and the last quarter, e.g., thefi rst quarter contains
the 25% of datasets with the lowest original accuracies.
Figure 6 shows the detailed results of accuracy improvements of
thesixstudiedmethodsunderfouroriginalaccuracyclasses.Similar
to Figure 4 and 5, in Figure 6, red means there are no considerably
differences, and blue indicates our method is considerably better
than the compared method.
FromFigure6,weobservethatourmethodcanwinalltheother
methods in 3 quarters: the second, the third and the last. In the
first quarter, our method wins two of the baseline methods and do
418

first quarter second quarter third quarter last quarter
MCPLSADSAAALCESSRS MCPLSADSAAALCESSRS MCPLSADSAAALCESSRS MCPLSADSAAALCESSRSâˆ’1001020ACC Improvement
Figure6:Resultsofaccuracyimprovementsunderfourorig-
inal accuracy classes
not lose to any method. In addition, considering the mean value
(whitepointsinthebox)ormedianvalue(blacklinesinthebox),
our method still ranksfi rst in all quarters.
6.2 Retraining vs. Finding Faults
Inthepoint-of-viewoftraditionalsoftwareengineering,testingcanbeconsideredasameanstoachievereliability[
9].Themaincontent
oftestingistogenerateorselecterror-triggeringinputs(i.e.,test
cases)tofindthefaultsofsoftware.Ascanbeseen,findingfaults
is stillone of the coretargets of current DLtesting [25, 34,40, 51].
In a traditional software,fi nding faults are actionable for the de-
velopers of the software, since they are actually helpful to improve
the reliability of the software: for each fault, the developers canlocatethe buggy code, and fixit. Compared with the traditional
software, DL models follow a data-driven programming paradigm,
wherethecoreunderlyinglogicofDLmodelsisobtainedviathe
distribution of training data under the neural network architecture
(e.g., layer numbers and neuron numbers). The fundamental differ-
ences between DL models and traditional software have weakened
the actionability offi nding faults in the following two respects:
â€¢Difficultto locatethefaultsofDLmodels.Thecomplexstruc-
ture of DL models obviously diminishes the probability to
locate the buggy part of the neural network architecture in
DL models. To our knowledge, there are no obvious connec-
tionsbetweentheerroroutputsandsomespecificareasof
the neural network.
â€¢Difficult to fixthe faults of DL models. As a result of the
statistical nature, DL models can not guarantee to predictcorrectly when dealing with a single input, which meansalthough the developers know the bugs, they may fail to
retrain DL models tofi x it.
The above limitations bring us back to the angel of statistics
on DL testing, e.g., retraining DL models, which may be more
actionable to improve the reliability of DL models.6.3 Similarities and differences with Active
Learning
Herewediscussthesimilaritiesanddifferencesbetweenourmethod
MCP in DL testing and AI methods in Active Learning. Both meth-
ods aim to save labeling costs and select the unlabeled instances
withhigherprioritytolabel,whichassumeaninstancewithhigher
priority is more effective to train DL models.
ItisworthemphasizingthatourmethodMCPhassomeinherent
differences with methods in Active Learning:
â€¢Application scenario. Active Learning is usually focusing
ontrainingscenarios,inwhichthereisapoolofunlabeled
instances, and Active Learning methods select samples totrain DL models. Meanwhile, our method focuses on test-
ingscenarios,inwhichtheunlabeledtestdatasetfromthe
testing contexts shares common components but have non-
ignorable differences with training dataset used to train DL
models.
â€¢Training Frequency. In the training scenario, developers
couldtrainDLmodelsin manyroundswith tinyincrements
of labeled samples recommended by Active Learning meth-
ods. On the contrary, there is an obvious division of laborbetween the developers and the testers in software engi-
neering. Our method MCP is designed as one timefeedback
technologyfor testers toreport enoughefficient samplesto
developers for retraining.
7 THREATS TO VALIDITY
Four aspects may become the threats to validity of this paper.
First, the selection of DL models could be a threat to validity. In
thispaper,wetrytoalleviatethisissuebyapplyingthreefamousDL
network structures and training three DL models with competitive
prediction accuracies.
Second, selection of subject dataset could be a threat to validity.
We use three famous multiple-classification datasets and have gen-
erated as many simulated datasets as possible, eliminating some
contingency.Inthefuturewewillevaluateourtechnologyonmany
different kinds of image datasets as well as some natural language
datasets in the form of speechand text.
Third, the simulation of data distribution shift could also be a
threattovalidity.Bothadversarialexamplesandtransformation-generated examples in the synthesized test cases may, to some
degree,representrobustnessissues.Moresimulationmethodsare
needed to verify our results.
Lastbutveryimportantthreatishowwecanguaranteethatour
synthetic dataset does not deviate much from the original train-
ing dataset. We have manually checked the synthetic datasets. For
each simulated dataset containing synthetic images, we have man-
ually checked 100 pictures to affirm that human can recognize the
synthetic images the same as the original one.
8 RELATED WORK
In this section, we propose three aspects of the related work.
TestingDLSoftware. More and more researchers are focusing
on the testingof well trained DL software artifacts.Many conven-
tionalconceptsortechniquesinnormalsoftwaretestinghavebeen
tuned and applied to DL software testing.
419In DeepXplore [ 34], Pei et al.fi rstly proposed neuron activation
coveragetoevaluatetestadequacyinDLsoftware.InDeepGauge
[25], Ma et al. introduced 5 kinds of multi-granularity coverage
criteria which are deemed to reflect behaviors of DL software in
finer granularity. In DeepCT [ 24], Ma et al. proposed a combinato-
rialtesting (CT)coverage guidedtestgeneration techniquewhich
contains several CT criteria formed for DL systems.
Besides,inDeepTest[ 44],metamorphictestingwhichcanrelieve
oracle problem has been applied in DL software to generate novel
test cases. DeepRoad [ 51] uses the technology of GAN to generate
self-drivingimagesinvariousweathercircumstancesforDLtest-
ing automatically. DeepHunter [ 47] is a fuzzing testing framework
guided by coverage which can generate abundant test cases for
detecting potential faults of general-service DL software. TransRe-pair [
41] combines mutation with metamorphic testing to test and
renovate the consistency of machine translation software.
New perspectives and newfi ndings have also been applied to
DL software testing. DeepBillboard [ 53] is a systematic real-world
testingapproachforpracticalself-drivingsystemsbygeneratingre-silient and robust printable adversarial billboards. DeepImportance
[11]isasystematictestingframeworkcontainingatestadequacy
criterion motivated by importances for DL systems, which can es-
tablish a functional layer-wise understanding of the DL systemcomponents and assess the semantic diversity of a test suite. In
DeepInspect [ 45], Chen et al. presented a testing method to detect
category-based confusion and bias errors, not per-image violations
in DNN-driven imageclassification software automatically. Other
works about therealistic failure [ 52], repair [ 16], and deployment
[13] of DL software, which are related to platforms, frameworks,
andmodelparameters/structures,arealsopublishedbyresearchers
in this area.
ImprovingDLSoftware. Theexistingstudiesaboutimproving
DL software are mainly from the aspects of training dataset refine-
ment and weight values of DL software. To alleviate the lackingof representative training data, researchers proposed some data
augmentation[ 8]anddatagenerationtechniques[ 12].Withhelpof
these technologies, MODE[ 28] can generateand select new train-
inginputsbyconductingmodelstatedifferentialanalysis.Gaoet
al.proposedatechnology[ 10]thatre-purposesthetechnologyof
fuzzing testing based on mutation to augment the training data of
DNNs, with the aim of strengthening the robustness of DL models.
In addition to augmenting training data, some researchers directly
updatetheundesirableweightvalues.Apricot[ 49]canimproveDL
modelsiterativelybyanovelweight-adaptationapproachwhich
generates many reduced DL models (rDLM) providing insights on
theadjustmentmagnitudeanddirectionoftheweightstohandle
the misclassified test cases.
ActiveLearning. Aimingtoreducelabelingeffortintraininga
goodclassifier,the techniqueofactivelearninghasbeenappliedin
controlling the labeling process in deep learning.
As traditional active learning are ineffective when applied to
deeplearninginthesettingofbatchtraining,Ozanetal.[ 38]de-
finedthisproblemascore-setselection,i.e.,choosingasetofpointscompetitive for the remaining points. Their experiments show that
the proposed approach obviously surpasses existing approachesin image classification experiments by a wide margin. But core-set selection would be prohibitively expensive to apply in deeplearning because they depend on feature representations that need
to be learned. Cody et al. [ 7] showed that they could greatly im-
provethecomputationalefficiencybyusingasmallproxymodel
to perform data selection for active learning. Experimental results
show that this â€œselection via proxyâ€ (SVP) approach can give an
orderofmagnitudeimprovementindataselectionruntimewithout
harming thefi nal accuracy of the target. William et al. [ 4] com-
paredensemble-based approacheswithMonte-CarloDropout and
geometric approaches for active learning with CNN classifiers and
high-dimensionaldata. Theyfound thatensemble-basedmethodsshow better performances and lead to more correct predictive un-
certainties,whicharethebasisformanyactivelearningalgorithms.
9 CONCLUSION
Based on the statistically-orientated nature, the quality of DL mod-
els is evaluated statistically on the performance of all testing in-
puts collected from a specific application context. The lower-than-
expected performance of DL models is inherently the result of
differentdistributionsofthetrainingdataandthetestingdata.OneofthemaintasksoftestersforDLmodelsistocollectusefulinputs
fromthetestingcontextandreportthemtothedevelopersforre-
training.The factof expensivelabelingcost greatlyweakens thepossibility of employing enough labeled inputs to retrain the DL
models.
To overcome it, we propose Multiple-Boundary Clustering and
Prioritization (MCP), a technique to cluster the samples into the
boundaryareas of multipleboundaries forDL modelsand specify
the priority to select samples evenly from all boundary areas, to
makesureenoughusefulsamplesforeachboundaryreconstruction.
The resultsof the experimentsdemonstrate that ourmethod MCP
is very effective in the retraining of DL model. At the same time,
the time costs of MCP are also lower than baseline methods.
In the future, we will explore whether our MCP works well
insuchtestingscenario:thedistributionsoftrainingdatasetand
testing(unseen)datasetdifferalot,e.g.,theaccuracyoftheDNN
model under retraining on the testing dataset is very low. We want
to know whether the low accuracy will affect the reliability of
predictive probabilities, and then affect the selection of the testing
subset.
10 REPEATABILITY
WeprovidealldatasetsandPythoncodeusedtoconductthisstudy
at https://github.com/actionabletest/MCP.
ACKNOWLEDGEMENTS
The work is supported by National Key R&D Program of China(Grant No. 2018YFB1003901) and the National Natural Science
Foundation of China (Grant No. 61932012, 61872177, 61832009,
and 61772259). We thank the anonymous referees for their helpful
comments on this paper.
REFERENCES
[1]2019.Discoverthecurrentstateoftheartinobjectsclassification.https://rodrigob.
github.io/are_we_there_yet/build/classification_datasets_results.html. Accessed
May 12, 2019.
[2]2019. Softmaxfunction. https://en.wikipedia.org/wiki/Softmax_function/. Ac-
cessed May 4, 2019.
420[3]Dario Amodei, Rishita Anubhai, Eric Battenberg, Carl Case, Jared Casper, Bryan
Catanzaro, Jingdong Chen, Mike Chrzanowski, Adam Coates, Greg Diamos,
Erich Elsen, Jesse Engel, Linxi Fan, Christopher Fougner, Tony Han, Awni Y.
Hannun, Billy Jun, Patrick LeGresley, Libby Lin, Sharan Narang, Andrew Y. Ng,
SherjilOzair,RyanPrenger,JonathanRaiman,SanjeevSatheesh,DavidSeetapun,
ShubhoSengupta,YandongWang,Zhiqian Wang,ChongWang,BoXiao,Dani
Yogatama,JunZhan,andZhenyaoZhu.2016. DeepSpeech2:End-to-EndSpeech
Recognition in English and Mandarin. In ICML.
[4]WilliamH.Beluch,TimGenewein,AndreasNurnberger,andJanM.Kohler.2018.
The Power of Ensembles for Active Learning in Image Classification. In 2018
IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR).
[5]L. De Capitani and D. De Martini. 2011. On stochastic orderings of the WilcoxonRankSumteststatisticWithapplicationstoreproducibilityprobabilityestimation
testing.Statistics and Probability Letters 81, 8 (2011), 937â€“946.
[6]ChenyiChen,AriSeff,AlainKornhauser,andJianxiongXiao.2015. Deepdriving:
Learning affordance for direct perception in autonomous driving. In Proceedings
of the IEEE International Conference on Computer Vision. 2722â€“2730.
[7]CodyColeman,ChristopherYeh,StephenMussmann,BaharanMirzasoleiman,
Peter Bailis, Percy Liang, Jure Leskovec, and Matei Zaharia. [n.d.]. Selection via
proxy: Efficient data selection for deep learning.
[8]EkinDogusCubuk,BarretZoph,DandelionMane,VijayVasudevan,andQuocV.
Le. 2019. AutoAugment: Learning Augmentation Policies from Data. https:
//arxiv.org/pdf/1805.09501.pdf
[9]P. G. Frankl, R. G. Hamlet, B. Littlewood, and L. Strigini. 1998. Evaluatingtesting methods by delivered reliability [software]. Software Engineering IEEE
Transactions on 24, 8 (1998), 586â€“601.
[10]XiangGao,RiponKSaha,MukulRPrasad,andAbhikRoychoudhury.2020. Fuzz
TestingbasedDataAugmentationtoImproveRobustnessofDeepNeuralNet-
works.In Proceedingsofthe42thInternationalConferenceonSoftwareEngineering
(ICSE â€™20).
[11]Simos Gerasimou, Hasan Ferit Eniser, Alper Sen, and Alper Cakan. 2020.Importance-Driven Deep Learning System Testing. In Proceedings of the 42th
International Conference on Software Engineering (ICSE â€™20).
[12]IanGoodfellow,JeanPouget-Abadie,MehdiMirza,BingXu,DavidWarde-Farley,
Sherjil Ozair, Aaron Courville, and Yoshua Bengio. 2014. Generative Adversarial
Nets. InAdvancesinNeuralInformationProcessingSystems27,Z.Ghahramani,
M.Welling,C.Cortes,N.D.Lawrence,andK.Q.Weinberger(Eds.).CurranAsso-
ciates, Inc., 2672â€“2680. http://papers.nips.cc/paper/5423-generative-adversarial-
nets.pdf
[13]QianyuGuo,SenChen,XiaofeiXie,LeiMa,QiangHu,HongtaoLiu,YangLiu,
Jianjun Zhao, and Xiaohong Li. 2019. An empirical study towards characterizing
deep learning development and deployment across different frameworks and
platforms.In 201934thIEEE/ACMInternationalConferenceonAutomatedSoftware
Engineering (ASE). IEEE, 810â€“822.
[14]JavadHamidzadeh,RezaMonsefi,andHadiSadoghiYazdi.2015. IRAHC:Instance
Reduction Algorithm using Hyperrectangle Clustering. Pattern Recogn. 48, 5
(May 2015), 1878â€“1889. https://doi.org/10.1016/j.patcog.2014.11.005
[15]Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger.2017. Densely connected convolutional networks. In Proceedings of the IEEE
conference on computer vision and pattern recognition. 4700â€“4708.
[16]Md Johirul Islam, Rangeet Pan, Giang Nguyen, and Hridesh Rajan. 2020. Repair-
ingDeepNeuralNetworks:FixPatternsandChallenges.In 42ndInternational
Conference on Software Engineering.
[17]Jinhan Kim, Robert Feldt, and ShinYoo.2019. Guiding Deep LearningSystem
Testing Using Surprise Adequacy. In Proceedings of the 41st International Confer-
ence on Software Engineering (Montreal, Quebec, Canada) (ICSE â€™19). IEEE Press,
Piscataway, NJ, USA, 1039â€“1049. https://doi.org/10.1109/ICSE.2019.00108
[18] YannLeCun,YoshuaBengio,andGeoffreyHinton.2015. Deeplearning. nature
521, 7553 (2015), 436.
[19]Y.LeCun andC.Cortes.2019. TheMNISTdatabase ofhandwrittendigits. http:
//yann.lecun.com/exdb/mnist/. Accessed May 4, 2019.
[20]Xin Li and Yuhong Guo. 2013. Adaptive active learning for image classification.
InProceedings of the IEEE Conference on Computer Vision and Pattern Recognition.
859â€“866.
[21]Zenan Li, Xiaoxing Ma, Chang Xu, Chun Cao, Jingwei Xu, and Jian LÃ¼. 2019.Boosting Operational DNN Testing Efficiency Through Conditioning. In Pro-
ceedings of the 2019 27th ACM Joint Meeting on European Software Engineer-ing Conference and Symposium on the Foundations of Software Engineering
(Tallinn,Estonia) (ESEC/FSE2019).ACM,NewYork,NY,USA,499â€“509. https:
//doi.org/10.1145/3338906.3338930
[22]MingLiangandXiaolinHu.2015. Recurrentconvolutionalneuralnetworkfor
objectrecognition.In ProceedingsoftheIEEEconferenceoncomputervisionand
pattern recognition. 3367â€“3375.
[23]Yibin Liu, Yanhui Li, Jianbo Guo, Yuming Zhou, and Baowen Xu. 2018. Con-necting software metrics across versions to predict defects. In 25th Interna-
tional Conference on Software Analysis, Evolution and Reengineering, SANER2018, Campobasso, Italy, March 20-23, 2018, Rocco Oliveto, Massimiliano DiPenta, and David C. Shepherd (Eds.). IEEE Computer Society, 232â€“243. https://doi.org/10.1109/SANER.2018.8330212
[24]L. Ma, F. Juefei-Xu, M. Xue, B. Li, L. Li, Y. Liu, and J. Zhao. 2019. DeepCT:
Tomographic Combinatorial Testing for Deep Learning Systems. In 2019 IEEE
26thInternationalConferenceonSoftwareAnalysis,EvolutionandReengineering
(SANER). 614â€“618. https://doi.org/10.1109/SANER.2019.8668044
[25]LeiMa,FelixJuefei-Xu,FuyuanZhang,JiyuanSun,MinhuiXue,BoLi,ChunyangChen,TingSu,LiLi,YangLiu,JianjunZhao,andYadongWang.2018. DeepGauge:
multi-granularity testing criteria for deep learning systems. In Proceedings of the
33rd ACM/IEEE International Conference on Automated Software Engineering, ASE
2018,Montpellier,France,September3-7,2018.120â€“131. https://doi.org/10.1145/
3238147.3238202
[26]Lei Ma, Fuyuan Zhang, Jiyuan Sun, Minhui Xue, Bo Li, Felix Juefei-Xu, Chao
Xie, Li Li, Yang Liu, Jianjun Zhao, et al .2018. Deepmutation: Mutation testing of
deeplearningsystems.In 2018IEEE29thInternationalSymposiumonSoftware
Reliability Engineering (ISSRE). IEEE, 100â€“111.
[27]L.Ma,F.Zhang,J.Sun,M.Xue,B.Li,F.Juefei-Xu,C.Xie,L.Li,Y.Liu,J.Zhao,
and Y. Wang. 2018. DeepMutation: Mutation Testing of Deep Learning Systems.
In2018 IEEE 29th International Symposium on Software Reliability Engineering
(ISSRE). 100â€“111. https://doi.org/10.1109/ISSRE.2018.00021
[28]ShiqingMa,YingqiLiu,Wen-ChuanLee,XiangyuZhang,andAnanthGrama.
2018. MODE:AutomatedNeuralNetworkModelDebuggingviaStateDifferential
Analysis andInput Selection. In Proceedings ofthe 2018 26thACM Joint Meeting
on European Software Engineering Conference and Symposium on the Foundations
ofSoftware Engineering (LakeBuena Vista,FL, USA) (ESEC/FSE 2018).ACM,New
York, NY, USA, 175â€“186. https://doi.org/10.1145/3236024.3236082
[29]JunhuaMao,XuWei,YiYang,JiangWang,ZhihengHuang,andAlanLYuille.
2015. Learning like a child: Fast novel visual concept learning from sentencedescriptions of images. In Proceedings of the IEEE international conference on
computer vision. 2533â€“2541.
[30]J. Nam, W. Fu, S. Kim, T. Menzies, and L. Tan. 2018. Heterogeneous Defect
Prediction. IEEE Transactions on Software Engineering 44, 9 (Sep. 2018), 874â€“896.
https://doi.org/10.1109/TSE.2017.2720603
[31]Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and An-drew Y Ng. 2011. Reading digits in natural images with unsupervised feature
learning. (2011).
[32]N.Krizhevsky,H.Vinod,C.Geoffrey,M.Papadakis,andA.Ventresque.[n.d.]. The
cifar-10 dataset. http://www.cs.toronto.edu/~kriz/cifar.html. Accessed May 4,
2019.
[33]Nicolas Papernot, Fartash Faghri, Nicholas Carlini, Ian Goodfellow, Reuben Fein-
man,AlexeyKurakin,CihangXie,YashSharma,TomBrown,AurkoRoy,etal .
2016. Technical report on the cleverhans v2. 1.0 adversarial examples library.
arXiv preprint arXiv:1610.00768 (2016).
[34] KexinPei,YinzhiCao,JunfengYang,andSumanJana.2017. DeepXplore:Auto-
mated Whitebox Testing of Deep Learning Systems. In Proceedings of the 26th
Symposium on Operating Systems Principles, Shanghai, China, October 28-31, 2017.
1â€“18. https://doi.org/10.1145/3132747.3132785
[35]Dong Ping and N. P Galatsanos. 2002. Affine transformation resistant water-marking based on image normalization. In International Conference on Image
Processing.
[36]J. Romano, J. D.Kromrey,J. Coraggio,J. Skowronek,and L.Devine. 2006. Explor-
ing methods for evaluating group differences on the NSSE and other surveys:
Arethet-testandcohenâ€™sdindicesthemostappropriatechoices.In Inannual
meeting of the Southern Association for Institutional Research.
[37]JÃ¼rgen Schmidhuber. 2015. Deep learning in neural networks: An overview.
Neural networks 61 (2015), 85â€“117.
[38]OzanSenerandSilvioSavarese.2018. Activelearningforconvolutionalneural
networks: A core-set approach. In International Conference on Learning Represen-
tations(ICLR 2018).
[39]Burr Settles. 2009. Active learning literature survey. Technical Report. University
of Wisconsin-Madison Department of Computer Sciences.
[40]QingkaiShi,JunWan,YangFeng,ChunrongFang,andZhenyuChen.2019. Deep-
Gini:PrioritizingMassiveTeststoReduceLabelingCost. CoRRabs/1903.00661
(2019). arXiv:1903.00661 http://arxiv.org/abs/1903.00661
[41]Zeyu Sun, Jie M Zhang, Mark Harman, Mike Papadakis, and Lu Zhang. 2020.
AutomaticTestingandImprovementofMachineTranslation.In Proceedingsof
the 42th International Conference on Software Engineering (ICSE â€™20).
[42]IlyaSutskever,OriolVinyals,andQuocVLe.2014. Sequencetosequencelearning
withneuralnetworks.In Advancesinneuralinformationprocessingsystems.3104â€“
3112.
[43]Bart Thomee, David A Shamma, Gerald Friedland, Benjamin Elizalde, Karl Ni,
Douglas Poland, Damian Borth, and Li-Jia Li. 2015. YFCC100M: The new data in
multimedia research. arXiv preprint arXiv:1503.01817 (2015).
[44]YuchiTian,KexinPei,SumanJana,andBaishakhiRay.2018.DeepTest:Automated
Testing of Deep-neural-network-driven Autonomous Cars. In Proceedings of the
40thInternationalConferenceonSoftwareEngineering (Gothenburg,Sweden) (ICSE
â€™18).ACM,NewYork,NY,USA,303â€“314. https://doi.org/10.1145/3180155.3180220
[45]YuchiTian,ZiyuanZhong,VicenteOrdonez,GailKaiser,andBaishakhiRay.2020.
Testing DNN Image Classifier for Confusion & Bias Errors. In 42nd International
421Conference on Software Engineering.
[46]Ji Wan, Dayong Wang, Steven Chu Hong Hoi, Pengcheng Wu, Jianke Zhu, Yong-
dong Zhang, and Jintao Li. 2014. Deep Learning for Content-Based Image Re-
trieval:AComprehensiveStudy.In Proceedingsofthe22NdACMInternational
Conference on Multimedia (Orlando, Florida, USA) (MM â€™14). ACM, New York,
NY, USA, 157â€“166. https://doi.org/10.1145/2647868.2654948
[47]XiaofeiXie,LeiMa,FelixJuefei-Xu,MinhuiXue,HongxuChen,YangLiu,JianjunZhao,BoLi,JianxiongYin,andSimonSee.2019. DeepHunter:ACoverage-guided
FuzzTestingFrameworkforDeepNeuralNetworks.In Proceedingsofthe28th
ACMSIGSOFTInternationalSymposiumonSoftwareTestingandAnalysis (Beijing,
China)(ISSTA 2019). ACM, New York, NY, USA, 146â€“157. https://doi.org/10.
1145/3293882.3330579
[48]Yang You, Aydin BuluÃ§, and James Demmel. 2017. Scaling Deep Learning on
GPUandKnightsLandingClusters.In ProceedingsoftheInternationalConference
for High Performance Computing, Networking, Storage and Analysis (Denver,
Colorado) (SCâ€™17).ACM,NewYork,NY,USA,Article9,12pages. https://doi.
org/10.1145/3126908.3126912[49]Hao Zhang and WK Chan. 2019. Apricot: A Weight-Adaptation Approach to
FixingDeepLearningModels.In 201934thIEEE/ACMInternationalConference
on Automated Software Engineering (ASE). IEEE, 376â€“387.
[50]JieMZhang,MarkHarman,LeiMa,andYangLiu.2020. Machinelearningtesting:
Survey, landscapes and horizons. IEEE Transactions on Software Engineering
(2020).
[51]Mengshi Zhang, Yuqun Zhang, Lingming Zhang, Cong Liu, and Sarfraz Khur-shid. 2018. DeepRoad: GAN-based metamorphic testing and input validation
frameworkforautonomousdrivingsystems.In Proceedingsofthe33rdACM/IEEE
InternationalConferenceonAutomatedSoftwareEngineering,ASE2018,Montpellier,
France, September 3-7, 2018. 132â€“142. https://doi.org/10.1145/3238147.3238187
[52]Ru Zhang, Wencong Xiao, Hongyu Zhang, Yu Liu, Haoxiang Lin, and Mao Yang.
2020. AnEmpiricalStudyonProgramFailuresofDeepLearningJobs.In 42nd
International Conference on Software Engineering.
[53]HushengZhou,WeiLi,YuankunZhu,YuqunZhang,BeiYu,LingmingZhang,andCongLiu.2020. Deepbillboard:Systematicphysical-worldtestingofautonomous
drivingsystems.In Proceedingsofthe42thInternationalConferenceonSoftware
Engineering (ICSE â€™20).
422