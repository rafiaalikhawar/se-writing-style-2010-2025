Training Data Debugging for the Fairness of Machine Learning
Software
Yanhui Li
State Key Laboratory for Novel
Software Technology, Nanjing
University, China
yanhuili@nju.edu.cnLinghan Mengâˆ—
State Key Laboratory for Novel
Software Technology, Nanjing
University, China
menglinghan@smail.nju.edu.cnLin Chenâ€ 
State Key Laboratory for Novel
Software Technology, Nanjing
University, China
lchen@nju.edu.cn
Li Yu
State Key Laboratory for Novel
Software Technology, Nanjing
University, China
yuli@smail.nju.edu.cnDi Wu
Momenta, Suzhou,
China
wudi@momenta.aiYuming Zhou
State Key Laboratory for Novel
Software Technology, Nanjing
University, China
zhouyuming@nju.edu.cn
Baowen Xu
State Key Laboratory for Novel
Software Technology, Nanjing
University, China
bwxu@nju.edu.cn
ABSTRACT
Withthewidespreadapplicationofmachinelearning(ML)software,
especiallyinhigh-risktasks,theconcernabouttheirunfairnesshas
beenraisedtowardsbothdevelopersandusersofMLsoftware.The
unfairness of ML software indicates the software behavior affected
bythesensitivefeatures(e.g.,sex),whichleadstobiasedandillegaldecisionsandhasbecomeaworthyproblemforthewholesoftware
engineering community.
According to the â€œdata-drivenâ€ programming paradigm of ML
software,weconsidertherootcauseoftheunfairnessasbiasedfea-
tures in training data. Inspired by software debugging, we propose
anovelmethod, Linear-regressionbased Training DataDebugging
(LTDD),to debugfeaturevaluesintrainingdata,i.e.,(a)identify
which features and which parts of them are biased, and (b) exclude
thebiasedpartsofsuchfeaturestorecoverasmuchvaluableand
unbiased information as possible to build fair ML software. We
conductanextensivestudyonninedatasetsandthreeclassifiers
to evaluate the effect of our method LTDD compared with fourbaseline methods. Experimental results show that (a) LTDD can
betterimprovethefairnessofMLsoftwarewithlessorcomparable
damagetotheperformance,and(b)LTDDismoreactionablefor
fairness improvement in realistic scenarios.
âˆ—Linghan Meng is the co-first author.
â€ Lin Chen is the corresponding author.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation
on the first page. Copyrights for components of this work owned by others than ACMmustbehonored.Abstractingwithcreditispermitted.Tocopyotherwise,orrepublish,topostonserversortoredistributetolists,requirespriorspecificpermissionand/ora
fee. Request permissions from permissions@acm.org.
ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
Â© 2022 Association for Computing Machinery.
ACM ISBN 978-1-4503-9221-1/22/05...$15.00
https://doi.org/10.1145/3510003.3510091KEYWORDS
Debugging, Fairness, ML Software, Training Data
ACM Reference Format:
YanhuiLi,LinghanMeng,LinChen,LiYu,DiWu,YumingZhou,andBaowen
Xu.2022.TrainingDataDebuggingfortheFairnessofMachineLearning
Software.In 44thInternationalConferenceonSoftwareEngineering(ICSEâ€™22),
May 21â€“29, 2022, Pittsburgh, PA, USA. ACM, New York, NY, USA, 13 pages.
https://doi.org/10.1145/3510003.3510091
1 INTRODUCTION
Machinelearning(ML)softwarehaspenetratedmanyaspectsofour
dailylives,whoseresultsareemployedinhigh-stakeapplications
to make decisions or predictions. For example, people have applied
ML software to identify credit risks [ 2], to prove loan applications
[4],topredictheartdisease[ 1],andeventoestimatereoffending
probabilities[ 10].AlongwiththewidespreadusageofMLsoftware,
theconcernaboutitsfairnesshasalsobeenraisedtowardsdevel-
opers, users, and regulators of ML software. The unfairness of ML
softwareusuallyindicatesthediscriminatorytreatmentofdifferent
groups divided by sensitive features [ 40], e.g., sex. According to
biased decisions made by ML software, one group (e.g., male) may
haveaparticularadvantage,i.e.,withmoreopportunitytoobtain
â€œfavorableâ€ decisions1, called privileged, over the other group (e.g.,
female), called unprivileged. Under such circumstances, the biased
treatment not only undermines the objectivity of ML software but
also violates some anti-discrimination laws [14].
The fairness of ML software has been considered a worthy soft-
ware engineering(SE) research problem[ 46], wherefairness has
cometobeseenasacorenon-functionalqualityproperty[ 47]of
MLsoftware.BrunandMeliou[ 20]reportedanewkindofsoftware
defect, â€œfairness defectâ€, which is related to software behavior in
1Toillustrate,forloanapplications,â€œapprovalâ€isthefavorabledecision,andâ€œrejectionâ€
is the unfavorable decision.
22152022 IEEE/ACM 44th International Conference on Software Engineering (ICSE)
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:54:36 UTC from IEEE Xplore.  Restrictions apply. ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA Yanhui Li, Linghan Meng, Lin Chen, Li Yu, Di Wu, Yuming Zhou, and Baowen Xu
a biased manner and needs to be tackled. Chakraborty et al. [ 25]
arguedthat,whendiscoveringunfairproblemswithMLsoftware,
it is the job of software engineers to reduce discrimination. Biswas
and Rajan [ 18] conducted an empirical evaluation of fairness on 40
real-world ML software under a comprehensive set of fairness indi-
cators.Theyobservedthatperformanceoptimizationtechniques
might lead to unfairness.
Followingthedata-drivenprogrammingparadigm[ 47],MLsoft-
wareobtainsitsdecisionlogicfromtrainingdata[ 13].Thebehavior
of ML software to a large extent is determined by the quality of
training data [ 44], i.e., biased training data may lead to the trained
ML software with statistical discrimination. Researchers have tried
toexplaintheeffectoftrainingdataonthefairnessofMLsoftware
from the following angles:
â€¢The size of feature sets . Zhang and Harman [ 46] compared
the results of ML software with different sizes of feature
sets and reported that enlarging the size of feature sets in
training data would increase ML software fairness.
â€¢Biasedlabelsandimbalanceddistributions.Chakraborty etal.
[23] assumed that label information might contain bias and
proposed a novel method, Fairway, to remove training sam-
pleswithbiasedlabels.Recently,they[ 22]postulatedroot
causes of unfairness as a combination of biased labels and
imbalancedinternaldistributionsandproposedFair-Smote
to remove biased labels and rebalance internal distributions.
Thispaperdeviatesfrompreviousstudies,asitadoptsadifferent
angle: the root cause of the unfairness could be biased features in
trainingdata.Here,wedefinebiasedfeaturesas featuresthatare
highlyrelatedtothesensitivefeatureintrainingdata.Inthedevelop-
mentprocessof MLsoftware,developershavenoticed thatbiased
featuresmightleadtounfairbehaviorsofMLsoftware.Asreported
inAmazonfairnessissues[ 8],whendevelopersconstructedAma-
zonâ€™s same-day delivery service,they observed that some features
in the training data are highly related to the race (the sensitive fea-
ture that is already excluded in the training process) of customers,
e.g., the zipcode feature could obviously deduce whether the deliv-
eryaddressisinwhiteornon-whitecommunities.Asaresult,even
though thedelivery serviceruns withoutobtaining the raceinfor-
mation,itstillprefersthedeliveryaddresswiththezipcodeinwhite
communities, and causes unfairness between white and non-white
customers. This kind of fairness issue about biased features has
beenconsideredfairnessbugs[ 20],whichasksfornovelmethods
todebug(formallyfindandresolve)biase dfeatures.However,to
ourknowledge,therearenopreviousstudiesaboutMLsoftware
debugging techniques focusing on biased features.
Toobtainunbiasedfeatures,wetryto debugfeaturevaluesin
training data, i.e., (a) identify which features and which parts of
themarebiased,and(b)excludethebiasedpartsofsuchfeatures
torecoverasmuchusefulandunbiasedinformationaspossible.To
achieve this goal, we propose a novel method, Linear-regression
basedTraining DataDebugging( LTDD),whichemploysthelinear
regression model [ 50] to identify the biased features (see details
inSection3).Onthewhole,ourmethodcomprisesthefollowing
three steps: (a) adopting Wald test [ 30] withğ‘¡-distribution to check
whether the features contain significant bias, (b) estimating the
biased parts of training features as explained variances [ 49]i nt h eoriginal feature values by the sensitive feature, and (c) removing
thebiasedpartsoftrainingfeaturesbysubtractingtheestimated
values from original feature values to construct fair ML software.
To evaluate our method LTDD, we conduct an extensive empir-
ical study on nine tasks [ 1â€“3,5â€“7,9,10] with common sensitive
features (sex, race, and age) under three widely used ML classifiers,
LogisticRegression,NaiveBayes,andSVM.Toassesstheperfor-
manceofLTDD,weintroducefourmethodsasthebaselines:two
state-of-the-art fairness methodsFair-Smote [ 22] at FSEâ€™2021and
Fairway[ 23]atFSEâ€™2020,andtwowidelystudiedpre-processing
methods Reweighing [ 31] and Disparate Impact Remover [ 27]. Our
experimentalresultssupporttheclaimthatLTDDperformswell:
comparedwiththebaselines,LTDDcanlargelyimprovethefairnessofMLsoftware,withlessorcomparabledamagetoitsperformance.
Our study makes the following two contributions:
â€¢Strategy.Thispaperproposesanovelfairnessmethod,Linear-
regression based Training Data Debugging (LTDD), as anefficient strategy to alleviate ML softwareâ€™s unfairness by
identifyingbiasedfeaturesandremovingtheirbiasedparts
in training data.
â€¢Study.Thispapercontainsanextensiveempiricalstudyon
nine tasks under three ML classifiers. The results of ourexperiment show that LTDD can largely improve the fair-
ness of ML software, with less or comparable damage to its
performance.
The rest of this paper is structured as follows. We introduce
backgroundinSection2andpresentadetaileddescriptionofour
method LTDD in Section 3. Section 4 presents the experimental
settings,includingstudieddatasets,baselinemethods,prediction
settings, and used classifiers. Sections 5 and 6 give the experimen-
tal results and important further discussions. Finally, Section 7
describes threats to validity, and Section 8 concludes our paper.
2 BACKGROUND
2.1 Preliminaries
We introduce some concepts and symbols about the fairness of
ML software here, which facilitate the readers to understand the
following parts.
MLSoftware.Forbinaryclassificationtasks2,MLSoftware SML
would be considered as a function mapping the domain feature
vectors x=[ğ‘¥1,ğ‘¥2,...,ğ‘¥ğ‘‘]âˆˆRğ‘‘into class labels ğ‘¦âˆˆ{0,1}, i.e.,
SML:Rğ‘‘â†’{0,1}. Normally, for a new input x,w eu s e ğ‘¦to
denote the actual label, while Ë†ğ‘¦=SML(x)indicates the predicted
label by ML Software.
Sensitive Feature. The sensitive feature is a feature ğ‘¥ğ‘ in the
domainfeaturevector x,whichdividessamplesintotwocategories
(called privileged and unprivileged), in which the benefits received
differconsiderably.Forexample,sexisoneofthecommonsensitive
features.Asobservedin[ 2,9,23],themalegroupisoftenregarded
astheprivilegedgroupwithmorefavorablelabelspredictedbyML
Software.
GroupFairness.MLsoftware hasgroupfairness,if thesensitive
feature doesnot affect thegroup probability ofdecision outcomes
2Here, we focus on the ML Software for binary classification tasks. Our definition can
be easily generalized to multiple classification tasks.
2216
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:54:36 UTC from IEEE Xplore.  Restrictions apply. Training Data Debugging for the Fairness of Machine Learning Software ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
[46],i.e.,privilegedgroupsandunprivilegedgroupshave(almost)
equalprobabilitiesofbeingpredictedasfavorablelabels.Groupfair-
ness is widely studied [ 23,47] and guaranteed by legal regulations
on fairness [27, 28]. In this study, we focus on group fairness.
2.2 Related Work
We introduce the related work from three aspects: fairness testing
and evaluation, bias removal algorithm, and data debugging.
2.2.1 Fairness Testing and Evaluation. Fairness testing hopes to
find instances of bias in the data set and evaluate the fairness of
themodelâ€™spredictionresults.AEQUITAS[ 42]findsinstancesof
discriminationinthedatasetandgeneratesmoreinstancestore-
trainfurther.Itappliestwosearchstrategiesglobalsearchbasedon
randomtechnologyandlocalsearchconductedbydisturbingthe
instancesfoundinglobalsearch.AffectedbyAEQUITAS,Aggarwal
etal.[11]proposedablack-boxtechniqueforfairnesstestingbased
on global and local search combined with symbolic execution and
local interpretable models. Zhang et al. [ 48] proposed a white-box
testing technique to find and generate test cases, which aims at
complex DNN models and introduces gradient-based methods into
global and local search.
2.2.2 Bias Removal Algorithm. Bias removal algorithms can be
divided into three categories: pre-processing, in-processing, and
post-processing.
(a)Thepre-processingalgorithmaffectsthetrainingofthemodel
by modifying the data to achieve fairness. As the pre-processingalgorithm is the most related work to this study, we detailedly
summarize pre-processing approaches in the following three folds:
â€¢datapoint modification which deletes, generates, or assigns
weights to datapoints of training data ([22, 23, 31]);
â€¢feature selection which leverages feature construction to
choosemoresuitablefeaturesthatleadtobothhighaccuracy
and fairness [37, 46];
â€¢feature modification which modifies features as a debias-ing method. Disparate Impact Remover (DIR) is the most
relatedfeaturemodificationmethod[ 27]andhasalsobeen
employed as one of four baselines (see Section 4.2). DIR
focuses on the predictability of sensitive features from non-
sensitive features (non-sensitive â†’sensitive), which is an
opposite strategy of our method to evaluate the bias.
Comparedwithrelatedwork,ourmethodemployslinearregres-
sion to estimate the biased association of non-sensitive features
fromsensitivefeatures(i.e.,sensitive â†’non-sensitive,seeSection3),
which is a novel angle to assess and remove the bias.
(b) The in-processing method is to modify the model to make
the prediction result of the model fairer. Zhang et al. [ 45]p r o -
posedamethodtoimprovethemodelthroughnegativefeedback.
It combines an adversarial model to obtain the difference between
sensitive features and non-sensitive features to reduce bias.
(c) The post-processing method achieves prediction fairness by
modifying the prediction results of the model. Hardt et al. [ 29]
proposedapost-processingmethodbasedonequalopportunitydif-ference,whichachievesthegoaloffairnessbytransferringthecost
of bad classification from unprivileged groups to decision makers.2.2.3 Data Debugging. Thegoalofdatadebuggingistolocateand
modify the data that causes program errors. Chad and Ronald [ 39]
introducedprogramchippingtosimplifyinputsthatcauseprogram
errors, which can automatically delete or cut off certain parts ofthe inputs. To debug the input data, Lukas et al. [
32] introduced
analgorithmcalledddmax,whichmaximizesthesubsetofinputthat the program can still process to repair the input data. They
arguedthatddmaxisthefirstgeneraltechnologytorepairfaulty
inputsautomatically.Wuetal.[ 43]proposedacomplaint-driven
datadebuggingsystemnamedRainwhichallowsuserstocomplain
about the intermediate or final output of the query and returns the
smallestsetoftrainingsubsetthatcansolvethebugafterdeletion.
3 OUR APPROACH
Inthissection,wepresentadetaileddescriptionofourapproach.
First, we show a motivation example. Next, we employ linear re-
gression to model biased features. Finally, we present our method
Linear-regression based Training Data Debugging (LTDD).
3.1 Motivation Example
InSection1,wequotedareport[ 8]aboutAmazonfairnessissues
to show that, when biased features appear in the training data, ML
software might discriminate between privileged and unprivileged
groups. Here, we propose a more detailed motivation exampleto show why and how features are biased, i.e., why features areconsidered related to sensitive features and how they cause MLsoftware unfairness. This example is also helpful to demonstrate
the following steps in Sections 3.2 and 3.3.
Figure1(a)presentsasnippettakenfromtheCOMPASdataset
[10]whichisusedtoevaluatethepossibilityofacriminaldefendant
reoffending. The snippet contains six samples with the sensitive
featureâ€œRaceâ€andthenon-sensitivefeatureâ€œAgeâ€.Hereweconsider
thesnippet3asasmallbutrepresentativemotivationexampleto
showthetrainingdatasetwithbiasedfeatures.Ascanbeseenin
Figure 1(a), we have the following observations:
â€¢Biasbetweenracegroupsisobservedintrainingdata.Thereareobviouslydifferentratesoffavorablelabels(no-reoffend)
between privileged (white) and unprivileged (non-white)
groups,e.g.,allthreewhitepeoplearelabeledasno-reoffend
(the favorable decision). In contrast, only one out of three
non-white is labeled as no-reoffend.
â€¢Whyâ€œAgeâ€containsracebias?Thereisahighassociation
betweenâ€œRaceâ€andâ€œAgeâ€:theâ€œAgeâ€valuesoftheprivileged
group(threewhitepeople)arerelativelymuch largerthan
that of the unprivileged group (three non-white people).
â€¢Howâ€œAgeâ€ causesML software biased,even without know-
ing â€œRaceâ€ information? Since larger â€œAgeâ€ values ( >33)
imply white people, the race bias may be introduced by the
â€œAgeâ€ values when training ML software.
3.2 Linear Regression Modeling
As discussed above, some features in the training data might be
biased,leadingtounfairnessinMLsoftware.Toobtainunbiased
3For brevity, we focus on the six samples here and discuss the biased association
betweenthesensitivefeatureâ€œRaceâ€andthenon-sensitivefeatureâ€œAgeâ€inthissnippet.
In Section 6.2, we will extend the scope to the whole set of COMPAS dataset.
2217
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:54:36 UTC from IEEE Xplore.  Restrictions apply. ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA Yanhui Li, Linghan Meng, Lin Chen, Li Yu, Di Wu, Yuming Zhou, and Baowen Xu
GroupDomain features
Label Sensitive Non-sensitive
Race Age ...
PrivilegedWhite 36 ...no-reoffend
White 38 ...no-reoffend
White 34 ...no-reoffend
UnprivilegedNon-white 30 ...no-reoffend
Non-white 32 ... reoffend
Non-white 28 ... reoffend
(a) Association between Age and Race.Employ linear regression equation
Age = ğ‘+ğ‘Â·Race+ğœ‡
to estimates Ë†ğ‘=30 andË†ğ‘=6
36 =ğ‘+ğ‘Â·1+ğœ‡
38 =ğ‘+ğ‘Â·1+ğœ‡
34 =ğ‘+ğ‘Â·1+ğœ‡
30 =ğ‘+ğ‘Â·0+ğœ‡
32 =ğ‘+ğ‘Â·0+ğœ‡
28 =ğ‘+ğ‘Â·0+ğœ‡
(b) Calculate bias partsRemove biased parts from Age:
Ë†Age = Ë†ğ‘+Ë†ğ‘Â·Race
Ageğ‘¢=A g eâˆ’Ë†Age
0=3 6 âˆ’(30+6Â·1)
2=3 8 âˆ’(30+6Â·1)
-2 = 34 âˆ’(30+6Â·1)
0=3 0 âˆ’(30+6Â·0)
2=3 2 âˆ’(30+6Â·0)
-2 = 28 âˆ’(30+6Â·0)
(c) Remove biased parts
Figure 1: An example of biased features (Age) which are highly related to sensitive features (Race).
features,wehavetodebugfeaturevaluesintrainingdata,i.e.,(a)
identify which features and which parts of them are biased, and (b)
removethebiasedpartstorecoverasmuchhelpfulandunbiased
information as possible.
The main idea of our training data debugging is to apply the
linear regression equation [ 50] to analyze the association between
non-sensitive features and sensitive features, which is a simple but
effective method applied in many SE research areas, such as defect
prediction [ 49] and software effort estimation [ 12]. Specifically, we
buildalinearregressionmodelforthenon-sensitivefeatures ğ‘¥ğ‘›on
the sensitive features ğ‘¥ğ‘ :
ğ‘¥ğ‘›=ğ‘+ğ‘Â·ğ‘¥ğ‘ +ğœ‡ (1)
where the symbols ğ‘,ğ‘, andğœ‡denote the population regression
intercept, slope, and residual, respectively. We employ Wald test
[30] withğ‘¡-distribution to check whether the null hypothesis (that
the slope of the linear regression model is zero) holds. When the ğ‘-
valuesâ‰¥0.05,weignorethecurrentnon-sensitivefeatureandmove
tothenext;otherwise,weconsiderthat ğ‘¥ğ‘›containssignificant bias
and conduct the following steps to estimate and exclude bias.
By calculating on values of ğ‘¥ğ‘›andğ‘¥ğ‘ in training data of ML
software, we obtain the estimates Ë†ğ‘andË†ğ‘forğ‘andğ‘, respectively.
Basedontheaboveestimates,wehavethefollowingequationto
measure the association between ğ‘¥ğ‘›andğ‘¥ğ‘ :
Ë†ğ‘¥ğ‘›=Ë†ğ‘+Ë†ğ‘Â·ğ‘¥ğ‘  (2)
whereË†ğ‘¥ğ‘›couldbeconsideredasthepredictedvaluegeneratedfrom
Equation 1. Intuitively, Ë†ğ‘¥ğ‘›is the explained variance in the original
non-sensitivefeatures ğ‘¥ğ‘›bythesensitive feature ğ‘¥ğ‘ .Weconsider
Ë†ğ‘¥ğ‘›as thebiasedpart ofğ‘¥ğ‘›, remove Ë†ğ‘¥ğ‘›fromğ‘¥ğ‘›, and denote the
remaining unbiased part as ğ‘¥ğ‘¢ğ‘›:
ğ‘¥ğ‘¢
ğ‘›=ğ‘¥ğ‘›âˆ’Ë†ğ‘¥ğ‘› (3)
Fig.1(b)andFig.1(c)showthedetailofcalculatingthesample
estimates and the remaining values based on the six samples in
Fig. 1(a). In detail, after translating categorical variables white/non-
white into numerical variables 1/0, we have the following linear
regression equations:
/bracketleftbig36,38,34,30,32,28/bracketrightbigğ‘‡=ğ‘+ğ‘Â·/bracketleftbig1,1,1,0,0,0/bracketrightbigğ‘‡+ğœ‡(4)
We find that ğ‘-value <0.05 and obtain the estimates Ë†ğ‘=30 and
Ë†ğ‘=6.Finally,weobtaintheunbiasedagevalue Ageğ‘¢bysubtracting
the predicted values Ë†Age=30+6Â·Race:Ageğ‘¢=Ageâˆ’Ë†Age.A s
can be seen in Fig. 1(c), the distributions of â€œAgeâ€ values underprivileged and unprivileged groups become independent of â€œRaceâ€
afterremovingthebiasedparts,i.e.,therevisedâ€œAgeâ€valuesintwo
groups are the same.
3.3 Our Method
Based on the linear regression model, we propose our algorithm
Linear-regression based Training Data Debugging (LTDD), shown
in Algorithm 1.
Given the training dataset Sğ‘¡ğ‘Ÿ={/angbracketleftx1,ğ‘¦1/angbracketright,Â·Â·Â·,/angbracketleftxğ‘›,ğ‘¦ğ‘›/angbracketright}, where
foranyğ‘—(1â‰¤ğ‘—â‰¤ğ‘›),xğ‘—=[ğ‘¥ğ‘—
1,...,ğ‘¥ğ‘—
ğ‘‘]isağ‘‘-dimensionvectorto
denoteğ‘‘featurevalues, ğ‘¥ğ‘—
ğ‘‘isassumedtobethesensitivefeature
valueandtheothervalues ğ‘¥ğ‘—
1,Â·Â·Â·,ğ‘¥ğ‘—
ğ‘‘âˆ’1arenon-sensitivefeature
values, and ğ‘¦ğ‘—âˆˆ{0,1}, our method LTDD comprises the following
three steps:
(1)Identify the biased features and estimate the biased parts of
them.For each non-sensitive feature ğ‘¥ğ‘–, we evaluate the as-
sociation between the sensitive features ğ‘¥ğ‘‘andğ‘¥ğ‘–in the
training dataset. It is worth noting that, since the associa-
tionbetweensomenon-sensitivefeaturesandthesensitive
feature may be trivial, we employ Wald test (line 7) with
ğ‘¡-distribution to check whether the null hypothesis (that
the slope Ë†ğ‘of the linear regression model is zero) holds.
Specifically,weintroducethe ğ‘-valueofWaldtesttoavoid
unnecessaryremoving steps, i.e., consider â€œ ğ‘-value <0.05â€
(line 8) as a precondition. If â€œ ğ‘-value <0.05â€ holds, we cal-
culate the estimates Ë†ğ‘ğ‘–andË†ğ‘ğ‘–of the linear regression model
(line 9), which are sorted in Eğ‘andEğ‘(line 10).
(2)Exclude the bias parts from training samples. In this step, for
any training sample /angbracketleftxğ‘—,ğ‘¦ğ‘—/angbracketright, we conduct the following two
operators to eliminate bias: deleting the sensitive feature
(line 12) and revising the non-sensitive feature values by
removing the association (line 13-14). After that, we build a
(fair) ML software SMLbased on the revised and unbiased
training dataset (line 15), i.e., SML:Rğ‘‘âˆ’1â†’{0,1}.
(3)Apply the same revision on the testing samples. As we revised
thedimensionanddistributionoftrainingdata,weapplythe
samerevisiononthetestingsample xğ‘¡ğ‘’tofitMLsoftware
(line 16-18), i.e., delete the sensitive feature and revise the
other attributes by the estimates Eğ‘[ğ‘–]andEğ‘[ğ‘–]calculated
on the training samples. Finally, we use SMLto predict the
label of xğ‘¡ğ‘’(line 19).
2218
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:54:36 UTC from IEEE Xplore.  Restrictions apply. Training Data Debugging for the Fairness of Machine Learning Software ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
Algorithm 1: Linear-regression based Traning Data
Debugging LTDD (Sğ‘¡ğ‘Ÿ,xğ‘¡ğ‘’)
Input:The training dataset Sğ‘¡ğ‘Ÿ={/angbracketleftx1,ğ‘¦1/angbracketright,Â·Â·Â·,/angbracketleftxğ‘›,ğ‘¦ğ‘›/angbracketright},
where xğ‘—=[ğ‘¥ğ‘—
1,...,ğ‘¥ğ‘—
ğ‘‘]is ağ‘‘-dimension vector to denote
theğ‘‘attribute values, ğ‘¥ğ‘—
ğ‘‘is the sensitive feature value and
the other ğ‘¥ğ‘—
1,Â·Â·Â·,ğ‘¥ğ‘—
ğ‘‘âˆ’1are non-sensitive feature values,
ğ‘¦ğ‘—âˆˆ{0,1}, and the testing sample xğ‘¡ğ‘’=[ğ‘¥ğ‘¡ğ‘’
1,...,ğ‘¥ğ‘¡ğ‘’
ğ‘‘].
Output: a ML software SMLand the predicted label SML(xğ‘¡ğ‘’)
forxğ‘¡ğ‘’.
1initialize (ğ‘‘âˆ’1)-dimension array Eğ‘[1:ğ‘‘âˆ’1]withEğ‘[ğ‘–]=0,
which is used to store the estimation result of intercept Ë†ğ‘ğ‘–;
2initialize (ğ‘‘âˆ’1)-dimension array Eğ‘[1:ğ‘‘âˆ’1]withEğ‘[ğ‘–]=0,
which is used to sort the estimation result of slope Ë†ğ‘ğ‘–;
3construct the column vector ğ‘‰ğ‘‘of the sensitive feature values from
Sğ‘¡ğ‘Ÿ:ğ‘‰ğ‘‘=[ğ‘¥1
ğ‘‘,Â·Â·Â·,ğ‘¥ğ‘›
ğ‘‘]ğ‘‡;
4forğ‘–âˆˆ{1,Â·Â·Â·,ğ‘‘âˆ’1}do
5construct the column vector ğ‘‰ğ‘–of the current non-sensitive
feature values: ğ‘‰ğ‘–=[ğ‘¥1
ğ‘–,Â·Â·Â·,ğ‘¥ğ‘›
ğ‘–]ğ‘‡;
6apply the linear regression model on ğ‘‰ğ‘–:ğ‘‰ğ‘–=ğ‘ğ‘–+ğ‘ğ‘–Â·ğ‘‰ğ‘‘+ğœ‡;
7conduct Wald test with ğ‘¡-distribution to get the ğ‘-value;
8ifğ‘-value<0.05then
9 estimate Ë†ğ‘ğ‘–andË†ğ‘ğ‘–forğ‘ğ‘–andğ‘ğ‘–;
10 insertË†ğ‘ğ‘–andË†ğ‘ğ‘–intoEğ‘andEğ‘:Eğ‘[ğ‘–]=Ë†ğ‘ğ‘–,Eğ‘[ğ‘–]=Ë†ğ‘ğ‘–;
11for/angbracketleftxğ‘—,ğ‘¦ğ‘—/angbracketrightâˆˆS ğ‘¡ğ‘Ÿdo
12remove the sensitive feature from xğ‘—:xğ‘—=xğ‘—[1:ğ‘‘âˆ’1];
13forğ‘–âˆˆ{1,Â·Â·Â·,ğ‘‘âˆ’1}do
14 remove the biased part based on the estimation:
ğ‘¥ğ‘—
ğ‘–=ğ‘¥ğ‘—
ğ‘–âˆ’(E ğ‘[ğ‘–]+E ğ‘[ğ‘–]Ã—ğ‘¥ğ‘—
ğ‘‘);
15train ML software SMLfrom the revised (ğ‘‘âˆ’1)-dimension
training data;
16remove the sensitive feature from xğ‘¡ğ‘’:xğ‘¡ğ‘’=xğ‘¡ğ‘’[1:ğ‘‘âˆ’1];
17forğ‘–âˆˆ{1,Â·Â·Â·,ğ‘‘âˆ’1}do
18apply the same revision on the testing sample xğ‘¡ğ‘’:
ğ‘¥ğ‘¡ğ‘’
ğ‘–=ğ‘¥ğ‘¡ğ‘’
ğ‘–âˆ’(E ğ‘[ğ‘–]+E ğ‘[ğ‘–]Ã—ğ‘¥ğ‘¡ğ‘’
ğ‘‘);
19returnSMLandSML(xğ‘¡ğ‘’);
4 EXPERIMENT SETUPS
In this section, we will introduce the experiment setups. Code and
data are publicly available online and reusable (see Section 8.1).
4.1 Studied Datasets
Weemployninedatasetstoevaluatetheperformanceofourmethod,
allofwhichhavebeenwidelyusedinprevioussoftwarefairness
studies [23, 46].
Adult[3].Thisdatasetcontains48,842sampleswith14features.
The goal of the data set is to determine whether a personâ€™s an-
nual income can be larger than 50k. This dataset has two sensitive
features sex and race.
COMPAS [10]. COMPAS is the abbreviation of Correctional Of-
fender Management Profiling for Alternative Sanctions, which is a
commercialalgorithmforevaluatingthepossibilityofacriminal
defendant committing a crime again. The dataset contains the vari-
ablesusedbytheCOMPASalgorithmtoscorethedefendantandTable 1: The datasets used in this experiment
Dataset #Feature Size Sensitive feature
Adult 14 48,842 sex/race
COMPAS 28 7,214 sex/race
Default 24 30,000 sex
German 20 1,000 sex
Heart 14 297 age
Bank 16 45,211 age
Student 33 1,044 sex
MEPS15 139 15,830 race
MEPS16 139 15,675 race
thejudgmentresultswithintwoyears.Thereareover7000rowsin
this dataset, with two sensitive features sex and race.
DefaultofCreditCardClients (Defaultforshort)[ 9].Thepurpose
ofthisdatasetistodeterminewhethercustomerswilldefaulton
payment through customersâ€™ information. It contains30,000 rows
and 24 features, including one sensitive feature sex.
German Credit (German for short) [ 2]. This data set contains
1000 rows and 20 features, where each person is divided into good
or bad credit risk according to feature values. The sensitive feature
in this dataset is sex.
Heart Disease (Heart for short) [ 1]. This dataset judges whether
the patient has heart disease based on the collected information.
The data set includes 76 features, where age is the sensitive feature
in this dataset.
BankMarketing (Bankforshort)[ 5].Thegoalofthisdatasetis
to predict whether the client will subscribe to a term deposit. It
contains 45211 pieces of data, and its sensitive feature is age.
StudentPerformance (Studentforshort)[ 6].Thisdatasetanalyzes
studentperformanceinsecondaryeducation.Itcontains1044pieces
of data with the sensitive feature sex.
MedicalExpenditurePanelSurvey (MEPSforshort)[ 7].Itcollects
data about the health services used by respondents, the cost and
frequencyofservices,anddemographics.Weusethesurveydata
for the calendar years 2015 and 2016, named MEPS15 and MEPS16,
respectively.Theycontainmorethan15,000rows,andthesensitive
feature is race.
Since there are two sensitive features (sex and race) in two
datasets Adult and COMPAS, we convert the 9 datasets into 11
scenarios by considering one sensitive feature in one scenario,
i.e.,theAdult(COMPAS)datasetappearstwiceasAdult_sexand
Adult_race (COMPAS_sex, COMPAS_race).
4.2 Baseline Methods
As described in Section 3, our method LTDD is a pre-processing
method,i.e.,debuggingtrainingdatabeforetheconstructionofML
software to reduce the bias. We introduce four fairness methods
as the baselines: two state-of-the-art methods Fairway at FSEâ€™2020
andFair-SmoteatFSEâ€™2021,andtwowidelyusedpre-processing
methodsReweighingandDisparateImpactRemover(DIR)forcom-
parison.
-Fair-Smote [ 22]: Fair-Smote is a pre-processing method that
usesthemodifiedSMOTEmethodtomakethedistributionof
2219
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:54:36 UTC from IEEE Xplore.  Restrictions apply. ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA Yanhui Li, Linghan Meng, Lin Chen, Li Yu, Di Wu, Yuming Zhou, and Baowen Xu
DatasetTrain set
Test setDivide 
into85%
15%Trained 
modelRemove 
Association 
and Train
EvaluationRemove 
AssociationTest100 times
Figure 2: Prediction settings for our experiment.
sensitive features in the data set consistent and then deletes
biased data through situation testing.
-Fairway[ 23]:Fairwayisahybridalgorithmthatcombines
pre-processingandin-processingmethods.Ittrainsthemod-
els separately based on samples with different sensitive fea-
tures to remove biased data points. Then it uses the Flash
technique [ 35] for multi-objective optimization, including
model performance indicators and fairness indicators.
-Reweighing [ 31]: Reweighing is a pre-processing method
thatcalculatesaweightvalueforeachdatapointbasedon
theexpectedprobabilityandtheobservedprobability,tohelp
the unprivileged class have a greater chance of obtaining
favorable prediction results.
-DisparateImpactRemover(DIR)[ 27]:thismethodisapre-
processingtechnology,whoseprimarygoalistoeliminate
DisparateImpactandincreasefairnessbetweengroupsby
modifying feature values except for the sensitive features.
Our code for implementing LTDD and allbaselines is based on
Python3.7.6,whereReweighingandDIRareimplementedusing
AIF360 [16] (version 0.3.0).
4.3 Prediction Settings and Classifiers
Following the training and testing process in Fairway [ 23], we
randomly divide thestudied datasetinto twoparts: 85%is usedto
preprocessandtrain4,andtheremaining15%isusedasthetestset.
Since our studied methods have randomness, as shown in Figure 2,
we repeat our experiment 100 times to reduce this influence.
Like the previous researches on fairness testing, we introduce
logisticregressionmodel[ 23,24]asamainclassifiertotestfairness
in this experiment. To test the effect of our method under other
classifiers,wealsoemployNaiveBayesandSVM,whicharealso
widelyusedclassifiersinSEresearch[ 38,41].Whenweimplement
allclassifiers,weemploythedefaultsettingsoftheSklearnmodule.
4.4 Fairness and Performance Metrics
Asreportedin[ 17],theincreasingofsoftwarefairnessmaydamage
theperformance,whichiscalledtheaccuracy-fairnesstradeoff.Our
experiment focuses on both fairness and performance metrics to
check the relationship between fairness and performance changed
after applying fairness methods to ML software.
4InFairway[ 23],70%arefortrainingand15%areforvalidation.Forothermethods
without validation,we employ the total 85% (70%+15%) for training.FairnessMetrics. DisparateImpact(DI)andStatisticalParity
Difference(SPD)arethemainindicatorsweusetomeasurefairness.GiventhestudiedMLsoftware
SML,thesample x,thelabel ğ‘¦,the
predicted Ë†ğ‘¦=SML(x)ofx,andthesensitivefeature ğ‘¥ğ‘ ,wedefine
Disparate Impact and Statistical Parity Difference as follows:
Disparate Impact (DI) [27]. It indicates the ratio of the probabili-
tiesoffavorableresultsobtainedbytheunprivileged( ğ‘¥ğ‘ =0)and
privileged ( ğ‘¥ğ‘ =1) classes.
DI=ğ‘(Ë†ğ‘¦=1|ğ‘¥ğ‘ =0)/ğ‘(Ë†ğ‘¦=1|ğ‘¥ğ‘ =1)
StatisticalParityDifference (SPD)[21].ItissimilartoDI,butit
represents the difference between the unprivileged and privileged
classes to obtain favorable results.
SPD=ğ‘(Ë†ğ‘¦=1|ğ‘¥ğ‘ =0)âˆ’ğ‘(Ë†ğ‘¦=1|ğ‘¥ğ‘ =1)
WhenreportingDI,wecalculateitsabsolutedistancefrom1,i.e.,
|1âˆ’DI|[19]. While for SPD, we report the absolute value result
|SPD|[23].
We take DI and SPD as the main fairness metrics due to the
following reasons. (a) DI and SPD are designed to indicate thedifference between the decision distributions of privileged and
unprivilegedindividuals,whichisspecificallyprohibitedbyanti-
discriminationlaws[ 27];(b)Basedontheircalculation,DIandSPD
areindependentofthelabelinformationaboutthedecisions(i.e.,
theyneedonlytheprediction Ë†ğ‘¦=SML(x)ratherthanthelabel ğ‘¦),
whichissuspectedofcontainingbias[ 23].Besides,inSection6.3,
we will present the results under other fairness metrics, including
AOD and EOD [15].
PerformanceMetrics: weintroducethreeperformancemetrics
to measure the performance of the classifiers.
Accuracy (ACC for short). It measures the ratio of correct pre-
dictions on total data:
ACC =(|ğ‘‡ğ‘ƒ|+|ğ‘‡ğ‘|)/|ğ‘‡ğ‘œğ‘¡ğ‘ğ‘™|
whereğ‘‡ğ‘ƒdenotesthetruepositivesamples, ğ‘‡ğ‘denotesthetrue
negative samples, and ğ‘‡ğ‘œğ‘¡ğ‘ğ‘™denotes the total samples.
Recall. Recall represents the probability of being predicted as
positive samples in samples that are actually positive:
Recall =|ğ‘‡ğ‘ƒ|/(|ğ‘‡ğ‘ƒ|+|ğ¹ğ‘|)
whereğ¹ğ‘denotes the false negative samples.
False Alarm. It is also called the false positive rate:
False Alarm =|ğ¹ğ‘ƒ|/(|ğ¹ğ‘ƒ|+|ğ‘‡ğ‘|)
whereğ¹ğ‘ƒdenotes the false positive samples.
Whencalculatingtheseaboveindicators(includingbothfairness
andperformance),weemployPythonAIF360module[ 16]which
integrates all these indicators.
4.5 Analysis Method
Toverifywhetherthedifferencebetweenthemetricvaluesobtained
by our method and the baselines is statistically significant, we use
the Wilcoxon rank sum test [ 26]. If the difference between the two
setsofresultsissignificant,thenthe ğ‘-valueobtainedbythetest
should be less than 0.05.
Then,tomeasuretheeffectsizeofthetwosetsofresults,weuse
Cliffâ€™sdelta ğ›¿[36].Ifthevalueof |ğ›¿|islessthan0.147,thedifference
between the two sets is negligible; if it is greater than 0.147 but
2220
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:54:36 UTC from IEEE Xplore.  Restrictions apply. Training Data Debugging for the Fairness of Machine Learning Software ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
Figure 3: Comparison results of our approach with the original ML software under five indicators. For DI and SPD, more
negative differences mean more fairness improved.
lessthan0.330,thenthedifferenceisrelativelysmall;ifitisgreater
than0.330butlessthan0.474,thenthereisamediumdifference;
if it is greater than 0.474, then the difference between the two is
considered large.
According to ğ‘-value and ğ›¿value, we mark â€œW/T/Lâ€ [ 33,34] for
twosetsofcomparedresults.â€œWâ€meansourresultwins,wherethe
corresponding ğ‘-valueislessthan0.05,andthe ğ›¿valueisgreater
than 0.147. â€œLâ€ means our results lose, where the ğ‘-value is less
than 0.05, and the ğ›¿value is smaller than -0.147. Otherwise, the
situation is â€œTâ€, which means the two sets of results are tied.
5 EXPERIMENT RESULTS
In this section, we present the results of our three RQs, along with
their motivations, approaches, and findings.
RQ1: How well does our algorithm improve the
fairness of ML software?
MotivationandApproach. Wefirstneedtocheckwhetherour
methodcaneliminatethediscriminationthatexistsintrainingdata.
Meanwhile,wehopethatourmethodcandamagetheperformance
of ML software as little as possible. The main classifier used in this
RQisthelogisticregressionmodel.Toeliminatetheinfluenceof
randomness, we repeat the experiment 100 times and report the
averagevalues.Wecomparetheresultsofourmethodandbaselines
under five indicators: (a) DI and SPD as fairness indicators, and (b)
ACC, Recall, and False Alarm as performance indicators.
Results. InFigure3,wereportthechangeofourmethodsapplying
to original ML software. The blue and orange bars in the figurerepresent the changes in
|1âˆ’DI|and|SPD|, respectively. Based
on the calculation of |1âˆ’DI|and|SPD|, the smaller the value of
|1âˆ’DI|and|SPD|, the fairer the prediction result of the model.
Therefore, the more negative the difference observed, the more
ourmethodimproves thefairnessofthe standardclassifier.Itcan
be seen that our method LTDD significantly improves the fairness
of the standard classifier in most scenarios. Especially, on Compas
dataset, LTDD reduces the value of |1âˆ’DI|by 1.2.
Besides, the other three bars represent the changes of ACC,
Recall,andFalseAlarm.Ascanbeseen,ourmethodhasminimal
impactontheperformanceofMLsoftwareinmostscenarios.Evenunder the two data sets COMPAS and German, our method has
surprisingly improved the Recall of the original ML software.
Answer to RQ1: Our method can greatly improve the fairness
of the original ML software and slightly damage its perfor-
mance.
RQ2: How well does LTDD perform compared
with the state-of-the-art fairness algorithms?
Motivation and Approach. We compare our method LTDD with
the state-of-the-art algorithms to see if our method can perform
betterthanthesealgorithms.Wehopethatourmethodcanachievethat,comparedwithbaselines,(a)itcanbetterimprovethefairnessof ML software; (b) the negative impact of our algorithm on perfor-mancewouldbelessorcomparable.SimilartoRQ1,thisRQapplies
the logistic regression model as the main classifier and repeats the
experiment 100 times to eliminate the influence of randomness.
Results for fairness. The comparison results under fairness met-
ricsareshowninTable2.Thegraybackgroundinthetableindicates
thatourmethodhaswontheresultofthebaseline(theWilcoxon
ğ‘-valueislessthan0.05,andtheCliffâ€™sdelta ğ›¿isgreaterthan0.147).
The black background indicates that our method loses to the base-
line (the Wilcoxon ğ‘-value is less than 0.05, and the Cliffâ€™s delta
ğ›¿islessthan-0.147).Thewhitebackgroundindicatesatie.Itcan
be seen that: (1) Our method is superior to other baselines in most
cases in terms of average values (less values means more fairness).
(2)Ourmethodhaswonthebaselinesin37outof55casesunder
DIand38outof55casesunderSPD.(3)Comparedwiththebest
baseline Fair-Smote, LTDD has comparable performance under DI
(3winsand3losses),whileitperformsbetterunderSPD(5wins
and 3 losses). We conclude that our method significantly improves
the fairness of ML software and surpasses the baselines in most
cases.
Resultsforperformance. Welistthecomparisonresultsunder
performance indicators in Table 3. To compare the effects of differ-
ent methods on the performance loss, we compare our method and
baselineswiththeresultsofthe originalclassifierswithoutapply-
ingfairnessalgorithms.Generally,allmethodshavelossestothe
performanceoftheoriginalclassifiers.IntermsofACC,Recall,and
2221
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:54:36 UTC from IEEE Xplore.  Restrictions apply. ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA Yanhui Li, Linghan Meng, Lin Chen, Li Yu, Di Wu, Yuming Zhou, and Baowen Xu
Table 2: Fairness comparison of our method and baselines with logistical regression. Lessvalues means more fairness.
Indicators MethodsDatasets
W/T/L Adult
_RaceAdult
_SexCompas
_RaceCompas
_SexGerman Default Heart Bank MEPS15 MEPS16 Student
|1 - DI|Original 0.5894 0.8531 0.7929 1.3061 0.1151 0.3139 0.5525 0.0446 0.6590 0.6946 0.1705 9/2/0
Reweighing 0.2744 0.4533 0.1244 0.1278 0.0286 0.0866 0.4358 0.0423 0.3525 0.3193 0.1387 6/5/0
DIR 0.6837 0.8787 0.8261 1.3117 0.9144 0.2740 0.5645 0.0999 0.6665 0.7041 0.1635 10/1/0
Fairway 0.5099 nan* 0.5639 1.6904 0.1359 0.3071 0.5204 0.0466 0.6576 0.6953 0.1903 9/2/0
Fair-Smote 0.2184 0.2655 0.0801 0.0851 0.1445 0.0655 0.4276 0.0451 0.1089 0.1792 0.1811 3/5/3
LTDD 0.2027 0.2136 0.1381 0.0790 0.0286 0.0850 0.2866 0.0463 0.2688 0.2867 0.1686
|SPD|Original 0.0899 0.1659 0.2037 0.2605 0.1141 0.0279 0.3289 0.0227 0.1166 0.1115 0.0714 9/2/0
Reweighing 0.0400 0.0653 0.0535 0.0494 0.0269 0.0064 0.2397 0.0181 0.0434 0.0343 0.0583 6/3/2
DIR 0.1383 0.2101 0.2061 0.2604 0.0396 0.0226 0.3483 0.0426 0.1180 0.1138 0.0679 10/1/0
Fairway 0.0598 0.0018 0.1828 0.2956 0.1327 0.0254 0.3114 0.0198 0.1124 0.1134 0.0793 8/2/1
Fair-Smote 0.0789 0.1005 0.0372 0.0399 0.0780 0.0211 0.2438 0.0208 0.0112 0.0188 0.0741 5/3/3
LTDD 0.0293 0.0272 0.0616 0.0347 0.0270 0.0059 0.1416 0.0224 0.0309 0.0296 0.0708
The background color indicates the comparison result between LTDDandbaseline methods . The gray background indicates that our method wins the baseline, that is to say, the ğ‘-value is less than
0.05andthe ğ›¿isgreaterthan0.147;theblackbackgroundindicatesthatourmethodlosestothebaseline,thatistosay,the ğ‘-valueislessthan0.05andthe ğ›¿valueislessthan-0.147;awhitebackground
indicates a tie.
*The classifier predicts all results for the privileged class as 0.
Table 3: Performance comparison of our method and baselines with logistical regression.
Indicators MethodsDatasets
W/T/L Adult
_RaceAdult
_SexCompas
_RaceCompas
_SexGerman Default Heart Bank MEPS15 MEPS16 Student
ACCOriginal 0.8217 0.8217 0.6383 0.6403 0.6995 0.8086 0.8233 0.7977 0.8648 0.8614 0.9321
Reweighing 0.8212 0.8136 0.6404 0.6430 0.6864 0.8079 0.8347 0.7745 0.8623 0.8560 0.9342 0/6/5
DIR 0.8206 0.8217 0.6404 0.6415 0.3143 0.8058 0.8327 0.7769 0.8644 0.8600 0.9362 0/8/3
Fairway 0.7850 0.7607 0.6375 0.6383 0.7021 0.8068 0.8227 0.7703 0.8639 0.8577 0.9324 0/6/5
Fair-Smote 0.7659 0.7540 0.6364 0.6282 0.6073 0.7042 0.8318 0.7778 0.8547 0.8522 0.9323 0/3/8
LTDD 0.8211 0.8059 0.6392 0.6391 0.6863 0.8079 0.8218 0.7993 0.8605 0.8569 0.9287 0/7/4
RecallOriginal 0.4187 0.4187 0.5618 0.5652 0.9697 0.2293 0.7714 0.8162 0.4024 0.3599 0.9074
Reweighing 0.4177 0.3660 0.5820 0.5872 0.9754 0.2255 0.7802 0.7231 0.3708 0.3220 0.9124 2/4/5
DIR 0.5058 0.4810 0.5594 0.5638 0.0596 0.2159 0.7904 0.7496 0.3988 0.3546 0.9121 2/6/3
Fairway 0.2782 0.0028 0.5978 0.5873 0.9520 0.2126 0.7894 0.7179 0.3896 0.3538 0.9119 2/3/6
Fair-Smote 0.7403 0.7068 0.6257 0.6203 0.5885 0.6012 0.7870 0.7559 0.3599 0.3262 0.9102 5/2/4
LTDD 0.4186 0.3405 0.5670 0.5906 0.9737 0.2257 0.7672 0.8149 0.3616 0.3177 0.9055 1/6/4
False AlarmOriginal 0.0516 0.0516 0.2983 0.2978 0.9207 0.0259 0.1321 0.2187 0.0390 0.0377 0.0447
Reweighing 0.0520 0.0457 0.3103 0.3111 0.9785 0.0258 0.1181 0.1792 0.0359 0.0359 0.0449 4/4/3
DIR 0.0803 0.0713 0.2927 0.2952 0.0766 0.0254 0.1265 0.1983 0.0396 0.0381 0.0405 2/7/2
Fairway 0.0553 0.0006 0.3310 0.3187 0.0480 0.0239 0.1482 0.1826 0.0382 0.0400 0.0478 4/3/4
Fair-Smote 0.2261 0.2312 0.3503 0.3620 0.3462 0.2665 0.1290 0.2026 0.0427 0.0418 0.0463 2/2/7
LTDD 0.0524 0.0479 0.3002 0.3209 0.9746 0.0259 0.1301 0.2146 0.0358 0.0346 0.0496 3/6/2
The background color indicates the comparison result between fairness methods and theoriginal method. The gray background indicates that the fairness method wins the original method, that is to
say, theğ‘-value is less than 0.05 and the ğ›¿is greater than 0.147; the black background indicates that the fairness method loses to the original method, that is to say, the ğ‘-value is less than 0.05, and the ğ›¿
is less than -0.147; a white background indicates a tie.
False Alarm, our method significantly damages the performance of
the original model in 10 out of 33 cases, which is less than 3 out of
4baselines.Besides,ourmethodhasonly2caseswithsignificant
damage under False Alarm, which is the best among all fairness
methods. It can be said that our method maintains the original per-
formanceinmostcases,i.e.,thenegativeimpactofouralgorithm
on performance is less or comparable.
AnswertoRQ2:Ourmethodperformsbetterthanbaselinesin
the improvement of fairness indicators with the performance
damage less than or comparable to baselines.
RQ2a: How well does our method compare with the state-of-
the-art fairness algorithms under different classifiers?Table 4: Fairness comparison between our method and base-
lines with three classifiers.
vs. Reweighing vs. DIR vs. Fair-Smote vs. Original
DI SPD DI SPD DI SPD DI SPD
WIN 1717292812172727
TIE 1110 4511 966
LOSS 560010 700
Because Fairwayâ€™s model optimization part is designed for logistic regression models, we do not
compare with Fairway here.
MotivationandApproach. ToverifythegeneralizationofLTDD,
we also conduct our experiments under two other classifiers NB
and SVM. Similarly, we run the experiment 100 times and compare
it with baselines under fairness and performance indicators.
2222
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:54:36 UTC from IEEE Xplore.  Restrictions apply. Training Data Debugging for the Fairness of Machine Learning Software ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
Table5:Comparisonofperformanceindicatorsbetweenoriginalclassifiersandfairnessmethodsin11scenariosfor3classifiers
Methods Reweighing vs. original DIR vs. original Fair-Smote vs. original LTDD vs. original
Indicators ACC RecallFalse
AlarmACC RecallFalse
AlarmACC RecallFalse
AlarmACC RecallFalse
Alarm
WIN 531 4 255 51 78 431 3
TIE 18 16 13 21 20 19 12 6 5 18 14 14
LOSS 10 14 6 10 8 9 16 10 20 10 16 6
Resultsforfairness. Weshowthecomparisonoffairnessmetrics
inTable4.Wecountthenumberofwin,tie,andlosscasescompared
withthebaselinesandsummarizetheresultsofthethreeclassifiers
together. When the number of wins is greater than or equal to
the number of ties plus losses, we mark this column with the gray
background,whichmeansthatourmethodissignificantlybetter
thanthebaseline.Itcanbeseenthatin7of8scenarios,ourmethod
is significantly better than baselines. When compared with Fair-
Smote, our results on DI are similar, but our number of wins is still
slightly higher than Fair-smote.
Resultsforperformance. Welistthecomparisonresultsofthe
fairnessindicatorsinTable5.Wecomparethefourmethodswith
theresultsoftheoriginalclassifierandcountthenumberofW/T/L.
It can be seen that our method has the least number of losses in
termsofACCandFalseAlarmcomparedwiththeoriginalclassifier:
there are 10 and 6 cases that produce losses, respectively.
Answer to RQ2a: With three different classifiers, our method
stilldefeatsbaselines inmostcasesunderfairnessindicators
and also has the least loss under ACC and False Alarm.
RQ3: Is our method actionable for fairness
improvement in realistic scenarios?
Motivation and Approach. In previous RQs, we have compared
the prediction results of our method and baselines, and the results
haveshownthatourmethodcangeneratefairerpredictions.Inthis
RQ, we introduce a new indicator, the rate of favorable decisions ,t o
measuretheactionabilityoffairnessmethods.Ourmeasurement
is basedon the followingpoints: (a)in realistic scenarios,the rate
offavorabledecisionsislimitedduetothefinitesocialresources,
e.g., for the loan application, the rate of â€œapprovalâ€ is restricted; (b)
beforeapplyingfairnessmethods,originalMLsoftwarehasmade
favorable and unfavorable decisions, among which the original
favorableratecouldbeconsideredasabenchmark;(c)afterapplying
fairness methods,ML softwaremight changethe favorablerate. If
thefavorablerateessentiallyincreases,itwouldalsoraisetheneed
for social resources, even beyond the boundaries of available social
resources,reducingactionability.InthisRQ,wewillcompareour
method with Fair-Smote under the new indicator.
Results. The result of the comparison is shown in Figure 4, where
we employ two endpoints to indicate the favorable rates of the
privileged and non-privileged classes and connect them to drawa line. The short horizontal stroke indicates the whole favorable
rate. The figureâ€™s red, blue, and green points represent the original
value, the result of Fair-smote, and that of LTDD, respectively.
Itcanbeseenthatinmost(10outof11)cases,thewaythatLTDD
improvesfairnessistoincreasethefavorablerateofnon-privilegedclasses and to reduce the favorable rate of privileged classes, sothat the whole favorable rate is very closeto the original value.
Thismeansthatourmethoddoesnotneedmoresocialresourcesto
achieveinmostcases.Onthecontrary,Fair-Smotetriestoincreasethefavorablerateofprivilegedandnon-privilegedclasses,resultinginasignificantincreaseofthewholefavorablerate.Forexample,ontheAdultdataset,thewholefavorablerateofFair-Smoteincreases
about 1.5 times. This means that the implementation of Fair-Smote
needs much more social resources.
Answer to RQ3: Our method improves the fairness indicators
while ensuring that the whole favorable rate is close to the
original value and does not need more social resources.
6 DISCUSSION
WefurtherdiscusstheaimsandresultsofourmethodLTDDinthe
following six points.
6.1 Distribution of biased features
As described in Sections 3.2 and 3.3, we judge whether features
are biased according to the ğ‘-value generated by Wald test [ 30]
of linear regression model. Here we calculate the distribution of
biased features with ğ‘-value <0.05.
Figure5showstheaverageproportionofbiasedfeaturesin11
scenarios, where the blue bars indicate the proportion. We can see
thatin8outofthe11scenes,thepercentsofbiasedfeaturesareover
60%. In the COMPAS data set, when the race isused as a sensitive
feature, all other features are considered biased, which means that
thehighassociationwiththesensitivefeatureisuniversalinthe
data set. Besides, we have also observed that the proportions of
biasedfeaturesinthethreedatasets,Bank,Student,andGerman,
arethelowestthree.AccordingtotheresultsofRQ1,thefairness
optimization effect of these three data sets is also the worst among
alldatasets.Wearguethattheproportionofbiasedfeaturesmay
be related to the fairness improvement effect of LTDD.
6.2 A case study on the COMPAS dataset
This section will use an example to show how our method elimi-
natesbiasedpartsfrombiasedfeatures,narrowingthedifferencein
these biased features between privileged and unprivileged classes.
AsshowninFigure6,thisisthedistributionoftheâ€œAgeâ€feature
for privileged (white) and unprivileged (non-white) categories on
theCOMPASdataset.Theorangedottedlinerepresentsthemedian
value, the purple triangle represents the average value, and the
green diamonds indicate the outliers.
Figure 6(a) is the original distribution of the â€œAgeâ€ feature. It
can be seen that the mean and median values of â€œAgeâ€ of white
2223
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:54:36 UTC from IEEE Xplore.  Restrictions apply. ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA Yanhui Li, Linghan Meng, Lin Chen, Li Yu, Di Wu, Yuming Zhou, and Baowen Xu
Figure 4: Comparison of our approach with Fair-Smote on favorable rates. The favorable rates of privileged and non-privileged
classes are represented by two endpoints, while the whole favorable rate is represented by the short horizontal stroke.
Figure 5: Distribution (%) of biased non-sensitive features.
(a) Before (b) After
Figure 6:An exampleto show howLTDD revises biasedfea-
tures on COMPAS dataset. Figures 6(a) and 6(b) show the
original and revised Age distribution (by normalizing into
[0,1]) of Non-white and White on COMPAS dataset
people are obviously higher than that of non-white people. Af-ter removing the association between the â€œAgeâ€ feature and the
â€œRaceâ€feature,thedistributionsofâ€œAgeâ€betweenthetwogroups
are shown in Figure 6(b). It can be seen that both sets of values
have been â€œshiftedâ€, which makes thedistribution between the twoTable6:ComparisonofAODandEOD betweenourmethod
and Fair-Smote in 11 scenarios for 3 classifiers
ModelsLSR NB SVM
meanW/T/L meanW/T/L meanW/T/L
AODFair-Smote 0.04935/3/30.05353/7/10.10774/4/3LTDD 0.0429 0.0476 0.0991
EODFair-Smote 0.07094/4/30.05802/8/10.06452/4/5LTDD 0.0653 0.0573 0.0613
groups closer. From the mean and median, we can see that after re-
movingbiasedparts,ourmethod narrowsthedistributiondifference
between the privileged group and the unprivileged group.
6.3Performanceunderotherfairnessindicators
As observed in previous studies [ 18], there may be conflicts be-
tween different fairness indicators. To verify the performance of
ourmethodunderotherfairnessindicators,wecompareourmethod
withthestate-of-the-artmethodFair-Smotemethodunderother
two indicators AOD and EOD [15] with three classifiers.
ThedifferencebetweentheresultsofourmethodandFair-Smote
is shown in Table 6. As can be seen from the table, our methodis slightlybetterthan Fair-Smotein most scenarios.The average
value of the two indicators of our method in the three classifiers is
lower (lower means more fairness) than Fair-Smote. Except for the
EODindicatorunderSVM,thenumberofwinsforourmethodis
greaterthan thenumberoflosses. Combinedwiththeconclusion
in RQ2, it can be seen that our method has a greater improvement
under other fairness indicators compared to Fair-Smote.
6.4 Differences between DI and SPD
Intuitively,DIandSPDaretwosimilarfairnessmetrics.Inthissub-section,wewillconductadditionalanalysistoshowthedifferences
between DI and SPD.
(a) We give a case study to show the differences between DI and
SPD. Given two groups ğ´,ğµand their favorable rates under two
fairnessmethods ğ‘“1,ğ‘“2:ğ‘Ÿ1
ğ´=0.32andğ‘Ÿ1
ğµ=0.2forğ‘“1,andğ‘Ÿ2
ğ´=0.6
andğ‘Ÿ2
ğµ=0.4 forğ‘“2. According to the calculation of DI and SPD, we
2224
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:54:36 UTC from IEEE Xplore.  Restrictions apply. Training Data Debugging for the Fairness of Machine Learning Software ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
obtain the following results:
DI(ğ‘“1)=ğ‘Ÿ1
ğ´/ğ‘Ÿ1
ğµ=1.6
DI(ğ‘“2)=ğ‘Ÿ2
ğ´/ğ‘Ÿ2
ğµ=1.5
SPD(ğ‘“1)=ğ‘Ÿ1
ğ´âˆ’ğ‘Ÿ1
ğµ=0.12
SPD(ğ‘“2)=ğ‘Ÿ1
ğ´âˆ’ğ‘Ÿ1
ğµ=0.2
Wewoulddrawdifferentconclusionsfromthecomparisonbetween
ğ‘“1andğ‘“2underDIandSPD:forDI, ğ‘“2performsbetterthan ğ‘“1(more
close to 1 means better); for SPD, ğ‘“1performs better than ğ‘“2(more
close to 0 means better).
(b) We compute the Pearson correlation coefficient between the
values of DI and SPD in 11 studied scenarios for five methods. We
observe that the coefficient is very small in some scenarios, e.g.,
0.08 for the German dataset, which means the values of DI and
SPDdonothavehighcorrelations.Hence,oneofthemcannotbe
directly substituted for the other.
6.5 Performance with other association
approaches?
This paper employs the linear regression model to obtain the as-
sociation between features. It is unclear how other association
approacheswork.Inthissection,weconductanexperimentwith
anotherassociationapproach,polynomialregression,whichisan
extension of simple linear regression:
ğ‘¥ğ‘›=ğ‘+ğ‘1Â·ğ‘¥ğ‘ +ğ‘2Â·ğ‘¥2
ğ‘ +Â·Â·Â·+ğ‘ğ‘‘Â·ğ‘¥ğ‘‘
ğ‘ 
whereğ‘¥ğ‘ andğ‘¥ğ‘›denote sensitive and non-sensitive features. To
makethefittingresulthavealargedifferencefromthelinearregres-sion,wesetthedegree(
ğ‘‘)ofthepolynomialto20(linearregression
couldbeconsideredasaspecificpolynomialregressionwhen ğ‘‘=1).
We compare the results of linear regression and polynomial
regressionunderfairnessindicators.Weobservethatinmostcases,theresultsbetweenthetwomethodsarecomparable,andtheresults
of linear regression are significantly better/worse in 4/1 scenarios.
6.6 Insight of our method
Our method LTDD explores a simple but effective model, linear
regression,toidentify,estimate,andremovethehighassociation
between non-sensitive features and sensitive features as the biased
parts of the non-sensitive features. The results of our experiments
support the claim that LTDD can largely improve the fairness of
MLsoftware,withlessorcomparabledamagetoitsperformance.
Our study may lead to three insights for the following studies in
the fairness of ML software:
(a) The distribution and association of feature values in training
datawouldbehelpfultoestimateandimprovethefairnessofML
software. We encourage following researchers to employ more
effective methods to model the feature distribution and association.
(b) Although ML software is much different from traditional
software,theresearchmethodsintraditionalSEarealsoapplicable
to the related research topic of ML software and can achieve good
performance.Weencouragefollowingresearcherstoapplymore
traditional SE methods and ideas to the research of ML software.
(c) For fairness research, our study provides a new practical
perspective: we employ the rate of favorable decisions to measurethe actionability of fairness methods, which indicates the actual
costofsocialresourcesforfairnessmethods.Wehopethatfuture
researchers can take into account the actual cost while focusing on
fairness.
7 THREATS TO VALIDITY
In this section, we discuss the threats to validity of our approach.
First of all, the choice of data set may be a threat. We employ
9datasetswidelyusedinfairnesstestingtoevaluateourmethod.
However,thesedatasetsmaynotbesufficient,becausethedistri-
bution and characteristics of different data sets are not the same.
We will introduce more datasets to test our method in the future.
Second, the choice of classifiers may be a threat. In the exper-
iment, we mainly use the logistic regression model for fairness
testing,whichisusedinmanyfairnessstudies,buttheinfluenceofthemethodondifferentlearnersmaybedifferent.Althoughwealso
testtheperformanceundertheothertwolearners,therearestillmore complex learners worth experimenting with. In the future,
wewilltesthowourmethodperformsincomplexneuralnetworks.
Finally, thefairness indicators we use may also be a threat. Pre-
vious studies have pointed out that there may be contradictions
betweendifferentfairnessindicators.Therearesomanyfairness
indicators proposed at present, and it is impossible to satisfy all
fairnessindicators[ 24].Therefore,theindicatorsshouldbeselected
reasonablyaccordingtotheactualscenario.Inthepaper,wemainly
discusstheresultsunderDIandSPD,andchecktheeffectofour
method under other indicators in the discussion.
8 CONCLUSION
The widespread usage of machine learning software has raised
concerns about its fairness. More and more SE researchers havepaid attention to the fairness of ML software. The training set is
consideredtobeoneofthereasonsforthebiasofMLsoftware.This
paper adopts a new perspective: the root cause of the unfairness
could be biased features in training data.
Toobtainfeaturesunbiased,wetrytodebugfeaturevaluesin
training data: identify which features and which parts of them are
biasedandexcludethebiasedpartsofsuchfeatures.Toachievethisgoal,weproposeanovelmethod, Linear-regressionbased Training
DataDebugging (LTDD), which employs the linear regression
modeltoidentifythebiasedfeatures.Weintroduceninedatasetstoevaluateourmethodandcompareourmethodswithfourbaselines.
The results show that our method performs better in improving
thefairnessindicators,anddamagestheperformanceslightly.In
thefuture,wehopetoconductresearchonmoreextensivedatasets
to explore different characteristics of the datasets.
8.1 Replication Package
We provide all datasets and source code used to conduct this study
athttps://github.com/fairnesstest/LTDD.
ACKNOWLEDGEMENTS
The work is supported by the National Natural Science Founda-
tionofChina(GrantNo.62172202,61872177,61772259,62172205,
61832009, 61772263).
2225
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:54:36 UTC from IEEE Xplore.  Restrictions apply. ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA Yanhui Li, Linghan Meng, Lin Chen, Li Yu, Di Wu, Yuming Zhou, and Baowen Xu
REFERENCES
[1]1988. Heart Disease Data Set. https://archive.ics.uci.edu/ml/datasets/heart+
disease.
[2]1994. Statlog (German Credit Data) Data Set. https://archive.ics.uci.edu/ml/
datasets/statlog+(german+credit+data).
[3] 1996. Adult Data Set. https://archive.ics.uci.edu/ml/datasets/adult.
[4]2011. The algorithm that beats your bank manager. https://www.forbes.com/sites/parmyolson/2011/03/15/the-algorithm-that-beats-your-bank-
manager/#15da2651ae99.
[5]2012. BankMarketingDataSet. https://archive.ics.uci.edu/ml/datasets/Bank+
Marketing.
[6]2014. Student Performance Data Set. https://archive.ics.uci.edu/ml/datasets/
Student+Performance.
[7] 2015. MEPS Data Set. https://meps.ahrq.gov/mepsweb.[8]
2016. Amazon just showed us that unbiased algorithms can be inadvertently
racist. https://www.businessinsider.com/how-algorithms-can-be-racist-2016-4.
[9]2016. default of credit card clients Data Set. https://archive.ics.uci.edu/ml/
datasets/default+of+credit+card+clients.
[10] 2017. compas-analysis. https://github.com/propublica/compas-analysis.[11]
Aniya Aggarwal, Pranay Lohia, Seema Nagar, Kuntal Dey, and Diptikalyan Saha.
2019. Black box fairness testing of machine learning models. In Proceedings
of the ACM Joint Meeting on European Software Engineering Conference and
Symposium on the Foundations of Software Engineering, ESEC/SIGSOFT FSE 2019,
Tallinn,Estonia,August26-30,2019,MarlonDumas,DietmarPfahl,SvenApel,and
AlessandraRusso(Eds.).ACM,625â€“635. https://doi.org/10.1145/3338906.3338937
[12]SousukeAmasakiandChrisLokan.2015.Ontheeffectivenessofweightedmoving
windows: Experiment on linear regression based software effort estimation.
Journal of Software: Evolution and Process 27, 7 (2015), 488â€“507.
[13]SaleemaAmershi,AndrewBegel,ChristianBird,RobertDeLine,HaraldGall,Ece
Kamar, Nachiappan Nagappan, Besmira Nushi, and Thomas Zimmermann. 2019.
Software engineering for machine learning: A case study. In 2019 IEEE/ACM 41st
InternationalConferenceonSoftwareEngineering:SoftwareEngineeringinPractice
(ICSE-SEIP). IEEE, 291â€“300.
[14]Solon Barocas and Andrew D Selbst. 2016. Big dataâ€™s disparate impact. Calif. L.
Rev.104 (2016), 671.
[15]R.K.E.Bellamy,K.Dey,M.Hind,S.C.Hoffman,S.Houde,K.Kannan,P.Lohia,J.Martino,S.Mehta,A.Mojsilovi,S.Nagar,K.NatesanRamamurthy,J.Richards,D.
Saha, P. Sattigeri, M. Singh, K. R. Varshney, and Y. Zhang. 2019. AI Fairness 360:
An extensible toolkit for detecting and mitigating algorithmic bias. IBM Journal
ofResearchandDevelopment 63,4/5(2019),4:1â€“4:15. https://doi.org/10.1147/JRD.
2019.2942287
[16]Rachel K. E. Bellamy, Kuntal Dey, Michael Hind, Samuel C. Hoffman, Stephanie
Houde, Kalapriya Kannan, Pranay Lohia, Jacquelyn Martino, Sameep Mehta,
Aleksandra Mojsilovic, Seema Nagar, Karthikeyan Natesan Ramamurthy, John T.
Richards, Diptikalyan Saha, Prasanna Sattigeri, Moninder Singh, Kush R. Varsh-
ney, and Yunfeng Zhang. 2018. AI Fairness 360: An Extensible Toolkit forDetecting, Understanding, and Mitigating Unwanted Algorithmic Bias. CoRR
abs/1810.01943(2018). arXiv:1810.01943 http://arxiv.org/abs/1810.01943
[17]RichardBerk,HodaHeidari,ShahinJabbari,MatthewJoseph,MichaelKearns,
JamieMorgenstern,SethNeel,andAaronRoth.2017. Aconvexframeworkfor
fair regression. arXiv preprint arXiv:1706.02409 (2017).
[18]Sumon Biswas and Hridesh Rajan. 2020. Do the machine learning models ona crowd sourced platform exhibit bias? an empirical study on model fairness.
InProceedingsofthe28thACMJointMeetingonEuropeanSoftwareEngineering
Conference and Symposium on the Foundations of Software Engineering. 642â€“653.
[19]Sumon Biswas and Hridesh Rajan. 2020. Do the machine learning models ona crowd sourced platform exhibit bias? an empirical study on model fairness.
InESEC/FSEâ€™20: 28thACM JointEuropean SoftwareEngineeringConference and
SymposiumontheFoundationsofSoftwareEngineering,VirtualEvent,USA,No-
vember 8-13, 2020, Prem Devanbu, Myra B. Cohen, and Thomas Zimmermann
(Eds.). ACM, 642â€“653. https://doi.org/10.1145/3368089.3409704
[20]Yuriy Brun and Alexandra Meliou. 2018. Software fairness. In Proceedings of the
201826thACMJointMeetingonEuropeanSoftwareEngineeringConferenceand
Symposium on the Foundations of Software Engineering. 754â€“759.
[21]Toon Calders and Sicco Verwer. 2010. Three naive Bayes approaches for
discrimination-free classification. Data Min. Knowl. Discov. 21, 2 (2010), 277â€“292.
https://doi.org/10.1007/s10618-010-0190-x
[22]Joymallya Chakraborty, Suvodeep Majumder, and Tim Menzies. 2021. Biasin machine learning software: why? how? what to do?. In ESEC/FSE â€™21: 29th
ACM Joint European Software Engineering Conference and Symposium on the
Foundations ofSoftware Engineering,Athens, Greece,August23-28, 2021, Diomidis
Spinellis, Georgios Gousios, Marsha Chechik, and Massimiliano Di Penta (Eds.).
ACM, 429â€“440. https://doi.org/10.1145/3468264.3468537
[23]Joymallya Chakraborty, Suvodeep Majumder, Zhe Yu, and Tim Menzies. 2020.Fairway: a way to build fair ML software. In ESEC/FSE â€™20: 28th ACM Joint
European Software Engineering Conference and Symposium on the Foundations of
SoftwareEngineering,VirtualEvent,USA,November8-13,2020,PremDevanbu,MyraB.Cohen,andThomasZimmermann(Eds.).ACM,654â€“665. https://doi.
org/10.1145/3368089.3409697
[24]Joymallya Chakraborty, Kewen Peng, and Tim Menzies. 2020. Making FairML Software using Trustworthy Explanation. In 35th IEEE/ACM International
ConferenceonAutomatedSoftwareEngineering,ASE2020,Melbourne,Australia,
September21-25,2020.IEEE,1229â€“1233. https://doi.org/10.1145/3324884.3418932
[25]JoymallyaChakraborty,TianpeiXia,FahmidMFahid,andTimMenzies.2019.
Softwareengineeringforfairness:A casestudywithhyperparameteroptimiza-
tion.arXiv preprint arXiv:1905.05786 (2019).
[26]L. De Capitani and D. De Martini. 2011. On stochastic orderings of the
Wilcoxon Rank Sum test statisticâ€“With applications to reproducibility prob-
ability estimation testing. Statistics & Probability Letters 81, 8 (2011), 937â€“946.
https://doi.org/10.1016/j.spl.2011.04.001
[27]Michael Feldman, Sorelle A. Friedler, John Moeller, Carlos Scheidegger, andSuresh Venkatasubramanian. 2015. Certifying and Removing Disparate Im-
pact. InProceedings of the 21th ACM SIGKDD International Conference on Knowl-
edge Discovery and Data Mining, Sydney, NSW, Australia, August 10-13, 2015,Longbing Cao, Chengqi Zhang, Thorsten Joachims, Geoffrey I. Webb, Dra-gos D. Margineantu, and Graham Williams (Eds.). ACM, 259â€“268. https:
//doi.org/10.1145/2783258.2783311
[28]JosephLGastwirth.1988. AclarificationofsomestatisticalissuesinWatsonv.
Fort Worth Bank and Trust. JurimetricsJ. 29 (1988), 267.
[29]Moritz Hardt, Eric Price, and Nati Srebro. 2016. Equality of Opportunity in
SupervisedLearning.In AdvancesinNeuralInformationProcessingSystems29:
Annual Conference on Neural Information Processing Systems 2016, December 5-10,
2016, Barcelona, Spain, Daniel D. Lee, Masashi Sugiyama, Ulrike von Luxburg,Isabelle Guyon, and Roman Garnett (Eds.). 3315â€“3323. https://proceedings.
neurips.cc/paper/2016/hash/9d2682367c3935defcb1f9e247a97c0d-Abstract.html
[30]Georg Heinze and Michael Schemper. 2002. A solution to the problem of separa-
tion in logistic regression. Statistics in medicine 21, 16 (2002), 2409â€“2419.
[31]Faisal Kamiran and Toon Calders. 2011. Data preprocessing techniques for
classification withoutdiscrimination. Knowl. Inf.Syst. 33, 1(2011), 1â€“33. https:
//doi.org/10.1007/s10115-011-0463-8
[32]LukasKirschner,EzekielO.Soremekun,andAndreasZeller.2020. Debugging
inputs. In ICSE â€™20: 42nd International Conference on Software Engineering, Seoul,
South Korea, 27 June - 19 July, 2020, Gregg Rothermel and Doo-Hwan Bae (Eds.).
ACM, 75â€“86. https://doi.org/10.1145/3377811.3380329
[33]Yibin Liu, Yanhui Li, Jianbo Guo, Yuming Zhou, and Baowen Xu. 2018. Con-necting software metrics across versions to predict defects. In 25th Interna-
tional Conference on Software Analysis, Evolution and Reengineering, SANER2018, Campobasso, Italy, March 20-23, 2018, Rocco Oliveto, Massimiliano DiPenta, and David C. Shepherd (Eds.). IEEE Computer Society, 232â€“243. https:
//doi.org/10.1109/SANER.2018.8330212
[34]Linghan Meng, Yanhui Li, Lin Chen, Zhi Wang, Di Wu, Yuming Zhou, and
BaowenXu.2021. MeasuringDiscriminationtoBoostComparativeTestingfor
MultipleDeepLearningModels.In 43rdIEEE/ACMInternationalConferenceon
SoftwareEngineering,ICSE2021,Madrid,Spain,22-30May2021.IEEE,385â€“396.
https://doi.org/10.1109/ICSE43902.2021.00045
[35]VivekNair,ZheYu,TimMenzies,NorbertSiegmund,andSvenApel.2018.Findingfaster configurations using flash. IEEE Transactions on Software Engineering 46, 7
(2018), 794â€“811.
[36]JeanineRomano,JeffreyDKromrey,JesseCoraggio,JeffSkowronek,andLinda
Devine.2006. ExploringmethodsforevaluatinggroupdifferencesontheNSSE
and other surveys: Are the t-test and Cohenâ€™s d indices the most appropriate
choices.In annualmeetingoftheSouthernAssociationforInstitutionalResearch.
Citeseer, 1â€“51.
[37]Ricardo Salazar, Felix Neutatz, and Ziawasch Abedjan. 2021. Automated feature
engineeringforalgorithmic fairness. Proceedingsofthe VLDBEndowment 14,9
(2021), 1694â€“1702.
[38]Qinbao Song, Martin Shepperd, Michelle Cartwright, and Carolyn Mair. 2006.
Software defect association mining and defect correction effort prediction. IEEE
Transactions on Software Engineering 32, 2 (2006), 69â€“82.
[39]Chad D. Sterling and Ronald A. Olsson. 2007. Automated bug isolation via
program chipping. Softw. Pract. Exp. 37, 10 (2007), 1061â€“1086. https://doi.org/10.
1002/spe.798
[40]Latanya Sweeney. 2013. Discrimination in online ad delivery. Commun. ACM 56,
5 (2013), 44â€“54.
[41]Chakkrit Tantithamthavorn, Ahmed E Hassan, and Kenichi Matsumoto. 2018.
Theimpactofclassrebalancingtechniquesontheperformanceandinterpretation
of defect prediction models. IEEE Transactions on Software Engineering 46, 11
(2018), 1200â€“1219.
[42]SakshiUdeshi,PryanshuArora,andSudiptaChattopadhyay.2018. Automated
directedfairnesstesting.In Proceedingsofthe33rdACM/IEEEInternationalConfer-
enceonAutomatedSoftwareEngineering,ASE2018,Montpellier,France,September
3-7, 2018, Marianne Huchard, Christian KÃ¤stner, and Gordon Fraser (Eds.). ACM,
98â€“108. https://doi.org/10.1145/3238147.3238165
2226
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:54:36 UTC from IEEE Xplore.  Restrictions apply. Training Data Debugging for the Fairness of Machine Learning Software ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
[43]Weiyuan Wu, Lampros Flokas, Eugene Wu, and Jiannan Wang. 2020. Complaint-
drivenTrainingDataDebuggingforQuery2.0.In Proceedingsofthe2020Inter-
national Conference on Management of Data, SIGMOD Conference 2020, online
conference[Portland,OR,USA],June14-19,2020,DavidMaier,RachelPottinger,
AnHaiDoan,Wang-ChiewTan,AbdussalamAlawini,andHungQ.Ngo(Eds.).
ACM, 1317â€“1334. https://doi.org/10.1145/3318464.3389696
[44]Muhammad Bilal Zafar, Isabel Valera, Manuel Gomez Rodriguez, and Krishna P
Gummadi. 2017. Fairness beyond disparate treatment & disparate impact: Learn-
ing classification without disparate mistreatment. In Proceedings of the 26th
internationalconference on world wide web . 1171â€“1180.
[45]Brian Hu Zhang, Blake Lemoine, and Margaret Mitchell. 2018. Mitigating Un-
wanted Biases with AdversarialLearning. In Proceedings of the 2018AAAI/ACM
Conference on AI, Ethics, and Society, AIES 2018, New Orleans, LA, USA, February
02-03, 2018, Jason Furman, Gary E. Marchant, Huw Price, and Francesca Rossi
(Eds.). ACM, 335â€“340. https://doi.org/10.1145/3278721.3278779
[46]Jie M. Zhang and Mark Harman. 2021. "Ignorance and Prejudice" in Software
Fairness. In 43rd IEEE/ACM International Conference on Software Engineering,
ICSE2021,Madrid,Spain,22-30May2021 .IEEE,1436â€“1447. https://doi.org/10.1109/ICSE43902.2021.00129
[47]JieMZhang,MarkHarman,LeiMa,andYangLiu.2020. Machinelearningtesting:
Survey, landscapes and horizons. IEEE Transactions on Software Engineering
(2020).
[48]PeixinZhang,JingyiWang,JunSun,GuoliangDong,XinyuWang,XingenWang,
JinSongDong,andTingDai.2020.White-boxfairnesstestingthroughadversarial
sampling.In ICSEâ€™20:42ndInternationalConferenceonSoftwareEngineering,Seoul,
South Korea, 27 June - 19 July, 2020, Gregg Rothermel and Doo-Hwan Bae (Eds.).
ACM, 949â€“960. https://doi.org/10.1145/3377811.3380331
[49]Yuming Zhou, Hareton Leung, and Baowen Xu. 2009. Examining the potentially
confounding effect of class size on the associations between object-oriented
metricsandchange-proneness. IEEETransactionsonSoftwareEngineering 35,5
(2009), 607â€“623.
[50]Yuming Zhou, Baowen Xu, Hareton Leung, and Lin Chen. 2014. An in-depth
study of the potentially confounding effect of class size in fault prediction. ACM
Transactions on Software Engineering and Methodology (TOSEM) 23, 1 (2014),
1â€“51.
2227
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:54:36 UTC from IEEE Xplore.  Restrictions apply. 