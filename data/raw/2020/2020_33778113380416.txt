Near-Duplicate DetectioninWebApp ModelInference
Ra
hulkrishnaYandrapally
University of British Columbia
Vancouver,BC,Canada
rahulyk@ece.ubc.caAndrea Stocco∗
Università della Svizzera italiana
Lugano,Switzerland
andrea.stocco@usi.chAli Mesbah
University ofBritish Columbia
Vancouver,BC,Canada
amesbah@ece.ubc.ca
ABSTRACT
Automatedweb testingtechniques infer modelsfroma given web
app, which are usedfor test generation. From a testing viewpoint,
such an inferred model should contain the minimal set of states
that are distinct, yet, adequately cover the app’s main function-
alities. In practice, models inferred automatically are aﬀected by
near-duplicates, i.e., replicas of the same functional webpage dif-
fering only by small insigniﬁcant changes. We present the ﬁrst
study of near-duplicate detection algorithms used in within app
model inference. We ﬁrst characterize functional near-duplicates
by classifying a random sample of state-pairs, from 493 kpairs of
webpagesobtainedfromover6,000websites,intothreecategories,
namelyclone,near-duplicate,anddistinct.Wesystematicallycom-
pute thresholds that deﬁne the boundaries of these categories for
each detection technique. We then use these thresholds to evalu-
ate10near-duplicatedetectiontechniquesfromthreediﬀerentdo-
mains, namely, information retrieval, web testing, and computer
vision on nine open-source web apps. Our study highlights the
challenges posedinautomaticallyinferring a modelforanygiven
webapp.Ourﬁndingsshowthatevenwiththebestthresholds,no
algorithmisabletoaccuratelydetectallfunctionalnear-duplicates
within apps,withoutsacriﬁcing coverage.
CCS CONCEPTS
•Software and its engineering →Software testing and de-
bugging.
KEYWORDS
near-duplicatedetection,reverseengineering,model-basedtesting
ACMReference Format:
Rahulkrishna Yandrapally, Andrea Stocco, and Ali Mesbah. 2020. Near-
Duplicate Detection in Web App Model Inference . In 42nd International
Conference on Software Engineering (ICSE ’20), May 23–29, 2020, Seoul, Re-
public of Korea. ACM, New York, NY, USA, 12 pages. https://doi.org/10.
1145/3377811.3380416
1 INTRODUCTION
Automatedtechniquessuchaswebappcrawlersarewidelyusedto
reverse-engineer state-basedmodelsasaviablevehicleforvarious
∗Thisworktook placewhile Andrea Stocco wasapost doc atUBC.
Permission to make digital or hard copies of all or part of this w ork for personal or
classroomuseisgrantedwithoutfeeprovidedthatcopiesarenotmadeordistributed
for proﬁt or commercial advantage and that copies bear this notice and the full cita-
tionontheﬁrstpage.Copyrightsforcomponents of thisworkowned byothersthan
the author(s) must be honored. Abstracting with credit is permitted. To copy other-
wise, orrepublish,to post onserversorto redistributeto lists,requirespriorspeciﬁc
permissionand/orafee. Request permissionsfrompermissions@acm.org.
ICSE ’20,May 23–29,2020,Seoul, Republic ofKorea
© 2020 Copyrightheld by the owner/author(s). Publicationrightslicensed to ACM.
ACM ISBN978-1-4503-7121-6/20/05...$15.00
https://doi.org/10.1145/3377811.3380416analysis and testing tasks such as automatedtest generation. The
stateinsuchmodelsrepresentsthedynamicwebpageoftheapp,as
representedbytheDocumentObjectModel(DOM)inthebrowser.
Crawlersare capableofeﬃciently exploring a largestatespaceof
any given web app. However, an adequate model should contain
only the minimal set of distinctstates that represent the web app
functionalities, while discarding insigniﬁcant states that do not
contributetoexposingnewfunctionalitytotheenduser.Instances
of such states are pages only diﬀering by small cosmetic changes,
which are also referred to as near-duplicates in the literature [23,
24,29,34].Todiscardsuchnear-duplicatewebpages,crawlershave
adopted state abstraction functions over the DOM [26, 36, 37, 45]
as a proxy for the similarity of webpages. The downside of these
abstractionsis that minimal changes totheDOM can resultin du-
plicate states in the model, even if such DOM changes are not re-
ﬂected on the ﬁnal UI visually, and therefore might not be rep-
resentative of a new webpage functionality. From an end-to-end
(E2E) testing perspective, clone and near-duplicate states in web
appmodelsnegativelyimpacttheiraccuracyandcompleteness,un-
dermining thequalityof thetest suites generated from such mod-
els interms ofsize, runtime, and coverage.
Clone and near-duplicate detection acrossdiﬀerent web apps
hasbeenanactiveresearch topicinmany ﬁelds[23,24,29,34].In
information retrieval, the content of a webpage has been the pri-
maryfocus,becausethepurposeofwebsearchengines istoindex
and retrieve information from webpages through search queries.
Computer vision techniques have been employed to detect visu-
ally similar webpages, for instance in phishing detection [2, 21].
Otherapproachesleverage stateabstractionsbasedonthesimilar-
ity of URLs, textual content and the DOM [17, 44, 53]. Detecting
near-duplicate pages is a challenging problem as there is no gen-
erally accepted deﬁnition of near-duplicate states and there is no
uniﬁedstandardagainstwhichatechniquecanbeassessed[28,29].
Asecondchallengepertainstotheselectionofsimilarity thresholds
that such techniques need as input to determine when two pages
are similar. These thresholds are usually educated guesses, as no
systematic means have been proposed so far to estimate them au-
tomatically.
In this work, we are interested in detecting distinct states in
web app modelsinthecontext of functional E2Eweb testing. Our
aim is to study the nature of duplicate states occurring withina
web app, and provide a systematic approach to selecting thresh-
oldsforinferringanoptimalmodel,i.e.,havingthelowestnumber
of(near-)duplicatestates.Tothisend,weevaluatethecapabilityof
10 near-duplicate detection algorithms in identifying clone, near-
duplicate, and distinct web app states. We adopt techniques from
three diﬀerent domains—information retrieval, web testing, and
computervision—wherethetextualcontent,theDOMtree,andthe
1862020 IEEE/ACM 42nd International Conference on Software Engineering (ICSE)
ICSE ’20,May 23–29, 2020,Seoul, Republic of Korea R ahulkrishnaYandrapally,AndreaStocco, and AliMesbah
visualscreenshotofthepageareusedtomeasurethesimilaritybe-
tween states. Our goal is to assess whether textual, structural, or
visual features are related with semantic properties of webpages
and provide meaningful means to understanding their degree of
functional relatedness from anE2Etestingperspective.
To select the similarity thresholds for ﬁne-tuning such
techniques, we ﬁrst crawled 6 kwebsites randomly selected from
Alexa’s top million URLs. We retrieved 493 kpairs of states
belonging to the same application, and computed the similarity
distance betweenthese pairs using each near-duplicate algorithm.
We then manually classiﬁed 1 krandom state-pairs into three
categories of clone, near-duplicate, or distinct. We used our
empirical data of distances to choose thresholds for each
algorithm through statistical and optimization search methods.
We evaluated their accuracy in automatically classifying clones
and near-duplicates in the remaining unlabelled portion of the
dataset. Further, we evaluated these conﬁgured algorithms on a
subject set of nine unseen web apps, for which manual ground
truthmodelswere previouslycreated.
Our work makes thefollowingnovel contributions:
•Theﬁrststudyof10diﬀerentnear-duplicatedetectiontech-
niques appliedinthecontext ofweb appmodelinference.
•Aclassiﬁcationofdiﬀerentcategoriesofnear-duplicatesoc-
curringwithina web app.
•Systematic ways of threshold selection for near-duplicate
detection as well as an empirical evaluation of their eﬀec-
tiveness in testmodels.
•Thetoolsetcomprisingthe10near-duplicatedetectionalgo-
rithms,which is availablefor download[5].
•A dataset of 99 kmanually classiﬁed pairs of webpages, of
which(1)1,5 kpairsarerandomlysampledfrom6kwebsites,
and (2) 97.5 kfrom nine real-size web apps.Ourdataset can
be used by others to conduct similar near-duplicate detec-
tionstudies and is also availablefordownloadpublicly[5].
Our results show that even with the best thresholds, no
algorithm is able to accurately detect all functional
near-duplicates within apps. In practice, existing near-duplicate
detectiontechniquesare notdesigned toﬁndfunctionalsimilarity
in a way that human testers regularly assess while testing web
apps. For certain types of near-duplicates, we observed that the
model deteriorates over time as the crawl progresses. For
instance, although RTED was able to achieve a high accuracy F1
score of 0.95 initially, the ﬁnal produced model had only an F1of
0.45. This deterioration is due to the accumulation of numerous
near-duplicates to the model, which decreases precision. Our
results underline the need for further research in devising
techniques geared speciﬁcally toward web test models, i.e., that
candistinguishbetweendiﬀerent typesofnear-duplicatessuchas
thosefoundin ourstudy.
2 REDUNDANCIES IN WEB APP MODELS
In practice, web testing is often performedin an end-to-end (E2E)
fashion, by verifying the correctness of the web app state in re-
sponse to user events and interactions with the GUI (e.g., clicks,
andformssubmissions).Thistaskisperformedeithermanuallyby
Figure1: Exampleof near-duplicateweb pages.
t
esters, or by writing test scripts with test automation tools such
as Selenium [46].
Automated techniques, on the other hand, generate web test
cases from models that are inferred through reverse-engineering
techniques. A popular method to model construction for modern
web apps is automated state exploration, also known as web app
crawling [35, 54]. Such techniques dynamically analyze the web
app under test by automatically ﬁring events and checking the
webpage for changes. When new state changes are detected, the
model is updated to reﬂect the event causing the new state. Gen-
eratedmodelscanberepresented invariousformatssuchas UML
statediagrams,FiniteStateMachines(FSM),orState-FlowGraphs
(SFG) [35,42,54].
To avoid redundancies in the model, states that are identical
or highly similar to previously encountered states should be dis-
carded. For instance, let us consider Figure 1, a web app in which
thehomepageshowsalistofphones.Whentheuserclicksonany
of the phones in that list, she is redirected to another web page
displayingthedetailedcharacteristicsoftheselectedphone.From
a functional testing viewpoint, however, a page containing a list
of 20 phones is conceptually the same as one listing the same 20
phones plusoneextra phone.
Theproblemofdetectingalreadyvisitedstatescanbecastasan
equivalenceproblem :giventwowebpagestates p1andp2explored
by the crawler, a state abstraction function determines whether
p1≃p2.Moreformally:
Deﬁnition 1 (State Abstraction Function) .A state abstraction
function(SAF) Aisapair(dist,t),wheredistisasimilarityfunction,
andtis a threshold deﬁned over the values of dist. Given two web
pages,p1,p2,Adetermineswhetherthe distancebetween p1andp2
fallsbelow t.
A(dist,p1,p2,t) 
true:dist(p1,p2)<t
false:otherwise
In practice, Ais deﬁned based on the similarity of some ab-
stracted notion of the web pages such as their URLs, textual con-
tent, DOM structure, or screenshot image. However, the amount
andnatureofchanges occurringinawebpagewithrespecttothe
functionality of the app is not always directly proportionalto the
amount of changes in the DOM tree, textual content, or visual as-
pectsof thepage.
Let us consider using a crawler equipped with a SAF based on
DOM content similarity on our sample web app of Figure 1. This
SAF is less tolerant to content (textual) changes occurringin web
pages. Therefore, each page displaying a new phone’s
187Near-Duplicate Detection in Web App Model Inference I CSE ’20,May 23–29, 2020,Seoul, Republic of Korea
Table 1: Near-Duplicatedetectionalgorithms includedinour study.
Domain Algorithm Input Description D istanceOutput
InformationRetrieval
W
ebSearch simhash [20] DOM(content) 64 bitﬁngerprintingtechnique which uses features extractedfrom
the webpagecontentHamming distance of two64 bitdigests
Malware detection TLSH [38] DOM(content) Locality-sensitive 256 bit hashing scheme that is robust to minor
variations of theinputHamming distance of two256 bitdigests
WebTesting
RTED [37,39] DOM(Tree) Minimum-costsequenceofnodeeditoperationsthattransformone
DOM treeinto anotherTreeedit distance value normalized bythesum of
nodes in thetwotrees
Levenshtein [30,36] DOM(String) Minimum number of single-character edits required to transform
one string into anotherEdit distance value normalized by the sum of the
string lengths
String Equality(baseline) DOM(String) String equality comparison Boolean value
Computer Vision
ImageHashing PHash [59] Screenshot 128 bit perceptual hash that represent the lowest frequencies of
pixel brightness, to which discrete cosine transform (DCT) is ap-
plied to retrieveabrightness matrixHamming distance of two128 bitdigests
Block-mean [57] Screenshot 256 bit perceptual hash obtained by dividing the image into non-
overlappingblocks,whichareencryptedwithasecretkeyandnor-
malized median valueis calculatedHamming distance of two256 bitdigests
Whole ImageComparison Histogram [52] Screenshot Color distribution of adigital image χ2distance between twocolor histograms
PDiﬀ [58] Screenshot Adopts a human-like concept of similarity that uses spatial, lumi-
nance, andcolor sensitivityNumber ofdiﬀerentpixels normalized bythemax-
imum number of pixels in thetwoimages
Structural Similarity SSIM [4] Screenshot Simulates thehighsensitivityofhumanvisualsystemtostructural
distortions while compensating for non-structural distortionsNormalized structural distortion value
FeatureDetection SIFT [32] Screenshot Computes local feature vectors and image descriptors which are
invarianttogeometric aﬃne transformationslikescaling/ rotationNumber of diﬀerent key-points normalized bythe
maximum number of key-pointsin both images
characteristics might be considered a diﬀerent state and man y
functionally similar occurrences of already modelled pages (i.e.,
near-duplicates) would be included in the model. If we use this
inﬂated model to generate test cases, the overall functional
coverage does not change when the generated tests exercise the
phone details page multiple times, thus potentially wasting
precious testingtimeand resources.
On theother hand, another “better” SAF, for instance based on
theDOMtreesimilaritywithaproperthresholdvalue,wouldcon-
sider all such phone detail pages as the same, providing a more
concise model for the web application of our example. However,
ahigh thresholdvaluemightcauseotherrelevant functionalityto
beabstractedawayas well,resulting inanincompletemodel.
Near-duplicate detection techniques have been studied for re-
ducingtheoccurrenceofredundantsimilarpages acrosswebapps,
e.g., inweb search engines [20]orphishing detection[38].Anun-
derstanding of whether such techniques apply also in detecting
functional near-duplicates withinthe same web app is missing in
theliterature.Despiteitsprevalenceandimportance,thisproblem
isunderstudied,becauseitishardtodeﬁneanotionofequivalence
for two arbitrary webpages. Moreover, in the general case, decid-
ingaprioriwhichabstractionfunctionandwhichthresholdwould
work best for a given web app is a challenging task, as it requires
substantial domain-speciﬁc knowledgeof theweb appunder test.
3 NEAR-DUPLICATE ALGORITHMS
Inthiswork,westudy10near-duplicatedetectionalgorithmsfrom
threediﬀerentdomains,namely,informationretrieval,webtesting,
and computer vision. Table 1 presents the techniques, along with
thedomaintheybelongto,theinputtypes,ashortdescription,and
their distance output.3.1 Information Retrieval
Near-duplicate detection has been applied to index the massive
volume of web pages continuously retrieved by web crawlers for
search engines. The overall goal is to select only a relevant set of
pagesbasedontheprovidedusersearchstring.Inthissetting,per-
formance is the most important factor, therefore hashing mecha-
nisms have beenadoptedduetotheir design simplicityand speed
of comparison. As an input, the web page content is typically the
primaryfocuswhendesigning algorithms usedin thisdomain.
We chose two content hashing algorithms from this domain:
(1)simhash [20], a popularand eﬀective web page ﬁngerprinting
technique adopted by Google to index web pages [29], and
(2) Trend Locality Sensitive Hash ( TLSH) [38], a hashing
technique for ﬁngerprinting source code, employed for malware
detection[55].
3.2 Web Testing
In the web testing domain, researchers have studied DOM-based
abstractions to compare webpages during the crawling of the ap-
plicationundertest.Theassumptionisthattwowebpagessharing
similaritiesamongtheirDOMsarelikelytorepresentpageshaving
analogousfunctionalities,henceitisworthwhiletoconsiderthem
the same. The DOM can be treated either as a tree-like structure,
oras a simplestringofcharacters.
We chose three diﬀerent similarity algorithms over the DOM
that have been employed as state abstraction functions in prior
web testing research [36, 37, 49]: (1) tree edit distance with the
RTEDalgorithm[39],(2)Levenshtein distance[30]overthestring
representedbytheDOM,and(3)stringequalitybetweentwoDOM
strings, which weuseas baseline.
188ICSE ’20,May 23–29, 2020,Seoul, Republic of Korea R ahulkrishnaYandrapally,AndreaStocco, and AliMesbah
3.3 Computer Vision
Imagesimilarityisoneofthemaintopicsincomputervision.Many
techniques have been proposed and studied, at diﬀerent levels of
granularity,rangingfromlow-levelpixelmatchinguptohigh-level
feature-based matching. Thesetechniques are appliedinindexing
and searching, summarization, object detection and tracking, fa-
cial recognition, and also copyright image detection. We consider
diﬀerent classes of image-based algorithms.
Image hashing techniques map visually identical or
nearly-identical images to the same (or similar) digest called
image hash. We chose two image hashing algorithms:
(1) block-mean hash [57] and (2) perceptual hash (PHash) [59],
which have been used in multimedia security for image retrieval,
authentication, indexing and copy detection. Whole image
matching techniques focus instead on individual pixels
composing the image. Color-Histogram [43] and Perceptual Diﬀ
(PDiﬀ) [58] have been successfully applied in previous web
testing work for detecting cross-browser incompatibilities [33]. A
downside of thosetechniques is that they are aﬀected by changes
in coordinates of web elements common in responsive web
layouts.Structural similarity techniques quantify image quality
degradation. For instance, Structural Similarity Index (SSIM) [4]
hasbeenshowntobeeﬀectiveduetothehighlystructurednature
of web apps’ GUIs [21]. Lastly, feature detection techniques have
been widely employed for near-duplicate image detection. For
instance, Scale Invariant Feature Transform (SIFT) [32] has been
appliedtoaid web test repair [51]and phishing detection[2].
To the best of our knowledge, this work is the ﬁrst to consider
and evaluate visual image similarity as a near-duplicate detection
techniqueforweb applicationcrawling.
4 EMPIRICAL STUDYDESIGN
Thegoal of ourstudyis todetermine how existing near-duplicate
detectiontechniques canbeemployedtoobtainanoptimalmodel
of a web applicationthat canbeusedforE2Etesting.
RQ1:Whattypeoffunctional near-duplicatesexist withinapps?
RQ2:How well can functional near-duplicatesbedetected?
RQ3:What is the impact of near-duplicates and detection tech-
niquesininferringa web-app model?
First, in Section 5, we randomly sample 1,000 within-app
state-pairs from a dataset created by crawling 6 krandomly
selected URLs. We characterize the changes occurring between
states within an app and identify how they lead to diﬀerent
classes of functional near-duplicates (RQ 1). We label these 1,000
pairs as either clones, near-duplicates or distinct states, and
compute the distance between them for all the ten near duplicate
techniques described inSection3.
In Section 6, using these labelled pairs, we compute statistical
andoptimalthresholdstoﬁne-tuneeachnearduplicatetechnique.
Through this, we aim to determine whether such randomly sam-
pled distances from a large dataset can be used to automatically
classify state-pairsinunseen webappsand detectnear-duplicates
(RQ2).
In Section 7, we determine the best near-duplicate detection
techniques and application-speciﬁc thresholds to infer web app
models for nine open-source web apps covering the diﬀerentnear-duplicate categories. Finally, we analyze these models to
determine how diﬀerent kinds of near-duplicates impact model
inference (RQ 3).
5 RQ 1:NEAR-DUPLICATES IN WEB APPS
Inordertodeterminewhatkinds offunctionalnear-duplicates oc-
cur within apps, we ﬁrst create a dataset of within app state-pairs
andtheircalculateddistances foreachnear-duplicatedetectional-
gorithm.Then,wemanuallycharacterizethenatureofdiﬀerences
betweenpairs ofpages and classify themin a randomsample.
5.1 DatasetCreation
First, we crawl randomly selected website URLs from the top one
million as provided by Alexa,1a popular website that ranks sites
based on their global popularityfor a weekusingC/r.sc/a.sc/w.sc/l.sc/j.sc/a.sc/x.sc [36],
an event-driven crawler for exploring highly dynamic web apps.
We conﬁgured C/r.sc/a.sc/w.sc/l.sc/j.sc/a.sc/x.sc to run using the Chrome browser, with
itsdefaultsimplestateabstractionfunction,namelystringequality
(see Table1),and a runtimelimitof ﬁveminutes for each crawl.
To account for network communication errors and the tool’s
explorationlimitations,e.g.,onsitesthatrequirelogincredentials,
weﬁlteredoutsitesforwhichthecrawlmodelsobtainedcontained
less than 10 states. After this ﬁltering stage, we retained 1,064 dif-
ferent sites accounting for 30,202 states from the original 6,359
web crawls. We then created all possible 677,415 pair-wise com-
binations of states withineach crawl ,which wecall state-pairs .
5.1.1 Computing Distances. We computed the distance for each
state-pairusingeachofthe10algorithmspresentedinTable1.We
discarded thestate-pairs for which thedistance couldnotbecom-
puted correctly, such as the case of DOM-based tree edit distance
ofmalformedHTMLtrees.Theﬁnaldataset,called DS,contained
1,031 sites and 29,704 states, from which 493,088 state-pairs with
properlycomputeddistances were obtained.
5.1.2 Distance Normalization. The raw distances which quantify
the diﬀerence between two given pages have diﬀerent output
spaces based onthe page characteristic used by thetechnique. As
an example, given a state-pair of web pages, PDiﬀ outputs the
number of perceptually diﬀerent pixels between their
screenshots, whereas BlockHash returns the hamming distance
between image hashes. For the sake of comprehensibility, we
normalized all distances computed by each algorithm, as
described in the Distance Output column of Table 1, but we never
compareoutputsof diﬀerent techniques.
5.2 Classiﬁcation of Changes
Togainabetterunderstandingofwhatchanges withinwebpages
characterize near-duplicates, we classify the diﬀerences of the
state-pairs in our dataset from the point of view of a human
testerwho is interested in functionalitycoverage.
5.2.1 Procedure. Manually examining state-pairs is a time
consumingtask requiringfamiliarity withthefunctionalityofthe
application. Therefore, we randomly sampled a set, called RS, of
1,000 state-pairs from our ﬁnal dataset of 493,088 state-pairs,
1http://www.alexa.com
189Near-Duplicate Detection in Web App Model Inference I CSE ’20,May 23–29, 2020,Seoul, Republic of Korea
which allows us to have a conﬁdence level of 99% with a 4%
margin of error in deriving a representative statistic. For each
state-pair (pi,pj) ∈ S, the authors of the paper visually analyzed,
in isolation, the screenshot images (and the original web pages
where necessary) of the two web app states from a functional
testing perspective, to obtain a set Dof diﬀerences. Each
diﬀerence in Dis deﬁned as ∆(pi,pj)={δ(ei,ej)}whereδ(ei,ej)
is a pair of non-identical web elements in which ei∈piand
ej∈pj. Finally, each author assigned a descriptive label to each
detecteddiﬀerence.
5.2.2 DiﬀerenceCategorization. Afterenumeratingalldiﬀerences
across the 1,000 state-pairs in RS, the authors reviewed them to-
gether to resolve conﬂicts and reached consensus on equivalence
classes ofdiﬀerences. Ourstudyrevealed thefollowingcategories.
Deﬁnition2 (Unrelated( U)).Givenadiﬀerence δ(ei,ej),neither
ofeiorejarerelated toany functionalityoﬀered bytheweb app.
Examples of these diﬀerences include changes in background
images, or GUI widgets related to advertisement (see red ovals
in Figure 2a).
Deﬁnition3 (Duplicated( D)).Givenadiﬀerence δ(ei,ej),eiand
ejreplaceeachotherintheoriginalpages piandpjwithoutadding
any new functionalitytoeither page.
Two distinct subcategoriesof duplicateddiﬀerences emerged:
•Replacement (D1):D1:ei≡ejmeaning the diﬀerence rep-
resents a functionality or content that is equivalent. For in-
stance, in Figure 2b, thered ovals highlight equivalent con-
tent.
•Addition(D2):D2:ei=∅ ∧∃e′
i∈pi:e′
i=ej∨δ(e′
i,ej) |=
D1meaning the non-empty exinδhas a duplicate e/y.altin
thesamepage,andthereforeitsadditiondoesnotaﬀectthe
overall functionality of the page. For example, in Figure 2c,
theovalidentiﬁes a duplicationofanexistingfunctionality.
Deﬁnition4 (New(N)).Givenadiﬀerence δ(ei,ej),δrepresents
a new functionalityora semantically diﬀerent content, i.e.:
δ(ei,ej) |=N:(ei=∅)∧(/nexistse′
i∈p1s.te′
i=ej∨δ(e′
i,ej) |=D).
For example, the search box in Figure 1 is absent in phone de-
scriptionpages and is anexampleof new functionality.
5.2.3 State-Pair Classification. Following the classiﬁcation of
diﬀerences described above, we classiﬁed state-pairs from a
functional point of view, in three distinct categories deﬁned as
follows.
Deﬁnition 5 (Functional Clone ( Cl)).Given two web pages p1
andp2, the state-pair ( p1,p2) is a functional clone ( Cl) if there are
no semantic, functional or perceptible diﬀerences between them,
deﬁned as Cl:∆(p1,p2)=∅.
Deﬁnition6 (FunctionalDistinct( Di)).Giventwowebpages p1
andp2,p1isfunctionallydistinctfrom p2ifthereisanysemanticor
functional diﬀerence between thetwopages, Di:∃δ(ei,e2) |=N.
Deﬁnition 7 (Functional Near-Duplicate ( Nd)).Given two
web pages p1andp2,p1is a functional near-duplicate of p2if the
changes between the states do not change the overall
functionality beingexposed: Nd:∆/ne}ationslash|=Cl∧/nexists(δ(e1,e2) |=N) ∈∆.
(a)N ear-Duplicate(Nd 1): BackgroundImage Changes
(b)N ear-Duplicate(Nd 2): DynamicData
(c)N ear-Duplicate(Nd 3): DuplicatedFunctionality
Figure2: Diﬀerent subclassesof near-duplicatestate-pairs.
We further observed three ﬁne-grained subclasses of
near-duplicates inourdataset.
Cosmetic( Nd1)when changes related to the aesthetics of the
webpage such as advertisements or background images
occur, which leave the functionalities unaltered (see
Figure 2a): Nd1:∆(p1,p2) ∋δ(e1,e2) |=U
Dynamicdata( Nd2)when both states of the pair are generated
from the same template and populated with dynamic data,
according to a user query or app business logic (see Fig-
ure2b):Nd2:∆(p1,p2) ∋δ(e1,e2) |=D1∨U
Duplication( Nd3)when there are additional web elements in a
pagethefunctionalityandsemantics ofcontent ofwhichis
entirely represented within the other page (see Figure 2c):
Nd3:∃δ(e1,e2) |=D2∈∆(p1,p2)
Following these deﬁnitions, we manually labelled the 1,000
state-pairs in RS, and found 441 clones, 275 near-duplicates (45
Nd1,219Nd2,11Nd3),and 284distinct pairs.
6 RQ 2:CLASSIFICATION OFSTATE-PAIRS
6.1 Subject Systems
To address RQ 2(and later RQ 3), we need to infer models with dif-
ferent algorithms and thresholds numerous times, which requires
web apps withdeterministic behaviours.
190ICSE ’20,May 23–29, 2020,Seoul, Republic of Korea R ahulkrishnaYandrapally,AndreaStocco, and AliMesbah
Table 2: SubjectSetwith Manual Classiﬁcation
Bins
States
Pairs
ClonesNear-Duplicates
DistinctNd2N d 3Total
Addressbook 25 131 8,515 26 52 2,295 2,347 6,142
P
etClinic 14 149 11,175 2 1,433 180 1,613 9,411
Claroline 36 189 17,766 2,707 71 0 71 14,988
Dimeshift 21 153 11,628 375 570 0 570 10,683
PageKit 20 140 9,730 0 904 3,044 3,948 5,782
Phoenix 10 150 11,175 1 25 4,580 4,605 6,569
PPMA 23 99 4851 64 467 0 467 4,320
MRBS 14 151 11,325 27 4,044 0 4,044 7,254
MantisBT 53 151 11,325 2 1,117 0 1,17 10,206
Total 2 16 1,313 97,490 3,204 8,683 10,099 18,782 75,355
To this aim, we selected nine open-source web apps (Table 2)
u
sed in previous research of web testing [15, 16, 49, 50], as
subjects: Claroline ( v. 1.11.5) [7], Addressbook ( v. 8.2.5) [40],
PPMA (v. 0.6.0) [11], MRBS ( v. 1.4.9) [12] and MantisBT ( v.
1.1.8) [13] are open-source PHP-based applications while
Dimeshift ( commit 261166d ) [8], Pagekit ( v. 1.0.16) [9], Phoenix ( v.
1.1.0) [10] and PetClinic ( commit 6010d5 ) [6] are web apps that
cover popular JavaScript frameworks Backbone.js ,Vue.js,
Phoenix/React andAngularJS ,respectively.
Notethattheseninesubjectappsarenotpartofthedataset DS.
6.2 ManualClassiﬁcation (Ground Truth)
We set out to create manually labelled models for each subject,
which wecanuseas ground truthsforcomparisonoftechniques.
First, we use C/r.sc/a.sc/w.sc/l.sc/j.sc/a.sc/x.sc to create a master crawl model with
default depth-ﬁrst exploration strategy, default state abstraction
functionbasedonDOMstringequality,andamaximumtimebud-
get of one hour,which allow us tocapturea large portionof each
app’s statespace.
Next, we created state-pairs from the states in each model, as
follows. The authors of this paper manually classiﬁed each state-
pair into a clone, near-duplicate (with subcategories) or distinct
category, following the same procedure described in Section 5.2.
In addition, we also assigned each state to a binthat represents a
part of the application’s state space devoted to a certain function-
ality. As such, each bin is a logical container for all dynamically
generatedconcretewebpagesuponcrawling(e.g.,allwebpagesre-
lated tologin). We consider the ﬁrst concrete instance of a bin B
to be acoverage of Bby that crawl model. Additional concrete in-
stancesofabinareconsideredclonesornear-duplicatesofthebin
B.
Table2showsthemastercrawlcharacteristicsforeachwebapp
as well as our classiﬁcation outcome. In the rest of the paper, we
refertotheninemastercrawlswithmanuallyclassiﬁed97.5 kstate-
pairs of the nine apps as subject set (SS), and to our manual clas-
siﬁcation and identiﬁed bins as ground truth .Our classiﬁcation of
thesubject-setdidnotﬁndanynear-duplicatesofcategory Nd1in
SSas the subjects did not feature unrelated changes ( U) such as
advertisements, commonly found in other kind of websites. Man-
tisBT has the most bins (53), representing a state-space ﬁve times
bigger than that of Phoenix, which has the smallest number of
bins (10). Addressbook, PageKit and Phoenix have a high numberTable 3: Averagewebpage characteristics
state(DOMand Screenshot)across thetwo datasets
DOM I/m.sc/a.sc/g.sc/e.sc
Tree Source Content Pixels
(
# nodes) (length) (length) (#)
Dataset(D S ) 810 105,445 45,575 3,575,837
Subjects( SS) 290 17,655 6,216 1,190,230
of near-duplicates of category N d3, diﬀerently from the other six.
To study how diﬀerent near-duplicate categories impact web-app
model inference, we group these three subjects referring to them
asNd3-Apps and theother sixas Nd2-Apps .
Table3compares thesubjectswebpage characteristicsinterms
of DOM size, complexity, and image size to DS. For example, the
content of a web page in DSon an average is almost eight times
thatoftheweb pages in SS.
6.3 Threshold-Based Classiﬁcation
We aim to evaluate the eﬀectiveness of the near-duplicate detec-
tion algorithms in classifying a given pair as either clone, near-
duplicate,ordistinct.Essentially,thisisamulti-classclassiﬁcation
problem, which we propose to solve using a classiﬁcation func-
tionΓ. Function Γtakes as inputs a near-duplicate detection algo-
rithmfand computes the distance between two given states in a
state-pair (p1,p2),classifying thepairtoacategoryaccording toa
threshold-pair (tc,tn),as follows:
Γ(p1,p2,f,tc,tn) 
Cl:f(p1,p2)<tc
D:f(p1,p2)>tn
Nd:otherwise
Toevaluate Γ, weneed to ﬁnd appropriatethreshold values for
each algorithmthat maximizetheclassiﬁcationscores.
6.3.1 Threshold Determination. We employ two diﬀerent
approaches, namely, statistical andoptimization ,toﬁnd a suitable
threshold-pair (tc,tn)for each algorithm. In the statistical
approach, we follow a data-based approach in which we use the
distance distributions of diﬀerent classes (Figure 3). In the
optimization approach, instead, we determine the thresholds that
maximize the classiﬁcation score on a given labelled set, a
commonly adopted strategy in machine learning for
hyper-parameters selectionofpredictive models[47].
Deﬁnition 8 (Statistical Threshold Pair (Stc,Stn)).Threshold
Stcis the 3rd quartile ( Q3) of the distances calculated by a tech-
niqueonagiven setofclonestate-pairs,whereas,threshold Stnis
themediandistanceona given set of near-duplicatestate-pairs.
Deﬁnition 9 (Optimal Threshold Pair ( Oc,On)).Given a la-
belled set of clones, near-duplicates and distinct state-pairs, the
optimal thresholds OcandOnare retrieved by a Bayesian opti-
mizationsearchthatmaximizestheaverage F1classiﬁcationscore
forΓover allthreeclasses.
Figure 3 shows the distribution of distance values among the
threeclasses,foreachconsideredalgorithm.Asthebox-plotsshow,
191Near-Duplicate Detection in Web App Model Inference I CSE ’20,May 23–29, 2020,Seoul, Republic of Korea
Figure3:NormalizedDistancedistributionoflabelledpair sinthedataset DS.Withineachbox-plot, fromlefttoright:clone,
near-duplicateanddistinctpairs.
Table4:Estimatedstatistical( St)andoptimal( O)thresholds
for clone ( c)andnear-duplicate( n)bounds, in dataset DS
Stc_DSStn_DS Oc_DS On_DS
TLSH 0.00794 0.00794 0.01742 0.07052
L
evenshtein 0.00638 0.01089 0.00704 0.07029
RTED 0.00000 0.00000 0.00007 0.04099
SimHash 0.00000 0.00000 0.00044 0.00108
BlockHash 0.00000 0.04082 0.00301 0.13371
HYST 6.52E-11 1.29E-09 1.15E-09 1.49E-08
PDIFF 0.00160 0.03800 0.00120 0.20080
PHASH 0.01754 0.17544 0.04018 0.32232
SIFT 0.16691 0.27993 0.10192 0.61876
SSIM 0.01000 0.08000 0.02020 0.15560
aclearseparationbetweendistancevaluesamongclasseseme rged
uponstatisticalanalysis(despitesomeoverlapscausedbyoutliers),
whichmotivatesusingthisdatatodeterminestatisticalthresholds
onRS.Forinstance,clones(left-mostplotforalltechniques)have
low distances, whereas distinct pairs have high distance scores.
Near-duplicates, as expected, lie in between those two categories
forall10techniquesconsidered inourstudy.Weusequartiledata
for choosing thresholds since prior work [31] has shown that the
median value is a better estimator of the central tendency than
mean insuch cases.
We refer to thefourthresholds { Stc_DS,Stn_DS,Oc_DS,On_DS}
asuniversal thresholds , as the state-pairs in DSrepresent a large
set of randomlyselected real-worldwebpages (see Section5.1).
6.3.2 ClassificationAccuracy. ToaddressRQ 2,weevaluatetheal-
gorithms by comparing the eﬀectiveness of Γ(Section 6.3.1) with
corresponding state-pair inputs. We evaluate the eﬀectiveness of
Γusing the F1measure, which is the harmonic mean of precision
Pr(ratio of correctly classiﬁed pairs to total number of classiﬁed
pairsineachclass),andrecall Re(ratioofcorrectlyclassiﬁedpairs
totheactualnumber ofpairs thatbelongtotheclass).
Sincewehavemorethantwoclasses,wetreatitasamulti-class
classiﬁcationproblem,andobtaintheaverage F1overthescoresof
allthreeclasses( Cl,Nd,D).However,thedatasetsareunbalanced,
i.e., the ratio of state-pairs of the classes are not equal; hence, we
employ macro-averaging, to avoid favouring classes with higher
representation [48]. We calculate the F1score of each algorithm
usingΓwiththeuniversal thresholds(seeTable4)ontwodisjoint
inputs: 1) a manually labelled random sample of 500 state-pairs,
TS,fromthedataset DS,and2)the 97.5klabelledpairsfrom SS.Table5:F1MeasureforStatisticalandOptimalthresholdsets
Algorithms
tatistical optimal All
(Stc_DS,Stn_DS) (Oc_DS,On_DS)
TS SS A vgTS SS AvgTS SS Avg
TLSH 0.50 0.40 0.45 0.56 0.44 0.50 0.53 0.42 0 .48
Levenshtein 0.54 0.46 0.50 0.59 0.48 0.54 0.57 0.47 0.52
RTED 0.50 0.45 0.47 0.57 0.50 0.54 0.53 0.48 0.50
SIMHash 0.48 0.17 0.33 0.48 0.17 0.33 0.48 0.17 0.33
BlockHash 0.62 0.54 0.58 0.66 0.50 0.58 0.64 0.52 0.58
HYST 0.52 0.37 0.44 0.57 0.31 0.44 0.55 0.34 0.44
PDIFF 0.63 0.57 0.60 0.67 0.53 0.60 0.65 0.55 0.60
PHASH 0.59 0.43 0.51 0.63 0.40 0.52 0.61 0.41 0.51
SIFT 0.59 0.44 0.52 0.61 0.47 0.54 0.60 0.45 0.53
SSIM 0.62 0.53 0.57 0.65 0.48 0.56 0.64 0.50 0.57
Average 0.56 0.44 0.50 0.60 0.43 0.51 0.58 0.43 0.51
R
andom 0.32 0.32 0.32 0.32 0.32 0.32 0.32 0.32 0.32
While the scores on T Scan validate these thresholds, scores
onSSassess the viability of discovering universal thresholds for
anear-duplicate detectionalgorithmfor unseen web apps.
6.3.3 Findings(RQ 2).Table5showsthe F1classiﬁcationscoresfor
alltechniques onthetwolabelledsets, TSandSS.As abaseline
tocomparethetechniques,weusea stratiﬁed-random-classiﬁer [1]
that classiﬁes each state-pair randomly based on proportions of
classes inthelabelledset.
Allevaluated techniquesperformbetteron TSthanSSwhen
universal thresholds are used (+15% on average). This result is not
surprisingas TSis sampledfrom DS,as well as RSfromwhich
wederived thesethresholds. SS,ontheotherhand,iscompletely
disjoint and diﬀerent from DS(Table3).
Although statistical andoptimalthresholdshavesimilaroverall
averageF1scores(0.50,0.51),itisimportanttonoticethatoptimal
thresholds perform worse than statistical thresholds on SS, con-
trarytoexpectation.
Thesetwoﬁndingsessentiallyindicatethat,thedistancethresh-
oldsforoptimalclassiﬁcation ofstate-pairs canvary basedonthe
characteristics of theparticularweb app.The thresholds obtained
from a labelled data such as RSare therefore, not necessarily ap-
plicableforarandomunseenwebapp.Hence, universalthresholds
thatcan classify any givenstate-pairmaynot befeasible .
Amongst the techniques, SimHash has the lowest average F1
score (0.17) onSS, almost 90% worse than the random baseline.
192ICSE ’20,May 23–29, 2020,Seoul, Republic of Korea R ahulkrishnaYandrapally,AndreaStocco, and AliMesbah
The results concur with ﬁndings of a previous study [29], which
pointstothefactthatthealgorithmispooratdistinguishingstates
that belongtothesame app.
On average, ﬁve out of top six techniques belong to the com-
putervisiondomain. PDIFFisthebestwithaclassiﬁcation F1score
of0.60,>85%better thanthe baseline and >13%,>20%better than
Levenshtein and TLSH, the best techniques in DOM and IR cat-
egories, respectively. On average, most visual techniques outper-
form DOM and IR techniques (with the exception of PHashand
color-histogram ). OnSS,PDIFFagain outperforms all techniques
whileBlockHash andSSIM, both visual, are the only other tech-
niques thathave an F1scoreofmorethan 0.50.
7 RQ 3:IMPACTONINFERRED MODELS
With RQ 3, we evaluate the impact of the near-duplicate detection
algorithms inautomatedweb appmodelinference.
RQ3.1:Howcanclassiﬁcationthresholdsbeappliedtostateabstrac-
tionfunctions(SAFs)?
RQ3.2:Candomainknowledgebeemployedtoimprovetheobtained
models?
RQ3.3:How doeseﬃciencyof SAFsimpactthe obtainedmodels?
Speciﬁcally,weevaluatethequalityofcrawlmodelsinferredus-
ingeachofthenear-duplicatedetectionalgorithmsas stateabstract
function(SAF) (seeDeﬁnition1)alongwiththedeterminedthresh-
olds.C/r.sc/a.sc/w.sc/l.sc/j.sc/a.sc/x.sc already includes all DOM-based algorithms de-
scribedinSection3.2;weaddedthecomputervisionand informa-
tionretrievalnear-duplicatealgorithmswithin C/r.sc/a.sc/w.sc/l.sc/j.sc/a.sc/x.sc asSAFs.
Morespeciﬁcally,weintegratedtheimplementationsofPDiﬀ,SIFT,
and SSIM from the open-source computer-vision library OpenCV,
and thepubliclyavailable versions ofTLSH2andsimhash.3
Sinceweneedtorunandanalyzemanycrawlsessions(i.e.,nine
apps, 10 algorithms, diﬀerent threshold sets), we limit the crawl
session with a maximumruntime ofﬁve minutes.
Model Quality. We measure the quality of a generated model
through its F1score, the harmonic mean of PrandRe. Lower pre-
cision (Pr) denotes a greater redundancy inthemodeland is com-
puted as the ratio of unique states ( bins) covered by the model to
the total number of states in the model. Recall ( Re) quantiﬁes the
applicationstatecoverage achieved in themodeland is computed
asthenumberof binscovered bythemodeltothetotalnumberof
binsidentiﬁedbyhumans,forthecorrespondingapp,inthe ground
truth(seeSection6.2).
The recall Reof a crawl model is highly dependent on the abil-
ity of the SAF to reliably distinguish the distinct state-pairs and
itsprecision Pronitsabilitytoexcludenear-duplicatesandclones
of states already present from the model. Crawlers, however, typ-
ically expect one single similarity threshold for deciding if a state
is new to be added to the model; i.e., they do not distinguish be-
tween clone/near-duplicate. Therefore, we frame the problem of
ﬁndingoptimalthresholdsforaSAFasmaximizingthe F1scoreof
itsdistinct-pairdetection .
2https://github.com/idealista/tlsh
3https://github.com/albertjuhe/charikars_algorithmTable 6: Distinctpair ( Pr,Re,F1) onexisting datasets
TS SS A verage
Pr Re F 1P r Re F 1Pr Re F 1
On_DS0.81 0.81 0.80 0.89 0.53 0.64 0.85 0.67 0.72
Stn_DS0.63 0.90 0.73 0.87 0.76 0.78 0.75 0.83 0.76
7.1 Thresholds for SAFs(RQ 3 .1)
Beforeweemploythenear-duplicatetechniques asSAFsincrawl-
ing and evaluate the generated models, which is a manual and
timeconsumingprocess,we assess thetechniques and the univer-
salthresholds based on the F1score of the distinct-pair detection,
which indicates theapplicabilityof thetechniques as SAFs.
Findings (RQ 3.1).In the distinct state-pair detectionscores from
RQ2showninTable6,scores on TSallowustoassess theability
ofatechniquetodistinguish distinctstate-pairsin thewild,while
SSletsussimulateeach techniqueasaSAF ongenerated models
captured in our subject-set . In contrast to the RQ 2results, where
boththethresholdsetshadbetteraverage classiﬁcation F1onTS
compared to SS, Table 6 shows that statistical threshold had bet-
ter distinct state-pair detection F1of 0.78 on SSthan 0.73 in TS.
Optimalthreshold On_DS,whichishigher/stricterthan Stn_DS,in
terms of actual threshold value, as shown in Table 4, has a poor
recallonSS(53%) compared to TS(81%). Also in TSstatistical
threshold has the highest recall, but by sacriﬁcing precision; the
optimal threshold emerges with a better overall F1score through
a25%betterprecisionon TS.Thesamethreshold,however,could
notimprove precisionin SSbuthas 50%lower recall.
As we optimized our threshold to be stricter to ﬁt the distribu-
tion inDS, we ended up misclassifying distinct pairs to be near-
duplicates in SSbecause of the diﬀerences in the distributions
betweenthetwodata-sets.AswepointedoutinRQ 2,theseresults
showtheinfeasibilityofﬁndinguniversalthresholdsasthedistances
for state-pairs are highly inﬂuenced by the intrinsic characteristics
ofthe web apptheybelongto .
7.2 UsingApplication Knowledge(RQ 3.2)
These results for universal thresholds prompted us to investigate
whetherhavingknowledgeofthewebappcharacteristicshelpsin
selecting better thresholds to improve the detection rates of the
techniques.
We use the manually labelled models (see Section 6.2) in the
subject-set ( SS) for each app to represent application knowledge.
In order to use this application knowledge, we apply the
near-duplicate threshold deﬁnitions in Deﬁnition 8 and
Deﬁnition 9 to each subject in SSto derive Stn_SSandOn_SS
respectively. In addition to these two thresholds, through initial
experiments, we have observed that category Nd3
near-duplicates overlap with distinct ( Di) pairs and it is not
possible to design a threshold that can distinguish them. We
therefore created a new threshold deﬁnition that sacriﬁces the
precision of distinct pair detection by allowing misclassiﬁcation
ofNd3near-duplicates as Diforbetterrecall ( Re).
193Near-Duplicate Detection in Web App Model Inference I CSE ’20,May 23–29, 2020,Seoul, Republic of Korea
Table 7: Inferredmodel F1score
Universal App-SpeciﬁcStn_DS
On_DS
Avg
Stn_SS
Stn3_SS
On_SS
Avg
AddressBook 0.33 0.27 0.30 0.17 0.46 0.41 0.34
P
etClinic 0.36 0.25 0.31 0.50 0.50 0.52 0.51
Claroline 0.30 0.18 0.24 0.42 0.42 0.44 0.43
DimeShift 0.31 0.22 0.26 0.33 0.33 0.38 0.34
PageKit 0.30 0.27 0.29 0.27 0.39 0.37 0.34
Phoenix 0.44 0.29 0.37 0.24 0.47 0.42 0.38
PPMA 0.31 0.19 0.25 0.49 0.49 0.51 0.49
MRBS 0.37 0.35 0.36 0.43 0.43 0.46 0.44
MantisBT 0.24 0.18 0.21 0.26 0.26 0.27 0.26
Average 0.33 0.24 0.29 0.34 0.41 0.42 0.39
N
d2-Apps 0.32 0.23 0.27 0.40 0.40 0.430.41
Nd3-Apps 0.36 0.28 0.32 0.23 0.440.40 0.35
Table8: Inferredmodel F1f or each algorithm
for selectedthresholdsThresholds
Apps
TLSH
SIMHash
Levenshtein
RTED
BlockHash
PHASH
HYSTPDIFFSIFTSSIMAverageAllFiveAll 0.10 0.05 0.47 0.620.46 0.39 0.41 0.34 0.34 0.35 0.35
Nd2 0.10 0.04 0.48 0.62 0.47 0.39 0.41 0.36 0.31 0.39 0.36
Nd3 0.10 0.06 0.43 0.62 0.43 0.40 0.41 0.29 0.39 0.28 0.34On_SSAll 0.15 0.08 0.48 0.55 0.54 0.49 0.54 0.45 0.42 0.51 0.42
Nd2 0.17 0.08 0.53 0.61 0.52 0.49 0.58 0.46 0.37 0.52 0.43
Nd3 0.10 0.10 0.37 0.430.58 0.49 0.45 0.42 0.52 0.51 0.40Stn3_SSAll 0.09 0.03 0.46 0.67 0.57 0.50 0.55 0.43 0.36 0.48 0.41
Nd2 0.08 0.02 0.47 0.62 0.55 0.50 0.53 0.44 0.35 0.46 0.40
Nd3 0.10 0.07 0.44 0.760.60 0.51 0.60 0.42 0.37 0.51 0.44
Deﬁnition 10. S tn3is deﬁned as the medianof the data distri-
butionofmanuallylabellednear-duplicates { Nd1∨Nd2}.Inother
words,Stn3isStncomputedafter excluding Nd3near-duplicates.
We refer to these thresholds obtained by applying application
knowledgein SSforeachalgorithmas app-speciﬁcthresholds .We
crawled each of our subjects with two universal and three app-
speciﬁc thresholds with each technique as a SAF, separately, and
assess thequalityof thegenerated models.
Findings (RQ 3.2).Table 7 shows the average F1of crawls for all
algorithms for each threshold. Overall, as expected, the univer-
sal optimal near-duplicate threshold On_DShas the worst score
of0.24; only half of the 0.42 scored by the best threshold On_SS,
theoptimalthreshold derived withapplicationknowledge. Onav-
erage, app-speciﬁc thresholds improve the model quality by 34%
comparedtouniversal thresholdsunderliningtheneedto consider
appcharacteristicstochoosethresholds .ForNd3-Apps ,itcanbeseen
thatStn3_SSderivedusingthestatisticalDeﬁnition10signiﬁcantly
(90%) improves the F1score over the Stn_SS, showing that thresh-
old design needs to consider ﬁne-grained near-duplicate categories
prevalent intheapp undertest.
Applicationknowledgeimproves generatedmodels.Table8shows theaverage F1scores foreach algorithmfor ﬁve
minute crawls on our subjects. RTEDconsistently outperforms
other techniques with an F1score of 0.62 averaged over all ﬁve
thresholds. it is 29% better than Levenshtein, the next best
algorithm.
The results for visual techniques in Table8 are contrary to our
expectation, given that, in RQ 2, they convincingly outperformed
the DOM and IR techniques in state-pair classiﬁcation using Γ.
Apart from being slow compared to DOM based algorithms as
shown in Table 9, visual techniques, rely on characteristics that
cannot directly capture diﬀerences corresponding to web
elements (e.g., SIFT keypoints). Techniques such as RTED, which
use a DOM characteristic on the other hand, can reliably capture
diﬀerences in individual web elements between given two web
pages, essential to be able to classify states similar to a human
tester.
In IR techniques, SimHash is not able to distinguish even two
completely diﬀerent states in our subject-set as already seen in
RQ2. TLSH on the other hand, fails to calculate digests for app
states of our subjects due to lack of enough complexity as shown
inTable3—thecontentinoursubjectsis1/9thofthecontentsize
inthewild.Therefore,weexcludeSimHashandTLSHfromfurther
analysis.
7.3 Impact of Eﬃciency (RQ 3.3)
Ananalysis ofvisitedstatesperminuteor speedofthealgorithms,
shown in Table 9, seems to suggest that faster algorithms such as
RTED(25 states per minute) could explore more states in a given
crawl time and improve its Rewheras, slower algorithms such as
PDiﬀ,whichcouldonlyexplore fourstates perminuteonanaver-
ageare ata clear disadvantage.
Table 8 shows that for all remaining eight techniques with the
exceptionofSIFT, On_SSforNd2-Appsand Stn3_SSforNd3-Apps
is thebestthreshold conﬁguration.
Table 9 shows the statistics of the 5-min crawls for each tech-
nique with their best threshold conﬁguration. Coverage ( Re) data
suggeststhat5minuteswasnotenoughtocoveralloftheappstate-
space. Therefore, we experiment with a longer crawl time, i.e., 30
minutes.Giventheexponentialnatureofincreaseinmanualeﬀort
to analyze larger crawl models, we limit this experiment to the
best performing techniques tuned with thresholds from the best
5-minute crawls presented in Table 9.We select the top fourtech-
niques based on F1scores, however, as discussed before, since the
slower algorithms were placed at a disadvantage in the 5-minute
crawls,wealsoincludePDiﬀandSSIMthatproducedmodelswith
thebestprecision( Pr)scoresof0.91and0.85(respectively12%and
6%betterthanRTEDwhich has thebest F1scoreof 0.66).
Findings (RQ 3.3).AverageF1scores shown in Table 10 for 30
minute crawls indicate that, when tuned correctly and given
enough time, Histogram, BlockHash, RTED and Levenshtein can
all perform well on Nd2-Apps meaning that they managed to
discard near-duplicates of type Nd2reasonably well. However, it
is surprising to see that PDiﬀ and SSIM score higher than all of
them on Nd3-Apps. Thus, we decided to analyze how F1has
changed over the 30 minutes for Nd3-Apps as opposed to the
Nd2-Apps.
194ICSE ’20,May 23–29, 2020,Seoul, Republic of Korea R ahulkrishnaYandrapally,AndreaStocco, and AliMesbah
Table 9: Techniques SpeedandInferred model( Re,Pr,F1)
for best5-minutecrawlsLevenshtein
RTED
BlockHash
PHASH
HYST
PDIFF
SIFT
SSIM
Speed 1 12517 16 16 45 8
Recall 0.420.61 0.49 0.49 0.55 0.30 0.28 0.39
Precision 0.84 0.79 0.75 0.79 0.72 0.910.710.85
F1 0.54 0.66 0.54 0.520.58 0.44 0.39 0.51
Table 10: Inferredmodel F1f or 30-Minutecrawls
Apps
BlockHash
Hyst
Levenshtein
PDiﬀ
RTED
SSIM
All 0 .51 0.57 0.53 0.52 0.62 0.56
Nd2 0.57 0.62 0.59 0.51 0.66 0.52
Nd3 0.39 0.47 0.42 0.56 0.52 0.64
A plot of F1o f the model over its states percentage for RTED
crawls is shown in Figure 4. The ﬁgure highlights that for
Nd3-Apps, the model deteriorates as states being added are
near-duplicates, mostly of type Nd3, while, the models of
Nd2-Apps seem to stabilize as Nd2near-duplicates are being
detected and discarded. During the manual analyses of models,
we observed that the Nd3near-duplicates are dynamically
created, typically through user-interactions that result in
addition/removal of web elements whose functionality already
exists in the state (e.g., addition/deletion of new rows in a table).
Not only is this newly created state a near-duplicate that will eat
into precious testing time, but each time the crawler revisits this
state, it may invoke the same creation path adding even more
duplicates resultingina never-ending loop.
Eﬃciency may negatively impact the generated model in time-
limitedcrawlsfor Nd3apps.
Given that RTED is the best algorithm and was ﬁne-tuned to
producebestmodelforeachapplication,thissurprisingrevelation
points to the limitation of existing crawlers and threshold based
SAFs and shows that threshold based crawling may never produce
an accurate and complete model of modernweb apps with dynamic
Nd3near-duplicates .WethereforethinkthatfutureSAFsshouldin-
corporatecharacteristicsthatrepresentfunctionalityandcrawlers
should bedesigned to utilizenear-duplicate detectionto establish
the natureof duplicationinstead of quantifying thecomputeddif-
ferences to actively guide the exploration to discover newer func-
tionality.
8 THREATS TOVALIDITY
Externalvalidity threatsconcernthegeneralizationofourﬁndings.
We considered only nine web apps and experiments with other
subjectsystemsarenecessarytofullyconﬁrmthegeneralizability
of our results, and corroborate our ﬁndings. We tried to mitigate
Figure4: Normalized F1o ver %(statesin model)
during 30-minutecrawlsof RTED
this threat selecting real-world web apps with diﬀerent sizes, per-
taining to diﬀerent domains, and adoptedin previous web testing
work [15,16,49].Another threat concerns theselectionof thresh-
olds for near-duplicate detection techniques, whose results may
notgeneralize tootheralgorithms.Wemitigatedthis threat byse-
lecting 10 techniques from three diﬀerent domains: web testing,
computervisionandinformationretrieval. Internalvalidity threats
concern uncontrolled factors that may have aﬀected our results.
A possible threat is represented by the manually created ground
truth,whichwasunavoidablebecausenoautomatedmethodcould
provideus withtheideal classiﬁcationof web pages. Tominimize
thisthreat,theauthorsofthispapercreated,inisolation,aground
truth. Then, the two established a discussion to produce a single
ground truthforeach web app.
For reproducibility of the results, we made our tool, datasets
andusedsubjectsystemsavailable[5],alongwithrequiredinstruc-
tions.
9 RELATED WORK
A large body of research has addressed the analysis of web sites
structurevia clustering for clone detectionand duplicateremoval
ofweb pages [18,19, 22–25,29,34,41,56].
Henzinger [29] performed an evaluation of two near-duplicate
detectionalgorithmsbasedonshingling ona largedataset of1.6B
web pages. Manku et al. [34] followed up on the work using
simhash to detect near-duplicates for web information retrieval,
data extraction, plagiarism and spam detection with promising
results. Fetterly et al. [23] study the evolution of near-duplicate
web pages over time and concludethat near-duplicates have little
variability over time, and two pages that have been found to be
near-duplicates of one another will continue to be so for the
foreseeablefuture.
Our study is diﬀerent from the above work as we aim to de-
tectnear-duplicates withinweb apps and not across diﬀerent web
apps.Regarding detectionof within app near-duplicates, Calefato
et al. [19] proposea method to identify near-duplicates as well as
functional clone web pages based on a manual visual inspection
of the GUI. Crescenzi et al. [22] propose a structural abstraction
for web pages as well as a clustering algorithm that groups web
pagesbasedonthisabstraction.DiLuccaetal.[24,25]evaluatethe
Levenshtein distanceandthetagfrequencymethodsfordetecting
near-duplicate web pages. Eyk et al. apply simhash and broders
near-duplicatedetectionwithin Crawljax[27].
195Near-Duplicate Detection in Web App Model Inference I CSE ’20,May 23–29, 2020,Seoul, Republic of Korea
In mobiletesting research, researchers [3, 14] used mobile GUI
widget hierarchies in order to design optimal state abstractions.
Ourstudydidnotconsidersuchtechniquesastheyarenotdirectly
applicableforweb applications.
Tothebestofourknowledge, ourworkis theﬁrstonetostudy
diﬀerent near-duplication detection algorithms (from diﬀerent
ﬁelds) as SAFs in a web crawler. This paper is the ﬁrst to propose
a systematic categorization of near-duplicates in web apps, from
a functional E2E testing perspective and to study the impact of
near-duplicate detection on generated web application models
and web testing. Moreover, our paper is the ﬁrst to discuss
selection of thresholds for near-duplicate detection, an important
ﬁrststep.
10 CONCLUSIONS AND FUTUREWORK
Automaticallyassertingtheequalityoftwocomplexwebpagesisa
diﬃcultproblem,whichthestateabstractionfunctionofacrawler
needs to solve at runtime during the exploration. The problem is
further complicated by the presence of near-duplicates that need
tobedetectedandmappedtothelogicalpagesinordertoproduce
meaningful crawlmodels.
We study ten existing near-duplicate detection techniques
from three diﬀerent domains and compare their eﬀectiveness as
state abstraction functions in a crawler. Our results show that
near-duplicates characterized by dynamic data, as categorized in
the study, are detectable when application knowledge is
employed. However, near-duplicates characterized by duplication
of web elements, that are often a by-product of state exploration,
cannot behandled bythreshold-based modelinference.
Future work includes devising novel types of abstraction func-
tions, incorporating both page structural and visual characteris-
tics in a single hybrid solution to detect diﬀerent kinds of near-
duplicates.
REFERENCES
[1] [n.d.]. Stratiﬁed Random Classiﬁer. https://scikit-learn.org/stable/modules/
generated/sklearn.dummy.DummyClassiﬁer.html. Package:scikit-learn.
[2] S. Afroz and R. Greenstadt. 2011. PhishZoo: Detecting Phishing Websites by
Looking at Them. In 2011 IEEE Fifth International Conference on Semantic Com-
puting. 368–375. https://doi.org/10.1109/ICSC.2011.52
[3] D.Amalﬁtano, A.R.Fasolino, andP.Tramontana.2011. AGUI Crawling-Based
Technique for Android Mobile Application Testing. In 2011 IEEE Fourth Inter-
national Conference on Software Testing, Veriﬁcation and Validation Workshops .
252–261. https://doi.org/10.1109/ICSTW.2011.77
[4] and A. C. Bovik, H. R. Sheikh, and E. P. Simoncelli. 2004. Image quality assess-
ment: from error visibility to structural similarity. IEEE Transactions on Image
Processing 13, 4(April 2004), 600–612. https://doi.org/10.1109/TIP.2003.819861
[5] anon. 2019. Near-Duplicate Study Tools and DataSet For Replication. https:
//github.com/NDStudyICSE2019/NDStudy. GitHubRepository.
[6] app1 2018. Angular version of the Spring PetClinic web application. https://
github.com/spring-petclinic/spring-petclinic-angular.
[7] app3 2015. Claroline. Open Source Learning Management System. https:
//sourceforge.net/projects/claroline/.
[8] app4 2018. DimeShift: easiestway to track yourexpenses. https://github.com/
jeka-kiselyov/dimeshift.
[9] app52018. Pagekit:modularandlightweightCMS..https://github.com/pagekit/
pagekit.
[10] app6 2018. Phoenix: Trello tribute done in Elixir, Phoenix Framework, React
and Redux. https://github.com/bigardone/phoenix-trello.
[11] app72018. PHP PasswordManager. https://github.com/pklink/ppma.
[12] app82018. Meeting Room Booking System. https://mrbs.sourceforge.io/.
[13] app92018. MantisBugTracker. https://github.com/mantisbt/mantisbt.
[14] Young-Min Baek and Doo-Hwan Bae. 2016. Automated Model-Based Android
GUI Testing Using Multi-Level GUI Comparison Criteria. In Proceedings of the31stIEEE/ACMInternationalConferenceonAutomatedSoftwareEngineering (Sin-
gapore,Singapore) (ASE2016) .AssociationforComputingMachinery,NewYork,
NY, USA, 238?249. https://doi.org/10.1145/2970276.2970313
[15] Matteo Biagiola, Andrea Stocco, Ali Mesbah, Filippo Ricca, and Paolo Tonella.
2019. Web Test Dependency Detection. In Proceedings of 27th ACM Joint Eu-
ropean Software Engineering Conference and Symposium on the Foundations of
Software Engineering (ESEC/FSE2019) .ACM,12pages.
[16] Matteo Biagiola, Andrea Stocco, Filippo Ricca, and Paolo Tonella. 2019.
Diversity-basedWebTestGeneration.In Proceedingsof27thACMJointEuropean
Software Engineering Conference and Symposium on the Foundations of Software
Engineering (ESEC/FSE2019) .ACM,12 pages.
[17] Lorenzo Blanco, Nilesh Dalvi, and Ashwin Machanavajjhala.2011. Highly Eﬃ-
cient Algorithms for Structural Clustering of Large Websites. In Proceedings of
the 20th International Conference on World Wide Web (WWW ’11) . ACM, 437–
446.
[18] Andrei Z. Broder, Steven C. Glassman, Mark S. Manasse, and Geoﬀrey Zweig.
1997. Syntactic Clustering of the Web. Comput. Netw. ISDN Syst. 29, 8-13 (Sept.
1997), 1157–1166.
[19] Fabio Calefato, Filippo Lanubile, and Teresa Mallardo. 2004. Function Clone
Detection in Web Applications: A Semiautomated Approach. J. Web Eng. 3, 1
(May2004),3–21.
[20] MosesS.Charikar.2002. SimilarityEstimationTechniquesfromRoundingAlgo-
rithms.In Proceedings of theThiry-fourth Annual ACMSymposium on Theoryof
Computing (Montreal, Quebec, Canada) (STOC ’02) . ACM, New York, NY, USA,
380–388. https://doi.org/10.1145/509907.509965
[21] Teh-ChungChen,ScottDick,andJamesMiller.2010. DetectingVisuallySimilar
Web Pages:Application to Phishing Detection. ACMTrans.Internet Technol. 10,
2, Article5 (June 2010), 38pages. https://doi.org/10.1145/1754393.1754394
[22] ValterCrescenzi,PaoloMerialdo,andPaoloMissier.2005. ClusteringWebPages
Based on Their Structure. Data Knowledge Engineering 54, 3 (Sept. 2005), 279–
299.
[23] MarcNajorkDennisFetterly,MarkManasse.2004. OntheEvolutionofClusters
of Near-Duplicate Web Pages, In Journal of Web Engineering. Journal of Web
Engineering 2, 228–246.
[24] GiuseppeA.DiLucca,MassimilianoDiPenta,AnnaRitaFasolino,andPasquale
Granato. 2001. Clone Analysis inthe Web Era:anApproach to Identify Cloned
Web Pages. In Proceedings of the International Workshop of Empirical Studies on
Software Maintenance - November2001- Florence-Italy . 107–113.
[25] Giuseppe Antonio Di Lucca, Massimiliano Di Penta, and Anna Rita Fasolino.
2002. An Approach to Identify Duplicated Web Pages. 2013 IEEE 37th Annual
Computer Softwareand Applications Conference 00,undeﬁned (2002), 481.
[26] Cristian Duda, Gianni Frey, Donald Kossmann, Reto Matter, and Chong Zhou.
2009. AJAXCrawl:MakingAJAX ApplicationsSearchable.In Proceedingsofthe
2009IEEEInternationalConferenceonDataEngineering (ICDE’09) .IEEE,78–89.
[27] E.D.C. Van Eyk and W. J. Van Leeuwen. 2014. Performance of near-duplicate
detection algorithmsfor Crawljax . B.S.Thesis.
[28] Taher H. Haveliwala, Aristides Gionis, Dan Klein, and Piotr Indyk. 2002. Eval-
uating Strategies for Similarity Search on the Web. In Proceedings of the 11th
International Conference on World Wide Web (Honolulu, Hawaii, USA) (WWW
’02).ACM,NewYork,NY,USA,432–442. https://doi.org/10.1145/511446.511502
[29] MonikaHenzinger.2006.FindingNear-duplicateWebPages:ALarge-scaleEval-
uation of Algorithms. In Proceedings of the 29th Annual International ACM SI-
GIRConferenceonResearchandDevelopmentinInformationRetrieval (SIGIR’06) .
ACM,284–291.
[30] VILevenshtein.1966. BinaryCodesCapableofCorrectingDeletions,Insertions
and Reversals. Soviet PhysicsDoklady 10(1966), 707.
[31] Christophe Leys, Christophe Ley, Olivier Klein, Philippe Bernard, and Laurent
Licata.2013. Detectingoutliers:Donotusestandarddeviationaroundthemean,
use absolute deviation around the median. Journal of Experimental Social Psy-
chology49,4 (2013), 764– 766. https://doi.org/10.1016/j.jesp.2013.03.013
[32] D. G. Lowe. 1999. Object recognition from local scale-invariant features. In
Proceedings of Seventh IEEE International Conference on Computer Vision , Vol. 2.
1150–1157.
[33] Sonal Mahajan and William G.J. Halfond. 2014. Finding HTML Presentation
Failures Using Image Comparison Techniques. In Proc. of the 29th ACM/IEEE
InternationalConferenceonAutomatedSoftwareEngineering(ASE’14) .ACM,91–
96.
[34] GurmeetSinghManku,ArvindJain,andAnishDasSarma.2007.DetectingNear-
duplicatesforWebCrawling.In Proceedingsofthe 16thInternational Conference
on World Wide Web (WWW ’07) .ACM,141–150.
[35] Ali Mesbah. 2015. Advances in Testing JavaScript-based Web Applications . Ad-
vancesinComputers,Vol. 97. Elsevier,Chapter5, 201–235.
[36] AliMesbah,ArievanDeursen,andStefanLenselink.2012. CrawlingAjax-based
Web Applications through Dynamic Analysis of User Interface State Changes.
ACMTransactionson theWeb 6,1 (2012), 3:1–3:30.
[37] AminMilaniFardandAliMesbah.2013. Feedback-directedExploration ofWeb
Applications to Derive Test Models. In Proceedings of the International Sympo-
siumon Software Reliability Engineering(ISSRE) . IEEE, 278–287.
196ICSE ’20,May 23–29, 2020,Seoul, Republic of Korea R ahulkrishnaYandrapally,AndreaStocco, and AliMesbah
[38] J. Oliver, C. Cheng, and Y. Chen. 2013. TLSH – A Locality Sensitive Hash. In
2013FourthCybercrimeand TrustworthyComputingWorkshop .7–13.
[39] MateuszPawlikandNikolausAugsten.2015. EﬃcientComputationoftheTree
EditDistance. ACMTrans.DatabaseSyst. 40,1,Article3(March2015),40pages.
[40] PHP AddressBook. 2015. Simple, web-based address & phone book. http:
//sourceforge.net/projects/php-addressbook. Accessed: 2018-10-01.
[41] LakshmishRamaswamy,Arun Iyengar,Ling Liu,and Fred Douglis.2004. Auto-
maticDetectionofFragmentsinDynamicallyGeneratedWebPages.In Proceed-
ings of the 13th International Conference on World Wide Web (WWW ’04) .ACM,
443–454.
[42] Filippo RiccaandPaoloTonella. 2001. Analysisandtesting ofWebapplications.
InProceedingsofthe23rdInternationalConferenceonSoftwareEngineering (ICSE
’01).IEEE, 25–34.
[43] ShauvikRoyChoudhary,MukulR.Prasad,andAlessandroOrso.2013. X-PERT:
AccurateIdentiﬁcationofCross-browserIssuesinWeb Applications.In Proc.of
the2013International ConferenceonSoftware Engineering (ICSE’13) .IEEE Press,
702–711.
[44] SreedeviSampath.2012. AdvancesinUser-Session-BasedTestingofWebAppli-
cations.Advances in Computers 86(2012), 87–108.
[45] M. Schur, A. Roth, and A. Zeller. 2015. Mining Workﬂow Models from Web
Applications. IEEETransactionsonSoftwareEngineering 41,12(Dec2015),1184–
1201. https://doi.org/10.1109/TSE.2015.2461542
[46] Selenium 2018. SeleniumHQ Web Browser Automation. http://www.
seleniumhq.org/. Accessed: 2017-08-01.
[47] JasperSnoek,HugoLarochelle,andRyanPAdams.2012. PracticalBayesianOp-
timizationof MachineLearningAlgorithms. In Advances in Neural Information
ProcessingSystems25 ,F.Pereira,C.J.C.Burges,L.Bottou,andK.Q.Weinberger
(Eds.). Curran Associates, Inc., 2951–2959. http://papers.nips.cc/paper/4522-
practical-bayesian-optimization-of-machine-learning-algorithms.pdf
[48] Marina Sokolova and Guy Lapalme. 2009. A systematic analysis of perfor-
mance measures for classiﬁcation tasks. Information Processing Management
45,4 (2009), 427– 437. https://doi.org/10.1016/j.ipm.2009.03.002[49] Andrea Stocco, Maurizio Leotta, Filippo Ricca, and Paolo Tonella. 2016.
Clustering-AidedPageObjectGenerationforWebTesting.In Proceedingsof16th
International Conference onWeb Engineering (ICWE2016) .Springer,132–151.
[50] Andrea Stocco, Maurizio Leotta, Filippo Ricca, and Paolo Tonella. 2017.
APOGEN: Automatic Page Object Generator for Web Testing. Software Qual-
ity Journal 25,3 (Sept. 2017), 1007–1039.
[51] Andrea Stocco, Rahulkrishna Yandrapally, and Ali Mesbah. 2018. Visual Web
TestRepair.In Proceedings ofthe 201826thACMJointMeeting onEuropean Soft-
ware Engineering Conference and Symposium on the Foundations of Software En-
gineering (Lake Buena Vista, FL, USA) (ESEC/FSE 2018) . ACM, New York, NY,
USA, 503–514. https://doi.org/10.1145/3236024.3236063
[52] Michael J. Swain and Dana H. Ballard. 1992. Indexing via Color Histograms.
InActive Perception and Robot Vision , Arun K. Sood and Harry Wechsler (Eds.).
Springer BerlinHeidelberg,261–273.
[53] Anastasios Tombros and Zeeshan Ali. 2005. Factors Aﬀecting Web Page Simi-
larity.InProceedingsofthe27thEuropeanConferenceonAdvancesinInformation
Retrieval Research (ECIR2005) .Springer-Verlag,487–501.
[54] Paolo Tonella,Filippo Ricca,andAlessandroMarchetto.2014. Recent Advances
inWeb Testing. Advances in Computers 93 (2014), 1–51.
[55] J. Upchurch and X. Zhou. 2016. Malware provenance: code reuse detection in
malicious software at scale. In 2016 11th International Conference on Malicious
and Unwanted Software (MALWARE) . 1–9. https://doi.org/10.1109/MALWARE.
2016.7888735
[56] YitongWangandMasaruKitsuregawa.2001. LinkBasedClusteringofWebSearch
Results. Springer BerlinHeidelberg,225–236.
[57] B. Yang, F. Gu, and X. Niu. 2006. Block Mean Value Based Image Perceptual
Hashing. In 2006 International Conference on Intelligent Information Hiding and
Multimedia . 167–172.
[58] Hector Yee, Sumanita Pattanaik, and Donald P. Greenberg. 2001. Spatiotempo-
ralSensitivityandVisualAttentionforEﬃcientRenderingofDynamicEnviron-
ments.ACMTrans.Graph. 20,1 (Jan.2001), 39–65.
[59] Christoph Zauner.2010. Implementation and benchmarkingof perceptual image
hash functions . Ph.D. Dissertation.
197