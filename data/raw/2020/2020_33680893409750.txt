DeepSearch: A Simple and Effective Blackbox Attack
for Deep Neural Networks
Fuyuan Zhang
MPI-SWS, Germany
fuyuan@mpi-sws.orgSankalan Pal Chowdhury
MPI-SWS, Germany
sankalan@mpi-sws.orgMaria Christakis
MPI-SWS, Germany
maria@mpi-sws.org
ABSTRACT
Although deep neural networks have been very successful in image-
classification tasks, they are prone to adversarial attacks. To ge-
nerate adversarial inputs, there has emerged a wide variety of tech-
niques, such as black- and whitebox attacks for neural networks. In
this paper, we present DeepSearch, a novel fuzzing-based, query-
efficient, blackbox attack for image classifiers. Despite its simplicity,
DeepSearch is shown to be more effective in finding adversarial
inputs than state-of-the-art blackbox approaches. DeepSearch is
additionally able to generate the most subtle adversarial inputs in
comparison to these approaches.
CCS CONCEPTS
â€¢Computing methodologies â†’Neural networks ;â€¢Software
and its engineering â†’Software testing and debugging.
KEYWORDS
Adversarial Attack, Deep Neural Networks, Blackbox Fuzzing
ACM Reference Format:
Fuyuan Zhang, Sankalan Pal Chowdhury, and Maria Christakis. 2020. 
DeepSearch: A Simple and Effective Blackbox Attack for Deep Neural Net-
works. In Proceedings of the 28th ACM Joint European Software Engineering 
Conference and Symposium on the Foundations of Software Engineering (ES-
EC/FSE â€™20), November 8â€“13, 2020, Virtual Event, USA. ACM, New York, NY, 
USA, 13 pages. https://doi.org/10.1145/3368089.3409750
1 INTRODUCTION
Deep neural networks have been impressively successful in pattern
recognition and image classification [30, 39, 42]. However, it is
intriguing that deep neural networks are extremely vulnerable to
adversarial attacks [72]. In fact, even very subtle perturbations of
a correctly classified  image, imperceptible to the human eye, may
cause a deep neural network to change  its prediction. This poses
serious security risks to deploying deep neural networks in safety
critical applications.
Various adversarial attacks have been developed to evaluate the
vulnerability of neural networks against adversarial perturbations.
Early work on generating adversarial examples has focused on
whitebox attacks [12, 24, 40, 49, 51, 57]. In the whitebox setting,
attackers have full access to the network under evaluation, which
ESEC/FSE â€™20, November 8â€“13, 2020, Virtual Event, USA
Â© 2020 Copyright held by the owner/author(s).
ACM ISBN 978-1-4503-7043-1/20/11.
https://doi.org/10.1145/3368089.3409750enables them to calculate gradients of the network. Many gradient-
based attacks have been shown to be highly effective. However, in
several real-world scenarios, having complete access to network
parameters is not realistic. This has motivated the development of
blackbox adversarial attacks.
In the blackbox setting, attackers assume no knowledge about
the network structure or its parameters and may only query the
target network for its prediction when given particular inputs. One
important metric to measure the efficiency of blackbox attacks is
the number of queries needed, because queries are essentially time
and monetary costs for attackers, e.g., each query to an online,
commercial machine-learning service costs money. Evaluating the
robustness of deep neural networks in a query-limited blackbox
setting is already standard. Gradient-estimation-based blackbox at-
tacks [ 5,13], although effective, require a huge number of queries,
which makes generating an attack too costly. Various state-of-the-
art blackbox attacks (e.g., [ 27,32,33,50]) can already achieve suc-
cessful attacks with low number of queries. However, constructing
query-efficient blackbox attacks is still open and challenging.
In this paper, we develop a blackbox fuzzing-based technique
for evaluating adversarial robustness of neural networks. The two
key challenges of applying fuzzing here are (1)to maintain a high
attack success rate, and (2)to require a low number of queries.
In many cases, without careful guidance while searching, a naive
fuzzing approach, e.g., random fuzzing, is not able find adversarial
examples even after a huge number of queries. To improve attack
success rate, we introduce carefully designed feedback to guide our
search so that images are efficiently fuzzed toward the decision
boundaries. To reduce the number of queries, we adapt hierarchical
grouping [ 50] to our setting so that multiple dimensions can be
fuzzed simultaneously, which dramatically reduces query numbers.
Furthermore, a refinement step, which can be viewed as a back-
ward search step for fuzzing, can effectively reduce distortion of
adversarial examples. Therefore, we extend fuzz testing and show
how to apply it on neural networks in a blackbox setting.
Our approach. Inspired by the linear explanation of adversa-
rial examples [ 24], we develop DeepSearch, a simple, yet effec-
tive, query-efficient, blackbox attack, which is based on feedback-
directed fuzzing. DeepSearch targets deep neural networks for
image classification. Our attack is constrained by the ğ¿âˆdistance
and only queries the attacked network for its prediction scores
on perturbed inputs. The design of our approach is based on the
following three aspects:
(1)Feedback-directed fuzzing : Starting from a correctly classified
image, DeepSearch strategically mutates its pixels to values
that are more likely to lead to an adversarial input. The
800This work is licensed under a Creative Commons Attribution International 4.0 License.
ESEC/FSE â€™20, November 8â€“13, 2020, Virtual Event, USA Fuyuan Zhang, Sankalan Pal Chowdhury, and Maria Christakis
fuzzing process continues until it either finds an adversarial
input or it reaches the query limit.
(2)Iterative refinement : Once an adversarial input is found, our
approach starts a refinement step to reduce the ğ¿âˆdistance
of this input. The iteration of refinement continues until ei-
ther the query limit is reached or some termination criterion
is met. Our evaluation shows that iterative refinement is able
to find subtle adversarial inputs generated by only slightly
perturbing pixels in the original image.
(3)Query reduction : By utilizing the spatial regularities in in-
put images, DeepSearch adapts an existing hierarchical-
grouping strategy [ 50] to our setting and dramatically re-
duces the number of queries for constructing successful at-
tacks. The query-reduction step significantly improves the
efficiency of our fuzzing and refinement process.
We evaluate DeepSearch against four state-of-the-art blackbox
attacks in a query-limited setting, where attackers have only a
limited query budget to construct attacks. For our evaluation, we
use three popular datasets, namely SVHN [ 53], CIFAR- 10[38], and
ImageNet [ 61]. For SVHN and CIFAR- 10, we further attack neural
networks with state-of-the-art defenses based on adversarial train-
ing [49]. Our experimental results show that DeepSearch is the most
effective in attacking both defended and undefended neural net-
works. Moreover, it outperforms the other four attacks. Although
it is important to develop defense techniques against blackbox
adversarial attacks, it is not the focus of this paper and we leave it
for future work.
Contributions. We make the following contributions:
(1)We present a simple, yet very effective, fuzzing-based black-
box attack for deep neural networks.
(2)We perform an extensive evaluation demonstrating that
DeepSearch is more effective in finding adversarial examples
than state-of-the-art blackbox approaches.
(3)We show that the refinement step in our approach gives
DeepSearch the advantage of finding the most subtle adversa-
rial examples in comparison to related approaches.
(4)We show that the hierarchical-grouping strategy is effective
for query reduction in our setting.
Outline. The next section briefly introduces background. In
Sect. 3, we present DeepSearch for binary classifiers, that is, net-
works that classify inputs into two classes. Sect. 4 generalizes the
technique to multiclass classifiers, which classify inputs into mul-
tiple classes. In Sect. 5, we extend our technique with iterative
refinement such that the generated adversarial examples are more
subtle. We adapt hierarchical grouping for query reduction in Sect. 6.
We present our experimental evaluation in Sect. 7, discuss related
work in Sect. 8, and conclude in Sect. 9.
2 BACKGROUND
In this section, we introduce some notation and terminology. Let Rğ‘›
be theğ‘›-dimensional vector space for input images. We represent
images as column vectors x=(ğ‘¥1,...,ğ‘¥ğ‘›)ğ‘‡, whereğ‘¥ğ‘–âˆˆR(1â‰¤ğ‘–â‰¤
ğ‘›) is theğ‘–th coordinate of x. We also write x(ğ‘–)to denote the ğ‘–th
coordinateğ‘¥ğ‘–, i.e., x(ğ‘–)=ğ‘¥ğ‘–, and each such coordinate represents
an image pixel. Now, let ğ¶ğ‘š={ğ‘™1,...,ğ‘™ğ‘š}be a set of labels for ğ‘šclasses, where ğ‘™ğ‘–is the label for the ğ‘–th class ( 1â‰¤ğ‘–â‰¤ğ‘š). A deep
neural network that classifies images from Rğ‘›intoğ‘šclasses inğ¶ğ‘š
is essentially a function N:Rğ‘›â†’ğ¶ğ‘š. For an input xâˆˆRğ‘›,N(x)
is the label that the network assigns to x.
Assume that input xis correctly classified, and xâ€²is generated by
applying subtle perturbations to x. These perturbations are subtle
when the distance between xandxâ€²inRğ‘›is sufficiently small
according to a distance metric. When this is so and N(x)â‰ N(xâ€²),
we say that xâ€²is an adversarial example [72]. In other words, the
network is tricked into classifying xâ€²into a different class than x
even though they are very similar.
In this paper, we use the ğ¿âˆdistance metric. The ğ¿âˆdistance
between xandxâ€²is defined as the maximum of their differences
along any coordinate dimension ğ‘–(1â‰¤ğ‘–â‰¤ğ‘›):
||xâˆ’xâ€²||ğ¿âˆ=max
ğ‘–(|ğ‘¥ğ‘–âˆ’ğ‘¥â€²
ğ‘–|)
Forğ‘‘âˆˆR, we writeB(x,ğ‘‘)to denote the set of images within
distanceğ‘‘from x, i.e.,B(x,ğ‘‘)={xâ€²|||xâˆ’xâ€²||ğ¿âˆâ‰¤ğ‘‘}, which is
anğ‘›-dimensional cube. Based on the above, a deep neural network
Nislocally robust for a correctly classified input xwith respect to
distanceğ‘‘if it assigns the same label to all images in B(x,ğ‘‘).
We mention here that numerous attacks are optimized for one
distance metric (e.g., [ 5,6,13,24,27,32,40,49,50,57]), just like ours.
Although there exist other distance metrics, e.g., ğ¿0andğ¿2, and
extending attacks from one metric to another is possible, developing
an attack that performs best in all distance metrics is not realistic.
Many state-of-the-art attacks are the most effective in one metric,
but their extension to other metrics performs worse than attacks
specifically designed for that metric. Our paper focuses on a query-
efficientğ¿âˆattack (as in [ 32,50]), and our technique outperforms
the state-of-the-art in this setting.
Query-limited blackbox threat model. We assume that attack-
ers have no knowledge of the target network and can only query the
network for its prediction scores, e.g., logits or class probabilities.
Moreover, we assume that attackers have a query budget, which can
be viewed as time or monetary limits in real-world settings. Thus,
the blackbox attack we consider in this paper can be described as
follows. Given an input x, distanceğ‘‘, and query budget ğ¿, an at-
tacker aims to find an adversarial example xâ€²inB(x,ğ‘‘)by making
at mostğ¿queries to the neural network.
3 FUZZING BINARY CLASSIFIERS
In this section, we present the technical details of how DeepSearch
fuzzes (linear and non-linear) binary classifiers. We first introduce
our approach for linear binary classifiers, which serves as the mathe-
matical foundation. Then, we generalize our approach to non-linear
binary classifiers through iterative linear approximations.
3.1 Linear Binary Classifiers
Abinary classifier classifies inputs into two classes, denoted with
labelsğ¶2={ğ‘™1,ğ‘™2}, according to the definition below.
Definition 1(Binary Classifier) .Given a classification function
ğ‘“:Rğ‘›â†’R, abinary classifierNğ‘“:Rğ‘›â†’ğ¶2is defined as follows:
Nğ‘“(x)=ğ‘™1, ifğ‘“(x)>0
ğ‘™2, ifğ‘“(x)<0
801DeepSearch: A Simple and Effective Blackbox Attack for Deep Neural Networks ESEC/FSE â€™20, November 8â€“13, 2020, Virtual Event, USA
If functionğ‘“is linear, thenNğ‘“is alinear binary classifier, otherwise
it is non-linear.
The set of valuesDğ‘“={x|ğ‘“(x)=0}constitute the decision
boundary ofNğ‘“, which classifies the domain Rğ‘›into the two classes
inğ¶2.
As an example, consider Fig. 1a, showing a linear binary classifier
Nğ‘“:R2â†’ğ¶2. Observe that input x0is classified in ğ‘™1whereas
xâ€²
0is inğ‘™2. Note that the decision boundary of a linear classifier
Nğ‘“:Rğ‘›â†’ğ¶2is a hyperplane; it is, therefore, a straight line in
Fig. 1a. Now, assume that x0is correctly classified and that the dash-
dotted square represents B(x0,ğ‘‘). Then, xâ€²
0is adversarial because
Nğ‘“(x0)â‰ Nğ‘“(xâ€²
0), which is equivalent to ğ‘“(x0)ğ‘“(xâ€²
0)<0.
Example. We give an intuition on how DeepSearch handles
linear binary classifiers using the example of Fig. 1a. Recall that
x0is a correctly classified input for which ğ‘“(x0)>0. To find
an adversarial example, DeepSearch fuzzes x0with the goal of
generating a new input xâ€²
0such thatğ‘“(xâ€²
0)<0.
Fuzzing is performed as follows. Input x0has two coordinates
ğ‘¥â„andğ‘¥ğ‘£, for the horizontal and vertical dimensions. DeepSearch
independently mutates each of these coordinates to the minimum
and maximum values that are possible within B(x0,ğ‘‘), with the in-
tention of finding the minimum value of ğ‘“inB(x0,ğ‘‘). For instance,
when mutating ğ‘¥â„, we obtain inputs x0[ğ‘™â„/ğ‘¥â„]andx0[ğ‘¢â„/ğ‘¥â„]in
the figure. Values ğ‘™â„andğ‘¢â„are, respectively, the minimum and
maximum that ğ‘¥â„may take, and x0[ğ‘™â„/ğ‘¥â„]denotes substituting ğ‘¥â„
withğ‘™â„(similarly for x0[ğ‘¢â„/ğ‘¥â„]). We then evaluate ğ‘“(x0[ğ‘™â„/ğ‘¥â„])
andğ‘“(x0[ğ‘¢â„/ğ‘¥â„]), and forğ‘¥â„, we select the value ( ğ‘™â„orğ‘¢â„) that
causes function ğ‘“todecrease . This is because, in our example, an
adversarial input xâ€²
0must make the value of ğ‘“negative. Let us
assume that ğ‘“(x0[ğ‘¢â„/ğ‘¥â„])<ğ‘“(x0[ğ‘™â„/ğ‘¥â„); we, thus, select ğ‘¢â„for
coordinateğ‘¥â„.
DeepSearch mutates coordinate ğ‘¥ğ‘£in a similar way. It evaluates
ğ‘“(x0[ğ‘™ğ‘£/ğ‘¥ğ‘£])andğ‘“(x0[ğ‘¢ğ‘£/ğ‘¥ğ‘£]), and selects the value that causes ğ‘“
to decrease. Let us assume that ğ‘“(x0[ğ‘¢ğ‘£/ğ‘¥ğ‘£])<ğ‘“(x0[ğ‘™ğ‘£/ğ‘¥ğ‘£]); we,
thus, selectğ‘¢ğ‘£forğ‘¥ğ‘£.
Next, we generate input xâ€²
0by substituting each coordinate in
x0with the boundary value that was previously selected. In other
words, xâ€²
0=x0[ğ‘¢â„/ğ‘¥â„,ğ‘¢ğ‘£/ğ‘¥ğ‘£], and since ğ‘“(xâ€²
0)<0,DeepSearch
has generated an adversarial example. Note that ğ‘“(xâ€²
0)is actually
the minimum value of ğ‘“inB(x0,ğ‘‘).
DeepSearch for linear binary classifiers. We now formalize
how DeepSearch treats linear binary classifiers. Consider a linear
classification function ğ‘“(x)=wğ‘‡x+ğ‘=Ãğ‘›
ğ‘–=1ğ‘¤ğ‘–ğ‘¥ğ‘–+ğ‘, where
wğ‘‡=(ğ‘¤1,...,ğ‘¤ğ‘›)andğ‘âˆˆR. Note thatğ‘“is monotonic with respect
to all of its variables ğ‘¥1,...,ğ‘¥ğ‘›. For instance, if ğ‘¤ğ‘–>0, thenğ‘“is
monotonically increasing in ğ‘¥ğ‘–.
Recall thatB(x,ğ‘‘)denotes the set of inputs within distance ğ‘‘âˆˆ
Rof an input x.B(x,ğ‘‘)may be represented by an ğ‘›-dimensional
cubeI=ğ¼1Ã—...Ã—ğ¼ğ‘›, whereğ¼ğ‘–=[ğ‘™ğ‘–,ğ‘¢ğ‘–]is a closed interval bounded
byğ‘™ğ‘–,ğ‘¢ğ‘–âˆˆRwithğ‘™ğ‘–â‰¤ğ‘¢ğ‘–for1â‰¤ğ‘–â‰¤ğ‘›. Intuitively, value ğ‘™ğ‘–(resp.ğ‘¢ğ‘–)
is the lower (resp. upper) bound on the ğ‘–th dimension of x. An input
xâ€²is avertex ofIif each of its coordinates xâ€²(ğ‘–)is an endpoint of
ğ¼ğ‘–for1â‰¤ğ‘–â‰¤ğ‘›, i.e., xâ€²(ğ‘–)=ğ‘¢ğ‘–orğ‘™ğ‘–(1â‰¤ğ‘–â‰¤ğ‘›).
Due to the monotonicity of ğ‘“, the maximum and minimum values
ofğ‘“onImay be easily calculated by applying ğ‘“to vertices ofI. For
x0x0'
Dff(x)>0
f(x)<0
B(x0,d)(lh,xv)
(xh,lv)(uh,xv)(xh,uv)(a) Linear classifier
x0x2
Dff (x) > 0
f (x) < 0
B(x0,d)x1 (b) Non-linear classifier
Figure 1: DeepSearch for binary classifiers.
example, consider a one-dimensional linear function ğ‘“(ğ‘¥)=âˆ’2ğ‘¥,
whereğ‘¥âˆˆ[âˆ’ 1,1], that is,âˆ’1and1are the lower and upper bounds
forğ‘¥. Sinceğ‘“(1)<ğ‘“(âˆ’1), we get a maximum value of ğ‘“atğ‘¥=âˆ’1
and a minimum value of ğ‘“atğ‘¥=1.ğ‘-dimensional linear functions
can be treated similarly. We write ğ‘“(I)for the values of ğ‘“onI, i.e.,
ğ‘“(I)={ğ‘“(x)|xâˆˆI} , and have the following theorem (whose
proof can be found in [84]).
Theorem 1.Given a linear classification function ğ‘“(x)=wğ‘‡x+ğ‘,
where wğ‘‡=(ğ‘¤1,...,ğ‘¤ğ‘›)andğ‘âˆˆR, anğ‘›-dimensional cube I=
ğ¼1Ã—...Ã—ğ¼ğ‘›, whereğ¼ğ‘–=[ğ‘™ğ‘–,ğ‘¢ğ‘–]for1â‰¤ğ‘–â‰¤ğ‘›, and an input xâˆˆI,
we have:
(1)minğ‘“(I)=ğ‘“(xâ€²), where xâ€²(ğ‘–)=ğ‘™ğ‘–(resp. xâ€²(ğ‘–)=
ğ‘¢ğ‘–) ifğ‘“(x[ğ‘¢ğ‘–/ğ‘¥ğ‘–])>ğ‘“(x[ğ‘™ğ‘–/ğ‘¥ğ‘–])(resp.ğ‘“(x[ğ‘¢ğ‘–/ğ‘¥ğ‘–]) â‰¤
ğ‘“(x[ğ‘™ğ‘–/ğ‘¥ğ‘–])) for 1â‰¤ğ‘–â‰¤ğ‘›
(2)maxğ‘“(I)=ğ‘“(xâ€²), where xâ€²(ğ‘–)=ğ‘¢ğ‘–(resp. xâ€²(ğ‘–)=
ğ‘™ğ‘–) ifğ‘“(x[ğ‘¢ğ‘–/ğ‘¥ğ‘–])>ğ‘“(x[ğ‘™ğ‘–/ğ‘¥ğ‘–])(resp.ğ‘“(x[ğ‘¢ğ‘–/ğ‘¥ğ‘–]) â‰¤
ğ‘“(x[ğ‘™ğ‘–/ğ‘¥ğ‘–])) for 1â‰¤ğ‘–â‰¤ğ‘›
According to the above theorem, we can precisely calculate the
minimum and maximum values of ğ‘“in anyğ‘›-dimensional cube. In
particular, assume a correctly classified input xfor whichğ‘“(x)>0.
For each dimension ğ‘–(1â‰¤ğ‘–â‰¤ğ‘›) ofx, we first construct inputs
x[ğ‘™ğ‘–/ğ‘¥ğ‘–]andx[ğ‘¢ğ‘–/ğ‘¥ğ‘–]. We then compare the values of ğ‘“(x[ğ‘™ğ‘–/ğ‘¥ğ‘–])
andğ‘“(x[ğ‘¢ğ‘–/ğ‘¥ğ‘–]). To generate a new input xâ€², we select the value
of itsğ‘–th coordinate as follows:
xâ€²(ğ‘–)=ğ‘™ğ‘–, ifğ‘“(x[ğ‘¢ğ‘–/ğ‘¥ğ‘–])>ğ‘“(x[ğ‘™ğ‘–/ğ‘¥ğ‘–])
ğ‘¢ğ‘–, ifğ‘“(x[ğ‘¢ğ‘–/ğ‘¥ğ‘–])â‰¤ğ‘“(x[ğ‘™ğ‘–/ğ‘¥ğ‘–])
As shown here, selecting a value for xâ€²(ğ‘–)requires evaluating
functionğ‘“twice, i.e.,ğ‘“(x[ğ‘™ğ‘–/ğ‘¥ğ‘–])andğ‘“(x[ğ‘¢ğ‘–/ğ‘¥ğ‘–]). Therefore, for ğ‘›
dimensions, ğ‘“must be evaluated 2ğ‘›times. In practice however, due
to the monotonicity of ğ‘“, evaluating it only once per dimension
is sufficient. For instance, if ğ‘“(x[ğ‘¢ğ‘–/ğ‘¥ğ‘–])already decreases (resp.
increases) the value of ğ‘“in comparison to ğ‘“(x), there is no need to
evaluateğ‘“(x[ğ‘™ğ‘–/ğ‘¥ğ‘–]). Valueğ‘¢ğ‘–(resp.ğ‘™ğ‘–) should be selected for the
ğ‘–th coordinate. Hence, the minimum value of ğ‘“can be computed
by evaluating the function exactly ğ‘›times. If, for the newly gen-
erated input xâ€², the sign of ğ‘“becomes negative, xâ€²constitutes an
adversarial example.
We treat the case where ğ‘“(x)<0for a correctly classified input
xanalogously. DeepSearch aims to generate a new input xâ€²such
that the sign of ğ‘“becomes positive. We are, therefore, selecting
coordinate values that cause ğ‘“toincrease .
802ESEC/FSE â€™20, November 8â€“13, 2020, Virtual Event, USA Fuyuan Zhang, Sankalan Pal Chowdhury, and Maria Christakis
Algorithm 1: DeepSearch for binary classifiers.
Input: input xâˆˆRğ‘›, initial input xinitâˆˆB( x,ğ‘‘),
functionğ‘“:Rğ‘›â†’R, distanceğ‘‘âˆˆR
Output: xâ€²âˆˆB( x,ğ‘‘)
1Function ApproxMax( x,ğ‘“,(ğ¼1,...,ğ¼ ğ‘›))is
2 xâ€²:=(0,...,0)
3 foreach 1â‰¤ğ‘–â‰¤ğ‘›do
4 ifğ‘“(x[ğ‘¢ğ‘–/ğ‘¥ğ‘–])>ğ‘“(x[ğ‘™ğ‘–/ğ‘¥ğ‘–])then
5 xâ€²:=xâ€²[ğ‘¢ğ‘–/ğ‘¥â€²
ğ‘–]
6 else
7 xâ€²:=xâ€²[ğ‘™ğ‘–/ğ‘¥â€²
ğ‘–]
8 return xâ€²
9
10Function ApproxMin( x,ğ‘“,(ğ¼1,...,ğ¼ ğ‘›))is
11 xâ€²:=(0,...,0)
12 foreach 1â‰¤ğ‘–â‰¤ğ‘›do
13 ifğ‘“(x[ğ‘¢ğ‘–/ğ‘¥ğ‘–])>ğ‘“(x[ğ‘™ğ‘–/ğ‘¥ğ‘–])then
14 xâ€²:=xâ€²[ğ‘™ğ‘–/ğ‘¥â€²
ğ‘–]
15 else
16 xâ€²:=xâ€²[ğ‘¢ğ‘–/ğ‘¥â€²
ğ‘–]
17 return xâ€²
18
19Function DS-Binary( x,xinit,ğ‘“,ğ‘‘ )is
20 construct intervals (ğ¼1,...,ğ¼ ğ‘›)such thatB(x,ğ‘‘)=ğ¼1Ã—...Ã—ğ¼ğ‘›
21 initialize x0:=xinitandğ‘˜:=0
22 ifğ‘“(x0)>0then
23 repeat
24 xğ‘˜+1:=ApproxMin(xğ‘˜,ğ‘“,(ğ¼1,...,ğ¼ ğ‘›))
25 ğ‘˜:=ğ‘˜+1
26 untilNğ‘“(x)â‰ Nğ‘“(xğ‘˜), orğ‘˜=MaxNum
27 else
28 repeat
29 xğ‘˜+1:=ApproxMax(xğ‘˜,ğ‘“,(ğ¼1,...,ğ¼ ğ‘›))
30 ğ‘˜:=ğ‘˜+1
31 untilNğ‘“(x)â‰ Nğ‘“(xğ‘˜), orğ‘˜=MaxNum
32 return x ğ‘˜
3.2 Non-Linear Binary Classifiers
We generalize our technique to non-linear binary classifiers. In
this setting, DeepSearch iteratively approximates the minimum and
maximum values of ğ‘“inB(x,ğ‘‘).
Example. As an example, consider the non-linear classification
functionğ‘“shown in Fig. 1b. Since ğ‘“is non-linear, the decision
boundaryDğ‘“of the binary classifier is a curve.
Starting from correctly classified input x0,DeepSearch treatsğ‘“
as linear withinB(x0,ğ‘‘)and generates x1, exactly as it would for a
linear binary classifier. To explain how x1is derived, we refer to the
points in Fig. 1a. Suppose we first mutate the horizontal dimension
ofx0(using the lower and upper bounds) and find that ğ‘“(ğ‘™â„,ğ‘¥ğ‘£)>
ğ‘“(ğ‘¢â„,ğ‘¥ğ‘£). To increase the chances of crossing the decision boundary,
we choose the bound for the horizontal dimension of x0that gives
us the lower value of ğ‘“, i.e., we select ğ‘¢â„for horizontal coordinate
ğ‘¥â„. Then, we mutate the vertical dimension of x0and find that
ğ‘“(ğ‘¥â„,ğ‘¢ğ‘£)>ğ‘“(ğ‘¥â„,ğ‘™ğ‘£). This means that we select ğ‘™ğ‘£for verticalcoordinateğ‘¥ğ‘£. Hence, we derive x1=(ğ‘¢â„,ğ‘™ğ‘£). Observe that input
x1is not adversarial. Unlike for a linear binary classifier however,
where the minimum value of ğ‘“inB(x0,ğ‘‘)is precisely computed,
the non-linear case is handled by iteratively approximating the
minimum. In particular, after generating x1,DeepSearch iterates
starting from x1, while again treating ğ‘“as linear inB(x0,ğ‘‘). As a
result, our technique generates input x2, which is adversarial.
The reason we can treat non-linear binary classifiers as linear
ones is that perturbations of pixels are only allowed in a very small
ğ‘›-dimensional cube, constrained by the ğ¿âˆdistance. Within such a
small space, we can effectively approximate non-linear functions
using iterative linear approximations.
DeepSearch for non-linear binary classifiers. Alg. 1 shows
DeepSearch for binary classifiers. It uses iterative approximations to
search for adversarial examples. Note that our technique is blackbox,
and consequently, it cannot differentiate between linear and non-
linear classifiers. Alg. 1 is, therefore, the general algorithm that
DeepSearch applies to fuzz any binary classifier.
The main function in Alg. 1 is DS-Binary . Input xinitis the input
from which we start the first iteration, e.g., it corresponds to x0
in Fig. 1. Input xis used to compute B(x,ğ‘‘), and for now, assume
thatxis equal to xinit. (We will discuss why xis needed in Sect. 5.)
In addition to these inputs, the algorithm also takes a classification
functionğ‘“and the distance ğ‘‘.
Function DS-Binary assigns xinittox0and constructs ğ‘›intervals
ğ¼1,...,ğ¼ğ‘›to representB(x0,ğ‘‘)(lines 20â€“21). Then, based on the sign
ofğ‘“(x0), our algorithm iteratively approximates the minimum
(lines 23â€“26) or the maximum (lines 28â€“31) value of ğ‘“inB(x0,ğ‘‘).
DS-Binary terminates when either an adversarial example is found
or it has reached MaxNum iterations. To find adversarial examples
inğ‘˜iterations, we evaluate ğ‘“at most 2ğ‘›+ğ‘›(ğ‘˜âˆ’1)times.
ApproxMin andApproxMax implement Thm. 1 to calculate the
minimum and maximum values of function ğ‘“in theğ‘›-dimensional
cubeğ¼1Ã—...Ã—ğ¼ğ‘›. Whenğ‘“is linear, calling these functions on any
input xâˆˆğ¼1Ã—...Ã—ğ¼ğ‘›does not affect the computation. In other words,
the minimum and maximum values are precisely computed for any
x. Whenğ‘“is non-linear, it is still assumed to be linear within the
ğ‘›-dimensional cube. Given that the size of the cube is designed to
be small, this assumption does not introduce too much imprecision.
As a consequence of this assumption however, different inputs in
theğ‘›-dimensional cube lead to computing different minimum and
maximum values of ğ‘“. For instance, in Fig. 1b, calling ApproxMin
onx0returns x1, while calling it on x1returns x2.
4 FUZZING MULTICLASS CLASSIFIERS
In this section, we extend our technique for blackbox fuzzing of
binary classifiers to multiclass classifiers.
4.1 Linear Multiclass Classifiers
Amulticlass classifier classifies inputs in ğ‘šclasses according to the
following definition.
Definition 2(Multiclass Classifier) .For classification function
ğ‘“:Rğ‘›â†’Rğ‘š, which returns ğ‘švalues each corresponding to one
803DeepSearch: A Simple and Effective Blackbox Attack for Deep Neural Networks ESEC/FSE â€™20, November 8â€“13, 2020, Virtual Event, USA
class inğ¶ğ‘š={ğ‘™1,...,ğ‘™ğ‘š}, amulticlass classifier Nğ‘“:Rğ‘›â†’ğ¶ğ‘šis
defined as
Nğ‘“(x)=ğ‘™ğ‘—, iffğ‘—=arg maxğ‘–ğ‘“ğ‘–(x),
whereğ‘“ğ‘–:Rğ‘›â†’Rdenotes the function derived by evaluating ğ‘“
for theğ‘–th class, i.e., ğ‘“(x)=(ğ‘“1(x),...,ğ‘“ğ‘š(x))ğ‘‡.
In other words, a multiclass classifier Nğ‘“classifies an input xin
ğ‘™ğ‘—ifğ‘“ğ‘—(x)evaluates to the largest value in comparison to all other
functionsğ‘“ğ‘–.
Functionğ‘“of a multiclass classifier Nğ‘“may be decomposed into
multiple binary classifiers such that the original classifier can be
reconstructed from the binary ones. First, to decompose a multiclass
classifier into binary classifiers, for any pair of classes ğ‘™ğ‘–andğ‘™ğ‘—
(1â‰¤ğ‘–,ğ‘—â‰¤ğ‘š), we define a classification function ğ‘”ğ‘–ğ‘—:Rğ‘›â†’R
asğ‘”ğ‘–ğ‘—(x)=ğ‘“ğ‘–(x)âˆ’ğ‘“ğ‘—(x). We then construct a binary classifier
Nğ‘”ğ‘–ğ‘—:Rğ‘›â†’{ğ‘™ğ‘–,ğ‘™ğ‘—}as follows:
Nğ‘”ğ‘–ğ‘—(x)=ğ‘™ğ‘–, ifğ‘”ğ‘–ğ‘—(x)>0
ğ‘™ğ‘—, ifğ‘”ğ‘–ğ‘—(x)<0
As usual, the set of values Dğ‘”ğ‘–ğ‘—={x|ğ‘“ğ‘–(x)âˆ’ğ‘“ğ‘—(x)=0}constitutes
the pairwise decision boundary of binary classifier Nğ‘”ğ‘–ğ‘—, which
classifies the domain Rğ‘›into the two classes {ğ‘™ğ‘–,ğ‘™ğ‘—}. As an example,
consider Fig. 2a depicting a multiclass classifier Nğ‘“:R2â†’ğ¶3,
whereğ‘“is linear and ğ¶3={ğ‘™1,ğ‘™2,ğ‘™3}. Assume thatNğ‘“correctly
classifies input xinğ‘™2. Based on the above, linear binary classifiers
Nğ‘”21andNğ‘”23also classify xinğ‘™2, i.e.,ğ‘”21(x)>0andğ‘”23(x)>0.
Second, a multiclass classifier may be composed from multiple
binary classifiers as follows. An input xis classified in class ğ‘™ğ‘–by
multiclass classifier Nğ‘“if and only if it is classified in ğ‘™ğ‘–by allğ‘šâˆ’1
binary classifiersNğ‘”ğ‘–ğ‘—for1â‰¤ğ‘—â‰¤ğ‘š,ğ‘–â‰ ğ‘—, whereğ‘™ğ‘–âˆˆğ¶ğ‘šand
ğ‘”ğ‘–ğ‘—(x)=ğ‘“ğ‘–(x)âˆ’ğ‘“ğ‘—(x):
Nğ‘“(x)=ğ‘™ğ‘–, iffâˆ€1â‰¤ğ‘—â‰¤ğ‘š,ğ‘–â‰ ğ‘—:Nğ‘”ğ‘–ğ‘—(x)=ğ‘™ğ‘–
For instance, in Fig. 2a, if both Nğ‘”21andNğ‘”23classify input xin
classğ‘™2, then the multiclass classifier also classifies it in ğ‘™2.
Based on the above, a multiclass classifier has an adversarial
input if and only if this input is also adversarial for a constituent
binary classifier.
Corollary 1.LetNğ‘“be a multiclass classifier and xâˆˆRğ‘›a
correctly classified input, where Nğ‘“(x)=ğ‘™ğ‘–andğ‘™ğ‘–âˆˆğ¶ğ‘š. There
exists an adversarial example xâ€²âˆˆB(x,ğ‘‘)forNğ‘“, whereğ‘‘âˆˆR, if
and only if xâ€²is an adversarial example for a binary classifier Nğ‘”ğ‘–ğ‘—
(1â‰¤ğ‘—â‰¤ğ‘š,ğ‘–â‰ ğ‘—), whereğ‘”ğ‘–ğ‘—(x)=ğ‘“ğ‘–(x)âˆ’ğ‘“ğ‘—(x):
Nğ‘“(xâ€²)â‰ ğ‘™ğ‘–, iffâˆƒ1â‰¤ğ‘—â‰¤ğ‘š,ğ‘–â‰ ğ‘—:Nğ‘”ğ‘–ğ‘—(xâ€²)â‰ ğ‘™ğ‘–
Example. This corollary is crucial in generalizing our technique
to multiclass classifiers. Assume a correctly classified input x, for
whichNğ‘“(x)=ğ‘™ğ‘–. According to the above corollary, the robustness
ofNğ‘“inB(x,ğ‘‘)reduces to the robustness of all ğ‘šâˆ’1binary
classifiers{Nğ‘”ğ‘–ğ‘—|1â‰¤ğ‘—â‰¤ğ‘š,ğ‘–â‰ ğ‘—}inB(x,ğ‘‘). We, therefore, use
DeepSearch for binary classifiers to test each binary classifier in this
set. If there exists an adversarial input xâ€²for one of these classifiers,
i.e.,Nğ‘”ğ‘–ğ‘—(xâ€²)â‰ ğ‘™ğ‘–for someğ‘—, then xâ€²is also an adversarial input
forNğ‘“, i.e.,Nğ‘“(xâ€²)â‰ ğ‘™ğ‘–.
Let us consider again the example of Fig. 2a. Recall that multiclass
classifierNğ‘“correctly classifies input xin classğ‘™2, and so do binary
xx3'
Dg21
B(x,d)x1'
Dg23(a) Linear classifier
x0x2
Dg21
B(x0,d)x1
Dg23 (b) Non-linear classifier
Figure 2: DeepSearch for multiclass classifiers.
classifiersNğ‘”21andNğ‘”23, i.e.,ğ‘”21(x)>0andğ‘”23(x)>0. As a result,
DeepSearch tries to generate inputs that decrease the value of each
of these functions in B(x,ğ‘‘)in order to find adversarial examples.
Functionğ‘”21evaluates to its minimum value in B(x,ğ‘‘)for input
xâ€²
1, and function ğ‘”23for input xâ€²
3. Observe that xâ€²
3is an adversarial
example forNğ‘”23, and thus also forNğ‘“, whereas xâ€²
1is not.
DeepSearch for linear multiclass classifiers. Let us assume
a linear classification function ğ‘“(x)=Wğ‘‡x+b, where Wğ‘‡=
(wğ‘‡
1,...,wğ‘‡ğ‘š)ğ‘‡,wğ‘–âˆˆRğ‘›(1â‰¤ğ‘–â‰¤ğ‘š), and b=(ğ‘1,...,ğ‘ğ‘š)ğ‘‡âˆˆRğ‘š.
Then,ğ‘“ğ‘–, which denotes the function derived by evaluating ğ‘“for
theğ‘–th class, is of the form ğ‘“ğ‘–(x)=wğ‘‡
ğ‘–x+ğ‘ğ‘–for1â‰¤ğ‘–â‰¤ğ‘š.
For any pair of class labels ğ‘™ğ‘–andğ‘™ğ‘—, functionğ‘”ğ‘–ğ‘—is defined as
ğ‘”ğ‘–ğ‘—(x)=ğ‘“ğ‘–(x)âˆ’ğ‘“ğ‘—(x)=(wğ‘‡
ğ‘–âˆ’wğ‘‡
ğ‘—)x+(ğ‘ğ‘–âˆ’ğ‘ğ‘—). Hence,ğ‘”ğ‘–ğ‘—is also
linear, andNğ‘”ğ‘–ğ‘—is a linear binary classifier.
Assume that classifier Nğ‘“correctly classifies input xâˆˆRğ‘›inğ‘™ğ‘–,
Nğ‘“(x)=ğ‘™ğ‘–(ğ‘™ğ‘–âˆˆğ¶ğ‘š). According to Cor. 1, the robustness of Nğ‘“in
B(x,ğ‘‘)(ğ‘‘âˆˆR)reduces to the robustness of each binary classifier
Nğ‘”ğ‘–ğ‘—(1â‰¤ğ‘—â‰¤ğ‘š,ğ‘–â‰ ğ‘—)inB(x,ğ‘‘). To find an adversarial example
for a binary classifier Nğ‘”ğ‘–ğ‘—inB(x,ğ‘‘),DeepSearch must generate an
input xâ€²âˆˆB(x,ğ‘‘)such thatğ‘”ğ‘–ğ‘—(xâ€²)<0. (Recall that by definition
ğ‘”ğ‘–ğ‘—(x)>0.) Since all functions ğ‘”ğ‘–ğ‘—(1â‰¤ğ‘—â‰¤ğ‘š,ğ‘–â‰ ğ‘—)are linear, we
easily find their minimum values in B(x,ğ‘‘)as follows.
Letğ¼1,...,ğ¼ğ‘›be intervals such that B(x,ğ‘‘)=ğ¼1Ã—...Ã—ğ¼ğ‘›, where
ğ¼ğ‘˜=[ğ‘™ğ‘˜,ğ‘¢ğ‘˜]for1â‰¤ğ‘˜â‰¤ğ‘›. As in Sect. 3.1, for each dimension
ğ‘˜,DeepSearch evaluates function ğ‘“twice to compare the values
ofğ‘“(x[ğ‘¢ğ‘˜/ğ‘¥ğ‘˜])andğ‘“(x[ğ‘™ğ‘˜/ğ‘¥ğ‘˜]). To generate a new input xâ€²
ğ‘—for
which function ğ‘”ğ‘–ğ‘—evaluates to its minimum value, we select its
ğ‘˜th coordinate as follows:
xâ€²
ğ‘—(ğ‘˜)=ğ‘™ğ‘˜, ifğ‘”ğ‘–ğ‘—(x[ğ‘¢ğ‘˜/ğ‘¥ğ‘˜])>ğ‘”ğ‘–ğ‘—(x[ğ‘™ğ‘˜/ğ‘¥ğ‘˜])
ğ‘¢ğ‘˜, ifğ‘”ğ‘–ğ‘—(x[ğ‘¢ğ‘˜/ğ‘¥ğ‘˜])â‰¤ğ‘”ğ‘–ğ‘—(x[ğ‘™ğ‘˜/ğ‘¥ğ‘˜]).
Note that, although we calculate the minimum value of ğ‘šâˆ’1
linear functions, we still evaluate ğ‘“2ğ‘›times. This is because a
functionğ‘”ğ‘–ğ‘—is defined as ğ‘”ğ‘–ğ‘—(x)=ğ‘“ğ‘–(x)âˆ’ğ‘“ğ‘—(x), whereğ‘“ğ‘–(x)and
ğ‘“ğ‘—(x)are the values of ğ‘“(x)for theğ‘–th andğ‘—th classes, respectively.
If the sign of ğ‘”ğ‘–ğ‘—(xâ€²
ğ‘—)becomes negative for some ğ‘—, then DeepSearch
has found an adversarial example for Nğ‘“inB(x,ğ‘‘).
4.2 Non-Linear Multiclass Classifiers
We now extend our technique to non-linear multiclass classifiers.
Analogously to Sect. 3.2, DeepSearch iteratively approximates the
minimum values of functions ğ‘”ğ‘–ğ‘—inB(x,ğ‘‘).
804ESEC/FSE â€™20, November 8â€“13, 2020, Virtual Event, USA Fuyuan Zhang, Sankalan Pal Chowdhury, and Maria Christakis
Algorithm 2: DeepSearch for multiclass classifiers.
Input: input xâˆˆRğ‘›, initial input xinitâˆˆB( x,ğ‘‘),
functionğ‘“:Rğ‘›â†’Rğ‘š, distanceğ‘‘âˆˆR
Output: xâ€²âˆˆB( x,ğ‘‘)
1Function DS-Multiclass( x,xinit,ğ‘“,ğ‘‘ )is
2 construct intervals (ğ¼1,...,ğ¼ ğ‘›)such thatB(x,ğ‘‘)=ğ¼1Ã—...Ã—ğ¼ğ‘›
3 computeNğ‘“(x)and assume thatNğ‘“(x)=ğ‘™ğ‘–
4 initialize x0:=xinitandğ‘˜:=0
5 define{ğ‘”ğ‘–ğ‘—|ğ‘”ğ‘–ğ‘—(x)=ğ‘“ğ‘–(x)âˆ’ğ‘“ğ‘—(x),1â‰¤ğ‘—â‰¤ğ‘š,ğ‘–â‰ ğ‘—}
6 repeat
7ğ‘Ÿ:=arg min ğ‘—ğ‘”ğ‘–ğ‘—(xğ‘˜)
8 xğ‘˜+1:=ApproxMin(xğ‘˜,ğ‘”ğ‘–ğ‘Ÿ,(ğ¼1,...,ğ¼ ğ‘›))
9ğ‘˜:=ğ‘˜+1
10 untilNğ‘“(x)â‰ Nğ‘“(xğ‘˜), orğ‘˜=MaxNum
11 return x ğ‘˜
Example. As an example, consider Fig. 2b depicting a multiclass
classifierNğ‘“:R2â†’ğ¶3, whereğ‘“is non-linear and ğ¶3={ğ‘™1,ğ‘™2,ğ‘™3}.
Assume thatNğ‘“classifies input x0in classğ‘™2, and thus, so do non-
linear binary classifiers Nğ‘”21andNğ‘”23.
Let us also assume that ğ‘”21(x0)<ğ‘”23(x0). Sinceğ‘”21evaluates to
a smaller value than ğ‘”23for input x0, we consider it more likely to
have an adversarial example. In other words, we first approximate
the minimum value of ğ‘”21because it is closer to becoming negative
for the initial input. DeepSearch treatsğ‘”21as linear withinB(x0,ğ‘‘)
and generates x1. Observe that input x1is not adversarial. Now,
assume that ğ‘”21(x1)>ğ‘”23(x1). As a result, DeepSearch tries to
find the minimum of function ğ‘”23inB(x0,ğ‘‘), also by treating it as
linear. It generates input x2, which is an adversarial example for
classifiersNğ‘”23andNğ‘“.
DeepSearch for non-linear multiclass classifiers. Alg. 2 is
the general DeepSearch algorithm for multiclass classifiers. The
inputs are the same as for Alg. 1. For now, assume that xis equal
toxinit. Again, the algorithm executes at most MaxNum iterations,
and it terminates as soon as an adversarial example is found.
Function DS-Multiclass assigns xinittox0and constructs ğ‘›
intervalsğ¼1,...,ğ¼ğ‘›to representB(x0,ğ‘‘). It also computes the class
labelğ‘™ğ‘–ofx, and defines functions ğ‘”ğ‘–ğ‘—(1â‰¤ğ‘—â‰¤ğ‘š,ğ‘–â‰ ğ‘—)(lines 2â€“5).
The rest of the algorithm uses ApproxMin from Alg. 1 to iteratively
approximate the minimum of one function ğ‘”ğ‘–ğ‘—per iteration, which
is selected on line 7 such that its value for input xğ‘˜is smaller in
comparison to all other constituent binary classification functions.
Intuitively, ğ‘”ğ‘–ğ‘—corresponds to the binary classifier that is most
likely to have an adversarial example near xğ‘˜. This heuristic allows
our algorithm to find an adversarial example faster than having to
generate an input xğ‘˜+1for allğ‘šâˆ’1functionsğ‘”ğ‘–ğ‘—per iteration.
To find an adversarial example in ğ‘˜iterations, we need at most
2ğ‘›+ğ‘›(ğ‘˜âˆ’1)queries for the value of ğ‘“.
An alternative objective function. In each iteration of Alg. 2,
we construct a different objective function ğ‘”ğ‘–ğ‘—and approximate its
minimum value. An alternative choice of an objective function is ğ‘“ğ‘–
itself. In multiclass classification, decreasing the value of ğ‘“ğ‘–amounts
to decreasing the score value of the ğ‘–th class, which implicitly
increases the score values of other classes.We refer to the algorithm derived by substituting lines 7â€“8 of
Alg. 2 with the following assignment as Alg. 2â€™ :
xğ‘˜+1:=ApproxMin(xğ‘˜,ğ‘“ğ‘–,(ğ¼1,...,ğ¼ğ‘›))
It uses ApproxMin to iteratively approximate the minimum value
ofğ‘“ğ‘–. We find it very effective in our experiments.
5 ITERATIVE REFINEMENT
The closer the adversarial examples are to a correctly classified
input, the more subtle they are. Such adversarial examples are said
to have a low distortion rate. In this section, we extend DeepSearch
with an iterative-refinement approach for finding subtle adversarial
examples. On a high level, given an input xand a distance ğ‘‘, for
which we have already found an adversarial example xâ€²in region
B(x,ğ‘‘),DeepSearch iteratively reduces distance ğ‘‘as long as the
smaller region still contains an adversarial example. If none is found,
the distance is not reduced further.
LetI=ğ¼1Ã—...Ã—ğ¼ğ‘›be anğ‘›-dimensional cube, where ğ¼ğ‘–=[ğ‘™ğ‘–,ğ‘¢ğ‘–]
is a closed interval bounded by ğ‘™ğ‘–,ğ‘¢ğ‘–âˆˆRwithğ‘™ğ‘–â‰¤ğ‘¢ğ‘–for1â‰¤ğ‘–â‰¤ğ‘›.
For an input xwith anğ‘–th coordinate x(ğ‘–)âˆˆ(âˆ’âˆ,ğ‘™ğ‘–]âˆª[ğ‘¢ğ‘–,+âˆ)
for1â‰¤ğ‘–â‰¤ğ‘›, we define a projection operator Proj that maps xto
a vertex ofIas follows
Proj(I,x)(ğ‘–)=ğ‘¢ğ‘–, ifx(ğ‘–)â‰¥ğ‘¢ğ‘–
ğ‘™ğ‘–, ifx(ğ‘–)â‰¤ğ‘™ğ‘–,
where Proj(I,x)(ğ‘–)denotes the ğ‘–th coordinate of Proj(I,x). As
an example, consider Fig. 3a showing a linear multiclass classifier.
Input x2is a projection of x1.
Using this operator, the minimum and maximum values of a
linear classification function ğ‘“may also be projected on I, and we
have the following theorem (whose proof is available in [84]).
Theorem 2.Letğ‘“(x)=wğ‘‡x+ğ‘be a linear classification function,
andI1,I2twoğ‘›-dimensional cubes such that I1âŠ†I2. Assuming
thatxis a vertex ofI2, we have:
(1) if minğ‘“(I2)=ğ‘“(x), then minğ‘“(I1)=ğ‘“(Proj(I1,x))
(2) if maxğ‘“(I2)=ğ‘“(x), then maxğ‘“(I1)=ğ‘“(Proj(I1,x))
In Fig. 3a, assume that input x0is correctly classified in class
ğ‘™2. Then, in regionB(x0,ğ‘‘1), functionğ‘”23obtains its minimum
value for input x1. When projecting x1to vertex x2ofB(x0,ğ‘‘2),
notice thatğ‘”23evaluates to its minimum for input x2in this smaller
region.
Example. Fig. 3 shows two multiclass classifiers Nğ‘“:R2â†’ğ¶3,
whereğ¶3={ğ‘™1,ğ‘™2,ğ‘™3}. In Fig. 3a, function ğ‘“is linear, whereas in
Fig. 3b, it is non-linear. For correctly classified input x0, we assume
thatNğ‘“(x0)=ğ‘™2, and thus,Nğ‘”21(x0)=ğ‘™2andNğ‘”23(x0)=ğ‘™2.
In both subfigures, assume that x1is an adversarial example
found by DS-Multiclass (see Alg. 2) inB(x0,ğ‘‘1). Once such an
example is found, our technique with refinement uses bisect search
to find the smallest distance ğ‘‘â€²such that the projection of x1on
B(x0,ğ‘‘â€²)is still adversarial. In Fig. 3, this distance is ğ‘‘2, and input
x2constitutes the projection of x1onB(x0,ğ‘‘2). So,x2is closer to
x0, which means that it has a lower distortion rate than x1. In fact,
since we are using bisect search to determine distance ğ‘‘2,x2is the
closest adversarial input to x0that may be generated by projecting
x1on smaller regions.
805DeepSearch: A Simple and Effective Blackbox Attack for Deep Neural Networks ESEC/FSE â€™20, November 8â€“13, 2020, Virtual Event, USA
Dg21x1
B(x0,d1)x0x3
B(x0,d3)x2
B(x0,d2)x4
Dg23
(a) Linear classifier
x2
B(x0,d1)x1
x0
x3 B(x0,d2)B(x0,d3)x4
Dg23Dg21 (b) Non-linear classifier
Figure 3: DeepSearch with iterative refinement.
However, in region B(x0,ğ‘‘2), there may be other vertices that
are adversarial and get us even closer to x0with projection. To find
such examples, we apply DS-Multiclass again, this time starting
from input x2and searching in region B(x0,ğ‘‘2). As a result, we
generate adversarial input x3in the subfigures. Now, by projecting
x3to the smallest possible region around x0, we compute x4, which
is the adversarial example with the lowest distortion rate so far.
Assume that applying DS-Multiclass for a third time, starting
from x4and searching inB(x0,ğ‘‘3), does not generate any other
adversarial examples. In this case, our technique returns x4.
DeepSearch with iterative refinement. Alg. 3 describes our
technique with iterative refinement. Each iteration consists of a
refinement and a search step, which we explain next.
The refinement step (lines 3â€“4) first calculates the ğ¿âˆdistance
ğ‘‘between xandxâ€². In Fig. 3, input xof the algorithm is x0, and
xâ€²isx1. So,xâ€²is an adversarial input that was generated by our
technique, and consequently, a vertex of B(x,ğ‘‘). On line 4, we
use bisect search to find the minimum distance ğ‘‘â€²such that the
input derived by projecting xâ€²onB(x,ğ‘‘â€²)is still adversarial. In
Fig. 3, this is distance ğ‘‘2, and Proj(B(x,ğ‘‘â€²),xâ€²)of the algorithm
corresponds to adversarial input x2in the figure.
Note that this refinement is possible because of Thm. 2, which
guarantees that a linear function ğ‘“evaluates to its minimum in
B(x,ğ‘‘â€²)for the input derived with projection. When ğ‘“is non-
linear, it might not evaluate to its minimum for adversarial input
Proj(B(x,ğ‘‘â€²),xâ€²). However, it is still the case that this projected
input is closer to x, i.e.,||xâˆ’Proj(B(x,ğ‘‘â€²),xâ€²)||ğ¿âˆâ‰¤||xâˆ’xâ€²||ğ¿âˆ,
and thus, has a lower distortion rate.
After selecting an input from which to start the search (line
5), the search step (lines 6â€“9) calls function DS-Binary (Alg. 1)
orDS-Multiclass (Alg. 2), depending on whether ğ‘“is a binary
classification function. The goal is to search for another adversarial
example (other than Proj(B(x,ğ‘‘â€²),xâ€²)) in regionB(x,ğ‘‘â€²). In Fig. 3,
an adversarial input found by this step, when starting the search
from x2, isx3, which is also a vertex of B(x0,ğ‘‘2).
On lines 10â€“13, we essentially check whether the search step
was successful in finding another adversarial input xâ€²â€². However,
DS-Binary andDS-Multiclass might not return an adversarial
example. If they do, like input x3in Fig. 3, the algorithm iterates
(line 14). If not, like when starting the search from input x4in the
figure, which is a projection of x3onB(x0,ğ‘‘3), then we return the
projected input and terminate.Algorithm 3: DeepSearch with iterative refinement.
Input: input xâˆˆRğ‘›, adversarial input xâ€²âˆˆB( x,ğ‘‘)(ğ‘‘âˆˆR),
functionğ‘“:Rğ‘›â†’Rğ‘š
Output: an adversarial input xâ€²â€²âˆˆB( x,ğ‘‘â€²)(ğ‘‘â€²â‰¤ğ‘‘)
1Function DS-Refinement( x,xâ€²,ğ‘“)is
2 repeat
3ğ‘‘:=||xâˆ’xâ€²||ğ¿âˆ
4 apply bisect search to find the smallest distance ğ‘‘â€²â‰¤ğ‘‘
such that input Proj(B(x,ğ‘‘â€²),xâ€²)is an adversarial
example.
5 choose an xnewâˆˆB( x,ğ‘‘â€²), from which to start a new
search, e.g. xnew=Proj(B(x,ğ‘‘â€²),xâ€²).
6 ifğ‘“is binary then
7 xâ€²â€²:=DS-Binary( x,xnew,ğ‘“,ğ‘‘â€²)
8 else
9 xâ€²â€²:=DS-Multiclass( x,xnew,ğ‘“,ğ‘‘â€²)
10 if xâ€²â€²is an adversarial example then
11 xâ€²:=xâ€²â€²
12 else
13 xâ€²:=Proj(B(x,ğ‘‘â€²),xâ€²)
14 until xâ€²â€²is not an adversarial example
15 return xâ€²andğ‘‘â€²
6 HIERARCHICAL GROUPING
For anğ‘›-dimensional input x, our technique makes at least ğ‘›queries
per iteration. For high-dimensional inputs, it could cost a signifi-
cantly large number of queries to perform even one iteration of our
attack. One basic strategy for query reduction is to divide pixels
of an input image into different groups and mutate all pixels in a
group to the same direction, e.g., all pixels in a group are moved to
their upper bounds. Thus, we only need one query for all pixels in
the same group. To exploit spatial regularities in images for query
efficiency, we adapt hierarchical grouping [50] to our setting.
DeepSearch with hierarchical grouping. Alg. 4 summarizes
our technique with hierarchical grouping, which consists of the
following three main steps.
(1)Initial grouping (line 8): For anğ‘›-dimensional input image x,
we first divide the ğ‘›dimensions intoâŒˆğ‘›
ğ‘˜2âŒ‰setsğº1,...,ğºâŒˆğ‘›
ğ‘˜2âŒ‰,
where each set ğºğ‘–(1â‰¤ğ‘–â‰¤ âŒˆğ‘›
ğ‘˜2âŒ‰) contains indices corre-
sponding to ğ‘˜Ã—ğ‘˜neighboring pixels in x. This amounts to
dividing the original image xintoâŒˆğ‘›
ğ‘˜2âŒ‰square blocks. The
definition of Initial-Group({1,...,ğ‘›},ğ‘˜)is omitted due to
space limitations.
(2)Fuzzing (line 10): We extend DeepSearch to handle groups
of pixels and write DeepSearch( x,xâ€²,ğ‘“,ğ‘‘,G)to mean such
an extension. For each set ğºğ‘–âˆˆG, our technique mutates all
coordinates that correspond to indices in the set toward the
same direction at the same time. Hence, DeepSearch only
compares two values per set, namely ğ‘“[ğ‘¢ğ‘–1/ğ‘¥ğ‘–1,...,ğ‘¢ğ‘–ğ‘™/ğ‘¥ğ‘–ğ‘™]
andğ‘“[ğ‘™ğ‘–1/ğ‘¥ğ‘–1,...,ğ‘™ğ‘–ğ‘™/ğ‘¥ğ‘–ğ‘™], whereğ‘–1,...,ğ‘–ğ‘™âˆˆğºğ‘–andğ‘™=|ğºğ‘–|.
(3)Group splitting (line 11â€“13): If the current partition of the
image is still too coarse for DeepSearch to find adversarial
examples, we perform DeepSearch in finer granularity. We
further divide each set ğºğ‘–intoğ‘šÃ—ğ‘šsubsetsğºğ‘–,1,...,ğºğ‘–,ğ‘šÃ—ğ‘š,
806ESEC/FSE â€™20, November 8â€“13, 2020, Virtual Event, USA Fuyuan Zhang, Sankalan Pal Chowdhury, and Maria Christakis
Algorithm 4: DeepSearch with hierarchical group-
ing.
Input: input xâˆˆRğ‘›, initial input xinitâˆˆB( x,ğ‘‘), initial group size
ğ‘˜, parameterğ‘šfor group splitting, function ğ‘“:Rğ‘›â†’Rğ‘š,
distanceğ‘‘âˆˆR, query budget ğ¿
Output: xâ€²âˆˆB( x,ğ‘‘)
1Function Divide-Group(G,ğ‘š)is
2 foreachğºğ‘–âˆˆGdo
3 divideğºğ‘–intoğ‘šÃ—ğ‘šsubset{ğºğ‘–,1,...,ğº ğ‘–,ğ‘šÃ—ğ‘š}
4Gâ€²:=Gâˆª{ğºğ‘–,1,...,ğº ğ‘–,ğ‘šÃ—ğ‘š}
5 returnGâ€²
6
7Function DS-Hierarchy( x,xinit,ğ‘“,ğ‘˜,ğ‘š )is
8G:=Initial-Group({1,...,ğ‘›},ğ‘˜)andxâ€²:=xinit
9 repeat
10 xâ€²:=DeepSearch( x,xâ€²,ğ‘“,ğ‘‘,G)
11 if1<ğ‘˜/ğ‘šthen
12G:=Divide-Group(G,ğ‘š)
13 ğ‘˜:=ğ‘˜/ğ‘š
14 untilNğ‘“(x)â‰ Nğ‘“(xâ€²), orreached query budget ğ¿
15 return xâ€²
where each set ğºğ‘–,ğ‘—(1â‰¤ğ‘—â‰¤ğ‘šÃ—ğ‘š) contains indices cor-
responding to ğ‘˜/ğ‘šÃ—ğ‘˜/ğ‘šneighboring pixels in x. After
splitting all sets, the total number of sets is multiplied by
ğ‘šÃ—ğ‘š. This results in a more fine-grained partition of input
x. We then go back to step (2).
In the query-limited setting, we use single-step DeepSearch on
line10, i.e., we fix MaxNum to 1 in Alg. 2 and Alg. 2â€™, and choose xinit
to be a vertex ofB(x,ğ‘‘)to avoid unnecessary queries. Hence, when
there areâŒˆğ‘›
ğ‘˜2âŒ‰sets inG, the total number of queries per iteration
in Alg. 4 reduces to âŒˆğ‘›
ğ‘˜2âŒ‰.
7 EXPERIMENTAL EVALUATION
We evaluate DeepSearch by using it to test the robustness of deep
neural networks trained for popular datasets. We also compare its
effectiveness with state-of-the-art blackbox attacks. Our experi-
ments are designed around the following research questions:
RQ1: IsDeepSearch effective in finding adversarial examples?
RQ2: IsDeepSearch effective in finding adversarial examples with
low distortion rates?
RQ3: IsDeepSearch a query-efficient blackbox attack?
RQ4: Is the hierarchical grouping of DeepSearch effective in im-
proving query efficiency?
We make our implementation open source1. Our experimental
data, including detected adversarial examples, are also available via
the provided link.
7.1 Evaluation Setup
Datasets and network models. We evaluate our approach on
deep neural networks trained for three well known datasets, namely
SVHN [ 53] (cropped digits), CIFAR- 10[38], and ImageNet [ 61]. For
1https://github.com/Practical-Formal-Methods/DeepSearcheach dataset, we randomly selected 1000 correctly classified images
from the test set on which to perform blackbox attacks.
For SVHN and CIFAR- 10, we attack two wide ResNet w 32-10
[83] networks, where one is naturally trained (without defense) and
the other is adversarially trained with a state-of-the-art defense
[49]. For SVHN, the undefended network we trained has 95.96%
test accuracy, and the adversarially trained network has 93.70%test
accuracy. During adversarial training, we used the PGD attack [ 49]
(that can perturb each pixel by at most 8on the 0â€“255pixel scale) to
generate adversarial examples. For CIFAR- 10, we trained an unde-
fended network with 95.07%test accuracy. The defended network
we attack is the pretrained network provided in Madryâ€™s challenge2.
For ImageNet, we attack a pretrained Inception v3 network [ 71],
which is undefended.
Defenses for ImageNet networks are also important, and we
would have attacked defended ImageNet networks if they were
publicly available. In this work, we did not attack such networks
(using the defense in [ 49]) for the following reasons. First, there are
no publicly available ImageNet networks that use the defense in
[49]. Second, none of the state-of-the-art attacks that we used for
comparison in our paper (i.e., [ 27,32,33,50]) have been evaluated on
defended networks for this dataset. Therefore, we did not compare
DeepSearch with these attacks on such networks. Third, due to the
extremely high computational cost, implementing the defense in
[49] for an ImageNet network is impractical.
Existing approaches. We compare DeepSearch with four state-
of-the-art blackbox attacks for generating adversarial examples:
â€¢The NES attack [ 32], optimized for the ğ¿âˆdistance metric,
is developed for various settings, including a query-limited
setting. It uses natural evolution strategies (NES) [ 62] for gra-
dient estimation and performs projected gradient-descent
(PGD) [ 49] style adversarial attacks using estimated gra-
dients. We compare with the NES attack developed for a
query-limited setting, i.e., QL-NES.
â€¢The Bandits attack [ 33] extended gradient-estimation-based
blackbox attacks by integrating gradient priors, e.g., time-
dependent and data-dependent priors, through a bandit op-
timization framework. The Bandits attack can perform both
ğ¿2andğ¿âˆattacks.
â€¢The Simple Blackbox Attack [ 27] is optimized for the ğ¿2dis-
tance metric. Starting from an input image, it finds adversa-
rial examples by repeatedly adding or subtracting a random
vector sampled from a set of predefined orthogonal candidate
vectors. We compare with their SimBA algorithm, which can
also be easily constrained using ğ¿âˆdistance.
â€¢The Parsimonious blackbox attack [ 50], optimized for the ğ¿âˆ
distance metric, encodes the problem of finding adversarial
perturbations as finding solutions to linear programs. For
an input xand distance ğ‘‘, it searches among the vertices
ofB(x,ğ‘‘)and finds adversarial examples by using efficient
algorithms in combinatorial optimization.
DeepSearch implementation. We briefly introduce some im-
plementation details of DeepSearch . In our implementation, the
classification function ğ‘“of multiclass classifiers maps input images
2https://github.com/MadryLab/cifar10_challenge
807DeepSearch: A Simple and Effective Blackbox Attack for Deep Neural Networks ESEC/FSE â€™20, November 8â€“13, 2020, Virtual Event, USA
to the logarithm of class probabilities predicted by neural networks.
In this setting, the objective function in Alg. 2 (resp. Alg. 2â€™) corre-
sponds to logit loss [12] (resp. cross-entropy loss [23]).
To reduce the number of queries, for input xand distance ğ‘‘, we
choose xinitâˆˆB( x,ğ‘‘)such that it is derived from xby setting
the values of all its pixels to the lower bounds in B(x,ğ‘‘). In the
refinement step, when calculating new adversarial examples within
smaller distances, we set xnewtoxinitfor convenience.
In our experiments, we used Alg. 2 to attack the undefended
networks. We used Alg. 2â€™ to attack the defended networks since
they are more robust against cross-entropy model attacks. To attack
the SVHN and CIFAR- 10networks, we set the initial grouping size
to4Ã—4. For ImageNet, we set the initial grouping size to 32Ã—32
due to their large image size. For group splitting, we set ğ‘š=2so
that we always divide a group into 2Ã—2subgroups.
In the hierarchical-grouping setting, we mutate groups of pixels
in random orders. To avoid overshooting query budgets, we mutate
groups in batches, and the batch size is 64in all our experiments.
Parameter settings. For all datasets, we set ğ¿âˆdistanceğ‘‘=8
on the 0â€“255pixel scale to perform attacks. For both SVHN and
CIFAR- 10networks, we set the query budget to 20,000. For the
ImageNet network, we set the query budget to 10,000as done in
related work [33, 50] .
For the QL-NES attack, we set ğœ=0.001, size of NES population
ğ‘›=100, learning rate ğœ‚=0.001, and momentum ğ›½=0.9for SVHN
and CIFAR- 10. We setğœ=0.01, size of NES population ğ‘›=100,
learning rate ğœ‚=0.0001, and momentum ğ›½=0.9for ImageNet.
For the Bandits attack, we set OCO learning rate ğœ‚=0.001,
image learning rate â„=0.0001, bandits exploration ğ›¿=0.1, finite
difference probe ğœ‚=0.1, and tile size to 16for SVHN and CIFAR- 10.
We set OCO learning rate ğœ‚=1, image learning rate â„=0.0001 ,
bandits exploration ğ›¿=1, finite difference probe ğœ‚=0.1, and tile
size to 64for ImageNet.
For the Parsimonious attack, we use the parameters mentioned
in their paper for CIFAR- 10and ImageNet networks. For SVHN,
we use the same parameters as for CIFAR- 10. Moreover, their im-
plementation offers both cross-entropy and logit loss to construct
attacks. We tried both loss functions in our experiments and select
the one with better performance for comparison.
7.2 Metrics
In our evaluation, we use the following metrics.
Success rate. The success rate measures the percentage of in-
put images for which adversarial examples are found. The higher
this rate, the more effective a given technique in finding adversa-
rial examples. Assume we write findAdv(x)to denote whether
an adversarial example is found for input x. If so, we have that
findAdv(x)=1; otherwise, we have findAdv(x)=0. For a set of
images X={x1,...,xğ‘˜}, the success rate of a given technique is:
ASR(X)=1
ğ‘˜ğ‘˜Ã•
ğ‘–=1findAdv(xğ‘–)
Average distortion rate. Let sets X={x1,...,xğ‘˜}andXadv=
{xâ€²
1,...,xâ€²
ğ‘˜}be input images and adversarial examples, respectively.Table 1: Results on SVHN networks.
AttackSuccess
rateAvg.
ğ¿âˆAvg.
ğ¿2Avg.
queriesMed.
queries
Undefended network
QL-NES 62.4% 2.58% 1.80% 2157 1700
Bandits 99.2% 3.43% 2.69% 762 573
SimBA 84.7% 4.65% 3.47% 1675 1430
Parsimonious 100% 4.59% 7.63% 337 231
DeepSearch 100% 1.89% 3.17% 229 196
Defended network
QL-NES 40.5% 4.10% 4.19% 5574 3900
Bandits 55.3% 4.38% 4.74% 2819 944
SimBA 65.9% 4.96% 3.95% 2687 2633
Parsimonious 78.9% 4.86% 8.08% 2174 423.5
DeepSearch 83.1% 3.35% 5.58% 1808 458
The average distortion rate between XandXadvwith respect to
theğ¿âˆdistance is:
AvgDRğ¿âˆ(X,Xadv)=1
ğ‘˜ğ‘˜Ã•
ğ‘–=1||xğ‘–âˆ’xâ€²
ğ‘–||ğ¿âˆ
||xğ‘–||ğ¿âˆ
As shown here, the lower this rate, the more subtle the adversa-
rial examples. For approaches that achieve similar misclassification
rates, we use this metric to determine which approach finds more
subtle perturbations. The average distortion rate with respect to ğ¿2
can be derived by substituting ğ¿âˆwithğ¿2in the above definition. In
our experimental results, we also include the average ğ¿2distortion
for reference.
Average queries. For blackbox attacks, we use the number of
queries required to find adversarial examples to measure their effi-
ciency. An approach that requires more queries to perform attacks
is more costly. For each approach, we calculate the average number
of queries required to perform successful attacks. We also list their
mean number of queries for reference. We point out that queries
made by the refinement step of DeepSearch are not counted when
calculating average queries because refinement starts after adversa-
rial examples are already found.
7.3 Experimental Results
DeepSearch outperforms all other blackbox attacks in success rate,
average queries, and average distortion rate. Experimental results
are shown in Fig. 4 and Tabs. 1â€“5.
Results on success rate (RQ1). DeepSearch is very effective in
finding adversarial examples for both undefended and defended
networks. Compared to other blackbox attacks, DeepSearch has
the highest attack success rate.
For undefended networks, the success rate of DeepSearch is
close to 100% for all three datasets. For the SVHN defended net-
work, DeepSearch has a success rate of 83.1%, which is 4.2%higher
than that of the Parsimonious attack (the second best attack in
success rate). For the CIFAR- 10defended network, DeepSearch has
a success rate of 47.7%.
Results on average distortion rate (RQ2). DeepSearch has
found adversarial examples with the lowest average ğ¿âˆdistortion
808ESEC/FSE â€™20, November 8â€“13, 2020, Virtual Event, USA Fuyuan Zhang, Sankalan Pal Chowdhury, and Maria Christakis
(a) SVHN (defended)
 (b) CIFAR- 10(defended)
 (c) ImageNet (undefended)
Figure 4: Results on success rate w.r.t number of queries.
Table 2: Results on CIFAR- 10networks.
AttackSuccess
rateAvg.
ğ¿âˆAvg.
ğ¿2Avg.
queriesMed.
queries
Undefended network
QL-NES 52.8% 1.24% 0.99% 1360 1100
Bandits 92.6% 2.66% 2.34% 838 616
SimBA 71.6% 3.36% 2.19% 1311 1150
Parsimonious 100% 3.36% 6.36% 339 238.5
DeepSearch 100% 1.64% 3.08% 247 196
Defended network
QL-NES 30.1% 2.71% 3.09% 4408 3200
Bandits 39.2% 2.95% 4.39% 2952 1176
SimBA 41.2% 3.46% 4.50% 2425 2424
Parsimonious 47.4% 3.45% 6.61% 1228 366
DeepSearch 47.7% 2.48% 4.70% 963 196
Table 3: Results on ImageNet undefended network.
AttackSuccess
rateAvg.
ğ¿âˆAvg.
ğ¿2Avg.
queriesMed.
queries
QL-NES 90.3% 1.83% 1.75% 2300 1800
Bandits 92.1% 2.15% 2.61% 930 496
SimBA 61% 3.15% 0.67% 4379 4103
Parsimonious 98.3% 3.16% 6.35% 660 241
DeepSearch 99.3% 1.50% 3.05% 561 196
rate for networks of all datasets. For the CIFAR- 10undefended net-
work, QL-NES has a success rate of only 52.8%. To have a more fair
comparison, for those 52.8%images that are successfully attacked
by both DeepSearch and QL-NES, we further calculate the average
distortion rate of the adversarial examples found by DeepSearch .
We find it to be 1.2%, which is lower than that of QL-NES. Al-
though DeepSearch is anğ¿âˆattack, adversarial examples found by
DeepSearch also have low average ğ¿2distortion rate.
Results on query efficiency (RQ3). DeepSearch outperforms
all other attacks in query efficiency. Compared to the Parsimonious
attack (the second best attack in query efficiency), DeepSearch
reduces the average queries by 15â€“32%across all datasets.
Results on query reduction (RQ4). We demonstrate the effec-
tiveness of hierarchical grouping in DeepSearch for query reduc-
tion. We use DeepSearch to attack networks with different initialTable 4: Query reduction for SVHN and CIFAR- 10. For each
dataset, success rate (resp. average queries) is shown in the
first (resp. second) row.
Dataset 1 2 Ã—2 4Ã—4 8Ã—8 16Ã—16
Undefended network
SVHN100% 100% 100% 100% 100%
742 300 229 238 242
CIFAR-10100% 100% 100% 100% 100%
462 301 247 255 259
Defended network
SVHN81.3% 82.4% 83.1% 83.6% 83.9%
3143 2292 1808 1565 1591
CIFAR-1047.7% 47.4% 47.7% 47.6% 47.6%
2292 1156 963 935 946
Table 5: Query reduction for ImageNet. Success rate (resp.
average queries) is shown in the first (resp. second) row.
Dataset 8Ã—8 16Ã—16 32Ã—32 64Ã—64 128Ã—128
ImageNet99.1% 99.1% 99.1% 99.4% 99.4%
765 580 533 554 580
group sizes and show their corresponding success rates and average
queries in Tabs. 4 and 5.
We first notice that the initial group size can only slightly affect
attack success rate. For the SVHN defended network, the success
rate increases from 81.3%to83.9%as initial group size increases
from 1to16Ã—16. However, the changes in success rate are negligible
for all other networks.
On the other hand, we observe that hierarchical grouping im-
proves query efficiency dramatically. We take the average queries
of group size 1and4Ã—4as an example for SVHN and CIFAR- 10
networks. For the SVHN undefended network, we see a 69.1%de-
crease of average queries from 742to229. For the SVHN defended
network, average queries are reduced by 42.5%from 3143 to1808.
For the CIFAR- 10undefended network, the average queries are
decreased by 46.5%from 462to247. For the CIFAR- 10defended
network, average queries are decreased by 58%from 2292 to963,
and for the ImageNet network, from group size 8Ã—8to32Ã—32, we
decreased the average queries by 30.3%from 765to533.
809DeepSearch: A Simple and Effective Blackbox Attack for Deep Neural Networks ESEC/FSE â€™20, November 8â€“13, 2020, Virtual Event, USA
7.4 Threats to Validity
We have identified the following three threats to the validity of our
experiments.
Datasets and network models. Our experimental results may
not generalize to other datasets or network models. However, we
used three of the most popular datasets for image classification,
SVHN, CIFAR-10, and ImageNet. Moreover, our network models
have very high test accuracy and the defense we use based on
adversarial training is state of the art.
Existing approaches. The second threat is related to the choice
of existing approaches with which we compare. DeepSearch uses
iterative linearization of non-linear neural networks and is tailored
to theğ¿âˆdistance metric. We, thus, compare with approaches
that can also perform ğ¿âˆattacks. To our knowledge, the blackbox
attacks with which we compared are all state-of-the-art ğ¿âˆattacks.
Fairness of comparison. The selection of parameters for each
approach could affect the fairness of our comparison. We tried
various parameters for each attack and choose the ones yielding
best performance.
8 RELATED WORK
Adversarial robustness. Szegedy et al. [ 72] first discovered
adversarial examples in neural networks and used box-constrained
L-BFGS to find them. Since then, multiple whitebox adversarial
attacks have been proposed: FGSM [ 24], BIM [ 40], DeepFool [ 51],
JSMA [ 57], PGD [ 49], and C&W [ 12]. Goodfellow et al. [ 24] first
argued that the primary cause of adversarial examples is the linear
nature of neural networks, and they proposed FGSM that allows fast
generation of adversarial examples. BIM improved FGSM by extend-
ing it with iterative procedures. DeepFool [ 51] is another method
that performs adversarial attacks through iterative linearization of
neural networks.
Blackbox adversarial attacks are more difficult than whitebox
ones, and many blackbox attacks require a large number of queries.
Papernot et al. [ 55,56] explored blackbox attacks based on the
phenomenon of transferability [ 55,72]. Chen et al. [ 13] and Bhagoji
et al. [ 5] proposed blackbox attacks based on gradient estimation
[41,68]. Uesato et al. [ 75] used SPSA [ 67]. Ilyas et al. [ 32,33] used
NES [ 62] and proposed the Bandits attack. Narodytska et al. [ 52]
performed a local-search-based attack. The boundary attack [ 6]
only requires access to the final decision of neural networks. Guo
et al. [ 27] further considered perturbations in low frequency space.
Moon et al. [ 50] leveraged algorithms in combinatorial optimization.
Although research on developing adversarial attacks is moving
fast, research on defending neural networks against adversarial
attacks is relatively slow [ 1,2,9â€“12,15,20,29,46,65]. Many defense
techniques are shown to be ineffective soon after they have been
developed. We refer to the work of Carlini et al. [ 8] for a more
detailed discussion on evaluating adversarial robustness.
Testing deep neural networks. Recently, significant progress
has been made on testing neural networks. Several useful test cov-
erage criteria have been proposed to guide test case generation:
DeepXplore [ 58] proposed neuron coverage and the first whitebox
testing framework for neural networks; DeepGauge[47] proposeda set of finer-grained test coverage criteria; DeepCT [ 48] further
proposed combinatorial test coverage for neural networks; Sun et
al. [69] proposed coverage criteria inspired by MC/DC; Kim et al.
[36] proposed surprise adequacy for deep learning systems. Sekhon
et al. [ 63] and Li et al. [ 43] pointed out the limitation of existing
structural coverage criteria for neural networks. Li et al. [ 63] also
discussed improvements for better coverage criteria.
Moreover, Sun et al. [ 70] proposed the first concolic testing
[22,64] approach for neural networks. DeepCheck [ 26] tests neural
networks based on symbolic execution [ 14,37]. TensorFuzz [ 54]
proposed the first framework of coverage-guided fuzzing for neural
networks. DeepHunter [ 82] considered various mutation strategies
for their fuzzing framework. Wicker et al. [ 78] extracted features
from images and computed adversarial examples using a two-player
turn-based stochastic game. DLFuzz [ 28] proposed the first differ-
ential fuzzing framework for deep learning systems. DeepTest [ 73]
and DeepRoad [ 86] proposed testing tools for autonomous driving
systems based on deep neural networks. For more on testing neural
networks, we refer to the work of Zhang et al. [ 85] that surveys
testing of machine-learning systems.
Formal verification of deep neural networks. Verification of
neural networks is more challenging than testing. Early work [ 59]
used abstract interpretation [ 16] to verify small-sized neural net-
works. Recent work [ 25,35,66] used SMT [ 3] techniques and con-
sidered new abstract domains.
Liu et al. [ 44] classified recent work in the area into five cate-
gories: Reachability-analysis based approaches include MaxSens
[81], ExactReach [ 80], and AI2[21]; NSVerify [ 45], MIPVerify [ 74]
and ILP[ 4] are based on primal optimization; Duality [ 18], Conv-
Dual [ 79] and Certify [ 60] use dual optimization; Fast-Lin and
Fast-Lip [ 77], ReluVal [ 76] and DLV [ 31] combine reachability with
search; Sherlock [ 17], Reluplex [ 34], Planet [ 19] and BaB [ 7] com-
bine search with optimization. Lie et al. [ 44] provide a more detailed
comparison and discussion of the above mentioned work.
9 CONCLUSION AND FUTURE WORK
We proposed and implemented DeepSearch , a novel blackbox-
fuzzing technique for attacking deep neural networks. DeepSearch
is simple and effective in finding adversarial examples with low
distortion, and it outperforms state-of-the-art blackbox attacks
in a query-limited setting. In our future work, we will continue
improving the effectiveness of DeepSearch for an even more query-
efficientğ¿âˆattack. We are also interested in extending DeepSearch
to construct query-efficient ğ¿2attacks.
Designing effective defenses to secure deep neural networks
against adversarial attacks is non-trivial. In this paper, we did not
focus on proposing defenses against blackbox attacks. Instead, we
attacked neural networks with adversarial training-based defenses
[49]. Another interesting direction for future work is to develop
defense techniques that specifically target blackbox attacks, for
instance by identifying patterns in their sequences of queries.
ACKNOWLEDGMENTS
We are grateful to the reviewers for their valuable feedback.
This work was supported by DFG grant 389792660 as part of
TRR 248 (see https://perspicuous-computing.science).
810ESEC/FSE â€™20, November 8â€“13, 2020, Virtual Event, USA Fuyuan Zhang, Sankalan Pal Chowdhury, and Maria Christakis
REFERENCES
[1]Anish Athalye and Nicholas Carlini. 2018. On the Robustness of the CVPR 2018
White-Box Adversarial Example Defenses. CoRR abs/1804.03286 (2018).
[2]Anish Athalye, Nicholas Carlini, and David A. Wagner. 2018. Obfuscated Gra-
dients Give a False Sense of Security: Circumventing Defenses to Adversarial
Examples. In ICML (PMLR, Vol. 80) . PMLR, 274â€“283.
[3]Clark W. Barrett and Cesare Tinelli. 2018. Satisfiability Modulo Theories. In
Handbook of Model Checking . Springer, 305â€“343.
[4]Osbert Bastani, Yani Ioannou, Leonidas Lampropoulos, Dimitrios Vytiniotis,
Aditya V. Nori, and Antonio Criminisi. 2016. Measuring Neural Net Robustness
with Constraints. In NIPS . 2613â€“2621.
[5]Arjun Nitin Bhagoji, Warren He, Bo Li, and Dawn Song. 2018. Practical Black-Box
Attacks on Deep Neural Networks Using Efficient Query Mechanisms. In ECCV
(LNCS, Vol. 11216) . Springer, 158â€“174.
[6]Wieland Brendel, Jonas Rauber, and Matthias Bethge. 2018. Decision-Based
Adversarial Attacks: Reliable Attacks Against Black-Box Machine Learning Mod-
els. In ICLR . OpenReview.net.
[7]Rudy Bunel, Ilker Turkaslan, Philip H. S. Torr, Pushmeet Kohli, and Pawan Ku-
mar Mudigonda. 2018. A Unified View of Piecewise Linear Neural Network
Verification. In NeurIPS . 4795â€“4804.
[8]Nicholas Carlini, Anish Athalye, Nicolas Papernot, Wieland Brendel, Jonas
Rauber, Dimitris Tsipras, Ian J. Goodfellow, Aleksander Madry, and Alexey Ku-
rakin. 2019. On Evaluating Adversarial Robustness. CoRR abs/1902.06705 (2019).
[9]Nicholas Carlini and David A. Wagner. 2016. Defensive Distillation is Not Robust
to Adversarial Examples. CoRR abs/1607.04311 (2016).
[10] Nicholas Carlini and David A. Wagner. 2017. Adversarial Examples Are Not
Easily Detected: Bypassing Ten Detection Methods. In AISec@CCS . ACM, 3â€“14.
[11] Nicholas Carlini and David A. Wagner. 2017. MagNet and â€œEfficient Defenses
Against Adversarial Attacksâ€ Are Not Robust to Adversarial Examples. CoRR
abs/1711.08478 (2017).
[12] Nicholas Carlini and David A. Wagner. 2017. Towards Evaluating the Robustness
of Neural Networks. In S&P. IEEE Computer Society, 39â€“57.
[13] Pin-Yu Chen, Huan Zhang, Yash Sharma, Jinfeng Yi, and Cho-Jui Hsieh. 2017.
ZOO: Zeroth Order Optimization Based Black-box Attacks to Deep Neural Net-
works Without Training Substitute Models. In AISec@CCS . ACM, 15â€“26.
[14] Lori A. Clarke. 1976. A System to Generate Test Data and Symbolically Execute
Programs. TSE2 (1976), 215â€“222. Issue 3.
[15] Cory Cornelius. 2019. The Efficacy of SHIELD Under Different Threat Models.
CoRR abs/1902.00541 (2019).
[16] Patrick Cousot and Radhia Cousot. 1977. Abstract Interpretation: A Unified
Lattice Model for Static Analysis of Programs by Construction or Approximation
of Fixpoints. In POPL . ACM, 238â€“252.
[17] Souradeep Dutta, Susmit Jha, Sriram Sankaranarayanan, and Ashish Tiwari. 2018.
Output Range Analysis for Deep Feedforward Neural Networks. In NFM (LNCS,
Vol. 10811) . Springer, 121â€“138.
[18] Krishnamurthy Dvijotham, Robert Stanforth, Sven Gowal, Timothy A. Mann,
and Pushmeet Kohli. 2018. A Dual Approach to Scalable Verification of Deep
Networks. In UAI. AUAI Press, 550â€“559.
[19] RÃ¼diger Ehlers. 2017. Formal Verification of Piece-Wise Linear Feed-Forward
Neural Networks. In ATVA (LNCS, Vol. 10482) . Springer, 269â€“286.
[20] Logan Engstrom, Andrew Ilyas, and Anish Athalye. 2018. Evaluating and Un-
derstanding the Robustness of Adversarial Logit Pairing. CoRR abs/1807.10272
(2018).
[21] Timon Gehr, Matthew Mirman, Dana Drachsler-Cohen, Petar Tsankov, Swarat
Chaudhuri, and Martin Vechev. 2018. AI2: Safety and Robustness Certification of
Neural Networks with Abstract Interpretation. In S&P. IEEE Computer Society,
3â€“18.
[22] Patrice Godefroid, Nils Klarlund, and Koushik Sen. 2005. DART: Directed Auto-
mated Random Testing. In PLDI . ACM, 213â€“223.
[23] Ian Goodfellow, Yoshua Bengio, and Aaron Courville. 2016. Deep Learning . MIT
Press.
[24] Ian J. Goodfellow, Jonathon Shlens, and Christian Szegedy. 2015. Explaining and
Harnessing Adversarial Examples. In ICLR .
[25] Divya Gopinath, Guy Katz, Corina S. Pasareanu, and Clark W. Barrett. 2018.
DeepSafe: A Data-Driven Approach for Assessing Robustness of Neural Networks.
InATVA (LNCS, Vol. 11138) . Springer, 3â€“19.
[26] Divya Gopinath, Kaiyuan Wang, Mengshi Zhang, Corina S. Pasareanu, and
Sarfraz Khurshid. 2018. Symbolic Execution for Deep Neural Networks. CoRR
abs/1807.10439 (2018).
[27] Chuan Guo, Jacob R. Gardner, Yurong You, Andrew Gordon Wilson, and Kilian Q.
Weinberger. 2019. Simple Black-box Adversarial Attacks. In ICML . PMLR.
[28] Jianmin Guo, Yu Jiang, Yue Zhao, Quan Chen, and Jiaguang Sun. 2018. DLFuzz:
Differential Fuzzing Testing of Deep Learning Systems. In ESEC/FSE . ACM, 739â€“
743.
[29] Warren He, James Wei, Xinyun Chen, Nicholas Carlini, and Dawn Song. 2017.
Adversarial Example Defense: Ensembles of Weak Defenses Are Not Strong. InWOOT . USENIX.
[30] Geoffrey Hinton, Li Deng, Dong Yu, George Dahl, Abdel-rahman Mohamed,
Navdeep Jaitly, Andrew Senior, Vincent Vanhoucke, Patrick Nguyen, Tara Sainath,
and Brian Kingsbury. 2012. Deep Neural Networks for Acoustic Modeling in
Speech Recognition. Signal Process. Mag. 29 (2012), 82â€“97. Issue 6.
[31] Xiaowei Huang, Marta Kwiatkowska, Sen Wang, and Min Wu. 2017. Safety
Verification of Deep Neural Networks. In CAV (LNCS, Vol. 10426) . Springer, 3â€“29.
[32] Andrew Ilyas, Logan Engstrom, Anish Athalye, and Jessy Lin. 2018. Black-Box
Adversarial Attacks with Limited Queries and Information. In ICML (PMLR,
Vol. 80) . PMLR, 2142â€“2151.
[33] Andrew Ilyas, Logan Engstrom, and Aleksander Madry. 2019. Prior Convictions:
Black-box Adversarial Attacks with Bandits and Priors. In ICLR .
[34] Guy Katz, Clark W. Barrett, David L. Dill, Kyle Julian, and Mykel J. Kochenderfer.
2017. Reluplex: An Efficient SMT Solver for Verifying Deep Neural Networks. In
CAV (LNCS, Vol. 10426) . Springer, 97â€“117.
[35] Guy Katz, Derek A. Huang, Duligur Ibeling, Kyle Julian, Christopher Lazarus,
Rachel Lim, Parth Shah, Shantanu Thakoor, Haoze Wu, Aleksandar Zeljic, David L.
Dill, Mykel J. Kochenderfer, and Clark W. Barrett. 2019. The Marabou Framework
for Verification and Analysis of Deep Neural Networks. In CAV (LNCS, Vol. 11561) .
Springer, 443â€“452.
[36] Jinhan Kim, Robert Feldt, and Shin Yoo. 2019. Guiding Deep Learning System
Testing Using Surprise Adequacy. In ICSE . IEEE Computer Society/ACM, 1039â€“
1049.
[37] James C. King. 1976. Symbolic Execution and Program Testing. CACM 19 (1976),
385â€“394. Issue 7.
[38] Alex Krizhevsky. 2009. Learning Multiple Layers of Features from Tiny Images .
Technical Report. University of Toronto.
[39] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. 2017. ImageNet Clas-
sification with Deep Convolutional Neural Networks. CACM 60 (2017), 84â€“90.
Issue 6.
[40] Alexey Kurakin, Ian J. Goodfellow, and Samy Bengio. 2017. Adversarial Examples
in the Physical World. In ICLR . OpenReview.net.
[41] Peter D. Lax and Maria Shea Terrell. 2014. Calculus with Applications . Springer.
[42] Yann LeCun, LÃ©on Bottou, Yoshua Bengio, and Patrick Haffner. 1998. Gradient-
Based Learning Applied to Document Recognition. In Proc. IEEE . IEEE Computer
Society, 2278â€“2324.
[43] Zenan Li, Xiaoxing Ma, Chang Xu, and Chun Cao. 2019. Structural Coverage
Criteria for Neural Networks Could Be Misleading. In ICSE (NIER) . IEEE Computer
Society/ACM, 89â€“92.
[44] Changliu Liu, Tomer Arnon, Christopher Lazarus, Clark W. Barrett, and Mykel J.
Kochenderfer. 2019. Algorithms for Verifying Deep Neural Networks. CoRR
abs/1903.06758 (2019).
[45] Alessio Lomuscio and Lalit Maganti. 2017. An Approach to Reachability Analysis
for Feed-Forward ReLU Neural Networks. CoRR abs/1706.07351 (2017).
[46] Pei-Hsuan Lu, Pin-Yu Chen, Kang-Cheng Chen, and Chia-Mu Yu. 2018. On the
Limitation of MagNet Defense Against L1-Based Adversarial Examples. In DSN
Workshops . IEEE Computer Society, 200â€“214.
[47] Lei Ma, Felix Juefei-Xu, Fuyuan Zhang, Jiyuan Sun, Minhui Xue, Bo Li, Chunyang
Chen, Ting Su, Li Li, Yang Liu, Jianjun Zhao, and Yadong Wang. 2018. DeepGauge:
Multi-Granularity Testing Criteria for Deep Learning Systems. In ASE. ACM,
120â€“131.
[48] Lei Ma, Fuyuan Zhang, Minhui Xue, Bo Li, Yang Liu, Jianjun Zhao, and
Yadong Wang. 2018. Combinatorial Testing for Deep Learning Systems. CoRR
abs/1806.07723 (2018).
[49] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and
Adrian Vladu. 2018. Towards Deep Learning Models Resistant to Adversarial
Attacks. In ICLR . OpenReview.net.
[50] Seungyong Moon, Gaon An, and Hyun Oh Song. 2019. Parsimonious Black-Box
Adversarial Attacks via Efficient Combinatorial Optimization. In ICML . PMLR.
[51] Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, and Pascal Frossard. 2016.
DeepFool: A Simple and Accurate Method to Fool Deep Neural Networks. In
CVPR . IEEE Computer Society, 2574â€“2582.
[52] Nina Narodytska and Shiva Prasad Kasiviswanathan. 2017. Simple Black-Box
Adversarial Attacks on Deep Neural Networks. In CVPR Workshops . IEEE Com-
puter Society, 1310â€“1318.
[53] Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and An-
drew Y Ng. 2011. Reading Digits in Natural Images with Unsupervised Feature
Learning. In NIPS Workshop on Deep Learning and Unsupervised Feature Learning .
[54] Augustus Odena, Catherine Olsson, David Andersen, and Ian J. Goodfellow. 2019.
TensorFuzz: Debugging Neural Networks with Coverage-Guided Fuzzing. In
ICML (PMLR, Vol. 97) . PMLR, 4901â€“4911.
[55] Nicolas Papernot, Patrick D. McDaniel, and Ian J. Goodfellow. 2016. Trans-
ferability in Machine Learning: From Phenomena to Black-Box Attacks Using
Adversarial Samples. CoRR abs/1605.07277 (2016).
[56] Nicolas Papernot, Patrick D. McDaniel, Ian J. Goodfellow, Somesh Jha, Z. Berkay
Celik, and Ananthram Swami. 2017. Practical Black-Box Attacks Against Machine
Learning. In AsiaCCS . ACM, 506â€“519.
811DeepSearch: A Simple and Effective Blackbox Attack for Deep Neural Networks ESEC/FSE â€™20, November 8â€“13, 2020, Virtual Event, USA
[57] Nicolas Papernot, Patrick D. McDaniel, Somesh Jha, Matt Fredrikson, Z. Berkay
Celik, and Ananthram Swami. 2016. The Limitations of Deep Learning in Adversa-
rial Settings. In EuroS&P . IEEE Computer Society, 372â€“387.
[58] Kexin Pei, Yinzhi Cao, Junfeng Yang, and Suman Jana. 2017. DeepXplore: Auto-
mated Whitebox Testing of Deep Learning Systems. In SOSP . ACM, 1â€“18.
[59] Luca Pulina and Armando Tacchella. 2010. An Abstraction-Refinement Approach
to Verification of Artificial Neural Networks. In CAV (LNCS, Vol. 6174) . Springer,
243â€“257.
[60] Aditi Raghunathan, Jacob Steinhardt, and Percy Liang. 2018. Certified Defenses
Against Adversarial Examples. In ICLR . OpenReview.net.
[61] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma,
Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C.
Berg, and Fei-Fei Li. 2015. ImageNet Large Scale Visual Recognition Challenge.
IJCV (2015), 211â€“252.
[62] Tim Salimans, Jonathan Ho, Xi Chen, and Ilya Sutskever. 2017. Evolution Strate-
gies as a Scalable Alternative to Reinforcement Learning. CoRR abs/1703.03864
(2017).
[63] Jasmine Sekhon and Cody Fleming. 2019. Towards Improved Testing for Deep
Learning. In ICSE (NIER) . IEEE Computer Society/ACM, 85â€“88.
[64] Koushik Sen, Darko Marinov, and Gul Agha. 2005. CUTE: A Concolic Unit Testing
Engine for C. In ESEC/FSE . ACM, 263â€“272.
[65] Yash Sharma and Pin-Yu Chen. 2018. Bypassing Feature Squeezing by Increasing
Adversary Strength. CoRR abs/1803.09868 (2018).
[66] Gagandeep Singh, Timon Gehr, Markus PÃ¼schel, and Martin T. Vechev. 2019. An
Abstract Domain for Certifying Neural Networks. PACMPL 3 (2019), 41:1â€“41:30.
Issue POPL.
[67] James C. Spall. 1992. Multivariate Stochastic Approximation Using a Simulta-
neous Perturbation Gradient Approximation. TAC 37 (1992), 332â€“341. Issue
3.
[68] James C. Spall. 2003. Introduction to Stochastic Search and Optimization . John
Wiley and Sons.
[69] Youcheng Sun, Xiaowei Huang, and Daniel Kroening. 2018. Testing Deep Neural
Networks. CoRR abs/1803.04792 (2018).
[70] Youcheng Sun, Min Wu, Wenjie Ruan, Xiaowei Huang, Marta Kwiatkowska, and
Daniel Kroening. 2018. Concolic Testing for Deep Neural Networks. In ASE.
ACM, 109â€“119.
[71] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbig-
niew Wojna. 2016. Rethinking the Inception Architecture for Computer Vision.
InCVPR . IEEE Computer Society, 2818â€“2826.
[72] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan,
Ian J. Goodfellow, and Rob Fergus. 2014. Intriguing Properties of Neural Networks.InICLR .
[73] Yuchi Tian, Kexin Pei, Suman Jana, and Baishakhi Ray. 2018. DeepTest: Automated
Testing of Deep-Neural-Network-Driven Autonomous Cars. In ICSE. ACM, 303â€“
314.
[74] Vincent Tjeng, Kai Y. Xiao, and Russ Tedrake. 2019. Evaluating Robustness of
Neural Networks with Mixed Integer Programming. In ICLR . OpenReview.net.
[75] Jonathan Uesato, Brendan Oâ€™Donoghue, Pushmeet Kohli, and AÃ¤ron van den
Oord. 2018. Adversarial Risk and the Dangers of Evaluating Against Weak
Attacks. In ICML (PMLR, Vol. 80) . PMLR, 5032â€“5041.
[76] Shiqi Wang, Kexin Pei, Justin Whitehouse, Junfeng Yang, and Suman Jana. 2018.
Formal Security Analysis of Neural Networks Using Symbolic Intervals. In Secu-
rity. USENIX, 1599â€“1614.
[77] Tsui-Wei Weng, Huan Zhang, Hongge Chen, Zhao Song, Cho-Jui Hsieh, Luca
Daniel, Duane S. Boning, and Inderjit S. Dhillon. 2018. Towards Fast Computation
of Certified Robustness for ReLU Networks. In ICML (PMLR, Vol. 80) . PMLR, 5273â€“
5282.
[78] Matthew Wicker, Xiaowei Huang, and Marta Kwiatkowska. 2018. Feature-Guided
Black-Box Safety Testing of Deep Neural Networks. In TACAS (LNCS, Vol. 10805) .
Springer, 408â€“426.
[79] Eric Wong and J. Zico Kolter. 2018. Provable Defenses Against Adversarial
Examples via the Convex Outer Adversarial Polytope. In ICML (PMLR, Vol. 80) .
PMLR, 5283â€“5292.
[80] Weiming Xiang, Hoang-Dung Tran, and Taylor T. Johnson. 2017. Reachable Set
Computation and Safety Verification for Neural Networks with ReLU Activations.
CoRR abs/1712.08163 (2017).
[81] Weiming Xiang, Hoang-Dung Tran, and Taylor T. Johnson. 2018. Output Reach-
able Set Estimation and Verification for Multilayer Neural Networks. TNNLS 29
(2018), 5777â€“5783. Issue 11.
[82] Xiaofei Xie, Lei Ma, Felix Juefei-Xu, Minhui Xue, Hongxu Chen, Yang Liu, Jianjun
Zhao, Bo Li, Jianxiong Yin, and Simon See. 2019. DeepHunter: A Coverage-Guided
Fuzz Testing Framework for Deep Neural Networks. In ISSTA . ACM, 146â€“157.
[83] Sergey Zagoruyko and Nikos Komodakis. 2016. Wide Residual Networks. In
BMVC .
[84] Fuyuan Zhang, Sankalan Pal Chowdhury, and Maria Christakis. 2020. DeepSearch:
A Simple and Effective Blackbox Attack for Deep Neural Networks. CoRR
abs/1910.06296 (2020).
[85] Jie M. Zhang, Mark Harman, Lei Ma, and Yang Liu. 2019. Machine Learning
Testing: Survey, Landscapes and Horizons. CoRR abs/1906.10742 (2019).
[86] Mengshi Zhang, Yuqun Zhang, Lingming Zhang, Cong Liu, and Sarfraz Khur-
shid. 2018. DeepRoad: GAN-Based Metamorphic Testing and Input Validation
Framework for Autonomous Driving Systems. In ASE. ACM, 132â€“142.
812