Why Security Defects Go Unnoticed during Code
Reviews? A Case-Control Study of the Chromium
OS Project
Rajshakhar Paul, Asif Kamal Turzo, Amiangshu Bosu
Department of Computer Science
Wayne State University
Detroit, Michigan, USA
fr.paul, asifkamal, amiangshu.bosug@wayne.edu
Abstract ‚ÄîPeer code review has been found to be effective in
identifying security vulnerabilities. However, despite practicing
mandatory code reviews, many Open Source Software (OSS)
projects still encounter a large number of post-release security
vulnerabilities, as some security defects escape those. Therefore,
a project manager may wonder if there was any weakness
or inconsistency during a code review that missed a security
vulnerability. Answers to this question may help a manager
pinpointing areas of concern and taking measures to improve
the effectiveness of his/her project‚Äôs code reviews in identifying
security defects. Therefore, this study aims to identify the factors
that differentiate code reviews that successfully identiÔ¨Åed security
defects from those that missed such defects.
With this goal, we conduct a case-control study of Chromium
OS project. Using multi-stage semi-automated approaches, we
build a dataset of 516 code reviews that successfully identiÔ¨Åed
security defects and 374 code reviews where security defects
escaped. The results of our empirical study suggest that the are
signiÔ¨Åcant differences between the categories of security defects
that are identiÔ¨Åed and that are missed during code reviews.
A logistic regression model Ô¨Åtted on our dataset achieved an
AUC score of 0.91 and has identiÔ¨Åed nine code review attributes
that inÔ¨Çuence identiÔ¨Åcations of security defects. While time to
complete a review, the number of mutual reviews between two
developers, and if the review is for a bug Ô¨Åx have positive impacts
on vulnerability identiÔ¨Åcation, opposite effects are observed from
the number of directories under review, the number of total
reviews by a developer, and the total number of prior commits
for the Ô¨Åle under review.
Index Terms‚Äîsecurity, code review, vulnerability
I. I NTRODUCTION
Peer code review (a.k.a. code review) is a software quality
assurance practice of getting a code change inspected by
peers before its integration to the main codebase. In addition
to improving maintainability of a project and identiÔ¨Åcation
of bugs [3], [11], code reviews have been found useful
in preventing security vulnerabilities [12], [34]. Therefore,
many popular Open Source Software (OSS) projects such
as, Chromium, Android, Qt, oVirt, and Mozilla as well as
commercial organizations such as, Google, Microsoft, and
Facebook have integrated code reviews in their software de-
velopment pipeline [11], [50]. With mandatory code reviews,
many OSS projects (e.g., Android, and Chromium OS) requireeach and every change to be reviewed and approved by
multiple peers [3], [50]. Although mandatory code reviews
are preventing a signiÔ¨Åcant number of security defects [12],
[44], these projects still report a large number of post-release
security defects in the Common Vulnerabilities and Exposure
(a.k.a. CVE) database1. Therefore, a project manager from
such a project may wonder if there was any weakness or
inconsistency during a code review that missed a security
vulnerability. For example, she/he may want to investigate:
i) if reviewers had adequate expertise relevant to a particular
change, ii) if reviewers spent adequate time on the reviews, or
iii) if the code change was too difÔ¨Åcult to understand. Answers
to these questions may help a manager pinpointing areas of
concern and taking measures to improve the effectiveness of
his/her project‚Äôs code reviews in identifying security defects.
To investigate these questions, this study aims to identify
the factors that differentiate code reviews that successfully
identiÔ¨Åed security defects from those that missed such defects.
Since code reviews can identify vulnerabilities very early in
the software development pipeline, security defects identiÔ¨Åed
during code reviews incur signiÔ¨Åcantly less cost, as the longer
it takes to detect and Ô¨Åx a security vulnerability, the more
that vulnerability will cost [35]. Therefore, improving the
effectiveness of code reviews in identifying security defects
may reduce the cost of developing a secure software.
With this goal, we conducted a case-control study of
the Chromium OS project. Case-control studies, which are
common in the medical Ô¨Åeld, compare two existing groups
differing on an outcome [52]. We identiÔ¨Åed the cases and the
controls based on our outcome of interest, namely whether
a security defect was identiÔ¨Åed or escaped during the code
review of a vulnerability contributing commit (VCC). Using a
keyword-based mining approach followed by manual valida-
tions on a dataset of 404,878 Chromium OS code reviews,
we identiÔ¨Åed 516 code reviews that successfully identiÔ¨Åed
security defects. In addition, from the Chromium OS bug
repository, we identiÔ¨Åed 239 security defects that escaped code
reviews. Using a modiÔ¨Åed version of the SZZ algorithm [9]
1https://cve.mitre.org/cve/
13732021 IEEE/ACM 43rd International Conference on Software Engineering (ICSE)
1558-1225/21/$31.00 ¬©2021 IEEE
DOI 10.1109/ICSE43902.2021.00124
followed by manual validations, we identiÔ¨Åed 374 VCCs
and corresponding code reviews that approved those changes.
Using these two datasets, we conduct an empirical study and
answer the following two research questions:
(RQ1): Which categories of security defects are more likely
to be missed during code reviews?
Motivation: Since a reviewer primarily relies on his/her
knowledge and understanding of the project, some cate-
gories of security defects may be more challenging to
identify during code reviews than others. The results
of this investigation can help a project manager in two
ways. First, it will allow a manager to leverage other
testing /quality assurance methods that are more effective
in identifying categories of vulnerabilities that are more
likely to be missed during code reviews. Second, a man-
ager can arrange training materials to educate developers
and adopt more effective code review strategies for those
categories of security vulnerabilities.
Findings:The results suggest that some categories of
vulnerabilities are indeed more difÔ¨Åcult to identify during
code reviews than others. The identiÔ¨Åcation of a vulnera-
bility that requires an understanding of a few lines of the
code context (e.g., unsafe method, calculation of buffer
size, and resource release) are more likely to be identiÔ¨Åed
during code reviews. On the other hand, vulnerabilities
that require either code execution (e.g., input validation)
or understanding of larger code contexts (e.g., resource
lifetime, and authentication management ) are more likely
to remain unidentiÔ¨Åed.
(RQ2): Which factors inÔ¨Çuence the identiÔ¨Åcation of security
defects during a code review?
Motivation:Insights obtained from this investigation can help a
project manager pinpoint areas of concern and take
targeted measures to improve the effectiveness of his/her
project‚Äôs code reviews in identifying security defects.
Findings:We developed a Logistic Regression model
based on 18 code review attributes. The model, which
achieved an AUC of 0.91, found nine code review
attributes that distinguish code reviews that missed a
vulnerability from the ones that did not. According to
the model, the likelihood of a security defect being
identiÔ¨Åed during code review declines with the increase
in the number of directories/Ô¨Åles involved in that change.
Surprisingly, the likelihood of missing a vulnerability
during code reviews increased with a developer‚Äôs re-
viewing experience. Vulnerabilities introduced in a bug
Ô¨Åxing commit were more likely to be identiÔ¨Åed than those
introduced in a non-bug Ô¨Åx commit.
The primary contributions of this paper are:
An empirically built and validated dataset of code reviews
that either identiÔ¨Åed or missed security vulnerabilities.
An empirical investigation of security defects that es-
caped vs. the ones that are identiÔ¨Åed during code reviews.
A logistic regression model to identify relative impor-tance of various factors inÔ¨Çuencing identiÔ¨Åcation of se-
curity defects during code reviews.
An illustration of conducting a case-control study in the
software engineering context.
We make our script and the dataset publicly available at:
https://zenodo.org/record/4539891.
Paper organization: The remainder of this paper is orga-
nized as follows. Section II provides a brief background on
code reviews and case-control study. Section III details our
research methodology. Section IV describes the results of our
case-control study. Section V discusses the implications based
on the results of this study. Section VI discusses the threats to
validity of our Ô¨Åndings. Section VII describes related works.
Finally, Section VIII provides the future direction of our work
and concludes this paper.
II. B ACKGROUND
This section provides a brief background on security vul-
nerabilities, code reviews, and case-control studies.
A. Security Vulnerabilities
A vulnerability is a weakness in a software component
that can be exploited by a threat actor, such as an attacker,
to perform unauthorized actions within a computer system.
Vulnerabilities result mainly from bugs in code which arise due
to violations of secure coding practices, lack of web security
expertise, bad system design, or poor implementation quality.
Hundreds of types of security vulnerabilities can occur in code,
design, or system architecture. The security community uses
Common Weakness Enumerations (CWE) [41] to provide an
extensive catalog of those vulnerabilities.
B. Code reviews
Compared with the traditional heavy-weight inspection pro-
cess, peer code review is more light-weight, informal, tool-
based, and used regularly in practice [3]. In addition to their
positive effects on software quality in general, Code review can
be an important practice for detecting and Ô¨Åxing security bugs
early in a software development lifecycle [34]. For example,
an expert reviewer can identify potentially vulnerable code
and help the author to Ô¨Åx the vulnerability or to abandon
the code. Peer code review can also identify attempts to
insert malicious code. Software development organizations,
both OSS and commercial, have been increasingly adopting
tools to manage the peer code review process [50]. A tool-
based code review starts, when an author creates a patch-set
(i.e. all Ô¨Åles added or modiÔ¨Åed in a single revision), along with
a description of the changes, and submits that information to a
code review tool. After reviewers are assigned, the code review
tool then notiÔ¨Åes selected reviewers regarding the incoming
request. During a review, the tools may highlight the changes
between revisions in a side-by-side display. The review tool
also facilitates communication between the reviewers and the
author in the form of review comments, which may focus on
a particular code segment or the entire patchset. By uploading
a new patchset to address the review comments, the author
1374can initiate a new review iteration. This review cycle repeats
until either the reviewers approve the change or the author
abandons. If the reviewers approve the changes, then the
author commits the patchset or asks a project committer to
integrate the patchset to the project repository.
C. Case-Control Study
Case-control studies, which are widely used in the medical
Ô¨Åeld, is a type of observational study, where subjects are
selected based on an outcome of interest, to identify factors
that may contribute to a medical condition by comparing those
with the disease or outcome (cases) against a very similar
group of subjects who do not have that disease or outcome
(controls) [31]. A case-control study is always retrospective,
since it starts with an outcome and then traces back to inves-
tigate exposures. However, it is essential that case inclusion
criteria are clearly deÔ¨Åned to ensure that all cases included in
the study are based on the same diagnostic criteria. To measure
the strength of the association between a given exposure and
an outcome of interest, researches who conduct case-control
studies usually use Odds Ratio (OR), which represents the
odds that an outcome will occur given an exposure, compared
to the odds of the outcome occurring in the absence of that
exposure [52].
Although case control studies are predominantly used in the
medical domain, other domains have also used this research
method. In the SE domain, Allodi and Massaci conducted
case-control studies to investigate vulnerability severities and
their exploits [1]. In a retrospective study, where two groups
naturally emerge based on an outcome, the case-control study
framework can provide researchers guidelines in selecting
variables, analyzing data, and reporting results. We believe that
a case-control design is appropriate for this study, since we
are conducting a retrospective study, where two groups differ
based on an outcome. In our design, each of the selected cases
is a vulnerability contributing commit forming two groups:
1) cases‚Äìvulnerabilities identiÔ¨Åed during code reviews and 2)
controls ‚Äìvulnerabilities escaped code reviews.
III. R ESEARCH METHODOLOGY
Our research methodology focused on identifying vulner-
ability contributing commits that went through code reviews
and had its‚Äô security defects either getting identiÔ¨Åed or es-
caping. In the following subsections, we detail our research
methodology.
A. Project Selection
For this study, we select the Chromium OS project for the
following Ô¨Åve reasons‚Äì (i) it is one of the most popular OSS
projects, (ii) it is a large-scale matured project containing more
than 41.7 million Source Lines of Code (SLOC) [25]. (iii)
it has been conducting tool-based code reviews for almost a
decade, (iv) it maintains security advisories2to provide regular
updates on identiÔ¨Åed security vulnerabilities, and (v) it has
2https://www.chromium.org/chromium-os/security-advisoriesbeen subject to prior studies on security vulnerabilities [14],
[30], [38], [39], [43].
B. Data Mining
The code review repositories of the Chromium OS project
is managed by Gerrit3and is publicly available at: https:
//chromium-review.googlesource.com/. We wrote a Java ap-
plication to access Gerrit‚Äôs REST API to mine all the publicly
available code reviews for the project and store the data in
a MySQL database. Overall, we mined 404,878 code review
requests spanning March 2011 to March 2019. Using an
approach similar to Bosu et al. [10], we Ô¨Åltered the bot
accounts, using a set of keywords (e.g., ‚Äòbot‚Äô, ‚ÄòCI‚Äô, ‚ÄòJenkins‚Äô,
‚Äòbuild‚Äô, ‚Äòauto‚Äô, and ‚Äòtravis‚Äô) followed by manual validations,
to exclude the comments not written by humans. To identify
whether multiple accounts belong to a single person, we follow
a similar approach as Bird et al. [8], where we use the
Levenshtein distance between two names to identify similar
names. If our manual reviews of the associated accounts
suggest that those belong to the same person, we merge those
to a single account.
C. Building a dataset of cases (i.e. vulnerabilities identiÔ¨Åed
during code reviews)
We adopted a keyword-based semi-automated mining ap-
proach, which is similar to the strategy used by Bosu et
al. [12], to build a dataset of vulnerabilities identiÔ¨Åed during
code reviews. Our keyword-based mining was based on the
following three steps:
(Step I) Database search: We queried our MySQL database
of Chromium OS code reviews to select review comments
that contain at least one of the 105 security-related keywords
(Table I). Bosu etal. [12] empirically developed and validated
a list of 52 keywords to mine code review comments asso-
ciated with the 10 common types of security vulnerabilities.
Using Bosu etal‚Äôs keyword list as our starting point, we added
additional 53 keywords to this list based on the NIST glossary
of security terms [27]. Our database search identiÔ¨Åed 7,572
code review comments that included at least one of these 105
keywords (Table I).
(Step II) Preliminary Ô¨Åltering: Two of the authors indepen-
dently audited each code review comment identiÔ¨Åed during the
database search to eliminate any reviews that clearly did not
raise a security concern. We excluded a review comment in
this step only if both auditors independently determined that
the comment does not refer to a security issue. To illustrate
the process let‚Äôs examine two code review comments with
the same keyword ‚ÄòoverÔ¨Çow‚Äô. The Ô¨Årst comment‚Äì ‚Äúno check
for overÔ¨Çow here?‚Äù potentially raises a concern regarding an
unchecked integer overÔ¨Çow and therefore was included for
a detailed inspection. While the second comment ‚Äì‚ÄúI‚Äôm not
sure but can specifying overÔ¨Çow: hidden; to a container hide
scroll bars?‚Äù seems to be related to UI rendering and was
discarded during this step. This step discarded 6,235 comments
3https://www.gerritcodereview.com/
1375TABLE I
KEYWORDS TO MINE CODE REVIEWS THAT IDENTIFY SECURITY DEFECT
Vulnerability
TypeCWE ID Keywords*
Race Condition 362 - 368 race, racy
Buffer OverÔ¨Çow 120 - 127 buffer, overÔ¨Çow, stack, strcpy, str-
cat,strtok, gets, makepath, splitpath,
heap, strlen
Integer
OverÔ¨Çow190, 191, 680 integer, overÔ¨Çow, signedness, width-
ness, underÔ¨Çow
Improper
Access22, 264, 269,
276, 281 -290improper, unauthenticated, gain ac-
cess, permission, hijack, authenti-
cate, privilege, forensic, hacker, root
Cross Site Script-
ing (XSS)79 - 87 cross site, CSS, XSS, malform,
Denial of
Service (DoS) /
Crash248, 400 - 406,
754, 755denial service, DOS, DDOS, crash
Deadlock 833 deadlock
SQL Injection 89 SQL, SQLI, injection
Format String 134 format, string, printf, scanf
Cross Site
Request Forgery352 cross site, request forgery, CSRF,
XSRF, forged
Encryption 310, 311, 320-
327encrypt, decrypt, password, cipher,
trust, checksum, nonce, salt
Common
keywords- security, vulnerability, vulnerable,
hole, exploit, attack, bypass, back-
door, threat, expose, breach, vio-
late, fatal, blacklist, overrun, inse-
cure, scare, scary, conÔ¨Çict, trojan,
Ô¨Årewall, spyware, adware, virus, ran-
som, malware, malicious, risk, dan-
gling, unsafe, leak, steal ,worm,
phishing, cve, cwe, collusion, covert,
mitm, sniffer, quarantine, scam,
spam, spoof, tamper, zombie
*Approximately half of the keywords in this list are adopted from the prior
study of Bosu et al. [12]. Keywords in italic are our additions to this list.
and retained the remaining 1,337 comments for a detailed
inspection.
(Step III) Detailed Inspection: In this step, two of the
authors independently inspected the 1,337 review comments
identiÔ¨Åed from the previous step, any subsequent discussion
included in each review, and associated code contexts to
determine whether a security defect was identiÔ¨Åed in each
review. If any vulnerability is conÔ¨Årmed, the inspectors also
classiÔ¨Åed it according to the CWE speciÔ¨Åcation [41]. Similar
to Bosu et al. [12], we considered a code change vulnerable
only if: (a) a reviewer indicated potential vulnerabilities, (b)
our manual analysis of the associated code context found the
code to be potentially vulnerable, and (c) the code author either
explicitly acknowledged the presence of the vulnerability
through a response (e.g., ‚ÄòGood catch‚Äô, ‚ÄòOops!‚Äô) or implicitly
acknowledged it by making the recommended changes in a
subsequent patch. Agreement between the two inspectors was
computed using Cohen‚Äôs Kappa () [15], which was measured
as 0.94 (almost perfect4). ConÔ¨Çicting labels were resolved
during a discussion session. At the end of this step, we
identiÔ¨Åed total 516 code reviews that successfully identiÔ¨Åed
security vulnerabilities.
4Cohen‚Äôs Kappa values are interpreted as following: 0 - 0.20 as slight, 0.21
- 0.40 as fair, 0.41 - 0.60 as moderate, 0.61 - 0.80 as substantial, and 0.81 -
1 as almost perfect agreementD. Building a dataset of controls (i.e. vulnerabilities escaped
during code reviews)
We searched the Monorail-based bug tracking system hosted
at: https://bugs.chromium.org/, to identify a list of security
defects for the Chromium OS project. We used the bug tracker
instead of the CVE database, since the bug tracker includes
more detailed information for each security defect (e.g., link
to Ô¨Åxing commit and link to Gerrit where the Ô¨Åx was code
reviewed). Moreover, some of the security defects may not be
reported in the CVE, if it was identiÔ¨Åed during testing prior
to its public release. We used the following Ô¨Åve-step approach
to build this dataset. We also illustrate this process using an
example security defect: #935175.
(Step I) Custom search: We use a custom search (i.e.,
(Type=Bug-Security status:Fixed OS=Chrome),
to Ô¨Ålter security defects for the Chromium OS projects
with the status as ‚ÄòFixed‚Äô. Our search result identiÔ¨Åed total
591 security defects. We exported the list of defects as a
comma-separated values( i.e., csv) Ô¨Åle, where each issue is
associated with a unique ID.
(Step II) Identifying vulnerability Ô¨Åxing commit:
The Monorail page for each ‚ÄòFixed‚Äô issue includes
detailed information (e.g., commitid, owner, review
URL, list of modiÔ¨Åed Ô¨Åles, and reviewer) regarding its
Ô¨Åx. For example, http://crbug.com/935175 details the
information for the security defect #935175 including
the ID of the vulnerability Ô¨Åxing commit (i.e. ‚Äò
56b512399a5c2221ba4812f5170f3f8dc352cd74‚Äô).
We wrote a Python script to automate the extraction of
the review URLs and commit ids for each security defect
identiÔ¨Åed in Step I. Finally, we excluded the security Ô¨Åxes
that were not reviewed on Chromium OS‚Äôs Gerrit repository
(e.g., third-party libraries). At the end of this step, we were
left with 239 security defects and its‚Äô corresponding Ô¨Åxes.
(Step III) Identifying vulnerability contributing commit(s):
We adopted the modiÔ¨Åed version of the SZZ algorithm [9]
to identify the vulnerability introducing commits from the
vulnerability Ô¨Åxing commits identiÔ¨Åed in Step II. Our modiÔ¨Åed
SZZ algorithm uses the git blame andgit bisect
subcommands and is adopted based on the approaches fol-
lowed in two prior studies [38], [49] on VCCs. For each line
in a given Ô¨Åle, the git blame subcommand names the
commit that last commitid that modiÔ¨Åed it. The heuristics
behind our custom SZZ are as following:
1) Ignore changes in documentations such as release notes
or change logs.
2) For each deleted / modiÔ¨Åed, blame the line that was
deleted / modiÔ¨Åed, since if a Ô¨Åx needed to change a line,
that often means that it was part of the vulnerability.
3) For every continuous block of code inserted in the bug
Ô¨Åxing commit, blame the lines before and after the block,
since security Ô¨Åxes are often done by adding extra checks,
often right before an access or after a function call.
4) If multiple commits are marked based on the above steps,
mark commits as VCCs based on higher amount of lines
1376Fig. 1. A vulnerability contributing commit (VCC) for the security defect #935175 and the code review that missed it
TABLE II
ATTRIBUTES OF A CODE REVIEW THAT MAY INFLUENCE IDENTIFICATION OF A VULNERABILITY
Type Name DeÔ¨Ånition Rationale
LocationNumber of Ô¨Åles under re-
viewNumber of Ô¨Åles under review in a review request. Changes that involve greater number of Ô¨Åles are more likely
to be defect-prone [28], yet more time-consuming to review.
Number of directory un-
der reviewNumber of directory where Ô¨Åles have been modiÔ¨Åed in a
review request.Ifthe developers group multiple separate changes into a single
commit, the review of those comprehensive changes could be
harder.
Code churn Number of lines added / modiÔ¨Åed / deleted in a code
review.Larger changes are more likely to have vulnerability [12],
[45], [46] and require more time to comprehend.
Lines of code Numbers of lines of code in the Ô¨Åle before Ô¨Åx. Larger components are more difÔ¨Åcult to understand.
Comple xity McCabe‚Äô s Cyclomatic Complexity [33]. DifÔ¨Åculty to comprehend a Ô¨Åle increases with its cyclomatic
complexity.
isbugÔ¨Åx Code review request that is submitted to Ô¨Åx a bug Abug Ô¨Åx review request may draw additional attention from
the reviewers, as bugs often foreshadow vulnerabilities [14]
ParticipantAuthor‚Äô s coding experi-
enceNumber of code commits the author has submitted (i.e.,
both accepted and rejected) prior to this commit.Experienced authors‚Äô code changes may be subject to less
scrutiny and therefore may miss vulnerabilities during re-
views.
Reviewer‚Äôs reviewing ex-
perienceNumber of code reviews that a developer has participated
as a reviewer (i.e., code not committed by him/her) prior
to this commit.Experienced reviewers may be more likely to spot security
concerns.
Reviewer‚Äôs coding experi-
enceNumber of code commits that a reviewer has submitted
(i.e., both accepted and rejected) prior to this commit.Experienced developers provide more useful feedback during
code review [13] and may have more security knowledge.
Review processReview time The time from the beginning to the end of the review
process. We deÔ¨Åne the review process to be complete
when the patchset is ‚ÄòMerged‚Äô to the main project branch
or is ‚ÄòAbandoned‚Äô.Acursory code review is more likely to miss security defects
that require thorough reviews.
Number of reviewers in-
volved (NR f)Number of reviewers involved in reviewing Ô¨Åle f AsLinus‚Äôs law suggest, the more eyeballs, the less likelihood
of a defect remaining unnoticed.
HistoricalReview ratio (RRa;f) The ratio between the number of prior reviews from
developerato a Ô¨Ålefand the total number of prior
reviews to that Ô¨Åle. If the developer aparticipated in i
of therprior reviews in Ô¨Åle fthen:RRa;f=i
rAdeveloper who has reviewed a particular Ô¨Åle more may
have better understanding of its design.
Commit ratio (CRa;f) The ratio between the number of commits to a Ô¨Åle fby
authoraand the total number of commits to that Ô¨Åle. If
authoramakesiof thecprior commits then CRa;f=
i
cAdeveloper who makes frequent changes in a Ô¨Åle may have
better understanding of its design
Weighted recent commits
(RCa;f)Ifa Ô¨Ålefhas totalnprior commits and author a
makes three of three of the prior n commits (e.g., i;j;k ),
where n denotes the latest commit, then: RCa;f =
(i+j+k)
(1+2+3+:::+n)=2(i+j +k)
n(n+1)Adeveloper who makes recent commits may have better
understanding about the current design.
Total commit Total number of commits made on the current Ô¨Åle Files that have too many prior commits might require extra
attention from the reviewers.
Mutual reviews Number of reviews performed by the current reviewer
and authorBetter understanding about the author‚Äôs coding style might
help the reviewer to investigate defects.
Number of review com-
mentsTotal number of review comments in the current Ô¨Åle Higher number of review comments indicate the Ô¨Åle has gone
through a more detailed review.
File ownership (FOa;f) The ratio between the number of lines modiÔ¨Åed by
a developer and total number of lines in that Ô¨Åle. If
developerawritesiof totalnlines in Ô¨Ålef, thenFOa;f
=inThe owner of a Ô¨Åle may be better suited to review that Ô¨Åle.
1377until at least 80% lines are accounted for.
We manually inspect each of the VCCs identiÔ¨Åed by our
modiÔ¨Åed SZZ algorithm as well as corresponding vulnerability
Ô¨Åxing commits to exclude unrelated commits or include addi-
tional relevant commits. At the end of this step, we identiÔ¨Åed
total 374 VCCs. Figure 1 shows a VCC for the security defect
#935175 identiÔ¨Åed through this process.
(Step IV) Identifying code reviews that approved VCCs: A
git repository mirror for the Chromium OS project is hosted
at https://chromium.googlesource.com/ with a gitiles5based
frontend. We used the REST API of gitiles to query this
repository to download commit logs for each VCC identiÔ¨Åed in
the previous step. Using a REGEX parser, we extract the URLs
of the code review requests that approved VCCs identiÔ¨Åed in
Step III. For example, Figure 1 also includes the URL of the
code review that missed the security defect #935175. At the
end of this step, we identiÔ¨Åed total 374 code reviews that
approved our list of VCCs.
(Step V) CWE classiÔ¨Åcation of the VCCs: 124 out of the
374 VCCs in our dataset had a CVE reported in the NIST
NVD database6For example CVE-2019-5794 corresponds
to the security defect #935175. For such VCCs, we ob-
tained the CWE classiÔ¨Åcation from the NVD database. For
example, NVD classiÔ¨Åes #935175 as a ‚ÄòCWE-20: Improper
Input Validation‚Äô. For the remaining 250 VCCs, two of the
authors independently inspected each VCC as well as its Ô¨Åxing
commits to understand the coding mistake and classify it
according to the CWE speciÔ¨Åcation. ConÔ¨Çicting labels were
resolved through discussions.
E. Attribute Collection
To answer the research questions motivating this study, we
computed 18 attributes for each of the 890 code reviews (i.e.
516 cases + 374 controls). Majority of the attributes selected
in this study have been also used in prior studies investigating
the relationship between software quality and code review
attributes [28], [29], [36], [55]. Table II presents the list of
our attributes with a brief description and rationale behind
the inclusion of each attribute to investigate our research
objectives. Those attributes are grouped into four categories: 1)
vulnerability location, 2) participant characteristics, 3) review
process, 4) historical measures. We use several Python scripts
and SQL queries to calculate those attributes from our curated
dataset and our MySQL database of Chromium OS code
reviews.
IV. R ESULTS
Following subsections detail the results of the two research
question introduced in the Section I based on our analyses of
the collected dataset.
5https://gerrit.googlesource.com/gitiles/
6https://nvd.nist.gov/A. RQ1: Which categories of security defects are more likely
to be missed during code reviews?
For both identiÔ¨Åed and escaped security defects cases, we
either obtained a CWE classiÔ¨Åcation from the NVD database
or manually assign one for those without any NVD reference.
The 890 VCCs (i.e., both identiÔ¨Åed and escaped cases) in our
dataset represented 86 categories of CWEs. However, for the
simplicity of our analysis, we decreased the number of distinct
CWE categories by combining similar categories of CWEs
into a higher level category. The CWE speciÔ¨Åcation already
provides a hierarchical categorization scheme7to represent the
relationship between different categories of weaknesses. For
example, both CWE-190 (Integer OverÔ¨Çow or Wraparound)
and CWE-468 (Incorrect Pointer Scaling) belong to the higher
level category: CWE-682 (Incorrect Calculation). Using the
higher level categories from the CWE speciÔ¨Åcation [41], we
reduce the number of distinct CWE types in our dataset to
15. During this higher level classiÔ¨Åcation, we also ensured
no common descendants among these Ô¨Ånal 15 categories.
Table III shows the Ô¨Åfteen CWE categories represented in
our dataset, their deÔ¨Ånitions, and both the number and ratios
of identiÔ¨Åed /escaped cases, in a descending order based on
their total number of appearances.
The results of a Chi-Square (2) test suggest that some
categories of CWEs are signiÔ¨Åcantly (2=491.69,p value<
0:001) more likely to remain undetected during code reviews
than the others. Chromium OS reviewers were the most
efÔ¨Åcient in identifying security defects due to ‚ÄòCWE-676:
Use of potentially dangerous function‚Äô. For example, follow-
ing C functions are strcpy(), strcat(), strlen(),
strcmp(), sprintf() unsafe as they do not check for
buffer length and may overwrite memory zone adjacent to the
intended destination. As the identiÔ¨Åcation of a CWE-676 is
relatively simple and does not require much understanding of
the associated context, no occurrences of dangerous functions
escaped code reviews. Reviewers were also highly efÔ¨Åcient
in identifying security defects due to ‚ÄòCWE-404: Improper re-
source shut down or release‚Äô that can lead to resource leakage.
‚ÄòCWE 682: Incorrect calculation‚Äô, which includes calculation
of buffer size and unsecured mathematical operation (i.e., large
addition/multiplication or divide by zero), were also more
likely to be identiÔ¨Åed during code reviews (80%). The other
categories of CWEs that were more likely to be identiÔ¨Åed
during code reviews include: improper exception handling
(CWE-703) and synchronization mistakes (i.e., CWE- 662,
and CWE-362).
On the other hand, Chromium OS reviewers were the
least effective in identifying security defects due to insuf-
Ô¨Åcient veriÔ¨Åcation of data authenticity (CWE-345), as all
such occurrences remained undetected. InsufÔ¨Åcient veriÔ¨Åcation
of data can lead to an application accepting invalid data.
Although Improper input validations (CWE-20) were frequent
occurrences (i.e., 72), those remained undetected during 88%
code reviews. Improper input validation can lead to many
7https://cwe.mitre.org/data/graphs/1000.html
1378critical problems such as uncontrolled memory allocation and
SQL injection. Approximately 88% security defects caused
by improper access control (CWE-284) also remained unde-
tected as reviewers were less effective in identifying security
issues due to improper authorization and authentication, and
improper user management. The other categories of CWEs that
were more likely to remain unidentiÔ¨Åed during code reviews
include: operation on a resource after expiration or release
(CWE-672) and exposure of resources to wrong spheres
(CWE-668).
Our manual examinations of the characteristics of these
CWE categories suggest that security defects that can be
identiÔ¨Åed based on a few lines of the code context (e.g.,
unsafe method, calculation of buffer size, and resource release)
are more likely to be identiÔ¨Åed during code reviews. On
the other hand, Chromium OS reviewers were more likely
to miss CWEs requiring either code execution (e.g., input
validation) or understanding of larger code contexts (e.g.,
resource lifetime, and authentication management ).
Finding 1: The likelihood of a security defect‚Äôs identiÔ¨Åca-
tion during code reviews depends on its CWE category.
Observation 1(A): Security defects related to the synchro-
nization of multi-threaded application, calculation of vari-
able and buffer size, exception handling, resource release,
and usage of prohibited functions were more likely to be
identiÔ¨Åed during code reviews.
Observation 1(B): Security defects related to the user input
neutralization, access control, authorization and authentica-
tion management, resource lifetime, information exposure,
and datatype conversion were more likely to remain unde-
tected during code reviews.
B. RQ2: Which factors inÔ¨Çuence the identiÔ¨Åcation of security
defects during a code review?
To investigate this research question, we developed a lo-
gistic regression model. Logistic regression is very efÔ¨Åcient
in predicting a binary response variable based one or more
explanatory variables [7]. In this study, we use the factors
described in the Table II as our explanatory variables, while
our response variable is a boolean that is set to TRUE, if a
code review‚Äôs security defect was identiÔ¨Åed by reviewer(s) and
FALSE otherwise.
Being motivated by recent Software Engineering studies
[37], [55], we adopt the model construction and analysis ap-
proach of Harrell Jr. [22], which allows us to model nonlinear
relationships between the dependent and explanatory variables
more accurately. The following subsections describe our model
construction and evaluation steps.
1) Correlation and Redundancy Analysis: If the explana-
tory variables to construct a model are highly correlated with
each other, they can generate a overÔ¨Åtted model. Following,
Sarle‚Äôs V ARCLUS (Variable Clustering) procedure [51], we
use the Spearman‚Äôs rank-order correlation test () [54] to
LinesOfCode
TotalCommit
ReviewRatio
FileOwnership
CommitRatio
WeightedRecentCommit
IsBugFix
ReviewerReviewingExperience
ReviewerCodingExperience
AuthorCodingExperience
MutualReviews
Complexity
NumOfDirectoryUnderReview
CodeChurn
NumOfFilesUnderReview
ReviewTime
NumOfReviewComments
NumOfReviewersInvolved1.0 0.6 0.2Spearman œÅFig. 2. Hierarchical clustering of explanatory variables according to Spear-
man‚Äôs jjand Sarle‚Äôs V ARCLUS. The dashed line indicates the high correla-
tion coefÔ¨Åcient threshold (jj = 0:7)
determine highly correlated explanatory variables and con-
struct a hierarchical representation of the variable clusters
(Figure 2). We retain only one variable from each cluster of
highly correlated explanatory variables. We use j0:7j as
our threshold, since it has been recommended as the threshold
for high correlation [24] and has been used as the threshold
in prior SE studies [37], [55].
We found four clusters of explanatory variables that have
jj>0:7‚Äì (1) total lines of code (totalLOC ) and number
of commits (totalCommit ), (2) commit ratio, weighted recent
commit, and Ô¨Åle ownership, (3) reviewer‚Äôs coding experience
and reviewer‚Äôs reviewing experience, (4) amount of code
churn, directory under review, and number of Ô¨Åles under
review. From the Ô¨Årst cluster, we select total number of commit
in the Ô¨Åle. From the second cluster, we select Ô¨Åle ownership.
From the third and fourth cluster, we select reviewer‚Äôs review-
ing experience and number of directory under review respec-
tively. Despite not being highly correlated, some explanatory
variables can still be redundant. Since redundant variables
can affect the modelled relationship between explanatory and
response variables, we use the redun function of the rms R
package with the threshold R20:9[19] to identify potential
redundant factors among the remaining 12 variables and found
none.
2) Degrees of Freedom Allocation: A model may be overÔ¨Åt-
ted, if we allocate degrees of freedom more than a dataset can
support (i.e., number of explanatory variables that the dataset
can support). To minimize this risk, we estimate the budget
for degrees of freedom allocation before Ô¨Åtting our model.
As suggested by Harrell Jr. [22], we consider the budget for
degrees of freedom to bemin(T;F ) 15, where T represents the
number of rows in the dataset where the response variable
is set to TRUE and F represents the number of rows in the
dataset where the response variable is set to FALSE. Using this
formula, we compute our budget for degrees of freedom = 24,
since our dataset has 516 TRUE instances and 374 FALSE
instances.
For maximum effectiveness, we allocate this budget among
1379TABLE III
DISTRIBUTION OF CHROMIUM OS CWE S IDENTIFIED /ESCAPED CODE REVIEWS
CWE ID CWE DeÔ¨Ånition #IdentiÔ¨Åed %IdentiÔ¨Åed #Escaped %Escaped
662 Improper Synchronization 109 89.34 13 10.66
362 Concurrent Execution using Shared Resource with Improper Synchronization
(‚ÄôRace Condition‚Äô)73 87.95 10 12.05
682 Incorrect Calculation 65 79.27 17 20.73
20 Improper Input Validation 9 11.11 72 88.89
703 Improper Check or Handling of Exceptional Conditions 58 72.50 22 27.50
404 Improper Resource Shutdown or Release 71 94.67 4 5.33
284 Improper Access Control 8 12.12 58 87.88
672 Operation on a Resource after Expiration or Release 3 4.62 62 95.38
119 Improper Restriction of Operations within the Bounds of a Memory Buffer 42 72.41 16 27.59
676 Use of Potentially Dangerous Function 53 100.0 0 0.0
668 Exposure of Resource to Wrong Sphere 15 30.0 35 70.0
704 Incorrect Type Conversion or Cast 1 5.88 16 94.12
345 Insuf Ô¨Åcient VeriÔ¨Åcation of Data Authenticity 0 0.0 13 100.0
665 Improper Initialization 5 50.0 5 50.0
19 Data Processing Errors 0 0.0 10 100.0
all the survived explanatory variables in such a way that the
variables that have more explanatory powers (i.e., explana-
tory variables that have more potential for sharing nonlinear
relationship with the response variable) to be allocated with
higher degrees of freedom than the explanatory variables
that have less explanatory powers. To measure this potential,
we compute Spearman rank correlations (2) between the
dependent variable and each of the 12 surviving explanatory
variables (Figure 3). Based on the results of this analysis, we
split the explanatory variables into two groups‚Äì (1) we allocate
three degrees of freedom to three variables, i.e., number of
directory under review, review ratio, and reviewer‚Äôs reviewing
experience, and (2) we allocate one degree of freedom for
the remaining nine variables. Although isBugFix has higher
potential than reviewer‚Äôs reviewing experience, we cannot
assign more than one degree of freedom for isBugFix as it
is dichotomous. As suggested by Harrell Jr. [22], we limit
the maximum allocated degree of freedom for an explanatory
variable below Ô¨Åve to minimize the risk of overÔ¨Åtting.
3) Logistic Regression Model Construction: After elimi-
nating highly correlated explanatory variables and allocating
appropriate degrees of freedom to the surviving explanatory
variables, we Ô¨Åt a logistic regression model using our dataset.
We use the rcs function of the rms R package [19] to Ô¨Åt
the allocated degrees of freedom to the explanatory variables.
4) Model Analysis: After model construction, we analyze
the Ô¨Åtted model to identify the relationship between the
response variable and each of the explanatory variables. We
describe each step of our model analysis in the following.
Assessment of explanatory ability and model stability: To
assess the performance of our model, we use Area Under
the Receiver Operating Characteristic (AUC) curve [21]. Our
model achieves an AUC of 0.914. To estimate how well the
model Ô¨Åts our dataset, we calculate Nagelkerke‚Äôs Pseudo R2[47]8. Our model achieves a R2value of 0:6375, which is
considered to be a good Ô¨Åt [47].
Power of explanatory variables estimation: We use the
Wald statistics (Wald 2) to estimate the impact of each
explanatory variable on the performance our model. We use
theanova function of the rms R package to estimate the
relative contribution (Wald 2) and statistical signiÔ¨Åcance (p)
of each explanatory variable to the model. The larger the
Wald2value is, the more explanatory power the variable
wields on our model. The results of our Wald 2tests (Table
IV) suggest that number of directory under review wields the
highest predictive power on the Ô¨Åtted model. ReviewRatio,
isBugFix, ReviewerReviewingExperience, and TotalCommit are
the next four most signiÔ¨Åcant contributors. Number of review
comments, number of mutual reviews, cyclomatic complexity
of the Ô¨Åle, and review time also wield signiÔ¨Åcant explana-
tory powers. However, experience of code author, number of
reviewers involved in the review process, and proportion of
ownership of the Ô¨Åle do not contribute signiÔ¨Åcantly on the
Ô¨Åtted model.
We use the summary function of the RMS R package to
analyze our model Ô¨Åt summary. Table IV also shows the
contributions of each explanatory variable to Ô¨Åt our model
using the ‚Äòdeviance reduced by‚Äô values. For a generalized
linear model, deviance can be used to estimate goodness /
badness of Ô¨Åt. A higher value of residual deviance indicates
worse Ô¨Åt and a lower value indicates the opposite. A model
with a perfect Ô¨Åt would have zero residual deviance. The
NULL deviance value, which indicates how well the response
variable is predicted by a model that includes only one
intercept (i.e., the grand mean), is estimated as 1211:05 for
our dataset. The deviance of a Ô¨Åtted model decreases once
we add explanatory variables. This decrement of residual
8For Ordinary Least Square (OLS) regressions, Adjusted R2is used
to measure a model‚Äôs goodness of Ô¨Åt. Since it is difÔ¨Åcult to compute
AdjustedR2for a logistic regression model, the PseudoR2is commonly
used to measure its goodness of Ô¨Åt. The advantage of using Nagelkerke‚Äôs
PseudoR2is that it‚Äôs range is similar to the AdjustedR2range used for
OLS regressions [53].
1380deviance would higher for a variable with higher predictive
power than for a variable with lower predictive power. For
example, the explanatory variable ‚Äúdirectory under review‚Äù,
which has the highest predictive power, reduces the residual
deviance by 195.701 with a loss of three degrees of freedom.
We can imply that the variable ‚Äúdirectory under review‚Äù adds
195:701
1211:05100% = 16:16% explanatory power to Ô¨Åt the
model. Similarly, ‚Äúreviewers experience‚Äù reduces the residual
deviance by 104.104 with a loss of three degrees of freedom.
Therefore, ‚Äúreviewers experience‚Äù adds104:104
1211:05100% =
8:6% explanatory power to Ô¨Åt the model. Overall, our ex-
planatory variables decrease the deviance by 571:73 with a
loss of 17 degrees of freedom. Hence, we can imply that
our explanatory variables add571:74
1211:05100% = 47:21%
explanatory power to Ô¨Åt the model which can be considered
as a signiÔ¨Åcant improvement over the null model.
Examination of variables in relation to response: Since
Odds Ratio (OR) is recommended to measure the strength
of relationship between an explanatory variable and the out-
come [52], we compute the OR of each explanatory variable
in our Ô¨Åtted model (Table IV) using 95% conÔ¨Ådence interval.
In this study, the OR of an explanatory variable implies how
the probability of getting a true outcome (i.e., a vulnerabil-
ity getting identiÔ¨Åed) increases with a unit change of that
variable. Therefore, an explanatory variable with OR > 1
would increase the probability of a security defect getting
identiÔ¨Åed during code reviews with its increment and vice
versa. Since the explanatory variables used in our model have
varying ranges (i.e., while ‚Äònumber of directory under review‚Äô
varies between from 1 to 10, the ‚Äòreviewing experience‚Äô
varies between 0 to several hundreds), we cannot draw a
generic conclusion by comparing the numeric OR value of an
explanatory variable against the OR of another variable that
has a different range.
Table IV shows that the OR of ‚Äònumber of directory under
review‚Äô is 0.76 (i.e. <1), indicating that, if the ‚Äònumber of
directory under review‚Äô increases, a code review is more likely
to miss a security defect. On the other hand, the odds ratio
of the variables ReviewRatio andIsBugFix are well above 1,
which imply that if the review request is marked as a bug
Ô¨Åx commit or the review conducts a signiÔ¨Åcant number of
prior review to that Ô¨Åle, the security defect is more likely
to be identiÔ¨Åed during code review. Since IsBugFix is a
dichotomous variable, interpretation of its OR value (4.55)
is straightforward. It indicates that vulnerabilities in a bug
Ô¨Åx commit were 4.55 times more likely to be identiÔ¨Åed
during code reviews than a non-bug Ô¨Åx commit. The results
also suggest positive impact of review time on vulnerability
identiÔ¨Åcation. Surprisingly, the overall reviewing experience
of a developer does not increase his/her ability to identify
security defects.
NumOfDirectoryUnderReview
ReviewRatio
IsBugFix
ReviewerReviewingExperience
TotalCommit
AuthorCodingExperience
NumOfReviewComments
MutualReviews
NumOfReviewersInvolved
Complexity
ReviewTime
FileOwnership
0.05 0.10 0.15 0.20Spearman  œÅ2    Response :is_identified
Adjusted  œÅ2Fig. 3. Dotplot of the Spearman multiple 2of each explanatory variable and
the response. The larger values of 2indicate higher potential for a nonlinear
relationship.
Finding 2: Our model has identiÔ¨Åed nine code review
factors that signiÔ¨Åcantly differ between code reviews that
successfully identiÔ¨Åed security defects and those failed.
Number of directories impacted by a code change has the
most predictive power among those nine factors.
Observation 2(A): The probability of a vulnerability getting
identiÔ¨Åed during a code review decreases with the increase
in number of directories, number of prior reviews by the
reviewer, number of prior commits in the Ô¨Åle, and number
of review comments authored on a Ô¨Åle during the current
review cycle.
Observation 2(B): The probability of a vulnerability getting
identiÔ¨Åed during a code review increases with review time,
number of mutual reviews between the code author and a
reviewer, cyclomatic complexity of the Ô¨Åle under review, if
the change belongs to a bug Ô¨Åx, and a reviewer‚Äôs number
of priors review with the Ô¨Åle.
V. I MPLICATIONS
In this section, we describe the implications of our Ô¨Åndings.
A. Findings from RQ1
Table III suggests that reviewers detect CWE-662 and
CWE-362 in most of the cases. Both of the CWEs are related
to the security issues for multi-threaded applications (Improper
synchronization and race condition). Hence, we can infer that
Chromium OS developers have adequate expertise in securing
multi-threaded programs. Developers are also detecting issues
related to improper calculation of array buffer or variable size
in most of the cases which can overcome the possibility of
potential buffer and/or integer overÔ¨Çow/underÔ¨Çow. However,
identifying security issues with user input sanitization remains
a concern. Most of the issues related to improper input vali-
dation have been escaped during code review which can lead
to security vulnerabilities such as SQL injection attack, cross-
site scripting attach, and IDN holograph attack. Chromium
OS project manager may tackle this problem in two possible
ways. First, they may leverage additional quality assurance
1381TABLE IV
EXPLANATORY POWERS OF THE CODE REVIEW ATTRIBUTES TO PREDICT THE LIKELIHOOD OF A VULNERABILITY TO BE IDENTIFIED DURING CODE
REVIEWS
Allocated D.F. Deviance Residual Deviance Deviance Reduced By (%) Odds Ratio Pr(>Chi)
NULL 1211.05
NumOfDirectoryUnderRe view 3 195.70 1015.35 16.16 0.76<0.001***
ReviewerReviewingExperience 3 104.10 911.25 8.60 0.99<0.001***
ReviewRatio 3 85.49 825.76 7.06 1.83<0.001***
IsBugFix 1 67.14 758.62 5.54 4.55<0.001***
TotalCommit 1 40.03 718.59 3.30 0.97<0.001***
NumOfRe viewComments 1 26.56 692.03 2.19 0.98<0.001***
ReviewTime 1 23.86 668.17 1.97 1.01<0.001***
MutualRe views 1 14.95 653.22 1.23 1.01<0.001***
Comple xity 1 12.17 641.05 1.01 1.12<0.001***
AuthorCodingExperience 1 0.95 640.10 0.08 0.99 0.33
FileOwnership 1 0.60 639.50 0.05 1.52 0.44
NumOfRe viewersInvolved 1 0.18 639.32 0.02 1.04 0.67
Total 18 571.73 47.21%
Statistical signiÔ¨Åcance of explanatory power according to Wald 2likelihood ratio test:
* p<0.05; ** p <0.01; *** p <0.001;
practices, such as static analysis, fuzzy testing that are known
to be effective in identifying these categories of vulnerabilities.
Second, education / training materials may be provided to
reviewers to improve their knowledge regard these CWEs.
B. Findings from RQ2
Herzig and Zeller Ô¨Ånd that when developers commit loosely
related code changes that affect all related modules, the like-
lihood of introducing bug increases [23]. Such code changes
are termed as tangled code changes. Our study also Ô¨Ånds that
if the code change affects multiple directories, the security
defect is more likely to escape code review. Reviewing tan-
gled code changes can be challenging due to difÔ¨Åculties in
comprehension. To tackle this issue we recommend: 1) trying
to avoid code changes dispersed across a large number of
directories, when possible, 2) spending additional time during
such changes as our results also suggest positive impact of
review time on vulnerability identiÔ¨Åcation, and 3) integrate a
tool, such as the one proposed by Barnett et al. [4] to help
reviewers navigate tangled code changes.
Our results also suggest that Chromium OS reviewers,
who have participated in higher number of code reviews for
were less likely to identify security defects. There may be
several possible explanations for this result. First, developers
who participates in a large number of reviews may become
less cautious (i.e., review fatigue) and miss security defects.
Second, developers who review lot of changes may have to
spend less time per review, as code reviews are considered
as secondary responsibilities in most projects. Therefore, such
developers become less effective in identifying security defects
due to hasty reviews. Finally, identiÔ¨Åcation of security defects
may require special skillsets that do not increase a developer‚Äôs
participation in non-security code reviews. While we do not
have a deÔ¨Ånite explanation, we would recommend project
managers to be more aware of ‚Äòreview fatigue‚Äô and avoid
overburdening a person with a larger number of reviews.
The likelihood of Ô¨Åle‚Äôs vulnerability escaping increases with
the total number of commit it has encountered during itslifetime. A Ô¨Åle with higher number of commits indicates more
frequent changes in that Ô¨Åle than others due to bugs or design
changes. Since bugs often foreshadow vulnerabilities [14],
developers should be more cautious while reviewing Ô¨Åles that
frequently go through modiÔ¨Åcations.
Interestingly, if a code change is marked as a bug Ô¨Åx,
developers are more likely to identify security defects (if
exists) during code reviews, which suggests extra cautions
during such reviews. Therefore, an automated model may be
used to predict and assign tags (e.g., ‚Äòsecurity critical‚Äô) to code
changes that are more likely to include vulnerabilities to draw
reviewers‚Äô attentions and seek their cautiousness.
Unsurprisingly, the likelihood of a security defect getting
identiÔ¨Åed increases with review time (i.e., time to conclude a
review). Although, taking too much time to complete a review
would slow the development process, reviewers should make a
trade-off between time and careful inspection, and try to avoid
rushing reviews of security critical changes. The number of
mutual reviews between a pair of developers also has a positive
effect on the likelihood of security defect identiÔ¨Åcation. When
two developers review each other‚Äôs code over a period of
time, they become more aware of each other‚Äôs coding styles,
expertise, strengths, and weaknesses. That awareness might
help one to pay attention to areas that he/she thinks the
other has a weakness or where he/she may make a mistake.
Since mutual reviews have positive impact, we recommend
promoting such relationships.
VI. T HREATS TO VALIDITY
Since case control studies originate from the medical
domain,one may question whether we can use this study
framework to study SE research questions. We would like to
point out that prior SE studies have adopted various research
designs, such as systematic literature review, controlled ex-
periment, ethnography, and focus group that have originated
in other research domains. Although, the results of this study
do not rely on the case-control study framework, we decided
to use this design, since: 1) our study satisÔ¨Åes the criteria
1382for using this framework, and 2) following a established
methodological framework strengthens an empirical research
such as this study.
Our keyword-based mining technique to identify whether a
code review identiÔ¨Åes security defect or not poses a threat to
validity. We may miss a security defect if the review comments
do not contain any of the keywords that we used. However, as
we are only considering those reviews that belong to security
defects and ignoring the rest, we are considering that false-
negative labeling of security defect will not make any impact
on our study. Nevertheless, as we manually check all the
security defects while assigning CWE ID, we Ô¨Ånd no false-
positive labelling a code review as related to security defect.
Another threat to is the categorization of CWE ID. As one
of our authors manually checks all the codes to Ô¨Ånd weakness
type and assign the best match CWE ID for each weakness,
that author might categorize a weakness with a wrong or less
suited CWE ID. To minimize the effect of this threat, another
author randomly chooses 200 source code Ô¨Åles and manually
assign CWE ID following a similar process without knowing
the previously labeled CWE ID. We Ô¨Ånd that 194 out of 200
labels fall in the same group of CWE IDs that were labeled
earlier. So, we are considering that this threat will not make
any signiÔ¨Åcant change in our results.
Another threat is the measure we take to calculate the
developer‚Äôs experience. We can interpret the term ‚Äúexperience‚Äù
in many ways. And in many ways, measuring of experience
will be complex. For example, we cannot calculate the amount
of contribution of a developer to other projects. Although a
different experience measure may produce different results,
we believe our interpretation of experience is reasonable as
that reÔ¨Çects the amount of familiarity with current project.
Finally, results based on a single project or even a handful
of projects can be subject to lack of external validity. Given
the manual work involved in the data collection process, it
is often infeasible to include multiple projects. Moreover,
historical evidence provides several examples of individual
cases that contributed to discovery in physics, economics, and
social science (see ‚ÄúFive misunderstandings about case-study
research‚Äù by Flyvjerg [20]). Even in the SE domain case
studies of the Chromium project [14], [17], Apache case study
by Mockus et al. [42], and Mozilla case study by Khomh et
al.[26] have provided important insights. To promote building
knowledge through families of experiments, as championed
by Basili [5], we have made our dataset and scripts publicly
available [48].
VII. R ELATED WORK
Code review technologies are widely used in modern soft-
ware engineering. Almost all the large scale projects have
adopted peer code review practices with the goal of improving
product quality [50]. Researchers have justiÔ¨Åed the beneÔ¨Åt
of code reviews to identify missed defects [6], [32]. Prior
studies also Ô¨Ånd that peer code review can be very effective in
identifying security vulnerability [12]. That is why developers
use 10-15% of their working hours in reviewing other‚Äôs code[11]. However, despite the popularity and evidence in support,
some researchers explore that peer code reviews are not
always performed effectively, which decelerates the software
development process [16].
Despite putting lots of efforts in code review to keep
product secured, a signiÔ¨Åcant number of security vulnerability
is reported every year and the number is ever-increasing.
Although some prior studies [6], [18] have questioned about
the effectiveness of peer code review in identifying security
vulnerabilities, they did not explore the factors that could be
responsible for this ineffectiveness. Researchers have intro-
duced several metrics of code reviews over time that can be
used to identify security vulnerability [2], [38], [39]. However,
they did not investigate the state of those attributes when code
review cannot identify security vulnerabilities.
Meneely and Williams Ô¨Ånd that the engagement of too
many developers to write a source code Ô¨Åle can make that
Ô¨Åle more likely to be vulnerable; termed that situation as ‚Äútoo
many cooks in kitchen‚Äù [40]. But, they do not explore what
characteristics of code review was responsible. Munaiah et al.
use natural language processing to get the insights from code
review that missed a vulnerability [44]. They investigate code
review comments of Chromium project and Ô¨Ånd that code re-
views that have discussions containing higher sentiment, lower
inquisitiveness, and lower syntactical complexity are more
likely to miss a vulnerability. To the best of our knowledge,
no prior study has sought to identify the difference in security
defects that are identiÔ¨Åed in code review and security defects
that are escaped. Also, no prior studies introduce attributes that
can be impactful in distinguishing code reviews where security
defects get identiÔ¨Åed and code reviews where security defects
get escaped.
VIII. CONCLUSION
In this case-control study, we empirically build two
datasets‚Äì a dataset of 516 code reviews where security defects
were successfully identiÔ¨Åed and a dataset of 374 code reviews
where security defects were escaped. The results of our
analysis suggest that the likelihood of a security defect‚Äôs iden-
tiÔ¨Åcation during code reviews depends on its CWE category.
A logistic regression model Ô¨Åtted on our dataset achieved
an AUC score of 0.91 and has identiÔ¨Åed nine code review
attributes that inÔ¨Çuence identiÔ¨Åcations of security defects.
While time to complete a review, the number of mutual
reviews between two developers, and if the review is for a
bug Ô¨Åx have positive impacts on vulnerability identiÔ¨Åcation,
opposite effects are observed from the number of directories
under review, the number of total reviews by a developer, and
the total number of prior commits for the Ô¨Åle under review.
Based on the results of this study, we recommend: 1) adopting
additional quality assurance mechanisms to identify security
defects that are difÔ¨Åcult to identify during code reviews,
2) trying to avoid tangled code changes when possible, 3)
assisting the reviewers to comprehend tangled code changes,
4) balancing review loads to avoid review fatigue, and 4)
promoting mutual reviewing relationship between developers.
1383REFERENCES
[1] L. Allodi and F. Massacci, ‚ÄúComparing vulnerability severity and
exploits using case-control studies,‚Äù ACM Transactions on Information
and System Security (TISSEC), vol. 17, no. 1, pp. 1‚Äì20, 2014.
[2] H. Alves, B. Fonseca, and N. Antunes, ‚ÄúSoftware metrics and security
vulnerabilities: dataset and exploratory study,‚Äù in 2016 12th European
Dependable Computing Conference (EDCC). IEEE, 2016, pp. 37‚Äì44.
[3] A. Bacchelli and C. Bird, ‚ÄúExpectations, outcomes, and challenges
of modern code review,‚Äù in Proceedings of the 2013 International
Conference on Software Engineering. IEEE, 2013, pp. 712‚Äì721.
[4] M. Barnett, C. Bird, J. Brunet, and S. K. Lahiri, ‚ÄúHelping developers
help themselves: Automatic decomposition of code review changesets,‚Äù
in2015 IEEE/ACM 37th IEEE International Conference on Software
Engineering, vol. 1. IEEE, 2015, pp. 134‚Äì144.
[5] V . R. Basili, F. Shull, and F. Lanubile, ‚ÄúBuilding knowledge through
families of experiments,‚Äù IEEE Transactions on Software Engineering,
vol. 25, no. 4, pp. 456‚Äì473, 1999.
[6] M. Beller, A. Bacchelli, A. Zaidman, and E. Juergens, ‚ÄúModern code
reviews in open-source projects: Which problems do they Ô¨Åx?‚Äù in Pro-
ceedings of the 11th working conference on mining software repositories,
2014, pp. 202‚Äì211.
[7] V . Bewick, L. Cheek, and J. Ball, ‚ÄúStatistics review 14: Logistic
regression,‚Äù Critical care, vol. 9, no. 1, p. 112, 2005.
[8] C. Bird, A. Gourley, P. Devanbu, M. Gertz, and A. Swaminathan, ‚ÄúMin-
ing email social networks,‚Äù in Proceedings of the 2006 international
workshop on Mining software repositories, 2006, pp. 137‚Äì143.
[9] M. Borg, O. Svensson, K. Berg, and D. Hansson, ‚ÄúSzz unleashed: an
open implementation of the szz algorithm - featuring example usage in a
study of just-in-time bug prediction for the jenkins project,‚Äù Proceedings
of the 3rd ACM SIGSOFT International Workshop on Machine Learning
Techniques for Software Quality Evaluation - MaLTeSQuE 2019, 2019.
[Online]. Available: http://dx.doi.org/10.1145/3340482.3342742
[10] A. Bosu and J. C. Carver, ‚ÄúImpact of developer reputation on code re-
view outcomes in oss projects: An empirical investigation,‚Äù in 2014 ACM
/ IEEE International Symposium on Empirical Software Engineering and
Measurement, ser. ESEM ‚Äò14, Torino, Italy, 2014, pp. 33:1‚Äì33:10.
[11] A. Bosu, J. C. Carver, C. Bird, J. Orbeck, and C. Chockley, ‚ÄúProcess
aspects and social dynamics of contemporary code review: Insights from
open source development and industrial practice at microsoft,‚Äù IEEE
Transactions on Software Engineering , vol. 43, no. 1, pp. 56‚Äì75, 2016.
[12] A. Bosu, J. C. Carver, H. Munawar, P. Hilley, and D. Janni, ‚ÄúIdentifying
the characteristics of vulnerable code changes: an empirical study,‚Äù in
Proceedings of the 22nd ACM SIGSOFT International Symposium on
Foundations of Software Engineering, 2014, pp. 257‚Äì268.
[13] A. Bosu, M. Greiler, and C. Bird, ‚ÄúCharacteristics of useful code
reviews: An empirical study at microsoft,‚Äù in 2015 IEEE/ACM 12th
Working Conference on Mining Software Repositories. IEEE, 2015,
pp. 146‚Äì156.
[14] F. Camilo, A. Meneely, and M. Nagappan, ‚ÄúDo bugs foreshadow
vulnerabilities?: a study of the chromium project,‚Äù in Proceedings of
the 12th Working Conference on Mining Software Repositories. IEEE
Press, 2015, pp. 269‚Äì279.
[15] J. Cohen, ‚ÄúA coefÔ¨Åcient of agreement for nominal scales,‚Äù Educational
and psychological measurement, vol. 20, no. 1, pp. 37‚Äì46, 1960.
[16] J. Czerwonka, M. Greiler, and J. Tilford, ‚ÄúCode reviews do not Ô¨Ånd
bugs. how the current code review best practice slows us down,‚Äù
in2015 IEEE/ACM 37th IEEE International Conference on Software
Engineering, vol. 2. IEEE, 2015, pp. 27‚Äì28.
[17] M. di Biase, M. Bruntink, and A. Bacchelli, ‚ÄúA security perspective
on code review: The case of chromium,‚Äù in 2016 IEEE 16th Interna-
tional Working Conference on Source Code Analysis and Manipulation
(SCAM). IEEE, 2016, pp. 21‚Äì30.
[18] A. Edmundson, B. Holtkamp, E. Rivera, M. Finifter, A. Mettler, and
D. Wagner, ‚ÄúAn empirical study on the effectiveness of security code
review,‚Äù in International Symposium on Engineering Secure Software
and Systems. Springer, 2013, pp. 197‚Äì212.
[19] H. FE, ‚ÄúRegression modeling strategies,‚Äù https://hbiostat.org/R/rms/,
[Online; accessed July 05, 2020].
[20] B. Flyvbjerg, ‚ÄúFive misunderstandings about case-study research,‚Äù Qual-
itative inquiry, vol. 12, no. 2, pp. 219‚Äì245, 2006.
[21] J. A. Hanley and B. J. McNeil, ‚ÄúThe meaning and use of the area under a
receiver operating characteristic (roc) curve.‚Äù Radiology, vol. 143, no. 1,
pp. 29‚Äì36, 1982.[22] F. E. Harrell Jr, Regression modeling strategies: with applications to
linear models, logistic and ordinal regression, and survival analysis.
Springer, 2015.
[23] K. Herzig and A. Zeller, ‚ÄúThe impact of tangled code changes,‚Äù in
2013 10th Working Conference on Mining Software Repositories (MSR).
IEEE, 2013, pp. 121‚Äì130.
[24] D. Hinkle, H. Jurs, and W. Wiersma, ‚ÄúApplied statistics for the behav-
ioral sciences,‚Äù 1998.
[25] B. D. O. Hub, ‚ÄúSummary of chromium os project,‚Äù https:
//www.openhub.net/p/chromiumos/analyses/latest/languagessummary,
[Online; accessed on July 05, 2020].
[26] F. Khomh, T. Dhaliwal, Y . Zou, and B. Adams, ‚ÄúDo faster releases
improve software quality?: an empirical case study of mozilla Ô¨Årefox,‚Äù
inProceedings of the 9th IEEE Working Conference on Mining Software
Repositories. IEEE Press, 2012, pp. 179‚Äì188.
[27] R. Kissel, Glossary of key information security terms. Diane Publishing,
2011.
[28] O. Kononenko, O. Baysal, L. Guerrouj, Y . Cao, and M. W. Godfrey,
‚ÄúInvestigating code review quality: Do people and participation mat-
ter?‚Äù in Proceedings of the 31st International Conference on Software
Maintenance and Evolution (ICSME). IEEE, 2015, pp. 111‚Äì120.
[29] A. Krutauz, T. Dey, P. C. Rigby, and A. Mockus, ‚ÄúDo code review
measures explain the incidence of post-release defects?‚Äù Empirical
Software Engineering, vol. 25, no. 5, pp. 3323‚Äì3356, 2020.
[30] R. Lagerstr ¬®om, C. Baldwin, A. MacCormack, D. Sturtevant, and
L. Doolan, ‚ÄúExploring the relationship between architecture coupling
and software vulnerabilities,‚Äù in International Symposium on Engineer-
ing Secure Software and Systems. Springer, 2017, pp. 53‚Äì69.
[31] S. Lewallen and P. Courtright, ‚ÄúEpidemiology in practice: case-control
studies,‚Äù Community Eye Health, vol. 11, no. 28, p. 57, 1998.
[32] M. V . M ¬®antyl ¬®a and C. Lassenius, ‚ÄúWhat types of defects are really dis-
covered in code reviews?‚Äù IEEE Transactions on Software Engineering,
vol. 35, no. 3, pp. 430‚Äì448, 2008.
[33] T. J. McCabe, ‚ÄúA complexity measure,‚Äù IEEE Transactions on software
Engineering, no. 4, pp. 308‚Äì320, 1976.
[34] G. McGraw, ‚ÄúSoftware security: building security in,‚Äù volume 1.
Addison-Wesley Professional, 2006.
[35] ‚Äî‚Äî, ‚ÄúAutomated code review tools for security,‚Äù Computer, vol. 41,
no. 12, pp. 108‚Äì111, 2008.
[36] S. McIntosh, Y . Kamei, B. Adams, and A. E. Hassan, ‚ÄúThe impact of
code review coverage and code review participation on software quality:
A case study of the qt, vtk, and itk projects,‚Äù in Proceedings of the 11th
Working Conference on Mining Software Repositories. ACM, 2014,
pp. 192‚Äì201.
[37] ‚Äî‚Äî, ‚ÄúAn empirical study of the impact of modern code review practices
on software quality,‚Äù Empirical Software Engineering, vol. 21, no. 5, pp.
2146‚Äì2189, 2016.
[38] A. Meneely, H. Srinivasan, A. Musa, A. R. Tejeda, M. Mokary,
and B. Spates, ‚ÄúWhen a patch goes bad: Exploring the properties
of vulnerability-contributing commits,‚Äù in 2013 ACM/IEEE Interna-
tional Symposium on Empirical Software Engineering and Measurement.
IEEE, 2013, pp. 65‚Äì74.
[39] A. Meneely, A. C. R. Tejeda, B. Spates, S. Trudeau, D. Neuberger,
K. Whitlock, C. Ketant, and K. Davis, ‚ÄúAn empirical investigation
of socio-technical code review metrics and security vulnerabilities,‚Äù
inProceedings of the 6th International Workshop on Social Software
Engineering, 2014, pp. 37‚Äì44.
[40] A. Meneely and L. Williams, ‚ÄúSecure open source collaboration: an em-
pirical study of linus‚Äô law,‚Äù in Proceedings of the 16th ACM conference
on Computer and communications security, 2009, pp. 453‚Äì462.
[41] Mitre Corporation, ‚ÄúCommon weakness enumeration,‚Äù http://cwe.mitre.
org/, [Online; accessed on January 13, 2020].
[42] A. Mockus, R. T. Fielding, and J. Herbsleb, ‚ÄúA case study of open
source software development: the apache server,‚Äù in Proceedings of the
22nd international conference on Software engineering . Acm, 2000,
pp. 263‚Äì272.
[43] N. Munaiah and A. Meneely, ‚ÄúVulnerability severity scoring and boun-
ties: why the disconnect?‚Äù in Proceedings of the 2nd International
Workshop on Software Analytics. ACM, 2016, pp. 8‚Äì14.
[44] N. Munaiah, B. S. Meyers, C. O. Alm, A. Meneely, P. K. Murukannaiah,
E. Prud‚Äôhommeaux, J. Wolff, and Y . Yu, ‚ÄúNatural language insights from
code reviews that missed a vulnerability,‚Äù in International Symposium on
Engineering Secure Software and Systems. Springer, 2017, pp. 70‚Äì86.
1384[45] N. Nagappan and T. Ball, ‚ÄúUse of relative code churn measures to
predict system defect density,‚Äù in Proceedings of the 27th International
Conference on Software Engineering, 2005, pp. 284‚Äì292.
[46] ‚Äî‚Äî, ‚ÄúUsing software dependencies and churn metrics to predict Ô¨Åeld
failures: An empirical case study,‚Äù in First International Symposium
on Empirical Software Engineering and Measurement (ESEM 2007).
IEEE, 2007, pp. 364‚Äì373.
[47] N. J. Nagelkerke et al., ‚ÄúA note on a general deÔ¨Ånition of the coefÔ¨Åcient
of determination,‚Äù Biometrika, vol. 78, no. 3, pp. 691‚Äì692, 1991.
[48] R. Paul, A. K. Turzo, and A. Bosu, ‚ÄúA dataset of Vulnerable Code
Changes of the Chormium OS project,‚Äù Feb. 2021. [Online]. Available:
https://doi.org/10.5281/zenodo.4539891
[49] H. Perl, S. Dechand, M. Smith, D. Arp, F. Yamaguchi, K. Rieck, S. Fahl,
and Y . Acar, ‚ÄúVccÔ¨Ånder: Finding potential vulnerabilities in open-source
projects to assist code audits,‚Äù in Proceedings of the 22nd ACM SIGSAC
Conference on Computer and Communications Security, 2015, pp. 426‚Äì437.
[50] P. C. Rigby and C. Bird, ‚ÄúConvergent contemporary software peer review
practices,‚Äù in Proceedings of the 2013 9th Joint Meeting on Foundations
of Software Engineering, 2013, pp. 202‚Äì212.
[51] W. Sarle, ‚ÄúSas/stat user‚Äôs guide: The varclus procedure. sas institute,‚Äù
Inc., Cary, NC, USA, 1990.
[52] M. S. Setia, ‚ÄúMethodology series module 2: case-control studies,‚Äù Indian
journal of dermatology, vol. 61, no. 2, p. 146, 2016.
[53] T. J. Smith and C. M. McKenna, ‚ÄúA comparison of logistic regression
pseudo r2 indices,‚Äù Multiple Linear Regression Viewpoints, vol. 39,
no. 2, pp. 17‚Äì26, 2013.
[54] L. Statistics, ‚ÄúSpearman‚Äôs rank-order correlation,‚Äù Laerd Statistics, 2013.
[55] P. Thongtanunam, S. McIntosh, A. E. Hassan, and H. Iida, ‚ÄúReview
participation in modern code review,‚Äù Empirical Software Engineering ,
vol. 22, no. 2, pp. 768‚Äì817, 2017.
1385