XAITools in the PublicSector:A CaseStudy onPredicting
Combined SewerOverflows
Nicholas Maltbie
maltbind@mail.uc.edu
Universityof Cincinnati
Cincinnati,Ohio,USANanNiu‚àó
Universityof Cincinnati
Cincinnati,Ohio,USA
nan.niu@uc.edu
MatthewVanDoren
MetropolitanSewerDistrict of GreaterCincinnati
Cincinnati,Ohio,USA
matthew.vandoren@cincinnati-oh.govReese Johnson
MetropolitanSewerDistrict of GreaterCincinnati
Cincinnati,Ohio,USA
reese.johnson@cincinnati-oh.gov
ABSTRACT
Artificial intelligence and deep learning are becoming increasingly
prevalentincontemporarysoftwaresolutions.Explainableartificial
intelligence (XAI) tools attempt to address the black box nature of
thedeeplearningmodelsandmakethemmoreunderstandableto
humans. In this work, we applythree state-of-the-art XAI tools in
a real-world case study. Our study focuses on predicting combined
sewer overflow events for a municipal wastewater treatment orga-
nization.Throughadatadriveninquiry,wecollectbothqualitative
information via stakeholder interviews and quantitative measures.
These help us assess the predictive accuracy of the XAI tools, as
well as the simplicity, soundness, and insightfulness of the pro-
duced explanations. Our results not only show the varying degrees
thattheXAItoolsmeettherequirements,butalsohighlightthat
domain experts can draw new insights from complex explanations
that maydifferfrom theirprevious expectations.
CCS CONCEPTS
¬∑Computingmethodologies ‚ÜíMachinelearning ;Neuralnet-
works;¬∑Software andits engineering ‚ÜíProcess validation .
KEYWORDS
explainability,AI, casestudy,goal-question-metric(GQM)
ACMReference Format:
Nicholas Maltbie,Nan Niu,Matthew Van Doren,and Reese Johnson. 2021.
XAIToolsinthePublicSector:ACaseStudyonPredictingCombinedSewer
Overflows.In Proceedingsofthe29thACMJointEuropeanSoftwareEngineer-
ingConferenceandSymposiumontheFoundationsofSoftwareEngineering
(ESEC/FSE ‚Äô21), August 23≈õ28, 2021, Athens, Greece. ACM, New York, NY,
USA,13pages.https://doi.org/10.1145/3468264.3468547
‚àóCorresponding author.
Permissionto make digitalor hard copies of allorpart ofthis work for personalor
classroom use is granted without fee provided that copies are not made or distributed
forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation
on the first page. Copyrights for components of this work owned by others than ACM
mustbehonored.Abstractingwithcreditispermitted.Tocopyotherwise,orrepublish,
topostonserversortoredistributetolists,requirespriorspecificpermissionand/ora
fee. Request permissions from permissions@acm.org.
ESEC/FSE ‚Äô21, August 23≈õ28,2021, Athens,Greece
¬©2021 Associationfor Computing Machinery.
ACM ISBN 978-1-4503-8562-6/21/08...$15.00
https://doi.org/10.1145/3468264.34685471 INTRODUCTION
Artificial Intelligence (AI) has become so ubiquitous that many
decisions nowadays in our daily life are shaped by it, e.g., news
feed suggestions and shopping item recommendations.To putthe
flourish of AI into perspective, Adadi and Berrada [ 2] highlight the
reportsforecastingthatfrom2017to2021,theglobalinvestment
on AI will increase from $12 billion US dollars to $52.2 billion, and
the revenues from the AI enabled industries worldwide is expected
to growfrom $480 billionto $2.59 trillion.
TheunstoppablepenetrationofAIalsoreaches intothepublic
sector.Forexample,theEuropeanCommissionenvisionsthatAI
could be used to serve citizens 24/7 in faster, more agile, and more
accessibleways[ 22].However,somepublicAIserviceshavealready
shown harmful consequences. In the U.S., for instance, AI was
used to allocate caregiver hours for people with disabilities, but
dramaticallyloweredthenumberofhoursinmultiplecaseswithout
anyexplanationormeaningfulopportunitytocontestthedecisions
madebythe proprietary algorithm [ 52].
AlthoughAImistakesareinevitable,thelackof explainability
raisessignificantconcernsfromthecitizensandpublicorganiza-
tions about AI-based decision making‚Äôs accountability, fairness,
responsibility, and transparency. Explainable artificial intelligence
(XAI) addresses these concerns by aiming to make AI more under-
standable to users,soas to increasethe users‚Äô trustandrelianceon
the AI system.
Consequently,many XAItoolsare builttoautomaticallygener-
ate explanations from deep learning models. Deep learning models
havebeenabletoachievenear-humanaccuracylevelsinvarious
typesofclassificationandpredictiontasksincludingimages,text,
speech,andvideodata[ 7].Thesedeeplearningmodelsareoften
opaqueintheirnatureandhencereferredtoas≈Çblackboxes≈ædueto
thedifficultyinunderstandinghowtheyoperate[ 20].Aninfluential
XAItoolisLIME(LocalInterpretableModel-agnosticExplanations)
proposedbyRibeiroandhiscolleagues[ 43].LIMEcanapproximate
a black box model locally in the neighborhood of any prediction
of interest. Anillustration given byRibeiro et al.[43]is that,once
a model predicts a patient has the flu, LIME shows with relative
weightsthat ≈Çsneeze≈æand ≈Çheadache≈æcontributeto this particular
predictionwhereas≈Çnofatigue≈æisevidenceagainstit.Identifyinga
fewweightedfeaturesasinLIMEisonlyonewaytoproduceexpla-
nations.Othersextractrules,visualizesaliencemaps,orimplement
othermethodologies [ 2].
1032ESEC/FSE ‚Äô21, August 23≈õ28, 2021,Athens,Greece Nicholas Maltbie, Nan Niu,MatthewVanDoren, andReese Johnson
Milleretal.[31]arguethatmostXAIresearchersarecurrently
building tools for themselves, rather than for the intended user.
Even the seminal work on LIME scored the lowest on Miller et al.‚Äôs
‚ÄòDataDriven‚Äôcriteria,becauseRibeiro etal.[43]constructedtheir
ownunderstandingofhowpeoplemightevaluateexplanationsand
recruited human subjects on Amazon Mechanical Turk to perform
theexperiments.Behavioralexperimentsconductedwithlayper-
sonsaresimplificationsof,andthereforecannotreplace,puttingthe
explanationintothereal-worldapplicationandlettingtheactual
end user(typicallyadomainexpert) test it[ 15].
To gain insights into application-grounded XAI evaluations, we
conducted a case study on how public services might exploit deep
learningtopredictcombinedseweroverflows(CSOs).Combined
sewer systems transport various sources of water from residential,
industrial, and commercial customers as well as storm runoff. A
problem with these systems is handling CSO events when the sys-
tem is overwhelmed by surges of water and the combined sewer
system is forced to discharge untreated water into the local en-
vironment. With infrastructures like sensor networks collecting
real-timewaterflowdata,alongwiththeavailabilityofcontribut-
ing sources like the rainfall data, public sewer services show keen
interestsindeeplearningtechniquesthatarecapableofoffering
high degrees ofpredictive accuracyas well as explainability.
Thispapermakestwomaincontributions.Weperformagoal-
question-metric analysis of explainability to quantitatively mea-
surethreestate-of-the-artXAItools,andweinterviewtwodomain
experts to qualitatively assess the XAI results on ‚ÄòData Driven‚Äô1
CSO predictions. Our study not only updates some commonly held
beliefs about explainability, but also emphasizes the engineering
considerations of incorporating explainability intothe entire deep
learning‚Äôsdevelopmentworkflow.Inwhatfollows,wepresentback-
groundinformationinSection 2,detailourcasedesigninSection 3,
analyze the results in Section 4, discuss our work‚Äôs implications in
Section5,anddraw someconcluding remarks inSection 6.
2 BACKGROUND
2.1 Explainable Artificial Intelligence(XAI)
andTools
InAI,thehighlevelofdifficultyforthesystemtoprovideasuitable
explanation for how it arrived at an answer is referred to as the
black box problem [ 2]. This difficulty is particularly prominent for
deep learning models, because a deep neural network trained end-
to-end can be as complex as an accurate explanation of why the
modelworks[ 17].ThecomplexitycanbeillustratedbyResNet[ 21],
which incorporates about 5 √ó107learned parameters and executes
about 1010floating point operations to classify a single image. XAI
triestodemystifytheblackboxesastheybeginmakingdecisions
previouslyentrustedtohumans.Thus,explainability√êtheability
tointerprettheinnerworkingsorthelogicofreasoningbehindthe
decision making√êhelps to achieve an AI system‚Äôs:
‚Ä¢accountability : justifyingthe decisions andactions,
‚Ä¢fairness: havingimpartialtreatment andbehavior,
1By ‚ÄòData Driven‚Äô, we mean explicitly referencing articles on explanation in social
science, and testing if the produced explanations are appropriate for the intended
users [31].‚Ä¢responsibility : answering for one‚Äôs decisions and identify-
ingerrorsorunexpectedresults, and
‚Ä¢transparency : describing, inspecting, and reproducing the
mechanismsthroughwhichthe decisions are made.
AdadiandBerrada[ 2]identified17XAItechniquesbysurveying
381 papers published between 2004 and 2018. According to the sur-
vey, most recent work done in the XAI field offers a post-hoc,local
explanation.Becauseonlyafewmodels,suchaslinearregression
or decision trees, are inherently interpretable, generating post-hoc
explanations is necessary for complex models like deep neural net-
works. Post-hoc XAI tool can therefore be applied to any classifier
orregressorthatisappropriatefortheapplicationdomain√êeven
those that are yet to be proposed [ 43]. Local explanations justify
why asingleprediction was made,whichare incontrastto global
explanations tryingto understandthe entire reasoningleading to
allpossible outcomes.
What the XAI tools do can be classified by how they emulate
the processing of the data to draw connections between the in-
puts and outputs. Gilpin et al.‚Äôs taxonomy [ 16] organizes XAI tools
by their function to (1) extract rules to summarize decisions, (2)
createasaliencemaptohighlightasmallportionofthecomputa-
tion which is most relevant, and (3) employ a simplified proxy that
behaves similarly to the original model. For instance, Ben√≠tez et
al.[5]transformeddeepneuralnetworkstofuzzyrulesthroughan
equivalence-by-approximation process, Simonyan et al.[45] pro-
ducedasaliencemapbydirectlycomputingtheinputgradient,and
Ribeiroet al.[43] used a local linear model in LIME as a simplified
proxyfor the full model.
With the increased usage of XAI techniques, evaluating their
efficacybecomesimportanttoinformpractitionersabouttooladop-
tions.Miller etal.‚Äôssurveyof23XAIpapers[ 31]showsthatrigorous
humanbehavioralexperimentsarenotcurrentlybeingundertaken.
As the verb to explain is a three-place predicate: ≈ÇSomeone ex-
plainssomethingtosomeone[ 23]≈æ,Millerandhiscolleagues[ 31]
argue that most XAI tools explain things (e.g., feature or neuron
importance) to the AI researchers but notto the intended users.
Doshi-Velez and Kim [ 15] further argue thatthe best way to show
how an XAI technique works is to evaluate the tool by consulting
domain expert grounded in the exact application task. Although
costly, the application-grounded evaluations provide direct and
strongevidence(orlackthereof)ofXAI‚Äôsfulfillmentoftherequire-
ments.
2.2 Explainability as aNon-Functional
Requirement(NFR)
In software engineering, functional requirements describe what the
system does, whereas non-functional requirements (NFRs) focus on
how well the system does it [ 10,38]. Making classifications, rec-
ommendations, and predictions are among the common functional
requirements of an AI system [ 11], and doing so in an explain-
able wayis often regardedas a non-functional concern [ 28]. Thus,
researchersconsiderexplainabilityto be an NFR.
In a survey study with 107 participants (90 from Brazil and 17
fromGermany),ChazetteandSchneider[ 9]elicitedtheparticipants‚Äô
expectations from an explanation. Chazette and Schneider‚Äôs online
1033XAIToolsin the Public Sector: A CaseStudyonPredictingCombinedSewerOverflows ESEC/FSE ‚Äô21, August 23≈õ28, 2021,Athens,Greece
questionnaireusedahypotheticalscenariowherethesurveypar-
ticipantswoulduseavehicle‚ÄôsAI-basednavigationsystemwhile
driving on a route they had traveled before; however, AI suggested
a different route than usual. Of the 103 codes analyzed from all
theresponses, 36(35%) expresseddesirein knowing whatspecific
pieceofinformationsupportedandinfluencedthesuggestion,12
(12%)wanted toknowthe howofthealgorithm‚Äôsinnerreasoning,
and 55 (53%) expressedwillingness to understand whysomething
happened (e.g., ≈Çwhy the [usual] route is not being suggested≈æ and
≈Çbenefitsofthe newroute when comparedto the usual≈æ[ 9]).
Thesurveyresultsclearlyshowthatpeople‚Äôsexplainabilityre-
quirementsaredifferent.ChazetteandSchneider[ 9]furtherpointed
outthatelicitingexplainabilityshouldalsoconsiderlawsandnorms,
culturalandcorporatevalues,domainaspects,andpracticalproject
constraintssuchastimeandbudget.TheEuropeanUnion,forin-
stance, debated about a general ≈ÇRight to Explanation≈æ [ 18] which
is partly enshrined in certain regulations [ 41]. Such policies, along
with thegloballyemergingethicsguidelines [ 25],are makingAI√ê
especiallyAI incitizenservices√êmore auditable.
NFRs may interact: the attempts to achieve one NFR can hurt or
help the achievement of another [ 34]. For example, generating a
post-hocexplanationimposesadditionalcomputationaloverhead,
possiblyhurtinganAIservice‚Äôsresponsiveness.Meanwhile,35%of
thecodesin[ 9]correspondedtotheresponsesinwhichtheusers
perceived explanations as a way to reduce obscurity due to the
moreinformationabouttheAIsystemanditsoutcomes.Yet15%
cautioned that too technical or lengthy explanations might add
more obscurity. Recognizing the trade-offs between explainability
andotherNFRsisthereforeimportantforprioritizingrequirements
andmakingdesignchoices.
Insummary,therequirementsengineeringliteraturesuggests
that explainability is an NFR, or a softgoal whose satisfaction is
a matter of degree without a clear-cut criterion [ 10]. Because ex-
plainabilityisnotatechnicalconceptbuttightlycoupledtohuman
understanding,examiningwhatXAItoolsactuallydoandwhatthey
shoulddomustbecarriedoutwithrespecttotherelevantaspectin
relevant contexts. Understanding the degree to which existing XAI
tools satisfy the explainability softgoal in an application-grounded
taskisprecisely the focusofour research.
3 CASE STUDYDESIGN
3.1 ProblemContext
Nearly860citiesandtownsacrosstheU.S.havecombinedsewer
systems,whichmanagestormwateraswellaswastewater,creating
what the U.S. Environmental Protection Agency (EPA) considers
to be the largest unaddressed risk to human health from the water
infrastructure [ 50]. According to an EPA report [ 50], about 850 bil-
lion gallons of untreated wastewater is discharged into waterways
annually in the U.S. The excess water from storms carries dust,
trash, and debris from developed regions and washes them into
the combined sewer system. When these combined sewer systems
areoverwhelmed,theywilldischargeuntreatedwastewaterinto
nearby waterways at an outfall site. This is defined as a combined
sewer overflow (CSO) event. Figure 1shows a simplified view of
the causes ofCSOevents.
domestic
wastewaterstormwater
runoff industrial
wastewater
pipemaximum conveyance
capacity 
wastewater treatmentriverFigure 1: Illustration of a CSO (combined sewer overflow)
siteandhow overflow can lead into nearby water sources.
In the U.S., the over 9,000 CSO outfall sites account for approxi-
mately5,000infectionsannually,damageshabitatsforanimalsin
wetlands,killingfishinrivers,andclosuresofrecreationalwater-
ways and beaches [ 50]. Thisproblem is not unique to the U.S., but
occurs all around the world. For example, an average of 39 million
tons of untreated wastewater is dumped into the river Thames
in London, UK annually due to the CSO events [ 13]. Even mod-
erncitieswithcombinedsewersystemssuchasShenzhen,China
have to handle mitigation of pollution into rivers from the CSO
events [47]. With increasing levels of urbanization and changes
inweatherpatternsduetoclimate change,theseproblemsare ex-
pected to become more severe and require new solutions to handle
theminthe future [ 13].
Weworkedwithawastewatertreatmentorganization,Metropol-
itanSewerDistrictofGreaterCincinnati(MSDGC),thatservicesan
operatingareaofabout300squaremiles,over850,000customers,
andover3,000milesofcombinedsewers.MSDGChassetupalarge
scalesensornetworktocollectdataandremotelyoperatetheirsys-
tem.Someof the olderoutflowsites intheir systemcanonly hold
a limited amount of water before they will overflow and cause a
CSOevent.ThecurrentpracticeofMSDGCis toreferenceweather
forecast, then alert citizens if a CSO event may occur within the
nextday.
Since MSDGC is a public service, they need to be able to jus-
tify their reasoningfor their decisions, especiallywhen their deci-
sionsaffect thesafetyofcustomers.This needfor transparencyis
why their current system of mostly relying on weather forecasts is
preferred.Theycanjustifytheirdecisionseasily,quicklyidentify
mistakes,andutilizethisinformationforfuturewarning.Ideally,
alertingcustomersearlybeforeaCSOeventoccurscanhelpkeep
their customers safe. When using weather forecasts, a warning
maybesenteverytimealargestorm isexpected.However,many
of the alerts sent are false positives leading to customers simply
ignoring them. Reducing the high false positives inpredicting the
CSOevents(≈ÇpredictingCSOs≈æforshort)isthemainreasonwhy
MSDGCisexploringdeep learningsolutions.
Drawingfrompriorexperience[ 6,36],wedesignedanexploratory
case study [ 55] to investigate the use of deep learning and XAI
tools to predict the CSOs within the real-life context of MSDGC.
Inparticular,weworkedwithtwodomainexpertsfromMSDGC:
1034ESEC/FSE ‚Äô21, August 23≈õ28, 2021,Athens,Greece Nicholas Maltbie, Nan Niu,MatthewVanDoren, andReese Johnson
Table 1: Sample of rainfall data which is sampled every
minutefromasensorandreturnsthedepthofrainfallmea-
sured inan area upstreamoftheCSOoutfall site.
Timestamp Rainfall (inches)
Oct12, 201814:29 0.0006
Oct12, 201814:30 0.0006
Oct12, 201814:31 0.0006
Oct12, 201814:32 0.0006
Oct12, 201814:33 0.0015
Table 2: Sample of level, velocity, and flow data from the
manholesiteupstreamoftheoutflowsensor.Thisdatawas
sampled at arate ofonceevery 5 minutes.
Timestamp LevelVelocity Flow
Aug 20, 20192:15:00 1.706 0.920 0.066
Aug 20, 20192:20:00 1.673 0.861 0.060
Aug 20, 20192:25:00 1.648 0.789 0.054
Aug 20, 20192:30:00 1.634 0.753 0.051
Aug 20, 20192:35:00 1.618 0.779 0.052
a hydrologist and an operational manager. They were points of
contactforourcasestudyandweretheemployeesthatmanaged
thedata.Theyarerepresentativeofourstakeholdersduetotheir
organizationalrolesandworkingexperiences.Wecommunicated
viaemailsaswellasonlinemeetingsanddiscussionsthroughout
the course of the case study due to restrictions relating to COVID-
19. These meetings were conducted in an informal interview style
where we co-designed the goal-question-metric framework2with
thedomainexpertsandwepresentedresultsandmaderevisions
as needed. When presenting results of the XAI tools to the domain
experts,weusedthevisualsandinteractiveelementsprovideddi-
rectly from the selected XAI tools to observe how well the tool
could provide insight to domain experts more familiar with the
data butnot familiarwith AI research.Our workseeks to provide
some examples of successes and problems of conducting further
researchintohowdomainexpertscanuseXAItoolstobetterapply
andutilizedeeplearningmodels.
The data collected by our wastewater treatment organization
wastakenfromvarioussensorsataCSOoutflowsite,amanhole
approximately 450 ft upstream of the outflow site, and a rainfall
sensorforthearea.Thesiteisconsideredtobe≈Çoverflowing≈æwhen-
everthelevelofwaterattheCSOsiteexceedsthesite‚Äôscapacity.
Eachof thesesitescollectsdata independentlyfrom eachotherat
different rates. The slowest sampling rate is one sample for every 5
minutes while the fastest is every minute. In order to handle the
inconsistencyandvariationsinrealdata,weusedlinearinterpo-
lation to handle variations in sampling time to synchronize the
samples from each of our sources. As illustrations with fictitious
data, Table 1shows a sample of the rainfall data, Table 2shows
samples from sensors in a manhole a few minutes upstream in a
pipe upstream, and Table 3shows a sample of the synchronized
andinterpolateddataset.
2The frameworkwill befurther discussed in Section 3.3.Table 3: From our dataset, we collected three features (flow,
level, velocity) from the manhole upstream of the outflow
site,onefeature(outfall)fromtheoutfallsiteitself,andone
feature (rainfall) from the rainfall sensor. This is a sample
of the synchronized and interpolated data points from our
dataset.
Timestamp FlowLevelVelocity Rainfall Outfall
Aug 17, 20197:35 0.0381.441 0.673 0.0 45.78
Aug 17, 20197:40 0.0321.424 0.590 0.0 45.78
Aug 17, 20197:45 0.0351.395 0.654 0.1 45.79
Aug 17, 20197:50 0.0321.366 0.624 0.1 45.80
Figure 2: Figure from [ 43] showing an abstraction of how
LIMEformsalocal,lineardecisionboundaryfromthemore
complex decision space.
3.2 Deep Learning Solution andXAITools
Weareusingadeeplearningmodeltotakeadvantageoftheyear
ofcontinuousdatacollectedbyourwastewatertreatmentorgani-
zation and the smart network they have developed. Since our data
issequentialinnature,weareusingaLongShortTermMemory
(LSTM)cellstructureinourmodel,whichisinlinewithourrecent
workondeeplearningbasedCSOpredictions[ 8,19].Inorderto
simplify the problem, we first check when all of the CSO events
occur,thenpasstheLSTMthefeaturesfromTable 3for12hours
ofdata.We utilizedan Adamoptimizer[ 27]andtensorflow [ 1] to
create andtrainour model3.
Simply giving our deep learning model to the wastewater treat-
ment organization is not sufficient to meet their needs for trans-
parency and justification. Therefore, we applied various XAI tools
to our deep learning solution. There are quite a few tools that pro-
videexplanationsforLSTM-basedmodels[ 2].Inordertoselectand
compare tools for our work, the tools we used must be available
to the engineers at the wastewater treatment organization even
without our direct input so they can continue to use and expand
on our work. Given this constraint, the tool should be open source,
compatible withour solution, andeasyto use.
In addition to the above selection criteria, we wanted to use the
XAItoolstoexplainhowtheLSTMhasmadeadecisionofoverflow
for the CSO site. Ideally this can be used to help inform future
decisionsforourstakeholdersattheMSDGC.Thesetoolsshould
3Oursource codeand resultsareshared at https://doi.org/10.5281/zenodo.4818970.
1035XAIToolsin the Public Sector: A CaseStudyonPredictingCombinedSewerOverflows ESEC/FSE ‚Äô21, August 23≈õ28, 2021,Athens,Greece
(a)LIME (b) SHAP
(c) RuleMatrix
Figure 3: Illustrating the explanations generated from the XAI tools: (a) LIME‚Äôs explanation displays the most influential
features supporting or opposing a prediction decision, (b) SHAP‚Äôs explanation visualizes how much input features affect the
CSOpredictions,and(c)RuleMatrix‚Äôs explanationgenerates ahierarchyofrulesby usingthe input features.
increasetransparencyandaccountabilityofthedeeplearningmodel
bybetterunderstanding howitoperates.
Fromtheserequirements,weselectedtheXAItoolsofLIME[ 43],
SHAP[30]basedonDeepLIFT[ 44],andRuleMatrix[ 32].Eachof
thesestate-of-the-arttoolscantakeanLSTM-basedmodelanda
givensamplefromourdataset,andthenproduceanexplanation
forhowthedeeplearningmodelmadeadecision.Thetoolshave
differentassumptions andmake differentexplanations.
‚Ä¢LIMEcreates a local approximation of the deep learning
model‚Äôsoutputspacebysamplingvariousinputsfromour
dataset. LIME then uses this approximation of the output
space to determine which features in the input space are
the most significant to determine the model‚Äôs prediction.
Figure2shows a representation of how a linear boundary
iscreatedforagivensampleofinterest.Thisidentifiesthe
most significant features and which classes these features
support.‚Ä¢SHAPusesbackpropagationandcomputesshapelyvalues
todeterminehow muchinfluencetheinputsofeachlayer
have on the next layer. Through backpropogation, these val-
ues are progressed from each layer starting with the output
backtowardstheinputofthedeeplearningmodel.Thisis
then used to create a significance map of how much each
individualinputinfluencedthefinalprediction.Thesevalues
ofinfluencecan range dynamically onagradient.
‚Ä¢RuleMatrix creates a global approximation of the deep
learning model‚Äôs decisions. This is done through a set of
rulesorganized hierarchicallywhere eachruleinsequence
divides the dataset based on a threshold for a given input
feature. These rules, though only approximations, are inher-
ently explainableto humans.
Figure3providessampleexplanationsillustratingtheoutputs
from our chosen XAI tools. Some key differences of these tools
can be seen through how they represent their explanations. For
1036ESEC/FSE ‚Äô21, August 23≈õ28, 2021,Athens,Greece Nicholas Maltbie, Nan Niu,MatthewVanDoren, andReese Johnson
Goal
Question
MetricAccuracy (w.r.t. predictions) Explainability (w.r.t. explanations)
Predicting all CSOs? Predicting only CSOs? Simple? Disruptive? Sound?
Recall Precision Entropy Stakeholder
InterviewsPrediction
Change
Figure4:Thegoal-question-metric(GQM)frameworkguidesourcasestudy.Eacharrowrepresentsdefiningaconceptinmore
detail.The diagram progresses fromhigh levelgoalsinto more concrete questions, then finally to the measurable metrics.
example,betweenLIMEandSHAP,SHAPgivesagradientofvalues
toeachfeatureastohowmuchithelpedorhurtaprediction.LIME
attributes influence to different features instead of a gradient for
eachfeaturetotheprediction.SHAPusesbackpropagationtoassign
influencefromthepredictiontotheinputspacewhileLIMEsamples
otherdatapointsinthelocaloutputspacetogenerateexplanations.
RuleMatrix operates in a completely different manner as it creates
awholenewmodelmakingitdifficulttodirectlycomparetoSHAP
and LIME. These differences motivate us to establish a coherent
framework for evaluatingthe XAItools.
3.3 Research Questions
Wefollowthegoal-question-metric(GQM)approach[ 46]tocrit-
ically understand the three chosen XAI tools in the context of
deep learning based CSO prediction. The analysis is drawn from
our GQM analysis of visual requirments analytics tools [ 37,40].
ComparedwithmanyXAIstudiesthatfocusedonevaluatingthe
produced explanations with lay persons or AI researchers [ 31], we
collected the feedback directly from two domain experts, i.e., a
hydrologistandanoperationalmanagerattheMSDGC.Wewere
in constant email communications with the two domain experts.
Furthermore,we heldthreeone-hourvirtualmeetingswiththese
two experts to understand the data shared with the research team,
toelicittheirexplainabilityrelatedconcerns,andtointerviewthem
whilepresenting the explanation results from the XAItools.
The structure of our GQM analysis is presented in Figure 4
where two general goals of accuracy and explainability of CSO
prediction are addressed. Relevant questions are used to refine the
goals. While we measure accuracy by well-known metrics of recall
and precision, the questions of explainability are explicitly built on
human behavioral studies, making our case study directly ‚ÄòData
Driven‚ÄôaccordingtoMiller etal.[31].Inparticular,weconsidertwo
studiesonexplainabilityfromcognitivepsychologyandbehavioral
sciences.
Lombrozo [ 29] conducted human subject experiments to decide
whatcausedagiveneventfromasetofpossiblechoicesthenjus-
tify these decisions, and showed that people disproportionately
preferred simpler explanations over more likely ones, indicating
sometrade-offbetweenthesimplicityandsoundnessofexplana-
tions.Moreover,Thagard[ 49],indevelopinghiswell-knownECHO
model to characterize the cognitive processes responsible for se-
lecting between competing explanatory hypotheses, reported that
peoplepreferredtheexplanationsconsistentwiththeirpriorknowl-
edge. Therefore, we also investigate how disruptive the XAI tools‚Äôexplanations are, compared to the domain experts‚Äô existing CSO
understandings. As shown in Figure 4, our case study addresses
five researchquestions(RQs):
‚Ä¢RQ1: How complete does the XAI-enabled deep learning
solution predict CSOs?
We measured this through recallwhich is the number of
correctly identified CSO events out of all CSO events in the
dataset. LIME and SHAP make post-hoc predictions directly
fromtheLSTM-baseddeeplearningmodel.Consequently,
LIME and SHAP have the same recall value as the LSTM.
RuleMatrix, on the other hand, requires a re-computation of
recallaccording to the generatedrules.
‚Ä¢RQ2: How much noise is there as the XAI-enabled deep
learningsolution predicts CSOevents?
Wemeasuredthisthrough precision,thenumberofcorrectly
identifiedCSOeventsoutofallpredictedeventsbythedeep
learning model. Just as with recall, LIME and SHAP make
post-hoc predictions and thus have the same precision as
the original LSTM, whereas this metric will need to be re-
computedfor the rule-basedmodelcreatedbyRuleMatrix.
‚Ä¢RQ3:HowsimplearetheexplanationsgeneratedbytheXAI
tools?
In order to evaluate the simplicity of these tools, we used
bothnumericmetricsaswellasinterviews.Foraquantita-
tive metric, we computed the entropyof the explanations
produced by the XAI tools. Entropy measures the uncer-
tainty,ordisorder,ofadistribution[ 42]whichcanbeused
to approximate how much unique information and variabil-
ityisintheexplanation.Inaddition,weinterviewedthetwo
expertsandshowedthemexplanationsfromthevariousXAI
tools.
‚Ä¢RQ4:HowsoundaretheexplanationsgeneratedbytheXAI
tools?
Soundnessofanexplanationcanbedifficulttoinvestigate
sinceitdependsonthebackground ofastakeholder,asdis-
cussed by Gilpin et al.[16]. However, XAI tools such as
DeepLIFT [ 44], SHAP [ 43], and Layer-wise Relevance Prop-
agation [ 4] all attempt to assess the ≈Çcorrectness≈æ of the
generatedexplanationsbymaskingthemostsignificantdata
valuesidentifiedbyanXAItoolfromasampletoexamine
1037XAIToolsin the Public Sector: A CaseStudyonPredictingCombinedSewerOverflows ESEC/FSE ‚Äô21, August 23≈õ28, 2021,Athens,Greece
the corresponding prediction changes of a given deep learn-
ing model. Thus, we applied this ≈Çprediction change≈æ metric
asaquantitativemeasureofthesoundnessofanexplanation.
Wealso interviewedthetwodomain expertsto assesstheir
confidence in the plausibility of the XAI tools‚Äô explanations.
‚Ä¢RQ5: How much new insight, if any, do the XAI tools‚Äô ex-
planationsoffer?
To our stakeholders from the MSDGC, performance is an
important aspectas anAIsystem must bothperformbetter
and be as interpretable as their existing system to justify its
use. It is clear to us that our stakeholders want to know if
deeplearningcanprovideanewperspectiveontheproblem.
However,havingamodeldeviatetoomuchfromtheirexpec-
tations may make it difficult to trust. Through interviewing
thestakeholders,weassessedhowdeeplearningequipped
with the explanations can help provide new insights toward
identifyingtheCSOevents,therebypotentiallydisrupting
someaspectsofthe MSDGC‚Äôs practice.
4 RESULTS AND ANALYSIS
As mentioned in Section 3, we designed an LSTM deep learning
solutiontopredictCSOsfortheMSDGC.Weanalyzedtheresults
fromthisdeeplearningmodelaswellastheXAItoolsdescribed
in Section 3.2. These results are from an LSTM that predicts if a
CSO event will occur within the next hour after being given the
previous 12 hoursofdata.
4.1 PredictiveAccuracy: Recall andPrecision
ThemostimportantmetricformodelperformancetotheMSDGCis
accurately identifyingevents.Ifthesolution failsto identifyevents
(i.e., recall is low), it will not be able to warn the citizens in the
serving area. If there are too many falsepositives (i.e., precision is
low), citizenswilllikely ignore the warnings.
As discussed in Section 3.3, LIME and SHAP had the same recall
andprecisionvaluesastheLSTMmodel.RuleMatrix‚Äôsrecalland
precisionneededtobere-computedoncetheresultingruleswere
generated.TocalculatetheaccuracymeasuresofbothLSTMand
RuleMatrix, we used a 2-month long test subset of the dataset and
thenevaluatedthepredictionsbasedonthegivenlabelsfromthe
wastewater treatment organization: ≈Çelevated≈æ means CSO events
occurred;≈Çnormal≈æmeans otherwise.
TherecallandprecisionresultsareplottedinFigure 5.Thefigure
showstheresultsofpredictingwhetheraCSOeventwillhappen
within the next hour for every 5 minute interval over a 2-month
duration. The recall and precision of the deep learning model (and
henceLIMEandSHAP)areapproximately80%and45%respectively.
TherecallandprecisionforRuleMatrixareonlyabout40%and20%
respectively, representing a 50% recall drop and a 55% precision
drop.
While disappointed in the RuleMatrix‚Äôs accuracy levels, the two
domain experts believed LSTM‚Äôs CSO predictions were encour-
aging and agreed with our suggestions of improving the model
performances by incorporatingdata from more CSO sites. During
the interviews, the experts were also interested in how the LSTM‚Äôs
performances would comparetotheMSDGC‚Äôs currentpracticeofRecall Elevated
Precision ElevatedRecall Normal
Precision Normal020406080100
Performance Metrics for Predicting CSO Events
LSTM
RuleMatrix
Figure 5: Answering RQ 1and RQ 2. Recall of predict-
ing CSOs (≈Çelevated≈æ class): LIME=SHAP=LSTM=78.9%,
RuleMatrix=43.2%. Precision of predicting CSOs:
LIME=SHAP=LSTM=41.3%, RuleMatrix=17.3%. Re-
call of predicting non-CSO events (≈Çnormal≈æ class):
LIME=SHAP=LSTM=99.6%, RuleMatrix=99.3%. Precision
of predicting non-CSO events: LIME=SHAP=LSTM=99.9%,
RuleMatrix=99.8%.
relying onweatherforecastto informthecitizens aboutpotential
CSOevents.
Toinvestigatethis,wefurthercollectedrainfalldatafromNOAA
DIVER [14] for the area around the CSO site for the same date
and time range that our deep learning model was tested on. When
usingaconstantrainfallthresholdforagivenday(i.e.,0.5inches
of rainfall per day), a recall of 100% and a precision of 20% were
obtained. The low precision level helped illustrate the exploration
ofthedeeplearningsolutions.AlthoughLSTM‚Äôs41.3%doubledthe
CSOpredictions‚Äôprecision,itisimportanttonotethattheNOAA
dataset can only collect rainfall data for each day, whereas the
LSTM-based deeplearning model makes predictions continuously
for a time range in the future. This continuous prediction of the
deep learning model could lead to lower recall as a prediction one
hour before an event may be correct but half an hour before an
eventmaybeincorrect.LSTMdoesnotpredicteveryintervalbefore
theCSOeventcorrectlybutitdoesidentify78.9%ofthe5minute
intervals an hour before a CSO event from Figure 5. The deep
learning model is more precise than onlyconsideringrainfall data
andcanbesubstantiallyimprovedinthefuture.Thevalueadded
by thedeep learning modelredefines theproblem and,withsome
improvementtothedeeplearningmodel,couldgivethewastewater
treatmentorganizationtoproactivelyaddressCSOeventsbefore
they occur.The above analysessuggest:
Finding1: While RuleMatrix‚Äôs recallandprecision
are low,LSTM (andhence LIMEandSHAP) achieves
about80%recallandperforms more precisely than
the currentpractice.In addition,LSTM providesnew
predictive capabilities for every five minutesbefore a
CSOeventas opposedto daily weather forecasts.
1038ESEC/FSE ‚Äô21, August 23≈õ28, 2021,Athens,Greece Nicholas Maltbie, Nan Niu,MatthewVanDoren, andReese Johnson
10 19 100 720
Explanation Size0123456Explanation Entropy
Analysis of Explanation Complexity
XAI Tool
SHAP
LIME
Random
RuleMatrix
Figure6:Explanationcomplexitymeasuredbyentropy.Ran-
dom is sampled from a uniform distribution, representing
a baseline entropy. Entropy is computed from the Scikit-
Learnlibrary[ 39]withalog2scalefortheseresults.Greater
entropy valuesindicatemore complex explanations(RQ 3).
4.2 ExplanationSimplicity andSoundness
OurRQ 3andRQ4investigatetowhatextent,≈Çsimplerexplanations
favored over more likely ones≈æ [ 29], holds in our case study. As
illustrated in Table 3, our LSTM model had a total of 720 input
parametersconsistingoffivefeaturessampledatarateofevery5
minutesfor12hours.ToquantitativelyevaluateallthreeXAItools,
we computed SHAP for all input parameters, and experimented
LIME for 10 (default), 100, and 720 (all) parameters. RuleMatrix
hadafixednumber ofdecisions sowe usedthe 19 rulesthat were
produced.
In order to quantify simplicity, we computed the entropy to
measurethevariationanduniquenessoftheexplanationsproduced
byeachtool.Theexplanationofeachinstancewasamatrixsharing
the same shape as the input dataset. The influence of each element
onthefinalpredictionwasstoredinthismatrix.RuleMatrixcreated
asetofrules,sotoapproximatethislevelofinformationwesimply
used a string of ones and zeros where a one indicates a rule was
usedandazeroindicatesthatarulewasskipped.Whencomputing
entropy, a set of numbers of the same value has an entropy of zero
whileasetofcompletelyuniqueorrandomnumberswouldhave
much greater entropy. We repeated a random sampling 1,000 times
toidentifyanaccuraterepresentationofrandomentropy.Thisis
limited to a small sampling due to the time it takes to create an
explanation.
Results in Figure 6show an increase in entropy with increase in
explanation size (i.e., the number of parameters in an explanation).
This trend is expected as more unique values are being added to
the explanations. As far as the XAI tools are concerned, LIME
had complexity almost equivalent to a random sampling, and it
could also be filtered to the most significant results. SHAP had
much less entropy for the 720 parameters that it explained, and10 100 200
Top K Features Removed0.000.020.040.060.080.100.120.14Absolute Change in Prediction
Distribution of Change when Removing Prominent Features
XAI Tool
SHAP
LIME
Figure7:Explanationsoundnessgeneratedbyremovingthe
most significant ùëòfeatures identified by SHAP and LIME
from a sample and then evaluating how much the removal
changed the prediction. Greater change in prediction im-
plies more soundness ofthe explanations(RQ 4).
hadawidervariancethanarandomsampling.Wespeculatethat
this is due to the majority of the features have little significance,
potentiallyresultingfrombackpropagationandhowtheSHAPtool
parsestheLSTMstructure.RuleMatrix‚Äôsentropywasonparwith
a random sample of that size but the variation was wide. Note that
RuleMatrix produced what is commonly believed to be inherently
explainableandsimpleintermsofcitingasmalllistofcausesfor
eachprediction.
Explanation soundness (RQ 4) evaluates how ≈Çcorrect≈æ an ex-
planation is at determining the decision made by the black box
LSTM. We quantifiedhow much thedeep learning model‚Äôs predic-
tion would change when removing the most significant features at
specifictimestampswithinthedataset.Thismeasuregivesusan
experimental verification of the importance of the identified fea-
turesintheexplanations.SinceRuleMatrixdidnotdistinguishin
theexplanationswhichelementswerethemostsignificantforeach
individualprediction,wecouldnotdirectlyevaluateRuleMatrixby
using the ≈Çprediction change≈æprocedure.
Figure7shows the soundness results when removing the top
ùëòfeatures for SHAP and LIME. For the top 10 features, SHAP
andLIMEhaveachangeinpredictionofabout0.0001and0.0007,
respectively. When increasing ùëòto 100, SHAP has a much wider
range,centeredat0.0217whileLIMEhasa0.0028average.When
ùëòincreasesfurtherto200,SHAPandLIME‚Äôsdistributionscluster
around 0.1050 and 0.0088 respectively. Figure 7‚Äôs results suggest
that LIME has the slightly more sound results for a smaller ùëò, but a
largerùëòseems to manifestSHAP‚Äôssoundnessbetter.
Theinterviewswiththetwodomainexpertsconfirmedourobser-
vationsandprovidednewviews.AlthoughRuleMatrixisinherently
more explainable than LIME and SHAP, the hydrologist and the
operational manager at the MSDGC did not find the rule hierarchy
capturedtherelevantknowledgeintheCSOdomain.BothLIME
andSHAPwerewellreceivedbytheexperts,withmorepreferences
shiftingtoSHAPasthisXAItoolexhibitsmoresoundnesswhenall
thefeaturesareconsideredtogether.Despitebeingsoundatsmaller
ùëòandbeingflexibleintermsofhavingacustomizablenumberof
1039XAIToolsin the Public Sector: A CaseStudyonPredictingCombinedSewerOverflows ESEC/FSE ‚Äô21, August 23≈õ28, 2021,Athens,Greece
SHAP LIME 10 LIME 100 LIME 720
XAI Tool020406080100SigniÔ¨Åcance Percentage
SigniÔ¨Åcance by Feature for Explanations
Flow
Level
Velocity
Rainfall
Outfall Level
Figure8:Theaverageinfluenceeachfeaturehadontheover-
all prediction. Observing where the influence comes from
helpstoestablishbothsoundnessofresults(RQ 4)andtoex-
plore new insights(RQ 5).
parametersinanexplanation,LIMEcouldbeconfusingwhenin-
corporatingmanyfeatures.Surprisingly,fromFigure 6‚Äôsentropy
perspective,SHAPissimplerthanLIMEwhenallthefeaturesare
taken into account. Based on the analyses of RQ 3and RQ 4, we
summarize:
Finding2: Explanation‚Äôs simplicity does notalways
come at the costofsoundness.Domainexpertswould
clearlyfavorsoundnessover simplicity,andinour
casestudy ofCSOpredictions, SHAP‚Äôsmore sound
explanations withmanyparametersturnoutto be
alsosimpler.
4.3 NewInsights from theExplanations
Ourstakeholderinterviewsincludedaninteractivesessionwhere
the two domain experts could explore the XAI tools beyond the
results that the research team had prepared. We highlight some
concrete insights gained from the interactive session, which fo-
cusedmoreonLIMEandSHAP,duetoRuleMatrix‚Äôslowrecalland
precision levels.
Figure8shows the general results as to which features from the
datasetarethemostinfluentialtothedeeplearningmodelacrossall
predictions. SHAP distributedinfluencefairlyevenlybetween the
featureswhileLIMEheavilyfavoredRainfall.AsLIMEincreasedto
includemorefeaturesintheexplanation,thedistributionevened
outmore.Inadditiontothis,avisualizationoftheinfluencebytime
isshowninFigure 9.SHAPheavilyfavoredthetimerightbefore
the CSO event whileLIME favoredthe start of the sample.Similar
toFigure 8,theinfluenceofLIMEinFigure 9evenedoutasmore
features were includedinthe explanation.
TheexpertsstatedthatLIMEresultswereusefulinidentifying
themostsignificant features when lookingatthetop10elements.
Meanwhile, theywereable to interpretnewinsights from the sig-
nificance plots generated by SHAP such as Figure 3(b) and the
summaryplotofFigure 8:theyfoundapatterninthecorrelation
betweenvelocity,flow,andlevelandsuggestedwhytheseattributes
mightbemoresignificantinafewsampleevents.Atfirst,theyhad
not assumed the feature of velocity to be useful for predictions but2 4 6 8 10 12
Hour of Input Sequence0.000.050.100.150.200.250.300.35SigniÔ¨Åcance PercentageDataset-Analysis/charts/SigniÔ¨Åcance by Hour for Explanations
SHAP
LIME10
LIME100
LIME720
Figure9:Theaverageinfluenceeachhouroftimebeforethe
predictedeventhadontheoverallprediction.Thisdistribu-
tion helps to identify the new insight about what specific
featuresto focuson and whento focuson them(RQ 5).
realized and discussed how correlations between velocity and flow
could help predict future CSO events. This provided a new insight
tohelpguidefuture developmentandresourcesfortheiranalysis
that challengedtheirinitialview.
Another major insight was the prominence of Rainfall in the
decisionmakingprocessofthemodel,asshowninFigure 8.This
confirmed our stakeholders‚Äô expectations as excess storm water
is the main cause of CSO events and helped them to trust the
results. However, the dominant influence of Rainfall leveled off not
only when LIME‚Äôs explanations involved more features, but also
when SHAP was applied. Given that LIME and SHAP achieved the
samerecallandprecisionlevels,thepatternsrevealedinFigure 9
offers remarkable insights into whento focus on which features.
While weather forecast‚Äôs Rainfall could still be dependent upon
in alerting CSOs to the relevant citizens 12 hours ahead of the
time, paying additional and equal attention to sensor network data
likevelocitycouldpotentiallyleave2≈õ4hoursfortheMSDGCto
dispatchengineersonsitetoprevent,alleviate,orotherwisemanage
the CSOevents.Basedonthe interviews, we conclude:
Finding3: XAItoolsof our casestudy,especially
LIMEandSHAP,have the potentialto disrupt
stakeholder expectationsof the influentialfactors
ofCSOs,as well as to take justifiableactions to
amelioratethe CSOsituation.
4.4 Threatsto Validity
Ourinquiryisanexploratorycasestudy[ 55]aimedatinvestigat-
ing the contemporary CSO phenomenon in depth and within its
real-life context. We discuss some of the most important aspects
that must be considered when interpreting our case study‚Äôs results.
AthreattoconstructvalidityisourchoiceofthethreeXAItools.
AsmentionedinSection 3.2,ourtoolselectionswereguidedbythe
considerations of being open source, being compatible with our
LSTM solution, and being easy to use for the stakeholders in the
1040ESEC/FSE ‚Äô21, August 23≈õ28, 2021,Athens,Greece Nicholas Maltbie, Nan Niu,MatthewVanDoren, andReese Johnson
Accuracy Explainability
Maximize 
RecallMaximize 
PrecisionFoster 
SimplicityEnhance 
SoundnessEmbrace 
Disruptiveness
LIME SHAP RuleMatrix
----
+++
-
+
++
++- +
 -
--
Figure10:SoftgoalInterdependenceGraph(SIG)fortheXAI
tools. The undirected lines represent goal decompositions,
informed by our GQM analysis (cf. Figure 4). The arrows
representsoftgoalcontributions[ 10]:≈Ç--≈æmeansbreaks,≈Ç -≈æ
meanshurts,≈Ç +≈æmeanshelps, and≈Ç ++≈æmeansmakes.
wastewater treatment organization even without our assistance.
To those ends, we limited our scope to evaluate the XAI tools as
is, without any further adjustment or customization. Another con-
struct validity relates to our use of entropy. Insights provided by
theXAItoolsprovideaddedvaluetostakeholdersandcanguidefu-
ture development and improvements for CSO prediction.Although
entropy directly measures variance and uniqueness and thus im-
pliesmoreoninformationdensity,wefoundittobeusefulwhen
quantitativelymeasuring howcomplex aresult was.
Athreattointernalvalidityconcernsthesizeofthereal-world
datasetsharedwithusbyourstakeholderorganization.Ourdataset
was limited in scale, e.g., the dataset was heavily biased towards
thenegativeclass(normalflow).Wethereforeaugmenteditwith
oversamplingtoeffectivelytrainthedeeplearningmodel.Theaug-
mentationsmighthavehadunintendedconsequencesontheresults
oftheXAItoolsasthiswasnotfullyexploredinourwork.More
historicaldataoftheCSOsitewouldhelpevaluateourassumptions
ofour augmentations,mitigating the threatto internal validity.
We believe our study‚Äôs conclusion validity is high. First and
foremost, we set out to overcome the ≈Çdata driven≈æ weakness of
currentXAIstudies[ 31].Referringexplicitlytotherelevantliter-
ature [29,49] allowed our inquires to stay focused and our con-
clusions to be well grounded. Furthermore, bias is mitigated by
investigating XAI tools not developed by the research team. Al-
though the two domain experts are only a small sample in the
field,theyhaverealstakeinthepotentialchangestobeintroduced
by deep learning. Last but not least, we share our source code at
https://doi.org/10.5281/zenodo.4818970inordertofacilitaterepli-
cation andexpansion ofour results.
5 DISCUSSION
5.1 SatisficingExplainability
Explainability, as an NFR discussed in Section 2.2, is satisficed [ 10]
inamatterofdegrees.BasedonourGQManalysisfromFigure 4
andthe quantitative and qualitative results presentedin Section 4,
webuildaSoftgoalInterdependenceGraph(SIG)inFigure 10.Inthe
SIG,eachoftheXAItoolscontributeseitherpositivelyornegatively
tothesoftgoals.FromFigure 10,wenotethatnoneoftheXAItoolsthat we investigated makes all positive contributions, indicating
thetoolsarealllimitedinsomeaspects.Atoolcannothelpmeet
somesoftgoalwithouthurtingsomeothers,suggestingthetrade-
offs among the softgoals. Interestingly, Figure 10does not reveal
the well-known trade-off between recall and precision. Instead,
theLSTM (andhenceLIMEandSHAP) achieved higherrecall and
higher precision than RuleMatrix, showing that RuleMatrix is less
fit for making CSO predictions. However, as of now, LIME and
SHAP do not perform accurately enough to be adopted by our
stakeholders.
Onemightarguethekeytoimprovingaccuracyiswiththeun-
derlyingdeeplearningmodel(namely LSTMinour case),making
explainabilityless ofa concern. We argueconsideringexplainabil-
ity, even when accuracy levels are not ideal, is still valuable. For
example,aninterestingcontrastbetweenLIMEandSHAPiswhere
theinfluencefor CSO predictionscomes from.Although different,
theexplanationsprovidedbyLIMEandSHAParebothvalidand
showhowtheseXAImethodologiesdivergeinoperation.Thedeep
learning model predicts events one hour into the future. However,
LIMEprioritizesinfluenceofrainfallfrommanyhoursbeforeaCSO
event, whereas SHAP prioritizes influence from all features evenly
immediately preceding a CSO event. Deciphering the black-box
deeplearningmodels,thoughwithvaryingdegreesofsatisficing
explainability, is of vital importance for ensuring public sector‚Äôs
transparency andaccountability.
5.2 Data DrivenExplainability
As we drive our inquiry by explicitly referencing the explainability
findingsfrom[ 29,49],wecastourcasestudy‚Äôsresultsinlightof
therelevantliterature.Lombrozo[ 29]showedthatpeopledispro-
portionately prefer simpler explanations over more likely ones;
however,Lombrozo‚Äôsworkwascarriedoutwithstudentsubjects
who were not domain experts in the field. Through our case study,
we have seen that the hydrologist and the operational manager
greatly favoredthe soundness of the explanations.It isalso worth
mentioning that simplicity does not necessarily correlate with size
(i.e., the number of features an explanation has). By computing
entropytomeasuretherandomnessoftheinformationcontained
intheexplanations(cf.Figure 6),weobservethatsoundnessand
simplicity co-exist inSHAP‚Äôsexplanations.
Thagard[ 49]reportedthatpeopleprefertheexplanationsthat
are consistent with their prior knowledge. Our domain experts
conformedlargelytoThagard‚Äôsconjecture.TheyconfirmedtheXAI
tools‚Äô outputs generally cohered with what they expected. In some
occasions, we noticed that the XAI tools‚Äô explanations refuted our
domainexperts‚Äôexpectations.Theexpertskeptanopenmindset,
andwereabletofindnewinsightsfromthedataset.Specifically,the
resultsofSHAPgavemoreinfluencetovelocitythantheexperts
initially expected (cf. Figure 8). Since SHAP mostly drew influence
from right before the CSO event (cf. Figure 9), they were able to
reasonthatthisexplanationdrewuponinformationthattheymight
have overlooked. Because of these observations, LIME or SHAP
alonemight not be able to uncover the new insights. In the SIG
of Figure 10, therefore, it is the synergyof LIME and SHAP that
contributespositivelytowardthe≈Çembracedisruptiveness≈æsoftgoal.
1041XAIToolsin the Public Sector: A CaseStudyonPredictingCombinedSewerOverflows ESEC/FSE ‚Äô21, August 23≈õ28, 2021,Athens,Greece
Requirements
GatheringData 
PreparationModel 
DevelopmentModel 
EvaluationRevision and
Feedback
Figure 11: Simplified view of the software engineering pro-
cess forAI-based systemsproposed by Amershi et al.[3].
5.3 SoftwareEngineeringforAI-Based Systems
Animportantlessonlearnedfromourcasestudyisthatoneshall
nottreatexplainabilityassomethingtoaddafteradeeplearning
modelisbuilt.Ourexperienceadvocatesstronglyforexplainability
to be engineered throughout the deep learning project. Amershi et
al.[3]breaksthesoftwareengineeringprocessformachinelearning
into nine stages. A simplified view of this process into four phases
is shown in Figure 11. Explainability is so broadly scoped that it
influencedourdecisionsforeachphaseofthesoftwareengineering
process.
‚Ä¢RequirementsGathering . We interviewed our stakehold-
ers to identify what needs to be explained and why. The
critical requirement of warning citizens about the CSO risks
helped us to better understand the role that XAI might play
inaccountability,andtobetterbuildthedeeplearningmodel
inmakingCSOpredictions.
‚Ä¢Data Preparation .Wemadeafewassumptionsaboutthe
dataandapplieddatacleaningandaugmentationbyinter-
polatingdata points to synchronize the variousdata sources
(cf.Tables 1≈õ3).Understandingthecompositionofdatafrom
each of the sensors and their preparation helped us better
contextualizethe results ofthe XAItools‚Äô explanations.
‚Ä¢Model Development . Integrating XAI into the deep learn-
ing model not only required extra effort, but also led to per-
formance decrements duetotheresources requiredtogen-
erateandvisualizeexplanations.FortoolslikeRuleMatrix,
anadditionalstepwasrequiredtopredictCSOsaccording
to the generatedrules.
‚Ä¢ModelEvaluation .Explanationscanbeconsumedbymore
thanjustAIresearchersorlaypersons;fromourwork,we
have found that domain experts cancontextualizetheseex-
planations or use them to gain new insights into the task at
hand. The ≈Çrevision and feedback≈æ of Figure 11may involve
exploring different numbers of top features from explana-
tionsprovidedbyLIME.Additionalfeedbackcouldbelinked
tootherphases.Forexample,theinsightgainedfromSHAP‚Äôs
resultsdiscussedinSection 4.3helpedelicitanewrequire-
ment of using deep learning to inform engineer-dispatch
decisions 2≈õ4hoursprior to alikely CSOevent.
XAItoolscanbeintegratedintoallphasesofFigure 11asshown
throughourcase study.They helped informdecisions throughout
software development and can be integrated into pipelines as a
method of verifying model performance or to diagnose issues and
their causes. As noted by Zhang et al.[56], there is a need for
identifying how and why deep learning models make decisions tosatisfyotherbroadlyscopedrequirements,suchasfairness,privacy,
androbustness.Webelievetheseconcernsmustbeincorporated
into all the phases of machine learning development, and our case
studyhasdemonstratedthefeasibilityofengineeringexplainability
withstate-of-the-artXAItools.
6 CONCLUDINGREMARKS
Research on explainability of XAI tools with domain experts using
real-worlddataissignificanttoensuringthetool‚Äôsabilitytobeused
with new topics. Our work is a proof of concept for how a GQM
analysis might be used to assess various XAI tools. Despite not
developingan immediately applicable product fordomain experts,
we believe that this research can serve as a foundation to assist
development of XAI tools that are more useful for a broader set of
stakeholdersbyprovidingaframeworkbasedonGQMforpotential
analysis.
Inourexploratorycasestudy,weemployedqualitativeandquan-
titativemethodstoevaluatethepredictions‚Äôaccuracyandtheexpla-
nations‚Äôsimplicity,soundness,anddisruptiveness.Throughcom-
paring thenumeric metrics and reviewing the resultsof thestake-
holder interviews, we are able to build upon existing psychological
results and contextualize them with respect to the modern XAI
tools. Domain experts welcome new insights and more complex
explanations with multiple causes. Our findings do not directly re-
fute the work of Lombrozo [ 29] and Thagard [ 49], but rather build
upon their work in noting that the different levels of complexity
maybeappropriatefordifferentstakeholdersdependingontheir
background [ 16].
To further expand upon this work, explanations from more XAI
toolscanbeinvestigatedfornewinsightsandforsupportingdiffer-
entsoftwareengineeringtasks(e.g.,[ 12,24,48,51]).Futurework
can also explore how to effectively, efficiently, dynamically, and
continuously present value-added explanations of deep learning
modeltostakeholders[ 33].Additionally,wewanttousemoredata
to expand our case study and investigate how seasonal rainfall
differences affect the XAI results. Last but not least, we seek to
employ diverse empirical methods in our future work, such as case
studiesco-designwithdomainexperts[ 53,54]andtheoreticalrepli-
cations [26,35]. Investigating explainability as a non-functional
requirementofAI-basedsystemsisanopenareaofresearch.Thein-
dividualmetricsand interviewquestions ofourstudyareonly first
steps to spark adiscussionof howthey can be improvedfurther.
ACKNOWLEDGMENTS
We appreciate the anonymous reviewers for their constructive and
insightful suggestions towards improving this manuscript. We also
thankRichardChiangandDr.CathyMaltbieforhelpeditingand
proofreading.
REFERENCES
[1]Mart√≠n Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen,
Craig Citro, Greg S. Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, San-
jay Ghemawat, Ian Goodfellow, Andrew Harp, Geoffrey Irving, Michael Isard,
YangqingJia,RafalJozefowicz,LukaszKaiser,ManjunathKudlur,JoshLevenberg,
DandelionMan√©,RajatMonga,SherryMoore,DerekMurray,ChrisOlah,Mike
Schuster, Jonathon Shlens, Benoit Steiner, Ilya Sutskever, Kunal Talwar, Paul
Tucker,VincentVanhoucke,VijayVasudevan,FernandaVi√©gas,OriolVinyals,
Pete Warden, Martin Wattenberg, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng.
1042ESEC/FSE ‚Äô21, August 23≈õ28, 2021,Athens,Greece Nicholas Maltbie, Nan Niu,MatthewVanDoren, andReese Johnson
2015. TensorFlow: Large-Scale Machine Learning on Heterogeneous Systems.
Retrieved June2021 from https://www.tensorflow.org/
[2]AminaAdadiandMohammedBerrada.2018. PeekingInsidetheBlack-Box:A
Survey on Explainable Artificial Intelligence (XAI). IEEE Access 6 (2018), 52138≈õ
52160.https://doi.org/10.1109/ACCESS.2018.2870052
[3]Saleema Amershi, AndrewBegel,Christian Bird, Robert DeLine, Harald C.Gall,
EceKamar,NachiappanNagappan,BesmiraNushi,andThomasZimmermann.
2019. Software Engineering for Machine Learning: A Case Study. In Proceedings
ofthe41stIEEE/ACMInternationalConference onSoftwareEngineering: Software
EngineeringinPractice (ICSE-SEIP‚Äô19) .Montreal,Canada,291≈õ300. https://doi.
org/10.1109/ICSE-SEIP.2019.00042
[4]Sebastian Bach, Alexander Binder, Gr√©goire Montavon, Frederick Klauschen,
Klaus-RobertM√ºller,andWojciechSamek.2015. OnPixel-WiseExplanationsfor
Non-Linear Classifier Decisions by Layer-Wise Relevance Propagation. PloS one
10,7 (2015), e0130140. https://doi.org/10.1371/journal.pone.0130140
[5]Jos√© Manuel Ben√≠tez, Juan Luis Castro, and Ignacio Requena. 1997. Are Artifi-
cial Neural Networks Black Boxes? IEEE Transactions on Neural Networks 8, 5
(September 1997),1156≈õ1164. https://doi.org/10.1109/72.623216
[6]Tanmay Bhowmik, Vander Alves, and Nan Niu. 2014. An Exploratory Case
Study onExploitingAspectOrientationin MobileGamePorting. In Integration
of Reusable Systems , Thouraya Bouabana-Tebibel and Stuart H. Rubin (Eds.).
Springer, Chapter 11,241≈õ261. https://doi.org/10.1007/978-3-319-04717-1_11
[7]SupriyoChakraborty,Richard Tomsett,Ramya Raghavendra,Daniel Harborne,
Moustafa Alzantot, Federico Cerutti, Mani B. Srivastava, Alun D. Preece, Simon
Julier,RaghuveerM.Rao,TroyD.Kelley,DaveBraines,MuratSensoy,Christo-
pher J. Willis, and Prudhvi Gurram. 2017. Interpretability of Deep Learning
Models: ASurvey of Results.In ProceedingsoftheIEEEInternationalConference
on Ubiquitous Intelligence and Computing (UIC‚Äô17) . San Francisco, CA, USA, 1≈õ6.
https://doi.org/10.1109/UIC-ATC.2017.8397411
[8]Harshitha Challa, Nan Niu, and Reese Johnson. 2020. Faulty Requirements Made
Valuable: On the Role of Data Quality inDeep Learning. In Proceedings of the7th
IEEEInternationalWorkshoponArtificialIntelligenceforRequirementsEngineering
(AIRE‚Äô20) .Zurich,Switzerland,61≈õ69. https://doi.org/10.1109/AIRE51212.2020.
00016
[9]LarissaChazetteandKurtSchneider.2020. ExplainabilityasaNon-Functional
Requirement: Challenges and Recommendations. Requirements Engineering 25, 4
(December 2020),493≈õ514. https://doi.org/10.1007/s00766-020-00333-1
[10]Lawrence Chung, Brian A. Nixon, Eric Yu, and John Mylopoulos. 1999. Non-
Functional Requirements inSoftwareEngineering . Springer.
[11]Fabiano Dalpiaz and Nan Niu. 2020. Requirements Engineering in the Days
of Artificial Intelligence. IEEE Software 37, 4 (July/August 2020), 7≈õ10. https:
//doi.org/10.1109/MS.2020.2986047
[12]HoaKhanhDam,TruyenTran,andAdityaGhose.2018. ExplainableSoftware
Analytics. In Proceedings ofthe 40th ACM/IEEE International Conference on Soft-
wareEngineering:NewIdeasandEmergingResults(ICSE‚Äô18) .Gothenburg,Sweden,
53≈õ56.https://doi.org/10.1145/3183399.3183424
[13]Department for Environment Food & Rural Affairs. 2015. Creating a River
Thamesfitforourfuture:AnupdatedstrategicandeconomiccasefortheThames
Tideway Tunnel. Retrieved June 2021 from https://assets.publishing.service.gov.
uk/government/uploads/system/uploads/attachment_data/file/471847/thames-
tideway-tunnel-strategic-economic-case.pdf
[14]DIVER. 2020. Web Application: Data Integration Visualization Exploration and Re-
portingApplication,NationalOceanicandAtmosphericAdministration. Retrieved
June2021 from https://www.diver.orr.noaa.gov
[15]Finale Doshi-Velez and Been Kim. 2017. Towards A Rigorous Science of Inter-
pretable MachineLearning. (2017). arXiv: 1702.08608
[16]LeilaniH.Gilpin,DavidBau,BenZ.Yuan,AyeshaBajwa,MichaelSpecter,and
Lalana Kagal.2018. ExplainingExplanations: AnOverviewofInterpretability
of Machine Learning. In Proceedings of the 5th IEEE International Conference
on Data Science and Advanced Analytics (DSAA‚Äô18) . Turin, Italy, 80≈õ89. https:
//doi.org/10.1109/DSAA.2018.00018
[17]XavierGlorotandYoshuaBengio.2010. UnderstandingtheDifficultyofTraining
DeepFeedforwardNeuralNetworks.In ProceedingsoftheThirteenthInternational
Conference on Artificial Intelligence and Statistics (AISTATS‚Äô10) . Sardinia, Italy,
249≈õ256. http://proceedings.mlr.press/v9/glorot10a.html
[18]Bryce Goodman and Seth R. Flaxman. 2017. European Union Regulations on
AlgorithmicDecision-Makinganda≈ÇRighttoExplanation≈æ. AIMagazine 38,3
(2017), 50≈õ57. https://doi.org/10.1609/aimag.v38i3.2741
[19]HemanthGudaparthi,ReeseJohnson,HarshithaChalla,andNanNiu.2020. Deep
LearningforSmartSewerSystems:AssessingNonfunctionalRequirements.In
Proceedings of the 42nd IEEE/ACM International Conference on Software Engineer-
ing: Software Engineering in Society (ICSE-SEIS‚Äô20) . Seoul, South Korea, 35≈õ38.
https://dl.acm.org/doi/10.1145/3377815.3381379
[20]Riccardo Guidotti, Anna Monreale, Salvatore Ruggieri, Franco Turini, Fosca
Giannotti, and Dino Pedreschi. 2019. A Survey of Methods for Explaining Black
BoxModels. Comput.Surveys 51,5(January2019),93:1≈õ93:42. https://doi.org/
10.1145/3236009[21]Kaiming He, Xiangyu Zhang, Shaoqing Ren,and Jian Sun.2016. Deep Residual
LearningforImageRecognition.In ProceedingsoftheIEEEConferenceonComputer
VisionandPatternRecognition(CVPR‚Äô16) .LasVegas,NV,USA,770≈õ778. https:
//doi.org/10.1109/CVPR.2016.90
[22]High-Level Expert Group on Artificial Intelligence, European Commission. 2019.
Policy and Investment Recommendations for Trustworthy AI . Retrieved June 2021
fromhttps://digital-strategy.ec.europa.eu/en/policies/expert-group-ai
[23]Denis J. Hilton. 1990. Conversational Processes and Causal Explanation. Psy-
chological Bulletin 107, 1 (January 1990), 65≈õ81. https://doi.org/10.1037/0033-
2909.107.1.65
[24]JirayusJiarpakdee,ChakkritTantithamthavorn,andJohnGrundy.2021. Practi-
tioners‚ÄôPerceptionsoftheGoalsandVisualExplanationsofDefectPrediction
Models. (2021). arXiv: 2102.12007
[25]Anna Jobin, Marcello Ienca, and Effy Vayena. 2019. The Global Landscape of
AIEthicsGuidelines. NatureMachineIntelligence 1(September2019),389≈õ399.
https://doi.org/10.1038/s42256-019-0088-2
[26]Charu Khatwani, Xiaoyu Jin, Nan Niu, Amy Koshoffer, Linda Newman, and Juha
Savolainen. 2017. Advancing Viewpoint Merging in Requirements Engineering:
ATheoreticalReplication and ExplanatoryStudy. Requirements Engineering 22,
3 (September 2017),317≈õ338. https://doi.org/10.1007/s00766-017-0271-0
[27]DiederikP.KingmaandJimmyBa.2014. Adam:AMethodforStochasticOpti-
mization. (2014). arXiv: 1412.6980
[28]MaximilianA.K√∂hl,KevinBaum,MarkusLanger,DanielOster,TimoSpeith,and
Dimitri Bohlender. 2019. Explainability as a Non-Functional Requirement. In
Proceedingsofthe27thIEEEInternationalRequirementsEngineeringConference
(RE‚Äô19).JejuIsland,SouthKorea,363≈õ368. https://doi.org/10.1109/RE.2019.00046
[29]TaniaLombrozo.2007. Simplicityandprobabilityincausalexplanation. Cognitive
Psychology 55, 3 (November 2007), 232≈õ257. https://doi.org/10.1016/j.cogpsych.
2006.09.006
[30]Scott M Lundberg and Su-In Lee. 2017. A Unified Approach to Interpret-
ing Model Predictions. In Proceedings of the 31st International Conference on
Neural Information Processing Systems (NIPS‚Äô17) . Long Beach, CA, USA, 4765≈õ
4774.http://papers.nips.cc/paper/6930-a-universal-analysis-of-large-scale-
regularized-least-squares-solutions
[31]Tim Miller, Piers Howe, and Liz Sonenberg. 2017. Explainable AI: Beware of
Inmates Runningthe Asylum. (2017). arXiv: 1712.00547v2
[32]Yao Ming, Huamin Qu, and Enrico Bertini. 2019. RuleMatrix: Visualizing and
Understanding Classifiers with Rules. IEEE Transactions on Visualization and
Computer Graphics 25, 1 (January 2019), 342≈õ352. https://doi.org/10.1109/TVCG.
2018.2864812
[33]Nan Niu, Sjaak Brinkkemper, Xavier Franch, Jari Partanen, and Juha Savolainen.
2018. Requirements Engineering and Continuous Deployment. IEEE Software 35,
2 (March/April 2018),86≈õ90. https://doi.org/10.1109/MS.2018.1661332
[34]NanNiuandSteveEasterbrook.2007. So,YouThinkYouKnowOthers‚ÄôGoals?
ARepertoryGridStudy. IEEESoftware 24,2(March/April2007),53≈õ61. https:
//doi.org/10.1109/MS.2007.52
[35]NanNiu, AmyKoshoffer, LindaNewman,Charu Khatwani,ChaturaSamaras-
inghe,andJuhaSavolainen.2016. AdvancingRepeatedResearchinRequirements
Engineering: A Theoretical Replication of Viewpoint Merging. In Proceedings of
the 24th IEEE International Requirements Engineering Conference (RE‚Äô16) . Beijing,
China, 186≈õ195. https://doi.org/10.1109/RE.2016.46
[36]NanNiu,AlejandraYepezLopez,andJing-RuC.Cheng.2011. UsingSoftSystems
MethodologytoImproveRequirementsPractices:AnExploratoryCaseStudy. IET
Software5,6(December2011),487≈õ495. https://doi.org/10.1049/iet-sen.2010.0096
[37]Nan Niu, Sandeep Reddivari, and Zhangji Chen. 2013. Keeping Requirements
on Track via Visual Analytics. In Proceedings of the 21st IEEE International
Requirements Engineering Conference (RE‚Äô13) . Rio de Janeiro, Brazil, 205≈õ214.
https://doi.org/10.1109/RE.2013.6636720
[38]Nan Niu, Li Da Xu, Jing-Ru C. Cheng, and Zhendong Niu. 2014. Analysis of
ArchitecturallySignificantRequirementsforEnterpriseSystems. IEEESystems
Journal8, 3 (September 2014), 850≈õ857. https://doi.org/10.1109/JSYST.2013.
2249892
[39]F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M.
Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cour-
napeau,M.Brucher,M.Perrot,andE.Duchesnay.2011. Scikit-learn:Machine
Learning in Python. Journal of Machine Learning Research 12 (2011), 2825≈õ2830.
https://jmlr.org/papers/volume12/pedregosa11a/pedregosa11a.pdf
[40]SandeepReddivari,ShirinRad,TanmayBhowmik,NisreenCain,andNanNiu.
2014.VisualRequirementsAnalytics:AFrameworkandCaseStudy. Requirements
Engineering 19, 3 (September 2014), 257≈õ279. https://doi.org/10.1007/s00766-
013-0194-3
[41]Regulation(EU)2016/679oftheEuropeanParliamentandoftheCouncilof27
April 2016.2016. General Data Protection Regulation . Retrieved June2021 from
https://eur-lex.europa.eu/eli/reg/2016/679/oj
[42]Alfr√©d R√©nyi. 1961. On Measures of Entropy and Information. In Proceedings
of the Fourth Berkeley Symposium on Mathematical Statistics and Probability,
Volume 1: Contributions to the Theory of Statistics . The Regents of the University
of California.
1043XAIToolsin the Public Sector: A CaseStudyonPredictingCombinedSewerOverflows ESEC/FSE ‚Äô21, August 23≈õ28, 2021,Athens,Greece
[43]MarcoTulioRibeiro,SameerSingh,andCarlosGuestrin.2016. ≈ÇWhyShouldI
Trust You?≈æ Explaining the Predictions of Any Classifier. In Proceedings of the
22ndACMSIGKDDInternationalConferenceonKnowledgeDiscoveryandData
Mining (KDD‚Äô16) .SanFrancisco,CA,USA,1135≈õ1144. https://doi.org/10.1145/
2939672.2939778
[44]Avanti Shrikumar, Peyton Greenside, and Anshul Kundaje. 2017. Learn-
ing Important Features Through Propagating Activation Differences. (2017).
arXiv:1704.02685
[45]Karen Simonyan, Andrea Vedaldi, and Andrew Zisserman. 2014. Deep Inside
ConvolutionalNetworks: Visualising Image Classification Modelsand Saliency
Maps. (2014). arXiv: 1312.6034v2
[46]RiniVanSolingenandEgonBerghout.1999. TheGoal/Question/MetricMethod:A
Practical Guide for Quality Improvement of Software Development . McGraw-Hill.
[47]GianniTalamini,DiShao,X.Su,X.Guo,andX.Ji.2016.CombinedSewerOverflow
InShenzhen,China:TheCaseStudyofDashaRiver. WITTransactionsonEcology
and the Environment 210(2016), 785≈õ796. https://doi.org/10.2495/SDP160661
[48]Chakkrit Tantithamthavorn, Jirayus Jiarpakdee, and John Grundy. 2020. Explain-
able AIfor SoftwareEngineering. (2020). arXiv: 2012.01614
[49]Paul Thagard. 1989. Explanatory Coherence. Behavioral and Brain Sciences 12, 3
(September 1989),435≈õ502. https://doi.org/10.1017/S0140525X00057046
[50]United States Environmental Protection Agency. 2004. Report to Congress:
Impacts and control of CSOs and SSOs. Retrieved June 2021 from https://www.epa.gov/npdes/2004-npdes-cso-report-congress
[51]WentaoWang,NanNiu,HuiLiu,andZhendongNiu.2018. EnhancingAutomated
RequirementsTraceabilitybyResolvingPolysemy.In Proceedingsofthe26thIEEE
International Requirements Engineering Conference (RE‚Äô18) . Banff, Canada, 40≈õ51.
https://doi.org/10.1109/RE.2018.00-53
[52]Meredith Whittaker, Kate Crawford, Roel Dobbe, Genevieve Fried, Elizabeth
Kaziunas, Varoon Mathur, Sarah Myers West, Rashida Richardson, Jason Schultz,
and Oscar Schwartz. 2018. AI Now Report . Retrieved June 2021 from https:
//ainowinstitute.org/AI_Now_2018_Report.pdf
[53]ChristineT.WolfandJeanetteBlomberg.2019. EvaluatingthePromiseofHuman-
AlgorithmCollaborationsinEverydayWorkPractices. ProceedingsoftheACM
onHuman-ComputerInteraction 3,EICS(June2019),143:1≈õ143:23. https://doi.
org/10.1145/3359245
[54]ChristineT.WolfandJeanetteBlomberg.2019. ExplainabilityinContext:Lessons
from an Intelligent System in the IT Services Domain. In Joint Proceedings of the
ACMIUI2019Workshops (IUI‚Äô19) .LosAngeles,CA,USA. http://ceur-ws.org/Vol-
2327/IUI19WS-ExSS2019-17.pdf
[55]RobertK.Yin.2008. CaseStudyResearch:DesignandMethods . SagePublications.
[56]JieM.Zhang,MarkHarman,LeiMa,andYangLiu.2020. MachineLearningTest-
ing:Survey,LandscapesandHorizons. IEEETransactionsonSoftwareEngineering
(2020).https://doi.org/10.1109/TSE.2019.2962027
1044