DistXplore : Distribution-Guided Testing for Evaluating and
Enhancing Deep Learning Systems
LongtianWang
Xi’anJiaotongUniversity
ChinaXiaofeiXie∗
SingaporeManagementUniversity
SingaporeXiaoning Du
MonashUniversity
Australia
Meng Tian
SingaporeManagementUniversity
SingaporeQing Guo
IHPC andCFAR, Agency for Science,
Technology andResearch
SingaporeZheng Yang
TTE Lab, Huawei
China
ChaoShen∗
Xi’anJiaotongUniversity
China
ABSTRACT
Deep learning (DL) models are trained on sampled data, where
the distribution of training data diﬀers from that of real-world
data (i.e., the distribution shift), which reduces the model’s robust-
ness. Various testing techniques have been proposed, including
distribution-unaware and distribution-aware methods. However,
distribution-unawaretestinglackseﬀectivenessbynotexplicitly
considering the distribution of test cases and may generate redun-
danterrors(withinsame distribution).Distribution-awaretesting
techniques primarily focus on generating test cases that follow the
trainingdistribution,missingout-of-distributiondatathatmayalso
be validandshouldbe consideredin the testingprocess.
Inthispaper,weproposeanoveldistribution-guidedapproach
for generating validtest cases with diversedistributions, which
can better evaluate the model’s robustness ( i.e., generating hard-to-
detecterrors)andenhancethemodel’srobustness( i.e.,enriching
trainingdata).Unlike existingtestingtechniquesthat optimize in-
dividualtestcases, DistXplore optimizestestsuitesthatrepresent
speci/f_ic distributions. To evaluate and enhance the model’s robust-
ness, we design two metrics: distribution diﬀerence , which maxi-
mizesthesimilarityindistributionbetweentwodiﬀerentclasses
of data to generate hard-to-detect errors, and distribution diversity ,
which increase the distribution diversity ofgeneratedtestcases for
enhancing the model’s robustness. To evaluate the eﬀectiveness
ofDistXplore inmodelevaluationandenhancement,wecompare
DistXplore with14state-of-the-artbaselineson10modelsacross
4 datasets. The evaluation results show that DistXplore not only
detects a larger number of errors ( e.g., 2×+ on average), but also
identi/f_ies more hard-to-detect errors ( e.g., 10.5%+ on average); Fur-
thermore, DistXplore achievesahigherimprovementinempirical
∗Corresponding authors
ESEC/FSE ’23,December 3–9, 2023, SanFrancisco, CA, USA
©2023 Copyright held bytheowner/author(s).
ACM ISBN979-8-4007-0327-0/23/12.
https://doi.org/10.1145/3611643.3616266robustness( e.g.,5.2%moreaccuracyimprovementthanthebase-
lines on average).
CCSCONCEPTS
•Softwareanditsengineering →Softwaretestinganddebug-
ging;•Computing methodologies →Neural networks .
KEYWORDS
Deep learning, software testing, distribution diversity, model en-
hancement
ACM ReferenceFormat:
Longtian Wang, Xiaofei Xie, Xiaoning Du, Meng Tian, Qing Guo, Zheng
Yang, and Chao Shen. 2023. DistXplore : Distribution-Guided Testing for
Evaluating and Enhancing Deep Learning Systems. In Proceedings of the
31stACMJoint EuropeanSoftwareEngineeringConferenceand Symposium
on the Foundations of Software Engineering (ESEC/FSE ’23), December 3–9,
2023, San Francisco, CA, USA. ACM, New York, NY, USA, 13pages.https:
//doi.org/10.1145/3611643.3616266
1 INTRODUCTION
Deep learning (DL) has achieved great success in many applica-
tions such as autonomous driving [ 42], healthcare [ 47], face recog-
nition [18] and speech recognition [ 68]. It is widely known that
DL models suﬀer from the issue of poor robustness, making them
vulnerabletoadversarialattacks.Therefore,itiscrucialtosystemati-
callytestDLsystemsbeforedeployment,especiallyinsafety-critical
scenarios.
Machine learning (ML) involves the process of learning a model
fromsampleddata( i.e.,trainingdata)tomakedecisionsonaspe-
ci/f_ic task. The general steps of ML tasks include data collection,
model training,model evaluation,and model deployment.Due to
the huge input space, it is impossible to collect all data for train-
ing, thus, high-quality data that follows a certain distribution is
collected for training. As shown in Fig. 1, for a speci/f_ic task ( e.g.,
digit classi/f_ication), there is a vast amount of task-relevant data
for digits ( i.e., thevaliddata shown in the dashed rectangle) in the
whole input space( i.e.,alldatashowninthe solid rectangle). The
task-irrelevant data ( e.g., noisy data and non-digit data) is referred
to asinvaliddata (e.g., the dataset fin Fig.1) with respect to the
Thiswork islicensedunderaCreativeCommonsAttribution4.0Interna-
tional License.
68
ESEC/FSE ’23, December3–9, 2023,San Francisco, CA, USA LongtianWang,XiaofeiXie, XiaoningDu,Meng Tian,QingGuo, Zheng Yang,andChao Shen
…
…
  b
In-Distribution Data
Out–of-Distribution Data (Valid) Invalid Data
All Input Valid Input In-distribution Input
  a c
 d
  e   f
Figure1: Data sampling and an illustrativeexampleof DL system
giventask.Asmallsubsetofthevaliddata( e.g.,thedataset aandb
in Fig.1) is collected for training the model. However, the training
distribution is often diﬀerent from the distribution of valid data
(due to the distribution shift), which greatly aﬀects the model’s ro-
bustness. A fundamental assumption is that the model is intended
to handle the in-distribution data (ID) that follows the distribution
of training data [ 4], but it is hard to correctly predict data ( e.g.,
the dataset c,d, andein Fig.1) that does not follow the training
distribution, i.e., out-of-distributiondata (OOD), which highlights
the needfor testingbefore deployment.
DLtestingaimstogeneratetestcasesthat evaluatetherobustness
of DL systems, i.e., discover the data that is valid but cannot be
predicted correctly ( e.g., the dataset dandein Fig.1), andenhance
the robustness, i.e., retraining model by including test cases data
withdiversedistribution(dataset c,d,andeinFig.1).Manystudies
havebeenconductedfortestingDLsystems[ 9,22,46,54,57,63],
wherevalidityanddistribution aretwoimportantpropertiesoftest
cases.Acommonapproachtoguarantee validityistoconstrainthe
degree of the mutation ( e.g., the distance between the new test and
theoriginalseedisconstrainedwithina /u1D43F/u1D45Dball).However,existing
methods( e.g.,DeepTest[ 54],DeepHunter[ 63],andTensorFuzz[ 43])
often ignore the distribution [ 4,9], which limits their eﬀectiveness
inevaluationandenhancement( e.g.,redundanterrorswithinthe
similar distribution are generated). Recently, some studies [ 4,9,22,
55]haveattemptedtoaddressthisbyincorporatingdistribution-
aware testing, which characterizes the training distribution via
VariationalAuto-Encoder(VAE)orGenerativeAdversarialNetwork
(GAN). However, these methods only generate ID data while OOD
data is considered as “invalid”. We argue thatthe OOD datais just
datathatdoesnotfollowthedistributionofthecollectedtraining
data but could still be valid and should be handled properly in real-
worlddeploymentenvironment.Forexample,asshowninFig. 2,
for each dataset, the input on the right side in a row is mutated
from its left-side sample, the inputs on the right are considered
as“invalid”databyexistingdistribution-awaretesting[ 9].These
data could still be visually valid, even though they are identi/f_ied as
"invalid"databyVAE[ 9].Forexample,althoughthedistributionof
theimagesin Out-of-DistributionData(Valid) inFig.1isdiﬀerent
from the distribution of the training data ( e.g., the digits written
inverydiﬀerentways),theycouldstillbethepotentialinputsto
the deployed DL systems. Therefore, it is crucial to test both in-
distribution(ID)andout-of-distribution(OOD)datathatarevalid
before deployingthe DL system.
The quality of test cases depends on the testing goals, i.e., what
kindofdataismoreusefulinrobustnessevaluationandenhance-
ment in this paper. For evaluating model’s robustness, although
OOD data is likely to trigger incorrect decisions of the model, theySVHN MNIST Fashion MNIST
Figure2:ExamplesofOODdatathatareconsideredas invalidby[9].
Left:original inputs,Right:generatedinputs
couldalsobeeasilydetectedbyOODdetectionmethods.Forexam-
ple,state-of-the-arttestingtechniquescan easilygeneratealarge
number of errors ( e.g., thousands of errors in [ 46,63]), but most of
them tend tobe weak errors thatcan be detected or/f_ilteredbyex-
isting defense techniques ( e.g., adversarial example detection [ 58]).
It is similar to traditional software testing, where defenses such as
parsersandexceptionhandlingcan/f_ilteroutweakerrors.Thus,for
DLtesting,itisimportantandchallengingtodiscoverstrongerrors
that can evade the state-of-the-art defenses. For model enhance-
ment,thegeneralgoalistoreducethedistributionshiftbetween
the training data and real-world data. Hence, how to generate tests
withdiversedistributions( e.g.,covering /u1D450,/u1D451,/u1D452)isanotherchallenge.
These diverse tests can be added to the training data for improving
the modelgeneralizabilityandrobustness.
Tothisend,inthispaper,weproposeanoveldistribution-guided
testing framework (named DistXplore ) for better evaluating and
enhancing DLsystems, i.e.,togenerate hard-to-detect anddiverse
errors.DistXplore adoptsthesearch-basedapproachtoadaptively
generatetestcaseswiththeguidanceofdistribution.Unlikeexisting
techniques that optimize test cases individuality, the optimization
ofDistXplore is performed on a test suite that represents a speci/f_ic
distribution.Speci/f_ically,weleverageMaximumMeanDiscrepancy
(MMD)[13]tomeasuretheclosenessbetweentwodistributions.For
model evaluation, DistXplore maximizes the distribution closeness
betweenthedataintwodiﬀerentclassesforgeneratingstatistically
indistinguishable errors, which are diﬃcult to defend. To enhance
the model’s robustness, we proposea metric to measure the distri-
bution diversity of the test cases, guiding DistXplore to generate
testsuiteswithvariousdistributions.Thetestcaseswithdiverse
distributions are more likely to cover a wider range of unseen data
andimprove the model’srobustness.
We conduct a comprehensive evaluation to demonstrate the
usefulness and the eﬀectiveness of DistXplore in evaluating and
enhancingthemodel’srobustness.Speci/f_ically,weselect10mod-
elson4datasets,andcompare DistXplore with14state-of-the-art
tools covering 4 diﬀerent types of techniques ( i.e., adversarial at-
tacks,distribution-unaware testing,distribution-aware testing,and
robustness-oriented testing). The results demonstrate that 1) the
statistically indistinguishable errors generated by DistXplore are
harder to detect by two state-of-the-art defense techniques, e.g.,
the attack-as-defense [ 69] can only detect 66% errors generated
byDistXplore , but almost 100% errors from adversarial attacks and
distribution-aware testing. 2) DistXplore is more eﬃcient in detect-
ing errors, e.g., on average it detects 2 ×+ errors compared to the
best baseline. 3) The test cases generated by DistXplore are more
usefulinimprovingthemodel’srobustness, e.g.,5.2%moreaccuracy
improvement thanthe baselinesonaverage.
Tosummarize,this paper makes the following contributions:
•We /f_irst discuss the limitation of existing distribution-aware and
distribution-unaware testing techniques in terms of validity and
69DistXplore : Distribution-GuidedTestingforEvaluating andEnhancingDeep Learning Systems ESEC/FSE ’23, December3–9, 2023,San Francisco, CA, USA
Invalid DataID DataTest suite 2 Test suite 1
Test suite 3 Test suite 4Training Data
in class 0Distribution-guided GenerationTraining Data
in other classes
....
All Input Valid Input ID Input Generated Test Suite……
濗濼瀆瀇瀅濼濵瀈瀇濼瀂瀁 濗濼濹濹濸瀅濸瀁濶濸 澸濝濪濙濦濧濝濨濭
濧濴瀅濺濸瀇 濖濿濴瀆瀆澸濝濪濙濦濧濝濨濭
OOD Data 
(Valid).... ..
.. ..
Seed Suite 
Figure3: Illustrationof testsuitegeneration
distribution.Thenweproposeanoveldistribution-guidedtesting
technique for generatinghard-to-detect errors and diverse data
covering a wider range of unseen data. To the best of our knowl-
edge,thisisthe/f_irstdistribution-guidedtestingforgenerating
test suites withdiversedistributions.
•Technically, we design two distribution-based metrics ( i.e., distri-
bution diﬀerence and distribution diversity) to guide the testing
forgeneratingstatisticallyindistinguishableerrorsandtestcases
withdiversedistributions,respectively.
•Wedemonstratetheusefulnessof DistXplore indiscoveringstrong
errorsandenhancingmodel’srobustnessbycomparingitwith
14 state-of-the-artmethods.
2 PRELIMINARYAND OVERVIEW
2.1 Preliminary
2.1.1 DeepNeuralNetwork. ADeepNeuralNetwork(DNN)can
berepresentedasafunction /u1D453:/u1D44B→/u1D44Cthatmapsan /u1D45B-dimensional
input/u1D465∈/u1D44Btoan/u1D45A-dimensionaloutput /u1D466.alt∈/u1D44C.ADNNusuallyis
thecompositionoflayersdenotedas /u1D453=/u1D4590◦/u1D4591◦...◦/u1D459/u1D458.Weuse
/u1D453/u1D456(/u1D465)to representthe outputofthe /u1D456/u1D461/uni210Elayer,where /u1D4530(/u1D465)=/u1D465and
/u1D453/u1D458(/u1D465)=/u1D466.alt.Forexample,theoutput /u1D466.altinclassi/f_icationisaprobability
vector for /u1D45Apossible classes( e.g.,10 classesinCIFAR-10).
2.1.2 Data Validity. Let/u1D44Bbe the whole input space with /u1D45Bdi-
mensions ( i.e.,R/u1D45B). We use /u1D44Dtodenote allpossibleinputsthat are
relevant to the given task ( e.g., all images of digits 0-9). /u1D44Dis con-
sideredasvaliddatawithrespecttothetaskastheycouldbethe
potential inputs when the trained model is deployed in real-world.
Theinputs /u1D44B\/u1D44D:{/u1D465:/u1D465∈/u1D44B∧/u1D465∉/u1D44D}areinvaliddata, e.g.,thedata
ofothertasksandlow-qualitydata.Itisdiﬃculttopreciselyde/f_ine
the validity of the data. In practice, the /u1D43F/u1D45Dnorm [40] is usually
usedtoguarantee thevalidityofthe generateddataby theexisting
DL testing and adversarial attack techniques. Speci/f_ically, given
a valid input /u1D465, the new test case /u1D465′generated by adding some
perturbationson /u1D465isconsideredasvalidif ||/u1D465′−/u1D465||/u1D45D</u1D451,where/u1D451
isasafe radius.
2.1.3 DataDistribution. Sincevalidinputs /u1D44Dcanbein/f_inite,itis
notpossibletocollectallofthemfortraining.Inpractice,aDNN
/u1D453isusuallytrainedfromcollecteddata /u1D447(i.e.,trainingdata)that
follows a distribution D/u1D447, called in-distribution (ID) data. Some
generativemodelssuchas variational autoencoders(VAE)[ 30]and
generativeadversarialnetworks(GAN)[ 9]areusedtoapproximate
the ID data distribution[ 22].
There is often a distribution shift between D/u1D44DandD/u1D447(i.e., the
trainingdatacannotrepresentthereal-worlddata),makingthatthemodelunderperformsontheout-of-distribution(OOD)data.Hence,
testcaseswith diversedistributionsaremorelikelytorevealthe
weaknessesofthemodel.Ontheotherhand,theOODtestcasescan
enrich the trainingdata such that the distribution of newtraining
dataset iscloser to the distributionof training data.
Note that the validity and the out-of-distribution of the data are
diﬀerent in this paper. The valid data is anypotential inputs of the
modelwithrespecttothetask,andisusuallyofhighquality.The
out-of-distributiondatareferstothedatathatdoesnotfollowthe
distribution of speci/f_ic training data. The valid data can be ID or
OOD, depending on the training data collected. The OOD data can
also be valid or invalid, depending on the relevance and quality
of the data. To measure the validity, we adopt the widely used
measurement, i.e.,/u1D43F/u1D45Dnorm. To measure the distribution diﬀerence,
we adopt the metric Maximum MeanDiscrepancy de/f_inedbelow.
2.1.4 Maximum Mean Discrepancy. Maximum Mean Discrepancy
(MMD)isacommonteststatistictomeasuretheclosenessbetween
two sets of samples drawn from two distributions. Assume we
have two sets of samples /u1D44B={/u1D4651,...,/u1D465/u1D45A}and/u1D44C={/u1D466.alt1,...,/u1D466.alt/u1D45B}
drawn from two distributions D/u1D44BandD/u1D44C, MMD calculates the
distancebetweenthetwosetsofsamplesinauniversalreproducing
kernelHilbertspace(RKHS)[ 51].TheempiricalestimationofMMD
between the two distributions in RKHS, denoted as /u1D440/u1D440/u1D437(/u1D44B,/u1D44C),
can be calculatedas:
1
/u1D45A2/u1D45A/summationdisplay.1
/u1D456,/u1D457=1/u1D458(/u1D465/u1D456,/u1D465/u1D457) −2
/u1D45A/u1D45B/u1D45A,/u1D45B/summationdisplay.1
/u1D456,/u1D457=1/u1D458(/u1D465/u1D456,/u1D466.alt/u1D457) +1
/u1D45B2/u1D45B/summationdisplay.1
/u1D456,/u1D457=1/u1D458(/u1D466.alt/u1D456,/u1D466.alt/u1D457)
wherekisameasurableandboundedkernelofaRKHS,MMDis
zero if and only if D/u1D44B=D/u1D44C. As mentioned in [ 44] that Gauss-
ianandLaplacekernelsareuniversal,weuseGasussiankernelto
calculateMMD.More details aboutMMD can refer to [ 14].
2.2 Overviewof DistXplore
Fig.3shows the main idea of our approach. We mainly consider
classi/f_icationtaskinthispaper.Speci/f_ically, DistXplore considers
thedatadistributionineachclassseparately, i.e.,togeneratetest
cases with diverse distributions for each class. To measure the dis-
tribution diversity of the test cases, we calculate the distribution
diﬀerence( i.e.,MMD)betweenthetestsuitefromaclassandthe
training data in each of other classes, and then measure the di-
versityofthesedistributiondiﬀerences.Weconsiderdistribution
diﬀerencesbetweentestcasesandthedataindiﬀerentclasses,since
eachinputmaybeclassi/f_iedintoanyclassbyamodel,representing
the diﬀerent decision behaviors of the model. Therefore, we aim to
generate diverse test cases by considering the diversity of distribu-
tion diﬀerences between the generated test cases and training data
ofdiﬀerentclasses.
As shown inFig. 3, given the initial test suitesampled from the
trainingdataofaclass,whichrepresentsthetrainingdistributionof
the class, the goal is to generate new test suites that have diﬀerent
distributiondistanceswiththetrainingdatainotherclasses( e.g.,
class1,2,3).Thedistributioncurveofthetestsuites( i.e.,redcurve)
shiftsfromtheoriginaldistribution( i.e.,bluecurve)tothetarget
distribution ( i.e., green or orange curve), thus DistXplore generates
test suites that are more likely to be predicted incorrectly. For
robustnessevaluation,thegoalistogenerateerrorsthatarehardto
70ESEC/FSE ’23, December3–9, 2023,San Francisco, CA, USA LongtianWang,XiaofeiXie, XiaoningDu,Meng Tian,QingGuo, Zheng Yang,andChao Shen
Real-world
T rain
PGD
DeepHunter
DistXplore
VAE
Figure4: Diversity of data distribution on MNIST
detect. The more similar the distribution of the test suite ( e.g., class
0) is tothe distribution of the training data in the target class ( e.g.,
class 1), the harder it is to detect the errors, because the errors are
statistically indistinguishable from the target class. For robustness
enhancement, DistXplore isusedtogeneratetestsuiteswithdiverse
distributions(insteadofonlyhard-to-defenderrors)thatcanenrich
thetrainingdatabyaddingunseendata,thusimprovingthemodel’s
robustness.
3 DISTRIBUTION-GUIDED TESTING
3.1 TestingGoals
In this paper, we mainly focus on two objectives: model evaluation
andenhancement.Wedesigntheobjectivefunctionsthatguidethe
test casegeneration, i.e.,optimize test suites.
3.1.1 ModelEvaluation. Toevaluatethemodel’srobustness,we
aimtogeneratetheerroneousinputsthatarehardtobedetected
by existing defense techniques. Speci/f_ically, just as any dataset can
follow aspeci/f_icdistribution, thedatawithinindividualclassesin
classi/f_icationtasksalsopossesstheirowndistribution.Duetothe
diﬀerences between these diﬀerent classes, their data distributions
arealsoverydiﬀerent( e.g.,dogsandbirds).Awell-trainedmodelis
capable of accurately distinguishing the diﬀerences between these
classes.InFig. 4,theblueareasrepresentthediﬀerentdistributions
ofdatafordiﬀerentclassesintheMNISTdataset. Conversely,ifthe
data distributions between two classes are very similar, the model
maystruggle tomakeaccuratepredictions.Thus, DistXplore aims
to generate test cases (in a class) that are statistically similar to the
training data inotherclasses.
Formally, given a DNN /u1D453and a test suite /u1D446/u1D450belonging to a
sourceclass /u1D450,we de/f_ineitsdistributiondiﬀerencewith respectto
the training data ( /u1D447/u1D450′) inanothertarget class /u1D450′as:
/u1D437/u1D439/u1D453(/u1D446/u1D450,/u1D450′)=/u1D440/u1D440/u1D437(/u1D453/u1D459(/u1D446/u1D450),/u1D453/u1D459(/u1D447/u1D450′))
where/u1D453/u1D459refersto the outputofthe layer /u1D459and/u1D450′≠/u1D450.
The distribution diﬀerence is measured on a speci/f_ic layer of
the DNN. In this paper, we select the logits layer, i.e., the layer
before the softmax layer, which is frequently used in previous
works[26,31,70].Intuitively,thesmallerthevalue /u1D437/u1D439/u1D453(/u1D446/u1D450,/u1D450′),the
more diﬃcult it is for the model /u1D453to distinguish /u1D446/u1D450and/u1D447/u1D450′. Hence,
itismorelikelytogenerateundetectableerrorsbyminimizingtheir
distributiondiﬀerence.
3.1.2 Model Enhancement. The model’s robustness can be im-
proved if the distribution of training data ( /u1D447) is closer to the distri-
butionofreal-worldvaliddata( /u1D44D),i.e.,toaddmoreunseenvalid
datatotrainingdata.However,itisimpossibletodirectlycollectall real-world data. Therefore, we could adjust the objective to
generate data that is as diverse as possible, aiming to make the
distribution of the generated data more closely resemble that of
real-worldvaliddata( /u1D44D).Toprovideaeasyunderstandingofthe
fundamental concept behind generating diverse data to enhance
model’s robustness, we conducted a qualitative analysis, as de-
picted in Fig. 4. In this visualization, we show the distribution of
training data (represented in blue) and the distributions of speci/f_ic
errors generated by diﬀerent types of tools: adversarial attack tool
(PGD[37]),distribution-unawaretestingtool(DeepHunter [ 63]),
distribution-awaretestingtool(VAE[ 55]),andDistXplore .Addition-
ally, we include some real-world data examples, which represent a
wide range ofpossible data samples.
Theresultsofthisanalysishighlighttwokeyobservations:1)the
modelisnotrobustduetothedistributionshiftbetweenthetraining
dataandreal-worlddata.Byutilizingvarioustools,wecangenerate
valid OOD data that helps reduce the distribution shift, and further
enhancetherobustnessbyincorporatingpreviouslyunknowndata
intothetrainingset.2)Theerroneousinputsgeneratedbyexisting
tools exhibit limited diversity, while DistXplore aims to generate
testcaseswithdiversedistributions,suchthatthedistributionof
the generated data could be closer to real-world data distribution.
We propose a metric to measure the distribution diversity of
testsuites,whichcanguidethegeneratonofdiversedata.Given
a DNN/u1D453that performs the classi/f_ication on /u1D45Aclasses(denoted as
/u1D436/u1D453),andasetoftestsuites /u1D447/u1D446/u1D450inaclass/u1D450,thedistributiondiversity
isde/f_inedas:
/u1D437/u1D456/u1D463(/u1D447/u1D446/u1D450)=/summationtext.1
/u1D450′∈/u1D436/u1D453\/u1D450|{B(/u1D437/u1D439/u1D453(/u1D446,/u1D450′))|∀/u1D446∈/u1D447/u1D446/u1D450)}|
|/u1D436/u1D453\/u1D450| ·/u1D458
where/u1D436/u1D453\/u1D450representstheotherclassesexcept /u1D450,Bisaninterval
abstractionfunctionthatmapsaconcreteMMDvaluetoaninterval,
and/u1D458isthenumberofintervalsbetween /u1D450andeachofotherclasses.
The basic idea is to measure the diversity of distribution dif-
ferences between the current test suites and the training data of
otherclasses.Sincethediﬀerencebetweentwodistributions( i.e.,
MMD)isacontinuousvariable,weadopttheintervalabstractionto
spilt its values into /u1D458intervals ( i.e.,/u1D458distributions). The numerator
and the denominator represent the number of intervals covered
andthetotalnumberofintervalsbetweenthecurrentclass /u1D450and
other classes, respectively. As shown in Fig. 3, the distribution ade-
quacyismeasuredfromtwoperspectives:1) DistributionDiﬀerence
Diversity: for a given target class /u1D450′, multiple intervals between
the testsuites and the trainingdata of /u1D450′canbe covered. 2) Target
Class Diversity : multiple classes ( i.e.,/u1D436/u1D453\/u1D450) are used to guide the
testgeneration,whichallowstoconsidertherelationshipsbetween
every twoclasses.
Intuitively, the test suites in multiple intervals have diﬀerent
distributions. To enhance the model’s robustness, the training data
should cover the distributions as many as possible, i.e., to increase
thedistributiondiversity.Notethat,onlyusingthestrongerrors
(i.e.,undetectable)isnotsuﬃcienttoimprovethewholerobustness
asitcannothandleerrorswithdiﬀerentdistributions(seetheresults
in Section 4).InFig.3, thegenerated test suites( i.e.,Testsuite1, 2,
3,4)havediversedistributions( i.e.,diﬀerentredcurves),andare
addedintothe training data for retraining.
71DistXplore : Distribution-GuidedTestingforEvaluating andEnhancingDeep Learning Systems ESEC/FSE ’23, December3–9, 2023,San Francisco, CA, USA
We select the classes in the same task as targets because the
classi/f_icationisbasedontheirrelationships, i.e.,tochoosearela-
tivelysuitableclass(withhigherprobability).Thesetargetsmaybe
incompleteintermsofcharacterizingthedistributiondiversity.We
canalsoselectothertargetstoguidethetestgenerationsuchasthe
classesinothertasksaslongasthegeneratedtestsarevalid.For
example,wecanselecttheclassesinCIFAR-10orRomannumerals
as the targets of MNIST task. We plan to evaluate the eﬀects of
more diﬀerenttargets inthe future work.
3.2 Distribution-GuidedTestGeneration
Toachievebothtestinggoals,weuseageneticalgorithm(GA)to
solvetheproblem.Withoutlossofgenerality,theobjectivefunction
canbede/f_inedas /u1D437/u1D439/u1D453(/u1D446/u1D450,/u1D450′) ≈/u1D463(i.e.,todecrease /u1D437/u1D439/u1D453(/u1D446/u1D450,/u1D450′)until
it is close to a small value /u1D463), where/u1D446/u1D450is the test suite belonging
to/u1D450,/u1D450′is a target class and /u1D463is a constant value. For the goal of
model evaluation, /u1D463is set as 0, i.e., to generate /u1D446/u1D450that is statisti-
cally indistinguishable from the training data in /u1D450′. For the goal
ofmodelenhancement, DistXplore generatestestsuites thatcover
morediverseintervals.Consideratargetinterval [/u1D4630,/u1D4631]thatwe
aim to cover, the objective function is de/f_ined as /u1D437/u1D439/u1D453(/u1D446/u1D450,/u1D450′) ≈/u1D463,
where/u1D463∈ [/u1D4630,/u1D4631]canbeanyvaluewithintherange.Thegeneral
objective functioncan be:
argmin
/u1D446/u1D450|/u1D437/u1D439/u1D453(/u1D446/u1D450,/u1D450′) −/u1D463|
Algorithm 1shows thesearch-based method to solve theobjec-
tive function. The inputs include the DNN /u1D453, a seed test suite /u1D446/u1D450
fromclass /u1D450,atargetclass /u1D450′(/u1D450′≠/u1D450)andthetargetdistributiondif-
ference/u1D463. The output is the new test suite that can reach the target
distribution diﬀerence. The seed test suite can be collected from
trainingdatasetortestingdataset.We /f_irstconstructa population
thatcontains /u1D45Atestsuites(Line 1-3)bymutatingtheseedtestsuite
/u1D45Atimes.Notethatthechromosomeisatestsuite(includingmul-
tipleinputs)insteadofasingleinput.Itrepeatedlyoptimizesthe
population(Line 4-16)forminimizingthedistributiondiﬀerence.In
eachiteration,we/f_irstcalculatethe/f_itnessesoftheupdatedpopula-
tion(Line 6).Thenweupdatethenewpopulationwiththestandard
crossoverandmutation.Ifthebestchromosome /u1D446inthepopulation
satis/f_ies the objective or timeout, then the optimization process
terminates(Line 9-10).Thedistributiondiﬀerencedecreasesduring
optimization until itis less than a pre-de/f_inedvalue /u1D716. Forexample,
/u1D716=0indicates that /u1D437/u1D439/u1D453(/u1D446,/u1D450′)is equal to /u1D463. Note that /u1D437/u1D439/u1D453(/u1D446,/u1D450′)is
decreasing for the two test goals, because the distribution of the
initial test suite is often far from the distribution of the training
data inthe target class /u1D450′.
We keep the chromosomethat has the best /f_itness unchanged
(i.e.,nocrossoverormutation)toensurethattheoptimizationdoes
notgetworse(Line 11).Forothers,we/f_irstselecttwochromosomes
based on the tournament strategy [ 39] (Line13-14). A uniform
crossover is performed between the selected two chromosomes
in the input level, i.e., genes in a chromosome are inputs of the
model/u1D453(Line15). Each gene in the chromosome /u1D446can be selected
to mutatewithaselection probability /u1D45F(Line16).
In this paper, we mainly focus on image classi/f_ication tasks. Dis-
tXplorecanbeeasilyextendedtootherdomains.Weselectthedi-
verse image transformations ( e.g., translation, rotation, brightness)Algorithm1: Testgeneration
Input : /u1D453:the targetDNN, /u1D446/u1D450:aseed test suite from class /u1D450,/u1D450′:
the targetclass, /u1D463:targetdistribution diﬀerence
Output : /u1D446′/u1D450:the newtest suite
Const : /u1D45A:population size, /u1D461:tournamentsize, /u1D45F:mutation rate
1/u1D443/u1D45C/u1D45D:=∅;
2for/u1D456∈ [0,/u1D45A)do
3/u1D443/u1D45C/u1D45D:=/u1D443/u1D45C/u1D45D/uniontext.1/u1D45A/u1D462/u1D461/u1D44E/u1D461/u1D452_/u1D452/u1D44E/u1D450/uni210E(/u1D446/u1D450);
4whileTruedo
5for/u1D446∈/u1D443/u1D45C/u1D45Ddo
6 /u1D453/u1D456/u1D461/u1D446=/u1D437/u1D439/u1D453(/u1D446,/u1D450′) −/u1D463;
7for/u1D446∈/u1D443/u1D45C/u1D45Ddo
8 if∀/u1D442∈/u1D443/u1D45C/u1D45D./u1D453/u1D456/u1D461 /u1D446≤/u1D453/u1D456/u1D461/u1D442then
9 if/u1D453/u1D456/u1D461/u1D446≤/u1D716ortimeout then
10 returnS;
11 continue ;
12 else
13 /u1D4461:=/u1D461/u1D45C/u1D462/u1D45F_/u1D460/u1D452/u1D459/u1D452/u1D450/u1D461(/u1D443/u1D45C/u1D45D,/u1D461);
14 /u1D4462:=/u1D461/u1D45C/u1D462/u1D45F_/u1D460/u1D452/u1D459/u1D452/u1D450/u1D461(/u1D443/u1D45C/u1D45D,/u1D461);
15 /u1D446:=/u1D450/u1D45F/u1D45C/u1D460/u1D460/u1D45C/u1D463/u1D452/u1D45F (/u1D4461,/u1D4462);
16 /u1D446:=/u1D45A/u1D462/u1D461/u1D44E/u1D461/u1D452_/u1D45D/u1D45F/u1D45C/u1D44F(/u1D446,/u1D45F);
usedinDeepTest[ 54]andDeepHunter[ 63].Foreachselectedgene,
themutation randomly selects a transformation function to mutate
it.Toguarantee thevalidityofthegeneratedinputs,weadoptthe
conservative strategy [ 63] that constrains the transformation with
both/u1D43F0and/u1D43F∞.
4 EVALUATION
We have implemented DistXplore in Python 3.6 based on DL frame-
work Keras (ver.2.3.1) with Tensor/f_low (ver.1.15.2). To evaluate the
eﬀectivenessof DistXplore inthemodelevaluationandmodelen-
hancement, we aim to answer the following research questions
(RQs), where RQ1 and RQ2 are to demonstrate the eﬀectiveness in
modelevaluation,RQ3andRQ4aretoevaluatethemodelenhance-
ment, andRQ5 isto study the generalization of DistXplore .
•RQ1:Howeﬀectiveis DistXplore indetectingerrors1thatcan
bypassthe defensemethods?
•RQ2: Howeﬃcient is DistXplore for discoveringvalid errors?
•RQ3: How eﬀective is DistXplore in improving the robustness of
the DL modelundertesting?
•RQ4: How useful are distribution diﬀerence diversity and target
class diversityinimprovingrobustness?
•RQ5: CanDistXplore be generalizedto otherdomains?
4.1 Setup
4.1.1 DatasetsandDNNModels. Weselectfourdatasets( i.e.,MNIST,
Fashion-MNIST, CIFAR-10, and SVHN) and six DNNs ( i.e., LeNet-4,
LeNet-5,VGG16,ResNet-20,Inception-v3,andInception-ResNet-v2)
thatarecommonlyusedinexistingworks[ 11,16,20,33,55,60,61].
4.1.2 Baselines. To evaluate the eﬀectiveness of DistXplore , we
select 4typesof approaches including 14 state-of-the-art baselines
for the comparisons: 6 adversarial attacks, 4 distribution-unaware
1The errorin the paper refersto the erroneous inputsthat aremissclassi/f_ied.
72ESEC/FSE ’23, December3–9, 2023,San Francisco, CA, USA LongtianWang,XiaofeiXie, XiaoningDu,Meng Tian,QingGuo, Zheng Yang,andChao Shen
testingtechniques,3distribution-awaretestingtechniquesand1
robustness-orientedtesting.
•Adversarial Attack . We select 6 adversarial attack techniques, in-
cluding3classicalones i.e.,BIM[29],PGD[37],andC&W[ 5],and
3newones, i.e.,DI-2-FGSM(D2F)[ 60],SI-NI-FGSM(SNF)[ 33],
and TI-FGSM (TIF) [ 11] to generate adversarial examples and
compare themwiththe errorsgeneratedby DistXplore .
•Distribution-unawareTesting .WeselectDeepHunter[ 63],Neuron
Path Coverage (NPC) [ 62], and Combinatorial Testing (CT) [ 50]
as the baselines. DeepHunter is con/f_igured with two diﬀerent
coverageguidance, i.e.,k-multisectionNeuronCoverage(KMNC),
NeuronBoundaryCoverage(NBC).KMNCandNBCaredesigned
totestthemajorfunctionregionandthecorner-caseregion[ 35];
NPCis con/f_iguredwithStructure-basedNeuronPathCoverage
(SNPC),whichisdesignedtotestthedecisionlogic;CTtakesthe
relationships between neurons in adjacent layers into considera-
tionwhen testingDNN models.
•Distribution-aware Testing . We select three recent distribution-
awaretestingtechniques[ 22,27,55]asbaselines.In[ 27],thetest
selectioncriteriaareproposedtomeasuretheSurpriseAdequacy
(SA)oftestcases.WeselecttheLikelihood-basedSA(LSA)that
measuresthetrainingdistributionwithKernelDensityEstima-
tion as a baseline. In [ 55], a variational auto-encoder (VAE) is
usedtospeci/f_icallygeneratein-distribution test cases.In [ 22],a
hierarchical distribution-aware (HDA) testing is proposed based
ontheglobaldistributionandlocaldistribution.Wedenotethese
twobaselinesas VAE andHDA,respectively.
•Robustness-oriented Testing. To evaluate the robustness enhance-
ment,we selectthestate-of-the-art robustness-orientedtesting
techniqueRobot [ 57]as our baseline.
4.1.3 Defense Methods. To evaluate the strengths of generated
errorsbydiﬀerenttechniques,weselecttwostate-of-the-artdefense
methodsthat detectadversarialexamples as follows:
•Dissector [56], which dissects the outputs of intermediate layers
andcalculatesascoreforthegiveninput.Thescoreshowsthe
degreeofsimilaritybetweentheinputandbenigndata.ForLeNet-
4andLeNet-5,weselectthefullyconnectedlayers.Foreﬃciency,
/f_iveintermediatelayersareselectedforlargermodel.Thedetails
are providedonour website [ 2].
•Attackas Defense(A2D) [69],which detects adversarialsamples
basedontheobservationthatadversarialsamplesarelessrobust
than benign ones. It measures the robustness of the given inputs
withexistingadversarialattacks.WeuseJSMA[ 45](thatisdif-
ferentfrombaselineadversarialattacks)tocalculatetheattack
costofeachinputfor detecting whether itisabnormal input.
4.1.4 ExperimentSetup.
SeedSelection. Foreachtask,werandomlyselect100seedinputs
foreachclassfromtrainingdataset.Totally,weselect1,000seeds
thatareusedbyallbaselines.NotethattheHDAapproachproposes
adistribution-awarestrategytoselectseeds,hencewecon/f_igure
HDAwithtwoinitialseedconstructionstrategies:1)usingthesame
1,000seedinputsasusedfor otherbaselinesfor afaircomparison
(denoted as /u1D43B/u1D437/u1D434) and 2) using the HDA’s own seed selection to
select1,000 initialseedinputs(denotedas /u1D43B/u1D437/u1D434/u1D45C).Con/f_igurationofDistXplore. Weusethe100initialseedsselected
ineachclassasaseedtestsuite.Foreachclass /u1D450,werunDistXplore
multiple times ( i.e., 9) by setting diﬀerent target classes /u1D450′with
Algo.1. Finally, for each model, we run DistXplore 90 times ( i.e.,
10 source classes ×9 target classes). We set the/f_itness function as
minimizing the distribution diﬀerence ( i.e., the values of /u1D463and/u1D716in
Algo.1arecon/f_iguredas0).Notethat,tocalculatethediﬀerenceef-
/f_iciently,werandomlyselectanother100samplesfromthetraining
datainclass /u1D450′insteadofallofthem.Wefoundthatthedistribu-
tion distance between the selected samples and the corresponding
class oftrainingdatais closetozero(MMD), whichindicates that
theselectedtrainingsamplescan representthedistributionofthe
wholetraining data.
Foreachrunof DistXplore ,welimitthetotalnumberofiterations
in GA as 30. We empirically con/f_igured the population size, the
tournament size, and the mutation rate as as 100, 20, and 0.01,
respectively.Duetothelimitofthespace,theexperimentsabout
the impact of the parameters are put on our Website [ 2]. For the
robustnessenhancement,wedonotexplicitlygeneratetestcases
foreachinterval(see /u1D437/u1D456/u1D463(/u1D447/u1D446/u1D450)inSection 3.1.2).Instead,wemap
thedistributiondiﬀerenceineachiteration( i.e.,the/f_itnessvalue)
to an interval. During the optimization process, the distribution
distance is decreasing in multiple iterations, covering diﬀerent
intervals.Toensurethevalidityofthegeneratedtestcases,weadopt
amoreconservativecon/f_igurationcomparedtoDeepHunter[ 63]
to constrain the mutation.
Con/f_iguration of Baselines. For the three classic adversarial at-
tacks,weperformthetargetattackforeachseedinputbyselecting
otherclassesasthetargets, i.e.,wegenerate9adversarialexamples
foreachseedinput.Forthethreenewadversarialattacks,asthey
arenotdesignedfortargetattacks,weperformuntargetattackwith
the defaultcon/f_igurationsprovided.
Note that LSA is a test selection metric instead of a testing tool.
Toperformthecomparison,wedevelopanewtestingtoolbasedon
DeepHunter, i.e., using LSA asthe guidance to generatetestcases.
Forothers,wefollowtheirdefaultcon/f_igurationstorunDeep-
Hunter,CT,NPC,HDA,VAE,andRobot.Speci/f_ically,eachmodelis
testedfor5,000 iterationsby DeepHunter (KMNC and NBC), CT,
andNPC.Eachseedisoptimizedwith50,30,and30iterationsby
HDA, VAE,andRobot,respectively. More detailedsettingscanbe
foundonour website [ 2].
RQ Setup. To demonstrate the capability of DistXplore in gen-
erating strong errors for model evaluation ( RQ1), we collect the
test suite in the last iteration for every pair (/u1D450,/u1D450′)(i.e., the best
chromosome returns from Algo 1). For each model, we collect a
total number of 90 chromosomes over 90 pairs, which are used
to evaluate the strength of these errors. The strength of errors is
measured by the success rate of bypassing defenses. In addition,
wealsoevaluatetheeﬃciencyof DistXplore fordiscoveringvalid
errors (RQ2). To evaluate the eﬃciency, we count all the errors
generatedduringthe 30iterations. Speci/f_ically,we select two met-
ricsforthecomparisons:thetotalnumberoferrorsandthesuccess
rate of generating errors for each seed. To evaluate the validity of
generated errors, we perform a human study to manually check
the validity ofthe discoverederrors.
73DistXplore : Distribution-GuidedTestingforEvaluating andEnhancingDeep Learning Systems ESEC/FSE ’23, December3–9, 2023,San Francisco, CA, USA
Table1: Resultsof bypassingthedefense techniqueson datasetsMNIST (M),Fashion MNIST (FM),CIFAR-10(C),and SVHN (S)and DNNs
LeNet-4(L-4),LeNet-5(L-5),VGG16(V-16), ResNet-20 (R-20), Inception-ResNet-v2 (IR-V2),and Inception-v3 (I-V3).
DS Model Defense DistX BIM PGD C&W D2F SNF TIA KMNC NBC CT NPC LSA HDA /u1D43B/u1D437/u1D434/u1D45CVAE
ML-4Dissector 0.971.00 1.00 1.00 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 1.00 1.00 1.00
A2D 0.581.00 1.00 1.00 0.99 0.81 0.99 0.87 0.88 0.73 0.69 0.67 1.00 1.00 1.00
L-5Dissector 0.930.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.97 0.97 0.97 0.99 1.00 1.00
A2D 0.680.99 1.00 1.00 1.00 0.78 1.00 0.81 0.79 0.78 0.84 0.85 0.99 1.00 1.00
FML-4Dissector 0.850.95 1.00 1.00 0.98 0.97 0.99 0.95 0.95 0.90 0.93 0.89 0.91 0.97 -
A2D 0.350.91 0.99 0.98 0.87 0.74 0.77 0.80 0.80 0.60 0.63 0.46 0.95 0.97 -
L-5Dissector 0.820.96 0.96 0.99 0.93 0.89 9.95 0.89 0.87 0.85 0.95 0.87 0.86 0.96 -
A2D 0.440.87 0.89 0.97 0.85 0.87 0.88 0.55 0.59 0.53 0.94 0.60 0.92 0.99 -
CV-16Dissector 0.830.99 0.98 0.98 0.96 0.92 0.95 0.96 0.95 0.95 0.95 0.94 0.91 0.93 -
A2D 0.590.99 0.95 0.98 0.92 0.81 0.91 0.77 0.77 0.78 0.77 0.83 0.89 0.93 -
R-20Dissector 0.890.99 0.99 0.99 0.94 0.93 0.94 0.89 0.89 0.92 - 0.92 0.90 0.90 -
A2D 0.390.96 0.95 0.78 0.93 0.95 0.83 0.56 0.56 0.89 - 0.97 0.91 0.89 -
IR-V2Dissector 0.840.98 0.98 0.99 0.90 0.89 0.90 0.87 0.87 0.86 - 0.86 0.91 0.90 -
A2D 0.240.76 0.81 0.76 0.36 0.51 0.36 0.47 0.49 0.53 - 0.51 0.53 0.66 -
I-V3Dissector 0.830.97 0.97 0.98 0.92 0.91 0.92 0.89 0.90 0.90 - 0.89 0.93 0.93 -
A2D 0.270.82 0.84 0.72 0.41 0.50 0.35 0.49 0.46 0.42 - 0.43 0.56 0.55 -
SV-16Dissector 0.860.99 0.99 0.99 1.00 0.96 0.99 0.98 0.99 0.95 0.96 0.94 0.94 0.96 0.99
A2D 0.360.95 0.97 0.98 1.00 0.91 0.99 0.62 0.75 1.00 1.00 0.57 0.57 0.90 0.97
R-20Dissector 0.880.99 0.99 0.99 0.98 0.95 0.98 0.92 0.92 0.95 - 0.92 0.94 0.97 0.99
A2D 0.440.98 0.97 0.96 0.98 0.95 0.97 0.90 0.85 1.00 - 0.98 0.65 0.91 0.99
Todemonstratethecapabilityinenhancingrobustness,weselect
test suites with diverse distributions ( i.e.,distribution diﬀerence
diversityandtargetclassdiversity ).Foreachpair (/u1D450,/u1D450′),wesplitthe
distribution diﬀerence [ /u1D437/u1D4391,/u1D437/u1D43930] into 10 intervals, where /u1D437/u1D439/u1D45B
represents the best /f_itness value in the /u1D45B/u1D461/uni210Eiteration. Note that the
/f_itness values in multiple iterations may fall into the same interval.
To achieve the distribution diﬀerence diversity , we randomly select
aniterationfromeachintervalandcollectitsbestchromosome( i.e.,
10chromosomesforeachpair).Toachievethe targetclassdiversity ,
weconsiderallofotherclassesasthetargets( i.e.,9targetsforeach
source).Finally,wecollect900testsuites(10intervals ×90pairs)for
/f_ine-tuningin RQ3(e.g.,Testsuite1,2,3,4, ...inFig.3).Toconduct
a fair comparison, we collect the same number of test cases by
eachbaselineforretraining.Speci/f_ically,foradversarialattacks,we
con/f_igure diﬀerent parameters such that we can generate multiple
adversarial examples for each seed input. For testing tools, we /f_irst
generate a large number of errors, and then randomly select the
same number ofinputsfor retraining.
ForRQ4, we evaluate the usefulness of distribution diﬀerence
diversityandtargetclassdiversity inrobustnessenhancement. We
collect two sets for retraining: 1) we only consider the distribution
diﬀerence diversity and ignore the target class diversity . We ran-
domly select one target class and collect multiple chromosomes
fromeachinterval,denotedas DistXplore/u1D451/u1D453(e.g.,Testsuite1,2 in
Fig.3).2)Weselectalltargetclassesforthe targetclassdiversity but
restrict their intervals. For each target class, we randomly select
some chromosomes from only one interval, denoted as DistXplore/u1D461.
(e.g.,Testsuite1,3 inFig.3).Notethat,tomakeafaircomparison
with the results in RQ3, we control the number of test cases in
DistXplore/u1D451/u1D453andDistXplore/u1D461by collecting multiplechromosomes
fromaninterval,suchthattheyhavethesamesizewiththedata
using inRQ3 ( i.e.,900test suites).
Fortherobustnessmeasurementin RQ3andRQ4,weselectthe
empirical robustness thatiscommonlyused in previousworks[ 22,
57]. Theempirical robustness is measured by the accuracy on avalidationdataset.Togeneratesuchavalidationdataset,weselecta
new set of initial seeds (1,000) that diﬀers from the seeds in testing.
Thenwerun DistXplore andotherbaselinestogenerateerrorsbased
on new seeds. These errors found by diﬀerent tools form a new
testsetforevaluatingempiricalrobustness.Consideringthatthe
transformationstrategiesarediﬀerentindiﬀerenttypesoftools,we
try to construct a balanced dataset for a fair comparison, including
9,000errorsfromeachtypeoftool, i.e.,adversarialattacks(3,000for
each ofBIM, PGD,and C&W), distribution-unaware testing (4,500
for each con/f_iguration of DeepHunter), distribution-aware testing
(3,000 for each of LSA, VAE, and HDA), and distribution-guided
testing(100 for eachsource-target pair).
ForRQ5, we evaluate the generalization ability of DistXplore by
adaptingittotwoNLPclassi/f_icationtasks: i.e.,sentimentanalysison
IMDB[36]andnewsclassi/f_icationonAG’sNews[ 67].We/f_ine-tune
the pre-trainedmodelBERT[ 8]onthe twodatasets,respectively.
Duetotheintrinsicdiﬀerencesbetweenimagesandtextualdata,
we develop the text speci/f_ic mutation strategies. The details about
thetextmutationcanbefoundontheWebsite[ 2].Asothertesting
toolsaremainlyusedinimagedomain,weselecttwoNLPadver-
sarialattacks( i.e.,PWWS[ 48]andTextFooler[ 23])asthebaselines.
Additionally, we select the state-of-the-art method WDR [ 41] as
the defense technique as Dissector and A2D are not suitable for
BERTpre-trainedmodels.
We follow the existing work [ 63] and repeat each experiment
5 times to reduce the eﬀect of the randomness during the test
generation.
4.2 Results
4.2.1RQ1:Strength of Errors. We evaluated our method using
three metrics: the unique number of errors , thesuccess rate , and the
strength of errors . Theunique number oferrors represents the total
number of erroneous inputs generated within a given time budget.
This metric is widely used in existing DL testing works [ 1,3,6,10,
24,34,57,65,72,73]andprovidesameasureoftheeﬀectivenessof
74ESEC/FSE ’23, December3–9, 2023,San Francisco, CA, USA LongtianWang,XiaofeiXie, XiaoningDu,Meng Tian,QingGuo, Zheng Yang,andChao Shen
DLtesting.The successrate measuresthepercentageofseedinputs
from which the testing tools can generate at least one erroneous
input.ThismetrichasbeenemployedinDLtestingandadversarial
attacktools[ 3,11,33,60,65,66].Ahighersuccessrateindicatesthat
ourmethodiscapableofgeneratingerrorsforalargerproportionof
seed inputs. The strength of errors quanti/f_ies the severity or impact
oftheerroneousinputsgenerated.Weemphasizetheimportance
ofgeneratingstrongerrors,asweakererrorscanbeeasilydetected
byexisting defensetools.
Table1shows the results on the strength of generated errors
by diﬀerent methods. For Dissector, we use AUROC to indicate the
capabilityon detectingerrors. For Attack-as-Defense ,we show the
proportionof errors thatcan be detected.The symbol- incolumn
NPCindicatesthatNPCcannotbeusedtotesttheseDNNssince
thecriticalpathscannotbeextracted.Thesymbol-incolumnVAE
indicates thatthe VAE methoddoes not work well onthe selected
taskas mentionedin[ 9].
The overall results show that DistXplore (columnDistX) can
generate more strong errors that are diﬃcult to be detected by
defense techniques compared with baselines. Speci/f_ically, all errors
generatedbyadversarialattacksunderperform DistXplore ,which
maybebecausethattheyonlyaddminorperturbations.Wealso
found that the new advrsarial attacks outperform the classic ad-
versarial attacks ( i.e., BIM, PGD, C&W). Compared with testing
techniques, we can see that DistXplore performs better in most
cases. Comparing the results between distribution-unaware testing
(i.e., KMNC, NBC, CT, and NPC) and distribution-aware testing
(i.e., HDA//u1D43B/u1D437/u1D434/u1D45Cand VAE), we found that distribution-unaware
testing tends to perform better because it generates some OOD
data, indicating that ID errors (from distribution-aware testing)
are easier to detect. DistXplore explicitly considers the distribution
diﬀerence, which guides to generate statistically indistinguishable
errorsthat are more diﬃcult to detect.
Compared to other distribution-aware testing ( i.e., HDA//u1D43B/u1D437/u1D434/u1D45C
and VAE), we found that the errors generated by LSA are harder
to detect because LSA can also generate OOD data based on the
surprise guidance. DistXplore performs better than LSA since it
considers the distribution diﬀerence between each two classes and
optimizes each test suite, making the discovered errors statistically
indistinguishablecomparedwithotherclasses.
AnswerstoRQ1-1 : Comparedwith adversarialattacks and
existing DL testing techniques, DistXplore is more eﬀective in
generating hard-to-detect errors. Existing distribution-aware
testing techniques mainly focus on generating in-distribution
data that could be easier to detect.
Fig.5showstherelationshipbetweenthedistributiondiﬀerence
and the strength of errors. Due to the space limit, other results are
put on our website [ 2]. For each pair (/u1D450,/u1D450′), we collect the best
chromosome /u1D446aftereachiterationandcalculate:1) MMD_target :
thedistributiondiﬀerencebetween /u1D446andthetrainingdataoftarget
class/u1D450′, 2)MMD_source : the distribution diﬀerence between /u1D446and
the training data of source class /u1D450, 3)Error Rate : the proportion
of errors in /u1D446, 4)Error_target Rate : the proportion of errors (in /u1D446)
predictedasthetargetclass and,5) Dissector andA2D:theresults0 10 20 30
Iteration0.00.20.40.60.81.0MNIST
0 10 20 30
Iteration0.00.20.40.60.81.0Fashion-MNIST
MMD_target
MMD_sourceError Rate
Error_target RateDissector
A2D
Figure 5: The average results during the optimizationof DistXplore
(model: LeNet-5)
Table2: Resultsof eﬃciency on four datasets
DS Mod Metric DistX KMNC NBC LSA HDA VAE
ML-4Time(s) 257.7 737.9 471.4 1271.9 1937.8 1166.2
#Error21,008.6 4655.4 8029.8 8985.2 58.8 32.8
Succ.R 1.00 0.65 0.96 0.96 0.59 0.33
L-5Time(s) 159.4 1260.0 758.9 2200.0 1563.7 3810.0
#Error 9602.4 3414.4 6445.8 7132.4 8.8 43.4
Succ.R 0.89 0.61 0.95 0.960.09 0.43
FML-4Time(s) 258.9 734.1 549.0 1002.8 1963.7 -
#Error21,082.8 10,396.8 15,409.8 19,607.2 97.4 -
Succ.R 1.00 0.73 0.80 0.92 0.97 -
L-5Time(s) 160.8 1810.4 1137.0 1234.0 1894.4 -
#Error18,747.4 11,064.4 15,562.8 17,756.4 94.8 -
Succ.R 0.99 0.72 0.99 0.97 0.95 -
CV-16Time(s) 613.924,820.1 13316.1 3031.2 7255.9 -
#Error26,131.4 5924.6 8510.6 10,853.2 81.4 -
Succ.R 1.00 0.79 0.94 0.93 0.82 -
R-20Time(s) 605.6 4768.5 2956.1 2931.6 6102.5 -
#Error30,016.8 8683.4 10,176.6 13,701.6 96.2 -
Succ.R 0.97 0.68 0.82 0.73 0.96 -
SV-16Time(s) 581.824,412.3 12,488.2 6903.4 6893.6 6448.1
#Error29,793.4 2342.4 2856.2 3936.2 75.8 98.6
Succ.R 1.00 0.70 0.70 0.70 0.76 0.99
R-20Time(s) 606.6 4435.4 2842.5 6645.7 6533.7 5749.1
#Error29,627.4 4122.8 5508.6 9653.6 76.8 98.6
Succ.R 1.00 0.53 0.75 0.81 0.77 0.99
detectedbythediﬀerentdefensetechniques.Weaveragetheresults
fromallpairs,andnormalizetheresultsfrom0to1exceptfor Error
RateandError_target Rate for easier comparison.
The results show that, during the optimization, the distribution
of/u1D446is getting closer to the training distribution of the target class
(seeMMD_target )andgettingfartherawayfromthesourceclass
(seeMMD_source ). Meanwhile, Error Rate andError_target Rate are
increasing,indicatingthatmoreerrorsaregeneratedandgradually
become statistically indistinguishable between the original class
/u1D450andtarget class /u1D450′.Theeﬀectofindistinguishabilitycan befur-
ther con/f_irmed by the detection results ( i.e.,Dissector andA2D):
errors become indistinguishable and diﬃcult to detect while the
MMD_target decreases.
Answers to RQ1-2 : The distribution diﬀerence is useful in
guidingthegenerationofstatisticallyindistinguishableerrors,
makingthemmore diﬃcultto detect.Comparedwithothers,
DistXplore generatesmore diverseerrors.
75DistXplore : Distribution-GuidedTestingforEvaluating andEnhancingDeep Learning Systems ESEC/FSE ’23, December3–9, 2023,San Francisco, CA, USA
4.2.2RQ2: Eﬀiciency of DistXplore. We further study the eﬃ-
ciency of DistXplore in discovering errors, as shown in Table 2.
Note that we do not set the same time to run all tools as diﬀer-
ent tools have diﬀerent con/f_iguration methods. We emphasize that
this papermainly focuses ongenerating high-quality ( i.e., hard-to-
detect) errors rather than merely comparing the total number of
errorswithinasettime,asmanyweakerrorscanbeeasilydetected
bydefensemethods(see RQ1 results).
In Table2, we show the time used for each tool under its con/f_ig-
uration(Time(s))andthetotalnumberoferrors(#Error).Dueto
the space limit, other results are put on our website [ 2]. We do not
showtheresultsofadversarialattacksherebecausetheydiﬀerfrom
the settings of testing tools, i.e., they generate an adversarial exam-
ple for each seed. Overall, we can observe that DistXplore (column
DistX)generatesmoreerrorswhileusestheshortesttime.Wecould
alsoobservethattheexistingdistribution-awaretestingtendsto
be slower due to time-consuming distribution measurements, such
astheKernel Density Estimationand VAE.Table 2alsoshows the
success rate of generating errors for each seed. The results show
thatDistXplore hasahighersuccessratethanotherbaselines.We
also notice a exception that LSA achieves higher success rate on
MNISTLeNet-5.Weconjecturethatitisduetotheoptimizationob-
jectiveof DistXplore thatminimizesthedistributiondistance,rather
thanspeci/f_icallyguidingmisclassi/f_icationforindividualsamples.
In somespeci/f_icdatasets, theoptimization may not requireerrors
for certainseeds.
In order to evaluate the validity of the generated inputs, we
conducted a manual investigation by randomly selecting 10,000
erroneous inputs from the testing outputs of each model and calcu-
latingthe averagevalidity ratio.The validityratios werefound to
be 98.5%+, 96.5%+, 98.7%+, and 95.3%+ for MNIST, Fashion-MNIST,
CIFAR-10, and SVHN datasets, respectively. The results demon-
strate that DistXplore is capable of generating valid inputs with
high proportions. More details are providedonour website [ 2].
Answers to RQ2 : Compared to other DL testing tools, DistX-
ploreachievesthehighesteﬃciencyintermsofthenumberof
errors generated per second and success rates. Moreover, Dis-
tXploreis more eﬀective in terms of generating valid samples.
4.2.3RQ3:RobustnessEnhancement. Foreachtool,we/f_ine-tune
the original model 20 epochs following previous works [ 25,38,49]
by adding the new data generated from each tool, and evaluate the
empirical robustness of the new model on the validation dataset
we created. Note that all data in validation dataset is predicted
incorrectly by the original model. Table 3shows the accuracy of
/f_ine-tuned models on the validation dataset. As expected, DistX-
ploreoutperformstheadversarialattacks,distribution-awaretest-
ing, distribution-unaware testing, and robustness-oriented testing.
The overall results demonstrated the eﬀectiveness of DistXplore in
improvingrobustness.Inaddition,LSAachievesthesecondbestre-
sults which outperform the results of other baselines, because LSA
can generate some OOD test cases, increasing the diversity. The
three modern adversarial attack techniques perform worse than
the three classictechniques, because these techniquesare designed
for untargetattack, whichdecreasethe distributiondiversity.D-G Adv D-U D-ADistXplore
BIM
PGD
C&W
KMNC
NBC
LSA
HDA
RobOT0.59 0.75 0.38 0.83
0.07 0.95 0.09 0.53
0.07 0.95 0.09 0.53
0.07 0.95 0.10 0.53
0.36 0.66 0.37 0.73
0.30 0.58 0.31 0.70
0.47 0.55 0.28 0.80
0.04 0.60 0.06 0.51
0.07 0.80 0.11 0.52CIFAR-10
0.20.40.60.8
D-G Adv D-U D-ADistXplore
BIM
PGD
C&W
KMNC
NBC
LSA
HDA
VAE
RobOT0.60 0.54 0.48 0.87
0.05 0.79 0.14 0.71
0.06 0.74 0.13 0.72
0.07 0.74 0.14 0.71
0.27 0.50 0.32 0.79
0.26 0.59 0.29 0.78
0.40 0.42 0.35 0.85
0.03 0.59 0.11 0.71
0.04 0.49 0.12 0.70
0.06 0.69 0.14 0.71SVHN
0.10.20.30.40.50.60.70.8
Figure6: Accuracy on diﬀerenttypesof dataset (model: VGG16)
AnswerstoRQ3-1 :Overall, DistXplore iseﬀectiveinimprov-
ing robustness by generating data with diﬀerent distributions.
Distribution-awaretestingtechniquesonlyconsiderIDdata,
makingitperform poorlyonerrorsgeneratedbyothertools.
To further interpret the results, we analyze the accuracy on dif-
ferentkindsofvalidationdataset,whichisshowninFig. 6.Otherre-
sultsareshowninthewebsite[ 2].Recallthatourvalidationdataset
includes 9,000 errors from distribution-guided testing DistXplore
(i.e.,D-G), 9,000 errors from adversarial attacks ( i.e.,Adv), 9,000 er-
rorsfromdistribution-awaretestingLSA,HDA,andVAE( i.e.,D-A),
and 9,000 errors from distribution-unaware testing ( i.e.,D-U). Note
thatthedataset D-GandD-Ucovermorediversetransformations
(e.g.,rotationandtranslation)whilethedataset AdvandD-Aare
mainly created by the noise-based transformation. Speci/f_ically, the
image transformation directly determines the distribution of the
generated test cases[ 4] that further aﬀects the accuracy evaluation.
Takinginto account thatthese toolsusediﬀerenttransformations,
we build such a balanced validationdataset for a fairer comparison.
Not surprisingly,each tool usuallyachieves betteraccuracy on
thevalidationdatageneratedbythesametypeoftools,becausethey
have similardistribution, whilethedata from othertypes oftools
are more likely tobe OOD.For example, BIM, PGD, and C&W get
muchhigheraccuracyon Advdatasetsincetheaddedtrainingdata
and theAdvdata are very similar ( i.e., adding minor perturbation).
However, the tools with only noise-based perturbation ( i.e., BIM,
PGD, C&W, HDA, VAE, and RobOT) achieve much lower accuracy
on the data D-GandD-Uthat use very diﬀerent transformation.
Their accuracy on D-G(<0.09) is relatively lower than that on D-U
(>0.09), indicating some errors generated by DistXplore are harder
to predict.
Comparing the results between DeepHunter and DistXplore ,
which use the same transformations, we found that DeepHunter
achieves lower accuracy than DistXplore onD-Gdata because Dis-
tXploregeneratestestcaseswith diversedistributions,whichmay
be OOD for DeepHunter. As for the data D-Ugenerated by Dee-
pHunter, the accuracy of DistXplore is slightly higher than that
of DeepHunter, which indicates that the errors from DistXplore
couldcoversomedistributionofthedatageneratedbyDeepHunter.
Considering the distribution-aware testing HDA and VAE, as they
onlygenerate IDdata,theyperformmuchworseon otherdataset.
Consider the results of distribution-aware testing (HDA and
VAE), adversarial attacks (BIM, PGD and C&W), and robustness-
orientedtesting,whichusethesametransformation,wefoundthat
HDAandVAEachieveloweraccuracy(seeTable 3),indicatingthat
76ESEC/FSE ’23, December3–9, 2023,San Francisco, CA, USA LongtianWang,XiaofeiXie, XiaoningDu,Meng Tian,QingGuo, Zheng Yang,andChao Shen
Table3: Resultsof robustnessenhancement usingthetestcases generatedby diﬀerenttools on four datasets
D M DistXplroe BIM PGD C&W D2F SNF TIF KMNC NBC CT NPC LSA HDA /u1D43B/u1D437/u1D434/u1D45CVAE Robot
ML-4 0.81 0.64 0.65 0.59 0.57 0.60 0.56 0.52 0.54 0.63 0.63 0.72 0.61 0.63 0.55 0.52
L-5 0.81 0.66 0.65 0.66 0.41 0.49 0.46 0.65 0.68 0.45 0.45 0.76 0.59 0.57 0.57 0.56
FML-4 0.73 0.56 0.60 0.59 0.59 0.57 0.60 0.61 0.61 0.57 0.61 0.71 0.61 0.60 - 0.48
L-5 0.80 0.58 0.58 0.55 0.49 0.54 0.53 0.54 0.56 0.55 0.50 0.75 0.55 0.58 - 0.41
CV-16 0.83 0.53 0.53 0.53 0.30 0.33 0.30 0.73 0.70 0.36 0.36 0.80 0.51 0.55 - 0.52
R-20 0.76 0.61 0.61 0.62 0.45 0.46 0.46 0.60 0.60 0.53 - 0.70 0.62 0.64 - 0.62
IR-2 0.97 0.92 0.93 0.92 0.91 0.89 0.90 0.91 0.92 0.91 - 0.92 0.89 0.89 - 0.89
I-3 0.99 0.93 0.93 0.93 0.92 0.91 0.92 0.92 0.93 0.93 - 0.93 0.91 0.90 - 0.89
SV-16 0.66 0.55 0.54 0.54 0.22 0.28 0.27 0.55 0.57 0.29 0.30 0.59 0.50 0.50 0.49 0.52
R-20 0.54 0.49 0.49 0.49 0.36 0.37 0.36 0.44 0.44 0.42 - 0.46 0.44 0.43 0.43 0.36
Table4: Resultsof robustnesswithdiﬀerentdistribution diversity
Dataset Model DistXplore DistXplore/u1D451/u1D453DistXplore/u1D461
MNISTLeNet-4 0.81 0.61 0.76
LeNet-5 0.81 0.63 0.74
FMNISTLeNet-4 0.73 0.62 0.68
LeNet-5 0.80 0.65 0.72
CIFAR-10VGG16 0.83 0.62 0.80
ResNet-20 0.76 0.63 0.74
IR-V2 0.97 0.87 0.93
I-V3 0.99 0.89 0.95
SVHNVGG16 0.66 0.55 0.62
ResNet-20 0.54 0.42 0.47
onlyconsideringIDislesseﬀectiveinimprovingtherobustness,
especiallyonOOD data.
The data generated by diﬀerent testing tools may have diﬀer-
ent distributions, depending on their transformation and guidance
strategies.Allthesedatacouldbethepotentialinputsinthereal-
worlddeployment,andtestcasesgeneratedbyatoolmaynotcover
alldistributions.Forexample,although DistXplore isdesignedto
increasethedistributiondiversity,itdoesnotalwayscoverthedata
distribution from other tools. In general, it can cover more unseen
distributionsif we gradually increasethe distributiondiversity.
AnswerstoRQ3-2 :DistXplore cangeneratetestcaseswith
diversedistributions,whichcanidentifymoreunseendatafor
further robustnessimprovement.
4.2.4 RQ4: Usefulness of Distribution Diversity. Table4shows the
results about the usefulness of the distribution diﬀerence diversity
and target diversity. DistXplore ,DistXplore/u1D451/u1D453, andDistXplore/u1D461rep-
resentstheaccuracyofmodels/f_ine-tunedwithdiﬀerentdata(see
more con/f_iguration details in Section 4.1.4). Note that the number
of data used in DistXplore/u1D451/u1D453,DistXplore/u1D461, andDistXplore are the
same.Comparedtotheresults DistXplore ,wefoundthat theaccu-
racydrops ifonlyconsidering thedistributiondiﬀerence diversity
(DistXplore/u1D451/u1D453)ortargetdiversity( DistXplore/u1D461),whichindicatesthe
usefulnessofboth kindsofdiversityinimprovingthe robustness.
Answers to RQ4 : Bothdistribution diﬀerence diversity and
targetclass diversity are useful inimprovingthe robustness.
4.2.5 RQ5:GeneralizationAbility. Table5showstheresultsonthe
strength of generated errors by diﬀerent methods, i.e., the percent-
age of errors that can be detected by existing detection methods.Theoverallresultsshowthat DistXplore canstillgeneratestrong
errorsthantheselectedbaselines.Moreover,theresultsalsodemon-
strate the generalizabilityof DistXplore to otherdomains.
Discussion on application scope. This paper primarily focuses
on the classi/f_ication task, which is one of the most popular and
important machinelearning tasks, and hasbeen widelystudied in
theresearchareaofDLtesting[ 21,24,27,32,35,43,50,52,54,57,
59,63,64]. While there is much less work on testing generation
tasksin theliteratureduetothechallengeof de/f_iningtestoracles,
i.e., how to de/f_ine the errors. Recently, researchers proposed a
fewmetamorphicrelations[ 17,53]formachinetranslationtasks
to overcome the problem. It is noteworthy that the challenge of
test oracle is orthogonal to the problem we aim to solve in the
paper.Consideringthatnoneoftheexistingworkslookintodata
distribution,webelievethat DistXplore couldalsoplayanimportant
role in generating test cases with better diversity for generation
tasks in view that data distribution is a fundamental concept for
generallearningtasks.
Speci/f_ically, DistXplore canbeextendedtogenerationtasksby
modifying the feedback of distribution diﬀerences. Currently, in
classi/f_icationtasks,weselectotherclassesastargetstoguidethe
generation of test suites for achieving diverse distributions due
to the classi/f_ication characteristics. For generation tasks that do
not have classes, suppose there is a generation model that can
generatehumanfacesfollowingaspeci/f_icdistribution(basedonthe
trainingsamples),wecanselectotherdatasets,suchasImageNet[ 7],
CIFAR [28], or other image datasets, as the targets to guide the
generation of test suites such that the test suites can also have
diversedistributions.However,howtoselectthetargetdistribution
and how eﬀectively they can helpwith the testing require further
explorationandevaluation.Weleavetheextensiontogeneration
tasksas our future work.
Answers to RQ5 :DistXplore is also useful in testing NLP
models.
5 THREATS TO VALIDITY
There aresomethreatsthat could aﬀectthevalidityoftheresults.
The selected models and datasets are threats to the validity. We
mitigatethesethreatsbyselectingthepopulardatasetsandmod-
els that are used by existing DL testing works. The randomness
could be a threat, which is mitigated by generating a large number
of test cases over a relatively long time and running each tool 5
timesinourexperiments.Inaddition,wemakeourexperimental
77DistXplore : Distribution-GuidedTestingforEvaluating andEnhancingDeep Learning Systems ESEC/FSE ’23, December3–9, 2023,San Francisco, CA, USA
Table5: Resultsof bypassingthedefense techniquesfor NLP tasks
Dataset Defense DistXplore PWWS TextFooler
IMDB WDR 0.19 0.96 0.94
AG’s News WDR 0.22 0.98 0.99
resultspubliclyavailable.Theselectionoftheseedinputsisathreat.
We mitigate it by selecting a large number seeds (1,000) that are
usedbyallbaselines.ThelayerselectedforcalculatingtheMMD
could be a threat to aﬀect the results. We mitigate thisproblem by
selectingthecommonlyusedlayer, i.e.,logitslayer.Inthefuture,
we plan to evaluate DistXplore by selecting diﬀerent layers and
their combinations. Another threat is that the empirical robustness
depends on the validation dataset, and the transformations used
in selected tools are diﬀerent, which could be a threat to aﬀect
theresults.Tomitigatethisproblem,wetryourbesttoassemble
a balanced validation dataset comprised of data generated from
diﬀerenttypesoftestingtools(9,000inputsgeneratedbyeachtype
of tool). Moreover, we choose a new set of seeds to generate the
validation dataset in order to avoid the overlapping between the
newtraining dataset andvalidation dataset.
6 RELATED WORK
6.1 Distribution-UnawareTesting
Duetothediﬀerencesbetweentraditionalsoftwareanddeepneural
networks, some coverage criteria have been proposed. The general
ideaistode/f_ine metrics formeasuring thebehaviorsofthetarget
DNNs while the distribution is not explicitly considered. The Neu-
ron Coverage [ 46] isthe /f_irst DL coverage criterion that measures
the percentage of neurons activated by the given inputs. Ma et
al.[35]thenextendedtheNeuronCoverageandproposedasetof
/f_ine-grainedcoveragecriteriasuchask-multisectionNeuronCover-
age(KMNC),NeuronBoundaryCoverage(NBC),andTop-kNeuron
Coverage(TKNC).Althoughthedistributionisnotexplicitlycon-
sidered, there could be some implicit relationship between them.
Forexample,NBCde/f_inesthecoveredupperandlowercornercase
regions, which is more related to OOD data. NPC [ 62] proposes
twopath-basedcoveragecriteriatomeasurethecoverageonthe
decisionlogic.Apathrepresentsapossibledecisionlogic.Basedon
thecoveragecriteria,someautomatedtestingtechniqueshavebeen
developed such as DeepXplore [ 46], DLFuzz [ 15], DeepTest [ 54],
DeepHunter [ 63], DeepStellar[ 12], andTensorFuzz [ 43]
Althoughthesetechniquescouldalsogeneratetestcaseswith
diﬀerent distributions, none of them explicitly considers the dis-
tribution.Forexample,alotoferrorsaregeneratedbuttheymay
followthesimilar distributions.In addition,the existingworksdo
not consider the strength of generated errors. Diﬀerently, DistX-
ploregenerates strong errors that are statistically indistinguishable
andenhancesrobustnesswithdiﬀerentdistributions.
6.2 Distribution-AwareTesting
Recently, some testing works start to discuss the eﬀect of distribu-
tionfortesting,whichisbasedonthefactthataDLmodelistrained
onsampledtrainingdatafollowingaspeci/f_icdistribution.Berend et
al.[4]conductedanempiricalstudyontherelationshipsbetween
data distribution and existing testing techniques. They call for the
attention of data-distribution awareness when designing testingmethods. Zhou et al.[71] study the robustness of DNNs with distri-
butionawareness.Hu etal.[19]studythedistribution-awareseed
selectionmethodsforDNNs.Dola etal.[9]developthedistribution-
awaretestingtechniquethatbasicallygeneratesthein-distribution
databytheVariationalAutoencoders(VAEs).Toledo etal.[9]pro-
posedthedistribution-awareveri/f_ication.Itusesagenerativemodel
to represent the data distribution of the trained model, and then
changes the original model such that all the inputs to the DNN fol-
lowthelearneddistribution.Themostrecentwork[ 22]proposeda
hierarchical distribution-aware testing method that measures both
ofglobaldistributionandlocal distribution.
Besides, Kim et al.[27] propose LSA and DSA to measure the
surprise adequacy (SA) of the test cases, i.e., the surprise degree of
asingletestcasecomparedwiththetrainingdata.Althoughboth
DistXplore andSAconsiderthedistancebetweentestcase(s)and
training data, there are some key diﬀerences: 1) DistXplore mea-
sures the distribution diﬀerence between two setsof data while SA
measures the surprise of a singletest case In addition, considering
the distance calculation, DistXplore is more eﬃcient. 2) DistXplore
ismore/f_ine-grainedandconsidersintra-classandinter-classdis-
tribution shifts while SA mainly considers the distance between
a test case and all training data. 3) The goals are not totally the
same.SAisatestselectionmethodthatmainlyselectssurprising
data. However, it is not clear whether the surprising data (from
SA) is eﬀective in generating hard-to-detect errors or enhancing
model’srobustness,whichisourmainfocus.Theevaluationresults
demonstratethat DistXplore ismore eﬀective.
7 CONCLUSION
In this paper, we propose a distribution-guided testing approach
to evaluate and enhance DL models. To the best of our knowledge,
thisisthe/f_irstworkthatexplicitlygeneratestestcaseswithdiverse
distributions.Wediscussedtherelationshipbetweenvalidityand
distribution, where valid out-of-distribution data is ignored by ex-
isting distribution-aware testing. We evaluated the eﬀectiveness of
DistXplore on10modelsandcompareditwith14state-of-the-art
tools. The results demonstrate that DistXplore is eﬃcient and eﬀec-
tiveindiscoveringhard-to-defenderrorsandimprovingrobustness.
Data Availability : We provide the source code and data on:
https://github.com/l1lk/DistXplore
ACKNOWLEDGMENTS
ThisresearchispartiallysupportedbytheNationalResearchFoun-
dation,Singapore,andtheCyberSecurityAgencyunderitsNational
Cybersecurity R&D Programme (NCRP25-P04-TAICeN), the Lee
KongChianFellowship,theNationalResearchFoundationSinga-
poreandDSONationalLaboratoriesundertheAISingaporePro-
gramme(AISGAwardNo:AISG2-GC-2023-008),NationalKeyR&D
Program of China (2020AAA0107702), National Natural Science
FoundationofChina(U21B2018,62161160337,61822309,U20B2049,
61773310, U1736205, 61802166) and Shaanxi Province Key Industry
Innovation Program (2021ZDLGY01-02). Any opinions, /f_indings
and conclusions or recommendations expressed in this material
are thoseoftheauthor(s)anddo not re/f_lectthe viewsofNational
Research Foundation, Singapore and Cyber Security Agency of
Singapore.
78ESEC/FSE ’23, December3–9, 2023,San Francisco, CA, USA LongtianWang,XiaofeiXie, XiaoningDu,Meng Tian,QingGuo, Zheng Yang,andChao Shen
REFERENCES
[1]Zohreh Aghababaeyan, Manel Abdellatif, Mahboubeh Dadkhah, and Lionel
Briand.2023. DeepGD:AMulti-ObjectiveBlack-BoxTestSelectionApproach
for Deep Neural Networks. arXiv preprint arXiv:2303.04878 (2023). https:
//doi.org/10.48550/arXiv.2303.04878
[2] Anonymous. 2022. DistXplore .https://sites.google.com/view/distxplore
[3]Muhammad Hilmi Asyro/f_i, Zhou Yang, and David Lo. 2021. Crossasr++: A
modular diﬀerential testing framework for automatic speech recognition. In
Proceedingsofthe29thACMJointMeetingonEuropeanSoftwareEngineeringCon-
ferenceandSymposiumontheFoundationsofSoftwareEngineering .1575–1579.
https://doi.org/10.1145/3468264.3473124
[4]David Berend, Xiaofei Xie, Lei Ma, Lingjun Zhou, Yang Liu, Chi Xu, and Jianjun
Zhao. 2020. Cats arenot /f_ish: Deeplearning testing calls for out-of-distribution
awareness.In Proceedingsofthe35thIEEE/ACMInternationalConferenceonAuto-
matedSoftwareEngineering .1041–1052. https://doi.org/10.1145/3324884.3416609
[5]Nicholas Carlini and David Wagner. 2017. Towards evaluating the robustness of
neural networks. In 2017 ieee symposium on security and privacy (sp) . Ieee, 39–57.
https://doi.org/10.48550/arXiv.1608.04644
[6]Jaganmohan Chandrasekaran, Yu Lei, Raghu Kacker, and D Richard Kuhn.
2021. A combinatorial approach to testing deep neural network-based au-
tonomous driving systems. In 2021 IEEE International Conference on Software
Testing, Veri/f_ication and Validation Workshops (ICSTW) . IEEE, 57–66. https:
//doi.org/10.1109/ICSTW52544.2021.00022
[7]Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. 2009. Im-
agenet: A large-scale hierarchical image database. In 2009 IEEE conference on
computervisionandpattern recognition .Ieee,248–255. https://doi.org/10.1109/
CVPR.2009.5206848
[8]JacobDevlin,Ming-WeiChang,KentonLee,andKristinaToutanova.2019. BERT:
Pre-trainingofDeepBidirectionalTransformersforLanguageUnderstanding.In
Proceedingsofthe2019ConferenceoftheNorthAmerican ChapteroftheAssocia-
tionforComputationalLinguistics:HumanLanguageTechnologies,NAACL-HLT
2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers) , Jill
Burstein,ChristyDoran,andThamarSolorio(Eds.).AssociationforComputa-
tionalLinguistics,4171–4186. https://doi.org/10.18653/V1/N19-1423
[9]Swaroopa Dola, Matthew B Dwyer, and Mary Lou Soﬀa. 2021. Distribution-
aware testing of neural networks using generative models. In 2021 IEEE/ACM
43rd International Conference on Software Engineering (ICSE) . IEEE, 226–237.
https://doi.org/10.1109/ICSE43902.2021.00032
[10]SwaroopaDola,MatthewBDwyer,andMaryLouSoﬀa.2023. InputDistribution
Coverage: Measuring Feature Interaction Adequacy in Neural Network Testing.
ACMTransactionsonSoftwareEngineeringandMethodology 32,3(2023),1–48.
https://doi.org/10.1145/3576040
[11]YinpengDong,TianyuPang,HangSu,andJunZhu.2019. Evadingdefensesto
transferableadversarialexamplesbytranslation-invariantattacks.In Proceedings
oftheIEEE/CVFConferenceonComputerVisionandPatternRecognition .4312–4321.
https://doi.org/10.1109/CVPR.2019.00444
[12]Xiaoning Du, Xiaofei Xie, Yi Li, Lei Ma, Yang Liu, and Jianjun Zhao. 2019. Deep-
stellar:Model-basedquantitativeanalysisofstatefuldeeplearningsystems.In
Proceedingsofthe201927thACMJointMeetingonEuropeanSoftwareEngineering
ConferenceandSymposiumon theFoundationsofSoftwareEngineering .477–487.
https://doi.org/10.1145/3338906.3338954
[13]Arthur Gretton, Karsten Borgwardt, Malte Rasch, Bernhard Schölkopf, and Alex
Smola.2006. A kernel methodforthe two-sample-problem. Advancesinneural
informationprocessingsystems 19(2006). https://doi.org/10.5555/2976456.2976521
[14]Arthur Gretton,Karsten MBorgwardt,MalteJ Rasch,BernhardSchölkopf,and
Alexander Smola. 2012. A kernel two-sample test. The Journal of Machine
LearningResearch 13,1(2012),723–773. https://doi.org/10.5555/2188385.2188410
[15]Jianmin Guo, Yu Jiang, Yue Zhao, Quan Chen, and Jiaguang Sun. 2018. Dl-
fuzz: Diﬀerential fuzzing testing of deep learning systems. In Proceedings of
the201826thACMJointMeetingonEuropeanSoftwareEngineeringConference
and Symposium on the Foundations of Software Engineering . 739–743. https:
//doi.org/10.1145/3236024.3264835
[16]QianyuGuo,SenChen,XiaofeiXie,LeiMa,QiangHu,HongtaoLiu,YangLiu,
Jianjun Zhao, and Xiaohong Li. 2019. An empirical study towards characterizing
deep learning development and deployment across diﬀerent frameworks and
platforms.In 201934thIEEE/ACMInternationalConferenceonAutomatedSoftware
Engineering (ASE) . IEEE,810–822. https://doi.org/10.1109/ASE.2019.00080
[17]ShashijGupta,PinjiaHe,ClaraMeister,andZhendongSu.2020. Machinetransla-
tiontestingviapathologicalinvariance.In Proceedingsofthe28thACMJointMeet-
ing on European Software Engineering Conference and Symposium on the Founda-
tions of Software Engineering . 863–875. https://doi.org/10.1145/3368089.3409756
[18]GuoshengHu,YongxinYang,DongYi,JosefKittler,WilliamChristmas,StanZ
Li, and Timothy Hospedales. 2015. When face recognition meets with deep
learning:anevaluationofconvolutionalneuralnetworksforfacerecognition.
InProceedings of the IEEE international conference on computer vision workshops .
142–150. https://doi.org/10.1109/ICCVW.2015.58[19]QiangHu,YuejunGuo,MaximeCordy,XiaofeiXie,Lei Ma,MikePapadakis,and
YvesLeTraon.2022. Anempiricalstudyondatadistribution-awaretestselection
for deeplearning enhancement. ACMTransactions onSoftwareEngineeringand
Methodology (TOSEM) 31,4 (2022), 1–30. https://doi.org/10.1145/3511598
[20]QiangHu,YuejunGuo,XiaofeiXie,MaximeCordy,MikePapadakis,andYves
LeTraon.2023. LaF:Labeling-FreeModelSelectionforAutomatedDeepNeu-
ral Network Reusing. ACM Trans. Softw. Eng. Methodol. (jul 2023). https:
//doi.org/10.1145/3611666 Just Accepted.
[21]Qiang Hu, Yuejun Guo, Xiaofei Xie, Maxime Cordy, Mike Papadakis, Lei Ma,
andYvesLeTraon.2023. Aries:EﬃcientTestingofDeepNeuralNetworksvia
Labeling-Free Accuracy Estimation. In 2023 IEEE/ACM 45th International Confer-
enceonSoftwareEngineering(ICSE) .IEEE,1776–1787. https://doi.org/10.1109/
ICSE48619.2023.00152
[22]WeiHuang,XingyuZhao,Alec Banks, VictoriaCox, and XiaoweiHuang.2022.
Hierarchical Distribution-Aware Testing of Deep Learning. arXiv preprint
arXiv:2205.08589 (2022).https://doi.org/10.1109/ICSE43902.2021.00032
[23]Di Jin, Zhijing Jin, Joey Tianyi Zhou, and Peter Szolovits. 2020. Is bert really
robust?astrongbaselinefornaturallanguageattackontextclassi/f_icationand
entailment. In Proceedings of the AAAI conference on arti/f_icial intelligence , Vol. 34.
8018–8025. https://doi.org/10.1609/AAAI.V34I05.6311
[24]Haibo Jin, Ruoxi Chen, Haibin Zheng, Jinyin Chen, Yao Cheng, Yue Yu, Tiem-
ingChen,andXianglongLiu.2023. Excitementsurfeitedturnstoerrors:Deep
learning testing framework based on excitable neurons. Information Sciences 637
(2023), 118936. https://doi.org/10.1016/j.ins.2023.118936
[25]Jaeyoung Kang, Behnam Khaleghi, Tajana Rosing, and Yeseong Kim. 2022.
Openhd: A gpu-powered framework for hyperdimensional computing. IEEE
Trans.Comput. 71,11(2022),2753–2765. https://doi.org/10.1109/TC.2022.3179226
[26]Yigitcan Kaya, Bilal Zafar, Sergul Aydore, Nathalie Rauschmayr, and Krishnaram
Kenthapadi.2022. Generatingdistributionaladversarialexamplestoevadestatis-
tical detectors. (2022).
[27]Jinhan Kim, Robert Feldt, and Shin Yoo. 2019. Guiding deep learning system
testing using surprise adequacy. In 2019 IEEE/ACM 41st InternationalConference
onSoftwareEngineering(ICSE) .IEEE,1039–1049. https://doi.org/10.1109/ICSE.
2019.00108
[28]AlexKrizhevsky,GeoﬀreyHinton,etal .2009. Learningmultiplelayersoffeatures
from tinyimages. (2009).
[29]AlexeyKurakin,IanJGoodfellow,andSamyBengio.2018. Adversarialexamples
in the physical world. In Arti/f_icial intelligence safety and security . Chapman and
Hall/CRC, 99–112. https://doi.org/10.48550/arXiv.1607.02533
[30]JokinLabaien,EkhiZugasti,andXabierDeCarlos. 2021. DA-DGCEx: Ensuring
validity of deep guided counterfactual explanations with distribution-aware au-
toencoder loss. arXiv preprint arXiv:2104.09062 (2021).https://doi.org/10.48550/
arXiv.2104.09062
[31]YandongLi,LijunLi,LiqiangWang,TongZhang,andBoqingGong.2019.Nattack:
Learning the distributions of adversarial examples for an improved black-box
attackondeepneuralnetworks.In InternationalConferenceonMachineLearning .
PMLR, 3866–3876. https://doi.org/10.48550/arXiv.1905.00441
[32]ZhongLi,MinxuePan,YuPei,TianZhang,LinzhangWang,andXuandongLi.
2022. RobustLearningofDeepPredictiveModelsfromNoisyandImbalanced
Software Engineering Datasets. In 37th IEEE/ACM International Conference on
Automated Software Engineering . 1–13.https://doi.org/10.1145/3551349.3556941
[33]JiadongLin,ChuanbiaoSong,Kun He,LiweiWang, andJohnE Hopcroft.2019.
Nesterov accelerated gradient and scale invariance for adversarial attacks. arXiv
preprint arXiv:1908.06281 (2019).https://doi.org/10.48550/arXiv.1908.06281
[34]Yuying Liu, Pin Yang, Peng Jia, Ziheng He, and Hairu Luo. 2022. MalFuzz:
Coverage-guided fuzzing on deep learning-based malwareclassi/f_ication model.
Plosone17,9 (2022), e0273804. https://doi.org/10.1371/journal.pone.0273804
[35]Lei Ma, Felix Juefei-Xu, Fuyuan Zhang, Jiyuan Sun, Minhui Xue, Bo Li, Chun-
yangChen,TingSu,LiLi,YangLiu,etal .2018. Deepgauge:Multi-granularity
testingcriteriafordeeplearningsystems.In Proceedingsofthe33rdACM/IEEE
International Conference on Automated Software Engineering . 120–131. https:
//doi.org/10.1145/3238147.3238202
[36]AndrewMaas,RaymondEDaly,PeterTPham,DanHuang, AndrewYNg,and
ChristopherPotts.2011. Learningwordvectorsforsentimentanalysis.In Pro-
ceedingsofthe49thannualmeetingoftheassociationforcomputationallinguistics:
Human language technologies . 142–150. https://doi.org/10.5555/2002472.2002491
[37]Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and
AdrianVladu.2017.Towardsdeeplearningmodelsresistanttoadversarialattacks.
stat1050 (2017), 9. https://doi.org/10.48550/arXiv.1706.06083
[38]Arun Mallya and Svetlana Lazebnik. 2018. Packnet: Adding multiple tasks to
a single network by iterative pruning. In Proceedings of the IEEE conference on
Computer Vision and Pattern Recognition . 7765–7773. https://doi.org/10.1109/
CVPR.2018.00810
[39]Brad L Miller, David E Goldberg, et al .1995. Genetic algorithms, tourna-
ment selection, and the eﬀects of noise. Complex systems 9, 3 (1995), 193–212.
https://doi.org/10.1162/evco.1996.4.2.113
[40]AH Money, JF Aﬄeck-Graves, ML Hart, and GDI Barr. 1982. The linear re-
gression model: Lp norm estimation and the choice of p. Communications in
79DistXplore : Distribution-GuidedTestingforEvaluating andEnhancingDeep Learning Systems ESEC/FSE ’23, December3–9, 2023,San Francisco, CA, USA
Statistics-Simulation and Computation 11, 1 (1982), 89–109. https://doi.org/10.
1080/03610918208812247
[41]Edoardo Mosca, Shreyash Agarwal, Javier Rando-Ramirez, and Georg Groh.
2022. " That Is a Suspicious Reaction!": Interpreting Logits Variation to De-
tect NLP Adversarial Attacks. arXiv preprint arXiv:2204.04636 (2022).https:
//doi.org/10.18653/v1/2022.acl-long.538
[42]Khan Muhammad,Amin Ullah,JaimeLloret, Javier DelSer, andVictorHugo C.
de Albuquerque. 2021. Deep Learning for Safe Autonomous Driving: Current
ChallengesandFutureDirections. IEEETransactionsonIntelligentTransportation
Systems22,7 (2021), 4316–4336. https://doi.org/10.1109/TITS.2020.3032227
[43]Augustus Odena and Ian J. Goodfellow. 2019. TensorFuzz: Debugging Neural
Networks with Coverage-Guided Fuzzing. In ICML.https://doi.org/10.48550/
arXiv.1807.10875
[44]LiwenOuyangandAaronKey.2021. MaximumMeanDiscrepancyforGener-
alizationinthePresenceofDistributionandMissingnessShift. arXivpreprint
arXiv:2111.10344 (2021).https://doi.org/10.48550/arXiv.2111.10344
[45]Nicolas Papernot, Patrick McDaniel, Somesh Jha, Matt Fredrikson, Z Berkay Ce-
lik, and Ananthram Swami. 2016. The limitations of deep learning in adversarial
settings. In 2016 IEEE European symposium on security and privacy (EuroS&P) .
IEEE,372–387. https://doi.org/10.1109/EuroSP.2016.36
[46]KexinPei,YinzhiCao,JunfengYang,andSumanJana.2017. Deepxplore:Auto-
mated whitebox testing of deep learning systems. In proceedings of the 26th Sym-
posiumonOperatingSystemsPrinciples . 1–18.https://doi.org/10.1145/3361566
[47]AdnanQayyum,JunaidQadir,MuhammadBilal,andAlaAl-Fuqaha.2021. Secure
andRobustMachineLearningforHealthcare:ASurvey. IEEEReviewsinBiomed-
ical Engineering 14 (2021), 156–180. https://doi.org/10.1109/RBME.2020.3013489
[48]Shuhuai Ren, Yihe Deng, Kun He, and Wanxiang Che. 2019. Generating natu-
rallanguageadversarialexamplesthroughprobabilityweightedwordsaliency.
InProceedings of the 57th annual meeting of the association for computational
linguistics . 1085–1097. https://doi.org/10.18653/v1/P19-1103
[49]SourjyaRoy,PriyadarshiniPanda,GopalakrishnanSrinivasan,andAnandRaghu-
nathan. 2020. Pruning /f_ilters while training for eﬃciently optimizing deep
learning networks. In 2020 International Joint Conference on Neural Networks
(IJCNN). IEEE,1–7. https://doi.org/10.1109/IJCNN48605.2020.9207588
[50]Jasmine Sekhon and Cody Fleming. 2019. Towards improved testing for deep
learning. In 2019 IEEE/ACM 41st International Conference on Software Engi-
neering: New Ideas and Emerging Results (ICSE-NIER) . IEEE, 85–88. https:
//doi.org/10.1109/ICSE-NIER.2019.00030
[51]IngoSteinwart.2001. Onthein/f_luenceofthekernelontheconsistencyofsup-
port vector machines. Journal of machine learning research 2, Nov (2001), 67–93.
https://doi.org/10.1162/153244302760185252
[52]Youcheng Sun, Min Wu, Wenjie Ruan, Xiaowei Huang, Marta Kwiatkowska, and
DanielKroening.2018. Concolictestingfordeepneuralnetworks.In Proceedings
ofthe33rdACM/IEEEInternationalConferenceonAutomatedSoftwareEngineering .
109–119. https://doi.org/10.1145/3238147.3238172
[53]Zeyu Sun, Jie M Zhang, Yingfei Xiong, Mark Harman, Mike Papadakis, and
Lu Zhang. 2022. Improving machine translation systems via isotopic replace-
ment. InProceedings of the 44th International Conference on Software Engineering .
1181–1192. https://doi.org/10.1145/3510003.3510206
[54]Yuchi Tian, Kexin Pei, Suman Jana, and Baishakhi Ray. 2018. Deeptest: Auto-
matedtestingofdeep-neural-network-drivenautonomouscars.In Proceedings
of the 40th international conference on software engineering . 303–314. https:
//doi.org/10.1145/3180155.3180220
[55]Felipe Toledo, David Shriver, Sebastian Elbaum, and Matthew B Dwyer. 2021.
Distribution Models for Falsi/f_ication and Veri/f_ication of DNNs. In 2021 36th
IEEE/ACM International Conference on Automated Software Engineering (ASE) .
IEEE,317–329. https://doi.org/10.1109/ASE51524.2021.9678590
[56]HuiyanWang,JingweiXu,ChangXu,XiaoxingMa,andJianLu.2020. Dissector:
Inputvalidationfordeeplearningapplicationsbycrossing-layerdissection.In
2020 IEEE/ACM 42nd International Conference on Software Engineering (ICSE) .
IEEE,727–738. https://doi.org/10.1145/3377811.3380379
[57]JingyiWang,JialuoChen,YouchengSun, XingjunMa,DongxiaWang,JunSun,
and Peng Cheng. 2021. Robot: Robustness-oriented testing for deep learning
systems.In 2021IEEE/ACM43rdInternationalConferenceonSoftwareEngineering
(ICSE). IEEE,300–311. https://doi.org/10.1109/ICSE43902.2021.00038[58]Jingyi Wang, Guoliang Dong, Jun Sun, Xinyu Wang, and Peixin Zhang. 2019.
Adversarialsample detection fordeepneural network through modelmutation
testing. In 2019 IEEE/ACM 41stInternational Conference on SoftwareEngineering
(ICSE). IEEE,1245–1256. https://doi.org/10.1109/ICSE.2019.00126
[59]YanXiao,IvanBeschastnikh,YunLin,RajdeepSinghHundal,XiaofeiXie,DavidS
Rosenblum,andJinSongDong.2022. Self-checkingdeepneuralnetworksfor
anomalies andadversaries in deployment. IEEE Transactions on Dependable and
SecureComputing (2022).https://doi.org/10.1109/TDSC.2022.3200421
[60]Cihang Xie, Zhishuai Zhang, Yuyin Zhou, Song Bai, Jianyu Wang, Zhou Ren,
and AlanLYuille. 2019. Improvingtransferabilityof adversarial exampleswith
input diversity. In Proceedings of the IEEE/CVF Conference on Computer Vision
and PatternRecognition . 2730–2739. https://doi.org/10.1109/CVPR.2019.00284
[61]Xiaofei Xie, Hongxu Chen, Yi Li, Lei Ma, Yang Liu, and Jianjun Zhao. 2019.
Coverage-guidedfuzzingforfeedforwardneuralnetworks.In 201934thIEEE/ACM
InternationalConference onAutomatedSoftware Engineering (ASE) . IEEE,1162–
1165.https://doi.org/10.1109/ASE.2019.00127
[62]XiaofeiXie,TianlinLi,JianWang,LeiMa,QingGuo,FelixJuefei-Xu,andYangLiu.
2022. NPC: N euron P ath C overage via Characterizing Decision Logic of Deep
NeuralNetworks. ACMTransactionsonSoftwareEngineeringandMethodology
(TOSEM) 31,3 (2022), 1–27. https://doi.org/10.1145/3490489
[63]XiaofeiXie,LeiMa,FelixJuefei-Xu,MinhuiXue,HongxuChen,YangLiu,Jianjun
Zhao, Bo Li, Jianxiong Yin, and Simon See. 2019. Deephunter: a coverage-guided
fuzztestingframeworkfordeepneuralnetworks.In Proceedingsofthe28thACM
SIGSOFT International Symposium on Software Testing and Analysis . 146–157.
https://doi.org/10.1145/3293882.3330579
[64]BingYu,HuaQi,QingGuo,FelixJuefei-Xu,XiaofeiXie,LeiMa,andJianjunZhao.
2020. Deeprepair: Style-guided repairing for dnnsin the real-worldoperational
environment. arXiv preprint arXiv:2011.09884 (2020).https://doi.org/10.1109/TR.
2021.3096332
[65]Daniel Hao Xian Yuen, Andrew Yong Chen Pang, Zhou Yang, Chun Yong
Chong, Mei Kuan Lim, and David Lo. 2023. ASDF: A Diﬀerential Testing
Framework for Automatic Speech Recognition Systems. In 2023 IEEE Con-
ference on Software Testing, Veri/f_ication and Validation (ICST) . IEEE, 461–463.
https://doi.org/10.1109/ICST57152.2023.00050
[66]Jianping Zhang, Jen-tse Huang, Wenxuan Wang, Yichen Li, Weibin Wu, Xi-
aosen Wang, Yuxin Su, and Michael R Lyu. 2023. Improving the Transferabil-
ity of Adversarial Samples by Path-Augmented Method. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition . 8173–8182.
https://doi.org/10.1109/CVPR52729.2023.00790
[67]Xiang Zhang, Junbo Zhao, and Yann LeCun. 2015. Character-level convolutional
networksfortextclassi/f_ication. Advancesinneuralinformationprocessingsystems
28(2015). https://doi.org/10.5555/2969239.2969312
[68]Zixing Zhang, Jürgen Geiger, Jouni Pohjalainen, Amr El-Desoky Mousa, Wenyu
Jin,andBjörnSchuller.2018. Deeplearningforenvironmentallyrobustspeech
recognition:Anoverviewofrecentdevelopments. ACMTransactionsonIntelligent
Systems and Technology (TIST) 9, 5 (2018), 1–28. https://doi.org/10.1145/3178115
[69]Zhe Zhao, Guangke Chen, Jingyi Wang, Yiwei Yang, Fu Song, and Jun Sun. 2021.
Attack as defense: Characterizing adversarial examples using robustness. In Pro-
ceedingsofthe30thACMSIGSOFTInternationalSymposiumonSoftwareTesting
and Analysis . 42–55.https://doi.org/10.1145/3460319.3464822
[70]Tianhang Zheng, Changyou Chen, and Kui Ren. 2019. Distributionally adversar-
ial attack. In Proceedings of the AAAI Conference on Arti/f_icial Intelligence , Vol. 33.
2253–2260. https://doi.org/10.1609/aaai.v33i01.33012253
[71]LingjunZhou,BingYu,DavidBerend,XiaofeiXie,XiaohongLi,JianjunZhao,
and Xusheng Liu.2020. Anempirical study on robustness of DNNs with out-of-
distribution awareness. In 2020 27th Asia-Paci/f_ic Software Engineering Conference
(APSEC). IEEE,266–275. https://doi.org/10.1109/APSEC51365.2020.00035
[72]TaherehZohdinasab,VincenzoRiccio,AlessioGambi,andPaoloTonella.2021.
Deephyperion: exploring the feature space of deep learning-based systems
through illumination search. In Proceedings of the 30th ACM SIGSOFT Inter-
national Symposium on Software Testing and Analysis . 79–90.https://doi.org/10.
1145/3460319.3464811
[73]TaherehZohdinasab,VincenzoRiccio,AlessioGambi,andPaoloTonella.2023.
Eﬃcientandeﬀectivefeaturespaceexplorationfortestingdeeplearningsystems.
ACMTransactionsonSoftwareEngineeringandMethodology 32,2(2023),1–38.
https://doi.org/10.1145/3544792
Received 2023-02-02; accepted 2023-07-27
80