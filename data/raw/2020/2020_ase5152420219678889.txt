Restoring the Executability of Jupyter Notebooks by
Automatic Upgrade of Deprecated APIs
Chenguang Zhu1, Ripon K. Saha2, Mukul R. Prasad2, and Sarfraz Khurshid1
1The University of Texas at Austin,2Fujitsu Research of America, Inc.
Email: cgzhu@utexas.edu, rsaha@fujitsu.com, mukul@fujitsu.com, khurshid@utexas.edu
Abstract ‚ÄîData scientists typically practice exploratory pro-
gramming using computational notebooks, to comprehend new
data and extract insights. To do this they iteratively reÔ¨Åne their
code, actively trying to re-use and re-purpose solutions created
by other data scientists, in real time. However, recent studies
have shown that a vast majority of publicly available notebooks
cannot be executed out of the box. One of the prominent reasons
is the deprecation of data science APIs used in such notebooks,
due to the rapid evolution of data science libraries. In this work
we propose R ELANCER , an automatic technique that restores
the executability of broken Jupyter Notebooks, in near real time,
by upgrading deprecated APIs. R ELANCER employs an iterative
runtime-error-driven approach to identify and Ô¨Åx one API issue
at a time. This is supported by a machine-learned model which
uses the runtime error message to predict the kind of API repair
needed - an update in the API or package name, a parameter, or
a parameter value. Then R ELANCER creates a search space of
candidate repairs by combining knowledge from API migration
examples on GitHub as well as the API documentation and
employs a second machine-learned model to rank this space of
candidate mappings. An evaluation of R ELANCER on a curated
dataset of 255 un-executable Jupyter Notebooks from Kaggle
shows that R ELANCER can successfully restore the executability
of 56% of the subjects, while baselines relying on just GitHub
examples and just API documentation can only Ô¨Åx 38% and 36%
of the subjects respectively. Further, pursuant to its real-time
use case, R ELANCER can restore execution to 49% of subjects,
within a 5 minute time limit, while a baseline lacking its machine
learning models can only Ô¨Åx 24%.
Index Terms‚Äîdata science, API migration, software evolution
I. I NTRODUCTION
The ready availability of sensors and inexpensive compute
resources, coupled with a number of signiÔ¨Åcant advances in
machine learning and data analytics has fueled an explosive
growth in the Ô¨Åeld of data science [1], [2]. Kaggle, the relatively
nascent online community of data scientists, already boasts
over 6 million users and hosts 50,000 public datasets [3].
Data scientists typically follow the paradigm of exploratory
programming, iteratively reÔ¨Åning code to comprehend new data
and extract meaningful insights from it [ 4], [5]. To do this they
actively try to re-use and re-purpose solutions created by other
data scientists, in real time. Indeed, Kaggle proudly claims that:
"Inside Kaggle you‚Äôll Ô¨Ånd all the code & data you need to do
your data science work" [3]. Jupyter Notebooks [ 6] - interactive
computational structures which interleave code snippets, natural
language text, computation results, and visualizations - have
become the medium of choice for data scientists to program,
record, and share data analyses [ 7], [8]. Kaggle hosts 400,000such public notebooks [ 3]. Similarly, there are currently nearly
10 million Jupyter Notebooks hosted on GitHub, with the
number having grown 8-fold over just the last two years [9].
A data scientist pursuing exploratory programming by re-
using existing Jupyter Notebooks would require such note-
books to be easily reproducible or at the least executable.
In fact, producing "reproducible computational workÔ¨Çows"
was intended to be one of the deÔ¨Åning features of Jupyter
Notebooks (and computational notebooks in general) [ 10],
[11]. However, this promise has not been realized in practice.
A large-scale empirical study by Pimental et al. on Jupyter
Notebooks projects on GitHub found that only 24.11% of
their selected notebooks could execute without errors, and
only 4.03% reproduced the original results [ 12]. The concerns
about executability are echoed in a recent survey of data
scientists [ 13]. Similarly, our study of a sample of machine
learning notebooks on Kaggle (reported in Section II-A)
revealed that 47% of them could not be executed.
Recent studies have identiÔ¨Åed a number of root causes for
why archived Jupyter Notebooks cannot be easily re-executed.
These include, ambiguity in the execution order of the notebook
cells [ 14], lack of knowledge of the notebook‚Äôs execution
environment [ 15], and references to external resources (such
as data on external servers) [ 12], [14]. An emerging body of
work also aims at restoring the executability of such notebooks
by automatically inferring correct cell execution order [ 14]
or inferring an appropriate execution environment [ 15]. Other
studies have highlighted the landscape of rapidly evolving
Python libraries, particularly data science libraries, which
leads to unhandled deprecation issues in programs using these
libraries [ 16], [17], [18]. In fact, our study of Kaggle notebooks
(Section II-A) found that deprecated APIs was at least one of
the reasons (if not the only reason) for the unexecutability of
at least 31% of the unexecutable notebooks. Therefore, in this
paper, we propose a technique to restore the executability of
Jupyter Notebooks by automatically diagnosing, inferring, and
upgrading deprecated APIs used by the notebook.
There is a rich body of existing work on computing API
mappings across pairs of libraries, languages, or platforms,
to be then used to migrate a project from a source library
(platform) to a target library (platform) [ 19], [20], [21], [22],
[23], [24], [25], [26]. There is also recent work that can
perform an end-to-end migration [ 27], [28], [18]. However,
our target use-case - automatically making broken Jupyter
Notebooks executable in near real-time, by Ô¨Åxing deprecated
2402021 36th IEEE/ACM International Conference on Automated Software Engineering (ASE)
DOI 10.1109/ASE51524.2021.000312021 36th IEEE/ACM International Conference on Automated Software Engineering (ASE) | 978-1-6654-0337-5/21/$31.00 ¬©2021 IEEE | DOI: 10.1109/ASE51524.2021.9678889
978-1-6654-0337-5/21/$31.00  ¬©2021  IEEE
APIs - presents some unique challenges and opportunities,
which precludes the direct application of existing solutions.
First, many of the API mapping techniques [ 20], [23], [29], [30],
[31], [32] only create the mappings rather than perform an end-
to-end migration, which our use-case mandates. Second, all the
techniques that perform complete migration [ 27], [28], [18] rely
on a reference implementation in the source library/platform
as a strong oracle for migration. Our use-case lacks such a
reference implementation - we only have an unexecutable
notebook. Third, each of the existing techniques rely on
a single information source to compute the mappings or
migration, typically either existing examples of updates (e.g.,
on GitHub) [ 27], [28] or API documentation [ 18]. However,
given the rapid evolution of data science libraries, public
examples of recent API changes may not exist [ 18]. Further,
even the API documentation may not comprehensively capture
all deprecation information [ 16]. This motivates a migration
technique that combines knowledge from both sources. Finally,
our use-case of supporting exploratory programming requires
a technique that can Ô¨Åx broken notebooks (chosen by the user
for exploration) in near real time. Traditional API migration
techniques are not similarly constrained.
Insights. We design our approach based on three key insights.
First, we observe that individual API upgrades (even different
instances of the same change) can be successfully diagnosed
and performed independent of each other, i.e., one instance
at a time. Second, we build on the observation made in other
recent work [ 17], [18] that runtime error messages in Python,
in particular those of Python data science programs, are often
accurate enough to unambiguously diagnose the broad cause
of error. SpeciÔ¨Åcally, we observe that for deprecated APIs the
runtime error can be used to accurately Ô¨Çag the kind of program
element requiring a change - the API name or package name,
a parameter, or the value of a parameter. The third insight
is that previous instances of API upgrades (e.g., on GitHub)
and API library documentation are complementary sources
of knowledge that can collectively inform an automated API
migration technique.
Proposed approach. Based on these insights, in this work
we propose R ELANCER1, a technique for automatically
restoring the executability of broken Jupyter Notebooks, in near
real-time, by upgrading deprecated APIs. R ELANCER employs
a divide-and-conquer approach to upgrade a broken notebook,
using runtime errors to iteratively identify and Ô¨Åx one API
issue at a time. This iterative strategy is supported by two
key components. The Ô¨Årst component is a machine learned
model that uses the runtime error message to predict the kind
of repair action to perform, speciÔ¨Åcally a change in the API
name (or package), a parameter name, or a parameter value.
This decision accelerates the search by directing focus to the
appropriate search space. R ELANCER creates this space by
aggregating candidates from both GitHub examples as well as
API documentation. In the case of name changes to the API or
1from the French word meaning to revive, since R ELANCER revives broken
notebooks by restoring their executability.its package, this space could be fairly large. Thus, R ELANCER
uses a second machine learned model to produce a ranked list
of candidate API mappings organically combining knowledge
from the two sources.
We evaluate R ELANCER on a curated dataset of 255
unexecutable Jupyter Notebooks from Kaggle. R ELANCER
can successfully restore the executability of 56% of the
subjects, while baselines relying on just GitHub examples
and just API documentation can only Ô¨Åx 38% and 36% of
the subjects respectively. We note that a response time of up
to a few minutes is recognized as real-time or near real-time
performance for Big Data and data analytics applications [ 33],
[34]. Therefore, we use 5 minutes as a threshold of near real-
time performance for the purposes of this paper. Pursuant to its
real-time use case, within a 5 minute time-span R ELANCER can
restore execution to 49% of subjects while a baseline lacking
its machine learning models can only Ô¨Åx 24%.
This paper makes the following contributions:
Problem: We highlight the problem of restoring execution
of Jupyter Notebooks by upgrading deprecated APIs, in
real time, in order to support the use-case of exploratory
programming typically employed by data scientists.
Technique: An automated machine learning based tech-
nique R ELANCER , to orchestrate this upgrade for unexe-
cutable notebooks.
Evaluation: An evaluation of R ELANCER on a curated
dataset of 255 unexecutable Jupyter Notebooks mined
from Kaggle, and ablation studies comparing its perfor-
mance against 5 different baselines.
Implementation & Dataset: A public release of the
source code and dataset used in this paper, to promote
replication and open science, is available at https://sites.
google.com/view/relancer.
II. M OTIVATION
In this work we restrict our scope to Jupyter Notebooks
on Kaggle, the most popular online forum for data scientists.
Further, we focus on notebooks written in Python, the language
of choice in data science [ 9] and speciÔ¨Åcally ones performing
predictive (i.e., machine learning (ML)) tasks, since ML is one
of most important end-points for data science.
A. A Study on Kaggle Notebooks
We hypothesize that the problem of unexecutable notebooks,
reported by recent studies [ 12], [14], [15] for GitHub projects,
is also mirrored on Kaggle. We further hypothesize that the
issue of deprecated DS APIs, due to the rapid evolution of
DS libraries [ 16], [18] should be a signiÔ¨Åcant factor in the
unexecutability. To validate these hypotheses, we conduct a
lightweight study of Jupyter Notebooks on Kaggle.
Data Collection. We use Meta Kaggle [ 35], the ofÔ¨Åcial dump
of Kaggle meta data, as the data source for this study. Meta
Kaggle links to Jupyter Notebooks on Kaggle, including
competitions, datasets, kernels, and discussions on a daily
basis. We downloaded the snapshot of Meta Kaggle on July 20,
2412020 for this study. It consists of 49,061 datasets and 381,556
notebooks (kernels).
Sampling. To keep the study simple and efÔ¨Åcient, we impose
several Ô¨Åltering criteria. Starting with Python notebooks for
predictive tasks, we exclude datasets containing media data
(e.g., images, audio, and video), or spanning over multiple or
large Ô¨Åles (i.e., >500 MB). We calculated that we require a
sample size of 4,000 notebooks to achieve a 99% conÔ¨Ådence
level within a margin of error of 2% [36]. To this end, using
upvotes on Kaggle as a proxy of quality, we sorted all datasets
that passed our Ô¨Åltering criteria by the number of upvotes and
similarly notebooks for each dataset by the same metric. We
selected top 100 notebooks (or as many available if fewer) per
dataset, to respect diversity. This yielded 4043 notebooks from
the top 350 datasets.
Execution Environment. To execute the Jupyter Notebooks,
we convert them into Python programs using nbconvert [37].
We set up a uniÔ¨Åed virtual execution environment on top of
the standard Anaconda [ 38] scientiÔ¨Åc computing environment
of Python 3.6, also used in other studies [ 12], [14], [15]. This
standard environment includes all the popular libraries (over
200 library packages). Further, we performed a static import
analysis on our notebook corpus and manually installed the 20
most commonly used Python libraries.
Execution & Analysis. Among the 4043 notebooks in our
study corpus, 2,155 notebooks executed successfully while
1,888 notebooks, i.e., 47% of the total corpus did not execute
due to at least one error. Consistent with previous studies on
GitHub [ 12], [14], this shows that unexecutable notebooks
is also a signiÔ¨Åcant problem on Kaggle, although the exact
numbers differ due to platform differences.
Deprecation Errors. To quantify the impact of API dep-
recation on notebook executability, we created a ground-
truth list of deprecated APIs by manually inspecting the
release notes of 12 popular DS libraries, namely: scikit-
learn [ 39], pandas [ 40], seaborn [ 41], NumPy [ 42], SciPy [ 43],
XGBoost [ 44], Plotly [ 45], TensorFlow [ 46], Keras [ 47],
statsmodels [ 48], imbalanced-learn [ 49], and CatBoost [ 50].
These libraries are a subset of the top-20 libraries found in our
Kaggle study, which systematically report API deprecations
in their documentation. In total, we manually identiÔ¨Åed 317
upgrades (i.e., deprecations) through the release notes, including
198 function name deprecations and 119 parameter deprecations.
Checking these against our corpus of 1,888 unexecutable
notebooks showed that 582 of the notebooks, i.e., 31%
contained at least one of these deprecated APIs.
B. A Motivating Example
Figure 1 presents a fragment of a real-world Jupyter Note-
book on Kaggle [ 51] with a patch generated by R ELANCER ,
that Ô¨Åxes all API deprecation errors. The original notebook,
submitted in 2017, is a high-quality notebook that won a bronze
medal on Kaggle. Further, it has gathered 37,767 views and 161
forks (as of April 12th, 2021). This indicates that the notebook
has been very useful to other data scientists for learning orexpediting their work. However, the original notebook no longer
executes with the latest default Anaconda environment due
to several deprecated APIs. In particular, Figure 1 shows Ô¨Åve
locations containing three deprecated APIs:
1)sklearn.cross_validation.train_test_split
2)sklearn.cross_validation.
grid_search.GridSearchCV
3)sklearn.cross_validation.ShuffleSplit
The Ô¨Årst two APIs are deprecated due to a
change in the package structure, i.e., moving
from sklearn.cross_validation to sklearn.
model_selection [52]. This kind of deprecation is
called function deprecation [16]. The deprecation in third
API is more complicated. In addition to the aforementioned
package change, the parameter n_iter was replaced by
n_splits starting from version 0.18 [ 52]. This is called
parameter deprecation [16]. Therefore, if a data scientist wants
to run the notebook on her own machine, often the Ô¨Årst step
of trying to re-use it, she needs to laboriously Ô¨Ånd, diagnose,
and Ô¨Åx each of the deprecation errors, one by one. This is a
big barrier to re-use an otherwise high-quality notebook.
Although API migration is a well-established research area
and there are even a few recent techniques that perform end-
to-end API migration [ 27], [28], [18], the Jupyter Notebook in
Figure 1 would be outside the scope of these techniques. First,
these techniques [ 27], [28] rely on a single information source,
typically either GitHub or the API documentation. However, for
the aforementioned example, the patch for ShuffleSplit with
the argument change is not directly available on GitHub. On the
other hand, a tool that only uses documentation may not Ô¨Ånd
the correct mapping for other deprecated APIs in the example
easily since cross_validation andmodel_selection are
not textually similar. A brute-force trial of each API in the
documentation would be impractical, especially with multiple
deprecated APIs. SpeciÔ¨Åcally, in our experiment, this notebook
could not be Ô¨Åxed using only documentation within a 30-minute
time limit (Section V, RQ2).
To overcome these challenges, R ELANCER effectively com-
bines two primary sources of information - GitHub examples
and API documentation - and Ô¨Åxes the problem iteratively
guided by the error message. SpeciÔ¨Åcally, R ELANCER Ô¨Årst
executes the original notebook, and Ô¨Ånd an ImportError at
line 235, as shown in Figure 2.
At this point, the repair action predictor component of
RELANCER automatically identiÔ¨Åes that the fully qualiÔ¨Åed
name of a module needs to be changed. R ELANCER identiÔ¨Åes
the affected module name from the error message and also
identiÔ¨Åes and localizes the affected API by analyzing the ab-
stract syntax tree (AST) of line 240. Then R ELANCER searches
for potential mappings: API old!API new on GitHub and
documentation and passes them to its learning-to-rank model.
For this speciÔ¨Åc case, R ELANCER ranks the correct API new:
model_selection.train_test_split on top. The reason
is that although the textual similarity between old and new
package names is low, this change is frequent on Github. While
applying a patch, R ELANCER not only Ô¨Åxes the actual reference
2421# kaggle.com/sagarnildass/predicting-boston-house-prices
. . ....
22import numpy as np # linear algebra
. . ....
235-from sklearn import cross_validation
235+from sklearn import model_selection
. . ....
240-X_train, X_test, y_train, y_test = cross_validation. train_test_split(features, prices, test_size=0.2, random_state=42)
240+X_train, X_test, y_train, y_test = model_selection. train_test_split(features, prices, test_size=0.2, random_state=42)
241 print("Training and testing split was successful.")
. . ....
284 from sklearn.tree import DecisionTreeRegressor
285-from sklearn.cross_validation import ShuffleSplit
285+from sklearn.model_selection import ShuffleSplit
. . ....
290 # Create 10 cross-validation sets for training and testing
291-cv = ShuffleSplit(X.shape[0], n_iter=10, test_size=0.2, random_state=0)
291+cv = ShuffleSplit(X.shape[0], n_splits=10, test_size=0.2, random_state=0)
292 train_sizes = np.rint(np.linspace(1, X.shape[0] *0.8-1,9)).astype(int)
. . ....
514 from sklearn.metrics import make_scorer
515-from sklearn.grid_search import GridSearchCV
515+from sklearn.model_selection import GridSearchCV
516...
Fig. 1: A motivating and illustrative example of R ELANCER .
T
raceback (most recent call last):
File "predicting-boston-house-prices.py", line 235, in <module>
from sklearn import cross_validation
ImportError: cannot import name ‚Äôcross_validation‚Äô
Fig.
2: The error message thrown by line 235.
but also Ô¨Åxes the necessary import statements. For example, as a
result of changing cross_validation.train_test_split
tocross_validation.train_test_split . R ELANCER
also Ô¨Åxes lines 235 and 285. Now after validation, R ELANCER
Ô¨Ånds a new error message pointing to another error at line 291.
Although the deprecated API involved two issues: deprecated
module name and parameter name, the Ô¨Årst issue at line 285
is already Ô¨Åxed by R ELANCER while Ô¨Åxing the previous
API. Therefore, the new error shows a TypeError at line
291. R ELANCER predicts that the appropriate repair action
for this error is a parameter name change and identiÔ¨Åes the
problematic parameter name from the error message. Then
RELANCER again leverages Github and documentation to get
all the candidate parameter names and validates one by one.
In this way, R ELANCER Ô¨Åxes all the deprecated errors until
the notebook fully executes.
In summary, this is a large notebook that contains 701 lines
of code containing Ô¨Åve unique API deprecation errors at seven
locations. To give an idea about the number of candidate
patches, R ELANCER had on average 355 choices per location.
However, R ELANCER systematically narrowed down the search
space and Ô¨Åxed all the errors in ten minutes.
Predict 
Repair Action
Error MessageLocalize Buggy 
ElementsAggregate Candidate 
Mappings
Rank Candidate 
MappingsCreate Candidate 
ProgramsExecute Pass?No
Yes
Output InputFig. 3: Overview of R ELANCER .
III. R ELANCER
Given a Jupyter Notebook that does not currently execute,
due to one or more deprecated APIs, R ELANCER automatically
applies necessary program transformations to upgrade all the
deprecated APIs so that the resulting notebook executes without
any error. Figure 3 presents an overview of R ELANCER . At
a high level, R ELANCER iteratively performs the following
operations until the notebook is error-free:
1)Analysis of Error. This step analyzes the current error
message: Ô¨Årst to predict the atomic repair action that is
required to solve that particular error and then to identify
the deprecated API.
2)Aggregation of Candidate Mappings. This step mines
the Github repositories and API documentation to aggre-
gate the candidate mapping between program elements:
PEdep!PEnew where a PE is an API or a parameter
name, or an argument value.
3)Ranking of Candidate Mappings. This step employs a
learning-to-rank model to rank the candidate mapping in
such a way that the more promising a mapping is, the
higher it is in the ranking.
2434)Creation and Validation of ModiÔ¨Åed Programs. This
step creates a candidate program for each mapping one
by one based on the ranking, and executes it to check if
it Ô¨Åxes the current error.
The subsequent sections describe each of the above steps of
RELANCER in more detail.
A. Step-1: Analysis of Error Message
The analysis of error messages consists of two parts, namely
(1) predicting the repair action to take and (2) localizing the
buggy element to apply it on.
1) Prediction of Repair Action: There are mainly three
kinds of API deprecation issues in Python programs: function
deprecation, parameter deprecation, and parameter‚Äôs value
deprecation [16]. Broadly, these map to the kind of repair action
required to Ô¨Åx the bug. We make the observation that typically
the error message is indicative of the kind of deprecation. For
example, Ô¨Åxing a function deprecation requires renaming the
API. Some example errors for such a deprecation are:
ImportErr
or: cannot import name ‚Äôjaccard_similarity_score‚Äô.
ModuleNotFoundError: No module named ‚Äôsklearn.grid_search‚Äô.
Intuitively, there are some common patterns in the messages,
such as a particular name cannot be imported or found.
However, they are not exactly the same and may vary across
libraries and APIs. To leverage this information systematically
and robustly, we build a machine learning classiÔ¨Åer to predict
the repair action directly from the error message. SpeciÔ¨Åcally,
the classiÔ¨Åer predicts one of three different repair actions: i)
changing the fully qualiÔ¨Åed name of a function, ii) changing
parameter name, and iii) changing an argument value.
Building the ClassiÔ¨Åer. We Ô¨Årst apply standard text pre-
processing techniques to clean the error message, such as
removing non-alphabetic characters, tokenizing and normaliz-
ing text. Then we convert the text into a matrix of token counts
where each word becomes a feature and value represents the
count of that token. So after this step, each error message in the
training dataset becomes a tuple of numeric values representing
text in the error message and the target is the corresponding
repair action. To predict the repair action from the error
message, we use Linear Support Vector ClassiÔ¨Åcation [ 53]
since it is generally accepted that it is well suited for text
classiÔ¨Åcation [54].
2) Creation of Training Data: To create a training dataset
for the repair action predictor, we need instances of pairs
of buggy and Ô¨Åxed versions of notebooks corresponding to
deprecated APIs, with the buggy version providing the error
message and an executable Ô¨Åxed version as the ground truth for
the required repair action. However, Kaggle does not provide
for the tracked versioning and collaborative project editing
(like GitHub) to facilitate gathering such instances. Therefore,
we devise a novel mutation-based technique to systematically
create our training data.
Manually
Inspect
Ground-truth
API Mappings Generate
Mutation 
Templates
Mutation
Templates 
Notebook
MutantsApply
MutationError 
MessagesRun
<Error Message, 
Repair Action> 
Pairs Kaggle 
Notebooks
Library Release 
NotesRemove Test
Data
Target
API Mappings Fig.
4: Mutation-based training dataset creation.
Approach. Our approach is inspired by techniques for data
augmentation [ 55] and simulation-based training data genera-
tion [ 56], [57], [58] used to generate synthetic, but realistic,
training data for machine learning. SpeciÔ¨Åcally, we use the
insight that given a known instance of an API deprecation
PEi
dep!PEi
new(which by deÔ¨Ånition states the repair action)
and a working notebook that uses the API PEi
new, we can
re-create the buggy version of the notebook by the deprecation
mutation PEi
new!PEi
dep, and execute the buggy notebook
to generate the error message. This provides a pair of an
error message and its corresponding repair action, i.e., a single
instance of training data. Similarly, we use the set of manually
collected deprecation mapping fPE1
dep!PE1
new;PE2
dep!
PE2
new;:::g and the set of executable notebooks identiÔ¨Åed
in our Kaggle study (Section II-A), and systematically inject
deprecation mutations into executable notebooks that are using
the new version of the respective API. Figure 4 summarizes
our overall workÔ¨Çow.
3) Localization of Buggy Program Elements: As Figure 2
shows, typically an error message describes a single problem
and consists of a stack trace, followed by an error description in
natural language text. R ELANCER exploits this broad structure
of the error message to extract (1) the buggy line number in the
main Ô¨Åle and (2) the speciÔ¨Åc program element on it, implicated
in the deprecation. To do this, R ELANCER analyzes the error
message along with the corresponding source Ô¨Åle as follows.
RELANCER Ô¨Årst extracts the line number of the most recent
(last) call site for the main source Ô¨Åle from the stack trace in
the error message‚Äìe.g., Line 235 in Figure 2. Second, it extracts
all the deprecation-related program elements, speciÔ¨Åcally
module/function/parameter names and values at that line from
the AST of that source Ô¨Åle. Third, R ELANCER searches all the
candidate program elements that it identiÔ¨Åed from the source
code, in the text portion of the error message to pinpoint the
exact program element that the error is complaining about. For
our motivating example, it is the cross_validation module.
B. Step-2: Aggregation of Candidate Mappings
Given a deprecated program element PEdepidentiÔ¨Åed in
the previous step, where PE is a module name, a function
name, a parameter name, or a parameter value, R ELANCER
utilizes two sources of information: i) the API documentation
244and ii) Github to identify all possible candidate mappings:
PEdep!PEnew.
1) Mining API documentation: The objective of this step is
to create a knowledge base ofÔ¨Çine that consists of each API
from the latest version of each library under consideration. Most
of the popular libraries use a very similar format document
structure for their API documentation. For each API method,
RELANCER parses (using Beautiful Soap [59]) the HTML page
to collect its parameter names and the set of possible discrete
values (if available). It then stores the information of these
libraries in an internal JSON database.
2) Mining of Github: Since R ELANCER focuses on Ô¨Åxing
onePEdepat a time, its on-demand GitHub search identiÔ¨Åes
a list of potential PEnewthat can replace PEdep.
Identifying API Upgrade Mappings. To identify plausible
API upgrades for a given previously unseen API, we leverage
the idea of mining code changes[ 60]. R ELANCER leverages
Github‚Äôs own search engine through the provided REST
API [61] to search relevant example changes efÔ¨Åciently.
First, given a deprecated API, Fdepthat has been identiÔ¨Åed in
the previous step and needs to be Ô¨Åxed, R ELANCER tokenizes
the fully qualiÔ¨Åed name of Fdepand then constructs a GitHub
search query by taking each token plus the keywords: ‚Äúupdate‚Äù,
‚Äúupgrade‚Äù, ‚Äúreplace‚Äù, and ‚Äúdeprecate‚Äù. Then R ELANCER exe-
cutes this query on GitHub using GitHub‚Äôs REST API, which
returns a list of commits sorted by GitHub‚Äôs ‚ÄúBest Match‚Äù
metric [62] in descending order.
Second, R ELANCER excludes irrelevant commits, i.e., com-
mits that do not contain an upgrade of Fdep. Such irrelevant
commits can be introduced due to GitHub‚Äôs imprecision of
searching. For each commit in searching result, R ELANCER
extracts the abstract syntax tree (AST) of the program version
before ( V0) and after ( V1) the commit. It discards a commit if
the AST of V0does not include Fdep.
Third, R ELANCER analyzes the AST difference of V0and
V1. For a deleted deprecated API call node FdepinV0, we
consider an added node in V1as an update of Fdepif the
deletion and addition occur at the same level of the AST, and
the deleted and added nodes are under the same parent. If
there are multiple addition candidates for a deletion, we pair
the deleted node with the addition candidate whose name has
the highest textual similarity with it. After AST node changes
are identiÔ¨Åed, R ELANCER discards any commit that does not
involve a change in Fdep. Finally, for each update of Fdep,
RELANCER extracts the corresponding new API: Fnew, adding
them to a list. This resulting list consists of the candidate APIs
mined from GitHub and is cached (stored locally) for future
use, to avoid repeating the search if R ELANCER encounters
the same deprecated API call multiple times.
Identifying Parameter Mappings. The APIs in data science
libraries generally have a large set of parameters but few
discrete options per parameter [ 63]. Furthermore, popular
data science programming languages like Python can accept
parameters in the form of key-value pairs. Therefore, mining
the exact change (patch) that R ELANCER needs for the subjecterror is challenging. However, since R ELANCER narrows down
the scope of search in the previous steps quite a lot: i) by Ô¨Årst
Ô¨Åxing function deprecation by Fnewand ii) then identifying
the deprecated parameter name Pdepfrom the error message,
it launches a focused search to collect the set of all parameter
names and the values developers used on GitHub for that
speciÔ¨Åc API. Finally, R ELANCER returns a map where the
keys are comprised of all the parameter names and values
represent the set of values that parameter can take. Then a
parameter mapping: Pdep!Pnewis established where Pnew
is not currently used in the current call site.
3) Compiling the Final List of Candidate Mappings: As we
discussed in Section III-A , RELANCER allows three kinds of
upgrades: i) API name changes, ii) parameter name changes,
and iii) argument value changes. Furthermore, R ELANCER
performs one single upgrade at a time. Therefore, for a given
deprecated program element, i.e., an API, a parameter, or a
value, R ELANCER takes a union of both sets of elements mined
from: i) the API documentation and ii) GitHub to compile a Ô¨Å-
nal set of candidates: PEdep!fPE1
new;PE2
new;::;PEn
newg.
C. Step-3: Ranking of Candidate Mappings
For a given deprecated API: Fdep, there can be hundreds of
candidate APIs:fPE1
new;PE2
new;::;PEn
newg. Further, there
can be multiple errors in a program, which can increase the
search space exponentially. Therefore, an effective ranking
strategy is important for the success of R ELANCER . Learning to
rank is an effective ranking strategy that has been found useful
in many applications, including various software engineering
tasks such as defect prediction [ 64] and bug localization [ 65].
Inspired by these applications, we develop a machine learning
based learning-to-rank strategy to rank all the candidate Ô¨Åxes
for a given deprecated API. However, for any machine learning
technique, designing the features that are related to the target
is critical. We design the following four features for our task:
two features from the API documentation and two features
from GitHub.
Occurrences on GitHub (OG). The number of times
RELANCER found a mapping: Fdep!Fnew on GitHub
commits.
Percentage of OG (POG). It is calculated by the ratio of
OG to the number of times R ELANCER found a mapping
where Fdepexists.
Distance between Fully QualiÔ¨Åed Names (D FQN ).RE-
LANCER computes the distance between the fully qualiÔ¨Åed
name of FdepandFnew by the Damerau‚ÄìLevenshtein
Distance algorithm [ 66]. Then the value is normalized
between 0 and 1, with zero corresponding to identical names.
Distanceof Simple Name (D Simple ).RELANCER follows
the same approach as DFQN , but it computes the scores
between the simple names (without considering package or
module name) of FdepandFnew.
Training Learning-to-Rank Model. We create training data
from our ground-truth dataset that are not part of test data. For
each ground-truth mapping: Fdep!Fnew,Fnew is used as a
positive instance in the training dataset. However, to learn the
245characteristics of meta-features for the correct mappings, we
also need some negative candidate mappings that syntactically
represent a correct replacement but functionally not. So we use
the same technique described in Section III-B to Ô¨Ånd a complete
list of candidate program elements and randomly choose four
negative examples for Fdep. We follow the same procedure to
collect all the positive and negative instances for each target
mapping in the ground truth dataset. Then we compute all
the four aforementioned features for each of the positive and
negative mappings. Therefore, our Ô¨Ånal training dataset has a
collection of data samples where each row presents an old to
new API mapping in terms of the four feature values along
with the information whether it is a valid mapping.
Now we formulate our learning-to-rank algorithm as a binary
classiÔ¨Åcation problem, where our objective is to determine
whether a particular mapping is correct. To this end, we use
LightGBM [ 67] as our classiÔ¨Åcation model, which is a widely
used machine learning technique in practice.
Ranking Candidates in Operation. In operation, for a
given deprecated API: Fdep, RELANCER gets all the candidates:
fF1
new;F2
new;::;Fn
newgfrom Step-2 and passes to the trained
LightGBM model. For each candidate ( Fi
new), the model
computes a probability score for being it to be correct. Then
RELANCER uses that probability score to rank all the candidate
mappings. It should be noted that once function mapping is
found and parameter name or value is identiÔ¨Åed from the error
message, the number of candidates for a particular parameter
is not large. Therefore, R ELANCER follows the order in the
documentation to rank parameter mappings.
D. Step-4: Creation and Validation of ModiÔ¨Åed Programs.
Given a ranked list of candidate mappings, R ELANCER starts
iterating through each mapping from the top of the ranked list.
For a given mapping: PEdep!PEnew, RELANCER replaces
PEdepbyPEnew to create a new version of the candidate
program and validates the change through execution. If the
type of error message is unchanged, R ELANCER moves to
the next mapping. However, if R ELANCER gets a new error
message, it assumes that it Ô¨Åxed the current error and goes
to Step-1 to analyze the error message. R ELANCER continues
these steps until there is no error in the notebook or the entire
search space of candidate mappings is exhausted, or a given
time limit is exceeded.
IV. E XPERIMENTAL SETUP
A. Implementation
RELANCER is implemented in the Python programming
language. It has mainly three components: i) mining API
documentation, ii) on-demand searching of edit-examples on
Github, and iii) AST manipulation and validation. We use
Beautiful Soup [59] for parsing the API documentation HTML
pages to extract relevant information. We use nbconvert [37]
to convert Jupyter Notebooks into Python programs to ease the
process of automatic running and generating error messages.
We use GitHubREST API [61] for searching potential examples
on GitHub and then use LibCST [68], a popular static analysisframework for Python to analyze AST diff and so on. We also
use LibCST for making necessary changes to generate potential
Ô¨Åxes for the deprecation errors. We use scikit-learn [39] and
LightGBM [67]‚Äìtwo popular open source machine learning
libraries to implement our ML models.
B. Creation of Dataset
Since R ELANCER is a machine learning based technique,
to evaluate it properly, we need a training dataset and a test
dataset that are mutually exclusive. Our preliminary study in
Section II-A, yielded 582 unexecutable notebooks that contain
at least one deprecated API. However, since the objective of
RELANCER is to Ô¨Åx only deprecated errors, we wanted to Ô¨Ålter
out those notebooks that have irrelevant errors. However, it is
very challenging to detect all such irrelevant errors.
Error analysis provides an automated way to detect candidate
deprecated errors since a previous study [ 17] suggests that
certain kinds of error messages are related to API usage
problems, which can lead us to deprecated errors. However, an
error message only gives a description of the Ô¨Årst error during
the execution of the program. Therefore, we targeted capturing
at least those notebooks where the Ô¨Årst error is related to
deprecated APIs. We performed an automated analysis on the
error messages to identify the initial list of candidate deprecated
errors that contains one of the Ô¨Åve errors: i) import error, ii)
module not found error, iii) type error, iv) value error, or v)
attribute error. Then we searched those APIs in the set of our
ground-truth API dataset. Finally, we identiÔ¨Åed 255 notebooks
where the Ô¨Årst failure is due to a deprecation error. Therefore,
we use all the 255 notebooks as our test dataset to evaluate
RELANCER . Table I provides more details about the dataset.
C. Training RELANCER
Training Repair Action Model. To keep the training dataset
and test dataset mutually exclusive, our mutation based frame-
work (described in Section III-A 2) did not use any deprecated
APIs, Ftest=fF1
test;F2
test;::gfrom the test dataset. To this
end, we eliminated Ftestfrom our ground-truth dataset, FGand
only used the remaining deprecated APIs ( Ftr=FG Ftest)
to design mutation operators. We use all the 2,155 executable
notebooks obtained in our preliminary study (Section II-A)
as seed programs to create the training data as described in
Section III-A 2. Finally, we constructed a training dataset of
500 <error message, repair action> pairs.
We also followed the approach described in Section III-C
to construct the training data for the learning-to-rank model
fromFtr. This gave us a training dataset of 485 instances.
D. Research Questions
We evaluate R ELANCER with respect to the following
research questions.
RQ-1: How effective is R ELANCER at Ô¨Åxing API deprecation
issues of Jupyter Notebooks?
RQ-2: Does R ELANCER need different sources of information
to perform well?
RQ-3: Do the machine learning models in R ELANCER help
with upgrading more APIs in near real-time?
246TABLE I: Subjects: Jupyter Notebooks on Kaggle
Description Total Max Min Median Average
#Datasets 110 - - - -
#Notebooks 255 - - - -
#Notebooks per dataset - 13 1 1 2.32
#Libraries 8 - - - -
LOC - 739 15 120 152.02
LOC (including comments) - 1193 21 208 263.81
142
5
5.7%
5521.6% 93.5%49
19.2%Notebooks Fixed by RELANCER
Notebooks Containing Irrelevant Errors
Notebooks Timed out
Notebooks not Fixed for Limitations
Fig. 5: Effectiveness of R ELANCER
E. Experimental ConÔ¨Ågurations
All the experiments are performed on a 4-core Intel(R)
Core(TM) i7-8650 CPU @ 1.90 GHz machine with 16GB of
RAM, running Ubuntu 16.04, with Python 3.6.10 and Conda
4.7.10. We set a timeout of 30 minutes for each notebook.
V. E VALUATION
A. RQ-1: Effectiveness of RELANCER
To evaluate the effectiveness of R ELANCER , we ran it on
all the 255 notebooks in our test dataset with a timeout
RELANCER RELANCER g ithub RELANCER d oc020406080100120140# Fixed Notebooks142
9
692
Fig. 6: Effect of removing one source of information1-from sklearn.metrics import jaccard_similarity_score
2+from sklearn.metrics import jaccard_score
3...
4knn_yhat = neigh.predict(X_test)
5-print("KNN Jaccard index: %.2f" %
jaccard_similarity_score(y_test, knn_yhat))
6+print("KNN Jaccard index: %.2f" %
jaccard_score(y_test, knn_yhat, average="micro"))
Fig. 7: Another illustrative example of R ELANCER .
of 30 minutes for each notebook. Then we measure the
effectiveness in terms of the number of Ô¨Åxed notebooks
and time to Ô¨Åx the notebooks completely. Figure 5 shows
that R ELANCER was able to Ô¨Åx 142 out of 255 notebooks
completely. Among the 142 notebooks it Ô¨Åxed, 107 notebooks
required upgrading only the fully qualiÔ¨Åed name of APIs.
Other 35 notebooks required changing parameter names or
argument values with or without the fully qualiÔ¨Åed name of
APIs. As we have shown in the motivating example, R ELANCER
can Ô¨Åx multiple errors in notebooks that require sophisticated
Ô¨Åxes in both API‚Äôs fully qualiÔ¨Åed name and parameters. The
highest number of deprecated APIs R ELANCER has upgraded
in a single notebook is eight. Further, it should be noted that
Python is a rich language that allows for optional parameters
which use a default value if that is not explicitly given by
developers. However, sometimes even that default value can
be deprecated, leaving the notebook broken. In this case,
RELANCER can explicitly pass the problematic parameter with
the correct value. For example, as Figure 7 presents after the
deprecated API jaccard_similarity_score got changed
tojaccard_score , the default value of average is no longer
valid. So R ELANCER added that parameter in the call site with
the correct value.
The size of the patches generated by R ELANCER varied from
2 to 25 lines of code (LOC) with a median size of 3 LOC.
When R ELANCER was able to Ô¨Åx a notebook, the execution
time varies from 2 seconds to 28 minutes, with on average
two minutes (median 17 seconds) per notebook. This result
demonstrates that R ELANCER can be used in real time to Ô¨Åx
deprecated errors.
Validation. We manually validated all the successful patches
to make sure that R ELANCER used the latest API and/or
parameter for each deprecated API. Further, we also conducted
an objective evaluation to make sure that the Ô¨Åxed notebooks
serves the actual developer‚Äôs intention. To this end, we
leveraged the output cell feature of Jupyter Notebooks. More
speciÔ¨Åcally, Jupyter Notebooks often have the output cell that
stores the accuracy from the original run. We were able to
identify the original accuracy from the output cell for 82
notebooks and found that the new accuracy we got after Ô¨Åxing
the deprecation errors is within the 1% (median) of original
accuracy. It should be noted that most machine learning models
have inherent randomness. Therefore, we believe that such a
small accuracy difference is not surprising.
2471-import plotly.plotly as py
2+import chart_studio.plotly as py
3py.iplot(data, filename=...)
Fig. 8: Example of moving an API to another library
Answer
to RQ1: RELANCER restored execution to 56%
of the notebooks, Ô¨Åxing as many as eight errors in a single
notebook. The median time to Ô¨Åx a notebook is only 17
secs, making it suitable for real-time use.
B.
RQ-2: Contribution of Different Sources of Information
To understand whether both sources of information: the API
documentation and Github help R ELANCER achieve its full
capability, we created two baseline tools from R ELANCER ,
which use only a single source of information along with
error message. R ELANCER github does not use any information
from the API documentation and R ELANCER docdoes not
use any information from GitHub. All other functionalities
in two baselines are similar to R ELANCER . Here we would
like to stress the fact that since there is no tool available
for upgrading APIs for Jupyer Notebooks, to the best of our
knowledge, we cannot directly compare any existing technique.
However, conceptually the baselines: R ELANCER github and
RELANCER docroughly simulate the approaches that use only
Github such as Meditor [ 27] and only the API documentation
along with error messages such as SOAR [18].
The experimental results in Figure 6 show that R ELANCER
outperforms both R ELANCER github and R ELANCER docsigniÔ¨Å-
cantly in terms of the number of Ô¨Åxed notebooks. More speciÔ¨Å-
cally, R ELANCER Ô¨Åxed 142 notebooks while R ELANCER github
and R ELANCER docÔ¨Åxed 96 and 92 notebooks, respectively.
There may be two reasons why R ELANCER doccannot Ô¨Åx a
particular bug. First, an API can be completely removed from
a library. Figure 8 presents such an example where function
plotly.plotly is completely removed from plotly version 4
distribution package. According to the ofÔ¨Åcial documentation,
it has moved to chart-studio [69]. Therefore, the replacement is
not simply available in the latest version of the documentation
ofplotly. Second, many APIs move to different modules or
the name changes so signiÔ¨Åcantly that it is hard to create
a mapping between the old API and the new API by a
particular heuristic. For example, in our motivating example in
Figure 1, the module sklearn.grid_search is deprecated
and replaced by sklearn.model_selection starting from
scikit-learn 0.18 [ 52]. For all these cases, Github can help rank
the correct program element toward the top.
Similarly, not all possible example changes for deprecated
APIs with their parameters are available on Github. For exam-
ple,tensorflow.train.RMSPropOptimizer is deprecated
and used by a notebook in our dataset, but our on-demand
search did not provide any useful candidate. This may often
happen for less popular APIs.
1 5 10 15 20 25 30
E
xecution Time (min)406080100120140#Fixed Notebooks
RELANCER
RELANCER r andom
RELANCER t ext
RELANCER n aiveFig. 9: Number of Ô¨Åxed notebooks over time.
In summary, all these results demonstrate that R ELANCER
cannot achieve its full potential without harnessing the informa-
tion from multiple sources since each source plays an important
role for a speciÔ¨Åc task.
Answer to RQ2: Both sources of information contribute
to the overall effectiveness of R ELANCER . RELANCER
can successfully restore the executability of 56% of the
subjects, while baselines relying on just GitHub examples
and just API documentation can only Ô¨Åx 38% and 36%
of the subjects respectively.
C.
RQ-3: Contribution of Machine Learning Models
RELANCER uses two machine learned models to explore the
search space of all the candidate program elements efÔ¨Åciently.
However, it may be plausible to think that two simple static
models can be used instead. Therefore, to better understand the
contribution of these two models on the end-to-end performance
of R ELANCER : we have created three baselines:
RELANCER random .In this baseline, R ELANCER ranks the
repair actions randomly instead of using its repair action model.
RELANCER text.In this baseline, R ELANCER ‚Äôs learning-to-
rank model is replaced by an edit distance based ranking.
More speciÔ¨Åcally, we combine the candidate elements from
both GitHub and API documentation, and sort them by
the Damerau‚ÄìLevenshtein Distance score [ 66] between the
deprecated element and the new element in descending order.
RELANCER naive .In this baseline, we replace both machine
learned models byR ELANCER random and R ELANCER textto
understand the effect of two models in aggregation.
Figure 9 presents the experimental results in terms of the
number of notebooks Ô¨Åxed by R ELANCER and the baselines
over their execution time. From the results, it is evident that
even we replace one machine learned model with a simple
248baseline model, the performance of R ELANCER deteriorates.
More speciÔ¨Åcally, R ELANCER random , R ELANCER text, and
RELANCER naive Ô¨Åx 108, 102, and 78 notebooks, respec-
tively, which is less than the 142 Ô¨Åxed by R ELANCER . This
indicates that both machine learned models contribute to
the overall performance of R ELANCER . We performed an
unpaired t-test between the result of R ELANCER with that
of each baseline separately. The p-values of R ELANCER
vs R ELANCER random , RELANCER vs R ELANCER text, and
RELANCER vs R ELANCER naive are 0.0014, 0.0002, and
<0.0001, respectively. The test result indicates that the R E-
LANCER ‚Äôs performance improvement over any baseline is not
just numerically but also statistically signiÔ¨Åcant.
If we reduce the timeout of R ELANCER and other baselines,
as expected the number of notebooks Ô¨Åxed by each technique
reduces. However, due to its machine learning models, the
performance of R ELANCER deteriorates proportionally less
than the other baselines. If we set a timeout of 5 minutes
for all the tools, R ELANCER can Ô¨Åx 124 (49%) notebooks
of notebooks while the na√Øve baseline can Ô¨Åx only 60 (24%)
notebooks.
Answer to RQ3: The two machine learned models help
RELANCER realize its real-time use case. R ELANCER can
Ô¨Åx 49% of subjects within 5 minutes, while a baseline
lacking its machine learning models can only Ô¨Åx 24%.
D.
Limitations and Threats to Validity
1) Limitations: RELANCER was unsuccessful in Ô¨Åxing 113
notebooks. To understand the reasons why R ELANCER was
unable to Ô¨Åx those notebooks, we manually investigated each
of them and found one of the following reasons:
RELANCER is limited to Ô¨Åx only deprecation errors. We
found that there are 55 notebooks that contain some other
irrelevant errors such as missing dependent Ô¨Åles, accessing
incorrect columns in the dataset, and so on. Therefore, although
RELANCER Ô¨Åxed the deprecation errors in these notebooks,
Ô¨Ånally the notebook still failed due to the other errors.
Lack of Type Inference. RELANCER determines the fully
qualiÔ¨Åed name of an API based on a static analysis to
upgrade it properly. However, since Python is a dynamically
typed language, R ELANCER cannot determine the type of
some complex APIs, especially when it is dependent on
the return type of another API. For example, the API:
as_matrix() in Figure 10 has been deprecated [ 70]. To
Ô¨Åx this API, R ELANCER needs to know the fully qualiÔ¨Åed
name: pandas.DataFrame.as_matrix() which should be
determined by the returned type of df.drop() . Currently,
RELANCER does not infer the return types of third-party APIs
as the dynamic typing of Python makes it complex to retrieve
accurate return-type information statically.
Lack of Complex Repair Actions. Currently, R ELANCER ‚Äôs
repair actions include replacing one API or variable name by
another API or variable name respectively. However, some
Ô¨Åxes require complex repair actions. For example, Figure 11
represents a deprecated API where the Ô¨Åx requires changing its1X = df.drop([‚ÄôRadiation‚Äô, ...], axis=1). as_matrix()
Fig. 10: A deprecation issue requires type inference to Ô¨Åx.
1-scipy.misc.toimage(array)
2+PIL.Image.fromarray(array.astype(‚Äôuint8‚Äô))
Fig. 11: Example of a complex repair action.
parameter by another method call. Although R ELANCER suc-
cessfully changed the fully qualiÔ¨Åed name of the API, replacing
the parameter array with the API array.astype(‚Äôuint8‚Äô)
is currently out of its scope.
Change in Functionality. Figure 12 presents an
example where due to a change in the functionality
of the deprecated API: tensorflow.placeholder ,
the new API requires calling another API:
tf.compat.v1.disable_eager_execution() with
it. Currently R ELANCER cannot infer such a change in
functionality and thus is unable to Ô¨Åx this kind of errors.
2) External Validity: Our framework has only been instanti-
ated for Ô¨Åxing API deprecation issues of Python-based Jupyter
Notebooks, and has been only evaluated on 255 notebooks.
Therefore, our results may not hold outside this scope. We
tried to mitigate this risk by targeting real-world highly-voted
datasets on Kaggle.
3) Quality of Data: The performance of R ELANCER is
limited by the quality of the training data. To mitigate this
threat, we constructed the ground truth set from 12 most popular
libraries and manually veriÔ¨Åed each deprecated API.
VI. R ELATED WORK
Restoring the executability of Jupyter Notebooks. Recently,
Wang et al. proposed Osiris [ 14], the Ô¨Årst technique targeting
unexecutable Jupyter Notebooks, speciÔ¨Åcally ones impacted
by ambiguous or undeÔ¨Åned order of cell execution. Osiris
reconstructs possible execution orders by automatically sat-
isfying dependencies between code cells. In follow up work
they propose SnifferDog [ 15] that restores executability to a
notebook by inferring and providing a compatible environment,
comprised of correctly-versioned packages, for it. Our work
complements [ 14] and [ 15] by remedying a third cause of
notebook unexecutability, namely the deprecation of APIs.
Moreover, in contrast to SnifferDog, which attempts to re-create
theoriginal execution environment of a notebook, R ELANCER
1-x=tf.placeholder(tf.float32,shape=[None,20],...)
2+x=tf.compat.v1.disable_eager_execution()
3+x=tf.compat.v1.placeholder(tf.float32,shape=[None,20],...)
Fig. 12: Example of functionality change.
249ports it to the user‚Äôs (and/or most recent) environment, in line
with its target use-case of exploratory programming.
API Migration. Research on the broad topic of API migration
goes back more than two decades [ 19], [20], [21], [22], [23],
[24], [25], [26]. In early work by Chow et al. [ 19], a library
maintainer is tasked with annotating API functions undergoing
interface changes with migration rules. Catchup! [ 21] records
API refactoring actions as a library maintainer evolves an API.
The recorded migration rules [ 19] or API refactorings [ 21] are
then used to migrate client applications. Such semi-automatic
approaches entail additional effort by library maintainers, while
RELANCER is completely automatic.
A second class of techniques automatically infer API
mappings and/or migration refactorings. SemDiff [ 20] and
LibSync [ 23] both mine API migration patterns across different
versions of a library, such as renamed methods or changed
parameters, from migrated client code. Zhong et al. [ 29]
broaden the scope of API migration by inferring API mappings
between related libraries in Java and C#. They do so by using
textual similarity to align client code of the two libraries and
then inferring corresponding APIs based on their common
usage pattern in the aligned client code. Nguyen et al. [ 30],
[31], [32] also infer API mappings between Java and C#
libraries but use statistical machine translation instead. All
the above techniques focus only on identifying API mappings.
By contrast, R ELANCER performs an end-to-end migration,
including modifying the target code.
Recent work has proposed fully automated API migration
techniques [ 27], [28], [18]. Meditor [ 27] targets automatic
migration of a system from one library version to another.
It mines migration-related code changes from open source
repositories and instantiates them in the target system, in
a context-sensitive manner to produce the migrated system,
which is provided to the developer for review. Similarly,
APPEVOLVE [28] automatically migrates Android apps by
mining API changes from existing projects, applying them
to the target app, and validating the patched app through
differential testing. In emerging research, SOAR [ 18] proposes
a technique to migrate data science programs from one library
to another (e.g., TensorFlow [ 46] to PyTorch [ 71]). SOAR infers
a likely API mapping from the documentation of the source
and target libraries. Then it synthesizes candidates instances
of the migrated program, using differential testing against the
original (reference) program as an oracle, pruning the search
space using logical constraints generated from the Python
interpreter‚Äôs error messages. Unlike the above, R ELANCER
organically integrates knowledge from two sources - migration
examples and API documentation - to derive its search space.
This feature is one of the keys to its performance (Section V-B).
Further, A PPEVOLVE and SOAR use differential testing against
the reference implementation as a strong oracle to derive the
correct migration. Meditor relies on human inspection for
validation. In R ELANCER ‚Äôs use case, there is no executable
reference implementation. Thus, R ELANCER uses a carefully
orchestrated pair of machine learned models to effectivelypredict the correct migration.
Program Repair. Some elements of R ELANCER ‚Äôs design are
also inspired by the body of work on Automatic Program Repair
(APR) [ 72], [73], [74], [75], [76], [77], [78], [79], [80]. APR
techniques try to Ô¨Åx functional bugs in client code by implicitly
or explicitly searching a space of program mutations for a
patch, typically using a test suite as an oracle. At a high level
RELANCER also searches a space of program modiÔ¨Åcations,
but unlike APR techniques it needs to perform multiple API
migrations (vs. Ô¨Åxing a single functional bug) and do so in real
time, without the aid of a strong oracle like a test suite. Because
of these key differences, R ELANCER relies on machine learned
models powered by multiple knowledge sources - runtime
error messages, API migration examples on GitHub, and API
documentation - as its primary vehicle for efÔ¨Åciently navigating
the search space, rather than a test suite.
VII. C ONCLUSION
Data scientists typically practice exploratory programming
using computational notebooks, iteratively reÔ¨Åning their code
and actively trying to re-use solutions created by other data
scientists. However, most publicly available notebooks cannot
be executed. One of the prominent reasons is the deprecation
of data science APIs. In this work, we proposed R ELANCER ,
an automatic technique that restores the executability of broken
Jupyter Notebooks, by upgrading deprecated APIs. R ELANCER
integrates an iterative runtime error driven approach with a
combined search space sourced from API migration examples
and API documentation. Both features are orchestrated through
machine learned models. An evaluation of R ELANCER on
Kaggle notebooks showed that it is effective in restoring
executability to 56% of the subjects, and that its error-driven
approach, use of a combined search space, and its machine
learned models all contribute to its efÔ¨Åcacy.
VIII. A CKNOWLEDGMENTS
This work was partially supported by a gift from the Fujitsu
Research of America, Inc. and National Science Foundation
Grant No. CCF-1718903.
REFERENCES
[1]Wharton School of Business, University of Pennsylvania. (Accessed in
2021) What‚Äôs driving the demand for data scientists? https://knowledge.
wharton.upenn.edu/article/whats-driving-demand-data-scientist/.
[2]Forbes. (Accessed in 2021) Why data science is such a hot career right
now. https://www.forbes.com/sites/quora/2017/10/25/why-data-science-
is-such-a-hot-career-right-now/.
[3] Kaggle. (Accessed in 2021) Kaggle. https://www.kaggle.com.
[4]M. Beth Kery and B. A. Myers, ‚ÄúExploring exploratory programming,‚Äù in
IEEE Symposium on Visual Languages and Human-Centric Computing ,
2017, pp. 25‚Äì29.
[5]A. Head, F. Hohman, T. Barik, S. M. Drucker, and R. DeLine, ‚ÄúManaging
messes in computational notebooks,‚Äù in Conference on Human Factors
in Computing Systems, 2019, pp. 1‚Äì12.
[6] Jupyter. (Accessed in 2021) Jupyter notebook. https://jupyter.org.
[7]J. M. Perkel, ‚ÄúWhy jupyter is data scientists‚Äô computational notebook of
choice,‚Äù Nature, vol. 563, no. 7732, pp. 145‚Äì147, 2018.
[8]D. Toomey, Jupyter for data science: Exploratory analysis, statistical
modeling, machine learning, and data visualization with Jupyter. Packt
Publishing Ltd, 2017.
250[9]J. Inc. (Accessed in 2021) We downloaded 10,000,000
jupyter notebooks from github ‚Äì this is what we learned.
https://blog.jetbrains.com/datalore/2020/12/17/we-downloaded-10-
000-000-jupyter-notebooks-from-github-this-is-what-we-learned/.
[10] T. Kluyver, B. Ragan-Kelley, F. P√©rez, B. Granger, M. Bussonnier,
J. Frederic, K. Kelley, J. Hamrick, J. Grout, S. Corlay, P. Ivanov, D. Avila,
S. Abdalla, C. Willing, and J. development team, ‚ÄúJupyter notebooks
‚Äì a publishing format for reproducible computational workÔ¨Çows,‚Äù in
International Conference on Electronic Publishing, 2016, pp. 87‚Äì90.
[11] M. Ragan-Kelley, F. Perez, B. Granger, T. Kluyver, P. Ivanov, J. Frederic,
and M. Bussonnier, ‚ÄúThe jupyter/ipython architecture: a uniÔ¨Åed view of
computational research, from interactive exploration to communication
and publication,‚Äù in AGU Fall Meeting Abstracts, 2014, pp. H44D‚Äì07.
[12] J. F. Pimentel, L. Murta, V . Braganholo, and J. Freire, ‚ÄúA large-scale study
about quality and reproducibility of jupyter notebooks,‚Äù in International
Working Conference on Mining Software Repositories, 2019, pp. 507‚Äì517.
[13] S. Chattopadhyay, I. Prasad, A. Z. Henley, A. Sarma, and T. Barik,
‚ÄúWhat‚Äôs wrong with computational notebooks? pain points, needs, and
design opportunities,‚Äù in Conference on Human Factors in Computing
Systems, 2020, pp. 1‚Äì12.
[14] J. Wang, K. Tzu-Yang, L. Li, and A. Zeller, ‚ÄúAssessing and restoring
reproducibility of jupyter notebooks,‚Äù in International Conference on
Automated Software Engineering, 2020, pp. 138‚Äì149.
[15] J. Wang, L. Li, and A. Zeller, ‚ÄúRestoring execution environments of
jupyter notebooks,‚Äù in International Conference on Software Engineering,
2021, pp. 138‚Äì149.
[16] J. Wang, L. Li, K. Liu, and H. Cai, ‚ÄúExploring how deprecated
python library apis are (not) handled,‚Äù in International Symposium on
Foundations of Software Engineering, 2020, pp. 233‚Äì244.
[17] Z. Zhang, H. Zhu, M. Wen, Y . Tao, Y . Liu, and Y . Xiong, ‚ÄúHow do
python framework apis evolve? an exploratory study,‚Äù in International
Conference on Software Analysis, Evolution and Reengineering, 2020,
pp. 81‚Äì92.
[18] A. Ni, D. Ramos, A. Yang, I. Lynce, V . Manquinho, R. Martins, and
C. Le Goues, ‚ÄúSoar: a synthesis approach for data science api refactoring,‚Äù
inInternational Conference on Software Engineering, 2021, pp. 112‚Äì124.
[19] K. Chow and D. Notkin, ‚ÄúSemi-automatic update of applications in
response to library changes,‚Äù in International Conference on Software
Maintenance, 1996, p. 359.
[20] B. Dagenais and M. P. Robillard, ‚ÄúRecommending adaptive changes
for framework evolution,‚Äù ACM Transactions on Software Engineering
Methodology, vol. 20, no. 4, pp. 1‚Äì35, 2011.
[21] J. Henkel and A. Diwan, ‚ÄúCatchup! capturing and replaying refactorings
to support api evolution,‚Äù in International Conference on Software
Engineering, 2005, pp. 274‚Äì283.
[22] S. Meng, X. Wang, L. Zhang, and H. Mei, ‚ÄúA history-based matching
approach to identiÔ¨Åcation of framework evolution,‚Äù in International
Conference on Software Engineering, 2012, pp. 353‚Äì363.
[23] H. A. Nguyen, T. T. Nguyen, G. Wilson Jr, A. T. Nguyen, M. Kim, and
T. N. Nguyen, ‚ÄúA graph-based approach to api usage adaptation,‚Äù ACM
Sigplan Notices, vol. 45, no. 10, pp. 302‚Äì321, 2010.
[24] T. Sch√§fer, J. Jonas, and M. Mezini, ‚ÄúMining framework usage changes
from instantiation code,‚Äù in International Conference on Software
Engineering, 2008, pp. 471‚Äì480.
[25] W. Wu, Y .-G. Gu√©h√©neuc, G. Antoniol, and M. Kim, ‚ÄúAura: a hybrid
approach to identify framework evolution,‚Äù in International Conference
on Software Engineering, 2010, pp. 325‚Äì334.
[26] Z. Xing and E. Stroulia, ‚ÄúApi-evolution support with diff-catchup,‚Äù IEEE
Transactions on Software Engineering, vol. 33, no. 12, pp. 818‚Äì836,
2007.
[27] S. Xu, Z. Dong, and N. Meng, ‚ÄúMeditor: inference and application of api
migration edits,‚Äù in International Conference on Program Comprehension,
2019, pp. 335‚Äì346.
[28] M. Fazzini, Q. Xin, and A. Orso, ‚ÄúAutomated api-usage update for
android apps,‚Äù in International Symposium on Software Testing and
Analysis, 2019, pp. 204‚Äì215.
[29] H. Zhong, S. Thummalapenta, T. Xie, L. Zhang, and Q. Wang, ‚ÄúMining
api mapping for language migration,‚Äù in International Conference on
Software Engineering, 2010, pp. 195‚Äì204.
[30] A. T. Nguyen, H. A. Nguyen, T. T. Nguyen, and T. N. Nguyen, ‚ÄúStatistical
learning approach for mining api usage mappings for code migration,‚Äù
inInternational Conference on Automated Software Engineering, 2014,
pp. 457‚Äì468.[31] A. T. Nguyen, T. T. Nguyen, and T. N. Nguyen, ‚ÄúLexical statistical
machine translation for language migration,‚Äù in International Symposium
on Foundations of Software Engineering, 2013, pp. 651‚Äì654.
[32] ‚Äî‚Äî, ‚ÄúDivide-and-conquer approach for multi-phase statistical migration
for source code (t),‚Äù in International Conference on Automated Software
Engineering, 2015, pp. 585‚Äì596.
[33] X. Liu, N. Iftikhar, and X. Xie, ‚ÄúSurvey of real-time processing systems
for big data,‚Äù in International Database Engineering & Applications
Symposium, 2014, pp. 356‚Äì361.
[34] J. Cohen. (Accessed in 2021) Near real-time vs. real-time analytics.
https://harperdb.io/blog/near-real-time-vs-real-time-analytics/.
[35] M. Kaggle. (Accessed in 2021) Meta kaggle. https://www.kaggle.com/
kaggle/meta-kaggle.
[36] Creative Research Systems. (Accessed in 2021) The survey system.
https://www.surveysystem.com/sscalc.htm.
[37] Jupyter. (Accessed in 2021) nbconvert. https://github.com/jupyter/
nbconvert.
[38] Anaconda. (Accessed in 2021) Anaconda. https://anaconda.org.
[39] scikit learn. (Accessed in 2021) scikit-learn: machine learning in python.
https://scikit-learn.org.
[40] pandas. (Accessed in 2021) pandas ‚Äì python data analysis library. https:
//pandas.pydata.org.
[41] seaborn. (Accessed in 2021) seaborn: statistical data visualization. https:
//seaborn.pydata.org.
[42] NumPy. (Accessed in 2021) Numpy. https://numpy.org.
[43] SciPy. (Accessed in 2021) Scipy. https://www.scipy.org.
[44] XGBoost. (Accessed in 2021) Xgboost: extreme gradient boosting. https:
//github.com/dmlc/xgboost.
[45] Plotly. (Accessed in 2021) Plotly: The front end for ml and data science
models. https://plotly.com.
[46] TensorFlow. (Accessed in 2021) TensorÔ¨Çow. https://www.tensorÔ¨Çow.org.
[47] Keras. (Accessed in 2021) Keras: the python deep learning api. https:
//keras.io.
[48] statsmodels. (Accessed in 2021) statsmodels. https://www.statsmodels.
org.
[49] imbalanced learn. (Accessed in 2021) imbalanced-learn. https://github.
com/scikit-learn-contrib/imbalanced-learn.
[50] CatBoost. (Accessed in 2021) Catboost - open-source gradient boosting
library. https://catboost.ai.
[51] Kaggle. (Access in 2021) Predicting boston house prices. https://www.
kaggle.com/sagarnildass/predicting-boston-house-prices.
[52] scikit learn. (Accessed in 2021) Scikit documentation. https://scikit-
learn.org/stable/whats_new/v0.18.html.
[53] ‚Äî‚Äî. (Accessed in 2021) sklearn.svm.linearsvc. https://scikit-learn.org/
stable/modules/generated/sklearn.svm.LinearSVC.html.
[54] T. Joachims, ‚ÄúText categorization with support vector machines: Learning
with many relevant features,‚Äù in European Conference on Machine
Learning, 1998, pp. 137‚Äì142.
[55] C. Shorten and T. M. Khoshgoftaar, ‚ÄúA survey on image data augmen-
tation for deep learning,‚Äù Journal of Big Data, vol. 6, no. 1, pp. 1‚Äì48,
2019.
[56] G. Ros, L. Sellart, J. Materzynska, D. Vazquez, and A. M. Lopez, ‚ÄúThe
synthia dataset: A large collection of synthetic images for semantic
segmentation of urban scenes,‚Äù in Conference on Computer Vision and
Pattern Recognition, 2016, pp. 3234‚Äì3243.
[57] A. Gaidon, Q. Wang, Y . Cabon, and E. Vig, ‚ÄúVirtual worlds as proxy
for multi-object tracking analysis,‚Äù in Conference on Computer Vision
and Pattern Recognition, 2016, pp. 4340‚Äì4349.
[58] S. R. Richter, V . Vineet, S. Roth, and V . Koltun, ‚ÄúPlaying for data: Ground
truth from computer games,‚Äù in European Conference on Computer Vision,
2016, pp. 102‚Äì118.
[59] BeautifulSoup. (Accessed in 2021) Beautifulsoup. https://www.crummy.
com/software/BeautifulSoup.
[60] A. Koyuncu, K. Liu, T. F. Bissyand√©, D. Kim, J. Klein, M. Monperrus,
and Y . Le Traon, ‚ÄúFixminer: Mining relevant Ô¨Åx patterns for automated
program repair,‚Äù Empirical Software Engineering, vol. 25, no. 3, pp.
1980‚Äì2024, 2020.
[61] GitHub. (Accessed in 2021) Github rest api. https://docs.github.com/en/
rest.
[62] ‚Äî‚Äî. (Accessed in 2021) Search. https://docs.github.com/en/rest/
reference/search.
[63] R. Bavishi, C. Lemieux, R. Fox, K. Sen, and I. Stoica, ‚ÄúAutopandas:
neural-backed generators for program synthesis,‚Äù in Conference on Object-
Oriented Programming, Systems, Languages, and Applications, 2019, pp.
1‚Äì27.
251[64] X. Yang, K. Tang, and X. Yao, ‚ÄúA learning-to-rank approach to software
defect prediction,‚Äù IEEE Transactions on Reliability, vol. 64, no. 1, pp.
234‚Äì246, 2014.
[65] T.-D. B. Le, D. Lo, C. Le Goues, and L. Grunske, ‚ÄúA learning-to-rank
based fault localization approach using likely invariants,‚Äù in International
Symposium on Software Testing and Analysis, 2016, pp. 177‚Äì188.
[66] Wikipedia. (Accessed in 2021) Damerau‚Äìlevenshtein distance. https:
//en.wikipedia.org/wiki/Damerau%E2%80%93Levenshtein_distance.
[67] LightGBM. (Accessed in 2021) Lightgbm. https://lightgbm.readthedocs.
io/en/latest/.
[68] LibCST. (Accessed in 2021) Libcst. https://github.com/Instagram/
LibCST.
[69] plotly. (Accessed in 2021) Version 4 migration guide in python. https:
//plotly.com/python/v4-migration.
[70] pandas. (Accessed in 2021) What‚Äôs new in 1.0.0. https://pandas.pydata.
org/docs/whatsnew/v1.0.0.html#deprecations.
[71] PyTorch. (Accessed in 2021) Pytorch. https://pytorch.org.
[72] W. Weimer, T. Nguyen, C. Le Goues, and S. Forrest, ‚ÄúAutomatically
Ô¨Ånding patches using genetic programming,‚Äù in International Conference
on Software Engineering, 2009, pp. 364‚Äì374.
[73] D. Kim, J. Nam, J. Song, and S. Kim, ‚ÄúAutomatic patch generation
learned from human-written patches,‚Äù in International Conference onSoftware Engineering, 2013, pp. 802‚Äì811.
[74] S. Mechtaev, J. Yi, and A. Roychoudhury, ‚ÄúAngelix: Scalable multi-
line program patch synthesis via symbolic analysis,‚Äù in International
Conference on Software Engineering, 2016, pp. 691‚Äì701.
[75] R. K. Saha, Y . Lyu, H. Yoshida, and M. R. Prasad, ‚ÄúElixir: Effective object
oriented program repair,‚Äù in International Conference on Automated
Software Engineering, 2017, pp. 648‚Äì659.
[76] S. Saha, R. K. Saha, and M. R. Prasad, ‚ÄúHarnessing evolution for
multi-hunk program repair,‚Äù in International Conference on Software
Engineering, 2019, pp. 13‚Äì24.
[77] R. Bavishi, H. Yoshida, and M. R. Prasad, ‚ÄúPhoenix: Automated data-
driven synthesis of repairs for static analysis violations,‚Äù in International
Symposium on Foundations of Software Engineering, 2019, pp. 613‚Äì624.
[78] T. Lutellier, H. V . Pham, L. Pang, Y . Li, M. Wei, and L. Tan, ‚ÄúCoconut:
Combining context-aware neural translation models using ensemble for
program repair,‚Äù in International Symposium on Software Testing and
Analysis, 2020, pp. 101‚Äì114.
[79] C. L. Goues, M. Pradel, and A. Roychoudhury, ‚ÄúAutomated program
repair,‚Äù Communications of the ACM, vol. 62, no. 12, pp. 56‚Äì65, 2019.
[80] L. Gazzola, D. Micucci, and L. Mariani, ‚ÄúAutomatic software repair: A
survey,‚Äù IEEE Transactions on Software Engineering , vol. 45, no. 1, pp.
34‚Äì67, 2019.
252