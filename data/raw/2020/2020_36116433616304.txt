TheEarlyBIRDCatches the Bug: On ExploitingEarlyLayersof
Encoder Models for MoreEﬀicient CodeClassification
Anastasiia Grishina
anastasiia@simula.no
Simula ResearchLaboratory
Oslo, NorwayMaxHort
maxh@simula.no
Simula ResearchLaboratory
Oslo, NorwayLeonMoonen
leon.moonen@computer.org
Simula ResearchLaboratory &
BI Norwegian Business School
Oslo, Norway
ABSTRACT
The use of modern Natural Language Processing (NLP) techniques
has shown to be beneﬁcial for software engineering tasks, such
as vulnerability detection and type inference. However, training
deepNLPmodelsrequiressigniﬁcantcomputationalresources.This
paperexplorestechniquesthataimatachievingthebestusageof
resources andavailable information in thesemodels.
We propose a generic approach, EarlyBIRD, to build compos-
ite representations of code from the early layers of a pre-trained
transformer model. We empirically investigate the viability of this
approach onthe CodeBERT model by comparingthe performance
of 12 strategies for creating composite representations with the
standardpractice ofonly usingthe last encoderlayer.
Ourevaluationonfourdatasetsshowsthatseveralearlylayer
combinations yield better performance on defect detection, and
some combinations improve multi-class classiﬁcation. More specif-
ically,weobtaina+2averageimprovementofdetectionaccuracy
on Devign with only 3 out of 12 layers of CodeBERT and a 3.3x
speed-up of ﬁne-tuning. These ﬁndings show that early layers can
beusedtoobtainbetterresultsusingthesameresources,aswellas
toreduce resourceusage duringﬁne-tuningandinference.
CCSCONCEPTS
•Softwareand its engineering ;•Computing methodologies
→Neural networks ;Natural language processing ;
KEYWORDS
sustainability, model optimization, transformer, code classiﬁcation,
vulnerability detection,AI4Code,AI4SE,ML4SE
ACM Reference Format:
Anastasiia Grishina, Max Hort, and Leon Moonen. 2023. The EarlyBIRD
CatchestheBug:OnExploitingEarlyLayersofEncoderModelsforMore
Eﬃcient Code Classiﬁcation. In Proceedings of the 31st ACM Joint European
Software Engineering Conference and Symposium on the Foundations of Soft-
wareEngineering(ESEC/FSE’23),December3–9,2023,SanFrancisco,CA,USA.
ACM,NewYork,NY,USA, 13pages.https://doi.org/10.1145/3611643.3616304
ESEC/FSE ’23, December 3–9, 2023, San Francisco, CA,USA
©2023 Copyright heldby theowner/author(s).
ACM ISBN 979-8-4007-0327-0/23/12.
https://doi.org/10.1145/3611643.36163041 INTRODUCTION
Automationofsoftwareengineering(SE)taskssupportsdevelopers
increationandmaintenanceofsourcecode.Recently,deeplearning
(DL) models have been trained on large open-source code corpora
and used to perform code analysis tasks [ 3,8,27,38]. Motivated
by the naturalness hypothesis stating that code and natural lan-
guage share statistical similarities, researchers and tool vendors
have started training deep NLP models on code and ﬁne-tuning
them on SE tasks [ 11]. Amongst others, such models have been
appliedtotypeinference[ 17],codeclonedetection[ 50],programre-
pair[9,15,47,48],anddefectprediction[ 7,30,35,44].InNLP-based
approaches,SEtasksarefrequentlytranslatedtocodeclassiﬁcation
problems. For example, detection of software vulnerabilities is a
binary classiﬁcation problem, bug type inference is a multi-class
classiﬁcationsetting,andtypeinferenceisamulti-labelmulti-class
classiﬁcation task in case a type is predicted for each variable in
the program.
Most modern NLP models build on the transformer architec-
ture [42]. This architecture uses attention mechanism and consists
ofanencoderthatconvertsaninputsequencetoa representation
throughaseriesoflayers,followedby decoderlayersthatconvert
this representation to an output sequence. Although eﬀective in
terms of learning capabilities, the transformer design results in
multi-layer models that need large amounts of data for training
from scratch. A well-known disadvantage of these models is the
high resource usage that is required for training due to both model
and data sizes. While a number of pre-trained models have been
published recently,ﬁne-tuningthesemodelsforspeciﬁctasks still
requires additionalcomputational resources [ 27].
Thispaperexplorestechniquesthataimatoptimizingtheuseof
resourcesandinformationavailableinmodelsduringﬁne-tuning.
In particular, we consider open white-box models, for which the
weightsfromeachlayercanbeextracted.Wefocusonencoder-only
models,astheyarecommonlyusedforSEclassiﬁcationtasks,in
particular,thetransformer-basedencoders.Thestandardpractice
inencodermodelsistoobtaintherepresentationoftheinputse-
quence from the last layer of the model [ 14], while information
from earlier layers is usually discarded [ 21]. I.e., while the early
layers are used to computethe values of the last layer, they are
generally not considered as individual representations of the in-
put in the way that the last layer is. To exemplify the amount of
discardedinformation atinference,whenﬁne-tuninga 12-layered
encoder,suchasCodeBERT[ 14],forbugdetection,92%ofthecode
embeddingsare ignored.1However,it hasbeenshownfornatural
1That is, theweightsfrom 11 out of 12 layers areignored for classiﬁcation.
Thiswork islicensedunderaCreativeCommonsAttribution4.0Interna-
tional License.
895
ESEC/FSE ’23, December3–9, 2023,San Francisco, CA, USA AnastasiiaGrishina, MaxHort,andLeonMoonen
language that the early layers of an encoder capture lower-level
syntacticalfeaturesbetterthanthelaterlayers[ 6,24,32,40],which
can beneﬁtdownstream tasks.
Inspiredbythelineofresearchthatexploitsearlylayersofmod-
els,weproposeEarlyBIRD,2anovelandgenericapproachforbuild-
ingcompositerepresentationsfromtheearlylayersofapre-trained
encodermodel.EarlyBIRDaimstoleverageallavailableinforma-
tioninexistingpre-trainedencodermodelsduringﬁne-tuningto
either improve results or achieve competitive results at reduced
resource usage during code classiﬁcation. We empirically evaluate
EarlyBIRDonCodeBERT[ 14],apopularpre-trainedencodermodel
forcode,andfourbenchmarkdatasetsthatcoverthreecommonSE
tasks:defectdetectionwiththeDevignandReVealdatasets[ 20,51],
bug type inference with the data from Yasunaga et al. [ 47], and
exceptiontypeclassiﬁcation[ 7].Theevaluationcomparesthe base-
linerepresentation that uses the last encoder layer with results
obtained via EarlyBIRD. We both ﬁne-tune the full-size encoder
and its pruned version with only several early layers present in
the model. The latter scenario analyzes the trade-oﬀ between only
using apartialmodelandthe performance impact onSE tasks.
Contributions: Inthispaper,wemakethefollowingcontributions:
(1) We propose EarlyBIRD, an approach for creating composite rep-
resentations of code using the early layers of a transformer-based
encoder model. The goal is to achieve better code classiﬁcation
performanceatequalresourceusageorcomparableperformance
at lower resourceusage.
(2)Weconductathoroughempiricalevaluationoftheproposedap-
proach.WeshowtheeﬀectofusingcompositeEarlyBIRDrepre-
sentations while ﬁne-tuning the original-size CodeBERT model on
fourreal-worldcodeclassiﬁcationdatasets.WerunEarlyBIRDwith
10diﬀerentrandominitializationsofnon-ﬁxedtrainableparame-
tersandmarktheEarlyBIRDrepresentationsthatyieldstatistically
signiﬁcant improvement over the baseline.
(3) We investigate resource usage and performance of pruned models.
We analyze the trade-oﬀ between removing the later layers of a
modelandthe impact this has onclassiﬁcation performance.
Mainﬁndings: WithEarlyBIRD,weachieveperformanceimprove-
mentsoverthebaselinecoderepresentationwiththemajorityof
representations obtained from single early layers on the defect
detectiontaskandselectedcombinationsonbugtypeandexcep-
tiontypeclassiﬁcation.Moreover,outofthereduced-sizemodels
withprunedlater layers, we obtaina +2average accuracy improve-
mentonDevignwith3.3xspeed-upofﬁne-tuning,aswellas+0.4
accuracyimprovement with3.7xspeed-uponaveragefor ReVeal.
The remainder of the paper is organized as follows. We present
related work in Section 2and provide background details of the
studyinSection 3.ThemethodologyisdescribedinSection 4which
is followed by experimental setup in Section 5. We present and
discuss results inSection 6andconclude withSection 7.
2 RELATED WORK
Here, we give an overview of language models for SE tasks and
recent encoder models, speciﬁcally, as well as diﬀerent approaches
to use early layers ofencoder models.
2Early-layer Based ImprovementorReduction of resourcesuseD2.1 Transformers in SoftwareEngineering
The availability of open source code and increased hardware capa-
bilities popularized training and usage of Deep Learning, including
NLPandLargeLanguageModels(LLMs),forSEtasks.Todate,deep
NLPmodelshavealreadybeenappliedinatleast18SEtasks[ 28].
Pre-trainedlanguagemodelsavailableforﬁne-tuningonSEtasks
largelybuildonthetransformerarchitecture,sequence-to-sequence
models,andtheattentionmechanism[ 8,9,42].Onewidelyused
benchmarktotestdiﬀerentdeeplearningarchitecturesonSEtasks
is CodeXGLUE [ 27]. The benchmark provides data, source code for
modelevaluation,andaleader-boardrankingmodelperformance
ondiﬀerenttasks[ 27].
SEtaskscanbetranslatedtoinputsequenceclassiﬁcationand
generationofcodeor text.Examplesofgenerative tasksinSE are
codecompletion, coderepair,generation ofdocumentation from
code and vice versa, and translation between diﬀerent program-
ming languages. Such tasks are frequently approached with neural
machinetranslationmodels.Fulltransformermodelsfortranslation
fromaprogramminglanguage(PL)toanaturallanguage(NL)or
PL-PLtasksincludePLBART[ 1],PYMT5[ 10],TFix[4],CodeT5[ 43],
Break-It-Fix-It[ 47].Alternatively,generativemodelscaninclude
the decoder-only part of the transformer as in GPT-type models.
In this case, the decoder both represents the input sequence and
transforms it into theoutput sequence.Decoder-based modelsfor
code include,for example,Codex andCodeGPT [ 8,27].
Inthetasksthatrequirecodeordocumentationrepresentation
and their subsequent classiﬁcation, the encoder-only architectures
are used more frequently than in translation tasks. Examples of
code classiﬁcation problems are code clone detection, detection of
general bugs, such as the presence of swapped operands, wrong
variablenames,syntaxerrors,orsecurityvulnerabilities.Anumber
of encoder models for code applied a widely-used bi-directional
encoder,BERT[ 12],topre-trainitoncode,withsomemodiﬁcations
oftheinput.Inthisway,theCodeBERT[ 14],GraphCodeBERT[ 16],
CuBERT [ 20], and PolyglotCodeBERT [ 2] models were created.
Indetail,the12-layerRoBERTa-basedCodeBERTmodelwaspre-
trainedonNL-PLtasksinmultiplePLsandutilizedonlythetextual
features of code. Note that RoBERTa is a type of BERT model with
optimized hyper-parameters and pre-training procedures [ 26]. To-
getherwiththedecoder-onlyCodeGPTmodel,theencoder-only
CodeBERT model was used as a baseline in CodeXGLUE. Graph-
CodeBERTutilizesbothtextualandstructuralpropertiesofcodeto
encodeitsrepresentations.PolyglotCodeBERTistheapproachthat
improves ﬁne-tuning ofthe CodeBERTmodel ona multi-lingual
dataset for a target task even if the target task tests only one PL.
This paper focuses on the ﬁne-tuning strategies which, in con-
trast to PolyglotCodeBERT, do not increase the resource usage for
ﬁne-tuning.CuBERTisa24-layerpre-trainedtransformer-based
encoder testedon a numberof codeclassiﬁcationtasks, including
exceptiontypeclassiﬁcation.Wetesttheperformanceofthepro-
posed EarlyBIRD composite representations on defect detection,
including the use of one of CodeXGLUE benchmarks, as well as
on error and exception type classiﬁcation tasks. However, the goal
of this paper is to achieve improvement over the baseline model
whenitisﬁne-tuned withcompositecoderepresentations.Wedo
notaimtocompareresultswithothermodels,butratherpropose
896TheEarlyBIRDCatchesthe Bug: On Exploiting Early Layersof EncoderModels... ESEC/FSE ’23, December3–9, 2023,San Francisco, CA, USA
anapproachthatisapplicabletotransformer-basedencodersfor
source code and show its performance gains compared to the same
modelusage withoutthe proposedapproach.
2.2 Use ofEarlyEncoderLayers
A number of studies explored diﬀerent approaches to use informa-
tion from early layers of DL models for sequence representation,
suchasprobingsinglelayers,pruningandvariablelearningrates.
Onewaytoleverageinformationfromearlymodellayersistogive
diﬀerent priority to layers while ﬁne-tuning the models [ 19,39].
For example, the layer-wise learning rate decay (LLRD) strategy
and re-initialization of late encoder layers yielded improvement
overthestandardﬁne-tuningofBERTonNLPtasks[ 49].TheLLRD
strategy was initially developed to tune the later encoder layers
with larger learning rate. In this way, the later layers can be better
adapted to a downstream task under consideration, because the
later layers are assumed to learn complex task-speciﬁc features
of input sequences [ 19]. Moreover, Peters et al. [ 33] showed that
theperformanceofﬁne-tuningimprovesiftheencoderlayersare
updatedduringﬁne-tuningincomparisonwithtrainingonlythe
classiﬁer ontop ofﬁxed(frozen) encoder layers.
Pruning later layers of transformer models is another way to
consider only early layers for ﬁne-tuning [ 13,31,36]. Sajjad et
al.[36]investigatedhowtheperformanceoftransformermodels
onNLPisaﬀectedwhenreducingtheirsizebypruninglayers.They
consideredsixpruningstrategies,includingdroppingfromdiﬀerent
directions,alternated layerdropping,ordroppinglayersbasedon
importance, for four pre-trained models: BERT [ 12], RoBERTa [ 26],
XLNET[46],ALBERT[ 22].Bypruningmodellayers,Sajjadetal.
were able to reduce the number of parameters to 60% of the ini-
tialparametersetwhilemaintainingahighlevelofperformance.
While the performance on downstream tasks varies in their study,
the lower layers are critical for maintaining performance when
ﬁne-tuning for downstream tasks. In other words, dropping ear-
lierlayersisdetrimentaltoperformance.Overall,pruninglayers
reducesmodelsizeandinturnreducesﬁne-tuningandinference
time.InlinewiththeworkofSajjadetal.[ 36],weextendourexper-
imentswiththepruningoflaterlayersandkeepingearlierlayers
present inthe model(see RQ2 inSection 6).
Theuseofinformationfromsingleearlylayers inanumberof
EarlyBIRDexperimentsisalsoinspiredbyPetersetal.[ 32].Intheir
study, Peters et al. present an empirical evidence that language
models learn syntax and part-of-speech information on earlier lay-
ers of a neural network, while more complex information, such
as semantics and co-reference relationships, are captured better
by deeper (later) layers. In another study, Karmakar and Robbes
probed pre-trained models of code,including CodeBERT, on tasks
of understanding syntactic information, structure complexity, code
length,andsemanticinformation[ 21].WhileKarmakarandRobbes
probedfrozen earlylayersof diﬀerentmodelsforcodeinasingle
strategy,weuse12diﬀerentstrategiesforcombiningunfrozenearly
layersduringﬁne-tuningandfocusonthetasksofbugdetection
or bug type classiﬁcation. Similarly, Hernández López et al. [ 18]
probed diﬀerent layers of ﬁve pre-trained models, including Code-
BERT[14]andGraphCodeBERT[ 16],andfoundthatmostsyntactic
information is encoded in the middle layers. The novelty of ourstudy with respect to Karmakar and Robbes is that we combine
earlylayersinadditiontoextractingeachofthem,whileKarmakar
and Robbes extracted early layer representations and used them
withoutcomposing newrepresentations.
3 ENCODERS FORCODE CLASSIFICATION
In this section,we presentthebackgroundontransformermodels
and diﬀerent uses of the encoder-decoder—or full transformer—
architecture, as well as its encoder-only and decoder-only variants.
Because our study focuses on encoder-only open-source models
availableforﬁne-tuning,thedistinctionbetweentransformertypes
isnecessary for understanding the methodology.
Insequence-to-sequencegenerationscenarios,thetransformer
modelconsistsofamulti-layerencoderthatrepresentstheinput
sequence and a decoder that generates the output sequence based
on the sequence representation from the encoder and the available
output generated at previous steps [ 42]. For source code classi-
ﬁcation tasks, the transformer is frequently reduced to only its
encoder followed by a classiﬁcation head , a component added to
theencodertocategorizetherepresentationintodiﬀerentclasses.
Dropping the decoder for classiﬁcation is motivated by resource
eﬃciency, because the decoder is conceptually only needed for
token generation from the input sequence. During classiﬁcation of
aninput,theencoder representsthesequenceandpassesittothe
classiﬁcationhead.Basedonthisdesign,anumberofpre-trained
encodershavebeenpublishedinrecentyears,suchasBERTand
RoBERTa which were pre-trained on natural language, and similar
models pre-trained oncode, or a combinationof code and natural
language[ 12,26].Thegoalofpre-traininginthe pre-trainandﬁne-
tunescenario is to capture language patterns in general, so that
they can serve as a basis for domain-speciﬁc downstream tasks.
Pre-trainedmodelscanbeﬁne-tunedondiﬀerentdownstreamtasks
inNLPandSE.
Processingtheinputsequencefor classiﬁcationconsistsofsev-
eralsteps: tokenization, initial embedding, encoding the sequence
with an encoder, and passing the sequence representation through
aclassiﬁcationhead .Tokenizationsplitstheinputsequence,adds
specialtokens, matches thetokens totheirID’sinthevocabulary
of tokens, and uniﬁes the resulting token length for samples in
a dataset. Embedding transforms the one-dimensional token ID
toaninitialmulti-dimensionalstaticvectorrepresentationofthe
token andisusually apart of thepre-trainedencoder model.This
representation is updated using the attention mechanism of the
encoder. Because of attention, the representation of the input is
inﬂuencedbyalltokens inthe sequence,soitiscontextualized.
CodeBERTisaRoBERTa-basedmodelwith12encoderlayerspre-
trainedon6programminglanguages(Python,Java,JavaScript,PHP,
Ruby, and Go), as wellas text-to-code tasks [ 14]. Pre-trainingwas
done on the masked language modeling (MLM) and replaced token
detection(RTD)tasks.Thesetasksrespectivelytrainthemodelto
derive what token is masked in MLM, and in RTD predict whether
any token in an original sequence is swapped with a diﬀerent
token that should not be in the sequence. CodeBERT outputs a
bidirectional encoder representation of the input sequence, which
means that the model considers context from pre-pending and
subsequent wordsto represent eachtoken inthe inputsequence.
897ESEC/FSE ’23, December3–9, 2023,San Francisco, CA, USA AnastasiiaGrishina, MaxHort,andLeonMoonen
A pre-trained model is usually released with a pre-trained to-
kenizer. The pre-trained tokenizer ensures that token ID’s corre-
spondtothoseprocessedduringpre-training.Thetokenizeralso
addsspecialtokens,suchasa CLStokenatthestartofeachinput
sequence, PADtokensto unifylengths ofinputsequences,andthe
EOStoken to signify the end of the input string and the start of
padding sequence [ 12]. All tokens are transformed by the model
ineachencoderlayer.Outofalltokens,the CLStokenrepresenta-
tionfromthelastlayer,whichisupdatedbyallencoderlayers,is
typicallyusedas arepresentation for the full sequence.
The standard practice of using the CLStoken from the last en-
coderlayerismotivatedbythepre-trainingprocedure.Forexample,
in MLM, the model predicts the masked token based on the CLS
tokenrepresentationfromthe12thlayerofBERTandCodeBERT.
However, the choice of token to represent the full sequence in ﬁne-
tuning can be diﬀerent. For example, in PLBART [ 1], a transformer
modelforcodewithbothanencoderandadecoder,the EOStokenis
usedforrepresentingtheinputsequence.Inthispaper,wepropose
diﬀerentwaystorepresenttheinputsequenceanduseinformation
from early layers ofthe modelinan eﬀective way.
4 METHODOLOGY
In this paper, the architecture of the code classiﬁcation model con-
sists of ﬁve parts: (1) a tokenizer, (2) an embedding layer, (3) an
encoderwithseverallayers,(4)asetofoperationstocombinese-
quencerepresentationsfromencoderlayerswithEarlyBIRD,and
(5)aclassiﬁcationhead.Theoutputofeachstepisusedasinputinto
the next step. An overview of the architecture is shown in Figure 1
anddescribedbelow.Themaindiﬀerencebetweenthisarchitecture
and the classiﬁcation architecture discussed in Section 3is step (4);
the standardarchitecture only consists ofsteps (1–3)and(5).
Steps (1)–(3) use a pre-trained tokenizer, embedder, and encoder.
EarlyBIRDisformulatedinagenericwayandcanbeappliedtoany
encoder, but for our experiments, we ﬁx the CodeBERT model and
tokenizer.Instep(4),wecombineinformationfromallthelayers
or from only some of the early layers of the encoder, as opposed
tothebaselinethatusesthelastlayeroftheencoder.Finally,the
classiﬁcationheadinstep(5)consistsofonedropoutlayerandone
linearlayer withsoftmax.
The encodermodel represents eachtoken of an input sequence
withavectorof size /u1D43B,alsoknownashiddensize.For eachinput
sequenceoflength /u1D446,andahiddensize /u1D43B,weobtainamatrixofsize
/u1D446×/u1D43Bforeachof /u1D43FlayersofthebasemodelasshowninFigure 1.
Input
source
code
sample i(1) Tokenizer(3) Encoder
Layers
1
(5) Classiﬁcation
head  L(4) Combination
Layers
HH1 RS
SCLS
token i1
...
token iN
EOS
PAD
...
PAD(2) Embedding
Layer
HS
Figure 1:Modelarchitecture forcodeclassiﬁcation.Forexample,theCodeBERTarchitectureisﬁxedwith12encoder
layers,i.e., /u1D43F=12forthatmodel.Alltheinformationavailablein
the encoder for one input sequence is stored in a tensor of size
/u1D43F×/u1D446×/u1D43B.TheEarlyBIRDcombinationsmustproduceonevector /vec/u1D445
ofsize/u1D43Bthatrepresentsthe input,asshown inFigure 1.Keeping
the output code representation of size /u1D43Bis required to provide a
faircomparisonofEarlyBIRDcompositerepresentationswiththe
standardcoderepresentationobtainedfromthelastlayer.Inthis
way, the dimension of the classiﬁcation head is the same for all
combinations of early layers and has minimal possible inﬂuence
duringﬁne-tuning.
Asastrategyforsystematicallyinvestigatingcompositerepre-
sentations, we create a grid-search over three typical operations
tocombineoutputsofneuralnetworklayers–maximumpooling
(max pool), weighted sum and slicing – and two dimensions to
apply the operations: over tokens and/or layers. For the tokens
dimension,weeitheruseallofthetokensfromaspeciﬁclayeror
onlytheCLStoken.Amonglayers,weeithersliceonelayer,sumor
takemaximumvaluesoveralllayers. Thechoiceofconsideringev-
ery token of a layer is motivated by the fact the transformer-based
modelsexhibitvaryingdegreesofattentionfordiﬀerenttypesofto-
kens[29],whichindicatesthatsolelyusingthe CLStokenmightnot
be the best choice for tasks [ 37]. We also experiment with diﬀerent
sizesofthemodel.Thecombinationstrategiesthatusealllayersof
thepre-trainedmodelaredividedintotwocategories:thestrategies
thatuseCLStokensfromtheencoderlayers;thestrategiesthatuse
more tokens thanjust CLSfrom encoder layers.
Whenweslicethe CLStokenandapplyeachoftheoperations
over layers, we obtain the following CLS-tokencombinations:
(i)baseline:CLStoken from the last layer,i.e.,layer no. /u1D43F;
(ii)CLStoken from one layer3no./u1D459, /u1D459∈ {1,...,(/u1D43F−1)};
(iii) max poolover CLStokens from alllayers {/u1D459}/u1D43F
/u1D459=1;
(iv) weightedsum over CLStokens from alllayers {/u1D459}/u1D43F
/u1D459=1.
Thesecondsetofcombinationsusesrepresentationsofallthe
tokens intokenizedinputsequences,includingthe CLStoken. We
ﬁrstapplymaxpoolingoperationtoeitheralltokensoralllayers
and use the rest of operations. Then we apply weighted sum as the
ﬁrstoperation followedbymax poolorslicing of alayer:
(v) max pooltokens from one layer no. /u1D459, /u1D459∈ {1,...,/u1D43F};
(vi)maxpooloveralllayersforeachtokenintheinputsequence,
max poolover tokens;
(vii)maxpooloveralllayersforeachtokenintheinputsequence;
weightedsum over tokens;
(viii)max pool over all tokens for each layer no. /u1D459, /u1D459∈ {1,...,/u1D43F};
weightedsum over layers
(ix)weightedsumovertokensfromonelayerno. /u1D459, /u1D459∈ {1,...,/u1D43F};
(x)weighted sum over tokens for each one layer no. /u1D459, /u1D459∈
{1,...,/u1D43F};weightedsum over alllayers;
(xi)weighted sum over all layers for each token in the input
sequence;weightedsum over alltokens.
Notethatweightsintheweightedsumsarelearnableparameters.
However,theaddednumberoflearnableparametersforﬁne-tuning
3Weuseeachlayer /u1D459inthecombinationsseparatelyifwedenote /u1D459, /u1D459∈ {1,...,/u1D43F},
and specifythe set of layers {/u1D459}/u1D43F
/u1D459=1if several layersareused at once.
898TheEarlyBIRDCatchesthe Bug: On Exploiting Early Layersof EncoderModels... ESEC/FSE ’23, December3–9, 2023,San Francisco, CA, USA
Layer
1...
Layer
l...
Layer
LR CLS
token i1
...
token iN
EOS
PAD
...
PAD
(a)Baseline:CLStokenoflayerL(i).Layer
1...
Layer
l...
Layer
LR CLS
token i1
...
token iN
EOS
PAD
...
PAD
(b)Fullmodel,CLSoflayer l<L(ii).Layer
1...
Layer
lR CLS
token i1
...
token iN
EOS
PAD
...
PAD
(c) Prunedmodel,CLS of last layer l(xii).
Layer
1...
Layer
l...
Layer
L...
...max pooling (iii)
weighted sum (iv) 
R
extracted
CLS
tokensCLS
token i1
...
token iN
EOS
PAD
...
PAD
(d)Combinations of CLS tokens fromalllayers(iii,iv).Layer
1...
Layer
l...
Layer
Lmax pooling (v)
or
weighted sum (ix)
over tokens
for layer l
Layer
lRCLS
token i1
...
token iN
EOS
PAD
...
PAD
(e)Combinations of alltokens froma singlelayer (v,ix).
max pooling (vi, vii)
or
weighted sum (xi)
over layers
for each tokenmax pooling (vi)
or
weighted sum
(vii, xi)
over tokens 
R
Layer
1Layer
lLayer
L...
......
......
...... ......
...
combined information
across layersCLS
token i1
...
token iN
EOS
PAD
...
PAD
(f) Combinations of all tokens combined across layers using max
pooling or weightedsum (vi, vii, xi).Layer
1...
Layer
l...
Layer
Lmax pooling (viii)
or
weighted sum (x)
over tokens
for each layer l...
...weighted sum
(viii, x) 
R
combined information
across tokensCLS
token i1
...
token iN
EOS
PAD
...
PAD
(g)Combinationsofalltokenscombinedforeachsinglelayerﬁrst
usingmaxpooling or weightedsum (viii,x).
Figure2:Combinationsofearlyencoderlayersthatleadtocoderepresentationvector /vec/u1D445foreachtokenizedinputsequence.
Thelatin numbering in brackets corresponds tothecombinations describedin Section 4. Observe that thepresentation order
hasbeen designed to preserve spaceby grouping similar combinationsinthesamesubﬁgure.
constitutes 0.00042%4of the number of learnable parameters in the
baselineconﬁguration.Forthisreason,wementionthatthemodels
withcombinations (ii-x)have thesamemodelsize whilebearingin
mindthe overheadoflearnableweightsinthe weightedsums.
In addition to experiments with token combinations, we also
investigate performance of the model with ﬁrst /u1D459</u1D43Flayers and
the baselinetoken combination, describedas follows:
(xii)CLStokenfromthelastlayerofthemodelwith /u1D459</u1D43Fencoder
layers.
Note that the baseline combination (i) with the usage of the CLS
token from layer /u1D43Fcorrespondsto (ii)and(xii) if /u1D459=/u1D43F.
4Weighted sum over tokens adds /u1D446=512learnable weights. Because the weights
of the sum are shared across the layers, the maximum number of added weights is
/u1D43F+/u1D43B=524out of 124M learnable weights in the base model. Combinations without
weighted sums do not add extra learnable parameters to the base model. Weighted
sum over layersadds learnable /u1D43F=12weightsfor CodeBERT.The combinations are presented in Figure 2. Similar combina-
tionsarepresentedclosetoeachotherorarecombinedinthesame
imageiftheyonlyhaveminordiﬀerencesandsharethemajorparts.
Forexample,in Figure 2c,weillustratecombinations (iii)and(iv),
becausebothofthemuse CLStokensfromalllayerscombinedusing
max pooling or weighted sum. The roman numbers which indicate
combinationtypesarepreservedeitherinthedescriptionsbelow
theﬁguresorintheﬁguresthemselves,buttheorderischanged.
We mention combination number corresponding to the description
inthecurrentsection,suchasbaselinecombination(i)inFigure 2a
or combination (ii) for CLStoken from one early layer in Figure 2b.
We highlight what parts of encoder layer outputs are used for each
combination with color. White cells correspond to the tokens that
are not used in early layer combinations. The goal of all combi-
nationsistoobtainavectorrepresentation /vec/u1D445foreachinputcode
899ESEC/FSE ’23, December3–9, 2023,San Francisco, CA, USA AnastasiiaGrishina, MaxHort,andLeonMoonen
sample. For example, in Figure 2a, we consider the last layer /u1D43Fand
extractonly the CLStoken markedas /vec/u1D445.
AnotherremarkontheEarlyBIRDcombinationsconcernsthe
usage of all tokens or only code tokens. Code tokens are those that
correspondto tokenizedinputwords or sub-words and are shown
in Figure 2astoken/u1D4561,...,token/u1D456/u1D441for an input sequence /u1D456of size
/u1D456/u1D441.For each combination that uses more than just a CLStoken, i.e.,
combinations (v-xi), we experiment with code tokens only, as well
as with all tokens, including CLS,EOS, andPAD. The motivation
to check code tokens exclusively stems from the hypothesis that
information inspecialtokens mayintroduce noiseintoresults.
5 EXPERIMENTALSETUP
In this section, we describe the datasets used for empirical evalua-
tionandimplementationdetailsofﬁne-tuningwiththeproposed
EarlyBIRD approach. We investigate binary and multi-task code
classiﬁcation scenarios to explore generalisability ofour results.
5.1 DatasetsforSourceCodeClassiﬁcation
Weﬁne-tuneandtesttheCodeBERTmodelusingtheEarlyBIRD
approach on four datasets. The datasets span three tasks: defect
detection,errortypeclassiﬁcationandexceptiontypeclassiﬁcation
— with 2, 3, and 20 classes, respectively. They also contain data
intwoprogramminglanguages,C++andPython.Inaddition,the
chosen datasets have similar train subset sizes. In this way, we aim
toreducetheeﬀectofthemodel’sexposuretodiﬀerentamounts
of training data during ﬁne-tuning. Statistics of the datasets are
providedinTable 1.We reportthesize ofthetrain/validation/test
splits. In addition, we compute the average number of tokens in
the input sequences upon tokenization with the pre-trained Code-
BERT tokenizer. Because the maximum input sequence size for the
CodeBERT model is limited to /u1D446=512,the number of tokens is
indicative of how much information the model gets access to or
howmuchinformation iscut oﬀ,incaseoflonginputs.
Devign: This dataset contains functions in C/C++ from two open-
sourceprojectslabelledasvulnerableornon-vulnerable[ 51].We
reuse the train/validation/test split from the CodeXGLUE Defect
detection benchmark.5The dataset is balanced: the ratio of non-
vulnerable functionsis54%.
ReVeal: Similarly to Devign, ReVeal is a vulnerability detection
dataset of C/C++ functions [ 7]. The dataset is not balanced: it con-
tains90%non-vulnerablecodesnippets.BoththeDevignandRe-
Veal datasets contain real-world vulnerable and non-vulnerable
functionsfrom open-sourceprojects.
Break-It-Fix-It(BIFI): Thedatasetcontainsfunction-levelcode
snippets in Python with syntax errors [ 47]. We use the original
buggyfunctionsandformulateataskofclassifyingthecodeinto
threeclasses:UnbalancedParentheseswith43%ofthetotalnumber
of code examples in BIFI, Indentation Error with 31% code samples,
InvalidSyntaxcontaining26%samples.Thetrain/testsplitprovided
inthedatasetisreused,andthevalidationsetisextractedas10%
oftraining data.
ExceptionType: ThedatasetconsistsofshortfunctionsinPython
withaninserted __HOLE__ tokeninplaceofoneexceptionincode.6
5https://github.com/microsoft/CodeXGLUE/tree/main/Code-Code/Defect-detection
6https://github.com/google-research/google-research/tree/master/cubertTable 1:Statistics ofFine-TuningDatasets.
Dataset #classesAvg #
tokens#code samples
Train Valid Test
Devign 2 614 21,854 2,732 2,732
ReVeal 2 512 18,187 2,273 2,274
BIFI 3 119 20,325 2,259 15,055
ExceptionType 20 404 18,480 2,088 10,348
The task is to predict one of 20 masked exception types for each
input function and is unbalanced. The dataset was initially created
from the ETH Py150 Open corpus7as described in the original
paper [20]. We reuse the train/validation/test split provided by the
authors.
5.2 Implementation
ThearchitectureisbasedontheCodeBERT8tokenizerandencoder
model.Themodeldeﬁnesthemaximumsequencelength,hidden
size, and has 12 layers, so /u1D446=512,/u1D43B=768,/u1D43F=12.Hyper-
parameters in the experiments are set to /u1D435=64, learning rate is
1/u1D452-5,anddropoutprobabilityis 0.1.Ifthetokenizedinputsample
is longer than /u1D446=512, we prune the tokens in the end to make the
inputﬁtintothemodel.Werunﬁne-tuningwithAdamoptimizer
andtestingforeachcombination10timeswithdiﬀerentseedsfor10
epochsandreporttheperformanceforthebestepochonaverage
over 10 runs. The best epoch is deﬁned by measuring accuracy
on a validation set. We use Python 3.7 and Cuda 11.6, and run
experiments onone NvidiaVolta A100GPU.
5.3 EvaluationMetrics
To present the impact of early layer combinations, we compare
the accuracy on the test set for all datasets, because it allows us
to compare our results with other benchmarks. In addition, we
reportweightedF1-scoredenotedasF1(w)foradetailedanalysis
of selectedcombinationsto account for class imbalance. To obtain
theweightedF1-score,theregularF1-scoreiscalculatedforeach
labelandtheirweightedmeanistaken.Theweightsareequalto
the number ofsamples inaclass.
WealsoreportresultsoftheWilcoxonsigned-ranktestonthe
corresponding metrics for the combinations that show improve-
mentoverthebaseline[ 45].TheWilcoxontestisanon-parametric
testsuitableforthesettinginwhichdiﬀerentmodelvariantsare
testedonthesametestset,becauseitisapairedtest.TheWilcoxon
testchecksthenullhypothesiswhethertworelatedpairedsamples
comefromthesamedistribution.Werejectthenullhypothesisif
p-valueislessthan /u1D6FC=0.05.Incaseweobtainimprovementofa
metricoverthebaselinewithanEarlyBIRDcombinationandthe
nullhypothesisisrejected,weconcludethatthecombinationper-
formsbetterandtheresultisstatisticallysigniﬁcant.Forthepruned
models, we compute Vargha and Delaney’s /u1D43412non-parametric ef-
fectsizemeasureoftheperformancechangeforaccuracyandF1(w)
7https://www.sri.inf.ethz.ch/py150
8https://huggingface.co/microsoft/codebert-base
900TheEarlyBIRDCatchesthe Bug: On Exploiting Early Layersof EncoderModels... ESEC/FSE ’23, December3–9, 2023,San Francisco, CA, USA
withthresholdsof0.71,0.64and0.56forlarge,mediumandsmall
eﬀectsizes[ 41].
5.4 Research Questions
During our empirical evaluation of composite EarlyBIRD code rep-
resentations, we addressthe following researchquestions:
RQ1.CompositeCodeRepresentationswithSameModelSize:
What is the eﬀect of using combinations (ii-xi) of early layers with
thesamemodelsizeincomparisontothebaselineapproachofusing
onlytheCLStokenfromthelastlayer,i.e.,combination(i),forcode
representation on model performance in the code classiﬁcation
scenario? The goal is to ﬁnd out whether any of the EarlyBIRD
combinationtypesworkconsistentlybetterfordiﬀerentdatasets
andtasks.
RQ2.PrunedModels: Whatistheeﬀectofreducingthenumberof
pre-trained encoder layers in combinations (xii) on resource usage
andmodelperformanceoncodeclassiﬁcationtasks?Asopposedto
RQ1,inwhichweconsiderthecombinationsthatdonotreducethe
modelsize, this research question is devotedto investigationof the
trade-oﬀ between using less resources with reduced-size models
andperformance variation interms ofclassiﬁcation metrics.
For both research questions, we evaluate the composite repre-
sentationsonbinaryandmulti-taskcodeclassiﬁcationscenariosto
explore generalisability oftheresults obtained for the binary case.
We investigate if and what combinations result in better perfor-
mance,averagedover10runswithdiﬀerentseeds.Forcombinations
that improve the baseline on average, we also explore if the results
are statisticallysigniﬁcant according to the Wilcoxon test.
6 RESULTS AND DISCUSSION
6.1 EarlyBIRD with Fixed-Size Models
To answer RQ1, we explore one-layer combinations, multi-layer
combinations,andestimatethestatisticalsigniﬁcanceoftheper-
formanceimprovement.
6.1.1 Combinationsof Tokens in Single Selected Early Layers. Fig-
ure3shows a heatmap of the diﬀerence of the mean accuracy
obtainedwitheachcombinationthatusesonlyoneselectedearly
layercomparedtothebaseline.Inaddition,weshowthevalueof
the diﬀerence in mean accuracy for each combination type and
layer number. Note that the scale is logarithmic and in the most
extreme case spans the interval from ca. -37 to +2. Negative values
are shown in black, and positive values are shown in white. Diﬀer-
ences that are statistically signiﬁcant according to the Wilcoxon
test are marked with a star (∗) next to the value. Combinations
that correspond to the baseline are marked with “bsln” and have
zerodiﬀerence,bydeﬁnition.TheresultsfortheweightedF1-score
show a similar pattern as those for the mean accuracy. They are
visualizedinthe same wayinFigure 4.
TheﬁrstrowsinFigures 3aand3bcorrespondtothecombina-
tions (ii) CLS token layer /u1D459. With this combination type, average
improvement over the baseline is achieved with the majority of
earlylayers.Speciﬁcally,weobtainaccuracyimprovementsranging
from +0.2 to +2.0 for Devign in 8 out of 11 layers, and accuracy
improvements from +0.1 to +0.8 for ReVeal in 9 out of 11 layers.123456789101112
Layer(ii) CLS token layer l
(v) max pool all tokens
(v) max pool code tokens
(ix) w sum all tokens
(ix) w sum code tokens-3.3*0.92.0*1.3*1.01.4*1.4*0.7 -0.01 0.01 0.2 bsln
-2.0*-0.8 -0.1 0.4 0.9 1.3*1.0*1.2*1.2*0.9*0.5 -0.2
-2.0*-0.8*-0.1*0.3*1.0*1.3*1.0*1.1*0.7*0.9*0.4*-0.4*
-5.4*-5.0*-4.0*-4.8*-5.1*-4.6*-4.7*-3.7*-4.5*-2.9*-3.5*-3.2*
-5.2*-4.3*-5.3 -4.5 -3.4 -4.4*-5.0*-3.3*-3.9*-3.5*-3.7 -4.6Devign
-10-10+1
(a)Devign,accuracy.
123456789101112
Layer(ii) CLS token layer l
(v) max pool all tokens
(v) max pool code tokens
(ix) w sum all tokens
(ix) w sum code tokens0.8*0.5 0.1 -0.3 0.1 0.6 0.6 0.4 -0.00 0.4 0.2 bsln
0.7*0.8*0.80.8*0.7 0.8 0.9*0.7 0.5 0.3 0.04 0.2
0.7*0.8 0.8 0.8 0.8 0.8 1.0 0.6 0.5 0.4 -0.1 0.1
-1.6 -1.6 -1.5 -1.9 -1.0 -1.6 -1.6*-1.1 -1.2 -1.2 -0.4 0.02
-2.0*-2.3*-1.3 -1.4*-1.4 -1.1 -2.8*-0.6 -1.4 -1.3 -1.6 -0.9ReVeal
-10-10+1
(b) ReVeal, accuracy.
123456789101112
Layer(ii) CLS token layer l
(v) max pool all tokens
(v) max pool code tokens
(ix) w sum all tokens
(ix) w sum code tokens-27.0*-14.7*-11.1*-6.7*-2.8*-1.8*-1.0*-0.7*-0.4*-0.2*-0.03 bsln
-26.4*-16.2*-11.6*-8.2*-3.4*-1.6*-1.1*-0.8*-0.5*-0.2 -0.01 0.1
-26.5*-16.2*-11.6*-8.2*-3.4*-1.6*-1.1*-0.8*-0.5*-0.2*0.01 -0.01
-28.3*-19.9*-15.5*-11.3*-7.1*-4.2*-1.9*-1.5*-1.1*-0.8*-0.5*-0.2*
-28.3*-19.9*-15.5*-11.4*-7.2*-4.4*-1.8*-1.3*-1.2*-0.7*-0.6 -0.3BIFI
-10-10+1
(c) BIFI,accuracy.
123456789101112
Layer(ii) CLS token layer l
(v) max pool all tokens
(v) max pool code tokens
(ix) w sum all tokens
(ix) w sum code tokens-27.3*-15.4*-10.7*-8.2*-7.4*-6.2*-4.2*-2.8*-2.3*-1.7*-1.1*bsln
-27.4*-15.9*-10.7*-8.7*-7.4*-5.1*-3.2*-2.5*-2.1*-1.6*-0.7 -0.00
-27.4*-15.9*-10.7*-8.7*-7.4*-5.2*-3.2*-2.4*-2.0*-1.7*-0.8*0.2*
-36.5*-31.5*-25.9*-22.7*-22.2*-14.6*-13.2*-10.7*-9.4*-8.8*-6.8*-1.0
-36.4*-31.5*-25.9*-22.7*-22.3*-14.7*-13.3*-10.7*-9.7*-9.0*-7.6*-1.0Exception Type
-10-10+1
(d)Exception Type,accuracy.
Figure 3: Diﬀerence of mean accuracy between EarlyBIRD
and baseline ( bsln) performance. The star∗indicates a statis-
ticallysigniﬁcantdiﬀerencew.r.t. the baseline.
Thedynamicofthemetricchangeoverselectedlayernumbersis
diﬀerent for Devign and ReVeal. In detail, the average performance
ofcombination (ii)isbestwithlayer3onDevign(a+2.0accuracy
improvement)andwithlayer1forReVeal(a+0.8accuracyimprove-
ment). The best improvement in terms of F1(w) matches with layer
3for Devignandwithlayer 2for ReVeal, as showninFigure 4.
Max pooling over all available tokens from a selected layer in
combination(v)alsoachievesperformanceimprovementoverthe
baseline, as shown in rows 2 and 3 of Figures 3a,3b. In general,
layers4–11yieldhigheraccuracyandlayers2–11higherF1(w)with
max pooling for Devign than the baseline. For ReVeal, all layers
exceptlayer11resultinbetteraverageaccuracyandlayers2–10
901ESEC/FSE ’23, December3–9, 2023,San Francisco, CA, USA AnastasiiaGrishina, MaxHort,andLeonMoonen
123456789101112
Layer(ii) CLS token layer l
(v) max pool all tokens
(v) max pool code tokens
(ix) w sum all tokens
(ix) w sum code tokens-2.5*2.0*2.4*1.7 1.5 1.8 2.2*1.2 0.7 0.2 0.1 bsln
-1.4*0.1 0.8 1.0 2.0*2.3*1.62.1*2.1*1.8 1.4 -0.4
-1.4*0.1*0.8*0.9*2.0*2.2*1.6*2.0*1.7*1.7*1.4*-0.1*
-5.4*-6.0*-4.1*-5.2*-6.8*-5.2*-5.9*-5.5*-5.8*-3.9*-4.6*-6.6*
-5.0*-3.7 -6.3 -6.0 -4.7*-7.0*-7.2 -4.1*-5.5*-3.8 -6.6 -10.8Devign
-10-10+1
(a)Devign,F1(w).
123456789101112
Layer(ii) CLS token layer l
(v) max pool all tokens
(v) max pool code tokens
(ix) w sum all tokens
(ix) w sum code tokens-1.3*0.6*0.2 0.2 0.3 0.2 0.4*-0.1 -0.2 0.04 0.05 bsln
-1.1*0.04 0.3 0.7*0.4 0.1 0.2 0.3 0.2 -0.03 -0.2 -0.6*
-1.1*0.1*0.3*0.7*0.4*0.2*0.3*0.2*0.2*0.1*-0.2*-0.5*
-2.0*-1.9*-1.9*-2.1*-2.1*-2.0*-1.9*-2.0*-2.0*-1.7*-1.4*-1.2*
-2.2*-2.4 -1.9 -1.9*-1.8 -2.0 -2.4 -1.7 -2.2 -1.8 -2.0 -1.5*ReVeal
-10-10+1
(b) ReVeal, F1(w).
123456789101112
Layer(ii) CLS token layer l
(v) max pool all tokens
(v) max pool code tokens
(ix) w sum all tokens
(ix) w sum code tokens-27.2*-14.8*-11.2*-6.8*-2.8*-1.8*-1.0*-0.7*-0.4*-0.2*-0.03 bsln
-26.8*-16.3*-11.7*-8.3*-3.4*-1.6*-1.1*-0.8*-0.5*-0.2 -0.01 0.1
-26.9*-16.4*-11.8*-8.3*-3.5*-1.6*-1.1*-0.8*-0.5*-0.2*0.01 -0.01
-28.5*-20.1*-15.7*-11.5*-7.1*-4.2*-1.9*-1.5*-1.1*-0.8*-0.5*-0.2*
-28.5*-20.1*-15.7*-11.6*-7.4*-4.4*-1.8*-1.3*-1.2*-0.7*-0.6 -0.3BIFI
-10-10+1
(c) BIFI,F1(w).
123456789101112
Layer(ii) CLS token layer l
(v) max pool all tokens
(v) max pool code tokens
(ix) w sum all tokens
(ix) w sum code tokens-30.8*-17.2*-12.0*-9.0*-7.9*-6.5*-4.4*-2.9*-2.4*-1.8*-1.1*bsln
-33.3*-18.8*-12.8*-10.1*-8.3*-5.7*-3.6*-2.6*-2.2*-1.8*-0.8*-0.1
-33.4*-18.9*-12.8*-10.1*-8.2*-5.8*-3.5*-2.6*-2.2*-1.8*-1.0*0.1*
-37.8*-32.7*-26.7*-23.3*-23.3*-15.2*-13.8*-10.8*-9.6*-9.0*-6.8*-1.1
-37.7*-32.8*-26.8*-23.3*-23.4*-15.3*-13.8*-10.8*-9.8*-9.2*-7.6*-0.9Exception Type
-10-10+1
(d)Exception Type,F1(w).
Figure4:DiﬀerenceofmeanweightedF1-scores(F1(w))be-
tweenEarlyBIRDandbaseline( bsln).Thestar∗indicatesa
statistically signiﬁcantdiﬀerencew.r.t. thebaseline.
have higher average F1(w). Max pooling over all tokens, including
special tokens, achieves the best statistically signiﬁcant average
improvement ofaccuracyof+0.9ofallcombinationsfor ReVeal.
The weighted sum of all tokens or code tokens exclusively in
combination(ix)doesnotimprovethebaselineperformance.We
assumethatﬁne-tuningfor10epochsisnotenoughforthistype
ofcombination, becausethe lossatepoch10 onbothtraining and
validationsplitsishigherforcombinations(ix)thanforcombina-
tions with max pooling. Since the goal of this study is to use the
same or less resources for ﬁne-tuning, we have not ﬁne-tuned this
combination for more than10 epochs.Whilecombinations (ii)and(v)perform betterfor themajority
of layers onthe defect detection task, multi-class classiﬁcation for
bugor exception type prediction does not beneﬁt from the combi-
nations to the same extent as the binary task. Only max pooling
of tokens of the last encoder layer achieves better performance
than the baseline for BIFI (+0.1 accuracy, +0.1 weighted F1-score
improvements)andExceptionType(a+0.2accuracy,+0.1weighted
F1-score improvements) datasets.
Theimpactofusingalltokensorcodetokensexclusivelydepends
on the dataset. The diﬀerence between performance of single-layer
combinations with max pooling of all tokens and only code tokens
constitute0.0-0.1accuracyorF1(w).Forthemulti-classtasks,the
average results improve with the use of each later layer in the
model. We obtain performance improvement with the max pooling
combination(v),whileotherone-layercombinationsdonotperform
betterthanthe baseline.
The best performing results on Devign and Exception Type
classiﬁcationdatasetsarestatisticallysigniﬁcantaccordingtothe
Wilcoxon test. For ReVeal, the second best result is statistically
signiﬁcant. We havenot obtainedstatistically signiﬁcantimprove-
ments for BIFI. We explain it by the fact that the baseline metric is
already high, i.e., 96.7 accuracy. Achieving improvement is usually
more challenging when the baselineperforms at this level.
Inessence,thecombinationsthatinvolve CLStokenscorrespond-
ing to the single layer (ii), as well as the max pooling combina-
tions (v) perform better on average for defect detection datasets
DevignandReveal.However,onlythemaxpoolingcombination(v)
of tokens from the last encoder layer outperforms the baseline
onaverageformulti-classdatasetsBIFIandExceptionType.The
weightedsum oftokensfromaselectedlayer(ix) performsworse
thanthebaselineifﬁne-tunedforthesamenumberofepochsforall
tasks. Multi-class classiﬁcation tasks require the information from
the last layer for better performance in our experiments, while the
binary task of defect detection allows us to use early layers and
improve the performance over the baseline.
6.1.2 Multi-LayerCombinations. Theaverageperformancediﬀer-
encewiththebaselineofcombinationsthatutilizeearlylayersis
shown as heatmaps in Figures 5and6. We include the value of the
average performance diﬀerence and add a star (∗) to the number if
the diﬀerence is statistically signiﬁcant. Again, negative values are
showninblack,andpositive valuesare showninwhite.
Whenweuseallinformationfromtheavailablelayers,theim-
provement over the baseline is less than what was observed in
Section6.1.1, where one speciﬁc layer has been used. In detail, out
ofcombinationsthatinvolve CLStokensfromallearlylayers,no
combination performs better than the baseline for ReVeal, BIFI,
orExceptionTypedatasets.However,thebestimprovement(+0.6
accuracy) out of experiments with all layers is obtained on Devign
withtheweightedsumof CLStokensinthecombination(iv),which
islessthanthemaximumimprovementwiththecombinationsfrom
one selected early layer in Section 6.1.1. The improvement of F1(w)
is shown in Figure 6. We obtained slightly better improvements
of F1(w) for Devign, no F1(w) improvement for the unbalanced
ReVealdataset. TheaverageF1(w)diﬀerencewiththebaselinefor
multi-classtasksare the same as accuracydiﬀerence.
902TheEarlyBIRDCatchesthe Bug: On Exploiting Early Layersof EncoderModels... ESEC/FSE ’23, December3–9, 2023,San Francisco, CA, USA
D(iii) max pool CLS tokens
(iv) w sum CLS tokens
(vi) max pool layers, max pool all tokens
(vi) max pool layers, max pool code tokens
(vii) max pool layers, w sum all tokens
(vii) max pool layers, w sum code tokens
(viii) max pool all tokens, w sum layers
(viii) max pool code tokens, w sum layers
(xi) w sum layers, w sum all tokens
(xi) w sum layers, w sum code tokens
(x) w sum all tokens, w sum layers
(x) w sum code tokens, w sum layers-0.01
0.6*
0.3
0.2
-7.1*
-7.3*
-7.5*
-6.8*
-7.1*
-5.3*
-4.2*
-5.5*
R-0.3
-0.3
0.3
0.04
0.02
-0.6
-14.7*
-15.3
-3.6
-0.1*
-1.8*
-2.7
B0.04
0.02
0.01
0.1
-0.1
-0.1
-9.3*
-6.9*
-0.7*
-0.7*
-0.7*
-0.7*
E-0.3
-1.3*
-0.9
-1.0*
-6.0*
-6.1*
-57.2*
-56.0*
-12.3*
-11.9*
-11.5*
-11.7*
-10-10+1
Datasets
Figure 5: Diﬀerence of average accuracy between EarlyBIRD
andbaselineperformanceonD(Devign),R(ReVeal),B(BIFI),
E (Exception Type). The star∗indicates a statistically signiﬁ-
cantdiﬀerencew.r.t. thebaseline.
If we consider the combinations that involve all tokens, the
combination(vi)withtwomaxpoolingoperationsoutperformsthe
baselineforDevign,Reveal,andBIFIwithaccuracyimprovement
between +0.1 and +0.3. No combination that involves all layers
outperforms the baseline on average for Exception Type dataset.
Combinations that involve one max pooling and one weighted
sum of all tokens perform worse or neutral in comparison with
thebaseline.Thecombinationswithonlyweightedsumsperform
worse thanthe baselineonaverage.
Answer to RQ1. EarlyBIRD achieves statistically signiﬁcant ac-
curacy and F1-score improvements for defect detection datasets
byusingsingle-layercombinationsthatinvolvethe CLStoken
or max pooling over all tokens. For bug type and exception type
classiﬁcation,maxpoolingofthetokensfromthelastencoder
layer has improved the performance. Weighted sum of tokens
does not improve performance over the baseline.
6.2 PrunedModels
This section is devoted to the combinations of early layers that are
initialized with the ﬁrst /u1D459</u1D43Fearly layers from the pre-trained
modelandﬁne-tunedas /u1D459-layermodels—combinations(xii).We
startbycomparingtheperformanceofusingthe CLStokenfrom
layer/u1D459of the full-size model, i.e., combination (ii), and using the
CLStoken from layer /u1D459of the model that has /u1D459layers in total —
combination (xii). Figure 7presents average accuracy obtained
with thesetwo combinations dependingon the used layer,aswell
as the baseline combination of using CLSfrom the last layer /u1D43F=
12of CodeBERT. On average, the pruned models with reduced
size perform on par with the full-size model for defect detection
on the balanced Devign dataset, and for bug type and exception
typeclassiﬁcation.However,theperformanceofthetwoanalogicalD(iii) max pool CLS tokens
(iv) w sum CLS tokens
(vi) max pool layers, max pool all tokens
(vi) max pool layers, max pool code tokens
(vii) max pool layers, w sum all tokens
(vii) max pool layers, w sum code tokens
(viii) max pool all tokens, w sum layers
(viii) max pool code tokens, w sum layers
(xi) w sum layers, w sum all tokens
(xi) w sum layers, w sum code tokens
(x) w sum all tokens, w sum layers
(x) w sum code tokens, w sum layers0.1
0.7
0.6
0.4
-11.4*
-11.7*
-10.2*
-10.5*
-12.1*
-7.3*
-5.3*
-7.8*
R-0.2
-0.1
-0.2
-0.3*
-2.4*
-2.0*
-13.6*
-16.5*
-3.0*
-2.0*
-2.7*
-2.4*
B0.04
0.02
0.01
0.1
-0.1
-0.1
-9.5*
-7.0*
-0.7*
-0.7*
-0.7*
-0.7*
E-0.3
-1.3*
-1.1*
-1.3*
-6.6*
-6.6*
-60.9*
-59.8*
-12.1*
-11.9*
-11.3*
-11.4*
-10-10+1
Datasets
Figure6:DiﬀerenceofmeanweightedF1-scorebetweencom-
binationsandbaseline.DatasetsareabbreviatedtoD(Devign),
R (ReVeal), B (BIFI), E (Exception Type). The star∗indicates
astatistically signiﬁcantdiﬀerencew.r.t. the baseline.
combinations diverges for the unbalanced defect detection dataset
ReVeal inlayers 4and6–11.
Most importantly, the results show that reducing the model size
andusingthe CLStokenfromthelastlayerofthereducedmodel
performs on par with the baseline for the defect detection task.
The best improvement with the reduced model is achieved with
the 3-layer encoder for Devign and the 1-layer encoder for ReVeal.
Thisresultshowsthatitispossibletobothreduceresourcesand
improve the model’s performance during ﬁne-tuning on the defect
detection taskwithboth abalancedandunbalanceddataset.
To explore the trade-oﬀ between resource usage and perfor-
mance degradation for bug type and exception type identiﬁcation,
58606264accuracyDevign
88.088.589.089.590.090.5ReVeal
123456789101112
Used layer #687480869298accuracyBIFI
123456789101112
Used layer #45505560657075Exception Type
(ii) full-size model
(xii) reduced model
(i) baseline
Figure7:Modelperformancewithasubsetof /u1D459</u1D43Flayers(xii)
vs.models with alllayers (ii);CLStoken fromlayer /u1D459.
903ESEC/FSE ’23, December3–9, 2023,San Francisco, CA, USA AnastasiiaGrishina, MaxHort,andLeonMoonen
Table2:Comparisonofreducedsizemodelswiththebaseline.Wereportmetricperformanceforthebaselineanddiﬀerencewith
the baseline for reduced models, average time for one-epoch ﬁne-tuning (Time) in mm:ss format, speed-up and performance
variationobtainedwithmodelswith /u1D459layers.Statisticallysigniﬁcantimprovementsaremarkedinbold,statisticallyinsigniﬁcant
performancelossesaremarkedwith*,and /u1D43412eﬀectsizesforaccuracyandF1(w),ifany,areindicatedbycellcolor,respectively
large,medium,and small.The best metricimprovementwith highestspeed-up factorsare underlined.
Devign ReVeal BIFI ExceptionType
/u1D459Time Speed-up Acc F1(w) Time Speed-up Acc F1(w) Time Speed-up Acc F1(w) Time Speed-up Acc F1(w)
128:50 1.0x 61.7 60.4 6:57 1.0x 89.2 88.5 8:41 1.0x 96.7 96.7 7:22 1.0x 75.4 75.3
11.08:03 1.1x +0.3 -0.1* 6:56 1.0x +0.6+0.37:07 1.2x -0.1*-0.1*6:33 1.1x -1.0-1.0
10.07:13 1.2x +0.1+0.36:20 1.1x +0.2 -0.0* 6:22 1.4x -0.3-0.36:01 1.2x -1.8-1.8
9.06:40 1.3x +0.3+0.55:53 1.2x +0.3-0.1*5:46 1.5x -0.5-0.55:29 1.3x -2.2-2.3
8.05:52 1.5x +0.9+1.15:15 1.3x +0.2-0.1*5:13 1.7x -0.6-0.64:55 1.5x -3.0-3.0
7.05:23 1.6x +1.4+2.24:44 1.5x +0.3 +0.24:43 1.8x -1.1-1.14:20 1.7x -4.1-4.4
6.04:54 1.8x +1.4+2.04:25 1.6x +0.3 +0.1 4:10 2.1x -1.7-1.73:50 1.9x -6.1-6.6
5.04:03 2.2x +1.0+1.53:45 1.9x +0.1+0.23:31 2.5x -3.0-3.03:15 2.3x -7.3-7.7
4.03:22 2.6x +1.2+1.63:06 2.2x -0.2*+0.22:53 3.0x -6.8-6.82:41 2.7x -8.3-9.2
3.02:41 3.3x +2.0+2.42:28 2.8x +0.1 +0.32:15 3.8x -11.3-11.32:10 3.4x -10.7-12.0
2.02:00 4.4x +1.0+1.91:52 3.7x +0.4+0.61:34 5.5x -14.7-14.81:34 4.7x -15.4-17.1
1.01:18 6.8x -3.2-2.31:13 5.7x +0.8-1.20:58 9.0x -27.2-27.30:59 7.4x -27.3-30.7
weshowtheaveragespeed-upofoneﬁne-tuningepochandtheper-
formancelosscomparedtothebaselineforBIFIandExceptionType
datasets in Table 2. We also report the corresponding values for
DevignandReVeal,forwhichbothgainsandlossesofperformance
are indicated. The speed up is reported as a scaling factor of the
baseline time. The metric diﬀerence is shown as gain or loss of the
weightedF1-scoreandaccuracycomparedtothebaselineperfor-
mance. Statistically signiﬁcant improvements are reported in bold,
while statistically insigniﬁcant losses are marked with a star (∗).
The/u1D43412eﬀectsizesareindicatedbythreeshadesofblueasthecell
color, with the darkest shade indicating a large eﬀect ( /u1D43412>0.71),
themiddleshadeindicating a medium eﬀect( /u1D43412>0.64),and the
lightest shade indicating a small eﬀect ( /u1D43412>0.56). We also under-
lineanddiscussselectedresultsthatimprovethemetricvaluesand
reduce resourceusage.
Themajorityofcombinations(xii)withprunedmodelsoutper-
formthebaselineforDevignandReVeal.Furthermore,modelswith
2–10 layers show statistically signiﬁcant improvements of both
metricsonDevign,withthe3-layermodelachieving+2accuracy
improvementwitha3.3-timesaveragespeed-upofﬁne-tuningwith
the same hardware and software. Not only does the 3-layer model
improve the accuracy over CodeBERT baseline to 63.7, but also
outperformsseveralothermodelstestedonDevignandreported
on the CodeXGLUE benchmark [ 27]. In particular, our pruned 3-
layer CodeBERT model outperforms the full-transformer model
PLBART [ 1], and code2vec code representations pre-trained on
abstractsyntaxtreesandcodetokensinajointmanner[ 1].How-
ever,ourprunedmodeldoesnotoutperformthebestperforming
modelreportedonCodeXGLUE,CoText,whichachieves66.62ac-
curacy [34].
Modelswith1and11layersachievestatisticallysigniﬁcantaccu-
racyimprovementsforReVeal.However,the1-layermodelreduces
theF1(w)score.Theuseoflayer11doesnotimpactthespeedof
ﬁne-tuning, while the 1-layer model yields the 3.7x acceleration of
the baseline ﬁne-tuning speed. The lack of speed-up with 11-layermodelcanbeexplainedbythe factthat the numberof trainablepa-
rametersdoesnotdecreaselinearlywiththeremovaloflaterlayers,
sincetheadditionalembeddinglayerandclassiﬁcationheadremain
unchanged.The2-layermodelresultsinthebestimprovementof
F1(w) which is statistically signiﬁcant. The 2-layer model improves
accuracy on ReVeal as well. For Devign and ReVeal, statistically
signiﬁcant improvements have large eﬀectsize.
ForBIFI,weobtainstatisticallyinsigniﬁcantdecreaseofF1(w)
andaccuracyaccordingtotheWilcoxontestwhichbringsabout
1.2x speed-up of the ﬁne-tuning with the 11-layer model. If we
decreasethe numberof layersto 8,the performance on BIFI stays
withinthe(baselinemetric −1)limit,butwegainupto1.7xaverage
speed-upofone-epochﬁne-tuning.Incaseofusingmodelswith1–
10layers,weobserveastatisticallysigniﬁcantchangeofdistribution
anddecreaseofmetric values.
For the unbalanced Exception Type dataset, the performance
dropsfasterandthespeed-upislessprominentthanforBIFI.The
changeofmean valuesof the metrics for allmodelsisstatistically
signiﬁcant.Indetail,themetricsdecreaseby-1.0absolutemetric
valueat11layerswith1.1xﬁne-tuningspeed-upandby-1.8with
10 layers with 1.2x speed-up. We explain the sharper decline of
the combinations performance by the lower baseline metric val-
ues (75.39 accuracy, 75.30 weighted F1-score) than in the case of
BIFI(96.7accuracyandweightedF1-score).ForBIFI,statistically
insigniﬁcantdeteriorationhavesmalleﬀectsize.However,forboth
BIFI and Exception Type datasets, we observe deterioration of per-
formanceoflarge eﬀectsize withprunedmodels.
We conclude that for the BIFI dataset with high-performing
baselineand3classes,theperformancelossatremovingeachlayer
islessthanfortheExceptionTypeclassiﬁcationdatasetwithlower
baseline performance and 20 classes. The resource usage, which
is correlated with time spent on tuning, decreases faster for BIFI
than for Exception Type. This is partially explained by a larger
classiﬁcation head for the Exception Type dataset, because this
dataset has 20 classes as opposed to only 3 classes in BIFI. In other
904TheEarlyBIRDCatchesthe Bug: On Exploiting Early Layersof EncoderModels... ESEC/FSE ’23, December3–9, 2023,San Francisco, CA, USA
words,weobservethattheBIFIdatasethasastrongbaselinethat
is hard to outperform with pruning. By contrast, the complexity of
theExceptionTypedatasetcaninﬂuencetheresultsintheopposite
way:Thebaselineperformanceisalreadynotverystrong,andit
proves hardto further improve onitwithearly layers only.
Answer to RQ2. We obtain performance improvements over
the baseline as well as ﬁne-tuning speed-ups for both defect
detection datasets by using the CLStoken from the last layer
of pruned models. For multi-class classiﬁcation, performance
decreases upon pruning each layer from the end of the model.
Thedecrease issharper forthe datasetwith 20exception types
thanfor the taskwith3bugtypes.
6.3 Threatsto Validity
Themainthreattoexternalvalidityisthattheresultsareempirical
and may not generalize to all code classiﬁcation settings, including
other programming languages, tasks, and encoder-based models
for code. We have tested EarlyBIRD combinations on code in C
for defect detection and Python for bug type and exception type
classiﬁcation in this study. The choice of the CodeBERT as the
encoder model and its internal structure aﬀects the results. For
instance, an encoder model that takes smaller input sequences can
performworse onthe same datasets, becauselargerpartsof input
codesequenceshavetobeprunedinthiscase.Theexternalvalidity
can be improvedbytestingonmore datasets andencoder models.
The threats to internal validity concern the dependency of mod-
els on initializations of trainable parameters and the choice of
methods. Classiﬁcation head and weighted sums with trainable
parametersinourexperimentsdependontheinitializationofthe
parameters and can lead the model to arrive at diﬀerent local min-
ima during ﬁne-tuning. To reduce the eﬀect of diﬀerent random
initializations, we have ﬁne-tuned and tested all EarlyBIRD combi-
nations10 times withdiﬀerentrandom seeds.
In addition, we used the Wilcoxon test to verify whether the
achievedimprovementsarestatisticallysigniﬁcant.However,the
Wilcoxon test only estimates whether measurements of baseline
valuesandEarlyBIRDcombinationsaredrawnfromdiﬀerentdistri-
butions.Thereportedtimesspentonﬁne-tuningandcorresponding
speed-upshavethepurposeofillustratingthereductioninresource
usage, and will depend on the hardware used. Even when using
factors of speed-up for pruned models, there is a chance that these
numbers willbe diﬀerentonotherhardware conﬁgurations.
We implemented the algorithms and statistical procedures in
Python, with the help of widely used libraries such as PyTorch,
NumPyandSciPy.However,wecannotguaranteetheabsenceof
implementationerrorswhichmayhave aﬀectedour evaluation.
7 CONCLUDINGREMARKS
Inthispaper,wehaveproposedEarlyBIRD,anapproachtocombine
early layers of encoder models for code, and tested diﬀerent early-
layer combinations on the software engineering tasks of defect
detection, bug type and exception type classiﬁcation. Our study
ismotivatedbythehypothesisthatearlylayerscontainvaluable
information that is discarded by the standard practice of repre-
senting the code with the CLStoken from the last encoder layer.EarlyBIRDprovideswaysto improvethe performanceofexisting
modelswiththesameresourceutilization,aswellasforresource
usage reduction while obtaining comparable results to the baseline.
Results: UsingEarlyBIRD,weobtainstatisticallysigniﬁcantim-
provements over the baseline for the majority of the combinations
that involve a single encoder layer on defect detection, and with
selectedEarlyBIRD combinationson bugtype andexception type
classiﬁcation. Max pooling of tokens from selected single layers
yields performance improvements for all datasets. Both the classiﬁ-
cationperformanceandtheaverageﬁne-tuningtimeforoneepoch
areimprovedbypruningthepre-trainedmodeltoitsearlylayers
and using the CLStoken from the last layer of the pruned model.
For defect detection, this results in a +2.0 increase in accuracy and
a 3.3x ﬁne-tuning speed-up on Devign, and up to +0.8 accuracy
improvement with a 3.7x speed-up on ReVeal. Pruned models do
not leadto multi-class classiﬁcationperformance gains,but they
do show a ﬁne-tuning speed-up and the associated decrease in
resourceconsumption.
Theresultsshow thatprunedmodels withreducedsizeeither
workbetterorcanresultinareductionofresourceusageduring
ﬁne-tuningwithdiﬀerentlevelsofperformancevariation,which
indicates thepotentialofEarlyBIRDinresource-restricted scenar-
ios of deploying defect detection and but type classiﬁcation in
production environments. For example, EarlyBIRD achieves a 2.1x
speed-upfor BIFI whilereducing accuracyfrom 96.7to 95.0.
Future Work: The study can be extended by investigating the
generalization to other encoder models. We are in the process of
studyingtheperformanceofEarlyBIRDwithtwonewencodermod-
els:StarEncoder[ 23]andContraBERT_C[ 25].Anotherdirection
for futureresearch iswhetherthe typesoflayercombinationand
pruning,aswehaveinvestigatedinthispaperforencoderarchitec-
tures,arealsoeﬀectivetechniquesfordecoderandencoder-decoder
architectures. Moreover, it would be of interest to experiment with
othercodeclassiﬁcationtasks,suchasgeneralbugdetectionandthe
predictionofvulnerabilitytypes.Thelattercouldbeinvestigated
usingtheCWEtypesfromtheCommonWeaknessEnumerationas
labeledinthe CVEﬁxesdataset [ 5].
ACKNOWLEDGMENTS
ThisworkissupportedbytheResearchCouncilofNorwaythrough
thesecureITproject(IKTPLUSS#288787).MaxHortissupported
throughtheERCIM‘AlainBensoussan’FellowshipProgramme.The
empirical evaluation presented in this paper was performed on the
ExperimentalInfrastructureforExplorationofExascaleComputing
(eX3), ﬁnancially supported by the Research Council of Norway
undercontract#270053,aswellasonresourcesprovidedbySigma2,
the National Infrastructure for High Performance Computing and
Data Storage inNorway.
DATA AVAILABILITY
Tosupportopenscienceandallowforreproductionandveriﬁcation
of our work, all artifacts are made available through Zenodo at the
following URL: https://doi.org/10.5281/zenodo.7608802 .
905ESEC/FSE ’23, December3–9, 2023,San Francisco, CA, USA AnastasiiaGrishina, MaxHort,andLeonMoonen
REFERENCES
[1]Wasi Ahmad, Saikat Chakraborty, Baishakhi Ray, and Kai-Wei Chang. 2021.
Uniﬁed Pre-training for Program Understanding and Generation. In Conference
oftheNorthAmericanChapteroftheAssociationforComputationalLinguistics:
HumanLanguageTechnologies .AssociationforComputationalLinguistics,Online,
2655–2668. https://doi.org/10.18653/v1/2021.naacl-main.211
[2]Touﬁque Ahmed and Premkumar Devanbu. 2022. Multilingual Training for
Software Engineering. In Proceedings of the 44th International Conference on
Software Engineering . ACM, Pittsburgh Pennsylvania, 1443–1455. https://doi.
org/10.1145/3510003.3510049
[3]MiltiadisAllamanis,EarlT.Barr,PremkumarDevanbu,andCharlesSutton.2018.
ASurveyofMachineLearning forBigCode andNaturalness. Comput.Surveys
51,4 (July2018),81:1–81:37. https://doi.org/10.1145/3212695
[4]Berkay Berabi, Jingxuan He, Veselin Raychev, and Martin Vechev. 2021. TFix:
Learning to Fix Coding Errors with a Text-to-Text Transformer. In International
Conference onMachineLearning , Vol. 139. PMLR, VirtualEvent, 780–791.
[5]GuruBhandari,AmaraNaseer,andLeonMoonen.2021. CVEﬁxes:Automated
Collection of Vulnerabilities and Their Fixes from Open-Source Software. In
International Conference on Predictive Models and Data Analytics in Software
Engineering (PROMISE) . ACM,30–39. https://doi.org/10.1145/3475960.3475985
[6]Terra Blevins, Omer Levy, and Luke Zettlemoyer. 2018. Deep RNNs Encode Soft
Hierarchical Syntax. In Proceedings of the 56th Annual Meeting of the Association
for Computational Linguistics (Volume 2: Short Papers) . Association for Computa-
tional Linguistics, Melbourne, Australia, 14–19. https://doi.org/10.18653/v1/p18-
2003
[7]Saikat Chakraborty, Rahul Krishna, Yangruibo Ding, and Baishakhi Ray. 2022.
Deep Learning Based Vulnerability Detection: Are We There Yet? IEEE Transac-
tionsonSoftwareEngineering 48,9(Sept.2022),3280–3296. https://doi.org/10.
1109/tse.2021.3087402
[8]MarkChen,JerryTworek,HeewooJun,QimingYuan,HenriquePondedeOliveira
Pinto,JaredKaplan,HarriEdwards,YuriBurda,NicholasJoseph,GregBrockman,
Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish
Sastry,PamelaMishkin,BrookeChan,ScottGray,NickRyder,MikhailPavlov,
Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe
Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis,
ElizabethBarnes,ArielHerbert-Voss,WilliamHebgenGuss,AlexNichol,Alex
Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain,
WilliamSaunders,ChristopherHesse,AndrewN.Carr,JanLeike,JoshAchiam,
VedantMisra,EvanMorikawa, AlecRadford,MatthewKnight,MilesBrundage,
Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam
McCandlish, Ilya Sutskever, and Wojciech Zaremba. 2021. Evaluating Large
LanguageModelsTrainedonCode. https://doi.org/10.48550/arXiv.2107.03374
arXiv:2107.03374 [cs]
[9]Zimin Chen, Steve James Kommrusch, Michele Tufano, Louis-Noël Pouchet,
DenysPoshyvanyk,andMartinMonperrus.2019. SEQUENCER:Sequence-to-
SequenceLearningforEnd-to-EndProgramRepair. IEEETransactionsonSoftware
Engineering 47,9 (2019), 1943–1959. https://doi.org/10.1109/tse.2019.2940179
[10]Colin Clement, Dawn Drain, Jonathan Timcheck, Alexey Svyatkovskiy, and
NeelSundaresan.2020. PyMT5:Multi-ModeTranslationofNaturalLanguage
and Python Code with Transformers. In Proceedings of the 2020 Conference on
Empirical Methods in Natural Language Processing (EMNLP) . Association for
Computational Linguistics, Online, 9052–9065. https://doi.org/10.18653/v1/2020.
emnlp-main.728
[11]P.Devanbu.2015. NewInitiative:TheNaturalnessofSoftware.In Proceedings-
International Conference on Software Engineering , Vol. 2. IEEE Computer Society,
543–546. https://doi.org/10.1109/icse.2015.190
[12]JacobDevlin,Ming-WeiChang,KentonLee,andKristinaToutanova.2019. BERT:
Pre-trainingofDeepBidirectionalTransformersforLanguageUnderstanding.
InConference of the North American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies (NAACL, Vol 1) . Association for
Computational Linguistics,4171–4186. https://doi.org/10.18653/v1/n19-1423
[13]Angela Fan, Edouard Grave, and Armand Joulin. 2019. Reducing Transformer
DepthonDemandwithStructuredDropout. https://doi.org/10.48550/arXiv.1909.
11556arXiv:1909.11556 [cs, stat]
[14]Zhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xiaocheng Feng, Ming Gong,
Linjun Shou, Bing Qin, Ting Liu, Daxin Jiang, and Ming Zhou. 2020. CodeBERT:
A Pre-Trained Model for Programming and Natural Languages. In Findings of
theAssociationforComputationalLinguistics:EMNLP2020 .Online,1536–1547.
https://doi.org/10.18653/v1/2020.ﬁndings-emnlp.139
[15]Michael Fu, Chakkrit Tantithamthavorn, Trung Le, Van Nguyen, and Dinh
Phung. 2022. VulRepair: A T5-based Automated Software Vulnerability Re-
pair. InProceedings of the 30th ACM Joint European Software Engineering Con-
ference and Symposium on the Foundations of Software Engineering (ESEC/FSE
2022). Association for Computing Machinery, New York, NY, USA, 935–947.
https://doi.org/10.1145/3540250.3549098
[16]Daya Guo, Shuo Ren, Shuai Lu, Zhangyin Feng, Duyu Tang, Shujie Liu, Long
Zhou, Nan Duan, Alexey Svyatkovskiy,Shengyu Fu,Michele Tufano, Shao KunDeng, Colin Clement, Dawn Drain, Neel Sundaresan, Jian Yin, Daxin Jiang, and
Ming Zhou. 2021. GraphCodeBERT: Pre-training Code Representations with
Data Flow. In International Conference on Learning Representations, ICLR 2021 .
VirtualEvent, Austria,1–18. arXiv: 2009.08366 [cs]
[17]VincentJ.Hellendoorn,ChristianBird,EarlT.Barr,andMiltiadisAllamanis.2018.
DeepLearningTypeInference.In JointEuropeanSoftwareEngineeringConference
and Symposium on the Foundations of Software Engineering (ESEC/FSE) . ACM,
NewYork, NY, USA,152–162. https://doi.org/10.1145/3236024.3236051
[18]JoséAntonioHernándezLópez, MartinWeyssow,JesúsSánchezCuadrado,and
Houari Sahraoui. 2023. AST-Probe: Recovering Abstract Syntax Trees from
HiddenRepresentationsofPre-TrainedLanguageModels.In Proceedingsofthe
37thIEEE/ACMInternationalConferenceonAutomatedSoftwareEngineering (ASE
’22).AssociationforComputingMachinery,NewYork,NY,USA,1–11. https:
//doi.org/10.1145/3551349.3556900
[19]Jeremy Howard and Sebastian Ruder. 2018. Universal Language Model Fine-
tuningforTextClassiﬁcation.In Proceedingsofthe56thAnnualMeetingofthe
AssociationforComputationalLinguistics(Volume1:LongPapers) .Association
for Computational Linguistics,Melbourne, Australia,328–339. https://doi.org/
10.18653/v1/p18-1031
[20]Aditya Kanade, Petros Maniatis, Gogul Balakrishnan, and Kensen Shi. 2020.
Learning andEvaluatingContextualEmbeddingof SourceCode. In Proceedings
of the 37th International Conference on Machine Learning . PMLR, 5110–5121.
arXiv:2001.00059 [cs]
[21]Anjan Karmakarand Romain Robbes. 2021. WhatDo Pre-Trained Code Models
KnowaboutCode?.In 202136thIEEE/ACMInternationalConferenceonAutomated
SoftwareEngineering(ASE) .1332–1336. https://doi.org/10.1109/ase51524.2021.
9678927
[22]Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush
Sharma,andRaduSoricut.2020. ALBERT:ALiteBERTforSelf-supervisedLearn-
ing of Language Representations. https://doi.org/10.48550/arXiv.1909.11942
arXiv:1909.11942 [cs]
[23]RaymondLi,LoubnaBenAllal,YangtianZi,NiklasMuennighoﬀ,DenisKocetkov,
Chenghao Mou,MarcMarone, ChristopherAkiki,JiaLi, JennyChim, Qian Liu,
EvgeniiZheltonozhskii,TerryYueZhuo,ThomasWang,OlivierDehaene,Mishig
Davaadorj,JoelLamy-Poirier,JoãoMonteiro,OlehShliazhko,NicolasGontier,
NicholasMeade,ArmelZebaze,Ming-HoYee,LogeshKumarUmapathi,JianZhu,
Benjamin Lipkin, Muhtasham Oblokulov, Zhiruo Wang, Rudra Murthy, Jason
Stillerman,SivaSankalpPatel,DmitryAbulkhanov,MarcoZocca,MananDey,
ZhihanZhang,NourFahmy,UrvashiBhattacharyya,WenhaoYu,SwayamSingh,
SashaLuccioni,PauloVillegas,MaximKunakov,FedorZhdanov,ManuelRomero,
TonyLee,NadavTimor,JenniferDing,ClaireSchlesinger,HaileySchoelkopf,Jan
Ebert,TriDao,MayankMishra,AlexGu,JenniferRobinson,CarolynJaneAn-
derson, Brendan Dolan-Gavitt, Danish Contractor, Siva Reddy, Daniel Fried,
Dzmitry Bahdanau, Yacine Jernite, Carlos Muñoz Ferrandis, Sean Hughes,
ThomasWolf,ArjunGuha,LeandrovonWerra,andHarmdeVries.2023. Star-
Coder: Maythe SourceBewithYou! https://doi.org/10.48550/arXiv.2305.06161
arXiv:2305.06161 [cs]
[24]NelsonF.Liu, Matt Gardner,YonatanBelinkov,Matthew E. Peters, and NoahA.
Smith.2019. LinguisticKnowledgeand TransferabilityofContextualRepresen-
tations.https://doi.org/10.48550/arXiv.1903.08855 arXiv:1903.08855 [cs]
[25]ShangqingLiu,BozhiWu, XiaofeiXie,GuozhuMeng,andYangLiu.2023. Con-
traBERT: Enhancing Code Pre-Trained Models via Contrastive Learning. In
Proceedings of the 45th International Conference on Software Engineering (ICSE
’23).IEEEPress,Melbourne,Victoria,Australia,2476–2487. https://doi.org/10.
1109/icse48619.2023.00207
[26]YinhanLiu,MyleOtt,NamanGoyal,JingfeiDu,MandarJoshi,DanqiChen,Omer
Levy,MikeLewis,LukeZettlemoyer,andVeselinStoyanov.2019. RoBERTa:A
RobustlyOptimizedBERTPretrainingApproach. https://doi.org/10.48550/arXiv.
1907.11692 arXiv:1907.11692 [cs]
[27]ShuaiLu,DayaGuo,ShuoRen,JunjieHuang,AlexeySvyatkovskiy,Ambrosio
Blanco,ColinClement,DawnDrain,DaxinJiang,DuyuTang,GeLi,LidongZhou,
LinjunShou,LongZhou,MicheleTufano,MingGong,MingZhou,NanDuan,
NeelSundaresan,ShaoKunDeng,ShengyuFu,andShujieLiu.2021. CodeXGLUE:
AMachineLearningBenchmarkDatasetforCodeUnderstandingandGeneration.
InProceedingsoftheNeuralInformationProcessingSystemsTrackonDatasetsand
Benchmarks . 1–16. arXiv: 2102.04664 [cs]
[28]Changan Niu, Chuanyi Li, Bin Luo, and Vincent Ng. 2022. Deep Learning Meets
Software Engineering:A SurveyonPre-Trained Models of SourceCode. https:
//doi.org/10.48550/arXiv.2205.11739 arXiv:2205.11739 [cs]
[29]MatteoPaltenghiandMichaelPradel.2021. ThinkingLikeaDeveloper?Com-
paring the Attention of Humans with Neural Models of Code. In 2021 36th
IEEE/ACM International Conference on Automated Software Engineering (ASE) .
867–879. https://doi.org/10.1109/ase51524.2021.9678712
[30]CongPan,MinyanLu,andBiaoXu.2021. AnEmpiricalStudyonSoftwareDefect
Prediction Using CodeBERT Model. Applied Sciences 11, 11 (May 2021), 4793.
https://doi.org/10.3390/app11114793
[31]DavidPeer,SebastianStabinger,StefanEngl,andAntonioRodriguez-Sanchez.
2022. Greedy-Layer Pruning: Speeding up Transformer Models for Natural
906TheEarlyBIRDCatchesthe Bug: On Exploiting Early Layersof EncoderModels... ESEC/FSE ’23, December3–9, 2023,San Francisco, CA, USA
Language Processing. Pattern RecognitionLetters 157 (May 2022), 76–82. https:
//doi.org/10.1016/j.patrec.2022.03.023 arXiv:2105.14839 [cs]
[32]Matthew Peters, Mark Neumann, Luke Zettlemoyer, and Wen-tau Yih. 2018.
DissectingContextualWordEmbeddings:ArchitectureandRepresentation.In
Proceedings of the 2018 Conference on Empirical Methods in Natural Language
Processing . Association for Computational Linguistics, Brussels, Belgium, 1499–
1509.https://doi.org/10.18653/v1/d18-1179
[33]Matthew E. Peters, Sebastian Ruder, and Noah A. Smith. 2019. To Tune or Not to
Tune?AdaptingPretrainedRepresentationstoDiverseTasks.In Proceedingsofthe
4thWorkshoponRepresentationLearningforNLP(RepL4NLP-2019) .Associationfor
ComputationalLinguistics,Florence,Italy,7–14. https://doi.org/10.18653/v1/w19-
4302
[34]LongPhan, HieuTran,DanielLe,Hieu Nguyen,JamesAnnibal,AlecPeltekian,
andYanfangYe.2021. CoTexT:Multi-taskLearningwithCode-TextTransformer.
InWorkshoponNaturalLanguageProcessingforProgramming(NLP4Prog2021) .
Association for Computational Linguistics, Online, 40–47. https://doi.org/10.
18653/v1/2021.nlp4prog-1.5
[35]Rebecca Russell, Louis Kim, Lei Hamilton, Tomo Lazovich, Jacob Harer, Onur
Ozdemir, Paul Ellingwood, and Marc McConley. 2018. Automated Vulnerability
Detection in SourceCode Using Deep Representation Learning.In International
Conference on Machine Learning and Applications (ICMLA) . IEEE, Orlando, FL,
757–762. https://doi.org/10.1109/icmla.2018.00120
[36]Hassan Sajjad, Fahim Dalvi, Nadir Durrani, and Preslav Nakov. 2023. On the
Eﬀect of Dropping Layersof Pre-trainedTransformer Models. Computer Speech
& Language 77 (Jan. 2023), 101429. https://doi.org/10.1016/j.csl.2022.101429
arXiv:2004.03844 [cs]
[37]RishabSharma,FuxiangChen,FatemehFard,andDavidLo.2022. AnExploratory
Study on Code Attention in BERT. In Proceedings of the 30th IEEE/ACM Interna-
tional Conference on Program Comprehension (ICPC ’22) . Association for Comput-
ingMachinery,NewYork,NY,USA,437–448. https://doi.org/10.1145/3524610.
3527921
[38]Tushar Sharma, Maria Kechagia, Stefanos Georgiou, Rohit Tiwari, and Federica
Sarro.2021. ASurveyonMachineLearningTechniquesforSourceCodeAnalysis.
https://doi.org/10.48550/arXiv.2110.09610 arXiv:2110.09610 [cs]
[39]Chi Sun, Xipeng Qiu, Yige Xu, and Xuanjing Huang. 2019. How to Fine-Tune
BERT for Text Classiﬁcation?. In Chinese Computational Linguistics (Lecture
Notes in Computer Science) , Maosong Sun, Xuanjing Huang, Heng Ji, Zhiyuan
Liu, and Yang Liu (Eds.). Springer International Publishing, Cham, 194–206.
https://doi.org/10.1007/978-3-030-32381-3_16
[40]Chi Sun, Xipeng Qiu, Yige Xu, and Xuanjing Huang. 2020. How to Fine-
TuneBERTforTextClassiﬁcation? https://doi.org/10.48550/arXiv.1905.05583
arXiv:1905.05583 [cs]
[41]AndrásVarghaandHaroldD.Delaney.2000. ACritiqueandImprovementofthe
CLCommonLanguageEﬀect SizeStatistics ofMcGrawand Wong. Journalof
Educational and Behavioral Statistics 25, 2 (June 2000), 101–132. https://doi.org/
10.3102/10769986025002101[42]Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention Is All
You Need. In International Conference on Neural Information Processing Sys-
tems (NeurIPS) , I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus,
S. Vishwanathan, and R. Garnett (Eds.). Curran Associates, Inc., 5998–6008.
arXiv:1706.03762 [cs]
[43]Yue Wang, Weishi Wang, Shaﬁq Joty, and Steven C.H. Hoi. 2021. CodeT5:
Identiﬁer-awareUniﬁedPre-trainedEncoder-DecoderModelsforCodeUnder-
standing and Generation. In Conference on Empirical Methods in Natural Lan-
guageProcessing .AssociationforComputationalLinguistics,OnlineandPunta
Cana, Dominican Republic,8696–8708. https://doi.org/10.18653/v1/2021.emnlp-
main.685
[44]Hongwei Wei, Guanjun Lin, Lin Li, and Heming Jia. 2021. A Context-Aware
NeuralEmbeddingforFunction-LevelVulnerabilityDetection. Algorithms 14,11
(Nov. 2021),335. https://doi.org/10.3390/a14110335
[45]FrankWilcoxon.1992. IndividualComparisonsbyRankingMethods. In Break-
throughs in Statistics: Methodology and Distribution , Samuel Kotz and Norman L.
Johnson (Eds.). Springer, New York, NY, 196–202. https://doi.org/10.1007/978-1-
4612-4380-9_16
[46]Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdi-
nov, and Quoc V. Le. 2020. XLNet: Generalized Autoregressive Pretrain-
ing for Language Understanding. https://doi.org/10.48550/arXiv.1906.08237
arXiv:1906.08237 [cs]
[47]Michihiro Yasunaga and Percy Liang. 2021. Break-It-Fix-It: Unsupervised Learn-
ing for Program Repair. In International Conference on Machine Learning . PMLR,
12. arXiv: 2106.06600 [cs]
[48]HeYe,MatiasMartinez,andMartinMonperrus.2022.NeuralProgramRepairwith
Execution-BasedBackpropagation.In Proceedingsofthe44thInternationalCon-
ferenceonSoftwareEngineering (ICSE’22) .AssociationforComputingMachinery,
NewYork, NY, USA,1506–1518. https://doi.org/10.1145/3510003.3510222
[49]Tianyi Zhang, Felix Wu, Arzoo Katiyar, Kilian Q. Weinberger, and Yoav
Artzi. 2021. Revisiting Few-sample BERT Fine-tuning. In NeurIPS 2021 . 1–22.
arXiv:2006.05987 [cs]
[50]Gang Zhao and Jeﬀ Huang. 2018. DeepSim: Deep Learning Code Functional
Similarity.In Proceedingsofthe201826thACMJointMeetingonEuropeanSoftware
EngineeringConferenceandSymposiumontheFoundationsofSoftwareEngineering .
ACM, Lake Buena Vista FL USA, 141–151. https://doi.org/10.1145/3236024.
3236068
[51]Yaqin Zhou, Shangqing Liu, Jingkai Siow, Xiaoning Du, and Yang Liu. 2019. De-
vign: Eﬀective Vulnerability Identiﬁcation by Learning Comprehensive Program
Semantics via Graph Neural Networks. In International Conference on Neural
Information Processing Systems (NeurIPS) . Curran Associates, Inc., Vancouver,
Canada., 11. arXiv: 1909.03496 [cs]
Received 2023-02-02; accepted 2023-07-27
907