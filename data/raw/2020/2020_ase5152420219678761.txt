SAT UNE: A Study-Driven Auto-Tuning Approach
for ConÔ¨Ågurable Software VeriÔ¨Åcation Tools
Ugur KocAustin MordahlyShiyi WeiyJeffrey S. FosterzAdam A. Porter
Department of Computer Science, University of Maryland, College Park, MD, USA
fukoc, aporterg@cs.umd.edu
yDepartment of Computer Science, The University of Texas at Dallas, Richardson, TX, USA
faustin.mordahl, sweig@utdallas.edu
zDepartment of Computer Science, Tufts University, Medford, MA, USA
jeffrey.foster@tufts.edu
Abstract ‚ÄîMany program veriÔ¨Åcation tools can be customized
via run-time conÔ¨Åguration options that trade off performance,
precision, and soundness. However, in practice, users often run
tools under their default conÔ¨Ågurations, because understanding
these tradeoffs requires signiÔ¨Åcant expertise. In this paper,
we ask how well a single, default conÔ¨Åguration can work in
general, and we propose SAT UNE, a novel tool for automatically
conÔ¨Åguring program veriÔ¨Åcation tools for given target programs.
To answer our question, we gathered a dataset that runs four
well-known program veriÔ¨Åcation tools against a range of C
and Java benchmarks, with results labeled as correct, incorrect,
or inconclusive (e.g., timeout). Examining the dataset, we Ô¨Ånd
there is generally no one-size-Ô¨Åts-all best conÔ¨Åguration. Moreover,
a statistical analysis shows that many individual conÔ¨Åguration
options do not have simple tradeoffs: they can be better or worse
depending on the program.
Motivated by these results, we developed SAT UNE, which
constructs conÔ¨Ågurations using a meta-heuristic search. The
search is guided by a surrogate Ô¨Åtness function trained on
our dataset. We compare the performance of SAT UNE to three
baselines: a single conÔ¨Åguration with the most correct results
in our dataset; the most precise conÔ¨Åguration followed by the
most correct conÔ¨Åguration (if needed); and the most precise
conÔ¨Åguration followed by random search (also if needed). We Ô¨Ånd
that SAT UNE outperforms these approaches by completing more
correct tasks with high precision. In summary, our work shows
that good conÔ¨Ågurations for veriÔ¨Åcation tools are not simple to
Ô¨Ånd, and SAT UNE takes an important step towards automating
the process of Ô¨Ånding them.
Index Terms‚ÄîEmpirical software engineering; software anal-
ysis; testing, veriÔ¨Åcation, and validation.
I. I NTRODUCTION
Static program veriÔ¨Åcation tools are a promising approach
for reasoning about the correctness of software. Because the
algorithms that back such tools present various precision,
soundness, and performance1tradeoffs [1]‚Äì[3], program ver-
iÔ¨Åcation tools often include a host of options for tuning the
analysis. However, deciding how to set these options can be
quite challenging, as it may require deep knowledge of the
analysis algorithms [2], [4]. Thus in practice, many users rely
on a default conÔ¨Åguration recommended by developers for
1In this work, when we speak of ‚Äúperformance‚Äù, we are referring to the
quality of producing correct results; in other words, maximizing the number
of true positive and true negative results produced.typical scenarios. Unfortunately, prior work suggests that the
tradeoffs among options depend on the features of the program
being analyzed [5]‚Äì[9].
In response to these challenges, several researchers have
explored ways to choose a program veriÔ¨Åer, or conÔ¨Åguration
of a veriÔ¨Åer to best Ô¨Åt a target program [6], [10]‚Äì[15] and
have selectively applied settings of the analysis conÔ¨Åguration
options [5], [8], [9], [16]. Other work aims to develop an
understanding of certain kinds of conÔ¨Åguration options in
static analysis frameworks or tools [2], [3], [17]. To our
knowledge, the prior work has focused on relatively small and
speciÔ¨Åc conÔ¨Åguration or tool spaces. As a result, the effects
of conÔ¨Ågurations on program veriÔ¨Åcation tools is still poorly
understood, and it remains difÔ¨Åcult to tune such tools.
In this paper, we aim to address this gap in two steps. First,
to better understand the conÔ¨Ågurability of program veriÔ¨Åcation
tools, we perform an empirical study in which we construct
and analyze a dataset of runs of four popular tools on a
range of benchmarks. Second, driven by the results of the
empirical study, we propose SAT UNE, a novel technique for
automatically tuning the large conÔ¨Åguration spaces of software
veriÔ¨Åcation tools.
Our empirical study examines four tools that participate in
the annual software veriÔ¨Åcation competition (SV-COMP) [18]:
CBMC [19] and Symbiotic [20], [21] verify C/C++ programs,
andJBMC [22] and JayHorn [23] verify Java programs. We
created a ground-truth dataset by running sampled conÔ¨Ågura-
tions of the four tools on a subset of SV-COMP benchmarks.
Each of the 517748 tool‚ÄìconÔ¨Åguration‚Äìbenchmark triples in
our dataset is labeled as either producing a correct, incorrect,
or inconclusive (e.g., timeout) result. We then analyzed the
data to answer two research questions. First, we ask whether,
for each tool, there exists a one-size-Ô¨Åts-all conÔ¨Åguration that
produces a superset of complete, correct results (RQ1). We
found that even the most-correct-conÔ¨Åg‚Äîthe conÔ¨Åguration that
produces the most true positive and true negative results for a
tool‚Äîis unable to complete many veriÔ¨Åcation tasks that other
conÔ¨Ågurations could. Second, we use statistical analysis to
investigate the impact of individual conÔ¨Åguration options on
the tools‚Äô performance and precision (RQ2). We found that for
each tool, at most half of the option settings have a statistically
3302021 36th IEEE/ACM International Conference on Automated Software Engineering (ASE)
DOI 10.1109/ASE51524.2021.000382021 36th IEEE/ACM International Conference on Automated Software Engineering (ASE) | 978-1-6654-0337-5/21/$31.00 ¬©2021 IEEE | DOI: 10.1109/ASE51524.2021.9678761
978-1-6654-0337-5/21/$31.00  ¬©2021  IEEE
signiÔ¨Åcant effect on the number of correct or incorrect results
produced by a tool. In other words, many option settings do not
change the results much. We also found that the option settings
that do have signiÔ¨Åcant effects increase the number of correct
results in some programs and decrease it in others, suggesting
their effects can vary greatly from program to program. (See
Section II for details of our empirical study.)
Overall, our study suggests that no single conÔ¨Åguration is
sufÔ¨Åcient for general use, and conÔ¨Ågurations may need to be
tuned to the target program. Thus, we propose SAT UNE, a
novel technique that aims to Ô¨Ånd a good tool conÔ¨Åguration for
a given target program. Because the tools‚Äô conÔ¨Åguration spaces
are very large and complex, SAT UNE Ô¨Ånds conÔ¨Ågurations
using a meta-heuristic search driven by a Ô¨Åtness function,
where the Ô¨Åtness of a conÔ¨Åguration is its predicted likelihood
to terminate with a correct result on the target program. We
implement the Ô¨Åtness function as a machine learning model
trained on the dataset from the empirical study. One key
feature of SAT UNE is that it is both tool- and language-
agnostic, as the approach only requires a labeled training
dataset, the ability to run a tool from the command line,
and the ability to process its output. (Section III describes
SAT UNE.)
We evaluate SAT UNE by comparing it against three base-
lines that simulate ways a user might tune a veriÔ¨Åcation
tool: Ô¨Årst, using the conÔ¨Åguration most-correct-conÔ¨Åg that
produces the most correct results from our dataset; second,
using the conÔ¨Åguration best-precision-conÔ¨Åg that is the most
precise (i.e., maximizes#corrects
#corrects+#incorrects) and, if it does not
complete, trying most-correct-conÔ¨Åg; and third, using best-
precision-conÔ¨Åg and, if it does not complete, using random
search. We found that, compared to these baselines, SAT UNE
provides the best balance between precision and number
of complete results (RQ3). For example, in the best case,
SAT UNE was able to analyze 169 (27%) more programs with
higher precision than any baseline. We also evaluated SAT UNE
in terms of run time, and found that it was 2‚Äì4 faster than
random search and was comparable to the second baseline
(RQ4). (Section IV presents our evaluation of SAT UNE.)
In summary, our results suggest there is no one-size-Ô¨Åts-
all best conÔ¨Åguration for the studied program veriÔ¨Åcation
tools, and that effects of individual conÔ¨Åguration options can
vary greatly from program to program. Thus, we believe that
SAT UNE takes an important Ô¨Årst step toward automating the
process of Ô¨Ånding good program veriÔ¨Åcation tool conÔ¨Ågura-
tions.
We have made our dataset, results, and the implementation
of SAT UNE publicly available at https://zenodo.org/record/
5218510.
II. E MPIRICAL STUDY OF CONFIGURABLE VERIFICATION
TOOLS
To better understand the conÔ¨Ågurability of program veri-
Ô¨Åcation tools, we studied four popular tools to examine the
effects of both conÔ¨Ågurations as a whole and of individual
conÔ¨Åguration options.A. Study Setup
Table I lists the veriÔ¨Åcation tools we used in our study.
We chose these tools because (1) they are among the best-
performing tools in SV-COMP, (2) they come with conÔ¨Ågura-
tion options that impact the tools‚Äô performance and precision,
(3) they provide sufÔ¨Åcient conÔ¨Åguration option documentation
so we can understand what individual option settings do, and
(4) they target programs written in two widely-used program-
ming languages: CBMC andSymbiotic verify C programs, and
JBMC andJayHorn verify Java programs.
In our study, we focus on the conÔ¨Åguration options that af-
fect analysis performance, soundness, and/or precision, instead
of those that format output or toggle speciÔ¨Åc checkers.
Column 3 in Table I shows the number of options we
use in each veriÔ¨Åcation tool. For each option, we identiÔ¨Åed
its domain (i.e., the settings it can take on). In terms of
domain, there were three different types of options in these
veriÔ¨Åcation tools. First, some options may be passed to the
tool alone as an argument; i.e., as boolean Ô¨Çags. Inherently, a
boolean Ô¨Çag option has two possible settings: fset, unsetg. For
instance, CBMC has 11 boolean Ô¨Çag options, such as --partial-
loops, which allows CBMC to model paths that only partially
execute loops, rather than fully unwinding them [24]. Second,
options may have categorical settings. CBMC has 6 such
options, such as --mm, which speciÔ¨Åes the memory model for
concurrent applications [19]. This option can take on any of the
following settings: fsc, tso, psog. Finally, some options take
on numerical values. Due to the large number of settings, for
numerical options we use a set of representative values from
their domains. For instance, the --unwind option of CBMC
(which speciÔ¨Åes the depth to which loops should be unwound)
accepts a positive integer; we considered the following values
for this option:f1, 5, 10, 20, 100g. We included all settings of
boolean and categorical options. Each speciÔ¨Åc combination of
option settings is a conÔ¨Åguration. Column 4 shows the number
of possible conÔ¨Ågurations that can be created with the options
and settings that we use.
1) ConÔ¨Åguration Sampling: The large number of options
for our subject tools makes it infeasible to study the tools‚Äô
behaviors under all conÔ¨Ågurations (column 4 in Table I).
Research on combinatorial interaction testing has shown that
sampling conÔ¨Åguration spaces using covering arrays is an
effective way to explore the behavior of conÔ¨Ågurable soft-
ware [25], [26]. Furthermore, past research also indicates that
changes in the behavior of software tend to be caused by
interactions of only a few options [27]. We therefore create
a 3-way covering array, which is a list of conÔ¨Ågurations
that include all 3-way combinations of conÔ¨Åguration option
settings [25], for each tool, using an existing covering array
generator [28]. Column 5 in Table I shows the number of
sample conÔ¨Ågurations, i.e., the sizes of the covering arrays.
2) Target Programs: All of our target programs are taken
from the SV-COMP competition [18]. To our knowledge, the
SV-COMP program set is the largest collection of veriÔ¨Åcation
benchmarks for which the ground truths (i.e., whether the
benchmark is safe or unsafe according to some property)
331TABLE I: Subject veriÔ¨Åcation tools.
ToolTarget # of ConÔ¨Åg Sample Dataset
lang. options space size size size
CBMC 5.11 C 21 2.9109295 295,000
Symbiotic 6.1.0 C 16 9.810582 54,940
JayHorn 0.6-a Java 12 7.5106256 94,208
JBMC 5.10 Java 27 7.21010200 73,600
are known. Furthermore, this program set aggregates multiple
benchmarks from across the literature, increasing the gener-
ality of the conclusions we make. It consists of over 10,000
benchmark programs in C and Java. In our study, we selected
a subset on which we ran the veriÔ¨Åcation tools with each
sampled conÔ¨Åguration.
For the two Java tools, we used all 368 benchmark programs
from SV-COMP 2019. These Java programs are written with
assertions, and the veriÔ¨Åcation tools check if these assertions
always hold. Among the 368 programs, 204 (55.4%) are
known to be unsafe. For C tools, the SV-COMP 2018 bench-
mark has 9,523 programs in total.2We randomly selected
a subset of 1,000 programs that are subject to only one
veriÔ¨Åcation check. Out of the 1,000 programs we selected,
there are 335 programs that are subject to concurrency safety
veriÔ¨Åcation, 41 to memory safety veriÔ¨Åcation, 65 to integer
overÔ¨Çow veriÔ¨Åcation, 485 to reachability veriÔ¨Åcation, and 74
to veriÔ¨Åcation of termination. Among the 1,000 programs, 517
are known to be unsafe.
3) Dataset Collection: We executed each sampled conÔ¨Åg-
uration of the subject tools once on each benchmark task
to create the dataset for our study.3In each execution, we
used a 1-minute timeout. For the purpose of studying the
conÔ¨Åguration spaces, this timeout is sufÔ¨Åcient because, based
on SV-COMP 2018 and 2019 results, 95%, 94%, 100%, and
85% of the conclusive runs took less than 1 minute for CBMC ,
Symbiotic, JayHorn, and JBMC , respectively [29], [30]. We
say a veriÔ¨Åcation run is conclusive if it outputs a judgement
that the target program is safe (veriÔ¨Åed) or unsafe (rejected by
the veriÔ¨Åer). In total, we performed 517,748 veriÔ¨Åcation runs.
The sizes of the datasets for each tool range from 54,940 to
295,000 runs (last columns of Table I).
All experiments were conducted on an Ubuntu 16.04LTS
machine with 24 Intel Xeon Silver 4116 CPUs @ 2.10GHz
and 144GB RAM.
4) Research Questions: Our study answers two research
questions:
RQ1: Do the subject tools have any one-size-Ô¨Åts-all
conÔ¨Ågurations?
RQ2: What is the impact of individual conÔ¨Åguration
option settings on the tools‚Äô results?
RQ1 deals with the behavior of conÔ¨Ågurations as a whole.
We used the dataset to determine whether any subject tool has
2SV-COMP 2019 data was not available when we started this research. The
benchmark set for Cis mostly the same between 2018 and 2019.
3Symbiotic does not check concurrency safety. Thus, we did not run Sym-
biotic for the 335 programs that are subject to concurrency safety veriÔ¨Åcation.TABLE II: Results of the sample conÔ¨Ågurations.
Tool# of veriÔ¨Åcation tasks (i.e., programs)
AllNever
solvedCorrect / Incorrect / Inconclusive
worst- most- best-
precision correct precision
CBMC 1000 42524 / 456 / 20 635 / 262 / 103 426 / 1 / 573
Symbiotic 665 338 150 / 92 / 423 261 / 1 / 403 261 / 1 / 403
JayHorn 368 62121 / 98 / 149 227 / 37 / 104 184 / 2 / 182
JBMC 368 2159 / 204 / 5 331 / 0 / 37 331 / 0 / 37
a one-size-Ô¨Åts-all conÔ¨Åguration, i.e., a conÔ¨Åguration that can
complete all veriÔ¨Åcation tasks that were completed by at least
one sampled conÔ¨Åguration.
RQ2 focuses on the effects of individual conÔ¨Åguration op-
tion settings. To address this research question, we aggregated
the number of correct and incorrect veriÔ¨Åcation results for
each tool. We then performed a main effects screening analysis
using ANOV A [31]. In this analysis, we treat the conÔ¨Åguration
options as cardinal or ordinal factors (i.e., independent vari-
ables) and the number of correct and incorrect results as the
responses (i.e., dependent variables). We create two models
for each tool, one for each response, using the least square
method. Each model shows the effect that each factor has on
the response along with standard error and the p-value. In our
analysis, we consider the factors with statistically signiÔ¨Åcant
effects as those with p-value< 0:05.
B. Study Results
1) RQ1: Do the subject tools have any one-size-Ô¨Åts-all
conÔ¨Ågurations?: Table II presents results for the subject tools.
Columns 2 and 3 show the total number of tasks to verify and
the number of tasks the subject tool could not complete within
the 1-minute timeout under anyconÔ¨Åguration. For each tool,
we identiÔ¨Åed the worst-precision-conÔ¨Åg, the most-correct-
conÔ¨Åg, and the best-precision-conÔ¨Åg. The worst-precision-
conÔ¨Åg is the conÔ¨Åguration which had the lowest precision (i.e.,
#corrects
#corrects+#incorrects). The most-correct-conÔ¨Åg is the one which
classiÔ¨Åed the most programs correctly, and the best-precision-
conÔ¨Åg is the conÔ¨Åguration that had the highest precision.
Columns 4, 5, and 6 in Table II show the number of correct /
incorrect / inconclusive results for the worst-precision-conÔ¨Åg,
most-correct-conÔ¨Åg, and best-precision-conÔ¨Åg, respectively.
We Ô¨Ånd that no tool has a single conÔ¨Åguration that could
correctly verify all tasks other conÔ¨Ågurations did. Even the
most-correct-conÔ¨Åg could not correctly verify 10% to 32% of
the tasks that other conÔ¨Ågurations did.
For CBMC, there is a large variance in the behavior of
different conÔ¨Ågurations. The most-correct-conÔ¨Åg only com-
pleted 635 tasks (63.5%) correctly. However, in aggregate,
96% of the veriÔ¨Åcation tasks could be completed correctly
by some conÔ¨Åguration of CBMC. We observe similar results
forJayHorn. Its most-correct-conÔ¨Åg veriÔ¨Åed 227 (62%) tasks
correctly, yet 83% of the tasks were correctly veriÔ¨Åed by
some conÔ¨Åguration. Furthermore, both conÔ¨Ågurations produce
many more incorrect results than the best-precision-conÔ¨Åg
For example, CBMC ‚Äôs most-correct-conÔ¨Åg had 262 incorrect
3321int main() {
2float x = 1.0f;
3float x1 = x/2.5f;
4
5while(x16=x) {
6 x = x1;
7 x1 = x/2.5f;
8}
9
10 assert(x == 0);
11
12 return 0;
13}
(a)P1(safe)1#define N 1000000
2int main() {
3int i, a[N];
4for (i=0; i<N; i++)
5 a[i] = 1;
6for (i=0; i<N; i++)
7 a[i] = 2;
8for (i=0; i<N; i++)
9 a[i] = 3;
10 for (i=0; i<N; i++)
11 assert(a[i] == 2);
12 return 0;
13}
(b)P2(unsafe)
Fig. 1: SimpliÔ¨Åed code examples from the SV-COMP 2018.
results compared to only 1 incorrect result using its best-
precision-conÔ¨Åg. Depending on the user‚Äôs requirements, this
may be an unacceptable tradeoff.
The most-correct-conÔ¨Ågs for Symbiotic andJBMC are more
promising. Symbiotic‚Äôs most-correct-conÔ¨Åg (which was also
its best-precision-conÔ¨Åg) correctly veriÔ¨Åed 39% of tasks, and
49% of tasks that could be correctly completed by any
conÔ¨Åguration. JBMC also had the same best-precision-conÔ¨Åg
and most-correct-conÔ¨Åg, which happened to be its default used
in SV-COMP. This conÔ¨Åguration completed 331 (90%) tasks
correctly, with 0 incorrect results. JBMC could complete all
but 2 (i.e., 99.5%) tasks correctly with some conÔ¨Åguration.
The above Ô¨Åndings suggest that even for the tools with
better single conÔ¨Ågurations, there is still signiÔ¨Åcant room for
improvement if the right conÔ¨Åguration can be identiÔ¨Åed for a
given veriÔ¨Åcation task.
2) RQ2: What is the impact of individual conÔ¨Åguration
option settings on the tools‚Äô results?: The results of the main
effects screening analysis are summarized in Table III. Column
2 lists the option settings with statistically signiÔ¨Åcant effects on
one or both responses. The number next to each tool‚Äôs name in
Column 1 is the number of options with statistically signiÔ¨Åcant
settings. Overall, for all veriÔ¨Åcation tools, at most half of con-
Ô¨Åguration options had at least one setting with a statistically
signiÔ¨Åcant effect on the veriÔ¨Åcation results. Furthermore, a
majority of such option settings presented tradeoffs, in that,
they either increased or decreased both responses together
(highlighted as underlined blue in Table III).
SpeciÔ¨Åcally, there were 12 (18%), 5 (20%), 10 (21%), and 9
(14%) option settings with statistically signiÔ¨Åcant effects for
CBMC, Symbiotic, JayHorn, and JBMC , respectively. More
notably, for CBMC ,JayHorn, and JBMC , 67%, 40%, and 56%
of such signiÔ¨Åcant option settings presented tradeoffs. None of
Symbiotic‚Äôs settings presented tradeoffs in our models.
As an example of the tradeoffs a settings can present,
CBMC ‚Äôs --partial-loops has an estimated effect of decreasing
the number of correct and incorrect results by 83 and 77,
respectively. This option allows partial execution of loops,
which can make Ô¨Ånding counterexamples at small unwinding
bounds easier [24]. The drawback is that it may model spurious
paths that do not exist in the original program, which couldTABLE III: ConÔ¨Åguration option settings with statistically
signiÔ¨Åcant effects on the veriÔ¨Åcation results, ranked by the
size of the estimated effect on the Correct response. Options
inunderlined blue present tradeoffs (i.e., increase or decrease
both corrects and incorrects together). Options without a set-
ting (e.g., --partial-loops of CBMC ) are boolean Ô¨Çag options.
Tool ConÔ¨Åguration option settingsSigniÔ¨Åcant effects
on response
Correct Incorrect
CBMC (9)--partial-loops -82.64 -76.63
--nondet-static 36.91 -19.16
--paths def 49.22 19.24
--full-slice 32.70 34.30
--reÔ¨Åne-strings 31.77 16.69
--no-assumptions 18.88 20.82
--solver z3 43.90 N/A
--paths:Ô¨Åfo -24.13 -14.32
--solver boolvector -36.57 N/A
--depth 100 45.06 90.23
--depth 1000 7.29 22.82
--solver yices -31.58 N/A
Symbiotic (5)--overÔ¨Çow-with-clang -29.69 14.42
--explicit-symbolic 9.44 N/A
--no-slice 13.93 -26.01
--undeÔ¨Åned-retval-nosym 6.10 N/A
--repeat-slicing 2 -7.12 8.22
JayHorn (6)-initial-heap-size 100 -47.65 -8.45
-heap-mode bounded -25.53 2.77
-heap-mode auto 23.74 N/A
-heap-limit 1 -44.90 24.05
-solver eldarica -39.09 -22.67
-heap-limit 10 -10.14 2.75
-initial-heap-size 10 24.37 N/A
-inline-size 100 -19.40 -5.36
-bounded-heap-size 10 16.75 N/A
-step-heap-size 10 N/A -6.43
JBMC (8)--path def 80.56 30.76
--localize-faults -40.00 -37.54
--paths Ô¨Åfo -42.13 -13.65
‚Äìjava-threading -15.07 N/A
--full-slice -16.09 9.19
--slice-formula 12.17 7.45
--depth 100 N/A 74.68
--depth 1000 N/A 34.63
--symex-driven-lazy-load 28.93 N/A
cause incorrect results.
We use two code examples in Figure 1 to illustrate the
tradeoff --partial-loops presents. Both examples were extracted
from the SV-COMP 2018 program set. In Figure 1a, P1
(extracted from the program Float divtrue-unreach-call:c)
is a safe program in that the assertion at line 10 always holds.
This is because the loop at lines 5-8 keeps dividing xby
2:5until it reaches a very small number that is below the
sensitivity of the Ô¨Çoat type in C. The loop eventually ends as
the pre- and post- division values become 0. In our dataset, 116
conÔ¨Ågurations incorrectly judged P1as unsafe, and they all
set --partial-loops. This is because the analysis insufÔ¨Åciently
333unwinds the while loop, and the --partial-loops option allows
the analysis to accept the partial loop execution as a valid path.
To successfully verify P1, a conÔ¨Åguration needs to include a
sufÔ¨Åcient level of loop unwinding and disable --partial-loops.
On the other hand, P2(extracted from the program
standard init5 false unreach call ground:c ) is an un-
safe program in that the assertion at line 11 in Figure 1b
never holds. In our dataset, P2was correctly judged as unsafe
only by the 63 conÔ¨Ågurations that set --partial-loops. For P2,
analyzing all loop iterations is not necessary to determine that
the assertion will fail as long as the Ô¨Årst iteration of the loop
at lines 8-9 is analyzed. Therefore, partially accepting loops
is a safe assumption for P2. The conÔ¨Ågurations that did not
use this option (including the best-precision-conÔ¨Åg) spent too
much time in loop unwinding and eventually timed out. The
above examples illustrate that the effectiveness of a tool‚Äôs
conÔ¨Åguration options may depend on the target program.
However, not all options present tradeoffs. In Table III,
some option settings (19 out of 36) have uniform effects.
These option settings can have uniformly positive effects
(i.e., they increase the number of corrects and/or decrease
the number of incorrects), or uniformly negative effects (i.e.,
they decrease the number of corrects and/or decrease the
number of incorrects). For example, setting --explicit-symbolic
inSymbiotic is estimated to produce 9 more correct results
without a signiÔ¨Åcant effect on the number incorrect results.
This option makes Symbiotic initialize parts of memory with
non-deterministic values. Without this option, evaluation is
done with symbolic values, a costly step that requires tracking
many more execution paths, causing Symbiotic to timeout.
Interestingly, all of Symbiotic‚Äôs options had uniform effects.
One can use such uniform options in the conÔ¨Åguration if the
goal is to increase the likelihood of completing a veriÔ¨Åcation
task. Indeed, we conÔ¨Årmed that the most-correct-conÔ¨Åg set 4
out of 5 of these options consistently with the models‚Äô esti-
mated effects (e.g., disabling --overÔ¨Çow-with-clang). However,
recall our answer to RQ1; doing so still cannot produce a one-
size-Ô¨Åts-all conÔ¨Åguration that completes all the veriÔ¨Åcation
tasks other conÔ¨Ågurations did.
In summary, not all conÔ¨Åguration option settings have
signiÔ¨Åcant effects on the veriÔ¨Åcation results. Those that do
often present tradeoffs that depend on the target program,
further supporting our argument that these tools likely do not
have any one-size-Ô¨Åts-all conÔ¨Åguration that would apply to all
target programs.
III. T HESAT UNE APPROACH
The results of our empirical study motivated us to design
an automated approach to tune the conÔ¨Åguration spaces of
static veriÔ¨Åcation tools so they can successfully verify more
programs. As shown in Section II, the four tools under evalu-
ation have very precise best-precision-conÔ¨Ågs. However, these
conÔ¨Ågurations can complete fewer tasks relative to the total
number of tasks all conÔ¨Ågurations can complete. Therefore,
we designed SAT UNE (for Simulated Annealing Tune) withthe goal of outperforming the tools‚Äô most-correct-conÔ¨Åg ‚Äì in
other words, to maximize the number of correct results.
At a high level, given a tool and target program, SAT UNE
searches through the tools‚Äô conÔ¨Åguration space to Ô¨Ånd a con-
Ô¨Åguration that is likely to complete with a correct veriÔ¨Åcation
result on the target program. We made three key design
choices, based on the results of the study in Section II, that
differentiate SAT UNE from other approaches.
The Ô¨Årst design choice is the adaptation of a meta-heuristic
search algorithm. The Ô¨Åndings in Section II-B demonstrate
that there is no one-size-Ô¨Åts-all conÔ¨Åguration for any tool.
Thus, for each target program, it is necessary to explore the
conÔ¨Åguration space for a suitable conÔ¨Åguration. However, it
is infeasible to explore every conÔ¨Åguration of a veriÔ¨Åcation
tool with even a modest number of conÔ¨Åguration options. A
meta-heuristic search algorithm probabilistically explores such
search spaces to quickly locate a suitable conÔ¨Åguration with
which to run the tool for a given veriÔ¨Åcation task. This choice
is critical for both the efÔ¨Åcacy and efÔ¨Åciency of SAT UNE.
The second design choice is that of the Ô¨Åtness function,
f. To perform a meta-heuristic search, we need a method to
determine the Ô¨Åtness of a conÔ¨Åguration‚Äîthat way, the search
knows whether a new candidate conÔ¨Åguration is better than the
current best conÔ¨Åguration. The only way to know a conÔ¨Ågura-
tion‚Äôs true Ô¨Åtness would be to run the veriÔ¨Åcation tool, observe
whether it completes, and validate its result. However, the vali-
dation step may not even be possible to perform automatically,
and even if it were, it would be prohibitively expensive to do
repeatedly throughout the search. Instead, we use f, which is
a learned model that effectively approximates the Ô¨Åtness of a
tool conÔ¨Åguration (such approximations are commonly known
as asurrogate Ô¨Åtness function in the literature [32]). Our model
needs to be trained once for each tool, but then incurs little
overhead when queried during search. The model steers the
search toward conÔ¨Ågurations that are likely to complete a
veriÔ¨Åcation task with a correct result.
The third design choice is in the data we use to train f. As
demonstrated in Section II-B, there are many conÔ¨Åguration
options that present tradeoffs in terms of producing correct
and incorrect results, based on the speciÔ¨Åc target programs.
These options should be evaluated for each veriÔ¨Åcation task
individually and set in a way that will increase the chance
of getting a correct result. Thus, we train fnot only on the
results of conÔ¨Ågurations in the dataset, but also on the features
of the target program, so that it can learn the ways that options
interact with program features (see Section III-B).
Figure 2 shows the workÔ¨Çow of SAT UNE. To use SAT UNE,
a user provides a target program and a veriÔ¨Åcation tool with
an input conÔ¨Åguration.4SAT UNE then runs the tool on the
target program using the provided input conÔ¨Åguration. If the
run produces a conclusive result, it is reported to the user; in
this case, SAT UNE incurs no overhead. If the run does not lead
to a conclusive result, the meta-heuristic conÔ¨Åguration search
4In our evaluation, we use the best-precision-conÔ¨Åg as the input conÔ¨Ågu-
ration (Section IV).
334Fig. 2: WorkÔ¨Çow of the SAT UNE approach.
begins. We now discuss two key components of SAT UNE: the
meta-heuristic conÔ¨Åguration search and the surrogate Ô¨Åtness
function.
A. Meta-heuristic ConÔ¨Åguration Search
There exist various meta-heuristic search algorithms in the
literature, such as tabu search [33], hill climbing [34], genetic
algorithms [35], and simulated annealing [36]. Among them,
simulated annealing [36] has been shown to be more effective
in Ô¨Ånding Ô¨Åtter objects (like covering arrays [28], [37]‚Äì[39]
and orthogonal arrays [40]) in large and complex combinato-
rial spaces quickly [25], [41], [42]. Considering the similarity
in the search spaces of the mentioned successful applications,
we decided to derive our meta-heuristic conÔ¨Åguration search
algorithm from simulated annealing [36], [43], [44].
At a high level, simulated annealing is a stochastic search
algorithm [45] that iteratively searches for a good solution
by altering the current state to generate a new candidate state
called a neighboring state. If the neighboring state is judged to
be better than the current state according to some heuristic, it is
accepted as the current state for the next search iteration. If the
neighboring state is judged to be worse than the current state, it
may still be accepted probabilistically to help the model avoid
becoming stuck in local optima. The probability of selecting
a worse conÔ¨Åguration decreases over the search. This is done
through the three control parameters: the initial temperature
T0, the cooling rate Rby which the temperature ( T) is
reduced every iteration, and the stopping temperature Ts.
Higher temperatures lead to higher probabilities of accepting
inferior candidates (i.e., inferior candidates are more likely to
be accepted in early iterations than in later iterations), which
allows simulated annealing to be more Ô¨Çexible and exploratory
early in the search process. The search ends either when an
acceptable solution is found according to some criteria or the
temperature falls below Ts.
Algorithm 1 depicts our meta-heuristic conÔ¨Åguration search
algorithm that adapts simulated annealing. The inputs to this
algorithm are: (1) the tool‚Äôs conÔ¨Åguration space CS=hO; Diwhere Ois the set of conÔ¨Åguration options and Dis their
domains, such that di2Dis the set of possible values that
option oi2Ocould take on (sampled for integer domains and
exhaustive otherwise); (2) the target program P; and (3) the
surrogate Ô¨Åtness function f.
Lines 2-7 perform initialization. First, we initialize the
control parameters5asT0=1,Ts=10 5,R=10 4. Then, we
set the running temperature Tto the starting temperature T0,
and the isConclusive Ô¨Çag is initialized as ?indicating
an inconclusive result. At line 5, the current conÔ¨Åguration
cand the best conÔ¨Åguration care both initialized with the
default conÔ¨Åguration (or a randomly generated one if the
default is not available). At line 6, the program representation
vector Vis initialized with the features extracted from P(see
Section III-B). At line 7, fis used to compute the cost of c,Ec,
using the concatenation of Vandcas denoted byhV+ +ci. Ec
is the probability of getting either an inconclusive or incorrect
result if cwere used to run the veriÔ¨Åcation tool on the target
program represented with features V.
Lines 8 to 18 implement an iterative search process that
aims to select a conÔ¨Åguration that is likely to complete the
veriÔ¨Åcation task with a correct result. On line 8, Ts<T
checks that the temperature Thas not decreased below the
stopping temperature Ts[36], which is the standard stopping
condition for simulated annealing. In addition, the search
stops if the veriÔ¨Åcation run is conclusive, because our goal
is to Ô¨Ånd a conÔ¨Åguration which will produce a conclusive
veriÔ¨Åcation result rather than an optimal one. During each
iteration of the inner loop (lines 9-14), the algorithm Ô¨Årst
generates a new neighboring conÔ¨Åguration c0(line 10). To
generate a new neighboring conÔ¨Åguration, we change the value
of a single option oiin the current conÔ¨Åguration to another
random value from its domain di. This simple approach has
been shown to be effective for exploring large search spaces
in a cost-effective manner in similar search problems (e.g.,
combinatorial testing [28], [37], [42], [46]). The algorithm
then computes the cost of c0using f(line 11), and reduces
the running temperature by the cooling rate (line 13).
This random conÔ¨Åguration generation repeats until one of
the acceptance conditions on line 14 is met: either E<0,
meaning c0is better than caccording to f, or the algorithm de-
cides to accept the inferior c0with probability e kE=T[47].
This probability reduces with T. Once a state is accepted, cand
Ecare updated (line 15). If cis the best so far (i.e., Ec<Ec),
thencandEcare also updated accordingly (lines 16-17).
When a new cis found, we run the veriÔ¨Åcation tool using c
for the task P. If the veriÔ¨Åer produces a conclusive result, the
search ends and we return the result (line 19). Otherwise, the
search continues to the next iteration.
Note that running the veriÔ¨Åcation tool is the most expensive
step of the Algorithm 1. In comparison, learning f, computing
5We empirically determined these values with preliminary experiments that
showed that T0andTsdid not impact the performance signiÔ¨Åcantly, while R
did. Larger Rvalues (0.01, 0.001) caused the search end too quickly without
sufÔ¨Åcient exploration while smaller values (10 5) caused longer search times.
335Algorithm 1 Meta-heuristic conÔ¨Åguration search.
1:function CONFIG SEARCH (CS=hO; Di,P,f)
2:T0 1;Ts 10 5;R 10 4.control parameters
3:T T0
4:isConclusive ?
5:c c getRandomOrDefault(CS )
6:V getProgramRepresentation(P )
7:Ec Ec f(hV+ +ci) .cost forhV+ +ci
8: while Ts< T^:isConclusive do
9: repeat
10: c0 getNeighboringConfig (CS; c)
11: Ec0 f(hV+ +c0i)
12: E Ec0 Ec
13: T T (TR)
14: until E < 0_rand(0; 1)< e kE=T
15: c; Ec c0; Ec0 .accept
16: ifEc< Ecthen
17: c; Ec c; Ec .best conÔ¨Åg so far
18: isConclusive runV erifier (P; c)
19:returnhisConclusive; ci
program representations, generating random conÔ¨Ågurations,
and computing their cost take negligible time.
B. Learning the Surrogate Fitness Function
We learn the surrogate Ô¨Åtness function fusing the dataset
we created for the empirical study in Section II. In this
dataset, each data point is of the form hV+ +ci=X where
X2 fcorrect, incorrect, inconclusiveg is the veriÔ¨Åcation
result and + +is the concatenation operator. By including both
the program features and conÔ¨Åguration in each data point,
our models can learn from the interactions between them. f
is trained to differentiate between data points with either an
incorrect orinconclusive veriÔ¨Åcation result and data points
with a correct veriÔ¨Åcation result. Effectively, freturns the
probability of producing an incorrect or inconclusive for a
givenhV+ +ci combination as its cost. Formally,
Ec=f(hV+ +ci) = P[incorrectjhV+ +ci]
+P[inconclusivejhV+ +ci]
Recall that Algorithm 1 aims at minimizing Ec, which
translates to locating conÔ¨Ågurations that are more likely to
produce conclusive and correct results.
Past research has applied a variety of models and features
to learn from program code [48]‚Äì[51]. In this work, we use a
simple Bag of Words model [52] for its simplicity and relative
efÔ¨Åcacy in representing programs for classifying static analysis
results [50]. We represent a program as a frequency vector
by counting the frequencies of program instructions like load,
store, allocate, and call; and certain constructs like branches,
loops, functions, and primitive/array/pointer/compound types.
This simplicity makes our approach somewhat language-
agnostic (i.e., extendable to any programming language by
identifying the relevant program instructions and constructs).C. Implementation
We instantiated SAT UNE to tune the conÔ¨Åguration spaces of
CBMC ,Symbiotic, JayHorn, and JBMC . We learned the Ô¨Åtness
functions with a random forest algorithm from Weka [53]‚Äì
[55]. To construct the Bag of Words model, we counted
the occurrence of intermediate representation (IR) instruction
types and different program features (e.g., loops, branches,
and function calls) in our benchmarks. The program features
were collected via simple static analyses for C and Java. C
programs (targeted by CBMC andSymbiotic) are represented
by frequency vectors for 11 program features and 46 LLVM
IR instructions [56]. Java programs (targeted by JayHorn and
JBMC ) are represented by frequency vectors of 9 program
features and 23 WALA IR instructions [57]. LLVM and
WALA are popular frameworks for the analysis of C and Java
programs, respectively.
We implemented SAT UNE in600 lines of Java code with
a command-line interface (cli). The SAT UNEcli takes a
veriÔ¨Åcation tool, a target program to be veriÔ¨Åed, and the initial
conÔ¨Åguration as input.
IV. E VALUATION
In this section, we discuss the experimental setup for
evaluating SAT UNE and answer two research questions on
how well it performs.
A. Experimental Setup
We trained SAT UNE‚Äôs Ô¨Åtness function on the dataset we
generated in Section II. We then evaluated it against other po-
tential strategies for selecting a veriÔ¨Åcation tool conÔ¨Åguration,
in terms of both correctness of the results and running time.
1) Training of SAT UNE‚Äôs Ô¨Åtness function: We split the
dataset of each veriÔ¨Åcation tool into Ô¨Åve equal partitions that
are disjoint by benchmark programs. Four of them are used for
training the Ô¨Åtness function, f, while one partition is held out
to evaluate SAT UNE (using the finternally). We then rotated
the partitions and repeated this process 25 times to perform
5-fold cross-validation [58] with Ô¨Åve different random seeds.
Since these repetitions allow all of the data to be used for both
training and evaluation (at different iterations), we were able
to evaluate SAT UNE on the entire dataset.
In total, we trained 100 surrogate Ô¨Åtness functions (5 ran-
dom seeds5-fold cross-validation 4 tools). We report the
total number of conclusive results across all cross-validation
sets as SAT UNE‚Äôs results.
2) Comparison Baselines: To the best of our knowledge,
SAT UNE is the Ô¨Årst tool- and language-agnostic approach
that automatically conÔ¨Ågures program veriÔ¨Åcation tools. We
designed three baselines that simulate ways a user might use
and tune a veriÔ¨Åcation tool.
The Ô¨Årst baseline simulates a user who would only try a
single conÔ¨Åguration of a tool and accept whatever outcome
that conÔ¨Åguration gives. We assume the user does not have
extensive domain expertise. Rather than manually tuning the
tool for a target program, they simply try a conÔ¨Åguration that
336TABLE IV: Results for SAT UNE and three baselines. The numbers in normal font are the median of 5 runs (with different
random seeds for SAT UNE andprecision!random), and the numbers in the smaller font are the semi-interquartile range (SIQR).
ForSymbiotic andJBMC , the results of precision!correct are not shown because they are the same as the most-correct-conÔ¨Åg.
Tool Approach Correct Incorrect Inconclusive Precision
CBMCSAT UNE 804 12 196 13 0 3 80.40%
precision! random 738 13 195 8 67 13 79.10%
precision! correct 704 0 262 0 34 34 72.88%
most-correct-conÔ¨Åg 635 0 262 0 103 0 70.79%
SymbioticSAT UNE 277 2 1 0 387 2 99.64%
precision! random 264 1 1 0 400 1 99.62%
most-correct-conÔ¨Åg 261 0 1 0 403 0 99.62%
JayHornSAT UNE 240 0 13 2 115 2 94.86%
precision! random 211 4 14 2 143 2 93.79%
precision! correct 247 0 38 0 83 0 86.67%
most-correct-conÔ¨Åg 227 0 37 0 104 0 85.98%
JBMCSAT UNE 348 2 7 3 13 1 98.08%
precision! random 343 2 6 1 19 1 98.28%
most-correct-conÔ¨Åg 331 0 0 0 37 0 100%
does well overall. In our evaluation, we use the most-correct-
conÔ¨Åg of each tool as this baseline.
The second baseline simulates a user who has more time
to try to get a correct result. In this case, the user Ô¨Årst tries a
highly precise conÔ¨Åguration (e.g., best-precision-conÔ¨Åg) and
if the result is inconclusive, tries again on a less precise
but performant conÔ¨Åguration. In our evaluation, we Ô¨Årst run
each tool‚Äôs best-precision-conÔ¨Åg. If it produces a conclu-
sive result, we report it. Otherwise, we fall back to run
the most-correct-conÔ¨Åg and report the result. We call this
baseline precision!correct . Because the best-precision-conÔ¨Åg
and most-correct-conÔ¨Åg are the same for Symbiotic andJBMC
(Section II), the results of precision!correct are the same as
most-correct-conÔ¨Åg for these tools.
Finally, the third baseline simulates a user whose target
program may be difÔ¨Åcult for a veriÔ¨Åcation tool to complete.
The user also has a large amount of time to experiment with
different conÔ¨Ågurations to Ô¨Ånd one that may work. In this
baseline, we start by using the best-precision-conÔ¨Åg. If it
fails to complete within the timeout, a random search begins
based off of the best-precision-conÔ¨Åg. The neighbor generation
strategy is the same as SAT UNE, in that for each iteration
we randomly alter a single setting of a conÔ¨Åguration option.
The random search continues until it Ô¨Ånds a conÔ¨Åguration that
Ô¨Ånishes within the time limit, or it reaches 60 attempts. We
call this baseline precision!random.
3) Metrics: We use three metrics in our evaluation: (1) the
number of correct veriÔ¨Åcation results, (2) precision (i.e., the
percentage of conclusive results that are correct), and (3) the
total run time to complete each veriÔ¨Åcation task in minutes.
Each experiment was repeated Ô¨Åve times, and we report the
median and semi-interquartile range (SIQR) values for the Ô¨Årst
two metrics.
4) Research Questions: Our evaluation aims to answer two
research questions:
RQ3: Can SAT UNE correctly verify more programs than
baselines?RQ4: How efÔ¨Åcient is SAT UNE?
RQ3 compares SAT UNE to the three baselines in terms of
number of correct results and precision. Due to the randomness
in both approaches, we also performed statistical analysis to
determine whether the results produced by precision!random
are signiÔ¨Åcantly different from those produced by SAT-
UNE.RQ4 aims to determine how efÔ¨Åcient SAT UNE‚Äôs meta-
heuristic conÔ¨Åguration search is. We explore the distribution
of execution times to determine, for each tool, how SAT UNE
compares to the three baselines.
B. Experimental Results
1) RQ3: Can SAT UNE correctly verify more programs?:
Table IV presents the results of SAT UNE and the three base-
lines for each tool using median and SIQR metrics. Overall,
we Ô¨Ånd that compared to the baselines, SAT UNE consistently
achieves the best balance between the number of correct
results and precision.
In all tools but JayHorn, SAT UNE completes more tasks
correctly than any other baseline strategy. In Symbiotic, SAT-
UNE produced 16 and 13 more correct results than most-
correct-conÔ¨Åg and precision!random, respectively, without
any more incorrect results. In JayHorn, precision!correct
completed 7 more tasks correctly than SAT UNE, but at the cost
of 25 more incorrect results. Notably, SAT UNE allowed CBMC
to complete every task, at the cost of only a single more incor-
rect result than the next-best baseline (precision!random).
In terms of precision, SAT UNE achieved higher precision
than all other baselines for every tool but JBMC . SAT UNE was
still highly precise in JBMC (98.08%), but recall that JBMC ‚Äôs
best-precision-conÔ¨Åg achieved 100% precision. Still, SAT UNE
was able to complete 17 more veriÔ¨Åcation tasks correctly than
most-correct-conÔ¨Åg inJBMC .
Interestingly, we found that SAT UNE was able to correctly
complete some veriÔ¨Åcation tasks that no single conÔ¨Åguration
in our study could (i.e., tasks that are in the ‚Äúnever solved‚Äù
column of Table II). SAT UNE correctly completed 1, 8, 1,
337and 3 such tasks for CBMC ,Symbiotic, JBMC , and JayHorn,
respectively. This suggests that SAT UNE‚Äôs Ô¨Åtness function was
able to generalize to conÔ¨Ågurations it had not previously seen
in the training data.
Last, we discuss the variations in the results using the
SIQR metric. Overall, SAT UNE andprecision!random had
small variations in their results while most-correct-conÔ¨Åg and
precision!correct had none. It is expected that no variation
is present in the most-correct-conÔ¨Åg and precision!correct
results because both are deterministic. The variations of SAT-
UNE and precision!random are due to the randomness in
their search process. For SAT UNE, its largest variation from
theCBMC results is still relatively small, accounting for about
1% of the total number of tasks.
2) RQ4: How efÔ¨Åcient is SAT UNE?:Figure 3 illustrates
the distribution of the execution times to verify each program
(y-axis in logarithmic scale) as box-plots for all of our exper-
iments. Each box-plot represents the conclusive veriÔ¨Åcation
runs of a tool using SAT UNE or a baseline approach. The
width of the box-plots reÔ¨Çects the population size, i.e., the
number of correct +incorrect results (shown in Table IV).
50% of the data points fall inside the box. The line inside the
box is the median, and the lower and upper ends of the box
correspond to the Ô¨Årst and third quartiles, respectively. The
red dot and number show the longest time it takes for each
approach to complete one task. The other dots on the central
line of each box show the outliers.
We found that all four approaches were relatively fast for
most tasks, with 75% of tasks being completed in under a
minute in all cases. Still, we see that even in the worst case
(red dots), SAT UNE was 2-4x faster than precision!random.
This is attributable to SAT UNE‚Äôs Ô¨Åtness function‚ÄîspeciÔ¨Åcally,
because it screens conÔ¨Ågurations in advance and only runs
them if they are judged to be Ô¨Åt. In JayHorn, where SAT UNE
had the highest median and maximum run time, it generated
a median of 143 conÔ¨Ågurations and only ran 3 of them. In
contrast, random search generated and ran a median of 9
conÔ¨Ågurations.
Figure 3 also shows that most-correct-conÔ¨Åg was the only
baseline that consistently did better than SAT UNE in terms of
median execution time. This is expected since most-correct-
conÔ¨Åg only runs a single conÔ¨Åguration (with 60 seconds
timeout). More notably is that SAT UNE is comparable to (or,
in the case of CBMC , outperforms) precision!correct , which
only runs two conÔ¨Ågurations. These Ô¨Åndings demonstrate the
efÔ¨Åciency of SAT UNE and the signiÔ¨Åcant advantage its Ô¨Åtness
function gives it over other search strategies.
V. T HREATS TO VALIDITY
Here we enumerate the potential threats to the validity
of our work and the steps we took to mitigate them. First,
the benchmark dataset we used from SV-COMP is primarily
composed of artiÔ¨Åcial benchmarks. Thus, the conclusions
we made about the relative quality of SAT UNE compared
to other baselines may not generalize to large, real-world
9.134.6
2.8
1.813.329.3
116.435.5
2
14.318.5
0.9CBMC Symbiotic JayHorn JBMC
SATune
precise‚Üírandom precise‚Üícorrectmost-correct-configSATune
precise‚Üírandommost-correct-configSATune
precise‚Üírandom precise‚Üícorrectmost-correct-configSATune
precise‚Üírandommost-correct-config141632 Task completion time in minutes (log)Fig. 3: Execution time of SAT UNE and the baselines.
programs. Unfortunately, we are unaware of a large real-
world benchmark for which the ground truths are known,
but we believe the large number of programs we used and
the diversity of the SV-COMP benchmark partially mitigate
this potential threat. Second, the conÔ¨Åguration samples in our
empirical study may not be representative of the tools‚Äô full
conÔ¨Åguration spaces. Our conÔ¨Åguration samples include every
three-way combinations of conÔ¨Åguration option settings, and
past research in conÔ¨Ågurable software indicates that the ma-
jority of program behaviors are attributable to the interaction
of few options [27]. Finally, there could be variance in the
performance of SAT UNE (and precision!random) caused by
non-deterministic operations in machine learning and conÔ¨Ågu-
ration selection. We partially mitigated this potential threat by
running 5 replications of each experiment, and reporting the
median and semi-interquartile range values which suggested
small variations (RQ3).
VI. R ELATED WORK
To the best of our knowledge, this work is the Ô¨Årst to use
meta-heuristic search and machine learning to tune software
veriÔ¨Åcation tools with large conÔ¨Åguration spaces. Our work
is related to work that (1) studies the conÔ¨Åguration spaces of
analysis tools, (2) selects a static analysis tool or a conÔ¨Ågura-
tion of a static analysis tool that is most suited to a given task,
(3) selectively applies algorithms in a static analysis, (4) uses
machine learning models as Ô¨Åtness functions in meta-heuristic
search, and (5) tunes high performance computing systems for
a given system architecture and hardware.
Studies of Tool ConÔ¨Åguration Spaces. We believe we
are the Ô¨Årst work to systematically study the conÔ¨Åguration
338spaces of static program veriÔ¨Åcation tools. Other work has
engaged in similar goals with other types of static analyzers,
speciÔ¨Åcally focusing on the tradeoffs presented by different
conÔ¨Ågurations and conÔ¨Åguration option settings [1]‚Äì[3]. Wei
et al. present a study that evaluates the tradeoffs in the 162
different conÔ¨Ågurations of a numerical static analysis for Java
programs [2]. Smaragdakis et al. [1] and Lhot ¬¥ak and Hendren
[3] instantiate multiple variants of context-sensitive points-
to analysis for Java to understand the tradeoffs of different
design decisions. The tools we studied present much larger
conÔ¨Åguration spaces than those in the past studies that required
us to apply statistical analysis to understand the impact of
conÔ¨Åguration options.
ConÔ¨Åguration and tool selection. Our work is also highly
relevant to those that select strategies (i.e., full conÔ¨Ågurations)
within a static analysis tool [6], [10], predict or rank which
static analysis tool is suitable for a given task [11]‚Äì[13], and
more broadly attempt to learn performance models of tool
conÔ¨Åguration spaces [59]‚Äì[68]. Beyer and Dangl present a
selection approach that uses four manually deÔ¨Åned binary
program features to select between three manually deÔ¨Åned
veriÔ¨Åcation strategies for CPA CHECKER [6]. Richter and
Wehrheim present P ESCO, which uses machine learning to
rank Ô¨Åve CPA CHECKER veriÔ¨Åers [10]. SAT UNE differs from
these efforts in that it explores large and complex conÔ¨Åg-
uration spaces in a tool- and language-agnostic way using
meta-heuristics, instead of using predeÔ¨Åned conÔ¨Ågurations,
manually deÔ¨Åned heuristics, or ranking tools.
Other researchers [11]‚Äì[15] have explored selecting a veri-
Ô¨Åcation tool or SAT solver from a set that would be the most
appropriate for a given task. For example, Tulsian et al. present
MUX, a machine learning-based approach that uses features
extracted from Windows device drivers to select the fastest
veriÔ¨Åcation tool to analyze them [11]. Xu et al. developed
SAT ZILLA 2012 [14], which is the most recent version of the
SAT ZILLA tool [15]. SAT ZILLA 2012, given a SAT problem
and a set of SAT solvers as input, attempts to select a solver
that will perform the best in terms of run time. Our research
complements these works by selecting a conÔ¨Åguration of a
single static analysis tool. The conÔ¨Åguration spaces we select
from are much larger than the sets of tools these works used,
motivating the need for a meta-heuristic search strategy.
Finally, some software product line (SPL) research is closely
related to our work, in that these works use machine learning
or statistical methods to model the effects of setting conÔ¨Åg-
urations. SPLC ONQUEROR is a well-known tool that tries to
compute the optimal conÔ¨Ågurations of a tool, given a target
metric (e.g., precision or run time) [60]. Similarly, Ha and
Zhang‚Äôs DeepPerf models a tool‚Äôs conÔ¨Åguration space by
training deep neural networks [63]. Nair et al.‚Äôs WHAT pre-
dicts a performance model using a small number of sampled
conÔ¨Ågurations by performing dimensionality reduction on a
target program‚Äôs conÔ¨Åguration space [66]. FLASH, by Nair et
al., builds a performance model using sequential model-based
optimization, which allows the model to continue learning
about the conÔ¨Åguration space as it explores it and producessamples [67]. Finally, Nair et al., demonstrated that cheap and
inaccurate predictors that rank conÔ¨Ågurations often perform
as well or better than other more expensive approaches [68].
Other researchers have focused not on performance models,
but rather on evaluating and contributing sampling approaches
for SPLs. Pereira et al. evaluated six different conÔ¨Åguration
sampling approaches to determine their relative strengths and
weaknesses for selecting representative conÔ¨Ågurations [64].
Oh et al. contributed a sampling approach which models
feature spaces as counting binary decision diagrams, and then
produces truly random samples of SPL conÔ¨Ågurations [65].
While SAT UNE also aims to select conÔ¨Ågurations, it also takes
into account the features of the target program, which allows
it to consider the tradeoffs conÔ¨Åguration options of the static
program veriÔ¨Åcation tools present.
Selective Static Analysis. Other related work selectively
applies static analysis algorithms to parts of the target program.
Among analysis algorithms, context sensitivity is the most
studied [5], [8], [9], [16]. For example, Wei and Ryder present
an adaptive context-sensitive analysis that uses eight features
extracted from the points-to and call graphs of JavaScript pro-
grams [8]. Other algorithms such as Ô¨Çow sensitivity have also
been used to develop selective static analysis [69]. Our work
similarly studies the relationship between program features
and analysis algorithms to achieve a good balance between
performance, precision, and soundness, but we consider a
wide range of conÔ¨Åguration options while being agnostic
to the analysis algorithm. In addition, instead of developing
new analysis algorithms or tools, our approach automatically
conÔ¨Ågures existing veriÔ¨Åcation tools.
ML Models as Fitness Functions. Several researchers ex-
plore the use of machine learning models as ‚Äúsurrogate‚Äù Ô¨Åtness
functions. Brownlee et al. demonstrated that a Markov network
could be an effective surrogate Ô¨Åtness function in genetic
algorithms for feature selection in Case-Based Reasoning [70].
Jin and Sendhoff use ensembles of neural networks to improve
the performance of evolutionary algorithms [71]. Singh et al.
evaluated both regression models and radial basis functions as
surrogate Ô¨Åtness functions in simulated annealing [72]. While
these papers focus on improving the meta-heuristic algorithms,
we are not aware of any other work that automatically learns
surrogate Ô¨Åtness functions for tuning veriÔ¨Åcation tools with
large conÔ¨Åguration spaces.
High Performance Computing. Lastly, a distantly related
line of work includes automatically tuning high performance
computing (HPC) systems for a given system architecture
and hardware. Agakov et al. use machine learning to perform
iterative optimization for HPC systems at compile time [73].
Ansel et al. present an extensible framework, O PENTUNER ,
that enables writing domain-speciÔ¨Åc HPC tuners [74]. For a
more comprehensive review of the literature on HPC system
tuning, we refer readers to a recent survey by Ashouri et
al [75]. Our work differs from the work above in that we
use a meta-heuristic search augmented with a surrogate Ô¨Åtness
function for tuning program veriÔ¨Åcation tools‚Äîwhich are not
HPC systems‚Äîto get a desired veriÔ¨Åcation outcome.
339VII. C ONCLUSIONS AND FUTURE WORK
In this paper, we presented SAT UNE, the Ô¨Årst auto-tuning
approach for static program veriÔ¨Åcation tools with large con-
Ô¨Åguration spaces. The design of SAT UNE is motivated by an
empirical study that provided important insights on the charac-
teristics of conÔ¨Åguration spaces and the impacts of individual
conÔ¨Åguration options on the precision and overall correct-
ness of the veriÔ¨Åcation tools‚Äô results. First, we demonstrated
that there is no one-size-Ô¨Åts-all conÔ¨Åguration in any tool.
Even the most-correct-conÔ¨Åg could not complete certain tasks
that other conÔ¨Ågurations did. Second, we found that many
conÔ¨Åguration options present tradeoffs between precision and
performance, and they should be tuned individually for given
target programs to get the most out of the tools‚Äô capabilities.
SAT UNE is novel in that it uses a simple meta-heuristic
search algorithm with surrogate Ô¨Åtness functions learned from
data to explore large conÔ¨Åguration spaces and avoid running
conÔ¨Ågurations that are likely to produce incorrect results. It
is tool- and language-agnostic. We applied SAT UNE to four
popular veriÔ¨Åcation tools for both C and Java programs and
evaluated its performance using the ground-truth datasets. The
evaluation shows that SAT UNE achieves the best balance
between performance and precision improvements compared
to the baselines we used.
In future work, we will integrate other machine learning
techniques, such as neural networks, into SAT UNE to train the
surrogate Ô¨Åtness function. This will elide the need to manually
identify and extract program features, and enable SAT UNE to
take advantage of more complex structural information that
neural networks can potentially learn. We will also extend
the conÔ¨Åguration generation step of SAT UNE to incorporate
the Ô¨Åndings from our empirical study in Section II-B. This
will enable more effective scanning of the conÔ¨Åguration space
and help improve the tools‚Äô precision by better avoiding
conÔ¨Ågurations that are likely to lead to incorrect results. We
will also extend our dataset and apply SAT UNE to additional
tools targeting other programming languages.
ACKNOWLEDGMENTS
This work is partly supported by NSF grants CCF-2007314,
CCF-2008905 and CCF-2047682, and the NSF graduate re-
search fellowship program.
REFERENCES
[1] Y . Smaragdakis, M. Bravenboer, and O. Lhot ¬¥ak, ‚ÄúPick your
contexts well: Understanding object-sensitivity,‚Äù SIGPLAN Not. ,
vol. 46, no. 1, p. 17‚Äì30, Jan. 2011. [Online]. Available: https:
//doi.org/10.1145/1925844.1926390
[2] S. Wei, P. Mardziel, A. Ruef, J. S. Foster, and M. Hicks,
‚ÄúEvaluating design tradeoffs in numeric static analysis for java,‚Äù in
Programming Languages and Systems - 27th European Symposium
on Programming, ESOP 2018, Held as Part of the European Joint
Conferences on Theory and Practice of Software, ETAPS 2018,
Thessaloniki, Greece, April 14-20, 2018, Proceedings. Cham: Springer
International Publishing, 2018, pp. 653‚Äì682. [Online]. Available:
https://doi.org/10.1007/978-3-319-89884-1 23
[3] O. Lhot ¬¥ak and L. Hendren, ‚ÄúEvaluating the beneÔ¨Åts of context-sensitive
points-to analysis using a bdd-based implementation,‚Äù ACM Trans.
Softw. Eng. Methodol., vol. 18, no. 1, Oct. 2008. [Online]. Available:
https://doi.org/10.1145/1391984.1391987[4] T. Xu, L. Jin, X. Fan, Y . Zhou, S. Pasupathy, and R. Talwadker,
‚ÄúHey, you have given me too many knobs!: Understanding and
dealing with over-designed conÔ¨Åguration in system software,‚Äù in
Proceedings of the 2015 10th Joint Meeting on Foundations of Software
Engineering, ser. ESEC/FSE 2015. New York, NY , USA: Association
for Computing Machinery, 2015, p. 307‚Äì319. [Online]. Available:
https://doi.org/10.1145/2786805.2786852
[5] S. Jeong, M. Jeon, S. Cha, and H. Oh, ‚ÄúData-driven context-sensitivity
for points-to analysis,‚Äù Proc. ACM Program. Lang., vol. 1, no. OOPSLA,
pp. 100:1‚Äì100:28, Oct. 2017.
[6] D. Beyer and M. Dangl, ‚ÄúStrategy Selection for Software VeriÔ¨Åcation
Based on Boolean Features,‚Äù in Leveraging Applications of Formal
Methods, VeriÔ¨Åcation and Validation. VeriÔ¨Åcation , ser. Lecture Notes in
Computer Science, T. Margaria and B. Steffen, Eds. Cham: Springer
International Publishing, 2018, pp. 144‚Äì159.
[7] Y . Smaragdakis, G. Kastrinis, and G. Balatsouras, ‚ÄúIntrospective analy-
sis: Context-sensitivity, across the board,‚Äù in Proceedings of the 35th
ACM SIGPLAN Conference on Programming Language Design and
Implementation, ser. PLDI ‚Äô14. New York, NY , USA: ACM, 2014,
pp. 485‚Äì495.
[8] S. Wei and B. G. Ryder, ‚ÄúAdaptive context-sensitive analysis
for javascript,‚Äù in 29th European Conference on Object-Oriented
Programming, ECOOP 2015, July 5-10, 2015, Prague, Czech
Republic. Dagstuhl, Germany: Schloss Dagstuhl‚ÄìLeibniz-Zentrum
fuer Informatik, 2015, pp. 712‚Äì734. [Online]. Available: https:
//doi.org/10.4230/LIPIcs.ECOOP.2015.712
[9] Y . Li, T. Tan, A. M√∏ller, and Y . Smaragdakis, ‚ÄúScalability-Ô¨Årst pointer
analysis with self-tuning context-sensitivity,‚Äù in Proceedings of the
2018 26th ACM Joint Meeting on European Software Engineering
Conference and Symposium on the Foundations of Software Engineering,
ser. ESEC/FSE 2018. New York, NY , USA: ACM, 2018, pp. 129‚Äì140.
[10] C. Richter and H. Wehrheim, ‚ÄúPesco: Predicting sequential combinations
of veriÔ¨Åers,‚Äù in International Conference on Tools and Algorithms for
the Construction and Analysis of Systems, Springer. Cham: Springer
International Publishing, 2019, pp. 229‚Äì233.
[11] V . Tulsian, A. Kanade, R. Kumar, A. Lal, and A. V . Nori, ‚ÄúMux:
algorithm selection for software model checkers,‚Äù in Proceedings of the
11th Working Conference on Mining Software Repositories, ACM. New
York, NY , USA: ACM, 2014, pp. 132‚Äì141.
[12] M. Czech, E. H ¬®ullermeier, M.-C. Jakobs, and H. Wehrheim, ‚ÄúPredicting
Rankings of Software VeriÔ¨Åcation Tools,‚Äù in Proceedings of the 3rd ACM
SIGSOFT International Workshop on Software Analytics, ser. SWAN
2017. New York, NY , USA: ACM, 2017, pp. 23‚Äì26.
[13] Y . Demyanova, T. Pani, H. Veith, and F. Zuleger, ‚ÄúEmpirical software
metrics for benchmarking of veriÔ¨Åcation tools,‚Äù Formal Methods in
System Design, vol. 50, no. 2, pp. 289‚Äì316, Jun. 2017.
[14] L. Xu, F. Hutter, J. Shen, H. H. Hoos, and K. Leyton-Brown,
‚ÄúSatzilla2012: Improved algorithm selection based on cost-sensitive
classiÔ¨Åcation models,‚Äù Proceedings of SAT Challenge, vol. 2012, 2012.
[15] E. Nudelman, K. Leyton-Brown, A. Devkar, Y . Shoham, and H. Hoos,
‚ÄúSatzilla: An algorithm portfolio for sat,‚Äù Solver description, SAT
competition, vol. 2004, 2004.
[16] Y . Li, T. Tan, A. M√∏ller, and Y . Smaragdakis, ‚ÄúPrecision-guided context
sensitivity for pointer analysis,‚Äù Proc. ACM Program. Lang., vol. 2, no.
OOPSLA, pp. 141:1‚Äì141:29, Oct. 2018.
[17] O. Lhot ¬¥ak and L. Hendren, ‚ÄúScaling java points-to analysis using
spark,‚Äù in Proceedings of the 12th International Conference on Compiler
Construction, ser. CC‚Äô03. Berlin, Heidelberg: Springer-Verlag, 2003,
p. 153‚Äì169.
[18] D. Beyer, ‚ÄúAutomatic veriÔ¨Åcation of c and java programs: Sv-comp
2019,‚Äù in Tools and Algorithms for the Construction and Analysis of
Systems, D. Beyer, M. Huisman, F. Kordon, and B. Steffen, Eds. Cham:
Springer International Publishing, 2019, pp. 133‚Äì155.
[19] D. Kroening and M. Tautschnig, ‚ÄúCbmc ‚Äì c bounded model checker,‚Äù
inTools and Algorithms for the Construction and Analysis of Systems,
E.¬¥Abrah ¬¥am and K. Havelund, Eds. Berlin, Heidelberg: Springer Berlin
Heidelberg, 2014, pp. 389‚Äì391.
[20] J. Slab `y, J. Strej Àácek, and M. Trt ¬¥ƒ±k, ‚ÄúChecking properties described by
state machines: On synergy of instrumentation, slicing, and symbolic
execution,‚Äù in International Workshop on Formal Methods for Indus-
trial Critical Systems, Springer. Berlin, Heidelberg: Springer Berlin
Heidelberg, 2012, pp. 207‚Äì221.
[21] J. Slaby, J. Strej Àácek, and M. Trt ¬¥ƒ±k, ‚ÄúSymbiotic: synergy of instrumen-
tation, slicing, and symbolic execution,‚Äù in International Conference on
340Tools and Algorithms for the Construction and Analysis of Systems,
Springer. Cham: Springer International Publishing, 2013, pp. 630‚Äì632.
[22] L. Cordeiro, P. Kesseli, D. Kroening, P. Schrammel, and M. Trtik,
‚ÄúJBMC: A bounded model checking tool for verifying Java bytecode,‚Äù
inComputer Aided VeriÔ¨Åcation (CAV), ser. LNCS, vol. 10981. Cham:
Springer International Publishing, 2018, pp. 183‚Äì190.
[23] T. Kahsai, P. R ¬®ummer, H. Sanchez, and M. Sch ¬®af, ‚ÄúJayhorn: A frame-
work for verifying java programs,‚Äù in International Conference on
Computer Aided VeriÔ¨Åcation, Springer. Cham: Springer International
Publishing, 2016, pp. 352‚Äì358.
[24] ‚ÄúCbmc, understanding loop unwinding,‚Äù 2014,
http://www.cprover.org/cprover-manual/cbmc/unwinding 2021-04-019.
[25] C. Nie and H. Leung, ‚ÄúA survey of combinatorial testing,‚Äù ACM
Computing Surveys, vol. 43, pp. 11:1‚Äì11:29, February 2011.
[26] C. Yilmaz, S. Fouch ¬¥e, M. B. Cohen, A. Porter, G. Demiroz, and U. Koc,
‚ÄúMoving Forward with Combinatorial Interaction Testing,‚Äù Computer,
vol. 47, no. 2, pp. 37‚Äì45, Feb. 2014.
[27] T. Th ¬®um, S. Apel, C. K ¬®astner, I. Schaefer, and G. Saake, ‚ÄúA
classiÔ¨Åcation and survey of analysis strategies for software product
lines,‚Äù ACM Comput. Surv. , vol. 47, no. 1, Jun. 2014. [Online].
Available: https://doi.org/10.1145/2580950
[28] U. Koc and C. Yilmaz, ‚ÄúApproaches for computing test-case-aware
covering arrays,‚Äù Software Testing, VeriÔ¨Åcation and Reliability, vol. 28,
no. 7, p. e1689, 2018.
[29] ‚ÄúResults of the competition,‚Äù 2019, https://sv-comp.sosy-
lab.org/2019/results/results-veriÔ¨Åed/, Accessed: 2021-04-22.
[30] ‚ÄúResults of the competition,‚Äù 2018, https://sv-comp.sosy-
lab.org/2018/results/results-veriÔ¨Åed/, Accessed: 2021-04-22.
[31] T. Speed, ‚ÄúIntroduction to Ô¨Åsher (1926) the arrangement of Ô¨Åeld exper-
iments,‚Äù in Breakthroughs in statistics. New York, NY: Springer New
York, 1992, pp. 71‚Äì81.
[32] A. E. Brownlee, J. R. Woodward, and J. Swan, ‚ÄúMetaheuristic design
pattern: Surrogate Ô¨Åtness functions,‚Äù in Proceedings of the Companion
Publication of the 2015 Annual Conference on Genetic and Evolutionary
Computation, ser. GECCO Companion ‚Äô15. New York, NY , USA:
Association for Computing Machinery, 2015, p. 1261‚Äì1264.
[33] F. Glover and M. Laguna, Tabu Search. Boston, MA: Springer US,
1998, pp. 2093‚Äì2229.
[34] B. Selman and C. P. Gomes, ‚ÄúHill-climbing search,‚Äù Encyclopedia of
cognitive science, vol. 81, p. 82, 2006.
[35] M. Srinivas and L. M. Patnaik, ‚ÄúGenetic algorithms: A survey,‚Äù Com-
puter, vol. 27, no. 6, p. 17‚Äì26, Jun. 1994.
[36] S. Kirkpatrick, C. D. Gelatt, and M. P. Vecchi, ‚ÄúOptimization by
simulated annealing,‚Äù science, vol. 220, no. 4598, pp. 671‚Äì680, 1983.
[37] M. B. Cohen, C. J. Colbourn, and A. C. H. Ling, ‚ÄúAugmenting
simulated annealing to build interaction test suites,‚Äù in Proc. of the 14th
Int‚Äôl Symposium on Software Reliability Engineering, ser. ISSRE ‚Äô03.
Washington, DC, USA: IEEE Computer Society, 2003, pp. 394‚Äì405.
[38] R. C. Bryce and C. J. Colbourn, ‚ÄúOne-test-at-a-time heuristic search for
interaction test suites,‚Äù in Proceedings of the 9th annual conference on
Genetic and evolutionary computation, ser. GECCO ‚Äô07. New York,
NY , USA: ACM, 2007, pp. 1082‚Äì1089.
[39] H. Mercan, C. Yilmaz, and K. Kaya, ‚ÄúChip: A conÔ¨Ågurable hybrid
parallel covering array constructor,‚Äù IEEE Transactions on Software
Engineering, vol. 45, no. 12, pp. 1270‚Äì1291, 2018.
[40] R. Wang and R. Safadi, ‚ÄúGenerating mixed multilevel orthogonal arrays
by simulated annealing,‚Äù in Computing Science and Statistics. Springer,
1992, pp. 557‚Äì560.
[41] J. Stardom, Metaheuristics and the search for covering and packing
arrays. Simon Fraser University Burnaby, 2001.
[42] J. Torres-Jimenez and E. Rodriguez-Tello, ‚ÄúNew bounds for binary
covering arrays using simulated annealing,‚Äù Information Sciences, vol.
185, no. 1, pp. 137‚Äì152, 2012.
[43] P. J. Van Laarhoven and E. H. Aarts, Simulated annealing . Dordrecht:
Springer Netherlands, 1987.
[44] V . Granville, M. Kriv ¬¥anek, and J.-P. Rasson, ‚ÄúSimulated annealing: A
proof of convergence,‚Äù Pattern Analysis and Machine Intelligence, IEEE
Transactions on, vol. 16, no. 6, pp. 652‚Äì656, 1994.
[45] F. Neumann and C. Witt, Stochastic Search Algorithms. Berlin,
Heidelberg: Springer Berlin Heidelberg, 2010, pp. 21‚Äì32.
[46] B. S. Gulavani and S. K. Rajamani, ‚ÄúCounterexample Driven ReÔ¨Ånement
for Abstract Interpretation,‚Äù in Tools and Algorithms for the Construction
and Analysis of Systems, ser. Lecture Notes in Computer Science.Berlin, Heidelberg: Springer, Berlin, Heidelberg, Mar. 2006, pp. 474‚Äì
488.
[47] A. Bach, ‚ÄúBoltzmann‚Äôs probability distribution of 1877,‚Äù Archive for
History of Exact Sciences, vol. 41, no. 1, pp. 1‚Äì40, 1990.
[48] M. Allamanis, E. T. Barr, C. Bird, and C. Sutton, ‚ÄúSuggesting Accurate
Method and Class Names,‚Äù in Proceedings of the 2015 10th Joint
Meeting on Foundations of Software Engineering, ser. ESEC/FSE 2015.
New York, NY , USA: ACM, 2015, pp. 38‚Äì49.
[49] U. Koc, P. Saadatpanah, J. S. Foster, and A. A. Porter, ‚ÄúLearning a
classiÔ¨Åer for false positive error reports emitted by static code analysis
tools,‚Äù in Proceedings of the 1st ACM SIGPLAN International Workshop
on Machine Learning and Programming Languages, ser. MAPL 2017.
New York, NY , USA: ACM, 2017, pp. 35‚Äì42.
[50] U. Koc, S. Wei, J. S. Foster, M. Carpuat, and A. A. Porter, ‚ÄúAn empirical
assessment of machine learning approaches for triaging reports of a java
static analysis tool,‚Äù in 2019 12th IEEE Conference on Software Testing,
Validation and VeriÔ¨Åcation (ICST), IEEE. USA: IEEE Computer
Society, 2019, pp. 288‚Äì299.
[51] K. Heo, H. Oh, and K. Yi, ‚ÄúMachine-learning-guided Selectively
Unsound Static Analysis,‚Äù in Proceedings of the 39th International
Conference on Software Engineering, ser. ICSE ‚Äô17. Piscataway, NJ,
USA: IEEE Press, 2017, pp. 519‚Äì529.
[52] Z. S. Harris, ‚ÄúDistributional structure,‚Äù Word, vol. 10, no. 2-3, pp. 146‚Äì
162, 1954.
[53] L. Breiman, ‚ÄúRandom forests,‚Äù Machine learning, vol. 45, no. 1, pp.
5‚Äì32, 2001.
[54] M. Hall, E. Frank, G. Holmes, B. Pfahringer, P. Reutemann, and I. H.
Witten, ‚ÄúThe weka data mining software: an update,‚Äù ACM SIGKDD
explorations newsletter, vol. 11, no. 1, pp. 10‚Äì18, 2009.
[55] E. Frank, M. Hall, G. Holmes, R. Kirkby, B. Pfahringer, I. H. Witten,
and L. Trigg, Weka-A Machine Learning Workbench for Data Mining.
Boston, MA: Springer US, 2010, pp. 1269‚Äì1277.
[56] C. Lattner and V . Adve, ‚ÄúLlvm: A compilation framework for lifelong
program analysis & transformation,‚Äù in International Symposium on
Code Generation and Optimization, 2004. CGO 2004., IEEE. USA:
IEEE Computer Society, 2004, pp. 75‚Äì86.
[57] IBM, ‚ÄúT. J. Watson Libraries for Analysis (WALA),‚Äù http://wala.
sourceforge.net/, 2006.
[58] R. R. Picard and R. D. Cook, ‚ÄúCross-validation of regression models,‚Äù
Journal of the American Statistical Association , vol. 79, no. 387, pp.
575‚Äì583, 1984.
[59] J. Guo, D. Yang, N. Siegmund, S. Apel, A. Sarkar, P. Valov, K. Czar-
necki, A. Wasowski, and H. Yu, ‚ÄúData-efÔ¨Åcient performance learning for
conÔ¨Ågurable systems,‚Äù Empirical Software Engineering, vol. 23, no. 3,
pp. 1826‚Äì1867, 2018.
[60] N. Siegmund, A. Grebhahn, S. Apel, and C. K ¬®astner, ‚ÄúPerformance-
inÔ¨Çuence models for highly conÔ¨Ågurable systems,‚Äù in Proceedings of
the 2015 10th Joint Meeting on Foundations of Software Engineering,
2015, pp. 284‚Äì294.
[61] J. Guo, K. Czarnecki, S. Apel, N. Siegmund, and A. Wasowski,
‚ÄúVariability-aware performance prediction: A statistical learning ap-
proach,‚Äù in 2013 28th IEEE/ACM International Conference on Auto-
mated Software Engineering (ASE). IEEE, 2013, pp. 301‚Äì311.
[62] N. Siegmund, M. Rosenm ¬®uller, M. Kuhlemann, C. K ¬®astner, S. Apel,
and G. Saake, ‚ÄúSpl conqueror: Toward optimization of non-functional
properties in software product lines,‚Äù Software Quality Journal, vol. 20,
no. 3, pp. 487‚Äì517, 2012.
[63] H. Ha and H. Zhang, ‚ÄúDeepperf: Performance prediction for conÔ¨Ågurable
software with deep sparse neural ne twork,‚Äù in 2019 IEEE/ACM 41st
International Conference on Software Engineering (ICSE), 2019, pp.
1095‚Äì1106.
[64] J. Alves Pereira, M. Acher, H. Martin, and J.-M. J ¬¥ez¬¥equel, ‚ÄúSampling
effect on performance prediction of conÔ¨Ågurable systems: A case
study,‚Äù in Proceedings of the ACM/SPEC International Conference
on Performance Engineering, ser. ICPE ‚Äô20. New York, NY , USA:
Association for Computing Machinery, 2020, p. 277‚Äì288. [Online].
Available: https://doi.org/10.1145/3358960.3379137
[65] J. Oh, D. Batory, M. Myers, and N. Siegmund, ‚ÄúFinding near-
optimal conÔ¨Ågurations in product lines by random sampling,‚Äù in
Proceedings of the 2017 11th Joint Meeting on Foundations of
Software Engineering, ser. ESEC/FSE 2017. New York, NY , USA:
Association for Computing Machinery, 2017, p. 61‚Äì71. [Online].
Available: https://doi.org/10.1145/3106237.3106273
341[66] V . Nair, T. Menzies, N. Siegmund, and S. Apel, ‚ÄúFaster discovery
of faster system conÔ¨Ågurations with spectral learning,‚Äù Automated
Software Engg., vol. 25, no. 2, p. 247‚Äì277, Jun. 2018. [Online].
Available: https://doi.org/10.1007/s10515-017-0225-2
[67] V . Nair, Z. Yu, T. Menzies, N. Siegmund, and S. Apel, ‚ÄúFinding faster
conÔ¨Ågurations using Ô¨Çash,‚Äù IEEE Transactions on Software Engineering,
vol. 46, no. 7, pp. 794‚Äì811, 2020.
[68] V . Nair, T. Menzies, N. Siegmund, and S. Apel, ‚ÄúUsing bad learners
to Ô¨Ånd good conÔ¨Ågurations,‚Äù in Proceedings of the 2017 11th Joint
Meeting on Foundations of Software Engineering, ser. ESEC/FSE 2017.
New York, NY , USA: Association for Computing Machinery, 2017, p.
257‚Äì267. [Online]. Available: https://doi.org/10.1145/3106237.3106238
[69] H. Oh, H. Yang, and K. Yi, ‚ÄúLearning a strategy for adapting a program
analysis via bayesian optimisation,‚Äù in Proceedings of the 2015 ACM
SIGPLAN International Conference on Object-Oriented Programming,
Systems, Languages, and Applications, ser. OOPSLA 2015. New York,
NY , USA: ACM, 2015, pp. 572‚Äì588.
[70] A. E. I. Brownlee, O. Regnier-Coudert, J. A. W. McCall, and S. Massie,
‚ÄúUsing a markov network as a surrogate Ô¨Åtness function in a genetic
algorithm,‚Äù in IEEE Congress on Evolutionary Computation. Barcelona,
Spain: IEEE, 2010, pp. 1‚Äì8.
[71] Y . Jin and B. Sendhoff, ‚ÄúReducing Ô¨Åtness evaluations using clustering
techniques and neural network ensembles,‚Äù in Genetic and Evolutionary
Computation ‚Äì GECCO 2004, K. Deb, Ed. Berlin, Heidelberg: Springer
Berlin Heidelberg, 2004, pp. 688‚Äì699.
[72] H. K. Singh, T. Ray, and W. Smith, ‚ÄúSurrogate assisted simulated
annealing (sasa) for constrained multi-objective optimization,‚Äù in IEEE
Congress on Evolutionary Computation. Barcelona, Spain: IEEE, 2010,
pp. 1‚Äì8.
[73] F. Agakov, E. Bonilla, J. Cavazos, B. Franke, G. Fursin, M. F. P.
O‚ÄôBoyle, J. Thomson, M. Toussaint, and C. K. I. Williams, ‚ÄúUsing
machine learning to focus iterative optimization,‚Äù in Proceedings of the
International Symposium on Code Generation and Optimization, ser.
CGO ‚Äô06. USA: IEEE Computer Society, 2006, p. 295‚Äì305.
[74] J. Ansel, S. Kamil, K. Veeramachaneni, J. Ragan-Kelley, J. Bosboom,
U.-M. O‚ÄôReilly, and S. Amarasinghe, ‚ÄúOpentuner: An extensible frame-
work for program autotuning,‚Äù in Proceedings of the 23rd International
Conference on Parallel Architectures and Compilation, ser. PACT ‚Äô14.
New York, NY , USA: Association for Computing Machinery, 2014, p.
303‚Äì316.
[75] A. H. Ashouri, W. Killian, J. Cavazos, G. Palermo, and C. Silvano, ‚ÄúA
survey on compiler autotuning using machine learning,‚Äù ACM Comput.
Surv., vol. 51, no. 5, Sep. 2018.
342