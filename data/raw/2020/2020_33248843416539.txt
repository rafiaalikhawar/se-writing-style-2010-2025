MockSniffer: Characterizing and Recommending Mocking
Decisions for Unit Tests
Hengcheng Zhu‚àó
Southern University of Science and
Technology
Shenzhen, China
zhuhc2016@mail.sustech.edu.cnLili Wei‚àó
The Hong Kong University of Science
and Technology
Hong Kong, China
lweiae@cse.ust.hkMing Wen
Huazhong University of Science and
Technology
Wuhan, China
mwenaa@hust.edu.cn
Yepang Liu‚Ä†
Southern University of Science and
Technology
Shenzhen, China
liuyp1@sustech.edu.cnShing-Chi Cheung‚Ä†
The Hong Kong University of Science
and Technology
Hong Kong, China
scc@cse.ust.hkQin Sheng
WeBank Co Ltd
Shenzhen, China
entersheng@webank.com
Cui Zhou
WeBank Co Ltd
Shenzhen, China
cherryzzhou@webank.com
ABSTRACT
Inunittesting,mockingispopularlyusedtoeasetesteffort,reduce
test flakiness, and increase test coverage by replacing the actual
dependencies with simple implementations. However, there are no
clearcriteriatodeterminewhichdependenciesinaunittestshould
be mocked. Inappropriate mocking can have undesirable conse-
quences: under-mocking could result in the inability to isolate the
classundertest(CUT)fromitsdependencieswhileover-mocking
increases the developers‚Äô burden on maintaining the mocked ob-
jectsandmayleadtospurioustestfailures.Accordingtoexisting
work, various factors can determine whether a dependency should
be mocked. As a result, mocking decisions are often difficult to
makeinpractice.Studiesontheevolutionofmockedobjectsalso
showed that developers tend to change their mocking decisions:
17% of the studied mocked objects were introduced sometime af-
terthetestscriptswerecreatedandanother13%oftheoriginally
mocked objects eventually became unmocked. In this work, we
aremotivatedtodevelopanautomatedtechniquetomakemock-
ingrecommendationstofacilitateunittesting.Westudied10,846
testscriptsinfouractivelymaintainedopen-sourceprojectsthat
usemockedobjects,aimingtocharacterizethedependenciesthat
‚àóThisworkwasconductedwhenHengchengZhuwasavisitingstudentatHKUST(The
Hong Kong University of Science and Technology). The first two authors contributed
equally to this work.
‚Ä†Yepang Liu and Shing-Chi Cheung are corresponding authors.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation
onthe firstpage.Copyrights forcomponentsof thisworkowned byothersthan the
author(s)mustbehonored.Abstractingwithcreditispermitted.Tocopyotherwise,or
republish,topostonserversortoredistributetolists,requirespriorspecificpermission
and/or a fee. Request permissions from permissions@acm.org.
ASE ‚Äô20, September 21‚Äì25, 2020, Virtual Event, Australia
¬© 2020 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 978-1-4503-6768-4/20/09...$15.00
https://doi.org/10.1145/3324884.3416539are mocked in unit testing. Based on our observations on mock-
ing practices, we designed and implemented a tool, MockSniffer,
toidentifyandrecommendmocksforunittests.Thetoolisfully
automatedandrequiresonlytheCUTanditsdependenciesasin-
put. It leverages machine learning techniques to make mocking
recommendations by holistically considering multiple factors that
can affect developers‚Äô mocking decisions. Our evaluation of Mock-
Snifferon ten open-source projects showed that it outperformed
three baseline approaches, and achieved good performance in two
potential application scenarios.
CCS CONCEPTS
‚Ä¢Generalandreference ‚ÜíEmpiricalstudies ;‚Ä¢Softwareand
its engineering ‚ÜíSoftware maintenancetools ;Softwaretest-
ing and debugging .
KEYWORDS
Mocking, unit testing, recommendation system, dependencies
ACM Reference Format:
HengchengZhu, LiliWei, MingWen, YepangLiu, Shing-ChiCheung,Qin
Sheng,andCuiZhou.2020.MockSniffer:CharacterizingandRecommending
MockingDecisionsforUnitTests.In 35thIEEE/ACMInternationalConference
onAutomatedSoftwareEngineering(ASE‚Äô20),September21‚Äì25,2020,Virtual
Event, Australia. ACM, New York, NY, USA, 12 pages. https://doi.org/10.
1145/3324884.3416539
1 INTRODUCTION
Unittestinghasbeenwidelyadoptedtoassurethequalityofpro-
gramunits,namelyclasses,bytestingtheminisolation.Inpractice,
aclassundertest(CUT)iscommonlycoupledwithotherclassesin
aprogramoritsreferencedlibraries.Theseclassesarethedepen-
dencies of the CUT and often participate in its unit tests. Mocking
isadefactomechanismtoisolatetheCUTfromitsdependencies
in a test by simulating the behaviors of the dependencies using
4362020 35th IEEE/ACM International Conference on Automated Software Engineering (ASE)
mockedobjects[ 28].Itwasreportedthat23%oftheJavaprojects
with test scripts use mocking [32].
To conduct effective unit testing using mocking, developers first
needtomakemockingdecisions,i.e., decidingwhichdependencies
should be mocked. However, it is non-trivialto make proper mock-
ing decisions. A study showed that developers may change their
initial mocking decisions during development [ 36]. 17% of their
studied mocked objects were introduced sometime after the test
scripts were created. Another 13% of the originally mocked objects
eventuallybecameunmocked.Thissuggeststhattheoriginalmock-
ing decisions were later considered improper by the developers.
Makingpropermockingdecisionsischallengingbecause:(1)Mock-
ing decisions are unlikely to be made by considering only a single
factor. In practice, developers may need to consider multiple fac-
torsholisticallytomakeamockingdecision.(2)Mockingdecisions
are usually context-aware. Developers can make different mocking
decisions for the same dependency when testing different CUTs
according to the different usage scenarios of the dependency.
Inappropriatemocking decisionscan leadtoundesirable conse-
quences. On the one hand, dependencies that should be mockedcan be left unmocked in test scripts. Such under-mocking could
result in the inability to isolate the CUT from its dependencies,
which can seriously affect the efficiency of unit testing. The de-
velopers fixed this issue by mocking the real object [
1]. On the
otherhand,dependenciesthatdonotneedmockingcanbemocked
by developers. Such over-mocking increases developers‚Äô burden
on maintaining the mocked objects since they need to keep the
behaviorsofthemockedobjectsconsistentwiththerealimplemen-
tations.Inconsistenciesbetweenthemockedobjectsandthereal
implementationscancausespuriousfailuresintesting.Forexample,
in issue 16300 [ 5] of project Flink, the method getID() in mocked
ExecutionVertex returnsanullvalueandthuscausednullpointer
exceptions (NPEs) in test executions. However, in the real imple-
mentation of ExecutionVertex , the method getID() will never
return null.Inthiscase,thefailurecause dbyNPEsdoesnotreveal
arealbug.Developersreplacedthemocked ExecutionVertex with
a real one to fix this issue. The evolution of the project code can
furtherexacerbatetheproblemsincedevelopersneedtoupdatethe
mocked objects to catch up with the code evolution. If too many
dependenciesaremocked,itwouldbedifficultforthedevelopers
to update the mocked objects in time.
Given the challenges in making proper mocking decisions, stud-
ies were conducted to find out the factors that affect mocking deci-
sions. For example, Mostafa et al. [ 32] pointed out that production
classes are more frequently mocked than library classes. Spadini et
al. [36] categorized mockedobjects and found thatclasses dealing
withexternalresourcesareoftenmockedbydevelopers.Marriet
al.[31]revealedthatfilesystemAPIscanbemockedtofacilitate
unittesting. Thesestudies investigatedthe mockingpractices and
identifiedhigh-levelandintuitivefactorsthatcanaffectmocking
decisions.Inaddition,thesefactorsareallgenerictoCUTswithout
considering their interactions with the dependencies. With suchadvice, it is still difficult for developers to make proper mocking
decisionswhen writingtest scripts.Researchers havepointedout
the need for automated mock recommendation techniques [ 31,32].
Yet,noneoftheexistingworkhasproposedsuchatechnique.In
fact,thehigh-levelandproject-genericcharacteristicsofmockeddependenciesidentifiedbythesestudiescannoteffectivelyguide
the design of automated mock recommendation techniques.
This motivates us to conduct an empirical study to characterize
mocked dependencies at the code level by analyzing API usages,
data flows, control flows, etc. We aimed to identify those character-
istics that can be automatically extracted via code analysis so that
wecanleveragethemtobuildautomatedmockrecommendation
techniques. Inour empiricalstudy, weanalyzed 10,846test scripts
offourlarge-scaleopen-sourceprojects(suchasHadoop).When
conducting the empirical study, we not only studied the character-
istics of the dependencies themselves but also investigated their
interactions with the CUTs in different test cases. We made several
important observations that were not captured by existing studies.
Specifically,weidentifiedtencharacteristicsofmockedobjectsat
thecodelevel.Wefoundthatallofthetencharacteristicscanaffect
mocking decisions yet none of them is the sole determining fac-
tor. This provides evidence for the fact that mocking decisions are
madebyconsideringmultiplefactors,andthusweshallholistically
consider different factors to recommend mocking decisions. We
alsoobservedthatcontext-awarefactors,whichcapturetheinter-
actionsbetweendependencies andCUTs,arethe mostrelevantto
the mocking decisions. This indicates that an automated mocking
decision recommendation technique should be context-aware, i.e.,
considering the interactions between the dependencies and the CUTs.
Basedonourempiricalfindings,wefurtherproposedatechnique,
MockSniffer,torecommendmockingdecisionsinunittesting. Mock-
Sniffermakescontext-awarerecommendationsfordependencies
of CUTs in unit testing. It takes a CUT and its dependencies as
inputandoutputsarecommendedmockingdecisionforeachofthe
input dependencies. It also holistically combines various factors to
suggest mocking decisions by leveraging machine learning tech-
niques with features formulated from our empirical study findings.
MockSniffer learns the knowledge of making mocking decisions
from existing mocking practices andleveragesthe knowledge to
recommend future mocking decisions.
Inourevaluation,wetrainedandtested MockSniffer with546k
dataentriesofmockedandunmockeddependenciesextractedfrom
ten open-source projects. We compared the performance of Mock-
Snifferwith the generic mocking decision strategies adopted in
existingstudies.Ourresultsshowthat MockSniffer,whichperforms
context-aware mocking recommendations, can significantly out-perform the baseline methods as shown by the Mann-Whitney
U-Test[30].Wealso evaluated MockSniffer undertwo potentialap-
plication scenarios: (1) for mature projects, train MockSniffer with
dataextractedfromhistoricalreleasesofthesameprojecttoconduct
cross-versionmockingrecommendation,and(2)fornewprojects,
trainMockSniffer with data extracted from other projects to con-
duct cross-project mocking recommendation. Our evaluation re-
sults showed that MockSniffer achieved good performance in both
of these two application scenarios: it achieves an average F1-score
of 69.40% and 70.77% for the two application scenarios respectively.
To summarize, this paper makes three major contributions:
‚Ä¢We conducted an empirical study based on 10,846 test scripts of
fourlarge-scaleopen-sourceprojectsanddisclosedtencode-levelcharacteristicsofthemockingpracticesofreal-worlddevelopers.
We also validated our findings in a large-scale dataset consisting
4371public int countFiles() { // Production code
2 try{
3 return fileManager.scan().length;
4 }catch(IOException e) {
5 return 0;
6 }
7}
8
9public void test1() { // Test script
10 FileManager mgr = mock(FileManager.class);
11 when(mgr.scan()).thenReturn(new File[500]{});
12 UnderTest cut = new UnderTest(mgr);
13 assertEquals(500, cut.countFiles());
14}
1516public void test2() { // Test script
17 FileManager mgr = mock(FileManager.class);
18 when(mgr.scan()).thenThrow(new IOException());
19 UnderTest cut = new UnderTest(mgr);
20 assertEquals(0, cut.countFiles());
21}
Listing 1: Example Usage of Mocked Objects
of354kmockedandunmockeddependencies.Inourstudy,we
observedmockingdecisionsareaffectedholisticallybyvarious
factors, among which contextual features play an important role.
‚Ä¢Wedesignedandimplemented MockSniffer,thefirstautomated
techniquetorecommendmockingdecisionsforunittesting.Our
evaluationof MockSniffer onopen-sourceprojectsshowedthat
MockSniffer significantlyoutperformedthemockingstrategies
adoptedbyexistingstudiesandachievedgoodperformancein
two potential application scenarios.
‚Ä¢Inourstudy,wehavegeneratedalargelabeleddatasetconsisting
of546kdataentriesoftestcases,dependencies,andCUTs.We
releasedthisdatasetforpublicaccesstofacilitatefutureresearch
(https://doi.org/10.5281/zenodo.3783869).
2 BACKGROUND
In unit tests, mocked objects help decouple a CUT from its de-
pendencies. Mocked objects are usually created by leveraging a
mockingframework.Take test1() inListing1asanexample.The
production code at line 3 involves disk I/O. To save effort in set-
ting up the environment for testing, at line 11, developers create a
mocked FileManager objectusingMockito[ 8],apopularmocking
framework. Then, the mocked object mgrdirectly returns a File
arraywithoutaccessingthedisk(atline11).Thisalsospeedsuptest
executionsasdiskI/Ocanbeslow.Similarly,theproductioncode
at line 5 is not executed unless exceptions occur at line 3. To em-
ulatetheexceptionalscenarios,in test2(),developersconstruct
a mocked FileManager object and make it throw an exception
directly when the method scan()is invoked.
Apart from mocked objects created with mocking frameworks,
we also observed that developers can construct mocked objects by
creatingdummyclassesthatextendtheconcerneddependencies.
For example, in the test script of project HBase, developers created
a class KeyProviderForTesting , which is a subclass of the pro-
duction class KeyProvider . They mentioned in the document that
theclassistoreturnafixedsecretkeyfortesting.Instancesofsuch
classescreatedinthetestscriptsservethesamepurposeasthose
mocked objects created with mocking frameworks.
Mockinghasbeenwidelyusedinunittestgenerationtechniques.
For example, Arcuri et al. [ 13] enhanced EvoSuite [ 22]b yl e v e r -
aging mocking to increase code coverage and reduce flaky tests.Alshahwanetal.proposedAUTOMOCK[ 12]toimprovetheper-
formanceoftestcasegenerationbymockingtheenvironment.Till-mannetal.[
38]proposedasymbolic-execution-basedtechniqueto
generatemockedobjectsforunittesting.Althoughthesestudies
foundthatmockingcanfacilitatetestgeneration,theyalsoreported
that generated test cases with mocked objects can introduce spuri-
oustest failures(i.e.,falsealarms). Theunderlying reasonis that
there is no reliable mechanism to help decide which dependencies
to mock during test generation. When making mocking decisions,
these existing techniques have to resort to simple rules (e.g., all
databaseandfilesystemrelateddependenciesshouldbemocked).
Such mocking decisions contradict with the practices of real-world
developers,who oftenmock onlya smallportion ofdependencies
(e.g., file system related dependencies are not always mocked) [ 32],
and may result in substantial false alarms [ 15,34]. To reduce such
falsealarms,existingstudiesproposedseveralstrategies[ 12,15],
such as confining the values thatcan be returned by method calls
on mocked objects. Although these strategies can help reduce false
alarms, it would be better to mock only when necessary. In the fol-
lowing sections, we will study the mocking practices of developers
and figure out the factors that can affect mocking decisions.
3 DATA COLLECTION
Inordertounderstand themockingpracticesadoptedbydevelop-
ers, weconstructed a dataset byextracting CUTs, their dependen-
cies, and developers‚Äô mocking decisions from open-source projects.
This dataset will enable us to study whether developers share simi-
larpracticeswhenmakingmockingdecisions.Inthissection,we
present its construction process in detail.
3.1 Data Representation
Each entry in our dataset is a tuple: <ùëá,ùê∂ùëàùëá,ùê∑,ùêø >whereùëá
represents the test case, ùê∂ùëàùëárepresents the class under test, ùê∑
representsthedependency(aclassusedin ùëá,butnotùê∂ùëàùëá),andùêø‚àà
{ùëöùëúùëêùëòùëíùëë,ùë¢ùëõùëöùëúùëêùëòùëíùëë }isalabelthatrepresentswhether ùê∑ismocked
inthetestcase ùëá.Forexample, <TestCachingKeyProvider.test-
KeyVersion, CachingKeyProvider, KeyProvider, mocked> is
adataentryextractedfromHadoop[ 2].Itmeansthatdevelopers
mocked the dependency KeyProvider in the test case TestCachi-
ngKeyProvider.testKeyVersion forCUT CachingKeyProvider .
Such a data entry not only captures the developers‚Äô mocking de-
cisiononadependencybutalsolinksdependenciestoCUTs.We
include the links in our dataset because the mocking decisions for
the same dependency can vary across CUTs [36].
3.2 Subjects and Data Extraction
For constructing the dataset, we selected four actively-maintained
open-sourceprojects.Table1showstheinformationaboutthese
projects.TheseprojectsalluseMockito[ 8]ordefinedummyclasses
to construct mocked objects. As we can see from the table, theprojects are large-scale. In the following, we explain our data ex-
traction procedure.
Existing studies leveraged static analysis to identify mocked ob-
jectsintestcases.However,suchidentificationmaybeimprecise.
Forexample,developersofHadoopcreatedafactorymethod[ 6]to
create mocked objects of EventWriter . Depending on the value of
438Table 1: Selected Projects
Project Version Files LOC Data Entries Mocked
Hadoop 3.2.1 10,034 1.6M 325,335 14,771
Camel 3.1.0 18,245 1.3M 12,962 1,864HBase 2.2.3 3,976 738k 11,990 1,093Storm 2.1.0 2,354 282k 3,648 377
Total 34,609 3.9M 353,935 18,105
the field mockHistoryProcessing in the test class, the test cases
mayormaynotcreateamockedobjectof EventWriter .Itisdif-
ficultforstaticanalysistopreciselyinferwhethersuchobjectsin
each test case are mocked or not. In our work, to obtain a more
precise dataset, we leveraged dynamic analysis to identify mocked
objects and extract data entries. We explain the main ideas below.
For each test case ùëá, we first infer its class under test (i.e., the
ùê∂ùëàùëá),usingnamingheuristics.Accordingtocommonlyadopted
naming conventions, the name of a test class typically contains the
name of the CUT (e.g., TestMyClass is a test class for MyClass).
Hence,wecananalyzetheclassnameof ùëátoinferùê∂ùëàùëá.Next,to
analyze whether a dependency ùê∑is mocked in ùëá. In this paper,
we regard the non-CUT objects created during test case execution
andpassedtothe CUT directlyorindirectlyastestdependencies.
SuchobjectsareusuallypassedviamethodcallsontheCUTand
its dependencies. Therefore, we instrumented all method call sites
inùëáto log the exact type of each reference-type argument and
the type of the corresponding formal parameter. We observed that
mocked objects created with popular mocking frameworks (e.g.,
Mockito [ 8], EasyMock [ 3]) have special type names. For example,
when using Mockito [ 8], the type names of the mocked objects
areintheform Foo$MockitoMock$xxx ,where xxxisahashcode.1
Therefore,afterexecuting ùëá,wecananalyzetheloggedinformation
to infer whether an argument is a mocked object (i.e., determine
the labelùêø) via checking its type name and obtain the dependency
ùê∑, which is the type of the corresponding formal parameter.
As mentioned in Section 2, developers may construct mocked
objects by themselves rather than using a mocking framework. To
include such mocked objects in our dataset, we also considered
objects as mocked ones if they are instances of classes that (1) are
definedintestscripts,and(2)extendaclassintheproductioncode.
In this case, dependency ùê∑is identified as the production class
being extended (i.e., KeyProvider in the example in Section 2).
While the above approach may miss some test dependencies
(e.g.,thosespecifiedviaconfigurationfilesorassigneddirectlyto
apublicfield),ithelpeduscollect354kdataentriesfromthefour
open-sourceprojectsafterrunning50ktestcases.Suchcollected
data entries are already sufficient for our empirical study.
4 EMPIRICAL STUDY
To identify the factors that can affect the mocking decisions, we
conducted an empirical study on the projects based on the dataset
extracted(Section3).Weaimedtoderiveasetofrulestocapture
the characteristicsof themocked objects viaanalyzing theircode
1Wealsocheckedthepatterninothermockingframeworksinourimplementation.
We skip the details due to page limit.patterns. Such rules can further guide us to design automated tech-
niques to help developers make mocking decisions.
4.1 Setup
We adopted a two-stage scheme when conducting the empirical
study.Inthefirststage,wemanuallyinspectedasmallsubsetofthe
datasettodevisecode-levelcharacteristicsofthemockedobjects.In
the second stage, we formulated the code-level characteristics into
rules and conducted an automated validation of these rules with a
large-scale dataset, aiming to validate the derived characteristics.
Stage 1: Characteristics identification. Inthefirststage,we
manuallyinspected100dataentriesinthedatasettoidentifycharac-teristicsofthedependenciesthataremockedbydevelopers.Specifi-cally,werandomlysampled25dataentrieslabeledasmockedfrom
each of the four projects. For each sampled entry, we analyzed the
sourcecodeofthedependency,theCUT,andthetestscriptfrom
which the data entry is extracted. We inspected the data entries
using the open coding method [ 20] to identify common code-level
characteristics(e.g.,dataflow,controlflow,andAPIusage)ofthe
mocked dependencies and their interactions with the CUTs. Al-
thoughthesamplingsizeissmall,ouridentifiedcharacteristicscan
cover 94.82% (on average) of the mocking cases as shown in our
evaluation (see the recall of Baseline #3 in Table 5).
Stage2:Large-scalevalidation. Inthesecondstageoftheem-
pirical study, we further validated thecode-level characteristics of
mocked dependencies identified in stage 1 using the entire dataset.
Specifically,basedonthemanually-identifiedcharacteristics,we
formulatedseveralrulestoautomaticallyidentifythedataentries
that exhibit the characteristics. For each of the rules, we applied
ittoall354kdataentriesinourdatasettoobtainasubsetofdata
entries that match this rule. For each of the subsets, we computed
its mock ratio, i.e., the proportion of entries labeled as mocked. We
compared the mock ratio of each subset with the mock ratio of the
entire dataset, which is 5.1%. The larger the difference in the mock
ratio,themorelikelythecorrespondingcode-levelcharacteristic
can affect mocking decisions.
4.2 Results
Following the process described above, we made five observationsand formulated ten rules (i.e., code-level characteristics) in stage 1.
Inthefollowing,wewilldiscussourobservationsandtheformu-
lated rules. We will also present the mock ratio obtained in stage 2
for eachdata entrysubset thatmatches eachrule. Themock ratio
is presented in the brackets after each rule.
Observation 1: Classes related to environment or concur-
rency are often mocked. In 51 of the 100 manually-inspected
entries, the dependencies invoke APIs related to concurrency, net-
working,diskI/O,orAPIsprovidedbyonlineservices(e.g.,Amazon
AWS,MicrosoftAzure).TheseAPIscanbeslowtoexecuteorexhibit
inconsistent behaviors across different test runs. We formulated
the following rules based on this observation.
‚Ä¢Rule 1.1: Referencing environment-dependent or concur-
rentclasses(11.8%). ClassesmatchingthisrulecallAPIsrelated
totheenvironmentorconcurrency.Wemanuallybuiltalistof
suchAPIs(atclasslevel)inJDK,includingthoserelatedtonet-
working,diskI/O,concurrency,database,etc.Wefoundthatthese
439APIsarefrequentlyusedinmockedclassesbutinfrequentlyused
in unmocked ones. This rule matches the data entries where the
dependency references such APIs more frequently than aver-
ageofalldependenciesinourentiredataset.(Forsuccinctness,
weuse‚Äúmorethanaverage‚Äùtodescribewhereacertainmetric
computed with a subset is higher than the average value of that
metricinourentiredataset.)Here,weconsiderbothdirectand
transitivereferencestoincludethecaseswheretheseAPIsare
not called directly (i.e., called in the callees of the dependencies).
‚Ä¢Rule 1.2: Encapsulating external resources (7.4%). We also
considered classes that encapsulate external resources, which
usuallyimplementcertaininterfaces.Forexample,classesencap-
sulatingnetworkconnectionsusuallyimplementtheinterface
Closable . We manually built a list of such interfaces from the
classesencapsulatingexternalresourcesinourinspecteddataset.
This rule matches the data entries, in which the dependency
implements interfaces in the list.
‚Ä¢Rule1.3:Calling synchronized methods(13.8%). Classesper-
formingconcurrentoperationswouldusuallycall synchronized
methods. This rule matches the data entries where the depen-
dency calls more synchronized methods than average.
As the numbers in the brackets show, these rules produced subsets
ofdataentrieswithhighermockingratiosthanthatoftheentire
dataset(5.1%),indicatingthatdependenciesmatchingtheserules
are morelikely to bemocked. This is naturalbecause such depen-
dencies often complicate unit testing [ 36]. For example, to prepare
thetestenvironmentforaclassthatmakesnetworkrequests,de-
velopers need to set up a running server first. Besides, a class that
performs concurrentoperations mayproduce inconsistent results
acrossdifferenttestrunsduetoinherentnon-determinismofsched-uling.Suchinconsistencywouldmakethetestflaky[
27].Therefore,
developerswouldliketomocksuchclassestoavoidtheseproblems.
Observation2:Complicatedclassesareoftenmocked. We
foundthatin23ofthe100entries,thedependenciesarecomplicatedintermsofthenumberoffieldsorreferencedclasses.Taketheclass
Configuration in Hadoop as an example. It has 113 methods and
23 fields. Also, it imports 78 classes from other packages. To create
arealinstanceofsuchaclassinatest,developersneedtoinitializeallitsdependencies,manyofwhichmaynotberelevanttothetest.
Asaresult,developersmockedthisclassintestcases.Wedesigned
the following rules to find such complicated classes.
‚Ä¢Rule2.1:Excessivefields(8.8%). Classeswithlotsoffieldscan
representcomplexdatastructures.Suchclassesarenoteasytoset
upintestingandthusarelikelytobemocked.Thisrulematches
the data entries where the dependency has a larger number of
fields than the average of the whole dataset.
‚Ä¢Rule 2.2: A large number of dependencies (7.9%). Classes
withalargenumberofdependenciesarealsodifficulttosetupin
testingandarelikelymockedbydevelopers.Thisrulematchesthedataentrieswherethedependencyreferences(directlyand
transitively) more classes than average.
Thetworulesalsoproducedsubsetswithhighermockratios,which
shows that developers usually mock complicated classes.1// === Production code ===
2if(endpointConfig.isOverWrite()){
3 oStream.info.getFileSystemn().delete(...);
4}else {
5 throw new RuntimeCamelException(...);
6}
7
8// === Mock setup ===
9when(endpointConfig.isOverWrite()).thenReturn(false);
Listing 2: Example of Observation 4 in Project Camel
Observation3:Non-concretetypesareusuallymocked. We
observed that the dependencies in 43 of the 100 entries are non-
concrete types, which motivated us to design the following rule.
‚Ä¢Rule 3.1: Non-concrete types (8.3%). The rule matches data
entries where the dependency is an abstract class or an interface.
The rule increased the mocking ratioby 60.8% (from 5.1%to 8.2%),
indicating that developers often mock such non-concrete types
since they cannot be instantiated directly. Instead, developers have
to choose an implementation or create a mocked version.
Observation 4: Dependencies affecting the runtime con-
trol flows of methods in CUTs are often mocked. We found
that developers often mock a class and set different return val-
ues for its methods to cover different branches in CUT. Listing 2shows an example in the project Camel when testing the class
HDFSOutputStream .Here, endpointConfig isthedependency,and
thereturnvalueofitsmethod isOverWrite() isusedforabranch
conditionintheCUT.Developersmockedthedependency,stubbedthereturnvaluewith
false,andassertedthata RuntimeCamelExc-
eptionwouldbethrowninthetestscript,tocoverthebranchat
line 5. In stage 1, we found that 33 of the 100 entries have this
characteristic, which motivated us to design the following rules.
‚Ä¢Rule4.1:AffectingCUT‚Äôsruntimecontrolflowsviareturn
values(10.9%). Justliketheexampleabove,ifthereturnvalueis
usedforabranchcondition,developerscanmockthedependency,
provide different return values to cover different branches. This
rule matches a data entry if the return value of a method call on
the dependency is used in branch conditions in the CUT.
‚Ä¢Rule4.2:AffectingCUT‚Äôsruntimecontrolflowsviaexcep-
tions (12.6%). Similar to the case in Rule 4.1, developers can
mock a dependency to throw an exception to test the exception
handler. This rule matches a data entry if the CUT catches an
exception thrown by the dependency.
Theserules produced subsetswithmuchhigher mockratios,indi-
cating that classes affecting the runtime control flows of methods
in CUTs are frequently mocked.
Observation 5: Dependencies capturing the internal be-
haviorsoftheCUTsareoftenmocked. Theverificationfeature
of mocking frameworks is widely used by developers. Developers
often test whether the CUT is implemented correctly by enforcing
assertions on the method invocations on the dependencies. List-
ing3showsanexampleintheprojectStormwhentestingtheclass
RocketMqBolt .IntheCUT,themethodinvocationto send()onthe
dependency producer isdominatedbytwo ifconditions.Develop-
ers mocked the dependency and asserted that the method is called,
to test whether the branch conditions in the method execute()
(line 1 and 4) were designed correctly. We found that 13 of the 100
entrieshavethischaracteristicandwedesignedthefollowingrules:
4401if(batch) { // Production code in method execute()
2 //...
3}else {
4 if(async)
5 producer.send(prepareMessage(input), ...);
6}
7
8// Test script
9rocketMqBolt.execute(tuple);
10verify(producer).send(any(Message.class), ...);
Listing 3: Example of Observation 5 in Project Storm
‚Ä¢Rule 5.1: Conditional invocation (12.5%). Similar to the ex-
ample, dependencies whose methods are called conditionally are
often mocked. Therefore, this rule matches a data entry if any
method invocation to the dependency is dominated by a branch
condition or an exception handler in the CFG of the CUT.
‚Ä¢Rule5.2:Capturingintermediateresults(11.4%). Arguments
used to call a method of the dependency are usually the inter-
mediate results produced by the CUT, which can be differentbased on different inputs for the CUT. In practice, developersmaymockadependencyandcapturetheargumentspassedto
itsmethods.Bycheckingthecapturedargumentvalues,devel-
operscantestwhethertheintermediateresultsarecorrect,and
thus validate the implementation of the CUT. This rule matches
a data entry if an argument at a call site to the dependency is
data-dependent [11] on the parameters of a CUT method.
These two rules also produced subsets with much higher mock ra-
tios, indicating that dependencies capturing the internal behaviors
of the CUTs are often mocked.
Table2aggregatesthemockratiosinthesubsetsproducedby
applying each of the ten rules. As we can see from the table, allthe mock ratios of the subsets are higher than that of the wholedataset. This indicates that all the observations we made in our
manualinspectionaregeneralizableandouridentifiedcode-level
rules can characterize mocked dependencies. In particular, there
arefivesubsetswhosemockratiosaremorethan100%higherthan
thatofthewholedataset.Wefoundthattherulesthatwereused
to extract them can be divided into two groups:
(1)APIusages. Rule 1.1 and Rule 1.3 model the API usages of the
dependencies, which reflect the behaviors of the dependencies.
(2)Interactions. Rule4.1,Rule4.2,Rule5.1,andRule5.2modelthe
interactionsbetweentheCUTandthedependenciesshowing
how CUTs can affect mocking decisions.
Thehighmockratiosindicatethatthesetwotypesoffactorsare
more likely to affect mocking decisions.
5 MOCKSNIFFER
Our empirical study yielded five observations with ten code pat-
terns of the mocked dependencies. These findings also showed
thatmockingpracticesadoptedbydeveloperssharesomecommon
characteristics, which can be leveraged to guide future mocking
decisions.Therefore,based ontheempiricalfindings,wepropose
MockSniffer,afullyautomatedtechniquethatrecommendsmockingdecisionsfordevelopersbylearningfromexistingpractices.Specif-
ically,MockSniffer takes the CUT and its dependencies as input
and suggests a binary mocking decision for each of the dependen-
cies.Webuilt MockSniffer basedonbinaryclassificationmachineTable 2: Mock Ratios by Applying the Rules
Rule Matches Mocked Ratio Comparison
Rule 1.1 49,789 5,874 11.8% +131.4%
Rule 1.2 5,669 419 7.4% +45.1%Rule 1.3 33,356 4,611 13.8% +170.6%Rule 2.1 86,469 7,620 8.8% +72.5%Rule 2.2 104,416 8,204 7.9% +54.9%Rule 3.1 122,471 10,197 8.3% +62.7%Rule 4.1 74,510 8,119 10.9% +113.7%Rule 4.2 60,193 7,565 12.6% +147.1%Rule 5.1 64,488 8,089 12.5% +145.1%Rule 5.2 56,080 6,380 11.4% +123.5%
Dataset 353,935 18,105 5.1%
learningtechniques.Themechanismofbinaryclassificationisin
linewiththenatureofourtargetedproblem:providingthechar-
acteristics of the dependency and the CUT, to predict whether the
dependency should be mocked. We implemented MockSniffer on
various machine learning models and trained them with the mock-
ingpracticesextractedfromexistingtestcases. MockSniffer canthus
recommend mocking decisions by learning from existing practices.
5.1 Feature Engineering
Table3shows the16featuresusedinthe machinelearningmodel
ofMockSniffer. These features are either designed based on our
observations in Section 4.2 or adopted from the empirical findings
in existing studies [ 32,36] that can be directly applied to the code.
They fall into four categories.
Contextualinformation. ThisincludesRBFA,EXPCAT,COND-
CALL,AFPR,andCALLSITES.Thefirstfourarederiveddirectly
fromObservation4and5inSection4.2.Thesefeaturescapturethe
interactions between the CUT and the dependency by matching
certain patterns. However, there would be some scattered patterns
that lead to mock but not observed by us since each single of them
doesn‚Äôtappearfrequently.Asacomplement,wedesignedthefea-
ture CALLSITES, which counts the call sites to the dependency
intheCUT,withtheheuristicthatsuchscatteredpatternswould
appear when there are abundant call sites.
APIusage. UAPI,TUAPI,UINT,andSYNCmodeltheAPIusage
ofthedependencyandtherebyreflectitsbehaviors(i.e.,whether
it is related to the environment or concurrency). They are derived
from Rule 1.1 to 1.3. We split Rule 1.1 into UAPI and TUAPI, which
countsthedirectandtransitivereferencestotheAPIsinRule1.1.
OurheuristicisthatdependenciesreferringtosuchAPIsdirectly
and transitively may have different chances to be mocked.
Complexity. DEP, TDEP, and FIELD measure the complexity
ofthe dependencyand thusfallinto thiscategory. Thesefeatures
count the direct and indirect references to other classes, as well as
the fields of the dependency. While FIELD is derived directly from
Rule 2.1, we split Rule 2.2 into DEP and TDEP with the heuristic
that direct and indirect dependencies can affect mocking decisions
to a different extent.
Class meta-properties. ABS, INT, ICB, and JDK are related to
themeta-propertiesofthedependencyitself.Theyareadaptedfrom
Rule 3.1 and existing studies [32, 36].
For features derived from the rules under each observation, we
removed the thresholds and use numerical values since we want
441theclassifiertolearnthethresholds.Byusingthesefeatures, Mock-
Sniffercan holistically consider multiple factors and make wise
mocking decisions.
5.2 Classification Model
Feature extraction. To extract features from training or testing
datasets,webuiltstaticanalyzersforeachfeatureontopofSoot[ 40].
EachstaticanalyzertakesaspecifiedCUT,dependency,andthebyte
codeofthewholeprojectasinputandoutputthecorresponding
value of the feature. Table 3 shows the descriptions of our features.
For indicator features like ABS, INT, and JDK, the analyzers can be
implementedbyloadingthedependencyandcheckthecorrespond-
ingproperties.Forexample,theanalyzerofABSsimplyloadsthe
dependency and checks whether it is an abstract class. The analyz-
ers of EXPCAT, CONDCALL, and those numerical features can be
implementedbysearchinginthecallgraphbystartingfromthetest
method (i.e., the test case ùëá) and counts the occurrences of certain
patternsinthecode.TakeEXPPCATasanexample,theanalyzer
traversesthecallgraphfromthetestmethodandcountsthecall
sites to the dependencies that are wrapped by a try . . . catch
clause in the methods of the CUT.
Standardization. Trainingwithastandardizeddatasetcanachieve
better performance on many classifiers. Data standardization is ap-
plied independently on each numerical features. Typically, data
standardizationsubtractsthemeanvalueandscalesthevaluesto
unit variance. However, our dataset contains many outliers, which
may bias themean value and variance calculation.To address this
problem,wesubtractedthemedianfromthedatasetandthenscale
it according to the interquartile range [39].
Under-sampling. As shown in Table 1, the datasets of mocked
andunmockeddependenciesareoftenimbalanced.Theentriesla-
beledasmockedonlyaccountforaverysmallportionofthewhole
dataset(e.g.,only5.1%inTable1).Trainingclassificationmodels
directlywith suchimbalanceddatasetscan inducebiasinthe pre-
dictionresultsandresultinpoorpredictionperformance[ 41].To
address this issue, we adopted the random under-sampling tech-
niquetoobtainabalanceddataset,whichissimplebutperformsno
worse than other under-sampling techniques [ 29]. Specifically, we
randomlyremovedtheinstanceslabeledasunmocked,whichac-
count for the majority, until the dataset contains the same number
of mocked and unmocked data entries.
Model training. MockSniffer can be implemented on the top
of various classification models and different models may achievedifferentperformance.Wecomparedtheperformanceofdifferent
modelswiththedataextractedfromprojectsinTable1.Wetrained
classification models and performed 10-fold cross-validation using
Decision Tree [ 19], Naive Bayes [ 42], Ada Boosting [ 23], Gradi-
ent Boosting [ 24], Random Forest [ 18], and Support Vector Ma-
chine [17]. When training with Random Forest and Decision Tree,
we set the maximum tree depth to sqrtto prevent overfitting.
Forothermodels,weusedthedefaultsettingsprovidedbyscikit-
learn [33]. As shown in Figure 1, the accuracies of these models
rangefrom63.33%to78.81%,whereGradientBoostingisthebest.
Therefore,weusedGradientBoostingasthedefaultclassifier.Since
there is no ground truth on whether a mocking decision is correct,
we used the mocking decisions made by developers in our datasetFigure 1: Accuracy of Classification Models
as target variables when training. We expect that the number of in-correctmockingdecisionsmadebydevelopersissmall.Thechance
that such incorrect decisions significantly bias the model is low.
6 EVALUATION
Inthissection,wepresentourevaluationof MockSniffer.Weaimed
to explore the following research questions:
‚Ä¢RQ1(Theeffectivenessof MockSniffer ):Can MockSniffer ef-
fectively recommend mocking decisions and outperform exiting
mockingstrategies?Doesmachinelearninghelpmakebettermock-
ing decisions?
‚Ä¢RQ2 (The effectiveness of a single feature): Whichfeatures
are the most relevant to mocking decisions? Can MockSniffer effec-
tively make mocking decisions based on every single feature?
‚Ä¢RQ3(Potentialapplicationscenarios): Whatarethepotential
application scenarios of MockSniffer? How does MockSniffer per-
form in these scenarios?
We conducted three studies to answer each of these research
questions.Wecomparedtheperformanceof MockSniffer withthree
baselines,investigatedtheeffectivenessofeachfeature,andeval-
uatedMockSniffer in two potential application scenarios. In our
experiments,weusedthemockingdecisionsmadebydevelopersas
the ground truth. Since MockSniffer performs binary classification
to recommend mocking decisions, we adopted the metrics widely
used for evaluating binary classifiers: accuracy, precision, recall,
and F1-score to evaluate and compare the results of MockSniffer.
6.1 Evaluation Subjects
Ourevaluationof MockSniffer isbasedonthefourprojectslisted
in Table 1. To further evaluate whetherour findings on these four
projects can be generalized to other projects, we selected six more
large-scale,activelymaintainedprojects,wheremockingisoften
usedintheirtestcases.Table4liststheseprojects.Themethodology
tocollectthesixprojectsisthesameasthatadoptedinSection3.
Finally,wecombinedthesixprojectswiththeprojectsinTable1and
formed a set of ten projects for evaluation. We further extracted
data entries for the additional subjects by adopting the processdescribed in Section 3 and then extracted features for each data
entryleveragingthemethodologydescribedinSection5.1.After
thesesteps,wegotanevaluationdatasetcontaining546kentries,
where 31k of them are labeled as mocked.
6.2 Baselines
Sincetherearenoexistingautomatedmockingdecisiontechniques,
wecompared MockSniffer withthreebaselinesderivedfromexisting
studies, mocking strategies adopted by existing test generation
techniques, and our empirical findings.
442Table 3: Features Used by MockSniffer
Feature Source Description
UAPI
Observation 1# Direct references to the APIs in the list mentioned in Rule 1.1
TUAPI # Transitive references to the APIs in the list mentioned in Rule 1.1
UINT Indicates whether the dependency implements any interfaces in the list mentioned in Rule 1.2
SYNC # Call sites to synchronized methods by the dependency
DEP
Observation 2# Classes referenced by the dependency directly
TDEP # Classes referenced by the dependency transitively
FIELD # Fields in the dependency
ABSObservation 3Indicates whether the dependency is an abstract class
INT Indicates whether the dependency is an interface
RBFAObservation 4# Call sites to the dependency whose return value is used for branch conditions in CUT
EXPCAT # Call sites to the dependency that are surrounded by a try. . . catch structure in the CUT
CONDCALLObservation 5# Call sites to the dependency in the CUT that are dominated by a branch condition or an exception handler
AFPR # Call sites in the CUT whose arguments have data dependencies on parameters
CALLSITES Observation 4 and 5 # Call sites to the dependency in the CUT
ICB Existing study [32] Indicated whether the dependency is defined in the production code
JDK Existing study [36] Indicates whether the dependency is a type provided by the JDK
Table 4: Extra Projects for Evaluation
Project Version Files LOC Data Entries Mocked
Flink 1.10.0 8,828 899k 86,141 6,719Hive 3.1.2 5,990 1.3M 23,368 1,490CXF 3.3.5 7,293 672k 22,550 1,489Druid 0.17.0 4,556 596k 45,332 1,869Dubbo 2.7.6 2,210 166k 8,623 758Oozie 5.2.0 1,388 191k 5,539 278
Total 30,125 3.8M 191,553 12,603
Baseline #1: Existingheuristics. Existing studies yielded em-
pirical findings on the usage of mocking. We constructed a rule-
based approach from the empirical findings that can be directly
applied to the code. Specifically, it consists of three rules:
(1)ItdoesnotmockJDKclassessincetheyareoftennotmocked[ 36].
(2)It mocks all the interfaces since developers prefer mocking
interfaces over using their implementations [36].
(3)It mocksall the classesin thecodebase since developersmock
more classes in the codebase than in the libraries [32].
Specifically, Baseline #1 applies the rules as ordered above, and
recommendstomockadependencyifitmatchesanyoftheserules.
Ifnoneoftherulesmatches,itsuggestsnottomockthedependency.
Baseline #2: EvoSuite mock list. EvoSuite [ 22] contains a
list [4] of dependencies to mock. As such, we built a simple rule-
basedstrategybymockingallthedependenciesinthelist.Thisisa
conservativebaselinesinceitonlyconsidersalimitednumberof
JDK classes as dependencies to mock.
Baseline #3: Empirical rules. Our empirical study distilled
five observations with ten rules to guide the mocking decisions.
These code-level rules can be detected automatically. We designed
anaggressivebaseline,whichwoulddecidetomockifanyofthe
rules in Section 4.2 match.
6.3 RQ1: The Effectiveness of MockSniffer
Experimentsetup. Toevaluatetheeffectivenessof MockSniffer,
we performed intra-project prediction on the ten projects. We split
the entries labeled as mocked into ten folds, chose one fold for
    
 	

 


Figure 2: Cross Validation with Sampling
testing, and the others for training. Figure 2 illustrates the data
preparationprocess.Bythismechanism,eachoftheentrieslabeled
asmockedappearedonceinthetestset,whichmakestheperfor-
mancescoremoreobjective.Asmentionedbefore,weperformed
under-samplingfortheunmockedentriesinthedataset.Tomin-imize the influence on the credibility of the performance scores,
werepeatedthesamplingprocessfor100timesandreportedthe
average performance scores of 1,000 (10 folds √ó100 samplings)
runs.Afterthat,weappliedthethreebaselinesontheevaluation
dataset and compared their performance with MockSniffer.
Results. AsshowninTable5,onaverage, MockSniffer correctly
distinguished85.42%ofthemockingdecisions.Fortheinstancesit predicted as mocked, 83.95% of them are true positives, which
covers87.89%oftheinstancelabeledasmocked. MockSniffer out-
performed the three baselines by large margins on the ten projects
intermsofaccuracy,precision,andF1-score.Wealsoperformedthe Mann-Whitney U-Test [
30] to evaluate the result differences.
The p-values ranged from 0 .00009 to 0 .0086, which showed that
MockSniffer outperformed the baselines significantly ( ùëù<0.01).
MockSniffer performed better than the first two baselines due
tothefollowingreasons.First, MockSniffer takesintoaccountthe
contextual information, namely the interactions between the CUT
andthedependencywhileneitherofthebaselinescapturessuchin-
formation. Therefore, it can distinguish between different mocking
decisions on the same dependency with different CUTs. Besides,MockSniffer can cover the scenarios where the dependency does
not match the rules in the first two baselines but are mocked for
otherreasons (e.g.,mock toincreasethe branchcoverage justlike
443Table 5: Performance of MockSniffer and the Baselines
Metric (%) Accuracy Precision Recall F1-Score
Approach*MSB#1 B#2 B#3 MSB#1 B#2 B#3 MSB#1 B#2 B#3 MSB#1 B#2 B#3
Hadoop 82.90 64.46 48.71 63.30 80.36 60.01 13.93 57.86 87.09 86.71 0.50 97.92 83.59 70.93 0.96 72.74
Flink 82.92 68.58 48.90 54.64 81.32 62.54 8.89 52.46 85.51 92.68 0.24 98.88 83.35 74.68 0.46 68.55Hive 84.36 58.52 49.90 63.94 84.46 58.52 47.06 58.82 84.32 58.52 1.61 92.95 84.33 58.52 3.11 72.05Camel 79.52 64.91 48.52 53.40 76.19 62.28 6.35 51.80 86.12 75.58 0.22 97.74 80.80 68.29 0.42 67.71CXF 83.05 59.37 48.82 54.73 84.67 59.48 15.69 52.60 80.83 58.83 0.54 95.84 82.65 59.15 1.04 67.92Druid 83.41 64.50 49.06 52.70 83.53 66.75 2.70 51.50 83.38 57.78 0.05 92.78 83.39 61.94 0.10 66.23HBase 86.58 75.28 49.30 54.61 84.39 68.87 20.00 52.47 89.89 92.26 0.47 98.03 87.00 78.87 0.91 68.35Dubbo 88.55 65.45 49.14 54.55 85.75 69.07 23.96 52.49 92.71 55.94 0.79 95.78 89.03 61.81 1.53 67.82Oozie 91.14 64.24 46.58 52.34 91.12 61.35 0.00 51.40 91.57 76.98 0.00 85.61 91.17 68.28 0.00 64.24Storm 91.77 52.74 49.32 52.83 87.75 53.12 0.00 51.58 97.48 46.61 0.00 92.68 92.27 49.65 0.00 66.27
Average 85.42 63.81 48.82 55.70 83.95 62.20 13.86 53.30 87.89 70.19 0.44 94.82 85.76 65.21 0.85 68.19
* MS, B#1, B#2, and B#3 stands for MockSniffer and the three baselines respectively.
the example in Listing 2). Second, MockSniffer considers the be-
havior of the dependencies by analyzing their invoked methods.
Baseline#2mockedalltheclassesintheEvoSuitemocklist,which
contains the classes that are related to the environment or con-
currency. However, the list contains only classes in JDK. Non-JDK
classesinvokingAPIsrelatedtoenvironmentorconcurrencyare
not included. As a result, Baseline #2 suffered from a low recall. In
comparison, MockSniffer can identify such cases by also analyzing
themethodsinvokedbythedependenciesandidentifyingtransitive
references to APIs related to the environment or concurrency.
Baseline #3 outperformed MockSniffer in terms of recall. This
is because baseline #3 decided to mock the dependency when any
of the rules in Section 4.2 match. Although such an aggressive
approach can identify most of the mocked instances (94.82% recall
on average), it would mispredict the dependencies that should not
bemockedasmocked,andthussufferedfromlowprecision(53.30%
on average). The comparison between MockSniffer and Baseline #3
showsthatitisnotenoughtosimplyapplytherulesinSection4.2to
recommend mocking decisions. To make better mocking decisions,
we need to combine these rules holistically.
Answer to RQ1: MockSniffer outperformedexistingmocking
strategies. Machine learning can help make better mocking
decisions.
6.4 RQ2: The Effectiveness of a Single Feature
MockSniffer uses multiple features together to make mocking deci-
sions. As such, we want to investigate how relevant is each feature
to mocking decisions, as well as the effectiveness of MockSniffer by
using only that feature.
Experimentsetup. Theexperimentconsistedoftwosteps.First,
we studiedthe relevance of eachfeature to themocking decisions
by performing the Chi-squared test [ 25]. Then, we ran modified
versions of MockSniffer trained with each single feature to investi-
gate how each feature contributes to the performance of MockSnif-
ferwith a process similar to that in Section 6.3.
Results. Table6showstheresultsoftheChi-squaredtest.For
most of the features, the Chi-squared statistic values are larger
than100,indicatingthatthesefeaturesarerelevanttothemocking
decisions. Also, features describing the interactions between the
CUTandthedependency(e.g.,EXPCAT,CONDCALL)andfeaturesTable6:Chi2StatisticsandPerformanceofIntra-projectPre-
diction Using Single Feature
Metric (%) Chi2Accuracy Precision Recall F1-
Score
ABS 2394.05 60.56 59.71 68.82 62.80
AFPR 4835.57 56.76 63.99 39.54 43.31
CALLSITES 18996.25 57.72 63.94 49.31 49.11CONDCALL 20728.24 58.53 64.55 45.56 49.15DEP 1490.05 71.31 70.90 73.43 71.57
EXPCAT 30968.33 58.42 70.40 34.27 41.84FIELD 8.46 67.77 64.56 80.74 71.19
ICB 1281.75 60.14 62.34 55.70 56.57
INT 2506.90 59.05 61.41 55.13 56.14
JDK 3103.92 59.68 55.72 96.30 70.52
RBFA 3455.96 58.80 65.39 47.27 50.47SYNC 18120.28 54.17 72.80 40.33 36.62TDEP 2349.46 68.91 70.69 65.67 67.37TUAPI
117270.05 65.73 71.53 66.33 63.41
UAPI 1035.86 62.92 73.84 52.66 55.36UINT 60.01 51.15 59.41 61.71 43.68
Average 60.73 65.70 58.30 55.57
reflecting the API usages of the dependency (e.g., TUAPI, SYNC)
achievedthehighestChi-squaredstatisticvalues,indicatingthat
these two factors are more relevant to mocking decisions.
Inaddition,Table6presentstheaverageperformanceof Mock-
Snifferonthetenprojectsbyusingeachofthefeatures.Byusing
each of the features, MockSniffer achieved an average accuracy,
precision,recall,andF1-scoreof60.73%,65.70%,59.30%,and55.57%,
respectively.Theperformanceismuchlowerthanthatusingallthefeatures,whichshowsthat,bytakingmultiplefactorsintoaccount,
MockSniffer can suggest better mocking decision than considering
a single one. This is because there is more than one factor that
drives developers to mock, but each of these features took just one
of the factors into account. Instead, MockSniffer combines these
scatteredfactorstogetherandconsidersthemholistically,andthus,
can suggest better mocking decisions.
AnswertoRQ2: Context-awareandAPIusagerelatedfeatures
arethemostrelevanttomockingdecisions,butonlyconsidering
each single of them is not enough.
444Table 7: Performance of MockSniffer in Potential Applica-
tion Scenarios
Metric (%) Accuracy Precision Recall F1-Score
Settings*CVPCPPCVPCPPCVPCPPCVPCPP
Hadoop 75.20 75.84 75.08 73.06 73.74 79.46 74.68 76.13
Flink 79.79 74.12 75.39 69.76 78.95 89.64 79.37 78.46
Hive 69.57 70.41 65.90 74.60 60.96 48.21 65.50 58.57
Camel 72.33 66.72 67.11 66.51 73.31 68.94 72.49 67.70
CXF 61.59 71.84 71.56 73.46 40.00 67.55 51.01 70.37
Druid 68.75 66.39 66.50 68.37 56.90 61.43 64.33 64.71
HBase 83.82 77.11 76.42 75.00 87.02 79.26 84.25 77.07
Dubbo 68.33 71.24 66.68 72.22 44.76 54.32 56.93 61.99
Oozie 64.11 69.74 73.92 80.72 30.67 62.93 44.61 70.68
Storm 73.29 70.27 71.16 72.86 60.02 67.48 66.99 70.06
Average 71.68 70.97 78.82 72.65 60.63 67.92 66.02 69.57
* CVP and CPP stands for cross-version and cross-project prediction, respectively.
6.5 RQ3: Potential Application Scenarios
The ultimate goal of MockSniffer is to recommend mocking de-
cisions in real-world projects during software development. As
such,wedesignedexperimentstoinvestigatetheeffectivenessof
MockSniffer in potential application scenarios.
Experiment setup. When using MockSniffer to recommend
mocking decisions on a project, there are two potential scenarios.
(1)Cross-version prediction (CVP). For mature projects, devel-
opers can train MockSniffer with the data extracted from the
historicalreleasesoftheproject,andrecommendmockingdeci-
sions for the new test cases in subsequent releases. To evaluate
MockSniffer inthisscenario,wecollectedfiveconsecutivere-
leasesofthetensubjectprojects.Foreachrelease,weextracted
data entries and their corresponding feature leveraging the
sameprocessasthatdescribedinSection3and5.Afterthat,we
performed incremental predictions on each of the projects. For
each release, we trained MockSniffer with the dataset extracted
from its prior releases, and predict on those newly added in-
stancesinthecurrentrelease(i.e.,dataentriesthatarenotin
the dataset extracted from prior releases). Since the number of
newly added instances between each pair of releases varies, we
reported the weighted average of the performance scores:
Score =/summationdisplay.1
ùëñScore ùëñ√ó#new instances ùëñ//summationdisplay.1
ùëñ#new instances ùëñ
in which ùëñrefers to the ùëñthrelease of the project used in the
experiment.
(2)Cross-projectprediction (CPP). Fornewprojects,theirhis-
toricalmockingpracticesareinsufficienttotrain MockSniffer.
As such, developers can train MockSniffer with the data ex-
tracted from other projects (e.g., the vast open-source code
repositories). To evaluate MockSniffer in this scenario, with the
evaluation dataset, we picked the data entries from one project
for testing and used the remaining for training. We repeated
this procedure on each of the ten projects.
As mentioned in Section 5, we performed under-sampling when
preparingthedataset,tomaintainthereliabilityoftheperformance
scores.Werepeatedthesamplingprocessfor100timesandreported
the average scores of the 100 runs.
Results. Table7showstheperformancescoresof MockSniffer in
thetwoscenarios.Theaccuracyachievedby MockSniffer rangedfrom 61.59% to 83.82%, and on average, it made 71.68% and 70.97%
correctdecisionsunderthetwosettings,respectively.Theprecision
in cross-version prediction (78.82% on average) is higher than that
incross-projectprediction(72.65%).Thisisbecausethemocking
decisions within the same project share more similarities. Existing
mockingpracticesinthehistoricalreleasesofaprojectarelikely
to be adopted by the test cases in its subsequent releases.
However, the recall of cross-version prediction (60.63%) is lower
thanthatincross-projectprediction(67.92%).Thisisbecausede-
velopers may adopt new mocking practices that do not exist in the
currentproject,butsuchpracticesmaybeadoptedbyotherprojects.
In this case, cross-project prediction can transfer the knowledge
from one project to another, and thus help developers make proper
mocking decisions. For example, MockSniffer suffered from ex-
tremely low recall when performing cross-version prediction on
theprojectOozie(30.67%).Wemanuallyinspectedtheprediction
process and found that low recall happened when predicting on
Oozie5.0bylearningfromOozie4.2and4.3.Thereasonisthatsig-
nificant changes took place from Oozie 4.x to 5.x, for example, the
workflow graph generator was completely rewritten and the Oozie
launcherwasmovedfromMapReduceMappertoYARNapplicationmaster[
21].Developersalsoadoptednewmockingpracticesinthe
newer versions. Due to such great changes, the knowledge learned
from Oozie 4.x could not capture mocked object usages in Oozie
5.x Inthis case, cross-projectcan help increasethe recall bylever-
agingtheknowledgelearnedfromotherprojects.Specifically,by
performing cross-projectprediction, MockSniffer achieved arecall
of 62.93% on project Oozie, which is 105.18% higher than that in
cross-version prediction.
Answer to RQ3: MockSniffer can be applied in two application
scenarios (cross-version prediction and cross-project prediction).
In these application scenarios, MockSniffer can effectively rec-
ommendpropermockingdecisionsbylearningfromhistorical
mocking practices or mocking practices of other projects.
7 DISCUSSION
7.1 Threats to Validity
In the data collection process, we inferred the name of CUTs from
the class name of the test cases according to a widely adopted
naming convention. We may fail to identify CUTs for those testcases that do not follow such a naming convention. In addition,
wefocusedonmethodargumentswhenextractingdependencies.
This may miss some dependencies that are not method arguments(e.g., those specified via test configuration files and those assigned
directly to a public field). Although our dataset does not include allthedependenciesusedinunittests,itislargeenoughforourstudy.
The selected subject projects use only two mocking frameworks
Mockito[
8]andEasyMock[ 3].Therearealsoothermockingframe-
workssuchasjMock[ 7].Sincetherearedifferencesinthefunction-
alitiesprovidedbytheseframeworks,developersmayhaveslightly
different practices when using them. Such differences could not
be covered by our study. However, according to a prior study [ 32],
Mockitoand EasyMockare thetwo mostpopular mockingframe-
works.Theyareusedby70%and20%of5,000randomlysampled
445projects,respectively. Assuch, ourstudy resultscan beappliedto
most of the Java projects.
When learning from existing mocking practices, MockSniffer as-
sumes that the mocking decisions made by the developers are cor-
rect.However,developerscanalsomakeimpropermockingdeci-
sions and change their decisions. To address this threat, we trained
MockSniffer with 546k data entries extracted from ten different
projects. The large number of training data entries can mitigate
the problem induced by the small portion of improper mocking
decisions in the dataset.
7.2 Future Work
Unit tests generation with mock recommendation. Some ex-
isting studies can generate unit tests with synthesized mockedobjects [
12,13,26]. However, these studies cannot select which
dependencies to mock. Directly using such techniques can induce
problemsofover-mockingsincetheychoosetomockallthedepen-
dencies of classes under test. In the future, we plan to combine our
technique with these existing studies to generate unit tests with
properly-selected mocked dependencies.
Generalizing MockSniffer tootherlanguages. In this paper,
westudiedthemockingpracticesinJavaandproposed MockSnif-
fertorecommendmockingdecisions.BesidesJava,mockingisalso
widelyusedinprojectswritteninotherprogramminglanguages.
In the future, we plan to further extend MockSniffer to recommend
mockingdecisionsforprojectsinotherlanguagessuchasC#and
Javascript. Mocking frameworks like Moq [ 9] and Sinon.js [ 10]
arefrequentlyusedinC#andJavaScriptprojects.RecommendingmockingdecisionsforC#canbesimilartoJavasinceC#andJava
sharesimilarprogrammingparadigms.Incomparison,thelanguage
features of Javascript are different from that of Java. Such different
language features can induce different mocking practices. In thefuture, we plan to also study the mocking practices of projectsin different languages and generalize MockSniffer to recommend
mocking decisions for them.
8 RELATED WORK
Empirical studieson mocking. Studies were conducted to ana-
lyze the usage of mocking as well as understanding the intentions
behind the mocking decisions. Marri et al. [ 31] were among the
first to analyze the use of mocked objects in testing file-system-
dependent software and highlighted the need for automated identi-fication of APIs that need to be mocked (i.e., automated techniques
to recommend mocking decisions). Mostafa et al. [ 32] analyzed
the use of four popular mocking frameworks in 5,000 open-source
projects. They found that mocking frameworks are used in 23% of
theprojectsthathavetestcode,anddevelopersusuallymockasmall
portionofthedependencies.Spadinietal.[ 35]studiedtheusage
of mocked objects in three open-source projects and one industrial
system and distilled that developers usually mock dependencies
that make testing difficult. More recently, Spadini et al. studied the
evolutionofmockedobjects[ 36].Eventhoughtheneedforauto-
mated techniques to recommend mocking decisions was proposed
early by Marri et al. [ 31], none of these existing studies produce
such an automated technique. The existing studies either providedstatistical evidence to show the popularity of mocking or identi-fied general characteristics of mocking practices. In comparison,
ourstudyidentifiedspecificcode-levelcharacteristicsofmocked
objects.Basedonsuchcode-levelcharacteristics,wewereableto
proposethefirstautomatedtechnique MockSniffer torecommend
mocking decisions by leveraging machine learning techniques.
Automatic test generation with mocking. Another line of
related work aimed to improve test generation techniques with the
helpofmocking [ 12,13,15,26].Forexample, Arcurietal.[ 13]e x-
tendedEvoSuite[ 22],aJavatestgenerationtoolbyaddingmocked
objects to its generated test cases. By mocking the environmental
interactions in the test cases, their technique improved the code
coverageandreducedthenumberofunstabletestsbymorethan
50%.Otherstudiesusedmockingtosimulatetheenvironmentoper-
ations to control the environment-related dependencies. Taneja et
al.proposedMODA[ 37]togeneratetestsfordatabaseapplications
by mockingdatabase operations. Thisapproach is alsoadopted to
handleinteractionswithnetworking[ 14]andwebservices[ 16,43].
While these work leveraged mocking to improve test generation
techniques,theyselectedthedependenciestomockmerelybased
on single factors without considering the interactions between the
dependency and different CUTs. As a result, these approaches can
selectthesamedependenciestomockevenfordifferentprojects.
However, as shown in our empirical study(Section 4.2), contextual
information is a key factor that affects mocking decisions. Without
taking such contextual information into account, these techniques
cangeneratetestcaseswiththeproblemofover-mockingorunder-
mocking. In comparison, our technique combines several different
code-levelfeaturesofthedependenciesandtheclassesundertest
to recommend context-aware mocking decisions. As shown in our
evaluation, our technique can outperform the baselines that adopt
single factors to make mocking decisions.
9 CONCLUSION
In this work, we conducted an empirical study on four large-scale,
actively-maintainedopen-sourceprojectsandidentifiedtencode-
level characteristics of the mocked dependencies. Our identified
characteristicscapturedboththefeatureofthedependenciesthem-selvesandtheinteractionsbetweenthedependenciesandtheCUTs.
Based on our identified characteristics, we further proposed Mock-
Sniffer, an automated technique that recommends mocking deci-
sions.MockSniffer leverages machine learning techniques and rec-
ommends context-aware mocking decisions by learning from exist-
ing mocking practices. Our evaluation shows that MockSniffer can
effectively recommend mocking decisions and can outperform the
generic mocking strategies adopted by existing studies.
ACKNOWLEDGMENTS
WethanktheanonymousreviewersofASE‚Äô20fortheirconstructive
comments. This workis supportedby the National Natural Science
Foundation of China (No. 61932021), the WeBank-HKUST Joint
Laboratory,theGeneralResearchFund(No.16211919)bytheHong
Kong ResearchGrant Council, and theGuangdong Provincial Key
Laboratory(No.2020B121201001).LiliWeiwassupportedbythe
PostdoctoralFellowshipSchemebytheHongKongResearchGrant
Council.
446REFERENCES
[1]2013.CAMEL-6826 apache/camel@dae6366. Retrieved May 2020 from https:
//github.com/apache/camel/commit/dae6366
[2]2013.hadoop/hadoopTestCachingKeyProvider.java at branch-3.2.1apache/hadoop.
Retrieved May 2020 from https://github.com/apache/hadoop/blob/branch-
3.2.1/hadoop-common-project/hadoop-common/src/test/java/org/apache/
hadoop/crypto/key/TestCachingKeyProvider.java#L59
[3] 2020. EasyMock. Retrieved May 2020 from https://easymock.org/
[4]2020.EvoSuite MockList. Retrieved May 2020 from https://github.com/EvoSuite/
evosuite/blob/master/runtime/src/main/java/org/evosuite/runtime/mock/
MockList.java
[5]2020.[FLINK-16300]ASFJIRA. RetrievedMay2020fromhttps://issues.apache.
org/jira/browse/FLINK-16300
[6]2020.hadoop/TestJobHistoryEventHandler.javaatbranch-3.2.1apache/hadoop. Re-
trieved May 2020 from https://github.com/apache/hadoop/blob/branch-3.2.1/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-
mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/jobhistory/
TestJobHistoryEventHandler.java#L1084
[7]2020.jMock-AnExpressiveMockObjectLibraryforJava. RetrievedMay2020
from http://jmock.org/
[8] 2020. Mickito frmework site. Retrieved May 2020 from https://site.mockito.org/
[9]2020.moq/moq4: Repo for managing Moq 4.x - GitHub. Retrieved May 2020 from
https://github.com/moq/moq4
[10]2020.Sinon.JS - Standalone test fakes, spies, stubs and mocks for JavaScript. Works
with any unit testing framework. Retrieved May 2020 from https://sinonjs.org/
[11]Alfred V Aho. 1988. Compilers : principles, techniques, and tools. Addison-Wesley
Pub. Co., Reading, Mass.
[12]Nadia Alshahwan, Yue Jia, Kiran Lakhotia, Gordon Fraser, David Shuler, and
PaoloTonella.2010. AUTOMOCK:AutomatedSynthesisofaMockEnvironment
for Test Case Generation. In Practical Software Testing: Tool Automation and
Human Factors, 14.03. - 19.03.2010 (Dagstuhl Seminar Proceedings), Vol. 10111.Schloss Dagstuhl - Leibniz-Zentrum f√ºr Informatik, Germany. http://drops.
dagstuhl.de/opus/volltexte/2010/2618/
[13]AndreaArcuri,GordonFraser,andJuanPabloGaleotti.2014. Automatedunit
testgenerationforclasseswithenvironmentdependencies.In ACM/IEEEInterna-
tional Conference on Automated Software Engineering, ASE ‚Äô14, Vasteras, Sweden -
September 15 - 19, 2014. ACM, 79‚Äì90. https://doi.org/10.1145/2642937.2642986
[14]Andrea Arcuri, Gordon Fraser, and Juan Pablo Galeotti. 2015. Generating
TCP/UDP network data for automated unit test generation. In Proceedings
of the 2015 10th Joint Meeting on Foundations of Software Engineering, ES-EC/FSE 2015, Bergamo, Italy, August 30 - September 4, 2015. ACM, 155‚Äì165.
https://doi.org/10.1145/2786805.2786828
[15]Andrea Arcuri, Gordon Fraser, and Ren√© Just. 2017. Private API Access andFunctional Mocking in Automated Unit Test Generation. In 2017 IEEE Inter-
national Conference on Software Testing, Verification and Validation, ICST 2017,Tokyo, Japan, March 13-17, 2017. IEEE Computer Society, 126‚Äì137. https:
//doi.org/10.1109/ICST.2017.19
[16]Thilini Bhagya, Jens Dietrich, and Hans W. Guesgen. 2019. Generating MockSkeletons for Lightweight Web-Service Testing. In 26th Asia-Pacific Software
Engineering Conference, APSEC 2019, Putrajaya, Malaysia, December 2-5, 2019.
IEEE, 181‚Äì188. https://doi.org/10.1109/APSEC48747.2019.00033
[17]Bernhard E. Boser, Isabelle Guyon, and Vladimir Vapnik. 1992. A Training
AlgorithmforOptimalMarginClassifiers.In ProceedingsoftheFifthAnnualACM
ConferenceonComputationalLearningTheory,COLT1992,Pittsburgh,PA,USA,
July 27-29, 1992. ACM, 144‚Äì152. https://doi.org/10.1145/130385.130401
[18]Leo Breiman. 1996. Bias, variance, and arcing classifiers. Technical Report. Tech.
Rep. 460, Statistics Department, University of California, Berkeley ....
[19]LeoBreiman,J.H.Friedman,R.A.Olshen,andC.J.Stone.1984. Classification
and Regression Trees. Wadsworth.
[20]John W Creswell and Cheryl N Poth. 2016. Qualitative inquiry and research
design: Choosing among five approaches. Sage publications.
[21]The Apache Software Foundation. 2018. The Apache Software Founda-
tion Announces Apache ¬ÆOozie(TM) v5.0.0. Retrieved May 2020 from
http://www.globenewswire.com/news-release/2018/04/18/1481007/0/en/The-
Apache-Software-Foundation-Announces-Apache-Oozie-TM-v5-0-0.html
[22]GordonFraserandAndreaArcuri.2011. EvoSuite:automatictestsuitegeneration
for object-oriented software. In SIGSOFT/FSE‚Äô11 19th ACM SIGSOFT Symposium
on the Foundations of Software Engineering (FSE-19) and ESEC‚Äô11: 13th European
Software Engineering Conference (ESEC-13), Szeged, Hungary, September 5-9, 2011.
ACM, 416‚Äì419. https://doi.org/10.1145/2025113.2025179
[23]YoavFreundandRobertE.Schapire.1995. Adecision-theoreticgeneralization
of on-line learning and an application to boosting. In Computational Learning
Theory,SecondEuropeanConference,EuroCOLT‚Äô95,Barcelona,Spain,March13-15,
1995,Proceedings (LectureNotesinComputerScience),Vol.904.Springer,23‚Äì37.
https://doi.org/10.1007/3-540-59119-2_166
[24]Jerome H. Friedman. 2001. Greedy function approximation: A gradient boosting
machine. Ann. Statist. 29, 5 (2001), 1189‚Äì1232.[25]KarlPearsonF.R.S.1900. X.Onthecriterionthatagivensystemofdeviations
fromtheprobableinthecaseofacorrelatedsystemofvariablesissuchthatit
can be reasonably supposed to have arisen from random sampling. The London,
Edinburgh, and Dublin Philosophical Magazine and Journal of Science 50, 302
(1900), 157‚Äì175. https://doi.org/10.1080/14786440009463897
[26]Stefan J. Galler, Andreas Maller, and Franz Wotawa. 2010. Automatically extract-
ingmockobjectbehaviorfromDesignbyContract ‚Ñ¢specificationfortestdata
generation. In The 5th Workshopon Automation of SoftwareTest, AST 2010, May
3-4, 2010,Cape Town, SouthAfrica. ACM,43‚Äì50. https://doi.org/10.1145/1808266.
1808273
[27]Qingzhou Luo, Farah Hariri, Lamyaa Eloussi, and Darko Marinov. 2014. An
empiricalanalysisofflakytests.In Proceedingsofthe22ndACMSIGSOFTInter-
nationalSymposiumonFoundationsofSoftwareEngineering,(FSE-22),HongKong,China, November 16 - 22, 2014. 643‚Äì653. https://doi.org/10.1145/2635868.2635920
[28]TimMackinnon,SteveFreeman,andPhilipCraig.2001. Endo-Testing:UnitTesting
withMockObjects. Addison-WesleyLongmanPublishingCo.,Inc.,USA,287‚Äì301.
[29]InderjeetManiandIZhang.2003. kNNapproachtounbalanceddatadistributions:
a case study involving information extraction. In Proceedings of workshop on
learning from imbalanced datasets, Vol. 126.
[30]Henry B Mann and Donald R Whitney. 1947. On a test of whether one oftwo random variables is stochastically larger than the other. The annals of
mathematical statistics (1947), 50‚Äì60.
[31]Madhuri R. Marri, Tao Xie, Nikolai Tillmann, Jonathan de Halleux, and Wolfram
Schulte.2009. AnEmpiricalStudyofTestingFile-System-DependentSoftware
withMockObjects.In Proceedingsofthe4thInternationalWorkshoponAutomation
ofSoftwareTest,AST2009,Vancouver,BC,Canada,May18-19,2009.IEEEComputer
Society, 149‚Äì153. https://doi.org/10.1109/IWAST.2009.5069054
[32]ShaikhMostafaandXiaoyinWang.2014. AnEmpiricalStudyontheUsageof
Mocking Frameworks in Software Testing. In 2014 14th International Conference
on Quality Software, Allen, TX, USA, October 2-3, 2014. 127‚Äì132. https://doi.org/
10.1109/QSIC.2014.19
[33]F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M.
Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cour-
napeau,M.Brucher,M.Perrot,andE.Duchesnay.2011. Scikit-learn:Machine
Learning in Python. Journal of Machine Learning Research 12 (2011), 2825‚Äì2830.
[34]SinaShamshiri,Ren√©Just,Jos√©MiguelRojas,GordonFraser,PhilMcMinn,and
AndreaArcuri.2015. DoAutomaticallyGeneratedUnitTestsFindRealFaults?
An Empirical Study of Effectiveness and Challenges (T). In 30th IEEE/ACM
International Conference on Automated Software Engineering, ASE 2015, Lin-coln, NE, USA, November 9-13, 2015. IEEE Computer Society, 201‚Äì211. https:
//doi.org/10.1109/ASE.2015.86
[35]DavideSpadini,MauricioFinavaroAniche,MagielBruntink,andAlbertoBac-
chelli. 2017. To mock or not to mock?: an empirical study on mocking practices.
InProceedings of the 14th International Conference on Mining Software Reposi-
tories, MSR 2017, Buenos Aires, Argentina, May 20-28, 2017, Jes√∫s M. Gonz√°lez-
Barahona,AbramHindle,andLinTan(Eds.).IEEEComputerSociety,402‚Äì412.
https://doi.org/10.1109/MSR.2017.61
[36]DavideSpadini,Maur√≠cioFinavaroAniche,MagielBruntink,andAlbertoBac-
chelli.2019. Mockobjectsfortestingjavasystems-Whyandhowdevelopersuse them, and how they evolve. Empirical Software Engineering 24, 3 (2019),
1461‚Äì1498. https://doi.org/10.1007/s10664-018-9663-0
[37]KunalTaneja,YiZhang,andTaoXie.2010. MODA:automatedtestgenerationfordatabaseapplicationsviamockobjects.In ASE2010,25thIEEE/ACMInternational
Conference on Automated Software Engineering, Antwerp, Belgium, September
20-24,2010 ,CharlesPecheur,JamieAndrews,andElisabettaDiNitto(Eds.).ACM,
289‚Äì292. https://doi.org/10.1145/1858996.1859053
[38]NikolaiTillmannandWolframSchulte.2006. Mock-objectgenerationwithbehav-ior.In21stIEEE/ACMInternationalConferenceonAutomatedSoftwareEngineering
(ASE 2006), 18-22 September 2006, Tokyo, Japan. IEEE Computer Society, 365‚Äì368.
https://doi.org/10.1109/ASE.2006.51
[39]Graham Upton and Ian Cook. 1996. Understanding statistics. Oxford University
Press.
[40]Raja Vall√©e-Rai, Phong Co, Etienne Gagnon, Laurie J. Hendren, Patrick Lam,and Vijay Sundaresan. 1999. Soot - a Java bytecode optimization framework.InProceedings of the 1999 conference of the Centre for Advanced Studies on
Collaborative Research, November 8-11, 1999, Mississauga, Ontario, Canada. 13.
https://dl.acm.org/citation.cfm?id=782008
[41]Rongxin Wu, Ming Wen, Shing-Chi Cheung, and Hongyu Zhang. 2018. Change-
Locator:locatecrash-inducingchangesbasedoncrashreports. Empir.Softw.Eng.
23, 5 (2018), 2866‚Äì2900. https://doi.org/10.1007/s10664-017-9567-4
[42]Harry Zhang. 2004. The Optimality of Naive Bayes. In Proceedings of the Sev-
enteenthInternationalFloridaArtificialIntelligenceResearchSocietyConference,
Miami Beach, Florida, USA, Valerie Barr and Zdravko Markov (Eds.). AAAI Press,
562‚Äì567. http://www.aaai.org/Library/FLAIRS/2004/flairs04-097.php
[43]Linghao Zhang, Xiaoxing Ma, Jian Lu, Tao Xie, Nikolai Tillmann, and Peli de
Halleux.2012.EnvironmentalModelingforAutomatedCloudApplicationTesting.
IEEE Software 29, 2 (2012), 30‚Äì35. https://doi.org/10.1109/MS.2011.158
447