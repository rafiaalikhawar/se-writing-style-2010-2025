EDITSUM: A Retrieve-and-Edit Framework for
Source Code Summarization
Jia Li
Key Lab of High ConÔ¨Ådence Software
Technology, MoE (Peking University)
Beijing, China
lijia@stu.pku.edu.cnYongmin Li
Key Lab of High ConÔ¨Ådence Software
Technology, MoE (Peking University)
Beijing, China
liyongmin@pku.edu.cnGe Li*
Key Lab of High ConÔ¨Ådence Software
Technology, MoE (Peking University)
Beijing, China
lige@pku.edu.cn
Xing Hu
School of Software Technology
Zhejiang University, Ningbo, China
xinghu@zju.edu.cnXin Xia
Faculty of Information Technology
Monash University, Melbourne, Australia
Xin.Xia@monash.eduZhi Jin*
Key Lab of High ConÔ¨Ådence Software
Technology, MoE (Peking University)
Beijing, China
zhijin@pku.edu.cn
Abstract ‚ÄîExisting studies show that code summaries help
developers understand and maintain source code. Unfortunately,
these summaries are often missing or outdated in software
projects. Code summarization aims to generate natural language
descriptions automatically for source code. According to Groset al., code summaries are highly structured and have repetitive
patterns (e.g. ‚Äúreturn true if...‚Äù). Besides the patternized words,
a code summary also contains important keywords, which are
the key to reÔ¨Çecting the functionality of the code. However,
the state-of-the-art approaches perform poorly on predicting the
keywords, which leads to the generated summaries suffer a loss ininformativeness. To alleviate this problem, this paper proposes
a novel retrieve-and-edit approach named E
DITSUM for code
summarization. SpeciÔ¨Åcally, EDITSUM Ô¨Årst retrieves a similar
code snippet from a pre-deÔ¨Åned corpus and treats its summaryas a prototype summary to learn the pattern. Then, E
DITSUM
edits the prototype automatically to combine the pattern in theprototype with the semantic information of input code. Ourmotivation is that the retrieved prototype provides a good start-
point for post-generation because the summaries of similar code
snippets often have the same pattern. The post-editing process
further reuses the patternized words in prototype and generateskeywords based on the semantic information of input code. We
conduct experiments on a large-scale Java corpus (2M) and
experimental results demonstrate that E
DITSUM outperforms the
state-of-the-art approaches by a substantial margin. The human
evaluation also proves the summaries generated by EDITSUM
are more informative and useful. We also verify that EDITSUM
performs well on predicting the patternized words and keywords.
Index T erms‚ÄîCode summarization, Information retrieval,
Deep learning
I. I NTRODUCTION
During software development and maintenance, developers
spend around 59% of their time on program comprehension
activities [1]‚Äì[3]. A code summary provides a concise naturallanguage description for a code snippet, which can help
developers understand the program quickly and correctly [4].Unfortunately, the code summaries are often mismatched,missing or outdated in the software projects [5]. Additionally,
* Corresponding authorsmanually writing summaries during the development is time-consuming for developers. Therefore, it is important to exploreautomatic code summarization approaches.
Traditional approaches generate code summaries based on
the template-based approaches and information retrieval (IR)based approaches. Template-based approaches [4], [6] Ô¨Årstlyextract the keywords from the source code, and then Ô¨Åll thekeywords into the predeÔ¨Åned templates to generate a codesummary. The IR-based approaches use code summaries ofsimilar code snippets as outputs directly. Among these IR-based approaches, they retrieve the similar code snippets byvarious similarity metrics [7], [8] from open-source softwarerepositories in GitHub or software Q&A sites [9], [10].Although the traditional approaches are simple, they haveachieved good results. This is because code summaries arehighly structured and contain many repetitive patterns, e.g.,‚Äúreturn true if...‚Äù and ‚Äúcreate a new...‚Äù [11]. The manually-
crafted templates and retrieved summaries provide a lot ofreusable patternized words, which play an key role in the codesummaries. However, for template-based approaches, manu-ally deÔ¨Åning templates is time-consuming and laborious, andrequires a lot of expert experience. For IR-based approaches,there may be semantic inconsistencies between the retrievedsummary and the input code.
With the development of deep learning, there is an emerging
interest in applying neural networks for automatic code sum-marization. Previous studies [12]‚Äì[14] often adopt the encoder-decoder architecture [15] to learn the mapping between wordsand even the grammatical structure from source code tonatural language based on the large-scale corpus. By virtueof the naturalness of the source code [16], [17], these neuralmodels can mine patterns for generating code summariesfrom a large corpus. Besides the patternized words, a codesummary also contains important keywords, which have a lowfrequency in training data, but are the key to reÔ¨Çecting thefunctionality of source code (more details can be found inSection II). However, the state-of-the-art nerual models [12]‚Äì
UI*&&&"$.*OUFSOBUJPOBM$POGFSFODFPO"VUPNBUFE4PGUXBSF&OHJOFFSJOH	"4&
%0*"4&2021 36th IEEE/ACM International Conference on Automated Software Engineering (ASE) | 978-1-6654-0337-5/21/$31.00 ¬©2021 IEEE | DOI: 10.1109/ASE51524.2021.9678724

	
    
[14] perform poorly on predicting keywords. For example,
LeClair et al. [14] found 21% summaries written by humansin the test set contain words with the frequency of less than100, but only 7% summaries generated by their proposedapproach contain these words. The lack of keywords leadsto the generated summaries suffer a loss in informativeness,which have a negative impact on program comprehension.
Recently, Wei et al. [18] and Zhang et al. [19] proposed
two retrieval-based neural models to address the problem of
keywords. They used the IR techniques to get the similarcode and its summary, and then input the retrieved resultsand the input code into the encoder. With the assistance ofthe retrieved summary, their models can accurately generatepatternized words. However, their models only treated theretrieved results as auxiliary information and don‚Äôt solve theproblem of keywords.
In this paper, we propose a novel retrieve-and-edit ap-
proach E
DITSUMfor code summarization. The improvement
by template-based approaches proves that the importance ofthe patterns in code summaries. The improvement by IR-
based approaches shows that the summaries of similar codesnippets often have the same pattern. So, we treat the summaryof similar code as a prototype and extract the pattern fromthe prototype. Considering the inconsistencies between theprototype and input code, we design a neural network tofurther edit the prototype automatically based on the semanticinformation of input code. Our motivation is that the patternin a prototype tells the neural model ‚Äúhow to say‚Äù and thesemantic information of input code tells the neural model‚Äúwhat to say‚Äù.
E
DITSUMconsists of two modules: a Retrieve module and
an Edit module. In the Retrieve module, given an input code
snippet, we use IR techniques to retrieve the similar codesnippet from a large parallel corpus and treat the summary ofthe similar code snippet as a prototype. Then, the Edit module
generates a summary by fusing the pattern in prototype andsemantic information of input code. SpeciÔ¨Åcally, we proposea sequence-to-sequence (seq2seq) neural network to learn to
revise the prototype based on the semantic differences of theinput code and the similar code. To represent the semanticdifferences, we calculate an edit vector by concatenating the
weighted sums of insertion word embeddings (words in input
code but not in similar code) and deletion word embeddings(words in similar code but not in input code). After that, werevise the prototype summary conditioning on the edit vectorto obtain a new summary.
To evaluate our approach, we conduct experiments on a
real-world Java dataset. The dataset comes from the Sourcererrepository
1and has been processed by LeClair et al. [14], in-
cluding removing duplicates and dividing into training, valida-tion, and test sets by projects. We employ the mainstream eval-uation metric BLEU [20], METEOR [21] and ROUGE [22]score that are widely used in summary generation task toevaluate the generated summaries. Experimental results show
1https://www.ics.uci.edu/lopes/datasets/TABLE I: The patterns of summaries in dataset.
Real Sampleswrite atestÔ¨Ånish tothemesa logger
write thistilemap toanxml Ô¨Åle
write thebuffer totheoutput stream
write grid data tothegeotiff Ô¨Åle
write cdlrepresentation tooutput stream
Pattern write to
Real Samplesthis method sets thehelp button visible
this method sets thevaule ofÔ¨Åeld
this method sets asearch argument forlist
this method sets theclient id
this method sets therange asadouble
Pattern this method sets
Real Samplesconvert animage toanarray ofinteger
convert thisippacket toareadable string
convert ajingle description toxml
convert thespeciÔ¨Åed string toaurl
convert thedate tothegiven timezone
Pattern convert to
that E DITSUM performs substantially better than the IR-
based baselines and outperforms the state-of-the-art neuralbaselines. The human evaluation and qualitative analysis provethe summaries generated by E
DITSUMare informative and
useful for developers to understand programs. Besides, weverify that E
DITSUMnot only accurately generates patternized
words, but also generates more keywords.
Our main contributions are outlined as follows:
‚Ä¢We propose a novel retrieve-and-edit approach, namely
EDITSUM, for code summarization. We use the sum-
maries of similar code snippets as prototypes to assist
in generating summaries.
‚Ä¢We design an effective editing module for summary
generation, which can combine the pattern in prototypewith the semantic information of code.
‚Ä¢We conduct extensive experiments to evaluate our ap-proach on a large-scale Java dataset. The experimental
results show that E
DITSUMsubstantially outperforms the
state-of-the-art approaches.
Paper Organization. The rest of this paper is organized
as follows. Section II describes motivating examples. Section
III presents our proposed approach. Section IV and SectionV describe the experimental setup and results. Section VIand Section VII discuss some results and describe the related
work, respectively. Finally, Section VIII concludes the paperand points out future directions.
II. M
OTIV ATING EXAMPLES
A closer look at the code summarization dataset shows that
patterns such as ‚Äúcreates a new‚Äù, ‚Äúreturns true if ‚Äù, ‚Äúload
into‚Äù, ‚Äúconvert into‚Äù are very frequent [11]. Table I showssome samples from the dataset provided by LeClair et al.[14]. The bold words are patternized words, and the dashedwords denote the keywords. Such a code summary can beregarded as composed of patternized words and keywords.
The pattern ensures the readability of the summary, and thekeywords reÔ¨Çect the functionality of the source code. A good
code summary should contains suitable patternized words andmeaningful keywords.
Input Code:
publicIterator getPrefixes( StringnamespaceURI) {
Listl= URIMap. get(namespaceURI);
return(l == null) ? null:l.iterator();
}
Similar Code:
publicStringgetPrefix( StringnamespaceURI) {
List<String> l = URIMap .get(namespaceURI);
return(l == null) ? null:l.get(0);
}
Rencos (Input Code): returns aniterator over the values to aspecified url.
Human-written (Input Code): return an iterator over all prefixes to a url
Human-written (Similar Code): return a prefix corresponding to a url
Fig. 1: An example of the input code and similar code.
However, previous models perform well on predicting the
patternized word, ignoring the importance of keywords. As
Figure 1 shows, for the input code, we use the open-source
search engine Lucene2to retrieve the most similar code
snippet from the training corpus. The retrieval metric is basedon the lexical level similarity of the source code.
In Figure 1, the summaries of input code and similar
code have the same pattern (return...to a url ), but there are
semantic differences between the similar code and input code.Although the two Java methods are lexically similar, the inputcode returns all preÔ¨Åxes, while the similar code returns acertain preÔ¨Åx. In Figure 1, the state-of-the-art neural modelRencos [19] can correctly predict the patternized words (e.g.,return, to), but it performs poorly on keywords (e.g., preÔ¨Åxes).The code summaries generated by Rencos achieve high scoreson the patternized words, but they do not clearly express thepurposes of the programs.
In this paper, we address that both pattern and keywords are
important for a code summary. Inspired by previous studies,
we propose a retrieve-and-edit approach by combining the
pattern in existing summaries and the semantic informationof input code to generate informative summaries with suitablepatterns.
III. P
ROPOSED APPROACH
In this paper, we propose a retrieve-and-edit approach
named E DITSUMfor source code summarization, which can
combine the strengths of traditional approaches and neuralmodels. The overall framework of our model is shown inFigure 2. Our approach E
DITSUM consists of a Retrieve
module and an Edit module and generates a summary in threesteps:
Step 1: Selecting a suitable prototype summary. We use
a massive training set as the retrieval corpus. Given an inputcode, the Retrieve module uses the search engine to search forthe similar code-summary pair from the corpus. The retrievalprocess is explained in Section III-A.
Step 2: Extracting the semantic information of the input
code. In Figure 2, we mark the lexical differences between thetwo Java methods. We Ô¨Ånd that the different words between
2https://lucene.apache.org/the two methods reÔ¨Çect their semantic differences to a certainextent, such as ‚ÄúIteration‚Äù vs ‚ÄúString‚Äù, and ‚ÄúPreÔ¨Åxes‚Äù vs‚ÄúPreÔ¨Åx‚Äù. Therefore, we calculate an edit vector based on thelexical differences between similar code and input code torepresent their semantic differences. The details of this part isdescribed in Section III-B.
Step 3: Combining the pattern in prototype with semantic
information of input code. To this end, we design a neuraledit module to revise the prototype based on the semanticdifferences between the input code and similar code. Thedetails is presented in Section III-B.
A. Retrieve Module
In our approach, the Retrieve module aims to retrieve the
similar code-summary pair from a corpus given the input
code. Inspired by previous studies [18], [19], we choose thelexical-level similarity as retrieval metric. SpeciÔ¨Åcally, weadoptBM25[23] as the similarity evaluation metric, which is
a bag-of-words retrieval function to estimate the relevance ofdocuments to a given query. Given a query and a document,based on TF-IDF [24], the BM25function calculates the term
frequency in the document of each keyword in the query andmultiplies it by the inverse document frequency of this term.The more relevant two documents have, the higher the valueofBM25score. We leverage the open-source search engine
Lucene to build the Retrieve module. Since the size of the
training set is quite large (over 1.9M), we use it as the retrievalcorpus. We Ô¨Årst tokenize the source code and summaries andprocess each code and summary pair into a document, add itto the index library, and store it on disk.
As shown in Figure 2, we use different strategies to select
prototypes for training and testing. In testing, we search forthe most similar code from the training set and treat itssummary as the prototype. During the training phase, as wealready know the targrt summary, we Ô¨Årst retrieve top-20 code-summary pairs based on the summary similarity. Then, wereserve the retrieved summaries as prototypes whose Jaccard
similarity [25] to target summary in the range of [0.3, 0.7].TheJaccard similarity measures text similarity from a bag-
of-words view, that is formulated as
J(A,B)=|A‚à©B|
|A‚à™B|(1)
whereAandBare two bags of words and |¬∑|denotes the
number of elements in a collection. The motivation behind
Ô¨Åltering out summaries with Jaccard similarity <0.3 is the
edit module performs well only if a prototype is lexically
similar to its target summary [26]. Besides, we hope the editmodule does not copy the prototype so we discard summarieswhere the prototype and target summary are nearly identical(i.e.Jaccard similarity >0.7). We do not use code similarity
to construct training data, because similar code snippets maycorrespond to totally different summaries. This is not con-ducive to our model learning how to revise a prototype. Thepreliminary experiments also show that constructing trainingdata based on code similarity may cause the model to fail toconverge.
Similar Code:
publicStringget Prefix( StringnamespaceURI) {
List<String> l= URIMap. get(namespaceURI);
return(l == null) ? null:l.get(0);
} 
 
 
 	!	
		
$!  %'
$!  %(
#"
$!  %(&	! 	!
	 
 ! 	!	 $	! 
 
   	!
	 
 !	 $	!  $	! 
			
%		"		Input Code:
publicIterator get Prefixes( StringnamespaceURI) {
Listl= URIMap. get(namespaceURI);
return(l == null) ? null:l.iterator();
} 
 	
		
 
&			

	     
 
       
'
			
Fig. 2: The overall framework of our approach.
B. Edit Module
After that, the key step is to combine the pattern in the
prototype and the semantic information of input code to
generate a new summary. The structure of the Edit moduleis shown in Figure 3. Firstly, we utilize the prototype encoderto get the vector representation of prototype. Secondly, wecompute the edit vector based on the lexical differences of
two code snippets. The edit vector represents the semanticdifferences between the similar code and input code. Lastly,
the summary decoder is used to generate a new summaryconditioning on the prototype representation and edit vector.
1) Prototype Encoder: The prototype encoder takes the
prototype Y
/primeas input. We Ô¨Årst map the one-hot vector of a
tokenw/prime
iinto a word embedding y/prime
i:
y/prime
i=W/latticetop
ew/prime
i,i‚àà[1,n] (2)
wherenis the length of prototype, Weis a trainable word
embedding matrix. To leverage the contextual information, weuse a bidirectional long short-term memory (Bi-LSTM) [27]unit to process the sequence of word embeddings. At i-th time
step, the hidden state h
iof the Bi-LSTM can be represented
by:
‚àí ‚Üíhi=L S T M/parenleftBig‚àí ‚Üíhi‚àí1,y/prime
i/parenrightBig
;‚Üê ‚àíhi=L S T M/parenleftBig‚Üê ‚àíhi+1,y/prime
i/parenrightBig
(3)
hi=/bracketleftBig‚àí ‚Üíhi‚äï‚Üê ‚àíhi/bracketrightBig
(4)
where‚äïis a concatenation operation. Finally, the prototype
Y/primeis transformed to vector representation {hi}n
i=1.
2) Edit V ector: The edit vector zaims to reÔ¨Çect the
semantic differences between the input code Xand similar
codeX/prime. Suppose that XandX/primeonly differ by a single word
w. Then one might think that the edit vector zshould be equal
to the word embedding for w. Generalizing this intuition tomulti-word edits, the multi-word insertions can be represented
as the sum of the inserted word vectors, and similarly formulti-word deletions [26].
As shown in Figure 3, we deÔ¨Åne I={w|w‚ààX‚àßw/‚ààX
/prime}
as a insertion word set, and D={w/prime|w/prime/‚ààX‚àßw/prime‚ààX/prime}as a
deletion word set. Because different words inÔ¨Çuence the edit-ing process unequally, we represent the differences betweenXandX
/primeusing the weighted sum of word embeddings:
fdiff(X,X/prime)=/summationdisplay
w‚ààIŒ±wŒ¶(w)‚äï/summationdisplay
w/prime‚ààDŒ≤w/primeŒ¶(w/prime) (5)
whereŒ¶(w)is the word embedding of word wand‚äïdenotes
a concatenation operation. Œ±wis the weight of a insertion word
w, that is computed by the attention mechanism:
Œ±w=exp(ew)/summationtext
w‚ààIexp(ew)(6)
ew=v/latticetop
Œ±tanh(W Œ±[Œ®(w)‚äïhn]) (7)
where vŒ±andWŒ±are trainable parameters, and hnis the
Ô¨Ånal hidden state of prototype encoder. Œ≤w/primeis obtained with
a similar process. Then we compute the edit vector zby fol-
lowing linear projection, which can be regarded as a mappingfrom code differences to summary differences.
z=t a n h( W¬∑f
diff+b) (8)
where Wandbare two trainable parameters.
3) Summary Decoder: After that, we revise the prototype
based on the edit vector to get a new summary. The purposeof the summary decoder is to generate a new summary.
As shown in Figure 3, the decoder takes the prototype
representation {h
i}n
i=1and the edit vector zas inputs and
generate a summary by a LSTM unit with attention. The
hidden state of the decoder is compute by
si=L S T M( si‚àí1,yi‚àí1‚äïz) (9)
Fig. 3: The structure of the Edit module.
wheresi‚àí1means the previous hidden state of the decoder,
yi‚àí1is the (i‚àí1)-th word embedding of ground-truth sum-
mary. We concatenate the edit vector to every input embedding
of the decoder, so the edit information can be utilized in theentire generation process.
To introduce the information of the prototype, we then
compute a context vector c
iby attention mechanism, which is
a weighted sum of prototype representation {hi}n
i=1:
ci=n/summationdisplay
j=1Œ∑i,jhj (10)
where attention weights are obtained by
Œ∑i,j=exp(ei,j)/summationtextn
k=1exp(ei,k)(11)
ei,j=v/latticetop
Œ∑tanh(W Œ∑[hj‚äïsi]) (12)
where vŒ∑andWŒ∑are two trainable parameters. Based on
the previous word yi‚àí1, hidden state of the decoder siand
the context vector cifrom prototype, our model compute the
probability of i-th token yi:
p(yi)=s o f t m a x( Wp[yi‚àí1‚äïsi‚äïci]+b p) (13)
where Wpandbpare two trainable parameters.
C. Loss Function
During training, E DITSUMtakes a token sequence of the
input code X, a summary of the input code Y, a token
sequence of the similar code X/prime, and the prototype Y/primeas
inputs, respectively. We optimize parameters of E DITSUM
by maximizing the probability of p(Y|z,Y/prime). The overallTABLE II: The statistics of datasets.
Dataset Train Valid Test
Count 1,954,807 104,273 90,908
Avg. tokens in code 29.67 29.68 30.17
Avg. tokens in summary 7.594 7.710 7.654
(a) Code length distribution. (b) Summary length distribution.
Fig. 4: Length distribution of test data.
objective function of our model is to minimize the following
loss function:
L(Œ∏)=‚àíN/summationdisplay
i=1L/summationdisplay
t=1logP/parenleftbig
yi
t|zi,Y/prime
i,yi
<t/parenrightbig
(14)
whereŒ∏is all trainable parameters. Nis the total number of
training instances and Lis the length of each ground-truth
summary.
During testing, we utilize the prototype encoder to represent
prototypes and compute edit vectors. Then, the summarydecoder is used to generate directly a summary conditioningon the prototype representation and edit vector in Equation
(13).
IV . E XPERIMENTAL SETUP
A. Dataset
Following previous studies [14], [18], we conduct exper-
iments on a public large-scale Java dataset3provided by
LeClair et al. [14]. The raw dataset contains 5.1 million Java
methods, which is collected by Lopes et al. [28] from theSourcerer repository. Because the raw dataset contains a largenumber of samples (such as repeated and auto-generated code)that are not suitable for evaluating neural models, LeClair etal. cleaned and pre-processed the dataset.
SpeciÔ¨Åcally, they Ô¨Årst extracted Java methods and Javadocs
from the source code repository. Assuming the Ô¨Årst sentenceof the Javadoc describes the method‚Äôs behavior [29], theyextracted the Ô¨Årst sentence of the Javadoc as the summary of amethod and Ô¨Åltered out non-English samples. Considering theauto-generated and duplicate code might affect the evaluation,they removed these samples using heuristic rules [30] andadded unique, auto-generated code to the training set. Afterthat, they split camel case and underscore tokens and set themto lower case. Finally, they divided the dataset by project intotraining, validation, and test set, meaning that all methodsin one project are grouped into one set. They argued thatthe pre-processing of the dataset is necessary for evaluatingthe performance of neural models. The statistical results ofthe dataset are shown in Table II. Figure 4 shows the lengthdistribution of source code and summary on the test set.
B. Implementation Details
Our model is implemented based on the Pytorch
4frame-
work. We set word embedding and LSTM hidden states to
100 dimensions and 256 dimensions, respectively. We set thebatch size to 128 and train the model using Adam [31] withthe initial learning rate of 0.001. The learning rate is decayedwith a factor of 0.95 every epoch. To mitigate overÔ¨Åtting,
we use dropout with 0.2. To prevent exploding gradient, we
clip the gradients norm by 5. According to the statistics ofthe dataset in Table II and Figure 4, the maximum lengthsof code and summary are set to 100 and 15, respectively.The vocabulary sizes of the code and summary are 50,000and 50,000, respectively. The out-of-vocabulary tokens arereplaced by UNK. We train the model for a maximum of 30epochs and perform an early stop if the validation performancedoes not improve for 5 consecutive iterations. During thetesting phase, we use a beam search and set the beam sizeto 5. We conduct all experiments on two Nvidia GTX TITANXp GPUs with 12 GB memory. Each experiment is run threetimes and the average results are reported.
C. Evaluation Metrics
Following the previous studies [14], [18], [19], we evaluate
all approaches using the metric BLEU [20], METEOR [21],
ROUGE-L [22] and ROUGE-W [22]. We regard a generated
3http://leclair.tech/data/funcom/
4https://pytorch.org/summary ÀÜYas a candidate and a huamn-written summary Y
as a reference.
BLEU calculates the n-gram similarity between the gener-
ated sequence and reference sequence. The BLEU score ranges
from 1 to 100 as a percentage value. The higher the BLEU,the closer the candidate is to the reference. It computes then-gram precision of a candidate sequence to the reference:
BLEU‚àíN=BP¬∑exp/parenleftBigg
N/summationdisplay
n=1wnlogpn/parenrightBigg
(15)
wherepnis the ratio of length nsub-sequences in the
candidate that are also in the reference. In this paper, we reportthe BLEU1-BLEU4 scores. BP is brevity penalty.
METEOR calculates the similarity scores between a pair of
sentences by:
METEOR =/parenleftbig
1‚àíŒ≥¬∑frag
Œ≤/parenrightbig
¬∑P¬∑R
Œ±¬∑P+(1‚àíŒ±)¬∑R(16)
wherePandRare the unigram precision and recall, frag
is the fragmentation fraction. Œ±,Œ≤andŒ≥are three penalty
parameters whose default values are 0.9, 3.0 and 0.5, respec-
tively.
ROUGE-L computes F-score based on Longest Common
Subsequence (LCS). Suppose the lengths of ÀÜYandYarem
andn, then:
Plcs=LCS(X,Y)
m,Rlcs=LCS(X,Y)
n(17)
Flcs=/parenleftbig
1+Œ≤2/parenrightbig
PlcsRlcs
Rlcs+Œ≤2Plcs(18)
whereŒ≤=Plcs/RlcsandFlcsis the value of ROUGE-
L. ROUGE-W [29] is an improved version of ROUGE-L,which is based on Weighted Longest Common Subsequence
(WLCS).
V. E
XPERIMENTAL RESULTS
To evaluate our approach, in this section, we aim to answer
the following three research questions:
‚Ä¢RQ1: How does the E DITSUMperform compared to the
state-of-the-art neural baselines?
‚Ä¢RQ2: How does the E DITSUMperform compared to the
IR-based baselines?
‚Ä¢RQ3: Does E DITSUMperform better than previous ap-
proaches for tackling the keywords problem?
A. RQ1: EDITSUM vs. Neural Baselines
1) Baselines: To answer this research question, we compare
our approach E DITSUMto six state-of-the-art neural models.
‚Ä¢CODE-NN [12] is the Ô¨Årst neural network-based model
for code summarization task. It maps the source code
sequence into word embeddings, then uses an LSTM unitas a decoder to generate summaries, and employs theattention mechanism to introduce information from theword embeddings.
TABLE III: The performance of our model compared with baselines.
Approaches Params BLEU1 BLEU2 BLEU3 BLEU4 METEOR ROUGE-L ROUGE-W
Retrieve module - 32.06 17.83 14.39 12.87 28.62 36.82 25.31
LSI - 31.38 17.05 13.48 12.07 27.71 35.09 24.02
VSM - 31.91 17.52 14.02 12.70 28.26 36.21 24.81
NNGen - 33.48 18.86 14.99 13.44 29.97 38.57 26.07
CODE-NN 36.3M 32.23 14.71 8.558 6.090 29.35 37.64 25.85
DeepCom 37.9M 31.88 16.02 10.10 7.491 31.79 42.45 28.51
attendgru 37.7M 39.00 22.02 14.87 11.27 36.42 48.95 27.96
ast-attendgru 39.7M 39.32 22.19 14.98 11.42 36.99 49.40 33.58
Rencos 57.3M 34.40 19.82 14.34 11.74 34.53 46.35 31.64
Re2Com 28.4M 41.69 25.78 19.70 16.79 38.04 47.65 33.22
EDITSUM 26.4M 45.83 28.37 21.17 16.88 42.93 53.17 37.19
‚Ä¢DeepCom [13] is a seq2seq model with an attention
mechanism that uses LSTM units as the encoder and
decoder. It proposed a SBT algorithm to convert theAST into a token sequence. It is the Ô¨Årst model tointroduce structural information of source code into code
summarization.
‚Ä¢attendgru [14] is an encoder-decoder model with an
attention mechanism, which takes the code sequence as
input and the summary as output.
‚Ä¢ast-attendgru [14] is also a seq2seq model with an atten-
tion mechanism. Different from attendgru, it introducesthe structural information of the source code by usingan encoder to process the traversal sequence of AST. Itconcatenates the information from the two encoders asinput to the decoder and generates code summaries.
‚Ä¢Rencos [19] is a retrieval-based neural model that aug-
ments an attentional encoder-decoder model with the
retrieved two most similar code snippets for better sourcecode summarization.
‚Ä¢Re2Com [18] is a retrieval-based neural model that uses
the summary of the similar code snippet as an exemplar
to generate a code summary.
For a fair comparison, we re-implement the attendgru andast-attendgru based on LSTM units. The embedding size andLSTM states of all baselines are set to 256 dimensions.
2) Results: We calculate the BLEU, METEOR, and
ROUGE scores between the summaries generated by differentapproaches and human-written summaries. The experimentalresults are shown in Table III. We notice that CODE-NNperforms the worst among all approaches. This is becauseCODE-NN directly uses word embeddings as the input ofdecoder, and does not further extract the semantic informationfrom the source code. This shows that whether the semanticinformation of the code can be effectively mined has agreater impact on the performance of the code summarizationmodel. Both DeepCom and attendgru use the encoder-decoderframework, but DeepCom performs worse. This is becausethe traversal sequence of the AST input by DeepCom isabout 7 times longer than the token sequence of code inputby attendgru. This also veriÔ¨Åes the weakness of LSTM inprocessing long sequences [32]. The difference between ast-attendgru and attendgru is that the former introduces thestructural information in the AST, but the improvement islimited. This is because custom identiÔ¨Åers are removed fromthe AST used in ast-attendgru, which limits the structuralinformation in the AST. Both Rencos and Re
2Com combine
the information retrieval technology with neural networks, but
the former is less effective. Rencos retrieved two similar code
snippets from the corpus and directly used them as input to themodel. Re
2Com retrieved a similar code from the corpus, and
then sent the summary of the similar code into the model as anexemplar. The experimental results show that the summary ofsimilar code contains more valuable and reusable informationthan similar code that may contain noise. This also proves thatit is reasonable for us to use the summaries of similar code asthe prototypes.
From Table III, we can see that E
DITSUMperforms the
best among all neural models, which improves the state-of-the-
art Re2Com by 9.93% in BLEU1, 12,85% in METEOR and
11.58% in ROUGE-L. In particular, compared with Rencos
and Re2Com, E DITSUMperforms much better on all metrics.
This is because Rencos and Re2Com are the ensemble neural
models, and they directly enter the retrieved results and theinput code into the model. While E
DITSUMregards the pro-
totype summary as an initial draft for post-generation, whichprovides many reusable patternized words. So, E
DITSUM
focuses on learning how to revise the prototype based onthe differences between the input code and the similar code.Besides, the number of parameters of E
DITSUMis the smallest
among all baselines. It also shows E DITSUMis efÔ¨Åcient.
Compared to other metrics, we Ô¨Ånd that E DITSUM has
a small improvement on BLEU4. This is because the im-provement by E
DITSUMmainly comes from predicting more
keywords. However, the average length of the summaries inthe test set is 7, and these keywords are mainly 1-3 words.Therefore, E
DITSUMhas a great improvement on BLEU1-
BLEU3, and a relatively small improvement on BLEU4.
B. RQ2: EDITSUM vs. IR Baselines
1) Baselines: To answer this research question, we compare
our approach E DITSUMto four IR-based baselines.
‚Ä¢Retrieve module is a component of our approach, whose
details are described in Section III-A. We use the sum-
mary of similar code as output directly.
‚Ä¢Latent Semantic Indexing (LSI) [8] is an IR technique
to analyze the semantic relationship between terms in
documents. Given a code snippet, we use LSI to retrievethe similar code from the training set and use its summaryas output. The retrieval metric is the cosine distance ofthe 500-dimensional LSI vector of the source code.
‚Ä¢Vector Space Model (VSM) [8] represents the code
as a vector using Term Frequency-Inverse Document
Frequency (TF-IDF). It uses cosine similarity to retrievethe summary of the similar code from the training set.
‚Ä¢NNGen [33] is an approach for generating commit
messages based on nearest neighbors algorithm. It Ô¨Årstencodes code changes into the form of a ‚Äùbag of words‚Äù,
then uses the cosine distance to select the nearest kcode
changes. Finally, it chooses the message of the code
change with the highest BLEU score as the Ô¨Ånal result.In this paper, we set kas 5.
2) Results: We calculate the BLEU, METEOR, and
ROUGE scores between the summaries generated by differ-ent IR-based approaches and human-written summaries. Theexperimental results are shown in Table III. From Table III,the Retrieve module performs better compared with otherIR-based approaches. This shows that it is effective for ourRetrieve module to retrieve similar code based on the lexicalsimilarity. LSI and VSM use different ways (LSI vectorsand TF-IDF) to map source code into a vector, and theirperformance is similar. Compared with LSI and VSM, NNGendirectly uses BLEU score as the retrieval metric, so it gets ahigher BLEU score. It is worth noting that the BLEU3 andBLEU4 score of the IR-based baselines even exceeds thatof some neural models (i.e, CODE-NN and DeepCom). Thisshows that the summaries output by the IR-based approacheshave better precision scores of the 3-gram phrase and 4-gramphrase. This proves that the retrieved summaries contains alot of valuable words, which can be used to generate thenew summaries. However, there is still a gap between thesummaries output by the IR-based approaches and the human-written summaries due to the differences between the similarcode and the input code.
Our model signiÔ¨Åcantly outperforms IR-based baselines
in terms of all metrics, which proves the effectiveness ofthe our Edit module. Compared to the IR-based baselines.our approach E
DITSUM treats the retrieved summary as a
prototype, and then revise the prototype conditioning on thesemantic differences between similar code and input code. By
combining the advantages of neural networks and IR-basedapproaches, E
DITSUMachieves the best performance.
C. RQ3: Tackling keywords problem
1) Metrics: According to the information retrieve technolo-
gies [24], the keywords in the summaries often are informative
and are more likely to be low-frequency words. The statisticsshow 94.8% of tokens in the summary vocabulary of thedataset have a frequency of less than 100. However, as wedescribed in Section I and II, previous neural network modelsTABLE IV: The number of correctly generated low-frequencywords (the rate of keywords in parentheses)
Approaches ‚â§10 ‚â§20 ‚â§50 ‚â§100
ast-attendgru 262 624 1,575 2,801
Rencos 410 948 2,254 3,791
Re2Com 422 (64.69%) 1,093 (75.21%) 2,808 4,886
EDITSUM 476 (74.58%) 1270 (86.38%) 3066 5260
TABLE V: The results (standard deviation in parentheses) ofhuman evaluation.
Approaches Naturalness Informativeness Usefulness
Retrieve module 1.790 (0.68) 0.778 (0.59) 0.612 (0.12)
ast-attendgru 1.713 (0.76) 1.288 (0.79) 1.108 (0.89)
Rencos 1.822 (0.73) 1.320 (0.36) 1.140 (0.29)
Re2Com 1.860 (0.64) 1.465 (0.52) 1.341 (0.23)
EDITSUM 1.933 (0.31) 1.802 (0.348) 1.790 (0.29)
perform poorly on predicting low-frequency words. To mea-sure the ability of our approach on generating keywords, wecollect all correctly predicted words on the test set, calculatethe frequency of these words on the training set, and countthe words with frequencies less than 10, 20, 50, and 100. Thecorrectly predicted words refers to the overlap between thegenerated summary and the reference summary. For the wordswith frequencies less than 10 and 20, we manually counted therate of keywords among these words.
2) Results: The statistical results are shown in Table IV.
Compared with ast-attendgru, Rencos and Re
2Com perform
better on predicting the low-frequency words. This shows
that the summaries of similar code snippets contain a lot of
reusable information. We also can see that E DITSUM can
predict more low-frequency words and more keywords thanother baselines, which means that E
DITSUM alleviates the
problem of predicting keywords. The goal of learning howto revise a prototype makes our model focuses to generate
more keywords.
D. Human Evaluation
1) Metrics: Although the metrics in Section IV-C can
calculate the lexical similarity between the generated sum-
maries and the reference summaries, they can not reÔ¨Çect thesimilarity at the semantic level. Moreover, the ultimate goal ofthe automatic code summarization model is to help develop-ers understand the functionality of the program. Therefore,we conduct a human evaluation to measure the quality ofsummaries generated by different baselines on the test set.Following the previous work [18], we measure three aspects,including the naturalness (grammaticality and Ô¨Çuency of the
generated summaries), informativeness (the amount of content
carried over from the input code to the generated summaries,ignoring Ô¨Çuency of the text), and usefulness (what extent the
generated summary is useful for developers to understandcode). All three scores are integers, ranging from 0 to 2
(from bad to good). We invite 10 volunteers with 3-5 yearsof Java development experience and excellent English ability
for 1 hour each to evaluate the generated summaries in the
form of a questionnaire. The 10 volunteers are computerscience Ph.D. students and are not co-authors of this paper.We randomly select 500 samples generated by Ô¨Åve models(100 from Retrieve module, 100 from ast-attendgru, 100 fromRe
2Com, 100 from Rencos, and 100 from E DITSUM). The 500
samples are divided into Ô¨Åve groups, with each questionnairecontaining one group. We randomly list the summary pairs and
the corresponding input code on the questionnaire and removetheir labels. Each group is evaluated by two volunteers, andthe Ô¨Ånal result of a pair of summaries is the average of twovolunteers. V olunteers are allowed to search the Internet forrelated information and unfamiliar concepts.
2) Results: The evaluation results are shown in Table V.
The standard deviations of all approaches are small, indicatingthat their scores are about the same degree of concentration.Our model is better than all baselines in three aspects. TheRetrieve module can generate more Ô¨Çuent summaries than the
ast-attendgru because its outputs are directly retrieved from thetraining set. We also notice that the scores on informativeness
of Ô¨Åve models are higher than those on usefulness. This indi-
cates that the generated summaries really contain information
about the code, but this information may be redundant ornot completely correct, so they only provide limited help fordevelopers to understand the code.
VI. D
ISCUSSION
A. Qualitative Analysis
We present three examples generated by different ap-
proaches from the test set, as shown in Table VI. These exam-ples show that the summaries generated by E
DITSUMhave a
very high semantic similarity with human-written summaries.From Table VI, previous models cannot generate keywords
accurately, and the generated summaries cannot reÔ¨Çect the
intention of the programs concisely. For example, in case 1,the aim of this program is to set the color to a darker shade.However, Re
2Com simply describes it as setting the selected
color to the speciÔ¨Åed color, which is useless for developers tounderstand the program. While our model E
DITSUMperforms
well in both patternized words (e.g. set, to) and keywords (e.g.darker shade). Besides, compared with Retrieve module, wecan Ô¨Ånd that our Edit module can make good use of the patternin the prototype and revise it based on the semantics of theinput code.
B. Performance for Different Lengths
We also analyze the performance of different models on
different code and summary lengths (number of tokens). We
calculate the BLEU score for each sample on the test set andaverage the scores by length. The experimental results areshown in Figure 5 and Figure 6. From these Ô¨Ågures, we canobserve that E
DITSUMoutperforms the Re2Com with different
code and summary lengths. As the lengths of the code and
summary increase, E DITSUMkeeps a stable improvement over
Re2Com. The performance of our model is always better thanTABLE VI: Examples of generated summaries.
Case ID Example
1public void drawSelected(){
if(unselectedColor instanceof Color){
setPaint(((Color)unselectedColor).darker());
}else{
setPaint(java.awt.Color.yellow);
}
}
Retrieve Module: set the series colors to the chart
ast-attendgru: draws the selected set of the selected color
Rencos: p method description p
Re2Com: set the selected color to the speciÔ¨Åed color
EDITSUM: set the color to a darker shade
Human-written: set the color to a darker shade
2public void close() throws IOException {
this.servletInputStream.close();
}
Retrieve Module: close the resources used by the work factory
ast-attendgru: close the underlying servlet
Rencos: close the server
Re2Com: close the resources used by the work factory
EDITSUM: close the underlying stream
Human-written: close the underlying stream
3public int read() throws IOException{
if(chunkSize==-1){
return -1;
}
if(pos<chunkSize){
pos++;
return in.read();
}else{
readChunksize();pos=0;if(chunkSize<=0){
return -1;
}pos=1;
return in.read();
}
}
Retrieve Module: read some bytes from the stream
ast-attendgru: reads the next byte
Rencos: reads the next byte
Re2Com: read some bytes from the stream
EDITSUM: read the next byte of data from this input stream
Human-written: read the next byte of data from this input stream
15202530
25 50 75 100
Code LengthBLEU Score (%)variable EditSum Re2Com
Fig. 5: BLEU scores for different code lengths.
the baseline on the complicated code snippets with a relatively
large length. This also shows the robustness of our model.
C. Threats to V alidity
There are three main threats to the validity of our model.
Firstly, we only conducted experiments on a Java dataset.
Although Java may not be representative of all programminglanguages, the experimental dataset is large and safe enough
010203040
10 20 30 40
Summary LengthBLEU Score (%)variable EditSum Re2Com
Fig. 6: BLEU scores for different summary lengths.
to show the effectiveness of our model. Previous studies [18],
[19] also only conducted experiments on this Java dataset.Besides, our model uses only language-agnostic features andcan be applied in a drop-in fashion to other programminglanguages. Secondly, we cannot guarantee that the scoresof human evaluation are fair. To mitigate this threat, weevaluate every code-summary pair by two evaluators and usethe average score of the two evaluators as the Ô¨Ånal result.Finally, the Retrieve module retrieves similar code based onlexical similarity. This may result in retrieved code and inputcode being similar only at the lexical level, but their summariesare quite different. To address this threat, we use a large-scale Java dataset (2M) to increase the scale and diversity of
retrieval corpus. The experimental results in Table III prove
that the performance of our retrieval module is comparable tothe performance of some neural network models (CODE-NN,DeepCom). We also propose an Edit module to alleviate thisthreat through revising the prototype according to the semanticdifferences between input code and retrieved code.
VII. R
ELATED WORK
As an integral part of software development, code sum-
maries describe the functionalities of source code. A conciseand clear summary can help developers quickly understand thepurpose of the program. However, it is very time-consuming
and labor-intensive to write a summary manually. Therefore,
more and more researchers are exploring automatic codesummarization technology. Automatic code summarization ap-proaches vary from manually-crafted templates [?], [6], [34],[35], information retrieval [7]‚Äì[10] and neural networks [12]‚Äì[14], [18], [19].
A. Template-based Approaches
Early studies generated code summaries based on template-
based approaches. Given the signature and body of a method,
Sridhara et al. [?] identiÔ¨Åed the content for the summary and
generated natural language text that summarizes the method‚Äôsactions. Moreno et al. [6] determined the class and methodstereotypes and used them, in conjunction with heuristics,to select the information to be included in the summaries.Then they generated the summaries using existing lexical-ization tools. McBurney et al. [34] utilized the PageRankalgorithm [36] to select the important methods in the givenmethod‚Äôs context and used a template-based system to generateEnglish descriptions of Java methods. Generating summariesbased on templates can improve the readability of summaries,but deÔ¨Åning templates is a time-consuming task and requiresextensive domain knowledge. Besides, templates of differentprojects cannot be migrated to each other.
B. IR-based Approaches
Information retrieval technologies are also widely used in
automatic code summarization. Haiduc et al. [8] represented
the source code as a vector using two algorithms (VSM and
LSI) and retrieved relevant terms from a code corpus. Theserelevant terms were integrated into a code summary. Eddyet al. [7] proposed a hierarchical probabilistic model that
retrieved relevant terms from the code corpus and fused theminto the summaries. Wong et al. [10] applied a token-basedcode clone detection tool to retrieve similar code snippetsin large-scale software repositories. Although promising, IR-based approaches have two main limitations: Ô¨Årst, they failto extract accurate keywords used to identify similar codesnippets when identiÔ¨Åers and methods are poorly named.Second, they rely on the size and diversity of the retrievalcorpus.
C. Neural Network-based Approaches
Recently, more and more neural networks are applied to
generate code summaries. Iyer et al. [12] used a Recurrent
Neural Network (RNN) [37] with an attention mechanism as adecoder to generate code summaries and achieved good resultson C# and SQL summaries. Because source code contains richstructural information, Hu et al. [13] proposed a neural modelnamed DeepCom to utilize the structural information of code.They proposed a SBT algorithm to convert the AST into atoken sequence, then designed a seq2seq model to generatesummaries for Java methods based on the AST sequence.LeClair et al. [14] proposed two neural models (attendgruand ast-attendgru) to generate the summaries by combiningthe sequence information and structure information of thecode. Wei et al. [18] proposed an exemplar-based summarygeneration framework that used the summary of the similar
code snippet as an exemplar to assist in generating a target
summary. Zhang et al. [19] proposed a retrieval-based neuralmodel that augments an attentional seq2seq model with theretrieved two most similar code snippets for better source codesummarization.
Different from the retrieval-based neural models [18], [19],
we regard the retrieved summary as a prototype and combinethe pattern in prototype with semantic information of inputcode. While previous models formulate it as a multi-sourceseq2seq task, in which the input code, prototype, and similarcode are all fed to the decoder. The experimental results alsoprove the superiority of our approach.
VIII. C
ONCLUSION AND FUTURE WORK
In this paper, we argue that code sumaries are composed of
patternized words and keywords, and emphasize the shortcom-
ings of previous models in predicting keywords. To alleviate
this problem, we propose a retrieve-and-edit approach named
EDITSUMfor code summarization. E DITSUMcontains two
modules. A Retrieve module for retrieving the similar code
snippet. An Edit module treats the summary of similar code asa prototype, and combine the pattern in prototype and semanticinformation of input code to generate a target summary. We
conducted extensive experiments on a large-scale Java dataset.
The experimental results show that E
DITSUMsubstantially
outperforms the state-of-the-art neural baselines and the IR-based baselines. Human evaluation and case analysis provethat E
DITSUM can generate concise and informative sum-
maries, which can effectively help developers understand theintent of the programs. The analysis of the experimentalresults shows that E
DITSUMcan generate more keywords and
performs well on code and summaries of different lengths.In the future, we will explore how to generate standard andmeaningful code summaries for various software projects.
ACKNOWLEDGMENTS
This research is supported by the National Key R&D
Program of China under Grant No. 2020AAA0109400, andthe National Natural Science Foundation of China under GrantNos. 62072007, 61832009, 61620106007.
R
EFERENCES
[1] X. Xia, L. Bao, D. Lo, Z. Xing, A. E. Hassan, and S. Li, ‚ÄúMeasuring
program comprehension: A large-scale Ô¨Åeld study with professionals,‚Äù
IEEE Transactions on Software Engineering, vol. 44, no. 10, pp. 951‚Äì976, 2017.
[2] S. C. B. de Souza, N. Anquetil, and K. M. de Oliveira, ‚ÄúA study of the
documentation essential to software maintenance,‚Äù in Proceedings of
the 23rd annual international conference on Design of communication:documenting & designing for pervasive information, 2005, pp. 68‚Äì75.
[3] A. J. Ko, B. A. Myers, M. J. Coblenz, and H. H. Aung, ‚ÄúAn exploratory
study of how developers seek, relate, and collect relevant information
during software maintenance tasks,‚Äù IEEE Transactions on software
engineering, vol. 32, no. 12, pp. 971‚Äì987, 2006.
[4] G. Sridhara, E. Hill, D. Muppaneni, L. Pollock, and K. Vijay-Shanker,
‚ÄúTowards automatically generating summary comments for java meth-ods,‚Äù in Proceedings of the IEEE/ACM international conference on
Automated software engineering, 2010, pp. 43‚Äì52.
[5] S. C. B. de Souza, N. Anquetil, and K. M. de Oliveira, ‚ÄúA study of the
documentation essential to software maintenance,‚Äù in Proceedings of
the 23rd annual international conference on Design of communication:
documenting & designing for pervasive information, 2005, pp. 68‚Äì75.
[6] L. Moreno, J. Aponte, G. Sridhara, A. Marcus, L. Pollock, and K. Vijay-
Shanker, ‚ÄúAutomatic generation of natural language summaries for java
classes,‚Äù in 2013 21st International Conference on Program Compre-
hension (ICPC). IEEE, 2013, pp. 23‚Äì32.
[7] B. P. Eddy, J. A. Robinson, N. A. Kraft, and J. C. Carver, ‚ÄúEvaluating
source code summarization techniques: Replication and expansion,‚Äù in
2013 21st International Conference on Program Comprehension (ICPC) .
IEEE, 2013, pp. 13‚Äì22.
[8] S. Haiduc, J. Aponte, L. Moreno, and A. Marcus, ‚ÄúOn the use of
automated text summarization techniques for summarizing source code,‚Äùin2010 17th Working Conference on Reverse Engineering . IEEE, 2010,
pp. 35‚Äì44.
[9] E. Wong, T. Liu, and L. Tan, ‚ÄúClocom: Mining existing source code for
automatic comment generation,‚Äù in 2015 IEEE 22nd International Con-
ference on Software Analysis, Evolution, and Reengineering (SANER).
IEEE, 2015, pp. 380‚Äì389.
[10] E. Wong, J. Yang, and L. Tan, ‚ÄúAutocomment: Mining question and an-
swer sites for automatic comment generation,‚Äù in 2013 28th IEEE/ACM
International Conference on Automated Software Engineering (ASE) .
IEEE, 2013, pp. 562‚Äì567.[11] D. Gros, H. Sezhiyan, P. Devanbu, and Z. Yu, ‚ÄúCode to comment ‚Äútrans-
lation‚Äù: Data, metrics, baselining & evaluation,‚Äù in 2020 35th IEEE/ACM
International Conference on Automated Software Engineering (ASE) .
IEEE, 2020, pp. 746‚Äì757.
[12] S. Iyer, I. Konstas, A. Cheung, and L. Zettlemoyer, ‚ÄúSummarizing source
code using a neural attention model,‚Äù in Proceedings of the 54th Annual
Meeting of the Association for Computational Linguistics (V olume 1:
Long Papers), 2016, pp. 2073‚Äì2083.
[13] X. Hu, G. Li, X. Xia, D. Lo, and Z. Jin, ‚ÄúDeep code comment gener-
ation,‚Äù in 2018 IEEE/ACM 26th International Conference on Program
Comprehension (ICPC). IEEE, 2018, pp. 200‚Äì20 010.
[14] A. LeClair, S. Jiang, and C. McMillan, ‚ÄúA neural model for gener-
ating natural language summaries of program subroutines,‚Äù in 2019
IEEE/ACM 41st International Conference on Software Engineering
(ICSE). IEEE, 2019, pp. 795‚Äì806.
[15] I. Sutskever, O. Vinyals, and Q. V . Le, ‚ÄúSequence to sequence learning
with neural networks,‚Äù Advances in Neural Information Processing
Systems, vol. 27, pp. 3104‚Äì3112, 2014.
[16] M. Allamanis, E. T. Barr, P. Devanbu, and C. Sutton, ‚ÄúA survey
of machine learning for big code and naturalness,‚Äù ACM Computing
Surveys (CSUR), vol. 51, no. 4, pp. 1‚Äì37, 2018.
[17] A. Hindle, E. T. Barr, M. Gabel, Z. Su, and P. Devanbu, ‚ÄúOn the
naturalness of software,‚Äù Communications of the ACM, vol. 59, no. 5,
pp. 122‚Äì131, 2016.
[18] B. Wei, Y . Li, G. Li, X. Xia, and Z. Jin, ‚ÄúRetrieve and reÔ¨Åne:
exemplar-based neural comment generation,‚Äù in 2020 35th IEEE/ACM
International Conference on Automated Software Engineering (ASE) .
IEEE, 2020, pp. 349‚Äì360.
[19] J. Zhang, X. Wang, H. Zhang, H. Sun, and X. Liu, ‚ÄúRetrieval-based
neural source code summarization,‚Äù in 2020
 IEEE/ACM 42nd Interna-
tional Conference on Software Engineering (ICSE). IEEE, 2020, pp.1385‚Äì1397.
[20] K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu, ‚ÄúBleu: a method for
automatic evaluation of machine translation,‚Äù in Proceedings of the 40th
annual meeting of the Association for Computational Linguistics , 2002,
pp. 311‚Äì318.
[21] S. Banerjee and A. Lavie, ‚ÄúMeteor: An automatic metric for mt evalua-
tion with improved correlation with human judgments,‚Äù in Proceedings
of the acl workshop on intrinsic and extrinsic evaluation measures formachine translation and/or summarization, 2005, pp. 65‚Äì72.
[22] C.-Y . Lin, ‚ÄúRouge: A package for automatic evaluation of summaries,‚Äù
inText summarization branches out, 2004, pp. 74‚Äì81.
[23] S. Robertson and H. Zaragoza, The probabilistic relevance framework:
BM25 and beyond. Now Publishers Inc, 2009.
[24] S. Qaiser and R. Ali, ‚ÄúText mining: use of tf-idf to examine the
relevance of words to documents,‚Äù International Journal of Computer
Applications, vol. 181, no. 1, pp. 25‚Äì29, 2018.
[25] P. Jaccard, ‚ÄúThe distribution of the Ô¨Çora in the alpine zone. 1,‚Äù New
phytologist, vol. 11, no. 2, pp. 37‚Äì50, 1912.
[26] K. Guu, T. B. Hashimoto, Y . Oren, and P. Liang, ‚ÄúGenerating sentences
by editing prototypes,‚Äù Transactions of the Association for Computa-
tional Linguistics, vol. 6, pp. 437‚Äì450, 2018.
[27] S. Hochreiter and J. Schmidhuber, ‚ÄúLong short-term memory,‚Äù Neural
computation, vol. 9, no. 8, pp. 1735‚Äì1780, 1997.
[28] C. Lopes, ‚ÄúUci source code data sets,‚Äù http://www. ics. uci. edu/-
lopes/datasets/, 2010.
[29] D. Kramer, ‚ÄúApi documentation from source code comments: a case
study of javadoc,‚Äù in Proceedings of the 17th annual international
conference on Computer documentation, 1999, pp. 147‚Äì153.
[30] K. Shimonaka, S. Sumi, Y . Higo, and S. Kusumoto, ‚ÄúIdentifying auto-
generated code by using machine learning techniques,‚Äù in 2016 7th
International Workshop on Empirical Software Engineering in Practice
(IWESEP). IEEE, 2016, pp. 18‚Äì23.
[31] D. P. Kingma and J. Ba, ‚ÄúAdam: A method for stochastic optimization,‚Äù
arXiv preprint arXiv:1412.6980, 2014.
[32] P. Koehn and R. Knowles, ‚ÄúSix challenges for neural machine trans-
lation,‚Äù in Proceedings of the First Workshop on Neural Machine
Translation, 2017, pp. 28‚Äì39.
[33] Z. Liu, X. Xia, A. E. Hassan, D. Lo, Z. Xing, and X. Wang, ‚ÄúNeural-
machine-translation-based commit message generation: how far are we?‚ÄùinProceedings of the 33rd ACM/IEEE International Conference on
Automated Software Engineering, 2018, pp. 373‚Äì384.
[34] P. W. McBurney and C. McMillan, ‚ÄúAutomatic source code summa-
rization of context for java methods,‚Äù IEEE Transactions on Software
Engineering, vol. 42, no. 2, pp. 103‚Äì119, 2015.
[35] Y . Oda, H. Fudaba, G. Neubig, H. Hata, S. Sakti, T. Toda, and
S. Nakamura, ‚ÄúLearning to generate pseudo-code from source code using
statistical machine translation,‚Äù in 2015 30th IEEE/ACM International
Conference on Automated Software Engineering (ASE). IEEE, 2015,
pp. 574‚Äì584.
[36] S. Kamvar, T. Haveliwala, and G. Golub, ‚ÄúAdaptive methods for the
computation of pagerank,‚Äù Linear Algebra and its Applications, vol.
386, pp. 51‚Äì65, 2004.
[37] T. Mikolov, M. KaraÔ¨Å ¬¥at, L. Burget, J. ÀáCernock `y, and S. Khudanpur,
‚ÄúRecurrent neural network based language model,‚Äù in Eleventh annual
conference of the international speech communication association, 2010.
