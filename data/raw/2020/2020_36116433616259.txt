Discovering Parallelisms in Python Programs
Siwei Wei
StateKey Laboratory ofComputer
Science,InstituteofSoftware,Chinese
Academy ofSciences, and University
ofChineseAcademy ofSciences
Beijing, China
weisw@ios.ac.cnGuyang Song
AntGroup
Beijing, China
guyang.sgy@antgroup.comSenlinZhu
AntGroup
Hangzhou, China
senlin.zsl@antgroup.com
RuoyiRuan
AntGroup
Hangzhou, China
ruoyi.ruanry@antgroup.comShihao Zhu
StateKey Laboratory ofComputer
Science,InstituteofSoftware,Chinese
Academy ofSciences, and University
ofChineseAcademy ofSciences
Beijing, China
zhush@ios.ac.cnYanCai∗
StateKey Laboratory ofComputer
Science,InstituteofSoftware,Chinese
Academy ofSciences, and University
ofChineseAcademy ofSciences
Beijing, China
yancai@ios.ac.cn
ABSTRACT
Parallelizationisapromisingwaytoimprovetheperformance
of Python programs. Unfortunately, developers may miss paral-
lelization possibilities, because they usually do not concentrate
on parallelization. Many approaches have been proposed to par-
allelizePythonprogramsautomatically,however,theyareeither
domain-speciﬁcorrequiremanualannotation.Thustheycannot
solve the problem well in general. In this paper, we propose PyPar,
aneﬀectivetoolaimingatdiscoveringparallelizationpossibilities
inreal-worldPythonprograms. PyPardoesn’tneedmanualannota-
tion and is universally applicable. It ﬁrst drives a data-dependence
analysisto determinewhether two piecesofcodecanrun concur-
rently. The key is the use of a graph-theoretic approach. Next, it
adopts a dynamic selection strategy to eliminate ineﬃcient par-
allelisms. Finally, PyParproduces a parallelism report as well as
a referential parallelized program, which is built by PyParusing
one of the three parallelization methods (thread-based, process-
based, and Ray-based). We have implemented a prototype of Py-
Parandevaluateditonsixwell-designedwidely-usedreal-world
Pythonpackages: Scikit-Image,SciPy,librosa,trimesh,Scikit-learn
andseaborn.Intotal,1,240functionsaretested,and PyParfound
127parallelizablefunctionsamongthem.Basedonmanualﬁlter-
ing,only7ofthemarefalsepositives(i.e.,a94.5%precision).The
remaining120areparallelizable(almost10%amongallfunctions
under test), and most of them can be eﬃciently sped up by gaining
an acceleration of up to 90% , with an average of 44%. The accel-
eration in practice is close to theoretical estimation. The results
show that even well-designed practical Python programs can be
∗Corresponding author
ESEC/FSE ’23, December 3–9, 2023, San Francisco, CA,USA
©2023 Copyright heldby theowner/author(s).
ACM ISBN 979-8-4007-0327-0/23/12.
https://doi.org/10.1145/3611643.3616259further parallelized for speeding up, and PyParcan bring eﬀective
andeﬃcient parallelization on real-world Python programs.
CCSCONCEPTS
•Softwareanditsengineering →Softwarelibrariesandreposito-
ries;•Computing methodologies →Parallel algorithms .
KEYWORDS
Parallelism,Python,Ray
ACM Reference Format:
Siwei Wei, Guyang Song,SenlinZhu,Ruoyi Ruan,ShihaoZhu,and YanCai
.2023.DiscoveringParallelisms inPythonPrograms.In Proceedingsof the
31stACMJointEuropeanSoftwareEngineeringConferenceand Symposium
on the Foundations of Software Engineering (ESEC/FSE ’23), December 3–9,
2023, San Francisco, CA, USA. ACM, New York, NY, USA, 13pages.https:
//doi.org/10.1145/3611643.3616259
1 INTRODUCTION
Python [46–48] is a widely used high-level interpreted language.
It is becoming more and more popular [ 10]. Python has a vast
community and an increasing number of open-source packages,
making it user-friendly and accessible. It is now used in various
areas,suchasmachinelearning,scientiﬁccomputing,andimage
processing.
However,Python’spoorperformancehasalwaysbeenoneofits
majorshortcomings[ 22,23,50].TheinterpretativenatureofPython
improves its ﬂexibility but makes it hundreds of times slower than
compiled languages. Also, Python has a global interpreter lock
(GIL)[18].GILpreventsPythonbytecodefrombeinginterpreted
ondiﬀerentthreadsconcurrently,makingitdiﬃculttobeneﬁtfrom
thread-basedparallelization.
Parallelprocessing[ 16,23,24,57]isoneofthemostimportant
waystoimprovetheperformanceofPythonprogramsbyleveraging
themulti-coreprocessingcapabilityofmoderncomputers.There
areawealthofPythonparallelizationframeworks,whichwillbe
introducedinSection 2.1.Eachof these frameworkshasitsmerits
and drawbacks. These frameworks make parallel processing in
Thiswork islicensedunderaCreativeCommonsAttribution4.0Interna-
tional License.
832
ESEC/FSE ’23, December3–9, 2023,San Francisco, CA, USA SiweiWei, Guyang Song,SenlinZhu, Ruoyi Ruan, Shihao Zhu, andYanCai
sp1 = fft (in1, fshape, axes=axes )
sp2 = fft (in2, fshape, axes=axes )
ret = ifft (sp1 * sp2, fshape, axes=axes )for i, sigma in enumerate (sigmas ):
    lambda1, lambdas = \
        compute_hessian_eigenvalues (
            image, sigma,
    sorting= "abs" ,
    mode=mode, cval=cval )
    ...
(a)
(b)
Figure1:Parallelismsinreal-worldpackagesfoundby PyPar
Python easier. Nevertheless, the key issue remains as to where and
how to parallelize a (piece of) Python program, and it is still left to
programmers.
ManyautomaticparallelizationmethodsforPythonhavebeen
proposed, aiming at transforming serial programs into parallelized
versionsautomatically.AutoMPHC[ 41]andAutoParallel[ 39]ap-
ply the polyhedral optimization technique [ 20,25,37] to Python
programs. QuantCloud [ 59] and HPAT [ 44] aim at parallelizing
domain-speciﬁc Python programs. Pydron [ 32] uses a dynamic
scheduling strategy to parallelize tasks during execution. However,
thesemethodsallhavecertaindrawbackssuchasinvolvingman-
ual annotations, causing large overhead, and being designed for
domain-speciﬁcprogramsonly.Thesedrawbacksreducetheuseful-
ness of the above-mentioned methods, hindering them from being
appliedto generalreal-world Pythonprograms.
Whenwritingprograms,programmersusuallyconcentratemore
on correctness and readability than the possibility of improving
performancebyparallelization.Also,someprogrammersmaybe
domainexpertswhoareprofessionalincertainareasbutarenot
so familiar with parallelization techniques. As a consequence, pro-
grammerscanmissalargeamountofparallelizationpossibilities.
This happens even in widely-used Python packages. Figure 1gives
two examples, where the pieces of code in red dotted-line boxes
are parallelizable. Speciﬁcally, Figure 1(a) is a piece of code in
function ﬁlters.frangi from scikit-image [ 45], which implements
amulti-scaleﬁlteringalgorithm.Theparallelismshowsthecom-
putation of eigenvalues at diﬀerent scales (i.e., diﬀerent sigmas)
can be done concurrently. Figure 1(b) shows three lines in function
signal._freq_domain_conv from SciPy [ 49]. It calculates the convo-
lution of two signals. Fast Fourier transformations (FFT) of two
inputsignals are ﬁrstcalculated,andthey can run concurrently.
In this paper, we propose PyPar, an eﬀective and easy-to-use
toolfordiscoveringparallelizationpossibilitiesinPythonprograms.
Compared with existing automatic parallelization methods, PyPar
isfastandfreeofmanualannotation.Moreimportantly, PyParis
universally applicable to real-world Pythonpackages.
PyParaimstoﬁndparallelismsthatdon’tchangethebehavior
oftheoriginalprogram. PyPardiscoversprogramsegmentsthat
canrunconcurrentlybasedondatadependence[ 55].Firstly, PyPar
decomposesthetargetprogramintosmallpiecescalledprogram
units. Then, by utilizing data dependence analysis, a relation Δbe-
tweenprogramunitsisdeﬁned,whichindicateswhetheraprogram
unitshouldbeexecutedbeforeanother.Afterthat,programunitsthat don’t have to run serially are selected, and they are composed
intotasksusingheuristicstrategies.Meanwhile,thetargetprogram
isexecutedand timedtodeterminetherunningtime ofeachpart.
Basedonthetimingresults, PyParselectstasksthatareworthwhile
toparallelize.Finally,areportcontainingthediscoveredparallelism
andexpectedtimesavingisgenerated,andaparallelizedversion
ofthe target program isproduced.
Amongtheproceduresof PyPar,thekeypointistoﬁndparal-
lelism using the extracted dependence relation. To achieve this, we
proposeagraph-theoreticapproach.Firstly,adependencegraph
representing the dependence relation between program units is
built. For sequentially executed program units, we assign depth
tothemandﬁndparallelismaccordingtothedepth.Forprogram
units in a loop, we use strongly connected components (SCC) to
determine whether a program unit can run concurrently with it-
self in diﬀerent iterations. The soundness and completeness of the
proposedalgorithminﬁndingparallelismsusingthedependence
relation are provedinSection 3.4.
Wehaveimplementedaprototypetool PyParandevaluatedit
on six widely-used and well-designed real-world Python packages.
PyParfound almost 10% of all 1,240 functions under test to be
parallelizable,withaprecisionof94.5%.Afterparallelizingthem,
we gainup to 91% speed acceleration with anaverage of44%. The
evaluation result shows (1) there are lots of parallelization possibil-
ities in real-world Python programs; (2) PyParcan eﬀectively ﬁnd
parallelisms, and (3) the parallelisms found by PyParcan improve
the performance ofthe target programs.
In summary,the contributionof this paper isthree-fold:
•We propose an eﬀective method to detect parallelization
possibilities inreal-world Pythonprograms.
•We present two proportions to show the soundness and
completenessofour algorithms.
•We have implemented a prototype tool PyPar(available
athttps://github.com/PyParTool/PyPar ).Weconductedan
evaluationonsixwidely-usedPythonpackagesandfound
120parallelizable functions.
2 BACKGROUNDAND RELATED WORK
Inthissection,wewillintroduce(1)parallelcomputingframeworks
in Python and (2) automatic parallelization techiques for compiled
languagesandPython.The latter willbe our focus.
2.1 ParallelComputing in Python
In Python, there are mainly two types of parallel processing meth-
ods: process-based ones and thread-based ones. Process-based par-
allelizationusesprocessestoexecuteparalleltasks.Itcanintroduce
extra overhead because creating and recycling processes can be ex-
pensiveandtransferringlargedataamongprocessesalsocostsalot.
Incomparison,threadsarelight-weighted,andthereisnocostto
transfer data between threads. However, there is global interpreter
lock (GIL)[ 18] inPython, which makessure that therewill be only
onethreadexecutingPythonbytecodeatatime.1GILessentially
prevents two threads from executing concurrently. Thus threads
1Actually, it is the cpython interpreter that has GIL. But as cpython is the default and
mostwidely used implementation of Python,weuse the notionin thispaper.
833DiscoveringParallelismsin PythonPrograms ESEC/FSE ’23, December3–9, 2023,San Francisco, CA, USA
are useless for accelerating Python programs, and they are mainly
usedfor asynchronous tasks.
There are lots of parallel processing frameworks for Python,
somerepresentatives are shownbelow.
Python built-inparallelprocessing modules:
•Multiprocessing [8] is a built-in module that provides process-
basedparallelism.ItimplementsaProcessclass tomanagepro-
cess,andAPIs for data transferring,synchronization,etc.
•Threading [9] is a built-in module which provides thread-based
parallelism. It is useful mainly for I/O bound tasks. APIs for
managing thread-local data andlocksare provided.
•Concurrent.futures [6]isabuilt-inmoduleforexecutingcallables
asynchronously.Itsupportsboththread-basedandprocess-based
parallelizationand implementsthe Executorclassasa high-level
interfacefor running callablesconcurrently.
Third party parallelprocessing packages:
•Joblib[7] provides light-weight pipelining for Python. It imple-
mentsaclassParallelforwritingparallelloops.Italsoprovides
interfacesfor operating sharedmemory,reusableworkers, etc.
•Spark[58] is a multi-language engine for data science that sup-
ports distributed computing. The resilient distributed datasets
(RDDs) is its core data structure.Only high-level parallel opera-
tionsonRDDssuch as mapandreduce are provided.
•Dask[40] is a package for parallel and distributed computing
in Python. It provides parallelized versions of commonly used
data structures, such as DataFrame in pandas [ 30]. In addition, it
provideseasy-to-useAPIs for executingcode asynchronously.
•Ray[31]isauniﬁeddistributedcomputingframeworkinPython.
Itprovideseasy-to-useAPIs forexecutingtasksinparallel.Ray
also provides the actor model as a stateful worker. Some opti-
mizationtechniquessuchasread-onlysharedmemoryarealso
leveragedfor betterperformance.
2.2 AutomaticParallelization
2.2.1 Automatic parallelization for compiled languages. Automatic
parallelization forcompiled languages suchas Fortran and C/C++,
has been thoroughly studied, yielding a variety of automatic paral-
lelizationtools,includingCetus[ 27],Pluto[21],Polariscompiler
[19], andPar4all [ 15].
Automaticparallelizationtechniquesincompiledlanguagesfo-
cus mostly on nested loops, since usually they are performance
bottlenecks,andparallelizingthemcanachievesigniﬁcantimprove-
mentinperformance.Suchtechniquesarecalledloopnestoptimiza-
tions[38,54].Amongthem,thepolyhedraloptimization[ 20,25,37]
is the most widely used. It uses the polyhedral model to represent
nested loops, where each loop iteration corresponds to a lattice
point in a polyhedron. The parallelization can be achieved by per-
forming aﬃne or none-aﬃne transformations on the polyhedral
model,whichresults inan equivalentbut parallelizable loop.
Though loop nest optimizations are widely applicable in low-
levelcompiledlanguages,theyarenotsuitableforPython.Loops
can be hundreds of times slower in Python because of its interpre-
tative nature. In practice, Python is not used to directly implement
algorithmssuch asmatrix multiplicationandfactorization,which
contain nested loops asthe performance bottleneck. Instead, such
algorithms are implemented in low-levelcompiled languages andareimportedintoPythonprograms,asisdoneinnumpy[ 35].There
arefewcasesinreal-worldPythonprogramsthatcanbeparallelized
using loopnestoptimization techniques.
2.2.2 Automatic Parallelization for Python. There are some works
about automatically parallelizing Python programs in recent years.
We classify them into 3 categories and show why they are not
suitable for discovering parallelism in real-world Python programs.
PolyhedralOptimizationinPython: AutoParallel[ 39]andau-
toMPHC[ 41]bothparallelizePythonprogramsusingpolyhedral
optimizationtechniques.AutoParallelimplementsaneasy-to-use
Python module that achieves polyhedral optimization by wrap-
pingthePluto[ 21]parallelizationtool.AutoMPHCusespolyhedral
optimizationaswellasothertechniquesforexploitinghardware
resources. Both methods need manual annotation to assist paral-
lelization. Evaluations are conducted on a special set of Python
programs [ 13] that are suitable for polyhedral optimization and
achieve signiﬁcant performance improvement. However, as dis-
cussed in Section 2.2.1, Python usually dispatches algorithms with
nested loops to low-level compiled languages and imports them;
thus loop nest optimization techniques are not suitable for most
real-world Pythonprograms.
Domain Speciﬁc Parallelization: HPAT [44] and QuantCloud
[59]proposeautomaticparallelizationtechniquesforspeciﬁcdo-
mains using semantics. HPAT focus on parallelizing programs sim-
ilartologisticregression[ 56]andproposeadataﬂowframework
[33] to ﬁnd data distributions. It requires the input programs to
follow certain prerequisites and needs user annotations as assis-
tance. QuantCloud focuses on parallelizing quantitative ﬁnance
applications.Thesemethodsbothconcentrateonspecialtypesof
programs, making them hard to be widelyapplicable. The need of
manual annotations and prior semantic knowledge hinder them
from gaining large-scale automatic applications; and thus they are
not suitable for discovering parallelism missed by programmers in
real-world packages.
DynamicParallelization: Pydron[32]proposestodynamically
ﬁnd and dispatch parallelizable tasks. To achieve this, it ﬁrst needs
the userannotation to indicatewhich functionis the performance
bottleneckandneedsoptimization. Duringexecution,Pydrondy-
namically builds the dependence relationship between tasks, ﬁnds
tasks that can run concurrently, and then dispatches them. This
methodcanparallelizesomePythonprograms,butithasshortcom-
ings.Dynamicparallelizationcanintroducesigniﬁcantoverhead.
Also,the needofmanual annotation iscostlyto satisfy.
3 OURAPPROACH: PYPAR
3.1 Overview
PyPartakesapieceofPythonprogramasinput.Itprocessesthe
target program and produces a parallelism report and a referential
parallelizedcode as output.ItsworkﬂowisshowninFigure 2.
In step (a), PyPardecomposes an input Python program (e.g.,
a"helloworld"program)intoprogramunits.Instep(b),adepen-
dence graph with program units as nodes is built, yielding the
dependence relation between program units. In step (c), PyPar
utilizes the dependence graph to ﬁnd program units that can be ex-
ecuted concurrently (e.g., the program units in the same rectangle).
834ESEC/FSE ’23, December3–9, 2023,San Francisco, CA, USA SiweiWei, Guyang Song,SenlinZhu, Ruoyi Ruan, Shihao Zhu, andYanCai
Python Code
h = hello()
w = world()
r = hw(h, w)
hello() world()
h = ... w = ...
hw(h, w)
r = ...hello() world()
h = ... w = ...
hw(h, w)
r = ...hello() world()
h = ... w = ...
hw(h, w)
r = ...h = hello()
w = world()task1
task2h = hello()
w = world()task1
task2hello() : 1.0s
world() : 1.0s
hw(h, w) : 1.0s
found parallelism:
h = hello()
w = world()
save time: 1.0sreport@ray.remote
def task_0 (world):
w = world()
return (w,)
@ray.remote
def task_1 (hello):
h = hello()
return (h,)
tmp_0 = task_0.remote(world)
tmp_1 = task_1.remote(hello)
(w,) = ray.get(tmp_0)
(h,) = ray.get(tmp_1)
r = hw(h, w)
(b).
extract
dependence(c).
discover
parallelism(d).
recompose(a).
decompose(e). execute and time
(f).
select
(h).
generate
report(g).
rewrite
Figure 2:The workﬂow of PyParon a"helloworld"program
r = 2 * f(f1(a), f2(b)) + g(c)
r = 2 * .. + ..f(.., ..) g(c)f1(a) f2(b)
coarse-grained statementprogram units
with hierarchical structure
Figure 3:Decomposition ofacoarse-grained statement
Instep(d),theparallelizableprogramunitsarerecomposedtoform
coarse-grained tasks based on heuristics which aims to reduce the
number of tasks to avoid extra overhead. To ﬁlter the tasks unwor-
thy of parallelization, the target program is executed and timed to
assign a weight for each task, as shown in step (e). In step (f), only
taskswithsuﬃcientlyhighweightsareselected.Finally,instep(h),
aparallelismreportisgenerated,showingpossibleparallelismin
the target program and expected time savings after parallelization.
Also, in step(g),a referential parallelized code isgenerated, inthis
ﬁgure, the parallelized program is implemented using Ray, and
other parallelization frameworks can also be used. These steps will
be illustratedindetailinthe following sub-sections.
3.2 Decomposition
In step (a), PyPardecomposes the target program into small units,
whichwe callprogram units.
The procedure of decomposition is given by Algorithm 1. It ﬁrst
parsesthetargetprogramandobtainsanabstractsyntaxtree(AST),
usingthebuilt-inmoduleAST[ 1](line1).Thenallthestatement
sub-treesareextractedfromtheAST(line2).Astatementcanbe
coarse-grained and should be further decomposed. For example,
considerthestatementinFigure 3,whichhasacomplexinternal
structure. Since function calls usually cost the most in Python pro-
grams,weonlyextractfunctioncallsintargetstatements.Thesame
procedureisrepeatedfor coarse-grainedfunctioncalls, andeven-
tually,wegetadecompositionwithahierarchicalstructure.The
proceduretodecomposecoarse-grainedstatementsisdenotedasde-
composeStmt.Coarse-grained statementsare further decomposed
whileotherstatements are retained(lines4–10).Algorithm1: Decompose
Input:code
Output:programUnits
1ast=parseAST( code);
2statements =getStatementSubtrees( ast);
3programUnits =∅;
4forstmt∈statements do
5ifisCoarseGrained( stmt)then
6 units=decomposeStmt( stmt);
7 programUnits =programUnits ∪units;
8else
9 programUnits =programUnits ∪{stmt};
10end
11end
12returnprogramUnits ;
Figures2and3give examples of decomposition. In Figure 2, the
"hello world" program consists of three statements, and they are
all coarse-grained. The statement ℎ=ℎ/u1D452/u1D459/u1D459/u1D45C()is decomposed to
program units ℎ=...andℎ/u1D452/u1D459/u1D459/u1D45C(). The other two statements are
decomposed similarly. Finally, the program is decomposed into six
programunits.Figure 3givesanexampleofdecomposingcoarse-
grained statements. All function calls are extracted from the state-
ment,whichtogetherwiththeoriginalstatementareprogramunits.
The program unitsform ahierarchical structure.
3.3 Extracting Dependence
To determine valid orders of executing program units, we adopt
data dependenceanalysis which hasbeen studiedin depth[ 14,17,
26,28,52,55]. Data dependence analysis is originally conducted
at statement level, and we extend it to program unit level. The
notationsusedareconsistentwith[ 14,26,55].Inthissection,we
deﬁnethedirectdatadependencerelation /u1D6FF∗,andthendeﬁnethe
indirectdata dependence relation Δasthe transitive closure of /u1D6FF∗.
3.3.1 Extracting /u1D437/u1D452/u1D453and/u1D448/u1D460/u1D452Sets.The read-and-write portion is
all we need to make sure that a program unit can run normally
and to determine its eﬀects. In PyPar, we use the set of variables a
program unit uses or modiﬁes to represent the portion of memory
aprogramunitreads orwrites,which isstraightforwardtoimple-
ment. There might be cases where two variables pointing to the
835DiscoveringParallelismsin PythonPrograms ESEC/FSE ’23, December3–9, 2023,San Francisco, CA, USA
h = hello()
w = world()
r = hw(h, w)Use Def
hello()    {hello} {}
h = .. {} {h}
world()   {world} {}
w = .. {} {w}
hw(h, w)  {hw,h,w} {}
r = ..  {} {r}hello() world()
h = .. w = ..
hw(h, w)
r = ..compose compose
composeRAW RAW
(a) (b) (c)
Figure 4: Dependence analysis for sequentially executed
statements
for data in data_list:
res.append(f(data))Use Def
f(data) {f,data} {}
res.append(..) {res} {res}f(data)
res.append(..)compose
WAW.WAR,RAW
(a) (b) (c)
Figure 5:Dependence analysis forloopstatement
same memory address. But such situations are rare (see Section
4), and the resulting false positives can be easily identiﬁed and
corrected. To be consistent with previous work [ 14], we deﬁne the
/u1D448/u1D460/u1D452(read) and /u1D437/u1D452/u1D453(write) setsas:
Definition 1(/u1D448/u1D460/u1D452,/u1D437/u1D452/u1D453)./u1D448/u1D460/u1D452isafunctionwhich mapsapro-
gramunit /u1D462tothesetofvariables /u1D462reads./u1D437/u1D452/u1D453isafunctionwhich
mapsaprogramunit /u1D462to theset ofvariables /u1D462writes.
Examples of /u1D448/u1D460/u1D452//u1D437/u1D452/u1D453sets are shown in Figures 4and5. Figure
4(a) shows the original program segment, and Figure 4(b) gives
the/u1D448/u1D460/u1D452//u1D437/u1D452/u1D453sets. The program unit ℎ/u1D452/u1D459/u1D459/u1D45C()reads the variable
ℎ/u1D452/u1D459/u1D459/u1D45Cbecause it calls this variable, while it writes nothing. The
program unit ℎ=..reads nothing, while it writes the variable ℎ
because it assigns a new value to this variable. /u1D448/u1D460/u1D452//u1D437/u1D452/u1D453sets of
other program units are computed similarly. Figure 5(a) shows the
original program segment, and Figure 5(b) gives the /u1D448/u1D460/u1D452//u1D437/u1D452/u1D453sets.
The programunit /u1D453(/u1D451/u1D44E/u1D461/u1D44E)reads thevariables /u1D453and/u1D451/u1D44E/u1D461/u1D44Ebecause
onevariableiscalledandtheotheristakenasanargument,whileit
writesnothing.Theprogramunit /u1D45F/u1D452/u1D460./u1D44E/u1D45D/u1D45D/u1D452/u1D45B/u1D451 (..)readsthevariable
/u1D45F/u1D452/u1D460because it calls a method /u1D44E/u1D45D/u1D45D/u1D452/u1D45B/u1D451of the/u1D45F/u1D452/u1D460object. The /u1D45F/u1D452/u1D460
variable is also put into the /u1D437/u1D452/u1D453set because the call of method
/u1D44E/u1D45D/u1D45D/u1D452/u1D45B/u1D451maychangethe internal state ofthe /u1D45F/u1D452/u1D460object.
To make the process of extracting /u1D448/u1D460/u1D452//u1D437/u1D452/u1D453sets easier and vi-
ableforstaticanalysis,weproposeseveralsimpliﬁcations.Foran
arraysubscriptexpression /u1D44E[/u1D456],if/u1D456isaloopindex,wetake /u1D44E[/u1D456]asa
singlevariablein /u1D448/u1D460/u1D452//u1D437/u1D452/u1D453sets;otherwise,wetake /u1D44E,/u1D456in/u1D448/u1D460/u1D452//u1D437/u1D452/u1D453
sets respectively. Also, we assume a function call of an external
function won’t change the value of arguments. These simpliﬁca-
tions might result in false positive parallelisms. Figure 6gives such
anexample.Acodepieceinfunction /u1D459/u1D456/u1D45B/u1D44E/u1D459/u1D454._/u1D45A/u1D44E/u1D461/u1D453/u1D462/u1D45B/u1D450/u1D460./u1D452/u1D465/u1D45D/u1D45A from
SciPyisshown,where PyParreportedthatthestatementwithin
the red dotted-line boxes is parallelizable in diﬀerent iterations.
However, the function /u1D45D/u1D44E/u1D451/u1D452_/u1D448/u1D449_/u1D450/u1D44E/u1D459/u1D450(which is implemented in
compiled languages, so PyParcan’t infer its behavior) modiﬁes its
ﬁrstargument(whichis /u1D434/u1D45A),sothisstatementisnotparallelizable.
The task of reﬁning data dependence analysis to eliminate such
false positives is left as future work, see Section 5. In practice, such
falsepositivesarerare,asshowninSection 4.Moreover,thesefalse
positives can be easily foundandcorrected.
3.3.2 ConstructingDependenceGraph. The/u1D448/u1D460/u1D452//u1D437/u1D452/u1D453setisusedto
deﬁnethedependencerelation betweenprogram units. We deﬁnefor ind in product (...):
    ...
    pade_UV_calc (Am, n, m )
    ...
Figure 6:Afalsepositive caused by simpliﬁcations
four dependencies, three of them are data dependencies, which
have been discussed in detail in [ 26,55]. And one comes naturally
from the hierarchical structure duringdecomposition.
•RAW:Read-after-write (RAW) is also called ﬂow-dependency.
Twoprogramunits /u1D462and/u1D463haveaRAWdependenceifduringthe
execution, (1) /u1D462executes before /u1D463and (2)/u1D462writes some memory
which will later be read by /u1D463. That is, /u1D437/u1D452/u1D453(/u1D462) ∩/u1D448/u1D460/u1D452(/u1D463)≠∅.
To preserve program behavior, /u1D462must be executed before /u1D463;
otherwise /u1D463cannotread correctvalues.
•WAR:Write-after-read (WAR) is also called anti-dependency.
Twoprogramunits /u1D462and/u1D463haveaWARdependenceifduringthe
execution,(1) /u1D462executesbefore /u1D463and(2)/u1D462readssomememory
which is latter overwrote by /u1D463. That is, /u1D448/u1D460/u1D452(/u1D462) ∩/u1D437/u1D452/u1D453(/u1D463)≠∅.
Again,/u1D462must be executed before /u1D463; otherwise what /u1D462reads will
be changedby /u1D463.
•WAW:Write-after-write(WAW)isalsocalledoutputdependency.
Two program units /u1D462and/u1D463have a WAR dependence if during
theexecution,(1) /u1D462executesbefore /u1D463and(2)/u1D462writesthesame
memory as /u1D463. That is, /u1D437/u1D452/u1D453(/u1D462) ∩/u1D437/u1D452/u1D453(/u1D463)≠∅. Then/u1D462must be
executedbefore /u1D463;otherwise,thememorythat /u1D462,/u1D463bothwrites
willbe changedandaﬀectsubsequent computations.
•Compose: Compositiondependencecomesnaturallyfromthe
hierarchical structure when decomposing a coarse-grained state-
ment, see Figure 3. For two program units /u1D462,/u1D463where/u1D462is a
sub-expressionof /u1D463,/u1D462mustbe executedbefore /u1D463.
Withthesetypesofdependence,wethenconstructadependence
graph as below, in which nodes are program units and directed
edgesarethe dependence relationbetweencomputationunits, as
showninFigure 4(c).
Definition 2 (dependencegraph). A dependencegraphofa
pieceofprogramisagraph /u1D43A=⟨/u1D449,/u1D438⟩,where/u1D449isthesetofprogram
units, and /u1D438⊆/u1D449×/u1D449is the directed edge set. For /u1D462,/u1D463∈/u1D449,(/u1D462,/u1D463) ∈/u1D438
ifandonlyifthereisoneofthefourtypesofdependence(RAW,WAR,
WAW,Compose) from /u1D462to/u1D463.
The dependence graph of sequentially executed program pieces
is a directed acyclic graph (DAG). However, the dependence graph
of a program piece inside a loop can be cyclic. Consider a program
unit/u1D462insidealoopstatement(suchasaForstatement).Diﬀerent
from the sequential case, /u1D462can be executed multiple times. As a
result, there can be dependence between /u1D462and itself, resulting in a
loopinthe dependence graph, as showninFigure 5(c).
We deﬁnethe directdata dependence relation:
Definition 3(directdatadependencerelation). Thedirect
data dependence relation is a binary relation over the set of program
units.Fortwoprogramunits /u1D462,/u1D463,/u1D462/u1D6FF∗/u1D463ifandonlyifthereisoneof
the four types of dependence (RAW, WAR, WAW, Compose) from /u1D462to
/u1D463,i.e.thereis anedge in the dependencegraphfrom /u1D462to/u1D463.
We then deﬁnethe indirectdata dependence relation:
836ESEC/FSE ’23, December3–9, 2023,San Francisco, CA, USA SiweiWei, Guyang Song,SenlinZhu, Ruoyi Ruan, Shihao Zhu, andYanCai
Definition 4 (indirect data dependence relation). The
indirectdatadependencerelationisabinaryrelationoverthesetof
programunits.Itisthetransitiveclosureof /u1D6FF∗.Fortwoprogramunits
/u1D462,/u1D463,/u1D462Δ/u1D463ifandonlyifthereareprogramunits /u1D4641,...,/u1D464/u1D45B,/u1D45B≥1,such
that/u1D462/u1D6FF∗/u1D4641,...,/u1D464/u1D45B/u1D6FF∗/u1D463, i.e. there is a path of length at least one in the
dependencegraphfrom /u1D462to/u1D463.
Twoprogramunitscanbeexecutedconcurrentlywithoutchang-
ingsemantics if neither /u1D462Δ/u1D463nor/u1D463Δ/u1D462.
3.4 DiscoveringParallelism
For sequentially executed program units, the dependence graph
is a DAG. To ease our presentation, we add an /u1D452/u1D45B/u1D461/u1D45F/u1D466.altnode in the
dependence graph that has edges to all nodes with 0 in-degree.
Similarly,weaddan /u1D452/u1D465/u1D456/u1D461nodethathasedgesfromallnodeswith0
out-degree. (Obviously, each graph has exactly one /u1D452/u1D45B/u1D461/u1D45F/u1D466.altnode and
one/u1D452/u1D465/u1D456/u1D461node.) We deﬁnethe /u1D451/u1D452/u1D45D/u1D461ℎofaprogram unitas follows:
Definition 5 (depth). Given a graph /u1D43A=⟨/u1D449,/u1D438⟩, the/u1D451/u1D452/u1D45D/u1D461ℎof
/u1D462∈/u1D449isthelength(i.e.,thenumberofedges)ofthelongestpathfrom
/u1D452/u1D45B/u1D461/u1D45F/u1D466.altto/u1D462.Thatis,∀/u1D462∈/u1D449,
/u1D451/u1D452/u1D45D/u1D461ℎ(/u1D462)=max
/u1D45D∈/u1D45D/u1D44E/u1D461ℎ(/u1D452/u1D45B/u1D461/u1D45F/u1D466.alt,/u1D462){/u1D459/u1D452/u1D45B/u1D454/u1D461ℎ(/u1D45D)}
The/u1D451/u1D452/u1D45D/u1D461ℎis well deﬁned because the dependence graph is a
DAG. We then deﬁnethe /u1D451/u1D452/u1D45D/u1D461ℎ/u1D460/u1D45D/u1D44E/u1D45B ofaprogram unit:
Definition 6 (depthspan). Given a graph /u1D43A=⟨/u1D449,/u1D438⟩, the
/u1D451/u1D452/u1D45D/u1D461ℎ/u1D460/u1D45D/u1D44E/u1D45B of/u1D462∈/u1D449\ {/u1D452/u1D45B/u1D461/u1D45F/u1D466.alt,/u1D452/u1D465/u1D456/u1D461 }is the set of all depths between
/u1D462and its shallowest (i.e., with the least depth) successor. That is,
∀/u1D462∈/u1D449\{/u1D452/u1D45B/u1D461/u1D45F/u1D466.alt,/u1D452/u1D465/u1D456/u1D461 },
/u1D451/u1D452/u1D45D/u1D461ℎ/u1D460/u1D45D/u1D44E/u1D45B (/u1D462)={/u1D451∈/u1D441|/u1D451/u1D452/u1D45D/u1D461ℎ(/u1D462) ≤/u1D451<min
/u1D460∈/u1D460/u1D462/u1D450/u1D450(/u1D462){/u1D451/u1D452/u1D45D/u1D461ℎ(/u1D460)}}
Figure7(a)showsanexampletoillustratethetwoconcepts /u1D451/u1D452/u1D45D/u1D461ℎ
and/u1D451/u1D452/u1D45D/u1D461ℎ/u1D460/u1D45D/u1D44E/u1D45B . In the ﬁgure, we have /u1D451/u1D452/u1D45D/u1D461ℎ(/u1D434)=1because the
longestpathfrom /u1D452/u1D45B/u1D461/u1D45F/u1D466.altto/u1D434is{/u1D452/u1D45B/u1D461/u1D45F/u1D466.alt,/u1D434}whoselengthis 1.The
/u1D451/u1D452/u1D45D/u1D461ℎsof/u1D452/u1D45B/u1D461/u1D45F/u1D466.alt,/u1D435,/u1D436 ,and/u1D452/u1D465/u1D456/u1D461arecomputedsimilarly.Andwehave
/u1D451/u1D452/u1D45D/u1D461ℎ/u1D460/u1D45D/u1D44E/u1D45B (/u1D434)={1,2}because/u1D451/u1D452/u1D45D/u1D461ℎ(/u1D434)=1and/u1D451/u1D452/u1D45D/u1D461ℎ(/u1D452/u1D465/u1D456/u1D461)=3
where the /u1D452/u1D465/u1D456/u1D461is the only successor of A. The /u1D451/u1D452/u1D45D/u1D461ℎ/u1D460/u1D45D/u1D44E/u1D45B of/u1D435and
/u1D436are computedsimilarly.
Giventwoprogramunits /u1D462,/u1D463,iftheysharesomecommondepth,
i.e.,/u1D451/u1D452/u1D45D/u1D461ℎ/u1D460/u1D45D/u1D44E/u1D45B (/u1D462)∩/u1D451/u1D452/u1D45D/u1D461ℎ/u1D460/u1D45D/u1D44E/u1D45B (/u1D463)≠∅,thentheyareparallelizable.
Intuitively,thisisbecause /u1D462and/u1D463canbeputonthesame"level"in
the dependence graph. Formally,we showthis as Proposition 1:
Proposition 1.Giventwonodes /u1D462,/u1D463in adependencegraph,if
/u1D451/u1D452/u1D45D/u1D461ℎ/u1D460/u1D45D/u1D44E/u1D45B (/u1D462) ∩/u1D451/u1D452/u1D45D/u1D461ℎ/u1D460/u1D45D/u1D44E/u1D45B (/u1D463)≠∅,thenneither /u1D462Δ/u1D463nor/u1D463Δ/u1D462.
In Figure 7(a),/u1D434is parallelizable with both /u1D435and/u1D436by propo-
sition1,aswehave /u1D451/u1D452/u1D45D/u1D461ℎ/u1D460/u1D45D/u1D44E/u1D45B (/u1D434) ∩/u1D451/u1D452/u1D45D/u1D461ℎ/u1D460/u1D45D/u1D44E/u1D45B (/u1D435)={1}≠∅and
/u1D451/u1D452/u1D45D/u1D461ℎ/u1D460/u1D45D/u1D44E/u1D45B (/u1D434) ∩/u1D451/u1D452/u1D45D/u1D461ℎ/u1D460/u1D45D/u1D44E/u1D45B (/u1D436)={2}≠∅.
Ontheotherhand,wealsoaimtoﬁndallpossibleparallelism
opportunities. Based on /u1D451/u1D452/u1D45D/u1D461ℎ/u1D460/u1D45D/u1D44E/u1D45B , we can ﬁnd all, as shown in
Proposition 2:
Proposition 2.Givena node /u1D462in a dependencegraph, ifthere
exists some /u1D464≠/u1D462suchthat neither /u1D462Δ/u1D464nor/u1D464Δ/u1D462,then there exists
/u1D463≠/u1D462 /u1D460./u1D461./u1D451/u1D452/u1D45D/u1D461ℎ/u1D460/u1D45D/u1D44E/u1D45B (/u1D462) ∩/u1D451/u1D452/u1D45D/u1D461ℎ/u1D460/u1D45D/u1D44E/u1D45B (/u1D463)≠∅.
Forexample,inFigure 7(a),neither /u1D434Δ/u1D435nor/u1D435Δ/u1D434;soweknow
there is some node /u1D463such that /u1D451/u1D452/u1D45D/u1D461ℎ/u1D460/u1D45D/u1D44E/u1D45B (/u1D434) ∩/u1D451/u1D452/u1D45D/u1D461ℎ/u1D460/u1D45D/u1D44E/u1D45B (/u1D463)≠∅,
whichcan be /u1D435or/u1D436.AB
Centry
exitdepth=0
depth=3depth=1
depthspan={1, 2}depth=1
depthspan={1}
depth=2
depthspan={2}A B
C
D
(a) (b) Figure 7: Using dependencegraph to ﬁnd parallelizablepro-
gram units, parallelizable nodes are in red dotted-line boxes
Algorithm2: Findparallelizablesinserialprograms
Input:G: the dependencegraph
Output:parallelizable : parallelizable programunits
1G=addEntryExit( G);
2depth=getDepth( G);
3depthspan =getDepthspan( G,depth);
4parallelizable =∅;
5for/u1D451=1,2···/u1D451/u1D452/u1D45D/u1D461ℎ[/u1D452/u1D465/u1D456/u1D461] −1do
6/u1D446={/u1D462∈/u1D449|/u1D451∈/u1D451/u1D452/u1D45D/u1D461ℎ/u1D460/u1D45D/u1D44E/u1D45B [/u1D462]};
7if|/u1D446| ≥2then
8 parallelizable =parallelizable ∪S;
9end
10end
11returnparallelizable ;
Due to page limit, the proofs of the two propositions are shown
inAppendix inthe supplementary ﬁle.
Based on the above, we present Algorithms 2and3to ﬁnd all
parallelizable program units. Algorithm 2works for program units
in sequentially executed programs and Algorithm 3works for pro-
gram units in loops (to be explained in the next paragraph). It ﬁrst
addsnodes /u1D452/u1D45B/u1D461/u1D45F/u1D466.altand/u1D452/u1D465/u1D456/u1D461tothedependencegraph(line1).Then
the/u1D451/u1D452/u1D45D/u1D461ℎofeachnodeinthedependencegraphiscalculatedusing
abreadth-ﬁrstsearchalgorithm(line2),andthe /u1D451/u1D452/u1D45D/u1D461ℎ/u1D460/u1D45D/u1D44E/u1D45B iscal-
culatedaccording to itsdeﬁnition(line3).After that, itenumerates
allpossibledepths.Andifonedepthiscontainedin /u1D451/u1D452/u1D45D/u1D461ℎ/u1D460/u1D45D/u1D44E/u1D45B of
at least two program units; then such program units are marked
parallelizable (lines4–10).
Consider program units within a loop, such as the two program
units in Figure 5(c). They can execute multiple times. We therefore
have to determine whether a program unit /u1D462in diﬀerent iterations
can be executed concurrently. That is, to determine whether the
relation/u1D462Δ/u1D462holds.Fromgraphtheory,wehave /u1D462Δ/u1D462ifandonlyif /u1D462
isinsomenon-trivialstronglyconnectedcomponents(SCC).If /u1D462is
notinanynon-trivialSCC,then /u1D462isparallelizable.Figure 7(b)gives
an example. There are three SCCs: {/u1D434,/u1D435},{/u1D436}and{/u1D437}, among
which only {/u1D437}is trivial, so only /u1D437can execute concurrently in
diﬀerentiterations ofthe loop.
Algorithm 3ﬁndsparallelizableswithinaloop.Firstly,theTarjan
algorithm[ 43]isusedtoﬁndSCCs(line1).NodesintrivialSCCs
are markedparallelizable (lines2–7).
The comprehension expression inPython [ 2], which is actually
aspecializedloop,ishandledsimilarlyto handling generalloops.
837DiscoveringParallelismsin PythonPrograms ESEC/FSE ’23, December3–9, 2023,San Francisco, CA, USA
Algorithm3: Findparallelizablesinloop
Input:G: the dependencegraph
Output:parallelizable : parallelizableprogramunits
1SCCs=getSCCs( G);
2parallelizable =∅;
3forscc∈SCCsdo
4if/u1D456/u1D460/u1D447/u1D45F/u1D456/u1D463/u1D456/u1D44E/u1D459 (/u1D460/u1D450/u1D450)then
5 parallelizable =parallelizable ∪scc;
6end
7end
8returnparallelizable ;
Algorithm4: Recompose parallelizable program units
Input:parallelizable:the set of parallelizableprogramunits,
G: the dependencegraph
Output:tasks: the set of tasks
1tasks ={{/u1D462}|/u1D462∈/u1D45D/u1D44E/u1D45F/u1D44E/u1D459/u1D459/u1D452/u1D459/u1D456/u1D467/u1D44E/u1D44F/u1D459/u1D452 };
2whiletruedo
3for/u1D448,/u1D449∈/u1D461/u1D44E/u1D460/u1D458/u1D460,/u1D448 ≠/u1D449do
4 if∀/u1D462∈/u1D448,∀/u1D463∈/u1D449,/u1D462Δ/u1D463∨/u1D463Δ/u1D462and
∃/u1D462∈/u1D448,∃/u1D463∈/u1D449,/u1D462/u1D6FF∗/u1D463∨/u1D463/u1D6FF∗/u1D462then
5 tasks =(tasks−{/u1D448,/u1D449}) ∪ {/u1D448∪/u1D449};
6 end
7end
8iftasksnotupdated then
9 break;
10end
11end
12returntasks;
3.5 Recomposition
Inthisstep,werecomposeparallelizableprogramunitstoconstruct
tasks that correspond to a process or thread and can run concur-
rently. It is possible to assign each parallelizable program unit a
task. But this will add extra overhead because creating and recy-
cling processes or threads can be expensive. In general, we want
thenumberoftaskstobeassmallaspossible,whilenotharming
the totalparallelism.
PyParusesheuristicstrategiestocomposeparallelizableunits
into tasks. It aims to merge "adjacent" parallelizable program units.
ThisideaisstraightforwardandtheprocedureisshowninAlgo-
rithm4.Initially,wetakeeachparallelizableprogramunitasatask
(line 1). Then we try to merge diﬀerent tasks. At each step, for
two diﬀerent tasks /u1D448,/u1D449, let/u1D462/u1D456,/u1D463/u1D457,(/u1D456=1,2···|/u1D448|, /u1D457=1,2···|/u1D449|)
beparallelizableprogramunitsbelongingto /u1D448,/u1D449respectively.If
every pair of program units /u1D462/u1D456and/u1D463/u1D457are dependent (i.e., either
/u1D462/u1D456Δ/u1D463/u1D457or/u1D463/u1D457Δ/u1D462/u1D456)andthereexistsapair /u1D462/u1D4561,/u1D463/u1D4571s.t./u1D462/u1D4561/u1D6FF∗/u1D463/u1D4571∨/u1D463/u1D4571/u1D6FF∗/u1D462/u1D4561,
then we merge /u1D448,/u1D449intoasingletask(line2-10).
Whenloops areinvolved,theaboveheuristicstrategy canalso
be applied to compose parallelizables. For a program unit /u1D462in a
loop which is parallelizable between diﬀerent iterations (i.e., a map
operation),we ﬁrst dynamicallymeasure thenumber ofiterations
ofthisloop.Ifthenumberofiterationsislargerthanathreshold
(say 10), then the iteration of the loop is blocked and executions of
/u1D462inthe same blockare packedintothe same task.
3.6 DynamicSelection
In addition to parallelizability, the usefulness of parallelizable pro-
gramunitsisalsocrucial.Parallelismmaynotnecessarilyleadtoacceleration. For example, if all tasks run very quickly, it is not
worthwhiletoparallelizethem.Todeterminewhetheritisworth
parallelizing, we need toassign weight to each task and select the
mostpromisingones according to their weights.
PyPardynamically determines the weight of tasks. The running
timeofthetargettaskistakenasitsweightbecauseitisstraight-
forward andaccurate. PyPardirectly runs the target program and
timesitscomponents.Thebuilt-intracemoduleofPython[ 11]is
used to time each function call, and each task’s running time is
estimated with the timing result. The number of executions of the
targetprogramisconﬁgurable,andtherunningtimesareaveraged.
Amongallthe tasks,onlythosewhose runningtime ishigher
than a predetermined threshold are selected. If a selected task does
not have othertasks to run concurrentlywith (becausesuch tasks
arenotselected),thenitwillberemoved.Iftherearestillremaining
tasks,PyParthen generates a parallelism report to show paralleliz-
able tasks and expected time savings. The expected time savings
are computed by subtracting the maximum running time of all
parallelizable tasks from the total running time of all parallelizable
tasks.That is,
/u1D460/u1D44E/u1D463/u1D452/u1D451_/u1D461/u1D456/u1D45A/u1D452=Σ/u1D461/u1D44E/u1D460/u1D458∈/u1D45D/u1D44E/u1D45F/u1D44E/u1D459/u1D459/u1D452/u1D459/u1D456/u1D467/u1D44E/u1D44F/u1D459/u1D452 _/u1D461/u1D44E/u1D460/u1D458/u1D460/u1D45F/u1D462/u1D45B/u1D45B/u1D456/u1D45B/u1D454_/u1D461/u1D456/u1D45A/u1D452(/u1D461/u1D44E/u1D460/u1D458)−
/u1D45A/u1D44E/u1D465/u1D461/u1D44E/u1D460/u1D458∈/u1D45D/u1D44E/u1D45F/u1D44E/u1D459/u1D459/u1D452/u1D459/u1D456/u1D467/u1D44E/u1D44F/u1D459/u1D452 _/u1D461/u1D44E/u1D460/u1D458/u1D460/u1D45F/u1D462/u1D45B/u1D45B/u1D456/u1D45B/u1D454_/u1D461/u1D456/u1D45A/u1D452(/u1D461/u1D44E/u1D460/u1D458)
For example, if 3 tasks /u1D4611,/u1D4612and/u1D4613are parallelizable, and their
running times are 1s, 2s, 2.5s, respectively. Then the expected time
savingwillbe /u1D460/u1D44E/u1D463/u1D452/u1D451_/u1D461/u1D456/u1D45A/u1D452=1/u1D460+2/u1D460+2.5/u1D460−2.5/u1D460=3/u1D460.
Inpractice,testinputisfedtotargetprogramtoﬁndparallelisms
in the test phase, and the parallelisms can be applied in the release
versioninwhichotherinputscan be used.
3.7 Rewriting
PyParcanrewritetheoriginalPythonprogramtoaparallelizedver-
sion, given a desirable parallelization framework. Currently, PyPar
supports rewriting to code using concurrent.futures [ 6] (python
built-inparallelmodule)orray[ 31](adistributedcomputingframe-
work). Rewriting to other parallel frameworks can be implemented
similarly. Due tosome implementation challenges, auto-rewritten
code may suﬀer from issues in readability and performance for
someparallelizationframeworks. We discuss this inSection 4.5.2.
3.8 Discussion on PossibleIssues
In this section, we discuss some important issues regarding the
usage andusefulnessof PyPar.
3.8.1 InputSensitivity. ThedynamicselectioninSection 3.6uses
speciﬁcinputtorunthetargetprogram.Onemayarguethatthe
detected parallelisms seem to be dependent on the choice of input.
Here we explainthat the impact of input-sensitivityislimited.
Firstly, the usefulness of parallelization can not be assured with-
outknowingtheinputdistributioninreal-worldcases.Ifthetarget
programrunsquicklywithallreal-worldinputs,parallelizationwill
not be useful. Parallelization can be useful only if it can achieve
considerable acceleration with some real-world inputs. Thus the
knowledge of input distribution is indispensable. The dependence
on input is not a drawback of PyParbecause no parallelization
toolcanguaranteeusefulnesswithoutknowingreal-worldinput
distribution.Secondly,theinabilityof PyPartoproveusefulnessof
838ESEC/FSE ’23, December3–9, 2023,San Francisco, CA, USA SiweiWei, Guyang Song,SenlinZhu, Ruoyi Ruan, Shihao Zhu, andYanCai
discovered parallelismsdoes not mean it is useless. PyParaims not
toprove usefulness,but toﬁnd parallelizationpossibilities. Andit
does ﬁnd parallelizationpossibilities,which is useful at least with
the input used in the dynamic selection step. Thirdly, the dynamic
selection step can be thought of as a way to take input distribu-
tionintoconsideration.Whentheinputusedisrepresentativeof
real-world inputs,the discoveredparallelism islikely to be useful.
3.8.2 FalsePositives. AsSection 3.3shows, because of the impre-
cision of dependenceanalysis, PyParmay report false-positive par-
allelisms.Hereweexplainwhyfalsepositivesarealmostinevitable
anddiscuss howto handle them.
Falsepositivesexistbecausethedependenceanalysisisnotaccu-
rate. According toRice’s theorem [ 42],there is no general method
to statically infer program properties; so the static dependence
analysis can not be accurate, and there is a trade-oﬀ between false
positivesandfalsenegatives.Thelackoftypeinformation(since
Python is dynamically typed) and the use of compiled functions
makeitevenhardertoconductdependenceanalysisforPython.To
be sound, a parallelization tool will lose a large number of paral-
lelization possibilities, which makes it less useful. So PyParadopts
anunsoundanalysistocaptureasmanyparallelizationpossibilities
as possible. The experiment in Section 4.3shows that PyParis
highly precise among real-world programs.
To eliminate false positives, programmers need to manually
check discovered parallelisms. Usually, with domain knowledge of
the target program, it is relatively easy for programmers to ﬁnd
falsepositives.Ifthesemanticsofthetargetprogramisavailable,
programmers will know what task statements do, and they will
know whether there are dependencies. Programmers can know
whatafunction reallydoesfromitsnameorcomments,and then
theycanknowwhethertherearesideeﬀects.Forexample,consider
theprograminFigure 1(b),thecodeinthedottedlineboxperforms
FastFourierTransformationfortwoarrays,andsincethetwoarrays
areindependent,itcanbeconcludedthatthereisnodependence
between the two statements. Also, there is a simple automatic
method to check false-positives. Run the original program and the
corresponding parallelized version with the same random seed,
thencomparetheirresults.Iftheresultdiﬀers,thentheparallelism
is false-positive. Otherwise, the parallelism is highly likely to be
correct.This methodissimplebut eﬀective,as Section 4.3shows.
3.8.3 Human Costs. The human costs involved when using PyPar
arerelativelylow,andtheyarelistedasfollows.Firstly,program-
mers need to manually check the discovered parallelisms and rule
out falsepositives. Thisprocess willnot costtoo muchbecauseof
the domain knowledgeof programmers (see Section 3.8.2) and the
highprecisionof PyPar(seeSection 4.3).Secondly,programmers
may want to slightly adjust the auto-rewritten code to increase its
readabilityandreduceoverhead,asdiscussedinSections 3.7and
4.5.2.This processisrelativelysimple.
4 EVALUATION
This section presents an evaluation of PyParon real-world Python
packages.TheevaluationisconductedonamachinewithIntel(R)
Xeon(R) Platinum 8260 CPU (96 cores in total) installed with the
Ubuntu20.04.3OS andthe oﬃcial cpython3.8.10 interpreter.Table 1:Statistics ofthe selected Python packages
Package Version Category #Stars(GitHub) LOC #Func
scikit-image 0.19.3 Image processing 5.2k 69.0k 3,100
scipy 1.9.1 Scientiﬁc computing 10.6k 283.5k 12,028
librosa 0.9.2 Audio processing 5.6k 25.8k 630
trimesh 3.16.3 Triangularmeshes 2.2k 50.7k 1,945
scikit-learn 1.2.2 Machinelearning 54.3k 182.3k 8,939
seaborn 0.11.2 Datavisualization 10.7k 23.4k 894
4.1 PackagesUnderTest
Toevaluatetheeﬀectivenessof PyPar,wehandpickedasetofsix
real-world open-source Python packages. We select packages from
twocollectionsofPythonprograms[ 3,4]onGitHubasfollows.We
ﬁrstskip packages undersomecategoriesthat are not suitable for
evaluating PyPar.Thisisbecausesomeofthemaremainlywritten
in low-level compilable languages (such as Fortran or C++) for
performancereasonsandonlyprovidePythonAPIs.Thesepackages
include NumPy [ 35] for numerical computing, TensorFlow [ 12] for
machinelearning,etc.Andsomearealreadywellparallelized,such
aspackagesunderthecategoriesofDatabaseandWebCrawling.
Also,applications in some categories are not performance-critical,
such as packages under the categories of Command-line Tools and
GUI Development.
Afterthat,weselectpackagesunderthecategoriesofAudio,Im-
ageProcessing,MachineLearning,DataVisualization,Science,and
Meshing.Foreachofthesecategories,weselect themostpopular
package under it according to GitHub stars. Finally, 6 widely-used
real-world open-source Python packages are selected: scikit-image,
SciPy,librosa,trimesh,scikit-learn ,andseaborn.Table1summarizes
the statistics of these package, including their versions, categories,
the number of stars, the number of lines of code (LOC), and the
number offunctions("#Func").We brieﬂyintroduce thembelow.
•Scikit-image [45]isaPythonpackagefordigitalimageprocess-
ing.Itisacollectionofimageprocessingalgorithms,including
APIs for ﬁltering, morphology,segmentation,andothers.
•SciPy[49]isaPythonpackageforscientiﬁccomputing,including
supportforlinearalgebra,sparsematrices,etc.ItextendsNumPy
[35]andprovidesadditionaltoolsandeasy-to-usedatastructures.
•Librosa[29]isaPythonpackageformusicandaudioanalysis.It
providesbuildingblocksformusicinformationretrievalsystems.
•Trimesh [5]isaPythonpackageforprocessingtriangularmeshes.
Itprovidesutilitiesforloadingandprocessingtriangularmeshes,
especiallyfor watertight surfaces.
•Scikit-learn [36]isaPythonpackageformachinelearning.It
provides simple and eﬃcient tools for predictive data analysis. It
isaccessible to everybody andreusableinvariouscontexts.
•Seaborn [51] is a Python data visualization package based on
matplotlib.Itcomeswithahigh-levelinterfacefordrawingat-
tractive andinformative statisticalgraphics.
4.2 Input Generation
As discussed in Section 3.6, dynamic selection is important for
determining whether some discovered parallelism can bring ac-
celeration. To time a function, we have to run it. So we need to
ﬁnd or generate inputs for it. However, inputs that are suitable for
ﬁndingparallelismsarenotimmediatelyavailableforthefollowing
839DiscoveringParallelismsin PythonPrograms ESEC/FSE ’23, December3–9, 2023,San Francisco, CA, USA
Table 2:Parallelization statistics
Package #Func Time #Reported #FP #Par. Par.Rate Precision
scikit-image 242 15m 25 0 25 10.3% 100.0%
scipy 490 30m 65 5 60 12.2% 92.3%
librosa 65 5m 3 0 3 4.6% 100.0%
trimesh 26 2m 6 0 6 23.1% 100.0%
scikit-learn 391 24m 23 2 21 5.4% 91.3%
seaborn 26 2m 5 0 5 19.2% 100.0%
Total 1,240 78m 127 7 120 9.7% 94.5%
reasons.First,theinputsprovidedbythesepackagesarenotsuit-
able. This is because the contained sample inputs for functions are
trivial,theyaremostlytestinputsfordemonstratingpackagesetup
andcannotreﬂecttheusefulnessofparallelisms.Second,itisnot
feasibletouseopen-sourceprogramsthatusethesepackagesfor
inputs.Thisisbecausesuchprogramsusuallyonlycoverasmall
portion of functions in target packages, thus they cannot test them
thoroughly. Furthermore, even such programs themselves need
inputs,buttypicallyonlytrivialinputsareavailable.Tosolvethis
problem,weuseempiricalmethodstogenerateadditionalinputs
for thesepackages.
4.2.1 InputGenerationforscikit-image. scikit-image isanimage
processing package, and most of its functions have at least one
input argument named "image" or "im". As the input image gets
larger,suchfunctionsrunslower,andweexpectthatwithaninput
imageofpropersize,thefunctionwillrunforsometime,makingthe
timingresultmeaningful.Theinputimagesarerandomlygenerated
and we use binary search to determine the proper size. If there are
multiple imageinputs,we keeptheirshape the same.
Besides the image arguments, other arguments need to be deter-
mined.scikit-image usespytest[34] for unit tests, with test scripts
coveringmostfunctions.Weruntheunittestandusebuilt-intrace
module[11]tocollectargumentvalues.Thesevaluesareusedto
feed argumentsother than images, andtheyremain the same dur-
ing the binary search. Finally, we generated inputs successfully for
242functions, makingthemrun for more than1second.
4.2.2 InputGenerationforSciPy,Scikit-learn,andSeaborn. SciPy
isanextensionof NumPy.Mostofitsfunctionshandledataoftype
/u1D45B/u1D451/u1D44E/u1D45F/u1D45F/u1D44E/u1D466.altdeﬁned in NumPy, which is n-dimensional arrays, and
vectors and matrices are special types of /u1D45B/u1D451/u1D44E/u1D45F/u1D45F/u1D44E/u1D466.alt.SciPyalso has a
richsetof pytestunittestscripts,andbytracingtheirexecutions,we
collectvaluesofargumentsforeachfunction.Manyofthemhaveat
least one argument withtype /u1D45B/u1D451/u1D44E/u1D45F/u1D45F/u1D44E/u1D466.alt.We expectthese functions
to run slower as the /u1D45B/u1D451/u1D44E/u1D45F/u1D45F/u1D44E/u1D466.alttyped input gets larger. Similar to
whatisdescribedinSection 4.2.1,weuseabinarysearchmethod
to determine the proper size of input /u1D45B/u1D451/u1D44E/u1D45F/u1D45F/u1D44E/u1D466.alt. The scaled /u1D45B/u1D451/u1D44E/u1D45F/u1D45F/u1D44E/u1D466.alt
inputsaregeneratedrandomly.Iftherearemultiple /u1D45B/u1D451/u1D44E/u1D45F/u1D45F/u1D44E/u1D466.alttyped
inputs,theirsizesarescaledproportionally.Inputsthatarenotof
type/u1D45B/u1D451/u1D44E/u1D45F/u1D45F/u1D44E/u1D466.altremain the same. Similarly, scikit-learn uses/u1D45B/u1D451/u1D44E/u1D45F/u1D45F/u1D44E/u1D466.alt
object to represent training data for machine learning, and seaborn
uses/u1D45B/u1D451/u1D44E/u1D45F/u1D45F/u1D44E/u1D466.altobject to represent the data it aims to visualize. So
weadaoptedthesameapproachtogenerateinputforthem.Finally,
we generated inputs successfully for 490 functions in SciPy, 391
functions in scikit-learn , and 26 functions in seaborn, making them
run for longer than1second.4.2.3 Input Generation for librosa and trimesh. We didn’t ﬁnd any
empiricalrulesforfunctioninputin librosaandtrimesh.Soweman-
ually construct inputs, using their documentations as references.
Finally,weconstructinputsfor65functionsin librosa,andfor26
functionsin trimesh,makingthemrun for more than1second.
4.2.4 CanonicityofGeneratedInputs. Todetectusefulparallelisms
in real-world cases, it is essential to use canonical input. Based
onourunderstandingofthesepackages,wethinkourinputgen-
eration scheme can generate canonical inputs. In a sense, these
packages all provide data processing functions. For example, scikit-
imageprocessesimage, librosaprocessesaudio,and SciPyprocesses
multi-dimensional arrays. The input generation scheme generates
testinputsbyincreasingthesizeofsampleinputs.Sampleinputs
represent some typical use cases, and thus their scaling-ups should
also be canonical. For example, to test scikit-image , we scalesmall
sampleimages to largertest images,and to test librosa, we extend
small sample audiosto longer audios. The generatedinputsare of
reasonable size, and similar inputs can be used in real-world cases.
4.3 DiscoveringParallelism
4.3.1 MainResults. Inthissection,weinvestigatehoweﬀective
PyParis at identifying possible parallelisms. We conduct evalu-
ations on functions and time them. In the selection step , tasks
running for less than 10% of the total time are eliminated. After
that, parallelizable functions whose expected time saving is less
than 20% are discarded. The remaining functions are considered
worthwhile to parallelize. We manually check the correctness of
reportedparallelisms.
TheresultisshowninTable 2,includingthenumberoffunctions
("#Func")undertest,thetimecost("Time")by PyPar,thenumber
of reported parallelizable functions ("#Reported"), the number of
false positives ("#FP"), the number of functions ("#Par.") that can
be parallelized and the corresponding rate ("Par.Rate") out of all
functionsundertest, andthe precision of PyPar.
Thefoundparallelismscanbeclassiﬁedintotwotypes.Inthe
ﬁrsttype,twoprogram unitsexecutedserially intheoriginalpro-
gramareparallelizable.Figure 1(b)givessuchanexample.Inthe
secondtype,aprogramunitwithinaloopisparallelizableindiﬀer-
entiterations. Figure 1(a) gives such an example.
Firstly,theresultshowsthatthereareasigniﬁcantnumberof
parallelismsinPythonpackages.Wespeculatethattheignorance
of parallelisms in real-world Python programs should be common,
becausewhenwritingcode,developersalwaysconcentratemore
oncorrectnessandreadabilitythanonparallelization,Theresult
corroboratesourspeculation.Amongallthe1,240functions,atleast
120 functions (9.7% of all the functions under test) have paralleliza-
tion possibilities. So the ignorance of parallelization possibilities is
not rare. The parallelizable rate in diﬀerent packages are diﬀerent,
whichismainlybecausetheyhandlediﬀerenttypesoftasks.For
example,package librosaprocessesaudio,andelementsinanau-
dio are usually handled in chronologicalorder.Thusthere are few
parallelization possibilities in this package. Package scikit-image
processes image, and diﬀerent parts (i.e. pixels, channels) of an
image are usually processed seperately. Thus there are many paral-
lelizationpossibilitesinthis package.
840ESEC/FSE ’23, December3–9, 2023,San Francisco, CA, USA SiweiWei, Guyang Song,SenlinZhu, Ruoyi Ruan, Shihao Zhu, andYanCai
Table 3:RecallAssessment
Package #Func #Reported #False Negatives #Par Recall
trimesh 26 6 0 6 100.0%
seaborn 26 5 2 7 71.4%
Total 52 11 2 13 84.6%
Secondly,theresultshowsthat PyParcanﬁndmanyparallelisms.
Among all the 1,240 functions, PyParreported 127 parallelizable
functions.Bymanualchecking,wefoundonly7of themarefalse
positives (a 94.5% precision). And all 7 false positives are similar to
thecaseinFigure 6,thatisonly5.5%outofallreportedones.Thus,
we speculate that most real-world programs follow our assump-
tions(Section 3.3)well.Andwefoundthatthefalsepositivesare
easy to ﬁnd by manually checking the parallelism report. Also, the
automaticmethodtodetectfalsepositivesdescribedinSection 3.8.2
can ﬁnd all false positives. The time cost by PyParin ﬁnding paral-
lelisms(excludingthetimebyrunningfunctions)isshownin Time
Costcolumn.Fromthetable,theoveralltimecostforanalyzingall
1,240 functions is only about 78 minutes, i.e., about 3.78 seconds
perfunction.Theresultshowsthat PyParisfastinﬁndingparal-
lelisms.Overall,therichnessofparallelismsfoundby PyPar,the
high precision, and the low time cost all show that PyParis useful
inﬁndingparallelisms.
4.3.2 Comparison with Previous Work. We compare the perfor-
mance of PyParwith previous works. Among the 5 tools discussed
inSection 2.2.2forautomaticparallelizationofPythonprograms,
autoMPHC [ 41] is unavailable, HPAT [ 44] and QuantCloud [ 59]
are domain-speciﬁc and require manual annotations for each func-
tion, and Pydron [ 32] is applicable only for Python2.x. So only
autoParallel[ 39]can be used.
During the evaluation, autoParallel detected only 7 parallelisms
in all the 1,240 functions, andthese parallelismscan also be found
byPyPar.Thisisbecause autoParallelfocuses onpolyhedral opti-
mization and only accepts a speciﬁc form of for loop. PyParfound
signiﬁcantly more parallelisms thanautoParallel.
4.3.3 Recall Assessment. To evaluate the eﬀectiveness of PyParin
ﬁnding parallelisms, precision alone is not enough. A study of how
manyparallelizationpossibilities PyParmisses(i.e.,false-negatives)
isalsoinneed.However,thegroundtruthishardtoobtainbecause
theparallelismsoftargetpackagesarenotimmediatelyavailable
andmanuallyﬁndingparallelismsisexpensive.Soitishardtoassess
the overall recall. Instead, we manually assessed the parallelizabil-
ityoftrimeshandseaborn,tworelativelysmallpackageswith52
functions undertest intotal. The manually found parallelisms are
usedas ground truth.
The result is shown in Table 3, including the number of func-
tionsundertest("#Func"),thenumberofparallelizablefunctions
("#Reported"),thenumberoffalsenegatives,thenumberoftotal
parallelizable functions, andthe recall.
Fromthetable,therecallof PyParishigh,withonly2paralleliza-
tion possibilities missed in all the 13 parallelisms. The 2 missed
parallelismscontainsanif-statementwithinafor-loop.Onlyone
branchof the if-statement is takenduring execution; PyParcannot
infer this and fails to discover the parallelisms. The result shows
thatPyPar’sperformance issatisfactory inthesetwopackages.4.4 MeasuringAcceleration
In thissection,we investigatewhether theparallelisms foundcan
accelerate the original function and how much acceleration can be
obtained.Todoso,weﬁrstrewritetheparallelizablefunctionsinto
parallelizedversions using diﬀerentPythonparallel frameworks.
Diﬀerent Python parallel frameworks have their advantages and
disadvantages,andeachhasitssuitableapplicationscenarios.We
select the classic thread-based parallelization and process-based
parallelization as well as the Ray [ 31] framework. The ﬁrst two are
ThreadPoolExecutorandProcessPoolExecutorinPythonbuilt-in
parallel module concurrent.futures [ 6]. Ray is a Python paralleliza-
tionframework that supports distributedcomputing.
Based on these three parallelization framework, we have four
versions for each parallelizable function: (1) the original version
(denoted "serial"), (2) the version parallelized using ThreadPoolEx-
ecutor (denoted "thread"), (3) the version parallelized using Pro-
cessPoolExecutor (denoted "process"), (4) and the version paral-
lelized using Ray (denoted "ray"). We then time their executions.
For each function, the best one in the 3 parallelized versions is
selectedas paralleltime,i.e.,
/u1D45D/u1D44E/u1D45F/u1D44E/u1D459/u1D459/u1D452/u1D459_/u1D461/u1D456/u1D45A/u1D452=/u1D45A/u1D456/u1D45B{/u1D461ℎ/u1D45F/u1D452/u1D44E/u1D451_/u1D461/u1D456/u1D45A/u1D452,/u1D45D/u1D45F/u1D45C/u1D450/u1D452/u1D460/u1D460 _/u1D461/u1D456/u1D45A/u1D452,/u1D45F/u1D44E/u1D466.alt_/u1D461/u1D456/u1D45A/u1D452}
Wealsocalculatetheexpectedrunningtimeafterparallelization(de-
noted "theory") by substracting expected time savings (see Section
3.6) from serialtime.That is,
/u1D461ℎ/u1D452/u1D45C/u1D45F/u1D466.alt_/u1D461/u1D456/u1D45A/u1D452=/u1D460/u1D452/u1D45F/u1D456/u1D44E/u1D459_/u1D461/u1D456/u1D45A/u1D452−/u1D460/u1D44E/u1D463/u1D452/u1D451_/u1D461/u1D456/u1D45A/u1D452
/u1D461ℎ/u1D452/u1D45C/u1D45F/u1D466.alt_/u1D461/u1D456/u1D45A/u1D452is a lower bound of running time after parallelization
andistakenforreference.Finally,thetheory_timeandparallel_time
are normalizedbythe serialtime to eliminatethe scalediﬀerence.
Alltheseversionsarerepeated100timesandtheirrunningtimes
are averaged. After 100 times of repetition, we observed that the
average standard deviation of normalized running times is only
0.046(<5%), which is low. Thus we think the 100 times repetition
canreducetheimpactofthestochasticityofthescheduler,andcan
giverobustresults.Theparallelizedversionusingrayisexecuted
locally(notdistributed).TheoverallstatisticsareshowninTable
4andtheirdistributionsareshowninFigure 8usingtheboxplot
[53]. Due to page limit, the detailed statistics on each program are
given inthe supplementary ﬁle.
Firstly, the result shows that found parallelisms can achieve
considerable acceleration. From Table 4, the parallelized functions
can achieve an average acceleration of 47%, 44%, 53%, 57%, 39%,
and 47% on the six packages, respectively. In the best case, the
parallelized functions achieve an acceleration of 87%, 90%, 62%,
82%, 85%, and 85% on the six packages, respectively. Even in the
worstcase,theparallelizedfunctionscanachieveanacceleration
of 19%, 9%, 42%, 15%, 5%, and 7% on the six packages, respec-
tively, which is still considerable. Overall, the parallelized code
can achieve an average acceleration of 44% over the serial version,
and in the best case it can bring an acceleration of up to 90% on
functionscipy.signal._lti_conversion.ss2tf .Thestatisticsshowthat
parallelisms foundare useful.
Secondly,theresultshowsthatthepracticalaccelerationachieved
byparallelizationisveryclosetotheoreticalacceleration.AsFigure
8shows,the practical acceleration (measured by /u1D45D/u1D44E/u1D45F/u1D44E/u1D459/u1D459/u1D452/u1D459_/u1D461/u1D456/u1D45A/u1D452) is
veryclosetothetheoreticalacceleration(measuredby /u1D461ℎ/u1D452/u1D45C/u1D45F/u1D466.alt_/u1D461/u1D456/u1D45A/u1D452).
However, using one parallelization framework solely (measured
841DiscoveringParallelismsin PythonPrograms ESEC/FSE ’23, December3–9, 2023,San Francisco, CA, USA
Table 4:Runningtimestatistics(normalized by serial time)
Package Avg. Theory Avg. Parallel Best Parallel Worst Parallel
scikit-image 0.467 0.534 0.135 0.813
scipy 0.510 0.562 0.100 0.915
librosa 0.437 0.470 0.383 0.576
trimesh 0.429 0.430 0.178 0.851
scikit-learn 0.536 0.610 0.146 0.947
seaborn 0.459 0.533 0.152 0.929
Total 0.499 0.555 0.100 0.947
Figure8:Therunningtimedistributioninboxplotwherethe
valuesare normalized
respectivelyby /u1D461ℎ/u1D45F/u1D452/u1D44E/u1D451_/u1D461/u1D456/u1D45A/u1D452,/u1D45D/u1D45F/u1D45C/u1D450/u1D452/u1D460/u1D460_/u1D461/u1D456/u1D45A/u1D452,and/u1D45F/u1D44E/u1D466.alt_/u1D461/u1D456/u1D45A/u1D452)cannot
achievesatisfactoryperformance,becauseeachparallelizationframe-
work has its merits and shortcomings, and diﬀerent parallelization
frameworks should be appliedindiﬀerentsituations.
4.5 Discussion on Evaluation
4.5.1 Observations. We found that the maximal degree of paral-
lelismofmostparallelizablefunctionsfoundby PyParislimitedby
theprogramitself.Thatis,thesourcecodeofthetargetfunction
has determined the maximal number of tasks that can runconcur-
rently, and increasing the number of available CPU cores won’t
bringfurther acceleration.
Also,wefoundthatprocess-basedparallelizationsuﬀersfrom
the high overhead of creating, recycling processes, and transfer-
ringdata.Whendatatransferredbetweenprocessesarelarge,its
overhead can exceed thetime saved byparallelization, causingno
acceleration. In some cases, thread-based parallelization doesn’t
bring acceleration because the global interpreter lock (GIL) pre-
ventsPythonbytecodefrombeingexecutedondiﬀerentthreads,
as discussed in Section 2. While in many other cases, parallelizable
tasks cost the most time by calling pre-compiled functions written
inlow-levellanguagesratherthaninterpretingPythonbytecode.In
such cases, thread-based parallelization advantages from the lower
overheadofcreatingandrecyclingthreadsandtransferringdata,
thusperformsbetterthanothertypesofparallelization.Inmany
cases, ray has superior performance than process-based paralleliza-
tion, because it adopts many heuristic strategies, such as read-only
sharedmemory,to improve its performance.
4.5.2 Implementation Limitations. There are several limitations
of our prototype auto-rewriting implementation. First, the auto-
rewritten code can be of poor readability. This is because the auto-
rewrittencodecanberedundant.Forexample,considertheauto-
rewrittencodeinFigure 2.Twoassistantfunctions /u1D461/u1D44E/u1D460/u1D4581and/u1D461/u1D44E/u1D460/u1D4582
areintroduced.However,itisnotnecessarysinceeachtaskonly
callsonefunction.Also,thereturnvalueofeachtaskiscontainedin
atuple,butitisalsonotnecessarysinceonlyonevalueisreturned.
Second, the auto-rewritten code can introduce extra overhead. Forexample, consider a task that uses the return value of /u1D434./u1D454/u1D452/u1D461(/u1D456)
where/u1D434is a large object and the return value is small. The auto-
rewrittencodewillpass /u1D434tothetaskbecauseitreadsA,whichcan
introduce large overhead when using process-based parallelization
frameworks,sincepassingalargeobjectbetweenprocessesiscostly.
Itisbettertocompute /u1D434./u1D454/u1D452/u1D461(/u1D456)outsidethetaskandpassthereturn
value toit. Third, as the parallelism detected can be false-positive,
the auto-rewritten code can introduce errors.
Currently,wefocusmoreondiscoveringparallelismsthanrewrit-
ing, but we think the ﬁrst two limitations can be addressed and
the third limitation can be mitigated in a future version with more
advancedprogramanalysistechniques.During theevaluation,we
found that there are some cases when the auto-rewritten code
introduces extraoverhead, andwe correctsuch casesmanually.
5 FUTUREWORK
There are some possible directions to improve the performance
ofPyPar. Firstly,PyParuses a simple data dependence analysis
method, which can result in false positives. By reﬁning the data
dependence analysis, the precision can be increased. Secondly, the
built-intracemoduleisusedtotimeeachpartofthetargetprogram,
which adds overhead and perturbs running time. Amore accurate
timingtechniquecanhelpalot.Thirdly,weusedempiricalinput
generation methods. There are still a large number of functions
that remain untested. An eﬀective input generation strategy can
signiﬁcantlyimprovetheperformanceof PyPar.Fourthly,amethod
toeﬃcientlydiscoverparallelismswhenprogramsevolveisinneed
to reduce the costofrecomputation.
6 CONCLUSION
Parallelization is an important technique to improve the perfor-
manceofPythonprograms.Inthispaper,weproposed PyPar,an
eﬀective tool aiming at ﬁnding parallelizationpossibilities missed
by programmers in large-scale real-world Python programs. Py-
Paris easy to use and universally applicable. The evaluation result
on four widely-used Python packages shows that there are rich
parallelisms(almost10%ofallfunctions)missedbyprogrammers
even in well-designed real-world Python programs and PyParis
eﬀective in ﬁnding them. The parallelisms found by PyParcan
bringsigniﬁcant (upto 90%andonaverage44%) acceleration.
7 DATA AVAILABILITY
OurPyPar is open-sourced (available at :
https://github.com/PyParTool/PyPar ).Thepackagesusedfortest
are open-sourced, and their links can be found in corresponding
references.
ACKNOWLEDGEMENTS
ThisworkissupportedinpartbytheNationalKeyResearchand
Development Program of China (No. 2022YFB3104004), National
Natural Science Foundation of China (NSFC) (Grant No. 61932012,
62232016), the Key Research Program of Frontier Sciences, CAS
(Grant No. ZDBS-LY-7006), the Youth Innovation Promotion As-
sociationoftheChineseAcademyofSciences(YICAS)(GrantNo.
Y2021041),andsponsoredbyCCF-AFSGResearch Fund.
842ESEC/FSE ’23, December3–9, 2023,San Francisco, CA, USA SiweiWei, Guyang Song,SenlinZhu, Ruoyi Ruan, Shihao Zhu, andYanCai
REFERENCES
[1][n.d.]. ast—AbstractSyntaxTrees. https://docs.python.org/3/library/ast.html .
Accessed:2022-12-21.
[2][n.d.]. ast — Abstract Syntax Trees. https://docs.python.org/3/library/ast.html#
ast.comprehension . Accessed:2022-12-21.
[3][n.d.]. Awesome Python. https://github.com/vinta/awesome-python . Accessed:
2022-12-21.
[4][n.d.]. AwesomeScientiﬁcComputing. https://github.com/nschloe/awesome-
scientiﬁc-computing . Accessed:2022-12-21.
[5][n.d.]. Basic Installation - trimesh 3.17.1 documentation. https://trimsh.org/ .
Accessed:2022-12-21.
[6][n.d.]. concurrent.futures — Launching parallel tasks. https://docs.python.org/3/
library/concurrent.futures.html . Accessed:2022-12-21.
[7][n.d.]. Joblib: running Python functions as pipeline jobs. https://joblib.
readthedocs.io/en/latest/ . Accessed:2022-12-21.
[8][n.d.]. multiprocessing—Process-basedparallelism. https://docs.python.org/3/
library/multiprocessing.html . Accessed:2022-12-21.
[9][n.d.]. threading — Thread-based parallelism. https://docs.python.org/3/library/
threading.html . Accessed:2022-12-21.
[10][n.d.]. TIOBE Index - TIOBE. https://www.tiobe.com/tiobe-index/
programminglanguages_deﬁnition/ . Accessed:2022-12-21.
[11] [n.d.]. trace—TraceortrackPythonstatementexecution. https://docs.python.
org/3/library/trace.html . Accessed:2022-12-21.
[12]Martín Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen,
Craig Citro, Greg S. Corrado, Andy Davis, Jeﬀrey Dean, Matthieu Devin, San-
jay Ghemawat, Ian Goodfellow, Andrew Harp, Geoﬀrey Irving, Michael Isard,
YangqingJia,RafalJozefowicz,LukaszKaiser,ManjunathKudlur,JoshLevenberg,
DandelionMané,RajatMonga,SherryMoore,DerekMurray,ChrisOlah,Mike
Schuster, Jonathon Shlens, Benoit Steiner, Ilya Sutskever, Kunal Talwar, Paul
Tucker,VincentVanhoucke,VijayVasudevan,FernandaViégas,OriolVinyals,
Pete Warden, Martin Wattenberg, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng.
2015. TensorFlow: Large-Scale Machine Learning on Heterogeneous Systems.
https://www.tensorﬂow.org/ Softwareavailable from tensorﬂow.org.
[13]MiguelÁAbella-González,PedroCarollo-Fernández,Louis-NoëlPouchet,Fab-
rice Rastello, and Gabriel Rodríguez. 2021. PolyBench/Python: benchmarking
Pythonenvironmentswithpolyhedraloptimizations.In Proceedingsofthe30th
ACMSIGPLANInternationalConferenceonCompilerConstruction .59–70. DOI:
https://doi.org/10.1145/3446804.3446842 .
[14]AlfredV.Aho,MonicaS.Lam,RaviSethi,andJeﬀreyD.Ullman.2006. Compilers:
Principles, Techniques, and Tools (2nd Edition) . Compilers: Principles, Techniques,
and Tools (2nd Edition).
[15]Mehdi Amini, Béatrice Creusillet, Stéphanie Even, Ronan Keryell, Onig Goubier,
Serge Guelton, Janice Onanian McMahon, François-Xavier Pasquier, Grégoire
Péan,PierreVillalon,etal .2012. Par4all:Fromconvexarrayregionstohetero-
geneous computing. In 2nd International Workshop on Polyhedral Compilation
Techniques, Impact(Jan 2012) .
[16]Zina A Aziz, Diler Naseradeen Abdulqader, Amira Bibo Sallow, and Her-
man Khalid Omer. 2021. Python parallel processing and multiprocessing: A
rivew.Academic Journal of Nawroz University 10, 3 (2021), 345–354. DOI:
https://doi.org/10.25007/ajnu.v10n3a1145 .
[17]Utpal Banerjee. 1988. An introduction to a formal theory of dependence
analysis. The Journal of Supercomputing 2 (1988), 133–149. DOI:
https://doi.org/10.1007/bf00128174 .
[18]DavidBeazley.2010. Understandingthepythongil.In PyCONPythonConference.
Atlanta,Georgia .
[19]BillBlume,RudolfEigenmann,KeithFaigin,JohnGrout,JayHoeﬂinger,David
Padua,PaulPetersen,BillPottenger,LawrenceRauchwerger,PengTu,etal .1994.
Polaris: The next generation in parallelizing compilers. In Proceedings of the
SeventhWorkshoponLanguagesandCompilersforParallelComputing .Citeseer,
141–154.
[20]UdayBondhugula,MuthuBaskaran,SriramKrishnamoorthy,JagannathanRa-
manujam, Atanas Rountev, and Ponnuswamy Sadayappan. 2008. Automatic
transformationsforcommunication-minimizedparallelizationandlocalityop-
timization in the polyhedral model. In International Conference on Compiler
Construction .Springer,132–146. DOI: https://doi.org/10.1007/978-3-540-78791-
4_9.
[21]UdayBondhugula,AlbertHartono,JRamanujam,andPSadayappan.2008. Pluto:
A practical and fully automatic polyhedral program optimization system. In
Proceedings of the ACM SIGPLAN 2008 Conference on Programming Language
Designand Implementation(PLDI08),Tucson,AZ(June 2008) . Citeseer.
[22]Xing Cai, Hans Petter Langtangen, and Halvard Moe. 2005. On the per-
formance of the Python programming language for serial and parallel sci-
entiﬁc computations. Scientiﬁc Programming 13, 1 (2005), 31–56. DOI:
https://doi.org/10.1155/2005/619804 .
[23]Lisandro Dalcín, Rodrigo Paz, Mario Storti, and Jorge D’Elía. 2008. MPI for
Python:PerformanceimprovementsandMPI-2extensions. J.ParallelandDistrib.
Comput.68,5 (2008), 655–662. DOI: https://doi.org/10.1016/j.jpdc.2007.09.005 .[24]JustoGonzalez,Julian Taylor,SandraCastro,JeﬀKern,JensKnudstrup,Stefano
Zampieri, Alisdair Manning, Sanjay Bhatnagar, Lindsey Davis, Kumar Golap,
etal.2019. Pythoncodeparallelization,challengesandalternatives. Astronomical
DataAnalysisSoftwareand SystemsXXVI 521(2019), 515.
[25]Tobias Grosser, Hongbin Zheng, Raghesh Aloor, Andreas Simbürger, Armin
Größlinger, and Louis-Noël Pouchet. 2011. Polly-Polyhedral optimization in
LLVM.In ProceedingsoftheFirstInternationalWorkshoponPolyhedralCompilation
Techniques(IMPACT) , Vol. 2011.1.
[26]JohnLHennessy and DavidAPatterson. 2011. Computerarchitecture:a quanti-
tativeapproach . Elsevier. DOI: https://doi.org/10.1016/0026-2692(93)90111-q .
[27]Sang-Ik Lee, Troy A Johnson, and Rudolf Eigenmann. 2003. Cetus–an extensible
compiler infrastructure for source-to-source transformation. In International
Workshop on Languages and Compilers for Parallel Computing . Springer, 539–553.
DOI:https://doi.org/10.1007/978-3-540-24644-2_35 .
[28]Dror E Maydan, John L Hennessy, and Monica S Lam. 1991. Eﬃcient and ex-
act data dependence analysis. In Proceedings of the ACM SIGPLAN 1991 con-
ference on Programming language design and implementation . 1–14. DOI:
https://doi.org/10.1145/113445.113447 .
[29]Brian McFee, Colin Raﬀel, Dawen Liang, Daniel P Ellis, Matt McVicar, Eric
Battenberg,andOriolNieto.2015. librosa:Audioand musicsignalanalysisin
python.In Proceedingsofthe14thpythoninscienceconference ,Vol.8.18–25. DOI:
https://doi.org/10.25080/majora-7b98e3ed-003 .
[30]WesMcKinneyetal .2011. pandas:afoundationalPythonlibraryfordataanalysis
and statistics. Pythonfor highperformanceand scientiﬁccomputing 14,9(2011),
1–9.
[31]Philipp Moritz, Robert Nishihara, Stephanie Wang, Alexey Tumanov, Richard
Liaw,EricLiang,MelihElibol,ZonghengYang,WilliamPaul,MichaelIJordan,
etal.2018. Ray:AdistributedframeworkforemergingAIapplications.In 13th
USENIXSymposiumonOperatingSystemsDesignand Implementation(OSDI18) .
561–577.
[32]Stefan C Müller, Gustavo Alonso, and André Csillaghy. 2014. Scaling astroinfor-
matics:Python+automaticparallelization. Computer 47,9(2014),41–47. DOI:
https://doi.org/10.1109/mc.2014.262 .
[33]Flemming Nielson, Hanne R Nielson, and Chris Hankin. 2015. Principles of
programanalysis . springer. DOI: https://doi.org/10.1007/978-3-662-03811-6 .
[34] BrianOkken. 2022. PythonTestingwith pytest . Pragmatic Bookshelf.
[35] Travis EOliphant.2006. Aguide toNumPy . Vol. 1. TrelgolPublishing USA.
[36]F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M.
Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cour-
napeau,M.Brucher,M.Perrot,andE.Duchesnay.2011. Scikit-learn:Machine
Learning in Python. Journal of Machine Learning Research 12 (2011), 2825–2830.
DOI:https://dl.acm.org/doi/10.5555/1953048.2078195 .
[37]Louis-Noël Pouchet, Cédric Bastoul, Albert Cohen, and John Cavazos.
2008. Iterative optimization in the polyhedral model: Part II, multidi-
mensional time. ACM SIGPLAN Notices 43, 6 (2008), 90–100. DOI:
https://doi.org/10.1145/1379022.1375594 .
[38]Louis-Noël Pouchet, Uday Bondhugula, Cédric Bastoul, Albert Cohen, Jagan-
nathanRamanujam,PonnuswamySadayappan,andNicolasVasilache.2011.Loop
transformations:convexity,pruningandoptimization. ACMSIGPLANNotices 46,
1 (2011), 549–562. DOI: https://doi.org/10.1145/1925844.1926449 .
[39]Cristian Ramon-Cortes, Ramon Amela, Jorge Ejarque, Philippe Clauss, and
Rosa M Badia. 2020. AutoParallel: Automatic parallelisation and distributed
execution of aﬃne loop nests in Python. The International Journal of
High Performance Computing Applications 34, 6 (2020), 659–675. DOI:
https://doi.org/10.1177/1094342020937050 .
[40]MatthewRocklin.2015. Dask:Parallelcomputationwithblockedalgorithmsand
task scheduling.In Proceedings ofthe 14thpythoninscience conference , Vol.130.
SciPy Austin,TX, 136. DOI: https://doi.org/10.25080/majora-7b98e3ed-013 .
[41]Jun Shirako, Akihiro Hayashi, Sri Raj Paul, Alexey Tumanov, and Vivek
Sarkar. 2022. Automatic Parallelization of Python Programs for Distributed
Heterogeneous Computing. arXiv preprint arXiv:2203.06233 (2022). DOI:
https://doi.org/10.1007/978-3-031-12597-3_22 .
[42]Michael Sipser. 2008. Introduction to the Theory of Computation. Acm Sigact
News27,1 (2008). DOI: https://doi.org/10.1145/230514.571645 .
[43]RobertTarjan.1972. Depth-ﬁrstsearchandlineargraphalgorithms. SIAMjournal
oncomputing 1,2 (1972), 146–160. DOI: https://doi.org/10.1109/swat.1971.10 .
[44]Ehsan Totoni, Todd A. Anderson, and Tatiana Shpeisman. 2017. HPAT: high
performance analytics with scripting ease-of-use. In Proceedings of the Inter-
national Conference on Supercomputing, ICS 2017, Chicago, IL, USA, June 14-
16, 2017, William D. Gropp, Pete Beckman, Zhiyuan Li, and Francisco J. Ca-
zorla (Eds.). ACM, 9:1–9:10. https://doi.org/10.1145/3079079.3079099 DOI:
https://doi.org/10.1145/3079079.3079099 .
[45]Stefan Van der Walt, Johannes L Schönberger, Juan Nunez-Iglesias, François
Boulogne, Joshua D Warner, Neil Yager, Emmanuelle Gouillart, and Tony Yu.
2014. scikit-image: image processing in Python. PeerJ2 (2014), e453. DOI:
https://doi.org/10.7287/peerj.preprints.336v2 .
[46]GuidoVanRossumandFredL.Drake.2009. Python3ReferenceManual . CreateS-
pace, Scotts Valley, CA.
843DiscoveringParallelismsin PythonPrograms ESEC/FSE ’23, December3–9, 2023,San Francisco, CA, USA
[47]GuidoVanRossumandFredLDrakeJr.1995. PythonReferenceManual . Centrum
voorWiskunde en Informatica Amsterdam.
[48]Guido Van Rossum and Fred L Drake Jr. 1995. Python tutorial . Centrum voor
Wiskunde en Informatica Amsterdam,The Netherlands.
[49]Pauli Virtanen, Ralf Gommers, Travis E Oliphant, Matt Haberland, Tyler
Reddy, David Cournapeau, Evgeni Burovski, Pearu Peterson, Warren Weckesser,
Jonathan Bright, et al .2020. SciPy 1.0: fundamental algorithms for scien-
tiﬁc computing in Python. Nature methods 17, 3 (2020), 261–272. DOI:
https://doi.org/10.1038/s41592-019-0686-2 .
[50]Michael Wagner, Germán Llort, Estanislao Mercadal, Judit Giménez, and
Jesús Labarta. 2017. Performance analysis of parallel python appli-
cations. Procedia Computer Science 108 (2017), 2171–2179. DOI:
https://doi.org/10.1016/j.procs.2017.05.203 .
[51]Michael L. Waskom. 2021. seaborn: statistical data visualization. Journal of
Open Source Software 6, 60 (2021), 3021. https://doi.org/10.21105/joss.03021 DOI:
https://doi.org/10.21105/joss.03021 .
[52]Mark Weiser. 1984. Program slicing. IEEE Transactions on software engineering 4
(1984), 352–357. DOI: https://dl.acm.org/doi/10.5555/800078.802557 .
[53]David F Williamson, Robert A Parker, and Juliette S Kendrick. 1989. The box
plot:asimplevisualmethodtointerpretdata. Annalsofinternalmedicine 110,11(1989), 916–921. DOI: https://doi.org/10.7326/0003-4819-110-11-916 .
[54]Michael Edward Wolf. 1992. Improving locality and parallelism in nested loops .
stanford university.
[55]MichaelWolfeandUtpalBanerjee.1987. Datadependenceanditsapplication
toparallelprocessing. InternationalJournalofParallelProgramming 16(1987),
137–178. DOI: https://doi.org/10.1007/bf01379099 .
[56] Raymond EWright.1995. Logisticregression. (1995).
[57]GiancarloZaccone.2015. Pythonparallelprogrammingcookbook .PacktPublishing
Ltd.
[58]MateiZaharia,ReynoldSXin,PatrickWendell,TathagataDas,MichaelArmbrust,
Ankur Dave, Xiangrui Meng, Josh Rosen, Shivaram Venkataraman, Michael J
Franklin, et al .2016. Apache spark: a uniﬁed engine for big data processing.
Commun. ACM 59,11(2016), 56–65.
[59]PengZhang,YuxiangGao,andXiangShi.2018. QuantCloud:ASoftwarewith
AutomatedParallelPythonforQuantitativeFinanceApplications.In 2018IEEE
InternationalConferenceonSoftwareQuality,ReliabilityandSecurity,QRS2018,
Lisbon,Portugal,July16-20,2018 .IEEE,388–396. https://doi.org/10.1109/QRS.
2018.00052 DOI:https://doi.org/10.1109/qrs.2018.00052 .
Received 2023-02-02; accepted 2023-07-27
844