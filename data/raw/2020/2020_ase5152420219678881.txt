Data-Driven Design and Evaluation
of SMT Meta-Solving Strategies:
Balancing Performance, Accuracy, and Cost
Malte Mues
TU Dortmund University
Dortmund, Germany
malte.mues@tu-dortmund.deFalk Howar
TU Dortmund University
Dortmund, Germany
falk.howar@tu-dortmund.de
Abstract —Many modern software engineering tools integrate
SMT decision procedures and rely on the accuracy and per-
formance of SMT solvers. We describe four basic patterns forintegrating constraint solvers (earliest verdict, majority vote,feature-based solver selection, and verdict-based second attempt)that can be used for combining individual solvers into meta-decision procedures that balance accuracy, performance, and cost– or optimize for one of these metrics. In order to evaluatethe effectiveness of meta-solving, we analyze and minimize 16
existing benchmark suites and benchmark seven state-of-the-art SMT solvers on 17k unique instances. From the obtained
performance data, we can estimate the performance of differentmeta-solving strategies. We validate our results by implementingand analyzing two strategies. As additional results, we obtain (a)the ﬁrst benchmark suite of unique SMT string problems withvalidated expected verdicts, (b) an extensive dataset containingdata on benchmark instances as well as on the performance ofindividual decision procedures and several meta-solving strategieson these instances, and (c) a framework for generating data thatcan easily be used for similar analyses on different benchmarkinstances or for different decision procedures.
I. I NTRODUCTION
Modern software analysis tools are complex and rely on
SMT solvers for automated reasoning. Since the introduction
of Z3 [1] and CVC4 [2], many research solvers have beendeveloped and specialize in different aspects of certain SMTtheories. Researchers and industry frequently use these solversin their own tools. Decision procedures for constraints overstring variables and string operations, e.g., are one key enablerfor the formal analysis of Web-applications that rely heavilyon string variables for passing URLs and request parameters(e.g., [3], [4], [5], [6], [7]). The development of securityanalysis tools and advances in the area of string theory solving(e.g., [8], [9], [10], [11], [12], [13]) are closely connected. Inthe ﬁeld of J
AVA Web-application analysis the J AVA String
Analyzer [5] was presented more than 15years ago and
evolved into the integration of string method encoding basedon modern SMT solvers in symbolic execution engines likeSymbolic PathFinder [14]. Nevertheless, until today, newer
J
AVA analyzers [7], [15], [16] still report about problems
and challenges in the encodings of string operations whileexploring J
AVA code.Recent advances in string theory solving have been ac-
companied by new tools for testing and fuzzing SMT solvers(e.g., [17], [18], [19], [20]). These tools uncovered variousbugs in the implementation of decision procedures. Moreover,at SMT-COMP 2020 the only two competing solvers in thestring related categories, CVC4 [10] and Z3str4
1, had a couple
of disagreements on the correct solutions for the providedbenchmark tasks
2.
While this is natural for cutting edge research tools, users
of this technology need strategies for integrating such compo-nents without jeopardizing the validity of obtained researchresults and for minimizing the harmful potential of bugs— or at least methods for analyzing potential bias andconﬁdence in obtained analysis results. Existing approachesthat are employed by individual tool developers or researchcommunities are constructive and analytical and include thedevelopment of proof/witness-producing analyses (e.g., [21],[22], [23]), portfolio-techniques (e.g., [24]), tool integrationplatforms [25], and benchmarking (e.g., [26]).
We propose to increase accuracy and validity through in-
tegration of multiple decision procedures. In this paper, wedemonstrate that a data-driven design of such meta-solvingstrategies is possible. We discuss a set of four basic patternsfor the integration of multiple solvers, balancing response time,performance, accuracy, and cost for obtaining the results. Weuse these basic patterns for constructing integrated analysesfrom seven constraint solvers. We validate the design pat-terns by evaluating the performance of these seven individualsolvers and four integrated solvers. To run the evaluation,we pre-processed and prepared existing string benchmarksleading to the ﬁrst string benchmark suite of SMT stringproblems with validated expected verdicts. We describe aframework for analyzing data that can be used for the designand evaluation of meta-solving strategies. We propose fourdifferent meta-solving strategies using the framework and haveimplemented two of them. We simulated the other two withthe available data and evaluate all four strategies against the
1https://z3str4.github.io
2https://smt-comp.github.io/2020/disagreements/qf-slia-single-query
1792021 36th IEEE/ACM International Conference on Automated Software Engineering (ASE)
DOI 10.1109/ASE51524.2021.000262021 36th IEEE/ACM International Conference on Automated Software Engineering (ASE) | 978-1-6654-0337-5/21/$31.00 ©2021 IEEE | DOI: 10.1109/ASE51524.2021.9678881
978-1-6654-0337-5/21/$31.00  ©2021  IEEE
seven individual solvers on the benchmark set.
Our evaluation shows that all tested solvers have individual
performance proﬁles and that no solver dominates on all
benchmark instances. Nevertheless, CVC4 is the best single
solver in the evaluation and can only be outperformed bymore expensive meta-solving strategies. Moreover, the earliest
verdict strategy, which is often touted as a viable solution,
was prone to incorrect verdicts in our experiments. Using theintroduced verdict-based second attempt pattern allows us to
increase performance and reliability at moderate costs.
We accompany our paper with a corresponding artifact
that consists of all data produced in the experiments (in adatabase) along with all the scripts used for generating the datareported in tables and ﬁgures in this paper as well as of theinfrastructure that was used for conducting the experiments. Itcontains the per instance performance of each solver allowingbetter comparison between solvers in the future and make iteasy to extend the comparison by other solvers. This data isavailable for others interested in detailed performance data ofa solver [27].
Outline. In the next section, we present a selection of relevant
related work for this paper. Section III presents solver integra-
tion pattern and Section IV describes our preparation of thebenchmarks used for the data-driven design of concrete meta-solving strategies presented in Section V and their evaluationin Section VI. We discuss the results and draw conclusions inSection VII and Section VIII.
II. R
ELATED WORK
The presented approach builds on ideas from three areas:
Integration of formal methods, meta SMT-Solving, and stringtheory solving.
A. Integration of F ormal Methods
The integration of formal methods is still challenging as
many formal methods tend to be designed as single method,
but the recent history shows that integration becames more im-portant. H ¨ahnle et al. [28] discuss this challenge for deductive
veriﬁcation tools. Damiani et al. [29] demonstrate how tool re-ﬁnement can be used to orchestrate multiple veriﬁcation toolsin a divide-and-conquer style. The electronic tool integrationplatform [25] was an early approach to standardize how toolscould be combined. The SV-COMP uses competition tools toconﬁrm produced witnesses enforcing some kind of provabilityof the verdict [22].
B. Meta Solving
The idea of solver abstraction layers and meta solving
support have been proposed for the ﬁrst time more than
a decade ago (c.f. [30]). The solver abstractions are oftendeveloped hand in hand with a veriﬁcation tool. They allowto access the solvers as a library in the language of the tooland support features the developer of the software veriﬁcationtool have been interested in. CPA
CHECKER ,e.g., comes along
with J AVASMT [31], JD ART introduced the abstraction library
JCONSTRAINTS [32], and Cok developed JSMTLIB [33]along with the development of SMT-Lib. In the Python eco-system,
PYSMT [34] is very popular as an abstraction layer
and for C++, META SMT [35] formed around the K LEE tool.
MachSMT [36] is a recently introduced approach for machinelearning based selection of an SMT solver based on expectedperformance.
META SMT supports parallel solving in the K LEE solving
chain [37] and is used for speeding up K LEE. In the original
META SMT paper [35], the authors described and already
noticed the importance of cross checking solver results adecade ago. They crosschecked all the results obtained fromdifferent solver backends of
META SMT against Z3. It does not
support any kind of inter solver checking or unsatisﬁable corevalidation on the ﬂy.
PYSMT has a parallel solving support as
well, but no other advanced meta solving strategies comparableto the C
VCSEQCORES solving strategy we propose in this
paper (c.f. Section III).
A unique feature of JC ONSTRAINTS is that it supports the
validation of SMT-Lib models in the J AVA semantics. This
is useful in the case of ground truth label validation and weadapted the code for model validation to match the completeUnicode theory of SMT-Lib 2.6.
ZaligVinder [26] is a framework executing multiple solvers
on different benchmarks and visualises the results per bench-mark. While this is useful tool and similiar to parts of theproposed workﬂow in this paper, the ZaligVinder’s authorshave not run any analysis of the results nor the benchmarks.
C. String Theory Solver
ABC Aydin et al. [38] presented a model counting string
theory solver called ABC in 2015 and still maintain it. This
research project is embedded into Bultan et al.’s book onstring analysis for software veriﬁcation [13]. The tool is notdistributed as a pre-built release binary, so we have built themaster
3branch and used the resulting binary without any
further adoptions.
Ostrich Chen et al. presented O STRICH [12] more recently
in 2019 and the development is still active. The solver is
supposed to be easily extensible and has one solving strategythat relies on simple functions. For more complex functionsand nondeterministic string operations, the solver incorporatesthe S
LOTH [39] solver in the background. S LOTH relies on
model checking algorithms like IC3. In this comparison, weused O
STRICH ‘s ofﬁcial release version 1.0.14. Since ostrich
appears to include S LOTH and is presented as the more
efﬁcient string solver in the paper on O STRICH , we excluded
SLOTH from our experiments.
Norn Abdulla et al. introduced N ORN in 2014 [40]. The paper
also established the Norn benchmark used until today. One
focus of the Norn solver are regular membership queries andthis is also dominant in the Norn benchmark. While N
ORN
shines on the regular membership queries in SMT problems,the older versions of CVC4 and the Z3
STR family used for
3on: https://github.com/vlab-cs-ucsb/ABC/commit/8b10049
4https://github.com/uuveriﬁers/ostrich/releases/tag/v1.0.1
180problem
A B ··· N
vote
verdictwait for allproblem
A B ··· N
verdictearliest w/
conditionproblem
A
B C
verdictoneverdict-based
decisionproblem
A B
verdictoneinput-based
decision
Fig. 1: Basic constraint-solver integration patterns: majority vote, earliest verdict, verdict-based second attempt or validation,
feature-based / capability-based solver selection (from left to right).
comparison in the original paper have already been better than
NORN back in 2014 on the Kaluza benchmark. However, we
excluded N ORN from the experiments. N ORN might have its
place in a a meta-solver tackling workloads with many regularexpression membership constraints.
Trau Abdulla et al. [11] presented T
RAU in 2018. It is a CE-
GAR based solver, but is not actively maintained5. Therefore,
we excluded it from the comparison in this paper in favor of
actively maintained solvers, but data for Trau is included inthe artifact.
Z3 sequence solver (SEQ) Z3 [1] with its own sequence
solver [9] is one of the SMT solvers that have been around
for over a decade now. As there are a couple of other stringbackends in Z3 available that have been integrated over time,we refer in the paper to the sequence solver backend as SEQ.We used Z3 in a slightly newer version than the ofﬁcial 4.8.10release build from its master branch
6.
Z3str-Family Ganesh et al. [8], [41], [42] developed the
Z3str solver family. The newest member in the family is
Z3STR47, while Z3 STR3 is part of the ofﬁcial Z3 distribution.
Whenever we run Z3 STR3, it is the above mentioned Z3
binary conﬁgured to use the appropriate backend.
CVC4 Barett et al. [2] introduced CVC4 a very com petitive
main stream SMT solver that is often used as Z3’s counterpart
in evaluations. CVC4 is a DPLL(T) style solver with many
different specialized theory backends, also one for strings [10].We used the ofﬁcial 1.8 release version
8.
Princess R¨ummer et al. presented P RINCESS [43] a full SMT
solver written in Scala. While the others are written in C++.Princess is based on Presburger arithmetic. We used a recentofﬁcial release
9.
S3Trinh et al. [4] presented S3, the symbolic string solver for
vulnerability analysis with a competitive performance. Withthe introduction of N
ORN, some soundness issues [40] have
been reported. There was S3P [44] as a follow up version butit is not actively maintained at the moment. We excluded it
5https://github.com/diepbp/z3-trau
6https://github.com/Z3Prover/z3/commit/4c3c15c
7https://z3str4.github.io
8https://github.com/CVC4/CVC4/releases/tag/1.8
92020-03-12: http://www.philipp.ruemmer.org/princess-sources.shtmlfrom our experiments. There was only one update version in2020 requiring an outdated Ubuntu
10.
III. S OLVER INTEGRATION PATTERNS
The goal of the integration of multiple SMT solvers is
usually an increase in the number of solved tasks or somekind of cross-validation between solvers. Depending on thechosen strategy, this increase in performance and reliability iseither paid for by longer response times or increased hardwaredemands. We derive four basic patterns from portfolio solversproposed in literature (c.f. [30], [32], [35], [45]). Figure 1sketches them. The remainign section explains these integra-tion in detail. These four basic patterns can be combinedhierarchically as a basis for more complex meta-solver ormulti-solver decision procedures.
A. Majority V ote
The Majority V ote pattern (left of Figure 1) integrates
different constraint solvers (or multiple instances of the same
solver)AtoNby analyzing a problem instance with all
solvers. After the return of all solvers or the exhaustion ofthe resource share per solver (e.g., time limit), a majority vote(or any other aggregation of the obtained individual verdicts)is computed as a ﬁnal verdict.
Beneﬁts vs. Cost. This pattern can increase the conﬁdence in
obtained results and can help with identifying potential bugs
in underlying constraint solvers. But it is very expensive asusingnsolvers can lead to an n-fold increase in response
time (if solvers are executed sequentially) and/or resources(in case solvers are executed in parallel). Solvers that solvea problem fast or fail early are optimal candidates for thispattern. Solvers that tend to timeout in the given resource limitwithout reaching a result are less helpful as they increase thecost without hardening the result in the worst case.
B. Earliest V erdict
The Earliest V erdict pattern (center-left of Figure 1) in-
tegrates constraint solvers AtoNby analyzing a problem
instance with all solvers. The ﬁrst obtained verdict is used as
the ﬁnal verdict — errors and unknowns may be disregarded.
10https://trinhmt.github.io/home/S3/
181Beneﬁts vs. Cost. This pattern optimizes response times and
the number of successful analyses at the expense of parallel re-
source consumption. It is often instantiated by calling the sameconstraint solver with different random seeds or conﬁgurationoptions in order to proﬁt from effects of randomization
11[30].
As we will show in Figure 3, this pattern is not suitedfor improving correctness and requires sufﬁcient trust in theinvolved solvers. The invested resources pay off if the totalnumber of solved instances is more important than correctnessor if all solvers are known to be reliable upfront.
C. V erdict-based second attempt or validation
This pattern (center-right of Figure 1) integrates multiple
constraint solvers by ﬁrst calling solver Aand then deciding
on a next step on the basis of the obtained verdict. Potential
next steps can be (a) using the verdict of Aas ﬁnal verdict,
(b) invoking another solver on the original problem (e.g., inthe case that Adid not provide a verdict), or (c) using another
solver or tool to validate the result computed by A. Strategies
for validation comprise (i) evaluating the problem instancewith an obtained model, (ii) checking an unsatisﬁable coreby a second solver, (iii) checking an obtained model and theoriginal problem with another solver, or (iv) switching theencoding or tactic (e.g., the F
EAL solver integration [32]).
Beneﬁts vs. Cost. While this pattern can increase conﬁdence
in obtained results and the number of successful analyses ina more meaningful way than the previous two patterns, itrequires deeper integration between solvers for communicatingmodels and unsatisﬁable cores or additional tools for evaluat-ing instances and models. This leads to higher running timesand requires support in the solvers for tracking unsatisﬁablecores.
D. Feature-based / capability-based solver selection.
This pattern (center-right of Figure 1) integrates multiple
solvers by selecting one constraint solver for a problem
instance based on features of the problem instance (e.g., usedoperators and sorts) and based on capabilities of individualsolvers. The pattern is, e.g., instantiated in works that trainmodels for selecting a solver that will most likely analyze aproblem instance successfully.
Beneﬁts vs. Cost. Feature-based or capability-based solver se-
lection can optimize performance and the number of successful
analyses with fewer resources than for example the earliestverdict pattern but relies on the existence and identiﬁcationof characteristic features in problem instances and/or solvers.This can add additional complexity to the task of designingan integration of constraint solvers and may easily be biasedthrough training sets that are not representative.
IV . SMT S
TRING BENCHMARKS
We need a representative benchmark set that includes ex-
pected verdicts as a basis for data-driven design decisions andas a basis for the comparative evaluation of the meta-solving
11http://cvc4.cs.stanford.edu/wiki/Tutorials#Parallel Solvingstrategies to be developed. Since (a) we have no means ofdeciding if a given benchmark set is representative of all SMTstring problems (or even of some particular application arealike program analysis), and since (b) most of the existingbenchmark sets from the literature do not include expectedverdicts, we simply collected all the benchmarks sets wecould ﬁnd. We use the combined problem instances as thebasis for a new benchmark set after removing duplicatesto reduce potential bias and optimize resource consumptionduring experiments. In order to gain at least some conﬁdencein the adequacy of the new benchmark set, we analyze itscomposition and diversity. Finally, we compute and validateexpected verdicts for all instances.
A. Collection and Pre-Processing of Instances
We use the following benchmark suites from the literature.
PyEx
12[10],Pisa [46],Norn13[40],Trau Light14[11],
Leetcode Strings15,IBM Appscan16,Sloth [39],
Woorpje17[47],Kaluza18[48],StringFuzz19[17],
Z3str320,Cashew [49], [50], Joaco [51], [50],
Stranger [3],Kausler [52], [51], [50], and
BanditFuzz21[18]. We extend this set with SMT problems
obtained by running JD ART [53], a J AVA program analysis
tool, with a modiﬁcation that exports SMT veriﬁcation tasks
on the J AVA programs of SV-COMP 2021 [54] and refer to
the corresponding subset of instances as SVCOMP.
Duplicate Removal. Duplicates of the same task in a bench-
mark inﬂate the task set artiﬁcially without any contributionto the explanatory power of the benchmark set regardingtool performance and, even worse, may introduce bias intoconclusions that can be drawn.
We use a form of structural identity for removing duplicates:
for a set xof variables, let F
xdenote the set of syntacti-
cally correct benchmark instances over variables from x. Let
ϕ∈Fxbe a benchmark instance. We can rename variables
inϕand write ϕ[x/y][y/z ]for the instance that is obtained
by replacing all occurrences of variable ywith variable xand
all occurrences of variable zwith variable ysimultaneously
inϕ. A renaming then is a mapping π:x/mapsto→xand we write
π(ϕ)to denote the application of πtoϕ, i.e., the instance
ϕ[π(x1)/x2][π(x2)/x2][...]forx1,x2,...∈x.
Deﬁnition 1 (Identity up-to renaming). Two benchmark in-
stancesϕ,ψ∈Fxareidentical up-to renaming iff there exists
a bijective renaming π:x/mapsto→xfor which ϕ=π(ψ).
Identity up-to renaming is an equivalence relation on Fx.
Expressions (assert (= x "abc")) and(assert (=
12Taken from: https://cvc4.github.io/papers/cav2017-strings
13Taken form: http://user.it.uu.se/ ∼jarst116/norn/
14Taken from: https://z3str4.github.io/# trau light
15Taken from: https://z3str4.github.io/# leetcode strings
16Taken from: https://z3str4.github.io/# ibm appscan
17Taken from: https://z3str4.github.io/# woorpje word equations
18Taken from: https://z3string.github.io/benchmarks
19http://stringfuzz.dmitryblotsky.com/problems/
20Taken from: https://z3str4.github.io/# z3str3 regression
21https://github.com/j29scott/BanditFuzz Public
182Kaluza Cashew Stranger Sloth Norn StringFuzz Joaco Appscan SVCOMP Z3str3 BanditFuzz PyEx Leetcode Pisa Kausler WWE Trau Light
Size 47 284 394 4 40 1 027 1 065 94 8 198 243 357 8 414 2 666 12 120 809 100
Unique 2551 393 4 35 1018 913 71 8 154 238 357 8334 2642 12 120 786 100
General
= 60 701 421 7 1 44 34 050 1 506 10 23 194 − 655 513 171 640 26 286 520 − −
not 28 287 1 171 7 9 968 40 97 19 96 58 207 1 124 039 240 688 13 − − −
Type Cast − − −− − − − − 364 − − − −− − − −
ITE 3 521 − −− − − − 8 23 1 − 585 571 133 919 16 − − −
=> − − −− − − − 4− − − − −− − − −
or 3 − 8 5 − − 39 3− 9 − − − 5 489 − −
and 35 128 − −− 478 − − 4 319 1 − 3 777 965 166 377 15 − − −
exists − − −− 44 − − − − − − − −− − − −
assert 104 447 5 276 80 86 5 399 94 151 1 410 54 550 641 1 985 8 334 2 646 51 143 141 18 167 300
String
= 68 298 3 670 77 56 − 58 551 1 153 41 43 298 − 182 964 36 407 50 143 392 17 396 300
++ 42 239 5 350 52 18 2 832 33 409 785 14 11 278 36 136 868 7 803 13 23 746 34 772 1 200
to_re 81 800 1 108 312 30 15 234 44 084 1 964 76 3 79 1 − −− − − −
in_re 9 597 905 4 33 4 882 1 465 68 8 1 56 242 − −− − − −
at − − −− − 35 790 − − 23 7 843 151 122 67 454 − − − −
substr − − −− − − − 4 4 9 749 3 287 647 24 533 10 1 232 − −
prefixof − − −− − − − 4 4 6 200 4 611 −− − − −
suffixof − − −− − − − 9− 5 212 − −− − − −
contains − − −− − − − − 1 71 268 327 387 914 27 − − −
indexof − − −− − − − 8− 26 1 181 3 301 028 26 804 7 − − −
replace − − − 11 − − − − − 16 790 136 868 3 10 − − −
replace_all − − − 7 − − − − − − − − −− − − −
to_int − − −− − − 5− − − 24 − −− − − −
from_int − − −− − − 91 − − − − − −− − − −
lower − − −− − − − − 1− − − −− − − −
upper − − −− − − − − 1− − − −− − − −
Regular Expressions
none − − −− 95 − − − − − − − −− − − −
allchar − − − 6 − − 150 − 1− 275 − −− − − −
++ 72 203 195 176 8 6 337 22 655 1 464 8 9 18 18 − −− − − −
union − 8 156 − 6 029 19 964 665 60 1 8 − − −− − − −
inter − − −− − − − − − − 16 − −− − − −
* − − 20 19 7 025 20 001 263 8 1 72 82 − −− − − −
+ − 16 8 5 − 20 110 151 − − 1 117 − −− − − −
range − − 24 5 1 919 − 83 − 7 3 − − −− − − −
Numeric
str.len 15 987 701 − 1 1 109 34 135 12 7 667 128 1 022 3 342 014 118 099 8 238 771 −
neg 9 071 − −− 372 − 19 − 24 − − 17 428 251 − − − −
/ 9 − −− − − − − − − − − −− − − −
* − − −− 144 − − − − − − − 2− − 771 −
>= 2 896 276 −− − − 6− 82 6 207 3 201 201 39 096 − − 384 −
> − − − 1 − − 1 9− 18 300 115 11 602 − − − −
< 3 042 − −− − − 1− 260 9 288 79 382 − − − −
<= 1 868 4−− 951 85 11 − 324 3 208 − 42 901 − − 387 −
- 1 692 − −− − − 11 9 39 1 − 5 152 302 122 609 15 238 − −
+ 4 703 − −− 473 − 2 5− 5 − 1 868 006 25 562 − − − −
mod − − −− − − − − − − − − 15− − − −
TABLE I: No benchmark suite contained operations str.<, str.<=, re.all, str.replace_re,
str.replace_re_all, re.comp, re.diff, re.ˆ,re.loop, str.is_digit, str.to_code, str.from_code,
re.opt
y "abc")), e.g., are identical up-to renaming since we can
renamextoyand vice versa.
We have removed all duplicate instances based on identity
up-to renaming from the original benchmark sets (reducingthem from a total of 62 835 to 17 737 tasks). Detailed resultsare reported in the ﬁrst two rows of Table I (Size and Unique).Four benchmark suites shrunk signiﬁcantly in size (more than10 % reduction): Kaluza (reduced by 95 %),Joaco (reduced
by24 %), and StringFuzz (reduced by 14 %), and SVCOMP
(reduced by 22 %). For the Kaluza set, we found more
than 2000 identical instances in one extreme case. A similar
observation has been made by Brennan et al. in their workon string normalization [49]. They report that the subset ofthe Kaluza benchmark used for their research on constraintcaching, contains many binary duplicates, i.e., identical ﬁles.
After removing duplicates, the benchmark sets still con-
tains many symmetries, e.g., instances (assert (= x
"abc")) and(assert (= "abc" x)) or(assert
(< x 5)) and(assert (> 5 x)). We did not attempt
to reduce symmetry since structure may have an inﬂuence onthe performance of search heuristics in solvers.
Character Encoding. One problem encountered in the ex-
isting benchmarks are inconsistencies in the Unicode encod-
ing. The Unicode theory requires, e.g., using \u{09} for
encoding the tab character, while some solvers also acceptthe escaping \t, and Z3 traditionally used a hex encoding
for characters (\x09 for tab). In order to avoid inconsistent
results as a consequence from these differences, we prepare a
183UNSAT
0 1 2 3 4 5 6 7SAT018 7 78 78 186 359 448 1235
3+0 0+3 0+7 0+37 0+116 0+352 0+446 0+1235
1228 22 101 125 871 1017 195
131+0 1+0 0+6 0+107 0+864 0+1017 0+194
2558 2 1997 1
447+0 2+0 0+1997 0+1
3947 20 1
934+0 17+1 0+1
44125 6
4076+0 6+0
51311 27
1305+0 27+0
61307
1305+0
72467
2467+0
Fig. 2: The ﬁgure shows verdict combinations for seven solvers
on all benchmark instances. Each cell in row iand column
jhasisatisﬁable verdicts and junsatisﬁable verdicts. The
heatmap (yellow) highlights clustering of instances. Every cellshows number of contained instances (bold) as well as numberof conﬁrmed models + checked unsatisﬁable cores. Green andred coloring indicates the voting-based expected verdict (satand unsat) in cases where verdicts could not be validated.
corresponding variant of benchmark instances and used these
where appropriate without further mention.
B. Features in Benchmark Suites
We analyze the composition and diversity in the combined
benchmark set by counting occurrences of relevant operations
in instances from every source. Table I lists the used operatorsorganized into four groups: general SMT, string theory, regularexpressions, and numeric. We arranged the benchmarks intogroups as well, focusing on string concatenation with regularexpression membership queries on the left to pure wordequation benchmarks on the right.
We observe that the group between Appscan and
Kausler use string operations str.prefixof,
str.suffixof, and str.contains, which from
our experience occur frequently in program security analysis.Benchmark sets right of and including PyEx do not use any
regular expressions. None of the benchmarks contain thestring order operations str.< orstr.<=. Only SVCOMP
has instances that use str.lower andstr.upper, which
are not yet included in the ofﬁcial SMT-Lib v2.6 standardbut are already supported by some solvers.
The benchmark sets contain relatively few instances that
combine numeric values and strings: the functions for con-version between code points and strings (str.to_codeandstr.from_code) do not appear in any benchmark
suite andJoaco is the only benchmark that contains the
str.from_int andstr.to_int operations. Since pars-
ing Unicode byte patterns into string characters and numericvalues from strings occurs frequently in Web-applications, weconjecture that the set of presented benchmarks is not adequatefor predicting solver performance for analyses in this domain.C. Generating Expected V erdicts.
While expected verdicts are essential as ground truth when
comparing the performance of decision procedures, verdictscan only be found for the Cashew, Joaco,Kaluza,
Kausler, Stranger, and Z3str3 benchmark sets in the
literature. We compute expected verdicts for the combinedbenchmark set in three steps: First, we compute the verdictsof seven SMT solvers (ABC, CVC4, O
STRICH ,PRINCESS ,
SEQ, Z3 STR3, Z3 STR4) for every problem. As CVC4 and
SEQ return most deﬁnitive verdicts (cf. Section VI), we try
to validate the verdicts of these solvers in a second step:satisﬁable verdicts from either solver are validated by eval-
uating corresponding models on problem instances using the
JC
ONSTRAINTS library. Unsatisﬁable verdicts from CVC4
are validated by checking the corresponding unsatisﬁable corewith SEQ. Validated verdicts are used as expected verdicts.For satisﬁable verdicts, a model with conﬁrming evaluation ofthe model in the context of the problem is a proof of theverdict. For unsatisﬁable verdicts a conﬁrmed unsatisﬁablecore is at least an argument that two solvers agree on theunsatisﬁable part of the problem. For the cases in whichvalidation was not successful, we determine the (likely correct)expected verdict by majority vote. We have not observed aconﬁrmed satisﬁable model and a conﬁrmed unsatisﬁable coreon the same problem. If it happens, the conﬁrmed satisﬁablemodel should deﬁne the expected verdict. The test of asatisﬁable model is easier from a theoretical point of viewthan the proof of unsatisﬁability and we have implementedthe conformance test independent of a solver. In any case,a manual root cause analysis on the instance to identify thereasons for the disagreeing verdicts is recommended.
Results are summarized in Figure 2. Cells group problem
instances according to the respective number of obtainedsatisﬁable and unsatisﬁable votes. Cells show the number
of corresponding problems (bold) and the number of val-idated models and validated unsatisﬁable cores (m+c). Wehighlighted accumulations of verdicts with increase in theintensity of yellow as background color in the cell. Thebiggest cluster containing 4 125 instances got 4 satisﬁablevotes and 0 unsatisﬁable votes. In the data, a cluster with 1 997instances received 2 satisiﬁable votes and 3 unsatisﬁable votescatching attention. The other larger cluster manifested withclear satisﬁable or unsatisﬁable votes. Overall, we can providevalidated verdicts for 97.48% of all benchmark problems using
cross-checking for unsastisﬁable cores and model tests forprovided models with JC
ONSTRAINTS . We cannot provide
expected verdicts for the 36instances on which validation
failed and voting led to a tie (cells 0/0 and 1/0). Pleasenote that validated models reported in these cells are not anerror but were obtained by using SEQ in a mode that tracksunsatisﬁable cores and performs differently from the defaultconﬁguration of SEQ used for voting.
V. D
ATA-DRIVEN DESIGN OF META-SOLVING STRATEGIES
Using the benchmark suite described in the previous section
and the performance data of the seven individual SMT solvers
184ABCCVC4
OSTRICHPRINCESSSEQ
Z3STR3Z3STR404,0008,00012,000# sat. instances
ABCCVC4
OSTRICHPRINCESSSEQ
Z3STR3Z3STR402,0004,0006,000# unsat. instancesUNSAT
SAT
UNKNOWN
ERROR
OUT OF MEMORY
TIMEOUT
Fig. 3: Verdicts and inconclusive results of SMT solvers ABC, CVC4, O STRICH ,PRINCESS , SEQ, Z3 STR3, and Z3 STR4o n
the benchmark suite presented in Section IV for satisﬁable instances (left) and unsatisﬁable instances (right).
TABLE II: Performance of individual SMT solvers and meta-solving strategies: (likely) correct verdicts, (likely) incorrect
verdicts, unknowns, errors, timeouts, and CPU time (estimated for simulated meta-solving strategies). V OTE is omitted as it is
used as basis for computing expected verdicts. The table contains 17 701 tasks for wich we could establish a ground truth ofthe 17 737 tasks in the benchmark.
EARLIESTEARLIESTTRUSTED
CVCSEQCORES
CVCSEQEVA L
ABC CVC4 SEQ Z3STR3
Z3STR4
PRINCESSOSTRICH
correct 17 255 17 449 16 953 17 290 12 462 17 020 16 117 11 629 14 182 5 730 6 016
unknown 3 247 37 0 0 0 1 163 14 10 814 426
error 0 0 62 2 851 260 76 137 583 1 11 239
timeout 0 0 649 407 53 421 1 505 5 759 920 1 156 6
incorrect 443 5 0 2 4 335 0 2 13 2 002 0 14
CPU time (s) 173 058 s 819 873 s 259 390 s 182 625 s 736 680 s 178 688 s 574 027 s 1 749 373 s 395 750 s 430 048 s 153 314 s
ABC, CVC4, O STRICH ,P RINCESS , SEQ, Z3 STR3, and
Z3STR4, on this benchmark, we design ﬁve meta-solving
strategies. Let us start by analyzing the performance databefore discussing the resulting design decisions.
A. Performance Data from Benchmarks
We focus primarily on analyzing correct verdicts in this
section and will analyze resource consumption in Section VI.
Figure 3 summarizes the performance of the individual solvers,split into satisﬁable instances (left) and unsatisﬁable instances(right). We can make two observations:
1) ABC and Z3
STR4 produce many incorrect verdicts
(compared to our expected verdicts) for unsatisﬁablebenchmark instances.
22We perform an online trans-
formation of character encodings, as detailed in theprevious section. Moreover, the ground truth labels arevalidated but not proven. Both may contribute to theobserved behavior. For the data-driven design of meta-solving strategies, we simply observe that these two
22For Z3 STR4, we checked the incorrectly satisﬁable instances through
Z3STR4’s Java API, which does yield the expected unsatisifable answers. We
assume a bug in the SMT-Lib frontend of Z3 STR4 but were not able to locate
it. For ABC, it was not possible to analyze returned models to ﬁnd a root
cause as ABC’s API for accessing models is currently not compatible withthe model validator in JC
ONSTRAINTS . These types of bugs are to expected
in research tools and will hopefully be ﬁxed soon and must not be taken as
an indicator of bad solver performance!solvers frequently produce verdicts that deviate from the
majority of solvers on instances deemed unsatisﬁable,i.e., we will be skeptical about their satisﬁable verdicts.
2) P
RINCESS and O STRICH solve signiﬁcantly fewer in-
stances than the other solvers. We will not considerthese solvers when optimizing for resource consumptionand deﬁnitive verdicts without analyzing features ofbenchmark instances.
B. Designing Meta-Solving Strategies
For the remainder of this paper, we focus on ﬁve meta-
solving strategies (V
OTE,E ARLIEST ,E ARLIEST TRUSTED ,
CVCSEQEVA L, and C VCSEQCORES ), implementing three in-
tegration patterns (majority vote, earliest verdict , and verdict-
based second attempt ). While we do not present a meta-
solving strategy based on the input-based decision pattern, we
provide some results on training input-based predictors forsolver performance in Section VI.
V
OTE .The V OTE strategy instantiates the majority vote pat-
tern, executing all seven individual solvers in parallel. Allobtained results are aggregated into a ﬁnal verdict, ignoringtimeouts, errors, and out of memory failures. We have not actu-ally implemented this design and instead compute its expectedperformance (i.e., correct verdicts) from the performance dataof the individual solvers. The meaningfulness of the results
185obtained with this strategy is limited within the scope of this
paper since it was used to compute expected verdicts on thebenchmark suite.
E
ARLIEST .The E ARLIEST strategy instantiates the earliest
verdict pattern in a straightforward manner: all solvers are run
in parallel and the earliest satisﬁable orunsatisﬁable verdict
is returned immediately. Failing solvers or unknown verdicts
are ignored. As for V OTE, we have not actually implemented
this strategy and only compute its expected performance.
EARLIEST TRUSTED .This strategy reﬁnes the E ARLIEST
strategy, based on the ﬁrst observation reported in the previoussubsection: an earliest unsatisﬁable verdict is returned immedi-
ately. Any earliest satisﬁable verdict is disregarded from ABC
and Z3
STR4 and only used if it is reported by one of the other
solvers. We have not implemented this strategy and computeits expected performance.
C
VCSEQEVA L .The C VCSEQEVA L strategy instantiates the
verdict-based second attempt or validation pattern and is
based on two observations. First, P RINCESS and O STRICH
solve signiﬁcantly fewer benchmark instances than the othersolvers (cf. above). Second, CVC4 and SEQ use dif ferent
theories internally and we expect complementing performanceproﬁles. Hence, C
VCSEQEVA L starts by calling CVC4 with a
timeout of one minute. Then, if CVC4 returns an unsatisﬁable
verdict, it simply returns the verdict. If CVC4 returns a
satisﬁable verdict (and a model), C VCSEQEVA L validates the
model by evaluating it on the problem in question, using the
JCONSTRAINTS library (cf. Section II). In case the model can
be validated, the satisﬁable verdict is returned. In all other
cases, C VCSEQEVA L invokes SEQ for a ﬁnal verdict. As
for CVC4, models are checked for satisﬁable verdicts and,
in case the model cannot be validated, reported as unknown.
We expect that C VCSEQEVA L computes deﬁnitive verdicts for
most benchmark instances.
CVCSEQCORES .The C VCSEQCORES strategy reﬁnes the
CVCSEQEVA L strategy in cases were CVC4 returns an un-
satisﬁable verdict: in these cases, C VCSEQCORES tries to
validate the verdict by checking the returned unsatisﬁable corewith SEQ. As for unvalidated satisﬁable verdicts, unconﬁrmed
unsatisﬁable cores are reported as unknown. We expect that
C
VCSEQCORES produces only very few incorrect verdicts at
the price of computing slightly fewer deﬁnitive verdicts than
CVCSEQEVA L since both solvers have to agree on unsatis-
ﬁable verdicts. As the unsatisﬁable core might be a smaller
problem than the original task, we expect that C VCSEQCORES
solves more tasks than a solver alone as the unsat corevalidation is less expensive.
While we only consider individual patterns in this work, it
would be easy to combine multiple patterns into complexermeta-solving strategies, e.g., for validating earliest verdicts ina second attempt, balancing response time and accuracy. 1 5 30 90 180 300
 0  2000  4000  6000  8000  10000  12000  14000  16000  18000Response time (s) per problem
# Solved ProblemsABC
CVC4
CvcSeqCores
CvcSeqEvalEarliest
EarliestTrusted
SEQ
Z3STR3Z3STR4
OSTRICH
PRINCESS
Fig. 4: Number of benchmark instances solved with satisﬁableor unsatisﬁable answer by individual solvers and meta-solvingstrategies sorted by increasing response time. E
ARLIEST and
EARLIEST TRUSTED are predictions, other times are measured.
VI. E V ALUATION
We conduct a series of experiments in order to evaluate the
effectiveness of our data-driven approach to designing meta-solving strategies by addressing the following three concreteresearch questions.
RQ1. Do meta-solving strategies beat individual solvers interms of response time, correctness, and cost?
RQ2. What are the observable trade-offs between responsetime, correctness, and cost?
RQ3. Can we reliably predict if a solver will return adeﬁnitive and correct verdict based on features ofa problem instance?
We have implemented the C
VCSEQEVA L and C VCSEQCORES
strategies on top of the JC ONSTRAINTS solver abstraction
layer in J AVA and have used these implementations to ob-
tain data on performance and resource consumption on thebenchmark suite. For the three meta-solving strategies that wedid not implement, we estimate resource consumption basedon the data recorded in the experiments with the individualsolvers and the instantiated integration pattern.
All results were obtained using the BenchExec [55] frame-
work for executing the seven individual solvers (ABC, CVC4,
O
STRICH ,PRINCESS , SEQ, Z3 STR3, and Z3 STR4) as well
as two meta-solving strategies (C VCSEQEVA L and C VCSEQ-
CORES ) on the benchmark suite presented in Section IV. We
used an Intel i9-7920X CPU (24 vCores) with 128 GiB RAMrunning Ubuntu 20.04 LTS. Each run (i.e., one solver on onebenchmark problem) was provisioned with 2.5 GB RAM, 1vCore, and a 5 minute timeout in the BenchExec conﬁguration.
Table II reports the number of correct and incorrect verdicts,
other results, and accumulated CPU time for the seven indi-
186vidual constraint solvers and for four meta-solving strategies
(analyzing V OTE would not be meaningful since it was used
to compute expected verdicts). Figure 4 plots response time(needed time to solve an instance) against correctly solved in-stances (sorted by increasing response time), showing detailedresource proﬁles in terms of wall time consumption.
A. Effectiveness of Meta-Solving (RQ1)
As can be seen in Table II, the meta-solving strate-
gies outperform the individual constraint solvers with re-
spect to the number of correct verdicts, exceeding thebest individual solver ( CVC4) by 429 correct verdicts or
2.4%(E
ARLIEST TRUSTED ). Only the C VCSEQCORES meta-
solving strategy produces 67fewer correct verdicts as CVC4.
The results are more mixed for the number of incorrect
results: E ARLIEST inherits a very high number of incorrect
results from ABC and Z3 STR4 and while the E ARLIEST -
TRUSTED strategy was explicitly designed to disregard poten-
tially incorrect verdicts from these two solvers, it still producesa greater number of incorrect verdicts than CVC4, SEQ,
and P
RINCESS . The more conservative C VCSEQEVA L strategy
also suffers from 2incorrect verdicts — which is comparable
to SEQ solver and only slightly worse than CVC4. Only the
CVCSEQEVA L strategy produces no incorrect verdicts and is
on par with CVC4 and the P RINCESS solver in this respect.
With respect to accumulated CPU time, E ARLIEST is esti-
mated to be almost as cheap as the cheapest individual solver;
EARLIEST TRUSTED , on the other hand, is only better than
Z3STR3 and more expensive than most individual solvers.
As this strategy require seven cores to run, the paid CPUtime masks the fast response time. Figure 4 shows that
E
ARLIEST answers close to 16k problems in less than one
second response time per problem. E ARLIEST TRUSTED still
answers over 15k problems in less than one second response
time. The best single solver answers around 14k of the
problems in less than one second. The C VCSEQEVA L and
CVCSEQCORES meta-solving strategies are almost on par with
the faster constraint solvers, where checking of unsatisﬁablecores increases resource consumption by 44%. In terms of
response times, we observe for these two strategies a constantoverhead compared to the solving times of CVC4 and SEQ
that vanishes if the response time passes two seconds. Thisoverhead originates from the startup time of the JVM as thisstrategy is implemented as part of JC
ONSTRAINTS in J AVA
while booth solvers are written in C++. The JVM startup isincluded in the in measured time presented in the ﬁgure. Thesame overhead is included for O
STRICH and P RINCESS .
B. Trade-Offs (RQ2)
Comparing the proﬁles of the meta-solving strategies, we
disregard the E ARLIEST meta-solving strategy that seems to
be problematic on the concrete benchmark suite and for theconcrete selection of solvers as it is strongly affected by incor-rect verdicts. The E
ARLIEST TRUSTED strategy pays for a very
competitive response time with a high resource consumption.This may be an interesting performance proﬁle in scenarioswere horizontal scaling is cheap if the strategy can be furtherreﬁned to exclude likely incorrect verdicts. The C
VCSEQEVA L
strategy balances correct verdicts, resource consumption, andresponse time very well. The C
VCSEQCORES strategy pays
for zero incorrect verdicts with a 44% increase in resource
consumption (compared to C VCSEQEVA L). The increase in
response time seems to be negligible.
C. Feature-based Solver Selection (RQ3)
While we do not present an instance of the input-based
decision integration pattern in this paper, we want to evaluate
if the obtained data could help designing such a meta-solving
strategy, especially since other works have presented input-based decision strategies. To this end, we have trained randomforest classiﬁers to predict whether a solver can solve a bench-mark instance correctly based on the number of occurrencesfor each SMT primitive in the instance. Training of classiﬁerswas implemented using Python’s sklearn library and the
implementation is available as part of the provided artifact.
Table III shows the achieved precision, recall, and F
1
score, as well as the most inﬂuential features for individualsolvers. Reported numbers are averages (std. deviations) froma ﬁve-fold cross-validation with a randomized 50/50 split into
training set and test set. For individual runs, precision, recall,andF
1score are computed as weighted averages of the metrics
for each label to account for the imbalance in the size ofthe different classes (some solvers can solve many benchmarkinstances correctly). As can be seen in the table, the trainedclassiﬁers achieve very high values for precision, recall, andF
1score.
When using input-based decisions with the goal of reducing
resource consumption, precision is the most relevant metric,as it expresses the percentage of correct “solver can solveinstance” predictions. The very high values for the precisionof the trained models seems encouraging but has, of course,been achieved with strongly biased training and test sets for thesolvers that solve many benchmark instances correctly, whichraises the suspicion that simply always predicting that a solvercan solve an instance would already yield a high precision.
A more detailed report of the classiﬁcation results (not
included in the paper but can easily obtained from the dataand scripts in the artifact) shows, however, that the trainedclassiﬁers do not fall in this trap: even for the solvers thatcan solve many instances, the trained classiﬁers achieve highprecision and reasonable recall for the smaller (i.e., cannotsolve) class. For this class individually, recall is the moreinteresting metric as it indicates how frequently the classiﬁerwould be able to prevent us from using a solver that cannotsolve an instance successfully. Precision ranges from 75%
to88% and recall ranges from 50% to75% in these cases,
showing that even though the training set is very unbalanced,classiﬁers identify features that predict performance. For thesolvers with reasonably balanced training sets, precision andrecall are high for both classes.
The identiﬁed important variables provide additional expla-
nation and validation of the achieved precision. As an example,
187TABLE III: Prediction of solver behavior (deﬁnitive correct verdict or not) based on features of problem instances; features
count contained SMT operations (occurrences per type of operation): Precision, Recall, F1Score, and ﬁve most important
features for seven individual solvers.
Solver Precision Recall F1Score Important Variables
ABC 0.99 (0.00) 0.99 (0.00) 0.99 (0.00) str_eq 0.09 (0.01) str_to_re 0.09 (0.01) emptystr 0.07 (0.01) re_concat 0.07 (0.01) str_concat 0.07 (0.00)
CVC4 0.98 (0.00) 0.99 (0.00) 0.99 (0.00) cast 0.10 (0.01) emptystr 0.09 (0.01) str_to_re 0.05 (0.00) and_op 0.05 (0.01) re_union 0.05 (0.01)
OSTRICH 0.94 (0.00) 0.92 (0.01) 0.93 (0.00) indexof 0.09 (0.02) contains 0.09 (0.02) str_concat 0.08 (0.01) not_op 0.07 (0.00) and_op 0.07 (0.01)
PRINCESS 0.90 (0.01) 0.91 (0.01) 0.90 (0.00) str_eq 0.09 (0.01) contains 0.08 (0.00) indexof 0.07 (0.01) not_op 0.06 (0.00) uminus 0.05 (0.01)
SEQ 0.96 (0.00) 0.98 (0.00) 0.97 (0.00) len 0.09 (0.00) str_concat 0.08 (0.00) str_eq 0.07 (0.01) equals 0.07 (0.00) str_to_re 0.06 (0.00)
Z3STR3 0.95 (0.00) 0.97 (0.00) 0.96 (0.00) plus 0.10 (0.01) indexof 0.08 (0.01) str_eq 0.07 (0.00) contains 0.07 (0.01) not_op 0.06 (0.00)
Z3STR4 0.97 (0.00) 0.98 (0.00) 0.97 (0.00) replace 0.11 (0.01) str_concat 0.11 (0.00) plus 0.05 (0.00) len 0.05 (0.00) substr 0.05 (0.01)
for CVC4, the cast operator is identiﬁed as an important
variable in the experiments (cf. in Table III). This matchesour observation that CVC4 is good in casting between data
types. For Z3
STR3, theplus andindexof operators are
important for the decision as Z3 STR3 does not support the
combination of integer theory and string theory.
These observations show that the the learned classiﬁers (to
some degree) use features that relate benchmark instances andsolver capabilities. This suggests that learned classiﬁers couldbe used for instantiating the input-based decision and that a
proper evaluation of the achievable performance is worthwhileof further investigation. Finally, the information on importantfeatures could alternatively be used for the data-driven manualdesign of input-based decisions.
VII. D
ISCUSSION AND THREATS TO VALIDITY
We can now summarize our observations, draw some initial
conclusions about the application of meta-solving strategies,and discuss some threats to the validity of the obtained results.
Concept Validity. The different performance proﬁles of the
analyzed meta-solving strategies (w.r.t. accuracy, resource con-
sumption, and response time) demonstrate that it is possibleto optimize for different application scenarios. We can easilyderive ideas for more involved combinations of solvers, basedon the four patterns: If horizontal scaling is cheap as sufﬁ-ciently many CPU cores are available, taking the ﬁrst returningsolver with ﬁlter conditions seems the best strategy and mightlead to a further improvement over E
ARLIEST TRUSTED .I f
less CPU cores are available, adding more solvers to improvethe checking of unsatisﬁable cores of C
VCSEQCORES (so that
the results get closer to the ones obtained with C VCSEQEVA L)
may be the right strategy. Currently, the timeouts for checkingunsatisﬁable cores are a limit for this strategy.
Moreover, we were able to draw conclusions from per-
formance data of individual solvers that guided our designdecisions. Since we did not actually implement the strategieswe simulated and did not simulate the strategies we imple-mented, we did not actually demonstrate that our analysis ofthe response time and resource consumption of meta-solvingstrategies is accurate. Especially for the large group of bench-mark instances that can be solved in fractions of a second,forking of processes and inter-process communication maylead to slower response times and higher resource consumptionthan estimated, especially if many solvers are used in parallel.
Threats to Internal Validity. We identify two threats to
internal validity of results obtained for the analyzed meta-solving strategies: seeding of SMT solvers may impact results
and expected verdicts may be wrong.
SMT Solvers use heuristics that are traditionally seeded.
Z3 and CVC4 tend to be seeded with ﬁx seeds unless they
are altered by passing seeds explicitly. Therefore, we havenot observed varying results originating from seeding in ourexperiments. Nevertheless, running the same solver with dif-ferent seeds might impact the performance. As most problemsin the combined benchmark set have only few variables andthe run times for many problems are milliseconds, we do notexpect this to have a signiﬁcant impact in obtained results. Wedo not expect different random choices in these small sets ofvariables to change this.
Naturally, majority votes do not guarantee correctness and
may lead to spurious results. One could argue that majorityvotes should hence not be used for the computations of groundtruth labels. Following such an argument would not changethe reported results in a signiﬁcant way: In the presented data,validated result are computed for 97.5% of all benchmark
instances. Majority votes are only used for the remaining2.5%of instances and among these a signiﬁcant fraction are
unanimous. On the other hand, disagreement of solvers mayprovide guidance towards difﬁcult features and bugs. Finally,even validated verdicts can theoretically be spurious but wedeem this highly unlikely as models and unsatisﬁable coreswere conﬁrmed with independently developed tools.
We want to emphasize the importance of cross checking
SMT solver results and the method for the evaluation of metasolving strategies rather than establishing a single strategy. Theexperiments need to be repeated periodically to be valuable inthe long run. A consequence is that our ground truth labelsmay be wrong if a bug in a solver leads to consistently votingfor the wrong answers. Due to the model and unsatisﬁablecore validation applied along with the majority vote, we donot expect this to be the case in a signiﬁcant amount of casesin this data set.
Threats to External Validity Results obtained in this study
on the performance of individual meta-solving strategies may
not generalize well for several reasons.
First, we evaluated the multi-solver strategies and solver
only on Unicode theory benchmarks, which is only a subset ofthe whole SMT world. Then, the study does not consider cloudsettings where horizontal scaling is achieved by splitting workacross the network layer. We do not expect the Earliest V erdict
based strategies to perform comparably in such a setting dueto the short run times in most cases. Third, the string theory
188solver ﬁeld advances quickly and the measurements in this
paper will be outdated in the future. We report on a snapshotview on the state of implementations. Finally, as shown inTable I, the benchmarks have slightly different proﬁles but arestill quite homogeneous and skip certain parts (in terms ofoperators) of the SMT-Lib standard. We tried to control forthis by using all benchmarks we could ﬁnd in the literature.Still, the observations made on the used benchmarks set maynot extrapolate well to the complete SMT-Lib Unicode stringtheory.
Summarizing, we are conﬁdent that data-driven design of
meta-solving strategies is possible and beneﬁcial. Concreteresults depend on the benchmarks that are used for analysisand tuning as well as on the distribution of capabilities insolvers.
VIII. C
ONCLUSION
In this paper, we evaluated different SMT meta-solving
strategies for string theory solvers in terms of performance,accuracy, and costs. To this end, we ﬁrst collected benchmarksets used in the literature to evaluate the performance ofseven SMT solvers on these benchmarks. After collection, weremoved duplicates form the benchmarks reducing the overallsize from 62 835 to 17 737 tasks. These tasks are analyzedin more detail regarding their homogeneity in the benchmarksets. We used the computed performance data to generate anexpected verdict for each task based on either a validatedsatisﬁable model or a conﬁrmed unsatisﬁable core. Otherwise,we use majority voting to deﬁne a expected result label.
The paper presents four integration patterns for SMT meta-
solving strategies. Based on these patterns, we deﬁned fourdifferent meta-solving strategies in addition to the vote strategyused to establish the expected result labels. The evaluationdemonstrates that the earliest verdict strategies are only suit-able in the SMT domain on string problems, if some kindof solver selection is performed upfront. Otherwise, a earliestverdict strategy will return incorrect results compared to theestablished ground truth. Cheaper meta-solving strategies thatcombine in this concrete example only CVC4 and SEQ with
answer validation require less parallelization power of theCPU compared to the fastest solver approach and archivecomparable results. We conclude that for the string theorydomain, meta-solving strategies are well suited for boostingthe capability of the SMT decision layer in an analysis. Theinﬂuence of the SMT solver choice should be mentioned inthe evaluation of an algorithm implementation more often inthe future. Future work should investigate the portability ofthe results to other SMT domains.
R
EFERENCES
[1] L. De Moura and N. Bjørner, “Z3: An efﬁcient SMT solver,” in
International conference on Tools and Algorithms for the Construction
and Analysis of Systems. Springer, 2008, pp. 337–340.
[2] C. Barrett, C. L. Conway, M. Deters, L. Hadarean, D. Jovanovi ´c, T. King,
A. Reynolds, and C. Tinelli, “CVC4,” in Computer Aided V eriﬁcation,
G. Gopalakrishnan and S. Qadeer, Eds. Springer, 2011, pp. 171–177.[3] F. Yu, M. Alkhalaf, and T. Bultan, “Stranger: An automata-based string
analysis tool for PHP,” in Tools and Algorithms for the Construction
and Analysis of Systems, J. Esparza and R. Majumdar, Eds. Berlin,Heidelberg: Springer Berlin Heidelberg, 2010, pp. 154–157.
[4] M.-T. Trinh, D.-H. Chu, and J. Jaffar, “S3: A symbolic string solver for
vulnerability detection in web applications,” in Proceedings of the 2014
ACM SIGSAC Conference on Computer and Communications Security .
ACM, 2014, pp. 1232–1243.
[5] A. S. Christensen, A. Møller, and M. I. Schwartzbach, “Precise analysis
of string expressions,” in Proc. 10th International Static Analysis Sym-
posium (SAS), ser. LNCS, vol. 2694. Springer-Verlag, June 2003, pp.1–18, available from
http://www.brics.dk/JSA/ .
[6] P. Saxena, D. Akhawe, S. Hanna, F. Mao, S. McCamant, and D. Song, “A
symbolic execution framework for javascript,” in 2010 IEEE Symposium
on Security and Privacy, 2010, pp. 513–528.
[7] M. Mues, T. Schallau, and F. Howar, “Jaint: A framework for user-
deﬁned dynamic taint-analyses based on dynamic symbolic executionof Java programs,” in Integrated F ormal Methods, B. Dongol and
E. Troubitsyna, Eds. Cham: Springer International Publishing, 2020,pp. 123–140.
[8] M. Berzish, V . Ganesh, and Y . Zheng, “Z3str3: A string solver with
theory-aware heuristics,” pp. 55–59, 2017.
[9] N. Bjørner, N. Tillmann, and A. V oronkov, “Path feasibility analysis
for string-manipulating programs,” in International Conference on Tools
and Algorithms for the Construction and Analysis of Systems. Springer,2009, pp. 307–321.
[10] A. Reynolds, M. Woo, C. Barrett, D. Brumley, T. Liang, and C. Tinelli,
“Scaling up DPLL(T) string solvers using context-dependent simpliﬁ-cation,” in International Conference on Computer Aided V eriﬁcation.
Springer, 2017, pp. 453–474.
[11] P. A. Abdulla, M. Faouzi Atig, Y . Chen, B. P. Diep, L. Hol ´ık, A. Rezine,
and P. R ¨ummer, “Trau: SMT solver for string constraints,” in 2018
F ormal Methods in Computer Aided Design (FMCAD), Oct 2018, pp.1–5.
[12] T. Chen, M. Hague, A. W. Lin, P. R ¨ummer, and Z. Wu, “Decision
procedures for path feasibility of string-manipulating programs withcomplex operations,” Proceedings of the ACM on Programming Lan-
guages , vol. 3, no. POPL, pp. 1–30, 2019.
[13] T. Bultan, F. Yu, M. Alkhalaf, and A. Aydin, String Analysis for Software
V eriﬁcation and Security. Springer, 2017.
[14] G. Redelinghuys, W. Visser, and J. Geldenhuys, “Symbolic execution of
programs with strings,” in Proceedings of the South African Institute for
Computer Scientists and Information Technologists Conference , 2012,
pp. 139–148.
[15] M. Mues and F. Howar, “JDart: Dynamic symbolic execution for Java
bytecode (competition contribution),” in International Conference on
Tools and Algorithms for the Construction and Analysis of Systems.Springer, 2020, pp. 398–402.
[16] A. Shamakhi, H. Hojjat, and P. R ¨ummer, “Towards string support in
JayHorn (competition contribution),” in International Conference on
Tools and Algorithms for the Construction and Analysis of Systems,J. F. Groote and K. G. Larsen, Eds. Cham: Springer InternationalPublishing, 2021, pp. 443–447.
[17] D. Blotsky, F. Mora, M. Berzish, Y . Zheng, I. Kabir, and V . Ganesh,
“StringFuzz: A fuzzer for string solvers,” in International Conference
on Computer Aided V eriﬁcation. Springer, 2018, pp. 45–51.
[18] J. Scott, F. Mora, and V . Ganesh, “Banditfuzz: A reinforcement-learning
based performance fuzzer for smt solvers,” in Software V eriﬁcation,
M. Christakis, N. Polikarpova, P. S. Duggirala, and P. Schrammel, Eds.Cham: Springer International Publishing, 2020, pp. 68–86.
[19] D. Winterer, C. Zhang, and Z. Su, “On the unusual effectiveness of
type-aware operator mutations for testing SMT solvers,” Proc. ACM
Program. Lang., vol. 4, no. OOPSLA, Nov. 2020. [Online]. Available:https://doi.org/10.1145/3428261
[20] ——, “Validating SMT solvers via semantic fusion,” in Proceedings of
the 41st ACM SIGPLAN Conference on Programming Language Designand Implementation, 2020, pp. 718–730.
[21] F. Besson, P.-E. Cornilleau, and D. Pichardie, “Modular SMT proofs for
fast reﬂexive checking inside Coq,” in Certiﬁed Programs and Proofs,
J.-P. Jouannaud and Z. Shao, Eds. Berlin, Heidelberg: Springer BerlinHeidelberg, 2011, pp. 151–166.
[22] D. Beyer, M. Dangl, D. Dietsch, and M. Heizmann, “Correctness
witnesses: Exchanging veriﬁcation results between veriﬁers,” in Pro-
189ceedings of the 2016 24th ACM SIGSOFT International Symposium on
F oundations of Software Engineering, 2016, pp. 326–337.
[23] D. Beyer, M. Dangl, T. Lemberger, and M. Tautschnig, “Tests from
witnesses,” in International Conference on Tests and Proofs. Springer,
2018, pp. 3–23.
[24] L. Kotthoff, “Algorithm selection for combinatorial search problems: A
survey,” in Data Mining and Constraint Programming. Springer, 2016,
pp. 149–190.
[25] B. Steffen, T. Margaria, and V . Braun, “The electronic tool integration
platform: concepts and design,” International Journal on Software Tools
for Technology Transfer, vol. 1, no. 1-2, pp. 9–30, 1997.
[26] M. Kulczynski, F. Manea, D. Nowotka, and D. B. Poulsen, “The power
of string solving: Simplicity of comparison,” in Proceedings of the
IEEE/ACM 1st International Conference on Automation of Software Test,2020, pp. 85–88.
[27] M. Mues and F. Howar, “Reproduction Package for the ASE
2021 AEC Committee,” Jul. 2021. [Online]. Available: https://doi.org/10.5281/zenodo.5226127
[28] R. H ¨ahnle and M. Huisman, Deductive Software V eriﬁcation: From Pen-
and-Paper Proofs to Industrial Tools. Cham: Springer InternationalPublishing, 2019, pp. 345–373.
[29] F. Damiani, R. H ¨ahnle, and M. Lienhardt, “Abstraction reﬁnement for
the analysis of software product lines,” in Tests and Proofs, S. Gabmeyer
and E. B. Johnsen, Eds. Cham: Springer International Publishing, 2017,pp. 3–20.
[30] C. M. Wintersteiger, Y . Hamadi, and L. De Moura, “A concurrent
portfolio approach to SMT solving,” in International Conference on
Computer Aided V eriﬁcation. Springer, 2009, pp. 715–720.
[31] E. G. Karpenkov, K. Friedberger, and D. Beyer, “JavaSMT: A uniﬁed
interface for SMT solvers in Java,” in Working Conference on V eriﬁed
Software: Theories, Tools, and Experiments. Springer, 2016, pp. 139–148.
[32] F. Howar, F. Jabbour, and M. Mues, “JConstraints: a library for working
with logic expressions in Java,” in Models, Mindsets, Meta: The What,
the How, and the Why Not? Springer, 2019, pp. 310–325.
[33] D. R. Cok, “jsmtlib: Tutorial, validation and adapter tools for smt-libv2,”
inNASA F ormal Methods Symposium. Springer, 2011, pp. 480–486.
[34] M. Gario and A. Micheli, “PySMT: a solver-agnostic library for fast
prototyping of SMT-based algorithms,” in SMT workshop, vol. 2015,
2015.
[35] H. Riener, F. Haedicke, S. Frehse, M. Soeken, D. Große, R. Drechsler,
and G. Fey, “metaSMT: focus on your application and not on solverintegration,” International Journal on Software Tools for Technology
Transfer, vol. 19, no. 5, pp. 605–621, 2017.
[36] J. Scott, A. Niemetz, M. Preiner, S. Nejati, and V . Ganesh, “MachSMT:
A machine learning-based algorithm selector for SMT solvers,” in Tools
and Algorithms for the Construction and Analysis of Systems,J .F .Groote and K. G. Larsen, Eds. Cham: Springer International Publishing,2021, pp. 303–325.
[37] H. Palikareva and C. Cadar, “Multi-solver support in symbolic execu-
tion,” in Computer Aided V eriﬁcation, N. Sharygina and H. Veith, Eds.
Berlin, Heidelberg: Springer Berlin Heidelberg, 2013, pp. 53–68.
[38] A. Aydin, L. Bang, and T. Bultan, “Automata-based model counting
for string constraints,” in International Conference on Computer Aided
V eriﬁcation. Springer, 2015, pp. 255–272.
[39] L. Hol ´ık, P. Jank ˚u, A. W. Lin, P. R ¨ummer, and T. V ojnar, “String
constraints with concatenation and transducers solved efﬁciently,” Proc.
ACM Program. Lang., vol. 2, no. POPL, Dec. 2017. [Online]. Available:
https://doi.org/10.1145/3158092[40] P. A. Abdulla, M. F. Atig, Y .-F. Chen, L. Hol ´ık, A. Rezine, P. R ¨ummer,
and J. Stenman, “Norn: An SMT solver for string constraints,” in
International Conference on Computer Aided V eriﬁcation. Springer,2015, pp. 462–469.
[41] Y . Zheng, X. Zhang, and V . Ganesh, “Z3-str: A z3-based string solver for
web application analysis,” in Proceedings of the 2013 9th Joint Meeting
on F oundations of Software Engineering, 2013, pp. 114–124.
[42] Y . Zheng, V . Ganesh, S. Subramanian, O. Tripp, M. Berzish, J. Dolby,
and X. Zhang, “Z3str2: an efﬁcient solver for strings, regular ex-pressions, and length constraints,” F ormal Methods in System Design,
vol. 50, no. 2-3, pp. 249–288, 2017.
[43] P. R ¨ummer, “A constraint sequent calculus for ﬁrst-order logic with
linear integer arithmetic,” in Proceedings, 15th International Conference
on Logic for Programming, Artiﬁcial Intelligence and Reasoning, ser.LNCS, vol. 5330. Springer, 2008, pp. 274–289.
[44] M.-T. Trinh, D.-H. Chu, and J. Jaffar, “Progressive reasoning over
recursively-deﬁned strings,” in International Conference on Computer
Aided V eriﬁcation. Springer, 2016, pp. 218–240.
[45] M. Mues and F. Howar, “Jdart: Portfolio solving, breadth-ﬁrst search
and smt-lib strings (competition contribution),” Tools and Algorithms
for the Construction and Analysis of Systems, vol. 12652, p. 448, 2021.
[46] T. Tateishi, M. Pistoia, and O. Tripp, “Path-and index-sensitive string
analysis based on monadic second-order logic,” ACM Transactions on
Software Engineering and Methodology (TOSEM) , vol. 22, no. 4, pp.
1–33, 2013.
[47] J. D. Day, T. Ehlers, M. Kulczynski, F. Manea, D. Nowotka, and
D. B. Poulsen, “On solving word equations using sat,” in International
Conference on Reachability Problems. Springer, 2019, pp. 93–106.
[48] P. Saxena, D. Akhawe, S. Hanna, F. Mao, S. McCamant, and D. Song, “A
symbolic execution framework for JavaScript,” in 2010 IEEE Symposium
on Security and Privacy. IEEE, 2010, pp. 513–528.
[49] T. Brennan, N. Tsiskaridze, N. Rosner, A. Aydin, and T. Bultan,
“Constraint normalization and parameterized caching for quantitativeprogram analysis,” in Proceedings of the 2017 11th Joint Meeting on
F oundations of Software Engineering, 2017, pp. 535–546.
[50] J. Thom ´e, L. K. Shar, D. Bianculli, and L. Briand, “Benchmark
suite for ”an integrated approach for effective injection vulnerabilityanalysis of web applications through security slicing and hybridconstraint solving”,” 2018. [Online]. Available: http://dx.doi.org/10.21227/H2ZQ1N
[51] J. Thom ´e, L. K. Shar, D. Bianculli, and L. Briand, “An integrated
approach for effective injection vulnerability analysis of web appli-cations through security slicing and hybrid constraint solving,” IEEE
Transactions on Software Engineering, vol. 46, no. 2, pp. 163–195, 2020.
[52] S. Kausler and E. Sherman, “Evaluation of string constraint solvers in the
context of symbolic execution,” in Proceedings of the 29th ACM/IEEE
international conference on Automated software engineering, 2014, pp.259–270.
[53] K. Luckow, M. Dimja ˇsevi´c, D. Giannakopoulou, F. Howar, M. Isberner,
T. Kahsai, Z. Rakamari ´c, and V . Raman, “JDart: A dynamic symbolic
analysis framework,” in International Conference on Tools and Algo-
rithms for the Construction and Analysis of Systems. Springer, 2016,pp. 442–459.
[54] D. Beyer, “Software veriﬁcation: 10th comparative evaluation (SV-
COMP 2021),” Tools and Algorithms for the Construction and Analysis
of Systems, vol. 12652, p. 401, 2021.
[55] D. Beyer, S. L ¨owe, and P. Wendler, “Reliable benchmarking: Re-
quirements and solutions,” International Journal on Software Tools for
Technology Transfer, vol. 21, no. 1, pp. 1–29, 2019.
190