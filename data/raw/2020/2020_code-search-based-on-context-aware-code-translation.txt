Code Search based on Context-aware Code Translation
Weisong Sun
weisongsun@smail.nju.edu.cn
State Key Laboratory for Novel
Software Technology
Nanjing University, ChinaChunrong Fangâˆ—
fangchunrong@nju.edu.cn
State Key Laboratory for Novel
Software Technology
Nanjing University, ChinaYuchen Chen
yuc.chen@outlook.com
State Key Laboratory for Novel
Software Technology
Nanjing University, China
Guanhong Tao
taog@purdue.edu
Purdue University
West Lafayette, Indiana, USATingxu Han
hantingxv@163.com
School of Information Management
Nanjing University, ChinaQuanjun Zhang
quanjun.zhang@smail.nju.edu.cn
State Key Laboratory for Novel
Software Technology
Nanjing University, China
ABSTRACT
Code search is a widely used technique by developers during soft-
ware development. It provides semantically similar implementa-
tionsfromalargecodecorpustodevelopersbasedontheirqueries.
Existing techniques leverage deep learning models to construct
embeddingrepresentationsforcodesnippetsandqueries,respec-
tively.Featuressuchasabstractsyntactictrees,controlflowgraphs,
etc., are commonly employed for representing the semantics of
code snippets. However, the same structure of these features does
not necessarily denote the same semantics of code snippets, and
viceversa.Inaddition,thesetechniquesutilizemultipledifferent
word mapping functions that map query words/code tokens to
embedding representations. This causes diverged embeddings of
thesameword/tokeninqueriesandcodesnippets.Weproposea
novelcontext-awarecodetranslationtechniquethattranslatescode
snippets into natural language descriptions (called translations).
The codetranslationis conductedon machineinstructions, where
the context information is collected by simulating the execution
of instructions. We further design a shared word mapping func-tion using one single vocabulary for generating embeddings for
both translations and queries. We evaluate the effectiveness of our
technique, called TranCS, on the CodeSearchNet corpus with 1,000
queries.ExperimentalresultsshowthatTranCSsignificantlyout-
performsstate-of-the-arttechniquesby49.31% to66.50%interms
ofMRR(mean reciprocal rank).
CCS CONCEPTS
â€¢Software and its engineering â†’Search-based software en-
gineering.
âˆ—Corresponding author.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation
on the first page. Copyrights for components of this work owned by others than ACMmustbehonored.Abstractingwithcreditispermitted.Tocopyotherwise,orrepublish,topostonserversortoredistributetolists,requirespriorspecificpermissionand/ora
fee. Request permissions from permissions@acm.org.
ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
Â© 2022 Association for Computing Machinery.
ACM ISBN 978-1-4503-9221-1/22/05...$15.00
https://doi.org/10.1145/3510003.3510140KEYWORDS
code search, deep learning, code translation
ACM Reference Format:
WeisongSun,ChunrongFang,YuchenChen,GuanhongTao,TingxuHan,
and Quanjun Zhang. 2022. Code Search based on Context-aware Code
Translation. In 44th International Conference on Software Engineering (ICSE
â€™22), May 21â€“29, 2022, Pittsburgh, P A, USA. ACM, New York, NY, USA,
13 pages. https://doi.org/10.1145/3510003.3510140
1 INTRODUCTION
Software development is usually a repetitive task, where same
orsimilarimplementationsexistinestablishedprojectsoronline
forums.Developerstendtosearchforthosehigh-qualityimplemen-
tationsforreferenceorreuse,soastoenhancetheproductivityandqualityoftheirdevelopment[
4,5,16].Existingstudies[ 5,56]show
thatdevelopersoftenspend19%oftheirtimeonfindingreusablecodeexamplesduringsoftwaredevelopment.Codesearch(CS)is
anactiveresearchfield[ 7,18,41,49,54,56,62â€“64,67],whichaims
atdesigningadvancedtechniquestosupportcoderetrievalservices.
Given a query by the developer, CS retrieves code snippets thatare related to the query from a large-scale code corpus, such as
GitHub[17]andStackOverflow[ 26].Figure1showsanexample.
Thequeryâ€œhowtocalculatethefactorialofanumberâ€inFigure1(a)
isprovidedbythedeveloper,whichisusuallyashortnaturallan-
guage sentence describing the functionality of the desired code
snippet [35]. The method/function [ 27,48,56,62] in Figure 1(b) is
a possible code snippet that satisfies the developerâ€™s requirement.
/g8 /g16/g17/g24/g12/g10/g23/g0/g12/g13/g14/g27/g0/g28/g21/g23/g18/g13/g15/g10/g21/g12/g0/g2/g10/g14/g18/g0/g14/g17/g11/g24/g25/g15/g1/g0/g42
/g7 /g12/g13/g14/g27/g0/g28/g21/g23/g18/g13/g15/g10/g21/g12/g0/g08/g0/g5/g06
/g00 /g10/g14/g18/g0/g10/g0/g08/g0/g5/g06
/g00 /g28/g13/g15/g0/g2/g06/g0/g10/g0/g05/g08/g0/g14/g17/g11/g24/g25/g15/g06/g0/g10/g3/g3/g1/g0/g42
/g02 /g28/g21/g23/g18/g13/g15/g10/g21/g12/g0/g08/g0/g28/g21/g23/g18/g13/g15/g10/g21/g12/g0/g4/g0/g10/g06
/g01/g41
/g04 /g15/g25/g18/g17/g15/g14/g0/g28/g21/g23/g18/g13/g15/g10/g21/g12/g06
/g03/g41/g10/g13/g40/g0/g18/g13/g0/g23/g21/g12/g23/g17/g12/g21/g18/g25/g0/g18/g10/g25/g0/g28/g21/g23/g18/g13/g15/g10/g21/g12/g0/g13/g28/g0/g21/g0/g14/g17/g11/g24/g25/g15/g6
/g2/g24/g1/g0/g07/g0/g20/g13/g26/g25/g0/g22/g14/g10/g16/g16/g25/g18/g0 /g1/g1 /g2/g21/g1/g0/g07/g0/g20/g17/g25/g15/g40
Figure 1: An Example of Query and Code Snippet
Existing CS techniques can be categorized into traditional meth-
odsthat use keyword matching between queries and code snippets
3882022 IEEE/ACM 44th International Conference on Software Engineering (ICSE)
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 07:54:11 UTC from IEEE Xplore.  Restrictions apply. ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA Weisong Sun and Chunrong Fang, et al.
suchasinformationretrieval-basedcodesearch[ 4,27,40,51,54]
andquery reformulation-basedcodesearch [ 22,31,36,37,48,52],
anddeeplearningmethods thatencodequeriesandcodesnippets
into embedding representations capturing semantic information.
Traditionalmethodssimplytreatqueriesandcodesnippetsasplain
texts,andretrievequery-relatedcodesnippetsbyonlylookingat
matchedkeywords.Theyfailtocapturethesemanticsofbothquery
sentencesandcodesnippets.Deeplearning(DL)methodstransform
input queries and code snippets into embedding representations.
Specifically, for a given query, all the words in the query sentence
arefirstrepresentedaswordembeddingsandthenfedtoaDLmodeltoproduceaqueryembedding[
18,56].Foracodesnippet,multiple
aspects are extracted as features, such as tokens, abstract syntactic
trees (ASTs), and control flow graphs (CFGs). These features are
transformed into corresponding embeddings and processed by an-
other DL model to produce a code embedding [ 12,18,56,64,67].
Thecodesearchtaskishencetofindsimilarpairsbetweenquery
embeddingsand codeembeddings.While DLmethods surpasstra-
ditional methods in capturing the semantics of queries and code
snippets,theirperformancesarestilllimitedduetotheinsufficiency
of encoding semantics and the embedding discrepancy betweenqueries and code snippets. Existing techniques miss either data
dependenciesamongcodestatementslikeMMAN[ 62]orcontrol
dependenciessuchasDeepCS[ 18],CARLCS-CNN[ 56],andTabCS
[64]. Furthermore, the embedding representations of code snippets
are largely different from those of query sentences written in natu-
rallanguage,causingsemanticmismatchduringthecodesearch
task. For example, MMAN [ 62] uses different word mapping func-
tions(thatmapawordortokentoanembeddingrepresentation)
to encode queries, and tokens, ASTs, and CFGs in code snippets.For the widely used word
lengthin both queries and code snip-
pets, the embedding representations are different in those word
mappingfunctions,leadingtopoorcodesearchperformanceaswe
will discuss in Section 3 and experimentally show in Section 5.2.2.
Weproposeanovelcontext-awarecodetranslationtechnique
that translates code snippets into natural language descriptions
(called translations). Such a translation can bridge the represen-
tation discrepancy between code snippets (in programming lan-
guages)andqueries(innaturallanguage).Specifically,weutilize
a standard program compiler and a disassembler to generate the
instruction sequence of a code snippet. However, the context infor-
mation such as local variables, data dependency, etc., are missed
fromtheinstructionsequence.Wehencesimulatestheexecutionof
instructionstocollectthosedesiredcontexts.Asetofpre-defined
translationrulesarethenusedtotranslatetheinstructionsequence
andcontextsintotranslations.Suchacodetranslationiscontext-
aware. The translations of code snippets are similar to those de-scriptions in queries, in which they share a range of words. Wehence design a shared word mapping mechanism using one sin-gle vocabulary for generating embeddings for both translations
andqueries,substantiallyreducingthesemanticdiscrepancyand
improving the overall performance (see results in Section 5.2.2).
In summary, we make the following contributions.
â€¢Weproposeacontext-awarecodetranslationtechniquethat
transforms code snippets into natural language descriptions
with preserved semantics.â€¢We introduce a shared word mapping mechanism, which
bridges the discrepancy of embedding representations from
code snippets and queries.
â€¢WeimplementacodesearchprototypecalledTranCS.We
evaluate it on the CodeSearchNet corpus [ 25] with 1,000
queries.ExperimentalresultsshowthatTranCSimprovesthe
top-1 hit rate of code search by 67.16% to 102.90% compared
to state-of-the-art techniques. In addition, TranCS achieves
MRRof0.651,outperformingDeepCS[ 18]andMMAN[ 62]by
66.50% and 49.31%, respectively. The source code of TranCS
and all the data used in this paper are released and can be
downloaded from the website [58].
2 BACKGROUND
2.1 Machine Instruction
Sincethecontext-aware codetranslationtechniqueweproposeis
performedatthemachineinstructionlevel,wefirstintroducethe
background about machine instructions.
A program runs by executing a sequence of machine instruc-
tions [11]. A machine instruction consists of an opcode specifying
the operation to be performed, followed by zero or more operands
embodyingvaluestobeoperatedupon[ 33,45].Forexample,inJava
Virtual Machine, istore_2 is a machine instruction where istore
is an opcode whose operation is â€œstore intinto local variable", and
2is an operand that represents the index of the local variable. Ma-
chine instructions have been widely used in software engineering
activities, such as malware detection [ 3,11,45], API recommenda-
tion[47],codeclonedetection[ 60],programrepair[ 15],andbinary
code search [ 65]. Machine instructionsare generated by disassem-
blingthebinaryfiles,suchasthe .classfileinJava.Therefore,itis
alsocalledbytecode[ 15,47,57]orbytecodemnemonicopcode[ 60]
in some of the works mentioned above. For ease of understanding,
the terminology â€œinstructionâ€ is used uniformly in this paper.
2.2 Deep Learning-based Code Search
/g2/g14/g8/g9/g0/g5/g13/g10/g15/g15/g9/g18/g17/g4/g19/g9/g16/g20/g4/g19/g9/g16/g20/g0/g3/g13/g7/g14/g8/g9/g16
/g2/g14/g8/g9/g0/g3/g13/g7/g14/g8/g9/g16/g2/g3
/g5/g10/g12/g10/g11/g6/g16/g10/g18/g20
/g1/g4
Figure 2: A General Framework of DL-based CS techniques
AsshowninFigure2,wecanobservethatdeeplearning(DL)-
based CS techniquesusually consist of threecomponents, a query
encoder, a code encoder, and a similarity measurement component.
Thequeryencoderisanembeddingnetworkthatcanencodethe
queryğ‘given by the developer into a ğ‘‘-dimensional embedding
representation ğ’†ğ‘âˆˆRğ‘‘. To train such a query encoder, existing
DL-basedCStechniqueshavetriedvariousneuralnetworkarchi-
tectures, such as RNN [ 18], LSTM [ 62], and CNN [ 64]. In DL-based
CSstudies,itisacommonpracticetousecodecommentsasqueries
during the training phase of the encoder [ 18,56,62]. Code com-
mentsarenaturallanguage descriptionsused toexplain whatthe
389
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 07:54:11 UTC from IEEE Xplore.  Restrictions apply. code snippets want to do [ 24]. For example, the first line of Fig-
ure 3(a) is a comment for the code snippet ğ‘ ğ‘. Therefore, we do
not strictly distinguish the meaning of the two terms comment and
query, and use the term comment during encoder training, and
queryatothertimes.Thecodeencoderisalsoanembeddingnet-
work that can encode ğ‘›code snippets in the code corpus ğ‘†into
corresponding embedding representations ğ‘¬ğ‘†âˆˆRğ‘›Ã—ğ‘‘. Inexisting
DL-basedCStechniques,thecodeencoderisusuallymuchmore
complicatedthanthequeryencoder.Forexample,thecodeencoder
of MMAN [ 62] consists of three sub-encoders that are built on the
LSTM[23],Tree-LSTM[ 59],andGGNN[ 32]architectureswiththe
goal of encoding different features of the code snippet, e.g., tokens,
ASTs,andCFGs.Thesimilaritymeasurementcomponentisused
to measure the cosine similarity between ğ’†ğ‘and each ğ’†ğ‘ âˆˆğ‘¬ğ‘ . The
targetofDL-basedCStechniquesistorankallcodesnippetsin ğ‘†
by thecosine similarity [ 18]. Thehigher the similarity, the higher
relevance of the code snippet to the given query.
3 MOTIVATION
Inthissection,westudythelimitationsofcommonlyusedrepresen-
tationsofcodesnippetsaswellastherepresentationdiscrepancy
between code snippets and comments in existing works [18, 62].
/g8/g6/g6/g0/g27/g25/g34/g27/g42/g34/g25/g41/g29/g0/g41/g32/g29/g0/g40/g42/g35/g0/g37/g30/g0/g25/g36/g0/g33/g36/g41/g0/g25/g39/g39/g25/g43
/g7/g38/g42/g26/g34/g33/g27/g0/g33/g36/g41/g0/g27/g25/g34/g20/g39/g39/g25/g43/g22/g42/g35/g2/g33/g36/g41/g23/g24/g0/g25/g39/g39/g25/g43/g1/g0/g46
/g00/g33/g36/g41/g0/g40/g42/g35/g0/g19/g0/g5/g17
/g00/g33/g36/g41/g0/g33/g0/g19/g0/g5/g17
/g02/g30/g37/g39/g0/g2/g17/g0/g33/g0/g18/g0/g25/g39/g39/g25/g43/g5/g34/g29/g36/g31/g41/g32/g17/g0/g33/g4/g4/g1/g0/g46
/g01/g40/g42/g35/g0/g19/g0/g40/g42/g35/g0/g4/g0/g25/g39/g39/g25/g43/g23/g33/g24/g17
/g04/g45
/g15 /g39/g29/g41/g42/g39/g36/g0 /g40/g42/g35/g17
/g16/g45/g6/g6/g0/g27/g25/g34/g27/g42/g34/g25/g41/g29/g0/g41/g32/g29/g0/g0/g40/g42/g35/g0/g37/g30/g0/g25/g36/g0/g33/g36/g41/g0/g25/g39/g39/g25/g43/g8
/g38/g42/g26/g34/g33/g27/g0/g33/g36/g41/g0/g27/g25/g34/g20/g39/g39/g25/g43/g22/g42/g35/g2/g33/g36/g41/g23/g24/g0/g25/g39/g39/g25/g43/g1/g0/g46 /g7
/g33/g36/g41/g0/g39/g29/g40/g42/g34/g41/g0/g19/g0/g5/g17 /g00
/g33/g36/g41/g0/g33/g36/g28/g29/g44/g0/g19/g0/g5/g17 /g00
/g43/g32/g33/g34/g29/g2/g33/g36/g28/g29/g44/g0/g18/g0/g25/g39/g39/g25/g43/g5/g34/g29/g36/g31/g41/g32/g1/g0/g46 /g02
/g39/g29/g40/g42/g34/g41/g0/g19/g0/g39/g29/g40/g42/g34/g41/g0/g4/g0/g25/g39/g39/g25/g43/g23/g33/g36/g28/g29/g44/g24/g17/g01
/g33/g36/g28/g29/g44/g4/g4/g17 /g04
/g45 /g15
/g39/g29/g41/g42/g39/g36/g0/g39/g29/g40/g42/g34/g41/g17/g45 /g16
/g2/g25/g1/g0/g21/g37/g28/g29/g0/g22/g36/g33/g38/g38/g29/g41/g0/g1/g2 /g2/g26/g1/g0/g21/g37/g28/g29/g0/g22/g36/g33/g38/g38/g29/g41/g0/g1/g3
Figure 3: Code Snippets
/g14/g12/g21/g9/g26/g26/g12/g31/g10/g29/g22
/g25/g12/g26/g12/g22/g27
/g12/g26/g26/g12/g31/g13/g21/g24/g14/g20
/g26/g15/g28/g29/g26/g23
/g27/g29/g22/g16/g24/g26
/g19/g8
/g6/g19/g7
/g12/g26/g26/g12/g31/g5/g21/g15/g23/g17/g28/g18
/g27/g29/g22/g4/g8
/g19/g12/g26/g26/g12/g31/g13/g21/g24/g14/g20
/g19/g4/g4/g8
/g6 /g27/g29/g22/g14/g12/g21/g9/g26/g26/g12/g31/g10/g29/g22
/g25/g12/g26/g12/g22/g27
/g12/g26/g26/g12/g31/g13/g21/g24/g14/g20
/g26/g15/g28/g29/g26/g23
/g27/g29/g22/g30/g18/g19/g21/g15
/g19/g8
/g6/g19 /g7
/g12/g26/g26/g12/g31/g5/g21/g15/g23/g17/g28/g18
/g27/g29/g22/g4/g8
/g19/g12/g26/g26/g12/g31/g13/g21/g24/g14/g20
/g19/g4/g4/g8
/g6/g27/g29/g22
/g2/g12/g1/g0/g1/g2/g32/g27/g0/g9/g10/g11 /g2/g13/g1/g0/g1/g3/g32/g27/g0/g9/g10/g11
Figure 4: Abstract Syntactic Trees
Figure 3 shows two code snippets for calculating the sum of a
given intarray.Figure3(a)usesa forstatementtoloopoverallthe
elementsinthearray(line5)andaddtheirvaluestovariable sum
(line6). Figure 3(b)employs a whilestatement forthe sametask
(lines5-8).Semantically,thetwocodesnippetshavetheexactsame
meaning. In Figure 4, we show the abstract syntax trees (ASTs) for
the above two code snippets ğ‘ ğ‘(left figure) and ğ‘ ğ‘(right figure),/g8/g14/g0/g30/g25/g27/g40/g37/g38/g33/g25/g34/g2/g36/g41/g35/g26/g29/g38/g1
/g9/g14/g0/g30/g25/g27/g40/g37/g38/g33/g25/g34/g16/g7
/g10/g14/g0/g27/g37/g41/g36/g40/g29/g38/g16/g7
/g11/g14/g0/g30/g37/g38/g2/g27/g37/g41/g36/g40/g29/g38/g15/g16/g36/g41/g35/g26/g29/g38 /g1
/g29/g36/g28/g30/g37/g38
/g13/g14/g0/g38/g29/g40/g41/g38/g36/g0/g30/g25/g27/g40/g37/g38/g33/g25/g34/g12/g14/g0/g30/g25/g27/g40/g37/g38/g33/g25/g34/g16/g30/g25/g27/g40/g37/g38/g33/g25/g34 /g4/g27/g37/g41/g36/g40/g29/g38
/g11/g14/g0/g27/g37/g41/g36/g40/g29/g38/g3/g3/g19/g25/g34/g39/g29/g22/g38/g41/g29/g8/g14/g0/g27/g25/g34/g17/g38/g38/g25/g42/g21/g41/g35 /g2/g25/g38/g38/g25/g42/g1
/g9/g14/g0/g39/g41/g35/g16/g7
/g10/g14/g0/g33/g16/g7
/g11/g14/g0/g30/g37/g38/g2/g33/g15/g16/g25/g38/g38/g25/g42/g6/g34/g29/g36/g31/g40/g32/g1
/g29/g36/g28/g30/g37/g38
/g13/g14/g0/g38/g29/g40/g41/g38/g36/g0/g39/g41/g35/g12/g14/g0/g39/g41/g35/g16/g39/g41/g35 /g3/g25/g38/g38/g25/g42/g23/g33/g24
/g11/g14/g0/g33/g3/g3/g19/g25/g34/g39/g29/g22/g38/g41/g29
/g2/g25/g1/g0/g1/g2/g43/g39/g0/g18/g19/g20 /g2/g26/g1/g0/g1/g1/g43/g39/g0/g18/g19/g20
Figure 5: Control Flow Graphs
respectively. Observethatthe sub-treescircledin dottedlinesare
different for the two code snippets. Such representations cause the
inconsistency of code semantics, leading to inferior results in code
searchaswewillshowinSection5.2.1.Controlflowgraph(CFG)
is also commonly used for representing code snippets. Figure 5
depictstheCFGsforthetwocodesnippets ğ‘ ğ‘andğ‘ 1(seeFigure1(b)
inSection1).Thetaskof ğ‘ 1istocalculatethefactorialofagiven
number,while ğ‘ ğ‘istocalculatethesumofagivenarray.Thetwo
code snippets have completely different goals. However, the CFGs
shown in Figure 5 have the same graph structure, which cannot
differentiatethesemanticdifferencebetweenthetwocodesnippets.
This example delineates the insufficiency of utilizing CFGs for rep-
resenting code semantics. Our experimental results in Section 5.2.1
showthatastate-of-the-arttechniqueMMAN[ 62]leveragingASTs
and CFGs has a limited performance.
Existingtechniquesleveragedeeplearningmodels(i.e.,theen-
coders introduced in Section 2.2) for code search, where code snip-
petsandcommentsneedtobetransformedintonumericalforms
in order to train those models and produce desired outputs. A com-
mon way is to build vocabularies for code snippets and comments,
and construct corresponding numerical representations (e.g., word
embeddings).Awordmappingfunctionisadictionarywiththekey
ofatokenincodesnippetsorawordincomments(fromvocabular-ies)andthe valueofafixed-lengthreal-valued vector. DeepCS[
18]
builds four mapping functions for method names (MN), API se-quences (APIs), tokens, and comments, separately. MMAN [
62]
utilizesfourdifferentmappingfunctionsfortokens,ASTs,CFGs,and comments, respectively. The embeddings in these mappingfunctions are randomly initialized and learned during the train-
ingprocessoftheencoder.Suchalearningprocedureintroduces
discrepantembedding representationsfor asamekey (e.g.,a code
token). For instance, ASTs are composed of code tokens, whichshare a portion of same keys with the token vocabulary. Token
namescanalsoappearincomments.Forexample,morethan50%
ofkeysappearinbothcodesnippetsandcommentsvocabularies
used by DeepCS and MMAN. Inconsistent embeddings for same
words/tokens can lead to unsuitable matches between code snip-
pets and comments, causing poor performance of code search (see
Section 5.2.2).Our solution.
Weproposeanovelcodesearchtechnique,called
TranCS,thatbetterpreservesthesemanticsofcodesnippetsand
bridges the discrepancy between code snippets and comments. Dif-
ferentfromexistingtechniquesthatleverageASTsandCFGs,we
390
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 07:54:11 UTC from IEEE Xplore.  Restrictions apply. ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA Weisong Sun and Chunrong Fang, et al.
/g4/g13/g0/g30/g35/g33/g24/g0/g25/g28/g34/g0/g19/g29/g28/g33/g34/g17/g28/g34/g0/g2/g2
/g5/g13/g0/g33/g34/g29/g32/g21/g0/g25/g28/g34/g0/g2/g25/g28/g34/g29/g0/g26/g29/g19/g17/g26/g0/g36/g17/g32/g25/g17/g18/g26/g21/g0/g19/g21/g16/g3/g18/g11/g19/g21/g15/g20 /g2
/g6/g13/g0/g30/g35/g33/g24/g0/g25/g28/g34/g0/g19/g29/g28/g33/g34/g17/g28/g34/g0/g2/g2
/g7/g13/g0/g33/g34/g29/g32/g21/g0/g25/g28/g34/g0/g2/g25/g28/g34/g29/g0/g26/g29/g19/g17/g26/g0/g36/g17/g32/g25/g17/g18/g26/g21/g0/g14/g3/g14/g17/g10/g11/g23/g2
/g8/g13/g0/g26/g29/g17/g20/g0/g25/g28/g34/g0/g22/g9/g15/g21/g11/g22/g32/g29/g27/g0/g26/g29/g19/g17/g26/g0/g36/g17/g32/g25/g17/g18/g26/g21/g0/g14/g3/g14/g17/g10/g11/g23/g2
/g9/g13/g0/g26/g29/g17/g20/g32/g21/g22/g21/g32/g21/g28/g19/g21 /g0/g9/g18/g18/g9/g24/g3/g9/g18/g18/g9/g24 /g22/g32/g29/g27/g0/g26/g29/g19/g17/g26/g0/g36/g17/g32/g25/g17/g18/g26/g21/g0/g9/g18/g18/g9/g24/g3/g9/g18/g18/g9/g24/g2
/g10/g13/g0/g23/g21/g34/g0/g26/g21/g28/g23/g34/g24/g0/g29/g22/g0/g17/g32/g32/g17/g37/g0/g9/g18/g18/g9/g24/g3/g9/g18/g18/g9/g24/g2
/g11/g13/g0/g25/g22/g0/g17/g28/g20/g0/g29/g28/g26/g37/g0/g25/g22/g0/g25/g28/g34/g0/g22/g9/g15/g21/g11/g25/g33/g0/g23/g32/g21/g17/g34/g21/g32/g0/g29/g32/g0/g21/g31/g35/g17/g26/g0/g34/g29/g0/g25/g28/g34/g0/g15/g11/g17/g12/g20/g13/g34/g24/g21/g28/g0/g23/g29/g0/g34/g29/g0/g4/g4/g2
/g5/g4/g13/g0/g26/g29/g17/g20/g0/g25/g28/g34/g0/g22/g9/g15/g21/g11/g8/g3/g1/g22/g32/g29/g27/g0/g26/g29/g19/g17/g26/g0/g36/g17/g32/g25/g17/g18/g26/g21/g0/g19/g21/g16/g3/g18/g11/g19/g21/g15/g20 /g2
/g5/g5/g13/g0/g26/g29/g17/g20/g32/g21/g22/g21/g32/g21/g28/g19/g21 /g0/g9/g18/g18/g9/g24/g3/g9/g18/g18/g9/g24 /g22/g32/g29/g27/g0/g26/g29/g19/g17/g26/g0/g36/g17/g32/g25/g17/g18/g26/g21/g0/g9/g18/g18/g9/g24/g3/g9/g18/g18/g9/g24/g2
/g5/g6/g13/g0/g26/g29/g17/g20/g0/g25/g28/g34/g0/g22/g9/g15/g21/g11/g8/g4/g22/g32/g29/g27/g0/g26/g29/g19/g17/g26/g0/g36/g17/g32/g25/g17/g18/g26/g21/g0/g14/g3/g14/g17/g10/g11/g23/g2
/g5/g7/g13/g0/g26/g29/g17/g20/g25/g28/g34/g0/g22/g9/g15/g21/g11/g8/g5/g22/g32/g29/g27/g0/g9/g18/g18/g9/g24/g3/g9/g18/g18/g9/g24/g15/g22/g9/g15/g21/g11/g8/g4/g16/g2
/g5/g8/g13/g0/g25/g28/g34/g0/g32/g21/g33/g35/g26/g34/g0/g25/g33/g0/g25/g28/g34/g0/g22/g9/g15/g21/g11/g8/g3/g17/g20/g20/g0/g25/g28/g34/g0/g22/g9/g15/g21/g11/g8/g5/g14/g0 /g30/g35/g33/g24/g0/g32/g21/g33/g35/g26/g34/g0/g25/g28/g34/g29/g0/g22/g9/g15/g21/g11/g8/g6/g2
/g5/g9/g13/g0/g33/g34/g29/g32/g21/g0/g25/g28/g34/g0/g22/g9/g15/g21/g11/g8/g6/g25/g28/g34/g29/g0/g26/g29/g19/g17/g26/g0/g36/g17/g32/g25/g17/g18/g26/g21/g0/g19/g21/g16/g3/g18/g11/g19/g21/g15/g20 /g2
/g5/g10/g13/g0/g25/g28/g19/g32/g21/g27/g21/g28/g34/g0 /g26/g29/g19/g17/g26/g0/g36/g17/g32/g25/g17/g18/g26/g21/g0/g14/g3/g14/g17/g10/g11/g23/g18/g37/g0/g19/g29/g28/g33/g34/g17/g28/g34/g0/g3/g2
/g5/g12/g13/g0/g23/g29/g34/g29/g6/g2
/g6/g6/g13/g0/g26/g29/g17/g20/g0/g25/g28/g34/g0/g22/g9/g15/g21/g11/g8/g7/g1/g22/g32/g29/g27/g0/g26/g29/g19/g17/g26/g0/g36/g17/g32/g25/g17/g18/g26/g21/g0/g19/g21/g16/g3/g18/g11/g19/g21/g15/g20 /g2
/g6/g7/g13/g0/g32/g21/g34/g35/g32/g28/g25/g28/g34/g22/g9/g15/g21/g11/g8/g7/g1/g22/g32/g29/g27/g0/g27/g21/g34/g24/g29/g20/g2
Figure 6: Code Translations of ğ‘ ğ‘andğ‘ ğ‘
directly translate code snippets into natural language sentences.
Specifically,weutilizeastandardprogramcompilerandadisassem-
bler to generate the instruction sequence of a code snippet. Such a
sequence,however,lacksthecontextinformationsuchaslocalvari-
ables, data dependency, etc. We propose to simulate the execution
ofinstructionstocollectthosedesiredcontexts.Asetofpre-defined
translationrulesarethenusedtotranslatetheinstructionsequence
and contexts into natural language sentences. Details can be found
inSection 4.Figure 6showcases thetranslations ofthetwocode
snippetsğ‘ ğ‘andğ‘ ğ‘by TranCS. The different colors denote different
variablenamesusedin ğ‘ ğ‘(blue)and ğ‘ ğ‘(red).Thenumbers/wordsin
bold (e.g., valueand22) denote the data and control dependencies
amonginstructions.Observethat thetranslationsof ğ‘ ğ‘andğ‘ ğ‘are
the same except for local variable names. The overall semanticsdescribed by the sentences in Figure 6 are the same. The transla-
tions are similar to those descriptions in comments, in which they
sharearangeofwords.Wehencedesignasharedwordmapping
functionusingonesinglevocabularyforgeneratingembeddings
forbothcodesnippetsandcomments,substantiallyreducingthe
semantic discrepancyand improving theoverall performance (see
results in Section 5.2.2).
4 METHODOLOGY
4.1 Overview
Figure7illustratestheoverviewofourTranCS.Thetoppartshows
thetrainingprocedureofTranCSandthebottompartshowsthe
usageofTranCSforagivenquery.Duringthetrainingprocedureof
TranCS, two types of input data are leveraged: comments and code
snippets.ThecommentsinFigure7arenaturallanguagedescrip-
tions that appear above the code snippet (e.g., Javadoc comments),
notinthecodebody.ThesecommentsareinputtoTranCSinpairs
withthecorrespondingcodesnippetstotrainCEncoderandTEn-
coder.Forcomments,TranCStransformsthemintovectorrepresen-
tations ğ‘½ğ¶usingasharedwordmappingfunction.Forcodesnippets,
theyaredifferentfromnaturallanguageexpressionssuchascom-
ments.Inthispaper,weaimtobuildahomogeneousrepresentation
/g9/g29/g21/g22/g0/g15/g28/g24/g30/g30/g22/g33/g32/g9/g29/g27/g27/g22/g28/g33/g32
/g11/g28/g32/g33/g31/g34/g20/g33/g24/g29/g28/g32/g5/g8
/g16
/g16/g31/g18/g28/g32/g26/g18/g33/g24/g29/g28/g32
/g14/g34/g22/g31/g37/g5/g9
/g2/g18/g1/g0/g9/g29/g28/g33/g22/g36/g33/g4/g18/g35/g18/g31/g22/g0 /g9/g29/g21/g22/g0/g16/g31/g18/g28/g32/g26/g18/g33/g24/g29/g28 /g2/g19/g1/g0/g12/g29/g21/g22/g26/g0/g16/g31/g18/g24/g28/g24/g28/g23 /g11/g28/g30/g34/g33 /g13/g34/g33/g30/g34/g33
/g5/g10/g5/g6/g9/g1/g5/g7/g9/g1/g2
/g16/g29/g30/g4/g25/g0/g9/g29/g21/g22/g0/g15/g28/g24/g30/g30/g22/g33/g32/g5/g9/g2/g4/g3/g1/g9/g20/g10/g14/g17/g14/g17/g13/g1/g18/g12/g1/g9/g20/g10/g17/g6/g8/g4/g8
/g4/g9/g5/g8/g0/g24/g20/g29/g28/g32/g33/g17/g5
/g6/g8/g0/g24/g32/g33/g29/g31/g22/g17/g7
/g38
/g17 /g18/g19
/g9/g10/g28/g20/g29/g21/g22/g31
/g9/g10/g28/g20/g29/g21/g22/g31
/g9/g21/g15
/g2/g5/g3/g1/g7/g11/g19/g15/g18/g22/g16/g11/g17 /g21/g1/g18/g12/g1/g9/g20/g10/g17/g6/g8/g16/g10/g28/g20/g29/g21/g22/g31
/g18
/g19
/g3/g10
Figure 7: Framework of TranCS
betweencommentsandcodesnippets,whichcanbettercapturethe
shared semantic information of these two types. Specifically, we
propose a context-aware code translation, which translates code
snippets into natural language descriptions as shown in the dotted
box (detailsare discussed in Section4.2). The natural languagede-scriptions translated from code snippets are also transformed into
vector representations ğ‘½ğ‘‡using the same shared word mapping
function. TranCS leverages the two vector representations ğ‘½ğ¶and
ğ‘½ğ‘‡for building two encoders (i.e., CEncoder and TEncoder) that
generateembeddingswithpreservedsemanticsforbothcomments
and code snippets. CEncoder takes in the comment vector repre-
sentations ğ‘½ğ¶andproducesconcise embeddingrepresentations ğ’†ğ¶
thatpreservessemanticinformationfromthecomments.TEncoder
generates embedding representations ğ’†ğ‘‡for code snippets. Details
of training these two encoders are elaborated in Section 4.3. When
TranCSisdeployedforusage,ittakesinaqueryfromthedeveloper
and passes it to CEncoder, which produces an embedding ğ’†ğ‘for
the query. TranCS then compares the query embedding ğ’†ğ‘with
those code embeddings ğ’†ğ‘‡from the training set. A top-k selection
methodisleveragedforprovidingcodesnippetstothedeveloper,
which are semantically similar to the query.
4.2 Context-aware Code Translation
The goal of context-aware code translation is to translate code
snippets into natural language descriptions according to the pre-
definedtranslationrules.AsshowninthedottedboxofFigure7,
this phase consists of two steps. In step ÂŒ, given code snippets,
TranCSutilizesastandardcompileranddisassemblertogenerate
their instruction sequences. In step Â, TranCS applies the pre-
definedtranslationrulestotranslatetheinstructionsequencesintonatural languagedescriptions. We discuss thetwo steps indetail in
the following sections.
4.2.1 Instruction Generation. In this step, TranCS takes in code
snippets and produces their instruction sequences. In practice, for
agivencodesnippet,TranCSfirst utilizesastandardprogramcom-
piler and disassembler to generate the disassembly representation
of the code snippet. For example, TranCS integrates javacversion
1.8.0_144 (a compiler) and javapversion 1.8.0_144 (a disassem-
bler) to generate the disassembly representations for code snippets
writtenintheJavaprogramminglanguage.Forthecodesnippets
that can not be compiled, the main reason is due to the lack of
class/methoddefinitionsaroundthem.WeuseJCoffee[ 20]tomake
391
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 07:54:11 UTC from IEEE Xplore.  Restrictions apply. Code Search based on Context-aware Code Translation ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
themcompilable byaddingclass/method definitionsaroundthem
to complement the missing pieces. Then, TranCS parses the disas-
semblyrepresentationandextractstheinstructionsequence.For
example, Figure 8(a) shows an instruction sequence, which is gen-
erated by inputting the code snippet shown in Figure 3(a) into
TranCS.
/g6/g15/g0/g32/g26/g36/g35/g40/g41/g23/g6
/g7/g15/g0/g32/g40/g41/g36/g39/g28/g23/g8
/g8/g15/g0/g32/g26/g36/g35/g40/g41/g23/g6
/g9/g15/g0/g32/g40/g41/g36/g39/g28/g23/g9
/g10/g15/g0/g32/g33/g36/g24/g27/g23/g9
/g11/g15/g0/g24/g33/g36/g24/g27/g23/g7
/g12/g15/g0/g24/g39/g39/g24/g44/g33/g28/g35/g30/g41/g31
/g13/g15/g0/g32/g29/g23/g32/g26/g34/g37/g30/g28 /g8/g8
/g7/g6/g15/g0/g32/g33/g36/g24/g27/g23/g8
/g7/g7/g15/g0/g24/g33/g36/g24/g27/g23/g7
/g7/g8/g15/g0/g32/g33/g36/g24/g27/g23/g9
/g7/g9/g15/g0/g32/g24/g33/g36/g24/g27
/g7/g10/g15/g0/g32/g24/g27/g27
/g7/g11/g15/g0/g32/g40/g41/g36/g39/g28/g23/g8
/g7/g12/g15/g0/g32/g32/g35/g26 /g9/g4 /g0/g7
/g7/g14/g15/g0/g30/g36/g41/g36 /g10
/g8/g8/g15/g0/g32/g33/g36/g24/g27/g23/g8
/g8/g9/g15/g0/g32/g39/g28/g41/g42/g39/g35
/g2/g24/g1/g0/g17/g35/g40/g41/g39/g42/g26/g41/g32/g36/g35/g0 /g19/g28/g38/g42/g28/g35/g26/g28/g6/g15/g0/g37/g42/g40/g31/g0/g32/g35/g41/g0/g26/g36/g35/g40/g41/g24/g35/g41/g0/g21/g37/g26/g22/g5
/g7/g15/g0/g40/g41/g36/g39/g28/g0/g32/g35/g41/g0/g21/g37/g40/g22/g0/g32/g35/g41/g36/g0/g33/g36/g26/g24/g33/g0/g43/g24/g39/g32/g24/g25/g33/g28/g0 /g21/g37/g43/g22/g5
/g8/g15/g0/g37/g42/g40/g31/g0/g32/g35/g41/g0/g26/g36/g35/g40/g41/g24/g35/g41/g0/g21/g37/g26/g22/g5
/g9/g15/g0/g40/g41/g36/g39/g28/g0/g32/g35/g41/g0/g21/g37/g40/g22 /g32/g35/g41/g36/g0 /g33/g36/g26/g24/g33/g0/g43/g24/g39/g32/g24/g25/g33/g28/g0 /g21/g37/g43/g22/g5
/g10/g15/g0/g33/g36/g24/g27/g0/g32/g35/g41/g0/g21/g37/g40/g22/g0/g29/g39/g36/g34/g0/g33/g36/g26/g24/g33/g0/g43/g24/g39/g32/g24/g25/g33/g28/g0 /g21/g37/g43/g22/g5
/g11/g15/g0/g33/g36/g24/g27 /g39/g28/g29/g28/g39/g28/g35/g26/g28 /g0/g21/g37/g40/g22 /g29/g39/g36/g34/g0 /g33/g36/g26/g24/g33/g0/g43/g24/g39/g32/g24/g25/g33/g28/g0 /g21/g37/g43/g22/g5
/g12/g15/g0/g30/g28/g41/g0/g33/g28/g35/g30/g41/g31/g0 /g36/g29/g0/g24/g39/g39/g24/g44/g0/g21/g37/g40/g22/g5
/g13/g15/g0/g32/g29/g0/g32/g35/g41/g0/g21/g37/g40/g22/g0/g32/g40/g0/g30/g39/g28/g24/g41/g28/g39/g0/g36/g39/g0/g28/g38/g42/g24/g33/g0/g41/g36/g0/g32/g35/g41/g0/g21/g37/g40/g22/g0/g41/g31/g28/g35/g0/g30/g36/g0/g41/g36/g0/g21/g37/g32/g22/g5
/g7/g6/g15/g0/g33/g36/g24/g27/g0/g32/g35/g41/g0/g21/g37/g40/g22/g0/g29/g39/g36/g34/g0/g33/g36/g26/g24/g33/g0/g43/g24/g39/g32/g24/g25/g33/g28/g0 /g21/g37/g43/g22/g5
/g7/g7/g15/g0/g33/g36/g24/g27 /g39/g28/g29/g28/g39/g28/g35/g26/g28 /g0/g21/g37/g40/g22 /g29/g39/g36/g34/g0 /g33/g36/g26/g24/g33/g0/g43/g24/g39/g32/g24/g25/g33/g28/g0 /g21/g37/g43/g22/g5
/g7/g8/g15/g0/g33/g36/g24/g27/g0/g32/g35/g41/g0/g21/g37/g40/g22/g0/g29/g39/g36/g34/g0/g33/g36/g26/g24/g33/g0/g43/g24/g39/g32/g24/g25/g33/g28/g0 /g21/g37/g43/g22/g5
/g7/g9/g15/g0/g33/g36/g24/g27 /g32/g35/g41/g0/g21/g37/g40/g22 /g29/g39/g36/g34/g0 /g24/g39/g39/g24/g44/g0/g21/g37/g40/g22/g5
/g7/g10/g15/g0/g32/g35/g41/g0/g39/g28/g40/g42/g33/g41 /g32/g40/g0 /g32/g35/g41/g0/g21/g37/g40/g22/g0/g24/g27/g27/g0/g32/g35/g41/g0/g21/g37/g40/g22/g06/g37/g42/g40/g31 /g32/g35/g41/g0 /g39/g28/g40/g42/g33/g41/g5
/g7/g11/g15/g0/g40/g41/g36/g39/g28/g0/g32/g35/g41/g0/g21/g37/g40/g22 /g32/g35/g41/g36/g0 /g33/g36/g26/g24/g33/g0/g43/g24/g39/g32/g24/g25/g33/g28/g0 /g21/g37/g43/g22/g5
/g7/g12/g15/g0/g32/g35/g26/g39/g28/g34/g28/g35/g41/g0 /g33/g36/g26/g24/g33/g0/g43/g24/g39/g32/g24/g25/g33/g28/g0 /g21/g37/g43/g22 /g25/g44/g0 /g26/g36/g35/g40/g41/g24/g35/g41/g0/g21/g37/g26/g22/g5
/g7/g14/g15/g0/g30/g36/g41/g36 /g21/g37/g32/g22/g5
/g8/g8/g15/g0/g33/g36/g24/g27/g0/g32/g35/g41/g0/g21/g37/g40/g22 /g29/g39/g36/g34/g0 /g33/g36/g26/g24/g33/g0/g43/g24/g39/g32/g24/g25/g33/g28/g0 /g21/g37/g43/g22/g5
/g8/g9/g15/g0/g39/g28/g41/g42/g39/g35 /g32/g35/g41 /g21/g37/g40/g22 /g29/g39/g36/g34/g0 /g34/g28/g41/g31/g36/g27/g5
/g2/g25/g1/g0/g20/g39/g24/g35/g40/g33/g24/g41/g32/g36/g35/g0 /g18/g42/g33/g28/g40
Figure 8: An Example of Instruction Sequence and Transla-
tion Rules. [pc] and [pv] indicate filling in a constant and
variable, respectively. [ps] indicates filling a value popped
from the operand stack, while [pi] indicates filling in an in-struction index.
In addition to the instruction sequence, TranCS also extracts
the local variable table from the disassembly representation, which
willbe usedin thesubsequent instructiontranslationprocess. For
example,Listing1showsanexampleofalocalvariabletable(Lo-
calVariableTable)thatpresentsthelocalvariablesinvolvedinthe
codesnippet indetail, andisgenerated alongwith theinstruction
sequenceinFigure8(a).Detailsabouttheusageoflocalvariables
are introduced in Section 4.2.2.
1 LocalVariableTable:
2 Start Length Slot Name Signature
3 02 4 0 this LCalArraySum;
4 0 24 1 array [I
5 2 22 2 sum I
6 42 0 3 i I
Listing 1: An Example of Local Variable Table
4.2.2 Instruction Translation. In this step, TranCS takes in instruc-
tionsequencesandproducestheirnaturallanguagedescriptions.Inthissection,wefirstintroducethetranslationrulesusedinTranCS,
then introduce the instruction context, and finally present how
TranCS implements context-aware instruction translation.
Translation Rules (TR). TRusedinTranCSismanuallycon-
structed based on the instruction specification. In practice, to con-
structTR,wecollectedalloperationsanddescriptionsofinstruc-
tions from themachine instruction specification, such asJava Vir-
tual Machine Specification [ 33]. An operation is a short naturallanguagedescriptionofaninstruction.Forexample,theinstruction
istoreâ€™s operation is:
â€œstore intinto local variable.â€
From this operation, we can know the behavior of istoreis to
store an intvalue into a local variable. A description is a long
natural language description of an instruction, which details theinteraction of the instruction on the local variables and operand
stack. For example, istoreâ€™s description is:
â€œTheindexisanunsignedbytethatmustbeanindexintothelocalvariable
array of the current frame. The valueon the top of the operand stack must
be of type int. It is popped from the operand stack, and the value of the
local variable at indexis set to value.â€
From this description, we can know that istorefirst pops an int
value from the operand stack and then stores the value into the
index-th position of the local variable array. If we only use the
operation as the translation of the instruction, the translation will
be inaccurate due to the loss of some important context. If we only
usethedescriptionasthetranslationofinstructions,thetranslation
will be too long. However, research in the field of natural language
processing(NLP)remindsusthatcapturingthesemanticsoflong
textsismoredifficultthanshorttexts[ 2,61].Basedontheabove,
westrivetomaketheinstructiontranslationshortandrelatively
accurate. Therefore, we use the operation as the basis, combing
the context specified inthe description,to manuallycollate atrans-
lationforeachinstruction.Suchatranslationdelicatelybalances
shortness and accuracy. For example, the translation we collate for
the instruction istoreas follows:
â€œstore int[ps] into local variable [pv].â€
where [ps] and [pv] denote placeholders that specifies the position
wherethecontextwillbefilled,anddetailsaboutinstructioncontext
arediscussedinSection Context-aware Instruction Translation .
For example, Figure 8(b) shows the result of TranCS using TR to
translate the instruction sequence in Figure 8(a).
Instruction Context. The context of an instruction consists of
constants, local variables, and data and control dependencies with
other instructions. Constants and local variables are directly deter-
mined by operands. As shown in Figure 8(a), an opcode is followed
by zero or more operands. An operand can be a constant, or an
indexofalocalvariable,oranindexofaninstruction.Forexample,
in Figure 8(a), the operand 0 following the opcode iconstrepre-
sents a constant, while the operand 2 following the opcode istore
representstheindexofthelocalvariable ğ‘ ğ‘¢ğ‘šshowninListing1;
Control dependencies between instructions are explicitly passed
through the indices of the instruction. The indices are also directly
specifiedbyoperands.Forexample,theoperand22followingthe
opcode if_icmpge representstheindexoftheinstruction iload_2
atline22inFigure8(a).Datadependenciesbetweeninstructions
are implicitly passed through the operand stack. As described in
SectionTranslation Rules (TR) ,withtheguidanceofthedescrip-
tion,wecanknowhoweachinstructioninteractswiththeoperand
stack,suchaspoppingorpushingdata.Iftheinstruction ğ‘–ğ‘pops
(i.e., uses) the data that is pushed onto the operand stack by the
instruction ğ‘–ğ‘, then we say that ğ‘–ğ‘is data dependent on ğ‘–ğ‘. For
example, Figure 9(a) shows the changes of the operand stack as
theopcodesequenceinFigure8interactswiththeoperandstack.
The values in the operand stack are the carriers that reflect data
392
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 07:54:11 UTC from IEEE Xplore.  Restrictions apply. ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA Weisong Sun and Chunrong Fang, et al.
dependencies between instructions. Figure 9(b) shows the data and
control dependencies between the instructions in Figure 8(a). In
this figure, nodes represent instructions; the labels of nodes are
instructionsâ€™indices;thesolidanddashededgesrepresentdataand
control dependencies, respectively.
/g21/g36/g36/g21/g40
/g40/g21/g31/g17/g25/g20/g5/g40/g21/g31/g17/g25/g21/g36/g36/g21/g40
/g40/g21/g31/g17/g25
/g40/g21/g31/g17/g25/g20/g8 /g40/g21/g31/g17/g25/g20/g9/g8/g13/g29/g31/g34/g21/g24/g20/g7/g4
/g4/g13/g29/g23/g34/g33/g37/g18/g20/g4/g5/g13/g29/g37/g18/g34/g36/g25/g20/g6/g4
/g6/g13/g29/g23/g34/g33/g37/g18/g20/g4/g7/g13/g29/g37/g18/g34/g36/g25/g20/g7 /g9/g13/g21/g31/g34/g21/g24/g20/g5
/g10/g13/g21/g36/g36/g21/g40/g31/g25/g33/g27/g18/g28/g31/g25/g33/g27/g18/g28
/g40/g21/g31/g17/g25
/g11/g13/g29/g26/g20/g29/g23/g32/g35/g27/g25/g6/g6/g40/g21/g31/g17/g25/g20/g5
/g5/g4/g13/g29/g31/g34/g21/g24/g20/g6 /g5/g5/g13/g29/g31/g34/g21/g24/g20/g5 /g5/g6/g13/g29/g31/g34/g21/g24/g20/g7/g40/g21/g31/g17/g25/g20/g6
/g21/g36/g36/g21/g40
/g40/g21/g31/g17/g25/g20/g5
/g5/g7/g13/g29/g21/g31/g34/g21/g24/g40/g21/g31/g17/g25/g20/g7
/g40/g21/g31/g17/g25/g20/g5
/g5/g8/g13/g29/g21/g24/g24 /g5/g9/g13/g29/g37 /g18/g34/g36/g25/g20/g6/g5/g10/g13/g29/g29/g33/g23/g0/g7/g0/g5 /g5/g12/g13/g27/g34/g18 /g34/g0/g8/g6/g6/g13/g29/g31/g34/g21/g24/g20/g6 /g6/g7/g13/g29/g36/g25/g18/g17/g36/g33/g1
/g2 /g4
/g8/g2/g1/g3
/g5/g6
/g7
/g2/g2 /g2/g3
/g2/g4 /g2/g5
/g2/g6/g2/g7
/g2/g9
/g3/g3
/g3/g4
/g2/g21/g1/g0/g14/g28/g21/g33/g27/g25/g37/g0/g34/g26/g0/g18/g28/g25/g0/g18/g35/g25/g36/g21/g33/g24/g0/g19/g18/g21/g23/g30 /g2/g22/g1/g0/g17/g33/g37/g18/g36/g17/g23/g18/g29/g34/g33/g0 /g15/g25/g35/g25/g33/g24/g25/g33/g23/g40/g0 /g16/g36/g21/g35/g28
Figure 9: An Example of the Changes of the Operand Stackand Instruction Dependency Graph
Context-aware Instruction Translation.
The basic idea of
context-aware instruction translation is to simulate the execution
of instructions by statically traversing the instruction sequence
fromtoptodown.Inthetraversalprocess,wecollectthecontext
of each instruction, which will be used to update the TR-based
translations of the current or other related instructions.
Inthe actualexecutionofinstructions, aframeiscreated when
thecorrespondingcodesnippetisinvoked[ 34].Aframecontainsa
localvariablearrayandalast-in-first-outstack(i.e.,operandstack).
Thesizesofthelocalvariablearrayandtheoperandstackarede-
terminedatcompile-time.Thelocalvariablearraystoresalllocal
variables used in the instructions. For example, the local variables
showninListing1areusedintheinstructionsequenceshownin
Figure8(a).Theindicesofthelocalvariablearraycorrespondsto
that in LocalVariableTable shown in Listing 1, where the â€˜Slotâ€™ col-
umn presents indices of the local variables. The names and indices
of local variables are determined at compile-time, but their values
are dynamically updated with the execution of the instructions.
The values in the operand stack are also dynamically updated with
the execution of the instructions. As mentioned earlier, the con-
textofaninstructionincludesconstants,localvariables,dataand
control dependencies. Among them, constants, local variables and
controldependenciesarecloselyrelatedtoinstructionsâ€™operands.
Theycanbeeasilydeterminedbytheoperands(forconstants)or
byretrievingtheinstructionsequence(forcontroldependencies)
using the index specified by the operand. However, determining
the values of local variables is a challenging task because they are
dynamicallyupdatedwiththeexecutionoftheinstruction.Anal-
ogously,thedeterminationofdatadependenciesisachallenging
task because they are implicitly passed through the operand stack.
The values in operand stack are also dynamically updated with
the execution of the instruction. Therefore, we need to know in
advance how the instruction interacts with the local variable array
(e.g.,settingvalue)ortheoperandstack(e.g.,poppingorpushing
data). In practice, we obtain such information from the descriptionofeachinstruction.Thedescriptionofeachinstructionhasbeen
introduced when we introduced the translation rules earlier. With
the guidance of the description, we divide the instructions into the
followingfourcategoriesaccordingtowhethertheyinteractwiththe local variable array or the operand stack.
Category 1, expressed as Iğ‘†.I nIğ‘†, the instruction only interacts
with the operand stack. Iğ‘†can be subdivided into the following
three types:
Iğ‘ƒğ‘ˆ.Inthistype,theinteractionistopushtheoperandonto
the operand stack.
Iğ‘ƒğ‘‚.In this type, the interaction is to pop values from the
operand stack.
Iğ‘ƒğ‘‚ğ‘ˆ.In this type, the interaction is composed of popping
values from the operand stack, performing the operation,
andpushingtheresultoftheoperationtotheoperandstack.
Category 2, Iğ‘‰.I nIğ‘‰, the instruction only interacts with the
localvariablearray.Theinteractionistoloadthevaluefromthe
localvariablearray,or storethenewvalueintoit.Thistypeofin-
structiondoesnotinteractwiththeoperandstack.Forexample,theinstruction
iinc 3,1onlyinteractswiththelocalvariablespecified
by the first operand, not with the operand stack.
Category3, Iğ‘†ğ‘‰.InIğ‘†ğ‘‰,theinstructioninteractswiththeoperand
stackaswellasthelocalvariablearray.Forexample,theinstruction
istore_2 first loads the integer value from the operand stack, and
then stores the value into a local variable.
Category4, Iğ‘‚.InIğ‘‚,the instructionneitherinteractswith the
operandstacknorwiththelocalvariablearray,suchastheinstruc-
tion gotoandnop. Table 1 shows the categories of instructions.
Basedontheaboveclassification,TranCSusesAlgorithm1to
perform context-aware instruction translation. TranCS takes aninstruction sequence (
ğ¼), translation rules ( ğ‘‡ğ‘…), a local variable
array(ğ‘‰),andthedepthoftheoperandstack( ğ‘‘)asinputs. ğ‘‡ğ‘…,ğ‘‰
andğ‘‘have been introduced earlier. TranCS first initializes an stack
with a depth of ğ‘‘to store intermediate results produced during
traversing ğ¼(line 1). TranCS then traverses ğ¼from top to down
(lines2â€“33).Foreach ğ‘–âˆˆğ¼,TranCSfirstgeneratesitstranslation
ğ‘¡based on ğ‘‡ğ‘…(line 3). Then, TranCS extracts the operands from
ğ‘–(line 4). The operands are used to update ğ‘†andğ‘¡in subsequent
processes. TranCS determines ğ‘–â€™s category according to the pre-
defined categories shown in Table 1. According to ğ‘–â€™s category,
TranCS uses different processes to update ğ‘†andğ‘¡(line 5 â€“ 31).
Forexample,Figure9(a)showsanexampleofthechangesoftheoperand stack when TranCS traverses the instruction sequenceshown in Figure 8(a) from top to down. After traversing all the
instructionsin ğ¼,thealgorithmfinishesandoutputs ğ¼â€™stranslations
ğ‘‡.Forexample,Figure6showsthetranslationgeneratedbyTranCS
for the instruction sequence shown in Figure 8(a).
4.3 Model Training
The goal of model training is to train two encoders, which will be
deployed to support code search service. This phase consists of
twostepsasshowninFigure7.Instep Â,giventranslationsand
comments,TranCStransformsthemintovectorrepresentations ğ‘½ğ¶
andğ‘½ğ‘‡using a shared word mapping function. In step Â, TranCS
leverages ğ‘½ğ¶andğ‘½ğ‘‡to train CEncoder and TEncoder.
393
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 07:54:11 UTC from IEEE Xplore.  Restrictions apply. Code Search based on Context-aware Code Translation ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
Table 1: The Category of Instructions
.Category Iğ‘†
Iğ‘‰Iğ‘†ğ‘‰Iğ‘‚
Type Iğ‘ƒğ‘ˆIğ‘ƒğ‘‚Iğ‘ƒğ‘‚ğ‘ˆ
Instructionsaconst_null,
anewarray,
iconst, fconst,
bipush,
dconst_<d>,
fconst_<f>,
iconst_<i>,
jsr, jsr_w,
lconst_<l>,
ldc, ldc_w,
ldc2_w, new,
sipushareturn, if_icmpge,
ireturn, athrow,
dreturn, freturn,
if_acmp<cond>,
if_icmp<cond>,
if<cond>, ifnonnull,
ifnull, invokedynamic,
invokeinterface,
invokespecial, invokestatic,
invokevirtual, ireturn,
ishl, ishr, lookupswitch,
lreturn, monitorexit, pop,
pop2, putfield,
putstatic, tableswitchaaload, arraylength, baload,
caload, d2f, d2i, d2l, dadd,
daload, dcmp<op>, ddiv, dmul,
dneg, drem, dsub, dup, dup_x1,
dup_x2, dup2, dup2_x1, dup2_x2,
f2d, f2i, f2l, fadd, faload, fcmp<op>,
fdiv, fmul, fneg, frem, fsub, getfield,
getstatic, i2b, i2c, i2d, i2f, i2l, i2s,
iadd, iaload, iand, idiv, imul, ineg,
instanceof, ior, irem, isub, iushr,
ixor, l2d, l2f, l2i, ladd, laload, land,
lcmp, ldiv, lmul, lneg, lor, lrem,
lshl, lshr, lsub, lushr, multianewarray,
lxor, newarray, saload, swapiinc,
wideaastore, aload,
aload_<n>, astore
astore_<n>,
bastore, castore, dastore,
dload, dload_<n>,
dstore, dstore_<n>,
fastore, fload,
fload_<n>, fstore,
fstore_<n>, iastore,
iload, iload_<n>,
istore, istore_<n>,
lastore, lload,
lload_<n>, lstore,
lstore_<n>, sastoregoto,
checkcast,
goto_w,
nop,
ret,
return
Algorithm 1 Context-aware Instruction Translation
Input:An instruction sequence, ğ¼; Translation Rules, ğ‘‡ğ‘…;
A local variable array, ğ‘‰; The depth of the operand stack, ğ‘‘.
Output: Instruction Translation, ğ‘‡;
1:ğ‘†â†initialize an empty operand stack with a depth of ğ‘‘.
2:for each ğ‘–inğ¼do
3:ğ‘¡â†generate the TR-based translation of ğ‘–based on ğ‘‡ğ‘…;
4:ğ‘œğ‘ğ‘’ğ‘Ÿğ‘ğ‘›ğ‘‘ğ‘  â†extract the operands from ğ‘–;
5:ifğ‘–âˆˆIğ‘ƒğ‘ˆthen
6:ğ‘†â†pushğ‘œğ‘ğ‘’ğ‘Ÿğ‘ğ‘›ğ‘‘ğ‘  ontoğ‘†;
7:ğ‘¡â†replace [pc] in ğ‘¡withğ‘œğ‘ğ‘’ğ‘Ÿğ‘ğ‘›ğ‘‘ğ‘  ;
8:end if
9:ifğ‘–âˆˆIğ‘ƒğ‘‚then
10: ğ‘£ğ‘ğ‘™ğ‘¢ğ‘’ğ‘ â†pop values from ğ‘†byğ‘œğ‘ğ‘’ğ‘Ÿğ‘ğ‘›ğ‘‘ğ‘  ;
11:ğ‘¡â†replace [ps] in ğ‘¡withğ‘£ğ‘ğ‘™ğ‘¢ğ‘’ğ‘ ;
12:end if
13:ifğ‘–âˆˆIğ‘ƒğ‘‚ğ‘ˆthen
14: ğ‘£ğ‘ğ‘™ğ‘¢ğ‘’ğ‘ â†pop values from ğ‘†byğ‘œğ‘ğ‘’ğ‘Ÿğ‘ğ‘›ğ‘‘ğ‘  ;
15:ğ‘¡â†replace [ps] in ğ‘¡withğ‘£ğ‘ğ‘™ğ‘¢ğ‘’ğ‘ ;
16:ğ‘›ğ‘’ğ‘¤_ğ‘£ğ‘ğ‘™ğ‘¢ğ‘’â†do operation;
17:ğ‘†â†pushğ‘›ğ‘’ğ‘¤_ğ‘£ğ‘ğ‘™ğ‘¢ğ‘’ontoğ‘†;
18:end if
19:ifğ‘–âˆˆIğ‘‰then
20: ğ‘£ğ‘ğ‘Ÿğ‘–ğ‘ğ‘ğ‘™ğ‘’ â†get variable from ğ‘‰byğ‘œğ‘ğ‘’ğ‘Ÿğ‘ğ‘›ğ‘‘ğ‘  ;
21:ğ‘¡â†replace [pv] in ğ‘¡withğ‘£ğ‘ğ‘Ÿğ‘–ğ‘ğ‘ğ‘™ğ‘’;
22:end if
23:ifğ‘–âˆˆIğ‘†ğ‘‰then
24: ğ‘£ğ‘ğ‘™ğ‘¢ğ‘’ğ‘ â†pop values from ğ‘†byğ‘œğ‘ğ‘’ğ‘Ÿğ‘ğ‘›ğ‘‘ğ‘  ;
25:ğ‘¡â†replace [ps] in ğ‘¡withğ‘£ğ‘ğ‘™ğ‘¢ğ‘’ğ‘ ;
26: ğ‘£ğ‘ğ‘Ÿğ‘–ğ‘ğ‘ğ‘™ğ‘’ â†get variable from ğ‘‰byğ‘œğ‘ğ‘’ğ‘Ÿğ‘ğ‘›ğ‘‘ğ‘  ;
27:ğ‘¡â†replace [pv] in ğ‘¡withğ‘£ğ‘ğ‘Ÿğ‘–ğ‘ğ‘ğ‘™ğ‘’;
28:end if
29:ifğ‘¡contains [pi] then
30:ğ‘¡â†replace [pi] in ğ‘¡withğ‘œğ‘ğ‘’ğ‘Ÿğ‘ğ‘›ğ‘‘ğ‘  ;
31:end if
32:ğ‘‡â†ğ‘‡âˆª{ğ‘¡}
33:end for
34:outputğ‘‡;4.3.1 Shared Word Mapping. In TranCS, both translations and
comments are natural language sentences. Sentence embedding
isgeneratedbasedonwordembedding[ 43,50].Wordembedding
techniques can map words into fixed-length vectors (i.e., embed-
dings) so that similar words are close to each other in the vector
space [42, 43].
Awordembeddingtechniquecanbeconsideredawordmapping
functionğœ“, which can map a word ğ‘¤ğ‘–into a vector representation
ğ’˜ğ‘–, i.e., ğ’˜ğ’Š=ğœ“(ğ‘¤ğ‘–). As aforementioned, both translations and com-
ments are natural language sentences, so we design a shared word
mappingfunction.Toimplementsucha ğœ“,webuildasharedvocab-
ularythatincludestop- ğ‘›frequentlyappearedwordsintranslations
and comments. We further transform the vector representations
ofthewordsintoanembeddingmatrix ğ¸âˆˆRğ‘›Ã—ğ‘š,whereğ‘›isthe
size of the vocabulary, ğ‘šis the dimension of word embedding. The
embeddingmatrix ğ¸=(ğœ“(ğ’˜1),...,ğœ“(ğ’˜ğ‘–))ğ‘‡isinitializedrandomly
and learned in the training process along with the two encoders.Basedonthisembeddingmatrix,TranCScantransformstransla-
tions and comments into the vector representations ğ‘½ğ¶andğ‘½ğ‘‡.A
simplewayofsentencevectorrepresentationsistoviewitasabag
of words and add up all its word vector representations [30].
4.3.2 EncoderTraining. Inthissection,wefirstintroducethear-
chitecture of CEncoder and TEncoder, then present how to jointly
train the two encoders.
EncoderArchitecture. AsdescribedinSection4.3.1,inTranCS
both translations and comments are natural language sentences.
Therefore,wecanusethesamesequenceembeddingnetworkto
designcommentencoder(CEncoder)andtranslationencoder(TEn-coder)insteadofdesigningdifferentembeddingnetworksforthem
as the previous DL-based CS techniques, such as DeepCS [ 18] and
MMAN [62]. In practice, TranCS applies the LSTM architecture to
designCEncoderandTEncoder.Consideratranslation/comment
sentence ğ‘ =ğ‘¤1,Â·Â·Â·,ğ‘¤ğ‘ğ‘ comprising a sequence of ğ‘ğ‘ words,
TranCS first uses the shared word mapping function to produce
vectorrepresentations ğ’—ğ‘ .Then,TranCSpasses ğ’—ğ‘ totheencoder
394
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 07:54:11 UTC from IEEE Xplore.  Restrictions apply. ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA Weisong Sun and Chunrong Fang, et al.
(i.e., CEncoder or TEncoder) that generates embeddings ğ’†ğ‘ . The
hidden state ğ’‰ğ‘ 
ğ‘–of theğ‘–-th word in ğ‘ is calculated as follows:
ğ’‰ğ‘ 
ğ‘–=ğ¿ğ‘†ğ‘‡ğ‘€(ğ’‰ğ‘ 
ğ‘–âˆ’1,ğ’˜ğ‘–) (1)
where ğ’˜ğ‘–represents the vector of the word ğ‘¤ğ‘–and comes from the
embedding matrix ğ¸.
In addition, TranCS uses attention mechanism proposed by Bah-
danauetal.[ 1]toalleviatethelong-dependencyprobleminlong
text sequences [ 2]. The attention weight for each word ğ‘¤ğ‘–is calcu-
lated as follows:
ğ›¼ğ‘ 
ğ‘–=ğ‘’ğ‘¥ğ‘(ğ‘“(ğ’‰ğ‘ 
ğ‘–)Â·ğ’–ğ‘ )
/summationtext.1ğ‘ğ‘ 
ğ‘—=1ğ‘’ğ‘¥ğ‘(ğ‘“(ğ’‰ğ‘ 
ğ‘—)Â·ğ’–ğ‘ )(2)
whereğ‘“(Â·)denotes a linear layer; ğ’–ğ‘ denotes the context vector
which is a high level representation of all words in ğ‘ ; andÂ·denotes
theinnerprojectof ğ’‰ğ‘ 
ğ‘–andğ’–ğ‘ .Thecontextvector ğ’–ğ‘ israndomly
initializedandjointlylearnedduringtraining.Then, ğ‘ â€™sfinalem-
bedding representation ğ’†ğ‘ can be calculated as follows:
ğ’†ğ‘ =ğ‘ğ‘ /summationdisplay.1
ğ‘—=1ğ›¼ğ‘ 
ğ‘–Â·ğ’‰ğ‘ 
ğ‘–(3)
Joint Training. Now we present how to jointly train the two
encoders (i.e., CEncoder and TEncoder) of TranCS to transform
both translations and comments into a unified vector space with a
similarity coordination. We follow a widely adopted assumption
thatifatranslationandacommenthavesimilarsemantics,their
embeddingrepresentationsshouldbeclosetoeachother[ 18,56,62].
In other words, given a code snippet ğ‘ whose translation is ğ‘¡and a
comment ğ‘, we want it to predict a high similarity between ğ‘¡andğ‘
ifğ‘is a correct comment of ğ‘ , and a little similarity otherwise.
In practice, we first translate all code snippets into translations.
Then, we construct each training instance as a triple /angbracketleftğ‘¡,ğ‘+,ğ‘âˆ’/angbracketright: for
eachtranslation ğ‘¡thereisapositivecomment ğ‘+(aground-truth
commentof ğ‘ )andanegativecomment ğ‘âˆ’(anincorrectcommentof
ğ‘ ).Theincorrectcomment ğ‘âˆ’isselectedrandomlyfromthepoolof
all correct comments. When trained on the set of /angbracketleftğ‘¡,ğ‘+,ğ‘âˆ’/angbracketrighttriples,
TranCS predicts the cosine similarities of both /angbracketleftğ‘¡,ğ‘+/angbracketrightand/angbracketleftğ‘¡,ğ‘âˆ’/angbracketright
pairs and minimizes the ranking loss [10, 14]:
L(ğœƒ)=/summationdisplay.1
/angbracketleftğ’•,ğ’„+,ğ’„âˆ’/angbracketrightâˆˆğºğ‘šğ‘ğ‘¥(0,ğ›½âˆ’ğ‘ğ‘œğ‘ (ğ’•,ğ’„+)+ğ‘ğ‘œğ‘ (ğ’•,ğ’„âˆ’))(4)
whereğœƒdenotes the model parameters; ğºdenotes the training
dataset;ğ›½isasmallandfixedmarginconstraint; ğ’•,ğ’„+andğ’„âˆ’are
theembeddedvectors of ğ‘¡,ğ‘+andğ‘âˆ’,respectively. Intuitively,the
ranking loss encourages the cosine similarity between a transla-
tionanditscorrectcommenttogoup,andthecosinesimilarities
between a translation and incorrect comments to go down.
4.4 Deployment of TranCS
Afterthetwoencoders(i.e.,CEncoderandTEncoder)aretrained,
wecandeployTranCSonlineforcodesearchservice.Figure7(2)
showsthedeploymentofTranCS.Forasearchquery ğ‘givenbythe
developer, TranCS first uses the shared word mapping function to
transformitintovectorrepresentation ğ’—ğ‘.TranCSfurtherpasses
ğ’—ğ‘into CEncoder to generate the embedding ğ’†ğ‘. Then, TranCSmeasuresthesimilaritybetween ğ’†ğ‘andeach ğ’†ğ‘¡âˆˆğ’†ğ‘‡.Thesimilarity
is calculated as follows:
ğ‘ ğ‘–ğ‘š(ğ‘,ğ‘¡)=ğ‘ğ‘œğ‘ (ğ’†ğ‘,ğ’†ğ‘¡)=ğ’†ğ‘Â·ğ’†ğ‘¡
/bardblğ’†ğ‘/bardbl/bardblğ’†ğ‘¡/bardbl(5)
TranCSranksall ğ‘»bytheirsimilaritieswith ğ‘.Thehigherthesimi-
larity,thehigherrelevanceofthecodesnippetto ğ‘.Finally,TranCS
outputs the code snippets corresponding to the top- ğ‘˜translations
to the developer.
5 EVALUATION AND ANALYSIS
We conduct experiments to answer the following questions:
RQ1.WhatistheeffectivenessofTranCSwhencomparedwith
state-of-the-arttechniques?
RQ2.What is the contribution of key components in TranCS,
i.e., context-aware code translation and shared word map-
ping?
RQ3.What is the robustness of TranCS when varying the
query length and code length?
5.1 Experimental Setup
5.1.1 Dataset. WeevaluatetheperformanceofourTranCSona
corpusofJavacodesnippets,collectedfromthepublicCodeSearch-Netcorpus[
9].Actually,wehaveconsideredthedatasetreleasedby
baselines(i.e.,DeepCS[ 18]andMMAN[ 62]).However,the dataset
ofDeepCSonlycontainsthecleanedJavacodesnippetswithoutthe
raw data, unable to generate the CFG for MMAN. And the dataset
of MMAN is not publicly accessible.
Werandomlyshufflethedatasetandsplititintotwoparts,i.e.,
69,324samplesfortrainingand1,000samplesfortesting.Itisworth
mentioning adifference betweenour dataprocessing andthe one
in[18].In[18],theproposedapproachisverifiedonanotherisolated
datasettoavoidthebias.Sincetheevaluationdatasetdoesnothave
thegroundtruth,theymanuallylabelledthesearchedresults.As
possible subjective bias exists in manual evaluation [7, 62], in this
paper, we also adopt the automatic evaluation. Figure 10(a) and (b)
showthelengthdistributionsofcodesnippetsandcommentson
the training set. For a code snippet, its length refers to the number
of lines of the code snippet. For a comment, its length refers tothe number of words in the comment. From Figure 10(a), we can
observethatthelinesofmostcodesnippetsarelocatedbetween20to40.Thiswasalsoobservedinthequotein[
38]â€œFunctionsshould
hardlyeverbe20lineslongâ€.FromFigure10(b),itisnoticedthat
almost all comments are less than 20 in length. This also confirms
the challenge of capturing the correlation between short text with
itscorrespondingcodesnippet.Figure10(c)and(d)showthelength
distributionsofcodesnippetsandcommentsontestingdata.We
canobservethat,despiteshufflingrandomly,thedistributionsofdatasizes(i.e.,lengths)inthetwodatasetsareconsistent,sowe
can conclude that the testing set is representative.
5.1.2 Evaluation Metrics. In the evaluation, we consider the com-
ment of the code snippet as the query, and the code snippet it-self as the ground-truth result of code search, which is similarto [
21,56,62] but different from [ 7,18]. During the testing time,
we treat each comment in the 1,000 testing samples as a query, the
codesnippet correspondingto thequeryas thecorrectresult, and
395
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 07:54:11 UTC from IEEE Xplore.  Restrictions apply. Code Search based on Context-aware Code Translation ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
0 20 40 60 80 100 120 140 1600200040006000800010000Count
(a) Code Snippets on Training Set0 1 02 03 04 05 06 07 08 00200040006000800010000Count
(b) Comments on Training Set
0 20 40 60 80 100 120 140 160050100150200Count
(c) Code Snippets on Testing Set0 1 02 03 04 05 06 07 08 0050100150200Count
(d) Comments on Testing Set
Figure 10: Length Distributions
the other 999 code snippets as distractor results. We adopt two
automaticevaluationmetricsthatarewidelyusedincodesearch
studies [7,18,21,56,62] to measure the performance of TranCS,
i.e.,successrateat ğ‘˜(SuccessRate@k )andmeanreciprocalrank
(MRR).
SuccessRate@k measures the percentage of queries for which
the correct result exists in the top ğ‘˜ranked results [ 28,62], which
is computed as follows:
SuccessRate@k =1
|ğ‘„||ğ‘„|/summationdisplay.1
ğ‘–=1ğ›¿(ğ¹ğ‘…ğ‘ğ‘›ğ‘˜ğ‘„ğ‘–â‰¤ğ‘˜) (6)
whereğ‘„denotes a set of queries and |ğ‘„|is the size of ğ‘„;ğ›¿(Â·)de-
notesafunctionwhichreturns1iftheinputistrueandreturns0
otherwise; ğ¹ğ‘…ğ‘ğ‘›ğ‘˜ğ‘„ğ‘–referstotherankpositionofthecorrectresult
fortheğ‘–-thqueryin ğ‘„.SuccessRate@k isimportantbecauseabet-
ter CS technique should allow developers to discover the expected
code snippets by inspecting fewer returned results. The higher the
SuccessRate@k value, the better the code search performance.
MRRis the average of the reciprocal ranks of results of a set of
queriesğ‘„[56,62]. The reciprocal rankof a query is the inverse of
the rank of the correct result. MRRis computed as follows:
MRR=1
|ğ‘„||ğ‘„|/summationdisplay.1
ğ‘–=11
ğ¹ğ‘…ğ‘ğ‘›ğ‘˜ğ‘„ğ‘–(7)
The higher the MRRvalue, the better the code search performance.
Meanwhile, as developers prefer to find the expected code snip-
petswithshortinspection, weonlytest SuccessRate@k andMRR
on the top-10 (that is, the maximum value of ğ‘˜is 10) ranked list
following DeepCS [ 18] and MMAN [ 62]. In other words, when the
rank ofğ‘„ğ‘–is out of 10, then 1/ğ¹ğ‘…ğ‘ğ‘›ğ‘˜ ğ‘„ğ‘–is set to 0.
5.1.3 Baselines. Inthispaper,wecomparethefollowingbaselines:
â€¢DeepCS [18].DeepCSisoneoftherepresentativeDL-based
CStechniques.DeepCSusestwokindsofmodelarchitecture
to design the code encoder to embed three aspects of the
code snippet, i.e., two RNNs for method names and APIsequences, and a multi-layer perceptron (MLP) for tokens.
Its query encoder also uses RNN architecture.
â€¢MMAN[62].MMANisoneofthestate-of-the-artDL-based
CStechniques.MMANusesmultiplekindsofmodelarchitec-
turestodesign thecodeencoderto embedmultipleaspects
ofthe codesnippet,i.e.,one LSTMforToken, aTree-LSTM
for AST, and a GGNN for CFG. Its query encode uses LSTM
architecture.
5.1.4 Implementation Details. To train our model, we first shuffle
the trainingdata and setthe mini-batch sizeto 32.The size ofthe
vocabulary is 15,000. For each batch, the code snippet is padded
with a special token /angbracketleftğ‘ƒğ´ğ·/angbracketrightto the maximum length. We set the
word embedding size to 512. For LSTM unit, we set the hidden size
to 512. The margin ğ›½is set to 0.6. We update the parameters via
AdamW optimizer [ 29] with the learning rate 0.0003. To prevent
over-fitting, we use dropout with 0.1. In TranCS, the comment and
the code snippet share the same embedding weights. All models
areimplementedusing thePyTorch 1.7.1frameworkwithPython
3.8. Allexperiments are conductedon aserver equipped withone
Nvidia Tesla V100 GPU with 31 GB memory, running on Centos
7.7. All the models in this paper are trained for 200 epochs, and we
select the best model based on the lowest validation loss.
5.2 Evaluation Results
In this section, we present and analyze the experimental results to
answer the research questions.
5.2.1 RQ1:Effectiveness of TranCS. Table 2 shows the overall
performance of TranCS and two baselines, measured in terms of
SuccessRate@k andMRR. The columns SR@1,SR@5andSR@10
show the results of the average SuccessRate@k over all queries
whenğ‘˜is 1, 5 and 10, respectively. The column MRRshows the MRR
valuesofthethreetechniques.Fromthistable,wecanobservethat
forSR@k, the improvements of TranCS to DeepCS are 102.90%,
45.80%and32.48%when ğ‘˜is1,5,and10,respectively.Theimprove-
mentstoMMANare67.16%,35.94%,and25.42%,respectively.For
MRR, the improvements TranCS to DeepCS and MMAN are 66.50%
and49.31%,respectively.Wecandrawtheconclusionthatunder
all experimental settings, our TranCS consistently achieves higher
performanceintermsofbothtwometrics,whichindicatesbetter
code search performance.
Table 2: Overall Performance of TranCS and Baselines
Tech SR@1S R @5S R@10 MRR
DeepCS 0.276 0.524 0.622 0.391
MMAN 0.335 0.562 0.657 0.436
TranCS 0.560 0.764 0.824 0.651
The CodeSearchNet corpus also provides 99 realistic natural
languagesqueriesandexpertannotationsforlikelyresults.Each
query/result pair was labeled by a human expert, indicating the
relevance of the result for the query. We also conduct experiments
on99 queriesprovidedbythe CodeSearchNetcorpusfor theJava
396
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 07:54:11 UTC from IEEE Xplore.  Restrictions apply. ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA Weisong Sun and Chunrong Fang, et al.
programminglanguage.Weusethesamemetric,normalizeddis-
counted cumulative gain (NDCG [ 55]), to evaluate baselines and
TranCS.OurTranCSachievesNDCGof0.223,outperformingDeepCS
(0.138) and MMAN (0.173) by 62% and 30%, respectively.
Table 3: Contribution of Key Components in TranCS
Tech SR@1S R @5S R@10 MRR
TokeCS 0.247 0.477 0.586 0.359
TranCS (CCT) 0.352 0.569 0.664 0.455
TokeCS (SWM) 0.264 0.483 0.592 0.370
DeepCS (SWM) 0.295 0.511 0.615 0.399
TranCS (CCT+SWM) 0.560 0.764 0.824 0.651
5.2.2 RQ2:ContributionofKey Components. We experimentally
verified the effectiveness of two key components of TranCS i.e.,
context-awarecodetranslation(CCT)andsharedwordmapping
(SWM). In Table 3, TranCS(CCT) and TranCS(CCT+SWM) are two
specialversionsofTranCS,amongwhichtheformerusestwodiffer-
entwordmappingfunctionstotransforminstructiontranslations
andcommentstovectorrepresentations,whilethelatterusesSWM.
In other words, if it is only CCT, TranCS uses two vocabularies. In
the case of CCT+SWM, TranCS uses a shared vocabulary. More-
over, numerous existing studies [ 53,56,68] including DeepCS [ 18]
and MMAN [ 62] have shown that tokens of code snippets play a
keyroleincodesearchtasks.Therefore,weassumethatthisisa
scenariowherethecodesnippetisnottranslated,andwedirectly
pass the tokens of the code snippet into the model to train the
codeencoder.Theeffectivenessofthetoken-basedCStechnique
(TokeCS) is shown in the second line of Table 3. To demonstrate
theeffectivenessofSWM,wealsotriedtoapplySWMtoTokeCS,
DeepCSandMMAN.ToapplySWMtoTokeCS,weuseaunified
word mapping function to transform tokens and comments. In
DeepCS,theauthorusesfourwordmappingfunctionstotransform
theMN,APIS,Tokenandcommentsintovectorrepresentations.To
apply SWM to DeepCS, we first merge the four vocabularies into a
shared vocabulary by extracting the union of them. Then, we use a
unified word mapping function to transform MN, APIS, Token and
comments. In MMAN, the author not only uses LSTM architecture
to embed tokens, but also uses Tree-LSTM and GGNN to embed
ASTandCFG,whilethethreearchitecturescannotshareaword
mappingfunction.Therefore,SWMcannotbeappliedtoMMAN.
TheeffectivenessofToke(SWM),DeepCS(SWM)areshowninlines
4â€“5ofTable3.Fromthelines2â€“3ofTable3,wecanobservethat
forSR@k,theimprovementsofTranCS(CCT)toTokeCSare42.51%,
19.29% and 13.31% when ğ‘˜is 1, 5, and 10, respectively. For MRR,
the improvement to TokeCS is 26.74%. Therefore, we can conclude
thatCCTcontributestoTranCS.For SR@k,theimprovementsof
TranCS(CCT+SWM)toTranCS(CCT)are59.09%,34.27%and24.10%.
ForMRR, the improvement of TranCS(CCT+SWM) to TranCS(CCT)
is 43.08%. Therefore, we can conclude that SWM contributes to
TranCS. Besides, we can also observe that SWM also has slight
improvements to TokeCS and DeepCS. Therefore, we can draw the
conclusionthatSWMandCCT,whichpromoteeachother,improve
the performance of TranCS jointly.5.2.3 RQ3:Robustness of TranCS. To analyze the robustness of
TranCS, we studied two parameters (i.e., code length and comment
length)thatmayhaveanimpactontheembeddingrepresentations
of translations and comments. Figure 11 shows the performance of
TranCS based on different evaluation metrics with varying param-
eters.FromFigure11,wecanobservethatinmostcases,TranCS
maintainsastableperformanceeventhoughthecodesnippetlength
orcommentlengthincreases,whichcanbeattributedtocontext-
aware code translation and shared word mapping we proposed.
When the length of the code snippet exceeds 20 (a common range
describedinSection5.1.1),theperformanceofTranCSdecreases
as the length increases. It means that when the length of the code
snippets or comments exceeds the common range, as the lengthcontinues to increase, it will be more difficult to capture their se-
mantics.Overall,theresultsverifytherobustnessofourTranCS.
0 5 10 15 20 25 30 350.00.20.40.60.81.0ScoreSR@1 SR@5 SR@10 MRR
(a) Varying Code Snippet Lengths0 5 10 15 20 25 30 350.00.20.40.60.81.0ScoreSR@1 SR@5 SR@10 MRR
(b) Varying Comment Lengths
Figure 11: Robustness of TranCS
6 CASE STUDY
/g6/g39/g33/g28/g24/g0/g36/g40/g21/g34/g16/g30/g25/g31/g25/g32/g37/g17/g32/g18/g28/g36/g37/g2 /g18/g28/g36/g37/g12/g17/g32/g37/g25/g26/g25/g35/g14 /g0/g30/g28/g36/g37/g4/g0/g28/g32/g37/g0/g28/g4/g0/g28/g32/g37/g0/g29/g1/g0/g42
/g7/g28 /g32 /g37/g0/g25/g30/g25/g31/g25/g32/g37/g0/g13/g0/g30/g28/g36/g37/g5/g26/g25/g37/g2/g28/g1/g11
/g8 /g30/g28/g36/g37/g5/g36/g25/g37 /g2/g28/g4/g0/g30/g28/g36/g37/g5/g26/g25/g37/g2/g29/g1/g1/g11
/g9 /g30/g28/g36/g37/g5/g36/g25/g37 /g2/g29/g4/g0/g25/g30/g25/g31/g25/g32/g37/g1/g11
/g10/g41
/g2/g22/g1/g0/g15/g33/g24/g25/g0/g20/g32/g28/g34/g34/g25/g37/g0/g1/g1
/g2/g23/g1/g0/g15/g33/g24/g25/g0/g20/g32/g28/g34/g34/g25/g37/g0/g1/g2/g36/g40/g21/g34/g0/g37/g40/g33/g0/g25/g30/g25/g31/g25/g32/g37/g36/g0/g28/g32/g0/g37/g27/g25/g0/g30/g28/g36/g37
/g2/g21/g1/g0/g19/g38/g25/g35/g40/g0/g1
/g6 /g39/g33/g28/g24/g0/g36/g40/g21/g34/g16/g30/g25/g31/g25/g32/g37/g17/g32/g18/g28/g36/g37 /g2/g18/g28/g36/g37/g12/g17/g32/g37/g25/g26/g25/g35/g14 /g0/g30/g28/g36/g37/g4/g0/g28/g32/g37/g0/g28/g4/g0/g28/g32/g37/g0/g29/g1/g0/g42
/g7 /g15/g33/g30/g30/g25/g23/g37/g28/g33/g32/g36/g5/g36 /g40/g21/g34/g2/g30/g28/g36/g37/g4/g0/g28/g4/g0/g29/g1/g11
/g8/g41
Figure12:ExampleofTwoCodeSnippetsImplementingthe
Same Functionality
This isa case to studythe performance of TranCSin retrieving
codewithimplantationdifference.Figure12(b)and(c)showtwo
codesnippetsthatimplementthesamefunctionality,i.e.,swapping
397
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 07:54:11 UTC from IEEE Xplore.  Restrictions apply. Code Search based on Context-aware Code Translation ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
twoelementsinthelist.Thefirstone( ğ‘ 1)implementsthefunctional-
ity from scratch, and the second one ( ğ‘ 2) directly calls the external
APIğ¶ğ‘œğ‘™ğ‘™ğ‘’ğ‘ğ‘¡ğ‘–ğ‘œğ‘›.ğ‘ ğ‘¤ğ‘ğ‘ (). We use TranCS to convert the two code
snippets into corresponding translations, which are very different,
meaningTranCScaneffectivelydifferentiatesemanticallysimilar
code but differs in APIs used. This is because TranCS reserves API
information (e.g., name, parameter) when generating code transla-
tion.Forexample,asshowninFigure13,thetranslationsproduced
by TranCS reserve the information of the API ğ¶ğ‘œğ‘™ğ‘™ğ‘’ğ‘ğ‘¡ğ‘–ğ‘œğ‘›.ğ‘ ğ‘¤ğ‘ğ‘ ()
invoked by ğ‘ 2, including the parameters (e.g., list) and the method
nameswap.
/g3/g8/g0/g19/g22/g9/g12/g0/g23/g13/g14/g13/g23/g13/g21/g11/g13/g0 /g19/g16/g24/g25/g0/g14/g23/g22/g20/g0 /g19/g22/g11/g9/g19/g0/g27/g9/g23/g16/g9/g10/g19/g13/g0 /g7/g5/g11/g12/g2
/g4/g8/g0/g19/g22/g9/g12/g0/g16/g21/g25/g0/g16/g0/g14/g23/g22/g20/g0 /g19/g22/g11/g9/g19/g0/g27/g9/g23/g16/g9/g10/g19/g13/g0 /g5/g2
/g5/g8/g0/g19/g22/g9/g12/g0/g16/g21/g25/g0/g17/g0/g14/g23/g22/g20/g0 /g19/g22/g11/g9/g19/g0/g27/g9/g23/g16/g9/g10/g19/g13/g0 /g6/g2
/g6/g8/g0/g16/g21/g27/g22/g18/g13/g0 /g11/g19/g9/g24/g24/g0 /g1/g9/g7/g7/g4/g3/g12/g5/g9/g8/g11 /g24/g25/g9/g25/g16/g11/g0 /g20/g13/g25/g15/g22/g12/g0 /g11/g13/g2/g10/g2
/g7/g8/g0/g23/g13/g25/g26/g23/g21/g0 /g27/g22/g16/g12/g0 /g14/g23/g22/g20/g0 /g20/g13/g25/g15/g22/g12/g2
Figure 13: Translations of the Code Snippet ğ‘ 2
7 THREATS TO VALIDITY
The metrics used in this paper are SuccessRate@k andMRRfor
evaluating the effectiveness of TranCS and existing techniques.
These are the same metrics adopted in MMAN [ 62]. We do not use
another metric Precision@k that measures the percentage of rel-
evant results in the top ğ‘˜returned results for each query [ 18]. This
is due to the constraint that the relevant results need to be labelled
manually,whichis empiricallylessfeasibleandcanintroducehu-
man biases. We hence focus on the two metrics SuccessRate@k
andMRRin the paper.
TranCSiscurrentlyonlyevaluatedonJavaprogramsandmayre-
quiremodificationsforextendingtootherprogramminglanguages.
ThecorecontributionofTranCSisthecontext-awarecodetrans-
lation technique. To realize the context-aware code translation,
TranCSrequiresasetoftranslationrules,suchastheoperations
anddescriptionsofinstructions.InordertoextendTranCStoother
programminglanguages,correspondingtranslationrulesneedto
be designed and provided. We plan to evaluate the performance of
TranCS on these programming languages in future work.
8 RELATED WORK
EarlyCStechniqueswerebasedonIRtechnology,suchas[ 4,27,40,
51,54].Thesetechniquessimplyconsiderqueriesandcodesnippets
asplaintextandthenusekeywordmatching.Toalleviatetheprob-
lemofkeywordmismatch[ 8,22]andnoisykeywords[ 19],many
queryreformulation(QR)-basedCStechniques[ 22,31,36,37,48,52]
have been proposed one after another. For example, the words
fromWordNet[ 44],orStackOverflow[ 48]areusedtoexpanduser
queries.However,QR-basedCStechniquesconsidereachwordinde-
pendently,whileignoringthecontextoftheword.Inaddition,bothIR-basedandQR-basedCStechniquesonlytreatthecodesnippetas
plain text, and cannot capture the deep semantics of the code snip-
pet.Tobettercapturethesemanticsofqueriesandcodesnippets,deeplearning(DL)-basedCStechniques[ 7,13,18,25,53,56,62,66]
have been proposed one after another. Gu et al. [ 18] first apply DL
to the code search task. They first encode both the query and aset of code snippets into corresponding embeddings using MLP
orRNN,andthenrankthecodesnippetsaccordingtothecosine
similarityofembeddings.OtherDL-basedCStechniquesaresimilar
to DeepCS [ 18] with onlya difference in choosingthe embedding
architecture.Forexample,tocapturethesemanticsofotheraspects
of the code snippet, MMAN [ 62] integrates multiple embedding
networks (i.e., LSTM, Tree-LSTM and GGNN) to capture semantics
of multiple aspects, such as Token, AST, and CFG. CodeBERT [ 13],
CoaCor [66], andbaselines in CodeSearchNetChallenge [ 25] only
treatthecodesnippetasplaintext(tokensequence),whichmiss
richerinformationsuchasAPIs,AST,andCFG,etc.TBCNN[ 46]is
atree-basedconvolutionalneuralnetworkthatencodestheASTof the code snippet. Our baseline MMAN has encoded AST us-
ingtree-basedneuralnetworksandisinferiortoourTranCS.All
these works have a similar idea that first transforms both codesnippets and queries into embedding representations into a uni-fiedembeddingspacewithtwoencoders,andthenmeasuresthecosine similarity of these embedding representations. However,
TranCS differs from previous work in two major dimensions: 1)
TranCSfirsttranslatesthecodesnippetintosemantic-preserving
naturallanguagedescriptions.Inthiscase,thegeneratedtransla-
tionsandcommentsarehomogeneous.2)Basedoncodetranslation,
TranCSnaturallyusesasharedwordmappingmechanism,which
can produce consistent embeddings for the same words, therebybetter capturing the shared semantic information of translations
and comments.
9 CONCLUSION
In this paper, we propose a context-aware code translation tech-nique, which can translate code snippets into natural language
descriptionswithpreservedsemantics.Inaddition,weproposea
shared word mapping mechanism to produce consistent embed-
dingsforthesamewords/tokensincommentsandcodesnippets,
so as to capture the shared semantic information. On the basisofcontext-awarecodetranslationandsharedwordmapping,we
implementanovelcodesearchtechniqueTranCS.Weconductcom-prehensiveexperimentstoevaluatetheeffectivenessofTranCS,andexperimentalresultsshowthatTranCSisaneffectiveCStechnique
and substantially outperforms the state-of-the-art techniques.
Infuturework,wewillfurtherexplorethefollowingtwodimen-
sions: (1) as shown in Figure10, statistical results on large-scale
data sets show that most code snippets have no more than 20 lines.
Within this range, TranCS is robust and stable. Constructing repre-
sentationsoflongcodesnippetsisstillanopenproblem,andweleave it to future work. (2) LSTM encoder is just a component of
TranCS, which can be easily replaced with more advanced (includ-
ing pre-trained) models in [ 6,39]. We will explore more advanced
models in future work.
ACKNOWLEDGEMENT
The authors would like to thank the anonymous reviewers for
insightful comments. This work is supported partially by National
Natural Science Foundation of China(61690201, 62141215).
398
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 07:54:11 UTC from IEEE Xplore.  Restrictions apply. ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA Weisong Sun and Chunrong Fang, et al.
REFERENCES
[1]Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2015. Neural Machine
Translation by Jointly Learning to Align and Translate. In Proceedings of the 3th
International Conference on Learning Representations. San Diego, CA, USA, 1â€“15.
[2]Yoshua Bengio, Patrice Y. Simard, and Paolo Frasconi. 1994. Learning long-term
dependencies with gradient descent is difficult. IEEE Transactions on Neural
Networks 5, 2 (1994), 157â€“166.
[3]Daniel Bilar. 2007. Opcodes as predictor for malware. International Journal of
Electronic Security and Digital Forensics 1, 2 (2007), 156â€“168.
[4]Joel Brandt, Mira Dontcheva, Marcos Weskamp, and Scott R. Klemmer. 2010.
Example-centric programming: integrating web search into the development
environment.In Proceedingsofthe28thInternationalConferenceonHumanFactors
in Computing Systems. ACM, Atlanta, Georgia, USA, 513â€“522.
[5]JoelBrandt,PhilipJ.Guo,JoelLewenstein,MiraDontcheva,andScottR.Klemmer.
2009. Two studies of opportunistic programming: interleaving web foraging,
learning, and writing code. In Proceedings of the 27th International Conference on
Human Factors in Computing Systems. ACM, Boston, MA, USA, 1589â€“1598.
[6]Nghi D. Q. Bui, Yijun Yu, and Lingxiao Jiang. 2021. Self-Supervised Contrastive
Learning for Code Retrieval and Summarization via Semantic-Preserving Trans-
formations. In Proceedings of the 44th International ACM SIGIR Conference on
Research and Development in Information Retrieval. ACM, Virtual Event, Canada,
511â€“521.
[7]JosÃ©Cambronero,HongyuLi,SeohyunKim,KoushikSen,andSatishChandra.
2019. Whendeeplearningmetcodesearch.In Proceedingsofthe13JointMeeting
on European Software Engineering Conference and Symposium on the Foundations
of Software Engineering. ACM, Tallinn, Estonia, 964â€“974.
[8]ClaudioCarpinetoandGiovanniRomano.2012. ASurveyofAutomaticQuery
Expansion in Information Retrieval. Comput. Surveys 44, 1 (2012), 1â€“50.
[9]CodeSearchNet. 2019. CodeSearchNet Data. site: https://github.com/github/
CodeSearchNet#data. Accessed: 2021.
[10]RonanCollobert,JasonWeston,LÃ©onBottou,MichaelKarlen,KorayKavukcuoglu,
andPavelP.Kuksa.2011. NaturalLanguageProcessing(Almost)fromScratch.
Journal of Machine Learning Research 12, ARTICLE (2011), 2493â€“2537.
[11]YuxinDing,WeiDai,ShengliYan,andYumeiZhang.2014. Controlflow-based
opcodebehavioranalysisforMalwaredetection. Computers&Security 44(2014),
65â€“74.
[12]Chunrong Fang, Zixi Liu, Yangyang Shi, Jeff Huang, and Qingkai Shi. 2020.
Functionalcodeclonedetectionwithsyntaxandsemanticsfusionlearning.In
Proceedings of the 29th International Symposium on Software Testing and Analysis.
ACM, Virtual Event, USA, 516â€“527.
[13]Zhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xiaocheng Feng, Ming Gong,
Linjun Shou, Bing Qin, Ting Liu, Daxin Jiang, and Ming Zhou. 2020. CodeBERT:
A Pre-Trained Model for Programming and Natural Languages. In Proceedings of
the25thConferenceonEmpiricalMethodsinNaturalLanguageProcessing:Findings.
Association for Computational Linguistics, Online Event, 1536â€“1547.
[14]Andrea Frome, Gregory S. Corrado, Jonathon Shlens, Samy Bengio, Jeffrey
Dean, Marcâ€™Aurelio Ranzato, and TomÃ¡s Mikolov. 2013. DeViSE: A Deep Visual-
Semantic Embedding Model. In proceedings of the 27th Annual Conference Neural
Information Processing Systems . Curran Associates Inc., Lake Tahoe, Nevada,
United States, 2121â€“2129.
[15]Ali Ghanbari, Samuel Benton, and Lingming Zhang. 2019. Practical program
repair via bytecode mutation. In Proceedings of the 28th International Symposium
on Software Testing and Analysis. ACM, Beijing, China, 19â€“30.
[16]Mohammad Gharehyazie, Baishakhi Ray, and Vladimir Filkov. 2017. Some from
here, some from there: cross-project code reuse in GitHub. In Proceedings of the
14th International Conference on Mining Software Repositories . IEEE Computer
Society, Buenos Aires, Argentina, 291â€“301.
[17] Inc. GitHub. 2008. GitHub. site: https://github.com. Accessed: 2021.[18]
Xiaodong Gu, Hongyu Zhang, and Sunghun Kim. 2018. Deep code search. In
Proceedingsofthe40thInternationalConferenceonSoftwareEngineering.ACM,
Gothenburg, Sweden, 933â€“944.
[19]XiaodongGu,HongyuZhang,DongmeiZhang,andSunghunKim.2016. Deep
API learning. In Proceedings of the 24th International Symposium on Foundations
of Software Engineering. ACM, Seattle, WA, USA, 631â€“642.
[20]Piyush Gupta, Nikita Mehrotra, and Rahul Purandare. 2020. JCoffee: Using
Compiler Feedback to Make Partial Code Snippets Compilable. In Proceedings of
the36thInternationalConferenceonSoftwareMaintenanceandEvolution.IEEE,
Adelaide, Australia, 810â€“813.
[21]RajarshiHaldar,LingfeiWu,JinjunXiong,andJuliaHockenmaier.2020. AMulti-
Perspective Architecture for Semantic Code Search. In Proceedings of the 58th
Annual Meeting of the Association for Computational Linguistics. Association for
ComputationalLinguistics, Online, 8563â€“8568.
[22]Emily Hill, Lori L. Pollock, and K. Vijay-Shanker. 2009. Automatically captur-
ingsourcecodecontextofNL-queriesforsoftwaremaintenanceandreuse.In
Proceedings of the 31st International Conference on Software Engineering. IEEE,
Vancouver, Canada, 232â€“242.[23]Sepp Hochreiter and JÃ¼rgen Schmidhuber. 1997. Long Short-Term Memory.
Neural Computation 9, 8 (1997), 1735â€“1780.
[24]XingHu,GeLi,XinXia,DavidLo,andZhiJin.2018. Deepcodecommentgenera-tion.InProceedingsofthe26thInternationalConferenceonProgramComprehension.
ACM, Gothenburg, Sweden, 200â€“210.
[25]Hamel Husain, Ho-Hsiang Wu, Tiferet Gazit, Miltiadis Allamanis, and Marc
Brockschmidt.2019. CodeSearchNetChallenge:EvaluatingtheStateofSemantic
Code Search. CoRRabs/1909.09436(2019), 1â€“6.
[26]Stack Exchange Inc;. 2008. Stack Overflow. site: https://stackoverflow.com/.
Accessed: 2021.
[27]Iman Keivanloo, Juergen Rilling, and Ying Zou. 2014. Spotting working code ex-
amples.In Proceedingsofthe36thInternationalConferenceonSoftwareEngineering.
ACM, Hyderabad, India, 664â€“675.
[28]MertKilickaya,AykutErdem,NazliIkizler-Cinbis,andErkutErdem.2017. Re-
evaluatingAutomaticMetricsforImageCaptioning.In Proceedingsofthe15th
ConferenceoftheEuropeanChapteroftheAssociationforComputationalLinguistics.
Association for Computational Linguistics, Valencia, Spain, 199â€“209.
[29]DiederikP.KingmaandJimmyBa.2015.Adam:AMethodforStochasticOptimiza-tion.InProceedingsofthe3thInternationalConferenceonLearningRepresentations
â€“ Poster. OpenReview.net, San Diego, CA, USA, 1â€“15.
[30]Quoc V. Le and TomÃ¡s Mikolov. 2014. Distributed Representations of Sentences
andDocuments.In Proceedingsofthe31thInternationalConferenceonMachine
Learning. JMLR.org, Beijing, China, 1188â€“1196.
[31]OtÃ¡vioAugustoLazzariniLemos,AdrianoCarvalhodePaula,FelipeCapodifoglio
Zanichelli, andCristina Videira Lopes.2014. Thesaurus-basedautomatic query
expansionforinterface-drivencodesearch.In Proceedingsofthe11thWorking
Conference on Mining Software Repositories. ACM, Hyderabad, India, 212â€“221.
[32]Yujia Li, Daniel Tarlow, Marc Brockschmidt, and Richard S. Zemel. 2016. Gated
Graph Sequence Neural Networks. In Proceedings of the 4th International Confer-
ence on Learning Representations. OpenReview.net, San Juan, Puerto Rico, 1â€“20.
[33]Tim Lindholm, Frank Yellin, Gilad Bracha, Alex Buckley, and Daniel Smith. 2021.
The Java Virtual Machine Specification. site: https://docs.oracle.com/javase/
specs/jvms/se8/html/index.html. Accessed: 2021.
[34]Tim Lindholm, Frank Yellin, Gilad Bracha, Alex Buckley, and Daniel Smith. 2021.
The Java Virtual Machine Specification-Local Variables. site: https://docs.oracle.
com/javase/specs/jvms/se8/html/jvms-2.html#jvms-2.6. Accessed: 2021.
[35]Chao Liu, Xin Xia, David Lo, Cuiyun Gao, Xiaohu Yang, and John Grundy. 2020.
OpportunitiesandChallengesinCodeSearchTools. CoRRabs/2011.02297(2020),
1â€“35.
[36]MeiliLu,XiaobingSun,ShaoweiWang,DavidLo,andYucongDuan.2015. Query
expansion via WordNet for effective code search. In Proceedings of the 22nd
International Conference on Software Analysis, Evolution, and Reengineering. IEEE
Computer Society, Montreal, QC, Canada, 545â€“549.
[37]FeiLv,HongyuZhang,Jian-GuangLou,ShaoweiWang,DongmeiZhang,and
Jianjun Zhao. 2015. CodeHow: Effective Code Search Based on API Understand-
ing and Extended Boolean Model (E). In Proceedings of the 30th International
Conference on Automated Software Engineering. IEEE Computer Society, Lincoln,
NE, USA, 260â€“270.
[38]RobertCMartin.2009. Cleancode:ahandbookofagilesoftwarecraftsmanship.
Pearson Education.
[39]Antonio Mastropaolo, Simone Scalabrino, Nathan Cooper, David Nader-Palacio,
Denys Poshyvanyk, Rocco Oliveto, and Gabriele Bavota. 2021. Studying the
Usage of Text-To-Text Transfer Transformer to Support Code-Related Tasks. In
Proceedings of the 43rd International Conference on Software Engineering. IEEE,
Madrid, Spain, 336â€“347.
[40]CollinMcMillan,MarkGrechanik,DenysPoshyvanyk,QingXie,andChenFu.
2011. Portfolio:finding relevantfunctions andtheir usage.In Proceedingsof the
33rd International Conference on Software Engineering. ACM, Waikiki, Honolulu ,
HI, USA, 111â€“120.
[41]Collin McMillan, Negar Hariri, Denys Poshyvanyk, Jane Cleland-Huang, andBamshad Mobasher. 2012. Recommending source code for use in rapid soft-
wareprototypes.In Proceedingsofthe34thInternationalConferenceonSoftware
Engineering. IEEE Computer Society, Zurich, Switzerland, 848â€“858.
[42]TomÃ¡s Mikolov,KaiChen, Greg Corrado, and Jeffrey Dean. 2013. Efficient Esti-
mationofWordRepresentationsinVectorSpace.In Proceedingsofthe1stInterna-
tional Conference on Learning Representations, Workshop Track . OpenReview.net,
Scottsdale, Arizona, USA, 1â€“12.
[43]Tomas Mikolov, Ilya Sutskever, Kai Chen, Gregory S. Corrado, and Jeffrey Dean.
2013. DistributedRepresentationsofWordsandPhrasesandtheirCompositional-ity.InProceedingsofthe27thAnnualConferenceonNeuralInformationProcessing
Systems. Curran Associates Inc., Lake Tahoe, Nevada, United States, 3111â€“3119.
[44]GeorgeA.Miller.1995. WordNet:ALexicalDatabaseforEnglish. Commun.ACM
38, 11 (1995), 39â€“41.
[45]RobertMoskovitch,ClintFeher,NirTzachar,EugeneBerger,MarinaGitelman,
Shlomi Dolev, and Yuval Elovici. 2008. Unknown Malcode Detection UsingOPCODE Representation. In Proceedings of the First European Conference on
Intelligence and Security Informatics. Springer, Esbjerg, Denmark, 204â€“215.
399
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 07:54:11 UTC from IEEE Xplore.  Restrictions apply. Code Search based on Context-aware Code Translation ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
[46]LiliMou,GeLi,LuZhang,TaoWang,andZhiJin.2016. ConvolutionalNeural
NetworksoverTreeStructuresforProgrammingLanguageProcessing.In Pro-
ceedings of the 30th Conference on Artificial Intelligence. AAAI Press, Phoenix,
Arizona, USA, 1287â€“1293.
[47]Tam TheNguyen, Hung VietPham, Phong MinhVu, andTungThanh Nguyen.
2016. Learning API usages from bytecode: a statistical approach. In Proceedings
ofthe38thInternationalConferenceonSoftwareEngineering.ACM,Austin,TX,
USA, 416â€“427.
[48]Liming Nie, He Jiang, Zhilei Ren, Zeyi Sun, and Xiaochen Li. 2016. Query
ExpansionBasedonCrowdKnowledgeforCodeSearch. IEEETransactionson
Services Computing 9, 5 (2016), 771â€“783.
[49]An Examination of Software Engineering Work Practices. 1997. Janice Singer
and Timothy C. Lethbridge and Norman G. Vinson and Nicolas Anquetil. In
Proceedingsofthe7thconferenceoftheCentreforAdvancedStudiesonCollaborative
Research. IBM, Toronto, Ontario, Canada, 174â€“188.
[50]Hamid Palangi, Li Deng, Yelong Shen, Jianfeng Gao, Xiaodong He, Jianshu Chen,
XinyingSong,andRababK.Ward.2015. DeepSentenceEmbeddingUsingthe
Long Short Term Memory Network: Analysis and Application to Information
Retrieval. CoRRabs/1502.06922(2015), 1â€“15.
[51]Denys Poshyvanyk, Maksym Petrenko, Andrian Marcus, Xinrong Xie, and
DapengLiu.2006. SourceCodeExplorationwithGoogle.In Proceedingsofthe
22nd International Conference on Software Maintenance. IEEE Computer Society,
Philadelphia, Pennsylvania, USA, 334â€“338.
[52]Mohammad Masudur Rahman and Chanchal K. Roy. 2018. Effective Reformula-
tion of Query for Code Search Using Crowdsourced Knowledge and Extra-Large
DataAnalytics.In Proceedingsofthe34thInternationalConferenceonSoftware
Maintenance and Evolution. IEEE Computer Society, Madrid, Spain, 473â€“484.
[53]Saksham Sachdev, Hongyu Li, Sifei Luan, Seohyun Kim, Koushik Sen, and Satish
Chandra. 2018. Retrieval on source code: a neural code search. In Proceedings of
the2ndInternationalWorkshoponMachineLearningandProgrammingLanguages.
ACM, Philadelphia, PA, USA, 31â€“41.
[54]Caitlin Sadowski, Kathryn T. Stolee, and Sebastian G. Elbaum. 2015. How devel-
opers search for code: a case study. In Proceedings of the 10th Joint Meeting on
European Software Engineering Conference and Symposium on the Foundations of
Software Engineering. ACM, Bergamo, Italy, 191â€“201.
[55]Hinrich SchÃ¼tze, Christopher D Manning, and Prabhakar Raghavan. 2008. Intro-
duction to information retrieval. Vol. 39. Cambridge University Press Cambridge.
[56]JianhangShuai,LingXu,ChaoLiu,MengYan,XinXia,andYanLei.2020. Improv-ingCodeSearchwithCo-AttentiveRepresentationLearning.In Proceedingsofthe
28th International Conference on Program Comprehension. ACM, Seoul, Republic
of Korea, 196â€“207.
[57]Fang-Hsiang Su, Jonathan Bell, Kenneth Harvey, Simha Sethumadhavan, Gail E.
Kaiser, and Tony Jebara. 2016. Code relatives: detecting similarly behaving
software.In Proceedingsofthe24thInternationalSymposiumonFoundationsof
Software Engineering. ACM, Seattle, WA, USA, 702â€“714.
[58]Weisong Sun and Yuchen Chen. 2022. Source Code and Dataset of TranCS. site:
https://github.com/wssun/TranCS. Accessed: 2022.
[59]Kai Sheng Tai, Richard Socher, and Christopher D. Manning. 2015. Improved
Semantic Representations From Tree-Structured Long Short-Term Memory Net-
works.In Proceedingsofthe53rdAnnualMeetingoftheAssociationforCompu-
tational Linguistics. The Association for Computer Linguistics, Beijing, China,
1556â€“1566.
[60]Michele Tufano, Cody Watson, Gabriele Bavota, Massimiliano Di Penta, Martin
White,andDenysPoshyvanyk.2018. Deeplearningsimilaritiesfromdifferent
representationsofsourcecode.In Proceedingsofthe15thInternationalConference
on Mining Software Repositories. ACM, Gothenburg, Sweden, 542â€“553.
[61]Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
Aidan N. Gomez, undefinedukasz Kaiser, and Illia Polosukhin. 2017. Attention is
A l lY o uN eed .I n Proceedings of the 31st Annual Conference on Neural Information
Processing Systems. Curran Associates Inc., Long Beach, CA, USA, 5998â€“6008.
[62]Yao Wan, Jingdong Shu, Yulei Sui, Guandong Xu, Zhou Zhao, Jian Wu, and
PhilipS.Yu.2019. Multi-modalAttentionNetworkLearningforSemanticSource
Code Retrieval. In Proceedings of the 34th International Conference on Automated
Software Engineering. IEEE, San Diego, CA, USA, 13â€“25.
[63]Xin Xia, Lingfeng Bao, David Lo, Pavneet Singh Kochhar, Ahmed E. Hassan, and
ZhenchangXing.2017. Whatdodeveloperssearchforontheweb? Empirical
Software Engineering 22, 6 (2017), 3149â€“3185.
[64]LingXu,HuanhuanYang,ChaoLiu,JianhangShuai,MengYan,YanLei,andZhou
Xu. 2021. Two-Stage Attention-Based Model forCode Search with Textual and
StructuralFeatures.In Proceedingsofthe28thInternationalConferenceonSoftware
Analysis, Evolution and Reengineering. IEEE, Honolulu, HI, USA, 342â€“353.
[65]Yinxing Xue, Zhengzi Xu, Mahinthan Chandramohan, and Yang Liu. 2019. Accu-
rate and Scalable Cross-Architecture Cross-OS Binary Code Search with Emula-
tion.IEEE Transactions on Software Engineering 45, 11 (2019), 1125â€“1149.
[66]Ziyu Yao, Jayavardhan Reddy Peddamail, and Huan Sun. 2019. CoaCor: Code
AnnotationforCodeRetrievalwithReinforcementLearning.In Proceedingsofthe
28thTheWorldWideWebConference.ACM,SanFrancisco,CA,USA,2203â€“2214.[67]ChenZeng,YueYu,ShanshanLi,XinXia,ZhimingWang,MingyangGeng,Bailin
Xiao,WeiDong,andXiangkeLiao.2021. deGraphCS:EmbeddingVariable-based
Flow Graph for Neural Code Search. CoRRabs/2103.13020(2021), 1â€“21.
[68]QihaoZhu,ZeyuSun,XiranLiang,YingfeiXiong,andLuZhang.2020. OCoR:
AnOverlapping-AwareCodeRetriever.In Proceedingsofthe35thInternational
Conference on Automated Software Engineering. IEEE, Melbourne, Australia, 883â€“
894.
400
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 07:54:11 UTC from IEEE Xplore.  Restrictions apply. 