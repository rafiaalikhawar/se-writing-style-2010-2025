Generating and Visualizing Trace Link Explanations
Yalin Liu, Jinfeng Lin, Oghenemaro Anuyah, Ronald Metoyer, Jane Cleland-Huang
University of Notre Dame
Notre Dame, IN
yliu26,jlin6,oanuyah,rmetoyer,JaneHuang@nd.edu
ABSTRACT
Recentbreakthroughsindeep-learning(DL)approacheshavere-
sulted in the dynamic generation of trace links that are far moreaccurate than was previously possible. However, DL-generated
links lack clear explanations, and therefore non-experts in the do-
maincanfinditdifficulttounderstandtheunderlyingsemanticsof
the link, making it hard for them to evaluate the link‚Äôs correctness
or suitability for a specific software engineering task. In this paper
we present a novel NLP pipeline for generating and visualizing
trace link explanations. Our approach identifies domain-specificconcepts, retrieves a corpus of concept-related sentences, mines
concept definitions and usage examples, and identifies relations be-tweencross-artifactconceptsinordertoexplainthelinks.Itapplies
apost-processingsteptoprioritizethemostlikelyacronymsand
definitions and to eliminate non-relevant ones. We evaluate ourapproach using project artifacts from three different domains of
interstellar telescopes, positive train control, and electronic health-
caresystems,and thenreportcoverage,correctness, andpotential
utility of the generated definitions. We design and utilize an expla-
nation interface which leverages concept definitions and relations
to visualize and explain trace link rationales, and we report results
from a user study that was conducted to evaluate the effectiveness
of the explanation interface. Results show that the explanationspresented in the interface helped non-experts to understand the
underlyingsemanticsofatracelinkandimprovedtheirabilityto
vet the correctness of the link.
KEYWORDS
Software traceability, explanation interface, concept mining
ACM Reference Format:
Yalin Liu, Jinfeng Lin, Oghenemaro Anuyah, Ronald Metoyer, Jane Cleland-
Huang . 2022. Generating and Visualizing Trace Link Explanations. In
44thInternationalConferenceonSoftwareEngineering(ICSE‚Äô22),May21‚Äì
29, 2022, Pittsburgh, PA, USA. ACM, New York, NY, USA, 12 pages. https:
//doi.org/10.1145/3510003.3510129
1 INTRODUCTION
Software traceability establishes connections between related arti-
facts,andthenutilizesthoselinkstosupportnumeroussoftware
engineering tasks such as safety assurance, impact analysis, and
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation
onthe firstpage.Copyrights forcomponentsof thisworkowned byothersthan the
author(s)mustbehonored.Abstractingwithcreditispermitted.Tocopyotherwise,or
republish,topostonserversortoredistributetolists,requirespriorspecificpermission
and/or a fee. Request permissions from permissions@acm.org.
ICSE ‚Äô22, May 21‚Äì29, 2022, Pittsburgh, PA, USA
¬© 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 978-1-4503-9221-1/22/05...$15.00
https://doi.org/10.1145/3510003.3510129complianceverification[ 12].However,giventhenon-trivialcost
and effort of manually creating trace links, researchers have vested
significant effort into automating the process using information
retrieval(IR)[ 7,29,39],machinelearning(ML)[ 11],andmorere-
cently, deep-learning techniques (DL) [ 22,34]. In general, a project
stakeholder will issue a trace query, generate a set of links, and
inspect the resulting links to accept or reject individual resultseither as a standalone vetting activity [
28,43] or at point-of-use.
TracelinksgeneratedusingIRandMLapproachesareofteneasy
to analyze, but tend to deliver relatively low accuracy on large
industrialdatasets[ 37,39].However,recentadvancesinDLtracing
techniqueshavereturnedfarhigher-degreesofaccuracy.Forexam-
ple, in tracing from requirements to code, Lin et al., showed that
their TraceBERT approach, which leveraged pretrained BERT mod-
els and applied multi-staged fine-tuning, delivered highly accurate
trace results for three large, open-source systems achieving Mean
Average Precision (MAP) scores greater than 0.86 across several
large datasets [ 34]. Unfortunately, DL-generated trace links can
be difficult to interpret without supporting explanations of their
underlying semantics.
Forexampleconsiderarequirementstatingthat‚ÄòTherobotshall
movetothenextpositionintheorderspecifiedbythetaskplan‚Äô,
and a corresponding design definition that ‚ÄòThe RCU shall publish
an AckermannDriveStamped message to the robot‚Äôs control topic‚Äô.
Despite having no meaningful common terms the artifacts are
linked because the designsolution contributes towardssatisfying
therequirement.Ananalysisoftheconceptsshowsthat Ackerman-
nDriveStamped messages are closely associated with movements
(i.e., carry velocity, angles, and timestamps) and that task plans
ofteninvolvemovement.Adomainexpert,inthiscase,someone
familiar with the Robotic Operating System (ROS), could likely in-
spectthetwoartifacts,applytheirinnateknowledgeofthedomain,
and intuitively understand the connection between the Ackerman-
nDriveStamped messages and the robot‚Äôs movement. However,someone lacking domain expertise or knowledge of the specific
project may have difficulty understanding the underlying concepts
andconnectingthetwoartifacts.Wethereforebelievethatexpla-
nations are useful across a range of expertise levels including non-
experts(e.g.,students)andthosewithpartialdomainknowledge
(e.g., onboarding team members).
This paper therefore addresses the challenge of trace link ex-
plainability, defined as the ability to explain why two artifacts are
relatedtoeachother. Traceexplanations canincludebothtextual
information as well as visualizations, and are designed to facilitate
reasoningandunderstandingofsemanticrelations.Theyarepar-
ticularly important when analysts or trace link users lack domain
expertisetoindependentlyunderstandtheunderlyingsemanticsof
a trace link, especially as it has been shown that non-experts often
mistakingly discard correct links during the link vetting process
10332022 IEEE/ACM 44th International Conference on Software Engineering (ICSE)
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:20:26 UTC from IEEE Xplore.  Restrictions apply. ICSE ‚Äô22, May 21‚Äì29, 2022, Pittsburgh, PA, USA Liu et al.
Figure 1: Workflow of our method for extracting the explanation elements from scratch.
[28].Ourworkaddressesacurrentgapintheliterature,asprevious
studies have focused on tracelink accuracy [ 39,54], maintainabil-
ity [41] and efficiency[ 33], while overlooking the importance of
explainability.Inpriorwork,Dick[ 30],proposedtheuseoftrace
rationales in which link creators or vetters would document the
rationales behind a link. However, manually annotating links with
rationales requires nontrivial effort, thereby increasing the overall
costandeffortofcreatinglinks.Second,aspreviouslystated,DL
approaches often produce links with underlying rationales that are
obscure to non-experts. We therefore propose an approach for ex-
plainingtracelinksthataimtoautomaticallygenerateexplanations
throughmining, extracting,and learningrationalesfrom external
knowledge sources.
Our NLP pipeline executes the following automated steps to
generatealinkexplanation.First,itextractsdomain-specificcon-
ceptsfromtheartifacts,andthenusestheseconceptswithinsearch
queries to retrieve the broader context of each concept from di-
verseknowledgesources.Theresultingdatasetconstitutesa con-
text corpus for the target project. Next, it applies Natural Language
Processing (NLP) techniques to extract various forms of structured
knowledgefromthecontextcorpus,andfinally,incorporatesthis
structured knowledge into a trace link explanation that includes
bothtextualdescriptionsandvisualizationtechniques.Weevaluate
our approach against three industrial datasets, reporting accuracy
and coverage metrics. Further, we conduct a controlled user study
andreportresultsshowingthatutilizingthegenerateddescriptions
as rationales within a trace link explanation interface helps non-
expertstounderstandartifactandlinksemanticsandtoperform
the trace link vetting task more effectively.
Our work makes three primary contributions. First, we propose,
implement, andevaluate anNLP pipelinefor automaticallyidenti-
fying domain specific concepts and mining acronym expansions,
definitions,andcontextualizedusageexamples.Second,weaddressthe challenging problem of data sparsity for specific project do-
mainsbyintegratingbothtop-downandbottom-updatamining
techniques so that we can adapt our approach to different domains
withdifferentdatasources.Finally,weevaluatetheeffectiveness
of our approach through designing and evaluating an explanation
interface with non-expert users. The remainder of this paper isorganized as follows. Section 2 provides additional backgroundinformation. Sections 3 and 4 describe the three datasets used in
our study and present the NLP pipeline used to generate each part
of our trace link explanation. Section 5 takes a quantitative look at
each technique and its utility across three datasets, while Section 6
describes our design of the explanation interface and describes the
controlleduser-studythatwasconductedtoevaluateitseffective-
ness.FinallySections7to9discussthreatstovalidity,relatedwork
and present conclusions.
2 TRACE LINK EXPLANATIONS
As previously explained, we seek to generate explanations that
explain the rationale for trace links in a way that is useful for non-
experts,astheyaretheuserswhoexperiencethegreatestdifficulty
in evaluating the correctness of a link.
2.1 The Semantic Gap
Various types of artifacts, such as requirements and design, are
often written using different and potentially mismatched terminol-
ogy. This mismatch can make it challenging for non-experts in the
domaintounderstandwhytwoartifactsareconnectedbyatrace
link. A few researchers have explored ways to bridge this gap. Guoetal.proposedatechniqueforgeneratingtracelinkrationales[
25];
however,theirexplanationsweredeeplycoupledwithheuristics
embeddedintheirunderlyingtrace-linkgenerationalgorithms[ 26].
Liu et al. [ 36] used the generalized vector space model to improve
1034
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:20:26 UTC from IEEE Xplore.  Restrictions apply. Generating and Visualizing Trace Link Explanations ICSE ‚Äô22, May 21‚Äì29, 2022, Pittsburgh, PA, USA
Table 1: Software project datasets used for explanatory and evaluation purposes are drawn from three distinct domains
Name DomainSource Target TraceAvailableCount Artifact Description Count Artifact Description Links
CCHIT Electronic Health Records 117 World Vista Requirements 588 CCHIT Regulations 1065 http://coest.org
CM1 NASA - Telescope 54 Low-level requirements 23 High-level requirements 46 http://coest.org
PTC Positive Train Control 263 Subsystem requirements 964 System requirements 583 Proprietary
the quality of generated trace links, and used the HiGrowth al-
gorithm[ 59]toconstructahierarchicalmodelinwhichconcepts
were linked through synonyms, acronyms, ancestors, and siblings;
however, they did not consider generating trace link explanations.
Inthispaper,weexplorethese,andadditionaltechniques,witha
focusonexplainingthesemanticsofeachindividualartifactand
the conceptual relationships across pairs of linked artifacts.
2.2 Artifact Semantics
Tracelinkexplanationsshouldfirstprovidedescriptionsofconcepts
withinindividualartifacts.Weidentifiedthreeimportantaspects
ofartifactexplanations forenhancingtheunderstandabilityoftech-
nical artifacts. These included (1) acronym expansions, (2) concept
definitions,and(3) contextualizedusageexamples.Building upon
ourpreviousexamplewecouldexpandtheinternalprojectacronym
RCUto‚ÄúRoboticControlUnit",provideadefinitionforan Acker-
mannDriveStampedmessage as‚ÄòTime stampeddrive commandfor
robots with Ackermann steering‚Äô, and provide an example context
for the use of the term AckermannDriveStamped message.
2.3 Link Semantics
Theexplanationalsoneedstodescribethesemanticrelationship
between two linked artifacts by identifying related concepts across
source and target artifacts. We explore two primary ways in which
concept-to-concept associations could be explained. First, as a
tripletwrittenas <ùëêùëñ,ùë£,ùëê ùëó>,wheretheconcepts ùëêùëñandùëêùëóarecon-
nectedwithadescriptivephrase ùë£toindicatetheircorrelation[ 38].
AsLiuetal.[ 36]demonstrated,therearemultiplewaysinwhich
two semantically related concepts can be connected over a rela-
tion path. Examples might include ( ‚ÄòAckermannDriveStamped msg‚Äô,
‚Äòpublished to ‚Äô, ‚Äòteleop topic‚Äô)o r( ‚Äòteleop‚Äô, ‚Äòcontrols‚Äô,‚Äòrobot movement‚Äô).
Whilewewouldideallyusenaturallanguage[ 56]togenerateex-
planations,dynamicallyconstructingclearandconcisesentencesisadifficultchallenge,andthereforewefocusthispaperontheinitial
challenge of discovering meaningful triplets that explain trace link
relations.
2.4 Proposed Solution
Fig. 1 provides a high-level overview of our approach. In Step 1,
we analyze project-level artifacts to extract domain-specific con-
cepts ‚Äì focusing on noun phrases. This step produces thousands of
candidate phrases, including both domain-specific and commonly
used phrases. Step 2 then filters the list of concepts identified inthe project artifacts to remove general concepts (e.g., data struc-ture,userinterface).Theremainingconceptsbecomethetargetsof our explanations ‚Äì first for individual artifacts, and second as
part of the trace link explanations. In step 3, we retrieve a domain
corpus of sentences containing these concepts, exploring two tech-
niquesbasedontop-downfilteringofalargedomaincorpus,andbottom-upsearch,drivenbytheproject-specificconcepts.Thispro-
duces a large corpus of sentences ‚Äì each of which includes at least
one targeted domain-specific concept. Step 4 applies a variety of
NLPtechniquestoexpandacronyms,generatedefinitions,discover
context, and to build relation triplets ‚Äì all of which are needed
intheexplanation.InStep5,webuildamachinelearningquality
control modelwhich automatically filters non-relevantsentences
to improve the accuracy of our explanations. Finally, in Step 6,
theseexplanatoryelementsarevisualizedandpresentedtotheuser
within the explanation interface.
3 PROJECT DOMAINS: DATASETS
Throughout the remainder of this paper we focus explanations
and experimental analysis on three target software engineering
domainsofelectronichealth-care,aspacetelescope,andpositive
train control (cf. Table 1). The domains were selected for their
diversity, availability of project artifacts, and because each one rep-
resented a technical domain with specific terminology and jargon.
Twodatasetsarepubliclyavailable,whilstoneisproprietaryand
provided by our industrial collaborators.
‚Ä¢CCHIT is from the domain of electronic health-care records
(EHR)andincludestracelinksbetween117requirementsfrom
theUSVeteran‚ÄôsWorldVistahealthcaresystem(e.g.,‚ÄòThesystem
shallallowevent-delaycapabilityforpre-admission,discharge,
and transfer orders‚Äô), and 588 regulatory requirements specified
by the USA Certification Commission for Health Information
Technology(CCHIT)(e.g.,‚ÄòThesystemshallprovidetheability
tosendaqueryformedicationhistorytoPBMorpharmacyto
capture and display medication list from the EHR‚Äô).
‚Ä¢CM1includes 54 low-level requirements for a NASA spacecraft
telescope (e.g., ‚ÄòThe TMALI CSC serves as an intermediate man-
ager of EVENT data supplied by the DCI Driver...‚Äô), traced to
23higherlevelones(e.g.,‚ÄòTheDPU-TMALIshallbecapableof
makingdataavailablefromtheDCItoDPU-DPA.DPU-TMALI
will populate a ring buffer...‚Äô). This dataset is quite small and the
project contains obscure technical jargon with limited online
documentation.
‚Ä¢PTCisfromthedomainofPositiveTrainControlandisprovided
by our industry collaborator. It traces 263 subsystem require-
ments to 964 system requirements. We cannot provide examples
due to the proprietary nature of this dataset.
4 MINING EXPLANATION ELEMENTS
The concept detection step is designed to extract high-quality
phrases, representing key domain concepts, from a text corpus.
Severalresearchershaveshownthebenefitsofaphrase-basedap-
proach based on information retrieval, taxonomy construction and
1035
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:20:26 UTC from IEEE Xplore.  Restrictions apply. ICSE ‚Äô22, May 21‚Äì29, 2022, Pittsburgh, PA, USA Liu et al.
topic modeling [ 16,20,21,52]. For example, Liu et al. [ 35]p r o -
posedatechniquethatintegratesphraseminingtechniqueswith
phrasal segmentation, and argued that their approach outperforms
many other approaches. It starts by identifying the most common
n-gramsandthenappliesqualitycriteriatoremovelow-quality,less
commonconcepts;butindoingsoitremovespotentiallyobscure
technical phrases that are of particular relevance for explaining
hard-to-understand trace links. For example, the previously dis-
cussed‚ÄòAckermannDriveStampedmessage‚Äôwouldbeunlikelyto
survivethefilteringprocess.Theapproachrequiresuserstoprovide
a small set of example phrases for training purposes.
Shang[52]proposed Autophrase whichusedphrasesfromsec-
tion titles in repositories such as wikipedia, to train their model to
extractconceptphrasescomposedofnouns,adjectives,andadverbs.
Asaprecursortotheworkwepresentinthispaper,weconductedan
initialseriesofexperimentsusingAutophrasetrainedonWikipedia
toproduceahugeconceptlistcoveringawiderangeofdomains.
However,wefoundthatAutophrasedidnotperformwellinourtar-
geted software engineering domains, primarily because Wikipedia
lackedsufficienttrainingdataforourdomains.Autophrasenotonly
overlooked important concepts but also extracted phrases with in-
correct grammar and/or redundant adjectives. We concluded that a
key obstacle in using ML based models to identify core concepts
for trace link explanations is the lack of sufficient training data for
specific software engineering project domains.
4.1 Adopting a Dependency Analysis Approach
Previous research has noted the importance of noun-phrases inthecreation andcomprehension oftracelinks[
42,60,61].Given
theissuesweencounteredinextractingmeaningfuldomaincon-
ceptsusingMLtechniques,coupledwiththeimportanceofnoun
phrases, we opted to leverage Stanford Dependencies (SD) analysis
[13,14] and focused upon detecting meaningful noun phrases as
domainconcepts.StanfordDependencyanalysisidentifiesdirect
relationsbetweentokenswithinasentencebycategorizingtheir
relationships into pre-defined types. This technique is a rule-based
approachbuiltusingapre-trainedphrasestructuregrammarparser.
The‚Äúcompound‚ÄùrelationsinSDrefertoanouncompoundmodifier
of an NP POS-tag that is used to annotate the head noun in a noun
phrase[15].AsillustratedinFig.2,weusethistypeofdependency
to determine the boundary of a noun phrase ‚Äì in this example,
applied to a sentence from the EHR domain.
4.2 Filtering out General Concepts
Analyzing project artifacts in this way tends to produce a large
number of concept phrases ‚Äì some of which are domain specific
phrases, worthy of explanation, while others are commonly occur-ring phrases with well-understood meaning. As we do not wish to
clutterourexplanationswithsuperfluousinformation,wereduce
theconceptlisttoincludeonlydomain-specificones.Weachieve
this through generating a black list of general concepts. This is
achievedbyapplyingtheStanfordDependencyanalysistoamas-sive domain-independent corpus,identifying themost commonly
occurringconcepts,andstoringthemastheblacklist.Forpurposes
of this study, we used the UMBC webBase corpus [ 27], which was
built using web-scraping in 2007 by the Stanford WebBase project.The dataset contains English paragraphs with over three billion
words. It is 13GB in compressed tar file format and is 48G when
uncompressed.Afterapplyingtheconceptdetectiononthiscorpus
we obtained 2,614,601 noun concepts. We ranked concepts by their
frequency, included concepts that appeared more than 1,000 times
in our black list. The black list represented the top 39,504 ( 1.5%) of
the detectedconcepts. Foreach of ourthree datasets,we removed
any concept found in the black list. Examples of project-specific
concepts as well as blacklisted ones are listed in Table 2.
4.3 Constructing a Concept Domain Corpus
The next step in our pipeline focuses on building a domain cor-
pusinwhicheachsentenceincludesatleastoneproject-relevant
domain concept. Sentences are mined from either an existing gen-
eral corpus or through searching the internet for relevant text.However, we observed a huge discrepancy in the amount of text
available for different domains, and this likely accounts for the dif-
ferent outcomes we report later in this paper for each of our three
projects. Popular domains, such as biomedical and finance, have
numerousrelatedwhitepapersandwebsitesdescribingthedomain,
and sometimes even a textually rich, well-organized corpus (e.g.
NCBIdiseasecorpus[ 19],ReutersCorpora[ 31]),collatedbydomain
experts.However, manysoftware projectshaveno previouslycol-
latedtextcorpus,andfurthermore,somerelativelyobscuredomains
have very limited web presence. To address these dual problems,
weapplyautomatedcorpuscollectiontechniquestobuildadomain
corpusforeachtargetedprojectwhilstensuringthatretrieveddocu-
ments have sufficient affinity to the targeted project. Our objective
is to mine a focused corpus covering all concepts in the target
project, and we explored both ‚Äútop-down‚Äù and ‚Äúbottom-up‚Äù corpus
collection strategies.
4.3.1 Top-downapproach: Thetop-downapproachstartsfroma
relatively large corpus and operates downwards to identify and
retainrelevanttext,whilsteliminatedallotherparts.Thecorpus
documentsarefirsttokenizedintosentences,andthentheKnuth-
Morris-Pratt(KMP) [ 48]string matchingalgorithmis usedtoeffi-
ciently examine whether the sentence containsat least one ofthe
identifiedprojectconcepts.Matchingsentencesarethenmapped
totheirassociatedconcepts.WeutilizedtheArXivrepository[ 2]
Table 2: Examples of Project and Blacklisted Concepts
CCHIT CM1
Patient Health Information EEPROM filesystemHL7 / ASTM Continuity X-ray sensitive CCD imagerHIPAA Risk Assessment DPU Task MonitoringCardiovascular Tests FSW TasksNCPDP Script DCI Error Interrupt
PTC BlackList
Wayside Data User Interface
EMP Protocol Team Manager
OBU Transitions Family Health
Class C Protocol Network Operator
Train Control Functions Useful Results
1036
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:20:26 UTC from IEEE Xplore.  Restrictions apply. Generating and Visualizing Trace Link Explanations ICSE ‚Äô22, May 21‚Äì29, 2022, Pittsburgh, PA, USA
(a) Three criteria for a concept definition.
(b) Handle the transformed sentence with a clause
Figure 2: An example of using using Stanford Dependency based rules for concept definition and context extraction. This
approach is more robust than pattern matching on complex sentences
whichincludesabstractsandthefull-textofacademicpapersacross
physics,computerscience,biologyandfourotherlargedomains
[1], and then downloaded 248GB of plain text data through an API
provided by ArXiv for this purpose.
4.3.2 Bottom-upapproach: Thebottom-upapproachstartswith
anemptydomaincorpusandthengraduallyaddsdatabysearch-
ingexternalresourcesusingapublicsearchengine(i.e.,Bingfor
thisstudy). Searchqueries areformulatedfrom thepreviouslyex-
tracted project concepts, and retrieved websites are scraped to find
sentencescontainingthetargetedconcept.Weinformallyexperi-
mentedwith severalquerytemplatesand foundthatthe template
‚Äúwhat is inbody: <ùëêùëúùëõùëêùëíùëùùë° >in<ùëëùëúùëöùëéùëñùëõ ùëõùëéùëöùëí >‚Äù returned the
most consistently relevant results using the Bing search engine,
where <ùëêùëúùëõùëêùëíùëùùë° >referstoaprojectconceptthathasbeenauto-
matically detected and <ùëëùëúùëöùëéùëñùëõ ùëõùëéùëöùëí >(i.e., Electronic Health
Record, Positive Train Control, and NASA) helps the search en-
gine to narrow the scope of the search and to disambiguate similar
concepts across different domains. Once results were returned, we
apply the same technique as the top-down approach to remove
sentences that do not contain at least one project concept.
4.4 Extracting Explanation Elements
WethenapplyaseriesofNLPtechniquestoextractthefollowing
descriptions and definitions from the constructed corpus.
4.4.1 Acronym descriptions: We define a concept as an acronym if
allthealphabeticcharactersintheconceptareupper-case.Wethen
utilizetheSchwartz-Hearstalgorithm[ 50]tominetheacronyms
from the collected corpus. This algorithm leverages pattern match-
ingandheuristicrules(e.g.phraselength)todetectthelongand
short forms of the acronyms and returns them as mapped pairs. As
reportedbytheauthors,theSchwartz-Hearstalgorithmachieved82%recallatprecisionof96%whenappliedtothebiomedicaldo-
main, outperforming methods previously proposed by Chang et al.
[10] and Pustejovsky et al. [47].
4.4.2 DefinitionsandContext. Giventhecorpusofconcept-specific
sentences,weutilizeStanfordDependenciesheuristicrulestoex-
tractdefinitionsandcontextdescriptions.Forcontextextraction,
we first check whether a concept is the nominal subject of a sen-
tenceandisconnectedtoanotherwordviaa‚Äúnsubj‚Äùdependency.
Anominalsubjectisanounphrasewhichisthesyntacticsubject
ofaclause[ 14],andthereforeapplyingthisruleensuresthatthe
sentence focuses on the target concept. To identify definitions, wealso check the verb connected to the target concept. As the depen-
denciesformadirectedrelationgraphwelocatetheassociatedverbbysimplysearchingforverbsthatareatmosttwo-hopsawayfromthethetargetconcepts,whilesimultaneouslyconstrainingpathsto
containonly‚Äúnsubj‚Äò‚Äôand‚Äúcop‚Äù(copula)dependencies,wherethe
‚Äúcop‚Äùdependencyreferstoarelationbetweenacopularverbandits
complements.Asdefinitionsusuallyfollowasetofknownpatterns,
such as "<concept> is/are/do something", we only select the sen-
tences whose verbs are either ‚Äúis‚Äù, ‚Äúare‚Äù, or with pos tags of ‚ÄúVBZ‚Äù
(referringtopresenttenseverbs).Weobservedthedependencies-
based approach to be more robust for handling complex sentences
than a simple pattern matching approach, as illustrated in Fig. 2
whichshowshowrulescanbeusedtoextractadefinitionfor‚ÄúCCD‚Äù
from sentences with different levels of complexity.
4.4.3 Filtering non-domain acronyms, definitions, and descriptions:
Theautomated extractionmethodsinevitably introduceimprecise
resultsbyincludingsentencesandacronymsoutsidethescopeof
thetargetprojectdomains.Toaddressthisissue,wedevelopeda
deep learning topic model and leveraged it as a binary classifier to
identifywhetheranacronym‚Äôslongname,adefinition,oracontextsentencebelongstothetargetprojectornot.Wetrainedthemodel
in a weakly supervised manner by utilizing actual artifacts (i.e.,
requirements,designdefinitions)fromthetargetprojectaspositive
1037
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:20:26 UTC from IEEE Xplore.  Restrictions apply. ICSE ‚Äô22, May 21‚Äì29, 2022, Pittsburgh, PA, USA Liu et al.
examplesandsentencesfromothersources(e.g.artifactsfromother
projects),as negative examples. We used these example to train the
model to identify sentences belonging to each target project.
The trained model accepts a tokenized candidate sentence as
input,andleveragesaBERT-basedLanguageModel(LM)toanalyze
the semantic meaning of the tokenized sentence. It uses a small
Multi-layerPerceptron(MLP)topredictalikelihoodscorebetween
0 and 1, representing the affinity between each sentence and the
targetedprojectdomain.Inthisstudy,weusedSciBERT[ 9]asour
languagemodel, whichhas beenpre-trained withmassivepapers
in difference science domains.Based on initial observations of the
results, wefiltered out allsentences scoringless than 0.5.Further,
werankedallremainingdefinitions,contextualinformation,and
acronymexplanationsaccordingtotheirscores,andineachcase,
selected the one with the highest score. We evaluated the accuracy
achievedwithandwithoutthisfilteringsteptotestitsutilityonthe
entirecorpusofretrievedsentencesforallthreeprojectsandfound
that accuracy improved from 63.33% to 86.7% for the bottom-upapproach and from 50.94% to 66.04% for the top-down approach
thereby demonstratingthe importance of this part of the pipeline.
4.4.4 RelationDiscovery: Thepreviousstepshavefocusedonde-
scribing concepts found in each individual artifact. In this step we
redirectourattentiontodescribingthelinkitselfbyapplyingthree
techniques for identifying relations between concepts. First, we
extract and formulate relations between concepts by exploring the
simple <Subject, Verb, Object >grammar in the corpus we have col-
lected, by leveraging Stanford dependencies that incorporate verb
tokens. More precisely, we use the ‚Äúnsubj‚Äù and ‚Äúxsubj‚Äù tags to find
thesubjectoftheverband‚Äúobj‚Äùdependenciestoidentifytheobject
ofthe verb,andonlyaccept tripletsforwhichthe verbrepresents
a hierarchical or equivalency relationship (e.g., includes), as this
impliesaparent-childrelationwhichcanbeusedtobuildmeaning-
fulcross-artifactrelations.Forexample,thetriplet <navigational
information, includes, operational hazards >could help us to un-
derstand why a requirement stating that ‚ÄúThe OBU shall transmit
navigational information tothebackoffice‚Äùislinkedtothederived
requirementthat‚ÄùTheWIUshalldetect operational hazards .‚Äù We
createdasetofeightseedverbsrepresentinghierarchicalrelations,
and then expanded this set by retrieving four additional synonyms
fromWordNet[ 44].Wethencombinealloftheretrievedtriplets
intoaknowledgegraph,inwhichthesubjectsandobjectsofthetriplet relations are used as vertices, and their inter-connecting
verbs are used as edges. Given two concepts distributed across a
pair of linked source and target artifacts, we use Dijkstra‚Äôs Algo-
rithm[18]tofindtheshortestpathbetweeneachcandidateconcept
pair.Thepath,includingitsnodesandedges,constitutesapotential
explanation for an underlying trace link.
In addition, we consider two concepts as equivalent when their
lemmatized forms are identical or when one concept is a sub-sequence of another (e.g. ‚ÄòTMALI‚Äô versus ‚ÄòTMALI event queue‚Äô),in which case we consider the shorter concept to be a semantic
abstraction of the longer one.
5 QUANTITATIVE ANALYSIS
In our first set of experiments, we investigate the potential use-fulness of artifact and trace link explanations by evaluating theTable 3: Number of acronyms exist in projects, and the pre-
cision and recall score for explaining these acronyms
Top-down Bottom-up
Acronym Precision Recall Precision Recall
CCHIT 109 51.79% 26.61% 100.00% 11.01%
CM1 46 0.00% 0.00% 0.00% 0.00%
PTC 318 27.54% 8.72% 70.00% 2.20%
correctness of the generated explanation elements and the percent-
age of artifacts and trace links for which associated elements were
mined.Foreachexplanationelementandeachofthethreeprojects
we addressed the following research questions:
RQ1:What percentage of the generated explanation elements are
correct with respect to the project domain?
RQ2:What percentage of the identified elements (i.e, acronyms or
domainconcepts inartifacts)have correspondingcorrectexplana-
tory elements for use in trace link explanations?
To answer these questions, three researchers evaluated the cor-
rectness of the generated acronyms, definitions, and context de-
scriptions. In some cases, our domain knowledge was sufficient
todirectlydeterminewhetheraconceptwas correct; however, in
othercases,werevieweddocumentationmanualsandwhitepapers
to discover or confirm the correct meaning of the concept.
5.1 Acronym Evaluation
Table3reportsresultsforretrievingacronymdescriptionsforall
three projects. The top-down approach generated descriptions for
57,1,and69acronymsatprecisionsof~52%,0%,and~28%forfor
CCHIT, CM1, and PTC respectively. This compares to only 15 and
11 acronym descriptions at 100% and ~64% precision for CCHIT
andPTC, andno acronymsfound forCM1. Thereare twoparticu-
larly notable observations. First, the bottom-up approach returned
fewer,butmoreaccurateresults.Thisapproachlikelyperformed
better because search queriesincluded more domain-specific con-
text than the top-down approach. Second, the pipeline completely
failed to retrieve any correct acronym descriptions for the CM1
project. While early phases of the pipeline produced 14 acronyms
the quality filter correctly eliminated 13 of these as the definitions
came from different domains. There are several primary reasons
forthisfailure.First,theacronymsinCM1‚Äôsdesignspecification
refer to very low-level architectural components; second, the CM1
domain contains more technical jargon than either CCHIT or PTC;
andthird, thedomain ofinterstellarsatellites hasfar feweronline
descriptions within white papers, websites, or other documents.
Forall ofthesereasons, theacronym expansionfailedin theCM1
project but produced useful results for CCHIT and PTC. These
results contrast clearly with prior claims that the Schwartz-Hearst
[50] algorithm returned 96% accuracy; however those prior results
focused on the mechanisms for extracting acronym descriptions
from a document in which correct descriptions were available.
5.2 Definition and Context Evaluation
Tables 4a and 4b report results from performing the manual evalu-
ation for top ranked definitions and contexts for each concept. The
1038
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:20:26 UTC from IEEE Xplore.  Restrictions apply. Generating and Visualizing Trace Link Explanations ICSE ‚Äô22, May 21‚Äì29, 2022, Pittsburgh, PA, USA
Table4:Coverageandaccuracyoftheextractedsentencesas
concept definition and context
Top-down Bottom-up
Extr. Correct Acc. Extr. Correct Acc.
CCHIT 15 12 80.00% 12 11 91.67%
CM1 1 1 100.00% 3 3 100.00%
PTC 16 7 43.75% 1 1 100.00%
Total 32 20 16 15
(a)Numberandaccuracyofprojectconceptswhichhavedefini-
tions extracted from corpus
Top-down Bottom-up
Extr. Correct Acc. Extr. Correct Acc.
CCHIT 85 55 64.71% 44 39 86.67%
CM1 7 2 28.57% 8 7 87.50%
PTC 88 17 19.32% 21 19 90.48%
Total 180 74 73 65
(b)Numberandaccuracyofprojectconceptswhichhavecontext
extracted from corpus
top-downapproachretrieved15,1and16definitionswithaccura-
ciesof80%,100%,and~44%forCCHIT,CM1,andPTCrespectively,
whilethebottom-upapproachretrieved12,3,and1definitionsat
accuracies of ~92%, 100%, and 100% respectively. In the case of con-
textual descriptions the top-down approach retrieved 85, 7, and 88
definitionsataccuraciesofapproximately65%,29%,and19%,while
the bottom-up approach retrieved 44, 8, and 21 definitions at accu-
racies of approximately 87%, 88%, and 90%. Overall, the bottom-up
approachgenerallyretrievedfewerbutmoreaccurateresults.Again
we observe low retrieval rates for CM1, with only one definition
inthetop-downapproachandonlythreeinbottom-up;however,
these were retrieved at 100% accuracy. The bottom-up approach
also underperformed for the PTC dataset, only finding one correct
acronym definition. Despite sending 1,780 unique concept queries
to the search engine, only 4.3% of them returned sentences with
directconceptmatches.Thiscomparedto16.2%and36.2%direct
matches for CCHIT and CM1. This coverage problem has been ob-
served by previous researchers. For example,Zeng et al., reported
that onlyabout half ofthe 30,000 termsfound in theMESH Medi-
cal taxonomy, appeared anywhere in the PubMed database of 30
million paper abstracts [ 58]. False positives could likely be further
reduced by providing a more diverse set of training examples.
5.3 Concept Relation Evaluation
To evaluate concept relations, we examined the generated paths
for coverage and correctness. Unfortunately, the knowledge graph
sufferedfromapathsparsityissueandthereforefewmeaningful
pathswerefoundbetweenconceptsinpairedartifacts.Thespar-
sitywasprimarilycausedbylimitingverbstothoserepresenting
hierarchicalrelationships,therebysignificantlyreducingthesizeof
the triplet set. While accepting a broader set of verbs, creates a far
largersetoftripletsandmanymorecross-artifactrelations,thema-
jority of these paths do not produce meaningful explanations. We
thereforeoptedtofavorprecisionoverrecallandexcludedmulti-
hop paths generated from the knowledge graph in our explanationinterface.However,theheuristicrulesforequivalenciesandsub-
sequences, along with the 1-hop paths retrieved several interesting
explanations such as <ùêºùê∂ùê∑ ‚àí9,ùë¢ùë†ùëíùëë ùëìùëúùëü,ùëèùëñùëôùëôùëñùëõùëî >, resulting in 9,
26 and 96 concept relation explanations for the three projects.
5.4 Leveraging available Project Glossaries
While our bottom-up approach returned fairly accurate results,
there were a large number of artifacts for which no supporting
definitions were retrieved. We therefore decided to explore the
combinationofboththebottom-upandtop-downtechniquealong-
side definitionsprovided byproject-specificglossaries, and subse-
quently identified and retrieved a glossary for each project. The
CCHITprojecttracestherequirementsinWordVistaEHRsystem
to the CCHIT regulatory requirements, and we used a glossary
fromtheWordVistaproject[ 5]containing352acronymswiththeir
expanded names, and 451 concepts with associated definitions and
contextualexamples.TheprovidedCM1glossary[ 3],provideslong
names for 64 acronyms used in the project. Finally, we derived the
PTCglossaryfromthearchitecturespecificationdocument[ 4]of
Interoperable Train Control Network (ITCnet) released by the Me-
teorcomm company and containing 44 acronyms with long names
and 69 concepts with definitions. We checked for overlap of defini-
tions. Results reported in Fig. 3 bottom-up (green), top-down (red)
approaches, and glossaries (purple), show that different projectconcepts were provided by each of the three techniques. Addingthe definitions generated by the bottom-up approach to each of
theexistingglossariesincreasedexplanationelementsby~195%forCCHIT,~33%forCM1,and150%forPTC.Ofcourseactualgainsare
highlydependentuponthecompletenessofthebaselineglossary.
Given the benefits of combining glossary and generated data, and
theaccuracyofthebottom-upapproach,weusedthesetwodata
sourcesintheuserstudydescribedinthenextsectionofthispaper.
6 TRACE LINK EXPLANATION INTERFACE
We designed and developed an explanation interface and used it to
address the following two research questions:
RQ4:Does the trace link explanation interface help users to eval-
uate the similarity between two linked artifacts more effectively
than without the explanation?RQ5:
Which aspects of the explanation interface are most helpful
to users?
6.1 Designing the Explanation Interface
WeadoptedtheexplanationdesignframeworkintroducedbyAnuyah
etal.[8]toguidethedesignandevaluationofourexplanationinter-
face, and engaged in a series of three participatory design sessions
thatincludedsixmembersofourteamwithexpertiseinUX-Design
and/orSoftwareEngineering.ThroughthesesessionsweidentifiedSoftwareEngineeringtasks,suchas impactanalysis andcompliance
verification thatwouldutilizetraceability,andidentifiedmultiple
typesofusers.Ofthese,wefocusedprimarilyonpeoplewithout
domain expertise (e.g., project newcomers) and people perform-ing tasks across skill boundaries (e.g., a business analyst tracingfrom requirements to low-level models or code) where they may
be exposed to terminology they are not familiar with.
1039
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:20:26 UTC from IEEE Xplore.  Restrictions apply. ICSE ‚Äô22, May 21‚Äì29, 2022, Pittsburgh, PA, USA Liu et al.
(a) CCHIT
 (b) CM1
 (c) PTC
Figure 3: Number of the project concepts explained by project glossaries as well as our top-down and bottom-up approaches
We then identified a set of tasks that a user would perform
using the interface. These included identifying artifact types, ana-
lyzingartifactcontent,judgingwhethertwoartifactsarerelated,
and providing feedback on the correctness of the link. We then
brainstormed different ways of presenting the link explanationsin support of these tasks, and created a set of low-fidelity proto-
types. Each prototype was explored to understand its strengths
andweaknesseswithrespecttosupportingthepotentialusertasksandleveraginghumanperceptualcapabilities[
40,45].Thisactivity
informed several design decisions, each of which is reflected in the
UI presented in 5.
First, our relation extraction and acronym expansion process
resultsinsemanticrelationsamonglinkedconcepts.Connection
marksusealinetoshowapairwiserelationshipbetweentwoitemsandthusareappropriateforshowingtherelationshipbetweentwo
concepts [ 45]. Second, concepts themselves are nominal types and
colorencodingsareparticularlyeffectivefornominaldata.Addi-
tionally,usingthesamecolorsforthesameconceptsacrossthetwo
artifacts creates a perceptual grouping [ 45]. Finally, every concept
isaccompaniedbyaquantitative‚Äúimportancevalue‚Äù.Sizeencod-
ings,whilenotidealforshowingspecificvalues,areappropriatefor
comparison purposes and thus we chose to encode the importance
of a concept using font size [45].
Thecombinedvisualelementsareintendedtoprovideanoverview
ofthetotalrelatedelementsbetweenthetwoartifacts,howthey
areconnected,andhowstrongtheconnectionsare.Weadoptan
overview+detailapproach[ 55]wheretheuserisprovidedanat-a-
glanceoverviewandcanobtaindetailsthroughmouse-overinterac-tionsontheconceptnodesorconceptwordsthemselvestogetdefi-nitions,orontheconnectionedgestoobtainasemanticrelationship
description. We iterated through medium-fidelity mockups created
inFigma[ 17]toexploretheeffectivenessofthevisualencodings
andinteraction strategies.Ourfinal prototypewas builtasa fully
functionalwebapplicationforuseinourevaluationandaninter-
activedemoisavailableathttps://trace-exp-study.github.io/pages.
6.2 Study Design
We conducted a controlled user study to evaluate the effectiveness
ofourtracelinkexplanationinterface.Wedesignedourstudytohave two treatment conditions; showing explanations for the links
(TC 1) versus hiding explanations (TC 2). Our goal was to examine
theeffect thatreceiving explanationswould havefor identifying
correct and incorrect trace links.
6.2.1 Participants. We recruited eight participants with the re-
quirement of having some prior experience working on a software
engineering project. We did not exclude participants accordingto the number of years of experience or their specific role in a
project. All of our participants are currently graduate students of a
universityintheUnitedStatesandwererecruitedthroughemail.
Participants were tasked to perform trace vetting on 30 links‚Äìof
which 18 were correct links, while the remaining 12 were incor-
rect. Each participant evaluated 10 links from each of the three
projects. Furthermore, 43% of definitions and acronyms were from
theprojectglossaries,51%weregenerateddynamicallyusingthe
bottom-up approach, and 6% were found in both.
Figure 4: Two experiment groups were formed by samplingan equal number of trace links from each of the threeprojects. For each link we created a version with an expla-nationandonewithoutanexplanation,andforagivenlink,Group 1 received the explanation, whilst Group 2 did not.Each group received half the links with explanations andhalf without, meaning that the two groups received oppo-sitetreatments.Wethenassignedanequalnumberofstudyparticipantstoeachgroup.Theorderoflinkswasrandomlyshuffled for each participant, who was then asked to evalu-ate the correctness of each presented link.
1040
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:20:26 UTC from IEEE Xplore.  Restrictions apply. Generating and Visualizing Trace Link Explanations ICSE ‚Äô22, May 21‚Äì29, 2022, Pittsburgh, PA, USA
Figure 5: Snapshot of the trace link explanation user interface. In part 1, we used a text panel to display concept level expla-
nations including long acronym names, definitions, context and inter concept relationships based upon the current cursor
(roll-over) position. In part 2, we visualize the links by highlighting the concepts as they appear in the source and target arti-
facts.Inpart3,weenlargetheconceptsbasedontheirimportanceandunderscoretheonesthathaveavailableexplanations.
6.2.2 Procedure. Webeganthestudybybriefingparticipantsabout
the task that they would perform. Next, we directed participants
to our web-based explanation interface using the URL that we pro-
vided. Participants were asked to vet links as correct, incorrect, or
don‚Äôtknow(iftheywereunabletoconfidentlymakeadetermina-
tion). Participants completed all of the vetting tasks in a random
order.Inthiscase,linksfromthethreedomainsdiscussedinsec-
tion3werepresentedrandomly.Duringthesession,participants
wereaskedtoverballyexplaintheirdecisiononvettingthelinks,
enablingustocollectbothqualitativeandquantitativedata.The
study took about 30-45 minutes to complete.
We exposed each participant to the two treatment conditions,
with the aim of understanding the extent to which explanations
can guide them in differentiating between correct and incorrectlinks. To address the potential bias of learning effects, we used
abetween-subjectsexperimentaldesignputtingeachparticipant
in one of two groups, such that participants in one group were
exposedtotheoppositetreatmentconditionforeachofthelinks,
than participants in the second group (see Figure 4). Further, the
orderinwhichlinkswerepresentedtouserswasrandomizedfor
each person.
6.3 Results
We examined our data using a combination of quantitative and
qualitativemeasurestounderstandtheimpactofourexplanation
design on trace link vetting.
6.3.1 Quantitative result outcomes. Overall, our results show that
there was a significant improvement in accurately vetting links
when participants were provided with explanations ( ùúå=.01994,ùëù< .05.,ùëë=0.820852). This finding indicates that the explanations
helpedtoguideourparticipantsforidentifyingcorrectandincorrect
tracelinks.However,whenweexaminedthedifferencesforeach
domainindependently,weobservedthattherewerenostatistical
improvements ( ùúåùëÉùëáùê∂=.551677, ùúåùê∂ùëÄ1=.18026,ùúåùê∂ùê∂ùêªùêºùëá=.21684). We
attribute this finding to the fact that the data collected in each
domain was too small to arrive at a statistical conclusion.
6.3.2 Qualitative insights. All participants responded positively to
the explanations and expressed how helpful they were for the vet-
tingtasks.Noneoftheparticipantswereexpertsinthedomainsun-
der study and were therefore unfamiliar with many of the domain-
specific technical terms and acronyms in the artifact content. This
resulted in many participants selecting ‚Äúdon‚Äôt know‚Äù when they
werenotgivenexplanations.P4stated, ‚ÄúIdon‚Äôtknowwhatthisterm
means. I want to know what things mean.‚Äù P6 noted, ‚ÄúThe expla-
nations helped me to not just easily identify keywords, but better
understand what they mean.‚Äù
Participants also noted that receiving explanations reduced the
mentaleffortittookforthemtocomprehendthecontentofthearti-facts,especiallywhenthelengthwaslong.Mostoftheparticipantsstruggled to understand the content meaning when presented with
long artifacts without explanations, often resulting in the ‚Äúdon‚Äôt
know‚Äù response. P8, for example, when presented with a long arti-
factdescription,stated ‚ÄúThisartifactrequiresalotofthinking.There
is too much text and too much function names and abbreviations
without explanation.‚Äù
Several participants noted that the explanations helped them
easily find common terms or keywords in the artifact content. For
links without explanations, however, participants often resorted
1041
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:20:26 UTC from IEEE Xplore.  Restrictions apply. ICSE ‚Äô22, May 21‚Äì29, 2022, Pittsburgh, PA, USA Liu et al.
to using the find feature on their browser to identify common
terms in the artifact content. Being able to easily identify these
commontermsintheexplanationsmadeitpossibleforparticipants
to more quickly make a decision. P8 noted that, ‚ÄúBecause drug
interactions was highlighted, I identified what was specifically not to
beincluded.Contexthelpedmeknowwhatthesystemisexpecting.‚Äù P8
also commented about finding keywords in large content artifacts,
‚ÄúIf the text is too huge, highlighting the keywords was helpful. I could
easily jump to the important part.‚Äù While we did not collect timing
information for participants tasks, anecdotally, participants made
decisions more quickly when presented with the explanation.
Wealsoobservedthatthesemanticconnectionsbetweenartifact
concepts helped our participants in identifying if artifacts were
linked. While some participants struggled to determine links when
the concept terms did not exactly match, they noted that seeing
the relationship path made it possible for them to not only see that
artifacts were linked, to examine the relationship more directly to
buildconfidenceintheirunderstanding.Forexample,P3noted,‚ÄúI
caneasilyseethatthesearenotsemanticallythesame.‚Äù P7foundit
‚Äú...helpful to see overlapping keywords and phrases.‚Äù
Whilethefeedbackwasoverwhelminglypositive,participants
alsonotedsomeareasforimprovement.Forinstance,theynoted
that some concept terms were too ambiguous to understand, even
with definitions. Some of them also mentioned that they needed
contextforfunctionsandotherartifactsthatwerereferencedinthe
artifactcontent.P7stated,‚ÄúIdon‚Äôtknowwhatcertificationisbeing
referred to here. I need some context.‚Äù
7 THREATS TO VALIDITY
Our study carries a few threats to validity. With respect to internal
validity,weevaluatedourapproachonthreedifferentsystemsfromdiverse domains for which golden answer sets defining correct and
incorrectlinks werealreadyprovided. Weuseddomain documen-
tation to evaluate the correctness of the generated definitions and
descriptions. With respect to external validity, we observed trends
across the datasets ‚Äì such as the observation that the bottom-upapproach returned more precise results than the top down one.
However, we cannot draw general conclusions based only on three
datasets. For example, while we have hypothesized that the rea-son for CM1‚Äôs underperformance is that it is a highly scientificsystem with jargon-filled project artifacts, and therefore general
domain documents failed to provide relevant concepts, we cannot
categorically support such generalizations at this time.
As with any NLP pipeline, we made numerous design decisions,
and whilst we justified our decisions, it is likely that different com-
binationsoftechniqueswouldreturndifferentresults.Finally,withrespecttoconstructvalidity,weusedmetricstoshowthedegreeof
coverage of the mined acronyms, definitions, and contextual expla-
nations;howev er,coveragedoesnotmeasureusefulness.Tothat
endweconductedasmalluserstudy,andwhilstourparticipants
weregraduatestudentsandnotcurrentlyworkinginindustry,they
served as reasonable proxies for project newcomers and other non
domainexpertsworkingonaproject.Neverthelesstomorerigor-
ously evaluate whether our approach is useful we need to apply it
with real project stakeholders in an actual project context.8 RELATED WORK
Inadditiontothepreviouslydescribedrelatedwork,webrieflysum-marizeothercloselyrelatedworkinconceptminingandgenerating
traceability rationales.
Concept and Relation Mining: Numerous researchers have focused
ontechniquesforminingconceptsandtheirrelationsfromtheweb.
Angeli et.al[ 6] proposed dependency analysis based method for
triplet relation mining in 2015. Our relation extraction approach is
basedonthesameideawhilemodifyandsimplifytheheuristicrules
to focus on the given noun phrases we detected in project artifacts.
Taxonomyexpansionalgorithms[ 51,53]focusonexpandinganex-
istingontologybyextractingnewconceptsfromlargeopencorpus
and leverage deep learning models to insert the concept into the
concepthierarchy.Thelinkexplanationtaskcanbenefitfromthese
methods when an initial domain concept is available. In the soft-
ware engineering domain, researchers have proposed or evaluated
techniques for ontology building in order to capture key project
concepts[ 23,32]however,theseapproacheswereusedinthetrace
link creation algorithms and not applied for link explanations.
TraceabilityRationales: HullandDickproposed RichTraceability
as a means ofexplicitly capturing satisfaction arguments between
requirements and design, thereby documenting the rationale fora link [
30]; however, performing this task manually is very time
consuming. Other researchers, such as Balasabrumanian et al. [ 49]
andZismanetal.[ 57]proposedspecifictraceabilitymeta-models
describing link semantics; however, while these techniques create
semantically typed links, they fail to explainthe purpose of indi-
vidual links. Guo et al., proposed a technique that utilized domain-
specific knowledge bases to support trace link generation [ 24,26],
andthenaugmentedtheknowledgebasewithrationalepatternstoprovideaninitialexplanationforthelink.However,theirapproachiscloselycoupledwithaheuristicapproachtotracelinkgeneration,
whereasourapproachisnotdependentuponaspecifictracingtech-
nique.Finally,afewresearchershaveexploredtheuseofontologies
to generate semantically meaningful trace links [8, 46].
9 CONCLUSION
In this paper we sought to extract acronym descriptions, defini-
tions, contextual examples, and cross-artifact relations by applying
anNLPpipelinetoprojectartifactsandlargegeneraldatacorpii.Our goal was to use the generated descriptions within an inter-
facetoexplainwhytwoartifactswereconnectedthroughatrace
link.GeneratingtracelinkexplanationsrepresentsarelativelynewresearchchallengedesignedtosupportrecentadvancesinDLtrace-
abilitymodelswhichhavesignificantlyimprovedtheaccuracyof
generated trace links, but lack clear explanations.
Ourquantitativeanalysisshowedthatthegenerateddescriptions
and definitions were quite precise and were able to augmentmanu-
ally created project glossaries. While we were not able to generate
definitionsforallidentifiedprojectconcepts,ouruserstudycon-
ductedwithnon-expertsacrossthreedifferentsoftwaredomains
demonstratedthattheexplanationsminedfromglossariesandaug-
mented with dynamically retrieved definitions and descriptions
aidedusersinevaluatingtracelinksdespitelackingexpertiseinthe
domain.Infutureworkwewillfocusonenhancedtechniquesfor
providing more comprehensive coverage of all domain concepts.
1042
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:20:26 UTC from IEEE Xplore.  Restrictions apply. Generating and Visualizing Trace Link Explanations ICSE ‚Äô22, May 21‚Äì29, 2022, Pittsburgh, PA, USA
We share all of the artifacts for CCHIT and CM1 datasets1and
the associated code2into the public domain to empower other
traceabilityresearcherstotakeupthechallengeofgeneratingcom-
plete,correct,andmeaningfultracelinkexplanationsinorderto
make DL-generated trace links more useful for a broader set of
stakeholders.
10 ACKNOWLEDGMENT
TheworkinthispaperisprimarilyfundedundertheUSANational
Science Foundation grant CCF-1901059.
REFERENCES
[1] [n.d.]. Arxiv domains. https://arxiv.org/
[2][n.d.]. Arxiv Repository, API for bulk data access. https://arxiv.org/help/bulk_
data
[3] [n.d.]. CM1 glossary. https://tinyurl.com/CM1Requirements[4]
[n.d.]. Positive Train Control (PTC) glossary. https://tinyurl.com/PTCGlossary
[5][n.d.]. World Vista Electronic Health Care System glossary. https://tinyurl.com/
WorldVistaGlossary
[6]Gabor Angeli, Melvin Jose Johnson Premkumar, and Christopher D Manning.
2015. Leveraging linguistic structure for open domain information extraction. In
Proceedings of the 53rd Annual Meeting of the Association for Computational Lin-
guisticsandthe7thInternationalJointConferenceonNaturalLanguageProcessing
(Volume 1: Long Papers). 344‚Äì354.
[7]Giuliano Antoniol, Gerardo Canfora, Gerardo Casazza, Andrea De Lucia, and
Ettore Merlo. 2002. Recovering Traceability Links between Code and Documen-
tation.IEEE Trans. Software Eng. 28, 10 (2002), 970‚Äì983. https://doi.org/10.1109/
TSE.2002.1041053
[8]Oghenemaro Anuyah, William Fine, and Ronald Metoyer. 2021. Design Decision
Framework for AI Explanations. Mensch und Computer 2021-Workshopband
(2021).
[9]Iz Beltagy, Kyle Lo, and Arman Cohan. 2019. Scibert: A pretrained language
model for scientific text. arXiv preprint arXiv:1903.10676 (2019).
[10]JeffreyT Chang,HinrichSch√ºtze,and RussBAltman. 2002. Creatingan online
dictionary of abbreviations from MEDLINE. Journal of the American Medical
Informatics Association 9, 6 (2002), 612‚Äì620.
[11]JaneCleland-Huang,AdamCzauderna,MarekGibiec,andJohnEmenecker.2010.
A machine learning approach for tracing regulatory codes to product specific
requirements.In Proceedingsofthe32ndACM/IEEEInternationalConferenceon
Software Engineering - Volume 1, ICSE 2010, Cape Town, South Africa, 1-8 May
2010, Jeff Kramer, Judith Bishop, Premkumar T. Devanbu, and Sebasti√°n Uchitel
(Eds.). ACM, 155‚Äì164. https://doi.org/10.1145/1806799.1806825
[12]Jane Cleland-Huang, Orlena Gotel, Jane Huffman Hayes, Patrick M√§der, and
Andrea Zisman. 2014. Software traceability: trends and future directions. In
Proceedings of the on Future of Software Engineering, FOSE 2014, Hyderabad, India,
May 31 - June 7, 2014 , James D. Herbsleb and Matthew B. Dwyer (Eds.). ACM,
55‚Äì69. https://doi.org/10.1145/2593882.2593891
[13]Marie-CatherineDeMarneffe,BillMacCartney,ChristopherDManning,etal .
2006. Generatingtypeddependencyparsesfromphrasestructureparses..In Lrec,
Vol. 6. 449‚Äì454.
[14]Marie-Catherine De Marneffe and Christopher D Manning. 2008. Stanford typed
dependencies manual. Technical Report. Technical report, Stanford University.
[15]Marie-Catherine De Marneffe and Christopher D Manning. 2008. Stanford typed
dependencies manual. Technical Report. Technical report, Stanford University.
[16]Paul Deane. 2005. A nonparametric method for extraction of candidate phrasal
terms. In Proceedings of the 43rd Annual Meeting of the Association for Computa-
tional Linguistics (ACL‚Äô05). 605‚Äì613.
[17]Figma Design. 2017. Figma: the collaborative interface design tool.(2017). Re-
trieved September 17 (2017), 2017.
[18]Edsger W Dijkstra. 1959. A note on two problems in connexion with graphs.
Numerische mathematik 1, 1 (1959), 269‚Äì271.
[19]Rezarta Islamaj Doƒüan, Robert Leaman, and Zhiyong Lu. 2014. NCBI diseasecorpus: a resource for disease name recognition and concept normalization.
Journal of biomedical informatics 47 (2014), 1‚Äì10.
[20]Ahmed El-Kishky, Yanglei Song, Chi Wang, Clare Voss, and Jiawei Han. 2014.
Scalable topical phrase mining from text corpora. arXiv preprint arXiv:1406.6312
(2014).
1https://zenodo.org/record/6040328
2Github:https://github.com/yliu26/TraceLinkExplanation
[21]Katerina Frantzi, Sophia Ananiadou, and Hideki Mima. 2000. Automatic recogni-
tion of multi-word terms:. the c-value/nc-value method. International journal on
digital libraries 3, 2 (2000), 115‚Äì130.[22] Jin Guo, JinghuiCheng, and JaneCleland-Huang. 2017. Semantically enhanced
softwaretraceabilityusingdeeplearningtechniques.In Proceedingsofthe39thIn-
ternationalConferenceonSoftwareEngineering,ICSE2017,BuenosAires,Argentina,
May 20-28, 2017, Sebasti√°n Uchitel, Alessandro Orso, and Martin P. Robillard
(Eds.). IEEE / ACM, 3‚Äì14. https://doi.org/10.1109/ICSE.2017.9
[23]Jin Guo and Jane Cleland-Huang. 2016. Ontology learning and its application in
software-intensiveprojects.In Proceedingsofthe38thInternationalConferenceon
SoftwareEngineering,ICSE2016,Austin,TX,USA,May14-22,2016-Companion
Volume, Laura K. Dillon, Willem Visser, and Laurie A. Williams (Eds.). ACM,
843‚Äì846. https://doi.org/10.1145/2889160.2889264
[24]Jin Guo, Jane Cleland-Huang, and Brian Berenbach. 2013. Foundations for an
expert system in domain-specific traceability. In 21st IEEE International Require-
ments Engineering Conference, RE 2013, Rio de Janeiro-RJ, Brazil, July 15-19, 2013.
42‚Äì51. https://doi.org/10.1109/RE.2013.6636704
[25]JinGuo,NatawutMonaikul,andJaneCleland-Huang.2015. Tracelinksexplained:
An automated approach for generating rationales. In 23rd IEEE International
RequirementsEngineeringConference,RE2015,Ottawa,ON,Canada,August24-28,2015,DidarZowghi,VincenzoGervasi,andDanielAmyot(Eds.).IEEEComputer
Society, 202‚Äì207. https://doi.org/10.1109/RE.2015.7320423
[26]JinGuo,NatawutMonaikul,CodyPlepel,andJaneCleland-Huang.2014. Towards
anintelligentdomain-specifictraceabilitysolution.In ACM/IEEEInternational
ConferenceonAutomatedSoftwareEngineering,ASE‚Äô14,Vasteras,Sweden-Sep-
tember15-19,2014,IvicaCrnkovic,MarshaChechik,andPaulGr√ºnbacher(Eds.).
ACM, 755‚Äì766. https://doi.org/10.1145/2642937.2642970
[27]Lushan Han, Abhay L Kashyap, Tim Finin, James Mayfield, and Jonathan Weese.
2013. UMBC_EBIQUITY-CORE:Semantic textualsimilarity systems.In Second
Joint Conference on Lexical and Computational Semantics (* SEM), Volume 1: Pro-
ceedings of the Main Conference and the Shared Task: Semantic Textual Similarity.
44‚Äì52.
[28]Jane Huffman Hayes and Alex Dekhtyar. 2005. Humans in the traceability loop:
can‚Äôt live with ‚Äôem, can‚Äôt live without ‚Äôem. In The 3rd International Workshop
on Traceability in Emerging Forms of Software Engineering, co-located with the
ASE 2005 Conference, TEFSE@ASE 2005, Long Beach, CA, USA, November 88, 2005,
Jonathan I. Maletic, Jane Cleland-Huang, Jane Huffman Hayes, and Giuliano
Antoniol (Eds.). ACM, 20‚Äì23. https://doi.org/10.1145/1107656.1107661
[29]JaneHuffmanHayes,AlexDekhtyar,andSenthilKarthikeyanSundaram.2006.
AdvancingCandidateLinkGenerationforRequirementsTracing:TheStudyof
Methods. IEEETrans.SoftwareEng. 32,1(2006),4‚Äì19. https://doi.org/10.1109/
TSE.2006.3
[30]ElizabethHull,KenJackson,andJeremyDick.2010. Requirementsengineering.
Springer Science & Business Media.
[31]David D Lewis, Yiming Yang, Tony Russell-Rose, and Fan Li. 2004. Rcv1: A
newbenchmarkcollectionfortextcategorizationresearch. Journalofmachine
learning research 5, Apr (2004), 361‚Äì397.
[32]Yonghua Li and Jane Cleland-Huang. 2013. Ontology-based trace retrieval. In
7thInternationalWorkshoponTraceabilityinEmergingFormsofSoftwareEngi-
neering,TEFSE2013,19May,2013,SanFrancisco,CA,USA,NanNiuandPatrick
M√§der(Eds.).IEEEComputerSociety,30‚Äì36. https://doi.org/10.1109/TEFSE.2013.
6620151
[33]Jinfeng Lin, Yalin Liu, and Jane Cleland-Huang. 2020. Supporting Program
ComprehensionthroughFastQueryresponseinLarge-ScaleSystems.In ICPC
‚Äô20:28thInternationalConferenceonProgramComprehension,Seoul,Republicof
Korea, July 13-15, 2020. ACM, 285‚Äì295. https://doi.org/10.1145/3387904.3389260
[34]Jinfeng Lin, Yalin Liu, Qingkai Zeng, Meng Jiang, and Jane Cleland-Huang. 2021.
Traceability Transformed: Generating more Accurate Links with Pre-Trained
BERTModels.In 43rdIEEE/ACMInternationalConferenceonSoftwareEngineering,
ICSE2021,Madrid,Spain,22-30May2021 .IEEE,324‚Äì335. https://doi.org/10.1109/
ICSE43902.2021.00040
[35]JialuLiu,JingboShang,ChiWang,XiangRen,andJiaweiHan.2015. Miningqual-
ity phrases frommassive text corpora. In Proceedings ofthe 2015 ACM SIGMOD
International Conference on Management of Data. 1729‚Äì1744.
[36]Yalin Liu, Jinfeng Lin, Qingkai Zeng, Meng Jiang, and Jane Cleland-Huang. 2020.
Towardssemanticallyguidedtraceability.In 2020IEEE28thInternationalRequire-
ments Engineering Conference (RE). IEEE, 328‚Äì333.
[37]SugandhaLohar,SorawitAmornborvornwong,AndreaZisman,andJaneCleland-
Huang. 2013. Improving traceaccuracy through data-driven configuration and
composition of tracing features. In Joint Meeting of the European Software En-
gineering Conference and the ACM SIGSOFT Symposium on the Foundations of
SoftwareEngineering,ESEC/FSE‚Äô13,SaintPetersburg,RussianFederation,August18-
26, 2013, Bertrand Meyer, Luciano Baresi, and Mira Mezini (Eds.). ACM, 378‚Äì388.
https://doi.org/10.1145/2491411.2491432
[38]Yi-Ju Lu and Cheng-Te Li. 2020. GCAN: Graph-aware co-attention networks for
explainable fake news detection on social media. arXiv preprint arXiv:2004.11648
(2020).
[39]Andrea De Lucia, Andrian Marcus, Rocco Oliveto, and Denys Poshyvanyk. 2012.
InformationRetrievalMethodsforAutomatedTraceabilityRecovery. In Software
andSystemsTraceability,JaneCleland-Huang,OllyGotel,andAndreaZisman
(Eds.). Springer, 71‚Äì98. https://doi.org/10.1007/978-1-4471-2239-5_4
1043
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:20:26 UTC from IEEE Xplore.  Restrictions apply. ICSE ‚Äô22, May 21‚Äì29, 2022, Pittsburgh, PA, USA Liu et al.
[40]Jock Mackinlay. 1986. Automating the design of graphical presentations of
relational information. Acm Transactions On Graphics (Tog) 5, 2 (1986), 110‚Äì141.
[41]PatrickM√§der andAlexanderEgyed. 2016. Dodevelopers benefitfrom require-
mentstraceabilitywhenevolvingandmaintainingasoftwaresystem?.In Software
Engineering 2016, Fachtagung des GI-Fachbereichs Softwaretechnik, 23.-26. Februar
2016, Wien, √ñsterreich (LNI, Vol. P-252), Jens Knoop and Uwe Zdun (Eds.). GI,
109‚Äì110. https://dl.gi.de/20.500.12116/748
[42]Anas Mahmoud and Nan Niu. 2015. On the Role of Semantics in Automated
RequirementsTracing. Requir.Eng. 20,3 (Sept.2015), 281‚Äì300. https://doi.org/
10.1007/s00766-013-0199-y
[43] SalomeMaro,Jan-PhilippStegh√∂fer,JaneHuffmanHayes,JaneCleland-Huang,
and Miroslaw Staron. 2018. Vetting Automatically Generated Trace Links: What
InformationisUsefultoHumanAnalysts?.In 26thIEEEInternationalRequirements
EngineeringConference,RE2018,Banff,AB,Canada,August20-24,2018,Guenther
Ruhe, Walid Maalej, and Daniel Amyot (Eds.). IEEE Computer Society, 52‚Äì63.
https://doi.org/10.1109/RE.2018.00-52
[44]George A Miller. 1995. WordNet: a lexical database for English. Commun. ACM
38, 11 (1995), 39‚Äì41.
[45] Tamara Munzner. 2014. Visualization analysis and design. CRC press.
[46]MShMurtazinaandTVAvdeenko.2019. Anontology-basedapproachtosupport
forrequirementstraceabilityinagiledevelopment. ProcediaComputerScience
150 (2019), 628‚Äì635.
[47]JamesPustejovsky,Jos√©Castano,BrentCochran,MaciejKotecki,andMichael
Morrell. 2001. Automatic extraction of acronym-meaning pairs from MEDLINE
databases. In MEDINFO 2001. IOS Press, 371‚Äì375.
[48]RobbiRahim,IskandarZulkarnain,andHendraJaya.2017. Areview:searchvisu-
alization with Knuth Morris Pratt algorithm. In IOP Conference Series: Materials
Science and Engineering, Vol. 237. IOP Publishing, 012026.
[49]BalasubramaniamRamesh and Matthias Jarke.2001. TowardReference Models
forRequirementsTraceability. IEEETrans.Softw.Eng. 27(January2001),58‚Äì93.
Issue 1. https://doi.org/10.1109/32.895989
[50]ArielSSchwartzandMartiAHearst.2002. Asimplealgorithmforidentifying
abbreviationdefinitionsinbiomedicaltext.In Biocomputing2003 .WorldScientific,
451‚Äì462.
[51]ChaoShang,SarthakDash,MdFaisalMahbubChowdhury,NandanaMihinduku-
lasooriya,andAlfioGliozzo.2020. Taxonomyconstructionofunseendomainsvia
graph-based cross-domain knowledge transfer. In Proceedings of the 58th Annual
Meeting of the Association for Computational Linguistics. 2198‚Äì2208.
[52]Jingbo Shang, Jialu Liu, Meng Jiang, Xiang Ren, Clare R Voss, and Jiawei Han.
2018. Automatedphraseminingfrommassivetextcorpora. IEEETransactionsonKnowledge and Data Engineering 30, 10 (2018), 1825‚Äì1837.
[53]JiamingShen,ZhihongShen,ChenyanXiong,ChiWang,KuansanWang,and
JiaweiHan.2020. TaxoExpan:Self-supervisedtaxonomyexpansionwithposition-
enhanced graph neural network. In Proceedings of The Web Conference 2020.
486‚Äì497.
[54]Yonghee Shin,Jane Huffman Hayes, andJane Cleland-Huang. 2015. Guidelines
forBenchmarkingAutomatedSoftwareTraceabilityTechniques.In 8thIEEE/ACM
International Symposium on Software and Systems Traceability, SST 2015, Florence,
Italy, May 17, 2015, Patrick M√§der and Rocco Oliveto (Eds.). IEEE Computer
Society, 61‚Äì67. https://doi.org/10.1109/SST.2015.13
[55]Ben Shneiderman. 2003. The eyes have it: A task by data type taxonomy forinformation visualizations. In The craft of information visualization. Elsevier,
364‚Äì371.
[56]Kai Shu, Limeng Cui, Suhang Wang, Dongwon Lee, and Huan Liu. 2019. de-
fend:Explainablefakenewsdetection.In Proceedingsofthe25thACMSIGKDD
International Conference on Knowledge Discovery & Data Mining. 395‚Äì405.
[57]GeorgeSpanoudakis,AndreaZisman,ElenaP√©rez-Mi√±ana,andPaulKrause.2004.
Rule-based generation of requirements traceability relations. Journal of Systems
and Software 72, 2 (2004), 105‚Äì127.
[58]Qingkai Zeng, Jinfeng Lin, Wenhao Yu, Jane Cleland-Huang, and Meng Jiang.2021. Enhancing Taxonomy Completion with Concept Generation via FusingRelational Representations. In KDD ‚Äô21: The 27th ACM SIGKDD Conference on
Knowledge Discovery and Data Mining, Virtual Event, Singapore, August 14-18,2021, Feida Zhu, Beng Chin Ooi, and Chunyan Miao (Eds.). ACM, 2104‚Äì2113.
https://doi.org/10.1145/3447548.3467308
[59]Qingkai Zeng, Mengxia Yu, Wenhao Yu, Jinjun Xiong, Yiyu Shi, and Meng Jiang.
2019. Faceted hierarchy: A new graph type to organize scientific concepts and a
construction method. In Proceedings of theThirteenth Workshop onGraph-Based
Methods for Natural Language Processing (TextGraphs-13). 140‚Äì150.
[60]Xuchang Zou, Raffaella Settimi, and Jane Cleland-Huang. 2006. Phrasing in
DynamicRequirementsTraceRetrieva.In 30thAnnualInternationalComputer
Software and Applications Conference, COMPSAC 2006, Chicago, Illinois, USA,
September17-21,2006.Volume1.IEEEComputerSociety,265‚Äì272. https://doi.
org/10.1109/COMPSAC.2006.66
[61]Xuchang Zou, Raffaella Settimi, and Jane Cleland-Huang. 2010. Improving auto-
matedrequirementstraceretrieval:astudyofterm-basedenhancementmethods.Empir.Softw.Eng. 15,2(2010),119‚Äì146. https://doi.org/10.1007/s10664-009-9114-
z
1044
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:20:26 UTC from IEEE Xplore.  Restrictions apply. 