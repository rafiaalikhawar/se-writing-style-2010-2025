arXiv:2505.05283v2  [cs.SE]  9 May 2025Software Development Life Cycle Perspective: A Survey of Benchmarks for
Code Large Language Models and Agents
KAIXIN WANG, Xi‚Äôan Jiaotong University, China
TIANLIN LI, Nanyang Technological University, Singapore
XIAOYU ZHANG‚àó,Nanyang Technological University, Singapore
CHONG WANG, Nanyang Technological University, Singapore
WEISONG SUN, Nanyang Technological University, Singapore
YANG LIU, Nanyang Technological University, Singapore
BIN SHI‚àó,Xi‚Äôan Jiaotong University, China
Code large language models (CodeLLMs) and agents have shown great promise in tackling complex software engineering tasks.
Compared to traditional software engineering methods, CodeLLMs and agents offer stronger abilities, and can flexibly process inputs
and outputs in both natural and code. Benchmarking plays a crucial role in evaluating the capabilities of CodeLLMs and agents, guiding
their development and deployment. However, despite their growing significance, there remains a lack of comprehensive reviews of
benchmarks for CodeLLMs and agents. To bridge this gap, this paper provides a comprehensive review of existing benchmarks for
CodeLLMs and agents, studying and analyzing 181 benchmarks from 461 relevant papers, covering the different phases of the software
development life cycle (SDLC). Our findings reveal a notable imbalance in the coverage of current benchmarks, with approximately
60% focused on the software development phase in SDLC, while requirements engineering and software design phases receive minimal
attention at only 5% and 3%, respectively. Additionally, Python emerges as the dominant programming language across the reviewed
benchmarks. Finally, this paper highlights the challenges of current research and proposes future directions, aiming to narrow the gap
between the theoretical capabilities of CodeLLMs and agents and their application in real-world scenarios.
CCS Concepts: ‚Ä¢Software and its engineering ‚ÜíSoftware development techniques ;‚Ä¢General and reference ‚ÜíSurveys and
overviews ;‚Ä¢Computing methodologies ‚ÜíArtificial intelligence .
Additional Key Words and Phrases: Code Large Language Models, Software Development Life Cycle, Benchmarks, Software Engineering
1 INTRODUCTION
Large Language Models (LLMs) have ushered in a transformative era in software engineering (SE), particularly through
specialized variants known as Code Large Language Models (CodeLLMs). With the capabilities to handle tasks such
as code generation, translation, and completion, these models have outperformed traditional SE methods and have
been widely used in academia and industry [ 61,62,71]. For example, GitHub Copilot [ 2], powered by GPT-4, has been
shown to improve developer productivity significantly. Developers using Copilot code 55% faster and are 73% more able
to maintain workflow on repetitive tasks [ 83]. Similarly, Codex [ 23], a model with 12 billion parameters, achieved a
72.31% success rate in solving complex Python programming challenges, outperforming traditional models.
‚àóCorresponding author
Authors‚Äô addresses: Kaixin Wang, kxwang@stu.xjtu.edu.cn, Xi‚Äôan Jiaotong University, Xian, China; Tianlin Li, tianlin001@e.ntu.edu.sg, Nanyang
Technological University, Singapore, Singapore; Xiaoyu Zhang, joshingrain@gmail.com, Nanyang Technological University, Singapore, Singapore;
Chong Wang, chong.wang@ntu.edu.sg, Nanyang Technological University, Singapore, Singapore; Weisong Sun, weisong.sun@ntu.edu.sg, Nanyang
Technological University, Singapore, Singapore; Yang Liu, yangliu@ntu.edu.sg, Nanyang Technological University, Singapore, Singapore; Bin Shi,
shibin@xjtu.edu.cn, Xi‚Äôan Jiaotong University, Xian, China.
Manuscript submitted to ACM 12 Wang et al.
/0/1/2/3/4 /5/1/6/1/7/8 /i255
/i255 /10/7/11/4 /7/1/1/5/4 /7/11/12/13/14/10/15 /4 /16/4 /8 /17/8 /4 /18/7/i255 /12/19/14/20/21/22/23
/24/25/26 /27/28 /29
/30/20/31/32/33/34/30/35
/36/15 /17/37/37/4 /38/4 /16/17/8 /4 /18/7/i255 /12/39/14/20/40/41/42/43 /31/33
/44/18/45/1/15 /4 /7/11/i255 /12/39/14/46/23/22/22/25/22/26 /28 /29
/47/48/1/16/4 /38/4 /16/17/8 /4 /18/7/i255 /12/39/14/49/22/26 /29/50/27/25/28 /29
/51/25/52 /26 /53/25/54/26 /55/27/i255 /57/58/59/40/23/26 /27/60/55/52 /53/28
/40/33/61/21/23/29/54/62
/22/31/63/33/64/32/23/27/65/50
/44/17/7/17/11/1/6/1/7/8 /i255 /12/66/14/67
/47/18/38 /8 /68/17/5 /1/i255
/i255 /69/1/37/4 /11/7/i255 /12 /70/14/71/5/16/72/4 /8 /1/16/8 /3/5/17/15 /i255 /69/1/37/4 /11/7/i255 /12/66/14/67
/71/15 /11/18/5/4 /8 /72/6/i255/69/1/37/4 /11/7/i255 /12/39/14/30/73/40/31
/69/17/8 /17/74/17/37/1/i255 /45/1/37/4 /11/7/i255 /12/66/14/67
/75/76 /i255 /69/1/37/4 /11/7/i255 /12 /77/14/40/26 /65/55
/31/30/25/60/40/23/60/55/i255 /25/27/53/i255 /31/65/22/23/23/27/40/23/60/55
/31/65/22/23/23/27/78/79/55/22/53/29
/40/23/63/22/25/79
/47/18/38 /8 /68/17 /5 /1 /i255
/69/1 /80/1 /15 /18/48/6/1 /7/8 /i255
/12 /39/66/81/14/36 /18 /45 /1 /i255 /82 /1 /7 /1 /5 /17 /8 /4 /18 /7 /i255 /12 /70 /70 /14/35/21/83/25/27/33/84/25/52
/35/21/83/25/27/33/84/25/52 /85
/35/21/83/25/27/33/84/25/52 /64/33/86
/42/32/20/20
/42/32/20/20/85
/42/32/20/20/64/33/86
/32/26 /87/30/55/53/23/32/23/27/65/50
/30/52 /25/29/29/33/84/25/52
/30/55/27/30/55/53/23
/30/55/53/23/22 /33/84/25/52
/33/84/55/30/55/53/23/32/23/27/65/50
/35/21/83/25/27/33/84/55
/30/55/34/25/73 /25
/20/22 /25 /87/83/25/54 /26 /65 /30/55/53/23
/46 /33 /62/64 /32/23 /27/65 /50
/63/23 /84 /23 /84 /25 /52
/30 /55 /53 /23 /25 /87 /23 /27 /54 /32 /23 /27 /65 /50 /88
/35/21/83/25/27 /33 /84 /25 /52 /64 /89
/42/25 /54 /50 /61/62/64 /89
/42/32/89 /20
/42/21/52 /54 /26 /20 /73 /64 /86
/42/21/52 /54 /26 /52 /26 /27 /87 /21 /25 /52 /i255 /35 /21 /83 /25 /27 /33 /84 /25 /52
/40 /64 /90 /23 /27 /65 /50 /83/25/22 /91
/40 /54 /52 /64 /22 /23 /60 /55
/51 /23 /22 /26 /52 /55 /87 /23 /84 /25 /52
/30 /50 /26 /32 /23 /27 /65 /50
/46 /51 /33 /84 /25 /52
/62 /20 /20 /31
/30 /55 /53 /23 /30 /55 /27 /54 /23 /29 /54 /29
/73 /26 /84 /23 /30 /55 /53 /23 /32 /23 /27 /65 /50
/73 /23 /23 /54 /30 /55 /53 /23
/30 /55 /53 /23 /46 /55 /22 /65 /23 /29
/42/42/30 /55 /53 /23
/20 /52 /55 /54 /78 /30 /55 /53 /23/31 /60 /26 /53 /23 /22 /78 /64 /84 /88/92/23 /90 /78 /65 /55 /53 /23/63/23 /29 /26 /87 /27 /78 /30 /55 /53 /23 /42 /25 /54 /20 /52 /55 /54 /32 /23 /27 /65 /50 /32 /25 /90 /23 /52 /32 /23 /27 /65 /50 /63/31/64 /93 /94 /94 /94 /40 /55 /90 /55 /31 /65 /22 /26 /60 /54/20 /25 /54 /50 /32 /23 /27 /65 /50/30 /55 /32 /40 /62 /40 /55 /90 /55 /54 /55 /21 /26 /52 /52 /23/33 /20 /43 /30 /64 /49 /26 /54 /65 /50 /23 /27 /29/32 /26 /55/30/55/53/23 /22/42/73/62/87/23 /27/54 /32/23 /27/65 /50/88/42/73/64 /90/23/27/65 /50/88/63/23/23/60/64 /32/23/27/65 /50/30/55/32/40/62/40/55/90/55/54 /55/21/26 /52 /52 /23/88/33/20/43 /30/64 /49/26 /54 /65/50/23/27/29/32/26 /55/30/55/53/23/22/42/73
/62/87/23/27/54 /32/23/27/65/50/88/42/73/64/90/23/27/65/50/63/23
/23/60/64/32/23/27/65/50/24 /21/43 /30/23/33/95/23/65/64/30/31/34/40/25/65/23/73/23/29/29/73/23/25/91/64/32/23/27/65/50/63/55/83/25
/26 /27/23/84/25/52/31/54/21/53/23/27/54/33/84/25/52/96/1/97/8 /98/8 /18/98/47/99/100/i255 /12/39/101/14 /92
/26 /91/26 /31/61/73/31/60/26 /53/23/22/31/60/26 /53/23/22 /64/31/102/27/31/60/26 /53/23/22 /64/63/49/31/60/26 /53/23/22 /64/40/23/25/52 /26 /29/54/26 /65/30/31/20/43 /63/33/40/32/43 /40/63/49/25/87/87/52 /23/63/32
/61/62/30/55/31/61/73/33/35/40
/31/61/73/86/23/22/83/26
/54/23/36/18/45/1/i255 /36/18/6/48/15 /1/8 /4 /18/7/i255 /12/19/14
/40/23/60/55/32/23/27/65/50/40/23/60/55/33/84/25/52/30/22/55/29/29/30/55/53/23/33/84/25/52/36/18/45/1/i255 /75/7/45/1/5/37/8 /17/7/45/4 /7/11
/i255 /17/7/45/i255 /0/1/17/37/18/7/4 /7/11/i255 /12/39/19/14
/43 /27/103/26 /90/23/27/65/50/30/55/53/23/61
/62 /30/31/93/61/62
/30/55/31/61/62
/30/55/31/61/62/85
/63/61/62/32/23/27/65/50
/30/40/104/89/33/84/25/52
/i255/i255 /30/40/104/89/33/51/62/73/64/89
/30/55/53/23/42/42/73/104
/40/33/84/25/52
/30/55/53/23/62/60/23/95
/31/60/23/65/33/84/25/52
/46/62/104/34/64/33/84/25/52/36/18/45/1/i255 /96/5/17/7/37/15 /17/8 /4 /18/7/i255 /12/77/14
/20/55/52 /102/35/21/83/25/27/33/84/25/52
/42/30/33/84/25/52
/30/55/53/23/86/22/25/27/29/41/65/23/25/27
/42/21/52 /54/26 /20/73/64/33/36/18/45/1/i255 /47/3/6/6/17/5 /4 /106/17/8 /4 /18/7/i255 /12 /19/14
/30/41/63/33/64/34/34
/63/23/23/60/30/55/83
/86/73/64/30/55/53/23/31/21/83/96/107/48/1/i255 /76 /7/38 /1/5 /1/7/16/1/i255 /12 /101/14
/73/25/83/90/53/25/27/23/54
/43 /53/32/23/27/65/50/36/18/45/1/i255 /0/1/8 /5 /4 /1/80/17/15 /i255 /12 /101/14
/63/23/23/60/30/31
/30/55/53/23/31/23/25/22 /65/50/34/23/54/108/18 /7 /98 /109 /3 /7 /16 /8 /4 /18 /7 /17 /15 /i255 /12 /39 /110 /14
/40/23/30/55/53/23
/40/23/53/30/55/53/23/88
/40/42/30/32/23/27/65/50
/62/53/84/32/23/27/65/50
/35/42 /30/55/22 /60/63/111/64 /30/112 /33/30/30/41
/33/103 /103 /26 /32/23/27/65 /50 /113/33 /30/42/23/22 /65 /21/22 /102/33 /84 /25 /52 /20 /23 /22 /103/33 /34/62/42/33/73/46 /25 /26 /22 /30 /55 /53 /23 /22
/31 /55 /65 /26 /25 /52 /32 /26 /25 /29 /64 /32 /23 /27 /65 /50 /114 /50 /25 /27 /87 /28 /29/30 /55 /53 /23 /62 /22 /23 /27 /25 /113 /25 /52 /23 /22 /25 /29/96 /1 /37 /8 /4 /7 /11 /i255
/12 /101 /70 /14 /115 /3 /15 /7 /1 /5 /17 /74 /4 /15 /4 /8 /107 /i255 /69 /1 /8 /1 /16 /8 /4 /18 /7 /i255
/i255 /i255 /i255 /i255 /i255 /i255 /i255 /i255 /i255 /i255 /i255 /i255 /i255 /i255 /i255 /i255 /12 /39 /116 /14
/113 /40 /33 /62 /86/42 /51 /63/46 /55 /22 /83/62/43/63/26 /84 /23 /22 /29 /23 /51 /21 /52/63/23 /84 /26 /87 /27/32 /26 /87 /64 /51 /21 /52/40 /33 /51 /33 /62 /73/51 /21 /52 /63/23 /23 /20 /23 /65 /91 /23 /22/51 /21 /52 /27 /20 /25 /54 /65 /50 /20 /25 /26 /22 /29/31 /102 /31 /23 /51 /40/20 /22 /26 /83/23 /51 /21 /52/63/22 /25 /60 /23 /22/51 /104 /63/33 /34/30/24 /21 /52 /26 /23 /54/30 /22 /55 /29 /29 /51 /21 /52/30 /51 /33 /103 /26 /95 /23 /29/96 /1 /37 /8 /i255 /82 /1 /7 /1 /5 /17 /8 /4 /18 /7 /i255 /12 /19 /14/42 /23 /54 /50 /55 /53 /29 /78 /86 /23 /29 /54/31 /46 /93 /93 /94/31 /92/86 /32 /23 /27 /65 /50 /88 /71 /37 /37 /1 /5 /8 /4 /18 /7 /i255 /82 /1 /7 /1 /5 /17 /8 /4 /18 /7 /i255 /12 /39 /14/91 /25 /27 /53 /23 /28 /29 /i255 /90 /23 /27 /65 /50 /83/25/22 /91 /36 /18 /45 /1 /i255 /1 /45 /4 /8 /4 /7 /11 /i255 /12 /101 /14/20 /117 /30 /41/42/42/43 /86 /31/30 /55 /33 /53 /20 /26 /52 /55 /54/69/1 /38 /1 /16 /8 /i255 /69/1 /8 /1 /16 /8 /4 /18 /7 /37 /i255 /12 /19 /14/24 /43 /86 /64 /63/23/103 /23 /65 /54 /29 /118 /24/32 /23 /25 /22 /29/32/21/87/29 /119 /120 /25 /22
/47/18/38 /8 /68/17/5 /1/i255/44/17
/4 /7/8 /1/7/17/7/16/1/i255/12 /101/77/14/121/5 /18/11/5 /17 /6/i255/0/1 /48/17 /4 /5 /i255 /12 /13/14/63/23/103 /23/65 /54 /29 /118/24/31/92/33
/64 /90/23/27/65 /50/31/92/62/122 /32/23/27/65 /50/88/31/92/33
/33/122 /32/23/27/65/50/88/61/21
/26 /95/32/21/87/29/35/104
/42/62
/34/33
/51/62/73/46/43 /89/42/25
/27/102/32/21/87/29/32/21/87/29/43 /27/20/102/86/102/60/23/32/21/87/29 /100/18/11/i255 /121/17/5 /37/4 /7/11/i255 /12 /19/14/73/55/87/50/21/90/73/55/87/20/42/73/55/87/50/21/90/64/78/119 /94 /115/3/15 /7/1/5 /17/74/4 /15 /4 /8 /107/i255 /0/1/48/17/4 /5 /i255 /12 /70/14/51/21/52 /118/120/42/25
/27/102/51/21/52 /29/118/24/51/120 /32/23/27/65/50/51/24/32/23/27/65/50/64/54/22/25/27/29/33/95/54/22/25/65/54/46/26 /95
/69/1/16/18/6/48
/4 /15 /17/8 /4 /18/7/i255 /12/101/14/92/25
/63/23
/65/33/95/23/32/23/27/65/50
/71/121/76/i255 /44/4
/37/3/37/1/i255 /69/1/8 /1/16/8 /4 /18/7/i255 /12/101/14/40/41/32
/104/31/86/62/20/43/62/20/43 /42
/104/118/30
/36/18/45/1/i255 /36/15 /18/7/1/i255 /69/1/8 /1/16/8 /4 /18/7/i255 /12/19/14/20/41/24/93/94/118/30/55/53/23/34/23
/54/32/26 /87/30/52 /55/27/23/32/23/27/65/50
/36/5/18/37/37/98/37/8 /17/11/1/i255 /12/110/14/36/5/18/37/37/98/37/8 /17/11/1/i255 /12/110/14/89/65/55/53/23/23/84/25/52/30/55/53/23/89/113/73/104/33/30/55/53/23/31/65/55/60/23/35/21
/83/25
/27/33/84/25/52 /20/25/65/91/63/23
/84/32/23/27/65/50/20/22/55/120 /23/65/54/33/84/25/52 /88/31/92/33
/122/20/55/52/102/32/23/27/65/50/88
/i255
Fig. 1. Taxonomy of CodeLLM benchmarks across the SDLC. This figure categorizes existing CodeLLM benchmarks according to
different phases of SDLC. The numbers indicate how many benchmarks correspond to each phase or task. Entries marked with an
asterisk (*) denote benchmarks that can be used to evaluate the capabilities of CodeLLM agents. The blank spaces indicate that there
are no relevant benchmarks for the current task.
Furthermore, researchers develop a series of agents that build upon CodeLLMs and SE tools. These CodeLLM-based
agents demonstrate superior performance in addressing SE challenges while significantly enhancing development
efficiency. For example, Cursor [ 73], an AI-powered code editor built on CodeLLM technology, integrates contextual
code understanding capabilities directly within the development environment, enabling developers to generate, refactor,
and debug code through natural language instructions. These advances highlight the growing influence of CodeLLM and
agents in research and real-world applications and their potential to revolutionize the software development process.
CodeLLMs and agents possess two distinctive capabilities that are reshaping the traditional SE paradigm. ‚ù∂First,
they can effectively process inputs and outputs that combine natural language (NL) and programming language (PL),
Manuscript submitted to ACMSoftware Development Life Cycle Perspective: A Survey of Benchmarks for Code Large Language Models and Agents 3
demonstrating exceptional performance on complex tasks such as bug fixing and code optimization. For instance, devel-
opers can leverage natural language instructions to conveniently guide code generation and debugging processes [ 24],
substantially reducing development effort and associated costs. ‚ù∑Second, CodeLLMs possess relatively comprehensive
code comprehension and reasoning capabilities, enabling them to handle diverse programming scenarios and multiple
SE tasks. Existing studies demonstrate that developers can efficiently accomplish more complex SE tasks, including
generating an entire repository [ 119,138]. Thus, traditional SE benchmarks are insufficient for evaluating the versatile
applications of CodeLLMs and agents.
Recently, researchers have made notable contributions by proposing various benchmarks [ 79,96,98,152] to more
comprehensively evaluate CodeLLMs and agents in SE tasks. For example, SWE-bench [ 79] is designed to evaluate
the ability of CodeLLMs and agents to automatically resolve GitHub issues. However, despite the rapid emergence
of numerous benchmarks, a natural question remains: Are these benchmarks sufficient to evaluate the full
capabilities of CodeLLMs and agents? And what further improvements are needed in the future?
Answering this question is particularly challenging, as it remains unclear whether the application scenarios of
CodeLLMs and agents have been fully explored, let alone whether the existing benchmarks are sufficiently comprehen-
sive. As CodeLLMs and agents are expected to increasingly replace human effort across various SE activities [ 52,135],
this paper seeks to address the question by systematically mapping existing benchmarks to specific phases of the
Software Development Life Cycle (SDLC), as illustrated in Fig. 1, which is commonly regarded as a comprehensive
process covering as many SE activities as possible. This mapping reveals which phases have received adequate re-
search attention and where significant gaps remain. Through analysis of current benchmark limitations and statistical
benchmark usage, this paper proposes challenges and potential directions for future benchmark development, aimed at
enabling a comprehensive evaluation of CodeLLMs and agents across different SDLC phases.
1.1 Motivation
The main motivations for this survey are as follows:
‚Ä¢Compared to traditional SE methods, CodeLLMs demonstrate superior performance on complex tasks and en-
hanced cross-task generalization capabilities. This fundamental advancement necessitates specialized benchmarks
that exceed conventional evaluation approaches. While traditional SE benchmarks typically assess performance
on isolated tasks, CodeLLM evaluation demands a comprehensive assessment across multiple tasks and progres-
sively complex scenarios. Therefore, a comprehensive study of benchmarks for CodeLLMs is highly valuable for
understanding LLM capabilities and accelerating the development of more sophisticated code models.
‚Ä¢Benchmarks serve as essential instruments for evaluating models and techniques across specified tasks. High-
quality benchmarks establish standardized and equitable evaluation metrics, facilitating comparative analysis of
diverse models on identical tasks and elucidating their respective strengths and limitations. These standardized
frameworks not only guide targeted model improvements but also provide a foundational structure for industry
practitioners to select and optimize appropriate solutions. However, the rapid advancement of artificial intelligence
capabilities has created a significant gap, as existing evaluation benchmarks struggle to keep pace with these
developments. This misalignment between evaluation approaches and model capabilities potentially undermines
accurate assessment of state-of-the-art models, particularly in the SE domain. In light of this, conducting
comprehensive analyses of benchmarks for CodeLLMs and agents and understanding their limitations and
Manuscript submitted to ACM4 Wang et al.
Table 1. Comparison Between Existing Surveys and Our Work.
SurveyObjectStudy of
BenchmarkSDLC Phases
CodeLLM AgentRequirements
EngineeringDesignSoftware
DevelopmentTestingSoftware
Maintenance
Zhang et al. [193] √ó √ó ‚úì ‚úì √ó ‚úì ‚úì ‚úì
Gonzalez et al. [49] √ó √ó √ó ‚úì ‚úì ‚úì ‚úì ‚úì
Hou et al. [62] ‚úì√ó √ó ‚úì ‚úì ‚úì ‚úì ‚úì
Yang et al. [182] ‚úì√ó √ó √ó √ó ‚úì√ó √ó
Zheng et al. [200] ‚úì√ó √ó ‚úì ‚úì ‚úì ‚úì ‚úì
Zheng et al. [199] ‚úì√ó ‚úì√ó √ó ‚úì ‚úì √ó
Ours ‚úì ‚úì ‚úì ‚úì ‚úì ‚úì ‚úì ‚úì
challenges becomes imperative for ensuring reliable assessment and appropriate application of these increasingly
powerful models and agents in real-world SE scenarios.
‚Ä¢Although existing surveys have examined benchmarks and models on SE tasks, they still have limitations. We
have compared related surveys in Table 1 and observed that previous studies have either focused exclusively on
traditional machine learning models without involving advanced CodeLLMs [ 49,193], or lacked comprehensive
benchmark analysis [ 62,182,200]. Even recent review has studied the CodeLLM-specific benchmarks [ 199],
but it still limit the scope to specific tasks on the development and testing phases, leaving significant gaps in
analyzing benchmarks across various requirements throughout SDLC. Worse still, existing reviews generally
neglect the emergence of powerful CodeLLM-based agents and their corresponding evaluation benchmarks,
thereby failing to analyze the methodological limitations and technical challenges these advanced systems
present for assessment. Given these realities, it becomes imperative to establish a comprehensive survey of
benchmarks for CodeLLMs and agents from the perspective of SDLC.
1.2 Review Questions
This survey provides a comprehensive analysis of benchmarks for CodeLLMs across various phases of the SDLC,
identifies their limitations, and proposes directions for future research. Specifically, this survey addresses the following
research questions:
‚Ä¢RQ1: Do existing benchmarks comprehensively cover the practical requirements of the SDLC? (answered in ¬ß3)
‚Ä¢RQ2: How are current benchmarks distributed in terms of (i) use frequency, (ii) release year, and (iii) the
programming languages they target? (answered in ¬ß4)
‚Ä¢RQ3: What are the key future directions in the evaluation of CodeLLMs and agents? (answered in ¬ß5)
1.3 Collection Strategy
To ensure a comprehensive and systematic collection of research at the intersection of large language models (LLMs)
and software engineering, we implemented a rigorous approach for literature collection. Our process comprised four
primary stages, as illustrated in Fig. 2. The details are as follows:
(1)Keywords Summary: First, we conducted preliminary literature reading and summarized 20 different keyword
combinations: "Code Large Language Model", "Code LLMs", "AI4SE", "LLM4SE", "AI for SE", "Code Agents
Manuscript submitted to ACMSoftware Development Life Cycle Perspective: A Survey of Benchmarks for Code Large Language Models and Agents 5
/0 /1 /2 /3 /4 /5 /6 /7 /i255 /9 /10 /11/11/12 /5 /2 /13 /10 /14 /15 /16 /17 /12 /18 /16 /4 /19 /i255 /20 /16 /15 /18 /1 /5 /16 /19 /21 /9 /19 /4 /3 /14 /12 /15 /15 /i255 /22 /23 /24 /12 /19 /7 /16 /4 /19 /13 /10 /14 /15 /16 /17 /12 /18 /16 /4 /19 /i255 /9 /1 /12 /5 /17 /25
/26 /27 /28 /29 /30 /31/30 /32 /33 /27 /34 /i255 /27 /28 /33 /36 /30 /32 /37 /i255 /33 /32 /36
/38 /39 /31/31/33 /27 /30 /40 /30 /32 /37 /i255 /41 /28 /34 /42 /43 /27 /36 /38 /44/45 /46 /27 /28 /28 /32 /i255 /47 /48 /28 /i255 /29 /30 /47 /28 /27 /33 /47 /39 /27 /28
/49 /33 /38 /28 /36 /i255 /43 /32 /i255 /50 /43 /39 /27 /i255 /46 /27 /30 /47 /28 /27 /30 /33 /44/45 /28 /33 /27 /46 /48 /i255 /43 /32 /i255 /51 /43 /43 /37 /29 /28
/45 /46 /48 /43 /29 /33 /27 /44/52 /43 /27 /42 /33 /27 /36 /i255 /33 /32 /36 /i255 /49 /33 /46 /41 /42 /33 /27 /36
/38 /32 /43 /42 /49 /33 /29 /29 /i255 /38 /33 /31/53 /29 /30 /32 /37
/38 /47 /27 /33 /47 /28 /37 /30 /28 /38 /44
/54 /55 /56 /57 /58 /i255 /60 /61 /60 /62 /63 /64 /65/54 /56 /66 /55 /60 /61 /60 /62 /63 /64 /65/54 /57 /67 /55 /60 /61 /60 /62 /63 /64 /65 /54 /66 /68 /i255 /69 /62 /70 /71 /72 /63 /73 /64 /65
Fig. 2. Research process for literature collection. This process includes keywords summary, publication search, publication filtering,
and snowball expansion.
Benchmarks", "Programming Agents Benchmarks", "LLM Agents for Software Engineering", "AI for Requirements
Engineering", "AI for Software Design", "AI for Software Development", "AI for Software Testing", "AI for Software
Maintenance", "Programming Benchmarks", "Code Benchmarks", "Code Generation", "Code Understanding",
"Code Completion", "Automated Programming", "Automated Testing".
(2)Publication Search: We performed automated literature searches using predefined keywords across four major
academic databases: IEEE Xplore, ACM Digital Library, Elsevier Science Direct, and Springer. These databases
provided broad coverage of core research in LLMs and software engineering. In total, we retrieved 1,347 relevant
research papers.
(3)Publication Filtering: We performed a manual selection of the retrieved articles based on the following criteria,
resulting in 321 articles.
‚Ä¢Research on the application of LLMs in software engineering.
‚Ä¢Priority is given to papers accepted by influential journals (e.g., TOSEM, TSE, IJCN, JMLR, TDSC, TOCHI) and
conferences (e.g., ICSE, ASE, FSE, ACL, ICML, IJCAI, ISSTA, NeurIPS, PLDI, SIGKDD, S&P, CHI).
‚Ä¢The selection of publications on arXiv is based on two key criteria, namely the quality of the paper and the
reputation of the author.
‚Ä¢Papers published after 2022. Based on the commonly accepted definition of LLMs [ 194], we selected literature
published from 2022 onwards.
‚Ä¢All papers that propose code-related benchmarks are directly included in the scope of this survey.
(4)Snowball Expansion: Considering that keyword-based searches have the potential to overlook relevant studies,
we employed both forward and backward snowball sampling techniques, starting from the initial literature
set, to include additional relevant papers. Through this process, we added an additional 140 papers, ensuring
the thoroughness and completeness of the research. In the end, we collected a total of 461 papers that met the
criteria.
We conducted a strict quality assessment of the included papers. Specifically, each paper was independently reviewed
by two co-authors with backgrounds in both SE and LLMs. If there were differences in their assessments, a third
co-author was involved in the discussion to reach an agreement. This review process aimed to ensure that all the studies
included in the review were of reliable quality and high relevance. The remainder of the paper is organized as follows:
¬ß2 introduces the background of the Large Language Models for Software Engineering (LLM4SE) and SDLC. ¬ß3 outlines
the benchmarks corresponding to each phase of the SDLC. ¬ß4 provides a statistical analysis of the collected benchmarks.
¬ß5 summarizes the key findings of our survey and discusses potential future research directions, and ¬ß6 concludes this
paper.
Manuscript submitted to ACM6 Wang et al.
/0 /1 /2 /3 /4 /5 /1 /6/1 /7 /8 /9 /i255 /11 /7 /12 /4 /7 /1 /1 /5 /4 /7 /12 /13 /1 /9 /4 /12 /7 /14 /1 /9 /8 /4 /7 /12 /15 /16 /17 /8 /18 /19 /5 /1 /i255 /13 /1 /20 /1 /21 /16 /22 /6/1 /7 /8
/23 /24 /25 /26 /25 /27 /25 /28 /29 /30 /i255 /32 /28 /33 /24 /34 /35 /25 /28 /29 /30
/33 /28 /36 /i255 /37 /33 /24 /25 /36 /33 /27 /25 /28 /29 /i255
/38 /27 /33 /39 /40 /41 /42 /24 /36 /40 /43 /i255
/44 /40 /45 /46 /25 /43 /40 /47/40 /28 /27 /48/49 /47/50 /24 /40 /47/40 /28 /27 /33 /27 /25 /42 /28 /51 /i255
/44 /40 /33 /24 /25 /35 /25 /28 /29 /i255 /27 /41 /40 /i255 /52 /40 /48 /25 /29 /28 /i255
/25 /28 /i255 /53 /42 /36 /40/52 /40 /54 /25 /28 /40 /48 /i255 /32 /43 /26 /41 /25 /27 /40 /26 /27 /46 /43 /40 /30 /i255
/55/42 /36 /46 /24 /40 /48 /30 /i255 /33 /28 /36 /i255
/49 /28 /27 /40 /43 /54 /33 /26 /40 /48 /i255 /33 /48 /i255 /33 /i255
/56 /24 /46 /40 /50 /43 /25 /28 /27 /i255 /54 /42 /43 /i255
/49 /47/50 /24 /40 /47/40 /28 /27 /33 /27 /25 /42 /28/38 /34 /48 /27 /40 /47/33 /27 /25 /26 /i255
/37 /40 /43 /25 /54 /25 /26 /33 /27 /25 /42 /28 /i255 /42 /54 /i255
/53 /42 /43 /43 /40 /26 /27 /28 /40 /48 /48 /30 /i255
/44 /40 /24 /25 /33 /57 /25 /24 /25 /27 /34 /30 /i255 /33 /28 /36 /i255
/58 /40 /43 /54 /42 /43 /47/33 /28 /26 /40/15 /16 /17 /8 /18 /19 /5 /1 /i255 /59/19 /4 /7 /8 /1 /7 /19 /7 /60 /1
/53 /42 /28 /27 /25 /28 /46 /42 /46 /48 /i255
/55/42 /36 /25 /54 /25 /26 /33 /27 /25 /42 /28 /i255 /27 /42 /i255 /61 /25 /62 /i255
/56 /46 /29 /48 /30 /i255 /23 /28 /41 /33 /28 /26 /40 /i255
/58 /40 /43 /54 /42 /43 /47/33 /28 /26 /40 /30 /i255 /33 /28 /36 /i255
/32 /36 /33 /50 /27 /i255 /27 /42 /i255 /53 /41 /33 /28 /29 /40
Fig. 3. The phases and their core tasks within the SDLC.
2 BACKGROUND
This section provides an overview of the background on SDLC and LLM4SE, establishing the foundation for the
subsequent sections.
2.1 LLMs for Software Engineering (LLM4SE)
Software engineering (SE) is a discipline focused on the design, development, testing, implementation, and maintenance
of software systems. In recent years, the emergence of artificial intelligence in software engineering (AI4SE) has made
remarkable advancements in the field [ 62,117]. AI4SE provides intelligent solutions for software engineering tasks,
thereby enhancing developer productivity and software quality. Similar to the developments in the field of NLP, AI4SE
is undergoing a historic transformation from statistical models and recurrent neural networks (RNNs) to pre-trained
transformers and LLMs [193].
LLMs demonstrate significant potential to revolutionize software engineering practices by enhancing productivity,
code quality, and innovation [ 57]. These models assist developers in writing code more efficiently, debugging applications,
and comprehending complex codebases [ 31]. Across various code-related tasks, LLMs have demonstrated outstanding
performance, including code generation [ 175], vulnerability detection [ 204], and program repair [ 77,173]. More
recently, researchers have developed LLM-based agents for SE tasks that integrate planning capabilities, tool usage, and
feedback mechanisms to autonomously complete complex development workflows [ 81,103]. These advancements have
accelerated the rapid evolution of LLM4SE, establishing it as one of the most dynamic research directions in the field.
To assess the capabilities of LLMs in code-related tasks, several benchmarks have been established, such as Hu-
manEval [ 23] and MBPP [ 10]. However, existing evaluation frameworks primarily focus on traditional tasks such as
code generation, understanding, and repair, without comprehensively covering all phases of the SDLC. The coverage
of current benchmarks across the entire SDLC remains underexplored within the community, and systematic studies
evaluating LLMs‚Äô capabilities across various phases of the full life cycle are lacking. To address this gap, we classify
the benchmark datasets according to the distinct phases of the SDLC, aiming to uncover tasks inadequately addressed
in existing research and highlight overlooked code capabilities, while simultaneously providing direction for future
research.
2.2 Software Development Life Cycle (SDLC)
In the early phases of software development, where SDLC is absent, the process is often disorganized and inefficient.
The lack of clear phase divisions and formal standards frequently results in a range of issues, including poor project
management, unclear progress tracking, unstable code quality, and vague requirements. With the introduction of SDLC,
the software development process has been significantly improved. The SDLC offers a structured, systematic approach
Manuscript submitted to ACMSoftware Development Life Cycle Perspective: A Survey of Benchmarks for Code Large Language Models and Agents 7
to software development, with each phase having defined goals and tasks. By segmenting the software development
process into distinct phases, teams can more effectively manage project progress, allocate resources, and control risks,
ultimately improving both software quality and customer satisfaction.
The SDLC provides a framework for the entire software development process, as shown in Fig. 3. A typical SDLC
includes key phases such as requirements engineering, design, software development, testing, and software maintenance,
each with its specific goals and tasks. The requirements engineering phase primarily focuses on gathering, analyzing, and
documenting detailed requirements. The design phase aims to translate the requirements into a software architecture
blueprint, covering both high-level system architecture and detailed design for each submodule. The development phase
focuses on selecting appropriate programming languages and development tools based on the design documents and
implementing system functionalities. The testing phase aims to validate whether the software meets the requirements,
functions correctly, and performs as expected, while generating test and defect reports. Following testing and deployment,
the software enters the maintenance phase, ensuring stability and performance during use, alongside necessary bug
fixes and updates [ 26]. Research on the use of LLMs throughout the SDLC is vital for understanding their impact and
potential in software engineering. However, the integration of LLMs into the SDLC is still in its early phases, with
current research primarily focusing on specific software development tasks. Comprehensive research covering the
entire SDLC remains a significant gap in the field.
3 BENCHMARK TAXONOMY BY THE SDLC
To address RQ1, we conduct an in-depth analysis of the benchmarks across all software development life cycle phases.
We categorize these benchmarks according to their primary evaluation objectives, and then analyze their distinctive
characteristics, strengths, and limitations. The SDLC consists of five phases: requirements engineering, software design,
software development, testing, and software maintenance. In the following analysis, we first identify the core tasks
within each phase and then thoroughly study the relevant benchmarks.
3.1 Requirement Engineering
Table 2. Benchmarks for requirements engineering tasks.
Tasks Benchmarks
Requirements Elicitation Pure [42], Jain‚Äôs [76], CPSBENCH [80]
Requirements Classification PROMISE [147]
Requirements Modeling Ferrari‚Äôs [41]
Requirements Specification Krishna‚Äôs [90]
Requirement Validation Reinpold‚Äôs [143], REQuestA [39], rSDE-Bench [67]
Requirements Management /
The essence of requirements engineering is to achieve a precise understanding and formal definition of the re-
quirements established by the users or stakeholders. LLMs are increasingly being applied in the field of requirements
engineering, including tasks such as requirements gathering, analysis, specification, and verification [ 35]. In the follow-
ing subsections, we provide a detailed overview of the key tasks in Requirements Engineering and their corresponding
benchmarks, as shown in Table 2.
Manuscript submitted to ACM8 Wang et al.
3.1.1 Requirements Elicitation. Requirements elicitation is a pivotal task in requirements engineering, involving the
systematic collection, organization, and specification of customer needs to ensure high-quality product development.
This process is widely recognized as the most critical component within the requirements engineering phase [ 6].
PURE [42] contains 79 publicly available natural language requirements documents with around 34,000 sentences. It is
one of the earliest and most widely used benchmarks for requirements elicitation. Building on this, Jain et al. [76]
extend the research to software engineering contracts, where complex legal language makes requirement extraction
more challenging. They create a dataset of 251 expired contracts across 13 industries, offering a new perspective for
evaluating LLMs in domain specific contexts. CPSBENCH [80] further expands the focus to cyber physical systems
(CPS), introducing 12 real-world industrial requirements documents and 30 tutorial cases. Each sample includes
requirement descriptions, entities, system interactions, and a problem diagram, providing a structured, context-rich
evaluation setting.
3.1.2 Requirements classification. Requirement classification involves categorizing the requirements into functional
and non-functional requirements. The PROMISE [147] dataset is frequently utilized for evaluating performance in
requirements classification. It comprises 625 requirements, including 254 functional requirements and 371 non-functional
requirements.
3.1.3 Requirements Modeling. Requirement modeling involves the process of transforming the requirements into
formal models using modeling languages such as UML and SysML [ 181].Ferrari‚Äôs [41] benchmark consists of 28
industrial requirement documents from various application domains, with requirement models manually constructed
based on these documents. This dataset can be used to evaluate the performance of LLMs in requirement modeling
tasks.
3.1.4 Requirements Specification. Requirements specification is the process of converting the analyzed requirements
into the software requirements specification, which provides detailed descriptions of the system‚Äôs functional, per-
formance, and interface requirements. Krishna‚Äôs [90] benchmark adopts a manually created software requirements
specification that adheres to IEEE specifications [ 1]. This benchmark undergoes rigorous review by a team of software
engineering experts and includes the problem background, stakeholder information, functional and non-functional
requirements, and use case analysis.
3.1.5 Requirement Validation. The purpose of requirements validation is to ensure that the requirements accurately
reflect the stakeholders‚Äô needs [157].
Reinpold [143] benchmark focuses on requirements validation in the smart grid domain, covering system specifica-
tions and requirements descriptions. In contrast, rSDE-Bench [67] focuses on the fields of web and game development.
It includes 53 coding tasks and 616 requirement instances, and utilizes unit tests for automated evaluation. Additionally,
REQuestA [39] conducts requirements validation from the question answer (QA) perspective. This dataset includes
387 QA pairs, covering common validation issues.
Manuscript submitted to ACMSoftware Development Life Cycle Perspective: A Survey of Benchmarks for Code Large Language Models and Agents 9
Table 3. Benchmarks for software design tasks.
Tasks Benchmarks
Architectural Design /
Detailed
DesignUI Design Rico[29], SCapRepo and ScreenRepo[167], Screen2words[163], ReDraw[126]
Algorithm Design CLRS [159]
Database Design /
Summary - Requirement Engineering
(1)Benchmark coverage across requirement engineering tasks is still incomplete. Existing benchmarks
primarily focus on requirement elicitation and validation, while key areas such as modeling, specification,
classification, and especially requirement management remain sparse or entirely absent.
(2)The challenges in constructing benchmarks for this phase include a lack of standardization and
limited generalizability . First, most datasets are collected independently and are not open-source, which
restricts reproducibility and fair comparison across models and studies. Second, some benchmarks rely on
specific cases or proprietary datasets, which limits their generalizability and may not fully capture the diversity
of real-world requirements engineering scenarios.
3.2 Software Design
The software design phase is the process of transforming the collected requirements into a software architecture
blueprint [ 84]. This phase includes architecture design and detailed design, such as user interface (UI) design, algorithm
design, and database design. The related tasks and benchmarks are shown in Table 3.
3.2.1 Architectural Design. Architectural design is the process of creating the overall structure and organization of a
software system‚Äôs components, interactions, and relationships. Unfortunately, we do not identify relevant benchmarks.
3.2.2 Detailed design. Detailed design is the process of refining the software architecture into a more detailed repre-
sentation. This task includes designing the user interface (UI), algorithms, and databases.
UI Design. UI Design refers to the process of designing UI for software applications or websites. Current UI Design
benchmarks predominantly focus on the mobile development domain. Rico [29] is the first large scale graphical
user interface (GUI) dataset specifically designed for mobile application UI analysis. It provides resources such as
screenshots, view hierarchies, and semantic annotations, covering 27 application domains. In contrast, SCapRepo
and ScreenRepo [167] focus on the GUI description and retrieval. The former builds a database pairing screen-
shots with descriptions, while the latter creates a resource library containing 303,000 screenshots for GUI retrieval.
Screen2Words [163] emphasizes semantic interaction of UI pages rather than visual appeal. It is the first large scale
manually annotated android screen functional description dataset. In contrast, ReDraw [126] focuses more on un-
derstanding components. It collects application interface and component annotation information from non-game
applications on Google Play.
Algorithm Design. Algorithm design is the process of solving a problem by creating a series of steps or rules. The
CLRS [159] benchmark is designed to assess the computational reasoning abilities of LLMs. The benchmark includes
30 fundamental algorithms selected from a classic textbook, covering key topics such as sorting, searching, dynamic
programming, and others.
Manuscript submitted to ACM10 Wang et al.
Database Design. Database design refers to the design of efficient and secure systems for data storage and retrieval.
However, no benchmark related to database design have been identified.
Summary - Software Design
(1)Benchmark coverage is imbalanced across design tasks. Current benchmarks focus heavily on UI
design, while architectural and database design tasks lack dedicated benchmarks.
(2)The diversity of UI design tasks is limited, and there are still limitations in algorithm design.
Existing algorithm design benchmarks mainly focus on specific algorithm tasks, lacking cross-domain applica-
bility. Moreover, UI design benchmarks are predominantly limited to mobile application domain, and there is a
lack of a comprehensive framework to assess the capabilities of CodeLLMs across diverse design scenarios.
3.3 Software Development
Software development is the core phase of SDLC, where software design is translated into executable code. In the
following subsections, we provide a detailed overview of the key tasks in software development and their corresponding
benchmarks, as summarized in Table 4.
3.3.1 Code Generation. The objective of code generation is to automatically generate code based on natural language
descriptions. In this field, benchmarks are categorized into various levels according to the granularity of the code
generation tasks, such as function-level, class-level, and repository-level benchmarks. Furthermore, there are benchmarks
focused on specific areas, including competitive programming, multilingual code generation, domain-specific tasks, and
others.
Function-level benchmarks. Function-level benchmarking focuses on the implementation of individual functions,
making it suitable for small scale code generation task. HumanEval [23] contains 164 Python programming problems
written by humans, each accompanied by an average of 9.6 test cases. HumanEval+ [112] substantially increases
the number of test cases for each problem, with an average of 764.1 test cases, improving the coverage of edge
cases and boundary conditions. HumanEval-ET [34] further introduces approximately 100 test cases. In contrast,
MBPP [10] focuses on basic programming tasks and includes approximately 974 problems with relatively low complexity.
MBPP+ [112] extends the average number of test cases to 21.2, resulting in a 35-fold improvement in test rigor.
MBPP-ET [34] also introduces approximately 100 additional test cases to strengthen boundary condition evaluation.
Additionally, BigCodeBench [209] targets high complexity scenarios, focusing on evaluating the model‚Äôs ability to
handle long prompts and invoke specific libraries, making it an important supplement to function-level benchmarks.
Class-level benchmarks. Class-level benchmarks focus on the design of classes and the implementation of their
attributes and methods, specifically for object-oriented programming. ClassEval [37] includes 100 class-level code
generation examples, used to assess the performance of LLMs in class-level code generation tasks.
Repository-level benchmarks. To simulate real-world development more accurately, repository-level benchmarks have
emerged in recent years. Early benchmarks such as CoNaLa [184] and ConCode [75] are not designed for LLMs and lack
test cases, limiting their applicability. CoNaLa focuses on Python, while ConCode is designed for Java. CoderEval [185]
is the first benchmark to focus on context dependent, non-independent function generation scenarios. It extracts
complete task contexts from real open-source projects and supports both Java and Python. EvoCodeBench [98]
addresses data leakage by introducing dynamic dataset updates. HumanEvo [195] simulates project evolution over time
Manuscript submitted to ACMSoftware Development Life Cycle Perspective: A Survey of Benchmarks for Code Large Language Models and Agents 11
Table 4. Benchmarks for software development tasks.
Tasks Benchmarks
Code GenerationFunction levelHumanEval[23], HumanEval+[112], HumanEval-ET[34],
MBPP[10], MBPP+[112], MBPP-ET[34], BigCodeBench[209]
Class level ClassEval[37]
Repository levelConCode[75], CoderEval[185], EvoCodeBench[98],
HumanEvo[195], CoNaLa[184], PragmaticCode[5],
FEA-Bench[102], Deveval[99], CodeagentBench [191]
CompetitionsAPPS[ 60], CodeContests[ 104], LiveCodeBench[ 130], LeetCode[ 56],
CodeForces[140]
MultimodalMMCode[100], Plot2Code[171], Spider2-v[15], Web2code[189],
Design2Code[149] MatPlotBench[183] BabelBench[165]
MultilingualHumanEval-X[197], MathQA-X[9], MBXP[9], MultiPL-T[16],
Multilingual HumanEval[9], R-benchmark[124], Rtl-repo[7],
Verilogeval[115], ChiBench[150], FVEval[87]
Multi-domainDS-1000[91], RoboScript[20], PathBench[63], CoBRA[123],
Robotouille[50], EPIC-Kitchens[28], BioCoder[154],
MLAgentBench[70] ML-bench[153] Deep-Bench[27]
OthersJuICe[4], Exec-CSN[174] , Race[196], LessLeak-Bench[203],
Domaineval[207], StudentEval[11]
Text-to-SQLSingle-table WikiSQL [202]
Variants of SpiderSpider[187], Spider-Syn[45], Spider-DK[46], Spider-Realistic[30],
CSPIDER[125]
Complex SQL query BIRD[97], KaggleDBQA[94]
Multi-turn dialogue SParC[188], CoSQL[186]
Others Ehrsql[95], Termite[141]
Code CompletionRepository-Level RepoBench[116], RepoEval[190]
Cross-File CrossCodeEval[33]
Code Understand-
ing and ReasoningQAInfibench[101],CodeQA[111], CS1QA[93], CoSQA[69],
CoSQA+[48], DQABench[198]
Reasoning CRUXEval[51], CRUXEVAL-X[176], CodeMMLU[122], REval[21]
Understanding CodeApex[44],SpecEval[120],FAUN-Eval[64]
Code Translation PolyHumanEval[155], MCEval[18], CodeTransOcean[179], MultiPL-E[17]
Code Summarization CODE-NN[74], DeepCom[65], TL-CodeSum[66]
Type Inference Lambdanet[169], IdBench[162]
Code Retrieval DeepCS[53], CodeSearchNet[72]
Non-FunctionalReCode[ 164], RedCode[ 55], RMCBench[ 22], AdvBench[ 210], HMCorp[ 177], Dùõº-C8[131],
ECCO[ 161], EffiBench[ 68], GEC[ 136], Mercury[ 36], EvalPerf[ 113], ENAMEL[ 139],
FairCoder[38], SocialBias-Bench[109], Zhang‚Äôs[192], CodeArena[180], Galeras[145]
by incorporating version histories. PragmaticCode [5] focuses on environment consistency. It uses Java repositories
to build tasks and is equipped with complete dependency configurations, supporting actual compilation and execution
processes. FEA-Bench [102] focuses on incremental feature development. Its tasks are derived from pull requests across
83 code repositories and aim to evaluate the model‚Äôs ability to edit code in multi-component codebases. Deveval [99] is
suitable for cross-domain code generation. It includes 1,874 manually annotated samples from 117 repositories. Finally,
CodeagentBench [191] focuses on evaluating AI agents for repository-level code generation. Its data is sourced from
Manuscript submitted to ACM12 Wang et al.
open-source Python repositories and includes documentation, dependencies, installation scripts, and test suites to
simulate real development environments.
Competitive Code Generation benchmarks. Several benchmarks for code generation are derived from well-known
competitive programming platforms. APPS [60] contains 10,000 Python programming problems, covering three difficulty
levels: beginner, interview, and competition. In contrast, CodeContests [104] focuses on high-difficulty competitive
programming tasks. It evaluates algorithmic reasoning and data structure, using problems collected from the Codeforces
platform. LiveCodeBench [130] emphasizes reasoning and problem solving in complex algorithmic tasks. To avoid
data leakage, the benchmark is continuously updated with problems from platforms such as LeetCode, AtCoder, and
Codeforces. Guo et al. [ 56] propose a LeetCode competition benchmark that includes 180 recent contest problems
from July 2023 to January 2024. Each problem is accompanied by 100 test cases. CodeForces [140] supports multiple
programming languages and solution types, collecting various programming problems from the Codeforces platform.
Multilingual benchmarks. To support a broader range of programming language evaluations, researchers have
proposed several multilingual benchmarks. HumanEval-X [197] extends HumanEval to five languages: Python, C++,
Java, JavaScript, and Go. MultiPL-T [16] focuses on low-resource programming languages. R-benchmark [124]
is the first benchmark designed for the R programming language. Recent studies also explore code generation for
hardware description languages. For Verilog, representative benchmarks include RTL-Repo [7],VerilogEval [115], and
ChiBench [150]. For SystemVerilog, FVEval [87] broadens code generation evaluation in hardware design scenarios.
Multimodal benchmarks. Multimodal large language models (MLLMs) have driven the emergence of new programming
paradigms, and several benchmarks have been proposed to evaluate their capabilities. MMCode [100] is the first
benchmark focused on image-to-code generation, covering a variety of tasks ranging from basic programming to
mathematical problem solving. Plot2Code [171] focuses on the reconstruction of scientific data visualization charts and
evaluates the model‚Äôs ability to generate reproducible Python code from images. Similarly, MatPlotBench [183] also
focuses on data visualization techniques, but it differs by handling the multimodal conversion from natural language
and structured data to graphical outputs. Spider2-v [15] focuses on more complex real-world enterprise systems,
emphasizing MLLMs and agents‚Äô ability to handle multiple tasks simultaneously. Web2Code [189] focuses on web
development and evaluates the model‚Äôs ability to generate HTML code from webpage screenshots. It also introduces a
QA module to assess the model‚Äôs understanding of semantics. Design2Code [149] focuses on web page reconstruction,
evaluating the model‚Äôs ability to generate HTML and CSS from web pages and ensuring visual consistency with the
original page. Finally, BabelBench [165] is one of the most comprehensive multimodal benchmarks, focusing on
evaluating the model‚Äôs generalization ability and cross task adaptability. It includes assessments of perception, reasoning,
and code generation across image, text, and tabular data.
Multi-domain benchmarks. As code generation extends into specialized domains, benchmarks for various fields
have emerged. DS1000 [91] focuses on the field of data science, aiming to evaluate the model‚Äôs performance in data
analysis tasks. BioCoder [154] focuses on bioinformatics and includes tasks such as parsing biological data formats and
using bioinformatics tool APIs. MLAgentBench [70] assesses the ability of LLM agents to conduct machine learning
experiments, including file operations, code execution, and result analysis. It covers 15 subdomains such as image
classification and time series forecasting. Building on this, MLBench [153] consists of two components: ML LLM Bench
and ML Agent Bench. ML LLM Bench evaluates the ability to generate Python and Bash scripts, while ML Agent Bench
measures agent performance in tasks like environment setup, data acquisition, and cross-file information retrieval.
DeepBench [27] focuses on code generation in deep learning workflows. It covers the full pipeline from preprocessing
to inference and supports tasks such as classification, regression, and recommendation across different data modalities,
Manuscript submitted to ACMSoftware Development Life Cycle Perspective: A Survey of Benchmarks for Code Large Language Models and Agents 13
including tabular data, images, and text. In robotics, several benchmarks including RoboScript [20],PathBench [63],
CoBRA [123],Robotouille [50], and the widely used EPIC Kitchens dataset [ 28] enable evaluation of control, path
planning, and manipulation tasks. These benchmarks support research on code generation in embodied AI.
Other benchmarks. Some benchmarks focus on specific aspects of code generation. JuICe [4] is the first to explore
interactive code generation. Exec-CSN [174] evaluates code executability and introduces a sandboxing process using
LLMs to ensure that generated code can run. Race [196] assesses code quality from four key perspectives: readability,
maintainability, correctness, and runtime efficiency. LessLeak-Bench [203] focuses on detecting and mitigating
data leakage issues in CodeLLM benchmarks. DomainEval [207] focuses on cross-domain generalization, covering
2,454 topics across six application areas. StudentEval [11] is designed for educational use and includes 1,749 basic
programming prompts.
3.3.2 Text-to-SQL. LLMs have become a key factor in driving the development of text-to-SQL tasks. We have classified
the existing benchmarks into the following categories: single-table benchmarks, Spider and its variants, complex SQL
benchmarks, multi-turn dialogue benchmarks, and other related benchmarks.
Single-table Benchmarks. WikiSQL [202] contains over 24,000 tables from Wikipedia. However, these queries are
relatively simple, primarily limited to single-table queries, which restricts their effectiveness in real-world applications.
Spider and its variants. Spider [187] is the most widely used benchmark for text-to-SQL tasks. It contains 10,181
natural language questions and 5,693 complex SQL queries, spanning 138 domains and 200 relational databases. Despite
its large scale, many queries include overly specific references to column names and values, which limits its ability
to reflect realistic user queries. To improve the realism and difficulty of the benchmark, several variants of Spider
have been proposed. Spider-Syn [45] enhances linguistic diversity through synonym replacement and structurally
complex rewrites. Spider-DK [46] incorporates domain knowledge to simulate more natural query expressions.
Spider-Realistic [30] omits explicit schema information, such as column names, to better reflect real users queries.
CSpider [125] is a Chinese translation of Spider that preserves the original schema structure and data distribution.
These extended versions collectively improve the realism of the Spider benchmark, making it more challenging and
representative for evaluating text-to-SQL model.
Complex SQL benchmarks. BIRD [97] focuses on query efficiency in the presence of complex database schemas and
external knowledge reasoning. It covers 95 databases across 37 specialized domains. In contrast, KaggleDBQA [94] is
closer to real-world application scenarios. It is built on eight real databases, and its SQL queries rely less on explicit
column names, which better simulates actual user query behavior.
Multi-turn dialogue benchmarks. SParC [188] extends the Spider benchmark to multi-turn dialogue scenarios, aiming
to evaluate a model‚Äôs contextual understanding and reasoning capabilities in interactive queries. It covers 138 domains
and 200 databases. CoSQL [186] constructs a larger dataset, containing over 30,000 dialog turns and more than 10,000
SQL queries. It further strengthens the evaluation of a model‚Äôs ability to handle complex relational queries, such as
multi-table joins and aggregation operations.
Other benchmarks. EhrSQL [95] is the first text-to-SQL benchmark specifically designed for the clinical domain.
Termite [141] focuses on data contamination issues by introducing an encrypted dataset to construct the benchmark,
ensuring data timeliness and reducing overlap with pre-trained corpora.
3.3.3 Code Completion. Code completion aims to assist developers by inferring the next line of code or completing code
snippets based on the provided context. This task can be divided into two categories: repository-level code completion
and cross-file code completion.
Manuscript submitted to ACM14 Wang et al.
Repository-Level Code Completion. RepoBench [116] is a benchmark for evaluating repository-level code completion,
comprising three subtasks: RepoBench-R (cross-file retrieval), RepoBench-C (context-aware completion), and RepoBench-
P (end-to-end workflow evaluation). In comparison, RepoEval [190] emphasizes the functional validity and semantic
correctness of code completions. Built from high-quality GitHub repositories, it covers line-level, API call-level, and
function body-level tasks, with correctness verified through the execution of actual unit tests.
Cross-File Code Completion. CrossCodeEval [33] is a diverse, multilingual cross-file code completion benchmark
designed to evaluate a model‚Äôs ability to understand cross-file contexts. This benchmark is constructed from real-world
open-source projects across four popular programming languages: Python, Java, TypeScript, and C#.
3.3.4 Code Understanding and Reasoning. Code understanding and reasoning are essential for CodeLLMs. Code
comprehension enables CodeLLMs to analyze and understand both the syntax and semantics of code, and code
reasoning enables CodeLLMs to infer logical relationships within the code and predict its behavior.
Question-Answering (QA). Infibench [101] is designed for open-ended code question answering, containing 234
Stack Overflow questions, covering 15 programming languages and 5 core domains. CodeQA [111] focuses on source
code understanding by automatically generating question-answer pairs from comments in Java and Python codebases.
CS1QA [93] focuses on introductory programming education and includes 9,237 "question-code-answer" triples. In the
field of code retrieval-based question answering, CoSQA [69] provides 20,604 real user queries paired with corresponding
Python functions. Its enhanced version, CoSQA+ [48], improves retrieval accuracy and question answering quality by
aligning each query with multiple semantically relevant code snippets. Finally, DQABench [198] expands question
answering benchmarks to the database field. It includes topics such as general database concepts, specific database
system knowledge, and real-world database tasks.
Code Reasoning. CRUXEval [51] divides tasks into two reasoning modes: input-to-output (CRUXEval-I) and
output-to-input (CRUXEval-O), evaluating code understanding through 800 Python functions with input-output
pairs. Its multilingual extension, CRUXEval-X [176], supports cross-lingual code reasoning evaluation. In contrast,
CodeMMLU [122] uses fine-grained multiple-choice questions, covering over 50 domains and 10 languages with 20,000
questions. REval [21] shifts the focus to program intermediate states, introducing metrics such as code coverage and
execution path prediction to evaluate semantic understanding beyond the final output.
Code Understanding. CodeApex [44] evaluates the capabilities of LLMs in program understanding through 250
multiple-choice questions, 476 C++ programming tasks, and 1,330 erroneous code snippets. Complementing this,
SpecEval [120] assesses code understanding through four hierarchical tasks: judgment, selection, filling, and generation,
providing a structured evaluation from syntactic cues to deeper reasoning. In contrast, FAUN-Eval [64] focuses on
evaluating model performance in three realistic scenarios: question answering, bug localization, and code editing,
covering five programming languages.
3.3.5 Code Translation. Code Translation is a task that involves converting code from one programming language
to another. PolyHumanEval [155] extends the HumanEval benchmark to support 14 programming languages. It
evaluates code translation by verifying whether the translated code yields the same output as the original code when
provided with the same input. MCEval [18] offers a broader evaluation, covering 40 languages and 16,000 samples.
CodeTransOcean [179] further emphasizes the quality of code translation, providing large-scale multilingual data and
systematically benchmarking cross-lingual translation performance. Additionally, MultiPL-E [17] translates an existing
Python benchmark into 18 different languages, covering high-resource/low-resource languages and static/dynamic
typing.
Manuscript submitted to ACMSoftware Development Life Cycle Perspective: A Survey of Benchmarks for Code Large Language Models and Agents 15
3.3.6 Code Summarization. Code summarization refers to the task of generating natural language summaries for
code snippets, functions, or entire programs. CODE-NN [74] is an early dataset for C# and SQL summarization,
created by pairing StackOverflow post titles with code snippets from accepted answers. In contrast, DeepCom [65]
provides a larger scale dataset based on Java, sourced from GitHub repositories between 2015 and 2016. Building on this,
TL-CodeSum [66] introduces a more refined, method-level Java summarization benchmark, with data from GitHub
repositories during the same time period.
3.3.7 Type Inference. Type inference is the task of automatically determining the types of variables, functions, or
expressions in a programming language. Lambdanet [169] integrates 300 widely used TypeScript codebases from
GitHub, each containing a large number of user-defined type annotations and type variables. In contrast, IdBench [162]
focuses on evaluating the quality of identifier embeddings in source code. It provides semantic similarity scores
annotated by humans, making it also applicable to type inference task.
3.3.8 Code Retrieval. Code retrieval is the task of finding relevant code snippets or functions based on a given query.
CodeSearchNet [72] is a representative benchmark for this task. It contains around 6 million code-documentation
pairs, supports six programming languages, and is capable of supporting cross-language and diverse code retrieval
scenarios. DeepCS [53] evaluates Java code retrieval by extracting query and code snippet sets from Stack Overflow
and high-quality Java projects.
3.3.9 Non-functional Benchmarks. Non-functional benchmark assess the system‚Äôs performance, behavior, and con-
straints, rather than its specific functional correctness.
Robustness. The robustness of CodeLLMs refers to their ability to maintain stable performance in the presence of
input perturbations or noise. ReCode [164] assesses the behavior of models under a variety of input modifications,
covering over 30 types of natural transformations, such as documentation strings, function naming, formatting, and
more.
Security. The security of CodeLLMs is crucial for generating safe code and preventing exploitation by malicious actors.
RedCode [55] evaluates model security using 160 high-risk prompts based on function signatures and documentation,
along with 4,050 high-risk test cases in Python and Bash, to check if the model or agent generates harmful code.
RMCBench [22] assesses the model‚Äôs ability to prevent malicious code generation with 473 adversarial prompts in
text-to-code and code-to-code scenarios. AdvBench [210] provides 520 adversarial instructions to evaluate the model‚Äôs
defense against jailbreak attacks, becoming a key resource in language model security research.
Copyright. Copyright issues in CodeLLMs mainly involve the legal ownership of AI-generated code and potential
infringement risks. The HMCorp dataset [ 177], built on real-world Python and Java programming tasks, is currently the
largest benchmark for distinguishing AI-generated code from human-written code. To address possible infringement,
Dùõº-C8[131] targets plagiarism detection by collecting Java code snippets produced by both humans and models,
enabling the evaluation and identification of unauthorized code reuse.
Efficiency. The efficiency of CodeLLMs refers to their ability to optimize runtime performance, resource usage,
and execution time while maintaining the correctness of generated code. ECCO [161] focuses on evaluating code
efficiency in competitive programming settings. It includes 1,300 Python problems, each with an average of 3.1 public
and 17.3 private test cases. EffiBench [68] also targets competitive programming scenarios. Built on LeetCode problems,
it provides human-written reference solutions and uses execution time and memory usage as evaluation metrics.
GEC [136] evaluates the ability to optimize inefficient code by providing paired efficient and inefficient solutions.
Manuscript submitted to ACM16 Wang et al.
Mercury [36] introduces new metrics beyond functional correctness, combining correctness with runtime performance
across Python problems of varying difficulty. EvalPerf [113] is designed for performance sensitive scenarios, using
challenging inputs and lightweight tools to measure code performance. ENAMEL [139] builds on HumanEval and
includes problems ranging from easy to hard, along with human-written efficient reference solutions.
Bias. Bias in CodeLLMs appears as societal, cultural, gender, and racial biases in generated code. FairCoder [38]
focuses on biases in tasks like code generation and test case generation within sensitive areas such as recruitment,
education, and healthcare. SocialBias-Bench [109] provides a broader bias evaluation framework, covering seven
task categories with a total of 343 instances. It aims to assess whether models exhibit biases related to gender, race, or
income when performing programming tasks. In addition to social bias, Zhang et al. [192] propose a comprehensive
dataset covering 6 coding tasks and 30 real-world scenarios to evaluate provider bias in CodeLLMs and agents, which is
manifested as the preference for paid services from specific providers in code recommendation and generation.
Alignment with human preferences. Aligning LLMs with human preferences means that the outputs should align
with human values and aesthetics. CodeArena [180] includes 397 high-quality samples, covering 40 task categories
and 44 programming languages, providing a comprehensive evaluation aligns with human preferences.
Explainability. Explainability refers to the transparency and comprehensibility of a model‚Äôs behavior and decision-
making process, which increases user trust. Galeras benchmark [ 145] quantifies how factors like code complexity and
number of comments, influence model performance, thereby uncovering the underlying process of code generation.
Privacy. Privacy in CodeLLMs refers to safeguarding sensitive information during both training and inference.
Currently, there is a lack of benchmarks specifically designed to assess the privacy of CodeLLMs.
Summary - Software Development
(1)Current benchmarks provide comprehensive coverage across the software development phase,
with particular emphasis on code generation tasks. Existing code generation benchmarks encompass
diverse scenarios, including function-level, class-level, repository-level, multilingual, multimodal, and multi-
domain scenarios, offering a robust foundation for evaluating code generation capabilities.
(2)The limitations of existing software development benchmarks . In text-to-SQL benchmarks, most
queries are manually designed, failing to reflect real-world scenarios. Code summarization benchmarks lack
high-quality manually labeled data. Many code retrieval benchmarks are limited to English, ignoring multilin-
gual challenges. Type inference benchmarks do not cover complex or rare data types and overlook cross-type
collaborative inference. Benchmarks for code understanding and reasoning cannot assess the ability to under-
stand code in repositories.
3.4 Testing
Software testing aims to identify and fix defects, ensure compliance with requirements, and enhance software quality. The
following subsections will systematically outline the key tasks in software testing and their corresponding benchmarks,
as shown in Table 5.
3.4.1 Vulnerability Detection. Vulnerability detection aims to identify hidden security risks in codebases. Current
vulnerability detection benchmarks are primarily divided into two main categories: synthetic benchmarks and real-world
benchmarks.
Manuscript submitted to ACMSoftware Development Life Cycle Perspective: A Survey of Benchmarks for Code Large Language Models and Agents 17
Table 5. Benchmarks for testing tasks.
Tasks Benchmarks
Vulnerability
DetectionSynthetic
BenchmarksGREAT[59], MVD[211], FormAI[156]
Real-World
BenchmarksDiverseVul[ 25], Devign[ 205], Big-Vul[ 40], REVEAL[ 19],
VulDeePecker[ 106], VulnPatchPairs[ 144], SySeVR[ 105],
PrimeVul[ 32], Draper[ 208], VUDENC[ 166], Juliet[ 3],
CrossVul[133], CVEfixes[12]
Test Generation Methods2Test[158], SF110[43], SWTBench[129]
Assertion Generation Kande‚Äôs benchmark[86]
Code Editing PYCOMMITS[168], CoEdPilot [110]
Defect Detections JIT-Defects4J[132], Bears [121], Bugs.jar[146]
Synthetic benchmarks. GREAT [59] uses the ETH Py150 [ 85] dataset to simulate code errors by replacing variables in
Python functions. Each error has a corresponding correct version, with around 2 million training samples and 755,000
test cases. MVD [211] is one of the most comprehensive benchmarks in terms of vulnerability types. The data is sourced
from the SARD and NVD databases, containing 181,641 code snippets, covering 40 different types of vulnerabilities.
FormAI [156] proposed a C program vulnerability detection dataset generated by LLMs. It contains 112,000 (v1 version)
and 265,000 (v2 version) compilable code samples, each annotated with the vulnerability type and its specific location.
Real-World benchmarks. DiverseVul [25] is a large scale C/C++ vulnerability detection benchmark containing 18,945
vulnerable functions and 330,492 non-vulnerable functions across 155 common weakness enumeration (CWE) categories.
In contrast, Devign [205] provides high-quality samples labeled by experts and collected from four open source projects,
offering a more realistic reflection of the challenges in real-world vulnerability detection.
Big-Vul [40] simulates a realistic vulnerability distribution with a 1:20 ratio of vulnerable to clean code. REVEAL [19]
andVulDeePecker [106] provides datasets labeled by experts, collected from real-world projects such as Linux and
Chromium. VulnPatchPairs [144] and CVEfixes [12] map vulnerabilities to their corresponding patches, which is par-
ticularly useful for downstream tasks such as vulnerability localization and automated patch generation. SySeVR [105]
andJuliet [3] focus on the systematic collection of program samples for detailed security analysis. PrimeVul [32]
introduces refined annotation for higher accuracy. Draper [208] is a large scale benchmark but suffers from class im-
balance. VUDENC [166] focuses on Python vulnerabilities, broadening the range of languages studied. CrossVul [133]
further covers more than 40 programming languages and supports cross-language vulnerability detection evaluation.
3.4.2 Test Generation. Test generation involves the automatic or manual creation of test cases to validate the expected
behavior of a software system and to identify potential defects. Methods2Test [158] specializes in large-scale Java
unit test generation, offering 780,944 JUnit test-method pairs from 91,385 GitHub repositories. SF110 [43] includes 110
open-source Java projects and 23,886 classes, but it primarily consists of low-complexity classes and small projects,
limiting its effectiveness in complex scenarios. SWTBench [129] assesses whether the generated tests can reproduce
issues. The dataset contains over 1,900 samples, each with a problem description, reference patches, and a set of
standardized test cases.
3.4.3 Assertion Generation. Assertion Generation is a crucial task in the field of hardware design. Kande‚Äôs bench-
mark [86] consists of two manually crafted designs and eight modules derived from the designs of Hack@DAC
hardware security competitions.
Manuscript submitted to ACM18 Wang et al.
Table 6. Benchmarks for software maintenance tasks
Tasks Benchmarks
Program RepairDefects4J[82], SWE-bench[79], SWA-Bench [160], SWEE-Bench[160],
QuixBugs[108], HUMANEVALFIX[128], ManyBugs[92], BugsInPy[170],
TypeBugs[134]
Log Parsing Loghub[206], LogPM[58], Loghub-2.0[78],
Vulnerability RepairVul4j[13], ManyVuls4J[107], VjBench[172], VJBench-trans[172],
ExtractFix[47],
Decompilation WaDec[148], ExeBench[8]
API Misuse Detection ROBUSTAPI[201], APIMU4C[54]
Code Clone Detection POJ104[127], CodeNet[137], BigCloneBench[151]
3.4.4 Code Editing. Code editing is a task that involves modifying existing code, which is essential for the long-term
maintenance and evolution of software systems. PyCommits [168] is constructed from 1,650 open source Python
repositories, capturing real code editing operations reflected in commit histories, including additions, deletions, and
modifications. CoEdPilot [110] broadens the code editing benchmark across five programming languages: JavaScript,
Java, Go, Python, and TypeScript. It utilizes data gathered from the top 100 repositories with the highest star ratings on
GitHub for each language.
3.4.5 Defect Detection. Defect detection is a critical task aimed at identifying potential bugs or errors within a codebase,
playing a vital role in ensuring the reliability and correctness of software systems. JIT-Defects4J [132] focuses on
Just-In-Time defect prediction (JIT-DP) and includes 27,391 accurately labeled change records from 21 open-source
projects. In contrast, Bears [121] targets reproducible bugs and collects 251 real-world defect instances from 72 projects.
Meanwhile, Bugs.jar [146] offers a larger scale dataset, covering 20,948 methods from 8 popular Java projects, including
1,158 pairs of buggy and corresponding fixed methods.
Summary - Testing
(1)The coverage of benchmarks in the testing phase is limited . Current benchmarks primarily focus
on vulnerability detection and test generation tasks, while benchmarks for other tasks, such as assertion
generation, defect detection, and code editing, are underdeveloped and lack sufficient maturity.
(2)The limitations of existing testing phase benchmarks . The benchmarks used for testing mainly focus
on unit testing, with limited support for integration and interactive testing. Vulnerability detection benchmarks
emphasize static vulnerabilities, ignoring runtime and configuration vulnerabilities. Assertion generation
benchmarks are narrow, focusing on specific test cases without assessing broader application contexts. Code
editing benchmarks fall short in supporting collaborative development and evaluating code style consistency.
Defect detection benchmarks do not cover all types of defects encountered in real-world applications.
3.5 Software Maintenance
Software maintenance refers to the process of modifying and updating software after deployment to fix defects, improve
performance, and adapt to evolving requirements or environments. The following subsections present key maintenance
tasks and their corresponding benchmarks, as summarized in Table 6.
Manuscript submitted to ACMSoftware Development Life Cycle Perspective: A Survey of Benchmarks for Code Large Language Models and Agents 19
3.5.1 Program Repair. Program repair is the task of automatically fixing bugs in software code. It is essential for
ensuring the reliability and correctness of software systems. Defects4J [82] is the most widely used benchmark in
this field. It includes 357 bugs from five popular open source Java projects. Each bug is reproducible with associated
test cases, covering diverse defect types and complexity levels. QuixBugs [108] focuses on algorithmic problems,
offering 40 bugs with diverse failure modes that present more challenging repair scenarios. HumanEvalFix [128]
injects faults into HumanEval reference solutions, generating 164 buggy functions across six programming languages.
ManyBugs [92] is a key benchmark for C program repair, collecting 185 real-world bugs from nine large-scale projects,
each accompanied by developer patches and test cases. BugsInPy [170] extends program repair benchmark to the
Python ecosystem. SWE-bench [79] targets repository-level program repair tasks derived from GitHub issues and
pull requests. It provides multiple variants, including SWE-bench Lite, SWE-bench Verified, and SWE-bench Java, to
support different evaluation needs. SWA-Bench [160] evaluates the ability of AI agents to perform automated code
repair in real-world Python applications. It reconstructs historical dependency environments using Python scripts and
docker images. In comparison, SWEE-Bench [160] focuses on less popular Python projects and aims to evaluate the
code repair capabilities of CodeLLM agents in different environments.
3.5.2 Log Parsing. Log parsing refers to extracting meaningful information from log files generated by software.
Loghub [206] is currently the largest publicly available log parsing dataset, encompassing 16 domain-specific subsets
with a total of 77GB of log data. Each subset contains 2,000 manually labeled samples. However, the dataset contains
labeling errors. Khan et al. [ 89] proposed guidelines to address these issues. To improve accuracy, LogPM [58] focuses
on character level parsing and introduces detailed metrics for parameter extraction, replacing traditional accuracy
measures. Loghub-2.0 [78] expands the original dataset with 14 new high-quality subsets, each containing an average
of 3.6 million logs, improving data size, consistency, and reproducibility.
3.5.3 Vulnerability Repair. The vulnerability repair task aims to automatically fix vulnerabilities or security flaws
in software code, which is crucial for ensuring the security of software systems. Vul4J [13] is a reproducible Java
vulnerability dataset, containing 79 defects from 51 open-source projects, covering 25 CWE types. It includes test cases
that trigger the vulnerabilities and patches provided by developers, all manually verified. However, its limited size
and vulnerability types restrict its application. To address these limitations, ManyVuls4J [107] expands the number
of vulnerabilities to 103 and includes more projects and cases. VJBench [172] adds 42 real Java vulnerabilities not
covered by Vul4J and incorporates 12 new CWE categories. VJBench-trans [172] generates 150 functionally equivalent
variants by renaming identifiers and adjusting program structures, avoiding overlap with pre-trained large language
model corpora. Additionally, ExtractFix [47] focuses on C/C++ program repair, providing corresponding patches for
each program.
3.5.4 Decompilation. Decompilation is a task that involves converting low-level machine code or bytecode back into
high-level source code. WaDec [148] is a dataset consisting of 51,768 C programs from Hugging Face and 377 additional
samples from GitHub, all compiled into the WebAssembly (Wasm) format. It is designed to evaluate model capabilities
in source-to-bytecode translation and comprehension. ExeBench [8] focuses on multilingual decompilation tasks,
encompassing executable files in C, C++, Python, and Java, along with their corresponding disassembly code.
3.5.5 API Misuse Detection. API Misuse Detection is a task that involves identifying incorrect or improper usage
of APIs within software code. ROBUSTAPI [201] is a dataset designed to evaluate the capabilities of CodeLLMs in
detecting API misuse. It comprises 1,208 real-world programming questions collected from Stack Overflow, covering 18
Manuscript submitted to ACM20 Wang et al.
representative Java APIs. For C language API misuse, APIMU4C [54] provides 2,172 handcrafted test cases and 100
real-world misuse cases extracted from widely used open-source projects.
3.5.6 Code Clone Detection. Code clone detection identifies duplicate or similar code snippets in repositories, which
is crucial for maintaining code quality, reducing redundancy, and improving software maintainability. POJ104 [127]
includes 104 programming problems, each with 500 different C/C++ implementations. It is the default dataset for code
clone detection on the CodeXGlue platform, used to evaluate models in scenarios with multiple solutions to a single
problem. CodeNet [137] expands on programming languages and task variety, with over 14 million code snippets
from AOJ and AtCoder platforms, covering 55 languages. BigCloneBench [151] focuses on large-scale Java code clone
detection across 25,000 real-world projects, evaluating clone detection in real software engineering environments.
Summary - Software Maintenance
(1)Benchmark in the software maintenance phase still suffers from limited coverage. Existing
benchmarks are primarily focused on Program Repair and Vulnerability Repair, whereas benchmarks for other
tasks, such as Log Parsing, Decompilation, API Misuse Detection, and Code Clone Detection, remain under
development and have not yet reached full maturity.
(2)Benchmark limitations in the software maintenance phase. While benchmarks for Program Repair
and Vulnerability Repair have matured, they do not fully reflect the continuous evolution of real-world code.
Log Parsing benchmarks face challenges in annotation quality and domain diversity. The API Misuse Detection
benchmark is limited to Java and C, with no support for other languages. Additionally, Code Clone Detection
benchmarks are still lacking in identifying logic similarity, not just syntactical similarity.
3.6 Cross-Phase Benchmarks
Unlike traditional benchmarks that focus on isolated phases of the SDLC, recent studies have proposed comprehensive
cross-phase benchmarks to systematically evaluate the performance of CodeLLMs in realistic software engineering
scenarios.
CodeXGLUE [118] is one of the earliest and most widely adopted cross-task benchmark, covering core tasks
such as code clone detection, defect detection, code generation, and completion. HumanEvalPack [128] extends
the original HumanEval to six programming languages, focusing on code generation, repair, and explanation tasks.
Additionally, XCodeEval [88] introduces an execution engine for code to evaluate models‚Äô performance in generating
code in real-world environments. It covers seven tasks, including code understanding and generation, and supports 17
programming languages. CodeScope [178] expands on this by supporting eight task types and up to 43 programming
languages, with evaluation based on the correctness of execution. In comparison, DevBench [96] focuses on a systematic
evaluation across the SDLC, making it the only benchmark to cover all SDLC phases, supporting four widely used
languages. ProjectEval [114] evaluates the ability of agents to generate project-scale Python applications from natural
language instructions and automates verification through simulated user interactions. Building on SWE-Bench, SWE-
PolyBench [142] extends the scope to multi-language and multi-task scenarios, emphasizing agents‚Äô performance in
defect repair, feature addition, and code refactoring tasks, across four widely used languages.
Manuscript submitted to ACMSoftware Development Life Cycle Perspective: A Survey of Benchmarks for Code Large Language Models and Agents 21
3.7 Summary
(1)The coverage of the current benchmarks. The current benchmarks remain significantly imbalanced across
the SDLC. Most existing benchmarks are focused on the software development phase, while upstream phases such
as Requirements Engineering and Software Design have received limited attention. In the development phase, the
evaluation landscape is relatively mature, with several widely adopted benchmarks covering function-level, class-level,
repository-level, cross-file, multilingual, and multimodal dimensions. These benchmarks provide a broad assessment of
code generation capabilities. In contrast, benchmark coverage in Requirements Engineering is notably lacking. Key
tasks, such as requirements management, still lack publicly available datasets. Many benchmarks in this phase are
independently constructed by researchers, limiting openness and reusability. Similarly, in the Software Design phase,
there is no established framework for evaluating tasks like architectural and database design, making it difficult to
measure model performance effectively.
(2)The evolutionary trend of the current benchmarks. The evolution of the benchmarks exhibits a clear shift
from basic capability assessment toward more realistic and application-oriented evaluation. Early benchmarks, such as
CoNaLa, mainly evaluated the ability of models to generate simple code snippets. Later, benchmarks like HumanEval
and MBPP introduced function-level tasks with straightforward structures, designed to test basic syntactic and semantic
understanding. As evaluation needs grew, ClassEval incorporated class-level tasks, CrossCodeEval addressed cross-file
dependencies, and SWE-bench extended the scope to the repository level. Recent benchmarks have added further
complexity. For example, FEA-Bench targets incremental functional development, while HumanEvo simulates code
evolution through version updates. This progression reflects not only an increase in task complexity but also a broader
expansion in evaluation dimensions, including multilingual settings, multimodal tasks, and diverse application domains.
Overall, benchmarks for CodeLLMs are evolving to become more challenging, realistic, and aligned with practical
software engineering scenarios. This trend places higher demands on model capabilities and introduces new challenges
in ensuring the scientific rigor of evaluation frameworks.
Answer to RQ1 ‚Äì Current benchmarks for CodeLLMs have relatively limited coverage in the requirements
engineering and software design phases. In contrast, benchmarks in the software development phase are the
most numerous and widely adopted. Additionally, there is an emerging trend of benchmarks evolving from
evaluating fundamental capabilities toward assessments within realistic engineering environments.
4 STATISTICS AND ANALYSIS OF STUDIED BENCHMARKS
To address RQ2, we conduct a comprehensive analysis of the relevant information from existing benchmarks. Specifically,
Fig. 4 illustrates the frequency distribution of benchmark usage; Fig. 5 shows the distribution of release years for these
benchmarks; Fig. 6 presents the prevalence of different programming languages across the benchmarks; and Fig. 7
depicts the distribution of programming language usage across various phase of the SDLC.
The usage frequency of benchmarks . As shown in Fig. 4: ‚ù∂Code generation tasks are the most frequently applied
tasks in research, with the usage frequency of related benchmarks significantly higher than those of other tasks. Among
them, HumanEval has been used 62 times, making it the most frequently used benchmark. ABPP (46 papers) and APPS
(23 papers) are also widely adopted, indicating that this task is receiving considerable attention in current research. ‚ù∑
The text-to-SQL task is also highly regarded, with commonly used benchmarks including Spider (26 papers), BIRD (13
papers), and Spider-Realistic (7 papers), all of which have been practically applied in various studies. ‚ù∏In other task
Manuscript submitted to ACM22 Wang et al.
/uni00000013 /uni00000015/uni00000013 /uni00000017/uni00000013 /uni00000019/uni00000013
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
/uni00000017/uni00000018/uni00000019/uni00000019/uni0000001a/uni0000001a/uni0000001b/uni0000001b/uni0000001c/uni0000001c/uni00000014/uni00000013/uni00000014/uni00000014/uni00000014/uni00000016/uni00000014/uni00000016/uni00000014/uni0000001b/uni00000015/uni00000016/uni00000015/uni00000019/uni00000017/uni00000019/uni00000019/uni00000015
Fig. 4. Usage frequency of current
main benchmarks
/uni00000033/uni00000055/uni00000048/uni00000015/uni00000013/uni00000014/uni0000001b /uni00000015/uni00000013/uni00000014/uni0000001b /uni00000015/uni00000013/uni00000014/uni0000001c /uni00000015/uni00000013/uni00000015/uni00000013 /uni00000015/uni00000013/uni00000015/uni00000014 /uni00000015/uni00000013/uni00000015/uni00000015 /uni00000015/uni00000013/uni00000015/uni00000016 /uni00000015/uni00000013/uni00000015/uni00000017/uni00000013/uni00000014/uni00000013/uni00000015/uni00000013/uni00000016/uni00000013/uni00000017/uni00000013/uni00000018/uni00000013/uni00000031/uni00000058/uni00000050/uni00000045/uni00000048/uni00000055/uni00000003/uni00000052/uni00000049/uni00000003/uni00000025/uni00000048/uni00000051/uni00000046/uni0000004b/uni00000050/uni00000044/uni00000055/uni0000004e/uni00000056
/uni00000015
/uni00000013 /uni00000013 /uni00000013 /uni00000013 /uni00000013/uni00000015/uni00000018
/uni00000014 /uni00000014/uni00000013 /uni00000013/uni00000014 /uni00000014/uni00000013/uni00000014/uni00000015/uni00000019 /uni00000019
/uni00000015/uni0000001b/uni0000001c/uni00000014/uni0000001a/uni00000018/uni00000014
/uni00000014/uni00000015/uni00000018
/uni00000014/uni00000018
/uni00000016/uni00000015/uni00000019/uni00000018
/uni00000013/uni00000014 /uni00000014/uni00000015/uni00000016/uni00000017/uni00000018/uni00000035/uni00000048/uni00000054/uni00000058/uni0000004c/uni00000055/uni00000048/uni00000050/uni00000048/uni00000051/uni00000057/uni00000003/uni00000028/uni00000051/uni0000004a/uni0000004c/uni00000051/uni00000048/uni00000048/uni00000055/uni0000004c/uni00000051/uni0000004a
/uni00000036/uni00000052/uni00000049/uni00000057/uni0000005a/uni00000044/uni00000055/uni00000048/uni00000003/uni00000027/uni00000048/uni00000056/uni0000004c/uni0000004a/uni00000051
/uni00000036/uni00000052/uni00000049/uni00000057/uni0000005a/uni00000044/uni00000055/uni00000048/uni00000003/uni00000027/uni00000048/uni00000059/uni00000048/uni0000004f/uni00000052/uni00000053/uni00000050/uni00000048/uni00000051/uni00000057/uni00000037/uni00000048/uni00000056/uni00000057/uni0000004c/uni00000051/uni0000004a
/uni00000036/uni00000052/uni00000049/uni00000057/uni0000005a/uni00000044/uni00000055/uni00000048/uni00000003/uni00000030/uni00000044/uni0000004c/uni00000051/uni00000057/uni00000048/uni00000051/uni00000044/uni00000051/uni00000046/uni00000048Fig. 5. Distribution of publication years for benchmarks across all phases of the SDLC
areas, code search tasks often use CodeSearchNet (18 papers), while program repair tasks typically rely on Defects4J
(13 papers). Additionally, the cross-task benchmark CodeXGLUE has been adopted by 11 papers, indicating that the
community‚Äôs interest in cross-phase scenarios is steadily increasing.
The distribution of publication years for existing benchmarks across SDLC Phases. As shown in Fig. 5, ‚ù∂
The number of benchmarks focused on the software development phase is significantly higher than those in other
phases. This trend accelerated notably between 2023 and 2024, reflecting the growing research interest in this area.
In contrast, benchmarks for software design and requirements engineering remain underrepresented, indicating an
imbalance in current research priorities. ‚ù∑Overall, the development of benchmarks progressed slowly before 2022,
then rapidly increased, reaching its peak in 2024, marking a clear turning point.
The distribution of benchmarks in programming languages. As shown in Fig. 6, ‚ù∂Python has the highest level
of support, appearing in 100 benchmarks, followed by Java (64 benchmarks) and C/C++ (46 benchmarks), emphasizing
their dominance in CodeLLM benchmarks. ‚ù∑Languages like JavaScript, Go, PHP, C#, TypeScript, Ruby, and SQL are
supported in only 13 to 27 benchmarks, while most other languages appear in fewer than 10 benchmarks. ‚ù∏Overall,
the distribution of programming languages in benchmarks closely aligns with their popularity.
The distribution of programming languages across different SDLC phases. As shown in Fig. 7: ‚ù∂Python
is the most commonly used language overall, but the most prevalent language varies across different phases. C/C++
dominates the testing phase, Java is the dominant language in the maintenance phase, and Python leads in the other
phases. ‚ù∑This trend may be related to the characteristics of each language. The efficiency of C++ makes it particularly
suitable for testing scenarios with high performance requirements. But its complex syntax and significant maintenance
overhead reduce its usage in software maintenance. In contrast, Java‚Äôs relatively simple syntax and robust exception
handling capabilities enhance maintainability, making it more frequently used in benchmarks during the maintenance
phase.
Manuscript submitted to ACMSoftware Development Life Cycle Perspective: A Survey of Benchmarks for Code Large Language Models and Agents 23
/uni00000013 /uni00000015/uni00000013 /uni00000017/uni00000013 /uni00000019/uni00000013 /uni0000001b/uni00000013 /uni00000014/uni00000013/uni00000013
/uni00000031/uni00000058/uni00000050/uni00000045/uni00000048/uni00000055/uni00000003/uni00000052/uni00000049/uni00000003/uni00000025/uni00000048/uni00000051/uni00000046/uni0000004b/uni00000050/uni00000044/uni00000055/uni0000004e/uni00000056/uni00000026/uni00000036/uni00000036/uni00000039/uni00000048/uni00000055/uni0000004c/uni0000004f/uni00000052/uni0000004a/uni00000027/uni00000044/uni00000055/uni00000057/uni00000035/uni00000044/uni00000046/uni0000004e/uni00000048/uni00000057/uni0000002f/uni00000058/uni00000044/uni0000002d/uni00000058/uni0000004f/uni0000004c/uni00000044/uni0000002b/uni00000037/uni00000030/uni0000002f/uni00000025/uni00000044/uni00000056/uni0000004b/uni00000035/uni00000033/uni00000048/uni00000055/uni0000004f/uni00000036/uni00000046/uni00000044/uni0000004f/uni00000044/uni00000036/uni0000005a/uni0000004c/uni00000049/uni00000057/uni0000002e/uni00000052/uni00000057/uni0000004f/uni0000004c/uni00000051/uni00000036/uni00000034/uni0000002f/uni00000035/uni00000058/uni00000045/uni0000005c/uni00000037/uni0000005c/uni00000053/uni00000048/uni00000036/uni00000046/uni00000055/uni0000004c/uni00000053/uni00000057/uni00000026/uni00000006/uni00000033/uni0000002b/uni00000033/uni0000002a/uni00000052/uni0000002d/uni00000044/uni00000059/uni00000044/uni00000036/uni00000046/uni00000055/uni0000004c/uni00000053/uni00000057/uni00000026/uni00000012/uni00000026/uni0000000e/uni0000000e/uni0000002d/uni00000044/uni00000059/uni00000044/uni00000033/uni0000005c/uni00000057/uni0000004b/uni00000052/uni00000051
/uni00000016/uni00000016/uni00000017/uni00000017/uni00000017/uni00000018/uni00000019/uni00000019/uni0000001a/uni0000001b/uni0000001b/uni0000001c/uni0000001c/uni00000014/uni00000016/uni00000014/uni00000017/uni00000014/uni00000017/uni00000014/uni0000001b/uni00000014/uni0000001b/uni00000014/uni0000001c/uni00000015/uni0000001a/uni00000017/uni00000019/uni00000019/uni00000017/uni00000014/uni00000013/uni00000013
Fig. 6. Distribution of benchmarks
across programming languages
/uni00000037/uni00000052/uni00000057/uni00000044/uni0000004f /uni00000035/uni00000048/uni00000054/uni00000058/uni0000004c/uni00000055/uni00000048/uni00000050/uni00000048/uni00000051/uni00000057
/uni00000028/uni00000051/uni0000004a/uni0000004c/uni00000051/uni00000048/uni00000048/uni00000055/uni0000004c/uni00000051/uni0000004a/uni00000036/uni00000052/uni00000049/uni00000057/uni0000005a/uni00000044/uni00000055/uni00000048
/uni00000027/uni00000048/uni00000056/uni0000004c/uni0000004a/uni00000051/uni00000036/uni00000052/uni00000049/uni00000057/uni0000005a/uni00000044/uni00000055/uni00000048
/uni00000027/uni00000048/uni00000059/uni00000048/uni0000004f/uni00000052/uni00000053/uni00000050/uni00000048/uni00000051/uni00000057/uni00000037/uni00000048/uni00000056/uni00000057/uni0000004c/uni00000051/uni0000004a /uni00000036/uni00000052/uni00000049/uni00000057/uni0000005a/uni00000044/uni00000055/uni00000048
/uni00000030/uni00000044/uni0000004c/uni00000051/uni00000057/uni00000048/uni00000051/uni00000044/uni00000051/uni00000046/uni00000048/uni00000013/uni00000015/uni00000013/uni00000017/uni00000013/uni00000019/uni00000013/uni0000001b/uni00000013/uni00000014/uni00000013/uni00000013/uni00000031/uni00000058/uni00000050/uni00000045/uni00000048/uni00000055/uni00000003/uni00000052/uni00000049/uni00000003/uni00000025/uni00000048/uni00000051/uni00000046/uni0000004b/uni00000050/uni00000044/uni00000055/uni0000004e/uni00000056/uni00000014/uni00000013/uni00000013
/uni00000014 /uni00000014/uni0000001a/uni0000001b
/uni0000001b /uni0000001c/uni00000019/uni00000017
/uni00000013 /uni00000013/uni00000017/uni00000014
/uni00000014/uni00000013 /uni00000014/uni00000014/uni00000017/uni00000019
/uni00000013 /uni00000013/uni00000016/uni00000018
/uni00000014/uni00000016
/uni0000001a/uni00000015/uni0000001a
/uni00000013 /uni00000013/uni00000015/uni00000016
/uni00000015 /uni00000014/uni00000033/uni00000055/uni00000052/uni0000004a/uni00000055/uni00000044/uni00000050/uni00000050/uni0000004c/uni00000051/uni0000004a/uni00000003/uni0000002f/uni00000044/uni00000051/uni0000004a/uni00000058/uni00000044/uni0000004a/uni00000048
/uni00000033/uni0000005c/uni00000057/uni0000004b/uni00000052/uni00000051
/uni0000002d/uni00000044/uni00000059/uni00000044
/uni00000026/uni00000012/uni00000026/uni0000000e/uni0000000e
/uni0000002d/uni00000044/uni00000059/uni00000044/uni00000036/uni00000046/uni00000055/uni0000004c/uni00000053/uni00000057Fig. 7. Distribution of programming languages used in current benchmarks across different
SDLC phases.
Answer to RQ2 ‚Äì HumanEval are currently the most widely adopted benchmarks for CodeLLMs, while
the cross-phase benchmark CodeXGLUE is receiving increasing attention. In recent years, the number of
benchmarks has grown significantly, with a particularly sharp rise since 2023. Furthermore, the dominant
programming language in benchmarks is different at each phase of the SDLC. Specifically, C/C++ and Java are
the dominant languages in the testing and maintenance phases, respectively, whereas Python is predominant
in the other phases.
5 KEY FINDINGS AND FUTURE DIRECTIONS
To address RQ3, we identify the current limitations and challenges in existing research based on the previous review
(¬ß5.1), and propose feasible research directions (¬ß5.2) to advance future benchmarks of CodeLLMs and agents. We
observe that the quality of benchmarks plays a crucial role in evaluating the CodeLLMs. Notably, a contemporaneous
study by Cao et al. [ 14] also conducted a systematic investigation of existing benchmarks for CodeLLMs. They analyze
key aspects such as quality, reliability, and reproducibility, and further propose practical guidelines for constructing
high-quality CodeLLM benchmarks.
5.1 Key Findings
‚Ä¢Finding 1: Requirements engineering benchmarks have limited task coverage, with insufficient stan-
dardization and generalizability. Existing benchmarks mainly focus on requirement elicitation and validation,
while modeling, specification, classification, and especially change management remain underrepresented. Most
datasets are isolated and non-open-source, hindering reproducibility and fair comparisons.
Manuscript submitted to ACM24 Wang et al.
‚Ä¢Finding 2: Design benchmarks are imbalanced in task coverage and lack domain diversity. Most existing
work focuses on UI design, with limited support for architectural and database design. UI benchmarks are often
domain-specific (e.g., mobile apps), and algorithm design benchmarks like CLRS cover only narrow tasks, lacking
generality across design scenarios.
‚Ä¢Finding 3: Current software benchmarks offer breadth but lack depth in realism, multilingualism, and
repository-level understanding. For example, text-to-SQL benchmarks often rely on synthetic queries and
lack real project scenarios. Code retrieval tasks are mostly restricted to English corpora. Type inference rarely
involves complex types. Furthermore, repository-level code comprehension has yet to be adequately explored.
‚Ä¢Finding 4: Software testing benchmarks remain narrow in scope and underdeveloped across several
critical tasks. Most benchmarks focus on unit testing and vulnerability detection, while areas such as assertion
generation, code editing, and integration or interactive testing receive limited attention. Existing benchmarks
also lack coverage of runtime vulnerabilities, configuration-related issues, and a comprehensive range of defect
types encountered in real-world systems.
‚Ä¢Finding 5: Maintenance benchmarks have limited task coverage and insufficient support for evolving
and diverse scenarios. While program repair benchmarks are relatively mature, others, such as log parsing,
API misuse detection, decompilation, and clone detection, are underdeveloped. Existing benchmarks lack support
for language diversity, real-world code evolution, and logical similarity in clones.
‚Ä¢Finding 6: Evaluation remains confined to Single Phase. Modern software development has gradually
evolved into a systematic process that spans the SDLC, with strong interconnections between its phases. However,
most existing benchmarking methods focus on specific phases and lack comprehensive end-to-end evaluation
across the SDLC. This fragmented assessment overlooks the dependencies between phases. For instance, even if
large language models perform well in code generation, flaws in earlier phases, such as requirements analysis
or system design, can still lead to project failure. As a result, current evaluation frameworks do not accurately
reflect the complexity of real-world end-to-end software engineering workflows.
‚Ä¢Finding 7: Benchmarks focus heavily on functional correctness. Most benchmarks in software development
focus on functional correctness, covering only part of software quality. Non-functional requirements (NFRs)‚Äîsuch
as efficiency, readability, robustness, maintainability, and alignment with user intent‚Äîare equally important
and should be addressed across all phases of the SDLC. However, the few existing NFR benchmarks are mostly
limited to the development phase, with little support for other phases.
‚Ä¢Finding 8: Current models and agents are dominated by single-modality inputs. Although MLLMs are
increasingly used, most code benchmarks still focus on text-based inputs and outputs. They overlook the cross-
modal capabilities of these models and fail to cover multimodal scenarios such as visual design and multimodal
requirements.
‚Ä¢Finding 9: Human‚Äìmodel collaboration is largely neglected. In practice, collaboration between CodeLLMs
and human developers is becoming increasingly common. However, current benchmarks evaluate models in
isolation, without considering human interaction. This limits our ability to assess their effectiveness in real-world
collaborative settings.
‚Ä¢Finding 10: Existing benchmarks fail to isolate language-programming from problem-solving. Most
existing benchmarks evaluate the end-to-end code generation capabilities of LLMs on specific tasks, typically
using natural language inputs and assessing the correctness of the generated code. However, this approach makes
Manuscript submitted to ACMSoftware Development Life Cycle Perspective: A Survey of Benchmarks for Code Large Language Models and Agents 25
it difficult to distinguish whether performance bottlenecks stem from problem understanding and reasoning or
from code expression, thus limiting the ability to identify the model‚Äôs specific weaknesses.
5.2 Future Directions
Based on the above findings, we propose corresponding future research directions as follows.
‚Ä¢Direction 1: Developing standardized requirements engineering benchmarks covering diverse tasks. To
overcome the limitations of existing research on the requirements engineering phase, future benchmark research
should prioritize the development of standardized evaluation frameworks that comprehensively assess model and
agent capabilities across diverse requirement engineering tasks, such as requirements analysis and specification
formalization. These frameworks should evaluate how effectively models can interpret stakeholder needs,
manage requirement conflicts, and generate consistent specification documentation. Additionally, developing
methodologies to evaluate non-functional requirements, such as performance and security, is another potential
research direction with significant practical implications for industrial software development processes.
‚Ä¢Direction 2: Covering various domains and scenarios on the design phase. To enhance domain and
scenario coverage of benchmarks in the design phase, expanding existing evaluation frameworks to support other
domains, such as architecture design, is a potential direction. Furthermore, future benchmarks should evaluate
model capabilities for processing inputs in different modalities, such as diagrams, pseudocode, and natural
language specifications, thereby ensuring a comprehensive assessment in multi-modal design environments.
‚Ä¢Direction 3: Designing software development benchmark for real application scenarios. Future bench-
marks targeting specific development tasks (e.g., text-to-SQL) should incorporate queries sourced from actual
software projects to more accurately represent real-world development needs. Additionally, benchmarks should
evaluate models‚Äô comprehension and generation abilities at the repository level and across multilingual environ-
ments to better align with practical software development scenarios.
‚Ä¢Direction 4: Expanding the scope of software testing benchmarks. To better reflect the complexity of
real-world software testing, future benchmarks on the testing phase should broaden their scope. For example,
assertion generation capabilities could be enhanced to handle a wider variety of scenarios, increasing their
effectiveness in diverse environments. Test generation benchmarks should incorporate integration testing and
extend support for interactive testing workflows. Defect detection benchmarks should expand the range of
detectable defect types to provide more comprehensive coverage.
‚Ä¢Direction 5: Covering a variety of tasks from real maintenance scenarios. To evaluate the models‚Äô
capabilities in real-world tasks, future benchmark research should encompass a broader range of representative
software maintenance tasks. For instance, program repair benchmarks should capture code evolution and simulate
changes in dynamic development environments. Log parsing evaluations should be capable of handling diverse
log formats and cross-domain data with improved precision.
‚Ä¢Direction 6: Conducting cross-phase benchmarks. To break the limitations of phase-specific evaluation
in existing benchmarks, there is a pressing need for designing comprehensive cross-phase and end-to-end
benchmarking methods. Such benchmarks should simulate realistic, full-stack development pipelines‚Äîfrom re-
quirements elicitation and design to implementation, testing, and maintenance, enabling assessment of CodeLLMs
not merely for task-specific performance but for their capacity to navigate interconnected development depen-
dencies. Especially for LLM-based agents, which are typically designed to perform complex end-to-end tasks,
Manuscript submitted to ACM26 Wang et al.
designing comprehensive benchmarks specifically targeting their characteristics is crucial to accurately evaluate
their overall capabilities and limitations. Advancing this direction will effectively bridge the gap between isolated
model capabilities and the integrated requirements of industrial-scale software development environments.
‚Ä¢Direction 7: Evaluating non-functional capabilities of models and agents. Beyond functional correctness,
non-functional attributes play a critical role in software quality. Attributes such as security, robustness, and
fairness directly impact system performance, reliability, and user trust. A promising future direction lies in
developing comprehensive benchmarks that assess non-functional capabilities across all phases of the SDLC,
enabling more holistic evaluation of models and agents in real-world contexts.
‚Ä¢Direction 8: Building multimodal benchmarks. MLLMs that accept multimodal inputs have demonstrated
emerging capabilities that traditional models do not have, such as code generation from UI sketches. To fully
evaluate these capabilities, future benchmarks should extend beyond text-only formats and incorporate structured
multimodal evaluation. Such designs could reflect real-world engineering tasks and better assess MLLMs‚Äô
effectiveness in cross-modal generation scenarios.
‚Ä¢Direction 9: Evaluating the capabilities of multi-agent systems. With advancements in technologies
such as Multi-Agent Collaborative Programming (MCP), multi-agent systems are demonstrating significant
potential for real-world applications. Future research should move beyond single-agent evaluation to explore
human-machine collaboration and multi-agent collaboration. Designing standardized benchmarks to assess
collaborative effectiveness, covering various aspects such as interaction efficiency and reduction in human effort,
represents a valuable direction for future work.
‚Ä¢Direction 10: Developing code-centric evaluation frameworks. Current research focuses on adapting
and improving NLP evaluation metrics to assess the general problem-solving capabilities of CodeLLMs and
agents through code syntax. However, compared to syntax, logical structure is more important for evaluating
the understanding and reasoning capabilities in SE tasks. A promising future direction is the development of
evaluation metrics and benchmarks tailored specifically to code, with a focus on assessing the correctness of
logical structures, such as through benchmarks for pseudocode generation and computational graph construction.
Answer to RQ3 ‚Äì We identify ten future research directions grounded in the key challenges currently facing
benchmark development. These directions span critical areas, including task standardization, cross-phase
evaluation, multimodal assessment, and multi-agent collaboration. These directions aim to build a more practical
evaluation benchmark to better meet the growing diversity in CodeLLM and agent capability assessment.
6 CONCLUSION
Through the lens of SDLC, this paper presents a comprehensive study of 181 benchmarks for CodeLLMs and agents
from 461 existing literatures, providing an in-depth analysis of limitations and challenges in benchmark research at
each SDLC phase. Our findings reveal significant distributional imbalances across the current benchmark landscape,
with substantial concentration in the software development, testing, and maintenance phases, while benchmarks
addressing requirements engineering and software design phases remain notably underrepresented Based on the
systematic analysis, this paper further identifies 10 findings and proposes 10 corresponding research directions, aiming
to establish the foundation for future research and promote the development of CodeLLM and agent technology and
their application in real-world scenarios.
Manuscript submitted to ACMSoftware Development Life Cycle Perspective: A Survey of Benchmarks for Code Large Language Models and Agents 27
REFERENCES
[1] 1998. IEEE Recommended Practice for Software Requirements Specifications. IEEE Std 830-1998 (1998), 1‚Äì40. doi:10.1109/IEEESTD.1998.88286
[2] 2023. GitHub Copilot ¬∑Your AI pair programmer. https://github.com/features/copilot/.
[3] Juliet Java 1.3. 2017. Juliet Java 1.3. https://samate.nist.gov/SARD/test-suites/111. Accessed: 12-12-2023.
[4]Rajas Agashe, Srinivasan Iyer, and Luke Zettlemoyer. 2019. JuICe: A large scale distantly supervised dataset for open domain context-based code
generation. arXiv preprint arXiv:1910.02216 (2019).
[5]Lakshya A Agrawal, Aditya Kanade, Navin Goyal, Shuvendu K Lahiri, and Sriram K Rajamani. 2023. Guiding language models of code with global
context using monitors. arXiv preprint arXiv:2306.10763 (2023).
[6]Khlood Ahmad, Mohamed Abdelrazek, Chetan Arora, John Grundy, and Muneera Bano. 2023. Requirements Elicitation and Modelling of Artificial
Intelligence Systems: An Empirical Study. doi:10.48550/arXiv.2302.06034 arXiv:2302.06034 [cs]
[7]Ahmed Allam and Mohamed Shalan. 2024. Rtl-repo: A benchmark for evaluating llms on large-scale rtl design projects. In 2024 IEEE LLM Aided
Design Workshop (LAD) . IEEE, 1‚Äì5.
[8]Jordi Armengol-Estap√©, Jackson Woodruff, Alexander Brauckmann, Jos√© Wesley de Souza Magalh√£es, and Michael FP O‚ÄôBoyle. 2022. ExeBench: an
ML-scale dataset of executable C functions. In Proceedings of the 6th ACM SIGPLAN International Symposium on Machine Programming . 50‚Äì59.
[9]Ben Athiwaratkun, Sanjay Krishna Gouda, Zijian Wang, Xiaopeng Li, Yuchen Tian, Ming Tan, Wasi Uddin Ahmad, Shiqi Wang, Qing Sun, Mingyue
Shang, Sujan Kumar Gonugondla, Hantian Ding, Varun Kumar, Nathan Fulton, Arash Farahani, Siddharth Jain, Robert Giaquinto, Haifeng Qian,
Murali Krishna Ramanathan, Ramesh Nallapati, Baishakhi Ray, Parminder Bhatia, Sudipta Sengupta, Dan Roth, and Bing Xiang. 2022. Multi-lingual
Evaluation of Code Generation Models. ArXiv abs/2210.14868 (2022). https://api.semanticscholar.org/CorpusID:253116642
[10] Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le,
et al. 2021. Program synthesis with large language models. arXiv preprint arXiv:2108.07732 (2021).
[11] Hannah McLean Babe, Sydney Nguyen, Yangtian Zi, Arjun Guha, Molly Q. Feldman, and Carolyn Jane Anderson. 2023. StudentEval: A Benchmark
of Student-Written Prompts for Large Language Models of Code . doi:10.48550/arXiv.2306.04556 arXiv:2306.04556 [cs]
[12] Guru Bhandari, Amara Naseer, and Leon Moonen. 2021. CVEfixes: automated collection of vulnerabilities and their fixes from open-source software.
InProceedings of the 17th International Conference on Predictive Models and Data Analytics in Software Engineering . 30‚Äì39.
[13] Quang-Cuong Bui, Riccardo Scandariato, and Nicol√°s E D√≠az Ferreyra. 2022. Vul4j: A dataset of reproducible java vulnerabilities geared towards
the study of program repair techniques. In Proceedings of the 19th International Conference on Mining Software Repositories . 464‚Äì468.
[14] Jialun Cao, Yuk-Kit Chan, Zixuan Ling, Wenxuan Wang, Shuqing Li, Mingwei Liu, Ruixi Qiao, Yuting Han, Chaozheng Wang, Boxi Yu, Pinjia
He, Shuai Wang, Zibin Zheng, Michael R. Lyu, and Shing-Chi Cheung. 2025. How Should We Build A Benchmark? Revisiting 274 Code-Related
Benchmarks For LLMs. arXiv:2501.10711 [cs.SE] https://arxiv.org/abs/2501.10711
[15] Ruisheng Cao, Fangyu Lei, Haoyuan Wu, Jixuan Chen, Yeqiao Fu, Hongcheng Gao, Xinzhuang Xiong, Hanchong Zhang, Wenjing Hu, and Yuchen
Mao. 2024. Spider2-v: How far are multimodal agents from automating data science and engineering workflows? 37 (2024), 107703‚Äì107744. https:
//proceedings.neurips.cc/paper_files/paper/2024/hash/c2f71567cd53464161cab3336e8fc865-Abstract-Datasets_and_Benchmarks_Track.html
[16] Federico Cassano, John Gouwar, Francesca Lucchetti, Claire Schlesinger, Anders Freeman, Carolyn Jane Anderson, Molly Q Feldman, Michael
Greenberg, Abhinav Jangda, and Arjun Guha. 2024. Knowledge transfer from high-resource to low-resource programming languages for code llms.
Proceedings of the ACM on Programming Languages 8, OOPSLA2 (2024), 677‚Äì708.
[17] Federico Cassano, John Gouwar, Daniel Nguyen, Sydney Nguyen, Luna Phipps-Costin, Donald Pinckney, Ming-Ho Yee, Yangtian Zi, Carolyn Jane
Anderson, Molly Q Feldman, et al .2022. Multipl-e: A scalable and extensible approach to benchmarking neural code generation. arXiv preprint
arXiv:2208.08227 (2022).
[18] Linzheng Chai, Shukai Liu, Jian Yang, Yuwei Yin, Ke Jin, Jiaheng Liu, Tao Sun, Ge Zhang, Changyu Ren, Hongcheng Guo, et al .2024. McEval:
Massively Multilingual Code Evaluation. arXiv preprint arXiv:2406.07436 (2024).
[19] Saikat Chakraborty, Rahul Krishna, Yangruibo Ding, and Baishakhi Ray. 2021. Deep learning based vulnerability detection: Are we there yet? IEEE
Transactions on Software Engineering 48, 9 (2021), 3280‚Äì3296.
[20] Junting Chen, Yao Mu, Qiaojun Yu, Tianming Wei, Silang Wu, Zhecheng Yuan, Zhixuan Liang, Chao Yang, Kaipeng Zhang, Wenqi Shao, et al .2024.
Roboscript: Code generation for free-form manipulation tasks across real and simulation. arXiv preprint arXiv:2402.14623 (2024).
[21] Junkai Chen, Zhiyuan Pan, Xing Hu, Zhenhao Li, Ge Li, and Xin Xia. 2025. Reasoning Runtime Behavior of a Program with LLM: How Far Are
We?. In Proceedings of the IEEE/ACM 47th International Conference on Software Engineering .
[22] Jiachi Chen, Qingyuan Zhong, Yanlin Wang, Kaiwen Ning, Yongkun Liu, Zenan Xu, Zhe Zhao, Ting Chen, and Zibin Zheng. 2024. RMCBench:
Benchmarking Large Language Models‚Äô Resistance to Malicious Code. In Proceedings of the 39th IEEE/ACM International Conference on Automated
Software Engineering . 995‚Äì1006.
[23] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas
Joseph, Greg Brockman, et al. 2021. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374 (2021).
[24] Xinyun Chen, Maxwell Lin, Nathanael Sch√§rli, and Denny Zhou. 2023. Teaching Large Language Models to Self-Debug. In The Twelfth International
Conference on Learning Representations .
[25] Yizheng Chen, Zhoujie Ding, Lamya Alowain, Xinyun Chen, and David Wagner. 2023. Diversevul: A new vulnerable source code dataset for deep
learning based vulnerability detection. In Proceedings of the 26th International Symposium on Research in Attacks, Intrusions and Defenses . 654‚Äì668.
Manuscript submitted to ACM28 Wang et al.
[26] Baijun Cheng, Cen Zhang, Kailong Wang, Ling Shi, Yang Liu, Haoyu Wang, Yao Guo, Ding Li, and Xiangqun Chen. 2024. Semantic-enhanced
indirect call analysis with large language models. In Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering .
430‚Äì442.
[27] Alireza Daghighfarsoodeh, Chung-Yu Wang, Hamed Taherkhani, Melika Sepidband, Mohammad Abdollahi, Hadi Hemmati, and Hung Viet Pham.
2025. Deep-bench: deep learning benchmark dataset for code generation . doi:10.48550/arXiv.2502.18726 arXiv:2502.18726 [cs]
[28] Dima Damen, Hazel Doughty, Giovanni Maria Farinella, Antonino Furnari, Evangelos Kazakos, Jian Ma, Davide Moltisanti, Jonathan Munro, Toby
Perrett, Will Price, et al .2022. Rescaling egocentric vision: Collection, pipeline and challenges for epic-kitchens-100. International Journal of
Computer Vision (2022), 1‚Äì23.
[29] Biplab Deka, Zifeng Huang, Chad Franzen, Joshua Hibschman, Daniel Afergan, Yang Li, Jeffrey Nichols, and Ranjitha Kumar. 2017. Rico: A mobile
app dataset for building data-driven design applications. In Proceedings of the 30th annual ACM symposium on user interface software and technology .
845‚Äì854.
[30] Xiang Deng, Ahmed Hassan Awadallah, Christopher Meek, Oleksandr Polozov, Huan Sun, and Matthew Richardson. 2020. Structure-grounded
pretraining for text-to-SQL. arXiv preprint arXiv:2010.12773 (2020).
[31] Juri Di Rocco, Davide Di Ruscio, Claudio Di Sipio, Phuong T Nguyen, and Riccardo Rubei. 2025. On the use of large language models in model-driven
engineering. Software and Systems Modeling (2025), 1‚Äì26.
[32] Yangruibo Ding, Yanjun Fu, Omniyyah Ibrahim, Chawin Sitawarin, Xinyun Chen, Basel Alomair, David Wagner, Baishakhi Ray, and Yizheng Chen.
2024. Vulnerability detection with code language models: How far are we? arXiv preprint arXiv:2403.18624 (2024).
[33] Yangruibo Ding, Zijian Wang, Wasi Ahmad, Hantian Ding, Ming Tan, Nihal Jain, Murali Krishna Ramanathan, Ramesh Nallapati, Parminder
Bhatia, Dan Roth, et al .2024. Crosscodeeval: A diverse and multilingual benchmark for cross-file code completion. Advances in Neural Information
Processing Systems 36 (2024).
[34] Yihong Dong, Jiazheng Ding, Xue Jiang, Ge Li, Zhuo Li, and Zhi Jin. 2025. Codescore: Evaluating code generation by learning code execution.
ACM Transactions on Software Engineering and Methodology 34, 3 (2025), 1‚Äì22.
[35] Erick Barros dos Santos, Lucas Sim√£o da Costa, Tha√≠s Marinho De Amorim, Bruno Sab√≥ia Arag√£o, Ismayle de Sousa Santos, Danilo Reis de
Vasconcelos, and Rossana Maria de Castro Andrade. 2020. Extraction of test cases procedures from textual use cases: is it worth it? Journal of
Software Engineering Research and Development 8 (2020), 9‚Äì1.
[36] Mingzhe Du, Anh Tuan Luu, Bin Ji, Qian Liu, and See-Kiong Ng. 2024. Mercury: A Code Efficiency Benchmark for Code Large Language Models .
doi:10.48550/arXiv.2402.07844 arXiv:2402.07844 [cs]
[37] Xueying Du, Mingwei Liu, Kaixin Wang, Hanlin Wang, Junwei Liu, Yixuan Chen, Jiayi Feng, Chaofeng Sha, Xin Peng, and Yiling Lou. 2023.
Classeval: A manually-crafted benchmark for evaluating llms on class-level code generation. arXiv preprint arXiv:2308.01861 (2023).
[38] Yongkang Du, Jen-tse Huang, Jieyu Zhao, and Lu Lin. 2025. FairCoder: Evaluating Social Bias of LLMs in Code Generation . doi:10.48550/arXiv.2501.
05396 arXiv:2501.05396 [cs]
[39] Saad Ezzini, Sallam Abualhaija, Chetan Arora, and Mehrdad Sabetzadeh. 2023. Ai-based question answering assistance for analyzing natural-
language requirements. In 2023 IEEE/ACM 45th International Conference on Software Engineering (ICSE) . IEEE, 1277‚Äì1289.
[40] Jiahao Fan, Yi Li, Shaohua Wang, and Tien N Nguyen. 2020. AC/C++ code vulnerability dataset with code changes and CVE summaries. In
Proceedings of the 17th International Conference on Mining Software Repositories . 508‚Äì512.
[41] Alessio Ferrari, Sallam Abualhaija, and Chetan Arora. 2024. Model Generation with LLMs: From Requirements to UML Sequence Diagrams. In 2024
IEEE 32nd International Requirements Engineering Conference Workshops (REW) (2024-06). 291‚Äì300. doi:10.1109/REW61692.2024.00044
[42] Alessio Ferrari, Giorgio Oronzo Spagnolo, and Stefania Gnesi. 2017. PURE: A Dataset of Public Requirements Documents. In 2017 IEEE 25th
International Requirements Engineering Conference (RE) . 502‚Äì505. doi:10.1109/RE.2017.29
[43] Gordon Fraser and Andrea Arcuri. 2014. A large-scale evaluation of automated unit test generation using evosuite. ACM Transactions on Software
Engineering and Methodology (TOSEM) 24, 2 (2014), 1‚Äì42.
[44] Lingyue Fu, Huacan Chai, Shuang Luo, Kounianhua Du, Weiming Zhang, Longteng Fan, Jiayi Lei, Renting Rui, Jianghao Lin, Yuchen Fang, et al .
2023. Codeapex: A bilingual programming evaluation benchmark for large language models. arXiv preprint arXiv:2309.01940 (2023).
[45] Yujian Gan, Xinyun Chen, Qiuping Huang, Matthew Purver, John R Woodward, Jinxia Xie, and Pengsheng Huang. 2021. Towards robustness of
text-to-SQL models against synonym substitution. arXiv preprint arXiv:2106.01065 (2021).
[46] Yujian Gan, Xinyun Chen, and Matthew Purver. 2021. Exploring underexplored limitations of cross-domain text-to-SQL generalization. arXiv
preprint arXiv:2109.05157 (2021).
[47] Xiang Gao, Bo Wang, Gregory J Duck, Ruyi Ji, Yingfei Xiong, and Abhik Roychoudhury. 2021. Beyond tests: Program vulnerability repair via crash
constraint extraction. ACM Transactions on Software Engineering and Methodology (TOSEM) 30, 2 (2021), 1‚Äì27.
[48] Jing Gong, Yanghui Wu, Linxi Liang, Zibin Zheng, and Yanlin Wang. 2024. CoSQA+: Enhancing Code Search Dataset with Matching Code. arXiv
preprint arXiv:2406.11589 (2024).
[49] Alexandra Gonz√°lez, Xavier Franch, David Lo, and Silverio Mart√≠nez-Fern√°ndez. 2024. Towards a Classification of Open-Source ML Models and
Datasets for Software Engineering. arXiv preprint arXiv:2411.09683 (2024).
[50] Gonzalo Gonzalez-Pumariega, Leong Su Yean, Neha Sunkara, and Sanjiban Choudhury. 2025. Robotouille: An Asynchronous Planning Benchmark
for LLM Agents. In The Thirteenth International Conference on Learning Representations . https://openreview.net/forum?id=OhUoTMxFIH
Manuscript submitted to ACMSoftware Development Life Cycle Perspective: A Survey of Benchmarks for Code Large Language Models and Agents 29
[51] Alex Gu, Baptiste Rozi√®re, Hugh Leather, Armando Solar-Lezama, Gabriel Synnaeve, and Sida I Wang. 2024. Cruxeval: A benchmark for code
reasoning, understanding and execution. arXiv preprint arXiv:2401.03065 (2024).
[52] Xiaodong Gu, Meng Chen, Yalan Lin, Yuhan Hu, Hongyu Zhang, Chengcheng Wan, Zhao Wei, Yong Xu, and Juhong Wang. 2025. On the
effectiveness of large language models in domain-specific code generation. ACM Transactions on Software Engineering and Methodology 34, 3 (2025),
1‚Äì22.
[53] Xiaodong Gu, Hongyu Zhang, and Sunghun Kim. 2018. Deep code search. In Proceedings of the 40th International Conference on Software Engineering .
933‚Äì944.
[54] Zuxing Gu, Jiecheng Wu, Jiaxiang Liu, Min Zhou, and Ming Gu. 2019. An empirical study on api-misuse bugs in open-source c programs. In 2019
IEEE 43rd annual computer software and applications conference (COMPSAC) , Vol. 1. IEEE, 11‚Äì20.
[55] Chengquan Guo, Xun Liu, Chulin Xie, Andy Zhou, Yi Zeng, Zinan Lin, Dawn Song, and Bo Li. 2024. RedCode: Risky Code Execution and Generation
Benchmark for Code Agents. arXiv preprint arXiv:2411.07781 (2024).
[56] Daya Guo, Qihao Zhu, Dejian Yang, Zhenda Xie, Kai Dong, Wentao Zhang, Guanting Chen, Xiao Bi, Yu Wu, YK Li, et al .2024. DeepSeek-Coder:
When the Large Language Model Meets Programming‚ÄìThe Rise of Code Intelligence. arXiv preprint arXiv:2401.14196 (2024).
[57] Md Asraful Haque. 2024. LLMs: A Game-Changer for Software Engineers? arXiv preprint arXiv:2411.00932 (2024).
[58] Shayan Hashemi, Jesse Nyyss√∂l√§, and Mika V M√§ntyl√§. 2024. LogPM: Character-Based Log Parser Benchmark. In 2024 IEEE International Conference
on Software Analysis, Evolution and Reengineering (SANER) . IEEE, 705‚Äì710.
[59] Vincent J Hellendoorn, Charles Sutton, Rishabh Singh, Petros Maniatis, and David Bieber. 2019. Global relational models of source code. In
International conference on learning representations .
[60] Dan Hendrycks, Steven Basart, Saurav Kadavath, Mantas Mazeika, Akul Arora, Ethan Guo, Collin Burns, Samir Puranik, Horace He, Dawn Xiaodong
Song, and Jacob Steinhardt. 2021. Measuring Coding Challenge Competence With APPS. ArXiv abs/2105.09938 (2021). https://api.semanticscholar.
org/CorpusID:234790100
[61] Soneya Binta Hossain and Matthew Dwyer. 2024. Togll: Correct and strong test oracle generation with llms. arXiv preprint arXiv:2405.03786 (2024).
[62] Xinyi Hou, Yanjie Zhao, Yue Liu, Zhou Yang, Kailong Wang, Li Li, Xiapu Luo, David Lo, John Grundy, and Haoyu Wang. 2024. Large language
models for software engineering: A systematic literature review. ACM Transactions on Software Engineering and Methodology 33, 8 (2024), 1‚Äì79.
[63] Hao-Ya Hsueh, Alexandru-Iosif Toma, Hussein Ali Jaafar, Edward Stow, Riku Murai, Paul HJ Kelly, and Sajad Saeedi. 2022. Systematic comparison
of path planning algorithms using PathBench. Advanced Robotics 36, 11 (2022), 566‚Äì581.
[64] Ruida Hu, Chao Peng, Jingyi Ren, Bo Jiang, Xiangxin Meng, Qinyun Wu, Pengfei Gao, Xinchen Wang, and Cuiyun Gao. 2024. A Real-World
Benchmark for Evaluating Fine-Grained Issue Solving Capabilities of Large Language Models. arXiv preprint arXiv:2411.18019 (2024).
[65] Xing Hu, Ge Li, Xin Xia, David Lo, and Zhi Jin. 2018. Deep code comment generation. In Proceedings of the 26th conference on program comprehension .
200‚Äì210.
[66] Xing Hu, Ge Li, Xin Xia, David Lo, Shuai Lu, and Zhi Jin. 2018. Summarizing source code with transferred api knowledge. (2018).
[67] Yue Hu, Yuzhu Cai, Yaxin Du, Xinyu Zhu, Xiangrui Liu, Zijie Yu, Yuchen Hou, Shuo Tang, and Siheng Chen. 2024. Self-Evolving Multi-Agent
Collaboration Networks for Software Development. doi:10.48550/arXiv.2410.16946 arXiv:2410.16946 [cs]
[68] Dong Huang, Yuhao Qing, Weiyi Shang, Heming Cui, and Jie Zhang. 2024. Effibench: benchmarking the efficiency of automatically generated code.
37 (2024), 11506‚Äì11544. https://proceedings.neurips.cc/paper_files/paper/2024/hash/15807b6e09d691fe5e96cdecde6d7b80-Abstract-Datasets_and_
Benchmarks_Track.html
[69] Junjie Huang, Duyu Tang, Linjun Shou, Ming Gong, Ke Xu, Daxin Jiang, Ming Zhou, and Nan Duan. 2021. Cosqa: 20,000+ web queries for code
search and question answering. arXiv preprint arXiv:2105.13239 (2021).
[70] Qian Huang, Jian Vora, Percy Liang, and Jure Leskovec. 2023. BENCHMARKING LARGE LANGUAGE MODELS AS AI RESEARCH AGENTS.
(2023).
[71] Binyuan Hui, Jian Yang, Zeyu Cui, Jiaxi Yang, Dayiheng Liu, Lei Zhang, Tianyu Liu, Jiajun Zhang, Bowen Yu, Kai Dang, et al .2024. Qwen2. 5-Coder
Technical Report. arXiv preprint arXiv:2409.12186 (2024).
[72] Hamel Husain, Ho-Hsiang Wu, Tiferet Gazit, Miltiadis Allamanis, and Marc Brockschmidt. 2019. Codesearchnet challenge: Evaluating the state of
semantic code search. arXiv preprint arXiv:1909.09436 (2019).
[73] Anysphere Inc. 2025. Cursor - The AI Code Editor. https://www.cursor.com Accessed: 2025-05-03.
[74] Srinivasan Iyer, Ioannis Konstas, Alvin Cheung, and Luke Zettlemoyer. 2016. Summarizing source code using a neural attention model. In 54th
Annual Meeting of the Association for Computational Linguistics 2016 . Association for Computational Linguistics, 2073‚Äì2083.
[75] Srinivasan Iyer, Ioannis Konstas, Alvin Cheung, and Luke Zettlemoyer. 2018. Mapping language to code in programmatic context. arXiv preprint
arXiv:1808.09588 (2018).
[76] Chirag Jain, Preethu Rose Anish, Amrita Singh, and Smita Ghaisas. 2023. A Transformer-Based Approach for Abstractive Summarization of
Requirements from Obligations in Software Engineering Contracts. In 2023 IEEE 31st International Requirements Engineering Conference (RE) .
169‚Äì179. doi:10.1109/RE57278.2023.00025
[77] Nan Jiang, Kevin Liu, Thibaud Lutellier, and Lin Tan. 2023. Impact of code language models on automated program repair. In 2023 IEEE/ACM 45th
International Conference on Software Engineering (ICSE) . IEEE, 1430‚Äì1442.
[78] Zhihan Jiang, Jinyang Liu, Junjie Huang, Yichen Li, Yintong Huo, Jiazhen Gu, Zhuangbin Chen, Jieming Zhu, and Michael R Lyu. 2024. A large-scale
evaluation for log parsing techniques: How far are we?. In Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and
Manuscript submitted to ACM30 Wang et al.
Analysis . 223‚Äì234.
[79] Carlos E Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, and Karthik R Narasimhan. 2024. SWE-bench: Can Language
Models Resolve Real-world Github Issues?. In The Twelfth International Conference on Learning Representations . https://openreview.net/forum?id=
VTF8yNQM66
[80] Dongming Jin, Shengxin Zhao, Zhi Jin, Xiaohong Chen, Chunhui Wang, Zheng Fang, and Hongbin Xiao. 2024. An Evaluation of Requirements
Modeling for Cyber-Physical Systems via LLMs. doi:10.48550/arXiv.2408.02450 arXiv:2408.02450 [cs]
[81] Haolin Jin, Linghan Huang, Haipeng Cai, Jun Yan, Bo Li, and Huaming Chen. 2024. From llms to llm-based agents for software engineering: A
survey of current, challenges and future. arXiv preprint arXiv:2408.02479 (2024).
[82] Ren√© Just, Darioush Jalali, and Michael D Ernst. 2014. Defects4J: A database of existing faults to enable controlled testing studies for Java programs.
InProceedings of the 2014 international symposium on software testing and analysis . 437‚Äì440.
[83] Eirini Kalliamvakou. 2023. Research: Quantifying GitHub Copilot‚Äôs Impact on Developer Productivity and Happiness. https://github.blog/news-
insights/research/research-quantifying-github-copilots-impact-on-developer-productivity-and-happiness/
[84] Alya Hannah Ahmad Kamal, Caryn Chuah Yi Yen, Gan Jia Hui, Pang Sze Ling, and Fatima tuz Zahra. 2020. Risk Assessment, Threat Modeling and
Security Testing in SDLC. arXiv:2012.07226 [cs.SE] https://arxiv.org/abs/2012.07226
[85] Aditya Kanade, Petros Maniatis, Gogul Balakrishnan, and Kensen Shi. 2020. Learning and evaluating contextual embedding of source code. In
International conference on machine learning . PMLR, 5110‚Äì5121.
[86] Rahul Kande, Hammond Pearce, Benjamin Tan, Brendan Dolan-Gavitt, Shailja Thakur, Ramesh Karri, and Jeyavijayan Rajendran. 2024. (Security)
Assertions by Large Language Models. IEEE Transactions on Information Forensics and Security (2024).
[87] Minwoo Kang, Mingjie Liu, Ghaith Bany Hamad, Syed Suhaib, and Haoxing Ren. 2024. FVEval: Understanding Language Model Capabilities in
Formal Verification of Digital Hardware. arXiv preprint arXiv:2410.23299 (2024).
[88] Mohammad Abdullah Matin Khan, M Saiful Bari, Do Long, Weishi Wang, Md Rizwan Parvez, and Shafiq Joty. 2024. Xcodeeval: An execution-based
large scale multilingual multitask benchmark for code understanding, generation, translation and retrieval. In Proceedings of the 62nd Annual
Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) . 6766‚Äì6805.
[89] Zanis Ali Khan, Donghwan Shin, Domenico Bianculli, and Lionel Briand. 2022. Guidelines for assessing the accuracy of log message template
identification techniques. In Proceedings of the 44th International Conference on Software Engineering . 1095‚Äì1106.
[90] Madhava Krishna, Bhagesh Gaur, Arsh Verma, and Pankaj Jalote. 2024. Using LLMs in Software Requirements Specifications: An Empirical
Evaluation. (2024), 475‚Äì483. doi:10.1109/RE59067.2024.00056
[91] Yuhang Lai, Chengxi Li, Yiming Wang, Tianyi Zhang, Ruiqi Zhong, Luke Zettlemoyer, Wen-tau Yih, Daniel Fried, Sida Wang, and Tao Yu. 2023.
DS-1000: A natural and reliable benchmark for data science code generation. In International Conference on Machine Learning . PMLR, 18319‚Äì18345.
[92] Claire Le Goues, Neal Holtschulte, Edward K Smith, Yuriy Brun, Premkumar Devanbu, Stephanie Forrest, and Westley Weimer. 2015. The ManyBugs
and IntroClass benchmarks for automated repair of C programs. IEEE Transactions on Software Engineering 41, 12 (2015), 1236‚Äì1256.
[93] Changyoon Lee, Yeon Seonwoo, and Alice Oh. 2022. CS1QA: A dataset for assisting code-based question answering in an introductory programming
course. arXiv preprint arXiv:2210.14494 (2022).
[94] Chia-Hsuan Lee, Oleksandr Polozov, and Matthew Richardson. 2021. KaggleDBQA: Realistic evaluation of text-to-SQL parsers. arXiv preprint
arXiv:2106.11455 (2021).
[95] Gyubok Lee, Hyeonji Hwang, Seongsu Bae, Yeonsu Kwon, Woncheol Shin, Seongjun Yang, Minjoon Seo, Jong-Yeup Kim, and Edward Choi. 2022.
Ehrsql: A practical text-to-sql benchmark for electronic health records. Advances in Neural Information Processing Systems 35 (2022), 15589‚Äì15601.
[96] Bowen Li, Wenhan Wu, Ziwei Tang, Lin Shi, John Yang, Jinyang Li, Shunyu Yao, Chen Qian, Binyuan Hui, and Qicheng Zhang. 2024. Devbench: A
comprehensive benchmark for software development. 3 (2024). arXiv:2403.08604 https://se-research.bytedance.com/publication/arxiv24a/arxiv24a.
pdf
[97] Jinyang Li, Binyuan Hui, Ge Qu, Jiaxi Yang, Binhua Li, Bowen Li, Bailin Wang, Bowen Qin, Ruiying Geng, Nan Huo, et al .2024. Can llm already
serve as a database interface? a big bench for large-scale database grounded text-to-sqls. Advances in Neural Information Processing Systems 36
(2024).
[98] Jia Li, Ge Li, Xuanming Zhang, Yihong Dong, and Zhi Jin. 2024. Evocodebench: An evolving code generation benchmark aligned with real-world
code repositories. arXiv preprint arXiv:2404.00599 (2024).
[99] Jia Li, Ge Li, Yunfei Zhao, Yongmin Li, Huanyu Liu, Hao Zhu, Lecheng Wang, Kaibo Liu, Zheng Fang, Lanshen Wang, Jiazheng Ding, Xuanming Zhang,
Yuqi Zhu, Yihong Dong, Zhi Jin, Binhua Li, Fei Huang, Yongbin Li, Bin Gu, and Mengfei Yang. 2024. DevEval: A Manually-Annotated Code Generation
Benchmark Aligned with Real-World Code Repositories. In Findings of the Association for Computational Linguistics: ACL 2024 , Lun-Wei Ku, Andre
Martins, and Vivek Srikumar (Eds.). Association for Computational Linguistics, Bangkok, Thailand, 3603‚Äì3614. doi:10.18653/v1/2024.findings-acl.214
[100] Kaixin Li, Yuchen Tian, Qisheng Hu, Ziyang Luo, Zhiyong Huang, and Jing Ma. 2024. MMCode: benchmarking multimodal large language
models for code generation with visually rich programming problems. In Findings of the Association for Computational Linguistics: EMNLP 2024
(Miami, Florida, USA, 2024-11), Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen (Eds.). Association for Computational Linguistics, 736‚Äì783.
doi:10.18653/v1/2024.findings-emnlp.42
[101] Linyi Li, Shijie Geng, Zhenwen Li, Yibo He, Hao Yu, Ziyue Hua, Guanghan Ning, Siwei Wang, Tao Xie, and Hongxia Yang. 2025. Infibench:
Evaluating the question-answering capabilities of code large language models. Advances in Neural Information Processing Systems 37 (2025),
128668‚Äì128698.
Manuscript submitted to ACMSoftware Development Life Cycle Perspective: A Survey of Benchmarks for Code Large Language Models and Agents 31
[102] Wei Li, Xin Zhang, Zhongxin Guo, Shaoguang Mao, Wen Luo, Guangyue Peng, Yangyu Huang, Houfeng Wang, and Scarlett Li. 2025. FEA-Bench:
A Benchmark for Evaluating Repository-Level Code Generation for Feature Implementation. arXiv preprint arXiv:2503.06680 (2025).
[103] Xinzhe Li. 2024. A review of prominent paradigms for llm-based agents: Tool use (including rag), planning, and feedback learning. arXiv preprint
arXiv:2406.05804 (2024).
[104] Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, R√©mi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin
Dal Lago, et al. 2022. Competition-level code generation with alphacode. Science 378, 6624 (2022), 1092‚Äì1097.
[105] Zhen Li, Deqing Zou, Shouhuai Xu, Hai Jin, Yawei Zhu, and Zhaoxuan Chen. 2021. Sysevr: A framework for using deep learning to detect software
vulnerabilities. IEEE Transactions on Dependable and Secure Computing 19, 4 (2021), 2244‚Äì2258.
[106] Zhen Li, Deqing Zou, Shouhuai Xu, Xinyu Ou, Hai Jin, Sujuan Wang, Zhijun Deng, and Yuyi Zhong. 2018. VulDeePecker: A Deep Learning-
Based System for Vulnerability Detection. In Proceedings 2018 Network and Distributed System Security Symposium . doi:10.14722/ndss.2018.23158
arXiv:1801.01681 [cs]
[107] Bo Lin, Shangwen Wang, Liqian Chen, and Xiaoguang Mao. 2024. There are More Fish in the Sea: Automated Vulnerability Repair via Binary
Templates. arXiv preprint arXiv:2411.18088 (2024).
[108] Derrick Lin, James Koppel, Angela Chen, and Armando Solar-Lezama. 2017. QuixBugs: A multi-lingual program repair benchmark set based on
the Quixey Challenge. In Proceedings Companion of the 2017 ACM SIGPLAN international conference on systems, programming, languages, and
applications: software for humanity . 55‚Äì56.
[109] Lin Ling, Fazle Rabbi, Song Wang, and Jinqiu Yang. 2025. Bias Unveiled: Investigating Social Bias in LLM-Generated Code. 39, 26 (2025), 27491‚Äì27499.
Issue 26. doi:10.1609/aaai.v39i26.34961
[110] Chenyan Liu, Yufan Cai, Yun Lin, Yuhuan Huang, Yunrui Pei, Bo Jiang, Ping Yang, Jin Song Dong, and Hong Mei. 2024. CoEdPilot: Recommending
Code Edits with Learned Prior Edit Relevance, Project-wise Awareness, and Interactive Nature. In Proceedings of the 33rd ACM SIGSOFT International
Symposium on Software Testing and Analysis . 466‚Äì478.
[111] Chenxiao Liu and Xiaojun Wan. 2021. CodeQA: A question answering dataset for source code comprehension. arXiv preprint arXiv:2109.08365
(2021).
[112] Jiawei Liu, Chunqiu Steven Xia, Yuyao Wang, and Lingming Zhang. 2024. Is your code generated by chatgpt really correct? rigorous evaluation of
large language models for code generation. Advances in Neural Information Processing Systems 36 (2024).
[113] Jiawei Liu, Songrun Xie, Junhao Wang, Yuxiang Wei, Yifeng Ding, and Lingming Zhang. 2024. Evaluating language models for efficient code
generation. arXiv preprint arXiv:2408.06450 (2024).
[114] Kaiyuan Liu, Youcheng Pan, Jing Li, Daojing He, Yang Xiang, Yexing Du, and Tianrun Gao. 2025. ProjectEval: A Benchmark for Programming
Agents Automated Evaluation on Project-Level Code Generation. arXiv preprint arXiv:2503.07010 (2025).
[115] Mingjie Liu, Nathaniel Pinckney, Brucek Khailany, and Haoxing Ren. 2023. Verilogeval: Evaluating large language models for verilog code
generation. In 2023 IEEE/ACM International Conference on Computer Aided Design (ICCAD) . IEEE, 1‚Äì8.
[116] Tianyang Liu, Canwen Xu, and Julian McAuley. 2023. Repobench: Benchmarking repository-level code auto-completion systems. arXiv preprint
arXiv:2306.03091 (2023).
[117] David Lo. 2023. Trustworthy and synergistic artificial intelligence for software engineering: vision and roadmaps. In 2023 IEEE/ACM International
Conference on Software Engineering: Future of Software Engineering (icse-fose) (2023). IEEE, 69‚Äì85. doi:10.1109/ICSE-FoSE59343.2023.00010
[118] Shuai Lu, Daya Guo, Shuo Ren, Junjie Huang, Alexey Svyatkovskiy, Ambrosio Blanco, Colin Clement, Dawn Drain, Daxin Jiang, Duyu Tang, et al .
2021. Codexglue: A machine learning benchmark dataset for code understanding and generation. arXiv preprint arXiv:2102.04664 (2021).
[119] Ziyang Luo, Can Xu, Pu Zhao, Qingfeng Sun, Xiubo Geng, Wenxiang Hu, Chongyang Tao, Jing Ma, Qingwei Lin, and Daxin Jiang. 2023. WizardCoder:
Empowering Code Large Language Models with Evol-Instruct. In The Twelfth International Conference on Learning Representations .
[120] Lezhi Ma, Shangqing Liu, Lei Bu, Shangru Li, Yida Wang, and Yang Liu. 2024. SpecEval: Evaluating Code Comprehension in Large Language
Models via Program Specifications. arXiv preprint arXiv:2409.12866 (2024).
[121] Fernanda Madeiral, Simon Urli, Marcelo Maia, and Martin Monperrus. 2019. Bears: An extensible java bug benchmark for automatic program
repair studies. In 2019 IEEE 26th international conference on software analysis, evolution and reengineering (SANER) . IEEE, 468‚Äì478.
[122] Dung Nguyen Manh, Thang Phan Chau, Nam Le Hai, Thong T. Doan, Nam V. Nguyen, Quang Pham, and Nghi D. Q. Bui. 2024. CodeMMLU: A
Multi-Task Benchmark for Assessing Code Understanding Capabilities of CodeLLMs . doi:10.48550/arXiv.2410.01999 arXiv:2410.01999 [cs]
[123] Matthias Mayer, Jonathan K√ºlz, and Matthias Althoff. 2024. CoBRA: A composable benchmark for robotics applications. In 2024 IEEE International
Conference on Robotics and Automation (ICRA) . IEEE, 17665‚Äì17671.
[124] Tanha Miah and Hong Zhu. 2024. User-centric evaluation of ChatGPT capability of generating R program code. arXiv e-prints (2024), arXiv‚Äì2402.
[125] Qingkai Min, Yuefeng Shi, and Yue Zhang. 2019. A Pilot Study for Chinese SQL Semantic Parsing. In Proceedings of the 2019 Conference on Empirical
Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP) , Kentaro Inui,
Jing Jiang, Vincent Ng, and Xiaojun Wan (Eds.). Association for Computational Linguistics, Hong Kong, China, 3652‚Äì3658. doi:10.18653/v1/D19-1377
[126] Kevin Moran, Carlos Bernal-C√°rdenas, Michael Curcio, Richard Bonett, and Denys Poshyvanyk. 2018. Machine learning-based prototyping of
graphical user interfaces for mobile apps. IEEE transactions on software engineering 46, 2 (2018), 196‚Äì221.
[127] Lili Mou, Ge Li, Lu Zhang, Tao Wang, and Zhi Jin. 2016. Convolutional neural networks over tree structures for programming language processing.
InProceedings of the AAAI conference on artificial intelligence , Vol. 30.
Manuscript submitted to ACM32 Wang et al.
[128] Niklas Muennighoff, Qian Liu, Armel Zebaze, Qinkai Zheng, Binyuan Hui, Terry Yue Zhuo, Swayam Singh, Xiangru Tang, Leandro Von Werra, and
Shayne Longpre. 2023. Octopack: Instruction tuning code large language models. arXiv preprint arXiv:2308.07124 (2023).
[129] Niels M√ºndler, Mark N. M√ºller, Jingxuan He, and Martin Vechev. 2024. SWT-bench: testing and validating real-world bug-fixes with code agents.
37 (2024), 81857‚Äì81887. https://proceedings.neurips.cc/paper_files/paper/2024/hash/94f093b41fc2666376fb1f667fe282f3-Abstract-Conference.html
[130] King Han Naman Jain, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, Armando Solar-Lezama, Koushik Sen, and Ion Stoica. 2024.
Livecodebench: Holistic and contamination free evaluation of large language models for code. arXiv preprint arXiv:2403.07974 (2024).
[131] Phuong T Nguyen, Juri Di Rocco, Claudio Di Sipio, Riccardo Rubei, Davide Di Ruscio, and Massimiliano Di Penta. 2024. GPTSniffer: A CodeBERT-
based classifier to detect source code written by ChatGPT. Journal of Systems and Software 214 (2024), 112059.
[132] Chao Ni, Wei Wang, Kaiwen Yang, Xin Xia, Kui Liu, and David Lo. 2022. The best of both worlds: integrating semantic features with expert
features for defect prediction and localization. In Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on
the Foundations of Software Engineering . 672‚Äì683.
[133] Georgios Nikitopoulos, Konstantina Dritsa, Panos Louridas, and Dimitris Mitropoulos. 2021. CrossVul: a cross-language vulnerability dataset with
commit data. In Proceedings of the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of
Software Engineering . 1565‚Äì1569.
[134] Wonseok Oh and Hakjoo Oh. 2022. PyTER: effective program repair for Python type errors. In Proceedings of the 30th ACM Joint European Software
Engineering Conference and Symposium on the Foundations of Software Engineering . 922‚Äì934.
[135] Ipek Ozkaya, Anita Carleton, John Robert, and Douglas Schmidt. 2023. Application of Large Language Models (LLMs) in Software Engineering:
Overblown Hype or Disruptive Change? Carnegie Mellon University, Software Engineering Institute‚Äôs Insights (blog). https://doi.org/10.58012/6n1p-
pw64 Accessed: 2025-05-02.
[136] Yue Pan and Chen Lyu. 2023. Measuring Efficient Code Generation with GEC. In Proceedings of the 14th Asia-Pacific Symposium on Internetware .
249‚Äì258.
[137] Ruchir Puri, David S Kung, Geert Janssen, Wei Zhang, Giacomo Domeniconi, Vladimir Zolotov, Julian Dolby, Jie Chen, Mihir Choudhury, Lindsey
Decker, et al. 2021. Codenet: A large-scale ai for code dataset for learning a diversity of coding tasks. arXiv preprint arXiv:2105.12655 (2021).
[138] Chen Qian, Wei Liu, Hongzhang Liu, Nuo Chen, Yufan Dang, Jiahao Li, Cheng Yang, Weize Chen, Yusheng Su, Xin Cong, et al .2024. ChatDev:
Communicative Agents for Software Development. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics
(Volume 1: Long Papers) . 15174‚Äì15186.
[139] Ruizhong Qiu, Weiliang Will Zeng, James Ezick, Christopher Lott, and Hanghang Tong. 2024. How efficient is llm-generated code? a rigorous &
high-standard benchmark. arXiv preprint arXiv:2406.06647 (2024).
[140] Shanghaoran Quan, Jiaxi Yang, Bowen Yu, Bo Zheng, Dayiheng Liu, An Yang, Xuancheng Ren, Bofei Gao, Yibo Miao, Yunlong Feng, et al .2025.
CodeElo: Benchmarking Competition-level Code Generation of LLMs with Human-comparable Elo Ratings. arXiv preprint arXiv:2501.01257 (2025).
[141] Federico Ranaldi, Elena Sofia Ruzzetti, Dario Onorati, Leonardo Ranaldi, Cristina Giannone, Andrea Favalli, Raniero Romagnoli, and Fabio Mas-
simo Zanzotto. 2024. Investigating the Impact of Data Contamination of Large Language Models in Text-to-SQL Translation. arXiv preprint
arXiv:2402.08100 (2024).
[142] Muhammad Shihab Rashid, Christian Bock, Yuan Zhuang, Alexander Buccholz, Tim Esler, Simon Valentin, Luca Franceschi, Martin Wistuba,
Prabhu Teja Sivaprasad, Woo Jung Kim, et al .2025. SWE-PolyBench: A multi-language benchmark for repository level evaluation of coding agents.
arXiv preprint arXiv:2504.08703 (2025).
[143] Lasse M Reinpold, Marvin Schieseck, Lukas P Wagner, Felix Gehlhoff, and Alexander Fay. 2024. Exploring LLMs for Verifying Technical System
Specifications Against Requirements. arXiv preprint arXiv:2411.11582 (2024).
[144] Niklas Risse and Marcel B√∂hme. 2024. Uncovering the limits of machine learning for automatic vulnerability detection. In 33rd USENIX Security
Symposium (USENIX Security 24) . 4247‚Äì4264.
[145] Daniel Rodriguez-Cardenas, David N Palacio, Dipin Khati, Henry Burke, and Denys Poshyvanyk. 2023. Benchmarking causal study to interpret
large language models for source code. In 2023 IEEE International Conference on Software Maintenance and Evolution (ICSME) . IEEE, 329‚Äì334.
[146] Ripon K Saha, Yingjun Lyu, Wing Lam, Hiroaki Yoshida, and Mukul R Prasad. 2018. Bugs. jar: A large-scale, diverse dataset of real-world java bugs.
InProceedings of the 15th international conference on mining software repositories . 10‚Äì13.
[147] J. Sayyad Shirabad and T.J. Menzies. 2005. The PROMISE Repository of Software Engineering Databases. School of Information Technology and
Engineering, University of Ottawa, Canada. http://promise.site.uottawa.ca/SERepository
[148] Xinyu She, Yanjie Zhao, and Haoyu Wang. 2024. WaDec: Decompiling WebAssembly Using Large Language Model. In Proceedings of the 39th
IEEE/ACM International Conference on Automated Software Engineering . 481‚Äì492.
[149] Chenglei Si, Yanzhe Zhang, Ryan Li, Zhengyuan Yang, Ruibo Liu, and Diyi Yang. 2025. Design2Code: Benchmarking Multimodal Code Generation
for Automated Front-End Engineering. doi:10.48550/arXiv.2403.03163 arXiv:2403.03163 [cs]
[150] Rafael Sumitani, Jo√£o Victor Amorim, Augusto Mafra, Mirlaine Crepalde, and Fernando Magno Quint√£o Pereira. 2024. ChiBench: a Benchmark
Suite for Testing Electronic Design Automation Tools. arXiv preprint arXiv:2406.06550 (2024).
[151] Jeffrey Svajlenko, Judith F. Islam, Iman Keivanloo, Chanchal K. Roy, and Mohammad Mamun Mia. 2014. Towards a big data curated benchmark of
inter-project code clones. In 2014 IEEE International Conference on Software Maintenance and Evolution (2014). IEEE, 476‚Äì480. doi:10.1109/ICSME.
2014.77
Manuscript submitted to ACMSoftware Development Life Cycle Perspective: A Survey of Benchmarks for Code Large Language Models and Agents 33
[152] Boyin Tan, Junjielong Xu, Zhouruixing Zhu, and Pinjia He. 2025. AL-bench: a benchmark for automatic logging . doi:10.48550/arXiv.2502.03160
arXiv:2502.03160 [cs]
[153] Xiangru Tang, Yuliang Liu, Zefan Cai, Yanjun Shao, Junjie Lu, Yichi Zhang, Zexuan Deng, Helan Hu, Kaikai An, Ruijun Huang, Shuzheng Si, Sheng
Chen, Haozhe Zhao, Liang Chen, Yan Wang, Tianyu Liu, Zhiwei Jiang, Baobao Chang, Yin Fang, Yujia Qin, Wangchunshu Zhou, Yilun Zhao,
Arman Cohan, and Mark Gerstein. 2024. ML-bench: evaluating large language models and agents for machine learning tasks on repository-level code .
doi:10.48550/arXiv.2311.09835 arXiv:2311.09835 [cs]
[154] Xiangru Tang, Bill Qian, Rick Gao, Jiakang Chen, Xinyun Chen, and Mark B Gerstein. 2024. BioCoder: a benchmark for bioinformatics code
generation with large language models. Bioinformatics 40, Supplement_1 (2024), i266‚Äìi276.
[155] Qingxiao Tao, Tingrui Yu, Xiaodong Gu, and Beijun Shen. 2024. Unraveling the Potential of Large Language Models in Code Translation: How Far
Are We? arXiv preprint arXiv:2410.09812 (2024).
[156] Norbert Tihanyi, Tamas Bisztray, Ridhi Jain, Mohamed Amine Ferrag, Lucas C Cordeiro, and Vasileios Mavroeidis. 2023. The formai dataset:
Generative ai in software security through the lens of formal verification. In Proceedings of the 19th International Conference on Predictive Models
and Data Analytics in Software Engineering . 33‚Äì43.
[157] Norbert Tihanyi, Ridhi Jain, Yiannis Charalambous, Mohamed Amine Ferrag, Youcheng Sun, and Lucas C. Cordeiro. 2024. A new era in software
security: towards self-healing software via large language models and formal verification. doi:10.48550/arXiv.2305.14752 arXiv:2305.14752 [cs]
TLDR: The ESBMC-AI framework is presented as a proof of concept, leveraging the well-recognized and industry-adopted Efficient SMT-based
Context-Bounded Model Checker and a pre-trained transformer model to detect and fix errors in C programs, particularly in critical software
components..
[158] Michele Tufano, Shao Kun Deng, Neel Sundaresan, and Alexey Svyatkovskiy. 2022. Methods2Test: A dataset of focal methods mapped to test cases.
InProceedings of the 19th International Conference on Mining Software Repositories . 299‚Äì303.
[159] Petar VeliÀá ckoviƒá, Adri√† Puigdom√®nech Badia, David Budden, Razvan Pascanu, Andrea Banino, Misha Dashevskiy, Raia Hadsell, and Charles
Blundell. 2022. The clrs algorithmic reasoning benchmark. In International Conference on Machine Learning (2022). PMLR, 22084‚Äì22102. https:
//proceedings.mlr.press/v162/velickovic22a
[160] Konstantinos Vergopoulos, Mark Niklas M√ºller, and Martin Vechev. 2025. Automated Benchmark Generation for Repository-Level Coding Tasks.
arXiv preprint arXiv:2503.07701 (2025).
[161] Siddhant Waghjale, Vishruth Veerendranath, Zora Zhiruo Wang, and Daniel Fried. 2024. ECCO: Can We Improve Model-Generated Code Efficiency
Without Sacrificing Functional Correctness? arXiv preprint arXiv:2407.14044 (2024).
[162] Yaza Wainakh, Moiz Rauf, and Michael Pradel. 2019. Evaluating semantic representations of source code. (2019). https://www.semanticscholar.
org/paper/Evaluating-Semantic-Representations-of-Source-Code-Wainakh-Rauf/cb66cbcab02f08a2492e0f53983cdd3ec06618f3
[163] Bryan Wang, Gang Li, Xin Zhou, Zhourong Chen, Tovi Grossman, and Yang Li. 2021. Screen2words: Automatic mobile UI summarization with
multimodal learning. In The 34th Annual ACM Symposium on User Interface Software and Technology . 498‚Äì510.
[164] Shiqi Wang, Zheng Li, Haifeng Qian, Chenghao Yang, Zijian Wang, Mingyue Shang, Varun Kumar, Samson Tan, Baishakhi Ray, Parminder Bhatia,
Ramesh Nallapati, Murali Krishna Ramanathan, Dan Roth, and Bing Xiang. 2023. ReCode: Robustness Evaluation of Code Generation Models. In
Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , Anna Rogers, Jordan Boyd-Graber,
and Naoaki Okazaki (Eds.). Association for Computational Linguistics, Toronto, Canada, 13818‚Äì13843. doi:10.18653/v1/2023.acl-long.773
[165] Xuwu Wang, Qiwen Cui, Yunzhe Tao, Yiran Wang, Ziwei Chai, Xiaotian Han, Boyi Liu, Jianbo Yuan, Jing Su, Guoyin Wang, Tingkai Liu, Liyu
Chen, Tianyi Liu, Tao Sun, Yufeng Zhang, Sirui Zheng, Quanzeng You, Yang Yang, and Hongxia Yang. 2024. BabelBench: An Omni Benchmark for
Code-Driven Analysis of Multimodal and Multistructured Data . doi:10.48550/arXiv.2410.00773 arXiv:2410.00773 [cs]
[166] Laura Wartschinski, Yannic Noller, Thomas Vogel, Timo Kehrer, and Lars Grunske. 2022. VUDENC: vulnerability detection with deep learning on a
natural codebase for Python. Information and Software Technology 144 (2022), 106809.
[167] Jialiang Wei, Anne-Lise Courbis, Thomas Lambolais, Binbin Xu, Pierre Louis Bernard, G√©rard Dray, and Walid Maalej. 2024. GUing: a mobile GUI
search engine using a vision-language model. (2024). doi:10.1145/3702993
[168] Jiayi Wei, Greg Durrett, and Isil Dillig. 2024. Coeditor: Leveraging Repo-level Diffs for Code Auto-editing. In The Twelfth International Conference
on Learning Representations . https://openreview.net/forum?id=ALVwQjZRS8
[169] Jiayi Wei, Maruth Goyal, Greg Durrett, and Isil Dillig. 2020. Lambdanet: Probabilistic type inference using graph neural networks. arXiv preprint
arXiv:2005.02161 (2020).
[170] Ratnadira Widyasari, Sheng Qin Sim, Camellia Lok, Haodi Qi, Jack Phan, Qijin Tay, Constance Tan, Fiona Wee, Jodie Ethelda Tan, Yuheng Yieh,
et al.2020. Bugsinpy: a database of existing bugs in python programs to enable controlled testing and debugging studies. In Proceedings of the 28th
ACM joint meeting on european software engineering conference and symposium on the foundations of software engineering . 1556‚Äì1560.
[171] Chengyue Wu, Yixiao Ge, Qiushan Guo, Jiahao Wang, Zhixuan Liang, Zeyu Lu, Ying Shan, and Ping Luo. 2024. Plot2Code: A Comprehensive Benchmark
for Evaluating Multi-modal Large Language Models in Code Generation from Scientific Plots . doi:10.48550/arXiv.2405.07990 arXiv:2405.07990 [cs]
[172] Yi Wu, Nan Jiang, Hung Viet Pham, Thibaud Lutellier, Jordan Davis, Lin Tan, Petr Babkin, and Sameena Shah. 2023. How effective are neural
networks for fixing security vulnerabilities. In Proceedings of the 32nd ACM SIGSOFT International Symposium on Software Testing and Analysis .
1282‚Äì1294.
[173] Chunqiu Steven Xia, Yuxiang Wei, and Lingming Zhang. 2023. Automated program repair in the era of large pre-trained language models. In 2023
IEEE/ACM 45th International Conference on Software Engineering (ICSE) . IEEE, 1482‚Äì1494.
Manuscript submitted to ACM34 Wang et al.
[174] Yiqing Xie, Alex Xie, Divyanshu Sheth, Pengfei Liu, Daniel Fried, and Carolyn Rose. 2024. Codebenchgen: Creating scalable execution-based code
generation benchmarks. arXiv preprint arXiv:2404.00566 (2024).
[175] Frank F Xu, Uri Alon, Graham Neubig, and Vincent Josua Hellendoorn. 2022. A systematic evaluation of large language models of code. In
Proceedings of the 6th ACM SIGPLAN international symposium on machine programming . 1‚Äì10.
[176] Ruiyang Xu, Jialun Cao, Yaojie Lu, Hongyu Lin, Xianpei Han, Ben He, Shing-Chi Cheung, and Le Sun. 2024. CRUXEval-X: A Benchmark for
Multilingual Code Reasoning, Understanding and Execution. arXiv preprint arXiv:2408.13001 (2024).
[177] Xiaodan Xu, Chao Ni, Xinrong Guo, Shaoxuan Liu, Xiaoya Wang, Kui Liu, and Xiaohu Yang. 2024. Distinguishing LLM-generated from Human-
written Code by Contrastive Learning. ACM Transactions on Software Engineering and Methodology (2024).
[178] Weixiang Yan, Haitian Liu, Yunkun Wang, Yunzhe Li, Qian Chen, Wen Wang, Tingyu Lin, Weishan Zhao, Li Zhu, Hari Sundaram, and Shuiguang
Deng. 2024. CodeScope: An Execution-Based Multilingual Multitask Multidimensional Benchmark for Evaluating LLMs on Code Understanding and
Generation. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , Lun-Wei Ku, Andre
Martins, and Vivek Srikumar (Eds.). Association for Computational Linguistics, Bangkok, Thailand, 5511‚Äì5558. doi:10.18653/v1/2024.acl-long.301
[179] Weixiang Yan, Yuchen Tian, Yunzhe Li, Qian Chen, and Wen Wang. 2023. CodeTransOcean: A Comprehensive Multilingual Benchmark for Code
Translation. In Findings of the Association for Computational Linguistics: EMNLP 2023 , Houda Bouamor, Juan Pino, and Kalika Bali (Eds.). Association
for Computational Linguistics, Singapore, 5067‚Äì5089. doi:10.18653/v1/2023.findings-emnlp.337
[180] Jian Yang, Jiaxi Yang, Ke Jin, Yibo Miao, Lei Zhang, Liqun Yang, Zeyu Cui, Yichang Zhang, Binyuan Hui, and Junyang Lin. 2024. Evaluating and
aligning codellms on human preference. arXiv preprint arXiv:2412.05210 (2024).
[181] Yilong Yang, Wei Ke, Jing Yang, and Xiaoshan Li. 2019. Integrating UML with service refinement for requirements modeling and analysis. IEEE
Access 7 (2019), 11599‚Äì11612.
[182] Zhou Yang, Jieke Shi, Premkumar Devanbu, and David Lo. 2024. Ecosystem of large language models for code. arXiv preprint arXiv:2405.16746
(2024).
[183] Zhiyu Yang, Zihan Zhou, Shuo Wang, Xin Cong, Xu Han, Yukun Yan, Zhenghao Liu, Zhixing Tan, Pengyuan Liu, Dong Yu, Zhiyuan Liu, Xiaodong
Shi, and Maosong Sun. 2024. MatPlotAgent: Method and Evaluation for LLM-Based Agentic Scientific Data Visualization. In Findings of the
Association for Computational Linguistics: ACL 2024 , Lun-Wei Ku, Andre Martins, and Vivek Srikumar (Eds.). Association for Computational
Linguistics, Bangkok, Thailand, 11789‚Äì11804. doi:10.18653/v1/2024.findings-acl.701
[184] Pengcheng Yin, Bowen Deng, Edgar Chen, Bogdan Vasilescu, and Graham Neubig. 2018. Learning to Mine Aligned Code and Natural Language
Pairs from Stack Overflow. In International Conference on Mining Software Repositories (MSR) . ACM, 476‚Äì486. doi:10.1145/3196398.3196408
[185] Hao Yu, Bo Shen, Dezhi Ran, Jiaxin Zhang, Qi Zhang, Yuchi Ma, Guangtai Liang, Ying Li, Qianxiang Wang, and Tao Xie. 2024. Codereval: A
benchmark of pragmatic code generation with generative pre-trained models. In Proceedings of the 46th IEEE/ACM International Conference on
Software Engineering . 1‚Äì12.
[186] Tao Yu, Rui Zhang, He Yang Er, Suyi Li, Eric Xue, Bo Pang, Xi Victoria Lin, Yi Chern Tan, Tianze Shi, Zihan Li, et al .2019. Cosql: A conversational
text-to-sql challenge towards cross-domain natural language interfaces to databases. arXiv preprint arXiv:1909.05378 (2019).
[187] Tao Yu, Rui Zhang, Kai Yang, Michihiro Yasunaga, Dongxu Wang, Zifan Li, James Ma, Irene Li, Qingning Yao, Shanelle Roman, et al .2018. Spider:
A large-scale human-labeled dataset for complex and cross-domain semantic parsing and text-to-sql task. arXiv preprint arXiv:1809.08887 (2018).
[188] Tao Yu, Rui Zhang, Michihiro Yasunaga, Yi Chern Tan, Xi Victoria Lin, Suyi Li, Heyang Er, Irene Li, Bo Pang, Tao Chen, et al .2019. Sparc:
Cross-domain semantic parsing in context. arXiv preprint arXiv:1906.02285 (2019).
[189] Sukmin Yun, Haokun Lin, Rusiru Thushara, Mohammad Qazim Bhat, Yongxin Wang, Zutao Jiang, Mingkai Deng, Jinhong Wang, Tianhua Tao,
Junbo Li, Haonan Li, Preslav Nakov, Timothy Baldwin, Zhengzhong Liu, Eric P. Xing, Xiaodan Liang, and Zhiqiang Shen. 2024. Web2Code:
a large-scale webpage-to-code dataset and evaluation framework for multimodal llms. In Advances in neural information processing systems
(2024), A. Globerson, L. Mackey, D. Belgrave, A. Fan, U. Paquet, J. Tomczak, and C. Zhang (Eds.), Vol. 37. Curran Associates, Inc., 112134‚Äì112157.
https://proceedings.neurips.cc/paper_files/paper/2024/file/cb66be286795d71f89367d596bf78ea7-Paper-Datasets_and_Benchmarks_Track.pdf
[190] Fengji Zhang, Bei Chen, Yue Zhang, Jacky Keung, Jin Liu, Daoguang Zan, Yi Mao, Jian-Guang Lou, and Weizhu Chen. 2023. Repocoder:
Repository-level code completion through iterative retrieval and generation. arXiv preprint arXiv:2303.12570 (2023).
[191] Kechi Zhang, Jia Li, Ge Li, Xianjie Shi, and Zhi Jin. 2024. Codeagent: Enhancing code generation with tool-integrated agent systems for real-world
repo-level coding challenges. arXiv preprint arXiv:2401.07339 (2024).
[192] Xiaoyu Zhang, Juan Zhai, Shiqing Ma, Qingshuang Bao, Weipeng Jiang, Chao Shen, and Yang Liu. 2025. Unveiling Provider Bias in Large Language
Models for Code Generation. arXiv preprint arXiv:2501.07849 (2025).
[193] Ziyin Zhang, Chaoyu Chen, Bingchang Liu, Cong Liao, Zi Gong, Hang Yu, Jianguo Li, and Rui Wang. 2023. Unifying the perspectives of nlp and
software engineering: A survey on language models for code. arXiv preprint arXiv:2311.07989 (2023).
[194] Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, et al .
2023. A survey of large language models. arXiv preprint arXiv:2303.18223 1, 2 (2023).
[195] Dewu Zheng, Yanlin Wang, Ensheng Shi, Ruikai Zhang, Yuchi Ma, Hongyu Zhang, and Zibin Zheng. 2024. Towards more realistic evaluation of
LLM-based code generation: an experimental study and beyond. arXiv preprint arXiv:2406.06918 (2024).
[196] Jiasheng Zheng, Boxi Cao, Zhengzhao Ma, Ruotong Pan, Hongyu Lin, Yaojie Lu, Xianpei Han, and Le Sun. 2024. Beyond Correctness: Benchmarking
Multi-dimensional Code Generation for Large Language Models. arXiv:2407.11470 [cs.SE] https://arxiv.org/abs/2407.11470
Manuscript submitted to ACMSoftware Development Life Cycle Perspective: A Survey of Benchmarks for Code Large Language Models and Agents 35
[197] Qinkai Zheng, Xiao Xia, Xu Zou, Yuxiao Dong, Shan Wang, Yufei Xue, Zihan Wang, Lei Shen, Andi Wang, Yang Li, et al .2023. Codegeex: A
pre-trained model for code generation with multilingual evaluations on humaneval-x. arXiv preprint arXiv:2303.17568 (2023).
[198] Yihang Zheng, Bo Li, Zhenghao Lin, Yi Luo, Xuanhe Zhou, Chen Lin, Jinsong Su, Guoliang Li, and Shifu Li. 2024. Revolutionizing Database Q&A
with Large Language Models: Comprehensive Benchmark and Evaluation . doi:10.48550/arXiv.2409.04475 arXiv:2409.04475 [cs]
[199] Zibin Zheng, Kaiwen Ning, Yanlin Wang, Jingwen Zhang, Dewu Zheng, Mingxi Ye, and Jiachi Chen. 2024. A Survey of Large Language Models for
Code: Evolution, Benchmarking, and Future Trends. doi:10.48550/arXiv.2311.10372 arXiv:2311.10372 [cs]
[200] Zibin Zheng, Kaiwen Ning, Qingyuan Zhong, Jiachi Chen, Wenqing Chen, Lianghong Guo, Weicheng Wang, and Yanlin Wang. 2025. Towards an
understanding of large language models in software engineering tasks. Empirical Software Engineering 30, 2 (2025), 50.
[201] Li Zhong and Zilong Wang. 2024. Can LLM Replace Stack Overflow? A Study on Robustness and Reliability of Large Language Model Code
Generation. In Proceedings of the AAAI Conference on Artificial Intelligence , Vol. 38. 21841‚Äì21849.
[202] Victor Zhong, Caiming Xiong, and Richard Socher. 2017. Seq2sql: Generating structured queries from natural language using reinforcement
learning. arXiv preprint arXiv:1709.00103 (2017).
[203] Xin Zhou, Martin Weyssow, Ratnadira Widyasari, Ting Zhang, Junda He, Yunbo Lyu, Jianming Chang, Beiqi Zhang, Dan Huang, and David Lo.
2025. LessLeak-Bench: A First Investigation of Data Leakage in LLMs Across 83 Software Engineering Benchmarks. doi:10.48550/arXiv.2502.06215
arXiv:2502.06215 [cs]
[204] Xin Zhou, Ting Zhang, and David Lo. 2024. Large language model for vulnerability detection: Emerging results and future directions. In Proceedings
of the 2024 ACM/IEEE 44th International Conference on Software Engineering: New Ideas and Emerging Results . 47‚Äì51.
[205] Yaqin Zhou, Shangqing Liu, Jingkai Siow, Xiaoning Du, and Yang Liu. 2019. Devign: Effective vulnerability identification by learning comprehensive
program semantics via graph neural networks. Advances in neural information processing systems 32 (2019).
[206] Jieming Zhu, Shilin He, Pinjia He, Jinyang Liu, and Michael R Lyu. 2023. Loghub: A large collection of system log datasets for ai-driven log
analytics. In 2023 IEEE 34th International Symposium on Software Reliability Engineering (ISSRE) . IEEE, 355‚Äì366.
[207] Qiming Zhu, Jialun Cao, Yaojie Lu, Hongyu Lin, Xianpei Han, Le Sun, and Shing-Chi Cheung. 2024. DOMAINEVAL: An Auto-Constructed
Benchmark for Multi-Domain Code Generation. arXiv preprint arXiv:2408.13204 (2024).
[208] Yufan Zhuang, Sahil Suneja, Veronika Thost, Giacomo Domeniconi, Alessandro Morari, and Jim Laredo. 2021. Software vulnerability detection via
deep learning over disaggregated code graph representation. arXiv preprint arXiv:2109.03341 (2021).
[209] Terry Yue Zhuo, Minh Chien Vu, Jenny Chim, Han Hu, Wenhao Yu, Ratnadira Widyasari, Imam Nur Bani Yusuf, Haolan Zhan, Junda He, Indraneil
Paul, et al .2024. Bigcodebench: Benchmarking code generation with diverse function calls and complex instructions. arXiv preprint arXiv:2406.15877
(2024).
[210] Andy Zou, Zifan Wang, Nicholas Carlini, Milad Nasr, J Zico Kolter, and Matt Fredrikson. 2023. Universal and transferable adversarial attacks on
aligned language models. arXiv preprint arXiv:2307.15043 (2023).
[211] Deqing Zou, Sujuan Wang, Shouhuai Xu, Zhen Li, and Hai Jin. 2019. ùúáVulDeePecker: A Deep Learning-Based System for Multiclass Vulnerability
Detection. IEEE Transactions on Dependable and Secure Computing 18, 5 (2019), 2224‚Äì2236.
Manuscript submitted to ACM