Towards Training Reproducible Deep Learning Models
Boyuan Chen
Centre for Software Excellence,
Huawei Canada
Kingston, Canada
boyuan.chen1@huawei.comMingzhi Wen
Huawei Technologies
Shenzhen, China
wenmingzhi@huawei.comYong Shi
Huawei Technologies
Shenzhen, China
young.shi@huawei.com
Dayi Lin
Centre for Software Excellence,
Huawei Canada
Kingston, Canada
dayi.lin@huawei.comGopi Krishnan Rajbahadur
Centre for Software Excellence,
Huawei Canada
Kingston, Canada
gopi.krishnan.rajbahadur1@huawei.comZhen Ming (Jack) Jiang
York University
Toronto, Canada
zmjiang@eecs.yorku.ca
ABSTRACT
ReproducibilityisanincreasingconcerninArtificialIntelligence
(AI),particularlyintheareaofDeepLearning(DL).Beingableto
reproduceDLmodelsiscrucialforAI-basedsystems,asitisclosely
tied to various tasks like training, testing, debugging, and audit-
ing. However, DLmodels are challengingto be reproduced dueto
issues like randomness in the software (e.g., DL algorithms) and
non-determinism in the hardware (e.g., GPU). There are various
practices to mitigate some of the aforementioned issues. However,
manyofthemareeithertoointrusiveorcanonlyworkforaspe-
cificusagecontext.Inthispaper,weproposeasystematicapproach
totrainingreproducibleDLmodels.Ourapproachincludesthree
mainparts:(1)asetofgeneralcriteriatothoroughlyevaluatethe
reproducibilityofDLmodelsfortwodifferentdomains,(2)auni-
fied framework which leverages a record-and-replay technique
tomitigatesoftware-relatedrandomnessandaprofile-and-patch
technique to control hardware-related non-determinism, and (3) a
reproducibilityguidelinewhichexplainstherationalesandthemit-igationstrategiesonconductingareproducibletrainingprocessfor
DL models. Case study results show our approach can successfully
reproduce six open source and one commercial DL models.
CCS CONCEPTS
â€¢Softwareanditsengineering â†’Empiricalsoftwarevalida-
tion.
KEYWORDS
Artificial Intelligence, Deep Learning, Software Engineering, Re-
producibility
ACM Reference Format:
BoyuanChen,MingzhiWen,YongShi,DayiLin,GopiKrishnanRajbahadur,
and Zhen Ming (Jack) Jiang. 2022. Towards Training Reproducible Deep
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation
on the first page. Copyrights for components of this work owned by others than ACMmustbehonored.Abstractingwithcreditispermitted.Tocopyotherwise,orrepublish,topostonserversortoredistributetolists,requirespriorspecificpermissionand/ora
fee. Request permissions from permissions@acm.org.
ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
Â© 2022 Association for Computing Machinery.
ACM ISBN 978-1-4503-9221-1/22/05...$15.00
https://doi.org/10.1145/3510003.3510163LearningModels.In 44thInternationalConferenceonSoftwareEngineering
(ICSE â€™22), May 21â€“29, 2022, Pittsburgh, PA, USA. ACM, New York, NY, USA,
13 pages. https://doi.org/10.1145/3510003.3510163
1 INTRODUCTION
In recent years, Artificial Intelligence (AI) has been advancing
rapidly both in research and practice. A recent report by McKinsey
estimates that AI-based applications have the potential market val-
uesrangingfrom$3.5and$5.8trillionannually[ 11].Manyofthese
applications,whichcanperformcomplextaskssuchasautonomous
driving[34],speechrecognition[ 24],andhealthcare[ 29],areen-
abledbyvariousDeepLearning(DL)models[ 46].Unliketraditional
softwaresystems,whichareprogrammedbasedondeterministic
rules (e.g., if/else), the DL models within AI-based systems are con-
structedinastochasticwayduetotheunderlyingDLalgorithms,
whose behavior may not be reproducible and trustworthy [ 26,54].
EnsuringthereproducibilityofDLmodelsisvitalfornotonlymany
productdevelopmentrelatedtaskssuchastraining[ 50],testing[ 18],
debugging [ 56] and legal compliance [ 2], but also facilitating scien-
tific movements like open science [66, 67].
OneoftheimportantstepstowardsreproducibleAI-basedsys-
temsistoensurethereproducibilityoftheDLmodelsduringthe
training process. A DL model is reproducible, if under the same
training setup (e.g., the same training code, the same environment,
and the same training dataset), the resulting trained DL modelyields the same results under the same evaluation criteria (e.g.,
thesameevaluationmetricsonthesametestingdataset)[ 56,57].
Unfortunately, recent studies show that AI faces reproducibility
crisis[37,41],especiallyforDLmodels[ 32,44,48,50,56,58,63,65].
In general, there are three main challenges associated with this:
â€¢Randomnessinthesoftware[61] :Randomnessisessentialin
DLmodeltraininglikebatchordering,datashuffling,andweight
initialization for constructing robust and high-performing DL
models[14,56].However,randomnesspreventstheDLmodels
frombeingreproduced.Toachievereproducibilityinthetraining
process, the current approach is to set predefinedrandom seeds
beforethetrainingprocess.Althoughthisapproachiseffectivein
controlling the randomness, it has three drawbacks: (1) it might
cause the training process to converge to local optimums and
notabletoexploreotheroptimizationopportunities;(2)itisnon-
trivial to select the appropriate seeds as there are no existing
22022022 IEEE/ACM 44th International Conference on Software Engineering (ICSE)
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:54:36 UTC from IEEE Xplore.  Restrictions apply. ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA Chen and Wen, et al.
techniques for tuning random seeds during the hyperparameter
tuningprocess;(3)non-trivialmanualeffortsareneededtolocate
randomness introducing functions and instrument them with
seeds for the imported libraries and their dependencies.
â€¢Non-determinism inthe hardware [6] :TrainingDLmodels
requires intensive computing resources. For example, many ma-
trixoperationsoccurinthebackwardpropagation,whichcon-
sistsofahugeamountoffloatingpointoperations.AsGPUshave
way more numbers of cores than CPUs, GPUs are often used for
runningDLtrainingprocessesduetotheirabilitytoprocessmul-
tiple operations in parallel. However, exe cuting floating point
calculationinparallelbecomesasourceofnon-determinismsince
theresultsoffloating-pointoperationsaresensitivetocompu-
tation orders due to rounding errors [ 33,56]. In addition, GPU
specific libraries (e.g., CUDA [ 4] and cuDNN [ 27]) by default
auto-select the optimal primitive operations based on compar-ing different algorithms of operations during runtime (i.e., the
auto-tuning feature). However, the comparison results might be
non-deterministic dueto issueslike floatingpoint computation
mentioned above [ 14,56]. These sources of non-determinism
from hardware need to be controlled in order to construct repro-
ducible DL models. Case-by-case solutions have been proposed
to tackle specific issues. For example, both Pytorch [ 55] and Ten-
sorFlow[ 22]provideconfigurationsondisablingtheauto-tuning
feature.Unfortunately,noneofthesetechniqueshavebeenem-
pirically validated in literature. Furthermore, there is still a lack
of a general technique which can work across different software
frameworks.
â€¢Lack of systematic guidelines [56] : Various checklists and
documentationframeworks[ 19,30,53]havebeenproposedon
asset management to support DL reproducibility. There are gen-
erallyfourtypesofassetstomanageinmachinelearning(ML)
in order to achieve model reproducibility: resources (e.g., dataset
and environment), software (e.g., source code), metadata (e.g.,
dependencies),andexecutiondata(e.g.,executionresults)[ 43].
However,priorwork[ 23,25,43]showstheseassetsshouldnot
be managed with the same toolsets (e.g., Git) used for source
code [23]. Hence, newversion managementtools (e.g., DVC [ 12]
andMLflow[ 1])arespecificallydesignedformanagingMLas-
sets. However, even by adopting the techniques and suggestions
mentionedabove,DLmodelscannotbefullyreproduced[ 56]due
toproblemsmentionedintheabovetwochallenges.Asystem-
atic guideline is needed for both researchers and practitioners in
order to construct reproducible DL models.
Toaddresstheabovechallenges,inthispaper,wehaveproposed
a systematic approach towards training reproducible DL models.
Our approach includes a set of rigorously defined evaluation crite-
ria,arecord-and-replay-basedtechniqueformitigatingrandomnessinsoftware,andaprofile-and-patch-basedtechniqueformitigating
non-determinism from hardware. We have also provided a system-
aticguidelineforDLmodelreproducibilitybasedonourexperience
on applying our approach across different DL models. Case studies
onsixpopularopensourceandonecommercialDLmodelsshow
that our approach can successfully reproduce the studied DL mod-
els(i.e.,thetrainedmodelsachievetheexactsameresultsunderthe
evaluation criteria). To facilitate reproducibility of our study, weprovideareplicationpackage[ 21],whichconsistsoftheimplemen-
tation of open source DL models, our tool, and the experimental
results. In summary, our paper makes thefollowing contributions:
â€¢Althoughthere arepreviousresearch workwhich aimed atre-
producible DL models (e.g., [ 50,56]), to the authorsâ€™ knowledge,
our approach is the first systematic approach which can achieve
reproducible DL models during the training process. Case study
resultsshowthatallthestudiedDLmodelscanbesuccessfully
reproduced by leveraging our approach.
â€¢Compared to existing practices for controlling randomness in
the software (a.k.a., presetting random seeds [ 5,56]), our record-
and-replay-based technique is non-intrusive and incurs minimal
disruption on the existing DL development process.
â€¢Compared to the previous approach on verifying model repro-
ducibility[ 56],ourproposedevaluationcriteriahastwoadvan-
tages: (1) it is more general, as it covers multiple domains (Clas-
sification and Regression tasks), and (2) it is more rigorous, as it
evaluatesmultiplecriteria,whichincludesnotonlytheevalua-
tionresultsonthetestingdataset,butalsotheconsistencyofthe
training process.
Paper Organization. Section 2 provides background information
associatedwithDLmodelreproducibility.Section3describesthe
details of our systematic approach to training reproducible DL
models.Section4presentstheevaluationofourapproach.Section5
discusses the experiences and lessons learned from applying our
approach. Section 6 presents our guideline. Section 7 describes the
threats to validity of our study and Section 8 concludes our paper.
2 BACKGROUND AND RELATED WORK
Inthissection,wedescribethebackgroundandtherelatedwork
associated with constructing reproducible DL models.
2.1 The Need for Reproducible DL models
Severaltermshavebeenusedinexistingliteraturetodiscussthe
concepts of reproducibility in research and practice [ 5,14,17,19,
40,50,56,57]. We follow the similar definitions used in [ 56,57],
whereaparticularpieceofworkisconsideredas reproducible,ifthe
samedata,samecode,andsameanalysisleadtothesameresults
or conclusions. On the other hand, replicable research refers to
that different data (from the same distribution of the original data)
combinedwith samecodeand analysis resultinsimilar results.In
thispaper,wefocusonthereproducibilityofDLmodelsduringthetrainingprocess.Thesametrainingprocessrequirestheexactsame
setup, which includes the same source code (including training
scripts and configurations), the same training and testing data, and
the same environment.
Training reproducible DL models is essential in both research
andpractice.Ononehand,itfacilitatestheopensciencemovement
by enabling researchers to easily reproduce the same results. Open
sciencemovement [ 66,67]promotessharing researchassetsina
transparent way, so that the quality of research manuscripts canbe checked and improved. On the other hand, many companies
arealsointegratingthecutting-edgeDLresearchintotheirprod-
ucts. Having reproducible DL models would greatly benefit the
product development process. For example, if a DL model is repro-
ducible, the testing and debugging processes would be much easier
2203
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:54:36 UTC from IEEE Xplore.  Restrictions apply. Towards Training Reproducible Deep Learning Models ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
astheproblematicbehaviorcanbeconsistentlyreproduced[ 18].In
addition, manyDL-based applicationsnowrequire regulatorycom-
plianceandaresubjecttorigorousauditingprocesses[ 13].Itisvital
thatthebehavioroftheDLmodelsconstructedduringtheauditing
process closely matches with that of the released version [2].
2.2 Current State of Reproducible DL Models
2.2.1 Reproducibilitycrisis. In2018,Huston[ 41]mentioneditis
very difficult to verify many claims published in research papers
due to the lack of code and the sensitivity of training conditions
(a.k.a., the reproducibility crisis in AI). Similarly, Gundersen and
Kjensmo [ 37] surveyed 400 research papers from IJCAI and AAAI
and found that only 6% of papers provided experiment code. Simi-
larly,insoftwareengineeringresearch,Liuetal.[ 50] surveyed 93
SE research papers which leveraged DL techniques and only 10.8%
ofresearchdiscussedreproducibilityrelatedissues.IsdahlandGun-
dersen [44] surveyed 13 state of the art ML platforms and found
thepopularMLplatformsprovidedbywell-knowncompanieshavepoorsupportforreproducibility,especiallyintermsofdata.Instead
of verifying and reporting the reproducibility of different research
work, we focus on proposing a new approach which can construct
reproducible DL models.
2.2.2 Effortstowardsimprovingreproducibility. Variouseffortshave
been devoted to improve the reproducibility of DL models:
(E1) Controlling Randomness from software. Liu et al. [ 50]
found that the randomness in software could impact the repro-ducibility of DL models and only a few studies (e.g., [
28,36,40])
reportedusingpresetseedstocontroltherandomness.Similarly,
Pham et al. [ 56] found that by controlling randomness in software,
the performance variances in trained DL models decrease signif-icantly. Sugimura and Hartl [
64] mentioned that a random seed
needs to be set as a hyperparameter prior to training for repro-
ducibility.Determined.AI[ 5],acompanythatfocusesonproviding
servicesforDLmodeltraining,alsosupportssettingseedsforre-
producing DL experiments. However, none of the prior studies
discussed how to properly set seeds or the performance impact of
different set of seeds. Compared to presetting random seeds, our
record-and-replay-basedtechniquetocontroltherandomnessin
the software is non-intrusive and incurs minimal disruption on the
existing DL development.
(E2) Mitigating non-determinism in the hardware. Pham et
al. [56] discussed using environment variables to mitigate non-
determinismcausedbyfloatingpointroundingerrorandparallelcomputation.Jooybaretal[
45]designedanewGPUarchitecture
for deterministic operations. However, there has been a lack of
thoroughassessmentoftheproposedsolutions.Inaddition,ourap-proachmainlyfocusesonmitigatingnon-determinismoncommon
hardware instead of proposing new hardware design.
(E3) Existing guidelines and best practices. To address the re-
producibility crisis mentioned above, major AI conferences such as
NeurIPS, ICML, and AAAI hold reproducibility workshops and ad-
vocate researchers to independently verify the results of published
research papers as reproducibility challenges. Various documen-tationframeworksforDLmodels[
30,53]orchecklists[ 19]have
beenproposedrecently.Thesedocumentationsspecifytherequired
information and artifacts (e.g., datasets, code, and experimentalresults) that are needed to reproduce DL models. Similarly, Ghanta
et al [32] investigated AI reproducibility in production, where they
mentioned many factors need to be considered to achieve repro-ducibility such as pipeline configuration and input data. Tatman
etal.[65]indicatedthathighreproducibilityisachievedbyman-
aging code, data, and environment. They suggest in order to reach
the highest reproducibility, the runtime environment should beprovided as hosting services, containers, or VMs. Sugimura andHartl [
64] built an end-to-end reproducible ML pipeline which
focusesondata,feature,model,andsoftwareenvironmentprove-
nance. In our study, we mainly focus on model training with theassumption that the code, data, and environment should be con-sistent across repeated training processes. However, even with
consistent assets mentioned above, it is still challenging to achieve
reproducibility due to the lack of tool support and neglection of
certain sources of non-determinism [32, 44, 48, 56, 58, 63, 65].
2.3 Industrial Assessment
Huaweiis a large IT company, which provides many products and
services relying on AI-based components. To ensure the quality,
trustworthiness,transparency,andtraceabilityoftheproducts,prac-titioners in Huaweihave been investigating approaches to training
reproducible DL models. We worked closely with 20 practitioners,
who are either software developers or ML scientists with Ph.D
degrees. Their tasks are to prototype DL models and/or produc-tionalizeDLmodels.Wefirstpresentedthecurrentresearchand
practices on verifying and achieving reproducibility in DL models.
Then we conducted a two hour long semi-formal interview with
thesepractitionerstogathertheiropinionsonwhethertheexisting
work can help them address their DL model reproducibility issues
in practice. We summarized their opinions below:Randomness in the Software
:Practitionersareawarethatcur-
rently the most effective approach to control the randomness inthe software is to set seeds prior to training. However, they are
reluctant to adopt such practice due to the following two reasons:
(1)a variety of usage context: for example, in software testing, they
would like to reserve the randomness so that more issues can be
exposed. However, after the issue is identified, they find it difficult
toreproducethesameissueinthenextrun.Settingseedscannot
meettheirneedsinthiscontext.(2) Sub-optimalperformance: DL
models often require fine-tuning to reach the best performance.
Currently,theDLtrainingreliesoncertainlevelsofrandomnessto
avoid local optimums. Setting seeds may have negative impacts on
themodelperformance.AlthoughtoolslikeAutoML[ 42]havebeen
recentlywidelyadoptedforselectingtheoptimalhyperparameters,
therearenoexisting techniqueswhich incorporaterandomseeds
as part of their tuning or searching processes.Non-determinismintheHardware
:Thereareresearchandgrey
literature (e.g.,technical documentations[ 14], blogposts [ 15]) de-
scribing techniques to mitigate the non-determinism in hardware
orproposingnewhardwarearchitecture[ 45].However,inanin-
dustrialcontext,adoptingnewhardwarearchitectureisimpractical
due to the additional costs and the lack of evaluation and support.
Inaddition,thementionedapproaches(e.g.,settingenvironment
variables) are not extensively evaluated on the effectiveness and
2204
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:54:36 UTC from IEEE Xplore.  Restrictions apply. ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA Chen and Wen, et al.
overhead. Hence, a systematic empirical study is needed before
applying such techniques in practices.
ReproducibilityGuidelines :Theyhavealreadyappliedbestprac-
ticestomanagetheassets(e.g.,codeanddata)usedduringtraining
processes by employing data and experiment management tools.
However,theyfoundtheDLmodelsarestillnotreproducible.In
addition, they mentioned that existing techniques in this area does
not cover all of their use cases. For example, existing evaluation
criteria for DL model reproducibility works for classification tasks
(e.g., [56]), but not for regression tasks, which are the usage con-
texts for many DL models within Huawei. Hence, they prefer a
systematicguidelinewhichstandardizesmanyofthesebestprac-
tices across various sources and usage context so that they can
promote and enforce them within their organizations.
Inspiredbytheabovefeedback,webelieveitisworthwhileto
proposeasystematicapproachtowardstrainingreproducibleDL
models.Wewilldescribeourapproachindetailsinthenextsection.
3 OUR APPROACH
Here we describe our systematic approach towards reproducing
DL models. Section 3.1 provides an overview of our approach. Sec-
tion 3.2 to 3.6 explain each phase in detail with a running example.
3.1 Overview
There are different stages in the DL workflow [ 23]. The focus of
ourpaperistrainingreproducibleDLmodels.Hence,weassume
thedatasetsandextractedfeaturesarealreadyavailableandcanbe
retrieved in a consistent manner.
Figure 1 presents the overview of our approach, which consists
of five phases. (1) During the Conducting initial training phase, we
prepare the training environment and conduct the training process
twicetogeneratetwoDLmodels: ğ‘€ğ‘œğ‘‘ğ‘’ğ‘™ğ‘¡ğ‘ğ‘Ÿğ‘”ğ‘’ğ‘¡andğ‘€ğ‘œğ‘‘ğ‘’ğ‘™ğ‘Ÿğ‘’ğ‘ğ‘Ÿğ‘œ.(2)
Duringthe Verifyingmodelreproducibility phase,thetwoDLmodels
fromthepreviousphaseareevaluatedonasetofcriteriatocheckif
theyyieldthesameresults. Ifyes, ğ‘€ğ‘œğ‘‘ğ‘’ğ‘™ğ‘¡ğ‘ğ‘Ÿğ‘”ğ‘’ğ‘¡isreproducibleand
theprocessiscompleted.Wealsowillupdatethereproducibility
guideline if there are any new mitigation strategies that have been
introducedduringthisprocess.Ifnot,wewillproceed tothenext
phase. (3) During the Profiling and diagnosing phase, the system
callsandfunctioncallsareprofiled.Suchdataisusedtodiagnose
and identify the root causes behind non-reproducibility. (4) During
theUpdating phase, to mitigate newly identified sources of non-
determinism, the system calls that need to be intercepted by the
record-and-replaytechniqueareupdatedandthenon-deterministicoperationsduetohardwarearepatched.(5)Duringthe Record-and-
replayphase,thesystemcalls,whichintroducerandomnessduring
training, are first recorded and then replayed. Two DL models,
ğ‘€ğ‘œğ‘‘ğ‘’ğ‘™ğ‘¡ğ‘ğ‘Ÿğ‘”ğ‘’ğ‘¡andğ‘€ğ‘œğ‘‘ğ‘’ğ‘™ğ‘Ÿğ‘’ğ‘ğ‘Ÿğ‘œ,areupdatedwiththeDLmodelsduring
the recording and replaying steps, respectively. These two updated
DLmodelsareverifiedagaininPhase2.Thisprocessisrepeated
until we have a reproducible DL model.
Toeaseexplanation,intherestofthesection,wewilldescribe
our approach using LeNet-5 as our running example. LeNet-5 [ 47]
is a popular open source DL model used for image classification.
ThedatasetusedfortrainingandevaluationisMNIST[ 9],which
consists of a set of 60,000 images for training, 10,000 images fortesting.Eachimageisassignedalabelrepresentingthehandwritten
digits from 0 to 9.
3.2 Phase 1 - Conducting initial training
The objective of this phase is to train two DL models under the
sameexperimentalsetup.Thisphasecanbefurtherbrokendown
into the following three steps:
Step1-Settinguptheexperimentalenvironment. Inthisstep,weset
uptheexperimentalenvironment,whichincludesdownloadingand
configuringthefollowingexperimentalassets:thedataset(s),the
source code for the DL model, and the runtime environment based
on the required software dependencies and the hardware speci-fications [
43]. Generally the experimental assets are recorded in
documentationslikeresearchpapers,reproducibilitychecklist[ 19],
or model cards [ 53] and data sheets [ 30]. For our running example,
documentations are from research papers [ 47,56]. The code for
LeNet-5isadaptedfromapopularopensourcerepository[ 49],and
the MNIST dataset is downloaded from [ 9]. We further split the
dataset into three parts: training, validation, and testing similar
tothepriorwork[ 56].Inparticular,wesplitthe10,000imagesin
testinginto7,500imagesand2,500images.The7,500imagesareused for validation in the training process, and the 2,500 images
are used to evaluate the final model, which arenot exposed to the
training process. We deploy the following runtime environment:
for the software dependencies, we use Python 3.6 with TensorFlow
1.14GPUversion.Forthehardwarespecification,weuseaSUSE
Linux Enterprise Server 12 machine with a Tesla-P100-16GB GPU.
The GPU related libraries are CUDA 10.0.130 and CuDNN 7.5.1.
Step 2 - Training the target DL model. In this step, we invoke the
trainingscriptstogeneratethetargetDLmodel,called ğ‘€ğ‘œğ‘‘ğ‘’ğ‘™ğ‘¡ğ‘ğ‘Ÿğ‘”ğ‘’ğ‘¡.
Duringthetrainingprocess,wecollectthefollowingsetofmetrics:
lossvalues,thetrainingepochs,andthetrainingtime.Thissetof
metrics is called ğ‘ƒğ‘Ÿğ‘œğ‘ğ‘’ğ‘ ğ‘ ğ‘€ğ‘’ğ‘¡ğ‘Ÿğ‘–ğ‘ğ‘ ğ‘¡ğ‘ğ‘Ÿğ‘”ğ‘’ğ‘¡. In our running example, we
invokethetrainingscriptsforLeNet-5toconstructtheDLmodel
and record its metrics.
Step3-Verifyingassetsandretraining. Inthisstep,wefirstverify
whethertheexperimentalassetsareconsistentwiththeinforma-
tionprovidedinstep1.Therearemanyapproachestoverifyingtheexperimental assets. For example, to verify the dataset(s), we check
iftheSHA-1checksumisconsistent.Toverifythesoftwareenvi-
ronment,wecheckthesoftwaredependencyversionsbyreusing
the same environment (e.g., docker, VM) or simply checking all
the installed software packages by commands like pip list . Once
the assets are verified, we perform the same training process asstep 2 to generate another DL model, named as
ğ‘€ğ‘œğ‘‘ğ‘’ğ‘™ğ‘Ÿğ‘’ğ‘ğ‘Ÿğ‘œ.W e
alsorecordthesamesetofmetrics,calledas ğ‘ƒğ‘Ÿğ‘œğ‘ğ‘’ğ‘ ğ‘ ğ‘€ğ‘’ğ‘¡ğ‘Ÿğ‘–ğ‘ğ‘ ğ‘Ÿğ‘’ğ‘ğ‘Ÿğ‘œ,
during the training process. The two DL models along with the
recorded set of metrics will be used in the next phase for verifying
model reproducibility. In our running example, we reuse the same
experimental environment without modifying the source code and
the datasets to ensure the asset consistency. Then we repeat the
trainingprocesstocollectthesecondLeNet-5modelanditsmetrics.
2205
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:54:36 UTC from IEEE Xplore.  Restrictions apply. 2206Towards Training Reproducible Deep Learning Models ICSE '22, May 21-29, 2022, Pittsburgh, PA, USA 
Approach :[Yes 0 ;:ï¿½ï¿½ï¿½ 
l Unsupported 
Reproduc oon-detï¿½stic 
ï¿½ ï¿½ï¿½eratiom-Â· 
1. Conducting initial 
training 2. Verifying model 
reproducibility 3. Profiling and 
diagnosing 4. Updating 5. Record-andÂ­
replay 
Artifacts 
Guideline Model """d Evaluation Process Model._ Evaluation Process Random profile 
------------------------ --mï¿½ï¿½- ----- metriï¿½ ------------- --ï¿½-':!' ï¿½- ----ï¿½ï¿½ï¿½ -------------
Figu re 1: An overview of our approach. 
3.3 Phase 2 -Verifying model reproducibility 
The objective of this phase is to verify if the current training process 
is reproducible by comparing the two DL models against a set of 
evaluation criteria. This phase consists of the following three steps: 
Step 1 -Verifying the reproducibility of the training results. In this 
step, we evaluate the two DL models, Modeltar get and Modelrepro on 
the same testing dataset. Depending on the tasks, we use different 
evaluation metrics: 
â€¢ Classification tasks: For classification tasks, we evaluate three 
metrics: the overall accuracy, per-class accuracy, and the predicÂ­
tion results on the testing dataset. Consider the total number of 
instances in testing datasets is N, â€¢â€¢ ,. The number of correctly 
labeled instances is Ncorrecr For label i, the number of instances 
are Ntest;. The correctly labeled instances of label i is Ncorr.,t; . 
Hence, the overall accuracy is calculated as: Overall accuracy = 
ï¿½. For each label i, the per-class accuracy is calculated as: 
ust 
N 
Per-class accuracy (label i) = Nmcr1 â€¢ In addition, we collect the 
te.st1 
prediction results for every instance in the testing dataset. 
â€¢ Regression tasks: For regression tasks, we evaluate the Mean 
Absolute Error (MAE). The total number of instances in the 
testing dataset is Ntest Consider for each instance, the true obÂ­
served value is Xi and the predicted value is Yt. MAE is cal-
'LNtut IY1-Xd culated as: MAE = M Ntest . These metrics are called as 
EvaluationMetrics,ar get and EvaluationMetricsrepro for these two 
models, respectively. In our running example, we use evaluation 
metrics for classification tasks as LeNet-5 is used for image clasÂ­
sification. 
Step 2 - Verifying the reproducibility of the training process. In 
this step, we compare the collected metrics for Modeltar get and 
Modelre pro (i.e., EvaluationMetrics,ar get vs. Evalu ationMetricsrâ€¢pro 
and ProcessMetricsro pro vs. ProcessMetrics,a rgâ€¢<) by a Python script. 
For evaluation metrics, we check if EvaluationMetrics,ar get and 
EvaluationMetrics,. pro are exactly identical. For process metrics, 
we check if the loss values during each epoch, and the number of 
epochs are the same. 
2206 Step 3-Reporting the results. A DL model is reproducible if both 
the evaluation metrics and the process metrics are identical (except 
for the training time). If the DL models are not reproducible, we 
move on to the next phase. 
In our running example, the two DL models emit different evaluaÂ­
tion metrics. The overall accuracy for the two models are 99.16% and 
98.64%, respectively. For per-class accuracy, the maximum absolute 
differences could be as large as 2.3%. Among the 2,500 prediction 
results, 48 of them are inconsistent. None of the loss values during 
the epochs are the same. The total number of trainin g epochs are 
50 as it is pre-configured. This result shows that the two DL models 
are not reproducible. Hence, we proceed to the next phase. 
3.4 Phase 3 -Profiling and diagnosing 
The objective of this phase is to identify the rationales on why the 
DL models are not reproducible through analysis of the profiled 
results. The output of this phase is a list of system calls that introÂ­
duce software-related randonmess and a list of library calls that 
introduce hardware related non-determinism. This phase consists 
of the following four steps: 
Step 1 -Profiling. This step is further divided into two sub-steps 
based on the type of data, which is profiled: 
Step 1.1 -Profiling system calls: After inspecting the documentation 
and the source code of the DL frameworks, we have found that the 
randonmess from software can be traced to the underlying system 
calls. For example, in TensorFlow, the random number generator 
is controlled by a special file (e.g., /dev/urandom) in the Linux 
environment. When a random number is needed in the training, 
the kernel will invoke a system call to query /dev/urandom for 
a sequence of random bytes. The sequence of random bytes is 
then used by the random generation algorithm (e.g., the Philox 
algorithm [60]) to generate the actual random number used in the 
training process. 
Step 1.2 - Profiling library calls: To mitigate the sources of nonÂ­
determinism in the hardware, popular DL frameworks start to proÂ­
vide environment variables to enhance reproducibility. For examÂ­
ple, in Ten sorFlow 2.1 and above, setting the environment variable 
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:54:36 UTC from IEEE Xplore.  Restrictions apply. ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA Chen and Wen, et al.
TF_CUDNN_DETERMINISTIC tobe"true"couldindicatethecuDNN
librariestodisabletheauto-tuningfeatureandusethedeterministic
operationsinsteadofnon-deterministicones.However,thereare
still many functions that could introduce non-determinism even
after the environment variable is set. In addition, lower versions
of TensorFlow (e.g., 1.14), which does not support such configura-
tion, are still widely used in practice. To address this issue, Nvidia
hasreleasedanopensource repository [ 15]todocumenttheroot
causes of the non-deterministic functions and is currently working
onprovidingpatchesforvariousversionsofTensorFlow.Notallthe
operations could be made deterministic and ongoing efforts are be-
ing made [ 17]. Hence, to diagnose the sources of non-determinism
in hardware, we perform function level profiling to check if any ofthe functions are deemed as non-deterministic. Different from pro-
filingthesystemcalls,whichextractscallinformationatthekernel
level, the goal of profiling the library calls is to extract all the in-
voked function calls at framework level (e.g., tensorflow.shape ).
Inourrunningexample,werepeatthetrainingprocessofLeNet-
5withtheprofilingtools.Weuse stracetoprofilethelistofsystem
callsinvokedduringthetrainingprocess. straceexposestheinter-
actions between processes and the system libraries and lists all the
invoked system calls. We use cProfile , a C-based profiling tool,
to gather the list of invoked functions at the framework level.
Step 2 - Diagnosing sources of randomness. In this step, we analyze
the recorded data from straceto identify the set of system calls
which can contribute to software-related randomness. We consult
with the documentation of system calls and identify the list of
system calls, which causes randomness. This list varies depending
on the versions of the operating systems. For example, the system
callgetrandom isonlyusedinlaterversionofLinuxkernel(version
3.17 and after). Prior to 3.17, only /dev/urandom is used. Hence,
wehavetonotonlysearchforthelistofrandomnessintroducing
systemcallsinthe stracedata,butalsocheckingifthefunction
parameterscontain "/dev/urandom" .Figure2(a)showsasnippet
ofthesampleoutputsfrom straceinourrunningexample.Each
line corresponds to one system call. For example, line 10 showsthat the program from
/usr/bin/python3 is executed with the
scriptmnist_lenet_5.py and the return value is 0. The system
call recorded at line 20 reads from "/dev/urandom" , and system
call (getrandom ) recorded at line 51 is also invoked. Both of the
two system calls introduce software-related randomness.
Step 3 - Diagnosing sources of non-determinism in hardware. In this
step,wecross-checkwiththeNvidiadocumentation[ 15]toseeif
any of the library functions invoked during the training process
triggersthenon-determinismfunctionsattheharwarelayer.Ifsuchfunctionsexist,wecheckifthereisacorrespondingpatchprovided.
If no such patch exists, we will document the unsupported non-
deterministicoperationsandfinishthecurrentprocess.Ifthepatchexists, we will move on to the next phase. Figure 2(b) shows a snip-
pet of the sample outputs of cProfile for our running example.
Thefunctions softmax,weights,bias_add areinvoked3,101,and
2times,respectively.Wefindthat bias_add leveragestheCUDA
implementationof atomicAdd() ,whichiscommonlyusedinma-
trixoperations.Thebehaviorof atomicAdd() isnon-deterministic
because of the order of parallel computations is undetermined,Â‹ÂÂ‡Â›Â•Â–Â‡Â Â…ÂƒÂÂÂ•
É¨É¥Â‡ÂšÂ‡Â…Â˜Â‡Å¿É‘ÅµÂ—Â•Â”ÅµÂ„Â‹ÂÅµÂ’Â›Â–ÂŠÂ‘ÂÉªÉ‘Å™ÆƒÉ‘Â’Â›Â–ÂŠÂ‘ÂÉªÉ‘Å™É‘ÂÂÂ‹Â•Â–ÉÂÂ‡ÂÂ‡Â–ÉÉ¬ÅœÂ’Â›É‘Æ„Å™
É¥ÂšÉ®ÂˆÂˆÂˆÂ†É­Â†É¬É©É¨É¯É¥ÅµÆ‹É©É¬ Â˜ÂƒÂ”Â• Æ‹ÅµÆ€Ê°É¥
ÅœÅœÅœ ÅœÅœÅœ
É©É¥Â‘Â’Â‡ÂÂƒÂ–Å¿É	Å™É‘ÅµÂ†Â‡Â˜ÅµÂ—Â”ÂƒÂ Â†Â‘ÂÉ‘Å™ÉÆ€Ê°É«
ÅœÅœÅœ ÅœÅœÅœ
É¬É¨Â‰Â‡Â–Â”ÂƒÂÂ†Â‘Â Å¿É‘ÉÂšÂˆÂ„ÉÂšÂ…ÉªÉÂšÉ«É«ÉÂšÂ‡É©ÉÂšÉ¥É­ÉÂšÉ­É¬ÉÂš É°É¬ÉÂšÉ®É¥ÉÂšÂ…ÂƒÉÂšÉ«É¯ÉÂšÉ«Â„ÉÂšÂ†ÉªÉÂšÉ­É¬
ÉÂšÉ°Â†ÉÂšÂ…Â„ÉÂšÉ¯ÂˆÉ‘Å™É¨É­Å™É¥Æ€Ê°É¨É­
ÅœÅœÅœ ÅœÅœÅœ
É¨É¥É¥Â‡ÂšÂ‹Â–ÉÂ‰Â”Â‘Â—Â’ Å¿É¥Æ€Ê°Å£
É¨É¥É¨Ê«Ê«Ê«Â‡ÂšÂ‹Â–Â‡Â†Â™Â‹Â–ÂŠÉ¥Ê«Ê«Ê«
ÂÂ…ÂƒÂÂÂ• 	Â‹ÂÂ‡ÂÂƒÂÂ‡Å›ÂÂ‹ÂÂ‡ÂÂ‘ Å¿ÂˆÂ—ÂÂ…Â–Â‹Â‘ÂÆ€
Éª ÂÂÉÂ‘Â’Â•ÅœÂ’Â›Å›É©É¯É®É­Å¿ Â–ÂˆÅœÂÂÅœÂ•Â‘ÂˆÂ–ÂÂƒÂšÆ€
É¨É¥É¨ Â„ÂƒÂ•Â‡ÉÂÂƒÂ›Â‡Â”ÅœÂ’Â›Å›É®É«É©Å¿Â™Â‡Â‹Â‰ÂŠÂ–Â•Æ€
ÅœÅœÅœ ÅœÅœÅœ
É© ÂÂÉÂ‘Â’Â•ÅœÂ’Â›Å›É©É­É©É®Å¿ Â–ÂˆÅœÂÂÅœÂ„Â‹ÂƒÂ•ÉÂƒÂ†Â†Æ€(a). The sample outputs of strace.
(b). The sample outputs of cProfile.
Figure2:Sampleoutputsnippetsfrom straceand cProfile.
whichcausesroundingerrorinfloatingpointcalculation[ 15,56].
The other function calls do not trigger non-deterministic behavior.
3 . 5 Ph a s e4-U p dating
In this phase, we update our mitigation strategies based on the
diagnosisresultsfromthepreviousphase.Thisphasecanbefurther
broken down into two steps:
Step1-Updatingthelistofsystemcallsforrecording. Fortheran-
domness introducing functions, we will add them into the list ofintercepted system calls for our record-and-replay technique, so
thatthereturnvaluesoftherelevantsystemcallscanbesuccessfullyrecorded(describedinthenextphase).Inourrunningexample,we
willaddtheinvocationofreading /dev/urandom andgetrandom
into thelist of interceptedsystem callsto mitigate randomnessin
the software.
Step 2- Applying theright patches for non-deterministiclibrarycalls.Forthenon-deterministicfunctionsrelatedtohardware,wecheckifthereareexistingpatchesthataddresssuchproblemsandintegratethem into the training scripts. In our running example, after check-
ingthedocumentationfromtheNvidiarepository[ 15],wefound
onepatch,which replaces bias_add callswith _patch_bias_add.
We then integrated the patch to the source code of the training
scripts by adding these two lines of code: from tfdeterminism
import patch andpatch(). In this way, during the subsequent
trainingprocessofLeNet-5,thenon-deterministicfunctionswill
be replaced with the deterministic alternatives.
3 . 6 Ph a s e5-R ec o r d-and-Replay
As explained in Section 2, presetting random seeds is not preferred
by practitioners due to various drawbacks. There are libraries (e.g.,
numpy) which support the recording and replaying of random
statesthroughexplicitAPIcalls.However,thismethodisalsoin-
trusiveandwouldincuradditionalcostswedescribedbefore.More
importantly,mainstreamDLframeworkssuchasTensorFlowand
PyTorch do not provide such functionality. Hence, we propose arecord-and-replaytechnique(overviewshowninFigure3)toad-
dress these challenges. This phase has two steps:
2207
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:54:36 UTC from IEEE Xplore.  Restrictions apply. Towards Training Reproducible Deep Learning Models ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
Dynamic library
System libraryTraining processRecording Replaying
Random ProfileTraining process
Dynamic library
System libraryOutputRead
ReturnAPI hook API hook
Figure 3: Our record-and-replay technique.
S t e p1-R e c o r ding.In this step, we record the random values re-
turned by system calls during the training process. We will run
the identical training process as in Phase 1 with the our record-ing technique enabled. We leverage the API hook mechanism tointercept the system calls by pointing the environment variable
LD_PRELOAD to our self-implemented dynamic library. It tells the
dynamic loaders to look up symbols in the dynamic library de-fined in
LD_PRELOAD first. The functions of the dynamic library
will be first loaded into the address space of the process. Our dy-
namiclibraryimplementsalistoffunctionswhichhavethesame
symbols of the randomness introducing system calls in the system
libraries. These self-implemented functions will be loaded first and
invoketheactualrandomnessintroducingsystemcallstogetthe
returned random bytes. The sequences of random bytes emitted
bythesystemcallsarethenrecordedintoanuser-definedobject.
These objects are then serialized and written into files called the
random profile. We replace ğ‘€ğ‘œğ‘‘ğ‘’ğ‘™ğ‘¡ğ‘ğ‘Ÿğ‘”ğ‘’ğ‘¡andğ‘ƒğ‘Ÿğ‘œğ‘ğ‘’ğ‘ ğ‘ ğ‘€ğ‘’ğ‘¡ğ‘Ÿğ‘–ğ‘ğ‘ ğ‘¡ğ‘ğ‘Ÿğ‘”ğ‘’ğ‘¡
with the DL model and the process metrics generated in this step.
In our running example, two types of system calls are inter-
cepted (i.e., getrandom and the read of /dev/urandom ) and the
returnvaluesaresuccessfullyrecorded.Theoutputtedrandompro-
file is stored at a pre-defined path in the local file system called
urandom.conf andgetrandom.conf . For the process-related met-
rics,wecollectthelossvaluesforeachepoch(e.g.,thelossvalueof
thefirstepochis1.062),thetrainingtime(106.9seconds),andthe
number of training epochs (50).
Step2-Replaying. Inthisstep,werepeatthesametrainingprocess
asthepreviousstepwhilereplayingtherandomvaluesstoredinthe
random profile by leveraging the API hook mechanism. As shown
in Figure 3, our dynamic library will search for existing random
profile.Ifsuchrandomprofileexists,therecordedrandombytesare
usedtotoreplacetherandombytesreturnedbythesystemcalls.
We also replace ğ‘€ğ‘œğ‘‘ğ‘’ğ‘™ğ‘Ÿğ‘’ğ‘ğ‘Ÿğ‘œandğ‘ƒğ‘Ÿğ‘œğ‘ğ‘’ğ‘ ğ‘ ğ‘€ğ‘’ğ‘¡ğ‘Ÿğ‘–ğ‘ğ‘ ğ‘Ÿğ‘’ğ‘ğ‘Ÿğ‘œwith the DL
modelandtheprocessmetricsgeneratedinthisstep.Inourrunning
example, we compare the execution logs between our recording
andreplayingstepsandverifythatthesamesetofrandomnumbers
are generated in these two steps.
Once this phase is completed, the two updated DL models are
senttoPhase2forverifyingtheirreproducibilityagain.ThisprocessisrepeateduntiltheDLmodelisshowntobereproducibleorwefindcertainsourcesofnon-determinismthatcurrentlydonothaveexist-ingsolutions.Forexample,thefunction
tf.sparse.sparse_den_m
atmulisnotedin[ 15]thatnosolution hasbeenreleasedyet.The
reasonsfornon-reproducibilityshouldbeincludedinthedocumen-
tations along with released DL models.Table 1: The DL models used in our case study.
Models Datasets # of Labels Setup Task
LeNet-1 [47]
MNIST [9] 10
AllClassificationLeNet-4 [47]
LeNet-5 [47]
ResNet-38 [39]CIFAR-10 [3] 10ResNet-56 [39]
WRN-28-10 [69] CIFAR-100 [3] 100 G1-G10
ModelX Dataset X - G1-G5 Regression
Inourrunningexample,intermsof ğ‘€ğ‘œğ‘‘ğ‘’ğ‘™ ğ‘Ÿğ‘’ğ‘ğ‘Ÿğ‘œandğ‘€ğ‘œğ‘‘ğ‘’ğ‘™ ğ‘¡ğ‘ğ‘Ÿğ‘”ğ‘’ğ‘¡,
ğ¸ğ‘£ğ‘ğ‘™ğ‘¢ğ‘ğ‘¡ğ‘–ğ‘œğ‘›ğ‘€ğ‘’ğ‘¡ğ‘Ÿğ‘–ğ‘ğ‘ ğ‘Ÿğ‘’ğ‘ğ‘Ÿğ‘œandğ¸ğ‘£ğ‘ğ‘™ğ‘¢ğ‘ğ‘¡ğ‘–ğ‘œğ‘›ğ‘€ğ‘’ğ‘¡ğ‘Ÿğ‘–ğ‘ğ‘ ğ‘¡ğ‘ğ‘Ÿğ‘”ğ‘’ğ‘¡are identical
(i.e., overall accuracy, the per-class accuracy, and prediction results
on the testing datasets of two DL models are identical). Except for
thetrainingtime,the ğ‘ƒğ‘Ÿğ‘œğ‘ğ‘’ğ‘ ğ‘ ğ‘€ğ‘’ğ‘¡ğ‘Ÿğ‘–ğ‘ğ‘ ğ‘Ÿğ‘’ğ‘ğ‘Ÿğ‘œandğ‘ƒğ‘Ÿğ‘œğ‘ğ‘’ğ‘ ğ‘ ğ‘€ğ‘’ğ‘¡ğ‘Ÿğ‘–ğ‘ğ‘ ğ‘¡ğ‘ğ‘Ÿğ‘”ğ‘’ğ‘¡
arealsoidentical.Inconclusion,weconsiderthetrainedLeNet-5
models to be reproducible.
4 RESULTS
In this section, we evaluate our approach against open source andcommercial DL models. Section 4.1 describes our case study setup.
Section 4.2 presents the analysis of our evaluation results.
4.1 Case Study Setup
We have selected six commonly studied Computer Vision (CV)
relatedDLmodelssimilartopriorstudies[ 35,38,51,52,56].The
implementations of these models are adapted from a popular open
source repository used by prior studies [35, 38, 49, 52].
Table1showsthedetailsaboutthestudieddatasetsandthemod-
els.ThestudiedmodelsareLeNet-1,LeNet-4,LeNet-5,ResNet-38,
ResNet-56,andWRN-28-10.ThesemodelsmainlyleveragetheCon-
volutionalNeuralNetwork(CNN)astheirneuralnetworkarchitec-
tures. We use popular open source datasets like MNIST, CIFAR-10,
and CIFAR-100. The models and datasets have been widely studied
and evaluated in prior SE research [ 31,38,51,52]. For models in
LeNet family,we train for 50epochs. For models inResNet family
and WRN-28-10, we train for 200 epochs [56].
WealsostudyModelXusedinacommercialsystemfrom Huawei.
ModelX is a LSTM-based DL model used to forecast energy usages.
ModelX is trained with the early-stopping mechanism and theepochs are not deterministic. The training process will automat-ically stop when the loss values have not improved for 5 epochs.
The maximum number of epochs in the training is set to be 50.
ModelX uses proprietary time-series data as their training and test-
ing datasets and is deployed in systems, which are used by tensof millions of customers. Due to the company policy and reviewstandards, we cannot disclose the detail design of the DL model.
The implementation of other open source models is disclosed in
our replication package [21].
Forbothopensourceandcommercialmodels,weperformthe
training processes with different setups. In total, we have 16 differ-
ent setups listed in Table 2:
â€¢First, there are two general groups of setups, CPU-based and
GPU-based,toassesswhetherourapproachcanaddressdifferent
sources of hardware-related non-determinism. For CPU-based
2208
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:54:36 UTC from IEEE Xplore.  Restrictions apply. ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA Chen and Wen, et al.
Table2:Theinformationofalltheexperimentsetups.R&R
represents Record-and-Replay.
ID Hardware Software Seed R & R Patch
C1
CPUTF1.14---
C2 Yes - -
C3 - Yes -
C4
TF2.1---
C5 Yes - -
C6 - Yes -
G1
GPUTF1.14---
G2 Yes - -
G3 - - Yes
G4 Yes - Yes
G5 - Yes Yes
G6
TF2.1---
G7 Yes - -
G8 - - Yes
G9 Yes - Yes
G10 - Yes Yes
experiments(i.e.,C1-C6),weonlytrainthemodelsoftheLeNet
family and ResNet family, as the training for WRN-28-10 and
thecommercialprojecttakesextremelylongtime(longerthan
aweek)andnotpracticaltouseinfield.ForGPU-basedexper-
iments (i.e., G1 - G10), we conduct experiments on training all
theaforementionedmodels.TheCPUusedfortheexperiments
is Intel(R) Xeon(R) Gold 6278C CPU with 16 cores and the GPU
weuseisTesla-P100-16GB.TheGPUrelatedlibrariesare(CUDA
10.0.130 andcuDNN 7.5.1),and (CUDA10.1 and cuDNN 7.6) for
TensorFlow1.14andTensorFlow2.1,respectively.Weusetwo
sets of hardware related libraries due to compatibility issues
mentioned in the official TensorFlow documentation [16].
â€¢Then,within thesame hardwaresetup, wealso conductexperi-
mentsbyvaryingthesoftwareversions.Foropensourcemodels,
we use both TensorFlow 1.14 and TensorFlow 2.1. We choose
to carry out our experiments on these two TensorFlow versions
as major changes [ 8] have been made from TensorFlow 1.X to
TensorFlow2.Xandtherearestillmanymodelswhichuseeither
orbothversions.Hence,wewanttoverifyifourapproachcan
workwithbothversions.ForModelX,weonlyuseTensorFlow
1.14 as it currently only supports the TensorFlow 1.X APIs.
â€¢For CPU-based experiments in a particular software version
(e.g., TensorFlow 1.14), we have three setups: C1 is to run the
training process without setting seeds and without enabling the
record-and-replaytechnique.C2istorunthetrainingwithseeds,
whereasC3istorunthetrainingwithrecord-andreplayenabled.
ForGPU-basedexperimentsinaparticularsoftwareversion(e.g.,
TensorFlow 1.14), we have five setups: G1 and G2 are similar to
C1andC2.G3istoruntheexperimentswithpatchingonlyto
evaluate the variance related to software randomness. G4 and
G5 are both running with patches, but configured with either
setting seeds or enabling record-and-replay, respectively.
Foreachsetup,weruntheexperiments16timessimilartoaprior
study[56].Thetrainingdatasetissplitintobatchesandfedintothetrainingprocess;thevalidationdatasetisusedforevaluatingthe
lossesduringtraining;andthetestingdatasetisusedforevaluatingthefinalmodels.Inotherwords,thetrainingandvalidationdataset
are known to the trained DL models, while the testing data is
completelynewtothemodeltomimictherealisticfieldassessment.
We further divide the 16 runs of DL experiments into 8 pairs,
eachofwhichconsistsoftworuns.Wethencomparetheevaluation
metrics from each pair of runs to verify reproducibility. For the
setupswithrandomseedsconfigured,wechoose8mostcommonlyusedrandomseedsforeachpair(e.g.,0and42)[
10].Wecollectthe
process and evaluation metrics as described in Section 3.3.
In addition, for each experiment, we also collect the running
timeforeachoftheaboveexperimenttoassesstheruntimeover-
head incurred by our approach. We only focus on the experiments
conductedonGPU,asGPU-basedexperimentsareexecutedona
physical machine. CPU-based experiments are conducted on a vir-
tual machine in the cloud environment, which can introduce large
variances caused by the underlying cloud platform [ 62]. For exam-
ple,comparingthetimeofG1andG3couldrevealtheperformance
impact on enabling deterministic patch for GPU. Comparing the
timeofG3andG5couldrevealtheoverheadintroducedthrough
record-and-replay technique. To statistically compare the time dif-
ferences,weperformthenon-parametricWilcoxonrank-sumtest
(WSR).Toassessthemagnitudeofthetimedifferencesamongdiffer-
ent setups, we also calculate the effect size using Cliffâ€™s Delta [ 59].
Finally, as our approach also stores additional data (e.g., the
recorded random profile during the store-and-replay phase), we
evaluatethestorageoverheadbroughtbyourapproachbycompar-
ing the size of DL models with the size of random profiles.
4.2 Evaluation Analysis and Results
Hereweevaluateifthestudiedmodelsarereproducibleafterap-
plyingourapproach.Thenwestudythetimeandstorageoverhead
associated with our approach.Reproducibility by applying our approach
. The results show
that, the six open source models can be successfully reproduced by
applyingourapproachwithdefaultsettings.Inotherwords,allthe
predictions are consistent between the target model and the repro-
ducedmodel.Thedefaultrecord-and-replaytechniqueintercepts
two types of randomness introducing system calls (i.e., the read of
/dev/urandom andgetrandom ). The default patch is the version
0.3.0 of tensorflow-determinism released in PyPI for TensorFlow
1.14.For TensorFlow2.1, weneed tosetthe environmentvariable
TF_CUDNN_DETERMINISTIC to "true". The results demonstrate the
effectiveness of our approach on training reproducible DL models.
Unfortunately,ModelXundersuchdefaultsetupcannotberepro-
duced. While applying our approach, during the profiling and diag-nosingphase,wefoundonelibraryfunction(
unsorted_segment_s
um)invokedfromModelX,whichcannotbemitigatedbythedefault
patch.Wecarefullyexaminedthesolutionsdescribedin[ 15]and
discovered an experimental patch that could resolve this issue. We
appliedtheexperimentalpatchalongwiththerecord-and-replay
techniqueandareabletoachievereproducibilityforModelX,i.e.,
all the predictions are consistent.Overhead
. We evaluate the overall time overhead incurred by our
approachbycomparingtrainingtimebetweenthesetupwithout
2209
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:54:36 UTC from IEEE Xplore.  Restrictions apply. Towards Training Reproducible Deep Learning Models ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
seed,record-and-replay,andpatchagainstthesetupwithrecord-
and-replayandpatch(a.k.a.,ourapproach).Weonlycomparethe
training time among open source models, as ModelX adopts the
early-stopping mechanism as described above (Section 4.1). Asshown in Table 3, training takes longer when applying our ap-proach than the setups without. This is mainly because patchedfunctions adopt deterministic operations, which do not leverage
operations(e.g., atomicAdd )thatsupportparallelcomputation.The
time overhead ranges from 24% to 114% in our experiments. Al-though our approach makes training on GPU slower, compared
with training on CPUs, training on GPU with our approach is still
muchfaster(e.g.,trainingWRN-28-10onCPUtakesmorethan7
days). We further evaluate the time overhead brought by patching
and record-and-replay alone. We compare the setup with patch-
ing enabled against the setups without it(e.g., G1 vs. G3). We also
compare the setupwith record-and-replay, patching enabledwith
the setup with patching only (e.g., G3 vs. G5). The results show
that the record-and-replay technique does not introduce statistical
significantoverhead( ğ‘âˆ’ğ‘£ğ‘ğ‘™ğ‘¢ğ‘’ >0.05).Inotherwords,patchingis
the main reason that our approach introduces the time overhead.
Table3:Comparingthetimeandstorageoverhead.Time(O)
represents the average training time (in hours) for originalsetup, and Time(R) represents the average training time (inhours)forthesetupusingourapproach(Time(R)).Thetimeis italicized if p-value is <0.001 and the effect size is large
with (*). RP represents for Random Profile.
Model Time(O)/Time(R) Model Size RP Size
LeNet-1 0.017/0.023 (*) 35 KB 13 KB
LeNet-4 0.019/0.027 (*) 224 KB 13 KB
LeNet-5 0.021/0.028 (*) 267 KB 13 KB
ResNet-38 1.243/1.561 (*) 4.8 MB 13 KB
ResNet-56 1.752/2.179 (*) 7.6 MB 13 KB
WRN-28-10 7.08/14.979 (*) 279 MB 13 KB
ModelX - 675 KB 38 KB
Table3alsoshowstheaveragesizeoftrainedDLmodelsandthe
randomprofiles.Theabsolutestoragesizesoftherandomprofile
are very small, ranging between 13 KB to 38 KB depending on the
DLmodels.Comparedtothesizeofthemodel,thebiggestmodel
isWRN-28-10(279MB).Therandomprofileisonly0.005%ofthe
model in terms of the size. When the model is less complex (e.g.,
LeNet-1), the additional cost becomes more prominent. In LeNet-1,
therandomprofileincurs37%additionalstorage.However,thetotal
storage size when combining the model and the random profile for
LetNet-1 is less than 50 KB, which is acceptable under most of the
use cases.
 
Summary: Case study results show that our approach can
successfullyreproduceallthestudiedDLmodels.Patching(i.e.,
replacenon-deterministicoperationsfromhardwarewithde-
terministicones)incurslargetimeoverheadasthetrade-offfor
ensuringdeterministicbehavior.Therecord-and-replaytech-
nique does not incur additional time overhead in the training
process with very small additional storage sizes.5 DISCUSSIONS
In this section, we conduct the variance analysis and discuss the
lessons learnt when applying our approach.
5.1 Variance Analysis
Tomeasurethevariancesintroducedbydifferentsourcesofnon-
determinism,wecomparetheevaluationmetricsamongdifferent
setups.Suchanalysis demonstratesthevariancesbetweenourap-
proachwiththestate-of-the-arttechniquestowardsreproducingDLmodels.Forexample,variancescausedbysoftwareareanalyzedby
comparing the evaluation metrics between each pair in G3, where
patchingisenabledtoeliminatehardwarenon-determinism(i.e.,
theapproachproposedby[ 15]).Tomeasurethevariancescaused
by hardware, we compare the evaluation metrics between each
pairinG2orG7,wheretherandomseedsarepresettoeliminate
softwarerandomness(i.e.,theapproachproposedby[ 56]).Inad-
dition to measuring the software variance and hardware variance,
whichresultfromapplyingtwostate-of-the-arttechniques,wealso
showthevariancesincurredfromtheoriginalsetupwithnopresetseed,record-and-replaynotenabled,andpatchingnotenabled.Theresultsofourapproach,whichincurszerovariances,arealsolisted
in the table.
ThedetailedresultsareshowninTable4.Weonlyincludethere-
sultsforthesixopensourceprojectsduetoconfidentialityreasons.
Three evaluation metrics are used: overall accuracy, per-class accu-
racy, and the consistency of predictions. For each type of metric,
we calculate the maximum differences and the standard deviations
of the differences.
For example, for ResNet-38, the largest variance of overall ac-
curacyintheoriginalsetup is2.0%,whilethelargestvariancesin-
troduced by software randomness and hardware non-determinism
are 1.4% and 1.2%, respectively. For per-class accuracy, the largest
varianceintheoriginalsetupis10.1%,whilethelargestvariancesin-
troduced by software randomness and hardware non-determinism
are 6.8% and 4.9%. For predictions, the largest number of incon-sistent predictions in the original setup is 219, while the largest
numberofinconsistentpredictionscausedbysoftwarerandomness
and hardware non-determinism are 216 and 209, respectively.
In summary, the variances caused by software are generally
larger than those caused by hardware, yet the variances caused
byhardwarearenotnegligibleandneedtobecontrolledinorder
totrainreproducibleDLmodels.Theresultsdemonstratetheim-
portanceandeffectivenessofapplyingourapproachfortraining
reproducible DL models, as our approach is the only one that does
not introduce any variances.
5.2 Generalizability in other DL frameworks
Other than the DL framework studied in Section 4.2, we have also
applied our approach on another popular DL framework, PyTorch.
Experiment results show that for common models such as LeNet-5
andResNet-56withPyTorchversion1.7,ourapproachcanworkout
of the box. In the future, we also plan to experiment our approach
onmoreDLframeworksandmoreDLmodelsacrossdifferenttasks.
2210
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:54:36 UTC from IEEE Xplore.  Restrictions apply. ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA Chen and Wen, et al.
Table4:Comparingvariancesbetweenourapproachandthestate-of-the-arttechniques.Softwarevariancereferstothetech-
nique for only controlling hardware non-determinism [15]. Hardware variance refers to the technique for only controlling
software randomness [56]. Original variance refers to the variance caused by the original setup.
Our Variance Software Variance Hardware Variance Original VarianceDiff SDev Diff SDev Diff SDev Diff SDev
Overall acc.LeNet1 0 0 0.8% 0.2% 0 0 1.7% 0.3%
LeNet4 0 0 0.7% 0.1% 0 0 0.8% 0.1%
LeNet5 0 0 0.5% 0.1% 0 0 0.5% 0.1%
ResNet38 0 0 1.4% 0.3% 1.2% 0.3% 2.0% 0.4%
ResNet56 0 0 1.2% 0.3% 0.8% 0.2% 1.7% 0.3%
WRN-28-10 0 0 1.4% 0.4% 1.7% 0.5% 2.4% 0.5%
Per-class acc.LeNet1 0 0 3.7% 0.8% 0 0 4.8% 1.2%
LeNet4 0 0 1.7% 0.3% 0 0 3.0% 0.6%
LeNet5 0 0 2.3% 0.4% 0 0 2.5% 0.5%
ResNet38 0 0 6.8% 1.2% 4.9% 0.9% 10.1% 1.9%
ResNet56 0 0 6.8% 1.1% 5.3% 0.8% 10.5% 1.9%
WRN-28-10 0 0 35.0% 5.0% 25.0% 3.0% 40.9% 7.8%
PredictionsLeNet1 0 0 48 14.1 0 0 50 17.04
LeNet4 0 0 31 3.8 0 0 29 3.8
LeNet5 0 0 28 3.5 0 0 26 3.5
ResNet38 0 0 216 10.1 209 11.3 219 10.7
ResNet56 0 0 198 8.6 188 8.8 198 8.0
WRN-28-10 0 0 485 18.0 453 12.3 542 18.7
5.3 Documentations on DL Models
Mitchell et al. [ 53] proposed Model Cards to document ML models.
A typical model card includes nine sections (e.g., Model Details
andIntendedUse),eachofwhichcontainsalistofrelevantinfor-
mation. For example, in the Model Details section, it suggests that
the â€œInformation about training algorithms, parameters, fairnessconstraints or other applied approaches, and featuresâ€ should beaccompanied with released models. Such a practice would help
other researchersor practitionersto evaluate ifthe models canbe
reproduced. However, the current practice would still miss certain
details. We share our experience below to demonstrate this point.
TensorFlowandKerasaretwoofthemostwidelyusedDLframe-
works. Keras is a set of high level APIs designed for simplicity and
usability for both software engineers and DL researchers, while
TensorFlowoffersmorelowleveloperationsandismoreflexibleto
design and implement complex network structures. There are two
waysofusingKerasandTensorFlowinDLtraining.Thefirstwayis
to import Keras and TensorFlow separately by first calling import
kerasand then verify if the backend of Keras is TensorFlow. If
yes, TensorFlow can be imported by import tensorflow . This
way is referred to as Keras_first. The second way is to directly use
the Keras API within TensorFlow by first importing TensorFlow.
Then we use another import statement from tensorflow import
keras. This way is referred to as TF_first. We conduct experiments
to evaluate if the two different usage of APIs have an impact on
trainingreproducibleDLmodels.Asaresult,thefollowingfindings
are presented:
â€¢When training on CPUs, using Keras_first will lead to unre-
producible results even after mitigating all the sources of non-
determinism.ThisissuecanbereproducedbyusingvariousKerasversionfrom2.2.2to2.2.5.Onthecontrary,using TF_firstwith
thesamesettingwillyieldreproducibleresults.Thisissuedoes
not exist in training on GPUs.
â€¢WhiletrainingwithKerasversion2.3.0andabove,weareable
toreproducetheresultsbothfor Keras_first andTF_firstusing
our approach. Ho wever, the DL models trained using Keras_first
andTF_firstare not consistent with each other.
Bothfindingshavebeensubmittedasissuereportstotheofficial
Kerasdevelopmentteamwhosuggestedustousenewerversions
of Keras instead [ 7,20]. The findings highlight that not only the
versions of dependencies, but also how the dependent software
packages are used can impact the reproducibility of DL models.
Unfortunately, existing DL model documentation frameworks like
Model cards [53] do not specify how the software dependencies
should be described. Hence, we suggest ML practitioners look into
theapproachadoptedfortraditionalsoftwareprojectslikesoftware
bills of materials (SBOM) [ 68] for rigorously specifying software
dependencies.
6 GUIDELINE
Inthissection,weproposeaguidelineforresearchersandpracti-
tioners who are interested in constructing reproducible DL models.
Our guideline consists of five steps:
(1)Use documentation frameworks such as Model Cards to docu-
ment the details such as model training. Consider leveraging
SBOM to document software dependencies. Ensure the docu-
mentation co-evolves with the model development process.
(2)Use asset management tools such as DVC [ 12] and MLflow [ 1]
tomanagetheexperimentalassetsusedduringtrainingprocess.
To mitigate the risks of introducing non-determinism from
2211
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:54:36 UTC from IEEE Xplore.  Restrictions apply. Towards Training Reproducible Deep Learning Models ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
assets, we suggest using virtualization techniques to provide a
complete runtime environment.
(3)Useanddocumenttheappropriateevaluationcriteriadepend-
ingonthedomainoftheDLmodels.Someofthesemetrics(e.g.,
evaluation metrics ) may be domain specific, whereas other
metrics (e.g., process metrics) are general.
(4)Randomnessinthesoftwareandnon-determinismfromhard-
warearetwoofthemainchallengespreventingthereproducibil-
ity of DL models. Use record-and-replay technique to mitigate
sources of randomness in the software when presetting seed is
not preferred. Use patching to mitigate the non-determinism
from hardware if the overhead is acceptable.
(5)If DL models are still not reproducible by applying our ap-
proach, double check if the list of system calls which introduce
randomness changes or if the deterministic operations are not
currentlysupportedbythehardwarelibraries.Documentthe
unsupported non-deterministic operations and search for alter-
native operations on the same operation.
7 THREATS TO VALIDITY
ExternalValidity. Currently,wefocusonDLtrainingusingPython
along with TensorFlow and Keras framework under Linux. We are
currently working on extending our approach to support DL mod-
els developed in other DL frameworks and additional operating
systems. In addition,we have applied our approaches ontwo pop-
ular domains of DL: classification and regression tasks. We plan
to investigate other tasks such as Natural Language Processingand Reinforcement Learning. GPUs and CPUs are common and
widely adopted hardware for DL training. Hence, in this paper, we
choose to focus on evaluating the DL training on GPUs and CPUs.
However, DL training on other hardware such as TPU and edge
devices also might encounter reproducibility issues. We believe theideaofourapproachcanbeappliedinthesecontextsaswell.Future
work is welcomed to extend our approach to different platforms.
Internal Validity. When measuring the variances incurred by
different sources of non-determinism, we control the other con-founding factors to ensure internal validity. For example, when
measuring the overall accuracy variance caused by randomness in
software,weonlycomparetherunswithpatchingenabledandwith
the same dependencies. In addition, in our evaluation, we repeatthe model training process for at least 16 times for each setup to
observe the impact of different non-deterministic factors.ConstructValidity.
The implementation code for the DL models
used in our case studies has been careful reviewed by previousresearchers [
35,38,52,56]. Our record-and-replay technique for
controlling the software factors work when low level random func-
tions are dynamically linked and invoked.
8 CONCLUSIONS
Reproducibility is a rising concern in AI, especially in DL. Priorpracticesandresearchmainlyfocusonmitigatingthesourcesofnon-determinism separately without a systematic approach andthorough evaluation. In this paper, we propose a systematic ap-
proachtoreproducingDLmodelsthroughcontrollingthesoftware
andhardwarenon-determinism.Casestudiesonsixopensource
and one commercial DL models show that all the models can besuccessfullyreproducedbyleveragingourapproach.Inaddition,
we present a guideline for training reproducible DL models and
describe some of the lessons learned based on our experience of
applyingourapproachinpractice.Last,weprovideareplication
package [21] to facilitate reproducibility of our study.
REFERENCES
[1]2021(accessedAugust,2021). Anopensourceplatformforthemachinelearning
lifecycle. https://mlflow.org/
[2]2021 (accessed August, 2021). Assessment List for Trustworthy Artificial Intelli-
gence(ALTAI)forself-assessment. https://digital-strategy.ec.europa.eu/en/library/
assessment-list-trustworthy-artificial-intelligence-altai-self-assessment
[3]2021 (accessed August, 2021). The CIFAR-10 and CIFAR-100 datasets. https:
//www.cs.toronto.edu/~kriz/cifar.html
[4]2021 (accessed August, 2021). CUDA Toolkit. https://developer.nvidia.com/cuda-
toolkit
[5]2021 (accessed August, 2021). Determined AI Reproducibility. https://docs.
determined.ai/latest/topic-guides/training/reproducibility.html
[6]2021 (accessed August, 2021). Determinism in Deep Learning (S9911).
https://developer.download.nvidia.com/video/gputechconf/gtc/2019/
presentation/s9911-determinism-in-deep-learning.pdf
[7]2021 (accessed August, 2021). Inconsistent results when using two styles of import
statements - Issue 14672. https://github.com/keras-team/keras/issues/14672
[8]2021 (accessed August, 2021). Migrate your TensorFlow 1 code to TensorFlow 2.
https://www.tensorflow.org/guide/migrate
[9]2021 (accessed August, 2021). The Mnist Database of handwritten digits. http:
//yann.lecun.com/exdb/mnist/
[10]2021 (accessed August, 2021). Most common random seeds. https://www.kaggle.
com/residentmario/kernel16e284dcb7
[11]2021(accessedAugust,2021). NotesfromtheAIFrontierInsightsfromHundredsof
Use Cases. https://www.mckinsey.com/featured-insights/artificial-intelligence/
notes-from-the-ai-frontier-applications-and-value-of-deep-learning
[12]2021 (accessed August, 2021). Open-source Version Control System for Machine
Learning Projects. https://dvc.org/
[13]2021(accessedAugust,2021). ProposalforaREGULATIONOFTHEEUROPEAN
PARLIAMENTANDOFTHECOUNCILLAYINGDOWNHARMONISEDRULESON
ARTIFICIALINTELLIGENCE(ARTIFICIALINTELLIGENCEACT)ANDAMENDING
CERTAIN UNION LEGISLATIVE ACTS. https://eur-lex.europa.eu/legal-content/
EN/TXT/?uri=CELEX%3A52021PC0206
[14]2021 (accessed August, 2021). Reproducibility in Pytorch. https://pytorch.org/
docs/stable/notes/randomness.html
[15]2021 (accessed August, 2021). Tensorflow Determinism. https://github.com/
NVIDIA/framework-determinism
[16]2021 (accessed August, 2021). TensorFlow GPU Support. https://www.tensorflow.
org/install/source#gpu
[17]2021 (accessed August, 2021). Tensorflow RFC for determinism. https://github.
com/tensorflow/community/blob/master/rfcs/20210119-determinism.md
[18]2021 (accessed August, 2021). Testing for Deploying Machine Learning
Models. https://developers.google.com/machine-learning/testing-debugging/
pipeline/deploying
[19]2021 (accessed August, 2021). The Machine Learning Reproducibility Checklist.
https://www.cs.mcgill.ca/~jpineau/ReproducibilityChecklist.pdf
[20]2021(accessedAugust,2021). Unreproducibleresultswhendirectlyimportkerasin
CPUenvironment-Issue14671. https://github.com/keras-team/keras/issues/14671
[21]2022(accessedFeb,2022). Thereplicationpackage. https://github.com/nemo9cby/
ICSE2022Rep
[22]MartÃ­nAbadi,PaulBarham,JianminChen,ZhifengChen,AndyDavis,Jeffrey
Dean,MatthieuDevin,SanjayGhemawat,GeoffreyIrving,MichaelIsard,Manju-nathKudlur,JoshLevenberg,RajatMonga,SherryMoore,DerekGordonMurray,
Benoit Steiner, Paul A. Tucker, Vijay Vasudevan, Pete Warden, Martin Wicke,
Yuan Yu, and Xiaoqiang Zheng. 2016. TensorFlow: A System for Large-Scale
Machine Learning. In 12th USENIX Symposium on Operating Systems Design and
Implementation, OSDI 2016, Savannah, GA, USA, November 2-4, 2016. USENIX
Association, 265â€“283.
[23]Saleema Amershi, Andrew Begel, Christian Bird, Robert DeLine, Harald C. Gall,
EceKamar,NachiappanNagappan,BesmiraNushi,andThomasZimmermann.
2019. Software engineering for machine learning: a case study. In Proceedings of
the41stInternationalConferenceonSoftwareEngineering:SoftwareEngineeringin
Practice,ICSE(SEIP)2019,Montreal,QC,Canada,May25-31,2019 ,HelenSharp
and Mike Whalen (Eds.). IEEE / ACM, 291â€“300.
[24]Dario Amodei, Sundaram Ananthanarayanan, Rishita Anubhai, Jingliang Bai,
Eric Battenberg, Carl Case, Jared Casper, Bryan Catanzaro, Jingdong Chen, MikeChrzanowski, Adam Coates, Greg Diamos, Erich Elsen, Jesse H. Engel, Linxi Fan,ChristopherFougner,AwniY.Hannun,BillyJun,TonyHan,PatrickLeGresley,Xi-angang Li, Libby Lin, Sharan Narang, Andrew Y. Ng, Sherjil Ozair, Ryan Prenger,
2212
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:54:36 UTC from IEEE Xplore.  Restrictions apply. ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA Chen and Wen, et al.
ShengQian,JonathanRaiman, SanjeevSatheesh,DavidSeetapun,ShubhoSen-
gupta, Chong Wang, Yi Wang, Zhiqian Wang, Bo Xiao, Yan Xie, Dani Yogatama,
Jun Zhan, and Zhenyao Zhu. 2016. Deep S peech2: End-to-End Speech Recogni-
tion in English and Mandarin. In Proceedings of the 33nd International Conference
onMachineLearning,ICML2016,NewYorkCity,NY,USA,June19-24,2016 (JMLR
Workshop and Conference Proceedings).
[25]Amine Barrak, Ellis E. Eghan, and Bram Adams. 2021. On the Co-evolution
of ML Pipelines and Source Code - Empirical Study of DVC Projects. In 28th
IEEE International Conference on Software Analysis, Evolution and Reengineering,
SANER 2021, Honolulu, HI, USA, March 9-12, 2021. IEEE, 422â€“433.
[26]Miles Brundage, Shahar Avin, Jasmine Wang, Haydn Belfield, Gretchen Krueger,
Gillian K. Hadfield, Heidy Khlaaf, Jingying Yang, Helen Toner, Ruth Fong, Tegan
Maharaj,PangWeiKoh,SaraHooker,JadeLeung,AndrewTrask,EmmaBluemke,
Jonathan Lebensbold, Cullen Oâ€™Keefe, Mark Koren, Theo Ryffel, J. B. Rubinovitz,
Tamay Besiroglu, Federica Carugati, Jack Clark, Peter Eckersley, Sarah de Haas,
Maritza Johnson, Ben Laurie, Alex Ingerman, Igor Krawczuk, Amanda Askell,
Rosario Cammarota, Andrew Lohn, David Krueger, Charlotte Stix, Peter Hender-
son, Logan Graham, Carina Prunkl, Bianca Martin, Elizabeth Seger, Noa Zilber-
man, SeÃ¡n Ã“ hÃ‰igeartaigh, Frens Kroeger, Girish Sastry, Rebecca Kagan, Adrian
Weller, Brian Tse, Elizabeth Barnes, Allan Dafoe, Paul Scharre, Ariel Herbert-
Voss, Martijn Rasser, Shagun Sodhani, Carrick Flynn, Thomas Krendl Gilbert,
Lisa Dyer, Saif Khan, Yoshua Bengio, and Markus Anderljung. 2020. Toward
Trustworthy AI Development: Mechanisms for Supporting Verifiable Claims.
CoRRabs/2004.07213(2020). arXiv:2004.07213 https://arxiv.org/abs/2004.07213
[27]Sharan Chetlur, Cliff Woolley, Philippe Vandermersch, Jonathan Cohen, John
Tran, Bryan Catanzaro, and Evan Shelhamer. 2014. cuDNN: Efficient Primitives
for Deep Learning. CoRRabs/1410.0759 (2014). arXiv:1410.0759 http://arxiv.org/
abs/1410.0759
[28]CÃ©dricColas,OlivierSigaud,andPierre-YvesOudeyer.2018. HowManyRandom
Seeds? Statistical Power Analysis in Deep Reinforcement Learning Experiments.
CoRRabs/1806.08295(2018). arXiv:1806.08295 http://arxiv.org/abs/1806.08295
[29]AndreEsteva,AlexandreRobicquet,BharathRamsundar,VolodymyrKuleshov,
Mark DePristo, Katherine Chou, Claire Cui, Greg Corrado, Sebastian Thrun, and
Jeff Dean. 2019. A guide to deep learning in healthcare. Nature medicine 25, 1
(2019), 24â€“29.
[30]TimnitGebru,JamieMorgenstern,BrianaVecchione,JenniferWortmanVaughan,
Hanna M. Wallach, Hal DaumÃ© III, and Kate Crawford. 2018. Datasheets forDatasets. CoRRabs/1803.09010 (2018). arXiv:1803.09010 http://arxiv.org/abs/
1803.09010
[31]Simos Gerasimou, Hasan Ferit Eniser, Alper Sen, and Alper Cakan. 2020.
Importance-drivendeeplearningsystemtesting.In ICSEâ€™20:42ndInternational
Conference on Software Engineering, Seoul, South Korea, 27 June - 19 July, 2020,
Gregg Rothermel and Doo-Hwan Bae (Eds.). ACM, 702â€“713.
[32]Sindhu Ghanta, Lior Khermosh, Sriram Subramanian, Vinay Sridhar, Swami-
nathanSundararaman,DulcardoArteaga,QianmeiLuo,DrewRoselli,Dhanan-
joyDas,andNishaTalagala.2018. Asystemsperspectivetoreproducibilityin
production machine learning domain. (2018).
[33]David Goldberg. 1991. What Every Computer Scientist Should Know About
Floating-Point Arithmetic. ACM Comput. Surv. 23, 1 (1991), 5â€“48.
[34]Sorin Mihai Grigorescu, Bogdan Trasnea, Tiberiu T. Cocias, and Gigel Macesanu.
2020. A survey of deep learning techniques for autonomous driving. J. Field
Robotics37, 3 (2020), 362â€“386.
[35]Jiazhen Gu, Huanlin Xu, Haochuan Lu, Yangfan Zhou, and Xin Wang. 2021.
DetectingDeepNeuralNetworkDefectswithDataFlowAnalysis.In 51stAnnual
IEEE/IFIPInternationalConferenceonDependableSystemsandNetworksWorkshops,
DSN Workshops 2021, Taipei, Taiwan, June 21-24, 2021.
[36]Xiaodong Gu, Hongyu Zhang, and Sunghun Kim. 2018. Deep code search. In
Proceedingsofthe40thInternationalConferenceonSoftwareEngineering,ICSE2018,
Gothenburg,Sweden,May27-June03,2018,MichelChaudron,IvicaCrnkovic,
MarshaChechik,andMarkHarman(Eds.).ACM,933â€“944. https://doi.org/10.
1145/3180155.3180167
[37]OddErikGundersenandSigbjÃ¸rnKjensmo.[n.d.]. StateoftheArt:Reproducibil-
ity in Artificial Intelligence. In Proceedings of the Thirty-Second AAAI Conference
on Artificial Intelligence, (AAAI-18).
[38]QianyuGuo,SenChen,XiaofeiXie,LeiMa,QiangHu,HongtaoLiu,YangLiu,
JianjunZhao,andXiaohongLi.2019. AnEmpiricalStudyTowardsCharacterizing
Deep Learning Development and Deployment Across Different Frameworks and
Platforms. In 34th IEEE/ACM International Conference on Automated Software
Engineering, ASE 2019, San Diego, CA, USA, November 11-15, 2019.
[39]Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep Residual
Learning for Image Recognition. In 2016 IEEE Conference on Computer Vision
andPatternRecognition,CVPR2016,LasVegas,NV,USA,June27-30,2016.IEEE
Computer Society, 770â€“778.
[40]PeterHenderson,RiashatIslam,PhilipBachman,JoellePineau,DoinaPrecup,and
David Meger. 2018. Deep Reinforcement Learning That Matters. In Proceedings
of the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18). AAAI
Press.[41]Matthew Hutson. 2018. Artificial intelligence faces reproducibility crisis. Science
(New York, N.Y.) 359 (02 2018), 725â€“726. https://doi.org/10.1126/science.359.6377.
725
[42]Frank Hutter, Lars Kotthoff, and Joaquin Vanschoren (Eds.). 2019. Automated
MachineLearning-Methods,Systems,Challenges. Springer. https://doi.org/10.
1007/978-3-030-05318-5
[43]SamuelIdowu,DanielStrÃ¼ber,andThorstenBerger.2021. AssetManagement
inMachineLearning:ASurvey.In 43rdIEEE/ACMInternationalConferenceon
Software Engineering: Software Engineering in Practice, ICSE (SEIP) 2021, Madrid,Spain, May 25-28, 2021. IEEE, 51â€“60.
[44]
Richard Isdahl and Odd Erik Gundersen. 2019. Out-of-the-box reproducibility: A
surveyofmachinelearningplatforms.In 201915thinternationalconferenceon
eScience (eScience). IEEE, 86â€“95.
[45]HadiJooybar,WilsonW.L.Fung,MikeOâ€™Connor,JosephDevietti,andTorM.
Aamodt.2013.GPUDet:adeterministicGPUarchitecture.In ArchitecturalSupport
forProgrammingLanguagesandOperatingSystems,ASPLOS2013,Houston,TX,
USA, March 16-20, 2013. ACM, 1â€“12. https://doi.org/10.1145/2451116.2451118
[46]Yann LeCun, Yoshua Bengio, and Geoffrey E. Hinton. 2015. Deep learning. Nat.
(2015).
[47]Y. Lecun, L. Bottou, Y. Bengio, and P. Haffner. 1998. Gradient-based learning
applied to document recognition. Proc. IEEE (1998).
[48]Brian Lee, Andrew Jackson, Tom Madams, Seth Troisi, and Derek Jones. 2019.Minigo: A Case Study in Reproducing Reinforcement Learning Research. In
ReproducibilityinMachineLearning,ICLR2019Workshop,NewOrleans,Louisiana,
United States, May 6, 2019. OpenReview.net.
[49]Wei Li. 2017. cifar-10-cnn: Play deep learning with CIFAR datasets. https:
//github.com/BIGBALLON/cifar-10-cnn.
[50]Chao Liu, Cuiyun Gao, Xin Xia, David Lo, John C. Grundy, and Xiaohu Yang.2020. On the Replicability and Reproducibility of Deep Learning in SoftwareEngineering. CoRRabs/2006.14244(2020). arXiv:2006.14244 https://arxiv.org/
abs/2006.14244
[51]LeiMa,FelixJuefei-Xu,FuyuanZhang,JiyuanSun,MinhuiXue,BoLi,ChunyangChen,TingSu,LiLi,YangLiu,JianjunZhao,andYadongWang.2018. DeepGauge:
multi-granularity testing criteria for deep learning systems. In Proceedings of
the 33rd ACM/IEEE International Conference on Automated Software Engineering,
ASE 2018, Montpellier, France, September 3-7, 2018, Marianne Huchard, Christian
KÃ¤stner, and Gordon Fraser (Eds.). ACM, 120â€“131.
[52]ShiqingMa,YingqiLiu,Wen-ChuanLee,XiangyuZhang,andAnanthGrama.
2018. MODE: automatedneuralnetworkmodeldebuggingvia statedifferential
analysis and input selection. In Proceedings of the 2018 ACM Joint Meeting on
European Software Engineering Conference and Symposium on the Foundations of
Software Engineering, ESEC/SIGSOFT FSE 2018, Lake Buena Vista, FL, USA, No-
vember04-09,2018,GaryT.Leavens,AlessandroGarcia,andCorinaS.Pasareanu
(Eds.).
[53]MargaretMitchell,SimoneWu,AndrewZaldivar,ParkerBarnes,LucyVasserman,
Ben Hutchinson, Elena Spitzer, Inioluwa Deborah Raji, and Timnit Gebru. 2019.
ModelCardsforModelReporting.In ProceedingsoftheConferenceonFairness,
Accountability, and Transparency, FAT* 2019, Atlanta, GA, USA, January 29-31,
2019, danah boyd and Jamie H. Morgenstern (Eds.). ACM, 220â€“229.
[54]DavidLorgeParnas.2017. Therealrisksofartificialintelligence. Commun.ACM
60, 10 (2017), 27â€“31. https://doi.org/10.1145/3132724
[55]AdamPaszke,SamGross,FranciscoMassa,AdamLerer,JamesBradbury,GregoryChanan,TrevorKilleen,ZemingLin,NataliaGimelshein,LucaAntiga,AlbanDes-
maison, Andreas KÃ¶pf,Edward Yang, Zachary DeVito, MartinRaison, Alykhan
Tejani,SasankChilamkurthy,BenoitSteiner,LuFang,JunjieBai,andSoumith
Chintala. 2019. PyTorch: An Imperative Style, High-Performance Deep Learning
Library. In Advances in Neural Information Processing Systems 32: Annual Con-
ferenceonNeuralInformationProcessingSystems2019,NeurIPS2019,December
8-14,2019,Vancouver,BC,Canada,HannaM.Wallach,HugoLarochelle,AlinaBeygelzimer, Florence dâ€™AlchÃ©-Buc, Emily B. Fox, and Roman Garnett (Eds.).
8024â€“8035.
[56]Hung Viet Pham, Shangshu Qian, Jiannan Wang, Thibaud Lutellier, JonathanRosenthal, Lin Tan, Yaoliang Yu, and Nachiappan Nagappan. [n.d.]. Problemsand Opportunities in Training Deep Learning Software Systems: An Analysis
ofVariance.In 35thIEEE/ACMInternationalConferenceonAutomatedSoftware
Engineering, ASE 2020, Melbourne, Australia, September 21-25, 2020.
[57]JoellePineau,PhilippeVincent-Lamarre,KoustuvSinha,VincentLariviÃ¨re,AlinaBeygelzimer,Florencedâ€™AlchÃ©Buc,EmilyFox,andHugoLarochelle.2020.Improv-
ingReproducibilityinMachineLearningResearch(AReportfromtheNeurIPS
2019 Reproducibility Program). arXiv:2003.12206 [cs.LG]
[58]Edward Raff. 2019. A Step Toward Quantifying Independently Reproducible
MachineLearningResearch.In AdvancesinNeuralInformationProcessingSystems
32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS2019, December 8-14, 2019, Vancouver, BC, Canada, Hanna M. Wallach, Hugo
Larochelle, Alina Beygelzimer, Florence dâ€™AlchÃ©-Buc, Emily B. Fox, and Roman
Garnett (Eds.).
[59]JeanineRomano,JeffreyDKromrey,JesseCoraggio,andJeffSkowronek.2006.
Appropriate statistics for ordinal level data: Should we really be using t-test and
2213
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:54:36 UTC from IEEE Xplore.  Restrictions apply. Towards Training Reproducible Deep Learning Models ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
Cohenâ€™sd for evaluating group differences on the NSSE and other surveys. In
annual meeting of the Florida Association of Institutional Research, Vol. 13.
[60]John K. Salmon, Mark A. Moraes, Ron O. Dror, and David E. Shaw. 2011. Parallel
randomnumbers:aseasyas1,2,3.In ConferenceonHighPerformanceComputing
Networking, Storage and Analysis, SC 2011, Seattle, WA, USA, November 12-18,
2011. ACM, 16:1â€“16:12. https://doi.org/10.1145/2063384.2063405
[61]SimoneScardapaneandDianhuiWang.2017. Randomnessinneuralnetworks:
an overview. Wiley Interdiscip. Rev. Data Min. Knowl. Discov. 7, 2 (2017).
[62]Joel Scheuner, JÃ¼rgen Cito, Philipp Leitner, and Harald C. Gall. 2015. Cloud
WorkBench:BenchmarkingIaaSProvidersbasedonInfrastructure-as-Code.In
Proceedingsofthe24thInternationalConferenceonWorldWideWebCompanion,
WWW 2015, Florence, Italy, May 18-22, 2015 - Companion Volume. ACM, 239â€“242.
https://doi.org/10.1145/2740908.2742833
[63]PeterSugimuraandFlorianHartl.2018. Buildingareproduciblemachinelearning
pipeline. arXiv preprint arXiv:1810.04570 (2018).
[64]Peter Sugimura and Florian Hartl. 2018. Building a Reproducible Machine Learn-
ing Pipeline. CoRRabs/1810.04570 (2018). arXiv:1810.04570 http://arxiv.org/abs/1810.04570
[65]Rachael Tatman, Jake VanderPlas, and Sohier Dane. 2018. A practical taxonomy
of reproducibility for machine learning research. (2018).
[66]Ruben Vicente-Saez and Clara Martinez-Fuentes. 2018. Open Science now: Asystematic literature review for an integrated definition. Journal of business
research88 (2018), 428â€“436.
[67]Michael Woelfle, Piero Olliaro, and Matthew H Todd. 2011. Open science is a
research accelerator. Nature chemistry 3, 10 (2011), 745â€“748.
[68]Curtis Yanko. 2021 (accessed August, 2021). Using a Software Bill of Materi-
als (SBOM) is Going Mainstream. https://blog.sonatype.com/software-bill-of-
materials-going-mainstream
[69]Sergey Zagoruyko and Nikos Komodakis. 2016. Wide Residual Networks. In
ProceedingsoftheBritishMachineVisionConference2016,BMVC2016,York,UK,
September19-22,2016.BMVAPress. http://www.bmva.org/bmvc/2016/papers/
paper087/index.html
2214
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:54:36 UTC from IEEE Xplore.  Restrictions apply. 