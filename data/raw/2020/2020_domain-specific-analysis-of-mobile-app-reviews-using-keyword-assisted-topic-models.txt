Domain-Specific Analysis of Mobile App Reviews Using
Keyword-Assisted Topic Models
Miroslav Tushev, Fahimeh Ebrahimi, and Anas Mahmoud
The Division of Computer Science and Engineering
Louisiana State University
Baton Rouge, Louisiana
mtushe1@lsu.edu,febrah1@lsu.edu,amahmo4@lsu.edu
ABSTRACT
Mobile application (app) reviews contain valuable information for
appdevelopers.Aplethoraofsupervisedandunsupervisedtech-
niques have been proposed in the literature to synthesize useful
userfeedbackfromappreviews.However,traditionalsupervised
classification algorithms require extensive manual effort to label
groundtruthdata,whileunsupervisedtextminingtechniques,suchastopicmodels,oftenproducesuboptimalresultsduetothesparsityofusefulinformationinthereviews.Toovercometheselimitations,
inthispaper,weproposeafullyautomaticandunsupervisedap-
proach for extracting useful information from mobile app reviews.
TheproposedapproachisbasedonkeyATM,akeyword-assisted
approachforgeneratingtopicmodels.keyATMovercomestheprob-
lemof datasparsity byusingseeding keywordsextracted directly
from the review corpus. These keywords are then used to generate
meaningful domain-specific topics. Our approach is evaluated over
twodatasetsofmobileappreviewssampledfromthedomainsof
Investing and Food Delivery apps. The results show that our ap-
proachproducessignificantlymorecoherenttopicsthantraditional
topic modeling techniques.
ACM Reference Format:
MiroslavTushev,FahimehEbrahimi,andAnasMahmoud.2022.Domain-
Specific Analysis of Mobile App Reviews Using Keyword-Assisted Topic
Models. In 44th International Conference on Software Engineering (ICSE â€™22),
May 21â€“29, 2022, Pittsburgh, PA, USA. ACM, New York, NY, USA, 12 pages.
https://doi.org/10.1145/3510003.3510201
1 INTRODUCTION
The explosive growth and widespread of mobile technology in
the past decade has changed the way software is produced andconsumed. More users now rely on mobile software than everbefore. According to App Annie - the mobile market data andanalytics platform, the average user spends around 4.2 hours aday using apps [
43]. In response to this massive demand, mobile
app marketplaces, such as Google Play and the Apple App Store
hasgrowndramaticallyinsize,offeringusersvirtuallyunlimited
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation
on the first page. Copyrights for components of this work owned by others than ACMmustbehonored.Abstractingwithcreditispermitted.Tocopyotherwise,orrepublish,topostonserversortoredistributetolists,requirespriorspecificpermissionand/ora
fee. Request permissions from permissions@acm.org.
ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
Â© 2022 Association for Computing Machinery.
ACM ISBN 978-1-4503-9221-1/22/05...$15.00
https://doi.org/10.1145/3510003.3510201choices of apps. For instance, as of 2020, more than four million
appswereavailabletodownloadontheAppleAppStorealone[ 74].
Popularappstoresenableuserstosharetheirexperiencewith
appdevelopersviaratingsandtextualreviews.Thisuniquechan-
nelof userfeedbackcreated anopportunityfor appdevelopersto
monitor their end usersâ€™ reactions to the different releases of their
apps.Recently,analyzingmobileappreviewshasattractedacon-
siderable attention from the research community [ 21]. Researchers
haveutilizedsupervisedandunsupervisedmachinelearningalgo-
rithmstoextractinformativefeedbackfromuserreviews,including
feature requests and bug reports as well as user goals and their
rationale [14, 25, 44, 47, 53, 65, 73].
In general, review mining techniques achieve adequate levels
ofaccuracy,however,theysufferfromseverallimitations.Forin-
stance, supervised classification techniques rely on the presence
ofground-truthdatasetsthattypicallyrequiresignificantmanual
effort to generate [ 25,44,65,72]. Furthermore, these techniques
areconstrainedtoasinglerubricofpredefinedcategoriesand,as
a result, require additional data and model tweaking to general-
izeoverdomain-specificfeedback[ 84].Forexample,usersofthe
Ridesharing app Uber might complain about wait times and rates,
whileusersoftheInvestingappRobinhoodmightraiseconcerns
aboutthe appâ€™srequest for theirsocial securityor bankinforma-
tion. These categories of user feedback can be easily missed in the
ground truthdata.Consequently,a one-size-fits-all approach may
not be suitable for domain-specific user feedback [21, 69, 84].
To avoid the drawbacks of supervised techniques, unsupervised
topic modeling techniques, such as Latent Dirichlet Allocation
(LDA)[9],havebeenappliedtoextractusefulinformationfromapp
store reviews [ 22,26,28,33,62,67]. However, LDA does not per-
formwellwhendealingwithsmallandunstructuredtext[ 6,30,85].
Short text artifacts, such as user reviews [ 81], do not typically
contain enough information for statistical bag-of-words models
to establish semantic connections between words [ 1]. Therefore,
generated topics can be hard to interpret and rationalize and of-
ten require anextensive calibration ofhyperparameters to avoid
misclassifications[12, 30, 87].
Toovercometheselimitations,inthispaperweproposeanew
approach for extracting useful user feedback from app store re-
views. The proposed approach is based on the keyword-assisted
topicmodelkeyATM[ 35].keyATMreliesonasetofrepresentative
seedwordstomodelthetopicsofalargedocumentcollectionby
findingevidenceontheunderrepresentedtopics.Suchseedscan
be extracted from the document corpus automatically by applying
automated text summarization techniques. Our proposed approach
isevaluatedusingtwodatasetsofuserreviewssampledfromthe
7622022 IEEE/ACM 44th International Conference on Software Engineering (ICSE)
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:18:33 UTC from IEEE Xplore.  Restrictions apply. ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA Tushev et al.
domainsofInvestingandFoodDeliveryapps.Thequalityofgener-
atedtopicsisassessedusingasetofintrinsicandextrinsicmeasures
of topic coherence [8].
The rest of this paper is organized as follows. Section 2 formally
describesLDAanditsextension,keyATM.Section3introducesour
approach.Section4evaluatesourapproachusingtwodatasetsof
mobileappreviews.Section5discussesourmainfindingsandtheir
potentialimplications.Section6describesrelatedwork.Section7
addresses the limitations of our study. Finally, Section 8 concludes
our paper and discusses future work.
2 BACKGROUND
Topic models are statistical techniques that are commonly used for
discovering latent topics in text collections. In topic modeling, atopic can be described as a collection of words which representathematicconceptinacorpus,anddocumentsinthecorpusare
representedas probabilisticdistributions overthese topics.In what
follows, we introduce the most commonly used topic modelingapproach, LDA, and its extension - the keyword-assisted topic
model (keyATM).
2.1 Latent Dirichlet Allocation (LDA)
Introduced by Blei et. al [ 9], LDA is an unsupervised technique for
modelingtopicsinacollectionofdocuments.LDAutilizeswordco-occurrenceinformationinordertogrouprelatedwordsintoasingle
topic. To infer topics from a corpus of documents, LDA represents
documentsasrandommixturesoverlatenttopics.Formally,LDA
calculatestwoDirichletdistributions:theword-topicdistribution
ğœ™ğ‘˜fortopicğ‘˜andthedocument-topicdistribution ğœƒğ‘‘fordocument
ğ‘‘.Thehyperparameters ğ›¼andğ›½aretypicallyusedaspriorsfor ğœ™
andğœƒ.Foreachword ğ‘–inthedataset,thetopic ğ‘§ğ‘–isdrawnfromthe
ğœƒğ‘‘distribution and the word ğ‘¤ğ‘–is drawn from the ğœ™ğ‘§ğ‘–distribution.
LDAâ€™s usecases include traditional topic extraction for long
texts[10,45,61,76],tagrecommendationforsearchengines[ 42],
softwaresystemscategorization[ 79],andbuglocalization[ 51].De-
spiteitsadvantages,LDAsuffersfromseverallimitationswhenit
comes to processing online user-generated text. For instance, in
the context of app feedback analysis, mobile app reviews are often
short, personal, and contain colloquial terms. Thus, they are too
semantically-restricted for complex distributional approaches suchas LDA to operate, leading LDA to generate random topics or even
overfit the data [ 3,59,63,77,87]. Furthermore, LDA, by design,
tendstogeneralizeoverlargertopicsinordertobettermodelfre-
quently occurring words. Therefore, more specific and nuanced
topics are often left ignored [ 35]. This limitation is critical for user
reviewanalysisasusefulinformationinuserreviewsistypically
domain-specific [21, 69, 80, 84].
2.2 Keyword-Assisted Topic Modeling
Keyword-assistedtopicmodeling(keyATM)isanoveltechnique
thathasbeenproposedtoimproveupontraditionaltopicmodels,
such as LDA [ 20,35]. The key idea behind keyATM is that it in-
corporates user-defined seed words for topic-word distributions.
Each potential topic can be supplemented by specific keywords
thatarebelievedtodescribeatheme.Formally,keyATMmodifies
the traditional LDA algorithm in two ways:(1)The word-topic distribution ğœ™ğ‘˜is replaced with a â€œmix-
tureâ€oftwodistributions:aseed-topicdistribution ğœ™ğ‘ and
aregular-topicdistribution ğœ™ğ‘Ÿ.Theseed-topicdistribution
can only select words from the initial seed set, while the
regular-topicdistributionmayselectanywordsinthecor-
pus, including the seed words. The parameter ğœ‹ğ‘˜controls
the probability of drawing a word from either ğœ™ğ‘ orğœ™ğ‘Ÿ.
(2)Todrawthedocument-topicdistribution ğœƒğ‘‘,foreachdocu-
mentğ‘‘, a binary vector /vecğ‘of the length ğ‘†(number of seeded
topics) is generated. /vecğ‘takes the values of 1 if ğ‘‘contains any
keywordfromarespectiveseedsetand0otherwise.Next,
a document-group distribution ğœğ‘‘is sampled from /vecğ‘with a
hyperparameter ğœfromwhichagroupvariable ğ‘”isdrawn.
Eachgrouprepresentsaseedsetselectedfromthecorpus.Finally, the group-topic distribution
ğœ“ğ‘”is used as prior to
drawğœƒğ‘‘.
Algorithm1formallydescribesthecompletekeyword-assisted
topic modelâ€™s generative process [35].
Algorithm 1 keyATMâ€™s topic generative process.
1:fortopicğ‘˜=1...ğ‘‡do
2:choose regular topic distribution ğœ™ğ‘Ÿ
ğ‘˜âˆ¼Dir(ğ›½ğ‘Ÿ)
3:choose seeded topic distribution ğœ™ğ‘ 
ğ‘˜âˆ¼Dir(ğ›½ğ‘ )
4:choose parameter ğœ‹ğ‘˜ âŠ²prob. of drawing from seeded topic
5:end for
6:forseed setğ‘ =1...ğ‘†do
7:choose group-topic distribution ğœ“ğ‘ âˆ¼Dir(ğ›¼) âŠ²of length T
8:end for
9:fordocument ğ‘‘=1...ğ·do
10:choose a binary vector /vecğ‘ âŠ²of length S
11:choose a document-group distribution ğœğ‘‘âˆ¼Dir(ğœ/vecğ‘)
12:choose a group variable ğ‘”âˆ¼Mult(ğœğ‘‘)
13:chooseğœƒğ‘‘âˆ¼Dir(ğœ“ğ‘”) âŠ²of length T
14:forwordğ‘–=1...ğ‘ğ‘‘do
15: choose a topic ğ‘§ğ‘–,ğ‘‘âˆ¼Mult(ğœƒğ‘‘)
16: chooseğ‘¥ğ‘–âˆ¼Bern(ğœ‹ğ‘§ğ‘–)âŠ²choose which topic distr. to draw from
17: ifğ‘¥ğ‘–is 0then
18: select a word ğ‘¤ğ‘–âˆ¼Mult(ğœ™ğ‘Ÿğ‘§ğ‘–) âŠ²from regular
19: else
20: select a word ğ‘¤ğ‘–âˆ¼Mult(ğœ™ğ‘ ğ‘§ğ‘–) âŠ²from seeded
21: end if
22:end for
23:end for
Our main assumption in this paper is that keyATM can over-
comethelimitationsofLDAwhendealingwithmobileappreviews.
Inparticular,toaddressthedomain-specificityproblem,keyATM
utilizes a binary vector /vecğ‘which elevates the less-common topics
for the provided seed words. These seed words can be extracted in
advancebasedonexpertopinioninordertosupplementkeyATM
with a high-level overview of the user review corpus. The main ad-
vantageofkeyATMisthatoncetheinitialseedsareprovided,itcan
collect additional semantically-related keywords from regular top-
ics (line 2, line 16). By combining seeded and regular distributions,
keyATM generates more cohesive and focused topics, overcoming
the main limitation of using LDA to model semantically-restricted
user reviews.
763
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:18:33 UTC from IEEE Xplore.  Restrictions apply. Domain-Specific Analysis of Mobile App Reviews Using Keyword-Assisted Topic Models ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
3 APPROACH
TheproposedapproachisdepictedinFig.1.Ourapproachcanbe
dividedintothreesteps.Inthefirststep,weapplyseveralheuristics
to extract informative user reviews. In the second step, we pre-
process and summarize extracted reviews in order to generate a
representative set of important keywords, or seeds, for the corpus.
In the third step, seeds are fed into keyATM to generate a topic
distributionoverextractedreviews.Inwhatfollows,wedescribe
each of these steps using illustrative examples.
3.1 Informative Review Extraction
Mobileappreviewsvaryinquality.Previousresearchhasshown
thatappreviewsdonotfollowawell-definedstructureandoften
containspellingfluctuations,colloquialterms,andspam[ 39,82].
Therefore, a large proportion of app store reviews is simply un-
informative [ 39]. As topic models are particularly susceptible to
generating uninterpretable topics from semantically poor docu-
ments [18], the first step of our approach is to improve the quality
of our review corpus by filtering out uninformative feedback.
To detect informative reviews, we adopt Guo and Singhâ€™s ap-
proachforsynthesizingpotentiallymeaningfuluserstoriesfrom
mobileappreviews[ 24].Auserstory canbedefinedasarelation-
ship between an action that a user took and a problem that an app
produced in response to that action. Such stories describe usersâ€™experiences and outcomes when interacting with their apps. Forexample, a user might complain that their navigation app loses
GPSsignal whenindrivemode. Suchastory contains potentially
usefulinformationforappdevelopersasitoutlinesthecondition
(indrivemode )underwhichaproblem(GPSlostsignal )occurred.
Thesestoriesarecommonlypresentinlow-starappreviews(one
andtwostars)giventhatappproblemsareoftenaccompaniedby
lowratings[ 31,38,81].Ourexpectationisthattopicsmodeledafter
reviews with user stories will be more coherent, and thus, more
interpretable.
To identify reviews that might contain user stories, common
temporal conjunctions are used, including words such as after,
as soon as, before,every time, then,until,when,whenever, while,
andduring. Temporal conjunctions indicate temporal and causal
ordering of events that was found to be particularly helpful for
mitigating problems of text sparseness [ 55]. For example, consider
the four app reviews in Example 1. R1,R2, andR3are informative
reviewsthatcontainuserstoriesintheformofaction-problempairs
(temporalconjunctions areunderlined). R1describesanaction of
scrollthroughthepages andaproblem,a crash.Suchareviewcan
helpatopicmodeltobuildasemanticassociationbetween crash
andscrolling. R4is a false positive.
Example 1
â€¢R1: This app crashes when I scroll through the pages.
â€¢R2:Theywant yourSSN beforeyoucaneven look.Itâ€™s
definitely a scam.
â€¢R3: To verify identity it requires u take a picture but
thenimmediately crashes.
â€¢R4: Still waiting, after a month, to be approved.3.2 Seed Generation
Under this step of our approach, we seek to generate sets of repre-
sentativeseeds(keywords)fromextracteduserreviews.Tocorrectly
modeltheunderlyinglatenttopicstructure,thesekeywordshave
to be representative of as many themes in the review corpus as
possible [35]. Therefore, the keyword generation process requires
a prioriknowledge of the domain of interest which might not be
always readily available [ 4,5]. For example, to model the represen-
tative topics of app reviews in the domain of Investing apps, the
researcherhastoknowthespecificthemesthatusersdiscussinthe
reviews andthe corresponding keywordsto generalize overthese
themes.Extractingsuchkeywordstypicallyinvolvesmanuallyclas-
sifying userreviews into representative topics,which nullifies the
advantageofunsupervisedtechniques.Toaddressthislimitation,in
our adaptation of keyATM, instead of determining seeds manually,
we use extractive summarization.
A summary can be described as a short and concise descrip-
tion that encompasses the main themes in a collection of docu-
ments[37,48].Inextractivesummarization,textartifacts(e.g.,re-
views,comments,andtweets)whichcontainthemostimportant
keywordsinthecorpusareextractedaspotentialsummariesofthe
entire corpus. In a sense, each generated summary represents a po-
tential latent theme in a collection of documents, thus can be used
to provide representative keywords (seeds) for keyATM. Common
extractivesummarizationtechniques,suchasHybridTF.IDF[ 34]
andSumBasic [ 60],have beenapplied tosummarize unstructured
online user feedback (e.g., tweets, YouTube comments, and user
reviews) [ 36,68,83,84] and have been shown to achieve very high
levelsofagreementwithhuman-generatedsummaries.Basedon
theseobservations,inouranalysis,weutilizesuchtechniquesto
extract the initial set of seeding keywords from the corpus.
Togeneratesummariesfromappuserreviews, HybridTF.IDF[34]
isoftenutilized.TF.IDFconsistsoftwocomponents:1)TF-Term
Frequency, or how many times a term appears in a document
and 2) IDF - Inverse Document Frequency, or how much infor-mation a term provides. TF.IDF-based methods have shown ac-ceptable accuracy levels across a variety of text summarization
tasks [2,16,32,40,56]. However, short texts, such as user reviews,
poseauniquechallengetoTF.IDF.Inparticular,becauseshorttextscontainonlyahandfulofwords,theprobabilityofindividualterms
occurringmultipletimesinasinglereviewislow.Therefore,the
majorityofwordsareassignedthesameTFvalue.HybridTF.IDF
addressesthisissuebycalculatingtermfrequencyasthenumber
of times a term ğ‘¡appears across all reviews divided by the total
number of terms in the review collection. Formally, the Hybrid
TF.IDF of a term ğ‘¡can be computed as:
Hybrid TF.IDF (ğ‘¡,ğ‘‘)=ğ‘“ğ‘¡,ğ·/summationtext.1ğ‘“ğ‘¡,ğ‘‘Ã—log|ğ·|
|ğ‘‘âˆˆğ·:ğ‘¡âˆˆğ‘‘|(1)
whereğ‘“ğ‘¡,ğ·is the frequency of term ğ‘¡in all documents,/summationtext.1ğ‘“ğ‘¡,ğ‘‘is
the total number of terms in the collection, |ğ·|is the number of
documents in the corpus, and |ğ‘‘âˆˆğ·:ğ‘¡âˆˆğ‘‘|is the number of
documents that contain ğ‘¡. The total weight of a document ğ‘‘is cal-
culated by summing up all termsâ€™ weights. Ho wever, in the current
form,HybridTF.IDFwouldbebiasedtowardlongerreviewsastheycontain more terms. To work around this problem, a normalization
764
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:18:33 UTC from IEEE Xplore.  Restrictions apply. ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA Tushev et al.
 
 
Text preprocessing 
Summarization  
(Hybrid TF-IDF) 
App developer 
 App users 
Keyword (seed) extraction 
Temporal conjunctions Informative Review extraction Topic generation 
1-2 star rating 
 Keyword (seed) selection 
keyATM 
Figure 1: Our proposed approach for app review topic modeling
factorğ‘›ğ‘“is introduced. The modified Hybrid TF.IDF formula for a
document ğ‘‘can be defined as follows:
Hybrid TF.IDF (ğ‘‘)=1
max(ğ‘›ğ‘“,|ğ‘‘|)Ã—|ğ‘‘|/summationdisplay.1
ğ‘–=1Hybrid TF.IDF (ğ‘¡ğ‘–,ğ‘‘)(2)
Thenormalizationfactoristypicallydefinedastheupper-boundof
the required summary length (number of words) and can be deter-
minedexperimentally.Theactualsummarizationisthenperformed
byrankingthereviewsbytheirtotalweight.Toavoidasituation
wheresummarieswithsimilarwordsarerankedtogether,asimi-
larity threshold,calculated as thecosine of the angle betweenthe
vectorized representationsof each twosummaries, is used. Anop-
timal similarity threshold can be determined experimentally based
on the desired uniqueness of summaries.
To illustrate our summarization step, consider the summaries
in Example 2 generated for a dataset of reviews sampled from the
domainofInvestingappswithathresholdof0.1(notwosummaries
should have a cosine similarity greater than 0.1). Each summary
encompasses a separate topic, such as taking money from a userâ€™s
account(S 1),appcrashing(S 2),problemswithsellingastock(S 3),
and issues with customer support (S 4).
Example 2
â€¢S1: This app takes out money even after you close your
account...
â€¢S2: This app now crashes 100% of the time, every time I
open it
â€¢S3:Allowedmeto buystock,butwhen itriedto sell my
stock they didnâ€™t sell it
â€¢S4: Great until you need customer support, once you
need support youâ€™re on your own
To improve the accuracy of summarization, text processing
strategies are often used. Before generatingsummaries, extracted
app reviews are first converted to lowercase and tokenized into
individual words,with punctuation,URLs, and otherspecial sym-
bolsremoved.Additionalsplittingstrategies,suchassplittingdigits
and alpha-characters are performed (e.g. 2hrsbecomes 2 hrs). Eng-
lish stop-words, such as will, this, it, are removed based on the list
provided in the NLTK package [ 7]. Additional cohort-specific stop-
wordsaremanuallyidentifiedandaddedtothelist.Thesewords
include app names (e.g., robinhood andacorn), frequent words (e.g.,
yeahandwell),and short1-2letterwords thatdonotcontain anysemanticinformation.Finally,lemmatizationisappliedtothere-
sultinglistofwords.Lemmatizationisanormalizationtechnique
which reduces the number of distinct entries in the data. More
specifically, lemmatization converts a word toits dictionary form.
Thisprocessisappliedtoimprovetheperformanceofclustering
algorithmsbycollapsingdifferentformsofthesamewordintoa
single entity [ 41]. Example 3 shows the processed summaries from
Example 2.
Example 3â€¢S
1: take, money, even, close, account
â€¢S2: app, crash, time, every, time, open
â€¢S3: allow, buy, stock, try, sell, stock, sell
â€¢S4: great, need, customer, support, need, support
3.3 Topic generation
Sinceeachsummarysuccinctlydescribesaseparatetheme,seeds
are generated by obtaining distinct terms from the processed sum-
maries.Forexample,theterms great,need,customer,and support
are extracted from S4to describe a theme discussing issues with
customer support. These seeds are then supplied to keyATM for
the topic modeling step. Our assumption is that these terms canprovide enough semantic information for keyATM to be able to
generalizeoverthewholedatasetofextracteduserreviews.Inwhatfollows,weempiricallyevaluateourassumptionusingtwodatasets
of mobile app reviews.
4 EVALUATION
In this section, we illustrate the operation of our proposed topicgeneration approach over two datasets of user reviews sampled
from the domains of Investing and Food Delivery apps. We further
evaluate our generated topics by comparing them to the topics
generatedbyLDA.Ourmainresearchquestionis: How well does
our approach perform in comparison to LDA?
4.1 Data Collection
To demonstrate the operation of our approach, we apply it on
twodatasetsofmobileappreviewssampledfromthedomainsof
Investing and Food Delivery apps. Investing apps have becomeincreasingly popular in recent years due to the increasing inter-
estincryptocurrencytrading.Zero-commissiontradingfeesand
continuousmediacoveragehavemultipliedthepopularityeffect
of these apps by bringing in millions of new first-time traders. For
example,Robinhood,asimplifiedInvestingapp,reportedthatmore
765
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:18:33 UTC from IEEE Xplore.  Restrictions apply. Domain-Specific Analysis of Mobile App Reviews Using Keyword-Assisted Topic Models ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
than 6 million new users have joined the platform in 2021 right
after the WallStreetBets subreddit controversy [ 23,57]. Similarly,
the domain of Food Delivery has experienced an unprecedentedgrowth during the COVID-19 pandemic as the demand for Food
Delivery services has significantly increased. For example, the four
major Food Delivery apps: DoorDash, UberEats, GrubHub, andPostmates reported a significant increase in revenue during the
stay-at-homeorderof2020[ 75].Infact,themarketsegmentoffood
delivery apps, currently estimated at $126.91 billion, is expected to
grow to $192.16 billion by 2025 [70].
To collect user reviews for our analysis, we selected the most
popular apps from both domains. To identify these apps, the top
100appsinthecategoriesofFinance(Investing)andFood&Drink
(Food Delivery) on Google Play and the Apple App Store were
examined. Apps which met the following criteria were included in
our analysis:
(1)For an app to be included in our analysis, we only consid-
eredappswith10,000reviewsormore.Thisnumberofre-
views is necessary in order to include only popular and
well-established apps in our dataset.
(2)For the Investing domain, banking â€œall-in-oneâ€ apps were
excludedasthemajorityoftheseappsdidnotprovideInvest-
ing services. For Food Delivery apps, specific restaurantsâ€™
deliveryapps,suchas PapaJohnâ€™sPizza&Delivery official
app, were also excluded.
After examining the top 100 apps, eight Investing and five Food
Deliveryappswereincluded.Foreachoftheseapps,wecollectedalltextualreviewsuptoJanuary1stof2021fromtheAppleAppStore
andGooglePlayusingPythonwebscrappers12.Overall,370,820
appreviewswerecollectedforoursetofInvestingappsand266,544
reviews were collected for the set of Food Delivery apps. Out ofthese reviews, only 1-2 star rating reviews which included userstories (See Section 3.1) were considered in our analysis, a totalof 20,760 reviews for the domain of Investing apps and 130,676
reviewsfortheFoodDeliveryapps.Thedistributionofextracted
reviews over our apps is shown in Table 1.
4.2 Evaluation measures
Duetothefactthatthereistypicallynoground-truthdocument-
topic distribution that exists for every corpus, evaluating topic
modelscanbeachallengingtask.Toaddressthischallenge,several
topicevaluationtechniqueshavebeenproposedintheliterature.
Fromamongthesetechniques,NormalizedPointwiseMutualInfor-
mationhasbeenfoundtobeverycloselycorrelatedwithhuman
judgment of topic quality [15, 46, 71].
Introduced by Bouma [ 11], Normalized Pointwise Mutual Infor-
mation (NPMI) is an information-theoretic measure of information
overlapbetweenwords.NPMIcanbemeasuredbycountinghow
manytimestwowordsappearinthesamedocumentversushow
manytimestheyappearseparately.Formally,fortwowords ğ‘¤ğ‘–and
ğ‘¤ğ‘—, NPMI can be calculated as:
NPMI(ğ‘¤ğ‘–,ğ‘¤ğ‘—)=logğ‘(ğ‘¤ğ‘–,ğ‘¤ğ‘—)
ğ‘(ğ‘¤ğ‘–)ğ‘(ğ‘¤ğ‘—)
âˆ’logğ‘(ğ‘¤ğ‘–,ğ‘¤ğ‘—)(3)
1https://pypi.org/project/app-store-scraper/
2https://pypi.org/project/google-play-scraper/Table 1: The number of user reviews extracted for each app
in our dataset.
Investing Food delivery
App Reviews App Reviews
Robinhood 7872UberEats 58933
Acorn 4342DoorDash 34917
Stash 2445Grubhub 17784
E*TRADE 1605Postmates 17610
Fidelity 1496Seamless 1432
TD Ameritrade 1403
Schwab 1079
Personal Capital 509
whereğ‘(ğ‘¤ğ‘–,ğ‘¤ğ‘—)isthenumberofdocumentsinwhich ğ‘¤ğ‘–andğ‘¤ğ‘—
appeartogether,and ğ‘(ğ‘¤ğ‘–)ğ‘(ğ‘¤ğ‘—)isthethenumberofdocuments
whichcontain ğ‘¤ğ‘–andğ‘¤ğ‘—respectively.NPMIisnormalizedusing
the negative log-transformed count of the number of times ğ‘¤ğ‘–and
ğ‘¤ğ‘—appeared together. This results in a value between -1 ( ğ‘¤ğ‘–and
ğ‘¤ğ‘—never occur together) and 1 ( ğ‘¤ğ‘–andğ‘¤ğ‘—only occur together).
TheunderlyingassumptionbehindusingNPMIforevaluating
topic quality is that words of cohesive topics should be well con-
nected,orhaverelativelyhighaveragepairwiseNPMI.Forexample,
Fig. 2 shows a sample NPMI graph of a word set sampled from the
review corpus of the Food Delivery domain. The graph shows that
the words cold,driver,food,lost,late,andhotformadense-setof
well-connected nodes (words). This is expected given that these
words frequently appear in the same reviews, for example, â€œdrivers
are always either late or lost I always get my food cold and my drink
hot.â€The word discount, while connected to food, stands at a fur-
thersemanticdistancefromotherwordsasitdoesnotappearas
frequently with them in the same reviews.
TherearetwomainstrategiestocomputeNPMI:intrinsicandex-
trinsic [8]. Intrinsic NPMI is calculated based on the co-occurrence
of topic words within the corpus. In contrast, the extrinsic strategyuses external datasets of human-produced textual knowledge, such
as Wikipedia, to compute words co-occurrence, and thus semantic
relatedness, of words. Intrinsic NPMI scores computed over the
corpus can show how well the model learned the underlying data,
or the extent that generated topic models accurately represent the
content of a corpus. Extrinsic NPMI, on the other hand, shows
how common generated topics are in daily language, which can be
analogous to how a human examining the quality of topics would
decide whether they are coherent or not [71].
Inour analysis,weemploy bothstrategiesfor computingtopic
coherence. For extrinsic evaluation, we used the entire English
Wikipedia dump of October 2017. The dump included 5 million
articles,133word-lengthperarticleonaverage,packedintoa16GB
JSON file. Each article was tokenized and preprocessed and special
non-ASCII symbols were removed. The coherence of a given topic
is calculated as the average NPMI between its 10 most probable
words in a topic. Formally, topic coherence is calculated as:
ğ‘ğ‘œâ„ğ‘’ğ‘Ÿğ‘’ğ‘›ğ‘ğ‘’ (ğ‘¡)=1
45Ã—9/summationdisplay.1
ğ‘–=110/summationdisplay.1
ğ‘—=ğ‘–+1NPMI(ğ‘¤ğ‘–,ğ‘¤ğ‘—) (4)
766
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:18:33 UTC from IEEE Xplore.  Restrictions apply. ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA Tushev et al. 
food  cold 
hot 
late driver  
lost 
discount  
Figure2:AconnectedNPMIgraphofwordsextractedfrom
the review corpus of food delivery apps.
4.3 Model Configuration
Tocomparetheperformanceofourapproachtothebaseline(LDA),
weperformhyperparametertuninginordertoachievethemaxi-
mum coherence score possible over both datasets. In addition to
the number of topics ( ğ¾), LDA has two hyperparameters, ğ›¼and
ğ›½. We use the implementation of LDA from the Gensim Python
package3, whereğ›¼is inferred from the corpus automatically and ğ›½
is set to 1 /ğ¾. As a standard practice of topic models evaluation, we
trainLDAfor ğ¾={10,20,...,100}.Theseboundsof ğ¾wereselected
based on the coherence score, where a sharp decline indicates that
a model no longer can generate coherent topics.
One of the main parameters that determines the performance of
ourapproachisthequalityoftheseedingwords.Smallernumberof
seeds might not convey enough semantic information for keyATM
to capture meaningful topics, while larger number of keywordsmightbetoogeneraltoformcohesivetopics.Toselectthesetof
seeds, we calculate the pairwise NPMI scores for each pair of seeds
ineachindividualsummaryreview.Wethenattempttooptimize
thesetofseedsbyremovingthebottom ğ‘›-thpercentileoftheseeds
(seeding words which share the lowest average pairwise similarity
with other seeds). The main assumption is that by removing these
potentially unrelated words we can produce a more focused set of
seeds, and thus, better topics. For example, Table 2 shows the aver-
ageNPMIscoreforagroupofwordssampledfromthesummary
reviews of the Food Delivery corpus. Words such as refuse,first,
andaddcan be removed due to their low average pairwise simi-
laritytoother wordsinthegroup.Todeterminehowmanyseedstoconsider,we includefourmodelconfigurationsinouranalysis:
ğ‘›={0,5,15,25}.keyATM_ ğ‘›referstokeyATMbeingtrainedafter
theğ‘›% of seeds at the lower end of NPMI score are removed. Each
keyATMconfigurationisthentrainedfordifferentvaluesof ğ¾to
determine the optimal /angbracketleftğ‘›,ğ¾/angbracketrightconfiguration for the review corpus.
4.4 Evaluation Results
To answer our research question, we trained an LDA model and
eachconfigurationofkeyATMoverourtwodomainsofappreviews
for various ğ¾values. The coherence scores for the trained models
are shown in Fig. 3. For each ğ¾, we compared the mean topic
coherencescoresbyperforminganindependenttwo-tailedt-test
betweenLDAandeachconfigurationofkeyATM.Tomeasurethe
effectsize,Cohenâ€™sdwascomputedfor10-40topicsandHedgesâ€™
g was calculated for 50-100 topics [ 27]. For the two-sample means
test, the d values of 0.2, 0.5, and 0.8 are typically interpreted as
3https://pypi.org/project/gensim/Table2:ThepairwiseNPMIscoresforanexamplekeyword
setsampledfromtheFoodDeliverydomain.â€œrefuseâ€isre-
moved at the bottom 5th percentile, â€œfirstâ€ is removed at the
bottom 15thpercentile, and â€œaddâ€is removed atthe bottom25th percentile.
service
charge
fee
large
make
flat
use
place
add
first
refuse
service 10.100.140.070.080.060.130.050.010.060.10
charge 0.1010.320.080.080.090.050.070.100.050.07
fee 0.140.3210.130.090.150.060.040.240.040.00
large 0.070.080.1310.110.200.060.100.100.070.07
make 0.080.080.090.1110.060.070.080.080.080.06
flat 0.060.090.150.200.0610.010.020.090.050.09
use 0.130.050.060.060.070.0110.050.030.120.05
place 0.050.070.040.100.080.020.0510.060.130.03
add 0.010.100.240.100.080.090.030.0610.03-0.05
first 0.060.050.040.070.080.050.120.130.0310.02
refuse 0.100.070.000.070.060.090.050.03-0.050.021
small, medium, and large effect sizes respectively. The results of
our analysis are presented in Tables 3 and 4.
Theresults showthat ourapproach outperformsLDA interms
ofextrinsicandintrinsiccoherencescoresforbothdomainsover
allvaluesof ğ¾,withminorexceptions.FortheInvestingdomain,
LDAâ€™stopicqualitydeclinessharplyafter20topics,whileourap-
proachmaintainsarelativelyflatcoherencecurveoverthewhole
range ofğ¾. In terms of significance of the obtained results, our ap-
proach performs significantly better when the number of topics is
high (ğ¾â‰¥50). Forğ¾=10, our approach significantly outperforms
LDA in terms of intrinsic coherence, suggesting that our approach
infersthetopicsfromtheunderlyingInvestingdatasetbetter.For
the Food Delivery domain, we observe a similar trend, with LDAâ€™s
topiccoherencedroppingmoresharplyafterthe ğ¾=30mark,thus,
every keyATM configuration significantly outperforms LDA for
ğ¾â‰¥50. Furthermore, some configurations, such as keyATM-15
and keyATM-25 significantly outperform LDA across all ğ¾â‰¥20 in
termsofextrinsiccoherence.Intermsofeffectsize,ourapproach
consistentlyoutperformsLDAwiththemediumeffectsize,espe-
ciallyfortheFoodDeliverydataset.Sucheffectsizessuggestthat
69% or more of the topics produced by our approach are of higher
quality than that of LDA.
We further observed that some keyATM configurations outper-
formeachotherfordifferent ğ¾values.Forexample,intheInvesting
domain,keyATM-25performsthe bestwhen ğ¾={10,40,80,100},
butproducesslightlyworseresultsfortherestofthe ğ¾s.Tocom-
pare the performance of various keyATM configurations, we used
an independent t-test for every pair of configurations. The gen-eral trend suggests that removing some percentile of unrelatedkeywords somewhat helps the model to produce more coherenttopicsforcertain
ğ¾values.However,notallimprovementswere
statistically significant. In what follows, we discuss these trends
along with their potential implications in greater detail.
5 DISCUSSION AND IMPACT
Based on our results, the main implication of our study suggests
thatitispossibletoavoidacostlytaggingprocessofagroundtruth
dataset and still outperform a traditional topic modeling technique
767
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:18:33 UTC from IEEE Xplore.  Restrictions apply. Domain-Specific Analysis of Mobile App Reviews Using Keyword-Assisted Topic Models ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
Figure3: Topicintrinsic andextrinsic coherencescorescalculated forourtwo datasetsof appreviews acrossalltested model
configurations and ğ¾. For example, keyATM-5 is a keyATM model with the bottom 5th percentile of keywords (seeds) removed
from each summary.
Table 3: Independent t-test results (t-values) for the difference of means of extrinsic (Ext) and intrinsic (Int) coherence of the
baseline (LDA) and the various keyATM model configurations over the dataset of Investing app reviews. Significant values
are shown in bold font (p <0.05). Effect size was calculated using Cohenâ€™s d for n>50 and Hedgesâ€™ g for n â‰¤50. Effect size can be
interpreted as follows: s - small (d â‰¤0.2), m - medium (0.2 <d<0.8), l - large (d â‰¥0.8).
keyATM-0 keyATM-5 keyATM-15 keyATM-25
# of topics Ext Int Ext Int Ext Int Ext Int
10 0.792.60l0.971.90 1.43 2.86l1.53 3.22l
20 0.47 0.74 0.12 0.88 0.21 1.62 -0.08 -0.10
30 1.840.68 0.760.20 1.31 0.38 0.86 0.65
40 1.43 1.64 1.633.54m0.98 1.78 2.23s1.77
50 2.42s1.54 2.13s2.13s3.02m0.45 2.64m0.47
60 1.01 1.91 1.013.21m1.54 2.93m1.50 3.62m
70 2.09s3.50m1.902.18s2.88s2.23s2.01s1.81
80 1.404.33m2.24s1.92 1.63 2.72s3.03s3.91m
90 0.982.36s1.282.17s2.45s1.56 1.95 3.48m
100 2.47s4.58m3.05s2.70s3.17s3.25s4.45m2.49s
Table 4: Independent t-test results (t-values) for the difference of means of extrinsic (Ext) and intrinsic (Int) coherence of the
baseline (LDA) and the various keyATM model configurations over the dataset of Food Delivery app reviews. Significant values
are shown in bold font (p <0.05). Effect size was calculated using Cohenâ€™s d for n>50 and Hedgesâ€™ g for n â‰¤50. Effect size can be
interpreted as follows: s - small (d â‰¤0.2), m - medium (0.2 <d<0.8), l - large (d â‰¥0.8).
keyATM-0 keyATM-5 keyATM-15 keyATM-25
# of topics Ext Int Ext Int Ext Int Ext Int
10 -0.06 0.00 0.51 0.34 0.43 0.09 0.75 -0.19
20 1.75 0.62 1.58 1.08 2.32m0.68 2.15m0.32
30 1.66 0.07 1.74 -0.73 2.28m-0.57 2.38m-0.26
40 2.37m1.69 2.48m1.72 2.42m1.80 2.39m1.76
50 3.58m2.69m3.76m2.72m4.03m2.77m3.87m2.80m
60 3.61m3.40m3.83m3.22m3.88m3.30m4.08m3.48m
70 4.22m3.85m4.36m3.86m4.29m3.93m4.37m3.80m
80 3.76m3.42m3.80m3.57m4.08m3.56m3.74m3.34m
90 4.26m3.95m4.44m4.11m4.50m4.10m4.64m4.04m
100 5.75l5.84l6.06l5.81l5.99l5.45m6.17l5.88l
768
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:18:33 UTC from IEEE Xplore.  Restrictions apply. ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA Tushev et al.
overappreviewdata.Withonlyahandfulofhyperparametersto
consider,ourstudyoutlinesa first-of-its-kind approachforinferring
meaningfullatenttopicsfromasemantically-restrictedcorpusof
user reviews.
Ourresultsprovideevidencethatitispossibletosignificantly
improvethequalityoftheinferredtopicsbysupplementingatopic
modelwithasmallset(3-20)ofrepresentativekeywords.Infact,
even when the supplied keyword set is incomplete, the model is
stillabletocorrectlyinferlatenttopicsandprovideinterpretable
results. For example, Table 5 presents the top-5 topics in termsof coherence generated for the reviews in our datasets. Topic 3of the Investing app reviews is about losing money due to pricefluctuations on a trading day. However, only a handful of words
hinting toward that specific topic were provided to the model: sell,
stock, and lose. Nevertheless, the model correctly identified the
main idea of the topic and discovered new related words. Another
exampleisTopic1intheFoodDeliveryreviewcorpus,whichseemstobediscussingaproblemwithcustomerservice.Interestingly,the
keywords customer andservicewere never supplied to this topic.
Nevertheless, the model was still able to create a topic with a high
coherence score.An interestingcase is Topic2 from theInvesting
domain-noneofthekeywordsappearedinthetop-10wordsofthe
topic,however,thetopicâ€™sthemeisinterpretable,pointingoutto
user interface (UI) problems. In fact, this topic which was obtained
withğ¾=10doesnotappearinLDAâ€™stopicsatallforthesimilar
values of ğ¾={10,20,30}. This shows that our approach is able
toextractunderrepresentedtopicsfromasemantically-restricted
corpus where LDA typically fails.
As expected, smaller sets of focused keywords (seeds) produced
the highest-quality topics. A large number of general keywords
still managed to produce coherent topics, although such keywords
arelesslikelytoappearinthetop-10listofthetopic.Thissuggests
that the quality of the resulting model is influenced by the quality
of the supplied keywords, therefore the main effort should be fo-
cusedontheprocessofimprovingtheseedingwordsforthetopics.
Thisprocess,inturn,isdependentontheunderlyingdataset.For
example, using different summarization parameters can producedifferentseeds.Assuch,theseparametersshouldbetunedalong
with the rest of the hyperparameters, such as ğ¾,ğ›¼, andğ›½.
In terms of specific model configurations, our results show a
general trend toward higher coherence score produced by the con-
figurations with a higher percentile removed keywords ( ğ‘›). This
suggeststhatthereisno one-size-fits-all approachwhenitcomes
to deriving an optimal model: each document corpus requires a
comprehensive hyperparameter tuning strategy to create the best-
performingmodel.Forinstance,intermsofcoherence,wefound
thatkeyATM-15performedthebestwhen ğ¾wassetto10forthe
Investing dataset and ğ¾=50 for the Food Delivery dataset. This
differencein ğ¾canbeexplainedbythedifferenceindatasetsize;
theFoodDeliverydatasethasabout6.5timesmorereviewsthan
the Investing dataset.
Intermsofimpact,ourexpectationisthatourapproachwould
advancethestateoftheartbyenablingfurtherempiricalinvesti-
gations related to using topic modeling in software engineeringtasks. In particular, LDA has long been used to provide supportfor basic Software Engineering activities, such as requirements
traceability[ 29],buglocalization[ 78],coderetrieval[ 50],andmostrecently mobile app review analysis. However, LDAâ€™s performance
has always been hindered by the limited semantic and syntactic
natureofsoftwareartifacts,whethersourcecode,bugreports,re-
quirements specifications, or software user reviews [ 17,29,54,84].
Our proposed approach aims to address these challenges by as-
sessingLDAtoproducemorecohesivetopicsthatcanbepointed
out by a few important words extracted from the corpus. Such
words can be automatically determined using basic text summa-
rization techniques that have been shown to achieve high levels of
agreementswithhumangeneratedsummariesovercollectionsof
software user feedback [ 36,68,83]. This relatively simple method-
ology can overcome the limitations often associated with other
expensive solutions, such as using machine learning to tune LDAâ€™s
parameters [ 64], thus, enabling the development of more practical
software engineering tools.
In terms of practical impact, our results highlight the need for a
more nuanced and domain-aware approach for extracting informa-
tion from user reviews. Similar to existing topic modeling and text
classification algorithms, our approach can facilitate a transition
from domain knowledge torequirements specifications. However,
the key advantage of our approach over existing techniques is that
it can provide more fine-grained requirements information and
avoid the need to manually label a subset of reviews. The informa-
tion obtained from keyATM can then be effectively used by app
developers.Forinstance,developersofInvestingappsmayobserve
that UI (Topic 2) is a major concern of Investing app users, thus,
concentrate their development effortsto improve user experience
byextractingreviewswheretheUItopichasahighprobability,and
maybe even derive a more nuanced set of keywords (e.g., screen,
color,graphs,etc.)tosplitthetopicintoanylevelofdetailrequired.
After the release of their app, developers can further monitor user
feedback and reallocate their resources more efficiently to quickly
address any emerging user concerns.
6 RELATED WORK
Theproblemofextractingvaluableinformationfromappreviews
hasreceivedsignificantattentionintheliterature[ 14,25,26,36,44,
47,49,53,58,72,73,80,82,84,86].Awidevarietyofsupervisedand
unsupervised techniques have been used to mine such reviews for
differentcategoriesoffeedback.Forexample,GuoandSingh[ 24]
proposed Caspar - an approach for extracting user stories from
informativeappreviews.Auserstoryisanâ€œaction-problemâ€pair
of events wherea problem with an appis triggered by a userâ€™sac-
tion.Theauthorsapplieddependencyparsingtoextracttemporally-relateduserstoriesfromappreviewsandtrainabidirectionalLSTM
network for classification. Panichella et al. [ 66]p r o po s edat oo lf o r
classifying user reviews into useful software maintenance cate-
gories. The approach uses NLP heuristics, such as common linguis-
tic patterns, to formulate features for the classifier. The authors
foundthatthestructureofareviewanditssentimentcanpredict
the maintenance category with high precision and recall. Williams
et al. [84] proposed a methodology to extract domain-specific user
feedbackfromappreviewsandtweets.TheauthorsutilizedHybrid
TF.IDFtoidentifyimportantwordsfromappreviewsandthenused
PMI to derive relationships between them to form specific user
concerns in a given app domain.
769
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:18:33 UTC from IEEE Xplore.  Restrictions apply. Domain-Specific Analysis of Mobile App Reviews Using Keyword-Assisted Topic Models ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
Table 5: The top-5 generated topics for the Investing and Food Delivery domains by the best performing model configurations.
keyATM-15withK=10wasusedfortheInvestingdomain,andkeyATM-15withK=50wasusedforFoodDelivery.Seedsare
highlighted in bold. Example reviews are selected from the top-10 reviews of each topic.
Domain Topics Most probable words Example review
InvestingTopic 1 trade,day, money, trad-
ing,stock, time, market,
platform, service, like...price could swing $.10-$.30/share easily between the time youâ€™ve submitted a purchase
and when it actually submits. Trading hours open up at 8am and only last until 5pm, the
worst trading hours by far
Topic 2 see, update, change, new,
stock, like, screen, look,
show, viewPleasechangethecolorandfontofnewRobinhood,itâ€™shardtofocusandithurtmyeyes
if I look into app for while, not impress with this update.
Topic 3 money,sell,stock,lose,
price, market, time, buy,
trade, tradingWhen I decided to sell my shares my orders were not executed at my price and the stock
kept going down. Lost of hundred dollars.
Topic 4 account, money, email,
customer, bank, time, ser-
vice,day,say,supportThe customer service is HORRIBLE!!!!!!!!... Contacted customer service 2 or 3 times and
after 24 hours... They say it takes about 10 days to close and get money transferred back
to my bank.
Topic 5 money, account, take,
bank, invest, fee, charge,
back, day, transferReasonformegivingalowratingisduetothefactthatallitseemstheappisdoingis
taking my money. Iâ€™m getting over drafts, I STILL donâ€™t see where my $700 in roundups
went
Food deliveryTopic 1 help,problem, would,
service,could, customer,
app,great,able, orderCustomer service is really bad. 2 days in a row they canceled my orders
Topic 2 app,work,address, or-
der, try, time, use, update,
even, getWhenItypeinmyaddressandzipcodeandthenhitFindRestaurants,itsays"Stateis
UNSET should be VALID"
Topic 3 drive,driver,food,minute,
around, house, away,
street, go, carWatched the driver drive to the next town over before delivering our food.
Topic 4 app, order, time, second,
crash,t r y ,twice, open,
use, everyOver the past few days the app has been crashing every single time I open it.
Topic 5 app, ever, number, use,
bad, give, account, sign,
one,starI canâ€™t even log in. It asks for my phone number...
Along the lines of our work, active learning was proposed to
reducethemanualeffortrequiredtoclassifyappreviews.Active
learning algorithms learn the data incrementally by selecting a
small sample of reviews for manual labeling and predicting the
remaining labels. The steps are repeated until the desired accuracy
is reached. For example, Dhinakaran et. al. [ 19] evaluated an active
learningpipelineforappreviewclassification.Theauthorsclassi-
fiedreviewsintofourcategories:featurerequest,bugreport,user
experience, and rating. Several uncertainty sampling metrics were
proposed and evaluated, such as Least Confident Prediction, Small-
estMargin,andHighestEntropy.Theexperimentwasconducted
on Maalej et al.â€™s labeled datasets [ 52]. The authors showed that
activelearnersthatemploybinaryclassifiersweremoreeffective
for review classification tasks than passive learners.
Recently,unsupervisedtechniqueshavebeenusedtodiscover
latent topics in user reviews. Most of these techniques modify and
extend LDA to increase its effectiveness for user generated feed-
back[12,13,59,62,69].Forinstance,Mehrotraetal.[ 59]proposed
groupingrelatedshortusertweetsbasedonhashtagsbeforesup-
plying them to the LDA model. Qiao et al. [ 69] introduced theLatentProductDefectMiningModel(LPDM)forcollectingdomain-
specific product defects from customer reviews. This approach
augmentsLDA byincludinglatentproductcomponents andtheir
descriptions.Whilethesetechniquessharethesamegoalofimprov-
ing topic cohesiveness, to the best of our knowledge, our proposed
approachisthefirsttoutilizekeyATMaswellastextsummarization
techniques to generate more cohesive topics of user reviews.
7 THREATS TO VALIDITY
Thestudyconductedinthispapersuffersfromseveralmethodologi-
calconstraintsthatmightjeopardizethevalidityofourresults.One
majorexternalvaliditythreatstemsfromthefactthatourresults
might not be generalizable to other application domains of mobile
apps.Inanattempttoovercomethisthreat,weperformedouranal-ysisontwodifferentdomainsofapps,InvestingandFoodDelivery.
Furthermore, our datasets have drastically different sizes to ensure
thatourapproachisabletoproducecoherenttopicsregardlessof
the amount of data available.
An internal validity threat might arise from the fact that we
removed a large amount of reviews during our informative review
770
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:18:33 UTC from IEEE Xplore.  Restrictions apply. ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA Tushev et al.
extracting step in order to increase the quality of the review cor-
pus.Therefore,someofthelatenttopicsfromtheremovedreviews
mighthavebeenmissed.However,performingreviewfilteringis
a standard practice for any supervised or unsupervised machine
learning task. These methodologies have been shown to remove
high percentage of uninformative reviews in review corpora with
high levels of precision [ 24,38]. Furthermore, the hyperparame-
tersettingsusedtotuneLDAmightaffecttheinternalvalidityof
the study. However, there are no robust optimization methodology
available for every parameter for every scenario. Therefore, we ap-
plied a standard procedure of training several models with various
ğ¾parameter values until the best results were obtained.
Another internal validity threat might stem from the apps se-
lected for each domain. While we acknowledge that there are
dozens, if not hundreds, of apps available under the Investing and
Food Delivery domains, most of these apps do not have a sufficient
number of reviews. Including such less popular apps would be
problematicasthegeneratedtopicswouldbebiasedtowardapps
with more reviews. Nonetheless, we acknowledge the fact that the
results of our analysis might not necessarily generalize over other
apps in our domains or over other domains.
Construct validity is the degree to which the various perfor-
mance measures used in the study accurately capture the concepts
theypurporttomeasure.Aconstructvaliditythreatmightberaised
about the reliability of the coherence measures used to evaluate
thequalityofgeneratedtopics.Toaddressthesethreats,weused
two types of intrinsic and extrinsic coherence measures that are
based on the semantic relatedness of topic words. These measures
have been extensively used in the literature and have been shown
to achieve high correlation levels with human judgment [ 71]. Gen-
erally speaking, topic models are often utilized as a means to an
end, where generated topics are used to enable other tasks, such as
informationretrieval,orevenusedasinputformachinelearning
algorithms,thus,theyarebetterevaluatedinthatcontext.Nonethe-
less, further evaluation using human judges is necessary to paint a
full picture of topic quality.
8 CONCLUSIONS
Inthispaper,weproposedanapproachforanalyzingmobileapp
user feedback using keyword-assisted topic models. The proposed
approach relies on a set of seeding words (keywords) extracted
from the corpus to generate more cohesive topics. We showed that
these keywords can be automatically extracted from the corpus
usinggeneral-purposeextractivesummarizationtechniques.The
proposed approach was evaluated using two datasets of user re-
views,sampledfromthedomainsofInvestingandFoodDelivery
apps.Theresultsshowedthatourproposedkeywordassistedtopic
modelingapproachwasabletosignificantlyoutperformLDAon
both intrinsic and extrinsic measures of topic cohesiveness. Fur-thermore, our approach was able to model topics that are oftenoverlooked by LDA. Our findings in this paper are intended to
advance the state-of-the-artin mobile app review analysisas well
asenablefurtherempiricalinvestigationsintotopicmodelingfor
software user feedback. To achieve these goals, our work in this
paper will be extended along two main directions:â€¢Automatictuning:wewillcontinuetoevaluatetheproposed
approach over larger datasets of mobile app reviews and
across more application domains. Our objective is to devise
automated optimization strategies for tuning the differentparameters of our underlying topic modeling approach in
different settings.
â€¢Tool support: a working prototype which will implementour findings in this paper will be implemented and made
publiclyavailable.Suchaprototypewillenableustoexamine
the applicability and usability of our approach as well as its
overall effectiveness in practical settings.
ACKNOWLEDGMENT
This work was supported by the U.S. National Science Foundation
(Award CNS 1951411).
REFERENCES
[1]CharuAggarwalandChengxiangZhai.2012. Asurveyoftextclusteringalgo-
rithms. In Mining Text Data. Springer, 77â€“128.
[2]Nasser Alsaedi, Pete Burnap, and Omer Rana. 2016. Temporal TF-IDF: A high
performance approach for event summarization in twitter. In International Con-
ference on Web Intelligence. 515â€“521.
[3]LeticiaAnaya. 2011. ComparingLatent DirichletAllocation andLatentSemantic
Analysis as Classifiers. ERIC.
[4]David Andrzejewski and Xiaojin Zhu. 2009. Latent dirichlet allocation with
topic-in-set knowledge. In Workshop on Semi-Supervised Learning for Natural
Language Processing. 43â€“48.
[5]David Andrzejewski, Xiaojin Zhu, and Mark Craven. 2009. Incorporating do-
main knowledge into topic modeling via Dirichlet forest priors. In International
Conference on Machine Learning. 25â€“32.
[6]Lidong Bing, Wai Lam, and Tak-Lam Wong. 2011. Using query log and socialtaggingtorefinequeriesbasedonlatenttopics.In InternationalConferenceon
Information and Knowledge Management. 583â€“592.
[7]StevenBird.2006. NLTK:theNaturalLanguageToolkit.In InteractivePresentation
Sessions. 69â€“72.
[8]Stuart Blair, Yaxin Bi, and Maurice Mulvenna. 2020. Aggregated topic modelsfor increasing social media topic coherence. Applied Intelligence 50, 1 (2020),
138â€“156.
[9]DavidBlei,AndrewNg,andMichaelJordan.2003. LatentDirichletAllocation.
The Journal of Machine Learning research 3 (2003), 993â€“1022.
[10]Levent Bolelli, Åeyda Ertekin, and Lee Giles. 2009. Topic and trend detection
intextcollectionsusingLatentDirichletAllocation.In EuropeanConferenceon
Information Retrieval. 776â€“780.
[11]Gerlof Bouma. 2009. Normalized (pointwise) mutual information in collocation
extraction. German Society for Computational Linguistics 30 (2009), 31â€“40.
[12]Laura Galvis Carreno and Kristina Winbladh.2013. Analysisof user comments:
An approach for software requirements evolution. In International Conference on
Software Engineering. 582â€“591.
[13]DimpleChehal,ParulGupta,andPayalGulati.2021. Implementationandcom-
parison of topic modeling techniques based on user reviews in e-commerce
recommendations. Journal of Ambient Intelligence and Humanized Computing 12,
5 (2021), 5055â€“5070.
[14]NingChen,JialiuLin,StevenHoi,XiaokuiXiao,andBoshenZhang.2014. AR-
miner: mining informative reviews for developers from mobile app marketplace.
InInternational Conference on Software Engineering. 767â€“778.
[15]KahyunChoi,JinHaLee,CraigWillis,andStephenDownie.2015.TopicModelingUsersâ€™InterpretationsofSongstoInformSubjectAccessinMusicDigitalLibraries.
InJoined Conference on Digital Libraries. 183â€“186.
[16]Hans Christian, Mikhael Pramodana Agus, and Derwin Suhartono. 2016. Single
document automatic text summarization using term frequency-inverse docu-ment frequency (TF-IDF). ComTech: Computer, Mathematics and Engineering
Applications 7, 4 (2016), 285â€“294.
[17]AndreaDeLucia,MassimilianoDiPenta,RoccoOliveto,AnnibalePanichella,andSebastianoPanichella.2012.UsingIRmethodsforlabelingsourcecodeartifacts:Is
it worthwhile?. In International Conference on Program Comprehension. 193â€“202.
[18]Stefan Debortoli, Oliver MÃ¼ller, Iris Junglas, and Jan Brocke. 2016. Text min-
ing for information systems researchers: An annotated topic modeling tutorial.
Communications of the Association for Information Systems 39, 1 (2016), 7.
[19]Venkatesh Dhinakaran, Raseshwari Pulle, Nirav Ajmeri, and Pradeep Murukan-
naiah. 2018. App review analysis via active learning: reducing supervision effort
771
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:18:33 UTC from IEEE Xplore.  Restrictions apply. Domain-Specific Analysis of Mobile App Reviews Using Keyword-Assisted Topic Models ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
withoutcompromisingclassificationaccuracy.In IEEEInternationalRequirements
Engineering Conference. 170â€“181.
[20]ShuseiEshima,KosukeImai,and TomoyaSasaki.2020. Keywordassistedtopic
models.arXiv preprint arXiv:2004.05964 (2020).
[21]Necmiye Genc-Nayebi and Alain Abran. 2017. A systematic literature review:
Opinion mining studies from mobile app store user reviews. Journal of Systems
and Software 125 (2017), 207â€“219.
[22]MariaGomez,RomainRouvoy,MartinMonperrus,andLionelSeinturier.2015.
A recommender system of buggy app checkers for app store moderators. In
International Conference on Mobile Software Engineering and Systems. 1â€“11.
[23] Oscar Gonzalezand DavidPriest. 2021. RobinhoodBacklash: WhatYou Should
Know About the GameStop Stock Controversy. https://www.cnet.com/personal-
finance/investing/robinhood-backlash-what-you-should-know-about-the-
gamestop-stock-controversy/. Accessed: 2021-12-29.
[24]HuiGuoandMunindarSingh.2020. Caspar:Extractingandsynthesizinguser
stories of problems from app reviews. In International Conference on Software
Engineering. 628â€“640.
[25]Emitza Guzman, Muhammad El-Haliby, and Bernd Bruegge. 2015. Ensemble
methodsforappreviewclassification:Anapproachforsoftwareevolution(n).
InInternational Conference on Automated Software Engineering. 771â€“776.
[26]Emitza Guzmanand Walid Maalej. 2014. How do userslike this feature? Afine
grained sentiment analysis of app reviews. In IEEE International Requirements
Engineering Conference. 153â€“162.
[27]Larry Hedges. 1981. Distribution theoryfor Glassâ€™s estimator ofeffect size and
related estimators. Journal of Educational Statistics 6, 2 (1981), 107â€“128.
[28]Kazuyuki Higashi, Hiroyuki Nakagawa, and Tatsuhiro Tsuchiya. 2018. Improve-
mentofUserReviewClassificationUsingKeywordExpansion(S).In International
Conference on Software Engineering & Knowledge Engineering. 125â€“124.
[29]AbramHindle,ChristianBird,ThomasZimmermann,andNachiappanNagappan.
2012. Relating requirements to implementation via topic analysis: Do topics
extractedfromrequirementsmakesensetomanagersanddevelopers?.In IEEE
International Conference on Software Maintenance. 243â€“252.
[30]Liangjie Hong and Brian Davison. 2010. Empirical study of topic modeling in
twitter. In Workshop on Social Media Analytics. 80â€“88.
[31]Leonard Hoon, Rajesh Vasa, Jean-Guy Schneider, and Kon Mouzakis. 2012. A
preliminary analysis of vocabulary in mobile app user reviews. In Computer-
Human Interaction Conference. 245â€“248.
[32]Eduard Hovy, Chin-Yew Lin, et al .1999. Automated text summarization in
SUMMARIST. Advances in Automatic Text Summarization 14 (1999), 81â€“94.
[33]Claudia Iacob and Rachel Harrison. 2013. Retrieving and analyzing mobileapps feature requests from online reviews. In Conference on Mining Software
Repositories. 41â€“44.
[34]David Inouye and Jugal Kalita. 2011. Comparing twitter summarization al-gorithms for multiple post summaries. In International Conference on Privacy,
Security,RiskandTrustandInternationalConferenceonSocialComputing.298â€“
306.
[35]JagadeeshJagarlamudi,HalDaumÃ©,andRaghavendraUdupa.2012. Incorporating
lexical priors into topic models. In Conference of the European Chapter of the
Association for Computational Linguistics. 204â€“213.
[36]Nishant Jha and Anas Mahmoud. 2018. Using frame semantics for classifying
and summarizing application store reviews. Empirical Software Engineering 23, 6
(2018), 3734â€“3767.
[37]ElhamKhabiri,JamesCaverlee,andChiao-FangHsu.2011. Summarizinguser-
contributedcomments.In InternationalAAAIConferenceonWebandSocialMedia ,
Vol. 5.
[38]Hammad Khalid, Emad Shihab, Meiyappan Nagappan, and Ahmed E Hassan.2014. What do mobile app users complain about? IEEE software 32, 3 (2014),
70â€“77.
[39]Mubasher Khalid, Muhammad Asif, and Usman Shehzaib. 2015. Towards im-
proving the quality of mobileapp reviews. International Journal of Information
Technology and Computer Science 7, 10 (2015), 35.
[40]RahimKhan,YurongQian,andSajidNaeem.2019. ExtractivebasedTextSum-
marization Using K-Means and TF-IDF. International Journal of Information
Engineering & Electronic Business 11, 3 (2019).
[41]Tuomo Korenius, Jorma Laurikkala, Kalervo JÃ¤rveli, and Martti Juhola. 2004.Stemming and lemmatization in the clustering of finnish text documents. In
International Conference on Information and Knowledge Management. 625â€“633.
[42]Ralf Krestel, Peter Fankhauser, and Wolfgang Nejdl. 2009. Latent Dirichlet
Allocation for tag recommendation. In Recommender Systems Conference. 61â€“68.
[43]Donny Kristianto. 2021. Winning the Attention War: Consumers in Nine Major
MarketsNowSpendMorethanFourHoursaDayinApps. https://www.appannie.
com/en/insights/market-data/q1-2021-market-index/. Accessed: 2021-05-31.
[44]ZijadKurtanoviÄ‡andWalidMaalej.2017. Mininguserrationalefromsoftware
reviews. In IEEE International Requirements Engineering Conference. 61â€“70.
[45]RetnoKusumaningrum,IhsanAjiWiedjayanto,SatriyoAdhy,etal .2016. Clas-
sification of Indonesian news articles based on Latent Dirichlet Allocation. In
International Conference on Data and Software Engineering. 1â€“5.[46]Jey Han Lau, David Newman, and Timothy Baldwin. 2014. Machine reading
tea leaves: Automatically evaluating topic coherence and topic model quality. In
ConferenceoftheEuropeanChapteroftheAssociationforComputationalLinguistics.
530â€“539.
[47]Xiaozhou Li, Boyang Zhang, Zheying Zhang, and Kostas Stefanidis. 2020. A
Sentiment-Statistical Approach for Identifying Problematic Mobile App Updates
Based on User Reviews. Information 11, 3 (2020), 152.
[48]ClareLlewellyn,ClaireGrover,andJonOberlander.2014. Summarizingnews-
paper comments. In International AAAI Conference on Web and Social Media,
Vol. 8.
[49]Mengmeng Lu and Peng Liang. 2017. Automatic classification of non-functional
requirements from augmented app userreviews.In International Conference on
Evaluation and Assessment in Software Engineering. 344â€“353.
[50]Stacy Lukins, Nicholas Kraft, and Letha Etzkorn. 2008. Source Code RetrievalforBugLocalizationUsingLatentDirichletAllocation.In ReverseEngineering.
155â€“164.
[51]Stacy Lukins, Nicholas Kraft, and Letha Etzkorn. 2010. Bug localization usingLatent Dirichlet Allocation. Information and Software Technology 52, 9 (2010),
972â€“990.
[52]WalidMaalej,ZijadKurtanoviÄ‡,HadeerNabil,andChristophStanik.2016. On
theautomaticclassificationofappreviews. RequirementsEngineering 21,3(2016),
311â€“331.
[53]Walid Maalej and Hadeer Nabil. 2015. Bug report, feature request, or simply
praise? On automatically classifying app reviews. In IEEE International Require-
ments Engineering Conference. 116â€“125.
[54]Anas Mahmoud and Gary Bradshaw. 2017. Semantic topic models for source
code analysis. Empirical Software Engineering 22, 4 (2017), 1956â€“2000.
[55]InderjeetMani,MarcVerhagen,BenWellner,ChungminLee,andJamesPuste-
jovsky. 2006. Machine learning of temporal relations. In International Conference
onComputationalLinguisticsandMeetingoftheAssociationforComputational
Linguistics. 753â€“760.
[56]UshaManjari,Syed Rousha,DasiSumanth,andSirishaDevi. 2020. Extractive
TextSummarizationfromWebpagesusingSeleniumandTF-IDFalgorithm.In
International Conference on Trends in Electronics and Informatics. 648â€“652.
[57]John McCrank. 2021. Robinhood Added 6 Million Crypto Users in Last Two
Months. https://finance.yahoo.com/news/robinhood-added-6-million-crypto-
212636002.html. Accessed: 2021-12-29.
[58]StuartMcIlroy,NasirAli,HammadKhalid,andAhmedEHassan.2016. Analyzing
and automatically labelling the types of user issues that are raised in mobile app
reviews.Empirical Software Engineering 21, 3 (2016), 1067â€“1106.
[59]RishabhMehrotra,ScottSanner,WrayBuntine,andLexingXie.2013. Improving
LDAtopicmodelsformicroblogsviatweetpoolingandautomaticlabeling.In
Conference on Research and Development in Information Retrieval. 889â€“892.
[60]Ani Nenkova and Lucy Vanderwende. 2005. The impact of frequency on summa-
rization.MicrosoftResearch,Redmond,Washington,Tech.Rep.MSR-TR-2005 101
(2005).
[61]XiaochuanNi,Jian-TaoSun,JianHu,andZhengChen.2009. Miningmultilingual
topics from Wikipedia. In International Conference on World Wide Web. 1155â€“
1156.
[62]Ehsan Noei, Feng Zhang, and Ying Zou. 2019. Too many user-reviews, what
shouldappdeveloperslookatfirst? TransactionsonSoftwareEngineering (2019).
[63]JeungminOh,DaehoonKim,UichinLee,Jae-GilLee,andJunehwaSong.2013.
Facilitatingdeveloper-user interactionswith mobileappreview digests. In CHI
Extended Abstracts on Human Factors in Computing Systems. 1809â€“1814.
[64]AnnibalePanichella,BogdanDit,RoccoOliveto,MassimilanoDiPenta,Denys
Poshynanyk,andAndreaDeLucia.2013. Howtoeffectivelyusetopicmodelsfor
softwareengineeringtasks?Anapproachbasedongeneticalgorithms.In 2013
35th International Conference on Software Engineering. IEEE, 522â€“531.
[65]Sebastiano Panichella, Andrea Di Sorbo, Emitza Guzman, Corrado Visaggio,
GerardoCanfora,andHaraldGall.2015. HowcanIimprovemyapp?Classifyinguserreviewsforsoftwaremaintenanceandevolution.In InternationalConference
on Software Maintenance and Evolution. 281â€“290.
[66]Sebastiano Panichella, Andrea Di Sorbo, Emitza Guzman, Corrado Visaggio, Ger-
ardo Canfora, and Harald Gall. 2016. Ardoc: App reviews development oriented
classifier. In International Symposium on Foundations of Software Engineering.
1023â€“1027.
[67]Dae Hoon Park, Mengwen Liu, ChengXiang Zhai, and Haohong Wang. 2015.Leveraging user reviews to improve accuracy for mobile app retrieval. In In-
ternational Conference on Research and Development in Information Retrieval.
533â€“542.
[68]Elizabeth PochÃ©, Nishant Jha, Grant Williams, Jazmine Staten, Miles Vesper, and
Anas Mahmoud. 2017. Analyzing user comments on YouTube coding tutorial
videos. In International Conference on Program Comprehension. 196â€“206.
[69]Zhilei Qiao, Xuan Zhang, Mi Zhou, Gang Alan Wang, and Weiguo Fan. 2017. A
domainorientedLDAmodelforminingproductdefectsfromonlinecustomer
reviews. (2017).
[70]ResearchandMarkets.2021. GlobalOnlineFoodDeliveryServicesMarketReport
2021:MarketisExpectedtoReach$192.16Billionin2025,from$126.91Billion
772
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:18:33 UTC from IEEE Xplore.  Restrictions apply. ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA Tushev et al.
in 2021 - Long-term Forecast to2030. https://www.prnewswire.com. Accessed:
2021-07-24.
[71]Michael RÃ¶der, Andreas Both, and Alexander Hinneburg. 2015. Exploring the
space of topic coherence measures. In International Conference on Web Search
and Data Mining. 399â€“408.
[72]FurqanRustam,ArifMehmood,MuhammadAhmad,SaleemUllah,DostMuham-
mad Khan, and Gyu Sang Choi. 2020. Classification of shopify app user reviews
using novel multi text features. IEEE Access 8 (2020), 30234â€“30244.
[73]Andrea Di Sorbo, Sebastiano Panichella, Carol Alexandru, Junji Shimagaki, Cor-
radoVisaggio,GerardoCanfora,andHaraldGall.2016. Whatwoulduserschange
inmyapp?Summarizingappreviewsforrecommendingsoftwarechanges.In
International Symposium on Foundations of Software Engineering. 499â€“510.
[74]Statista. 2021. Number of available apps in the Apple App Store from 2008 to
2020. https://www.statista.com/statistics/268251/number-of-apps-in-the-itunes-
app-store-since-2008/. Accessed: 2021-05-31.
[75]LeviSumagaysay.2020. Thepandemichasmorethandoubledfood-deliveryappsâ€™
business. Now what? https://www.marketwatch.com. Accessed: 2021-07-24.
[76]Shaheen Syed and Marco Spruit. 2017. Full-text or abstract? Examining topic
coherencescoresusingLatentDirichletAllocation.In InternationalConference
on Data Science and Advanced Analytics. 165â€“174.
[77]Maria Terzi, Maria-Angela Ferrario, and Jon Whittle. 2011. Free text in userreviews: Their role in recommender systems. In Workshop on Recommender
Systems and the Social Web at International Conference on Recommender Systems .
45â€“48.
[78]Stephen Thomas, Meiyappan Nagappan, Dorothea Blostein, and Ahmed Hassan.
2013. Theimpactofclassifierconfigurationandclassifiercombinationonbug
localization. IEEE Transactions on Software Engineering 39, 10 (2013), 1427â€“1443.
[79]Kai Tian, Meghan Revelle, and Denys Poshyvanyk. 2009. Using Latent Dirichlet
Allocation for automatic categorization of software. In International Working
Conference on Mining Software Repositories. 163â€“166.
[80]Miroslav Tushev, Fahimeh Ebrahimi, and Anas Mahmoud. 2020. Digital Discrim-
inationinSharingEconomyARequirementsEngineeringPerspective.In IEEE
International Requirements Engineering Conference. 204â€“214.
[81]Rajesh Vasa, Leonard Hoon, Kon Mouzakis, and Akihiro Noguchi. 2012. A
preliminaryanalysisofmobileappuserreviews.In Computer-HumanInteraction
Conference. 241â€“244.
[82]JianyuWang,RuiWen,ChunmingWu, YuHuang, andJianXion. 2019. Fdgars:
Fraudster detection via graph convolutional networks in online app review
system. In World Wide Web Conference. 310â€“316.
[83]GrantWilliamsandAnasMahmoud.2017. MiningTwitterFeedsforSoftware
User Requirements. In International Requirements Engineering Conference. 1â€“10.
[84]Grant Williams, Miroslav Tushev, Fahimeh Ebrahimi, and Anas Mahmoud. 2020.
Modeling user concerns in Sharing Economy: the case of food delivery apps.
Automated Software Engineering 27, 3 (2020), 229â€“263.
[85]XiaohuiYan,JiafengGuo,YanyanLan,andXueqiCheng.2013. Abitermtopic
modelforshorttexts.In InternationalConferenceonWorldWideWeb .1445â€“1456.
[86]HuiYangandPengLiang.2015. IdentificationandClassificationofRequirements
from App User Reviews.. In International Conference on Software Engineering &
Knowledge Engineering. 7â€“12.
[87]Wayne Xin Zhao, Jing Jiang, Jianshu Weng, Jing He, Ee-Peng Lim, Hongfei Yan,
and Xiaoming Li. 2011. Comparing twitter and traditional media using topic
models. In European Conference on Information Retrieval. 338â€“349.
773
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:18:33 UTC from IEEE Xplore.  Restrictions apply. 