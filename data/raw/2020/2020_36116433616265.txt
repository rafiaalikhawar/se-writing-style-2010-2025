Neural-BasedTestOracleGeneration: A Large-Scale Evaluation
and LessonsLearned
Soneya BintaHossain∗
sh7hv@virginia.edu
University ofVirginia
USAAntonioFilieri
aﬁlieri@amazon.com
AmazonWebServices
USAMatthewB.Dwyer
matthewbdwyer@virginia.edu
University ofVirginia
USA
SebastianElbaum
selbaum@virginia.edu
University ofVirginia
USAWillem Visser
vissie@amazon.com
AmazonWebServices
USA
ABSTRACT
Deﬁning test oracles is crucial and central to test development, but
manualconstructionoforaclesisexpensive.Whilerecentneural-
based automated test oracle generation techniques have shown
promise, their real-world eﬀectiveness remains a compelling ques-
tionrequiringfurtherexplorationandunderstanding.Thispaper
investigatestheeﬀectivenessofTOGA,arecentlydevelopedneural-
based method for automatic test oracle generation. TOGA utilizes
EvoSuite-generatedtestinputsandgeneratesbothexceptionand
assertionoracles.Ina Defects4j study,TOGAoutperformedspec-
iﬁcation,search,andneural-basedtechniques,detecting57bugs,
including 30 unique bugs not detected by other methods. To gain a
deeper understanding of its applicability in real-world settings, we
conductedaseriesofexternal,extended,andconceptualreplication
studies ofTOGA.
In a large-scale study involving 25 real-world Java systems,
223.5K test cases, and 51K injected faults, we evaluate TOGA’s
ability to improve fault-detection eﬀectiveness relative to the state-
of-the-practice and the state-of-the-art. We ﬁnd that TOGA mis-
classiﬁes the type of oracleneeded 24% ofthe time and that when
it classiﬁes correctly around 62% of the time it is not conﬁdent
enough to generate any assertion oracle. When it does generate
an assertion oracle, more than 47% of them are false positives, and
the true positive assertions only increase fault detection by 0.3%
relative to prior work. These ﬁndings expose limitations of the
state-of-the-art neural-based oracle generation technique, provide
valuable insights for improvement, and oﬀer lessons for evaluating
future automatedoracle generation methods.
CCSCONCEPTS
•Softwareanditsengineering →Softwaretestinganddebug-
ging;•Computing methodologies →Neural networks .
∗Partof theworkwas donewhileinterning at Amazon Web Services
ESEC/FSE ’23, December 3–9, 2023, San Francisco, CA,USA
©2023 Copyright heldby theowner/author(s).
ACM ISBN 979-8-4007-0327-0/23/12.
https://doi.org/10.1145/3611643.3616265KEYWORDS
Neural Test Oracle Generation, TOGA, EvoSuite, Mutation Testing
ACM Reference Format:
Soneya Binta Hossain, Antonio Filieri, Matthew B. Dwyer, Sebastian El-
baum, and Willem Visser. 2023. Neural-Based Test Oracle Generation: A
Large-ScaleEvaluationandLessonsLearned.In Proceedingsofthe31stACM
JointEuropeanSoftwareEngineeringConferenceandSymposiumontheFoun-
dations of Software Engineering (ESEC/FSE ’23), December 3–9, 2023, San
Francisco, CA, USA. ACM, New York, NY, USA, 13pages.https://doi.org/10.
1145/3611643.3616265
1 INTRODUCTION
Testing is the standard method for validating that a program meets
its requirements. To test a program a test input is passed to the
program and its output is judged by a test oracle that asserts a
propertyoftheexpectedprogrambehavioronthegiveninput[ 37].
A test suite is comprised of a set of input-oracle pairs, called test
cases. The key value of a test suite lies in its ability to detect faults.
Fault detection relies on the choice of good test inputs – to ensure
thatanyfaultystatementsareexecuted–and,foreachinput,atest
oraclethatcandetecterrorstatesintroducedbyfaultsandjudge
themagainst necessary conditions for correctness [ 57].
A rich literature on test adequacy metrics exists to assess the
fault exposure ability of test inputs [ 11,59] and a growing litera-
ture existsontheimportanceof strong test oraclesfor faultdetec-
tion [24,25,27,46,51,60,62]. Unfortunately, manual development
of high-quality test suites is extremely costly, time consuming,
and error-prone [ 4,27]. Consequently researchers have focused on
methods for automating the generation of test cases. They have
been particularly successful in developing a range of cost-eﬀective
methods for generating high-quality test inputs [ 1,8,10,19,21,32,
34,35,48,61].Automaticallygeneratingeﬀectivetestoracleshas
proven more challenging and many of these techniques have used
eitherimplicitoracleswhichusechecksenforcedbytheruntime
system, such as null pointer dereference exceptions, diﬀerential
oracles which compare the output of two programs or program
versions against eachother [ 36],ormetamorphic whichcheckfor
knownrelationsbetweenmutationoftheinputsandcorresponding
changes ofthe outputs [ 47].
To unleash their full potential, test generation techniques must
go beyondimplicit, diﬀerential,and metamorphicoracles, pairing
test inputs with input value-speciﬁc oracle assertions that capture
intended program behavior.Researchers haveexplored the useof
Thiswork islicensedunderaCreativeCommonsAttribution4.0Interna-
tional License.
120
ESEC/FSE ’23, December3–9, 2023,San Francisco, CA, USA SoneyaBintaHossain, Antonio Filieri,Ma/t_thewB.Dwyer, Sebastian Elbaum,andWillem Visser
naturallanguageprocessing(NLP)andpatternmatchingtechniques
togeneratetestoraclesbasedoncodecommentsandtextdocumen-
tation[6,7,22,38,52].Suchtechniquesareabletoinfer assertion
oraclesthat check actual program output against expected output,
andexception oracles that capture intended exceptional behavior of
theprogramundertest.Morerecentlyneuraltechniqueshavebeen
appliedtogeneratetestoraclesusingatransformer-basedmodel
that learns from the method under test and developer-written test
cases[54,55,58].Followingonthiswork,theTOGA[ 17]neural-
based test oracle generator was recently found to outperform prior
work in detecting real faults. We explain TOGA in detail in § 2, but
brieﬂy: given a method under test, a test preﬁxwhich is a code
fragment that includes the test input and a sequence of operations
to drive theprogram under test into a desired execution state, and
optional documentation strings, TOGA predicts whether an excep-
tionoracleoranassertionoracleshouldbegenerated,andinthe
lattercase,itgeneratesthepredicatewithintheassertionoracle.On
astudyperformedonthe Defects4j benchmark[ 29]consistingof
835 bugs from 17 Java applications, TOGA was reported to outper-
formotherneural-basedassertiongenerationtechniques[ 54,55,58]
by detecting 57 bugs, of which 30 unique bugs not detected by any
othercompeting techniques.
WhiletheoriginalevaluationofTOGAwasfocusedmostlyon
theDefects4j benchmark,inthiswork,wesettoevaluateitseﬀec-
tiveness on larger benchmarks and with perspectives and methods
that more closely resemble those of industrial practice. To this
end, we conduct a series of three external replications of TOGA
with the objectives of validating its original fault detection ﬁnd-
ings, characterizingits precision (i.e., the frequency of generating
correct oracles), measuring the fault-detection power of the gen-
erated correct oracles, and broadening our understanding of its
generalizabilityto awider setofprograms.
To distinguish our research questions (RQ) from those in the
TOGA paper,we subscript references to the latter with /u1D447.
RQ1 (Exact Replication of RQ3 /u1D447):In RQ1, our hypothesis is
thatRQ3 /u1D447waswell-designedandexecuted,therefore,weshould
obtainsimilarresults.Tothisend,weconductedan exactreplication
oftheDefects4j faultstudy as describedinthe originalpaper.
We were able to obtain the same results. However, we found
that the majority (67%) of total reported bugs could be identiﬁed
by Java “implicit oracle” (i.e., exceptional behavior triggered by
executingthetestpreﬁxalone).Suchpreﬁxeseliminatetheneed
fortestoraclesgeneratedbyTOGAandledtoanoverestimationof
TOGA’sfaultdetectioncapability.Animportantlessonfromthis
ﬁndingisthatfutureexperimentalevaluationshouldincludethe
implicit oracle as a baseline to correctly attribute the bug detection
capabilities ofgeneratedoracles (§ 3.5).
RQ2 (Conceptual Replication of RQ2 /u1D447):This replication
studyevaluatesTOGA’sperformanceonabroaderandnewerset
ofinputs.Wehypothesizethatanewdatasetwouldyieldsimilar
resultstotheoriginalTOGAstudy,indicatingitsreplicability(same
methodonnewdataset).Tothisend,from25large-scaleJavaap-
plications,weconstructedanewdatasetwithatotalof223ktest
cases–eachhavingatestpreﬁxandasingleassertionorexception
oracle,whereas RQ2 /u1D447studied61kinputsfrom aheld-out test set.
GoingbeyondtheTOGApaper,wecomputedadditionalmetrics
fordeeperinsightsintotechniqueperformance.Forinstance,wecalculatedthefalsepositiveratesforeachtypeofgroundtruthlabel:
“No Exception” (18%), “Exception Expected” (81%), and “Assertion”
(47%). Furthermore, we computed a “no assertion generation rate”
of 62%, indicating that even when TOGA correctly predicts the
needforanassertionoracle,itmaynotgenerateoneconﬁdently.
Additionally, we analyzed false positive rates for diﬀerent types of
assertionoracles(74%FPR for assertEquals,Table 4),whichoﬀers
furtherinsightsintoTOGA’sassertionoraclesgenerationcapability.
In RQ2/u1D447, the stated overall accuracy for assertion oracle inference
is 69%, while our ﬁndings show an overall accuracy of 52%. For
exceptionaloracleinference,TOGAreported86%accuracy,while
our ﬁndings indicate a lower accuracy of 75%. Moreover, when
considering only the “exception expected” ground truth, we found
an accuracy 19%, which was not reported in the original paper.
TheseresultsindicatethatTOGAdidnotgeneralizeeﬀectivelyto
the large anddiversedataset studied.
Moreover, TOGA’s high false positive rates raise concerns about
itspracticalusefulness.Widelycitedstudies[ 12,28,45]havedemon-
strated that high false positives are a major barrier to the adoption
ofautomatedsoftwaretestinginindustryandtoolsthatgenerate
more than 10% false positives waste developers time causing devel-
operstolosetrustinthemandgraduallyabandonthem.Toimprove
TOGA-liketechniquesgoingforward,itiscrucialtosigniﬁcantly
reduce falsepositive rates. Inthis context, ourﬁndings oﬀervalu-
ableguidancebyidentifyingthespeciﬁctypesoftestoraclesand
assertionsthatpredominantlycontributetofalsepositives.Thus,
presenting potential opportunities for future reﬁnement to ensure
their usefulness in real-world scenarios. An important lesson from
this study is that future research should comprehensively evaluate
the precision andrecallof oracle generationtechniques (§ 3.5).
RQ3 (Conceptual Replication of RQ3 /u1D447):This study inves-
tigatestherelativestrengthoftheassertionoraclesgeneratedby
TOGA and EvoSuite. Our motivation for this research question
is twofold: ﬁrstly, strong assertion oracles are crucial for detect-
ingspeciﬁcationviolationsand assertionsare stronglycorrelated
with the fault-detection eﬀectiveness of a test suite [ 46,50,53,62];
secondly, constructing strongassertion oracles requires an under-
standingofprogramspeciﬁcationandTOGAisdesignedtoleverage
naturallanguagespeciﬁcations (docstrings).
TOGAleveragesEvoSuite-generatedpreﬁxes,reapingbeneﬁts
fromEvoSuite’ssearch-basedtechniquethatcreatestestinputsopti-
mizingcoverage,faultdetectionandminimizingfalsepositive/ﬂaky
tests[2,15,39,49,56].Furthermore,TOGAemploysadeeplearning
approach utilizing the powerful CodeBERT model, enabling it to
learn from large-scale open source codebases and natural language
codedocumentation(docstrings).ThisgivesTOGAthepotentialto
improve ontraditionalrule-based static techniques,which do not
utilizenaturallanguagespeciﬁcations.Welimitthisstudyto34K
test preﬁxes, from the RQ2 experiment, on which TOGA generated
non-empty and correct assertion oracles and only consider Evo-
Suiteassertionsforthosepreﬁxes.OurhypothesisisthatTOGA,
by leveraging both EvoSuite preﬁxes and a broader understanding
of the code’s intended behavior learned from codes and docstrings,
would generate strong assertion oracles capable of identifying a
signiﬁcant number offaultsnot detectedbyEvoSuite.
We employ mutation testing, a scalable and eﬀective method,
to measure the fault-detection eﬀectiveness of test assertions [ 3,5,
121Neural-BasedTestOracle Generation: A Large-Scale Evaluation andLessonsLearned ESEC/FSE ’23, December3–9, 2023,San Francisco, CA, USA
13,40,41,46,62]. From a pool of 51K injected faults, 20.5K were
detectedbytheJavaimplicitoracle.EvoSuiteassertionsdetected
an additional 9,814 faults. Finally, with the addition of TOGA’s
assertions, an additional 105 faults were detected. This suggests
that the added value of TOGA-generated assertions with EvoSuite
preﬁxesislimited,anditsusemaynotbewarrantedgiventhecosts
associatedwithits high false-positive rate(from RQ2).
Our primary contributions are (1) a series of replication studies
thatbroadentheunderstandingofTOGA’sapplicability,general-
izability, precision, fault detection power; 2) the identiﬁcation of
limitations of the latest learning-based test oracle generation ap-
proach; (3) the identiﬁcation of actionable lessons learned that can
beappliedtofuturestudiesofsuchtechniques;and(4)asubstantial
datasetconstructedbyanexternalgroup,consistingof223Ktest
casesfrom 25 large-scale applicationsfor evaluating test oracles.
2 TOGA
TOGA is an automated test oracle generation technique [ 17]. It
takes two inputs: the unit context, comprised of the method un-
dertestandassociateddocstrings,andtestpreﬁx.Thetestpreﬁx
istypicallygeneratedwithanauxiliarytestgenerator;following
the original paper [ 17], we also adopt EvoSuite. TOGA has three
majorcomponents:ExceptionOracleClassiﬁer,AssertionOracle
Generator,andAssertion Oracle Ranker.
Inthereminderofthissection,webrieﬂysummarizethefunc-
tioning androle ofEvoSuite andofTOGA’sthree components.
EvoSuite isasearch-basedunittestgenerationtoolforJava[ 19,
20]. It automatically produces a code fragment, called test preﬁx ,
thatdeﬁnestheinputsforeachgeneratedtest.EvoSuiteassumes
thattheunitundertestiscorrectinordertogeneratetestoraclesfor
each preﬁx that detect regression bugs. These oracles can take two
forms. Assertion oracles check program output against expected
output.Exceptionoraclescheckifanexpectedexceptionisthrown.
Listing1:EvoSuitetestswithassertionandexceptionoracles
public void test00() throws Throwable {
Stack<Integer> stack0 = newStack<Integer>();
Integer integer0 = newInteger(0);
stack0.push(integer0);
assertEquals(1, stack0.size()); //assertion oracle
}
public void test11() throws Throwable {
Stack<Integer> stack0 = newStack<Integer>();
try{
stack0.pop();
//exception oracle
fail("Expecting exception: EmptyStackException" );
}catch(EmptyStackException e) {
verifyException( "Stack" , e);
}
}
Listing1shows two test cases generated by EvoSuite for the
JavaStackclass.test00teststhepushmethod:insertsanelement,
and checks if the size of the stack is equals to 1 with an explicit
assertion oracle. test11calls the popmethod without pushing
anything on to the stack, therefore, an expected behavior is to
throwan Exception .Iftheexceptionisnotthrownthenthetest
willfail,whichischeckedwiththe exception oracle.
2.1 Exception Oracle Classiﬁer(EOC)
The Exceptional Oracle Classiﬁer (EOC) is a pretrained CodeBERT
[18] model trained on both natural language and code-maskedlanguage modeling and ﬁne-tuned on binary classiﬁcation. For
ﬁne-tuning,theyusedadatasetcalledMethods2Test*,acorpusof
methodcontext(c),testpreﬁx(p),andbinarylabel( 0/1).Foragiven
pairof(c,p),label 1indicatesthattheexecutionofthetestpreﬁx
should throw an exception, and label 0indicates that no exception
should be thrown.
For example, in Listing 1,test11pops from an empty stack
whichshouldthrowan EmptyStackException .Giventhistestpre-
ﬁx,itisexpectedthatEOCwillpredicta “1".Onthecontrary,given
the test preﬁx from test00in Listing 1, the expected prediction
is0,meaningthatnoexceptionshouldbethrown.Asmentioned
earlier,"no exception should be thrown" isJava’simplicitoracle.
2.2 Assertion Oracle Generator (AOG)
WhenEOC classiﬁesthat anexception should notbe thrown fora
unit context and test preﬁx, the Assertion Oracle Generator (AOG)
is invoked to generate a set of assertion candidates. Note that AOG
isanon-ML-basedalgorithm(Algorithm1in[ 17])thatgenerates
assertions based on the type of the variable being checked. It is
worth noting that the target variable is extracted from the Evo-
Suitegeneratedassertion.Basedonthevariabletype,TOGAgen-
erates ﬁve types of JUnit assertions: assertNull ,assertNotNull ,
assertTrue ,assertFalse ,assertEquals .Forexample,ifavari-
able is an Object, AOG may generate assertion candidates us-
ingassertNull() ,assertNotNull() andassertEquals meth-
ods.Similarly,forvariableswith boolean type,assertTrue() and
assertFalse() oracles can be generated.
Generating assertEquals is more complex as it requires two
values:expectedvalueandthevariablebeingchecked.Forderiving
an expectedvalue,TOGAdrawsfrom the mostfrequentlyappear-
ingconstantvaluesintheAORtrainingdata(GlobalDictionary).
For each type, this dictionary holds the top K values. Similar to
the global dictionary, they also construct a local dictionary from
the input test preﬁxes, consisting of variables and constants in
the preﬁx. We refer readers to Section 4.4 of [ 17] for more de-
tailsonthelocalandglobaldictionary.Ourexperimentalstudies
(RQ2)showthatTOGAmostlyuses 0or1fortheexpectedvalue
inassertEquals for numerical domains. Out of the 16059 false
positiveassertEquals oracles generated by TOGA, 14913 (93%)
assertionsusedeither 0or1asexpectedvalue.Only7%oraclesused
someothervariablesfromthetestpreﬁxastheexpectedvalue.Con-
sequently,thistypeofassertionresultsinalargenumberoffalse
positives (73% FPR). For example, in Figure 1, fortest01, TOGA
generated an incorrect assertion by comparing stack size with 0
when the expectedvalueshould be 1.
2.3 Assertion Oracle Ranker (AOR)
Like EOC, theAssertionOracle Ranker(AOR)is alsoapretrained
CodeBERT[ 18]modeltrainedonbothnaturallanguageandcode-
masked language modeling, however, ﬁne-tuned on ranking tasks
insteadofbinaryclassiﬁcation.Forﬁne-tuning,asuperviseddataset,
Atlas*,wasused.Atlas*isacorpusofmethodcontext(c),testpreﬁx
(p), assertion (a),and a binary label ( 0/1).As this is a rankingtask,
for a pair of (c,p), only one assertion in the candidate set will have
a binary label “1"’, indicating the most preferred assertion from
the candidates. The restof the assertionswill be labeledas “0"’for
that given pair of(c,p).
122ESEC/FSE ’23, December3–9, 2023,San Francisco, CA, USA SoneyaBintaHossain, Antonio Filieri,Ma/t_thewB.Dwyer, Sebastian Elbaum,andWillem Visser
//true positive TOGA assertion
public void test03() throws Throwable {
Stack<Object> stack0 = new
Stack<Object>();
boolean boolean0 = stack0.isEmpty();
assertTrue(boolean0); //ES oracle
}// 0, assertTrue(boolean0)///false positive TOGA assertion
public void test01() throws Throwable {
Stack<Object> stack0 = new
Stack<Object>();
Integer integer0 = newInteger(1920);
stack0.push(integer0);
intint0 = stack.size();
assertEqual(1, int0); // ES oracle
}/// 0, assertEquals(0, int0)public void test05() throws Throwable {
Stack<Object> stack0 = new
Stack<Object>();
try{
stack0.peek();
fail();
}catch(NoSuchElementException e) {
verifyException( ""Stack"", e);
}
}// 1, correct classificationpublic void test06() throws Throwable {
Stack<Object> stack0 = new
Stack<Object>();
try{
stack0.pop();
fail();
}catch(NoSuchElementException e) {
verifyException( ""Stack"", e);
}
}/// 0, incorrect classification
Figure1:TestOracleGeneratedbyTOGAforStackclass.For test03,acorrectassertioningenerated;for test01,afalsepositive
assertionis generated; for test05, TOGAcorrectlypredicts that exception is expected; for test06, TOGAincorrectlypredicts
no exception,whenitshould.
During assertion oracle inference, for an input (c, p, a), AOR
predictsabinarylabelandassignsaconﬁdencescore.Basedonthe
labelandachievedconﬁdencescore,thehighest-rankedassertion
will be selected as the output. The model does not output any
assertion oracle when it is not conﬁdent enough; in our study,
TOGA did not generate any assertion for more than 62% of the
correctlyclassiﬁedtest preﬁxes.
2.4 SampleTestOraclesGenerated by TOGA
InFigure 1,weprovideafewexamplesoftheTOGA-generatedtest
oraclesforthe Stackclass.UsingEvoSuite,wehavegenerated13
testcases,11withassertionoraclesandtwowithexceptionoracles.
Wehaveusedthemethodundertest,itsdocstrings(availableforall
methods),andthetestpreﬁxestopredicttestoracleswithTOGA.
TOGA generated four assertion oracles, two correct (e.g., test03)
and two false positive assertions (e.g., test01). For the remaining
seven out of the 11 test preﬁxes, TOGA correctly classiﬁed that
they should notthrow any exception; however, it did not generate
any assertions. Out of the two test preﬁxes with an EvoSuite ex-
ception oracle, TOGA classiﬁed one correctly (e.g., test05) and one
incorrectly (e.g., test06). For both assertion and exception oracle
inference,false positive rateis50%.
2.5 TOGA Original Findings
The originalTOGA study includedthree researchquestions.
RQ1/u1D447evaluated whether TOGA’s grammar represents most
developer-writtenassertionsornot.Theyfoundthat82%developer-
written assertions from the ATLAS dataset [ 58] can be represented
withtheirgrammar.
RQ2/u1D447evaluatedTOGA’soracleinferenceaccuracyonheld-out
testsets. For exceptionoracleinference, theyusedMethods2Test*
dataset [17], and TOGA achieved 86% accuracy, 55% precision, 30%
recall, and .39 F1-score. For assertion oracle inference, the Atlas*
[17] datasetwas used,and TOGAachieved96% in-vocabaccuracy
and69%overallaccuracy.
RQ3/u1D447evaluatedTOGA-generatedoraclesfault-detectioneﬀec-
tivenesson Defects4j faultdatabase.Using364testpreﬁxesgen-
erated on the ﬁxed versions, TOGA detected 57 out of the 835
Defects4j bugs, where ﬁve were detected by exception oracle, 14
weredetectedbyassertionoracle,and38weredetectedbyEvoSuite
preﬁxesthrowinguncaughtexceptions.EvoSuitedetected120bugs.
3 EXPERIMENTALSTUDY
We investigate the following researchquestions:
Figure 2:TOGAoracle inferenceof57 Defects4j bugs
RQ1 (Exact Replication of RQ3 /u1D447):How many of the bugs
reported in the original study could be exclusively detected by
TOGA’s explicit assertion and exception oracles and how many
have been detected by implicit oracles, i.e., the uncaught exception
thrownbyEvoSuite preﬁxes(e.g.,adereferencednull pointer)?
RQ2(ConceptualReplicationofRQ2 /u1D447):HowpreciseisTOGA
in classifying the type of oracle required and in generating correct
assertion oracles?
RQ3(ConceptualReplicationofRQ3 /u1D447):Whatistheadded
value of TOGAgenerated assertionsin detecting faults relative to
the state-of-thepractice?
3.1 RQ1(Exact Replication ofRQ3 /u1D447)
Thisresearchquestioninvestigatesthe57 Defects4j bugsreported
asdetectedbyTOGAintheoriginalstudy.Tothisend,weobtain
the original paper replication package [ 16], run the experiments
asindicated, andexamined thedetectedbugs andthe oraclesthat
catch themusing our owntoolsandscripts.
3.1.1 Original Artifacts and Procedure
The bug-detection eﬀectiveness of the TOGA-generated test ora-
cles was evaluated on the Defects4j benchmark, a dataset of real
bugs[29],consistingof17Javaapplicationscontainingatotalof
835labeledbugs.Foreachbug,thedatasetkeepsboththebuggy
andﬁxedversions.Eachbugislabeledwithauniquebugid,and
the ﬁxed and buggy versions for that bug can be identiﬁed and run
onasetoftest caseseﬃciently.
TOGA’s bug detection capabilities have been investigated using
the following protocol[ 17]:
(1)RunEvoSuite ontheﬁxedversionsoftheprogramsforthree
minutesto generatetest cases;
(2)RunEvoSuite -generatedtests on the buggy versions and keep
123Neural-BasedTestOracle Generation: A Large-Scale Evaluation andLessonsLearned ESEC/FSE ’23, December3–9, 2023,San Francisco, CA, USA
Table 1:Defects4j bug detection(Bug ID: JxPath5) by TOGA.TOGA correctly predictedthat “no exceptionis expected” (note
thatthisisan“implicitassertion”,whichisthedefaultassumptionofarun-timesystemandJUnittestrunner).However,it
generatedonefalsepositiveassertionandnoassertionfortherest.RunningtheEvoSuitepreﬁxaloneonthebuggyversion
detected abugas thetestfailed due to uncaught exceptionthrownby theEvoSuite preﬁx.
EvoSuiteTest TOGAInput 1 TOGAInput 2 TOGAInput 3
public void test48() throws Throwable {
/** EvoSuite prefix: **/
QName qN0 = newQName("");
VariablePointer vP0 = newVariablePointer(qN0);
NodePointer nP0 = NodePointer.newChildNodePointer(vP0,
qN0, qN0);
BasicVariables bV0 = newBasicVariables();
VariablePointer vP1 = newVariablePointer(bV0, qN0);
intint0 = nP0.compareTo(vP1);
/** end of EvoSuite prefix **/
assertEquals(1, int0);
assertEquals(Integer.MIN_VALUE, vP1.getIndex()); }public void test48() throws
Throwable {
/** EvoSuite Prefix **/
assertEquals(1, int0);
}public void test48() throws
Throwable {
/** EvoSuite Prefix **/
assertEquals(Integer.MIN_VALUE,
vP1.getIndex());
}public void test48() throws
Throwable {
/** EvoSuite Prefix **/
}
TOGAOracle Inference: assertEquals(0, int0) Empty Empty
Aggregated Test:public void test48() throws
Throwable {
/** EvoSuite Prefix **/
assertEquals(0, int0)}public void test48() throws
Throwable {
/** EvoSuite Prefix **/ }public void test48() throws
Throwable {
/** EvoSuite Prefix **/ }
Test Execution Status: FP: Failed onFixedTP:Passed onFixed,Failed on
BuggyTP: Passed on Fixed, Failed on
Buggy
recordsofthefailedtestcases.Thesetestsarecalled“bug-reaching
tests” as they failed on the buggy versions, indicating that they
exercisedbuggy behavior.
(3) Separate test preﬁx and assertions in the “bug-reaching tests”
test cases. When a bug-reaching test contains more than one as-
sertion, it is replicated into multiple tests composed of a single
assertionandatestpreﬁxthatcomputesthevariablecheckedby
the assertion. For example, from the EvoSuite test case with two
assertions shown in Table 1, three test cases were generated. In
total364test preﬁxeswere usedfor this study.
(4)TOGAprocessestestpreﬁxesand EvoSuite assertions(todeter-
mine the variable to be asserted) and generates oracles for each of
the preﬁxes. TOGA oracles can be assertions or exception oracles,
where the test preﬁx iswrappedwithin atry-catch block.
(5) TOGA tests ( EvoSuite + TOGA assertion) are executed on both
ﬁxedand buggyversion,whereabugis classiﬁedasdetectedifthe
test passesonthe ﬁxedversionwhilefailingonthe buggy one.
Toconductthis replication,we followedthe sameprotocol,uti-
lizedtheidenticalsetofEvoSuitetestpreﬁxesprovidedintheTOGA
replication package.
3.1.2 Results
IntheSankeydiagramshowninFigure 2,wereporttheanalysisof
all 57 bug detection reported by TOGA. We ﬁrst categorize the 364
TOGA inputs based on the expected oracle types: exception and
assertionoracles.Ofthe60TOGAinputsconsistingoftestpreﬁxes
expected to throw an exception, 53 were misclassiﬁed, resulting
in an 88% false positive rate. The 7 correctly classiﬁed exception
oracles found5distinct bugs.
For the remaining 304 inputs (bottom left branch of diagram),
TOGA correctly classiﬁed 261 preﬁxes that an exception is not
expected, and 43 are misclassiﬁed. For 140 test preﬁxes, TOGA did
not generate any explicit assertion oracles. For 121 preﬁxes, TOGA
generated an explicit assertion oracle, only 58 of them are true
positive and they detected 14 unique bugs. Rest of the generated
assertions (63), either failed on the ﬁxed versions (FP + FN) or
passedonthe buggy version(TN).67% (38 of 57) of the bugs reported as detected, were found
by implicit assertions through a TOGA oracle expressing that “a
test should fail if no exception is predicted but is thrown when
executing the test on the buggy version”. This is a default oracle of
theJavarun-timesystemanddefaultbehavioroftheJUnit(used
byDefects4j ) test framework that a test must fail on uncaught
exception. Therefore, these 38 out of the 57 bugs would still be
detected by Java run-time system by simply running the EvoSuite
test preﬁxeswithoutany TOGA-generatedoracle.
In Table1, we provide an example of how TOGA detected a bug
intheJxPathapplication.Theﬁrstrowshowsthetestgenerated
byEvoSuite and three TOGA inputs generated from the same test
preﬁx:oneperassertionandonecontainingonlythepreﬁx.Note
thatTOGAutilizesthe EvoSuite assertstatementtoidentifythe
variableforwhichtogenerateassertions.Thesecondrowshows
the output from TOGA. For all three inputs TOGA correctly pre-
dictedthatnoexceptionisexpected(implicitoracle),however,it
generated only one assertion for input 1. For inputs 2 and 3, TOGA
didnotgenerateanyexplicitassertionoracle.Whenrunningtheag-
gregatedtests(thirdrow),TOGAtest1failedintheﬁxedversionas
it is an incorrect assertion. For TOGA tests 2 and 3, only EvoSuite
test preﬁxeswere runonbothversions (ﬁxed and buggy),and the
bugwasdetected.Thisisoneofthe38bugsthatcanbedetected
withoutany TOGA oracles, Java implicitoracle suﬃces. This eval-
uationsettinghastwonegativeconsequences:1)itoverestimates
TOGA’s fault detection capability relative to the state-of-the-art
considering that 67% bugs can be detected by the standard implicit
test oracle, and 2) it does not control for the fact that other tech-
niques that do not explicitly generate the “No Exception” oracle
could be misrepresentedwiththis protocol.
For instance, seq2seq, which utilizes the same EvoSuite preﬁxes,
should theoretically be capable of detecting these 38 implicitly
detectedbugs,unlessthegeneratedassertionsfailedontheﬁxed
versionoftheprogramandsohavebeenclassiﬁedasfalsepositives.
Similarly,JDoctorwhichalsousesthesameEvoSuitepreﬁxes,
shouldalsodetectthese38bugs,unlessitgeneratesalargepercent-
age of false positives. However, according to RQ3 /u1D447, FPR is only .4%
124ESEC/FSE ’23, December3–9, 2023,San Francisco, CA, USA SoneyaBintaHossain, Antonio Filieri,Ma/t_thewB.Dwyer, Sebastian Elbaum,andWillem Visser
(2/364testsarefalsepositives).Therefore,JDoctor,intheory,should
alsodetectthesebugs.Anexactreplicationofthesemethodswas
not possible because neither the TOGA paper nor its replication
package provided data regarding how these tools were run, e.g.,
time spent running each technique, the preparation of inputs, and
toolconﬁgurations.
Thetotalnumberofmisclassiﬁcation(total:96)andgeneration
ofincorrectassertionsfailingonﬁxedversions(total:49)sumto
40% of the 364 tests preﬁxes in the original TOGA study. These
tests may fail when they should not and it would require engineer
time to triage, diagnose, and repair the tests. This cost might be ac-
ceptableifTOGAwereableto generatevaluableassertions,butfor
54% (140/261) of correctly classiﬁed test preﬁxes, TOGA generated
no assertions and the faults detected from TOGA generated asser-
tions comprise only 19 of the 57 reported in the original paper. We
investigatethe precisionandvalueofTOGA generatedassertions
further inRQ2 andRQ3.
RQ1 Findings: Out of the 57 bugs reported in the original
study,5weredetectedbyexceptionoracles,14weredetectedby
explicit assertion oracles generated by TOGA, and 38 were due
to uncaught exceptions thrown by EvoSuite-generated preﬁxes
that can be detected by the run-time system (implicit oracle)
withoutrequiring any TOGA-generatedoracles.
3.2 RQ2(Conceptual Replication ofRQ2 /u1D447)
In this research question, we investigate the ability of TOGA to
generate non-trivial and precise oracles on a large set of programs
andgeneratedoracles.
3.2.1 New Artifacts
Westudy25large-scaleopen-sourceJavaapplicationsfromGitHub
andApacheCommonsProper[ 43].8oftheartifactscomprisetheof-
ﬁcialEvoSuitebenchmark[ 9]and17wereselectedfromtheApache
Commons packages. We use the 8 EvoSuite artifacts because: (i)
they have several thousand stars and users (min: 3.3k and max:
9.8K) on GitHub attesting to their popularity and adoption among
developers, (ii) many researchers have studied them to evaluate
testadequacymetrics,fault-detectiontechniquesandautomated
test/oracle generation methods [ 29,46,62], and (iii) they have a
large code base with multiple modules and thereby better reﬂect
real-world software complexity. For our study, we use the latest
stable release of these artifacts as of Sep 30th, 2022.The Apache
CommonsarepopularJavautilitypackages,frequentlyusedinsoft-
ware engineeringempirical studies [ 29,46,62]andhaveactively
maintainedlarge-scalecodebasesandtestsuites.Ofthe43Apache
Commons packages, 22 are Java 8 compatible, a prerequisite for
thelatestEvoSuite.EvoSuitewasunabletogeneratetestsfor5of
those,leaving17packagesforourstudy..WeuseOpenJDK8torun
EvoSuite,Maven(3.6.3)tobuildJavaclasses,JUnit(4.12)toexecute
test suites, and TOGA replication package [ 16] to generate oracles.
3.2.2 New Procedure
TOGAinputpreﬁxesrequirefollowingaspeciﬁcpatternwithex-
actly one assertion at the end, and the variable under test is ex-
tracted from that assertion. EvoSuite’s test format allows TOGAto easilyparse and decomposelarge testsinto multiple singleas-
sertion tests. Due to this reason, we also generate EvoSuite tests
instead ofsuing the developer written tests.
GeneratingGroundTruth. Weneedtogeneratethegroundtruth
to determine whether a test oracle generated by TOGA is a false
positive. To this end, for all artifacts, we download the latest stable
releases(showninTable 2)withnoknownfaults,meaningthatthe
programs’ implementations are correct. EvoSuite is a regression-
basedtechniquethatassumestheimplementationoftheprogram
under test as correctand generates test oracles based on the exe-
cutedbehavior.Therefore,weconsideredtheEvoSuite-generated
tests as the ground truth to detect the false positive oracles gener-
atedbyTOGA–thisistheapproachtakenintheoriginalTOGA
study.Weallocatesixminutesperclassfortestgenerationasrec-
ommendedbytheauthorsofEvoSuite[ 20].Inalarge-scalestudy,
EvoSuitedevelopersandotherresearchers[ 49],foundthatEvoSuite
occasionallygeneratesnon-compiling(4%)andﬂaky(3.4%)tests,
however,nofalsepositivesaregeneratedbyEvoSuite.Wealsoﬁnd
the sameand following the samerecommendationsfrom [ 49], we
detect and remove non-compiling and ﬂaky tests and report the
totaltest casesper artifact inTable 2.
Listing 2:False Positive Assertion
public void test1() throws Throwable {
Locale locale0 = newLocale( "TZea6h)b" ,"LE$&{r\f+E=b+Uz}rR" ,
"TZea6h)b" );
Locale locale1 = Locale.KOREAN;
List<Locale> list0 = LocaleUtils.localeLookupList(loca le0,
locale1);
assertEquals(4, list0.size()); /*EvoSuite Assertion*/
///AssertionError: expected:<1> but was:<4>*
assertEquals(1,list0.size()); ///false positive assertion by TOGA
}
Listing 3:Type Incorrect Assertion
public void test1() throws Throwable {
MutableLong mutableLong0 = newMutableLong();
MutableLong mutableLong1 = newMutableLong();
mutableLong1.incrementAndGet();
boolean boolean0 = mutableLong0.equals(mutableLong1);
assertEquals(( short)1, mutableLong1.shortValue()); /*EvoSuite
Assertion*/
///incompatible types: short cannot be converted to boolea n
assertTrue(mutableLong1.shortValue()); ///TOGA assertion with
type error"
}
Generating TOGA Oracles. Following a similar procedure as
TOGA, we split test cases with multiple assertions into multiple
testcaseswithasingleassertionandatestpreﬁxthatcomputesthe
variablecheckedintheassertion. Wecompileandexecutethesetest
casestoconﬁrmthatalldecomposedtestssuccessfullycompileand
pass,resultingin223,557 test caseswith eitheranassertion oracle
oranexceptionoracle.Finally,weconstructinputsforTOGA(focus
method,testpreﬁx,doc-string)andgenerateoraclepredictions.We
constructTOGAgeneratedtestcaseswithEvoSuitetestpreﬁxes
and TOGA-generatedoraclesand execute the test cases to count
thefalsepositives.Wecategorizetheinputtestpreﬁxesbasedon
thetypeoforacleexpected(assertionorexception)andpresentour
ﬁndingsinTable 3.
3.2.3 Results
InTable3,theﬁrstcolumnshowstheartifactname,andthesecond
column shows the total test input preﬁxes processed by TOGA.
Columns 3-7representTOGA predictionresults when the ground
125Neural-BasedTestOracle Generation: A Large-Scale Evaluation andLessonsLearned ESEC/FSE ’23, December3–9, 2023,San Francisco, CA, USA
Table 2:Overview ofArtifact Descriptions andAssociated Metrics:SLOC,JavaDoc,and Test Size.
Artifact (version) DescriptionProgram
Size(SLOC)JavaDoc (L)Test Size
(SLOC)Test
Case(#)
JSON(20220924) JSON library forJava 4,220 3,549 27,351 883
async-http-client(2.12.3) Asynchronous HTTP library 16,299 3,288 69,889 1,534
bcel(6.5.0) BytecodeEngineeringLibrary 35,571 15,569 263,796 7,073
commons-beanutils(1.9.4) ReﬂectionandIntrospectionAPI 11,725 15,074 29,939 1,654
commons-collections4(4.4) Utilities forJavaCollections Framework 6,697 6,532 2,149 576
commons-conﬁguration2(2.8.0) Utilities forreadingconﬁguration ﬁles 4,553 4,882 2,858 1,764
commons-dbutils(1.7) JavautilityforJDBCdevelopment 3,079 4,158 10,144 479
commons-geometry (1.0) Geometric types andutilities 14,168 12,195 48,862 4,274
commons-imaging (1.0-alpha3) Javaimagelibrary 32,275 5,003 103,120 4,997
commons-jcs3(3.1) Cachingsystem 14,328 6,045 33,542 705
commons-jexl3(3.2.1) ScriptingutilitiesforJavaapplication 2,295 2,267 5,613 3,819
commons-lang3(3.12.0) Javahelperutilities 5,321 6,671 8,131 5,816
commons-net(3.8.0) Network utilities/protocolimplementations 19,407 16,574 42,841 2,701
commons-numbers(1.0) Javautilityfornumbertypes 5,502 5,566 56,339 1,270
commons-pool2(2.11.1) ObjectPoolingLibrary 1,197 1,335 2,688 749
commons-rng(1.4) Pseudo-randomgenerators 9,654 161,553 16,241 1,359
commons-validator(1.7) Client andserversidedatavalidation 7,829 7,338 22,352 1,737
commons-vfs(2.9.0) Virtual File Systemlibrary 7,751 4,966 13,890 1,181
commons-weaver(2.0) Utilityto enhance compiled Javaclasses 4,527 1,546 14,462 310
http-request(6.0) Library formakingHTTP requests 1,395 1,511 10,450 208
joda-time(2.11.2) Date andtime library 32,312 31,127 156,345 7,111
jsoup(1.15.3) Javalibrary forHTML 13,905 4,627 186,371 2,974
scribejava(8.3.1) OAuth library 2,173 1,303 14,094 1,019
spark(2.9.3) Framework forcreating webapplications 6,124 4,361 39,273 1,429
springside4(5.0.0-SNAPSHOT) JavaEEapplication reference architecture 9,336 4,387 33,357 2,246
Total: 271,643 331,427 1,214,097 57,868
truth is“assertionoracle”,meaningthatTOGA should predict “no
exception” and generate an assertion oracle for that test preﬁx.
Columns8-9presentresultswhenthegroundtruthis“exception
oracle”, meaning that TOGA should predict that the test preﬁx
throws an exception.In total,we evaluateTOGAon223,557pre-
ﬁxes;ideally,TOGAshouldgenerateanassertionoraclefor202,475
ofthepreﬁxes,andpredictanexceptionoraclefortheremaining
21,082preﬁxes.
TheﬁrststepforTOGAistopredictwhethertheexecutionof
a test preﬁx should throw an exception or not. Our study shows
that 18.3% (column 4) of the assertion preﬁxes are misclassiﬁed
and 81.7% test preﬁxes are correctly classiﬁed by TOGA for a total
misclassiﬁcationrateof24.1%.For62%oftheassertiontestpreﬁxes,
TOGA could not generate an assertion oracle (column 5) and for
38%, an assertion was generated (column 6). Out of the 62k test
preﬁxeson which TOGA generated an assertion, 47.5%(column 7)
of them were false positive – assertions that failed when combined
with the test preﬁx and run on the original program. The false
positive rate for TOGA generated assertions was as high as 73%
– for the Apache commons-numbers package. Listing 2shows an
common example of a false positive assertion generated by TOGA
involving an assertEquals with an incorrect value predicted. We
also encountered assertions that do not compile due to “incompati-
bletypes"errorsbecauseTOGAgeneratedtypeincorrectassertions,
anexampleofwhichisshowninListing 3.Whilethislatterclass
of incorrect assertion is easier to ﬁlter out, it only represented 563
ofthe more than29Kfalse positive assertions inthe study.In Table 4, we show the total number of each type of asser-
tionsgeneratedbyTOGAandthecorrespondingfalsepositiverate.
The highest false positive rate is generated for assertEquals . We
conjecture thatTOGAstrugglestogeneratethistypeofassertion
because it requires a second value, the expected value, to compare
withthevariablebeingchecked.TOGAcollectsthemostfrequently
appearingconstantsandvariablesfromthetestpreﬁxandduring
the training of the model, which appears to not be an eﬀective
strategy based on the high false positive rates. TOGA uses the val-
ues0and1veryfrequentlyresultinginfalsepositiveslikethose
shown in Figure 1, Table1, and Listing 2. The second highest false
positive rate is for assertTrue oracles. The lowest false positive
is achieved for assertNotNull , however, this type of oracle has
limited fault detection power [ 62]. TOGA did not generate any
assertNull assertions inour experiments.
Forexceptionoracleprediction,TOGA’smisclassiﬁcationrate
is 81% on average and it has reached as high as 94% for some
artifacts.Exceptionoraclesareessentialintestingtoensurethat
an exception should be thrown when test inputs trigger defensive
programming, such as the checking of preconditions at runtime.
Forexample,whena popoperationisperformedonanemptystack,
anEmptyStackException should be thrown.
Thehighratesofmisclassiﬁcationandfalsepositiveassertions
found in this study suggest that use of TOGA is impractical for use
onreal-world software at present.
126ESEC/FSE ’23, December3–9, 2023,San Francisco, CA, USA SoneyaBintaHossain, Antonio Filieri,Ma/t_thewB.Dwyer, Sebastian Elbaum,andWillem Visser
Table 3:BreakdownofTOGA’sTest OracleGenerationPerformance(AO:Assertion Oracle,EO:ExceptionOracle)
ArtifactTotal
Test
Preﬁx (#)GroundTruth:AO GroundTruth:EO
Correctly Classiﬁed
Total
Assertion
Preﬁx (#)Mis-
Classiﬁed (%)No
Assertion
Predicted (%)AssertionPredicted
Total(%)False
Positive(%)Total
Exception
Preﬁx (#)Mis-
Classiﬁed (%)
JSON-java 13,995 13,794 13% 49% 38% 51% 201 82%
commons-conﬁguration2 1,519 1,000 14% 40% 46% 44% 519 71%
spark 6,194 5,637 35% 57% 9% 39% 557 78%
commons-geometry 5,516 4,244 3% 64% 33% 56% 1,272 89%
http-request 7,074 7,052 33% 50% 17% 18% 22 50%
commons-collections4 1,589 1,338 8% 41% 51% 31% 251 66%
springside4 4,814 3,858 9% 58% 33% 44% 956 78%
commons-rng 1,804 1,352 22% 63% 14% 65% 452 70%
commons-vfs 1,423 997 6% 46% 48% 49% 426 74%
commons-numbers 41,412 41,163 36% 61% 4% 73% 249 47%
commons-lang3 14,918 13,455 9% 59% 32% 38% 1,463 71%
commons-pool2 12,134 11,961 36% 37% 27% 54% 173 67%
commons-beanutils 1,780 1,036 9% 47% 44% 50% 744 69%
commons-validator 2,659 2,261 4% 39% 57% 48% 398 83%
jsoup 22,721 21,844 1% 36% 63% 44% 877 88%
commons-weaver 284 187 11% 50% 39% 42% 97 77%
commons-net 3,865 2,462 3% 34% 63% 54% 1,403 86%
async-http-client 2,500 2,008 2% 48% 50% 37% 492 94%
commons-jexl3 4,275 3,219 5% 37% 58% 62% 1,056 76%
commons-jcs3 5,860 4,861 3% 37% 59% 56% 999 84%
commons-dbutils 818 635 2% 42% 56% 38% 183 93%
bcel 24,049 20,657 5% 60% 35% 47% 3,392 83%
commons-imaging 10,731 8,397 5% 64% 31% 58% 2,334 88%
joda-time 30,527 28,408 25% 43% 32% 46% 2,119 84%
scribejava 1,096 649 5% 26% 69% 28% 447 81%
Total (average %): 223,557 202,475 36,949 (18.3%) 102,606(62%) 62,920 (38%) 29,883 (47.5%) 21,082 17,149 (81%)
Table 4: Assertion Oracles by TOGA and Their Associated
FalsePositive Rates.
Assertion Type TotalFalse Positive
assertTrue 17,025 9,543(56%)
assertFalse 5,375 1,864(34.7%)
assertNull 0 0(0%)
assertNotNull 18,785 1,854(9.9%)
assertEquals 21,735 16,059 (74%)
Total: 62,920 29,320 (47%)
RQ2 Findings: TOGA misclassiﬁes the type of oracle required
foratestpreﬁx24%ofthetime.Whenitcorrectlypredictsthat
anassertionisrequired,62%ofthetimeitfailstogeneratean
assertionandwhenitdoesgenerateanassertion,nearlyhalf
ofthose,47%,are false positive.
3.3 RQ3(Conceptual Replication ofRQ3 /u1D447)
Assertion oracles playa critical rolein detecting functional bugs
causedbyincorrectimplementationsandarehighlycorrelatedwith
thefault-detectioneﬀectivenessofatestsuite[ 46,62].Duetotheir
importance, this research questioninvestigates the fault-detection
eﬀectiveness ofTOGA-generatedassertions relative to EvoSuite.We address several limitations of the TOGA Defects4j study
(RQ3/u1D447) and carefully control our experimental setup to ensure a
fair comparison with EvoSuite. First, RQ3 /u1D447only considered bug-
reaching EvoSuite test preﬁxes, which limited TOGA’s ability to
detect bugs outside those preﬁxes. Our study considers all preﬁxes
allowingTOGAtoexploititspotentialtodetectfaultsthatEvoSuite,
despite having the capability to reach them, fails to detect with
its own assertions. Second, we explicitly control for the faults that
aredetectedbyJavaimplicitoracleandsolelyfocusonthefaults
detectedbytest assertions.
To ensure a fair comparison, we take several measures. For both
EvoSuite and TOGA, we onlyconsider the test cases on which
TOGAgenerated non-empty andcorrectassertionoraclesduringour
RQ2experiment. Thissetofassertionoraclesrepresentsavariant
of the ground-truth assertion oracles generated by EvoSuite, as
they share the same test preﬁxes, test the same variables, and pass
successfully on the program version from which they have been
generated,therebymirroring thecurrentprogrambehaviormuch
like EvoSuite. Therefore, EvoSuite and TOGA both have the exact
samesetofpreﬁxes,andanequalnumberoftestassertions.The
diﬀerence between the test suites are the types and strength of the
assertions.
127Neural-BasedTestOracle Generation: A Large-Scale Evaluation andLessonsLearned ESEC/FSE ’23, December3–9, 2023,San Francisco, CA, USA
EvoSuitepreﬁx:
public void test22() throws Throwable {
Character character0 = Character.valueOf( '8');
charchar0 = CharUtils.toChar(character0, '(');}
EvoSuitepreﬁx +EvoSuiteassertion:
public void test22() throws Throwable {
Character character0 = Character.valueOf( '8');
charchar0 = CharUtils.toChar(character0, '(');
assertEquals( '8', char0);}
EvoSuitepreﬁx +TOGAassertion:
public void test22() throws Throwable {
Character character0 = Character.valueOf( '8');
charchar0 = CharUtils.toChar(character0, '(');
assertNotNull(char0);}
Figure 3: Test cases across suites: same preﬁxes, diﬀerent
oracles.
3.3.1 New Artifacts
This researchquestion includes all 25artifacts studied inRQ2. We
generate variations of each program using mutation testing, which
injects minor code modiﬁcations (mutations), e.g., altering con-
ditional predicates and arithmetic operators, to deviate from the
intended behavior of the original program. A modiﬁed program
is called a mutant, and a mutant is killedif any test fails when
running on it, thus detecting the change. A limitation of the TOGA
Defects4J study is that tests are generated on ﬁxed program ver-
sions – where the bugs the test oracles aim to detect have been
ﬁxed – and then those test preﬁx+TOGA-generated oracles are
executed on the buggy versions to catch those same bugs. This
isunrealisticsinceﬁxedprogramsarenotavailablewhentesting
buggyversions.Mutationtestingoﬀersanalternativethathasbeen
showntohave astatisticallysigniﬁcantcorrelationwith realfault
detection[ 3,30,40],andisbeingincreasinglyusedinindustry[ 42].
This made it the method of choice in previous studies aimed at
measuring thefault detection eﬀectivenessof test oracles[ 46,62],
which is also the goal of this study. Including both real bugs (RQ1)
andmutantsoflarge-scaleapplications(RQ3)providesabroader
perspective on the replication of TOGA and its evaluation method-
ology.
Inourresearch,weusePIT,astate-of-the-artmutationtesting
tool for the Java programs [ 13]. PIT is compatible with test frame-
works like JUnit and can be easily integrated into development
environments[ 14].Furthermore,severalstudiessuggestedthatPIT
ismoreeﬀectivethanmanyotherexistingmutationtestingtools
in assisting the generation of strong tests, meaningful mutants,
and a lower number of equivalent mutants [ 31,33,44]. We use
thelatestmavenpluginforPIT-1.9.8and,asrecommendedbythe
developer of the tool, we only used strongmutators (the rest of
the PITparametersare setto theirdefaultvalues).
3.3.2 New Procedure
Toevaluatefaultdetectioneﬀectivenesswhilecontrollingforthe
numberofassertionsgeneratedacrosstechniques,weexcludeall
testpreﬁxesforwhichTOGAdidnotgenerateanyassertionoracles,
generated a false positive assertion, or test preﬁxes with exception
oracles.Then,foreachartifact,wegeneratethreediﬀerentversions
oftestsuites–TS1,TS2,andTS3–ofthesamesizecontainingthose
same set of preﬁxes, but with diﬀerent assertion oracles. (Figure 3
showsasample test from eachsuite.)
•TS1(EvoSuite preﬁxonly):each test case contains an EvoSuite-generated test preﬁx detect-
ingfaultsthroughimplicitoracles, e.g.,uncaught exception.
•TS2 (EvoSuite preﬁx + EvoSuite assertion): each test case contains
anEvoSuite-generatedtestpreﬁxandasingleEvoSuite-generated
assertion oracle for the preﬁx.
•TS3 (EvoSuite preﬁx + TOGA assertion): each test case contains
an EvoSuite-generated test preﬁx and a single TOGA-generated
assertion oracle for the preﬁx.
First,werunthetestpreﬁxesfrom TS1onthesetofbuggyprograms
(mutants)tocatchbugswithoutanyexplicitassertions.Second,we
run the tests from TS2to record how many additional bugs can
be detected by EvoSuite assertions. Third, we run TS3to detect
more faults using TOGA-generated assertions. This would allow
ustoevaluatetheaddedvalueofTOGAassertionsoverEvoSuite.
Notably, since EvoSuite’s tests are part of TOGA’s input, it is far to
assume they are available at no extra cost whenever TOGA is used.
3.3.3 Results
Table5reportsthedataforthestudy.Foreachartifact(column1)
it provides the number of tests (column 2), number of generated
mutantsexecutedbyatleastonetest(column3),andthenumberof
mutantsdetectedbythediﬀerenttestsuites:byimplicitchecksin
theruntimesystem(column4),byEvoSuiteassertions(excluding
thosealreadydetectedbyimplicitchecks,i.e.,beforereachingthe
assertion)(column5),andbyTOGAassertions(again,excluding
thosedetectedbyimplicitchecks)(column7).Webreakoutthedata
forEvoSuiteandTOGA,toreportthemutantsuniquelydetectedby
EvoSuite assertions (column 6), and the mutants uniquely detected
byTOGA assertions (column 8).
AsshowninTable 5,wehavegeneratedmorethan51,000faulty
programs using mutation testing, with 20,597 of those detected
withoutanyexplicitassertionoracles.WeuseTS1todetectthose
mutants. When adding EvoSuite-generated assertions to pair with
theEvoSuitetest preﬁxes(TS2), an additional 9,814mutantswere
detected including 3,026 unique ones not detected by TOGA asser-
tions. TS3, EvoSuite preﬁxes with TOGA assertions detected 6,893
mutants, with105being distinct from the ones foundbyTS2.
Listing4: Fault detectedby EvoSuite assertion and missed by
TOGA
private HttpURLConnection createConnection() {
try{
finalHttpURLConnection connection;
if(httpProxyHost != null)
connection = CONNECTION_FACTORY.create(url, createProx y());
else
connection = CONNECTION_FACTORY.create(url);
connection.setRequestMethod(requestMethod); ///fault: method
call removed
return connection;
}catch(IOException e) {
throw new HttpRequestException(e);}
}
public void test1280() throws Throwable {
URL uRL0 = MockURL.getHttpExample();
HttpRequest httpRequest0 = HttpRequest.options(uRL0);
String string0 = HttpRequest.CONTENT_TYPE_FORM;
HttpURLConnection htCon = httpRequest0.getConnection() ;
assertEquals( "OPTIONS" , htCon.getRequestMethod()); /* Fault
Detected By EvoSuite Assertion*/
assertNotNull(htCon.getRequestMethod());} ///Fault Missed by TOGA
Assertion
128ESEC/FSE ’23, December3–9, 2023,San Francisco, CA, USA SoneyaBintaHossain, Antonio Filieri,Ma/t_thewB.Dwyer, Sebastian Elbaum,andWillem Visser
Table 5:Fault-Detection PerformanceofEvoSuite vs.TOGAAssertions.
Artifact Tests(#)Generated
Mutants (#)MutantDetectedby
Implicit
Oracle (#)EvoSuite
Assertion(#)EvoSuite
Unique(#)TOGA
Assertion(#)TOGA
Unique(#)
async-http-client 635 1,366 337 398 169 229 0
bcel 3,911 5,142 2,203 618 241 377 0
commons-beanutils 227 1,437 363 434 223 212 1
commons-collections4 512 425 248 35 0 44 9
commons-conﬁguration2 225 1,906 854 276 27 249 0
commons-dbutils 222 295 52 58 1 59 2
commons-geometry 688 3,158 1,407 611 99 512 0
commons-imaging 1,088 3,551 1,073 676 257 420 1
commons-jcs3 1,251 1,861 407 374 145 229 0
commons-jexl3 1,191 3,988 2,509 522 71 451 0
commons-lang3 2,707 5,325 1,581 1,508 661 847 0
commons-net 667 1,847 382 500 158 344 2
commons-numbers 514 757 166 181 30 152 1
commons-pool2 1,531 859 492 140 9 136 5
commons-rng 68 1,133 384 114 0 114 0
commons-validator 666 1,722 452 462 27 436 1
commons-vfs 349 1,317 590 243 36 208 1
commons-weaver 33 199 39 53 23 30 0
http-request 959 238 76 20 3 17 0
joda-time 4,836 6,702 3,582 957 355 652 50
JSON-java 2,629 1,088 303 185 69 119 3
jsoup 7,910 4,110 2,393 640 172 497 29
scribejava 322 690 197 159 73 86 0
spark 301 983 232 267 80 187 0
springside4 936 1,286 275 383 97 286 0
Total: 34,378 51,385 20,597 9,814 3,026 6,893 105
TOGA uses EvoSuite preﬁxes and assertions to generate its own
assertions.Ourstudyindicatesthatthoseassertionsarelesseﬀec-
tivethanthoseinEvoSuite.Evenwiththesamesetofpreﬁxes,and
the same number of assertions as EvoSuite, TOGA detected nearly
3,000(30%)fewermutantsthanEvoSuite.Outof25artifacts,TOGA
did slightly better for only two artifacts: commons-collections4
andcommons-dbutils. For commons-rng, EvoSuiteandTOGAde-
tectedsamemutants.Fortheremaining22,EvoSuitedetectedmore
mutants, indicating EvoSuite assertions are stronger thanTOGA.
Insummary,startingfromthe setofpreﬁxesonwhich TOGA
providedmeaningfulassertions(non-emptyandnot-falsepositives),
out of the 51,385 generated mutants, EvoSuite assertions killed
30,411(59%)whileTOGA-generatedassertionskilled105additional
mutants (0.3%).
RQ3Finding: Despitehavingthesamenumberoftestcases
withthesamesetofpreﬁxesandexactlythesamenumberof
assertions, TOGA detected 30% less faults and 96% less unique
faults than EvoSuite assertions. 105 mutants (0.3% of the total)
were killedexclusivelybyTOGA.
Additional observations. To better understand when TOGA may
struggle or excel, we perform a deeper examination of the cases in
whichthe generatedassertions are the same ordiﬀer.
As TOGA uses EvoSuite assertions to extract the variable to
assert on, for a given test preﬁx, there is no diﬀerence between theassertTrue andassertFalse oraclesgeneratedbyEvoSuiteand
TOGA. In this study, 33% of the total assertions are of assertTrue
andassertFalse type for both EvoSuite andTOGA.
When the asserted variable is of type objector primitive types
(int,float,double,long),wehavealreadyseenthatTOGAstrug-
glestogenerateeﬀective assertEquals oraclesastheyrequirea
precise expected value. In this study, we ﬁnd that 17% of TOGA
assertions are assertEquals and 50% are assertNotNull , while
thedistributionfor EvoSuiteis57% of assertEquals and 10% are
assertNotNull oracles. This shift in distribution has implications
asassertEquals predicates are stronger than assertNotNull .
Listing4exempliﬁesthisdiﬀerenceforthe http-request arti-
fact, where PIT removes the setRequestMethod method call. Evo-
Suite generated an assertEquals oracle that compares the return
valueofsetRequestMethod withtheexpectedoutputandisable
to kill the mutant. TOGA generated an assertNotNull oracle that
only checks whether the value is not null and thus is not able to
killthe mutant.
WefoundacommonpatternamongtheTOGAassertionsthat
revealed unique mutants: they are able to leverage the local con-
stantsinthetestpreﬁxtogenerateoracles.Listing 5providesan
example for the commons-pool2 artifact, which has a method’s re-
turnvalueoftypeIntegerobjectmutated.EvoSuitegeneratesan
assertionthatonlychecksthenotnullcondition.Whereas,using
thelocal constantsinthetestpreﬁx( integer0 ),TOGAgenerates
anassertEquals oracle that compares two objects, thus killing
129Neural-BasedTestOracle Generation: A Large-Scale Evaluation andLessonsLearned ESEC/FSE ’23, December3–9, 2023,San Francisco, CA, USA
the mutant. However, this feature is also one the main sources for
the high percentage of false positives as shown in Table 4where
73%ofthe generated assertEquals oracles are false positive.
Listing 5: Fault detected by TOGA assertion and missed by
EvoSuite
public Integer getFetchDirection() {
return fetchDirection; ///Fault: return value modified
}
public void test58() throws Throwable {
StatementConfiguration.Builder scb = new
StatementConfiguration.Builder();
Integer int0 = newInteger((-587));
scb.fetchDirection(integer0);
StatementConfiguration sc0 = scb.build();
Integer int1 = sc0.getFetchDirection();
assertNotNull(int0); ///Fault Missed By EvoSuite Assertion
assertEquals(int0, int1); /*Fault Detected By TOGA Assertion*/
}
3.4 Threatsto Validity
OurreplicationforRQ1suﬀersfromthesameexternalthreatstova-
lidity of the original paper, and somewhat reduced internal threats
giventhatwewereabletosuccessfullyruntheoriginaltoolsand
scriptswithassistancefromTOGA’sauthors.Theﬁrstreplication
does,however,addresswhatmaybeconsideredaconstructvalid-
itythreatintheoriginalpaperinthattheintendedconcepttobe
measured, the value-addedby TOGA, does not account for what a
baselinetechniquecan already ﬁnd.
To mitigate the threats to external validity of the original pa-
per, we extended it through our replication with 25 open-source
Javaapplicationsfromvariousdomainsandorganizations.These
applications vary in program and test suite size, number of test
cases with assertions and exception oracles, and the size of their
Javadoc. Introducing these applications may have shifted the input
distributionexpectedbyTOGA,althoughitseemsunlikelygiven
the powerful CodeBERT model it uses. We also address the lim-
ited number of faults available in the original paper by generating
a large number of mutants as proxy for real bugs. A threat that
remainsisthegeneralizationofthestudytotestsuitesgenerated
through other tools beyond EvoSuite, a limitation inherited by the
currentimplementationofTOGA but not intrinsic ofthe method.
Inadditiontotheoriginalreplicationpackage,wehaveimple-
mentedseveraltoolsandscriptstoconductourexperiments,which
may have bugs. To run test suites, we have used JUnit, and to
generatemutants,wehaveusedPIT.Eventhoughthesetoolsare
well-establishedandhavebeenusedinnumerousstudies,theymay
haveunknownbugs.Tomitigatethethreats,wehaveperformed
extensivesanitytestsandruneachexperimentmultipletimesto
makesurethatwegetconsistentresults,besidesmakingalldata
andcode available at [ 23]for anyone to review.
3.5 LessonsLearned
Besides shining a new light on the speciﬁc performance of TOGA,
thisstudywillhopefullyinformfutureevaluationmethodologies
forsimilaroracleinferenceapproaches.Inparticular,besidescon-
tributing a large dataset for future evaluations of such techniques,
we summarize belowthree actionablelessons learned.
1) JUnit implicit oracle detected over 50% of both real and injected
bugs. The ability of detecting unexpected exceptional behaviors
via the sole execution of the test preﬁxes that emerged from ourexperiments is also consistent with previous studies [ 17,26,46].
Therefore, in a realistic evaluation setting, one should use these
implicit, i.e., “NoException”oraclesas the baselineandreportthe
fault-detectioneﬀectivenessimprovementrelativetotheimplicit
oracle.(RQ1)
2)The precisionof thegeneratedoraclesshouldbea centralevalu-
ationmetricforarealisticassessmentoforaclegenerationmethods.
Imprecise oracles will require developers to diagnose and repair
failingtestsduetofalsepositiveassertions.Recallshouldbealways
evaluatedtogetherwithprecision. (RQ2)
3) Using test preﬁxes generated from ﬁxed programs to catch bugs
inthe buggyversionsmayinappropriatelybiasﬁndings.In such
cases, just executing the test preﬁxes generated from the ﬁxed ver-
sions on the buggy versions is often suﬃcient to detect many bugs.
Moresystematicevaluationapproaches,suchasmutationtesting,
thatmorecloselyreﬂecthowadevelopercanpracticallyassessa
test suiteinthereal world should be included intheevaluationof
automatedtest generationmethods.(RQ3)
4 CONCLUSIONS
Inthispaper,wereplicatedwithadiﬀerentandbroaderexperimen-
tal protocol TOGA [ 17], a recent neural-based oracle generation
method.Ourstudyaimedat1)investigatingmorecloselytheadded
bugdetectionvalueofTOGA-generatedoracles,2)evaluatingthe
precisionofTOGAintermsoffalsepositivebugreportsfromits
oracle, and 3) evaluating the defect prediction recall using muta-
tion testing methods. For our ﬁrst objective, we obtained results
consistent with the original study: TOGA detected the same 57
bugs.However,upondeeperinvestigation,only19detectionsare
imputabletoTOGA’sexceptionorassertionoracles,whileforthe
other 38the soleexecutionofthe test preﬁx– generatedbyEvo-
Suite – threw exceptions making the test fail. The second and
third objectives involved a broader set of subjects. While TOGA
generated assertions only for half of the test preﬁxes, 47% of the
assertionsproducedfalsepositivereports,besidesmisclassifying
whetheranexceptionor anassertion oraclewas needed foranav-
erageof24%ofthetestpreﬁxes.Finally,wediﬀerentiallycompared
the mutation killings counts of JUnit failures due to unexpected
exceptions,EvoSuiteassertions,andTOGAassertions,observing
that out of the 51,385mutants, only 105 were killedexclusively by
TOGA assertions.
Overall,whileTOGA’sinnovativeapproachwilllikelybearfruit-
fulfutureresearchdirections,ourstudysuggeststheneedfordeeper
investigation of the reasons behind ﬁndings produced by learning-
based methods, and a challenge for the research community to
develop techniques for reducing their currently too high false posi-
tive rateto enable industrialadoption.
ACKNOWLEDGEMENTS
This material is based in part upon work supported by the DARPA
ARCOS program under contract FA8750-20-C-0507, by The Air
ForceOﬃceofScientiﬁcResearchunderawardnumberFA9550-21-
0164, and by Lockheed Martin Advanced Technology Laboratories.
We are thankful to Gabriel Ryan for assisting in running the
TOGA replication package.
130ESEC/FSE ’23, December3–9, 2023,San Francisco, CA, USA SoneyaBintaHossain, Antonio Filieri,Ma/t_thewB.Dwyer, Sebastian Elbaum,andWillem Visser
REFERENCES
[1]Shaukat Ali, Lionel C Briand, Hadi Hemmati, and Rajwinder Kaur Panesar-
Walawege. 2009. A systematic review of the application and empirical inves-
tigation of search-based test case generation. IEEE Transactions on Software
Engineering 36,6 (2009), 742–762. https://doi.org/10.1109/TSE.2009.52
[2]M Moein Almasi, Hadi Hemmati, Gordon Fraser, Andrea Arcuri, and Janis Bene-
felds.2017. Anindustrialevaluationofunittestgeneration:Findingrealfaults
in a ﬁnancial application. In 2017 IEEE/ACM 39th International Conference on
SoftwareEngineering:SoftwareEngineeringinPracticeTrack(ICSE-SEIP) .IEEE,
263–272. https://doi.org/10.1109/ICSE-SEIP.2017.27
[3]J.H. Andrews, L.C. Briand, and Y. Labiche. 2005. Is mutation an appropriate tool
for testing experiments? [software testing]. In Proceedings. 27th International
Conference on Software Engineering, 2005. ICSE 2005. 402–411. https://doi.org/10.
1109/ICSE.2005.1553583
[4]EarlT.Barr,MarkHarman,PhilMcMinn,MuzammilShahbaz,andShinYoo.2015.
TheOracleProbleminSoftwareTesting:ASurvey. IEEETransactionsonSoftware
Engineering 41,5 (2015), 507–525. https://doi.org/10.1109/TSE.2014.2372785
[5]MoritzBeller,Chu-PanWong,JohannesBader,AndrewScott,MateuszMachalica,
SatishChandra,andErikMeijer.2021. Whatitwouldtaketousemutationtesting
inindustry—astudyatfacebook.In 2021IEEE/ACM43rdInternationalConference
on Software Engineering: Software Engineering in Practice (ICSE-SEIP) . IEEE, 268–
277.https://doi.org/10.1109/ICSE-SEIP52600.2021.00036
[6]AriannaBlasi,AlbertoGoﬃ,KonstantinKuznetsov,AlessandraGorla,MichaelD.
Ernst, Mauro Pezzè, and Sergio Delgado Castellanos. 2018. Translating Code
Comments to Procedure Speciﬁcations. In Proceedings of the 27th ACM SIGSOFT
International Symposium on SoftwareTesting and Analysis (Amsterdam, Nether-
lands)(ISSTA 2018) . Association for Computing Machinery, New York, NY, USA,
242–253. https://doi.org/10.1145/3213846.3213872
[7]Arianna Blasi, Alessandra Gorla, Michael D Ernst, Mauro Pezzè, and Antonio
Carzaniga. 2021. MeMo: Automatically identifying metamorphic relations in
Javadoc comments for test automation. Journal of Systems and Software 181
(2021), 111041. https://doi.org/10.1016/j.jss.2021.111041
[8]MarcelBöhme,Van-ThuanPham,Manh-DungNguyen,andAbhikRoychoudhury.
2017.Directedgreyboxfuzzing.In Proceedingsofthe2017ACMSIGSACConference
onComputerandCommunicationsSecurity .2329–2344. https://doi.org/10.1145/
3133956.3134020
[9]JoséCampos,AndreaArcuri,GordonFraser,andRuiAbreu.2014. Continuous
TestGeneration:EnhancingContinuousIntegrationwithAutomatedTestGener-
ation.InProceedingsofthe29thACM/IEEEInternationalConferenceonAutomated
SoftwareEngineering (Vasteras,Sweden) (ASE’14).AssociationforComputing
Machinery,NewYork,NY,USA,55–66. https://doi.org/10.1145/2642937.2643002
[10]Chen Chen, Baojiang Cui, Jinxin Ma, Runpu Wu, Jianchao Guo, and Wenqian
Liu.2018. Asystematicreviewoffuzzingtechniques. Computers&Security 75
(2018), 118–137. https://doi.org/10.1016/j.cose.2018.02.002
[11]John Joseph Chilenski and Steven P Miller. 1994. Applicability of modiﬁed
condition/decision coverage to software testing. Software Engineering Journal 9,
5 (1994), 193–200. https://doi.org/10.1049/sej.1994.0025
[12]Maria Christakis and Christian Bird. 2016. What developers want and need
fromprogramanalysis:anempiricalstudy.In Proceedingsofthe 31stIEEE/ACM
international conference on automated software engineering . 332–343. https:
//doi.org/10.1145/2970276.2970347
[13]Henry Coles, Thomas Laurent, Christopher Henard, Mike Papadakis, and An-
thonyVentresque.2016. Pit:apracticalmutationtestingtoolforjava.In Proceed-
ingsofthe25thinternationalsymposiumonsoftwaretestingandanalysis .449–452.
https://doi.org/10.1145/2931037.2948707
[14]Mickaël Delahaye and Lydie du Bousquet. 2013. A Comparison of Mutation
AnalysisToolsforJava.In 201313thInternationalConferenceonQualitySoftware .
187–195. https://doi.org/10.1109/QSIC.2013.47
[15]Xavier Devroey, Sebastiano Panichella, and Alessio Gambi. 2020. Java unit
testing tool competition: Eighth round. In Proceedings of the IEEE/ACM 42nd
International Conference on Software Engineering Workshops . 545–548. https:
//doi.org/10.1145/3387940.3392265
[16]Elizabeth Dinella, Gabriel Ryan, Shuvendu K. Lahiri, and Todd Mytkowicz. 2022.
Replication Artifact for TOGA: A Neural Method for Test Oracle Generation .https:
//doi.org/10.5281/zenodo.6210589
[17]Elizabeth Dinella, Gabriel Ryan, Todd Mytkowicz, and Shuvendu K. Lahiri. 2022.
TOGA:ANeuralMethodforTestOracleGeneration.In Proceedingsofthe44th
InternationalConferenceonSoftwareEngineering (Pittsburgh,Pennsylvania) (ICSE
’22). Association for Computing Machinery, New York, NY, USA, 2130–2141.
https://doi.org/10.1145/3510003.3510141
[18]Zhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xiaocheng Feng, Ming Gong,
LinjunShou,BingQin,TingLiu,DaxinJiang,etal .2020. Codebert:Apre-trained
modelfor programmingand natural languages. arXiv preprint arXiv:2002.08155
(2020).
[19]GordonFraserandAndreaArcuri.2011. Evosuite:automatictestsuitegeneration
forobject-oriented software.In Proceedings ofthe19th ACMSIGSOFT symposium
andthe13thEuropeanconferenceonFoundationsofsoftwareengineering .416–419.https://doi.org/10.1145/2025113.2025179
[20]GordonFraser andAndrea Arcuri.2014. A large-scale evaluation of automated
unittestgenerationusingevosuite. ACMTransactionsonSoftwareEngineering
and Methodology (TOSEM) 24,2 (2014), 1–42. https://doi.org/10.1145/2685612
[21]Patrice Godefroid, Nils Klarlund, and Koushik Sen. 2005. DART: Directed
automated random testing. In Proceedings of the 2005 ACM SIGPLAN confer-
ence on Programming language design and implementation . 213–223. https:
//doi.org/10.1145/1064978.1065036
[22]AlbertoGoﬃ,AlessandraGorla,MichaelD.Ernst,andMauroPezzè.2016. Au-
tomaticGenerationofOraclesforExceptionalBehaviors.In Proceedingsofthe
25th International Symposium on Software Testing and Analysis (Saarbrücken,
Germany) (ISSTA2016) .AssociationforComputingMachinery,NewYork,NY,
USA,213–224. https://doi.org/10.1145/2931037.2931061
[23]SoneyaBintaHossain.2023. Artifact:Neural-BasedTestOracleGeneration:A
Large-scaleEvaluationandLessonsLearned. (82023). https://doi.org/10.6084/
m9.ﬁgshare.21973091.v4
[24]SoneyaBintaHossainandMatthewBDwyer.2022. ABriefSurveyonOracle-
based Test Adequacy Metrics. arXiv preprint arXiv:2212.06118 (2022).https:
//doi.org/10.48550/arXiv.2212.06118
[25]Soneya Binta Hossain, Matthew B. Dwyer, Sebastian Elbaum, and Anh Nguyen-
Tuong. 2023. Measuring and Mitigating Gaps in Structural Testing. In 2023
IEEE/ACM45thInternationalConferenceonSoftwareEngineering(ICSE) .1712–
1723.https://doi.org/10.1109/ICSE48619.2023.00147
[26]AliRezaIbrahimzada,YigitVarli,DilaraTekinoglu,andReyhanehJabbarvand.
2022. PerfectistheEnemyofTestOracle.In Proceedingsofthe30thACMJoint
European Software Engineering Conference and Symposium on the Foundations
of Software Engineering (Singapore, Singapore) (ESEC/FSE 2022) . Association for
Computing Machinery, 70–81. https://doi.org/10.1145/3540250.3549086
[27]GunelJahangirova,DavidClark,MarkHarman,andPaoloTonella.2016. Test
oracle assessment and improvement. In Proceedings of the 25th International
SymposiumonSoftwareTestingandAnalysis .247–258. https://doi.org/10.1145/
2931037.2931062
[28]Brittany Johnson, Yoonki Song, Emerson Murphy-Hill, and Robert Bowdidge.
2013. Whydon’tsoftwaredevelopersusestaticanalysistoolstoﬁndbugs?.In
2013 35th International Conference on Software Engineering (ICSE) . IEEE, 672–681.
https://doi.org/10.1109/ICSE.2013.6606613
[29]RenéJust,DarioushJalali,andMichaelDErnst.2014. Defects4J:Adatabaseofex-
istingfaultstoenablecontrolledtestingstudiesforJavaprograms.In Proceedings
ofthe2014InternationalSymposiumonSoftwareTestingandAnalysis .437–440.
https://doi.org/10.1145/2610384.2628055
[30]RenéJust,DarioushJalali,LauraInozemtseva,MichaelD.Ernst,ReidHolmes,and
Gordon Fraser. 2014. Are Mutants a Valid Substitute for Real Faults in Software
Testing?.In Proceedingsofthe22ndACMSIGSOFTInternationalSymposiumon
Foundations of Software Engineering (Hong Kong, China) (FSE 2014) . Association
for Computing Machinery, New York, NY, USA, 654–665. https://doi.org/10.
1145/2635868.2635929
[31]Marinos Kintis, Mike Papadakis, Andreas Papadopoulos, Evangelos Valvis, Nicos
Malevris, and Yves Le Traon. 2018. How eﬀective are mutation testing tools?
AnempiricalanalysisofJavamutationtestingtoolswithmanualanalysisand
real faults. Empirical Software Engineering 23, 4 (2018), 2426–2463. https:
//doi.org/10.1007/s10664-017-9582-5
[32]KiranLakhotia,PhilMcMinn,andMarkHarman.2010.Anempiricalinvestigation
intobranchcoverageforCprogramsusingCUTEandAUSTIN. JournalofSystems
and Software 83,12(2010), 2379–2391. https://doi.org/10.1016/j.jss.2010.07.026
[33]Thomas Laurent, MikePapadakis,Marinos Kintis, Christopher Henard, Yves Le
Traon,andAnthonyVentresque.2017. AssessingandImprovingtheMutation
Testing Practice of PIT. In 2017 IEEE International Conference on Software Testing,
Veriﬁcation and Validation (ICST) . 430–435. https://doi.org/10.1109/ICST.2017.47
[34]ValentinJMManès,HyungSeokHan,ChoongwooHan,SangKilCha,Manuel
Egele,EdwardJSchwartz,andMaverickWoo.2019. Theart,science,andengi-
neeringoffuzzing:Asurvey. IEEETransactionsonSoftwareEngineering 47,11
(2019), 2312–2331. https://doi.org/10.1109/TSE.2019.2946563
[35]KeMao,MarkHarman,andYueJia.2016. Sapienz:Multi-objectiveautomated
testingforandroidapplications.In Proceedingsofthe25thinternationalsymposium
onsoftwaretestingandanalysis .94–105. https://doi.org/10.1145/2931037.2931054
[36]William M McKeeman. 1998. Diﬀerential testing for software. Digital Technical
Journal10,1 (1998), 100–107.
[37]Glenford J Myers, Corey Sandler, and Tom Badgett. 2011. The art of software
testing. John Wiley& Sons.
[38]Rahul Pandita, Xusheng Xiao, Hao Zhong, Tao Xie, Stephen Oney, and Amit
Paradkar. 2012. Inferring method speciﬁcations from natural language API
descriptions.In 201234thInternationalConferenceonSoftwareEngineering(ICSE) .
815–825. https://doi.org/10.1109/ICSE.2012.6227137
[39]Sebastiano Panichella, Alessio Gambi, Fiorella Zampetti, and Vincenzo Riccio.
2021. Sbst tool competition 2021. In 2021 IEEE/ACM 14th International Workshop
onSearch-BasedSoftwareTesting(SBST) .IEEE,20–27. https://doi.org/10.1109/
SBST52555.2021.00011
131Neural-BasedTestOracle Generation: A Large-Scale Evaluation andLessonsLearned ESEC/FSE ’23, December3–9, 2023,San Francisco, CA, USA
[40]Goran Petrović, Marko Ivanković, Gordon Fraser, and René Just. 2021. Does
mutation testing improve testing practices?. In 2021 IEEE/ACM 43rd International
Conference on Software Engineering (ICSE) . IEEE, 910–921. https://doi.org/10.
1109/ICSE43902.2021.00087
[41]GoranPetrovic, Marko Ivankovic, Gordon Fraser, and RenéJust. 2021. Practical
mutation testing at scale: A view from Google. IEEE Transactions on Software
Engineering (2021).https://doi.org/10.1109/TSE.2021.3107634
[42]GoranPetrović, Marko Ivanković, Gordon Fraser, and RenéJust. 2022. Practical
MutationTestingatScale:AviewfromGoogle. IEEETransactionsonSoftware
Engineering 48,10(2022),3900–3912. https://doi.org/10.1109/TSE.2021.3107634
[43]Apache Commons Proper. 2022. Apache Commons Proper – A repository of
reusable Java components. https://commons.apache.org/components.html , Last
accessed on2022-10-11.
[44]ShwetaRani,BhartiSuri,andSunilKumarKhatri.2015.Experimentalcomparison
of automated mutation testing tools for java. In 2015 4th International Conference
onReliability,InfocomTechnologiesandOptimization(ICRITO)(TrendsandFuture
Directions) . 1–6.https://doi.org/10.1109/ICRITO.2015.7359265
[45]CaitlinSadowski,EdwardAftandilian,AlexEagle,LiamMiller-Cushon,andCiera
Jaspan. 2018. Lessons from building static analysis tools at google. Commun.
ACM61,4 (2018), 58–66. https://doi.org/10.1145/3188720
[46]David Schuler and Andreas Zeller. 2013. Checked coverage: an indicator for
oraclequality. Softwaretesting,veriﬁcationandreliability 23,7(2013),531–551.
https://doi.org/10.1109/ICST.2011.32
[47]SergioSegura,GordonFraser,AnaB.Sanchez,andAntonioRuiz-Cortés.2016. A
Surveyon Metamorphic Testing. IEEETransactionson Software Engineering 42,9
(2016), 805–824. https://doi.org/10.1109/TSE.2016.2532875
[48]KoushikSen,DarkoMarinov,andGulAgha.2005. CUTE:Aconcolicunittesting
engine for C. ACM SIGSOFT Software Engineering Notes 30, 5 (2005), 263–272.
https://doi.org/10.1145/1095430.1081750
[49]SinaShamshiri,RenéJust,JoséMiguelRojas,GordonFraser,PhilMcMinn,and
AndreaArcuri.2015. DoAutomaticallyGeneratedUnitTestsFindRealFaults?
AnEmpiricalStudyof EﬀectivenessandChallenges (T).In 2015 30thIEEE/ACM
InternationalConferenceonAutomatedSoftwareEngineering(ASE) .201–211. https:
//doi.org/10.1109/ASE.2015.86
[50]Kavir Shrestha and Matthew J. Rutherford. 2011. An Empirical Evaluation of
AssertionsasOracles.In 2011FourthIEEEInternationalConferenceonSoftware
Testing,VeriﬁcationandValidation .110–119. https://doi.org/10.1109/ICST.2011.50
[51]MattStaats,MichaelWWhalen,andMatsPEHeimdahl.2011. Programs,tests,
andoracles:thefoundationsoftestingrevisited.In 201133rdinternationalcon-
ferenceonsoftwareengineering(ICSE) .IEEE,391–400. https://doi.org/10.1145/
1985793.1985847[52]ShinHweiTan,DarkoMarinov,LinTan,andGaryT.Leavens.2012. @tComment:
Testing Javadoc Comments to Detect Comment-Code Inconsistencies. In 2012
IEEEFifthInternationalConferenceonSoftwareTesting,VeriﬁcationandValidation .
260–269. https://doi.org/10.1109/ICST.2012.106
[53]Valerio Terragni, Gunel Jahangirova, Paolo Tonella, and Mauro Pezzè. 2020.
Evolutionaryimprovement ofassertion oracles. In Proceedingsof the28thACM
JointMeetingonEuropeanSoftwareEngineeringConferenceandSymposiumonthe
FoundationsofSoftwareEngineering .1178–1189. https://doi.org/10.1145/3368089.
3409758
[54]MicheleTufano,DawnDrain,AlexeySvyatkovskiy,ShaoKunDeng,andNeel
Sundaresan. 2020. Unit test case generation with transformers and focal context.
arXiv preprint arXiv:2009.05617 (2020).
[55]Michele Tufano, Dawn Drain, Alexey Svyatkovskiy, and Neel Sundaresan. 2022.
Generating accurate assert statements for unit test cases using pretrained trans-
formers. In Proceedings of the 3rd ACM/IEEE International Conference on Automa-
tion ofSoftwareTest . 54–64.https://doi.org/10.1145/3524481.3527220
[56]Tássio Virgínio,Luana Almeida Martins, Larissa Rocha Soares, RailanaSantana,
Heitor Costa, and Ivan Machado. 2020. An empirical study of automatically-
generatedtestsfromthe perspectiveoftestsmells.In Proceedingsof theXXXIV
Brazilian Symposium on Software Engineering . 92–96. https://doi.org/10.1145/
3422392.3422412
[57]JeﬀreyM.Voas. 1992. PIE: A dynamic failure-based technique. IEEETransactions
onsoftwareEngineering 18,8 (1992), 717. https://doi.org/10.1109/32.153381
[58]Cody Watson, Michele Tufano, Kevin Moran, Gabriele Bavota, and Denys Poshy-
vanyk. 2020. On learning meaningful assert statements for unit test cases. In
Proceedings of the ACM/IEEE 42nd International Conference on Software Engineer-
ing. 1398–1409. https://doi.org/10.1145/3377811.3380429
[59]Elaine J. Weyuker. 1988. The evaluation of program-based software test data
adequacy criteria. Commun. ACM 31, 6 (1988), 668–675. https://doi.org/10.1145/
62959.62963
[60]MichaelWhalen,GregoryGay,DongjiangYou,MatsP.E.Heimdahl,andMatt
Staats. 2013. Observable modiﬁed condition/decision coverage. In 2013 35th
International Conference on Software Engineering (ICSE) . 102–111. https://doi.
org/10.1109/ICSE.2013.6606556
[61]Michal Zalewski. 2017. American fuzzy lop (AFL). URL: http://lcamtuf. coredump.
cx/aﬂ(2017).
[62]YuchengZhangandAliMesbah.2015.Assertionsarestronglycorrelatedwithtest
suiteeﬀectiveness.In Proceedingsofthe201510thJointMeetingonFoundations
ofSoftwareEngineering . 214–224. https://doi.org/10.1145/2786805.2786858
Received 2023-02-02; accepted 2023-07-27
132