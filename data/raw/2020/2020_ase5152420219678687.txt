Performance Testing for Cloud Computing with
Dependent Data Bootstrapping
Sen He
Department of Computer Science
University of Texas at San Antonio
San Antonio, USA
sen.he@utsa.eduTianyi Liu
Department of Computer Science
University of Texas at San Antonio
San Antonio, USA
tianyi.liu@utsa.eduPalden Lama
Department of Computer Science
University of Texas at San Antonio
San Antonio, USA
palden.lama@utsa.edu
Jaewoo Lee
Department of Computer Science
University of Georgia
Athens, USA
jaewoo.lee@uga.eduIn Kee Kim
Department of Computer Science
University of Georgia
Athens, USA
inkee.kim@uga.eduWei Wang
Department of Computer Science
University of Texas at San Antonio
San Antonio, USA
wei.wang@utsa.edu
Abstract —To effectively utilize cloud computing, cloud practice
and research require accurate knowledge of the performance of
cloud applications. However, due to the random performanceﬂuctuations, obtaining accurate performance results in the cloudis extremely difﬁcult. To handle this random ﬂuctuation, priorresearch on cloud performance testing relied on a non-parametricstatistic tool called bootstrapping to design their stop criteria.However, in this paper, we show that the basic bootstrappingemployed by prior work overlooks the internal dependencywithin cloud performance test data, which leads to inaccurateperformance results.
We then present Metior, a novel automated cloud performance
testing methodology, which is designed based on statisticaltools of block bootstrapping, the law of large numbers, andautocorrelation. These statistical tools allow Metior to properly
consider the internal dependency within cloud performance testdata. They also provide better coverage of cloud performanceﬂuctuation and reduce the testing cost. Experimental evaluationon two public clouds showed that 98% of Metior’s tests could
provide performance results with less than 3% error . Metior also
signiﬁcantly outperformed existing cloud performance testingmethodologies in terms of accuracy and cost – with up to 14%increase in the accurate test count and up to 3.1 times reductionin testing cost.
I. I NTRODUCTION
Cloud computing is widely adopted today due to its high
cost-efﬁciency. To effectively utilize the cloud, cloud users
need to have accurate knowledge of the performance oftheir applications. Accurate knowledge of cloud performanceallows them to determine whether a virtual machine (VM)conﬁguration (e.g., type and count of VMs) or an auto-scalingpolicy satisfy their performance requirements [1–3]. Accurateperformance data are also required for cloud research toevaluate new optimization algorithms or to be used as thetraining and testing data to develop new models [1, 4–9].
Performance testing is a standard procedure to obtain
the performance for any applications [10, 11]. For a cloudapplication, performance testing can be used to obtain itsperformance results as point estimates. That is, to obtain themean or the percentile of its performance, such as the meanthroughput or 90%ile execution time [9, 12, 13]. There aretwo fundamental requirements for cloud performance testing.First, the performance results should be accurate. Second, ascloud performance testing also incurs cloud usage expenditure,this testing should not cause excessive cost.
Performance testing is typically conducted by repeatedly
executing the application-under-test (AUT) with a set ofrepresentative workloads/inputs until a stop criterion deems
that the results obtained from the test are accurate. This stopcriterion is the key to ensure the above two requirements aresatisﬁed. However, as shown in prior work, due to the randomperformance ﬂuctuation [12–17], it is extremely challengingto design good stop criteria for cloud performance testing.
To handle the random performance ﬂuctuation, prior stud-
ies relied on non-parametric statistics tools to design theirstop criteria. In particular, prior studies have employed non-parametric bootstrapping, which is a re-sampling techniqueused to calculate the conﬁdence interval (CI) of a performancetesting result (e.g., the CI of the mean performance) [9, 12, 13].The conﬁdence interval is typically viewed as the margin-of-error [18, 19]. Therefore, if the width of the conﬁdence intervalis smaller than a (user-) predeﬁned maximum allowed error,the performance result is deemed accurate enough and theperformance test can be stopped.
Unfortunately, prior work had shown that the performance
testing methodology using bootstrapping cannot always pro-vide accurate performance results [13, 20]. Prior work con-cluded that the inaccuracy was partially caused by the test’sincomplete coverage of the cloud performance ﬂuctuations.However, in our research, we discovered that there is anotherfundamental issue related to the bootstrapping methodology.
In this paper, we ﬁrst present an analysis of the existing
bootstrapping-based testing methodology [12]. This analysisreveals a critical issue with this methodology that is unknownto the current research – the overlooked internal dependency
6662021 36th IEEE/ACM International Conference on Automated Software Engineering (ASE)
DOI 10.1109/ASE51524.2021.000652021 36th IEEE/ACM International Conference on Automated Software Engineering (ASE) | 978-1-6654-0337-5/21/$31.00 ©2021 IEEE | DOI: 10.1109/ASE51524.2021.9678687
978-1-6654-0337-5/21/$31.00  ©2021  IEEE
within the performance test data. Because cloud performance
ﬂuctuation is mainly caused by the hardware resource con-tention from multi-tenancy (i.e., cloud applications/VMs shar-ing hardware) [15, 21], the performance data obtained fromthe tests are internally correlated. That is, the performance datafrom continuous tests within a short period are similar to eachother, and the performance ﬂuctuations may also have repeatedpatterns. However, the basic bootstrapping employed by priorstudies does not consider this internal dependency, causingincorrectly calculated conﬁdence intervals, which in turn,lead to incorrect performance results. Consequently, a newperformance testing methodology that considers the internaldependency of cloud performance tests is required.
Moreover, our analysis also shows that an advanced boot-
strapping technique originally designed for time series data,called Block Bootstrapping, can retain the internal dependencyduring the re-sampling [22]. Hence, it may provide moreaccurate performance results. Nonetheless, blindly applyingblock bootstrapping does not guarantee accurate results, asits block size must be tuned for individual cloud applicationsand cloud platforms. Moreover, enough tests must also beconducted to fully cover the potential cloud ﬂuctuations.
Based on the above analysis, we developed Metior ,an o v e l
automated cloud performance testing methodology. Metior
has two components. The ﬁrst component is the new stopcriterion to determine when performance tests can be stoppedand accurate performance results are obtained. The new stopcriterion is based on block bootstrapping and the “law oflarge numbers.” Block bootstrapping allows Metior to properly
consider the internal dependency of performance test data,while the “law of large numbers” ensures good coverage ofcloud performance ﬂuctuations. To handle the varying blocksize, Metior employs a novel technique that can automatically
determine the best block size for each cloud application andplatform [30]. The second component is the low-cost testexecution strategy. As the performance of continuous testsis similar to each other, there is no need to continuouslyexecute the AUT. Therefore, Metior executes the AUT in small
intervals/periods (one day) and intermittently (4 executionsper hour) to reduce the overall number of executions and theassociated cloud usage cost. We also provided a systematicapproach to determine the interval length and the frequencyof the intermittent execution using the aforementioned bestblock size and autocorrelation [23]. Metior is implemented
as a fully automated performance tester for several publicclouds, including Amazon Web Services (AWS) [24], GoogleCloud [25], and Chameleon cloud [26].
We evaluated Metior with six benchmarks on two public
clouds, AWS [24] and Chameleon [26], using six differentVM conﬁgurations. The results show that Metior can provide
accurate performance results – among the thousands of testsconducted, 98% of them provided performance results withless than 3% errors. Metior also signiﬁcantly outperformed
existing cloud performance testing methodologies in terms ofaccuracy and cost – it could increase accurate test count byup to 14% and reduce the testing cost by up to 3.1 times. Wealso applied Metior to a state-of-the-art cloud performance
prediction technique. The evaluation results showed that byproviding more accurate training data, Metior could improve
the accuracy of the prediction technique by 17.3% on average.
The contributions of this paper include:
1. An analysis of the basic bootstrapping which reveals that
the overlooked internal dependency of cloud performance test
data caused inaccurate cloud performance results.
2. A reliable and automated cloud performance testing
methodology, Metior , which is designed based on block boot-
strapping, the law of large numbers, and autocorrelation.
3. A thorough evaluation of Metior on two public clouds
with 33 different benchmarks/VM conﬁgurations to show theaccuracy and cost beneﬁts of Metior .
4. A case study showing how Metior beneﬁts recent cloud
research by bringing in reliable performance results.
The rest of this paper is organized as follows: Section II
provides the analysis on the basic bootstrapping; Section IIIpresents the design of the Metior ; Section IV provides ex-
perimental evaluations. Section V presents a case study ofapplying Metior in cloud research. Section VI discusses the
limitation of Metior . Section VII discusses related work, and
Section VIII concludes the paper.
II. A
NALYSIS OF BOOTSTRAPPING CLOUD PERFORMANCE
A performance test typically involves repeatedly executing
the AUT with one or more representative inputs. For eachexecution, the performance data, such as the execution time,
latency, or throughput, are recorded. Based on the performancedata from a series of executions, the performance testing
result, such as the mean latency or 90%ile execution time, canbe calculated. The conﬁdence interval (CI) of the performancetesting result can also be calculated, which is usually viewed asthe margin-of-error of this result [18, 19]. The width of thisCI is usually used as the stop criterion of the performancetest– if the CI width is smaller than a user-deﬁned maximum
allowed error (e.g., 3% maximum error), then the test can be
stopped [12, 19]. In this paper, we call a performance testingresult accurate if it indeed has an error less than the maximum
allowed error.
For non-cloud performance testing, the CIs are usually
computed using t-value or z-value, assuming the performanceis normally distributed [19]. However, the performance ofcloud applications is usually not normal [13]. Therefore, cur-rent research employed a non-parametric statistics technique,bootstrapping (BT), to calculate the CIs for cloud performancetesting results [9, 12, 27, 28]. Unfortunately, the performancetests conducted with the current bootstrapping-based method-ology tend to provide inaccurate results. This section providesan analysis of the cause of this inaccuracy.
A. Background on Bootstrapping
Basic Bootstrapping. Bootstrapping (BT) is essentially a
resampling technique. Without loss of generality, here we
show how to use bootstrapping to determine the CI of the90%ile execution time of some performance test data with a
667Density
Generate distribution
forusing1...CCI
for01 567 4 8 32
34 281 6 5 40
62 101 8 7 58
84 076 7 8 53S1*
S2*
SC*- - - - - - - - - - - - - - - - - - - - - - - -.
.
.CBootstrap
ResamplesInitial Sample S
withnelements
..
.1
C2 95%
(a) Basic Bootstrap: resampling and computing CI w. 95% conﬁdence level01 567 4 8 32
23 801 7 2 64
34 345 2 6 15
56 534 4 5 37S1*
S2*
SC*- - - - - - - - - - - - - - - - - - - - - - - -..
.CBootstrap
ResamplesInitial Sample S
withnelements
(b) Block bootstrap: resampling
Fig. 1. Examples of resampling and computing CIs with two bootstrapping methods.
95% conﬁdence level (CL). Let Sbe the set of execution times
obtained from a performance test with nrepeated executions.
That is,|S|=n. Letθbe the 90%ile execution time calculated
fromS. A resample of S, denoted by S∗, is constructed
by randomly selecting execution times from S. Figure 1a
illustrates the construction of S∗[28, 29]. Each selection
randomly picks one execution time from S. This selection is
then repeated for ntimes to select nexecution times from S
with replacement. These nexecution times constitute S∗.
The resampling is repeated for ctimes to generate c
resamples, denoted by S∗
1,S∗
2,...,S∗
c. Usually, cshould be
larger than 1000 for bootstrapping to work properly [28, 29].
For each resample, its 90%ile can be calculated, providing c
90%ile execution times. Let these 90%iles be θ1,θ2,...,θ c.
In bootstrapping, the empirical distribution constructed fromthese 90%iles is considered as a close approximation of thedistribution of θ(the 90%ile of S). Therefore, the center 95%
area of this distribution is then the CI of θ. More speciﬁcally,
θ
1,θ2,...,θ care ﬁrst sorted, and the 2.5%ile and 97.5%ile of
the sorted list are the lower and upper bounds of the CI.
Intuitively, bootstrapping works if the resampling on S
closely resembles how Sis obtained from the real population.
Sis essentially a random sample of the real population.
The resampling on S, in turn, views Sas the population.
The resamples, S∗
1,S∗
2,...,S∗
c, are then the random samples
ofS. If the resampling process closely resembles how S
is sampled from the population, then the variation of theresamples also closely resembles the variation when samplingthe real population. Therefore, the distribution of the resamplescan then be used to empirically calculate the CI for statisticsestimates of the real population. However, as we will showlater, the resampling process of basic bootstrapping does notresemble how cloud performance data are obtained from thereal population, and hence, cannot always provide reliable CIs.
Block Bootstrapping. Block bootstrapping is mainly used
to bootstrap time series, where the data have internal depen-dencies and/or seasonality [30]. Although performance testdata are not strictly time series, they still contain internaldependencies. Therefore, after we discovered that the basicbootstrapping could not preserve the internal data dependen-cies during resampling, we started to experiment with blockbootstrapping, which considerably outperformed the basicbootstrapping. Therefore, to provide a more comprehensiveanalysis, block bootstrapping is also introduced here.
In block bootstrapping, a resample, S
∗, is also constructedfromSusing random selections. However, for each selection,
instead of just selecting one data point (e.g., one executiontime), a block of continuous data points is selected. Byselecting a block of data points, the internal dependency withinthe data points is preserved. If there are bdata points in a
block, then
n
brandom selections will be performed to obtain
S∗. Figure 1b illustrates the procedure of block bootstrapping.
Similarly to basic bootstrapping, cresamples are constructed,
and the CI was computed using these resamples.
Clearly, the size of the block (i.e., the number of selected
data points) is an important parameter. If the block is too largeor too small, then the internal dependency may be incorrectlyresampled. For this analysis, we used a ﬁxed block size of 24.However, in Metior , the block size is automatically adjusted
for each cloud application and cloud platform.
B. Cloud Performance Data Internal Dependency
and Bootstrapping
In this analysis, we used the performance test data provided
by the PT4Cloud data sets [13]. More speciﬁcally, the per-formance data of two benchmarks, the ftfrom NAS Parallel
Benchmark Suite (NPB) [31] and InMemory Analytics (IMA)
from the Cloud Suite [32], on two public clouds, Chameleon(CHM) and Amazon Web Service (AWS), are analyzed. Foreach benchmark, its one-week continuous execution perfor-mance data are used here. For Chameleon, the Large VM data
are used. For AWS, the m5.2xlarge VM data are used. More
details about these data sets are provided in Section IV.
1) Internal Dependence of Cloud Performance Test Data:
Figure 2 gives the trace of the execution times of IMA
when it was executed in AWS. Due to space limitation, wecannot show the execution time traces for ftand Chameleon,
although the same conclusion can be reached with them. AsFigure 2 shows, the execution times of IMA had considerable
ﬂuctuation, and these execution times showed some degreeof internal dependency and repeated patterns. In particular,continuous executions had similar execution times.
In addition to the visual illustration, we also evaluated the
internal dependency of these performance test data quantita-tively using Autocorrelation (ACF ) [23]. ACF computes the
internal Pearson Correlation Coefﬁcient (PCC) of a data set– it computes the PCC between the original data set and aderived data set obtained by shifting the data points in theoriginal data set by 1. Hence, ACF effectively evaluates theinternal dependency between two consecutive data points in
668(a) Original performance test data (ACF=0.44) (b) A basic bootstrapped resample (ACF=0) (c) A block bootstrapped resample (ACF=0.47)
Fig. 2. Traces of the execution times from a one-week performance test for IMA-AWS, including the original performance test data and two bootstrapped
resamples. Traces are down-sampled to 200 for better visibility.
Fig. 3. Traces of the ACFs of 1000 resamples generated by two bootstrapping
methods for IMA-AWS, along with the ACF of the original testing data.
TABLE I
ACF FOR EACH BENCHMARK ,AND THE A VERAGE ACF OF THE
RESAMPLES GENERATED BY TWO BOOTSTRAPPING (BT) METHODS .
Benchmark Original Basic BT Block BT
ft - CHM 0.24 0 0.23
f t-A W S 0.16 0 0.15
IMA - CHM 0.44 0 0.41
IMA - AWS 0.36 0 0.33
the series. Table I gives the ACF of the two benchmarks on
AWS and Chameleon. As Table I shows, the ACFs for thesebenchmarks are above 0.1, indicating the existence of internaldata dependency [33].
1
Prior studies on cloud performance also observed these
internal dependencies [13, 15]. The performance ﬂuctuationin the cloud is mainly caused by the hardware resourcecontention between simultaneously running VMs [34]. As theset of VMs that are running simultaneously usually do notchange rapidly, the performance of a cloud application usuallyalso varies little within a short period, which explains theexistence of the internal data dependency.
2) Bootstrapping and Internal Data Dependency: To il-
lustrate the impact of the internal data dependency on boot-strapping, we applied the basic and block bootstrapping tothe four performance test data sets of ftand IMA. For each
bootstrapping, 1000 resamples were generated (i.e., cis 1000),
following the standard practice [28, 29].
Figure 2 also shows the traces of two resamples using the
basic and block bootstrapping for IMA on AWS. As Figure 2
shows, the basic bootstrapping resample is more random thanthe block bootstrapping resample. Figure 3 gives the ACFs ofthese bootstrap resamples for IMA on AWS, which reveals
the main issue of the basic bootstrapping – its resamples
1Note that, ACF and PCC evaluate the existence of linear correlation.
Because the internal dependency of cloud performance data is unlikely
strongly linear, the ACFs for cloud performance data are usually less than 0.5.Here, we use ACF mainly to show the existence of internal dependency andshow that block bootstrapping preserves the same level of internal dependency.(a) IMA-AWS (b) IMA-Chameloen
(c) ft-AWS (d) ft-Chameloen
Fig. 4. The distributions and CIs for 90%ile execution times generated bytwo bootstrapping methods. Shaded areas show the ranges covered by the CIs.
usually had very low ACF. For the majority of the basic
bootstrap resamples, the ACF was nearly 0, indicating thatthese resamples nearly had no internal data dependencies.However, the ACFs of the resamples from block bootstrappingwere usually close to the original performance data.
Table I also reports the average ACF of the two bootstrap-
ping methods for all four performance data sets, where thebasic bootstrapping has average ACFs of nearly 0. However,block bootstrapping has average ACFs very close to theoriginal testing data, indicating that block bootstrapping canindeed preserve a similar level of internal dependency.
Fundamentally, the resampling of the basic bootstrapping is
different than how the original performance data are collected.In basic bootstrapping, the resampling assumes each executionis independent. However, for the original test, consecutiveexecutions had similar execution environments (e.g., similarco-running VMs). Hence, continuous executions are not inde-pendent, unlike assumed in the basic bootstrapping. However,the resampling in the block bootstrapping assumes consecutiveexecutions are correlated, and thus has a better resemblanceto the original performance test.
This difference in resample ACFs, in turn, leads to the
669Fig. 5. Numbers of performance tests that produced results (90%ile exec.
time) with less than 3% errors.
differences of the CIs generated based on these resamples.
Figure 4 shows the CIs of the 90%ile execution time for eachpair of benchmark and cloud. As Figure 4 shows, the CIsgenerated by the basic bootstrapping are usually smaller thanthose generated by block bootstrapping. Smaller CI indicatesthat basic bootstrapping estimated that the performance resultshad smaller errors. However, if the estimated error is smallerthan the actual error, the users would incorrectly assume theirperformance results are accurate and stop their tests too early.
At last, we conducted 100 performance tests using the
basic and block bootstrapping to test for the 90%ile executiontimes of the above benchmarks. For these tests, the maximumallowed error was set to be 3%. Therefore, the tests werestopped when the CIs were smaller than (−3%,+3%). After
the tests, the accuracy of the 90%ile execution times wascompared with the ground truth 90%ile execution times,where were calculated based on another 5-week data for eachbenchmark from the PT4Cloud data set. Figure 5 gives thenumber of accurate tests (i.e., had an error less than 3%).As Figure 5 shows, for fton both clouds and IMA on AWS,
block bootstrapping indeed increased the number of accuratetests. Therefore, block bootstrapping should be used in cloudperformance testing than the basic bootstrapping.
For IMA on Chameleon, however, the testing result accuracy
was worse with block bootstrapping. The worse results weredue to two issues. First, the CIs were generated based on aﬁxed set of testing data, and thus, may produce inaccurateresults if the test data do not cover all potential performanceﬂuctuations [13]. Second, we used a ﬁxed block size (24) inthese tests. However, the proper block size depends on thebehavior of the cloud application and the cloud. This resultshows that block bootstrapping cannot be blindly applied tocloud performance testing without addressing these two issues.
III. T
HEDESIGN OF Metior
This section presents our cloud performance testing method-
ology, Metior , which addresses the aforementioned two issues
of block bootstrapping. It also employs a periodical andintermittent AUT execution strategy to reduce testing cost.
To address the incomplete test coverage issue, the stop
criterion used by Metior employs the “law of large numbers,”
with states that the experimental mean should be close tothe true mean when the number of trials is large and tendto become closer to the true mean as the number of trialsincreases [35]. For cloud performance testing, this intuitionmay be rephrased as: if the mean from a large number ofexecutions in the cloud covers all potential ﬂuctuations and isclose to the true mean, then adding a substantial number ofmore executions should not signiﬁcantly change the value ofthe mean. Based on this intuition, the stop criterion of Metior
stops a test when the performance result obtained from the teststays unchanged after adding signiﬁcantly more executions.Note that, although the “law of large numbers” is only aboutthe mean, this intuition can also be applied to percentiles ofthe performance, as shown in Section IV.
To address the issue where the block size varies with cloud
application and cloud platform, we employed a methodologydeveloped by Politis and White to automatically select theblock size [30]. Intuitively, this automatic selection ﬁnds the“optimal” block size using the minimum block size thatprovides a non-negligible autocorrelation [30]. By adoptingthe automatic selection technique, the “optimal” block sizeallows block bootstrapping to retain similar level of internaldata dependency as the original sample. In Metior , for each
bootstrapping, this automatic block size selection is performedso that each performance test uses its own block size.
Metior is implemented as a fully automated performance
tester. To apply Metior , its user only need to provide the
VM conﬁguration, a cloud application (i.e., AUT) and itsinput data, a maximum allowed error, and a conﬁdencelevel. Metior automatically allocates VMs, conducts tests, and
applies bootstrapping, using a cloud service’s programminginterface. Currently, Metior support cloud services including
AWS, Google Cloud, and Chameleon. Note that, Metior is
not designed to generate or prioritizing inputs. Instead, it isdesigned to provide accurate cloud performance testing resultsfor any inputs.
A. Overview of Metior
Figure 6a gives the overall workﬂow of Metior . In Step 1,
Metior executes the AUT repeatedly for one day. Let the set
of performance data collected from these executions be S.I n
Step 2, Metior executes the AUT repeatedly for another day
to obtain a new data set T.Tis then combined with Sto
obtainS
/prime, i.e.,S/prime=S∪T. Note that, Section III-C provides
the rationale for conducting the tests in terms of days.
In Step 3, Metior compares SandS/primeto determine if there is
a signiﬁcant change in the performance results obtained from
StoS/prime. This comparison follows the intuition of the “law of
large numbers” – if the extra data in S/primedoes not signiﬁcantly
change the performance results, then the performance resultfromSis deemed accurate, and the test can be stopped.
However, if the change from StoS
/primeis signiﬁcant, Metior
deems that more executions are required. Hence, in Step 4,Metior letS
/primebecome the S. It then goes back to Step 2 to
conduct more executions and collect more performance data togenerate a new S
/prime. With the new SandS/prime, a new comparison
is performed at Step 3 to determine if the test can be stopped.
B. The Stop Criterion of Metior
Metior ’s stop criterion uses block bootstrapping to deter-
mine if there is a signiﬁcant change in the performance results
obtained from SandS/prime. Similar to existing cloud performance
testing methodologies, the user of Metior needs to select a
670Step 1: 
Execute the AUT forone day and collectperf data. Let the 
perf data set be  S.Step 2: 
Execute the AUT forone more day to collectperf data. Let new data
set be T. Combine S 
and T as data set S',i.e., S' = S  T
Step 4: Let S' be the new S,
i.e., S <= S'Stop test, and 
report the perf resultStep 3: 
compare S and S' to
check if S can provide
accu. perf result?
Yes NoStart
(a) Workﬂow of Metior .Step 3-1: 
Compute the perf 
result,  and ', and
their perc. diff  %,
from S and S'.
Inputs: S, S':  perf test datae%   :  max allowed errorcl%  :  conf levelStep 3-3: 
Is (-e%  L%) and 
(H%  e%)?Step 3-2: Block bootstrap S
and S' to establish the CI for % with 
cl% CL.  Let this CI  
be(L%,H%),  
S cannot  
provide 
accu. result.S can provide 
accu. result.Yes No
(b)Metior ’s stop criterion.
Fig. 6. Overall workﬂow of using Metior to conduct a performance test.
maximum allowed error, denoted by e%, and a conﬁdence
level, denoted by cl%. Metior uses block bootstrapping to
determine if the performance results obtained from Sand
S/primehave a maximum possible change (difference) larger than
e%withcl%conﬁdence. If the maximum possible change
(difference) is less than e%, then Metior deems that Scan
provide a performance result with less than e%error under
cl%conﬁdence. This stop criterion is based on the following
intuition – if both SandS/primecan provide accurate performance
results with less than e%error, then the maximum difference
between the performance results obtained from SandS/prime
should usually be smaller than e%.
Figure 6b gives the steps performed by Metior ’s stop
criterion. These steps correspond to the internal operations
of Step 3 in Figure 6a. The ﬁrst step of this stop criterion(Step 3-1 in Figure 6b) is to calculate the performance results,θandθ
/prime, fromSandS/prime. Let the percentage difference of θ
andθ/primebeδ%(δ%=θ/prime−θ
θ). In Step 3-2, Metior calculates
the CI of δ%withcl%conﬁdence. This CI represents the
maximum possible difference between the performance resultsobtained from SandS
/primewithcl%possibility. This CI is
calculated using the comparison block bootstrapping with thefollowing procedure [29]. First, two resamples, S
∗andS/prime∗,
are resampled from SandS/primeusing block bootstrapping. As
stated above, the block sizes are automatically selected basedon the data of SandS
/prime. Second, the percentage difference
δ∗%between the performance results (mean or percentile)
ofS∗andS/prime∗is computed. Third, the above resampling
is performed 1000 times, providing 1000 differences, i.e.,δ
∗
1%,δ∗
2%,...,δ∗
1000%. Fourth, the 1000 δ∗%s are sorted, and
the center cl%of the sorted list then gives the CI of δ%with
cl%conﬁdence.
Let the CI of δ%be(L%,H %). In Step 3-3, Metior checks
if the conditions, −e%<L%andH%<e%, are true. If
both conditions are true, then the maximum possible differencebetween the performance results of SandS
/primeis less than e%,
and the performance result of Sis considered to be accurate
byMetior . Hence, the test can be stopped. Otherwise, the test
continues to Step 4 in Figure 6a.
C. Low-cost Test Execution in Metior
Because cloud performance ﬂuctuates over time, we choose
to conduct the performance test in small intervals/periods of
executions rather than a speciﬁc number of executions. Thatis, in Figure 6a, Metior executes the AUT for one day in Step1 and 2. Moreover, our analysis on bootstrapping in Section IIshows that continuous executions usually have similar perfor-mance. Therefore, there is no need to continuously execute theAUT. Instead, the AUT can be executed intermittently whilestill providing good coverage of performance ﬂuctuations. Ascloud usage cost is charged in terms of seconds or minutes,intermittently execution can reduce the number of executionsand the cloud usage expenditure.
Nevertheless, the interval length and intermittent frequency
must be carefully chosen to obtain accurate performanceresults. Prior work employed a similar periodical and intermit-tent execution strategy [13]. However, the prior work did notprovide a systematic approach to determine the interval lengthand intermittent frequency. Here, we employed a data-drivenapproach. To determine the interval length, we evaluated the“optimal” block sizes for the performance tests data of ftand
IMA in Section II using the aforementioned automatic block
size selection technique. The largest block size we found was24 hours of executions. Therefore, to ensure the executioncount in Step 2 is indeed signiﬁcantly large, we set the intervallength to be one day so that Step 2 can provide at least onenew block of data. To determine the intermittent frequency,we again used the performance test data from Section II todetermine how many executions can be discarded from the testdata without signiﬁcantly reducing the autocorrelation (ACF).We discovered that it needed at least four executions per hourto maintain an ACF similar (less than 0.1 smaller) to theoriginal data. We used 0.1 as the threshold because less-than-0.1 ACF is considered as no correlation [33].
In summary, based on the above analysis, in Step 1 and 2 of
Metior , we choose to execute the AUT intermittently 4 times
per hour and repeat hourly for one day.
IV . E
XPERIMENTAL EV ALUATION
This section describes the experimental evaluation of
Metior , which answers the following two research questions:
1) What is the accuracy of the performance results obtainedMetior ? 2) What is the cost of applying Metior ?
A. Experiment Setup
Benchmarks and Clouds. For this evaluation, we used the
performance data sets from PT4Cloud [13]. PT4Cloud datasets provide the performance data of executing six benchmarkson two public clouds, AWS and Chameleon, continuouslyfor eight (8) weeks. Table II gives the details of these sixbenchmarks, and Table III gives the types and counts of the six
671TABLE II
BENCHMARKS USED IN THE EV ALUATION .
Benchmark Domain Type of Perf. Origin
ft HPC execution time NPB [31]
ep HPC execution time NPB [31]
JPetStore (JPS) Web throughput J2EE [36]
YCSB DB throughput YCSB [37]
TPC-C DB throughput OLTPBench [38]
InMem Analy. (IMA) ML execution time CloudSuite [32]
TABLE III
VM CONFIGURATIONS FROM CHAMELEON (C) AND AWS (A) USED IN
THE EV ALUATION .
Conﬁg. VM Cnt x VM Type Cores / VM M e m/V M
C-S 4 x Small 1 2GB
C-M 2 x Medium 2 4GB
C-L 1 x Large 4 8GB
A-S 4 x m5.large 2 8GB
A-M 2 x m5.xlarge 4 16GB
A-L 1 x m5.2xlarge 8 32GB
Virtual Machine (VM) conﬁgurations used in PT4Cloud. Each
benchmark was executed on all six VM conﬁgurations, exceptfor benchmarks ft,ep, and IMA, which could not be executed
on the small VMs of Chameleon (i.e., C-Sm in Table III) due
to inefﬁcient memory. Here, a pair of benchmark and VMconﬁguration is called a benchmark conﬁguration. In total, all
33 benchmark conﬁgurations from PT4Cloud were evaluated.
Evaluation Methodology. We partitioned the 8-week per-
formance data for each benchmark conﬁguration into twoparts. The ﬁrst part contained 3-week of data and was usedto conduct performance tests, whereas the rest 5-week datawere used as ground truth. The 3-week data have a largenumber of performance data points, with each data pointcontains the performance (execution time or throughput) ofone execution. From each data point, a performance test couldbe conducted (simulated). For instance, starting from a datapoint, a performance test with Metior can be simulated by
continuously reading in data points until Metior ’s stop criterion
deems that the performance test can be stopped. After theperformance test is stopped, the data points read during thetest can then be used to calculate the performance results.Because the 3-week data of each benchmark conﬁgurationcontain hundreds or thousands of data points, repeating thesimulated performance test starting from every data point ledto hundreds or thousands of simulated performance tests foreach benchmark conﬁguration, allowing a thorough evaluation.
In this evaluation, the performance tests were used to obtain
performance results as the mean and 90%ile of the executiontime or throughput, reﬂecting the average and tail performance.The maximum allowed errors (i.e., the e%in Figure 6) were
set to be 3%, 5%, and 10%. The conﬁdence level (i.e., thecl%in Figure 6) was chosen to be 95%.
Baselines. As stated above, the last 5-week data of each
benchmark conﬁguration were used to obtain the ground truthperformance. For cloud performance testing, the ground truthshould be the performance results obtained from extremelylong performance tests that can truly cover all performanceﬂuctuations. Our current performance results showed that theperformance results from 4 (or more) weeks of executionsare usually stable enough to be used as ground truth. Besidesthe ground truth, we also compared Metior with three state-
of-the-art cloud performance testing methodologies. The ﬁrstmethodology, BasicBT, used the basic bootstrapping [12]. Thesecond methodology, CoV, uses the changes in the coefﬁcientof variation of the performance data as the stop criterion [20].The third methodology is PT4Cloud [13].
Metrics. The percentage error of each performance result
obtain with Metior ,Perf
Metior , is calculated by comparing
it with the ground truth performance, Perftrue, using the
following equation,
err=|Perf Metior−Perftrue
Perftrue|×100%. (1)
Because large numbers of performance tests were conducted
using Metior for each benchmark conﬁguration, we report the
percentage of tests that provided performance results withless than the maximum allowed error. For example, if themaximum allowed error is 3%, then we report the percentageof the performance tests that indeed provided performanceresults with less than 3% error. Moreover, recall that, in thispaper, we call a performance test result as accurate if it indeed
has an error less than the maximum allowed error.
Open Data. Our data and source code are available at
https://doi.org/10.5281/zenodo.5093934.
B. Accuracy Evaluation with 3% Maximum Allowed Error
1) Accuracy of Metior: Figure 7 gives the percentage of
the Metior performance tests that provided accurate mean
performance (i.e., with less than 3% error). As Figure 7 shows,
100% of Metior ’s tests conducted on AWS provided mean
performance with less than 3% error. On Chameleon, exceptfor benchmark conﬁgurations of JPS-C-S and TPCC-C-M,
Metior ensured more than 90% of the tests were accurate for
all benchmark conﬁgurations. The benchmark performance onChameleon had larger ﬂuctuations than AWS, making it moredifﬁcult to obtain accurate results. Nonetheless, even for JPS-
C-Sand TPCC-C-M, the tests could still provide performance
results with low error. For JPS-C-S, the largest error among
all the tests conducted for it was only 4.2%.
2For TPCC-C-
M, the largest error was 5.8%. Figure 9a gives the averagepercentages of Metior ’s tests for mean performance that were
accurate. Overall, 96% of the tests on Chameleon and 100%of AWS tests had less than 3% errors. 98% of all tests on twoclouds had less than 3% errors.
Figure 8 gives the percentage of the Metior performance
tests that provided accurate 90%ile performance (i.e., with lessthan 3% error). Again, 100% of Metior ’s tests conducted on
AWS provided 90%ile performance with less than 3% errors.On Chameleon, except for the conﬁgurations of YCSB-C-S
2Because hundreds or thousands of tests were conducted for each bench-
mark conﬁguration, it is impossible to provide the error for each test in
Figure 7 and Figure 8. Due to space limitation, we also cannot provide themax and average error for tests of each benchmark conﬁguration.
672FT (C-L) FT (C-M) EP (C-L) EP (C-M) IMA (C-L) JPS (C-L) JPS (C-M) JPS (C-S)TPCC (C-L) TPCC (C-M)FT (A-L) FT (A-M) FT (A-S) EP (A-L) EP (A-M) EP (A-S)IMA (A-L) IMA (A-M) IMA (A-S) JPS (A-L) JPS (A-M) JPS (A-S)TPCC (A-L) TPCC (A-M) TPCC (A-S) YCSB (A-L) YCSB (A-M) YCSB (A-S)0102030405060708090100Percentage (%)BasicBT CoV PT4Cloud Metior
Fig. 7. Percentages of the tests that provided mean performance results with less-than-3% errors.
FT (C-L) FT (C-M) EP (C-L) EP (C-M) IMA (C-L) IMA (C-M)JPS (C-L) JPS (C-M) JPS (C-S)TPCC (C-L) TPCC (C-M) TPCC (C-S) YCSB (C-L) YCSB (C-S)FT (A-L) FT (A-M) FT (A-S) EP (A-L) EP (A-M) EP (A-S)IMA (A-L) IMA (A-M) IMA (A-S) JPS (A-L) JPS (A-M) JPS (A-S)TPCC (A-L) TPCC (A-M) TPCC (A-S) YCSB (A-L) YCSB (A-M) YCSB (A-S)0102030405060708090100Percentage (%)BasicBT CoV PT4Cloud Metior
Fig. 8. Percentages of the tests that provided 90%ile performance results with less-than-3% errors.
A-3% C-3% T-3% A-5% C-5% T-5% A-10% C-10% T-10%5060708090100Percentage (%)BasicBT CoV PT4Cloud Metior
(a) Tests for mean performance.
A-3% C-3% T-3% A-5% C-5% T-5% A-10% C-10% T-10%405060708090100Percentage (%)BasicBT CoV PT4Cloud Metior
(b) Tests for 90%ile performance.
Fig. 9. Summary of the percentages of the performance tests that were
accurate. Results are reported for three (3%, 5%, and 10%) maximum allowederrors on AWS (A), Chameleon (C) and all tests (T).
and YCSB-C-L, Metior ensured more than 90% of the tests
were accurate for all benchmark conﬁgurations. Similar to the
mean performance, Metior ’s performance results still had low
errors for YCSB-C-S and YCSB-C-L – the maximum errors of
the tests conducted for YCSB-C-S and YCSB-C-L were only
3.6% and 4.6%, respectively.2Figure 9b gives the average
percentages of Metior ’s tests for 90%ile performance that were
accurate. Overall, 96% of the Chameleon tests and 100% ofAWS tests had less than 3% errors. 98% of all tests on twoclouds had less than 3% errors.
Note that, for mean performance results, the tests for ﬁve
benchmark conﬁgurations could not stop with just 3-week ofdata. For 90%ile performance results, the performance testsfor YCSB-C-M could not stop with 3-week of data. These
benchmark conﬁgurations are not shown in Figure 7 andFigure 8, but are discussed in the next section.
2) Unstoppable Benchmark Conﬁgurations: For the mean
performance, for ﬁve benchmark conﬁgurations, IMA-C-M,
TPCC-C-S, YCSB-C-L, YCSB-C-M, and YCSB-C-S, Metior
deemed that all their performance tests should not stop (i.e.,could not provide accurate results) with just 3-week data. Forthe 90%ile performance, Metior deemed that all the tests of
YCBS-C-M should not stop with just 3-week data. Therefore,
the results of these benchmark conﬁgurations are not shown inFigure 7 and Figure 8. Note that, as PT4Cloud only provided8 weeks of performance data, it was also impossible for usto continue the tests without running into ground truth data.Therefore, we terminated the tests once all 3-week data wereused and report these tests as unstoppable.
For three benchmark conﬁgurations, IMA-C-M, TPCC-C-S,
and YCSB-C-M, 3-week data indeed could not provide mean
or 90%ile performance with less than 3% error. For YCSB-C-L
and YCSB-C-S, although 3-week data could provide accurate
performance results, their performance data had high varia-tions. These high variations resulted in wide CIs for their meanor 90%ile performance, making it impossible to conclude thatthe performance results from 3-week data were accurate with95% conﬁdence. Therefore, we concluded that Metior indeed
should not stop the tests and behaved as expected for thesebenchmark conﬁgurations. These unstoppable tests also reﬂectthe difﬁculty of conducting cloud performance testing underlarge performance ﬂuctuation and show the importance ofexploring reliable cloud performance testing methodologies.
6733) Comparison with PT4Cloud: The percentages of the
accurate tests for PT4Cloud are also given in Figure 7 and
Figure 8. As the ﬁgures show, except for the above fourconﬁgurations, Metior had more accurate tests than PT4Cloud
for all other benchmark conﬁgurations. When testing for themean of IMA-C-L and 90%ile of TPCC-C-M, PT4Cloud was
slightly better than Metior with less than 5% more accurate
tests. When testing for the mean of TPCC-C-M and 90%ile
ofYCSB-C-L, P4Cloud had 17% and 22% more accurate tests
than Metior . Nonetheless, when testing for the mean of TPCC-
C-M, the average and maximum errors of the tests conductedby PT4Cloud and Metior were similar – the average errors
for PT4cloud and Metior were 1% and 2%, and the maximum
errors for PT4Cloud and Metior were 3.8% and 5.8%.
2For
the 90%ile tests of YCSB-C-L, the average errors of PT4cloud
and Metior were both 3%, whereas PT4Cloud maximum error
(8.6%) was higher than Metior (4.6%). Metior ’s similar or
even better average or maximum errors show that althoughMetior had fewer accurate tests for these two conﬁgurations,
the accuracy of individual tests of Metior was still similar to
those of PT4Cloud, and Metior ’s tests also had errors very
close to the 3% maximum desired error. Moreover, Metior
was more accurate for all other benchmark conﬁgurations.
Figure 9 also gives the overall accuracy of PT4Cloud,
which shows that PT4Cloud’s overall accuracy was similar orworse than Metior . Especially when testing for the 90%ile on
Chameleon with 3% max error, Metior has 17% more accurate
tests than PT4Cloud. PT4Cloud stopped the test with theanticipation that the distribution (i.e., most of the percentiles)had less than 3% error. However, it did not imply that the meanor every percentile also had less than 3% error. Therefore, theerrors for some point estimates may still be larger than 3%,causing P4Cloud to have lower accuracy than Metior .
Note that, for several benchmark conﬁgurations, PT4Cloud
could not stop the tests with 3-week of data (in addition tothose discussed in Section IV-B2). The PT4Cloud bars of theseconﬁgurations are omitted in Figures 7 and 8.
4) Comparison with BasicBT method: Figures 7 and 8
also show the percentages of tests that had less than 3%errors for BasicBT [12]. As both ﬁgures show, BasicBT hadlower or similar accuracy than Metior for every benchmark
conﬁguration. In several cases, BasicBT was signiﬁcantly lessaccurate than Metior , such as JPS-C-L and TPCC-C-L.
Figure 9 gives the overall accuracy of BasicBT. On AWS,
BasicBT had performed reasonably well with 97% of allBasicBT’s tests had less than 3% errors. Nonetheless, it wasstill lower than the 100% of Metior . Moreover, the large
performance ﬂuctuation on Chameleon had made it particu-larly difﬁcult for BasicBT to stay accurate. On Chameleon,78% of BasicBT’s tests for mean and 66% of BasicBT’stests for 90%ile had less than 3% errors, whereas 96% ofMetior ’s tests on either mean or 90%ile were accurate. As
analyzed in Section II, BasicBT has two issues of incompleteperformance ﬂuctuation coverage and not considering internaldata dependency, causing the relatively low accuracy.Mean 90%ile Average0123
0.2
0.04
0.27
0.04
0.24
0.04Normalized
ExecutionsBasicBT CoV Metior PT4Cloud
Fig. 10. The number of executions conducted by evaluated testing method-
ologies, normalized to Metior .
5) Comparison with the CoV method: Figures 7 and 8 also
show the percentages of tests that had less than 3% errors for
the CoV method [20]. Overall, CoV had the lowest accuracyamong the three cloud performance testing methodologies. OnAWS, 89% of CoV’s tests could provide accurate means or90%iles. On Chameleon, 61% of CoV’s tests could provideaccurate means, and 45% of its tests could provide accurate90%ile performance. The CoV method was not designed toobtain accurate performance results with a maximum desirederror. Instead, it was designed to detect if the performanceresults from microbenchmarks were stable enough. Hence,because of having a different design goal, CoV’s accuracywas lower than the other methodologies in this evaluation.
C. Testing Cost for 3% Max Allowed Errors
To obtain the mean performance, Metior conducted 290
executions per test on average. For the 90%ile performance,
Metior conducted 306.2 executions per test on average. These
executions roughly translate into three days of execution pertest. As we used existing data sets, we do not have the AWSbills for these tests. Therefore, we estimated the testing costbased on the test execution time and AWS’s cloud usage rates(Chameleon is a free research cloud without charges). Theestimation shows that Metior ’s cost for one test on AWS
ranged from $1.2 to $57.6, with an average cost of $17.6.
Figure 10 compares the average number of executions per
test required by the evaluated performance testing method-ologies. As Figure 10 shows, the number of executions re-quired by PT4Cloud was 3.1 times of those used by Metior
on average. This high number of executions was becausePT4Cloud was designed to obtain performance distributions.When a whole distribution is deemed accurate (i.e., with lessthan 3% error) by PT4Cloud, most of the percentiles of theperformance distribution had about 3% error, which requiredmany more tests than just obtaining one accurate mean orpercentile. Consequently, PT4Cloud incurs unnecessary costsfor performance tests that only need to obtain point estimates,and its cost may be prohibitively high when a cloud applicationhas a large number of inputs that need to be tested. Note that,Metior ’s reduction in execution count does not only imply less
monetary cost, it also indicates a reduction in temporal cost,and hence, a faster development/deployment cycle.
BasicBT and CoV required fewer executions than Metior ,
as shown in Figure 10. Speciﬁcally, the average executioncounts per test for BasicBT were 20% to 27% of Metior ’s
execution counts, and CoV’s execution counts were about 4%
674ofMetior . However, as BasicBT and CoV had lower accuracy
than Metior , these fewer executions did not indicate a cost
reduction, but indicated that BasicBT and CoV stopped their
tests too early before obtaining accurate performance results.
D. Sensitivity to Maximum Allowed Errors
We also evaluated Metior with 5% and 10% maximum
allowed errors. The evaluation results are summarized in
Figure 9. As Figure 9 shows, Metior ’s retained its accuracy
when the maximum allowed errors were increased. On AWS,100% of Metior ’s tests were still accurate. On Chameleon,
more than 92% of Metior ’s tests were accurate when the
maximum error was 5%, and more than 94% of Metior ’s tests
were accurate when the maximum error was 10%.
The accuracy of PT4Cloud, BasicBT, and CoV was also
improved under 5% and 10% maximum errors. However,PT4Cloud still had lower overall accuracy than Metior . More-
over, the numbers of executions per test of PT4Cloud werestill 3.2 and 3.6 times more than Metior for 5% than 10%
maximum errors. BasicBT still struggled on Chameleon andhad considerably lower accuracy than Metior even with larger
maximum errors. The accuracy of CoV was even lower thanBasicBT, as it is not designed to obtain performance withspeciﬁed maximum errors.
V. C
ASE STUDY AND APPLICATION OF Metior
A Case Study. To demonstrate the usefulness of Metior
to cloud researchers and practitioners, we applied Metior to
a cloud optimization technique, CherryPick [6], which selectsthe best VM conﬁguration for a cloud application by predictingits performance on different VM conﬁgurations. CherryPickemploys Gaussian Process (GP) to make the prediction [39],which requires a training data set which consists of theperformance of the application running in other VMs. Here,we applied Metior to obtain accurate performance results as
the training sets to show that Metior can improve CherryPick’s
prediction accuracy.
More speciﬁcally, CherryPick was used to predict the per-
formance of the six benchmarks in Table II when they wereexecuting on the AWS 2× m5.xlarge conﬁguration (i.e., A-
M). The training data sets were composed of performance
data points for ﬁve VM conﬁgurations, including 4× m5.large
(A-S), 1× m5.2xlarge (A-L ), 1× m5.large, 2× m5.large and
1×m5.xlarge. The original CherryPick techniques asked for
6 data points for each VM conﬁguration in the training set.However, for the Metior -enhanced CherryPick, the data points
for each VM conﬁguration must be many enough to providean accurate mean using the Metior methodology. That is,
Metior was used to obtain accurate mean performances for
each VM conﬁguration, then the acquired mean performancesof each VM conﬁguration were used as training data sets forCherryPick to make predictions. New tests were conductedfor the VM conﬁgurations (i.e., 1× m5.large, 2× m5.large and
1×m5.xlarge) that are not included in the PT4Cloud data
sets. For a thorough evaluation, 10000 performance predictionsJPS ft ep IMA YCSB TPCC Avg020406080100Perc error (%)Original Metior -enhanced
Fig. 11. Average prediction errors for the original CherryPick and Metior -
enhanced CherryPick.
were made using the original and Metior -enhanced Cherryp-
ick. Then for each performance prediction, its percentage error
was computed by comparing the prediction with the groundtruth performance, which was the same ﬁve-week performanceused in the previous evaluation. The mean absolute percentageerrors (MAPE) are then reported in Figure 11. Note that, thiscase study was only conducted on AWS, as Chameleon onlyoffered three VM conﬁgurations (CherryPick requires morethan 3 conﬁgurations).
As Figure 11 shows, Metior -enhanced CherryPick had
higher accuracy than the original CherryPick for every bench-mark. On average, Metior -enhanced CherryPick’s prediction
error was 17.3% less than the original CherryPick. Moreover,Metior -enhanced CherryPick had more stable predictions (i.e.,
less variation in prediction accuracy). The main beneﬁt ofMetior is that it can provide more accurate training data
sets, which contained many more samples than the originalCherryPick. Note that, simply adding more training data donot always increase CherryPick’s accuracy, because without areliable performance testing methodology, it is unclear howmany more training data should be added to ensure goodaccuracy. The authors of CherryPick also noted that GPwas considered as just accurate enough to separate fast VMconﬁgurations from slower ones. Nonetheless, a later studyshowed that a GP model with better accuracy could improvethe accuracy of identifying better-performing VMs [40]. Thehigher accuracy could improve CherryPick’s ability at VMconﬁguration optimization [40]. These results illustrate that, byproviding more reliable training sets and accurate performanceresults, Metior is valuable for both cloud research and practice.
Application of Metior. The necessity of accurate cloud
performance results was documented by prior cloud perfor-mance test studies [9, 12, 13, 20]. In cloud deployments,a key step is to select the proper VM conﬁguration thatmeets the performance requirement [1, 6, 41]. The selection ofauto-scaling policies also requires determining a proper VMconﬁguration that meets the performance requirement as thescaling target [5, 42, 43]. The most reliable way to determineif a VM conﬁguration meets a performance requirement isperformance testing. For cloud research, obtaining accurateperformance results is also the fundamental requirement. Ac-curate performance results are required to evaluate new cloudsystem/application designs [44] and develop new optimizationtechniques (as shown with the case study).
675VI. T HREATS TO VALIDITY
Execution Environment Changes. Metior assumes that
the execution environments, including the cloud hardware
infrastructure and the statistical behavior of multi-tenancy,remain unchanged during the performance test and after thedeployment. In our experience, the hardware infrastructure atChameleon and AWS remained unchanged for years, and themulti-tenancy behaviors were also consistent within at leasta year. Therefore, performance tests conducted with Metior
with only a few days or weeks of executions can accuratelyprovide the performance of cloud deployments within at leastone year. However, if the execution environment changes, newperformance tests should be conducted.
Nonetheless, the cloud execution environment does experi-
ence long-term (e.g., after multiple years) changes. Therefore,new performance testing methodologies are required to tacklethis long-term performance change. We plan to redesignMetior into cloud APIs to allow it to detect performance
variations caused by execution environment changes.
Other Cloud Applications, Test Inputs, and Cloud Ser-
vice providers. Although we strive to provide a comprehen-sive evaluation, the exact accuracy of Metior may change with
the cloud applications, performance test inputs, and the cloudservice providers. Nonetheless, we expect Metior ’s behavior
to be generally consistent over a variety of cloud applications,inputs, and cloud services. Moreover, Metior is used to obtain
the accurate performance of a cloud application given one testinput. Metior does not aim at determining what test inputs
should be included in the performance tests.
VII. R
ELATED WORK
Cloud Performance Testing. Performance testing is a fun-
damental task in computer science [10, 11]. Jain documentedthe methodology of using CI for performance measurement indetail [19]. Maricq et al. recently improved this methodologyto cloud computing by employing the basic bootstrapping [12].Wang et al. also employed basic bootstrapping in cloudperformance testing [9]. However, as shown in this paper,the basic bootstrapping based testing methods had loweraccuracy partially due to overlooking internal data depen-dency. PT4Cloud was a performance testing technique forobtaining performance distributions of cloud applications [13].Our work is inspired by PT4Cloud, especially in the use ofperiodical and intermittent executions. However, PT4Clouddetermined the parameters of interval length and intermittentfrequency empirically, whereas Metior employed a systematic
approach to select these parameters. Moreover, as shownin Section IV-B3, PT4Cloud had higher errors and highercosts than Metior . Laaber et al. proposed a stop criterion for
executing microbenchmark in the cloud using the coefﬁcientof variation [20]. However, as shown in Section IV-B3, thisstop criteria had lower accuracy, as it was not designed toobtain performance results with a maximum allowed error.Alghmadi et al. proposed a stop condition for performancetesting by determining the repetitions in performance data [45].As shown in prior work, this stop condition was not suitablefor testing performance in cloud computing [13]. Duet bench-marking is a technique to compare the performance of twocloud system designs or optimizations [44, 46]. While thistechnique provides accurate comparisons, it was not designedto determine the exact performance of a cloud application.
Other Related Work Cloud performance prediction models
were built to predict a cloud application’s performance on aVM conﬁguration or cloud service to aid cloud resource allo-cation [1, 6, 40, 47]. These models used performance testingresults as their training data. Some studies also predicted andestimated non-cloud software performance with models andsimulators [48–55]. As shown with our case study (Section V),Metior can provide more accurate cloud performance results
as reliable training and testing data sets to facilitate the devel-opment of performance modeling and simulation. There werealso studies on test inputs generation and prioritization forperformance testing [43, 56, 57, 57–70]. Several studies alsoinvestigated performance change identiﬁcation in conﬁgurablesystems [71, 72]. These studies are orthogonal to Metior ,a s
Metior focused on providing accurate performance results for
any test inputs and/or conﬁgurations.
VIII. C
ONCLUSION AND FUTURE WORK
This paper addressed the cloud performance testing prob-
lem. We ﬁrst conducted an analysis to show that the basicbootstrapping could not always provide accurate performanceresults due to the overlooked internal dependency with cloudperformance data. We then present Metior , a reliable auto-
mated performance testing methodology using block boot-strapping, which considers the internal dependency of cloudperformance data. To improve performance ﬂuctuation cov-erage and further reduce testing cost, Metior also employed
the “law of large numbers” and conducted tests periodicallyand intermittently. Experiment results showed that Metior
ensured that more than 98% of tests had less than 3% error.For future work, we will use Metior to test different types
of cloud services, more cloud service providers, as well asadditional types of applications. We also plan to implementMetior as a serverless API so that it can easily used by
cloud practitioners for performance testing and for long termperformance ﬂuctuation monitoring.
A
CKNOWLEDGMENT
This work was supported by the National Science Foun-
dation under grants CCF-1617390, CCF-1618310, and CNS-1911012. The views and conclusions contained herein arethose of the authors and should not be interpreted as neces-sarily representing the ofﬁcial policies or endorsements, eitherexpressed or implied of NSF. The authors would like to thankthe anonymous reviewers for their insightful comments.
R
EFERENCES
[1] Neeraja J. Yadwadkar, Bharath Hariharan, Joseph E. Gonzalez,
Burton Smith, and Randy H. Katz. Selecting the Best VM
Across Multiple Public Clouds: A Data-driven PerformanceModeling Approach. In ACM Symp. on Cloud Computing, 2017.
676[2] Stoyan Stefanov. YSlow 2.0. In CSDN Software Development
2.0 Conference, 2008.
[3] Marissa Mayer. In Search of A better, faster, strong Web, 2009.
[4] Timothy Zhu, Michael A. Kozuch, and Mor Harchol-Balter.
WorkloadCompactor: Reducing datacenter cost while providingtail latency SLO guarantees. In ACM Symp. on Cloud Comput-
ing, 2017.
[5] Ming Mao and Marty Humphrey. Auto-scaling to Minimize
Cost and Meet Application Deadlines in Cloud Workﬂows. InProc. of Int’l Conf. for High Performance Computing, Network-ing, Storage and Analysis, 2011.
[6] Omid Alipourfard, Hongqiang Harry Liu, Jianshu Chen, Shiv-
aram Venkataraman, Minlan Yu, and Ming Zhang. CherryPick:Adaptively Unearthing the Best Cloud Conﬁgurations for BigData Analytics. In USENIX Symp. on Networked Systems
Design and Implementation, 2017.
[7] F. L. Ferraris, D. Franceschelli, M. P. Gioiosa, D. Lucia,
D. Ardagna, E. Di Nitto, and T. Sharif. Evaluating the AutoScaling Performance of Flexiscale and Amazon EC2 Clouds. InInt’l Symp. on Symbolic and Numeric Algorithms for ScientiﬁcComputing, 2012.
[8] Mark Grechanik, Qi Luo, Denys Poshyvanyk, and Adam Porter.
Enhancing Rules For Cloud Resource Provisioning Via LearnedSoftware Performance Models. In ACM/SPEC on Int’l Conf. on
Performance Engineering, 2016.
[9] W. Wang, N. Tian, S. Huang, S. He, A. Srivastava, M. L.
Soffa, and L. Pollock. Testing Cloud Applications under Cloud-Uncertainty Performance Effects. In Int’l Conf. on Software
Testing, V eriﬁcation and V alidation, 2018.
[10] Yutong Zhao, Lu Xiao, Xiao Wang, Bihuan Chen, and Yang Liu.
Localized or Architectural: An Empirical Study of PerformanceIssues Dichotomy. In Int’l Conf. on Software Engineering:
Companion Proceedings, 2019.
[11] Andreas Burger, Heiko Koziolek, Julius R ¨uckert, Marie
Platenius-Mohr, and G ¨osta Stomberg. Bottleneck Identiﬁca-
tion and Performance Modeling of OPC UA CommunicationModels. In Proc. of ACM/SPEC Int’l Conf. on Performance
Engineering, ICPE ’19, 2019.
[12] Aleksander Maricq, Dmitry Duplyakin, Ivo Jimenez, Carlos
Maltzahn, Ryan Stutsman, and Robert Ricci. Taming Perfor-mance Variability. In USENIX Symp. on Operating Systems
Design and Implementation, 2018.
[13] Sen He, Glenna Manns, John Saunders, Wei Wang, Lori Pol-
lock, and Mary Lou Soffa. A Statistics-Based PerformanceTesting Methodology for Cloud Applications. In Proc. of ACM
Joint Meeting on European Software Engineering Conf. andSymp. on the F oundations of Software Engineering, 2019.
[14] Philipp Leitner and J ¨urgen Cito. Patterns in the Chaos: A
Study of Performance Variation and Predictability in Public IaaSClouds. ACM Trans. Internet Technol., 16(3):15:1–15:23, April
2016.
[15] Alexandru Iosup, Simon Ostermann, Nezih Yigitbasi, Radu
Prodan, Thomas Fahringer, and Dick Epema. PerformanceAnalysis of Cloud Computing Services for Many-Tasks Sci-entiﬁc Computing. IEEE Transcations on Parallel Distributed
System, 22(6):931–945, June 2011.
[16] Christoph Laaber, Joel Scheuner, and Philipp Leitner. Software
Microbenchmarking in the Cloud. How Bad is It Really?Empirical Software Engineering, 24(4):2469–2508, 2019.
[17] Catia Trubiani, Pooyan Jamshidi, Jurgen Cito, Weiyi Shang,
Zhen Ming Jiang, and Markus Borg. Performance Issues? HeyDevOps, Mind the Uncertainty. IEEE Software, 36(2):110–117,
2019.
[18] James E. II Bartlett, W. Joe Kotrlik, and Chadwick C. Higgins.
Organizational research: Determining appropriate sample sizein survey research. Information Technology, Learning, and
Performance Journal, 19(1):43, 2001.[19] Raj Jain. The Art of Computer Systems Performance Analysis:
Techniques for Experimental Design, Measurement, Simulation,and Modeling. John Wiley & Sons, 1990.
[20] Christoph Laaber, Stefan W ¨ursten, Harald C. Gall, and Philipp
Leitner. Dynamically Reconﬁguring Software Microbench-marks: Reducing Execution Time without Sacriﬁcing ResultQuality . In Proc. of European Software Engineering Conference
and Symp. on the F oundations of Software Engineering, 2020.
[21] David Shue, Michael J. Freedman, and Anees Shaikh. Perfor-
mance Isolation and Fairness for Multi-tenant Cloud Storage.InProc. of USENIX Conf. on Operating Systems Design and
Implementation, 2012.
[22] Soumendra Nath Lahiri. Resampling Methods for Dependent
Data. Springer Science & Business Media, 2013.
[23] John A Gubner. Probability and Random Processes for Elec-
trical and Computer Engineers. Cambridge University Press,2006.
[24] Amazon. Amazon Web Services. https://aws.amazon.com.
[Online].
[25] Google. https://cloud.google.com/appengine/docs. [Online].[26] A Conﬁgurable Experimental Environment for Large-scale
Cloud Research. https://www.chameleoncloud.org/. [Online].
[27] A Colin Cameron and Pravin K Trivedi. Microeconometrics:
Methods and Applications. Cambridge university press, 2005.
[28] Bradley Efron. The Jackknife, the Bootstrap and Other Resam-
pling Plans. SIAM, 1982.
[29] A. C. Davison and D. V . Hinkley. Bootstrap Methods and Their
Application. Cambridge University Press, 2013.
[30] Dimitris N. Politis and Halbert White. Automatic Block-Length
Selection for the Dependent Bootstrap. Econometric Reviews,
23(1):53–70, 2004.
[31] D.H. Bailey, E. Barszcz, J.T. Barton, D.S. Browning, R.L.
Carter, L. Dagum, R.A. Fatoohi, P.O. Frederickson, T.A. Lasin-ski, R.S. Schreiber, et al. The NAS Parallel BenchmarksSummary and Preliminary Results. In Int’l Conf. on Super-
computing, 1991.
[32] Michael Ferdman, Almutaz Adileh, Onur Kocberber, Stavros
V olos, Mohammad Alisafaee, Djordje Jevdjic, Cansu Kaynak,Adrian Daniel Popescu, Anastasia Ailamaki, and Babak Falsaﬁ.Clearing the Clouds: A Study of Emerging Scale-out Workloadson Modern Hardware. In Int’l Conf. Architectural Support for
Programming Languages and Operating Systems, 2012.
[33] Patrick Schober, Christa Boer, and Lothar A Schwarte. Corre-
lation Coefﬁcients: Appropriate Use and Interpretation. Anes-
thesia & Analgesia, 126(5):1763–1768, 2018.
[34] M. Hajjat, R. Liu, Y . Chang, T. S. E. Ng, and S. Rao.
Application-speciﬁc conﬁguration selection in the cloud: Impactof provider policy and potential of systematic testing. In IEEE
Conf. on Computer Communications (INFOCOM), 2015.
[35] Frederik Michel Dekking, Cornelis Kraaikamp, Hendrik Paul
Lopuha ¨a, and Ludolf Erwin Meester. A Modern Introduction to
Probability and Statistics. Springer Science & Business Media,2005.
[36] Oracle. Java Platform, Enterprise Edition (Java EE) 7.
https://docs.oracle.com/javaee/7/index.html. [Online].
[37] Brian F. Cooper, Adam Silberstein, Erwin Tam, Raghu Ra-
makrishnan, and Russell Sears. Benchmarking Cloud ServingSystems with YCSB. In Proc. of ACM Symposium on Cloud
Computing, 2010.
[38] Djellel Eddine Difallah, Andrew Pavlo, Carlo Curino, and
Philippe Cudre-Mauroux. OLTP-Bench: An Extensible Testbedfor Benchmarking Relational Databases. Proc. VLDB Endow.,
7(4), December 2013.
[39] Carl Edward Rasmussen. Gaussian processes in machine
learning. In Summer School on Machine Learning, pages 63–71.
Springer, 2003.
[40] C. Hsu, V . Nair, V . W. Freeh, and T. Menzies. Arrow: Low-Level
677Augmented Bayesian Optimization for Finding the Best Cloud
VM. In IEEE Int’l Conf on Distributed Computing Systems ,
2018.
[41] Maciej Malawski, Gideon Juve, Ewa Deelman, and Jarek
Nabrzyski. Cost- and Deadline-constrained Provisioning forScientiﬁc Workﬂow Ensembles in IaaS Clouds. In Proceedings
of the International Conference on High Performance Comput-ing, Networking, Storage and Analysis, 2012.
[42] Jing Jiang, Jie Lu, Guangquan Zhang, and Guodong Long.
Optimal Cloud Resource Auto-Scaling for Web Applications.InCluster , Cloud and Grid Computing (CCGrid), 2013 13th
IEEE/ACM International Symposium on, pages 58–65, 2013.
[43] M. Albonico, S. D. Alesio, J. Mottu, S. Sen, and G. Suny ´e.
Generating Test Sequences to Assess the Performance of ElasticCloud-Based Systems. In IEEE Int’l Conference on Cloud
Computing (CLOUD), 2017.
[44] Ali Abedi and Tim Brecht. Conducting Repeatable Experiments
in Highly Variable Cloud Computing Environments. In Pro-
ceedings of the 8th ACM/SPEC on International Conference onPerformance Engineering, 2017.
[45] H. M. Alghmadi, M. D. Syer, W. Shang, and A. E. Hassan.
An Automated Approach for Recommending When to StopPerformance Tests. In Int’l Conf. on Software Maintenance
and Evolution, 2016.
[46] Lubom ´ır Bulej, V ojt ˇech Hork ´y, Petr Tuma, Franc ¸ois Farquet,
and Aleksandar Prokopec. Duet Benchmarking: ImprovingMeasurement Accuracy in the Cloud. In ACM/SPEC Int’l Conf.
on Performance Engineering, ICPE ’20, 2020.
[47] Luke Bertot, St ´ephane Genaud, and Julien Gossa. Improving
Cloud Simulation Using the Monte-Carlo Method. In Euro-Par:
Parallel Processing, 2018.
[48] Steffen Becker, Lars Grunske, Raffaela Mirandola, and Sven
Overhage. Performance prediction of component-based sys-tems. In Ralf H. Reussner, Judith A. Stafford, and Clemens A.Szyperski, editors, Architecting Systems with Trustworthy Com-
ponents, pages 169–192, Berlin, Heidelberg, 2006. SpringerBerlin Heidelberg.
[49] Catia Trubiani, Indika Meedeniya, Vittorio Cortellessa, Aldeida
Aleti, and Lars Grunske. Model-Based Performance Analysisof Software Architectures under Uncertainty. In Proceedings of
Int’l ACM Sigsoft Conf. on Quality of Software Architectures ,
2013.
[50] Ivan Postolski, Victor Braberman, Diego Garbervetsky, and
Sebastian Uchitel. Simulator-Based Diff-Time PerformanceTesting. In Proc. of Int’l Conf. on Software Engineering: New
Ideas and Emerging Results, 2019.
[51] N. Siegmund, S. S. Kolesnikov, C. K ¨astner, S. Apel, D. Batory,
M. Rosenm ¨uller, and G. Saake. Predicting Performance via
Automated Feature-interaction Detection. In Int’l Conf. on
Software Engineering (ICSE), 2012.
[52] Guoliang Zhao, Safwat Hassan, Ying Zou, Derek Truong, and
Toby Corbin. Predicting Performance Anomalies in SoftwareSystems at Run-time. ACM Transactions on Software Engineer-
ing and Methodology, December 2020.
[53] Xusheng Xiao, Shi Han, Dongmei Zhang, and Tao Xie. Context-
Sensitive Delta Inference for Identifying Workload-DependentPerformance Bottlenecks. In International Symposium on Soft-
ware Testing and Analysis, ISSTA 2013, 2013.
[54] H. Ha and H. Zhang. DeepPerf: Performance Prediction for
Conﬁgurable Software with Deep Sparse Neural Network. InIEEE/ACM Int’l Conf on Software Engineering (ICSE), 2019.
[55] Yuhui Lin, Adam Barker, and John Thomson. Modelling VM
Latent Characteristics and Predicting Application Performanceusing Semi-supervised Non-negative Matrix Factorization. InIEEE Int’l Conf. on Cloud Computing (CLOUD), 2020.
[56] Emilio Coppa, Camil Demetrescu, and Irene Finocchi. Input-
sensitive Proﬁling. In Proc. of the Conf. on ProgrammingLanguage Design and Implementation, 2012.
[57] Sudipta Chattopadhyay, Lee Kee Chong, and Abhik Roychoud-
hury. Program Performance Spectrum. In Proc. of ACM Conf.
on Languages, Compilers and Tools for Embedded Systems,2013.
[58] Mark D. Syer, Zhen Ming Jiang, Meiyappan Nagappan,
Ahmed E. Hassan, Mohamed Nasser, and Parminder Flora.Continuous Validation of Load Test Suites. In Int’l Conf. on
Performance Engineering, 2014.
[59] Cornel Barna, Marin Litoiu, and Hamoun Ghanbari. Autonomic
Load-testing Framework. In Int’l Conf. on Autonomic Comput-
ing, 2011.
[60] Jacob Burnim, Sudeep Juvekar, and Koushik Sen. WISE:
Automated Test Generation for Worst-case Complexity. InProc.s of Int’l Conference on Software Engineering, 2009.
[61] Pingyu Zhang, Sebastian Elbaum, and Matthew B. Dwyer.
Automatic generation of load tests. In Proc. of Int’l Conf. on
Automated Software Engineering, 2011.
[62] Bihuan Chen, Yang Liu, and Wei Le. Generating Performance
Distributions via Probabilistic Symbolic Execution. In Proc. of
Int’l Conf on Software Engineering, 2016.
[63] Michael Pradel, Markus Huggler, and Thomas R. Gross. Per-
formance Regression Testing of Concurrent Classes. In Int’l
Symp. on Software Testing and Analysis, 2014.
[64] Xue Han, Tingting Yu, and David Lo. PerfLearner: Learning
from Bug Reports to Understand and Generate PerformanceTest Frames. In ACM/IEEE Int’l Conf on Automated Software
Engineering, 2018.
[65] D. Marijan and M. Liaaen. Effect of Time Window on the
Performance of Continuous Regression Testing. In IEEE Int’l
Conf. on Software Maintenance and Evolution (ICSME), 2016.
[66] Pengyu Nie, Ahmet Celik, Matthew Coley, Aleksandar Mil-
icevic, Jonathan Bell, and Milos Gligoric. Debugging thePerformance of Maven’s Test Isolation: Experience Report. InProceedings of the 29th ACM SIGSOFT International Sympo-sium on Software Testing and Analysis, ISSTA 2020, 2020.
[67] Marija Selakovic, Thomas Glaser, and Michael Pradel. An
Actionable Performance Proﬁler for Optimizing the Order ofEvaluations. In Proceedings of the 26th ACM SIGSOFT Inter-
national Symposium on Software Testing and Analysis, ISSTA2017, 2017.
[68] A. Banerjee, S. Chattopadhyay, and A. Roychoudhury. Static
analysis driven cache performance testing. In IEEE 34th Real-
Time Systems Symposium, 2013.
[69] Hazem Samoaa and Philipp Leitner. An Exploratory Study of
the Impact of Parameterization on JMH Measurement Resultsin Open-Source Projects. In ACM/SPEC Int’l Conf. on Perfor-
mance Engineering, ICPE ’21, 2021.
[70] Hammam M. AlGhamdi, Cor-Paul Bezemer, Weiyi Shang,
Ahmed E. Hassan, and Parminder Flora. Towards reducing thetime needed for load testing. Journal of Software: Evolution
and Process, page e2276, 2020.
[71] S. M ¨uhlbauer, S. Apel, and N. Siegmund. Identifying Soft-
ware Performance Changes Across Variants and Versions. InIEEE/ACM Int’l Conf. on Automated Software Engineering(ASE), 2020.
[72] C. Trubiani and S. Apel. PLUS: Performance Learning for
Uncertainty of Software. In IEEE/ACM 41st International
Conference on Software Engineering: New Ideas and EmergingResults (ICSE-NIER), 2019.
678