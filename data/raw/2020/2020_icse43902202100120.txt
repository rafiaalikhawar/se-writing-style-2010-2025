Enhancing Genetic Improvement of Software
with Regression Test Selection
Giovani Guizzo, Justyna Petke, Federica Sarroand Mark Harmany
Department of Computer Science, University College London (UCL), London, United Kingdom
yFacebook, London, United Kingdom
fg.guizzo, j.petke, f.sarro, mark.harmang@ucl.ac.uk
Abstract ‚ÄîGenetic improvement uses artiÔ¨Åcial intelligence to
automatically improve software with respect to non-functional
properties (AI for SE). In this paper, we propose the use of
existing software engineering best practice to enhance Genetic
Improvement (SE for AI).
We conjecture that existing Regression Test Selection (RTS)
techniques (which have been proven to be efÔ¨Åcient and effective)
can and should be used as a core component of the GI search
process for maximising its effectiveness.
To assess our idea, we have carried out a thorough empirical
study assessing the use of both dynamic and static RTS techniques
with GI to improve seven real-world software programs.
The results of our empirical evaluation show that incorporation
of RTS within GI signiÔ¨Åcantly speeds up the whole GI process,
making it up to 68% faster on our benchmark set, being still
able to produce valid software improvements.
Our Ô¨Åndings are signiÔ¨Åcant in that they can save hours to
days of computational time, and can facilitate the uptake of
GI in an industrial setting, by signiÔ¨Åcantly reducing the time
for the developer to receive feedback from such an automated
technique. Therefore, we recommend the use of RTS in future
test-based automated software improvement work. Finally, we
hope this successful application of SE for AI will encourage other
researchers to investigate further applications in this area.
Index Terms‚ÄîGenetic Improvement, Regression Test Selection,
Search Based Software Engineering, Genetic Programming
I. I NTRODUCTION
Genetic Improvement (GI) is an ArtiÔ¨Åcial Intelligence tech-
nique used to improve a given property of an existing software
in an automated way [1]. The software property can be either
functional (e.g., bug Ô¨Åxing [2]‚Äì[4]) or non-functional (e.g.,
runtime [5]‚Äì[7], memory usage [8], [9], energy consump-
tion [10]‚Äì[12]), provided such a property can be formulated
as a Ô¨Åtness function. The Ô¨Åtness function guides the iterative
search process by quantitatively measuring the level of im-
provement of the software.
At each iteration, multiple variants of the software are pro-
duced and tested in order to assess whether the transformations
preserve the overall software functionality, and then their level
of improvement is measured. As one can infer, this process of
constantly testing different versions of the software is time-
consuming [5], [13]‚Äì[15], especially when the software is
accompanied by a costly test suite. Even for toy programs
with relatively small test suites, GI executions can take several
hours or even days [16]. One possible way of solving this
problem is to select only a subset of test cases to execute
instead of using the whole test suite.Regression Test Selection (RTS) [17] has been extensively
studied in the Software Engineering literature with the main
purpose of selecting subsets of test cases for newer versions
of the software. RTS has been successfully used [18] to,
following a set of code modiÔ¨Åcations, select only the subset
of test cases that can test the changed code. Differently from
Regression Test Minimisation and Regression Test Prioriti-
sation that aim at permanently removing redundant/useless
test cases and ordering test cases for execution, respectively,
RTS temporarily selects subsets of test cases for the imminent
testing task. The underlying objective is to reduce the overall
cost of software testing by simply executing fewer test cases
at each iteration.
We hypothesise that existing RTS techniques (which have
been proven to be efÔ¨Åcient and effective [17], [19], [20])
can and should be used as a core component of the GI
search process for maximising its effectiveness. Prior work
showed the effectiveness of RTS in various contexts, but
never for non-functional GI. In this context, different/unseen
challenges can arise when RTS is applied because not only
it concerns whether the software variants pass all test cases
(which Automated Program Repair ‚Äî APR ‚Äî is solely con-
cerned with), but also whether the non-functional properties
will be affected. In other words, speeding up the GI process
impacts the search itself, because the software speed-up is
one of the Ô¨Åtness functions that guides the search. Therefore,
a thorough investigation is needed to discover and report the
magnitude of such effects. SpeciÔ¨Åcally, we present results to
show how RTS can imbue GI Ô¨Åtness computation with a
faster procedure, whilst also maintaining the correctness of
the generated improved software.
The main objective of this paper is to investigate: i) how ef-
fective are the RTS techniques in the context of GI; ii) what is
the efÔ¨Åciency gain provided by such techniques; and iii) what
is the overall trade-off between efÔ¨Åcacy and efÔ¨Åciency gain in
various scenarios.
In order to answer these questions and evaluate the effec-
tiveness of RTS in the context of GI, we carry out an extensive
empirical evaluation with two state-of-the-art RTS techniques
based on dynamic and static analysis [19] over seven real-
world programs.
The main contributions of this work are:
1) Assessment of the effectiveness of RTS in the context of
GI with extensive experimentation;
13232021 IEEE/ACM 43rd International Conference on Software Engineering (ICSE)
1558-1225/21/$31.00 ¬©2021 IEEE
DOI 10.1109/ICSE43902.2021.00120
2) Report of results that can shape how GI is designed and
applied in the future;
3) Integration of state-of-the-art RTS techniques into an
open-source genetic improvement tool.
The results of our empirical evaluation show that incorpo-
ration of RTS within a GI framework can signiÔ¨Åcantly speed
up the whole GI process, making it up to 68% faster on
our benchmark set, with minimal loss to generality to the
whole test set. In fact, the combination of both dynamic and
static RTS with GI led to improved program variants that
passed all the tests for the given program. The results are
signiÔ¨Åcant in that they can save hours to days of computational
time yet still be able to Ô¨Ånd valid software improvements.
This should provide a faster uptake of the techniques in
an industrial setting, by signiÔ¨Åcantly reducing the time for
the developer to receive feedback from such an automated
technique. Therefore, we recommend the use of RTS in future
test-based automated software improvement work. Finally, we
hope this successful application of SE for AI will encourage
other researchers to investigate further applications in this area.
II. B ACKGROUND
This section presents the background on the two main topics
of this paper: RTS and GI.
A. Regression Test Selection
Regression Testing is a task performed to assess whether
changes to a given software harm any of its pre-existing
functionalities [17]. The default strategy is retest-all , which
basically re-runs all the available test cases against the mod-
iÔ¨Åed program. However, the cost of this strategy becomes
prohibitive as the software and its test suite grow in size
and complexity. In order to speed-up the process and avoid
the execution of the whole test suite, a set of regression test
strategies have been proposed [17]. The most common ones
are test case prioritisation, test suite minimisation, and test
case selection.
Test case prioritisation aims at rearranging the test cases
execution order to maximise fault detection or to reveal
faults earlier. Test suite minimisation focuses on permanently
removing obsolete or irrelevant test cases from the test suite.
Finally, test case selection (also known as Regression Test
Selection ‚Äì RTS) techniques select test cases based on current
changes between one software version to another. Its main
objective is to avoid the execution of test cases that do not
exercise the modiÔ¨Åed code. Note that RTS mostly depends
on the differences between versions of the software, whereas
test suite minimisation and test case prioritisation do not
necessarily rely on such information. This work focuses on
RTS, hence we present it in more detail.
According to Yoo and Harman [17], a subset of test cases
T0from the test suite Tshould contain all available test cases
that can reveal the fault in the modiÔ¨Åed version P0of the
original program P. A test case tis fault revealing in relation
toPandP0if the result of the execution of tis different in
both versions. In order to check whether tis fault revealing,tmust be executed against PandP0. The underlying and
reasonable assumption is that P(t)halted and produced the
correct output. Hence, for a new version P0, one should select
all test cases that traverse the modiÔ¨Åed code in P0, or that used
to traverse a deleted piece of code in P0, and then compare the
results. If an RTS technique selects all modiÔ¨Åcation-traversing
test cases in relation to PandP0, this technique is called safe.
In this work, we use two RTS techniques: a modiÔ¨Åcation-
based approach (dynamic analysis) and a Ô¨Årewall approach
(static analysis) [17]. The modiÔ¨Åcation-based approach imple-
mentation is based on the Ekstazi [18] tool. Ekstazi performs
dynamic analysis over a given program in order to Ô¨Ånd
dependency between Java Ô¨Åles. It applies hashing functions
over the content of the Ô¨Åles, and when a mismatch between
old hashes and new ones occur, all test cases that depend
on such a Ô¨Åle are selected. On the other hand, the Ô¨Årewall
approach implementation is based on the STARTS [21] tool.
STARTS collects dependency information between classes
with static analysis before executing the tests. This technique
then analyses the modiÔ¨Åcations and selects all test cases with
dependencies to entities inside the ‚ÄúÔ¨Årewall‚Äù.
Both tools have been evaluated in the literature [19], [20]
and have been shown to be relatively inexpensive and safe.
However, as shown by Chen and Zhang. [22], RTS can also
be used in other testing contexts. The authors compared both
tools for speeding up Mutation Testing, showing that both are
able to select the adequate test cases to test a given mutant.
We believe that RTS can also be used in the GI context with
signiÔ¨Åcant beneÔ¨Åts.
B. Genetic Improvement and EfÔ¨Åciency
Genetic Improvement (GI) consists of the improvement
of existing software through search based algorithms [1].
Search Based algorithms try to Ô¨Ånd an approximately optimum
solution for a given problem for which the true optimum
solution cannot be found in feasible time [23]. GI can be
seen as part of the Search Based Software Engineering (SBSE)
Ô¨Åeld [24], in which search based algorithms are used to solve
hard Software Engineering problems.
During GI optimisation, software undergoes transformations
in order to improve a set of properties, either functional or
non-functional [1]. The most common type of functional im-
provement is APR [13], [14], [25], in which a faulty program
is modiÔ¨Åed until the failing test suite passes. In non-functional
GI however, the goal is to improve the software‚Äôs memory us-
age [8], [9], execution time [5]‚Äì[7], energy consumption [10]‚Äì
[12], and other non-functional properties, whilst maintaining
the functional properties of the software, measured with the
use of the program‚Äôs test suite, i.e., test cases should pass
after the program transformation. Either way, the GI process
is guided by a Ô¨Åtness function that measures the level of
functional or non-functional improvement.
At each GI iteration, usually a new version of the software
is generated, its Ô¨Åtness computed by executing the test suite
against it, and then the best versions found are stored for
further use during the optimisation process. This process may
1324go on for hundreds or thousands of iterations, each of which
imposing the cost of executing the test suite against the
candidate solution. The efÔ¨Åciency of GI becomes a problem
when the software is accompanied by costly test suites, which
has been pointed out in APR work [13], [14].
GI tools already implement some efÔ¨Åciency improvement
mechanisms. For example, Gin [26], an all-purpose GI tool
for Java programs, performs in-memory compilation which
removes the overhead of writing transformed classes to disk
before compiling. Gin also applies a proÔ¨Åling phase before
starting the optimisation in order to identify target classes and
methods, thus avoiding the transformation of uncovered code.
However, the execution can still take several hours, even for
relatively small programs [16]. We hypothesise that RTS can
provide a signiÔ¨Åcant efÔ¨Åciency improvement for GI.
As far as we are aware, there is only one related work
in the literature. Mehne et al. [14] used an ad-hoc RTS
(called ‚ÄúTest-Case Pruning‚Äù) and a localisation technique with
GenProg [2], an APR tool for C programs. Their technique
only applies patches in speciÔ¨Åc places, selects test cases that
cover the patched functions, and then executes only these test
cases when the patch needs to be validated. The results show
that Test-Case Pruning provides a speed-up of 1.8 with no
additional overhead, whilst mostly maintaining the correctness
of the patches.
Differently from the work of Mehne et al. [14], we propose a
more in-depth evaluation of the impact of RTS on multiple GI
properties, such as safety, efÔ¨Åciency, improvement capability,
and the trade-off between these properties. Furthermore, we
investigate RTS in the context of non-functional improvement
as opposed to APR, though the techniques can also be applied
in the APR context. We chose to target runtime improvement
as Ô¨Åtness evaluation is even more costly than in the case of
APR, as it requires multiple runs of each of the software
variants to get reliable Ô¨Åtness measures. Therefore, efÔ¨Åciency
of improvements of the whole GI process would be most
beneÔ¨Åcial.
The next section presents the Research Questions and the
experimental set-up used to answer them.
III. R ESEARCH QUESTIONS
In this work we aim to answer the following Research
Questions (RQs):
RQ1. Effectiveness: How effective is regression test selec-
tion in the context of Genetic Improvement of software?
RQ2. EfÔ¨Åciency: What is the efÔ¨Åciency gain when using
Regression Test Selection with Genetic Improvement?
RQ3. Trade-Off: What is the trade-off between efÔ¨Åciency
and efÔ¨Åcacy of the Genetic Improvement process with
various Regression Test Selection strategies in different
application scenarios?
In the following subsections we discuss in detail the moti-
vation for each of the RQs and the measures we deÔ¨Åned to
gather the answers. In Section IV we describe the techniques,
tools and program subjects we used in our empirical study.A. RQ1 ‚Äì Effectiveness
How effective is regression test selection in the context of
genetic improvement of software? ‚Äì This question is asked
to evaluate whether RTS harms the general functionality of
GI. We deÔ¨Åne herein ‚Äúeffectiveness‚Äù as a combination of
functional validity and improvement levels of the software
produced by the GI optimisation process. SpeciÔ¨Åcally, we are
concerned that RTS techniques may discard important test
cases, and consequently the improved software resulting from
the GI optimisation may fail when tested against the whole
test suite. Moreover, depending on the selected test cases, the
non-functional improvement capability of GI can be affected.
If this is the case, then the use of RTS with GI may impose
serious disadvantages that can affect the overall results of GI,
rendering its application infeasible in practice.
We deÔ¨Åne the ‚ÄúRelative Safety‚Äù (RS) measure as follows:
RS(si; p) =jPTsi;pj
jTj(1)
wherejTjis the number of test cases in the test suite T
of program p; andjPTsi;pjis the number of passing test
cases in Twhen executed against the best improved version
ofpobtained by the strategy sin the i-th independent run.
Effectively, this measure computes the percentage of test cases
in the whole test suite Tthat pass when executed against the
improved software version. The greater the RS, the safer the
RTS technique.
We further analyse the results of the experiments in order
to investigate whether the RTS techniques have any impact on
the Ô¨Ånal outcome of GI in terms of improvement. For this end,
we deÔ¨Åne the ‚ÄúRelative Improvement Change‚Äù (RIC) measure
as follows:
RIC (si; p) =improvement (si; p)
originalImprovement (p)(2)
improvement (si; p)is the level of improvement (Ô¨Åtness
value) of a valid version of the improved program pob-
tained by the strategy siin the i-th independent run; and
originalImprovement (p)is the average level of improve-
ment for the program pwhen using GI without RTS. In
summary, RIC computes the magnitude of the improvement
obtained by a given strategy when compared to the results of
GI with no RTS strategy. Thus, if RIC > = 1:0, then it means
that the respective RTS strategy improves the capability of GI
to improve software, otherwise there is a negative impact.
The results of RS andRIC are compared using the
Kruskal-Wallis statistical test [27] and Vargha-Delaney ÀÜA12
effect size [28]. The former is used to assess if the difference
between the techniques is statistically signiÔ¨Åcant across the 20
independent runs, whereas the latter measures the magnitude
of the difference. Both tests are non-parametric, thus they do
not assume normal distribution of the data.
B. RQ2 ‚Äì EfÔ¨Åciency
What is the efÔ¨Åciency gain when using regression test
selection with genetic improvement? ‚Äì This question focuses
1325on the main beneÔ¨Åt of using RTS with GI: efÔ¨Åciency gain. We
want to unveil what is the magnitude of the savings in terms
of GI execution time during the optimisation process.1Whilst
our implementation of GI works on improving the execution
time of a program, the RTS techniques aim at improving the
execution time of GI itself.
The overhead of RTS techniques comes mainly from the
data collection on test cases and class dependencies during
such a phase, and from the Ô¨Åltering of test cases for subsequent
use (Section II-A). As one can infer, these tasks can become
quite costly when a program is accompanied by thousands
of methods and test cases. Certainly, it is undesirable to use
an RTS technique that introduces more overhead than the
efÔ¨Åciency gain it provides. Therefore, the cost of the strategies
is also included in the efÔ¨Åciency evaluation.
We deÔ¨Åne the ‚ÄúRelative Cost‚Äù (RC) of a strategy as follows:
RC(si; p) =cost (si; p)
originalCost (p)(3)
where cost (si; p)is the sum of overhead time and optimisation
time of the strategy sat the i-th independent run for program
p; and originalCost (p)is the average cost of the default
implementation of GI without any RTS for program p. The
lower the RC, the better. If the result of RC is greater or
equal than 1:0, then we can state that the RTS technique does
not reduce the overall cost of GI in that speciÔ¨Åc context. On the
other hand, if the result is lower than 1:0, then we can quantify
how much execution time can be saved by the strategy and
compare to others, e.g., a RC of 0:4means that the strategy
saved 60% in execution time.
Similarly to RQ1, we apply the Kruskal-Wallis and Vargha-
Delaney ÀÜA12tests over RC to identify statistical differences
between the different test case selection strategies.
C. RQ3 ‚Äì Trade-Off
What is the trade-off between efÔ¨Åciency and efÔ¨Åcacy of
the genetic improvement process with various regression test
selection strategies in different application scenarios? ‚Äì Fi-
nally, RQ3 takes into account the two measured properties
and weighs them on a multi-objective space to identify if the
trade-off is positive in different scenarios. For example, we
want to identify whether the strategies provide greater cost
savings than safety loss.
In order to answer this question, we Ô¨Årst weigh the relative
cost of each strategy versus its safety. If the strategies yield
RS = 1 andRIC1, i.e., the improved programs do not
fail when tested with the whole test suite and do not harm
the improvement capabilities of GI, then the comparison is
straightforward: the cheapest strategy provides the best trade-
off between the measures. However, in case of obtaining
results that are conÔ¨Çicting (i.e., failing test cases or negative
changes in improvement), we have to perform a more careful
investigation on the real trade-off.
1Not to be confused with the savings in execution cost of the improved
version of the program.With that in mind, we deÔ¨Åne a few GI application use
cases and evaluate the trade-offs of each RTS strategy in
such scenarios. The Ô¨Årst use case is when the engineer
wants to Ô¨Ånd the perfect improvement, i.e., they let the GI
optimisation process execute until the end and then select the
best improved software. The second use case takes place when
the engineer needs a fast improvement, regardless of how good
this improvement is. In other words, the engineer stops the
optimisation process after the Ô¨Årst positive improvement found
by GI in which the software does not fail. The last scenario
concerns diversity. In this case, the engineer wants to Ô¨Ånd a
plethora of improved software versions, and then choose one
(or many) from the set of non-failing ones. In all of the posed
use cases, we evaluate the efÔ¨Åciency of each strategy against
a different property of interest.
For the ‚Äúperfect improvement‚Äù use case (herein abbreviated
asPimprov ), the property is the level of improvement of the
best and valid found improved version. If a strategy is both
faster to execute and provides better improvement, then it is
naturally preferred for this scenario.
In the ‚Äúfast improvement‚Äù use case ( Fimprov ), the property
of interest is the validity of the Ô¨Årst improved version found.
The improved version must pass all test cases in the test suite,
not only the ones selected by the RTS technique, otherwise
the engineer stopped the optimisation and was left with no
valid software. The level of improvement for Fimprov is not
important, as long as it is a positive one.
Finally, in the ‚Äúdiverse improvement‚Äù use case ( Dimprov ),
the engineer will choose an improved software of their liking,
thus the property of interest is the number of valid and positive
improvement versions. The more options the engineer has,
the better suited the RTS strategy is for this use case. The
underlying question posed here is: how fast the RTS techniques
can successfully fulÔ¨Ål each of these use cases? This question is
answered in a qualitative manner using the concept of Pareto
optimality [29].
IV. E XPERIMENTAL DESIGN
In this Section we describe in detail the techniques, the
program subjects and the tools we use in our empirical study.
In order to allow reproducibility, we provide a replication
package at https://doi.org/10.5522/04/12890792.
A. Techniques
In order to answer our research questions, we use two state-
of-the-art RTS techniques (based on the Ekstazi and STARTS
tools ‚Äì Section II-A) in combination with Gin [26]. Namely,
we compare the following strategies in our experiment:
GI ‚Äì GI with no RTS;
GI+Random ‚Äì randomly selects a subset of test cases
without guidance;
GI+Ekstazi ‚Äì GI using Ekstazi as a dynamic analysis RTS
technique;
GI+STARTS ‚Äì GI using STARTS as a static analysis RTS
technique.
1326We did not use the default test case selection mechanism
implemented in Gin, since it showed to be infeasible. For
instance, whilst Ekstazi and STARTS both took less than 10
minutes to execute on commons-codec , Gin‚Äôs selection took 11
hours. The GI+Random strategy is used as a matter of sanity
check.
We compare these implementations using seven subject
programs over 20 independent runs (more details in Sec-
tion IV-C). Multiple independent runs are needed to cater
for the stochastic nature of search based algorithms [30]. The
result of each independent run (best improved version p0of the
program p) is collected and then evaluated in terms of number
of passing test cases.
Conveniently, Gin applies a proÔ¨Åling mechanism before
the actual optimisation process to gather information about
‚Äúhot spots‚Äù in the code, which are later focused on by the
GI optimisation. The output of this preprocessing is a csv
Ô¨Åle containing which methods and how many times they
were executed, along with the set of test cases to test each
method during the optimisation process. If no RTS technique
is selected, then the whole test suite is assigned to test all
methods equally. On the other hand, if an RTS technique is
selected, then it collects dependency data and assigns the test
cases to each ‚Äúhot spot‚Äù based on its own selection mechanism.
In order to answer questions about efÔ¨Åciency of the GI
process with and without RTS, we sum the proÔ¨Åling overhead
and the execution time of the GI process. Because all runs
share the same stopping condition as a constant number of
Ô¨Åtness evaluations (conÔ¨Åguration parameters are presented in
Section IV-C), we can determine which strategy can Ô¨Ånish
the optimisation process the fastest and how much time can
be saved in relation to the default GI implementation without
RTS, whilst also considering the overhead incurred by each
technique.
B. Experimental Procedure
For each program under improvement, each of the 20
independent runs is preceded by a proÔ¨Åling task. Hence, Ô¨Årst
we perform the proÔ¨Åling task without any RTS, each of which
generating a proÔ¨Åling csv Ô¨Åle containing the hot spots, and
all test cases are assigned to all hot spots. The execution time
of this phase (original overhead) is then stored and serves
as a baseline for further comparisons. Then, we compute the
overhead of the other strategies over the 20 independent runs.
After performing the experiments without RTS, we collect
and compute the average execution time. The execution time
of a single run is simply the execution time taken to complete
the optimisation process. Then, we compute the execution time
of the other strategies over the 20 independent runs.
Finally, we sum the execution time of both the proÔ¨Åling
task and the optimisation process for each strategy, program,
and independent run. This is the total computational cost of
GI with the respective strategy, which can tell us whether the
total cost of such a strategy is lower than the total cost of the
default GI implementation.TABLE I
SUBJECT PROGRAMS .LLOC: NUMBER OF LOGICAL LINES OF CODE
(EXECUTABLE LINES ); #T: NUMBER OF TEST CASES IN THE PROGRAM ‚ÄôS
TEST SUITE ; T. LLOC: NUMBER OF LOGICAL LINES OF TEST CODE ; COV:
STATEMENT AND BRANCH COVERAGE PERCENTAGES OBTAINED BY THE
TEST SUITE ; TESTTIME:EXECUTION TIME OF THE TEST SUITE (MM:SS).
Program LLOC #T T. LLOC Cov Test Time
codec-1.14 9 044 1 081 13 276 96/91 00:15
compress-1.20 25 978 1 170 22 059 84/75 01:39
csv-1.7 1 845 325 4 864 89/85 00:06
Ô¨Åleupload-1.4 2 425 82 2 284 80/76 00:04
imaging-1.0 31 320 583 7 427 73/59 00:52
text-1.3 8 703 898 12 872 97/96 00:05
validator-1.6 7 409 536 8 352 86/76 00:11
At the end, for each program, strategy, and independent run,
we obtain a a testing cost and a set of improved software
variants. Then the software variants are tested against the
whole test suites in order to check for validity.
C. Subject Programs
We focus on the non-functional GI optimisation [1] of
execution time, i.e., the improvement goal of the GI im-
plementation is to reduce the overall execution time of the
program under improvement.2The seven subject programs
are presented in Table I. These programs are part of the
Apache Commons project3, a set of well-known and widely
used libraries. We selected such programs because they are
different in size, build with no errors, comply with all the
requirements needed to run with Gin, have relatively large
and passing test suites, and have different testing times.
Furthermore, differently from related work [16], the use of
non-trivial programs provides another investigation angle on
the cost and results of non-functional GI. In other words, with
such extensive experimentation, we can evaluate the RTS tools
in a wider range of scenarios.
D. Genetic Improvement Framework
We chose Gin [26] for our experiments. It has been designed
speciÔ¨Åcally for the improvement of Java programs. Gin pro-
vides several optimisations for running GI on Java software,
including in-memory compilation which removes the overhead
of writing transformed classes to disk before compiling. It also
applies a proÔ¨Åling phase that helps identify which parts of code
are covered by a given test suite and thus preventing uncovered
parts from being modiÔ¨Åed.
The search algorithm used in the experiments is Genetic
Programming (GP) [31], which has recently been updated in
Gin. It uses tournament selection, uniform crossover and a
mutation strategy that picks between a delete, replace, copy
and swap mutation operators uniformly at random before
applying the selected operator to program statements at the
abstract syntax tree level. The parameters were set based on
previous work we found that uses this algorithm [2], [32]. We
2Not to be confused with the execution time of GI itself.
3https://commons.apache.org/
1327TABLE II
ALGORITHM PARAMETERS .
Parameter Value
Population Size 40
Generations 20
Test Repetitions 10
Independent Runs 20
Tournament Percentage 20%
Mutation Probability 50%
TABLE III
PERCENTAGE OF SELECTED TEST CASES .MEDIAN ACROSS ALL 20
INDEPENDENT RUNS .
Program +Ekstazi +STARTS +Random
codec 4.65% 4.65% 35.96%
compress 4.60% 16.99% 67.96%
csv 72.26% 96.45% 30.97%
Ô¨Åleupload 40.24% 42.68% 47.56%
imaging 2.11% 81.34% 39.52%
text 4.45% 4.45% 37.25%
validator 16.04% 29.66% 45.52%
Median 4.65% 29.66% 39.52%
use the default runtime Ô¨Åtness evaluation as implemented in
Gin and set the number of test evaluations for each program to
10 for more reliable runtime measurements. Table II presents
the parameters used by the algorithm.
V. R ESULTS
This section presents the results of the experiments and
provides answers to the RQs formulated in the previous
section. Table III presents the number of test cases selected by
each strategy for each program. This selection was performed
in the proÔ¨Åling phase and the time taken to do such a task is
computed as overhead.
A. Answer to RQ1 ‚Äì Effectiveness
The Ô¨Årst part of the effectiveness analysis concerns the Rel-
ative Safety ( RS‚Äì Equation 1) of the best patches generated
by the GI algorithm in each independent run when re-executed
against all test cases, as opposed to tested only by the test cases
selected by the RTS techniques.
Our Ô¨Årst Ô¨Ånding is that almost all resulting patches are valid.
The only exceptions are Gin+Random results for commons-csv
andcommons-text , and Gin+STARTS for commons-text .
Gin+Random generated Ô¨Åve invalid patches in Ô¨Åve inde-
pendent runs for which 60 test cases failed in total, i.e.,
the resulting patches were deemed invalid by the whole test
suite. Gin+STARTS only generated one invalid patch (out
of 140) for which two test cases failed. Hence, the RS of
Gin+Random and Gin+STARTS is always higher than 0:995,
and for Gin+Ekstazi RSis always 1:0.
The statistical tests showed no difference between the
results, thus we can state that the RTS techniques can safely
be used with GI.
The second part of this RQ concerns the Relative Improve-
ment Change ( RIC ‚Äì Equation 2) of GI when using the RTSTABLE IV
RQ1. M EDIAN RELATIVE IMPROVEMENT CHANGE (RIC) OF
STRATEGIES .GREATER VALUES ARE BETTER . BESTRIC VALUES OR
VALUES STATISTICALLY EQUIVALENT TO THE BEST ONES ARE
HIGHLIGHTED IN BOLD (P-VALUES <0:05).
Program GI +Ekstazi +STARTS +Random p-value
codec 1 3.36 2.29 0.95 1.10e-06
compress 1 2.53 5.81 3.96 2.07e-07
csv 1 2.89 3.8 1.68 7.10e-04
Ô¨Åleupload 1 2.55 1.97 2.08 6.50e-02
imaging 1 0.64 0.95 0.46 0.55240
text 1 2.40 0.95 0.57 2.70e-03
validator 1 4.92 2.81 4.04 1.90e-04
Median 1 2.55 2.29 1.68 ‚Äì
TABLE V
RQ1. E FFECT SIZES FOR THE RELATIVE IMPROVEMENT CHANGE
(RIC). EFFECT SIZES GREATER THAN 0:5MEAN POSITIVE IMPROVEMENT
FOR THE LEFT STRATEGY . DIFFERENCES : N = NEGLIGIBLE , S = SMALL , M
=MEDIUM ,AND L = LARGE . LARGE EFFECT SIZES ARE HIGHLIGHTED IN
BOLD .
Program GI/+Ekstazi GI/+STARTS +Ekstazi/+STARTS
codec 0.04 (L) 0.14 (L) 0.56 (S)
compress 0.13 (L) 0.005 (L) 0.3 (M)
csv 0.16 (L) 0.2 (L) 0.49 (N)
Ô¨Åleupload 0.26 (L) 0.4 (S) 0.63 (S)
imaging 0.58 (S) 0.48 (N) 0.48 (N)
text 0.29 (L) 0.56 (S) 0.73 (L)
validator 0.11 (L) 0.22 (L) 0.66 (M)
strategies. Unlike RS, we observed signiÔ¨Åcant changes in the
results. Table IV presents the median RIC of each strategy,
alongside the Kruskal-Wallis p-value results, whereas Table V
presents the effect size results for the RIC comparisons.
For all programs, GI+Ekstazi presented signiÔ¨Åcant positive
or statistically equivalent RIC to the best one, i.e., by using
Ekstazi with GI, the quality of the resulting patches was
at least equal or better in terms of obtained improvement.
Similarly, STARTS also presented favourable results for six
out of seven programs. For six out of seven ( 86%), the
differences between GI and GI+Ekstazi or GI+STARTS are
statistically large.
One possible explanation for this positive impact on im-
provement is that, during the optimisation process with RTS,
considerably fewer tests are executed. In such a case, the
execution time differences between an improved version of
the software and the original one are more signiÔ¨Åcant to the
search process. In other words, even with a small improvement
of a few hundred milliseconds, the improvement is deemed
more important by the GI algorithm because the cost of
testing the program is also relatively small. Without RTS,
the same execution time improvement would be ‚Äúdiluted‚Äù by
thousands of test cases that do not test the changed code.
Hence, the GI algorithm puts more emphasis on such small
improvements and better explores the search space around
those modiÔ¨Åcations.
Answer to RQ1: State-of-th-art RTS strategies are feasible
1328TABLE VI
RQ2. M EDIAN RELATIVE COST (RC) OF STRATEGIES .LOWER VALUES
ARE BETTER . BESTRC VALUES (OR VALUES STATISTICALLY EQUIVALENT
TO THE BEST ONES )ARE HIGHLIGHTED IN BOLD (P-VALUES <0:05).
Program GI +Ekstazi +STARTS +Random p-value
codec 1.00 0.39 0.40 0.77 3.11e-09
compress 1.00 0.32 0.38 0.89 2.12e-12
csv 1.00 1.03 0.93 0.82 7.97e-03
Ô¨Åleupload 1.00 1.11 0.93 1.05 2.81e-02
imaging 1.00 0.79 0.98 0.40 6.48e-09
text 1.00 0.67 0.78 1.15 2.60e-05
validator 1.00 0.82 0.47 1.01 1.50e-02
Median 1.00 0.79 0.78 0.89 ‚Äì
TABLE VII
RQ2. E FFECT SIZES FOR THE RELATIVE COST (RC). EFFECT SIZES
GREATER THAN 0:5MEAN GREATER COST FOR THE LEFT STRATEGY .
DIFFERENCES : N = NEGLIGIBLE , S = SMALL , M = MEDIUM ,AND L =
LARGE . LARGE EFFECT SIZES ARE HIGHLIGHTED IN BOLD .
Program GI/+Ekstazi GI/+STARTS +Ekstazi/+STARTS
codec 0.95 (L) 0.89 (L) 0.55 (N)
compress 0.99 (L) 0.98 (L) 0.30 (M)
csv 0.49 (N) 0.65 (M) 0.70 (M)
Ô¨Åleupload 0.35 (S) 0.63 (S) 0.74 (L)
imaging 0.80 (L) 0.52 (N) 0.24 (L)
text 0.82 (L) 0.76 (L) 0.35 (S)
validator 0.63 (S) 0.73 (L) 0.64 (M)
when used with GI. In our experiments, they showed almost
100% safety, with only one improved software version failing
when tested against all test cases. Moreover, such techniques
presented positive changes in the improvements obtained by
the GI algorithm.
B. Answer to RQ2 ‚Äì EfÔ¨Åciency
This Section presents the results for the efÔ¨Åciency RQ.
Figure 1 depicts the Relative Cost (RC) (Equation 3) of the
strategies in relation to GI without RTS, e.g., a RC of 0:4
means that using GI with a given RTS strategy costs 60%
less in terms of execution time when compared to the cost
of using GI with no RTS. Similarly, Table VI presents the
median RC for each strategy and the respective results of
the Kruskal-Wallis p-value test in the last column. Finally,
Table VII presents the Vargha-Delaney ÀÜA12effect size results
for the pairwise comparisons. It is worth restating that the
cost includes both overhead and speed-up of the optimisation
process.
For six out of seven programs ( 86%), using either Ekstazi
or STARTS as RTS strategy showed a signiÔ¨Åcant improvement
in execution time (p-value <0:05). Overall, GI+Ekstazi and
GI+STARTS can save up to 68% in relative execution time
(22% on average). Moreover, for Ô¨Åve out of seven programs
(71%), this difference is considered statistically large accord-
ing to the effect size analysis.
The results do not always show a relationship between the
number of test cases (Table III) and the relative cost (i.e., the
fewer the test cases the lower the cost). This happens becausesome test cases might be more expensive than others to run.
For example, Ekstazi selects considerably fewer test cases for
Ô¨Åleupload ,imaging , and validator , yet the relative cost to run
them is greater than the cost paid by other strategies which
select more (but cheaper) test cases for these projects.
We investigated the projects more closely and we found that,
projects characteristics play a role. The cost reduction does not
seem high when there is a common dependency in the source
code (e.g., csv) or few methods are tested by multiple test cases
(e.g., text). In such cases, the RTS strategies end up selecting
almost all test cases because they test the improved and highly
dependent method, or the improved method is thoroughly
tested with expensive test cases. On the other hand, the cost
reduction is high when multiple computationally expensive test
cases test different parts of the code (e.g., compress ). In other
words, the test cases are more isolated (i.e., test only the unit)
and the targeted method only requires a few inexpensive tests.
Figure 2 presents the cumulative cost of the proÔ¨Åling and
optimisation phases of each strategy over the 20 independent
runs. For programs with expensive test suites (e.g., commons-
compress andcommons-imaging ), the RTS techniques saved
in total from a couple of hours to more than a day of
computational time.
It is worth noting that random test case selection during
proÔ¨Åling does not add any overhead. However, the selected test
cases still incur an execution cost, and the random selection
sometimes selects fewer test cases, sometimes selects almost
all test cases, hence the differences in results with respect to
no test selection.
In our speciÔ¨Åc case, the total amount of time required to
run the experiments without RTS was over 180 CPU hours,
whereas with Ekstazi and STARTS this time was reduced to
114 and 118 hours respectively. In other words, using RTS
saved us approximately 64 hours of execution time on average
(more than a third of the cost). Shorter execution times will
allow GI adoption for larger programs, for which the running
cost might still be prohibitive and environmentally unfriendly
without RTS.
Considering both patch validity and changes in improve-
ment (Section V-A), and the overall efÔ¨Åciency of the strategies,
GI+STARTS and GI+Ekstazi are preferred. GI+Random fails
more often and does not provide much beneÔ¨Åt in improvement
and cost savings. Henceforth, we will not discuss the results
of GI+Random and will focus our attention on the other two
RTS strategies.
Answer to RQ2: Using RTS in combination with GI can
signiÔ¨Åcantly reduce the overall relative cost of the optimisation
process (up to 68% depending on the program). The reduction
is statistically signiÔ¨Åcant in 86% of the cases and large in 71%
of the cases. Furthermore, when considering the cumulative
total cost of experimentation, using RTS can save more than
a third of the execution time.
C. Answer to RQ3 ‚Äì Trade-Off
This section presents the results for the three considered use
case scenarios of GI: ‚Äúperfect improvement‚Äù ( Pimprov ), ‚Äúfast
1329Fig. 1. RQ2. Relative Cost (RC) of strategies. Lower values are better. The dashed line represents the median cost of GI without RTS.
Fig. 2. RQ2. Cumulative execution times Results are for four GI strategies i) without RTS, ii) and iii) with RTS, and iv) GI with random test selection.
TABLE VIII
RQ3. Pimprov USE CASE .MEDIAN IMPROVEMENT FOUND IN SECONDS .
GREATER VALUES ARE BETTER .
Program GI +Ekstazi +STARTS
codec 1.29 4.35 2.96
compress 2.46 6.23 14.28
csv 0.82 2.25 2.95
Ô¨Åleupload 0.05 0.13 0.11
imaging 11.98 7.63 12.62
text 1.46 3.52 1.40
validator 0.29 1.44 0.82
Median 1.29 3.52 2.95
improvement‚Äù ( Fimprov ), and ‚Äúdiverse improvement‚Äù Dimprov .
ForPimprov , the best strategy in this scenario is the one that
is able to Ô¨Ånd the best valid improvement. Table VIII shows the
median best improvement in seconds found for each program.
GI+Ekstazi obtained the best improved software in most of
the cases (four out of seven), whilst also obtaining the highest
median improvement across projects.
Since the RTS strategies are safe when used in combination
with GI (with the exception of GI+STARTS for commons-
csvas shown in Section V-A), then the comparison for the
Fimprov use case is straightforward: the strategy that can Ô¨Ånd
a positive improvement the quickest is preferred. ConsideringTABLE IX
RQ3. Fimprov USE CASE .MEDIAN TIME IN SECONDS EACH STRATEGY
TOOK TO FIND A POSITIVE AND VALID IMPROVEMENT . LOWER VALUES
ARE BETTER .
Program GI +Ekstazi +STARTS
codec 711.32 215.76 188.19
compress 730.20 251.14 244.01
csv 388.12 474.82 425.26
Ô¨Åleupload 287.02 261.31 354.38
imaging 768.03 562.79 659.11
text 237.69 137.69 137.38
validator 131.78 79.96 142.68
Median 388.12 251.14 244.01
this scenario, Table IX shows how long each strategy took
in seconds to Ô¨Ånd the Ô¨Årst positive and valid improvement.
On average, GI+STARTS Ô¨Ånds an improvement in the Ô¨Årst
244 seconds of the search process, and for three out of seven
programs it is also the quickest strategy.
Finally, for the last use case Dimprov , we are concerned
with the diversity of improved software found by the strate-
gies. Table X presents the median number of different valid
and positive improvements found by each strategy. For Ô¨Åve
out of seven programs, GI+Ekstazi Ô¨Ånds a wider variety of
improvements.
1330TABLE X
RQ3. Dimprov USE CASE .MEDIAN NUMBER OF POSITIVE AND VALID
IMPROVEMENTS FOUND . GREATER VALUES ARE BETTER .
Program GI +Ekstazi +STARTS
codec 6 16 9
compress 13.50 28.50 41.50
csv 26 48 24
Ô¨Åleupload 2 4 2
imaging 27.50 21 23
text 12 19.50 18.50
validator 36 44 7
Median 13.5 21 18.5
Answer to RQ3: If the engineer is concerned with either
Ô¨Ånding the perfect improvement or a wider variety of positive
and valid improved software, then using GI+Ekstazi is the best
option. However, if the main objective is to Ô¨Ånd a positive
and valid improvement as fast as possible, then GI+STARTS
is recommended. In most of the cases considered (i.e., 19 out
of 21 comparisons) using GI+RTS provides better results than
traditional GI without RTS. Therefore, we recommend the use
of RTS in future GI work.
VI. R ELATED WORK
The work most related to ours is the one that applies re-
gression testing selection techniques to a genetic improvement
process. To the best of our knowledge, only the work of Mehne
et al. [14] has investigated any kind of RTS in this context, and
only for APR. The authors deÔ¨Åned their own RTS technique
and evaluated the results in terms of speed-up. Their results
show a speed-up of up to 1.8 the original cost of automated
program repair of C programs using GenProg [2].
Other works use other kinds of regression techniques, such
as test case prioritisation and sampling. Venugopal et al. [15]
proposed and evaluated a history-based test case prioritisation
for APR. The approach consisted in prioritising test cases that
are most likely to fail whilst also sampling test cases in order
to make the software variant to fail faster. The authors achieved
up to 57.5% in execution time savings during patch validation.
Similarly, Qi et al. [13] proposed TrpAutoRepair , an on-line
prioritisation technique for APR. The idea behind the approach
is to avoid the overhead of pre-gathering information about
test cases executions, and use information already obtained
during the execution of candidate patches. TrpAutoRepair was
able to perform at least as well as GenProg for 15 out of 16
programs, while signiÔ¨Åcantly improving the repair efÔ¨Åciency,
as measured in the number of test case executions.
Fast et al. [25] incorporated random sampling of test cases
in the Ô¨Åtness function computation for APR. Their approach
Ô¨Årst samples a subset of passing test cases and all failing test
cases. The software variant is only tested against the remaining
test cases if all test cases in the subset pass. The authors also
compared their approach to a regression technique for test
suite minimisation based on a Genetic Algorithm (GA) [23].
The results showed that their approach is able to reduce the
computational effort needed to test patches in 81%.APR is concerned exclusively with one functional property
(i.e., bug Ô¨Åxing), while GI concerns with any functional (e.g.,
addition of a new feature, bug-Ô¨Åxing) and non-functional
improvement (e.g., runtime, energy consumption). Only one
previous work [14] uses RTS for GI functional improvement
by applying a single ad-hoc RTS dynamic technique. Other
works investigate other types of regression techniques, such as
test suite minimisation and prioritisation, which are inherently
different to RTS.
Our work differentiates from the ones presented in this
section since we focus solely on RTS and we consider non-
functional improvement (as opposed to APR). State-of-the-art
RTS techniques have not been extensively evaluated in the
context of GI, specially in regards to non-functional improve-
ment. We are the Ô¨Årst to investigate the impact of any test
selection strategy in the context of non-functional automated
software improvement using GI and with both dynamic and
static RTS techniques.
Furthermore, all related work focuses on program repair
and uses the same tool, namely GenProg. We note that the
GP implementation in Gin also follows the GenProg search
strategy. However, unlike previous work, our focus is on Java
software.
In this work, through extensive experimentation with real-
world Java programs, we evaluate the beneÔ¨Åts of RTS along
multiple angles: efÔ¨Åciency, effectiveness, and the underlying
trade-off between those properties, i.e., we evaluate the safety,
efÔ¨Åciency, and improvement impact of RTS. Especially the last
angle, to the best of our knowledge, has not been considered
before.
VII. T HREATS TO VALIDITY
a) Threats to External Validity: As it happens to most
software engineering papers with empirical evaluations, the
set of programs used in the experiments might not be rep-
resentative of the whole population. In order to mitigate
this threat, we have selected well-know, non-trivial software
projects of different sizes, test times, and coverages. Another
threat regards to the fact that we only compared the results
of two RTS techniques, one based on dynamic and one based
on static analysis. Although there are other techniques in the
literature [17], it would be infeasible to compare all of them,
thus we decided to compare only state-of-the-art [20].
b) Threats to Internal Validity: To reduce internal threats,
we used the same conÔ¨Åguration for all experiments. This con-
Ô¨Åguration was used in previous work [2], [32], thus allowing
for further experimental comparisons. Moreover, we executed
all experiments on the same machine with the same resources.
With these precautions, we intended to prevent other factors
to impact the results of our experiments.
c) Threats to Construct Validity: The execution times of
optimisation algorithms such as GI may Ô¨Çuctuate due to their
stochastic nature. In order to cater for these Ô¨Çuctuations, we
performed multiple independent runs and analysed the results
with statistical signiÔ¨Åcance tests and effect size analysis, as
suggested by Arcuri and Briand [30]. To further mitigate
1331noise in the execution time measurements, we repeated each
test case execution 10 times and considered the median as a
more accurate data source. Finally, we took extra care when
selecting statistical tests to avoid making assumptions that
would jeopardise the validity of our results (e.g., we used
non-parametrised tests because we could not assume normal
distribution of the data).
VIII. C ONCLUSIONS
Genetic improvement (GI), an ArtiÔ¨Åcial Intelligence tech-
nique, has been successfully used to improve various software
properties, ranging from reduction of software‚Äôs runtime [5]‚Äì
[7], optimisation of energy [10]‚Äì[12] and memory [8], [9]
consumption, through to bug Ô¨Åxing [2]‚Äì[4] and addition of
new software features [33]. However, it has yet to see wider
uptake. A major obstacle is its high demand on runtime of the
GI process to Ô¨Ånd the desired improvements.
The biggest bottleneck for runtime of a GI process comes
from the Ô¨Åtness evaluation, which requires running the whole
test suite for each and every evolved program version, in order
to ensure that regression bugs are not introduced during the
GI process.
Regression Test Selection (RTS), a traditional Software En-
gineering approach, aims to reduce testing effort by selecting
only the most relevant tests for a given task. However, it has
not yet been tried in the context of genetic improvement of
software.
We investigated the use of two state-of-the-art RTS tech-
niques in an open source GI framework and conducted an
extensive study on seven large real-world software to show that
these can indeed speed-up the GI process without impacting
the validity and improvement gain of the evolved software. In
particular, we show that integration of RTS does not hinder the
validity of the improved patches. In fact, all evolved software
(but one out of 280) when tested against the test cases selected
using either RTS approach, always passed the whole test suite,
thus showing safety.
We observed efÔ¨Åciency gains of the whole genetic improve-
ment process of up to 68%. Given that the GI process might
run for hours, days, or even weeks, such an efÔ¨Åciency gain is
non-trivial, impacting not only the execution times of scientiÔ¨Åc
experiments, but also the carbon footprint of the servers used
to run GI in the industry [34].
Therefore, we recommend the integration of RTS techniques
in test-based automated improvement software. We also rec-
ommend the static STARTS RTS technique for Ô¨Ånding quick
improvements, while we conjecture the dynamic Ekstazi might
be best when the improvement gain or software validity is of
most concern.
Our proposed software engineering approach will thus re-
duce the feedback time from the automated software improve-
ment system to the developer. Therefore, we hope it will
contribute to wider and faster uptake of GI techniques.
There are still several questions to be answered that could
help speed up the process even further. Other test case selec-
tion strategies could be investigated. Moreover, it would beuseful to know what characteristics a given test suite should
have in order to be effective with the GI processes. Initial
work has been done in this direction, investigating standard test
suite measures [16], [35], yet no strong correlation between
test suite metrics and their effectiveness in the improvement
process has been found. If known, test case selection for
GI could be further improved by applying such metrics by
providing further guidance to RTS. This is a direction we
would like to investigate in future work.
ACKNOWLEDGEMENTS
This research is funded by the ERC advanced fellowship
grant 741278 (EPIC: Evolutionary Program Improvement Col-
laborators) and by the EPSRC Fellowship grant EP/P023991/1
(Automated Software Specialisation Using Genetic Improve-
ment).
DATA AVAILABILITY
To aid reproducibility we provide a replication package at
https://doi.org/10.5522/04/12890792.
REFERENCES
[1] J. Petke, S. O. Haraldsson, M. Harman, W. B. Langdon, D. R. White, and
J. R. Woodward, ‚ÄúGenetic Improvement of Software: A Comprehensive
Survey,‚Äù IEEE Transactions on Evolutionary Computation , vol. 22, no. 3,
pp. 415‚Äì432, 2018.
[2] C. Le Goues, M. Dewey-V ogt, S. Forrest, and W. Weimer, ‚ÄúA systematic
study of automated program repair: Fixing 55 out of 105 bugs for $8
each,‚Äù in Proceedings of the 34th International Conference on Software
Engineering (ICSE) . IEEE, 2012, pp. 3‚Äì13.
[3] Y . Yuan and W. Banzhaf, ‚ÄúToward better evolutionary program repair:
An integrated approach,‚Äù ACM Trans. Softw. Eng. Methodol. , vol. 29,
no. 1, pp. 5:1‚Äì5:53, 2020.
[4] G. An, A. Blot, J. Petke, and S. Yoo, ‚ÄúPyggi 2.0: language independent
genetic improvement framework,‚Äù in Proceedings of the ACM Joint
Meeting on European Software Engineering Conference and Symposium
on the Foundations of Software Engineering, ESEC/SIGSOFT FSE 2019,
Tallinn, Estonia, August 26-30, 2019 , M. Dumas, D. Pfahl, S. Apel, and
A. Russo, Eds. ACM, 2019, pp. 1100‚Äì1104.
[5] F. de Almeida Farzat, M. de Oliveira Barros, and G. H. Travassos,
‚ÄúChallenges on applying genetic improvement in javascript using a high-
performance computer,‚Äù J. Softw. Eng. Res. Dev. , vol. 6, p. 12, 2018.
[6] W. B. Langdon and M. Harman, ‚ÄúOptimising Existing Software with
GP,‚Äù IEEE Transactions on Evolutionary Computation , vol. 19, no. 1,
pp. 1‚Äì18, 2015.
[7] J. Petke, M. Harman, W. B. Langdon, and W. Weimer, ‚ÄúSpecialising
Software for Different Downstream Applications Using Genetic Im-
provement and Code Transplantation,‚Äù IEEE Transactions on Software
Engineering , vol. 44, no. 6, pp. 574‚Äì594, 2017.
[8] F. Wu, W. Weimer, M. Harman, Y . Jia, and J. Krinke, ‚ÄúDeep parameter
optimisation,‚Äù in Proceedings of the Genetic and Evolutionary Com-
putation Conference, GECCO 2015, Madrid, Spain, July 11-15, 2015 ,
S. Silva and A. I. Esparcia-Alc ¬¥azar, Eds. ACM, 2015, pp. 1375‚Äì1382.
[9] E. M. Schulte, J. DiLorenzo, W. Weimer, and S. Forrest, ‚ÄúAutomated
repair of binary and assembly programs for cooperating embedded
devices,‚Äù in Architectural Support for Programming Languages and
Operating Systems, ASPLOS ‚Äô13, Houston, TX, USA - March 16 - 20,
2013 , V . Sarkar and R. Bod ¬¥ƒ±k, Eds. ACM, 2013, pp. 317‚Äì328.
[10] B. R. Bruce, J. Petke, M. Harman, and E. T. Barr, ‚ÄúApproximate oracles
and synergy in software energy search spaces,‚Äù IEEE Trans. Software
Eng., vol. 45, no. 11, pp. 1150‚Äì1169, 2019.
[11] N. Burles, E. Bowles, A. E. I. Brownlee, Z. A. Kocsis, J. Swan, and
N. Veerapen, ‚ÄúObject-oriented genetic improvement for improved energy
consumption in google guava,‚Äù in Search-Based Software Engineering
- 7th International Symposium, SSBSE 2015, Bergamo, Italy, Septem-
ber 5-7, 2015, Proceedings , ser. Lecture Notes in Computer Science,
M. de Oliveira Barros and Y . Labiche, Eds., vol. 9275. Springer, 2015,
pp. 255‚Äì261.
1332[12] E. M. Schulte, J. Dorn, S. Harding, S. Forrest, and W. Weimer, ‚ÄúPost-
compiler software optimization for reducing energy,‚Äù in Architectural
Support for Programming Languages and Operating Systems, ASPLOS
‚Äô14, Salt Lake City, UT, USA, March 1-5, 2014 , R. Balasubramonian,
A. Davis, and S. V . Adve, Eds. ACM, 2014, pp. 639‚Äì652.
[13] Y . Qi, X. Mao, and Y . Lei, ‚ÄúEfÔ¨Åcient automated program repair through
fault-recorded testing prioritization,‚Äù in IEEE International Conference
on Software Maintenance (ICSM) . IEEE, 2013, pp. 180‚Äì189.
[14] B. Mehne, H. Yoshida, M. R. Prasad, K. Sen, D. Gopinath, and S. Khur-
shid, ‚ÄúAccelerating Search-Based Program Repair,‚Äù in Proceedings of
the 11th International Conference on Software Testing, VeriÔ¨Åcation and
Validation (ICST) . IEEE, 2018, pp. 227‚Äì238.
[15] Y . Venugopal, P. Quang-Ngoc, and L. Eunseok, ‚ÄúModiÔ¨Åcation point
aware test prioritization and sampling to improve patch validation in
automatic program repair,‚Äù Applied Sciences (Switzerland) , vol. 10,
no. 5, pp. 1‚Äì14, 2020.
[16] M. Lim, G. Guizzo, and J. Petke, ‚ÄúImpact of Test Suite Coverage on
OverÔ¨Åtting in Genetic Improvement of Software,‚Äù in Proceedings of the
Symposium on Search Based Software Engineering (SSBSE) . Springer,
2020.
[17] S. Yoo and M. Harman, ‚ÄúRegression testing minimization, selection and
prioritization: A survey,‚Äù Software Testing VeriÔ¨Åcation and Reliability ,
vol. 22, no. 2, pp. 67‚Äì120, 2012.
[18] M. Gligoric, L. Eloussi, and D. Marinov, ‚ÄúPractical regression test
selection with dynamic Ô¨Åle dependencies,‚Äù in Proceedings of the 2015
International Symposium on Software Testing and Analysis (ISSTA) ,
2015, pp. 211‚Äì222.
[19] C. Zhu, O. Legunsen, A. Shi, and M. Gligoric, ‚ÄúA Framework for
Checking Regression Test Selection Tools,‚Äù in Proceedings of the 2019
41st International Conference on Software Engineering (ICSE) , vol.
2019-May, 2019, pp. 430‚Äì441.
[20] O. Legunsen, F. Hariri, A. Shi, Y . Lu, L. Zhang, and D. Marinov, ‚ÄúAn
extensive study of static regression test selection in modern software
evolution,‚Äù in Proceedings of the 24th ACM SIGSOFT International
Symposium on Foundations of Software Engineering (FSE) . ACM Press,
2016, pp. 583‚Äì594.
[21] O. Legunsen, A. Shi, and D. Marinov, ‚ÄúSTARTS: STAtic regression
test selection,‚Äù in 2017 32nd IEEE/ACM International Conference on
Automated Software Engineering (ASE) . IEEE, 2017, pp. 949‚Äì954.
[22] L. Chen and L. Zhang, ‚ÄúSpeeding up Mutation Testing via Regression
Test Selection: An Extensive Study,‚Äù in Proceedings of the 11th In-
ternational Conference on Software Testing, VeriÔ¨Åcation and Validation
(ICST) . IEEE, 2018, pp. 58‚Äì69.
[23] M. Gendreau and J.-Y . Potvin, Handbook of Metaheuristics , 3rd ed.
Springer, 2019.
[24] M. Harman, P. McMinn, J. T. De Souza, and S. Yoo, ‚ÄúSearch based
software engineering: Techniques, taxonomy, tutorial,‚Äù Lecture Notes
in Computer Science (including subseries Lecture Notes in ArtiÔ¨Åcial
Intelligence and Lecture Notes in Bioinformatics) , vol. 7007 LNCS, pp.
1‚Äì59, 2011.
[25] E. Fast, C. L. Goues, S. Forrest, and W. Weimer, ‚ÄúDesigning better
Ô¨Åtness functions for automated program repair,‚Äù in Proceedings of
the 12th Annual Genetic and Evolutionary Computation Conference
(GECCO) , 2010, pp. 965‚Äì972.
[26] A. E. I. Brownlee, J. Petke, B. Alexander, E. T. Barr, M. Wagner, and
D. R. White, ‚ÄúGin: Genetic Improvement Research Made Easy,‚Äù in
Proceedings of the Genetic and Evolutionary Computation Conference
(GECCO) . New York, NY , USA: ACM, 2019, pp. 985‚Äì993.
[27] W. H. Kruskal and W. A. Wallis, ‚ÄúUse of Ranks in One-Criterion
Variance Analysis,‚Äù Journal of the American Statistical Association ,
vol. 47, no. 260, pp. 583‚Äì621, 1952.
[28] A. Vargha and H. D. Delaney, ‚ÄúA critique and improvement of the CL
common language effect size statistics of McGraw and Wong,‚Äù Journal
of Educational and Behavioral Statistics , vol. 25, no. 2, pp. 101‚Äì132,
2000.
[29] C. A. C. Coello, ‚ÄúMulti-objective Optimization,‚Äù in Handbook of Heuris-
tics. Springer International Publishing, 2018, pp. 1‚Äì28.
[30] A. Arcuri and L. Briand, ‚ÄúA Hitchhiker‚Äôs guide to statistical tests for
assessing randomized algorithms in software engineering,‚Äù Software
Testing, VeriÔ¨Åcation and Reliability , vol. 24, no. 3, pp. 219‚Äì250, 2014.
[31] J. R. Koza, Genetic programming: on the programming of computers by
means of natural selection . MIT press, 1992, vol. 1.
[32] Z. Y . Ding, Y . Lyu, C. S. Timperley, and C. L. Goues, ‚ÄúLeveraging
program invariants to promote population diversity in search-basedautomatic program repair,‚Äù in Proceedings of the 6th International
Workshop on Genetic Improvement, GI@ICSE 2019, Montreal, Quebec,
Canada, May 28, 2019 , J. Petke, S. H. Tan, W. B. Langdon, and
W. Weimer, Eds. ACM, 2019, pp. 2‚Äì9.
[33] E. T. Barr, M. Harman, Y . Jia, A. Marginean, and J. Petke, ‚ÄúAutomated
software transplantation,‚Äù in Proceedings of the 2015 International
Symposium on Software Testing and Analysis, ISSTA 2015, Baltimore,
MD, USA, July 12-17, 2015 , M. Young and T. Xie, Eds. ACM, 2015,
pp. 257‚Äì269.
[34] C. Calero and M. Piattini, Green in Software Engineering . Springer
Publishing Company, Incorporated, 2015.
[35] J. Yi, S. H. Tan, S. Mechtaev, M. B ¬®ohme, and A. Roychoudhury,
‚ÄúA correlation study between automated program repair and test-suite
metrics,‚Äù Empirical Software Engineering , vol. 23, no. 5, pp. 2948‚Äì2979,
2018.
1333