Towards Exploring the Limitations of Active
Learning: An Empirical Study
Qiang Hu1, Yuejun Guo1, Maxime Cordy1, Xiaofei Xie2‚àó, Wei Ma1, Mike Papadakis1, and Yves Le Traon1
1University of Luxembourg, Luxembourg2Nanyang Technological University, Singapore
Abstract ‚ÄîDeep neural networks (DNNs) are increasingly de-
ployed as integral parts of software systems. However, due to
the complex interconnections among hidden layers and massivehyperparameters, DNNs must be trained using a large numberof labeled inputs, which calls for extensive human effort forcollecting and labeling data. Spontaneously, to alleviate thisgrowing demand, multiple state-of-the-art studies have developeddifferent metrics to select a small yet informative dataset forthe model training. These research works have demonstratedthat DNN models can achieve competitive performance using acarefully selected small set of data. However, the literature lacksproper investigation of the limitations of data selection metrics,which is crucial to apply them in practice. In this paper, weÔ¨Åll this gap and conduct an extensive empirical study to explorethe limits of data selection metrics. Our study involves 15 dataselection metrics evaluated over 5 datasets (2 image classiÔ¨Åcationtasks and 3 text classiÔ¨Åcation tasks), 10 DNN architectures, and20 labeling budgets (ratio of training data being labeled). OurÔ¨Åndings reveal that, while data selection metrics are usuallyeffective in producing accurate models, they may induce a loss ofmodel robustness (against adversarial examples) and resilience tocompression. Overall, we demonstrate the existence of a trade-offbetween labeling effort and different model qualities. This pavesthe way for future research in devising data selection metricsconsidering multiple quality criteria.
Index T erms‚Äîdeep learning, data selection, active learning,
empirical study
I. I NTRODUCTION
Deep learning (DL) has achieved tremendous success in
various cutting-edge application domains, such as image pro-
cessing [1], machine translation [2], autonomous vehicles [3],and robotics [4]. Two key elements to achieve high-performingpredictions are well-designed deep neural networks (DNNs)(with appropriate architecture and parameters) and a carefullychosen set of labeled training data. However, data labelingis expensive and time-consuming because it requires a largeamount of human effort. For example, that it took more than 3years to prepare the Ô¨Årst version of the ImageNet [5] dataset.Thus, acquiring labeled data is seen as a major obstacle to thewidespread adoption of DL [6].
One established solution to reduce data labeling cost is
active learning [7], i.e., incremental methods to select infor-mative subsets of training data to undergo labeling in a waythat the produced model is as accurate as if it was trained onall data. With active learning, engineers can thus compromiselabeling effort with model performance (e.g., classiÔ¨Åcationaccuracy). There has been much research on devising active
‚àócorresponding author.learning methods, each relying on different data selectionmetrics. These metrics [8]‚Äì[11] typically exploit informationof the DNN under training (e.g., its gradient or uncertainty)to select the most informative data to label next.
On the other hand, recent work on DL testing and debugging
[12]‚Äì[19] have proposed different metrics for test generation
and test selection, i.e., the problem of selecting test data that
are more likely to be misclassiÔ¨Åed by the model [20]. As inactive learning scenarios, these test data can then be used toimprove the model (by retraining).
The proliferation of data selection metrics (coming from
active learning and testing) makes it challenging for engineersto decide which one they should use. Indeed, the differentmetrics have been evaluated on a restricted set of problems(mostly image classiÔ¨Åcation datasets) under incomparable ex-perimental settings (different models and labeling budget) [18],[19]. There is, therefore, a need for a comprehensive study ofall these data selection metrics on a common ground involvingdifferent classiÔ¨Åcation tasks.
Another gap in the current body of knowledge is that most
experimental studies evaluate the metric wrt. the test accuracyof the trained models.
1Other key quality indicators have
been ignored, such as the model robustness to adversarialattacks and the model performance after compression. Thelack of consideration for these indicators raises practical issueswhen deploying DL models trained using active learning(consider, e.g., biomedical image segmentation [22] or mobileDL applications [23]). Hence, one should make sure that activelearning can produce models whose quality is not limited toclassiÔ¨Åcation accuracy but extends to allquality indicators
relevant for the use case.
To Ô¨Åll these gaps, in this paper, we conduct a compar-
ative empirical study to explore the potential limitations ofactive learning. Our study involves 15 data selection metricsevaluated over 5 datasets (2 image classiÔ¨Åcation tasks and3 text classiÔ¨Åcation tasks), 10 DNN architectures, and 20labeling budgets (ratio of training data that can be labeled).SpeciÔ¨Åcally, our study aims to answer the following fourresearch questions:
‚Ä¢RQ1: How effective are the different data selection
metrics for producing accurate models? We answer this
question by measuring how fast (i.e., how many training dataare required) the accuracy of the trained model converges to
1Rarely, some studies [21] measure empirically the robustness of the model
to adversarial attacks.
9172021 36th IEEE/ACM International Conference on Automated Software Engineering (ASE)
DOI 10.1109/ASE51524.2021.000852021 36th IEEE/ACM International Conference on Automated Software Engineering (ASE) | 978-1-6654-0337-5/21/$31.00 ¬©2021 IEEE | DOI: 10.1109/ASE51524.2021.9678672
978-1-6654-0337-5/21/$31.00  ¬©2021  IEEE
the accuracy of the fully trained model (i.e., trained with the
full training set). Besides, we employ random selection asa baseline to compare the effectiveness of each metric. Ourresults indicate signiÔ¨Åcant differences between metrics (up to45.9% of test accuracy), especially when less than 50% of thetraining data is used.
‚Ä¢RQ2: How robust are models trained with active learn-
ing? We answer this question by measuring the theoretical
robustness of the trained models (using the CLEVER score[24]) as well as their empirical robustness against multipleadversarial attacks. For the image classiÔ¨Åcation task, ourresults reveal a gap between the fully trained models and thosetrained with active learning (up to 1.49 CLEVER score and23.39% of success rate), whereas there are no such differencesin the text classiÔ¨Åcation task.
‚Ä¢RQ3: Do models trained using active learning maintain
accuracy after compression? Model compression, a well-
known technique to embed DL models in resource-constraineddevices, has the downside effect of reducing the accuracydue to precision loss in the computed model weights. Weinvestigate whether models trained with active learning aremore sensitive to this phenomenon than fully trained models.Our results reveal that, for the image classiÔ¨Åcation task,training on 50% of data entails a loss in test accuracy upto 7.47% higher compared to training with the full dataset.
‚Ä¢RQ4: What is the relationship between the amount of
training data, and model robustness and accuracy aftercompression? We conduct additional experiments where we
increase the data budget of data selection metrics. Our resultsindicate that with the growth of training data, the robustnessof the model will also increase. The model can achieve similarrobustness with the fully trained model only using 35% data,and even outperform it when more data are used. This indicatesthat the training process promoted by active learning can beused as an effective way to increase robustness. As for modelcompression, there appears to be no relationship betweenaccuracy decay induced by compression and data budget.
With our extensive empirical study, we provide practical
guidance to engineers in balancing the beneÔ¨Åts of data selec-tion metrics with their potential side effects. That is, we showand quantify the existence of a trade-off between the efÔ¨Åciencyof model training (in particular, the data labeling effort) andmodel properties of interest (viz. robustness and accuracy afterquantization). Doing so, we also open research directions toexplore this trade-off and new data selection metrics aimedtowards the different quality criteria.
In summary, the main contributions of this paper are:
‚Ä¢We conduct the largest empirical study that investigatesthe effectiveness of training data selection metrics ondifferent classiÔ¨Åcation tasks (image and text).
‚Ä¢Beyond (test) accuracy, we explore the effects of reducingthe number of training data on the adversarial robustnessof the models and their accuracy after compression. We,therefore, reveal the potential effects that active learningcan have on these quality indicators.‚Ä¢Thereby, we reveal a potential trade-off between labelingcost and the aforementioned quality indicators. This pavesthe way for future research in designing multi-objectivedata selection metrics aiming at optimizing this trade-offunder a constrained labeling budget.
The rest of this paper is organized as follows. Section II
introduces some background knowledge of this work. SectionIII presents an overview of the study. Section IV introducesthe implementation and empirical conÔ¨Ågurations. Section Vdetails the results of our study. Section VI presents the relatedworks, and Section VII concludes this paper.
II. B
ACKGROUND
We brieÔ¨Çy introduce the background related to our work,
including DNNs, test selection and active learning, adversarialattacks on DNNs, and model compression.
Throughout the paper, Xrefers to the training set for a
N-class classiÔ¨Åcation DNN. x‚ààX‚äÜR
dis an input and
x/primeis its adversarial example. yandyxindicate the true and
predicted labels, respectively. pi(x),0‚â§i‚â§N, represents
the predicted probability of xbelonging to the ith class, and
correspondingly, y=a r gm a x
i=1:N(pi(x)).
A. Deep Neural Networks
In general, a DNN consists of multiple layers, i.e., an input
layer, several hidden layers, and an output layer. As shownin Figure 1, each layer comprises a number of neurons (colorcircles). The neuron with the parameters, also called a unitor a node, is the basic entity of computation of a DNN. Itreceives information from the input data or the other neuronsand computes an output by an activation function. The trainingprocess of a DNN is mainly about tuning the parameters toreach a minimum prediction error concerning true labels. Inthis paper, we focus on the classiÔ¨Åcation task where the outputof a DNN classiÔ¨Åer is the probability of belonging to eachcategory given an input. For instance, the input data in Figure1 is predicted to be in class Dog with a probability of 0.90.
Generally, there are two typical types of DNNs, i.e., Feed-
Forward Neural Networks (FNNs) and Recurrent Neural Net-works (RNNs). In an FNN, the information only moves in theforward direction from the input layer to the output layer. Thistype of DNNs is widely used in image processing applications.On the other hand, an RNN utilizes different inter-units (i.e.,memory cells, control units) to propagate the input informationin a backward way within an RNN layer, allowing the networkto retain knowledge. RNNs usually deal with sequential dataprocessing due to their ability to capture temporal informationof the data. In this work, we study both FNNs and RNNs.
B. Active Learning
Active learning, a well-known concept in both the software
engineering (SE) community and machine learning (ML)
community, trains a model incrementally with several steps.In a typical active learning procedure, in the beginning, amodel is randomly initialized. In each step of the training,it selects a few data from the unlabeled dataset to label and
918	
	


	
		

	
 	
	
	
	
	
	
	
Fig. 1. An example of a DNN (feed-forward neural networks) classiÔ¨Åer.
then retrains the model for better performance. In other words,
the goal for each step is to reduce the cost of labeling asmuch as possible by selecting the most informative data toannotate while improving the accuracy of a pre-trained model.Therefore, active learning can help in model evolution. Forinstance, in reality, a model needs to be updated over timedue to the rapid growth of new unlabeled data.
C. Test Selection
In the traditional Ô¨Åeld of SE, test selection has attracted
extensive research attention for a long time. For instance, test
selection is widely applied in regression testing [25] whichaims at reducing the size of test suites since executing theentire set is remarkably costly. Depending on the purpose,the selected test suites can help to eliminate redundant testcases (test suite minimization), test relevant changed partsof the software (test case selection), and locate faults early(test case prioritization). A similar concept to test selection isfeature selection [26]‚Äì[28] that is thoroughly studied as well.The difference is that feature selection focuses on seeking theoptimal features in a data set to allow efÔ¨Åcient execution, whiletest selection tries to reduce the size of data.
In practice, for DL-based software systems, collecting unla-
belled data is easy and cheap but labeling all of them requiresheavy work and speciÔ¨Åc domain knowledge. Following thesame spirit of test selection in traditional SE, recent researchproposed some test selection metrics for DL systems, likethe neuron coverage-based test selection. These metrics selectthe most useful subset of unlabeled test data for both testingDNNs and improving the performance of pre-trained DNNsvia retraining. We consider that these test selection metrics arepromising to apply in active learning for two main reasons.First, the procedure of test selection and then retraining,generally speaking, can be regarded as one-step active learning‚Äì active learning being by nature an incremental process.Second, the test selection metrics share the same goal ofdetermining the most useful data given a DNN. For instance,an active learning process could measure this utility as thenumber of new neurons that these data activate.
D. Adversarial Attacks on DNNs
DNNs have been proven to be vulnerable to adversarial
examples, which causes considerable security concerns [29].
Therefore, evaluating the ability (robustness) of a DNN to dealwith adversarial examples is a crucial part of DNN testing. Anadversarial example is a variant of input data by introducing asmall perturbation that is hardly recognized by human beingsbut can easily fool DNNs. The perturbation is not just randomnoise but carefully calculated by some adversarial attacks. Inthis study, we employ three powerful attacks (FGSM, JSMA,C&W) for image classiÔ¨Åcation, and two (PWWS and DWB)for text classiÔ¨Åcation.
‚Ä¢FGSM. Goodfellow et al. [30] proposed the fast gradient
sign method (FGSM) to generate adversarial examples, whichis the Ô¨Årst gradient-based and one of the most used attacks.FGSM crafts x
/primebyx/prime=x+/epsilon1¬∑sign (/triangleinv xJ(x,y))where/epsilon1
controls the perturbation size. sign (¬∑)is the sign function.
The sign of a real number is -1 for a negative value, 1 fora positive value, and 0 for value 0. /triangleinv
xJ(x,y)computes the
gradient of the training loss Jgivenxand its true class y.
‚Ä¢JSMA. The Jacobian-based Saliency Map Attack (JSMA)
[31] Ô¨Årst computes a saliency map by the Jacobian matrix. Themap presents how inÔ¨Çuential each feature (e.g., each pixel) ofthe input is to predict a particular class. Through exploitingthis map with targeting a class that does not match the trueclass of a given test sample x, JSMA modiÔ¨Åes xat where
the pixels have high-saliency values to generate an adversarialexample that might be classiÔ¨Åed within a predeÔ¨Åned thresholdŒ≥(maximum fraction of features being perturbed).
‚Ä¢C&W. Proposed by Carlini and Wagner [32], C&W is
known as one of the strongest adversarial attacks. It usesa designed loss function fto replace the training loss,
then generates the adversarial example x
/primewhich minimizes
dis(x,x/prime)+c¬∑f(x/prime)wheredisis a distance metric and cis
a constant that controls the distance and the conÔ¨Ådence of x/prime.
‚Ä¢PWWS. The probability weighted word saliency (PWWS)
[33], a word-level attack, generates text adversarial examplesby replacing original words with synonyms searched from alexical database (e.g., WordNet [34]). Given a word, PWWSselects a substitution concerning two factors. 1) How does theclassiÔ¨Åcation change if replacing this word with a substitution?2) How does the classiÔ¨Åcation change if ignoring this word?
‚Ä¢DWB . The DeepWordBug (DWB) [35] is a black-box char-
level adversarial attack, which follows two steps to generateadversarial examples. First, DWB determines the words tomodify by the change of predictions before and after replacingthe words with Unknown tokens. Second, it modiÔ¨Åes the
selected words slightly by changing at most two letters perword through a predeÔ¨Åned manner, e.g., alter world towor1d.
E. Model Compression
Model compression is important for efÔ¨Åcient model deploy-
ment in software systems, especially when using large models,e.g., big DNNs. The goal of model compression is to reducethe model size while maximally maintaining the performancein terms of accuracy before practical deployment. Next, weintroduce the two compression techniques considered in ourwork.
‚Ä¢Model pruning. Model pruning lightens the model by
removing redundant and unimportant connections that havelittle impact on the performance. In this study, we apply two
919basic pruning strategies, the weight-level pruning [36] and the
neuron-level pruning [37]. The weight-level method sets theweights that are smaller than a threshold to zero. The neuron-level pruning removes the neurons that have a high chanceof being inactivated. Correspondingly, the related connections(weights) are also eliminated.
‚Ä¢Model quantization. In DNN, weights are stored in the
32-bit Ô¨Çoating-point format. To compress the model, thequantization technique converts the weights from 32-bit intolow-bit (e.g., 8-bit integer).
III. O
VERVIEW
We Ô¨Årst introduce the three-phase design of our empirical
study, then present data selection metrics, datasets and models,and evaluation measures that are studied in our work.
The secure life cycle of deep learning is composed of
some key stages [38] from the requirement analysis and data-label pair collection to the maintenance and evolution of thedeep learning model. This paper studies the effect of the datacollection (i.e., active learning) on the model developmentand deployment (i.e., the model quality). Figure 2 gives anoverview of our study, which consists of three phases, effec-tiveness analysis of model training with different data selectionmetrics, adversarial robustness analysis of the trained model,and performance analysis after model deployment. Overall,we compare the models trained using the entire dataset and asubset of data selected by a speciÔ¨Åc data selection metric.
A. Study Design
More speciÔ¨Åcally, in the Ô¨Årst phase (data collection), we
compare the effectiveness of each data selection metric for
training a model. Using different metrics (e.g., Entropy, Mar-gin), we iteratively select a subset of training data and train themodel, then observe the convergence trend of the test accuracy.For comparison, we also train two baseline models using entiretraining data and randomly selected data, respectively.
In the second phase (model development), we evaluate
the robustness of the trained model. In our study, we applymultiple metrics (e.g., empirical robustness, CLEVER score)to compare the robustness of models trained with the selectedand the model trained by the entire training data.
In the third phase, we focus on the model performance (test
accuracy) in the model deployment phase. In general, a DNNmodel usually consists of a huge number of parameters, e.g.,a VGG16 model requires about 258MB of hard disk memory.Before deploying such a large DNN model into the hardware(e.g., mobile devices), one has to consider the performanceincluding the required memory and inference speed. We adopttwo well-known techniques (model quantization and modelpruning) to optimize a trained model. Then we evaluate theaccuracy of the optimized models. Besides, we study theimpact of training data size on the robustness of models andthe accuracy of optimized models.
Finally, based on the results of our empirical study, we
provide some practical guidelines for the usage of activelearning on different tasks and summarize some potentialresearch directions.B. Datasets and DNN Models
We conduct experiments with two popular image datasets
(MNIST [39] and CIFAR-10 [40]) and three widely used textdatasets (IMDb [41], TagMyNews [42], and Yahoo! Answers[43]). MNIST includes 10-class grayscale images of hand-written digits. The dataset includes 60000 and 10000 trainingand test data, respectively. CIFAR-10 is a collection of 10-class color images (e.g., airplane, bird). The dataset consistsof 50000 and 10000 training and test data, respectively. IMDbis a dataset including movie reviews widely used for textsentiment analysis (binary classiÔ¨Åcation). Both the trainingand test sets include 25000 text reviews. TagMyNews providesnews headlines (text) in 7 categories (e.g., Sport, Business).We randomly collect 20000 data for training and 2000 datafor testing. Yahoo! Answers consists of text data of 10 topiccategories (e.g., Society & Culture, and Science & Mathemat-ics). We obtain this data from [33] directly with 3560 trainingdata and 889 test data.
For each dataset, we employ two DNN architectures to
reduce the model-dependent inÔ¨Çuence on the results. ForMNIST, we use two well-known convolutional neural net-works LeNet-1 and LeNet-5 [44], and for CIFAR-10, we selecttwo models NiN [45] and VGG16 [46], both of which achievehigh accuracy. For IMDb, TagMyNews, and Yahoo! Answers,we use two types of RNNs, LSTM, and GRU, derived from abase model [47]. Our companion website presents all detailsabout the models and training parameters [48].
C. Data Selection Metrics
Various data selection metrics have been proposed and
veriÔ¨Åed to reduce the labeling effort. Note that the data
selection metric is also known as the acquisition function inthe ML community. We include 14 data selection metrics fromboth the ML community (8 metrics) and the SE community (6metrics). We Ô¨Årst introduce the ones from the ML community.
‚Ä¢Entropy [8] considers the uncertainty of data using the
prediction output. This metric is based on the Shannon entropyof the prediction probability:
arg max
x‚ààX/parenleftBiggN/summationdisplay
i=1pi(x)l o gpi(x)/parenrightBigg
(1)
‚Ä¢Margin [8] computes a score for each data by the difference
between its top-2 prediction probabilities:
Margin (x)=p k(x)‚àípj(x) (2)
wherek=a r g m a x
i=1:N(pi(x))andj=a r g m a x
i={1:N }/k(pi(x)). The
training data with low scores will be selected for training.
‚Ä¢K-center [10] Ô¨Årstly divides data into Kgroups via some
unsupervised machine learning methods (e.g., K-means clus-tering), then selects the center of each group (if not enough,consider the data that are close to the center). These selecteddata are regarded as the representative of the entire group.
‚Ä¢Expected Gradient Length (EGL) [49] assumes that the
model has no knowledge of the true label of data in advance.
920








 
	%# 
 !
	&# 
!
 "

	'#

!

	




		 	
 	



"
 $
	(#
 
 
!
	




Fig. 2. Overview of experiment design
For each data, it computes the expectation of the gradients
by assigning all the labels to x. The data that have great
expectations are selected. EGL can be presented as follows:
EGL (x)=N/summationdisplay
i=1pi(x)||/triangleinvxJ(x,i)|| (3)
where||.||is the Euclidean norm. /triangleinvxJ(x,i)is the gradient of
the lossJgivenxand label i.
‚Ä¢Bayesian Active Learning by Disagreement (BALD) [50]
applies dropout to select the most uncertain data:
arg max
x‚ààX/parenleftBigg
1‚àícount/parenleftbig
mode/parenleftbig
y1
x,...,yT
x/parenrightbig/parenrightbig
T/parenrightBigg
(4)
whereTis the number of applying dropout to the model.
‚Ä¢Entropy-dropout and Margin-dropout [9] apply dropout
and calculate the average score of Entropy and Margin overall dropout models, e.g., Entropy-dropout is deÔ¨Åned as
arg max
x‚ààXT/summationtext
i=1Entropy/parenleftbig
xi/parenrightbig
T(5)
whereEntropy/parenleftbig
xi/parenrightbig
is the entropy of xat thei-th time.
‚Ä¢Adversarial active learning (Adversarial al)[11] lever-
ages an adversarial attack, DeepFool, to facilitate selectingdata. Concretely, the model predicts labels for each data, thenDeepFool introduces perturbations to each data until reachesan adversarial example. Finally, the data requiring smallerperturbations will be selected. Intuitively, such data are closeto the decision boundary of the model.
Next, we introduce the metrics from the SE community.
‚Ä¢Neuron Coverage (NC) [16] selects data that have the
largest neuron coverage:
arg max
x‚ààX|{neu|neu‚ààNeurons ‚àßactivate (neu,x )}|
|Neurons |(6)whereactivate (neu,x )indicates that the neuron neu is
activated by x, namely, the output of neu is greater than a
predeÔ¨Åned threshold. We highlight that this is a variant of theoriginal metric to Ô¨Åt active learning. The original NC aims atselecting data to cover all the neurons, however, in practice,just a few data are enough to reach 100% coverage [51].Namely, after a few selection steps, all the neurons have beenactivated at least once and the marginal increase of coveragewill be 0. Thus, we apply this variant to Ô¨Åt active learning.
‚Ä¢K-Multisection Neuron Coverage (KMNC) [15] improves
NC by splits the lower and upper bounds of a neuron‚Äôs outputintoksections. Instead of using the coverage of neurons, it
considers the coverage of sections. Similar to NC, the datawith high coverage will be selected.
‚Ä¢Multiple-Boundary Clustering and Prioritization (MCP)
[18] is an extension of Margin. First, it divides data intovarious ‚Äúboundary areas‚Äù based on the top-2 predicted classes,e.g., for data with 10 classes, there are P(10,2) = 90
permutations of pairwise classes. Next, MCP selects data fromeach area based on Margin.
‚Ä¢DeepGini [19] selects the most uncertainty data by:
arg max
x‚ààX/parenleftBigg
1‚àíN/summationdisplay
i=1(pi(x))2/parenrightBigg
(7)
‚Ä¢Likelihood-based Surprise Adequacy (LSA) and Distance-
based Surprise Adequacy (DSA) [17] measure the surprise
adequacy by the dissimilarity between a test data and thetraining set. The difference between LSA and DSA is thatLSA uses kernel density estimation to estimate the surpriseadequacy, while DSA uses Euclidean distance. Both select datawith the largest surprise adequacy.
We remark that although the initial purpose of these SE
metrics is not for active learning, their underlying selectioncriteria share similarities with the criteria that active learningmetrics rely on. For example, both Entropy (active learningmetric) and DeepGini (SE metric) metrics select the uncertaindata based on the output probabilities. Therefore, we believe
921that it is necessary to consider them in our study. Besides, no
existing research has revealed that whether these SE metricsÔ¨Åt in active learning or how they perform.
D. Evaluation Metrics
Effectiveness. To evaluate the effectiveness of a selection
metric, we use random selection as the baseline and calculatethe difference of accuracy between the models trained usingrandom selection and this metric. The difference is deÔ¨Åned by:
diff =
steps/summationdisplay
i=1/parenleftbig
Acci
TS‚àíAcciTR/parenrightbig
(8)
wheresteps is the total number of training steps. Acci
TSis
the test accuracy of the model trained by a selection metric,
andAcci
TRis by random selection. diff shows the degree of
a metric outperforming random selection.
Adversarial robustness. The robustness of DNNs refers to
the ability to cope with adversarial examples. The robustness
of DNNs can be evaluated in multiple ways. We introducetwo popular methods of robustness estimation, Empirical Ro-bustness [52] and CLEVER score [24]. 1) Empirical Robust-
ness This method quantiÔ¨Åes the robustness of a DNN model
through the success rate of crafting adversarial examples byan attack. In practice, given a DNN and a test set S,a n
attack attempts to craft an adversarial example based on eachtest data. The attack success rate is the ratio of adversarialexamples successfully generated:
ASR =|{x|x‚ààS‚àßy
x/prime/negationslash=y}|
|S|(9)
Recall that yx/primeandyare the predicted label of x/primeand true
label of x, respectively. A small ASR indicates a robust
DNN. 2) CLEVER Score The Cross-Lipschitz Extreme Value
fornEtwork Robustness (CLEVER) score calculates the lowerbound for crafting an adversarial example given an input,which is the least amount of perturbation required to fool aDNN model. A great CLEVER score indicates a robust DNN.
IV . I
MPLEMENTATION AND CONFIGURATION
Experimental environment. This project is implementedbased on Keras [53] and TensorFlow [54] frameworks. Werun all experiments on a high-performance computer clusterexcept the model pruning and quantization. Each cluster noderuns a 2.6 GHz Intel Xeon Gold 6132 CPU with an NVIDIATesla V100 16G SXM2 GPU. For the model pruning andquantization, we conduct the experiments on a MacBook Prolaptop with macOS Big Sur 11.0.1 with a 2GHz GHz Quad-Core Intel Core i5 CPU with 16GB RAM.
Active learning. We initialize an empty labeled pool and
an unlabeled pool with all the unlabeled data. Besides, weinitialize the with random weights. Next, in each trainingstep, a Ô¨Åxed number (step size) of data are selected from theunlabeled pool by a speciÔ¨Åc metric. Then the selected dataare merged into to labeled pool after annotation. The DNN isupdated by retraining using all the data in the labeled pool. TheTABLE I
CONFIGURATIONS OF ACTIVE LEARNING
Dataset Model Step size Stop point Entire training size
MNIST LeNet-1, LeNet-5 500 10000 60000
CIFAR-10 NiN, VGG16 2500 25000 50000
IMDb LSTM, GRU 500 12500 25000
TagMyNews LSTM, GRU 500 12500 25000
Yahoo!Answers LSTM, GRU 500 12500 25000
TABLE II
CONFIGURATIONS OF ADVERSARIAL ATTACKS .FOR THE DEFINITION OF
THE PARAMETERS (/epsilon1,Œ≥,c,dis),PLEASE REFER TO SECTION II.
DatasetFGSM JSMA C&W CLEVER
/epsilon1 Œ≥ c dis
MNIST 0.1, 0.2, 0.3 0.09, 0.1, 0.11 9, 10, 11L-1, L-2, L-infCIFAR-10 0.01, 0.02, 0.03 0.01, 0.02, 0.03 0.1, 0.2, 0.3
procedure terminates when the size of the labeled pool reachesa threshold (stop point). Table I lists the detailed settings. Notethat previous works have different parameter settings [8], [9],[20], [55], we balance these settings to set up our experiments.We implement data selection metrics based on [20].
Robustness. We use two public libraries, Foolbox [56] for
empirical robustness evaluation and ART [57] for CLEVERscore calculation. In empirical robustness, we apply threeattack methods, i.e., FGSM, JSMA, and C&W, for the imageclassiÔ¨Åcation task, and conduct three groups of experimentswith different parameters as in [58]. For the text classiÔ¨Åcationtask, we use PWWS and DWB with the default setting in[33], [35] to attack the text-related models. By default, onlythe correctly classiÔ¨Åed data undertake the attacks. In CLEVERscore, the setting of canddisfollows the conÔ¨Åguration in [58].
One difference is that we use 500 test data to calculate theCLEVER score, while [58] used 50. The detailed informationis shown in Table II. Note that the setting of CLEVER worksboth for the image and text classiÔ¨Åcation tasks.
Model compression In model quantization, we apply two
lightweight frameworks, CoreML [59] and TensorFlowLite[60], to transform DNNs into different bit-level versions. ForCoreML, we use three levels, 2-bit, 4-bit, and 8-bit. ForTensorÔ¨ÇowLite, we apply 8-bit and 16-bit level quantizationsin our models since it only supports these two levels. In modelpruning, for both the weight- and neuron-level, we prune aDNN into six compress versions with different degrees, from10% to 60% at 10% intervals, using the implementations by[36], [37]. As a reminder, model pruning is conducted afterthe model is well-trained.
Last but not least, to reduce the inÔ¨Çuence of randomness, we
repeat each experiment three times and compute the averageresults. In total, we trained and evaluated more than 2000models in this study. The source code can be found on
2.
V. E XPERIMENTAL RESULTS
In this section, we present the results and answer each
research question mentioned in Section I. In the remainingparts, we remark that ‚ÄúTE‚Äù, ‚ÄúTS‚Äù, and ‚ÄúRandom‚Äù represent
2https://github.com/code4papers/ALempirical
922TABLE III
EFFECTIVENESS WITH RESPECT TO RANDOM SELECTION (BASELINE ).
THE RESULTS ABOVE THE BASELINE ARE HIGHLIGHTED IN GRAY .
MNIST CIFAR-10 IMDb TagMyNews Yahoo!AnswersMetricLeNet-1 LeNet-5 NiN VGG16 LSTM GRU LSTM GRU LSTM GRUAverage
Entropy -131.88 -183.84 26.14 3.91 49.41 44.13 29.63 37.72 3.19 -20.06 -14.16
Margin 69.82 28.78 14.44 6.29 - -29.68 36.23 26.10 17.44 28.59
K-center -23.28 28.63 21.00 -7.67 62.94 39.15 25.60 36.63 9.52 15.97 20.84
EGL -11.71 -14.13 -30.46 -17.65 3.62 -31.02 -14.55 -14.85 -155.46 -82.90 -36.91
BALD 27.84 -25.22 21.98 9.04 57.62 53.44 28.42 27.43 20.02 6.41 22.69
Entropy-dropout -81.71 -165.96 22.88 3.18 64.81 48.91 30.23 34.43 2.29 17.51 -2.34
Margin-dropout 43.57 14.67 1.19 6.56 - -26.83 41.00 25.68 29.92 23.67
Adversarial al 33.23 22.69 -77.73 29.15 - - - - - - 1.835
NC -26.95 -182.01 -2.76 -10.95 36.28 12.69 30.77 25.27 -72.29 -93.89 -28.38
KMNC 3.80 -153.02 0.40 12.69 34.02 39.39 -4.35 0.83 -392.01 -375.40 -83.36
MCP 25.66 8.39 14.28 13.17 - -24.53 30.70 11.59 17.02 18.16
DeepGini -74.07 -213.23 21.33 12.98 - -30.62 31.98 20.47 -2.59 -21.56
LSA 19.83 22.37 3.41 8.97 -4.53 5.10 -1.22 -1.32 -385.71 -359.77 -69.28
DSA 9.26 23.62 1.37 10.49 5.54 5.34 -7.87 -0.78 -392.39 -359.47 -70.48
training using the entire dataset, the subset selected by data
selection metrics, and randomly selected data, respectively.Note that due to the page limitation, we only illustrate a part ofthe results in this paper, and the complete results are providedas supplementary material [48]. The conclusions we drawbelow generalize to all studied datasets/subjects and DNNs.
A. RQ1: Effectiveness of Data Selection Metrics
First, we show, in Figure 3, the performance of using dif-
ferent data selection metrics to train a DNN that achieves the
same test accuracy as the fully trained model. For comparison,a horizontal dashed line in each sub-Ô¨Ågure represents theaccuracy of the fully trained model. Overall, most metricsmanage to produce models with the same accuracy as the fullytrained model by using only 7% to 50% of training data.
Next, Table III offers a more detailed comparison of these
metrics, showing their effectiveness taking random selectionas a baseline (measured using Equation 8). Surprisingly, onlythree metrics (Margin, Margin-dropout, MCP) signiÔ¨Åcantlyoutperform the baseline in all cases. On the contrary, in mostcases (9 out of 10), EGL is worse than random selection. Thereare some metrics like LSA and DSA, which outperform thebaseline on the image classiÔ¨Åcation task but underperform thebaseline on the text classiÔ¨Åcation task, e.g., on Yahoo-LSTMand Yahoo-GRU, and the difference reaches up to 392.39. Weconjecture that LSA and DSA tend to select data based onsimilarity or distance. However, different from image data,the two texts data (sentence or document) might have a bigdifference in the hidden space even if they are in the samecategory. In this case, the inter-output of the model againstthese two data can be completely different which makes LSAand DSA select the wrong data.
SpeciÔ¨Åcally, considering the image classiÔ¨Åcation task, we
observe that in addition to Margin, Margin-dropout, and MCP,two other metrics, LSA and DSA, can always outperformthe baseline. What‚Äôs more, on LeNet-1 and LeNet-5, halfof the metrics are worse than the random baseline. Espe-cially, Entropy, Entropy-dropout, NC, and DeepGini get highnegative differences, e.g., -131.88 for Entropy on LeNet-1,which means these four metrics are much worse than randomselection. Turn to sub-Ô¨Ågures 3(a) and 3(b), in the Ô¨Årst fewtraining steps (where the big difference comes from), thesefour metrics achieve much less test accuracy than the others(also random selection). One explanation is that Entropy,Entropy-dropout, and DeepGini try to Ô¨Ånd the most uncertaindata. However, such uncertain data are hard to be learned bymodels that are not well-trained.
On the other hand, for the text classiÔ¨Åcation task, the output
probability-based metrics (e.g., Margin and BALD) performbetter than the coverage and surprise adequacy-based metrics.Extremely, LSA performs worse than the baseline on 5 (out of6) models. Besides, on model Yahoo
LSTM, the most efÔ¨Åcient
metric (Margin) is much better than the worst one (KMNC)with a 418.49% test accuracy gap. These results reÔ¨Çect thatboth coverage and surprise adequacy based-metrics are notsuitable for the text classiÔ¨Åcation task.
Answer to RQ1: The nature of the classiÔ¨Åcation task
signiÔ¨Åcantly affects the effectiveness of multiple data selectionmetrics (e.g., LSA and DSA). Therefore, the limitation ofactive learning experiments to a single target task ‚Äì evenwith multiple datasets ‚Äì constitutes a critical threat to externalvalidity. Some metrics, like Margin, Margin-dropout, andMCP, consistently perform well across all labeling budgets,models, and tasks.
B. RQ2: Adversarial Robustness
We then study the robustness of models trained by different
data selection metrics against adversarial attacks. Figure 4
illustrates the empirical robustness of the models againstvarious attacks. Once again, we have to distinguish the twotypes of tasks since the results of different metrics are greatlybiased on the tasks. For the image classiÔ¨Åcation task, the TEmodels are usually more robust than the TS models. However,for the text classiÔ¨Åcation task, the TS models are in somecases more robust than the TE models (ad vice-versa). Weconjecture that two factors may affect the robustness of themodels: (i) the number of data used for training and (ii) thetraining process. In active learning, the early selected data aretrained more times during incremental learning. Thus, the mostinformative data (according to the data selection metrics) havea larger inÔ¨Çuence on the model weights and, in turn, impact itsrobustness. On the other hand, since the selected data can berepresentative of the entire dataset to some extent, the differ-ence between the TS and TE models is small. Taking VGG16as an example, the difference varies from 1.67% to 17.21%in FGSM, from 1.42% to 3.21% in C&W, and from 0.32%to 10.54% in JSMA. Another observation that reinforces ourhypothesis regarding the importance of the training processis that none of the metrics performs consistently better thanrandom selection. We investigate this hypothesis in the RQ4experiments, where we consider different labeling budgets.
Table IV lists the CLEVER score of different models. As
for the text classiÔ¨Åcation task, models trained with activelearning either yield a small improvement (less than 0.58CLEVER score) or offer inconsistent beneÔ¨Åt (either increasingor decreasing the CLEVER score, depending on the consideredmetric and norm distance). Besides, none of the data selectionmetrics improves over the random selection, even the threemetrics which performed better in terms of effectiveness (viz.
9230 2000 4000 6000 8000 10000
Data size0.20.40.60.81.0Test accuracyDSA
LSA
Entropy
BALD
K_center
Margin
Entropy_dropout
Margin_dropout
EGL
NC
MCP
Adversarial_al
KMNC
DeepGini
Random
(a) MNIST, LeNet-10 2000 4000 6000 8000 10000
Data size0.20.40.60.81.0Test accuracyDSA
LSA
Entropy
BALD
K_center
Margin
Entropy_dropout
Margin_dropout
EGL
NC
MCP
Adversarial_al
KMNC
DeepGini
Random
(b) MNIST, LeNet-50 5000 10000 15000 20000 25000
Data size0.20.30.40.50.60.70.80.9Test accuracyDSA
LSA
Entropy
BALD
K_center
Margin
Entropy_dropout
Margin_dropout
EGL
NC
MCP
Adversarial_al
KMNC
DeepGini
Random
(c) CIFAR-10, NiN0 5000 10000 15000 20000 25000
Data size0.600.650.700.750.800.850.90Test accuracyDSA
LSA
Entropy
BALD
K_center
Margin
Entropy_dropout
Margin_dropout
EGL
NC
MCP
Adversarial_al
KMNC
DeepGini
Random
(d) CIFAR-10, VGG160 2000 4000 6000 8000 10000 12000
Data size0.500.550.600.650.700.750.800.85Test accuracyDSA
LSA
Entropy
BALD
K_center
Entropy_dropout
EGL
NC
KMNC
Random
(e) IMDb, LSTM
0 2000 4000 6000 8000 10000 12000
Data size0.500.550.600.650.700.750.800.85Test accuracyDSA
LSA
Entropy
BALD
K_center
Entropy_dropout
EGL
NC
KMNC
Random
(f) IMDb, GRU0 2000 4000 6000 8000 10000
Data size0.20.30.40.50.60.70.80.9Test accuracyDSA
LSA
Entropy
BALD
K_center
Margin
Entropy_dropout
Margin_dropout
EGL
NC
MCP
KMNC
DeepGini
Random
(g) TagMyNews, LSTM0 2000 4000 6000 8000 10000
Data size0.20.30.40.50.60.70.80.9Test accuracyDSA
LSA
Entropy
BALD
K_center
Margin
Entropy_dropout
Margin_dropout
EGL
NC
MCP
KMNC
DeepGini
Random
(h) TagMyNews, GRU0240 400 640 8000 8240 8400 8640
1D a D t  s iz0e80e20e.0e30e40e50e60e70e9Tz a tDccurDcy1S A
LSA
Ena ropy
BAL1
K_cznazr
MD rgs n
Ena ropy_dropoua
MDrgs n_dropoua
EGL
NC
MCP
KMNC
1zzpGs ns
RDndom
(i) Yahoo, LSTM0240 400 640 8000 8240 8400 8640
1D a D t  s iz0e80e20e.0e30e40e50e60e70e9Tz a tD ccurD cy1S A
LSA
Ena ropy
BAL1
K_cznazr
MD rgs n
Ena ropy_dropoua
MDrgs n_dropoua
EGL
NC
MCP
KMNC
1zzpGs ns
RDndom
(j) Yahoo, GRU
Fig. 3. Evolution of the test accuracy (y -axis) achieved by different data selection metrics given the number (x-axis) of training data.
FGSM-0.01 FGSM-0.02 FGSM-0.03CW-0.1 CW-0.2 CW-0.3JSM A-0.01 JSM A-0.02 JSM A-0.03
Entropy
BALD
K-center
Margin
Entropy-dropout
Margin-dropout
EGL
NC
MCP
Adversarial al
KMNC
DeepGini
LSA
DSA
Random
TE67.11 87.54 91.96 98.17 98.38 98.70 68.27 92.25 97.75
65.19 86.49 91.70 99.49 99.58 99.67 68.33 92.25 97.96
63.75 86.69 92.14 98.69 98.91 99.12 68.78 92.31 98.04
67.14 87.97 92.58 99.26 99.39 99.53 69.42 92.51 97.93
68.03 88.40 93.40 99.52 99.58 99.67 68.99 92.35 98.04
65.97 86.81 91.65 99.48 99.54 99.62 69.44 93.02 98.11
75.38 84.84 88.00 99.17 99.32 99.32 70.09 90.19 96.85
60.16 85.62 93.57 99.96 99.96 99.97 65.98 90.74 97.44
64.30 86.20 91.76 98.49 98.68 98.88 66.89 90.57 97.20
64.25 86.98 92.89 99.67 99.73 99.77 68.81 91.94 97.65
58.70 84.29 92.45 99.90 99.90 99.90 65.16 90.59 97.43
64.69 87.69 93.52 99.68 99.73 99.76 67.39 91.96 98.01
57.94 84.43 92.72 99.46 99.50 99.54 65.43 91.72 97.90
59.50 84.89 92.45 98.68 98.73 98.93 65.72 91.08 97.47
60.61 86.42 91.09 98.66 98.90 99.13 69.55 91.22 97.55
56.27 71.19 77.48 96.75 96.88 97.04 59.55 87.52 96.54
6065707580859095Success rate
(a) CIFAR-10, VGG16
TEEntropyBALDK_centerMarginEntropy_dropout Margin_dropout
EGL NCMCPKMNCDeepGini
LSA DSARandom
LSTM-PWWS
LSTM-DWB
GRU-PWWS
GRU-DWB82.11 80.98 83.64 83.55 81.38 82.20 81.74 82.30 81.11 80.08 80.64 81.21 80.02 79.88 81.23
65.57 63.50 68.53 68.63 65.77 67.47 68.53 66.73 68.00 63.73 67.70 67.40 65.80 65.53 67.20
81.98 79.56 80.97 80.19 81.21 81.11 79.52 80.82 80.69 82.07 80.37 79.77 79.03 81.06 81.2170.23 66.50 69.43 67.23 69.90 69.40 70.43 69.17 69.37 68.87 70.60 68.40 65.15 70.33 71.40
65.0 67.5 70.0 72.5 75.0 77.5 80.0 82.5Success rate
(b) TagMyNews
Fig. 4. Adversarial attack success rate (%) of VGG16 and TagMyNews. The number in each cell represents the success rate and the color gives a straightforward
visual comparison of different values. The most robust model is framed by a red rectangle. The lower the success rate, the better robustness.
Margin, Margin-dropout, and MCP). Overall, these results
corroborate our previous Ô¨Åndings. That is, for the imageclassiÔ¨Åcation task, active learning yields less robust models.
TABLE IV
THECLEVER SCORE OF CIFAR-10 (VGG16) AND TAGMYNEWS
(LSTM). T HE RESULTS THAT ARE BETTER THAN THE TE MODEL ARE
HIGHLIGHTED IN GRAY .THE HIGHER SCORE ,THE BETTER ROBUSTNESS .
CIFAR-10 (VGG16) TagMyNews (LSTM)MericL-1 L-2 L-inf L-1 L-2 L-inf
Etropy 3.7143 0.3289 0.0055 3.6466 1.1559 0.0531
Margin 3.6607 0.2136 0.0051 3.5496 2.0390 1.6712
K-center 3.6475 0.3132 0.0057 3.7970 0.9840 0.0132
EGL 4.2577 0.6996 0.0037 3.6881 0.7235 0.0276
BALD 3.8096 0.3176 0.0048 3.7424 1.3236 2.9914
Etropy-dropout 3.6881 0.1495 0.0075 3.4033 0.7465 0.4027
Margin-dropout 3.8213 0.3103 0.0047 3.5453 0.8562 0.0059
Adversarial al 4.2606 0.5242 0.0032 - - -
NC 4.2036 0.7017 0.0031 3.9315 0.9729 0.1879
KMNC 4.2628 0.6442 0.0340 3.1824 0.4198 0.0041
MCP 4.2955 0.4984 0.0041 3.7244 1.1289 0.8011
DeepGini 4.2616 0.5116 0.0031 3.4139 2.6286 1.6805
LSA 4.2449 0.7477 0.0041 3.8102 0.7150 0.0077
DSA 4.2567 0.6576 0.0035 3.7198 1.0144 0.0102
Random 3.9665 0.6499 0.0057 3.6295 1.6243 0.0065
TE 4.3636 0.7399 0.8053 3.3600 0.9910 1.6684Answer to RQ2: For the image classiÔ¨Åcation task, models
trained with active learning can have lower robustness thanmodels trained with the entire dataset, and the difference canbe up to 1.49 CLEVER score and 23.39% attack successrate. Therefore, experimental studies should involve evaluationmetrics beyond clean accuracy. For the text classiÔ¨Åcationtask, the results are inconsistent across attacks (empiricalrobustness) and distance norm (CLEVER score), indicatingthat other factors are at play when it comes to robustness,e.g., the training process.
C. RQ3: Test Accuracy After Model Compression
We compare the test accuracy of a model produced by
different data selection metrics before and after compression.
Table V shows the results of VGG16 (image classiÔ¨Åcation)and TagMyNews-LSTM (text classiÔ¨Åcation) by quantization.
On VGG16, the accuracy of the 2-bit compressed models
drops signiÔ¨Åcantly by at least 76.32%, no matter it was fullytrained or with active learning. By contrast, on TagMyNews-LSTM, the accuracy decay using 2-bit is relatively small (lessthan 9.42%). The reason could be that, 1)the quantization
924process has less impact on the LSTM layer, or 2)the text
data is less sensitive to the precision of weights. In both
cases, the compressed models achieve the same accuracy asthe original models with 16-bit (expect Margin and Entropy-dropout On TagMyNews-LSTM). SpeciÔ¨Åcally, compared withrandom selection, on both VGG16 and TagMyNews-LSTM,no metric always outperforms the baseline. Comparing withthe TE model, we found that for the image classiÔ¨Åcation task,the compressed TE model always maintains higher (with a gapup to 7.47%) test accuracy. However, for the text classiÔ¨Åcationtask, the TS model sometimes loses more test accuracy thanthe TS models after quantization. Surprisingly, with 2-bitquantization, the TE model gets the largest accuracy decay(-9.42%).
TABLE V
THE CHANGE OF TEST ACCURACY OF CIFAR-10 (VGG16) AND
TAGMYNEWS (LSTM) BEFORE AND AFTER MODEL QUANTIZATION .THE
BEST AND WORST RESULTS ARE HIGHLIGHTED IN GRAY AND ORANGE ,
RESPECTIVELY .
CIFAR-10 (VGG16) TagMyNews (LSTM)
CoreML TFLite CoreML TFLite
2-bit 4-bit 8-bit 8-bit 16-bit 2-bit 4-bit 8-bit 16-bit
Entropy -78.13 -4.77 -0.09 -0.53 0 -2.13 -0.2 0 0
Margin -78.59 -3.06 -0.14 -0.76 0 -2.5 -0.28 -0.03 -0.02
K-center -76.32 -2.28 -0.81 -5.18 0 -1.72 -0.1 +0.02 0
EGL -76.72 -2.35 -0.02 -0.63 0 -0.22 -0.18 +0.12 0
BALD -77.99 -2.34 -0.01 -0.2 0 -3.9 -0.03 +0.02 0
Etropy-dropout -77.92 -4.53 -0.76 -0.48 0 -2.82 -0.1 -0.03 -0.02
Margin-dropout -78.02 -1.56 -0.01 -0.39 0 -2.57 -0.22 -0.02 0
Adversarial al -79.77 -2.05 -0.05 -3.2 0 - - - -
NC -77.17 -8.85 -0.22 -2.39 0 -2.57 -0.12 +0.02 0
KMNC -78.92 -1.65 -0.06 -2.67 0 -2.88 -0.22 +0.03 0
MCP -79.72 -1.56 -0.02 -3.64 0 -4.17 -0.07 0 0
DeepGini -80.2 -3.47 0 -3.02 0 -8.73 -0.13 -0.03 0
LSA -78.16 -1.74 -0.01 -2.53 0 -4.1 -0.12 -0.02 0
DSA -78.28 -1.99 -0.04 -2.88 0 -2.88 0 +0.02 0
Random -77.82 -1.74 -0.24 -1.77 0 -2.32 -0.42 -0.13 0
TE -81.36 -1.38 0 -0.03 0 -9.42 -0.07 0 0
Figure 5 depicts the result by weight- and neuron-level prun-
ing. In general, with pruning more weights and neurons, thetest accuracy decreases gradually up to 36.20% and 81.46%on VGG16, respectively. However, on TagMyNews-LSTM, theaccuracy changes negligibly by increasing or decreasing up to0.4%. The reason might be that these two pruning methods canonly affect the Convolutional layer and the Dense layer, whileour LSTM models only contain one Dense layer to output theÔ¨Ånal prediction probability. Looking into VGG16, the neuron-level pruning affects the accuracy more than the weight-levelfor all the data selection metrics, which suggests that inpractical applications, the engineers should consider more theweight-level pruning than the neuron-level to minimize theaccuracy loss. Among these data selection metrics, DeepGiniand NC are always better than random selection. Besides,for the image classiÔ¨Åcation task, the TE model outperformsall TS models with a gap up to 33.4%. However, for thetext classiÔ¨Åcation task, the TE model has no advantage ofmaintaining test accuracy over TS models after pruning.
Answer to RQ3: Model compression inconsistently affects
the performance of the models trained with active learning andno data selection metric provides satisfactory results acrossall tasks. For the image classiÔ¨Åcation task, after compression,the fully trained models hold higher test accuracy than themodels trained with active learning, and the gap can be up to0 10 20 30 40 50 60
Percentage05101520253035Accuracy decreaseDSA
LSA
Entropy
BALD
K_center
Margin
Entropy_dropout
Margin_dropout
EGL
NC
MCP
Adversarial_al
KMNC
DeepGini
Random
TE
(a) VGG16, Pruning-weight0 10 20 30 40 50 60
Percentage01020304050607080Accuracy decreaseDSA
LSA
Entropy
BALD
K_center
Margin
Entropy_dropout
Margin_dropout
EGL
NC
MCP
Adversarial_al
KMNC
DeepGini
Random
TE
(b) VGG16, Pruning-neuron
0 10 20 30 40 50 60
Percenta ge0.10.00.10.20.30.4Accuracy decreaseDSA
LSA
Entropy
BALD
K_center
Margin
Entropy_dropout
Margin_dropout
EGL
NC
MCP
KMNC
DeepGini
Random
TE
(c) TagMyNews-LSTM,
Pruning-weight0 10 20 30 40 50 60
Percenta ge0.100.050.000.050.100.15Accuracy decreaseDSA
LSA
Entropy
BALD
K_center
Margin
Entropy_dropout
Margin_dropout
EGL
NC
MCP
KMNC
DeepGini
Random
TE
(d) TagMyNews-LSTM,
Pruning-neuron
Fig. 5. The change of test accuracy (y -axis) after model pruning with different
degrees (x-axis), i.e. the proportion of weights and neurons being pruned.
7.47% (quantization) and 33.4% (pruning). On the contrary,for the text classiÔ¨Åcation task, the fully trained models haveno advantage of test accuracy over the models trained withactive learning after model compression.
D. RQ4: Impact of Training Data Size
From Sections V-B and V-C, we found that the data selec-
tion metrics tend to produce less robust models and introduce
greater changes of accuracy after model compression. Wefurther extend our experiments to investigate the impact of thedata size on the quality (e.g., adversarial robustness and testaccuracy after compression) of models. Figure 6 shows theresults on VGG16 and TagMyNews-LSTM. For adversarialattacks, we use JSMA-0.01 for VGG16 and PWWS forTagMyNews-LSTM, respectively. For model compression, weemploy the 4-bit quantization by CoreML. On both models, weshow the results by two data selection metrics: the predictionprobability-based Entropy and neuron coverage (NC).
Figure 6(a) and Figure 6(b) show that TS models become
more robust with more training data added, especially, themodels could be more robust than the fully trained modelsin the end. According to the results, we can see that toachieve similar robustness with fully trained models, more than60% of training data are required for the image classiÔ¨Åcationtask, while only 35% of training data is enough for the textclassiÔ¨Åcation task. This corroborates our hypothesis (see RQ2)that the training process used by active learning (which makesdata selected earlier go through more training iterations thandata selected later) can yield more robust models comparedto a traditional training process involving all data from thebeginning. This Ô¨Ånding also opens the perspective of usingsuch an active learning process in conjunction with commonmethods to improve robustness, such as adversarial training.
Figure 6(c) and Figure 6(d) show the test accuracy decay
after model compression. For the text classiÔ¨Åcation task, thedifference is negligible throughout the increase in data size.
9250 10000 20000 30000 40000
Data size5560657075Success rate (%)NC
Entropy
(a) VGG16, Robustness02500 5000 7500 10000 12500 15000 17500
Data size787980818283848586Success rate (%)NC
Entropy
(b) TagMyNews-LSTM,
Robustness
0 10000 20000 30000 40000
Data size0102030405060Accuracy decreaseNC
Entropy
(c) VGG16, Quantization02500 5000 7500 10000 12500 15000 17500
Data size0.10.00.10.20.30.40.50.6Accuracy decreaseNC
Entropy
(d) TagMyNews-LSTM,
Quantization
Fig. 6. Impact of training data size (x-axis) on the performance (y -axis)
(success rate or accuracy decrease) of model. The horizontal dashed line shows
the attack success rate or change of test accuracy of a fully trained model.
For the image classiÔ¨Åcation task, the decay of accuracy keeps
stably lower than 10% until the data size reaches 37500, wherethe accuracy decreases signiÔ¨Åcantly by up to 60.73%. Theseresults reveal that the data size is not a key factor in the testaccuracy decay on model compression.
Answer to RQ4: Increasing the labeling budget of active
learning mitigates the loss in robustness compared to a fullytrained model. For example, using at least 60% training datafor the image classiÔ¨Åcation task (respectively, 35% for the textclassiÔ¨Åcation task) yields models with the same robustness asthe fully trained model. However, training with more data doesnot change our previous conclusions regarding the inconsistenteffect of model compression.
E. Discussion
We highlight our novel Ô¨Åndings Ô¨Årst, then discuss some
practical guidance and research directions accordingly.
Novel Ô¨Åndings and user guidance. 1) Finding: previous
studies were limited to test accuracy, the image classiÔ¨Åcation
task, and did not compare metrics from both the ML and SEcommunities. We reveal that the beneÔ¨Åts of active learning arehighly task-dependent. Some data selection metrics (LSA andDSA) are highly affected by the nature of classiÔ¨Åcation tasks,while some (Margin, Margin-dropout, and MCP) can achieveconsistently high performance. Guidance: engineers need tochoose data selection metrics according to speciÔ¨Åc tasks. 2)
Finding: the limitation of active learning also exists in theadversarial robustness and model compression, however, theevaluation was missing in the literature. We found that theactive learning process has some potential but notable impacton these indicators, e.g., for the image classiÔ¨Åcation task, thefully trained model is more robust than the model trainedby active learning. Guidance: engineers are recommended toconsider all these indicators when using active learning.
Research directions. 1) Since no existing data selection
metric can perform well on all the considered objectives,an interesting research direction is to design data selectionmetrics that can optimize multiple qualities such as accu-racy, robustness, and accuracy after compression. Regardingrobustness, an adequate metric should also effectively integratewith an adversarial training process, minimizing the amountof data to form which adversarial examples are generated toincrease the model robustness. 2)Our study focuses on two
types of classiÔ¨Åcation tasks (image and text). Recently, DLsystems have been applied in some SE-related tasks, such assource code function prediction and automatic program repair.Exploring how existing data selection metrics perform on thesetasks is a potential research direction. Besides, proposing asource code-oriented data selection metric for this kind ofsystem could be another contribution.
F . Threats to V alidity
First, threats to validity may lie in the selected datasets and
DNN models. Regarding the datasets, we employ Ô¨Åve popular
datasets across both image and text classiÔ¨Åcation tasks in ourstudy. As for the DNNs, we consider, in total, ten architectures(two for each dataset) to alleviate the model-dependent issue.Though the models we use perform well on the chosen tasks,an interesting direction of future work is to repeat the study onmore complex model architectures, such as transformer-basedmodels for text classiÔ¨Åcation [61], and observe whether thetrends remain.
Second, the parameter conÔ¨Åguration may induce a threat to
validity. Regarding the parameters in active learning, there isno universal rule for choosing the best settings. For instance,previous studies [8], [9], [20], [55] utilize different settings oflabeling budget and stop strategy. We mitigate this threat tovalidity in two ways: (1) we took the best and most commonpractices from the literature to design our active learningprocess and settings, and (2) our research questions concernthe speciÔ¨Åc impact of some parameters (e.g. RQ4 studies theimpact of the stop criteria). For robustness evaluation, wefollow a recent study [58] to set a range of perturbation sizesfor different adversarial attacks. As for model compression,our study involves multiple settings to reduce the potentialbias of the results.
Third, due to the space limitation, we only report the results
of two datasets covering the image and text classiÔ¨Åcationtasks. Nonetheless, we remark that the reported conclusionis generalized to all the datasets and DNNs. For example,for RQ2, in total, 31 out of 36 settings, the fully trainedimage-related models are more robust than the actively trainedmodels, which is consistent with our Ô¨Ånding. Besides, wereport all our complete results on our project site [48].
VI. RELATED WORK
We review related works in three aspects: data selection in
DL systems, empirical study on active learning, and empiricalstudy for DL systems.
Data selection in DL systems. We have already witnessed
the success of data selection in SE problems, such as system-atic literature review [62], defect prediction based on static
926code attributes [28], software cost modeling [63], and software
fault prediction [64]. Meanwhile, in the ML community, manydata selection metrics have been proposed mainly for activelearning. Recently, Ren et al. [65] surveyed the active learning
metrics and categorized them into uncertainty-based [8], [66],deep Bayesian active learning [9], density-based [10], [67],and other methods [11], [68], [69]. On the other hand, the SEcommunity has proposed some DL testing criteria as well asdata selection metrics. Inspired by the program coverage, Peiet al. [16] proposed the basic neuron coverage criterion, which
can be used as a data selection metric. Then, DeepGauge[15] introduced other DL test criteria, e.g., KMNC, NeuronBoundary Coverage. Kim et al. [17] proposed surprise guided
testing metrics based on the similarity between the trainingdata and test data. Moreover, some prediction probabilitybased data selection metrics [18], [19], [70] are also proposed.Most recently, Wang et al. [71] proposed a robustness-oriented
data selection metric, however, their metric can only selectdata that are generated by adversarial attacks, it is out ofour consideration. In our work, we comprehensively studiedalmost all these metrics not only on their effectiveness but alsothe potential limitations.
Empirical study on active learning. Active learning has
been widely studied over recent years, and some empiricalstudies of active learning have also been conducted fromdifferent domains. Ramirez-Loaiza et al. [72] utilized several
performance measures, e.g., accuracy, F1 score, and AUC, toevaluate active learning baselines. However, all these measuresare based on the correctness of classiÔ¨Åcation, while our studyincludes more evaluation methods, such as the adversarialrobustness and the performance change by model compression.Pereira-Santos et al. [73] conducted a large-scale empirical
study of active learning for three machine learning algorithms,C4.5, SVM, and 5NN. Similarly, the evaluation is limitedto the classiÔ¨Åcation correctness quantiÔ¨Åed by the Area underthe Learning Curve (ALC). In addition, some works targetspeciÔ¨Åc tasks by active learning. Settles et al. [49] analyzed
active learning for sequence labeling tasks such as informationextraction and document segmentation. For these speciÔ¨Åcproblems, the evaluation mainly lies in the learning curveand runtime. Yu et al. [62] empirically studied existing active
learning techniques for literature reviews, then proposed anovel one that outperforms the existing metrics concerning therecall vs. studies reviewed curve. Chen et al. [74] studied the
behavior of active learning for the word sense disambiguationtask. However, they only considered two basic data selectionmetrics, entropy and margin, and the evaluation only involvesaccuracy. Heilbron1 et al. [75] explored active learning for
the action localization task, which also only considered a verylimited number of (3) data selection metrics and comparedthe performance by ALC based on the correctness. Manabu[76] studied active learning with support vector machines(SVMs) for natural language processing. In their study, theyonly compared their proposed metrics with random selectionbased on accuracy. Besides, the Ô¨Ånding is mainly that SVMactive learning is suitable for Japanese word segmentation.To sum up, compared with previous empirical studies, our
study focuses on the application of active learning in deeplearning systems and is the Ô¨Årst one that studied both imageclassiÔ¨Åcation and text classiÔ¨Åcation tasks. More importantly,our work is the only one that analyzes the impacts of ac-tive learning on two important testing aspects during DNNdeployment, the adversarial robustness and the performanceof DNNs after model compression. Moreover, to the best ofour knowledge, our study is the Ô¨Årst that evaluates the dataselection metrics proposed by the SE community for activelearning.
Empirical study for DL systems. Recently, multiple em-
pirical research studies focus on exploring the DL issues thatare hard to be solved in theory. Guo et al. [77] studied the
performance difference between different DL frameworks aswell as the model changes after model migration. Zhang et
al.[78] and Chen et al. [79] studied the challenges in the
deployment phase of DL systems. Both of them revealed thatdeveloping a DL system is harder than developing softwaresystems. Ma et al. [20] performed a comparison study on
different data selection metrics. They investigated the abilityof each metric to identify misclassiÔ¨Åed input and improve thetest accuracy by retraining. What‚Äôs more, Zhang et al. [80]
conducted a comparative study about the capability of differentuncertainty metrics in distinguishing adversarial examples andbenign examples. Different from these works, our empiricalstudy focuses on exploring the limitations of active learning,especially the potential limitations of the model trained byactive learning, compared to the fully trained model.
VII. CONCLUSION
In this paper, we conducted a comprehensive empirical
study to explore the limitations of active learning. In total,more than 2000 models for image classiÔ¨Åcation and textclassiÔ¨Åcation tasks have been trained and systematically eval-uated. The results reveal that, when using active learning totrain a model, different data selection metrics yield modelsof signiÔ¨Åcantly different quality (in accuracy and robustness).For the image classiÔ¨Åcation task, a model trained with activelearning can achieve competitive test accuracy but suffers fromrobustness loss and are less to compression. However, thesedownsides rarely occur in text classiÔ¨Åcation models. Also, wefurther studied the relationship between the data budget andthe quality of a trained model. We found that the robustnessof the model increases with the amount of training data, andultimately reaches the robustness of the fully trained model.Based on these Ô¨Åndings, we provided some practical guidanceas well as research directions. We believe that our work couldgive engineers and researchers some valuable insights into thewhole secure life cycle of deep learning, especially in the datacollection and model evolution steps.
A
CKNOWLEDGMENTS
This work is supported by the Luxembourg National Re-
search Funds (FNR) through CORE project C18/IS/12669767/STELLAR/LeTraon.
927REFERENCES
[1] K. He, X. Zhang, S. Ren, and J. Sun, ‚ÄúDeep residual learning for image
recognition,‚Äù in Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition, 2016, pp. 770‚Äì778.
[2] M. Johnson, M. Schuster, Q. V . Le, M. Krikun, Y . Wu, Z. Chen,
N. Thorat, F. Vi ¬¥egas, M. Wattenberg, G. Corrado et al., ‚ÄúGoogle‚Äôs
multilingual neural machine translation system: enabling zero-shot trans-
lation,‚Äù Transactions of the Association for Computational Linguistics,
vol. 5, pp. 339‚Äì351, 2017.
[3] Y . Tian, K. Pei, S. Jana, and B. Ray, ‚ÄúDeeptest: Automated testing
of deep-neural-network-driven autonomous cars,‚Äù in Proceedings of the
40th International Conference on Software Engineering. New York,NY , USA: Association for Computing Machinery, 2018, pp. 303‚Äì314.
[4] J. Kober, J. A. Bagnell, and J. Peters, ‚ÄúReinforcement learning in
robotics: a survey,‚Äù The International Journal of Robotics Research,
vol. 32, no. 11, pp. 1238‚Äì1274, 2013.
[5] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, ‚ÄúImagenet:
a large-scale hierarchical image database,‚Äù in 2009 IEEE Conference on
Computer Vision and Pattern Recognition. Ieee, 2009, pp. 248‚Äì255.
[6] Y . Roh, G. Heo, and S. E. Whang, ‚ÄúA survey on data collection for ma-
chine learning: a big data-ai integration perspective,‚Äù IEEE Transactions
on Knowledge and Data Engineering, pp. 1‚Äì1, 2019.
[7] B. Settles, ‚ÄúActive learning,‚Äù Synthesis Lectures on ArtiÔ¨Åcial Intelligence
and Machine Learning, vol. 6, no. 1, pp. 1‚Äì114, 2012.
[8] D. Wang and Y . Shang, ‚ÄúA new active labeling method for deep learn-
ing,‚Äù in International Joint Conference on Neural Networks (IJCNN).
Beijing, China: IEEE, 2014, pp. 112‚Äì119.
[9] Y . Gal, R. Islam, and Z. Ghahramani, ‚ÄúDeep bayesian active learning
with image data,‚Äù in International Conference on Machine Learning.
PMLR, 2017, pp. 1183‚Äì1192.
[10] O. Sener and S. Savarese, ‚ÄúActive learning for convolutional neural net-
works: A core-set approach,‚Äù in International Conference on Learning
Representations. Vancouver, BC, Canada: OpenReview.net, 2018.
[11] M. Ducoffe and F. Precioso, ‚ÄúAdversarial active learning for deep
networks: a margin based approach,‚Äù arXiv preprint arXiv:1802.09841,
2018.
[12] X. Xie, L. Ma, F. Juefei-Xu, M. Xue, H. Chen, Y . Liu, J. Zhao,
B. Li, J. Yin, and S. See, ‚ÄúDeephunter: a coverage-guided fuzz testingframework for deep neural networks,‚Äù in Proceedings of the 28th ACM
SIGSOFT International Symposium on Software Testing and Analysis,2019, pp. 146‚Äì157.
[13] X. Xie, L. Ma, H. Wang, Y . Li, Y . Liu, and X. Li, ‚ÄúDiffchaser: Detecting
disagreements for deep neural networks.‚Äù in IJCAI, 2019, pp. 5772‚Äì
5778.
[14] X. Du, X. Xie, Y . Li, L. Ma, Y . Liu, and J. Zhao, ‚ÄúDeepstellar:
Model-based quantitative analysis of stateful deep learning systems,‚Äù inProceedings of the 2019 27th ACM Joint Meeting on European SoftwareEngineering Conference and Symposium on the F oundations of SoftwareEngineering, 2019, pp. 477‚Äì487.
[15] L. Ma, F. Juefei-Xu, F. Zhang, J. Sun, M. Xue, B. Li, C. Chen,
T. Su, L. Li, Y . Liu, J. Zhao, and Y . Wang, ‚ÄúDeepgauge: multi-granularity testing criteria for deep learning systems,‚Äù in Proceedings of
the 33rd ACM/IEEE International Conference on Automated SoftwareEngineering, 2018, pp. 120‚Äì131.
[16] K. Pei, Y . Cao, J. Yang, and S. Jana, ‚ÄúDeepxplore: automated whitebox
testing of deep learning systems,‚Äù Commun. ACM, vol. 62, no. 11, pp.
137‚Äî-145, 2019. [Online]. Available: https://doi.org/10.1145/3361566
[17] J. Kim, R. Feldt, and S. Yoo, ‚ÄúGuiding deep learning system testing
using surprise adequacy,‚Äù in 2019 IEEE/ACM 41st International Con-
ference on Software Engineering (ICSE). IEEE, 2019, pp. 1039‚Äì1049.
[18] W. Shen, Y . Li, L. Chen, Y . Han, Y . Zhou, and B. Xu, ‚ÄúMultiple-
boundary clustering and prioritization to promote neural network retrain-ing,‚Äù in IEEE/ACM International Conference on Automated Software
Engineering (ASE) . Melbourne, Australia: IEEE, 2020, pp. 410‚Äì422.
[19] Y . Feng, Q. Shi, X. Gao, J. Wan, C. Fang, and Z. Chen, ‚ÄúDeepgini:prioritizing massive tests to enhance the robustness of deep neuralbetworks,‚Äù in Proceedings of the 29th ACM SIGSOFT International
Symposium on Software Testing and Analysis , ser. ISSTA 2020. New
York, NY , USA: Association for Computing Machinery, 2020, p.177‚Äì188. [Online]. Available: https://doi.org/10.1145/3395363.3397357
[20] W. Ma, M. Papadakis, A. Tsakmalis, M. Cordy, and Y . L. Traon,
‚ÄúTest selection for deep learning systems,‚Äù ACM Trans. Softw.Eng. Methodol., vol. 30, no. 2, Jan. 2021. [Online]. Available:https://doi.org/10.1145/3417330
[21] X. Gao, R. K. Saha, M. R. Prasad, and A. Roychoudhury, ‚ÄúFuzz testing
based data augmentation to improve robustness of deep neural net-works,‚Äù in 2020 IEEE/ACM 42nd International Conference on Software
Engineering (ICSE). IEEE, 2020, pp. 1147‚Äì1158.
[22] L. Yang, Y . Zhang, J. Chen, S. Zhang, and D. Z. Chen, ‚ÄúSuggestive
annotation: a deep active learning framework for biomedical imagesegmentation,‚Äù in International Conference on Medical Image Comput-
ing and Computer-Assisted Intervention , Springer. Cham: Springer
International Publishing, 2017, pp. 399‚Äì407.
[23] M. Xu, J. Liu, Y . Liu, F. X. Lin, Y . Liu, and X. Liu, ‚ÄúA Ô¨Årst look at
deep learning apps on smartphones,‚Äù in The World Wide Web Conference.
New York, NY , USA: Association for Computing Machinery, 2019, pp.2125‚Äì2136.
[24] T.-W. Weng, H. Zhang, P.-Y . Chen, J. Yi, D. Su, Y . Gao, C.-J. Hsieh,
and L. Daniel, ‚ÄúEvaluating the robustness of neural networks: an ex-treme value theory approach,‚Äù in International Conference on Learning
Representations. Vancouver, BC, Canada: OpenReview.net, 2018.
[25] S. Yoo and M. Harman, ‚ÄúRegression testing minimization, selection and
prioritization: a survey,‚Äù Software Testing, V eriÔ¨Åcation and Reliability,
vol. 22, no. 2, pp. 67‚Äì120, 2012.
[26] W. Afzal and R. Torkar, ‚ÄúTowards benchmarking feature subset selection
methods for software fault prediction,‚Äù in Computational Intelligence
and Quantitative Software Engineering. Springer, 2016, pp. 33‚Äì58.
[27] T. Menzies, Z. Milton, B. Turhan, B. Cukic, Y . Jiang, and A. Bener,
‚ÄúDefect prediction from static code features: current results, limitations,new approaches,‚Äù Automated Software Engineering, vol. 17, no. 4, pp.
375‚Äì407, 2010.
[28] T. Menzies, J. Greenwald, and A. Frank, ‚ÄúData mining static code
attributes to learn defect predictors,‚Äù IEEE Transactions on Software
Engineering, vol. 33, no. 1, pp. 2‚Äì13, 2006.
[29] K. Ren, T. Zheng, Z. Qin, and X. Liu, ‚ÄúAdversarial attacks and defenses
in deep learning,‚Äù Engineering, vol. 6, no. 3, pp. 346‚Äì360, 2020.
[30] I. J. Goodfellow, J. Shlens, and C. Szegedy, ‚ÄúExplaining and harnessing
adversarial examples,‚Äù 2015.
[31] N. Papernot, P. McDaniel, S. Jha, M. Fredrikson, Z. B. Celik, and
A. Swami, ‚ÄúThe limitations of deep learning in adversarial settings,‚Äù inIEEE European Symposium on Security and Privacy (EuroS&P). IEEE,2016, pp. 372‚Äì387.
[32] N. Carlini and D. Wagner, ‚ÄúTowards evaluating the robustness of neural
networks,‚Äù in IEEE Symposium on Security and Privacy. IEEE, 2017,
pp. 39‚Äì57.
[33] S. Ren, Y . Deng, K. He, and W. Che, ‚ÄúGenerating natural language
adversarial examples through probability weighted word saliency,‚Äù inProceedings of the 57th Annual Meeting of the Association for Com-putational Linguistics. Florence, Italy: Association for ComputationalLinguistics, july 2019, pp. 1085‚Äì1097.
[34] C. Fellbaum, ‚ÄúWordnet,‚Äù in Theory and Applications of Ontology:
Computer Applications. Springer, 2010, pp. 231‚Äì243.
[35] J. Gao, J. Lanchantin, M. L. Soffa, and Y . Qi, ‚ÄúBlack-box generation of
adversarial text sequences to evade deep learning classiÔ¨Åers,‚Äù in 2018
IEEE Security and Privacy Workshops (SPW), May 2018, pp. 50‚Äì56.
[36] S. Han, J. Pool, J. Tran, and W. Dally, ‚ÄúLearning both weights and con-
nections for efÔ¨Åcient neural network,‚Äù Advances in Neural Information
Processing Systems, vol. 28, pp. 1135‚Äì1143, 2015.
[37] H. Hu, R. Peng, Y .-W. Tai, and C.-K. Tang, ‚ÄúNetwork trimming: a data-
driven neuron pruning approach towards efÔ¨Åcient deep architectures,‚ÄùarXiv preprint arXiv:1607.03250, 2016.
[38] L. Ma, F. Juefei-Xu, M. Xue, Q. Hu, S. Chen, B. Li, Y . Liu, J. Zhao,
J. Yin, and S. See, ‚ÄúSecure deep learning engineering: A software qualityassurance perspective,‚Äù arXiv preprint arXiv:1810.04538 , 2018.
[39] Y . Lecun, L. Bottou, Y . Bengio, and P. Haffner, ‚ÄúGradient-based learningapplied to document recognition,‚Äù Proceedings of the IEEE, vol. 86,
no. 11, pp. 2278 ‚Äì 2324, November 1998.
[40] A. Krizhevsky, ‚ÄúLearning multiple layers of features from tiny images,‚Äù
University of Toronto, Toronto, Tech. Rep., 2009.
[41] A. Maas, R. E. Daly, P. T. Pham, D. Huang, A. Y . Ng, and C. Potts,
‚ÄúLearning word vectors for sentiment analysis,‚Äù in Proceedings of the
49th Annual Meeting of the Association for Computational Linguistics:Human Language Technologies, 2011, pp. 142‚Äì150.
[42] D. Vitale, P. Ferragina, and U. Scaiella, ‚ÄúClassiÔ¨Åcation of short texts by
deploying topical annotations,‚Äù in European Conference on Information
Retrieval. Berlin, Heidelberg: Springer, 2012, pp. 376‚Äì387.
928[43] L. A. Adamic, J. Zhang, E. Bakshy, and M. S. Ackerman, ‚ÄúKnowledge
sharing and yahoo answers: everyone knows something,‚Äù in Proceedings
of the 17th international conference on World Wide Web, 2008, pp. 665‚Äì
674.
[44] Y . LeCun et al., ‚ÄúLenet-5, convolutional neural networks,‚Äù URL:
http://yann. lecun. com/exdb/lenet, vol. 20, no. 5, p. 14, 2015.
[45] M. Lin, Q. Chen, and S. Yan, ‚ÄúNetwork in network,‚Äù arXiv preprint
arXiv:1312.4400, 2013.
[46] K. Simonyan and A. Zisserman, ‚ÄúVery deep convolutional networks for
large-scale image recognition,‚Äù in International Conference on Learning
Representations, San Diego, CA, USA, 2015.
[47] F. Chollet, ‚ÄúKeras code examples,‚Äù 2020. [Online]. Available:
https://keras.io/examples/nlp/bidirectional\ lstm\ imdb/
[48] ‚ÄúActive learning empirical study homepage,‚Äù 2021. [Online]. Available:
https://sites.google.com/view/alempirical
[49] B. Settles and M. Craven, ‚ÄúAn analysis of active learning strategies for
sequence labeling tasks,‚Äù in Proceedings of the Conference on Empirical
Methods in Natural Language Processing. USA: Association forComputational Linguistics, 2008, pp. 1070‚Äì1079.
[50] A. Siddhant and Z. C. Lipton, ‚ÄúDeep bayesian active learning for
natural language processing: results of a large-scale empirical study,‚ÄùinProceedings of the 2018 Conference on Empirical Methods in
Natural Language Processing. Brussels, Belgium: Association forComputational Linguistics, October 2018, pp. 2904‚Äì2909.
[51] A. Odena, C. Olsson, D. Andersen, and I. Goodfellow, ‚ÄúTensorfuzz:
debugging neural networks with coverage-guided fuzzing,‚Äù in Proceed-
ings of the 36th International Conference on Machine Learning , ser.
Proceedings of Machine Learning Research, vol. 97. PMLR, June2019, pp. 4901‚Äì4911.
[52] S.-M. Moosavi-Dezfooli, A. Fawzi, and P. Frossard, ‚ÄúDeepfool: a simple
and accurate method to fool deep neural networks,‚Äù in Proceedings of the
IEEE Conference on Computer Vision and Pattern Recognition, 2016,pp. 2574‚Äì2582.
[53] F. Chollet et al., ‚ÄúKeras,‚Äù https://keras.io, 2015.
[54] M. Abadi, P. Barham, J. Chen, Z. Chen, A. Davis, J. Dean, M. Devin,
S. Ghemawat, G. Irving, M. Isard et al., ‚ÄúTensorÔ¨Çow: a system for large-
scale machine learning,‚Äù in 12th{USENIX} Symposium on Operating
Systems Design and Implementation ({OSDI} 16), ser. OSDI‚Äô16. USA:
USENIX Association, 2016, pp. 265‚Äì283.
[55] W. H. Beluch, T. Genewein, A. N ¬®urnberger, and J. M. K ¬®ohler, ‚ÄúThe
power of ensembles for active learning in image classiÔ¨Åcation,‚Äù inProceedings of the IEEE Conference on Computer Vision and PatternRecognition, 2018, pp. 9368‚Äì9377.
[56] J. Rauber, W. Brendel, and M. Bethge, ‚ÄúFoolbox: a python toolbox to
benchmark the robustness of machine learning models,‚Äù arXiv preprint
arXiv:1707.04131, 2017.
[57] M.-I. Nicolae, M. Sinn, M. N. Tran, B. Buesser, A. Rawat, M. Wistuba,
V . Zantedeschi, N. Baracaldo, B. Chen, H. Ludwig, I. Molloy, andB. Edwards, ‚ÄúArt: adversarial robustness toolbox v1.2.0,‚Äù CoRR, vol.
1807.01069, 2018. [Online]. Available: https://arxiv.org/pdf/1807.01069
[58] Y . Dong, P. Zhang, J. Wang, S. Liu, J. Sun, J. Hao, X. Wang,
L. Wang, J. S. Dong, and D. Ting, ‚ÄúThere is limited correlation betweencoverage and robustness for deep neural networks,‚Äù arXiv preprint
arXiv:1911.05904, 2019.
[59] M. Thakkar, Beginning machine learning in ios: CoreML framework,
1st ed. APress, 2019.
[60] M. Abadi, A. Agarwal, P. Barham, E. Brevdo, Z. Chen, C. Citro, G. S.
Corrado, A. Davis, J. Dean, M. Devin, S. Ghemawat, I. Goodfellow,A. Harp, G. Irving, M. Isard, Y . Jia, R. Jozefowicz, L. Kaiser,M. Kudlur, J. Levenberg, D. Man ¬¥e, R. Monga, S. Moore, D. Murray,
C. Olah, M. Schuster, J. Shlens, B. Steiner, I. Sutskever, K. Talwar,P. Tucker, V . Vanhoucke, V . Vasudevan, F. Vi ¬¥egas, O. Vinyals,
P. Warden, M. Wattenberg, M. Wicke, Y . Yu, and X. Zheng,‚ÄúTensorÔ¨Çow: Large-scale machine learning on heterogeneous systems,‚Äù2015, software available from tensorÔ¨Çow.org. [Online]. Available:https://www.tensorÔ¨Çow.org/
[61] P. Li, P. Zhong, K. Mao, D. Wang, X. Yang, Y . Liu, J.-x. Yin, and
S. See, ‚ÄúAct: an attentive convolutional transformer for efÔ¨Åcient textclassiÔ¨Åcation,‚Äù in Proceedings of the AAAI Conference on ArtiÔ¨Åcial
Intelligence, vol. 35, no. 15, 2021, pp. 13 261‚Äì13 269.
[62] Z. Yu, N. A. Kraft, and T. Menzies, ‚ÄúFinding better active learners for
faster literature reviews,‚Äù Empirical Software Engineering, vol. 23, no. 6,
pp. 3161‚Äì3186, 2018.[63] Z. Chen, T. Menzies, D. Port, and D. Boehm, ‚ÄúFinding the right data
forsoftware cost modeling,‚Äù IEEE Software, vol. 22, no. 6, pp. 38‚Äì46,
2005.
[64] H. Lu and B. Cukic, ‚ÄúAn adaptive approach with active learning in
software fault prediction,‚Äù in Proceedings of the 8th International Con-
ference on Predictive Models in Software Engineering , ser. PROMISE
‚Äô12. New York, NY , USA: Association for Computing Machinery,2012, p. 79‚Äì88.
[65] P. Ren, Y . Xiao, X. Chang, P.-Y . Huang, Z. Li, X. Chen, and X. Wang,
‚ÄúA survey of deep active learning,‚Äù arXiv preprint arXiv:2009.00236,
2020.
[66] N. Asghar, P. Poupart, X. Jiang, and H. Li, ‚ÄúDeep active learning for
dialogue generation,‚Äù arXiv preprint arXiv:1612.03929, 2016.
[67] Y . Geifman and R. El-Yaniv, ‚ÄúDeep active learning over the long tail,‚Äù
arXiv preprint arXiv:1711.00941, 2017.
[68] M. Fang, Y . Li, and T. Cohn, ‚ÄúLearning how to active learn: A deep
reinforcement learning approach,‚Äù arXiv preprint arXiv:1708.02383 ,
2017.
[69] B. Yang, J.-T. Sun, T. Wang, and Z. Chen, ‚ÄúEffective multi-label
active learning for text classiÔ¨Åcation,‚Äù in Proceedings of the 15th ACM
SIGKDD International Conference on Knowledge Discovery and DataMining, 2009, pp. 917‚Äì926.
[70] Z. Wang, H. You, J. Chen, Y . Zhang, X. Dong, and W. Zhang,
‚ÄúPrioritizing test inputs for deep neural networks via mutation analysis,‚ÄùinIEEE/ACM 43nd International Conference on Software Engineering
(ICSE), 2021.
[71] J. Wang, J. Chen, Y . Sun, X. Ma, D. Wang, J. Sun, and P. Cheng,
‚ÄúRobot: robustness-oriented testing for deep learning systems,‚Äù arXiv
preprint arXiv:2102.05913, 2021.
[72] M. E. Ramirez-Loaiza, M. Sharma, G. Kumar, and M. Bilgic, ‚ÄúActive
learning: an empirical study of common baselines,‚Äù Data Mining and
knowledge Discovery, vol. 31, no. 2, pp. 287‚Äì313, 2017.
[73] D. Pereira-Santos, R. B. C. Prud ÀÜencio, and A. C. de Carvalho, ‚ÄúEmpirical
investigation of active learning strategies,‚Äù Neurocomputing, vol. 326,
pp. 15‚Äì27, 2019.
[74] J. Chen, A. Schein, L. Ungar, and M. Palmer, ‚ÄúAn empirical study
of the behavior of active learning for word sense disambiguation,‚Äù inProceedings of the Human Language Technology Conference of theNAACL, Main Conference, 2006, pp. 120‚Äì127.
[75] F. C. Heilbron, J.-Y . Lee, H. Jin, and B. Ghanem, ‚ÄúWhat do i annotate
next? an empirical study of active learning for action localization,‚Äù inProceedings of the European Conference on Computer Vision (ECCV),2018, pp. 199‚Äì216.
[76] M. Sassano, ‚ÄúAn empirical study of active learning with support vector
machines forjapanese word segmentation,‚Äù in Proceedings of the 40th
Annual Meeting of the Association for Computational Linguistics, 2002,pp. 505‚Äì512.
[77] Q. Guo, S. Chen, X. Xie, L. Ma, Q. Hu, H. Liu, Y . Liu, J. Zhao, and
X. Li, ‚ÄúAn empirical study towards characterizing deep learning de-velopment and deployment across different frameworks and platforms,‚Äùin34th IEEE/ACM International Conference on Automated Software
Engineering (ASE). IEEE, 2019, pp. 810‚Äì822.
[78] T. Zhang, C. Gao, L. Ma, M. Lyu, and M. Kim, ‚ÄúAn empirical study
of common challenges in developing deep learning applications,‚Äù inInternational Symposium on Software Reliability Engineering (ISSRE).Berlin, Germany: IEEE, 2019, pp. 104‚Äì115.
[79] Z. Chen, Y . Cao, Y . Liu, H. Wang, T. Xie, and X. Liu, ‚ÄúA comprehensive
study on challenges in deploying deep learning based software,‚Äù inProceedings of the 28th ACM Joint Meeting on European SoftwareEngineering Conference and Symposium on the F oundations of SoftwareEngineering, 2020, pp. 750‚Äì762.
[80] X. Zhang, X. Xie, L. Ma, X. Du, Q. Hu, Y . Liu, J. Zhao, and
M. Sun, ‚ÄúTowards characterizing adversarial defects of deep learningsoftware from the lens of uncertainty,‚Äù in IEEE/ACM 42nd International
Conference on Software Engineering (ICSE), IEEE. New York, NY ,USA: Association for Computing Machinery, 2020, pp. 739‚Äì751.
929