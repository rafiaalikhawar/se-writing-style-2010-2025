Nezha:InterpretableFine-Grained Root CausesAnalysis for
MicroservicesonMulti-modal Observability Data
Guangba Yu
PengfeiChen∗
Sun Yat-sen University
ChinaYufeng Li
HongyangChen
Sun Yat-sen University
ChinaXiaoyun Li
Zibin Zheng
Sun Yat-sen University
China
ABSTRACT
Root cause analysis (RCA)inlarge-scale microservice systems is
a critical and challenging task. To understand and localize root
causes of unexpected faults, modern observability tools collect and
preserve multi-modalobservability data, including metrics, traces,
andlogs.Sincesystemfaultsmaymanifestasanomaliesindiﬀerent
datasources,existingRCAapproachesthatrelyonsingle-modal
data are constrained in the granularity and interpretability of root
causes.Inthisstudy,wepresent Nezha,aninterpretableand/f_ine-
grainedRCAapproachthatpinpointsrootcausesatthecoderegion
andresourcetypelevelbyincorporativeanalysisofmulti-modal
data.Nezhatransforms heterogeneous multi-modal data into a
homogeneousevent representationand extracts event patterns by
constructingandminingeventgraphs.Thecoreideaof Nezhais
to compare event patterns in the fault-free phase with those in
the fault-suﬀering phase to localize root causes in an interpretable
way. Practical implementation and experimental evaluations on
two microservice applications show that Nezhaachieves a high
top1 accuracy (89.77%) on average at the code region and resource
typelevelandoutperformsstate-of-the-artapproachesbyalarge
margin.Twoablationstudiesfurthercon/f_irmthecontributionsof
incorporatingmulti-modaldata.
CCS CONCEPTS
•Softwareanditsengineering →Softwarereliability ;Soft-
ware performance ;Cloudcomputing .
KEYWORDS
RootCauseAnalysis,Multi-modalObservabilityData,Microservice
ACMReference Format:
GuangbaYu,PengfeiChen,YufengLi,HongyangChen,XiaoyunLi,andZibin
Zheng.2023.Nezha:InterpretableFine-GrainedRootCausesAnalysisfor
Microservices on Multi-modal Observability Data. In Proceedings of the
31stACMJointEuropeanSoftwareEngineeringConferenceand Symposium
on the Foundations of Software Engineering (ESEC/FSE ’23), December 3–9,
2023, San Francisco, CA, USA. ACM, New York, NY, USA, 13pages.https:
//doi.org/10.1145/3611643.3616249
∗PengfeiChen is the corresponding author
Permissionto make digitalor hard copies of allorpart ofthis work for personalor
classroom use is granted without fee provided that copies are not made or distributed
forpro/f_itorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation
onthe/f_irstpage.Copyrights forcomponentsofthisworkownedbyothersthanthe
author(s)mustbehonored.Abstractingwithcreditispermitted.Tocopyotherwise,or
republish,topostonserversortoredistributetolists,requirespriorspeci/f_icpermission
and/or a fee. Request permissions from permissions@acm.org.
ESEC/FSE ’23, December 3–9, 2023, San Francisco, CA,USA
©2023 Copyright heldby the owner/author(s). Publicationrightslicensed to ACM.
ACM ISBN 979-8-4007-0327-0/23/12...$15.00
https://doi.org/10.1145/3611643.3616249"timestamp":"08:04:33",
"severity":"debug",
"message":"request	complete"0100
⚠ metric
trace
log
}
root
cause
Figure 1: Multi-modal observability data (i.e., metrics, traces,
and logs) are used to localize /f_ine-grained root causes (e.g.,
regions ofcodedefectsorresource types).
1 INTRODUCTION
Microservice architecture decomposes an application into many
smallpiecesofservices,allowingdeveloperstomodifyandredeploy
speci/f_icservicesratherthantheentireapplication[ 23,70].However,
interconnected services are deployed across multiple hosts and the
number of services in production increases rapidly as the business
develops.Manyfactors(e.g.,dynamiccontainerenvironmentand
complex service invocations) can aﬀect the performance and avail-
abilityofmicroservice applications,leadingtomoreperformance
andavailability issues[ 67,71].
TohelpSiteReliabilityEngineers(SREs)betterunderstandthe
performanceandavailabilityofapplications,observabilityispro-
posed with a capability of comprehensive visibility into distributed
applications [ 58]. Theprimary dataused inobservabilityare met-
rics, logs, and traces, which are referred to as “multi-modal observ-
ability data” [ 58]. As shown in Fig. 1, when a fault occurs, SREs
typicallyneedtocombinecluesextractedfrommulti-modalobserv-
abilitydatatolocalizetherootcause,whichisthemostfundamental
reason for the fault[ 62].
Manuallytroubleshootingrootcausesbasedonmulti-modaldata
is time-consuming and error-prone in complex microservice appli-
cations due to the explosion and heterogeneity of the observability
data. Over the years, many metrics-based approaches [ 6,26,35,
61,63,64],trace-based approaches [ 8,9,13,22,32,66,67,71], and
log-basedapproaches[ 3,20,36,45,51,55]havebeendevotedtoau-
tomaticallylocalizerootcauses.However,weidentifythreeprimary
limitationsofexisting root cause analysis(RCA) approaches.
(1)Insuﬃcient exploitation on multi-modal data. Most of the
state-of-the-art RCA approaches [ 3,32,35,55,64,67] only use
single-modaldatatopinpointrootcauses.Sincesystemfailures
may manifest as anomalies in diﬀerent modal data, adopting
single modal data could lose key RCA clues and thus impact
the accuracyofRCA approaches(details in§ 2.2).
(2)Coarse-grained root causes. Existing single-modal studies
mainly localize the root causes of microservice applications
553
ESEC/FSE ’23, December3–9, 2023,San Francisco, CA, USA Guangba Yu, Pengfei Chen, Yufeng Li, HongyangChen, Xiaoyun Li, andZibin Zheng
at the service level [ 3,6,35,51,64,66]. Such coarse-grained
rootcausesrequireSREstomanuallytroubleshootthetrueroot
causes (e.g., speci/f_ic code region or resource type) of the faulty
services, whichdelays the failure recovery process.
(3)Weak interpretability of root causes. Existing RCA tech-
niquesplaceinsuﬃcientemphasisontheinterpretabilityofroot
causes, which is crucial for SREs to fully understand the under-
lying issues. For example, PDiagnose [ 21] takes multi-modal
data as input by transforming heterogeneous logs and traces
into time series. However, this approach sacri/f_icesthe detailed
executioncontextofrequests(e.g.,logcontentsandtracepaths).
Asaresult,itfailstoprovidedetailedexplanationsregarding
where andwhy the faultyrequests occurred.
NezhaApproach. To overcomethe abovelimitations,we pro-
poseNezha, which is a protection deity with multiple arms in Chi-
nese mythology, an interpretable and /f_ine-grained RCA framework
by incorporative analysis of multi-modal data. When Anomaly De-
tector (§ 4.1) detects an anomaly on front-end service requests,
Data Integrator (§ 4.2) takes heterogeneous metrics, traces, and
logs from the fault-free construction and fault-suﬀering produc-
tionphaseasinputsandtransformsthemintothehomogeneous
event representation. Events in the same service instance are then
ordered according to their timestamps and event groups across
service instances are connected according to span IDs to construct
event graphs while overcoming clock synchronization problems.
Pattern Miner (§ 4.3) then extracts the common execution patterns
ofthe applicationfrom the eventgraphs.
After transforming multi-modal data into events (i.e., excluding
speci/f_ic request parameters), the event patterns in the construction
phasearesimilartothoseintheproductionphase.Duringthefault-
suﬀering phase, faults manifest themselves in one or more data
sources, which causes some event patterns to deviate from their
expected execution paths. Therefore, the critical problem of Nezha
istoidentify(1) whicheventpatternsdeviatefromexpectedexecution
paths,and(2)howthesepatternschange intheactualfault-suﬀering
phase. To localize such faulty event patterns, Expected Pattern
Ranker(§ 4.4.1)isproposedtosolvetheproblem(1)bypinpointing
excepted patterns that frequentlyoccur in the fault-free phase but
rarelyhappeninthefault-suﬀeringphase.Thefaultyeventpatterns
re/f_lect whichcode regionsor resources type are culprits, whichis
more/f_ine-grainedthantheexistingRCAapproachesofpinpointing
faultsat the service level (solution oflimitation2).
ActualPatternRanker(§ 4.4.2)isusedtosolveproblem(2)bylo-
catingtheactualpatternsthatfrequentlyoccurinthefault-suﬀering
phase but rarely happen in the fault-free phase. Such newly emerg-
ing patterns in the fault-suﬀering phase facilitate SREs to under-
stand faults. Eventually, Pattern Aggregator (§ 4.5) correlates ex-
pected patterns with actual patterns and takes pattern pairs as
rootcausecandidates. Nezhaoutputsactionablerootcauseswith
highinterpretabilitybycomparingexpectedpatternswithactual
patterns (solution oflimitation3).
We conductedextensivestudiestoevaluate Nezhaon twopop-
ular microservice applications, namely TrainTicket [ 12] and On-
lineBoutique [ 14]. Bene/f_iting fromtheuseofmulti-modal observ-
abilitydata, Nezhaachievesahightop-1accuracy(89.77%)andsur-
passesallcomparedapproachesbyalargemargin( 61.45%∼74.63%)whenidentifyingrootcausesattheservicelevel.Whenidentifying
rootcausesattheinner-servicelevel(i.e.,coderegionorresource
type),Nezhaoutperformsadvancedbaselinesby 67.47%∼74.85%
in a high top-1 accuracy. Moreover, two ablation studies further
con/f_irmthe contributionof incorporatingmulti-modal data.
Contributions. This study makes the following contributions,
•Weintroduceanovelapproachtorepresentheterogeneousmulti-
modal observability data (i.e., metrics, traces, and logs) in a uni-
/f_ied homogeneous event format. This representation enables the
construction of eventgraphs and facilitatesthe future integrated
analysisacrossmulti-modal observability data.
•Wepresent Nezha,aninterpretableand/f_ine-grainedmicroservice
RCA approach. Nezhais a statistical method to localize more
granular and actionable root causes (i.e., code region or resource
type) with high interpretability, which facilitates SREs to take
mitigationactions incon/f_idence.
•Weimplementtheprototypeof Nezha[69]andconductextensive
experimentsontwowidely-usedmicroserviceapplicationstoval-
idate the eﬀectiveness and eﬃciency of Nezha. The results show
thatNezhaoutperformsthestate-of-the-artRCA approaches at
both service andinner-service level.
•Weenhancetheobservabilityoftwowidely-usedmicroservice
applications OnlineBoutique and Trainticket, and open source
them at [ 46] and [47], which will facilitate the future anomaly
detection andRCA research onmulti-modal data.
2 BACKGROUNDAND MOTIVATION
2.1 Background
Observability. Observabilityisameasureofhowwelltheinternal
statesofasystemcanbeinferredfromknowledgeofitsexternal
outputs[58].It hasbeenincreasinglyapplied tomicroservicesbe-
cause cloud-native environments become more and more complex,
and the potential root causes of faults become more challenging
topinpoint.Observabilitytypicallyusesthreetypesoftelemetry
data — metrics, traces, and logs — to provide deep visibility into
distributed systems. Observability allows SREs to monitor systems
moreeﬀectively,helpingthemidentifyandconnecttheeﬀectsin
complex chains andtrace themback to their causes.
Metric.Metrics are numerical values that describe the status of
the microservices and infrastructure over a period of time. Metrics
can be further classi/f_ied as system-level metrics and application-
level metrics[ 27]. System-levelmetrics suchas memory andCPU
usage are immediate concerns whenever SREs identify a perfor-
mance degradation. SREs can identify issues caused by insuﬃcient
resourcesbyanalyzingsystem-levelmetrics.However,system-level
metrics cannothelpsolve problems causedbycode defects.
Application-level metrics (e.g.,request latency orsuccessratio)
arederivedfrommonitoringrequeststodescribetheapplication
status. Such metrics typically re/f_lect the faults’ surface rather than
rootcausesoffaults.Forexample,adecreaseinsuccessratere/f_lects
a decrease in availability, and SREs need to further analyze system-
level metrics, traces, and logs to identify the root cause. Therefore,
we analyze application-level metrics when performing anomaly
detection andfocusonsystem-level metrics inRCA.
Log.Logs are textual records of what operations are performed
duringprogram runtime.Alog consistsofatimestamp thattells
554Nezha: InterpretableFine-GrainedRoot Causes Analysis forMicroservicesonMulti-modalObservabilityData ESEC/FSE ’23, December3–9, 2023,San Francisco, CA, USA
Front
08:09:23	Serve	Pd.
08:09:24	Serve	Pd.
08:09:24	Serve	Pd.
08:09:25	Req.	FailProduct
08:09:23	Query	Pd.	ab5
08:09:24	Query	Pd.	ef2
08:09:24	Query	Pd.	Err
08:09:24	Query	Pd.	hg8Product
TraID:	12df	08:09:23	INFO	Query	Pd.	ab5
TraID:	14bj	08:09:24	INFO	Query	Pd.	ef2
TraID:	12df	08:09:24	ERR	Query	Pd.	Err
TraID:	249e	08:09:24	INFO	Query	Pd.	hg8Front
TraID:	12df	08:09:23	INFO	Serve	Pd.
TraID:	14bj	08:09:24	INFO	Serve	Pd.
TraID:	249e	08:09:24	INFO	Serve	Pd.
TraID:	12df	08:09:25	ERR	Req.	FailFront/Recv.
Recommend/ListRecommend
Product/ListProduct
log6log5 log4 log3log2log1
TraceID		Time		LogLevel	LogContext	
(a)	Before	integrating	logs	and	traces (b)	Integrating	logs	and	traces (c)	After 	integrating	logs	and	traces
Figure2:Anexampleofintegrationwithtracesandlogs.(a)SREscannotdistinguishwhichrequestthefailedlogsbelongto.(b)
The contentsofthelogs after integration.(c) SREs can track logs belongingto eachrequest after integration.
when it occurred, with a static structure and free-form text. The
logtemplateistheconstantpartofalogstatementinthecode[ 68].
Althoughlogsprovidevaluableinformationabouttheindividual
serviceinstance,thislocalizedknowledgelacksassociationsoflogs
for the same request across diﬀerent services. Analysis logs on
diﬀerentservicesindependentlycannotcharacterizethebehavior
ofthe overallsystem.
Trace.Tracesrepresenttheend-to-endpathsofrequeststhrough
adistributed system.Asa requestmoves throughasystem,every
operation performed is called a “span”, which records the caller
service, the callee service, and the operation time. Each trace corre-
sponds to a request and has a unique identi/f_ier (e.g., trace ID). A
span is a named and timed operation that shares the same trace
IDwithinthesametrace.EachspanalsohasauniquespanIDto
signify it. The root span (e.g., Front/Recv. in Fig.2(b)) is the /f_irst
spaninatrace,whiletheotherspansarecorrelatedbyparent-child
relationships based on the parent span ID properties (e.g., Fron-
t/Recv.istheparentof Recommend/ListRecommend inFig.2(b)).The
contributionoftracesislimitedinRCAbecausetheyonlyrecord
the coarse-grainedservice operation-level information.
2.2 Motivation
Thissectionpresentsfourmotivationswithexamplesfromawidely-
used microservice application, OnlineBoutique [ 14]. Details on the
applicationanddata collection willbe describedin§ 5.1.
Motivation1:EnhancingRCAthroughLogandTraceInte-
gration. Logs serve asavaluable resource tocapture the internal
states and behaviors of individual microservices. However, analyz-
inglogsforeachmicroserviceinisolationmayresultinalossof
globalcontext,leadingtoineﬀectiveRCA[ 72].ForaneﬃcientRCA
approach, it is essential to combine logs from diﬀerent microser-
vices. Nonetheless, as illustrated in Fig. 2(a), logs generated by a
singlemicroservicecaninterleave,asthemicroservicemayconcur-
rentlyservemultiplerequestsassociatedwithdistinctlogentries.
For a failed request, existent RCA approaches cannot determine
whichlogsspreadingacrossmultipleservicesareassociatedwith
the failedrequest to deduce the root cause.
Traces capture global service interactions across various servers
butoﬀerlimitedinsightintolocalbehaviorswithinindividualspans.
To capitalize on the bene/f_its of both logs and traces, we propose an
integrationoftraces,representingacoarse-grainedglobalview,and
logs,providinga/f_ine-grainedlocalview,tofurnishadetailedglobal
perspectiveforeachrequest.AsdepictedinFig. 2(b),weaccomplish
thisintegrationbyinsertingtraceIDsintologmessages,leveraging
distributed tracing frameworks such as Opentelemetry [ 48] and
SkyWalking [ 57]. Inserting trace IDs in log messages is a preva-
lent practice in numerous industrial systems (e.g., WeChat) andNormal	
PhaseCPU
ContentionError
ReturnNetwork
Jamtrace	anomaly log	anomaly system-level	metric	anomaly
time
Figure 3: The occurrences of related system-level metrics,
traces andlogs that can re/f_lect anomaly.
widelyadoptedframeworks(e.g.,SpringCloud).Byemployingthis
approach, when a request encounters a fault (e.g., the red line in
Fig.2(c)),RCAapproachescantracktherequest-leveldescriptive
information (e.g.,logs)to eﬀectivelyidentifythe root cause.
Motivation 2: Enhancing RCA through the Integration of
Metrics, Logs,andTraces. While logsand tracesoﬀerabundant
clues for RCA, certain anomalies may not be apparent in these
sources,leadingtoapotentiallossofcriticalinformationpertaining
torootcauses.Fig. 3showsthreeexamplesoftheoccurrencesof
system-level metrics, traces, and logs that can re/f_lect anomalies
whenCPUcontention,error return,andnetworkjamfaultswere
injectedintotheOnlineBoutiqueproductservice.Theanomaliesin
metrics were identi/f_ied using the /u1D458-/u1D70Erules, with further details to
bepresentedin§ 4.1.Logsandtracespansthatexclusivelyoccur
duringthefault-suﬀeringphase,butnotinthefault-freephase,are
consideredabnormal.
As demonstrated in Fig. 3, log and trace anomalies are absent
whenCPUcontentionandnetworkjamfaultsareinjected,because
these faults do not alter the execution paths of OnlineBoutique.
Inthesecases,thelog-basedortrace-basedRCAapproachesmay
miss the root causes. Fortunately, system-level metrics can provide
valuableinsightstolocatefaultsthatremainundetectedinlogsand
traces.Forinstance,anomaliesinCPUusageweredetectedwhen
CPUcontentionfaultswereintroduced.However,solelyrelyingon
metrics-based RCA approaches may not identify faults that are not
re/f_lected in metrics (e.g., error return fault in Fig. 3). The examples
inFig.3underscorethenecessityofintegratingmetricswithlogs
andtraces to eﬀectivelyidentifyroot causes.
Motivation 3: Enhancing RCA through the Integration of
Multi-modal Data in a Uni/f_ied Representation. As highlighted
in Table1, a signi/f_icant portion of existing studies primarily fo-
cusesonsingle-modaldataforrootcauseidenti/f_ication.SREsare
naturally inclined to merge the results of metrics-based, traces-
based, and logs-based methods to accurately pinpoint true root
causes. However, this integration process can be labor-intensive
andineﬀectiveduetoseveralreasons.(1)Whenindividuallyana-
lyzinglogs,correlationbetweenlogsondiﬀerentmachinesisnot
achievable,andasimilarissueariseswithmetrics.Consequently,
single-modal techniques are likely to produce less accurate results
compared to multi-modal approaches. (2) In situations where SREs
555ESEC/FSE ’23, December3–9, 2023,San Francisco, CA, USA Guangba Yu, Pengfei Chen, Yufeng Li, HongyangChen, Xiaoyun Li, andZibin Zheng
Table 1:Comparisonofstate-of-the-artRCA approaches.
Approach Metrics LogsTraces RCA Level
MicroScope [ 35]/enc-34/enc-37/enc-37 Service
MicroRCA[ 64]/enc-34/enc-37/enc-37 Service
SBLD[55]/enc-37/enc-34/enc-37 Error Log
LogFaultFlagger[ 1]/enc-37/enc-34/enc-37 Error Log
MicroRank[ 67]/enc-37/enc-37/enc-34 ServiceOperation
TraceAnomaly [ 37]/enc-37/enc-37/enc-34 ServiceOperation
Dejavu[33]/enc-34/enc-37/enc-34 Fault Type
CloudRCA[ 74]/enc-34/enc-34/enc-37 Service
PDiagnose[ 21]/enc-34/enc-34/enc-34ResourceType or Error Log
Nezha (ours) /enc-34/enc-34/enc-34CodeRegion or
ResourceType
employ metrics-based, traces-based, and logs-based approaches to
determine root causes, each method may propose a distinct root
cause.There is currently no eﬀective strategyto consolidate these
outcomes or distinguish the true root cause among them. As a
result, SREs are compelled to manually verify each result one by
one,whichisalabor-intensivetask.(3)Independentlymanaging
multiplesingle-modalRCAapproachesnecessitatesincreasedef-
fort. In light of these challenges, we advocate for the integration of
multi-modal data intoa uni/f_iedrepresentation,which will enhance
the eﬃciency andaccuracyofRCA.
Motivation 4: Facilitating Fault Mitigationthrough Unsu-
pervised Fine-grained RCA with Enhanced Interpretability.
Dejavu[33]andCloudRCA[ 74]aretwosupervisedmulti-modal
RCAapproachesthatnecessitateasubstantialtrainingdatasetwith
labels. However, acquiring such a dataset can be costly and imprac-
tical for each application. The multi-modal method PDiagnose [ 21]
convertsheterogeneousmulti-modaldataintotimeseriesandiden-
ti/f_iesrootcausesbyevaluatingabnormaltimeseries.Althoughthis
transformationcanyieldeﬀectiveresults,theprocessmayresult
in a loss of execution context, which is essential for developers
tocomprehendtheunderlyingrootcause.Preservingtheoriginal
executioncontextenhancesinterpretabilityinrootcauseidenti/f_i-
cation,subsequentlyincreasingSREs’con/f_idenceintheobtained
results.Moreover,localizingrootcausesattheservicelevelrequires
amorecoarse-grainedapproach.SREsmustexpendconsiderable
eﬀorttodeterminethespeci/f_iccoderegionorresourceaccountable
for faults. From the SREs’ perspective, a /f_ine-grained root cause
identi/f_ication can alleviate their workload and reduce the mean
timetomitigation.Therefore,wetrytoproposeanunsupervised
/f_ine-grained RCA apparoach with improved interpretability, which
facilitates more eﬀective faultmitigation.
2.3 ProblemFormulation
We formalize the problem of /f_ine-grained root cause localization
using multi-modal observability data. Suppose that a large-scale
microservicesystemwith /u1D441microservices,metrics,traces,andlogs
areaggregatedindividuallyateachmicroservice.Ina slidingtime
window (e.g., 1 minute), we have multi-modal observability data
de/f_inedas Θ=/braceleftBig/parenleftBig
/u1D703M/u1D45B,/u1D703T/u1D45B,/u1D703L/u1D45B/parenrightBig/bracerightBig/u1D441
/u1D45B=1,whereatthe /u1D45B-thmicroservice,
/u1D703M/u1D45B={M1,...,M/u1D45A}indicates/u1D45Ametrics,/u1D703T/u1D45B={T1,...,T/u1D461}denotes
/u1D461traces,and /u1D703L/u1D45B={L1,...,L/uni2113}represents /uni2113logs.Given thedata ΘCcollectedfromfault-free constructionphase
CandΘPcollectedfromfault-suﬀeringproductionphase P,we
/f_irst unify the multi-modal data in ΘCandΘPas events and
extract event patterns P={/u1D45D1,...,/u1D45D/u1D458}via constructing and min-
ingeventgraphs.Weattempttoidentify/f_ine-grainedrootcauses
throughthreephases:1)weidentifywhicheventpatternsdonot
follow expected execution paths and rank them into expected pat-
tern list/u1D43F/u1D456/u1D460/u1D461E,where/u1D43F/u1D456/u1D460/u1D461E={...,(/u1D45D/u1D456,/u1D446/u1D450/u1D45C/u1D45F/u1D452E(/u1D45D/u1D456)),...,}; 2)we pin-
point how expected patterns change in the actual fault-suﬀering
phase and rank them into actual pattern list /u1D43F/u1D456/u1D460/u1D461A, where/u1D43F/u1D456/u1D460/u1D461A=
{...,(/u1D45D/u1D457,/u1D446/u1D450/u1D45C/u1D45F/u1D452E(/u1D45D/u1D457)),...,}; 3) we correlate expected patterns with
actual patterns and takes pattern pairs as /f_inal root cause list
/u1D43F/u1D456/u1D460/u1D461R={...,(/u1D45D/u1D456,/u1D45D/u1D457,/u1D446/u1D450/u1D45C/u1D45F/u1D452E(/u1D45D/u1D456)),...}. SREs can check why /u1D45D/u1D456turn
into/u1D45D/u1D457to determinethe /f_inal solution to recover applications.
3 OVERVIEW
In this study, we present Nezha, an unsupervised /f_ine-grained RCA
approachby incorporative analysis of multi-modal data inaninter-
pretablemanner.Figure 4showstheoverallstructureof Nezha.The
activitiesof Nezhacanbedividedintothefault-freeconstruction
phaseandthe fault-suﬀeringproduction phase. Inthe construction
phase,Nezhatakesthefault-free observabilitydataas input,which
contains all request types of the application within a time window.
Obtainingashortwindowoffault-freedataistrivialforSREsbe-
causemostofthetimeisinnormalstatusandfaultsarescarcein
production environment [ 30]. We recommend setting the window
sizetothesameintervalasthemetriccollection(1minutebyde-
faultinthisstudy). Nezhaintegratestheseobservabilitydataand
minestheirpatternsoﬄine.Dataintegrationandpatternmining
in the constructionphaseare the same asin the productionphase.
In the production phase, 1○Anomaly Detector (§4.1) detects
whether the performance and availability of the system are abnor-
mal in real time. If an anomaly occurs, 2○Data Integrator (§4.2)
uni/f_ies the multi-modal data in the abnormal time window into
events and consolidates the events into event graphs. 3○Pattern
Miner(§4.3)extractscommonpatternsandcalculatessupportfor
them from the event graphs in parallel. 4○Expected Ranker inPat-
ternRanker (§4.4)ranksthepatternsthatfrequentlyoccurinthe
fault-free phase but rarely in the fault-suﬀering phase as the ex-
pectedpatterns. ActualRanker ranksthepatternsthatfrequently
occur in the fault-suﬀering phase but rarely in the fault-free phase
astheactualpatterns. 5○PatternAggregator (§4.5)aggregatesthe
expected patterns and actual patterns to determine the ranked list
ofrootcausecandidateswithinterpretability.Comparedwithex-
istent RCA approaches, Nezhaprovides /f_ine-grained root cause
candidates (i.e., region of code defect or resource type) and tells
SREs why candidates are anomalous.
4 DETAILED DESIGN
4.1 AnomalyDetector
As described in § 2.1,Nezhakeeps monitoring application-level
metricsofsystemstodetectanomalies.Guaranteeingtheuserexpe-
rience, which is typically re/f_lected in availability and performance,
is a critical requirement for cloud applications. The availabilityof
systemscanbere/f_lectedbytherequestsuccessratio,whichisthe
fraction of the number of successful requests to total requests over
556Nezha: InterpretableFine-GrainedRoot Causes Analysis forMicroservicesonMulti-modalObservabilityData ESEC/FSE ’23, December3–9, 2023,San Francisco, CA, USA
Data
Integrator
Log
Trace
Metric②		Data
Integrator③	Pattern
MinerConstrution	Phase
(fault-free)
Production	PhasePattern	
Miner
①Anomaly
Detector④	Pattern	Ranker
Fault-suffering	Patterns Trigger GraphsGraphs Fault-free	Patterns
⑤	Pattern
AggregatorExpected
PatternsPattern	
StorerPatterns
Events GraphsExpected	Ranker
Actual	Rankere1
e2e3
e4e5e6e7
e6 e1e7e3e4
e2e5e5e1e2
e1e2e5
e1e7		①	Expected:	
							Actual				:e1e3
e1e7
		②	Expected:	
							Actual				:e3e6
e3e4
		③	......Rank	List
Actual	
Patterns
Alert	Event Log	Event Span	Event
Figure 4: The overview of Nezha.Nezhauses the multi-modal observability data as input and outputs the ranked list of
suspicious /f_ine-grained root causes.
aperiodoftime.Theperformance issuesmanifestthemselves as
long-time request duration. To capture the tail latency, we use P90
latency,whichistheaveragelatencyfortheslowest10%ofrequests
over a period of time. Compared to the average latency, P90 can
show the tail latency explicitly. As aforementioned, the success
ratio and P90 latency delineate the system’s health status. We use
time interval to denote how often the metrics are collected. The
time intervalvalueissetfor one minuteinthis study.
Anomaly Detector uses /u1D458-/u1D70Erules [35] to determine whether the
target application isin an abnormal status.We calculate the mean
/u1D707and standard deviation /u1D70Eof success ratio and P90 latency in the
construction phase. In the production phase, Anomaly Detector
continuallymonitorsthesuccessratioandP90latencyoffront-end
serviceinaslidingtimewindow.Ifthesuccessratioislessthan /u1D707−
/u1D458/u1D70EorP90latencyisgreaterthan /u1D707+/u1D458/u1D70E(/u1D458=3bydefault),Anomaly
Detectordeclaresthecurrenttimewindowisabnormalandtriggers
a root cause analysis. /u1D458-/u1D70Erule is a simple but eﬀective approach
and is widely used in academia and industry [ 35,67,75]. InNezha,
thismodulecanbeeasilyreplacedwithotheranomalydetection
approaches(e.g.,USAD [ 2], RRCF[31]andJumpStarter [ 43]).
4.2 Data Integrator
After Anomaly Detector determines whether the application is
under abnormal status, Data Integrator /f_irst queries ΘPof the
application in the abnormal time window. Considering the het-
erogeneity of observability data, Data Integrator transforms the
observability data in the time windows into events, which is the
basic unit of Nezha. Theseevents in thesame service instance are
then ordered according to their timestamps and span IDs while
overcomingclocksynchronizationproblems.Eachrequestinthe
time windowcorrespondsto an eventgraph.
4.2.1 UniqueEvent Generation.
Definition 1 (Event /u1D452).An event/u1D452records theexecution status
of a system at a point in time. We use event set /u1D438/u1D456={/u1D4520,...,/u1D452/u1D45B}to
denote theset ofall events forrequest /u1D456.
Metric.In eachtime window, a service hasmany logs butonly
one metric sample. To overcome the heterogeneity between nu-
mericalmetricsandtext-structuredlogs,DataIntegratorreplaces
the set ofmetrics with suspiciousmetric alerts. This isreasonable
becauseanomalousmetricsthatcausealertsprovidemoreinforma-
tiontoRCAthannormalmetrics.Metricsalertsaregeneratedwhen
metricvaluesviolatethe /u1D458-/u1D70Eruleorstaticthresholds. Nezhaisalso
compatiblewithalarmsystemssuchasPrometheus Alertmanager.Front/Recv
Product/GetProductFront/Recv_Start
Raw	Trace Trace	EventProduct/GetProduct_Start
Product/GetProduct_End
Front/Recv_EndTransform
Figure 5:Transformation fromaraw trace to trace events.
Front/Recv_start
Request	failedProduct/ListProd_start
Product/ParseCa_start
Parse	catalog	failedGroup	A:	Front Group	E:	Product
Front/GetCart_endCart/GetCart_asyn
GetCart	with	userStart	list	product
Group	F:	ProductGroup	C:	Front
Group	D:	CartFront/GetProd_startGroup	B:	Front
Server	Span	Event
Client	Span	Event	Alert	Event Log	Event
CPU_AlertFront/Recv_end
Front/GetCart_startFront/GetProd_end
Product/ListProd_end
Product/ParseCa_endMem_Alert
Figure 6: An example of event graph in OnlineBoutique. All
trace events andlogevents have the sametrace ID.
Thetimebetweenthestartandendofanalertiscalled alerttime .
Wetreatalertsaseventsthatrepeatedlyoccurwithinthealerttime.
Asdiscussedin§ 2.1,application-levelmetricstypicallyre/f_lect
faults’symptomsratherthantherootcausesoffaults.Therefore,
we only consider the system-level metrics in RCA. In addition, we
foundthatsomealertsfrequentlyoccurnomatterwhetherthere
is a fault or not. Such regular alerts are not helpful to RCA and
sometimes even mislead SREs. To /f_ilter out these irrelevant metric
alerts,Nezhaexcludesthealertsintheproductionphasethatalso
occur inthe construction phase.
Log.Nezha/f_irst extracts and records the trace IDs, span IDs
(detailshownin§ 2.2),andtimestampsfromrawlogmessages.Data
Integratoradoptsastate-of-the-artlogparsingapproachDrain[ 19]
toextractthestaticlogtemplatesanddynamiclogparametersfrom
the raw log messages in a streaming manner. After log parsing,
wetreatthestaticlogtemplatesaslogevents.Todistinguishthe
log events associated with diﬀerent requests, each log event is
accompaniedbyitscorrespondingtraceID,spanID,andtimestamp.
Trace.The relation between parent and child span can be di-
vided into synchronous and asynchronous calls. With regard to
synchronouscalls,weconsiderthestartandendofaspanastwo
traceevents.Thesetwoeventmessagescanberepresentedasacon-
catenationofthespannamewith“start”or“end”string.Figure 5
557ESEC/FSE ’23, December3–9, 2023,San Francisco, CA, USA Guangba Yu, Pengfei Chen, Yufeng Li, HongyangChen, Xiaoyun Li, andZibin Zheng
shows an example of transforming traces with the synchronous
call to trace events. In terms of asynchronous calls, Data Integrator
representsthem aseventsconsisting ofspanname andthe“asyn”
string,e.g., /u1D452=“Cart/GetCart_asyn”.Eachtraceeventisaccompa-
niedbyits trace ID,span ID,andtimestamp from span records.
4.2.2 Event GraphConstruction.
Definition 2 (Event Graph /u1D454).An event graph /u1D454/u1D456=(/u1D438/u1D456,/u1D43F/u1D456/u1D45B/u1D458)
isadirectedgraphofeventsintheeventset /u1D438/u1D456.Adirectedlinkbetween
/u1D452/u1D457and/u1D452/u1D457+1(i.e.,/u1D452/u1D457→/u1D452/u1D457+1)denotesthat /u1D452/u1D457isfollowedby /u1D452/u1D457+1during
theexecution.Weuse /u1D43ACand/u1D43APto denotethesetof event graphs
fortheconstructionand production phase,respectively.
Foreachrequestinatimewindow,DataIntegratorconstructs
an eventgraph inthe following three steps.
(1)Ordereventsinthesamespan. Foreachspan,DataIntegrator
obtainsalllogandtraceeventswhichoccurredwithinthatspan.
DataIntegratorthenchronologicallyordersthelogandtrace
events into an event group based on their timestamps and adds
asequencerelationshipfromaneventtoitsnexteventinthe
group.Fig. 6showssomeexamplesofeventgroups.
(2)Insertmetriceventstoeventgroups. DataIntegratorinserts
the alert events after the /f_irst event of the event group if the
correspondingservicehasalerteventswithoutlossofgenerality.
It can also be inserted in other /f_ixed locations as agreed. If
multiple alert events of the same service are detected, all alarm
events will be sequentially inserted after the /f_irst event. For
instance,DataIntegratorinserts Mem_Alert andCPU_Alert of
cartservice intoits eventgroup inFig. 6.
(3)Insert child groups to parent groups. If the child span is
in the same service instance as the parent span (i.e., internal
function calls), Data Integrator inserts child groups after the
last event in parent groups whose timestamp is less than the
/f_irsteventinthechildgroup(e.g.,therelationbetweenGroup E
andFin Fig.6). We use timestamps directly because these two
groups are on the same service instance and on the same node,
so they do not have the problem of clock drift. For RPC call
spansacrossservices,DataIntegratorinsertsthechildgroup
afterthe/f_irsteventofitsparentgroupbasedontheparentspan
IDtoovercometheclockdrift(e.g.,therelationbetweenGroup
BandEinFig.6).
After these steps, we represent theheterogeneousmulti-modal
dataashomogeneouseventsandconstructtherelationshipsamong
eventsasgraphs.ThoughDeepTraLog[ 72],whichfocusesonanom-
aly detection rather than RCA, integrates logs and traces into
graphs, it does not consider metric clues. Nezhaovercomes the
shortcoming of DeepTraLog by innovatively transforming metrics
intoeventsandincorporatingthemintotheeventgraph.Thereason
whyweusegraphsratherthansequenceslikeMinesweeper[ 44]
to represent the relationshipsbetween events is that microservice
applicationsmaycontainasynchronouscalls.Theeventlocation
of asynchronous calls may change uncertainly in the sequential
sequences, making it diﬃcult for Nezhato mine for stable patterns.
Compared with PDiagnose [18] that transforms heterogeneous
multi-modaldataastimeseries, Nezharetainstheexecutioncontext
of requestswhen transforming multi-modal data as homogeneous
events,whichallowsfor improvedinterpretabilityinRCA.e1e2e3e5
e1e2e3e5e1e2e3e5
e4Req 1
Req 2
Req 1Construction	Phase	G C
e1e2e6
e1e2e3e5e1e2e6
e4Req 1
Req 2
Req 1Production	Phase	G P
Figure 7:Examples of event graphs inconstruction and pro-
ductionphase. /u1D445/u1D452/u1D45E/u1D456denotesthe /u1D456thkindofrequest(e.g.,login
or query product). The pattern /u1D4522→/u1D4523in/u1D43ACturned into
/u1D4522→/u1D4526in/u1D43APwhenafaultoccurred.
Table 2:Example of Nezhato perform RCA fromFig. 7.
PatternSupport ScoreDeepth Rank(/u1D446/u1D450/u1D45C/u1D45F/u1D452E)/u1D446C/u1D446P/u1D446/u1D450/u1D45C/u1D45F/u1D452E/u1D446/u1D450/u1D45C/u1D45F/u1D452A
/u1D4521→/u1D4522330.5 0.5 1 2
/u1D4522→/u1D4523310.75 0.25 2 1
/u1D4522→/u1D4524110.5 0.5 2 -
/u1D4523→/u1D4525310.75 0.25 3 -
/u1D4522→/u1D4526020.0 1 2 3
“-” meansthat thepattern is aggregatedby Nezha.
4.3 Pattern Miner
Definition3(Pattern /u1D491).Apattern /u1D491isasubgraphofcontigu-
ous events in theset ofeventgraphs /u1D43A.
After integrating multi-modal data into event graphs, Nezha
extractsthefault-freeandfault-suﬀeringpatternsfrom /u1D43ACand/u1D43AP
by traversing all event graphs in parallel. The patterns in the event
graphs are /f_inite because logs have been parsed into templates and
onlythedirectlyconnectedeventsareconsidered.Asanexample
in the left part of Fig. 7, a pattern /u1D4521→/u1D4522→/u1D4523matches the event
graphofthe/f_irstrequestbecausethegraphhas /u1D4521followedby /u1D4522
and/u1D4522followedby /u1D4523withoutothereventsinvolved.
Definition 4 (Support /u1D460).Given a pattern /u1D491’s count set /u1D436/u1D491=
{/u1D4501,...,/u1D450/u1D458},where/u1D450/u1D456denotes/u1D491occurs/u1D450/u1D456timesintheeventgraph /u1D454/u1D456,
thesupport /u1D460(/u1D491)ofpattern /u1D491isthesumofthecountsinallgraphs,
i.e.,/u1D460(/u1D491)=/summationtext.1/u1D458
/u1D456=0/u1D450/u1D456./u1D460C(/u1D491)and/u1D460P(/u1D491)denote the support of /u1D491in/u1D43AC
and/u1D43AP, respectively. We use SCandSPto denote the support set of
all patternsin the /u1D43ACand/u1D43AP,respectively.
After extracting patterns, Pattern Miner counts the occurrences
of each pattern to calculate the support of each pattern. Table 2
showsanexampleofPatternMinerminingpatternsandcalculating
supportsinFig. 7.Intermsofpattern /u1D4522→/u1D4523,itoccursinallthree
graphs in the construction phase. Therefore, /u1D460C(/u1D4522→/u1D4523)=3in
the construction phase. To prevent repeated calculations of SC,
wepersisttheresultsof SCintoPatternStorer .Whendiagnosing
faults,weareinterestedinidentifyingtherootcausesthatresultin
alargeportionof overallabnormalbehavior. Therefore,wediscard
thosepatternthatrarelyoccursby/f_ilteringpatternswhosesupport
less than /u1D460/u1D45A/u1D456/u1D45B(/u1D460/u1D45A/u1D456/u1D45B=5bydefault).
4.4 Pattern Ranker
After transforming multi-modal data as generalized events (i.e.,
excluding speci/f_ic request parameters), the event patterns in the
558Nezha: InterpretableFine-GrainedRoot Causes Analysis forMicroservicesonMulti-modalObservabilityData ESEC/FSE ’23, December3–9, 2023,San Francisco, CA, USA
construction phase are similar to those in the production phase.
During the fault-suﬀering phase, faults manifest themselves in one
or more data sources, which causes some event patterns to change.
Inotherwords,someeventpatternsdonotfollowtheirexpected
execution paths in the fault-free phase. Such patterns are likely to
revealinsightsintoeventsassociatedwithrootcauses.Forexample,
when a fault occurs between the code region of /u1D4522and/u1D4523in Fig.7,
SREscanchecktheevent graphsoneby oneandidentifythatthe
pattern/u1D4522→/u1D4523in/u1D43ACturns into /u1D4522→/u1D4526in/u1D43AP. SREs then take
the code region between /u1D4522and/u1D4523as the root cause candidate and
inspect the reason why /u1D4522→/u1D4523turns into /u1D4522→/u1D4526to determine
the /f_inal solution. We designed two rankers: Expected Ranker and
Actual Rankerto automate the above RCA process.
4.4.1 ExpectedPa/t_ternRanker. ExpectedPatternRankeraimsto
identifywhich event patterns do not follow expected execution paths
andtakethemas expectedpatterns .ThenSREscancheckwhythese
patterns do not follow expected execution paths to solve faults.
The core idea of Expected Ranker is to rank event patterns that
occur multiple times in the fault-free phase but rarelyin the fault-
suﬀeringphaseaboveothereventpatterns.Arankingscore /u1D446/u1D450/u1D45C/u1D45F/u1D452E
is de/f_ined to measure how much utility each pattern contributes
to root cause diagnosis. For each pattern /u1D491inC, Expected Pattern
Rankercomputes its rankingscore /u1D446/u1D450/u1D45C/u1D45F/u1D452E(/u1D491)as follows,
/u1D446/u1D450/u1D45C/u1D45F/u1D452E(/u1D491)=Pr(/u1D454∈/u1D43AC|/u1D491⊑/u1D454)
=/u1D460C(/u1D491)
/u1D460P(/u1D491) +/u1D460C(/u1D491).(1)
The/u1D446/u1D450/u1D45C/u1D45F/u1D452E(/u1D491)ofthe pattern /u1D491quanti/f_ies howdistinctive /u1D491isto
the/u1D43ACas opposed to the /u1D43AP. If pattern /u1D491occurs multiple times in
/u1D43ACwhilerarelyin /u1D43AP,/u1D491willbeassignedahigherscore.Therefore,
the patterns with higher ranking scores are more suspicious to be
rootcauses.Asanexample,anexceptionfaultoccursinthecode
region of /u1D4522and/u1D4523in the production phase in Fig. 7. From Fig. 7,
we can /f_ind that /u1D4522is always followed by /u1D4523in/u1D43ACbut rarely in
/u1D43AP. Thus, it is intuitive to infer that there is a fault in the code
region between /u1D4522and/u1D4523, causing /u1D4522not to follow /u1D4523. In terms
of Pattern Ranker, it computes the ranking score of /u1D4522→/u1D4523as
/u1D446/u1D450/u1D45C/u1D45F/u1D452E(/u1D4522→/u1D4523)=3
3+1=0.75, which is the highest score (i.e.,
mostsuspicious)inthe example.
4.4.2 Actual Pa/t_tern Ranker. Though Expected Pattern Ranker
presentseventpatternsthatdonotfollowexpectedexecutionpaths,
SREsalsoexpecttoknow howthesepatternschange intheactual
fault-suﬀeringphase,whichwillfacilitateSREstounderstandfaults.
Therefore,ActualPatternRankerisdesignedtopinpointthenewly
emergingpatternsthatbreaktheirexpectedexecutionpathsand
takethemas actualpatterns .ThecoreideaofActualRankeristo
rank event patterns that occur multiple times in the fault-suﬀering
phase but rarely in the fault-free phase above other event patterns.
For each pattern /u1D491inP, Actual Pattern Ranker de/f_ines its ranking
score/u1D446/u1D450/u1D45C/u1D45F/u1D452A(/u1D491)as follows,
/u1D446/u1D450/u1D45C/u1D45F/u1D452A(/u1D491)=Pr(/u1D454∈/u1D43AP|/u1D491⊑/u1D454)
=/u1D460P(/u1D491)
/u1D460C(/u1D491) +/u1D460P(/u1D491).(2)Ranking	List
:	0.7
:	0.6:	0.6:	0.8:	0.8
:	0.8e1e2
e2e3
e2e4
e5e6
e6e7
e5e8e1e2e3
e4
e5e6
e8e7Anomaly	Graph
:	0.8e1e2
:	0.7e5e6
:	0.6e5e8Aggregated	List
Figure 8: Example of constructing anomaly graphs to get an
aggregated list inPatternAggregator.
Nezhasearch...
 admin
Root Cause Analysis >>
1. Root Cause Service: adservice
Excepted Pattern: adservice/GetAds Start àadservice.java:130 àadservice.java:140
Actual     Pattern: adservice/GetAds Start àadservice.java:130 àadservice.java:151
2. Root Cause Service:  cartservice
Metric Alert: CPU Usage
Monitor Result:Root Cause Results     Anomaly Time: 2023-01-02 10:00:01      RCA Time: 2023-01-02 10:00:30
Figure 9:Ademoforinspecting root cause candidate.
Actually,not allpatternsinExpectedPattern RankerandActual
PatternRankerprovideusefulinformationforrootcausediagnoses.
For example, the pattern /u1D4521→/u1D4522is given a score 0.5 in Fig. 7.
But/u1D4521→/u1D4522occurs 3 times in both /u1D43ACand/u1D43AP, which is hardly
helpful for analyzing the underlying cause. Therefore, we specify a
minimumscorethreshold /u1D446/u1D450/u1D45C/u1D45F/u1D452mintoexcludesuchuselesspatterns.
In this way, the pattern /u1D491is placed in the ranked score list only
when/u1D446/u1D450/u1D45C/u1D45F/u1D452E(/u1D491) ≥/u1D446/u1D450/u1D45C/u1D45F/u1D452min. After excluding useless patterns, we
refer to the ranked pattern list output by Expected Pattern Ranker
astheexpectedpatternslistandtherankedpatternlist outputby
Actual Pattern Rankeras the actual patterns list.
4.5 Pattern Aggregator
Although Expected Ranker ranks patterns that point towards root
causes,itcansometimesreturnalonglistwhichisunfriendlyto
SREs.Thisisbecausefaultsmaycausedownstreampatternsofroot
causes in /u1D43APto change, resulting in all downstream patterns of
root causes in /u1D43AChaving a high score as root causes. For instance,
intheexampleinFig. 7,thepatterns /u1D4522→/u1D4523and/u1D4523→/u1D4525sharethe
samescoreasoneexceptionfaultbetween /u1D4522and/u1D4523causesneither
/u1D4523nor/u1D4525to occur in /u1D43AP. Actually, /u1D4522→/u1D4523and/u1D4523→/u1D4525point to
thesamefault.Checkingallpatternspointingtothesamefaultis
unnecessaryandburdensomeforSREs.Inthisstudy,werefertothe
downstreampatternswiththesamescorelike /u1D4523→/u1D4525asredundant
patterns.PatternAggregatoraimstoexcludetheredundantpatterns
ofExpectedRankertoprovideamorevaluablelistforSREstospeed
uptroubleshooting.
PatternAggregatorconstructsanomalygraphsbyassociating
expectedpatternsand/f_ilteringredundantpatternsby retainingthe
rootpatternsofgraphs.Ifbothpatterns /u1D452/u1D456→/u1D452/u1D457and/u1D452/u1D457→/u1D452/u1D458arein
the list and /u1D446/u1D450/u1D45C/u1D45F/u1D452(/u1D452/u1D456→/u1D452/u1D457) ≥/u1D446/u1D450/u1D45C/u1D45F/u1D452(/u1D452/u1D457→/u1D452/u1D458), Pattern Aggregator
will join /u1D452/u1D458into/u1D452/u1D456→/u1D452/u1D457(i.e.,/u1D452/u1D456→/u1D452/u1D457→/u1D452/u1D458). After iterating
allpatternsinthelist,PatternAggregatorobtainssomeanomaly
graphsconstructedfromthesepatterns.Phase 1○inFig.8shows
an example of constructing anomaly graphs. Pattern Aggregator
then chooses the root patterns of these anomaly graphs as /f_inal
559ESEC/FSE ’23, December3–9, 2023,San Francisco, CA, USA Guangba Yu, Pengfei Chen, Yufeng Li, HongyangChen, Xiaoyun Li, andZibin Zheng
expectedpatterns.Phase 2○inFig.8showsthatthepatternnumber
decreasedfrom 6to 3after excluding redundantpatterns.
To improve the interpretability of Nezha, we correlate the ex-
pectedpatterns withactual patterns to provide the complete fault
scene. For the expected pattern, we identify its associated actual
patternwiththecommonpre/f_ix.Forinstance,theexpectedpattern
/u1D4521→/u1D4522→/u1D4523hasthecommonpre/f_ix /u1D4521→/u1D4522astheactualpattern
/u1D4521→/u1D4522→/u1D4526inFig.7.Ifthereismorethanoneactualpatternwith
thecommonpre/f_ix,weselecttheactualpatternwiththehighest
score as the actual pattern.
Eventually, Nezhaoutputsarankedlistofrootcausescandidates
tobechecked.Candidatesarerankedindescendingorderbasedon
thescoreofthecandidate’sexpectedpattern.Forexpectedpatterns
with the same score, we place the pattern with the deeper depth in
the event graph further up the list because patterns with shallower
depths are more likely to be causedbyanomalypropagations.
Moreover,weimplementademoof Nezha,shownasFig. 9,to
demonstraterootcausescandidatesforSREs.AsshowninFig. 9,
one pair of expected and actual patterns constitutes a root cause
candidate.Forcandidateswithoutmetricalertevents, Nezhashows
the service name and the code region between events (e.g., 1st
candidateinFig. 9).Otherwise, Nezhadisplaysthemetricalertevent
andcorrespondingmonitoringview(e.g.,2ndcandidateinFig. 9).
Tosumup, Nezhaisabletoprovide/f_ine-grainedandactionableroot
cause candidates andtellsSREs why candidates are anomalous.
5 EXPERIMENTALEVALUATION
Inthissecion,weaimtoevaluate Nezhatoanswerthefollowing
researchquestions(RQs):
•RQ1:Howeﬀective is Nezhainservice-level RCA?
•RQ2:Howeﬀective is Nezhaininner-service-level RCA?
•RQ3:Howmuchdoes multi-modaldata contributeto RCA?
5.1 Experiment Setup
MicroserviceApplications. Wedeploytwoopen-sourcemicroser-
viceapplications:OnlineBoutique(OB)andTrainTicket(TT)inour
testbed.OnlineBoutiqueisamicroservicesystemfore-commerce
andTrainTicketprovidesarailwayticketingservicewhereusers
can check, book, and pay for train tickets. Both of them have been
widely used in many previous studies [ 7,29,37,67,70,72,76]. The
open-sourced OnlineBoutique and TrainTicket are not equipped
with adequate observability (e.g., traces are incomplete and logs do
notcontaintraceID).Inthisstudy,we/f_irstinstrumenttheOpen-
telemetry SDK [ 48] for each service to obtain complete traces. We
then modify one line of logging pattern in the logging con/f_igu-
ration of Java service [ 39] to insert trace and span IDs into logs.
For services implemented in other languages, we need to insert
2 lines of code into the source code based on Opentelemetry to
obtain trace and span IDs and modify the existent log statement
foreachlog(e.g.,Fig. 10).WiththestandardOpentelemetrytoolkit
andexamplesinmanyprogramminglanguages[ 38–41],itisnot
diﬃcult for developers to insert trace andspan IDs intothe logs.
ExperimentalPlatform. WedeploytheOnlineBoutiqueand
TrainTicket on a Kubernetes platform with 12 virtual machines,
each of which has a 8-core 2.10GHz CPU, 16GB memory, and runs
with Ubuntu 18.04 OS. We use Opentelemetry Collector [ 49] tolog. error( 'Req	Failed' )
trace_id	=	 '{trace:032x}' .format( trace=ctx .trace_id)
span_id	=	 '{span:016x}' .format( span=ctx .span_id)
log. error( 'Trace_id=%s	Span_id=%s	Req	Failed' ,	trace_id,	span_id )Source	Code
Modified	Code
Figure 10:Example ofcorrelation trace with logs.
collect traces and store them in Grafana Tempo [ 17]. Logs are
collected by Grafana Promtail [ 16] and stored in Grafana Loki [ 15].
Formetrics,weusecAdvisor[ 5]tocollectsystem-levelmetricsand
Istio[24]tocollect application-level metrics(e.g., requestlatency).
Moreover, Prometheus Node Exporter [ 54] on each node is used to
export metrics to Prometheus[ 53]databaseto persist them.
Data Collection. We use fault injection to mimic application
issuesfollowingthepreviouswork[ 35,37,42,67,72,73].Tomimic
resource issues, we inject CPU contention and network jam faults
in the same way as the existing work [ 35,37,67]. Mimicking code
defectsatprogramruntimeischallengingbecausewecannotstopor
restart services. To achieve this, we design some language-speci/f_ic
fault injectors for the characteristics of program language for Java,
Golang, and Python services [ 4,28,52]. We use the above injectors
toinjecterrorreturnandexceptioncodedefectsfollowingprevious
work[42,72,73].Weseteachfaultdurationto3minutestoemulate
theprocessbetweenfaultoccurrenceto/f_ix.Werandomlyinjectone
fault into one microservice following previous work [ 35,72,73]. In
total,weinject56faults(42resourceissuesand14codedefects)into
OnlineBoutiqueandobtaintracesandapplicationlogs.Weinject45
faults (20 resource issues and 25 code defects) into TrainTicket and
gettracesandapplicationlogs.Thecollectedmetricsencompass
application-level measurements (e.g., success ratio) and system-
level metrics (e.g.,CPUusage rate).
EvaluationMetric. Weusethe followingtwometricsto mea-
suretheeﬀectivenessof Nezhaandbaselinesbecausesomebase-
linescan only localizeroot causes at the service level.
•Top-k accuracy at service level ( /u1D434/u1D446@/u1D458)refers to the probabil-
ity that root cause services are includedinthe top-k results.
•Top-kaccuracyatinner-servicelevel( /u1D434/u1D43C/u1D446@/u1D458)referstothe
probabilitythattheinner-servicerootcauses(resourcetypeor
code region) are includedinthe top-k results.
ImplementationandSettings. Weimplementtheprototype
ofNezhabuiltonPython3.6.Allexperimentsareconductedona
Linux server with Intel Xeon Gold 5318Y 2.10GHz CPU, 256 GB
RAM, 1TB SSD Disk, and running Ubuntu 18.04. The minimum
score threshold /u1D446/u1D450/u1D45C/u1D45F/u1D452min, which is used to avoid the in/f_luence of
normal /f_luctuations,is setto 0.67(i.e.,2
3) by default. In this case,a
pattern/u1D491inExpectedPatternRankerissuspiciousifthesupport
of/u1D491in fault-suﬀering phase is less than half of the support of /u1D491in
fault-free phase.The in/f_luence of /u1D446/u1D450/u1D45C/u1D45F/u1D452minisdiscussedin§ 5.3.5.
5.2 Baselines
Weusethefollowingsixstate-of-the-artunsupervisedmetric-based,
trace-based,andlog-basedRCAapproachesasthebaselines.Wedo
not consider the supervisedapproaches because they need a large
training dataset withlabels, whichishardto obtain inpractice.
•MicroScope[ 35]isa metric-basedRCAapproachthatidenti/f_ies
rootcausesbasedonthecorrelationofmetricsalongdependency.
560Nezha: InterpretableFine-GrainedRoot Causes Analysis forMicroservicesonMulti-modalObservabilityData ESEC/FSE ’23, December3–9, 2023,San Francisco, CA, USA
Table 3:Comparisonofbaselines at service level.
ApproachOnlineBoutique TrainTicket
/u1D434/u1D446@1/u1D434/u1D446@3/u1D434/u1D446@5/u1D434/u1D446@1/u1D434/u1D446@3/u1D434/u1D446@5
MicroScope 12.5 41.07 55.35 17.78 26.67 35.56
MicroRCA 16.07 62.5 92.75 20.00 31.11 44.44
SBLD 19.64 23.21 25.00 15.56 22.22 24.44
LogFaultFlagger 19.64 21.42 23.21 17.78 24.44 24.44
MicroRank 41.07 48.21 62.5 15.56 24.44 35.56
TraceAnomaly 30.35 33.92 48.21 13.33 28.89 33.33
PDiagnose 41.07 73.21 82.14 8.89 13.33 22.22
Nezhaw/oML14.28 17.85 17.85 6.67 8.89 11.11
Nezhaw/oM26.78 33.92 35.71 55.56 62.22 68.89
Nezhaw/oL64.28 64.28 64.28 42.22 44.44 44.44
Nezha 92.86 96.43 96.43 86.67 97.78 97.78
•MicroRCA[ 64]presentsametric-basedRCAapproachthatlo-
calizes suspicious services by applying a PageRank method [ 50]
onthe extractedanomalysub-graph.
•LogFaultFlagger [ 1] is a log-based RCA approach that compares
passingandfailinglogsto /f_ind faultsinfailinglogs.
•SBLD [55] is a log-based RCA approach that analyzes the cover-
ageoflogeventsusingSpectrumalgorithms[ 25]to/f_indsuspi-
cious logevents.
•MicroRank [ 67] proposes a trace-based RCA approach that com-
binesthepersonalisedPagerankmethodandSpectrummethod
to locate suspiciousroot causes.
•TraceAnomaly [ 37] provides a trace-based RCA approach that
adoptsadeeplearningmethodtolearnnormalpatternsoftraces
oﬄine anddetectanomaloustraces onlineto perform RCA.
•PDiagnose[ 21]takesmetrics,traces,andlogsasinputandtrans-
forms them into time series. Then PDiagnose determines root
causes throughvotingabnormal time series.
5.3 EvaluationResults
5.3.1 RQ1: Eﬀectiveness at Service Level. The ground truths at the
service level are the known injected services. Table 3shows the ef-
fectiveness evaluation results of diﬀerent approaches at the service
level. From Table 3, we can observe that Nezhaoutperforms all the
baseline approaches signi/f_icantly and achieves high accuracy in
/u1D434/u1D446@1(90%),/u1D434/u1D446@3(97%),/u1D434/u1D446@5(97%)onaverage,illustrating that
Nezhacan successfully localize root causes at service level most of
the time. The excellent performance of Nezhais mainly attributed
to the fact that Nezhatakes multi-modal data as input and fuses
them so that it can capture the abnormal behaviours of a wider
range offaultsituations.
Themetric-basedapproachesMicroScopeandMicroRCAachieve
low/u1D434/u1D446@1and/u1D434/u1D446@5onaverage.Afteradetaileddissectionoftheir
RCA results,we /f_indthat MicroScopeandMicroRCAare adeptat
locating the root causes of resource issues but not good at code de-
fects. This is because MicroScope and MicroRCA only take metrics
into account, but many code defects do not manifest themselves
in metrics. The accuracy of MicroScope is lower than MicroRCA
becausethedesignofMicroScopeneverplacesthefrontendservice
as the root cause.
The two log-basedapproaches (i.e., SBLDand LogFaultFlagger)
obtain/u1D434/u1D446@1and/u1D434/u1D446@5lessthan30%.Thesetwoapproachesdonot
considermetrics.Thus,theymisstherootcausesoffaultscausedby
resources because these faults would not change the log sequences.Table 4:Comparisonofbaselines at inner-service level
ApproachOnlineBoutique TrainTicket
AIS@1 AIS@3 AIS@5 AIS@1 AIS@3 AIS@5
SBLD 14.28 17.85 17.85 15.56 22.22 24.44
LogFaultFlagger 19.64 21.42 21.42 15.56 24.44 24.44
PDiagnose 35.71 53.57 71.42 8.89 13.33 15.56
Nezhaw/oM26.78 33.92 35.71 55.56 62.22 68.89
Nezhaw/oL64.28 64.28 64.28 42.22 44.44 44.44
Nezha 92.86 96.43 96.43 86.67 97.78 97.78
The average /u1D434/u1D446@5of two trace-based approaches (i.e., MicroRank
andTraceAnomaly)reachalmost50%and40%,respectively.Both
approaches take traces as input and use the latency of spans to
/f_ind root causes. Therefore, they can only localize the faults that
have signi/f_icant impacts on latency. However, the error return and
exception faults do not manifest as anomalies in latency, which
results inlowaccuracyfor MicroRank andTraceAnomaly.
Multi-modal approach PDiagnose achieves better accuracy than
single-modalbaselinesinOnlineBoutique.However,theaccuracyof
PDiagnose degrades dramatically (from 82.14% to 22.22% at /u1D434/u1D446@5)
inTrainTicket.ThepoorperformanceofPDiagnoseisattributed
to two reasons. (1) PDiagnose transforms multi-modal data as time
seriesandlocalizesrootcausesbasedontheanomaloustimeseries.
However, not all faults cause time series anomalies, leading to
missingsomerootcauses.(2)PDiagnoseperformsRCAbasedon
a simple voting mechanism that ignores the service dependency
andanomalypropagation,whichischallengingtogetconsensus
inTrainTicket with41 microservices.
In conclusion, Nezhais eﬀective inmicroservice root cause diag-
nosisattheservicelevelandimproves /u1D434/u1D446@1by61.45%∼74.63%
and/u1D434/u1D446@5by28.51%∼73.28%onaveragecomparedtobaselines.
These results also validate our motivation to integrate multi-modal
data to facilitate root cause localizationin§ 2.2.
5.3.2 RQ2:EﬀectivenessatInner-serviceLevel. Thegroundtruthsat
theinner-servicelevelarethecoderegionorresourcetypeextracted
from the fault-injected operation. We only compare Nezhawith
SBLD, LogFaultFlagger, and PDiagnose because only these three
baselineshavetheabilitytoidentifyrootcausesattheinner-service
level. Considering that the above three baselines are designed to
pinpoint error logs rather than code regions, their result would be
determined to be correct if their output error logs are within the
code regionofroot causes.
Table4shows the eﬀectiveness of diﬀerent approaches at the
inner-service level. Nezhaperforms the best by taking all base-
linesintoconsideration,achieving /u1D434/u1D43C/u1D446@1of87%,/u1D434/u1D43C/u1D446@3of97%,
and/u1D434/u1D43C/u1D446@5of 97% on average. With the incorporation of multi-
modal data while retaining execution contexts, Nezhacan localize
root causes at the inner-service level more accurately. SBLD and
LogFaultFlaggerusethediﬀerencesinfrequencyandcoverageof
logs to locate root causes. Resource faults do not cause a signif-
icant diﬀerence in frequency and coverage of logs, so SBLD and
LogFaultFlaggercannotaccuratelyidentify theseroot causes.PDi-
agnose performs better than SBLD and LogFaultFlagger because
it considers valuable metrics and traces ignored by SBLD and Log-
FaultFlagger. However, the performance of PDiagnose degrades
dramatically in TrainTicket dataset because the voting mechanism
of PDiagnose is diﬃcult to get consensus in a system with many
561ESEC/FSE ’23, December3–9, 2023,San Francisco, CA, USA Guangba Yu, Pengfei Chen, Yufeng Li, HongyangChen, Xiaoyun Li, andZibin Zheng
10000 20000 30000 40000 50000 10 20 30 RCA Time (Second) 
Event Number / Minute  OB  TT 
Figure 11:Changeofdiagnosis timewith eventnumber.
services. To overcome the drawbacks of SBLD, LogFaultFlagger,
and PDiagnose, Nezhainserts metrics alert events into logs events
so thatNezhacan handle both the code defects and resource faults
whileretaining executioncontext.
To sum up,the results demonstrate the eﬀectiveness of Nezha
in localizing root causes at the inner-service level and improves
/u1D434/u1D43C/u1D446@1by67.47%∼74.85%and/u1D434/u1D43C/u1D446@5by53.61%∼75.96%on
averagecomparedto baselines.
5.3.3 Contribution of Multi-modal Data. We perform two ablation
studies to explore the contribution of multi-modal data, so we
derivethefollowingvariants: Nezhaw/o MLthatdropsmetrics
and logs, Nezha w/o Mthat drops metrics and Nezha w/o Lthat
drops logs. The ablation study results on service and inner-service
levelareshownatthebottomofTable 3andTable 4,respectively.
Weobservethateachdatasourcecontributestotheeﬀectivenessof
NezhabecauseNezhawith allmulti-modaldataperformsthebest.
In addition, we observe that the contributions degrees of logs
and metrics are not exactly the same in diﬀerent datasets. Nezha
w/oLis the second-best in OnlineBoutique dataset while Nezha
w/oMperformsthesecond-bestinTrainTicketdataset.Webelieve
this diﬀerence isdue to the distribution of fault types. In the On-
lineBoutique dataset, there are more resource issues, so Nezha w/o
Lperformsbetter.However,intheTrainTicketdataset,thereare
morecodedefects,so Nezhaw/o Mperformsbetter.Though Nezha
w/oMLperformstheworst in ablation studies,itdoesnot mean
thatthecontributiondegreeoftracedataislowbecausetraceisthe
core of linking logs of diﬀerent services and building event graphs.
5.3.4 Eﬀiciency of Nezha. The diagnosis time of Nezhais essential
forachievingtimelyRCA.Itdependshighlyonthesizeofeventsin
atimewindow.Toevaluatethescalabilityof Nezhawitheventsize,
weconductanexperimentonthechangesofdiagnosistimewith
theincreaseofevents.Thetimetocalculatefault-freepatternsisnot
includedinthediagnosistimebecausethepatternsinthefault-free
phase is calculated once and used multiple times. Fig. 11shows the
diagnosistimeof Nezhaunderthediﬀerenteventnumbersinatime
window. It can be seen that the diagnosis time increases linearly
with the number of events. In a time window of 50,000 events,
OnlineBoutiqueandTrainTickettake16secondsand30seconds
to determine root causes, respectively. Analysing TrainTicket data
needslongerthanOnlineBoutiquebecauseTrainTickethasalarger
number ofspans per traceand more complex dependencies. Thus,
Nezhatakes longer to traverseallspans oftraces.
5.3.5 SensitivityofNezha. Theminimumscorethreshold /u1D446/u1D450/u1D45C/u1D45F/u1D452min
is one crucial factor that may impact the performance of Nezha. As
stated in Sec. 4.3,/u1D446/u1D450/u1D45C/u1D45F/u1D452mincontrols the minimum score of patterns
thatwillbe consideredasrootcauses.Fig. 12showstheimpactof
0.2 0.4 0.6 0.8 80 84 88 92 96 Percent (%) 
Score min OB  ASI@1  ASI@3 TT  ASI@1  ASI@3 Figure 12:Impactofminimumscore threshold /u1D446/u1D450/u1D45C/u1D45F/u1D452min.
/u1D446/u1D450/u1D45C/u1D45F/u1D452minon/u1D434/u1D43C/u1D446@1and/u1D434/u1D43C/u1D446@3ofNezhaon two datasets. Smaller
/u1D446/u1D450/u1D45C/u1D45F/u1D452minusually leads to lower accuracy as some event patterns
withlowscorescausedbynormal/f_luctuationofthesystemaﬀect
the /f_inal results. According to Fig. 12,/u1D446/u1D450/u1D45C/u1D45F/u1D452min=0.67achieves the
best accuracy in both two datasets. The results also demonstrate
thatNezhaexhibitslesssensitivityto /u1D446/u1D450/u1D45C/u1D45F/u1D452minwhenitissetabove
0.6.Notethatthebestcon/f_igurationof /u1D446/u1D450/u1D45C/u1D45F/u1D452minhighlydependson
the characteristics ofdatasets.
6 DISCUSSION
6.1 LimitationsandFutureWork
Nezharelies on the anomaly detection approach triggers for RCA,
thusitcannotidentifyrootcausesforfaultsthatescapeanomaly
detection.Futureworkcanintegratemorerobustanomalydetection
algorithms (e.g., USAD [ 2]) and monitor additional metrics beyond
the P90 latency andsuccessratioto avoid missingfaults.
The applicability of Nezhais limited to troubleshooting faults
thatexhibitabnormalpatternsinmulti-modaldata.Somebyzan-
tine faults, such as returning an unreasonable result to the user,
cannot be identi/f_ied by Nezhabecause these faults do not manifest
themselves as any abnormal patterns. Troubleshooting for such
byzantinefaultslike [ 65]isthe future work to improve Nezha.
Thusifanon-changefaultisnot/f_ilteredbyme,wealsogenerate
a suspicion list for SRE to check, which increases the checking
burden on SRE. However, SRE believes that this false alarm burden
is acceptable compared to a missed alarm. In the future we will
add more pluggable components to/f_ilter outnon-changefaults as
muchas possible.
6.2 Threatsto Validity
The threats to the internal validity mainly lie in the fault-free data
collectionandminimumscorethreshold /u1D446/u1D450/u1D45C/u1D45F/u1D452min,whichcanintro-
duce bias on the eﬀectiveness of Nezha. (1) The accuracy of Nezha
canbeaﬀectediffault-freedataisnoisyorlackscertaintypesof
requests. To mitigate this threat, it is recommended to construct
fault-free data that includes a wide range of request types and has
a similar number of requests to the production phase. Capturing
workloads of systems and replaying these workloads is a common
approachinthesoftwaretestphase.SREscaneasilycollectsuch
fault-free data when replaying workloads. Even without replaying
workloads, collecting fault-free data within a short time window is
notdiﬃcultastheproductionenvironmentispredominantlyina
normalstatuswithlimitedfaults[ 30].(2)Asthepatternofasimilar
number of occurrences in the fault and fault-free phases hardly
provides a clue for RCA, the minimum score threshold /u1D446/u1D450/u1D45C/u1D45F/u1D452min
isusedtoexcludesuch uninformative patterns.We recommended
setting/u1D446/u1D450/u1D45C/u1D45F/u1D452minabove 0.6 because a pattern /u1D491is considered more
562Nezha: InterpretableFine-GrainedRoot Causes Analysis forMicroservicesonMulti-modalObservabilityData ESEC/FSE ’23, December3–9, 2023,San Francisco, CA, USA
suspiciousifthesupportof /u1D491infault-suﬀeringphaseislessthan
the support of /u1D491infault-free phase.
The external threat mainly comes from the microservice modi/f_i-
cation and experimental environment. (1) The integration analysis
ofNezhareliesontheinsertionoftraceIDintologs,whichmaynot
be available in some practical systems. However, with the standard
Opentelemetrytoolkitand examples invarious programminglan-
guages[38–41],itiseasyfordeveloperstoinserttraceandspanIDs
intothelogs.Forexample,forJavaapplications,weonlyneedto
modifyonelineofloggingpatternintheloggingcon/f_iguration[ 39].
(2)Nezhais evaluated on two widely-used microservice systems in
aKubernetes platform.Itneeds further eﬀort tovalidate the eﬀec-
tivenessof Nezhainmorecomplexreal-worldsystems.However,it
isreasonabletobelieve Nezhacanworkinsuchasystemaccord-
ingto currentresults. Thecomplexityof thesystems isalleviated
by constructing datasets that include more than 600 events for a
single request and involves parallel and asynchronous service calls.
Typically,arequestinvolvingdozensofservicecallsinindustrial
microservice systemscontains hundredsof log events [ 72].There-
fore,theamountofeventsiscomparable.Thecomplexityoffault
scenariosisalleviatedbyinjectingresourceissuesandcodedefects
from real faults in industrial systems [ 33,76] into microservices.
Thus, the faultscenarios inour evaluation are representative.
7 RELATED WORK
Metric-based RCA. Metric-based RCA approaches commonly di-
agnose problems by mining the relations between diﬀerent met-
rics[6,26,35,61,63,64].CauseInfer[ 6]andMicroscope[ 35]con-
struct acausality graphusing thePC-algorithm,andidentify root
causesbasedonthecorrelationofdiﬀerentmetricsalongthecausal
paths. MicroRCA [ 64] /f_irst extracts an anomalous sub-graph from
anattributegraphincludingservicedependenciesandperformance
metrics.ApersonalisedPageRankmethodisthenusedontheanom-
alysub-graphto locaterootcauses.MicroDiag[ 63] /f_irstderives a
metric causality graph byGrangerCausalitytests. Then itweighs
thecausalitygraphwiththepearsoncorrelationcoeﬃcientbetween
twometricsandrankstherootcauseswithPageRank.GROOT[ 60]
constructs the causality graph using monitoring events such as
performance metrics deviation events and ranks the most probable
rootcausesfromtheeventcausality graphbased onacustomized
PageRankalgorithm.Animportantdrawbackofmetrics-basedRCA
approachesisthatanalyzingmetricsprovidesasuper/f_icialanalysis
ofthesystem’soperationsbutnotadissectionofhowthesystem
isactually running.
Log-based RCA. Most of log-based RCA methods localize root
causes by comparing frequent patterns between logs of normal
and abnormal phase [ 1,11,34,36,55]. SBLD [55] applies spectrum
algorithms [ 25] to the logging domain by abstracting logs into
eventsandlocatingrootcausesbyanalyzingthecoverageofevents.
Facebookproposesafastdimensionalanalysisframeworktolocate
thefrequentitemsmostlikelytobetherootcausebyminingthe
diﬀerence between the frequent item sets of normal and abnormal
log [34]. LogCluster [ 36] uses log clustering to mine log sequences
andcomparesproductionwithtestlogsequencesto/f_indpreviously
unseen ones. However, current logs-based RCA approaches do not
takeintoaccountthecontextualinformationofrequestsandcannot
identifyfaultswhere logshave not changed.Trace-basedRCA. Variousworkandtools[ 10,48,56,57,59]
havebeenproposedtogeneratetracesbyinstrumentingtracecode
into the source code of applications. The empirical study in [ 76]
shows that microservice debugging can be improved by employing
proper tracing and visualization techniques and strategies. But the
studydoesnotprovideanautomaticRCAapproachbasedontraces.
GMTA [18] is a graph-based trace analysis approach implemented
anddeployedineBay.Itabstractstracesintodiﬀerentpathsandfur-
thergroupsthemintobusiness/f_lowsforarchitectureunderstanding
andproblemdiagnosis.Nevertheless,suchaggregatedtraceanal-
ysisapproachmaymaskasmallnumberofabnormaltracesthat
are critical to root cause localization. Based on the insight that a
microservice that is traversed by more abnormal and fewer nor-
mal traces is likely to be the root cause, T-Rank [ 66] proposes a
lightweightperformancediagnostictoolbuiltonspectrumanalysis.
However,iftwodiﬀerentmicroserviceshavesimilarcoverageinfor-
mation, T-Rank cannot distinguish between them. MicroRank [ 67]
andTraceRank[ 71]combinePageRankandspectrumalgorithms
to distinguish two diﬀerent microservices with similar coverage.
Nevertheless, the root causes output by MicroRank and TraceRank
(i.e., service instance level or operation level) are more coarse than
Nezha. Overall, traces-based RCA approaches are limited by the
granularity oftraces,which ismostly attheoperation level rather
thanthecoderegionlevel.Moreover,theabsenceofsystem-level
metricsalsomakestraces-basedapproachesimpossibletospecify
whether the faultsare causedbyresources.
8 CONCLUSION
In this study, we present Nezha, an interpretableand /f_ine-grained
RCAapproachbasedonmulti-modalobservabilitydata. Nezhauni-
/f_ies multi-modal data as events within a single solution and mines
event patterns by constructing event graphs. Nezhacorrelates and
ranks event patterns by identifying which event patterns do not
follow expected execution paths and how these patterns change
in the fault-suﬀering phase to localize root causes. We have imple-
mented a prototype of Nezhaand conducted extensive evaluations
on two widely-used microservice applications. Our results show
thatNezhaachievesahightop1accuracyatboththeserviceand
inner-service level and outperforms state-of-the-art approaches
by a large margin. Moreover, Nezhadeals with events with high
scalability,whichmakesitpractical for industrialsystems.
9 DATA-AVAILABILITYSTATEMENT
Thedataandtheimplementationof Nezhaarepubliclyavailable
at Zenodo [ 69] and Github1. The augmented OnlineBoutique and
TrainTicket are available at [ 46]and[47].
ACKNOWLEDGMENTS
We greatly appreciate the insightful feedback from the anonymous
reviewers. The research is supported by the National Key Research
andDevelopmentProgramofChina(2019YFB1804002),theNational
NaturalScienceFoundationofChina(No.62272495),theGuangdong
BasicandAppliedBasicResearchFoundation(No.2023B1515020054),
andthe CCF-LenovoBlueOcean Research Fund.
1https://github.com/IntelligentDDS/Nezha
563ESEC/FSE ’23, December3–9, 2023,San Francisco, CA, USA Guangba Yu, Pengfei Chen, Yufeng Li, HongyangChen, Xiaoyun Li, andZibin Zheng
REFERENCES
[1]Anunay Amar and Peter C. Rigby. 2019. Mining historical test logs to predict
bugs and localize faults in the test logs. In ICSE 2019 . IEEE / ACM, 140–151.
https://doi.org/10.1109/ICSE.2019.00031
[2]Julien Audibert, Pietro Michiardi,FrédéricGuyard,SébastienMarti,andMariaA.
Zuluaga. 2020. USAD: UnSupervised Anomaly Detection on Multivariate Time
Series. In KDD 2020 . ACM, 3395–3404. https://doi.org/10.1145/3394486.3403392
[3]Chetan Bansal, Sundararajan Renganathan,AshimaAsudani, OlivierMidy,and
MathruJanakiraman.2020. DeCaf:diagnosingand triagingperformance issues
in large-scale cloud services. In ICSE-SEIP 2020 . ACM, 201–210. https://doi.org/
10.1145/3377813.3381353
[4]Byteman. 2023. Java Byteman. https://github.com/bytemanproject/byteman .
Accessed Jan. 6,2023.
[5]cAdvisor.2023. cAdvisor. https://github.com/google/cadvisor . AccessedJan.6,
2023.
[6]Pengfei Chen, Yong Qi, Pengfei Zheng, and Di Hou. 2014. CauseInfer: automatic
and distributed performance diagnosis with hierarchical causality graph in large
distributedsystems.In INFOCOM2014 .IEEE,1887–1895. https://doi.org/10.1109/
INFOCOM.2014.6848128
[7]YufuChen,MengYan,DanYang,XiaohongZhang,andZiliangWang.2022. Deep
AttentiveAnomalyDetectionfor Microservice SystemswithMultimodal Time-
SeriesData.In ICWS2022 .IEEE,373–378. https://doi.org/10.1109/ICWS55610.
2022.00062
[8]Michael Chow,David Meisner,JasonFlinn,Daniel Peek, andThomasF.Wenisch.
2014. TheMysteryMachine:End-to-endPerformanceAnalysisofLarge-scale
Internet Services. In OSDI 2014 . USENIXAssociation, 217–231.
[9]Francois Doray and Michel Dagenais. 2017. Diagnosing Performance Variations
byComparingMulti-LevelExecutionTraces. IEEETPDS 28,2(2017),462–474.
https://doi.org/10.1109/TPDS.2016.2567390
[10]RodrigoFonseca,GeorgePorter,RandyH.Katz,ScottShenker,andIonStoica.
2007. X-Trace:APervasiveNetworkTracingFramework.In NSDI,2007 .USENIX,
271–284.
[11]Xiaoyu Fu, Rui Ren, Sally A. McKee, Jianfeng Zhan, and Ninghui Sun. 2014.
Digging deeper into cluster system logs for failure prediction and root cause
diagnosis.In CLUSTER2014 .IEEE,103–112. https://doi.org/10.1109/CLUSTER.
2014.6968768
[12]FudanSELab. 2023. TrainTicket. https://github.com/FudanSELab/train-ticket .
Accessed Jan. 6,2023.
[13]YuGan,MingyuLiang,SundarDev,DavidLo,andChristinaDelimitrou.2021.
Sage: practical and scalable ML-driven performance debugging in microservices.
InASPLOS 2021 . ACM,135–151. https://doi.org/10.1145/3445814.3446700
[14]GoogleCloudPlatform. 2023. OnlineBoutique. https://github.com/
GoogleCloudPlatform/microservices-demo . Accessed Jan. 6,2023.
[15]Grafana.2023. Grafanaloki. https://github.com/grafana/loki . AccessedJan.6,
2023.
[16]Grafana. 2023. Grafana promtail. https://grafana.com/docs/loki/latest/clients/
promtail/ . Accessed Jan. 6,2023.
[17]Grafana. 2023. Grafana Tempo. https://github.com/grafana/tempo . Accessed
Jan. 6,2023.
[18]Xiaofeng Guo, Xin Peng, Hanzhang Wang, Wanxue Li, Huai Jiang, Dan Ding,
Tao Xie, and Liangfei Su. 2020. Graph-Based Trace Analysis for Microservice
Architecture Understanding and Problem Diagnosis. In ESEC/FSE 2020 . ACM,
1387–1397. https://doi.org/10.1145/3368089.3417066
[19]Pinjia He, Jieming Zhu, Zibin Zheng, and Michael R. Lyu. 2017. Drain: An
Online Log Parsing Approach with Fixed Depth Tree. In ICWS 2017 . IEEE, 33–40.
https://doi.org/10.1109/ICWS.2017.13
[20]Shilin He, Qingwei Lin, Jian-Guang Lou, Hongyu Zhang, Michael R. Lyu, and
Dongmei Zhang. 2018. Identifying impactful service system problems via log
analysis.In ESEC/FSE2018 .ACM,60–70. https://doi.org/10.1145/3236024.3236083
[21]Chuanjia Hou, Tong Jia, Yifan Wu, Ying Li, and Jing Han. 2021. Diagnosing Per-
formanceIssues in MicroserviceswithHeterogeneous Data Source. In ISPA/BD-
Cloud/SocialCom/SustainCom, 2021 . IEEE, 493–500. https://doi.org/10.1109/ISPA-
BDCloud-SocialCom-SustainCom52081.2021.00074
[22]LexiangHuang and TimothyZhu. 2021. tprof:PerformancePro/f_iling viaStruc-
tural Aggregation and Automated Analysis of Distributed Systems Traces. In
SoCC 2021 . ACM,76–91. https://doi.org/10.1145/3472883.3486994
[23]ZichengHuang,PengfeiChen,GuangbaYu,HongyangChen,andZibinZheng.
2021. Sieve: Attention-based Sampling of End-to-End Trace Data in Distributed
Microservice Systems. In ICWS 2021 . IEEE, 436–446. https://doi.org/10.1109/
ICWS53863.2021.00063
[24] Istio. 2023. Istio. https://istio.io . Accessed Jan. 6,2023.
[25]James A. Jones, Mary Jean Harrold, and John T. Stasko. 2002. Visualization of
testinformationtoassistfaultlocalization.In ICSE2002 .ACM,467–477. https:
//doi.org/10.1145/581339.581397
[26]Srikanth Kandula, Ratul Mahajan, Patrick Verkaik, Sharad Agarwal, Jitendra
Padhye, and Paramvir Bahl. 2009. Detailed diagnosis in enterprise networks. In
SIGCOMM2009 . ACM,243–254. https://doi.org/10.1145/1592568.1592597[27]Kmaork. 2023. Hypno. https://docs.aws.amazon.com/prescriptive-
guidance/latest/implementing-logging-monitoring-cloudwatch/con/f_igure-
cloudwatch-ec2-on-premises.html . Accessed Jan. 6,2023.
[28]Kmaork. 2023. Hypno. https://github.com/kmaork/hypno . Accessed Jan. 6, 2023.
[29]Xing Li, Yan Chen, and Zhiqiang Lin. 2019. Towards automated inter-service
authorizationformicroserviceapplications.In SIGCOMM2019 .ACM,3–5. https:
//doi.org/10.1145/3342280.3342288
[30]XiaoyunLi,GuangbaYu,PengfeiChen,HongyangChen,andZhekangChen.2022.
Going through the Life Cycle of Faults in Clouds: Guidelines on Fault Handling.
InISSRE 2022 . IEEE,121–132. https://doi.org/10.1109/ISSRE55969.2022.00022
[31]YufengLi,GuangbaYu,PengfeiChen,ChuanfuZhang,andZibinZheng.2022.
MicroSketch: Lightweight and Adaptive Sketch Based Performance Issue Detec-
tionandLocalizationinMicroserviceSystems.In ICSOC2022 (LectureNotesin
Computer Science, Vol. 13740) . Springer, 219–236. https://doi.org/10.1007/978-3-
031-20984-0_15
[32]ZeyanLi,JunjieChen,RuiJiao,NengwenZhao,ZhijunWang,ShuweiZhang,
Yanjun Wu, Long Jiang, Leiqin Yan, and Zikai Wang. 2021. Practical Root Cause
LocalizationforMicroserviceSystemsvia TraceAnalysis.In IWQoS2021 . IEEE,
1–10.https://doi.org/10.1109/IWQOS52092.2021.9521340
[33]ZeyanLi,NengwenZhao,MingjieLi,XianglinLu,LixinWang,DongdongChang,
Xiaohui Nie, Li Cao, Wenchi Zhang, Kaixin Sui, Yanhua Wang, Xu Du, Guoqiang
Duan, and Dan Pei. 2022. Actionable and interpretable fault localization for
recurringfailuresinonlineservicesystems.In ESEC/FSE2022 .ACM,996–1008.
https://doi.org/10.1145/3540250.3549092
[34]Fan Fred Lin, Keyur Muzumdar, Nikolay Pavlovich Laptev, Mihai-Valentin Cure-
lea, Seunghak Lee, and Sriram Sankar. 2020. Fast Dimensional Analysis for Root
CauseInvestigationinaLarge-ScaleServiceEnvironment. Proc.ACMMeas.Anal.
Comput. Syst. 4,2 (2020), 31:1–31:23. https://doi.org/10.1145/3392149
[35]JinjinLin,PengfeiChen,andZibinZheng.2018. Microscope:PinpointPerfor-
manceIssueswithCausalGraphsinMicro-serviceEnvironments.In ICSOC2018 .
Springer, 3–20. https://doi.org/10.1007/978-3-030-03596-9_1
[36]QingweiLin,HongyuZhang,Jian-GuangLou,YuZhang,andXueweiChen.2016.
Logclusteringbasedproblemidenti/f_icationforonlineservicesystems.In ICSE
Companion 2016 . ACM,102–111. https://doi.org/10.1145/2889160.2889232
[37]PingLiu,HaowenXu,andetal.2020. Unsuperviseddetectionofmicroservice
trace anomalies through service-level deep bayesian networks. In ISSRE 2020 .
IEEE,48–58. https://doi.org/10.1109/ISSRE5003.2020.00014
[38]Sumo logic. 2023. Go TraceId and SpanId injection into logs con/f_iguration.
https://help.sumologic.com/docs/apm/traces/get-started-transaction-tracing/
opentelemetry-instrumentation/go/traceid-and-spanid-injection-into-logs/ .
Accessed June6,2023.
[39]Sumo logic. 2023. Java TraceId and SpanId injection into logs con/f_igura-
tion.https://help.sumologic.com/docs/apm/traces/get-started-transaction-
tracing/opentelemetry-instrumentation/java/traceid-spanid-injection-into-
logs-con/f_iguration/ . Accessed June6,2023.
[40]Sumologic.2023. JavaScriptTraceIdandSpanIdinjectionintologscon/f_iguration.
https://help.sumologic.com/docs/apm/traces/get-started-transaction-tracing/
opentelemetry-instrumentation/javascript/traceid-spanid-injection-into-logs/ .
Accessed June6,2023.
[41]Sumologic.2023. PythonTraceIdandSpanIdinjectionintologscon/f_iguration.
https://help.sumologic.com/docs/apm/traces/get-started-transaction-tracing/
opentelemetry-instrumentation/python/traceid-spanid-injection-into-logs/ .
Accessed June6,2023.
[42]ChangLou,PengHuang,andScottSmith.2020. Understanding,Detectingand
Localizing Partial Failures in Large System Software. In NSDI 2020 . USENIX
Association, 559–574.
[43]MinghuaMa,ShenglinZhang,JunjieChen,JimXu,HaozheLi,YongliangLin,
XiaohuiNie,BoZhou,YongWang,andDanPei.2021. Jump-StartingMultivariate
TimeSeriesAnomalyDetectionforOnlineServiceSystems.In USENIXATC2021 .
USENIXAssociation, 413–426.
[44]Vijayaraghavan Murali, Edward Yao, UmangMathur, and Satish Chandra. 2021.
ScalableStatisticalRootCauseAnalysisonAppTelemetry.In ICSE(SEIP)2021 .
IEEE,288–297. https://doi.org/10.1109/ICSE-SEIP52600.2021.00038
[45]KarthikNagaraj,CharlesEdwinKillian,andJenniferNeville.2012. Structured
ComparativeAnalysisofSystemsLogstoDiagnosePerformanceProblems.In
NSDI2012 . USENIXAssociation, 353–366.
[46]Nezha.2023. Augmented-OnlineBoutique. https://github.com/IntelligentDDS/
Augmented-OnlineBoutique . Accessed Jan. 6,2023.
[47]Nezha. 2023. Augmented-TrainTicket. https://github.com/IntelligentDDS/
Augmented-TrainTicket . Accessed Jan. 6,2023.
[48]Opentelemetry. 2023. Opentelemetry. https://opentelemetry.io . Accessed Jan. 6,
2023.
[49]Opentelemetry. 2023. OpenTelemetry Collector. https://github.com/open-
telemetry/opentelemetry-collector . Accessed Jan. 6,2023.
[50]LawrencePage,SergeyBrin,RajeevMotwani,andTerryWinograd.1999. The
PageRank citation ranking: Bringing order to the web. Technical Report. Stanford
InfoLab.
[51]YichengPan,MengMa,XinruiJiang,andPingWang.2021. Faster,deeper,easier:
crowdsourcingdiagnosisofmicroservicekernelfailurefromuserspace.In ISSTA
564Nezha: InterpretableFine-GrainedRoot Causes Analysis forMicroservicesonMulti-modalObservabilityData ESEC/FSE ’23, December3–9, 2023,San Francisco, CA, USA
2021. ACM,646–657. https://doi.org/10.1145/3460319.3464805
[52]Pingcap. 2023. Golang Failpoint. https://github.com/pingcap/failpoint . Accessed
Jan. 6,2023.
[53]Prometheus. 2023. Prometheus. https://github.com/prometheus/prometheus .
Accessed Jan. 6,2023.
[54]Prometheus. 2023. Prometheusnode exporter. https://github.com/prometheus/
node_exporter . Accessed Jan. 6,2023.
[55]Carl Martin Rosenberg and Leon Moonen. 2020. Spectrum-Based Log Diagnosis.
InESEM2020 . ACM,18:1–18:12. https://doi.org/10.1145/3382494.3410684
[56]BenjaminHSigelman,LuizAndreBarroso,MikeBurrows,PatStephenson,Manoj
Plakal, Donald Beaver, Saul Jaspan, and Chandan Shanbhag. 2010. Dapper, a
large-scaledistributed systems tracing infrastructure. (2010).
[57]Apache SkyWalking. 2023. Apache SkyWalking. https://skywalking.apache.org .
Accessed Jan. 6,2023.
[58]CindySridharan.2018. Distributedsystemsobservability:aguidetobuildingrobust
systems. O’ReillyMedia.
[59]Avishay Traeger, Ivan Deras, and Erez Zadok. 2008. DARC: dynamic analysis
of root causes of latency distributions. In SIGMETRICS 2008 . ACM, 277–288.
https://doi.org/10.1145/1375457.1375489
[60]Hanzhang Wang, Zhengkai Wu, Huai Jiang, Yichao Huang, Jiamu Wang, Selcuk
Kopru, and Tao Xie. 2021. Groot: An event-graph-based approach for root cause
analysis in industrial settings. In ASE 2021. IEEE, 419–429. https://doi.org/10.
1109/ASE51524.2021.9678708
[61]PingWang,JingminXu,MengMa,WeilanLin,DishengPan,YuanWang,and
Pengfei Chen. 2018. Cloudranger: root cause identi/f_ication for cloud native
systems. In CCGRID 2018 . IEEE/ACM, 492–502. https://doi.org/10.1109/CCGRID.
2018.00076
[62]Paul F Wilson,Larry DDell, and GaylorF Anderson.1996. Rootcause analysis:
atoolfortotalqualitymanagement. TheJournalforHealthcareQuality(JHQ) 18,
1 (1996), 40.
[63]LiWu, JohanTordsson, Jasmin Bogatinovski, Erik Elmroth, and OdejKao.2021.
MicroDiag: Fine-grained Performance Diagnosis for Microservice Systems. In
CloudIntelligence2021 . IEEE,31–36.
[64]LiWu,JohanTordsson,ErikElmroth,andOdejKao.2020. MicroRCA:RootCause
LocalizationofPerformanceIssuesinMicroservices.In NOMS2020 .IEEE/IFIP,
1–9.https://doi.org/10.1109/NOMS47738.2020.9110353
[65]Hiroyuki Yamada and Jun Nemoto. 2022. Scalar DL: Scalable and Practical
Byzantine Fault Detection for Transactional Database Systems. Proc. VLDB
Endow.15,7 (2022), 1324–1336.
[66]ZihaoYe,PengfeiChen,andGuangbaYu.2021. T-Rank:ALightweightSpectrum
based Fault Localization Approach for Microservice Systems. In CCGrid 2021 .
IEEE/ACM,416–425. https://doi.org/10.1109/CCGrid51090.2021.00051[67]GuangbaYu,PengfeiChen,HongyangChen,ZijieGuan,ZichengHuang,Linxiao
Jing,TianjunWeng,XinmengSun,andXiaoyunLi.2021. MicroRank:End-to-End
Latency Issue Localization with Extended Spectrum Analysis in Microservice
Environments.In WWW2021 .ACM,3087–3098. https://doi.org/10.1145/3442381.
3449905
[68]GuangbaYu,PengfeiChen,PairuiLi,TianjunWeng,HaibingZheng,andYuetang
Deng. 2023. LogReducer: Identify and Reduce Log Hotspots inKernel on the Fly.
InICSE2023 . IEEE,1763–1775. https://doi.org/10.1109/ICSE48619.2023.00151
[69]GuangbaYu,PengfeiChen,YufengLI,HongyangChen,XiaoyunLi,andZibin
Zheng. 2023. Artifact of Paper "Nezha: Interpretable Fine- Grained Root Causes
Analysis for Microservices on Multi-modal Observability Data" .https://doi.org/10.
5281/zenodo.8276375
[70]Guangba Yu, Pengfei Chen, and Zibin Zheng. 2019. Microscaler: Automatic
ScalingforMicroserviceswithanOnlineLearningApproach.In ICWS2019 .IEEE,
68–75.https://doi.org/10.1109/ICWS.2019.00023
[71]Guangba Yu, Zicheng Huang, and Pengfei Chen. 2021. TraceRank: Abnormal
service localization with dis-aggregated end-to-end tracing data in cloud native
systems. JournalofSoftware:EvolutionandProcess (2021),e2413. https://doi.org/
10.1002/smr.2413
[72]Chenxi Zhang, Xin Peng, Chaofeng Sha, Ke Zhang, Zhenqing Fu, Xiya Wu,
Qingwei Lin, and Dongmei Zhang. 2022. DeepTraLog: Trace-Log Combined
Microservice Anomaly DetectionthroughGraph-based DeepLearning. In ICSE
2022. IEEE,623–634. https://doi.org/10.1145/3510003.3510180
[73]ChenxiZhang,XinPeng,TongZhou,ChaofengSha,ZhenghuiYan,YiruChen,
and Hong Yang. 2022. TraceCRL: contrastive representation learning for mi-
croservice trace analysis. In ESEC/FSE 2022 . ACM, 1221–1232. https://doi.org/10.
1145/3540250.3549146
[74]YingyingZhang,ZhengxiongGuan,HuajieQian,LeiliXu,HengboLiu,Qingsong
Wen, Liang Sun, Junwei Jiang, Lunting Fan, and Min Ke. 2021. CloudRCA: A
Root Cause Analysis Framework for Cloud Computing Platforms. In CIKM 2021 .
ACM,4373–4382. https://doi.org/10.1145/3459637.3481903
[75]NengwenZhao,JunjieChen,ZhaoyangYu,HonglinWang,JiesongLi,BinQiu,
Hongyu Xu, Wenchi Zhang, Kaixin Sui, and Dan Pei. 2021. Identifying bad
software changes via multimodal anomaly detection for online service systems.
InESEC/FSE ’21 . ACM,527–539. https://doi.org/10.1145/3468264.3468543
[76]Xiang Zhou, Xin Peng, Tao Xie, Jun Sun, Chao Ji, Wenhai Li, and Dan Ding.
2021. Fault Analysis and Debugging of Microservice Systems: Industrial Survey,
Benchmark System, and Empirical Study. IEEE TSE 47, 2 (2021), 243–260. https:
//doi.org/10.1109/TSE.2018.2887384
Received 2023-03-02; accepted 2023-07-27
565