See discussions, st ats, and author pr ofiles f or this public ation at : https://www .researchgate.ne t/public ation/376101512
Testing Coreference Resolution Systems without Labeled Test Sets
Conf erence Paper  · No vember 2023
DOI: 10.1145/3611643.3616258
CITATIONS
3READS
40
4 author s, including:
Jialun Cao
Hong K ong Univ ersity of Scienc e and T echnolog y
44 PUBLICA TIONS    343 CITATIONS    
SEE PROFILE
Ming Wen
Huazhong Univ ersity of Scienc e and T echnolog y
74 PUBLICA TIONS    2,321  CITATIONS    
SEE PROFILE
All c ontent f ollo wing this p age was uplo aded b y Jialun Cao  on 21 Januar y 2025.
The user has r equest ed enhanc ement of the do wnlo aded file.TestingCoreferenceResolutionSystemswithoutLabeledTestSets
JialunCao
The Hong Kong Universityof Scienceand Technology,
GuangzhouHKUST Fok YingTungResearchInstitute
Hong Kong, China
jcaoap@cse.ust.hkYaojieLu
Chinese InformationProcessing Laboratory,
Institute of Software, Chinese Academy of Sciences
Beijing, China
luyaojie@iscas.ac.cn
Ming Wen
Schoolof CyberScienceand Engineering,
HuazhongUniversityof Scienceand Technology
Wuhan, China
mwenaa@hust.edu.cnShing-Chi Cheung∗
The Hong Kong Universityof Scienceand Technology,
GuangzhouHKUST Fok YingTungResearchInstitute
Hong Kong, China
scc@cse.ust.hk
ABSTRACT
Coreferenceresolution(CR)isatasktoresolvediﬀerentexpressions
(e.g.,namedentities,pronouns)thatrefertothesamereal-worlden-
tity/event.Itisacorenaturallanguageprocessing(NLP)component
that underlies and empowers major downstream NLP applications
suchasmachinetranslation,chatbots,andquestion-answering.De-
spiteitsbroadimpact,theproblemoftestingCRsystemshasrarely
been studied. A major diﬃculty is the shortage of a labeled dataset
fortesting.Whileitispossibletofeedarbitrarysentencesastest
inputs to a CR system, a test oracle that captures their expected
test outputs (coreference relations) is hard to de/f_ine automatically.
To address the challenge, we propose Crest, an automated testing
methodology for CR systems. Crestuses constituency and depen-
dencyrelations to construct pairs of test inputs subject to the same
coreference.Theserelationscanbe leveragedto de/f_inethemeta-
morphicrelationformetamorphictesting.Wecompare Crestwith
/f_ivestate-of-the-arttestgenerationbaselinesontwopopularCR
systems, and apply them to generate tests from 1,000 sentences
randomlysampledfromCoNLL-2012,apopulardatasetforcorefer-
enceresolution.Experimentalresultsshowthat Crestoutperforms
baselines signi/f_icantly. The issues reported by Crestare all true
positives ( i.e., 100% precision), compared with 63% to 75% achieved
bythe baselines.
CCS CONCEPTS
•Softwareanditsengineering →Consistency ;Softwarede-
fect analysis ;•Computing methodologies →Information
extraction .
KEYWORDS
Coreference resolutiontesting,Metamorphictesting,SE4AI
∗Corresponding author
Permissionto make digitalor hard copies of allorpart ofthis work for personalor
classroom use is granted without fee provided that copies are not made or distributed
forpro/f_itorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation
onthe/f_irstpage.Copyrights forcomponentsofthisworkownedbyothersthanthe
author(s)mustbehonored.Abstractingwithcreditispermitted.Tocopyotherwise,or
republish,topostonserversortoredistributetolists,requirespriorspeci/f_icpermission
and/or a fee. Request permissions from permissions@acm.org.
ESEC/FSE ’23, December 3–9, 2023, San Francisco, CA,USA
©2023 Copyright heldby the owner/author(s). Publicationrightslicensed to ACM.
ACM ISBN 979-8-4007-0327-0/23/12...$15.00
https://doi.org/10.1145/3611643.3616258ACMReference Format:
Jialun Cao, Yaojie Lu, Ming Wen, and Shing-Chi Cheung. 2023. Testing
CoreferenceResolutionSystemswithoutLabeledTestSets.In Proceedingsof
the31stACMJointEuropeanSoftwareEngineeringConferenceandSymposium
on the Foundations of Software Engineering (ESEC/FSE ’23), December 3–9,
2023, San Francisco, CA, USA. ACM, New York, NY, USA, 13pages.https:
//doi.org/10.1145/3611643.3616258
1 INTRODUCTION
Coreference resolution (CR) is a core natural language processing
(NLP)taskthatresolvesareal-worldentity/eventtowhichapro-
noun or phrase in a text refers [ 20,27,56,80]. The /f_irst example in
Figure2illustratesanapplicationofCR,resolvingthepronoun him
toJohn,andshereferstoSally.CRbringscohesionandcoherent
presentation style to natural language expressions [ 20], and also
permeates manyaspectsof daily life.
CR isanessentialcomponentthat empowersmanydownstream
NLPapplications.Thesecondto/f_ifthexamplesinFigure 2illustrate
theuseofCRresultsinfourtypicalNLPapplications.Example2
shows a user talking with her AI assistant Siri, asking it to send
a message to John. Siri needs to resolve the pronoun ItoSally
andhimtoJohnbefore correctly sending John a message. The
GoogleTranslateinExample3needstoresolvethepronoun itto
the funeral before correctly translating the English itto French
ellein the feminine singular form. The question-answering (QA)
system in Example 4 needs to resolve the pronoun shetoSally
in the context in order to return the correct answer. A recent
study even pointed out that ChatGPT[ 58] is limited in handling
coreference [ 41]. Indeed, various applications such as question-
answering (QA) [ 48], machine translation [ 67], chatbots [ 12,36,
51,77],andautomaticsummarization[ 3,42]performpredictions
basedonthe CR results.
Given the wide impact of CR systems, their quality assurance is
crucialtotheperformanceofNLPapplications.Arecentstudy[ 80]
benchmarkedexistingCRsystemsandrevealedthepotentialbias
in the reported evaluation of these systems. In particular, the eval-
uation is based on the commonly-used dataset ( i.e., CoNLL-2012
dataset[56]),onwhichexistingCRsystemsareexpectedtohave
been tuned. As such, high precision and recall are reported. The
resultsarebelievedtobesatisfactoryinthisdataset,whiletheevalu-
ationresultsarediﬀerentwhenthesesystemsaretestedusingother
datasets or those that involve infrequent tokens. The precision and
107
ESEC/FSE ’23, December3–9, 2023,San Francisco, CA, USA Jialun Cao, YaojieLu,Ming Wen, Shing-Chi Cheung
•Sentence 2.The fish in thelakeate the worm because itwas hungry .•Sentence 1.The fish inthelakeate the worm because itwas tasty.
ThefishNP
IN
lakeNN
theDT
clean in wormthe istastyPRPNPS
it becauseINSBAR
ateVBDNP
NP DTVP NNPPVP
NP
NN§DependencyStructure§CoreferenceResolution
§ConstituentStructure
DT
JJ
JJVBZADJPS
Figure1:IllustrationofCoreferenceResolution,Dependency
Structure,andConstituentStructure.
recallcandropto66.5%and72.4%respectively.Thestudyindicates
the needfor adiversetest setto evaluate CR systems’reliability.
Indeed, a de/f_icient CR system can greatly jeopardize the per-
formance of its downstream applications [ 67]. As pointed out by
Google, Microsoft, and many other companies, an unreliable CR
system has been one of the major reasons for the unsatisfactory
performanceof downstreamapplications such as knowledge graph
construction[ 33]andneuralmachinetranslation[ 67].Forinstance,
Example 5 in Figure 2shows a dialog where a chatbot failed to
resolve the pronoun himtoJohn. It repeatedly sought clari/f_ication
aboutthe message’sreceiver,resultinginapooruserexperience.
Metamorphictesting[ 11]isreportedtobethemostpopulartest-
ingapproachforarti/f_icialintelligencesystems[ 45].Metamorphic
testingrelieves(1)theconstructionofatestoraclebetweenaninput
and its output, which is diﬃcult for stochastic computation, and
(2)alabeledtestset,whichisexpensivetocollect.Nopriorwork
hasstudiedthedeploymentofautomatedmetamorphictestingto
CR systems despitetheir impact.Suppose Fis the programunder
test;F(/u1D465)andF(/u1D465′)are the program outputs based on an input /u1D465
and a follow-up input /u1D465′. The deployment of metamorphic testing
needs to address three key problems: ( P1) de/f_ining an eﬀective
metamorphicrelation(MR) R(/u1D465,/u1D465′,F(/u1D465),F(/u1D465′))forCRsystems,
(P2) generating a follow-up input /u1D465′that observes the de/f_ined MR
from a source input /u1D465and its outputF(/u1D465), and (P3) devising a
mechanismto checkif F(/u1D465)andF(/u1D465′)violateRautomatically.
P1.To address the /f_irst problem, a natural MR is that the CR
system should produce consistent results for a source input and its
follow-upinputwhen both share the same coreference.
P2.The MR requires constructing a follow-up input that pre-
serves the coreference of its source input. The construction is,
however, challenging. First, existing word/token replacement tech-
niques[22,24,47,64] do notguarantee the preservationof corefer-
ence after replacement. Indeed, the coreference in a sentence could
be easily changed with the replacement of one word. For example,
in Figure 1, with one adjacent word ( i.e.,hungryandtasty), the
coreferenceof itischangedfrom the/f_ishtotheworm .Also,apart
fromword replacement,adversarial techniquescould also be used
for data augmentation (typo insertion or random character dele-
tion) [47,64]. Yet, the design of these techniques cannot guarantee
the coreference that should be preserved. Second, coreference may
also be changed by replacing a word with its synonyms/antonyms.
Forinstance,considerthesentence“ ThecitycouncilmenrefusedtheSally: HiSiri.
Siri:Hello. How canIhelp you?
Sally:I’dliketosend amessage toJohn .
Siri :Sure. What message?
Sally: Please tellhimcome tohave thecakeIbaked together.
Siri :OK.Message hasbeen sent toJohn .Sally toldJohn to tasteherhomemade cake whenhecomes home.
*Clusters (text): [[Sally, her], [John, he],]
* Clusters (index range): [[0, 1], [5, 6]], [[2, 3], [9, 10]]]Example1.
Coreference
Resolution
Example2.
Chatbot *
Sally :Iwant tosend amessage toJohn .
Robot: Sure. What message?
Sally :Please tellhimcome tohave thecake withme.
Robot: Copy that. Buttowhom should the message besent?
Sally :……Example5.
Chatbot with
anill-trained
coreference
resolver *
* Thelines depicted thecoreference relations for“I-I”and“I-me”were omitted forsimplicity.0 1 2 3 4 5 6 7 8 9 10 11 12
Example4.
Question
AnsweringContext :Sally toldJohn to taste the cakeshemade.
User :Who hasbaked acake?
Robot :Sally.English :The funeral of the Queen Mother will take place on 
Friday. Itwill be broadcast live.
French :Les funérailles de la reine mère auront lieu vendredi . 
Ellesera retransmise endirect. (byGoogle Translate)Example3.
Machine
Translation
Figure 2: Examples of Coreference Resolution and Its Down-
streamApplications.
demonstratorsapermitbecausethey[feared/advocated]violence .”[75]
Ifthewordis“ feared”,then“they”likelyrefersto“ Thecitycoun-
cilmen”; whereas if it is “ advocated ”, then “they” likely refers to
“the demonstrators ”. Third, as revealed by our study (see Section 4),
existing semantic similarity metrics [ 16,62] are incapable of distin-
guishingwhether twosentences have the same coreference.
P3.ValidatingtheconsistencyoftworesultsmadebyCRsystems
canbetricky.Intuitively,directlycheckingtextconsistencycould
leadto ambiguity [ 49]. For example,considering the sentence:
•TheQueenMotherasked QueenElizabeth II [1]totransform her[1]
sister,Princess Margaret [2],intoaviableprincessbysummoning
a renownedspeech therapist, NancyLogue, to treat her[2]speech
impediment. [49]
Inthisexample,therearetwopronouns herreferringtodiﬀerent
referents( i.e.,QueenElizabethII andPrincessMargaret ,respectively).
Sosimplydeterminingtheconsistencyoftextscouldleadtofalse
negatives. Alternatively, if we use the clusters of index range (as
shown in Figure 2Example 1) to determine the consistency, the
indexcouldeasilyshiftduetowordreplacement.Forexample,if
we replace the word homemade with its synonym home-baked , the
index range of hewould shift from [9,10]to[11,12], because
the synonym home-baked will be tokenized to three tokens, so the
indices of home-baked are[6],[7],[8], respectively,pushing the
index ofhimforward to [11]. In other words, simply checking the
absoluteequivalenceofeithertextorindexrangecouldresultin
imprecise judgements.
Inthispaper,weintroduceametamorphictestingframework,
Crest, for CR systems. Our key insight is that replacing tokens
thatarerelatedtothecoreferenceinthetextislikelytochangethe
coreference of the text. Thus, accordingly, we generate the follow-
up inputs by mutating the phrases or tokens that are unrelated
to the coreference in the text. In particular, we identify the CR-
related tokens/phrases according to the syntactic information ( i.e.,
constituentstructureanddependencystructure)ofthetext,then
generatethefollow-upinputsbyreplacingtokensintherestofthe
108TestingCoreference ResolutionSystems without LabeledTestSets ESEC/FSE ’23, December3–9, 2023,San Francisco, CA, USA
sourceinput.Forexample,forSentence1inFigure 1,afteridentify-
ingthecoreferenceinthetext( i.e.,theworm andit)andparsingthe
textto itsdependencyandconstituentstructures, wecanidentify
thatnotonlythetokens/phrases theworm andit,butalsotheadjec-
tivetasty,whichdependsontheprecedingnominalsubject( nsubj)
itfromthedependencystructure,areCR-relatedtokens/phrases.
Replacingthesetokens/phrases is highly likely to aﬀect the text’s
coreference. So, we leave these tokens/phrases unchanged. Instead,
we replace the rest tokens/phrases in the source input. Speci/f_ic
to this example, it is safe to replace the tokens in the phrase in
the clean lake without aﬀecting the coreference. After the word
replacement, we also check the consistency of the constituency of
theCR-relatedtokens/phrases,ensuringthatthewordreplacement
would not change the coreference accidentally. Finally, to validate
theconsistencyoftheoutputs,wechecktheclustersofindexrange
andthecorrespondingclustersoftextsatthesametime.Also,to
betterquantifytheoutputconsistency,wecalculatetheprecision
andrecallusingapopular-usedlink-basedmetricBLANC(BiLateral
Assessment ofNoun-phrase Coreference)[ 61], so that the desired
thresholdscanbecustomizedfor/f_inerconsistencyrequirements.
In summary,our work makesthree majorcontributions:
•Weintroduce Crest,ametamorphictestingmethodologyforCR
systems.Inparticular,weformulatethetestingproblemonCR
systems,demonstratethechallengesoftestingthem,anddevelop
afeasible methodology to addressthe challenges.
•We propose a metamorphic relation for testing CR systems. It
addresses the diﬃculty in automatically constructing test inputs
duringgeneratingfollow-upinputsinaidofsyntaxanalysis.A
subsequenttest selection process further ensures thequalityof
test inputs.
•The experiment reveals the eﬀectiveness of Crest. It reaches
100% precision in issue detection, outperforming baselines (with
at best 74%). Moreover, regarding the quality of test inputs, only
63%∼79%oftestinputsgeneratedbybaselinesarevalid,lead-
ingtohighfalsepositiverates.Incomparison,100%testinputs
generatedby Crestarevalidandareabletorevealtruepositives.
2 PRELIMINARIES
2.1 Technology andTerminology ofCR
CR is a fundamental task and a long-researched area in the NLP
areatowardsnaturallanguageunderstanding[ 27,56].Toautomate
CR tasks, a variety of techniques have been proposed, including
deterministic[ 4,17,38,50,59,60],statistical[ 13,23]andneural[ 14,
15]CRsystems.Besides,recentend-to-endtechniques[ 39,40]have
madeabigstepbymakinguseofthepre-trainedlanguagemodel
(PLM) [34], external world knowledge [ 2] and eﬃcient learning
algorithms [ 35,78]. Yet, as a recent study [ 80] has pointed out that
theperformanceofstate-of-the-artCRsystemsisstillunsatisfactory
when infrequent test setsare used.
AdesiredCRsystemisexpectedtoidentifyalltheexpressions
(e.g.,names,pronouns,phrases)thatco-referringtoan entity/event
in the given text [ 5,27,49]. These expressions are known as men-
tions.Thementionsaregroupedinto clusters1accordingtothe
1Thereareotheralternativenamesfor clusters,including coreferencechains andsubjects.
In thispaper, weuse clusterconsistently.entity/eventstheyreferto.Allthementionsinoneclusterareex-
pected to refer to the same entity/event. For instance, in the text
“The/f_ishinthelakeatethewormbecauseitwastasty. ”,aCRsystem
isexpectedto/f_indallmentions{ The/f_ish,thelake,theworm,it}from
thetext.Next,theCRsystempartitionsthemintosingletons{ the
/f_ish} and {the lake}, and a cluster { the worm ,it}, and then outputs
the three clusters.2
2.2 OutputCluster ConsistencyChecking
Givenatextsequence X=[/u1D4651,/u1D4652,...,/u1D465/u1D441],amention /u1D45A=(/u1D460,/u1D452),
where/u1D460and/u1D452are, respectively, the start and end token position of
/u1D45Asuch that /u1D460≤/u1D452≤/u1D441. Let/u1D450denote a cluster{/u1D45A1,/u1D45A2,...,/u1D45A/u1D457},
where/u1D457≥2. Given a CR system F:X→C, it takes a textXas
input, and outputs a set of clusters C(={/u1D4501, ...,/u1D450/u1D45E}), where /u1D45E≥1.
Take Example 1 in Figure 2as an example, mentions include[2,3]
(i.e.John) and[9,10](i.e.he), the cluster of these two mentions is
[[2,3], [9,10]] .
Forclusters /u1D4501and/u1D4502,wesay/u1D4501and/u1D4502areconsistent if∀/u1D45A∈
/u1D4501,∃/u1D45A′∈/u1D4502such that /u1D45A≡/u1D45A′AND∀/u1D45A′∈/u1D4502,∃/u1D45A∈/u1D4501such that
/u1D45A′≡/u1D45A. Note that the equivalence relation ( i.e.,≡) of mentions
describes the equivalence beyond the exact index equivalence. For
instance,fortheexampleinFigure 2Example1,althoughtheindex
ofheisshiftedfrom [9]to[11]afterreplacing homemade tohome-
baked, these two mentions are still equivalent because they both
use the same pronoun ( i.e.,heto refer to the same entity John).
To evaluate the coreference consistency, we then adopt BLANC
(BiLateral Assessment of Noun-phrase Coreference) [ 43,61], a
popular-used metric, to calculate the precision and recall of the
coreference.Formally,suppose Gisthegroundtruthcoreferenceof
the given inputXconsisting of a set of clusters, Cis the predicted
coreference ofX. Let/u1D43F/u1D454and/u1D43F/u1D450represent the sets of coreference
linksinthe ground truthclusters Gandpredictedclusters C.
/u1D443=|/u1D43F/u1D454∩/u1D43F/u1D450|/|/u1D43F/u1D450|, /u1D445=|/u1D43F/u1D454∩/u1D43F/u1D450|/|/u1D43F/u1D454| (1)
We considerCiscorrectif both P and R are higher than the
customizedthresholds.Oppositely,ifanyofPandRlowersthan
the threshold,an issueisdetected.
2.3 ConstituentandDependency Structures
Constituent and dependency structures are two ways to analyze
the syntactic structure of sentences using constituency [ 85] and
dependency parsing [ 9] respectively. The constituent structure rep-
resents each sentence as a hierarchical phrase tree, where each
noderepresentsaconstituent[ 71],suchasnounphrases(NP),verb
phrases(VP),andprepositionalphrases(PP).Forexample,inFig-
ure1,themainconstituentsofthesentenceare{ The/f_ishintheclean
lake,ate the worm ,because it is tasty }. Speci/f_ically, the noun phrase
(NP)‘The /f_ish’ is the subjectof thesentence, and the prepositional
phrase(PP) ‘inthecleanlake’ modi/f_iesthenounphrase ‘The/f_ish’,
‘ate the worm’ is the verb phrase (VP) of the sentence with ‘ate’as
the verb and ‘the worm’ as the direct object, and ‘because it is tasty’
is the subordinate clause (SBAR) that explains the reason for the
/f_ish eating the worm.
The dependency structure represents a sentence as a directed
graphwiththewordsasnodesanddependencytypesasedges[ 52].
2Most CR systems do not output singletons. So in the evaluation, we assume each
output clusterof the CR systemcontains at least twomentions.
109ESEC/FSE ’23, December3–9, 2023,San Francisco, CA, USA Jialun Cao, YaojieLu,Ming Wen, Shing-Chi Cheung
Each dependency represents the relationship between a headword
and its dependents, which are words that depend on the headword
for their meaning. As shown in Figure 1,/f_ishdepends on the verb
ate,wormisthedirectobjectoftheverb ateanddependsonit, clean
modi/f_ieslakeanddepends onit, becauseintroducesa subordinate
clause and depends on ate,tastyis an adjective that modi/f_ies worm
anddepends onit.
3 APPROACHAND IMPLEMENTATION
This section introduces the work/f_low of Crestand describes its
implementation. The input of Crestis a list of unlabeled texts,
and the output is a list of suspicious CR issues. For each input,
Crestgenerates a set of follow-up inputs, pairs each follow-up
inputwithitssourceinput,andreportsanissueifanyinconsistency
isfound.Areportedissuecontainsasourceinput,afollow-upinput,
and the coreference of them resolved by the CR system under test.
Notethattheissuemayoccurinthecoreferenceofthesourceinput,
the coreference of the follow-up input, or both. Figure 3illustrates
the overviewof Crest.setCrestcomprisesthree main steps:
•Follow-upInputGeneration :Foreachsourceinput, Crestgen-
erates a set of follow-up inputs by modifying a single token at a
time whilepreservingthe coreference.
•Follow-upInputSelection :Crestfurtherselectsthegenerated
texts according to the syntactic information of the source and
follow-upinputs.
•Inconsistency Detection : The predicted coreference of the
sourceand follow-upinputsare compared.Ifthe predictionsare
inconsistent, Crestreports an issue.
3.1 Follow-up Input Generation
Crestdesignsthemethodologyfollowingageneralwork/f_lowof
metamorphictesting. To startwith,it generates aset of follow-up
inputs from a source input according to the MR. The MR in this
work is that a CR system should produce consistent results for the
sourceandfollow-upinputswhentheysharethesamecoreference.
Thefollow-upinputcanbesoconstructedthatitdiﬀersfromthe
sourceinputbyatleastonetoken.However,mosttokenreplace-
mentmechanismsinNLPtestingapproaches[ 7,22,24,30,69]do
notconsideriftherearecoreferencechangesinthefollow-upin-
puts. The follow-up inputs constructed in this way may not satisfy
theimmutabilityrequirementofthecoreferenceintheMR.Though
several approaches consider coreference [ 64,66], they manually
de/f_ine several templates ( e.g.,[He/She] is a[doctor] ) to generate
tests. While these approaches are limited in diversifying the gener-
atedsentencestructures,itisimpracticaltode/f_inetemplatesforall
sentence structures.
Itmotivatesustodesignagenerationalgorithm(Algorithm 1),
which observes the immutability requirement of coreference as
follows. The input of the algorithm includes a source input X(e.g.,
asentence),thegoldencoreference Cinthedataset,theCRsystem
under testF, and the maximum number of generated follow-up
inputs/u1D440.Givena source input X(e.g.,a sentence) and thegolden
coreferenceC, at the beginning, the source input is tokenized and
parsedtothedependencytreeandconstituenttree(lines2-3),ob-
tainingtheconstituentlabelsandthedependencyrelationsofto-
kens. Examples of parsed constituent structure and dependencystructure are illustrated in Figure 1. Then, according to theidenti-
/f_iedcoreferenceandthedependencystructure,weobtaintheindex
oftokensthatarerelatedtothecoreference(line4,seenextpara-
graph for details). Then, each token Xlooks up its replacement
iteratively unless the token is coreference-related (lines 6-7). For
each candidate token, we /f_ind the corresponding synonym and
antonym sets (lines 10-11) using WordNet [ 46]. Furthermore, in
ordertoincreasethediversityofthegeneratedfollow-upinputs,
we alsoinvolve oﬀ-the-shelfmaskedlanguagemodelstoperform
word perturbation. In particular, we mask the corresponding token
in the original sentence one by one and use a pre-trained language
model to predict a possible token as a replacement (line 12)3. After
obtainingasetofwordsthatcanbereplaced,wethentrytoreplace
itinorder to generateasetof follow-upinputs(line13).
Speci/f_ically, to obtain the index of coreference-related tokens
(Algorithm 1, line 4), we utilize dependency structures of texts.
Givenasourceinput X,itscoreferenceCanddependencystructure
/u1D451/u1D452/u1D45D/u1D461,ititerativelyobtainsclustersin C,andbreaksdowntoeach
mention in every cluster. For every mention, we add all the indices
of the mention, as well as that of the phrases/tokens that have
dependencyrelationswiththeseindices.Theformalequationfor
obtainingthe setofCR-relatedindices( N) isshownas follows:
N={/u1D457|∃/u1D456∈M,∀/u1D457∈{0,1,...|/u1D44B|}, /uni210E/u1D44E/u1D460/u1D437/u1D452/u1D45D/u1D452/u1D45B/u1D451/u1D452/u1D45B/u1D450/u1D466.alt(/u1D456, /u1D457)}(2)
where/u1D457iterates all the indices in the sentence /u1D44Bwith |X| tokens,
Mdenotes the set of indices of tokens in the coreference, /u1D456de-
notes any index in M,/uni210E/u1D44E/u1D460/u1D437/u1D452/u1D45D/u1D452/u1D45B/u1D451/u1D452/u1D45B/u1D450/u1D466.alt(/u1D456, /u1D457)represents a function
which checks whether there are dependency relations4between
the indices /u1D456and/u1D457, which returns a boolean value. We add the
index/u1D457into the returned set Nif there are dependency relations
with/u1D456and/u1D457. Speci/f_ically, for the dependency relations, we con-
sidernsubj(i.e., nominal subject, a nominal which is the syntactic
subject and the proto-agent of a clause [ 52]) andamod(i.e., adjecti-
valmodi/f_ier,theadjectivalphrase/wordthatservestomodifythe
noun/pronoun [ 52]) of pronouns in the sentence, because they are
two majordependency relations thatrelate to noun/noun phrases.
Finally,we return asetof indices N.
Furthermore,afteratokenthatcouldbeusedforreplacement
is identi/f_ied, the word replacement should go with a double check
utilizing the constituent structure. In particular, the procedure
works as follows (see Algorithm 3). Given a set of candidate to-
kens/u1D45F/u1D452/u1D45D/u1D459/u1D44E/u1D450/u1D452/u1D451/u1D446/u1D452/u1D461 (i.e.,synonymsorantonyms),thesourceinput X,
theindexoftokenunderreplacement /u1D456,theconstituentstructureof
X/u1D450/u1D45C/u1D45B/u1D460/u1D461,andthemaximumreplacementtimesofthegivenposition
(i.e.,/u1D456)/u1D440, the algorithm deals with every candidate token in the
/u1D45F/u1D452/u1D45D/u1D459/u1D44E/u1D450/u1D452/u1D451/u1D446/u1D452/u1D461 onebyone.Afterreplacingthe /u1D456-thtokeninthesource
textwitheverycandidatetoken(line6),itparsesthenewtextto
its constituent structure (line 7) to check whether the candidate
texthasthesameconstituentlabelasthe /u1D456-thtokeninthesource
text (line 8). If the label remains the same, then the new text is
regardedasafollow-upinputandaddedintotheoutputset.This
3In the implementation, we use a popular model ( i.e.,bert-large-cased ) to perform
word perturbation and to ensure the naturalness of the sentence, we set the minimal
probabilityofeachcandidatewordas 0.1.Onecouldeasilyreplacethelanguagemodel
with more advanced models and set up diﬀerent thresholds. Since it is not the core of
ourmethodology, wefollowthe same settingas adopted by the existingwork[ 69].
4In the implementation of Crest, we use the dependency parser provided by Stan-
fordCoreNLP[ 44].
110TestingCoreference ResolutionSystems without LabeledTestSets ESEC/FSE ’23, December3–9, 2023,San Francisco, CA, USA
Generate
Dependency 
Parsing
Constituent
StructureDependency
Structure
Source
InputCR
system
Original 
Coreference
Constituency 
Parsing……
Follow -up
Input 1
New 
Coreference
Report An Issue if Any 
Inconsistency is Found
•Source Input
•Original Coreference
•Follow -upInput
•New Coreference
Constituent
Structure 1
Follow -up Input Generation Follow -up Input Selection
 Inconsistency Detection
NKeep?Y
Compare coreference -related
tokens (✪)inconstituent structure
Discard the
follow -upinputParse✪
✪Coreference -
related Tokens
CRSyste m
Figure 3: Work/f_low of Crest.➊Given a source input ( e.g., a sentence), Crest/f_irst generates a set of follow-up inputs ( e.g.,
sentences that are similar to the source input while diﬀering in a few tokens) in the /f_irst phase. ➋The follow-up inputs are fed
tothe secondphase, validatingtheirsyntactic propertyconsistencywith thesourceinput.Afollow-upinputifconsistentis
pairedwithitsconcernedsourceinputtoformatest;otherwisediscarded. ➌Anissueisdetectedandreportediftheconsistency
oftheoutputcoreferenceisbelow aprede/f_ined threshold.
Algorithm1: generateFollowUpInput
Input:AsourceinputX,thegoldencoreference CofX,CR
systemundertestF,the number ofmaximum
generatedfollow-upinputs /u1D440
Output:A setofgeneratedfollow-upinputs /u1D45B/u1D452/u1D464/u1D44B/u1D460
1/u1D45B/u1D452/u1D464/u1D44B/u1D460←emptySet()
2/u1D451/u1D452/u1D45D/u1D461←dependencyParser( X)
3/u1D450/u1D45C/u1D45B/u1D460/u1D461←constituentParser(X)
4/u1D450/u1D45F/u1D456/u1D45B/u1D451←getCrfRelatedIndex( C,/u1D451/u1D452/u1D45D/u1D461)
5for/u1D456in 0...|X|do
6if/u1D456in/u1D450/u1D45F/u1D456/u1D45B/u1D451then
7 continue
8else
9 /u1D450/u1D44E/u1D45B/u1D451/u1D456/u1D451/u1D44E/u1D461/u1D452/u1D460←emptySet()
10 /u1D450/u1D44E/u1D45B/u1D451/u1D456/u1D451/u1D44E/u1D461/u1D452/u1D460 .add (getSynonym(X,/u1D456))
11 /u1D450/u1D44E/u1D45B/u1D451/u1D456/u1D451/u1D44E/u1D461/u1D452/u1D460 .add (getAntonym( X,/u1D456)
12 /u1D450/u1D44E/u1D45B/u1D451/u1D456/u1D451/u1D44E/u1D461/u1D452/u1D460 .add (getWordPerturb( X,/u1D456)
13 /u1D45B/u1D452/u1D464/u1D44B/u1D460.add(replaceAndCheck( /u1D450/u1D44E/u1D45B/u1D451/u1D456/u1D451/u1D44E/u1D461/u1D452/u1D460 ,X,/u1D456,
/u1D450/u1D45C/u1D45B/u1D460/u1D461,/u1D440))⊳See Algorithm 3
14return/u1D45B/u1D452/u1D464/u1D44B/u1D460
checkensuresthereplacementdoesnotchangetheconstituencyof
the token under replacement. The replace-and-check iterates until
thenumberofgeneratedfollow-upinputsreachesthemaximum
number/u1D440(lines3-4)or all tokensin thecandidate sethave been
replaced(line10).
3.2 Follow-up Input Selection
We further analyze the generated follow-up inputs and exclude
thosethatarenotlikelytopreservetheirsourceinput’scoreference.
Yet,checkingwhethercoreferencehasbeenchangedisasdiﬃcultas
asserting whether it is correct. Checking the coreference manually
is,however,notaviablesolutionbecauseitcanbesubjectiveandnot
scalable.Ourinsighttotacklethisproblemisinspiredbyrule-based
CRworks[ 38,59,60],whichleveragethedepthofthetoken/phrase
in a syntactic tree to determine discourse prominence [ 28] and
resolve coreference. Therefore, we check if the coreference-related
tokens in a follow-up input’s constituent structure have a depth
that diﬀers from that of their counterparts in the source input’sstructure.Ifso,thefollow-upinputisunlikelytopreserveitssource
input’s coreference andshould be excluded.
Inparticular,amentioninaclustercouldbeeitherasingletoken
(e.g.,him,it),oraphrase( e.g.,the/f_ish,ExecutivesatBackerSpielvogel
clientAvisInc. ).Sowhencalculatingthedepthofcoreference-related
tokens/phrases,wetreateachmentionaccordingly.Ifamentionisa
singletoken,wecalculatethedepthfromtheroottothistokeninthe
constituentstructure.Whileifamentionisaphrase,wecalculate
the depth from the root to the closet nested phrase outside the
mentionphrase.Forexample (seeFigure 1ConstituentStructure),
forthesingletoken it,thedepthis5( S - SBAR - S - NP - PRP ),
while for the mention the /f_ish, its depth is calculated from the root
to the nestednoun phrase( S - NP - NP ), resultingin3.
Algorithm 2works as follows. Given a source input Xand a
follow-upinputX′,thealgorithmdetermineswhetherthefollow-
upinputshouldbeexcludedornot.Ifthecoreference-relatedtokens
residediﬀerentlyintwoconstituencystructures,thenthefollow-up
inputshouldbeexcluded. Speci/f_ically,the algorithmiteratesevery
mention in clusters of the coreference in X(line 1-2). If a mention
appearinginXalsoexistsinthefollow-upinput,then Crestparses
X′toobtainitsconstituentstructure(line3-4).Thenitcalculates
the depth of the token or the closet phrase that nests the mention
intheconstituent structures ( /u1D450/u1D45C/u1D45B/u1D460/u1D461and/u1D450/u1D45C/u1D45B/u1D460/u1D461′)ofbothXandX′
(line5-6).Ifthedepthvaries,thenwediscardthefollow-upinput
(line7-9).Otherwise,ifnomentionsbreaktheconditiontilltheend
ofthe iteration, we then keep this follow-upinput(line9).
3.3 Inconsistency Detection
After the follow-up inputs are selected, each of them are paired
withitssourceinput.Foreachpairofsourceinputandfollow-up
input (X,X′), atestis conductedby(1)obtainingthe coreference
ofXresolved by the CR system under test F(X), (2) obtaining the
coreference ofX′resolved by the CR system under test F(X′),
and(3)checkingif F(X)andF(X′)areconsistent.Thetestfails
if inconsistencyoccurs.
Yet, validating the coreference consistency of two outputs by
a CR system is tricky because there can be more than one equiv-
alent token/phrase in a sentence. Thus, we use the token indices
to represent the mentions in coreference to avoid ambiguity. For
instance, for the examplein Section 1P3, instead of using the text
herto represent themention, we use [9,10]to represent the/f_irst
herreferring to Queen Elizabeth II , and use [31,32]to represent
111ESEC/FSE ’23, December3–9, 2023,San Francisco, CA, USA Jialun Cao, YaojieLu,Ming Wen, Shing-Chi Cheung
Algorithm2: selectFollowUpInput
Input:A sourceinputX,ageneratedfollow-upinput X′,
the constituent structure /u1D450/u1D45C/u1D45B/u1D460/u1D461,the coreferenceC
Output:A boolean valueindicating whether the generated
follow-upinputshould be kept ornot
1for/u1D450/u1D459/u1D462/u1D460/u1D461/u1D452/u1D45FinCdo
2for/u1D45A/u1D452/u1D45B/u1D461/u1D456/u1D45C/u1D45Bin/u1D450/u1D459/u1D462/u1D460/u1D461/u1D452/u1D45Fdo
3 if/u1D45A/u1D452/u1D45B/u1D461/u1D456/u1D45C/u1D45BinX′then
4 /u1D450/u1D45C/u1D45B/u1D460/u1D461′←ConstituentParser( X′)
5 /u1D451/u1D452/u1D45D/u1D461/uni210E←calDepth( /u1D45A/u1D452/u1D45B/u1D461/u1D456/u1D45C/u1D45B,/u1D450/u1D45C/u1D45B/u1D460/u1D461)
6 /u1D451/u1D452/u1D45D/u1D461/uni210E′←calDepth( /u1D45A/u1D452/u1D45B/u1D461/u1D456/u1D45C/u1D45B,/u1D450/u1D45C/u1D45B/u1D460/u1D461′)
7 if/u1D451/u1D452/u1D45D/u1D461/uni210E!=/u1D451/u1D452/u1D45D/u1D461/uni210E′then
8 returnFalse
9returnTrue;
Algorithm3: replaceAndCheck
Input:A sourceinputX,the index ofthe token under
replacement /u1D456,asetofcandidate tokens that could
be usedfor wordreplacement /u1D45F/u1D452/u1D45D/u1D459/u1D44E/u1D450/u1D452/u1D451/u1D446/u1D452/u1D461 ,the
constituent structure /u1D450/u1D45C/u1D45B/u1D460/u1D461,andthe maximum
number offollow-upinputs /u1D440
Output:A setoffollow-upinputs /u1D45B/u1D452/u1D464/u1D44B/u1D460
1/u1D45B/u1D452/u1D464/u1D44B/u1D460←emptySet()
2for/u1D45B/u1D452/u1D464/u1D447in/u1D45F/u1D452/u1D45D/u1D459/u1D44E/u1D450/u1D452/u1D451/u1D446/u1D452/u1D461 do
3iflen(/u1D45B/u1D452/u1D464/u1D44B/u1D460≤/u1D440)then
4 return/u1D45B/u1D452/u1D464/u1D44B/u1D460
5else
6 /u1D45B/u1D452/u1D464/u1D44B←replaceToken(X,/u1D456,/u1D45B/u1D452/u1D464/u1D447)
7 /u1D45B/u1D452/u1D464/u1D436/u1D45C/u1D45B/u1D460/u1D461←constituentParser( /u1D45B/u1D452/u1D464/u1D44B)
8 if/u1D45B/u1D452/u1D464/u1D436/u1D45C/u1D45B/u1D460/u1D461 (/u1D45B/u1D452/u1D464/u1D447)==/u1D450/u1D45C/u1D45B/u1D460/u1D461(X[/u1D456])then
9 /u1D45B/u1D452/u1D464/u1D44B/u1D460.add(/u1D45B/u1D452/u1D464/u1D44B)
10return/u1D45B/u1D452/u1D464/u1D44B/u1D460;
the second herreferring to Princess Margaret . The use of indices
allowseachmentionto be uniquely referenced..
Finally,Crestdetectsinconsistencyasfollows.Givenafollow-
up inputX′,CrestfeedsX′to the CR system to obtain the output
coreferenceC′. To avoid ambiguity of text comparing, Crestuses
token indices to represent mentions in coreference. Then, an in-
consistency is detected if C′diﬀers fromC.Crestdetermines
inconsistencyaccordingtotheircomputedBLANC(BiLateralAs-
sessment of Noun-phrase Coreference) [ 61] score, a widely-used
link-basedmetricforcoreferencemeasurement.IftheBLANCscore
is less than 1.00, an inconsistency is detected. Crestreports the
sourceXand follow-up inputs X′of the detected inconsistency,
togetherwiththeircoreference CandC′.
4 EVALUATION
Fourresearchquestions(RQs) are designedto evaluate Crest:
•RQ1.Areissuesreportedby Cresttruepositives? Crestde-
tects issues of CR systems when the metamorphic relation is
violated.Weinvestigatehowlikelyanissuereportedby Crestisa truepositive. We showthe numberoftests generated, detected
issues, and the true positive rates of each approach. We also
report the overheadandgeneralizabilityof Crest.
•RQ2.Canfollow-upinputsgeneratedby Crestpreserve
thecoreferencewell? Sincethemetamorphicrelationin Crest
assumesthegeneratedfollow-upinputspreservethecoreference
as the source inputs, we thus check whether this assumption
holdswell.Wecomparethenumberoffollow-upinputsthatpre-
servedornotpreservedcoreferenceforeachapproach,andpoint
outthatthehighfalsepositiveratescouldlargelybeattributed
to the coreference-changedfollow-upinputs.
•RQ3.Whatkindsofcoreferenceissuescan Crestreveal?
Weanalyzetheissuesfoundby Crestandcategorizedtheminto
six types. We explain each issue type with examples reported by
Crestandillustrate the statisticsof the reportedissues.
•RQ4. What is the impact of each step in the Crest?We
conduct experiments using diﬀerent setups and thresholds to
showthe impact ofeachstep inthe designedmethod.
4.1 EvaluationSetup
We implement Crestin Python, and conduct experiments on a
MacBook Pro with 2.3 GHz Quad-Core Intel Core i7 CPU, 2.3 GHz,
16GBmemory.Foreachsourceinput,wesetthemaximumnumber
ofgeneratedfollow-upinputsforeachgivensourceinputto20.One
could enlarge this number to /f_ind more inconsistencies. Moreover,
we set the threshold for both precision and recall of BLANC ( i.e.,
/u1D461/uni210E/u1D45F/u1D45Dand/u1D461/uni210E/u1D45F/u1D45F) to1.0.
Dataset. WeuseCoNLL(theConferenceonNaturalLanguage
Learning)-2012 [ 56] for evaluation because it is the most widely
usedevaluationbenchmarkforcoreferenceresolution[ 80].5This
dataset could also be used for other natural language processing
taskssuch as semanticrolelabeling,name entityrecognition,and
so on. We use the English annotation in the dataset for evalua-
tion.Speci/f_ically,CoNLL-2012providesthetrainingset(22,965sen-
tences),thevalidationset(2,791sentences),andthetestset(2,894
sentences) separately. In evaluation, source inputs are randomly
selected from the test set. In addition, in this paper, we refer to
thegroundtruthprovidedbythedatasetannotatedfollowingthe
OntoNotes [ 55] or determine the correct coreference following the
same annotation standard.
Baselines. For comparison, since there is no available testing
approach for CR system, we adapt three recent testing approaches,
SIT [24], PatInv [ 22] and CAT [ 69], for other NLP tasks ( i.e., neural
machinetranslation)asbaselines.Theseapproachesareselected
becausetheyalsogeneratetextsbyreplacingafewtokensofthe
original text, and pairing the generated and the original texts to
compare their output results. In particular, SIT [ 24] generates new
texts using BERT by replacingone noun and adjective token with
itssynonyms.CAT[ 69]considersthecontextwhenreplacingthe
synonyms, and replaces tokens regardless of the part of speech.
PatInv[22],ontheotherhand,replacestokenswithnon-synonyms,
aiming to diversify the generated texts. Besides, for behavioral
correctnessandrobustness,CheckList[ 64]andTextAttackgenerate
5Thereareintra-andcross-sentencecoreferenceannotations.Thediﬀerenceiswhether
thementionsofcoreferencecrossthesentenceornot.Inourevaluation,weusethe
intra-sentence coreference data to conduct the evaluation for simplicity, while our
methodologyisapplicableto both typesof coreference.
112TestingCoreference ResolutionSystems without LabeledTestSets ESEC/FSE ’23, December3–9, 2023,San Francisco, CA, USA
textsby transformationsuch astypoinsertion ( e.g., replacing his
withhi sbyaddingaspace)andchangingnamedentities.Wealso
includethemasbaselines.Inparticular,forCheckList,weuseallthe
transformation methods provided except negation. This is because
the metamorphic relation may no longer hold after negating the
originaltext’ssemanticmeaning.ForTextAttack,whichbundles
variousadversarialattackanddataaugmentationtechniques,we
excludethosemethodsthathavebeenusedinCheckListtoavoid
duplication,includingrandomcharacterdeletionandaugmentation
techniques based on word embedding. For comparison, we run
these baselines based on their artifacts using their default settings.
CR System Under Test. We evaluate the eﬀectiveness of
CrestontwoCRsystems.Oneisaneural-basedCRsystempro-
posed by Clark and Manning [ 14,15]. For implementation, we use
the NeuralCoref implementation provided by spaCy [ 29]6. The
other is a mention-ranking statistical CR system [ 13], using the
implementation provided by Stanford’s CoreNLP library [ 44]7. For
both systems,we use the defaultcon/f_igurations.
4.2 RQ1. Eﬀectiveness ofIssue Revealing
To evaluate the eﬀectiveness of Crest, we randomly sample1,000
sentencesfromthetestsetofCoNLL-2012,andadoptvariousap-
proachestogeneratefollow-upinputsbasedonthesesourceinputs.
For each source input, we set an upper bound of 20 to limit the
numberofgeneratedfollow-upinputsforfairness.Amongthe1,000
source inputs, 535 of them are correctly resolved using spaCy [ 29]
withthe NeuralCoref algorithm, while465ofthemare not.
The number of generated follow-up inputs (# Gen), the num-
ber of detected issues (# Issue) are shown in Table 1. We can see
that the number of issues generated and detected by Crestalmost
doubles than those by most of the baselines. In particular, the base-
lines/f_indthousandsofissues(488 ∼7,348).AlthoughTextAttack
reports the most issues (7,348), nearly 40% of them are false alarms.
Two factors contribute to the inferior performance of the baselines.
First,somebaselinesrestrictthecandidatereplacementtokensto
speci/f_ic part-of-speech tags, and such a restriction limits the num-
ber of test input candidates that can be generated. For example,
SITconsidersnounsandadjectivesforreplacement,whilePatInv
mainly considers verbs and adverbs for replacement. Second, some
baselinesadoptastricttestinputselectioncriterion.Forexample,
CATrequireshighsyntacticandsemanticsimilaritybetweenthe
source and generated sentences, thus excluding many sentences
that are valid test inputs. Note that it is true that adjusting the
parameters in baselines may probably yield more follow-up inputs
whileexploringbetterparametersofbaselinesisnotourmaingoal,
thus we leave itfor future exploration.
Fortherecallevaluation,itisimpracticaltogothroughallthe
source and follow-up inputs. So, we check the hits based on source
inputs,i.e., whetherthe465wrongly-resolvedsource inputscould
bereportedbyanyofthepairsthattheseinputsinvolve.Theresult
isshowninTable 1“#(%)Hits”.Itshowsthatthebaselinesrecall230
to463ofthewrongly-resolvedsourceinputs,withanaverageof349.
Cresthits 324 wrongly-resolved source inputs, which approaches
theaverage. Besides,itisnoteworthythatTextAttackachievesa
6spaCy Nerualcoref: https://spacy.io/universe/project/neuralcoref
7Stanford CoreNLP: https://stanfordnlp.github.io/CoreNLP/coref.htmlTable 1:Eﬀectiveness of IssueRevealing on NeuralCoref
Test Generation Error Detection
# Src(P+N) # Gen # Issues # (%)Hits Time # TP # FP Prec
SIT1,000(535+ 465) 10,488 2,338 230 / 465 (49.46) 0.02 1,652 686 0.71
PatInv 1,000(535+ 465) 9,445 1,881 167 / 465 (35.91) 0.44 1,398 483 0.74
CAT1,000(535+ 465) 16,640 2,197 425 / 465 (91.4) 0.96 1,531 666 0.73
Checklist 1,000(535+ 465) 3,744 488 334 / 465 (71.83) 0.01 350 138 0.72
TextAttack 1,000(535+ 465) 20,000 7,348 463/465(99.57) 0.274,6482,656 0.64
Crest1,000(535+ 465) 13,635 1,457 324 / 465 (69.68) 0.21 1,457 0 1.00
hit rate of 99.57%. Yet, a high hit rate could be trivially achieved
by raising alarms for every test. Therefore, we further evaluate the
baselines and Crestby examining the number of true positives
andfalsepositivesofthedetected issuesto assess its eﬀectiveness
intermsofprecision.Inparticular,atruepositivemeansthatthe
pairofsentencesreportedbytheapproachhasatleastoneerrorin
thepredictedcoreference.Afalsenegativemeansthereportedpair
ofsentences turnoutto have nocoreference errors.
Table1“Error Detection” shows that the precision (Prec) of
baselinesrangesfrom64%to74%ascomparedto100%achievedby
Crest.Itisclearthatthefalsepositiveratesofbaselinesarehigh.
Especially,thehighhitrateachievedbyTextAttackisatthecost
of low precision (64%). Essentially, the unsatisfactory results are
attributed to the change of the coreference during follow-up input
generation, makingthe inconsistencyafalse alarm.
Wefurtherelaborateonthedetectedissuesusingthreeexamples.
First, we use an issue on NeuralCoref revealed by Crestafter
replacingitwithsynonyms.InExample6,afterreplacing /f_irstwith
its synonym initiative, no coreference inthe follow-up input such
constructedcan be identi/f_ied. Crestreports itas an issue.
Example 6. An issue found by Crestby synonym replace-
ment.(Replace: /f_irst→initiative)
∨Just before Christmas break, therural township of Linpien,
onthePingtung Plain at themouth ofthe Linpien river[1],
heldits[1]</f_irst>ever WaxApple Festival.
×JustbeforeChristmasbreak, the ruraltownship ofLinpien, onthe
PingtungPlainat themouth ofthe Linpien river,[1ismissed]held
its[1ismissed]<initiative >ever WaxApple Festival.
Alternatively,ifwereplaceacoreference-unrelatedtokenwith
its antonym, the coreference should also preserve. As shown in
Example 7, after replacing diﬀerentwith an antonym same, the
coreferenceinthetextisstillpreserved.However,theCRsystem
under test fails to identify the second cluster in the text. Crestre-
ports an issueaccordingly.
Example 7. An issue found by Crestby antonym replace-
ment.(Replace: diﬀerent→same)
∨We[1]havehearda <diﬀerent >signofSirPaulMcCartneyplay-
ing to enthusiastic crowds in Red Square, and we[1]have been en-
thralledbythecelebrationsof yourown city[2]inPetersburg[2].
×We[1]have heard a <same>sign of Sir Paul McCartney play-
ing to enthusiastic crowds in Red Square, and we[1]have been
enthralled bythecelebrationsofyour own city [2ismissed]in
Petersburg[2ismissed].
The third example is a false positive reported by a baseline.
In Example 8, without considering the coreference during word
replacement,SITsubstitutes heartforpair,whichisamentionin
the coreference cluster. In that case, the CR system fails to identify
anycoreferencefromthesecondsentence,thusaninconsistencyis
113ESEC/FSE ’23, December3–9, 2023,San Francisco, CA, USA Jialun Cao, YaojieLu,Ming Wen, Shing-Chi Cheung
Figure 4:Analysison Test QualityandDistribution ofInconsistencies.
reported.Yet,itisactuallyafalsealarm,becausethesubstitution
changesthecoreference,notonlytheco-referringentity,butalso
themultiplicity,where theheartinsingularformcannotbereferred
to bythemunlikethepair.
Example 8. Afalsealarm (Replace: pair→heart)
∨She asks anyone who /f_inds the<pair>[1]to givethem[1]back.
∨Sheasksanyone who/f_inds the <heart>to givethem back.
In addition, an issue could be induced by the incorrect coref-
erence of the source input, the follow-up input, or both, so we
furthercategorizetheinconsistenciesreportedbyeachapproach
onNeuralCorefaccordingtowheretheissuesoccur.Theanalysisis
conductedonthe100pairssampledfromeachbaseline.Figure 4(A)
showsthedistribution.Theblackbars( Source)showthenumber
of inconsistencies in which the coreference of the source inputs is
wronglyresolved,whilethecoreferenceofthefollow-upinputis
correct.Likewise,thegraybars( Follow-up )meansthenumberof
inconsistencies where the coreference of the follow-up input goes
wrong.Thesilverbars( Both)meanbotharewrong.Whilethewhite
bars (False Alarm ) show the number of false positives. Figure 4(A)
shows that the Sourcecontributes less than the Follow-up , meaning
that more issues are revealed only by the follow-up inputs than
bythesourceinputsintheevaluation.PatInvgeneratesthemost
issue-revealing follow-up inputs (36), followed by our work (34).
Also,apartfromourwork,thenumberoffalsepositivesreported
bybaselinesishigh.We analyze the reason inSection 4.3.
Overhead . We present the overhead of generating follow-up
inputs for each approach. As shown in Table 1(Time), the average
timetogenerateafollow-upinputrangesfrom0.01to0.96seconds.
On the one hand, an approachcould be quick if it only /f_inds and
replaces wordsusing patternmatching orrules. Onthe otherhand,
theoverheadcomesmainlyfromthesyntacticandsemanticanal-
ysis. For example, it takes CAT around 1 second to generate one
follow-up input because CAT /f_ilters them by checking whether the
substitutedtokensaresemanticallysimilartotheoriginaltokens.
Similarly, Cresttakes around 0.27 second to generate a follow-up
inputbecause itrequires syntactic andcoreference analysis.
Generalizability . To investigate if the results can be gener-
alized to other CR systems, we repeat the previous experiments
ofCreston a popular statistical CR system [ 13]. The results are
shown in Table 2. Among 1,000 source sentences, 635 of them
canbecorrectlyresolvedbystatisticalCR,while365cannot.The
number of generated follow-up inputs is similar to that in Table 1.
Regardingthenumberofhits,21.37%to99.45%incorrectlyresolved
source inputs are included in the inconsistencies detected. Besides,
Crestoutperforms baselines signi/f_icantly in precision, reachingTable 2:Eﬀectiveness of IssueRevealing on StatisticCR
TestGeneration Error Detection
# Src (P+N) # Gen # Issues # (%)Hits Time # TP # FP Prec
SIT 1,000(635+365) 10,488 2,174 143 / 365 (39.18) 0.07 1,422 752 0.65
PatInv 1,000(635+365) 9,445 1,410 78 / 365 (21.37) 0.51 1,059 351 0.75
CAT 1,000(635+365) 16,640 2,513 324 / 365 (88.77) 1.09 1,697 816 0.68
Checklist 1,000(635+365) 3,744 479 199 / 365 (54.52) 0.03 312 167 0.65
TextAttack 1,000(635+365) 20,000 7,470 363/ 365(99.45) 0.364,596 2,874 0.62
Crest 1,000(635+365) 13,625 1,357 232 / 365 (63.56) 0.46 1,357 0 1.00
100%precision,comparedwith75%atbestreachedbybaselines.The
experimentshows thegeneralizabilityof Crestinhighprecision
inissuerevealingonboth neuralandstatisticalCR systems.
To explore why Crestmisses some wrongly resolved source
sentences,weincreasetheupperboundoffollow-upinputsto50,
butnomorehitsarefound(seeTable 3).We/f_indtworeasonsfor
thissituation.First,theCRsystemisincapabletoresolveortendsto
ignorecertaintokens( e.g.,rarewords)orstructures( e.g.,Sentences
with noun phrases with long attributives). For such defects, it is
potential to use these sentences as templates for training data aug-
mentation in order to /f_ine-tune the CR system. Second, the span of
output coreference diﬀers from that in the ground truth. For exam-
ple,onementioninapredictedcoreferenceis powersuppliers while
thegroundtruthis powersuppliers in the west .Thediﬀerencesin
output spans may arise from the diﬀerent boundary annotations
thatCR systemsadopt [ 84].For such defects, one may either stick
to an annotation standard or relax the criteria on inconsistency
detection ( e.g.,lower the thresholds for recallorprecision).
4.3 RQ2. Quality ofFollow-up Inputs
Wefurtheranalyzethequalityoffollow-upinputsbycomparing
the number of follow-up inputs that preserve the coreference as
the source input. In particular, since the results on the two CR
systems are similar,inthe following,we illustrate the analysison
NeuralCoref due to space limitation. The full experiment results
can be foundonline[ 6].
The result is shown in Figure 4(B). The gray bars represent
thenumberoffollow-upinputsthatpreservetheirsourceinputs’
coreference,whilethewhitebarsmeantheopposite.Wecansee
that the baselines only preserve 68% to 82% among all the gener-
ated follow-up inputs. In comparison, 100% of the follow-up inputs
generatedby Crestpreservethecoreference,indicatingthehigh
/f_idelity ofthe test inputsgeneratedby Crest.
Althoughsomebaselines checkcertainconstraints suchasse-
manticsimilaritywithinthegivencontexts[ 69]andsyntacticin-
variants [ 24], they do not consider the preservation of coreference
in test input generation. PatInv [ 22] aims to generate dissimilar
text, which could be eﬀective when the generated text preserves
thecoreference.Baselinesusingadversarialattacks,however,could
114TestingCoreference ResolutionSystems without LabeledTestSets ESEC/FSE ’23, December3–9, 2023,San Francisco, CA, USA
easilychangecoreferenceinthetext.Forexample,transformations
like random character deletion ( e.g., perturbing theytothe) and
typo insertion ( e.g., changing histotheir country toheir country
change coreference in the text, leading to invalid follow-up inputs.
We further analyze whether coreference changes in a text are
correlated to the changes in its semantics or naturalness. The anal-
ysishelpsustoanswerwhetherexisting approachesdesignedfor
the similarity of semantics or preservation of naturalness could
be deployed togenerate eﬀective test inputs for CR systems. Note
that in order to better measure the semantic similarity at a /f_iner
granularity,weadoptthelatesttool[ 30]tocapturethesubtlese-
mantic diﬀerence between twotexts.In particular,for naturalness
measurement,we adopt the following equation [ 30]:
/u1D441/u1D44E/u1D461/u1D462/u1D45F/u1D44E/u1D459/u1D45B/u1D452/u1D460/u1D460(/u1D44B)=|/u1D44B|/radicaltp/radicalvertex/radicalbt|/u1D44B|/productdisplay.1
/u1D456=1/u1D443(/u1D44B/u1D456|/u1D44B∖/u1D456) (3)
whichmeasurethenaturalnessofsentence Xinlengthof|/u1D44B|tokens.
X/u1D456represents the /u1D456-th token in the sentence X, and/u1D44B∖/u1D456represents
the tokens exceptthe /u1D456-th token.
ThestatisticsareshowninFigure 4(C). Thesemanticsimilarity
rangessimilarlyregardlessofwhetherthecoreferenceischanged.
In particular, the average similarity between coreference-changed
pairs is0.972, while that between coreference-preserved pairs is
0.969,whichisquitecloseto 0.972,withonly 0.003diﬀerence.Also,
the similarity between coreference-changed pairs is a bit higher
thanthecounterpart,indicatingthatahighsemanticsimilaritydoes
notnecessarilyleadtoahighpossibilitytopreservethecoreference.
Thus,itisimpracticaltodistinguishwhetherthecoreferencehas
been changedusing semantic similarity.
Similarly, we also show the change in naturalness to investigate
whether the change in language /f_luency could provide hints to
detectcoreferencechanges.AsshowninFigure 4(D),therangesof
coreference-changed ( 0.000to0.106) and -preserved ( 0.00to0.133)
arequiteclose,averagingat 0.0316and0.0313,respectively.Such
subtlediﬀerencesinnaturalnesscouldnotbeusedtodistinguish
whetherthecoreferenceischanged.Ouranalysissuggeststhattest
generationtechniques basedoncoreference are needed.
HumanEvaluation. Tobetterevaluatethesoundnessof Crest,
weconductedahumanstudyonthegeneratedfollow-upsentences.
We published thetaskonProli/f_ic[ 57], acrowd-sourcingplatform
that connects researchers with study participants for online re-
searchstudies.Werecruited/f_iveparticipantsandsetthenationsof
participants within the USA, UK, Australia, Canada, and Singapore
in order to ensure the participants are native speakers. We set /f_ive
participants following existing patterns [6]. In addition, we also
invited two experiencedand professional English communication
tutors.Inparticular,oneisanexperiencedtrainerinLinguisticsand
Methodology,andtheotherhasover30-yearexperienceinteaching
English.Toselectthesentencestobelabeled,werandomlysampled
100 pairs of original and follow-up sentences from the 13,689 pairs
(inTable1)generatedby Crestfor labeling.8
ThelabelingresultsareshowninFigure 5(B).Eachbarrepresents
thestatisticsfromoneparticipant. CT1andCT2refertotwoEnglish
8Toavoidrepeatedlyselectingthesameoriginalinputsentence,We/f_irstsample100
original sentences, then randomly select one follow-up from all the follow-ups for
eachoriginalsentence.communication tutors, User1-5 refer to /f_ive participants hired
from Proli/f_ic. The result (the dashed line in Figure 4(B)) shows
that more than half (66.7%) of the source sentences are considered
morenaturalthantheirfollow-upcounterparts,whichisreasonable
becausethesourcesentencesaretakenfromnewsletters,Wikipedia,
andotherreadingresourcesthatprofessionalwritersauthored.The
resultsalsoshowthat33%ofthefollow-upsentencesareconsidered
equallyormorenaturalthantheoriginalsentences.Indeed,7.7%of
thefollow-upsentencesareconsideredtobemorenaturalthanthe
sourcesentences.Furtheranalysisoftheresultrevealsthatonly21%
sourcesentencesareconsideredconsistentlybyalltheparticipants
to be more natural than their follow-ups. In other words, most
follow-upsentences(79%)areconsiderednaturalorequallynatural
byatleastoneparticipant.Thestudydemonstratesthenaturalness
ofthegeneratedfollow-upsentences,thusindicatingthesoundness
ofour methodology.
4.4 RQ3. Issues Reportedby Crest
Crestiscapableto/f_indCRissuesofdiversetypes.Inourevaluation,
Crest/f_inds six major types of CR issues from NeuralCoref [ 14],
includingSpanError(SE),MissingEntity(ME),ExtraEntity(EE),
MissingMention(MM),ExtraMention(EM),andCon/f_latedEntities
(CE). The issue types are derived from the error types of CR sys-
tems[37].Toeasetheunderstanding,weprovidethecorresponding
examples ofthe uncoveredissuesineachtype.
4.4.1 ExtraEntity(EE). IfaCRsystemidenti/f_iesanextraentity( i.e.,
cluster)inthegiventext,itisanExtraEntityissue.Theexample
below shows that a CR system identi/f_ies additional co-referring
clusterthe artsandthe art. Though they look similar, they are
referringto diﬀerentabstract concepts.
Example 9. AnExample ofExtraEntity (EE)
×Thecongressman [1]wasknownasaferventliberalandsupporter
ofthearts[2×]andhe[1]wasnotedfor his[1]successingetting
congress to /f_inancefornationalendowmentfor theart[2×].
4.4.2 MissingEntity(ME). Oppositeto ExtraEntity ,MissingEntity
describes an issue where the entity should have been identi/f_ied yet
ismissed.Examplesare omitteddueto spacelimitation.
4.4.3 ConflatedEntity(CE). IfaCRsystemmistakenlymergestwo
ormoreclustersofentitiesintoone,aCEissueoccurs.SeeExample
10,there are three clusters of entitiesin thesentence, while in the
output clusters, the CR system mistakenly merges the second and
the thirdclusterstogether,causing aCEissue.
Example 10. AnExample ofCon/f_lated Entity (CE)
∨TheScotts[1]ran it past their[1]vet[2]andthe vet[2]assured
them[1], as long as the hen[3]lefther[3]crate to do business,
she[3]wasnohurtto the pup.
×TheScotts[1]ran it past their[1]vet[2]andthe vet[2]assured
them[1], as long as thehen[2×]lefther[2×]crate to do business,
she[2×]wasnohurtto the pup.
4.4.4 ExtraMention (EM). Ifaclustercontainsoneormore men-
tionsthatarenotco-referringtotheentityastheremainingmen-
tionsinthatcluster,itisregardedasanExtraMentionissue.Forex-
ample,inthefollowingsentence,thename Tanshuianditspronoun
115ESEC/FSE ’23, December3–9, 2023,San Francisco, CA, USA Jialun Cao, YaojieLu,Ming Wen, Shing-Chi Cheung
(A)Numbers of Issue Types Found by Each Approach. (B)HumanEvaluation Result s
Figure5:BarChartsofErrorTypeDistribution(A)andNatu-
ralnessofGenerated Sentences (B).
herare correctly identi/f_ied, while the giant beast of development is
unrelatedto Tanshui,letalone referringto it.
Example 11. AnExample ofExtraMention(EM)
×LoversofTanshui[1],whilekeepingwatchover her[1],arefearful
thatthegiantbeast of development[1×]may eventuallycome
and bulldozeTanshui [1]away one pieceofland at atime.
4.4.5 MissingMention(MM). OppositetoEM,ifanymentionin
anyclusterismissing,thenitisaMissingMentionissue.MMissues
usuallyhappeninlongsentenceswheretherearemorethantwo
mentions referringto one entity/event.
4.4.6Span Error (SE) .ASE issuedescribes the situation where
the span of the identi/f_ied mention is larger or smaller than the
ground-truth mention. For example, in Example 12, the ground
truthCRis{ suchafeverchip,it },whiletheCRsystemidenti/f_iesthe
wrongspan for the /f_irstmentionas follows.
Example 12. AnExample ofSpan Error(SE)
∨However,Panpointsoutthattheusefulnessof sucha feverchip [1]
is still limited so that it [1]cannot yet bewidely promoted.
×However,Panpointsout that theuseful ofofsucha fever
chip[1×/u1D446/u1D45D/u1D44E/u1D45B /u1D438/u1D45F/u1D45F/u1D45C/u1D45F]is still limited, so that it[1∨]cannot yet be
widely promoted.
Furthermore,weshowthestatisticsofissuetypesfoundbyeach
approach in Figure 5(A). Notethat an incorrect coreference could
beattributedtomorethanoneissuetype,andthecoreferenceof
both source and follow-up inputs could be wrong. From the /f_igure,
we can see that Crestcould reveal issues in diverse types, and the
number of each issue type keeps a better balance than baselines.
On the contrary, baselines tend to reveal more Missing Mention
(MM) and Extra Mention (EM) issues than other issue types and
are ineﬀective inrevealingCon/f_latedEntity (CE)issues.
Besides, Divided Entity [ 37] could not be revealed by any of the
approaches. The reason is that this issue type usually occurs in
document-level coreference ( i.e., the coreference across sentences),
whileweusetheintra-sentencecoreference( i.e.,thecoreference
occurs within asentence)for evaluation.
4.5 RQ4. ImpactofEach Step
There are three main steps in Crest. To explain the impact of each
step,we conduct the following experiments.
4.5.1ImpactofStep1(Follow-upInputGeneration). The/f_irst
stepofCrestistogeneratefollow-upinputsofagivensourceinput.
Inthisstep,theparameter /u1D440prescribesthemaximumnumberof
follow-up inputs for one source input. To quantify the importanceTable 3: Impact of Steps 1 and 2 of Crest. The con/f_iguration
/u1D440meansthenumberofmaximumgeneratedfollow-upin-
puts (inAlgorithm 1) in Step1of Crest. The con/f_iguration
‘Filter’ denotes whether Step 2 of Crestis conducted ( w/) or
not(w/o).Thevaluesthataresensitivetothecon/f_iguration
are highlighted inyellow.
Con/f_iguration Test Generation Error Detection Coref-Preserving (%)
/u1D440Select #Gen #Issues #(%)Hits #TP #FP Prec (%)
10 w/ 7,947 1,314 324 /465 (69.68) 1,314 0 100.00 7,947 /7,947 (100.00)
20 w/ 13,635 1,457 324 /465 (69.68) 1,457 0 100.00 13,635/13,635(100.00)
30 w/ 16,611 1,488 324 /465 (69.68) 1,488 0 100.00 16,611/16,611(100.00)
50 w/ 18,698 1,519 324 /465 (69.68) 1,519 0 100.00 18,698/18,698(100.00)
10 w/o 9,112 1,755 397 /465 (85.38) 1,61114491.79 7,947/9,122(87.12)
20 w/o 15,630 1,965 397 /465 (85.38) 1,79616991.40 13,635/15,630(87.24)
30 w/o 19,047 2,021 397 /465 (85.38) 1,84118091.09 16,611/19,047(87.21)
50 w/o 21,503 2,071 397 /465 (85.38) 1,87819390.68 18698 /21503 (86.96)
ofstep1,weset /u1D440as10,20,30,and50,respectively,toshowthe
impact on the eﬀectiveness. The experiment settings and the CR
system undertestarethesameasthat inTable 1.Theexperiment
is shown in Table 3. As/u1D440increases (the entry ‘M’), the number
of follow-up inputs increases accordingly, ranging from 7,947 to
21,503, and more inconsistencies (‘# Issues’) are reported, from
1,313 to 2,070. Besides, note that the hit rate stays stable, meaning
that with at most 10 follow-ups generated, around 85% incorrect
coreferenceinthesourceinputscanbesuccessfullyidenti/f_ied,while
afterthat,simplyincreasingthenumberofgeneratedfollow-ups
can hardly hit more faults in the source coreference. Regarding the
precision,wecanseethatwithstep2( i.e.,follow-upinputselection)
enabled, the precision of Creststays at 100% regardless of how
manyissuesarereported.Therefore,weconcludethattheimpact
ofStep1mainlyliesinthenumberofrevealedissues,andwould
not aﬀectthe precision of Crest.
4.5.2 ImpactofStep2(Follow-upInputSelection). Step2further
/f_ilters out the follow-up inputs according to the CR-related tokens
in thegrammar structure.In particular,we showtheresults when
step 2 is enabled or disabled. The result is shown in Table 3(the
entry ‘Select’). We can see that when step 2 is disabled ( i.e.,w/o),
the precision dropsto around91% compared with 100% when it is
enabled (i.e.,w/), and the ratio of coreference preserved follow-ups
dropsfrom 100% to around 87%. Also,when step 2 isdisabled,the
number of true positives decreases by 297 (= 1611- 1314) ∼359 (=
1878 - 1518), accounting for 0.18 (=297/1611) ∼0.19 (= 359/1878)
percentdecrease.Tosumup,theresultvalidatesthesigni/f_icanceof
step2fromtwoaspects.First,by/f_ilteringoutabout13%coreference-
changed follow-ups, step 2 makes sure the precision of error detec-
tionreaches100%,a10%increasefromthatwithouttheselection.
Second, step 2 keeps a high precision with slightly fewer true posi-
tives detected,andwithouthit ratedecreases.
4.5.3 ImpactofStep3(InconsistencyDetection). Instep3, Crestre-
ports an inconsistency if any of the precision and recall of the
coreferenceislowerthan1.00( i.e.,/u1D461/uni210E/u1D45F/u1D45D=/u1D461/uni210E/u1D45F/u1D45F=1.0).Weusetwo
examplestoshowtheirimpacts.Example13showsthatwhen /u1D461/uni210E/u1D45F/u1D45D
drops while the /u1D461/uni210E/u1D45F/u1D45Fremains to be 1.00, there are extra corefer-
encelinksaremistakenlyidenti/f_iedfromthesentence.Inparticular,
there is only one link in the ground-truth ( ∨), while three links
are identi/f_ied, sothe precision of coreference is 0.33 (= 1/3). While
fortherecallofcoreference,sinceallthecoreferencelinksinthe
ground truth are identi/f_ied, the recall is 1.0. Example 14 shows
116TestingCoreference ResolutionSystems without LabeledTestSets ESEC/FSE ’23, December3–9, 2023,San Francisco, CA, USA
thatwhen /u1D461/uni210E/u1D45F/u1D45Fdropswhilethe /u1D461/uni210E/u1D45F/u1D45Dreserves,therearecoreference
links thatshouldbe identi/f_iedyetmissed.Example 14shows the
situation. In the ground truth, there are seven coreference links,
whileonlysixofthemarerecalled,sotherecallofcoreferenceis
0.86(=6/7), whilethe precision staysat 1.0.
Example 13.
∨[[‘the city’, ‘this city’]]
×[[’this city’, ’it’, ’the city’]]
Example 14.
∨[[‘himself’,‘God’,‘his’,‘he’], [‘his people’,‘They’]]
×[[‘his’,‘God’,‘he’,‘himself’]]
Tosumup,ahigher /u1D461/uni210E/u1D45F/u1D45Drequiresmorecorrectcoreferencelinks
regardlessofthenumberofmissinglinks,while /u1D461/uni210E/u1D45F/u1D45Frequiresmore
linkstobeidenti/f_iedthoughtheremaybeincorrectlinksinvolved.
5 RELATED WORK
5.1 TestingTechniques forNLPSystems
Varioustestingmethodologieshavebeenproposedforspeci/f_icNLP
taskssuchasneuralmachinetranslation[ 8,10,24,25,53,64,83],
codeanalysis[ 1,31,54],andQA[ 8].Arecentsurvey[ 45]pointsout
thatmetamorphictesting[ 11]isthemostpopulartestingapproach
for AI systems. A recent study [ 30] benchmarks state-of-the-art
metamorphic testing approaches and /f_inds that a large proportion
oftheirgeneratedinputsviolatethemetamorphicrelations,lead-
ingtohighfalsepositiverates.Thereareafewrecentworksthat
couldbeusedforCRsystemtesting.CheckList[ 64]listsamatrixof
linguisticcapabilities( e.g.,robustness)and testtypes ( e.g.,invari-
ancetest).Inthiswork,whetherthecoreferencecanbecorrectly
resolvedisconsideredalinguisticcapabilityto betestedinupper-
streamapplicationssuchasmachinecomprehension.Testsarethen
generatedeitherbytemplatesorbytextualtransformations( e.g.,
inserting typos ). Yet, text transformations have high probabilities
tochangecoreferenceinthesentence.TextFlint[ 74]providesamul-
tilingualrobustnessevaluationplatformthatincorporatesuniversal
texttransformation, adversarial attack,etc.For CR systems,trans-
formations such as random sentence deletion in a paragraph are
proposed. These transformations are only suitable for coreference
acrosssentences(paragraph-levelcoreference),whileourworkis
suitable for both sentence- and paragraph-level coreference. An-
other testing technique, ASTRAEA [ 66] targets the fairness of NLP
tasks such as CR . It generates sentences by changing occupations
andmale/femalepronounstotestthefairnessoftheNLPsystem
under test. Our work complements existing works. It focuses on
eﬀectivelygeneratingtestinputsthatrespectcoreference,which
has not been exploredbyprior techniques.
5.2 AdversarialTechniques forNLPSystems
Adversarial attack techniques aim to produce adversarial exam-
ples [70] to confuse AI systems. They perturb benign inputs by
applying certainadversarial perturbations,so thatthe generated
adversarialexamplessuccessfullyconfusetheAIsystemmaking
wrongpredictions.Inparticular,therearevariousadversarialattack
approachestargetingNLPsystems,includingcharacter-level[ 19,
26],word-level[ 21,81],sentence-level[ 32,63],andmultiple-level[ 18,72]attacks.Severalstudiessummarizethestate-of-the-artadver-
sarial attack algorithms [ 73,82] and defending algorithms [ 76].
Besides,severaloﬀ-the-shelftool-kits[ 47,65,79]areavailableto
generateadversarialattacksongeneral[ 47,79]orcertain[ 65]NLP
tasks.Popularperturbationssuchaswordreplacementhavebeen
incorporatedinthebaselinesinourevaluation.Anadversarialat-
tack technique [ 8], which assumes all input data to be labeled, was
recentlyproposedforCRsystems.Thistechniqueissubsumedby
thebaselineSIT[ 24], withonlythewordin themention replaced
withits semantic-relatedsubstitutions( e.g.,synonym).
6 THREATS OFVALIDITY
Wediscussthreethreatsinthiswork.First,theexperimentalpro-
cedures involve random sampling, which may induce randomness
in the results. To mitigate this threat, we sampled 1,000 samples as
seeds,whichis/f_ivetimesthesamplesizeofpriorwork[ 7,24,68,69].
Also, the experimental results show that our work outperforms
thebaselinessigni/f_icantly.Thedegreeofoutperformancesurpasses
randomness.Two,thenumberoftestsgeneratedbythebaselines
may be sensitive to parameter settings. To accommodate the varia-
tion, experiments are designed to evaluate the quality of generated
testsrather thanthe numberof testsgenerated. Inthecurrent eval-
uation,weadoptthedefaultparametersettings( e.g.,parameters,
languagemodels)ofthebaselinestofacilitatecomparison.Third,
the human evaluation of sentence naturalness is conducted over a
crowd-sourcingplatform,wheretheeducationlevelofthepartici-
pants varies. To mitigate the threat, we involve two professional
English tutors as participants as well.
7 CONCLUSION
In this paper, we introduce Crest, ageneralmethodologyfor val-
idating CR systems. It tackles the diﬃculty in automatically con-
structing test inputs by considering the coreference during test
generation in aid of syntax analysis. The experiment reveals the
eﬀectivenessof Crestbyachieving100%precisionininconsistency
detection, and100%ofthe generatedtests are of high quality.
ACKNOWLEDGEMENTS
ThisworkissupportedbytheNationalScienceFoundationofChina
(GrantNo.61932021,62141210,62002125),theHongKongResearch
GrantCouncil/GeneralResearchFund(GrantNo.16205722),andthe
Hong Kong Research Grant Council/Research Impact Fund (Grant
No. R5034-18), the Hong Kong Innovation and Technology Fund
(Grant No. MHP/055/19), the Young Elite Scientists Sponsorship
ProgrambyCAST(GrantNo.2021QNRC001).Theauthorswould
also like to thank the anonymous reviewers for their comments
andsuggestionsandexpressourappreciationtotheparticipants
for conductingthe human evaluation.
8 DATA-AVAILABILITYSTATEMENT
Werelease thereproductionpackageatthewebsite[ 6]forvalida-
tion.Thepackageincludes(1)sourcecodeof Crest,(2)experimen-
tal resultsof comparisons withbaselines,and(3) labelingresults of
human evaluation.
117ESEC/FSE ’23, December3–9, 2023,San Francisco, CA, USA Jialun Cao, YaojieLu,Ming Wen, Shing-Chi Cheung
REFERENCES
[1]Uri Alon, Meital Zilberstein, Omer Levy, and Eran Yahav. 2019. code2vec: Learn-
ing distributed representations of code. Proceedings of the ACM on Programming
Languages 3,POPL(2019), 1–29.
[2]RahulAralikatte,HeatherLent,AnaValeriaGonzalez,DanielHerschcovich,Chen
Qiu,AndersSandholm,MichaelRingaard,andAndersSøgaard.2019. Rewarding
CoreferenceResolversforBeingConsistentwithWorldKnowledge.In Proceed-
ings of the 2019 Conference on EMNLP-IJCNLP . Association for Computational
Linguistics,HongKong, China, 1229–1235.
[3]SalihaAzzam,KevinHumphreys,andRobertGaizauskas.1999.UsingCoreference
ChainsforTextSummarization.In ProceedingsoftheWorkshoponCoreference
andItsApplications (CorefApp’99) .AssociationforComputationalLinguistics,
CollegePark,Maryland, 77–84.
[4]Eric Bengtson and Dan Roth. 2008. Understanding the Value of Features for
CoreferenceResolution.In ProceedingsofEMNLP .AssociationforComputational
Linguistics,Honolulu, Hawaii, 294–303.
[5]AriBornstein,ArieCattan,andIdoDagan.2020. CoRe/f_i:ACrowdSourcingSuite
for Coreference Annotation. In Proceedings of EMNLP: System Demonstrations .
Associationfor Computational Linguistics,Online, 205–215.
[6] JialunCAO. 2023. CREST-Artifact. https://doi.org/10.5281/zenodo.8320669
[7]JialunCao, MeiziniuLi,YetingLi,Ming Wen,Shing-ChiCheung,andHaiming
Chen. 2022. SemMT: a semantic-based testing approach for machine translation
systems. ACMTOSEM 31,2 (2022), 1–36.
[8]Haixia Chai, Wei Zhao, Steﬀen Eger, and Michael Strube. 2020. Evaluation of
Coreference Resolution Systems Under Adversarial Attacks. In Proceedings of
the First Workshop on Computational Approaches to Discourse . Association for
Computational Linguistics,Online, 154–159.
[9]Danqi Chen and Christopher Manning. 2014. A Fast and Accurate Dependency
Parser using Neural Networks. In Proceedings of EMNLP . Association for Compu-
tationalLinguistics,Doha,Qatar, 740–750.
[10]Songqiang Chen, Shuo Jin, and Xiaoyuan Xie. 2021. Validation on Machine
Reading Comprehension Software without Annotated Labels: A Property-Based
Method. In Proceedings of the 29th ACM Joint Meeting on ESEC/FSE (ESEC/FSE
2021). Associationfor Computing Machinery, NewYork, NY, USA,590–602.
[11]Tsong Yueh Chen, S. C. Cheung, and Siu-Ming Yiu. 2020. Metamorphic Testing:
ANew ApproachforGeneratingNext TestCases,InTechnicalReportHKUST-
CS98-01. CoRRabs/2002.12543,11. arXiv: 2002.12543
[12]Yu-HsinChenandJinhoD.Choi.2016. CharacterIdenti/f_icationonMultiparty
Conversation:IdentifyingMentionsofCharactersinTVShows.In Proceedingsof
the17thAnnualMeetingoftheSpecialInterestGrouponDiscourseandDialogue .
Associationfor Computational Linguistics,LosAngeles, 90–100.
[13]Kevin Clark and Christopher D. Manning. 2015. Entity-Centric Coreference
Resolution with Model Stacking. In Proceedings of ACL and the 7th International
Joint Conference on Natural Language Processing . Association for Computational
Linguistics,Beijing, China, 1405–1415.
[14]KevinClarkandChristopherD.Manning.2016. DeepReinforcementLearning
for Mention-Ranking Coreference Models. In Proceedings of EMNLP . Association
for Computational Linguistics,Austin,Texas,2256–2262.
[15]KevinClarkandChristopherD.Manning.2016. ImprovingCoreferenceReso-
lutionbyLearningEntity-LevelDistributedRepresentations.In Proceedingsof
ACL. Associationfor Computational Linguistics,Berlin, Germany, 643–653.
[16]JacobDevlin,Ming-WeiChang,KentonLee,andKristinaToutanova.2019. BERT:
Pre-trainingofDeepBidirectionalTransformersforLanguageUnderstanding.In
Proceedings of the 2019 Conference of the NAACL: Human Language Technologies,
NAACL-HLT . Associationfor Computational Linguistics,Online, 4171–4186.
[17]Greg Durrett and DanKlein. 2013. Easy Victories and Uphill Battles in Corefer-
enceResolution.In Proceedings ofthe2013 ConferenceonEMNLP .Association for
Computational Linguistics,Seattle, Washington, USA,1971–1982.
[18]Javid Ebrahimi,AnyiRao,Daniel Lowd, and DejingDou. 2018. HotFlip: White-
Box Adversarial Examples for Text Classi/f_ication. In Proceedings of ACL . Associa-
tionfor Computational Linguistics,Melbourne, Australia,31–36.
[19]Steﬀen Eger, Gözde Gül Şahin, Andreas Rücklé, Ji-Ung Lee, Claudia Schulz,
Mohsen Mesgar, Krishnkant Swarnkar, Edwin Simpson, and Iryna Gurevych.
2019. TextProcessingLikeHumansDo:VisuallyAttackingandShieldingNLP
Systems.In Proceedingsofthe2019Conferenceof theNAACL:HumanLanguage
Technologies . ACL,Minneapolis, Minnesota,1634–1647.
[20]PradheepElango.2005. Coreferenceresolution:Asurvey. UniversityofWisconsin,
Madison, WI 1,12(2005), 12.
[21]Max Glockner,Vered Shwartz,andYoavGoldberg. 2018. BreakingNLI Systems
with Sentences that Require Simple Lexical Inferences. In Proceedings of ACL .
Associationfor Computational Linguistics,Melbourne, Australia,650–655.
[22]Shashij Gupta, Pinjia He, Clara Meister, and Zhendong Su. 2020. Machine Trans-
lationTestingviaPathologicalInvariance.In Proceedingsofthe28thACMJoint
Meeting on ESEC/FSE (ESEC/FSE 2020) . Association for Computing Machinery,
NewYork, NY, USA,863–875.
[23]AriaHaghighiandDanKlein.2010. CoreferenceResolutioninaModular,Entity-
CenteredModel.In Proceedings.ofNAACL .AssociationforComputationalLin-
guistics,USA,385–393.[24]Pinjia He, Clara Meister, and Zhendong Su. 2020. Structure-Invariant Testing for
MachineTranslation.In ProceedingsoftheACM/IEEE42ndInternationalConfer-
ence on SoftwareEngineering (ICSE’20) . AssociationforComputing Machinery,
NewYork, NY, USA,961–973.
[25]PinjiaHe,ClaraMeister,andZhendongSu.2021. TestingMachineTranslationvia
ReferentialTransparency.In 43rdIEEE/ACMInternationalConferenceonSoftware
Engineering, Madrid,Spain . IEEE,Madrid, Spain,410–422.
[26]Xuanli He, Lingjuan Lyu, Lichao Sun, and Qiongkai Xu. 2021. Model Extraction
and Adversarial Transferability, Your BERT is Vulnerable!. In Proceedings ofthe
2021ConferenceoftheNAACL:HumanLanguageTechnologies .Associationfor
Computational Linguistics,Online, 2006–2012.
[27]Lynette Hirschman and Nancy Chinchor. 1998. Appendix F: MUC-7 Coreference
TaskDe/f_inition(version3.0).In SeventhMessageUnderstandingConference(MUC-
7): Proceedings of a Conference Held in Fairfax, Virginia, April 29 - May 1, 1998 .
Proceedingsof aConferenceHeld in Fairfax, Virginia,Fairfax, Virginia,17.
[28]J Hobbs.1986. Resolving Pronoun References . MorganKaufmann Publishers Inc.,
San Francisco, CA, USA,339–352.
[29]MatthewHonnibal,InesMontani,So/f_ieVanLandeghem,andAdrianeBoyd.2020.
spaCy: Industrial-strength Natural Language Processing in Python.
[30]Jen-tse Huang, Jianping Zhang, Wenxuan Wang, Pinjia He, Yuxin Su, and
MichaelR.Lyu.2022. AEON:AMethodforAutomaticEvaluationofNLPTest
Cases. In Proceedings of the 31st ACM SIGSOFT International Symposium on Soft-
wareTestingandAnalysis (ISSTA2022) .AssociationforComputingMachinery,
NewYork, NY, USA,202–214.
[31]Srinivasan Iyer, Ioannis Konstas, Alvin Cheung, and Luke Zettlemoyer. 2016.
Summarizing source code using a neural attention model. In Proceedings of ACL .
Associationfor Computational Linguistics,Berlin, Germany, 2073–2083.
[32]MohitIyyer,JohnWieting,KevinGimpel,andLukeZettlemoyer.2018. Adversar-
ialExampleGenerationwithSyntacticallyControlledParaphraseNetworks.In
Proceedings of the 2018 Conference of the NAACL: Human Language Technologies,
Volume1(LongPapers) .AssociationforComputationalLinguistics,Melbourne,
Australia,1875–1885.
[33]HengJiandJoelNothman.2016. OverviewofTAC-KBP2016Tri-lingualEDLand
ItsImpactonEnd-to-EndKBP.In Proceedingsofthe2016TextAnalysisConference,
TAC 2016, Gaithersburg, Maryland,USA, November 14-15, 2016 . NIST, USA,15.
[34]Mandar Joshi, Omer Levy, Luke Zettlemoyer, and Daniel Weld. 2019. BERT
for Coreference Resolution: Baselines and Analysis. In Proceedings of the 2019
Conference on EMNLP-IJCNLP . Association for Computational Linguistics, Hong
Kong, China, 5803–5808.
[35]Yuval Kirstain, Ori Ram, and Omer Levy. 2021. Coreference Resolution without
SpanRepresentations.In Proceedingsofthe59thAnnualMeetingoftheACLandthe
11thInternationalJointConferenceonNaturalLanguageProcessing .Association
for Computational Linguistics,Online, 14–19.
[36]SatwikKottur,JoséM.F.Moura,DeviParikh,DhruvBatra,andMarcusRohrbach.
2018. Visual Coreference Resolution in Visual Dialog Using Neural Module
Networks. In Proceedings of ECCV , Vittorio Ferrari, Martial Hebert, Cristian
Sminchisescu,andYairWeiss(Eds.).SpringerInternationalPublishing,Cham,
160–178.
[37]JonathanK.KummerfeldandDanKlein.2013. Error-DrivenAnalysisofChal-
lengesinCoreferenceResolution.In Proceedingsofthe2013ConferenceonEMNLP .
Association for Computational Linguistics, Seattle, Washington, USA, 265–277.
[38]HeeyoungLee,YvesPeirsman,AngelChang,NathanaelChambers,MihaiSur-
deanu, and Dan Jurafsky. 2011. Stanford’s Multi-Pass Sieve Coreference Reso-
lution System at the CoNLL-2011 Shared Task. In Proceedings of the Fifteenth
ConferenceonComputationalNaturalLanguageLearning:SharedTask .Associa-
tionfor Computational Linguistics,Portland, Oregon, USA,28–34.
[39]KentonLee,LuhengHe,MikeLewis,andLukeZettlemoyer.2017. End-to-end
Neural Coreference Resolution. In Proceedings of the 2017 Conference on EMNLP .
Associationfor Computational Linguistics,Copenhagen,Denmark, 188–197.
[40]Kenton Lee, Luheng He, and Luke Zettlemoyer. 2018. Higher-Order Coreference
Resolutionwith Coarse-to-FineInference.In Proceedingsofthe2018Conference
oftheNorthAmericanChapteroftheAssociationforComputationalLinguistics:
Human LanguageTechnologies, Volume 2 (ShortPapers) . Associationfor Compu-
tationalLinguistics,NewOrleans,Louisiana, 687–692.
[41]Xianzhi Li, Xiaodan Zhu, Zhiqiang Ma, Xiaomo Liu, and Sameena Shah. 2023.
AreChatGPTandGPT-4General-Purpose SolversforFinancialTextAnalytics?
AnExamination onSeveral TypicalTasks. arXiv: 2305.05862 [cs.CL]
[42]Zhengyuan Liu, Ke Shi, and Nancy F. Chen. 2021. Coreference-Aware Dialogue
Summarization. In Proceedings of the 22nd Annual Meeting of the Special Interest
Group on Discourse and Dialogue, SIGdial 2021, Singapore and Online, July 29-
31, 2021, Haizhou Li, Gina-Anne Levow, Zhou Yu, Chitralekha Gupta, Berrak
Sisman,SiqiCai,DavidVandyke,NinaDethlefs,YanWu,andJunyiJessyLi(Eds.).
Associationfor Computational Linguistics,Singaporeand Online, 509–519.
[43]XiaoqiangLuo,SameerPradhan,MartaRecasens,andEduardHovy.2014. An
Extension of BLANC to System Mentions. In Proceedings of ACL . Association for
Computational Linguistics,Baltimore, Maryland, 24–29.
[44]ChristopherManning,MihaiSurdeanu,JohnBauer,JennyFinkel,StevenBethard,
and David McClosky. 2014. The Stanford CoreNLP Natural Language Processing
118TestingCoreference ResolutionSystems without LabeledTestSets ESEC/FSE ’23, December3–9, 2023,San Francisco, CA, USA
Toolkit.In Proceedingsof52ndAnnualMeetingoftheACL:SystemDemonstrations .
Associationfor Computational Linguistics,Baltimore, Maryland, 55–60.
[45]SilverioMart’inez-Fern’andez, JustusBogner,XavierFranch, MarcOriol,Julien
Siebert, Adam Trendowicz, Anna Maria Vollmer, and Stefan Wagner. 2022. Soft-
wareEngineeringforAI-BasedSystems:ASurvey. ACMTransactionsonSoftware
Engineering and Methodology 31(2022), 1 – 59.
[46]GeorgeA.Miller.1995. WordNet:ALexicalDatabaseforEnglish. Commun.ACM
38,11(nov1995),39–41.
[47]John Morris, Eli Li/f_land, Jin Yong Yoo, Jake Grigsby, Di Jin, and Yanjun Qi. 2020.
TextAttack: A Framework for Adversarial Attacks, Data Augmentation, and
Adversarial Training in NLP. In Proceedings of the 2020 Conference on EMNLP:
SystemDemonstrations . ACL,Online, 119–126.
[48]ThomasS.Morton.1999. UsingCoreferenceforQuestionAnswering.In Proceed-
ingsoftheWorkshoponCoreferenceandItsApplications(CorefApp’99) .Association
for Computational Linguistics,USA,85–89.
[49]Vincent Ng. 2017. Machine Learning for Entity Coreference Resolution: A Retro-
spective Look at Two Decades of Research. In AAAI (AAAI’17) . AAAI Press, San
Francisco, California, USA,4877–4884.
[50]VincentNgandClaireCardie.2002. ImprovingMachineLearningApproaches
to Coreference Resolution. In Proceedings of the 40th Annual Meeting of the ACL .
ACL,Philadelphia,Pennsylvania,USA,104–111.
[51]Yulei Niu, Hanwang Zhang, Manli Zhang, Jianhong Zhang, Zhiwu Lu, and Ji-
RongWen.2019. RecursiveVisualAttentioninVisualDialog.In IEEEConference
on Computer Vision and Pattern Recognition . Computer Vision Foundation / IEEE,
LongBeach,CA, USA,6679–6688.
[52]Joakim Nivre, Marie-Catherine de Marneﬀe, Filip Ginter, Yoav Goldberg, Jan
Hajič, Christopher D. Manning, Ryan McDonald, Slav Petrov, Sampo Pyysalo,
Natalia Silveira, Reut Tsarfaty, and Daniel Zeman. 2016. Universal Dependencies
v1: A Multilingual Treebank Collection. In Proceedings of the Tenth International
ConferenceonLanguageResourcesandEvaluation .EuropeanLanguageResources
Association, Portorož,Slovenia,1659–1666.
[53]DanielPesu,ZhiQuanZhou,JingfengZhen,andDaveTowey.2018. AMonte
Carlo Method for Metamorphic Testing of Machine Translation Services. In 3rd
IEEE/ACMInternationalWorkshoponMetamorphicTestingMET .ACM,Gothen-
burg,Sweden, 38–45.
[54]Michael Pradel and Koushik Sen. 2018. Deepbugs: A learning approach to name-
based bug detection. Proceedings of the ACM on Programming Languages 2,
OOPSLA (2018), 1–25.
[55]Sameer Pradhan, Alessandro Moschitti, Nianwen Xue, Hwee Tou Ng, Anders
Björkelund, Olga Uryupina, Yuchen Zhang, and Zhi Zhong. 2013. Towards
Robust Linguistic Analysis using OntoNotes. In Proceedings of Computational
NaturalLanguageLearning .AssociationforComputationalLinguistics,Seattle,
Washington, USA,143–152.
[56]Sameer Pradhan, Alessandro Moschitti, Nianwen Xue, Olga Uryupina, and
Yuchen Zhang. 2012. CoNLL-2012 Shared Task: Modeling Multilingual Un-
restrictedCoreferenceinOntoNotes.In JointConferenceonEMNLPandCoNLL
(CoNLL’12) . Associationfor Computational Linguistics,USA,1–40.
[57] Proli/f_ic. 2014. Proli/f_ic. https://www.proli/f_ic.co/
[58]ChengweiQin,AstonZhang,ZhuoshengZhang,JiaaoChen,MichihiroYasunaga,
andDiyiYang.2023. IsChatGPTaGeneral-PurposeNaturalLanguageProcessing
TaskSolver? arXiv: 2302.06476 [cs.CL]
[59]Karthik Raghunathan, Heeyoung Lee, Sudarshan Rangarajan, Nathanael Cham-
bers,MihaiSurdeanu,DanJurafsky,andChristopherManning.2010.AMulti-Pass
SieveforCoreferenceResolution.In Proceedingsofthe2010ConferenceonEMNLP .
Associationfor Computational Linguistics,Cambridge, MA,492–501.
[60]Marta Recasens, Marie-Catherine de Marneﬀe, and Christopher Potts. 2013. The
Life and Death of Discourse Entities: Identifying Singleton Mentions. In Pro-
ceedings of the 2013 Conference of the NAACL: Human Language Technologies .
Associationfor Computational Linguistics,Atlanta, Georgia,627–633.
[61]M.RecasensandE.Hovy.2011. Blanc:ImplementingtheRandIndexforCorefer-
enceEvaluation. Nat.Lang. Eng. 17,4 (oct 2011),485–510.
[62]NilsReimersand IrynaGurevych.2019. Sentence-BERT:SentenceEmbeddings
using Siamese BERT-Networks. In Proceedings of the 2019 Conference on EMNLP-
IJCNLP2019,HongKong,China,November3-7,2019 .AssociationforComputa-
tionalLinguistics,Online, 3980–3990.
[63]Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. 2018. Semantically
Equivalent Adversarial Rules for Debugging NLP models. In Proceedings of ACL .
Associationfor Computational Linguistics,Melbourne, Australia,856–865.
[64]Marco Tulio Ribeiro, Tongshuang Wu, Carlos Guestrin, andSameer Singh. 2020.
Beyond Accuracy: Behavioral Testing of NLP Models with CheckList. In Proceed-
ingsofACL . Associationfor Computational Linguistics,Online, 4902–4912.
[65]Walter Simoncini and Gerasimos Spanakis. 2021. SeqAttack: On adversarial
attacks for named entity recognition. In Proceedings of the 2021 Conference on
EMNLP: System Demonstrations . Association forComputational Linguistics, On-
line, 308–318.
[66]EzekielSoremekun,SakshiUdeshi,andSudiptaChattopadhyay.2022. Astraea:
Grammar-Based Fairness Testing. IEEE Transactions on Software Engineering 48,
12(2022), 5188–5211.[67]Dario Stojanovski and Alexander Fraser. 2018. Coreference and Coherence in
Neural Machine Translation: A Study Using Oracle Experiments. In Proceedings
oftheThirdConferenceonMachineTranslation .AssociationforComputational
Linguistics,Brussels, Belgium,49–60.
[68]Zeyu Sun, Jie M. Zhang, Mark Harman, Mike Papadakis, and Lu Zhang. 2020.
AutomaticTestingandImprovementofMachineTranslation.In Proceedingsof
theACM/IEEE 42ndInternationalConference onSoftware Engineering (ICSE’20) .
Associationfor Computing Machinery, NewYork, NY, USA,974–985.
[69]ZeyuSun,JieM.Zhang,YingfeiXiong,MarkHarman,MikePapadakis,andLu
Zhang. 2022. Improving Machine Translation Systems via Isotopic Replacement.
InProceedings of the 44th International Conference on Software Engineering (ICSE
’22). Associationfor Computing Machinery, NewYork, NY, USA,1181–1192.
[70]ChristianSzegedy,WojciechZaremba,IlyaSutskever,JoanBruna,DumitruErhan,
IanJ.Goodfellow,andRobFergus.2014. Intriguingpropertiesofneuralnetworks.
InInternationalConferenceonLearningRepresentations .InternationalConference
onLearningRepresentations,Banﬀ, Canada,10.
[71]AnnTaylor,MitchellMarcus,andBeatriceSantorini.2003. ThePennTreebank:
AnOverview . SpringerNetherlands, Dordrecht,5–22.
[72]EricWallace,ShiFeng,NikhilKandpal,MattGardner,andSameerSingh.2019.
UniversalAdversarialTriggersforAttackingandAnalyzingNLP.In Proceedingsof
the2019ConferenceonEMNLP-IJCNLP .AssociationforComputationalLinguistics,
HongKong, China, 2153–2162.
[73]Wenqi Wang, Run Wang, Lina Wang, Zhibo Wang, and Aoshuang Ye. 2023.
Towards a Robust Deep Neural Network Against Adversarial Texts: A Survey.
IEEE Transactions onKnowledgeand Data Engineering 35,3 (2023), 3159–3179.
[74]Xiao Wang, Qin Liu, Tao Gui, Qi Zhang, Yicheng Zou, Xin Zhou, Jiacheng Ye,
YongxinZhang,RuiZheng,ZexiongPang,QinzhuoWu,ZhengyanLi,Chong
Zhang, Ruotian Ma, Zichu Fei, Ruijian Cai, Jun Zhao, Xingwu Hu, Zhiheng Yan,
YidingTan,YuanHu,QiyuanBian,ZhihuaLiu,ShanQin,BolinZhu,XiaoyuXing,
JinlanFu,YueZhang,MinlongPeng,XiaoqingZheng,YaqianZhou,Zhongyu
Wei, Xipeng Qiu, and Xuanjing Huang. 2021. TextFlint: Uni/f_ied Multilingual
Robustness Evaluation ToolkitforNatural LanguageProcessing. In Proceedings
of the 59th Annual Meeting of the ACL and the 11th International Joint Confer-
ence on Natural Language Processing: System Demonstrations . Association for
Computational Linguistics,Online, 347–355.
[75]Terry Winograd. 1972. Understanding natural language. Cognitive Psychology 3,
1 (1972), 1–191.
[76]HanXu,YaoMa,Hao-ChenLiu,DebayanDeb,HuiLiu,Ji-LiangTang,andAnilK
Jain.2020. Adversarialattacksanddefensesinimages,graphsandtext:Areview.
InternationalJournal ofAutomationand Computing 17,2 (2020), 151–178.
[77]XintongYu,HongmingZhang,YangqiuSong,ChangshuiZhang,KunXu,and
Dong Yu. 2021. Exophoric Pronoun Resolution in Dialogues with Topic Regu-
larization. In Proceedings ofEMNLP . Association for Computational Linguistics,
Onlineand PuntaCana, Dominican Republic,3832–3845.
[78]Michelle Yuan, Patrick Xia, Chandler May, Benjamin Van Durme, and Jordan
Boyd-Graber. 2022. Adapting Coreference Resolution Models through Active
Learning.In Proceedingsofthe60thAnnualMeetingoftheAssociationforComputa-
tionalLinguistics(Volume1:LongPapers) .AssociationforComputationalLinguis-
tics,Dublin, Ireland, 7533–7549. https://doi.org/10.18653/v1/2022.acl-long.519
[79]Guoyang Zeng, Fanchao Qi, Qianrui Zhou, Tingji Zhang, Zixian Ma, Bairu Hou,
Yuan Zang, Zhiyuan Liu, and Maosong Sun. 2021. OpenAttack: An Open-source
Textual Adversarial Attack Toolkit. In Proceedings of the 59th Annual Meeting
oftheAssociationforComputationalLinguisticsandthe11thInternationalJoint
Conference onNaturalLanguageProcessing: System Demonstrations . Association
for Computational Linguistics,Online, 363–371.
[80]Hongming Zhang, Xinran Zhao, and Yangqiu Song. 2021. A Brief Survey and
Comparative Study of Recent Development of Pronoun Coreference Resolution
in English. In Proceedings of the Fourth Workshop on CRAC . ACL, Punta Cana,
Dominican Republic,1–11.
[81]HuangzhaoZhang,HaoZhou,NingMiao,andLeiLi.2019. GeneratingFluent
AdversarialExamplesforNaturalLanguages.In Proceedingsofthe57thConference
of the ACL 2019, Florence, Italy, July 28- August 2, 2019, Volume 1: Long Papers ,
Anna Korhonen, David R. Traum, and Lluís Màrquez (Eds.). Association for
Computational Linguistics,Florence, Italy, 5564–5569.
[82]Wei Emma Zhang, Quan Z. Sheng, Ahoud Alhazmi, and Chenliang Li. 2020.
AdversarialAttacksonDeep-LearningModelsinNaturalLanguageProcessing:
A Survey. ACM Trans. Intell. Syst. Technol. 11, 3, Article 24 (apr 2020), 41 pages.
[83]ZhiQuanZhouandLiqunSun.2018. MetamorphicTestingforMachineTrans-
lations: MT4MT. In Proceedings of the 25th Australasian Software Engineering
Conference (ASWEC) . IEEE Computer Society, Adelaide, SA,Australia,96–100.
[84]Enwei Zhu and Jinpeng Li. 2022. Boundary Smoothing for Named Entity Recog-
nition. In Proceedings of ACL . Association for Computational Linguistics, Dublin,
Ireland, 7096–7108.
[85]Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang, and Jingbo Zhu. 2013. Fast
andAccurateShift-ReduceConstituentParsing.In ProceedingsofACL .Associa-
tionfor Computational Linguistics,So/f_ia, Bulgaria, 434–443.
Received 2023-03-02; accepted 2023-07-27
119
View publication stats