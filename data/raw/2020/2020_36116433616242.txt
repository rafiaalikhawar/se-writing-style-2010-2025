On the Relationship between CodeVerifiability and
Understandability
Kobi Feldman
jcfeldman@wm.edu
Collegeof William & Mary
Williamsburg, Virginia,USAMartinKellogg
martin.kellogg@njit.edu
New Jersey Institute of Technology
Newark, New Jersey, USAOscarChaparro
oscarch@wm.edu
Collegeof William & Mary
Williamsburg, Virginia,USA
ABSTRACT
Proponents of software veri /f_ication have argued that simplercode
is easier to verify: that is, that veri /f_ication tools issue fewer false
positives and require less human intervention when analyzing sim-
plercode.Weempiricallyvalidatethisassumptionbycomparing
the number of warnings produced by four state-of-the-art veri /f_i-
cationtoolson211snippetsofJavacodewith20metricsofcode
comprehensibilityfrom human subjectsinsix prior studies.
Our experiments, based on a statistical (meta-)analysis, show
that, in aggregate, there is a small correlation ( /u1D45F=0.23) between
understandabilityandveri /f_iability.Theresultssupporttheclaim
thateasy-to-verifycodeisofteneasiertounderstandthancodethat
requires more e ﬀort to verify. Our work has implications for the
users and designers of veri /f_ication tools and for future attempts to
automaticallymeasurecodecomprehensibility:veri /f_icationtools
may have ancillary bene /f_its to understandability, and measuring
understandabilitymayrequirereasoningaboutsemantic,notjust
syntactic, code properties.
CCS CONCEPTS
•Softwareanditsengineering →Formalsoftwareveri /f_ica-
tion;•General andreference →Empirical studies .
KEYWORDS
Veri/f_ication, staticanalysis,code comprehension, meta-analysis
ACMReference Format:
Kobi Feldman, Martin Kellogg, and Oscar Chaparro. 2023. On the Relation-
ship between Code Veri /f_iability and Understandability. In Proceedings of
the31stACMJointEuropeanSoftwareEngineeringConferenceandSympo-
siumontheFoundationsofSoftwareEngineering(ESEC/FSE’23),December
3–9, 2023, San Francisco, CA, USA. ACM, New York, NY, USA, 13pages.
https://doi.org/10.1145/3611643.3616242
1 INTRODUCTION
Programmersmustdeeplyunderstandsourcecodeinordertoim-
plement new features, /f_ix bugs, refactor, review code, and do other
essential software engineering activities [ 4,56,69,90]. However,
Permissionto make digitalor hard copies of allorpart ofthis work for personalor
classroom use is granted without fee provided that copies are not made or distributed
forpro/f_itorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation
onthe/f_irstpage.Copyrights forcomponentsofthisworkownedbyothersthanthe
author(s)mustbehonored.Abstractingwithcreditispermitted.Tocopyotherwise,or
republish,topostonserversortoredistributetolists,requirespriorspeci /f_icpermission
and/or a fee. Request permissions from permissions@acm.org.
ESEC/FSE ’23, December 3–9, 2023, San Francisco, CA,USA
©2023 Copyright heldby the owner/author(s). Publicationrightslicensed to ACM.
ACM ISBN 979-8-4007-0327-0/23/12.
https://doi.org/10.1145/3611643.3616242understandingcodeischallengingandtime-consumingfordevel-
opers: studies [ 59,104] have estimated developers spend 58%–70%
oftheirtime understanding code.
Complexity is a major reason why code can be hard to under-
stand [3,5,6,69,78]: algorithms may be written in convoluted
ways or be composed of numerous interacting code structures and
dependencies. There are two major sources of complexity in code:
essential complexity , which is needed for the code to work, and
accidental complexity , which could be removed while retaining the
code’s semantics [ 5,14]. Whether the complexity is essential or
accidental,understandingcomplexcodedemandshighcognitive
eﬀort from developers [ 3,78].
Researchershaveproposedmanymetricstoapproximatecode
complexity [ 3,19,21,39,41,64,85,106] using vocabulary size
(e.g., Halstead’s complexity [ 36]), program execution paths ( e.g.,
McCabe’s cyclomatic complexity [ 58]), program data /f_low (e.g.,
Beyer’sDepDegree[ 9]),etc.Thesesyntacticmetricsareintended
to alert developers about complex code so they can refactor or sim-
plify it to remove accidental complexity [ 4,34,69], or to predict
developers’ cognitive load when understanding code [ 62,69,78].
However, recent studies have found that (some of) these metrics
(e.g.,McCabe’s)eitherweaklyordonotcorrelateatallwithcode
understandability as perceived by developers or measured by their
behavior and brain activity [ 26,69,78]. Other studies have demon-
stratedthatcertaincodestructures( e.g.,ifvsforloops, /f_latvsnested
constructs,orrepetitive codepatterns) leadto higherorlowerun-
derstandinge ﬀort(a.k.a. codeunderstandability orcomprehensibil-
ity)[3,13,26,41,43,52],whichdivergesfromthesimplisticway
metrics(e.g.,McCabe’s)measurecodecomplexity[ 3,26,41,45,78].
In this paper, we investigate the relationship between under-
standabilityand codeveri/f_iability—howeasyorharditisforade-
velopertouseaveri /f_icationtooltoprovesafetypropertiesaboutthe
code,suchastheabsenceofnullpointerviolationsorout-of-bounds
array accesses. Our research is motivated by the common assump-
tioninthesoftwareveri /f_icationcommunitythat simplercodeisboth
easiertoverifybyveri /f_icationtoolsandeasiertounderstandbydevel-
opers.Forexample,theCheckerFramework[ 68]usermanualstates
this assumption explicitly in its advice about unexpected warnings:
“rewrite your code to be simpler for the checker to analyze; this
islikelytomakeiteasierforpeopletounderstand,too”[ 91].The
documentation of the OpenJML veri /f_ication tool says [ 93]: “suc-
cess in checking the consistency of the speci /f_ications and the code
will depend on... the complexity and style in which the code and
speci/f_ications are written” [ 67]. This assumption is widely held by
veri/f_ication expertsbut has never been validatedempirically.
Theintuitionbehindthisassumptionisthataveri /f_iercanhandle
acertainamountofcodecomplexitybefore itissues awarning.If
211
ESEC/FSE ’23, December3–9, 2023,San Francisco, CA, USA KobiFeldman,MartinKellogg, andOscar Chaparro
itispossibletoremovethewarningbychangingthecode,thenthe
complexity thatcaused itmustbeaccidental rather than essential,
andthereforeremovingthewarningreducestheoverallcomplex-
ity of the code. For example, consider accessing a possibly-null
pointerinaJava-likelanguage.Asimplenullcheckmightusean if
statement. A more complexvariant withthe samesemantics might
dereference the pointer within a trystatement and use a catch
statement to intercept the resulting exception if the pointer is null.
Thesecond,moreconvolutedvariant(withitssigni /f_icantacciden-
tal complexity) might not be veri /f_ied—a null pointer dereference
does occur, but it is intercepted before it crashes the program. A
veri/f_ier would need to model exceptional control /f_low to avoid a
false positivewarning. Alternatively,averi /f_ier mightwarnabout
code that makes unstated assumptions. For example, by derefer-
encing a possibly-null pointer without checking it /f_irst, the code
assumesthatthepointerhasalreadybeenchecked.Averi /f_iermight
warn that this is unsafe unless a human provides a speci /f_ication
that the pointer is non-null. In that case, the veri /f_ier can verify the
dereference, but then must check that the value assigned to the
variablereallyisnon-null ateach assignment.A warningbecause
ofamissingspeci /f_icationcanalsoindicatecomplexitythatahuman
might need to reason about to understand the code: the human
might need to determine why it is safe to dereference the pointer.
Our goal is to empirically validate the purported relationship
betweenveri /f_iabilityandunderstandability—andthereforeeither
con/f_irmorrefutetheassumptionthateasy-to-understandcodeis
easy to verify (and vice-versa). To do so, we need to measure veri /f_i-
ability. Veri /f_iers analyze source code to prove the absence of partic-
ular classes of defects ( e.g., null dereferences) using soundanalyses.
A soundveri /f_ier can/f_ind all defects(of awell-de /f_ined class) in the
code.However,mostinterestingpropertiesofprogramsareundecid-
able[74],soallsoundveri /f_iersproducefalsepositivewarnings:that
is,theyconservativelyissueawarningwhentheycannotproducea
proof. The user of the veri /f_ier must sort the true positive warnings
thatcorrespondtorealbugsfromfalsepositivewarningsduetothe
veri/f_ier’simprecisionorduetotheneedtostatecodeassumptionsas
speci/f_ications.Thecombinationoffalsepositivewarningsandwarn-
ingsaboutunstated assumptions(which we refertoas“falseposi-
tives”or“warnings”forbrevity)isagoodproxyforveri /f_iabilitybe-
causethefewersuchwarningsinagivenpieceofcode,thelesswork
adeveloper using the veri /f_ier willneedto do to verifythat code.
Withthatinmind, wehypothesize thatacorrelationexistsbe-
tweena code snippet’s comprehensibility,as judged by humans,and
itsveri/f_iability,as measured byfalse positivewarnings .
Weconductedanempiricalstudytovalidatethishypothesis—
the/f_irst time that this common assumption in the veri /f_ication
community is tested empirically. Our study compares the num-
ber of warnings produced by three state-of-the-art, sound static
code veri /f_iers [60,68,93] and one industrial-strength, unsound
static analysis tool based on a sound core [ 16] with≈18k measure-
ments code understandability proxies collected from humans in
six prior studies [ 13,15,69,71,78,82] for 211 Java code snippets.
Suchmeasurements comefrom20metricsinfourcategories[ 62]:
(1) human-judged ratings, (2)program output correctness , (3)com-
prehension time,and(4)physiological (i.e.,brainactivity)metrics.
Weusedastatisticalmeta-analysistechnique[ 11,37]toexamine
the correlation between veri /f_iability and these understandabilitymetricsinaggregate.Giventhesmallsamplesizesoftheoriginal
studies and the danger of multiple comparisons, this meta-analysis
technique permits us to draw methodologically-sound conclusions
aboutthe overalltrends.
We found a small correlation between veri /f_iability and the prox-
ies for understandability, in aggregate ( /u1D45F=0.23); individually, 13
of 20 metrics were correlated with veri /f_iability. This trend suggests
thatmoreoftenthannot,codethatiseasiertoverifyiseasierfor
humansto understand.
Oneimplicationofthisresultisthatourresultsprovideevidence
for a relationship between the semantics of a piece of code and
its understandability, which may explain (in part) the apparent
ineﬀectiveness of prior syntactic approaches. This implies that the
veri/f_iability of a code snippet, as measured automatically by the
warnings issued by extant veri /f_ication tools, might be a useful
input to models of code understandability [ 15,78,94]. Another
implicationisthatwhenusingaveri /f_icationtool,developersshould
considermaking changes to the code to make it easier to verify
automatically: doing so is more likely than not to make the code
easierforahumantounderstand.Ifadevelopermakesachangeto
removeafalsepositivewithoutchangingthecode’ssemantics,any
complexity they remove must have been accidental. This means
that veri/f_ication tools provide a secondary bene /f_it beyond their
guaranteesoftheabsenceoferrors:codethatcanbeeasilyveri /f_ied
should be easier for future developers to improve andextend.
In summary,the main contributionsof this paper are:
•empiricalevidenceofacorrelationbetweencodeunderstand-
abilityandveri /f_iabilityderivedviameta-analysis,supporting
thecommon assumptionthateasier-to-verifycode iseasier
for humans to understand (and vice-versa). The results have
implicationsforthedesignanddeploymentofveri /f_ication
toolsandfordevelopingmoreaccurateautomatedmetrics
ofcode comprehensibility;and
•anonlinereplicationpackage[ 27]thatenablesveri /f_ication
andreplication ofour results andfuture research.
2 EMPIRICAL STUDYDESIGN
Our goal is to assess the correlation between human-based code
comprehensibility metrics and code veri /f_iability—i.e., how many
warnings static code veri /f_ication tools issue. Intuitively, our goal is
to check if code that is easy to verify is also easy for humans to un-
derstand.Tothatend,weformulatetheseresearchquestions(RQs):
RQ1Howdoindividual human-basedcodecomprehensibility
metrics correlate withtool-basedcode veri /f_iability?
RQ2How do human-based code comprehensibility metrics corre-
latewithtool-basedcode veri /f_iabilityinaggregate ?
RQ3Whatistheimpactofeachveri /f_icationtoolontheaggregated
correlation results?
RQ4Do diﬀerent kinds of comprehensibility metrics correlate
betterorworse withtool-basedveri /f_iability?
RQ1andRQ2encodeour hypothesis :thatacorrelationexists
between tool-based code veri /f_iability and human-based code com-
prehensibility. RQ1asks whether individual metrics correlate with
veri/f_iability.However,duetothelimitationsofpriorstudies,sample
sizes for the metrics considered individually are too small to draw
212On the RelationshipbetweenCode Verifiability andUnderstandability ESEC/FSE ’23, December3–9, 2023,San Francisco, CA, USA
Table 1: Datasets (DSs) of code snippets and understandability measurements/metrics used in our study. The metrics types are
“C” forcorrectness, “R”forratings,“T” fortime,and“P”forphysiological.
DS Snippets NCLOC Participants Understandability Task Understandability Metrics Meas.
1 [82] 23 CSalgorithms 6 - 20 41 students Determineprog.outputC:correct_output_rating (3-levelcorrectnessscoreforprogram output)
R:output_di ﬃculty(5-leveldi ﬃcultyscoreforprogram output)
T:time_to_give_output (seconds toread program andanswer a question)2,829
2 [71] 12 CSalgorithms 7 - 15 16 students Determineprog.outputP:brain_deact_31ant (deactivation of brainarea BA31ant)
P:brain_deact_31post (deactivation of brainarea BA31post)
P:brain_deact_32 (deactivation of brainarea BA32)
T:time_to_understand (seconds tounderstand program within 60 seconds)228
3 [15] 100OSSmethods 5 - 13 121students Rateprog.readability R:readability_level (5-levelscoreforreadability/easetounderstand) 12,100
6 [78] 50 OSSmethods 18 - 75 50 students and
13 developersRateunderst./answer QsR:binary_understandability (0/1 program understandabilityscore)
C:correct_verif_questions (%of correctanswers toveri /f_icationquestions)
T:time_to_understand (seconds tounderstand program)1,197
9 [13] 10 OSSmethods 10 - 34 104students Rateread./complete prog.C:gap_accuracy ([0-1] accuracyscorefor /f_illinginprogram blanks)
R:readability_level_ba (5-levelavg. scoreforreadabilityb/a codecompletion)
R:readability_level_before (5-levelscoreforreadabilitybeforecodecompletion)
T:time_to_read_complete (avg. seconds torate readabilityandcomplete code)716
F[69] 16 CSalgorithms 7 - 19 19 students Determineprog.outputP:brain_deact_31 (deactivation of brainarea BA31)
P:brain_deact_32 (deactivation of brainarea BA32)
R:complexity_level (scoreforprogram complexity)
C:perc_correct_output (%of subjects who correctlygaveprogram output)
T:time_to_understand (seconds tounderstand program within 60 seconds)935
reliableconclusions.Therefore, RQ2askswhetherthereisapat-
terntotheanswersto RQ1thatsummarizestheoverallcorrelation
trend. We answer RQ2statistically by combining the results of the
individualmetrics targetedby RQ1withameta-analysis.
RQ3asks whether veri /f_iability, measured using only one tool’s
warnings, correlates with comprehensibility. Intuitively, we aim
tocheck(1)ifthepatternsofcorrelations aresimilaracrosstools,
(indicating generalizability), and (2) whether any particular tool
dominates theresults. To answer thesecondpart, we usea “leave-
one-out”ablationanalysis,droppingeachtoolindividually. RQ4
asks whether there is any di ﬀerence in correlation between ver-
i/f_iability and di ﬀerent proxies for code comprehensibility. Based
on prior work [ 62], we focus on four metric categories: correctness ,
rating,time,andphysiological .Together,theanswersto RQ3and
RQ4helpusexplainour RQ1andRQ2results:theyexplorethe
tool(s) andmetric(s) responsible for the observedcorrelations.
ToanswerourRQs,we /f_irstcompiledasetofhuman-basedcode
comprehensibilitymeasurementsfrompriorstudies(section 2.1).
Then, we de /f_ined our metric for code veri /f_iability (section 2.2) and
executedfourveri /f_ication-basedtoolsonthesamecodesnippets
to measure how often each snippet cannot be veri /f_ied (sections 2.3
and2.4). Next, we conducted an analysis of the warnings produced
by the veri /f_iers to ensure that they met our de /f_inition of “false
positive” (section 2.5). Finally, we correlated the comprehensibility
metrics with the number of warnings produced by the tools and
analyzed the correlation results using a meta-analysis (section 2.6).
We did a correlation study rather than try to establish causation
because that would require expensive controlled experiments with
humansubjects.Sincecorrelationcannotexistwithoutcausation,it
is practical to re-use existing studies and establish correlation /f_irst
beforeattemptingacausationstudy,whichweleaveasfuturework.
2.1 CodeandUnderstandability Datasets
We used existing datasets (DSs) from six prior understandability
studies[13,15,69,71,78,82],whicharesummarizedintable 1.Each
study used a di ﬀerent set of code snippets and proxy metrics tomeasureunderstandabilityusingdi ﬀerentgroupsofhumansubjects
whoperformedspeci /f_ic understandabilitytasks—see table 1.
Toselectthesedatasets,weleveragedthesystematicliterature
review conducted by Muñoz et al.[62], who found ten studies that
measuredcodeunderstandabilitywithpubliclyavailabledata.From
these ten studies, we selected the /f_ive studies whose snippets were
writteninJava,sincetheveri /f_iersweconsideronlyworkonJava
code—seesection 2.3.Toidentifythedatasetsandfacilitaterepli-
cation, we use the same nomenclature asMuñoz et al.’s: DS1,DS2,
DS3, DS6, and DS9. Since those /f_ive studies were conducted before
2020,weperformedaliteraturesearchofadditionalcomprehensi-
bilitystudiesfrom2020toearly2023andfoundtheone byPeitek
et al.[69](Dataset ForDSF), whoalsousedJava snippets.
In total, we used ≈18k understandability measurements (see the
“Meas.”columnintable 1)for211Javacodesnippets,collectedfrom
364 human subjects using 20 metrics. The 211 snippet programs
are 5 to 75 non-comment/blank LOC or NCLOC—17 NCLOC on
avg.—with di ﬀerent complexity levels, as reported by the method-
ology of their respective studies [ 13,15,69,71,78,82]. Datasets
3, 6, and 9 derive from open source software projects (OSS)— e.g.,
Hibernate, JFreeChart, Antlr, Spring,& Weka [ 13,15,78]—and the
other datasets are implementations of algorithms from 1st-year
programming courses ( e.g., reversing an array) [ 69,71,82]. The
originalstudiesselectedshortcodesnippetstocontrolforpotential
cofoundingfactors that maya ﬀectunderstandability[ 69,71,82].
Weselectedtheunderstandabilitymetricsusedinthemeta-study
conducted by Muñoz et al.[62]—see table 1for the metrics, their
type,andabriefdescriptionofthem(ourreplicationpackagehas
fulldescriptions[ 27]).WealsousedMuñoz etal.’scategorization
of the metrics. Correctness metrics (marked with a Cin table1)
measurethecorrectnessoftheprogramoutputgivenbythepar-
ticipants. Time(T) metrics measure the time that participants took
toread,understand,orcompleteasnippet. Rating(R)metricsin-
dicate the subjective rating given by the participants about their
understanding of the code snippet or code readability, using Likert
213ESEC/FSE ’23, December3–9, 2023,San Francisco, CA, USA KobiFeldman,MartinKellogg, andOscar Chaparro
scales.Physiological (P) metrics measure the concentration level of
the participants during program understanding,via deactivation
measurementsofbrainareas( e.g.,BrodmannArea31orBA31[ 82]).
Study participants were mostly CS undergraduate/graduate stu-
dents with intermediate-to-high programming experience, as re-
ported in the original papers [ 13,15,69,71,78,82]. Only DS6’s
study includedprofessional developers [ 78]—see table 1.
Weusedalltheavailabledatafromthesixoriginalstudies.Their
measurements come in aggregate or individual form: e.g., the phys-
iological measurements in DS2 [ 71] and DSF [ 69] are provided
persnippetaveragedacrossparticipants,whiletheDS3measure-
mentscomeforeachparticipantandsnippet[ 15].Somestudiesalso
includedanunevennumbersofparticipantspersnippet,duetodif-
ferentmethodologicaldecisions.Forexample,DS9’sstudyincluded
arandomassignmentofparticipantstooneofsixsequencesof /f_ive
snippets [ 13]. In DS6’s study [ 78], six snippets were understood by
eightparticipantsandtheremaining44snippetswere understood
bynine.Forthesereasons,eachdataset’snumberofmeasurements
(see the “Meas.” column in table 1) is not always divisible by the
number ofsnippetsandparticipants.
2.2 ProxyforCodeVeri /f_iability
We de/f_inecode veri/f_iabilityas theeﬀortthat a developer incurs
when using a veri /f_ication tool to prove safety properties about a
snippet of code. Since measuring this e ﬀort is infeasible without
runningastudywithveri /f_icationtoolusers,weuse falsepositive
countsas an automatable proxyfor veri /f_iability.
Wede/f_inea“falsepositive”asaveri /f_ierwarningthatindicates
the veri/f_ier is unable to prove that a code snippet is correct due
to aweakness intheveri /f_ier(i.e.,undecidablilty [ 74])ordueto a
missingcodespeci /f_ication,whichahumanshouldprovide.Ine ﬀect,
a false positive warning represents a “fact” about the code that the
veri/f_ier needs,but cannotprove withthe snippet’scode only.
Our de/f_inition of “false positive” di ﬀers from the typical one
whenevaluatingtheprecisionandrecallofaveri /f_ier.Inthatcontext,
itisassumedthatcorrectspeci /f_icationsareexplicitandavailable,
and“falsepositive”meansafactthattheveri /f_iercannotprove,even
withaspeci /f_ication.Conversely,inourcontext,wewantaproxy
for thediﬃcultyof verifying a snippet. That di ﬃculty includes
bothwritingspeci /f_icationsandsuppressingfalsepositivewarnings,
so it is sensible to include both in our proxy for veri /f_iability. In
otherwords,thefewerwarningsaveri /f_ierissues,thelessworka
developer using the veri /f_ier mustdoto verifyacode snippet.
Weconsideredtwootherproxiesforveri /f_iability:numberoffalse
positives after writing speci /f_ications and number of “facts” veri /f_ied
aboutthecode.Wediscardedtheformerproxybecausethefullcon-
textofhowthesnippetsareintendedtobeusedisnotavailableand
becausewritingspeci /f_icationsiserror-andbias-prone.Thelatter
wasdiscardedbecausenoneoftheveri /f_iersprovidetheproxydi-
rectly,andbecauseapproximatingwhetherornotaveri /f_ierneedsto
evencheckafactisundecidableforsomepropertiesconsideredbya
veri/f_ier (e.g., determining what is considered a resource in the code
byaresourceleakveri /f_ier [48]),soaprecise count isimpossible.
2.3 Veri /f_ication Tools
We usedthe following criteriato selectveri /f_ication tools:(1)Eachtoolmustbebasedonasoundcore— i.e.,theunderlying
techniquemustgenerateaproof.
(2) Eachtoolmustbe activelymaintained.
(3) Eachtoolmustfail to verifyat leastone snippet.
(4) Eachtoolmustrun mostly automatically.
(5) Eachtoolmusttarget Java.
Criterion 1 requires that each tool be veri /f_ication-based. Our
hypothesis impliesthat the process ofveri /f_icationcanexposecode
complexity: that is, our purpose in running veri /f_iers is not to ex-
pose bugs inthe code but to observe when the tools produce false
positive warnings (due to code complexity). Therefore, each tool
must perform veri /f_ication under the hood ( i.e., must attempt to
constructaproof)forourresultstobemeaningful.Thiscriterion
excludes non-veri /f_ication static analysis tools such as FindBugs [ 7]
whichuseunsoundheuristics.Exploringwhetherthosetoolscor-
relate with comprehensibility is future work. However, criterion 1
doesnotrequire the tool to be sound: merelythat it be based ona
sound core. We permit soundiness [55] (and intentionally-unsound
tools) because practical veri /f_ication tools commonly only make
guarantees aboutthe absenceof defectsundercertainconditions.
Criteria 2through 5 are practical concerns. Criterion2 requires
the veri/f_ier to be state-of-the-art so that our results are useful to
thecommunity.Criterion3requireseachveri /f_iertoissueatleast
onewarning—fortoolsthatverifyapropertythatisirrelevantto
the snippets (and so do not issue any warnings), we cannot do a
correlationanalysis.Criterion4excludesproofassistantsandother
tools that require extensive manual e ﬀort. Criterion 5 restricts the
scopeofthestudy:wefocusedonJavacodeandveri /f_iers.Wemade
thischoicebecause(1)veri /f_iersareusuallylanguage-dependent,(2)
manypriorcodecomprehensibilitystudiesonhumansubjectsused
Java—e.g.,5/10studiesinMuñoz etal.[62]andnootherlanguage
hasmorethan2/10studies—and(3)Javahasreceivedsigni /f_icant
attentionfromtheprogramveri /f_icationcommunityduetoitspreva-
lence in practice. We discuss the threats to validity that this and
otherchoicescause insection 4.
2.3.1 Selected Verification Tools. By applying the criteria de /f_ined
above,we selectedfourveri /f_ication tools:
Infer[16] is anunsound, industrial static analysis tool based on
asoundcoreofseparationlogic[ 65]andbi-abduction[ 17].Sepa-
ration logic enables reasoning about mutations to program state
independently, making it scalable; bi-abduction is an inference pro-
cedure that automates separation logic reasoning. Infer is unsound
by design: despite internally using a sound, separation-logic-based
analysis, it uses heuristics to prune all but the most likely bugs
from its output, because it is tailored for deployment in industrial
settings.Inferwarnsaboutpossiblenulldereferences,dataraces,
andresourceleaks.We usedInfer version1.1.0.
TheCheckerFramework [68]isacollectionofpluggabletype-
checkers[ 30],whichenhanceahostlanguage’stypesystemtotrack
an additional code property, such as whether each object might be
null. The Checker Framework includes many pluggable typecheck-
ers. We used the nine that satisfy criterion 4, which prevent pro-
gramming mistakes related to: nullness [ 24,68], interning [ 24,68],
objectconstruction[ 47],resourceleaks[ 48],arraybounds[ 46],sig-
naturestrings[ 24],formatstrings[ 101],regularexpressions[ 86],
andoptionals[ 92]. We usedChecker Framework version3.21.3.
214On the RelationshipbetweenCode Verifiability andUnderstandability ESEC/FSE ’23, December3–9, 2023,San Francisco, CA, USA
Table 2:Numberofsnippets eachtoolwarns on andthetotal numberofwarnings perdataset.
Snippets Warned On TotalWarnings
Tool \ Dataset 1 2 3 6 9 F All 1 2 3 6 9 F All
Infer 0/23 0/12 1/100 5/50 0/10 1/16 7/211 0 0 1 7 0 1 9
CheckerFr. 3/23 0/12 18/100 28/50 4/10 3/16 56/211 7 0 51 83 4 3 148
JaTyC 3/23 1/12 88/100 40/50 10/10 2/16 144/211 14 3 327 537 37 6 924
OpenJML 14/23 6/12 69/100 41/50 10/10 13/16 153/211 29 11 808 219 24 29 1,120
All Tools 17/23 7/12 93/100 48/50 10/10 15/16 190/211 50 14 1,187 846 65 39 2,201
TheJava Typestate Checker (JaTyC) [60] is a typestate anal-
ysis[88].Atypestate analysisextendsatypesystemtoalsotrack
states—forexample,atypestatesystemmighttrackthata Fileis/f_irst
closed, then open, then eventually closed. Currently-maintained
typestate-basedJavastaticanalysistoolsincludeJaTyC[ 60](atype-
stateveri/f_ier)andRAPID[ 25](anunsoundstaticanalysistoolbased
onasoundcorethatpermitsfalsenegativeswhenveri /f_icationisex-
pensive).WechosetouseJaTyCratherthanRAPIDfortworeasons.
First,JaTyCshipswithspeci /f_icationsforgeneralprogrammingmis-
takes, but RAPID focuses on mistakes arising from mis-uses of
cloud APIs; the snippets in our study do not interact with cloud
APIs. Second, JaTyC is open-source, but RAPID is closed-source.
JaTyCwarnsaboutpossiblenulldereferences,incompleteprotocols
on objects that have a de /f_ined lifecycle (such as sockets or /f_iles),
andaboutviolationsofitsownershipdiscipline,whichissimilarin
spiritto Rust’s[ 50]. We usedJaTyC commit b438683.
OpenJML [93]convertsveri /f_icationconditionstoSMTformulae
and dispatches those formulae to an external satis /f_iability solver.
OpenJML veri /f_ies speci/f_ications expressed in the Java Modeling
Language(JML)[ 54];itisthelatestinaseriesoftoolsverifyingJML
speci/f_ications by reduction to SMT going back to ESC/Java [ 29].
OpenJMLveri /f_iestheabsenceofacollectionofacommonprogram-
mingerrors,includingout-of-boundsarrayaccesses,nullpointer
dereferences, integer over- and under /f_lows, and others. We used
OpenJML0.17.0-alpha-15 withthe defaultsolver z3 [ 23]v.4.3.1.
2.3.2 VerificationToolsConsideredbutNotUsed. Weconsidered
and discarded three other veri /f_iers: JayHorn [ 44], which fails cri-
terion 2 [ 79]; CogniCrypt [ 51], which fails criterion 3; and Java
PathFinder [ 38], whichfails criterion4.
2.4 SnippetPreparationandToolExecution
We acquired the snippets from prior work [ 62,69] but had to make
some modi /f_ications to prepare them for tool execution. DS3 in-
cluded 4 commented-out snippets, which we uncommented. To
make the snippets compilable, we created “stubs” for the classes,
methodcalls, etc.theyusewithoutmodifyingthesnippetsthem-
selves. Since the the snippetsthemselves didnot change,theirun-
derlying,measuredcodecomprehensibilitydidnotchangeeither:
intheoriginalstudies,thesnippetswereprovidedtothehumans
in isolation. At the same time, our modi /f_ications would change the
programs’ state if the snippets were to be executed. We performed
a manual analysis of tool warnings to ensure our modi /f_ications did
notcausespuriouswarnings(seesection 2.5).Wecreatedscripts
to execute the veri /f_iers on the snippets and display all veri /f_ication
failures for each tool. Table 2shows descriptive statistics of the
warnings issuedbyeachtooloneachdataset.2.5 CodeCorrectnessandWarningValidation
Weassumedthateverywarningissuedbyaveri /f_ieraboutasnippet
is a “false positive”, according to the de /f_inition we presented in
section2.2.Ineﬀect,thismeansthatwearetreatingthesnippetsas
if they are correct. For example, if a snippet dereferences a pointer
withoutanullcheck,weassumethatpointerisnon-null;ifasnippet
accessesanarraywithoutcheckingabound,weassumethatthe
bound was checked elsewhere in the program, etc.Each veri /f_ier
warning therefore represents some fact that the veri /f_ier needs,
butcannotprovewiththesnippet’scodeonly.Weconsiderthese
reasonable assumptions because: (1) no context about the snippets
is available (or was presented to the human subjects in the prior
studies), and (2) the snippets are likely to be correct as researchers
showedthemto humansinprior comprehensibilitystudies.
However,tocheckthatthese assumptionsdonot skewour cor-
relation analysis, we manually validated whether a representative
sampleoftoolwarningswereindeedfalsepositives(accordingto
ourde/f_initionfromsection 2.2).Afterexecutingtheveri /f_iers,one
author examined a representative subset of the warnings (a sample
of 344 of 2,201, at 95% con /f_idence level and 5% of error margin)
andrecordedthecauseofeach.Asecondauthorexaminedthe /f_irst
author’s assessment and both authors discussed the cases where
the assessment was incorrect or needed more details, reaching con-
sensus in case of disagreements (of which there were < 5). Of these
344warnings,nonewere“realbugs”inthesensethattheyareguar-
anteed to make the code fail when executed. Many do represent
potential bugs: that is, code that does not check boundary condi-
tionssuchasnullability;however,thesewarningscouldberemoved
by writing a speci /f_ication for the relevant veri /f_ier indicating the
assumptionsmade by the snippet. This means these warnings are
allfalse positives, according to our de /f_inition from section 2.2.
Inthesample,themostcommonreasonsforaveri /f_iertowarn
were:(1)violation’sofJaTyC’sRust-likerulesformutability,and
2)violationsoftheveri /f_iers’assumptionsaboutnullability.Other
common causes were possible integer over- and under /f_lows, too
large or too small array indexes, and unsafe casts. Our analysis of
warnings for each snippet indicates a fairly uniform distribution of
warning types over the datasets. Our replication package provides
our detailedanalysisofwarnings [ 27].
2.6 Correlation andAnalysis Methods
2.6.1 Aggregation. We aggregated the comprehensibility measure-
mentsandthenumberoftoolwarningsforeachcodesnippetinthe
datasets.Theresultingpairsofcomprehensibilityandveri /f_iability
valuesper snippetwere correlatedfor setsof snippets.
215ESEC/FSE ’23, December3–9, 2023,San Francisco, CA, USA KobiFeldman,MartinKellogg, andOscar Chaparro
Speci/f_ically, we averaged the individual code comprehensibil-
ity measurements per snippet for each metric. For example, for
eachsnippetinDS1weaveragedthe 41 time_to_give_output mea-
surements collected from the 41 participants in the correspond-
ingstudy[ 82].FollowingMuñoz etal.[62],weaverageddiscrete
measurements, which mostly come from Likert scale responses
in the original studies. For example, the metric output_di ﬃculty
(from DS1) is the perceived di ﬃculty in determining program out-
putusing a 0-4 discretescale. While there is noclearindication of
whetherLikertscalesrepresentordinalorcontinuousintervals[ 61],
we observed that the Likert items in the original datasets repre-
sentdiscretevaluesoncontinuousscales[ 62],soitisreasonable
to average these values to obtain one measurement per snippet.
Allphysiologicalmeasurementsgivenbytheoriginalstudiesare
averagedacrossallparticipants whounderstoodagiven snippet.
Regardingcodeveri /f_iability,wesummedupthenumberofwarn-
ings from the veri /f_ication tools for each snippet. We considered
averaging rather than summing up. However, since the correlation
coeﬃcientthatweused(seebelow)isrobusttodatascaling( i.e.,the
averageisessentiallyascaledsum),imbalancesinthenumberof
warnings from each tool do not change the correlation results. Fur-
ther, forRQ3, we performed an ablation experiment to investigate
possible e ﬀectsofwarning imbalances oncorrelation.
2.6.2 StatisticalMethods. WeusedKendall’s /u1D70F/u1D70F/u1D70F[49]tocorrelatethe
individualcomprehensibilitymetricsandthetoolwarningsbecause
(1) it does not assume the data to be normally distributed and have
alinearrelationship[ 20],(2)itisrobusttooutliers[ 20],and(3)it
has been used in prior comprehensibility studies [ 62,69,78]. As
in previous studies [ 69,78], we follow Cohen’s guidelines [ 20] and
interpretthecorrelationstrengthas nonewhen0≤|/u1D70F|<0.1,small
when0.1≤|/u1D70F|<0.3,mediumwhen0.3≤|/u1D70F|<0.5, andlarge
when0.5≤|/u1D70F|.
Toanswer RQ1,we/f_irststatedtheexpectedcorrelation(asei-
therpositiveornegative)betweeneachcomprehensibilitymetric
and code veri /f_iability that would support our hypothesis. For some
metrics,suchas correct_output_rating inDS1,a negativecorrelation
indicates support for the hypothesis—if humans can deduce the
correct output moreoften, the hypothesis predicts a lowernumber
ofwarningsfromtheveri /f_iers.Apositiveexpectedcorrelation,such
asfortime_to_understand inDS6,indicates thathighervaluesin
thatmetricsupportthehypothesis— e.g.,ifhumanstake longerto
understand a snippet, our hypothesis predicts that morewarnings
willbeissuedonthatsnippet.Wecomputedthecorrelation(and
its strength) between the comprehensibility metrics and code veri-
/f_iability and compared the observed correlations with the expected
ones to checkif the results validate orrefuteour hypothesis.
Toanswer RQ2,weperformedastatisticalmeta-analysis[ 11]
oftheRQ1correlationresults.Ameta-analysisisappropriatefor
answering RQ2because it combines individual correlation results
that come from di ﬀerent metrics as a single aggregated correla-
tionresult[ 11,62].Indisciplineslikemedicine,ameta-analysisis
used to combine the results of independent scienti /f_ic studies on
closely-related research questions ( e.g., establishing the e ﬀect of
atreatmentforadisease),where eachstudyreportsquantitative
results (e.g., a measured e ﬀect size of the treatment) with somedegreeoferror[ 11].Themeta-analysisstatisticallyderivesanes-
timateoftheunknowncommontruth( e.g.,thetruee ﬀectsizeof
the treatment), accounting for the errors of the individual stud-
ies. Typically, a meta-analysis follows the random-e ﬀects model
to account for variations in study designs ( e.g., diﬀerent human
populations)[ 11].Intuitively,arandom-e ﬀects-basedmeta-analysis
estimatesthetruee ﬀectsizeastheweightedaverageofthee ﬀect
sizesoftheindividualstudies[ 11],wheretheweightsareestimated
viastatisticalmethods( e.g.,SidikandJonkman’s[ 80]).
Since the comprehensibility measurements come from di ﬀerent
studies with di ﬀerent designs ( i.e., with diﬀerent goals, comprehen-
sibility interpretations and metrics, code snippets, human subjects,
etc.), arandom-e ﬀects meta-analysisis appropriate to estimatean
aggregated correlation.In our case, however,we /f_irst combine the
resultsoftheindividualcorrelationanalyses( i.e.,foreachmetric)
for each dataset into a single aggregated correlation per dataset, to
avoidthe“unit-of-analysis”problem(see[ 37],§3.5.2).Thisproblem
arises in meta-analysis when there are inputs that are not inde-
pendent ( i.e., are themselves correlated), typically because they
representmultiplemeasurementsobtainedonthesamepopulation.
Because most of our datasets include multiple metrics that were
derived from the same subjects and snippets, and therefore, are
related(e.g.,readability_level_ba andreadability_level_before from
DS9dependononeanother),anaïveapplicationofmeta-analysis
that treated each metric as independent would over-weight studies
with multiple metrics, because it would “double-count” their sta-
tistical power ( i.e., multiply the statistical power of the study by
the number of metrics it contains). We con /f_irmed that most of the
combinationsofmetricswithinasinglestudyshowedmediumor
largecorrelations(19/28combinationsaremediumorlargecorre-
lations; of those, 13 are large), so the “unit-of-analysis” problem
could seriously skewour results.
Dealingwiththeunit-of-analysisprobleminmeta-analysesof
smallnumbersofstudies(asinour case)with multiplecorrelated
metricsisanopenprobleminstatisticalmethodsresearch.Wecon-
sidered the recently-proposed correlated and hierarchical e ﬀects
(CHE)model[ 73],butdiscoveredthat(forourdata)itwashighly
sensitivetothechoiceofthe rhoparameter(whichrepresentsan
assumptionabouthowmuchvariancethereisbetweenthedi ﬀerent
metricsineachstudy).Sincewewantedtobeconservativeinour
choiceofstatisticalmethod,wechosethe“bruteforce”aggregation
approachsuggestedby[ 37],whichtradesstatisticalprecisionfor
simplicity and conservatism: it combines the correlation results of
the various metrics ineach study into a single estimateof correla-
tion,whichguaranteesthatnostatisticalpowerisderivedfromthe
presence ofmultiple metrics on thesamepopulation (even if such
power might be warranted). Though it also has a rhoparameter,
the results for our data are insensitive to the choice of rho, with
extremelyhighandlowvaluesof rhogivingnearly-identicalresults.
Allmeta-analysesinsection 3userho =0.6.
To perform the random-e ﬀects meta-analysis, we followed a
standard procedure for data preparation and analysis [ 11]. First,
we transformed Kendall’s /u1D70Fvalues into Pearson’s /u1D45F/u1D45F/u1D45Fvalues [100].
Then,wetransformedthe /u1D45Fvaluestobeapproximatelynormallydis-
tributed,usingFisher’sscaling.Next,wenormalizedthesignsofthe
individualmetriccorrelations( i.e.,the/u1D45Fvalues)sothatanegative
216On the RelationshipbetweenCode Verifiability andUnderstandability ESEC/FSE ’23, December3–9, 2023,San Francisco, CA, USA
correlation supports our hypothesis (the choice of negative is arbi-
trary; choosing positive leads to the same results with the opposite
sign):wemultipliedby-1thecorrelationvalueformetricswhere
apositivecorrelation wouldsupportthehypothesis. Thisstrategy
hasbeenusedinotherdisciplineswhencombiningdi ﬀerentmetrics
whosesignshaveoppositeinterpretations, e.g.,in[97].WeusedR’s
dmetarpackage (version 0.0.9000)toaggregatethe correlations of
the metrics from each study [ 37], and the R’s metaforpackage [ 99]
(version3.8-1)torunthemeta-analysisandgenerateforestplots
tovisualizethePearson’s /u1D45Fvalues,theirestimatedcon /f_idencein-
tervals,theestimatedweightsfortheaggregatedcorrelation,and
additionalmeta-analysisresults ( e.g.,p-valuesandheterogeneity).
Toanswer RQ3,weappliedthesamemethodologyas RQ2for
each individual tool’s warnings ( i.e., no aggregation was used).
We also performed a “leave one tool out” ablation experiment to
checkifanysingletoolwasdominatingtheoverallmeta-analysis
results. To answer RQ4, we repeated the same methodology for
onlythemetricsineachmetriccategory: time,correctness ,rating,
andphysiological —i.e.,weperformedfourmeta-analyses,onefor
eachmetric group.
Whileweprovidethe /u1D45D-valuesofallofthesestatisticalanalyses,
weemphasizethattheyshouldbeinterpretedwithcautiongiventhe
relativelysmallsamplesizes(and,for RQ1,thatfactthat20metrics
areconsidered).Forexample,DS2onlycontains12snippets,which
means only 12 data points were used for correlation for its metrics.
We also used a meta-analysis ( RQ2-RQ4 ) because interpreting the
individualmetricresults( RQ1)todrawgeneralconclusionsforour
hypothesiscanbemisleading[ 12].Ourmeta-analysisalsoobviates
the need for statistical correction to avoid multiple comparisons,
suchasHolm-Bonferroni’s[ 40]:themeta-analysisaggregatesall
of the results and informs us of the overall trend. We use the same
interpretation guidelines for Pearson’s /u1D45Fvalues that we used for
Kendall’s /u1D70F:smallwhen0.1≤|/u1D45F|<0.3,etc.[20,69,78].
3 STUDYRESULTS AND DISCUSSION
We present and discuss the results of our study in this section.
Scripts and data that generate these results are available in our
replication package [ 27].
3.1 RQ1: Individual Correlation Results
Table3summarizestheresultsofeachmetric’scorrelation(based
on Kendall’s /u1D70F) with the total number of warnings from all tools.
Weprovidedescriptivestatisticsabouttheseresults,butweempha-
size that these results should be interpreted with caution: “vote-
counting” ( e.g., checking the number of statistically-signi /f_icant
metrics ineach direction) canlead to misleading conclusions [ 12].
Weavoiddirectlydrawingconclusionsfromtheseresultsandin-
stead, we investigate the aggregated trend of all metrics with a
meta-analysisinsection 3.2.
Table3showsthatfor13ofthe20(65%)metrics,thedirectionof
thecorrelationsupportsourhypothesis.For4metrics,thereis no
correlation,andfortheremaining3metrics,thecorrelationisin
theoppositedirectionthanexpected.Table 3indicatesthestrength
ofthecorrelationintherightmostcolumn.Ofthemetricswherewe
found amediumor higher correlation, 8/8 are in the direction that
supportsourhypothesis.Fortheother5metricsthatsupportourTable3:CorrelationresultsbasedonKendall’s /u1D70F/u1D70F/u1D70F(K.’s/u1D70F/u1D70F/u1D70F)for
each dataset (DS) and Metric. A metric falls into one Type:
Correctness, Time,Rating,&Physiological.Theexpectedcor-
relationdirection( Exp.Cor. ),ifour hypothesisiscorrect,is
either Positive or Negative. We assess /u1D70F/u1D70F/u1D70F’s direction/strength,
compared to the expected correlation ( Exp?): ‘-’ means no
correlation, ‘Y/y’ means expected and measured correlations
match (thus supporting our hypothesis), and ‘N/n’ means
theydonotmatch.Capitallettersindarkercolors( Y/N)
mean amedium or higher correlation. Lowercase letters and
lighter colors ( y/n) mean a smallcorrelation. /u1D70F/u1D70F/u1D70F’s signi/f_i-
canceistested at the /u1D45D<0.05/u1D45D<0.05/u1D45D<0.05(*) &/u1D45D<0.01/u1D45D<0.01/u1D45D<0.01levels(**).
DS Metric Type Exp.Cor. K.’s /u1D70F/u1D70F/u1D70FExp?
1correct_output_rating C Negative -0.34* Y
output_di ﬃculty R Negative -0.43** Y
time_to_give_output T Positive 0.41** Y
2brain_deact_31ant P Negative -0.31 Y
brain_deact_31post P Negative -0.45 Y
brain_deact_32 P Negative -0.38 Y
time_to_understand T Positive 0.14 y
3readability_level R Negative -0.17* y
6binary_understand R Negative 0.01 -
correct_verif_questions C Negative 0.02 -
time_to_understand T Positive 0.05 -
9gap_accuracy C Negative -0.34 Y
readability_level_ba R Negative 0.08 -
readability_level_before R Negative 0.13 n
time_to_read_complete T Positive -0.23 n
Fbrain_deact_31 P Negative -0.18 y
brain_deact_32 P Negative -0.18 y
complexity_level R Positive 0.35 Y
perc_correct_out C Negative -0.16 y
time_to_understand T Positive -0.13 n
hypothesis and the 3 metrics that do not, their correlation is small.
Whilewecannotdirectlydrawconclusionsfromtheseresultsabout
theoveralltrend,theyaresuggestive.Weexaminetheaggregate
trend rigorouslywithameta-analysisinsection 3.2.
Withregardtometriccategories,3/4correctness( C)metrics,3/6
rating(R)metrics,2/5time( T)metrics,and5/5physiological( P)
metrics correlate with veri /f_iability. All 3 metrics that anti-correlate
withveri/f_iabilityareconcentratedintheratingandtimecategories.
These two metric categories are the most subjective: ratings are
opinions, and some time metrics require the human subjects to
signaltheexperimenterwhentheycompletethetask.Theseresults
suggest that there may be a relationship between metric categories
and the correlation with veri /f_iability; we further investigate the
diﬀerences between metric categoriesinsection 3.4.
3.2 RQ2: AggregateCorrelation Results
Becausedirectinterpretationoftable 3isdiﬃcultduetothedi ﬀerent
samplesizesofthevariousstudies,weperformedameta-analysis
tounderstandtheoveralltrend(see /f_ig.1).Asnotedinsection 2.6.2,
217ESEC/FSE ’23, December3–9, 2023,San Francisco, CA, USA KobiFeldman,MartinKellogg, andOscar Chaparro
RE Model
−0.91 −0.46 0.46
Pearson's r (negative correlation supports our hypothesis)f96321
1610501001223
10.69%   −0.36 [ −0.73,  0.16] 6.34%    0.01 [ −0.62,  0.64]25.39%    0.03 [ −0.25,  0.30]34.83%   −0.22 [ −0.40, −0.02] 7.89%   −0.43 [ −0.80,  0.20]14.85%   −0.52 [ −0.77, −0.14]
  100%   −0.23 [ −0.46,  0.03]Dataset Estimate [95% CI] # of Snippets Weight
p = 0.07 Heterogeneity test: Q = 6.74, df = 5, p = 0.24
Figure 1: Results of the random-e ﬀects meta-analysis of the
metrics in table 3, after aggregating the results by dataset to
avoid theunit-of-analysisproblem(section 2.6.2).
the results are presented by dataset rather than by metric to avoid
unit-of-analysiserrors[ 37].
The forest plot in /f_ig.1displays the observed correlation (Per-
son’s/u1D45F/u1D45F/u1D45Fvalue, obtained from the Kendall’s /u1D70Fvalue as described
insection 2.6.2)andthe95%con /f_idenceinterval( Estimate[95%
CI]), as well as the estimated weight for each dataset ( Weight).
This information is shown numerically and graphically in /f_ig.1.
Eachbox’ssizeisthedataset’s estimatedweight (alargerbox size
meansa largerweight),andthe box’smiddlepointrepresents the
correlationwithrespecttothedashedverticallineatzero.There
is a negative correlation ifthe box isto theleft ofthe verticalline;
positiveifitistotheright;allmetricshavebeennormalizedsothat
the expected correlation is negative (that is, a negative correlation
supports our hypothesis). The horizontal lines visualize the con-
/f_idence intervals for each dataset. At the bottom, the plot shows
theaggregatedcorrelation(ontheright)andrelatedinformation
calculated by the meta-analysis. The diamond at the bottom of the
plotvisualizestheaggregatedcorrelation;thewidthofthediamond
represents the con /f_idenceinterval.
Figure1shows a small aggregated correlation supporting an
aﬃrmative answer to RQ2(/u1D45F=−0.23, with a 95% CI that con-
tains negligible,small,medium correlations: /u1D45F=−0.46to/u1D45F=0.03,
/u1D45D=0.07). We interpret these results overall as support for the
hypothesisthat tool-basedveri /f_iability andhumans’abilityto un-
derstand code are correlatedto someextent.
The heterogeneity of the considered studies is non-negligible
(/u1D43C2=(/u1D444−/u1D451/u1D453)//u1D444=(6.74−5)/6.74=25.8%– not shown in /f_ig.1),
indicating that 25.8% of the correlation variation ( i.e., variance) we
observeisduetothestudiesmeasuringdi ﬀerentfactorsratherthan
due to chance. This result validates our choice of a random-e ﬀects
modelfor the meta-analysis.
The plots in /f_ig.1show wide con /f_idence intervals for all the
datasets except DS 3 and DS6, which indicates relatively high vari-
ability in the correlations. This indicates that most of these studieswere under-powered for our purpose: the number of snippets con-
sidered was not high enough to give the meta-analysis much con /f_i-
denceinthecorrelationresults.Themeta-analysiscorrespondingly
givesthelargestweightstothetwodatasetswiththemostsnippets:
about35%weighttoDS3 (with100snippets)andabout25%weight
to DS6 (with 50 snippets). Future work should explore running
understandability experiments with larger numbers of snippets,
whichwouldenable usto gain further con /f_idenceinour results.
3.3 RQ3: Correlation Results By Tool
Toanswer RQ3,werepeatedtheanalysesusedtoanswer RQ1and
RQ2independentlyforeachtool( i.e.,nowarningaggregation).We
alsorepeatedtheanalysisina“leave-one-out”ablationexperiment.
Wereportonlythesummaryresultsforeachtool( i.e.,theresults
ofthemeta-analyses)forspacereasons;forestplotssimilarto /f_ig.1
as well as the individual correlation results on each tool+metric
combination are available inour replication package [ 27].
Repeating the meta-analysis on only the warnings produced by
eachtoolindividuallygavesimilarresultstothemeta-analysisin
/f_ig.1,exceptforInfer.TheCheckerFrameworkresultssupportsour
hypothesismorestronglythantheoverallmeta-analysis( /u1D45F=−0.26,
95% CI of [−0.44,−0.06],/u1D45D=0.02). The results of OpenJML and
JaTyC support the hypothesis more weakly than the overall results
(/u1D45F=−0.12, with a 95% CI of [−0.29,0.07],/u1D45D=0.16for OpenJML
and/u1D45F=−0.17with a 95% CI of [−0.39,0.08],/u1D45D=0.14for JaTyC).
Inferhastoofewwarningstodrawmeaningfulconclusionsfrom
its results ( /u1D45F=−0.09witha95%CIof [−0.94,0.91],/u1D45D=0.60).
From theseresults, we conclude that, while sometools support
the hypothesis less strongly than the overall meta-analysis, all the
toolsbutInfershowthesametrend.Theseresultssupporttheover-
allmeta-analysisresults( RQ2):thecorrelationmeasuredfor3of
the 4 studied tools suggests that the correlation between veri /f_ia-
bility and understandability indeed exists (in small magnitude); no
tool shows a markedly di ﬀerent trend except Infer, whose trend is
not meaningfuldueto its small warning count.
We were alsoconcerned thatasingle tool might be dominating
theoverallresults.Tomitigatethisthreat,weperformedanablation
study by repeating the meta-analysis on warning data aggregated
from each combination of three tools ( i.e., excluding thewarnings
ofonetoolonly).Overall,theresultsareextremelysimilarforeach
combination of tools to the overall results—the results without
Infer are in fact nearly identical—with /u1D45Fvalues ranging from −0.23
to−0.20; CI lowerbounds ranging from −0.46to−0.37, and CI
upperbounds ranging from −0.01to0.06; and/u1D45Dvalues from 0.04
to0.10.We concludefromthisablationexperimentthatnosingle
tooldominatesthe RQ2results.
Taken together, the results in this section show that the correla-
tionfoundfor RQ2isnotentirelydrivenbyanytool:theoverall
resultsremainsimilar(ifslightlyweaker)foreverytoolindividually
exceptInferandforeachcombinationofthreetools( i.e.,without
eachtool).We interprettheseresults to mean thatthecorrelation
existsregardlessofthespeci /f_icveri/f_ierinuse—meaningthatour
results apply to veri /f_ication ingeneral.
218On the RelationshipbetweenCode Verifiability andUnderstandability ESEC/FSE ’23, December3–9, 2023,San Francisco, CA, USA
3.4 RQ4: Correlation Results by MetricType
In section 3.1, we observed that thecorrectnessandphysiological
metriccategoriesappearedtosupportourhypothesismorestrongly
than the rating and time categories. To test this observation, we
repeatedour meta-analysesfor eachofthe fourmetric categories.
The results refute the idea that these categories are a major
in/f_luenceontheresults.Thecorrectness,rating,andtimemetrics
show overall results similar to /f_ig.1, but with wider con /f_idence
intervals: /u1D45F=−0.28with 95% CI of [−0.70,0.27],/u1D45D=0.20for
correctness; /u1D45F=−0.25with 95% CI of [−0.54,0.09],/u1D45D=0.11for
rating;and /u1D45F=−0.22with95%CIof [−0.58,0.21],/u1D45D=0.23fortime.
The results for the physiological metrics show that they have a
minimal impact: /u1D45F=−0.32but with a huge 95% CI of [−1.00,0.98].
These results, especially for the physiological metrics, are likely
due to the smaller sample sizes created by considering only one
metrictype; e.g.,therearephysiologicalmetricsinonlytwodatasets
(DS2 and DSF) with 28 total snippets between them. The dataset
with the most weight in the overall results (DS3) only has rating
metrics,whichreducesthemeta-analysis’con /f_idenceintheother
types.Finally,theheterogeneityforthethreemetriccategorieswith
usefulresults( i.e.,notphysiological)ishigherthanintheoverall
results (with /u1D43C2of 57%, 49%, and 50% for correctness, rating, and
time metrics, respectively).
3.5 RobustnessExperiments
We ran additional experiments to probe the robustness of the /f_ind-
ingsfor the RQsandmitigate somethreatsto validity.
3.5.1 Handling Code Comments in Dataset 9. DS9’s original study
had3versionsofeachofits10snippets,withthreetypesofcode
comments:“good”,“bad”,andnocomments[ 13].Theresultspre-
sented elsewhere in this section used the “No comments” (NC)
version of DS9, because none of the four veri /f_iers use comments as
partoftheirlogic.However,thischoicemightbesourceofpossible
bias,soweanalyzedhowthecorrelationresultswouldchangeif
wehadusedthe“Goodcomments”(GC)or“Badcomments”(BC)
versionsofthedataset.Notethatbecausenoneoftheveri /f_ierstake
comments intoaccount,theirwarningsare exactlythesame—the
only diﬀerences are inthe comprehensibilitymeasurements.
Table4shows how the correlation results di ﬀer for the three
versions of DS9. A signi /f_icant diﬀerence is observed in the two
readability metrics: when the comments are bad, these metrics are
anti-correlatedwithveri /f_iability:thatis,humansratedthesnippets
onwhichthetoolsissuedmorewarningsas morereadable .Weseea
similarphenomenonforthetimemetrics,butitoccursonlyforthe
good(ratherthanbad)comments.Toexplainthisphenomenon,we
comparedthedistributionofthemetricsacrosscommentcategories
and analyzed the scatter plots of the data used for correlation. Our
analysis revealed that such disparity in correlation stems from
a combination of (1) outliers found in the human measurements
(likelyduetodatacollectionimprecisionsintheoriginalstudy[ 13])
and (2) the low number of data points in DS9. For example, we
found that bad comment code was rated more readable by a few
participantsthancodewithnocomments,eventhoughthesnippets
weresemanticallythesame.Theseunusualmeasurementsledto
outliersthathadaconsiderableimpactonthecorrelationresults
across comment categories(because ofthe small number ofdataTable4:Correlationresults(Kendall’s /u1D70F/u1D70F/u1D70F)ondiﬀerentversions
of DS9: "No" (NC), "Bad" (BC), and "Good Comments" (GC). A
** indicates statistical signi /f_icanceat the /u1D45D<0.01/u1D45D<0.01/u1D45D<0.01level.
Metric Exp.Cor. NC BC GC
gap_accuracy Negative -0.34 -0.18 -0.34
readability_level_ba Negative 0.08 0.44 -0.18
readability_level_before Negative 0.13 0.42 -0.05
time_to_read_complete Positive -0.23 -0.39 -0.75**
Table 5: Correlation results (Kendall’s /u1D70F/u1D70F/u1D70F) for OpenJML, for
each timeout-handling approach: (1) ignoretimeouts; (2)
under-estimate the warnings hidden by timeouts; (3) over-
estimate the warnings hidden by timeouts. /u1D70F/u1D70F/u1D70F’s signi/f_icance is
tested at the /u1D45D<0.05/u1D45D<0.05/u1D45D<0.05(*) and/u1D45D<0.01/u1D45D<0.01/u1D45D<0.01levels(**).
Approach
DS Metric 1:Ignore 2:Under 3:Over
3readability_level -0.20** -0.23** -0.17*
6binary_understand -0.07 -0.07 0.00
correct_verif -0.06 -0.06 0.00
time_to_understand 0.11 0.11 0.05
points).Thee ﬀectofthisphenomenonontheoverallresultsislow,
because DS9 is given very low weight (6.34%, lowest among all
datasets) bythe meta-analysisdueto its small sample size.
3.5.2 Handling OpenJML Timeouts. OpenJML uses an SMT solver
underthehood.ThoughmodernSMTsolversreturnresultsquickly
for most queries using sophisticated heuristics, some queries do
lead to exponential run time, making it necessary to set a time-
outwhenanalyzingacollection ofsnippets.Weuseda60minute
timeout, which led to 2/50 snippets in DS6 and 39/100 snippets
inDS3timingout(andzerointheotherdatasets).Weconsidered
threeapproachesinourcorrelationanalysis tohandletimeouts:(1)
ignoresnippetscontainingtimeoutsentirely,(2)counteachtimeout
aszerowarnings(butdocountanyotherwarningsissuedinthe
snippetbefore timingout),or(3) counteachsnippetthat timedout
asthemaximumwarningcountinthedataset.Alltheresultsfor
RQ1-RQ4 were produced by following approach 3.The reasonwe
choseapproach 3overapproach 2isthat timeoutstypicallyoccur
on the most complicated SMT queries, which might hide many
warnings. Therefore, approach 2 underestimates the warning count
thatano-timeoutrunofOpenJMLwouldencounter,whileapproach
3overestimates thewarningcountinano-timeoutrun.Were-ran
thecorrelation analysisunderallthreeconditions.The resultsare
in table5and do not show any signi /f_icant diﬀerences between the
strategies for timeouts—the overall direction and strength of the
correlationsaresimilar,andtheabsolutesizeofthedi ﬀerencesis
small, meaningthat the impact onthe meta-analysisisnegligible.
3.6 Results DiscussionandImplications
3.6.1 Program Semantics and Understandability. Veri/f_ication warn-
ingcounts(indirectly) encode program semantics ratherthansyn-
tacticpropertiesofthe code.Veri /f_icationtoolsaretryingto prove
semantic properties: checking syntactic properties is decidable, so
it is not the target of veri /f_iers (which /f_ind approximate solutions
219ESEC/FSE ’23, December3–9, 2023,San Francisco, CA, USA KobiFeldman,MartinKellogg, andOscar Chaparro
to undecidable, semantic problems, e.g., using SMT solvers). Our
results suggest that there might be complexity caused by seman-
tics, and veri /f_iers are well suited to reasoning about that kind of
complexity. Previous work using decidable syntactic metrics for
complexity[ 3,21,41,64,78,85,89,106]certainlycouldnotcapture
semantics (sinceany non-trival semantic propertyofa program is
undecidable [ 74])—see section 5.
On one side, the measured correlation between veri /f_iability and
understandability increases our con /f_idence that there is a semantic
componenttohumancodeunderstanding.Ontheotherside,the
smallcorrelationwemeasuredindicatesthatthereareotherfactors
to code understanding beyond justprogram semantics. Neitherof
theseconclusionsareparticularlysurprising,butprogramunder-
standabilityresearchhassofarmostlyfocusedonthenon-semantic
components (such as variable names or syntactic metrics—see sec-
tion5).Ourworkmotivatestheneedforfuturestudiesthatinvesti-
gate thesemantic component of code understanding; in particular,
the speci/f_ic semantic factors make code simple or complex, and
how theyimpactunderstandability.Fortunately,ourworkalsoof-
fersapathforward:theveri /f_icationcommunityhasalreadybuilt
manytoolsthatattempttoverifysemanticproperties( i.e.,veri/f_iers),
whichgivesusanopportunitytoleveragethoseexistingtoolstoim-
prove ourunderstandingof codecomplexity and understandability.
3.6.2 Incorporating Verifiability into Comprehensibility Models.
Most prior attempts to design automated metrics or models that
measure or predict code understandability have used syntactic fea-
turesthatdonotaccountforprogramsemantics[ 3,21,41,64,78,85,
89,106].Rather,theyusedsyntacticfeaturessuchascodebranch-
ing, vocabulary size, and executions paths, among other proxies
that attempt to capture code complexity (see section 5). Many of
these features have shown to be poor predictors of code under-
standability[ 3,26,41,45,78].Webelieveoneofthereasonsforthis
is because they do not capture complexity arising from program
semantics. Based on the link we found between veri /f_iability and
comprehensibility,wehypothesizethatsemanticcodeproperties
wouldleadto more accuratemodels ofunderstandability.
Future work should validate this hypothesis by incorporating
veri/f_iability into models that predict human-based comprehensibil-
ity[15,78,94]andmeasuringitsimpactonpredictionperformance.
Ifthelinkbetweenveri /f_iabilityandcomprehensibilityexists(asour
resultssuggest),veri /f_iabilityinformationshouldcomplementthe
syntactic features of these models. Veri /f_iability can be captured by
adaptingexistingveri /f_icationtoolsorbyleveragingtoolwarning
data.Forexample,wecouldprovidethenumberofwarningsatool
produces onasnippetas an inputfeature to thesemodels.
3.6.3 ReducingFalsePositives toIncrease CodeComprehensibility.
Developers could use the warning count of veri /f_iers to know when
code might be complex, i.e., when it might need to be refactored
toreduce accidental complexity.Whilecoding,thedevelopercan
monitor the warning count of veri /f_iers on a code snippet ( e.g., a
method they are writing or updating), knowing the code is cor-
rect. If this count increases, they could assess potential complex
parts of the method and come up with changes to the method that
wouldbe semantically equivalent( e.g., replacing recursion, which
istraditionallyhardforveri /f_ierstoreasonabout,withaloop).This
auxiliary bene /f_it of using veri /f_ication tools has not been studied intheliterature,andmightrepresentanopportunitytomakeveri /f_iers
more appealingto everydaydevelopers.
This usage scenario poses a research opportunity too: what if
wecouldautomaticallydetermineandsuggesttothedevelopera
semantically-equivalentrefactoringthatiseasiertoverify?Such
a refactoring would change the code to perform the same task,
butwouldcausea veri /f_iertoissuefewerwarnings. Themeasured
correlationbetweenveri /f_iabilityandunderstandabilitywouldmean,
more oftenthan not, thatapplyingsuch a refactoring might make
thecodeeasiertounderstand.Sinceitisunclearifsuchrefactoring
is possible, more research should be conducted. However, if it is
possibleandthedeveloperisawareofthecorrelation,weanticipate
they would be more willing to (1) use veri /f_iers in their everyday
codingtasks,and(2)accepttherefactoringsuggestion.Onepossible
issue with this approach is that our correlation includes warnings
causedbymissingspeci /f_ications;thereislargeexistingliterature
onspeci/f_icationinference( e.g.,[22,28,96])thatcouldbeleveraged
to focusonly onfalse positives when speci /f_ications are explicit.
3.6.4 CodeVerifiabilityvs.Understandabilityvs.Complexity. Our
study found a correlation between code understandability and ver-
i/f_iability, yet it did not /f_ind whether one of the two causes the
other(i.e.,correlationdoesnotimplycausation).Furtherresearch
isneededtodeterminewhetheronecausestheother,orwhether
thereareotherfactorsthatcauseboth.However,basedonourre-
sultsanddiscussion,wehypothesizethat codecomplexity causes
both humans and veri /f_ication tools to struggle to understand code.
Future studiesshould investigate this andotherpossible causes.
4 LIMITATIONSAND THREATS TO VALIDITY
Ourstudyshowsa correlation betweenveri /f_iabilityandunderstand-
ability,butdonotshowone causestheother.So,ourresultsmustbe
interpretedcarefully:furtherworkisneededtodeterminecausality.
Regardingthreats toexternal validity,the correlation we found
may not generalize beyond the speci /f_ic conditions of our study.
The snippets are all Java code, so the results may not generalize to
otherlanguages.Weonlyusedafewveri /f_iers,aswewerelimited
by parcity of practical tools that can analyze the snippets. While
limitationsorbugsinindividualtoolscouldskewourresults,we
mitigated this threat by re-running the experiments individually
for eachtooland withanablationexperiment(section 3.3),which
demonstratedthatnosingletooldominatestheresults.Thesnip-
pets are small compared to full programs; the comprehensibility of
larger programs may di ﬀer. Further, 3/6 datasets are snippets from
introductory CS courses rather than real-life programs, but this is
mitigatedbythe otherthree datasets of open-sourcesnippets.
Anotherthreatisthatsubjectsinthepriorstudiesweremostly
students. Only DS6 used professional software engineers (and only
13/63 participants—the other 50 were students), so our results may
not applyto more experienced programmers.Future work should
conduct understandabilitystudieswithprofessional engineers.
Beyondthedatasetsandtools,therearethreatstointernaland
construct validity. We assumed the snippets are correct as written,
andthateachveri /f_ierwarningthereforerepresentseitherafalse
positive or a speci /f_ication that a human would need to write to
verify the code. The presence of a bug would make a snippet seem
“harder to verify” in our analysis (because every veri /f_ier would
220On the RelationshipbetweenCode Verifiability andUnderstandability ESEC/FSE ’23, December3–9, 2023,San Francisco, CA, USA
warn about it), even if the snippet is easy for humans to under-
stand, skewing the results. We mitigated this threat by manually
examining a representative subset of the warnings as described in
section2.4; we didnot observe any bugsinthe snippets.
5 RELATED WORK
Codecomplexitymetrics. Researchershaveproposedmanymet-
rics for code complexity [ 3,21,41,64,78,85,106], though the
concept is not easyto de /f_ine due to di ﬀerentinterpretations [ 5,6].
Most metrics rely on simple, syntactic properties such as code
sizeorbranchingpaths[ 64,69].Thesemetricsareusedtodetect
complexcodesodeveloperscan simplifyitduringsoftwareevolu-
tion [4,34,69]. The motivation is that complex code is harder to
understand [ 3,78], which may have important repercussions on
developer e ﬀort and software quality ( e.g., bugs introduced due to
misunderstoodcode).Ourcorrelationresultsimplythatcodethat
is easier to verify might also be simple and easier to understand
by humans; we believe the underlying mechanism might be that
simple code /f_its into the expected code patterns of a veri /f_ication
technique.Ourresultsalsosuggestthatacomplexitymetricthat
aims to capture human understandability should consider not only
syntactic information aboutthe code,but alsoits semantics.
Empiricalvalidationofcomplexitymetrics. Scalabrino et
al.[78] collected code understandability measurements from devel-
opersandstudentsonopen-sourcecode.Theycorrelatedtheirmea-
surementswith 121syntacticcomplexitymetrics( e.g.,cyclomatic
complexity,LOC, etc.)anddeveloper-relatedproperties( e.g.,code
author’s experience and background). They found small correla-
tionsforonlyafewmetrics,butamodeltrainedoncombinationsof
metrics performed better. Another study found similar results [ 94].
Researchershaveexploredthelimitationsofclassicalcomplexity
metrics[3,26,41,45,78].Forexample,Ajami etal.[3]foundthat
diﬀerentcodeconstructs( e.g.,ifsvs.forloops)havedi ﬀerenteﬀects
onhowdeveloperscomprehendcode,implyingthatmetricssuchas
cyclomatic complexity, which weights code constructs equally, fail
tocaptureunderstandability[ 69].Recentworkhasproposednew
metricssuchasCognitiveComplexity(COG)[ 18,76],whichassigns
diﬀerent weights to di ﬀerent code constructs. Muñoz et al.[62]
conductedacorrelationmeta-analysisbetweenCOGandhuman
understandability. They found that time and rating metrics have a
modest correlation with COG, while correctness and physiological
metrics have no correlation. They did not take into account the
unit-of-analysisproblem intheirmeta-analyses.
Weextendpriorworkwithempiricalevidenceofthecorrelation
betweenveri /f_iabilityandhumanunderstandability.Tothebestof
our knowledge,we are the /f_irst to investigate this empirically.
Studyingcodeunderstandability. Researchershavestudied
code understandability and factors a ﬀecting it via controlled exper-
imentsanduserstudies[ 3,13,35,41,43,69,71,82].Preciselyde /f_in-
ingunderstandabilityisdi ﬃcult,sosomestudies[ 13,15,62,66,72]
use it interchangeably with readability (a di ﬀerent, yet related con-
cept). Measurements include the time to read, understand, or com-
plete code; the correctness of output given by the participants;
perceivedcodecomplexity,readabilityorunderstandability;and(re-
cently)physiologicalmeasuresfromfMRIscanners[ 69,71,82],bio-
metrics sensors [ 31,33,105], or eye-tracking devices [ 1,10,31,95].Ourstudyutilizesthesehuman-basedmeasurementsofunderstand-
abilityto assesstheircorrelation withveri /f_iability.
Factors that a ﬀect understandability include: code constructs [ 3,
43]andpatterns[ 13,41,52],identi/f_ierqualityandstyle[ 83,102],
comments[ 13],informationgatheringtasks[ 10,53,82,83],com-
prehension tools [ 87], code reading behavior [ 2,70,81], author-
ship[32],high-levelcomprehensionstrategies[ 81],programmer
experience[ 102,104],andtheuseofcomplexitymetrics[ 103].Our
workinvestigatesanew factorimpactingunderstandability:code
veri/f_iability.Ourresultssuggestthereisacorrelationbetweenthese
variables, yetfuture studiesare neededto assesscausality.
Studies of veri /f_ication and static analysis tools. A study
conducted to evaluate a code readability model [ 15] is closely re-
latedtoours.Themodelwasfoundtocorrelatemoderatelywith
snippetsonwhichFindBugs[ 7]issuedwarnings. Unlikethetools
inourstudy,FindBugsis notaveri/f_icationtool(itusesheuristicsto
/f_lagpossibly-buggycode).Wecorrelatedveri /f_iabilitywithhuman
understandability; the earlier study correlated FindBugs warnings
with an automated readability modeltrained on human judgments.
Though veri /f_ication and static analysis tools are becoming more
common in industry [ 8,75], studies of their use and the challenges
developers face in deploying them [ 8,57,63,84,98] suggest that
falsepositivesremainaprobleminpractice[ 42,63].Ourworkgives
anewperspectivetheproblemoffalsepositives.Wehaveshown
that the presence of false positives from veri /f_iers correlates with
morediﬃcult-to-understandcode.Wehopethatthisperspective
encouragesdeveloperstoviewfalsepositivesasopportunitiesto
improve theircode ratherthanas barriersto /f_inding defects[ 77].
6 CONCLUSIONSAND FUTUREWORK
Ourempiricalstudyonthecorrelationbetweentool-basedveri /f_i-
abilityandhuman-basedmetricsofcodeunderstandingsuggests
thereisa connection between whether a tool can verify a code
snippet and how easy it is for a human to understand. Though our
results are suggestive, our meta-analysis shows that extant studies
onhumancodeunderstandabilitylacksu ﬃcientpowertoenableus
todrawastrongerconclusion,somorestudiesofunderstandability
(preferably including many more snippets of code) are needed. Fur-
ther, our work has shown only a correlation:establishing acausal
link between veri /f_iability and understandability—perhaps through
amutual cause,such as complexity—remainsfuture work.
Veri/f_iabilityisapromisingalternativethatcomplementstradi-
tional code complexity metrics, and future work could combine
measuresof tool-basedveri /f_iabilitywith moderncomplexitymet-
rics such as cognitive complexity that seem to capture di ﬀerent
aspectsofhumanunderstandabilityintoauni /f_ied,automaticmodel.
Ourresultsarealsopromisingsupportfortheprospectofincreased
adoption of veri /f_iers: our results o ﬀer a new perspective on the
classic problem of false positives, since they suggest that false pos-
itivesfromveri /f_iersareopportunitiestoidentifypotentiallymore
complex code andmake itmore understandable byhumans.
ACKNOWLEDGEMENTS
We thank the anonymous reviewers and Michael D. Ernst for com-
ments that helped to improve this paper. Ji Meng Loh provided
invaluableadvice aboutour statisticalapproach.
221ESEC/FSE ’23, December3–9, 2023,San Francisco, CA, USA KobiFeldman,MartinKellogg, andOscar Chaparro
REFERENCES
[1]Amine Abbad-Andaloussi, Thierry Sorg, and Barbara Weber. 2022. Estimat-
ing Developers’ Cognitive Load at a Fine-grained Level Using Eye-tracking
Measures. In Intl.Conf.onProg.Compr. (ICPC) . 111–121.
[2]NahlaJ.Abid,BonitaSharif,NataliaDragan,HendAlrasheed,andJonathanI.
Maletic. 2019. Developer Reading Behavior While Summarizing Java Methods:
Size and Context Matters. In Intl.Conf.onSoft.Eng.(ICSE) . 384–395.
[3]Shulamyt Ajami, Yonatan Woodbridge, and Dror G. Feitelson. 2019. Syntax,
predicates,idioms —what really a ﬀects codecomplexity? Emp. Soft. Eng. 24,1
(2019), 287–328.
[4]Erik Ammerlaan, Wim Veninga, and Andy Zaidman. 2015. Old habits die hard:
Why refactoring for understandability does not give immediate bene /f_its. InIntl.
Conf.onSoft.Analysis, Evolution, and ReEng. (SANER) . 504–507.
[5]Vard Antinyan. 2020. Evaluating Essential and Accidental Code Complexity
Triggers by Practitioners’Perception. IEEE Soft. 37,6 (2020), 86–93.
[6]VardAntinyan,MiroslawStaron,andAnnaSandberg.2017. Evaluatingcode
complexity triggers, use of complexity measures and the in /f_luence of code
complexityonmaintenance time. Emp. Soft.Eng. 22,6 (2017), 3057–3087.
[7]NathanielAyewah,DavidHovemeyer,J.DavidMorgenthaler,JohnPenix,and
William Pugh. 2008. Using static analysis to /f_ind bugs. IEEE Software 25, 5
(2008), 22–29.
[8]MoritzBeller,RadjinoBholanath,ShaneMcIntosh,andAndyZaidman.2016.
AnalyzingtheStateofStaticAnalysis:ALarge-ScaleEvaluationinOpenSource
Software.In Intl.Conf.onSoft.Analysis,Evolution,andReEng.(SANER) ,Vol.1.
470–481.
[9]Dirk Beyer and Ashgan Fararooy. 2010. A Simple and E ﬀective Measure for
Complex Low-Level Dependencies.In Intl.Conf.onProg. Compr. (ICPC) .80–83.
[10]Dave Binkley, Marcia Davis, Dawn Lawrie, Jonathan I. Maletic, Christopher
Morrell, and Bonita Sharif. 2013. The impact of identi /f_ier style on e ﬀort and
comprehension. Emp. Soft.Eng. 18,2 (2013), 219–276.
[11]Michael Borenstein, Larry V. Hedges, Julian P. T. Higgins, and Hannah R. Roth-
stein. 2009. Introduction toMeta-Analysis . John Wiley& Sons.
[12]Michael Borenstein, Larry V. Hedges, Julian P. T. Higgins, and Hannah R. Roth-
stein.2009. VoteCounting-ANewNameforanOldProblem . JohnWiley&Sons,
251–255.
[13]JürgenBörstlerandBarbaraPaech.2016. Theroleofmethodchainsandcom-
mentsinsoftwarereadabilityandcomprehension—Anexperiment. Trans.on
Soft.Eng.(TSE) 42,9 (2016), 886–898.
[14] FrederickBrooksand H Kugler. 1987. No silver bullet . April.
[15]RaymondBuseandWestleyWeimer.2009. Learningametricforcodereadability.
Trans. onSoft.Eng.(TSE) 36,4 (2009), 546–558.
[16]Cristiano Calcagno, Dino Distefano, Jérémy Dubreil, Dominik Gabi, Pieter
Hooimeijer,MartinoLuca,PeterO’Hearn,IrenePapakonstantinou,JimPurbrick,
andDulmaRodriguez.2015. Movingfastwithsoftwareveri /f_ication.In NASA
Formal MethodsSymp. Springer, 3–11.
[17]Cristiano Calcagno, Dino Distefano, Peter O’Hearn, and Hongseok Yang. 2009.
Compositional shape analysis by means of bi-abduction. In Principles of Pro-
gramming Languages (POPL) . 289–300.
[18]G. Ann Campbell. 2018. Cognitive complexity:an overview and evaluation. In
Intl.Conf.onTechnicalDebt . 57–58.
[19]S.R. Chidamber and C.F. Kemerer. 1994. A metrics suite for object oriented
design.Trans. onSoft.Eng.(TSE) 20,6 (1994), 476–493.
[20]JacobCohen,PatriciaCohen,StephenG.West,andLeonaS.Aiken.2002. Ap-
plied Multiple Regression/Correlation Analysisfor theBehavioral Sciences (3 ed.).
Routledge.
[21]B. Curtis, S.B. Sheppard, P. Milliman, M.A. Borst, and T. Love. 1979. Measuring
thePsychologicalComplexityofSoftwareMaintenanceTaskswiththeHalstead
and McCabeMetrics. Trans. onSoft.Eng.(TSE) SE-5, 2 (1979), 96–104.
[22]Luis Damas and Robin Milner. 1982. Principal type-schemes for functional pro-
grams.In Proceedingsofthe9thACMSIGPLAN-SIGACTsymposiumonPrinciples
ofprogramminglanguages . 207–212.
[23]Leonardo De Moura and Nikolaj Bjørner. 2008. Z3: An e ﬃcient SMT solver. In
TACAS2008: Tools and Algorithmsfor theConstructionand Analysis of Systems
(TACAS). Budapest,Hungary, 337–340.
[24]Werner Dietl, Stephanie Dietzel, Michael D. Ernst, K ıvanç Muşlu, and Todd
Schiller.2011. Buildingandusingpluggabletype-checkers. In Intl.Conf.onSoft.
Eng.(ICSE) . Waikiki,Hawaii, USA,681–690.
[25]Michael Emmi,Liana Hadarean, Ranjit Jhala, Lee Pike,Nicolás Rosner, Martin
Schäf,AritraSengupta,andWillemVisser.2021. RAPID:checkingAPIusage
for the cloud in the cloud. In European Soft. Eng. Conf. and Symp. on the Found.
ofSoft.Eng.(ESEC/FSE) . 1416–1426.
[26]JanetFeigenspan,SvenApel,JorgLiebig,andChristianKastner.2011. Exploring
SoftwareMeasurestoAssessProgramComprehension.In Intl.Symp.onEmp.
Soft.Eng.and Meas.(ESEM) . 127–136.
[27]KobiFeldman,MartinKellogg,andOscarChaparro.2023. Onlinereplication
package. (2023). https://doi.org/10.5281/zenodo.8237328
[28]CormacFlanaganandKRustanMLeino.2001. Houdini,anannotationassistant
for ESC/Java. In FME 2001: Formal Methods for Increasing Software Productivity:International Symposium of Formal Methods Europe Berlin, Germany, March
12–16,2001Proceedings . Springer, 500–517.
[29]CormacFlanagan,KRustanMLeino,MarkLillibridge,GregNelson,JamesB
Saxe,andRaymieStata.2002. ExtendedstaticcheckingforJava.In Programming
Language Designand Implementation(PLDI) . 234–245.
[30]Jeﬀrey S. Foster, Manuel Fähndrich, and Alexander Aiken. 1999. A theory of
typequali /f_iers.InConf.onProgrammingLanguage Designand Implementation
(PLDI). Atlanta, GA, USA,192–203.
[31]Thomas Fritz, Andrew Begel, Sebastian C. Müller, Serap Yigit-Elliott, and
Manuela Züger. 2014. Using psycho-physiological measures to assess task
diﬃcultyin softwaredevelopment. In Intl.Conf.onSoft.Eng.(ICSE) . 402–413.
[32]Thomas Fritz, Jingwen Ou, Gail C. Murphy, and Emerson Murphy-Hill. 2010. A
degree-of-knowledge model to capture source code familiarity. In Intl. Conf. on
Soft.Eng.(ICSE) . 385–394.
[33]Davide Fucci, Daniela Girardi, Nicole Novielli, Luigi Quaranta, and Filippo
Lanubile. 2019. A Replication Study on Code Comprehension and Expertise
using Lightweight Biometric Sensors. In Intl. Conf. on Prog. Compr. (ICPC) .
311–322.
[34]JavierGarcía-Munoz,MarisolGarcía-Valls,andJulioEscribano-Barreno.2016.
Improved Metrics Handling in SonarQube for Software Quality Monitoring. In
Intl.Conf.onDistributedComp. and Art.Intel. 463–470.
[35]DanGopstein,Anne-LaureFayard,SvenApel,andJustinCappos.2020.Thinking
aloud about confusing code: a qualitative investigation of program comprehen-
sion and atoms of confusion. In European Soft. Eng. Conf. and Symp. on the
Found.ofSoft.Eng.(ESEC/FSE) . 605–616.
[36] MauriceH. Halstead.1977. ElementsofSoft.Science . Elsevier.
[37]Mathias Harrer, Pim Cuijpers, Furukawa Toshi A, and David D Ebert. 2021.
DoingMeta-AnalysisWithR:AHands-OnGuide (1sted.). Chapman&Hall/CRC
Press,BocaRaton,FLand London.
[38]Klaus Havelund and Thomas Pressburger. 2000. Model checking java programs
usingjavapath /f_inder.Intl.Jour.onSoft.ToolsforTechnologyTransfer 2,4(2000),
366–381.
[39]BrianHenderson-Sellers.1995. Object-orientedmetrics:measuresofcomplexity .
Prentice-Hall,Inc.
[40]Sture Holm. 1979. A simple sequentially rejective multiple test procedure.
Scandinavian journalofstatistics (1979), 65–70.
[41]Ahmad Jbara and Dror G. Feitelson. 2017. How programmers read regular
code:acontrolledexperimentusingeyetracking. Emp.Soft.Eng. 22,3(2017),
1440–1477.
[42]BrittanyJohnson,YoonkiSong,EmersonMurphy-Hill,andRobertBowdidge.
2013. Why don’tsoftware developers usestatic analysis tools to /f_ind bugs?. In
Intl.Conf.onSoft.Eng.(ICSE) . 672–681.
[43]JohnJohnson,SergioLubo,NishithaYedla,JairoAponte,andBonitaSharif.2019.
AnEmpiricalStudyAssessingSourceCodeReadabilityinComprehension.In
Intl.Conf.onSoft.Maint.and Evol.(ICSME) . 513–523.
[44]Temesghen Kahsai,PhilippRümmer, Huascar Sanchez,and Martin Schäf. 2016.
JayHorn:AframeworkforverifyingJavaprograms.In Intl.Conf.onComputer
AidedVeri /f_ication (CAV) . Springer, 352–358.
[45]CemKaner,SeniorMember,andWalterP.Bond.2004. SoftwareEngineering
Metrics:WhatDo They Measure and How Do We Know?. In Intl. Soft. Metrics
Symp.(METRICS) .
[46]Martin Kellogg,VlastimilDort,SuzanneMillstein,and MichaelD.Ernst. 2018.
Lightweight veri /f_ication of array indexing. In Intl. Symp. on Soft. Testing and
Analysis(ISSTA) . 3–14.
[47]Martin Kellogg, Manli Ran, Manu Sridharan, Martin Schäf, and Michael D.
Ernst. 2020. Verifying Object Construction. In Intl. Conf. on Soft. Eng. (ICSE) .
1447–1458.
[48]MartinKellogg,NargesShadab,ManuSridharan,andMichaelD.Ernst.2021.
Lightweightandmodularresourceleakveri /f_ication.In EuropeanSoft.Eng.Conf.
and Symp.onthe Found.ofSoft.Eng.(ESEC/FSE) .
[49]MauriceG.Kendall.1938. Anewmeasureofrankcorrelation. Biometrika 30,
1/2 (1938), 81–93.
[50]Steve Klabnik and Carol Nichols. 2018. The Rust Programming Language .https:
//doc.rust-lang.org/1.50.0/book/
[51]Stefan Krüger,Johannes Späth, Karim Ali, Eric Bodden,and Mira Mezini. 2018.
CrySL: An extensible approach to validating the correct usage of cryptographic
APIs. InEuropean Conf. on Object-Oriented Programming (ECOOP) . Amsterdam,
Netherlands, 10:1–10:27.
[52]ChrisLanghoutandMaurícioAniche.2021. AtomsofConfusioninJava.In Intl.
Conf.onProg.Compr.(ICPC) . 25–35.
[53]Thomas D.LaToza, David Garlan, James D.Herbsleb,and Brad A.Myers.2007.
Program comprehension as fact /f_inding. In European Soft. Eng. Conf. and the
Symp.ononthe Found.ofSoft.Eng.(ESEC/FSE) . 361–370.
[54]GaryTLeavens,AlbertLBaker,andClydeRuby.1998. JML:aJavamodeling
language.In FormalUnderpinningsofJavaWorkshop(atOOPSLA1998) .Citeseer,
404–420.
[55]Benjamin Livshits, Manu Sridharan, Yannis Smaragdakis, Ond řej Lhoták, J. Nel-
sonAmaral,Bor-YuhEvan Chang, SamuelZ.Guyer, Uday P.Khedker, Anders
222On the RelationshipbetweenCode Verifiability andUnderstandability ESEC/FSE ’23, December3–9, 2023,San Francisco, CA, USA
Møller, and Dimitrios Vardoulakis. 2015. In defense of soundiness: A manifesto.
Commun. ACM 58,2 (2015), 44–46.
[56]WalidMaalej,RebeccaTiarks,TobiasRoehm,andRainerKoschke.2014. Onthe
Comprehension of Program Comprehension. Trans. on Soft. Eng. and Methodol-
ogy(TSEM) 23,4 (2014), 1–37.
[57]NiloofarMansoor,TukaramMuske,AlexanderSerebrenik,andBonitaSharif.
2022. An EmpiricalAssessment of Repositioning of Static Analysis Alarms.In
Intl.Working Conf.onSourceCodeAnalysis& Manipulation .
[58]T.J. McCabe. 1976. A Complexity Measure. Trans. on Soft. Eng. (TSE) SE-2, 4
(1976), 308–320.
[59]RobertoMinelli,AndreaMocci,andMicheleLanza.2015. IKnowWhatYouDid
LastSummer-AnInvestigationofHowDevelopersSpendTheirTime.In Intl.
Conf.onProg.Compr. (ICPC) . 25–35.
[60]João Mota, Marco Giunti, and António Ravara. 2021. Java typestate checker. In
Intl.Conf.onCoord.Lang. and Models . Springer, 121–133.
[61]JacquelineMurray.2013.Likertdata:whattouse,parametricornon-parametric?
Intl.Jour.ofBusinessand Social Science 4,11(2013).
[62]Marvin Muñoz Barón, Marvin Wyrich, and Stefan Wagner. 2020. An Empirical
ValidationofCognitiveComplexityasaMeasureofSourceCodeUnderstand-
ability. In Intl.Symp. onEmp. Soft.Eng.and Meas.(ESEM) . 1–12.
[63]Marcus Nachtigall, Michael Schlichtig, and Eric Bodden. 2022. A large-scale
study of usability criteria addressed by static analysis tools. In Intl. Symp. on
Soft.Testingand Analysis(ISSTA) . 532–543.
[64]AlbertoS.Nuñez-Varela,HéctorG.Pérez-Gonzalez,FranciscoE.Martínez-Perez,
and Carlos Soubervielle-Montalvo. 2017. Source code metrics: A systematic
mappingstudy. Jour. ofSys.and Soft. 128(2017), 164–197.
[65]PeterO’Hearn,JohnReynolds,andHongseokYang.2001. Localreasoningabout
programs that alterdata structures.In Intl.WorkshoponComputerScienceLogic .
Springer, 1–19.
[66]Delano Oliveira, Reydne Bruno, Fernanda Madeiral, and Fernando Castor. 2020.
Evaluating Code Readability and Legibility: An Examination of Human-centric
Studies. In Intl.Conf.onSoft.Maint.and Evol.(ICSME) . 348–359.
[67]OpenJML Developers. 2022. OpenJML - formal meth-
ods tool for Java and the Java Modeling Language (JML).
https://www.openjml.org/documentation/introduction.html.
[68]Matthew M. Papi, Mahmood Ali, Telmo Luis Correa Jr., Je ﬀH. Perkins, and
Michael D. Ernst. 2008. Practical pluggable types for Java. In Intl. Symp. on Soft.
Testingand Analysis(ISSTA) . Seattle, WA, USA,201–212.
[69]NormanPeitek,SvenApel,ChrisParnin,AndréBrechmann,andJanetSiegmund.
2021. Program comprehension and code complexity metrics: An fMRI study. In
Intl.Conf.onSoft.Eng.(ICSE) . 524–536.
[70]NormanPeitek,JanetSiegmund,andSvenApel.2020. WhatDrivestheReading
OrderofProgrammers?AnEyeTrackingStudy.In Intl.Conf.onProg.Compr.
(ICPC). 342–353.
[71]Norman Peitek, Janet Siegmund, Sven Apel, Christian Kästner, Chris Parnin,
Anja Bethmann, Thomas Leich, Gunter Saake, and André Brechmann. 2018. A
look into programmers’ heads. Trans. on Soft. Eng. (TSE) 46, 4 (2018), 442–462.
[72]Valentina Piantadosi, Fabiana Fierro, Simone Scalabrino, Alexander Serebrenik,
and Rocco Oliveto. 2020. How does code readability change during software
evolution? Emp. Soft.Eng. 25,6 (2020), 5374–5412.
[73]James E Pustejovsky and Elizabeth Tipton. 2022. Meta-analysis with robust
varianceestimation:Expandingtherangeofworkingmodels. PreventionScience
23,3 (2022), 425–438.
[74]Henry Gordon Rice. 1953. Classes of recursively enumerable sets and their
decision problems. Trans. of the American Mathematical Society 74, 2 (1953),
358–366.
[75]Nick Rutar,Christian B. Almazan, and Je ﬀreyS. Foster. 2004. Acomparison of
bug/f_inding tools for Java. In Intl.Symp. onSoft.Reliab.Eng. 245–256.
[76]Rubén Saborido,Javier Ferrer,FranciscoChicano,and EnriqueAlba. 2022. Au-
tomatizingSoftwareCognitiveComplexityReduction. IEEEAccess 10(2022),
11642–11656.
[77]Caitlin Sadowski, Je ﬀrey Van Gogh, Ciera Jaspan, Emma Soderberg, and Collin
Winter.2015. Tricorder:Buildingaprogramanalysisecosystem.In Intl.Conf.
onSoft.Eng.(ICSE) , Vol. 1.598–608.
[78]Simone Scalabrino, Gabriele Bavota, Christopher Vendome, Mario Linares-
Vasquez, Denys Poshyvanyk, and Rocco Oliveto. 2019. Automatically assessing
codeunderstandability. Trans. onSoft.Eng.(TSE) 47,3 (2019), 595–613.
[79] Martin Schäf and Philipp Rümmer. 2022. personal communication.
[80]Kurex Sidik and Je ﬀrey N. Jonkman. 2005. Simple heterogeneity variance
estimation for meta-analysis. Jour. of the Royal Statistical Society: Series C
(AppliedStatistics) 54,2 (2005), 367–384.
[81]JanetSiegmund.2016. ProgramComprehension:Past,Present,andFuture.In
Intl.Conf.onSoft.Analysis, Evolution, and ReEng. (SANER) , Vol. 5.13–20.[82]JanetSiegmund,ChristianKästner,SvenApel,ChrisParnin,AnjaBethmann,
Thomas Leich, Gunter Saake, and André Brechmann. 2014. Understanding
understandingsourcecodewithfunctionalmagneticresonanceimaging.In Intl.
Conf.onSoft.Eng.(ICSE) . 378–389.
[83]Janet Siegmund, Norman Peitek, Chris Parnin, Sven Apel, Johannes Hofmeister,
ChristianKästner,AndrewBegel,AnjaBethmann,andAndréBrechmann.2017.
Measuringneurale ﬃciency of programcomprehension.In EuropeanSoft. Eng.
Conf.and Symp.onFound.ofSoft.Eng.(ESEC/FSE’17) . 140–150.
[84]JustinSmith,LisaNguyenQuangDo,andEmersonMurphy-Hill.2020. Why
Can’tJohnnyFixVulnerabilities:AUsabilityEvaluationofStaticAnalysisTools
for Security. In Symp.onUsablePrivacy and Security(SOUPS) . 221–238.
[85]Harry M. Sneed. 1995. Understanding software through numbers: A metric
based approach to programcomprehension. Jour. ofSoft. Maint.:Researchand
Practice7,6 (1995), 405–419.
[86]Eric Spishak, Werner Dietl, and Michael D. Ernst. 2012. A type system for
regular expressions. In FTfJP: 14th Workshop on Formal Techniques for Java-like
Programs . Beijing, China, 20–26.
[87]M.A.D.Storey,K.Wong,andH.A.Müller.2000.Howdoprogramunderstanding
tools aﬀect how programmers understand programs? Science of Computer
Programming 36,2 (2000), 183–207.
[88]Robert E. Strom and Shaula Yemini. 1986. Typestate: A programming language
conceptfor enhancing softwarereliability. IEEETransactions onSoftwareSngi-
neeringSE-12,1 (Jan. 1986),157–171.
[89]Amjed Tahir and Stephen G. MacDonell. 2012. A systematic mapping study
ondynamicmetricsandsoftwarequality.In Intl.Conf.onSoft.Maint.(ICSM) .
326–335.
[90]YidaTao,YingnongDang,TaoXie,DongmeiZhang,andSunghunKim.2012.
How do software engineers understand code changes? an exploratory study in
industry. In Symp.onthe Found.ofSoft.Eng.(FSE) . 1–11.
[91]The Checker Framework Developers. 2022. 2.4.5 What to do if a checker issues
awarningaboutyourcode. https://checkerframework.org/manual/#handling-
warnings.
[92]The Checker Framework Developers. 2022. Optional Checker for possibly-
presentdata. https://tinyurl.com/3surnw4a.
[93] The OpenJMLDevelopers.2022. OpenJML. https://www.openjml.org/.
[94]AsherTrockman,KeenenCates,MarkMozina,TuanNguyen,ChristianKästner,
and Bogdan Vasilescu. 2018. "Automatically assessing code understandability"
reanalyzed:combinedmetricsmatter.In Intl.Conf.onMiningSoft.Repositories
(MSR). 314–318.
[95]RachelTurner,MichaelFalcone,BonitaSharif,andAlinaLazar.2014. Aneye-
tracking study assessing the comprehension of c++ and Python source code. In
Symp.onEyeTracking Research and Applications . 231–234.
[96]Mohsen Vakilian, Amarin Phaosawasdi, Michael D Ernst, and Ralph E Johnson.
2015. Cascade:Auniversalprogrammer-assistedtypequali /f_ierinferencetool.
In2015IEEE/ACM37thIEEEInternationalConferenceonSoftwareEngineering ,
Vol. 1.IEEE,234–245.
[97]AnneM.vanValkengoedandLindaSteg.2019. Meta-analysesoffactorsmo-
tivatingclimatechangeadaptationbehaviour. NatureClimateChange (2019),
158–163.
[98]Carmine Vassallo, Sebastiano Panichella, Fabio Palomba, Sebastian Proksch,
HaraldC.Gall,andAndyZaidman.2020. Howdevelopersengagewithstatic
analysis tools in di ﬀerentcontexts. Emp. Soft.Eng. 25,2 (2020), 1419–1457.
[99]Wolfgang Viechtbauer. 2010. Conducting meta-analyses in R with the metafor
package. Journal of Statistical Software 36, 3 (2010), 1–48. https://doi.org/10.
18637/jss.v036.i03
[100]DavidWalker.2003. JMASM9:ConvertingKendall’sTauForCorrelationalOr
Meta-AnalyticAnalyses. Jour. ofM. A. Stat.Meth. 2,2 (2003).
[101]Konstantin Weitz, Gene Kim, Siwakorn Srisakaokul, and Michael D. Ernst. 2014.
A type system for format strings. In Intl. Symp. on Soft. Testing and Analysis
(ISSTA). 127–137.
[102]Eliane S. Wiese, Anna N. Ra ﬀerty, and Armando Fox. 2019. Linking Code
Readability,Structure,andComprehensionAmongNovices:It’sComplicated.
InIntl.Conf.onSoft.Eng.(ICSE) . 84–94.
[103]MarvinWyrich,AndreasPreikschat,DanielGraziotin,andStefanWagner.2021.
TheMindIsaPowerfulPlace:HowShowingCodeComprehensibilityMetrics
In/f_luences CodeUnderstanding. In Intl.Conf.onSoft.Eng.(ICSE) . 512–523.
[104]Xin Xia, Lingfeng Bao, David Lo, Zhenchang Xing, Ahmed E. Hassan, and
Shanping Li. 2018. Measuring Program Comprehension: A Large-Scale Field
Studywith Professionals. Trans. onSoft.Eng.(TSE) 44,10(2018), 951–976.
[105]MartinK.-C. Yeh,DanGopstein, YuYan, and YanyanZhuang.2017. Detecting
andcomparingbrainactivityinshortprogramcomprehensionusingEEG.In
FrontiersinEducation Conf.(FIE) . 1–5.
[106]H. Zuse. 1993. Criteria for program comprehension derived from software
complexitymetrics. In Workshop onProg.Compr . 8–16.
223