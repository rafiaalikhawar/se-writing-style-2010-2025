Fast Outage Analysis of Large-scale Production
Clouds with Service Correlation Mining
Yaohui Wangk, Guozheng Liy, Zijian Wangk, Yu Kangz, Yangfan Zhouk, Hongyu Zhangx, Feng Gao{,
Jeffrey Sun{, Li Yang{, Pochian Lee{, Zhangwei Xu{, Pu Zhaoz, Bo Qiaoz, Liqun Liz, Xu Zhangz, Qingwei Linz
School of Computer Science, Fudan University, China
kShanghai Key Laboratory of Intelligent Information Processing, China
ySchool of Electronics Engineering and Computer Science, Peking University, Beijing, China
zMicrosoft Research, Beijing, China
xSchool of Electrical Engineering and Computing, The University of Newcastle, Australia
{Microsoft Azure, Redmond, USA
Abstract ‚ÄîCloud-based services are surging into popularity
in recent years. However, outages, i.e., severe incidents that
always impact multiple services, can dramatically affect user
experience and incur severe economic losses. Locating the root-
cause service, i.e., the service that contains the root cause of the
outage, is a crucial step to mitigate the impact of the outage.
In current industrial practice, this is generally performed in a
bootstrap manner and largely depends on human efforts: the
service that directly causes the outage is identiÔ¨Åed Ô¨Årst, and
the suspected root cause is traced back manually from service
to service during diagnosis until the actual root cause is found.
Unfortunately, production cloud systems typically contain a large
number of interdependent services. Such a manual root cause
analysis is often time-consuming and labor-intensive. In this
work, we propose COT, the Ô¨Årst outage triage approach that
considers the global view of service correlations. COT mines the
correlations among services from outage diagnosis data. After
learning from historical outages, COT can infer the root cause
of emerging ones accurately. We implement COT and evaluate
it on a real-world dataset containing one year of data collected
from Microsoft Azure, one of the representative cloud computing
platforms in the world. Our experimental results show that COT
can reach a triage accuracy of 82.1% 83.5%, which outperforms
the state-of-the-art triage approach by 28.0% 29.7%.
Index Terms ‚Äîcloud computing, root cause analysis, outage
triage, machine learning
I. I NTRODUCTION
Cloud computing has become increasingly popular in recent
years. Many companies have migrated their services to various
cloud computing platforms, e.g., Microsoft Azure, Amazon
AWS, and Google Cloud. These platforms provide a variety
of services to millions of users from all over the world every
day. Availability is one of the most critical concern to cloud
computing platforms, inÔ¨Çuencing the user experience and the
cloud providers‚Äô revenue signiÔ¨Åcantly.
Although tremendous efforts have been devoted to main-
taining high service availability [1]‚Äì[4], cloud computing plat-
forms still encounter many incidents, i.e., unplanned interrup-
tions of the services. These incidents, especially outages ( i.e.,
This work was done at Microsoft Research (Beijing, China). Yaohui Wang
and Guozheng Li contribute equally to this work.a group of related severe incidents that may impact multiple
services), cause signiÔ¨Åcant economic losses. According to a
study conducted on 12.4 million US businesses1, a failure
that takes a top cloud provider ofÔ¨Çine in the US for 3 to 6
days would result in $15 billion of economic loss. In this
regard, once an outage occurs, it should be mitigated in a
timely manner to minimize its impact. One important step of
such outage mitigation is locating its root-cause service, which
allows the corresponding responsible team to Ô¨Åx the outage.
In current industrial practice, outages are typically declared
on an originating service, i.e., the service where the outage
manifests. The team responsible for this service will analyze
the outage, and may redirect the outage to the team of another
service, until it eventually reaches the root-cause service
where the outage can be Ô¨Åxed. Our empirical study (discussed
in Section II) on a representative large-scale cloud shows
that assigning an outage to wrong services leads to a time-
consuming root cause analysis process.
Unfortunately, fast and accurate root cause analysis of out-
age is a very challenging task. Firstly, the number of services
is vast in large-scale cloud computing platforms. The depen-
dencies among the services are incredibly sophisticated. Many
services have interdependencies ( e.g., the micro-service man-
agement service is used to deploy the resource management
service , while the computing node where the micro-service
management service is deployed is managed by the resource
management service ). Many dynamic dependencies are even
implicit for engineers ( e.g., asynchronous communication,
virtual routers, virtual disks), and some services deployed
on the same node may affect each other ( e.g., monitoring
services and functional services). When an outage occurs,
massive noisy alerts might be reported, usually as incident
tickets, due to the notorious Ô¨Çooding alarm problem in cloud
computing platforms [5]. As a result, it is difÔ¨Åcult to decide
the root-cause service. According to our empirical study, the
originating service and root-cause service are different for over
1https://www.lloyds.com/news-and-insights/risk-reports/library/
cloud-down
8852021 IEEE/ACM 43rd International Conference on Software Engineering (ICSE)
1558-1225/21/$31.00 ¬©2021 IEEE
DOI 10.1109/ICSE43902.2021.00085
52% outages. Secondly, during the root cause analysis process,
the engineers of one service typically have only a partial view
of the outage, since they may only be familiar with their own
service and its closely-related ones. Consequently, the outage
may be passed from one service team to another, resulting in
a long team assignment chain for root cause analysis. Our
empirical study shows that such a long chain dramatically
prolongs the time required for root cause analysis.
Incident triage, which aims at locating the root cause of an
incident in a small scope of services [6], has been extensively
investigated in the literature. For example, DeepCT [7] is a
state-of-the-art incident triage method. However, such methods
rely only on the information of the incident per se ,e.g., the
title and summary of incident report and engineers‚Äô discussions
during diagnosis. They do not take the global view of service
correlations into consideration. As a result, they are not
suitable for root cause analysis of outage, since an outage
typically involves many correlated services.
In this paper, we propose COT ( Correlation-based Outage
Triage) to facilitate fast and accurate outage root cause analy-
sis. COT is a Ô¨Årst attempt in the literature, to our knowledge,
which provides a global view of service correlations for outage
root cause analysis. COT collects all the incidents in the same
region and the adjacent time range with the outage. Based on
the historical outage diagnosis data, it Ô¨Ånds the correlations
among these incidents and builds an incident correlation graph.
By mapping incidents to their owning services, COT can
further obtain the service correlation graph, which indicates the
anomaly propagation patterns among services. It then takes the
features extracted from these graphs of historical outages with
their root-cause services as training samples to train a machine
learning model. This model can then be used to predict the
root-cause service for emerging outages.
We evaluate COT in two ways. We Ô¨Årst conduct a quanti-
tative experiment to compare the efÔ¨Åciency and effectiveness
of COT and DeepCT, the state-of-the-art approach. The result
shows that COT can perform root cause analysis of an outage
within one minute, and outperforms DeepCT by 28.0 29.7%
in accuracy. Second, we evaluate the usability of COT through
a representative real-world case study. The results show that
COT could facilitate engineers to understand outages and
locate the root-cause services.
The contributions of this work are summarized as follows.
We conduct the Ô¨Årst empirical study on the outage triage
problem in large-scale production clouds. The results are
based on representative real-world cloud services and can
facilitate further follow-up research.
We propose COT, a generic cross-service outage triage
approach. It is a correlation-based approach which pre-
dicts the root-cause service at the early stage of an outage
with high accuracy.
We comprehensively evaluate COT by comparing it with
the state-of-the-art triage approach and by conducting a
case study of two real-world outages.
The rest of the paper is organized as follows: Section II
presents the results of an empirical study of outage triage.
Fig. 1: The life cycle of an outage.
Section III describes the overall design of the correlation-
based outage triage approach, and Section IV introduces the
implementation details of COT and the experimental settings.
In Section V, we evaluate the performance and time efÔ¨Åciency
of COT and compare it to the state-of-the-art incident triage
method. In Section VI, we conduct a case study of two real-
world outages to show the effectivness of COT. Section VII
discusses the threats to the validity of our work. Section VIII
presents some related work. Section IX concludes the paper.
II. A NEMPIRICAL STUDY OF OUTAGE TRIAGE
This section presents the Ô¨Årst empirical study of outage
triage in real-world cloud computing platforms. The study is
based on Microsoft Azure, one of the representative cloud
computing platforms in the industry. In the following, we
Ô¨Årst introduce the life cycle of an outage in Microsoft Azure,
especially the differences between outage triage and incident
triage. Next, we analyze Noutages collected from Microsoft
Azure, ranging from 2020-01-01 to 2020-07-31, and present
an empirical investigation of the outage reassignment during
the outage triage process. Due to the conÔ¨Ådential policy of
Microsoft Azure, we use variables to represent the sensitive
data, instead of disclosing the speciÔ¨Åc Ô¨Ågures.
A. The Life Cycle of Outages
In cloud computing platforms, incidents/outages are the
unplanned interruptions of the services, which could be
caused by many factors, such as power failures, hardware
failures, conÔ¨Åguration problems, and code bugs. For each
incident/outage, engineers assign a severity level based on their
potential impact on users. The severity level ranges from 0 to
4, where incidents with severity 0 have the highest priority
and may bring negative impacts to customers, and severity
4 incidents are least important and do not need to be dealt
with immediately. Engineers only declare a small portion of
incidents as outages, which are severe incidents and often
involve multiple services in the cloud computing platform.
Previous research has investigated four typical incident
management procedures, including incident reporting, triage,
mitigation, and resolution [8]. As illustrated in Figure 1,
the life cycle of outages is similar to that of incidents. The
following mainly demonstrates the detailed practice of outage
triage and emphasizes the differences between outages and
886ApplicationApplication
InfrastructureComputing
StorageComputing
InfrastructureStorage
Network
NetworkOriginating Services Root-cause Services Fig. 2: Outages‚Äô originating services and root-cause services.
incidents. After an incident is reported, On-call Engineers
(OCEs) will declare it as outage if they Ô¨Ånd the incident
may cause severe impact to end-users. Next, OCEs will try
to restore the service as soon as possible through a set of
pre-deÔ¨Åned instructions, i.e., steps which tell engineer how
to minimize the impact. These instructions are related to the
incident which the outage is declared from. However, as many
outages may have cross-service impacts (Section II-B), the
instructions usually do not work. For these outages, engineers
need to do outage triage to locate the root-cause service
and the responsible team. During the outage triage process,
after assigning the outage to a team, the engineers in this
team will conÔ¨Årm whether or not they are responsible for the
outage. If not, they will reassign the outage to another team
which is potentially responsible. This process is called outage
reassignment and it is repeated until the responsible team of
the root-cause service is found.
After that, to contain the outage‚Äôs impact to end-users,
the responsible team will try to debug the service and use
quick-Ô¨Åxes to mitigate the outage. Finally, to avoid the same
failure from happening again, engineers need to study the
outage in-depth and Ô¨Åx the problem permanently in the outage
resolution phase. At the same time, to learn the lessons from
the outage and assess its impact, engineers from all impacted
services may dive into the details of the outage, Ô¨Ånd the related
incidents, and mark the correlations among those incidents.
B. An Empirical Analysis of Outage Triage
Based on the outage data collected from Microsoft Azure,
we conduct an empirical investigation of outage triage. The
services in the online computing platform have complicated
dependencies. To better understand the relationships among
different services, we divide the services into Ô¨Åve categories
according to their functionality, namely Infrastructure ,Net-
working ,Storage ,Compute , and Application . Then we investi-
gate the practice of outage triage from two aspects: the number
and time cost of outage reassignments, respectively.
Services of different categories play various roles in the
cloud computing platform. The Infrastructure category lies
on the lowest level of the cloud computing platform and is
responsible for the hardware availability. All other services
rely on the Infrastructure category. The Network category is
responsible for network connectivity. The Storage category
is responsible for the management of storage resources. The
Cross-level Outages Cross-service Outages All OutagesApplication
NetworkingCompute
Storage
Infrastructure0 N/8 N/4 N N/2Fig. 3: The amount of the cross-service and cross-level outages
(sqrt scale).
Computing category is responsible for the management of
computing resources. The Application category lies on the
highest level of the cloud computing platform. Services in this
category are exposed to customers and rely on the supporting
services of other categories, directly or indirectly.
(1)Number of Outage Reassignments. The incident man-
agement system (IcM) records the reassignment behaviors
among different services. Figure 2 shows the relationships
between originating services and root-cause services of the
collected outages. We can learn that the cross-level outages,
whose originating service level and root-cause service level
are different, exist in all categories except Infrastructure .
In particular, for the outages whose root-cause services be-
longing to the Networking category, around 60% are cross-
level. Note that each category contains multiple services, and
the outages which are not cross-level outages could still be
cross-service outages. We categorize the outages according to
their originating service level and compute the amount of the
cross-level outages and cross-service outages. The results in
Figure 3 show that for Storage ,Networking , and Infrastructure
categories, nearly all the cross-service outages are also cross-
level outages. For cross-service outages in the Application and
Compute categories, about 60% of them are cross-level.
We further analyze the average number of outage reassign-
ments from different originating service categories. The result
in Table I shows that the average reassignment number of all
outages is nearly one. In particular, the outages whose origi-
nating services are in the Compute category have signiÔ¨Åcantly
higher average reassignment rate. This fact further motivates
the necessity of accurate outage triage at the service level.
TABLE I: The average number of outage reassignments from
different originating service categories.
Level Networking Storage Compute Application All
Avg 0.678 0.407 1.395 0.802 0.963
Among all cross-level outages, around 14% are transferred
from low-level categories to high-level categories. We analyze
those outages and summarize the following two reasons.
1)Method Caller Errors. Services in high-level categories
always call the functions of services in the low-level
categories, but the root cause of outages may exist
in the caller side beside the callee side. For example,
8871T 1/6T 2T 4T 6T 8T 
Networking Storage ComputeApplica/g415on Fig. 4: The outage reassignment time cost in different cate-
gories.
anApplication service query data through a function
of a Storage service, but wrong parameters lead to
outages. These outages‚Äô originating services belong to
theStorage category, but the root-cause services are
from the Application category.
2)Symbiotic Errors. The services of different categories
could inÔ¨Çuence each other because they run on the same
physical nodes. For example, a service about managing
network resources is from the Networking category, and
a monitoring application also runs on the same node.
Some network outages occur to the cloud computing
platform because of monitors taking too many physical
nodes‚Äô resources. These outages‚Äô originating services are
from the Networking category, but the root-cause service
is the monitoring application.
(2)Outage Reassignment Cost. The reassignment cost
indicates the increment of the outage triage time because of
assigning outages to the wrong services. The IcM records
an accurate time when transferring the outage to the speciÔ¨Åc
services. We calculate the time difference of outages trans-
ferring into and out of a wrong service as the cost of one
outage reassignment. We can obtain the total reassignment
cost of an outage by adding up all the reassignment cost
of this outage. We categorize the outages according to the
originating service level and compute each category‚Äôs average
reassignment cost. The results in Figure 4 demonstrate that
theNetworking category has a relatively larger reassignment
cost than other categories.
Based on the above analysis of outage triage from the
aspects of outage reassignment rate and cost, the outage
reassignment at the service level could incur huge time cost.
Accurate outage triage could signiÔ¨Åcantly improve the efÔ¨Å-
ciency of outage mitigation and reduce the cost.
C. A Real-world Outage Triage Example
Fig. 5: The triage path for the ‚Äúunexpected VM restart‚Äù outage.
To help better understand the outage triage process, we show
a real-world triage process for an ‚Äúunexpected VM restart‚Äùoutage. Figure 5 shows the triage path for this outage. First, the
VM (Virtual Machine) monitor detects the ‚Äúunexpected VM
restart‚Äù event on some VMs and reports an incident to the IcM.
Engineers assess the incident at Ô¨Årst. They consider that this
incident may cause severe impact to end-users, and declare it
as an outage. When diagnosing the VMservice, engineers Ô¨Ånd
that the storage the VMs use is not accessible, and they transfer
the outage to the related Storage service. Then engineers
of the Storage service Ô¨Ånd that the CPU and Memory load
of some storage services is unusually high, and the Storage
service is crashed due to an OOM (Out of Memory) exception.
They also Ô¨Ånd the reason for this phenomenon is a Ô¨Çood
of new connections to the storage cluster, which is caused
by the restart of the SLB (Service Load Balancer). Finally,
the outage‚Äôs root cause is conÔ¨Årmed by the related Network
service, which is responsible for the SLB, and engineers in
theNetwork service mitigate this outage by slowing down
the SLB restart process. According to this case, engineers
transfer the outage from service to service and the engineers of
each service need to understand the outage Ô¨Årst, diagnose the
service to conÔ¨Årm whether they are responsible for the outage
and if not, they need to assign the outage to the engineers of
another service manually.
III. T HEPROPOSED APPROACH TO OUTAGE TRIAGE
The example explained in Section II-C shows that the
service teams only know their direct dependency and always
triage the outage following dependency relations. Therefore,
locating the root-cause service without reassignment requires
a global view of dependencies for all related services. It is not
a trivial task for large-scale cloud computing platform because
the dependencies of inner services are incredibly complicated
and change frequently. Many services have interdependencies,
many dependencies abstracted by the platform are even im-
plicit for engineers, and some services deployed on the same
node may affect each other. Besides, service dependencies are
not static but related to speciÔ¨Åc problems.
COT constructs the service correlation graph of an outage
based on the incidents. When a new outage occurs in the cloud
computing platform, COT collects all the incidents in the same
region and adjacent time range with the outage. Then it Ô¨Ålters
the incidents which are directly or indirectly related to the
outage, and construct an incident correlation graph from these
incidents. Finally it maps the incidents to services to construct
a service correlation graph for the outage. The outages caused
by the same root-cause service have similar symptoms, i.e.,
service correlation graph (Section VI), which inspires us to
train a machine learning model to predict the root cause.
Our model takes the service correlation graph of historical
outages as training samples, and the label of each outage is the
root-cause service, which is labeled by engineers after outage
diagnosis.
Figure 6 gives an overview of our method, which contains
Ô¨Åve steps: incident data fetching, meta-incident ID generating,
incident correlation graph building, service correlation graph
888Fig. 6: An Overview of COT.
building, and root-cause service prediction. In the following,
we further explain the details of each step.
A. Incident Data Fetching
Since the impact of one outage usually has locality in time
and space, we only fetch the incidents which are reported in
the same region and adjacent time range with the outage. In
Microsoft Azure, the medium time of Ô¨Ånding the correct root-
cause service is T. And our statistics on historical outages
show that most of the early incidents related to an outage are
reported within about 2Tbefore the outage declaration time.
So we set the start time of the time window to 2Tbefore the
outage declaration time, which allows us to capture most of
the related early incidents. Also, to examine how our approach
is better than the traditional outage triage process, the end time
should be smaller than T. So we choose three time windows:
[ 2T;1
3T],[ 2T;2
3T]and[ 2T; T]. The last time window
[ 2T; T]is to examine the performance of our approach when
we get the same incident information as human engineers.
Note that the range of the time window should be different
in different systems, as the statistics of the early outage related
incidents and the average outage triage time are different. But
the method to determine the time window should be general,
thus people can follow this method to evaluate the performance
of the approach using different time windows and choose the
best time window that Ô¨Åts their system.
B. Meta-Incident ID Generating
To utilize the historical outage diagnosis data, we need to
distinguish incidents by the abnormal symptoms they repre-
sent, i.e.,Ô¨Ånd past incidents that represent the same abnormal
symptoms as newly occurring incidents. The ideal way is to
use monitor IDs to identify different abnormal symptoms from
incidents. This is reasonable for a well-designed monitoring
system, where every monitor has a unique ID. But according
to our experience of Microsoft Azure, the IcM system is a
hub that gathers incidents reported by many different sources
from hundreds, even thousands, of services. It is hard to deÔ¨Åne
general rules to constrain how the monitors should report
incidents. So, some monitors may use the same identiÔ¨Åcation,
some may monitor several different services at the same time,
Fig. 7: An example of parsing the template from an incident
title.
and some different monitors may monitor the same properties
of the same resource type in different regions.
In Microsoft Azure, the title of an incident usually summa-
rizes the symptom of the incident. However, we cannot directly
use incident titles to distinguish different abnormal symptoms,
since they are usually log-like texts and may contain variables
like service metrics, time or location. Thanks to the well
studied log parsing technology in recent years, we can use
the log parsing method to extract incident report templates
from their titles and use the templates to distinguish incidents
by the abnormal symptoms they represent. We go through the
historical incident data and assign a unique ID, called meta-
incident ID, to each of the incident template to obtain an
‚Äúincident template - meta-incident ID‚Äù mapping.
We follow the widely-used log-parsing approach SLCT [9]
to extract the template from incident titles automatically. The
original SLCT [9] log parser builds the word vocabulary
over all the log texts, picks out candidate words that appear
frequently, and uses these words to extract templates from log
texts. But in our case, directly using SLCT to parse incident
titles may bring one problem: some location representations
may contain multiple words and numeric values, like West
US 2 , but the SLCT will treat each part of the location
representation separately. To make the incident parsing more
precise, we manually add the location representations in Mi-
crosoft Azure to the vocabulary.
Figure 7 shows a log parsing example. The log parser Ô¨Årst
turns all words to lower-cases. The words in the built vocab-
ulary stay unmodiÔ¨Åed, and words representing locations are
recognized as <location> tokens, and others are recognized
as<variable> tokens. Besides, to prevent the case that
two different services use the same template, we combine the
owning service with the template to identify the incidents.
889For each incident in the past, we can use this method to
extract its template. We consider each incident template as a
different abnormal symptom in the system and give each of
them a unique meta-incident ID. For a new incident, we can
use the same incident-parsing method to extract the incident
template and obtain its corresponding meta-incident ID.
C. Incident Correlation Graph GI
Since a large-scale cloud computing platform contains a
large number of services, the reported incidents may come
from many different services, and the amount is usually very
large even within a small time window. Among them, only
a small proportion of incidents are related to the outage
(Section VI). In current practice, Ô¨Ånding related incidents of
a new outage relies on heavy manual work of experienced
engineers.
We use the historical outage diagnosis data to solve this
problem. We Ô¨Årst build the meta-incident ID correlation graph
GMfrom historical data to infer the potential links for newly
occurring incidents. For each pair of incidents labeled as
correlated in the outage resolution phase (Section II-A), we
parse them and obtain a pair of meta-incident IDs. Then we
take the meta-incident IDs as nodes and the their correlations
as edges to build an meta-incident ID correlation graph GM.
GMis further used to build the incident correlation graphs for
new outages.
Now we can build the incident correlation graph GIas we
already get the meta-incident IDs for the incidents fetched
earlier and the correlations among the meta-incident IDs in
GM. Algorithm 1 illustrates this process. Its input contains
three parts: GMis the meta-incident ID correlation graph; I
is the set of incidents that are reported near the outage in time
and locality; IOis the incident of the outage ( i.e.,the incident
where the outage is declared from). Its output is the incident
correlation graph GI.NGIandEGIhold the incidents and
edges in GI, respectively. At the beginning of the procedure,
IOis the only element in NGI, and EGIis empty. Then for
each loop, we go through each incident in I, get its meta-
incident ID, and check whether there exist any links in GM
that can link this incident to the existing graph. We will repeat
this loop until no new incidents can be added to the graph.
D. Service Correlation Graph GS
As we have mentioned in Section II-A, every outage is
well studied after mitigation, so the same root-cause bug of
a service is unlikely to happen again in the future. Even for
two outages that have the same root-cause service, their root-
cause bugs are usually different, so as the abnormal metrics
observed by monitors and the reported incidents. Thus it is
hard to refer to historical incident patterns for new outages.
However, although the malfunction of the root-cause service
is usually caused by different bugs, the spreading paths of
the anomaly among dependent services caused by the root-
cause service have similar patterns (Section VI). So we group
the incidents in the incident correlation graph GIby their
reporting services to generate the service correlation graph GS.Algorithm 1 The procedure of building GI
Input: GM,I,IO
Output: GI
1:procedure BUILD INCIDENT CORRELATION GRAPH
2: NGI fIOg
3: EGI fg
4: N0
GI NGI
5: E0
GI EGI
6:loop:
7: foreach incident IainI N0
GIdo
8: IDa meta-incident ID of Ia
9: foreach incident IbinN0
GIdo
10: IDb meta-incident ID of Ib
11: if< ID a; IDb>inGMthen
12: add incident IatoNGI
13: add edge < Ia; Ib>toEGI
14: ifsize of N0
GI6=size of NGIthen
15: N0
GI NGI
16: E0
GI EGI
17: goto loop
18: GI=fNGI; EGIg
19: return GI
The edges in GIare mapped to GSaccordingly. Algorithm 2
illustrates this process. We Ô¨Årst gather all the owning services
of the incidents in GItoNGS. These services are nodes in GS.
And then we map the incident links in GIto the corresponding
service links in GS. These links are the edges in GS. This
graph helps us refer to historical anomaly spreading patterns
at the service level and helps us predict the root-cause service
using the machine learning algorithm.
Algorithm 2 The procedure of building GS
Input: GI
Output: GS
1:procedure BUILD SERVICE CORRELATION GRAPH
2: NGI nodes in GI
3: EGI edges in GI
4: NGS fg
5: EGS fg
6: foreach incident IiinNGIdo
7: S owning service of Ii
8: add service StoNGS
9: forincident link < Ia; Ib>inEGIdo
10: Sa; Sb owning services of Ia; Ib
11: add edge < Sa; Sb>toEGS
12: GS fNGS; EGSg
13: return GS
890E. Training and Predicting
After the previous steps, although we have reduced the
search space of the outage-related incidents and services,
the incident correlation graph GIand the service correlation
graph GScan still be large since the impact of an outage is
usually wide-range. Besides, these services may have interde-
pendencies among each other. So engineers still need to dive
into the speciÔ¨Åc services and spend much time for analysis.
However, as mentioned in Section III-D, the spreading paths
of the anomaly among dependent services caused by the
same root-cause service have the same pattern, so we can
utilize machine learning algorithms to learn the patterns from
historical service correlation graphs, and to predict the root-
cause service automatically. We tried two machine learning
algorithms: the SVM (Support Vector Machine) algorithm
and the Decision Tree algorithm, which are widely used in
a variety of classiÔ¨Åcation tasks.
Model Training: For each outage in the past, we use the
methods mentioned above to build its incident correlation
graph GIand service correlation graph GS. The label of each
outage is its root-cause service, which is labeled manually
after outage diagnosis. And we take the structural information
ofGSas a feature vector, which contains two parts: 1) the
number of incidents in each service, and 2) the links between
these services.
For the Ô¨Årst part, each element represents a service, and
its value is the number of incidents included by this service
inGS. For the second part, each element represents a link
between two services, and its value is either 0 or 1, indicating
the existence of the link in GS. Note that, we have a large
number of services, and the links among them form a sparse
matrix. This part only includes the service links which occur
at least once in the past.
Root-cause Service Prediction: To predict the root-cause
service of a new outage, we use the same methods to build
its incident correlation graph GIand service correlation graph
GS. We then extract the feature vector from GSusing the same
method as in the model training part. Finally, the machine
learning model takes the feature vector as the input and
predicts the root-cause service as the output.
IV. E XPERIMENT
In this section, we describe the details of our experiments.
We will Ô¨Årst present the details of the data we collect for
experiments. Then we introduce the techniques we use to
implement our approach. Finally, we introduce DeepCT, the
state-of-the-art triage approach we use for comparison.
A. Data Collection
All the data we use in this paper are collected from the
production environment of Microsoft Azure. In the IcM system
of Microsoft Azure, the data are stored in a distributed NoSQL
database. We collect 9 months of incident and outage data from
the IcM system for training. SpeciÔ¨Åcally, we Ô¨Årst use the 9
months of data to build the ‚Äúincident template - meta-incident
ID‚Äù mapping and the meta-incident ID correlation graph GM.We build the incident correlation graph GIand the service
correlation graph GSfor each outage in the 9 months. We then
use the method in Section III-E to train the machine learning
model.
We collect another 3 months of incident and outage data
from the IcM system for testing. The outages in the test dataset
happen after the ones in the training dataset. We build the
incident correlation graph GIand the service correlation graph
GSfor each outage in the 3 months, and use the method in
Section III-E to predict the root-cause service for each outage.
All these outages involve 225 underlying services in Mi-
crosoft Azure and all the data occupies about 133GB of disk
space.
B. Implementation
Our programs are written in Python3 , a scripting language
that is widely used in data mining and machine learning tasks.
We also use some third-party Python3 packages to facilitate
our development process. We use the Python3 library Scikit-
learn to build the machine learning models. Scikit-learn is an
open-source machine learning library that supports supervised
and unsupervised learning.
C. Compared Method
We compare COT, which is a correlation-based triage ap-
proach, with DeepCT [7], which is the state-of-the-art triage
approach based on text-similarity of incident reports. DeepCT
incorporates a novel GRU (Gated Recurrent Unit) model with
an attention-based mask strategy and a revised loss function.
It can incrementally learn knowledge from discussions and
update incident triage results [7]. DeepCT has demonstrated
its effectiveness on 14 large-scale online service systems.
The data required by DeepCT contains three parts: 1) the
title and summary of an incident report, 2) the incremental
discussions about an incident, and 3) the occurring environ-
ment information of an incident. DeepCT uses a CNN-based
text encoder to produce feature vectors from the Ô¨Årst two
parts of data, which are textual. The third part of the data
is a Ô¨Ånite set of discrete values, so it uses representation
learning [10] to embed each input-datum value into a Ô¨Åxed-
dimension vector [7]. DeepCT uses these data to train the
designed GRU-based model to help predict the root cause of
an incident.
We follow the original paper of DeepCT to implement the
CNN-based text encoder, the representation learning model,
and the GRU-based model. Since outages are declared from
incidents, the three parts of data DeepCT requires are available
in outages. So we can easily migrate DeepCT to Ô¨Åt the outage
triage problem. We use the same training and test dataset as
COT uses to train and evaluate DeepCT.
V. E XPERIMENTAL RESULTS
In this section, we present the experimental results of
our approach and try to answer the following three research
questions.
891Fig. 8: The accuracy of COT and DeepCT in predicting the
root-cause service for outages.
RQ1: How does our approach work in outage triage,
comparing to the state-of-the-art triage approach?
RQ2: How is the performance of our approach on dif-
ferent kinds of outages?
RQ3: How is the time efÔ¨Åciency of our approach?
A. Evaluation Metrics
The goal of COT is to facilitate the outage triage process,
i.e., to help engineers Ô¨Ånd the correct root-cause service at the
early stage of the outage. For each outage, we use COT to
predict its root-cause service, compare it to the ground truth,
and calculate the accuracy. Besides, to better understand the
result, we further inspect its performance on different kinds of
outages, i.e., the outages whose root-cause services belong to
different service categories.
B. Performance
Figure 8 shows the accuracy of COT and DeepCT in pre-
dicting the root-cause service with different time windows. As
it shows, COT SV M outperforms COT DecisionTree slightly at
any time window. And these two models outperform DeepCT
a lot. The highest accuracy of DeepCT is just 55.5% and the
accuracy of COT SV M is higher than DeepCT by 28.029.7%.
As the result shows, the accuracy of COT SV M and
COT DecisionTree changes slightly as the time window grows.
This tells that the early signals of the outage supply enough
information for us to Ô¨Ånd the root-cause service, and as the
time window grows, the accuracy does not increase much.
Answer to RQ1: COT can help predict the root-cause
service of an outage at its early stage with high accuracy. It
outperforms DeepCT by 28.029.7% in predicting the root-
cause service.
We use the result of COT DecisionTree with time window
[ 2T;1
3T]as an example to show the performance of COT
on different kinds of outages (Figure 9). Other results have
the similar pattern with only slight difference. As Figure 9
shows, for outages whose root-cause services belong to the
Computing, Storage, or Network category, the accuracy is
Fig. 9: The accuracy of COT DecisionTree for different kinds
of outages when setting time window to [ 2T;1
3T]
0 5 10 25 15 20
[-2T, T]
[-2T, 2/3T]
[-2T, 1/3T]Time WindowsTime (seconds)
Fig. 10: The average time COT spends on predicting the root-
cause service for an outage in different time windows.
relatively high. For outages whose root-cause services belong
to the Application orInfrastructure category, the accuracy is
relatively low.
Different from the underlying supporting services, the Ap-
plication services change more frequently, which causes a
lot of new signals to emerge. Therefore, the service cor-
relation graphs for these outages change a lot over time.
This phenomenon is natural since the development of user-
oriented services should satisfy the rapidly changing business
requirements. This causes the low accuracy of the Application
category.
The low accuracy of the Infrastructure category is because
the number of outages of this kind is small in our dataset,
as the Infrastructure services is more robust and fails less
frequently. The machine learning algorithm only gains limited
knowledge from small samples. Its result should be better as
we collect more training samples in the future.
Answer to RQ2: The performance of COT is different on
different kinds of outages. For the cloud system we studied,
COT performs better for outages whose root-cause service is in
theComputing, Storage, or Network category. Further research
should be taken to tackle the Application andInfrastructure
categories.
C. Time EfÔ¨Åciency
To evaluate the time efÔ¨Åciency of COT, we record the
time it spends to predict the root-cause services of outages in
different time windows. For the convenience of deployment,
we package the COT project as a Docker image, which is a
widely used container technique. We deploy it on a machine
892equipped with a quad-core Intel¬Æ Xeon¬Æ E5-2673 v4 CPU,
and with 16 GB memory. The operating system is Ubuntu
16.04.6 LTS, the Docker version is 18.09.7, and the Python
version used in the Docker container is 3.7.
Figure 10 shows the results. For each outage, as the range
of the time window grows, the average time of prediction
is 19.4324.62s. This indicates that COT is efÔ¨Åcient for the
outage triage task. Our experiment shows that data fetching
takes a large proportion of the time. This is because the amount
of incidents within the time window is usually large, and
the incidents are stored in a large-scale distributed NoSQL
database. In the future, we can improve the time efÔ¨Åciency
of COT by caching the incident data and reducing the data
fetching time.
Answer to RQ3: The average running time of COT is
within one minute, which is efÔ¨Åcient for the outage triage
task.
VI. C ASE STUDY
COT is shown to be very helpful for production outage
management. In particular, the service correlation graphs built
by COT can substantially facilitate the understanding of the
complex multi-hop dependencies among services, which con-
sequently helps with quick mitigation of the outages. In this
section, we conduct a case study of two real-world outages
(Outage AandOutage B) in Microsoft Azure to understand
how COT works and how COT helps save time in the real-
world outage triage scenarios.
A. Two Real-world Cases
The root cause of Outage Ais some chained reactions of the
system followed by the activation of the DDoS defense policy
in Microsoft Azure. The DDoS attack is quite frequent in the
cloud computing scenario. Thus, cloud computing platforms
usually build DDoS defense policy at the network service
level to defense such attack. In Outage A, the DDoS defense
policy is triggered by some vicious trafÔ¨Åc but its reaction is
too aggressive, so it also treats other normal network trafÔ¨Åc
as vicious too. This causes a network conÔ¨Åguration server
to suffer from network package loss and fail to synchronize
the conÔ¨Ågurations it stores. One important conÔ¨Åguration in
this server is the mapping from service/component ID to
VIPs (Virtual IP Address). Due to the failure of conÔ¨Åguration
synchronization, this mapping is missing for a DNS service.
When other services read this conÔ¨Åguration, they cannot gain
the exact VIP which matches the DNS service. And following
the longest match of the service ID‚Äôs preÔ¨Åx, what they get is
a scope of VIPs belonging to a subnet. This causes abnormal
network trafÔ¨Åcs to the subnet and triggered the DDoS defense
policy to protect the subnet, which chooses to drop some
network packages targeting at the subnet for some time.
Outage Bis caused by the misconÔ¨Åguration of some routers.
Two engineers intend to modify the TCP MSS (Maximum
Segment Size) conÔ¨Åguration of two routers. They should have
modiÔ¨Åed this in the network control plane protocol, but they
did this in the data plane. The consequence is that every TCP
NetworkX Data Factory 
StorageX Speech Services
Data Movement 
SQL DB(a) The anomaly spreading
pattern among services in
Outage A
NetworkX Data Factory 
StorageX Speech Services
Data Movement 
SQL DB(b) The anomaly spreading
pattern among services in
Outage B
Fig. 11: The anomaly spreading pattern among services in
Outage AandOutage B. Each circle in the Ô¨Ågure indicates a
service in Microsoft Azure. The size of the circle indicates
the relative number of incidents reported in that service.
Two circles are linked if they are related. Some unimportant
services are ignored in this graph for the convenience of
explanation.
packet received by the router uses intensive CPU time for
checking. This causes the CPU usage level to be very high, and
the router starts to drop some packages due to the protection
policy.
The root-cause service is called NetworkX for both out-
ages. But their root-cause bugs are different and are both quite
complicated. This causes the monitored metrics in the system
to be different and further causes the reporting incidents to be
different. For example, the incident reported by NetworkX in
Outage Ais a node connectivity failure, and in Outage Bit is a
conÔ¨Åguration synchronization failure. However, the spreading
patterns of the anomaly among services are similar. Figure 11
shows the service correlation graphs of these two outages.
They all cause the availability of the network services to
be low in a subnet. This affects many supporting services
which are widely deployed in different regions and sensitive
to packet loss and network latency, like the data management
services ( Data Movement andData Factory ) and the
storage services ( StorageX andSQL DB ). These supporting
services further affect the user-oriented services. COT is able
to catch the spreading pattern of anomaly starting from the
NetworkX service in Outage A, and use this knowledge to
predict the root-cause service of Outage Bwith high accuracy.
The triage practice for both outages suffers from Ô¨Çood-
ing alarm problems [5]. For example, during the impact of
Outage B, there are Kincidents reported by 225 services
from the affected region of Outage Bin total. Among them,
only around 3% incidents reported from 12 services (5.33%
of 225) are related to Outage B. So it is hard for engineers to
Ô¨Ånd related incidents/services directly in the IcM system. The
outage is Ô¨Årst declared from a user-oriented service ( Speech
Services , which is used to convert spoken audio to text),
and the triage practice of these two outages is like the case
we show earlier in Section II-C, i.e., from the user-oriented
service to the real root-cause service, with many reassignments
893in this triage process. This causes the triage time of these two
outages to be very high. COT can save engineers from such
a tedious triage process by predicting the root-cause service
at the early stage of the outage, quickly and accurately. For
example, when setting the time window to [ 2T;1
3T], COT
can correctly predict the root cause of Outage Band can help
save nearly 80% of the triage time in theory.
B. Lessons Learned
We summary the lessons learned from the above two cases
as follows:
1) The root-cause bug of an outage is usually very compli-
cated. As these bugs are well repaired after the outage,
the same bug may not happen again in the future. This
causes the indicators of the impacted service metrics to
be different even if the root-cause services of the two
outages are the same. So, using only incident content
data is not sufÔ¨Åcient in the outage triage task.
2) The traditional outage triage practice is heavily affected
by the Ô¨Çooding alarm problem [5] in the IcM. During
the impact of an outage, the proportion of related inci-
dents/services is very low. So it is hard for engineers
to identify the most important incidents and services.
Therefore, in traditional triage practice, manual diagno-
sis and chained reassignment process are inevitable.
3) COT catches the anomaly spreading patterns of different
root-cause services from the historical outage diagnosis
data. It uses such knowledge to predict the root-cause
service for newly occurring outages effectively and
efÔ¨Åciently. This helps to save a large amount of outage
triage time, and reduce the time to mitigate an outage.
VII. T HREATS TO VALIDITY
COT highly depends on the historical incident correlation
data and outage diagnosis data. Currently, we cannot handle
zero-day incidents and new anomaly spreading patterns well.
But this can be mitigated by continuously updating our model,
i.e., regenerate the meta-incident ID correlation graph and
retrain the machine learning model regularly. We also plan to
handle the cases where the services and their dependencies
change more frequently (e.g., the Application category in
Section V-B) in our future work.
Another threat comes from our implementation of
DeepCT [7]. We let two members of our team to carefully
follow the paper of DeepCT to re-implement it. We test the
performance of our implementation using the incident triage
data of Microsoft Azure, and the results we get are consistent
with the results described in the original paper, only with small
differences. So our implementation should be consistent with
the original DeepCT.
Limited to the data we can obtain, the effectiveness of
our approach has only been conÔ¨Årmed in Microsoft Azure.
However, as Microsoft Azure is a leading cloud computing
platform with a mature IcM system, our experience should
be representative and should be applicable to other large-
scale cloud computing platforms in the market. Besides,although the data we use cannot be disclosed, the method is
reproducible as we have well explained the required details to
implement COT in Section III. In the future, we will try to
evaluate COT on other cloud computing platforms to better
understand its generality.
VIII. R ELATED WORK
A. Incident Triage
Some work focuses on incident triage in large-scale online
systems [6] [7] [11]. Chen et al. [6] perform a comprehensive
empirical study of incident triage on 20 real-world, large-
scale online service systems. Their work studies the status
of incident triage from many aspects and explores the practi-
cability of bug triage methods on incident triage. It concludes
that traditional bug triage approaches may beneÔ¨Åt the incident
triage task, but they still need to be further improved to
Ô¨Åt the context of incident triage. The work conducted by
Chen et al . [7] is the most relevant one to our work. They
propose a deep-learning approach named DeepCT to solve
the continuous incident triage problem. The authors utilize
the historical discussion data of incidents, use the domain-
speciÔ¨Åc text encoding method to extract features from these
discussions, and train the deep-learning model. They show the
effectiveness of their approach on 14 real-world online service
systems.
The above work is different from ours because they mainly
focus on incident triage within a single online service, while
our work focuses on the outage triage problem in the large-
scale cloud computing platforms which may involve hundreds
of different services of different types.
B. Fault Detection and Localization
Fault detection and localization in cloud systems have been
widely studied in previous work [12]‚Äì[17]. These methods
require constructing the causality graph among components in
the system. For example, Mariani et al. [16] exploit machine
learning to detect anomalies in KPIs and exploit the causal
relationships among KPIs and centrality indices to identify the
causes of the failures. Nguy et al. [12] propose FChain, which
detects anomaly from system metrics ( e.g., CPU, memory,
network statistics), and locate the cause by using both fault
propagation patterns and inter-component dependencies.
However, existing methods are not suitable for outage triage
in large-scale production clouds. Firstly, the dependencies
among components in cloud computing platforms are incredi-
bly sophisticated. Many services have interdependencies, and
many dynamic dependencies are even implicit for engineers.
Secondly, when a highly-impactful outage occurs, massive
incidents are reported by different services simultaneously.
It is also infeasible to apply other methods in our problem
settings as they address incidents one by one and greatly
suffer from scalability issues. Our method jointly analyzes
these incidents. SpeciÔ¨Åcally, COT focuses on the relationship
among incidents/services and conducts mitigations altogether
to avoid redundant efforts.
894C. Bug Triage
Bug triage has been widely studied in previous work [18]‚Äì
[34]. Some of the work uses the learning-based approaches.
Jonsson et al . [22] propose an ensemble learning model that
combines several classiÔ¨Åers to help assign a bug to the correct
development team automatically. Lee et al . propose to use
a convolutional neural network (CNN) and word embedding
techniques to build an automatic bug triager. Some of the
work is based on information-retrieval methods. Xia et al. [35]
propose a method that uses topic modeling to map the bug
reports to their corresponding topics. It assigns the bug to
the developer by considering the correlation between the
developer and the topics of the bug. Hu et al. [36] model the
relationships among developers, source code components and
their associated bugs from historical bug Ô¨Åxing data, and use
this knowledge to help assign a bug to the correct developer.
Different from these work that targets at bug triage for
traditional software systems, our work focuses on outage triage
for large-scale cloud computing platforms, which involve
hundreds of distributed services.
IX. C ONCLUSION
In this paper, we conduct the Ô¨Årst systematic empirical
study on the cross-service outage triage problem in large-
scale cloud computing platforms. We also propose COT, a
novel data-driven correlation-based outage triage approach.
COT mines the anomaly spreading patterns among services
caused by different root-cause failures from historical outage
diagnosis data. It then uses machine learning algorithms to
help predict the root-cause service of a new outage, and to
accelerate the outage triage process. We evaluate our approach
on the production environment of Microsoft Azure, one of the
top cloud providers around the world. The data we collected
occupies about 133GB of disk space and contains records of
outages and incidents from a whole year. Experiments show
that our model outperforms the state-of-the-art triage approach,
which is based on text-similarity, by 28.0 29.7% in accuracy,
and its overhead is within one minute.
In the future, we intend to conduct in-depth research on
the cases where services and service dependencies change
frequently. We will also apply our approach to more pro-
duction cloud systems to have a deeper understanding of the
approach‚Äôs generality.
ACKNOWLEDGEMENT
We thank our colleagues in Microsoft Azure who developed
the incident management system, for their kind help and
support in this work. This work was supported by the National
Natural Science Foundation of China (Project No. 61672164).
Hongyu Zhang is supported by ARC DP200102940.
REFERENCES
[1] Q. Lin, K. Hsieh, Y . Dang, H. Zhang, K. Sui, Y . Xu, J. Lou, C. Li,
Y . Wu, R. Yao, M. Chintalapati, and D. Zhang, ‚ÄúPredicting node failure
in cloud service systems,‚Äù in Proceedings of ACM SIGSOFT sympo-
sium and European conference on Foundations of software engineering
(ESEC/FSE) , pp. 480‚Äì490, 2018.[2] D. Ameller, M. Galster, P. Avgeriou, and X. Franch, ‚ÄúA survey on quality
attributes in service-based systems,‚Äù Software Quality Journal , vol. 24,
no. 2, pp. 271‚Äì299, 2016.
[3] Z. Li, H. Zhang, L. O‚ÄôBrien, R. Cai, and S. Flint, ‚ÄúOn evaluating
commercial cloud services: A systematic review,‚Äù Journal of Systems
and Software , vol. 86, no. 9, pp. 2371‚Äì2393, 2013.
[4] S. Tilley, Software Testing in the Cloud: Perspectives on an Emerging
Discipline: Perspectives on an Emerging Discipline . IGI Global, 2012.
[5] Z. Chen, Y . Kang, L. Li, X. Zhang, H. Zhang, H. Xu, Y . Zhou, L. Yang,
J. Sun, Z. Xu, et al. , ‚ÄúTowards intelligent incident management: why
we need it and how we make it,‚Äù in Proceedings of ACM Joint Meeting
on European Software Engineering Conference and Symposium on the
Foundations of Software Engineering , pp. 1487‚Äì1497, 2020.
[6] J. Chen, X. He, Q. Lin, Y . Xu, H. Zhang, D. Hao, F. Gao, Z. Xu,
Y . Dang, and D. Zhang, ‚ÄúAn empirical investigation of incident triage
for online service systems,‚Äù in Proceedings of IEEE/ACM International
Conference on Software Engineering: Software Engineering in Practice
(ICSE-SEIP) , pp. 111‚Äì120, IEEE, 2019.
[7] J. Chen, X. He, Q. Lin, H. Zhang, D. Hao, F. Gao, Z. Xu, Y . Dang,
and D. Zhang, ‚ÄúContinuous incident triage for large-scale online service
systems,‚Äù in Proceedings of IEEE/ACM International Conference on
Automated Software Engineering (ASE) , pp. 364‚Äì375, IEEE, 2019.
[8] Y . Chen, X. Yang, Q. Lin, H. Zhang, F. Gao, Z. Xu, Y . Dang,
D. Zhang, H. Dong, Y . Xu, H. Li, and Y . Kang, ‚ÄúOutage prediction
and diagnosis for cloud service systems,‚Äù in Proceedings of World Wide
Web Conference (WWW) , pp. 2659‚Äì2665, 2019.
[9] R. Vaarandi, ‚ÄúA data clustering algorithm for mining patterns from
event logs,‚Äù in Proceedings of IEEE Workshop on IP Operations &
Management (IPOM)(IEEE Cat. No. 03EX764) , pp. 119‚Äì126, IEEE,
2003.
[10] Y . Bengio, A. Courville, and P. Vincent, ‚ÄúRepresentation learning: A
review and new perspectives,‚Äù IEEE Transactions on Pattern Analysis
and Machine Intelligence , vol. 35, no. 8, pp. 1798‚Äì1828, 2013.
[11] J. Gu, J. Wen, Z. Wang, P. Zhao, C. Luo, Y . Kang, Y . Zhou, L. Yang,
J. Sun, Z. Xu, et al. , ‚ÄúEfÔ¨Åcient customer incident triage via linking with
system incidents,‚Äù in Proceedings of ACM Joint Meeting on European
Software Engineering Conference and Symposium on the Foundations
of Software Engineering , pp. 1296‚Äì1307, 2020.
[12] H. Nguyen, Z. Shen, Y . Tan, and X. Gu, ‚ÄúFchain: Toward black-box
online fault localization for cloud systems,‚Äù in Proceedings of IEEE
International Conference on Distributed Computing Systems ICDCS ,
pp. 21‚Äì30, 2013.
[13] P. Chen, Y . Qi, P. Zheng, and D. Hou, ‚ÄúCauseinfer: Automatic and
distributed performance diagnosis with hierarchical causality graph in
large distributed systems,‚Äù in Proceedings of IEEE Conference on
Computer Communications INFOCOM , pp. 1887‚Äì1895, 2014.
[14] C. Sauvanaud, K. Lazri, M. Ka ÀÜaniche, and K. Kanoun, ‚ÄúAnomaly
detection and root cause localization in virtual network functions,‚Äù in
Proceedings of IEEE International Symposium on Software Reliability
Engineering ISSRE , pp. 196‚Äì206, 2016.
[15] J. Thalheim, A. Rodrigues, I. E. Akkus, P. Bhatotia, R. Chen,
B. Viswanath, L. Jiao, and C. Fetzer, ‚ÄúSieve: actionable insights
from monitored metrics in distributed systems,‚Äù in Proceedings of
ACM/IFIP/USENIX Middleware Conference , pp. 14‚Äì27, 2017.
[16] L. Mariani, C. Monni, M. Pezz `e, O. Riganelli, and R. Xin, ‚ÄúLocalizing
faults in cloud systems,‚Äù in Proceedings of IEEE International Confer-
ence on Software Testing, VeriÔ¨Åcation and Validation ICST , pp. 262‚Äì273,
2018.
[17] J. Lin, P. Chen, and Z. Zheng, ‚ÄúMicroscope: Pinpoint performance issues
with causal graphs in micro-service environments,‚Äù in Proceedings of
International Conference Service-Oriented Computing ICSOC , pp. 3‚Äì
20, 2018.
[18] S.-R. Lee, M.-J. Heo, C.-G. Lee, M. Kim, and G. Jeong, ‚ÄúApplying
deep learning based automatic bug triager to industrial projects,‚Äù in
Proceedings of ACM SIGSOFT symposium and European conference on
Foundations of software engineering (ESEC/FSE) , pp. 926‚Äì931, 2017.
[19] A. Tamrawi, T. T. Nguyen, J. M. Al-Kofahi, and T. N. Nguyen, ‚ÄúFuzzy
set and cache-based approach for bug triaging,‚Äù in Proceedings of
ACM SIGSOFT symposium and European conference on Foundations
of software engineering (ESEC/FSE) , pp. 365‚Äì375, 2011.
[20] G. Jeong, S. Kim, and T. Zimmermann, ‚ÄúImproving bug triage with bug
tossing graphs,‚Äù in Proceedings of ACM SIGSOFT symposium and Eu-
ropean conference on Foundations of software engineering (ESEC/FSE) ,
pp. 111‚Äì120, 2009.
895[21] Y . Tian, D. Wijedasa, D. Lo, and C. Le Goues, ‚ÄúLearning to rank for bug
report assignee recommendation,‚Äù in Proceedings of IEEE International
Conference on Program Comprehension (ICPC) , pp. 1‚Äì10, IEEE, 2016.
[22] L. Jonsson, M. Borg, D. Broman, K. Sandahl, S. Eldh, and P. Runeson,
‚ÄúAutomated bug assignment: Ensemble-based machine learning in large
scale industrial contexts,‚Äù Empirical Software Engineering , vol. 21,
no. 4, pp. 1533‚Äì1578, 2016.
[23] Z. Lin, F. Shu, Y . Yang, C. Hu, and Q. Wang, ‚ÄúAn empirical study
on bug assignment automation using chinese bug data,‚Äù in Proceedings
of International Symposium on Empirical Software Engineering and
Measurement , pp. 451‚Äì455, IEEE, 2009.
[24] G. Bortis and A. Van Der Hoek, ‚ÄúPorchlight: A tag-based approach to
bug triaging,‚Äù in Proceedings of International Conference on Software
Engineering (ICSE) , pp. 342‚Äì351, IEEE, 2013.
[25] J. Anvik and G. C. Murphy, ‚ÄúReducing the effort of bug report triage:
Recommenders for development-oriented decisions,‚Äù ACM Transactions
on Software Engineering and Methodology (TOSEM) , vol. 20, no. 3,
pp. 1‚Äì35, 2011.
[26] A. S. Badashian, A. Hindle, and E. Stroulia, ‚ÄúCrowdsourced bug
triaging: Leveraging q&a platforms for bug assignment,‚Äù in Proceedings
of International Conference on Fundamental Approaches to Software
Engineering , pp. 231‚Äì248, Springer, 2016.
[27] R. Shokripour, J. Anvik, Z. M. Kasirun, and S. Zamani, ‚ÄúA time-based
approach to automatic bug report assignment,‚Äù Journal of Systems and
Software , vol. 102, pp. 109‚Äì122, 2015.
[28] H. Naguib, N. Narayan, B. Br ¬®ugge, and D. Helal, ‚ÄúBug report assignee
recommendation using activity proÔ¨Åles,‚Äù in Proceedings of Working
Conference on Mining Software Repositories (MSR) , pp. 22‚Äì30, IEEE,
2013.
[29] R. Shokripour, J. Anvik, Z. M. Kasirun, and S. Zamani, ‚ÄúWhy socomplicated? simple term Ô¨Åltering and weighting for location-based
bug report assignment recommendation,‚Äù in Proceedings of Working
Conference on Mining Software Repositories (MSR) , pp. 2‚Äì11, IEEE,
2013.
[30] D. Matter, A. Kuhn, and O. Nierstrasz, ‚ÄúAssigning bug reports using
a vocabulary-based expertise model of developers,‚Äù in Proceedings of
IEEE International Working Conference on Mining Software Reposito-
ries, pp. 131‚Äì140, IEEE, 2009.
[31] P. Bhattacharya and I. Neamtiu, ‚ÄúFine-grained incremental learning and
multi-feature tossing graphs to improve bug triaging,‚Äù in Proceedings
of IEEE International Conference on Software Maintenance (ICSM) ,
pp. 1‚Äì10, IEEE, 2010.
[32] S. Wang, W. Zhang, and Q. Wang, ‚ÄúFixercache: Unsupervised caching
active developers for diverse bug triage,‚Äù in Proceedings of ACM/IEEE
International Symposium on Empirical Software Engineering and Mea-
surement , pp. 1‚Äì10, 2014.
[33] M. Linares-V ¬¥asquez, K. Hossen, H. Dang, H. Kagdi, M. Gethers, and
D. Poshyvanyk, ‚ÄúTriaging incoming change requests: Bug or commit
history, or code authorship?,‚Äù in Proceedings of IEEE International
Conference on Software Maintenance (ICSM) , pp. 451‚Äì460, IEEE, 2012.
[34] A. S. Badashian, A. Hindle, and E. Stroulia, ‚ÄúCrowdsourced bug
triaging,‚Äù in Proceedings of IEEE International Conference on Software
Maintenance and Evolution (ICSME) , pp. 506‚Äì510, IEEE, 2015.
[35] X. Xia, D. Lo, Y . Ding, J. M. Al-Kofahi, T. N. Nguyen, and X. Wang,
‚ÄúImproving automated bug triaging with specialized topic model,‚Äù IEEE
Transactions on Software Engineering , vol. 43, no. 3, pp. 272‚Äì297, 2016.
[36] H. Hu, H. Zhang, J. Xuan, and W. Sun, ‚ÄúEffective bug triage based
on historical bug-Ô¨Åx information,‚Äù in IEEE International Symposium on
Software Reliability Engineering , pp. 122‚Äì132, IEEE, 2014.
896