Software Architecture Recovery with Information Fusion
Yiran Zhang
Nanyang Technological University
Singapore
yiran002@e.ntu.edu.sgZhengzi Xu
Nanyang Technological University
Singapore
zhengzi.xu@ntu.edu.sgChengwei Liuâˆ—
Nanyang Technological University
Singapore
chengwei001@e.ntu.edu.sg
Hongxu Chen
Huawei Technologies Co., Ltd
Shenzhen, China
chenhongxu5@huawei.comJianwen Sun
Huawei Technologies Co., Ltd
Shenzhen, China
sunjianwen4@huawei.comDong Qiu
Huawei Technologies Co., Ltd
Shenzhen, China
dong.qiu@huawei.com
Yang Liu
Nanyang Technological University
Singapore
yangliu@ntu.edu.sg
ABSTRACT
Understanding the architecture is vital for effectively maintaining
and managing large software systems. However, as software sys-
tems evolve over time, their architectures inevitably change. To
keep up with the change, architects need to track the implementation-
level changes and update the architectural documentation accord-
ingly, which is time-consuming and error-prone. Therefore, many
automatic architecture recovery techniques have been proposed
to ease this process. Despite efforts have been made to improve
the accuracy of architecture recovery, existing solutions still suffer
from two limitations. First, most of them only use one or two type
of information for the recovery, ignoring the potential usefulness
of other sources. Second, they tend to use the information in a
coarse-grained manner, overlooking important details within it.
To address these limitations, we propose SARIF, a fully auto-
mated architecture recovery technique, which incorporates three
types of comprehensive information, including dependencies, code
text and folder structure. SARIF can recover architecture more
accurately by thoroughly analyzing the details of each type of in-
formation and adaptively fusing them based on their relevance
and quality. To evaluate SARIF, we collected six projects with pub-
lished ground-truth architectures and three open-source projects
labeled by our industrial collaborators. We compared SARIF with
nine state-of-the-art techniques using three commonly-used archi-
tecture similarity metrics and two new metrics. The experimental
results show that SARIF is 36.1% more accurate than the best of the
âˆ—Chengwei Liu is the corresponding author
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
ESEC/FSE â€™23, December 3â€“9, 2023, San Francisco, CA, USA
Â©2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-0327-0/23/12.
https://doi.org/10.1145/3611643.3616285previous techniques on average. By providing comprehensive archi-
tecture, SARIF can help users understand systems effectively and
reduce the manual effort of obtaining ground-truth architectures.
CCS CONCEPTS
â€¢Software and its engineering â†’Software maintenance tools ;
Software reverse engineering ;Maintaining software .
KEYWORDS
software architecture recovery, software module clustering, reverse
engineering, architecture comparison
ACM Reference Format:
Yiran Zhang, Zhengzi Xu, Chengwei Liu, Hongxu Chen, Jianwen Sun, Dong
Qiu, and Yang Liu. 2023. Software Architecture Recovery with Information
Fusion. In Proceedings of the 31st ACM Joint European Software Engineer-
ing Conference and Symposium on the Foundations of Software Engineering
(ESEC/FSE â€™23), December 3â€“9, 2023, San Francisco, CA, USA. ACM, New York,
NY, USA, 13 pages. https://doi.org/10.1145/3611643.3616285
1 INTRODUCTION
Software systems change over time to include new features or for
maintenance purposes. During this process, actual implementation
of the system may become different from the initial architectural
design, which is known as the architectural drift anderosion [51].
Architectural drift anderosion can lead to wrong decisions on de-
velopment activities that result in a waste of time and development
resources. In order to prevent such drift orerosion , architects need
to understand the implemented architecture. However, such task is
costly, which can take hundreds of hours of an expert for a system
with hundred thousands of lines of code [22].
To reduce the manual effort required to maintain the architec-
ture, various techniques have been proposed to recover the sys-
tem architecture from its code implementation. These techniques
aim to associate software implementation-level entities (i.e., files,
functions or classes) with high-level system components (i.e., clus-
ters of entities) by clustering the entities according to their struc-
tural dependencies or textual information. For example, ACDC [ 77],
Bunch [ 48], and FCA [ 76] cluster the entities based on the structural
1535
ESEC/FSE â€™23, December 3â€“9, 2023, San Francisco, CA, USA Yiran Zhang, Zhengzi Xu, Chengwei Liu, Hongxu Chen, Jianwen Sun, Dong Qiu, and Yang Liu
trio.c
trio.h trionan.c
trionan.htriostr.c
triostr.h
triop.h triodef.hlibxml.hxpath.c
ZLQFRQILJ .h
Figure 1: Trio module in Libxml2
dependencies of software systems. Architecture Recovery using
Concerns (ARC) [ 24] mostly relies on the semantic, i.e., topic mod-
eling results of the source codes, to cluster the entities. SADE [ 56]
clusters the software system by combining both the dependency-
based and textual-based results. However, according to [ 45], the
current results of architecture recovery techniques are not accurate
enough for real-world applications. We summarize the two major
limitations of the existing solutions as follows.
First, the existing research recovers the architecture based on
only one or two types of information to group software entities,
despite a rich set of information can be helpful. For example, de-
pendency between entities is the most commonly used informa-
tion. In addition to dependency, ARC [ 24] has proven that textual
information (i.e. information from code text) is also useful for ar-
chitecture recovery. Moreover, package or folder structures of soft-
ware systems also carry important structural information on the
architecture [ 36]. Techniques focusing on only one or two types of
information rather than all these aspects may produce one-sided
and inaccurate recovery results of architecture.
Second, previous research tends to use coarse-grained informa-
tion, which lacks the details that are critical to architecture recovery.
For example, in dependency-based works [ 48,76,77], all the depen-
dencies are treated to have equal contributions to the clustering.
However, different types of dependencies have different influences
on the architecture. According to [ 71],Function Call dependencies
can reflect much more architectural information than Use Type
dependencies. Moreover, dependencies introduced by different en-
tities also have different impact on the architecture. For example,
dependencies between public interfaces of modules are more im-
portant than that between internal implementations [7].
To overcome these limitations, we propose SARIF, a fully auto-
mated architecture recovery technique that fuses dependency, code
text and folder structure information to produce comprehensive
recovery results. SARIF is a cross-platform architecture recovery
tool that currently supports C, C++ and Java. To obtain fine-grained
information, SARIF 1) refines the entity-level dependencies with
entity importance and dependency type; 2) measures the strength
of file-level dependencies based on the entity-level ones; 3) uses
TF-IDF and LDA to focus on topic information instead of raw code
text; 4) selects folder structures based on their quality; Last, SARIF
builds an adaptive model to fuse different types of information and
clusters the software entities to recover the architecture.
We evaluate SARIF with established metrics MoJoFM [ 78], a2a [ 37],
ğ‘2ğ‘ğ‘ğ‘£ğ‘”[37] as well as two additional metrics, Adjusted Rand Index(ARI) [ 29] andğ‘2ğ‘ğ‘ğ‘‘ğ‘—, as we notice that the commonly used archi-
tecture similarity metrics may not be adequate to produce unbi-
ased architecture comparison results. We compare SARIF against
7 previous techniques on 9 software systems with human-labeled
ground-truth architecture. The experiments show that SARIF out-
performs previous architecture recovery methods by being 36.1%
more accurate than the best of the previous results on average.
Moreover, we did an extensive experiment on 900 GitHub projects
to further verify the generalizability of SARIF.
In summary, our main contribution includes:
â€¢We propose SARIF, a software architecture recovery algo-
rithm that leverages the fine-grained program dependencies,
code text and folder structure to produce comprehensive
recovery results.
â€¢We introduce ARI to the scenario of comparing architectures,
and design a new metric ğ‘2ğ‘ğ‘ğ‘‘ğ‘—which solves limitations of
ğ‘2ğ‘to measure the architecture similarities.
â€¢We evaluate SARIF with 9 existing techniques on 9 software
systems with ground truth architectures. The results show
that SARIF outperforms other techniques by being 36.1%
more accurate on average.
2 MOTIVATING EXAMPLE
In this section, we illustrate our motivation with some examples
of architecture recovery results of existing techniques. We collab-
orated with a big software vendor company (name anonymized)
to manually label the architecture of Libxml2 [25] (v2.4.22), an
XML toolkit in C. Then, we compared the architectures recovered
by state-of-the-art architecture recovery tools with our manual
labeling, and found that the recovery results were unsatisfactory.
For example, we manually checked their result on the triomodule
ofLibxml2 . The triomodule is a portable implementation of printf
function family. It contains 8 files which are marked with gray
background in Fig 1. The triomodule is one of the most distinctive
module in Libxml2 from both structural and textual perspectives.
Structurally, 14 out of 17 file dependencies are internal dependen-
cies. Textually, 1811 out of 1857 triowords in Libxml2 are from this
module. However, most of the state-of-the-art architecture recovery
tools still failed to recognize this module by dividing it into multi-
ple modules or combining them with other modules. We manually
analyze the results of two tool, ARC and Bunch, and summarize
the reasons for their failures as follows.
ARC is a text-based architecture recovery tool. Its clustering
result is shown in Fig 1 by the color-coding of nodesâ€™ frames. ARC
divides the triomodule into three clusters. Two of them (red and
blue) only comprise files from the triomodule. However, the third
cluster (yellow) includes 3 files from trioandwin32config.h , which
is a configuration file and not really related to triomodule. This
is probably because win32config.h contains the most triowords
outside the triomodule (11 out of 46), even though it has completely
different functionalities from the triomodule. This demonstrates
that solely use textual information may be inadequate since the
code text does not necessarily reflect the codeâ€™s functionality.
Bunch recovers the architecture based only on dependency infor-
mation. It fails to identify the triomodule and clusters these 8 files
together with another 31 files. For example, xpath.c is the gateway
1536Software Architecture Recovery with Information Fusion ESEC/FSE â€™23, December 3â€“9, 2023, San Francisco, CA, USA
Source Code
Text from 
Source CodeDependency
Function
ImportancenlocWeight with
Dependency
Type
Folder
StructureStatic
Code
Analysis
Add
Weight
Latent
Dirichlet
Allocation
Remove
Unreasonable
Folders Final 
Weighted
Graph
Code&Comment Topic
InformationTopic
Correlation
Corr
Final Folder
Structure
Get
Information
Weights
Add Topic Info
Adjust Weight 
Using FolderMerge
Community
Detection
Stage 1: Information Extraction Stage 2: Information Processing Stage 4: Clustering Stage 3: Information Integration
Â§3.2.2. Text Â§3.2.1. 
Dependency
Â§3.2.3. Function
ImportanceÂ§3.3.2. Add Dependency Weight
Â§3.2.4. Folder
StructureÂ§3.3.1. Add Function 
Importance Weight
LDA
Â§3.3.4. Folder CleaningFinal Weighted Graph
Â§3.3.3. Topic
Correlation
Â§3.4.3 Weight 
AdjustmentÂ§3.5. Community Detection
1 - Information Extraction 2 - Information RefinementStage 4: Clustering
3 -  Information Fusion
Source Code
Source Code
Weighted 
Dep. Graph
Final Folder
Structure
Weighted Topic
CorrelationÂ§3.4.2 
Topic Info 
IntegrationÂ§3.4.1 Weight Assignment
Merge
Clustering
Text 
InformationDependency
Function
ImportanceDependency  Weight ing
Folder
StructureLDA
  Folder CleaningFinal Weighted Graph
Topic
Information
Community 
Detection
1 - Information Extraction 2 - Information Refinement3 -  Information Fusion
Source Code
Source Code
Weighted 
Dep. Graph
Final Folder
Structure
Weighted Topic
CorrelationFusing
4 - ClusteringFunction
Information
Texual
InfomationDependency
Function
ImportanceDependency  Weight ing
Folder
StructureLDA
  Folder Cleaning         Weighted Graph
Topic
Information
Community 
Detection
Â§III.B - Information Extraction Â§III.C - Information RefinementÂ§III.D -  Information Fusion
Source
CodeFusing
Â§III.E - ClusteringFunction
InformationWeighted 
Dep. Graph
Topic
Correlation
Cleaned
FoldersTexual
InfomationDependency
Function
ImportanceDependency  Weight ing
Folder
StructureLDA
  Folder Cleaning         Weighted Graph
Topic
Information
Community
Detection
Â§III.B - Information Extraction Â§III.C - Information Refinement Â§III.D -  Information Fusion
Source
Code
Â§III.E - ClusteringFunction
InformationWeighted 
Dep. Graph
Topic
Correlation
Cleaned
FoldersFusing
Texual
InfomationDependency
Graph
Function
ImportanceDependency  Weight ing
Folder
StructureLDA
  Folder Cleaning         Weighted Graph
Topic
Information
Community
Detection
Â§III.B - Information Extraction Â§III.C - Information Refinement Â§III.D -  Information Fusion
Source
Code
Â§III.E - Clustering?Weighted 
Dep. Graph
Topic
Correlation
Cleaned
FoldersFusing
Texual
InfomationDependency
Graph
Folder
StructureWeighted 
Dep. Graph
Â§ 3.1 - Information Extraction Â§ 3.2 - Information Refinement Â§ 3.3 -  Information Fusion
Source
Code
Â§ 3.4 - ClusteringWeighted 
Dep. Graph
Topic
Information
Cleaned
FoldersWeight
Dep.
Weight
Entity
LDA
Clean
Folder
Folder 
CleaningEntity  
Weight ing
Fuse
Info.
Community
DetectionTexual
InfomationDependency
Graph
Folder
StructureWeighted 
Dep. Graph
Â§ 3.1 - Information Extraction Â§ 3.2 - Information Refinement Â§ 3.3 -  Information Fusion
Source
Code
Â§ 3.4 - ClusteringWeighted 
Dep. Graph
Topic
Information
Cleaned
FoldersWeight
Dep.
Weight
Entity
LDA
Clean
FolderFuse
Info.
Community
Detection
Figure 2: Overview of our approach
of its xpath module, and many other modules communicate with
this module via xpath.c . Especially, the triomodule has the closest
dependencies related to xpath.c , which misleads Bunch to cluster
xpath.c into the triomodule. However, according to the dependency
analysis result of the Depends tool [ 2], only less than 1% (13/1359)
of entity-level dependencies related to xpath.c are related to the
triomodule. This indicates that although dependency information
is proven to be useful in architecture recovery, inadequate usage
could also bring biases to the architecture recovery.
SARIF fuses various aspects of information from the source codes,
including both dependency and textual information, while using
them in a fine-grained manner to cluster the software components
more precisely. According to our experiment, the triomodule can
be perfectly identified by SARIF, and the recovered result is much
more similar to the ground truth compared to Bunch and ARC.
3 METHODOLOGY
The overview of SARIF is presented in Fig 2. SARIF first extracts
three types of information from the given software. Then, it refines
the information to obtain fine-grained information with the help of
program analysis and machine learning techniques. Next, it fuses
the information into one weighted graph to represent relations
of the entire project. Last, based on the graph, SARIF clusters the
system into different groups to recover the architecture.
3.1 Information Extraction
In this section, we will introduce the details of how to extract the
three aspects of information, i.e., dependencies, textual information
and folder structure.
3.1.1 Dependencies. The dependency is the most important infor-
mation for recovering the architecture of software systems, as it
contains all the structural interactions in the software system. We
use a dependency graph to represent the dependencies between
entities in software systems. The nodes of the graph represent soft-
ware entities (e.g. file, class, method/function, variable, etc.), and
the edges represent the relation between two entities (e.g. function
call, file import, etc.). SARIF uses Depends [2] to generate depen-
dencies for target software systems. Depends generates dependency
information by observing the syntactic structures among software
elements in C/C++/Java projects. It can detect entities include func-
tions/methods, variables, type definitions, etc, along with 13 types
of relations between these entities. In this sense, the dependency
model of Depends can tell exactly the categories of both the depen-
dency relation and the nodes. This detailed information allows us
to generate more accurate dependency graphs of software systems,while previous studies suggest that more accurate dependencies
generally result in better architecture recovery results [45, 46].
3.1.2 Text from Source Code. Textual information can help the
recovery since the developers embed their domain knowledge into
code texts by means of comments, names of methods, classes, etc.
For example, as discussed in Sec 2, triomodule contains 1811 word
trioout of a total of 1857 in the entire system, which is a strong
indication of the boudary of the triomodule.
The textual information of a software system mainly comes
from two sources: the source code and the documentation. SARIF
mainly focuses on the texts in the source code since SARIF is a
fully automated tool and it is hard to automatically match the
documentation to their corresponding codes.
SARIF uses Ctags [ 18] and comment parser [ 1] to extract words
from different fields of functions and comments. To reduce the
noise introduced by unimportant words, only the following kinds
of words will be extracted: 1) filename, 2) definitions of classes,
functions/methods, and global variables, 3) comments.
3.1.3 Folder Structure. The folder structure shows how the devel-
opers organize the files, which gives hints to recover its architecture.
Therefore, it is included as one of our information sources. However,
not all of the folder structures can provide positive suggestions for
the architecture recovery [ 22]. For example, many C/C++ projects
organize their header files into a separate folder which provides
no insight into the architecture. Moreover, some folders like the
â€™mapred/libâ€™ folder of Hadoop (v0.2) contains multiple libraries for
various purpose. However, these libraries actually belong to mul-
tiple independent submodules. As a result, grouping all the files
within this folder together is not reasonable. We will address how
to eliminate these improper folder structures in the next step.
3.2 Information Refinement
3.2.1 Dependency Graph. In this step, we start with the unweighted
dependency graph extracted in Sec 3.1.1. To make it reflect the archi-
tecture more accurately, we apply two types of weights to the graph.
The first weighting is for entities, i.e., the nodes in the dependency
graph, since different entities have different degrees of impact on ar-
chitecture. For example, interfaces of modules have more influence
on the architecture than the modulesâ€™ internal implementations [ 7],
thus they should have higher weights. The second weighting is for
dependencies (i.e., the edges in the dependency graph), since differ-
ent types of dependencies carry different amounts of architectural
information [71].
Weigh Entities by Importance. The dependency graph comprises
different types of entities, e.g. files, functions, variables, etc. It is
unfair to evaluate the importance of all entities directly since their
granularity is not comparable. Therefore, we start with evaluating
the function-level entities, then derive the weight or entities at other
granularities based on it. The reason for prioritizing function-level
entities is that entities at the variable level have limited information,
and entities at the class/file level are too coarse-grained.
To measure node importance in a graph, a widely used algo-
rithm is PageRank [ 12]. PageRank evaluates a nodeâ€™s importance
by its popularity, i.e. a node is important if it can be easily accessed
1537ESEC/FSE â€™23, December 3â€“9, 2023, San Francisco, CA, USA Yiran Zhang, Zhengzi Xu, Chengwei Liu, Hongxu Chen, Jianwen Sun, Dong Qiu, and Yang Liu
by many other nodes. However, in the dependency graph, PageR-
ank may overvalue utility functions like parsers. For example, in
Libxml2 , the most important function identified by PageRank is
xmlStrEqual , which is a utility function that compares the strings
in XML. However, this function has nearly no indication on the
architecture since it is accessed by almost every module in Libxml2 .
Conversely, interfaces, which are impactful on the architecture [ 7],
tend to be less accessible thus undervalued. This scenario con-
tradicts our starting point that interfaces are more influential on
architecture than internal implementations like utilities. To address
this, we use Inversed PageRank (IPR) [ 26], which is a variant of
PageRank, to determine the importance of each function. Differ-
ent from PageRank, IPR is designed to find the influential nodes,
i.e., the nodes that can access as many as other nodes. Intuitively,
module interfaces have access to the moduleâ€™s internal implementa-
tions, thus will be assigned with higher importance. For example, in
Libxml2 , the functions with top 2 importance are: 1) main function
ofxmllint.c , which is the top interface of the xmllint module. 2)
xmlXPathNewContext , which is one of the most commonly-used
API of the xpath module.
To be more detailed, IPR assigns importance to the functions in
two ways. The first portion of importance is assigned based on the
count and quality of a functionâ€™s callees. This method iteratively
computes the IPR values by propagating importance from callees
to their callers until the results converge. The second portion of
importance is equally spread across all nodes. A specific parameter,
known as the damping factor ğ‘‘, decides the distribution of overall
importance between these two sections. Mathematically, IPR is
defined by the following equation:
ğ¼ğ‘ƒğ‘…(ğ‘›ğ‘–)=ğ‘‘âˆ‘ï¸
ğ‘›ğ‘–âˆˆğ‘ƒ(ğ‘›ğ‘–)ğ¼ğ‘ƒğ‘…(ğ‘›ğ‘—)
ğ‘–ğ‘›_ğ‘‘ğ‘’ğ‘”ğ‘Ÿğ‘’ğ‘’(ğ‘›ğ‘—)+1âˆ’ğ‘‘
ğ‘(1)
whereğ‘›1âˆ¼ğ‘›ğ‘are nodes in graph, ğ‘is the total number of nodes,
ğ‘ƒ(ğ‘›ğ‘–)are the direct successors of ğ‘›ğ‘–,ğ‘‘is the damping factor. In
SARIF,ğ‘‘is set to 0.85, which follows the recommendation of the
algorithmâ€™s authors [12]. The result of IPR ranges from 0 to 1.
To assess the importance of the functions, a function dependency
graph is extracted from the dependency graph by keeping only
the function-level entities and dependencies. The importance of a
function is then defined as its IPR result. Based on the importance
of functions, the importance of higher-level entities (e.g. file, class)
is defined as the sum of the functions they contained. The reasoning
behind using summation is that the dependencies associated with
these higher-level entities are considerably fewer than those related
to functions. Utilizing the sum is to guarantee that these higher-
level entities, which are also crucial for the architecture, will not
be overlooked due to their lesser dependency count. On the other
hand, the importance of lower-level entities is determined by evenly
dividing the significance of the parent entity amongst them to
prevent them from overshadowing the other entities.
Weigh Dependencies by Type. Entity-level dependency has many
different types, and different types of dependencies have different
level of indication on the architecture. For example, according to
the previous study [ 71],Function Call dependencies can reflect
much more architectural information than Uses Type (i.e., a func-
tion accesses a specific type) dependencies. However, this limitedTable 1: The optimized weights of different types of depen-
dencies. "MixIn" type is not applicable to C/C++/Java.
Implement Throw Call Create ImplLink Extend Use
7.575 9.053 0.177 1.665 9.33 2.159 0.509
Parameter Import Cast Return Contain MixIn
5.146 8.300 0.701 5.702 4.478 N.A.
understanding of their relative usefulness in architecture is not
enough for us to manually assign appropriate weights to all 13
types of dependencies extracted by Depends .
Therefore, we designed an algorithm to automatically assign
reasonable weights to each type of dependency. Since the target
of this weighting is to make the dependency graph reflect more
architectural information, our algorithm optimizes the weights to
make the weighted dependency graph can be clustered into a better
architecture. Specifically, our algorithm consists of 5 steps: 1) Ran-
domly set the weights of different types of dependencies; 2) Extract
the unweighted dependency graph, then weigh the edges based on
step 1; 3) Cluster the weighted graph using community detection
algorithm [ 16]; 4) Calculate the Girvan-Newman modularity quality
(MQ) [ 63] of the clustering result, where MQ is a commonly used
metric for measuring the quality of a clustering result; 5) Iterate
step 1 to 4 to find the weight that can maximize the modularity.
To get a reasonable result from our algorithms, we collected 100
project for each of the languages supported by SARIF (i.e., C, C++,
Java). The projects are collected by searching for popular projects
on GitHub to ensure their quality, and the projects that will be used
in the following evaluation sections have been excluded to prevent
presenting overfitting results. The algorithm is implemented with
hyperopt [31] optimizer. The weight of each type is limited between
0.1 and 10, the converge condition is set to when the loss is less
than 1e-5, and the optimization results are shown in Table 1. The
resulting weights basically align with our intuitive expectation and
the previous [ 71]. For example, ImplLink (similar to Function Call )
is assigned a much higher weight than Use(similar to Uses Type ).
This result will be combined with the entity importance to form a
weighted dependency graph.
Merge Weight Results. Based on both the weight of dependency
type and related entities, the final dependency weight is defined as:
ğ‘Šğ‘’ğ‘–ğ‘”â„ğ‘¡(ğ‘’)=ğ‘¡ğ‘¦ğ‘ğ‘’ _ğ‘¤ğ‘’ğ‘–ğ‘”â„ğ‘¡(ğ‘’)Â·ğ‘–ğ‘šğ‘ğ‘¡(ğ‘ ğ‘Ÿğ‘)+ğ‘–ğ‘šğ‘ğ‘¡(ğ‘‘ğ‘ ğ‘¡)
2(2)
whereğ‘¡ğ‘¦ğ‘ğ‘’ _ğ‘¤ğ‘’ğ‘–ğ‘”â„ğ‘¡(ğ‘’)is defined in Table 1, ğ‘–ğ‘šğ‘ğ‘¡(ğ‘ ğ‘Ÿğ‘)andğ‘–ğ‘šğ‘ğ‘¡(ğ‘‘ğ‘ ğ‘¡)
are the importance of the dependencyâ€™s source and target entity
in Eq 1, respectively. A file-level dependency graph is then formed
by grouping all entities into it s corresponding files. The weight of
dependency between two files is defined as the sum of its corre-
sponding entity-level dependencies.
3.2.2 Textual Information. The refinement of textual information
comprises three steps: preprocessing, weighing and topic modeling.
Preprocessing. SARIF preprocesses the extracted raw text by split-
ting the terms according to common naming conventions of pro-
grams, such as snake-case or camel-case. For example, the function
name "plotFigure" will be split into "plot" and "figure". Then, SARIF
applies commonly-used preprocessing techniques including lemma-
tization and stop word removal to refine the words.
Weighing. SARIF assigns different weights to the words based on
three criteria: 1) The importance of texts extracted from different
1538Software Architecture Recovery with Information Fusion ESEC/FSE â€™23, December 3â€“9, 2023, San Francisco, CA, USA
parts of the source code is not equivalent. For example, a word
in comments is likely to be less important than that in function
declarations. Thus, the texts will be weighted according to where
they come from; 2) A word from a less important entity is less
likely to be useful. Therefore, SARIF modifies the weights of words
according to its source entity as defined in Sec 3.2.1; 3) SARIF uses
TF-IDF [ 66] to decrease the weight of common words. The final
weight of each word will be the product of the three weights:
ğ‘Šğ‘’ğ‘–ğ‘”â„ğ‘¡(ğ‘¤)=ğ‘Šğ‘’ğ‘–ğ‘”â„ğ‘¡ğ‘ ğ‘Ÿğ‘(ğ‘¤)Â·ğ‘Šğ‘’ğ‘–ğ‘”â„ğ‘¡ğ‘’ğ‘›ğ‘¡ğ‘–ğ‘¡ğ‘¦(ğ‘¤)Â·ğ‘‡ğ¹âˆ’ğ¼ğ·ğ¹(ğ‘¤)(3)
Topic Modeling. SARIF summarizes the words from each file into
a topic via Latent Dirichlet Allocation (LDA) [ 10]. Specifically, it
trains a LDA model with the weighted words. Each source file is as-
signed with a topic embedding using the trained model. Last, SARIF
calculates the correlation between files on the topic embeddings.
3.2.3 Folder Structure. In this step, we aim to filter out folders
that most likely to be useless. As discussed in Sec 3.1.3, most of
the folders that have no indication on the architecture contain
files from multiple modules. These files have little dependencies
with each other but are strongly related to files outside the folder.
Based on this characteristic, our filtering algorithm is designed to
remove folders that have high inter-dependencies and low inner-
dependencies. Specifically, if the folder has more inter-dependencies
than the inner-dependencies, it will be eliminated by being merged
into its parent folder.
3.3 Information Fusion
Based on the processed information, SARIF fuses them to have
a more comprehensive understanding of the system architecture.
To this end, SARIF first evaluates the quality of each type of in-
formation, and assigns weight accordingly. Then, SARIF fuses the
information into a unified graph.
3.3.1 Weight Assignment. The three types of information can re-
veal different degrees of ground truth architecture from different
perspectives. Intuitively, the information with higher quality (i.e.,
can produce results closer to the ground truth architecture) should
be assigned a higher weight. Thus, we need to estimate the quality
of the three types of information, then assign weights accordingly.
Among the three information, the dependency information rep-
resents the relations between software entities, which objectively
reveal the interactions among the modules. Such characteristics en-
sures that the dependencies are highly likely to have the capacity to
reveal an adequate portion of the architecture. However, the quality
of textual information and folder structure is not as objective as the
dependencies as they are produced by the developers and inherently
subjuctive. Therefore, their quality could vary greatly in different
software projects. For instance, recall the motivating example in
Sec 2, the triomodule in Libxml2 has strong textual characteristics
so that the textual information can be a good indicator to distin-
guish modules. However, all the source files of Libxml2 are located
in its root folder, which means that the folder structure is useless in
the architecture recovery. Moreover, in other software projects, the
situation could be completely different. The builtin module of Bash-
4.2, for example, contains implementations of built-in commands
likecd,echo, etc. Due to the wide variety of commands within the
module, its textual information is overly general, which reduces itsTable 2: The weight of information of Libxml2 andBash .
Project textual information folder structure
Libxml2 0.639 0.068
Bash 0.141 0.703
usefulness in recovering the architecture. Consequently, text-based
techniques such as ARC struggle in identifying the builtin module
by grouping its files with those having similar text feature from
other modules. Despite the builtin module lacks distinctive textual
features, its content is identical to that in the builtin folder, which
means the module can be easily identified by the folder structure.
To handle such extreme variance in the quality of textual in-
formation and folder structure, a dynamic weight is assigned to
them by comparing the similarity of the architecture recovered
through these information with the architecture recovered through
dependencies. The underlying reason for such an assignment is
that the architecture recovered through dependencies is expected
to have some similarity with the ground truth due to the inherent
relations between dependencies and architecture. For textual in-
formation or folder structure, if its quality is high, the recovered
architecture based on it should have a higher similarity with the
ground truth, and therefore more likely to have higher similarity
with the architecture recovered through dependencies.
To implement this design, SARIF first recover the architecture
based on only one type of information. The ways SARIF recover
architecture with each type of information solely are as follows:
â€¢Dependency: The weighted dependency graph in Sec 3.2.1 is
utilized for clustering, in which SARIF groups the files through
the community detection algorithm [16].
â€¢Textual: An undirected weighted graph is generated based on
the topic correlations between files from Sec 3.2.2. The nodes
in the graph represent files, and the edges are weighted accord-
ing to the topic correlation between the two files. The graph is
then clustered by the hierarchical clustering algorithm with the
complete linkage metric [54].
â€¢Folder: The clustering result is formed by the filtered folder struc-
ture from Sec 3.2.3.
Next, the similarities between different types of information are
measured based on the ğ‘2ğ‘ğ‘ğ‘‘ğ‘—similarity of the recovered architec-
tures, where ğ‘2ğ‘ğ‘ğ‘‘ğ‘—will be detailed in Sec 4.2. The weight of each
supporting information is defined as:
ğ‘Šğ‘’ğ‘–ğ‘”â„ğ‘¡ğ‘ =ğ‘2ğ‘ğ‘ğ‘‘ğ‘—(ğ´ğ‘‘ğ‘’ğ‘,ğ´ğ‘ ) (4)
whereğ‘ refers to textual information of folder information, ğ´ğ‘ is
the recovered architecture of the corresponding information. Our
method is tested on Libxml2 andBash to confirm its ability to detect
differences in the quality of textual information and folder structure.
The results of the weight assignment for textual information and
folder structure are presented in Table 2, which match well with
our manual inspection of the two projects.
3.3.2 Textual Information Integration. Based on the weight of tex-
tual information, the weight of edges is adjusted according to the
textual correlations between the corresponding files. All existing
edges are modified with a coefficient ğ‘ğ‘œğ‘’ğ‘“ğ‘¡:
ğ‘ğ‘œğ‘’ğ‘“ğ‘¡=1+ğ‘ğ‘œğ‘Ÿğ‘Ÿâˆ—ğ‘Šğ‘’ğ‘–ğ‘”â„ğ‘¡ğ‘¡ğ‘’ğ‘¥ğ‘¡ (5)
1539ESEC/FSE â€™23, December 3â€“9, 2023, San Francisco, CA, USA Yiran Zhang, Zhengzi Xu, Chengwei Liu, Hongxu Chen, Jianwen Sun, Dong Qiu, and Yang Liu
whereğ‘ğ‘œğ‘Ÿğ‘Ÿ is the correlation of topic embedding between the two
files on the edge, and ğ‘Šğ‘’ğ‘–ğ‘”â„ğ‘¡ğ‘¡ğ‘’ğ‘¥ğ‘¡ is the weight of textual infor-
mation we assigned in Eq 4. Moreover, the purpose of applying
ğ‘ğ‘œğ‘’ğ‘“ğ‘¡is to increase the weight of dependency between two files
if their textual features are similar, and vice versa. To this end, if
ğ‘ğ‘œğ‘’ğ‘“ğ‘¡between two files are greater than 0.8, which means their tex-
tual features are highly similar, a bi-directional edge will be added
between the files even if they do not have structural dependencies.
3.3.3 Folder Structure Integration. As the files in the filtered folders
are more likely to belong to the same modules, the weight of edges
between files that are located in the same folder are increased by:
ğ‘ğ‘œğ‘’ğ‘“ğ‘“=1
1âˆ’ğ‘Šğ‘’ğ‘–ğ‘”â„ğ‘¡ğ‘“ğ‘œğ‘™ğ‘‘ğ‘’ğ‘Ÿ(6)
whereğ‘Šğ‘’ğ‘–ğ‘”â„ğ‘¡ğ‘“ğ‘œğ‘™ğ‘‘ğ‘’ğ‘Ÿ is the folder weight defined in Eq 4. ğ¶ğ‘œğ‘’ğ‘“ğ‘“
ranges from 1 to + âˆ. This coefficient is designed to enhance re-
lations between files in the same high-quality folder since a well-
structured folder is a strong indicator of a module.
3.3.4 Generate Final Graph. We generate the final graph by updat-
ing the edge weights of the input dependency graph from Sec 3.2.1:
ğ‘Šğ‘’ğ‘–ğ‘”â„ğ‘¡ğ‘“ğ‘–ğ‘›ğ‘ğ‘™ =ğ‘Šğ‘’ğ‘–ğ‘”â„ğ‘¡ğ‘œğ‘Ÿğ‘–Â·ğ‘ğ‘œğ‘’ğ‘“ğ‘¡Â·ğ‘ğ‘œğ‘’ğ‘“ğ‘“ (7)
whereğ‘Šğ‘’ğ‘–ğ‘”â„ğ‘¡ğ‘œğ‘Ÿğ‘–is the weight defined in Eq 2. Thus far, the graph
carrying all types of information is generated by updating the
weights and is ready for clustering.
3.4 Clustering the Final Graph
In this final step, we cluster the graph that carries the fused infor-
mation into modules. We use the community detection algorithm
proposed by Clauset et al. [ 16] for clustering. The algorithm cluster
a graph by maximizing its modularity which is defined as:
ğ‘„ğ›¾=1
2ğ‘šâˆ‘ï¸
ğ‘–,ğ‘—
ğ´ğ‘–ğ‘—âˆ’ğ›¾ğ‘˜ğ‘–ğ‘˜ğ‘—
2ğ‘š
ğ›¿ ğ¶ğ‘–,ğ¶ğ‘—(8)
whereğ´ğ‘–ğ‘—is the weight of edge from node ğ‘–toğ‘—;ğ‘˜ğ‘–denote the sum
of weights of edges incident on ğ‘–; m is the total weight of edges; ğ›¾is
a parameter also known as resolution , which affect ğ‘„ğ›¾â€™s preference
on cluster size. The reason we choose this community detection
algorithm is that it runs much faster than most of the existing
clustering algorithms like the Bunch search algorithm, especially
when the software system is large. The granularity, i.e., the number
of clusters, of the final architecture can be controlled by adjusting
theresolution parameter. Since different granularity of architecture
might be desired for different use cases, no universal standard can
be established for the granularity of architecture. Therefore, the
resolution of SARIF is user-controllable, and general correlation
between the number of clusters and the resolution is shown in Fig 3
for user reference. For users unsure about what level of granularity
to choose, we suggest 20 to 30 clusters as a suitable setup for quickly
understanding an architecture. Accordingly, the default value of
resolution is set to 1.7.
4 SIMILARITY METRICS
To evaluate architecture recovery results, a common method is to
compare the result with human-labeled ground truth architectures.
In previous studies, the three most commonly used metrics are
0.3 0.5 1.0 2.0 5.0 10.0 20.0 50.0
Resolution050100150200Number of Clusters(1.7, 26)(34, 184)Figure 3: Number of cluster v.s. resolution
MoJoFM, a2a, and ğ‘2ğ‘ğ‘ğ‘£ğ‘”. However, each of these metrics has its
own limitations. In this section, we will discuss these limitations,
and introduce two new metrics to address them.
4.1 Previous Metrics and Their Limitations
MoJoFM [78] is defined by the following equation:
ğ‘€ğ‘œğ½ğ‘œğ¹ğ‘€(ğ‘€)=
1âˆ’ğ‘šğ‘›ğ‘œ(ğ´,ğµ)
max(ğ‘šğ‘›ğ‘œ(âˆ€ğ´,ğµ))
Ã—100% (9)
whereğ‘šğ‘›ğ‘œ(ğ´,ğµ)is the minimum number of move or join opera-
tions needed to transform an architecture ğ´intoğµ. MoJoFM is the
most commonly used metric for comparing architectures. However,
MoJoFM prefers architectures with many small clusters [ 46] due to
its low cost for merging two clusters.
Architecture-to-architecture (ğ‘2ğ‘) [37] is a distance-based mea-
surement, which is defined as:
ğ‘2ğ‘(ğ´,ğµ)=
1âˆ’ğ‘šğ‘¡ğ‘œ(ğ´,ğµ)
ğ‘ğ‘ğ‘œ(ğ´)+ğ‘ğ‘ğ‘œ(ğµ)
Ã—100% (10)
whereğ‘šğ‘¡ğ‘œ(ğ´,ğµ)is the minimum number of operations needed to
transform architecture ğ´intoğµ,ğ‘ğ‘ğ‘œ(ğ´)is the number of operations
needed to construct architecture ğ´from a null architecture. ğ‘2ğ‘
has nearly no preference over the number of clusters. However,
studies show that it has a limited variations [45], which means an
architecture that greatly differs from the ground truth may not
receive a low score as it deserves.
ğ’„2ğ’„ğ’„ğ’—ğ’ˆ[37] is a metric defined by the number of similar clusters
between two architectures:
ğ‘2ğ‘ğ‘ğ‘£ğ‘”(ğ´,ğµ)=|ğ‘ ğ‘–ğ‘šğ¶(ğ´,ğµ)|
|ğ´|Ã—100% (11)
where|ğ´|is the number of clusters in ğ´;ğ‘ ğ‘–ğ‘šğ¶(ğ´,ğµ)is the num-
ber of similar clusters between ğ´andğµ, where two clusters are
defined to be similar if their overlapping percentage passed the
user-defined threshold ğ‘¡â„ğ‘ğ‘£ğ‘”. According to our experiments which
will be detailed in the following section, ğ‘2ğ‘ğ‘ğ‘£ğ‘”â€™s performance could
be unstable in many scenarios.
4.2 New Metrics
To address the limitations of previous metrics, two additional met-
rics are introduced in this paper.
Adjusted Rand Index (ARI) [29] is a well-known metric used to
quantify the degree of similarity between two distinct partitionings
of a single entity set. The versatility of ARI allows it to be applied
in comparing two architectures of the same system. In general, the
ARI computes the proportion of entity pairs that are consistently
clustered in both architectures. The higher proportion of consistent
1540Software Architecture Recovery with Information Fusion ESEC/FSE â€™23, December 3â€“9, 2023, San Francisco, CA, USA
Table 3: Information of projects in evaluation.
Name Version Language NLOC File Cluster Domain
ArchStudio 4 Java 238K 2305 57 IDE
Bash 4.2 C 115K 405 14 Shell
Chromium svn-171054 C++ 11.7M 46,498 67 Browser
Hadoop 0.19.0 Java 225K 1703 67 Distributed Framework
ITK 4.5.2 C++ 1.23M 8,504 11 Image Processing
OODT 0.2 Java 81.4K 892 216 Data Management
Libxml2 2.4.22 C 81.1K 82 17 XML Parser
HDC 46ff87 C++ 25.7K 207 11 Camera Interface
HDF 0e196f C 153K 1,051 15 Driver Subsystem
pairs, the more similar the two partitionings are. Mathematically,
ARI is calculated as follows:
ğ´ğ‘…ğ¼=Ã
ğ‘–ğ‘— ğ‘›ğ‘– ğ‘—
2âˆ’hÃ
ğ‘– ğ‘ğ‘–
2Ã
ğ‘— ğ‘ğ‘—
2i
/ ğ‘›
2
1
2hÃ
ğ‘– ğ‘ğ‘–
2+Ã
ğ‘— ğ‘ğ‘—
2i
âˆ’hÃ
ğ‘– ğ‘ğ‘–
2Ã
ğ‘— ğ‘ğ‘—
2i
/ ğ‘›
2(12)
In this formula, ğ‘ğ‘–andğ‘ğ‘—correspond to the total count of entities
present in the ğ‘–-th cluster of architecture ğ´and theğ‘—-th cluster of
architecture ğµ, respectively. ğ‘›ğ‘–ğ‘—is the count of entities that are
both in the ğ‘–-th cluster of architecture ğ´and theğ‘—-th cluster of
architecture ğµ.ğ´ğ‘…ğ¼ is widely recognized and extensively tested,
exhibiting no evident drawbacks such as a low dynamic range or a
bias towards cluster size. Our evaluation which will be detailed in
the following section further proved its effectiveness.
ğ’‚2ğ’‚ğ’‚ğ’…ğ’‹is designed by us to address the issue of limited variation
observed in the ğ‘2ğ‘measure. The limited variation in ğ‘2ğ‘originates
from the excessively large denominator, which is the sum of the
architecture costs ğ‘ğ‘ğ‘œ(ğ´)+ğ‘ğ‘ğ‘œ(ğµ), used in the calculation of ğ‘šğ‘¡ğ‘œ.
To alleviate this issue, we decompose the distance (i.e., ğ‘šğ‘¡ğ‘œ) of
ğ‘2ğ‘into two distinct components: 1) ğ‘šğ‘¡ğ‘œğ‘š, which corresponds to
the cost of reassigning entities to a different cluster; and 2) ğ‘šğ‘¡ğ‘œğ‘ğ‘Ÿ,
reflecting the cost of adding or removing entities/clusters. Our
adjusted measure ğ‘2ğ‘ğ‘ğ‘‘ğ‘—is then defined as follows:
ğ‘2ğ‘ğ‘ğ‘‘ğ‘—=1âˆ’ğ›¼Â·ğ‘šğ‘¡ğ‘œğ‘š(ğ´,ğµ)
ğ‘šğ‘¡ğ‘œğ‘šğ‘ğ‘¥ğ‘š(ğ´,ğµ)âˆ’ğ›½Â·ğ‘šğ‘¡ğ‘œğ‘ğ‘Ÿ(ğ´,ğµ)
ğ‘ğ‘ğ‘œ(ğ´)+ğ‘ğ‘ğ‘œ(ğµ),
ğ‘¤â„ğ‘’ğ‘Ÿğ‘’ğ›¼ =ğ‘›ğ‘ â„ğ‘ğ‘Ÿğ‘’ğ‘‘+ğ‘šğ‘–ğ‘›(ğ‘›ğ‘ğ´,ğ‘›ğ‘ğµ)
ğ‘›ğ‘ â„ğ‘ğ‘Ÿğ‘’ğ‘‘+ğ‘›ğ‘‘ğ‘–ğ‘“ğ‘“+ğ‘šğ‘ğ‘¥(ğ‘›ğ‘ğ´,ğ‘›ğ‘ğµ),
ğ›½=ğ‘›ğ‘‘ğ‘–ğ‘“ğ‘“+ğ‘ğ‘ğ‘ (ğ‘›ğ‘ğ´âˆ’ğ‘›ğ‘ğµ)
ğ‘›ğ‘ â„ğ‘ğ‘Ÿğ‘’ğ‘‘+ğ‘›ğ‘‘ğ‘–ğ‘“ğ‘“+ğ‘šğ‘ğ‘¥(ğ‘›ğ‘ğ´,ğ‘›ğ‘ğµ)(13)
ğ‘›ğ‘ â„ğ‘ğ‘Ÿğ‘’ğ‘‘ andğ‘›ğ‘‘ğ‘–ğ‘“ğ‘“ denote the number of shared and unshared
elements, respectively. ğ‘›ğ‘ğ´andğ‘›ğ‘ğµare the counts of clusters in
architecture ğ´andğµ. The measure ğ‘2ğ‘ğ‘ğ‘‘ğ‘—quantifies the dissim-
ilarity between two architectures with two costs: 1) the cost of
reassignment of shared entities to different clusters; 2) the cost of
an addition or removal of entities or clusters. Each of the two costs
is individually normalized by their appropriate denominators. The
reassignment cost ğ‘šğ‘¡ğ‘œğ‘šis normalized by the maximum possible
reassignments ğ‘šğ‘¡ğ‘œğ‘šğ‘ğ‘¥ğ‘š given the number of entities and clusters,
as solved in [ 13]. The addition/removal cost ğ‘šğ‘¡ğ‘œğ‘ğ‘Ÿis normalized
by the cost of building both architectures from scratch.
In the final step, the costs associated with these two aspects
are weighted according to the number of shared and unshared
components. By normalizing the two costs using a more appropriate
denominator, the resulting dynamic range of ğ‘2ğ‘ğ‘ğ‘‘ğ‘—is significantlylarger than that of the original ğ‘2ğ‘metric. This expanded dynamic
range will be further proved in the evaluation section.
5 EVALUATION
To evaluate SARIF, we aim to answer the following RQs:
RQ1 : What is the performance of the proposed similarity metrics
in evaluating architecture recovery accuracy?
RQ2 : What is the accuracy of SARIF in architecture recovery com-
pared to related works?
RQ3 : What is the contribution to the architecture recovery for each
type of information and weighting?
5.1 Experimental Setup
5.1.1 Baseline Selection. Our selection of baseline techniques con-
sists of two parts. The first part comprises 5 tools who show the
most promising results in the previous empirical studies [ 21,45,46],
including ACDC, Bunch, ARC, WCA and LIMBO. ACDC [77]
is a pattern-based clustering algorithm. Bunch [48] is a search-
based technique that recovers architecture by optimizing TurboMQ.
ARC [24] recovers architecture with topic information rather than
dependency. WCA [49] and LIMBO [6] are two hierarchical clus-
tering algorithms that can be applied to architecture recovery.
The second part contains the latest research on architecture
recovery. We reviewed a total of 39 papers related to architecture
recovery technique in the recent five years [ 5,8,9,11,15,20,23,27,
28,30,32â€“35,38,39,41â€“44,47,50,53,55,56,59â€“62,65,68,70,72â€“
74,76,80â€“82]. 5 of 39 papers [ 11,56,60,76,80] contain link to their
supplementary materials, and 4 of them [ 56,60,76,80] provide
their artifact for reproduction. Among the 4 techniques, EVOL [ 80]
and CodeSum [ 60] take some intermediate data as input and we
failed to generate such data for them since the data formats are not
documented. Thus, the second part of baseline techniques comprises
the remaining two techniques: FCA [76] and SADE [56].
FCA [76] clusters software systems by performing operations
on the dependency matrix. It has good scalability and can cluster
very large software systems within a reasonable amount of time.
SADE [56] clusters software systems by combining the call graph
with the semantic similarity between the user-defined modules.
5.1.2 Baseline Implementation and Parameters. We obtained the
executable of ACDC, Bunch, FCA and SADE from the authorâ€™s web-
sites. For WCA, LIMBO and ARC, we adopted the implementations
from ARCADE [ 69]. Since WCA, LIMBO and ARC allows user to
select a preset number of clusters, we ran these tools with 10 to 100
clusters with an incremental step of 5. In addition, the ARC tool
takes the number of concerns as input as well. We experimented
ARC with 50 to 150 topics with a step of 10.
5.1.3 Data Collection. Our evaluation utilizes a two-part dataset.
The first part includes projects with well-established architectures,
which are used as a benchmark for assessing the accuracy of SARIF
and comparing SARIF to the baselines. 6 open-source projects with
ground-truth labeled in previous studies are included in this part:
ArchStudio4, Bash-4.2, Chromium, Hadoop, ITK and OODT [ 22,45].
Moreover, 3 projects, Libxml2 [ 25], Distributed Camera [ 3], and
Drivers Framework [ 4], have been labeled with our industry collab-
orators. Details of these projects are listed in Table 3. The size of
1541ESEC/FSE â€™23, December 3â€“9, 2023, San Francisco, CA, USA Yiran Zhang, Zhengzi Xu, Chengwei Liu, Hongxu Chen, Jianwen Sun, Dong Qiu, and Yang Liu
(a) MoJoFM, a2a, ARI and ğ‘2ğ‘ğ‘ğ‘‘ ğ‘—
 (b)ğ‘2ğ‘ğ‘ğ‘£ğ‘”w/ different thresholds
Figure 4: Metrics results of the merge experiment.
(a) MoJoFM, a2a, ARI and ğ‘2ğ‘ğ‘ğ‘‘ ğ‘—
 (b)ğ‘2ğ‘ğ‘ğ‘£ğ‘”w/ different thresholds
Figure 5: Metrics results of the 9 cluster experiment.
these projects, as measured by lines of code, ranges from about 20K
to 11M, which is either equivalent to or exceeds the scale of projects
used in previous studies [ 20,46,75]. The second part comprises
900 popular projects from Github. The purpose of including this
extensive dataset is to demonstrate the generalizability of SARIF.
To this end, we collected projects for each of the languages that
supported SARIF, i.e., C, C++ and Java. To avoid biased distribution
of project size, we gathered projects that fell within three different
size ranges: 0-10 MB, 10 MB-100 MB, and â‰¥100 MB. We used the
GitHub API to search for repositories with the highest number of
stars , and excluded the projects used in the parameter optimization
detailed in Sec 3.2.1. For each size range and language combination,
100 projects are collected, resulting in a total of 900 projects.
5.2 Metric Performance Evaluation (RQ1)
In RQ1, we aim to evaluate the metrics on their ability to measure
the similarity between architectures with two experiments. All five
metrics discussed in Sec 4 will be evaluated in this RQ: MoJoFM, a2a,
ğ‘2ğ‘ğ‘ğ‘£ğ‘”, ARI, andğ‘2ğ‘ğ‘ğ‘‘ğ‘—. Sinceğ‘2ğ‘ğ‘ğ‘£ğ‘”has a threshold parameter, four
different thresholds are tested following the settings of previous
studies [37, 45]: 66%, 50%, 33% and 10%.
Experiment 1: To evaluate the ability of the metrics in measuring
the architecture similarity, we design an experiment to simulate
the situation where the target architecture becomes less and less
similar to the ground truth. The expectation is that the metrics
should give a lower score when the target architecture becomes
dissimilar. Specifically, we start by comparing the ground truth
architecture of Hadoop with itself where all the metrics will give
the highest score (i.e. 1.0). Then, we modify the architecture by
merging its clusters and comparing it with the original ground
truth again to get the metric scores. As we merge more clusters,
the resulting architecture will be more different from the original
one, to which a lower score should be given by the metrics.The results are shown in Fig 4, where the x-axis shows the
number of clusters merged (i.e. the degree of dissimilarity) and
the y-axis shows the metric scores. Four metrics in Fig 4(a) fit our
expectation since their scores keep dropping during the merge.
However, the score of a2a is not dropping as much as the rest.
It complies with the conclusion in [ 45] that a2a has a relatively
small variation range. It will always give a relatively good score
for all kinds of architecture. As for ğ‘2ğ‘ğ‘ğ‘£ğ‘”shown in Fig 4(b), the
trend of score does not align well with our expectation. When
the threshold is set to 0.1, the score is stucked at 1, which means
ğ‘2ğ‘ğ‘ğ‘£ğ‘”could produce totally useless result if the threshold is set
inproperly. Moreover, for the other thresholds, ğ‘2ğ‘ğ‘ğ‘£ğ‘”is overall
dropping, but not stable enough: It fluctuates significantly during
the merge process, which mismatches our intuitive expectation.
Experiment 2: In the second experiment, we aim to test the metric
quality when the target architecture consists of different numbers of
clusters. To achieve this, we generate a set of 9 well-separated clus-
ters of data as the ground truth. Then, we use a k-means algorithm
to cluster the data into 1-30 clusters as 30 different architectures.
The architecture with 9 clusters is the same as the ground truth.
Therefore, all the metric scores will be 1. If the number of clusters
in the architecture becomes less or more than 9, the architecture
will become different from the ground truth, which is expected to
receive a lower score. The results are shown in Fig 5, where the
x-axis shows the number of result clusters.
As expected, all the metrics give a score of 1 to the architecture
with 9 clusters. Similar to the last experiment, four metrics shown
in Fig. 5(a) are in line with our expectations. Specifically, MoJoFM
gives all the architectures with more than 9 clusters a score close
to 1, which suggests that it favours smaller clusters. As for ğ‘2ğ‘ğ‘ğ‘£ğ‘”
shown in Fig 5(b), though it can sometimes give a reasonable result,
it still suffers from two limitations similar to the last experiment:
1) a proper threshold need to be selected, 2) the trend is not stable
enough even if a proper threshold is set.
Based on the result of these two experiments, we do not find
any substantial limitations of ARI and ğ‘2ğ‘ğ‘ğ‘‘ğ‘—. For MoJoFM, it can
also generate reasonable scores in most cases, though it favors
partitions with a higher number of clusters. For a2a, the trend of
score is satisfactory, but its dynamic range is limited. As for ğ‘2ğ‘ğ‘ğ‘£ğ‘”,
its performance relies heavily on a proper threshold, while may
still gives unreasonable results even if the threshold is appropriate.
Answering to RQ1: ARI and ğ‘2ğ‘ğ‘ğ‘‘ğ‘—are the best metrics in the
task of architecture similarity evaluation. MoJoFM, a2a, and
ğ‘2ğ‘ğ‘ğ‘£ğ‘”have their limitation in particular scenarios.
5.3 Recovered Architecture Evaluation (RQ2)
In this RQ, we evaluate SARIF against baselines by comparing their
recovered architectures against human-labeled ground truth.
5.3.1 Metric Results. We evaluate SARIF and all the previous tools
on the ground-truth projects with the setups detailed in Sec 5.1.
Although the outcome of RQ1 indicates that MoJoFM, a2a and
ğ‘2ğ‘ğ‘ğ‘£ğ‘”have their limitations, all five metrics are included in our
evaluation for the sake of completeness. The threshold of ğ‘2ğ‘ğ‘ğ‘£ğ‘”is
set to 0.66 since it gave the most reasonable result in RQ1. A total
1542Software Architecture Recovery with Information Fusion ESEC/FSE â€™23, December 3â€“9, 2023, San Francisco, CA, USA
Table 4: Similarities between recovered architectures and ground truths. Top-2 scores of each project are highlighted. All metric
values are timed by 100. (Abbreviations: M-MoJoFM, A-a2a, C- ğ‘2ğ‘ğ‘ğ‘£ğ‘”, R-ARI, J-ğ‘2ğ‘ğ‘ğ‘‘ğ‘—)
Technique ArchStudio Bash Chromium Hadoop ITK OODT HDC HDF Libxml2
M A C R J M A C R J M A C R J M A C R J M A C R J M A C R J M A C R J M A C R J M A C R J
ACDC 738624 20 5050 81 4 13 40 48 82 1 7 37 46 82 5 13 33 40 73 0 2 25 4485 17 244549 82 0 11 32 38 79 0 13 39 34 83 25 13 40
ARC 57 85 10 32 47 28 80 0 3 36 33 83 0 1 32 40 83 13 19 35 76 77 0 0 32 30 82 17 12 38 37 81 0 8 37 36 80 0 12 38 29 83 6 6 32
Bunch-NAHC 39 82 6 14 34 39 83 0 9 26 57 73 0 0 20 34 82 0 12 31 40 80 0 1 15 14 77 0 6 29 55 86 0 24 37 42 84 0 11 35 22 81 0 9 36
Bunch-SAHC 62 85 7 22 41 45 83 4 12 31 27 74 0 0 12 34 81 0 15 33 54 78 0 4 14 14 77 0 6 29 65 88 0 34 48 40 86 0 17 38 3886 11 20 47
FCA 60 82 9 10 43 41 80 2 13 40 58 76 0 3 29 49 82 7 11 38 44 73 0 1 25 5086 14 234453 78 0 10 36 32 78 0 15 42 37 84 10 8 37
LIMBO 27 80 0 -0 17 26 80 0 0 22 31 78 0 0 11 15 80 0 1 18 50 78 0 0 6 12 79 0 0 22 25 80 0 1 30 20 81 0 -0 31 22 84 3 2 43
SADE 42 83 0 11 41 45861721425887242648418322234460 81 0 9 24 21 78 17 13 34 728884153498822304734 84 17 22 44
WCA-UE 31 82 0 3 35 40 81 0 9 37 31 82 0 2 29 15 79 0 -1 19 49 86 0 -7 42 11 78 3 0 22 51 80 0 11 34 45 82 5 26 46 33 84 7 10 45
WCA-UENM 31 82 0 3 35 38 80 0 7 32 31 82 0 2 29 15 79 0 -1 19 4986 0 -7 4211 78 3 0 22 48 80 0 10 34 44 82 3 23 44 29 83 3 6 44
SARIF 67883642557087955646387173249598724495368862304429 80 14 20 41 909033596256881335504989332955
Table 5: Averaged scores of recovery techniques.
Technique MoJoFM a2a ğ‘2ğ‘ğ‘ğ‘£ğ‘” ARIğ‘2ğ‘ğ‘ğ‘‘ ğ‘—
ACDC 47 81 8 13 38
ARC 41 82 5 10 36
Bunch-NAHC 38 81 1 9 29
Bunch-SAHC 42 82 2 15 32
FCA 47 80 5 10 37
LIMBO 25 80 0 0 22
SADE 47 84 14 22 42
WCA-UE 34 82 2 6 35
WCA-UENM 33 81 1 5 34
Best Base. 47 84 14 22 42
SARIF 61 87 20 39 53
SARIF v.s. Base. +30.0% +3.0% +44.2% +78.0% +25.2%
Table 6: Result on OODT with refined granularity.
Technique MoJoFM a2a ğ‘2ğ‘ğ‘ğ‘£ğ‘” ARIğ‘2ğ‘ğ‘ğ‘‘ ğ‘—
Pre. Best 50 86 17 24 45
SARIF 56 88 19 48 55
SARIF v.s. Base. +11.6% +2.4% +9.2% +96.7% +20.8%
of 90 unique experiments are carried out to get all scores of each
metric for each tool on each project. The results of the metrics are
shown in Table 4. Out of a total of 40 metrics on 8 projects, we
achieved top-1 scores for 77.8% (35/45) results, and top-2 scores for
88.9% (40/45) results among all 10 algorithms. We summarized the
averaged metric result in Table 5. The â€™Best Base. â€™ presents the best
scores of the baseline techniques for each metric. SARIF is 36.1%
more accurate than the previous best results in terms of the average
of the leading percentage of all five metrics.
The only project we failed to achieve top 2 is OODT. The reason
is that for OODT, the ground truth architecture has 216 clusters,
while SARIF only recovers the project into 28 clusters with the
defaultğ‘Ÿğ‘’ğ‘ ğ‘œğ‘™ğ‘¢ğ‘¡ğ‘–ğ‘œğ‘› parameter. Such a huge discrepancy leads to the
low metric scores of SARIF. As discussed in Sec 3.4, our default res-
olution normally gives a result of 10 to 30 clusters which we think
is suitable for humans to understand. However, there is no standard
for the granularity of architecture since different granularity will be
desired in different use cases. Our default granularity mismatches
the ground-truth architecture of OODT, which is much fine-grained.
Therefore, we conducted an extensive experiment that increases
theresolution parameter by 20 times to make the granularity of
SARIF comparable to the ground truth of OODT. With the updated
resolution, SARIF clusters OODT into 180 clusters and the corre-
sponding metrics are shown in Table 6. With the updated resolution ,
our result is much more accurate than the baseline techniques.
In contrast to our default coarse-grained granularity, other lead-
ing techniques like ACDC, FCA or SADE tend to decompose the
system into much more clusters. This is the most common reasonTable 7: Time consumptions (in seconds).
Technique AS4 Bash Chromium Hadoop ITK OODT HDC HDF Libxml2
ACDC 46.5 10.4 28,259.3 20.2 345.7 16.1 10.1 16.0 9.6
ARC 2,073.6 510.0 23,769.3 981.5 8,266.0 657.2 110.1 915.2 619.7
FCA 66.5 10.4 14,213.0 27.0 771.9 24.2 10.5 22.1 9.5
LIMBO 292.5 13.9 10,538.6 70.6 1976.0 33.6 11.5 27.7 10.1
SADE 37.4 10.6 3,813.0 19.9 272.0 16.4 10.7 15.9 10.2
WCA-UE 78.4 10.9 4,368.0 27.6 471.3 19.4 10.8 18.2 10.2
WCA-NM 82.9 11.0 4,386.3 28.8 473.0 19.3 10.8 18.3 10.1
Bunch-NAHC 50.3 10.0 160,969.6 21.1 5,765.8 16.1 9.9 15.4 9.4
Bunch-SAHC 167.4 10.3 668,733.4 33.9 9,675.0 20.2 9.9 18.2 9.4
SARIF 92.2 74.5 4,499.3 110.1 1,195.6 84.3 22.0 44.1 45.9
for the metrics that we fall behind. For example, on ArchStudio,
ACDC results in 240 clusters while SARIF generates only 25 clus-
ters. As mentioned in RQ1, MoJoFM favors small clusters in nature.
Consequently, ACDC beats us on ArchStudio for MoJoFM, while
we achieved a higher score for the other four metrics.
5.3.2 Time Consumptions. To evaluate the scalability of SARIF, the
time consumption for the architecture recoveries is measured and
shown in Table 7. The experiments are conducted on an Ubuntu
server with dual 20-core Intel CPUs and 188 GB RAM. The results
show that SARIF can recover the architectures within a reasonable
timeframe: small and medium size projects can be finished within a
few minutes, while projects as large as Chromium, which has more
than 10 million lines of codes, can be finished with about 80 minutes.
It can be found that the time consumption of SARIF is about linearly
proportional to the size of the project measured by the lines of code,
which ensured our scalability on large projects. SARIF may take
longer time than some alternative tools, since we utilized both
dependency and textual information while other tools typically
only use one dimension. The inclusion of these two dimensions
inevitably increased the time cost since extracting them could be
time-consuming. For example, among a total of 4499.3 seconds
when recovering Chromium with SARIF, 3943.3 seconds (87.6%)
were spent on the information extraction, while the other processes
only take less than 10 minutes.
5.3.3 Case Study. To examine the practicalness of SARIF, we per-
form a case study, which compares the architecture recovery results
of SARIF against the ground truth labeled by the developers of a
real-world project Distributed Camera [ 3]. Fig 6(a) shows its archi-
tecture diagram from its source repository. The diagram reveals that
it has two primary components: the camera source and the camera
sink. Both the source and sink are comprised of three components:
the manager, channel and data-processing. However, while labeling
the ground truth architecture based on the diagram, we find it hard
to split the implementation of source and sink. For example, a total
1543ESEC/FSE â€™23, December 3â€“9, 2023, San Francisco, CA, USA Yiran Zhang, Zhengzi Xu, Chengwei Liu, Hongxu Chen, Jianwen Sun, Dong Qiu, and Yang Liu
Distributed
CameraSDKDistributed 
Camera
Camera
Source Manager
Channel Source
Camera Data
PostProcessDistributedCameraSource
Camera
Source Manager
Channel Source
Camera Data
PostProcessDistributedCameraSource
Camera
Sink Manager
Channel Sink
Camera Data
PreProcessDistributedCameraSink
Camera
Client
Virtual Camera 
HDFCamera Data 
UploadHDFService
(a) Reference Architecture [3]
 (b) Recovered v.s. Ground Truth
Figure 6: Reference architecture of Distributed Camera , and
comparison between the recovered architecture and the
ground truth.
of 26 source files are related to data-processing, but 22 of them are
common implementations shared by both the source and sink sides.
Following the architecture diagram, we labeled the data-processing
files into 3 modules: source, sink and common, though there are
only 4 files in the source and the sink part. The other modules with
similar issues like channel-related codes are also marked in this
way. Last, we labeled and put the files that we cannot decide on
their module with the diagram into a general module.
We then recover its architecture with SARIF. The result is shown
in Fig 6(b) with distribution map [ 19]. Each colored small square
represents a file. The files in the same box mean that they belong
to the same human-labeled ground truth module, and the color-
coding represents the result of SARIF. As shown in the figure, SARIF
successfully identifies the data-processing related module (colored
light pink) and channel module (colored pink), i.e., SARIF clusters
file with similar functionalities like data-processing together rather
than splitting them by source and sink. Although this result does
not align with the reference architecture, we believe that the result
of SARIF is even more reasonable. As previously stated, 22 out of 26
files in the data-processing module are shared by both source and
sink. From an implementation perspective, it is unfair to split the
module just because the minority four files can be well separated.
5.3.4 Extensive Validation. To further validate the generalizability
of SARIF, an extensive experiment is carried out on 900 GitHub
projects as detailed in Sec 5.1.3. To quantitatively compare the result
of SARIF with baselines, a ground truth architecture is required.
However, labeling a reliable ground truth architecture from scratch
requires extremely high manual efforts and extensive communica-
tion with the corresponding developers, which is time-consuming.
For example, labeling Chromium took 2 years from the previous
researcher [ 45]. Given our aim is to generate more ground truth
data to test the generalizability of SARIF, this accurate but time-
consuming method is not feasible. Therefore, we adopted an alter-
native labeling method. For each repository, if it contains a refer-
ence architecture diagram, we labeled the architecture accordingly.
While this alternative method may not be as accurate as traditional
labeling, it should sufficiently represent the developersâ€™ architec-
tural perception.
Using this method, we labeled a total of 23 projects. We compared
SARIF with baselines based on these labeled architectures. MetricTable 8: Accuracy with incomplete information/weight.
Variant MoJoFM a2a ğ‘2ğ‘ğ‘ğ‘£ğ‘” ARIğ‘2ğ‘ğ‘ğ‘‘ ğ‘—
SARIFavg 61 87 20 39 53
v.s. SARIF / / / / /
v.s. best base. +30.0% +3.0% +44.2% +78.0% +25.2%
w/o Folderavg 51 85 15 27 43
v.s. SARIF -16.0% -1.9% -25.5% -30.6% -17.9%
v.s. best base. +9.2% +1.1% +7.4% +23.5% +2.9%
w/o Textavg 52 85 13 29 47
v.s. SARIF -15.4% -2.5% -35.3% -26.0% -10.8%
v.s. best base. +10.0% +0.4% -6.7% +31.7% +11.6%
Dep. Onlyavg 49 83 7 22 43
v.s. SARIF -19.4% -4.3% -67.2% -43.4% -19.2%
v.s. best base. +4.8% -1.4% -52.6% +0.7% +1.2%
w/o Entity Impt.avg 56 86 19 32 47
v.s. SARIF -8.6% -1.2% -8.6% -19.1% -11.4%
v.s. best base. +18.8% +1.8% +31.8% +44.0% +11.0%
w/o Dep. Typeavg 60 85 14 31 46
v.s. SARIF -2.5% -2.2% -31.8% -21.6% -12.6%
v.s. best base. +26.7% +0.8% -1.7% +39.5% +9.4%
w/o Dep. Weightavg 55 85 12 28 43
v.s. SARIF -10.4% -2.2% -41.0% -27.1% -17.5%
v.s. best base. +16.5% +0.8% -14.9% +29.8% +3.2%
results indicate SARIFâ€™s accuracy exceeds baselines by 35.2%. All
data related to this extensive experiment, including project list,
labeled architecture, metric results and recovered architecture for all
900 projects are available on our website [ 67] for public verification.
Answering RQ2: SARIF has the best architecture recovery re-
sults compared to 9 related works with a 36.1% higher in ac-
curacy on average. An extensive experiment verified SARIFâ€™s
generalizability. In the case study, it produces a more reasonable
architecture than the human-label one.
5.4 Ablation Study (RQ3)
In this RQ, we aim to evaluate the impact of two factors on the
accuracy of SARIF: 1) fusing various types of information, and 2)
using fine-grained dependency weighting. Regarding the informa-
tion fusion, we will investigate the impact of excluding textual
information or folder structure, while dependency cannot be totally
excluded since our weight assignment algorithm is based on it. As
for the weighting of dependencies, we will evaluate the impact of
removing the weights for dependency types and entity importance.
Table 8 summarizes the average decrease in accuracy across
9 projects after removing information or weighing, and the com-
parison between incomplete SARIF and the best of the baseline
techniques. As shown in the table, SARIFâ€™s accuracy always de-
creases significantly after removing each type of information or
weight. The removal of folder structure has the greatest negative
impact on most metrics. The scores of different metrics declined
by about 16% to 30% (except for a2a, whose variation range is too
small as discussed in RQ1). This aligns with our expectations since
some of the human-labeled ground truth architectures are strongly
related to their folder structures [ 22]. Removing textual informa-
tion also reduces the scores by about 10% âˆ¼30%, showing that the
textual similarities can strongly indicate the architectural modules.
When both textual information and folder structure are removed,
our metric scores become comparable to the best baseline scores.
This suggests that fusing multiple information sources is necessary
for SARIF to outperform the baselines.
As for dependency weighing, removing weight by entity impor-
tance or dependency type leads to a decrease in accuracy of about
1544Software Architecture Recovery with Information Fusion ESEC/FSE â€™23, December 3â€“9, 2023, San Francisco, CA, USA
3% to 20%. Removing both weighting results in a drop of about 11%
to 26% in accuracy, which is comparable to the decrease by remov-
ing textual information or folder structure. This result confirms the
importance of using fine-grained dependency weighting.
Answering RQ3: Both information fusion and fine-grained de-
pendency weighting are critical to SARIF. Removing either de-
pendency weighting or one type of information will result in an
accuracy drop of approximately 10% to 30% for different metrics.
Removing two types of information will result in a low accuracy
that comparable to the best baseline techniques.
6 THREATS TO VALIDITY
We discuss the threats to the validity of our results to comprehend
the strength and limitations of SARIF. The first potential threat to
validity relates to the representativeness and generalizability of our
evaluation on SARIF. The ground-truth architectures used in the
evaluation only span 9 projects, which might not seem sufficient
for robustly inferring generalizability. To alleviate this concern,
we have gathered all publicly available ground-truth architectures
from previous studies, and labeled several additional ground-truths
with our industrial partners. Moreover, an extensive evaluation is
carried out on 900 Github projects to enhance the generalizability.
The second threat pertains to the parameter selection within
SARIF. Assuring the optimal parameter setup for SARIF in the
context of architecture recovery, is difficult. We have strived to
make our design more reasonable by following the original algo-
rithm designerâ€™s suggestions. Nevertheless, it is still uncertain if
our parameter selections are the best.
The final threat related to the accuracy of dependency extraction.
Even though DEPENDS is one of the most accurate open-source
dependency extractors [ 40], the dependency extracted by DEPENDS
could be inaccurate due to the complex nature of programming
languages and the limitation of static analysis.
7 RELATED WORK
The architecture recovery techniques are to automatically recover
software architectures from their implementations. Many of these
techniques use structural dependencies as their information source.
Bunch [ 48] clusters source files of software systems by maximizing
the modularity quality (MQ) of the dependency graph. WCA [ 49]
is a hierarchical clustering technique that commonly-used in the
scenario of software clustering. LIMBO [ 6] is another commonly
used technique employing information theory concepts for soft-
ware clustering. DAGC [ 57] is a technique that similar to Bunch
but optimized its search space. MCA and ECA [ 58] introduced
multi-objective optimization into software clustering. Cooperative
clustering technique (CCT) [ 54] is a consensus-based clustering
technique that is utilized in the software clustering scenario. Mo-
hammadi et al. [ 53] used available knowledge in the dependency
graph to create a neighborhood tree to drive the clustering. In ad-
dition to these techniques that use static dependencies, Xiao et
al. [79] shows that dynamic dependencies have some merits.
Apart from dependency-based techniques, most studies use tex-
tual information for architecture recovery. ARC [ 24] recovers thearchitecture using concerns. ZBR [ 17] is a recovery technique based
on natural language semantics, and it partitions textual informa-
tion into different zones to form clusters. Risi et al. [ 64] uses LSI to
extracts textual information then cluster the system with K-Means.
Kargar et al. [ 34] constructed a semantic dependency graph to re-
place the dependency graph to take the advantage that the semantic
information is independent of programming languages. They fur-
ther employed nominal information (i.e. the name of artifacts) to
improve their previous work in [ 35]. EVOL [ 80] is a semantic-based
technique that considers semantic outliers filtration and label prop-
agation to increase their accuracy. The textual-information-based
techniques may outperform dependency-based ones in specific
scenarios as shown in study [21].
There are also some techniques that use hybrid information for
clustering. Mkaouer et al. [ 52] proposed a many-objective search-
based approach using NSGA-III. Chhabra et at. [ 14] extracted com-
bined features with 24 different coupling schemes to explore how to
combine the dependencies and textual to produce the optimal clus-
tering result. SADE [ 15] clusters software systems by combining
the dependency information with the semantic similarity. They use
the semantic similarity between modules as the weight of call graph,
and cluster the graph using Louvain clustering algorithm. Jalali et
al. [65] unified dependency information and semantic information
to introduce a new multi-objective fitness function, and consider
the clustering as a multi-objective search problem. However, cur-
rent research on clustering with hybrid information mainly focus
on only the dependency and textual information, while neglects
many other useful information in the source codes.
8 CONCLUSION
In summary, we propose SARIF, an architecture recovery tech-
nique dedicates to comprehensively understanding the architecture
of software systems by fusing three aspects of information from
software systems. We evaluate SARIF and 9 previous architecture
recovery techniques on 9 software systems with ground truth ar-
chitectures. Three of the most commonly-used metrics and two
new metrics are utilized for evaluation. The experimental result
shows that SARIF outperforms the best baseline techniques by
36.1% on average. A case study on a real-world software system
shows that SARIF can effectively identify modules with similar
functionalities. For future work, SARIF can be further enhanced
by incorporating more domain knowledge on architecture such as
commonly-adopted architectural patterns.
9 DATA AVAILABILITY
The data, demo and detailed instructions are available at https:
//github.com/anonymous2f4a9d/SARIF_FSE23.
ACKNOWLEDGEMENTA
This research / project is supported by the National Research Foun-
dation, Singapore, and the Cyber Security Agency under its National
Cybersecurity R&D Programme (NCRP25-P04-TAICeN). Any opin-
ions, findings and conclusions or recommendations expressed in
this material are those of the author(s) and do not reflect the views
of National Research Foundation, Singapore and Cyber Security
Agency of Singapore.
1545ESEC/FSE â€™23, December 3â€“9, 2023, San Francisco, CA, USA Yiran Zhang, Zhengzi Xu, Chengwei Liu, Hongxu Chen, Jianwen Sun, Dong Qiu, and Yang Liu
REFERENCES
[1] 2022. Comment Parser. https://pypi.org/project/comment-parser/.
[2] 2022. DEPENDS. https://github.com/multilang-depends/depends.
[3]2022. Distributed Camera. https://gitee.com/openharmony/distributed_camera.
[4]2022. Drivers Framework. https://gitee.com/openharmony/drivers_framework.
[5]Amarjeet and Jitender Kumar Chhabra. 2018. Many-objective artificial bee colony
algorithm for large-scale software module clustering problem. Soft Computing
22, 19 (Oct. 2018), 6341â€“6361. https://doi.org/10.1007/s00500-017-2687-3
[6]Periklis Andritsos, Panayiotis Tsaparas, RenÃ©e J Miller, and Kenneth C Sevcik.
2004. LIMBO: Scalable clustering of categorical data. In International Conference
on Extending Database Technology . Springer, 123â€“146.
[7]Len Bass, Paul Clements, and Rick Kazman. 2003. Software architecture in practice .
Addison-Wesley Professional.
[8]Robert Benkoczi, Daya Gaur, Shahadat Hossain, and Muhammad A. Khan. 2018.
A design structure matrix approach for measuring co-change-modularity of
software products. In Proceedings of the 15th International Conference on Mining
Software Repositories . ACM, Gothenburg Sweden, 331â€“335. https://doi.org/10.
1145/3196398.3196409
[9]Tingting Bi, Peng Liang, Antony Tang, and Chen Yang. 2018. A systematic
mapping study on text analysis techniques in software architecture. Journal of
Systems and Software 144 (Oct. 2018), 533â€“558. https://doi.org/10.1016/j.jss.2018.
07.055
[10] David M Blei, Andrew Y Ng, and Michael I Jordan. 2003. Latent dirichlet allocation.
Journal of machine Learning research 3, Jan (2003), 993â€“1022.
[11] Evelien Boerstra, John Ahn, and Julia Rubin. 2022. Stronger Together: On Com-
bining Relationships in Architectural Recovery Approaches. (2022), 12.
[12] Sergey Brin and Lawrence Page. 1998. The anatomy of a large-scale hypertextual
web search engine. Computer networks and ISDN systems 30, 1-7 (1998), 107â€“117.
[13] Irene Charon, Lucile Denoeud, Alain Guenoche, and Olivier Hudry. 2006. Maxi-
mum transfer distance between partitions. Journal of Classification 23, 1 (2006),
103â€“121.
[14] Jitender Kumar Chhabra et al .2017. Improving modular structure of software sys-
tem using structural and lexical dependency. Information and software Technology
82 (2017), 96â€“120.
[15] Choongki Cho, Ki-Seong Lee, Minsoo Lee, and Chan-Gun Lee. 2019. Software
Architecture Module-View Recovery Using Cluster Ensembles. IEEE Access 7
(2019), 72872â€“72884. https://doi.org/10.1109/ACCESS.2019.2920427
[16] Aaron Clauset, Mark EJ Newman, and Cristopher Moore. 2004. Finding commu-
nity structure in very large networks. Physical review E 70, 6 (2004), 066111.
[17] Anna Corazza, Sergio Di Martino, Valerio Maggio, and Giuseppe Scanniello. 2011.
Investigating the use of lexical information for software system clustering. In
2011 15th European Conference on Software Maintenance and Reengineering . IEEE,
35â€“44.
[18] Ctags. 2022. Universal Ctags. https://github.com/universal-ctags/ctags.
[19] StÃ©phane Ducasse, Tudor GÃ®rba, and Adrian Kuhn. 2006. Distribution map. In
2006 22nd IEEE international conference on software maintenance . IEEE, 203â€“212.
[20] Milad Elyasi, Muhammed Esad Simitcioglu, Abdullah Saydemir, Ali Ekici, and
Hasan Sozer. 2022. HYGAR: a hybrid genetic algorithm for software architecture
recovery. In Proceedings of the 37th ACM/SIGAPP Symposium on Applied Comput-
ing. ACM, Virtual Event, 1417â€“1424. https://doi.org/10.1145/3477314.3507020
[21] Joshua Garcia, Igor Ivkovic, and Nenad Medvidovic. 2013. A comparative analysis
of software architecture recovery techniques. In 2013 28th IEEE/ACM International
Conference on Automated Software Engineering (ASE) . IEEE, 486â€“496.
[22] Joshua Garcia, Ivo Krka, Chris Mattmann, and Nenad Medvidovic. 2013. Obtaining
ground-truth software architectures. In 2013 35th International Conference on
Software Engineering (ICSE) . IEEE, 901â€“910.
[23] Joshua Garcia, Mehdi Mirakhorli, Lu Xiao, Yutong Zhao, Ibrahim Mujhid, Khoi
Pham, Ahmet Okutan, Sam Malek, Rick Kazman, Yuanfang Cai, and Nenad
Medvidovic. 2021. Constructing a Shared Infrastructure for Software Architecture
Analysis and Maintenance. In 2021 IEEE 18th International Conference on Software
Architecture (ICSA) . IEEE, Stuttgart, Germany, 150â€“161. https://doi.org/10.1109/
ICSA51549.2021.00022
[24] Joshua Garcia, Daniel Popescu, Chris Mattmann, Nenad Medvidovic, and Yuan-
fang Cai. 2011. Enhancing architectural recovery using concerns. In 2011 26th
IEEE/ACM International Conference on Automated Software Engineering (ASE 2011) .
IEEE, 552â€“555.
[25] GNOME. 2022. Libxml2. https://gitlab.gnome.org/GNOME/libxml2.
[26] Zoltan Gyongyi, Hector Garcia-Molina, and Jan Pedersen. 2004. Combating web
spam with trustrank. In Proceedings of the 30th international conference on very
large data bases (VLDB) .
[27] Adrian Hoff, Michael Nieke, and Christoph Seidl. 2021. Towards immersive
software archaeology: regaining legacy systemsâ€™ design knowledge via interactive
exploration in virtual reality. In Proceedings of the 29th ACM Joint Meeting on
European Software Engineering Conference and Symposium on the Foundations of
Software Engineering . ACM, Athens Greece, 1455â€“1458. https://doi.org/10.1145/
3468264.3473128[28] Jinhuang Huang, Jing Liu, and Xin Yao. 2017. A multi-agent evolutionary al-
gorithm for software module clustering problems. Soft Computing 21, 12 (June
2017), 3415â€“3428. https://doi.org/10.1007/s00500-015-2018-5
[29] Lawrence Hubert and Phipps Arabie. 1985. Comparing partitions. Journal of
classification 2, 1 (1985), 193â€“218.
[30] Jimin Hwa, Shin Yoo, Yeong-Seok Seo, and Doo-Hwan Bae. 2017. Search-Based
Approaches for Software Module Clustering Based on Multiple Relationship
Factors. International Journal of Software Engineering and Knowledge Engineering
27, 07 (Sept. 2017), 1033â€“1062. https://doi.org/10.1142/S0218194017500395
[31] hyperopt. [n.d.]. https://github.com/hyperopt/hyperopt.
[32] Carlo Ieva, Arnaud Gotlieb, Souhila Kaci, and Nadjib Lazaar. 2018. Discovering
Program Topoi via Hierarchical Agglomerative Clustering. IEEE Transactions on
Reliability 67, 3 (Sept. 2018), 758â€“770. https://doi.org/10.1109/TR.2018.2828135
[33] Habib Izadkhah and Mahjoubeh Tajgardan. 2019. Information Theoretic Objective
Function for Genetic Software Clustering. In The 5th International Electronic
Conference on Entropy and Its Applications . MDPI, 18. https://doi.org/10.3390/ecea-
5-06681
[34] Masoud Kargar, Ayaz Isazadeh, and Habib Izadkhah. 2017. Semantic-based
Software clustering using hill climbing. (2017), 6.
[35] Masoud Kargar, Ayaz Isazadeh, and Habib Izadkhah. 2019. Multi-programming
language software systems modularization. Computers & Electrical Engineering
80 (Dec. 2019), 106500. https://doi.org/10.1016/j.compeleceng.2019.106500
[36] Kenichi Kobayashi, Manabu Kamimura, Koki Kato, Keisuke Yano, and Akihiko
Matsuo. 2012. Feature-gathering dependency-based software clustering using
dedication and modularity. In 2012 28th IEEE International Conference on Software
Maintenance (ICSM) . IEEE, 462â€“471.
[37] Duc Minh Le, Pooyan Behnamghader, Joshua Garcia, Daniel Link, Arman Shah-
bazian, and Nenad Medvidovic. 2015. An empirical study of architectural change
in open-source software systems. In 2015 IEEE/ACM 12th Working Conference on
Mining Software Repositories . IEEE, 235â€“245.
[38] Junha Lee, Dae-Kyoo Kim, Jiwoo Park, and Sooyong Park. 2017. Class Modu-
larization Using Indirect Relationships. In 2017 22nd International Conference on
Engineering of Complex Computer Systems (ICECCS) . IEEE, Fukuoka, 110â€“119.
https://doi.org/10.1109/ICECCS.2017.23
[39] Ki-Seong Lee and Chan-Gun Lee. 2020. Identifying Semantic Outliers of Source
Code Artifacts and Their Application to Software Architecture Recovery. IEEE
Access 8 (2020), 212467â€“212477. https://doi.org/10.1109/ACCESS.2020.3040024
[40] Jason Lefever, Yuanfang Cai, Humberto Cervantes, Rick Kazman, and Hongzhou
Fang. 2021. On the Lack of Consensus among Technical Debt Detection Tools. In
Proceedings of the 43rd International Conference on Software Engineering: Software
Engineering in Practice (Virtual Event, Spain) (ICSE-SEIP â€™21) . IEEE Press, 121â€“130.
https://doi.org/10.1109/ICSE-SEIP52600.2021.00021
[41] Xiaocong Li, Li Zhang, and Ning Ge. 2017. Framework Information Based Java
Software Architecture Recovery. In 2017 24th Asia-Pacific Software Engineering
Conference Workshops (APSECW) . IEEE, Nanjing, 114â€“120. https://doi.org/10.
1109/APSECW.2017.15
[42] Daniel Link, Pooyan Behnam, Ramin Moazeni, and Barry Boehm. 2019. The
Value of Software Architecture Recovery for Maintenance. arXiv:1901.07700
[cs].
[43] Daniel Link, Pooyan Behnamghader, Ramin Moazeni, and Barry Boehm. 2019.
Recover and RELAX: Concern-Oriented Software Architecture Recovery for
Systems Development and Maintenance. arXiv:1903.06895 [cs].
[44] Daniel Link, Kamonphop Srisopha, and Barry Boehm. 2021. Study of the Utility
of Text Classification Based Software Architecture Recovery Method RELAX for
Maintenance. In Proceedings of the 15th ACM / IEEE International Symposium on
Empirical Software Engineering and Measurement (ESEM) . ACM, Bari Italy, 1â€“6.
https://doi.org/10.1145/3475716.3484194
[45] Thibaud Lutellier, Devin Chollak, Joshua Garcia, Lin Tan, Derek Rayside, Ne-
nad Medvidovic, and Robert Kroeger. 2015. Comparing software architecture
recovery techniques using accurate dependencies. In 2015 IEEE/ACM 37th IEEE
International Conference on Software Engineering , Vol. 2. IEEE, 69â€“78.
[46] Thibaud Lutellier, Devin Chollak, Joshua Garcia, Lin Tan, Derek Rayside, Nenad
MedvidoviÄ‡, and Robert Kroeger. 2017. Measuring the impact of code dependen-
cies on software architecture recovery techniques. IEEE Transactions on Software
Engineering 44, 2 (2017), 159â€“181.
[47] Thibaud Lutellier, Devin Chollak, Joshua Garcia, Lin Tan, Derek Rayside, Nenad
Medvidovic, and Robert Kroeger. 2018. Measuring the Impact of Code Dependen-
cies on Software Architecture Recovery Techniques. IEEE Transactions on Software
Engineering 44, 2 (Feb. 2018), 159â€“181. https://doi.org/10.1109/TSE.2017.2671865
[48] Spiros Mancoridis, Brian S Mitchell, Yihfarn Chen, and Emden R Gansner. 1999.
Bunch: A clustering tool for the recovery and maintenance of software system
structures. In Proceedings IEEE International Conference on Software Maintenance-
1999 (ICSMâ€™99). â€™Software Maintenance for Business Changeâ€™(Cat. No. 99CB36360) .
IEEE, 50â€“59.
[49] Onaiza Maqbool and Haroon Atique Babri. 2004. The weighted combined algo-
rithm: A linkage algorithm for software clustering. In Eighth European Conference
on Software Maintenance and Reengineering, 2004. CSMR 2004. Proceedings. IEEE,
15â€“24.
1546Software Architecture Recovery with Information Fusion ESEC/FSE â€™23, December 3â€“9, 2023, San Francisco, CA, USA
[50] Zsuzsanna Marian, Istvan-Gergely Czibula, and Gabriela Czibula. 2017. A Hier-
archical Clustering-Based Approach for Software Restructuring at the Package
Level. In 2017 19th International Symposium on Symbolic and Numeric Algo-
rithms for Scientific Computing (SYNASC) . IEEE, Timisoara, 239â€“246. https:
//doi.org/10.1109/SYNASC.2017.00046
[51] Nenad Medvidovic and Richard N Taylor. 2010. Software architecture: founda-
tions, theory, and practice. In 2010 ACM/IEEE 32nd International Conference on
Software Engineering , Vol. 2. IEEE, 471â€“472.
[52] Wiem Mkaouer, Marouane Kessentini, Adnan Shaout, Patrice Koligheu, Slim
Bechikh, Kalyanmoy Deb, and Ali Ouni. 2015. Many-objective software re-
modularization using NSGA-III. ACM Transactions on Software Engineering and
Methodology (TOSEM) 24, 3 (2015), 1â€“45.
[53] Sina Mohammadi and Habib Izadkhah. 2019. A new algorithm for software
clustering considering the knowledge of dependency between artifacts in the
source code. Information and Software Technology 105 (Jan. 2019), 252â€“256.
https://doi.org/10.1016/j.infsof.2018.09.001
[54] Rashid Naseem, Onaiza Maqbool, and Siraj Muhammad. 2013. Cooperative
clustering for software modularization. Journal of Systems and Software 86, 8
(2013), 2045â€“2062.
[55] Tobias Olsson. 2022. Incremental Clustering of Source Code: a Machine Learning
Approach . Ph.D. Dissertation. ISBN: 9789189460638 OCLC: 1296102631.
[56] Marios Papachristou. 2019. Software clusterings with vector semantics and the
call graph. In Proceedings of the 2019 27th ACM Joint Meeting on European Software
Engineering Conference and Symposium on the Foundations of Software Engineering .
ACM, Tallinn Estonia, 1184â€“1186. https://doi.org/10.1145/3338906.3342483
[57] Saeed Parsa and Omid Bushehrian. 2005. A new encoding scheme and a frame-
work to investigate genetic clustering algorithms. Journal of Research and Practice
in Information Technology 37, 1 (2005), 127â€“143.
[58] Kata Praditwong, Mark Harman, and Xin Yao. 2010. Software module clustering
as a multi-objective search problem. IEEE Transactions on Software Engineering
37, 2 (2010), 264â€“282.
[59] Amarjeet Prajapati. 2021. Software Module Clustering Using Grid-Based Many-
Objective Particle Swarm Optimization . preprint. In Review. https://doi.org/10.
21203/rs.3.rs-407806/v1
[60] Christos Psarras, Themistoklis Diamantopoulos, and Andreas Symeonidis. 2019. A
Mechanism for Automatically Summarizing Software Functionality from Source
Code. In 2019 IEEE 19th International Conference on Software Quality, Reliability
and Security (QRS) . IEEE, Sofia, Bulgaria, 121â€“130. https://doi.org/10.1109/QRS.
2019.00028
[61] Amit Rathee and Jitender Kumar Chhabra. 2017. Software Remodularization
by Estimating Structural and Conceptual Relations Among Classes and Using
Hierarchical Clustering. In Advanced Informatics for Computing Research , Dharm
Singh, Balasubramanian Raman, Ashish Kumar Luhach, and Pawan Lingras (Eds.).
Vol. 712. Springer Singapore, Singapore, 94â€“106. https://doi.org/10.1007/978-
981-10-5780-9_9 Series Title: Communications in Computer and Information
Science.
[62] Amit Rathee and Jitender Kumar Chhabra. 2018. Improving Cohesion of a Soft-
ware System by Performing Usage Pattern Based Clustering. Procedia Computer
Science 125 (2018), 740â€“746. https://doi.org/10.1016/j.procs.2017.12.095
[63] JÃ¶rg Reichardt and Stefan Bornholdt. 2006. Statistical mechanics of community
detection. Physical review E 74, 1 (2006), 016110.
[64] Michele Risi, Giuseppe Scanniello, and Genoveffa Tortora. 2012. Using fold-in
and fold-out in the architecture recovery of software systems. Formal Aspects of
Computing 24, 3 (2012), 307â€“330.
[65] Nafiseh Sadat Jalali, Habib Izadkhah, and Shahriar Lotfi. 2019. Multi-objective
search-based software modularization: structural and non-structural features.
Soft Computing 23, 21 (2019), 11141â€“11165.
[66] Gerard Salton and Christopher Buckley. 1988. Term-weighting approaches in
automatic text retrieval. Information processing & management 24, 5 (1988),513â€“523.
[67] SARIF. 2023. https://github.com/anonymous2f4a9d/SARIF_FSE23.
[68] Marcelo Schmitt Laser, Nenad Medvidovic, Duc Minh Le, and Joshua Garcia. 2020.
ARCADE: an extensible workbench for architecture recovery, change, and decay
evaluation. In Proceedings of the 28th ACM Joint Meeting on European Software
Engineering Conference and Symposium on the Foundations of Software Engineering .
ACM, Virtual Event USA, 1546â€“1550. https://doi.org/10.1145/3368089.3417941
[69] Marcelo Schmitt Laser, Nenad Medvidovic, Duc Minh Le, and Joshua Garcia.
2020. ARCADE: an extensible workbench for architecture recovery, change,
and decay evaluation. In Proceedings of the 28th ACM Joint Meeting on European
Software Engineering Conference and Symposium on the Foundations of Software
Engineering . 1546â€“1550.
[70] Anas Shatnawi, Abdelhak-Djamel Seriai, and Houari Sahraoui. 2017. Recovering
software product line architecture of a family of object-oriented product variants.
Journal of Systems and Software 131 (Sept. 2017), 325â€“346. https://doi.org/10.
1016/j.jss.2016.07.039
[71] Ioanna Stavropoulou, Marios Grigoriou, and Kostas Kontogiannis. 2017. Case
study on which relations to use for clustering-based software architecture recov-
ery. Empirical Software Engineering 22 (2017), 1717â€“1762.
[72] Jiaze Sun. 2018. PSO with Reverse Edge for Multi-Objective Software Module
Clustering. International Journal of Performability Engineering (2018). https:
//doi.org/10.23940/ijpe.18.10.p18.24232431
[73] Alvin Jian Jia Tan, Chun Yong Chong, and Aldeida Aleti. 2021. E-SC4R: Explaining
Software Clustering for Remodularisation. arXiv:2107.01766 [cs].
[74] Ana Paula M. Tarchetti, Luis Amaral, Marcos C. Oliveira, Rodrigo Bonifacio,
Gustavo Pinto, and David Lo. 2020. DCT: An Scalable Multi-Objective Module
Clustering Tool. In 2020 IEEE 20th International Working Conference on Source
Code Analysis and Manipulation (SCAM) . IEEE, Adelaide, Australia, 171â€“176.
https://doi.org/10.1109/SCAM51674.2020.00024
[75] Navid Teymourian, Habib Izadkhah, and Ayaz Isazadeh. 2020. A fast clustering
algorithm for modularization of large-scale software systems. IEEE Transactions
on Software Engineering (2020).
[76] Navid Teymourian, Habib Izadkhah, and Ayaz Isazadeh. 2022. A Fast Clustering
Algorithm for Modularization of Large-Scale Software Systems. IEEE Transactions
on Software Engineering 48, 4 (April 2022), 1451â€“1462. https://doi.org/10.1109/
TSE.2020.3022212
[77] Vassilios Tzerpos and Richard C Holt. 2000. Acdc: an algorithm for
comprehension-driven clustering. In Proceedings Seventh Working Conference on
Reverse Engineering . IEEE, 258â€“267.
[78] Zhihua Wen and Vassilios Tzerpos. 2004. An effectiveness measure for soft-
ware clustering algorithms. In Proceedings. 12th IEEE International Workshop on
Program Comprehension, 2004. IEEE, 194â€“203.
[79] Chenchen Xiao and Vassilios Tzerpos. 2005. Software clustering based on dy-
namic dependencies. In Ninth European Conference on Software Maintenance and
Reengineering . IEEE, 124â€“133.
[80] Kaiyuan Yang, Junfeng Wang, Zhiyang Fang, Peng Wu, and Zihua Song. 2022.
Enhancing software modularization via semantic outliers filtration and label
propagation. Information and Software Technology 145 (May 2022), 106818. https:
//doi.org/10.1016/j.infsof.2021.106818
[81] Keisuke Yano and Akihiko Matsuo. 2020. Moderate detection and removal of
omnipresent modules in software clustering. In 2020 IEEE International Conference
on Software Maintenance and Evolution (ICSME) . IEEE, 662â€“666.
[82] Maryam Zahid, Zahid Mehmmod, and Irum Inayat. 2017. Evolution in software
architecture recovery techniques â€” A survey. In 2017 13th International Conference
on Emerging Technologies (ICET) . IEEE, Islamabad, 1â€“6. https://doi.org/10.1109/
ICET.2017.8281704
Received 2023-02-02; accepted 2023-07-27
1547