ReMoS: Reducing Defect Inheritance in Transfer Learning via
Relevant Mo del S licing
Ziqi Zhang *
Key Laboratory of High-Conï¬dence
Software Technologies (MOE),
School of Computer Science,
Peking University
Beijing, China
ziqi_zhang@pku.edu.cnY uanchun Li *â€ 
Institute for AI Industry Research
(AIR), Tsinghua University
Beijing, China
liyuanchun@air.tsinghua.edu.cnJindong Wang
Microsoft Research
Beijing, China
jindong.wang@microsoft.com
Bingyan Liu
Key Laboratory of High-Conï¬dence
Software Technologies (MOE),
School of Computer Science,
Peking University
Beijing, China
lby_cs@pku.edu.cnDing Li
Key Laboratory of High-Conï¬dence
Software Technologies (MOE),
School of Computer Science,
Peking University
Beijing, China
ding_li@pku.edu.cnY ao Guoâ€ 
Key Laboratory of High-Conï¬dence
Software Technologies (MOE),
School of Computer Science,
Peking University
Beijing, China
yaoguo@pku.edu.cn
Xiangqun Chen
Key Laboratory of High-Conï¬dence
Software Technologies (MOE),
School of Computer Science,
Peking University
Beijing, China
cherry@sei.pku.edu.cnY unxin Liu
Institute for AI Industry Research
(AIR), Tsinghua University
Beijing, China
liuyunxin@air.tsinghua.edu.cn
ABSTRACT
Transfer learning is a popular software reuse technique in the deep
learning community that enables developers to build custom mod-
els (students) based on sophisticated pretrained models (teachers).
However, like vulnerability inheritance in traditional software reuse,
some defects in the teacher model may also be inherited by students,
such as well-known adversarial vulnerabilities and backdoors. Re-
ducing such defects is challenging since the student is unaware of
how the teacher is trained and/or attacked. In this paper, we propose
ReMoS, a relevant model slicing technique to reduce defect inheri-
tance during transfer learning while retaining useful knowledge from
the teacher model. Speciï¬cally, ReMoS computes a model slice (a
subset of model weights) that is relevant to the student task based on
the neuron coverage information obtained by proï¬ling the teacher
*Work is done while Ziqi Zhang was an intern and Y uanchun Li was a researcher at
Microsoft. â€ Correspondence to: Y uanchun Li, Y ao Guo.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proï¬t or commercial advantage and that copies bear this notice and the full citation
on the ï¬rst page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior speciï¬c permission and/or a
fee. Request permissions from permissions@acm.org.
ICSE â€™22, May 21â€“29, 2022, Pittsburgh, P A, USA
Â© 2022 Association for Computing Machinery.
ACM ISBN 978-1-4503-9221-1/22/05. . . $15.00
https://doi.org/10.1145/3510003.3510191model on the student task. Only the relevant slice is used to ï¬ne-tune the student model, while the irrelevant weights are retrained
from scratch to minimize the risk of inheriting defects. Our experi-
ments on seven DNN defects, four DNN models, and eight datasets
demonstrate that ReMoS can reduce inherited defects effectively (by
63% to 86% for CV tasks and by 40% to 61% for NLP tasks) and
efï¬ciently with minimal sacriï¬ce of accuracy (3% on average).
CCS CONCEPTS
â€¢Computing methodologies â†’Neural networks ;â€¢Software and
its engineering â†’ Dynamic analysis; â€¢Security and privacy â†’
Software security engineering.
KEYWORDS
Program slicing, deep neural networks, relevant slicing
ACM Reference Format:
Ziqi Zhang, Y uanchun Li, Jindong Wang, Bingyan Liu, Ding Li, Y ao Guo,
Xiangqun Chen, and Y unxin Liu. 2022. ReMoS: Reducing Defect Inheritance
in Transfer Learning via Relevant Mo del Slicing . In 44th International Con-
ference on Software Engineering (ICSE â€™22), May 21â€“29, 2022, Pittsburgh,
P A, USA. ACM, New Y ork, NY , USA, 13 pages. https://doi.org/10.1145/
3510003.3510191
18562022 IEEE/ACM 44th International Conference on Software Engineering (ICSE)
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:52:09 UTC from IEEE Xplore.  Restrictions apply. ICSE â€™22, May 21â€“29, 2022, Pittsburgh, P A, USA Ziqi Zhang, Y uanchun Li, Jindong Wang, Bingyan Liu, Ding Li, Y ao Guo, Xiangqun Chen, and Y unxin Liu

!
 

	

 
	

#

!"!
Figure 1: Defect inheritance is a common problem in both tra-
ditional software reuse and DNN model reuse.
1 INTRODUCTION
Software reuse is popular in traditional programs [ 16,19], with
examples ranging from copy-pasting a piece of code, inheriting aparent class, to importing a third-party library. Despite the great
convenience of software reuse, the vulnerability of the reused soft-
ware is one of the most concerning issues [ 18,52]. Developers are
often unaware of or unable to eliminate the vulnerabilities in thereused code, which may lead to unexpected consequences. For in-
stance, the Heartbleed bug in the OpenSSL library has led to severe
security issues in many applications built upon it [ 14]. The open-
source nature of many reusable packages increases the attack surface
since the attackers can inspect the reused code thoroughly to ï¬nd
ï¬‚aws to exploit. The threats and countermeasures of reusing vulner-
able modules have been studied by security researchers for a long
time [6, 8, 10].
The deep neural network (DNN for short) is now considered a
special type of software with great breakthroughs in recent years.
To reuse existing models and expedite model development, trans-
fer learning [40,51] has been proposed to reuse the knowledge in
one model to build new models. A typical transfer learning process
includes acquiring a public pretrained model (e.g. ResNet [ 24]o r
BERT [ 13]), adjusting the model architecture, and ï¬ne-tuning the
model using target-domain data [ 42]. Borrowing names from the
prior literature, we call the pretrained models as teachers and the new
models built upon them as students [64]. Thanks to the wide avail-
ability of sophisticated public models [ 3,4,68], transfer learning
is nowadays the default choice for most developers when building
their own deep learning applications.
However, similar to traditional software reuse, transfer learning
also leads to concerns about defect inheritance. Figure 1 illustrates
the defect inheritance problem in traditional software reuse and
model reuse. In software reuse, the developers download the public
library and use it in their applications. Similarly, in model reuse, the
developers download the public pretrained model and use transferlearning to obtain their customized models. In both cases, defectsmay be inherited during the software reuse process. An attackercan download the public library (model), analyze it to ï¬nd the de-
fects, and attack the deployed application (model) with the carefully
crafted inputs [6, 8].
The SE community takes the security risk of DNNs as an impor-
tant problem [ 37,38,60,71] as DNN models are extensively usedTable 1: Potential defects in the literature that may inherit dur-
ing transfer learning.
Task Defect Type Inheritance Rate
CVAdv ersarial Penultimate-Layer Guided [58] 58.01%
Vulnerability Neuron-Coverage Guided [21, 55] 52.58%
Backdoor Latent Data Poisoning [70] 72.91%
NLPAdv ersarial Greedy Word Swap [31] 64.86%
Vulerability Word Importance Ranking [29] 94.73%
BackdoorData Poisoning [20] 96.72%
Weight Poisoning [32] 97.85%
in various industrial and research scenarios. On the one hand, DNNs
are considered as an emerging type of software artifact (widely
known asâ€œsoftware 2.0â€). The widely-adapted model reuse is a type
of software reuse for DL developers. On the other hand, nowadays
DNNs are frequently embedded into software applications, such as
autonomous driving and facial recognition. The safety of DL models
is crucial for the reliability of relevant software.
We surveyed existing literature and summarized seven typical
defects that may be inherited during transfer learning. As shown in
Table 1, these defects can be categorized into two groups: adver-
sarial vulnerability and backdoor. Adversarial vulnerability is an
exploratory defect [25,36], where an attacker can obtain adversar-
ial examples from the public teacher model and fool the students[
21,55,58,64]. Backdoor is a causative defect, where hidden ma-
licious logic is injected into the teacher model purposely, and the
student models built upon it may also misbehave under certain back-
door triggers [ 20,20,28,32,45,70]. According to our measurement
study, the defects can easily be propagated from the teacher to the
students, with the inheritance rate ranging from 52.58% to 97.85%.
A major reason for DNN defect inheritance is the indiscriminate
training process [ 58]. Speciï¬cally, in conventional transfer learning,
the student model is initialized thoroughly with the teacherâ€™s weights,
including the weights for both student-related and defect-relateddecision boundaries. Thus the defect-related decision boundaries
are largely kept during ï¬ne-tuning since the student dataset is small-
scale and irrelevant to the defect-related decision boundaries [ 41].
The focus of this paper is how to reduce the inherited defects while
preserving student-related knowledge.
There are mainly two types of solutions for developers to reduce
inherited defects and enhance deep learning software reliability and
safety. One is to adopt defect mitigation techniques (such as ï¬ne-
pruning [ 44], etc.) after ï¬ne-tuning, which we call ï¬x-after-transfer
approaches. Another is the ï¬x-before-transfer approach [ 11], which
ï¬rst randomly initializes the student model, then extracts the student-
related knowledge from the teacher. The main problem of ï¬x-after-
transfer approaches is the poor effectiveness, since it is difï¬cult
for the student model to remove the inherited defects with limitedtraining data. The ï¬x-before-transfer approach [
11] is effective in
reducing inherited defects, but it sacriï¬ces the performance on the
student task and applicability because the student model has to be
trained from scratch with complicated conï¬gurations.
In this paper, we try to address the defect inheritance problem
from a different angle. We observe that a teacher model is trained to
serve multiple downstream tasks. However, a student model is often
1857
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:52:09 UTC from IEEE Xplore.  Restrictions apply. ReMoS: Reducing Defect Inheritance in T ransfer Learning via Re levant Mo del S licing ICSE â€™22, May 21â€“29, 2022, Pittsburgh, P A, USA
trained for a speciï¬c task. Thus, the teacher model may contain a
substantial amount of weights that are not useful for the speciï¬c
task that the student model tries to address. Having these irrelevant
weights enlarges the attack surfaces for backdoors and increases
the probability of exposing defects. Based on this observation, we
propose to selectively reuse the teacher knowledge that is relevant
to the student task. The idea is motivated by relevant slicing [ 5,22],
which computes a set of program statements that may affect the
slicing criterion. Our intuition is similar to the conventional software
engineering practice: only reusing the relevant code of a program
can decrease the probability of having bugs [6, 8, 10].
However, due to the lack of the interpretability of DNN models,
computing the relevant slice is not easy. For traditional software,the functionalities are speciï¬ed by the developers through linesof code, and the relevance of each line of code can be explicitly
computed [ 22]. On the contrary, the development process of DNN
models is quite different. The decision logic of a model is hidden
in millions of weights learned from data. The semantic meaning of
each weight is not understandable, making it difï¬cult to compute the
relevant part.
To solve the aforementioned problems, we introduce a proï¬ling-
based approach named ReMoS ( Relevant Mo delSlicing) that is
guided by neuron coverage to compute the relevant slice of the pre-trained model on the student task. The relevant slice is further used
in ï¬ne-tuning to reduce the inherited defects. ReMoS consists of
four steps: coverage frequency proï¬ling, ordinal score computation,
relevant slice generation, and ï¬ne-tuning. The ï¬rst stage records the
neuron coverage frequency by iterating all data samples and com-
putes the weight coverage frequency as the ï¬ne-grained relevance tothe student task. Ordinal score computation combines the weight sig-
niï¬cance inside the teacher model (measured by the magnitude) and
the relevance to the student task (measured by the weight coverage
frequency). The last step ï¬nds the model parts that are relevant to
the student task and resets the excluded weights. At last, the relevant
part of the teacher model is used to initialize the student model and
it is ï¬ne-tuned with the student dataset.
We evaluate ReMoS with both CV and NLP tasks on four models,
eight datasets, and on all seven inherited defects in Table 1. ForCV models, the inherited defects are reduced by 63% to 78% for
adversarial vulnerability and 86% for backdoors, with less than 2%
accuracy sacriï¬ce. For NLP models, the inherited defects are reduced
by 40% for adversarial vulnerability and 50% to 61% for backdoors,
with less than 3% accuracy drop. We also analyze the distribution
of the excluded weights of the relevant model slice and ï¬nd that it
mainly slices the high-level weights that are irrelevant to the student
task.
This paper makes the following research contributions:
â€¢We study the defect inheritance problem in the transfer learn-
ing scenario, summarize common defects in the existing liter-
ature, and quantify the inheritance rate of each kind of defect.
â€¢We propose ReMoS to reduce defects inherited from teacher
models during transfer learning. Our method is inspired byrelevant slicing in traditional software, which computes a
subset of parameters in the teacher model that are relevant to
student tasks while excluding the irrelevant ones to reduce
teacher-originated defects.â€¢We evaluate ReMoS against seven different types of DNN
defects in comparison with several state-of-the-art approaches
on various popular DNN architectures and datasets. The re-sults have demonstrated the effectiveness and efï¬ciency of
our approach.
2 PROBLEM FORMULATION
In this section, we formulate the defect inheritance problem, describe
the threat model, and deï¬ne the defenderâ€™s goal.
2.1 DNN Defects
The DNN defects considered in this paper are deï¬ned as follows:
DEFINITION 1. DNN Defects are the deviation of the actual and
expected results of a DNN model produced by certain input samples.
Speciï¬cally, we focus on two typical DNN defects that are widely
discussed in the literature, including adversarial vulnerability and
backdoor. They can be triggered by adversarial inputs and backdoor
inputs respectively.
Adversarial vulnerability means that a DNN is vulnerable to
adversarial inputs that are generated by adding special noise to nor-
mal inputs and lead to prediction errors [ 26]. The most common way
to generate adversarial inputs is to use gradient ascent techniques on
the white-box model [ 48]. In the software engineering community,
this vulnerability is viewed as a robustness issue, and many tech-
niques have been proposed to test [ 21,55,65], improve [ 15,17], or
verify [7, 43, 54, 60] the DNN robustness.
Abackdoor (or a trojan horse) is a hidden pattern purposely
injected into the model that produces unexpected output if a speciï¬c
trigger is presented in the input. Backdoor inputs can be easilysynthesized by attaching a backdoor trigger to normal inputs. Themost straightforward way to implant backdoor is data poisoning
[20,45],i.e. adding carefully designed samples into the training data
to let the model memorize misleading patterns. Recently, researchers
have extended the attacks to make the backdoor more effective [ 28,
32, 37], evasive [39], and transferable [70].
The two aforementioned defects share a similar goal: to disturb
the model output under a limited perturbation budget. This budget is
usually computed by a distance function ğ‘‘(Â·,Â·)which measures the
difference between a benign input and a malicious input. The goal
of the adversary is to generate a malicious sample Ëœğ‘¥:
Ëœğ‘¥=argmax
ğ‘¥/primeğ½(ğ‘“(ğ‘¥/prime;wğ‘‡),ğ‘¦)s.t.ğ‘‘(ğ‘¥/prime,ğ‘¥)<ğœ– (1)
where ğ½is the loss function and ğœ–is a pre-deï¬ned small value. For
the adversarial vulnerability, the distance function is usually the
ğ‘-norm distance between the two inputs: ğ‘‘(Ëœğ‘¥ğ‘ğ‘‘ğ‘£,ğ‘¥)=/bardblËœğ‘¥ğ‘ğ‘‘ğ‘£âˆ’ğ‘¥/bardblğ‘.
For backdoors, the adversary attempts to generate malicious inputs
by attaching a trigger sign onto normal inputs. In this case, the
distance is the ratio of different input dimensions between Ëœğ‘¥ğ‘¡ğ‘Ÿğ‘–and
ğ‘¥:ğ‘‘(Ëœğ‘¥ğ‘¡ğ‘Ÿğ‘–,ğ‘¥)=I[Ëœğ‘¥ğ‘¡ğ‘Ÿğ‘–â‰ ğ‘¥]
|ğ‘¥|.
In the community of SE and ML, these two defects and the cor-
responding attacks usually refer to the same thing [ 15,17,72], so
we will use the terms â€œdefectsâ€ (or â€œvulnerabilitiesâ€) and â€œattacksâ€
according to the different context.
1858
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:52:09 UTC from IEEE Xplore.  Restrictions apply. ICSE â€™22, May 21â€“29, 2022, Pittsburgh, P A, USA Ziqi Zhang, Y uanchun Li, Jindong Wang, Bingyan Liu, Ding Li, Y ao Guo, Xiangqun Chen, and Y unxin Liu
2.2 DNN Defect Inheritance
Unlike most existing work that discusses DNN defects of an indi-
vidual model, we focus on the defects propagating between models
during transfer learning.
Transfer learning scenario. Suppose a developer wants to train
a model ğ‘€ğ‘†with a dataset ğ·ğ‘†. To improve accuracy and reduce
training time, she decides to use transfer learning to build her model
upon a public pretrained model ğ‘€ğ‘‡.ğ‘€ğ‘‡andğ‘€ğ‘†are called the
teacher model and the student model respectively. The dataset ğ·ğ‘‡
that was used to train ğ‘€ğ‘‡is called the teacher dataset, which is
signiï¬cantly larger than the student dataset, i.e.|ğ·ğ‘‡|/greatermuch| ğ·ğ‘†|.
The most common transfer learning method is ï¬ne-tuning, where
the developer changes the model output and uses the local student
dataset to ï¬ne-tune the model. The change made on the model is
slight because the knowledge in the teacher model is general enough.
We are interested in the DNN defect inheritance problem in trans-
fer learning, which is deï¬ned as follows:
DEFINITION 2. DNN defect inheritance is the phenomenon that
the input samples which can trigger defects (mislead the output) in
the pretrained teacher model ğ‘€ğ‘‡can also produce misbehavior in
the student model ğ‘€ğ‘†.
Since the teacher models are usually publicly available, attackers
can easily analyze them to ï¬nd adversarial inputs or inject backdoors
into them. Such teacher-originated malicious inputs pose threats
to the student models deployed in the real world, even though the
student models are not white-box available.
Threat Model. We assume the attacker has read and/or write
access (white-box) to the pretrained teacher ğ‘€ğ‘‡, and thus he can
infer or inject defects in the teacher model. This assumption is
reasonable because many pretrained models are publicly available [ 3,
4,68]. The attacker can publish a malicious pretrained model as
well. We also assume the attacker has basic knowledge of the student
model ğ‘€ğ‘†such as the type of the task, but has no read or write access
to the student ğ‘€ğ‘†or student dataset ğ·ğ‘†. It is because the student
models are usually protected and served as a black box. Based on
the knowledge of the teacher model ğ‘€ğ‘‡, the attacker can generate
malicious inputs that can fool the student model ğ‘€ğ‘†.
2.3 Goal and Challenges
Defenderâ€™s goal. The defender in our scenario is the developer of
the student model ğ‘€ğ‘†who is unaware of how the teacher model ğ‘€ğ‘‡
is trained. Her goal is to optimize the transfer learning protocol to
reduce the defects inherited from ğ‘€ğ‘‡(reduce the misclassiï¬cation
rate on the malicious samples Ëœğ‘¥that are generated from ğ‘€ğ‘‡) while
retaining the student-related knowledge in ğ‘€ğ‘‡.
Figure 2 displays the difference of inherited knowledge between
conventional transfer learning (left) and transfer learning with Re-
MoS (right). Our objective is to reduce the common defects that are
shared by both the teacher model and the student model. Because the
pretrained teacher model is publicly available, removing the shared
defects will reduce the exposed attack surface of the student model
and improve the security level.
Speciï¬cally, we want to achieve the following objectives:
	


	

	




	


 

Figure 2: The goal of ReMoS is to reduce the common defectsshared by the teacher and the student.
â€¢Effectiveness
. The inherited defects in the student model ğ‘€ğ‘†
should be effectively reduced, i.e. reducing the inheritance
rate of the adversarial samples and backdoor samples.
â€¢Accuracy . The student model ğ‘€ğ‘†should still be able to reuse
the student-related knowledge in ğ‘€ğ‘‡to achieve high accuracy
on the student dataset ğ·ğ‘†.
â€¢Efï¬ciency and utility . Since efï¬ciency and ease of use are
major reasons why developers use transfer learning, the new
protocol should not bring too much overhead or introduce too
much complexity.
Reducing the inherited defects is challenging mainly due to the
limited interpretability of DNNs. A model is usually viewed asa black box whose knowledge can only be changed by feedingdifferent training data. It is unclear how to identify and separatedefect-related knowledge from student-related knowledge during
transfer learning.
3 RELEV ANT MODEL SLICING
Inspired by the relevant slicing technique in traditional programs,
we believe a similar technique for DNN models can help distin-
guish student-related and potential defect-related knowledge in a
model, and further mitigate the defect inheritance problem in transfer
learning.
3.1 Relevant Slicing for Traditional Programs
Relevant slicing [ 5] is an important slicing technique and has many
applications such as debugging[ 22], regression testing [ 9] and pri-
oritizing test case [ 27]. We ï¬rst give the formal deï¬nition of the
relevant slice as follows:
DEFINITION 3. (Relevant Slicing) Given a program ğ‘ƒand a
slicing criterion (a test case ğ‘¡and a target statement ğ‘ ), relevant
slicing is to compute a subset of program statements that inï¬‚uence or
have the potential to inï¬‚uence the statement ğ‘ during the execution
ofğ‘¡.
Relevant slicing can be used to reduce vulnerability in traditional
programs. An example is shown in Figure 3. Suppose the code
snippet on the left is a public library function that contains a potential
defect. If the input ğ‘is 0, a DivideByZero error may occur at line
8. If this piece of code is used without modiï¬cation in a custom
application, the bug may be exploited by attackers to pose threats to
1859
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:52:09 UTC from IEEE Xplore.  Restrictions apply. ReMoS: Reducing Defect Inheritance in T ransfer Learning via Re levant Mo del S licing ICSE â€™22, May 21â€“29, 2022, Pittsburgh, P A, USA
Figure 3: An example code snippet (left) and the relevant slice
with respect to the criterion < {ğ‘=0,ğ‘=4}, 14,ğ‘¤> (right).
the application. However, with the relevant slice, the statements that
are not related to the custom application can be removed to reduce
the attack surface. Suppose we only care about a limited number
of input cases in the custom application, thus we can compute the
relevant slice concerning the criterion whose inputs are the interested
cases. For example, if the interested input case is {ğ‘=0,ğ‘=4}and
the criterion is < {ğ‘=0,ğ‘=4}14,ğ‘¤>, the corresponding relevant
slice is shown in the right code snippet of Figure 3. In the custom
application, using the original library code (left) or the relevant slice
(right) does not inï¬‚uence the correctness of the program (the output
ğ‘¤at line 14). However, the latter can remove the inherited defect
and avoid the potential security risk (DivideByZero error).
3.2 Relevant Slicing for DNN Models
Similar to the example of reducing inherited defects in traditionalprograms through relevant slicing, we think it might be helpful to
adapt the concept of relevant slicing to DNN models to reduce defect
inheritance.
The idea of slicing a DNN model is not new. Zhang et al.[ 73]
proposed to represent the decision process of a DNN with a slice
(a subset of critical neurons and synapses that made a greater con-
tribution to the prediction) and used the computed slices to detect
adversarial inputs. Although our goal is different, we could follow
their deï¬nition and customize it based on our scenario:
DEFINITION 4. (Relevant Model Slicing) Given a DNN model
ğ‘€and a target domain dataset ğ·, relevant model slicing is to com-
pute a subset of model weights that are more relevant (bounded by a
threshold) to the inference of samples in ğ·and less relevant to the
samples outside ğ·.
If we could obtain such a relevant model slice, we can use it to
reduce inherited defects in transfer learning, just like using relevant
program slices to reduce potential vulnerability in traditional soft-
ware. Speciï¬cally, we can transfer the knowledge in the relevant
slice to the students to retain useful knowledge, whiling removing
the rest of irrelevant parts to avoid the defect-related knowledgepropagating into the student model. The goal can be formally de-
scribed as ï¬nding a subset of the teacher modelâ€™s weights wâŠ‚wğ‘‡
that, when wis used in transfer learning, can maximize the accuracy
of the student model on both clean data and malicious data that isgenerated from the teacher model.
max
wâŠ‚wğ‘‡/summationdisplay.1
(ğ‘¥,ğ‘¦)âˆˆğ·ğ‘†I[ğ‘“(ğ‘¥;ğ‘‡(w))=ğ‘¦]+I[ğ‘“(Ëœğ‘¥;ğ‘‡(w))=ğ‘¦)],(2)
where ğ‘‡(Â·)represents the transfer learning process and ğ‘‡(w) is the
weights after training. I[Â·]is the indicator function.
The concept of ï¬nding a subset of weights can also be found in
model pruning approaches [ 23] that are mainly used to reduce the
model size. For example, magnitude-based pruning [ 23] proposes to
trim the small-magnitude weights to save computation cost. However,
such a subset is not a relevant slice because the weights with larger
magnitudes are not necessarily only relevant to the target studenttask. They are relevant to all the domains that are included by the
teacher modelâ€™s domain.
4 OUR APPROACH
Computing the relevant slice as deï¬ned in Section 3.2 by directly
solving Equation 2 is difï¬cult due to the unpredictable DNN train-
ing process and unavailable prior knowledge about defects. In this
section, we introduce a heuristic algorithm to approximately ï¬ndthe relevant model slice and use it to reduce defect inheritance in
transfer learning.
4.1 Overview
The workï¬‚ow of ReMoS consists of four steps: coverage frequency
proï¬ling, ordinal score computation, relevant slice generation, and
ï¬ne-tuning. An illustration with a three-layer neural network isshown in Figure 4. For simplicity, we assume there are three data
samples in the student dataset.
Coverage Frequency Proï¬ling computes the coverage frequency
of each weight, which represents the relevance to the student task.
The weight coverage frequency is computed based on the neuron
coverage frequency proï¬led over the student dataset. The ï¬rst stage
of Figure 4 displays the neuron coverage frequency (in the neurons)
and the weight coverage frequency (on the weights).
The Ordinal Score Computation step computes a score for each
weight based on the static information of the teacher model (weight
magnitude) and the dynamic information on the student task (weight
coverage frequency). Because the range of weight magnitude and
the coverage frequency is different, ReMoS uses the ordinal score as
the yardstick. In the second stage of Figure 4, the ordinal score is
displayed in the pink circles on each weight.
Relevant Slice Generation identiï¬es the relevant slice based on
the ordinal scores. The weights with large scores are included in the
slice and others are reset. In the third stage of Figure 4, the size of
the slice is constrained to be nine. Therefore, the connections with
red circles are excluded by the slice and are reset.
Finally, the ï¬ne-tuning step uses the student dataset to train the
model as displayed in the last stage of Figure 4. This step is the same
as the conventional transfer learning, except that the student model
is initialized with the computed relevant slice.
We then explain why ReMoS can achieve the three objectives in
Section 2. First, because ReMoS only contains the weights that are
frequently covered by the student dataset, the relevance between the
slice and the student task is high. The accuracy of the student model
is not harmed and can be recovered by a fast ï¬ne-tuning phase.
1860
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:52:09 UTC from IEEE Xplore.  Restrictions apply. ICSE â€™22, May 21â€“29, 2022, Pittsburgh, P A, USA Ziqi Zhang, Y uanchun Li, Jindong Wang, Bingyan Liu, Ding Li, Y ao Guo, Xiangqun Chen, and Y unxin Liu


	




	
	
 
	 

	

	




	
	 	


	



	




	

	
	

	




	

	



	


	  
Figure 4: The workï¬‚ow of transfer learning with ReMoS.
Second, the weights that are less relevant to the student domain
are excluded from transfer learning, including the large-magnitude
weights that are potentially more relevant to the defects in the teacher
model. This design improves the effectiveness of reducing defect
inheritance. Third, in the implementation, the relevant slice accounts
for a large portion of the teacher model. Most of the student model
is initialized by the teacher model, which leads to higher efï¬ciency
when tuning the student model. At last, our slicing algorithm is not
restricted to any speciï¬c operator or model architecture. It can be
applied to both Convolutional Neural Networks and Transformer-
based NLP models.
4.2 Coverage Frequency Proï¬ling
We ï¬rst illustrate the formulation of the DNN model. A deep neu-
ral network model can be abstracted to be a set of layers: ğ‘€=
{ğ¿1,ğ¿2,Â·Â·Â·,ğ¿ğ¾}, where ğ¾is the number of layers (called depth). ğ¿1
is the input layer. Each layer ğ¿ğ‘˜consists of several neurons: ğ¿ğ‘˜=
{ğ‘›ğ‘˜,1,ğ‘›ğ‘˜,2,Â·Â·Â·,ğ‘›ğ‘˜,ğ‘¡ğ‘˜}andğ‘¡ğ‘˜denotes the number of neurons in the
layer ğ¿ğ‘˜. Layer ğ¿ğ‘˜(ğ‘˜>1) contains a weight matrix wğ‘˜âˆˆRğ‘¡ğ‘˜âˆ’1Ã—ğ‘¡ğ‘˜
that connects all the neurons of the preceding layer to the current
layer. Each value (weight) ğ‘¤ğ‘˜,ğ‘–,ğ‘—âˆˆwğ‘˜connects the neuron ğ‘›ğ‘˜âˆ’1,ğ‘–to
the neuron ğ‘›ğ‘˜,ğ‘—. For presentation simplicity, we mainly concentrates
on the linear layers in DNN models to describe our methodology.
However, our technique is also applicable to other model architec-
tures (as shown in the experiments of Section 5).
Lethğ‘˜denote the neuron activation (internal feature) of the ğ‘˜-th
layer. hğ‘˜,ğ‘–is the activation value of neuron ğ‘›ğ‘˜,ğ‘–. The computation
process of each layer can be formulated as (omitting the bias term
for simplicity):
hğ‘˜+1=ReLU(wğ‘˜Â·hğ‘˜), (3)
where ReLU is a popular activation function. We deï¬ne the output
of the model ğ‘€on the input ğ‘¥as the set of all internal features:
Run(ğ‘€,ğ‘¥)={h2,h3,Â·Â·Â·,hğ¾}. (4)
We then deï¬ne the coverage metric of DNN. We focus on the
widely used neuron coverage (NC). Let Cov represent the neuron
coverage function, which takes the internal features as input andoutputs the indexes of the covered neuron. The neuron coverage
function ï¬nds the neurons whose activation values are larger than athreshold ğ›¼. The neuron coverage on the sample ğ‘¥is formulated as:
Cov(ğ‘¥)=Cov(Run(ğ‘€,ğ‘¥))=Cov({h1,h2,Â·Â·Â·,hğ¾})
={vğ‘–|vğ‘–=I[hğ‘–>ğ›¼]}.(5)
Each vector vğ‘–represents which neurons at layer ğ¿ğ‘–are covered by
the data ğ‘¥.
ReMoS ï¬rst iterates the student dataset and records the coverage
information for each sample. Then the neuron coverage frequency
on the dataset ğ·ğ‘†is computed as:
Cov(ğ·ğ‘†)={/summationdisplay.1
ğ‘¥âˆˆğ·ğ‘†Cov(ğ‘¥)ğ‘–|ğ‘–=2,3,Â·Â·Â·,ğ¾}. (6)
The coverage frequency of the weight ğ‘¤ğ‘˜,ğ‘–,ğ‘— is the sum of the neuron
coverage frequency of two neurons that this weight connects:
CovW(ğ·ğ‘†)ğ‘˜,ğ‘–,ğ‘—=Cov(ğ·ğ‘†)ğ‘˜âˆ’1,ğ‘–+Cov(ğ·ğ‘†)ğ‘˜,ğ‘— (7)
The intuition is that if one neuron is frequently covered by ğ·ğ‘†, the
relevant weights should be frequently covered as well. Because the
student dataset is often small, proï¬ling does not cost much time or
computation resources.
4.3 Ordinal Score Computation
This step computes the ordinal scores to determine which weights
should be included in the relevant slice. The score combines thestatic information of the teacher model (weight magnitude) andthe dynamic information on the student task (weight coverage fre-
quency).
Under the premise that the knowledge in the slice is sufï¬cient for
ï¬ne-tuning, we want the slice to contain as little relevant knowledge
as possible. In this way, the risk of containing defect-related knowl-
edge is reduced. The weight magnitude is an important measurement
of the teacherâ€™s knowledge (the larger the weight magnitude is, the
more signiï¬cant it is to the teacher). Therefore, we use the absolute
weight values inside the slice as an indicator of the amount of teacher
knowledge. We formulate the objective of ReMoS as:
wğ‘…ğ‘’ğ‘€ğ‘œğ‘†=argmax
wâŠ‚wğ‘‡ğ´ğ¶ğ¶(ğ‘‡(w),ğ·ğ‘†)âˆ’/summationdisplay.1
ğ‘¤âˆˆw|ğ‘¤| (8)
Our heuristic solution to the above function is to select the weights
with small magnitude and large student-task relevance. However, one
challenge of combining the two indexes is the different value range.
1861
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:52:09 UTC from IEEE Xplore.  Restrictions apply. ReMoS: Reducing Defect Inheritance in T ransfer Learning via Re levant Mo del S licing ICSE â€™22, May 21â€“29, 2022, Pittsburgh, P A, USA
Take ResNet18 for example, the range of magnitude is [10âˆ’12,1.02].
The range of weight coverage frequency on the MIT Indoor Scenes
dataset [ 57]i s[0,5374] . To solve this problem, ReMoS uses the
order of one weight inside the whole model weight set as a uniï¬ed
metric.
Formally, for the weight ğ‘¤ğ‘˜,ğ‘–,ğ‘— , the ordinal magnitude score and
the ordinal coverage score is computed by:
ğ‘œğ‘Ÿğ‘‘ _ğ‘šğ‘ğ‘”ğ‘˜,ğ‘–,ğ‘—=ğ‘Ÿğ‘ğ‘›ğ‘˜(|ğ‘¤ğ‘˜,ğ‘–,ğ‘—|),
ğ‘œğ‘Ÿğ‘‘ _ğ‘ğ‘œğ‘£ğ‘˜,ğ‘–,ğ‘—=ğ‘Ÿğ‘ğ‘›ğ‘˜(CovW(ğ·ğ‘†)ğ‘˜,ğ‘–,ğ‘—),(9)
where ğ‘Ÿğ‘ğ‘›ğ‘˜ computes the index of the value in the ascending-sorted
array of all weights. The ordinal magnitude score ğ‘œğ‘Ÿğ‘‘ _ğ‘šğ‘ğ‘” is the
position of |ğ‘¤ğ‘˜,ğ‘–,ğ‘—|in the sorted array. The ordinal coverage score
ğ‘œğ‘Ÿğ‘‘ _ğ‘ğ‘œğ‘£ is the position of the weight coverage frequency CovW(ğ·ğ‘†)ğ‘˜,ğ‘–,ğ‘— .
The next step combines the two types of ordinal scores. The
intuition is that the weights with large ordinal coverage scores and
small ordinal magnitude scores should be included in the relevant
slice. Thus, we set the overall ordinal score as the ordinal coverage
score minus the ordinal weight score:
ğ‘œğ‘Ÿğ‘‘ğ‘˜,ğ‘–,ğ‘—=ğ‘œğ‘Ÿğ‘‘ _ğ‘ğ‘œğ‘£ğ‘˜,ğ‘–,ğ‘—âˆ’ğ‘œğ‘Ÿğ‘‘ _ğ‘šğ‘ğ‘”ğ‘˜,ğ‘–,ğ‘—. (10)
4.4 Relevant Slice Generation
This step identiï¬es which weights should be included in the slice
based on the ordinal scores.
Speciï¬cally, we rank the weights by the ordinal score, and the
weights with larger scores are selected as the relevant slice, i.e.
ğ‘ ğ‘™ğ‘–ğ‘ğ‘’(ğ·ğ‘†)={ğ‘¤ğ‘˜,ğ‘–,ğ‘—|ğ‘œğ‘Ÿğ‘‘ğ‘˜,ğ‘–,ğ‘—>ğ‘¡ğœƒ}. (11)
where ğœƒâˆˆ[0,1]is a hyper-parameter representing the ratio of
weights to be selected into the relevant slice (i.e. |w|/|wT|.ğ‘¡ğœƒis the
ordinal score threshold to control the size of the slice.
Finally, the weights in ğ‘ ğ‘™ğ‘–ğ‘ğ‘’(ğ·ğ‘†)are used to initialize the student
model. The weights outside of ğ‘ ğ‘™ğ‘–ğ‘ğ‘’(ğ·ğ‘†)are regarded as less relevant
to the student task and are set to zero to forget defect-related knowl-
edge in the teacher. The initialized student model is ï¬ne-tuned with
the student dataset, just like the normal transfer learning process, to
generate the ï¬nal student model.
5 EV ALUATION
To evaluate ReMoS, we mainly focus on the following aspects:
â€¢Defect mitigation effectiveness : How effective is ReMoS to
mitigate the inheritance of common defects? How much ac-
curacy needs to be sacriï¬ced? How does it compare to other
techniques? (In Section 5.2 and Section 5.3)
â€¢Generalizability: Is ReMoS general enough for different trans-
fer learning tasks? (We test our method on both CV and NLP
tasks, different models, different datasets, and different de-
fects to evaluate the generalizability.)
â€¢Efï¬ciency: How much time does ReMoS spend to get a sat-
isfactory student model? How is its efï¬ciency compared to
conventional transfer learning? (In Section 5.4)
â€¢Interpretability: How is the model trained by ReMoS different
from other student models? Why is ReMoS effective and
efï¬cient? (In Section 5.5)5.1 Experimental Setup
Overall methodology. In our experiments, we directly adopted var-
ious attacks in the prior literature [ 20,21,29,31,32,55,58,70]
to evaluate ReMoS. We ï¬rst downloaded pretrained models from
the Internet as the teacher models. Then we used our technique and
other baseline methods to train student models from the teachers. We
followed the prior attack literature, acted as the attackers who have
access to the teacher models, and tried to generate malicious inputs
that can fool the student models. These inputs were used to measure
how many defects each student has inherited from the teacher (if
more defects are inherited, the student will have a higher error rate
on the malicious inputs.) The models, datasets, and defects used for
CV tasks and NLP tasks are different, which will be introduced in
the corresponding subsections.
Implementation We implement ReMoS on PyTorch 1.7 [ 53] for
the ease of customizing DL training procedures. For the neuroncoverage implementation, we slightly changed the code base of
EvalDNN [63] to proï¬le the coverage frequency.
Evaluation metrics. There are mainly two metrics compared
across different student training techniques in our experiments, in-
cluding accuracy ( ğ´ğ¶ğ¶ ) and defect inheritance rate ( ğ·ğ¼ğ‘… ). The accu-
racy is computed by ğ´ğ¶ğ¶=ğ‘šğ‘’ğ‘ğ‘›(ğ‘¥,ğ‘¦)âˆˆğ·ğ‘†I[ğ‘“(ğ‘¥)=ğ‘¦]. The defect
inheritance rate is computed as the misclassiï¬cation rate on the ma-
licious dataset ğ·ğ‘€,i.e.ğ·ğ¼ğ‘…=ğ‘šğ‘’ğ‘ğ‘›(Ëœğ‘¥,ğ‘¦)âˆˆğ·ğ‘€I[ğ‘“(Ëœğ‘¥)â‰ ğ‘¦].ğ·ğ‘€is
formed by the generated misclassiï¬ed samples from ğ·ğ‘†.
The accuracy and defect inheritance rate may differ on different
datasets, making it difï¬cult to directly compare the performance of
different student training techniques. Thus, we additionally intro-duce the relative accuracy and relative defect inheritance rate forcomparison. Suppose
ğ´ğ¶ğ¶ğ‘ 
ğ‘“ğ‘¡is the accuracy achieved by the con-
ventional ï¬ne-tuning technique on student task ğ‘ andğ‘†is the set of
student tasks. For each student training technique, the mean relative
accuracy is computed as ğ‘Ÿğ´ğ¶ğ¶ğ‘š=ğ‘šğ‘’ğ‘ğ‘›ğ‘ âˆˆğ‘†{ğ´ğ¶ğ¶ğ‘ 
ğ‘š
ğ´ğ¶ğ¶ğ‘ 
ğ‘“ğ‘¡}(i.e. the mean
accuracy compared to the traditional ï¬ne-tuning technique), and the
mean relative defect inheritance rate is ğ‘Ÿğ·ğ¼ğ‘…ğ‘š=ğ‘šğ‘’ğ‘ğ‘›ğ‘ âˆˆğ‘†{ğ·ğ¼ğ‘…ğ‘ 
ğ‘š
ğ·ğ¼ğ‘…ğ‘ 
ğ‘“ğ‘¡}.
5.2 Defect Mitigation on CV Tasks
This section and Section 5.3 discuss the effectiveness of ReMoS on
CV and NLP tasks respectively.
Setup. Models and Datasets We used two state-of-the-art large-
scale CNN models ResNet18 and ResNet50 [ 24] on ï¬ve popu-
lar transfer learning datasets, including MIT Indoor Scenes [ 57],
Caltech-UCSD Birds [ 67], 102 Category Flowers [ 50], Stanford 40
Actions [ 69] and Stanford Dogs [ 30]. The selected datasets are rep-
resentative with the number of classes ranges from 40 to 200. The
number of parameters of the two models are 11M and 25M [2].
Baselines. We compare ReMoS with six representative baselines,
including ï¬ne-tuning, retraining, DELTA, mag-pruning, DELTA-R,
and Renofeation. Among them, ï¬ne-tuning and retraining do not
incorporate any defense at all. DELTA is a technique that is widely
used by deep learning developers to improve accuracy [ 34]. This
technique encourages the student model to have similar feature maps
as the teacher model. Mag-pruning is a state-of-the-art â€œ ï¬x-after-
transfer â€ approach for backdoor removal [ 44]. This technique prunes
1862
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:52:09 UTC from IEEE Xplore.  Restrictions apply. ICSE â€™22, May 21â€“29, 2022, Pittsburgh, P A, USA Ziqi Zhang, Y uanchun Li, Jindong Wang, Bingyan Liu, Ding Li, Y ao Guo, Xiangqun Chen, and Y unxin Liu
the small-magnitude weights. We followed the implementation of
[44] and set the pruning ratio to 80%.
DELTA-R and Renofeation are two representative â€œï¬x-before-
transfer â€ techniques. DELTA-R, similar to DELTA, cooperates fea-
ture regulation to learn from the teacher. Renofeation is a recent work
to mitigate adversarial vulnerability inheritance on CV tasks [ 11],
which cooperates feature regulation, internal dropout, and stochastic
weighted average to train the student model.
We used the conï¬gurations (learning rate, momentum, etc.) that
can achieve optimal accuracy on the student tasks. For the â€œ ï¬x-
after-transfer â€ techniques, we set the learning rate to 1e-2. For the
â€œï¬x-before-transfer â€, we set a larger learning rate to 1e-1. We set the
number of training iterations as 30K and keep it the same across all
techniques.
Defect Simulation. To simulate the situation of defect inheri-
tance, we need to generate malicious inputs based on the teacher
model and test whether they lead to misclassiï¬cation on the stu-
dent model. We consider two kinds of adversarial attacks, including
the penultimate-layer-guided adversarial attacks and the neuron-
coverage-guided adversarial attacks. For the former one, we followedthe state-of-the-art attack introduced by Rezaei et al.[
58] to generate
a set of malicious inputs ğ·ğ‘€. We selected this attack because it is
one of the known strongest attacks in white-box setting [61].
For the neuron-coverage-guided attacks, motivated by the prior
work on DNN testing [ 33], we combined various coverage metrics
and different strategies to generate ğ·ğ‘€. The coverage metrics in-
clude neuron coverage (NC), top-k neuron coverage (TKNC), and
strong neuron activation coverage (SNAC). TKNC covers the neu-
rons that have the highest ğ‘˜activation values (we select ğ‘˜as 10
follows [ 33]). SNAC covers the neurons that the activation value
is high enough inside the layer. The strategies include DeepX-plore (randomly selects inactivated neurons as the target neuronsto activate) [
55], DLFuzz (selects the most covered neurons) and
DLFuzz RR(combines three strategies in a round-robin manner [ 21]).
For each sample, we optimized the input gradient to cover as many
internal neurons as possible. We selected the last sample during the
optimization procedure as the malicious sample.
The simulation of backdoor defects was slightly more compli-
cated. We followed the settings of Latent Backdoor et al.[ 70] and
poisoned the training data of the teacher model. We assumed theattacker had a training dataset of a similar data distribution to the
student dataset. Some samples in the attackerâ€™s dataset were attached
by a trigger sign (e.g. a Firefox logo) and labeled mistakenly onpurpose. By tuning the pretrained model on the attackerâ€™s dataset,
we obtained a backdoored pretrained model. Then the backdoored
pretrained model was used as the teacher to generate student models.
Results. The defect reduction effectiveness of ReMoS on penultimate-
layer-guided adversarial samples, neuron-coverage-guided adversar-
ial samples, and backdoor samples are shown in Figure 5, Figure 6,
and Figure 7, respectively. Generally, ReMoS signiï¬cantly reduces
defect inheritance with minimal accuracy loss. The discussion on
each kind of defect is detailed below.
Adversarial vulnerability. We ï¬rst discuss the two types of ad-
versarial samples. Figure 5 shows the defect mitigation results on
penultimate-layer-guided adversarial samples. We plot the results onall of the ï¬ve datasets. The ï¬rst row displays the student accuracy
and the second row shows the defect inheritance rate.
Conventional transfer learning achieves high accuracy on the
student datasets, although at the cost of high ğ·ğ¼ğ‘… s. In Figure 5, 8 out
of 10 cases have higher ğ·ğ¼ğ‘… than 50%. The phenomenon implies
that the inheritance of adversarial vulnerability is practical and the
developers should pay attention to it. Retraining has the lowest ğ·ğ¼ğ‘…
but the accuracy is signiï¬cantly lower ( ğ‘Ÿğ´ğ¶ğ¶ğ‘š=0.58).
DELTA can partially increase the accuracy in some cases. How-
ever, itâ€™s ğ·ğ¼ğ‘… s are signiï¬cantly higher (over 75% on ResNet18).
Mag-pruning is ineffective to reduce the inherited adversarial vulner-
ability ( ğ‘Ÿğ·ğ¼ğ‘…ğ‘š=1.06), which demonstrates that the low-magnitude
weights are not critical to the teacher modelâ€™s knowledge, including
the defect-related knowledge. We can also observe that generally the
ğ·ğ¼ğ‘… s of ResNet50 are lower than ResNet18. Itâ€™s probably because
that the increased model capacity helps to increase the robustness.
This observation aligns with the prior work [48].
ReMoS only sacriï¬ces less than 2% model accuracy and reduces
over 75% inherited defects than the conventional ï¬ne-tuning (the
ğ‘Ÿğ·ğ¼ğ‘…ğ‘šis 0.25 for ResNet18 and 0.22 for ResNet50). Besides, in
many cases, the ğ·ğ¼ğ‘… of ReMoS are comparable with the students
trained from scratch (minimum value for ğ·ğ¼ğ‘… ).
The two â€œï¬x-before-transferâ€ techniques (DEL T A-R and Renofeation)
can partially improve the accuracy of the student models based on
retraining, but the performance is still lower than the ï¬ne-tuned
students. Besides, we can observe that such techniques have lower
accuracies on ResNet50 than ResNet18. We think the reason is that
the increased complexity of ResNet50 makes it more difï¬cult to
train on the small-scale student dataset.
Similarly, the defect reduction results on neuron-coverage-guided
adversarial samples are shown in Figure 6. For the ï¬ne-tuned stu-
dents, the ğ·ğ¼ğ‘… s (averagely 51.92%) are lower than that of penultimate-
layer-guided samples (averagely 75.50%). We think the reason isthat when generating adversarial samples, targeting on the mid-dle layers is less effective than targeting on the penultimate layer.
The latter can directly fool the last FC layer. On the contrary, the
turbulence made on the intermediate layers may be corrected bysubsequent layers (which are trained to have fault tolerance). Forneuron-coverage-guided samples, the target layers contain a large
number of intermediate layers.
It can also be observed that the inheritance rates differ between
various coverage metrics and strategies. For the coverage strategies,
the defects discovered by DLFuzz inherit most in transfer learning
(averagely 59.85%). It means that selecting the most covered neurons
can generate easier-to-inherit adversarial samples. For the coverage
metrics, TKNC is the worst effective (the average ğ·ğ¼ğ‘… is 40.14%).
We think the reason is that the top ğ‘˜neurons are too little compared
to the enormous neurons inside the model.
ReMoS reduces the inheritance rates in all cases by a large mar-
gin. The ğ‘Ÿğ·ğ¼ğ‘…ğ‘šis 0.37 over the two datasets. On average, 63% of
inherited defects that are generated by the neuron-coverage-guided
techniques can be eliminated by ReMoS.
Backdoor. The backdoor mitigation result is shown in Figure 7.
The average inheritance rate is 72.91% for ï¬ne-tuning and 62.61%
for mag-pruning. It means the backdoor knowledge is partially en-
coded in the low-magnitude weights. ReMoS reduces the inherited
defects to a similar level of retraining (the ğ‘Ÿğ·ğ¼ğ‘…ğ‘šis 0.14). It means
1863
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:52:09 UTC from IEEE Xplore.  Restrictions apply. ReMoS: Reducing Defect Inheritance in T ransfer Learning via Re levant Mo del S licing ICSE â€™22, May 21â€“29, 2022, Pittsburgh, P A, USA
Figure 5: The accuracy ( ğ´ğ¶ğ¶ ) and adversarial vulnerability inheritance rate ( ğ·ğ¼ğ‘… ) of ReMoS and other student training techniques.
Figure 6: The inheritance rate of adversarial inputs generated by different neuron-coverage-guided test generators on ResNet18.
Figure 7: The accuracy ( ğ´ğ¶ğ¶ ) and backdoor inheritance rate
(ğ·ğ¼ğ‘… ) of ReMoS and other baselines on ResNet50.
that the large-magnitude weights excluded by ReMoS are critical to
the backdoor knowledge. After resetting such weights, the backdoor
defect is mitigated by a large margin.
5.3 Defect Mitigation on NLP Tasks
Setup. Models and Datasets. We used two popular models BERT
and RoBERTa in the experiment [ 46,68]. ReMoS can also be used
for other NLP pretrained models. The datasets are SST-2 [ 62] andIMDB [ 47] for backdoor attacks and SST-2 and QNLI [ 56] for ad-
versarial attacks following the prior settings [32, 49].
Baselines. We took two baselines from the CV experiments: con-
ventional ï¬ne-tuning and magnitude-based pruning. The other base-
lines are not designed for NLP models and cannot achieve reasonable
accuracy, thus were excluded from the comparison.
Defect Simulation. The backdoor implementation followed the
prior work [ 32] which considers two scenarios: full domain knowl-
edge (FDK) and domain shift (DS). FDK means that the attacker
knows the student dataset and uses the identical dataset to embed the
backdoor. DS means the attack dataset is different from the victim
dataset. We used SST-2 and IMDB as the attack dataset and the
victim dataset to form the four scenarios. We adopted two effective
attack techniques from [32]: data poisoning and weight poisoning.
For adversarial vulnerability, we assumed the FDK scenario. We
adopted two public implementations from TextAttack: Kuleshov [ 31]
and TextFooler [ 29]. Kuleshov uses a greedy word swap strategy
to ï¬nd the adversarial samples. TextFooler uses the greedy word
importance ranking algorithm.
Results. The defect reduction results of ReMoS on the backdoor
and adversarial samples are displayed in Table 2 and Table 3, re-
spectively. The defect inheritance rate is reduced by 50% and 61%
1864
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:52:09 UTC from IEEE Xplore.  Restrictions apply. ICSE â€™22, May 21â€“29, 2022, Pittsburgh, P A, USA Ziqi Zhang, Y uanchun Li, Jindong Wang, Bingyan Liu, Ding Li, Y ao Guo, Xiangqun Chen, and Y unxin Liu
Figure 8: The convergence progress of conventional transfer
learning, retraining, and ReMoS.
for two kinds of backdoor samples. ReMoS can also reduce 40%inherited adversarial vulnerabilities. Then we will summarize the
results on the two defects.
Backdoor. As shown in Table 2, the backdoor attack is very ef-
fective and all ğ·ğ¼ğ‘… s of ï¬ne-tuning are over 88%, meaning that the
backdoor inheritance problem in NLP tasks is more severe than that
in CV . We think there are two reasons. First, for NLP tasks, the
high-dimensional trigger vector (translated from the trigger word) is
more distinct than triggers CV tasks. The difference between vectorsis larger than the difference between CV images (pixel values are be-tween 0 and 1 after normalized). Second, NLP models contain much
more parameters. BERT has 110M parameters [ 1] while ResNet50
has 25M parameters [ 2]. The more redundant parameters may be
easier to convey the backdoor logic.
With ReMoS, the ğ‘Ÿğ·ğ¼ğ‘…ğ‘šis 0.50 for data poisoning and 0.39
for weight poisoning, and the relative accuracy ğ‘Ÿğ´ğ¶ğ¶ğ‘šis about
0.97. The ğ‘Ÿğ·ğ¼ğ‘…ğ‘šof mag-pruning is 0.98. Both ReMoS and mag-
pruning are less effective on NLP models than the CV models. This
phenomenon implies that reducing the NLP backdoor inheritance
might be more difï¬cult.
Adversarial vulnerability. Table 3 displays the results on the
inherited adversarial vulnerability. For simplicity, we report the
average ğ·ğ¼ğ‘… over two attacks. For ï¬ne-tuned students, the average
ğ·ğ¼ğ‘… is 79.80%. Similar to the backdoor, the ğ·ğ¼ğ‘… s on NLP models
are higher than the CV models. Mag-pruning can partially reduce
theğ·ğ¼ğ‘… (theğ‘Ÿğ·ğ¼ğ‘…ğ‘šis 0.76). The ğ‘Ÿğ·ğ¼ğ‘…ğ‘šof ReMoS is much lower
(0.60) with slight accuracy drop ( ğ‘Ÿğ´ğ¶ğ¶ğ‘š=0.99).
5.4 Efï¬ciency and Overhead
We compare the training efï¬ciency of ReMoS with ï¬ne-tuning and
retraining in Figure 8. The convergence speed of ReMoS is close to
ï¬ne-tuning and signiï¬cantly faster than retraining. Both ReMoS and
ï¬ne-tuning achieve the optimal accuracy within 50 minutes on the
Scenes dataset and within 100 minutes on the Birds dataset.
Since the training process of ReMoS does not introduce additional
time cost, the overhead of ReMoS is just the slice computation time.
The proï¬ling step in slice computation is similar to performing a
forwarding pass with the training dataset, which only takes several
minutes. After proï¬ling, computing the ordinal scores and selecting
the relevant weights take less than a minute on ResNet18 and about
three minutes on ResNet50. Meanwhile, ReMoS can be easily inte-
grated into the conventional transfer learning pipeline without anyFigure 9: The distribution of
weight changes during train-
ing.Figure 10: The magnitude of
weights excluded by ReMoS
in each layer.
task-speciï¬c conï¬guration. Therefore, we believe ReMoS is efï¬cient
and easy-to-use enough for student model developers.
5.5 Interpreting the Slice
To further understand the efï¬ciency and effectiveness of ReMoS, we
inspect the weight pattern of the student model. First, we measured
the weight changes of the student models before and after training.
Speciï¬cally, we computed the weight difference ğ‘¤ğ‘†/ğ‘¤ğ‘‡between
the student model and the teacher model. The result is shown inFigure 9. For conventional transfer learning (ï¬ne-tuning), most of
the weight changes are around 1.0. It means that most weights do nothave to change too much to ï¬t the student dataset. Retraining suffers
from low performance because the weight changes are much larger
and the small student dataset is not enough. ReMoS signiï¬cantly
reduces the number of weights that require large-range adjustment,
thus the convergence speed is high.
We also studied the magnitude distribution of the weights ex-
cluded from the relevant slice (i.e. the weights that are reset to zero
in ReMoS). The result is shown in Figure 10. As the layer depth
increases, more weights with higher magnitude are excluded. This re-sult is intuitive because, in a DNN model, the weights at deep layers
are usually related to high-level features, thus may be less relevant
to the speciï¬c student tasks. Instead, some of them may contribute a
lot to certain defect-related decision logic. ReMoS excludes these
weights from the student model, thus it can reduce defect inheritance
while retaining useful knowledge.
6 RELATED WORK
The transferability of both adversarial attacks and backdoor attacks
has been extensively studied before. Wang et al.[ 64] and Rezaei et
al.[ 58] proposed to generate adversarial inputs targeting interme-
diate layers of the teacher models. These techniques are similar to
neuron-coverage-guided DNN testing techniques. BadNets [ 20] and
TrojanNN [ 45] have demonstrated the backdoor attacks are inher-
itable in their experiments. V arious approaches [ 66,70] have also
been proposed to make the backdoors more transferable. Similarly,
Jiet al. presented model-reuse attacks wherein malicious primitive
models may infect host ML systems [28].
To mitigate defect inheritance, one way is to improve the ro-
bustness of teacher models and making the robustness transferable
[12,59]. However, in this paper, we focus on the student model
developersâ€™ side and consider how to avoid inheriting defects from
public teacher models that may be insecure.
A straightforward solution for student model developers to reduce
defect inheritance is â€œï¬x-after-transfer â€, in which the developers
1865
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:52:09 UTC from IEEE Xplore.  Restrictions apply. ReMoS: Reducing Defect Inheritance in T ransfer Learning via Re levant Mo del S licing ICSE â€™22, May 21â€“29, 2022, Pittsburgh, P A, USA
Table 2: The defect reduction effectiveness of ReMoS against two backdoor attacks on NLP tasks. For each model, we include four
situations where the attackerâ€™s dataset may be the same or different as the student dataset.
Model DatasetData Poisoning Weight Poisoning
Fine-tune Mag-prune ReMoS Fine-tune Mag-prune ReMoS
ğ´ğ¶ğ¶ ğ·ğ¼ğ‘… ğ´ğ¶ğ¶ ğ·ğ¼ğ‘… ğ´ğ¶ğ¶ ğ·ğ¼ğ‘… ğ´ğ¶ğ¶ ğ·ğ¼ğ‘… ğ´ğ¶ğ¶ ğ·ğ¼ğ‘… ğ´ğ¶ğ¶ ğ·ğ¼ğ‘…
BER TFDKSST-2 to SST-2 92.70 100.00 92.35 100.00 91.27 39.09 92.29 100.00 92.44 100.00 90.92 29.82
IMDB to IMDB 87.96 96.11 88.24 96.15 85.53 61.73 89.34 96.15 89.48 96.09 87.00 37.72
DSSST-2 to IMDB 90.53 100.00 91.26 100.00 90.04 74.67 91.67 100.00 91.16 100.00 87.42 61.48
IMDB to SST-2 93.21 96.17 92.46 96.17 91.15 27.71 92.80 96.22 92.58 96.02 91.94 21.55
RoBERTaFDKSST-2 to SST-2 94.19 100.00 93.70 100.00 91.17 29.82 93.37 100.00 93.19 98.93 90.70 24.94
IMDB to IMDB 90.60 93.52 89.54 95.24 85.74 70.19 89.05 96.53 88.76 92.05 86.34 85.91
DSSST-2 to IMDB 92.11 99.88 92.27 100.00 90.32 24.14 91.85 100.00 90.82 99.53 88.71 30.83IMDB to SST-2 93.52 88.15 92.65 85.26 92.17 61.26 93.85 93.93 93.57 91.21 89.95 18.07
Average Relative V alue - - 0.99 0.99 0.97 0.50 - - 0.99 0.98 0.97 0.39
Table 3: The accuracy ( ğ´ğ¶ğ¶ ) and adversarial vulnerability in-
heritance rate ( ğ·ğ¼ğ‘… ) of ReMoS on NLP tasks.
Model Dataset Fine-tune Mag-prune ReMoS
BER TSST-2ğ´ğ¶ğ¶ 92.20 93.43 92.03
ğ·ğ¼ğ‘… 85.30 67.70 62.23
QNLIğ´ğ¶ğ¶ 89.42 88.84 88.45
ğ·ğ¼ğ‘… 74.94 62.11 45.16
RoBERTaSST-2ğ´ğ¶ğ¶ 94.40 94.06 92.08
ğ·ğ¼ğ‘… 84.94 58.48 49.46
QNLIğ´ğ¶ğ¶ 90.25 91.09 89.60
ğ·ğ¼ğ‘… 74.02 56.61 36.36
Average Relative V alueğ‘Ÿğ´ğ¶ğ¶ğ‘š - 1.00 0.99
ğ‘Ÿğ·ğ¼ğ‘…ğ‘š - 0.76 0.60
ï¬rst generate a student model following the standard transfer learn-
ing procedure, and then use normal defect mitigation techniques
[44,48] to remove the defects. However, due to the shortage of data
and limited knowledge about defects, such techniques (like adver-sarial training [
48]) are uneasy and ineffective. Fine-pruning [ 44]
is probably the most easy-to-use method against backdoors. Never-
theless, the experiments show that it can not guarantee effectiveness
against adversarial vulnerabilities.
The â€œï¬x-before-transfer â€ approach initializes the student model
from scratch and extracts knowledge from the teacher model during
transfer learning. This technique suffers from the performance of
the student model and the utility. Renofeation [ 11] uses knowledge
distillation together with several heuristic tricks to recover the model
accuracy. Nevertheless, its performance is inferior and the utility is
constrained to CNN models.
7 THREAT TO V ALIDITY AND DISCUSSIONS
One possible threat to validity is that the studied samples in theexperiment may not be representative. We tried our best to mini-mize this threat by covering as diverse vulnerabilities as possible,
including vulnerabilities on different domains (CV and NLP), differ-
ent model architectures (ResNet and BERT), and different datasets
(eight datasets in total).
In this paper, we focus on the scenario of two models (the teacher
and the student). However, there are three models in the real-world
backdoor setting. The generalized-teacher model is the publicly-
recognized model by the community. This model can be ï¬ne-tunedby an attacker to inject backdoor and become a domain-teacher
model. The domain-teacher model is published on the Internet, down-
loaded by a deep learning developer, and further ï¬ne-tuned into the
domain-student model (which inherits the defects). We simplify the
three-model setting into two models because, from the DL develop-
ersâ€™ perspective, the generalized teacher and the domain teacher are
essentially the same. Both of them may contain vulnerabilities that
may be inherited by the developersâ€™ student model.
For the retraining baseline, we think it is not suitable for the
orinary DL developers, although at some point retraining achieves
enough accuracy. Retraining large DL models from scratch requires
abundant dataset, multiple expensive GPU, and various trainingtricks. It is difï¬cult for ordinary DL developers to meet all these
requirements. Thus, reusing public model on the Internet becomes a
widely-accepted choice.
One intuitive solution to reduce the inherited defects is to dig
into the teacher model and deal with the important vulnerabilities.Because the teacher models are widely accessible on the Internet,
there should exist common vulnerabilities that have been understood
by the public. These vulnerabilities are called â€œuniversal adversarial
vulnerabilityâ€ [ 35], and only accounts for a small part of the adver-
sarial vulnerabilities in the teacher model. Thus, this solution maybe
not effective enough to reduce as much inherited defects as possible.
8 CONCLUSION
This paper proposes ReMoS, a relevant model slicing technique to re-
duce defect inheritance during transfer learning. ReMoS only reusesthe relevant parts inside the teacher model and retrains the irrelevant
parts to forget the defect-related knowledge. The relevant slice iscomputed based on the neuron coverage information by proï¬ling
the teacher model on the student dataset. Extensive experiments on
seven DNN defects, four DNN models, and eight datasets demon-
strate the effectiveness of the approach. ReMoS can reduce inherited
defects by 63% to 86% for CV tasks and by 40% to 61% for NLP
tasks with an accuracy sacriï¬ce less than 3%.
ACKNOWLEDGMENTS
We would like to thank the anonymous ICSE reviewers for their
valuable feedback of this paper. This work was partly supported by
the National Natural Science Foundation of China (62141208), and a
project sponsored by ZTE Communications. We thank Xihan Zhang
for the CV backdoor implementation.
1866
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:52:09 UTC from IEEE Xplore.  Restrictions apply. ICSE â€™22, May 21â€“29, 2022, Pittsburgh, P A, USA Ziqi Zhang, Y uanchun Li, Jindong Wang, Bingyan Liu, Ding Li, Y ao Guo, Xiangqun Chen, and Y unxin Liu
REFERENCES
[1] [n. d.]. BERT Explained â€“ A list of Frequently Asked Questions. https://
huggingface.co/transformers/pretrained_models.html.
[2] [n. d.]. Keras Applications. https://keras.io/api/applications/.
[3] [n. d.]. Pytorch Hub. https://pytorch.org/hub/.[4] [n. d.]. Tensorï¬‚ow Hub. https://www.tensorï¬‚ow.org/hub.[5]
Hiralal Agrawal, Joseph Robert Horgan, Edward W. Krauser, and Saul London.
1993. Incremental Regression Testing. In Proceedings of the Conference on
Software Maintenance, ICSM 1993, MontrÃ©al, Quebec, Canada, September 1993,
David N. Card (Ed.). IEEE Computer Society, 348â€“357. https://doi.org/10.1109/
ICSM.1993.366927
[6] Michael Backes, Sven Bugiel, and Erik Derr. 2016. Reliable third-party library
detection in android and its security applications. In Proceedings of the 2016 ACM
SIGSAC Conference on Computer and Communications Security. 356â€“367.
[7] Teodora Baluta, Zheng Leong Chua, Kuldeep S Meel, and Prateek Saxena.
2020. Scalable quantitative veriï¬cation for deep neural networks. arXiv preprint
arXiv:2002.06864 (2020).
[8] Ravi Bhoraskar, Seungyeop Han, Jinseong Jeon, Tanzirul Azim, Shuo Chen,
Jaeyeon Jung, Suman Nath, Rui Wang, and David Wetherall. 2014. Brahmastra:
Driving apps to test the security of third-party components. In 23rd USENIX
Security Symposium (USENIX Security 14). 1021â€“1036.
[9] David Binkley. 1998. The application of program slicing to regression testing.
Information and software technology 40, 11-12 (1998), 583â€“594.
[10] Kai Chen, Xueqiang Wang, Yi Chen, Peng Wang, Y eonjoon Lee, XiaoFeng Wang,
Bin Ma, Aohui Wang, Yingjun Zhang, and Wei Zou. 2016. Following devilâ€™s
footprints: Cross-platform analysis of potentially harmful libraries on android and
ios. In 2016 IEEE Symposium on Security and Privacy (SP). IEEE, 357â€“376.
[11] Ting-Wu Chin, Cha Zhang, and Diana Marculescu. 2021. Renofeation: A Simple
Transfer Learning Method for Improved Adversarial Robustness. In Proceedings
of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)
W orkshops. 3243â€“3252.
[12] Todor Davchev, Timos Korres, Stathi Fotiadis, Nick Antonopoulos, and Subrama-
nian Ramamoorthy. 2019. An Empirical Evaluation of Adversarial Robustness
under Transfer Learning. CoRR abs/1905.02675 (2019).
[13] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert:Pre-training of deep bidirectional transformers for language understanding. arXiv
preprint arXiv:1810.04805 (2018).
[14] Zakir Durumeric, Frank Li, James Kasten, Johanna Amann, Jethro Beekman,
Mathias Payer, Nicolas Weaver, David Adrian, V ern Paxson, Michael Bailey, et al .
2014. The matter of heartbleed. In Proceedings of the 2014 conference on internet
measurement conference. 475â€“488.
[15] Y ang Feng, Qingkai Shi, Xinyu Gao, Jun Wan, Chunrong Fang, and ZhenyuChen. 2020. DeepGini: prioritizing massive tests to enhance the robustness of
deep neural networks. In Proceedings of the 29th ACM SIGSOFT International
Symposium on Software T esting and Analysis. 177â€“188.
[16] William B Frakes and Kyo Kang. 2005. Software reuse research: Status and future.
IEEE transactions on Software Engineering 31, 7 (2005), 529â€“536.
[17] Xiang Gao, Ripon K. Saha, Mukul R. Prasad, and Abhik Roychoudhury. 2020.Fuzz testing based data augmentation to improve robustness of deep neural net-
works. In ICSE â€™20: 42nd International Conference on Software Engineering,
Seoul, South Korea, 27 June - 19 July, 2020, Gregg Rothermel and Doo-Hwan
Bae (Eds.). ACM, 1147â€“1158. https://doi.org/10.1145/3377811.3380415
[18] Antonios Gkortzis, Daniel Feitosa, and Diomidis Spinellis. 2019. A Double-Edged
Sword? Software Reuse and Potential Security Vulnerabilities. In International
Conference on Software and Systems Reuse. Springer, 187â€“203.
[19] Martin L Griss. 1998. Software reuse: architecture, process and organization for
business success. IEEE.
[20] Tianyu Gu, Brendan Dolan-Gavitt, and Siddharth Garg. 2017. Badnets: Identify-
ing vulnerabilities in the machine learning model supply chain. arXiv preprint
arXiv:1708.06733 (2017).
[21] Jianmin Guo, Y u Jiang, Y ue Zhao, Quan Chen, and Jiaguang Sun. 2018. DL-Fuzz: differential fuzzing testing of deep learning systems. In Proceedings
of the 2018 ACM Joint Meeting on European Software Engineering Confer-ence and Symposium on the F oundations of Software Engineering, ESEC/SIG-
SOFT FSE 2018, Lake Buena Vista, FL, USA, November 04-09, 2018 , Gary T.
Leavens, Alessandro Garcia, and Corina S. Pasareanu (Eds.). ACM, 739â€“743.
https://doi.org/10.1145/3236024.3264835
[22] Tibor GyimÃ³thy, ÃrpÃ¡d BeszÃ©des, and IstvÃ¡n ForgÃ¡cs. 1999. An Efï¬cient Relevant
Slicing Method for Debugging. In Software Engineering - ESEC/FSEâ€™99, 7th
European Software Engineering Conference, Held Jointly with the 7th ACM
SIGSOFT Symposium on the F oundations of Software Engineering, T oulouse,
France, September 1999, Proceedings (Lecture Notes in Computer Science), Oscar
Nierstrasz and Michel Lemoine (Eds.), V ol. 1687. Springer, 303â€“321. https:
//doi.org/10.1007/3-540-48166-4_19
[23] Song Han, Huizi Mao, and William J. Dally. 2016. Deep Compression: Com-
pressing Deep Neural Network with Pruning, Trained Quantization and Huffman
Coding. In 4th International Conference on Learning Representations, ICLR 2016,San Juan, Puerto Rico, May 2-4, 2016, Conference Track Proceedings, Y oshua
Bengio and Y ann LeCun (Eds.). http://arxiv.org/abs/1510.00149
[24] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep resid-ual learning for image recognition. In Proceedings of the IEEE conference on
computer vision and pattern recognition. 770â€“778.
[25] Ling Huang, Anthony D Joseph, Blaine Nelson, Benjamin IP Rubinstein, and
J Doug Tygar. 2011. Adversarial machine learning. In Proceedings of the 4th
ACM workshop on Security and artiï¬cial intelligence. 43â€“58.
[26] Y ujin Huang, Han Hu, and Chunyang Chen. 2021. Robustness of on-deviceModels: Adversarial Attack to Deep Learning Models on Android Apps. arXiv
preprint arXiv:2101.04401 (2021).
[27] Dennis Jeffrey and Neelam Gupta. 2008. Experiments with test case prioritization
using relevant slices. J. Syst. Softw. 81, 2 (2008), 196â€“221. https://doi.org/10.
1016/j.jss.2007.05.006
[28] Y ujie Ji, Xinyang Zhang, Shouling Ji, Xiapu Luo, and Ting Wang. 2018. Model-
Reuse Attacks on Deep Learning Systems. In Proceedings of the 2018 ACM
SIGSAC Conference on Computer and Communications Security, CCS 2018,
T oronto, ON, Canada, October 15-19, 2018, David Lie, Mohammad Mannan,
Michael Backes, and XiaoFeng Wang (Eds.). ACM, 349â€“363. https://doi.org/10.
1145/3243734.3243757
[29] Di Jin, Zhijing Jin, Joey Tianyi Zhou, and Peter Szolovits. 2020. Is bert really
robust? a strong baseline for natural language attack on text classiï¬cation and en-
tailment. In Proceedings of the AAAI conference on artiï¬cial intelligence, V ol. 34.
8018â€“8025.
[30] Aditya Khosla, Nityananda Jayadevaprakash, Bangpeng Y ao, and Li Fei-Fei. 2011.
Novel Dataset for Fine-Grained Image Categorization. In First W orkshop on Fine-
Grained Visual Categorization, IEEE Conference on Computer Vision and Pattern
Recognition. Colorado Springs, CO.
[31] V olodymyr Kuleshov, Shantanu Thakoor, Tingfung Lau, and Stefano Ermon. 2018.
Adversarial examples for natural language classiï¬cation problems. (2018).
[32] Keita Kurita, Paul Michel, and Graham Neubig. 2020. Weight Poisoning At-tacks on Pretrained Models. In Proceedings of the 58th Annual Meeting of the
Association for Computational Linguistics. 2793â€“2806.
[33] Seokhyun Lee, Sooyoung Cha, Dain Lee, and Hakjoo Oh. 2020. Effective white-
box testing of deep neural networks with adaptive neuron-selection strategy. In
ISSTA â€™20: 29th ACM SIGSOFT International Symposium on Software T esting and
Analysis, Virtual Event, USA, July 18-22, 2020, Sarfraz Khurshid and Corina S.
Pasareanu (Eds.). ACM, 165â€“176. https://doi.org/10.1145/3395363.3397346
[34] Xingjian Li, Haoyi Xiong, Hanchao Wang, Y uxuan Rao, Liping Liu, and JunHuan. 2019. Delta: Deep Learning Transfer using Feature Map with Attention
for Convolutional Networks. In 7th International Conference on Learning Repre-
sentations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net.https://openreview.net/forum?id=rkgbwsAcYm
[35]
Yingwei Li, Song Bai, Cihang Xie, Zhenyu Liao, Xiaohui Shen, and Alan Y uille.
2019. Regional Homogeneity: Towards Learning Transferable Universal Adver-
sarial Perturbations Against Defenses. arXiv preprint arXiv:1904.00979 (2019).
[36] Yingwei Li, Song Bai, Y uyin Zhou, Cihang Xie, Zhishuai Zhang, and Alan Y uille.
2020. Learning Transferable Adversarial Examples via Ghost Networks. In
Proceedings of the AAAI Conference on Artiï¬cial Intelligence, V ol. 34.
[37] Y uanchun Li, Jiayi Hua, Haoyu Wang, Chunyang Chen, and Y unxin Liu. 2021.DeepPayload: Black-box Backdoor Attack on Deep Learning Models through
Neural Payload Injection. arXiv preprint arXiv:2101.06896 (2021).
[38] Y uanchun Li, Ziqi Zhang, Bingyan Liu, Ziyue Y ang, and Y unxin Liu. 2021. Mod-
elDiff: Testing-Based DNN Similarity Comparison for Model Reuse Detection.
Proceeding of the 30th ACM SIGSOFT International Symposium on Software
T esting and Analysis (2021).
[39] Junyu Lin, Lei Xu, Yingqi Liu, and Xiangyu Zhang. 2020. Composite Backdoor
Attack for Deep Neural Network by Mixing Existing Benign Features. In CCS
â€™20: 2020 ACM SIGSAC Conference on Computer and Communications Security,
Virtual Event, USA, November 9-13, 2020 , Jay Ligatti, Xinming Ou, Jonathan Katz,
and Giovanni Vigna (Eds.). ACM, 113â€“131. https://doi.org/10.1145/3372297.
3423362
[40] Bingyan Liu, Yifeng Cai, Y ao Guo, and Xiangqun Chen. 2021. TransTailor:
Pruning the Pre-trained Model for Improved Transfer Learning. Proceedings of
the 35th AAAI Conference on Artiï¬cial Intelligence (2021).
[41] Bingyan Liu, Y ao Guo, and Xiangqun Chen. 2019. WealthAdapt: A generalnetwork adaptation framework for small data tasks. In Proceedings of the 27th
ACM International Conference on Multimedia. 2179â€“2187.
[42] Bingyan Liu, Y uanchun Li, Y unxin Liu, Y ao Guo, and Xiangqun Chen. 2020.Pmc: A privacy-preserving deep learning model customization framework for
edge computing. Proceedings of the ACM on Interactive, Mobile, W earable and
Ubiquitous T echnologies 4, 4 (2020), 1â€“25.
[43] Changliu Liu, Tomer Arnon, Christopher Lazarus, Clark Barrett, and Mykel JKochenderfer. 2019. Algorithms for verifying deep neural networks. arXiv
preprint arXiv:1903.06758 (2019).
[44] Kang Liu, Brendan Dolan-Gavitt, and Siddharth Garg. 2018. Fine-pruning: De-
fending against backdooring attacks on deep neural networks. In International
Symposium on Research in Attacks, Intrusions, and Defenses. Springer, 273â€“294.
1867
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:52:09 UTC from IEEE Xplore.  Restrictions apply. ReMoS: Reducing Defect Inheritance in T ransfer Learning via Re levant Mo del S licing ICSE â€™22, May 21â€“29, 2022, Pittsburgh, P A, USA
[45] Yingqi Liu, Shiqing Ma, Y ousra Aafer, Wen-Chuan Lee, Juan Zhai, Weihang
Wang, and Xiangyu Zhang. 2018. Trojaning Attack on Neural Networks. In 25th
Annual Network and Distributed System Security Symposium (NDSS). 18â€“221.
[46] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen,
Omer Levy, Mike Lewis, Luke Zettlemoyer, and V eselin Stoyanov. 2019. Roberta:
A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692
(2019).
[47] Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y . Ng,
and Christopher Potts. 2011. Learning Word V ectors for Sentiment Analysis.
InThe 49th Annual Meeting of the Association for Computational Linguistics:
Human Language T echnologies, Proceedings of the Conference, 19-24 June, 2011,
Portland, Oregon, USA, Dekang Lin, Y uji Matsumoto, and Rada Mihalcea (Eds.).
The Association for Computer Linguistics, 142â€“150. https://aclanthology.org/P11-
1015/
[48] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and
Adrian Vladu. 2018. Towards Deep Learning Models Resistant to Adversarial
Attacks. In 6th International Conference on Learning Representations, ICLR 2018,
V ancouver , BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings.
OpenReview.net. https://openreview.net/forum?id=rJzIBfZAb
[49] John Morris, Eli Liï¬‚and, Jin Y ong Y oo, Jake Grigsby, Di Jin, and Y anjun Qi.
2020. TextAttack: A Framework for Adversarial Attacks, Data Augmentation, and
Adversarial Training in NLP . In Proceedings of the 2020 Conference on Empirical
Methods in Natural Language Processing: System Demonstrations. 119â€“126.
[50] Maria-Elena Nilsback and Andrew Zisserman. 2008. Automated Flower Classiï¬-
cation over a Large Number of Classes. In Indian Conference on Computer Vision,
Graphics and Image Processing.
[51] Sinno Jialin Pan and Qiang Y ang. 2009. A survey on transfer learning. IEEE
Transactions on knowledge and data engineering 22, 10 (2009), 1345â€“1359.
[52] Ivan Pashchenko, Henrik Plate, Serena Elisa Ponta, Antonino Sabetta, and Fabio
Massacci. 2018. Vulnerable open source dependencies: Counting those that matter.
InProceedings of the 12th ACM/IEEE International Symposium on Empirical
Software Engineering and Measurement. 1â€“10.
[53] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gre-
gory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga,
et al .2019. Pytorch: An imperative style, high-performance deep learning library.
Advances in neural information processing systems 32 (2019), 8026â€“8037.
[54] Brandon Paulsen, Jingbo Wang, and Chao Wang. 2020. ReluDiff: differential
veriï¬cation of deep neural networks. In ICSE â€™20: 42nd International Conference
on Software Engineering, Seoul, South Korea, 27 June - 19 July, 2020, Gregg
Rothermel and Doo-Hwan Bae (Eds.). ACM, 714â€“726. https://doi.org/10.1145/
3377811.3380337
[55] Kexin Pei, Yinzhi Cao, Junfeng Y ang, and Suman Jana. 2017. Deepxplore:
Automated whitebox testing of deep learning systems. In proceedings of the 26th
Symposium on Operating Systems Principles. 1â€“18.
[56] Adam Poliak, Jason Naradowsky, Aparajita Haldar, Rachel Rudinger, and Ben-
jamin V an Durme. 2018. Hypothesis Only Baselines in Natural Language Infer-
ence. In Proceedings of the Seventh Joint Conference on Lexical and Computa-
tional Semantics, *SEM@NAACL-HLT 2018, New Orleans, Louisiana, USA, June
5-6, 2018, Malvina Nissim, Jonathan Berant, and Alessandro Lenci (Eds.). Asso-
ciation for Computational Linguistics, 180â€“191. https://doi.org/10.18653/v1/s18-
2023
[57] Ariadna Quattoni and Antonio Torralba. 2009. Recognizing indoor scenes. In 2009
IEEE Conference on Computer Vision and Pattern Recognition. IEEE, 413â€“420.
[58] Shahbaz Rezaei and Xin Liu. 2020. A Target-Agnostic Attack on Deep Models:
Exploiting Security Vulnerabilities of Transfer Learning. In 8th International Con-
ference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April
26-30, 2020. OpenReview.net. https://openreview.net/forum?id=BylVcTNtDS
[59] Ali Shafahi, Parsa Saadatpanah, Chen Zhu, Amin Ghiasi, Christoph Studer,David W. Jacobs, and Tom Goldstein. 2020. Adversarially robust transferlearning. In 8th International Conference on Learning Representations, ICLR
2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net. https:
//openreview.net/forum?id=ryebG04YvB
[60] David Shriver, Sebastian Elbaum, and Matthew B. Dwyer. 2021. Reducing DNN
Properties to Enable Falsiï¬cation with Adversarial Attacks. (2021).
[61] Shoaib Ahmed Siddiqui, Andreas Dengel, and Sheraz Ahmed. 2020. Bench-marking Adversarial Attacks and Defenses for Time-Series Data. In Neural In-
formation Processing - 27th International Conference, ICONIP 2020, Bangkok,
Thailand, November 23-27, 2020, Proceedings, Part III (Lecture Notes in Com-
puter Science), Haiqin Y ang, Kitsuchart Pasupa, Andrew Chi-Sing Leung, James T.
Kwok, Jonathan H. Chan, and Irwin King (Eds.), V ol. 12534. Springer, 544â€“554.https://doi.org/10.1007/978-3-030-63836-8_45
[62]
Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning,
Andrew Y Ng, and Christopher Potts. 2013. Recursive deep models for semantic
compositionality over a sentiment treebank. In Proceedings of the 2013 conference
on empirical methods in natural language processing. 1631â€“1642.
[63] Y ongqiang Tian, Zhihua Zeng, Ming Wen, Y epang Liu, Tzu-yang Kuo, and Shing-
Chi Cheung. 2020. EvalDNN: A toolbox for evaluating deep neural networkmodels. In 2020 IEEE/ACM 42nd International Conference on Software Engineer-
ing: Companion Proceedings (ICSE-Companion). IEEE, 45â€“48.
[64] Bolun Wang, Y uanshun Y ao, Bimal Viswanath, Haitao Zheng, and Ben Y . Zhao.
2018. With Great Training Comes Great Vulnerability: Practical Attacks against
Transfer Learning. In USENIX Security Symposium. USENIX Association, 1281â€“
1297.
[65] Jingyi Wang, Jialuo Chen, Y oucheng Sun, Xingjun Ma, Dongxia Wang, Jun Sun,
and Peng Cheng. 2021. RobOT: Robustness-Oriented Testing for Deep Learning
Systems. arXiv preprint arXiv:2102.05913 (2021).
[66] Shuo Wang, Surya Nepal, Carsten Rudolph, Marthie Grobler, Shangyu Chen, and
Tianle Chen. 2020. Backdoor Attacks against Transfer Learning with Pre-trained
Deep Learning Models. arXiv preprint arXiv:2001.03274 (2020).
[67] P . Welinder, S. Branson, T. Mita, C. Wah, F. Schroff, S. Belongie, and P . Perona.
2010. Caltech-UCSD Birds 200. Technical Report CNS-TR-2010-001. California
Institute of Technology.
[68] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue,
Anthony Moi, Pierric Cistac, Tim Rault, RÃ©mi Louf, Morgan Funtowicz, Joe Davi-
son, Sam Shleifer, Patrick von Platen, Clara Ma, Y acine Jernite, Julien Plu, Canwen
Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexan-
der M. Rush. 2020. Transformers: State-of-the-Art Natural Language Processing.
InProceedings of the 2020 Conference on Empirical Methods in Natural Language
Processing: System Demonstrations. Association for Computational Linguistics,
Online, 38â€“45. https://www.aclweb.org/anthology/2020.emnlp-demos.6
[69] Bangpeng Y ao, Xiaoye Jiang, Aditya Khosla, Andy Lai Lin, Leonidas Guibas,and Li Fei-Fei. 2011. Human action recognition by learning bases of action
attributes and parts. In 2011 International conference on computer vision. IEEE,
1331â€“1338.
[70] Y uanshun Y ao, Huiying Li, Haitao Zheng, and Ben Y . Zhao. 2019. Latent Back-
door Attacks on Deep Neural Networks. In Proceedings of the 2019 ACM SIGSAC
Conference on Computer and Communications Security, CCS 2019, London, UK,
November 11-15, 2019, Lorenzo Cavallaro, Johannes Kinder, XiaoFeng Wang,
and Jonathan Katz (Eds.). ACM, 2041â€“2055. https://doi.org/10.1145/3319535.
3354209
[71] Fuyuan Zhang, Sankalan Pal Chowdhury, and Maria Christakis. 2020. Deepsearch:
A simple and effective blackbox attack for deep neural networks. In Proceedings
of the 28th ACM Joint Meeting on European Software Engineering Conference
and Symposium on the F oundations of Software Engineering. 800â€“812.
[72] Xiyue Zhang, Xiaofei Xie, Lei Ma, Xiaoning Du, Qiang Hu, Y ang Liu, Jianjun
Zhao, and Meng Sun. 2020. Towards characterizing adversarial defects of deep
learning software from the lens of uncertainty. arXiv preprint arXiv:2004.11573
(2020).
[73] Ziqi Zhang, Y uanchun Li, Y ao Guo, Xiangqun Chen, and Y unxin Liu. 2020.
Dynamic slicing for deep neural networks. In Proceedings of the 28th ACM Joint
Meeting on European Software Engineering Conference and Symposium on the
F oundations of Software Engineering. 838â€“850.
1868
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:52:09 UTC from IEEE Xplore.  Restrictions apply. 