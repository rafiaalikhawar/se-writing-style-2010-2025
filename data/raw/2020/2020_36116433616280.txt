How Practitioners Expect Code Completion?
Chaozheng Wang
The Chinese University of Hong Kong
Hong Kong, China
adf111178@gmail.comJunhao Hu
Peking University
Beijing, China
junhaohu@stu.pku.edu.cnCuiyun Gao∗
The Chinese University of Hong Kong
Hong Kong, China
cuiyungao@outlook.com
Yu Jin
Tencent Inc.
Guangzhou, China
lenajin@tencent.comTao Xie
Key Lab of HCST (PKU), MOE; SCS,
Peking University
Beijing, China
taoxie@pku.edu.cnHailiang Huang
Tencent Inc.
Guangzhou, China
eraserhuang@tencent.com
Zhenyu Lei
Tencent Inc.
Guangzhou, China
rainylei@tencent.comYuetang Deng
Tencent Inc.
Guangzhou, China
yuetangdeng@tencent.com
ABSTRACT
Code completion has become a common practice for programmers
during their daily programming activities. It automatically predicts
the next tokens or statements that the programmers may use. Code
completion aims to substantially save keystrokes and improve the
programming efficiency for programmers. Although there exists
substantial research on code completion, it is still unclear what prac-
titioner expectations are on code completion and whether these
expectations are met by the existing research. To address these
questions, we perform a study by first interviewing 15 profession-
als and then surveying 599 practitioners from 18 IT companies
about their expectations on code completion. We then compare the
practitioner expectations with the existing research by conducting
a literature review of papers on code completion published in major
publication venues from 2012 to 2022. Based on the comparison,
we highlight the directions desirable for researchers to invest ef-
forts toward developing code completion techniques for meeting
practitioner expectations.
CCS CONCEPTS
•General and reference →Empirical studies ;•Computing
methodologies→Artificial intelligence .
KEYWORDS
Code completion, empirical study, practitioner expectations
∗Corresponding author. The author is also affiliated with Peng Cheng Laboratory and
Guangdong Provincial Key Laboratory of Novel Security Intelligence Technologies.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
ESEC/FSE ’23, December 3–9, 2023, San Francisco, CA, USA
©2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-0327-0/23/12. . . $15.00
https://doi.org/10.1145/3611643.3616280ACM Reference Format:
Chaozheng Wang, Junhao Hu, Cuiyun Gao, Yu Jin, Tao Xie, Hailiang Huang,
Zhenyu Lei, and Yuetang Deng. 2023. How Practitioners Expect Code Com-
pletion?. In Proceedings of the 31st ACM Joint European Software Engineer-
ing Conference and Symposium on the Foundations of Software Engineering
(ESEC/FSE ’23), December 3–9, 2023, San Francisco, CA, USA. ACM, New York,
NY, USA, 13 pages. https://doi.org/10.1145/3611643.3616280
1 INTRODUCTION
Code completion predicts code tokens or statements that program-
mers may input in their daily programming activities. It aims to
substantially save keystrokes and improve coding efficiency for
programmers. As reported by Li et al. [ 35], with code completion,
programmers can averagely save 3.65 keystrokes for completing
a token. Code completion has become the most frequently used
feature of modern integrated development environments (IDEs)
[10], and arouse increasingly high attention from both academia
and industry [1, 3, 7, 21, 30, 33, 37, 60–62, 67].
The research of code completion has roughly progressed through
three stages over time. First, traditional techniques of code com-
pletion [ 2,22,43] adopt static analysis and ineffectively provide
completion candidates according to pre-defined rules, which gener-
ally require non-trivial manual efforts. Second, machine learning
(ML) based techniques [ 25,45,52] such as N-gram models are pro-
posed to learn code statistics to improve completion accuracy. With
the development of deep learning (DL), various DL-based tech-
niques [ 47,62] have been proposed to implicitly learn patterns in
source code and achieve state-of-the-art performance. Third, some
DL-based techniques [ 17,30,33,60] employ pre-trained language
models (PLMs) such as BERT and GPT [ 15,20,53] for code com-
pletion, because PLMs are trained on large-scale corpora and can
encode a large amount of code-related knowledge into their pa-
rameters. Besides accurately predicting next tokens, PLM-based
techniques can also perform well in predicting code statements.
To investigate what practitioner expectations are on code com-
pletion and whether these expectations are met by the existing re-
search, in this paper, we follow a mixed-method approach [ 27,34],
which includes three stages. First, we start with semi-structured
1294
ESEC/FSE ’23, December 3–9, 2023, San Francisco, CA, USA Chaozheng Wang, Junhao Hu, Cuiyun Gao, Yu Jin, Tao Xie, Hailiang Huang, Zhenyu Lei, and Yuetang Deng
interviews with 15 professionals with an average of 8.4 years of
working experience in software industry. Through the interviews,
we qualitatively investigate the state of code completion practices,
issues faced by our interviewees when using code completion tools,
and their expectations on code completion. Second, we then per-
form an exploratory survey with 599 practitioners from 18 IT com-
panies to quantitatively validate our findings from the first stage.
Third, we conduct a literature review of research papers published
in major venues from 2012 to 2022 and compare the techniques
proposed in the papers against the practitioner expectations.
Specifically, we investigate the following four research questions:
RQ1: What is the state of code completion practices?
This research question studies code completion practices, in-
cluding code completion tools and usage scenarios (the scenarios
that practitioners use code completion). For completion tools, 72%
participants indicate that they often use code completion in daily
programming. Meanwhile, 96% participants indicate that built-in
tools in IDEs (e.g., tools built in Pycharm [ 6]) are the most perva-
sively used code completion tools. For usage scenarios, to facilitate
the investigation, we broadly divide the scenarios into two cate-
gories according to the completion granularity, including the token
level (e.g., completing a single identifier) and statement level (e.g.,
predicting code blocks). On average, 81% participants express that
they use token-level completion, whereas only 32% participants
express that they use statement-level completion.
RQ2: How important is code completion, and what are the
issues faced by practitioners when they use code completion?
In this research question, we investigate how practitioners per-
ceive the importance of code completion, and study the issues
faced by practitioners when they use code completion during pro-
gramming. For the importance perceived by the practitioners, 87%
participants agree or strongly agree with the importance of code
completion during programming. For the issues of code completion,
only 36% participants report no issues. More than half of the partic-
ipants consider erroneous completion andpainful tool installation as
main issues.
RQ3: What are practitioner expectations on code comple-
tion?
This research question focuses on practitioner expectations on
different granularity levels of code completion techniques, includ-
ing token-level and statement-level completion. We investigate
the expectations from multiple aspects, including usage scenarios ,
evaluation metrics ,access to service (online or offline), completion
effectiveness , and efficiency . On average, 88% and 57% participants
expect to use completion under token-level and statement-level
scenarios, respectively. For token-level completion , 85% partici-
pants expect that the results should appear fast. More than 80% of
participants expect the tools to support the completion of identi-
fiers, APIs, and paths, respectively. Most participants are satisfied
if a suggestion is among the first three candidates. For statement-
level completion , more than 70% of participants expect a tool
to complete their current editing statement, code skeleton, and
API arguments. Completion results are expected to appear in less
than two seconds. In addition, completion tools are preferred to be
locally deployed and installed easily.
RQ4: How well does existing research satisfy the practitioner
expectations?This research question investigates current code completion re-
search. We identify 34 papers about code completion techniques,
among which 26 and 12 papers are about token-level and statement-
level completion, respectively (where four papers [ 28,30,41,48]
can conduct both). Based on a literature review, we conclude the
overlapping and gap between techniques proposed by the existing
research and the practitioner expectations. For instance, most of
the proposed token-level techniques focus on API recommenda-
tionandidentifier completion scenarios, being consistent with the
practitioner expectations. However, no papers have covered the
path completion scenario, a scenario valued by 86% participants. In
addition, we obtain some findings about the completion efficiency,
evaluation metrics, and access to service.
Our research aims to provide future research directions on code
completion and help researchers consider the expectations of prac-
titioners when studying code completion techniques.
In summary, we make the following main contributions:
•We shed light on practitioner expectations on code comple-
tion techniques. We first interview 15 professionals and then
survey 599 practitioners from 18 IT companies. We investi-
gate their opinions on code completion, the scenarios that
they use for code completion, and their expectations on code
completion techniques.
•We comprehensively review papers on code completion pub-
lished in major publication venues in software engineering
and artificial intelligence fields from 2012 to 2022. Then we
compare the techniques proposed by the published papers
with the practitioner expectations, highlighting the aspects
desirable to be improved to meet the practitioner expecta-
tions.
2 RESEARCH METHODOLOGY
In this section, we introduce our overall research methodology
consisting of three main stages1as shown in Figure 1. Stage 1.
Interviewing 15 professionals about their practices on code comple-
tion, the issues that they have met, and their expectations on code
completion tools. Stage 2. Performing an online survey to validate
our findings on the practitioner expectations produced by Stage 1.
Stage 3. Conducting a literature review to analyze whether and
to what extent existing techniques have satisfied the practitioner
expectations.
2.1 Stage 1: Interviews
The interview stage serves as a preliminary means to identify the
factors related to practitioner expectations on code completion.
In this section, we introduce the interviews from three aspects
including the protocol, interviewees, and data analysis, respectively.
2.1.1 Protocol. Two authors of this paper conduct a series of in-
person, semi-structured, and in-depth interviews based on an in-
terview guide to explore the participants’ practices, issues, and
expectations on code completion. We continue our interviewing
until no new aspects show up. In the end, we invite 15 software
1The interviews and survey in Stages 1 and 2 have been approved by the relevant
institutional review board (IRB).
1295How Practitioners Expect Code Completion? ESEC/FSE ’23, December 3–9, 2023, San Francisco, CA, USA
Stage 1: Interviews
Semi-structured 
interview
Interview 
transcription
Open card sortingStage 2: Online Survey
Questionnaire
Pilot survey
Online survey
AnalysisStage 3: Literature Review
Paper collection
Analyze the capabilities 
of proposed techniques
RQsRQ1. 
PracticesRQ2. 
Importance and 
issuesRQ3. Practitioner 
expectationsRQ4. Gap analysis 
between research 
and expectationsQuestions and descriptions 
on code completion
Figure 1: Overview of research methodology.
professionals in the interviews and each interview lasts 45-60 min-
utes.
In each interview, we first ask the interviewees some demo-
graphic questions about their background including job roles and
working experience. Then, we ask the interviewees to freely talk
about what they regard as a desirable code completion tool and
their expectations of code completion. In the end, we investigate
their code completion practices and the issues that they have ever
met.
2.1.2 Interviewees. We invite interviewees from the IT industry.
The interviewees are working full-time in different roles such as
developers and testers. Eventually, 15 interviewees from seven IT
companies agree to participate in the interviews, among which
12 work in software development, 1 for software testing, and 2
for artificial intelligence research. The working experience of our
interviewees varies from 5 years to 13 years, with the average
working experience of 8.4 years.
2.1.3 Data Analysis. To analyze the interview results, following
the procedure in previous work [ 27,34], the first author of this pa-
per transcribes the interviews and uses NVivo [ 5] to conduct open
coding and generate opinion cards. Then another author verifies
the summarized opinion cards and provides suggestions for im-
provement, e.g., removing the opinion cards that make too general
comments on code completion. After incorporating the suggestions,
the two authors of this paper separately analyze and sort the opin-
ion cards into groups and label each group with a potential survey
description. To evaluate the level of agreement, we calculate Co-
hen’s Kappa coefficient (i.e., 0.67) of sorting cards between the two
authors. The result indicates a substantial agreement between them.
The two authors discuss their disagreements to reach a common
decision. To reduce the bias of the two authors in sorting cards, two
other authors of this paper have also reviewed and confirmed the
final set of survey descriptions and questions.
After the interviews, we derive four code completion practices
and six issues. We summarize three and six usage scenarios for the
practitioner expectations on token-level and statement-level code
completion, respectively. Furthermore, we derive 8 and 13 factors
that specifically affect the adoption of token-level and statement-
level completion, respectively, and draw three other general factors
of code completion tools.
Figure 2: Example for illustrating the API argument recom-
mendation scenario in the survey.
2.2 Stage 2: Online Survey
Based on the findings gathered during the interview process, we
conduct a large-scale online survey to further investigate and vali-
date the factors of code completion.
2.2.1 Survey Design. The survey consists of different types of
questions including single/multiple-choice, Likert scale (Options:
Strongly Disagree, Disagree, Neutral, Agree, Strongly Agree), and
short answer questions. For each question, we add an “I don’t un-
derstand" choice to filter the cases in which the participants do not
understand our descriptions.
The survey consists of six parts:
•Demographics. We first ask for demographic information
of the participants, including primary job roles and working
experience.
•Code completion tools. We ask the participants about the
factors of code completion tools. In particular, we investigate
the code completion tools they have ever used and their
practices on using code completion tools.
•Code completion usage scenarios. We investigate in what
scenarios practitioners use code completion. We summarize
three scenarios for token-level completion and six scenarios
for statement-level completion. For each scenario, we utilize
a screenshot or GIF for illustration, with an example shown
in Figure 2. The three token-level completion scenarios in-
clude:
(1)Identifier completion : completing the currently tying iden-
tifier such as variable names.
(2)API recommendation : recommending the API or function
to be used such as recommending “ read_csv " when typing
“pd." in Python.
(3)Path completion : completing the path of a referred file or
directory.
The studied statement-level scenarios include:
(1)Next statement prediction : completing the next statement
when finishing the last code statement.
(2)Completion of currently edited statement : completing the
currently edited code statement until the end of the state-
ment. For instance, predict “ in range(n): " when practition-
ers input “ for index " in Python.
(3)API argument recommendation : recommending the argu-
ments to be used in the called function or API as shown
in Figure 2.
(4)String completion : completing the currently typing string
such as logs.
(5)Skeleton prediction : predicting the skeleton of the code
statements to be used. The scenario requires code com-
pletion techniques only to predict functional code and
1296ESEC/FSE ’23, December 3–9, 2023, San Francisco, CA, USA Chaozheng Wang, Junhao Hu, Cuiyun Gao, Yu Jin, Tao Xie, Hailiang Huang, Zhenyu Lei, and Yuetang Deng
Table 1: Participants roles & working experience
Role Population <1y 1-3y 3-5y 5-10y >10y
Development 345 32 102 121 65 15
AI Model Design 173 10 46 81 42 4
Testing 27 6 8 4 6 3
Architect 4 0 1 2 1 0
Project Manager 2 0 0 0 1 1
Others 48 9 15 17 7 0
599 57 172 225 122 23
leave the detailed values (e.g., string and constant) for
practitioners to fill in themselves.
(6)Block content prediction : predicting the statements of a
code block such as function and loop block.
•Importance. We investigate how practitioners perceive the
importance of code completion in this part.
•Code completion issues. We investigate the issues faced
by the participants during using the code completion tools,
such as erroneous completion ,high completion latency ,high
resource consumption , and concern about data leak .
•Practitioner expectations. We study practitioner expecta-
tions on code completion tools of the two granularity levels,
i.e., token-level and statement-level completion. For each
level, we ask multiple aspects of the expectations including
usage scenarios ,evaluation metrics ,access to service (intranet,
internet, or locally deployed), completion effectiveness and
efficiency . Besides, we also investigate general aspects that
affect practitioner likelihood to adopt code completion tools
such as time consumption of tool installation .
At the end of the survey, we allow our participants to choose
to freely provide their comments, advice, and opinions about code
completion and our survey.
To check participants’ perceptions about the survey length and
clarity of questions, we conduct a pilot survey with a small set of
practitioners that are different from our interviewees and survey
participants before large-scale distribution. After receiving feed-
back from the pilot survey, we make minor modifications to the
survey, such as reducing the number of questions and incorporat-
ing additional images to illustrate usage scenarios, resulting in the
creation of a final version. We utilize widely-used questionnaire
websites [ 4,8] to distribute the survey and provide each participant
with the opportunity to join a drawing for winning a $100 shopping
card.
2.2.2 Participant Recruitment. We contact professionals working
full-time in IT companies in our social networks, and ask for their
help complete and disperse the survey. In particular, we send invita-
tions to professionals working in Microsoft, Intel, Tencent, Alibaba,
and other IT companies. We finally receive 611 survey responses
in total with an average completion time of 8.2 minutes. Among
them, we discard 12 responses that are completed within two min-
utes. The survey results presented in this paper are analyzed from
the remaining 599 valid responses. An overview of the surveyed
participants’ roles and their experience is depicted in Table 1. Most
participants are engaged in software development and have 3-5
years of programming experience.2.2.3 Result Analysis. We analyze the results based on the question
types. For multiple-choice and single-choice questions, we report
the percentage of each selected option. For Likert-scale questions,
we draw bar charts to illustrate the distributions of the Likert scores.
For the open-ended short answer questions, we conduct a qualita-
tive analysis of the results. Besides, we drop “I don’t understand”
ratings that form a small minority (less than 1%) of all the received
ratings following previous work [27].
2.3 Stage 3: Literature Review
For this survey, we carefully collect 34 papers from relevant inter-
national journals and conferences in three steps. First, we search
the dblp publication database2using the keyword: "code comple"
and setting the default search action as combined dblp search. Since
the dblp database uses the partial keyword matching strategy for
searching, the keyword "code comple" could stand for "code com-
plete", "code completion", "complete code", "code autocompletion"
and so on, which we believe is comprehensive enough to cover a
large part of the code completion papers. The reason for choos-
ing the DLBP database is that our literature review focuses on the
paper published in major publication venues of software engineer-
ing, artificial intelligence, and human-computer interaction fields.
These venues have already been collected in the DBLP database,
thus we do not involve the more conventional databases. Second,
we manually filter the results by removing irrelevant papers by
reading the title and abstract. Finally, we retrieve additional papers
by following references in the already found papers.
In addition, when we are collecting papers, we further filter the
results in the following two ways. First, considering the timelines,
we only collect papers published from 2012 to 2022 since our goal
is to dig out future research directions that meet the practitioner
expectations. Second, we only keep those papers that are published
in major publication conferences and journals because we believe
those papers in popular conferences and journals could represent
the current state of code completion research. The following is a
list of the conferences and journals we select: ICSE, ESEC/FSE, ASE,
ICPC, SANER, MSR, KBSE, ICSME, PLDI, OOSPLA, TSE, TOSEM,
EMSE, JSS, IST, ACL, EMNLP, NAACL, IJCAI, ICLR, NeurIPS, and
AAAI.
After the paper collection stage, we start to analyze each paper.
Two authors of this paper read the content and analyze the capabil-
ities of the proposed technique in terms of completion granularity,
scenarios, evaluation metrics, efficiency, and access to the code
completion service. For example, Izadi et al. [ 30] declare that they
utilize multi-task learning to train the model to predict the next
token and next statement. Thus, we decide that this paper works
on both token-level and statement-level completion. Svyatkovskiy
et al. [ 62] claim to provide web completion services and a client
completion model. Thus, we decide that access to the service could
be both online and local. Two authors discuss the disagreements
in the capability analysis of each paper and reach an agreement
through further paper reading.
2https://dblp.org/
1297How Practitioners Expect Code Completion? ESEC/FSE ’23, December 3–9, 2023, San Francisco, CA, USA
0.0 0.2 0.4 0.6 0.8 1.0OthersAiXcoderKiteT abnineCopilotIntelliCodeCompiler-based toolIDE built-in tool
(a) Statistics of code completion tools that the practitioners have used.
0% 10% 20% 30% 10% 20% 30% 40% 50% 60% 70% 80%
Percentage of Valid ResponsesI am eager for a better 
code completion tool [P3]I find programming laborious 
without code completion tools [P2]I often use code completion tools [P1]
Strongly Disagree Disagree Neutral Agree Strongly Agree
(b) Usage status of code completion tools.
Figure 3: Code completion tools.
3 RESULT ANALYSIS
In this section, we illustrate and analyze the results obtained in our
online survey. For the perspectives reflected in the interview stage,
the quoted statements are from interviewees.
3.1 RQ1: Code Completion Practices
In this research question, we investigate the practitioners’ code
completion practices, including the used code completion tools and
usage scenarios. The participants’ rating results for some descrip-
tions related to code completion practices are illustrated in Figure
3 and 4.
3.1.1 Code Completion Tools. We first ask the participants what
code completion tools they have used and the results are shown in
Figure 3 (a). From the results, we find that 96% participants express
they adopt IDE built-in code completion tools. Compiler-based
tools such as CCLS andClangd [2] are the second most popular
tools among surveyed practitioners. However, the third-party plug-
in tools are not prevalent, e.g., about 13% participants have used
Copilot [3] and IntelliCode [60].
We then investigate the practitioners’ usage status of code com-
pletion tools, with results illustrated in Figure 3 (b). Although most
practitioners (72%) indicate that they often adopt code completion
in daily programming, only 50% of participants agree that they
find programming laborious without code completion. Besides, 54%
participants are proactive in seeking a better code completion tool.
“I always keep tuned on the release of new code completion tools and
am eager to be the first to use them. ", as an interviewee stated.
Finding 1: The most commonly-used code completion tool is IDE
built-in tool. Other third-party plug-in tools such as IntelliCode
[61] and Copilot [ 3] are far less popular, for which only about
13% participants have used. In addition, most participants express
that they often use code completion tools, and 54% participants
are eager for a better code completion tool.
3.1.2 Usage Scenarios. In this section, we investigate in what sce-
narios the practitioners use code completion. According to [ 42], we
study the scenarios from two granularity levels including token-
level andstatement-level completion, respectively. Token-level com-
pletion aims to complete the currently typing token and the next
0% 10%20%30%40%50%60%70% 10%20%30%40%50%60%70%80%90%
Percentage of Valid ResponsesBlock content prediction [C9]Skeleton prediction [C8]String completion [C7]API argument recommendation [C6]Completion of currently edited statement [C5]Next statement prediction [C4]Path completion [C3]API recommendation [C2]Identifier completion [C1]Token Statement
Strongly Disagree Disagree Neutral Agree Strongly AgreeFigure 4: Scenarios that the practitioners use code comple-
tion.
token to be used for programmers (i.e., identifiers and methods).
Statement-level completion is able to predict multiple tokens and
code statements. We provide the definitions of the code completion
of two granularity levels in the survey to facilitate participants’
understanding of our descriptions and questions. The token-level
completion and statement-level completion involve three and six
scenarios, respectively, as introduced in Section 2.2. The partici-
pants’ ratings of the usage scenarios are shown in Figure 4. We
observe that the overall popularity of statement-level completion
scenarios is obviously less than that of token-level (32% v.s. 81% on
average).
Token-level completion. As can be seen in the upper part of
Figure 4, all three usage scenarios are popular among practitioners.
Specifically, about 85% participants use code completion to complete
identifiers and recommend APIs. More than 70% of the surveyed
practitioners adopt code completion for path completion in their
daily programming.
Finding 2: All three token-level code completion usage scenar-
ios are popular among the participants, among which identifier
completion andAPI recommendation are the most widely used
scenarios.
Statement-level completion. As shown in the lower part of Fig-
ure 4, the three most popular statement-level completion scenarios
areAPI argument recommendation ,completion of currently edited
line, and skeleton prediction , used by 46%, 45%, and 36% participants,
respectively. In addition, about 25% of participants adopt code com-
pletion to predict the next statement of code and string content.
Only 16% of survey participants adopt code completion in the block
content prediction scenario. This can be attributed to that 1) The
tools that support block content prediction such as Copilot are not
pervasively used, and 2) existing tools are not intelligent enough to
perfectly predict the whole block content. One interviewee said that
“The performance of existing completion tools is not good enough to
predict the whole block content. The predicted results may be severely
misleading in some cases, which wastes my time. "
Finding 3: The most frequently adopted statement-level code
completion scenarios are API argument recommendation ,comple-
tion of currently edited line , and skeleton prediction .
1298ESEC/FSE ’23, December 3–9, 2023, San Francisco, CA, USA Chaozheng Wang, Junhao Hu, Cuiyun Gao, Yu Jin, Tao Xie, Hailiang Huang, Zhenyu Lei, and Yuetang Deng
0%10% 20% 30% 40% 50% 60% 70% 80% 90%
Percentage of Valid ResponsesCode completion improves programming experience [M5]Code completion can reduce my typing mistakes [M4]Code completion can provide some hint [M3]Code completion can save my key strokes [M2]Code completion can improve my efficiency [M1]I think code completion is important
Strongly Disagree Disagree Neutral Agree Strongly Agree
Figure 5: Importance of code completion.
0% 10% 20% 30% 40% 50% 60% 10% 20% 30% 40% 50%
Percentage of Valid ResponsesPainful tool installation [I6]Erroneous completion [I5]Concern about data leakage [I4]High resource consumption [I3]High completion latency [I2]Unsatisfying ranking [I1]The tools are satisfying for me 
Strongly Disagree Disagree Neutral Agree Strongly Agree
Figure 6: Issues of code completion.
3.2 RQ2: Code Completion Importance and
Issues
3.2.1 Importance of Code Completion. We ask whether they think
code completion is important for programming, and their agree-
ment on the possible reasons for the importance. The results are
illustrated in Figure 5.
From the results, we can find that most participants (88%) agree
that code completion plays an important role. 90% participants
agree or strongly agree that code completion can provide some
hints on the implementing code. One interviewee described that
“Sometimes I forget how to type a long identifier or method name. In
this case, code completion can show me the candidates and provide
some hints for completing the code. ”. Besides, more than 85% of
participants think completion tools are helpful to reduce typing
mistakes, improve programming efficiency, save keystrokes, and
improve programming experience.
Finding 4: 88% participants agree that code completion tools
play an important role in programming. Most participants (90%,
87%, and 86%) think the tools can provide some hint, improve
programming efficiency, and improve their programming experi-
ence.
3.2.2 Code Completion Issues. Figure 6 illustrates the participants’
ratings of general code completion-related issues they faced dur-
ing programming, and thus we do not split them into token and
statement levels. Despite that code completion is an important
aspect of software development, only 36% of the participants con-
sider their current tools to be satisfactory. 45% and 44% participants
consider erroneous completion andpainful tool installation as the
main issues, respectively. Erroneous completion indicates that some
errors that practitioners need to modify occur in the completed
code. As an interviewer said: “ The completion is usually not exactly
what I need, and it is annoying for me to rectify the errors. For in-
stance, Clangd may complete the wrong function names if header
files or namespaces exist with identical names. ". Besides, 34% and
31% participants are not satisfied with the rankings of appropriate
tokens in the candidates and are concerned about confidential code
0% 10% 10% 20% 30% 40% 50% 60% 70% 80% 90%Path completion [T3]API recommendation [T2]Identifier completion [T1](a) Expectations on usage scenarios.
0% 10% 10% 20% 30% 40% 50% 60% 70% 80%Recall [T6]Average Rank [T5]Accuracy [T4]
(b) Expectations on evaluation metrics.
0% 10% 20% 30% 10% 20% 30% 40% 50% 60% 70% 80%Offline [T9]Internal Network [T8]Public Network [T7]
Strongly Disagree Disagree Neutral Agree Strongly Agree
(c) Expectations on access to completion service.
0% 10% 20% 30% 40% 50% 60% 70% 80% 90% 100%Rank of the needed token in 
completion candidates [T10]
1 2 3 4 5+
(d) Expectations on code completion effectiveness.
0% 10% 20% 30% 40% 50% 60% 70% 80% 90% 100%Maximum completion latency [T11]
100ms 200ms 300ms 400ms 600ms 800ms 1000ms
(e) Expectations on code completion efficiency.
Figure 7: Expectations on token-level code completion, where
horizontal axis denotes percentage of valid responses.
leakage, respectively. Among the participants, about a quarter of
them hold the view that the high resource (i.e., memory and CPU)
consumption and high completion latency are not acceptable. An
interviewee working on large C++ projects told us: “ The C++ code
completion tools like CCLS and Clangd parse the opened cpp files and
build indexes for them when programming, which may consume all
of my CPU and memory. ”.
Finding 5: The most concerned issues by practitioners are the
erroneous code completion ,painful tool installation , and unsatis-
factory ranking .
3.3 RQ3: Practitioners Expectation
In this research question, we comprehensively investigate the prac-
titioners’ expectations on code completion. Following the previous
work [ 27], the term “expectation” refers to the anticipated outcome
of specific aspects including usage scenarios ,evaluation metrics ,
access to service , completion effectiveness , and efficiency . Specifi-
cally, for usage scenarios , the “expectation” refers to the anticipated
completion that code completion techniques offer to practitioners
during the programming. For evaluation metrics , the “expectation”
indicates the anticipated metrics that gauge the efficacy of a code
completion technique. In terms of access to service , the term “ex-
pectation” pertains to how practitioners anticipate utilizing code
completion techniques. For effectiveness , practitioners’ “expecta-
tions” refer to that to what extent a code completion technique
can improve practitioners’ productivity by reducing the time and
1299How Practitioners Expect Code Completion? ESEC/FSE ’23, December 3–9, 2023, San Francisco, CA, USA
effort required to write source code. For efficiency , the “expecta-
tion” denotes the expected time that practitioners need to wait for
completion results.
We explore the above aspects and report practitioners’ expec-
tations on the two levels of granularity of code completion, i.e.,
token-level and statement-level completion. We also investigate
practitioners’ expectations on three other aspects of general com-
pletion tools that impact their usage experience, including time
consumption of installation ,personalized completion andadditional
information display .
3.3.1 RQ 3.1 Expectations on Token-Level Completion. The partici-
pant ratings for the expectations on token-level code completion
are shown in Figure 7.
Usage scenarios. We present the practitioners’ expectations on
usage scenarios of token-level completion in Figure 7 (a). Different
from the popularity studied in RQ1, this RQ explores whether prac-
titioners desire to use code completion in the scenarios. From the
figure we can find that, most participants show positive attitudes
towards the three scenarios. More than 88% of participants expect
to use code completion in Identifier completion andAPI recommen-
dation scenarios. Besides, 86% participants expect code completion
tools to complete the path they are typing. In one interviewee’s
opinion, path completion is very important when he works on a
large project, because he/she finds it very easy to make mistakes
on file names without path completion .
Finding 6: For token-level code completion, more than 85% of
participants agree that tools are supposed to support identifier
completion ,API recommendation andpath completion .
Evaluation metrics. We study participants’ expectations on eval-
uating token-level code completion techniques in Figure 7 (b). We
observe that Accuracy gains the most support rate at 79%. Besides,
more than 70% of participants care about Average rank of the needed
token in the candidates (e.g., Mean Reciprocal Rank MRR) and Re-
call.
Access to service . Figure 7 (c) shows that 82% participants expect
to use token-level completion tools locally. One participant stated
that: “ Locally deployed tools are the most stable ones, which will
not be affected by network status and I can use them anywhere. ". In
contrast, the support rate of online tools is relatively low. 62% and
50% participants are willing to use the online tools in the internal
network and in the public network, respectively.
Effectiveness. Figure 7 (d) shows the practitioner satisfaction ratio
against different ranks of needed tokens in completion candidates.
From our survey, if the code completion tool can rank the needed
token in the top 3 candidates, it will satisfy about 80% practitioners.
Efficiency. We show the participants seven GIFs with different
completion latency including 100, 200, 300, 400, 600, 800, and 1000
milliseconds. From Figure 7 (e), we find that if a token-level code
completion tool can give completion candidates within 200 millisec-
onds, it can satisfy 85% practitioners.
3.3.2 RQ 3.2 Expectations on Statement-Level Completion. The re-
sults of participants’ expectations on statement-level completion
can be accessed in Figure 8.
Usage scenarios. The practitioners’ expectations on the statement-
level code completion scenarios are illustrated in Figure 8 (a). We
0% 10% 20% 30% 40% 10% 20% 30% 40% 50% 60% 70% 80%Block content prediction [S6]Skeleton prediction [S5]String completion [S4]API argument recommendation [S3]Completion of currently edited statement [S2]Next statement prediction [S1](a) Expectations on usage scenarios.
0% 10% 20% 10% 20% 30% 40% 50% 60% 70% 80% 90%Grammatical correctness [S11]Readability [S10]Function similarity [S9]Edit similarity [S8]Overlapped N-grams [S7]
(b) Expectations on evaluation metrics.
0% 10% 20% 30% 10% 20% 30% 40% 50% 60% 70%Offline [S14]Internal Network [S13]Public Network [S12]
Strongly Disagree Disagree Neutral Agree Strongly Agree
(c) Expectations on access to completion service.
0% 10% 20% 30% 40% 50% 60% 70% 80% 90% 100%Minimum grammatical correctness rate [S16]Maximum manual modification rate [S15]
20 40 60 80 100
(d) Expectations on code completion effectiveness.
0% 10% 20% 30% 40% 50% 10% 20% 30% 40%Which one is more important? 
Effectiveness or Generated Code Quantity [S17]
Effectivess is much more important
Effectivess is more important
NeutralGenerated code quantity is more important
Generated code quantity is much  more important
(e) Effectiveness v.s. Generated code quantity.
0% 10% 20% 30% 40% 50% 60% 70% 80% 90% 100%Average time consumption of 
completing a line of code [S18]
<0.5s 0.5~1s 1~1.5s 1.5~2s >2s
(f) Expectations on average time consumption of generating a line of code.
0% 10% 20% 30% 40% 50% 60% 70% 80% 90% 100%Maximum time consumption of 
giving completion results [S19]
<0.5s 0.5~2s 2~5s 5~10s >10s
(g) Expectations on maximum time consumption of giving completion results.
Figure 8: Expectations on statement-level code completion,
where the horizontal axis denotes percentage of valid re-
sponses.
observe that practitioners’ expectations show a similar trend with
the popularity of usage scenarios as shown in Section 3.1. Like-
wise, The three most expected scenarios are skeleton prediction ,
completion of currently edited line andAPI argument recommenda-
tion, where all of them gain at least 60% support rate, and skeleton
prediction is the most expected. Besides, 51% participants agree
that statement-level completion tools are supposed to predict block
content such as method block and loop block. 53% participants
expect to use code completion tools to predict strings. Interestingly,
though the scenario of predicting the next statement of code is kind
of similar to completing the currently edited line, only 43% partic-
ipants regard next statement prediction as an important scenario
while the currently edited line completion obtaining a 62% support
rate. As one interviewee commented: “ When finishing a line of code,
I will start to type the next statement immediately instead of waiting
for completion results at the end of the line. ".
1300ESEC/FSE ’23, December 3–9, 2023, San Francisco, CA, USA Chaozheng Wang, Junhao Hu, Cuiyun Gao, Yu Jin, Tao Xie, Hailiang Huang, Zhenyu Lei, and Yuetang Deng
Finding 7: Skeleton prediction ,completion of currently edited line
andAPI argument recommendation are considered to be the most
expected statement-level code completion scenarios, which ap-
proximately aligns the popularity of the usage scenarios.
Evaluation metrics. We illustrate the practitioners’ expectations
on different evaluation metrics for statement-level code completion
techniques in Figure 8 (b). The top three preferred evaluation met-
rics are grammatical correctness ,readability andfunctional/structure
similarity , supported by more than 80% of the participants. However,
theoverlapping n-grams (e.g., BLEU score [ 49]) and edit similarity
receive the least support rate (i.e., 65% and 70%, respectively).
Access to service. Figure 8 (c) presents the likelihood of differ-
ent access to use statement-level code completion tools. Although
the support rate of local service drops 15% compared to token-
level completion (from 82% to 67%), locally deployed tools are still
the most expected. However, the willingness to adopt an online
statement-level code completion tool increases. For instance, 66%
participants agree or strongly agree to invoke online completion
services in the intranet (compared with 62% for token-level comple-
tion). Furthermore, the support rate of internet tools also increases
from 50% to 58%.
Moreover, among online services, we observe an obvious differ-
ence between the expectations of tools deployed on the intranet and
the internet (e.g., 58% and 66% in statement-level code completion).
According to our survey, the difference can be attributed to the
concern about data leakage and network stability of the internet.
As an interviewee stated in his/her department, Copilot was not
permitted to be installed because it would upload the code to the
cloud and might result in leaking confidential code.
Effectiveness. From Figure 8 (d), we find that only 26% partic-
ipants can accept the ratio of manual modification greater than
60% of completed code. Besides, 35% and 39% participants expect
the maximum modification ratio to be no more than 20% and 40%.
For grammatical correctness, if a statement-level code completion
tool ensures that at least 80% of completed code is grammatically
correct, it can satisfy 82% practitioners. Besides, 18% participants
have strict requirements on the correctness of generated code, i.e.,
they expect the tools to make no mistakes in code grammar.
The quantity of generated code is another important adoption
factor in practice. For instance, generating more code can reduce
programmers’ keystrokes. However, according to existing work
[17,21], the more code is completed, the more errors occur, result-
ing in more effort to modify them. Thus, we further investigate the
practitioners’ preference between completion effectiveness (gener-
ating less code with higher accuracy) and generated code quantity
(generating more code with more manual modifications). The re-
sults are illustrated in Figure 8 (e). From our survey, there is no very
clear winner between them (46% support rate for effectiveness and
40% for generated code quantity), and a few more participants con-
sider effectiveness more important. In this question, participants
mainly have two points of view:
(1)Effectiveness is more important. For the supporters of
effectiveness, completion results with errors may affect their
efficiency. A participant shared his/her experience with code
completion tools. “ It was very annoying for me to modify the
0% 10% 20% 30% 40% 50% 60% 70% 80% 90%Maximum Installation 
 time consumption [O1]
1min 5mins 10mins 20mins Other(a) Expectations on time consumption of tool installation.
0% 10% 10% 20% 30% 40% 50% 60% 70% 80% 90%I hope code completion tools can 
learn my programming habits [O3]I hope code completion tools can 
provide additional information [O2]
Strongly Disagree Disagree Neutral Agree Strongly Agree
(b) Expectations on additional information display and personalized comple-
tion.
Figure 9: Expectations on other aspects of general code com-
pletion tools, where the horizontal axis denotes the percent-
age of valid responses.
completion results. ... I’d rather have less code completed than
modify completion results. "
(2)Quantity is more important. Supporters of code quantity
expect statement-level completion tools to predict more code
at once, even if it requires revisions. One interviewee stated
that a tool that could forecast code with accurate logic (i.e.,
correct code skeleton) would be effective. He expressed con-
tentment in correcting erroneous details such as incorrect
strings, constant values, and identifier names present in the
predicted code. The interviewee acknowledged the tool’s
effectiveness in reducing the amount of code he would have
written otherwise.
Efficiency. Considering that statement-level completion is able to
generate multiple code statements, we study practitioners’ expec-
tations on the efficiency of statement-level code completion tools
from two aspects, which include the average time consumption of
completing a line (shown in Figure 8 (f)) and maximum latency of
generating completion results (shown in Figure 8 (g)), respectively.
From our survey, we observe that the speed of averagely taking
0.5 seconds to generate a line receives the highest support rate at
44%, and 39% participants expect the time to be no more than 1
second. For the maximum completion latency, the practitioners who
expect the code completion tools to generate completion results
in less than 2 seconds have the largest proportion (i.e., 47%). If a
statement-level code completion tool generates completion results
within 2 seconds, it can satisfy 67% practitioners. In addition, few
participants (i.e., 4%) express they are willing to wait for more than
5 seconds for completion.
Finding 8: Most practitioners expect statement-level code com-
pletion tools to take less than 0.5 seconds to complete one line
of code. Besides, To meet 67% practitioners’ expectations, tools
should generate completion results within 2 seconds.
3.3.3 RQ 3.3 Expectations on Other General Aspects of Code Com-
pletion Tools. In this section we present the practitioners’ expec-
tations on general aspects of code completion, and the results are
illustrated in Figure 9.
Figure 9 (a) shows practitioners’ expectations on the time cost of
installing code completion tools. 83% participants can be satisfied
if they can install the tool within 5 minutes. Besides, we observe
that 14% participants are willing to spend more than 20 minutes on
1301How Practitioners Expect Code Completion? ESEC/FSE ’23, December 3–9, 2023, San Francisco, CA, USA
installing tools. As one interviewee said: “ I would like to spend a
whole day on installing a code completion tool as long as it can really
improve my efficiency. "
From Figure 9 (b) we observe that most (76%) practitioners agree
or strongly agree that code completion tools are supposed to learn
their programming habits (e.g., preference for function names and
variables). In addition, 83% participants expect code completion
tools to simultaneously present some additional information about
the predicted code (e.g., showing users the API definition or doc-
umentation while recommending an API). One interviewee told
us: “Sometimes I forgot the order of arguments in the API, thus show-
ing me how the API is defined can greatly reduce my programming
mistake. "
3.4 RQ4: Existing Research
After our literary review, we identify 34 papers in total from the
popular conference and journals.[ 13,14,17–19,21,23,25,28–30,
33,36–41,45–48,50,54,55,57,58,60–62,64,67–69]. Table 2 shows
the capabilities of the surveyed code completion techniques. We
can observe that the research on token-level completion is much
more than that on statement-level completion (26 papers v.s. 12
papers, with four papers in both categories).
3.4.1 Token-Level Completion. Usage scenarios. As seen from
Table 2, all of the 26 collected token-level completion papers sup-
port API recommendation , which greatly meets the practitioners’
expectations. However, only less than half of these papers work on
identifier completion that also receives a high Likert Score, and none
of them mention path completion . This may be attributed to the fact
that most identifiers and path tokens are Out-of-Vocabulary (OoV)
words for the proposed techniques.
Evaluation metrics. In this part, accuracy is equivalent to top-1
accuracy and error rate, recall refers to the more general top-k
accuracy, and average rank refers to the average rankings of the ap-
propriate token in the prediction candidates (e.g., Mean Reciprocal
Rank, MRR). In Table 2, we find that most of the papers utilize recall
to evaluate the effectiveness of their techniques. Metric accuracy
andaverage rank are mentioned in 11 and 9 papers, respectively.
Moreover, other evaluation metrics are explored. Some researchers
attempt to explore better metrics as the proxy for users’ productiv-
ity such as saved keystrokes [ 25], and customized metrics including
“Explicit select”, “Typed select”, “Typing actions”, “Prefix length”,
and “Manual start”[13].
Finding 9: Most papers use recall to evaluate the effectiveness
of their token-level completion methods. Only 11 of 26 papers
involve accuracy , which is cared for by the most participants.
Access to service. Few papers have focused on employing their
techniques in industrial products. We classify the papers into two
categories including online and offline according to their deploy-
ment. The following are a few examples. Svyatkovskiy et al. [ 61,62]
develop their system as part of Intellicode extension in Visual Stu-
dio Code IDE [ 9], allowing programmers to use it locally. Izadi
et al. [ 30] utilize a Transformer architecture, claiming that they
put the model on the cloud and provide completion web services.
Moreover, Hindle et al. [ 25] mention that their tool is incorporated
into the locally deployed Eclipse plug-in. Finally, Bibaev et al. [ 13]Table 2: Capabilities of existing research. Likert Score denotes
the average weighted score of participants’ ratings (1 to 5
corresponds to strongly disagree to strongly agree).
DescriptionLikertPapersScoreToken-level completionUsage Scenarios
Identifier completion [T1] 4.50 [14, 30, 33, 36, 38, 39, 48, 50, 54]
API recommendation [T2][23, 25, 28, 29, 36, 41, 50, 62, 64]
4.51 [14, 33, 37–39, 54, 57, 58, 67]
[13, 19, 30, 40, 45, 48, 55, 61]
Path completion [T3] 4.39 -
Evaluation Metrics
Accuracy [T4] 4.19 [13, 14, 19, 25, 28–30, 38, 40, 48, 67]
Average rank [T5] 4.00 [23, 28–30, 33, 40, 61, 62, 64]
Recall [T6] 4.05[23, 28, 37, 45, 48, 54, 62]
[13, 19, 29, 36, 39, 55, 61]
Others [13, 14, 19, 23, 25, 38, 41, 50, 57, 58]
Access to service
Internet or Intranet [T7] 3.43 [13, 30, 62]
Locally deployed [T9] 4.20 [13, 25, 50, 57, 58, 61, 62]
Time Consumption
Mentioned[14, 28, 37, 48, 50, 54, 55, 64]
[13, 23, 25, 29, 30, 38, 61, 62]Statement-level completionUsage Scenarios
Next statement prediction [S1]3.27 [17, 21, 30, 46, 60]
[18, 41, 47, 69]
Completion of currently edited statement [S2] 3.74 [17, 28, 47, 60]
API argument recommendation [S3] 3.81 [17, 48, 60]
String completion [S4] 3.32 -
Skeleton prediction [S5] 3.88 [21]
Block content prediction [S6] 3.43 [17, 18, 68]
Evaluation Metrics
Overlapped n-grams [S7] 3.72 [17, 18, 21, 30, 41]
Edit similarity [S8] 3.81 [18, 41, 60, 68]
Function similarity [S9] 4.09 [21]
Readability [S10] 4.31 -
Grammatical correctness [S11] 4.42 -
Others [18, 28, 46, 47, 60, 68, 69]
Access to service
Internet or Intranet [S12] 3.54 [30, 60, 68]
Locally deployed [S14] 3.89 [69]
Time Consumption
Mentioned [17, 28, 30, 47, 60, 69]
and Pelsmaeker et al. [ 50] integrated their methods into the IntelliJ
IDE.
Efficiency. Among collected token-level code completion papers,
16 of them explicitly consider the time consumption. 13 techniques
proposed in [ 13,14,23,25,28–30,37,38,54,61,62,64] take less
than 200 milliseconds to predict a token. According to our results,
these techniques with such latency can satisfy more than 90% of
practitioners. However, three other methods take several seconds
to complete a single request [ 48,50,55], which is considered unac-
ceptable for token-level completion.
3.4.2 Statement-Level Completion. Usage scenarios. As seen from
Table 2, most papers focus on next statement prediction which re-
ceives only 43% support rate, but the more expected scenarios com-
pletion of currently edited line andAPI argument recommendation
obtain less attention. Three papers propose techniques for block
content prediction scenario. Moreover, only one paper proposes
techniques to predict the methods’ skeletons ( skeleton prediction ),
and no papers complete string content ( string completion ).
Finding 10: Most papers focus on next statement prediction sce-
nario, while only a few of them explore completion of currently
edited line andAPI argument recommendation .
Evaluation metrics. Most papers evaluate the generated code
viaoverlapped n-grams such as BLUE and ROUGE, and edit sim-
ilarity , which receive the lowest Likert Scores (i.e., 3.72 and 3.81,
respectively, as shown in Table 2) according to our survey. Only
one paper evaluates the generated code against human-written
code via function/structure similarity . Moreover, none of them men-
tion the readability andgrammatical correctness of the generated
1302ESEC/FSE ’23, December 3–9, 2023, San Francisco, CA, USA Chaozheng Wang, Junhao Hu, Cuiyun Gao, Yu Jin, Tao Xie, Hailiang Huang, Zhenyu Lei, and Yuetang Deng
code. Some papers also propose customized metrics to comprehen-
sively evaluate their statement-level completion techniques such
as click-through rate [18, 46, 47, 60, 68, 69].
Finding 11: Most papers focus on measuring overlapped n-grams
andedit similarity between the generated code and the human-
written code that are not preferred by the large majority of par-
ticipants. No paper evaluates the generated code via grammatical
correctness andreadability , which the practitioners value most.
Access to service. We also classify the papers into two categories,
including online and offline according to their deployment. The
following are a few examples. The work [ 60] proposes a tool called
Intellicode Compose, which deploys the models on the cloud and
also allows client-side caching. Moreover, Wen et al. [ 68] present
an android studio plugin as a web service. In addition, the tool
proposed in [ 69] serves as an Eclipse plug-in and provide offline
completion service.
Efficiency. Among the collected papers that propose statement-
level code completion techniques, only six of them explicitly discuss
the time consumption of their techniques. For example, the methods
proposed in [ 17,28,30,60,69] take less than 1 second to complete
code statements, while the method in [ 47] takes more than 5.5
seconds on average for prediction.
Finding 12: Time consumption, a critical adoption factor of
code completion tools, is missing in 40% and 50% of our collected
token-level and statement-level papers, respectively.
4 DISCUSSION
4.1 Implications
Our survey results highlight several implications for future code
completion research:
4.1.1 Focusing More on Practitioner-Valued Completion Scenarios.
For token-level code completion, existing research mostly focuses
onidentifier completion andAPI recommendation , meeting the ex-
pectations of practitioners. However, the scenario path completion
expected by 86% practitioners is missing in all 26 papers.
For statement-level completion, two valued scenarios currently
edited line completion andAPI argument recommendation are only
mentioned by four and three papers, respectively. In addition, only
one paper has proposed methods for the most expected skeleton pre-
diction scenario, and another important scenario string completion
is not mentioned in the collected papers. Therefore, future research
is encouraged to focus on practitioner-valued completion scenarios.
4.1.2 Personalizing Code Completion Results. Personalization of
code completion results is cared for by practitioners according to
our survey. For instance, 76% practitioners expect code comple-
tion tools to learn their programming habits so that they can keep
the programming style consistent and reduce modification (e.g.,
identifier naming habit and indentation habit). Additionally, per-
sonalizing completion results may improve the effectiveness. A
participant told us “ I wish code completion tools to learn from my
local code because it may provide a reference and be helpful to my
current implementation. " However, few prior studies have explored
how to personalize code completion results in the past decade,which is also a valuable direction for future research such as effec-
tively re-training the completion model locally based on local code
corpus.
4.1.3 Updating Evaluation Metrics. Evaluation metric is another
important aspect that is cared about by practitioners. Accuracy ,
which is the most valued metric of token-level code completion
according to our survey, is only mentioned in 40% collected papers.
In addition, most existing studies about statement-level code com-
pletion evaluate the generated code by comparing it with human-
written code in terms of overlapping n-grams (such as BLEU score
and ROUGE) [ 17] and editing similarity [18,42]. However, these
two metrics receive the least support rate among all the evaluation
metrics in our survey. Practitioners expect the techniques to use
the metric function/structure similarity for evaluation. A partici-
pant told us: “ ... BLEU score may be a good criterion to judge the
quality of generated natural language texts, however, it can hardly
evaluate whether a code snippet is satisfactory." Moreover, the met-
rics that participants value most such as grammatical correctness
andreadability of generated code are missing in existing research.
Thus, besides measuring character-level similarity between gener-
ated code and human-written code, future research is suggested
to update evaluation metrics such as involving the grammatical
correctness andreadability metrics.
4.1.4 Exploration on Balancing Effectiveness and Quantity of Gen-
erated Code. The quantity of generated code can greatly affect the
user experience. This is because generating more code is probable to
save more keystrokes for users but may bring more errors, making
programmers put effort to modify the completion results. From our
survey, 46% practitioners regard the effectiveness of statement-level
code completion weighs more than the quantity of generated code
(39%). Furthermore, we observe that most practitioners expect that
the completed code does not need intensive manual modification
in our survey. For instance, 74% participants cannot accept that
more than 40% of completed code needs manual modification. Thus,
researchers may study how the model performance changes along
with the increasing quantity of generated code and preliminarily
estimate how much code needs modification, thereby providing
suggestions on the quantity of generated code.
4.1.5 Paying More Attention to Code Completion Latency. Comple-
tion latency is one key aspect that substantially affects practitioners’
likelihood to adopt code completion tools. For instance, only 15%
of the participants can accept that tools take more than 400 mil-
liseconds to predict a token. If the latency is too large, practitioners
prefer to write code by themselves. However, the time consumption
factor is missing in nearly half of our collected papers. In addition,
among the papers that mention completion latency, still 30% papers
fail to fulfill practitioners’ expectations. Therefore, in addition to
the effectiveness-related metrics, researchers should also pay more
attention to the completion latency during comparison.
4.1.6 Exploring Lightweight Completion Techniques. How to access
completion services is a vital adoption factor of code completion
tools. From our survey, offline code completion services are the
most expected for both token-level and statement-level completion.
Among the token-level completion papers that discuss their access
to service, most of them propose offline tools. However, only one
1303How Practitioners Expect Code Completion? ESEC/FSE ’23, December 3–9, 2023, San Francisco, CA, USA
statement-level paper supports offline completion service. This may
be attributed to that statement-level approaches are mostly based
on large deep learning models and cannot be deployed efficiently on
ordinary PCs. Thus, how to implement lightweight completion ap-
proaches (e.g., model compression) for providing offline completion
services also deserves exploring.
4.1.7 Improving Robustness of Code Completion Techniques. In our
survey, many participants mention that the robustness of code
completion techniques is also vital to user experience. Robustness
requires that the completion results are not affected by slight per-
turbations in the input. From our survey, an interviewee shared his
experience with Tabnine: “ Tabnine can successfully predict the API
arguments if the variable is named ‘X_train’. However, if I modify the
name to ‘x_train’, the completion results will be totally different. " Be-
sides identifier changes, code completion tools are also supposed to
be robust to statement changes. A participant stated that: “ AiXCoder
can predict the block content of a method well, but the results turn
out to be terrible when I inserted an unrelated assignment statement. ”
However, no prior study focuses on improving the robustness of
code completion methods. Therefore, researchers are supposed to
put more effort into generating robust code completion results.
4.2 Threats to Validity
One of the threats in our survey is that there may be some par-
ticipants who do not fully understand the questions. For instance,
some participants have never used statement-level code comple-
tion tools. Therefore, they may be unfamiliar with the questions of
statement-level code completion. To reduce this threat, we utilize
one or two clear images or GIFs to describe each scenario and fa-
cilitate their better understanding of the questions. Furthermore,
some participants do not answer the questions seriously and the
results cannot reflect their beliefs. Therefore, we drop the responses
completed in less than two minutes. This is a common and tolerable
threat to validity in previous studies, e.g., [32].
Another threat is that our participants may not be representa-
tive of typical programmers. In addition, the survey responses are
self-reported by practitioners, which may not accurately represent
their true opinions. Our solution is to widely survey practitioners
working in many IT companies. We believe we have made this
threat have minimal impact on the results of our survey.
5 RELATED WORK
5.1 Code Completion
Traditional code completion focused on static analysis techniques
associated with manually defined rules to suggest code [ 22,43,51,
63]. For instance, researchers utilized type information [ 22,26],
similar code snippets [ 16] and history data [ 56] to predict needed
code tokens. Equipped with machine learning, a series of code
completion work using statistical language models was proposed
[23,25,45,48,52,64]. For instance, Hellen et al. [ 23] explicitly took
the techniques such as nested scopes into account and improved
the performance of the n-gram model.
With the development of deep learning, neural networks such
as RNN [ 59] and Transformer [ 66] showed great capability to learn
features from source code [ 23,33,36]. For instance, Li et al. [ 36]proposed a point mixture network to relieve the Out-of-Vocabulary
problem. Kim et al. [ 33] parsed the code into abstract syntax trees
and fed the trees into a Transformer-based model. In recent years,
pre-trained language models have been leveraged for predicting
multiple code tokens [ 30,60]. Svyatkovskiy et al. [ 60] proposed
GPT-C, which could predict code statements. Izadi et al. [ 30] raised
CodeFill, which combined the type and semantic information of
code, further improving the completion performance.
5.2 Studies on Code Completion Practices
Apart from research on code completion, some other work focus
on studying code completion practices [ 12,24,65,70]. For instance,
Vaithilingam et al. [ 65] asked participants to finish programming
tasks with or without Copilot [ 3] and determined whether Copilot
is useful. In addition, Ziegler et al. [ 70] focused on investigating
evaluation metrics of code completion, comparing the measurable
user data (objective data) and the user-reported productivity (sub-
jective data), and identified the most representative metric as a
proxy of productivity. The work [ 12,24] both explored the most
important tokens that needed to be completed. Vaithilingam et al.
[65] conducted a user study that asked participants to solve three
tasks via different code completion tools and compared the time
consumption. Arrebola et al. [ 11] concluded that half of the inter-
actions with code assistants are either dismissed, or interrupted,
or the completion proposals displayed have no direct contribution
to the task. The work [ 31] explored the relationship between the
design of code generation tools and their ease of use (user learn-
ability) and McNutt et al. [ 44] studied the design space and users’
expectations of code assistants in Jupyter Notebook.
However, no prior studies have investigated the practitioners’
expectations on code completion. In this paper, we conduct a large
scale user study, investigating practitioners’ expectations on mul-
tiple aspects of code completion tools. Moreover, we perform a
comprehensive literature review to reveal the gap between cur-
rent techniques and practitioners’ expectations, providing future
directions for researchers.
6 CONCLUSION
In this paper, we have interviewed 15 professionals and surveyed
599 practitioners on completion practices, issues that they face, and
their expectations on code completion techniques. These practition-
ers expect code completion techniques to extend usage scenarios ,
update evaluation metrics , and to be improved in terms of access to
service ,effectiveness , and efficiency . We have also compared the capa-
bility of the existing research with practitioners’ expectations via a
literature review, pointing out the aspects that should be improved
to meet the expectations of the practitioners.
ACKNOWLEDGMENT
This work was partially supported by National Natural Science
Foundation of China under Grant No. 62161146003, and the Tencent
Foundation/XPLORER PRIZE.
REFERENCES
[1] n.d.. aiXcoder. https://aixcoder.com/.
[2] n.d.. Clangd. https://clangd.llvm.org/.
[3] n.d.. Copilot. https://github.com/features/copilot/.
1304ESEC/FSE ’23, December 3–9, 2023, San Francisco, CA, USA Chaozheng Wang, Junhao Hu, Cuiyun Gao, Yu Jin, Tao Xie, Hailiang Huang, Zhenyu Lei, and Yuetang Deng
[4] n.d.. Google Forms. https://docs.google.com/forms.
[5]n.d.. NVivo. https://www.qsrinternational.com/nvivo-qualitative-data-analysis-
software/home.
[6] n.d.. Pycharm. https://www.jetbrains.com/pycharm/.
[7] n.d.. TabNine. https://www.tabnine.com/.
[8] n.d.. Tencent Questionnair. https://wj.qq.com/.
[9] n.d.. Visual Studio Code. https://code.visualstudio.com/.
[10] Sven Amann, Sebastian Proksch, Sarah Nadi, and Mira Mezini. 2016. A study of
visual studio usage in practice. In IEEE 23rd International Conference on Software
Analysis, Evolution, and Reengineering, SANER 2016, Suita, Osaka, Japan, March
14-18, 2016 - Volume 1 . 124–134.
[11] Fabio Villamarin Arrebola and Plinio Thomaz Aquino Junior. 2017. On source
code completion assistants and the need of a context-aware approach. In Human
Interface and the Management of Information: Supporting Learning, Decision-
Making and Collaboration: 19th International Conference, HCI International 2017,
Vancouver, BC, Canada, July 9–14, Part II 19 . 191–201.
[12] Gareth Ari Aye, Seohyun Kim, and Hongyu Li. 2021. Learning autocompletion
from real-world datasets. In 43rd IEEE/ACM International Conference on Software
Engineering: Software Engineering in Practice, ICSE (SEIP) 2021, Madrid, Spain,
May 25-28, 2021 . 131–139.
[13] Vitaliy Bibaev, Alexey Kalina, Vadim Lomshakov, Yaroslav Golubev, Alexander
Bezzubov, Nikita Povarov, and Timofey Bryksin. 2022. All you need is logs:
improving code completion by learning from anonymous IDE usage logs. In
30th ACM Joint European Software Engineering Conference and Symposium on
the Foundations of Software Engineering, ESEC/FSE 2022, Singapore, Singapore,
November 14-18, 2022 . 1269–1279.
[14] Pavol Bielik, Veselin Raychev, and Martin T. Vechev. 2016. PHOG: probabilistic
model for code. In 33nd International Conference on Machine Learning, ICML
2016, New York City, NY, USA, June 19-24, 2016 (JMLR Workshop and Conference
Proceedings, Vol. 48) . 2933–2942.
[15] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan,
Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, et al .2020. Language models are few-shot learners. Advances in Neural
Information Processing Systems 33: Annual Conference on Neural Information
Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual (2020).
[16] Marcel Bruch, Martin Monperrus, and Mira Mezini. 2009. Learning from examples
to improve code completion systems. In 7th joint meeting of the European Soft-
ware Engineering Conference and the ACM SIGSOFT International Symposium on
Foundations of Software Engineering, ESEC/FSE 2009, Amsterdam, The Netherlands,
August 24-28, 2009 . 213–222.
[17] Matteo Ciniselli, Nathan Cooper, Luca Pascarella, Denys Poshyvanyk, Massimil-
iano Di Penta, and Gabriele Bavota. 2021. An empirical study on the usage of
BERT models for code completion. In 18th IEEE/ACM International Conference on
Mining Software Repositories, MSR 2021, Madrid, Spain, May 17-19, 2021 . 108–119.
[18] Colin B. Clement, Shuai Lu, Xiaoyu Liu, Michele Tufano, Dawn Drain, Nan Duan,
Neel Sundaresan, and Alexey Svyatkovskiy. 2021. Long-range modeling of source
code files with eWASH: extended window access by syntax hierarchy. In 2021
Conference on Empirical Methods in Natural Language Processing, EMNLP 2021,
Virtual Event / Punta Cana, Dominican Republic, 7-11 November, 2021 . 4713–4722.
[19] Luiz Laerte Nunes da Silva Junior, Troy Costa Kohwalter, Alexandre Plastino, and
Leonardo Gresta Paulino Murta. 2021. Sequential coding patterns: How to use
them effectively in code recommendation. Inf. Softw. Technol. 140 (2021), 106690.
[20] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT:
pre-training of deep bidirectional transformers for language understanding. In
2019 Conference of the North American Chapter of the Association for Computational
Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN,
USA, June 2-7, 2019, Volume 1 (Long and Short Papers) . 4171–4186.
[21] Daya Guo, Alexey Svyatkovskiy, Jian Yin, Nan Duan, Marc Brockschmidt, and
Miltiadis Allamanis. 2021. Learning to complete code with sketches. In Interna-
tional Conference on Learning Representations .
[22] Tihomir Gvero, Viktor Kuncak, Ivan Kuraj, and Ruzica Piskac. 2013. Complete
completion using types and weights. In ACM SIGPLAN Conference on Program-
ming Language Design and Implementation, PLDI 2013, Seattle, WA, USA, June
16-19, 2013 . 27–38.
[23] Vincent J. Hellendoorn and Premkumar T. Devanbu. 2017. Are deep neural
networks the best choice for modeling source code?. In 2017 11th Joint Meeting
on Foundations of Software Engineering, ESEC/FSE 2017, Paderborn, Germany,
September 4-8, 2017 . 763–773.
[24] Vincent J. Hellendoorn, Sebastian Proksch, Harald C. Gall, and Alberto Bacchelli.
2019. When code completion fails: a case study on real-world completions. In
41st International Conference on Software Engineering, ICSE 2019, Montreal, QC,
Canada, May 25-31, 2019 . 960–970.
[25] Abram Hindle, Earl T. Barr, Zhendong Su, Mark Gabel, and Premkumar T. De-
vanbu. 2012. On the naturalness of software. In 34th International Conference on
Software Engineering, ICSE 2012, June 2-9, 2012, Zurich, Switzerland . 837–847.
[26] Daqing Hou and David M. Pletcher. 2010. Towards a better code completion
system by API grouping, filtering, and popularity-based ranking. In 2nd Interna-
tional Workshop on Recommendation Systems for Software Engineering, RSSE 2010,Cape Town, South Africa, May 4, 2010 . 26–30.
[27] Xing Hu, Xin Xia, David Lo, Zhiyuan Wan, Qiuyuan Chen, and Thomas Zimmer-
mann. 2022. Practitioners’ expectations on automated code comment generation.
In44th IEEE/ACM 44th International Conference on Software Engineering, ICSE
2022, Pittsburgh, PA, USA, May 25-27, 2022 . 1693–1705.
[28] Yasir Hussain, Zhiqiu Huang, Yu Zhou, and Senzhang Wang. 2020. CodeGRU:
context-aware deep learning with gated recurrent unit for source code modeling.
Inf. Softw. Technol. 125 (2020), 106309.
[29] Yasir Hussain, Zhiqiu Huang, Yu Zhou, and Senzhang Wang. 2023. Boosting
source code suggestion with self-supervised transformer gated highway. J. Syst.
Softw. 196 (2023), 111553.
[30] Maliheh Izadi, Roberta Gismondi, and Georgios Gousios. 2022. CodeFill: multi-
token code completion by jointly learning from structure and naming sequences.
In44th IEEE/ACM 44th International Conference on Software Engineering, ICSE
2022, Pittsburgh, PA, USA, May 25-27, 2022 . 401–412.
[31] Dhanya Jayagopal, Justin Lubin, and Sarah E Chasins. 2022. Exploring the
learnability of program synthesizers by novice programmers. In 35th Annual
ACM Symposium on User Interface Software and Technology, UIST . 1–15.
[32] Miryung Kim, Thomas Zimmermann, and Nachiappan Nagappan. 2014. An
empirical study of refactoring challenges and benefits at Microsoft. IEEE Trans.
Software Eng. 40, 7 (2014), 633–649.
[33] Seohyun Kim, Jinman Zhao, Yuchi Tian, and Satish Chandra. 2021. Code predic-
tion by feeding trees to transformers. In 43rd IEEE/ACM International Conference
on Software Engineering, ICSE 2021, Madrid, Spain, 22-30 May 2021 . 150–162.
[34] Pavneet Singh Kochhar, Xin Xia, David Lo, and Shanping Li. 2016. Practitioners’
expectations on automated fault localization. In 25th International Symposium
on Software Testing and Analysis, ISSTA 2016, Saarbrücken, Germany, July 18-20,
2016. 165–176.
[35] Jingxuan Li, Rui Huang, Wei Li, Kai Yao, and Weiguo Tan. 2021. Toward less
hidden cost of code completion with acceptance and ranking models. In IEEE
International Conference on Software Maintenance and Evolution, ICSME 2021,
Luxembourg, September 27 - October 1, 2021 . 195–205.
[36] Jian Li, Yue Wang, Michael R. Lyu, and Irwin King. 2018. Code completion
with neural attention and pointer networks. In Twenty-Seventh International
Joint Conference on Artificial Intelligence, IJCAI 2018, July 13-19, 2018, Stockholm,
Sweden . 4159–4165.
[37] Fang Liu, Ge Li, Bolin Wei, Xin Xia, Zhiyi Fu, and Zhi Jin. 2020. A self-attentional
neural architecture for code completion with multi-task learning. In ICPC ’20:
28th International Conference on Program Comprehension, Seoul, Republic of Korea,
July 13-15, 2020 . 37–47.
[38] Fang Liu, Ge Li, Bolin Wei, Xin Xia, Zhiyi Fu, and Zhi Jin. 2022. A unified multi-
task learning model for AST-level and token-level code completion. Empir. Softw.
Eng. 27, 91 (2022).
[39] Fang Liu, Ge Li, Yunfei Zhao, and Zhi Jin. 2020. Multi-task learning based pre-
trained language model for code completion. In 35th IEEE/ACM International
Conference on Automated Software Engineering, ASE 2020, Melbourne, Australia,
September 21-25, 2020 . 473–485.
[40] Fang Liu, Lu Zhang, and Zhi Jin. 2020. Modeling programs hierarchically with
stack-augmented LSTM. J. Syst. Softw. 164 (2020), 110547.
[41] Shuai Lu, Nan Duan, Hojae Han, Daya Guo, Seung-won Hwang, and Alexey
Svyatkovskiy. 2022. ReACC: a retrieval-augmented code completion framework.
In60th Annual Meeting of the Association for Computational Linguistics (Volume
1: Long Papers), ACL 2022, Dublin, Ireland, May 22-27, 2022 . 6227–6240.
[42] Shuai Lu, Daya Guo, Shuo Ren, Junjie Huang, Alexey Svyatkovskiy, Ambrosio
Blanco, Colin B. Clement, Dawn Drain, Daxin Jiang, Duyu Tang, Ge Li, Lidong
Zhou, Linjun Shou, Long Zhou, Michele Tufano, Ming Gong, Ming Zhou, Nan
Duan, Neel Sundaresan, Shao Kun Deng, Shengyu Fu, and Shujie Liu. 2021.
CodeXGLUE: a machine learning benchmark dataset for code understanding
and generation. In Neural Information Processing Systems Track on Datasets and
Benchmarks 1, NeurIPS Datasets and Benchmarks 2021, December 2021, virtual .
[43] David Mandelin, Lin Xu, Rastislav Bodík, and Doug Kimelman. 2005. Jungloid
mining: helping to navigate the API jungle. In ACM SIGPLAN 2005 Conference on
Programming Language Design and Implementation, Chicago, IL, USA, June 12-15,
2005. 48–61.
[44] Andrew M McNutt, Chenglong Wang, Robert A Deline, and Steven M Drucker.
2023. On the design of AI-powered code assistants for notebooks. In 2023 CHI
Conference on Human Factors in Computing Systems, CHI 2023 . 1–16.
[45] Anh Tuan Nguyen and Tien N. Nguyen. 2015. Graph-based statistical language
model for code. In 37th IEEE/ACM International Conference on Software Engineer-
ing, ICSE 2015, Florence, Italy, May 16-24, 2015, Volume 1 . 858–868.
[46] Anh Tuan Nguyen, Aashish Yadavally, and Tien N. Nguyen. 2022. Next syntactic-
unit code completion and applications. In 37th IEEE/ACM International Conference
on Automated Software Engineering, ASE 2022, Rochester, MI, USA, October 10-14,
2022. 180:1–180:5.
[47] Son Van Nguyen, Tien N. Nguyen, Yi Li, and Shaohua Wang. 2019. Combining
program analysis and statistical language model for code statement completion.
In34th IEEE/ACM International Conference on Automated Software Engineering,
ASE 2019, San Diego, CA, USA, November 11-15, 2019 . 710–721.
1305How Practitioners Expect Code Completion? ESEC/FSE ’23, December 3–9, 2023, San Francisco, CA, USA
[48] Tung Thanh Nguyen, Anh Tuan Nguyen, Hoan Anh Nguyen, and Tien N. Nguyen.
2013. A statistical semantic language model for source code. In Joint Meeting of the
European Software Engineering Conference and the ACM SIGSOFT Symposium on
the Foundations of Software Engineering, ESEC/FSE 2013, Saint Petersburg, Russian
Federation, August 18-26, 2013 . 532–542.
[49] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. Bleu: a
method for automatic evaluation of machine translation. In 40th Annual Meeting
of the Association for Computational Linguistics, ACL 2002, Philadelphia, PA, USA .
311–318.
[50] Daniël A. A. Pelsmaeker, Hendrik van Antwerpen, Casper Bach Poulsen, and
Eelco Visser. 2022. Language-parametric static semantic code completion. Proc.
ACM Program. Lang. 6, OOPSLA1 (2022), 1–30.
[51] Daniel Perelman, Sumit Gulwani, Thomas Ball, and Dan Grossman. 2012. Type-
directed completion of partial expressions. In ACM SIGPLAN Conference on Pro-
gramming Language Design and Implementation, PLDI 2012, Beijing, China - June
11 - 16, 2012 . 275–286.
[52] Sebastian Proksch, Johannes Lerch, and Mira Mezini. 2016. Intelligent code
completion with Bayesian networks. In Software Engineering 2016, Fachtagung
des GI-Fachbereichs Softwaretechnik, 23.-26. Februar 2016, Wien, Österreich (LNI,
Vol. P-252) . 25–26.
[53] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever,
et al.2019. Language models are unsupervised multitask learners. OpenAI blog
1, 8 (2019), 9.
[54] Veselin Raychev, Pavol Bielik, and Martin T. Vechev. 2016. Probabilistic model
for code with decision trees. In 2016 ACM SIGPLAN International Conference on
Object-Oriented Programming, Systems, Languages, and Applications, OOPSLA
2016, part of SPLASH 2016, Amsterdam, The Netherlands, October 30 - November 4,
2016. 731–747.
[55] Veselin Raychev, Martin T. Vechev, and Eran Yahav. 2014. Code completion
with statistical language models. In ACM SIGPLAN Conference on Programming
Language Design and Implementation, PLDI 2014, Edinburgh, United Kingdom -
June 09 - 11, 2014 . 419–428.
[56] Romain Robbes and Michele Lanza. 2008. How program history can improve code
completion. In 23rd IEEE/ACM International Conference on Automated Software
Engineering ASE 2008, 15-19 September 2008, L’Aquila, Italy . 317–326.
[57] André L. Santos and Brad A. Myers. 2017. Design annotations to improve API
discoverability. J. Syst. Softw. 126 (2017), 17–33.
[58] André L. Santos, Gonçalo Prendi, Hugo S. Sousa, and Ricardo Ribeiro. 2017.
Stepwise API usage assistance using n-gram language models. J. Syst. Softw. 131
(2017), 461–474.
[59] Alex Sherstinsky. 2018. Fundamentals of recurrent neural network (RNN) and
long short-term memory (LSTM) network. CoRR abs/1808.03314 (2018).
[60] Alexey Svyatkovskiy, Shao Kun Deng, Shengyu Fu, and Neel Sundaresan. 2020.
IntelliCode compose: code generation using transformer. In ESEC/FSE ’20: 28th
ACM Joint European Software Engineering Conference and Symposium on the
Foundations of Software Engineering, ESEC/FSE 2020, Virtual Event, USA, November8-13, 2020 . 1433–1443.
[61] Alexey Svyatkovskiy, Sebastian Lee, Anna Hadjitofi, Maik Riechert, Juliana Vi-
cente Franco, and Miltiadis Allamanis. 2021. Fast and memory-efficient neural
code completion. In 18th IEEE/ACM International Conference on Mining Software
Repositories, MSR 2021, Madrid, Spain, May 17-19, 2021 . 329–340.
[62] Alexey Svyatkovskiy, Ying Zhao, Shengyu Fu, and Neel Sundaresan. 2019. Pythia:
AI-assisted code completion system. In 25th ACM SIGKDD International Confer-
ence on Knowledge Discovery & Data Mining, KDD 2019, Anchorage, AK, USA,
August 4-8, 2019 . 2727–2735.
[63] Suresh Thummalapenta and Tao Xie. 2007. Parseweb: a programmer assistant for
reusing open source code on the web. In 22nd IEEE/ACM International Conference
on Automated Software Engineering ASE 2007, November 5-9, 2007, Atlanta, Georgia,
USA. 204–213.
[64] Zhaopeng Tu, Zhendong Su, and Premkumar T. Devanbu. 2014. On the localness
of software. In 22nd ACM SIGSOFT International Symposium on Foundations of
Software Engineering, ESEC/FSE 2014, Hong Kong, China, November 16 - 22, 2014 .
269–280.
[65] Priyan Vaithilingam, Tianyi Zhang, and Elena L. Glassman. 2022. Expectation vs.
experience: evaluating the usability of code generation tools powered by large
language models. In CHI 2022: CHI Conference on Human Factors in Computing
Systems, New Orleans, LA, USA, 29 April 2022 - 5 May 2022, Extended Abstracts .
332:1–332:7.
[66] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In Advances in Neural Information Processing Systems 30: Annual Con-
ference on Neural Information Processing Systems NeurIPS 2017, December 4-9,
Long Beach, CA, USA . 5998–6008.
[67] Yanlin Wang and Hui Li. 2021. Code completion by modeling flattened abstract
syntax trees as graphs. In Thirty-Fifth AAAI Conference on Artificial Intelligence,
AAAI 2021, Thirty-Third Conference on Innovative Applications of Artificial Intelli-
gence, IAAI 2021, The Eleventh Symposium on Educational Advances in Artificial
Intelligence, EAAI 2021, Virtual Event, February 2-9, 2021 . 14015–14023.
[68] Fengcai Wen, Emad Aghajani, Csaba Nagy, Michele Lanza, and Gabriele Bavota.
2021. Siri, write the next method. In 43rd IEEE/ACM International Conference on
Software Engineering, ICSE 2021, Madrid, Spain, 22-30 May 2021 . 138–149.
[69] Yixiao Yang, Yu Jiang, Ming Gu, Jiaguang Sun, Jian Gao, and Han Liu. 2017. A
language model for statements of software code. In 32nd IEEE/ACM International
Conference on Automated Software Engineering, ASE 2017, Urbana, IL, USA, October
30 - November 03, 2017 . 682–687.
[70] Albert Ziegler, Eirini Kalliamvakou, X. Alice Li, Andrew Rice, Devon Rifkin,
Shawn Simister, Ganesh Sittampalam, and Edward Aftandilian. 2022. Productivity
assessment of neural code completion. In MAPS@PLDI 2022: 6th ACM SIGPLAN
International Symposium on Machine Programming, San Diego, CA, USA, 13 June
2022. 21–29.
Received 2023-02-02; accepted 2023-07-27
1306