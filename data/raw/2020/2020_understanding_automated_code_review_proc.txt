Understanding Automated CodeReview Process and Developer
Experience in Industry
Hyungjin Kim
hjkim17.kim@samsung.com
SamsungResearch
Seoul,KoreaYonghwi Kwon
yhwi.kwon@samsung.com
SamsungResearch
Seoul, KoreaSangwooJoh
sangwoo.joh@samsung.com
SamsungResearch
Seoul,Korea
HyukinKwon
hyukin.kwon@samsung.com
SamsungResearch
Seoul,KoreaYeonhee Ryou
yeonhee.ryou@samsung.com
SamsungResearch
Seoul, KoreaTaeksu Kim
taeksu.kim@samsung.com
SamsungResearch
Seoul,Korea
ABSTRACT
Code Review Automation can reduce human efforts during code re-
viewbyautomaticallyprovidingvaluableinformationtoreviewers.
Nevertheless,itisachallengetoautomatetheprocessforlarge-scale
companies,such asSamsung Electronics,duetotheircomplexity:
variousdevelopmentenvironments,frequentreviewrequests,huge
size ofsoftware,anddiverseprocessamong the teams.
In this work, we show how we automated the code review
processforthoseintricateenvironments,andsharesomelessons
learned during two years of operation. Our unified code review
automation system, Code Review Bot , is designed to process review
requestsholisticallyregardlessofsuchenvironments, and checks
variousquality-assuranceitemssuchaspotentialdefectsinthecode,
coding style,test coverage,andopen sourcelicenseviolations.
Somekey findingsinclude:1)about60%ofissues foundby Code
Review Bot were reviewed and fixed in advance of product releases,
2)morethan70%ofdevelopersgavepositivefeedbackaboutthe
system,3)developersrapidlyandactivelyrespondedtoreviews,and
4) the automation did not much affect the amount orthe frequency
ofhumancodereviewscomparedtotheinternalpolicytoencourage
code review activities. Our findings provide practical evidence that
automating code reviewhelpsassure software quality.
CCS CONCEPTS
¬∑General and reference ‚ÜíEmpirical studies ; ¬∑Software and
its engineering ‚ÜíApplication specific development environments ;
Collaboration in software development ; Software defect analysis;
Empirical software validation.
KEYWORDS
code review,reviewbot, code reviewautomation, staticanalysis
Permissionto make digitalor hard copies of allorpart ofthis work for personalor
classroom use is granted without fee provided that copies are not made or distributed
forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation
on the first page. Copyrights for components of this work owned by others than ACM
mustbehonored.Abstractingwithcreditispermitted.Tocopyotherwise,orrepublish,
topostonserversortoredistributetolists,requirespriorspecificpermissionand/ora
fee. Request permissions from permissions@acm.org.
ESEC/FSE ‚Äô22, November 14≈õ18,2022, Singapore, Singapore
¬©2022 Associationfor Computing Machinery.
ACM ISBN 978-1-4503-9413-0/22/11...$15.00
https://doi.org/10.1145/3540250.3558950ACMReference Format:
Hyungjin Kim, Yonghwi Kwon, Sangwoo Joh, Hyukin Kwon, Yeonhee
Ryou, and Taeksu Kim. 2022. Understanding Automated Code Review
Process and Developer Experience in Industry. In Proceedings of the 30th
ACM Joint European Software Engineering Conference and Symposium on
the Foundations of Software Engineering (ESEC/FSE ‚Äô22), November 14≈õ18,
2022, Singapore, Singapore. ACM, New York, NY, USA, 10pages.https:
//doi.org/10.1145/3540250.3558950
1 INTRODUCTION
CodeReviewAutomation isanessentialactivitytoreducehuman
effortinmoderncodereview[ 22].Itgenerallycontainsaprocessof
writingcodereviewsthroughapredefinedsetoftools.Manystudies
havebeenconductedoncodereviewautomation,or bots,andlotsof
open-source projects use automation tools to support code review.
This automationallows developerstofocus onunderstanding the
intention of code changes, and spend more time discussing the
architectural designordomain-specific problems.
Inspiteofitsconvenience,codereviewautomationinreal-world
industry faces several challenges. Samsung Electronics manufac-
tures many types of electronic devices such as mobile phones, net-
workequipment,smartTVs,andhomeappliances.Therefore,many
projects in Samsung are different in their development environ-
ments,domains,sizes,programminglanguages,andprocesses.The
automation ismore challenging inthesesettings.
Thefirstchallengeisthevarietyofdevelopment environments.
Forexample,eachdevelopmentgroupusesdifferentreviewsystem
tomeetitsownbusinessrequirements,including GitHub[11],Helix
Swarm[18], orGerrit[20]. Each review system offersunique user
experiencesandfunctionalitiesthroughtheirAPIs.Forinstance,de-
veloperscancommunicatewithricherinformationinGitHubusing
MarkdownandHTMLformatmessages.Othersystems,however,
lackthis feature.
Frequentreviewrequestsisthesecondchallenge.Thisscalability
issueiscrucialtotheindustry‚Äôsdevelopmentprocessbecauseanau-
tomatedsystemwithpoorperformancemaybecomethebottleneck.
Every day, in Samsung Electronics , approximately ten thousands of
review requests are created from more than 10,000 projects, and
about 1.4 million lines of code are added and about one million
removed.
Thelastchallengehastodowiththediversityofdevelopment
processes among the teams. To ensure software quality, each team
definesitsownassurancepoliciesandusesvariousanalysistools
1398ESEC/FSE ‚Äô22, November14‚Äì18, 2022,Singapore, Singapore Hyungjin Kim,Yonghwi Kwon,Sangwoo Joh, Hyukin Kwon,Yeonhee Ryou,andTaeksu Kim
forthispurpose.Forexample,therearesometeamsthatperform
static analysis as part of Continuous Integration (CI), but other
groups use the tools during the integration testing phase. Some
groupsareusingJavaandKotlinasprogramminglanguages,but
most are using C/C++. Therefore, different tools are required for
differentlanguages.
In order to overcomethesechallenges, we developed Code Re-
view Bot, a unified code review automation system for Samsung
Electronics . It supports various review systems that are used in
thecompany.Inaddition,itprovidesavarietyofservicessuchas
code change size measurements in review requests, typo checks,
potentialdefectdetection, andcoding style checks.
We conducted a survey to collect feedback from developers,
as well as a quantitative analysis how much the automation re-
ducestheireffortsbyexaminingthenumber andlengthofreview
comments.Over70%ofdevelopersaresatisfiedwithoursystem,
accordingtothe survey. Mostdevelopersagreed that CodeReview
Botis helpful for improving their code quality. Also, almost all
respondents thought code review automationcould reduce 30% of
their efforts. More than 60% of defects were fixed, as our system
motivateddevelopers to focusonimprovingcode quality.
Key contributionsare:
‚Ä¢We present Code ReviewBot ,anautomaticcodereview sys-
tem. It provides a unified interface and a unique user experi-
ence inthe code reviewactivity.
‚Ä¢CodeReviewBot isscalable,socanprovidefeedbackforeach
reviewrequest quickly.
‚Ä¢CodeReview Bot supportsvarioustypesofanalyses,includ-
ingcommitsizemeasurements,typochecks,potentialdefect
detection, coding style checks, reporting unit tests, test cov-
erage measurements, and architecture metrics visualization.
‚Ä¢We evaluated our system qualitatively and quantitatively.
Wefoundthatmorethan70%ofdevelopersaresatisfiedwith
oursystem,andmorethan60%defectshavebeenfixedby
developers.
2 CHALLENGESINCODE REVIEW
AUTOMATION AT SAMSUNG ELECTRONICS
Automatingcodereviewhelpstoimproveproductivity,butadapting
automation to large-scale industries can be challenging. In this
section, we discuss the challenges associated with automating the
process,especiallywithin SamsungElectronics.
2.1 VarietyofDevelopmentEnvironments
Considering the complex environments in large organizations, it is
essentialfor code reviewautomation systemsto be designedwith
great care. We use many programming languages, two different
sourcecodemanagementsystems,andseveralreviewsystemsat
SamsungElectronics.
Variousprogramminglanguagesareusedinthecompany.39%
of developers use C/C++, 31% use Java/Kotlin, 17% use Python, 7%
use JavaScript (including TypeScript), and 4% use C# and so on. An
automation systemshould be ableto handle this variety.
We also use two kinds of source code management systems
(SCMs):GitandPerforceHelixCore.Developmentteamschoose
SCMsbasedonfactorssuchasprojectsizes,targetplatform,andbuild environments. Perforce Helix is a centralized tool with pow-
erful branch-out functionalities and versioning features suitable
for product lines in large-scale projects. On the other hand, Git is a
distributedsystemthatsupportsmoreagiledevelopmentprocesses
ratherthanPerforceHelix.
Moreover,weutilizeseveralcodereviewsystems:GitHub,Gerrit,
and Helix Swarm. Some teams use Perforce Helix as their SCM, so
they should conduct the code review using Helix Swarm. Other
teamsuseGitandGitHubfortheSCMandthecodereviewsystem,
respectively. There are also a number of teams that use Gerrit as
their review system to develop Tizen platform or Android apps,
because they are working with public open-source developers who
are familiar withthat system.
2.2 FrequentReviewRequests
In addition, the frequent request for code reviews makes it difficult
toautomatetheprocess.Especiallyforlarge-scaleindustries,perfor-
mance and rapid response are the key requirements of automation
tools[16].
InSamsungElectronics,thousandsofreviewrequestsarecreated
inadayfrommorethantenthousandprojects,andabout1.4million
lines of code are added, and one million lines are removed every
day.Themediannumberoflinesaddedandremovedis35and8,
respectively.Themeanvalueforaddedlinesis1,373withastandard
deviation of 8.70 and that for removed lines is 757 with a standard
deviation of4.75.
2.3 DiversityofProcesses
Dueto2.1and2.2,thedevelopmentprocessesvaryamongteams.
Differentprogramminglanguages require differenttoolsforstatic
analysis.Pylint,forexample,isusedforanalyzingPythoncode,and
FindBugs is used for Java. Different review systems also involve
differntContinuousIntegration(CI)tools.ToolsusedinSamsung
Electronicsandtheircharacteristics are as follows.
‚Ä¢Staticanalyzers Staticanalysistoolstrytofindusefulin-
formationsuchaspotentialdefectsusingonlysourcecode.
The tools are not only used by open-source projects, but
alsobymanyindustriesinpracticetovalidateandverifythe
codebaseinspiteoffalsepositives[ 3,13].Wealsouseopen-
sourcestaticanalyzers(e.g.,Cppcheck,Pylint,FindBugs,etc.)
as well as private in-house ones.
‚Ä¢Unit testing tools Unit tests are used to ensure that the
softwarebehavesasintended.Thetoolsforrunningunittests
andcheckingtestcoveragevarybyprogramminglanguages.
The way we show the test results for developers varies by
reviewsystemstoo.
‚Ä¢Architecture metrics instrument The use of software
architecture metrics is generally considered as desirable for
project management and evaluation [ 9]. We measure the
metrics with private in-house tools. The metrics provide an
overview of architectural attributes related to reliability and
maintainability.
‚Ä¢Open source license validator Open source license vio-
lation becomes one of the most important issues. We also
checkcompliance issueswithprivate validation tools.
1399UnderstandingAutomatedCode ReviewProcessandDeveloperExperience in Industry ESEC/FSE ‚Äô22, November14‚Äì18, 2022,Singapore, Singapore
‚Ä¢Codingstylechecker Codingstylecheckersdetectviola-
tionsofdeveloper-defined orpre-defined rules.These tools
allow projects to keep consistent aesthetics. Some tools also
suggestpatchestocomplywiththerule.Likestatictools,dif-
ferent programming languages require different style check-
ers,anddifferentreviewsystemsrequiredifferentkindsof
feedback.
DifferentreviewsystemsalsoinvolvedifferentContinuousIn-
tegration (CI) tools. For instance, a CI tool designed for GitHub
may analyze source code in the head commit of the pull request to
provide useful insights for reviewers. On the other hand, review
processofGerrit,using ChangesandPatchSets forchangeneeds
differentprocessconfigurationofCIthanGitHub.
3 CODE REVIEW BOT: AUTOMATICCODE
REVIEW SYSTEM
We developed a code review automation system for Samsung Elec-
tronics,CodeReviewBot (CRB),whichhelpstocreateaunifiedcode
reviewprocessregardless ofthe toolsandsystems.
Designgoalsof Code Review Bot are as follows:
(1)(Extensibility) Oneofourmajorgoalsistomake CRBaseasy
aspossibleforqualitytooldeveloperstoservicetheirtoolsto
different review systems. This is achieved by abstracting all
the review systems used in the company as Abstract Review
System,andprovidingunifiedAPIs.
(2)(Convenience) CRB provides an accessible user interface for
developers to configure the options of the services they use.
We try to simplifying the complex configuration of all the
tools in use with a few clicks. In addition, for all review
systems,CRBgives its feedbackinaconsistent way.
(3)(Scalability) Inordertohandlethemassiverequestsforre-
view in Samsung Electronics , CRBshould be scalable. We
accomplishthisbydistributingtherequestsamongseveral
workers andprocessing eachtaskasynchronously.
3.1 OverallArchitecture
Figure1showsanoverallarchitectureof CRB.Whenadeveloper
creates a review request using a review system, the review system
triggersCRBtostart analyses.Receivingthe request, eachservice
ofCRBinspectscodechangesfromthereviewrequestandgener-
ates review comments including its analysis results. The developer
confirms comments via the review system by herself and checks
detailedinformationfrom CRB.Withthehelpof CRB,reviewersare
abletoconcentrateonunderstandingsemanticsofthecodechange
andmakereviewcommentswithoutworryofsimpleerrorssuch
as typos,potentialdefects,orcoding convention violations.
Themainfeatureof CRBisaconnectionbetweenreviewsystems
and services, each of them is located on both sides of CRBback-
end.Adaptermodule takes a role of communication between a
reviewsystemand CRB.Duringtheinformationexchange, Abstract
ReviewSystem componentabstractsconcretereviewsystemsand
encapsulatestheir detail. Abstract Review System providescommon
interfaces for every review system and lets CRBeasily extend new
kindsofsystems.
Invocation of analysis services and scheduling between ser-
vices are controlled by Distributed Task Queue Worker module. Theworker controls an execution order and aggregates analysis results.
Moreover, it is designed for parallel processing to boost up the
performance of CRB. To meet the requirement, the worker compo-
nentisimplementedwith Celery≈õasimple,flexible,andreliable
distributed system for Python language [ 27]. Moreover, services
need to be activated or turned off during an operation with a small
amount of effort. Thus, we designed Pluginscomponent based on a
microservice architecture style [ 25] for scalability and extensibility.
Figure2illustratesthepluginarchitectureof CRB.Eachplugin
matches a relevant analysis service . Plugins are categorized based
on theinvocation timing , thecommunication protocol between a
plugin and a relevant service, and the execution type ≈õ synchronous
orasynchronous .Eachcategorydefinesdefaultbehaviorssuchas
an interpretation of a request, a service invocation, and a response
generation.
Starting points of services are classified into three categories:
whenareviewrequestiscreated( ùëÜ1),whenotherserviceshavebeen
finished (ùëÜ2), or when code fragments have been merged ( ùëÜ3). End-
pointsareseparatedintotwogroups: synchronousanalysis ,waiting
forthefinishofservicesincetheanalysistimeisshortenough( ùê∏1),
orasynchronous analysis , receiving an asynchronous result from
aservice via callbacksbecause ofthelongtime-consuminganaly-
sis (ùê∏2.) Based on the characteristics of services, we defined plugin
classes:RestSyncPlugin (ùëÜ1andùê∏1),RestAsyncPlugin (ùëÜ1andùê∏2),Af-
terAnalyzedPlugin (ùëÜ2),andAfterMergedPlugin (ùëÜ3).AsallAfterAna-
lyzedPlugins andAfterMergedPlugins are executed in synchronous,
wedidnotdefine AfterAnalyzedAsyncPlugin andAfterMergedAsync-
Plugin.
ConfigurationManager moduleisalsolocatedinsidethebackend.
Thecomponentmanagessettingsandconfigurationsofeachproject
and service. The configurations are managed by users via CRB
Frontend.CRBFrontend providesauserexperiencefordevelopers
to change preferences of their own projects. A developer can add a
newproject, select services tobe applied, andset in detail options
withasmall amount ofefforts.
3.2 Extensibility
Weintroducean AbstractReviewSystem toimprovetheextensibility
ofreviewsystems.EachreviewsystemhasitsownAPIswithwhich
one can retrieve information about a generalreview request, code
description, or code fragments. Moreover, writing a review com-
mentorcreationofcodechangesuggestioncanbealsoachieved
via the APIs. An abstract review system, which is located inside
CRB backend, is an abstraction of those APIs. It provides common
interfacesand bridges interfaceswith correspondingAPIs of each
reviewsystem.Forinstance,whentheabstractreviewsystemre-
ceives arequest towrite acomment, regardless of review systems,
CRBcan write a comment to the review system by using an API
of the abstract review system. CRBcan easily address new type of
reviewsystemwiththe helpof the abstract reviewsystem.
Nevertheless,somevariationpointsinreviewsystemstillremain.
For instance, rendering comments containing HTML or markdown
maybeunavailableforsomereviewsystemssuchasGerrit.GitHub
providesanAPItoretrievethelistofreviewersbutHelixSwarm
does not. It is important for the abstract review system to judge
1400ESEC/FSE ‚Äô22, November14‚Äì18, 2022,Singapore, Singapore Hyungjin Kim,Yonghwi Kwon,Sangwoo Joh, Hyukin Kwon,Yeonhee Ryou,andTaeksu Kim
CRB Backend
CRB Frontend
Distributed Task Queue Worker
 Adapter
Scheduler
Configuration Manager
Review Systems
Abstract Review System
 Plugins
Services
GitHub
P4/Swarm
Gerrit
Comment Writer
Review Request
Collector
Review Comment
GeneratorReviewability
Code Quality
Potential Defects
 Typo
Coding Style
 Test Coverage
OpenSourceValidation
LOC
 File Count
Task
Generator
Task
Caller
Request
Handler
Response
Handler
Project Manager
 Project Group Manager
 User Account Manager
Request
Review
Architecture Maturity
Figure 1:OverallArchitecture of Code Review Bot
Plugins
RestSyncPlugin
Typo
RestAsyncPlugin
AfterAnalyzedPlugin
DataPlatformPlugin
Test 
Coverage
Architecture
Maturity
Ground Rule
Potential Defects
 Reviewability
OpenSource
Validation
AfterMergedPlugin
 Coding Style
Figure 2:Plugin Architecture
whether the targeted review system supports certain functional-
ity or not. Variations are controlled by an inheritance, and each
instancefiltersoutunavailablefeatures byitself.
Sinceenoughabstractionisapplied,aservicedeveloper,some-
onewhowantstocreateanewservicedoesnothavetoconsider
therepresentationofeachreviewsystem.Shecanconcentrateon
the implementation of a successful service only without any worry
aboutthe userinterface compatibility.Moreover,if SamsungElec-
tronicsadaptsanewreviewsystem,theabstractreviewsystemcan
be appliedto CRBinconvenientway.
3.3 Convenience
CRBprovidesseveralpre-definedanalysesservices.Developerscan
easily configure the optionsofthe services withafewclicks.
InAddition, CRBreturnseachfeedbackinaconsistentwayto
eachreviewsystem.Dependingonthesortoffeedback,analyses
services are as follows:
‚Ä¢Inlinecommentservices Feedbackofsomeservicesare
delivered as inline comments. These services include Po-
tential Defects service,Typoservice, and Open-Source
Validation . Defects detected by each tool are listed, and
CRBindicates each line in which a defect has been found.Developers are able to identify problems and fix them more
quickly with the aid of CRB. Project managers can easily
turnonandoffeachservicethemselvesandsetconfiguration
withlittledifficulty.
‚Ä¢Summarycommentservices TheReviewability service
measureschangemetrics(e.g.thenumberofchangedfiles
orlines,etc.)andprovidesthemtoreviewers. TestCover-
ageservice and Architecture Maturity service both run
unit testing and code metrics measurements and provide
information regarding overall code quality to developers.
It is unfortunate that dynamic and static analysis for the
measurementrequiresmoreeffortfromtheprojectmanager
duringtheconfiguration. CRBlessensthehumaneffortby
providing 1) utility functionality for gathering test results
andtransformingthemintoaunifiedformatformainstream
test frameworks and 2) pre-defined containers that can be
usedfor running the metric measurements.
‚Ä¢Codesuggestionservices Therearesomeservicesthatsug-
gest code changes to resolve certain issues. Coding Style
servicechecksstyleviolations,andsometimescreatesare-
view request to fix those violations. The reason for this is
that many coding style tools reconstruct the entire target
file to comply their style, so inline comment feedback is not
always sufficient for resolving violations. Typoservice also
creates a code fix suggestion and helps developers easily
correctmisspells.
‚Ä¢CI-basedservices Someservicesrequireexecutionenviron-
ments to get their results, which means they need build sys-
tems orContinuousIntegrationsystems.These servicesin-
cludePotentialDefects ,ArchitectureMaturity ,andTest
Coverage services. They uses Docker, Jenkins or GitHub
Actions to build or test the submitted codes. Some divisions
made their own build systems, so CRBtriggers to start by
using the systems. Or, other divisions use CI systems which
are triggered by review systems directly such as GitHub Ac-
tions.Atthattime, CRBdoesnotbehaveanything,butwaits
the results ofCIsystems.
1401UnderstandingAutomatedCode ReviewProcessandDeveloperExperience in Industry ESEC/FSE ‚Äô22, November14‚Äì18, 2022,Singapore, Singapore
3.4 Scalability
One of the most painful points in applying a review automation
system to a large-scale industry is the performance problem by the
lack of scalability. A large-scale project sometimes consumes huge
amount of resources in analyses and execution time explosion may
burden developers. We applied a microservice architecture style to
CRBtoovercomethelimitation.Themicroservicearchitectureis
knownas agoodsolution for ascalablesystem[ 12].
Each services are deployed in physically separated nodes and
communicate with relevant plugins via HTTP protocol. Param-
eters and environments are shared via Redis[17], a distributed
in-memorydatabaseandainvocationschedulingis performedby
Celery[27], a distributed task queue. Each service operation is
processedindependently inparallel.
Deployment based on microservices achieves high level of scala-
bilityinaserviceexpansion.Separationofservicesfromtheplat-
form(i.e. CRBbackend)makesthescalingoutandupinconvenient
way andCRBsuccessfully adapted into Samsung Electronics with-
outperformance problems.
3.5 Status ofUsage
CRBhas been applied in Samsung Electronics for about two years
from2019.Morethaneightthousandreviewrequestsfrom6,439
projectshavebeencreatedduringtheperiodand2.7millionservice
executions happened. As the registration date of each project is
different, to compare the portion of review systems and frequently
executed services, we choose a single month (August 2021) and
calculatedthe statistics.
Swarm 35,724
(43.72%)
Gerrit28,208
(34.52%)GitHub
17,780
(21.76%)
(a)Review Requests
Swarm GitHub Gerrit0100200300400
Type
Add
Remove (b)LineChanges inRequests
0 10,000 20,000 30,000 40,000 50,000 60,000 70,000Typo
Reviewability
Potential Defects
Open Source
Coding Style
Ground Rule
Test Coverage
Archiecture
76,303
75,005
68,764
26,407
7,164
3,373
1,813
937
GitHub
Gerrit
Swarm
(c) Review Requests byServices
Figure 3:Review Request Statistics inAugust2021
Figure3ashows the number of review requests in a month.
Totally81,712reviewrequestswerecreatedandtheaveragerequest
count in a day is 3,714 (22 working days in a month). About 80%
ofrequestswerecreatedfromHelixSwarmandGerrit.Figure 3b
showschangedlinesinareviewrequestduringtheperiod.Median
valuesofadded lines per review requestforHelixSwarm,GitHub,
(a)Comment of Test CoverageServicefromGitHub
(b)Comment of PotentialDefect ServicefromGerrit
(c) Comment of TypoServicefromHelixSwarm
Figure 4:Commentsby Code Review Bot
andGerritare41,29,and26,respectively.Medianvaluesofremoved
linesare 7,7,and8,respectively.
Typo,Reviewability andPotentialDefects arethemostfrequently
used top three services (86 .7%). Number of service calls are illus-
tratedinFigure 3c.Someservicessuchas CodingStyle ,TestCoverage ,
andArchitectureMaturity takeslowerportionthantopfourservices
because the services can be applied to neither Helix Swarm nor
Gerrit.
1402ESEC/FSE ‚Äô22, November14‚Äì18, 2022,Singapore, Singapore Hyungjin Kim,Yonghwi Kwon,Sangwoo Joh, Hyukin Kwon,Yeonhee Ryou,andTaeksu Kim
Examplesofcommentsgeneratedby CRBforeachreviewsystem
are shown in Figure 4. Figure4ais an example of Test Coverage
service,whichshowslinecoverageforbothtotallinesandchanged
lines. Uncovered lines of the review request are emphasized to call
a reviewer‚Äôs attention. Figure 4bshows a comment generated into
aGerritreviewsystemwhichcontainspotentialdefectsinareview
request.ExampleofaHelixSwarmcommentcontainsseveraltypos
isshowninFigure 4c.
4 EMPIRICALSTUDIES
4.1 Research Questions
Weconductedqualitativeandquantitativeanalysestoseehowuse-
fulCodeReviewBot isfordevelopersinSamsungElectronics.We
madeasurveytounderstanddevelopers‚Äôsatisfactionwiththequal-
itativeevaluationandobtaineddatafromcodereviewcomments
from developers and analysis results from CRBfor the quantitative
evaluation.Fromtheresults,weperformedanempiricalstudyto
answer following researchquestions:
‚Ä¢RQ1: How useful is CRBfor developers, and how efficiently
dodevelopers use CRB?
‚Ä¢RQ2:DoesCRBattractdevelopers?Thatis,howmanyde-
fects and alarms from the system are approved and fixed by
developers?
‚Ä¢RQ3:DoesCRBquantitativelyreducetherevieweffort?Is
there significant difference in the amount of code review
written byreviewers before andafter the automation?
4.2 Experiment Setup
To answer RQ1, we conducted a survey from 93 developers in
Samsung Electronics with questionnaires listed in Table 1. The
questionnairesfocusonunderstandingfeedbackaboutasatisfaction
andusage patterns on CRB.
Table 1:QuestionnairesaboutUsageof Code Review Bot
Question
ùëÑ1: Are you satisfiedwith Code Review Bot ?
ùëÑ2:DoesCodeReviewBot effectivelyreducetimeandeffortin
the code reviewactivity?
ùëÑ3:Doyouthinkhowmuchtimeandeffortofthecodereview
are reducedby Code Review Bot ?
ùëÑ4:Doyouthink CodeReviewBot ishelpfulintheimprovement
ofthe code quality?
ùëÑ5: Whichbenefitsdoyou expectwith Code Review Bot ?
ùëÑ6:Whichservicesof CodeReviewBot areyousatisfiedthemost?
Wecountedthenumberoffixeddefectsandalarmswhichare
detectedby CRBforthequantitativeevaluationtoRQ2.A fixrate
of a review request is defined as a ratio of the number of fixed
alarmsoverthenumberofdetectedalarms intherequest.Wecol-
lectedfixratesfromwholereviewrequestswhichhaveoneormore
alarms(e.g.typosandpotentialdefects)overtheentireperiodafter
applying CRB.
Regarding efforts ofreviewers, we collected all reviews written
by reviewers for one year. By calculating the number of reviewsandlengthsofstringsineachcomment,wetriedtoobtainafinding
on whether there are significant changes between before and after
CRBhas been adapted. Review requests were categorized into two
groups. The first group contains review requests thathave at least
one review comment written by CRBand the second group does
not. We measured the average number of review comments per
week.Averagestringlengthsofreviewcommentsfromthesame
reviewerswerealsocalculated.Medianvalueswereusedforaverage
tomitigatedatabiasesbyoutliers.Weperformed Pairedsamplet-
testbetweentwogroupstoanswerRQ3.Tocollectenoughdata,we
targetedreviewers whohave createdcode reviewsinboth groups
for more thantenweeks.
4.3 Effectiveness of CodeReview Bot
We conducted a survey from developers in Samsung Electronics to
understand the satisfaction on CRBand to see the effects of the au-
tomation.Total1,108contributorswerechosenfromprojectswhich
are using CRBand questionnaires about satisfaction, effectiveness,
andexpectationof CRBlistedinTable1weresenttothem.From
93 respondents, responses have been collected, and the response
rateis 8.39%.
Table 2:Responses to Questionnaires
ùëÑ1.I‚Äômsatisfiedwithadapting Code Review Bot to myprojects.
Strongly
agreeAgree Neutral Disagree Strongly
disagree
36(38.7%)34(36.6%)9(9.7%)0(0.0%)1(1.1%)
ùëÑ2.Code ReviewBot helps to reduce time andeffortin code
review.
Strongly
agreeAgree Neutral Disagree Strongly
disagree
38(40.9%)28(30.1%)10(10.8%)3(3.2%)0(0.0%)
ùëÑ4.Code Review Bot helpsto improve codequality .
Strongly
agreeAgree Neutral Disagree Strongly
disagree
37(39.8%)37(39.8%)4(4.3%)1(1.1%)0(0.0%)
Table2showsresponsestoquestionnairesaboutthesatisfaction
withCRB. For a question about theoverallsatisfaction( ùëÑ1), 75.3%
of developers replied positive feedback. On the other hand, only
1.1%ofdevelopers answerednegative opinions.
Next, 71.0% respondents agreed that CRBis helpful to reduce
codereviewtimeandeffort( ùëÑ2)andonly3 .2%disagreed.Forthe
detailedquestionabouttheefficiency( ùëÑ3),36.6%reviewersthought
thatCRBreducesabout10%to30%oftheirreviewtimeand7 .5%
of them answered 30% to 50% of the review time might be reduced.
24.7%votedthatlessthan10%oftherevieweffortcouldbereduced
byCRB. In conclusion, more than 70% of respondents agree that
CRBhelps to save 30% or more efforts in code review. Next, ùëÑ4
asked about the effect of CRBon the code quality, and 79 .6% of
respondentsansweredpositive feedback.
1403UnderstandingAutomatedCode ReviewProcessandDeveloperExperience in Industry ESEC/FSE ‚Äô22, November14‚Äì18, 2022,Singapore, Singapore
0 10 20 30 40 50 60Finding defects
Reducing review effort
Measuring software metrics
Enhancing maintainability
Communication
Others
(a)Expected Impact of Code Review Automation
0 10 20 30 40 50Potential Defect
Typo
Coding Style
Architecture
Test Coverage
Reviewability
(b)Most EffectiveServices
Figure 5:Expectation andServicesUsage
From the result, we can make a conclusion that developers in
Samsung Electronics are adapting CRBwith a large amount of
satisfaction. Especially, reviewers agree that CRBmay efficiently
reduce code revieweffortsandimproves the code quality.
Wecollectedadditionalresponsestoinspectdevelopers‚Äôexpecta-
tionstoacodereviewautomationandtheirusagepattern.Bacchelli
and Bird showed that developers expect defect detection, code im-
provement,andfindingbettersolutionsfromthecodereview[ 4]
and Wessel et al. found that developers are also willing to get code
review metrics and additional information such as analysis results
(without having to go to another tool) followed by reducing main-
tenanceeffortfrom acode reviewautomation tool[ 30].
Respondentsexpectedseveralbenefitsfrom CRB(ùëÑ5)aslisted
inFigure 5. We can see that similar results have been shown with
priorstudies,and CRBcoversmanypartsofdevelopers‚Äôexpecta-
tions.Finally,developerschoosethemostusefulservices( ùëÑ6)inthe
followingorder: PotentialDefects ,Typo,CodingStyle ,Architecture
Maturity, andTest Coverage . Responses indicate that developers
and reviewers are satisfied with code review automation in quality
assurances andmaintainability.
RQ1. More than 70% developers are satisfying with CRBes-
pecially in the context of code quality improvement. Most
of them feel that about 30% of code review effort may be
reducedwiththe helpofcode reviewautomation.
4.4 Motivating Quality Improvementby
Automation
RegardingRQ2,weperformeda quantitativeanalysis oncodefixes
related to defects and alarms found by CRB. By answering about
thequestion,whether developersaremotivatedbyautomationor
not, we maysee the attractioneffectof CRB.
Table3and4showfixratesoftyposandpotentialdefectsfound
during a month, August 2021. Fix rate of typos from Swarm isTable 3:Fix RateofTypos
System Fixed Found Fix rate Found‚Ä†Fix rate‚Ä†
GitHub 793 14 ,384 5 .51% 1 ,092 72 .62%
Swarm 3 ,611 11,604 31 .12% 6 ,324 57 .10%
Gerrit 556 2 ,441 22 .78% 937 59 .34%
Total 4 ,960 28,429 17 .45% 8 ,853 59 .38%
Found‚Ä†=Adjusted detected typocounts,Fix rate‚Ä†=Adjusted fix rate
31.12% and the rate from Gerrit is 22 .78%. On the other hand, fix
rate from GitHub is only 5 .51% ≈õ 793 requests over 14,384 requests
contain typosdetectedby CRB.
Wefoundthatdeveloperstendtoskipthere-executioninthecase
oftypo.Developersdonotfeelanecessitytoperformtypochecking
again because fixing the alarms are easy, clear, and there will be
littlepossibilityofintroducingsideeffects.However,sincepotential
defectsdetectedbyacertainstaticanalysistool,developersareused
to explicitly run CRBagain to confirmthe result from the tool.
To complement the difference among review systems and ser-
vices, we filtered out analysis results when CRBwere executed just
onceinareviewrequest.Found‚Ä†andFixrate‚Ä†ofTable3indicate
the adjusted total number of typo and the adjusted fix rate, respec-
tively. Adjusted fix rate of GitHub is 72 .62% but adjusted fix rate of
the other review systems are lower than GitHub. For all the review
systems, we found that at least half of typo-detected requests were
fixed.
Table 4:Fix RateofPotential Defects
System Fixed Found Fix rate Found‚Ä†Fix rate‚Ä†
GitHub 5 ,571 11,375 48 .98% 5 ,806 95 .95%
Swarm 3 ,745 11,059 33 .86% 6 ,094 61 .45%
Gerrit 1 ,067 2,026 52 .67% 1 ,394 76 .54%
Total 10 ,383 24,460 42 .45% 13 ,294 78 .10%
Found‚Ä†=Adjusted detected typocounts,Fix rate‚Ä†=Adjusted fix rate
Inthecaseofpotentialdefects,theadjustedfixrateofGitHub,
Swarm, and Gerrit are 95 .95%, 61.45%, and 76 .54%, respectively.
Theaspectissimilarwiththecaseoftyposbutadjustedfixrates
are higher than values of typos. The adjusted fix rate of GitHub
showsveryhighvalueof95 .95%.Thatis,developersalwaystrigger
CRBagain after they fixthe potentialdefects.
RQ2. Developers are strongly motivated to improve code
quality from CRB. More than 60% defects andwarnings are
immediately fixedbydevelopers.
4.5 EncouragingReviewers
To investigate RQ3, we collected two kinds of data ≈õ 1) average
counts of code review comments by the weekly and 2) average
lengthofcomments≈õduringayearfromSeptember2020toAugust
2021.Weperformedastatisticalhypothesistestingbyusingthedata
asasampleandestimatethe fluctuationsofthemeanofthevalues.
Wesetupanullhypothesis( ùêª0)andanalternativehypothesis( ùêª1)
as follows:
1404ESEC/FSE ‚Äô22, November14‚Äì18, 2022,Singapore, Singapore Hyungjin Kim,Yonghwi Kwon,Sangwoo Joh, Hyukin Kwon,Yeonhee Ryou,andTaeksu Kim
‚Ä¢ùêª0: After adapting CRB, there is no difference in the code
reviewsize.
‚Ä¢ùêª1: After adapting CRB, there is asignificant differencein
the code reviewsize.
Apaired sample t-test with 95% confidence level was used for
the hypothesistesting.
Table 5:Differences inCodeReview after Applying CRB
(a)Review SizeMetrics
System By ùëÅ ùúá 0ùúé0ùúá1ùúé1
GitHub C 1 ,952 14.94 208 .66 12.90 60.05
Swarm C 815 13 .62 119 .19 9.36 64.79
Gerrit C 625 107 .00 1,900.80 26.70 234.90
GitHub L 1 ,952 19.20 61 .08 21.83 76.07
Swarm L 815 109 .86 159 .71 100.44 149.99
Gerrit L 625 31 .95 39 .33 31.81 26.11
C=By number of reviewcomments
L =By string length of reviewcomments
ùúá0,ùúé0=Beforeusing CRB,ùúá1,ùúé1=Afterusing CRB
(b)Paired Differences
System By PDM PDSD 95% CI ùë° ùëù
GitHub C ‚àí2.04 198 .56 (‚àí10.85,6.78)‚àí0.45 0 .651
Swarm C ‚àí4.26 97 .12 (‚àí10.93,2.42)‚àí1.25 0 .211
Gerrit C ‚àí80.40 1,832.60 (‚àí224.30,63.60)‚àí1.10 0 .273
GitHub L 2 .63 62 .06 (‚àí0.12,5.39) 1 .87 0 .061
Swarm L ‚àí9.42 86 .89 (‚àí15.39,‚àí3.44)‚àí3.09‚àó‚àó0.002
Gerrit L ‚àí0.14 17 .24 (‚àí1.50,1.21)‚àí0.21 0 .836
C=By number of reviewcomments,L =By string length of reviewcomments
PDM, PDSD=Meanand Standard deviation of paired differences
‚àóùëù<0.05,‚àó‚àóùëù<0.01
Table 5shows the paired sample t-test result of the difference in
means between before and after adopting CRB, including medians
of average comment counts by weekly and medians of comment
lengthsbyreviewers.Onlyonehypothesis ùêª0canberejectedfor
all review systems ≈õ Swarm & Length case. In other words, except
the one case, there are no significant differences whether CRBis
appliedornot. Table6isadetailedresultforreviewsystemservers
≈õ two GitHub servers, four Swarm servers, and two Gerrit servers.
Interestingly,abouthalfofallcasesreject ùêª0.AllrejectedLcases
haveapositivet-value.Thatis,adapting CRBaffectsreviewersto
writelongercomments.
Actually, one of the main reasons why average counts of review
comments changed is infected by policies ≈õ what kinds of activi-
tiesshouldbeperformedduringthecodereviewphase≈õofeach
development division. Regardless of adapting CRB, reviewers may
review code more and more when any activities are conducted for
them.Table6: Differences in Code Review after Applying CRBfor
Each Review System
Server System By ùëÅPDM PDSD ùë° ùëù
ùê∫ùêª1GitHub C 1 ,863 4 .50 38 .73 5.02‚àó‚àó0.000
ùê∫ùêª2GitHub C 89 ‚àí138.90 906 .90‚àí1.44 0 .152
ùëÜùëä1Swarm C 498 ‚àí7.59 117 .14‚àí1.44 0 .151
ùëÜùëä2Swarm C 136 9 .45 34 .13 3.23‚àó‚àó0.002
ùëÜùëä3Swarm C 155 ‚àí5.67 62 .35‚àí1.13 0 .259
ùëÜùëä4Swarm C 26 ‚àí3.54 6 .09‚àí2.96‚àó‚àó0.007
ùê∫ùëÖ1Gerrit C 561 ‚àí87.70 1,934.30‚àí1.07 0 .283
ùê∫ùëÖ2Gerrit C 64 ‚àí15.87 48 .03‚àí2.64‚àó0.010
ùê∫ùêª1GitHub L 1 ,863 2 .54 63 .44 1.73 0 .084
ùê∫ùêª2GitHub L 89 4 .62 15 .11 2.88‚àó‚àó0.005
ùëÜùëä1Swarm L 498 ‚àí16.12 102 .28‚àí3.52‚àó‚àó0.000
ùëÜùëä2Swarm L 136 5 .21 14 .27 4.25‚àó‚àó0.000
ùëÜùëä3Swarm L 155 ‚àí0.13 16 .22‚àí0.10 0 .923
ùëÜùëä4Swarm L 26 ‚àí12.90 180 .40‚àí0.36 0 .719
ùê∫ùëÖ1Gerrit L 561 ‚àí0.53 17 .82‚àí0.71 0 .480
ùê∫ùëÖ2Gerrit L 64 3 .26 10 .37 2.51‚àó0.015
C=By number of reviewcomments,L =By string length of reviewcomments
PDM, PDSD=Meanand Standard deviation of paired differences
‚àóùëù<0.05,‚àó‚àóùëù<0.01
RQ3. We can not say that CRBreduces code review effort
dramatically.Some development divisionshows significant
differences in the code review size but the others do not.
Reviewers usually tend to affect by the policies and develop-
ment release cycle of their development division more than
the automation.
5 DISCUSSION AND THREATS TO VALIDITY
Although CRBprovides unified interfaces for numerous review
systems, variancesinrendering review commentsbyeachreview
system are still remain. Reviewers might be more attracted by a
prettywrittenreviewcommentwithMarkdownorHTMLmarkups.
ReactivenessfromGitHubmaybehigherthanotherreviewsystems
becausetheyonlyallowaplaintextorasimplemarkup.Itwould
beaninterestingresearchtopiconwhatkindsofdifferencesamong
reviewsystemsarisebecause of the presentation divergence.
Thereisaninternalvalidityinmeasuringthequalityimprove-
ment motivation by CRBat section 4.4. In this study, we compared
thechangeofdetecteddefectcountsafterapplying CRBtoshow
howeffectively CRBattracteddevelopers,butdevelopersofsome
projects already use several kinds of static analysis tools them-
selves. Thus, the evaluation results may not reflect the direct effect
ofCRBintheviewofqualityassurance.Differentenvironmentsand
buildsystemsofeachprojectsmakeobstaclestoinvestigatehow
many developers start to use code analysis tools newly, not CRB.
Nevertheless, we still believe that CRBsuccessfully attracts quality
improvement of source code because the convenience of CRBobvi-
ouslyaidsdeveloperstoapplyanalysistoolsfortheirprojectsno
matter whether they have already run some tools themselves or
not.
Alotoffactorssuchasfunctionalitiesofacertainreviewsystem,
development process, policies of a team, release plans or human
resourceorganizationmightinfluenceonthequalityofsourcecode
1405UnderstandingAutomatedCode ReviewProcessandDeveloperExperience in Industry ESEC/FSE ‚Äô22, November14‚Äì18, 2022,Singapore, Singapore
or code review manners. The fact causes a threat to the internal
validity.Inthestudycase,wegathereddatawithaconsideration
onnotonlyreviewsystemsbutalsodevelopmentteamsandbusi-
nessdivisions.Respondentsareselectedfromvariousdevelopment
teamsfrommiscellaneoussoftwaredomainstoovercomethethreat.
Nevertheless, it is not able to guarantee that all research results in
section 4 are not biasedbecause ofthe factors.
The architecture of CRBinsubsection 3.1 is generic and ab-
stractedenoughtobeextendedexternally.Despitetheextensibility
of the architecture, it is not easy to directly apply CRBto open
sourceprojectsorotherindustriesratherthanSamsungElectron-
ics. Available services and schedulers need to be selected and built
basedontheconsiderationoftheorganization‚Äôsculture,policy,and
circumstance. Besides, as feedback from users also may be vary
becauseneedsofeachdevelopteam.ThecasestudyfromSamsung
Electronics may differ to the open source communities or other
industrialcases.
6 RELATED WORK
Static analysis is an efficient method to find out defects, and apply-
ing static analyzers in review automation is effective to improve
productivity. Singh et al. showed that about 16% of code review
comments can be covered by static analysis tool [ 26]. From the
caseofGoogle,Sadowskietal.provedthatprovidingstaticanalysis
results to developers in the code review phase is better than the
testphasebecausedevelopersarenotwillingtomodifymatured
code after testing and release [ 21]. Facebook also applies a static
analysisInferin the review process [ 8]. Developers on Facebook
were opened to fix the alarms and 70% of them were solved [ 10].
Static analyses are essential for code review automation, and Code
Review Bot has been foundedonstaticanalyses.
Code review automation, named a Bot, is also widely adopted in
open source communities [ 29]. Balachandran suggested ReviewBot ,
a review automation system based on static analyses, and success-
fullyshowedthefeasibilityofadaptingtotherealworld[ 5,6].In
this study, we developed Code Review Bot that provides several
services based on the static analysis, testing, and fix suggestions
andfoundits usefulnessinthe industrialarea.
Googleimplemented Tricoder,anovelsourcecodeanalysisframe-
work [23] and melted it into their workflow [ 23]. Microsoft also
combines several quality assurance tools into a framework and
performsanalysiswith CodeFlow [7].Rosie[19]isafullyautomated
review assistant that traverses whole repositories by itself and
cleansupthecodebaseandfixesdefectsautomaticallyinGoogle.
Code Review Bot takes a role of quality assurance automation in
SamsungElectronicsbasedondevelopers‚Äô reviewrequests.
Defect prediction is one of the most important next research
topics ofCode Review Bot .JITBot[15] successfully showed the fea-
sibilityofjust-in-timedefectprediction[ 14]inpracticebyadapting
into GitHub. One of the most interesting and impressive contri-
butions of JITBotis providing reasons to users with the help of
explainableartificialintelligence[ 1].Explanationand description
ofapredictiontoend-usersareimportantfortoolsaimingatreal
industryareas.Forthisreason,thestudygaveusaninspirationnot
only in the preparation of future work but also in the design of the
userexperience of Code Review Bot .Several studies on review automation using machine learning
havebeenperformed.Asthanaetal.suggestedanovelreviewerrec-
ommendation automation based on a balance between commit and
reviewhistory[ 2].SainiandBrittoshowedacaseinEricssonon
theapplicationofreviewrequestprioritizationbasedonamachine
learning [ 24]. Tufano et al. experimented with deep learning tech-
niques in the context of automating reviewer recommendations
and program synthesis to assist reviewers [ 28]. State-of-the-art
techniquesindicateadirectionofthenextstepof CodeReviewBot .
7 CONCLUSION
We introduced a unified code review automation system, Code
Review Bot . We have achieved extensibility and scalability for intri-
cate,various,anddiversedevelopmentenvironmentsinSamsung
Electronics by using our abstract review system. We have been
operating the system over two years for more than 10,000 projects,
andsharedsomelessons learned.
We investigated both quantitative and qualitative analyses on
satisfaction,usage,andeffectiveness.Mostofthe developerswere
satisfiedwith CodeReviewBot .Qualityassuranceserviceswerethe
mosthelpfulone,anddevelopersrapidlyandactivelyrespondedto
more than60% ofreviews.
Webelievethatthecodereviewautomationhelpsnotonlyforthe
quality of source code, but also for the rapid and worthwhile code
review activities of reviewers. We plan to study on the correlation
amongservices,reviewsystems,andusersreactions.Wehopeto
discoversomeinsightsformoreeffectivecodereviewinthefuture.
ACKNOWLEDGMENT
We would like to thank the anonymous reviewers, Yoonki Song,
Geunsik Lim, JaeHyun Yoo, Youil Kim, and Joonbae Park, for their
insightfulcommentsandfeedbackthathelpedimprovethepaper.
This work was supported in part by Intelligent Dev. Assistant &
Quality Tool Development (RAJ0122ZZ-35RF), Samsung Research,
SamsungElectronicsCo.,Ltd.
REFERENCES
[1]AlejandroBarredoArrieta,NataliaD√≠az-Rodr√≠guez,JavierDel Ser,AdrienBen-
netot,SihamTabik,AlbertoBarbado,SalvadorGarc√≠a,SergioGil-L√≥pez,Daniel
Molina, RichardBenjamins, RajaChatila,andFranciscoHerrera.2019. Explain-
ableArtificialIntelligence(XAI):Concepts,Taxonomies,OpportunitiesandChal-
lengestoward ResponsibleAI. arXiv: 1910.10045 [cs.AI]
[2]SumitAsthana,RahulKumar,RanjitaBhagwan,ChristianBird,ChetanBansal,
Chandra Maddila, Sonu Mehta, and B. Ashok. 2019. WhoDo: Automating Re-
viewerSuggestionsatScale.In Proceedingsofthe201927thACMJointMeetingon
EuropeanSoftware Engineering Conference and Symposium onthe Foundations of
Software Engineering (Tallinn, Estonia) (ESEC/FSE 2019) . Association for Comput-
ingMachinery,NewYork,NY,USA,937≈õ945. https://doi.org/10.1145/3338906.
3340449
[3]NathanielAyewah,WilliamPugh,J.DavidMorgenthaler,JohnPenix,andYuQian
Zhou. 2007. Evaluating Static Analysis Defect Warnings on Production Software.
InProceedings of the 7th ACM SIGPLAN-SIGSOFT Workshop on Program Analysis
for Software Tools and Engineering (San Diego, California, USA) (PASTE ‚Äô07) .
Association for Computing Machinery, New York, NY, USA, 1≈õ8. https://doi.
org/10.1145/1251535.1251536
[4]AlbertoBacchelliandChristianBird.2013. Expectations,Outcomes,andChal-
lengesofModernCodeReview.In Proceedingsofthe2013InternationalConference
on Software Engineering (San Francisco, CA, USA) (ICSE ‚Äô13) . IEEE Press, NY,
USA,712≈õ721.
[5]Vipin Balachandran. 2013. Fix-it: An extensible code auto-fix component in
ReviewBot.In 2013IEEE13thInternationalWorkingConferenceonSourceCode
AnalysisandManipulation(SCAM) .IEEEComputerSociety,NY,USA,167≈õ172.
https://doi.org/10.1109/SCAM.2013.6648198
1406ESEC/FSE ‚Äô22, November14‚Äì18, 2022,Singapore, Singapore Hyungjin Kim,Yonghwi Kwon,Sangwoo Joh, Hyukin Kwon,Yeonhee Ryou,andTaeksu Kim
[6]VipinBalachandran.2013. ReducingHumanEffortandImprovingQualityinPeer
Code Reviews Using Automatic Static Analysis and Reviewer Recommendation.
InProceedings of the 2013 International Conference on SoftwareEngineering (San
Francisco,CA,USA) (ICSE‚Äô13) .IEEEPress,NY,USA,931≈õ940. https://doi.org/
10.1109/ICSE.2013.6606642
[7]ChristianBird,TrevorCarnahan,andMichaelaGreiler.2015. LessonsLearned
from Building and Deploying a Code Review Analytics Platform. In Proceedings
ofthe12thWorkingConferenceonMiningSoftwareRepositories (Florence,Italy)
(MSR ‚Äô15) . IEEE Press,NY, USA,191≈õ201. https://doi.org/10.1109/MSR.2015.25
[8]Cristiano Calcagno, Dino Distefano, Jeremy Dubreil, Dominik Gabi, Pieter
Hooimeijer, Martino Luca, Peter O‚ÄôHearn, Irene Papakonstantinou, Jim Pur-
brick,andDulmaRodriguez.2015. MovingFastwithSoftwareVerification.In
NASA Formal Methods , Klaus Havelund, Gerard Holzmann, and Rajeev Joshi
(Eds.).SpringerInternational Publishing,Cham,3≈õ11.
[9]Shyam R. Chidamber and Chris F. Kemerer. 1991. Towards a Metrics Suite
for Object Oriented Design. In Conference Proceedings on Object-Oriented Pro-
gramming Systems, Languages, and Applications (Phoenix, Arizona, USA) (OOP-
SLA‚Äô91).AssociationforComputingMachinery,NewYork,NY,USA,197≈õ211.
https://doi.org/10.1145/117954.117970
[10]DinoDistefano,ManuelF√§hndrich,FrancescoLogozzo,andPeterW.O‚ÄôHearn.
2019. Scaling Static Analyses at Facebook. Commun. ACM 62, 8 (July 2019),
62≈õ70.https://doi.org/10.1145/3338112
[11] GitHub. 2021. GitHub. Retrieved from https://github.com/ .
[12]PooyanJamshidi,ClausPahl,NaborC.Mendon√ßa,JamesLewis,andStefanTilkov.
2018. Microservices:TheJourneySoFarandChallengesAhead. IEEESoftware
35,3 (2018), 24≈õ35. https://doi.org/10.1109/MS.2018.2141039
[13]Brittany Johnson, Yoonki Song, Emerson Murphy-Hill, and Robert Bowdidge.
2013. WhyDon‚ÄôtSoftwareDevelopersUseStaticAnalysisToolstoFindBugs?.
InProceedings of the 2013 International Conference on SoftwareEngineering (San
Francisco, CA, USA) (ICSE‚Äô13) . IEEE Press,NY, USA,672≈õ681.
[14]Yasutaka Kamei, Emad Shihab, Bram Adams, Ahmed E. Hassan, Audris Mockus,
Anand Sinha, and Naoyasu Ubayashi. 2013. A large-scale empirical study of
just-in-timequalityassurance. IEEETransactionsonSoftwareEngineering 39,6
(2013), 757≈õ773. https://doi.org/10.1109/TSE.2012.70
[15]Chaiyakarn Khanan, Worawit Luewichana, Krissakorn Pruktharathikoon, Ji-
rayusJiarpakdee,ChakkritTantithamthavorn,MorakotChoetkiertikul,Chaiy-
ong Ragkhitwetsagul, andThanwadee Sunetnanta.2020. JITBot: An Explainable
Just-in-TimeDefectPredictionBot . AssociationforComputingMachinery,New
York, NY, USA,1336≈õ1339. https://doi.org/10.1145/3324884.3415295
[16]GeunsikLim,MyungJooHam,JijoongMoon,andWookSong.2021. LightSys:
LightweightandEfficientCISystemforImprovingIntegrationSpeedofSoftware.
In2021IEEE/ACM43rdInternationalConferenceonSoftwareEngineering:Software
EngineeringinPractice(ICSE-SEIP) .IEEEPress,NY, USA,1≈õ10. https://doi.org/
10.1109/ICSE-SEIP52600.2021.00009
[17] Redis Ltd.2021. Redis. Retrieved from https://redis.io/ .[18] Perforce. 2021. Helix Swarm:FreeCodeReviewTool ForHelixCore. Retrieved
fromhttps://www.perforce.com/products/helix-swarm/ .
[19]Rachel Potvinand Josh Levenberg. 2016. WhyGoogle StoresBillions ofLines
of Code in a Single Repository. Commun.ACM 59,7(June 2016),78≈õ87. https:
//doi.org/10.1145/2854146
[20]Gerrit Code Review. 2021. Gerrit Code Review. Retrieved from https://www.
gerritcodereview.com/ .
[21]CaitlinSadowski,EdwardAftandilian,AlexEagle,LiamMiller-Cushon,andCiera
Jaspan. 2018. Lessons from building static analysis tools at google. Commun.
ACM61,4 (2018), 58≈õ66.
[22]Caitlin Sadowski, Emma S√∂derberg, Luke Church, Michal Sipko, and Alberto
Bacchelli. 2018. Modern Code Review: A Case Study at Google. In Proceedings of
the 40th International Conference onSoftware Engineering:Software Engineering
inPractice (Gothenburg,Sweden) (ICSE-SEIP‚Äô18) .AssociationforComputingMa-
chinery, New York, NY, USA, 181≈õ190. https://doi.org/10.1145/3183519.3183525
[23]CaitlinSadowski,JeffreyvanGogh,Ciera Jaspan,EmmaS√∂derberg,andCollin
Winter. 2015. Tricorder: Building a Program Analysis Ecosystem. In Proceedings
of the 37th International Conference on Software Engineering - Volume 1 (Florence,
Italy)(ICSE‚Äô15) . IEEE Press,NY, USA,598≈õ608.
[24]Nishrith Saini and Ricardo Britto. 2021. Using Machine Intelligence to Prioritise
CodeReviewRequests.In 2021IEEE/ACM43rdInternationalConferenceonSoft-
wareEngineering:SoftwareEngineeringinPractice(ICSE-SEIP) .IEEEPress,NY,
USA,11≈õ20. https://doi.org/10.1109/ICSE-SEIP52600.2021.00010
[25]Mary Shaw and David Garlan. 1996. Software Architecture: Perspectives on an
EmergingDiscipline . Prentice-Hall,Inc.,USA.
[26]Devarshi Singh, Varun Ramachandra Sekar, Kathryn T. Stolee, and Brittany
Johnson.2017. Evaluatinghowstaticanalysistoolscanreducecoderevieweffort.
In2017 IEEE Symposium on Visual Languages and Human-Centric Computing
(VL/HCC) .IEEE,NY,USA,101≈õ105. https://doi.org/10.1109/VLHCC.2017.8103456
[27] Ask Solem.2018. Celery-DistributedTaskQueue. Retrievedfrom https://docs.
celeryproject.org/ .
[28]Rosalia Tufano, Luca Pascarella, Michele Tufano, Denys Poshyvanyk, and
Gabriele Bavota. 2021. Towards Automating Code Review Activities. In 2021
IEEE/ACM 43rd International Conference on Software Engineering (ICSE) . IEEE
Press,NY, USA,163≈õ174. https://doi.org/10.1109/ICSE43902.2021.00027
[29]Mairieli Wessel, Bruno Mendes de Souza, Igor Steinmacher, Igor S. Wiese,
Ivanilton Polato, Ana Paula Chaves, and Marco A. Gerosa. 2018. The Power
of Bots: Characterizing and Understanding Bots in OSS Projects. Proc. ACM
Hum.-Comput. Interact. 2, CSCW, Article 182 (Nov. 2018), 19 pages. https:
//doi.org/10.1145/3274451
[30]Mairieli Wessel, Alexander Serebrenik, Igor Wiese, Igor Steinmacher, and
MarcoA.Gerosa.2020. WhattoExpectfromCodeReviewBotsonGitHub?A
Survey with OSS Maintainers. In Proceedings of the 34th Brazilian Symposium on
SoftwareEngineering (Natal,Brazil) (SBES‚Äô20) .AssociationforComputingMa-
chinery, New York, NY, USA, 457≈õ462. https://doi.org/10.1145/3422392.3422459
1407