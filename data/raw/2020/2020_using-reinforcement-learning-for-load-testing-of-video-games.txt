Using Reinforcement Learning for
Load Testing of Video Games
Rosalia Tufano
SEART @ Software Institute
Università della Svizzera italiana
SwitzerlandSimone Scalabrino
STAKE Lab
University of Molise
ItalyLuca Pascarella
SEART @ Software Institute
Università della Svizzera italiana
Switzerland
Emad Aghajani
SEART @ Software Institute
Università della Svizzera italiana
SwitzerlandRocco Oliveto
STAKE Lab
University of Molise
ItalyGabriele Bavota
SEART @ Software Institute
Università della Svizzera italiana
Switzerland
ABSTRACT
Differentfromwhathappensformosttypesofsoftwaresystems,
testing video games has largely remained a manual activity per-
formed by human testers. This is mostly due to the continuous
andintelligentuserinteractionvideogamesrequire.Recently,rein-
forcementlearning(RL)hasbeenexploitedtopartiallyautomate
functional testing. RL enables training smartagents that can even
achieve super-human performance in playing games, thus being
suitable to explore them looking for bugs. We investigate the pos-
sibilityofusingRLforloadtestingvideogames.Indeed,thegoal
ofgametestingisnotonlytoidentifyfunctionalbugs,butalsoto
examinethegame’sperformance,suchasitsabilitytoavoidlags
andkeepaminimumnumberofframespersecond(FPS)whenhigh-
demanding3Dscenesareshownonscreen.Wedefineamethod-ology employing RL to train an agent able to play the game as a
human while also trying to identify areas of the game resulting in
adropofFPS.Wedemonstratethefeasibilityofourapproachon
threegames.Twoofthemareusedasproof-of-concept,byinjecting
artificial performance bugs. The third one is an open-source 3D
gamethatweloadtestusingthetrainedagentshowingitspotential
to identify areas of the game resulting in lower FPS.
CCS CONCEPTS
•Softwareanditsengineering →Softwaremaintenancetools .
KEYWORDS
Reinforcement Learning, Load Testing
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation
on the first page. Copyrights for components of this work owned by others than ACMmustbehonored.Abstractingwithcreditispermitted.Tocopyotherwise,orrepublish,topostonserversortoredistributetolists,requirespriorspecificpermissionand/ora
fee. Request permissions from permissions@acm.org.
ICSE ’22, May 21–29, 2022, Pittsburgh, PA, USA
© 2022 Association for Computing Machinery.
ACM ISBN 978-1-4503-9221-1/22/05...$15.00
https://doi.org/10.1145/3510003.35106251 INTRODUCTION
The video game market is expected to exceed $200 billion in value
in2023[11].Insuchacompetitivemarket,releasinghigh-quality
gamesand,consequently,ensuringagreatuserexperience,isfunda-
mental. However, the unique characteristics of video games (from
hereon, games) make their quality assurance process extremely
challenging. Indeed, besides inheriting the complexity of software
systems,gamesdevelopmentandmaintenancerequireadiverseset
ofskillscoveredbymanystakeholders,includinggraphicdesigners,
story writers, developers, AI (Artificial Intelligence) experts, etc.
Also, games can hardly benefit from testing automation tech-
niques [36], since even just exploring the total space available in a
givengamelevelrequiresan intelligent interactionwiththegameit-
self.Forexample,inaracinggame,identifyingabugthatmanifests
whenthefinishlineiscrossedrequiresaplayerabletosuccessfully
drivethecarforthewholetrack(i.e., requirestheabilitytodrive
the car). Thus, random exploration is not a viable option here.
Therefore, it comes without surprise that game testing is largely
a manual process. Zheng et al.[50] report that 30 human testers
were employed for testing one of the games used in their study.
Also,thechallengesintestinggameshavebeenstressedbyLin etal.
[33],whoshowedthat80%ofthe50populargamestheystudied
have been subject to urgent updates.
To support developers with game testing, researchers have pro-
posed several techniques. These include approaches to test the sta-
bilityofgameservers[ 18,19,29],model-basedtesting[ 28]using
domain modeling for representing the game and UML state ma-
chinesforbehavioralmodeling,aswellastechniquesspecifically
designed for testing board games [ 21,41]. When looking at recent
techniquesaimedatproposingmoregeneraltestingframeworks,
those exploiting Reinforcement Learning (RL) are on the rise. This
isduetotheremarkableresultsachievedbyRL-basedtechniques
inplayinggameswithsuper-humanperformancereportedinthe
literature [13, 15, 26, 34, 35, 47].
RL is a machine learning technique aimed to train smartagents
able to interact with a given environment (e.g., a game) and to take
decisions to achieve a goal (e.g., win the game). RL is based on the
simple idea of trial and error: The agent performs actions in theenvironment (of which it only has a partial representation) and
receivesa rewardthatallowsittoassessitspastactions/behavior
with respect to the desired goal.
23032022 IEEE/ACM 44th International Conference on Software Engineering (ICSE)
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:59:43 UTC from IEEE Xplore.  Restrictions apply. ICSE ’22, May 21–29, 2022, Pittsburgh, PA, USA Rosalia Tufano, Simone Scalabrino, Luca Pascarella, Emad Aghajani, Rocco Oliveto, and Gabriele Bavota
Recently, researchers started using RL not only to play games
but also to test them and, in general, to improve their quality. The
commonideabehindtheseapproachesistoreducethehumaneffort
in playtesting using intelligent agents. RL-based agents have been
used to help game designers in balancing crucial parameters of the
game (e.g., power-up item effects) [ 37,49,51] and in testing the
gamedifficulty[ 25,43].Also,RL-basedagentshavebeenusedto
look for bugs in games [14, 16, 38, 50].
While agents are usually trained to play a game with the goal of
winning,theaforementionedworkstrainedtheagenttonotonly
advanceinthegamebutalsotoexploreittosearchforbugs.For
example, Ariyurek et al.[14] combine RL and Monte Carlo Tree
Search to find issues in the behavior of a game, given its design
constraints and game scenario graph. The ICARUSframework [ 38]
isabletoidentifycrashesandblockersbugs(e.g., thegamegetstuck
for a certain amount of time) while the agent is playing. Similarly,
theapproachbyZheng etal.[50],alsoexploitingRL,canidentify
bugs spotted by the agent during training (e.g., crashes). While
theseapproachespioneeredtheuseofRLforgametesting,theyare
mostly aimed at testing functional (e.g., finding crashes) or design-
related (e.g., level design) aspects. However, these are not the only
types of bug developers look for in playtesting. In a recent survey,
Politowski et al.[39] reported that for two out of the five games
theyconsidered(i.e.,LeagueofLegends byRiotand SeaofThieves
by Rare) developers partially automated game performance checks
(e.g.,frame-rate). Similarly, Naughty Dog used specialized profiling
tools1for finding which parts of a given scene caused a drop in
the number of frames per second(FPS) in The Last of Us. Truelove
etal.[45]reportthatgamedevelopersagreethat Implementation
responseproblems may severely impact the game experience.
Despitesuchastrongevidenceabouttheimportanceofdetecting
performanceissuesinvideogames,tothebestofourknowledgeno
previousworkintroducedautomatedapproachesforloadtesting
videogames.Wepresent RELINE(Reinforcementl Earningfor Load
testINg gamEs), an approach exploiting RL to train agents able to
play a given game while trying to load test it with the goal of
minimizing its FPS. The agent is trained using a rewardfunction
enclosing two objectives: The first aims at teaching the agent how
to advance in the game. The second rewards the agent when itmanages to identify areas of the game exhibiting low FPS. Theoutput of RELINEis a report showing areas in the game being
negative outliers in terms of FPS, accompanied by videos of thegameplays exhibiting the issue. Such “reports” can simplify theidentification and reproduction of performance issues, that areoftenreportedinopen-source3Dgames(see e.g.,[
1,4,5,7])and
that, in some cases, are challenging to reproduce (see e.g.,[3, 6]).
We experiment RELINEwith three games. The first two are
simple2Dgamesthatweuseasaproof-of-concept.Inparticular,
weinjectedinthegamesartificial“performancebugs”[ 22]tocheck
whether the agent is able to spot them. We show that the agent
trained using RELINEcan identify the injected bugs more often
than(i)arandomagent,and(ii)aRL-basedagentonlytrainedto
playthegame.Then,weuse RELINEtoloadtestanopen-source3D
game[44],showingitsabilitytoidentifyareasofthegamebeing
negative outliers in terms of FPS.
1https://youtu.be/yH5MgEbBOps?t=3494
Game
Info
sτ 
aτ 
rτ 
 Experience
FPS
Info
Reward Function
RL-model Gamestate
reward
action1 2
3 4
Figure 1: RELINEoverview
2 RL TO LOAD TEST VIDEO GAMES
In this section we explain, from an abstract perspective, the idea
behindRELINE. We describe in the study designs how we instanti-
atedRELINEtothedifferentgamesweexperimentwith(e.g., details
about the adopted RL models).
RELINErequires three main components: the gameto load test,
aRL model, representing the agent that must learn how to play the
gamewhileloadtestingit,anda rewardfunction,usedtorewardthe
agentsothatitcanevaluatetheworthofitsactionsforreaching
thedesiredgoal(i.e., playingwhileloadtesting).The RLmodel is
trainedthroughthe4-steploopdepictedinFig.1(seethecircled
numbers). The continuous lines represent steps performed at each
iteration of the loop, while the dashed ones are only performed
afterafirstiterationhasbeenrun(i.e., aftertheagentperformed
at least one action in the game). When the first episode (i.e., a run
ofthegame)ofthetrainingstarts(step1),ateachtimestep τthe
gameprovidesitsstate sτ.Suchastatecanbe,forexample,aset
of frames or a numerical vector representing what is happening in
thegame(e.g., theagent’sposition).The RLmodel takesasinput sτ
(step2)andprovidesasoutputtheaction aτtoperforminthegame
(step 3). When the agent has no experience in playing the game at
thestartofthetraining,theweightsoftheneuralnetworkinthe
RL model are randomly initialized, producing random actions. The
actionaτisexecutedinthegame(step4),which,inturn,generates
the subsequent state sτ+1.
Afterthefirstiteration(i.e., afterhavingreceivedatleastone aτ),
the gamealso produces,at eachiteration, the dataneeded tocom-
pute the reward function. In RELINEwe collect (i) the information
needed to assess how well the agent is playing the game (e.g., time
since the episode started and the episode score), and (ii) the FPSat time
τ. It is required that the game developer instruments the
game and provide APIs through which RELINEcan acquire such
pieces of information. We assume that this requires a minor effort.
Thereward function aims at training an agent that is able to
(i) play the game, thanks to the information indicating how well
theagentisplaying,and(ii)identifylow-FPSareas,thankstothe
informationabouttheFPS.Theoutputofthe rewardfunction isa
numberrepresentingtherewardobtainedbytheagentattime τ.In
RELINE, the reward function for a given action is composed of two
sub-functions: A game reward function, depending on how good
the actionis inthe game( rgτ), andaperformancereward function,
depending on how the action impacts the performance (rpτ).
2304
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:59:43 UTC from IEEE Xplore.  Restrictions apply. Using Reinforcement Learning for Load Testing of Video Games ICSE ’22, May 21–29, 2022, Pittsburgh, PA, USA
(a) (b)
 (c)
preliminary study case study
Figure 2: Screenshots of games used in the preliminary
study—Section 3 (a) CartPole and (b) MsPacman, and in thecase study—Section 4 (c) SuperTuxKart.
Wecombinesuchfunctionsin rτ=rgτ+rpτ.Thegamereward
function clearly depends on the game under test: A function de-
signedforaracinggamelikelymakesnosenseforarole-playing
game. In general, defining the reward function for learning to play
shouldbeperformedbyconsidering(i)whatthegoalofthegame
is (e.g.,drive ona track), and (ii) whichinformation the game pro-
vides about the “successful behavior of the player” (e.g., is there
ascore?).Eveniflessintuitive,theperformancerewardfunction
is game-dependent as well: Assuming a tiny FPS drop (e.g.,-1%),
the reward can be small for a role-playing game, in which it likely
does not affect the whole experience, while it should be high for
anactiongame,inwhichitcouldevencausethe(unfair)player’s
defeat.Unlikethegamerewardfunction,weexpecthoweverminor
changes to be requiredto adapt the performance reward function
to a different video game (i.e., tuning of the thresholds to use).
Thestatesτ,theaction aτ,andthereward rτarethenstoredinan
experience buffer. When enough experience has been accumulated,
it is used to update the network weights. How experience is stored
and used to update the network depends on the used RL model.
Theepisodeendswhenafinalstateisreached.Again,thedefi-
nition of the final state depends on the game, and it could be based
on a timeout (e.g., each episode lasts at most 90 seconds) or on
a specific condition that must be met (e.g., the agent crosses the
finishline).Oncetheepisodeends,thegameisreinitializedandthe
looprestarts.Thetrainingisperformedforanumberofepisodes
sufficient to observe a convergence in the total reward achieved
byanagentduringanepisode(e.g., ifthetrainedagentobtainsa
rewardof100fortenconsecutiveepisodesthetrainingisstopped).
3 PRELIMINARY STUDY: INJECTING
ARTIFICIAL PERFORMANCE ISSUES
Thispreliminarystudyaimsatassessingtheabilityof RELINEin
identifyingartificial“performancebugs”[ 22]wesimulateintwo
2D games. It is important to highlight that the goalof this study
is onlyto demonstrate theapplicability of RELINEfor loadtesting
games as a proof-of-concept. A case study on a 3D open-source
game is presented in Section 4.
3.1 Study Design
We select two 2D games, CartPole [ 2] and MsPacman [ 9]. The
former — Fig. 2-(a) — is a dynamic system in which an unbalanced
pole is attached to a moving cart, and the player must move the
cart to balance the pole and keep it in a vertical position.Theplayerlosesifthepoleismorethan12degreesfromvertical
orthecartmovestoofarfromthecenter.Thelatter—Fig.2-(b)—istheclassicPac-Mangameinwhichthegoalistoeatalldotswithouttouchingtheghosts.Bothgamesemploysimple2Dgraphicswhich
boundtheplayer’spossiblemovesinonlyone(e.g., leftandright,
for CartPole) or two (e.g., left, right, up, and down, for MsPacman)
dimensions. This is one of the reasons we selected these games
forassessingwhetheraRL-basedagentthatlearnedhowtoplay
themcanalsobetrainedtolookforartificial“performancebugs”
we injected. Also, both games are integrated in the popular Gym
Python toolkit [8] developed by OpenAI [17].
GymcanbeusedfordevelopingandcomparingRL-basedagents
in playing games. It acts as a middle layer between the environ-
ment(the game) and the agent(a virtual player). In particular, Gym
collects and executes actions(e.g.,go left, go right) generated by
the agent and returns to it the new state of the environment (i.e.,
screenshots)withadditionalinformationsuchasthescoreinthe
episode.Gymcomeswithasetofintegratedarcadegamesincluding
the two we used in this preliminary study.
3.1.1 Bug Injection. We injected two artificial “performance bugs”
inCartPoleandfourinMsPacman.Theideabehindthemissimple:
Whentheagentvisitsspecificareasforthefirsttimeduringagame,
the bugs reveal themselves (simulation of heavy resource loading).
A natural way of achieving this goal would have been to introduce
thebugsinthesourcecodeofthegameandtoimplementthelogic
tospotFPSdropsintheagentaccordingly.This,however,would
havesloweddownthetrainingoftheagent.Therefore,wechose
touseamorepracticallysoundapproach,inspiredbythesimula-
tion of Heavy-Weight Operation (HWO) operator for performance
mutation testing [ 22]: We directly assume that the agents observe
the bugs when they visit the designated areas and act accordingly.
InCartPole,theagent canonlymoveon the xaxis(i.e., leftor
right). When the game starts, the agent is in position x=0(i.e.,
centeroftheaxis)anditcanchangeitspositiontowardspositive
(by moving right) or negative (left) xvalues. The two bugs we
injected manifest when x∈[ −0.50,−0.45]andx∈[0.45,0.50]
— dashed lines in Fig. 2-(a). We use intervals rather than specific
values (e.g.,-0.45) because the position of the agent is a float: if
it moves to position -0.450001, we want to reward it during the
training for having found the injected bug. Concerning MsPacman,
weassumethataperformancebugmanifestswhentheagententers
the fourgatesindicated by the white arrows in Fig. 2-(b).
AsdetailedinSection3.1.4,weassesstheextenttowhich RELINE
isabletoidentifythebugsweinjectedwhileplayingthegames.To
have a baseline, we compare its results with those of a RL-basedagent only trained to play each of the two games (from hereon,
rl-baseline ), and with a random agent . SinceRELINEwill be trained
withthegoalofidentifyingthebugs(detailsfollow),weexpectit
to adapt its behavior to not only successfully play the game, but to
also exercise more often the “buggy” areas of the games.
3.1.2 Learning to Play: RL Models and Game Reward Functions.
We trainedthe rl-baseline agent(i.e., theone onlytrained tolearn
how to play) for CartPole using the cross-entropy method [40]a s
RL model. We choose this method because, despite its simplicity,it has been shown to be effective in applications of RL to small
environments such as CartPole [30].
2305
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:59:43 UTC from IEEE Xplore.  Restrictions apply. ICSE ’22, May 21–29, 2022, Pittsburgh, PA, USA Rosalia Tufano, Simone Scalabrino, Luca Pascarella, Emad Aghajani, Rocco Oliveto, and Gabriele Bavota
The core of the cross-entropy method is a feedforward neural
network(FNN)thattakesasinputthestateofthegameandprovides
as output the action to perform. The state of the game for CartPole
is a vector of dimension 4 containing information about the x
coordinateofthepole’scenterof mass,thepole’s speed,itsangle
withrespecttotheplatform,and itsangularspeed.Therearetwo
possible actions: go right, go left. Once initialized with random
weights, the agent (i.e., the FNN) starts playing while retaining the
experienceacquiredineachepisode:Theexperienceisrepresented
bythestate,theaction,andtherewardobtainedduringeachstep
of the episode.
The goal is to keep the pole in balance as long as possible or
until the maximum length of an episode (that we set to 1,000 steps)
isreached.The gamerewardfunction isdefinedsothattheagent
receives a +1 reward for each step it manages to keep the pole
balanced.The totalscore achievedis alsosavedat theend ofeach
episode. After n=16 consecutive episodes the agent stops playing,
selects the m=11 (70%) episodes having the highest score, and
uses the experience in those episodes to update the weights of the
FNN (nandmhave been set according to [30]).
Instead,wetrainedthe rl-baseline agentforMsPacmanusinga
Deep QNetwork (DQN)[34]. Inour context,a DQNis aConvolu-
tionalNeuralNetwork(CNN)thattakesasinputasetofcontiguous
screenshots of the game(in our case 4, as done inpreviousworks
[34,35]) representing the state of the game and returns, for each
possible action defined in the game (five in this case: go up, go
right,godown,goleft,donothing),avalueindicatingtheexpectedrewardfortheactiongiventhecurrentstate(Qvalue).Themultiple
screenshots are needed to provide more information to the model
aboutwhatishappeninginthegame(e.g., inwhichdirectionthe
agent is moving). The goal of the DQN is the same as the FNN:
selecting the best action to perform to maximize the reward given
the current state. Differently from the previous model, the DQN is
updated not on entire episodes but by randomly batching “expe-rience instances” among 10k steps saved during the most recent
episodes.An“experienceinstance”issavedaftereachstep τ,and
is represented by the quadruple ( sτ−1,aτ,sτ,rτ), wheresτ−1is the
inputstate, aτistheactionselectedbytheagent, sτistheresulting
state obtained by running aτinsτ−1andrτis the received reward.
TheCNNisinitializedwithrandomweights,andtheagentstarts
playing while retaining the experience of each step. When enough
experience instances have been collected (10k in our implementa-
tion[30]),theCNNstartsupdatingateachstepselectingarandom
batch of experience instances. The reward function for MsPacman
provides a +1 reward every time the agent eats one of the dots and
a 0 reward otherwise.
3.1.3 Instantiating RELINE : Performance Reward Functions. Totrain
RELINEtoplaywhilelookingfortheinjectedbugs,weuseasimple
performance reward function : In both the games, we give a reward
of +50 every time the agent, during an episode, spots one of the
injectedartificialbugs.Aspreviouslymentioned,thebugsreveal
themselves only the first time the agent visits each buggy position;
this means that the performance-based reward is given at most
twice for CartPole and four times for MsPacman.3.1.4 Data Collection and Analysis. Wecompare RELINEagainst
the two previously mentioned baselines: rl-baseline and theran-
domagent .BothRELINEandrl-baseline havebeentrainedfor3,200
episodesonCartPoleand1,000onMsPacman.Thedifferentnum-
bers are due to differences in the games and in the RL model weexploited. Inbothcases, we used a number ofepisodes sufficient
forrl-baseline tolearnhowtoplay(i.e., weobservedaconvergence
in the score achieved by the agent in the episodes).
Once trained, the agents have been run on both games for addi-
tional 1,000 episodes, storing the performance bugs they managed
to identify in each episode. Since different trainings could result in
modelsplayingthegamefollowingdifferentstrategies,werepeatedthisprocesstentimes.Thismeansthatwetrained10differentmod-
elsforboth RELINEandrl-baseline and,then,weused eachofthe
10modelstoplayadditional1,000episodescollectingthespotted
performancebugs.Similarly,weexecuted randomagent 10times
for 1,000 episodes each. In this case, no training was needed.
We report descriptive statistics (mean, median, and standard
deviation) of the number of performance bugs identified in the
1,000playedepisodesbythethreeapproaches.Ahighnumberof
episodes in which an approach can spot the injected bugs indicate
its ability to look for performance bugs while playing the game.
3.2 Preliminary Study Results
Table 1 shows for each of the two games (CartPole and MsPacman)
the number kof artificial bugs we injected and, for each of the
threetechniques(i.e.,RELINE ,rl-baseline ,andthe randomagent ),
descriptivestatisticsofthenumberofepisodes(outof1,000)they
managed to identify at least nof the injected bugs, with ngoing
from 1 to kat steps of 1.
For both games, it is easy to see that the random agent is rarely
able to identify the bugs. Indeed, this agent plays without any
strategyasitisabletoidentifybugsonlybychanceinafewepisodesoutofthe1,000itplays.Thisisalsoduetothefactthatthe random
agentquickly looses the played episodes due to its inability to play
the game. This confirms that these approaches are not suitable for
testing video games.
Concerning CartPole, both RELINEandrl-baseline are able to
spot at least one of the two bugs in several of the 1,000 episodes.
The median is 984 for RELINEand 706 for rl-baseline . The success
ofrl-baseline issoonexplainedbythecharacteristicsofCartPole:
Considering where we injected the bugs — see Fig. 2-(a) — by
playing the game it is likely to discover at least one bug (e.g., if the
playertendstomovetowardsleft,thebugontheleftwillbefound).
Whatitisinsteadunlikelytohappenbychanceistofindbothbugs
withinthesameepisode.Wefoundthatitisquitechallenging,even
for a human player, to move the cart first towards one side (e.g.,left) and, then, towards the other side (right) without losing dueto thepolemovingmore than12 degreesfrom vertical. Asit canbe seen in Table 1, RELINEsucceeds in this, on average, for 102
episodes out of 1,000 (median 47), as compared to the 5 (median 1)
ofrl-baseline .Thisindicatesthat RELINEispushedbythereward
function to explore the game looking for the injected bugs, evenif this makes playing the game more challenging. Similar results
have been achieved on MsPacman.
2306
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:59:43 UTC from IEEE Xplore.  Restrictions apply. Using Reinforcement Learning for Load Testing of Video Games ICSE ’22, May 21–29, 2022, Pittsburgh, PA, USA
Table 1: Number of episodes (out of 1,000) in which RELINE, rl-baseline , and the random agent identify the injected bugs.
Game#Injected #Bugs RELINE rl-baseline random agent
Bugs found mean median stdev mean median stdev mean median stdev
CartPole 21 965 984 47 715 706 107 12 11 4
2 102 47 177 5 1 7 0 0 0
MsPacman 41 971 989 59 700 680 228 24 23 52 966 985 63 356 343 169 17 16 33 914 941 87 114 80 90 1 1 14 879 907 106 25 23 17 1 1 1
In this case, the DQN is effective in allowing RELINEto play
whileexercisingthepointsinthegameinwhichweinjectedthe
bugs. Indeed, on average, RELINEwas able to spot all four injected
bugsin879outofthe1,000playedepisodes(median=907),while
rl-baseline could achieve such a result only in 25 episodes.
Summary of the Prelimiary Study. RELINEallowsobtain
agents able not only to effectively play a game but also tospotperformanceissues.Comparedto rl-baseline ,themain
advantageof RELINEisthatitidentifiesbugsmorefrequently
while playing.
4 CASE STUDY: LOAD TESTING AN OPEN
SOURCE GAME
Werunacasestudytoexperimentthecapabilityof RELINEinload
testing an open-source 3D game. Differently from our preliminary
study(Section3),wedonotinjectartificialbugs.Instead,weaim
at finding parts of the game resulting in FPS drops.
4.1 Study Design
For this study, we use a 3D kart racing game named SuperTuxKart
[44] — see Fig. 2-(c). This game has been selected due to the fol-
lowing reasons. First, we wanted a 3D game in which, as com-
pared to a 2D game, FPS drops are more likely because of the more
complexrenderingprocedures.Second,SuperTuxKartispopularopen-source project that counts, at the time of writing, over 3k
stars on GitHub. Third, it is available an open-source wrapper that
simplifies the implementation of agents for SuperTuxKart [10].
The existence of a wrapper like the one we used is crucial since
itallows,forexample,toadvanceinthegameframebyframe(thussimplifyingthegenerationoftheinputstotheRLmodel),toexecute
actions(e.g., throttleorbrake),andtoacquiregameinternals( e.g.,
kartcentering,distancetothefinishline).Also,usingthiswrapper,itispossibletocomputethetimeneededbythegametorendereach
frame and, consequently, calculate the FPS. Finally, the wrapper
allowstohavesimplifiedgraphics(e.g., removingparticleeffects,
like rain, that could make the training more challenging).
4.1.1 Learning to Play: RL Models and Game Reward Functions.
The training of the rl-baseline agent has been performed using the
DQN model previously applied in MsPacman.We use the previously mentioned PySuperTuxKart [10] to make
the agent interact with the game. For the sake of speeding upthe training, the screenshots extracted from the game have been
resizedto200x150pixelsandconvertedingrayscalebeforetheyare
providedasinputtothemodel.Moreover,aspreviouslydonefor
MsPacman,multiple(four)screenshotsarefedtothemodelateach
step. Thus, the representation of the state of the game provided to
themodelisa4 ×200×150tensor.Thedetailsofthemodelandits
implementation are available in our replication package [46].
A critical part of the learning process is the definition of the
gamerewardfunction.BeingSuperTuxKartaracinggame,anoption
could have been to penalize the agent for each additional steprequired to finish the game. Consequently, to maximize the final
score, the agent would have been pushed to reduce the number of
stepsand, therefore,to driveas fastas possibletowards thefinish
line.However,consideringthenon-trivialsizeofthegamespace,
sucharewardfunctionwouldhaverequiredalongtrainingtime.
Thus,wetookadvantageoftheinformationthatcanbeextracted
from the game to help the agent in the learning process.
SuperTuxKart provides two coordinates indicating where the
agent is in the game: path_done andcentering.
Theformerindicatesthepathtraversedbytheagentfromthe
starting line of the track, while the latter represents the distance
of the agent from the center of the track. In particular, centering
equals 0 if the agent is at the center of the track, and it movesawayfromzeroastheagentmovestoeitherside:goingtowards
rightresultsinpositivevaluesofthe centering value,goingleftin
negative values. We indicate these coordinates with x(centering )
andy(path_done ),andwedefine δyasthepathtraversedbythe
agentinaspecificstep:Given yithevaluefor path_done atstep
i, we compute δyasyi−yi−1. Basically, δymeasures how fast the
agent is advancing towards the finish line.
Givenxandδyforagivenstep i,wecomputetherewardfunction
as follows:
rgi=/braceleftBigg
−1i f|x|>θ
max(min( δy,M),0)otherwise
First, if the agent goes too far from the center of the track ( |x|>
θ),wepenalizeitwithanegativereward.Otherwise,iftheagent
isclosetothecenter( |x|≤θ),wecanhavetwoscenarios:(i)ifit
is notmoving towardsthe finishline ( δy≤0), wedo notgive any
reward(theminimumrewardis0);(ii)ifitismovingintheright
direction ( δy>0), we give a reward proportional to thespeedat
which it is advancing ( δy), up to a maximum of M.
2307
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:59:43 UTC from IEEE Xplore.  Restrictions apply. ICSE ’22, May 21–29, 2022, Pittsburgh, PA, USA Rosalia Tufano, Simone Scalabrino, Luca Pascarella, Emad Aghajani, Rocco Oliveto, and Gabriele Bavota
In our experimental setup, we set θ=20 because it roughly
representsthedoubleof |x|whentheagentapproachesthesidesof
theroadinthelevel,and M=10asitisthesamemaximumreward
alsogivenbythe performancerewardfunction,asweexplainbelow.
Finally,werewardtheagentwhenitcrossesthefinishlinewithan
additional +1,000 bonus.
4.1.2 Instantiating RELINE : Performance Reward Function. Tode-
finetheperformancerewardfunction ofRELINEforSuperTuxKart,
thefirststeptoperformistodefineawaytoreliablycapturethe
FPSofthegameduringthetraining.Inthisway,wecanrewardthe
agentwhenitmanagestoidentifylow-FPSpoints.Aspreviously
said,weusePySuperTuxKarttointeractwiththegameandsuch
a framework keeps the game frozen while the other instructions
ofRELINE(e.g.,theidentification ofthe actionto execute)arerun.
Sincetheframeworkrunsthegameinthesameprocessinwhich
we runRELINEand since we do not use threads, we can safely use
a simple method for computing the time needed to render the four
frames: We get the system time before ( Tbefore) and after ( Tafter)
we trigger the rendering of the frames and we compute the time
needed at step iasrTi=Tafter−Tbefore. Such a value is negatively
correlated with the FPS (higher rendering time means lower FPS).
Theperformance reward function we use is the following:
rpi=/braceleftBigg
10 if|x|≤θ∧rTi>t
0 otherwise
Wegiveaperformance-basedrewardof10whentheagenttakes
more than tmilliseconds to render the frames at a given point
(causinganFPSdrop).Weexplainthetuningof tlater.Wedonot
give such a reward when |x|>θ(the kart is far from the center)
sincewewanttheagenttospotissuesinpositionsthatarelikely
to be explored by real players (i.e., reasonably close to the track).
Finally,in RELINEwedonotgiveafixed +1,000bonusreward
whentheagentcrossesthefinishlinebutweassignabonuscom-
puted as 10 ×/summationtext.1steps
i=1rpi,i.e.,proportional to the total performance-
basedrewardaccumulatedbytheagentintheepisode.Thisisdone
to push the agent to visit more low-FPS points during an episode.
4.1.3 Data Collection and Analysis. As done in our preliminary
study, we compare RELINEwithrl-baseline (i.e.,the agent only
trained to play the game) and with a random agent .
Training rl-baseline and RELINE. Whileweuseddifferentre-
wardfunctionsforthetwoRLagents,weappliedthesametraining
processforbothofthem.Wetrainedeachmodelfor2,300episodes,
with one episode having a maximum duration of 90 seconds or
ending when the agent crosses the finish line of the racing track
(theagentisrequiredtoperformasinglelap).Wesetthe90seconds
limitsinceweobservedthat,bymanuallyplayingthegame, ∼70
seconds are sufficient to complete a lap. The 2,300 episodes thresh-
oldhasbeendefinedbycomputingtheaveragerewardobtainedby
thetwoagentsevery100episodesandbyobservingwhenaplateau
was reached by both agents. We found 2,300 episodes to be a good
compromiseforbothagents(graphsplottingtherewardfunction
are available in the replication package [46]).
The trained rl-baseline agent has been used to define the thresh-
oldtneeded for the RELINE’s reward function (i.e., for identifying
when the agent found a low-FPS point and should be rewarded).Inparticular,oncetrained,werun rl-baseline for300episodes,
storing the time needed by the game to render the subsequent fourframesaftereveryactionrecommendedbythemodel.
2Thisresulted
in a total of 48,825 data points sFPS, representing the standard FPS
of the game in a scenario in which the player is only focused on
completing the race as fast as possible.
Starting from the 48,825 sFPSdata points collected in the 300
episodes played by the trained rl-baseline agent, we apply the five-
σrule [23] to compute a threshold able to identify outliers. The
five-σrulestatesthatinanormaldistribution(suchas sFPS)99.99%
of observed data points lie within five standard deviations fromthe mean. Thus, anything above this value can be considered asan outlier in terms of milliseconds needed to render the frames.For this reason, we compute
tb=mean(sFPS)+5×sd(sFPS)as
a candidate base threshold to identify low-FPS points. However,
tbcannot be directly used as the tvalue of our reward function.
Indeed, we observed that the time needed for rendering frames
during the RELINE’s training is slightly higher as compared to the
time needed when the trained rl-baseline agent is used to play the
game. This is due to the fact that the load on the server (and in
particular on the GPU) is higher during training. To overcome this
issue, we perform the following steps.
At the beginning of the training, we run 100 warmup episodes in
whichwecollectthetimeneededtorenderthefourframesafter
each action performed by the agent. Then, we compute the first
(Qtr
1) and the third ( Qtr
3) quartile of the obtained distribution and
compare them to the Q1andQ3of the distribution obtained in
the 300 episodes used to define tb(i.e.,those played by the trained
rl-baseline agent).Duringthe warmupepisodes,theagentselectsthe
action to perform almost randomly (it still has to learn): Therefore,
itwouldnotbeabletoexploreasubstantialareaofthegame(i.e.,
of the racing track), thus not providing a distribution of timingscomparable with the ones obtained when the trained rl-baseline
agent that played the 300 episodes. For this reason, during the 100
warmupepisodes ofthetraining,theactiontoperformisnotchosen
bytheagentcurrentlyundertraining,butbythetrained rl-baseline
agent(i.e., thesameusedinthe300episodes).Thisdoesnotimpact
in any way the load on the server that remains the one we have
during the training of RELINEsince the only change we have is to
ask for the action to perform to the rl-baseline agent rather than to
theoneundertraining.However,thewholetrainingprocedure(e.g.,
capturing the frames and updating the network) stays the same.
We compute the additional “cost” brought by the training in
rendering the frames during the game using the formula δ=
max(Qtr
1−Q1,Qtr
3−Q3). We use the first and third quartiles since
theyrepresenttheboundariesofthecentralpartofthedistribution,
i.e.,they should be quite representative of the values in it. We took
asδthemaximumofthetwodifferencestobemoreconservative
inassigningrewardswhentheagentidentifieslow-FPSpoints.The
final value twe use inour reward function when training RELINE
to load test SuperTuxKart is defined as: t=tb+δ=18.36.3
2Since we wanted to measure the frames rendering time in a standard scenario in
which the agent was driving the kart, we stopped an episode if the agent got stuck
against some obstacle.
3Weidentifyaslow-FPSpointstheonesinwhichtheFPSislowerthan218.Sucha
numberis stillveryhigh,more thanenoughfor anyhumanplayer,inpractice.Note
that we run the game using high-performance hardware and, most importantly, with
thelowestgraphicsettings.Theequivalentinnormalconditionswouldbemuchlower.
2308
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:59:43 UTC from IEEE Xplore.  Restrictions apply. Using Reinforcement Learning for Load Testing of Video Games ICSE ’22, May 21–29, 2022, Pittsburgh, PA, USA
0 20 40 60 80 100 120 140milliseconds to render frames
actions1314
12
Figure 3: Rendering times for 300 episodes (same actions).
Thus,ifRELINEisable,duringthetraining,toidentifyapointin
the game requiring more than tmilliseconds to render four frames,
then it receives a reward as explained in Section 4.1.2.
Thetrainingof rl-baseline took∼3hours,while RELINErequires
substantially more time due to the fact that, after each step per-
formed by the agent, we collect and store information about thetime needed to render the frames (this is done million of times).
This pushed the training for RELINEup to∼30 hours.
Reliability of Time Measurements. It is important to clarify
thattheFPSofthegamecanbeimpactedbythehardwarespeci-
ficationsandthecurrentloadofthemachinerunningit.Inother
words,runningthesamegameontwodifferentmachinesoronthe
same machine in two different moments can result in variations of
the FPS. For this reason, all the experiments have been performed
onthesameserver,equippedwith2x64CoreAMD2.25GHzCPUs,512GBDDR43200MHzRAM,andannVidiaTeslaV100S32GBGPU.
Also,theprocessrunningthetrainingoftheagentsorthecollec-
tionofthe48,825 sFPSwiththetrained rl-baseline agentwasthe
only process running on the machine besides those handled by the
operatingsystem(Ubuntu20.04).Ontopofthat,theprocesswas
alwaysrunusingthe chrt --rr 1 option,thatinLinuxmaximizes
the priority of the process, reducing the likelihood of interruptions.
Despitetheseprecautions,itisstillpossiblethatvariationsare
observedintheFPSnotduetoissuesinthegame,buttoexternal
factors (e.g., changes in the load of the machine). To verify the
reliability of the collected FPS data, we run a constant agent per-
forming always the same actions in the game for 300 episodes. The
setofactionshasbeenextractedfromoneoftheepisodesplayed
by therl-baseline agent, that was able to successfully conclude the
race. Then,we plottedthe timeneeded by thegame torender the
four frames following each action made by the agent. Since we are
playing300timesexactlythesameepisode,weexpectto observe
the sametrend in termsof FPS foreach game. Ifthis is thecase, it
meansthatthewaywearemeasuringtheFPSisreliableenoughto
reward the agent when low-FPS points are identified.
Fig. 3 shows the achieved results: The y-axis represents the
millisecondsneededtorenderfourframesinresponsetoanagent’s
action(x-axis)performedinaspecificpartofthegame.While,as
expected, small variations are possible, the overall trend is quite
stable:Pointsofthegamerequiringlongertimetorenderframes
areconsistentlyshowingacrossthe300episodes,resultinginaclear
trend. We also computed the Spearman’s correlation [ 42] pairwise
across the 300 distributions, adjusting the obtained p-values using
the Holm’s correction [27].We found all correlations to be statistically significant (adjusted
p-values<0.05) with a minimum ρ=0.77 (strong correlation) and a
medianρ=0.91(verystrongcorrelation).Thisconfirmsthecommon
FPS trends across the 300 episodes.
RunningtheThreeTechniquestoSpotLow-FPSAreas. Af-
terthe 2,300trainingepisodes, weassumethatboth theRL-based
agentslearnedhowtoplaythegame,andthat RELINEalsolearned
how to spot low-FPS points. Then, as also done in our preliminary
study,wetrainbothagentsforadditional1,000episodes,storingthe time needed to render the frames in every single point they
exploredduringeachepisode(whereapointisrepresentedbyits
coordinates, i.e.,centering =xandpath_done =y).Wedothesame
also with the random agent .
Data Analysis. Theoutputofeachofthethreeagentsisalist
of points with the milliseconds each of them required to renderthe subsequent frames. Since each agent played 1,000 episodes,it is possible that the same point is covered several times by an
agent,withslightlydifferentFPSobserved(aspreviouslyexplained,
smallvariationsinFPSarepossibleandexpectedacrossdifferent
episodes).Weclassifyas low-FPSpoints thosethat requiredmore
thantmillisecondstorenderthefoursubsequentframesmorethan
50% of times they have been covered by an agent.
Thismeansthat,ifacrossthe1,000episodesapoint pisexercised
100 times by an agent, at least 51 times the threshold tmust be
exceeded to consider pas a low-FPS point. In practice, a developer
usingRELINEfor identifying low-FPS points could use a higher
threshold to increase the reliability of the findings. However, for
the sake of this empirical study, we decided to be conservative.
Then, we compare the characteristics of the low-FPS points
identified bythe threeapproaches. Specifically,we analyze:(i) how
many different low-FPS points each approach identified; (ii) thenumber of times each low-FPS point has been exercised by eachagent in the 1,000 episodes; (iii) the confidence of the identified
points(i.e., thepercentageoftimesanexercisedpointresultedin
low FPS). Given the low-FPS points identified by each agent, wedraw violin plots showing the distribution of timings needed to
rendertheframeswhentheagentexercisedthem(thehigherthe
timings, the lower the FPS). We compare these distributions using
Mann-Whitneytest[ 20]withp-valuesadjustmentusingtheHolm’s
correction[ 27].Wealsoestimatethemagnitudeofthedifferencesby
usingtheCliff’sDelta( d),anon-parametriceffectsizemeasure[ 24]
for ordinal data. We follow well-established guidelines to interpret
the effect size: negligible for |d|<0.10, small for 0 .10≤|d|<0.33,
medium for 0 .33≤|d|<0.474, and large for |d|≥0.474 [24].
4.2 Study Results
Fig. 4 summarizes the main findings of our case study. Fig. 4-(a)
showsthedistributionoftimeneededtorenderthegameframes
(i.e.,our proxy for FPS) for four groups of points. The first violin
plot on the left (i.e., Regular FPS) shows the timing for points that
have never resulted in a drop of FPS in any of the 3,000 episodes
played by the three agents (1,000 each). These serve as baseline to
better interpret the low-FPS points exercised by the agents. Theother three violin plots show the distributions of timing for the
low-FPS points identified by RELINE(blue),rl-baseline (green), and
therandom agent (red).
2309
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:59:43 UTC from IEEE Xplore.  Restrictions apply. ICSE ’22, May 21–29, 2022, Pittsburgh, PA, USA Rosalia Tufano, Simone Scalabrino, Luca Pascarella, Emad Aghajani, Rocco Oliveto, and Gabriele Bavota
(c)centering
0102030405060708090100 -100 -90-80 -70-60-50-40-30-20-10RELINE RL-BASELINE
min
medianmaxRegular FPS RANDOM
51%
99%
100%
(a) (b)173 low-FPS points 33 low-FPS points 90 low-FPS points
Avg. conf.: 89%
minmedianmax60%
94%
100%Avg. conf.: 90%
minmedianmax51%
76%
100%Avg. conf.: 77%
path donemilliseconds to render frames
01020304060
5055
45
35
25
15
5
05101520253035404550556065707580859095100
Figure 4: Results of the study: (a) reports the distributions
of timings for the low-FPS points with summary statistics,
while (b) and (c) depict the path done and centering coordi-
nates at which the such points were observed, respectively.
Below each violin plot we report the number of low-FPS points
identified by each agent and descriptive statistics (average, me-
dian,min,max)oftheconfidenceforthelow-FPSpoints.A100%
confidencemeansthatalltimesthatalow-FPSpointhasbeenex-
ercisedinthe1,000episodesplayedbytheagentitrequiredmore
thant=18.36millisecondstorenderthesubsequentframes.The
tthreshold is represented by the red horizontal line. On average,
RELINEexercisedeachlow-FPSpoint89timesinthe1,000episodes,
against the 210 of rl-baseline and the 829 of the random agent (the
same point can be exercised multiple times in an episode).
RELINEidentified 173 low-FPS points, as compared to the 33
ofrl-baseline and the 90 of the random agent . The confidence is
similar for RELINE(median=99%) and rl-baseline (median=94%),
while it is lower for the random agent (median=76%). Thus, the
low-FPSpointsidentifiedbythetwoRL-basedagentsare,overall,
quite reliable. Concerning the number of low-FPS points identified,
RELINEidentifies more points as compared to rl-baseline (173vs
33). This is expected since it has the explicit goal of load testingthe game, However, what could be surprising at first sight is the
highnumberof low-FPS pointsidentifiedbythe randomagent (90).
Fig. 4-(b) and Fig. 4-(c) help in interpreting this finding.
Fig. 4-(b) plots the path_done (ycoordinate) for eachlow-FPS
point identified by each agent, using the same color schema of the
violin plots (e.g., blue corresponds to RELINE).Ifmultiplepointsfallinthesamecoordinate(i.e., same path_done
but different centering ), they are shown with a red border. The
scale of the path_done has been normalized between 0 and 100,
where 0 corresponds to the starting line of the track and 100 to its
finishline.Similarly,Fig.4-(c)plotsthe centering (xcoordinate)
for the low-FPS points. The line at 0 represents the center of the
track,whilethecontinuouslinesinposition ∼-18and∼18depict
thelimitsofthetrack.Finally,thedashedlinesrepresentthearea
of the game we asked RELINEto explore: based on our reward
function, we penalize the agent for going outside the [-20, +20]
range that, normalized, corresponds to ∼[-36, +36]. Also rl-baseline
is penalized outside of this area.
Asexpected,the randomagent isnotabletoadvanceinthegame:
The low-FPS points it identifies are all placed near the starting line
—reddotsinFig.4-(b).Thisindicatesthatarandomagentcanbe
usedtoexerciseaspecificpartofagame,butitisnotabletoexplore
the game as a player would do. This is also confirmed by the red
dotsinFig.4-(c),withthe randomagent exploringareasofthegame
farfromthetrackandthatahumanplayerisunlikelytoexplore.
Also, it is worth noting that in SuperTuxKart each episode lasts
(based on our setting) 90 seconds if the agent does not cross the
finishline.However,asshowninourpreliminarystudy,inother
games such as MsPacman a random agent could quickly lose an
episode without having the chance to explore the game at all.
The low-FPS points identified by RELINE(blue dots) and by
rl-baseline (green) are instead closer to the track and, for what
concerns RELINE, they are within or very close the area of the
game we ask it to explore — see dashed lines in Fig. 4-(c). Thus, by
customizing the reward function, it is possible to define the area of
the game relevant for the load testing.
Looking at Fig. 4-(b), we can see that RELINEis also able to
identify low-FPS in different areas of the game with, however, a
concentration close to the beginning and the end of the game. It is
difficult to explain the reason for such a result, but we hypothesize
two possible explanations.
First, it is possible that the “central” part of the game simply
featureslesslow-FPSareas.Thiswouldalsobeconfirmedbythefact thatrl-baseline only found one low-FPS point in that part of
the game. Also, the training and the reward function could have
drivenRELINEtoexploremorethestartingandtheendingofthe
game. The starting part is certainly the most explored since, at the
beginning of the training, the agent is basically a random agent.
Thus, itmostly collects experience aboutlow-FPS points found in
thebeginningofthegamesince,similarlytothe randomagent ,itis
notableto advanceinthegame. Itisimportantto rememberthat
the data in Fig. 4 only refers to the 1,000 gamesplayed by RELINE
after the 2,300 training games, so we are not including the random
explorationdoneatthebeginningofthetraininginFig.4.However,
once the agent learns several low-FPS points in the starting of the
game, it can exercise them again and again to get a higher reward.
Concerning the end of the game, we set a maximum duration of
90secondsforeachgame,butweknowthatawell-trainedagent
can complete the lap in ∼70 seconds. It is possible that the agent
used the remaining time to better explore the last part of the game
before crossing the finish line, thus finding a higher number of
low-FPSpointsinthatarea.Additionaltrainings,possiblywitha
differentreward function, are needed to better explain our finding.
2310
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:59:43 UTC from IEEE Xplore.  Restrictions apply. Using Reinforcement Learning for Load Testing of Video Games ICSE ’22, May 21–29, 2022, Pittsburgh, PA, USA
Concerning the violin plots in Fig. 4-(a), we can see that RELINE
andrl-baseline exhibitasimilardistribution,with RELINEbeingable
toidentifysomestrongerlow-FPSpoints( i.e.,longertimetorender
frames). All distributions have, as expected, the median above the
tthreshold, with RELINE’s one being higher (24.54 vs21.69 for
rl-baseline and 19.39 for random agent ). The highest value of the
distributions is 65.92 (60.7 FPS) for RELINE, against 44.81 (89.3 FPS)
forrl-baseline and 50.73 (78.8 FPS) for random agent . Remember
that all these values represent milliseconds to load frames after an
action performed by the agents.
Table2:ResultsofMann-Whitneytest(adjusted p-value)and
Cliff’sDelta( d)whencomparingthedistributionsofrender-
ing times — boldface indicates higher times.
Test p-value OR
RELINEvsrl-baseline <0.001 0.34 (Medium)
RELINEvsrandom agent <0.001 0.36 (Medium)
rl-baseline vsrandom agent <0.001 0.16 (Small)
Table2showstheresultsofthestatisticalcomparisonsamongthe
three distributions. In each test, the approach reported in boldface
istheoneidentifyingstrongerlow-FPSpoints(i.e., moreextreme
points requiring longer rendering time for their frames). The ad-
justedp-valuesreportasignificantdifference( p-value<0.001)in
favor ofRELINEagainst both rl-baseline and therandom agent (in
both cases, with a medium effect size). Thus, the low-FPS points
identified by RELINEtend to require longer times to render frames.
Fig. 2-(c) shows an example of low-FPS point identified by RELINE:
Crashing against the sheep results in a drop of FPS.
Finally, it is worth commenting about the overlap of low-FPS
pointsidentifiedbythethreeagents.Indeed, RELINEandrl-baseline
found14low-FPSpointsincommon(i.e., samexandycoordinates),
whiletheoverlapisof11pointsfor RELINEandrandomagent ,and
10forrl-baseline andrandomagent .Themostinterestingfindingof
thisanalysisisthat rl-baseline wasabletoidentifyonly19points
missed by RELINE, while the latter found 159 points missed by
rl-baseline . This supports the role played by the reward function in
pushingRELINEto look for low-FPS points.
Summary of the Case Study. RELINEisthebestapproach
for finding low-FPS points in SuperTuxKart. A random agent
is not able to spot issues that require playing skills, and rl-
baselineonly finds a small portion of the low-FPS points.
5 THREATS TO VALIDITY
ThreatstoConstructValidity .Themainthreatstotheconstruct
validityofourstudyarerelatedtotheprocessweadoptedinour
case study (Section 4) to identify low-FPS points. Based on our
experiments, and in particular on the findings reported in Fig. 3,
ourmethodologyshouldbereliableenoughtoidentifyvariationsinFPS.Still,somelevelofnoisecanbeexpected,andforthisreasonall
our analyses have been run at least 300 times, while 1,000 episodes
were played by each of the experimented approaches.Concerning ourpreliminary study(Section3), itis clear thatthe
bugsweinjectedarenotrepresentativeofrealperformancebugs
in the subject games. However, they are inspired from a perfor-
mancemutationoperatordefinedintheliterature[ 22].Ourprelim-
inary study only serves as a proof-of-concept to verify whether, by
modifyingtherewardfunction,aRL-basedagentwouldadaptits
behavior to look for bugs while playing the game.
Threats to Internal Validity. In our case study, to ease the
trainingwedidnotusethe“real”game,butitswrappedversion, i.e.,
PySuperTuxKart[ 10].Whilethecoregameisthesame,theversion
we adopted does not contain the latest updates and it includes
additional Python code that may affect the rendering time. Weassume that such a time is constant for all the frames since itsimply triggers the frame rendering operation in the core game.
Besides,weforcedthegametorunwithlowestgraphicssettings
to speed up r endering: For example, we excluded dynamic lighting,
anti-aliasing, and shadows. Therefore, the low-FPS points found
in PySuperTuxKart may be irrelevant in the original game or with
othergraphicsettings.Also,weappliedthefive- σruletodefinea
thresholdfordefiningwhatalow-FPSpointis.Thethresholdwe
set might be not indicative of relevant performance issues.
Still, the goal of our study was to show that once set specific
requirements ( e.g.,the threshold t, the area to explore, etc.), the
agentisabletoadapttryingtomaximizeitsreward.Thus,wedo
not expect changes in the threshold to invalidate our findings.
Threats to conclusion validity. Inourdataanalysisweused
appropriatestatisticalprocedures,alsoadopting p-valueadjustment
when multiple tests were used within the same analysis.
Threats to External Validity Besides the proof-of-concept
studywepresentedinSection3,ourempiricalevaluationof RELINE
includes a single game. This does not allow us to generalize ourfindings. The reasons for such a choice lie in the high effort weexperiencedasresearchersin(i)buildingthepipelinetointeract
withthegame,(ii)findingandexperimentingwithareliableway
to capture the FPS, (iii) defining a meaningful reward function that
allowedtheagenttosuccessfullyplaythegameinthefirstplace
and,then,toalsospotlow-FPSpoints.Thesestepswerealongtrial-
and-error process with the most time consuming part being the
trainingsneededtotestthedifferentrewardfunctionsweexperi-
mentedbeforeconvergingtowardstheonespresentedinthispaper.Indeed,testinganewversionofarewardfunctionrequiredatleast
one week of work with the hardware at our disposal (including
implementation, training, and data analysis).
Thiswasalsoduetotheimpossibilityofusingmultiplemachines
or to run multiple processes in parallel on the same server. Indeed,
asexplained,usingtheexactsameenvironmenttorunallourexper-imentswasastudyrequirement.Itisworthnotingthat,becauseof
similar issues, other state-of-the-art approaches targeting different
game properties were experimented with only one game as well
(seee.g.,[16,37,48,51]).Webelievethatinstantiating RELINEona
newgamewouldbemucheasierbycollaboratingwiththegame
developers.Whilethiswouldonlyslightlysimplifythedefinition
of a meaningful reward function, the original developers of the
game could easily provide through APIs all information needed by
RELINE(including, e.g.,the FPS), cutting away weeks of work.
2311
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:59:43 UTC from IEEE Xplore.  Restrictions apply. ICSE ’22, May 21–29, 2022, Pittsburgh, PA, USA Rosalia Tufano, Simone Scalabrino, Luca Pascarella, Emad Aghajani, Rocco Oliveto, and Gabriele Bavota
6 RELATED WORK
Threerecentstudies[ 32,39,45]suggestthatfindingperformance
issuesinvideogamesisarelevantproblem,accordingtobothgame
developers[ 39,45]andplayers[ 32].Nevertheless,tothebestofour
knowledge, no previous work introduced automated approaches
for load testing video games. Therefore, in this section, we discuss
some important works on thequality assurance of video games in
general. We first introduce the approaches defined in the literature
for training agents able to automatically play and win a game.
Then, we show how such approaches are used for play-testing for
(i) finding functional issues and (ii) assessing game/level design
(e.g.,finding unbalanced levels or mechanics).
6.1 Training Agents to Play
Reinforcement Learning (RL) is widely used to train agents able to
automatically play video games. Mnih et al.[34,35] presented the
firstapproachbasedonhigh-dimensionalsensoryinput(i.e., raw
pixels from the game screen) able to automatically learn how toplay a game. The authors used a Convolutional Neural Network
(CNN) trained with a variant of Q-learning to train their agent.
The proposed approach is able to surpass human expert testers in
playing some games from the Atari 2600 benchmark.
Vinyalsetal.[47]introducedSC2LE,aRLenvironmentbasedon
thegame StarCraftII thatsimplifiesthedevelopmentofspecialized
agents for a multi-agent environment.
Hesselet al.[26] analyzed six extensions of the DQN algorithm
forRLandtheyreportedthecombinationsthatallowtoachievethe
best results in terms of training time on the Atari 2600 benchmark.
Bakeretal.[15]exploredtheuseofRLinamulti-agentenviron-
ment(i.e., thehideandseek game).Theyreport thatagentscreate
self-supervisedautocurricula[ 31],i.e.,curriculanaturallyemerging
fromcompetitionandcooperation.Asaresult,theauthorsfound
evidence of strategy learning not guided by direct incentives.
Berneret al.[13] reported that state-of-the-art RL techniques
weresuccessfullyusedinOpenAIFivetotrainanagentabletoplay
Dota2andtodefeattheworldchampionin2019(TeamOG).Finally,Mesentier etal.[
21]reportedthatAIagentscouldbeeasilytrained
to explore the states of a board game (Ticket to Ride ) performing
automated play-testing.
6.2 Testing of Video Games
Functionaltestingofvideogamesaimsatfindingunexpectedbe-
haviors in a game. Defining the test oracle, i.e.,determining if a
specificgamebehaviorisdefective,isnottrivial.Severalcategories
of test oracles were identified to determine if a bug was found:crash(the game stops working) [
38,50],stuck(the agent can not
winthegame)[ 38,50],gamebalance (gametooeasyortoohard)
[50],logical(an invalid state is reached) [ 50], anduser experience
bugs(related to graphic and sound, e.g.,glitches) [ 38,50]. While
heuristics can be used to find possible crash-, stuck-, and game-balance-related bugs [
50], logical and user-experience bugs may
require the developers to manually define an oracle.
Iftikharet al.[28] proposed a model-basedtesting approach for
automatically perform black-box testing of platform games. More
recent approaches mostly rely on RL.Pfauetal.[38]introducedICARUS,aframeworkforautonomous
play-testingaimed atfinding bugs.ICARUSsupports thefully au-
tomated detection of crashandstuckbugs, while it also provides
semi-supervised support for user-experience bugs.
Zhenget al.[50] used Deep Reinforcement Learning (DLR) in
theirapproach, Wuji.Wuji balancesthe aimof winningthegame
and exploring the space to find crash,stuck,game balance, and
logicalbugs in three video games (one simple, Block Maze and two
commercial, L10andNSH).
Bergdahl etal.[16]definedaDLR-basedmethodwhichprovides
supportforcontinuousactions(e.g., mouseorgame-pads)andthey
experimented it with a first-person shooter game.
Wuet al.[48] used RL to automatically perform regression test-
ing,i.e.,to compare the game behaviors in different versions of
agame.TheyexperimentedwithsuchanapproachonaMassive
Multiplayer Online Role-Playing Game (MMORPG).
Ariyurek et al.[14] experimented RL and Monte Carlo Tree
Search(MCTS)todefinebothsyntheticagents,trainedinacom-
pletely automated manner, and human-like agents, trained on tra-
jectories used by human testers.
Finally,AhumadaandBergel[ 12]proposedanapproachbased
on genetic algorithms to reproduce bugs in video games by recon-
structing the correct sequence of actions that lead to the desired
faulty state of the game.
6.3 Game- and Level-Design Assessment
One of the main goals of a video game is to provide a pleasantgameplay to the player. Assessing the game balance and other
aspectsrelatedtogame-andlevel-designis,therefore,ofprimary
importance.
For this reason, previous work defined several approaches for
automaticallyfindinggame-andlevel-designissuesinvideogames.
Zooket al.[51] proposed an approach based on Active Learning
(AL)tohelpdesignersperforminglow-levelparametertuning.They
experimented such an approach on a shoot ’em up game.
Gudmundsson et al.[25] introduced an approach based on Deep
Learningtolearnhuman-likeplay-testingfromplayerdata.They
usedaCNNtoautomaticallypredictthemostnaturalnextactiona
playerwouldtakeaimingtoestimatedifficultyoflevelsin Candy
Crush Saga andCandy Crush Soda Saga.
Zhaoet al.[49] report four case studies in which they experi-
menttheuseofhuman-likeagenttrainedwithRLtopredictplayer
interactionswiththegameandtohighlightpossiblegame-design
issues.Onasimilarnote,Pfau etal.[37]useddeepplayerbehav-
ioral models to represent a specific player population for Aion,a
MMORPG. They used such models to estimate the game balance
and they showed that they can be used to tune it.
Finally,Stahlke etal.[43]definedPathOS,atoolaimedathelping
developers to simulate players’ interaction with a specific game
level, to understand the impact of small design changes.
7 CONCLUSIONS AND FUTURE WORK
We presented RELINE, an approach that uses RL to load test video
games.RELINEcan be instantiated on different games using differ-
ent RL models and reward functions.
2312
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:59:43 UTC from IEEE Xplore.  Restrictions apply. Using Reinforcement Learning for Load Testing of Video Games ICSE ’22, May 21–29, 2022, Pittsburgh, PA, USA
Ourproof-of-conceptstudyperformedontwosubjectsystems
showsthefeasibilityofourapproach:Givenarewardfunctionable
torewardtheagentwhenartificialperformancebugsareidentified,
the agent adapts its behavior to play the game while looking for
those bugs.
We performed a case study on a real 3D racing game, Super-
TuxKart, showing the ability of RELINEto identify areas resulting
inFPSdrops.AscomparedtoaclassicRLagentonlytrainedtoplay
the game, RELINEis able to identify a substantially higher number
of low-FPS points (173 vs33).
Despite the encouraging results, there are many aspects that
deserve a deeper investigation and from which our future research
agenda stems. First, we plan additional tests on SuperTuxKart to
betterunderstandhowtheagentreactstochangesinthereward
function(e.g., isitpossibletofindmorelow-FPSpointsinthecentral
part of the game?). Also, with longer training times it should bepossibletotrainanagentabletoplaymorechallengingversions
of this game featuring additional 3D effects (e.g., rainy conditions),
possibly allowing to find new low-FPS points. We also plan to
instantiate RELINEon other game genres (e.g., role-playing games),
possibly by cooperating with their developers.
In our replication package [ 46], we release the code implement-
ing the models used in our study and the raw data of our experi-
ments.
ACKNOWLEDGMENT
This project has received funding from the European Research
Council (ERC) under the European Union’s Horizon 2020 research
and innovation programme (grant agreement No. 851720). Anyopinions, findings, and conclusions expressed herein are the au-
thors’ and do not necessarily reflect those of the sponsors.
REFERENCES
[1][n.d.]. 3D.City-PerformanceIssue42.https://github.com/lo-th/3d.city/issues/42.
[2] [n.d.]. CartPole. https://gym.openai.com/envs/CartPole-v0/.
[3][n.d.]. Dwarfcorp - Performance Issue 583. https://github.com/Blecki/dwarfcorp/
issues/583.
[4][n.d.]. Dwarfcorp -Performance Issue64.https://github.com/Blecki/dwarfcorp/
issues/64.
[5][n.d.]. Dwarfcorp - Performance Issue 904. https://github.com/Blecki/dwarfcorp/
issues/904.
[6][n.d.]. Dwarfcorp - Performance Issue 966. https://github.com/Blecki/dwarfcorp/
issues/966.
[7][n.d.]. Geostrike - Performance Issue 214. https://github.com/Webiks/GeoStrike/
issues/214.
[8] [n.d.]. Gym. https://gym.openai.com/.[9] [n.d.]. MsPacman. https://gym.openai.com/envs/MsPacman-v0/.
[10] [n.d.]. PySuperTuxKart. https://github.com/supertuxkart/stk-code.[11]
[n.d.]. VIDEOGAMES:INDUSTRYTRENDS,MONETISATIONSTRATEGIES
& MARKET SIZE 2020-2025 https://www.juniperresearch.com/researchstore/
content-digital-media/video-games-market-report.
[12]TomásAhumadaandAlexandreBergel.2020. ReproducingBugsinVideoGames
using Genetic Algorithms. In 2020 IEEE Games, Multimedia, Animation and Mul-
tiple Realities Conference (GMAX). IEEE, 1–6.
[13]Open AI. 2019. Dota 2 with large scale deep reinforcement learning. arXiv
preprint arXiv:1912.06680 (2019).
[14]S.Ariyurek,A.Betin-Can,andE.Surer.2021. AutomatedVideoGameTesting
UsingSyntheticandHumanlikeAgents. IEEETransactionsonGames 13,1(2021),
50–67. https://doi.org/10.1109/TG.2019.2947597
[15]BowenBaker,IngmarKanitscheider,TodorMarkov,YiWu,GlennPowell,Bob
McGrew, and Igor Mordatch. 2019. Emergent tool use from multi-agent autocur-
ricula.arXiv preprint arXiv:1909.07528 (2019).
[16]J.Bergdahl,C.Gordillo,K.Tollmar,andL.Gisslén.2020. AugmentingAutomated
Game Testing with Deep Reinforcement Learning. In 2020 IEEE Conference on
Games (CoG). 600–603. https://doi.org/10.1109/CoG47356.2020.9231552[17]Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, JohnSchulman, Jie Tang, and Wojciech Zaremba. 2016. OpenAI Gym.
arXiv:arXiv:1606.01540
[18]Bum Hyun Lim, Jin Ryong Kim, and Kwang Hyun Shim. 2006. A load test-ing architecture for networked virtual environment. In 2006 8th International
Conference Advanced Communication Technology, Vol. 1. 5 pp.–848. https:
//doi.org/10.1109/ICACT.2006.206095
[19]C. Cho, D. Lee, K. Sohn, C. Park, and J. Kang. 2010. Scenario-Based Approach
for Blackbox Load Testing of Online Game Servers. In 2010 International Confer-
ence on Cyber-Enabled Distributed Computing and Knowledge Discovery. 259–265.
https://doi.org/10.1109/CyberC.2010.54
[20] W. J. Conover. 1998. Practical NonparametricStatistics (3rd edition ed.). Wiley.
[21]Fernando De Mesentier Silva, Scott Lee, Julian Togelius, and Andy Nealen. 2017.
AIas evaluator:Search drivenplaytesting ofmodernboard games.In WS-17-01
(AAAI Workshop - Technical Report). AI Access Foundation, 959–966. 31st AAAI
Conference on Artificial Intelligence, AAAI 2017.
[22]PedroDelgado-Pérez,AnaBelénSánchez,SergioSegura,andInmaculadaMedina-
Bulo. 2021. Performance mutation testing. Software Testing, Verification and
Reliability 31, 5 (2021). https://doi.org/10.1002/stvr.1728
[23]E.W. Grafarend. 2006. Linear and Nonlinear Models: Fixed Effects, Random Ef-
fects, and Mixed Models. Walter de Gruyter. https://books.google.ch/books?id=
uHW2wAEACAAJ
[24]RobertJ.GrissomandJohnJ.Kim.2005. Effectsizesforresearch:Abroadpractical
approach (2nd edition ed.). Lawrence Earlbaum Associates.
[25]Stefan Freyr Gudmundsson, Philipp Eisen, Erik Poromaa, Alex Nodet, Sami Pur-
monen,BartlomiejKozakowski,RichardMeurling,andLeleCao.2018. Human-
likeplaytestingwithdeeplearning.In 2018IEEEConferenceonComputational
Intelligence and Games (CIG). IEEE, 1–8.
[26]Matteo Hessel, Joseph Modayil, Hado Van Hasselt, Tom Schaul, Georg Ostro-
vski,WillDabney,DanHorgan,BilalPiot,MohammadAzar,andDavidSilver.
2018. Rainbow: Combining improvements in deep reinforcement learning. In
Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 32.
[27]Sture Holm. 1979. A simple sequentially rejective multiple test procedure. Scan-
dinavian journal of statistics (1979), 65–70.
[28]S.Iftikhar,M.Z.Iqbal,M.U.Khan,andW.Mahmood.2015. Anautomatedmodel
based testing approach for platform games. In 2015 ACM/IEEE 18th International
Conference on Model Driven Engineering Languages and Systems (MODELS). 426–
435. https://doi.org/10.1109/MODELS.2015.7338274
[29]YungWoo Jung, Bum-Hyun Lim, Kwang-Hyun Sim, HunJoo Lee, IlKyu Park,
JaeYong Chung, and Jihong Lee. 2005. VENUS: The Online Game Simulator
Using Massively Virtual Clients.In Systems Modeling and Simulation: Theory and
Applications. 589–596.
[30]MaximLapan.2018. DeepReinforcementLearningHands-On:ApplyModernRL
Methods, with Deep Q-Networks, Value Iteration, Policy Gradients, TRPO, AlphaGo
Zero and More. Packt Publishing.
[31]Joel Z Leibo, Edward Hughes, Marc Lanctot, and ThoreGraepel.2019. Autocur-
ricula and the emergence of innovation from social interaction: A manifesto for
multi-agentintelligence research. arXiv preprint arXiv:1903.00742 (2019).
[32]XiaozhouLi,ZheyingZhang,andKostasStefanidis.2021. Adata-drivenapproach
for video game playability analysis based on players’ reviews. Information 12, 3
(2021), 129.
[33]Dayi Lin, C. Bezemer, and A. Hassan. 2016. Studying the urgent updates of
popular games on the Steam platform. Empirical Software Engineering 22 (2016),
2095–2126.
[34]Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis
Antonoglou,DaanWierstra,andMartinRiedmiller.2013. Playingatariwithdeep
reinforcement learning. arXiv preprint arXiv:1312.5602 (2013).
[35]Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness,
Marc GBellemare, Alex Graves, MartinRiedmiller, Andreas K Fidjeland,Georg
Ostrovski,etal .2015. Human-levelcontrolthroughdeepreinforcementlearning.
nature518, 7540 (2015), 529–533.
[36]LucaPascarella,FabioPalomba,MassimilianoDiPenta,andAlbertoBacchelli.
2018. How is video game development different from software developmentin open source?. In Proceedings of the 15th International Conference on Mining
Software Repositories, MSR 2018, Gothenburg, Sweden, May 28-29, 2018, Andy
Zaidman, Yasutaka Kamei, and Emily Hill (Eds.). ACM, 392–402.
[37]Johannes Pfau, Antonios Liapis, Georg Volkmar, Georgios N Yannakakis, andRainer Malaka. 2020. Dungeons & replicants: automated game balancing via
deepplayerbehaviormodeling.In 2020IEEEConferenceonGames(CoG).IEEE,
431–438.
[38]JohannesPfau,JanDavidSmeddinck,andRainerMalaka.2017. AutomatedGame
Testing with ICARUS: Intelligent Completion of Adventure Riddles via Unsu-
pervised Solving. In Extended Abstracts Publicationof the Annual Symposiumon
Computer-HumanInteractioninPlay (CHIPLAY’17ExtendedAbstracts) .153?164.
[39] Cristiano Politowski, Fabio Petrillo, and Yann-Gäel Guéhéneuc. 2021. A Survey
of Video Game Testing. arXiv preprint arXiv:2103.06431 (2021).
2313
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:59:43 UTC from IEEE Xplore.  Restrictions apply. ICSE ’22, May 21–29, 2022, Pittsburgh, PA, USA Rosalia Tufano, Simone Scalabrino, Luca Pascarella, Emad Aghajani, Rocco Oliveto, and Gabriele Bavota
[40]Reuven Y. Rubinstein and Dirk P. Kroese. 2004. The Cross Entropy Method: A
Unified Approach To Combinatorial Optimization, Monte-Carlo Simulation (Infor-
mation Science and Statistics). Springer-Verlag.
[41]Adam M. Smith, Mark J. Nelson, and Michael Mateas. 2009. Computational Sup-
port for Play Testing Game Sketches. In Proceedings of the Fifth AAAI Conference
onArtificialIntelligenceandInteractiveDigitalEntertainment (AIIDE’09).AAAI
Press, 167?172.
[42]C.Spearman.1904. TheProofandMeasurementofAssociationBetweenTwo
Things.American Journal of Psychology 15 (1904), 88–103.
[43]Samantha N. Stahlke, Atiya Nova, and Pejman Mirza-Babaei. 2020. Artificial
Players in the Design Process: Developing an Automated Testing Tool for Game
Level and World Design. Proceedings of the Annual Symposium on Computer-
Human Interaction in Play (2020).
[44] supertuxkart. [n.d.]. https://github.com/supertuxkart.
[45]AndrewTruelove,EduardoSantanadeAlmeida,andIftekharAhmed.2021. We’ll
Fix It in Post: What Do Bug Fixes in Video Game Update Notes Tell Us?. In 2021
IEEE/ACM 43rd International Conference on Software Engineering (ICSE). IEEE,
736–747.
[46] Rosalia Tufano. 2021. https://github.com/RosaliaTufano/rlgameauthors.[47]
Oriol Vinyals, Timo Ewalds, Sergey Bartunov, P. Georgiev, A. S. Vezhnevets,
MichelleYeo,AlirezaMakhzani,HeinrichKüttler,J.Agapiou,JulianSchrittwieser,JohnQuan,StephenGaffney,S.Petersen,K.Simonyan,T.Schaul,H.V.Hasselt,D.
Silver, T. Lillicrap, Kevin Calderone, Paul Keet, Anthony Brunasso, D. Lawrence,
Anders Ekermo, J. Repp, and Rodney Tsing. 2017. StarCraft II: A New Challenge
for Reinforcement Learning. ArXivabs/1708.04782(2017).
[48]Yuechen Wu, Yingfeng Chen, Xiaofei Xie, Bing Yu, Changjie Fan, and Lei Ma.
2020. Regression Testing ofMassively Multiplayer Online Role-Playing Games.
In2020 IEEE International Conference on Software Maintenance and Evolution
(ICSME). IEEE, 692–696.
[49]YunqiZhao,IgorBorovikov,AhmadBeirami,JasonRupert,CaedmonSomers,
Jesse Harder, Fernando de Mesentier Silva, John Kolen, Jervis Pinto, Reza
Pourabolghasem, Harold Chaput, James Pestrak, Mohsen Sardari, Long Lin,Navid Aghdaie, and Kazi A. Zaman. 2019. Winning Isn’t Everything: Train-ing Human-Like Agents for Playtesting and Game AI. CoRRabs/1903.10545
(2019). http://arxiv.org/abs/1903.10545
[50]Y.Zheng, X.Xie,T. Su,L. Ma,J.Hao, Z.Meng,Y.Liu, R.Shen,Y. Chen,andC.
Fan.2019. Wuji:AutomaticOnlineCombatGameTestingUsingEvolutionary
Deep ReinforcementLearning. In 2019 34thIEEE/ACM International Conference
on Automated Software Engineering (ASE). 772–784.
[51] Alexander Zook, Eric Fruchter, and Mark O. Riedl. 2014. Automatic playtesting
for game parameter tuning via active learning. ArXivabs/1908.01417(2014).
2314
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:59:43 UTC from IEEE Xplore.  Restrictions apply. 