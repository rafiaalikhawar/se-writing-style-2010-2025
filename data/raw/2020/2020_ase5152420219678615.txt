PYNOSE : A Test Smell Detector For Python
Tongjie Wang*
University of California, Irvine
Irvine, CA, United States
tongjiew@uci.eduYaroslav Golubev*
JetBrains Research
Saint Petersburg, Russia
yaroslav.golubev@jetbrains.comOleg Smirnov
JetBrains Research
Saint Petersburg State University
Saint Petersburg, Russia
oleg.smirnov@jetbrains.com
Jiawei Li
University of California, Irvine
Irvine, CA, United States
jiawl28@uci.eduTimofey Bryksin
JetBrains Research
Saint Petersburg State University
Saint Petersburg, Russia
timofey.bryksin@jetbrains.comIftekhar Ahmed
University of California, Irvine
Irvine, CA, United States
iftekha@uci.edu
Abstract â€”Similarly to production code, code smells also occur
in test code, where they are called test smells. Test smells have a
detrimental effect not only on test code but also on the production
code that is being tested. To date, the majority of the research ontest smells has been focusing on programming languages such asJava and Scala. However, there are no available automated toolsto support the identiï¬cation of test smells for Python, despiteits rapid growth in popularity in recent years. In this paper, westrive to extend the research to Python, build a tool for detectingtest smells in this language, and conduct an empirical analysisof test smells in Python projects.
We started by gathering a list of test smells from existing re-
search and selecting test smells that can be considered language-agnostic or have similar functionality in Pythonâ€™s standardUnittest framework. In total, we identiï¬ed 17 diverse test smells.
Additionally, we searched for Python-speciï¬c test smells bymining frequent code change patterns that can be considered aseither ï¬xing or introducing test smells. Based on these changes,we proposed our own test smell called Suboptimal Assert .T o
detect all these test smells, we developed a tool called P
YNOSE
in the form of a plugin to PyCharm, a popular Python IDE.Finally, we conducted a large-scale empirical investigation aimedat analyzing the prevalence of test smells in Python code. Ourresults show that 98% of the projects and 84% of the test suites inthe studied dataset contain at least one test smell. Our proposedSuboptimal Assert smell was detected in as much as 70.6% of the
projects, making it a valuable addition to the list.
Index T ermsâ€”Test smells, code smells, Python, empirical
studies, code change patterns, mining software repositories
I. I NTRODUCTION
Code smells were introduced to identify potential main-
tainability issues in software systems [1], however, later they
have been used as a measure of design quality of softwareprojects [2], [3], [4]. Researchers found that code smells areassociated with bugs [3], [5], fault-proneness [6], [7], andmaintainability issues in the code base [1]. While investigatingthe underlying reasons for introducing code smells, researchersattributed various factors to this, including developers strug-gling with deadlines [8] or not caring about the impact of theapplied design choices [1].
*The ï¬rst two authors contributed equally to this work.Similarly to production code, test code can also have code
smells, in which case they are called test smells. V an Deursen
et al. [9] deï¬ned test smells as being caused by poor designchoices (similarly to regular code smells) when developing testcases.
1Just like the code smells, test smells make the impacted
test code harder to maintain and comprehend [10]. Moreover,recent studies have shown that test smells also impact thequality of production code [11].
Since test smells have a negative impact on the quality of
production code, it is of great interest and importance to studyand detect them. To date, the majority of the research on testsmells has been focusing on statically typed languages likeJava and Scala [10], [11], [12], [13], [14], [15]. However, inrecent years, Python has been growing in popularity due tobeing the primary language used in Data Science and MachineLearning in particular [16], [17]. Furthermore, despite theempirical evidence against test smells, developers tend not tobe aware of the smells that exist in their tests [13], and thelack of efï¬cient tools can be one of the reasons for it. Tothe best of our knowledge, there are no works that study theexistence and prevalence of test smells in Python code, andno tools exist that speciï¬cally aim at identifying test smells inthis language.
In this paper, we aim to ï¬ll these gaps by curating a list
of possible test smells for Python, a tool for their detection,and an empirical study of their pervasiveness in Python code.We started by conducting a small-scale mapping study to ï¬nddifferent test smells studied in the literature and selectingtest smells that can be considered language-agnostic or haveanalogous functionality in Pythonâ€™s standard Unittest frame-
work. In total, we identiï¬ed 17 diverse test smells. Thesetest smells were all adopted from other papers dedicated todifferent programming languages, but it is natural to assumethat Python has its own speciï¬c test smells. To discoverthem, we used a tool called P
YTHON CHANGE MINER [18] to
1To avoid the ambiguity that exists in testing terminology between
languages and frameworks, in this paper, we will always refer to individual
tests or test methods as test cases and to classes that group them as test suites.
5932021 36th IEEE/ACM International Conference on Automated Software Engineering (ASE)
DOI 10.1109/ASE51524.2021.000592021 36th IEEE/ACM International Conference on Automated Software Engineering (ASE) | 978-1-6654-0337-5/21/$31.00 Â©2021 IEEE | DOI: 10.1109/ASE51524.2021.9678615
978-1-6654-0337-5/21/$31.00  Â©2021  IEEE
search for frequent change patterns in test suites. We manually
evaluated 159 patterns that occur in at least three differentprojects and identiï¬ed 32 possible changes that are related toassert functions in Unittest and are aimed at making the tests
more speciï¬c and simplify the understanding of the testinglogic. We bundled the less speciï¬c versions of these assertionstogether into a single Suboptimal Assert test smell. Thus, a
total of 18 smells were identiï¬ed for Python.
We developed P
YNOSE , a plugin for PyCharm [19] that is
able to detect these smells in the Python code. Using the tool,we performed an empirical study on the prevalence of testsmells in 248 Python projects. Our results indicate that testsmells are indeed common in Python test code, with 98% ofprojects and 84% of test suites having at least one test smell.
Overall, our contributions are as follows:
â€¢We conducted a small-scale mapping study and compileda list of test smells that are applicable to Python.
â€¢We identiï¬ed a new Python-speciï¬c test smell by ana-lyzing Python test code changes.
â€¢We developed a tool called P YNOSE as a plugin for
PyCharm that can detect test smells from Python projectsthat use the standard Unittest framework. P
YNOSE is
available for researchers and practitioners on GitHub:https://github.com/JetBrains-Research/PyNose.
â€¢We report the ï¬ndings pertaining to the pervasiveness oftest smells from an empirical study conducted on 248Python projects.
The rest of the paper is organized as follows. In Section II,
we discuss the existing works in the ï¬eld of test smellsdetection and analysis. Section III describes the choice oftest smells for Python and the search for Python-speciï¬ctest smells. In Section IV, we describe the development of
P
YNOSE and its evaluation, and in Section V, we describe
the empirical study that we conducted using the tool, as wellas its results. In Section VI, we discuss threats to the validityof our study, and, ï¬nally, in Section VII, we conclude ourpaper and discuss possible future work.
II. R
ELA TED WORK
Similarly to production code, test code should be designed
following proper established programming practices [20]. V anDeursen et al. [9] deï¬ned the term test smells as code smells
that are caused by poor design choices when developing testcases and also deï¬ned a catalog of 11 test smells. Later, severalresearchers extended this catalog [14], [21], [22], [23]. Whilethe majority of the research focused on test smells occurringin Java, several researchers investigated other languages anddomains. For example, Bleser et al. investigated test smells inScala [15], [24], while Peruma et al. [25] explored unit testsin mobile applications and identiï¬ed several new test smells.
Researchers have also been investigating the negative im-
pacts of test smells on software development [10], [11],[12], [13], [26]. By conducting two empirical studies, Bavotaet al. [10], [12] showed that test smells are widely spreadthroughout software systems, and most test smells have astrong negative impact on the comprehensibility of test suitesand production code. Spadini et al. [11] investigated therelationship between the presence of test smells and thechange- and defect-proneness of test code, as well as thedefect-proneness of the tested production code. They foundthat some test smells are more change-prone than others, andthey also found that production code tested by smelly tests iscomparatively more defect-prone. Tufano et al. [13] found thattest smells are usually introduced when the corresponding testcode is committed to the repository for the ï¬rst time, and theytend to remain in a system for a long time. Virg Â´Ä±nio et al. [26]
investigated correlations between test coverage and test smells,and found that test smells inï¬‚uence code coverage.
Investigating ways for an automated detection of test smells
has also received attention from the research community. V anRompaey et al. [27] proposed a set of metrics deï¬ned in termsof unit test concepts and compared the proposed detectiontechniques effectiveness with human review. Greiler et al. [14]analyzed the relationship between the development of a testï¬xture and possible test smells within it. They also designeda static analysis tool to identify ï¬xture-related test smells andevaluated them by discovering test smells in three industrialprojects. Palomba et al. [28] developed an automated textual-based approach for detecting several types of test smells.Compared with the code metrics-based techniques proposed
by Greiler et al. and V an Rompaey et al., the textual-basedtechnique proved to be more effective in detecting certain testsmells. Peruma et al. [25], [29] recently developed a tool called
TSDETECT capable of detecting 19 test smells in Java.
More recently, researchers have been investigating ways to
help testers refactor test smells. Lambiase et al. [30] presentedan IntelliJ-based plugin that enables an automated identiï¬ca-tion and refactoring of test smells using IntelliJ Platformâ€™sAPIs. Santana et al. [31] proposed another tool that can beused in an IDE, providing testers with an environment forautomated detection of lines of code affected by test smells,as well as a semi-automated refactoring for Java projects.Virg Â´Ä±nio et al. [32] presented a tool designed to analyze test
suite quality in terms of test smells. Their tool is the ï¬rstone that relies on both code coverage and the presence of testsmells to measure the quality of tests.
Overall, the majority of the mentioned research has been
focusing on Java. However, in recent years, Python has beengrowing more popular because of its important role in DataScience and Machine Learning in particular [16], [17]. To thebest of our knowledge, no tools exist that speciï¬cally aim atidentifying Python test smells. P
YNOSE addresses this gap.
Furthermore, there is no large-scale analysis regarding theprevalence of test smells in Python code, and our study isthe ï¬rst towards ï¬lling this gap in research.
III. S
ELECTING TEST SMELLS
The goal of our study is to build a tool that can identify
test smells in Python code as well as to assess to whatextent test smells are prevalent in Python test suites. Thegeneral pipeline of our study is demonstrated in Figure 1. InSection III, we curate the list of appropriate test smells by
594 ÅµÅ¨Æ˜ÅˆÅŸÄŸÎÃ±ÎÅŸÅˆÆ£Æ¯ÎÅµÄ¹ÎÇÃ±Æ›ÅˆÅµÆ·Æ£ÎÆ¯ÄŸÆ£Æ¯
Æ£Å¨ÄŸÅŸÅŸÆ£ÎÄ¹Æ›ÅµÅ¨ÎÆ¯Å‚ÄŸÎÅŸÅˆÆ¯ÄŸÆ›Ã±Æ¯Æ·Æ›ÄŸÎÃ±ÅªÄ—
Æ£ÄŸÅŸÄŸÄÆ¯ÎÃ±Æ˜Æ˜Æ›ÅµÆ˜Æ›ÅˆÃ±Æ¯ÄŸÎÅµÅªÄŸÆ£
'ÅˆÆ£ÄÅµÇÄŸÆ›ÎÂ£Ç–Æ¯Å‚ÅµÅªÍ¨Æ£Æ˜ÄŸÄÅˆÈŒÄÎÆ¯ÄŸÆ£Æ¯ÎÆ£Å¨ÄŸÅŸÅŸÆ£
ÄÇ–ÎÆ£Æ¯Æ·Ä—Ç–ÅˆÅªÄºÎÆ˜ÅµÆ˜Æ·ÅŸÃ±Æ›ÎÄÅ‚Ã±ÅªÄºÄŸÆ£ÎÅˆÅª
Å¨Ã±Æ¯Æ·Æ›ÄŸÎÂ£Ç–Æ¯Å‚ÅµÅªÎÆ˜Æ›ÅµÅ˜ÄŸÄÆ¯Æ£UÅ¨Æ˜ÅŸÄŸÅ¨ÄŸÅªÆ¯ÎÂ£Ç–uÅµÆ£ÄŸÍ“ÎÃ±ÎÆ¯ÅµÅµÅŸÎÄ¹ÅµÆ›
Ä—ÄŸÆ¯ÄŸÄÆ¯ÅˆÅªÄºÎÃ±ÅŸÅŸÎÆ£ÄŸÅŸÄŸÄÆ¯ÄŸÄ—ÎÆ¯ÄŸÆ£Æ¯ÎÆ£Å¨ÄŸÅŸÅŸÆ£
Ã™Ã±ÅŸÅˆÄ—Ã±Æ¯ÄŸÎÂ£Ç–uÅµÆ£ÄŸÎÄÇ–ÎÅ¨Ã±ÅªÆ·Ã±ÅŸÅŸÇ–
ÄŸÇÃ±ÅŸÆ·Ã±Æ¯ÅˆÅªÄºÎÄ—ÄŸÆ¯ÄŸÄÆ¯ÄŸÄ—ÎÆ¯ÄŸÆ£Æ¯ÎÆ£Å¨ÄŸÅŸÅŸÆ£Â®Æ¯Æ·Ä—Ç–ÎÆ¯Å‚ÄŸÎÆ˜Æ›ÄŸÇÃ±ÅŸÄŸÅªÄÄŸÎÅµÄ¹ÎÆ¯ÄŸÆ£Æ¯
Æ£Å¨ÄŸÅŸÅŸÆ£ÎÅˆÅªÎÂ£Ç–Æ¯Å‚ÅµÅªÎÄÅµÄ—ÄŸ
Â®Æ¯Æ·Ä—Ç–ÎÆ¯Å‚ÄŸÎÄÅµÍ¨ÅµÄÄÆ·Æ›Æ›ÄŸÅªÄÄŸÎÅµÄ¹ÎÆ¯ÄŸÆ£Æ¯
Æ£Å¨ÄŸÅŸÅŸÆ£ÎÅˆÅªÎÂ£Ç–Æ¯Å‚ÅµÅªÎÄÅµÄ—ÄŸÂ®ÄŸÄÆ¯ÅˆÅµÅªÎUUU Â®ÄŸÄÆ¯ÅˆÅµÅªÎÃ™ Â®ÄŸÄÆ¯ÅˆÅµÅªÎUÃ™
Fig. 1. The overall pipeline of the study.
conducting a systematic mapping study (Section III-A) and
then augmenting the list by identifying Python-speciï¬c testsmells (Section III-B).
A. Systematic mapping study of test smells
As a ï¬rst step, we conducted a small-scale systematic map-
ping study on test smells to curate a list of test smells discussed
in the literature. According to Kitchenham et al. [33], the goalof the mapping study is to survey the available knowledgeabout a topic.
Search Question. Our search question was phrased as fol-
lows: What test smells have been studied in literature to date?
Search Keywords. To determine the optimal set of search
keywords, we conducted a pilot search on two well-knowndigital libraries, IEEE and ACM. This process was intendedto identify relevant words utilized in test smell publications.We conducted our query only on the title and abstract of thepublication to avoid false positives. The ï¬nalized search stringis presented below.
Title: (â€œtest smellâ€ OR â€œtest smellsâ€) AND Abstract:
(â€œtest smellâ€ OR â€œtest smellsâ€).
Data Source. To discover relevant publications, we used
three of the most popular online paper search engines: ACMDigital Library, IEEE Xplore, and Scopus.
Search Period. To obtain as many related works as possible,
we queried all related studies before 2020. This resulted in alist of papers that were published between 2006 to 2020.
Initial Results. Our initial search of the three digital libraries
resulted in 54 publications. To narrow down the search results,next, we ï¬ltered out publications that were not part of ourinclusion criteria. A summary of the inclusion and exclusioncriteria used to ï¬lter the retrieved literature is shown in Table I.The ï¬ltering process helped us to reduce the number of studiessigniï¬cantly, however, this may have resulted in leaving outsome relevant studies. Thus, we conducted backward snow-balling [34] (i.e., looking for additional studies in the reference
lists of the selected studies, as suggested by Keele et al. [35]).In our work, we implemented a single iteration of backwardsnowballing.
To ensure the reliability of the selected studies, each
study was evaluated by three authors of this paper. EachTABLE I
INCLUSION AND EXCLUSION CRITERIA .
Inclusion Criteria
1. Publications that implement software engineering methodologies,approaches, and practices in test smell detection and refactoring.2. Available in digital format.
Exclusion Criteria
1. Publications that are not written in English.2. Websites, leaï¬‚ets, and grey literature.3. Published in 2021.4. Full-text not available online.5. Tools not associated with peer-reviewed papers.6. Duplicated publications.
selected study underwent an agreement process, and in caseof uncertainty and disagreement, we discussed it until wereached consensus. We ï¬nally ended up with with a set of29 studies. Next, we merged the lists of test smells mentionedin these papers, which resulted in a list of 33 different testsmells encountered in Java, Scala, and Android systems. Thefull list of papers and test smells is available online in thesupplementary materials [36].
Next, we considered the possibility of implementing each
test smell for Python. There were several reasons why someof the test smells could not be implemented:
The test smell is not applicable to Python. For example, the
Resource Optimism [9] test smell in Java occurs if a File
object is used without checking for its existence. However,in Python, ï¬les always associate with resources, because,according to the Python ofï¬cial documentation, â€œopen() is
the standard way to open ï¬les for reading and writing withPythonâ€ [37].
The test smell detection relies on the production code that is
being tested. For example, to identify Eager Test [9] and Lazy
Test [9], we need to know what the corresponding production
ï¬les and production classes are. A lot of recent works studytest-to-code traceability [38], [39], [40] and a lot of differentapproaches have been suggested. However, reliably makinga strict one-to-one connection between a test method anda production method in the static analysis environment isdifï¬cult [39], which is why leave the support of such testsmells for future work.
595The test smell detection is possible only when the test is
executed. For example, for the Test Run War [9], it is necessary
to actually run the test case, which is not possible in a static
analysis environment. Even after running, identifying such testsmells is non-trivial, and for practical purposes we had toexclude them.
Finally, we selected 17 test smells for implementing. We
list them below.
Assertion Roulette occurs when a test case has multi-
ple non-documented assertions. Multiple assertion statementswithout a descriptive message impact the readability, under-standability, and maintainability, as it becomes more difï¬cultto understand the reason why this test fails [9].
Conditional Test Logic runs against the rule that test cases
need to be simple and execute all statements in the productioncode. Conditions within the test case alter the behavior of thetest and lead to situations where the test fails to detect defectsin the production code under some conditions [25].
Constructor Initialization is made by developers who are
unaware of the purpose of the setUp() method that contains
the preparation needed to perform test cases. As a result, theywould deï¬ne a constructor for the test suite, which is not idealin practice [25].
Default Test occurs when an IDE creates default test suites
when the project is created and developers keep the defaultname. For example, PyCharm by default names the test suitesMyTestCase. These suites are meant to serve as an examplefor developers when writing unit tests and should be renamed.Not renaming them upfront causes developers to start addingtest cases into these ï¬les, making the default test suite acontainer of all test cases. This can also cause problems whenthe suites need to be renamed in the future [25].
Duplicate Assert occurs when a test case tests for the same
condition multiple times [25].
Empty Test occurs when a test case does not contain
executable statements. Such tests are possibly created fordebugging purposes and then forgotten about or contain com-mented out code [25].
Exception Handling occurs when passing or failing of a
test case is dependent on the production method explicitlythrowing an exception. Instead, developers should utilize spe-cial functionality of testing frameworks for that, such as anassertRaises() function [25].
General Fixture occurs when a test suite ï¬xture is too
general and some test cases only access a part of it. The ï¬xtureof a test suite is a special method that is executed before thetest cases in the suite and serves as a setup step. A drawbackof it being too general is that unnecessary work is being donewhen a test suite is run [9].
Ignored Test is caused by ignored test cases when it is
possible to suppress some test cases from running. These ig-nored test cases add unnecessary overhead by increasing codecomplexity and making comprehension more difï¬cult [25].
Lack of Cohesion of Test Cases occurs if test cases are
grouped together in one test suite but are not cohesive. Cohe-sion of a class is a metric that indicates how well various partsand responsibilities of a class are tied together. If test cases ina suite are not cohesive, this can cause comprehensibility andmaintainability issues [14].
Magic Number Test occurs when assert statements in a test
case contain numeric literals ( i.e., magic numbers) as param-
eters instead of more descriptive constants or variables [25].
Obscure In-Line Setup occurs when the test case contains
too many setup steps. This can hinder inferring the actualpurpose of the assertion in the test. Ideally, such preparationshould be moved to a ï¬xture or a separate method [14].
Redundant Assertion occurs when a test case contains
assertion statements that are either always true or always false,and are therefore unnecessary [25].
Redundant Print occurs when there is a print statement
within the test. Print statements are considered to be redundantin unit tests as unit tests are usually executed as a part of anautomated process with little to no human intervention [25].
Sleepy Test occurs when developers need to pause the
execution of statements in a test case for a certain duration(i.e., simulate an external event) and then continue with theexecution. Explicitly causing a thread to sleep can lead tounexpected results as the processing time for a task can varyon different devices [25].
Test Maverick was derived from the General Fixture de-
scribed above. If the test suite has a ï¬xture with setup, buta test case in this suite does not use this setup, this test caseis a maverick (outlier). The setup procedure will be executedbefore the test case is executed, but it is not needed [14].
Unknown Test occurs when the test case has no assertion
in it. It is possible to create a test case that does not useassertions, however, such a test is more difï¬cult to understandand interpret [25].
During this selection, we also decided to focus speciï¬cally
on the Unittest testing framework [41] that is included into
the Python Standard Library. Python also has a lot of popularthird-party testing frameworks like PyTest [42] and Robot [43],
however, certain test smells would look differently in differentframeworks, and it is out of the scope of this paper to supportthem all. There are two reasons for choosing speciï¬callyUnittest. Firstly, it remains one of the most popular testingframeworks in Python while also being the default one [44].Secondly, according to its documentation, Unittest was origi-
nally inspired by JUnit [41], which allows us to detect some
test smells from the literature that were originally proposed forJUnit, for example, ï¬xture-related test smells. Additionally,several other frameworks support launching test suites fromUnittest, and can therefore also be detected in this case.
B. Identifying Python-speciï¬c test smells
In addition to the test smells identiï¬ed above, our goal
was to include Python-speciï¬c test smells. To discover
Python-speciï¬c test smells, we used a tool called P
YTHON -
CHANGE MINER [18] to search for frequent change patterns in
the histories of test suites. We explain the steps of this process
in detail in this section.
5961) Project selection: To carry out this research, we needed
to collect a dataset of mature open-source Python projects. As
a starting point, we took GHTorrent [45], a large collection ofGitHub data, more speciï¬cally, their latest dump at the timeof the compilation, compiled in July 2020 [46]. To processit, we used a tool called PGA-create [47] that had beenpreviously used to create Public Git Archive (PGA) [48]. Thistool processes the SQL dump to create a CSV ï¬le with a listof projects that facilitates their convenient ï¬ltering. Next, weselected all projects with at least 50 stars, which allowed us toï¬lter out toy projects. We also only considered projects withPython as the main language that are not forks. This resultedin identifying 26,072 projects. Of them, we randomly selected10,000. The reason for not simply picking top projects by starsis that testing might be organized very differently in projectsof different scale, and simply picking the largest or the mostpopular repositories could skew our data towards a speciï¬ctype of projects.
Next, we analyzed the history of the projects to ï¬nd all
commits where at least one Python test ï¬le was changed. Wedeï¬ned a Python test ï¬le as any ï¬le with the .py extension
that has the word test in its ï¬lename, since Unittest has a
naming convention of having the word test in the name of
the test ï¬le [41]. We have conducted a small manual analysisby selecting 100 random Python ï¬les with the word test
in their name and checking whether they are actually relatedto tests. In this random sample, all 100 ï¬les were related totesting, with 96 explicitly containing test suites and test cases,and another 4 containing auxiliary methods and testing utils.
4,580 of the projects had at least one commit that changed
such ï¬les. As we were looking for code changes, we selectedthese 4,580 projects. Since our goal was to analyze thechanges themselves, for practical purposes, we decided toselect a smaller set of projects using the criteria recommendedin literature [49]. We selected projects with at least 1,000commits, 10 contributors, 2 years since the ï¬rst commit andno more than 1 year since the last push. This resulted in 450projects. For the purposes of this paper, we will call this thePrimary dataset; the list is available online [36].
2) Change pattern mining: To identify Python-speciï¬c test
smells, we started by mining the histories of the collectedprojects and ï¬nding patterns in the changes made to test ï¬lesthat might be considered as either ï¬xing or introducing atest smell. We extracted all changes made to Python test ï¬lesfrom the identiï¬ed 450 projects and processed these ï¬les using
P
YTHON CHANGE MINER [18].
PYTHON CHANGE MINER is a tool that we developed for
mining code change patterns in Python code. The tool isbased on the algorithm developed by Nguyen et al. [50] forJava. The parser in their tool is written speciï¬cally for thesyntax of the Java language, and their tool stores graphs andworks with them as Java objects, so we could not directlyreuse the tool. At the same time, the algorithm itself is notlanguage-speciï¬c, because it relies only on the abstract syntaxtrees (AST) of code before and after the change, which iswhy we implemented it for Python. The operation processof P
YTHON CHANGE MINER is similar to that of the tool by
Nguyen et al. Here, we brieï¬‚y explain the procedure.
PYTHON CHANGE MINER works in two stages: building
change graphs and mining patterns. In the ï¬rst stage, theversions of code before and after the change are parsed intoa special representation introduced by Nguyen et al. calledï¬ne-grained Program Dependence Graphs (fgPDGs). fgPDGs
are graphs with three types of nodes: data nodes (variables,
literals, constants, etc.), operation nodes (arithmetic, bit-wise
operations, etc.), and control nodes (control sequences like
if,while, for, etc.). These nodes are connected using two
types of edges: control edges represent a connection between
acontrol node and a node that it controls and data edges show
the ï¬‚ow of the data in the program, such edges also have labelsspecifying the ï¬‚ow of data.
Then, unchanged nodes in the two fgPDGs of code before
and after the change are connected together by special map
edges, resulting in new graphs called change graphs. We used
GumTree [51] to detect corresponding unchanged nodes in theversions before and after the change and connect them with amap edge. This is carried out on a function level and therefore,
this way, we obtain a special change graph that represents each
change to each testing function from the history of projects inour dataset. Y ou can ï¬nd an example of fgPDGs and a changegraph in the supplementary materials [36].
The second stage of P
YTHON CHANGE MINER involves
searching these change graphs for patterns. This part is alsodone similarly to the work of Nguyen et al. [50]. First, all pairsof nodes representing function calls that are also connectedwith the map edge are considered to be the initial patterns that
are then recursively expanded to contain new nodes. The pat-tern is deï¬ned by two thresholds: minimum size, indicating the
minimum number of graph nodes in the pattern, and minimum
frequency, indicating the minimum number of repetitions ofthe pattern in the corpus. Changing these parameters inï¬‚uenceswhat is considered to be a pattern and, therefore, how manypatterns are detected. This way, the patterns are expanding todetect isomorphic subgraphs within our corpus of graphs.
In our work, we use the same thresholds as Nguyen et
al.: minimum size of 3 and minimum frequency of 3. It is
possible that studying speciï¬cally the testing code requiresdifferent thresholds, we leave such analysis for future work.We additionally add a maximum size threshold of 20. This is
done to make the process faster by stopping the patterns fromgrowing too large. Our own preliminary experiments and ouranalysis of the results of Nguyen et al. demonstrated that themajority of discovered patterns are small. More speciï¬cally,the Depth pattern corpus provided by Nguyen et al. [50]
contains a total of 9,289 patterns, of which 8,697 (93.6%)patterns are 20 nodes or smaller. Since smaller patterns aremuch more frequent and are easier to analyze, we decided tofocus on them. An example of a discovered pattern is presentedin Figure 2.
3) Test smells detection: In total, P
YTHON CHANGE MINER
was able to discover 8,239 different patterns in the Primary
dataset. Of them, 652 patterns were cross-project, meaning
597VHOI 
*5
 UHVXOWV DVVHUW7UXH LQ
VHOI 
*5
 UHVXOWVDVVHUW,Q 
(a) Commit in the Obspy project [52].
VHOI IRR OOYPBEF DVVHUW7UXH LQ
VHOI IRR OOYPBEFDVVHUW,Q 
(b) Commit in the Numba project [53].
VHOI 
SDVVZRUG
 DFFRXQWGDWD DVVHUW7UXH LQ
VHOI 
SDVVZRUG
 DFFRXQWGDWDDVVHUW,Q 
(c) Commit in the Reviewboard project [54].
Fig. 2. An example of a change pattern identiï¬ed in several projects on
GitHub.
they were encountered in at least two different projects, and
159 appeared in at least three different projects. Three authorsof the paper independently manually labeled all 159 of suchchanges to discover changes that either ï¬x or introduce possi-ble test smells. The reason for focusing on these changes is thatthey are inherently more universal among different developers.Along with analyzing the code changes themselves, the authorsalso looked at the corresponding commit messages, sincecommit messages may contain the rationale for a change.After individual labeling, the authors discussed their labelsand reached a perfect agreement.
Of the studied 159 patterns, 70 (44%) constituted various
changes to assertion functionality, similar to the exampleshown in Figure 2. Three authors of the paper independentlycame to a conclusion that the candidates for possible Python-speciï¬c test smells can be found only within this group,because other common changes in testing code correlate tovarious other aspects of software engineering: data structures,data processing, etc., that are not directly related to testingitself. For example, popular changes include changing the levelof the logger (error, info, debug, etc.) or changing the shapeof anumpy array. Such patterns are important, but are not
directly related to testing or test smells.
We categorized assert-related change patterns into three
categories, which we describe below with speciï¬c examples.
i.Assertion changes that alter the logic. Often, when de-
velopers change an assertion in a test case, they do it to updatethe logic behind the test. For example, a pattern that occurredin six different projects is changing from assertEqual
toassertRegex. This way, instead of checking for an
exact equality between an object and a string, a regularexpression is passed that can support variations in strings. Onecommit message reads: Use a more permissive comparison for
jsonschema.V alidationError messages [55].
Another common pattern involved changing from
assertEqual toassertIn, where instead of one
correct result, there is a list of values. Conversely, anothercommon example is changing from assertIsNone to
another function assertIsInstance. This makes the
check more speciï¬c: the object is not compared to None
but rather needs to be an object of a new speciï¬c class. Onecommit message conveys a similar idea: NullSort instead of
None. A more descriptive placeholder for â€œdonâ€™t sortâ€ [56].ii.Assertion changes that do not alter the logic and use
more appropriate functions.
A large portion of the patterns involved keeping the asser-
tion logic the same, but replacing the assertion function witha more appropriate one to make the code succinct. In total,eight such patterns were identiï¬ed. These changes are Python-speciï¬c in the sense that they rely heavily on a wide range ofassertion functions that Unittest supports.
The most popular pattern is shown in Figure 2. It oc-
curs in seven different projects and moves from usingassertTrue(X in Y) toassertIn(X, Y). One com-
mit message describes this change in great detail: Use more
speciï¬c assertions for â€˜inâ€™ checks. A lot of old code usedâ€˜assertTrue(blah in blah)â€™, or variants on that, which didnâ€™t tellyou much if there was a failure. Nowadays, we have assertInand assertNotIn, which we can use instead. This switches ourtests to use these [54]. This commit message indicates that
the original code (before the change) can be considered a testsmell since using general assertions can make it difï¬cult toinfer the reason of failure by â€œhidingâ€ the actual assertion inits body, whereas using speciï¬c assertions can make it easier.
Another change that strives to remove the ambigu-
ity of a general assertion occurs in four repositories,and it moves from using assertFalse(X == Y) to
assertNotEqual(X, Y). Sometimes assertTrue is
changed to another speciï¬c assertion. For example, in threedifferent projects assertTrue(X <= Y) is changed to
Unittest â€™sassertLessEqual(X, Y). One commit mes-
sage expectedly comments this: Use more speciï¬c asserts in
unit tests [57].
In Python, it is considered bad practice to check the equality
of a boolean value when you can check the value itself,so in this case a boolean assertion is more correct andmore interpretable, which is reï¬‚ected in a common changepattern where assertEqual(X, False) is changed to
assertFalse(X).
In this section, we have given examples of some commit
messages that describe the changes along with the changepattern. We believe that these commit messages justify con-sidering the wrong choice of an assertion function in Unittest
as a test smell. We called this smell Suboptimal Assert .
iii.Assertion changes that do not alter the logic and use
less appropriate functions.
Interestingly, we also discovered seven change patterns that
move from an appropriate assertion function to a more generalone. Following the logic of the previous section, these can betreated as introducing a test smell.
The most popular such change is moving from a
more speciï¬c assertIsNotNone(X) toa more general
assertNotEqual(X, None). One commit message de-scribes this change as a Fix in test for Python 2.6 compatibil-
ity[58]. However, the changes in this pattern were made in
2014â€“2015, and since Python 2 is deprecated from 2020, thisis no longer a problem.
A similar message described commits in two dif-
ferent projects that moved from assertNotIn(X, Y)
598IÃ‚UÎÅ¨ÅµÄ—ÄŸ
iÃ±Æ·ÅªÄÅ‚ÎÂ£Ç– Å‚Ã±Æ›Å¨
ÅˆÅªÎIÃ‚UÎÅ¨ÅµÄ—ÄŸ
iÃ±Æ·ÅªÄÅ‚ÎÂ£Ç– Å‚Ã±Æ›Å¨
ÅˆÅªÎÆ¯Å‚ÄŸÎÄÃ±ÄÅ›ÄºÆ›ÅµÆ·ÅªÄ—Â€Æ˜ÄŸÅªÎÆ¯Å‚ÄŸÎ
ÅªÄŸÄÄŸÆ£Æ£Ã±Æ›Ç–ÎÆ˜Æ›ÅµÅ˜ÄŸÄÆ¯Â£Ã±Æ›Æ£ÄŸÎÃ±ÅŸÅŸÎÍ’Æ˜Ç–ÎÈŒÅŸÄŸÆ£Î
ÅˆÅªÆ¯ÅµÎÂ£Â®UHÅˆÅŸÄŸÎÅµÄÅ˜ÄŸÄÆ¯Æ£Â£Ã±Æ›Æ£ÄŸÎÆ¯Å‚ÄŸÎÂ£Â®UÎÆ¯Æ›ÄŸÄŸÎÅµÄ¹Î
ÄŸÃ±ÄÅ‚ÎÈŒÅŸÄŸÎÆ¯ÅµÎÈŒÅªÄ—ÎÃ±ÅŸÅŸÎÆ¯ÄŸÆ£Æ¯Î
Æ£Æ·ÅˆÆ¯ÄŸÆ£ÎÃ±ÅªÄ—ÎÄÃ±Æ£ÄŸÆ£Â£Ã±Æ›Æ£ÄŸÎÆ¯Å‚ÄŸÎÂ£Â®UÎÆ¯Æ›ÄŸÄŸÎÅµÄ¹Î
ÄŸÃ±ÄÅ‚ÎÄÃ±Æ£ÄŸÎÆ¯ÅµÎÄ—ÄŸÆ¯ÄŸÄÆ¯Î
ÅªÄŸÄÄŸÆ£Æ£Ã±Æ›Ç–ÎÆ¯ÄŸÆ£Æ¯ÎÆ£Å¨ÄŸÅŸÅŸÆ£iÅˆÆ£Æ¯ÎÄ—ÄŸÆ¯ÄŸÄÆ¯ÄŸÄ—ÎÆ¯ÄŸÆ£Æ¯Î
Æ£Å¨ÄŸÅŸÅŸÆ£ÎÅˆÅªÎÂ£Ç– Å‚Ã±Æ›Å¨
Â®Ã±ÇÄŸÎÆ¯Å‚ÄŸÎÅµÆ·Æ¯Æ˜Æ·Æ¯
ÅˆÅªÆ¯ÅµÎÃ±ÎeÂ®Â€uÎÈŒÅŸÄŸ
 iUÎÅ¨ÅµÄ—ÄŸIÃ‚UÎÅ¨ÅµÄ—ÄŸ
 iUÎÅ¨ÅµÄ—ÄŸ
Fig. 3. The pipeline of P YNOSE operation in two regimes: GUI mode and CLI mode.
toassertTrue(X not in Y) and two more dif-
ferent projects that moved from assertLess(X, Y)
toassertTrue(X < Y). The same changes can be
found for functions like assertGreater(X, Y) and
assertIsNone(X), with one commit message saying: re-
move fancy test assertions that are unavailable on 2.6 [59].
In total, we encountered 12 different suboptimal asserts (ei-
ther ï¬xed, introduced, or both). We extrapolated them to simi-
lar functions and opposite cases where necessary: for example,if there is a suboptimal assert that contains assertLess,
it can also be formulated for assertGreater, etc. This
resulted in a total of 32 different assertions that can beconsidered a part of the Suboptimal Assert test smell, the full
list is available online [36].
IV . P
YNOSE
Once we curated the list of test smells (explained in Sec-
tion III), our next goal was to implement a tool to identify themin actual Python code. We developed a tool called P
YNOSE
that currently identiï¬es 18 test smells (17 language-agnosticfrom the existing literature and one Python-speciï¬c elicited byus as described in Section III-B), and can be run from boththe graphical user interface and the command line. Figure 3shows the operating pipeline of P
YNOSE . In this section, we
explain it in greater details.
A. Tool internals
PYNOSE is implemented as a plugin for PyCharm [19], a
popular IDE for Python developed by JetBrains. The plugin
supports two different modes of operation: Graphical User
Interface (GUI) mode and Command Line Interface (CLI)
mode. Internally, P YNOSE uses Program Structure Interface
(PSI) [60] from JetBrainsâ€™ IntelliJ Platform (that PyCharm isbuilt upon) to parse Python source code and build syntacticand semantic code models for analysis. When the project isopened and the interpreter is set up, the tool uses PSI and otherrelated PyCharm API to gather all .py ï¬les in the project in
the form of PSIFile objects.
Next, the tool extracts all Python classes that are
sub-classes of unittest.TestCase. With the help of
PSI, P
YNOSE can deal with importing unittest or
unittest.TestCase under alias or test cases that are
not direct sub-classes of unittest.TestCase. After col-
lecting individual test suites, each detector class (correspond-ing to each test smell) invokes PsiElementVisitor to
create a custom visitor for the necessary PsiElement,which allows P
YNOSE to identify test smells. For ex-
ample, for the Magic Number Test, we use a custom
visitor of PyCallExpression to ï¬nd all assertions,
and then check if one of the provided arguments is aPyNumericLiteralExpression. If there is a match, theMagic Number Test smell is declared to be found.
For the test smells from the literature, we implemented
their detection in the same way as they are described in theoriginal papers, using the mentioned thresholds. For example,we detect Obscure In-Line Setup the same way as Greiler
et al. [14], by counting the number of local variables in atest case and ï¬‚agging the case as smelly if this number islarger than a threshold of 10, and detect Lack of Cohesion
of Test Cases the same way as Palomba et al. [28], by
calculating pairwise cosine similarities between test cases.Detection rules for all the supported test smells are presentedin Table II, the citations mark the works, from where thedetection rules were adapted from. Where necessary, we usedcode entities analogous to their counterparts in Java, forexample, @unittest.skip() decorator in the place of the
@Ignore annotation. If there were several different heuristics
to detect the same smell in different papers, we selected onebased on its recency and its convenience to implement usingthe PSI and the IntelliJ platfrom.
When the analysis is done, P
YNOSE can show the detected
test smells inside the IDE or save them to a JSON ï¬le forfurther analysis.
B. Evaluation
We conducted an experimental evaluation of the effective-
ness of P
YNOSE in correctly detecting test smells. As there
are no existing datasets containing information for all the sup-
ported smells, we decided to construct our own validation set.We randomly selected eight projects that did not make it intothePrimary dataset. We then used the deï¬nitions of test smells
to identify and tag test ï¬les with the information regarding thetypes of smells they exhibit. This process resulted in a totalof 37 annotated ï¬les. The list of projects, together with somestatistics about their testing ï¬les, is shown in Table III. Toensure an unbiased annotation process, three authors of thepaper individually did the labelling and discussed their resultsafterwards to reach a consensus. All the three authors haveexperience with Python development ranging from two to ï¬veyears, which includes exposure to developing unit tests.
Next, we ran P
YNOSE on the same set of projects and com-
pared our results against the oracle. We calculated precision,recall, and F1 score for each test smell. We also calculated
599TABLE II
THE DETECTION RULES FOR SUPPORTED TEST SMELLS .C ITA TIONS INDICA TE WORKS WHERE THE RULES WERE ADOPTED FROM .
Assertion Roulette A test case contains more than one assertion statement without an explanation/message. [29]
Conditional Test Logic A test case contains one or more control statements (i.e., if,for, while). [29]
Constructor Initialization A test suite contains a constructor declaration (an __init__ method). [29]
Default Test A test suite is called MyTestCase. [29]
Duplicate Assert A test case contains more than one assertion statement with the same parameters. [29]
Empty Test A test case does not contain a single executable statement. [29]
Exception Handling A test case contains either the try/except statement or the raise statement. [29]
General Fixture Not all ï¬elds instantiated within the setUp() method of a test suite are utilized by all test cases in this test suite. [29]
Ignored Test A test case contains the @unittest.skip decorator. [29]
Lack of Cohesion of Test Cases The mean of the pairwise cosine similarities between test cases in a test suite â‰¤0.4. [28]
Magic Number Test A test case contains an assertion statement that contains a numeric literal as an argument. [29]
Obscure In-Line Setup A test case contains ten or more local variables declarations. [14]
Redundant Assertion A test case contains an assertion statement in which (1) the expected and actual parameters of equality are the
same, e.g.,assertEqual(X, X) or (2) the assertion of truth is carried out on the unchangeable object, e.g.,
assertTrue(True). [29]
Redundant Print A test case invokes the print() function. [29]
Sleepy Test A test case invokes the time.sleep() function with no comment. [29]
Suboptimal Assert A test case contains at least one of the suboptimal asserts.
Test Maverick A test suite contains at least one test case that does not use a single ï¬eld from the SetUp() method. [14]
Unknown Test A test case does not contain a single assertion statement. [29]
TABLE III
EIGHT PROJECTS SELECTED FOR THE EV ALUA TION OF PYNOSE .THE
COLUMNS INDICA TE THE NUMBER OF TESTING FILES ,SUITES ,AND CASES
WITH Unittest .
Project T. Files T. Suites T. Cases
ali1234/vhs-teletext 12 23 56
cea/sec ivre 1 1 13
davidhalter/jedi 3 4 17
demisto/content 9 10 203
justiniso/polling 1 1 4
Lagg/steamodd 6 13 45
plamere/spotipy 3 17 114
pygridtools/drmaa-python 2 4 16
Total 37 73 468
the weighted average of these three metrics for all test smells
with the weights being the number of instances of each testsmell in the projects. The results of the conducted evaluationare presented in Table IV.
Several test smells were encountered very rarely in the
validation projects, with three of them having only a singleexample. This has to do with the fact that these test smells arejust rare in Python in general (see Section V-C2). However,these test smells have very robust deï¬nitions that are easyto detect: Default Test requires the tool to simply check the
name of the test suite, Constructor Initialization requires the
tool to simply check the presence of an __init__ method,
and Sleepy Test simply looks for the sleep() function in
the body of the test case.
As shown in Table IV, P
YNOSE achieves a high level of
correctness with F1 scores ranging from 81.5% to 100% fordifferent test smells. For the cases where the tool did notachieve 100%, we investigated the mismatch.
In one instance, Assertion Roulette was not detected because
of a non-conventional name of the test case, where the namestarted with a _symbol instead of the word test *as is
the convention. A human rater could tag such a test case asTABLE IV
THE RESULTS OF THE EV ALUA TION .INST .STANDS FOR INSTANCES AND
INDICA TES A TRUE NUMBER OF TEST SUITES WITH A GIVEN SMELL IN
THE V ALIDA TION DA TASET .
Test Smell Inst. Precision Recall F1
Assertion Roulette 42 100% 97.6% 98.8%
Conditional Test Logic 20 80% 100% 88.9%Constructor Initialization 1 100% 100% 100%Default Test 2 100% 100% 100%
Duplicate Assertion 6 100% 100% 100%Empty Test 1 100% 100% 100%
Exception Handling 10 100% 100% 100%General Fixture 11 100% 100% 100%
Ignored Test 3 100% 100% 100%
Lack of Cohesion 13 78.6% 84.6% 81.5%Magic Number Test 23 100% 82.6% 90.5%Obscure Inline Setup 3 100% 100% 100%Redundant Assertion 2 100% 100% 100%Redundant Print 2 100% 100% 100%
Sleepy Test 1 100% 100% 100%
Suboptimal Assert 10 100% 100% 100%Test Maverick 5 100% 100% 100%
Unknown Test 10 83.3% 100% 90.1%
Weighted average â€” 94.0% 95.8% 94.9%
having the Assertion Roulette test smell, however, P YNOSE
failed to do so. P YNOSE also incorrectly identiï¬ed several
Conditional Test Logic test smells. Conditional Test Logic
is detected by the presence of control statements (i.e., if,
for, etc.) irrespective of their impact on the assertion. For
example, the for statement can be used simply to assign a
variable and such cases are incorrectly tagged as Conditional
Test Logic by P YNOSE .Lack of Cohesion relies on the
cohesiveness of test cases in a test suite. P YNOSE measures
cohesiveness using cosine similarity, whereas human ratersused their subjective judgement, which resulted in a mismatchbetween the output of P
YNOSE and the opinion of the human
raters in several cases. Several Magic Number Tests were
not detected because the comparison to a literal occurred in
600assertions with complex parameters that are not yet supported.
For example, assertEqual(df.shape, (1, )) was
tagged as a Magic Number Test test smell by a human rater,
however, P YNOSE failed to do so, because the literal is located
in a tuple. Finally, two cases of Unknown Test turned out to be
false positives. The tool considered the test case to not haveassertions, when in reality an assertion was present, but it wasfrom the unsupported pytest framework.
For all test smells together, P
YNOSE achieves the precision
of 94% and the recall of 95.8%. Table V shows the comparisonbetween the obtained values and the reported numbers of
TSDETECT [29], a similar tool for Java. It can be seen that
the values are similar, however, we plan to conduct a morethorough and direct comparison of tools in the future.
TABLE V
THE COMPARISON OF PERFORMANCE BETWEEN PYNOSE AND TS DETECT .
Detector Language Precision Recall F1
TSDETECT [29] Java 96.0% 97.1% 96.5%
PYNOSE Python 94.0% 95.8% 94.9%
V. P REV ALENCE OF TEST SMELLS
After developing and validating P YNOSE , we conducted an
empirical study on test smell prevalence in open-source Pythonprojects. In this section, we present the details and the resultsof this study.
A. Selecting projects to analyze
The goal of our study was to analyze test smell prevalence
in Python projects using P
YNOSE . This was done to increase
the subject diversity among the existing empirical studies on
test smells, as well as to gain an understanding of how testsmells are diffused in Python code. We decided to study thepresence of test smells in the same Primary dataset that was
used for mining code change patterns. We decided to do sobecause the Primary dataset represents mature open-source
Python projects that use testing within them.
However, to make sure that the results of the study are
robust and do not depend on the results from Section III-B3,we decided to also run the tool on an additional dataset.To gather it, we used the same procedure as described inSection III-B1, but with one condition being slightly relaxed:we gathered projects with the number of commits between 500and 1,000, instead of at least 1,000 commits. This resulted in239 additional projects; the full list is available online [36]. Wewill refer to this dataset as the Secondary dataset. While we
draw our general conclusions from the Primary dataset, since
it contains more projects with larger histories, the purpose oftheSecondary dataset is to make sure that the reported results
are unbiased.
B. Methodology
We ran P
YNOSE on all the projects in the Primary and
Secondary datasets separately. We dropped the results where
not a single test suite was found, and only considered testsuites with at least one test case and test ï¬les with at least
one test suite. Test smells can occur on various levels ofgranularity: Constructor Initialization, Default Test, General
Fixture, and Lack of Cohesion manifest at the level of a test
suite as a whole, while other test smells such as Conditional
Test Logic are formulated at the test case level.
We analyzed the test smells using their appropriate granular-
ity. A test suite is considered smelly if it contains at least onetest case with a given smell. A test ï¬le can also be considereda valid object for comparison, however, even though in Pythonand in Unittest it is possible to have several test suites in one
test ï¬le, this granularity is still largely similar to a test suite,and often a test ï¬le contains just one or two test suites. We alsocalculated the distribution of test smells among projects to geta more coarse-grained picture of the test smells prevalence.
We studied the most common and the least common test
smells, as well as the prevalence of the newly proposedSuboptimal Assert . Additionally, we studied the co-occurrence
of different test smells in individual test suites and discussedthe correlations between test smells.
C. Results
In this section, we discuss the results of the empirical study
of the test smells prevalence in Python code.
1) General information: In total, at least one Unittest
test case was found in 248 projects out of the 450 in the
Primary dataset (55.1%). From here on out, all percentages are
calculated based on these 248 projects. In total, in these 248projects, P
YNOSE detected 9,158 test ï¬les, 16,681 test suites,
and 96,736 test cases. More detailed statistics are presentedin Table VI. It can be seen from the table that even matureprojects vary greatly by the amount of testing within them. Inour dataset, one test ï¬le on average had 1.8 test suites, andone test suite on average had 5.8 test cases.
TABLE VI
THE SUMMARY OF THE AMOUNT OF TESTING ENTITIES PER PROJECT .
Test ï¬les Test suites Test cases
Minimum 11 1
Mean 36.9 67.3 390.1
Maximum 323 870 5,121
2) Test smells distribution: The distribution of 18 detected
test smells is presented in Figure 4. In general, it can be seenthat the studied test smells are prevalent in Python code. Thereare only 5 projects (2%) that have no smells, however, all ofthem are very small projects, with the largest having only 13test cases. All the other projects (98%) have tests smells inone way or another. Test smells such as Assertion Roulette
and Conditional Test Logic are among the most common test
smells and occur in almost 90% of projects that use Unittest in
thePrimary dataset. Also among the most popular test smells
areMagic Number Test, General Fixture, and Unknown Test.
On the other end of the spectrum, we can see test smells that
rarely occur in Python code. Empty Test occurs in just 0.7%
of the test suites, although, interestingly, even these instances
6010.30.71.42.93.05.910.511.415.417.422.524.126.931.952.4
0.89.717.726.223.837.929.446.064.974.268.570.678.281.575.479.889.589.9
Default TestConstructor InitializationEmpty TestRedundant AssertionSleepy TestRedundant PrintIgnored TestObscure Inline SetupException HandlingLack of CohesionTest MaverickSuboptimal AssertDuplicate AssertionUnknown TestGeneral FixtureMagic Number TestConditional Test LogicAssertion Roulette
0 1 02 03 04 05 06 0
% of test suites8.6
0.021.0
Default TestConstructor InitializationEmpty TestRedundant AssertionSleepy TestRedundant PrintIgnored TestObscure Inline SetupException HandlingLack of CohesionTest MaverickSuboptimal AssertDuplicate AssertionUnknown TestGeneral FixtureMagic Number TestConditional Test LogicAssertion Roulette
0 1 02 03 04 05 06 07 08 09 0 1 0 0
% of projects Primary dataset
 Secondary dataset Primary dataset
 Secondary dataset
Fig. 4. The prevalence of different test smells among all projects and test suites in Primary and Secondary datasets. The percentages relate to projects that
use Unittest, the numbers near bars are shown for the Primary dataset.
are spread out among as much as 17.7% of the projects.
Constructor Initialization occurs in 9.7% of the projects and
0.3% of the test suites. Finally, the rarest of all test smells thatoccurs in only two projects and only three test suites withinthem, is Default Test. Figure 4 also shows that our introduced
Suboptimal Assert smell constitutes an important addition to
Python test smells: it occurs at least once in 70.6% of theprojects and 15.4% of the test suites.
In comparison with previous works that study Java and An-
droid code [25], it can be said that the lists of the most populartest smells generally look similar. It seems that Python codehas larger percentages by projects, however, direct comparisonhere should be carried out in future work. One speciï¬c testsmell that seems to be less prevalent in Python is Exception
Handling that occurs in 64.9% of the projects and only in 8.6%
of the test suites. Unittest supports a convenient list of as-
sertions like assertRaises, assertRaisesRegex and
others that may prevent the users from using try/except
keywords in tests.
It can also be seen that the results for the Primary dataset
and the Secondary dataset are similar to each other, with the
only noticeable exception being the Obscure In-Line Setup,
which is rarer in the Secondary dataset. The values for
Suboptimal Assert are also similar. This demonstrates that our
results obtained for the Primary dataset are unbiased.
Overall, our results show that various test smells are preva-
lent in Python code, even some of the rarer ones still occur inmore than a quarter of all projects. While some of them can beconsidered more subjective, others make it signiï¬cantly harderto maintain the code base and to interpret the results of testingin case of failure. We hope that in the future P
YNOSE can be
used to help developers and researchers to combat the spreadof test smells in their repositories.
3) Co-occurrence of test smells: In the previous section, we
discussed how prevalent different test smells are. However,such an approach considers test smells independently from0123456789 1 0 1 1 1 20510152025% of test suites
Number of different t est smells in a test suite
Fig. 5. The distribution of the number of different test smells among
individual test suites.
each other and does not fully describe the actual â€œsmellinessâ€
of code. To get a better understanding, we also analyzed theco-occurrence of test smells.
Figure 5 shows the distribution of how many different smells
co-exist within individual test suites. It can be seen that only16% of all test suites are free from smells. The remaining 84%of the test suites have at least one smell: 23.1% have exactlyone smell, 20.6% have two smells, 16.3% have three smells,and this number gradually decreases with the amount of co-occurring test smells. The highest occurring number in thePrimary dataset appears in a single test suite with 12 distinct
test smells. This large test suite with 25 test cases, in additionto all the most popular test smells, contains commented outempty test cases, catching errors with try/except instead
of using speciï¬c assertions of error messages, and sleepytests. It also uses assertEqual(X, True) instead of
assertTrue(X). We believe that helping developers ï¬ndsuch suites might be useful for the maintenance of the project.
Figure 5 also sheds a new light on the prevalence of test
smells in Python code. With more than half of all test suites
602having two different test smells or more, their effect on the
maintainability of code can become more complex.
We also additionally studied the co-occurrence of speciï¬c
pairs of test smells. For all pairs of test smells, we calculatedthe following value: what percentage of test suites that havetest smell X also have test smell Y . Two pairs of test smells arecompletely connected. Firstly, if the test is Empty (i.e., contains
no executable statements), it is automatically Unknown (i.e.,
has no direct assertions). Secondly, if there is a Test Maverick
in a test suite, this test suite automatically has a General Fix-
ture. Test Maverick occurs when the test suite has a setUp()
method with ï¬elds and the given test case does not use anyof the ï¬elds in it. Of course, this automatically means thatthere is at least one method that does not use all of the ï¬elds,which is the deï¬nition of a General Fixture. Other strongly
connected pairs are all associated with Assertion Roulette due
to its popularity. If a test suite has a Duplicate Assert, it has
anAssertion Roulette in 93.1% of the cases. It might not
be the case if the duplication has explicit messages (becauseAssertion Roulette is only considered if assertions have no
messages), but since it is very common to not write errormessages, duplicated assertions can become a roulette. Thesame goes for Redundant Assertion, 83.5% of the test suites
with which also have an Assertion Roulette. This also makes
sense, because if there is a redundant assertion, there probablyshould be some other assertion that is more meaningful.
This co-occurrence of test smells demonstrates that test
smells have relationship with one another that should beexplored in greater detail in the future.
VI. T
HREA TS TO VALIDITY
While we structured our study to avoid introducing bias and
worked to eliminate the effects of random noise, it is possiblethat our mitigation strategies may not have been effective. Thissection reviews the threats to validity to our study.
It is possible that during the systematic mapping study of
test smells we missed some test smells that are applicableto Python. Also, Python grammar is rather large, and isbeing actively updated, so P
YTHON CHANGE MINER does not
support all Python language constructs, and it is possible thatwe may have missed potential test smell changes becauseof this. We also relied on pattern detection thresholds fromthe original paper by Nguyen et al. [50], while it is possiblethat they could be different for Python and for testing code.However, the tool supports all the main features of Pythonand still produced a large number of code change patterns. Inaddition, P
YNOSE is built in such a way that it is simple to
add new test smells in the future.
The results of both parts of our studyâ€”searching for
Python-speciï¬c test smells and analyzing the prevalence oftest smells in Python codeâ€”rely on a speciï¬c set of open-source projects that we selected and might not generalize toall projects, including proprietary ones. However, we analyzedtwo moderately large datasets (Primary and Secondary) for our
tasks that were curated using various conditions suggested inthe literature. We believe that the similarity of results fromboth datasets demonstrates the reproducibility of the resultsof the empirical study.
It is possible for P
YNOSE to have some unnoticed errors in
its implementation. However, we tested the tool rigorously onsynthetic data and performed manual evaluation on real-worlddata to minimize the risk as much as possible.
One threat to validity is related to the detection of speciï¬c
test smells. Some of the implementations of test smells relyon speciï¬c thresholds that were picked from the literature. Itis possible that these thresholds are different for Python, andthis requires further study.
VII. C
ONCLUSIONS AND FUTURE WORK
Test smells are prevalent in commonly used programming
languages such as Java and have a detrimental effect not onlyon the quality of test code but also on the production code [11].
In this work, we presented P
YNOSE , the ï¬rst tool for test
smell detection in Python code that is capable of identifying18 test smells. 17 out of these 18 test smells were adaptedfrom test smells for other programming languages describedin the literature, and we added one test smell called Suboptimal
Assert by analyzing the most frequent changes made to test
ï¬les in 450 open-source Python projects. Experiments ona set of eight real-world projects showed that P
YNOSE is
capable of detecting test smells with 94% precision and 95.8%recall, which is on par with other publicly available tools fortest smell detection. Our empirical analysis shows that testsmells are prevalent in Python code, with 98% of the projectsand 84% of the test suites having at least one test smell inthem. The most frequent detected test smells were Assertion
Roulette, Conditional Test Logic, and Magic Number Test.W e
also observed that the proposed Python-speciï¬c Suboptimal
Assert smell occurs in the code rather often, being present in
as much as 70.6% of the projects.
Future research directions for this work include:
â€¢Supporting more test smells, including those that rely onproduction code.
â€¢Discovering more Python-speciï¬c smells, which requiresa speciï¬c analysis of the optimal pattern searchingparameters for Python.
â€¢Conducting a more thorough comparison of P YNOSE
to other tools, for example, to TSDETECT that works
with Java. It would also be of interest to employ thetools together to carry out a comparison of large Pythonand Java datasets from the standpoint of test smelldistribution.
â€¢Analyzing test smell prevalence in Python on a largerdataset of projects and in other dimensions, for example,it would be of great interest to see how test smellscorrelate with test coverage [26].
P
YNOSE is available on GitHub for use in the IDE and
for research: https://github.com/JetBrains-Research/PyNose,all the research artifacts of this study are also publicly avail-able: https://zenodo.org/record/5156098.
603REFERENCES
[1] M. Fowler and K. Beck, Refactoring: improving the design of existing
code. Addison-Wesley Professional, 1999.
[2] I. Deligiannis, M. Shepperd, M. Roumeliotis, and I. Stamelos, â€œAn
empirical investigation of an object-oriented design heuristic for main-
tainability,â€ Journal of Systems and Software, vol. 65, no. 2, pp. 127â€“139,
2003.
[3] W. Li and R. Shatnawi, â€œAn empirical study of the bad smells and class
error probability in the post-release object-oriented system evolution,â€Journal of systems and software, vol. 80, no. 7, pp. 1120â€“1128, 2007.
[4] G. A. Oliva, I. Steinmacher, I. Wiese, and M. A. Gerosa, â€œWhat can
commit metadata tell us about design degradation?â€ in Proceedings of
the 2013 International Workshop on Principles of Software Evolution.ACM, 2013, pp. 18â€“27.
[5] S. Olbrich, D. S. Cruzes, V . Basili, and N. Zazworka, â€œThe evolution
and impact of code smells: A case study of two open source systems,â€inProceedings of the 2009 3rd international symposium on empirical
software engineering and measurement. IEEE Computer Society, 2009,pp. 390â€“400.
[6] T. Hall, M. Zhang, D. Bowes, and Y . Sun, â€œSome code smells have a
signiï¬cant but small effect on faults,â€ ACM Transactions on Software
Engineering and Methodology (TOSEM), vol. 23, no. 4, p. 33, 2014.
[7] N. Zazworka, M. A. Shaw, F. Shull, and C. Seaman, â€œInvestigating the
impact of design debt on software quality,â€ in Proceedings of the 2nd
Workshop on Managing Technical Debt. ACM, 2011, pp. 17â€“23.
[8] M. Tufano, F. Palomba, G. Bavota, R. Oliveto, M. Di Penta, A. De Lucia,
and D. Poshyvanyk, â€œWhen and why your code starts to smell bad(and whether the smells go away),â€ IEEE Transactions on Software
Engineering, vol. 43, no. 11, pp. 1063â€“1088, 2017.
[9] A. V an Deursen, L. Moonen, A. V an Den Bergh, and G. Kok, â€œRefac-
toring test code,â€ in Proceedings of the 2nd international conference
on extreme programming and ï¬‚exible processes in software engineering(XP2001). Citeseer, 2001, pp. 92â€“95.
[10] G. Bavota, A. Qusef, R. Oliveto, A. De Lucia, and D. Binkley, â€œAn
empirical analysis of the distribution of unit test smells and their impacton software maintenance,â€ in 2012 28th IEEE International Conference
on Software Maintenance (ICSM). IEEE, 2012, pp. 56â€“65.
[11] D. Spadini, F. Palomba, A. Zaidman, M. Bruntink, and A. Bacchelli,
â€œOn the relation of test smells to software code quality,â€ in 2018
IEEE International Conference on Software Maintenance and Evolution(ICSME). IEEE, 2018, pp. 1â€“12.
[12] G. Bavota, A. Qusef, R. Oliveto, A. De Lucia, and D. Binkley, â€œAre
test smells really harmful? an empirical study,â€ Empirical Software
Engineering, vol. 20, no. 4, pp. 1052â€“1094, 2015.
[13] M. Tufano, F. Palomba, G. Bavota, M. Di Penta, R. Oliveto, A. De Lucia,
and D. Poshyvanyk, â€œAn empirical investigation into the nature of testsmells,â€ in Proceedings of the 31st IEEE/ACM International Conference
on Automated Software Engineering, 2016, pp. 4â€“15.
[14] M. Greiler, A. V an Deursen, and M.-A. Storey, â€œAutomated detection
of test ï¬xture strategies and smells,â€ in 2013 IEEE Sixth International
Conference on Software Testing, V eriï¬cation and V alidation. IEEE,2013, pp. 322â€“331.
[15] J. De Bleser, D. Di Nucci, and C. De Roover, â€œAssessing diffusion
and perception of test smells in scala projects,â€ in 2019 IEEE/ACM
16th International Conference on Mining Software Repositories (MSR) .
IEEE, 2019, pp. 457â€“467.
[16] S. Raschka, J. Patterson, and C. Nolet, â€œMachine learning in python:
Main developments and technology trends in data science, machinelearning, and artiï¬cial intelligence,â€ Information, vol. 11, no. 4, p. 193,
2020.
[17] D. Sarkar, R. Bali, and T. Sharma, â€œPractical machine learning with
python,â€ A Problem-Solvers Guide To Building Real-World Intelligent
Systems. Berkely: Apress, 2018.
[18] Y . Golubev, J. Li, V . Bushev, T. Bryksin, and I. Ahmed, â€œChanges
from the trenches: Should we automate them?â€ arXiv preprint
arXiv:2105.10157, 2021.
[19] PyCharm. (accessed: 01.08.2021) The python ide for professional
developers. [Online]. Available: https://www.jetbrains.com/pycharm/
[20] K. Reitz and T. Schlusser, The Hitchhikerâ€™s guide to Python: best
practices for development. Oâ€™Reilly Media, Inc., 2016.
[21] G. Meszaros, xUnit test patterns: Refactoring test code . Pearson
Education, 2007.[22] B. V an Rompaey, B. Du Bois, and S. Demeyer, â€œCharacterizing the
relative signiï¬cance of a test smell,â€ in 2006 22nd IEEE International
Conference on Software Maintenance. IEEE, 2006, pp. 391â€“400.
[23] M. Breugelmans and B. V an Rompaey, â€œTestq: Exploring structural
and maintenance characteristics of unit test suites,â€ in WASDeTT-1: 1st
International Workshop on Advanced Software Development Tools andTechniques. Citeseer, 2008.
[24] J. De Bleser, D. Di Nucci, and C. De Roover, â€œSocrates: Scala radar for
test smells,â€ in Proceedings of the Tenth ACM SIGPLAN Symposium on
Scala, 2019, pp. 22â€“26.
[25] A. Peruma, K. Almalki, C. D. Newman, M. W. Mkaouer, A. Ouni,
and F. Palomba, â€œOn the distribution of test smells in open sourceandroid applications: An exploratory study,â€ in Proceedings of the 29th
Annual International Conference on Computer Science and SoftwareEngineering, 2019, pp. 193â€“202.
[26] T. Virg Â´Ä±nio, R. Santana, L. A. Martins, L. R. Soares, H. Costa, and
I. Machado, â€œOn the inï¬‚uence of test smells on test coverage,â€ in Pro-
ceedings of the XXXIII Brazilian Symposium on Software Engineering ,
2019, pp. 467â€“471.
[27] B. V an Rompaey, B. Du Bois, S. Demeyer, and M. Rieger, â€œOn the
detection of test smells: A metrics-based approach for general ï¬xtureand eager test,â€ IEEE Transactions on Software Engineering, vol. 33,
no. 12, pp. 800â€“817, 2007.
[28] F. Palomba, A. Zaidman, and A. De Lucia, â€œAutomatic test smell detec-
tion using information retrieval techniques,â€ in 2018 IEEE International
Conference on Software Maintenance and Evolution (ICSME). IEEE,2018, pp. 311â€“322.
[29] A. Peruma, K. Almalki, C. D. Newman, M. W. Mkaouer, A. Ouni,
and F. Palomba, â€œtsdetect: an open source test smells detection tool,â€inProceedings of the 28th ACM Joint Meeting on European Software
Engineering Conference and Symposium on the Foundations of SoftwareEngineering, 2020, pp. 1650â€“1654.
[30] S. Lambiase, A. Cupito, F. Pecorelli, A. De Lucia, and F. Palomba,
â€œJust-in-time test smell detection and refactoring: The darts project,â€ inProceedings of the 28th International Conference on Program Compre-hension, 2020, pp. 441â€“445.
[31] R. Santana, L. Martins, L. Rocha, T. Virg Â´Ä±nio, A. Cruz, H. Costa, and
I. Machado, â€œRaide: a tool for assertion roulette and duplicate assertidentiï¬cation and refactoring,â€ in Proceedings of the 34th Brazilian
Symposium on Software Engineering, 2020, pp. 374â€“379.
[32] T. Virg Â´Ä±nio, L. Martins, L. Rocha, R. Santana, A. Cruz, H. Costa, and
I. Machado, â€œJnose: Java test smell detector,â€ in Proceedings of the 34th
Brazilian Symposium on Software Engineering, 2020, pp. 564â€“569.
[33] B. A. Kitchenham, D. Budgen, and P . Brereton, Evidence-based software
engineering and systematic reviews. CRC press, 2015, vol. 4.
[34] C. Wohlin, â€œGuidelines for snowballing in systematic literature studies
and a replication in software engineering,â€ in Proceedings of the 18th
international conference on evaluation and assessment in softwareengineering, 2014, pp. 1â€“10.
[35] S. Keele et al., â€œGuidelines for performing systematic literature reviews
in software engineering,â€ Technical report, V er. 2.3 EBSE TechnicalReport. EBSE, Tech. Rep., 2007.
[36] PyNose. (accessed: 01.08.2021) Supplementary materials for this paper.
[Online]. Available: https://zenodo.org/record/5156098
[37] P . Documentation. (accessed: 01.08.2021) File and directory access.
[Online]. Available: https://docs.python.org/3/library/ï¬lesys.html
[38] A. Kicsi, L. T Â´oth, and L. Vid Â´acs, â€œExploring the beneï¬ts of utilizing
conceptual information in test-to-code traceability,â€ in 2018 IEEE/ACM
6th International Workshop on Realizing Artiï¬cial Intelligence Synergiesin Software Engineering (RAISE) . IEEE, 2018, pp. 8â€“14.
[39] A. Kicsi, L. Vid Â´acs, and T. Gyim Â´othy, â€œTestroutes: A manually curated
method level dataset for test-to-code traceability,â€ in Proceedings of the
17th International Conference on Mining Software Repositories, 2020,pp. 593â€“597.
[40] M. Ghafari, C. Ghezzi, and K. Rubinov, â€œAutomatically identifying focal
methods under test in unit test cases,â€ in 2015 IEEE 15th Interna-
tional Working Conference on Source Code Analysis and Manipulation(SCAM). IEEE, 2015, pp. 61â€“70.
[41] Unittest. (accessed: 01.08.2021) Unit testing framework for python.
[Online]. Available: https://docs.python.org/3/library/unittest.html
[42] pytest. (accessed: 01.08.2021) Testing framework for python. [Online].
Available: https://docs.pytest.org/en/stable/
[43] R. Framework. (accessed: 01.08.2021) Testing framework for python.
[Online]. Available: https://robotframework.org/
604[44] G. L. Turnquist, Python Testing Cookbook. Packt Publishing Ltd, 2011.
[45] G. Gousios, â€œThe ghtorrent dataset and tool suite,â€ in Proceedings of
the 10th Working Conference on Mining Software Repositories, ser.
MSR â€™13. Piscataway, NJ, USA: IEEE Press, 2013, pp. 233â€“236.[Online]. Available: http://dl.acm.org/citation.cfm?id=2487085.2487132
[46] G. dumps. (accessed: 01.08.2021) Ghtorrent archive sql dumps.
[Online]. Available: http://ghtorrent-downloads.ewi.tudelft.nl/mysql/
[47] P . Create. (accessed: 01.08.2021) A tool to create the pga
dataset. [Online]. Available: https://github.com/src-d/datasets/tree/master/PublicGitArchive/pga-create
[48] V . Markovtsev and W. Long, â€œPublic git archive: A big code dataset
for all,â€ in Proceedings of the 15th International Conference on Mining
Software Repositories, 2018, pp. 34â€“37.
[49] E. Kalliamvakou, G. Gousios, K. Blincoe, L. Singer, D. M. German, and
D. Damian, â€œThe promises and perils of mining github,â€ pp. 92â€“101,2014.
[50] H. A. Nguyen, T. N. Nguyen, D. Dig, S. Nguyen, H. Tran, and M. Hilton,
â€œGraph-based mining of in-the-wild, ï¬ne-grained, semantic code changepatterns,â€ in 2019 IEEE/ACM 41st International Conference on Software
Engineering (ICSE), 2019, pp. 819â€“830.
[51] J. Falleri, F. Morandat, X. Blanc, M. Martinez, and M. Monperrus,
â€œFine-grained and accurate source code differencing,â€ in ACM/IEEE
International Conference on Automated Software Engineering, ASE â€™14,V asteras, Sweden - September 15 - 19, 2014, 2014, pp. 313â€“324.
[52] C. in Obspy. (accessed: 01.08.2021) A diff in the obspy
project. [Online]. Available: https://github.com/obspy/obspy/commit/7719290b8dd6940dd195a195120c075a4f94cf42
[53] C. in Numba. (accessed: 01.08.2021) A diff in the numba
project. [Online]. Available: https://github.com/numba/numba/commit/edbb26caad50cd0cc6352e6fe5b84fbd6edaaf9b
[54] C. in Reviewboard. (accessed: 01.08.2021) A commit message in the re-
viewboard project. [Online]. Available: https://github.com/reviewboard/reviewboard/commit/1758bf53057ee7b648ace1c557031d9460c88c00
[55] C. in Girder. (accessed: 01.08.2021) A commit message in the girder
project. [Online]. Available: https://github.com/girder/girder/commit/f4b6040618dbe9a7edca99d8f6344a316b5b1f10
[56] C. in Beets. (accessed: 01.08.2021) A commit message in the beets
project. [Online]. Available: https://github.com/beetbox/beets/commit/2b921b19fd5a23bc2e86060c2c12137d63dfe1db
[57] C. in Python-Chess. (accessed: 01.08.2021) A commit message in the
python-chess project. [Online]. Available: https://github.com/niklasf/python-chess/commit/944a0e682174ff32a6f9689176aa9016bab44a31
[58] C. in Gensim. (accessed: 01.08.2021) A commit message in the gensim
project. [Online]. Available: https://github.com/RaRe-Technologies/gensim/commit/342f10a2472fb22d811a398f5a6d49d1b6a88ab0
[59] C. in Requests. (accessed: 01.08.2021) A commit message in the
requests project. [Online]. Available: https://github.com/psf/requests/commit/a8555d811df0a4aaf2dd1f083ba0bc71679101ca
[60] P . S. Interface. (accessed: 01.08.2021) Program structure interface in
intellij platform. [Online]. Available: https://plugins.jetbrains.com/docs/intellij/psi.html
605