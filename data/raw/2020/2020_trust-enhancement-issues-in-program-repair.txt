Trust Enhancement Issues in Program Repair
Yannic Nollerâˆ—
National University of Singapore
Singapore
yannic.noller@acm.orgRidwan Shariffdeenâˆ—
National University of Singapore
Singapore
ridwan@comp.nus.edu.sg
Xiang Gaoâ€ 
National University of Singapore
Singapore
gaoxiang@comp.nus.edu.sgAbhik Roychoudhury
National University of Singapore
Singapore
abhik@comp.nus.edu.sg
ABSTRACT
Automatedprogramrepairisanemergingtechnologythatseeks
to automatically rectify bugs and vulnerabilities using learning,
search, and semantic analysis. Trust in automatically generatedpatches is necessary for achieving greater adoption of program
repair.Towardsthisgoal,wesurveymorethan100softwarepracti-
tionerstounderstandtheartifactsandsetupsneededtoenhance
trust in automatically generated patches. Based on the feedback
from the survey on developer preferences, we quantitatively evalu-
ateexistingtest-suitebasedprogramrepairtools.Wefindthatthey
cannotproducehigh-qualitypatcheswithinatop-10rankingand
anacceptabletimeperiodof1hour.Thedeveloperfeedbackfrom
ourqualitativestudyandtheobservationsfromourquantitative
examination of existing repair tools point to actionable insights to
drive program repair research. Specifically, we note that producing
repairswithinanacceptabletime-boundisverymuchdependentonleveraginganabstractsearchspacerepresentationofarichenoughsearch space. Moreover, while additional developer inputs are valu-
able for generating or ranking patches, developers do not seem tobe interested in a significant human-in-the-loop interaction.
ACM Reference Format:
YannicNoller,RidwanShariffdeen,XiangGao,andAbhikRoychoudhury.
2022.TrustEnhancementIssuesinProgramRepair.In 44thInternationalCon-
ferenceonSoftwareEngineering(ICSEâ€™22),May21â€“29,2022,Pittsburgh,PA,
USA.ACM,NewYork,NY,USA,13pages.https://doi.org/10.1145/3510003.
3510040
1 INTRODUCTION
Automated program repair technologies [ 14] are getting increased
attention.Inrecenttimes,programrepairhasfounditswayinto
the automatedfixing of mobile apps in the SapFixproject in Face-
book[28],automatedrepairbotsasevidencedbytheRepairnator
âˆ—Joint first authors
â€ Alternate email: gaoxiang9430@gmail.com
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation
onthe firstpage.Copyrights forcomponentsof thisworkowned byothersthan the
author(s)mustbehonored.Abstractingwithcreditispermitted.Tocopyotherwise,or
republish,topostonserversortoredistributetolists,requirespriorspecificpermission
and/or a fee. Request permissions from permissions@acm.org.
ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
Â© 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 978-1-4503-9221-1/22/05...$15.00
https://doi.org/10.1145/3510003.3510040project [44], and has found certain acceptability in companies such
as Bloomberg [ 17]. While all of these are promising, large-scale
adoption of program repair where it is well integrated into our
programming environments is considerably out of reach as of now.
In this article, we reflect on the impediments towards the usage
of program repair by developers. There can be many challenges
towards the adoption of program repair like scalability, applicabil-
ity,anddeveloperacceptability.Alotoftheresearchonprogram
repairhasfocusedonscalabilitytolargeprogramsandalsotolarge
search spaces [ 12,26,28,31]. Similarly, there have been various
worksongeneratingmulti-linefixes[ 13,31],orontransplanting
patches from one version to another [ 41] â€” to cover various use
cases or scenarios of program repair.
Surprisingly,thereisverylittleliteratureorsystematicstudies
fromeitheracademiaorindustryonthedevelopertrustinprogramrepair.Inparticular,whatchangesdoweneedtobringintothepro-gramrepairprocesssothatitbecomesviabletohaveconversationsonitswide-scaleadoption?Partofthegulfintermsoflackoftrust
comes froma lack ofspecifications â€”since the intendedbehavior
of the program is not formally documented, it is hard to trust that
theautomaticallygeneratedpatchesmeetthisintendedbehavior.
Overall, we seek to examine whether the developerâ€™s reluctance
touseprogramrepairmaypartiallystemfromnotrelyingonau-
tomaticallygeneratedcode.ThiscanhaveprofoundimplicationsbecauseofrecentdevelopmentsonAI-basedpairprogramming
1,
whichholdsoutpromiseforsignificantpartsofcodinginthefuture
to be accomplished via automated code generation.
Inthisarticle,wespecificallystudytheissuesinvolvedinenhanc-
ingdevelopertrustonautomaticallygeneratedpatches.Towards
this goal, we first settle on the research questions related to de-
velopertrustinautomaticallygeneratedpatches.Thesequestions
are divided into two categories (a) expectations of developers from
automatic repair technologies,and (b) understanding the possible
shortfall of existing program repair technologies with respect to
developer expectations. To understand the developer expectations
from program repair, we outline the following research questions.
RQ1To what extent are the developers interested to apply auto-
mated program repair (henceforth called APR), and how do
they envision using it?
RQ2Can software developers provide additional inputs that
would cause higher trust in generated patches? If yes, what
kind of inputs can they provide?
1Github Copilot https://copilot.github.com/
22282022 IEEE/ACM 44th International Conference on Software Engineering (ICSE)
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:59:42 UTC from IEEE Xplore.  Restrictions apply. ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA Yannic Noller, Ridwan Shariffdeen, Xiang Gao, and Abhik Roychoudhury
RQ3What evidence from APR will increase developer trust in
the patches produced?
For a comprehensive assessment of the research questions, we en-
gageinbothqualitativeandquantitativestudies.Ourassessment
of the questions primarily comes in three parts. To understand the
developer expectations from program repair, we conduct a detailed
survey (with 35 questions) among more than 100 professional soft-
ware practitioners. Most of our survey respondents are developers,
witha fewcoming frommore seniorroles suchas architects.The
surveyresults amounttoboth quantitativeandqualitative inputs
on the developer expectations since we curate and analyze respon-
dentsâ€™ comments on topics such as the expected evidence for patch
correctnessprovidedbyautomatedrepairtechniques.Basedonthe
surveyfindings,wenotethatdevelopersarelargelyopen-minded
in terms of trying out a small number of patches (no more than 10)
fromautomatedrepairtechniques,aslongasthesepatchesarepro-
duced within a reasonable time, say less than 1 hour. Furthermore,
the developers are open to receiving specifications from the pro-
gram repair method (amounting to evidence of patch correctness).
Theyarealsoopen-mindedintermsofprovidingadditionalspecifi-
cationsto driveprogramrepair.Themostcommon specifications
the developers are ready to give and receive are tests.
Basedonthecommentsreceivedfromsurveyparticipants,we
then conduct a quantitative comparison of certain well-known pro-
gram repair tools on the widely used ManyBugs benchmarks [ 20].
Tounderstandthepossibledeficiencyofexistingprogramrepair
techniqueswithrespecttooutlineddeveloperexpectationsasfound
from the survey, we formulate the following research questions.
RQ4Can existing APR techniques pinpoint high-quality patches
in the top-ranking (e.g., among top-10) patches within a
tolerable time limit (e.g., 0.5/1/2 hours)?
RQ5What is the impact of additional inputs (say, fix locations
and additional passing test cases) on the efficacy of APR?
Wenotethatmanyoftheexistingpapersonprogramrepairuselib-
eral timeout periods to generate repairs, while in our experiments
thetimeoutisstrictlymaintainedatnomorethanonehour.Weare
alsorestrictedtoobservingthefirstfewpatches,andweexamine
theimpactofthefixlocalizationbyeitherprovidingandnotpro-
viding the developer location. Based on a quantitative comparison
of well-known repair tools Angelix [ 31], CPR [40], GenProg [ 21],
Prophet[ 26]andFix2Fit[ 12]â€”weconcludethatthesearchspace
representationhasasignificantroleinderivingplausible/correct
patches within an acceptable time period. In other words, an ab-
stract representation of the search space (aided by constraints that
are managed efficiently or aided by program equivalence relations)
is at least as critical as a smart search algorithm to navigate the
patch space. We discuss how the tools can be improved to meet de-
veloperexpectations,eitherbyachievingcompilation-freerepairor
bynavigating/suggestingabstractpatcheswiththehelpofsimple
constraints (such as interval constraints).
Lastbutnottheleast,wenotethatprogramrepaircanbeseen
as automated code generation at a micro-scale. By studying thetrustissuesin automated repair,wecan alsoobtainaninitialun-
derstanding of trust enhancement in automatically generated code.2 SPECIFICATIONS IN PROGRAM REPAIR
The goal of APR is to correct buggy programs to satisfy given
specifications.Inthissection,wereviewthesespecificationsand
discuss how they can impact patch quality.
Test Suites as Specification. APR techniques such as GenProg [ 21]
andProphet[ 26]treattestsuitesascorrectnessspecifications.The
test suite usually includes a set of passing tests and at least one
failing test. The repair goal is to correct the buggy program to pass
allthegiventestsuites.Althoughtestsuitesarewidelyavailable,
theyareusuallyincompletespecificationsthatspecifypartofthe
intendedprogrambehaviors.Hence,theautomaticallygenerated
patchmayoverfitthetests,meaningthatthepatchedprogrammaystillfailonprograminputsoutsidethegiventests.Forinstance,the
following is a buggy implementation that copies ncharacters from
sourcearray srctodestinationarray dest,andreturnsthenumber
of copied characters. A buffer overflow happens at line 6 when the
sizeof srcordestislessthan n.Bytakingthefollowingthreetests
(oneofthemcantriggerthisbug)asspecification,aproducedpatch
(++index <nâ†¦â†’+ + index <n&&index <3)canmaketheprogram
pass the given tests. Obviously, the patched program is still buggy
on test inputs outside the given tests.
1 int lenStrncpy( char [] src, char [] dest, int n){
2 if(src == NULL || dest == NULL)
3 return 0;
4 int index = -1;
5 while (++index < n)
6 dest[index] = src[index]; // buffer overflow
7 return index;
8 }
Type src dest n Output Expected Output
Passing SOF COM 3 3 3
Passing DHT APP0 3 3 3
Failing APP0 DQT 4 *crash 3
Constraints as Specification. Instead of relying on tests, another
lineofAPRresearch,e.g.,ExtractFix[ 13]andCPR[ 40],takecon-
straintsascorrectnessspecifications.Constraintshavethepotentialtorepresentarangeofinputsoreventhewholeinputspace.Driven
byconstraints,thegoalofAPRistopatchtheprogramtosatisfy
theconstraints. However,unlike thetestsuite, theconstraintsare
notalwaysavailableinpractice;forthisreason,techniqueslikeAn-
gelix[31]andSemFix[ 34]taketestsasspecificationsbutextract
constraints from tests. Certain existing APR techniques take as in-
putcoarse-grainedconstraints,suchasassertionsorcrash-freecon-
straints.Forinstance,ExtractFixreliesonpredefinedtemplates
toinfer constraintsthat cancompletely fixvulnerabilities.For the
aboveexample,accordingtothetemplateforbufferoverflow,thein-
ferred constraint is index <sizeof(src)&&index <sizeof(dest).
Once the patched program satisfies this constraint, it is guaranteed
that the buffer overflow is completely fixed. Guarantees from such
fixing of overflows/crashes do not amount to a guarantee of the
full functional correctness of the fixed program.
Code Patterns as Specification. Besides test suites andconstraints,
code patterns can also serve as specifications for repair systems.
Specifically,givenabuggyprogramthatviolatesacodepattern,the
2229
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:59:42 UTC from IEEE Xplore.  Restrictions apply. Trust Enhancement Issues in Program Repair ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
repair goal is to correct the program to satisfy the rules defined by
thecodepattern.Thecodepatternscanbemanuallydefined[ 42],
from static analyzers [ 45], automatically mined from large code
repositories [ 3,4], etc. Similar to the inferred constraints, code
patterns cannot ensure functionality correctness.
3 SURVEY METHODOLOGY
Since constructing formal program specifications is notoriously
difficult, the specifications used by APR tools cannot ensure patch
correctness. Unreliable overfitting patches cause developers to lose
trust in APR tools. This motivates us to enquire/survey developers
on how APR can be enhanced to gain their trust.
Wedesignedandconductedasurveywithsoftwarepractitioners,
specifically to answer the first three research questions (RQ1-3).
In June 2021, we distributed a questionnaire to understand howdevelopers envision the usage of automated program repair and
whatcanbeprovidedtoincreasetrustinautomaticallygenerated
patches. Note that we followed our institutional guidelines and
received approval from the Institutional Review Board (IRB) of our
organization prior to administering the survey.
Survey Instrument. We asked in total 35 questions about how trust-
worthyAPRcanbedeployedinpractice.Ourquestionsarestruc-
tured into six categories:
C1UsageofAPR (RQ1):whetherandhowdeveloperswoulden-
gage with APR.
C2Availabilityofinputs/specifications (RQ2):whatkindofinput
artifacts developers can provide for APR techniques.
C3Impact on trust (RQ2): how additional input artifacts would
impact the trust in auto- generated patches.
C4Explanations (RQ3): what kind of evidence/explanation devel-
opers expect for auto-generated patches.
C5Usage of APR side-products (RQ3): what side-products of APR
are useful for the developers, e.g., for manual bug-fixing.
C6Background : the role and experience of the participants in the
software development process.
C1willprovideinsightsforRQ1,C2andC3forRQ2,andC4andC5
for RQ3. The questions are a combination of open-ended questions
like"How would you like to engage with an APR tool?" and close-
ended questions like "Would it increase your trust in auto-generated
patches if additional artifacts such as tests/assertions are used during
patching?" withMultipleChoiceora5-pointLikertscale.Theques-
tionnaire itself was created and deployed with Microsoft Forms. A
completelistofourquestionscanbefoundinTable1andinour
replication package [35].
Participants. We distributed the survey via two channels: (1) Ama-
zon MTurk, and (2) personalized email invitations to contacts from
global-wide companies. As incentives, we offered each participant
on MTurk 10 USD as compensation, while for each other partici-
pant, we donated 2 USD to a COVID-19 charity fund. We received
134 responses from MTurk. To filter low-quality and non-genuine
responses,wefollowedtheknownprinciples[ 10]andusedquality-
controlquestions.Inparticular,wemanuallyinspectedallresponses
and filteredout answersthat areirrelevant tothe actualquestion:
(1)wecheckedforsuspiciousanswers,whichoverloadkeywords,
e.g.,manyresponsesincludedamessageonAnnualPercentageRate
       	 
  !#!!!#"
		


Figure1:ResponsesforQ6.1 Whatisyour(main)roleinthe
software development process?

      	



		


Figure 2: Responses for Q6.2 How long have you worked in
software development?
(APR)insteadofautomatedprogramrepair,andthen(2)wechecked
the consistency of the responses with quality-control questions,
e.g., "Please describe briefly your role in software development"
and"Nameyourprimary activityinsoftwaredevelopment"atthe
beginning of the survey. After this manual post-processing, we
ended up with 34 validresponses from MTurk. From our company
contacts, we received 81 responses, from which all have been gen-
uineanswers.Fromthetotalof115validresponses,weselected 103
relevantresponses, which excluded responses from participants
whoclassifiedthemselvesasProjectManager,ProductOwner,Data
Scientist,orResearcher.Ourgoalwastoincludeanswersfromsoft-
warepractitionersthathavedaily,hands-onexperienceinsoftware
development.Figure1and2showtherolesandexperiencesforthe
final subset of the 103 participants.
Analysis. Forthequestionswitha5-pointLikertscale,weanalyzed
thedistributionofnegative(1and2),neutral(3),andpositive(4and5)responses.FortheMultipleChoicequestions,weanalyzedwhich
choices were selected most, while the open-ended "Other" choices
were analyzed and mapped to the existing choices or treated as
newonesifnecessary.Forallopen-endedquestions,weperformedaqualitativecontentanalysiscoding[
39]tosummarizethethemes
andopinions.Thefirstiterationoftheanalysisandcodingwasdone
by oneauthor, followedby thereview ofthe other authors.In the
following sections, we will discuss the most mentioned responses,
and indicate in the brackets behind the responses how often the
topicsarementionedamongthe103participants.Weusethechi-
square goodness of fit test [ 37](ð›¼=0.01) to check that our results
are significant and not a random observation. We also show thesignificance of the obtained trends/majorities with the Binomial
Test[1](ð›¼=0.05).Wepresentthecorresponding ð‘ƒvalues.Alldata,
statistics, and codes are included in our replication package [35].
4 SURVEY RESULTS
4.1 Developer engagement with APR (RQ1)
In this section, we discuss the responses for the questions in cat-
egoryC1andquestionQ2.8,whichwasexplicitlyexploringhow
2230
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:59:42 UTC from IEEE Xplore.  Restrictions apply. ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA Yannic Noller, Ridwan Shariffdeen, Xiang Gao, and Abhik Roychoudhury
Table 1: Complete list of questions from the developer survey. In total 35 questions in 6 categories.
Category Question Type
Q1.1 Are you willing to review patches that are submitted by APR techniques? 5-Point Likert Scale
Q1.2 How many auto-generated patches would you be willing to review before losing trust/interest in the technique? Selection+Other...
C1 Usage of Q1.3 How much time would you be giving to any APR technique to produce results? Selection+Other...
APR Q1.4 How much time do you spend on average to fix a bug? Selection+Other...
Q1.5 Do you trust a patch that has been adopted from another location/application, where a similar patch was already accepted by
other developers?5-Point Likert Scale
Q1.6 Would it increase your confidence in automatically generated patches if some kind of additional input (e.g., user-provided test
cases) were considered?5-Point Likert Scale
Q1.7Besidessomeadditionalinputthatistakenintoaccount,whatothermechanismdoyouseetoincreasethetrustinauto-generated
patches?Open-Ended
Q2.1 Can you provide additional test cases (i.e., inputs and expected outputs) relevant for the reported bug? 5-Point Likert Scale
C2 Availability Q2.2 Can you provide additional assertions as program instrumentation about the correct behavior? 5-Point Likert Scale
of Inputs Q2.3 Can you provide a specification for the correct behavior as logical constraint? 5-Point Likert Scale
Q2.4 Would you be fine with classifying auto-generated input/output pairs as incorrect or correct behavior? 5-Point Likert Scale
Q2.5 How many of such queries would you answer? Selection+Other...
Q2.6 For how long would you be willing to answer such queries? Selection+Other...
Q2.7 What other type of input (e.g., specification or artifact) can you provide that might help to generate patches? Open-Ended
Q2.8Pleasedescribehow youwouldliketo engagewithanAPRtool.For exampleshortlydescribethedialogue between you(as
user of the APR tool) and the APR tool. Which input would you pass to the APR tool? What do you expect from the APR tool?Open-Ended
Q3.1 Would it increase your trust in auto-generated patches if additional artifacts such as tests/assertions are used during patching? 5-Point Likert Scale
C3Impactontrust Q3.2 Which of the following additional artifacts will increase your trust? Multiple Choice
Q3.3 What are other additional artifacts that will increase your trust? Open-Ended
Q4.1 Would it increase your trust when the APR technique shows you the code coverage achieved by the executed test cases that
are used to construct the repair?5-Point Likert Scale
C4 Explanations
for generatedQ4.2 Would it increase your trust when the APR technique presents the ratio of input space that has been successfully tested by the
inputs used to drive the repair?5-Point Likert Scale
patches Q4.3 What other type of evidence or explanation would you like to come with the patches, so that you canselectan automatically
generated patch candidate with confidence?Open-Ended
Q5.1 Which of the following information (i.e., potential side-products of APR) would be helpful to validate the patch? Multiple Choice
C5 Usage of APR Q5.2 What other information (i.e., potential side-products of APR) would be helpful to validate the patch? Open-Ended
side-products Q5.3 Which of the following information (i.e., potential side-products of APR) would help you to fix the problem yourself (without
using generated patches)?Multiple Choice
Q5.4 What other information (i.e., potential side-products of APR) would help you to fix the problem yourself (without using
generated patches)?Open-Ended
Q6.1 What is your (main) role in the software development process? Selection+Other...
C6 Background Q6.2 How long have you worked in software development? Selection
Q6.3 How long have you worked in your current role? Selection
Q6.4 How would you characterize the organization where you are employed for software development related activities? Selection+Other...
Q6.5 What is your highest education degree? Selection+Other...
Q6.6 What is your primary programming language? Selection+Other...
Q6.7 What is your secondary programming language? Selection+Other...
Q6.8 How familiar are you with Automated Program Repair? 5-Point Likert Scale
Q6.9 Are you applying any Automated Program Repair technique at work? Yes/No
Q6.10 Which Automated Program Repair technique are you applying at work? Open-Ended





	
	

!$"#!"& $##!$"#!"&

%!!$"#!"&	#	!##"	%#&"!$!"	%#&
"#!#"	%#&	""!#"	%#&"#
""!$"#!"&	#	!##"!$"#!" ##"#!"##"&	
#!"#% "#% $#! #% #!#%
Figure 3: Results for the questions with the 5-point Likert Scale (103 responses).
the participants want to engagewith an APR tool. First of all, a
strong majority (72% of the responses, ð‘ƒ<.001) indicate that the
participants are willing to review auto-generated patches (see Q1.1
in Figure 3). This finding generally confirms the efforts in the APR
communitytodevelopsuchtechniques.Only7%oftheparticipants
arereluctanttoapplyAPRtechniques intheirwork.AsshowninFigure 4, we note that 72% ( ð‘ƒ<.001) of the participants want to
review only up to 5 patches, while only 22% would review up to 10
patches. Furthermore, 6% mention that it would depend on the spe-
cificscenario.Atthesametime,theparticipantsexpectrelatively
quickresults:63%( ð‘ƒ=.003)wouldnotwaitlongerthanonehour,
of which the majority (72% of them, ð‘ƒ<.001) prefer to not even
2231
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:59:42 UTC from IEEE Xplore.  Restrictions apply. Trust Enhancement Issues in Program Repair ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA

    	 







Figure 4: Cumulative illustration of the responses for Q1.2
How many auto-generated patches would you be willing toreview before losing trust/interest in the technique?
wait longer than 30 minutes. The expected time certainly depends
ontheconcretedeployment,e.g.,repaircanalsobedeployedalong
anightlyContinuousIntegration(CI)pipeline,butourresultsin-
dicatethatdirectsupportofmanualbugfixingrequires quickfix
suggestionorhints.Infact,82%( ð‘ƒ<.001)oftheparticipantsstate
thattheyusuallyspendnotmorethan2hoursonaveragetofixa
bug, and hence, the APR techniques need to be fast to provide a
benefitforthedeveloper.Toincreasetrustinthegeneratedpatches,
80% (ð‘ƒ<.001) agree that additional artifacts (e.g., test cases), which
are provided as input for APR, are useful (see Q1.6 in Figure 3). As
a consistency check, we asked a similar question at a later point
(seeQ3.1inFigure3),andobtainedthateven84%( ð‘ƒ<.001)agree
that additional artifacts can increase trust. The most mentionedother mechanisms to increase trust are the extensive validation of
thepatcheswithatestsuite andstaticanalysistools(17/103),the
actualmanualinvestigation ofthepatches(10/103),the reputation
oftheAPRtoolitself(9/103),the explanation ofpatches(8/103),and
the provisioning of additionally generated tests (7/103).
RQ1 â€“ Acceptability of APR: Additional user-provided arti-
facts like test cases are helpfulto increase trust in automatically
generated patches. However, our results indicate that fullde-
veloper trust requires a manual patch review. At the same time,
test reports of automated dynamic and static analysis, as well as
explanations of the patch, can facilitate the reviewing effort.
Theresponsesfortheexplicitquestionaboutdevelopersâ€™envi-
sioned engagement with APR tools (Q2.8) can be categorized into
four areas: the extent of interaction, the type of input, the expected
output,andtheexpected integration intothedevelopmentworkflow.
Interaction. Mostparticipants(71/103, ð‘ƒ<.001)mentionthatthey
preferarather lowamountofinteraction,i.e.,afterprovidingthe
initial input to the APR technique, there will be no further inter-
action. Only a few responses (6/103) mention theone-time option
toprovide moretestcasesor somesortofspecificationto narrow
downthesearchspacewhenAPRrunsintoatimeout,orthegener-
ated fixes are not correct. Only 3 participants envision a high level
of interaction, e.g., repeated querying of relevant test cases.
Input.Many participants appear ready to provide failing test cases
(22/103) or relevant test cases (20/103). Others mentioned that APR
shouldtakea bugreport asinput(15/103),whichcanincludethe
stacktrace,detailsoftheenvironment,andexecutionlogs.Some
also mentioned that they envision only the provision of the bare
minimum, i.e., the program itself or the repository with the source
code (11/103).Output.Besidesthegeneratedpatches,themostmentionedhelpful
output from an APR tool is an explanation of the fixed issue includ-
ingitsrootcause (9/103).Thisanswerisfollowedbytherequirement
to present not only one patch but a list of potential patches (8/103).
Additionally, some participants mentioned that it would be helpful
to produce a comprehensive test report (6/103).
Integration. Themostmentionedintegrationmechanismistoin-
volveAPRsmoothlyinthe DevOpspipeline (17/103),e.g.,whenever
a failing test is detected by the CI pipeline, the APR would be trig-
geredtogenerateappropriatefixsuggestions.Adeveloperwould
manuallyreviewthefailedtest(s)andthesuggestedpatches.Along
withtheintegrationtheparticipantsmentionedthattheprimary
goal of APR should be to save time for the developers (8/103).
RQ1 â€“ Interaction with APR: Developers envision a low
amount of interaction with APR, e.g., by only providing ini-
tial artifacts like test cases. APR should quickly(within 30 min -
60 min) generate a smallnumber (between 5 and 10) of patches.
Moreover, APR needs to be integrated into the existing DevOps
pipelines to support the development workflow.
4.2 Availability/Impact of Artifacts (RQ2)
Inthissection,welookmorecloselyinthecategoriesC2andC3toinvestigatewhichadditionalartifactscanbeprovidedbydevelopers,and how these artifacts influence the trust in APR. We first explore
the availability of additional test cases (69% positive, ð‘ƒ<.001),pro-
gram assertions (71% positive, ð‘ƒ<.001), and logical constraints (59%
positive,ð‘ƒ=.024) (see the results for Q2.1, Q2.2, and Q2.3 in Figure
3).Furthermore,58%( ð‘ƒ=.038)oftheparticipantsarepositiveabout
answering queries toclassify generated tests asfailing or passing.
Thiscanbeunderstoodasparticipantswanttohavelowinteraction
(i.e., asking questions to the tool), but if the tool is able to issue
queries,they areready toanswer someof them(typicallyrespon-
dents prefer to answer no more than 10 queries, ð‘ƒ=.001). Based
on the results for open-ended question Q2.7, the majority of the
participants(70/103, ð‘ƒ<.001)donotseeanyotheradditionalarti-
facts(beyondtests/assertions/logical-constraints/user-queries)thattheycouldprovidetoAPR.Themostmentionedresponsesbyotherparticipantsaredifferentformsof requirementsspecification (7/103),
e.g.,writteninadomain-specificlanguage, executionlogs (6/103),
documentation of interfaces with data types and expected value
ranges(5/103), errorstacktraces (4/103),relevant sourcecodeloca-
tions(3/103),andreferencesolutions(3/103),e.g.,existingsolutions
for similar problems.
RQ2 â€“ Artifact Availability: Software developers can provide
additional artifacts like test cases, program assertions, logical
constraints, execution logs, and relevant source code locations.
Regarding an increasein trust in patches through the incorpo-
ration of additional artifacts driving repair, 93% ( ð‘ƒ<.001) of the
participantsagreethatadditional testcasesarehelpful(seeFigure5).
Thisisalsointerestingfromtheperspectiveofrecentautomated
repairtools[ 40,52]whichperformautomatedtestgenerationto
achieve less overfitting patches. Logical constraints (70%, ð‘ƒ<.001)
2232
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:59:42 UTC from IEEE Xplore.  Restrictions apply. ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA Yannic Noller, Ridwan Shariffdeen, Xiang Gao, and Abhik Roychoudhury
		
     ! 	


Figure 5: Responses for Q3.2 Which of the following addi-
tional artifacts will increase your trust?
and program assertions (68%, ð‘ƒ<.001) perform worse in this re-
spect. Although user queries allow more interaction with the APR
technique, they would not necessarily increase trust more than
the other artifacts. Only 59% ( ð‘ƒ=.024) agreed on their benefit.
Mostoftheparticipants(88/103, ð‘ƒ<.001)didnotmentionatrust
gain by other artifacts. However, some participants (3/103) men-
tionednon-functionalrequirementslikeperformanceorsecurity
aspects, which is related to a concern that auto-generated patches
mayharmexistingperformancecharacteristicsorintroducenew
security vulnerabilities.
RQ2 â€“ Impact on Trust: Additional test cases would have a
great impact on the trustworthiness of APR. There exists the
possibilityofautomaticallygeneratingteststoincreasetrustin
the auto-generated patches.
4.3 Patch Explanation/Evidence (RQ3)
In this section, we explore which patch evidence and APR side-
productscansupporttrustinAPR(seecategoriesC4andC5).We
first proposed two possible pieces of evidence that could be pre-sentedalongwiththepatches:the codecoverage achievedbythe
executedtestcasesthatareusedtoconstructtherepair,andthe ra-
tioofinputspace thathasbeensuccessfullytestedbytheautomated
patch validation. 76% ( ð‘ƒ<.001) of the participants agree that code
coverage would increase trust, and 71% ( ð‘ƒ<.001) agree with the
input ratio (see Q4.1 and Q4.2 in Figure 3). The majority of the par-
ticipants (78/103, ð‘ƒ<.001) do not mention other types of evidence
thatwouldhelptoselectapatchwithconfidence.Nevertheless,the
mostmentionedresponseisa fixsummary (10/103),i.e.,anexpla-
nation of what has been fixed including the root cause of the issue,
how it has been fixed, and how it can prevent future issues. Other
participantsmentionthe successrate incaseofpatchtransplants
(5/103),anda testreport summarizingthepatchvalidationresults
(3/103). These responses match the observations for RQ1, where
weaskedhowdeveloperswanttointeractwithtrustworthyAPR
and what output they expect.
RQ3 â€“ Patch Evidence: Software developers want to see evi-
denceforthepatchâ€™scorrectnesstoefficientlyselectpatchcandi-
dates.Developerswanttoseeinformationsuchascodecoverage
as well as the ratio of the covered input space.
A straightforward way to provide explanations and evidence is
toprovideoutputsthatarealreadycreatedbyAPRasside-products.
Welistedsomeofthemandaskedtheparticipantstoselectwhichof
them would be helpful to validate the patches (see results in Figure
6). 85% (ð‘ƒ<.001) agree that the identified faultandfix locations
   	 
 !   "
Figure 6: Responses for Q5.1 Which of the following infor-
mation(i.e.,potentialside-productsofAPR)wouldbehelpfulto validate the patch?
		
   	 
 "$%"" "#"& !"!#" "  !" "!#" ""!"!!"#"%"!
Figure 7: Responses for Q5.3 Which of the following infor-
mation (i.e., potential side-products of APR) would help youtofixtheproblemyourself(withoutusinggeneratedpatches)?
are helpful to validate the patch followed by the generated test
caseswith 79% ( ð‘ƒ<.001) agreement. In addition, a few participants
emphasizetheimportanceofa testreport (4/103),anexplanationof
theroot cause and thefix attempt (4/103).
Finally,weexplorewhichside-productsaremostusefulforde-
velopers,evenwhenAPRcannotidentifythecorrectpatch.Figure7
showsthattheidentifiedfaultandfixlocationsareofmostinterest
(82%,ð‘ƒ<.001), followed by the generated test cases (75%, ð‘ƒ<.001).
Veryfewparticipantsaddthatanissuesummary(2/103)andthe
potentialresultsofadataflowanalysis(2/103)couldbehelpfultoo.
RQ3 â€“ APRâ€™s Side-Products: Our results indicate that side-
productsofAPRlikethe faultandfixlocations andthegenerated
testcases canassistmanualpatchvalidation,andhence,enhance
trust in APR.
5 EVALUATION METHODOLOGY
We nowinvestigate towhich extent existingAPR techniquessup-
port the expectations and requirements collected with our survey.
Notallaspectsofourdevelopersurveycanbeeasilyevaluated.For
example,theevaluationoftheamountofinteraction,theintegra-
tion into existing workflows, the output format for the efficient
patch selection, and the patch explanations, require additional case
studiesand furtheruser experiments.In thisevaluation, wefocus
on the quantitative evaluation of the relatively short patching time
(30-60 min), the limited number of patches to manually investigate
(5to10),handlingofadditionaltestcasesandlogicalconstraints,
andtheabilitytogeneratearepairataprovidedfixlocation.We
explorewhetherstate-of-the-artrepairtechniquescanproducecor-
rect patches under configurations that match these expectationsandrequirements.Specifically,weaimtoprovideanswerstothe
research questions RQ4 and RQ5.
APR Representatives. In our evaluation, we selected tools to repre-
sent a wide spectrum of state-of-the-art APR techniques: search-
based(GenProg[ 21]),semantic-based(Angelix[ 31]),thecombi-
nation of search-based and learning-based (Prophet [ 26]), and the
integrationoftestinginsiderepairtotackleoverfitting(Fix2Fit[ 12],
2233
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:59:42 UTC from IEEE Xplore.  Restrictions apply. Trust Enhancement Issues in Program Repair ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
CPR [40]). We further selected tools that apply on C due to our
evaluationsubjects. GenProg[ 21]isasearch-basedprogramre-
pair tool that evolves the buggy program by mutating program
statements.Itisawell-knownrepresentativeofthegenerate-and-
validate repair techniques. Angelix [ 31] is a semantic program
repair technique that applies symbolic execution to extract con-
straints, which serve as a specification for subsequent programsynthesis. Prophet [
26] combines search-based program repair
with machine learning. It learns a code correctness model from
open-source software repositories to prioritize and rank the gener-
atedpatches.Fix2Fit[ 12]combinessearch-basedprogramrepair
with fuzzing. It uses grey-box fuzzing to generate additional test
inputs to filter overfitting patches that crash the program. The test
generationprioritizesteststhatrefineanequivalenceclassbased
patch space representation. CPR [ 40] uses semantic program re-
pair andconcolic test generationfor refining abstractpatches and
for discarding overfitting patches. It takes a logical constraint as
additional user input to reason about the generated tests inputs.
SubjectPrograms. WeusetheManyBugs[ 20]benchmark,which
isawell-establishedbenchmarkinAPR,andalloftheconsidered
techniques/tools also use (some of) these subjects in their eval-uation. Therefore, it is a benchmark for which it is known thatthe examined tools can identify patches. Our goal is to evaluate
whethertheycan stillidentifypatches withchanged/limitedenvi-
ronmental conditions (e.g., timeout, set of available test cases etc).
Thebenchmarksetconsistsof185defectsin9open-sourceprojects.
For each subject, ManyBugs includes a test suite created by the
originaldevelopers. Note thatallofthestudiedrepairtechniques
requireand/orcanincorporateatestsuiteintheirrepairprocess.
For our evaluation, we filter the 185 defects that have been fixed
bythedeveloperatasinglefixlocation.Weremovedefectsfrom
"Valgrind" and "FBC" subjects due to the inability to reproduce the
defects. Finally, we obtain 60 defects in 6 different open-source
projects (see Table 2).
Table 2: Experiment subjects and their details
Program Description LOC Defects Tests
LibTIFF Image processing library 77k 7 78
lighttpd Web server 62k 2 295
PHP Interpreter 1046k 43 8471
GMP Math Library 145k 1 146
Gzip Data compression program 491k 3 12Python Interpreter 407k 4 355
Experimental Configurations and Setup. All tools are configured
to run in full-exploration mode; which will continue to generate
patches even after finding one plausible patch until the timeout or
thecompletionofexploringthesearchspace.Tostudytheimpactof
fixlocationsandtestcasevariations(seeRQ5),weevaluateeachtool
usingdifferentconfigurations(seeTable3).Ineachconfiguration
we provide the relevant source file to all techniques, however, with
"developerfixlocation"weprovidetheexactsourcelinenumberas
well.Notethateachsetupusesa1-hourtimeout,whichischosen
based on our survey responses: 63% ( ð‘ƒ=.003) of all participants
would expect results within 1 hour.Table 3: Experiment configurations
ID Fix Location Passing Tests Timeout
EC1 tool fault localization 100% 1hr
EC2 developer fix location 100% 1hr
EC3 developer fix location 0% 1hr
EC4 developer fix location 50% 1hr
EvaluationMetrics. Inordertoassessthetechniquesandsupport
theansweringofourresearchquestions,weconsiderthefollowingeightmetrics,whichareinspiredbyexistingstudiesinAPR[
24,25]:
M1the search space sizeof the repair tool, M2the number of
enumerated/explored patches,M3theexplored ratiowithrespect
to the search space, M4the number of non-compilable patches,M5
the number of non-plausible patches, i.e., patches that have been
explored but ruled out because existing or generated test cases are
violated, M6the number of plausible patches,M7the number of
correctpatches,and M8thehighest rankofacorrectpatch.M1-M6
help to analyze the overall search space creation and navigation of
each technique. The definition of the search space size (M1) for the
defect,aswellasthedefinitionofanenumerated/exploredpatch
(M2),varyforeachtool.Weincludeallexperimentprotocolsinour
replicationartifact,whichdescribeshowtocollectthesemetricsfor
each tool. M7-M9 assess the repair outcome, i.e., the identification
of thecorrectpatch. We define a patch as correctwhenever it is
semantically equivalent to the developer patch that is provided
in our benchmark. To check for the correct patch, we manuallyinvestigated only the top-10 ranked patches because our survey
concludedthatdeveloperswouldnotexplorebeyondthat.Notethatnotalltechniquesprovideapatchranking(e.g.,Angelix,GenProg,
and Fix2Fit). In these cases, we use the order of generation.
Hardware. All our experiments were conducted using Docker con-
tainersontopofAWS(AmazonWebServices)EC2instances.We
usedthec5a.8xlargeinstancetype,whichprovides32vCPUpro-
cessing power and 64GiB memory capacity.
Replication. Ourreplicationpackagecontainsallexperimentlogs
andsubjects,aswellasprotocolsthatdefinethemethodologyused
to analyze the output of each repair tool [ 35]. In particular, we
describe how to retrieve each evaluation metric for the specific
repair techniques.
Experimental Setup: Our experiments are meant to investi-
gatespecificaspectsconcerningtheincreaseof programrepair
adoption based on the results of our developer survey. We as-
sumethatthedeveloper/userisnotanAPRexpert,andhence,
wouldusethe defaultparameter settingsinsteadoffine-tuning
orextendingthetools.Furthermore,ourexperimentsuse strict
timeoutsand computation power restrictions. Other setups can
lead to different and better results.
6 EVALUATION RESULTS
Table4summarizesourevaluationresults.ForeachAPRtechnique
we show its performance under the given experimental configura-
tion (see Table 3). Each cell shows |ð‘ƒð‘ƒð‘™ð‘Žð‘¢ð‘ |/|ð‘ƒð¶ð‘œð‘Ÿð‘Ÿ|, where|ð‘ƒð‘ƒð‘™ð‘Žð‘¢ð‘ |
2234
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:59:42 UTC from IEEE Xplore.  Restrictions apply. ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA Yannic Noller, Ridwan Shariffdeen, Xiang Gao, and Abhik Roychoudhury
Table4:Experimentalresultsforthevariousconfigurations.Eachcellshowsthenumberofsubjects,forwhichthetechnique
was able to identify at least one Plausible /Correct patch with regard to the specific configuration. Please also see "Threats to
Validity of Experimental Results" in Section 7 to understand the context of these results fully.
Subject Def.Angelix Prophet GenProg Fix2Fit CPR
EC1 EC2 EC3 EC4 EC1 EC2 EC3 EC4 EC1 EC2 EC3 EC4 EC1 EC2 EC3 EC4 EC2 EC3 EC4
LibTIFF 73/1 3/1 3/1 3/1 1/0 1/0 1/0 1/0 5/0 5/0 5/0 5/0 5/1 4/1 4/1 4/1 4/2 4/2 4/2
lighttpd 2---- 1/0 0/0 0/0 0/0 1/0 1/0 1/0 1/0 1/0 1/0 1/0 1/0 ---
PHP 430/0 0/0 0/0 0/0 0/0 0/0 2/1 3/1 0/0 0/0 10/1 0/0 8/1 4/2 7/2 5/1 5/4 5/4 5/4
GMP 10/0 0/0 0/0 0/0 0/0 0/0 0/0 0/0 0/0 0/0 0/0 0/0 0/0 0/0 0/0 0/0 1/1 1/1 1/1
Gzip 30/0 1/0 1/0 1/0 0/0 1/1 1/1 1/1 0/0 0/0 0/0 0/0 0/0 0/0 0/0 0/0 3/1 3/1 3/1
Python 4---- 0/0 1/1 1/1 1/1 0/0 0/0 0/0 0/0 0/0 0/0 0/0 0/0 ---
Overall 603/1 4/1 4/1 4/1 2/0 3/2 5/3 6/3 6/0 6/0 16/1 6/0 14/2 9/3 12/3 10/2 13/8 13/8 13/8
Table5:Experimentalresultsfortheaverageexplorationra-
tio|PExpl|for EC1 and EC2.
SubjectAngelix Prophet GenProg Fix2Fit
EC1 EC2 EC1 EC2 EC1 EC2 EC1 EC2
LibTIFF 86 100 24 93 12 7 100 100
lighttpd -- 20 100 <1 51 100 100
PHP 96 100 22 96 <1 91 63 80
GMP 100 100 41 100 5 100 --
Gzip 100 100 6 100 18 100 100 100
Python -- 14 100 1 100 --
Overall 95 100 21 98 47 8 91 95
is the number of defects for which the tool was able to generate
at least one plausible patch (i.e., M6), and similarly |ð‘ƒð¶ð‘œð‘Ÿð‘Ÿ|is the
numberofdefectsforwhichthetoolwasabletogenerateacorrectpatchamongthetop-10plausiblepatches.Forexample,theLibTIFF
project has 7 defects, for which Angelix was able to generate 3
plausibleand1correctpatchforthesetupEC1(i.e.,1-hourtimeout,
tool fault localization, and all available test cases). Due to limita-
tions in its symbolic execution engine KLEE [ 6], Angelix and CPR
do not support lighttpd and python, and the corresponding cellsare marked with â€œ-â€. For CPR, we are not able to produce resultsfor EC1 because it does not have its own fault localization, andhence, requires the fix location as an input. Additionally, Table 5
presents the average patchexploration/enumeration ratio |ð‘ƒð¸ð‘¥ð‘ð‘™|
of the techniques with respect to the patch space size, computed as
a percentage of M2/M1 for each defect considered in each subject.
6.1 APR within realistic boundaries (RQ4)
The numbers in Table 4 show that the overall repair success is
limited. For example, Fix2Fit can generate plausible patches for
14 defectswith EC1, whileCPR can generatecorrect patches for8
defectsgiventhecorrectfixlocation.Comparedtopreviousstud-
ies, the number of plausible patches is lower in our experiments,mainly due to the 1-hour timeout. Prior research on program re-pair have experimented with 10-hour [
30], 12-hour [ 26,31] and
24-hour[12]timeouts,anddeterminedwhetheracorrectpatchcan
beidentifiedamong allgeneratedplausiblepatches.Thefocusof
these prior experiments was to evaluate the capability to generate
apatch,whereas,inourwork,wefocusontheperformancewithin
atolerabletimelimitsetbydevelopers.Notonlythetimeoutbutalso a scenario-specific parameter fine-tuning can affect the resultsgreatly.Forexample,whenwemodifythe synthesis-level parameter
of Angelix (a parameter that modulates the back-end synthesisof the tool, and hence, can affect the search space), we can seeadditionalpatchesbeinggenerated,suchasforadefectinLibtiff
(3edb9cd),intheEC3configuration.Ourreportedexperimentsonly
use thedefault parameters. In future, for a full investigation of the
repairtoolsâ€™capabilities,it willthereforebenecessarytoconduct
an exploration of the parameter choices in each repair tool, which
has not been done in this paper.
RQ4 â€“ Repair Success: Under ourtight constraints (i.e.,the 1-
hourtimeoutandthetop-10ranking)andtheirdefaultparametersetups,currentstate-of-the-artrepairtechniquescannotidentify
many plausible patches for the ManyBugs benchmark.
Automated program repair tools are only beginning to gain
adoption,andarestillanemergingtechnology.Wewanttoidentify
what it would take to increase the adoption of program repair.
Ingeneral,therepairsuccessofanAPRtechniqueisdeterminedby (1) its search space, (2) the exploration of this search space,and (3) the ranking of the identified patches. In a nutshell, this
means,ifthecorrectpatchisnotinthesearchspace,thetechnique
cannot identify it. If the correct patch is in the search space, but
APRdoesnotidentifyitwithinagiventimeoutorotherresource
limitations,itcannotreportitasaplausiblepatch.Ifitidentifiesthe
patchwithintheavailableresourcesbutcannotpinpointitinthe
(potentiallyhuge)spaceofgeneratedpatches,theuser/developer
will not recognize it. By means of these impediments for repair
success in real-world scenarios, we examine the considered repair
techniques. Our goal is to identify the concepts in APR that arenecessary to achieve the developersâ€™ expectations, and hence, to
improve the state-of-the-art approaches.
Search Space. Table 5 shows that Angelix explores almost its com-
plete search space within the 1-hour timeout, while Table 4 shows
thatitcanidentifyplausiblepatchesforonlyonedefect(withEC1).
Asdescribedin[ 30],theprogramtransformations(tobuild/explore
thesearchspace)byAngelixonlyincludethemodificationofexist-ing side-effect-free integer expressions/conditions and the additionofif-guards.Therefore,weconcludethatAngelixâ€™ssearchspaceistoolimitedtocontainthecorrectpatches.Theothertechniques,on
theotherhand,considerlargersearchspaces.Prophetalsocon-
siders the insertion of statements and the replacement of function
2235
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:59:42 UTC from IEEE Xplore.  Restrictions apply. Trust Enhancement Issues in Program Repair ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
calls.GenProgcaninsert/removeanyavailableprogramstatement.
Fix2Fit uses the search space by f1x [ 30], which combines the
search spaces of Angelix and Prophet to generate a larger search
space.CPRusesthesameprogramtransformationsasAngelixbutis designed to easily incorporate additional user inputs like custom
synthesis components to enrich its search space.
RQ4 â€“Search Space: Successful repair techniques need to con-
sider a wide range of program transformations and should be
able to take user input into account to enrich the search space.
SearchSpaceExploration. ProphetandGenProgshowarelatively
low exploration ratio with 21% and 4% respectively (see EC1 in
Table5),whichleadstoalownumberofplausiblepatchesinour
experiments. Instead, Fix2Fit fully explores the patch search space
formostoftheconsidereddefects(exceptforPHP),whichleadstoahighpossibilityoffindingaplausiblepatch.CPR(notshowninthetable)fullyexploresitssearchspaceinourexperiments.Incontrast
to Prophet and GenProg, Fix2Fit and CPR perform grouping and
abstracting of patches, to explore them efficiently. Fix2Fit groups
the patches by their behavior on test inputs and uses this equiv-alence relation to guide the generation of additional inputs. CPRrepresents the search space in terms of abstractpatches, which
are patch templates, accompanied by constraints. CPR enumerates
abstractpatchesinsteadofconcretepatches,andhence,canreasonaboutmultiplepatchesatoncetoremoveorrefinepatches.Prophet
and GenProg, however, explore and evaluate all concrete patches,
which causes a significant slowdown. Reduction of the patch vali-
dation time is possible if we can validate patches without the need
to re-compile the program for each concrete patch [8, 9, 49].
RQ4 â€“ Patch Space Exploration: A large/rich search space
requires an efficientexploration strategy, which can be achieved
by, e.g., using search space abstractions.
PatchRanking. AlthoughFix2Fitbuildsarichsearchspaceandcan
efficientlyexploreit,itstillcannotproducemanycorrectpatchesin
our experiments. One reason is that Fix2Fit can identify a correct
patchbutfailstopinpointitinthetop-10patchesbecauseitonly
applies a rudimentary patch ranking, which uses the edit-distance
betweentheoriginalandpatchedprogram.Forinstance,Fix2Fit
generates the correct patch for the defect 865f7b2in the LibTiff
subjectbutranksitbelowposition10,andhence,itisnotconsidered
in our evaluation. Furthermore, Fix2Fitâ€™s patch refinement and
rankingisbasedoncrash-avoidance,whichisnotsuitableforatest-
suiterepairbenchmarksuchasManyBugsthatdoesnotinclude
manycrashingdefects.CPRimprovesonthatbyleveragingtheuser-
provided logical constraint to reason about additionally generated
inputs, while the patch behaviors on these inputs are collected and
used to rank the patches. But still, overall, it cannot produce many
correct patches within the top-10. We also investigated how many
ofthecorrectpatchesarewithinthetop-5because72%( ð‘ƒ<.001)of
oursurveyparticipantsfavoredreviewingonlyupto5patches(see
Figure 4). We observed that most identified correct patches within
thetop-10arerankedveryhighsothatthereisnotmuchdifferenceifatop-5thresholdisapplied.Recentworks[
49,50]proposetheuseof the test behavior similarity between original/patched programs
to rank plausible patches, which is a promising future direction.
RQ4 â€“ Patch Ranking: After exploring the correct patch, an
effective patch ranking is the last impediment for the developer.
6.2 Impact of additional inputs (RQ5)
Providing Fix Location as User input. In Table 4, the column EC1
showstheresultswiththetoolâ€™sfaultlocalizationtechnique,and
columnEC2showstheresultsbyrepairingonlyatthedeveloper-
provided(correct)fixlocation.Intuitively,oneexpectsthatequipped
with the developer fix location, the results of each repair technique
should improve. However, the results by Angelix and GenProg
do not change (except for one more plausible patch with Angelix).
From the previous discussion about the search space, we conclude
thattheprogramtransformationsbyAngelixarethemainlimitingfactortotheextentthateventheprovisionofthecorrectfixlocation
has no impact. For GenProg we know from the EC3 configuration
that there is at least one correct patch in the search space (see
Table 4). Therefore, we conclude that GenProg suffers from its
inefficient space exploration so that even the space reduction by
settingthefixlocationhasnoimpact.Prophetinsteadcangenerate
two additional correct patches in EC2, and hence, benefits from
the precise fixlocation.The exploration ratio in Table5 shows thatProphet almost fully explores its search space in EC2, indicating a
smaller search space. Fix2Fit can generate one more correct patch
as compared to EC1. Similar to Prophet, Fix2Fit benefits fromthe precise fix location and can explore more of its search space.
Note that CPR is not included in the comparison between EC1 and
EC2 because it does not apply for EC1. However, for EC2, it can
generatethehighestnumberofcorrectpatches.Besidesitsefficientpatchspaceabstraction,weattributethistoitsabilitytoincorporate
additionaluserinputslikethefixlocationandtheuser-provided
logical constraint.
RQ5â€“FixLocation: Ourresultsshowthattheprovisionofthe
precise and correct fix location does not necessarily improve
the outcome of the state-of-the-art APR techniques due to their
limitationsinsearchspaceconstructionandexploration.How-
ever, being amenable to such additional inputs can significantly
improve the repair success, as shown by results from CPR.
VaryingPassingTestCases. Toexaminetheimpactofthepassing
testcases,weconsiderthedifferencesbetweenthecolumnsEC2,
EC3,andEC4inTable4.Ingeneral,morepassingtestcasescanlead
tohigh-qualitypatchesbecausetheyrepresentinformationabout
thecorrectbehavior.Inlinewiththis,weobservethatmorepassing
test cases lead to fewer plausible patches because the patch valida-
tioncanremovemoreoverfittingpatches.ForAngelixhowever,
weobservethatthereisnodifferenceduetoitslimitedsearchspace.
CPRisalsonotaffectedbythevaryingnumberofpassingtestcases.
It uses the failing test cases to synthesize the search space and the
passing test cases as seed inputs for its input generation. But since
CPR always fullyexploresthe searchspace inour experiments,the
variationoftheinitialseedinputshasnoeffectwithinthe1hour.
Overall, we observe three different effects: (a) For techniques with
2236
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:59:42 UTC from IEEE Xplore.  Restrictions apply. ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA Yannic Noller, Ridwan Shariffdeen, Xiang Gao, and Abhik Roychoudhury
a limited search space (e.g., Angelix), passing test cases have very
lowornoeffect.(b)Fortechniquesthatsufferfrominefficientspace
exploration strategies (e.g., GenProg and Prophet), having fewer
passing test cases can speeduptherepairpro cess and lead to more
plausible(possiblyoverfitting)patches.(c)Otherwise(e.g.,Fix2Fit),
variationsinthepassingtestcasescanstillinfluencetheranking.
Whether more tests are better depends on the APR strategy and its
characteristics,asdiscussedinSection6.1.Therefore,wesuggest
that APR techniques incorporate an intelligent test selection or
filtering mechanism, which is not yet studied extensively in thecontext of APR. Recently, [
27] suggested applying traditional re-
gression test selection and prioritization to achieve better repair
efficiency. Furtherdeveloping andusing sucha mechanism repre-
sents a promising research direction. Note that in the discussed
experiments, thefix locationwas definedbeforehand. However,if
APRtechniques useatest-basedfault localization technique(like
in EC1), thetest cases havean additional effect on thesearch space
and repair success.
RQ5 â€“ Test Cases: Variation of passing test cases causes dif-
ferenteffectsdependingonthecharacteristicsoftheAPRtech-
niques. Overall, one needs an intelligent test selection method.
7 THREATS TO VALIDITY
ExternalValidityofSurvey. Althoughwereachedouttodifferent
organizations in different countries, we cannot guarantee that our
survey results can be generalized to all software developers. Tomitigatethisthreat,wemadeallresearchartifactspubliclyavail-
able[35]sothatotherresearchersandpractitionerscanreplicate
our study. To reduce the risk of developers not participating or
thevolunteerbias,wedesignedthesurveyforashortcompletion
time(15-20min)andprovidedincentiveslikecharitydonationsand
(in the case of MTurk) monetary compensation. Another potential
threat to validity is that only 15% of all participants responded that
they arefamiliarwithAPR (seeQ6.8/9/10). Thisis tobe expected
as APR is not(yet) heavily applied in theindustry (with exceptionslikeFacebookandBloomberg).Toensurethattheparticipantshave
an idea of APR, we added a description and a link to an illustrative
video at the beginning of our survey form. We note that we are
exploring what it would take for developers to try out programrepair, since developers may have general preconceived notions.
Byfindingoutwhatwouldmakethedeveloperscomfortabletouse
APR, we can hope to increase adoption.
Construct Validity of Survey. In our survey, to encourage candid
responsesfromparticipants,wedidnotcollectanypersonallyiden-
tifyinginformation.Additionally,weappliedcontrolquestionsto
filter non-genuine answers. To mitigate the risk of wrong interpre-
tationofthecollectedresponses,weperformedqualitativeanalysis
coding, for which all codes have been checked and agreed by atleast two authors. Although we found general agreement across
participants for many questions, we consider our results only as a
first step towards exploring trustworthy APR.
InternalValidityofSurvey. Ourparticipantscouldhavemisunder-
stood our survey questions, as we could not clarify any particulars
due to the nature of online surveys. To mitigate this threat, weperformedasmallpilotsurveywithfivedevelopers,inwhichwe
askedforfeedbackaboutthequestions,thesurveystructure,and
the completion time. Additionally, there is a general threat that
participantscouldsubmitmultipleresponsesbecauseoursurvey
was completely anonymous.
ThreatstoValidityofExperimentalResults. Inourempiricalanalysis,
wedonotcoverallavailableAPRtools,butinstead,wecoverthe
main APR concepts: search-based, semantics-based, and machine-
learning-basedtechniques.WithManyBugs[ 20]wehavechosen
a benchmark that is a well-known collection of defects in open-source projects. Additionally, it includes many test cases, whichare necessary to evaluate the aspects of test case provision. The
metricsinourquantitativeevaluationmeasurethepatchgeneration
progress,measuringrepairefficiency/effectivenessviavariations
in configurations (EC1-EC4). To mitigate the threat of errors in oursetupofexperiments,weperformedpreliminaryrunswithasubset
of the benchmark and manually investigated the results.
OurexperimentalresultsinSection5explorethecapabilityofthe
repair tools to produce patches within a 1-hour timeout. Different
results may be observed if a different timeout is chosen. More
importantly, it is possible to get significantly better results from the
repairtoolsbyfine-tuningtheparametersoftherepairtools.For
example,whenwemodifythe synthesis-level parameterof Angelix
(aparameterthatmodulatestheback-endsynthesisofthetool,and
hence,canaffectthesearchspace),wecanseeadditionalpatches
being generated, such as for a defect in Libtiff. In our experiments,
wedidnotfine-tunesuchparametersbutinsteadusedthedefault
parameter settings, to simulate the experience of novice APR users.
It is entirely possible that more expert APR users will be able tousethetoolsmoreeffectivelytogetbetterresults.Theimpactof
parameter choices can also be rather nuanced e.g. Angelix is built
ontopofKLEEsymbolicexecutionengineandKLEEhasparameter
settings of its own. Furthermore, we only share the results for the
1-hour timeout as it is closer to the time tolerance mentioned by
our study participants.
8 RELATED WORK
Ourrelatedworkincludesconsiderationsoftrustissues[ 2,5,38]
and studies about the human aspects in automated program re-
pair[7,11,16,23,43],userstudiesaboutdebugging[ 36],andem-
piricalstudiesaboutrepairtechniques[ 18,24,25,29,33,46,48,51].
With regard to human aspects in automated program repair, our
surveystudycontributesnovelinsightsaboutthedevelopersâ€™expec-
tations on their interaction with APR and which mechanisms help
toincreasetrust.Withregardtoempiricalstudies,ourevaluation
contributes a fresh perspective on existing APR techniques.
Trust Aspects in APR. Trust issues in automated program repair
emergefromthegeneraltrustissuesin automation.LeeandSee[ 22]
discuss that users tend to reject automation techniques whenever
theydonottrustthem.Therefore,forthesuccessfuldeployment
of automated program repair in practice, it will be essential tofocus on its human aspects. With respect to this, our presented
surveycontributestotheknowledgebaseofhowdeveloperswantto
interact with repair techniques, and what makes them trustworthy.
2237
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:59:42 UTC from IEEE Xplore.  Restrictions apply. Trust Enhancement Issues in Program Repair ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
ExistingresearchontrustissuesinAPRfocusesmainlyonthe
effectofpatchprovenance,i.e.,thesourceofthepatch.Ryanand
Alarconetal.[ 2,38]performeduserstudies,inwhichtheyasked
developers to rate the trustworthiness of patches, while the re-
searchers varied the source of the patches. Their observations indi-
catethathuman-writtenpatchesreceiveahigherdegreeoftrust
thanmachine-generatedpatches.Bertrametal.[ 5]conductedan
eye-tracking study to investigate the effect of patch provenance.
Theyconfirmadifferencebetweenhuman-writtenandmachine-
generated patches and observe that the participants prefer human-
written patches in terms of readability and coding style. Our study,
ontheotherhand,explorestheexpectationsandrequirementsof
developers for trustworthy APR. The work of Weimer et al. [ 47]
proposed strategies to assess repaired programs to increase human
trust. Ourstudy resultsconfirm thatan efficientpatch assessment
is crucial and desired by the developers. We note that [ 47] focuses
on how to assess APR, while we focus on how to enhance/improve
APR in general, specifically in terms of its trust.
HumanAspectsinAPR. OtherhumanstudiesintheAPRcontextfo-
cus on how developers interact with APRâ€™s output, i.e., the patches.
Cambronero et al. [ 7] observed developers while fixing software
issues. They infer that developers would benefit from patch expla-
nationandsummariestoefficientlyselectsuitablepatches.They
propose toexplain theroles ofvariables andtheir relationto theoriginal code, to list the characteristics of patches, and to sum-marize the effect of the patches on the program. Tao et al. [
43]
exploredhowmachine-generatedpatchescansupportthedebug-
ging process.They conclude that,compared to debuggingknow-
ing only the buggy location, high-quality patches can support the
debuggingeffort,whilelow-qualitypatchescanactuallycompro-
mise it.Liang et al. [ 23] concluded thateven incorrect patchesare
helpful if they provide additional knowledge like fault locations.Fry et al. [
11] explored the understandability and maintainabil-
ity of machine-generated patches. While their participants labelmachine-generated patches as â€œslightlyâ€ less maintainable than
human-written patches, they also observe that some augmentation
of patches with synthesized documentation can reverse this trend.
Kimetal.[ 16]proposedtheirtemplate-basedrepairtechniquePAR
andevaluatedthepatchacceptabilitycomparedtoGenProg.AllofthesepreliminaryworksexplorethereactionsontheoutputofAPR.
While our findings confirm previous hypotheses, e.g., that fault lo-
cations are helpful side-products of APR [ 23] or that an efficient
patchselectionisimportant[ 23,47],ourworkalsoconsidersthe
input to APR, the interaction with APR during patch generation,
and how trust can be accomplished.
Debugging. ParninandOrso[ 36]investigatetheusefulnessofde-
buggingtechniquesinpractice.Theyobservethatmanyassump-
tions madeby automateddebugging techniquesoften donot hold
inpractice.Johnsonetal.[ 15]explorebarriersforthewideadop-
tionofstaticanalysistoolsandhowwellsuchtoolsfitintoactual
development workflows. They conduct interviews with developers
and discuss their feedback to identify how those techniques can be
improved. Although we focus on automated program repair, our
research theme is related to [ 36] and [15]. We strive to understand
howdeveloperswanttouseautomatedprogramrepairandwhether
current techniques support these aspects.EmpiricalEvaluationofAPR. Thelivingreviewarticleonautomated
program repair by Martin Monperrus [ 32] lists (at the point of
timewewrotethispaper)43empiricalstudies.Mostofthemare
concerned about patch correctness to compare the success of APR
techniques. Other frequently explored aspects are repair efficiency
[18,24,25,29], the impact of fault locations [ 24,25,48,51], and
the diversity of bugs [ 18,24,24]. Less frequently studied aspects
are the impact of the test suite [ 18,33] and its provenance [ 19,33],
specificallytheproblemoftest-suiteoverfitting[ 19,24],andhow
closethegeneratedpatchescometohuman-writtenpatches[ 46].
Ourempiricalevaluationisnotjustanotherempiricalassessmentof
APRtechnologies.Itisspecificallylinkedtothecollecteddeveloper
expectations from our survey. It limits the timeout to 1 hour, only
explores the top-10 patches, and explores various configurations of
passingtestsaswellastheimpactoffixlocations.Togetherwith
our survey results, our empirical/quantitative evaluation provides
thebuildingblockstocreatetrustworthyAPRtechniques,which
will need to be validated via future user studies with practitioners.
9 DISCUSSION
Inthispaper,wehaveinvestigatedtheissuesinvolvedinenhanc-
ingdevelopertrustinautomaticallygeneratedpatches.Through
adetailedstudywithmorethan100practitioners,weexplorethe
expectations and tolerance levels of developers with respect toautomated program repair tools. We then conduct a quantitativeevaluation of existing repair tools to simulate the experience ofnovice APR users. Our qualitative and quantitative studies indi-
catedirectionsthatneedtobeexploredtogaindevelopertrustin
patchesâ€”lowinteractionwithrepairtools,exchangeofartifacts
suchasgeneratedtestsasinputsaswellasoutputofrepairtools,
and paying attention to abstract search space representations over
and above search algorithmic frameworks. Each repair tool has
many parameters and we only used the default parameter settings
aswouldbeexpectedfromnoviceusersâ€”wedidnotexplorethe
various parameter settings. To understand the full capability of the
repair tools, in future it would be worthwhile to systematically
explore a large number of parameter settings and try out the tools
with various different timeouts.
We note that increasingly there is a move towards automated
code generation such as the recently proposed Github Copilot, but
thisraisesthequestionofwhethersuchautomaticallygenerated
code can be trusted. Developing technologies to support mixed
usageofmanuallywrittenandauto-generatedcode,whereprogram
repair can improve the automatically generated code â€“ could be an
enticing research challenge for the community.
DATASET FROM OUR WORK
Ourreplicationpackagewiththesurveyandexperimentartifacts
is available on Zenodo [35].
ACKNOWLEDGMENT
This research is supported by the National Research Foundation,
Prime Ministerâ€™s Office, Singapore under its Campus for ResearchExcellence and Technological Enterprise (CREATE) programme.
2238
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:59:42 UTC from IEEE Xplore.  Restrictions apply. ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA Yannic Noller, Ridwan Shariffdeen, Xiang Gao, and Abhik Roychoudhury
REFERENCES
[1]2008.Binomial Test. Springer New York, New York, NY, 47â€“49. https://doi.org/
10.1007/978-0-387-32833-1_36
[2]GeneM.Alarcon,CharlesWalter,AnthonyM.Gibson,RoseF.Gamble,August
Capiola, Sarah A.Jessup, and Tyler J. Ryan.2020. Would You FixThis Code for
Me? Effects of Repair Source and Commenting on Trust in Code Repair. Systems
8, 1 (2020). https://doi.org/10.3390/systems8010008
[3]JohannesBader,AndrewScott,MichaelPradel,andSatishChandra.2019. Getafix:
Learning to fix bugs automatically. Proceedings of the ACM on Programming
Languages 3, OOPSLA (2019), 1â€“27.
[4]Rohan Bavishi, Hiroaki Yoshida, and Mukul R Prasad. 2019. Phoenix: Automated
data-drivensynthesisofrepairsforstaticanalysisviolations.In Proceedingsof
the201927thACMJointMeetingonEuropeanSoftwareEngineeringConference
and Symposium on the Foundations of Software Engineering. 613â€“624.
[5]IanBertram,JackHong,YuHuang,WestleyWeimer,andZohrehSharafi.2020.
Trustworthiness Perceptions in Code Review: An Eye-Tracking Study. In Pro-
ceedingsofthe14thACM/IEEEInternationalSymposiumonEmpiricalSoftware
Engineering and Measurement (ESEM) (ESEM â€™20). Association for Computing
Machinery, New York, NY, USA. https://doi.org/10.1145/3382494.3422164
[6]Cristian Cadar, Daniel Dunbar, and Dawson Engler. 2008. KLEE: Unassisted and
Automatic Generation of High-Coverage Tests for Complex Systems Programs.
InProceedings of the 8th USENIX Conference on Operating Systems Design and
Implementation (San Diego, California) (OSDIâ€™08). USENIX Association, USA,
209â€“224.
[7]JosÃ© Pablo Cambronero, Jiasi Shen, JÃ¼rgen Cito, Elena Glassman, and Martin
Rinard. 2019. Characterizing Developer Use of Automatically Generated Patches.
In2019 IEEE Symposium on Visual Languages and Human-Centric Computing
(VL/HCC) . 181â€“185. https://doi.org/10.1109/VLHCC.2019.8818884
[8]LingchaoChen,YichengOuyang,andLingmingZhang.2021.FastandPreciseOn-
the-Fly Patch Validation for All. In 2021 IEEE/ACM 43rd International Conference
onSoftwareEngineering(ICSE).1123â€“1134. https://doi.org/10.1109/ICSE43902.
2021.00104
[9]Thomas Durieux, Benoit Cornu, Lionel Seinturier, and Martin Monperrus. 2017.
Dynamic patch generationfor null pointer exceptionsusing metaprogramming.
In2017 IEEE 24th International Conference on Software Analysis, Evolution and
Reengineering (SANER). 349â€“358. https://doi.org/10.1109/SANER.2017.7884635
[10]Kami Ehrich. 2020. Mechanical Turk: Potential Concerns and Their Solutions.
https://www.summitllc.us/blog/mechanical-turk-concerns-and-solutions.
[11]ZacharyP.Fry,BryanLandau,andWestleyWeimer.2012. AHumanStudyofPatch Maintainability. In Proceedings of the 2012 International Symposium on
Software Testing and Analysis (Minneapolis, MN, USA) (ISSTA 2012). Association
for Computing Machinery, New York, NY, USA, 177â€“187. https://doi.org/10.
1145/2338965.2336775
[12]XiangGao,SergeyMechtaev,andAbhikRoychoudhury.2019. Crash-Avoiding
ProgramRepair.In Proceedingsofthe28thACMSIGSOFTInternationalSymposium
onSoftwareTestingandAnalysis (Beijing,China) (ISSTA2019).Associationfor
Computing Machinery, New York, NY, USA, 8â€“18. https://doi.org/10.1145/
3293882.3330558
[13]XiangGao,BoWang,GregoryJ.Duck,RuyiJi,YingfeiXiong,andAbhikRoy-
choudhury. 2021. Beyond Tests: Program Vulnerability Repair via Crash Con-
straint Extraction. ACM Trans. Softw. Eng. Methodol. 30, 2, Article 14 (Feb. 2021),
27 pages. https://doi.org/10.1145/3418461
[14]Claire Le Goues, Michael Pradel, and Abhik Roychoudhury. 2019. AutomatedProgramRepair. Commun.ACM 62,12(Nov.2019),56â€“65. https://doi.org/10.
1145/3318162
[15]Brittany Johnson, Yoonki Song, Emerson Murphy-Hill, and Robert Bowdidge.2013. Why donâ€™t software developers use static analysis tools to find bugs?.In2013 35th International Conference on Software Engineering (ICSE). 672â€“681.
https://doi.org/10.1109/ICSE.2013.6606613
[16]Dongsun Kim, Jaechang Nam, Jaewoo Song, and Sunghun Kim. 2013. Automatic
PatchGenerationLearnedfromHuman-WrittenPatches.In Proceedingsofthe
2013InternationalConferenceonSoftwareEngineering (SanFrancisco,CA,USA)
(ICSE â€™13). IEEE Press, 802â€“811.
[17]S.Kirbas,E.Windels,O.McBello,K.Kells,M.Pagano,R.Szalanski,V.Nowack,E.
Winter,S.Counsell,D.Bowes,T.Hall,S.Haraldsson,andJ.Woodward.2021. On
The Introduction of Automatic Program Repair in Bloomberg. IEEE Software 38,
04 (jul 2021), 43â€“51. https://doi.org/10.1109/MS.2021.3071086
[18]XianglongKong,LingmingZhang,WEricWong,andBixinLi.2018. Theimpacts
oftechniques,programsandtestsonautomatedprogramrepair:Anempirical
study.JournalofSystemsandSoftware 137(2018),480â€“496. https://doi.org/10.
1016/j.jss.2017.06.039
[19]XuanBachDLe,FerdianThung,DavidLo,andClaireLeGoues.2018. Overfitting
insemantics-basedautomatedprogramrepair. EmpiricalSoftwareEngineering
23, 5 (2018), 3007â€“3033.
[20]Claire Le Goues, Neal Holtschulte, Edward K. Smith, Yuriy Brun, PremkumarDevanbu, Stephanie Forrest, and Westley Weimer. 2015. The ManyBugs and
IntroClass Benchmarks for Automated Repair of C Programs. IEEE TransactionsonSoftwareEngineering 41,12(2015),1236â€“1256. https://doi.org/10.1109/TSE.
2015.2454513
[21]ClaireLeGoues,ThanhVuNguyen,StephanieForrest,andWestleyWeimer.2012.
GenProg:AGenericMethodforAutomaticSoftwareRepair. IEEETransactions
onSoftwareEngineering 38,1(2012),54â€“72. https://doi.org/10.1109/TSE.2011.104
[22]John D. Lee and Katrina A. See. 2004. Trust in Automation: Designing for
Appropriate Reliance. Human Factors 46, 1 (2004), 50â€“80. https://doi.org/10.
1518/hfes.46.1.50_30392arXiv:https://doi.org/10.1518/hfes.46.1.50_30392
[23]Jingjing Liang, Ruyi Ji, Jiajun Jiang, Yiling Lou, Yingfei Xiong, and GangHuang. 2020. Interactive Patch Filtering as Debugging Aid. arXiv preprint
arXiv:2004.08746 (2020).
[24]Kui Liu, Li Li, Anil Koyuncu, Dongsun Kim, Zhe Liu, Jacques Klein, and
TegawendÃ© F BissyandÃ©. 2021. A critical review on the evaluation of automated
program repair systems. Journal of Systems and Software 171 (2021), 110817.
https://doi.org/10.1016/j.jss.2020.110817
[25]KuiLiu,ShangwenWang,AnilKoyuncu,KisubKim,TegawendÃ©F.BissyandÃ©,
DongsunKim,PengWu,JacquesKlein,XiaoguangMao,andYvesLeTraon.2020.OntheEfficiencyofTestSuiteBasedProgramRepair:ASystematicAssessmentof
16 Automated Repair Systems for Java Programs. In Proceedings of the ACM/IEEE
42nd International Conference on Software Engineering (Seoul, South Korea) (ICSE
â€™20).AssociationforComputingMachinery,NewYork,NY,USA,615â€“627. https:
//doi.org/10.1145/3377811.3380338
[26]Fan Long and Martin Rinard. 2016. Automatic Patch Generation by Learn-ing Correct Code. In Proceedings of the 43rd Annual ACM SIGPLAN-SIGACT
Symposium on Principles of Programming Languages (St. Petersburg, FL, USA)
(POPL â€™16). Association for Computing Machinery, New York, NY, USA, 298â€“312.
https://doi.org/10.1145/2837614.2837617
[27]YilingLou,SamuelBenton,DanHao,LuZhang,andLingmingZhang.2021. How
Does Regression Test Selection Affect Program Repair? An Extensive Study on 2
Million Patches. arXiv preprint arXiv:2105.07311 (2021).
[28]AlexandruMarginean,JohannesBader,SatishChandra,MarkHarman,YueJia,
KeMao,AlexanderMols,andAndrewScott.2019. SapFix:AutomatedEnd-to-
End Repair atScale. In 2019 IEEE/ACM41st International Conference onSoftware
Engineering:SoftwareEngineeringinPractice(ICSE-SEIP).269â€“278. https://doi.
org/10.1109/ICSE-SEIP.2019.00039
[29]Matias Martinez,Thomas Durieux, Romain Sommerard, Jifeng Xuan, andMartinMonperrus. 2017. Automatic repair of real bugs in java: a large-scale experiment
on the defects4j dataset. Empirical Software Engineering 22, 4 (2017), 1936â€“1964.
https://doi.org/10.1007/s10664-016-9470-4
[30]SergeyMechtaev,XiangGao,ShinHweiTan,andAbhikRoychoudhury.2018.
Test-EquivalenceAnalysisforAutomaticPatchGeneration. ACMTrans.Softw.
Eng. Methodol. 27, 4, Article 15 (Oct. 2018), 37 pages. https://doi.org/10.1145/
3241980
[31]Sergey Mechtaev, Jooyong Yi, and Abhik Roychoudhury. 2016. Angelix: Scalable
MultilineProgramPatchSynthesisviaSymbolicAnalysis.In Proceedingsofthe
38thInternational ConferenceonSoftwareEngineering (Austin,Texas) (ICSEâ€™16).
Association for Computing Machinery, New York, NY, USA, 691â€“701. https:
//doi.org/10.1145/2884781.2884807
[32]Martin Monperrus. 2018. The Living Review on Automated Program Repair. Tech-
nical Report hal-01956501. HAL/archives-ouvertes.fr.
[33]ManishMotwani,MauricioSoto,YuriyBrun,ReneJust,andClaireLeGoues.2020.
Quality of Automated Program Repair on Real-World Defects. IEEE Transactions
on Software Engineering (2020), 1. https://doi.org/10.1109/TSE.2020.2998785
[34]H.D.T. Nguyen, D. Qi, A. Roychoudhury, and S. Chandra. 2013. SemFix: ProgramRepairviaSemanticAnalysis.In InternationalConferenceonSoftwareEngineering .
[35]YannicNoller,RidwanShariffdeen,XiangGao,andAbhikRoychoudhury.2022.
Replication Package for "Trust Enhancement Issues in Program Repair". https:
//doi.org/10.5281/zenodo.5908381
[36]Chris Parnin and Alessandro Orso. 2011. Are Automated Debugging Tech-
niques Actually Helping Programmers?. In Proceedings of the 2011 International
Symposium on Software Testing and Analysis (Toronto, Ontario, Canada) (IS-
STAâ€™11).AssociationforComputingMachinery,NewYork,NY,USA,199â€“209.
https://doi.org/10.1145/2001420.2001445
[37]KarlPearson.1900. X.Onthecriterionthatagivensystemofdeviationsfrom
the probable in the case of a correlated system of variables is such that it canbe reasonably supposed to have arisen from random sampling. The London,
Edinburgh,andDublinPhilosophicalMagazineandJournalofScience 50,302(July
1900), 157â€“175. https://doi.org/10.1080/14786440009463897
[38]TylerJ.Ryan,GeneM.Alarcon,CharlesWalter,RoseGamble,SarahA.Jessup,
August Capiola, and Marc D. Pfahler. 2019. Trust in Automated Software Re-
pair.InHCIforCybersecurity,PrivacyandTrust,AbbasMoallem(Ed.).Springer
InternationalPublishing, Cham, 452â€“470.
[39]MargritSchreier.2012. Qualitativecontentanalysisinpractice. Sagepublications.
[40]Ridwan Shariffdeen, Yannic Noller, Lars Grunske, and Abhik Roychoudhury.
2021. ConcolicProgramRepair.In Proceedingsofthe42ndACMSIGPLANInterna-
tional Conference on Programming Language Design and Implementation (Virtual,
Canada)(PLDI2021) .AssociationforComputingMachinery,NewYork,NY,USA,
390â€“405. https://doi.org/10.1145/3453483.3454051
2239
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:59:42 UTC from IEEE Xplore.  Restrictions apply. Trust Enhancement Issues in Program Repair ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
[41]Ridwan Salihin Shariffdeen, Shin Hwei Tan, Mingyuan Gao, and Abhik Roy-
choudhury. 2021. Automated Patch Transplantation. ACM Trans. Softw. Eng.
Methodol. 30, 1, Article 6 (Dec. 2021), 36 pages. https://doi.org/10.1145/3412376
[42]ShinHweiTan,HiroakiYoshida,MukulRPrasad,andAbhikRoychoudhury.2016.
Anti-patterns in search-based program repair. In Proceedings of the 2016 24th
ACM SIGSOFT International Symposium on Foundations of Software Engineering.
727â€“738.
[43]Yida Tao, Jindae Kim, Sunghun Kim, and Chang Xu. 2014. Automatically Gener-
atedPatchesasDebuggingAids:AHumanStudy.In Proceedingsofthe22ndACM
SIGSOFT International Symposium on Foundations of Software Engineering (Hong
Kong, China) (FSE 2014). Association for Computing Machinery, New York, NY,
USA, 64â€“74. https://doi.org/10.1145/2635868.2635873
[44]Simon Urli, Zhongxing Yu, Lionel Seinturier, and Martin Monperrus. 2018. How
to Design a Program Repair Bot? Insights from the Repairnator Project. In 2018
IEEE/ACM 40th International Conference on Software Engineering: Software Engi-
neering in Practice Track (ICSE-SEIP). 95â€“104.
[45]Rijnard van Tonder and Claire Le Goues. 2018. Static automated program repair
forheapproperties.In Proceedingsofthe40thInternationalConferenceonSoftware
Engineering. 151â€“162.
[46]ShangwenWang,MingWen,LiqianChen,XinYi,andXiaoguangMao.2019.How
DifferentIsItBetweenMachine-GeneratedandDeveloper-ProvidedPatches?:
AnEmpiricalStudyontheCorrectPatchesGeneratedbyAutomatedProgram
Repair Techniques. In 2019 ACM/IEEE International Symposium on Empirical
SoftwareEngineeringandMeasurement(ESEM).1â€“12. https://doi.org/10.1109/
ESEM.2019.8870172[47]Westley Weimer,Stephanie Forrest, Miryung Kim, Claire Le Goues, and Patrick
Hurley.2016. TrustedSoftwareRepairforSystemResiliency.In 201646thAnnual
IEEE/IFIPInternationalConferenceonDependableSystemsandNetworksWorkshop
(DSN-W). 238â€“241. https://doi.org/10.1109/DSN-W.2016.64
[48]Ming Wen, Junjie Chen, Rongxin Wu, Dan Hao, and Shing-Chi Cheung. 2017.
An empirical analysis of the influence of fault space on search-based automated
program repair. arXiv preprint arXiv:1707.05172 (2017).
[49]Chu-Pan Wong, Priscila Santiesteban, Christian KÃ¤stner, and Claire Le Goues.
2021. VarFix:BalancingEditExpressivenessandSearchEffectivenessinAuto-
matedProgramRepair.In Proceedingsofthe29thACMJointMeetingonEuropean
SoftwareEngineeringConferenceandSymposiumontheFoundationsofSoftware
Engineering (Athens,Greece) (ESEC/FSE2021).AssociationforComputingMa-
chinery, New York, NY, USA, 354â€“366. https://doi.org/10.1145/3468264.3468600
[50]Yingfei Xiong, Xinyuan Liu, Muhan Zeng, Lu Zhang, and Gang Huang. 2018.
IdentifyingPatchCorrectnessinTest-BasedProgramRepair.In Proceedingsof
the 40th International Conference on Software Engineering (Gothenburg, Sweden)
(ICSE â€™18). Association for Computing Machinery, New York, NY, USA, 789â€“799.
https://doi.org/10.1145/3180155.3180182
[51]DehengYang,YuhuaQi,XiaoguangMao,andYanLei.2020. Evaluatingtheusageoffaultlocalizationinautomatedprogramrepair:anempiricalstudy. Frontiersof
ComputerScience 15,1(2020),151202. https://doi.org/10.1007/s11704-020-9263-1
[52]JYang,AZhikhartsev,YLiu,andLTan.2017. Bettertestcasesforbetterauto-
matedprogramrepair.In JointMeetingonFoundationsofSoftwareEngineering
(ESEC-FSE).
2240
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:59:42 UTC from IEEE Xplore.  Restrictions apply. 