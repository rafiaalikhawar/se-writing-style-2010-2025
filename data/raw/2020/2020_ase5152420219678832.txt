FIGCPS: Ef fective Failure-inducing Input
Generation for Cyber-Physical Systems with Deep
Reinforcement Learning
Shaohua Zhanga, Shuang Liua∗, Jun Sunb, Y uqi Chenb, Wenzhi Huanga, Jinyi Liua, Jian Liua, Jianye Haoa
aCollege of Intelligence and Computing, Tianjin University, Tianjin, China
bSingapore Management University, Singapore
{noisysilence, shuang.liu, wzhuang, jyliu, jianliu, jianye.hao}@tju.edu.cn, {junsun, yuqichen}@smu.edu.sg
Abstract —Cyber-Physical Systems (CPSs) are composed of
computational control logic and physical processes, which in-
tertwine with each other. CPSs are widely used in variousdomains of daily life, including those safety-critical systemsand infrastructures, such as medical monitoring, autonomousvehicles, and water treatment systems. It is thus critical toeffectively test them. However, it is not easy to obtain test caseswhich can fail the CPS. In this work, we propose a failure-inducing input generation approach FIGCPS, which requires
no knowledge of the CPS under test or any history logs of theCPS which are usually hard to obtain. Our approach adopts deepreinforcement learning techniques to interact with the CPS undertest and effectively searches for failure-inducing input guided byrewards. Our approach adaptively collects information from theCPS, which reduces the training time and is also able to exploredifferent states. Moreover, our approach is the ﬁrst attempt togenerate failure-inducing input for CPSs with both continuousaction space and high-dimensional discrete action space, whichare common for some classes of CPSs. The evaluation resultsshow that FIGCPS not only achieves a higher success rate than
the state-of-the-art approaches but also ﬁnds two new attacks ina well-tested CPS.
Index T erms—Test Case Generation, CPS, Deep Reinforcement
Learning
I. I NTRODUCTION
A Cyber-Physical System (CPS), is composed of com-
putational elements and physical processes, with different
spatial and temporal scales, modalities, and interactions [1].Nowadays, CPSs are widely used in safety-critical domainsand important urban infrastructures, such as autonomous ve-hicles, medical monitoring, water treatment system, and smartgrid [2]. As a result, those systems attract great attention fromcyber attackers, due to the potential of causing signiﬁcantdamage. There have been a large number of successful attacksreported lately [3]–[5].
The great threats encountered by CPS have stimulated
research and development on CPS testing [6], [7], attack,and defense methods [8]–[11]. One of the central problems isgenerating test suites to reveal the bugs/defects in the CPS. Thedifﬁculty is that the unsafe states are usually sparse and are dif-ﬁcult to reach, and more importantly, a sequence of input is re-quired in order to expose the failure. Existing approaches adoptlearning and feedback-guided fuzzing techniques [7], [12] toexplore potential failure-inducing inputs. Fuzzing approachesconduct mutation to obtain discrete actuator conﬁgurations,and select potential failure-inducing conﬁgurations through
*corresponding author.machine learning models trained with history log data [12]or enhanced with online observed packet information [7].These approaches are developed for CPS systems with discreteaction space, and pre-trained models are required. There isanother kind of approach that covers the test case generationproblem into an optimization problem and adopt differentoptimization strategies, e.g., simulated annealing [13], geneticalgorithms [14], gradient descent [15], cross-entropy [16],and Gaussian regression [17]–[19], to search for solutions.Yamagata et al. [20] ﬁrst propose to use Deep ReinforcementLearning (DRL) to search for failure-inducing input. Theirevaluation results with three CPS models show that the twoDRL algorithms, i.e., DDQN [21] and A3C [22], outperformexisting optimization approaches on all models, yet are worsethan the random strategy on one model. These approachestarget the CPSs with continuous action space.
Considering that CPSs have diverse action spaces, i.e., dis-
crete or continuous, and that none of the existing approachesconsider both action types. Fuzzing approaches target discreteaction space and require system logs or network packets,which are not always available (e.g., a third-party CPS systemwhich makes the logs and network packets private), to train aprediction model. The optimization approaches show unstablesuccess rates on different CPSs. In this work, we proposeto adopt the idea of deep reinforcement learning, whichtakes the CPS as environment, and trains an agent to learnsearching strategies through interacting with the environment.Taking into consideration the deﬁciencies of DDQN andA3C algorithms, we utilize a random network distillation(RND) mechanism to encourage exploring unseen states inthe searching strategy. Moreover, to tackle the challenge ofgenerating high-dimensional discrete actions, we adopt theidea of Wolpertinger [23] to map the actions in continuousspace into discrete space. In other words, we provide a uniﬁedsolution for CPS failure-inducing input generation (for bothdiscrete and continuous action space).
The primary targets of our method are those CPSs which
take either discrete or continuous inputs from the actuator/con-troller and obtain sensory readings in real-time. In our method,the system/simulator is treated as a black box, and interfaceswhich provide sensor and actuator/controller readings arerequired. Our method can also be applied to real systems,and only requires an interface which allows interacting withthe system through network communication (for injecting data
5552021 36th IEEE/ACM International Conference on Automated Software Engineering (ASE)
DOI 10.1109/ASE51524.2021.000562021 36th IEEE/ACM International Conference on Automated Software Engineering (ASE) | 978-1-6654-0337-5/21/$31.00 ©2021 IEEE | DOI: 10.1109/ASE51524.2021.9678832
978-1-6654-0337-5/21/$31.00  ©2021  IEEE
and reading system status). Note that such interfaces are often
for system diagnostic purposes.
We evaluate our approach with three CPSs, one with discrete
action space, and two with continuous action spaces. Ourexperimental results show that our approach is able to generatefailure-inducing input for both discrete and continuous actionsefﬁciently, and consistently, i.e., with a higher success rate, ascompared with existing approaches.
To summarize, we make the following contributions:
•We propose an effective approach FIGCPS to generate
failure-inducing input for CPS systems, with consider-ation of both the continuous and the high-dimensionaldiscrete input formats.
•We evaluate FIGCPS with three CPS simulators widely
adopted in related approaches. The evaluation resultsshow that our approach is effective in generating failure-inducing inputs in both continuous and high-dimensionaldiscrete input formats.
•We also compare FIGCPS with state-of-the-art input
generation approaches and the results show that ourapproach outperforms existing approaches in both successrate and the number of iterations required. We also ﬁndtwo new attacks in a well-tested CPS.
The remaining of the paper is organized as follows. First,
we present preliminaries in section II. We then introduce thedetails of our method in section III and evaluate our approachin section IV. Related works are discussed in section V andﬁnally, we conclude our method in section VI.
II. P
RELIMINARY
A. Deep Reinforcement Learning
Reinforcement learning is a machine learning method which
trains an agent through interacting with the environment,with the purpose of maximizing the long-term reward ofan observed state. The agent takes the current state fromthe environment as input, and returns the next action to theenvironment according to reward. Reinforcement learning isoften described as a Markov Decision Process (MDP), whichis a triple M=(S,A,P).Sis a set of states, Ais a set
of actions and Pis the transition probability. Prepresents the
probability distribution of (s
/prime,r)∈S×R, where s/primeis the next
state and ris the reward that the agent can get if it takes action
aover system state s. The aim of reinforcement learning is
generating an action an, based on previous states s0,...,sn−1,
rewardsr1,...,rnand actions a0,...,an−1, which maximizes
the expected value of the long-term rewards:
r=∞/summationdisplay
i=nγiri+1 (1)
where0<γ≤1is the discount factor. The reward
deﬁnition is task-speciﬁc, and it could be an integrated rewardcomposing of the external reward and the internal reward. Theexternal reward, which is usually simply referred to as reward,is deﬁned according to responses from the environment. Theinternal reward is optional. It is usually deﬁned based on thesystem state, and is meant to encourage the exploration ofthe state space. Deep reinforcement learning algorithms canbe mainly divided into three categories, i.e., value-based [24],policy-based [24], and actor-critic [24].
Value-based methods: Q-learning [24], [25] is a classical
value-based reinforcement learning algorithm. Deep Q Net-
work (DQN) [26] is a value-based method which combines Q-learning with deep neural networks. Double Deep Q Network(DDQN) [21] further improves the Deep Q Network with adual network to eliminate the over-estimation problem. DDQNis one of the standard and representative value-based DRLalgorithms. It has shown good performance on a range of tasks,yet it suffers from performance defects due to the fact that thevalue-based algorithm computes the Q-value for all actionson each step and is rather expensive for the high-dimensionaldiscrete action space or continuous action space [24].
Policy-based methods: Policy-Gradient [27] is a classical
policy-based DRL algorithm, which uses a network to approx-
imate the policy directly. It promotes the algorithm processingpower to high-dimensional discrete action space and continu-ous action space. However, it is hard to evaluate a policy, andit is likely to fall into a local optimal solution.
Actor-Critic methods: Actor-Critic [24] is a deep rein-
forcement learning framework that combines value-based and
policy-based strategies. A policy πis a probability distribution
from states in Sto actions in A. In the actor-critic framework,
there are two networks, i.e., the actor network and the criticnetwork. The actor network generates actions according topolicyπ, and the critic network estimates Q
πaccording to
the results of the run. Then, the policy πis updated with the
estimated Qπ. In the next phase, the actor will follow the new
policy. The actor-critic method repeats this process, and theaction-value function Q
πis deﬁned by
Qπ(s,a)=E/bracketleftBigg∞/summationdisplay
t=0γtRt+1|x0=x,a0=a/bracketrightBigg
(2)
whereEis the expected value. Asynchronous Advantage
Actor-Critic (A3C) [22] adopts the idea of multi-threading toaccelerate the learning process. All sub-processes run with thesame algorithm, and the learned knowledge is collected by themain process. However, the policy approximated in A3C is astochastic policy, that is hard to converge to global optimal.
Deep Deterministic Policy Gradient (DDPG) [28] is an
actor-critic, model-free deep reinforcement learning algorithmbased on the deterministic policy gradient. It extends the actor-critic approach with two innovative improvements, i.e., the ex-perience replay mechanism proposed in DQN and the doubleQ network proposed in DDQN. During the off-policy trainingof networks, the target network can minimize dependenciesbetween samples and the network, while the experience re-play mechanism can minimize correlations between data andovercome the non-stationary distribution problem of data.
There are four networks in DDPG, i.e., the estimated actor
network f
θπ, the target actor network fθπ/prime, the estimated
critic network QθQ, the target critic network QθQ/prime, with
corresponding parameters of θπ,θπ/prime,θQ,θQ/prime. Following
the standard actor-critic settings, the estimated actor networkgenerates actions according to the states of the environmentand the target actor network generates actions according to the
556critic
Enviroment
(CPSs)actor
optimizer
f  gradient
soft update
RND
optimizer
target 
network:
nsamplepredictor 
network:
ො ݊ఏෝ౤target actor 
network:݂ఏഏ’estimated actor 
network:݂ఏഏupdate  ߠగ
ܽ௧
soft update
(ݏ௧,ݐݔ݁_ݎ௧,ݏ௧ାଵ)
update  
ߠො௡MSE
ݏ௧ାଵݏ(݊௧ାଵ)ݏ௧ାଵ
݊݅_ݎ௧optimizer
estimated critic 
network:ܳఏೂ
target critic 
network:ܳఏೂ’update  ߠொQ gradient
ݕ௜gradient
݂ఏഏ’(ݏ௜ାଵ)݂ఏഏ(ݏ௜)
experience 
replay 
bufferN*(ݏ௜,ܽ௜,ݎ௜,ݏ௜ାଵ)(ݏ௧,ܽ௧,ݎ௧,ݏ௧ାଵ)2
33
51
5 58
497 7
66
6988agent
Fig. 1: The Overall Architecture of FIGCPS
states in samples from the replay buffer. The estimated critic
network and the target critic network evaluate the generatedactions together. Following the standard dual-network settings,the estimated networks are updated constantly with each batchsample and the target networks, which are used to minimizethe dependencies between samples and network parameters,are updated periodically by copying the parameters of theestimated networks in a soft-update way. DDPG has adoptedthe advantages of both the actor-critic method and DDQN. It isshown to perform well on problems with continuous or high-dimensional discrete action spaces [28], such as CPS models,compared with DDQN. Compared with A3C, DDPG learns adeterministic policy, which requires less data for training andthus converges faster.
III. O
UR APPROACH
Generating failure-inducing input for CPSs is critical. Ex-
isting approaches assume the availability of the system logdata [12] or network packets [7], which could be unavailablefor some scenarios, say testing of third-party systems, whichtake the network packets private or encrypt the networkpackets. Therefore, we take those situations into considerationand assume no extra information except for a CPS simulator.
A. Approach Overview
Fig.1 shows the overview of our approach. The steps are
annotated with circled numbers (e.g.,
1t o 9 ). We follow
the standard notations and use stto represent the state and
atto represent the action. We use extrt,inrt, andrttorepresent the external reward, the internal reward, and the
integrated reward, respectively. The structure of our methodconsists of the environment, i.e., the CPS, and the agent (boxedwith dotted line). The agent is further divided into the DDPGcomponent (in gray box) and the RND component.
B. Approach Details
Algorithm 1 shows the main idea of our method and
the circled numbers correspond to those in Fig.1. The main
process can be divided into three phases, i.e., generate action(
1 ), execute the action and compute reward ( 2-3 ), and
train model ( 4-9 ). Our approach proposes different agents
to handle different types of action spaces. The base model ofour approach is a DDPG [28] agent, which utilizes the actor-critic framework, where the actor network adopts a policy-based strategy to generate actions, and the critic networkadopts a value-based strategy to criticize the actor’s decisions.There are two actor networks, i.e., the estimated actor networkf
θπand the target actor network fθπ/prime, and two critic networks,
i.e., the estimated critic network QθQand the target critic
networkQθQ/prime. The estimated actor network fθπworks out an
action according to the given state, then the estimated criticnetworkQ
θQevaluates the action produced by fθπ. These two
networks train their parameters θπandθQusing the evaluation
ofQθQ. The target actor network fθπ/primeand the target critic
networkQθQ/primeupdate their parameters θπ/primeandθQ/primewithθπ
andθQin regular intervals.
We ﬁrst randomly initialize the parameters for the esti-
mated actor network fθπand estimated critic network QθQ
557Algorithm 1: Failure-inducing Input Generation
Input: CPS simulator M, episode limit L, step limit T
Output: A sequence of actions
1Randomly initialize QθQandfθπwith weights θQandθπ.
2Initialize QθQ/primeandfθπ/primewith weights θQ/prime←θQ,θπ/prime←θπ.
3Randomly initialize nandˆnwith weight θˆn.
4Initialize g/prime
ksdictionary of actions with elements of A.
5Initialize B,inw,extw,BS,Discrete andRandom.
6forepisode=1,Ldo
7 Randomly reset the state of CPS simulator M
8 fort=1,Tdo
9 Observe state stfrom M
10 1 ifDiscrete then
11 ˆa=fθπ(st)
12 Ak=gk(ˆa)
13 at= arg max
aj∈AkQθQ(st,aj)
14 else
15 at=fθπ(st)
16 2(extrt,st+1,end)←M(at)
17 if(st+1is unsafe) then
18 return allatinB
19 3 ifRandom then
20 inrt=||ˆn(st+1;θ)−n(st+1)||2
21 rt=inw∗inrt+extw∗extrt
22 else
23 rt=extrt
24 4 Add transition (st,at,rt,st+1)toB
25 if(|B|>BS)then
26 5 Sample N*( si,ai,ri,si+1)fromB
27 6yi=/braceleftbiggri end
ri+γ∗QθQ/prime/parenleftbig
si+1,fθπ/prime(si+1)/parenrightbig
28 7L/parenleftBig
θQ/parenrightBig
=1
N/summationdisplay
i[yi−QθQ(si,ai)]2
298∇θπJ≈
1
N/summationdisplay
i∇aQθQ(s,a)|a=fθπ(si)·∇θπfθπ(s)|si
309θQ/prime←τθQ+(1−τ)θQ/prime
θπ/prime←τθπ+(1−τ)θπ/prime
31 Updateˆnby minimizing inri
32end for
(line 1). Then the target actor network fθπ/primeand the target
critic network QθQ/primeare initialized with the parameters of the
corresponding estimated networks (line 2). The weights for
the RND networks are also randomly initialized (line 3). Themap of k-nearest-neighbor (KNN) is initialized in line 4. Allthe other parameters are initialized in line 5. Bis the replay
buffer, and BS is a limit, when the size of Bis larger than
BS, the training of networks starts. The in
wandextware
the coefﬁcients of internal reward inrtand external reward
extrt, respectively. Discrete andRandom switch on the
action space mapping and the RND process, respectively.
Line 7 to 31 is the process of one episode in our algorithm.
In the following, we describe the detailed algorithm accordingto the three phases, i.e., generate action (
1 ), execute the
action and compute reward ( 2-3 ), and train model ( 4-9) .
(1) Generate Action After the state stis obtained, the action
Fig. 2: Generate High-dimensional Discrete Actions
atis generated. Our approach considers the situations of
both the continuous actions and the high-dimensional discreteactions. For CPS taking continuous actions, e.g., A T and PTC,our method uses standard DDPG, which uses the estimatedactor network to generate action based on the obtained state(line 15). For the CPS system which takes high-dimensionaldiscrete input values, e.g., SW A T, with 26-dimensional discreteactions as input, our approach maps the continuous values to adiscrete space. As shown in Fig.2, the estimated actor networkf
θπgenerates a proactionˆa∈Rn(line 11), which is a
continuous action and is likely not a valid action, i.e., ˆa/∈A.
For instance, the vector [-0.0028, 0.0509, 0.0170, -0.0277,0.0020, 0.00668, 0.0161, 0.0241, 0.0304, 0.0044, -0.0160,0.1037, -0.0170, 0.0226, -0.0901, -0.0469, 0.0083, 0.0265, -0.0126, 0.03480, -0.0336, -0.0153, -0.01331, -0.0406, 0.0120,-0.0430] is an example pro
action generated for SW A T, in
which n is 26 and each bit corresponds to an actuator in SW A T.
Then we adopt gk:Rn→A, a KNN method that maps
from a continuous space (Rn) to a discrete set A(line 12).
The mapping function gkis deﬁned in (3).
gk(ˆa)=k
argmin
a∈A|a−ˆa|2 (3)
where a ∈A is a discrete action in the set A. The dashed
circle in Fig.2 illustrates this mapping process. It returns k(e.g., k is 6,710 for SW A T) actions A
kinAthat are closest to
ˆabyL2distance. However, due to the differences in the action
representation, actions with a low Q-value may occasionallysit closest to ˆaeven in an area where most actions have a
high Q-value (e.g., a
2in Fig.2). Moreover, some actions may
be close to each other in the action embedding space, butin certain states, they must be distinguished as one has aparticular low long-term value relative to its neighbors (e.g.,a
1in Fig.2 presents such actions).
To prevent picking actions like a1,a2, and to improve the
accuracy of the selected action, the ﬁnal part of the algorithmreﬁnes the choice of action by selecting the highest-scoringaction according to Q
θQ. The returned k actions have their Q
values. We reﬁne the choice of action by selecting the highest-scoring action according to Q
θQ(the last step in Fig.2):
at=πθ(s)=a r gm a x
a∈AkQθQ(s,a) (4)
After reﬁnement of action, we can get the ﬁnal action
chosen by the algorithm. For instance, the action [0, 0, 0,0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]is chosen corresponding to the pro
action generated.
(2) Execute Action and Compute Reward After generated,
the action is fed to the environment, i.e., the CPS, as input, and
558the environment returns the external reward (ext r), next state
(st+1), and a signal of the end state (line 16). The external
reward is used together with the internal reward (in r)t o
compute the reward for training. The extrfor the CPSs is
deﬁned on a per-model basis.
For SW A T, we deﬁne reward on a per-sensor basis. For the
case of underﬂow, the external reward is deﬁned as follows:
extr=⎧
⎪⎪⎪⎨
⎪⎪⎪⎩100
di+1−ds,Ls≤si+1<si≤Hs
−100
di+1−ds,Ls≤si≤si+1≤Hs
1 ,otherwise(5)
wherediis the distance of the sensor reading from its safety
threshold and dsis the distance of a sensor reading between
two steps. For the case of underﬂow attacks, the distances are
computed with di=si+1−Ls,ds=si+1−si. For the case
of overﬂow, the external reward is deﬁned as follows:
extr=⎧
⎪⎪⎪⎨
⎪⎪⎪⎩100
di+1−ds,Ls≤si≤si+1≤Hs
−100
di+1−ds,Ls≤si+1<si≤Hs
1 ,otherwise(6)
The distances are computed with calculated as follows: di=
Hs−si+1,ds=si−si+1.si+1 denotes the next reading
of a sensor, sidenotes the current reading of the sensor, Ls
denotes its lower safety threshold, Hsdenotes its upper safety
threshold. Considering that diis usually larger in order of
magnitude than ds, we use 100 as a factor to balance the
effects of diandds.
For the A T and PTC, we adopt the reward deﬁnition of
Yamagata et al. [20], which deﬁnes the unsafe state of a
system based on future reach safety properties. The unsafestate is considered to be reached whenever the safety propertyis violated. The external reward is deﬁned as follows.
ext
r(ϕ, s,i)=e x p (−[ϕ](s,i))−1 (7)
where sis a ﬁnite sequence of states, and ϕis a safety property.
One challenge is that the unsafe states in CPS are usually
sparse and difﬁcult to explore. To increase the probability ofexploring those states, we exploit the Random Network Dis-tillation (RND) [29] mechanism, which applies an explorationbonus to deep reinforcement learning methods to encouragethe exploration of the unseen part of the environment. RNDinvolves two neural networks: a ﬁxed and randomly initializedtarget network, and a predictor network. The target network n
maps observed next state s
t+1∈Sto a real number: S→R
and the predictor neural network ˆn:S→Ris trained with
gradient descent to minimize the prediction error betweennandˆnwith respect to its parameters θ
ˆn. The prediction
error can indicate the novelty of the explored states, andhigher errors indicate novel states, with the assumption thatthe network is likely to provide accurate predictions for thosestates that are explored before. We calculate the predictionerror with the expected mean square error (MSE) [30], andencourage our method to explore more unseen states by max-imizing MSE. MSE is widely adopted as the loss function forregression problems. Compared to other loss functions used inthe regression problems, MSE is more sensitive to anomalousdata and thus we choose MSE to encourage exploring unseenstates. Hence, the in
ris computed as follow:
inr=||ˆn(st+1;θ)−n(st+1)||2(8)
Finally, we set coefﬁcient for extrandinrrespectively,
and the reward can be computed as follow:
r=inr∗inw+extr∗extw (9)
(3) Train Model The process of training model is corre-
sponding to steps 5-9 in Algorithm 1. All networks are
trained with samples from replay buffer (line 26). The targetQ-value is computed by the target critic network and targetactor network (line 27). The estimated critic network updatesits parameter with loss function (line 28). Next, the policygradient of samples is calculated to train the estimated actornetwork (line 29). As for target actor network and targetcritic network, they update their parameters by copying fromestimated networks periodically in a soft-update way (line 30).Finally, in line 31, the predictor neural network for RND istrained by minimizing the MSE.
IV . E
V ALUA TION
A. Implementation
Our method is implemented in Python 3.5 and with Ten-
sorﬂow 1.8.0. Because it is difﬁcult to access real CPSsystems, we choose to use the simulators instead. The SW A Tsimulator [31] is implemented in Python 3, and the A T [32]and PTC [33] simulators are implemented in Matlab. Theproject is publicly accessible
1.
B. Research Questions
For evaluation, we aim at the following research questions:
•RQ1: Is FIGCPS ef fective in generating failure-inducing
input for both continuous actions and high-dimensionaldiscrete actions?
•RQ2: Can FIGCPS ef ﬁciently explore all unsafe states?
•RQ3: Does the RND mechanism improve the perfor-mance of exploring unsafe states?
•RQ4: How does FIGCPS perform compared with state-
of-the-art approaches?
The ﬁrst research question measures the generality of our
approach, i.e., whether our approach is able to effectivelywork for both continuous and high-dimensional discrete actionspaces. The second research question evaluates how manydifferent unsafe states can our approach detect stably andquickly. The third research question measures the effectivenessof the RND mechanism. The last research question comparesour approach against state-of-the-art approaches to measurethe effectiveness of our approach horizontally.
1https://github.com/noisysilence-max/Failure-Inducing-Input-Generation
559Fig. 3: The A T simulator [32]
TABLE I: The evaluated properties for A T [20]
id Formula
ϕ1 /squareω≤¯ω
ϕ2 /square(v≤¯v∧ω≤¯ω)
ϕ3 /square/parenleftbig/parenleftbig
g2∧/diamondmath[0,0.1]g1/parenrightbig
→/square[0.1, 1.0]¬g 2/parenrightbig
ϕ4 /square/parenleftbig/parenleftbig
¬g1∧/diamondmath[0,0.1]g1/parenrightbig
→/square[0.1,1.0]g1/parenrightbig
ϕ5/square4/logicalanddisplay
i=1/parenleftbig/parenleftbig
¬gi∧/diamondmath[0,0.1]gi/parenrightbig
→/square[0.1,1.0]gi/parenrightbig
ϕ6 /square/parenleftbig
/square[0,t1]ω≤¯ω→/square[t1,t2]v≤¯v/parenrightbig
ϕ7 /squarev≤¯v
ϕ8 /square/diamondmath[0,25]¬(v≤v≤¯v)
ϕ9 /square¬/square[0,20] (¬g 4∧ω≥¯ω)
ωis the Engine RPM, g is the Gear, and v is the V ehicle Speed.
C. Experiment Settings
We conduct our experiments on a desktop with a 6 core,
2.9 GHz Inter CPU and 128G bytes RAM. Windows 10 and
Matlab version 2018b are used. We use three representativeCPS simulators for evaluation, which are brieﬂy introduced.
The Secure Water Treatment (SW AT) [31] is a testbed
located in Singapore University of Technology and Design,
and is built for cyber-security research purposes. SW A T isa scaled-down and fully operational raw water puriﬁcationplant, capable of producing ﬁve gallons of safe drinking waterper minute. The cyber part of SW A T covers ProgrammableLogic Controllers (PLCs), a layered communications network,Human-Machine Interfaces (HMIs), a Supervisory Controland Data Acquisition (SCADA) workstation, and a historian,while data from sensors is available to the SCADA systemand recorded by the historian. We use a simulator of SW A Timplemented in Python. The simulator has 3 Level IndicatorTransmitter (LIT) sensors, i.e., LIT101, LIT301, LIT401,while LIT101 has a reading between 500 and 800, LIT301and LIT401 have a reading between 800 and 1000. There are26 actuators, which are used to control the simulator and eachof them can be set to either 0 or 1. The output of the simulatoris a 5-dimensional vector, which consists of the readings ofthree sensors and two timestamps. The input of the simulatoris a 26-bit vector, and each bit corresponds to an actuator.
The Automatic Transmission Controller (AT) [32] is a simu-
lator supported by Mathworks as a Matlab/Simulink example.
A T is a simulator of an automotive driving train, and Fig.3shows its architecture. There are four parts in the simulator,i.e., the Engine, the Transmission, the V ehicle, and the Shift-Logic. The inputs of the simulator are the readings of thethrottle and brake which are continuous values, and the outputs
Fig. 4: The PTC simulator [33]
TABLE II: The evaluated properties for PTC [20]
id Formula
ϕ26 /square[11,50]|μ|≤ 0.2
ϕ27 /square[11,50](rise∨fall=⇒/square[1,5]|μ|≤ 0.15)
ϕ30 /square[11,50]μ≥− 0.2
ϕ31 /square[11,50]μ≤0.2
ϕ32 /square[11,50](power∧/diamondmath[0,0.1]normal =⇒/square[1,5]|μ|≤ 0.2)
ϕ33 /square[11,50] (power =⇒− 0.3≤μ≤0.2)
ϕ34 /square[0,50]((rise∨fall)=⇒/square[1,5]−0.3≤μ≤0.2)
μis veriﬁcation measurement while rise, fall, power, and normal represent
the operational mode.
contain three values, i.e., the Engine RPM (Revolutions PerMinute), the Gear, and the V ehicle Speed. The type of theEngine RPM and the V ehicle Speed are real values and aregreater than or equal to 0, and the Gear is a vector of fourdiscrete values g
1,g2,g3,g4. More detailed information on A T
is available on the ofﬁcial website [32].
The Power Train Controller (PTC) [33] is a simulator
provided by Toyota Technical Center, which simulates a
controller of air-fuel (A/F) ratio for an internal combustionengine. The architecture of PTC is shown in Fig.4. There aretwo components, i.e., the Fuel Control System (FCS) and theV eriﬁcation and V alidation Stub System (VVSS). FCS takesthe pedal angle and the engine speed as input, computes thetarget A/F ratio (i.e., the A/F ref in Fig.4), and then adjuststhe physical components to get the target ratio. FCS alsosimulates the dynamics of the engine to get the real A/F ratio(i.e., the A/F in Fig.4). Then, VVSS calculates the controlerror according to the A/F and A/F ref readings and outputsthe veriﬁcation measurements. The other output of PTC isthe operational mode, which represents the different workingstates of the controller. PTC has 4 modes, i.e., startup, normal,power and fault. In a few seconds after beginning, PTC is inthe startup mode. When the pedal angle stays in (8.8
◦,70◦),
PTC goes into the normal mode and holds in that mode. PTCwill go into the power mode, when the pedal angle surpasses70
◦. When a sensor fails, PTC will go into the fault mode.
The SW A T system is chosen for the reason that it has an
action space of 226, and is a representative case for high-
dimensional discrete action space. A T and PTC are chosen asrepresentatives of continuous action spaces due to their wideadoption in the literature. For the SW A T simulator, the unsafestates are decided by the valuation of the sensor readings. Forthe A T and PTC simulators, the unsafe states are deﬁned bythe future reach safety properties in [20], which are shown in
560TABLE III: The success rate (%) and median time (s) taken to generate failure-inducing inputs for SW A T with different sampling
intervals
Method interval (S)LIT Overﬂow LIT Underﬂow
101 301 401 101 301 401
success rate time success rate time success rate time success rate time success rate time success rate time
FIGCPS D1 50 2,122 40 4,947 100 576.5 100 2,107 100 601.5 0 +∞
2 80 2,925 90 4,026 100 607.5 100 1,208 100 649 0 +∞
3 100 3,277.5 100 4,149 100 458 100 904.5 100 463.5 0 +∞
FIGCPS D+RND1 60 3,793.5 30 4,244 100 448.5 100 603 100 408.5 0+∞
2 90 4,454 90 4,182 100 606 100 898 100 576 0 +∞
3 100 2,670 100 3,582 100 544.5 100 1,023 100 427.5 10 6,012
The best results are highlighted in bold.
TABLE IV: The success rate (%) and median number of episodes of generating failure-inducing inputs for the A T model
PropertyRAND DDQN A3C FIGCPS C FIGCPS C+RND
success rate number success rate number success rate number success rate number success rate number
ϕ1 0- 100 28.5 91 31 96 14 84 15.5
ϕ2 0- 100 29 96 30 95 15 90 17
ϕ3 100 18.5 45 55 78 9.5 100 4.5 100 4
ϕ4 100 18.5 43 82 80 7.5 100 4 100 4
ϕ5 100 1 100 2 100 1 100 1 100 1
ϕ6 0- 100 8 91 5 100 2 100 2
ϕ7 0- 43 110 32 42 0- 0-
ϕ8 100 12 95 56 51 87 98 25 67 122
ϕ9 0- 74 27 56 21 97 18 54 71.5
AV G 44.4 - 77.8 - 75 - 87.3 - 77.2 -
The best results are highlighted in bold. - means there is no success runs to generate the failure-inducing input or the average of the values is not
valid. A VG is the mean number of success rates of different methods. RAND is the random strategy.
TABLE V: The success rate (%) and median number of episodes of generating failure-inducing inputs for the PTC model
PropertyRAND DDQN A3C FIGCPS C FIGCPS C+RND
success rate number success rate number success rate number success rate number success rate number
ϕ26 100 7 54 65.5 95 8 100 5 100 8
ϕ27 81 66 54 4 79 67 98 30 93 32
ϕ30 99 23 15 80 88 25.5 100 11 96 10.5
ϕ31 100 4 98 25 100 2 100 4 100 5.5
ϕ32 28 71 1 103 20 113.5 48 105.5 56 80.5
ϕ33 100 5 43 36 84 5.5 84 4 100 7
ϕ34 57 88 4 101.5 20 100 52 75 23 80
AV G 80.7 - 31.4 - 69.4 - 85.4 - 81.1 -
The best results are highlighted in bold. - means there is no success runs to generate the failure-inducing input or the average of the values is notvalid. A VG is the mean number of success rates of different methods.
TABLE VI: The median number of time (s) taken to
generate failure-inducing input for SW A T compared withSmart Fuzz [12]
Method Train (H)LIT Overﬂow LIT Underﬂow
101 301 401 101 301 401
FIGCPS D 0 3,277.5 4,149 458 904.5 463.5 +∞
FIGCPS D+RND 0 2,670 3,582 448.5 603 408.5 6,012*
LSTM-GA 48 333 496 729 511+∞+∞
LSTM-Rand 48 339 618 697 526+∞+∞
SVR-GA 12 385 439 696 569+∞+∞
SVR-Rand 12 337 577 681 655+∞+∞
Train is the time of training prediction models (hours). The values in the last
6 columns are the median time (seconds) used to generate the failure-inducinginput, the best results are highlighted in bold. +∞ represents failing to reach
the unsafe state in the given time limit. The numbers marked with a star meanfailing to generate the failure-inducing input for some experiment runs. GA isthe genetic algorithm, and Rand represents the random strategy.
Table I and Table II, respectively. An unsafe state is found if
the property is evaluated to be false in that state. We followthe reward deﬁnition proposed by Yamagata et al. [20] in ourapproach. For SW A T, we use FIGCPS
D, which is the DDPG
algorithm combined with the mechanism of mapping fromcontinuous action space to discrete action space, as the searchalgorithm to generate input. We further enhance FIGCPS
Dwith the RND [29] setting, which increases the randomnessof searching and name the algorithm FIGCPS
D+RND. We
set the exploration rate, learning rate, and coefﬁcient ofinternal reward to 0.3, 0.0001, 0.00001, respectively. For eachof the unsafe states, e.g., LIT101 underﬂow, we repeat theexperiment 10 times to eliminate the effect of randomness. Ineach experiment, we set the maximum number of iterations tobe 10 episodes. The sampling intervals are set to accumulatesensor reading changes. We set different sampling intervals,i.e., 1s, 2s, and 3s, to observe the effect of time intervalson experimental results. For A T and PTC, we use FIGCPS
C,
which is the original DDPG algorithm to generate input forthe other two simulators. Similarly, FIGCPS
C+RND refers
to FIGCPS Ccombined with the RND mechanism. For A T,
we set the exploration rate, learning rate, and coefﬁcientof the internal reward as 0.3, 0.0001, and 0 (FIGCPS
C)
or 25 (FIGCPS C+RND), respectively. For PTC, we set the
exploration rate, learning rate, and coefﬁcient of the internalreward as 0.7, 0.0001, 0 for FIGCPS
C, and 0.3, 0.0001, 25 for
FIGCPS C+RND, respectively. Those hyper-parameters are set
based on experiment experiences. For each property, we repeat
561Fig. 5: Sensor Readings of LIT 101 with FIGCPS D+RND (3s
interval)
the experiment 100 times, each of which has the maximum
limitation of 200 episodes.
D. Experiment Result
RQ1 and RQ2: Table III shows the success rate and the
execution time used for FIGCPS Dand FIGCPS D+RND to
reach the unsafe state with different sampling intervals, i.e.,
1s, 2s, 3s, on SW A T. To eliminate the effect of randomnesscaused by the starting position, we run our approach 10 timesfor each targeted unsafe state and report the median number.+∞indicates that despite approaching the given unsafe state,
none of the repetitions were able to cross the threshold. We canobserve that the sampling interval of 3 seconds shows the moststable performance. LIT401 overﬂow, LIT101 underﬂow, andLIT301 underﬂow have a 100% success rate for all samplingintervals. For LIT101 overﬂow, LIT 301 overﬂow, and LIT 401underﬂow, a larger sampling interval shows a higher successrate. The reason is that the readings of LIT101, LIT301, andLIT401 change slowly towards the target direction, and thuslarge time intervals accumulate more changes which betterguide the reward. Fig.5 shows the sensor reading of LIT101with the FIGCPS
D+RND strategy. The blue line shows the
sensor readings while searching for the overﬂow state andthe green line shows the sensor readings for the underﬂowstate. We can observe that the readings of the LIT101 godown initially. Our approach learns the underﬂow strategy afteraround 400 steps, and learns the overﬂow strategy after over600 steps. The trend is similar for LIT301.
In general, our approach is effective in generating failure-
inducing input for CPS with high-dimensional discrete actions.With a sampling interval of 3s, our approach is able to exploreall unsafe states with a 100% success rate, except for the LIT401 underﬂow state. In terms of execution time, our approachcan ﬁnd three unsafe states in a few minutes, and the othertwo within one hour (including training time).
Table IV shows the success rate and the median numbers of
episodes of ﬁnding attacks for A T within 100 simulations foreach formula and method with the sampling interval of ΔTFig. 6: Sensor Readings of LIT 301 Overﬂow withFIGCPS
D+RND and FIGCPS D(3s interval)
= 1. Table V shows the success rate and the median numbersof episodes of generation for PTC within 100 simulations foreach property and method with the sampling interval of ΔT=
5. We use - to indicate no value available. The best resultsfor each property are highlighted in bold. From Table IVand Table V, we can observe that FIGCPS
Ccan achieve the
highest average success rate for both A T and PTC. FIGCPS C
and FIGCPS C+RND also have the smallest median number
of episodes for the majority of the properties of A T and PTC.
[Answer to RQ1:] Our approach is effective in generating
failure-inducing input for CPS with both continuous and high-
dimensional discrete action space. [Answer to RQ2:] Our
approach can also efﬁciently, as indicated by the time andnumber of episodes, cover failure states, as indicated by thehigh success rate.
RQ3: From Table III, we can see that FIGCPS
D+RND out-
performs FIGCPS Dwith less time for most cases, and can
achieve a higher success rate than FIGCPS D, indicating that
the RND mechanism can actually improve the performance of
exploring unsafe states.
We further plot the sensor readings of LIT301 over-
ﬂow in the successful episode, shown in Fig.6, as an ex-ample to illustrate the differences between FIGCPS
Dand
FIGCPS D+RND. The blue line shows the sensor readings
with FIGCPS Dand the green line shows the sensor readings
with FIGCPS D+RND. We can observe that FIGCPS D+RND
learns the strategy faster than FIGCPS Dwith fewer steps
required, even with a disadvantaged starting point. We checkall 10 runs on the LIT301 overﬂow and ﬁnd that in 8 runs,FIGCPS
D+RND reach the unsafe state in less than 2 episodes,
and in 80% of the cases, FIGCPS D+RND is faster than
FIGCPS Dwith fewer episodes and steps.
However, Table IV and Table V show that FIGCPS C
performs a little bit better than FIGCPS C+RND. The reason
for the relative worse performance of FIGCPS C+RND on
A T and PTC is that there are discrete variables in the statesof A T (i.e., the Gear) and PTC (i.e., the Mode), while thechange of these variables will lead to a big increase of internal
562TABLE VII: The relative effect size measure and p-value
of A T
Property MethodRAND DDQN A3C
p p-value p p-value p p-value
ϕ1FIGCPS C 0.060 0 0.477 0.599 0.409 0.026
FIGCPS C+RND 0.080 0 0.443 0.189 0.392 0.009
ϕ2FIGCPS C 0.065 0 0.431 0.109 0.384 0.005
FIGCPS C+RND 0.050 0 0.460 0.350 0.404 0.021
ϕ3FIGCPS C 0.232 0 0.072 0 0.254 0
FIGCPS C+RND 0.224 0 0.073 0 0.241 0
ϕ4FIGCPS C 0.256 0 0.071 0 0.317 0
FIGCPS C+RND 0.249 0 0.049 0 0.320 0
ϕ5FIGCPS C 0.518 0.504 0.229 0 0.489 0.703
FIGCPS C+RND 0.581 0.006 0.285 0 0.557 0.068
ϕ6FIGCPS C 0.001 0 0.301 0 0.317 0
FIGCPS C+RND 0.001 0 0.328 0 0.354 0
ϕ7FIGCPS C 0.500 1 0.715 0 0.660 0
FIGCPS C+RND 0.500 1 0.715 0 0.660 0
ϕ8FIGCPS C 0.74 0 0.288 0.856 0.163 0
FIGCPS C+RND 0.980 0 0.849 0 0.466 0.401
ϕ9FIGCPS C 0.015 0 0.307 0 0.360 0.001
FIGCPS C+RND 0.230 0 0.699 0 0.603 0
pis the relative effect size measure, and the value less than 0.5
(highlighted in bold blue) means the methods reported in rows are
better than that reported in columns (e.g., DDPG is better thanRAND on 6 properties, better than DDQN/A3C on 8 properties);p-value measures the signiﬁcance of p, and the value less than 0.05
(highlighted in bold) means there is a signiﬁcant difference betweenthe two groups of data (e.g., DDPG is signiﬁcantly better than RAND,DDQN, A3C on 6, 5, and 7 properties).
rewardinr. For such variables, it takes a long time to
generate the appropriate input. According to [29], the RND
mechanism is sufﬁcient to deal with local exploration, i.e.,exploring the consequences of short-term decisions, whileglobal exploration that involves coordinated decisions overlong time horizons is beyond the reach of RND. Hence, RNDwill be counterproductive for some properties that need thesystem to keep the discrete variable constant over a long periodof time, e.g., ϕ
34for PTC.
[Answer to RQ3:] RND generally improves the exploration
performance, with exceptions on speciﬁc unsafe states thatrequire stable status over long time horizons.
RQ4: Table VI reports the results on SW A T compared with
the state-of-the-art approach Smart Fuzz [12]. +∞indicates
that despite approaching the given unsafe state, none of the
repetitions were able to cross the threshold. We can see thatFIGCPS
D+RND achieves compatible results on the LIT101
underﬂow attack, and better results on LIT401 overﬂow,LIT301 underﬂow attacks. Speciﬁcally, our method is ableto ﬁnd the attack of LIT301 underﬂow (with 100% successrate) and LIT401 underﬂow, which Smart Fuzz fails to ﬁnd inthe given time limit. These unsafe states have been reportedin an established benchmark of SW A T network attacks [34]which was manually crafted by experts. Our method performsa bit worse on the LIT101 overﬂow and LIT301 overﬂowattacks. The reason is that for LIT101 and LIT301, thereis a higher probability of selecting actuator conﬁgurationswhich drive the sensor readings to sink as illustrated by Fig.5.Therefore, our method needs more time to learn the strategyto drive the sensor readings to rise on the ﬂy, while SmartFuzz [12] leverages a trained model for prediction, and itsinput generation process does not involve training.
Note that Smart Fuzz [12] trains prediction models, e.g.,
LSTM and SVR, with 4-days data logs collected. The trainingTABLE VIII: The relative effect size measure and p-valueof PTC
Property MethodRAND DDQN A3C
p p-value p p-value p p-value
ϕ26FIGCPS C 0.418 0.042 0.016 0 0.380 0.003
FIGCPS C+RND 0.522 0.599 0.049 0 0.463 0.365
ϕ27FIGCPS C 0.253 0 0.024 0 0.276 0
FIGCPS C+RND 0.309 0 0.051 0 0.328 0
ϕ30FIGCPS C 0.304 0 0.004 0 0.297 0
FIGCPS C+RND 0.372 0.002 0.037 0 0.355 0
ϕ31FIGCPS C 0.405 0.018 0.098 0 0.382 0.003
FIGCPS C+RND 0.584 0.100 0.168 0 0.528 0.492
ϕ32FIGCPS C 0.406 0.008 0.265 0 0.356 0
FIGCPS C+RND 0.365 0 0.224 0 0.313 0
ϕ33FIGCPS C 0.488 0.203 0.047 0 0.320 0
FIGCPS C+RND 0.568 0.102 0.077 0 0.392 0.008
ϕ34FIGCPS C 0.511 0.775 0.259 0 0.331 0
FIGCPS C+RND 0.661 0 0.409 0 0.484 0.575
pis the relative effect size measure, and the value less than 0.5
(highlighted in bold blue) means the methods reported in rows are
better than that reported in columns (e.g., DDPG is better than RANDon 6 properties, better than DDQN/A3C on 7 properties); p-value
measures the signiﬁcance of p, and value less than 0.05 (highlighted
in bold) means there is a signiﬁcant difference between the twogroups of data (e.g., DDPG is signiﬁcantly better than RAND andDDQN/A3C on 5 and 7 properties).
takes 2 days for LSTM and half a day for SVR. Our method
learns the strategy on the ﬂy and does not require the pre-training effort, i.e., the training process is integrated with thesearching process. Therefore, the time for our method includesthe training time and the generation time, and that’s why ourmethod needs more time for LIT101 overﬂow and LIT301overﬂow attacks. We also add the training time of SmartFuzz [12] for comparison.
LIT401 underﬂow is the most difﬁcult case, which Smart
Fuzz [12] fails to ﬁnd, and our method only achieves a successrate of 10%. Through a detailed analysis, we ﬁnd that thereare only a very small number of actuator conﬁgurations whichcan make the reading of LIT401 decrease. Moreover, thereading of LIT401 decreases with a small amplitude (1/10 ofthe amount of increase) in each step. Therefore, Smart Fuzzcan hardly come across the inputs making the readings ofLIT401 decrease, due to its low probability, which makes thecorresponding ﬁtness value negligible. As for our approach,the FIGCPS
D+RND strategy shows its advantages in ﬁnding
the unsafe state, due to the RND network, which guidesthe searching towards unseen states, and thus increases theprobability of exploring the rare underﬂow readings.
We also reproduce the results of the state-of-the-art ap-
proach [20] as a comparison. The distribution of the numberof episodes is highly non-normal, so we choose to reportthe median number, rather than the average number, in ourevaluation. From Table IV, we can observe that our methodis better than the other approaches on most properties on boththe success rate and the median number of episodes for A T.
There is an exception that our method has failed for ϕ
7.W e
conduct further experiments for ϕ7withΔT = 5, 10, and the
success rate of our method can get 98% and 99%, respectively.ϕ
7is a property that only constraints that the vehicle speed
does not exceed the limit, while the vehicle speed has verylittle variation in a short period of time, which makes thereward change very small. Besides, the large sample rate willincrease the complexity of the state, so that the difﬁculty of the
563search will increase, correspondingly. As DDQN outperforms
DDPG in some simple cases [35], our method fails for ϕ7
withΔT = 1, while DDQN can succeed with a lower success
rate. As for A3C, the asynchronous mechanism can improveits data utilization, so that it can also succeed with ΔT=1 .
Table V shows the results of the compared method [20] for
PTC. We can observe that our method has better performancefor PTC than the compared methods and FIGCPS
Cperforms
the best for most properties. The RND network shows perfor-mance gain on some properties, i.e., ϕ
32.
To eliminate the effect of randomness in our experiment
results, and examine the signiﬁcance of the results, we furtherconduct statistical testing on the reported results.
We adopt the relative effect size measure [36] to test the data
of different methods. The relative effect size measure betweentwo random values X, Y is deﬁned as
p=P(X<Y)+1
2P(X=Y) (10)
Ifp< 0.5, it indicates that X is probably larger than Y and
vice versa. phas a close relationship to Cliff’s delta [37],
which is an effect size measurement to verify the differencebetween the two sets of data. The relative effect size measureis ordinal statistics which is calculated according to the relativeorder of different data sets, which does not require the normaldistribution assumption on the data under test [37] and issuitable for our problem.
We further conduct hypothesis testing to test the signiﬁ-
cance [36] of the effect size measurement. We set the signiﬁ-cance level as 0.05 and perform multiple comparisons betweenour methods and approaches in [20].
Table VII and Table VIII show the relative effect size
measure ptogether with the signiﬁcance level p-value of
hypothesis testing for the A T and PTC, respectively. pis
the relative effect size measure, and the value less than 0.5(highlighted in bold blue) means the methods reported in rowsare better than those reported in columns. For example, inTable VII, FIGCPS
Cis better than RAND on 6 properties
(ϕ1,ϕ2,ϕ3,ϕ4,ϕ6, andϕ9), better than DDQN/A3C on 8
properties (all except ϕ7). p-value measures the signiﬁcance
ofp, and the value less than 0.05 (highlighted in bold) means
there is a signiﬁcant difference between the two groups of data.For instance, in Table VII, FIGCPS
Cis signiﬁcantly faster than
A3C on 7 properties (ϕ1, ϕ2,ϕ3,ϕ4,ϕ6,ϕ8, andϕ9).
In Table VII, we can observe that FIGCPS Cis signiﬁ-
cantly faster than RAND, DDQN, and A3C on 6, 5, and7 properties, respectively. FIGCPS
C+RND is signiﬁcantly
faster than RAND, DDQN, A3C on 6, 4, and 5 properties.Table VIII shows that for PTC, FIGCPS
Cis signiﬁcantly
faster than RAND, DDQN, A3C on 5, 7, and 7 properties;FIGCPS
C+RND is signiﬁcantly faster than RAND, DDQN,
A3C on 3, 7, and 4 properties.
[Answer to RQ4:] Our approach outperforms state-of-the-art
approaches for both continuous and discrete actions. Particu-
larly, We ﬁnd 2 new unsafe states for SW A T which were notfound by other approaches.E. Threats to V alidity
Threats to internal validity mainly come from our imple-
mentation of the source code. To eliminate the threats, two
co-authors manually review the source code.
Threats to external validity mainly come from the simulators
and the libraries used to implement our approach. The simula-
tors we adopted are all widely adopted in the literature and arewell tested. The libraries we adopted in our implementationare also reliable and widely used.
Threats to construction validity mainly lie in the randomness
in our experiment. To eliminate the threats, we randomly
simulate multiple times and used the average value in ourevaluation. Moreover, we also conduct statistical testing toevaluate the signiﬁcance of our results.
V. R
ELA TED WORK
There are many approaches testing CPSs, and they can be
divided into two categories, i.e., model-based testing (MBT)and search-based testing (SBT). MBT methods exploit modelswhich deﬁne the correctness criteria of CPSs to generate testinputs automatically [38]. Clarke et al. [39] have proposed toadopt statistical model checking techniques to test CPSs, anduse cross-entropy to sample rare events. Buzhinsky et al. [40]proposed a framework to test the control software of CPSs,using the modular formal language NCES from the perspectiveof formal veriﬁcation technology. Based on the nonlinear arti-ﬁcial neural network model, Kosek and Gehrke [41] proposedan anomaly detection method to efﬁciently detect and evaluatethe exception in Cyber-Physical Intrusion in Smart Grids.
MBT methods are usually expensive, time-consuming, and
hard to apply. Moreover, it is difﬁcult to capture complexcontinuous dynamics and interactions between the system andits environment. Furthermore, the input space of a CPS isusually extremely large, with these inputs associated with re-quirements, which again bring great challenges for thoroughlytesting a given CPS. To address the above-mentioned chal-lenges, search-based testing (SBT) [42] methods employingsearch algorithms to test CPSs are proposed. Matinnejad etal. [43] test Simulink models by using a search algorithmto generate a small set of test cases. Ben Abdessalem etal. [44] verify automatic driving assistant systems using multi-objective optimization algorithms combined with neural net-works. Our approach is more closely related to search-basedtesting methods and thus we discuss those approaches in detail.
Robustness-Guided Falsiﬁcation of CPS The quantitative
semantics based on Metric Temporal Logic (MTL) [45] and its
variant, Signal Temporal Logic (STL) [46], [47], are adoptedin robustness-guided falsiﬁcation methods. The robustness ofan MTL formula is deﬁned as a numeric measurement of how”robust” a property holds in the given CPS system. Given thequantitative deﬁnition of robustness, the problem of falsifyingrobustness properties of a CPS system is converted into thenumerical minimization problem, which searches the systemparameters that minimize the robustness value. The robustness-guided falsiﬁcation approaches can be classiﬁed into black-boxmethods and grey-box methods.
The black-box methods can further be divided into methods
that translate the robustness-guided falsiﬁcation problem into
564a global optimization problem and the methods based on the
statistical modeling. There are a large number of methodsbased on global optimization techniques, such as methodsusing simulated annealing [13], genetic algorithms [14], andgradient descent [15]. Methods that use simulated anneal-ing transform the falsiﬁcation problem into a minimizationproblem with normative robustness on the set of all systemtrajectories. The method adopting the genetic algorithm usesthe thought of ant colony optimization in the process offalsiﬁcation input generation. The gradient descent methodparameterizes the system input, imposes small disturbanceson the system input in space and time, and uses the gradientdescent method to converge to the worst local system behavior.The methods using statistical modeling include the Cross-Entropy method [16] and Gaussian regression [17]–[19]. TheCross-Entropy (CE) [16] method samples the input proba-bility distribution approximating the distribution caused bythe robustness value of trajectories. The Gaussian Regressionmethod is used to introduce the falsiﬁcation process into thedomain estimation problem, and then the Gaussian Process isused to construct the approximate probability semantics of thetemporal formulas, thereby providing a higher probability forthe area where the formula is falsiﬁed.
The grey-box approaches are relevant to our work. Plaku et
al. [48] propose a method based on Rapidly-exploring RandomTree (RRT), which gradually generates the next operation bysampling the valid inputs. Dressosi et al. [49] propose an RRT-search method guided by robustness. Yamagata et al. [20]use Deep Reinforcement Learning (DRL) techniques (A3C,DDQN) to solve the problem. Unlike Dressosi et al., DRLconforms to the guided search process by predicting robustnesslearned from the system’s previous behaviours.
Fuzzing-based Methods Fuzzing is a random testing tech-
nique that automatically generates inputs randomly or with
feedback guidance [50]. Since being proposed, fuzzing hasbeen widely adopted in different testing scenarios, includingDeep Reinforcement Fuzzing [51], network protocols [52],web browsers [53], [54], etc. There are a lot of notable toolsthat have performed fuzzing on their programs, such as [55].American fuzzing lop [56] aims at programs, and makes use ofgenetic algorithms to increase the code coverage of test casesand detect more bugs. Fuzzing has been adopted widely in ﬁeldof CPS testing. CyFuzz [57] is an example designed for testingSimulink models of CPSs. It identiﬁes problematic compo-nents in the Simulink modeling environment, by studyingpublicly available bug reports. CPFuzz [58] combines fuzzingand falsiﬁcation to ﬁnd safety violations at the developmentphase for CPSs. Wijaya et al. [59] have proposed an approachfor construction models of anomaly detectors using supervisedlearning from the traces of normal and abnormal operation ofCPSs. Chen et al. [12] have adopted fuzzing at the networklevel of CPS. Speciﬁcally, it uses fuzzing of actuators withcommands which is not generated by the PLCs. It overrides thereal commands with the fuzzed commands to drive physicalsensors out of their safety threshold. Chen et al. [7] adoptedactive learning to the pre-training of fuzzing to improve theefﬁciency of fuzzing for CPSs. These fuzzing-based methodsall need system logs or network packets for CPSs to test, whilesuch messages are not always available. Our approach makesno assumptions on those information and directly interactswith the CPS models.
Our method employs DRL to generate test inputs for CPS.
Actually, there have been some existing works, which use DRLfor test input generation. Reddy et al. [60] proposed a solutionbased on reinforcement learning (RL), using a tabular, on-policy RL approach to guide the generator, which can generatevalid test inputs for programs. Kim et al. [61] used DDQN toreplace human-designed meta-heuristic algorithms in Search-based Software Testing. These two approaches only target thetraditional software. Qin et al. [62] proposed an interactivemulti-agent framework using DQN in which the system-under-design is shaped as an ego agent and its environment ismodeled by numerous adversarial agents. They evaluated theirmethod on two real-world case studies from the ﬁeld of self-driving cars. Their approach is only applicable to the CPSsystems with very simple inputs (e.g., 3 discrete inputs fortheir case studies), and is hard to handle real situations withvery complex inputs (e.g., 26-dimensional input in SW A T) dueto the limitations of DQN.
VI. C
ONCLUSION
We propose a method to generate failure-inducing input for
CPSs effectively, using deep reinforcement learning. We targetboth continuous actions and high-dimensional discrete actions,and design strategies to cater to both scenarios. Moreover,we incorporate Random Network Distillation (RND), a cu-riosity mechanism, to improve the capability of exploring rarestates. We experimentally evaluate our method with three CPSsimulators, including continuous action and high-dimensionaldiscrete action as input, and compare the performance ofour method with state-of-the-art approaches, i.e., Smart Fuzz,A3C, and DDQN. The experiment results show that ourmethod is able to effectively and efﬁciently generate failure-inducing input that can cover all targeted unsafe states withhigh success rates. Moreover, our approach outperforms state-of-the-art approaches on both success rate and the number ofiterations. In particular, our method is able to ﬁnd two newunsafe states which are not detected by previous approaches.
There are still some limitations which require further im-
provement. First, our method has only been evaluated on threeCPS simulators, more evaluations on large-scale real systemscould better validate the effectiveness and scaling capability ofour work. Second, due to the nature of the DRL algorithm thatthe training process and the searching process are integrated,our method needs a relatively long time to generate failure-inducing inputs under some circumstances. Therefore, moreexploration on reducing the generation time is required.
A
CKNOWLEDGMENT
This work is partially supported by the NSFC Y outh Funds
under Grant 61802275; the NSFC-General Technology BasicResearch Joint Funds under Grant U1836214; the NationalResearch Foundation Singapore under its NSoE Programme(Award Number: NSOE-DeST-2019-008); the new Genera-tion of Artiﬁcial Intelligence Science and Technology MajorProject of Tianjin under grant: 19ZXZNGX00010.
565REFERENCES
[1] U. N. S. Foundation, “Cyber-physical systems (cps),” https://www.nsf.
gov/publications/pubsumm.jsp?odskey=nsf18538&org=NSF, 2018, doc-
ument number: nsf18538. accessed: April 2021.
[2] R. Rajkumar, I. Lee, L. Sha, and J. Stankovic, “Cyber-physical sys-
tems: the next computing revolution,” in Design automation conference.
IEEE, 2010, pp. 731–736.
[3] A. Hassanzadeh, A. Rasekh, S. Galelli, M. Aghashahi, R. Taormina,
A. Ostfeld, and M. K. Banks, “A review of cybersecurity incidents inthe water sector,” Journal of Environmental Engineering, vol. 146, no. 5,
p. 03120003, 2020.
[4] I.-C. Alert, “Cyber-attack against ukrainian critical infrastructure,”
https://ics-cert.us-cert.gov/alerts/IR-ALERT-H-16-056-01, 2016, docu-ment number: IR-ALERT-H-16-056-01. accessed: April 2021.
[5] J. Leyden, “Water treatment plant hacked, chemical mix changed for
tap supplies,” https://www.theregister.co.uk/2016/03/24/water
utility
hacked/, 2016, accessed: April 2021.
[6] Y . Chen, C. M. Poskitt, and J. Sun, “Learning from mutants: Using code
mutation to learn and monitor invariants of a cyber-physical system,” in2018 IEEE Symposium on Security and Privacy (SP). IEEE, 2018, pp.648–660.
[7] Y . Chen, B. Xuan, C. M. Poskitt, J. Sun, and F. Zhang, “Active fuzzing
for testing and securing cyber-physical systems,” in Proceedings of the
29th ACM SIGSOFT International Symposium on Software Testing andAnalysis, 2020, pp. 14–26.
[8] Y . Chen, C. M. Poskitt, and J. Sun, “Towards learning and verifying
invariants of cyber-physical systems by code mutation,” in International
Symposium on F ormal Methods. Springer, 2016, pp. 155–163.
[9] J. Inoue, Y . Yamagata, Y . Chen, C. M. Poskitt, and J. Sun, “Anomaly
detection for a water treatment system using unsupervised machinelearning,” in 2017 IEEE International Conference on Data Mining
Workshops (ICDMW). IEEE, 2017, pp. 1058–1065.
[10] H. Choi, W.-C. Lee, Y . Aafer, F. Fei, Z. Tu, X. Zhang, D. Xu, and
X. Deng, “Detecting attacks against robotic vehicles: A control invariantapproach,” in Proceedings of the 2018 ACM SIGSAC Conference on
Computer and Communications Security, 2018, pp. 801–816.
[11] J. Giraldo, D. Urbina, A. Cardenas, J. V alente, M. Faisal, J. Ruths, N. O.
Tippenhauer, H. Sandberg, and R. Candell, “A survey of physics-basedattack detection in cyber-physical systems,” ACM Computing Surveys
(CSUR), vol. 51, no. 4, pp. 1–36, 2018.
[12] Y . Chen, C. M. Poskitt, J. Sun, S. Adepu, and F. Zhang, “Learning-
guided network fuzzing for testing cyber-physical system defences,” in2019 34th IEEE/ACM International Conference on Automated SoftwareEngineering (ASE). IEEE, 2019, pp. 962–973.
[13] H. Abbas and G. Fainekos, “Convergence proofs for simulated annealing
falsiﬁcation of safety properties,” in 2012 50th Annual Allerton Confer-
ence on Communication, Control, and Computing (Allerton). IEEE,2012, pp. 1594–1601.
[14] Y . S. R. Annapureddy and G. E. Fainekos, “Ant colonies for temporal
logic falsiﬁcation of hybrid systems,” in IECON 2010-36th Annual
Conference on IEEE Industrial Electronics Society. IEEE, 2010, pp.91–96.
[15] S. Yaghoubi and G. Fainekos, “Falsiﬁcation of temporal logic require-
ments using gradient based local search in space and time,” IF AC-
PapersOnLine, vol. 51, no. 16, pp. 103–108, 2018.
[16] S. Sankaranarayanan and G. Fainekos, “Falsiﬁcation of temporal proper-
ties of hybrid systems using the cross-entropy method,” in Proceedings
of the 15th ACM international conference on Hybrid Systems: Compu-tation and Control, 2012, pp. 125–134.
[17] T. Akazaki, “Falsiﬁcation of conditional safety properties for cyber-
physical systems with gaussian process regression,” in International
Conference on Runtime V eriﬁcation. Springer, 2016, pp. 439–446.
[18] S. Silvetti, A. Policriti, and L. Bortolussi, “An active learning approach
to the falsiﬁcation of black box cyber-physical systems,” in International
Conference on Integrated F ormal Methods. Springer, 2017, pp. 3–17.
[19] J. Deshmukh, M. Horvat, X. Jin, R. Majumdar, and V . S. Prabhu,
“Testing cyber-physical systems through bayesian optimization,” ACM
Transactions on Embedded Computing Systems (TECS), vol. 16, no. 5s,pp. 1–18, 2017.
[20] Y . Yamagata, S. Liu, T. Akazaki, Y . Duan, and J. Hao, “Falsiﬁcation
of cyber-physical systems using deep reinforcement learning,” IEEE
Transactions on Software Engineering, 2020.
[21] S. Gu, T. Lillicrap, I. Sutskever, and S. Levine, “Continuous deep q-
learning with model-based acceleration,” in International Conference
on Machine Learning. PMLR, 2016, pp. 2829–2838.[22] V . Mnih, A. P . Badia, M. Mirza, A. Graves, T. Lillicrap, T. Harley,
D. Silver, and K. Kavukcuoglu, “Asynchronous methods for deep rein-forcement learning,” in International conference on machine learning.
PMLR, 2016, pp. 1928–1937.
[23] G. Dulac-Arnold, R. Evans, H. van Hasselt, P . Sunehag, T. Lillicrap,
J. Hunt, T. Mann, T. Weber, T. Degris, and B. Coppin, “Deep re-inforcement learning in large discrete action spaces,” arXiv preprint
arXiv:1512.07679, 2015.
[24] R. S. Sutton and A. G. Barto, Reinforcement learning: An introduction.
MIT press, 2018.
[25] Sutton, Richard S. and Barto, Andrew G., Reinforcement Learning: An
Introduction. Cambridge, MA, USA: A Bradford Book, 2018.
[26] V . Mnih, K. Kavukcuoglu, D. Silver, A. Graves, I. Antonoglou, D. Wier-
stra, and M. Riedmiller, “Playing atari with deep reinforcement learn-
ing,” arXiv preprint arXiv:1312.5602, 2013.
[27] R. S. Sutton, D. A. McAllester, S. P . Singh, Y . Mansour et al., “Policy
gradient methods for reinforcement learning with function approxima-
tion.” in NIPs, vol. 99. Citeseer, 1999, pp. 1057–1063.
[28] T. P . Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y . Tassa,
D. Silver, and D. Wierstra, “Continuous control with deep reinforcementlearning,” arXiv preprint arXiv:1509.02971, 2015.
[29] Y . Burda, H. Edwards, A. Storkey, and O. Klimov, “Exploration by
random network distillation,” arXiv preprint arXiv:1810.12894, 2018.
[30] H. Marmolin, “Subjective mse measures,” IEEE transactions on systems,
man, and cybernetics, vol. 16, no. 3, pp. 486–489, 1986.
[31] “Secure water treatment (swat),” https://itrust.sutd.edu.
sg/itrust-labs-home/itrust-labsswat/, 2019, accessed: April 2021.
[32] “Modeling an automatic transmission controller,”
https://www.mathworks.com/help/simulink/examples/modeling-an-automatic-transmission-controller.html, 2016, accessed:April 2021.
[33] X. Jin, J. V . Deshmukh, J. Kapinski, K. Ueda, and K. Butts, “Powertrain
control veriﬁcation benchmark,” in Proceedings of the 17th international
conference on Hybrid systems: computation and control, 2014, pp. 253–262.
[34] J. Goh, S. Adepu, K. N. Junejo, and A. Mathur, “A dataset to support re-
search in the design of secure water treatment systems,” in International
conference on critical information infrastructures security. Springer,2016, pp. 88–99.
[35] S. Chen, “Comparing deep reinforcement learning methods for engi-
neering applications,” Ph.D. dissertation, Master dissertation, Faculty ofComputer Science, Otto-von-Guericke ..., 2018.
[36] F. Konietschke, M. Placzek, F. Schaarschmidt, and L. A. Hothorn, “npar-
comp: an r software package for nonparametric multiple comparisonsand simultaneous conﬁdence intervals,” Journal of Statistical Software
64 (2015), Nr . 9, vol. 64, no. 9, pp. 1–17, 2015.
[37] N. Cliff, Ordinal methods for behavioral data analysis. Psychology
Press, 2014.
[38] A. Aerts, M. Reniers, and M. R. Mousavi, “Model-based testing of
cyber-physical systems,” in Cyber-Physical Systems. Elsevier, 2017,
pp. 287–304.
[39] E. M. Clarke and P . Zuliani, “Statistical model checking for cyber-
physical systems,” in International symposium on automated technology
for veriﬁcation and analysis. Springer, 2011, pp. 1–12.
[40] I. Buzhinsky, C. Pang, and V . Vyatkin, “Formal modeling of testing
software for cyber-physical automation systems,” in 2015 IEEE Trust-
com/BigDataSE/ISPA, vol. 3. IEEE, 2015, pp. 301–306.
[41] A. M. Kosek and O. Gehrke, “Ensemble regression model-based
anomaly detection for cyber-physical intrusion detection in smart grids,”in2016 IEEE Electrical Power and Energy Conference (EPEC). IEEE,
2016, pp. 1–7.
[42] A. Arrieta, S. Wang, U. Markiegi, G. Sagardui, and L. Etxeberria,
“Search-based test case generation for cyber-physical systems,” in 2017
IEEE Congress on Evolutionary Computation (CEC). IEEE, 2017, pp.688–697.
[43] R. Matinnejad, S. Nejati, L. C. Briand, and T. Bruckmann, “Automated
test suite generation for time-continuous simulink models,” in proceed-
ings of the 38th International Conference on Software Engineering,2016, pp. 595–606.
[44] R. Ben Abdessalem, S. Nejati, L. C. Briand, and T. Stifter, “Testing
advanced driver assistance systems using multi-objective search andneural networks,” in Proceedings of the 31st IEEE/ACM International
Conference on Automated Software Engineering, 2016, pp. 63–74.
[45] R. Koymans, “Specifying real-time properties with metric temporal
logic,” Real-time systems, vol. 2, no. 4, pp. 255–299, 1990.
[46] O. Maler and D. Nickovic, “Monitoring temporal properties of contin-
uous signals,” in F ormal Techniques, Modelling and Analysis of Timed
and Fault-Tolerant Systems. Springer, 2004, pp. 152–166.
566[47] A. Donz ´e and O. Maler, “Robust satisfaction of temporal logic over
real-valued signals,” in International Conference on F ormal Modeling
and Analysis of Timed Systems. Springer, 2010, pp. 92–106.
[48] E. Plaku, L. E. Kavraki, and M. Y . V ardi, “Falsiﬁcation of ltl safety
properties in hybrid systems,” in International Conference on Tools and
Algorithms for the Construction and Analysis of Systems. Springer,
2009, pp. 368–382.
[49] T. Dreossi, T. Dang, A. Donz ´e, J. Kapinski, X. Jin, and J. V . Deshmukh,
“Efﬁcient guiding strategies for testing of temporal properties of hybridsystems,” in NASA F ormal Methods Symposium. Springer, 2015, pp.
127–142.
[50] A. Takanen, J. D. Demott, C. Miller, and A. Kettunen, Fuzzing for
software security testing and quality assurance. Artech House, 2018.
[51] K. B ¨ottinger, P . Godefroid, and R. Singh, “Deep reinforcement fuzzing,”
in2018 IEEE Security and Privacy Workshops (SPW) . IEEE, 2018,
pp. 116–122.
[52] G. Vigna, W. Robertson, and D. Balzarotti, “Testing network-based
intrusion detection signatures using mutant exploits,” in Proceedings of
the 11th ACM conference on Computer and communications security,2004, pp. 21–30.
[53] T. Guo, P . Zhang, X. Wang, and Q. Wei, “Gramfuzz: Fuzzing testing of
web browsers based on grammar analysis and structural mutation,” in2013 Second International Conference on Informatics & Applications(ICIA). IEEE, 2013, pp. 212–215.
[54] J. Wang, B. Chen, L. Wei, and Y . Liu, “Superion: Grammar-aware
greybox fuzzing,” in 2019 IEEE/ACM 41st International Conference on
Software Engineering (ICSE). IEEE, 2019, pp. 724–735.
[55] S. K. Cha, M. Woo, and D. Brumley, “Program-adaptive mutational
fuzzing,” in 2015 IEEE Symposium on Security and Privacy. IEEE,
2015, pp. 725–741.
[56] M. Zalewski, “American fuzzy lop,” http://lcamtuf.coredump.cx/aﬂ/,
2017, accessed: April 2021.
[57] S. A. Chowdhury, T. T. Johnson, and C. Csallner, “Cyfuzz: A differential
testing framework for cyber-physical systems development environ-ments,” in International Workshop on Design, Modeling, and Evaluation
of Cyber Physical Systems. Springer, 2016, pp. 46–60.
[58] F. Shang, B. Wang, T. Li, J. Tian, and K. Cao, “Cpfuzz: Combining
fuzzing and falsiﬁcation of cyber-physical systems,” IEEE Access, vol. 8,
pp. 166 951–166 962, 2020.
[59] H. Wijaya, M. Aniche, and A. Mathur, “Domain-based fuzzing for
supervised learning of anomaly detection in cyber-physical systems,”inProceedings of the IEEE/ACM 42nd International Conference on
Software Engineering Workshops, 2020, pp. 237–244.
[60] S. Reddy, C. Lemieux, R. Padhye, and K. Sen, “Quickly generating di-
verse valid test inputs with reinforcement learning,” in 2020 IEEE/ACM
42nd International Conference on Software Engineering (ICSE). IEEE,2020, pp. 1410–1421.
[61] J. Kim, M. Kwon, and S. Y oo, “Generating test input with deep rein-
forcement learning,” in 2018 IEEE/ACM 11th International Workshop
on Search-Based Software Testing (SBST). IEEE, 2018, pp. 51–58.
[62] X. Qin, N. Ar ´echiga, A. Best, and J. Deshmukh, “Automatic testing
and falsiﬁcation with dynamically constrained reinforcement learning,”arXiv preprint arXiv:1910.13645, 2019.
567