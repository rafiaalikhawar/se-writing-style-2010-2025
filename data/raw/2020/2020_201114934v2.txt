ProbingModel Signal-Awarenessvia
Prediction-PreservingInput Minimization
SahilSuneja∗
IBMResearch
YorktownHeights, NY, USA
suneja@us.ibm.comYunhui Zheng∗
IBMResearch
YorktownHeights, NY, USA
zhengyu@us.ibm.comYufanZhuang∗
IBMResearch
YorktownHeights, NY, USA
yufan.zhuang@ibm.com
JimA.Laredo
IBMResearch
YorktownHeights, NY, USA
laredoj@us.ibm.comAlessandroMorari
IBMResearch
YorktownHeights, NY, USA
amorari@us.ibm.com
ABSTRACT
ThisworkexploresthesignalawarenessofAImodelsforsource
codeunderstanding.Usingasoftwarevulnerabilitydetectionuse
case, we evaluate the models’ ability to capture the correct vul-
nerability signals to produce their predictions. Our prediction-
preserving input minimization ( P2IM) approach systematically
reduces the original source code to a minimal snippet which a
model needs to maintain its prediction. The model’s reliance on
incorrectsignalsisthenuncoveredwhenthevulnerabilityinthe
original code ismissing in the minimal snippet,both of which the
modelhoweverpredictsasbeingvulnerable.Wemeasurethesignal
awarenessofmodelsusinganewmetricwepropose-Signal-aware
Recall (SAR). We apply P2IMon three diﬀerent neural network
architectures across multiple datasets. The results show a sharp
drop in the model’s Recall from the high 90s to sub-60s with the
newmetric,highlightingthatthemodelsarepresumablypicking
up a lot of noise or dataset nuances while learning their vulner-
ability detection logic. Although the drop in model performance
may be perceived as an adversarial attack, but this isn’t P2IM’s
objective.Theideaisrathertouncoverthesignal-awarenessofa
black-box model in a data-driven manner via controlled queries.
SAR’s purpose is to measure the impact of task-agnostic model
training,andnottosuggestashortcomingintheRecallmetric.The
expectation,infact,isforSARtomatchRecallintheidealscenario
where the modeltruly captures task-speciﬁc signals.
1 INTRODUCTION
Recently, AI research has made inroads in source code understand-
ing and being able to perform tasks such as variable naming, func-
tion naming, summarygeneration, variablemisusedetection,and
vulnerabilitydetection,amongstothers[ 2–5,22,34,44].Evermore
sophisticated models are emerging and pushing the state of the art
rapidly.Althougheachnewmodelimprovesuponitspredecessor’s
predictionperformanceintermsofF1andaccuracymeasures,what
remainsrelativelyunexplorediswhetherthemodelsarepicking
uptherightsignals to arrive at theirpredictions.
We call this aspect of the model’s quality ‘signal awareness’,
and explore it using a software vulnerability detection use-case.
What used to be a domain traditionally dominated by static and
dynamic analysis is seeing assistance and competition from AI
∗Authorscontributed equally to thisresearch.models. The high false positives of static analyzers, and the lack
of completeness of dynamic analysis are a few reasons promoting
theentryofAIintothisﬁeld[ 27,56,62].However,unliketherules
andpath/ﬂowanalysisofstaticanalyzersandtheexecutiontracing
ofdynamicanalysis,itremainsunclearastowhatsignalstheAI
modelsactuallypickupfordetectingvulnerabilitiesinsourcecode.
Fromamodel’sperspective,itmightbedoinganexcellentjobof
learning a separator between healthy and buggy code samples. But
itmayverywellbethe casethatitdoessobypicking upnoiseor
certainnuancesfromthedatasetfortheirpredictions,whichare
not representative or even related to vulnerabilities. Although this
canleadtohighaccuracyscoresforAI-baseddetectors,whichmay
beperfectlyacceptableinatheoreticalorstatisticalsetting,thiscan
leadtoafalsesenseofsecuritywhenappliedtoreal-worldusage
as substitutesfor traditional sourcecode analyzers.
To this end, in this paper we present a systematic approach
to uncover a model’s vulnerability detection logicand evaluate
its ability to capture real signals. We use a data-driven explain-
abilityapproachakin to howthe model wouldhavedeveloped its
data-drivenlogic.Weborrowafaultisolationtechniquefromthe
Software Engineering domain called Delta Debugging [ 64]. The
coreideaofour Prediction-PreservingInputMinimization (P2IM)ap-
proach is to ﬁrst reduce the original source code input to a trained
model into a minimalsnippet, without changing the model predic-
tion. And then to verify whether the minimal snippet has the same
vulnerabilityproﬁleastheoriginalcode.Themodel’srelianceon
incorrect signals can then be uncovered when a vulnerability in
the original code is missing in the minimalsnippet, both of which
themodelhoweverpredictsasbeingvulnerable.Additionally,we
presentanewmetriccalled Signal-awareRecall(SAR) tomeasure
howwell amodelcaptures task-speciﬁc signals.
We apply P2IMon three diﬀerent neural network architectures,
operating at diﬀerent popular representations of source code in
theAIdomain:(i)aconvolutionalneuralnetwork(CNN)treating
codeasaphoto,(ii)arecurrentneuralnetwork(RNN)treatingcode
as a linear sequence of tokens, and (iii) a graph neural network
(GNN) operating on code as a graph. We apply these models for
vulnerability detection over three diﬀerent datasets, including a
real-worlddatasetweextractfromGithub.Resultsshowmorethan
85% of samples can be reduced while maintaining their vulnerable
prediction, across datasets and models. However, we observe a
sharp performance drop across all models when probing them forSahilSuneja, YunhuiZheng,YufanZhuang,JimA.Laredo,andAlessandro Morari
signalawareness,withRecalldroppingfromthehigh90stolow60s
withournewmetric,suggestingthatthemodelsarepresumably
pickingupfeatures irrelevantto the taskat hand.
Our work calls for a signal-aware supplement to the traditional
statisticalmeasurementsforAImodelsandeﬀortstofocusmore
on therealsignals during model training. Note that we are not
suggestingashortcomingintheRecallmetric.Infact,theexpectation
isforthemodel’sSARtoreachitsRecallintheidealscenariowhere
themodeltrulycapturedtask-speciﬁcsignals. Westronglybelieve
that the models are powerful enough to pick such relevant signals,
withappropriate guidanceduringtraining.Buttomotivateresearch
inthatdirection,theshortcomingsofcurrenttask-agnostictraining
needtoberevealed,whichispreciselywhatwetackleinthiswork.
In ourP2IMapproach, to probe a model’s signal awareness, we
maintain utmost fairness to the model. We keep the model un-
touched, and never alter its training process or change its training
distributionatall,whichisperformedontheoriginalunmodiﬁed
dataset. The model’s Recall and SAR are evaluated on the dataset’s
original test-set itself. Note that this work does not identify whata
model is learning, but rather ifthe model is learning vulnerability-
relatedsignals.Wetreatthetrainedmodelasablackboxandquery
itforitspredictiononiterativelysmallerversionsforeachvulnera-
ble sample from the test-set. With such input “perturbations”, our
goalisnottocraftprogramsforadversarialattacks[ 6,14,54,60],
but to uncover the signal-awareness of a black-box model in a
data-driven manner via controlled queries. That is, at a very su-
perﬁcial level, we ask the trained model if it feels a code sample
‘AAAA’ is vulnerable, where ‘ A’ represents any atomic code chunk
granularity; then if it feels ‘ AAA’ is vulnerable, then if it feels ‘ AA’
isvulnerable,andsoonuntilit’sverdictchanges.Wethentestif
theminimalreduced version, which preserves the model’s verdict,
actually contains the original ground-truth vulnerability. In our
experiments,weusetheInfertool[ 12]toverifybugexistencein
the reduced samples. However, P2IMis independent of the speciﬁc
bug-checkerbeingemployed. Toensuremodelfairness,weonly
feed valid compilable reduced samples to the model for its verdict,
withoutintroducing any newbugs.
Our method augments the common metrics toolbox with an
alternativewaytoexaminethemodelquality,givingreliabilityand
trustworthinesstoblack-boxAImodels. P2IMbringsinmultiple
enhancementsascomparedtopopularperturbation-basedmodel
interpretabilitymethodsthatworkonindividualinputinstances,
suchasLIME[ 43]andothersassummarizedinarecentsurvey[ 10].
While other approaches are able to derive parts of an input that
contribute most to the model’s ﬁnal prediction, unlike our method,
theycannottellifthehighlightedpartsarethecorrecttask-relevant
signals.Anothercontrastingcapability P2IMoﬀersistoquantify
how well a model learns the correct signals, thereby providing
a signal-aware mechanism to compare diﬀerent models. This is
especially useful when competing models have comparable per-
formance on traditional metrics (e.g., F1). Furthermore, the search
space, and thus the time complexity, for such approaches can be
huge,given thepossiblecombinationsof thediﬀerentpartsofthe
inputstobeexplored.ThankstotheDeltaDebuggingalgorithm,
P2IMdirectly minimizes the input, signiﬁcantly accelerating the
searchfor the relevantparts ofthe input.Thesalientfeaturesofourprediction-preservinginputminimiza-
tionapproach ( P2IM) are summarizedas follows:
•Black-box: P2IMrequires no knowledge about the target
model’s internals, and is applicable to all model types, in-
cluding classic machine learning as well as neural networks.
•Externally Veriﬁable: Emittedminimalcodeisvalidand
compilable,enablingcross-veriﬁcationwithtraditionalstatic
anddynamic analysistools.
•New Metric: We present a new Signal-aware Recall metric
to measure howwell a modelcaptures task-speciﬁc signals.
Thisenablesamorefairmodelevaluationandmeaningful
comparison.
This paper is organized as follows. We ﬁrst discuss our mo-
tivation behind this work in Section 2, and then present a brief
background on AI models for source code, as well as the Delta
Debugging technique, in Section 3. Then, we present the details of
ourP2IMapproachinSection4,alongwithafewexamplesdemon-
stratingP2IMin action. We evaluate the models’ signal awareness
usingourapproachinSection5.Section6coversrelatedwork,and
ﬁnally Section 7concludes this paper.
2 MOTIVATION
The rapid proliferation of AI for source code understanding is lead-
ingtoevermoresophisticatedmodels,whicharegettingbiggerand
better with each successive iteration. We have been experimenting
withthestateofthearttoassessitsqualityfromapracticalperspec-
tive.Wenoticedthatthemodelsseemtosuﬀerfromweakgenerality
and robustness–known AI frailties. These concerns become ever
moreimportantifthemodelsaretobeappliedtosensitivetasks,
such as ensuring code security. We observed the same weakness
acrossseveraldiﬀerentmodelsanddatasetsinavulnerabilityde-
tectionsetting,whichledustodoubtthequalityofthemodelsin
terms ofwhatis it that they are actually learning. We thus started
exploringwaysto probe the models’ signal-awareness .
Signal-awareness is diﬀerent than correctness — a model can
learn a perfect separator between buggy and healthy code, but
it can very well arrive at the separator by picking up dataset nu-
ances,asopposedtoreal vulnerabilitysignals. Thiscan becaused
when, for example, the model picks up unexpected correlations
betweencode samplesand samplelengths,orvariable names,or
certainprogrammingconstructs,whichmayhappentodiﬀerfor
buggyand healthy samples ina particular dataset. Learning a sepa-
rator based on these non-representative signals, which may lead to
great-looking performance numbers, is perfectly acceptable from a
model’sperspective.The model isindeed doing itsjobof learning
to classify,but this providesafalse sense of security.
Signal-awareness or verifying if the models are learning the
correctlogicrelevanttocodeanalysisiscrucialtogeneratetrust
inmodelsiftheyaretobeputintotheﬁeldincompetitionto,or
alongside,traditionalstaticanddynamicanalyzers.Furthermore,it
addsanimportantmeasureofmodelquality,beyondthetraditional
statisticalanalysismeasures,whichcanmorefairlycompareand
guide improvements across model evolutions. In this work, we
uncover this signal-awareness aspect of AI-for-code models and
quantify how much impact it has on their robustness as well as
reportedperformance numbers.Probing Model Signal-AwarenessviaPrediction-PreservingInput Minimization
Atypicalexampleofweakrobustnessiswhenanimageclassi-
ﬁer’s verdict on an input image changes on adding minor noise to
theimageimperceptibletothenakedeye[ 6,14,54].Asshownin
Table1,weobservethesameissuewithAI-for-codevulnerability
detection models, where evena 99F1 model ﬂips its prediction on
only slightly diﬀerent code variants. However, it should be reason-
abletoexpectahigh-qualitymodel,whichcorrectlypicksupthe
real vulnerability signals, to demonstrate prediction robustness,
if it is ever to be trusted in a practical security setting. Although
such “perturbations" canbe taken to be an adversarial attack on
the model [ 60], and similarly there can be defenses against such
attacks such as training the model with several diﬀerent code vari-
ants.But,thislineofthoughtiscomplementarytoourwork.These
observationsmerelytriggeredoursuspicionaroundadisconnect
betweenthereportedmodelperformancenumbersversustheac-
tualtask-awarelearning,similarinspirittootherprevailingdoubts
regarding modelquality[1, 7, 23].
Our goal is not to discredit AI-for-code models by reiterating
their brittleness, but to uncover and quantify how much impact
task-agnostic training has on their reported performance numbers.
Inparticular,weevaluatethetrainedmodelsontheoriginaltest-set
itself,while queryingthemodelwith iteratively reduced versions
ofeachsample,touncoverpreciselywhatportionoftheoriginal
sample does the model consider to be relevant for its prediction.
Then, by counting the occurrences across the test-set, where the
minimalportionsstillcontainthesamevulnerabilityastheoriginal
samples, we can measure how signal-aware the model is using our
proposedSARmetric.
Ourhopeisthatrevealingtheseshortcomingswouldmotivatefu-
tureresearchtowardsmoretask-awarelearning,potentiallyguided
by the SAR-Recall divide, to better utilize the potential of AI for
sourcecode understanding.
3 BACKGROUND
Inthissection,weﬁrstbrieﬂydescribethreediﬀerentneuralnet-
work models which have been popularly employed for learning
over source code, each operating at a diﬀerent code representation.
Theseinclude(i)aconvolutionalneuralnetworktreatingcodeas
a photo, (ii) a recurrent neural network treating code as a linear
sequenceoftokens, and (iii) agraphneural networkoperatingon
code as a graph. In section 5, we shall evaluate these models on
ourproposedSARmetric.Afterthemodeldescriptions,wecover
the basics of the Delta Debugging technique [ 64] which we use to
reduce the input samples to measure the models’ signal-awareness.
3.1 AI-for-codeModels
Convolutionalneuralnetworks(CNNs) learn on image inputs.
These are made up of convolutional and pooling layers. The for-
mer act as ﬁlters to extract features from input images, learning
increasinglycomplexpatternswhentheneuralnetworkbecomes
progressively deeper. Pooling layers, on the other hand, down-
sample the features so as to intensify the signal and control the
size of the neural network. This is done by selecting the most
strongly activated neurons (i.e., max-pooling)or takingtheir aver-
age (i.e., mean-pooling). CNNs have been successfully employedTable 1: Bad Robustness: a 99 F1 GNN model ﬂips on only
slightly syntactically diﬀerent but semantically identical
function variants. Dataset from [47], model from [53]. In
thispaper,we focuson type (a) reductions only.
(a) Removing tokens changesmodelpredictions
!"#!!"#$%$&$'')$
!*#!!"#$2$&$-3)$
!(#!$%&'$%++,4'.) $
!1#!!/$52$6$%7$8$
!-#!$$2$&$39)$
!4#!:$
!3#!%++,2.$&$/0/)$!"#!!"#$%$&$'')$
!*#!!"#$2$&$-3)$
!(#!$%&'$%++,4'.) $
!1#!!/$507$8$
!-#!$$2$&$39)$
!4#!:$
!3#!%++,2.$&$/0/)$
Ground Truth Buggy Buggy
Prediction Buggy Non-buggy
(b)Adding tokens changesmodelpredictions
!"!!"##$#%#&'(###
)"!$%&'#$**+,,-( #
'"!$**+$-#%#./.(#
0"!'(#)'"#1(###!"!!"##$#%#&'(###
)"!$%&'#$**+,,-( #
'"!$**+$-#%#./.(#
0"!&*+*,-.#
,"!'(#)'"#1(###
Ground Truth Buggy Buggy
Prediction Buggy Non-buggy
incomputervisiontasks suchasimagerecognitionandobjectde-
tection [19,28,42,51]. These have also been used in the context of
vulnerability detection with slight modiﬁcations [ 44]. In such a set-
ting,the sourcecode tokens are ﬁrstprojected intoan embedding
space and then fed into a CNN as a real-value matrix, similar to an
image.
Recurrentneuralnetworks (RNNs) aredesignedtolearnover
sequentialinputs,suchastextandaudio[ 35,45].A‘workingmem-
ory’ is maintained and updated by a series of input, output, and
forget ‘gates’, using the current input and the previous memory at
eachstep[ 9,21].Dependingontheusecase,RNNscanemitvector
representations at each step, or emit one ﬁnal representation for
a complete sequence. RNNs have been applied to source code by
treating it as sequences of tokens. Usually, one ﬁnal representa-
tionisextractedforapieceofcodeinthecontextofvulnerability
detection [34,44].
Graphicalneuralnetworks(GNNs) havegainedpopularitydue
totheiruniqueabilitytolearnovergraph-structureddatalikesocial
network graphsand molecular structures [ 13,65]. MostGNNs are
made up of three modules: (i) message passing, which decides how
information is exchanged among nodesvia edges, (ii) message ag-
gregation,whichdetermineshoweachnodecombinesthereceived
messages, and (iii) message updating, which controls how each
node updates their representation after one cycle of information
propagation[ 13,26,32].UsingGNNsonsourcecodeisanaturalﬁt
since multiple forms of graphs can be constructed on top of source
code,suchasabstractsyntaxtree,dataﬂowgraph,andcontrolﬂow
graph.Theyhaveachievedstate-of-the-artperformanceonmultipleSahilSuneja, YunhuiZheng,YufanZhuang,JimA.Laredo,andAlessandro Morari
softwareengineeringtasks,includingvulnerabilitydetection[ 68]
andcode summarization[30].
3.2 DeltaDebugging
DeltaDebugging( DD)[64]wasﬁrstintroducedtominimizefailure-
inducingbugreportsfortheMozillabrowser.Manyofthesebug
reports contain long HTML ﬁles or user actions with lots of ir-
relevant information, which makes it challenging to understand
the root causes. Therefore, developers were looking for techniques
thatcansimplifytestcasesandgenerateminimizedonestrigger-
ingthesamefailures.Asaresult, DDsigniﬁcantlysimpliﬁedthe
crash-inducing inputs in an automated manner and hence enabled
aproductive bugdiagnosisandrepairexperience.
Theinputto DDisasequencesatisfyingsomepredeﬁnedoracle.
For example, it’s failure-inducing HTML ﬁles or user actions in the
Mozilla web browser case. The goal is to ﬁnd a subset of the input
satisfyingthefollowingtworequirements:(1)thesubsetleadsto
the same outcome; and (2) not a single element can be removed to
preserve the outcome.Suchasubsetiscalled 1-minimal .
Algorithm1 SimpliﬁedDelta DebuggingAlgorithm
Input:
/u1D447: Oraclefunction. /u1D447(/u1D465)isT/r.sc/u.sc/e.scif/u1D465has certain predeﬁned property.
/u1D446: Inputsequence, where /u1D447(/u1D446)=T/r.sc/u.sc/e.sc
Output:
/u1D446′:Reduced outcome-preserving 1-minimal sequence
1:function DD(/u1D447,/u1D446)
2:/u1D45B←2,/u1D446′←/u1D446
3:Divide/u1D446equallyinto Δ1,...,Δ/u1D45Bandthe complements
∇/u1D458=/u1D446−Δ/u1D458, where/u1D458=1,...,/u1D45B
4:Testeach T(Δ1),...,T(Δ/u1D45B)andT(∇1),...,T(∇/u1D45B)
5:ifallF/a.sc/l.sc/s.sc/e.scthen
6:/u1D45B←2∗/u1D45B
7:if/u1D45B>|/u1D446|then
8: return/u1D446′
9:else
10: gotoline 3
11:else ifT(Δ/u1D456)=T/r.sc/u.sc/e.scthen
12:/u1D446′←Δ/u1D456
13:/u1D45B←2,/u1D446←Δ/u1D456
14:if|/u1D446′|=1then
15: return/u1D446′
16:else
17: gotoline 3
18:else ifT(∇/u1D457)=T/r.sc/u.sc/e.scthen
19:/u1D446′←∇/u1D457
20:/u1D45B←/u1D45B−1,/u1D446←∇/u1D457
21:gotoline 3
As presented in Algorithm 1, given an input sequence /u1D446and
anoraclefunction /u1D447,DDiterativelysplitstheinputsequenceand
produces 1-minimal /u1D446′infourmain steps:
•Split and test . In each iteration, DDsplits the sequence in
consideration into /u1D45Bsegments and /u1D45Bcorresponding comple-
ments(line3),whereacomplementisdeﬁnedas ∇/u1D456=/u1D446-Δ/u1D456.
DDtests all partitions using the provided oracle function/u1D447(line 4) and checks if some partitions lead to the same
outcome.
•Reduceasubset .Ifthetestresultofasubset Δ/u1D456isthesame
as/u1D446,DDtreatsΔ/u1D456as the sequence for the next iteration and
resets the granularity /u1D45B(lines11 –17).
•Reduce a complement . Otherwise, if complement ∇/u1D457is an
outcome-preserving input (line 18), DDadjusts/u1D45Band ex-
plores itwiththe same granularity (line20).
•Operateonaﬁnergranularity .Ifnoneofthepartitioncanpre-
servetheoutcome(line5), DDdoublesthepartitionnumber
/u1D45Bto split the sequenceintosmallersegments (line6).
In each round, DDtries to reduce the scope to a subset. In the
best-casescenario, DDworkslikeabinarysearch,whichcansys-
tematicallyandeﬃciently identifythe 1-minimal .
Our intuition behind data-driven model probing comes from
thisfailure-inducinginputsimpliﬁcationidea.Speciﬁcally,webuild
our method atop DDwhile replacing the Mozilla target with the
predictionmodel,andthefailure-inducingHTMLﬁleswithvulner-
ableprogramsamples. DD’sprocessofidentifyingaminimalsub-
sequenceoftheinputwhichleadstothesameoutput,thentrans-
lates to identifying the minimal sub-program (1-minimal) which
preserves the model’s prediction. Themodel’s signal-awareness is
then determined by testing the 1-minimal for the original vulnera-
bilityexistence.
4 DESIGN
Programming defects are an inevitable reality in software creation.
Vulnerabilities arise when such defects fall in a security-related
subsetsuch as null pointer dereference,buﬀer overﬂow,use-after-
free,amongstothers.Staticanalyzersdetectthesevulnerabilities
either by reasoning about the possible execution behaviours over a
programmodel,orbymatchingdefect-speciﬁcrules.Dynamicanal-
ysis,ontheotherhand,directlyexecutestheprogram,exploring
diﬀerentexecutionpathsto concretelyexposethedefects. Unlike
thetraditionalanalyzers,thelogicofAI-for-codemodelsisimplicit,
and not directly perceptible. In this section, we present our ap-
proachtowardsunderstandingthislogic,whiletreatingthemodels
asblackboxentities.Givenexplaining whatanAImodelislearning
is still an open problem, especially in the context of source code
understanding,weframeourexplorationintermsofdetecting if
the models are learningthe vulnerability-relevantsignals.
4.1P2IMWorkﬂow
Figure1depictstheoverallﬂow behindour prediction-preserving
inputminimization( P2IM)approach.Thesequenceofoperations
isas follows:
Step1.P2IMtakes as input a trained model and a program sample
whichthemodelpredictstobevulnerable.Thissamplecomesfrom
the test-setofthe dataset usedfor modeltraining itself.
Steps 2-4. P2IMthen iteratively keeps reducing the sample and
queryingthemodelforitspredictiononthereducedsubprogram,
so long as the model maintains its vulnerability prediction. This
process continues till a minimal snippet, called 1-minimal, is ex-
tractedfromtheprogramsample,suchthatremovingevenasingle
tokenfromitwouldchangethemodelprediction.Toeﬃcientlyand
systematically reduce a program sample,we employ the popularProbing Model Signal-AwarenessviaPrediction-PreservingInput Minimization
awarenessprecisely.Thus,wetaketheconservativeapproachof
giving the beneﬁt of doubt to the model, giving it credit for cap-
turingvulnerabilitysignalsbasedonthe1-minimalreached,even
when it may not actually be doing so (based on the global mini-
mum). This results in an upper bound measurement of the model’s
signal awareness (SAR). Nevertheless, as shall be revealed in the
next section, even measuring the upper bound itself is suﬃcient
to highlight the problems of the models not picking up the correct
signals duringlearning.
Checkerquality . The signal awareness measurement is bounded
by the quality of the checker used to verify bug existence in the re-
ducedsubprograms.Thiscanbedatasetdependentandcaninclude
(i)theoriginaldatasetlabeler,(ii)aline-based bugmatcherwhich
gives the beneﬁt of doubt to the model on partial matches, or (iii) a
goodstaticanalyzertunedtowardshighrecall. Fortheexperiment
settings and the datasets considered in this paper (Section 5.1), the
Inferanalyzer[ 12]workedquitewell,withfallbacktoline-based
bug matching for samples with diﬀering Infer verdict and the orig-
inal bug. Although Infer as a checker is a fortunate ﬁt given our
targetdatasets, P2IMisnotreliantonit.WhileInferprovidesfor
amoreaccurateSARbound,asimilarlackofsignalawarenessin
AI-for-code models isstilldetected,albeit withalooser bound, by
replacing Infer with purely a line-based bug matcher (less accu-
rate;moremodelfavoring). Althoughusingtheoriginaldataset
labeler might be even more accurate, and expands the dataset and
task applicability of P2IM, but it can be a harder task especially
with human-in-the-loop kind of labelers. Finally, the existence of a
perfect checker precludes the need for AI for code analysis. Yet, to
introducesomeaccountabilityintoday’sAI-for-codemodels,we
arguefor at leastaSAR-like sanity check.
5 EVALUATION
Weusethefollowingmethodologyfortestingthesignalawareness
ofvulnerabilitydetectionmodels. Thetrainedmodelswillassign
labels to the code samples in the test set as either being vulnerable
ornot,baseduponthepredictedclassprobabilities.Afterprediction,
allvulnerablesamplesinthetestsetfallundereitherTruePositives
(TP)orFalseNegatives( FN). Wesubjecteach TPpredictedbythe
modeltoP2IMreduction.Weﬁrstquerythemodelforitsprediction
on eachTPsample’s 1-minimal version. Then we check the 1-
minimal for the presence ( TP’) or absence ( FN’) of the original
programsample’sbug.Thiswaywesubdivide TPintosignal-aware
TP’andsignal-agnostic FN’.
Operating atop vulnerable samples ( TP+FN),Recallis the best
metric to target, to fairly compare diﬀerent models on the same
dataset. This is because number of vulnerable samples = TP+FN=
TP’+FN’+FNwillbethesameforallmodelsforthesamedataset.
Wepresentanewmetric-Signal-awareRecall(SAR),tomeasure
thesignal-awarenessofvulnerabilitydetectionmodels.So,while
Recall=TP/ (TP+FN), SAR is deﬁned as TP’/ (TP’+FN’+FN),
whereTP=TP’+FN’. Then we compare the two metrics for each
model, to highlight how much of its reported Recall is attributable
to task-relevantsignal learning(reﬂectedbySAR).
Forscalability,werun P2IMreductionacrossmultiplesamplesin
parallel.Asanexampleoftheruntimecost,ittakes{min:3,avg:119,
max:884} secondsfor the Githubdataset samples,describednext.5.1 Datasets
P2IM’ssignalawarenessmeasurementrequiresdatasetswithground
truthbuglocations.DatasetsfromDraper[ 44]andDevign[ 68]are
excluded because they donot specify buglocations.Samples from
VulDeePecker[ 34]andSySeVR[ 33]areslicesconvertedintolinear
sequences, not valid compilable code which models are trained
uponandthusexcludedfromexperiments.Therefore,weusethe
following three datasets which contain this granularity of line-
levelbuginformation.The train:validate:test splitiskeptat
80:10:10 for allexperiments.
5.1.1 Juliet. TheJulietTestSuite[ 40]containssyntheticexamples
withdiﬀerentvulnerabilitytypes,designedfortestingstaticana-
lyzers. From its 64K test cases, we extract 118K functions, amongst
whichalmost35%arevulnerable.Samplestaggedas‘ bad’,andwith
clear bug information as per manifest.xml ﬁle, are labeled as 1,
whilethe ones witha‘ good’tag are labeledas 0.
5.1.2 s-bAbI . The authors of s-bAbI [ 47] claim that the Juliet
datasetisfartoosmallandcomplextouseinlearningtopredictthe
labeledsecuritydefects.Theirproposeds-bAbI syntheticdataset
contains syntactically-valid C programs with non-trivial control
ﬂow, focusing solely on the buﬀer overﬂow vulnerability. We used
thes-bAbI generatortocreateabalanceddatasetofalmost475K
functions.Sampleswitha‘ UNSAFE’tagarelabeledas1,andthose
with‘SAFE’as 0.
5.1.3 Github dataset. Diﬀerent from the synthetic s-bAbI and
Juliet datasets, we also include a real-word dataset with bug lo-
cationandbugtypeinformation,whichwederivefromtheD2A
dataset[66].D2Aistrace-leveldatasetbuiltovermultipleGithub
projects- OpenSSL,FFMpeg,HTTPD,Nginxandlibtiff. It is gen-
eratedbyusingdiﬀerentialanalysisatoptheInferstaticanalyzer
outputs of consecutive versions before and after bug-ﬁxing com-
mits. From D2A’s traces, we derive function-level samples. From
each before-ﬁx trace associated with a bug, we extract the func-
tionsspeciﬁedbythereportedbuglocationsandlabelthem as1.
Wealsoextractthecorrespondingfunctionsintheafter-ﬁxtrace
andlabelthosepatchedbythecorrespondingcommitas0.After
deduplication,we have 6728functionsintotal.
5.2 Models
We apply P2IMon three diﬀerent neural network architectures
whichhavebeenpopularlyemployedforvulnerabilitydetection.
Theseoperate upondiﬀerentrepresentations of sourcecode.
CNN:Thismodeltreatssourcecodeasaphotoandtriestolearnthe
pictorialrelationshipbetweensource codetokensand underlying
bugs. Similar to [ 44], we apply token normalization before feeding
data into the model. This involves normalizing the function names
andvariablenamestoﬁxedtokenssuchas FuncandVar.Wesetthe
embeddinglayerdimensionas13,followedbya2d-convolutional
layerwithinputchannelas1,outputchannelas512,andkernelsize
as(9,13). The ﬁnalprediction isgenerated bya3-layermultilayer
perceptron(MLP)withoutputdimensionsbeing 64, 16, and2.
RNN:This model treats code as a linear sequence of tokens and
tries to detect bugs in source code using the temporal relationship
between itstokens. We base ourRNN implementation on [ 33]. TheSahilSuneja, YunhuiZheng,YufanZhuang,JimA.Laredo,andAlessandro Morari
Table 2: Comparing AI-for-code models using standard as
wellasproposedSignal-AwareRecall(SAR)metric.Notethe
drop in model quality (Recall →SAR) when probing it for
signalawareness.
Dataset Model Accuracy F1 Recall SAR
s-bAbI CNN 97.5 97.4 95.4 58.4
s-bAbI RNN 97.6 97.5 95.9 59.6
s-bAbI GNN 99.6 99.6 99.7 62.7
Juliet CNN 97.9 96.9 99.9 77
Juliet RNN 98 97 98.6 76
Juliet GNN 99.6 99.4 99.9 76.8
Github CNN 60 58.9 54.9 46.2
Github RNN 57.1 60 61.8 52.4
Github GNN 59.3 64.2 69.8 60.8
inputfunctionisalsonormalizedduringpreprocessing,thesame
astheCNNmodel.Wesettheembeddinglayerdimensionas500,
followed by a two-layer bi-directional GRU module with hidden
sizeequalsto256,theﬁnalpredictionisgeneratedbyasingle-layer
MLP.
GNN:Insteadofborrowingtechniquesfromimageandtime-series
domain,thismodeloperatesatamorenaturalgraph-levelrepresen-
tationofsourcecode,asper[ 53,68].Ittriestolearnvulnerability
signatures in termsof relationships between nodes and edges of a
CodeProperty Graph[ 59]. Following [ 68], we do not apply token
normalization during preprocessing. We set the embedding size
as64,followedby aGGNN layer [ 32]with hidden size256and 5
unrolling time steps. The node representations are obtained via
summation of all node tokens’ embedding, and the graph represen-
tation read-out is constructedas aglobal attention layer. Theﬁnal
predictionisgeneratedbya2-layerMLPwithoutputdimensions
256and2.
ThemodelsaretrainedoverthedatasetspresentedinSection5.1,
withthevulnerabilitydetectionproblemframedasabinaryclas-
siﬁcation task– predicting program samples as healthy (label 0)
or buggy (label 1). For all of the models, we set dropout rate as
0.2duringtraining,andusedtheAdam[ 25]optimizer.Wetuned
learningratein{10−3,10−4}andbatchsizein{24,36,128,256,512}.
Models are trained with 100 maximum epochs and early stopping
(patience = 10). Cross entropy loss, with class weight calculated
fromthetrainingset,isemployed.Foreachexperiment,wesave
the checkpointwiththe bestvalidation loss.
5.3 Results
Table2comparestheperformanceofthethreemodelsundertest,
uponthethreedatasetsasdescribedearlier.Includedarethecom-
monmeasuresofthemodels’classiﬁcationperformance.Themodel
reproductionsachieveperformancesimilartopreviouswork[ 44,
53], including on the real-world dataset with performance compa-
rable to that of [ 68], which also creates its dataset from Github but
lacksbuglocation information.
The focus is on how Recall compares with the proposed SAR
metric.Ascanbeseen,evenforthesimplesynthetics-bAbI dataset,Table 3:P2IMreductionstats.
Dataset Model % Samples Average
Reduced Reduction%
s-bAbI CNN 100 41
s-bAbI RNN 100 42.7
s-bAbI GNN 100 41.9
Juliet CNN 85.9 38.8
Juliet RNN 85.8 38.9
Juliet GNN 86.2 38.9
Github CNN 87 45.9
Github RNN 88.5 45.5
Github GNN 87.1 43.6
whichtargetsonlyonevulnerabilitytype(buﬀeroverﬂow),a95+
Recalldropsintothesub-60rangeacrosstheboard,whenweprobe
the models for signal awareness with our P2IMapproach. This
indicates that the models are picking up features not relevant to
vulnerabilitydetection,presumablylearningdatasetnuanceswhich
inﬂatestheirreportedperformancemeasures.Theresultsaresimilar
for the otherdatasets as well.
Table3showsthereductionstatisticsobtainedwith P2IM.Col-
umn4showsthatasigniﬁcantreductionof 39%-46%canbeachieved
in the samples, without the models changing there prediction, and
theratesaresimilaracrossthediﬀerentmodelsforthesamedataset.
Furthermore,morethan85%ofthesamplescanbereducedwhile
maintainingtheirvulnerableprediction,acrossdatasetsandmodels.
The restare amixoftwocategories:
(1)The samples which cannot be reduced due to the valid code
requirementwhichweenforceon P2IM,soastoonlyfeed
real compilable subprograms to the models. For these, we
cannotascertainforsurewhetherthemodeltrulycaptures
the vulnerabilitysignals.
(2)Thesampleswhichthemodeltrulyneedsasisformaking
its prediction, signifyingapotentialtrue signal capture.
Weobservedthecompilablecoderequirementisthemainreason
why tokens cannot be reduced. Taking the example of CNN+Juliet,
amongstthe 14.1%ofTPs(534/3775)whichcannotbereduced, 89.1%
ofthem(476/534)haveWindowsheaderswhicharenotcompilable
inour(Linux)environment.Similarly,forCNN+Github,allofthe
13%TPswhichcannotbe reducedare dueto compilation failures.
Table 4 shows the overlap between the subset of the program
sampleswiththesamepredictionacrossdiﬀerentmodels.Onthe
two synthetic datasets, the overlap percentage is >90% for the true
positive samples ( TP), as well as for the signal-aware true positives
(TP’), across all three models. This is unexpected since the models
use vastly diﬀerent architectures. Combined with the low SAR
values from Table 2, this suggests that the perfect performance
on synthetic datasets is signiﬁcantly inﬂuenced by dataset-speciﬁc
nuances,whichallthreemodelsarepickingupinaverysimilarway,
and missing real vulnerability signals. The corresponding overlap
on the real-world dataset is much lower, partially due to the fact
that there are more variety and less artiﬁcial nuances in real-world
data for models to pick up, which however contributes to their
performance drop when compared to synthetic datasets, as shown
inTable2.Probing Model Signal-AwarenessviaPrediction-PreservingInput Minimization
Table 4: Overlap between the subset of samples with the
same prediction across diﬀerent models. Shown is the over-
lap percentage for the TP (as well as signal-aware TP’) sub-
setsacross the{CNN, RNN,GNN} models foreachdataset.
Dataset TP Overlap% TP’ Overlap%
s-bAbI 95.3 91
Juliet 97.4 96.9
Github 49 49.3
As a takeaway, with the addition of SAR to the existing arsenal
of model performance metrics, it becomes possible to measure
how much of the model’s learning is actually task-aware. This can
additionally provide a more fair comparison, and more accurate
improvement guidanceacrossmodelevolutions.
6 RELATED WORK
In the software engineering community, eﬀorts have been made to
detect vulnerabilities by isolating relevant statements using pro-
gram slicing based techniques [ 18,48,55,59]. Essentially, the main
ideaofsuchapproachesistoidentifythesubsetsoftheprogram
that introduce the defects. Recently, program slicing has also been
used in AI-assisted vulnerability detection tasks [ 34] to extract
bug relevant programmingconstructs. If a vulnerability detection
model can capture the real signals, it should be able to identify
such subsets too. In this work, we treat models as black boxes and
feedthemwithdiﬀerentsubsetsofaprogramtoevaluatehowwell
theypick upthe realsignals. Andtoeﬃciently andsystematically
generatethesubsets,weborrowapopularfaultisolationtechnique
ofDelta Debugging.
The most relevant work using Delta Debugging (DD) and its
variant methods is software failure diagnosis and isolation [ 17,38,
63,64].ThemainadvantageofDDisthatitcansigniﬁcantlyreduce
the number of tests needed to locate the problem. To the best of
our knowledge, our approach is the ﬁrst attempt of using delta
debuggingtointerpretandcomparemodels’signalawareness.A
recent parallel eﬀort [ 41] also uses DD to minimize inputs to AI
modelsonsoftwareengineeringtasks.Itsfocusisonthequalitative
properties of the reduced code samples, as opposed to our SAR-
based quantiﬁcation of the impact of signal-agnostic training on
the models’reportedperformance numbers.
P2IMcan be viewed as belonging to the metamorphic testing
paradigm [ 8], applied to AI models [ 20,69]. In particular, based on
an input code snippet and its prediction by a model under test, we
systematically construct new “tests” by minimizing the original
snippet and make sure the model produces the same prediction
as the original input. Then we check the metamorphic relations
among the inputs and output predictions of multiple executions. A
testviolationcanbedetectediftheoriginalinputanditsminimal
versiondonot have the same vulnerability.
Incontrasttoclassicalstatisticallearningmodelsthathaveex-
plicitinput&outputrelationshipanderrorbounds,itishardtoun-
derstandwhatdeeplearningmodelslearnandtoproviderobustnessguarantees forit. Toalleviate theproblem, researchershave devel-
oped methods to either probe the model’s gradient [ 46,50,52,67],
as used in a recent AI-for-code paper [ 44], or to ﬁt explainable
linear models around a small local region of the model’s prediction
boundary [ 16,36,43]. Explanation methods have also been created
for graphical neural networks. GAT [ 57] and CGCN [ 58] use atten-
tion mechanism to attribute edge importance, while GnnExplainer
[61] tries to mask out irrelevant edges and node features while
maintaining max mutual information between inputs and outputs.
To explain models via concepts and prototypes, [ 31] proposes a
method to learn the similarity between inputs and a small set of
prototypesduringtrainingviaanauto-encoderstructure.Ourap-
proachiscomplementarytotheseapproachesasittreatsmodels
as blackboxes andgenerates the precise minimalrepresentation of
an input that a model absolutely requires to arrive at its prediction.
Theaddedbeneﬁtisthat P2IMcanﬁnalizetheexplanationwithout
approximating the proper threshold for ranked features, which
isconsiderablymoreconvenientandactionableforend-users.In
addition, the emitted minimal sequence is valid compilable code
andthuscan be independentlycross-veriﬁedwithexistingsource
code analyzers.
Finally, with regard to deep learning models’ robustness and
reliability research, metrics have beenproposed for distance ratio
among samples in the same class and in all other classes [ 24]. This
is based on the intuition that, for a reliable model, inputs in the
same class should be closer to each other in the model’s latent
space, as compared to inputs in the other class. Extensive research
hasalsocontributedtoconstructingsophisticatedattack/defense
methods and evaluating accuracy under ℓ/u1D45D-norms bounded adver-
sarialperturbations[ 11,15,29,37,39,49].Diﬀerentfromcurrent
metrics, we directly probe the existence of true signal inside the
models’1-minimalrepresentations. To thebestof our knowledge,
our metric SARis the ﬁrst of its kind to evaluate deep learning
models’signal awareness inthis domain.
7 CONCLUSION
Inthispaper,wepresentaprediction-preservinginputminimiza-
tion approach called P2IMto evaluate and compare the signal
awareness of AI-for-code models. In particular, P2IMsystemati-
callyreducesaprogramsampletoaminimalsnippetwhichamodel
needs to arrive at and stick to its original vulnerable prediction. By
checkingiftheminimalsnippethasthesamevulnerabilityasthe
originalsample, P2IMmeasuresthemodel’srelianceonincorrect
signals. We apply P2IMon three state-of-the-art neural network
modelsacrossmultipledatasets,andmeasuretheirsignalawareness
usinganewmetricwepropose– Signal-awareRecall(SAR).The
resultsshowasharpperformancedrop,whichsuggeststhemod-
elsarepresumablylearningalotofnoisesordatasetnuances,as
opposed to capturingvulnerability-related signals. SAR augments
thetraditionalmeasuresofmodelperformancewith anewmetric
tomeasure task-relevantlearning, which can more fairly compare
andguide improvements acrossmodelevolutions.SahilSuneja, YunhuiZheng,YufanZhuang,JimA.Laredo,andAlessandro Morari
REFERENCES
[1]Miltiadis Allamanis. 2019. The adverse eﬀects of code duplication in machine
learningmodelsofcode.In Proceedingsofthe2019ACMSIGPLANInternational
SymposiumonNewIdeas,NewParadigms,andReﬂectionsonProgrammingand
Software. 143–153.
[2]MiltiadisAllamanis, EarlT.Barr, ChristianBird, and CharlesSutton.2015. Sug-
gesting Accurate Method and Class Names. In Proceedings of the 2015 10th Joint
Meeting on Foundations of Software Engineering (Bergamo, Italy) (ESEC/FSE 2015) .
Associationfor Computing Machinery, NewYork, NY, USA,38–49.
[3]MiltiadisAllamanis,MarcBrockschmidt,andMahmoudKhademi.2018. Learning
to RepresentProgramswith Graphs. arXiv:1711.00740 [cs.LG]
[4]Miltiadis Allamanis, Hao Peng, and Charles Sutton. 2016. A Convolutional
AttentionNetworkforExtremeSummarizationofSourceCode.In Proceedingsof
the 33nd International Conference on Machine Learning, ICML 2016, June 19-24,
2016, Vol. 48.JMLR.org,NewYork, NY, USA,2091–2100.
[5]Rohan Bavishi,Michael Pradel, and Koushik Sen. 2018. Context2Name: ADeep
Learning-Based Approach to Infer Natural Variable Names from Usage Contexts.
arXiv:1809.05193 [cs.SE]
[6]Nicholas Carlini and David Wagner. 2017. Towards evaluating the robustness
of neural networks. In 2017 ieee symposium on security and privacy (sp) . IEEE,
39–57.
[7]Saikat Chakraborty, Rahul Krishna, Yangruibo Ding, and Baishakhi Ray. 2020.
Deep Learning based Vulnerability Detection: Are We There Yet? arXiv preprint
arXiv:2009.07235 (2020).
[8]T.Y.Chen,S.C.Cheung,andS.M.Yiu.1998. MetamorphicTesting:ANewApproach
for Generating Next Test Cases . Technical Report. Department of Computer
Science, The HongKong Universityof Science and Technology.
[9]JunyoungChung,CaglarGulcehre,KyungHyunCho,andYoshuaBengio.2014.
Empirical evaluation of gated recurrent neural networks on sequence modeling.
arXiv preprint arXiv:1412.3555 (2014).
[10]Arun Das and Paul Rad. 2020. Opportunities and Challenges in Explainable
Artiﬁcial Intelligence(XAI): A Survey. arXiv:2006.11371 [cs.CV]
[11]Logan Engstrom, Brandon Tran, Dimitris Tsipras, Ludwig Schmidt, and Alek-
sanderMadry.2019.ExploringtheLandscapeofSpatialRobustness.In Proceedings
of the 36th International Conference on Machine Learning (Proceedings of Machine
Learning Research,Vol.97) . PMLR, LongBeach,California, USA,1802–1811.
[12] Facebook.[n.d.]. Infer Static Analyzer. https://fbinfer.com/.
[13]Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E
Dahl. 2017. Neural message passing for quantum chemistry. In International
Conference onMachineLearning . PMLR, 1263–1272.
[14]Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. 2014. Explaining and
harnessing adversarial examples. arXiv preprint arXiv:1412.6572 (2014).
[15]Ian J. Goodfellow, Jonathon Shlens, and Christian Szegedy. 2015. Explaining and
Harnessing Adversarial Examples. arXiv:1412.6572[stat.ML]
[16]WenboGuo,DongliangMu,JunXu,PuruiSu,GangWang,andXinyuXing.2018.
LEMNA: ExplainingDeepLearningBased Security Applications.In Proceedings
ofthe2018ACMSIGSACConferenceonComputerandCommunicationsSecurity
(Toronto, Canada) (CCS’18).AssociationforComputingMachinery,NewYork,
NY, USA,364–379.
[17]Neelam Gupta, Haifeng He, Xiangyu Zhang, and Rajiv Gupta. 2005. Locating
Faulty Code Using Failure-Inducing Chops. In Proceedings of the 20th IEEE/ACM
International ConferenceonAutomatedSoftware Engineering (Long Beach,CA,
USA)(ASE ’05) . Association for Computing Machinery, New York, NY, USA,
263–272.
[18]BehnazHassanshahi,YaoqiJia,RolandH.C.Yap,PrateekSaxena,andZhenkai
Liang. 2015. Web-to-Application Injection Attacks on Android: Characterization
and Detection. In Computer Security – ESORICS 2015 . Springer International
Publishing,Cham,577–598.
[19]KaimingHe,XiangyuZhang,ShaoqingRen,andJianSun.2016. Deepresidual
learning for image recognition. In Proceedings of the IEEE conference on computer
vision and pattern recognition . 770–778.
[20]Pinjia He, Clara Meister, and Zhendong Su. 2020. Structure-invariant testing
formachinetranslation.In ICSE’20:42ndInternationalConferenceonSoftware
Engineering, Seoul, South Korea, 27 June -19 July,2020 . ACM,961–973.
[21]SeppHochreiterandJürgenSchmidhuber.1997. Longshort-termmemory. Neural
computation 9,8 (1997), 1735–1780.
[22]Srinivasan Iyer, Ioannis Konstas, Alvin Cheung, and Luke Zettlemoyer. 2016.
Summarizing Source Code using a Neural Attention Model. In Proceedings of the
54th AnnualMeetingoftheAssociationforComputational Linguistics .Association
for Computational Linguistics,Berlin, Germany, 2073–2083.
[23]Matthieu Jimenez, Renaud Rwemalika, Mike Papadakis, Federica Sarro, Yves
Le Traon, and Mark Harman. 2019. The importance of accounting for real-world
labellingwhenpredictingsoftwarevulnerabilities.In Proceedingsofthe201927th
ACM Joint Meeting on European Software Engineering Conference and Symposium
onthe FoundationsofSoftwareEngineering . 695–705.
[24]BhavyaKailkhura,BrianGallagher,SookyungKim,AnnaHiszpanski,andTYong-
JinHan.2019. Reliableandexplainablemachine-learningmethodsforaccelerated
material discovery. NPJ ComputationalMaterials 5 (2019), 1–9.[25]DiederikP.KingmaandJimmyBa.2017. Adam:AMethodforStochasticOpti-
mization. arXiv:1412.6980[cs.LG]
[26]ThomasNKipfandMaxWelling.2016. Semi-supervisedclassiﬁcationwithgraph
convolutional networks. arXiv preprint arXiv:1609.02907 (2016).
[27]Ugur Koc, Parsa Saadatpanah, Jeﬀrey S. Foster, and Adam A. Porter. 2017. Learn-
ing a Classiﬁer for False Positive Error Reports Emitted by Static Code Analysis
Tools. In Proceedings of the 1st ACM SIGPLAN International Workshop on Ma-
chine Learning and Programming Languages (Barcelona, Spain) (MAPL 2017) .
Associationfor Computing Machinery, NewYork, NY, USA,35–42.
[28]Alex Krizhevsky, Ilya Sutskever, and Geoﬀrey E Hinton. 2012. Imagenet classiﬁ-
cation with deep convolutional neural networks. Advances in neural information
processingsystems 25(2012), 1097–1105.
[29]Alexey Kurakin, Ian Goodfellow, and Samy Bengio. 2017. Adversarial examples
in the physical world. arXiv:1607.02533 [cs.CV]
[30]Alexander LeClair, Sakib Haque, Lingfei Wu, and Collin McMillan. 2020. Im-
proved code summarization via a graph neural network. In Proceedings of the
28thInternationalConference onProgramComprehension . 184–195.
[31]OscarLi,HaoLiu,ChaofanChen,andCynthiaRudin.2017. DeepLearningfor
Case-Based Reasoning through Prototypes: A Neural Network that Explains Its
Predictions. arXiv:1710.04806 [cs.AI]
[32]Yujia Li, Daniel Tarlow, Marc Brockschmidt, and Richard Zemel. 2017. Gated
GraphSequenceNeural Networks. arXiv:1511.05493 [cs.LG]
[33]ZhenLi,DeqingZou,ShouhuaiXu,HaiJin,YaweiZhu,andZhaoxuanChen.2018.
SySeVR:AFrameworkforUsingDeepLearningtoDetectSoftwareVulnerabilities.
arXiv:1807.06756 [cs.LG]
[34]Zhen Li, Deqing Zou, Shouhuai Xu, Xinyu Ou, Hai Jin, Sujuan Wang, Zhijun
Deng, and Yuyi Zhong. 2018. VulDeePecker: A Deep Learning-Based System for
Vulnerability Detection. In 25th Annual Network and Distributed System Security
Symposium, NDSS 2018, San Diego, California, USA, February 18-21, 2018 . The
Internet Society, Reston,VA.
[35]Pengfei Liu, Xipeng Qiu, and Xuanjing Huang. 2016. Recurrent neural network
fortextclassiﬁcationwithmulti-tasklearning. arXivpreprintarXiv:1605.05101
(2016).
[36]ScottMLundbergandSu-InLee.2017. Auniﬁedapproachtointerpretingmodel
predictions.In AdvancesinNeural InformationProcessingSystems,NeurIPS’17 .
[37]Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and
Adrian Vladu. 2018. Towards Deep Learning Models Resistant to Adversarial
Attacks. In InternationalConference onLearning Representations .
[38]Ghassan Misherghi and Zhendong Su. 2006. HDD: Hierarchical Delta Debug-
ging. InProceedings of the 28th International Conference on Software Engineering,
ICSE’06.
[39]Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, and Pascal Frossard. 2016.
DeepFool: A Simple and Accurate Method to Fool Deep Neural Networks. In
2016IEEE Conference onComputer Visionand PatternRecognition,CVPR’16 .
[40]NIST. 2017. Juliet Test Suite for C/C++ Version 1.3.
https://samate.nist.gov/SRD/testsuite.php.
[41] MdRaﬁqulIslam Rabin,Vincent JHellendoorn,and MohammadAminAlipour.
2021. Understanding Neural Code Intelligence Through Program Simpliﬁcation.
arXiv preprint arXiv:2106.03353 (2021).
[42]Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. 2015. Faster r-cnn:
Towardsreal-timeobjectdetectionwithregionproposalnetworks. arXivpreprint
arXiv:1506.01497 (2015).
[43]MarcoTulioRibeiro,SameerSingh,andCarlosGuestrin.2016. "WhyshouldI
trustyou?"Explainingthepredictionsofanyclassiﬁer.In Proceedingsofthe22nd
ACMSIGKDDinternationalconferenceonknowledgediscoveryanddatamining,
KDD’16.
[44]Rebecca L.Russell,LouisY. Kim,LeiH.Hamilton, TomoLazovich, JacobHarer,
OnurOzdemir,PaulM.Ellingwood,andMarcW.McConley.2018. Automated
Vulnerability Detection in Source Code Using Deep Representation Learning.
In17th IEEE International Conference on Machine Learning and Applications,
ICMLA’18 .
[45]HasimSak,AndrewWSenior,andFrançoiseBeaufays.2014. Longshort-term
memoryrecurrentneuralnetworkarchitecturesforlargescaleacousticmodeling.
(2014).
[46]Ramprasaath R Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedan-
tam,DeviParikh,andDhruvBatra.2017. Grad-cam:Visualexplanationsfrom
deep networks via gradient-based localization. In Proceedings of the IEEE interna-
tional conference oncomputer vision, ICCV’17 .
[47]CarsonD.Sestili,WilliamS.Snavely,andNathanM.VanHoudnos.2018. Towards
security defect prediction with AI. CoRRabs/1808.09897 (2018). http://arxiv.org/
abs/1808.09897
[48]L.K.SharandH.B.K.Tan.2012. Predictingcommonwebapplicationvulnerabil-
ities from input validation and sanitization code patterns. In 2012 Proceedings of
the 27th IEEE/ACM International Conference on Automated Software Engineering,
ASE’12.
[49]Mahmood Sharif, Sruti Bhagavatula, Lujo Bauer, and Michael K Reiter. 2016.
AccessorizetoaCrime:RealandStealthyAttacksonState-of-the-ArtFaceRecog-
nition. In Proceedings of the 2016 ACM SIGSAC Conference on Computer andProbing Model Signal-AwarenessviaPrediction-PreservingInput Minimization
Communications Security, CCS’16 .
[50]AvantiShrikumar,PeytonGreenside,andAnshulKundaje.2017. LearningIm-
portant Features Through Propagating Activation Diﬀerences. In Proceedings of
the 34thInternationalConference onMachineLearning, ICML’17 .
[51]KarenSimonyanandAndrewZisserman.2014.Verydeepconvolutionalnetworks
for large-scaleimage recognition. arXiv preprint arXiv:1409.1556 (2014).
[52]MukundSundararajan,AnkurTaly,andQiqiYan.2017. Axiomaticattribution
fordeepnetworks.In Proceedingsofthe34thInternationalConferenceonMachine
Learning, ICML’17 .
[53]Sahil Suneja, Yunhui Zheng, Yufan Zhuang, Jim Laredo, and Alessandro Morari.
2020. Learning to map source code to software vulnerability using code-as-a-
graph.CoRRabs/2006.08614 (2020). https://arxiv.org/abs/2006.08614
[54]ChristianSzegedy,WojciechZaremba,IlyaSutskever,JoanBruna,DumitruErhan,
Ian Goodfellow, and Rob Fergus. 2013. Intriguing properties of neural networks.
arXiv preprint arXiv:1312.6199 (2013).
[55]JulianThomé,LwinKhinShar,DomenicoBianculli,andLionelC.Briand.2018.
Securityslicingforauditingcommoninjectionvulnerabilities. JournalofSystems
and Software 137(2018), 766–783.
[56]OmerTripp,SalvatoreGuarnieri,MarcoPistoia,andAleksandrAravkin.2014.
ALETHEIA:ImprovingtheUsabilityofStaticSecurityAnalysis.In Proceedings
ofthe 2014 ACM SIGSACConference onComputer and Communications Security,
CCS’14.
[57]PetarVeličković,GuillemCucurull,Arantxa Casanova,AdrianaRomero,Pietro
Liò,and YoshuaBengio. 2018. GraphAttentionNetworks.In InternationalCon-
ference onLearning Representations, ICLR’18 .
[58]Tian Xie and Jeﬀrey C Grossman. 2018. Crystal graph convolutional neural
networks for an accurate and interpretable prediction of material properties.
Physical reviewletters 120, 14(2018).
[59]FabianYamaguchi,NicoGolde,DanielArp,andKonradRieck.2014.Modelingand
DiscoveringVulnerabilitieswithCodePropertyGraphs.In 2014IEEESymposium
onSecurityand Privacy, SP’14 .
[60]NoamYefet,UriAlon,andEranYahav.2020. Adversarialexamplesformodels
ofcode.ProceedingsoftheACMonProgrammingLanguages 4,OOPSLA(2020),1–30.
[61]Rex Ying, Dylan Bourgeois, Jiaxuan You, Marinka Zitnik, and Jure Leskovec.
2019. GNNExplainer: Generating Explanations for Graph Neural Networks.
arXiv:1903.03894 [cs.LG]
[62]Ulas Yüksel and Hasan Sözer. 2013. Automated Classiﬁcation of Static Code
Analysis Alerts: A Case Study. In Proceedings of the 2013 IEEE International
Conference on Software Maintenance (ICSM ’13) . IEEE Computer Society, USA,
532–535.
[63]AndreasZeller.2002. IsolatingCause-EﬀectChainsfromComputerPrograms.
InProceedingsofthe10thACMSIGSOFTSymposiumonFoundationsofSoftware
Engineering (Charleston,SouthCarolina,USA) (SIGSOFT’02/FSE-10) .Association
for Computing Machinery, NewYork, NY, USA,1–10.
[64]Andreas Zeller and Ralf Hildebrandt. 2002. Simplifying and Isolating Failure-
InducingInput. IEEE Trans. SoftwareEngineering 28,2 (Feb. 2002),183–200.
[65]Muhan Zhang and Yixin Chen. 2018. Link prediction based on graph neural
networks. arXiv preprint arXiv:1802.09691 (2018).
[66]YunhuiZheng,SaurabhPujar,BurnLewis,LucaBuratti,EdwardEpstein,BoYang,
JimLaredo,AlessandroMorari,andZhongSu.2021. D2A:ADatasetBuiltforAI-
BasedVulnerabilityDetectionMethodsUsingDiﬀerentialAnalysis.In Proceedings
of the ACM/IEEE 43rd International Conference on Software Engineering: Software
EngineeringinPractice (ICSE-SEIP’21) .AssociationforComputingMachinery,
NewYork, NY, USA.
[67]B. Zhou, A. Khosla, A.Lapedriza, A.Oliva, andA. Torralba. 2016. LearningDeep
FeaturesforDiscriminativeLocalization.In 2016IEEEConferenceonComputer
VisionandPatternRecognition(CVPR) .IEEEComputerSociety,LosAlamitos,CA,
USA,2921–2929.
[68]Yaqin Zhou, Shangqing Liu, Jingkai Siow, Xiaoning Du, and Yang Liu. 2019. De-
vign: Eﬀective Vulnerability Identiﬁcation by Learning Comprehensive Program
SemanticsviaGraphNeuralNetworks.In AdvancesinNeuralInformationProcess-
ingSystems(NeurIPS) 32 . Curran Associates,Inc.,Red Hook,NY, 10197–10207.
[69]ZhiQuanZhouandLiqunSun.2019. MetamorphicTestingofDriverlessCars.
Commun. ACM 62,3 (Feb. 2019),61–67.