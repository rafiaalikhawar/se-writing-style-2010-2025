CP-Detector: Using Configuration-related Performance
Properties to Expose Performance Bugs
Haochen He
National University of Defense
Technology, China
hehaochen13@nudt.edu.cnZhouyang Jia‚àó
National University of Defense
Technology, China
jiazhouyang@nudt.edu.cnShanshan Li
National University of Defense
Technology, China
shanshanli@nudt.edu.cn
Erci Xu
National University of Defense
Technology, China
xuerci@nudt.edu.cnTingting Yu
University of Kentucky
Lexington, KY, USA
tyu@cs.uky.eduYue Yu
National University of Defense
Technology, China
yuyue@nudt.edu.cn
Ji Wang
National University of Defense
Technology, China
wj@nudt.edu.cnXiangke Liao
National University of Defense
Technology, China
xkliao@nudt.edu.cn
ABSTRACT
Performance bugs are often hard to detect due to their non fail-
stop symptoms. Existing debugging techniques can only detect
performance bugs with known patterns (e.g., inefficient loops). Thekeyreasonbehindthisincapabilityisthelackofageneraltestoracle.
Here, we argue that the performance (e.g., throughput, latency,
execution time) expectation of configuration can serve as a strong
oracle candidate for performance bug detection. First, prior work
shows that most performance bugs are related to configurations.
Second, the configuration change reflects common expectation on
performance changes. If the actual performance is contrary to the
expectation, the related code snippet is likely to be problematic.
In this paper, we first conducted a comprehensive study on
173 real-world configuration-related performance bugs (CPBugs)
from 12 representative software systems. We then derived seven
configuration-related performance properties, which can serve as
the test oracle in performance testing. Guided by the study, we
designed and evaluated an automated performance testing frame-
work, CP-Detector, for detecting real-world configuration-related
performance bugs. CP-Detector was evaluated on 12 open-source
projects. The results showed that it detected 43 out of 61 existing
bugs and reported 13 new bugs.
CCS CONCEPTS
‚Ä¢Software and its engineering ‚ÜíSoftware performance.
‚àóCo-first author
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation
on the first page. Copyrights for components of this work owned by others than ACMmustbehonored.Abstractingwithcreditispermitted.Tocopyotherwise,orrepublish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
ASE‚Äô20,September 21‚Äì25, 2020, Virtual Event, Australia
¬© 2020 Association for Computing Machinery.
ACM ISBN 978-1-4503-6768-4/20/09...$15.00
https://doi.org/10.1145/3324884.3416531KEYWORDS
Performance bug detection, Software configuration, Performance
property
ACM Reference Format:
Haochen He, Zhouyang Jia, Shanshan Li, Erci Xu, Tingting Yu, Yue Yu, Ji
Wang, and Xiangke Liao. 2020. CP-Detector: Using Configuration-related
Performance Properties to Expose Performance Bugs. In 35th IEEE/ACM
International Conference on Automated Software Engineering (ASE ‚Äô20), Sep-
tember 21‚Äì25, 2020, Virtual Event, Australia. ACM, New York, NY, USA,
12pages.https://doi.org/10.1145/3324884.3416531
1 INTRODUCTION
Modern software systems are increasingly configurable and thus
becoming more adaptive to various scenarios. However, as config-
uration can (in)directly affect performance (e.g., altering resource
allocation), performance bugs‚Äô occurrences are also surging. A re-
cent study suggests more than half (59%) of performance bugs
are due to incorrect handling of configurations [ 33]. In this pa-
per, we term these performance bugs as Configuration-handling
Performance Bugs (CPBug). Note that a CPBug is different from
a misconfiguration where the former focuses on incorrect config-
uration handling in source code and the latter revolves around
user-induced configuration errors.
Figure1illustrates a real-world CPBug [ 17] related to the config-
uration option sort_buffer_size in MySQL, as well as the failure
symptom,rootcause,andfixmethod.Thisoptionisusedtoalterthe
buffer size for sorted results (triggered by GROUP BY z DESC ). Ide-
ally, a larger buffer should improve the sorting performance, since
MySQL can cache more results. However, for the SQL query in the
upper-right corner of Figure 1, users actually suffer from up to 4.2 √ó
slowdown instead of benefiting from larger buffers (i.e., increasefrom
2Mto8M). The root cause is redundant memory allocation.
Specifically, the buffer is allocated before each sub-query (Line 1),
and freed immediately at the end of the sub-query (Line 5). The fix
method is to allocate the buffer at the first sub-query (Line 2-3) and
reuse it at all subsequent sub-queries (Line 6-7). This bug serves as
UI*&&&"$.*OUFSOBUJPOBM$POGFSFODFPO"VUPNBUFE4PGUXBSF&OHJOFFSJOH	"4&
%
" 
"!  !
//


	
SELECT if
	 SELECT
1 - sort_keys = my_malloc 
(sort_buffer_size, ‚Ä¶); 
2 + if(!table_sort.sort_keys) 
3 +    sort_keys = my_malloc 
(sort_buffer_size, ‚Ä¶); 
4    ... 
5 - x_free(sort_keys);  6 + if(!subselect)  
7 +    x_free(sort_keys);
 !	

‚ÄúSELECT a, b, (SELECT 
x FROM t2 WHERE y=b 
ORDER BY z DESC LIMIT 
1) c FROM t1‚Äù 

  $        

 sort_buffer_size 


#!$	
	

	SELECT.
Figure 1: Redundant memory allocation in MySQL. This bug
happensbecause MySQL allocates memory in each sub-SELECT.
a representative example of performance bugs caused by incorrect
configuration handling. This is different from a user misconfigura-
tion, where users may suffer from performance degradations when
setting up a buffer size larger than the memory limitation.
There has been much research on detecting performance bugs.
Someresearchhasproposedprofiling-basedtechniques,whichaims
to detect performance bottlenecks that can cause significant slow-
downs [28, 55,67]. However, such slowdowns can be self-induced
due to necessarily intensive computation. The lack of test oracles
makes it difficult to decide if a slowdown indicates a performance
bug [46]. Alternatively, some research has proposed to use ineffi-
cient code patterns [ 46,59,61,65,66] as test oracles. For example,
Toddler [46] identifies loops with inefficiency memory-access pat-
terns which imply potential performance bugs. These works are
hard to detect CPBugs, which may or may not contain the patterns.
There has also been some research focuses on the relationship
between configuration and performance, including configuration-
performance modeling [ 35,56], and configuration-based perfor-
mance tuning [ 43,47]. The former aims to predict performance
for given configurations, while the latter studies the tendency of
performance changes when tuning configurations, and finds the
optimal configurations with regard to the performance. The perfor-
mance tendency can be obtained by sampling configuration values
and fitting corresponding performances. This is different from a
performance bug, which may be triggered by a certain value.
In this paper, we propose CP-Detector1, an automated testing
framework to detect CPBugs. The key insight of CP-Detector is
that when tuning a configuration option, a mismatch between
the expected and actual performance changes usually indicates a
CPBug.Forinstance,userswouldexpectperformanceimprovement
when allocating a larger buffer. If, however, the actual performance
drops, the mismatch between the expected and actual performance
changesmayindicateaCPBug.Thiskindofperformanceproperties
can be used as test oracles to expose CPBugs. For example, the
propertyinFigure 1canbedescribedas"increasingresource-related
configuration options should improve the performance".
To understand CPBugs and guide the design of CP-Detector,
we first conducted an empirical study on 173 real-world CPBugs
from 12 software systems. We found that 150 (86.7%) CPBugs can
1The annotated bug data set and tool can be found in our publicly available repository:
https://github.com/TimHe95/CP-Detectorbe exposed by detecting mismatches between the expected andactual performance changes when tuning configuration options.
We further studied the configuration options involved in the 150
CPBugs, and summarized seven performance properties from these
options. Each property can be formalized as a three-tuple: < Type,
Direction ,Expectation >, indicating when tuning configuration
options of a given Type(e.g., resource) according to the Direction
(e.g., increasing), the software should have the Expectation (e.g.,
improving) performance change.
CP-Detector contains two major steps to automate the process
ofexposingCPBugs.Givenaconfigurationoption:1)CP-Detector
suggests the performance properties that the option should hold
by learning configuration documentation. The challenge is to un-
derstand the natural languages and build relationships between
the languages and the properties. To address this, CP-Detector
applies natural language processing (NLP) and association rule
mining (ARM) techniques to automatically derive the properties
from the documentation. 2) CP-Detector samples value pairs of
the option and test if one value pair can expose a CPBug. This is
challenging when the option is numeric, since the value range may
be extremely large; thus, it is hard to test all pairs. In this regard,
we conduct an empirical study on numeric options to investigate
the value ranges that can trigger CPBugs, and propose a heuristic
sampling strategy to reduce the sampling space.
To evaluate the effectiveness of CP-Detector in detecting CP-
Bugs, we reproduced 38 known CPBugs from the 173 CPBugs in
our study, and to avoid over-fitting, we also reproduced 23 known
CPBugs not included in our study (61 CPBugs in total so-far all we
can reproduce), and evaluated CP-Detector on all 61 bugs. The
results show that CP-Detector can successfully expose 43 bugs
using the suggested performance properties. The rest cases escape
mainlybecausetheinformationprovidedbyconfigurationmanuals
is limited, so CP-Detector can not make the right suggestion on
the performance properties. On the other hand, Toddler [ 46], one
of the most effective bug detection tools among existing works,
detected 6 out of the 61 CPBugs. In the meantime, CP-Detector
detected 13 unknown CPBugs on the same set of software projects.
We have reported the 13 bugs to developers, and nine of them have
already been confirmed or fixed at the time of writing.
In summary, this paper makes the following contributions:
‚Ä¢We conducted an empirical study on 173 real-world CPBugs
from12softwaresystems.Thefindingsareusedtoguidethe
design of test inputs and test oracles.
‚Ä¢We designed and implemented CP-Detector, an automated
framework to detect CPBugs. CP-Detector can automati-
cally suggest performance properties for configuration op-
tions and generate configuration values to expose CPBugs.
‚Ä¢We evaluated CP-Detector on 12 software systems. The
resultsshowthatCP-Detectordetected43outof61known
CPBugs and 13 unknown bugs. Ten of the unknown bugs
have been confirmed or fixed by developers.
2 UNDERSTANDING CPBUGS
In this section, we take an in-depth look into the CPBugs through
an empirical study. We manually collected real-world CPBugs from
bug tracking systems, mailing lists, and fix commits of 12 software
Table 1: Software systems and CPBugs used in the studies.
Software #KW‚Ä†# CPBugs Software #KW # CPBugs
MySQL 398 35 Httpd 291 16
MariaDB 451 25 H2 17 1
MongoDB 425 26 Squid 4 1
PostgreSQL 14 5 Tomcat 25 3
RocksDB 16 3 GCC 618 39
Derby 18 3 Clang 192 16
‚Ä†KW: The CPBug candidates identified using performance-related keywords.
systems. In these CPBugs, we found a majority of configuration
options have expected performance changes when being tuned. We
alsostudiedtheperformancepropertiesoftheconfigurationoptions
involved in these CPBugs, and how the option values triggered the
CPBugs.Ourfindingsareusedtoguidethedesignof CP-Detector.
Studied Subjects. Table1shows the 12 software systems used
in our study. These projects cover a variety of domains, including
database,webserver,andcompiler.Theseprojectsareperformance-
critical, highly-configurable (e.g., Httpd has more than 1,100 con-
figuration options), and widely deployed in the field. Therefore,
CPBugs from these projects are likely to be rich in numbers and
severe in consequences [ 36]. Also, these projects are open-source
and well maintained by the community. This allows us to not only
check the buggy code snippets but also gather related details based
on the developers‚Äô discussions.
CPBug Collection. We collected CPBugs from three sources:
bug tracking systems (e.g., JIRA, Bugzilla), mailing lists, and fix
commits.Westartedbysearchingtheabovesourcesusingheuristic
keywords (e.g., "slow", "long time", "performance"). This process
identified2,469candidates.Wemanuallyanalyzedthesecandidates,
eachofwhichisdeemedasaCPBugiftuningaconfigurationoption
would cause a performance bug. This process yielded 173 (columns
3 and 6 in Table 1) CPBugs.
2.1 Prevalence of Unexpected Performance
Changes in CPBugs
Each CPBug has one or multiple triggering configuration option(s).
For each option, we use expected performance changes of two
values of the configuration option as performance properties, and
exposeCPBugsbydetectingviolationsoftheproperties.Toevaluate
to what extent the property-based approach can expose CPBugs,
we study the prevalence of unexpected performance changes when
tuning configuration options of the CPBugs.
Toachievethis,wemanuallystudiedallthe173CPBugscollected
above,andfound150(86.7%)ofthemhaveunexpectedperformance
changes when tuning configuration options. This result indicates
the performance expectation can serve as an effective oracle for
exposing CPBugs. The remaining 23 (13.3%) CPBugs cannot be ex-
posed mainly because the configuration options have inconsistent
expectations. For example, innodb_fill_factor defines the per-
centage of space that is filled during a sorted index build, with the
remaining space reserved for future index growth. In production,
it should be carefully tuned according to the workloads and hard-
ware. While in MySQL-74325 [ 19], when setting it to 100, indexedUPDATEs get 3.8√óslower because page split has to be performed
for every UPDATEdue to no empty space for the changed index.
Developer fixes this by preserving 1/16 of the space for any of
its value. Increasing or decreasing innodb_fill_factor does not
necessarily have common expected performance change.
Finding1 :Amajority(87.6%)oftheCPBugshaveunexpected
performance changes when tuning configuration options.
This result indicates the performance expectation can serve
as an effective oracle for exposing CPBugs.
2.2 Performance Properties in CPBugs
We studied the 150 CPBugs to understand the performance prop-
erties of the bug-introducing configuration options. The findings
can be used to guide the design of CP-Detector for automatically
extracting the properties to detect CPBugs for any target software.
We manually studied the semantics of the configuration options
involved in the CPBugs and summarized five semantic types, i.e.,
optimizationon-off, non-functionaltradeoff, resourceallocation, func-
tionalityon-off , andnon-influenceoption . Each configuration type
has two tuning directions, e.g., turning on and turning off, or in-
creasing and decreasing. Then, we analyzed the CPBugs and found
bothnon-functionaltradeoff andfunctionalityon-off options trig-
gered CPBugs in two directions, while other types only triggered
CPBugs in one direction. After that, we analyzed the expected and
actual performance changes from the bug descriptions for each
direction of each type.
The result is shown in Table 2, each performance property is
associated with its configuration type (Column 2), tuning direc-
tion (Column 3-4), and expected performance change (Column 5).
Thesethreepartscorrespondtothefactorsofthethree-tuple< Type,
Direction ,Expectation > defined in ¬ß 1. Besides the properties,
Column 6 shows the actual performance change of CPBugs, andColumn 7 shows the numbers of CPBugs that break each prop-
erty. For example, the first configuration type is optimizationon-off,
when tuning an optimization option from OFF to ON, it means
turning on an optimization strategy, and the performance is ex-
pected to be enhanced. If, however, the actual performance drops,
there is a potential CPBug. In our dataset, 18 CPBugs violate this
property. Below, we provide details of the properties in each type
of configuration options.
Optimizationon-off. In this category, a configuration option
is used to control an optimization strategy. Specifically, when the
optimizationisturnedon,theapplication‚Äôsperformanceisexpected
tobeimproved.ConsideraCPBugexample,MySQL-67432[ 18].For
SQL queries like " SELECT * FROM t WHERE c1<100 AND (c2<100
OR c3<100) ", MySQL can speed up at least 10% by enabling the
optimization strategy index_merge=ON , which can merge the in-
dexes of different columns. However, when the queries end with
"ORDER BY c1 LIMIT 10 ", the performance degrades by 10 √ówith
the optimization turned on. This is because the whole indexes of
the columns are merged (i.e., two index range scans, a merge, and
a ‚Äújoin‚Äù), whereas only the top 10 rows are required. Developers fix
this bug by changing the triggering conditions of the optimization
strategy controlled by index_merge.
Table 2: Performance Properties in CPBugs.
PP-IDPerformance Properties (PP) CPBugs
Configuration
Option TypeTuning Direction Expected
Performance ChangeActual
Performance Change# CPBugs
(Pct.) Source Value Target Value
PP-1 Optimization on-off OFF ON Rise (‚Üë‚Üë‚Üë‚Üë‚Üë‚Üë‚Üë‚Üë‚Üë‚Üë‚Üë‚Üë‚Üë‚Üë‚Üë‚Üë‚Üë) Drop (‚Üì‚Üì‚Üì‚Üì‚Üì‚Üì‚Üì‚Üì‚Üì‚Üì‚Üì‚Üì‚Üì‚Üì‚Üì‚Üì‚Üì) 18 (12.0%)
PP-2 Non-functional tradeoff Anti-performance‚Ä†Pro-performance‚Ä°Rise (‚Üë‚Üë‚Üë‚Üë‚Üë‚Üë‚Üë‚Üë‚Üë‚Üë‚Üë‚Üë‚Üë‚Üë‚Üë‚Üë‚Üë) Drop (‚Üì‚Üì‚Üì‚Üì‚Üì‚Üì‚Üì‚Üì‚Üì‚Üì‚Üì‚Üì‚Üì‚Üì‚Üì‚Üì‚Üì) 35 (23.3%)
PP-3 Non-functional tradeoff Pro-performance Anti-performance Drop (‚Üì‚Üì‚Üì‚Üì‚Üì‚Üì‚Üì‚Üì‚Üì‚Üì‚Üì‚Üì‚Üì‚Üì‚Üì‚Üì‚Üì) More-than-expected drop ( ‚Üì‚Üì‚Üì‚Üì‚Üì‚Üì‚Üì‚Üì‚Üì‚Üì‚Üì‚Üì‚Üì‚Üì‚Üì‚Üì‚Üì‚Üì‚Üì‚Üì‚Üì‚Üì‚Üì‚Üì‚Üì‚Üì‚Üì‚Üì‚Üì‚Üì‚Üì‚Üì‚Üì‚Üì)31 (20.7%)
PP-4 Resource allocation Small Large Rise (‚Üë‚Üë‚Üë‚Üë‚Üë‚Üë‚Üë‚Üë‚Üë‚Üë‚Üë‚Üë‚Üë‚Üë‚Üë‚Üë‚Üë) Drop (‚Üì‚Üì‚Üì‚Üì‚Üì‚Üì‚Üì‚Üì‚Üì‚Üì‚Üì‚Üì‚Üì‚Üì‚Üì‚Üì‚Üì) 24 (16.0%)
PP-5 Functionality on-off ON OFF Rise (‚Üë‚Üë‚Üë‚Üë‚Üë‚Üë‚Üë‚Üë‚Üë‚Üë‚Üë‚Üë‚Üë‚Üë‚Üë‚Üë‚Üë) Drop (‚Üì‚Üì‚Üì‚Üì‚Üì‚Üì‚Üì‚Üì‚Üì‚Üì‚Üì‚Üì‚Üì‚Üì‚Üì‚Üì‚Üì) 6 (4%)
PP-6 Functionality on-off OFF ON Drop (‚Üì‚Üì‚Üì‚Üì‚Üì‚Üì‚Üì‚Üì‚Üì‚Üì‚Üì‚Üì‚Üì‚Üì‚Üì‚Üì‚Üì) More-than-expected drop ( ‚Üì‚Üì‚Üì‚Üì‚Üì‚Üì‚Üì‚Üì‚Üì‚Üì‚Üì‚Üì‚Üì‚Üì‚Üì‚Üì‚Üì‚Üì‚Üì‚Üì‚Üì‚Üì‚Üì‚Üì‚Üì‚Üì‚Üì‚Üì‚Üì‚Üì‚Üì‚Üì‚Üì‚Üì)24 (16.0%)
PP-7 Non-influence option Random Random Keep (‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì) Drop (‚Üì‚Üì‚Üì‚Üì‚Üì‚Üì‚Üì‚Üì‚Üì‚Üì‚Üì‚Üì‚Üì‚Üì‚Üì‚Üì‚Üì) 12 (8.0%)
‚Ä°A value that implies better performance at the cost of lower security, consistency, integrity, and etc.‚Ä†Opposite to Pro-performance.
Non-functionaltradeoff. In this category, a configuration op-
tion is used to achieve the balance between performance and other
non-functional requirements of the program. We found two prop-
erties regarding this configuration type. In PP-2, tuning the config-
uration option value to relax a requirement is expected to achieve a
performance gain. For example, a database can achieve higher per-
formance by relaxing the ACID properties (a set of properties to en-
surecorrectnessandconsistency).IntheCPBugMySQL-77094[ 20],
innodb_flush_log_at_trx_commit controls the ACID property.
When setting to 2 (relaxed ACID), however, the performance of the
OLTP update benchmark is 10% lower than the performance when
setting to 1 (full ACID). The root cause is that two logging func-tions (i.e.,
commitandlog_write ) both use the log_sys->mutex
lock to write to the same buffer. This, in return, causes extra lock
contentions that hurt the performance. The fix is simply to use
two independent buffers and remove this lock. In this case, switch-
ing ananti-perf value (i.e., 1) to a pro-perf value (i.e., 2) leads to
performance drops, causing a CPBug.
As for PP-3, tuning the configuration option value to enable a
requirement is expected to have a performance loss. If, however,
the actual loss is more-than-expected, it still indicates a CPBug. Forexample, GCC uses
O0, O1, O2, O3 options to control the balance
betweencompilationtimeandbinaryexecutionefficiency.Ahigher
Olevel indicates more compilation time. But in GCC-17520 [ 11],
switching from O0toO2increases the compilation time from less
than 1 second to 1 minute. The cost is more-than-expected. (we
describe the thresholds to measure ‚Äúmore-than-expected‚Äù in ¬ß 3.2.3)
This is caused by a sub-optimal algorithm induced by a process
called"branchprediction"inthe O2level.Thisbugisfixedbyadding
an early drop condition in the algorithm. It makes the compilation
time reduce to less than 1 second with O2.
Resourceallocation. In this category, a configuration option is
used to control resource usages (e.g., RAM, CPU cores). Allocating
more resources generally results in better performance. Take the
bug in Figure 1as an example, increasing the memory allocation
to the sort buffer is expe cted to speed up the " SELECT ORDER BY "
operations. However, the performance of MySQL degrades by 4 √ó,
causing user‚Äôs complaints2.
2"This is pretty much the opposite of any other case I have seen. In fact, to make the
query perform faster, you need to set it to the smallest value" ‚Äì The user of MySQL who
reported this bug.Functionality on-off. In this category, a configuration option
isusedtocontrolanon-performancefunctionalitybutindirectlyin-
fluences the system‚Äôs performance. We have two properties in this
category.PP-5suggeststhatwhenanoptiondisablesafunctionality,
the system‚Äôs performance usually increases. For example, in Mari-
aDB, turning on log_slave_updates logs the updates of a slave
receivedfromamasterduringreplication.However,asdescribedin
MariaDB-5802 [ 13], disabling this functionality increases the time
fortheslavetocatchupduringreplicationby50%.Thishappensbe-causethehandlingcodeof
log_slave_updates=on getsoptimized
as the software evolves, while that of log_slave_updates=off es-
capes. Developers fixed this bug by applying the same optimization
to both situations.
While PP-6 suggests that when a configuration option enables
a functionality, the induced performance overhead is allowed but
shouldbewithinalimit.Forexample,inwebservers, VirtualHost
isusedtoenablemultiplevirtualhostsintheserver.Itisreasonable
thatsettingmorevirtualhostscauseslongerstartuptime.However,
asdescribedinHttpd-50002[ 9],thesingleserverstartuptimegrows
super-linearlyasthenumberofvirtualhostsincreases,anditwould
take 50√ómore time with 10,000 virtual hosts than with the default
setting. The root cause of this bug is Httpd uses a sub-optimal
way to parse the VirtualHost directives in the configuration file.
Developers fixed the bug by optimizing the data structure, which
decreasesthestartuptimefromseveralminutestomerely6seconds.
Non-influenceoption. In this category, a configuration option
is not supposed to influence the system‚Äôs performance, i.e., the
performanceisexpectedtoremainthesameaftertuningtheoption.
For example, users can choose proxy_http (default) or ajp (set
mod_jk) as the connector between Httpd and Tomcat. The two
connectors are expected to have similar file transfer speeds. But in
Httpd-33605[ 7],intheAIXoperating system,thefile transferspeed
degrades from ‚àº8MB/s to only 2KB/s when switching from default
proxy_http connector to ajp connector under the same network
condition. This is because the implementation of AIX socket buffer
is conflicted with the encapsulation of the socket implemented by
theajp13connector. Hence, simply removing the encapsulation of
the socket solved the bug.
2.3 Triggering Conditions of CPBugs
To test the software against a property of a configuration option,
we need to sample at least two values (referred to as VsrcandVtar)
MySQL-21727 32K  48M 4G MySQL-44723 8K  2G 
MySQL-80784 5M  16G 264-1 MySQL-78262 64K  64M 
MySQL-62478 1M  16G 256G MySQL-47529* 0  4M 16G 
MySQL-51325 5M  16G MySQL-38551* 0 2M 16G 
Apache-54852 1 2 64 Apache-58037 0 1 103 
Apache-48215 0 1 103 Apache-50002 0  103 104 
MariaDB-13328 5M  16G MariaDB-16283* 5M  128M 16G 
MariaDB-8696 64K  2M 64M MariaDB-1212* -1 1  102.5 106 
MariaDB-12556 0 1 103 MariaDB-145 0 3M 16G 
MariaDB-15016 1  64 MongoDB -17907* 256M 8G 16G 
MongoDB -20306 256M 12G 16G MongoDB -24139* 256M  1G  16G 
RocksDB-122 256M 2G 16G PostgreSQL -13750 1 2 64 
PostgreSQL -15585  1 2 64 Squid-3189* 4K  6G 16G 
4K  1G  16G 
* This bug requires specific workload. 
Figure 2: Value ranges that can trigger CPBugs (dark gray).
from that configuration option, as shown in the Column 3-4 of
Table2.Fornumericaloptions(e.g., resourceallocation options),itis
difficult to enumerate all value pairs, since the value ranges can be
extremely large. In this regard, we conducted a study to investigate
the value ranges of numeric options that can trigger CPBugs.
In our study, 24.7% (37/150) of the CPBugs are exposed by nu-
merical configuration options. We successfully obtained the value
ranges of 26 CPBugs (including 27 configuration options) out of
the 37 CPBugs by exhaustively and manually reproducing the bugs
with different option values. Note that some upper bounds can be
264, which is too large and may cause the performance "falls the
cliff" [27] due to resource limitations. This behavior is difficult to
be distinguished from actual CPBugs. Therefore, we limited the
upper bounds to our experimental resource limitations (e.g., CPU
cores, RAM).
As illustrated in Figure 2, the dark gray areas show the value
ranges where CPBugs can be exposed. Among 26 (96.3%) out of the
27 numeric options, the ranges contain the minimum or maximum
value of the option. For example, the CPBug MySQL-21727 can
be triggered when the configuration option sort_buffer_size is
between32Kand48M.Thistriggeringrangecontainstheminimum
value (i.e., 32K) of the overall acceptable range of the option (i.e.,
[32K,4G]).Thereasonbehindthisisthatnumericaloptionsusually
affect the program control flow much less than data flow. As a
result, changing numerical options tends to exaggerate or alleviate
an existing bug (if any) in the current program path, instead of
triggering a new bug in a different program path.
Finding2 :96.3%ofnumericconfigurationoptionscantrigger
CPBugs when being set to the minimum or maximum values.
The sampling numbers can be significantly reduced by fixing
Vsrcto the min values or fixing Vtarto the max values.
3 CP-DETECTOR DESIGN
Figure3showstheoverviewof CP-Detector,whichtakesconfigu-
ration documentation and the Software Under Test (SUT) as inputs
and outputs CPBugs. CP-Detector contains two major steps.
First, CP-Detector infers performance properties for each con-
figuration option, i.e., < Type,Direction ,Expectation >. Specifi-
cally, CP-Detector trains the configuration documentation into a
set of rules by natural language processing (NLP) and association
rule mining (ARM). These rules are used to distinguish differentCPBugs
DetectionSuggesting Configuration-related 
Performance Properties
NLP
Preprocessing
Association
Rule Mining
Suggesting
TypeType
Tuning
DirectionsTuning
Directions Analysis
Target
Option
Test
Scenarios
Tt
SUTSampling
Result
AnalysisRun
Potential
CPBugConf.
Docum.
   PropertyPerf.
Figure 3: Overview of CP-Detector.
configuration types. Therefore, given a new configuration option,
CP-Detectorcanlabelitwithaparticularconfigurationtype.Next,
CP-Detector infers the tuning direction for each labeled config-
uration option from the property specification (Table 2) and the
configuration documentation. As a result, given a value pair of
the new configuration option, CP-Detector can determine their
tuning direction. The expected performance change is obtained
according to the configuration type and the tuning direction.
Second, CP-Detector detects CPBugs by checking whether the
outputs of a pair of test executions break the performance prop-
erties of the participating configuration option. To achieve this,
CP-Detector samples a value pair of the option, serving as two
test inputs of the pair of test executions. Besides the participating
configurationoption,CPBugsmayalsorequireothertriggeringcon-ditions, i.e., workloads, other configuration options, or the running
stage of the software (e.g., start, service, shutdown). CP-Detector
combines these conditions as test scenarios, and checks the outputs
of the execution pair under each scenario. Finally, CP-Detector
tests each execution pair under each scenario multiple times, and
determines if there is a CPBug based on hypothesis testing [4].
3.1 Suggesting Performance Properties
Since the configuration documentation of an application often de-
scribes the names, functions, and usages of configuration options,
it is widely-exist, easy to get [ 63], and a good source to derive
configuration-related performance properties. We next describe
how to identify the type and the tuning direction for each configu-
ration option.
3.1.1 Identifying Configuration Types. Given a configuration op-
tion, CP-Detector can automatically label it with a configuration
type. To achieve this, CP-Detector leverages natural language
processing and association rule mining to derive a set of classifi-
cation rules from existing configuration documents. Specifically,
we manually analyzed a total of 500 configuration options from
12 applications and assigned each configuration option to one of
the five types (Table 2). Next, a set of classification rules are au-
tomatically derived from the configuration documents associated
with the 500 options, following three steps: 1) pre-processing the
documents to normalize words with both syntactic and semanticsimilarities; 2) mining association rules between word sequences
and configuration types, and 3) selecting optimal rules used for
labeling new configuration options.
Table 3: Domain specific synonym tags (partial‚Ä†)
Tag Name Words in base forms
Resource memory, buffer, thread, worker, cleaner
Volumn size, amount, number
PerfPositive performance, speed, throughput
PerfNegative latency, CPU time, responses time
OpposePerf integrity, compression, security, reliability
Self this option, this directive, <Config Name>
‚Ä†The complete tag and word lists can be found in our public repository.
NormalizingWords. In this step, CP-Detector identifies both
syntactic and semantic similarities of different words, and simi-
lar words will be regarded as the same one in the following min-
ing process. Each configuration of the dataset is in the form of
{S1,S2,...}‚ÜíTYPE, whereS1andS2stand for the first and the sec-
ond sentences in the configuration description, and TYPErefers to
the configuration type. CP-Detector first split the description into
sentences sharing the same type: S1‚ÜíTYPE,S2‚ÜíTYPE, etc. After
that, CP-Detector normalizes the words in each sentence from
both syntactic and semantic aspects. In specific, for each word,
CP-Detector infers the part-of-speech (POS) tag (e.g., noun, verb)
by using spaCy [ 3]. On the other hand, we manually studied the
descriptions of the 500 options, and defined domain-specific syn-
onym tags as shown in Table 3(traditional methods of identifying
synonymsmaybehardtodealwithwordsincomputerscience).We
alsoreferencedonlineresourcesaboutdomain-specificterminology
during the classification [ 1,5,23]. The words will be normalized ac-
cording to the syntactic and semantic tags. For example, both "size
of buffer" and "number of threads" describe resource-related con-
figuration options. CP-Detector normalizes both phrases into the
same form: { (NN,Volume) ,(IN),(NN,Resource) }, where NNand
INare part-of-speech tags, meaning "Noun, singular or mass" and
"Prepositionorsubordinatingconjunction"3.VolumeandResource
are synonym tags in the first column of Table 3.
MiningAssociation Rules. The goal of this step is to find the
word sequences that appear exclusively and frequently in the de-
scriptionsofaspecificconfigurationtype.Weusethedesignprinci-
pleof FeatureMine[ 40],atypicalARMalgorithmforclassification
on sequential data, and implement it by ourselves to achieve our
goal. The output of FeatureMine is a set of Class Association
Rules (CARs), which are in the form of { W1,W2, ...}‚ÜíTYPEin our
case(Wiistheithwordinthesequence).Meanwhile,thealgorithm
outputs the confidence for each CAR. The confidence is defined as
the conditional probability of occurrence of TYPEgiven {W1,W2, ...}
appears. One important parameter in the algorithm is min_support.
Weassign min_support =3,asusedinexistingsoftwareengineering
studies [ 42,68], which means { W1,W2, ...} should appear at least in
3 configurations of type TYPE. Another important parameter is the
length limitations of CARs. A short CAR (e.g., 1 or 2) will be less
informative; thus we restrict 3 ‚â§len(CAR)‚â§Len, whereLenis a
predefined threshold. We will evaluate how to choose Lenin ¬ß4.4.
3The complete part-of-speech tag list and corresponding meanings are available in
https://spacy.io/api/annotation#pos-universalSelecting Optimal Rules. The above mining approach may
generate millions of CARs with many of them are repetitive. When
a CAR is a sub-sequence of another CAR and the two CARs have
the same support, CP-Detector rules out the short one, since these
two CARs always appear at the same time and the longer one is
more informative. This process still leaves tens of thousands of
CARs. In this regard, CP-Detector selects a subset of CARs as
optimal rules, which are measured by Fscore‚Äì the harmonic mean
of 1) the averaged confidences of CARs in the subset, and 2) the
proportionofconfigurationtypesthatcanbeclassifiedbyusingthe
subset of CARs. The CARs are ranked by confidence. Simply choos-
ing the top-n CARs may get a high averaged confidence, but not
necessarily have high coverage of configuration options. Instead,
CP-Detector randomly samples NumCARs for each configura-
tion type. The CARs are weighed by confidences during sampling,
therefore, CARs with higher confidences are more likely to be
sampled. CP-Detector then calculates the Fscoreof the sampled
CARs. The above process will be exhaustively repeated until the
current highest Fscoreis the theoretical highest Fscorewith above
99.9% probability, according to the cumulative distribution function
(CDF) [2]. Then, CP-Detector selects the subset of CARs with
the highest Fscore. The exhaustive sampling process is a one-time
effort; users can directly use the optimal rules. Numis a predefined
threshold, and we will evaluate how to choose Numin ¬ß4.4.
AssigningConfigurationTypes. With the optimal rules avail-
able,CP-Detectordefinesavotingclassifier.Givenaconfiguration
description, all rules that match the description will vote for their
corresponding TYPE, and the weights are confidences of the rules.
CP-Detectorsuggeststhe TYPEwiththehighestweightasthecon-
figurationtype(whennorulesmatchthedescription,CP-Detector
can not suggest any type). For example, in MySQL, the description
of the option innodb_sort_buffer_size is "Specifies the size of
sort buffers used to sort data during creation of an InnoDB index".
CP-Detector can match the word sub-sequence {size, of, buffers,
to,data}usingtherule{ (NN,Amount) ,(IN),(NN,Resource) ,(IN),
(NN)}‚ÜíResource. The complete rules can be found in our public
repository.
3.1.2 Identifying Tuning Directions. The tuning direction for a con-
figuration option involves a pair of values (referred to as Vsrcand
Vtar). For most of the properties (i.e., PP-1, PP-4, PP-5, PP-6, and
PP-7), obtaining the value pairs is straightforward as long as the
configuration type is known. For example, once a configuration
option is labeled with the Optimization type, its tuning direction
clearly becomes (OFF‚ÜíON).
The exceptions are PP-2 and PP-3 of the Tradeoff type, because
one needs to know tuning VsrctoVtaris fromAnti-performance
toPro-performance (PP-2) or vice versa (PP-3). To address this, CP-
Detector infers the direction of value tuning by analyzing the
configuration documentation associated with each configuration
optionlabeledas Tradeoff.Specifically,givenaconfigurationoption,
CP-Detector ranks its values according to their influences on the
application‚Äôs performance. A value is ranked higher if it results
in performance improvement. Therefore, for an arbitrary pair of
configuration option values, the higher ranked one indicates Pro-
performance and the lower ranked one is Anti-performance.
To do this, similar to ¬ß 3.1, CP-Detector extracts performance-
relatedinformationfromdocumentation.CP-Detectorfirstlocates
the description of each value by matching its first appearance sen-
tence. Based on the synonym tags in Table 3, CP-Detector then
quantifies the degrees to which a configuration option value influ-
ences the performance. Specifically, a value will be scored +1 if one
ofthe followingsequences intheleft sideappears initsdescription:
Increase ‚ÜíPerfPositive Decrease ‚ÜíPerfPositive
Decrease ‚ÜíPerfNegative Increase ‚ÜíPerfNegative
Decrease ‚ÜíOpposePerf Increase ‚ÜíOpposePerf
These sequences explicitly indicate the value can increase the per-
formance. On the contrary, a value will be scored -1 if one of the
sequences in the right side appears. For example, in MongoDB, the
tradeoff option compressors has three values: snappy(balanced
computation and compression rates), zlib(higher compression
rates at the cost of more CPU consumption, compared to snappy.),
zstd(higher compression rates and CPU consumption when com-
pared to zlib). CP-Detector will rank the values as { snappy,zlib,
zstd}, since snappyhas the best performance, while zstdhas the
worse one.
3.2 Exposing CPBugs
CP-Detectorgeneratesvaluepairs< Vsrc,Vtar>associatedwiththe
tuning direction for the target configuration options (i.e., options
labeled by the five configuration types). The performance change
(after tuning the target configuration option OtfromVsrctoVtar)i s
usedtodetermineifaCPBugisexposed.Inadditionto Ot,aCPBug
mayneedspecificvaluesofotherconfigurationsorworkloadstobe
exposed. Therefore, CP-Detector also samples the values of other
configurationoptionsandenvironmentparameters(e.g.,workloads,
stage of program execution) to test Otunder different scenarios.
Wenextdescribetheprocessofsampling Ottogeneratevaluepairs,
the process sampling test scenarios to test Ot, and results checking.
3.2.1 Sampling Target Configurations Options. Once a configura-
tion option Otis assigned with one or more specific properties,
CP-Detector will generate value pairs, < Vsrc,Vtar>, for the tuning
direction of each property. According to Table 2, the tuning direc-
tions of PP-1, PP-5, and PP-6 involve binary options, so the value
pair contains only ON and OFF. The tuning direction of PP-7 is also
straightforward so a pair of random values is generated.
The tuning direction of PP-2 and PP-3 exhaustively samples
the pairs of enumerated values in terms of their ranking positions,
where the higher ranked value is assigned to Anti-performance and
the lower ranked value is assigned to Pro-performance. Suppose Ot
has three enumerated values ranked as { V1,V2,V3}, the value pairs
for PP-2 are < V2,V1>, <V3,V1>, <V3,V2>. The value pairs for PP-3
are <V1,V2>, <V1,V3>, <V2,V3>.
One challenge is that for numeric options, it is hard to test all
combinations of two values because the value ranges may be ex-
tremely large. To address this, guided by Finding 2, the sampling
numberscanbesignificantlyreducedbyfixing Vsrctotheminimum
configuration values or fixing Vtarto the maximum configuration
values. During the sampling process, a small step length between
VsrcandVtarmay lead to limited performance change, and cannot
expose CPBugs. While a large step length may lead to one of Vsrcor
Vtarlocated outside the value range which can trigger CPBugs. Inthis regard, CP-Detector first assigns Vsrcto the minimum value,
and increases Vtarexponentially (e.g, <1, 2>, <1, 4>, <1, 8> ...) until
the maximum value. Then, CP-Detector assigns Vtarto the max-
imum value, and decreases Vsrcexponentially until the minimum
value. This sampling strategy helps CP-Detector find the proper
VsrcandVtarwithin limited samples.
3.2.2 Sampling Test Scenarios. Given a target configuration option
Ot, CP-Detector now has two values (i.e., VsrcandVtar)o fOt,
and the expected performance change when tuning OtfromVsrc
toVtar. BesidesOt, a CPBug may need other triggering conditions,
including workloads, other configurations, or the running stages ofthesoftware(e.g.,start,service,shutdown).Inthisregard,wedefine
test scenarios S=<Workload, Configuration, Stage>. CP-Detector
will generate different scenarios, then test VsrcandVtarofOtunder
each scenario.
Workloads: CP-Detector uses both performance benchmark
tools and official performance test suite as workloads. Benchmark
toolsprovideavarietyofparameterswithwideranges.Togeneraterepresentativeworkloadcommands,CP-Detectorappliesthestate-of-artdistance-basedsamplingmethod[
39]whichsupportsflexible
sample size and is more representative [ 39,60] than traditional
n-wise sampling. After that, the official test suite is integrated with
thosecommandstogetthecompletesetofworkloadcommands.CP-
Detector also provides interfaces to accept customized workloads.
Configurations: In ¬ß2, there are 150 CPBugs that show expec-
tation mismatches. We manually analyzed the CPBugs and found
94.0% of them can be triggered by testing the combinations of two
options. This result indicates that CP-Detector can expose 94.0%
CPBugs by sampling one other option besides Ot. To do this, CP-
Detector uses the one-hot sampling strategy, i.e., changing one
option at one time while other options remain default values. As
for the constraints between configurations, CP-Detector needs to
filter out combinations that violate configuration constraints. Toachieve this, CP-Detector leverages SPEX [
64], which uses the
data-flow of program variables corresponding to the configuration
options to extract constraints. CP-Detector also allows users to
provide customized constraints.
Runningstages: CPBugsmayonlybetriggeredatspecificrunning
stages of software. For example, the CPBug Httpd-50002 shown in
¬ß2.2happened at the start stage. In this regard, we predefine run-
ning stages for each software domain, including {"start", "restart",
"service", "shutdown"} for servers and {"binary compilation", "bi-
nary execution"} for compilers. Then, CP-Detector tries to exposeCPBugs under each stage. For configuration options of the Resource
type, CP-Detector only uses the servicestage in servers, since
other stages do not use the resources.
3.2.3 Results Checking. CP-Detector finally checks if the actual
performance change of VsrcandVtarindicates a CPBug according
to Column 6 in Table 2. For PP-1, PP-2, PP-4, PP-5, and PP-7, it
is easy to check if the actual performance drops, i.e., P(Vsrc)>
P(Vtar),whereP(Vsrc)andP(Vtar)areperformancesof VsrcandVtar,
respectively. For PP-3 and PP-6, CP-Detector uses the following
rules to determine if the drops are more-than-expected:
P(Vsrc)/P(Vtar)>Tr1;P(Vsrc)‚àíP(Vtar)>Tr2.
whereTr1andTr2arepredefinedthresholds.Itmeans P(Vsrc)isbet-
ter thanP(Vsrc)more than Tr1times. while the absolute drop from
P(Vsrc)toP(Vtar)is large than Tr2. We will evaluate the thresholds
in ¬ß4.4.
Since performance can be influenced by many environment fac-
tors, such as network delay and system warm-up, an application
running repeatedly on the same machine can produce performance
results that differ with each execution. CP-Detector employs a
strategy to eliminate the performance bias. CP-Detector tests
each case 20 times repeatedly and uses hypothesis testing [ 4]t o
eliminatetheperformancebias.Specifically,CP-Detectorassumes
the performances of VsrcandVtaras two random variables, then
uses thet-test (Œ±=0.05) to check if the " >" relations hold in the
above rules. We set the null hypothesis that the relations do not
hold. When the null hypothesis is rejected, a CPBug is alarmed.
4 EVALUATION
To evaluate CP-Detector, we consider four research questions:
RQ1:How accurate is CP-Detector at suggesting performance
properties?
RQ2:How effective and efficient is CP-Detector at exposing both
known and unknown CPBugs?
RQ3:How does CP-Detector compare with the state-of-the-art
performance bug detection tool?
RQ4:HowdoCP-Detectorparametersinfluenceitseffectiveness?
4.1 RQ1: Accuracy of Suggesting Performance
Properties
To answer RQ1, we evaluated the accuracy of CP-Detector in
suggesting performance properties of configuration options. Given
a configuration option, this process contains two components: pre-
dicting the type of configuration option and identifying the tuning
direction. We evaluated the accuracy for each component. We ran-
domly sampled 500 configuration options from the 12 software
systems we studied. Three authors manually labeled the types of
the options by analyzing the configuration documents. Each label
was cross-checked and discussed until there was no disagreement.
This process took 70 working hours. The options were split into 10
sets to conduct a stratified 10-fold cross validation. We did not use
the configuration options included in our empirical studies, since
the options involved in the CPBugs have an unbalanced distribu-
tion. By default, we set the parameters during mining CARs with
Len=7 andNum=100, and introduce how to set these parameters in
¬ß4.4.
Predicting Configuration Types. We evaluated the precision
and recall of predicting each configuration type. We also calcu-
lated the weighted averages [ 57], which is defined as the averaged
precision/recall of each type weighted by the option number of
the type. We also compared our approach with a baseline method,
i.e., keyword searching. We used the same CAR mining algorithm
and restricted len( C A R )=1t ogenerate keywords for each type.
We setNum= 25, which has been tested to be able to achieve the
best result. A larger Num(e.g., 100) may improve the recall, but
significantly decreases the precision at the same time.Table 4: Precision and recall on inferring types of configu-
ration options (average result with stratified 10-fold cross
validation).
Type # OptionPrecision Recall
CPD‚Ä†Base.‚Ä°CPDBase.
Resource 143 93.9% 69.4% 92.4% 93.2%
Tradeoff 84 70.7% 61.3% 66.9% 21.4%
Optimization 73 69.4% 27.3% 65.8% 18.8%
Functionality 100 82.2% 56.2% 55.6% 39.1%
Non-influence 100 90.1% 35.0% 67.1% 70.0%
Weighted Average 83.3% 52.4% 71.8% 54.8%
‚Ä†CP-Detector.‚Ä°The keyword-based baseline method.
Table4shows the precision and recall of CP-Detector in pre-
dicting configuration types. CP-Detector is most effective in pre-
dicting the Resource type. This is because their option descriptions
often contain similar semantics, e.g., memory, buffer, CPU, etc.
While for Functionality options, CP-Detector has a good preci-
sion but the lowest recall. This is because the functionalities are
highly diverse and only limited common features (e.g., profiling,
monitoring) are identified by CP-Detector. Compared to the base-
line method, when considering the precision and recall together
(i.e., the harmonic mean of the precision and recall), CP-Detector
outperforms the baseline method in every type. This result suggests
that CP-Detector is effective (83.3% precision and 71.8% recall) in
predicting configuration types.
Identifying Tuning Directions. The second task is to deter-
minethetuningdirectionforeachvaluepairofagivenoption.This
task is challenging for Tradeoff configuration options, while the
tuning directions of other options are straightforward. Among 84
Tradeoff options, we need to check 162 pairs of tuning directions
(an option with 3 values, say A,B,C, implies 3 pairs: AB, AC, BC).
CP-Detectorsuccessfullypredicted139/162(85.8%)ofthem.Mean-
while, CP-Detector failed to identify 23 cases. This is because the
configuration documents do not always contain the relationship
among the values of a tradeoff option. For example, a common
tradeoff option for database is storage_engine . Different engines
produce different levels of performance, concurrency, consistency,
integrity, etc. But these properties are not described in the docu-
ments. The accuracy of other types are: 100% for both Resource and
Non-Influence, 94.4% for Functionality, 97.8% for Optimization (e.g.,
someFunctionality options are not Boolean; thus, CP-Detector
can not handle). ThisresultindicatesCP-Detectoriseffective(96.9%
accuracyin average) in identifying tuning directions.
4.2 RQ2: Effectiveness of Efficiency of
Detecting CPBugs
To evaluate CP-Detector in exposing CPBugs, we first applied CP-
Detector to a set of existing CPBugs. We then used CP-Detector
to find previously unknown CPBugs.
4.2.1 Detecting Existing CPBugs. We evaluate the effectiveness
and efficiency of CP-Detector in exposing CPBugs studied in ¬ß 2.
We tried to reproduce all the 173 studied CPBugs with our best
effort.Wesuccessfullyreproduced38bugs.Toavoidover-fitting,we
Table 5: The effectiveness of detecting existing CPBugs.
PP Violated # CPBugs # Exposed #F P
Optimization (PP-1) 7 5‚Ä†/6‚Ä°2
Tradeoff-1 (PP-2) 12 1 0/1 2 0
Tradeoff-2 (PP-3) 9 7/8 0
Resource (PP-4) 12 1 1/1 2 1
Functionality-1 (PP-5) 2 1/2 0
Functionality-2 (PP-6) 9 5/8 2
Non-Influence (PP-7) 6 4/6 0
N/A 4 0/0 2
TOTAL 61 4 3/5 4 7
‚Ä†#CPBugsexposedbyCP-Detector.‚Ä°#CPBugsexposedbyCP-Detector
given ideal properties.
followedthebugcollectionstepsin¬ß 2andsuccessfullyreproduced
23bugsthatarenotincludedin173studiedbugs.Reproducingthese
61 CPBugs took 500 working hours. We evaluate CP-Detector on
these 61 CPBugs. By default, CP-Detector sets Tr1= 3 andTr2=
5. We evaluate how to determine these thresholds in ¬ß 4.4.
Effectiveness. To evaluate the effectiveness, we assessed both
completeness : how many bugs can be exposed by CP-Detector
from the 61 known bugs, and soundness : how many false positives
CP-Detector produces. To measure the false positive, we applied
CP-Detector to the software versions where bugs have been fixed
bydevelopers,andobservedif CP-Detectorstillreportsbugs.The
results are shown in Table 5. CP-Detector successfully exposed
43 out of the 61 bugs. Among the exposed bugs, 19 bugs were
exposed by using the default workloads of CP-Detector, while 24
bugs required specific workloads collected from bug reports. CP-
Detector failed to expose 18 bugs due to the following reasons: 1)
The properties suggested by CP-Detector is not correct (11 cases);
2) The bug is not triggered when testing (3 cases), e.g., MongoDB-
30643[16]canbeexposedwhen7differentoptionsaresettospecific
values; 3) The option does not have any property (4 cases, row
"N/A"), e.g., MySQL-74325 [19]i n¬ß 2.1.
CP-Detector reported seven false positives. Three cases are
caused by bad application design. For example, in the case of PP-
4, allocating larger caches results in worse performances in both
buggy and fixed versions. This is because the query cache feature
is actually ill-designed: only in rare cases, increasing the cache size
improves the performance. This feature is removed since MySQL is
upgraded to v8.0. Four cases are caused by incorrect properties. For
example, in the "N/A" case, CP-Detector falsely regards -m32/64
as a non-influence option, which can affect performances depend-
ing on CPU architectures. This result indicates CP-Detector can
effectively (43/61) expose existing CPBugs with limited false positives
at the same time.
Efficiency. To measure the efficiency, we used the number of
valuepairsofeachconfigurationoptionrequiredtoexposeCPBugs.
We mainlyevaluatethenumbersfor numericconfigurationoptions
becausethesamplingapproachforotheroptionsisstraightforward,
i.e., enumerating all combinations. We evaluated the numbers on
all 17 CPBugs (from the 61 ones) with numeric options and com-
pared CP-Detector with a baseline method, i.e., uniform samplingTable 6: New CPBugs detected by CP-Detector
BugID Slowdown‚Ä°Version(s) Status
Clang #43576(1) 1.19√ó(E.T.) v7 - latest Confirmed
Clang #43576(2) 1.28√ó(E.T.) v7 - latest Confirmed
Clang #43084 2.9√ó(C.T.) v3 Fixing
Clang #44359 1.2√ó(E.T.) v7 - latest Pending
Clang #44518 2.0√ó(C.T.) v3 - latest Fixing
GCC #91852 2.8√ó(C.T.) v6 - latest Pending
GCC #91895 4.4√ó(C.T.) v4 Confirmed
GCC #91817 44√ó(C.T.) v4 Confirmed
GCC #91875 1.85√ó(E.T.) v 7-v 8 Confirmed
GCC #93037 1.12√ó(E.T.) v8 - latest Pending
GCC #93521 2.52√ó(E.T.) v8 - latest Confirmed
GCC #93535 4.50√ó(E.T.) v8 - latest Confirmed
GCC #94957 hang (C.T.) v7 - latest Fixing
‚Ä°C.T.: Compiling Time, E.T.: Execution Time.
(the sampled numbers satisfy uniform distribution). The results
showed that CP-Detector exposed 11 out of the 17 bugs. To make
a fair comparison, we tuned the sampling density of the baseline
method until the same number of bugs are exposed. As a result,
CP-Detector generated 106 value pairs (9.6 pairs each bug in aver-
age,stdev=3.1), whereas the baseline method generated 1,320 pairs
(120 pairs each bug in average, stdev=0). CP-Detector required
fewer pairs since fixing one of the two values on the minimum or
maximum value. This result indicates our sampling strategy can sig-
nificantly improve the efficiency (i.e., reduce the sampling numbers)
while remaining the same effectiveness.
4.2.2 Detecting Unknown CPBugs. We applied CP-Detector on
the options sampled in ¬ß 4.1to evaluate if CP-Detector can detect
unknown CPBugs. CP-Detector reported 17 CPBugs from Clang
andGCC,including13truepositivesand4falsepositivesaccording
to our manual analysis. We reported 13 true positives to developers
as shown in Table 6. To evaluate the impact of the reported bugs,
we calculated their slowdowns by comparing the performances
between the fixed version (if any) and the buggy version. If the
fixed version is not available, we examined the performance on the
other compiler for comparison. The CPBugs found by CP-Detector
have significant impacts (1.19 √ó‚àº1.85√óexecution times, 2.8 √ó‚àº
44√ócompilation times) on user‚Äôs experience and many have been
existing for years. Worse still, GCC #94957 [ 12] hangs for hours to
compile 8 lines of C++ code.
Meanwhile, CP-Detector reported four false positives in GCC
and Clang. When using a higher level of compile-time optimiza-
tion, the binary execution time usually decreases. In the two false
positives, however, the execution time also depends on the hard-
ware. The higher level optimization generates machine code that is
inefficient in the experimental hardware. Adding -march=native
can solve the problem because it makes the generated code suitable
for the compiling machine. These two cases can be eliminated by
adding configuration constraints. The other two false positives are
caused by incorrect properties suggested by CP-Detector. This
resultindicatesCP-DetectorcanexposeunknownCPBugswithhigh
impacts(up to 44√ó slowdowns and existing for years).
345678902468¬∑105
LenPer-Irrelevant
Tradeoff
Function
Optimization
Resource
020406080100
time in minutetime
(a)NumberofCARsandoverheadwithdif-
ferentLen5100 200 300 400 5000.10.20.30.40.50.60.70.80.91
Num(each type)Precision (Rand-n)
Recall (Rand-n)
Precision (Top-n)
Recall (Top-n)
(b) Precisions and recall of two CAR selec-
tion strategies with different Num.2468 1 0 1 2 1 4 1 6 1 80.50.60.70.80.91
Threshold (Tr 1)Tr2=0 Tr2=5
Tr2=10 Tr2=15
Tr2=20 Tr2=30
(c) F-score of different Tr1,Tr2
Figure 4: The Influence of CP-Detector Parameters.
4.3 RQ3: Comparison with the State-of-the-art
We compare CP-Detector with Toddler [ 46], which is one of the
most effective bug detection tools among existing works. Toddler
usesredundantmemoryaccesspatternstodetectperformancebugs
that are caused by inefficient loops. We evaluate the effectiveness
of Toddler in exposing the same CPBugs. The evaluation shows
that Toddler can detect 6 of the 61 existing CPBugs. This is be-
cause the CPBugs are caused by a variety of reasons, while Toddler
only focuses on the inefficient variable accesses in loops, which
account for a small proportion in our dataset. Thisresultindicates
CP-DetectorcandetectmoretypesofperformancebugsthanTod-
dler.CP-DetectorcanserveasacomplementarytoolwithToddler
in detecting general performance bugs.
4.4 RQ4: The Influence of Model Parameters
The effectiveness of CP-Detector can be affected by the selection
of four parameters: the length Lenand number Numof CARs,
and two thresholds (i.e., Tr1andTr2) to check PP-3 and PP-6. We
evaluate how these parameters impact CP-Detector.
The max length (Len ) of CARs affects the CAR candidates gen-
erated by association rule mining process. With longer Len,w e
can get more CAR candidates, but the time spent for mining grows
exponentially. To choose a reasonable value, we use the 500 option
descriptions in ¬ß 4.1to evaluate the CAR number and overhead
with different Len. As shown in Figure 4a, when Lenis larger than
7, the numbers of CAR candidates for all types of options start to
converge. This is intuitive because longer sequences are less likely
toappearmorethan min_support timestobecomeaCARcandidate.
The number (Num ) of CARs selected for each type of options
can affect the accuracy of the option type classifier. To evaluate
this, we use the same options in ¬ß 4.1and split it by 10 to con-
duct stratified 10-fold cross validation on precision and recall of
the classifier with different Num. Also, we compare our sampling
approach with a baseline method, i.e., using Top- NumCARs from
each type. Figure 4bshows the averaged precision and recall of the
option type classifier. As Numgrows, the precisions and recalls of
both strategies converge. When Numis small (i.e., less than 300),
our sampling strategy outperforms the Top- Numstrategy, since
the recall of the Top-Num strategy is limited. When Numis larger
than 100, the recall of our approach converges, while the precision
remains the same. Thus, we use 100 as the default value.To choose the best combination of Tr1andTr2, we evaluated
all CPBugs breaking PP-3 and PP-6. For each bug, we collected the
performance pairs (i.e., P(Vsrc)andP(Vtar)) in both buggy and fixed
versions.Givenacombinationof Tr1andTr2,atruepositivemeans
thecombinationsuggeststheperformancepairinthebuggyversionisabug;afalsepositivemeansthecombinationsuggeststhepairin
the fixed version is a bug; a false negative means the combination
suggests the pair in the buggy version is not a bug. We successfully
collecteddatafrom41bugs,andsplitthedataby10todothe10-fold
cross validation. Then we calculate the precision, recall and F-score
with different Tr1andTr2. As shown in Figure 4c, larger Tr1or
Tr2implies stricter conditions, thereby reducing false positives but
increasing false negatives (and vice versa). The optimal Tr1and
Tr2combination is 3 and 5 (blue line, with best F-score=90.6%). The
averageprecisionandrecallare94.1%and87.3%,respectively.Thus,
we use 3 and 5 as default values.
5 DISCUSSION
ImpactofWorkloads. Software workloads can affect the effec-
tiveness of CP-Detector in exposing new CPBugs. It is hard to
automaticallypredictreal-worldworkloadsthatcantriggerCPBugs.
Instead, CP-Detector provides interfaces to accept customized
workloads. For example, when the software end users reported a
performance issue, CP-Detector can leverage the workload con-
tained in the report and help developers confirm if the issue is
caused by a CPBug.
Quality of Performance Properties. We summarized seven
properties from 150 CPBugs. These properties may be limited in
twoaspects:1)Wemaymissapropertythatdoesnothappeninour
studied CPBugs; 2) The properties may be affected by other factors
andnotalwayshold.Forexample,in¬ß 4.2.2,CP-Detectorreported
two false positives which break our properties but are not bugs. In
this regard, we will investigate more bugs in the further work toimprove the completeness of properties. CP-Detector provides
user interfaces to accept customized property constraints, e.g., for
the false positives of ¬ß 4.2.2, the property of the optimization-level
option holds when setting -march=native.
ValueBoundsofNumericOptions. Whensamplingnumeric
options, CP-Detector needs to determine the lower and upperbounds. Simply using the maximum value of an integer variable
(e.g., 264for unsigned long) as the upper bound may result in a
misconfiguration [ 27]. For example, when a buffer value is larger
thanthememorysize,thesystemwillusethe swapmemory,andthe
performancedrops.ThisperformancelossisnotcausedbyaCPBug.
Directly using the memory size may still be problematic, since one
software project may have multiple buffers. To avoid this problem,
CP-Detector first extracts the lower and upper bounds from user
manuals(ifany).Otherwise,weempiricallysetthelowerandupper
bounds to 0 and 1/4 of the system resource, respectively. And we
monitor the resource usage by topto avoid resource overloading.
Reproducing Bugs. We successfully reproduced 61 out of 173
CPBugs. Note that reproducing performance bugs following thebug reports are not trivial [
32]. The main reasons why we failed
to reproduce many of them are missing of important steps and too
complicated workload. For instance, httpd #58037 [ 10] and Mon-
goDB #27753 [ 15] only show the symptom but miss bug-inducing
workload. MongoDB #27700 [ 14] requires distributed cluster and
complicated workload (vaguely described). Few CPBugs need spe-
cificenvironmenttotrigger(e.g.,httpd#42065[ 8]requireWindows
2003 Server), which, by construction, CP-Detector can not expose.
In this paper, only 11.5% (lowest) of MongoDB‚Äôs CPBug are repro-
ducible, and 56.3% (highest) for Clang.
Future Work .CP-Detector is far from perfect. First, trigger-
ing CPBugs sometimes require specific workload, environment or
timing [30,32]. One of our future work will lie in designing auto-
matic workload generation techniques to expose CPBugs for those
software systems that have limited or no test suite or benchmark
tools. Second, CP-Detector can report unexpected performance
drop by tuning options, but can not locate them, still leaving di-
agnose efforts of developers. So we will explore how to locate the
bug-inducing code of CPBugs to help developer fix them.
6 RELATED WORK
Performance Bug Detection. Some works focus on detecting
different types of performance problems: inefficient loops [ 29,45,
46], redundant roads [ 61], redundant collection traversals [ 48],
reusable data [ 44,62], false sharing in multi-thread programs [ 41],
inefficient synchronization [ 49,66], user-interface performance
problems [ 50], architectural impacts among methods [ 26], perfor-
mance anti-patterns [ 65] and tradeoffs [ 21] in ORM applications.
These works are effective in detecting certain types of performance
problems, which are different to the configuration-handling per-
formance bugs detected by CP-Detector. Recent short position
papers [37,53,54] have proposed a proof concept that using meta-
morphic testing to expose performance bug. While they did not
proposedanautomaticapproachorevaluateonlargescalesoftware
systems.Thedifferencebetweenourworkandmetamorphictestingis that they typically use multiple test executions to infer metamor-
phic relations, and verify those relations on follow-up tests. While
we conclude performance properties from bug study and generate
them from expert knowledge (e.g., user manuals).
HotspotsDetection. Someworksfocusonpinpointinghotspots
in programs via profiling: Perf [ 6], YourKit [ 31]. Similarly, several
following works address on generating the most time-consuming
workloads via profilers to help expose performance bottlenecks [ 28,55,67].Thelimitationofprofiling-basedmethodsisthatthehotspots
arenotnecessarilycausedbyperformancebugs.WhileCP-Detector
can use performance properties summarized from real-world bugs
to confirm if a hotspot if caused by a CPBug.
PerformanceModeling and Tuning. Many works [25, 35,38,
43,47,51,52] aim to predict performance for given configurations,
or study the tendency of performance changes to improve perfor-
mance when tuning configurations. These works focus on building
the relationship between performance and configuration, and find-
ingthefastestconfigurationofasoftwaresystem.Thisisdifferentto
find performance bugs caused by incorrect configuration handling.
UnderstandingofPerformanceBugs. Previousstudiesofper-
formance bugs have covered a wide range of characteristics in-
cluding root causes, fixing complexity, how they are introduced
and found [ 36,58]. Recently, some empirical studies [ 33,34] em-
phasize the importance of configuration-aware testing techniques
and provide insights on reducing the searching space of config-
urations. Some works help comprehend performance, including
performance distributions generation [ 24], and performance speci-
fications extracting via in-field data [ 22]. These works help under-
stand performance issues, while CP-Detector can expose CPBugs
automatically.
7 CONCLUSIONS
Performance bugs are hard to detect due to their non fail-stop
symptom. In this paper, we argue that the performance expectation
of configuration tuning can be leveraged to expose CPBugs. We
studied173real-worldCPBugsfrom12softwaresystemsandfound
most (86.7%) of CPBugs can be exposed by using the expectations.
Our findings also guide the inferring of performance expectations
and sampling of test inputs to trigger CPBugs. We design and im-
plement CP-Detector to detect real-world CPBugs. The result
shows that CP-Detector is effective in exposing both known and
unknown CPBugs. CP-Detector can be integrated into an IDE as
a regression test tool w.r.t. performance, or used asan assistant tool
to confirm performance-related bugs in bug tracking systems.
ACKNOWLEDGMENTS
This research was supported by National Key R&D Program of
China(ProjectNo.2017YFB1001802);NationalNaturalScienceFoun-
dation of China (Project No.61872373, 61702534 and 61872375).
REFERENCES
[1]Categoriesandsubcategoriesincomputerscience. https://en.wikipedia.org/wiki/
Category:Computing.
[2]Fit CDF for normal distribution. https://en.wikipedia.org/wiki/Cumulative_
distribution_function.
[3] spaCy. https://spacy.io.
[4]Statistical hypothesis testing. https://en.wikipedia.org/wiki/Statistical\
_hypothesis_testing.
[5]IEEE standard glossary of software engineering terminology. IEEE Std 610.12
(1990).
[6] Linux perf tool. https://perf.wiki.kernel.org/index.php/Main_Page , 2015.
[7] httpd 33605, Acc.: 2020. https://bz.apache.org/bugzilla/show_bug.cgi?id=33605 .
[8] httpd 42065, Acc.: 2020. https://bz.apache.org/bugzilla/show_bug.cgi?id=42065 .
[9] httpd 50002, Acc.: 2020. https://bz.apache.org/bugzilla/show_bug.cgi?id=50002 .
[10] httpd 58037, Acc.: 2020. https://bz.apache.org/bugzilla/show_bug.cgi?id=58037 .
[11]GCC 17520, Accessed: 2020. https://gcc.gnu.org/bugzilla/show_bug.cgi?id=17520 .
[12]GCC 94957, Accessed: 2020. https://gcc.gnu.org/bugzilla/show_bug.cgi?id=94957 .
[13] MariaDB 5802, Accessed: 2020. https://jira.mariadb.org/browse/MDEV-5802 .
[14]MongoDB 27700, Accessed: 2020. https://jira.mongodb.org/browse/server-27700 .
[15]MongoDB 27753, Accessed: 2020. https://jira.mongodb.org/browse/server-27753 .
[16]MongoDB 30643, Accessed: 2020. https://jira.mongodb.org/browse/server-30643 .
[17] MySQL 21727, Accessed: 2020. https://bugs.mysql.com/bug.php?id=21727 .
[18] MySQL 67432, Accessed: 2020. https://bugs.mysql.com/bug.php?id=67432 .
[19] MySQL 74325, Accessed: 2020. https://bugs.mysql.com/bug.php?id=74325 .
[20] MySQL 77094, Accessed: 2020. https://bugs.mysql.com/bug.php?id=77094 .
[21]Atlee,J.M.,Bultan,T.,andWhittle,J. View-centricperformanceoptimization
for database-backed web applications. In InternationalConferenceonSoftware
Engineering(ICSE) (2019).
[22]Br√ºnink, M., and Rosenblum., D. S. Mining performance specifications. In
EuropeanSoftwareEngineeringConferenceandtheInternationalSymposiumon
theFoundationsof Software Engineering (ESEC/FSE) (2016).
[23]Butterfield, A., Ngondi, G. E., and Kerr, A. A Dictionary of Computer Science.
Oxford University Press, 2016.
[24]Chen, B., Liu, Y., and Le., W. Generating performance distributions via proba-
bilistic symbolic execution. In International Conference on Software Engineering
(ICSE)(2016).
[25]Chen, T.-H., Shang, W., Hassan, A. E., Nasser, M., and Flora, P. Cacheopti-
mizer: Helping developers configure caching frameworks for hibernate-based
database-centric web applications. In European Software Engineering Confer-
ence and the International Symposium on the Foundations of Software Engineering
(ESEC/FSE) (2016).
[26]Chen, Z., Chen, B., Xiao, L., Wang, X., and Xu, B. Speed oo: prioritizing per-
formance optimization opportunities. In International Conference on Software
Engineering(ICSE) (2018).
[27]Coady, Y., Cox, R., DeTreville, J., Druschel, P., Hellerstein, J., Hume, A.,
Keeton, K., Nguyen, T., Small, C., Stein, L., and Warfield, A. Falling off the
cliff:Whensystemsgononlinear. In WorkshoponHotTopicsinOperatingSystems
(HotOS) (2005).
[28]Coppa, E., Demetrescu, C., and Finocchi., I. Input-sensitive profiling. In
Conferenceon Programming Language Design and Implementation (PLDI) (2012).
[29]Dhok, M., and Ramanathan, M. K. Directed test generation to detect loop
inefficiencies. In European Software Engineering Conference and the International
Symposiumon the Foundations of Software Engineering (ESEC/FSE) (2016).
[30]Ding, Z., Chen, J., and Shang, W. Towards the use of the readily available tests
from the release pipeline as performance tests. are we there yet? In International
Conferenceon Software Engineering (ICSE) (2020).
[31]GmbH,Y. Theindustryleaderin.NET&Javaprofiling. https://www.yourkit.com.
[32]Han,X.,Carroll,D.,andYu,T. Reproducingperformancebugreportsinserver
applications: The researchers‚Äô experiences. Journalof SystemsandSoftware 156
(2019), 268‚Äì282.
[33]Han, X., and Yu, T. An empirical study on performance bugs for highly con-
figurable software systems. In InternationalSymposiumonEmpiricalSoftware
EngineeringandMeasurement(ESEM) (2016).
[34]Han, X., Yu, T., and Lo, D. Perflearner: Learning from bug reports to understand
and generate performance test frames. In International Conference on Automated
Software Engineering (ASE) (2018).
[35]Jamshidi, P., Siegmund, N., Velez, M., K√§stner, C., Patel, A., and Agarwal,
Y. Transfer learning for performance modeling of configurable systems: An ex-
ploratoryanalysis. In InternationalConferenceonAutomatedSoftwareEngineering
(ASE)(2017).
[36]Jin, G., Song, L., Shi, X., Scherpelz, J., and Lu, S. Understanding and detecting
real-world performance bugs. In ConferenceonProgrammingLanguageDesign
andImplementation(PLDI) (2012).
[37]Johnston, O., Jarman, D., Berry, J., Zhou, Z. Q., and Chen, T. Y. Metamorphic
relations for detection of performance anomalies. In International Workshopon
MetamorphicTesting (MET) (2019).
[38]Juliana, A. P., Mathieu, A., Hugo, M., and Jean-Marc, J. Sampling effect on
performance prediction of configurable systems: A case study. In International
Conferenceon Performance Engineering (ICPE) (2020).
[39]Kaltenecker, C., Grebhahn, A., Siegmund, N., Guo, J., and Apel, S. Distance-
based sampling of software configuration spaces. In International Conference on
Software Engineering (ICSE) (2019).
[40]Leshl, N., Zaki, M. J., and Ogihara3, M. Mining features for sequence classifi-
cation. In ACM Knowledge Discovery and Data Mining (SIGKDD) (1999).
[41]Liu, T., and Berger., E. D. Sheriff: Precise detection and automatic mitigation of
false sharing. In Conference on Object-Oriented Programming Systems, Languages,
andApplications (OOPSLA) (2011).
[42]Michail, A. Data mining library reuse patterns in user-selected applications. In
InternationalConference on Automated Software Engineering (ASE) (1999).
[43]Nair,V.,Menzies,T.,Siegmund,N.,andApel.,S. Usingbadlearnerstofindgood
configurations. In EuropeanSoftwareEngineeringConferenceandtheInternational
Symposiumon the Foundations of Software Engineering (ESEC/FSE) (2017).[44]Nguyen, K., and Xu, G. Cachetor: Detecting cacheable data to remove bloat. In
EuropeanSoftwareEngineeringConferenceandtheInternationalSymposiumon
the Foundationsof Software Engineering (ESEC/FSE) (2013).
[45]Nistor, A., Chang, P.-C., Radoi, C., and Lu, S. Caramel: Detecting and fixing
performance problems that have non-intrusive fixes. In International Conference
on Software Engineering (ICSE) (2015).
[46]Nistor, A., Song, L., Marinov, D., and Lu, S. Toddler: Detecting performance
problems via similar memory-access patterns. In International Conference on
Software Engineering (ICSE) (2013).
[47]Oh, J., Batory, D., Myers, M., and Siegmund, N. Finding near-optimal configu-
rations in product lines by random sampling. In EuropeanSoftwareEngineering
Conference and the International Symposium on the Foundations of Software Engi-
neering (ESEC/FSE) (2017).
[48]Olivo, O., Dillig, I., and Lin, C. Static detection of asymptotic performancebugs in collection traversals. In Conference on Programming Language Design
andImplementation(PLDI) (2015).
[49]Pradel, M., Huggler, M., and Gross, T. R. Performance regression testing of
concurrent classes. In InternationalSymposiumonSoftwareTesting&Analysis
(ISSTA)(2014).
[50]Pradel, M., Schuh, P., Necula, G., and Sen, K. Eventbreak: Analyzing the
responsivenessofuserinterfacesthroughperformance-guidedtestgeneration. InConferenceonObject-OrientedProgrammingSystems,Languages,andApplications
(OOPSLA) (2014).
[51]Ravjot, S., Cor-Paul, B., Weiyi, S., and E., H. A. Optimizing the performance-
related configurations of object-relational mapping frameworks using a multi-
objective genetic algorithm. In InternationalConferenceonPerformanceEngineer-
ing (ICPE) (2016).
[52]Sarkar, A., Guo, J., Siegmund, N., Apel, S., and Czarnecki, K. Cost-efficient
sampling for performance prediction of configurable systems. In International
Conferenceon Automated Software Engineering (ASE) (2015).
[53]Segura, S., Troya, J., Dur√°n, A., and Cort√©s, A. R. Performance metamorphic
testing: A proof of concept. Information& Software Technology 98 (2018), 1‚Äì4.
[54]Segura,S.,Troya,J.,Dur√°n,A.,andRuiz-Cort√©s,A.Performancemetamorphic
testing: Motivation and challenges. In International Conference on Software
Engineering: New Ideas and Emerging Technologies Results Track (ICSE-NIER),
(2017).
[55]Shen, D., Luo, Q., Poshyvanyk, D., and Grechanik., M. Automating perfor-
mance bottleneck detection using search-based application profiling. In Interna-
tionalSymposiumon Software Testing & Analysis (ISSTA) (2015).
[56]Siegmund, N., Grebhahn, A., Apel, S., and K√§stner, C. Performance-influence
modelsforhighlyconfigurablesystems. In EuropeanSoftwareEngineeringConfer-
ence and the International Symposium on the Foundations of Software Engineering
(ESEC/FSE) (2015).
[57]Sokolovaa, M., and Lapalmeb, G. A systematic analysis of performance mea-
sures for classification tasks. InformationProcessing & Management (2009).
[58]Song,L.,andLu,S.Statisticaldebuggingforreal-worldperformanceproblems.In
ConferenceonObject-OrientedProgrammingSystems,Languages,andApplications
(OOPSLA) (2014).
[59]Song, L., and Lu, S. Performance diagnosis for inefficient loops. In International
Conferenceon Software Engineering (ICSE) (2017).
[60]Souto, S., d‚ÄôAmorim, M., and Gheyi, R. Balancing soundness and efficiency for
practical testing of configurable systems. In International Conference on Software
Engineering(ICSE) (2017).
[61]Su, P., Wen, S., Yang, H., Chabbi, M., and Liu, X. Redundant loads: A software
inefficiency indicator. In International Conference on Software Engineering (ICSE)
(2019).
[62]Toffola,L.D.,Pradel,M.,andGross,T.R. Performanceproblemsyoucanfix:Adynamicanalysisofmemoizationopportunities. In ConferenceonObject-Oriented
Programming Systems, Languages, and Applications (OOPSLA) (2015).
[63]Xiang, C., Huang, H., Yoo, A., Zhou, Y., and Pasupathy, S. Pracextractor:
Extracting configuration good practices from manuals to detect server miscon-
figurations. In USENIX Annual Technical Conference (ATC) (2020).
[64]Xu, T., Zhang, J., Huang, P., Zheng, J., Sheng, T., Yuan, D., Zhou, Y., andPasupathy, S. Do not blame users for misconfigurations. In Symposium on
Operating Systems Principles (SOSP) (2013).
[65]Yang, J., Subramaniam, P., Lu, S., Yan, C., and Cheung, A. How notto structure
your database-backed web applications: a study of performance bugs in the wild.
InInternationalConference on Software Engineering (ICSE) (2018).
[66]Yu, T., and Pradel, M. Syncprof: Detecting, localizing, and optimizing synchro-
nization bottlenecks. In International Symposium on Software Testing & Analysis
(ISSTA)(2016).
[67]Zaparanuks, D., and Hauswirth., M. Algorithmic profiling. In Conferenceon
Programming Language Design and Implementation (PLDI) (2012).
[68]Zimmermann, T., Weisgerber, P., Diehl, S., and Zeller, A. Mining versionhistories to guide software changes. In International Conference on Software
Engineering(ICSE) (2005).
