Cross-Device Record and Replay for Android Apps
Cong Li
State Key Lab for Novel Software
Technology and Department of
Computer Science and Technology,
Nanjing University, Nanjing, China
congli@smail.nju.edu.cnYanyan Jiang
State Key Lab for Novel Software
Technology and Department of
Computer Science and Technology,
Nanjing University, Nanjing, China
jyy@nju.edu.cnChang Xu
State Key Lab for Novel Software
Technology and Department of
Computer Science and Technology,
Nanjing University, Nanjing, China
changxu@nju.edu.cn
ABSTRACT
Cross-device replay for Android apps is challenging because apps
have to adapt or even restructure their GUIs responsively upon
screen-size or orientation change across devices. As a first ex-
ploratory work, this paper demonstrates that cross-device record
and replay can be made simple and practical by a one-pass, greedy
algorithm by the Rx framework leveraging the least surprise prin-
ciple in the GUI design. The experimental results of over 1,000
replay settings encouragingly show that our implemented Rx pro-
totype tool effectively solved non-trivial cross-device replay cases
beyond any known non-search-based workâ€™s scope, and had still
competitive capabilities on same-device replay with start-of-the-art
techniques.
CCS CONCEPTS
â€¢Software and its engineering â†’Software maintenance tools ;
Application specific development environments .
KEYWORDS
Android app testing, record and replay
ACM Reference Format:
Cong Li, Yanyan Jiang, and Chang Xu. 2022. Cross-Device Record and Replay
for Android Apps. In Proceedings of the 30th ACM Joint European Software
Engineering Conference and Symposium on the Foundations of Software Engi-
neering (ESEC/FSE â€™22), November 14â€“18, 2022, Singapore, Singapore. ACM,
New York, NY, USA, 13 pages. https://doi.org/10.1145/3540250.3549083
1 INTRODUCTION
Record and replay is a trending technology [ 24,41,49,54,61]. In
the context of Android apps, record and replay refers to collecting
runtime log and replaying the log on later app runs to â€œreproduceâ€
a past app execution [ 5,6,10,12,20,21,23,25,26,30,38,44,48,53,
55,56,65]. Record and replay for Android apps is the foundation
of a broad spectrum of testing and debugging technologies: failure
reproduction [ 40,59,69], regression testing [ 11,31,34,42,43,63,
68], test case minimization [17], to name a few.
The ability of cross-environment adaptation is preferred for mod-
ern software, which is expected to grow with continuous evolution
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
ESEC/FSE â€™22, November 14â€“18, 2022, Singapore, Singapore
Â©2022 Association for Computing Machinery.
ACM ISBN 978-1-4503-9413-0/22/11. . . $15.00
https://doi.org/10.1145/3540250.3549083
Pixel 3 XL (1440x2960)Pixel XL (1440x2560)
Figure 1: The GUI layout of Google Calculator is respon-
sively restructured to adapt to different screen sizes, posing
challenges to cross-device record and replay.
for changable environments [ 62]. This paper thereby studies a rel-
evant problem about cross-environment adaptation: a variant of
traditional record and replay for Android apps, namely cross-device
record and replay (or R&R cfor abbreviation), where an app exe-
cution is replayed on a distinct device of different screen size or
orientation. R&R cenables developers to â€œ record once and replay
everywhere â€ and benefits various testing and debugging practices.
With R&R c, cross-device compatibility test cases only need to be
recorded on a single device, and regression test cases can be auto-
matically ported to different hardware platforms.
R&R cis challenging because an appâ€™s GUI layout responsively
adapts to the screen size and orientation, and the restructured
GUI layout may cause exercising the same action on different de-
vices to require quite different event sequences (Figure 1). To our
surprise, assuming that each recorded event ğ‘’â€™s receiver object
should present on the replay-time GUI, noknown replay work
[5,6,10,12,20,21,23,25,26,30,38,44,48,53,55,56,65] is sensi-
tive to GUI restructuring and well supports cross-device record and
replay even for the Androidâ€™s default Calculator app. Although it is
theoretically possible to search for a replay (e.g., via test migration
[11,31,43,68]), search-based approaches are limited in scalabil-
ity for industrial-size apps and practical usage scenarios because
state-space search incurs costly backtracking and app restarts.
This paper presents the Rx replay framework to develop practical
R&R cimplementations, especially for apps with responsive GUI
restructuring. The framework also works for apps without any
GUI restructuring. Specifically, this paper treats an app to have
395ESEC/FSE â€™22, November 14â€“18, 2022, Singapore, Singapore Cong Li, Yanyan Jiang, and Chang Xu
â€œresponsiveâ€ GUI restructuring if it follows the responsive UI design
principle defined officially [ 18]. The framework leverages the least
surprise principle [28, 45, 50] in the field of GUI design that
(1)GUI widgets have spatial locality. Specifically, functionally
related widgets are spatially adjacent in the GUI layout; and
(2)Cross-device GUI adaptation typically obeys a limited set
of responsive patterns officially recommended by Android
documentation [4, 18].
Accordingly, to replay an event ğ‘’whose receiver widget is not
present on the current GUI layout, the Rx framework follows a
natural, greedy procedure without backtracking or app restart by
(1)Performing UI segmentation (such that spatially adjacent
widgets belong to the same segment) and matching to iden-
tify the replay-time correspondence segment of ğ‘’â€™s receiver
object. For example, the â€œ ğœ‹â€ button (left) is matched with the
green bar (middle) in Figure 1.
(2)Tentatively applying a list of revertible responsive actions
untilğ‘’â€™s receiver widget appears (e.g., clicking the green bar).
In the case that any reasonable attempt fails, the effects can
be reverted by executing its pre-defined inverse action.
This paper further demonstrates that the Rx framework enables
practical cross-device record and replay even empowered with well-
known heuristics for UI segmentation/matching and simple respon-
sive patterns. The experimental results are encouraging that our
prototype implementation can correctly complete cross-device re-
play tasks beyond any known (non-search- and) event-based replay
workâ€™s scope (with an 86.7% successful rate) and still have a competi-
tive same-device replay capability with state-of-the-art same-device
only replay techniques.
In summary, this paperâ€™s contributions are: (1) the first exploratory
work to demonstrate that cross-device record and replay can be
made simple and practical, and (2) publicizing the Rx prototype
tool and supplementary materials to facilitate future research via
https://sites.google.com/view/rx-framework/home.
The rest of this paper is organized as follows. The cross-device
record and replay problem is defined and analyzed in Section 2. The
Rx record and replay algorithms (the framework) are described in
Section 3. A simple practical realization of the framework (including
a prototype implementation) and evaluation results are presented
in Sections 4 and 5, respectively. Finally, we discuss related work
in Section 6 and conclude the paper in Section 7.
2 PROBLEM AND INSIGHTS
This section presents the problem of cross-device record and replay
and our insights to mitigate the challenges. Particularly, we present
a formulation of our problem in Section 2.1, analyze the challenges
in Section 2.2, and elaborate on our insights in Section 2.3.
2.1 Problem Formulation
Event-Based Record and Replay . This paperâ€™s scope is limited
to event-based record and replay, the most practical and widely-
adopted [ 30] replay technique by replaying (redoing) a captured se-
quence of (GUI or system) events on another app execution. Though
limited in replaying pixel-precise gestures and system I/O (e.g., net-
work traffics), the lightweight nature of event-based record andreplay still makes it a best practice for Android app testing and
debugging [11, 31, 42, 63].
In this paper, an event ğ‘’is an object (a key-value mapping in
which obj.field denotes the value of the key field ) where:
â€¢ğ‘’.type is the event type, e.g., ui:click ;
â€¢ğ‘’.recv denotes the eventâ€™s associated receiver object (an
Android view1for a GUI event, or null for a system event);
â€¢ğ‘’.params contains the eventâ€™s parameters that can not derive
fromğ‘’.recv , e.g., an input string for ui:input events;
â€¢ğ‘’.context is the eventâ€™s context object consisting of the
timestamp, GUI layout, etc.
To replay a logged event ğ‘’on a replay device, a typical event-
based replay implementation can either deliver the logged low-
level event coordinates [ 10,12,21,25,44,65], click the same re-
ceiver object by matching widget ID ğ‘’.recv.id[20,23,48], or con-
duct semantic-aware widget matching based on ğ‘’.recv.text and
ğ‘’.context.UI[11, 31, 35, 42, 43, 63].
Cross-Device Event-Based Record and Replay . To replay an
event sequence ğœ=[ğ‘’1,ğ‘’2,Â·Â·Â·,ğ‘’|ğœ|]on another device, existing
work widely assumes that any ğ‘’ğ‘–â€™s receiverğ‘’ğ‘–.recv should exist at
the replay time [ 5,6,10,12,20,21,23,25,26,38,44,48,53,55,56,65].
Unfortunately, this assumption breaks for real-world R&R ccases
when an appâ€™s GUI is responsively restructured to adapt to a deviceâ€™s
screen size or orientation. For example, all known replay work failed
cross-device replaying the calculation of â€œ 2ğœ‹â€ in Figure 1. Figure 2
depicts a more complicated (but practical) case that inserting a
table in Microsoft Word requires quite different event sequences:
1â†’2â†’3â†’4on a phone v.s. 1â†’ 2on a tablet.
2.2 Challenges
Conceptually, R&R cis a search problem. Given any replay oracle
that can determine replay success (e.g., triggering the same crash,
manifesting the same GUI changes, or producing a similar log), one
can exhaustively try all possible event sequences until an oracle-
satisfying one is successfully replayed [ 11,31,40,42,43,59,63,69].
However, both the replay oracle and the search are far from trivial
in implementing a practical R&R c:
Challenge 1 .(Replay Oracle) There lacks an automatic replay
oracle for determining cross-device â€œreplay successâ€ for responsive
apps that display different GUI layouts across devices.
Existing replay oracle either targets same-device replay [12, 21,
26,40,44,59,65,69] or optimistically assumes that an appâ€™s GUI
changes follow limited patterns on distinct devices [ 5,6,10,20,
23,25,38,48,53,55,56], e.g., resizing a scrollable list. Such replay
oracles, insensitive to GUI restructuring, cannot be adopted to real-
world R&R cbecause an event possibly has no correspondence on
the replay device, e.g., 1for Microsoft Word in Figure 2.
Challenge 2 .(Search Space) Replaying practical usage scenarios
for industrial-size apps yields huge search spaces.
Although it is theoretically possible to search for a replay (e.g.,
via test migration [ 11,31,43]), search-based approaches are lim-
ited in scalability for industrial-size apps, in which a single GUI
1View is the basic GUI widget in Android. Following Android, we use the term â€œviewâ€
to replace â€œwidgetâ€ in the following paper.
396Cross-Device Record and Replay for Android Apps ESEC/FSE â€™22, November 14â€“18, 2022, Singapore, Singapore
Pixel 3 XL Phone (Portrait)Pixel C Tablet (Landscape)
2
1
3
Scenario Step 1Scenario Step 2Scenario Step 2Scenario Step 1
Record
and
Replayview 
segmentmatched 
segmentsmatched
segments
2
1
4
Figure 2: Microsoft Word responsively adapts to screens of different sizes. The event sequences for inserting a table are
1â†’2â†’3â†’4and 1â†’2on a phone/tablet, respectively. Screenshots are cropped such that only a receiver objectâ€™s ad-
jacent views are displayed. Key events have a 1-1 correspondence on different devices: 3v.s. 1and 4v.s. 2. To replay
1â†’2on a phone, 1and 2are required to reveal the â€œinsertâ€ button. In the reverse replay direction, 1and 2have no
correspondence on a tablet and thus should be discarded.
layout may contain tens or even hundreds of clickable views. The
attempt to even replay a single event (e.g., 1for Microsoft Word)
may produce a sub-search-space of millions of event sequences.
Efficient implementation of the search is also challenging because
backtracking often requires costly app restarts.
2.3 Observations and Insights
Structure of an Event Sequence . An execution to be replayed
represents a meaningful usage scenario . Natural observation is that
all events in a usage scenario are not created equal: there are key
events indicating a functionally critical action to be performed,
while the others are triggering events for exposing key eventsâ€™ re-
ceiver objects2. Consequently, an event sequence can be decom-
posed into a series of scenario steps , each of which ends with a
single key event for completing the action preceded by a prefix of
triggering events. For example, to insert a table in Microsoft Word
(the usage scenario in Figure 2), there are two scenario steps: (1)
activate the â€œInsertâ€ tab, and (2) click the â€œTableâ€ button. The key
events for them are 3and 4for the phone (or 1and 2for the
tablet), respectively.
Replay Oracle . Despite that not all event receivers have corre-
spondences on another replay device, key events should maintain
a 1-1 mapping between the record and replay devices since a key
event denotes a purposed action in a scenario step and should be
executed regardless of device. Our replay oracle thus can be defined
asthe in-order replay of all key events .
Nevertheless, the definition of key/triggering events of a usage
scenario is subjective to the concerned developer or app user. For
example, a developer may consider 1to be the triggering event
for 2while another may consider 1and 2to be different
scenario steps. Pragmatically, maximizing the successfully replayed
events (instead of searching for key events) is a reasonably well
automatic replay oracle. This is because replaying a triggering event
2Such an observation has been exploited in other Android-related tasks, e.g., bench-
marking test case reuse [68].(whose receiver object exists on both devices) is not expected to
break the replay procedure.
Search Space . Instead of an enumerative search, we greedily maxi-
mize the number of events to be replayed by optimistically assuming
that each event ğ‘’âˆˆğœis a key event and attempting small GUI per-
turbations to expose ğ‘’.recv . If the attempt to replay ğ‘’fails,ğ‘’should
be a triggering event (on the replay device) and we should proceed
the replay with ğ‘’being dropped. The validity of such a surprisingly
simple greedy algorithm is threefold:
First, well-designed apps very likely follow the least surprise
principle [ 28,45,50] with the spatial locality of views , i.e., func-
tionally related views are positioned in a visually adjacent block
(namely, a view segment in this paper). Therefore, even if a key
eventğ‘’â€™s receiver ( ğ‘’.recv ) is not present on the replay device, a
human should have no obstacle in identifying ğ‘’.recv â€™s correspond-
ing view segment on the replay device given the record-time GUI
layout. Red boxes in Figure 2 denote our identified segments.
Second, view segments would pragmatically obey limited respon-
sive patterns , a small set of officially recommended UI design rules
(Material Design [ 4,18]) for adapting view segments to different
sizes. Each responsive pattern also defines what (responsive) actions
a human should perform when interacting with that segment. To
revealğ‘’.recv on the replay device, one can attempt to perform
any responsive action to ğ‘’.recv â€™s corresponding view segment. For
example, ListView is associated with the scrolling action, which is
useful in finding ğ‘’.recv on a smaller device. Another example is
that 1clicks the Androidâ€™s official â€œMore Optionsâ€ button (with a
customized icon by Microsoft Word). This is a natural responsive
action for revealing a menu for subsequent actions. Focusing only
on responsive actions is the key insight to avoid an exhaustive
search.
Finally, even if applying a responsive action (e.g., calling out a
menu, scrolling a view, etc.) fails to reveal ğ‘’.recv , its effects can
easily be reverted by performing its inverse action (e.g., closing a
menu or scrolling in the reverse direction): every responsive action
397ESEC/FSE â€™22, November 14â€“18, 2022, Singapore, Singapore Cong Li, Yanyan Jiang, and Chang Xu
Algorithm 1: The Rx replay framework. Underlined func-
tions are implementation-customizable.
Input: An appğ´to be replayed on device Ë†ğ·
1Function RxReplay(ğœ)
2 foreach eventğ‘’âˆˆğœdo
3 Ë†ğ‘Ÿâ†empty sequence;
4 repeat
5 Ë†ğ‘†â†FindSegment(ğ‘’);
6 ifğ‘’.recvâˆˆË†ğ‘†then // the receiver object is revealed
7 sendğ‘’toË†ğ·;break ; // replay the next event
8âŸ¨Ë†ğ‘’,Ë†ğ‘’âˆ’1âŸ©â† FindRespAction(ğ‘’,Ë†ğ‘†);
9 ifË†ğ‘’=âŠ¥then //ğ‘’should be a triggering event
10 send Ë†ğ‘ŸtoË†ğ·;break ; // revert side effects
11 send Ë†ğ‘’toË†ğ·;ğ‘Ÿâ†[Ë†ğ‘’âˆ’1]::Ë†ğ‘Ÿ;
12 until TRUE ;
13Function FindSegment(ğ‘’)
14Sâ† SegmentUI(ğ‘’.context.UI);
15 Ë†Sâ† SegmentUI(Ë†ğ·.UI);
16ğ‘€â†MaxweightBipartMatch (Sâˆª Ë†S,ğ‘Š)where
ğ‘Šğ‘†,Ë†ğ‘†=Similarity(ğ‘†,Ë†ğ‘†)forğ‘†âˆˆS,Ë†ğ‘†âˆˆË†S;
17 return Ë†ğ‘†âˆˆË†Swhereğ‘’.recvâˆˆğ‘†and(ğ‘†,Ë†ğ‘†)âˆˆğ‘€;
18Function FindRespAction(ğ‘’,Ë†ğ‘†)
19 forğ‘ƒâˆˆRespPatterns do
20âŸ¨Ë†ğ‘’,Ë†ğ‘’âˆ’1âŸ©â†ğ‘ƒ(ğ‘’,Ë†ğ‘†); // Applying a pattern returns a pair of
eventsâŸ¨Ë†ğ‘’,Ë†ğ‘’âˆ’1âŸ©such that Ë†ğ‘’âˆ’1reverts Ë†ğ‘’â€™s side effects
21 ifË†ğ‘’â‰ âŠ¥then returnâŸ¨Ë†ğ‘’,Ë†ğ‘’âˆ’1âŸ©;
22 returnâŸ¨âŠ¥,âŠ¥âŸ©;
should be reversible in a well-designed UI. If trying all responsive
actions still cannot reveal ğ‘’.recv ,ğ‘’is considered a triggering event
on the replay device and thus discarded, yielding our one-pass
greedy replay algorithm without costly backtracking or app restart.
In Figure 2, when replaying 1â†’2â†’3â†’4on a tablet, 1and
2are dropped because all attempts to manifest them fail, while 3
and 4are successfully replayed by 1and 2, respectively. When
replaying 1on a phone, consecutively applying two responsive
actions (further explained in Section 4.1) reveals 1â€™s receiver
object (thus 3can be replayed): calling out a menu ( 1)â†’revealing
a sibling tab ( 2), and 2is directly replayed by 4.
3 THE RX FRAMEWORK
The Greedy Replay Framework . Following the previous anal-
yses, given a recorded event trace ğœon deviceğ·, the Rx replay
framework (Algorithm 1) is an online algorithm that can handle a
stream of events without backtracking or app restarts, as contrast
to prior work [ 11,31,40,42,43,59,63,69]. For each event ğ‘’to
be replayed, the algorithm tries to reveal ğ‘’.recv â€™s correspondence
on the replay device Ë†ğ·and replayğ‘’3. Upon failure (Line 9), the
algorithm reverts all responsive actions and proceeds with ğ‘’being
discarded.
3This paperâ€™s visual notation convention is that Ë†â–¡denotes â–¡â€™s replay-time counterpart.Algorithm 2: Spatial-locality guided UI segmentation.
1Function SegmentUI(ğ‘†)
2Pâ†{âŸ¨ğ‘†â„“,ğ‘†ğ‘ŸâŸ©|(ğ‘†â„“âˆªğ‘†ğ‘Ÿ=
ğ‘†)âˆ§there exists a horizontal/vertical line separating ğ‘†â„“,ğ‘†ğ‘Ÿ};
3âŸ¨ğ‘†âˆ—
â„“,ğ‘†âˆ—ğ‘ŸâŸ©â† arg max
âŸ¨ğ‘†â„“,ğ‘†ğ‘ŸâŸ©âˆˆPNaturalness(ğ‘†â„“,ğ‘†ğ‘Ÿ);
4 ifNaturalness(ğ‘†âˆ—
â„“,ğ‘†âˆ—ğ‘Ÿ)<THRESHOLD then
5 return{ğ‘†}; // return current segment
6 else
7 return SegmentUI(ğ‘†âˆ—
â„“)âˆªSegmentUI(ğ‘†âˆ—ğ‘Ÿ);
Considering that both spatial locality and responsive patterns
are design principles from a human perspective, Rx as an exten-
sible framework leaves the three human-related components free
customizable (which are underlined in Algorithm 1):
â€¢SegmentUI(ğ‘†)for partitioning a given UI ( ğ‘†, a set of views)
into disjoint view segments. Views within the same segment
should be spatially adjacent and functionally related.
â€¢Similarity(ğ‘†,Ë†ğ‘†)for measuring the similarity between seg-
mentğ‘†and Ë†ğ‘†in terms of app functionality.
â€¢RespPatterns is a list of supported responsive actions. For
each pattern ğ‘ƒin the list,ğ‘ƒ(ğ‘’,Ë†ğ‘†)returns an event and its
inverseâŸ¨Ë†ğ‘’,Ë†ğ‘’âˆ’1âŸ©for applying and reverting ğ‘ƒtoË†ğ‘†. Ifğ‘ƒis not
applicable to Ë†ğ‘†,ğ‘’=Ë†ğ‘’=âŠ¥.
UI Segmentation and Matching . To find an eventâ€™s receiver ob-
ject on the replay device, we first segment both GUIs ( ğ‘’.context.UI
andË†ğ·.UI) into view segments by invoking SegmentUI (Line 5, Lines
13â€“17). We argue that any non-surprising GUI design should make
ğ‘’.recv â€™s containing segment ğ‘†to also appear on Ë†ğ·. Therefore, we
attempt to find the global maximum weighted bipartite matching
between view segments in the two GUIs and narrow the scope of
our replay to Ë†ğ‘†,ğ‘†â€™s correspondence on Ë†ğ·(Lines 16â€“17). In case that
ğ‘’.recv finds its equivalent counterpart in Ë†ğ‘†, we directly send ğ‘’to
Ë†ğ·(Lines 6â€“7) and complete the replay of ğ‘’.
Applying Responsive Patterns . Onceğ‘’.recv has no correspon-
dence in Ë†ğ‘†, the algorithm applies a sequence of responsive actions
to revealğ‘’.recv . Sometimes, multiple patterns in RespPatterns
can be applied (e.g., Options and Tranx(1)are applied when re-
playing 1on the phone in Figure 2). We generally recommend
that the list of responsive patterns RespPatterns can be sorted by
the simplicity (simpler patterns are closer to the list head), such
that the algorithm (Lines 19â€“21) automatically returns the simplest
pattern following Occamâ€™s razor [ 13] (Line 21). The insight is that
UI design should be simple without surprising a human [ 28,45,50],
and simplest patterns should involve the least perturbation of GUI
and the least side effects.
The attempt to reveal ğ‘’.recv may be repeated several times (the
loop in Lines 4). Each time a responsive action Ë†ğ‘’is performed, its
inverse Ë†ğ‘’âˆ’1is pushed to a stack ğ‘Ÿ(Line 11). If all attempts are failed
(Line 9),ğ‘’should be a triggering event on Ë†ğ·, and events in the stack
are popped to revert all side effects of responsive actions (Line 10).
The replay proceeds with trying to replay the next event.
Sometimes, the greediness of the algorithm may cause an un-
intended event ğ‘’(a triggering event on ğ·, but neither key nor
398Cross-Device Record and Replay for Android Apps ESEC/FSE â€™22, November 14â€“18, 2022, Singapore, Singapore
Table 1. List of 13 experience responsive patterns.
Rk Name Precondition (ğ‘Ÿ=ğ‘’.recvâˆˆğ‘†)âŸ¨Ë†ğ‘’,Ë†ğ‘’âˆ’1âŸ©
1 Expand(1)âˆƒË†ğ‘£âˆˆË†ğ‘†s.t.Ë†ğ‘£.text starts withğ‘Ÿ.text or
ğ‘Ÿ.text starts with Ë†ğ‘£.textclick Ë†ğ‘£,
click back
2 Expand(2)âˆƒË†ğ‘£âˆˆparent(Ë†ğ‘†)s.t.Ë†ğ‘£.text starts with
ğ‘Ÿ.text orğ‘Ÿ.text starts with Ë†ğ‘£.textclick Ë†ğ‘£,
click back
3 Expand(3)âˆƒË†ğ‘£âˆˆË†ğ‘†s.t.Ë†ğ‘£.desc =ğ‘Ÿ.text orğ‘Ÿ.desc =
Ë†ğ‘£.textclick Ë†ğ‘£,
click back
4 Scroll ğ‘Ÿ.parent is scrollable and Ë†ğ‘£âˆˆË†ğ‘†s.t.Ë†ğ‘£is
ğ‘Ÿ.parent â€™s counterpartscroll
â‡‹Ë†ğ‘£,
scroll
â‡‹Ë†ğ‘£
5 OptionsâˆƒË†ğ‘£âˆˆË†ğ‘†s.t.Ë†ğ‘£is Androidâ€™s official â€œmore op-
tionsâ€ buttonclick Ë†ğ‘£,
clickâŠ¥
6 Menu âˆƒË†ğ‘£âˆˆË†ğ‘†s.t.Ë†ğ‘£.desc contains text â€œclose/open
navigation drawerâ€click Ë†ğ‘£,
click back
7 Tranx(1)ğ‘Ÿis aTabandâˆƒË†ğ‘£âˆˆË†ğ‘†s.t.ğ‘ŸandË†ğ‘£are siblings click Ë†ğ‘£,
click back
8 Tranx(2)âˆƒğ‘£âˆˆğ‘†.âˆƒË†ğ‘£âˆˆË†ğ‘†s.t.ğ‘£is parent ofğ‘Ÿ,ğ‘£is aTab,
andË†ğ‘£isğ‘£â€™s counterpartclick Ë†ğ‘£,
click back
9 Pager(1)ğ‘Ÿ.parent is a Pager andâˆƒË†ğ‘£âˆˆË†ğ‘†s.t.Ë†ğ‘£is
ğ‘Ÿ.parent â€™s counterpartclick Ë†ğ‘£,
click back
10 Pager(2)âˆƒË†ğ‘£âˆˆË†ğ‘†s.t.Ë†ğ‘£.parent isPager andğ‘Ÿis a child
ofË†ğ‘£click Ë†ğ‘£,
click back
11 Divide(1)âˆƒË†ğ‘†â€²âˆˆË†ğ·.âˆƒË†ğ‘£â€²âˆˆË†ğ‘†â€²s.t.Ë†ğ‘†â€²is aNavUI with a
nested List Ë†ğ‘£â€², and Ë†ğ‘†is aFrag UIclick back ,
click Ë†ğ‘£âˆ—2
12 Divide(2)âˆƒâ„“âˆˆğ‘†.âˆƒğ‘£âˆˆğ‘†.âˆƒË†ğ‘£âˆˆË†ğ‘†s.t.ğ‘£â€™s parentâ„“is a
List ,ğ‘£is selected, and Ë†ğ‘£isğ‘£â€™s counterpartclick Ë†ğ‘£,
click back
13 NavigateâˆƒË†ğ‘£âˆˆË†ğ‘†s.t.Ë†ğ‘£is Androidâ€™s official â€œnavigation
upâ€ buttonclick Ë†ğ‘£,
click Ë†ğ‘£âˆ—2
1Responsive patterns (rows) are listed in the order of simplicity. A responsive
pattern returnsâŸ¨Ë†ğ‘’,Ë†ğ‘’âˆ’1âŸ©when its precondition is satisfied (otherwise, âŸ¨âŠ¥,âŠ¥âŸ©is
returned). Patterns 1â€“13 correspond to expand, reveal, transform, and divide pat-
terns defined in Material Design [ 18]. Patterns 11â€“12 also appear in Androidâ€™s
fragment documentation [4].
2Ë†ğ‘£âˆ—isğ‘£â€™s most semantically related view on Ë†ğ·. Our implementation selects the
view of a maximum Similarity with Ë†ğ‘†.
triggering on Ë†ğ·) to be replayed. Such (triggering) events usually
do not cause breaking UI changes, and thus replaying them will
not likely result in subsequent replay failures. The greedy nature
of the algorithm may also drive the replay to a â€œdead endâ€ in which
no responsive pattern is applicable, and all subsequent events are
discarded. As shown in our evaluation (Section 5), such failure cases
are mainly due to the app not strictly following responsive patterns.
Discussions . There can be alternatives to implement the interfaces
(SegmentUI ,Similarity , and RespPatterns ) required by the Rx
framework. Since all the three functions are classifiers related to
computer-human interaction, data-driven approach (e.g., statistical
learning) certainly applies.
However, considering that there is no publicly available dataset
for the cross-device replay task (collecting data and training classi-
fiers is generally less relevant to the R&R cproblem) and data-driven
approach has its unique challenges (e.g., interpretability, hyper-
parameter tuning, and privacy issues for using end-user traces),
we demonstrate the power of Rx framework by leveraging well-
known heuristics and simple rules for which all designs can be well
explained and justified.
4 A PRACTICAL REALIZATION OF RX
We encode Android and UI design domain-specific knowledge
as heuristic algorithms and responsive patterns as our proof-of-
concept Rx instantiation. Despite being simple, the encouragingevaluation results in Section 5 confirmed the practical merits of the
Rx framework.
4.1 SegmentUI ,Similarity , and RespPatterns
UI Segmentation . Our SegmentUI (Algorithm 2) algorithm cre-
ates an â€œinterpretableâ€ UI segmentation following the VIPS algo-
rithm [ 15]. Intuitively, the algorithm resembles printing out all
views inğ‘†on a paper and conducting the most â€œnaturalâ€ hori-
zontal or vertical cutting off, following the intuition that horizon-
tal/vertical lines are the most natural way for human beings to
separate functional blocks. We enumerate all possible cut-off lines
(Line 2) and find the partition âŸ¨ğ‘†âˆ—
â„“,ğ‘†âˆ—ğ‘ŸâŸ©that best bisects unrelated
functionalities (Line 3). Such â€œpaper cuttingâ€ is recursively con-
ducted on both parts ğ‘†âˆ—
â„“andğ‘†âˆ—ğ‘Ÿ(Line 7), until splitting ğ‘†yields an
unnatural bisection of views (Lines 4â€“5).
The naturalness of a bisection is measured by the function
Naturalness . Specifically, a natural bisection expects that (1) views
in the same part are both spatially and functionally â€œcloseâ€ to each
other, whereas (2) views in different parts are â€œfar awayâ€ from each
other. Such close/faraway measurements are captured by a set of
functionsğœğ‘–(1â‰¤ğ‘–â‰¤ğ‘›, the smaller the closer), e.g., ğœbg(ğ‘£,ğ‘£â€²)
regardsğ‘£andğ‘£â€²as close if they have similar background colors.
Givenğœğ‘–and their weights ğ‘¤ğ‘–,Naturalness(ğ‘†â„“,ğ‘†ğ‘Ÿ)measures the
bisectionâ€™s naturalness by a weighted combination
Naturalness(ğ‘†â„“,ğ‘†ğ‘Ÿ)=1Ã
1â‰¤ğ‘–â‰¤ğ‘›ğ‘¤ğ‘–Ã•
1â‰¤ğ‘–â‰¤ğ‘›ğ‘¤ğ‘–Â·Â©Â­
Â«Ã•
ğ‘£âˆˆğ‘†â„“Ã•
ğ‘£â€²âˆˆğ‘†ğ‘Ÿğœğ‘–(ğ‘£,ğ‘£â€²)
âˆ’Ã•
ğ‘£âˆˆğ‘†ğ‘ŸÃ•
ğ‘£â€²âˆˆğ‘†ğ‘Ÿğœğ‘–(ğ‘£,ğ‘£â€²)âˆ’Ã•
ğ‘£âˆˆğ‘†â„“Ã•
ğ‘£â€²âˆˆğ‘†â„“ğœğ‘–(ğ‘£,ğ‘£â€²)ÂªÂ®
Â¬
in which for ğ‘£andğ‘£â€²that are far away from the other, splitting
them intoğ‘†â„“andğ‘†ğ‘Ÿmakes positive contributions to the naturalness,
while pairing them into ğ‘†â„“orğ‘†ğ‘Ÿmakes negative contributions. We
explored 6 functions (i.e., ğœğ‘–) who share the same weight (i.e.,1
6)
in our prototype. We tuned THRESHOLD byCalculator andMS
Outlook (and it generalizes well to other subjects (Table 2) in our
experiments). Our supplementary material provides more details
on how these functions work.
Similarity Measurement . We measure the similarity between
segmentsğ‘†and Ë†ğ‘†via the textural information of views. For each
viewğ‘£, we collect textual descriptions from ğ‘£.text ,ğ‘£.desc ,ğ‘£.id, and
ğ‘£.hint . Collected texts are pre-processed with stop words removed,
tokenized into words, and bagged up as a document. Afterward,
we calculate the TF-IDF [ 29,33,60] vectorğ‘¢and Ë†ğ‘¢forğ‘†â€™s and Ë†ğ‘†â€™s
document, respectively. Finally, Similarity(ğ‘†,Ë†ğ‘†)is measured by
the standard cosine similarity of their corresponding vectors:
Similarity(ğ‘†,Ë†ğ‘†)=ğ‘¢Â·Ë†ğ‘¢
âˆ¥ğ‘¢âˆ¥2Ã—âˆ¥Ë†ğ‘¢âˆ¥2.
Responsive Patterns . We investigated the Top 100 apps in the
Google Play Store [ 22] and distilled 13 frequently used patterns
for gracefully responding to different screen sizes and orientations
as listed in Table 1. Each patterns is either defined by Material
Design [ 18] (i.e., Expand ,Reveal ,Divide , and Transform ) or explicitly
mentioned and recommended by Android documentation [ 4] for
user-friendly UIs. For example, on smaller screens, the Options
399ESEC/FSE â€™22, November 14â€“18, 2022, Singapore, Singapore Cong Li, Yanyan Jiang, and Chang Xu
Table 2. Information of evaluated apps
App Name Short Category #Install
Calculator GC Tools 108.7+
Youtube YT Video Pla. & Edi 109.7+
MS Todo MST Productivity 106.7+
MS Outlook MSO Productivity 108
MS Word MSW Productivity 109
AdobeReader AAR Productivity 108.7+
Firefox FF Communication 108
DoodleMaster DM Art & Design 106
AdobeSparkPost ASP Art & Design 107
Zedge ZDG Personalization 108
CollageMaker CM Photography 107.7+
Calm CA Health & Fitness 107
Audible ADB Books & Ref. 108
KingJamesBible KJB Books & Ref. 107
Webtoon WT Comics 107.7+
ESPN ES Sports 107.7+
Discord DC Communication 108
uDates UD Dating 105.7+
Remind RM Education 107
Spotify SP Music & Audio 108.7+
Reddit RD News & Magazines 107.7+
Summary First Set: 7 Apps; Second Set: 14 Apps
pattern hides some function-related and less-important buttons
via a â€œMore Optionsâ€ button; while the Divide pattern divides a
layered UI into multiple layers (UIs) [ 18]. Readers may refer to
our supplementary material for illustrative descriptions of these
responsive patterns.
4.2 The Rx Prototype Tool
We implement a prototype named Rx, which includes the frame-
work (Section 3) and a complete reference implementation of the
algorithms in Section 4.1, in âˆ¼10,000 lines of TypeScript and âˆ¼1,000
lines of Kotlin. SegmentUI ,Similarity , and RespPatterns are
implemented as modules and can be flexibly replaced by other
implementations.
The Rx recorder traces the events by running the app under
record in a VirtualXposed [ 57] container, which does not require
any root access to the Android system. It hooks system-level APIs
[2] (e.g., Activity#dispatchTouchEvent ) to trace runtime events
right before an event is consumed by the app. The Rx replayer
depends only on standard Android tools UI Automator [ 7] to capture
GUI layouts and Android debugging bridge (adb) [ 3] to exercise
GUI events.
5 EVALUATION
Our evaluation is designed around the following two research ques-
tions:
RQ1 To what extent does Rx push forward the state-of-the-art of
cross-device record and replay?
RQ2 What is the time and space (logging) overhead of Rx on cross-
device record and replay?
RQ3 What are the causes of replay failures for Rx?Table 3. Case study results of Rx over top commercial apps
adopting non-trivial responsive patterns
ID App (Scenario) Cross-Device Replay
#1 GC (Simple Calc) A B A C B C
#2 GC (Advanced Calc) A B A C B C
#3 GC (Show History) A B A C B C
#4 GC (Show Fraction) A B A C B C
#5 GC (Show His. Fra.) A B A C B C
#6 YT (Explore App) A B A C B C
#7 YT (Enable Dark Theme) A B A C B C
#8 YT (Browse Trending) A B A C B C
#9 YT (Subscribe TBS) A B A C B C
#10 MST (Goto Categories) A B A C B C
#11 MST (New+Del. Task) A B A C B C
#12 MST (Accom. Task) A B A C B C
#13 MST (Edit Task) A B A C B C
#14 MST (Del. List) A B A C B C
#15 MSO (Goto Folders) A B A C B C
#16 MSO (Check Email) A B A C B C
#17 MSO (Donâ€™t Disturb) A B A C B C
#18 MSO (Filter Email) A B A C B C
#19 MSO (Mark Email) A B A C B C
#20 MSW (Open Doc) A B A C B C
#21 MSW (Ins.+Del. Table) A B A C B C
#22 MSW (Ren. Doc) A B A C B C
#23 MSW (New+Save Doc) A B A C B C
#24 MSW (Goto Settings) A B A C B C
#25 AAR (Check ToUP) A B A C B C
#26 AAR (Open Welcome) A B A C B C
#27 FF (Nav. To Google) A B A C B C
#28 FF (New Privacy Tab) A B A C B C
#29 FF (Delete Browser Data) A B A C B C
#30 FF (Change Site Perm.) A B A C B C
Summary 93.3% 83.3% 83.3%
Arrow direction indicates a replay success. For example, A Cdenotes that a
trace record on Ais successfully replayed on Cbut not vice versa; A C
denotes that the replay is successful in both directions.
5.1 Methodology
Experimental Subjects . To answer these two questions, we eval-
uate Rx using two sets of top commercial apps from Google Play
Store [ 22] and conduct record and replay on three typical emulated
Android devices of different screen sizes and densities:
(1) Pixel XL Phone ( A): Portrait, 1440Ã—2560, 560dpi,
(2) Pixel 3 XL Phone ( B): Portrait, 1440Ã—2960, 560dpi,
(3) Pixel C Tablet ( C): Landscape, 2560Ã—1800, 320dpi.
The first set of apps for evaluation (first half of Table 2) are top-
downloaded apps that adopt non-trivial responsive patterns, whose
replay should be sensitive to GUI restructuring and is out of the
capability of any known available (non-search-based) record-and-
replay technique. This list contains heavy apps (e.g., MS Word and
Youtube ) and interesting non-trivial apps (e.g., Androidâ€™s official
Calculator app, which adopts a non-trivial reveal responsive
400Cross-Device Record and Replay for Android Apps ESEC/FSE â€™22, November 14â€“18, 2022, Singapore, Singapore
Table 4. Experimental results of comparing Rx with RERAN and SARA
ID App (Scenario)Same-Device Replay Cross-Device Replay
Rx RERAN SARA Rx SARA
#31 DM (Create New Canvas) AB C AB C AB C A B A C B C A B A C B C
#32 DM (Rate As Like) AB C AB C AB C A B A C B C A B A C B C
#33 ASP (Goto Craft) AB C AB C AB C A B A C B C A B A C B C
#34 ASP (Switch Tabs) AB C AB C AB C A B A C B C A B A C B C
#35 ASP (Search Sea) AB C AB C AB C A B A C B C A B A C B C
#36 ASP (Create Post) AB C AB C AB C A B A C B C A B A C B C
#37 ASP (Edit Post) AB C AB C AB C A B A C B C A B A C B C
#38 ZDG (Goto Feat. - G.I.U) AB C AB C AB C A B A C B C A B A C B C
#39 ZDG (Goto Cate. - Tech.) AB C AB C AB C A B A C B C A B A C B C
#40 ZDG (Search Video W.P.) AB C AB C AB C A B A C B C A B A C B C
#41 CM (Explore Shop) AB C AB C AB C A B A C B C A B A C B C
#42 CM (Change Settings) AB C AB C AB C A B A C B C A B A C B C
#43 CM (Make Grid) AB C AB C AB C A B A C B C A B A C B C
#44 CM (Make F.S.) AB C AB C AB C A B A C B C A B A C B C
#45 CM (Make M.F.) AB C AB C AB C A B A C B C A B A C B C
#46 CA (Random Nav.) AB C AB C AB C A B A C B C A B A C B C
#47 CA (Explore Home Medi.) AB C AB C AB C A B A C B C A B A C B C
#48 CA (Search Music Love) AB C AB C AB C A B A C B C A B A C B C
#49 CA (Change Settings) AB C AB C AB C A B A C B C A B A C B C
#50 ADB (Explore App) AB C AB C AB C A B A C B C A B A C B C
#51 ADB (Search Lovecraft) AB C AB C AB C A B A C B C A B A C B C
#52 ADB (Filter Lovecraft) AB C AB C AB C A B A C B C A B A C B C
#53 KJB (Explore App) AB C AB C AB C A B A C B C A B A C B C
#54 KJB (Search Love) AB C AB C AB C A B A C B C A B A C B C
#55 KJB (Edit Font Settings) AB C AB C AB C A B A C B C A B A C B C
#56 WT (Explore App) AB C AB C AB C A B A C B C A B A C B C
#57 WT (Choose Ori.:Sup.) AB C AB C AB C A B A C B C A B A C B C
#58 WT (Sort Canvas) AB C AB C AB C A B A C B C A B A C B C
#59 WT (Check App Ver.) AB C AB C AB C A B A C B C A B A C B C
#60 ES (Explore App) AB C AB C AB C A B A C B C A B A C B C
#61 ES (Search Lakers) AB C AB C AB C A B A C B C A B A C B C
#62 DC (Explore App) AB C AB C AB C A B A C B C A B A C B C
#63 DC (Send Message) AB C AB C AB C A B A C B C A B A C B C
#64 DC (Check Chan. Info) AB C AB C AB C A B A C B C A B A C B C
#65 DC (Set Status) AB C AB C AB C A B A C B C A B A C B C
#66 UD (Explore App) AB C AB C AB C A B A C B C A B A C B C
#67 UD (Chat With Diane) AB C AB C AB C A B A C B C A B A C B C
#68 RM (Explore App) AB C AB C AB C A B A C B C A B A C B C
#69 RM (Create Class) AB C AB C AB C A B A C B C A B A C B C
#70 RM (Edit Class) AB C AB C AB C A B A C B C A B A C B C
#71 SP (Explore App) AB C AB C AB C A B A C B C A B A C B C
#72 SP (Browse Hip Hop) AB C AB C AB C A B A C B C A B A C B C
#73 SP (Search RapCaviar) AB C AB C AB C A B A C B C A B A C B C
#74 SP (Cre.+Del. Playlist) AB C AB C AB C A B A C B C A B A C B C
#75 RD (Explore App) AB C AB C AB C A B A C B C A B A C B C
#76 RD (Join Typescript Leave) AB C AB C AB C A B A C B C A B A C B C
#77 RD (Check Andr. Comm.) AB C AB C AB C A B A C B C A B A C B C
Summary 83.7% 84.4% 35.5% 81.9% 78.7% 77.7% 33.0% 31.9% 31.9%
For same-device replay, a colored device icon ( A/B/C) indicates a replay success, and a dotted gray device icon ( A/B/C) indicates a replay failure. For
cross-device replay, arrow direction indicates a replay success. For example, A Cdenotes that a trace record on Ais successfully replayed on Cbut not vice versa;
A Cdenotes that the replay is successful in both directions.
401ESEC/FSE â€™22, November 14â€“18, 2022, Singapore, Singapore Cong Li, Yanyan Jiang, and Chang Xu
pattern). To the best of our knowledge, all known existing R&R c
techniques (including those not evaluated) will fail in cross-device
replaying Calculator (Figure 1).
The second set of apps for evaluation (second half of Table 2) are
top-downloaded apps whose replay falls into existing workâ€™s scope.
Particularly, we select the state-of-the-art open-source record and
replay tools RERAN [ 21] (for single-device record and replay) and
SARA [ 23] (for both single- and cross-device record and replay)
as our comparison baselines4. To ensure a fair comparison within
SARAâ€™s scope (SARA only supports the simplest expand responsive
pattern), we adopted the following filtering process to select the
second set of experimental apps: (1) Top-1 or top-2 apps of each
category ranked by AppBrains [ 9] are selected; (2) Any app that is
beyond the expand responsive pattern or incompatible with emu-
lated environments is excluded. We downloaded these apps from
ApkCombo [ 8], and the filtering process yielded 28 apps. To ensure
a fair comparison, we also excluded 14 apps that SARA failed to
parse, finally yielding the set of 14 apps for evaluation5.
For each evaluated app, we followed existing work [ 23,30] to
select and create usage scenarios. Particularly, we created 3â€“5 sce-
narios for each app that represent its most common functionalities.
We list them in Column 1 of Table 3 and Table 4. Readers may refer
to our website for detailed information on all experimental subjects,
including detailed descriptions, reproduction steps, and key events
of each usage scenario.
Answering RQ1 . The first part of the evaluation concerns whether
Rx is practically useful in cross-device replay . Particularly, we con-
ducted a case study and evaluated Rx on the 30 usage scenarios
of the first 7 apps in Table 2, each consisting of six runs: A B,
A C, and B C. Specifically, we record a usage scenario on
each device (phone or tablet) and replay them on the other two. For
this set of apps, we did not compare Rx with any existing technique
because the replay goes beyond the capabilities of them. We tried
to run SARA and RERAN on these cases but they failed for almost
every usage scenario.
The second part of the evaluation compares the effectiveness of Rx
with existing record and replay techniques within their scopes . This
involves two sub-cases:
(1)Same-device replay , in which we record and replay a usage
scenario on the same device ( A,B, and C), over the 47
usage scenarios of the 14 subjects for comparison. All three
techniques RERAN, SARA, and our Rx are evaluated.
(2)Cross-device replay of the simplest expand responsive pattern ,
which falls into the scope of SARA. In this experiment, the
same 47 usage scenarios are reused, each with six replay
settings: A B,A C, and B C. In this R&R ccase,
we only compare Rx with SARA because RERAN is designed
for same-device replay and failed for almost every usage
scenario.
4For closed-source tools, we contacted the authors of RANDR [ 48] and V2S [ 12] for
tool binaries but received no response. We also tried to compare with appetizer [ 10],
which was state-of-the-practice. Unfortunately, we have to exclude it because it was
extremely unstable and frequently lost events.
5However, we also evaluated the excluded 14 apps (only on Rx and RERAN), and the
experimental results are available on our website. Overall, Rx received a 78.3% and
82.5% successful rate in cross- and single-device record and replay, respectively, while
RERAN obtained a 74.4% single-device successful rate.Following our analysis in Section 2.3, we adopt the following
oracle to determine the success of cross-device replay for both parts
of the evaluation:
(1) Objectively, all key events are replayed in order;
(2)Subjectively, we confirm that the â€œgoalâ€ of the usage scenario
is successfully achieved.
For the same-device replay of the second part, we follow existing
work [ 12,21,23,30] to determine a replay a success if (1) all events
are replayed in order and (2) all visual state transitions meet a
human developerâ€™s expectation.
Answering RQ2 . We collect record/replay time statistics and com-
pare them with the original execution (without our instrumentation)
for each Rxâ€™s successful cross-device replay to evaluate the runtime
overhead of Rx. We also collect the disk usage of all logs (without
compression) generated by Rx to measure the space overhead.
Answering RQ3 . We manually inspect failed cases and categorize
their root causes. The detailed analyses can be found in Section 5.4.
Experimental Environments . Overall, the experiments consist
of 1,167 different replay settings (technique Ã—usage scenarioÃ—
device). We observed negligible flakiness in the replayed usage sce-
narios, and we consider a replay success if twoconsecutive replays
both satisfied the replay oracle. All experiments were conducted
on a quad-core Intel i7-7700 desktop with 32GiB RAM running
Ubuntu 20.04.1 LTS, with Android API 27. Rx and the two base-
lines are evaluated under the same emulated hardware/software
configuration.
5.2 Evaluation Results for RQ1: Effectiveness
Case Study . Table 3 displays the case study results. This exper-
iment involves 30 usage scenarios with each replayed six runs:
A B,A C, and B C, yielding 180 replay runs in total.
Overall, Rx succeeded in 86.7% (156/180), in which 80.0% (24/30)
scenarios succeeded in all six runs.
We would like to emphasize that these usage scenarios are out
of any known (non-search-based) techniqueâ€™s capabilities. For ex-
ample, SARA [ 23] attributes its replay failure of Calculator (re-
sembles A B) to â€œbreaking UI changesâ€. However, this is a reveal
responsive pattern recommended by the Androidâ€™s official guide-
line. On the other hand, Rx succeeded in all five usage scenarios of
theCalculator app. Readers may refer to our website for further
information, including a video of a C Areplay for Calculator ,
where cross-device replaying the event trace (consisting of 11 clicks
and 1 swipe) on Ayields a different trace of 19 clicks and 1 swipe
to make the replay successful.
Same-Device Record and Replay . The â€œSame-Device Replayâ€
column in Table 4 displays the evaluation results. We compare Rx,
RERAN, and SARA using the scenarios #31â€“#77 in Table 4. We
record and replay each scenario on the same device ( A,B,C),
yielding 141 replay runs for each evaluated technique.
Rx succeeded in 83.7% (118/141) replay runs. The numbers are
84.4% (119/141) for RERAN and 35.5% (50/141) for SARA. Overall,
Rx has a competitive successful replay rate with the pixel-precise,
state-of-the-art same-device replay technique RERAN, even though
the Rx replayer has a much more complex workflow.
402Cross-Device Record and Replay for Android Apps ESEC/FSE â€™22, November 14â€“18, 2022, Singapore, Singapore
On the other hand, Rx is the most stable tool in the evaluation
that produces consistent results over devices. For Rx, 80.9% (38/47)
of the usage scenarios were successfully replayed on all three de-
vices. The numbers are 66.0% (31/47) for RERAN and 23.4% (11/47)
for SARA. Rx has better stability than RERAN because replaying
raw events in RERAN suffers from minor non-deterministic device
behaviors (e.g., scrolling has small non-determinism across replay
runs). In contrast, Rxâ€™s semantic-aware matching mechanism can
tolerate such behaviors. Furthermore, the analyses in Section 5.4
show that the 9 failure cases for app CollageMaker are due to
missing accessibility information.
It is a surprise that SARA failed in so many cases. SARA fre-
quently froze (or even crashed) at record time due to flushing large
amounts of logs6. The offline self-replay of SARA also contributed
to many replay failures. The self-replay of SARA is expected to
be a faithful same-device replay used to translate raw information
(e.g., coordinate) to views. However, self-replaying a scrolling event
may unfaithfully yield different scroll distances due to minor non-
determinism of UI, and SARAâ€™s replay thereby fails if self-replay
diverges. Rx does not suffer from this because Rx grabs the view
information when recoding and does not need a self-replay phase
to translate.
Cross-Device Record and Replay . The â€œCross-Device Replayâ€
column in Table 4 displays the evaluation results. This experiment
is based on the previous 47 usage scenarios (#31â€“#77) created for
same-device replay. We excluded RERAN in this experiment because
replaying raw input events on another device of a different screen
makes nonsense. Each scenario consists of six runs, yielding 282
replay runs in total.
Rx succeeded in 79.4% (224/282) cases, in which 63.8% (30/47)
scenarios succeeded in all the six replay runs. Despite that all these
usage scenarios have been carefully chosen to fit into SARAâ€™s scope,
the numbers for SARA are 32.3% (91/282) and 19.1% (9/47), resulting
from similar failure causes to the same-device replay.
5.3 Evaluation Results for RQ2: Overhead
The performance evaluation results are summarized as follows (and
detailed results are available in the supplementary materials):
(1)The record-time logging costs 372ms and 23KiB of log (with-
out compression) per event on average. This includes the
time (âˆ¼300ms) to capture GUIs and space ( âˆ¼23KiB) saving
GUIâ€™s layout dump. Such a cost is required by any GUI layout
based replay technique [5, 20, 23, 48, 58].
(2)The replay-time UI segmentation, responsive pattern match-
ing, and responsive action execution cost 411ms on average,
where applying a responsive action occupies âˆ¼350ms.
(3) For events whose receiver does not exist at the replay time,
averagely 1.6 triggering events are additionally performed
to expose the key event receiver on the replay device.
6Our evaluation was conducted on an Android Emulated Device with hardware virtual-
ization over a mainstream desktop Intel i7 processor, which is significantly faster than
a mobile processor. Therefore, the performance issue of SARA should be considered
an implementation limitation.Overall, the runtime and space overhead is considered affordable
for a human operator/replayer, indicating that our Rx implementa-
tion enabled practical R&R cuses, e.g., cross-device unit testing for
an app developer.
Compared with existing event-based replay tools which are not
sensitive to GUI restructuring and assume any event ğ‘’â€™s receiver
should exist at the replay time [ 21,23,48], Rx pays only 7.31% time
overhead and 0.2 additional (triggering) events to gain the cross-
device replay capability towards GUI restructuring. This result is
consistent with the least surprise principle in GUI design that a
human should be able to find the key event with least GUI perturba-
tion. Moreover, capturing GUI layouts and executing events occupy
âˆ¼80% of the runtime overhead. This conforms to Wang et al. â€™s study
[58] and thereby the performance of Rx can be further improved
by replacing UI Automator with Toller.
The average log size of Rx is 23KiB (3.3KiB) per event and 198KiB
(29KiB) per usage scenario without (with) compression [ 71]. Such
a low7space usage is within our expectation because Rxâ€™s logging
is intentionally designed to use as less disk space as possible while
record the GUI dump of every event. Specifically, Rxâ€™s log maintains
a pool to deduplicate strings and views. Each view (and viewâ€™s prop-
erty) in the GUI layout is represented by its index in the pool, and
the GUI layout tree is then serialized as a flattened index sequence
by the pre-order of residing views.
5.4 Evaluation Results for RQ3: Failure Cases
Though largely outperforming existing work, Rx still failed at 105
out of 603 replays (17.4%), including both single- and cross-device
replay cases affecting 23 usage scenarios of 15 apps in Tables 3 and 4.
We analyze them in detail to shed light on future improvements.
The visual convention is that an italic text denotes the cause of a
replay failure, and underlined numbers denote the case counts.
Framework Limitations (2/105, 1.9%) . All event-based replay
algorithms (including Rx) assume that both record and replay are
conducted in a deterministic environment. However, a perfectly
deterministic environment simply does not exist. Even though the
experiments demonstrated that Rx can better handle minor UI non-
determinism than a pixel-precise replay mechanism (Rx produced
consistent and stable results over devices in the same-device replay
experiments), apps may contain dynamic contents (2) that change
over time (e.g., real-time news feeds) and result in replay failures.
Similar to prior work [ 5,6,10,20,23,25,55], Rx is limited in re-
playing pixel-precise complex gestures (e.g., pinch) and network
traffic. Considering that both deterministic replay (usually for fail-
ure reproduction) and pixel-precise replay are out of the scope of
event-based replay, we believe that the Rx framework has well-
supported cross-device record and replay.
Implementation Limitations (60/105, 57.1%) . Rx framework re-
lies on the interfaces SegmentUI ,Similarity , and RespPatterns
that are implementation-specific for performing human-related UI
understanding. Our current implementation is limited and results
in some replay failures.
The most frequent failure cause that affects our SegmentUI im-
plementation is overlapping views (30), where a horizontal/vertical
7Should note that our evaluation includes apps that are deemed to have complicated
GUIs, e.g., MS Word ,ESPN , and Webtoon .
403ESEC/FSE â€™22, November 14â€“18, 2022, Singapore, Singapore Cong Li, Yanyan Jiang, and Chang Xu
split cannot yield a correct partition. In these cases, a correct seg-
mentation must â€œcut throughâ€ at least one view, which is strictly
prohibited in Algorithm 2.
OurRespPatterns are also limited in identifying particular pat-
terns. For example, the ğ‘ƒex-sl pattern tries a fixed percentage of
scrolling offset for a scrollable list. However, sometimes unstable
scroll (4) events may cause the replayer to over-scroll the list and
miss the target event. Other related failure causes include failing
to model a pattern ( 8) and failing to recognize a known pattern ( 6).
Considering that our tool is designed to be extensible, these limita-
tions can be mitigated by either adding new patterns or rewriting
existing patterns for adapting to specific apps.
Finally, our tool implementation has its limitations ( 12). For
example, Rx does not support self-rendered apps (e.g., WebView
and game-engine empowered apps) by far because UI Automator
fails to dump them.
App Bugs (43/105, 41.0%) . To our surprise, the basic usage sce-
narios for experimental evaluation even revealed functional bugs (4)
in these top commercial apps. A crash bug is from AdobeReader
(usage scenario #25, specific to C, causing 4 replay failures),
which non-deterministically hits a null-pointer deference in the
appâ€™s native library libADCComponents.so . The triggering is non-
deterministic, potentially due to concurrency issues. Another non-
crash bug is from KingJamesBible (usage scenario #55, all devices)
where the view displaying the font of the current text failed to
refresh over font changes. Considering that the latter bug did not
affect replay, we still consider it a replay success in Table 4.
The final major cause of replay failures is appâ€™s accessibility bugs
(39) because our Rx Similarity andRespPatterns are based on
precise accessibility information, particularly viewâ€™s textual de-
scriptions that enable visually impaired people to use the phone
with text-to-speech technologies. These accessibility bugs can be
further classified into incomplete (33) or incorrect (6) accessibility
information. Rx cannot conduct correct UI segmentation and re-
sponsive pattern recognition on empty accessibility information
(likeCollageMaker ). Rx can be erroneously trapped into an un-
expected replay state on incorrect accessibility information (like
MS Outlook andRemind ). For a similar reason, people with vi-
sion impairment will suffer from using these apps. In this sense,
replay success is expected if Similarity andRespPatterns do
not depend on accessibility information.
We reported the accessibility bug in MS Outlook (with usage
scenario #15, in which â€œcloseâ€/â€œopenâ€ is labeled in the opposite), the
only evaluated app that provides an explicit in-app bug report mech-
anism8. We received positive confirmation from the developers, and
they claimed a bug fix in version 4.21.3.
5.5 Discussions
RQ1&2: Effectiveness and Overhead of the Rx Framework .
Overall, Rx succeeded in 498 of 603 replays (82.6%) with affordable
overhead. The evaluation results indicate that Rx well handles
cross-device replay cases over apps that have GUI restructuring.
For usage scenarios that are within existing workâ€™s scope, Rx gains
8Accessibility bug is a well-known source of Android appâ€™s bugs, and thus we did not
report the other bugs. Regarding more apps in the Play Store, approximately 45% of
GUIs have at least one ImageButton that misses accessibility textual description [ 16].a competitive or better capability than state-of-the-art techniques.
Furthermore, Rxâ€™s results are considerably more consistent and
stable across devices.
RQ3: A Call for Practical R&R c. The analyses of failure cases
first revealed that only very few replay failures (less than 1% in
all replays) are out of the capability of the Rx replay framework
(Algorithm 1). Perhaps this is our most significant implication:
event-based replay, even for the challenging cross-device case, can be
done greedily online .
Considering the other failure cases, this paper could be regarded
asa call for future research along R&R c. It is expected that a heuristic
algorithm (like the one in Section 4.1) cannot handle all real-world
cases: overlapping views cause SegmentUI to perform incorrect
segmentation, and the quality of accessibility information signifi-
cantly impacts Similarity andRespPatterns . These limitations
can be potentially resolved by data-driven approaches, e.g., by
learning distributed representations of views. We are optimistic
that learning-based approaches, e.g., V2S [ 12], will facilitate better
implementations of SegmentUI ,Similarity , and RespPatterns .
Threats to Validity . The first threat to validity is that only the
most popular apps are selected in the evaluation. Most of them are
commercial apps developed by professional teams. Therefore, the
evaluation results may not generalize to other apps, e.g., less well-
maintained open-source apps. We intentionally chose these most
popular apps because they usually incorporate complex patterns in
adapting to different screen sizes. The major conclusion concerning
the effectiveness of Rx replay framework should remain positive
on simpler apps with less complex responsive patterns9.
Another threat is that all usage scenarios are created by the
authors, which may be subject to bias. To best alleviate this issue, we
tried our best to select the most typical usage scenarios by following
existing work [ 23,30]. We also provide detailed descriptions of these
usage scenarios on our website to enable reproducible research in
the future.
The final threat is that the replay success is determined by a
human, even though such a human in the loop seems unavoidable.
The analyses in Section 2 show that key events are a good basis for
determining replay success. We strictly followed this guidance as
the replay oracle to avoid bias from humans.
6 RELATED WORK
Record and replay is a fundamental enabling technology for a broad
spectrum of testing and debugging practices and has been exten-
sively studied in various contexts.
Record and Replay for Android Apps . RERAN [ 21] exploits
Linuxâ€™s input subsystem ( /dev/input/event* ) for logging and re-
playing pixel-precise hardware-level input events, in which views
are located by absolute coordinates. Pixel-preciseness is a desirable
property for deterministic apps. However, as shown in our exper-
iments, even minor non-determinism can result in replay failure.
Other pixel-based approaches include VALERA [ 26] (a customized
system) and Mobiplay [ 44] (record and replay on a remote-desktop).
Such a pixel-precise treatment can easily fail to replay a trace on a
device of different screen size or orientation. To enable cross-device
9However, Rx does not well support apps that refuse a correct application of officially
recommended responsive patterns.
404Cross-Device Record and Replay for Android Apps ESEC/FSE â€™22, November 14â€“18, 2022, Singapore, Singapore
replay to some extent, one may proportionally scale the coordinates
[6,10,12,25,55,65] or localize a logged event ğ‘’â€™s receiver object
(at replay time) by its attributes [ 5,20,23,38,48,53,55,56,70], e.g.,
using textural information of views [ 5,20,23,35,48]. However, all
these techniques made a fundamental assumption that each logged
eventğ‘’â€™s receiver object exists on the replay-time GUI. Unfortu-
nately, modern apps often adopt Androidâ€™s responsive design that
could extensively restructure the UI layout across devices (usage
scenarios #48-77). This paper is the first to consider these cases as
in-scope.
Finally, as discussed in Section 2, one may incorporate an exhaus-
tive search for an oracle-satisfying event sequence [ 11,31,40,42,43,
59,63,69]. A sufficiently efficient search will ultimately solve R&R c
and many other problems (e.g., test input generation) and is still
an open problem. On the other hand, this paper as an exploratory
demonstrates that search may not be required for a practical R&R c.
Record and Replay for Other Event-Based Systems . There is
record and replay work for other event-based systems, e.g., Web ap-
plications. Web applications have a simpler single-threaded event
model, whose execution is easier to be deterministically traced
[1,37,47,52,66]. Upon that, a developer-friendly interactive de-
bugger [ 14] or further sophisticated dynamic analyses [ 51] can
be implemented. Generally, Web applications are simpler in the
execution model and responsive patterns, and have a particular
focus on the network side [ 41]. Thus, the record and replay of Web
application is a considerably different scope.
Deterministic Replay for Other Systems . A general programâ€™s
execution can also be made fully (or partially) deterministic via
runtime tracing. Such deterministic replay techniques may involve
input tracing [ 46], instruction tracing [ 19], or memory tracing [ 27].
These (deterministic) approaches could incur significantly higher
overhead and are more intrusive than a lightweight event-based
record and replay (this paperâ€™s scope). We do not discuss them
further because they target a different replay purpose.
GUI Understanding and Analyses . Finally, the implementations
of UI segmentation and responsive patterns are related to GUI under-
standing, e.g., learning probabilistic distribution of GUI layouts for
detecting and repairing GUI bad-smells and flaws. UIS-Hunter [ 64]
and Seenormly [ 67] exploit computer-vision techniques to detect vi-
olations with respect to Material Designâ€™s donâ€™t-do-that guidelines.
LabelDroid [ 16] and COALA [ 36] repair missing textual descrip-
tions for ImageView -like views using data-driven approaches. Gvt
[39] and OwlEye [ 32] detect gaps between GUI designs and their
implementations. We are optimistic that these techniques, orthog-
onal to this paperâ€™s scope, can facilitate further development of
cross-device record and replay technologies.
7 CONCLUSION
Record and replay is a foundational technology for a broad spectrum
of Android app testing and debugging practices. This paper made
the cross-device record and replay practical for industrial-scale
apps with respect to responsive GUI restructurings by leveraging
the principle of least surprise in GUI design, i.e., spatial locality
and responsive patterns, with promising experimental evaluationresults. We hope this work, which pushes forward the state-of-the-
art of cross-device record and replay for Android apps, will serve
as a call for future research along this line.
ACKNOWLEDGMENTS
The authors would like to thank the anonymous reviewers for
their valuable feedback. This work was supported by the Natu-
ral Science Foundation of China under Grant Nos. 61932021 and
62025202, and the Leading-edge Technology Program of Jiangsu
Natural Science Foundation under Grant No. BK20202001. The
authors would like to thank the support from the Collaborative
Innovation Center of Novel Software Technology and Industrializa-
tion, Jiangsu, China. Yanyan Jiang (jyy@nju.edu.cn) and Chang Xu
(changxu@nju.edu.cn) are the corresponding authors.
REFERENCES
[1]Silviu Andrica and George Candea. 2011. WaRR: A Tool for High-Fidelity Web
Application Record and Replay. In Proceedings of the 2011 IEEE/IFIP International
Conference on Dependable Systems Networks (DSN â€™11) . 403â€“410. https://doi.org/
10.1109/DSN.2011.5958253
[2]Android. 2021. Android API Reference . https://developer.android.com/reference/
[3]Android. 2021. Android Debug Bridge (adb) . https://developer.android.com/
studio/command-line/adb
[4]Android. 2021. Android Fragments . https://developer.android.com/guide/
fragments
[5]Android. 2021. Espresso . https://developer.android.com/training/testing/espresso
[6]Android. 2021. monkeyrunner . https://developer.android.com/studio/test/
monkeyrunner
[7]Android. 2021. UI Automator . https://developer.android.com/training/testing/ui-
automator
[8] ApkCombo. 2021. ApkCombo . https://apkcombo.com
[9]AppBrain. 2021. Google Play Ranking: The Top Free Overall . https://www.appbrain.
com/stats/google-play-rankings
[10] Appetizer. 2021. appetizerio/replaykit . https://github.com/appetizerio/replaykit
[11] Farnaz Behrang and Alessandro Orso. 2019. Test Migration between Mobile Apps
with Similar Functionality. In Proceedings of the 2019 IEEE/ACM International
Conference on Automated Software Engineering (ASE â€™19) . 54â€“65. https://doi.org/
10.1109/ASE.2019.00016
[12] Carlos Bernal-CÃ¡rdenas, Nathan Cooper, Kevin Moran, Oscar Chaparro, Andrian
Marcus, and Denys Poshyvanyk. 2020. Translating Video Recordings of Mobile
App Usages into Replayable Scenarios. In Proceedings of the 2020 ACM/IEEE
International Conference on Software Engineering (ICSE â€™20) . 309â€“321. https:
//doi.org/10.1145/3377811.3380328
[13] Anselm Blumer, Andrzej Ehrenfeucht, David Haussler, and Manfred K Warmuth.
1987. Occamâ€™s razor. Information processing letters 24, 6 (1987), 377â€“380.
[14] Brian Burg, Richard Bailey, Andrew J. Ko, and Michael D. Ernst. 2013. Interactive
Record/Replay for Web Application Debugging. In Proceedings of the 2013 Annual
ACM Symposium on User Interface Software and Technology (UIST â€™13) . 473â€“484.
https://doi.org/10.1145/2501988.2502050
[15] Deng Cai, Shipeng Yu, JiRong Wen, and WeiYing Ma. 2003. VIPS: a Vision-based
Page Segmentation Algorithm. (2003).
[16] Jieshan Chen, Chunyang Chen, Zhenchang Xing, Xiwei Xu, Liming Zhu, Guo-
qiang Li, and Jinshui Wang. 2020. Unblind Your Apps: Predicting Natural-
Language Labels for Mobile GUI Components by Deep Learning. In Proceedings
of the 2020 ACM/IEEE International Conference on Software Engineering (ICSE â€™20) .
322â€“334. https://doi.org/10.1145/3377811.3380327
[17] Wontae Choi, Koushik Sen, George Necula, and Wenyu Wang. 2018. DetReduce:
Minimizing Android GUI Test Suites for Regression Testing. In Proceedings of
the 2018 International Conference on Software Engineering (ICSE â€™18) . 445â€“455.
https://doi.org/10.1145/3180155.3180173
[18] Material Design. 2021. Responsive Patterns . https://material.io/archive/guidelines/
layout/responsive-ui.html#responsive-ui-patterns
[19] George W. Dunlap, Samuel T. King, Sukru Cinar, Murtaza A. Basrai, and Peter M.
Chen. 2002. ReVirt: Enabling Intrusion Analysis through Virtual-Machine Log-
ging and Replay. In Proceedings of the 2002 Symposium on Operating Systems
Design and Implementation (OSDI â€™02) . 211â€“224. https://doi.org/10.1145/844128.
844148
[20] Mattia Fazzini, Eduardo Noronha De A. Freitas, Shauvik Roy Choudhary, and
Alessandro Orso. 2017. Barista: A Technique for Recording, Encoding, and
Running Platform Independent Android Tests. In Proceedings of the 2017 IEEE
International Conference on Software Testing, Verification and Validation (ICST â€™17) .
405ESEC/FSE â€™22, November 14â€“18, 2022, Singapore, Singapore Cong Li, Yanyan Jiang, and Chang Xu
149â€“160. https://doi.org/10.1109/ICST.2017.21
[21] Lorenzo Gomez, Iulian Neamtiu, Tanzirul Azim, and Todd Millstein. 2013. RERAN:
Timing- and Touch-Sensitive Record and Replay for Android. In Proceedings
of the 2013 International Conference on Software Engineering (ICSE â€™13) . 72â€“81.
https://doi.org/10.1109/ICSE.2013.6606553
[22] Google. 2021. Google Play Store . https://play.google.com/store
[23] Jiaqi Guo, Shuyue Li, Jian-Guang Lou, Zijiang Yang, and Ting Liu. 2019. Sara:
Self-Replay Augmented Record and Replay for Android in Industrial Cases. In
Proceedings of the 2019 ACM SIGSOFT International Symposium on Software Testing
and Analysis (ISSTA â€™19) . 90â€“100. https://doi.org/10.1145/3293882.3330557
[24] Zhenyu Guo, Xi Wang, Jian Tang, Xuezheng Liu, Zhilei Xu, Ming Wu, M. Frans
Kaashoek, and Zheng Zhang. 2008. R2: An Application-Level Kernel for Record
and Replay. In Proceedings of the 2008 USENIX Conference on Operating Systems
Design and Implementation (OSDI â€™08) . 193â€“208. https://doi.org/10.5555/1855741.
1855755
[25] Matthew Halpern, Yuhao Zhu, Ramesh Peri, and Vijay J. Reddi. 2015. Mosaic:
Cross-Platform User-Interaction Record and Replay for The Fragmented An-
droid Ecosystem. In Proceedings of the 2015 IEEE International Symposium on
Performance Analysis of Systems and Software (ISPASS â€™15) . 215â€“224. https:
//doi.org/10.1109/ISPASS.2015.7095807
[26] Yongjian Hu, Tanzirul Azim, and Iulian Neamtiu. 2015. Versatile yet Lightweight
Record-and-Replay for Android. In Proceedings of the 2015 ACM SIGPLAN Inter-
national Conference on Object-Oriented Programming, Systems, Languages, and
Applications (OOPSLA â€™15) . 349â€“366. https://doi.org/10.1145/2814270.2814320
[27] Jeff Huang, Peng Liu, and Charles Zhang. 2010. LEAP: Lightweight Deterministic
Multi-Processor Replay of Concurrent Java Programs. In Proceedings of the 2010
ACM SIGSOFT International Symposium on Foundations of Software Engineering
(FSE â€™10) . 385â€“386. https://doi.org/10.1145/1882291.1882361
[28] Geoffrey James. 1987. Law of Least Astonishment. The Tao of Programming
(1987).
[29] Karen Sparck Jones. 1972. A Statistical Interpretation of Term Specificity and Its
Application in Retrieval. Journal of documentation (1972).
[30] Wing Lam, Zhengkai Wu, Dengfeng Li, Wenyu Wang, Haibing Zheng, Hui Luo,
Peng Yan, Yuetang Deng, and Tao Xie. 2017. Record and Replay for Android: Are
We There yet in Industrial Cases?. In Proceedings of the 2017 Joint Meeting on
Foundations of Software Engineering (ESEC/FSE â€™17) . 854â€“859. https://doi.org/10.
1145/3106237.3117769
[31] Jun-Wei Lin, Reyhaneh Jabbarvand, and Sam Malek. 2019. Test Transfer Across
Mobile Apps Through Semantic Mapping. In Proceedings of the 2019 IEEE/ACM
International Conference on Automated Software Engineering (ASE â€™19) . 42â€“53.
https://doi.org/10.1109/ASE.2019.00015
[32] Zhe Liu, Chunyang Chen, Junjie Wang, Yuekai Huang, Jun Hu, and Qing Wang.
2020. Owl Eyes: Spotting UI Display Issues via Visual Understanding. In Pro-
ceedings of the 2020 IEEE/ACM International Conference on Automated Software
Engineering (ASE â€™20) . 398â€“409. https://doi.org/10.1145/3324884.3416547
[33] Hans Peter Luhn. 1957. A Statistical Approach to Mechanized Encoding and
Searching of Literary Information. IBM Journal of research and development 1, 4
(1957), 309â€“317.
[34] Baoying Ma, Li Wan, Nianmin Yao, Shuping Fan, and Yan Zhang. 2020. Evo-
lutionary Selection for Regression Test Cases Based on Diversity. Frontiers of
Computer Science 15, 2 (2020), 2095â€“2236.
[35] Leonardo Mariani, Ali Mohebbi, Mauro PezzÃ¨, and Valerio Terragni. 2021. Seman-
tic Matching of GUI Events for Test Reuse: Are We There Yet?. In Proceedings of
the 2021 ACM SIGSOFT International Symposium on Software Testing and Analysis
(ISSTA â€™21) . 177â€“190. https://doi.org/10.1145/3460319.3464827
[36] Forough Mehralian, Navid Salehnamadi, and Sam Malek. 2021. Data-Driven
Accessibility Repair Revisited: On the Effectiveness of Generating Labels for
Icons in Android Apps. In Proceedings of the 2021 ACM Joint Meeting on European
Software Engineering Conference and Symposium on the Foundations of Software
Engineering (ESEC/FSE â€™21) . 107â€“118. https://doi.org/10.1145/3468264.3468604
[37] James Mickens, Jeremy Elson, and Jon Howell. 2010. Mugshot: Deterministic
Capture and Replay for Javascript Applications. In Proceedings of the 2010 USENIX
Conference on Networked Systems Design and Implementation (NSDI â€™10) . 11. https:
//doi.org/10.5555/1855711.1855722
[38] Kevin Moran, Richard Bonett, Carlos Bernal-CÃ¡rdenas, Brendan Otten, Daniel
Park, and Denys Poshyvanyk. 2017. On-Device Bug Reporting for Android
Applications. In Proceedings of the 2017 International Conference on Mobile Software
Engineering and Systems (MOBILESoft â€™17) . 215â€“216. https://doi.org/10.1109/
MOBILESoft.2017.36
[39] Kevin Moran, Boyang Li, Carlos Bernal-CÃ¡rdenas, Dan Jelf, and Denys Poshy-
vanyk. 2018. Automated Reporting of GUI Design Violations for Mobile Apps. In
Proceedings of the 2018 International Conference on Software Engineering (ICSE
â€™18). 165â€“175. https://doi.org/10.1145/3180155.3180246
[40] Kevin Moran, Mario Linares-VÃ¡squez, Carlos Bernal-CÃ¡rdenas, Christopher Ven-
dome, and Denys Poshyvanyk. 2016. Automatically Discovering, Reporting and
Reproducing Android Application Crashes. In Proceedings of the 2016 IEEE In-
ternational Conference on Software Testing, Verification and Validation (ICST â€™16) .
33â€“44. https://doi.org/10.1109/ICST.2016.34[41] Ravi Netravali, Anirudh Sivaraman, Somak Das, Ameesh Goyal, Keith Winstein,
James Mickens, and Hari Balakrishnan. 2015. Mahimahi: Accurate Record-and-
Replay for HTTP. In Proceedings of the 2015 USENIX Conference on Usenix Annual
Technical Conference (USENIX ATC â€™15) . 417â€“429. https://doi.org/10.5555/2813767.
2813798
[42] Minxue Pan, Tongtong Xu, Yu Pei, Zhong Li, Tian Zhang, and Xuandong Li. 2020.
GUI-Guided Test Script Repair for Mobile Apps. IEEE Transactions on Software
Engineering (2020), 1â€“1. https://doi.org/10.1109/TSE.2020.3007664
[43] Xue Qin, Hao Zhong, and Xiaoyin Wang. 2019. TestMig: Migrating GUI Test
Cases from IOS to Android. In Proceedings of the 2019 ACM SIGSOFT International
Symposium on Software Testing and Analysis (ISSTA â€™19) . 284â€“295. https://doi.
org/10.1145/3293882.3330575
[44] Zhengrui Qin, Yutao Tang, Ed Novak, and Qun Li. 2016. MobiPlay: A Remote
Execution Based Record-and-Replay Tool for Mobile Applications. In Proceedings
of the 2016 International Conference on Software Engineering (ICSE â€™16) . 571â€“582.
https://doi.org/10.1145/2884781.2884854
[45] Eric Steven Raymond. 2003. Applying the Rule of Least Surprise. The Art of Unix
Programming (2003).
[46] rr. 2021. rr. https://rr-project.org
[47] rrweb. 2021. rrweb: Record and Replay the Web . https://www.rrweb.io
[48] Onur Sahin, Assel Aliyeva, Hariharan Mathavan, Ayse K. Coskun, and Manuel
Egele. 2019. RandR: Record and Replay for Android Applications via Targeted
Runtime Instrumentation. In Proceedings of the 2019 IEEE/ACM International
Conference on Automated Software Engineering (ASE â€™19) . 128â€“138. https://doi.
org/10.1109/ASE.2019.00022
[49] Yasushi Saito. 2005. Jockey: A User-Space Library for Record-Replay Debugging.
InProceedings of the 2005 International Symposium on Automated Analysis-Driven
Debugging (AADEBUG â€™05) . 69â€“76. https://doi.org/10.1145/1085130.1085139
[50] Peter Seebach. 2001. The Cranky User: The Principle of Least Astonishment. IBM
DeveloperWorks (2001).
[51] Koushik Sen, Swaroop Kalasapur, Tasneem Brutch, and Simon Gibbs. 2013.
Jalangi: A Selective Record-Replay and Dynamic Analysis Framework for
JavaScript. In Proceedings of the 2013 Joint Meeting on Foundations of Software
Engineering (ESEC/FSE â€™13) . 488â€“498. https://doi.org/10.1145/2491411.2491447
[52] Sara Sprenkle, Emily Gibson, Sreedevi Sampath, and Lori Pollock. 2005. Auto-
mated Replay and Failure Detection for Web Applications. In Proceedings of the
2005 IEEE/ACM International Conference on Automated Software Engineering (ASE
â€™05). 253â€“262. https://doi.org/10.1145/1101908.1101947
[53] CulebraTester Team. 2021. CulebraTester . http://culebra.dtmilano.com/
[54] GDB Team. 2021. GDB: The GNU Project Debugger . https://www.gnu.org/
software/gdb/
[55] Ranorex Team. 2021. Ranorex . https://www.ranorex.com/
[56] Robotium Team. 2021. Robotium . https://github.com/RobotiumTech/robotium
[57] tiann. 2021. VirtualXposed . https://github.com/android-hacker/VirtualXposed
[58] Wenyu Wang, Wing Lam, and Tao Xie. 2021. An Infrastructure Approach to
Improving Effectiveness of Android UI Testing Tools. In Proceedings of the 2021
ACM SIGSOFT International Symposium on Software Testing and Analysis (ISSTA
â€™21). 165â€“176. https://doi.org/10.1145/3460319.3464828
[59] Martin White, Mario Linares-VÃ¡squez, Peter Johnson, Carlos Bernal-CÃ¡rdenas,
and Denys Poshyvanyk. 2015. Generating Reproducible and Replayable Bug
Reports from Android Application Crashes. In Proceedings of the 2015 IEEE In-
ternational Conference on Program Comprehension (ICPC â€™15) . 48â€“59. https:
//doi.org/10.1109/ICPC.2015.14
[60] Wikipedia. 2021. Tfâ€“Idf . https://en.wikipedia.org/wiki/Tf%E2%80%93idf
[61] Andreas Wundsam, Dan Levin, Srini Seetharaman, and Anja Feldmann. 2011.
OFRewind: Enabling Record and Replay Troubleshooting for Networks. In Pro-
ceedings of the 2011 USENIX Conference on USENIX Annual Technical Conference
(USENIX ATC â€™11) . 29. https://doi.org/10.5555/2002181.2002210
[62] Chang Xu, Yi Qin, Ping Yu, Chun Cao, and Jian Lu. 2020. Theories and Techniques
for Growing Software: Paradigm and Beyond. SCIENTIA SINICA Informationis
50, 11 (2020), 1595â€“1611.
[63] Tongtong Xu, Minxue Pan, Yu Pei, Guiyin Li, Xia Zeng, Tian Zhang, Yuetang
Deng, and Xuandong Li. 2021. GUIDER: GUI Structure and Vision Co-Guided
Test Script Repair for Android Apps. In Proceedings of the 2021 ACM SIGSOFT
International Symposium on Software Testing and Analysis (ISSTA â€™21) . 191â€“203.
https://doi.org/10.1145/3460319.3464830
[64] Bo Yang, Zhenchang Xing, Xin Xia, Chunyang Chen, Deheng Ye, and Shanping
Li. 2021. Donâ€™t Do That! Hunting Down Visual Design Smells in Complex UIs
Against Design Guidelines. In Proceedings of the 2021 IEEE/ACM International
Conference on Software Engineering (ICSE â€™21) . 761â€“772. https://doi.org/10.1109/
ICSE43902.2021.00075
[65] Shengcheng Yu, Chunrong Fang, Yang Feng, Wenyuan Zhao, and Zhenyu Chen.
2019. LIRAT: Layout and Image Recognition Driving Automated Mobile Testing
of Cross-Platform. In Proceedings of the 2019 IEEE/ACM International Conference
on Automated Software Engineering (ICSE â€™19) . 1066â€“1069. https://doi.org/10.
1109/ASE.2019.00103
[66] Lu Zhang and Chao Wang. 2017. RClassify: Classifying Race Conditions in Web
Applications via Deterministic Replay. In Proceedings of the 2017 International
406Cross-Device Record and Replay for Android Apps ESEC/FSE â€™22, November 14â€“18, 2022, Singapore, Singapore
Conference on Software Engineering (ICSE â€™17) . 278â€“288. https://doi.org/10.1109/
ICSE.2017.33
[67] Dehai Zhao, Zhenchang Xing, Chunyang Chen, Xiwei Xu, Liming Zhu, Guoqiang
Li, and Jinshui Wang. 2020. Seenomaly: Vision-Based Linting of GUI Animation
Effects against Design-Donâ€™t Guidelines. In Proceedings of the 2020 ACM/IEEE
International Conference on Software Engineering (ICSE â€™20) . 1286â€“1297. https:
//doi.org/10.1145/3377811.3380411
[68] Yixue Zhao, Justin Chen, Adriana Sejfia, Marcelo Schmitt Laser, Jie Zhang, Fed-
erica Sarro, Mark Harman, and Nenad Medvidovic. 2020. FrUITeR: A Framework
for Evaluating UI Test Reuse. In Proceedings of the 2020 ACM Joint Meeting on
European Software Engineering Conference and Symposium on the Foundations ofSoftware Engineering (ESEC/FSE â€™20) . 1190â€“1201. https://doi.org/10.1145/3368089.
3409708
[69] Yu Zhao, Tingting Yu, Ting Su, Yang Liu, Wei Zheng, Jingzhi Zhang, and William
G. J. Halfond. 2019. ReCDroid: Automatically Reproducing Android Application
Crashes from Bug Reports. In Proceedings of the 2019 International Conference on
Software Engineering (ICSE â€™19) . 128â€“139. https://doi.org/10.1109/ICSE.2019.00030
[70] Jiahuan Zheng, Liwei Shen, Xin Peng, Hongchi Zeng, and Wenyun Zhao. 2020.
MashReDroid: Enabling End-User Creation of Android Mashups Based on Record
and Replay. Science China Information Sciences 63, 10 (2020), 1869â€“1919.
[71] zlib. 2021. zlib: A Massively Spiffy Yet Delicately Unobtrusive Compression Library .
https://www.zlib.net
407