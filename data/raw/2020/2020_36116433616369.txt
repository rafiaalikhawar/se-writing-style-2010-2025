Efficient Text-to-Code Retrieval with Cascaded Fast and Slow
Transformer Models
Akhilesh Deepak Gotmareâˆ—
Salesforce AI Research
Singapore
akhilesh.gotmare@salesforce.comJunnan Li
Salesforce AI Research
Singapore
junnan.li@salesforce.com
Shafiq Joty
Salesforce AI Research
United States
sjoty@salesforce.comSteven C.H. Hoi
Salesforce AI Research
Singapore
shoi@salesforce.com
ABSTRACT
The goal of semantic code search or text-to-code search is to retrieve
a semantically relevant code snippet from an existing code data-
base using a natural language query. When constructing a practical
semantic code search system, existing approaches fail to provide an
optimal balance between retrieval speed and the relevance of the
retrieved results. We propose an efficient and effective text-to-code
search framework with cascaded fast and slow models, in which
a fast transformer encoder model is learned to optimize a scalable
index for fast retrieval followed by learning a slow classification-
based re-ranking model to improve the accuracy of the top K results
from the fast retrieval. To further reduce the high memory cost of
deploying two separate models in practice, we propose to jointly
train the fast and slow model based on a single transformer en-
coder with shared parameters. Empirically our cascaded method
is not only efficient and scalable, but also achieves state-of-the-art
results with an average mean reciprocal ranking (MRR) score of
0.7795 (across 6 programming languages) on the CodeSearchNet
benchmark as opposed to the prior state-of-the-art result of 0.744
MRR. Our codebase can be found at this link.
CCS CONCEPTS
â€¢Computing methodologies â†’Information extraction ;â€¢In-
formation systems â†’Learning to rank ;Language models .
KEYWORDS
Developer Productivity, Code retrieval, Text to Code Search, Trans-
former models, Cascaded retrieval schemes, top K retrieval
ACM Reference Format:
Akhilesh Deepak Gotmare, Junnan Li, Shafiq Joty, and Steven C.H. Hoi. 2023.
Efficient Text-to-Code Retrieval with Cascaded Fast and Slow Transformer
âˆ—Corresponding author: akhilesh.gotmare@salesforce.com.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
ESEC/FSE â€™23, December 3â€“9, 2023, San Francisco, CA, USA
Â©2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-0327-0/23/12. . . $15.00
https://doi.org/10.1145/3611643.3616369Models. In Proceedings of the 31st ACM Joint European Software Engineering
Conference and Symposium on the Foundations of Software Engineering (ES-
EC/FSE â€™23), December 3â€“9, 2023, San Francisco, CA, USA. ACM, New York,
NY, USA, 13 pages. https://doi.org/10.1145/3611643.3616369
1 INTRODUCTION
Building automatic tools that can enhance software developer pro-
ductivity has recently garnered a lot of attention in the deep learn-
ing and software engineering research communities. Code retrieval
systems can make developers more productive by enabling them to
search and reuse from the enormous volume of open-source reposi-
tories available online, thus speeding up the software development
lifecycle. Code search systems can be particularly of great value
for organizations with internal proprietary code. Indexing source
code data internally for search can prevent redundancy and boost
programmer productivity. A study by Xu et al . [60] surveys devel-
opers to understand the effectiveness of code generation and code
retrieval systems. Their results indicate that the two systems serve
complementary roles and developers prefer retrieval modules over
generation when working with complex functionalities, thus advo-
cating the need for better code search systems. Beyond their direct
utility in improving developer productivity, code search solutions
have also been leveraged to improve the performance of code gen-
eration systems, [ 35,41,46,64] when used as sub-components, thus
adding to the significance of research on improving text-to-code
retrieval.
Our primary focus in this work is to improve the performance of
text-to-code search solutions, as evaluated by these two aspects: the
speed of retrieval and the relevance of the retrieved results to the
input query. We propose to bring this improvement by cascading
two approaches with complementary strengths - fast retrieval sys-
tems that sacrifice relevance but offer high retrieval speed, and slow
retrieval systems that sacrifice speed but return results with higher
relevance. We find inspiration for this multi-stage approach from
recent progress in the text-to-image retrieval domain [ 12,28,29,37].
Researchers have shown impressive results on the traditional doc-
ument retrieval problem (text-to-text search) using transformer
based models [ 23,42,59]. This progress has in turn guided a lot
of research in the text-to-code retrieval domain [ 13,17]. Parallel
to the progress in natural language processing (NLP), language
models (LMs) pre-trained on code like CodeBERT [ 13], CodeGPT
[36], InstructGPT [ 45], Codex [ 8], PLBART [ 1] and CodeT5 [ 58]
388
ESEC/FSE â€™23, December 3â€“9, 2023, San Francisco, CA, USA Akhilesh Deepak Gotmare, Junnan Li, Shafiq Joty, and Steven C.H. Hoi
have now been proposed for understanding and generation tasks
involving programming languages. However, there has been very
limited research [ 57] on studying the similarities between the two
problem settings of text-to-image and text-to-code retrieval despite
their common theme of aligning data from two different modalities.
We believe further improvements in text-to-code search can be
achieved using the two stage paradigm that has been shown to be
effective for text-to-image search.
One could question the pertinence of text-to-code search given
the current state of research on code generation using transformer
based large language models (LLMs). Chen et al . [8]â€™s 12B parame-
ter Codex, Li et al . [30] â€™s 41B parameter AlphaCode, Nijkamp et al .
[43]â€™s 16B parameter CodeGen and Austin et al . [2]â€™s 137B parame-
ter LM use large scale autoregressive language models to demon-
strate impressive capabilities of generating multiple lines of code
from natural language descriptions, well beyond what previous gen-
eration models like GPT-C [ 51] could accomplish. Would developers
need text-to-code search when LLMs trained on code can generate
correct looking programs for a natural language prompt? We argue
that text-to-code retrieval would still be a valuable offering for de-
velopers for the following reasons: The impressive performance of
code generation systems is often predicated on being able to draw
many samples from the model [ 27] and machine-check them for
correctness. This setup will often not be the case in practice [ 10].
Code generation models also entail security implications (possibil-
ity of producing vulnerable or misaligned code) [ 8,25,47], making
their adoption tricky. Besides, some recent studies [ 33] have found
limitations with popular benchmarks like HumanEval that have
been relied on to measure the correctness of model generated pro-
grams, suggesting that the synthesized program correctness scores
of code LLMs have been overestimated.
Given this current landscape, code retrieval systems can serve
as attractive alternatives when building tools to assist developers.
With efficient implementations, code search for a single query can
typically be much faster for most practical index sizes than generat-
ing code with large scale LMs. As opposed to code generation, code
retrieval offers interpretability and the possibility of a much greater
control over the quality of the result as the index entries can be
verified beforehand. Another benefit with code search systems is
the ability to leverage additional data post training as this simply
requires extending the index by encoding new instances. Moreover,
a code generation system can be augmented with a code retrieval
system to improve the generation ability [46].
For semantic code search, deep learning based approaches [ 15,
17,50,61] involve encoding query and code independently into
dense vector representations in the same semantic space. Retrieval
is then performed using the representational similarity (based on co-
sine or euclidean distances) of these dense vectors. This framework
is often referred with different terms like representation/embed-
ding based retrieval [ 20], dense retrieval [ 23], two tower [ 14], or
fast/dual encoder [ 12,39] approach in different contexts. An orthog-
onal approach involves encoding the query and the code jointly
and training semantic code search systems as binary classifiers that
predict whether a code answers a given query [ 19,36] (referred to
as monoBERT style [ 44] or as slow classifier). With this approach,
the model processes the query paired with each candidate code se-
quence, meaning the text and code snippets are concatenated at the
NL QueryCandidate code snippets
Implement       bubble sort
â€¦def insertion_sort (items):for i in range(1, len (items)):      â€¦.def mergeSort (arr, l, r):    if l < r:m = l+(r-l)//2# Sort ï¬rst    â€¦
Transformer Encoder1100â€¦K nearest neighbor lookupPL Index (constructed ofï¬‚ine)22NL QueryTop-K code snippets
Implement       bubble sortdef mergeSort (arr, l, r):    if l < r:m = l+(r-l)//2# Sort ï¬rst    â€¦
â€¦â€¦0.010.90.053Transformer Classï¬erSearch result
Transformer EncoderFigure 1: CasCodeâ€™s Inference stage: The query ð‘¥ð‘–and the code
snippets are first encoded independently by the transformer
encoders. The top K candidates (based on nearest neighbor
lookup) are then passed to the classifier which jointly pro-
cesses the query with each of the filtered candidates to predict
the probability of their semantics matching.
input stage of the neural network. Intuitively, this approach helps
to sharpen the cross information between query and code and is a
better alternative for capturing matching relationships between the
two modalities (natural language (NL) and programming language
(PL)) than the simple similarity metric between the fastencoder
based sequence representations. While this latter approach can be
389Efficient Text-to-Code Retrieval with Cascaded Fast and Slow Transformer Models ESEC/FSE â€™23, December 3â€“9, 2023, San Francisco, CA, USA
NL QueryCandidate code snippets
Implement       bubble sort
â€¦def insertion_sort (items):for i in range(1, len (items)):      â€¦.def mergeSort (arr, l, r):    if l < r:m = l+(r-l)//2# Sort ï¬rst    â€¦
Transformer Encoderâ€¦0.010.010.010.90.050.050.030.02Transformer Classï¬erSearch result
NL QueryCandidate code snippets
Implement       bubble sort
â€¦def insertion_sort (items):for i in range(1, len (items)):      â€¦.def mergeSort (arr, l, r):    if l < r:m = l+(r-l)//2# Sort ï¬rst    â€¦
Transformer Encoderâ€¦K nearest neighbor lookupPL Index (constructed ofï¬‚ine)Transformer EncoderSearch result
Figure 2: Illustration of the the inference stage of fast encoder (left) and slow classifier (right) based semantic code search
approaches. With the encoder based approach, we independently compute representations of the natural language (NL) query
and candidate code sequences. The code snippet with representation nearest to the query vector is then returned as the search
result. With the classifier based approach, we jointly process the query with each code sequence to predict the probability of
the code matching the query description. The code sequence corresponding to the highest classifier confidence score is then
returned as the search result.
promising for code retrieval, previous works have mostly leveraged
it for tasks like text to code generation or binary classification in the
form of text-code matching [ 36]. Directly adapting this approach
to code search tasks would be impractical due to the large number
of candidates to be considered for each query. Inference with this
setup would require each candidate to be combined with the query
and passed through the classifier. We depict the complementary
nature of these approaches in Figure 2 when using a transformer
[54] encoder based model for retrieval and classification.
In order to leverage the potential of such nuanced classifier mod-
els for the task of retrieval, we propose a cascaded scheme (CasCode)
where we process only a limited number of candidates with the
classifier model. This limiting can be achieved by employing the
representation based ( fastencoder) approach and picking its top
few candidate choices for processing by the second classifier stage.
Our cascaded approach leads to state of the art performance on the
CodeSearchNet benchmark with an overall mean reciprocal ranking
(MRR) score of 0.7795, significantly improving over previous results
(best reported MRR score of 0.744from Guo et al . [16] ). We propose
a variant of the cascaded scheme with shared parameters, where a
single transformer model can serve in both the modes - encoding
in the representation based retrieval stage and classification in the
second stage. This shared variant can be achieved by multi-task
training [ 6,49] using the sum of the two objectives correspond-
ing to these two distinct task settings. CasCodeâ€™s shared variant
substantially reduces the memory requirements, while offering re-
trieval performance that is comparable to the separate variant withan MRR score of 0.7700. We also show improvements with our Cas-
Code approach for the AdvTest python dataset popularised by the
CodeXGLUE benchmark [ 36] to assess the generalization abilities
of code retrieval models when the function and variable names of
a program are normalised and thus unrelated to its semantics.
Figure 3 illustrates the trade off involved between inference
speed and retrieval performance (MRR) for different algorithmic
choices, where we have the ( fast) encoder model on one extreme,
and the ( slow) classifier model on the other. With CasCode, we
offer performance comparable to the optimal scores attained by the
classifier model, while requiring substantially lesser inference time,
thus making it computationally feasible.
Our key contributions in this paper are the following.
â€¢We first show that the performance of existing dense retrieval
models (CodeBERT and GraphCodeBERT) trained with contrastive
learning can be significantly improved when trained with larger
batch-size, these serve as stronger baselines for code retrieval.
â€¢To further push retrieval performance, we propose the cascaded
code search scheme (CasCode) that performs code retrieval in
two stages, and we analyze the trade-off of the inference speed
and retrieval performance.
â€¢We show that the transformer models in the two stages of Cas-
Code can be shared by training in a multi-task manner, which
significantly reduces the memory requirements.
â€¢With CasCode, we report state of the art text-to-code retrieval
performance on public benchmarks of CodeSearchNet and the
normalised AdvTest (Python) dataset from CodeXGLUE
390ESEC/FSE â€™23, December 3â€“9, 2023, San Francisco, CA, USA Akhilesh Deepak Gotmare, Junnan Li, Shafiq Joty, and Steven C.H. Hoi
Figure 3: Overview of the speed versus performance (MRR
metric 0-1: higher is better) trade-off of current code search
approaches. With CasCode, we are able to achieve perfor-
mance comparable to the optimal classifier based approach
(top right), while requiring substantially lesser inference
time. Areas of the circles here are proportional to model
sizes. For reference Fast Encoders require 125M parameters.
2 RELATED WORK
Our work is heavily inspired by recent progress in neural search
and ranking for natural language, where pre-trained transformer
language models have been extensively used. Karpukhin et al . [23]
finetune BERT [ 11] based encoders to build the passage retrieval
component of their open domain question answering (QA) system,
where the goal is to develop systems capable of answering questions
without any topic restriction. Efficient passage retrieval to select
candidate contexts is a critical step in such pipelines. Xiong et al .
[59] show improvements in transformer based dense retrieval of
text by using globally retrieved hard negatives when finetuning
the encoders, resulting in effective performance on web search and
QA. Chang et al . [7] propose novel pre-training objectives to train
transformer models that specialize at embedding-based large-scale
text retrieval.
Lin et al . [32] provide an exhaustive survey on the use of pre-
trained language models for text ranking and study the trade offs
involved in the different alternatives. In the single stage fashion, a
common approach is representation based ranking, where BERT-
based models (bi-encoders or fastencoders) are trained to indepen-
dently encode the query and documents, and inference involves
dot product based similarity search for retrieval [ 13,17]. Another
single stage approach is monoBERT [ 19,44] (slow classifier), where
query-document pairs are passed jointly to a BERT encoder and
the model predicts whether the input document is relevant to thequery or not. The monoBERT approach is computationally more
expensive, but also tends to be more accurate than the bi-encoder
approach. However, with the bi-encoder approach we can index
all the document representations offline. Thus at inference time,
we simply need to encode the query, making it a very attractive
retrieval setup. Achieving this inference speedup by caching rep-
resentations is not possible in the monoBERT setting, as it jointly
processes the query and document strings. As an alternative to
these two frameworks, Khattab and Zaharia [24] propose ColBERT
which performs late interaction between a query and document
after their independent encoding. This leads to performance that
is comparable to the monoBERT approach, but is less computa-
tionally expensive during inference. However, ColBERT requires
storing per token representations of all the document candidates
as inputs to the late interaction, and this can demand impractically
high storage. The limitations of monoBERT when handling a large
number of candidate documents inspire the need for multi-phase
retrieval, where the first phase can retrieve candidate documents
with the cost effective bi-encoder approach (dot-product retrieval),
followed by the second stage where only the top candidates from
the first stage are processed by a more expensive monoBERT model.
For code retrieval, we experimentally show that these two models
can share a majority of their parameters. Thus, a single encoder
backbone can serve in the two stages - first as the bi-encoder for
fast retrieval, and then as the more powerful monoBERT.
Early work on neural approaches to code search [ 50] leveraged
unsupervised word embeddings [ 38,48] to represent code snip-
pets as textual documents. Subsequently, supervised approaches
using LSTM architectures showed improvements [ 5,55] by also
leveraging data augmentation strategies to transform code snip-
pets while preserving their semantics [ 4]. Later with the advent of
the transformer architecture in natural language processing [ 54],
several works [ 13,17,22,56,58] employed transformer models for
code retrieval tasks and reported significant gains in performance
over previous approaches. Across a majority of these recent works,
CodeSearchNet by Husain et al . [21] has emerged as a standard
benchmark for calibrating code search performance. Researchers
have attempted to modify the pre-training of transformer models
for the code domain by embedding the structural information as-
sociated with programs in different forms. This has led to a string
of code pre-trained models like CodeBERT [ 13] which introduced
novel pre-training tasks for bimodal datasets containing text and
code, GraphCodeBERT [ 17] which is pre-trained on code using
tasks that embed structural information from the abstract syntax
trees (ASTs) of code inputs and SynCoBERT [ 56], a syntax aware
encoder architecture. In a related line of work, Lu et al . [36] propose
a benchmark (NL-code-search-WebQuery) where natural language
code search is framed as the problem of analysing a query-code
pair to predict whether the code answers the query or not. More
recently, Guo et al . [16] released UniXCoder. Unike CodeBERTâ€™s
encoder-only pre-training (that uses the masked language modeling
(MLM) and replaced token detection (RTD) objectives only), UniX-
Coder is pre-trained with a set of tasks like MLM, unidirectional
language modeling, span denoising, cross-modal contrastive learn-
ing and cross-modal generation and has shown to be a competitive
alternative for several code understanding and generation tasks.
391Efficient Text-to-Code Retrieval with Cascaded Fast and Slow Transformer Models ESEC/FSE â€™23, December 3â€“9, 2023, San Francisco, CA, USA
In contrast to the research theme of finding optimal pre-training
strategies for code, we focus on the adaptation or fine-tuning stages
of pre-trained models. Similar to the pre-training stage, this fine-
tuning stage also offers different training choices, which have been
underexplored so far. One could adapt a pre-trained model in the
fast encoder style or through the slow classifier style for retrieval.
CasCode proposes to combine these two approaches to achieve
optimal retrieval performance (speed and relevance) with any given
pre-trained code understanding model.
3 CASCADING TRANSFORMER MODELS FOR
TEXT-TO-CODE RETRIEVAL (CASCODE)
In this section, we describe our proposed CasCode approach in-
cluding details of training and inference phases of the fast encoder
stage (3.1), the slow classifier stage (3.2), our cascading scheme (3.3),
and the shared variant of CasCode (3.4).
3.1 Stage I: Fast Encoders
For the first stage of fast (bi-) encoders, we use the contrastive
learning framework [ 9], similar to the fine-tuning by Guo et al . [17] ,
who leverage pairs of natural language and source code sequences
to train text-to-code retrieval models. The representations of natural
language (NL) and programming language (PL) sequences that
match in semantics (a positive pair from the bimodal dataset) are
pulled together, while representations of negative pairs (randomly
paired NL and PL sequences) are pushed apart. The infoNCE loss (a
form of contrastive loss function [ 18]) used for this approach can
be defined as follows:
LinfoNCE =1
ð‘ð‘âˆ‘ï¸
ð‘–=1âˆ’logexp(ð‘“ðœƒ(ð‘¥ð‘–)ð‘‡ð‘“ðœƒ(ð‘¦ð‘–)/ðœŽ)Ã
ð‘—âˆˆBexp(ð‘“ðœƒ(ð‘¥ð‘–)ð‘‡ð‘“ðœƒ(ð‘¦ð‘—)/ðœŽ)(1)
whereð‘“ðœƒ(ð‘¥ð‘–)is the dense representation for the NL input ð‘¥ð‘–, and
ð‘¦ð‘–is the corresponding semantically equivalent PL sequence. ð‘
is the number of training examples in the bimodal dataset, ðœŽis
a temperature hyper-parameter to control the sharpness of the
modelâ€™s output probability distribution, and Bdenotes the current
training minibatch.
While the above approach applies for any model architecture,
Guo et al . [17] employ GraphCodeBERT and CodeBERT for ð‘“ðœƒin
their experiments. We refer to this approach as fast as it bene-
fits from caching of candidate encodings before query time. Dur-
ing inference, we are given a set of candidate code snippets C=
{ð‘¦1,ð‘¦2,...ð‘¦|C|}, which are encoded offline into an index {ð‘“ðœƒ(ð‘¦ð‘—)âˆ€ð‘—âˆˆ
C}. For a test NL query ð‘¥ð‘–, we then compute ð‘“ðœƒ(ð‘¥ð‘–)and return the
code snippet fromCcorresponding to the nearest neighbor (as per
the cosine similarity distance metric) in the index. During inference,
we are only required to perform the forward pass associated with
ð‘“ðœƒ(ð‘¥ð‘–)and the nearest neighbor lookup in the PL index, as the PL
index itself can be constructed offline. This makes the approach
very suitable for practical scenarios where the number of candidate
code snippets|C|could be very large.
Interestingly, a single encoder - either CodeBERT and Graph-
CodeBERT can be used to process the two modalities of text ( ð‘“ðœƒ(ð‘¥ð‘–))
and code (ð‘“ðœƒ(ð‘¦ð‘–)). This could be attributed to the NL-PL pre-training
of these models. Given this observation with the two code pre-
trained models, in all our experiments we process the NL and PL
inputs in the same manner, agnostic to their modality.3.2 Stage II: Slow Classifiers
Although the above retrieval approach is efficient for practical
scenarios, the independent encodings of the query and the code
make it less effective as these do not allow for self-attention style
interactions between NL and PL tokens. Similar to the monoBERT
approach, we could instead encode the query and the code candidate
jointly within a single transformer encoder and perform binary
classification for ranking. In particular, the model could take as
input the concatenation of NL and PL sequences [ð‘¥ð‘–;ð‘¦ð‘—]and predict
whether the two match in semantics.
The training batches for this binary classification setup can again
be constructed using the bimodal dataset (positive pairs denoting
semantic matches), and the negative pairs (mismatch) can be con-
structed artificially. Given a set of ð‘paired NL-PL semantically
equivalent sequences {ð‘¥ð‘–,ð‘¦ð‘–}ð‘
ð‘–=1, the cross-entropy objective func-
tion for this training scheme would be:
LCE=âˆ’1
ð‘ð‘âˆ‘ï¸
ð‘–=1,ð‘—â‰ ð‘–logð‘ðœƒ(ð‘¥ð‘–,ð‘¦ð‘–)+log(1âˆ’ð‘ðœƒ(ð‘¥ð‘–,ð‘¦ð‘—)) (2)
whereð‘ðœƒ(ð‘¥ð‘–,ð‘¦ð‘—)represents the probability that the NL sequence
ð‘¥ð‘–semantically matches the PL sequence ð‘¦ð‘—, as predicted by the
classifier. With a minibatch Bof positive pairs{ð‘¥ð‘–,ð‘¦ð‘–}âˆ€ð‘–âˆˆ B,
we can randomly pick ð‘¦ð‘—(ð‘—âˆˆB;ð‘—â‰ ð‘–) from the PL sequences
in the minibatch and pair it with ð‘¥ð‘–to serve as a negative pair.
When using a transformer encoder based classifier, the interactions
between the NL and PL tokens in the self-attention layers can
help in improving the precision of this approach over the previous
(independent encoding) one.
During inference, we can pair the NL sequence ð‘¥ð‘–with each
of theð‘¦ð‘—fromCand rank the candidates as per the classifierâ€™s
confidence scores of the pair being a match. This involves Cforward
passes (each on a joint NL-PL sequence, thus longer inputs than
the previous approach), making this approach computationally
infeasible when dealing with large retrieval sets. We refer to this
approach as the one using slow classifier for retrieval.
3.3 Cascading of the Two Stages
With a cascaded scheme (that we call CasCode), we can unify the
strengths of the two approaches - the speed of the fast encoders
with the precision of the slow classifiers . To build CasCode, we
first independently train the two stages discussed above - the fast
encoder stage using the infoNCE objective and the slow classifier
stage using the cross entropy loss. While these two approaches
are alternatives to each other, we employ them in a sequential
manner to perform retrieval. Figure 1 shows the overall framework
of our approach. This hybrid strategy combines the strengths of
the two approaches in the following manner - Given a query at
test time (inference stage), the first stage of fast encoders provides a
similarity score (based on the cosine distance between the query
and candidate encodings) for each candidate from the set Cof code
snippets. In practice, the size of the retrieval set ( |C|) can often be
very large, and varies from 4360 to52660 for the CodeSearchNet
datasets we study in our experiments. The top ð¾candidates based
on the similarity scores from the first stage are then passed to the
second stage of slow classifiers where each of them is paired with
the NL input (query) ð‘¥ð‘–and fed to the model. For a given pair, this
392ESEC/FSE â€™23, December 3â€“9, 2023, San Francisco, CA, USA Akhilesh Deepak Gotmare, Junnan Li, Shafiq Joty, and Steven C.H. Hoi
second stage classifier will return the probability of the NL and
PL components of the input matching in semantics. Using these as
confidence scores, the rankings of the ð¾candidates are refined.
The resulting scheme is preferable for ð¾<<|C|, as this would
add a minor computational overhead on top of what is required by
thefast encoder based retrieval. The second stage of refinement can
then improve retrieval performance provided that the value of ð¾
is set such that the recall of the fast encoder is reasonably high. ð¾
would be a critical hyper-parameter in this scheme, as setting a very
lowð¾would lead to high likelihood of missing the correct snippet
in the set of inputs passed to the second stage slow classifier , while a
very highð¾would make the scheme infeasible for retrieval. As we
discuss later in Section 4, CasCode with a ð¾as small as 10already
offers significant gains in retrieval performance over the baselines,
with marginal gains as we increment ð¾to100and beyond.
3.4 Making CasCode Memory Efficient
In order to minimize the memory overhead incurred by the two
stage model, we propose to share the weights of the transformer
layers of the fast encoders and the slow classifiers , by training a model
with the joint (sum) objective Lshared =LinfoNCE+L CE. Thus,
a single transformer model is trained to perform both encoding
based retrieval and classification based retrieval in this multi-task
learning setting. While the number of parameters in this shared
variant would be nearly half of the separate (non-shared) case, the
computational cost at inference would be the same. Note that we
would need some exclusive parameters for the classifier model,
specifically the classification head (a linear layer) on top of the
encoder hidden states output. Thus, in this shared parameter variant
of CasCode, the transformer model consuming the three kinds of
inputs - NL only and PL only (for the fast encoder stage) and NL-PL
(for the slow classifier stage) is identical except for the classification
head in the second stage.
4 EXPERIMENTS
4.1 Setup and Fast Retrieval Baseline
We use the CodeSearchNet corpus from Husain et al . [21] that in-
cludes six programming languages - Ruby, Javascript, Go, Python,
Java and Php. Our pre-processing and train-val-test splits are identi-
cal to the setting from Guo et al . [17] , who filter low-quality queries
and expand the retrieval set to make the code search task more chal-
lenging and realistic. Figure 2 shows 2 examples of bimodal pairs
from the resulting dataset and the statistics of the dataset after pre-
processing are provided in Table 1. Our primary evaluation metric
is Mean Reciprocal Ranking (MRR), computed as1
ð‘ð‘¡ð‘’ð‘ ð‘¡Ãð‘ð‘¡ð‘’ð‘ ð‘¡
ð‘–=11
ð‘Ÿð‘–,
where theð‘Ÿð‘–is the rank assigned to the correct code snippet (for
theð‘–-th queryð‘¥ð‘–) from the set of retrieval candidates C. We report
MRR on the scale of 0-1, some works (eg. [ 56]) use the 0-100 scale.
Our fast encoder baseline is based on the CodeBERT model
from Feng et al . [13] that is pre-trained on programming languages.
In order to have a strong baseline, we use a newer CodeBERT
checkpoint that we pre-train (using masked language modeling
and replaced token detection tasks) for longer, after we found that
the CodeBERT checkpoint from Feng et al . [13] was not trained
till convergence. When starting from our new checkpoint, we find
that the CodeBERT baseline, if fine-tuned with a larger batch-size(largest possible that we can fit on 8 A100 GPUs) and for a larger
number of epochs, is able to perform substantially better than the
results reported before. We report the baselines from Guo et al .
[17] in Table 3 along with the results for our replication of two
of these baselines. Previous studies have emphasized this effect -
larger batch sizes are known to typically work well when training
with the infoNCE loss in a contrastive learning framework, due to
more negative samples from the batch [9].
We also finetune GraphCodeBERT [ 17] as a structure aware
model pre-trained on programming languages. GraphCodeBERT
leverages data flow graphs during pre-training to incorporate struc-
tural information into its representations. However, for the code
search task, we report (Table 3) that GraphCodeBERT does not of-
fer any significant improvements in performance over CodeBERT,
when both variants are trained with a large batch size. As CodeBERT
performs competitively and has a relatively simpler architecture
(equivalent to RoBERTa-base[ 34] model with 12 layers, 768 dimen-
sional hidden states and 12 attention heads), we chose it as the fast
encoder baseline for the remainder of our experiments.
For finetuning on code search, we begin with the baseline im-
plementation of GraphCodeBERT (https://github.com/microsoft/
CodeBERT/tree/master/GraphCodeBERT) and adapt their setup to
also implement the CodeBERT model. For the cascaded schemes,
many of our training design decisions are therefore the same as
GraphCodeBERT. We use 8 A100 GPUs (each with 40GB RAM) to
train our baselines and CasCode variants. During training, we set
the batch-size to a value that occupies as much available GPU RAM
as possible, which is 576 for the CodeBERT and GraphCodeBERT
baseline finetuning with the infoNCE loss.
MRR scores on the test set for the CodeBERT baseline ( fasten-
coder) along with several other baselines including sparse meth-
ods like BM25 (implemented using Pyserini [ 31]), fine-tuned CNN,
BiRNN, multi-head attention models are shown in Table 3. Interest-
ingly, BM25 outperforms all other methods on the Python dataset,
this could be attributed to the simplicity of Python and its similarity
with natural language [ 53]. For the CodeBERT baseline and the
CasCode variants that we have proposed, along with MRR, we also
report Recall@K for ð¾={1,2,5,8,10}, that indicates the hit rate
(ratio of instances where we find the correct output in the top ð¾
results). We encourage future work on code search to report these
additional metrics, as these are important in evaluating the utility
of a retrieval system and are commonly reported in similar work
in text retrieval and text based image or video retrieval [3, 37]. As
alluded to in Section 3, for designing the cascaded scheme, we need
to pick að¾that is large enough to provide reasonably high recall,
and small enough for the second stage to be reasonably fast. To
guide our choice of ð¾, we show in Figure 4 the Recall@K (K varied
over the horizontal axis) for the 6 different programming languages,
with the fast encoder models, over the validation set. For our exper-
iments, we pick ð¾=10and100where the recall for all 6 datasets
is over 85%and90%, respectively. Note that CasCode is a general
framework and several different models can be employed in the two
stages. We pick fine-tuned CodeBERT for the fast encoder phase
of CasCode, as it is a simpler architecture than GraphCodeBERT
or UniXCoder [ 16] and gives strong performance on its own when
evaluated in the first stage only.
393Efficient Text-to-Code Retrieval with Cascaded Fast and Slow Transformer Models ESEC/FSE â€™23, December 3â€“9, 2023, San Francisco, CA, USA
Table 1: Data statistics of the filtered CodeSearchNet corpus for Go, Java, Javascript, PHP, Python and Ruby programming
languages. For each query in the dev and test sets, the answer is retrieved from the set of candidate codes (last row).
- Ruby Javascript Go Python Java PHP
Training examples 24,927 58,025 167,288 251,820 164,923 241,241
Dev queries 1,400 3,885 7,325 13,914 5,183 12,982
Testing queries 1,261 3,291 8,122 14,918 10,955 14,014
Candidate codes 4,360 13,981 28,120 43,827 40,347 52,660
Table 2: Examples of bimodal pairs (natural language/doc-
string with corresponding code sequence) from CodeSearch-
Net (Python).
Docstring: Prompt the user to continue or not
Code Snippet:
def continue_prompt ( message = " " ) :
answer = F a l s e
message = message + " " " \ n " Y e s " o r " No " t o c o n t i n u e : " " "
while answer not in ( " Yes " , " No " ) :
answer = prompt ( message , e v e n t l o o p = e v e n t l o o p ( ) )
i fanswer == " Yes " :
break
i fanswer == " No " :
break
return answer
Docstring: Sends a message to the framework scheduler.
Code Snippet:
def message ( s e l f , d a t a ) :
l o g g i n g . i n f o ( " " " D r i v e r s e n d s framework
mes sag e { } " " " .format ( d a t a ) )
return s e l f . d r i v e r . sendFrameworkMessage ( d a t a )
4.2 Results with CasCode
To build the model for the second phase of CasCode (separate)
on top of the CodeBERT based ( fast) encoders, we train the slow
classifiers independently but evaluate them by cascading with the
first phase. For this second stage model, we finetune the CodeBERT
pre-trained checkpoint (detailed above) with a classification head
on top (a linear layer on top of the hidden-states output) using
the CodeSearchNet dataset. On the validation set, we study the
performance of this finetuned classifier for retrieval and report the
MRR scores in Figure 5 for different values of ð¾, whereð¾is the
number of top candidates passed from the first ( fast encoder ) stage
to the second. Interestingly, the retrieval performance of this joint
classifier does not improve significantly beyond certain values of ð¾.
For example, increasing ð¾from 10to100only marginally improves
the MRR for Ruby, Javascript and Java, while for other languages
there is no significant improvement beyond ð¾=10. In CasCodeâ€™s
separate variant, we pair the fast encoder with this second stage
classifier model and the MRR scores for this approach and the rele-
vant baslines are provided in Table 3. With our cascaded approach,
we observe significant improvements over the fast encoder base-
lines, the overall MRR averaged over the six programming languges
for CasCode (separate) is 0.7795, whereas the fast encoder baseline
(CodeBERT) reaches 0.7422. The improvements with CasCode are
noticeably greater over the baseline for Ruby, Javascript, Python
Figure 4: Recall at different values of K over the validation
set of CodeSearchNet [ 21] when using a finetuned CodeBERT
encoder ( fast) for text-code retrieval.
Figure 5: Mean reciprocal ranking (MRR) at different values
of K over the validation set of CodeSearchNet [ 21] when us-
ing a finetuned CodeBERT ( slow) binary classifier (match or
not) for text-code retrieval. Note that with an increase in the
number of top candidates passed to the second stage, the in-
ference time would also increase, however we do not observe
substantial gains in MRR beyond top-K of 10.
and Java. We report modest improvements on the Go dataset, where
thefast encoder baseline is already quite strong ( 0.9145 MRR).
We also train fastand slow models with shared parameters ,
denoted by CasCode (shared). The training objective for this model
is the sum of the binary cross-entropy loss LCEand the infoNCE
lossLinfoNCE as described in Section 3. The shared variant of Cas-
Code attains an overall MRR score of 0.77, which is comparable
394ESEC/FSE â€™23, December 3â€“9, 2023, San Francisco, CA, USA Akhilesh Deepak Gotmare, Junnan Li, Shafiq Joty, and Steven C.H. Hoi
Table 3: Mean Reciprocal Ranking (MRR) scores of different methods on the codesearch task on 6 Programming Languages from
the CodeSearchNet corpus (test set). The first row indicates performance with the BM25 scoring using bag-of-words (sparse)
representations. The next set consists of four finetuning-based baseline methods (NBow: Bag of words, CNN: convolutional
neural network, BiRNN: bidirectional recurrent neural network, and multi-head attention), followed by the second set of
models that are pre-trained then finetuned for code search (RoBERTa: pre-trained on text by Liu et al . [34] , RoBERTa (code):
RoBERTa pre-trained only on code, CodeBERT: pre-trained on code-text pairs by Feng et al . [13] , GraphCodeBERT: pre-trained
using structure-aware tasks by Guo et al . [17] ), CodeT5-base: Encoder-decoder transformer model by Wang et al . [58] pre-
trained for code understanding and generation tasks, UniXCoder: unified cross-modal pre-trained model for code by Guo
et al. [16] . SynCoBERT: pre-trained using syntax-aware tasks by Wang et al . [56] . CasCode (separate): Our cascaded retrieval
scheme using two independent transformer encoder models, first in the fast/dual encoder stage and later in the slow classifier
(monoBERT-style) stage. CasCode (ours) shared: a single encoder model is used in both the stages of CasCode using model
parameter sharing. In the last four rows, we report the results with the shared and separate variants of our CasCode scheme.
Model/Method Ruby Javascript Go Python Java Php Overall
BM25 0.3859 0.3259 0.4978 0.9454 0.3272 0.3725 0.4758
As reported by Guo et al. [17]
NBow 0.162 0.157 0.330 0.161 0.171 0.152 0.189
CNN 0.276 0.224 0.680 0.242 0.263 0.260 0.324
BiRNN 0.213 0.193 0.688 0.290 0.304 0.338 0.338
selfAtt 0.275 0.287 0.723 0.398 0.404 0.426 0.419
RoBERTa 0.587 0.517 0.850 0.587 0.599 0.560 0.617
RoBERTa (code) 0.628 0.562 0.859 0.610 0.620 0.579 0.643
CodeBERT 0.679 0.620 0.882 0.672 0.676 0.618 0.693
GraphCodeBERT 0.703 0.644 0.897 0.692 0.691 0.649 0.713
CodeT5-base [58] - - - - - - 0.715
UniXCoder [16] - - - - - - 0.744
As reported by Wang et al. [56]
SynCoBERT 0.722 0.677 0.913 0.724 0.723 0.678 0.740
Replicated with a larger training batch-size
CodeBERT 0.7245 0.6794 0.9145 0.7305 0.7317 0.681 0.7436
GraphCodeBERT 0.7253 0.6722 0.9157 0.7288 0.7275 0.6835 0.7422
Ours (ð¾=10)
CasCode (shared) 0.7621 0.6948 0.9193 0.7529 0.7528 0.7001 0.7637
CasCode (separate) 0.7724 0.7087 0.9258 0.7645 0.7623 0.7028 0.7727
Ours (ð¾=100)
CasCode (shared) 0.7686 0.6989 0.9232 0.7618 0.7602 0.7074 0.7700
CasCode (separate) 0.7825 0.716 0.9272 0.7704 0.7723 0.7083 0.7795
to the separate variant. This slight difference can be attributed
to the limited model capacity in the shared case, as the same set
of transformer layers serve in the encoder and classifier models.
We also evaluate the MRR scores for the CasCode (shared) model
when used in the fast encoder stage only, and the test set MRR
scores were 0.7308,0.6634,0.9048,0.7193,0.7244,0.6803 for Ruby,
Javascript, Go, Python, Java and PHP respectively, with the overall
MRR being 0.7372. Thus the cascaded model that was trained in a
multi-task manner with a joint objective, gives competitive retrieval
performance, even when used only in its first stage.
The improvements in the MRR scores of both CasCode variants
- shared and separate over the CodeBERT fast encoder baseline
are statistically significant for all 6programming languages with
ð‘<0.0001 as per the one-tailed studentâ€™s t-test (recommended forretrieval by Urbano et al . [52] ) for bothð¾=10andð¾=100. We
also report the Recall@K metric for CasCode separate and shared
variants in Figure 6. For all six programming languages, we observe
improvements over the fast encoder baseline with our cascaded
scheme. Similar to our observation from Table 3, the shared variant
of CasCode is slightly worse than the separate one.
CasCode Training Details : For training the joint NL-PL clas-
sifier of CasCode (separate), we are able to use a batch size of 216.
This batch-size is lower than the fast encoder finetuning batch-
size because we are required to process joint NL-PL sequences
(ð‘“ðœƒ([ð‘¥ð‘–;ð‘¦ð‘–])which will be much longer in length than a NL only
or PL only sequence. For CasCodeâ€™s shared variant, we need to
further reduce the training batch size to 160, as we are required to
store activations from multiple forward passes for a given bimodal
395Efficient Text-to-Code Retrieval with Cascaded Fast and Slow Transformer Models ESEC/FSE â€™23, December 3â€“9, 2023, San Francisco, CA, USA
pair - NL only ð‘“ðœƒ(ð‘¥ð‘–), PL onlyð‘“ðœƒ(ð‘¦ð‘–)and joint NL-PL ð‘“ðœƒ([ð‘¥ð‘–;ð‘¦ð‘–]).
All models are trained for 100 epochs. For all our experiments we
use a learning rate of 2e-5, and use the Adam optimizer [ 26] to
update model parameters. For both the CasCode variants, when
performing evaluation on the development set (for early stopping),
we useð¾=100candidates from the fast encoder stage.
When running inference on a single A100 GPU on a single query
(batchsize of 1), the CodeBERT style fast encoder occupies 1482
MB of GPU RAM. This is also true for the slow binary classifiers
(monoBERT style) and the shared CasCode variants. The inference
stage memory requirement gets roughly doubled with CasCodeâ€™s
separate variant where there is no sharing of weights between the
fast encoder and slow classifier stages. This is expected because
the separate variant stores two different encoder models (identical
architecture except the classification head), but different weights)
on the GPU RAM.
4.3 Retrieval Speed Comparison
Having established the improvements in retrieval performance with
CasCode, we proceed to analyze the trade-off between inference
speed and performance, for the different methods discussed. For
each variant, we record the time duration (averaged over 100in-
stances) required to process (obtain a relevant code snippet from
the retrieval set) an NL query from the held-out set. We use the
Ruby dataset of CodeSearchNet for this analysis, which contains
4360 candidate code snippets for each NL query. We conduct this
study on a single Nvidia A100 GPU. Table 4 shows the results.
For the fast encoder approach (using infoNCE-finetuned Code-
BERT), we first incur some computational cost to encode all the
candidate code snippets and construct the PL index ( 6.76seconds
for Rubyâ€™s retrieval set). This computation is common to all ap-
proaches, except the slow (binary, joint) classifier one. Since this
computation can be performed offline before the model is deployed
to serve user queries, we do not include this cost in our results
in Table 4. With the PL index constructed beforehand, we report
the time required to encode a user NL query, and perform nearest
neighbor lookup on the PL index with the encoding, in the first
row of Table 4. This computation is again performed by all the
CasCode variants, and thus acts as the lower bound on time taken
by CasCode for retrieval. For the analysis to be as close to real
world scenarios as possible, we do not batch the queries (which can
provide further speed-ups, specially on GPUs) and encode them
one by one. Batching them would require assuming that we have
the NL queries beforehand, while in practice we would be receiving
them on the fly from users when deployed.
With the slow classifier approach, we would pair a given query
with each of the 4360 candidates, and thus this would lead to the
slowest inference of all the variants. For all variants of CasCode,
the inference duration listed in Table 4 includes the time taken by
thefast encoder based retrieval (first stage) along with the second
stage. For CasCodeâ€™s second stage, we can pass the ð¾combinations
(query concatenated with each of the top- ð¾candidate from the fast
stage) in a batched manner. The shared variant, while requiring
half the parameters, incurs the same computational cost when used
in the cascaded fashion. We note from Table 4 that at a minor drop
in the MRR score, lowering CasCodeâ€™s ð¾from 100to10can lead to
almost 3x faster inference.4.4 AdvTest (Normalized) Set Evaluation
Previous works [ 13,17] have employed a normalised variant of the
CodeSearchNet Python dataset called AdvTest to evaluate text-to-
code retrieval models. The function and variable names appearing
in the code snippets in the test and development sets of this python
dataset are normalized ( Func for function names, arg-i for the i-
th variable name). An example of this normalization is shown in
Table 7. This dataset was processed and released by [ 36] to test
the understanding abilities of code search systems as part of the
CodeXGLUE benchmark. We follow their lead in evaluating our
proposed CasCode on the AdvTest benchmark to study its retrieval
effectiveness in this challenging setting. When experimenting with
the AdvTest dataset, our focus is to is to compare the code un-
derstanding and retrieval abilities of different approaches in this
more rigorous evaluation setting than the regular CodeSearchNet
dataset, as the normalization scheme should prevent the models
from over-relying on the natural language semantics (English com-
ponents) of the candidate programs. For instance, the snippet "def
bubble_sort(): <python program here>" in the regular set-
ting would be easier to retrieve than the candidate "def Func():
<python program here>" in the normalized setting for the query
"Implement bubble sort" . In the normalized setting, the model
would have to rely on understanding the program semantics in-
stead of variable or function names, which tend to be closer to
natural language or plain English. Note that the AdvTest dataset
is not adversarially constructed [ 40,63] and it does not involve
any gradient based methods (adversarial attacks) to perturb inputs
beyond a simple normalisation function. We speculate that our
models would inherit the same vulnerabilities to adversarial input
perturbations as their NLP counterparts. Evaluating the robustness
of code search models to such sophisticated adversarial attacks is
beyond the current scope of our work.
The AdvTest dataset contains 251,820 training examples, 9,604
validation set examples and 19,210 test set examples. Each example
is a bimodal pair of natural language docstrings and corresponding
code snippets. During test time, all the 19,210 code snippets are
treated as candidates for a given test query. The code retrieval
results achieved by different approaches on this dataset are shown
in Table 5. Results in the first two rows are reported from [ 36] where
RoBERTa and CodeBERT are fine-tuned (batch size of 32) with the
infoNCE loss discussed before in the fast encoder framework. In
our re-implementation of the stronger baseline of CodeBERT, we
increase the training batch-size to 512. This leads to an improved
test MRR score of 0.3381. For CasCodeâ€™s separate variant, we fine-
tune the slow classifier stage with the binary cross entropy loss. This
second stage model is initialized from the CodeBERT pre-trained
checkpoint and trained on the 251,820 pairs. For each positive pair,
we can create a synthetic negative one by pairing a docstring with
a random code snippet. For CasCodeâ€™s shared variant, we train the
two stages jointly by tying the weights of the two encoder similar
to previous experiments from Section 4.2. The finetuning loss is
the sum of the infoNCE loss and binary CE loss computed using
the same minibatch.
From Table 5, we see that when using CasCode with ð¾=10
candidates, we observe a substantial improvement, the shared vari-
ant scores MRR of 0.4005 , and the separate one 0.3972 . Retrieval
396ESEC/FSE â€™23, December 3â€“9, 2023, San Francisco, CA, USA Akhilesh Deepak Gotmare, Junnan Li, Shafiq Joty, and Steven C.H. Hoi
Figure 6: Recall @ K = {1,2,5,8,10}with the fast encoder and CasCode (shared and separate) methods on the test set queries of
CodeSearchNet dataset.
Table 4: Inference speed comparison for different variants of the proposed methods. The number of parameters corresponding
to the classifier head are separated with a â€˜+â€™sign in the second column. Inference duration is averaged for 100queries from
the Ruby subset of CodeSearchNet, using a single A100 GPU. Constructing the PL index offline requires 6.76seconds for the
Ruby dataset and is not included in the durations listed here. MRR scores are reported on the entire test set. Throughput of the
retrieval model (measured in #queries processed per second) is listed in the last column.
Model #params Inference time (secs) MRR #queries/s
Fast encoders (CodeBERT style) 125M 0.0427 0.7245 23.42
Slow binary classifiers (monoBERT style) 125M + 0.5M 9.1486 0.7816 0.11
CasCode (separate, K=100) 250M + 0.5M 0.2883 0.7825 3.46
CasCode (shared, K=100) 125M + 0.5M 0.2956 0.7686 3.38
CasCode (separate, K=10) 250M + 0.5M 0.1022 0.7724 9.78
CasCode (shared, K=10) 125M + 0.5M 0.1307 0.7621 7.65
performance can be further improved to MRR score of 0.4299 with
the shared variant and 0.4398 with the separate, if we increase the
number of candidates ð¾to100. In Figure 8, we show an example
from this test set, where, for a given query, the first stage of fast
encoder (equivalent to the re-implemented CodeBERT baseline)
assigns a rank ð‘Ÿð‘–of 3 to the matching code snippet, and then the
slow classifier refines the ranking to 1. In cases when CasCode fails
at retrieving the correct code snippet as the top search result, our
qualitative analysis suggests that the resulting code snippet is often
closely related in semantics and functionality. The gap in perfor-
mance for deep learning models between the original unaltered
CodeSearchNet test set and the AdvTest one is nonetheless still an
open problem that suggests our current models over-rely on the
function and variable naming (as done by human programmers)
and less on the inherent structure of the code in representing source
code. Table 8 lists that UniXCoder [ 16] and CodeT5-base [ 58], when
used in a single stage (fast encoders), perform competitively on
the AdvTest benchmark. UniXCoderâ€™s performance with an MRRof0.413is significantly better than CodeBERTâ€™s MRR of 0.3381 ,
but worse than CasCodeâ€™s MRR of 0.4398. We expect additional im-
provements to CasCodeâ€™s performance on AdvTest by fine-tuning
a model like UniXCoder in its two stages. Subsequent to our work,
several submissions to the CodexGLUE AdvTest leaderboard seem
to have made this improvement. To the best of our knowledge, the
details of these approaches have not yet been released, preventing
any further analysis or comparison.
5 CONCLUSION AND FUTURE WORK
We propose CasCode, a cascaded text-to-code retrieval scheme con-
sisting of transformer encoder and joint binary classifier stages,
which achieves state of the art performance on the CodeSearchNet
benchmark with significant improvements over previous results.
We also propose a shared parameter variant of CasCode, where a
single transformer encoder can operate in the two different stages
397Efficient Text-to-Code Retrieval with Cascaded Fast and Slow Transformer Models ESEC/FSE â€™23, December 3â€“9, 2023, San Francisco, CA, USA
Code Snippet:
def d a y _ s t a r t _ u t ( s e l f , ut ) :
# s e t t i m e z o n e t o t h e one o f g t f s
o l d _ t z = s e l f . s e t _ c u r r e n t _ p r o c e s s _ t i m e _ z o n e ( )
ut = time . mktime ( time . l o c a l t i m e ( ut ) [ : 3 ]
+ ( 1 2 , 0 0 , 0 , 0 , 0 , âˆ’1 ) ) âˆ’43200
s e t _ p r o c e s s _ t i m e z o n e ( o l d _ t z )
return ut
Normalized code snippet:
def Func ( arg_0 , arg_1 ) :
arg_2 = arg_0 . s e t _ c u r r e n t _ p r o c e s s _ t i m e _ z o n e ( )
arg_1 = time . mktime ( time . l o c a l t i m e ( arg_1 ) [ : 3 ]
+ ( 1 2 , 0 0 , 0 , 0 , 0 , âˆ’1 ) ) âˆ’43200
s e t _ p r o c e s s _ t i m e z o n e ( arg_2 )
return arg_1
Figure 7: An example of the normalization performed for
constructing the AdvTest dataset by Lu et al. [36].
Table 5: Results on the AdvTest set [36] of CodeSearchNet.
Model/Method Test MRR
RoBERTa 0.1833
CodeBERT (original implementation) 0.2719
CodeBERT (our re-implemention w/ a larger bsz) 0.3381
CodeT5-base [58] 0.393
UniXCoder [16] 0.413
CasCode (shared, K=10) 0.4005
CasCode (separate, K=10) 0.3972
CasCode (shared, K=100) 0.4299
CasCode (separate, K=100) 0.4398
when trained in a multi-task fashion. With almost half of the pa-
rameter size and memory cost, CasCodeâ€™s shared variant offers
comparable performance to the non-shared (separate) variant.
Despite showing promising results, there are still some areas
for improving our method. One limitation of our current cascaded
scheme is that the computation spent in generating representations
in the first stage of fast encoders is not fully leveraged in the second
stage. Currently, we process raw token level inputs in the second
stage, but ideally the representations from the first stage should
be useful for the classification stage too [ 29]. Our initial attempts
along this direction did not turn fruitful, and future work could
address this aspect. To improve the inference speed of the two-stage
retrieval, future work could explore methods like quantization and
model distillation of the transformer models (e.g., employing the
ONNX runtime [ 62]). Another limitation warranting further in-
vestigation is associated with the training of the shared variant of
CasCode. Here, training with the multitask learning framework
(joint objective of infoNCE and binary cross entropy) leads to a
model that performs slightly worse than the separate variant (indi-
vidually finetuned models). We tried augmenting the capabilities of
this model with solutions like using independent CLS tokens for the
three modes (the model has to operate in NL only, PL only, NL-PL
concatenation), and adjusting the relative weight of the two losses
involved but failed to obtain any improvement over the separateInput NL Query: Creates a base Django project
Correct code snippet (retrieved by CasCodeâ€™s second stage):
def Func ( arg_0 ) :
i fos . path . e x i s t s ( arg_0 . _py ) :
arg_1 = os . path . j o i n ( arg_0 . _app_dir , arg_0 . _project_name )
i fos . path . e x i s t s ( arg_1 ) :
i farg_0 . _ f o r c e :
l o g g i n g . warn ( 'Removing e x i s t i n g p r o j e c t ')
s h u t i l . rmtree ( arg_1 )
else :
l o g g i n g . warn ( 'Found e x i s t i n g p r o j e c t ;
not c r e a t i n g ( use âˆ’âˆ’f o r c e t o o v e r w r i t e ) ')
return
l o g g i n g . i n f o ( 'C r e a t i n g p r o j e c t ')
arg_2 = s u b p r o c e s s . Popen ( 'cd { 0 } ; { 1 } s t a r t p r o j e c t { 2 }
> / dev / n u l l '.format (
arg_0 . _app_dir ,
arg_0 . _ v e _ d i r + os . sep + \
arg_0 . _project_name + os . sep + \
'bin'+ os . sep + \
'django âˆ’admin . py ',
arg_0 . _project_name ) ,
s h e l l =True )
os . w a i t p i d ( arg_2 . pid , 0 )
else :
l o g g i n g . e r r o r ( 'Unable t o f i n d Python i n t e r p r e t e r
i n v i r t u a l e n v ')
return
Top code snippet retrieved by CasCodeâ€™s first stage:
def Func ( ) :
arg_0 = Bunch ( DEFAULTS )
arg_0 . p r o j e c t _ r o o t = g e t _ p r o j e c t _ r o o t ( )
i f not arg_0 . p r o j e c t _ r o o t :
r a i s e RuntimeError ( " No t a s k s module i s imported ,
cannot d e t e r m i n e p r o j e c t r o o t " )
# t h i s a ss u m es an i m p o r t a b l e s e t u p . py
i farg_0 . p r o j e c t _ r o o t not in s y s . path :
s y s . path . append ( arg_0 . p r o j e c t _ r o o t )
try:
from s e t u p import arg_6
except I m p o r t E r r o r :
from s e t u p import s e t u p _ a r g s as arg_6
arg_0 . p r o j e c t = Bunch ( arg_6 )
return arg_0
Figure 8: An example from the test set of the AdvTest (nor-
malized variant) CodeSearchNet (Py) dataset with retrieved
queries from CasCodeâ€™s two stages. For the NL query "Creates
a base Django project", CasCode correctly retrieves the cor-
responding code snippet as the top result. The fast encoder
baseline (first stage of CasCode) presents this snippet as the
3rd result, this is then re-ranked to the top by CasCodeâ€™s
second stage.
variant. Lastly, similar to related work in NLP [ 7], designing in-
novative pre-training schemes to specifically improve code search
performance is also a promising direction for future work.
6 DATA AVAILABILITY
We provide our implementation (source code and instructions to
access datasets) as supplementary material to replicate the experi-
ments at this figshare URL.
398ESEC/FSE â€™23, December 3â€“9, 2023, San Francisco, CA, USA Akhilesh Deepak Gotmare, Junnan Li, Shafiq Joty, and Steven C.H. Hoi
REFERENCES
[1]Wasi Ahmad, Saikat Chakraborty, Baishakhi Ray, and Kai-Wei Chang. 2021. Uni-
fied Pre-training for Program Understanding and Generation. In Proceedings of
the 2021 Conference of the North American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies . Association for Computational
Linguistics, Online, 2655â€“2668. https://doi.org/10.18653/v1/2021.naacl-main.211
[2]Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk
Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le,
et al.2021. Program Synthesis with Large Language Models. arXiv preprint
arXiv:2108.07732 (2021). https://doi.org/10.48550/arXiv.2108.07732
[3]Max Bain, Arsha Nagrani, GÃ¼l Varol, and Andrew Zisserman. 2021. Frozen
in Time: A Joint Video and Image Encoder for End-to-End Retrieval. In 2021
IEEE/CVF International Conference on Computer Vision (ICCV) . 1708â€“1718. https:
//doi.org/10.1109/ICCV48922.2021.00175
[4]Nghi D. Q. Bui, Yijun Yu, and Lingxiao Jiang. 2020. Self-Supervised Contrastive
Learning for Code Retrieval and Summarization via Semantic-Preserving Trans-
formations. Proceedings of the 44th International ACM SIGIR Conference on Re-
search and Development in Information Retrieval (2020). https://doi.org/10.1145/
3404835.3462840
[5]Jose Cambronero, Hongyu Li, Seohyun Kim, Koushik Sen, and Satish Chandra.
2019. When deep learning met code search. In Proceedings of the 2019 27th ACM
Joint Meeting on European Software Engineering Conference and Symposium on the
Foundations of Software Engineering . 964â€“974. https://doi.org/10.1145/3338906.
3340458
[6]Rich Caruana. 1997. Multitask learning. Machine learning 28 (1997), 41â€“75.
https://doi.org/10.1023/A:1007379606734
[7]Wei-Cheng Chang, Felix X. Yu, Yin-Wen Chang, Yiming Yang, and Sanjiv Kumar.
2020. Pre-training Tasks for Embedding-based Large-scale Retrieval. In 8th
International Conference on Learning Representations, ICLR 2020, Addis Ababa,
Ethiopia, April 26-30, 2020 . OpenReview.net. https://doi.org/10.48550/arXiv.2002.
03932
[8]Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde, Jared
Kaplan, Harri Edwards, Yura Burda, Nicholas Joseph, Greg Brockman, et al .2021.
Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374
(2021). https://doi.org/10.48550/arXiv.2107.03374
[9]Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. 2020.
A simple framework for contrastive learning of visual representations. In Inter-
national conference on machine learning . PMLR, 1597â€“1607. https://doi.org/10.
48550/arXiv.2002.05709
[10] Ernest Davis. 2022. A short comment on AlphaCode. https://cs.nyu.edu/~davise/
papers/AlphaCode.html
[11] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT:
Pre-training of Deep Bidirectional Transformers for Language Understanding. In
Proceedings of the 2019 Conference of the North American Chapter of the Association
for Computational Linguistics: Human Language Technologies, Volume 1 (Long and
Short Papers) . Association for Computational Linguistics, Minneapolis, Minnesota,
4171â€“4186. https://doi.org/10.18653/v1/N19-1423
[12] Zi-Yi Dou, Aishwarya Kamath, Zhe Gan, Pengchuan Zhang, Jianfeng Wang,
Linjie Li, Zicheng Liu, Ce Liu, Yann LeCun, Nanyun Peng, Jianfeng Gao, and
Lijuan Wang. 2022. Coarse-to-Fine Vision-Language Pre-training with Fusion in
the Backbone. In Advances in Neural Information Processing Systems , S. Koyejo,
S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh (Eds.), Vol. 35. Curran
Associates, Inc., 32942â€“32956. https://doi.org/10.48550/arXiv.2206.07643
[13] Zhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xiaocheng Feng, Ming Gong,
Linjun Shou, Bing Qin, Ting Liu, Daxin Jiang, and Ming Zhou. 2020. CodeBERT:
A Pre-Trained Model for Programming and Natural Languages. In Findings of the
Association for Computational Linguistics: EMNLP 2020 . Association for Computa-
tional Linguistics, Online, 1536â€“1547. https://doi.org/10.18653/v1/2020.findings-
emnlp.139
[14] Daniel Gillick, Sayali Kulkarni, Larry Lansing, Alessandro Presta, Jason Baldridge,
Eugene Ie, and Diego Garcia-Olano. 2019. Learning Dense Representations for
Entity Retrieval. In Proceedings of the 23rd Conference on Computational Natural
Language Learning (CoNLL) . Association for Computational Linguistics, Hong
Kong, China, 528â€“537. https://doi.org/10.18653/v1/K19-1049
[15] Xiaodong Gu, Hongyu Zhang, and Sunghun Kim. 2018. Deep code search. In 2018
IEEE/ACM 40th International Conference on Software Engineering (ICSE) . IEEE,
933â€“944. https://doi.org/10.1145/3180155.3180167
[16] Daya Guo, Shuai Lu, Nan Duan, Yanlin Wang, Ming Zhou, and Jian Yin. 2022.
UniXcoder: Unified Cross-Modal Pre-training for Code Representation. In Pro-
ceedings of the 60th Annual Meeting of the Association for Computational Linguis-
tics (Volume 1: Long Papers) . Association for Computational Linguistics, Dublin,
Ireland, 7212â€“7225. https://doi.org/10.18653/v1/2022.acl-long.499
[17] Daya Guo, Shuo Ren, Shuai Lu, Zhangyin Feng, Duyu Tang, Shujie Liu, Long
Zhou, Nan Duan, Alexey Svyatkovskiy, Shengyu Fu, et al .2021. Graphcodebert:
Pre-training code representations with data flow. ICLR 2021 (2021). https:
//doi.org/10.48550/arXiv.2009.08366
[18] Michael Gutmann and Aapo HyvÃ¤rinen. 2010. Noise-contrastive estimation: A
new estimation principle for unnormalized statistical models. In Proceedings ofthe thirteenth international conference on artificial intelligence and statistics . JMLR
Workshop and Conference Proceedings, 297â€“304. https://doi.org/dl.acm.org/doi/
abs/10.5555/2503308.2188396
[19] Junjie Huang, Duyu Tang, Linjun Shou, Ming Gong, Ke Xu, Daxin Jiang, Ming
Zhou, and Nan Duan. 2021. CoSQA: 20,000+ Web Queries for Code Search and
Question Answering. In Proceedings of the 59th Annual Meeting of the Association
for Computational Linguistics and the 11th International Joint Conference on Natu-
ral Language Processing (Volume 1: Long Papers) . Association for Computational
Linguistics, Online, 5690â€“5700. https://doi.org/10.18653/v1/2021.acl-long.442
[20] Jui-Ting Huang, Ashish Sharma, Shuying Sun, Li Xia, David Zhang, Philip
Pronin, Janani Padmanabhan, Giuseppe Ottaviano, and Linjun Yang. 2020.
Embedding-based Retrieval in Facebook Search. Proceedings of the 26th ACM
SIGKDD International Conference on Knowledge Discovery & Data Mining (2020).
https://doi.org/10.1145/3394486.3403305
[21] Hamel Husain, Ho-Hsiang Wu, Tiferet Gazit, Miltiadis Allamanis, and Marc
Brockschmidt. 2019. Codesearchnet challenge: Evaluating the state of semantic
code search. arXiv preprint arXiv:1909.09436 (2019). https://doi.org/10.48550/
arXiv.1909.09436
[22] Aditya Kanade, Petros Maniatis, Gogul Balakrishnan, and Kensen Shi. 2019.
Learning and Evaluating Contextual Embedding of Source Code. In International
Conference on Machine Learning . https://doi.org/doi/abs/10.5555/3524938.3525412
[23] Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey
Edunov, Danqi Chen, and Wen-tau Yih. 2020. Dense Passage Retrieval for Open-
Domain Question Answering. In Proceedings of the 2020 Conference on Empirical
Methods in Natural Language Processing (EMNLP) . Association for Computational
Linguistics, Online, 6769â€“6781. https://doi.org/10.18653/v1/2020.emnlp-main.550
[24] Omar Khattab and Matei Zaharia. 2020. Colbert: Efficient and effective passage
search via contextualized late interaction over bert. In Proceedings of the 43rd
International ACM SIGIR conference on research and development in Information
Retrieval . 39â€“48. https://doi.org/10.1145/3397271.3401075
[25] Heidy Khlaaf, Pamela Mishkin, Joshua Achiam, Gretchen Krueger, and Miles
Brundage. 2022. A Hazard Analysis Framework for Code Synthesis Large Lan-
guage Models. ArXiv abs/2207.14157 (2022). https://doi.org/10.48550/arXiv.2207.
14157
[26] Diederik P. Kingma and Jimmy Ba. 2015. Adam: A Method for Stochastic Opti-
mization. In 3rd International Conference on Learning Representations, ICLR 2015,
San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings , Yoshua Bengio
and Yann LeCun (Eds.). https://doi.org/10.48550/arXiv.1412.6980
[27] Hung Le, Yue Wang, Akhilesh Deepak Gotmare, Silvio Savarese, and Steven
Chu Hong Hoi. 2022. Coderl: Mastering code generation through pretrained
models and deep reinforcement learning. Advances in Neural Information Process-
ing Systems 35 (2022), 21314â€“21328. https://doi.org/10.48550/arXiv.2207.01780
[28] Junnan Li, Dongxu Li, Caiming Xiong, and Steven C. H. Hoi. 2022. BLIP:
Bootstrapping Language-Image Pre-training for Unified Vision-Language Un-
derstanding and Generation. In International Conference on Machine Learning .
https://doi.org/10.48550/arXiv.2201.12086
[29] Junnan Li, Ramprasaath R. Selvaraju, Akhilesh Deepak Gotmare, Shafiq Joty,
Caiming Xiong, and Steven Hoi. 2021. Align before Fuse: Vision and Language
Representation Learning with Momentum Distillation. In NeurIPS . https://doi.
org/10.48550/arXiv.2107.07651
[30] Yujia Li, David H. Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser,
RÃ©mi Leblond, Tom, Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago,
Thomas Hubert, Peter Choy, Cyprien de, Masson dâ€™Autume, Igor Babuschkin,
Xinyun Chen, Po-Sen Huang, Johannes Welbl, Sven Gowal, Alexey, Cherepanov,
James Molloy, Daniel Jaymin Mankowitz, Esme Sutherland Robson, Push-
meet Kohli, Nando de, Freitas, Koray Kavukcuoglu, and Oriol Vinyals. 2022.
Competition-level code generation with AlphaCode. Science 378 (2022), 1092 â€“
1097. https://doi.org/doi:10.1126/science.abq1158
[31] Jimmy Lin, Xueguang Ma, Sheng-Chieh Lin, Jheng-Hong Yang, Ronak Pradeep,
and Rodrigo Nogueira. 2021. Pyserini: A Python Toolkit for Reproducible In-
formation Retrieval Research with Sparse and Dense Representations. In Pro-
ceedings of the 44th Annual International ACM SIGIR Conference on Research
and Development in Information Retrieval (SIGIR 2021) . 2356â€“2362. https:
//doi.org/10.1145/3404835.3463238
[32] Jimmy Lin, Rodrigo Nogueira, and Andrew Yates. 2021. Pretrained transformers
for text ranking: Bert and beyond. Synthesis Lectures on Human Language Tech-
nologies 14, 4 (2021), 1â€“325. https://doi.org/10.18653/v1/2021.naacl-tutorials.1
[33] Jiawei Liu, Chun Xia, Yuyao Wang, and Lingming Zhang. 2023. Is Your Code
Generated by ChatGPT Really Correct? Rigorous Evaluation of Large Language
Models for Code Generation. ArXiv abs/2305.01210 (2023). https://doi.org/10.
48550/arXiv.2305.01210
[34] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer
Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. RoBERTa: A
Robustly Optimized BERT Pretraining Approach. ArXiv abs/1907.11692 (2019).
https://doi.org/10.48550/arXiv.1907.11692
[35] Shuai Lu, Nan Duan, Hojae Han, Daya Guo, Seung-won Hwang, and Alexey Svy-
atkovskiy. 2022. ReACC: A Retrieval-Augmented Code Completion Framework.
InProceedings of the 60th Annual Meeting of the Association for Computational
399Efficient Text-to-Code Retrieval with Cascaded Fast and Slow Transformer Models ESEC/FSE â€™23, December 3â€“9, 2023, San Francisco, CA, USA
Linguistics (Volume 1: Long Papers) . Association for Computational Linguistics,
Dublin, Ireland, 6227â€“6240. https://doi.org/10.18653/v1/2022.acl-long.431
[36] Shuai Lu, Daya Guo, Shuo Ren, Junjie Huang, Alexey Svyatkovskiy, Ambrosio
Blanco, Colin B. Clement, Dawn Drain, Daxin Jiang, Duyu Tang, Ge Li, Lidong
Zhou, Linjun Shou, Long Zhou, Michele Tufano, Ming Gong, Ming Zhou, Nan
Duan, Neel Sundaresan, Shao Kun Deng, Shengyu Fu, and Shujie Liu. 2021.
CodeXGLUE: A Machine Learning Benchmark Dataset for Code Understanding
and Generation. CoRR abs/2102.04664 (2021). https://doi.org/10.48550/arXiv.
2102.04664
[37] Antoine Miech, Jean-Baptiste Alayrac, Ivan Laptev, Josef Sivic, and Andrew
Zisserman. 2021. Thinking Fast and Slow: Efficient Text-to-Visual Retrieval with
Transformers. In Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition . 9826â€“9836. https://doi.org/10.1109/CVPR46437.2021.00970
[38] Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. 2013.
Distributed Representations of Words and Phrases and their Compositionality.
InAdvances in Neural Information Processing Systems , C.J. Burges, L. Bottou,
M. Welling, Z. Ghahramani, and K.Q. Weinberger (Eds.), Vol. 26. Curran Asso-
ciates, Inc. https://doi.org/doi/10.5555/2999792.2999959
[39] Bhaskar Mitra, Eric Nalisnick, Nick Craswell, and Rich Caruana. 2016. A Dual
Embedding Space Model for Document Ranking. https://doi.org/10.48550/arXiv.
1602.01137
[40] John X. Morris, Eli Lifland, Jin Yong Yoo, Jake Grigsby, Di Jin, and Yanjun Qi.
2020. TextAttack: A Framework for Adversarial Attacks, Data Augmentation,
and Adversarial Training in NLP. In Conference on Empirical Methods in Natural
Language Processing . https://doi.org/10.18653/v1/2020.emnlp-demos.16
[41] Noor Nashid, Mifta Sintaha, and Ali Mesbah. 2023. Retrieval-Based Prompt Selec-
tion for Code-Related Few-Shot Learning. In 2023 IEEE/ACM 45th International
Conference on Software Engineering (ICSE). https://doi.org/10.1109/ICSE48619.
2023.00205
[42] Arvind Neelakantan, Tao Xu, Raul Puri, Alec Radford, Jesse Michael Han, Jerry
Tworek, Qiming Yuan, Nikolas Tezak, Jong Wook Kim, Chris Hallacy, Johannes
Heidecke, Pranav Shyam, Boris Power, Tyna Eloundou Nekoul, Girish Sastry,
Gretchen Krueger, David Schnurr, Felipe Petroski Such, Kenny Hsu, Madeleine
Thompson, Tabarak Khan, Toki Sherbakov, Joanne Jang, Peter Welinder, and
Lilian Weng. 2022. Text and Code Embeddings by Contrastive Pre-Training.
arXiv:2201.10005 [cs.CL]
[43] Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou,
Silvio Savarese, and Caiming Xiong. 2023. CodeGen: An Open Large Language
Model for Code with Multi-Turn Program Synthesis. ICLR (2023). https://doi.
org/10.48550/arXiv.2203.13474
[44] Rodrigo Nogueira, Wei Yang, Kyunghyun Cho, and Jimmy J. Lin. 2019. Multi-
Stage Document Ranking with BERT. ArXiv abs/1910.14424 (2019). https:
//doi.org/10.48550/arXiv.1910.14424
[45] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela
Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schul-
man, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell,
Peter Welinder, Paul F Christiano, Jan Leike, and Ryan Lowe. 2022. Training
language models to follow instructions with human feedback. In Advances in
Neural Information Processing Systems , S. Koyejo, S. Mohamed, A. Agarwal, D. Bel-
grave, K. Cho, and A. Oh (Eds.), Vol. 35. Curran Associates, Inc., 27730â€“27744.
https://doi.org/10.48550/arXiv.2203.02155
[46] Md Rizwan Parvez, Wasi Ahmad, Saikat Chakraborty, Baishakhi Ray, and Kai-
Wei Chang. 2021. Retrieval Augmented Code Generation and Summarization. In
Findings of the Association for Computational Linguistics: EMNLP 2021 . Association
for Computational Linguistics, Punta Cana, Dominican Republic, 2719â€“2734.
https://doi.org/10.18653/v1/2021.findings-emnlp.232
[47] Hammond Pearce, Baleegh Ahmad, Benjamin Tan, Brendan Dolan-Gavitt, and
Ramesh Karri. 2022. Asleep at the Keyboard? Assessing the Security of GitHub
Copilotâ€™s Code Contributions. In 2022 IEEE Symposium on Security and Privacy
(SP). 754â€“768. https://doi.org/10.1109/SP46214.2022.9833571
[48] Jeffrey Pennington, Richard Socher, and Christopher Manning. 2014. GloVe:
Global Vectors for Word Representation. In Proceedings of the 2014 Conference on
Empirical Methods in Natural Language Processing (EMNLP) , Alessandro Moschitti,
Bo Pang, and Walter Daelemans (Eds.). Association for Computational Linguistics,
Doha, Qatar, 1532â€“1543. https://doi.org/10.3115/v1/D14-1162
[49] Sebastian Ruder. 2017. An overview of multi-task learning in deep neural net-
works. arXiv preprint arXiv:1706.05098 (2017). https://doi.org/10.48550/arXiv.1706.05098
[50] Saksham Sachdev, Hongyu Li, Sifei Luan, Seohyun Kim, Koushik Sen, and Satish
Chandra. 2018. Retrieval on source code: a neural code search. In Proceedings of the
2nd ACM SIGPLAN International Workshop on Machine Learning and Programming
Languages . 31â€“41. https://doi.org/10.1145/3211346.3211353
[51] Alexey Svyatkovskiy, Shao Kun Deng, Shengyu Fu, and Neel Sundaresan. 2020.
Intellicode compose: Code generation using transformer. In Proceedings of the 28th
ACM Joint Meeting on European Software Engineering Conference and Symposium
on the Foundations of Software Engineering . 1433â€“1443. https://doi.org/10.1145/
3368089.3417058
[52] JuliÃ¡n Urbano, Harlley Lima, and Alan Hanjalic. 2019. Statistical Significance
Testing in Information Retrieval: An Empirical Analysis of Type I, Type II and
Type III Errors. In Proceedings of the 42nd International ACM SIGIR Conference
on Research and Development in Information Retrieval (Paris, France) (SIGIRâ€™19) .
Association for Computing Machinery, New York, NY, USA, 505â€“514. https:
//doi.org/10.1145/3331184.3331259
[53] Guido van Rossum. 1997. Comparing Python to Other Languages. https:
//www.python.org/doc/essays/comparisons/
[54] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is All
you Need. In Advances in Neural Information Processing Systems 30: Annual Con-
ference on Neural Information Processing Systems 2017, December 4-9, 2017, Long
Beach, CA, USA , Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M.
Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett (Eds.). 5998â€“6008.
https://doi.org/doi/10.5555/3295222.3295349
[55] Yao Wan, Jingdong Shu, Yulei Sui, Guandong Xu, Zhou Zhao, Jian Wu, and Philip
Yu. 2019. Multi-modal attention network learning for semantic source code
retrieval. In 2019 34th IEEE/ACM International Conference on Automated Software
Engineering (ASE) . IEEE, 13â€“25. https://doi.org/10.1109/ASE.2019.00012
[56] Xin Wang, Yasheng Wang, Fei Mi, Pingyi Zhou, Yao Wan, Xiao Liu, Li Li, Hao Wu,
Jin Liu, and Xin Jiang. 2021. SynCoBERT: Syntax-Guided Multi-Modal Contrastive
Pre-Training for Code Representation. arXiv preprint arXiv:2108.04556 . https:
//doi.org/10.48550/arXiv.2108.04556
[57] Yue Wang, Hung Le, Akhilesh Deepak Gotmare, Nghi DQ Bui, Junnan Li, and
Steven CH Hoi. 2023. Codet5+: Open code large language models for code
understanding and generation. arXiv preprint arXiv:2305.07922 (2023). https:
//doi.org/10.48550/arXiv.2305.07922
[58] Yue Wang, Weishi Wang, Shafiq Joty, and Steven CH Hoi. 2021. CodeT5: Identifier-
aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and
Generation. Proceedings of the 2021 Conference on Empirical Methods in Natural
Language Processing, EMNLP 2021 (2021). https://doi.org/10.18653/v1/2021.emnlp-
main.685
[59] Lee Xiong, Chenyan Xiong, Ye Li, Kwok-Fung Tang, Jialin Liu, Paul Bennett,
Junaid Ahmed, and Arnold Overwijk. 2021. Approximate nearest neighbor
negative contrastive learning for dense text retrieval. ICLR 2021 (2021). https:
//doi.org/10.48550/arXiv.2007.00808
[60] Frank F. Xu, Bogdan Vasilescu, and Graham Neubig. 2021. In-IDE Code Gen-
eration from Natural Language: Promise and Challenges. ACM Transactions
on Software Engineering and Methodology (TOSEM) 31 (2021), 1 â€“ 47. https:
//doi.org/10.1145/3487569
[61] Xin Ye, Hui Shen, Xiao Ma, Razvan Bunescu, and Chang Liu. 2016. From word
embeddings to document similarities for improved information retrieval in soft-
ware engineering. In Proceedings of the 38th international conference on software
engineering . 404â€“415. https://doi.org/10.1145/2884781.2884862
[62] Minjia Zhang, Samyam Rajbandari, Wenhan Wang, Elton Zheng, Olatunji Ruwase,
Jeff Rasley, Jason Li, Junhua Wang, and Yuxiong He. 2019. Accelerating Large
Scale Deep Learning Inference through {DeepCPU}at Microsoft. In 2019 USENIX
Conference on Operational Machine Learning (OpML 19) . 5â€“7.
[63] W. Zhang, Quan Z. Sheng, Ahoud Abdulrahmn F. Alhazmi, and Chenliang Li. 2019.
Adversarial Attacks on Deep-learning Models in Natural Language Processing.
ACM Transactions on Intelligent Systems and Technology (TIST) 11 (2019), 1 â€“ 41.
https://doi.org/10.1145/3374217
[64] Shuyan Zhou, Uri Alon, Frank F. Xu, Zhiruo Wang, Zhengbao Jiang, and Gra-
ham Neubig. 2023. DocPrompting: Generating Code by Retrieving the Docs.
InInternational Conference on Learning Representations (ICLR) . Kigali, Rwanda.
https://doi.org/10.48550/arXiv.2207.05987
400