Fair Preprocessing:Towards UnderstandingCompositional
Fairness
ofDataTransformersinMachine LearningPipeline
Sumon Biswas
Dept. of ComputerScience, IowaStateUniversity
Ames, IA,USA
sumon@iastate.eduHridesh Rajan
Dept. of ComputerScience, IowaStateUniversity
Ames, IA,USA
hridesh@iastate.edu
ABSTRACT
Inrecentyears,manyincidentshavebeenreportedwheremachine
learningmodelsexhibiteddiscriminationamongpeoplebasedon
race, sex, age, etc. Research has been conducted to measure and
mitigate unfairness in machine learning models. For a machine
learning task, it is a common practice to build a pipeline that in-
cludes an ordered set of data preprocessing stages followed by a
classifier. However, most of the research on fairness has consid-
ered a single classifier based prediction task. What are the fairness
impacts of the preprocessing stages in machine learning pipeline?
Furthermore,studiesshowedthatoftentherootcauseofunfairness
isingrainedinthedataitself,ratherthanthemodel.Butnoresearch
has been conducted to measure the unfairness caused by a specific
transformation made in the data preprocessing stage. In this paper,
we introduced the causal method of fairness to reason about the
fairness impact of data preprocessing stages in ML pipeline. We
leveraged existing metrics to define the fairness measures of the
stages. Then we conducted a detailed fairness evaluation of the
preprocessing stages in 37 pipelines collected from three differ-
ent sources. Our results show that certain data transformers are
causing the model to exhibit unfairness. We identified a number of
fairness patterns in several categories of data transformers. Finally,
we showed how the local fairness of a preprocessing stage com-
poses in the global fairness of the pipeline. We used the fairness
compositiontochooseappropriatedownstreamtransformerthat
mitigates unfairness in the machine learningpipeline.
CCS CONCEPTS
·Softwareanditsengineering →Softwarecreationandman-
agement;· Computingmethodologies →Machine learning.
KEYWORDS
fairness, machine learning, preprocessing, pipeline, models
ACM Reference Format:
Sumon Biswas and Hridesh Rajan. 2021. Fair Preprocessing: Towards Under-
standing Compositional Fairness of Data Transformers in Machine Learning
Pipeline. In Proceedings of the 29th ACM Joint European Software Engineer-
ing Conference and Symposium on the Foundations of Software Engineering
(ESEC/FSE ’21), August 23ś28, 2021, Athens, Greece. ACM, New York, NY,
USA, 13 pages. https://doi.org/10.1145/3468264.3468536
ESEC/FSE ’21, August 23ś28, 2021, Athens, Greece
© 2021 Copyright held by the owner/author(s).
ACM ISBN 978-1-4503-8562-6/21/08.
https://doi.org/10.1145/3468264.34685361 INTRODUCTION
Fairnessofmachinelearning(ML)predictionsisbecomingmoreim-
portant with the rapid increase of ML software usage in important
decisionmaking[ 5,22,30,50],andtheblack-boxnatureofMLalgo-
rithms [3,27]. There isa rich bodyof workon measuring fairness
of ML models [ 15,19,23,25,32,65,72,74] and mitigate the bias
[15,19,29,32,42,55,72,75].Recentwork[ 10,13,17,26,33,35]has
shownthatmoresoftwareengineeringeffortisrequiredtowards
detecting bias in complex environment and support developersin
building fairer models.
ThemajorityofworkonMLfairnesshasfocusedonclassification
taskwithsingleclassifier[ 3,12,25,27].However,real-worldma-
chine learning software operate in a complex environment[ 12,21].
Inan MLtask, thepredictionis madeafter goingthroughaseries
of stages such as data cleaning, feature engineering, etc., which
build the machine learning pipeline [ 4,71]. Studying only the fair-
nessoftheclassifiers(e.g., DecisionTree, LogisticRegression )fails
to capture the fairness impact made by other stages in ML pipeline.
In this paper, we conducted a detailed analysis on how the data
preprocessing stagesaffectfairnessinML pipelines.
Prior research observed that bias can be encoded in the data
itself and missing the opportunity to detect bias in earlier stage
of ML pipeline can make it difficult to achieve fairness algorith-
mically [22,31,35,44]. Additionally, bias mitigation algorithms
operatinginthe preprocessingstage were shownto besuccessful
[25,41].Therefore,itisevidentthatthepreprocessingstagesofML
pipelinecanintroducebias.However,nostudyhasbeenconducted
tomeasurethefairnessofthepreprocessingstagesandshowhowit
impactstheoverallfairnessofthepipeline.Inthispaper,weusedthe
causalmethodoffairnesstoreasonaboutthefairnessimpactofpre-
processing stages in ML pipeline. Then, we leveraged existing fair-
nessmetricstomeasurefairness ofthepreprocessingstages.Using
themeasures,weconductedathoroughanalysisonabenchmark
of37real-worldMLpipelinescollectedfromthreedifferentsources,
whichoperateonfivedatasets.TheseMLpipelinesallowedusto
evaluatefairnessofawideselectionofpreprocessingstagesfrom
different categories such as data standardization, feature selection,
encoding,over/under-sampling,imputation,etc.Forcomparative
analysis,wealsocollecteddatatransformerse.g., StandardScaler ,
MinMaxScaler ,PCA,l1-normalizer ,QuantileTransformer ,etc.,
from the pipelines as well as corresponding ML libraries, and eval-
uatedfairness.Finally,weinvestigatedhowfairnessofthesepre-
processing techniques (local fairness ) composes with other prepro-
cessing stages, and the whole pipeline (global fairness ). Specifically,
we answeredthe following three research questions.
RQ1(fairness of preprocessing stages): What are the fairness
measures of each preprocessing stage in ML pipeline? RQ2(fair
981This work is licensed under a Creative Commons Attribution International 4.0 License.
ESEC/FSE ’21, August 23–28, 2021,Athens,Greece SumonBiswas andHrideshRajan
transformers): What are the fair (and biased) data transformers
amongthecommonlyusedones? RQ3(fairnesscomposition):How
fairnessofdata preprocessing stagescomposes inML pipeline?
•Howlocal fairnesscompose intoglobalfairness?
•Does choosing a downstream transformer depend on the
fairnessofan upstream transformer?
To the best of our knowledge, we are the first to evaluate the
fairnessofpreprocessingstagesinMLpipeline.Ourresultsshow
that by measuring the fairness impact of the stages, the developers
wouldbeabletobuildfairerpredictionseffectively.Furthermore,
the libraries can provide fairness monitoring into the data trans-
formers,similartotheperformancemonitoringfortheclassifiers.
Ourevaluationonreal-worldMLpipelinesalsosuggestsopportuni-
tiestobuildautomatedtooltodetectunfairnessinthepreprocessing
stages,andinstrumentthosestagestomitigatebias.Wehavemade
the following contributionsinthis paper:
(1)We created a fairness benchmark of ML pipelines with several
stages.Thebenchmark,codeandresultsaresharedinourrepli-
cation package1in GitHub repository, that can be leveraged in
further researchonbuildingfair ML pipeline.
(2)WeintroducedthenotionofcausalityinMLpipelineandlever-
aged existing metrics to measure the fairness of preprocessing
stagesinML pipeline.
(3)Unfairnesspatternshavebeenidentifiedforanumberofstages.
(4)We identified alternative data transformers which can mitigate
biasinthe pipeline.
(5)Finally,weshowedthecompositionofstage-specificfairness
intooverallfairness,whichisusedtochooseappropriatedown-
stream transformerthat mitigates bias.
Thepaperisorganizedasfollows:ğ 2describesthemotivating
examples, ğ 3describes the existing metrics and our approach. In
ğ4,we describedthebenchmarkand experiments.ğ 5exploresthe
results, ğ6provides a comparative studyamong transformers, and
ğ7evaluates the fairness composition. Finally, ğ 9describes the
threatsto validity,ğ 10discussesrelatedwork, andğ 11concludes.
2 MOTIVATION
In this section, we present two ML pipelines which show that
thepreprocessingstageaffectsthefairnessofthemodelanditis
importantto study the biasinducedbycertaindata transformers.
2.1 Motivating Example 1
Yanget al.[71] studied the following ML pipeline which was origi-
nallyoutlinedbyPropublicaforrecidivismpredictionon Compas
dataset [5]. The goal is predict future crimes based on the data
of defendants. The fairness values, in terms of statistical parity
difference (SPD: -0.102) and equal opportunity difference (EOD:
-0.027), suggest that the prediction is biased towards2Caucasian
defendants when raceis considered as sensitive attribute. The
pipelineconsistsofseveralpreprocessingstagesbeforeapplying
LogisticRegression classifier.Datapreprocessingincludesclean-
ing,encodingcategoricalfeatures,andmissingvalueimputation.
Recentresearch[ 71]showedthatthedatatransformationinthis
1https://github.com/sumonbis/FairPreprocessing
2Biastowardsa group connotesthat the prediction favours that group.pipelineisnotsymmetricacrossgendergroupsi.e.,maledefendants
arefilteredmorethanthefemale.Dothesedatatransformationsin-
troduceunfairnessintheprediction?Ifyes,whataretheunfairness
measuresofthesetransformers?Isitpossibletoleverageexisting
metrics to measure the unfairness of each component? If we can
understandtheeffectofeachdatatransformer,itwouldbepossible
tochoosedatapreprocessingtechniquewiselytoavoidintroducing
biasas well as mitigate the inherentbiasindata orclassifier.
1df = pd. read_csv ( f_path )
2df = df [( df . days_b_screening_arrest <= 30)
3& ( df . days_b_screening_arrest >= −30)
4& ( df . is_recid != −1) & ( df . c_charge_degree != 'O')
5& ( df . score_text != 'N/A') ]
6df = df . replace ( 'Medium ' ,'Low ')
7labels = LabelEncoder () . fit_transform ( df . score_text )
8impute1_onehot = Pipeline ([
9 (' imputer1 ' , SimpleImputer ( strategy= ' most_frequent ' ) ) ,
10 ('onehot ' , OneHotEncoder(handle_unknown= ' ignore ' ) ) ])
11impute2_bin = Pipeline ([
12 (' imputer2 ' , SimpleImputer ( strategy= 'mean ') ) ,
13 (' discretizer ' , KBinsDiscretizer ( n_bins=4, encode= ' ordinal ' ,
strategy= 'uniform ' ) ) ])
14featurizer = ColumnTransformer( transformers=[
15 (' impute1_onehot ' , impute1_onehot , [ ' is_recid ' ]) ,
16 (' impute2_bin ' , impute2_bin , [ 'age ']) ])
17pipeline = Pipeline ([( ' features ' , featurizer ) ,
18 (' classifier ' , LogisticRegression () ) ])
2.2 Motivating Example 2
ThefollowingMLpipelineiscollectedfromthebenchmarkused
by Biswas and Rajan [ 10] for studying fairness of ML models. This
pipelineoperateson GermanCredit dataset.Here,thegoalistopre-
dictthecreditrisk(good/bad)ofindividualsbasedontheirpersonal
datasuchasage,sex,income,etc.Inthispipeline,beforetraining
the classifier, data has been processed using two transformers: PCA
forprincipalcomponentanalysis,and SelectKBest forselecting
high-scoringfeatures.Thefairnessvalue(SPD:0.005)showsthat
predictionisslightlybiasedtowards femalecandidates.However,if
thetransformersarenotapplied,thenpredictionbecomesbiased
towardsmale(SPD: -0.117). By applying one transformer at a time,
weobservedthat PCAaloneisnotcausingthechangeoffairness.In
thiscase, SelectBest iscausingbiastowards female,whichinturn
mitigatingtheoverallfairnessofthepipeline.Therefore,inaddition
to study the fairness of transformers in isolation, it is important to
understand howfairness of components composesin the pipeline.
1features = []
2features . append (( 'pca ', PCA(n_components=2) ) )
3features . append (( ' select_best ' , SelectKBest (6) ) )
4feature_union = FeatureUnion ( features )
5estimators = []
6estimators . append (( ' feature_union ' , feature_union ) )
7estimators . append (( 'RF ', RandomForestClassifier () ) )
8model = Pipeline ( estimators )
9model . fit ( X_train , y_train )
10y_pred = model . predict ( X_test )
Ourkeyideaistoleveragecausalreasoningandobservefairness
impact of a stage on prediction. To do that we create alternative
pipelinebyremovingastage.Forexample,fromtheabovepipeline,
we remove the SelectKBest and compare the predictions with
original pipeline. We observe that SelectKBest is causing 1.1% of
the female and 3.6% of the male participants to change predictions
from favorable(good credit)to unfavorable(bad credit).Since the
stage is causing more unfavorable decisions to male, the stage is
biasedtowardsfemale.Thus,weusedexistingfairnesscriteriato
measure fairnessimpact of astageandpropose novel metrics.
982Fair Preprocessing: TowardsUnderstandingCompositionalFairness of DataTransformersin MachineLearning Pipeline ESEC/FSE ’21, August 23–28, 2021,Athens,Greece
Processed 
dataCustom:
Data filtration
S6
PCA S1SelectBest S2Classifier:
RandomForestS3S1S3
S5
KBinsDiscretizer 
(f2)1-hot encoder 
(f1)
Classifier:
LogisticRegression
Binarize (f3)S4
S2
S7
Motivating example 1Mean
imputer (f2)Model
PredictionTest 
data
Model
PredictionTest 
data
Mode 
imputer (f1)
Motivating example 2
Processed 
data
 Training data
Training data
Custom:
Data filtration
Figure 1: ML pipelines for the motivating examples, having
asequenceofpreprocessing stages followed by aclassifier.
3 METHODOLOGY
Inthissection,first,wedescribethebackgroundofMLpipeline,fo-
cussingonthedatapreprocessingstages.Second,weformulatethe
method and metrics to measure fairness of a certain preprocessing
stagewithrespectto the pipeline itisusedwithin.
3.1 ML Pipeline
Amershi et al.proposed a nine-stage machine learning pipeline
withdata-oriented(collection,cleaning,andlabeling)andmodel-
oriented (model requirements, feature engineering, training, evalu-
ation,deployment,andmonitoring)stages[ 4].Otherresearch[ 1,7]
also described data preprocessing as an integral part of the ML
pipeline. The pipelines in the motivating examples are depicted in
Figure1, whichfollows therepresentationprovidedby Yang et al.
[71]. In this paper, we adapted the canonical definition of pipeline
fromScikit-Learnpipelinespecification[ 14,63],whichisaligned
withtheMLmodelsstudiedintheliteratureforfairclassification
tasks [3,8,10,26,27,71]. We are interested in investigating the
fairnessofthedatapreprocessingstagesinthepipeline,whichis
depictedwithgrey boxesinFigure 1.
To summarize, a canonical ML pipeline is an ordered set of m
stages with a set of preprocessing stages (S1,S2,...Sm−1) and a fi-
nalclassifier (Sm). Each preprocessing stage, Skoperates on the
data already processed by preceding stages S1,...Sk−1. A data
preprocessing stage Skcan be a data transformer or a set of cus-
tomoperations.A datatransformer isawell-knownalgorithmor
methodto performaspecificoperationsuch asvariableencoding,
feature selection, feature extraction, dimensionality reduction, etc.
onthedata[ 14].Forexample,inthesecondmotivatingexample,
two transformers ( PCAandSelectKBest ) have been used. Custom
transformation includesdata/task-specificcontextualoperationson
thedataset.Forexample,inğ 2.1(line 2-3),thedatainstancesthat
do not contain a value in the range [-30, 30] for the feature days_-
b_screening_arrest , have been filtered. This means the pipeline
ignoredthedataofthedefendantswithmorethan30daysbetween
theirscreeningandarrest.ThisformulationofMLpipelineallowed
ustoevaluatefairnessofthepreprocessingstagesinreal-worldML
tasks.
3.2 ExistingFairnessMetrics
Wehaveleveragedexistingfairnessmetricstomeasurethefairness
ofthewholepipeline.Manyfairnessmetricshavebeenproposedin
theliteratureformeasuringfairnessofclassificationtasks[ 8,9,22].
Ingeneral,thefairnessmetricscomputegroup-specificclassifica-
tion rates (e.g., true positives, false positives), and calculates thedifferencebetweengroupstomeasurethefairness.Inthispaper,we
adopted the representative group fairness metrics used by [ 10,26].
Specifically,weleveragedthefollowingmetrics:statisticalparity
difference (SPD) [ 25,41,72], equal opportunity difference (EOD)
[32], average odds difference (AOD) [ 32], and error rate difference
(ERD)[19].Givenadataset Dwithninstances,let,actualclassifi-
cationlabelbe Y,predictedclassificationlabelbe ˆY,andsensitive
attributebe A.Here,Y=1ifthelabelisfavorabletotheindivid-
uals,otherwise Y=0.Forexample,classificationtaskon German
Creditdatasetpredictsthecreditrisk(good/badcredit)ofindivid-
uals. In this case, Y=1 if the prediction is good credit , otherwise
Y=0. Suppose, for privileged group (e.g., White),A=1 and for
unprivilegedgroup(e.g., non-White ),A=0.SPDiscomputedby
observingtheprobabilityofgivingfavorablelabeltoeachgroup
and taking the difference. EOD measures the true-positive rate dif-
ference between groups. AOD calculates both true positive rate
andfalsepositiveratedifferenceandthentakestheaverage.ERD
calculatesthesumoffalsepositiveratedifferenceandfalsenegative
ratedifferencebetweengroups.Thedefinitionsofthesemetricsare
as follows:
SPD=P[ˆY=1|A=0]−P[ˆY=1|A=1]
EOD=P[ˆY=1|Y=1,A=0]−P[ˆY=1|Y=1,A=1]
AOD=(1/2){(P[ˆY=1|Y=1,A=0]−P[ˆY=1|Y=1,A=1])
+(P[ˆY=1|Y=0,A=0]−P[ˆY=1|Y=0,A=1])}
ERD=(P[ˆY=1|Y=0,A=0]−P[ˆY=1|Y=0,A=1])}
+(P[ˆY=0|Y=1,A=0]−P[ˆY=0|Y=1,A=1])} (1)
Disparateimpact(DI)andstatisticalparitydifference(SPD)both
measurethesameratei.e.,probabilityofclassifyingdatainstanceas
favorable, but DI computes the ratio of privileged and unprivileged
groups’rate,whereasSPDcomputesthedifference.Therefore,from
DI andSPD,we only usedSPD inour evaluation.
3.3 FairnessofPreprocessing Stages
Suppose, Pisapipelinewith mstagesandourgoalistoevaluate
thefairnessofthestage Sk,where1≤k<m.Inotherwords,we
wanttomeasurethefairness impactof Skonthe predictionmade
byP.Toachievethatweappliedthecausalreasoningforevaluating
fairness. The causality theorem was proposed by Pearl[53,54] and
furtherstudiedextensivelytoreasonaboutfairnessinmanysce-
narios[27,47,57,59,76].Causalitynotionoffairnesscapturesthat
everythingelsebeingequal, thepredictionwouldnot bechanged
in the counterfactual world where onlyan intervention happens
on a variable [ 27,47,57]. For example, Galhotra et al .proposed
causaldiscriminationscore forfairnesstesting[ 27].Theauthors
createdtestinputsbyalteringoriginalprotectedattributevalues
of each data instance, and observed whether prediction is changed
for those test inputs. If the intervention causes the prediction to
be changed, we call the software causally unfair with respect to
that intervention. In our case, if a preprocessing stage Skbe the
intervention, to measure the fairness of Sk, we have to capture
the prediction disparity caused by the intervention Sk. This causal
reasoningoffairnessisastrongernotionsinceitprovidescausality
insoftwarebyobservingchangesintheoutcomemadebyaspecific
stageinthe pipeline [ 27,54].
983ESEC/FSE ’21, August 23–28, 2021,Athens,Greece SumonBiswas andHrideshRajan
3.3.1 Causal Method to Measure Fairness of Preprocessing Stage.
Frompipeline P,weconstructanotherpipeline P∗byonlyexclud-
ingthestage SkfromP.Afterapplyingthestage SkinP∗,towhat
extent the prediction of P∗changes, and whether the change is
favorabletoanygroup?Broadly,thiscanbemeasuredbyobserving
the prediction difference between PandP∗and computing the
fairnessofthesechanges using the fairnessmetrics from ( 1).
Suppose,thepredictionsmadebythetwopipelinesare ˆY(P)and
ˆY(P∗).Let,Ibetheimpactsetfor Sk,whichdenotestheprediction
paritybetween ˆY(P)andˆY(P∗)suchthatfor ithdatainstance,if
ˆYi(P)=ˆYi(P∗), thenIi=0, otherwise 1. By causality, the fairness
ofpreprocessingstage(denotedby SF)iscalculatedbasedon [ˆY(P),
ˆY(P∗)]I=1withrespecttoafairnessmetric M,whichisshownin
(2a). We noticed that a few preprocessing stages, specifically the
encoderscannotberemovedwithoutreplacingwithanalternative
stage. For such situations, we have defined the fairness of Skwith
reference to anotherstage S′
k,denotedby SF(Sk|S′
k)in(2b).
Zelayaalsousedthesimilarmethodforquantifyingthe effect
of a preprocessing stage with a goal of computing volatility of a
stage[73].Volatility quantifieshow muchimpact apreprocessing
stagehasontheoutcomebycomputingtheprobabilityofprediction
changes.However,itdoesnotcapturethefairnessofthestage,since
a stage can cause high change in the prediction by maintaining
the predictions fair. Next in ğ 3.3.2, we have extended our causality
based formulation of (2a)for each fairness metric in (1)to capture
thefairnessimpactofeachpreprocessingstage.Similarto[ 27],the
benefitofthisformulationis,themeasuresdonotrequireanoracle,
sincetheprediction equivalenceofpipelines PandP∗serves the
goal of evaluating fairness of the stage. Note that the rest of the
definitionsinğ 3.3.2are independent of ( 2a) and(2b).
I=/braceleftbigg0ifˆYi(P)=ˆYi(P∗)
1otherwise, for alli∈ {1. ..n}
SF(Sk)=M[ˆY(P),ˆY(P∗)]I=1whereP∗=P \Sk (2a)
SF(Sk|S′
k)=M[ˆY(P),ˆY(P∗)]I=1whereP∗=(P \Sk)∪S′
k(2b)
3.3.2 Fairness MetircsforPreprocessingStage. We have leveraged
the definition of metrics SPD, EOD, AOD, and ERD from (1)to
capturethe stage-specific fairnessof Sk.Essentially,thenewmetrics
will identify the disparities between ˆYi(P)andˆYi(P∗)and use
correspondingfairnesscriteriatomeasurehowmuch Skfavorsa
specific group withrespectto othergroup(s).
Suppose,among ndata instances, nuare from the unprivileged
group and npfrom the privileged group. SFCSPDcomputes how
manyofthedatainstanceshavebeenchangedfromunfavorableto
favorable after applying the stage Sk. To do that we count changes
in both directions (unfavorable to favorable and favorable to un-
favorable),andtakethedifference.Thesignof SFCSPDpreserves
the direction of changes. Finally, the metric SFSPDis computed
by taking the difference of rates (SFRSPD) between unprivileged
and privileged groups. Note that the metric captures fairness by
measuringthedifferenceoffavorablechangeratesbetweengroups.
Simply counting the mismatches between ˆYi(P)andˆYi(P∗)could
provide degree of changes in SFCSPDbut would not capture fair-
ness. Furthermore, computing favorable changes to both groups
separately and evaluating the disparity between them captures
fairnessaccording to the originaldefinitionof SPD.SFCiSPD= 
1ifˆYi(P)=1andˆYi(P∗)=0
−1ifˆYi(P)=0andˆYi(P∗)=1
0otherwise
SFCSPD=n/summationdisplay.1
i=1SFCiSPD
SFRSPD(u)=SFCSFD(u)/nu,SFRSPD(p)=SFCSPD(p)/np
SFSPD=SFRSPD(u)−SFRSPD(p) (3)
Similarly, SFEODis defined using the following equation. In this
case,onlythetrue-positivechangesareconsideredassuggestedby
the definitionofEODfrom ( 1).
SFCiEOD= 
1ifYi=ˆYi(P)=1andˆYi(P∗)=0
−1ifˆYi(P)=0andYi=ˆYi(P∗)=1
0otherwise
SFCEOD=n/summationdisplay.1
i=1SFCiEOD
SFREOD(u)=SFCEOD(u)/nu,Y=1,SFREOD(p)=SFCEOD(p)/np,Y=1
SFEOD=SFREOD(u)−SFREOD(p) (4)
SinceAOD computes theaverage oftrue positive (TP)rate and
falsepositive(FP)rate,firstthechangesetforTPandFPpredictions
is computed. Then averaging the probability of changes for TP and
FP, the change rates are computed for both groups. Finally, SFAOD
iscalculatedbytakingthedifferenceofratesbetweenprivileged
andunprivilegedgroups.
SFCiTP= 
1ifYi=ˆYi(P)=1andˆYi(P∗)=0
−1ifˆYi(P)=0andYi=ˆYi(P∗)=1
0otherwise
SFCiFP= 
1ifˆYi(P)=1andYi=ˆYi(P∗)=0
−1ifYi=ˆYi(P)=0andˆYi(P∗)=1
0otherwise
SFCTP=n/summationdisplay.1
i=1SFCiTP,SFCFP=n/summationdisplay.1
i=1SFCiFP
SFRAOD(u)=(1/2){SFCTP(u)/nu,Y=1+SFCFP(u)/nu,Y=0}
SFRAOD(p)=(1/2){SFCTP(p)/np,Y=1+SFCFP(p)/np,Y=0}
SFAOD=SFRAOD(u)−SFRAOD(p) (5)
Finally,SFERDis computed using the change of count in both
false positives (FP) and false negatives (FN) as mentioned in the
definitionofERD in( 1).
SFCiFN= 
1ifˆYi(P)=0andYi=ˆYi(P∗)=1
−1ifYi=ˆYi(P)=1andˆYi(P∗)=0
0otherwise
SFCFN=n/summationdisplay.1
i=1SFCiFN
SFRERR(u)=SFCFP(u)/nu,Y=0+SFCFN(u)/nu,Y=1
SFRERR(p)=SFCFP(p)/np,Y=0+SFCFN(p)/np,Y=1
SFERR=SFRERR(u)−SFRERR(p) (6)
Thusfar,wehavefourfairnessmetrics( SFSPD,SFEOD,SFAOD,
andSFERD) to measure the fairness of the stage. In general, the
ratescomputed by each metric ( SFR)follow thesamerangeof the
originalmetrics [-1,1].Therefore,theabove metricshave arange
[-2, 2]. Positive values indicate bias towards unprivileged group,
negative values indicate bias towards privileged group, and values
very close to 0indicatefair preprocessing stage.
984Fair Preprocessing: TowardsUnderstandingCompositionalFairness of DataTransformersin MachineLearning Pipeline ESEC/FSE ’21, August 23–28, 2021,Athens,Greece
4 EVALUATION
In this section, we describe the benchmark dataset and pipelines
thatweusedforevaluation.Thenwepresenttheexperimentdesign
andresults for answeringthe researchquestions.
4.1 Benchmark
We collected ML pipelines used in prior studies for fairness evalua-
tion.First,BiswasandRajancollectedabenchmarkof40MLmodels
collected from Kaggle that operate on 5 different datasets e.g., Ger-
man Credit [34],Adult Census [45],Bank Marketing [49],Home
Credit[39] andTitanic[40]. However, the authors did not study
the fairness at the component level, rather the ultimate fairness of
theclassifierse.g., RandomForest ,DecisionTree ,etc.Werevisited
theseKagglekernelsandcollectedthepreprocessingstagesused
inthepipelines.Wenoticedthat HomeCredit dataset[39]inthis
benchmarkisnotunifiedliketheotherdatasets,distributedover
multiples CSV files, and the models under this dataset do not oper-
ateonthesamedatafiles.Hence,thesemodels(8outof40)arenot
suitable for comparing fairnessofdata preprocessing stages.
Second,wecollectedthepipelinesprovidedby Yangetal .[71].
Theauthorsreleased3pipelinesontwodifferentdatasets- Adult
CensusandCompas. Third,Zelaya[73] studied the volatility of
the preprocessing stages using two pipelines on a fairness dataset
i.e.,German Credit . We included these pipelines in our benchmark.
Thus,wecreatedabenchmarkof37MLpipelinesthatoperateon
fivedatasets.Thepipelineswiththestagesineachdatasetcategory
andtheirperformancesareshowninTable 1.Belowwepresenta
briefdescriptionofthe datasets andassociatedtasks.
Table 1: The preprocessing stages and performance mea-
sures(accuracy,f1 score)ofthepipelines inthebenchmark
German Stages AccF1AdultStages AccF1
GC1 PCA,SB 0.640.76AC1SS,LE 0.850.66
GC2 SMOTE,SS 0.740.81AC2MV 0.850.68
GC3 PCA 0.730.83AC3Custom(f) 0.870.66
GC4 LE,SS 0.730.82AC4PCA,SS 0.850.66
GC5 SS 0.740.83AC5LE 0.870.71
GC6 PCA,SS,LE 0.730.83AC6Custom(f),Custom(c) 0.850.65
GC7 PCA,SB 0.660.77AC7PCA,SS,Custom(f) 0.780.51
GC8 SS 0.720.81AC8SS,Custom(f),Stratify 0.850.67
GC9 SMOTE 0.670.77AC9SS 0.810.61
GC10 Usamp 0.60.81ACP10Impute 0.810.62
Bank Stages AccF1TitanicStages AccF1
BM1 Custom, LE,SS 0.90.56TT1MV, Custom(f),Encode 0.770.83
BM2 LE 0.910.61TT2MV, Custom(f) 0.780.72
BM3 LE,SS,Custom(f) 0.90.48TT3Custom(f),Impute 0.80.72
BM4 SS 0.890.33TT4Custom(f),Impute, RFECV 0.810.73
BM5 SS 0.88 0.23 TT5Custom(f) 0.830.76
BM6 FS,Stratify, SS 0.910.58TT6Custom(f) 0.820.74
BM7 FS 0.910.6TT7SS,LE,Custom(f) 0.820.77
BM8 Stratify 0.90.56TT8Custom(f) 0.830.76
Compas Stages AccF1
CP1 Filter, Impute1, Encode, Impute2, Kbins, Binarize 0.970.97
SB: SelectBest, SS: StandardScaler,LE*: LabelEncoder, Usamp:Undersampling, Cus-
tom(f/c):Custom featureengineeringor cleaning,FS:Featureselection,MV:Missing
valueprocessing,RFECV:Featureselectionmethod
* Fairness is measured with respect to areferencestage.
German Credit. The datasetcontains 1000 data instances and 20
featuresofindividualswhotakecreditfromabank[ 34].Thetarget
isto classifywhether the person has agood/badcreditrisk.Adult Census. The dataset is extracted by Becker [ 45] from 1994
census of United States. It contains 32,561 data instances and 12
features including demographic dataof individuals. The task is to
predict whether the person earns over 50Kinayear.
BankMarketing. This dataset contains a bank’s marketing cam-
paigndataof41,188individualswith20features[ 49].Thegoalis
to classifywhether aclientwillsubscribe to aterm deposit.
Titanic.The dataset contains information about 891 passengers
ofTitanic[ 40].Thetaskistopredictthesurvivaloftheindividuals
onTitanic. The sensitive attribute of this dataset is sex.
Compas.Thedatasetcontainsdataof6,889criminaldefendantsin
Florida.Propublicausedthisdatasetandshowedthattherecidivism
predictionsoftwareusedinUScourtsdiscriminatesbetweenWhite
andnon-White[ 5].Thetaskistoclassifywhetherthedefendants
willre-offend where raceisconsideredas the sensitive attribute.
4.2 Experiment Design
Eachpipelineinthebenchmarkconsistsofoneormorepreprocess-
ing stages followed by the classifier. In this paper, our main goal is
to evaluate the fairness of different preprocessing stages using the
fairnessmetricsdescribedinğ 3.Thebenchmark,codeandresults
are releasedinthe replication package [ 11].
The experiment designfor evaluating the pipelinesis shownin
Figure2. First, for each pipeline, we identified the preprocessing
stages. For example, the pipeline in ğ 2.1contains six preprocessing
stages. To evaluate the fairness of a stage Skin a pipeline P, we
create an alternative pipeline P∗by removing the stage Sk. For
stages that can not be removed, we replaced Skwith a reference
stageS′
k. Among the preprocessing stages shown in Table 1, we
foundonlytheencoderscannotberemoved.Weexperimentedwith
all the encoders in Scikit-Learn library [ 61], i.e.,OneHotEncoder ,
LabelEncoder ,OrdinalEncoder ,andfoundthat OneHotEncoder
does not exhibit any bias. Therefore, we used OneHotEncoder as
the reference stagefor the encoders inour experiment.
Second,theoriginaldatasetissplitintotraining(70%)andtest
set(30%).Thentwocopiesoftrainingdataareusedtotrainpipeline
PandP∗. After training the classifiers, two models predict the
label for the same set of test data instances. Then, similar to the
experimentation of [ 73], for each prediction label we compare the
two predictions ˆYi(P)andˆYi(P∗)with the true prediction label Yi.
This comparison providesthenecessarydata to computethefour
fairness metrics. Similar to [ 10,26], for each stage in a pipeline,
we run this experiment ten times, and then report the mean and
standard deviation of the metrics, to avoid inconsistency of the
randomness in the ML classifiers. Finally, we followed the ML best
practices sothat noiseisnot introduced evaluatingthefairness of
preprocessingstages.Forexample,whileapplyingsometransforma-
tion, lack of data isolation might introduce noise in the evaluation,
e.g., when applying PCAon dataset, it is important to train the PCA
onlyusingthetrainingdata.Ifweusethewholedatasettotrainthe
PCAandtransformdata,theninformationfromthetest-setmight
leak. Third, since a stage operates on the data processed by the
preceding stage(s), there are interdependencies between them. We
alwaysmaintainedtheorderofthestageswhileremovingorreplac-
ingastage(ğ 5).Toobservefairnessofdatatransformerswithout
interdependencies,we appliedthemonvanilla pipelines (ğ 6).
985ESEC/FSE ’21, August 23–28, 2021,Athens,Greece SumonBiswas andHrideshRajan
Sp
Dataset
DSkSqPrediction
SpDtrain
SqPrediction
Compute 
Fairness Dtest
Train 
classifier
CTrain 
classifier
C
ModelModel
XtestYtestFairness of 
stage
Sk Train-test split
Figure 2:Experiment designto measure fairness ofpreprocessing stages inmachinelearning pipeline.
5 FAIRNESS OFPREPROCESSINGSTAGES
In this paper, we used a diverse set of metrics, to evaluate fairness
ofpreprocessingstages.WhiledevelopinganMLpipelines,ifthe
developerhasacomprehensiveideaofthefairnessofpreprocessing
stages,itwouldbeconvenienttobuildafairpipeline.Theevaluation
hasbeendoneon69preprocessingstagesin37MLpipelinesfrom5
datasetcategories.For Compasdataset,wefoundonepipeline(ğ 2.1).
Five out of six stages in this pipeline exhibit no bias, which has
beendiscussed later.Forother4datasetcategories, theevaluation
hasbeenshowninFigure 3.Inthissection,first,wediscusshow
we caninterpret themetrics.Second, we answerthefirstresearch
questionanddiscuss the findingsfrom our evaluation.
5.1 WhatDotheMetricsImply?
We investigated the fairness of the preprocessing stages using four
metrics:SFSPD,SFEOD,SFAOD, andSFERD. These metrics measure
thefairnessofthestagesbyusingtheexistingfairnesscriteria,e.g.,
SFSPDmeasures the fairness of a stage with respect to statistical
parity difference ( SPD) criteria. These fairness criteria evaluate
algorithmic fairness of ML pipelines [ 15,25,72]. The unfairness
characterized by these criteria is measured based on the predic-
tion disparities, although the root cause can be the training data
orthealgorithm(e.g.,datapreprocessing,classifier)itself.There-
fore, when an ML model is identified as unfair, it implies that in
the given predictive scenario, the outcome is biased. Similarly,
the metrics proposed in this paper measure algorithmic unfair-
ness caused by a specific preprocessing stage with respect to its
pipeline. For instance, in Figure 3, pipeline GC4 has two stages:
LabelEncoder andStandardScaler . The fairnessmetricssuggest
thatLabelEncoder is biased towards unprivileged group (positive
value), and StandardScaler is biased towards privileged group
(negative value). The stages for which the measures are very close
to zero,can be consideredas fair preprocessing.
Themetricscanprovidedifferentfairnesssignalsforacertain
stage.Forexample,inAC4, SFERDshowspositivefairness,whereas
the other metrics suggest negative fairness for both the stages
PCAandStandardScaler .Thisdisparityoccursbecausedifferent
metrics accounts for different fairness criteria. In this case, SFERD,
depends on the false positive and false negative rate difference. No
othermetricisconcernedaboutthefalsenegativeratedifference,
and hence SFERDprovides a different fairness signal than other
metrics.Inpractice,appropriatefairnesscriteriacanvarydepending
onthetask,usagescenario,orinvolvedstakeholders.Studysuggests
that developers need to be aware of different fairness indicators
tobuildfairerpipelines[ 26].Therefore,wedefinedandevaluated
fairnessofstageswithrespectto multiple metrics.5.2 FairnessAnalysis ofStages
The pipelines used both built-in algorithm imported from libraries
i.e.,datatransformers [14],as wellas custompreprocessingstages.
The stages found in each pipeline are shown in Table 1, and the
fairness measures of those stages are plotted in Figure 3. Although
theunfairnessexhibitedbyastageiswithrespecttothepipeline,
wefoundfairnesspatternsofsomestagesandinvestigatedthem
further.Ingeneral,ourfindingsshowthatthestageswhichchange
the underlying datadistributionsignificantly, ormodifyminority
data are responsible for increasing biasinthe pipelines.
Finding 1 :Data filtering and missing value removal change the
datadistribution and hence introduce biasin ML pipeline.
Most of the real-world datasets contain missing values (MV) for
severalreasonssuchasdatacreationerrors,not-applicable(N/A)
attributes, incomplete data collection, etc. In our benchmark, Adult
CensusandTitaniccontain MV that required further processing in
thepipeline.7.4%rowsin AdultCensus and20.2%rowsin Titanic
haveatleastonemissingfeatureinthedataset.Thepipelineseither
remove the rows with MV or apply certain imputation [ 62] tech-
nique that replaces the MV with mean, median or most frequently
occurredvalues.RemovalofrowswithMVcansignificantlychange
the data distribution, which introduces bias in the pipeline. For ex-
ample,bothTT1andTT2removeddataitemswithMVbyapplying
df.dropna() method,whichintroducesbiasintheprediction(Fig-
ure3). Research has shown that MV are not uniformly distributed
over all groups and data items from minority groups often contain
more MV[ 22]. Ifthose dataitemsare entirely removed,therepre-
sentation of minority groups in the dataset becomes scarce. On the
otherhand,TT3appliedmean-imputationandTT4appliedboth
median- and mode-imputation using df.fillna() , which exhibits
fairnesscomparedtodataremoval.Whileourfindingssuggestthat
removing data items with MV introduces bias, the most popular
fairness tools AIF 360 [ 8], Aequitas [ 58], Themis-ML [ 6] ignore
these data items and remove entire row/column. Our evaluation
strategyconfirmsthatthetoolscanintegrateexistingimputation
methods [ 62] in the pipeline and allow users to choose appropri-
ate ones. Additionally, more research is needed to understand and
develop imputation techniques that are fairnessaware.
Finding2 :Newfeaturegenerationorfeaturetransformationcan
havelargeimpact onfairness.
We found that most of the feature engineering stages, especially
thecustomtransformationsexhibitbiasinthepipeline.Forexample,
the pipelines in Titanicdataset used custom feature engineering,
sincethedatasetcontainscompositefeatureswhichmayprovide
additional information about the individuals. For instance, TT8
operatesonthefeature nametocreate a newfeature titlee.g., Mr,
986Fair Preprocessing: TowardsUnderstandingCompositionalFairness of DataTransformersin MachineLearning Pipeline ESEC/FSE ’21, August 23–28, 2021,Athens,Greece
PCASB SMOTE SS PCALESS SS PCASSLEPCASB SS SMOTE Usamp
GC1 GC2 GC3 GC4 GC5 GC6 GC7 GC8 GC9 GC10German CreditSF_SPD SF_EOD SF_AOD SF_ERD
Custom LESSLE LE SSCT(f) SS SS FS Stratify SS FS Stratify
BM1 BM2 BM3 BM4 BM5 BM6 BM7 BM8Bank Marketing
-0.3-0.2-0.100.10.20.3
MVCT(f)Encode MVCT(f)ImputeCT(f) CT(f) ImputeRFECVCT(f) CT(f) SSLECT(f) CT(f)
TT1 TT2 TT3 TT4 TT5 TT6 TT7 TT8Titanic-0.3-0.2-0.100.10.20.3
SSLEMVCT(f)PCASSLECT(f) CT(c) PCASSCT(f)SSCT(f)Stratify SS Impute
AC1 AC2 AC3 AC4 AC5 AC6 AC7 AC8 AC9 AC10Adult Census
Figure3:Fairnessmeasures(Y-axis)ofpreprocessingstagesinpipelines(X-axis).Greylinesabovebarsindicatestandarderror.
Mrs,Dr,etc.Thistransformationaimsatbetterpredictionofthe
survivalofpassengersbyextractingthesocialstatus,butcreates
high biasbetween maleandfemale.
InAdultCensus ,thefeature eductionofindividualscontainvalues
suchaspreschool,10th,1st-4th,prof-school ,etc.,whichhavebeen
replacedbybroadcategoriessuch Dropout,HighGrad ,Mastersin
thepipelineAC7.Inaddition,insteadofusing ageascontinuous
value,thefeaturehasbeendiscretizedinto nnumberofbins.Inboth
cases,theoriginaldatavalueshavebeenmodified,whichhascaused
unfairnessinthepipeline.Nevertheless,somepipelines(AC3,AC8)
have custom feature transformations that are fair. Previous studies
showed that certain features contribute more to the predictive
quality of the model [ 28,56]. Feature importance in prediction
and corelation of features with the sensitive attribute also led to
bias detection [ 16,31] in ML models. However, does creating new
features (by removing certain semantics) from a potentially biased
featureincreasethefairness,isanopenquestion.Ourmethodto
quantify the fairness of such changes can guide further research in
this direction.
Finding 3 :Encodingtechniques shouldbe chosencautioslybased
ontheclassifier.
Two most used encoding techniques for converting categorical
featuretonumericalfeatureare OneHotEncoder andLabelEncoder .
OneHotEncoder createsnnewcolumnsbyreplacingonecolumnfor
eachofthe ncategories. LabelEncoder doesnotincreasethenum-
berofthecolumns,andgiveseachcategoryanintegerlabelbetween
0and(n−1).Inourevaluation,wefoundthat LabelEncoder intro-
ducesbiasin GermanCredit andTitanicdatasetbut OneHotEncoder
doesnotchangefairness.Since LabelEncoder imposesasequen-
tialorderbetweenthecategories,itmightcreatealinearrelation
with the target value, and hence have an impact on the classifier to
change fairness. For example, pipelines TT7 creates a new feature
calledFamilybased on the surname of the person. This feature has
alargenumberofuniquecategories(667uniqueonesin891datain-
stances). Therefore, the non-sparse representation in LabelEncoder
addsadditionalweighttothefeature,whichis causingunfairness
in TT7. Developers might avoid OneHotEncoder because it suffers
from the curse of dimensionality and the ordinal relation of data is
lost.Inthatcase,developersshouldbeawareofthefairnessimpact
oftheencoder.OnesolutionmightbeusingPCAfordimensionality
reduction, whichhas been done inGC7.Finding4 :Thevariabilityoffairnessofpreprocessingstagesdepend
onthedatasetsizeand overall predictionrateofthe pipelines.
Wehaveplottedthestandarderrorofthemetricsaserrorbars
in Figure 3. Firstly, it shows that the metrics in German Credit and
Titanicdataset are more unstable. The reason is that the size of
these two datasets is less than the other three datasets. German
Creditdataset has 1000 instances, and Titanichas 891 instances.
Adult Census andBank Marketing dataset have more than 30K
instances.Ifthesamplesizeislarge,datadistributiontendstobe
similar even after taking a random train-test split [ 26]. However,
when the dataset size is smaller, the distribution is changed among
differenttrain-testsplitting.Furthermore,wehavefoundthat SFERD
is more unstable than other metrics. SFERDdepends on the change
offalsepositiveandfalsenegativerates.However,inmostcases,
the pipelines are optimized for accuracy and precision, since these
aresomebestperformingonescollectedfromKaggle.Therefore,
before deploying preprocessing stages, it would be desirable to test
the stability ofover multiples executions.
Finding 5 :The unfairness of a preprocessing stage can be domi-
nated bydatasetortheclassifier used in the pipeline.
For theCompasdataset, we evaluated the six stages shown in
ğ2.1. All the stages exhibited data filtering show bias. The data
filtering also showed bias close to zero (less than .005) with respect
toallthemetrics.AlthoughYang etal.[71]arguedthatthispipeline
filters data in different proportions from maleandfemalegroup,
our evaluation confirms it does not cause unfairness. This pipeline
has been used by Propublica [ 5] to show the bias in the prediction.
Therefore,itisunderstandablethattheydidnotemployanypre-
processing that introduces bias in the pipelines. Other than that,
almostallthepreprocessingstagesin BankMarketing pipelinesalso
exhibit very little unfairness, which suggest that the preprocessing
onthis dataset are fair ingeneral.
Afewstagesshowdifferentbehaviorwhentheyareusedincom-
position with different classifiers. For example, StandardScaler
has been applied on both GC6 and GC8. While GC6 employs a
RandomForest classifier,GC8 uses K-Neighbors classifier.Wehave
observedtheoppositefairnessmeasuresfor StandardScaler inthese
twopipelines. Therefore, fairnesscanbe dominatedby theunder-
lyingpropertiesofdataorthepipelinewhereitisapplied.Wehave
furtherinvestigatedthisphenomenonbyapplyingtransformerson
differentclassifiers inthe nextsection.
987ESEC/FSE ’21, August 23–28, 2021,Athens,Greece SumonBiswas andHrideshRajan
-0.4-0.20.00.20.4
PCA
SB
SMOTE
SS
PCA
LE
SSSS
PCA
SS
LE
PCA
SBSS
SMOTE
Usamp
SS
LE
MV
CT(f)
PCA
SS
LE
CT(f)
CT(c)
PCA
SS
CT(f)
SS
CT(f)
Stratify
SS
Impute
Custom
LE
SS
LELE
SS
CT(f)
SSSS
FS
Stratify
SS
FS
Stratify
MV
CT(f)
Encode
MV
CT(f)
Impute
CT(f)CT(f)
Impute
RFECV
CT(f)CT(f)
SS
LE
CT(f)CT(f)SF_SPD SF_EOD SF_AOD SF_ERD Accuracy F1 score
German Credit Adult Census Bank Marketing Titanic
GC1GC2GC3GC4GC5GC6GC7GC8GC9GC10AC1AC2AC3AC4AC5AC6AC7 AC8AC9AC10BM1BM2BM3BM4BM5BM6BM7BM8TT1 TT2TT3TT4TT5 TT7TT6 TT8
Figure 4: Performance changes (green) are plotted with fairness (red) of the preprocessing stages. A positive or negative per-
formance change indicates performanceincrease ordecrease respectively,after applying the stage.
5.3 Fairness-PerformanceTradeoff
Inthissection,weinvestigatedthefairness-performancetradeoff
for the preprocessing stages. The original performances of each
pipeline have been reported in Table 1. To investigate fairness of a
stage, we created pipeline P∗by removing the stage from original
pipelineP(ğ2).Tounderstandfairness-performancetradeoff,we
evaluatedperformance(bothaccuracyandf1score)of P∗andPin
the same experimental setup. Then we computed the performance
differencetoobservetheimpactofthestageonperformance.For
example,Acc(P)−Acc(P∗)givestheaccuracyincrease(ordecrease,
if negative) after applying a stage. We plotted the performance
impacts ofthe stageswiththeirfairnessmeasures inFigure 4.
First, many preprocessingstages have negligible performance
impact. In Figure 4, 19 out 63 stages exhibits accuracy and f1 score
changeintherange[-0.005,0.005],whichindicatesperformance
change≤0.05%. We found that in all of these cases, except AC9
andAC10,thepreprocessingstagesarefairwithaverysmallde-
gree ofbias. Second, tradeoff between performance andfairness is
observed forthe stages which improve performance.17 stagesim-
prove accuracy or f1 score more than 0.05%, which further exhibits
moderate to high degree of bias. Overall, the most biased stages
- TT7(LE), TT8(CT), TT4(CT), TT1(MV), GC8(SS), are improving
performance.Thisstage-specifictradeoffisalignedwiththeoverall
performance-fairnesstradeoff discussedinpriorwork[ 10,17,26],
which can be compared quantitatively by the work of Hort et al .
[36].Third,wefoundthatsomestagesdecreasetheperformance,
either accuracy or f1 score.Surprisingly, mostof these stages also
exhibit high degree of bias. For instance, the most performance-
decreasing stages - BM4(SS), AC7(PCA), GC10 (Undersampling),
are showing more bias. Our fairness evaluation would facilitate
developers to identifyandremove such stagesinthe pipeline.
6 FAIRDATA TRANSFORMERS
In ğ5, we found that many data preprocessing stages are biased.
Many bias mitigation techniques applied in preprocessing stage
have been shown successful [ 25,41]. If we process data with ap-
propriatetransformer,thenitmightbepossibletoavoidbiasand
mitigateinherentbiasindataorclassifier.Evenifadatatransformer
isbiasedtowardsaspecificgroup,itcouldbeusefultomitigatebias
if original data or model exhibits bias towardsthe opposite group.
To that end, we want to investigate the fairness pattern of the data
transformers. However, in our evaluation (Figure 3), some trans-
formershavebeenusedonlyinspecificsituationse.g., SMOTEhasTable2:Transformerscollectedfrompipelinesandlibraries
Categories Stages Transformers
MV processing Imputation SimpleImputer, IterativeImputer
Categorical en-
codingEncoder Binarizer, KBinsDiscretizer, LabelBina-
rizer, LabelEncoder, OneHotEncoder
StandardizationScaling StandardScaler, MaxAbsScaler, MinMaxS-
caler, RobustScaler
Normalization l1-normalizer, l2-normalizer
Feature
engineeringNon-lineartransformation QuantileTransformer, PowerTransformer
Polynomialfeaturegener-
ationPCA, SparsePCA, MiniBatchSparsePCA,
KernelPCA
Featureselection SelectKBest, SelectFpr, SelectPercentile
SamplingOversampling SMOTE
Undersampling AllKNN
Stratification Random, Stratified
beenonlyappliedon GermanCredit dataset.Whatisthefairnessof
thistransformerwhenusedonotherdatasetsandclassifier?Inthis
section,wesetupexperimentstoevaluatethefairnessofcommonly
useddata transformersondifferentdatasets andclassifiers.
First,wecollectedtheclassifiersusedineachdatasetcategory
from the benchmark. Then, for each dataset, we created a set of
vanillapipelines.Avanillapipelineisaclassificationpipelinewhich
contain only one classifier. Second, we found a few categories of
preprocessing stages from our benchmark shown in Table 2. For
eachtransformerusedineachstage,wecollectedthealternative
transformersfromcorrespondinglibrary.Forexample,inourbench-
mark,StandardScaler fromScikit-Learnlibraryhasbeenusedfor
scalingdatadistributioninmanypipelines.Wecollectedotherstan-
dardizingalgorithmsavailableinScikit-Learn.Wefoundthatbe-
sidesStandardScaler ,Scikit-Learnalsoprovides MaxAbsScaler ,
MinMaxScaler , andNormalizer standardize data [ 61]. Similarly, a
data oversampling technique SMOTEhas been used in the bench-
mark, we collected another undersamplingtechnique ALLKNNand
a combination of over- and undersampling sampling technique
SMOTENNfromIMBLearnlibrary[ 48].Third,ineachofthevanilla
pipelines,weappliedthetransformersandevaluatedfairnessusing
themethodusedinğ 4.2withrespecttofourmetrics.Wefoundthat
pipelinesunder Titanicusescustomtransformation,andmostof
the built-in transformers are not appropriate for this dataset. So,
tobeabletomakethecomparisonconsistent,weconductedthis
evaluation on four datasets: German Credit ,Adult Census ,Bank
Marketing ,Compas. Finally, we did not use transformers for impu-
tationandencoderstages.Encodingtransformers(LabelEncoder,
OneHotEncoder),havebeenappliedonmostofthepipelinesand
988Fair Preprocessing: TowardsUnderstandingCompositionalFairness of DataTransformersin MachineLearning Pipeline ESEC/FSE ’21, August 23–28, 2021,Athens,Greece
-0.3-0.2-0.100.10.2
D R X S K D R X S K D R X S K D R X S K D R X S K D R X S K D R X S K D R X S K D R X S K D R X S K D R X S K D R X S K D R X S K D R X S K D R X S K D R X S K
SMOTE Usamp Combined Standard MinMax MaxAbs Robust Normalizer QuantileTrans PowerTrans PCA SparsePCA KernelPCA SelectBest SelectFpr SelectPercentile
Sampling Scaling Non-linear transfromer PCA Feature Selection
-0.3-0.2-0.100.10.2
D R X S K D R X S K D R X S K D R X S K D R X S K D R X S K D R X S K D R X S K D R X S K D R X S K D R X S K D R X S K D R X S K D R X S K D R X S K D R X S K
SMOTE Usamp Combined Standard MinMax MaxAbs Robust Normalizer QuantileTrans PowerTrans PCA SparsePCA KernelPCA SelectBest SelectFpr SelectPercentile
Sampling Scaling Non-linear transfromer PCA Feature Selection
-0.3-0.2-0.100.10.2
D R X S K D R X S K D R X S K D R X S K D R X S K D R X S K D R X S K D R X S K D R X S K D R X S K D R X S K D R X S K D R X S K D R X S K D R X S K D R X S K
SMOTE Usamp Combined Standard MinMax MaxAbs Robust Normalizer QuantileTrans PowerTrans PCA SparsePCA KernelPCA SelectBest SelectFpr SelectPercentile
Sampling Scaling Non-linear transfromer PCA Feature Selection
-0.3-0.2-0.100.1
D R X S K D R X S K D R X S K D R X S K D R X S K D R X S K D R X S K D R X S K D R X S K D R X S K D R X S K D R X S K D R X S K D R X S K D R X S K D R X S K
SMOTE Usamp Combined Standard MinMax MaxAbs Robust Normalizer QuantileTrans PowerTrans PCA SparsePCA KernelPCA SelectBest SelectFpr SelectPercentile
Sampling Scaling Non-linear transfromer PCA Feature SelectionSF_SPD SF_EOD SF_AOD SF_ERDGerman Credit
Adult Census
Bank Marketing
Compas
Figure5:Fairnessoftransformersonclassifiers,D: DecisionTree ,R:RandomForest ,X:XGBoost,S:SupportVector ,K:KNeighbors
their behavior has been understood. The fairness measures of each
transformerondifferentclassifiers have been plottedinFigure 5.
Fairnessamongthedatasetsfollowsasimilarpattern.Thisfur-
ther confirms that the unfairness is rooted in data. The Compas
dataset shows the least bias. Although racial discrimination has
been reported for this dataset [ 5], this is a more curated dataset
than the other three. By looking at the overall trend of fairness,
we observe that sampling techniques have the most biased impact
on prediction. Otherthan that feature selectiontransformers have
more impact thanotherones.
Finding 6 :Among all the transformers, applying sampling tech-
nique exhibits most unfairness.
Sampling techniques are often used in ML tasks when dataset
isclass-imbalanced.Unliketheothertransformers,samplingtech-
niques make horizontal transformation to the training data. The
oversamplingtechnique SMOTEcreates newdatainstances forthe
minority class by choosing the nearest data points in the feature
space.Undersamplingtechniquesbalancedatasetbyremovingdata
items from majority class. Although balancing dataset has been
showntoincreasefairness[ 22],ourevaluationsuggestthatinthree
outoffourdatasets,itincreasesbias.
FromFigure 5, we can see thatsampling techniques exhibitthe
mostunfairness.In GermanCredit dataset,differentclassifierreacts
differentlywhensamplingisdone. DecisionTree classifierexhibits
mostunfairnessforbothoversamplingandundersamplingtowards
privilegedgroupi.e., male.Interestingly,thecombinationofover-
andundersamplingalsofailstoshowfairness.Furthermore,both
German Credit andBankMarketing pipelines exhibitbiastowardsunprivilegedgroup,whichmightbedesiredwhencomparedtobias
towardsprivileged.
Finding7 :Selecting subsetoffeatures oftenincrease unfairness.
Selecting thebest performing feature can give performance im-
provement of the pipeline. However, unfairness can be encoded in
specific features [ 31]. While selecting bestfeatures, somefeatures
which encodes unfairness, can dominate the outcome. Thus, many
classifiers in German Credit ,Adult Census , andBank Marketing
show unfairness because of reduced number of features, which
hasbeenalsoobservedby ZhangandHarman [76].Surprisingly,
SelectFpr exhibited very little or no bias compared to the other
featureselectionmethods.Adetailedinvestigationsuggeststhat
SelectBest andSelectPercentile select only the kmost con-
tributingfeatures.However, SelectFpr performsfalsepositiverate
test on each feature, and if it falls below a threshold, the feature is
removed [ 60]. Therefore, it does not apply harsh pruning, which
contributesto the fairnessof the prediction.
Finding 8 :In most of the pipelines, feature standardization and
non-linear transformers are fair transformers.
Thesetransformersmodifythemeanandvarianceofthedata
by applying linear or non-linear transformation. However, they do
not change the feature importance on the classifiers. Therefore,in
most of the cases, these transformers (especially, StandardScaler
andRobustScaler ) are fair. Some classifiers show bias after apply-
ing these transformers such as, KNC in Compas. The unfairness
exhibited by those pipelines are introduced by the classifiers, since
theseclassifiersshowsimilarbiaspatternforothertransformers
989ESEC/FSE ’21, August 23–28, 2021,Athens,Greece SumonBiswas andHrideshRajan
-0.2-0.100.1
SSLEMVCustom(f)PCASSLECustom(f)Custom(c)PCASSCustom(f)SSCustom(f)StratifySSImputeAC1 AC2 AC3 AC4 AC5 AC6 AC7 AC8 AC9 AC10
SPDEODAODERD
-0.2-0.100.1
V_SPD V_EOD V_AOD V_ERDGlobal fairness difference
Local fairness
Figure 6: Comparison of global fairness change and local
fairness for AdultCensus datasetpipelines.
as well. The scalers can impact the fairness significantly if there
are many outliers in data. That is why we see more bias for the
scalersin GermanCredit dataset.Therefore,althoughstandardizing
transformers are fair in general, they can be biased in composition
withspecific classifier ordata property.
7 FAIRNESS COMPOSITION OFSTAGES
From our evaluation, we found that many data transformers have
fairnessimpactonMLpipeline.Inthissection,wecomparethe local
fairness(fairness measures of preprocessing stages) with the global
fairness(fairness measures of whole pipeline). First, we answer
whether the local fairness composes in the global fairness. Second,
we investigateif we canleverage thecompositionto mitigatebias
bychoosing appropriate transformers.
7.1 Composition ofLocalandGlobalFairness
Weevaluatedtheglobalfairnessof AdultCensus pipelines(Table 1)
using the four existing metrics from (1). We calculated the fairness
differenceofthesepipelinesbeforeandafterapplyingtheprepro-
cessingstages.Additionally,wehaveevaluatedthestage-specific
fairness metrics. Both the local fairness and difference in global
fairnessofthosepipelines have been plottedinFigure 6.
We can see that local and global fairness follow the sametrend
inmostofthepipelines.Thisconfirmsthatlocalfairnessisdirectly
contributing to the global fairness. However, the global fairness is
computedbasedontheoverallchangeintheprediction,whereas
the local fairness considers the predictions for only those data
instances which have been altered after applying a transformer
(3).Forexample,inFigure 6,forsomepipelines(e.g.,AC9,AC10),
global and local fairness exhibit different trends. In these cases,
the overall classification rate difference is not similar to the rate
difference of altered labels. This means that the stages changed the
labelssuchthatitshowsbiastowardsprivileged.Butwhenthose
changes in the labels are considered in addition to all the labels
(global fairness), the bias difference could not capture the actual
impactofthatstage.Wehaveverifiedthisobservationbymanually
inspectingthealteredpredictionlabels.Thus,wecanconcludethat
the local fairness composes to the global fairness. Specifically, if a
preprocessing stage shows bias for privileged group, it pulls the
globalfairnesstowardsthefairnessdirectionofprivilegedgroup.
However, only observing the global fairness difference, we can not
measure the fairnessofagiven stageortransformer.7.2 Bias Mitigation UsingAppropriate
Transformers
For a given transformer in an ML pipeline, a downstream trans-
formeroperatesondataalreadyprocessedbythegiventransformer
and anupstreamtransformer isappliedbefore thegivenone. Since
thefairnessofapreprocessingstagecomposestotheglobalfairness,
can we choosea downstream transformer to mitigatebias inML
pipeline?Inthissection,weempiricallyshowthattheglobalun-
fairness can be mitigated by choosing the appropriate downstream
transformer.
-0.100.1
Usamp SSMM MA RONOQTPTK-Neighbors Classifier
SPDEODAODERD-0.100.1XGB Classifier
Figure 7: Global fairness after applying the upstream trans-
former(left),andafterapplyingboththeupstreamandone
downstreamtransformer(right). Usamp: undersampling.
Consider the use-case of classification task on German Credit
datasetwithdifferentclassifierssimilartoFigure 5.Suppose,the
original pipeline is constructed using undersampling technique.
Sincethispipelineexhibitsbias,asshowninFigure 5,canwechoose
adownstreamdatastandardizingtransformerthatmitigatesthat
bias?Inthisusecase,undersamplingistheupstreamtransformer,
and any standardizing transformer is the downstream transformer.
We showed the evaluation for XGB classifier and KNC classifier,
since these two exhibits most bias when the upstream transformer
wasappliedinğ 6.Weplottedtheglobalfairnessafterapplyingonly
theupstreamtransformerintheleftofFigure 7.Wealsoreported
thelocalfairnessofthestandardizingtransformersinTable 3.Now,
sinceundersamplingmethodexhibitsbiastowardsprivilegedgroup
forXGB,welookforthetransformerthatisbiasedtowardsprivi-
legedgroup.InFigure 7,amongothertransformers, Normalizer is
the mostsuccessful to mitigatebiasofthe upstream transformer.
Similarly, for KNC, the upstream operator exhibits bias towards
privilegedgroup.FromTable 3,wecanseethat MinMaxScaler is
themostbiasedtransformertowardstheoppositedirection.Asa
result,applying MinMaxScaler mitigatesbiasthemost.Notethat
theotherdownstreamtransformersalsofollowthefairnesscom-
positionwithitsupstreamtransformer.Therefore,bymeasuring
fairnessofthepreprocessingstages,developerswouldbeableto
instrument the biasedtransformersandbuildfair ML pipelines.
8 DISCUSSION
We took the first step to understand the fairness of components in
MLpipelines.Ourmethodhelpstoprovidecausalityinsoftwareand
reasonaboutbehaviorof components based ontheimpactonout-
come.Thismethodcanbeextendedfurthertoevaluatethefairness
ofothersoftwaremodules[ 52]inMLpipelineandlocalizefaults
[70].Moreover,wefoundmostofthestagesexhibitedbias,toalow
990Fair Preprocessing: TowardsUnderstandingCompositionalFairness of DataTransformersin MachineLearning Pipeline ESEC/FSE ’21, August 23–28, 2021,Athens,Greece
Table3:Localfairnessofstagesasdownstreamtransformer.
Model Stage SF_SPD SF_EOD SF_AOD SF_ERD
XGBSS 0.001 0.021 -0.016 -0.073
MM -0.006 0.031 -0.029 -0.12
MA -0.036 0.005 -0.059 -0.128
RO 0.051 0.082 0.025 -0.113
NO -0.014 -0.056 0.012 0.135
QT 0.011 0.062 -0.02 -0.165
PT -0.029 0.011 -0.055 -0.132
KNCSS 0.035 0.025 0.055 0.059
MM 0.095 0.12 0.096 -0.05
MA 0.079 0.114 0.075 -0.078
RO 0.052 0.074 0.056 -0.036
NO 0.045 0.010 0.077 0.134
QT 0.077 0.104 0.078 -0.052
PT 0.035 0.032 0.050 0.036
SS:StandardScaler,MM:MinMaxScaler,MA:MaxAbsScaler,RO:RobustScaler,
NO:Normalizer, QT: QuantileTransformer, PT:PowerTransformer
orhigherdegree.Thefairnessmeasuresofdifferentcomponents
canbeleveragedtowardsfairness-awarepipelineoptimizationto
satisfy fairness constraints. For example, US Equal Employment
Commissionsuggestsselection-ratedifferencebetweengroupsless
than20%[ 68].Also,pipelineoptimizationtechniques,e.g.,TPOT
[51], Lara [46] can be potentially utilized for pipeline optimization.
Furthermore, research has been conducted to understand the
impact of preprocessing stages with respect to performance im-
provement [ 18,20,69]. This paper will open research directions
to develop preprocessingtechniques that improve performance by
keeping the fairness intact. We also reported a number of fairness
patternsof preprocessingstagesthat inducing bias in thepipeline
suchasmissingvalueprocessing,customfeaturegeneration,fea-
tureselection.Moreover,instrumentationofthestagescanmitigate
theinherentbiasoftheclassifiers.Itshowsopportunitiestobuild
automated tools for identifying fairness bugs in AI systems and
recommending fixes [ 37,38]. Finally, current fairness tools (e.g.,
AIF 360 [ 8], Aequitas [ 58]) can be augmented by incorporating
data preprocessing stages into the pipelines and letting users have
control over the data transformers and observe or mitigate bias.
Similarly, the libraries can provide API support to monitor fairness
ofthe transformers.
9 THREATS TO VALIDITY
Internal validity refers to whether the fairness measures used in
this paper actually captures the fairness of preprocessing stages.
To mitigate this threat, we used existing concepts and metrics to
build new set of metrics. Causality in software [ 53,54] has been
well-studied,andcausalreasoninginfairnesshasalsobeenpopular
[47,57,59,76], since it can provide explanation with respect to
changeintheoutcome.Besides,thismethoddonotrequireanoracle
becausethepredictionequivalencesprovidenecessaryinformation
to measure the impact of the intervention [ 27]. Furthermore, in
ğ7, we conducted experiments on local and global fairness to show
hownewmetrics composes inthe pipeline.
Externalvalidity isconcernedabouttheextentthefindingsof
thisstudycanbegeneralized.Toalleviatethisthreat,weconducted
experiments ona largenumberof pipelinevariations.We collected
thepipelines fromthree differentsources. Moreover,we collected
alternative transformers from the ML libraries for comparative
analysis. Finally, for the same dataset categories, we used multiple
classifiersandfairnessmetricssothatthefindingsarepersistent.10 RELATED WORKS
Fairness in ML Classification. The machine learning community
has defined different fairness criteria and proposed metrics to mea-
sure the fairness of classification tasks [ 15,19,22,23,25,32,54,
65,72,74].FollowingthemeasurementoffairnessinMLmodels,
many mitigation techniques have also been proposed to remove
bias [15,19,22,25,29,32,41ś43,55,72,75]. This body of work
mostlyconcentratesonthetheoreticalaspectoffairnessinasin-
gle classification task. Recently, software engineering community
hasalsofocusedonthefairnessinML,mostlyonfairnesstesting
[3,27,66,67]. These works propose methods to generate appropri-
ate test data inputs for the model and prediction on those inputs
characterizes fairness. Some research has been conducted to build
automated tools [ 2,64,67] and libraries [ 8] for fairness. In addi-
tion,empiricalstudieshavebeenconductedtocompare,contrast
between fairness aspects,interventions, tradeoffs, developerscon-
cerns,andhuman aspectsof fairness[ 10,26,33,35,77].
Fairness in Composition. Dwork and Ilvento argued that fairness
is dynamic in a multi-component environment [ 24]. They showed
that when multiple classifiers work in composition, even if the
classifiers are fair in isolation, the overall system is not necessarily
fair. Bower et al.discussed fairness in ML pipeline, where they
considered pipelineas sequence of multiple classification tasks
[12]. They also showed that when decisions of fair components
are compounded, the final decision might not be fair. For example,
whileinterviewingcandidatesintwostages,fairdecisionineach
stagemaynotguaranteeafairselection. D’Amour etal.studiedthe
dynamicsoffairnessinmulti-classificationenvironnementusing
simulation[ 21].Intheseresearch,fairnesscompositionisshown
over multiple tasks and the authors did not consider fairness of
components in single ML pipeline. We position our paper here
to study the impact of preprocessing stages in ML pipeline and
evaluate the fairnesscomposition.
11 CONCLUSION
Data preprocessing techniques are used in most of the machine
learningpipelinesincompositionwiththeclassifier.Studiesshowed
thatfairnessofmachinelearningpredictionsdependslargelyon
the data. In this paper, we investigated how the data preprocess-
ing stages affect fairness of classification tasks. We proposed the
causalmethodandleveragedexistingmetricstomeasurethefair-
nessofdatapreprocessingstages.Theresultsshowedthatmany
stages induce bias in the prediction. By observing fairness of these
data transformers, fairer ML pipelines can be built. In addition, we
showedthatexistingbiascanbemitigatedbyselectingappropri-
atetransformers.Wereleasedthepipelinebenchmark,code,and
results to make our techniques available for further usages. Future
research can be conductedtowards developing automated tools to
detectbiasinML pipeline stagesandinstrument that accordingly.
ACKNOWLEDGMENTS
This workwassupportedinpartbyUSNSF grantsCNS-15-13263,
CCF-19-34884, and Facebook Probability and Programming Award
(809725759507969).Wealsothankthereviewersfortheirinsightful
comments. All opinions are of the authors and do not reflect the
viewofsponsors.
991ESEC/FSE ’21, August 23–28, 2021,Athens,Greece SumonBiswas andHrideshRajan
REFERENCES
[1]MartínAbadi,PaulBarham,JianminChen,ZhifengChen,AndyDavis,Jeffrey
Dean,MatthieuDevin,SanjayGhemawat,GeoffreyIrving,MichaelIsard,etal .
2016. Tensorflow: A system for large-scale machine learning. In 12th USENIX
symposium on operating systems design and implementation (OSDI 16) . 265ś283.
[2]JuliusAdebayoandLalanaKagal.2016. Iterativeorthogonalfeatureprojection
for diagnosing bias in black-box models. arXiv preprint arXiv:1611.04967 (2016).
[3]Aniya Aggarwal, Pranay Lohia, Seema Nagar, Kuntal Dey, and Diptikalyan Saha.
2019. Blackboxfairnesstestingofmachinelearningmodels.In Proceedingsof
the201927thACMJointMeetingonEuropeanSoftwareEngineeringConference
and Symposiumonthe FoundationsofSoftwareEngineering . 625ś635.
[4]SaleemaAmershi,AndrewBegel,ChristianBird,RobertDeLine,HaraldGall,Ece
Kamar, Nachiappan Nagappan, Besmira Nushi, and Thomas Zimmermann. 2019.
Software engineering for machine learning: A case study. In 2019 IEEE/ACM 41st
InternationalConferenceonSoftwareEngineering:SoftwareEngineeringinPractice
(ICSE-SEIP) . IEEE,291ś300.
[5]JuliaAngwin,JeffLarson,SuryaMattu,andLaurenKirchner.2016. Machinebias:
There’s software used across the country to predict future criminals. And it’s
biased against blacks. ProPublica (2016).https://github.com/propublica/compas-
analysis
[6]NielsBantilan.2018. Themis-ml:Afairness-awaremachinelearninginterface
for end-to-end discrimination discovery and mitigation. Journal of Technology in
Human Services 36,1 (2018), 15ś30.
[7]Denis Baylor, Eric Breck, Heng-Tze Cheng, Noah Fiedel, Chuan Yu Foo, Zakaria
Haque, Salem Haykal, Mustafa Ispir, Vihan Jain, Levent Koc, et al .2017. TFX: A
tensorflow-basedproduction-scale machinelearning platform.In Proceedings of
the23rdACMSIGKDDInternationalConferenceonKnowledgeDiscoveryandData
Mining. 1387ś1395.
[8]Rachel KE Bellamy, Kuntal Dey, Michael Hind, Samuel C Hoffman, Stephanie
Houde, Kalapriya Kannan, Pranay Lohia, Jacquelyn Martino, Sameep Mehta,
Aleksandra Mojsilovic, et al .2018. AI Fairness 360: An extensible toolkit for
detecting, understanding, and mitigating unwanted algorithmic bias. arXiv
preprint arXiv:1810.01943 (2018).
[9]Reuben Binns. 2017. Fairness in machine learning: Lessons from political philos-
ophy.arXiv preprint arXiv:1712.03586 (2017).
[10]Sumon Biswasand Hridesh Rajan. 2020. Dothe Machine Learning Models on a
CrowdSourcedPlatformExhibitBias?AnEmpiricalStudyonModelFairness.
InProceedingsofthe28thACMJointMeetingonEuropeanSoftwareEngineering
ConferenceandSymposiumontheFoundationsofSoftwareEngineering (Virtual
Event, USA).642ś653. https://doi.org/10.1145/3368089.3409704
[11]SumonBiswasandHrideshRajan.2021. ReplicationPackagefor"FairPreprocess-
ing:TowardsUnderstandingCompositionalFairnessofDataTransformersinMa-
chineLearningPipeline". (2021). https://github.com/sumonbis/FairPreprocessing
[12]AmandaBower,SarahNKitchen,LauraNiss,MartinJStrauss,AlexanderVar-
gas, and Suresh Venkatasubramanian. 2017. Fair pipelines. arXiv preprint
arXiv:1707.00391 (2017).
[13]Yuriy Brun and Alexandra Meliou. 2018. Software fairness. In Proceedings of the
201826thACMJointMeetingonEuropeanSoftwareEngineeringConferenceand
Symposiumonthe FoundationsofSoftwareEngineering . 754ś759.
[14]Lars Buitinck, Gilles Louppe, Mathieu Blondel, Fabian Pedregosa, Andreas
Mueller, Olivier Grisel, Vlad Niculae, Peter Prettenhofer, Alexandre Gramfort,
JaquesGrobler,etal .2013. APIdesignformachinelearningsoftware:experiences
from the scikit-learn project. arXiv preprint arXiv:1309.0238 (2013).
[15]Toon Calders and Sicco Verwer. 2010. Three naive Bayes approaches for
discrimination-free classification. Data Mining and Knowledge Discovery 21,
2 (2010), 277ś292.
[16]JoymallyaChakraborty,KewenPeng,andTimMenzies.2020. MakingfairML
software using trustworthy explanation. In 2020 35th IEEE/ACM International
Conference onAutomatedSoftwareEngineering (ASE) . IEEE,1229ś1233.
[17]JoymallyaChakraborty,TianpeiXia,FahmidMFahid,andTimMenzies.2019.
Softwareengineeringforfairness:Acasestudywithhyperparameteroptimiza-
tion.arXiv preprint arXiv:1905.05786 (2019).
[18]PriyangaChandrasekarandKaiQian.2016. Theimpactofdatapreprocessingon
theperformanceofanaivebayesclassifier.In 2016IEEE40thAnnualComputer
Softwareand ApplicationsConference (COMPSAC) , Vol. 2.IEEE,618ś619.
[19]AlexandraChouldechova.2017. Fairpredictionwithdisparateimpact:Astudy
of biasin recidivism prediction instruments. Big data5,2 (2017), 153ś163.
[20]Sven F Crone, Stefan Lessmann, and Robert Stahlbock. 2006. The impact of
preprocessing on data mining: An evaluation of classifier sensitivity in direct
marketing. European Journal ofOperational Research 173, 3 (2006), 781ś800.
[21]AlexanderD’Amour,HansaSrinivasan,JamesAtwood,PallaviBaljekar,DSculley,
andYoniHalpern.2020. Fairnessisnotstatic:deeperunderstandingoflongterm
fairness via simulation studies. In Proceedings of the 2020 Conference on Fairness,
Accountability, and Transparency . 525ś534.
[22]LucasDixon,JohnLi,JeffreySorensen,NithumThain,andLucyVasserman.2018.
Measuringandmitigatingunintendedbiasintextclassification.In Proceedingsof
the 2018AAAI/ACMConference onAI, Ethics,and Society . 67ś73.[23]Cynthia Dwork, Moritz Hardt, Toniann Pitassi, Omer Reingold, and Richard
Zemel.2012. Fairnessthroughawareness.In Proceedingsofthe3rdinnovationsin
theoretical computer scienceconference . 214ś226.
[24]Cynthia Dwork and Christina Ilvento. 2018. Fairness under composition. arXiv
preprint arXiv:1806.06122 (2018).
[25]MichaelFeldman,SorelleAFriedler,JohnMoeller,CarlosScheidegger,andSuresh
Venkatasubramanian.2015. Certifyingandremovingdisparateimpact.In pro-
ceedingsofthe21thACMSIGKDDinternationalconferenceonknowledgediscovery
and datamining . 259ś268.
[26]Sorelle A Friedler, Carlos Scheidegger, Suresh Venkatasubramanian, Sonam
Choudhary, Evan P Hamilton, and Derek Roth. 2019. A comparative study
offairness-enhancinginterventionsinmachinelearning.In Proceedingsofthe
Conference onFairness,Accountability, and Transparency . 329ś338.
[27]Sainyam Galhotra, Yuriy Brun, and Alexandra Meliou. 2017. Fairness testing:
testing software for discrimination. In Proceedings of the 2017 11th Joint Meeting
onFoundationsofSoftwareEngineering . 498ś510.
[28]Damien Garreau and Ulrike Luxburg. 2020. Explaining the explainer: A first
theoretical analysis of LIME. In International Conference on Artificial Intelligence
and Statistics . PMLR, 1287ś1296.
[29]GabrielGoh,AndrewCotter,MayaGupta,andMichaelPFriedlander.2016. Satis-
fyingreal-worldgoalswithdatasetconstraints.In AdvancesinNeuralInformation
ProcessingSystems . 2415ś2423.
[30]Noah J Goodall. 2016. Can you program ethics into a self-driving car? IEEE
Spectrum 53,6 (2016), 28ś58.
[31]Nina Grgic-Hlaca, Muhammad Bilal Zafar, Krishna P Gummadi, and Adrian
Weller. 2018. Beyond Distributive Fairness in Algorithmic Decision Making:
FeatureSelectionfor ProcedurallyFairLearning..In AAAI. 51ś60.
[32]Moritz Hardt, Eric Price, and Nati Srebro. 2016. Equality of opportunity in
supervised learning. In Advances in neural information processing systems . 3315ś
3323.
[33]GalenHarrison,JuliaHanson,ChristineJacinto,JulioRamirez,andBlaseUr.2020.
An empirical study on the perceived fairness of realistic, imperfect machine
learning models. In Proceedings of the 2020 Conference on Fairness, Accountability,
and Transparency . 392ś402.
[34]Dr.HansHofmann.1994. GermanCreditDataset:UCIMachineLearningReposi-
tory.https://archive.ics.uci.edu/ml/datasets/Statlog+(German+Credit+Data)
[35]Kenneth Holstein, Jennifer Wortman Vaughan, Hal Daumé III, Miro Dudik, and
Hanna Wallach. 2019. Improving fairness in machine learning systems: What do
industrypractitionersneed?.In Proceedingsofthe2019CHIConferenceonHuman
FactorsinComputingSystems . 1ś16.
[36]MaxHort,JieMZhang,FedericaSarro,andMarkHarman.2021. Fairea:AModel
Behaviour Mutation Approach to Benchmarking Bias Mitigation Methods. In
Proceedings of the 29th ACM Joint Meeting on European Software Engineering
ConferenceandSymposiumontheFoundationsofSoftwareEngineering(toappear)
(Athens,Greece).
[37]Md Johirul Islam, Giang Nguyen, Rangeet Pan, and Hridesh Rajan. 2019. A
Comprehensive Study on Deep Learning Bug Characteristics. In ESEC/FSE’19:
The ACM Joint European Software Engineering Conference and Symposium on the
FoundationsofSoftwareEngineering (ESEC/FSE) (ESEC/FSE 2019) .
[38]Md Johirul Islam, Rangeet Pan, Giang Nguyen, and Hridesh Rajan. 2020. Repair-
ing Deep Neural Networks: Fix Patterns and Challenges. In ICSE’20: The 42nd
InternationalConference onSoftwareEngineering (Seoul, SouthKorea).
[39]Kaggle. 2017. Home Credit Dataset. https://www.kaggle.com/c/home-credit-
default-risk .
[40] Kaggle.2017. TitanicMLDataset. https://www.kaggle.com/c/titanic/data .
[41]Faisal Kamiran and Toon Calders. 2012. Data preprocessing techniques for
classificationwithoutdiscrimination. KnowledgeandInformation Systems 33,1
(2012), 1ś33.
[42]FaisalKamiran,AsimKarim,andXiangliangZhang.2012. Decisiontheoryfor
discrimination-awareclassification.In 2012IEEE12thInternationalConferenceon
DataMining . IEEE,924ś929.
[43]Toshihiro Kamishima, Shotaro Akaho, Hideki Asoh, and Jun Sakuma. 2012.
Fairness-awareclassifierwithprejudiceremoverregularizer.In JointEuropean
Conference on Machine Learning and Knowledge Discovery in Databases . Springer,
35ś50.
[44]KeithKirkpatrick.2017. It’snotthealgorithm,it’sthedata. Commun.ACM 60,2
(2017), 21ś23.
[45]Ron Kohavi. 1996. Scaling up the accuracy of naive-bayes classifiers: A decision-
treehybrid..In KDD,Vol.96.202ś207. https://archive.ics.uci.edu/ml/datasets/
adult
[46]Andreas Kunft, Asterios Katsifodimos, Sebastian Schelter, Sebastian Breß,
Tilmann Rabl, and Volker Markl. 2019. An intermediate representation for
optimizingmachinelearningpipelines. ProceedingsoftheVLDBEndowment 12,
11(2019), 1553ś1567.
[47]MattKusner,JoshuaLoftus,ChrisRussell,andRicardoSilva.2017. Counterfactual
fairness. In Proceedings of the 31st International Conference on Neural Information
ProcessingSystems . 4069ś4079.
[48]Guillaume Lemaître, Fernando Nogueira, and Christos K Aridas. 2017.
Imbalanced-learn:Apythontoolboxtotacklethecurse ofimbalanceddatasets
992Fair Preprocessing: TowardsUnderstandingCompositionalFairness of DataTransformersin MachineLearning Pipeline ESEC/FSE ’21, August 23–28, 2021,Athens,Greece
in machine learning. The Journal of Machine Learning Research 18, 1 (2017),
559ś563.
[49]Sérgio Moro, Paulo Cortez, and Paulo Rita. 2014. A data-driven approach to
predict the success of bank telemarketing. Decision Support Systems 62 (2014),
22ś31.https://archive.ics.uci.edu/ml/datasets/Bank+Marketing
[50]P Olson. 2011. The algorithm that beats your bank manager. CNN Money
(2011).https://www.forbes.com/sites/parmyolson/2011/03/15/the-algorithm-
that-beats-your-bank-manager/
[51]Randal S Olson and Jason H Moore. 2016. TPOT: A tree-based pipeline optimiza-
tiontoolforautomatingmachinelearning.In Workshoponautomaticmachine
learning. PMLR, 66ś74.
[52]Rangeet Pan and Hridesh Rajan. 2020. On Decomposing a Deep Neural Network
into Modules. In ESEC/FSE’2020: The 28th ACM Joint European Software Engi-
neeringConferenceandSymposiumontheFoundationsofSoftwareEngineering
(Sacramento, California, United States).
[53]Judea Pearl. 2000. Causality: Models, reasoning and inference cambridge univer-
sity press. Cambridge, MA, USA, 9 (2000), 10ś11.
[54]Judea Pearl. 2009. Causal inference in statistics: An overview. Statistics surveys 3
(2009), 96ś146.
[55]GeoffPleiss,ManishRaghavan,FelixWu,JonKleinberg,andKilianQWeinberger.
2017. Onfairness andcalibration.In Advances inNeuralInformationProcessing
Systems. 5680ś5689.
[56]MarcoTulioRibeiro,SameerSingh,andCarlosGuestrin.2018. Anchors:High-
precision model-agnostic explanations. In Proceedings of the AAAI Conference on
Artificial Intelligence , Vol. 32.
[57]Christopher Russell, Matt J Kusner, Joshua R Loftus, and Ricardo Silva. 2017.
Whenworldscollide:integratingdifferentcounterfactualassumptionsinfairness.
Advances in Neural Information Processing Systems 30. Pre-proceedings 30 (2017).
[58]PedroSaleiro,BenedictKuester,LorenHinkson,JesseLondon,AbbyStevens,Ari
Anisfeld, Kit T Rodolfa, and Rayid Ghani. 2018. Aequitas: A bias and fairness
audittoolkit. arXiv preprint arXiv:1811.05577 (2018).
[59]BabakSalimi,LukeRodriguez,BillHowe,andDanSuciu.2019. Interventional
fairness: Causal database repair for algorithmic fairness. In Proceedings of the
2019InternationalConference onManagementofData . 793ś810.
[60]ScikitLearn.2019. FeatureSelectionMethods. https://scikit-learn.org/stable/
modules/feature_selection.html .
[61]ScikitLearn.2019. PreprocessingAPIDocumentation. https://scikit-learn.org/
stable/modules/classes.html#module-sklearn.preprocessing .
[62]Scikit Learn. 2019. Scikit Learn SimpleImputer. https://scikit-learn.org/0.18/
modules/generated/sklearn.preprocessing.Imputer.html .
[63]Scikit-Learn Pipeline. 2020. Scikit-Learn API Documentation. https://scikit-
learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html .
[64]Kacper Sokol, Raul Santos-Rodriguez, and Peter Flach. 2019. FAT Forensics:
APythonToolboxforAlgorithmicFairness,AccountabilityandTransparency.arXiv preprint arXiv:1909.05167 (2019).
[65]TillSpeicher,HodaHeidari,NinaGrgic-Hlaca,KrishnaPGummadi,AdishSingla,
AdrianWeller,andMuhammadBilalZafar.2018. Aunifiedapproachtoquantify-
ing algorithmic unfairness: Measuring individual &group unfairness via inequal-
ityindices.In Proceedingsofthe24thACMSIGKDDInternationalConferenceon
KnowledgeDiscovery& Data Mining . 2239ś2248.
[66]FlorianTramer,VaggelisAtlidakis,RoxanaGeambasu,DanielHsu,Jean-Pierre
Hubaux,MathiasHumbert,AriJuels,andHuangLin.2017. FairTest:Discovering
unwarranted associations in data-driven applications. In 2017 IEEE European
SymposiumonSecurityand Privacy (EuroS&P) . IEEE,401ś416.
[67]SakshiUdeshi,PryanshuArora,andSudiptaChattopadhyay.2018. Automated
directedfairnesstesting.In Proceedings ofthe33rd ACM/IEEE InternationalCon-
ference onAutomatedSoftwareEngineering . 98ś108.
[68]US Equal Employment Opportunity Commission. 1979. Guidelines on Employee
Selection Procedures. https://www.eeoc.gov/laws/guidance/questions-and-
answers-clarify-and-provide-common-interpretation-uniform-guidelines .
[69]AlperKursatUysalandSerkanGunal.2014. Theimpactofpreprocessingontext
classification. InformationProcessing& Management 50,1 (2014), 104ś112.
[70]MohammadWardat,WeiLe,andHrideshRajan.2021. DeepLocalize:FaultLocal-
ization for Deep Neural Networks. In ICSE’21: The 43nd International Conference
onSoftwareEngineering (VirtualConference).
[71] KeYang,BiaoHuang, Julia Stoyanovich,and Sebastian Schelter.2020. Fairness-
Aware Instrumentation of Preprocessing Pipelines for Machine Learning. In
Workshop onHuman-In-the-Loop Data Analytics (HILDA’20) .
[72]MuhammadBilal Zafar, Isabel Valera, Manuel GomezRodriguez, and KrishnaP
Gummadi.2015. Fairness constraints:Mechanismsforfairclassification. arXiv
preprint arXiv:1507.05259 (2015).
[73]Carlos Vladimiro González Zelaya. 2019. Towards Explaining the Effects of Data
PreprocessingonMachineLearning.In 2019IEEE35thInternationalConference
onDataEngineering (ICDE) . IEEE,2086ś2090.
[74]Rich Zemel, Yu Wu, Kevin Swersky, Toni Pitassi, and Cynthia Dwork. 2013.
Learningfairrepresentations.In InternationalConferenceonMachineLearning .
325ś333.
[75]Brian Hu Zhang, Blake Lemoine, and Margaret Mitchell. 2018. Mitigating un-
wantedbiaseswithadversariallearning.In Proceedingsofthe2018AAAI/ACM
Conference onAI, Ethics,and Society . 335ś340.
[76]Junzhe Zhang and Elias Bareinboim. 2018. Fairness in decision-makingÐthe
causalexplanationformula.In ProceedingsoftheAAAIConferenceonArtificial
Intelligence , Vol. 32.
[77]Jie M Zhang and Mark Harman. 2021. łIgnorance and Prejudicež in Software
Fairness.In 2021IEEE/ACM43rdInternationalConferenceonSoftwareEngineering
(ICSE). IEEE,1436ś1447.
993