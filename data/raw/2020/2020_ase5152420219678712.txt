Thinking Like a Developer? Comparing the
Attention of Humans with Neural Models of Code
Matteo Paltenghi
Department of Computer Science
University of Stuttgart
Stuttgart, Germany
mattepalte@live.itMichael Pradel
Department of Computer Science
University of Stuttgart
Stuttgart, Germany
michael@binaervarianz.de
Abstract ‚ÄîNeural models of code are successfully tackling
various prediction tasks, complementing and sometimes even
outperforming traditional program analyses. While most workfocuses on end-to-end evaluations of such models, it often remainsunclear what the models actually learn, and to what extent theirreasoning about code matches that of skilled humans. A poorunderstanding of the model reasoning risks deploying modelsthat are right for the wrong reason, and taking decisions basedon spurious correlations in the training dataset. This paperinvestigates to what extent the attention weights of effectiveneural models match the reasoning of skilled humans. To thisend, we present a methodology for recording human attentionand use it to gather 1,508 human attention maps from 91participants, which is the largest such dataset we are awareof. Computing human-model correlations shows that the copyattention of neural models often matches the way humans reasonabout code (Spearman rank coefÔ¨Åcients of 0.49 and 0.47), whichgives an empirical justiÔ¨Åcation for the intuition behind copyattention. In contrast, the regular attention of models is mostlyuncorrelated with human attention. We Ô¨Ånd that models andhumans sometimes focus on different kinds of tokens, e.g.,strings are important to humans but mostly ignored by models.The results also show that human-model agreement positivelycorrelates with accurate predictions by a model, which calls forneural models that even more closely mimic human reasoning.Beyond the insights from our study, we envision the release ofour dataset of human attention maps to help understand futureneural models of code and to foster work on human-inspiredmodels.
I. I NTRODUCTION
Neural models that analyze source code [46] have become
extremely effective on various tasks, such as code summariza-
tion [5], [6], [31], bug detection [28], [48], bug injection [44],bug repair [16], and type inference [3], [26], [40], [47], [65].In essence, these models learn implicit rules, patterns, andheuristics from a large number of code examples, and thenapply them to previously unseen code. An implicit assumptionis that the models reason about code in a way similar to humansoftware developers. However, it currently remains unclearto what extent the computations performed by trained neuralmodels actually resemble how humans reason about code.
Understanding the relation between neural and human rea-
soning is an important step toward better understanding whyneural models of code work, or sometimes do not work.Current models are mostly black boxes and it remains difÔ¨Åcultto understand why a model succeeds or fails. Gaining abetter understanding of these models is crucial to validatethat a model is right for the right reasons, instead of, e.g.,picking up coincidental but meaningless correlations in adataset. Moreover, it will ultimately help build more effectivemodels by identifying current weaknesses and conÔ¨Årming whyparticular techniques are effective.
A popular way to increase the transparency of neural
networks is an attention mechanism [11], [60]. It assignsa weight to each part of the input, showing which partsof an input a model is most interested in. Recent workin natural language processing studies the effectiveness ofattention weights as an explanation technique by comparingthem with alternative explanation approaches [32], [66]. Asa Ô¨Årst attempt to understand attention weights in models ofcode, Bui et al. [13] artiÔ¨Åcially remove individual statements,observe how it affects a model‚Äôs predictions, and then comparethe importance of an input segment to attention weights. Aquestion that remains open is how attention weights in modelsof code relate to how humans reason about source code.
This paper presents the Ô¨Årst systematic study to compare
neural models of code with human reasoning. We comparethe attention weights of neural models with the attention thathumans pay when reasoning about source code examples.Our work focuses on the method summarization task, whichis interesting, as it requires a thorough understanding of apotentially complex piece of source code, and a populartask for neural models of code. For this task, we study twoneural models [1], [5], which offer two kinds of attentionweights: regular attention, showing what tokens the model
focuses on, and copy attention, showing what tokens the model
considers to copy verbatim into the output. We compare theseattention weights to humans working on the same methodsummarization task while participating in our study.
A key challenge for our work is capturing the attention of
humans while they reason about source code. We address thischallenge through a novel methodology for recording humanattention paid while solving a code-related task. Intuitively,the idea is to approximate the human attention with the time ahuman looks at a particular code element. To measure whichparts of the code the participants of our study pay attentionto, we show blurred source code to the participants, who must(temporarily) unblur individual tokens of the source code to
8672021 36th IEEE/ACM International Conference on Automated Software Engineering (ASE)
DOI 10.1109/ASE51524.2021.000812021 36th IEEE/ACM International Conference on Automated Software Engineering (ASE) | 978-1-6654-0337-5/21/$31.00 ¬©2021 IEEE | DOI: 10.1109/ASE51524.2021.9678712
978-1-6654-0337-5/21/$31.00  ¬©2021  IEEE
understand it. We gather a total of 1,508 human attention maps
from 91 participants, including Ô¨Åve examples for each of 250Java methods sampled from ten real-world projects.
Based on the dataset of human attention maps, we thor-
oughly study to what extent human attention matches theattention weights of learned models. We envision that abetter understanding of the human-machine relationships couldinspire either future explainability methods or human-inspiredmodels of code. Our study addresses the following researchquestions.
RQ1: Are the intrinsic attention weights of neural models
correlated with human attention? Answering this question
quantiÔ¨Åes how closely neural models resemble the human rea-soning. We Ô¨Ånd that neural models and humans agree on copyattention but not on regular attention, which experimentallyconÔ¨Årms the beneÔ¨Åt of copy attention in models of code.
RQ2: How does the distribution of attention across tokens
differ between models and humans? Answering this question
could highlight differences between models and humans thathelp understand their strengths and weaknesses. We Ô¨Ånd thatnone of the studied models closely mimics how humansdistribute their attention. Similar to copy attention, humanssometimes do not explore all tokens, but ignore tokens notrelevant for the prediction task.
RQ3: Do neural models and humans attend to the same
kinds of tokens, e.g., identiÔ¨Åers, separators, operators, andkeywords? If humans and models attend to different kinds of
tokens, then future neural models may want to take inspirationfrom the humans to better mimic their way of understandingsource code. We Ô¨Ånd that neural models pay more attentionto basic syntactic tokens, whereas humans pay more attentionto strings, operators, and keywords.
RQ4: Do learned models and humans struggle with the
same kinds of examples? Answering this question could reveal
complementary strengths, and it may motivate work toward ad-dressing the current weaknesses of neural models. We Ô¨Ånd thatmodels and humans largely agree on what methods are hardto summarize, showing the need for more effective models,especially on longer methods. We also Ô¨Ånd that models aremost effective on getter and setter methods, whereas humansunderstand a wider range of methods.
RQ5: How does the agreement between models and humans
relate to model effectiveness? This question is relevant to
check if creating models that more closely resemble humanreasoning is likely to yield more effective models. We Ô¨Åndthat if a model pays attention to the same tokens as humans,then the chance of making a correct prediction increases. Thisresult motivates research on human-inspired neural models.
In summary, this paper makes the following contributions:
‚Ä¢A novel methodology for recording human attentionduring code-related tasks.
‚Ä¢An in-depth study of similarities, differences, and corre-lations between the attention paid by models and humans,and its impact on model effectiveness.
‚Ä¢A dataset of 1,508 human attention maps, which we makeavailable for future work.Our replication package, tooling, and all data are
permanently available at https://github.com/MattePalte/thinking-like-a-developer.
II. B
ACKGROUND
a) Attention-Based Models: Neural network models
have recently been proposed to help developers on varioustasks [46]. Many of them use attention layers [1], [4]‚Äì[7], [31].An attention mechanism lets the network learn a weightedsum of the input units to compose a weighted context vectorfor downstream modules [11]. Besides improving a model‚Äôspredictions, attention weights also give some transparency tomodels by showing which parts of the inputs are most relevantto a model. Attention is used to attend to different parts ofsource code, e.g., source code tokens [5], paths in the abstractsyntax tree [6], or nodes of a graph representation of sourcecode [4], [17]. The attention weights of these models tellus which tokens, paths, or graph nodes explain a speciÔ¨Åcprediction. The meaning of attention weights is still underactive discussion [10], [13], [32], [66], but to the best ofour knowledge, no one has ever studied attention weightsby comparing them to the attention of programmers whileperforming a task on source code.
b) Copy Attention: Many models of code use copy
attention to learn to copy relevant tokens from the input codeverbatim to the predicted output. This idea was introduced withthe pointer network architecture [61] for solving combinatorialproblems characterized by an output vocabulary of arbitrarysize. A pointer network uses attention as a pointer to selecta part of the input sequence as the output. The output ofthe attention mechanism is a probability distribution over theinput. Copy attention has become popular to address the out-of-vocabulary problem in code summarization tasks, such asmethod naming. Allamanis et al. [5] notice that about 35%of the output tokens appear in the method body. Indeed,copy attention improves the effectiveness of models [1], [5],especially when using a dedicated attention layer [1], [21],[43]. A central assumption for these models is that the outputtokens are either in the Ô¨Åxed-size vocabulary, which is learnedduring training, or in the input sequence.
III. M
ETHODOLOGY
When studying the similarities and differences between
attention-based neural models and humans, we face two chal-lenges. (1) Creating an environment to capture the humanattention on code-related tasks. (2) Comparing the capturedhuman attention to the learned attention weights in neuralmodels. We present our methodology for addressing these twochallenges in Sections III-B and III-C, respectively, and westart by describing the prediction task our work focuses on.
A. Code Summarization Task
Among the many tasks that neural models of code are
applied to [46], we select code summarization. One reason
for this choice are the various neural models targeting thistask [1], [5]‚Äì[7], [31], [63], which could be impacted by our
868Ô¨Åndings. Another reason is that the task requires a model to
capture and summarize relatively large pieces of code, i.e.,there are several code elements to attend to.
Two variants of the code summarization task exist:
predicting the method name [5]‚Äì[7] and predicting a method-level comment [1], [19], [29]. Since both variants are popularin previous work, we select the method name predictiontask, as it simpliÔ¨Åes creating a corresponding human task.Nonetheless, our methodology could also be used to study thecomment prediction task in future work. Thus the task for themodel is to predict the original method name given the methodbody as an input. SpeciÔ¨Åcally, the models we study predictthe name as a sequence of tokens
1, e.g., ‚Äúget‚Äù, ‚Äúclient‚Äù, ‚Äúdata‚Äù
for a method called getClientData . We ask the human
participants of our study to perform a variant of this tasks:inspect a method body and then select the correct methodname among a set of seven alternatives. In addition to thecorrect solution, the alternatives consist of three names similarto the correct name and three randomly selected other names.The similar names are intended to stimulate the participant‚Äôsreasoning process, and we select them from the nearestneighbors of the correct name in a tf-idf-based bag-of-wordsencoding of method names. The set of alternatives in Figure 1includes
testInitializing DoesntTakeReadAction ,
which is the real name of the method, three close-byalternatives (
testToStringDoesntExhaustIterator ,
testCorrectProgressAndReadAction ,testAction ),
and three random options ( disableSyncScrollSupport ,
testDeepConflictingReturnTypes ,calculateTime-
stamp ). The tasks given to the models and the humans are
closely related since both the model and the humans mustinspect the method body and condense it into a summary ofthe functionality of the code.
As a dataset of methods, we sample 236 methods from
an existing Java corpus [5]. The corpus contains code fromseveral application domains and is a popular benchmark forneural models [6], [64].
B. Capturing Human Attention
The Ô¨Årst major challenge is how to capture the attention of
a human working on the code summarization task. We address
this challenge through a novel, gamiÔ¨Åcation-inspired atten-tion annotation interface, called Human Reasoning Recorder(HRR). The participants working with the HRR are not awarethat the ultimate goal is to capture how much attention theypay to speciÔ¨Åc code elements, but instead are simply asked toselect a suitable name for a method. The interface is dividedinto two main areas (Figure 1): an answer selection area,
which shows the names to choose from, and a code inspection
area, which shows the code of the method body.
The key idea to capture which parts of the code a participant
attends to is a deblurring mechanism. Initially, all code in themethod body is blurred, and the participant must deblur tokens
1The term ‚Äútoken‚Äù in this paper means subtokens that result from tokenizing
code as speciÔ¨Åed by the programming language and then further splitting
identiÔ¨Åers based on camel-case and other conventions.

	

	
	


Fig. 1. Interface of the Human Reasoning Recorder.
to understand the code. Participants can deblur a token in two
ways. On the one hand, moving the mouse over a token revealsthe token and its neighbors for the time the mouse pointer ison them. Based on our initial pilot study and the fact that theaverage number of pieces of information that a human can holdin short-term memory is seven [42], we set the neighborhoodto three tokens before and three tokens after the pointed-totoken, ignoring neighbors in other lines.
On the other hand, participants can also click on tokens
to make them permanently visible, even when the mousemoves away, and they can blur tokens again with anotherclick. This mechanism allows participants to pinpoint themost important parts of the already explored code. To forceparticipants to move the mouse away from the code whilereading the candidate method names, the answer selection areais also blurred by default and becomes visible only when themouse is in this section.
The HRR tracks which tokens a participant deblurs and
for how long each token is visible. To this end, the interfacecontinuously logs all mouse movements and clicks. Overall,the HRR captures the following information each time aparticipant summarizes a method:
DeÔ¨Ånition 1 (Human attention record). The human attention
record is a tuple hr=(id,uid,evts,s,r), where idis a
unique identiÔ¨Åer for a method in the dataset, uid is the unique
user identiÔ¨Åer, evts is a sequence of mouse-token interaction
events,sis the selected method name, and ris a rating given
by the user about the difÔ¨Åculty of naming the method.
The sequence evts contains events for a speciÔ¨Åc token
becoming visible and invisible, either through hovering or
clicking. The rating ris obtained by asking the participant
after each method to rate how difÔ¨Åcult choosing the namewas, on a scale from 1 (‚Äúeasy‚Äù) to 5 (‚Äúhard‚Äù).
Each participant in our study gets assigned a set of 20
methods that are randomly sampled from the test set of thedataset [5]. Following a common practice in human stud-ies [18], we consider the Ô¨Årst three questions as a warm-up,i.e., every participant produces 17 human attention records.The sampling is done such that each method is seen by m
different participants, where we set m=5 following related
studies [57], [62].
We recruit participants among undergraduate-level and
graduate-level university students in computer science, and
869	
			  
		
	

	




	

 

Fig. 2. Visualization of token-level attention weights derived from CNN model (left) and humans (right). Red boxes indicate which token was clicked by the
participant.
via Amazon Mechanical Turk (AMT). The AMT participants
must have at least 90% approval rate and 100 approvedtasks to participate in the study [34] and are rewarded withone US dollar upon successful completion [62]. To Ô¨Ålter outparticipants who misunderstand the task or fail to successfullycomplete it for other reasons, we measure the correctness ofthe selected method name on two levels: exactly correct when
the participant selects the original method name, and pseudo-
correct when the participant selects either the original name
or one of the three similar names.
For each combination of a number of correct and a number
of pseudo-correct answers we compute the probability that aparticipant performing random guessing obtains the same or abetter result. Based on this probability, we accept only partic-ipants that provide results with a probability of less than 5%to come from a random guesser. SpeciÔ¨Åcally, we consider themultinomial distribution, as we are dealing with an extensionof the binomial experiment, with three possible outcomes: A
for the participant selecting the exactly correct answer, Bfor
the participant selecting a pseudo-correct answer, and Cfor
the participant selecting a wrong name. For a random guesser,these outcomes have probabilities: P(A)=
1
7,P(B)=3
7,
P(C)=3
7. Given the number of exactly correct, pseudo-
correct, and wrong answers of a single participant as x,y
andzrespectively, we compute the probability for a randomly
acting participant to obtain the same or a better result:
Prndguesser (C=x,N =y,W =z)=
P(C=x,N =y,W =z)+
y/summationdisplay
i=1P(C=x+i,N =y‚àíi,W =z)+
z/summationdisplay
i=1P(C=x,N =y+i,W =z‚àíi)+z/summationdisplay
i=1P(C=x+i,N =y,W =z‚àíi)
where the vector of random variables X=(C,N,W )is
multinomially distributed with index n=1 7 and parameters
œÄ=(P(A),P(B),P(C)) = (1
7,3
7,3
7), i.e.,X‚àºMult (n,œÄ).
After Ô¨Åltering, the Ô¨Ånal dataset contains 1,508 human attentionrecords, which contain at least Ô¨Åve human annotations for250 methods. The records are gathered from those 91 outof 166 participants that pass our Ô¨Åltering. 26 of the acceptedparticipants are computer science students and 65 are recruitedvia AMT. On average, a human attention record contains about1,271 mouse-token interaction events.
C. Comparing Attention: Neural Models vs. Humans
The second major challenge is comparing the captured
human attention to learned attention weights in neural models.
Figure 2 illustrates the problem with an example method,where the model attention is shown on the left, and the humanattention is shown on the right. The following describes theneural models we study and how we compare them againstthe human attention records.
1) Neural Models: We study two attention-based neural
models representative of two widely-used architectures: aconvolutional attention model [5], called CNN model , and a
transformer-based model [1], called transformer model. De-
spite the recent popularity of transformers, we choose to studyalso a CNN model because it is one of the Ô¨Årst attention-basedmodels for this task and because it allows us to draw moregeneral conclusions. The CNN model also has the advantage oftaking arbitrarily sized inputs, whereas the transformer modelimposes a Ô¨Åxed input length.
Both models have a sequence-to-sequence architecture that
reasons about the method body as a sequence of tokens andthen predicts the tokens of the method name.
870We distinguish between two kinds of attention:
‚Ä¢Regular attention, which is implemented as convolutional
attention for the CNN model and as multi-head self-attention for the transformer model. The regular attentionshows which parts of the code the models pay mostattention to when reasoning about the meaning of themethod.
‚Ä¢Copy attention, a mechanism to tackle the out-of-vocabulary problem by optionally copying some tokensfrom the method body to the output. The copy attentionshows which parts of the method body the model con-siders as candidates for verbatim copying.
For both the CNN and the transformer, we train project-
speciÔ¨Åc models, as they outperform a single cross-projectmodel [5]. We leave the model architecture and all hyper-parameters in their default conÔ¨Ågurations, except for oneadaptation of the transformer model. The original model isdesigned to summarize a method into a Javadoc sentence. Weadapt the model to the method naming task by Ô¨Årst pre-trainingthe model on its original dataset [1] and by then Ô¨Åne-tuningthe project-speciÔ¨Åc models on the method naming dataset [5].Moreover, since the transformer model attends only to the Ô¨Årst150 tokens of a method, all results for this model considerthose tokens only.
To measure the effectiveness of the models, we compute
the F1-score of the top-most predicted name [5], [6]. Itconsists of comparing the set of predicted tokens and thetokens in the original name, and then computing the har-monic mean between precision =
# correctly predicted tokens
# predicted tokensand
recall =# correctly predicted tokens
# tokens in the original name. The average F1-score of the
CNN model and the transformer model are 0.40 and 0.46,respectively, which is in line with the originally reportedresults.
2) Measuring Human-Model (Dis)Agreement: We summa-
rize the attention spent by humans and models into vectorsand then compute correlations between the two. As a proxyof the human attention given to a token, we consider the totaltime that the token was visible, similarly to Ô¨Åxation time ineye tracking studies [22], [55]:
DeÔ¨Ånition 2 (Human attention) .Lethr=(id,uid,evts,s,r)
be a human attention record for a method body with ntokens.
The human attention vector is /vectorh=(h
1,h2,...,hn), wherehi
is the total time that the token at position ihas been visible
to the participant according to evts .
For each of the two kinds of attention, we summarize the
model‚Äôs attention as follows:DeÔ¨Ånition 3 (Model attention). Suppose a method body
withntokens and a sequence of ktokens predicted by the
model as the method name. The machine attention vector is
/vector m=(m
1,m2,...,mn), wheremi=mean(a1
i,...,aki)withaj
i
indicating the attention weight assigned by the model to the
token at position iof the method body during the prediction
of thejth output token.Computing the mean handles the fact that the models
produce one vector of attention weights for every predictedtoken in the method name. For example, for a predictedmethod name
getClientData , for each token in the method
body, we average the attention weights assigned to that tokenduring the prediction of the three tokens ‚Äúget‚Äù, ‚Äúclient‚Äù, and‚Äúdata‚Äù.
Given two attention vectors /vectorhand/vector m, we compute to what
extent they agree on the importance of tokens by computingSpearman‚Äôs rank coefÔ¨Åcient [58]. To this end, we convertthe attention vectors to a ranking of tokens, rg
handrgm,
and then compute Spearman‚Äôs rank correlation coefÔ¨Åcient asthe Pearson correlation coefÔ¨Åcient between the rank variables:Spearman =
cov(rg h,rgm)
œÉrgh,œÉrgmwherecov andœÉare the standard
deviation and the covariance, respectively. The coefÔ¨Åcientranges between -1 and 1, where 1 means that both attentionvectors perfectly agree, -1 means that the attention weightsyield exactly the inverse ranking, and zero means that both areunrelated. For all reported correlation coefÔ¨Åcients, we includeonly pairs of attention vectors with high-conÔ¨Ådence results(p-value‚â§0.05). A valid alternative measure to Spearman‚Äôs
coefÔ¨Åcient is the Kendall Tau [36]; it yields similar results asthose reported here and is omitted for space reasons.
IV . R
ESULTS
Our dataset contains 1,508 human attention records from
which we extract the same number of token-level attentionvectors, as explained in DeÔ¨Ånition 2. It comprises 250 methodsfrom ten different repositories, where each method is annotatedby at least Ô¨Åve of the 91 human annotators. On averageour participants took 57 seconds to name a single method,producing 1,271 mouse-token events each time.
A. RQ1: Correlation Between Model and Human Attention
To quantify the overall degree of agreement between the
attention paid by neural models of code and humans, we
compute the correlation between both. Figure 3 shows thedistribution of Spearman rank correlation coefÔ¨Åcients for thetwo kinds of model attention for each of the two models westudy. Each plot shows how many of the studied Java methodsfall into a speciÔ¨Åc correlation range. A correlation of 0.0 wouldmean that a model and the humans are completely unrelated,and a correlation of 1.0 would mean that both agree perfectlyon the relatively order of all attention weights. The Ô¨Årst andsecond plot show the results for regular and copy attentionof the CNN model, respectively, and likewise with the thirdand fourth plot for the transformer model. For example, thesecond plot shows that for the majority of methods, the copyattention weights of the CNN model have a rank correlationbetween 0.25 and 1.0 with the humans who inspect the samemethod.
For both models, the regular attention weights show a
weak correlation with humans, with means of 0.08 and -0.20. In contrast, the copy attention weights of both modelsshow a moderate to strong human-machine correlation, withmeans of 0.49 and 0.47. As an example, in Figure 2 both
871Fig. 3. Correlation between machine attention and human attention.
the copy attention of the CNN model (bottom-left) and the
human (bottom-right) are paying most of the attention tothe last few tokens of the method body. Another interestingobservation is that the human shows interest for the string
"testSynchronousDestination" , which the model seems
to overlook. We provide a separate discussion on strings inSection IV-C.
As a point of reference, the last plot in Figure 3 shows the
correlation between different humans who inspect the samemethod. The human-human agreement can be considered anupper bound of the expected model-human agreement, as itwould be unrealistic to expect a neural model to be closer tothe average human than another human. The mean human-human correlation is 0.59, which shows two points. First,it conÔ¨Årms that different participants in our study tend toattend to similar tokens, which is a prerequisite for comparingmodels against ‚Äúhumans‚Äù as a group. Second, it shows thatthe mean correlations on copy attention are relatively high, asthey are only 0.10‚Äì0.12 points lower than the human-humancorrelation.
Insight 1
: Neural models and humans often agree about
what tokens to copy verbatim from the input to the output,but less on what other tokens to attend to. The relatively highcorrelation for copy attention gives an empirical justiÔ¨Åcation tothe copy attention mechanisms used by many neural models.Fig. 4. Median value of normalized attention proÔ¨Åles across all studied
methods.
B. RQ2: Distribution of Attention Across Tokens
The following aims at understanding how models and
humans distribute their attention across the tokens in a method.
Intuitively, we are interested in how much the attention isfocused in a few highly relevant tokens, as opposed to beingroughly uniformly distributed. To quantify and visualize thedifferent attention distributions, we compute a normalizedattention proÔ¨Åle from each vector of attention weights:
DeÔ¨Ånition 4 (Normalized attention proÔ¨Åle). Given a vector of
attention weights /vector a, its normalized attention proÔ¨Åle is
/vector p=percentiles (sort(normalize (/vector a)))
where normalize divides all elements by the maximum value
in/vector a,sort sorts the vector in increasing order, and percentiles
projects a vector of arbitrary length into a sequence of 100
percentiles (with linear interpolation).
For example, suppose a very small method with Ô¨Åve tokens
and an attention vector /vector a=[ 0,4,0,2,3]. Normalizing and
sorting the attention vector yields [0,0,0.5,0.75,1], which is
then mapped into percentiles to give the a normalized attentionproÔ¨Åle where, e.g., the 20%-percentile is zero and the 50%-percentile is 0.5.
Figure 4 shows the normalized attention proÔ¨Åles for the
Ô¨Åve kinds of attention vectors we study: four from the neuralmodels and one from the humans. Each curve is the medianvalue across the attention vectors of all methods in our dataset.Intuitively, the more a curve is dented toward the lower-rightcorner, the more focused the attention vectors are on a smallnumber of tokens. The results show that copy attention tends tobe clearly more focused then regular attention, i.e., the modelstake a clear decision about which tokens may be worth copying
872to the output. In contrast, the regular attention, especially of
the CNN model, are more uniformly distributed with respectto their own copy counterparts. Both the regular and copyattention of the transformer disregards a large number oftokens.
The human proÔ¨Åle is in between the two transformer
attention mechanisms for the top-most attended token (top-right in Figure 4), and then decreases almost constantly untilit, perhaps surprisingly, reaches zero. Reaching zero meansthat the humans often completely ignore some tokens, i.e.,our participants could summarize a method based on only asubset of all its tokens. During manual inspection of attentionrecords, we notice that, especially on longer methods, humansoften focus on variables declared at the beginning and thenskim read to the end of the method, paying most attention toreturn statements and assertions.
Insight 2
: No attention mechanism, among those studied,
closely mimics the way humans distributes their attention onthe tokens of the method body.
Insight 3
: The transformer model seems to be overspecial-
ized in attending only a small subset of tokens, as comparedto the convolutional model.
Insight 4
: Sometimes the humans do not fully explore
the entire method, but base their answers on a subset of thetokens in the methods, typically the beginning (esp. variabledeclarations) and end (esp. returns and assertions).
Suggestion 1
: Future human-inspired transformer models
should be trained with an objective to attend to a largerportion of the source code, rather than overspecializing to afew tokens.
C. RQ3: Categories of Tokens
Source code is composed of different categories of tokens,
e.g., identiÔ¨Åers, separators, keywords, and strings. For the
code in our dataset, the most common token categories areidentiÔ¨Åers (46%) and separators (39.7%). To quantify howmuch attention a speciÔ¨Åc token category receives, we deÔ¨Ånethe following notion. Intuitively, it indicates how much moreor less attention a speciÔ¨Åc token category receives comparedto uniformly distributed attention.
DeÔ¨Ånition 5 (Distance from uniformity). Given a vector of
attention weights /vector afor a sequence Tof tokens, the distance
from uniformity of a subset SofTis:
DFU (S)=
/summationtext
t‚ààSat/summationtext
t‚ààTat‚àí|S|
|T|
|S|
|T|
whereatis the attention weight assigned to token t.
ADFU value of zero indicates that the token category is
getting exactly the attention of uniformly distributed attention.
A positive or negative value indicates that the token category isreceiving more or less attention, respectively. The lower boundis -1, where the token category is receiving no attention at all.
Figure 5 presents the DFU for different token categories.
The results show signiÔ¨Åcant differences between the mod-els and the humans. For example, the humans give moreFig. 5. Distance from uniformity (DFU) for different token categories.
	
	

		
	

		
	
	
Fig. 6. Attention maps from the transformer model and humans.
importance to keywords, operators, and strings, whereas themodels pay less attention to these token categories. Thisdifference suggests that these tokens play an important role incomprehending code, and they should not be left unattended byneural models. One can also observe differences between thedifferent kinds of model attention. For example, the regularattention by the transformer model is surprisingly high forseparators and relatively small for identiÔ¨Åers. Some tokencategories, especially Booleans, are mostly ignored both bythe models and the humans.
A manual inspection of various attention maps conÔ¨Årms
873our quantitative results. For example, consider the code in
Figure 6, which shows the attention maps of the CNNmodel‚Äôs regular attention (top) and of the humans who studiedthis method (bottom). The humans pay a lot more attentionto string literals, which indeed contain information relevantfor the method summarization task, whereas the model isoverlooking their importance in favor of other tokens. Theobservation illustrated by this example motivates work onneural models that ‚Äúunderstand‚Äù string literals, which currentlyare often abstracted away [27] or split with empty spaces [35],when other splits might be more effective (e.g., with slash).
Another Ô¨Ånding, is that humans tend to overlook curly
braces, which is visible in Figure 6 and also conÔ¨Årmed bytheDFU of curly braces being close to its lower limit
(DFU‚â§‚àí 0.9). In the upper part of Figure 6 we see
how the transformer model prefers syntactic tokens, such asdot, comma, and open parenthesis. After a thorough manualinspection, we conÔ¨Årmed this to be a general characteristic ofthe regular attention of the transformer model, which focuseson tokens that proceed and follow method calls, in an attemptto isolate method calls. This is explaining also the attentionproÔ¨Åle of the regular transformer in Figure 4, where the plateauon the top right is made of those tokens (i.e., dots, commas,and open parentheses) used to isolate method calls.
Insight 5
: High attention paid by the copy attention to
identiÔ¨Åers conÔ¨Årms their effectiveness in focusing primarily ontokens that might be copied verbatim into the method name.
Suggestion 2
: The focus of copy attention on identiÔ¨Åers
could be stressed even more by masking the other kinds oftokens.
Insight 6
: Strings, keywords, and operators are often
overlooked by the models, whereas the humans give moreattention to them.
Suggestion 3
: Future neural models could pay more at-
tention to strings, keywords, and operators, which humansconsider important during the method summarization task.
Insight 7
: Block-level separators, such as curly braces, are
attended mostly by the models, whereas the humans get thisinformation implicitly from the indentation of the formattedcode.
Suggestion 4
: Future models of code could encode basic
syntactic information into token embeddings, preventing themodel to focus its attention on purely syntactic tokens, suchas curly braces.
Insight 8
: Regular attention of transformers specializes
in the recognition of separators that proceed and followmethod invocations. This conÔ¨Årms and extends an analogousspecialization of multi-head attention observed also in naturallanguage processing [10]. We here conÔ¨Årm that transformersare attentive to separators also when applied to code-relatedtasks. We hypothesize that recognizing separators is importantfor the model to understand the role of individual tokens, e.g.,that a speciÔ¨Åc token represents the name of a called method.An interesting future work could be to further explore the rootcauses of this phenomenon.Fig. 7. Distance form uniformity (DFU) for different groups of identiÔ¨Åers.
Given the high attention given by models to identiÔ¨Åers, we
further analyze different groups of identiÔ¨Åers based on theirlength and popularity. Figure 7 shows how copy attention forboth models prefers long and popular identiÔ¨Åers over shortand rare ones, respectively.
Insight 9
: Long and frequent identiÔ¨Åers are good candi-
dates to be copied verbatim into the method name by theanalyzed models.
D. RQ4: Perceived DifÔ¨Åculty vs Model Effectiveness
Understanding which examples are more difÔ¨Åcult for hu-
mans and models could reveal in which measure they are
similar and if they can complement each other. We use therating given by the participant on how easy it was to name eachmethod to create a per-method average rating. We aggregatethe ratings of Ô¨Åve different annotators on the same method toget a more stable human ground-truth. Comparing these ratingsto the F1-score of the models shows a positive correlation(Pearson correlation 0.45 and 0.49 for CNN and transformer,respectively).
Insight 10
: The neural models and the humans agree on
which methods are more difÔ¨Åcult to name.
To investigate further which are those difÔ¨Åcult methods,
we also compare the performance of humans and modelson different method lengths and different groups of methods.Figure 8 shows how both humans and models are good onshorter methods, whereas predicting the name of longer onesis more challenging.
To better understand what kinds of methods models and
humans are successful on, we analyze Ô¨Åve groups of methods:getter, setter, checker, test starting respectively with ‚Äúget‚Äù,
‚Äúset‚Äù, ‚Äúis‚Äù or ‚Äúhas‚Äù, and ‚Äútest‚Äù, and other for all the remaining
methods. Figure 9 reports the models effectiveness, measuredwith F1-score, and the percentage of correctly (and pseudo-correctly) named methods by humans. It shows how gettersand setters are the easiest to predict for both humans andmodels. An interesting Ô¨Ånding is that humans are almost asgood on checkers as they are on getters, whereas this is notthe case for the models, which struggle with identifying thename of a method that is checking some property on the
874Fig. 8. Method length for different performance levels for human (correct or
wrong) and model (f1-score). The error bars measure the uncertainty aroundthe estimate.
object. For test methods and other methods we see a lower
percentage of correctly named methods for both humans andmodels. We note that humans have a high portion of pseudo-correct answers for the test methods, which could be due tothe presence of multiple reasonable method names for them.
Insight 11
: Beside getters and setters, neural models often
struggle to predict more challenging kinds of methods, such ascheckers and test methods, whereas the humans are successfulacross a wider range of methods.
Suggestion 5
: Models could learn from the human to name
more challenging types of methods on which humans performbetter, e.g., by using human attention traces during training.
In Table I, we consider both characteristics, presenting the
length of the different groups in terms of lines of code (LOCs).We see that test and others methods, which are hard formodels, are generally longer than other groups, showing onceagain the impact of method length on model effectiveness.
Insight 12
: Longer methods are harder to summarize, both
for models and humans.
Suggestion 6 : To obtain models that better complement hu-
man reasoning, future training datasets should include a largerportion of ‚ÄúdifÔ¨Åcult‚Äù examples for a more effective training, orat least provide different sub-datasets of increasing difÔ¨Åculty.To establish the difÔ¨Åculty of a method, we envision the useof heuristics or human labeling. In particular to select moredifÔ¨Åcult methods, we propose the following strategies: includemethods with high cyclomatic complexity [41], reduce thepercentage of getter and setter methods, reduce the percentageof methods for which the method body contains many or allsubtokens present in the method name (i.e., methods where themodel can exploit the copying mechanism), and increase thepercentage of longer methods. In addition to these automatedstrategies for increasing the difÔ¨Åculty, humans could rate asmall pool of methods based on their notion of difÔ¨Åculty,which could then serve as a curated benchmark.Fig. 9. Model effectiveness and human performance on predeÔ¨Åned groups.
The error bars measure the uncertainty around the estimate.
TABLE I
METHOD LENGTH OF METHOD BODIES FOR FIVE PREDEFINED GROUPS .
Lines of Code (LOCs)
Group Min Median Mean Max
Getter (20%) 3 3.0 8.6 107
Setter (12%) 3 3.0 4.2 13
Checker (4%) 3 4.0 4.9 11
Test (21%) 1 10.0 14.7 61
Other (43%) 3 8.0 14.8 142
Entire dataset 1 5.0 11.9 142
E. RQ5: Human-Model Agreement vs. Model Effectiveness
Given the moderate to strong correlation between neural
models and humans, a reader may wonder whether a stronger
correlation coincides with more accurate predictions by themodel. We address this question in two ways. One of them isin Figure 10, which shows for each method in our datasettwo pieces of information: (i) on the horizontal axis, theaccuracy of the model‚Äôs prediction, measured as the F1-score(Section III-C1) and (ii) on the vertical axis, the human-model agreement, measured as in RQ1. The four plots showthe CNN model on top and the transformer model at thebottom, with regular attention and copy attention on theleft and right, respectively. Each plot also shows the linearregression trend between two the axes. Overall, we observea moderate correlation between human-model agreement andmodel effectiveness. In particular, for the regular attention ofboth models the Pearson correlation coefÔ¨Åcients are 0.19 and0.40 (p-values 5¬∑10
‚àí4and2¬∑10‚àí17).
As a second way of addressing the question, we repeat the
measurements from RQ1, i.e., how much models and humansagree about what tokens to attend to, for those methods wherethe models make accurate predictions. ‚ÄúAccurate‚Äù here meansthat the F1 score of a prediction is at least 0.5. Table IIcompares the human-model correlation across all methodswith those methods where the respective model predicts the
875Fig. 10. Human-model correlation vs. model effectiveness.
TABLE II
HUMAN -MODEL CORRELA TION FOR ALL VS .ACCURA TE PREDICTIONS .
Spearman rank correlation (mean)
All methods Methods with F1 ‚â•0.5
CNN, regular attention 0.08 0.24
CNN, copy attention 0.49 0.55
Transformer, regular attention -0.20 0.02
Transformer, copy attention 0.47 0.55
name accurately. The results show that the human-model
correlation is clearly higher for accurate predictions, e.g.,increasing from 0.08 to 0.24 for the regular attention paidby the CNN model.
Insight 13
: A higher human-model correlation coincides
with more effective predictions by the neural models.
Suggestion 7 : Creating models that more closely mimic the
human attention seems a promising way toward more effectivemodels. Future work could use human attention datasets duringmodel training [51], [54] or use loss functions that nudge themodel‚Äôs attention to mimic humans.
V. T
HREA TS TO VALIDITY
1) Internal V alidity: Several factors may inÔ¨Çuence our
results. First, the human task of choosing the correct nameamong seven alternatives differs from the model‚Äôs task ofpredicting the entire name. Because both tasks are stronglyrelated and require to understand the meaning of a method,we consider the resulting attention vectors to allow for ameaningful comparison. Moreover, we ensure the human taskto be challenging by providing alternatives similar to thecorrect name (Section III-A). Second, using a crowd platformto hire participants risks getting submissions of mixed quality.We carefully Ô¨Ålter all human attention records based onthe overall performance of a participant (Section III-B). Amanual inspection of the dataset conÔ¨Årms that realistic codeexplorations are retained by the Ô¨Åltering. Third, computinghuman attention based on the time a token is visible canonly approximate actual attention. In computer vision, mousetracking has been established as a scalable way of capturingvisual attention [14], [33], and an in-depth study could assessthe accuracy of it on source code-related tasks in the future.Fourth, our HRR interface introduces some trade-offs com-pared to the eye tracking-based studies: On the one hand, welose the pixel-level precision of an eye tracker and are not ableto capture if a participant indeed looks at the unblurred code.On the other hand, HRR enables easier remote participation,supports code snippets of arbitrary length, and automaticallycaptures token-level attention. A further study of strengths andweaknesses of the two approaches for software engineeringstudies will be interesting future work. Fifth, the choice ofthe neighborhood size of unblurred tokens, which is threetokens before and three tokens after the clicked token in ourexperiments, could lead to different results. Sixth, truncatingthe input to transformers to the Ô¨Årst 150 tokens may inÔ¨Çuenceour results since the model cannot look beyond that limit.Finally, method length and code style may confound ourÔ¨Åndings. We partially study the inÔ¨Çuence of method lengthin Figure 8, but do not consider code style as it is difÔ¨Åcult toquantify. To mitigate the impact of both possible confounders,we randomly sample from multiple repositories methods withdifferent lengths and code styles.
2) Threats to External V alidity: Several factors may inÔ¨Çu-
ence the generalizability of our results. First, the participants ofour study may not fully represent other humans. We mitigatethis threat by recruiting participants through different waysand by retaining only participants that show high performance.Second, Ô¨Åndings on the code summarization task might notgeneralize to other code-related prediction tasks. As the taskhas been proved to be a good benchmark for the abstractionabilities of models of code [39], we envision our Ô¨Åndingsto at least partially generalize to other tasks. By makingour Human Reasoning Recorder available, we facilitate futurework to study different tasks. Third, we select a CNN-basedand a transformer-based model with token-level attention assubjects for the study. Other models, e.g., those based on ASTpaths [6], [7], may expose other attention patterns. Our datasetof human attention records is available for comparisons withother models.
VI. R
ELA TED WORK
A. Capturing Human Attention
Capturing the human reasoning has always been a chal-
lenging and demanding task, as witnessed by previous studieson computer vision [14], [50] and natural language process-ing [9], [15], [38], [49], [57]. Indirect experiments, suchas ours, have been also used in these Ô¨Åelds [14], [57]. Insoftware engineering, eye tracking [55] has been the basis of
876code comprehension studies [12], [20]. With 432 human eye
tracking records, the dataset by Bednarik et al. [12], was thelargest dataset on human attention on code available so far.Two tools have been proposed to ease the collection of eyedata [23], [37]. However, due to the equipment and calibra-tion requirements, eye tracking is not easily compatible withremote participation. Neuroimaging is another interesting, yetalso very involved way to measure the activity of programmersduring program comprehension tasks [45], [56]. Our workcontributes a deblurring-based interface for capturing humanattention on code, which complements existing techniquesby providing a lightweight and scalable technique that iscompatible with remote participation.
B. Comparing Models with Developers
Previous work [52] studies which parts of a method are
of interest for a group of ten Java developers performing a
code summarization task, and compares their eye tracking dataagainst a tf-idf method [25]. They also show that eye trackingdata from programmers can improve the tf-idf model. A Ô¨Årstattempt to compare neural models of code against humanattention captured via eye tracking [30] compares the gazeof single human participant against the Code2vec model [7].Our work contributes the Ô¨Årst in-depth, multi-participant studyto compare neural models of code with human attention.
C. Studies of Attention Mechanisms
The role of attention layers as an explanation technique is
still under active study, e.g., by measuring the effectiveness
of attention layers against other explanation techniques [32],[66]. Instead of comparing multiple explanation techniqueswith each other, our work compares a model against humanattention records. Bui et al. study attention in neural modelsof code by comparing attention weights with a metric basedon the perturbation of the input program [13]. Their papermentions that ‚Äúevaluations with real programmers can bemore convincing in validating whether [their] results matchthe actual importance viewed by human‚Äù, which matches themotivation for our work. Trying to understand the attentionof transformer models, some work highlights how the vari-ous attention heads of a transformer are redundant and thatsome attention heads specialize in attending syntactic tokens,such as punctuation [10]. A similar observation is that muchof the attention of transformer-based models is assigned topunctuation tokens [53]. Arous et al. show that integratinghuman rationales into an attention-based model for NLP canimprove its effectiveness [8]. The attention records gatheredin our work could serve as a basis of similar future work onmodels of code.
D. Neural Models of Code
There are various neural models of code, and we refer to
a recent review article [46] and a survey [2] for a detailed
discussion. Many models consider source code as a sequenceof tokens [24], [26], [59], [67], which also is the representationunderlying our study. Some tree-based models [6] and graph-based models [17] also adopt an attention mechanism, whichwould be interesting to compare against our human attentionrecords.
VII. C
ONCLUSION
Motivated by the success of neural models of code, com-
bined with the difÔ¨Åculties to understand what exactly themodels are learning, this paper presents the Ô¨Årst compre-hensive study to compare human and model attention. Theresults show how the copy mechanism is empirically verysimilar to the human attention. Moreover, we have pointedout important differences between models and humans, e.g.,the different attention weights given to basic syntactic tokens,such as curly braces, but also a tendency of neural modelsto underestimate the value of strings. We reveal that neuralmodels generally struggle on longer methods, and on methodsbeyond getters and setters, whereas the humans successfullyunderstand a wider range of methods. Our work also high-lights that human-model agreement positively correlates withaccurate predictions, which calls for neural models that evenmore closely mimic human reasoning. Together with the study,we release a novel dataset of 1,508 human attention maps onthe code summarization task, collected via the newly proposedHuman Reasoning Recorder, which could serve as an enablerof further research and human studies. Ultimately, we envisionthe usage of our dataset and our tool to produce ground-truthhuman annotations to fuel human-inspired neural models.
A
CKNOWLEDGMENT
This work was supported by the European Research Council
(ERC, grant agreement 851895), and by the German ResearchFoundation within the ConcSys and Perf4JS projects. We alsothank the anonymous reviewers for their insightful commentsand suggestions, which have contributed to improve the qualityof this work.
R
EFERENCES
[1] Wasi Uddin Ahmad, Saikat Chakraborty, Baishakhi Ray, and Kai-Wei
Chang. A Transformer-based Approach for Source Code Summarization.
arXiv:2005.00653 [cs, stat], May 2020.
[2] Miltiadis Allamanis, Earl T Barr, Premkumar Devanbu, and Charles
Sutton. A survey of machine learning for big code and naturalness.ACM Computing Surveys (CSUR), 51(4):81, 2018.
[3] Miltiadis Allamanis, Earl T. Barr, Soline Ducousso, and Zheng Gao.
Typilus: Neural type hints. In Proceedings of the 41st ACM SIGPLAN
Conference on Programming Language Design and Implementation,PLDI 2020, pages 91‚Äì105, New Y ork, NY , USA, June 2020. Associationfor Computing Machinery.
[4] Miltiadis Allamanis, Marc Brockschmidt, and Mahmoud Khademi.
Learning to Represent Programs with Graphs. arXiv:1711.00740 [cs],
May 2018.
[5] Miltiadis Allamanis, Hao Peng, and Charles Sutton. A Convolutional
Attention Network for Extreme Summarization of Source Code. InInternational Conference on Machine Learning (ICML), May 2016.
[6] Uri Alon, Shaked Brody, Omer Levy, and Eran Yahav. Code2seq: Gener-
ating Sequences from Structured Representations of Code. International
Conference on Learning Representations, February 2019.
[7] Uri Alon, Meital Zilberstein, Omer Levy, and Eran Yahav. Code2vec:
Learning distributed representations of code. Proceedings of the ACM
on Programming Languages, 3(POPL):40:1‚Äì40:29, January 2019.
877[8] Ines Arous, L. Dolamic, Jie Yang, Akansha Bhardwaj, Giuseppe Cuccu,
and P . Cudr ¬¥e-Mauroux. MARTA: Leveraging Human Rationales for
Explainable Text ClassiÔ¨Åcation. In Proceedings of the Thirty-F ourth
AAAI Conference on ArtiÔ¨Åcial Intelligence, 2021.
[9] Pepa Atanasova, Jakob Grue Simonsen, Christina Lioma, and Isabelle
Augenstein. A Diagnostic Study of Explainability Techniques for Text
ClassiÔ¨Åcation. In Proceedings of the 2020 Conference on Empirical
Methods in Natural Language Processing (EMNLP), pages 3256‚Äì3274,Online, November 2020. Association for Computational Linguistics.
[10] Joris Baan, Maartje ter Hoeve, Marlies van der Wees, Anne Schuth, and
Maarten de Rijke. Understanding Multi-Head Attention in AbstractiveSummarization. arXiv:1911.03898 [cs], November 2019.
[11] Dzmitry Bahdanau, Kyunghyun Cho, and Y oshua Bengio. Neural
Machine Translation by Jointly Learning to Align and Translate.arXiv:1409.0473 [cs, stat], May 2016.
[12] Roman Bednarik, Teresa Busjahn, Agostino Gibaldi, Alireza Ahadi,
Maria Bielikova, Martha Crosby, Kai Essig, Fabian Fagerholm, AhmadJbara, Raymond Lister, Pavel Orlov, James Paterson, Bonita Sharif,Teemu Sirki ¬®a, Jan Stelovsky, Jozef Tvarozek, Hana Vrzakova, and Ian
van der Linde. EMIP: The eye movements in programming dataset.Science of Computer Programming, 198:102520, October 2020.
[13] N. D. Q. Bui, Y . Y u, and L. Jiang. AutoFocus: Interpreting Attention-
Based Neural Networks by Code Perturbation. In 2019 34th IEEE/ACM
International Conference on Automated Software Engineering (ASE),pages 38‚Äì41, November 2019.
[14] Abhishek Das, Harsh Agrawal, Larry Zitnick, Devi Parikh, and Dhruv
Batra. Human Attention in Visual Question Answering: Do Humansand Deep Networks Look at the Same Regions? Computer Vision and
Image Understanding, 163:90‚Äì100, October 2017.
[15] Jay DeY oung, Sarthak Jain, Nazneen Fatema Rajani, Eric Lehman,
Caiming Xiong, Richard Socher, and Byron C. Wallace. ERASER: ABenchmark to Evaluate Rationalized NLP Models. In Proceedings of the
58th Annual Meeting of the Association for Computational Linguistics,pages 4443‚Äì4458, Online, July 2020. Association for ComputationalLinguistics.
[16] Elizabeth Dinella, Hanjun Dai, Ziyang Li, Mayur Naik, Le Song,
and Ke Wang. Hoppity: Learning Graph Transformations to Detectand Fix Bugs in Programs. In International Conference on Learning
Representations, September 2019.
[17] Elizabeth Dinella, Hanjun Dai, Ziyang Li, Mayur Naik, Le Song, and
Ke Wang. Hoppity: Learning graph transformations to detect andÔ¨Åx bugs in programs. In 8th International Conference on Learning
Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020.OpenReview.net, 2020.
[18] Janet Feigenspan, Christian K ¬®astner, Sven Apel, J ¬®org Liebig, Michael
Schulze, Raimund Dachselt, Maria Papendieck, Thomas Leich, andGunter Saake. Do background colors improve program comprehensionin the #ifdef hell? Empirical Software Engineering, 18(4):699‚Äì745,
August 2013.
[19] Zhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xiaocheng Feng,
Ming Gong, Linjun Shou, Bing Qin, Ting Liu, Daxin Jiang, andMing Zhou. CodeBERT: A Pre-Trained Model for Programming andNatural Languages. In Findings of the Association for Computational
Linguistics: EMNLP 2020, pages 1536‚Äì1547, Online, November 2020.Association for Computational Linguistics.
[20] Thomas Fritz, Andrew Begel, Sebastian C. M ¬®uller, Serap Yigit-Elliott,
and Manuela Z ¬®uger. Using psycho-physiological measures to assess
task difÔ¨Åculty in software development. In Proceedings of the 36th
International Conference on Software Engineering, ICSE 2014, pages402‚Äì413, New Y ork, NY , USA, May 2014. Association for ComputingMachinery.
[21] Sebastian Gehrmann, Y untian Deng, and Alexander Rush. Bottom-Up
Abstractive Summarization. In Proceedings of the 2018 Conference
on Empirical Methods in Natural Language Processing, pages 4098‚Äì4109, Brussels, Belgium, October 2018. Association for ComputationalLinguistics.
[22] D. T. Guarnera. Enhancing Eye Tracking of Source Code: A Specialized
Fixation Filter for Source Code. In 2019 IEEE International Conference
on Software Maintenance and Evolution (ICSME), pages 615‚Äì618,September 2019.
[23] Drew T. Guarnera, Corey A. Bryant, Ashwin Mishra, Jonathan I.
Maletic, and Bonita Sharif. iTrace: Eye tracking infrastructure fordevelopment environments. In Proceedings of the 2018 ACM Symposiumon Eye Tracking Research & Applications, ETRA ‚Äô18, pages 1‚Äì3, NewY ork, NY , USA, June 2018. Association for Computing Machinery.
[24] Rahul Gupta, Soham Pal, Aditya Kanade, and Shirish K. Shevade.
DeepÔ¨Åx: Fixing common C language errors by deep learning. InSatinder P . Singh and Shaul Markovitch, editors, Proceedings of the
Thirty-First AAAI Conference on ArtiÔ¨Åcial Intelligence, February 4-9,2017, San Francisco, California, USA , pages 1345‚Äì1351. AAAI Press,
2017.
[25] S. Haiduc, J. Aponte, L. Moreno, and A. Marcus. On the Use of
Automated Text Summarization Techniques for Summarizing SourceCode. In 2010 17th Working Conference on Reverse Engineering, pages
35‚Äì44, October 2010.
[26] Vincent J. Hellendoorn, Christian Bird, Earl T. Barr, and Miltiadis Al-
lamanis. Deep learning type inference. In Gary T. Leavens, AlessandroGarcia, and Corina S. Pasareanu, editors, Proceedings of the 2018 ACM
Joint Meeting on European Software Engineering Conference and Sym-posium on the F oundations of Software Engineering, ESEC/SIGSOFTFSE 2018, Lake Buena Vista, FL, USA, November 04-09, 2018, pages152‚Äì162. ACM, 2018.
[27] Vincent J. Hellendoorn and Premkumar Devanbu. Are deep neural
networks the best choice for modeling source code? In Proceedings of
the 2017 11th Joint Meeting on F oundations of Software Engineering,ESEC/FSE 2017, pages 763‚Äì773, New Y ork, NY , USA, August 2017.Association for Computing Machinery.
[28] Vincent J. Hellendoorn, Charles Sutton, Rishabh Singh, Petros Maniatis,
and David Bieber. Global relational models of source code. In8th International Conference on Learning Representations, ICLR 2020,Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020.
[29] Hamel Husain, Ho-Hsiang Wu, Tiferet Gazit, Miltiadis Allamanis, and
Marc Brockschmidt. CodeSearchNet Challenge: Evaluating the State ofSemantic Code Search. arXiv:1909.09436 [cs, stat], June 2020.
[30] T. D. Itoh, T. Kubo, K. Ikeda, Y . Maruno, Y . Ikutani, H. Hata,
K. Matsumoto, and K. Ikeda. Towards Generation of Visual AttentionMap for Source Code. In 2019 Asia-PaciÔ¨Åc Signal and Information
Processing Association Annual Summit and Conference (APSIPA ASC),pages 951‚Äì954, November 2019.
[31] Srinivasan Iyer, Ioannis Konstas, Alvin Cheung, and Luke Zettlemoyer.
Summarizing Source Code using a Neural Attention Model. In Proceed-
ings of the 54th Annual Meeting of the Association for ComputationalLinguistics (V olume 1: Long Papers), pages 2073‚Äì2083, Berlin, Ger-many, August 2016. Association for Computational Linguistics.
[32] Sarthak Jain and Byron C. Wallace. Attention is not Explanation.
arXiv:1902.10186 [cs], May 2019.
[33] M. Jiang, S. Huang, J. Duan, and Q. Zhao. SALICON: Saliency in
Context. In 2015 IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), pages 1072‚Äì1080, June 2015.
[34] J. Jiarpakdee, C. Tantithamthavorn, H. K. Dam, and J. Grundy. An
Empirical Study of Model-Agnostic Techniques for Defect PredictionModels. IEEE Transactions on Software Engineering, pages 1‚Äì1, 2020.
[35] Rafael-Michael Karampatsis, Hlib Babii, Romain Robbes, Charles Sut-
ton, and Andrea Janes. Big Code != Big V ocabulary: Open-V ocabularyModels for Source Code. In 2020 IEEE/ACM 42nd International
Conference on Software Engineering (ICSE), pages 1073‚Äì1085, October2020.
[36] M. G. Kendall. A New Measure of Rank Correlation. Biometrika,
30(1/2):81‚Äì93, 1938.
[37] Chris Lankford. Gazetracker: Software designed to facilitate eye
movement analysis. In Proceedings of the 2000 Symposium on Eye
Tracking Research & Applications, ETRA ‚Äô00, pages 51‚Äì55, New Y ork,NY , USA, November 2000. Association for Computing Machinery.
[38] Piyawat Lertvittayakumjorn and Francesca Toni. Human-grounded
Evaluations of Explanation Methods for Text ClassiÔ¨Åcation. In Pro-
ceedings of the 2019 Conference on Empirical Methods in NaturalLanguage Processing and the 9th International Joint Conference onNatural Language Processing (EMNLP-IJCNLP), pages 5195‚Äì5205,Hong Kong, China, November 2019. Association for ComputationalLinguistics.
[39] Shuai Lu, Daya Guo, Shuo Ren, Junjie Huang, Alexey Svyatk ovskiy,
Ambrosio Blanco, Colin Clement, Dawn Drain, Daxin Jiang, DuyuTang, Ge Li, Lidong Zhou, Linjun Shou, Long Zhou, Michele Tufano,Ming Gong, Ming Zhou, Nan Duan, Neel Sundaresan, Shao KunDeng, Shengyu Fu, and Shujie Liu. CodeXGLUE: A MachineLearning Benchmark Dataset for Code Understanding and Generation.arXiv:2102.04664 [cs], March 2021.
878[40] Rabee Sohail Malik, Jibesh Patra, and Michael Pradel. NL2Type:
Inferring JavaScript function types from natural language information.
InProceedings of the 41st International Conference on Software Engi-
neering, ICSE 2019, Montreal, QC, Canada, May 25-31, 2019, pages304‚Äì315, 2019.
[41] Thomas J. McCabe. A complexity measure. IEEE Transactions on
Software Engineering, 2(4):308‚Äì320, December 1976.
[42] George A. Miller. The magical number seven, plus or minus two: Some
limits on our capacity for processing information. Psychological Review,
63(2):81‚Äì97, 1956.
[43] Kyosuke Nishida, Itsumi Saito, Kosuke Nishida, Kazutoshi Shinoda,
Atsushi Otsuka, Hisako Asano, and Junji Tomita. Multi-style GenerativeReading Comprehension. In Proceedings of the 57th Annual Meeting
of the Association for Computational Linguistics, pages 2273‚Äì2284,Florence, Italy, July 2019. Association for Computational Linguistics.
[44] Jibesh Patra and Michael Pradel. Semantic bug seeding: a learning-
based approach for creating realistic bugs. In Diomidis Spinellis,Georgios Gousios, Marsha Chechik, and Massimiliano Di Penta, edi-tors, ESEC/FSE ‚Äô21: 29th ACM Joint European Software Engineering
Conference and Symposium on the F oundations of Software Engineering,Athens, Greece, August 23-28, 2021, pages 906‚Äì918. ACM, 2021.
[45] Norman Peitek, Janet Siegmund, Sven Apel, Christian K ¬®astner, Chris
Parnin, Anja Bethmann, Thomas Leich, Gunter Saake, and Andr ¬¥e
Brechmann. A look into programmers‚Äô heads. IEEE Transactions on
Software Engineering, 46(4):442‚Äì462, 2020.
[46] Michael Pradel and Satish Chandra. Neural software analysis. Commu-
nications of the ACM, 2021. To appear.
[47] Michael Pradel, Georgios Gousios, Jason Liu, and Satish Chandra.
TypeWriter: Neural type prediction with search-based validation. InProceedings of the 28th ACM Joint Meeting on European SoftwareEngineering Conference and Symposium on the F oundations of SoftwareEngineering, ESEC/FSE 2020, pages 209‚Äì220, New Y ork, NY , USA,November 2020. Association for Computing Machinery.
[48] Michael Pradel and Koushik Sen. DeepBugs: A learning approach to
name-based bug detection. Proceedings of the ACM on Programming
Languages, 2(OOPSLA):147:1‚Äì147:25, October 2018.
[49] Grusha Prasad, Yixin Nie, Mohit Bansal, Robin Jia, Douwe Kiela, and
Adina Williams. To what extent do human explanations of modelbehavior align with actual model behavior? arXiv:2012.13354 [cs],
December 2020.
[50] Arijit Ray, Yi Yao, Rakesh Kumar, Ajay Divakaran, and Giedrius
Burachas. Can Y ou Explain That? Lucid Explanations Help Human-AICollaborative Image Retrieval. Proceedings of the AAAI Conference on
Human Computation and Crowdsourcing, 7(1):153‚Äì161, October 2019.
[51] Laura Rieger, Chandan Singh, William Murdoch, and Bin Y u. Interpreta-
tions are Useful: Penalizing Explanations to Align Neural Networks withPrior Knowledge. In International Conference on Machine Learning,
pages 8116‚Äì8126. PMLR, November 2020.
[52] P . Rodeghero, C. Liu, P . W. McBurney, and C. McMillan. An
Eye-Tracking Study of Java Programmers and Application to SourceCode Summarization. IEEE Transactions on Software Engineering,
41(11):1038‚Äì1054, November 2015.
[53] Anna Rogers, Olga Kovaleva, and Anna Rumshisky. A Primer in
BERTology: What We Know About How BERT Works. Transactions
of the Association for Computational Linguistics, 8:842‚Äì866, 2020.
[54] Andrew Slavin Ross, Michael C. Hughes, and Finale Doshi-V elez. Right
for the Right Reasons: Training Differentiable Models by Constrainingtheir Explanations. In Proceedings of the Twenty-Sixth International
Joint Conference on ArtiÔ¨Åcial Intelligence, pages 2662‚Äì2670, Mel-bourne, Australia, August 2017. International Joint Conferences onArtiÔ¨Åcial Intelligence Organization.
[55] Zohreh SharaÔ¨Å, Bonita Sharif, Yann-Ga ¬®el Gu ¬¥eh¬¥eneuc, Andrew Begel,
Roman Bednarik, and Martha Crosby. A practical guide on conductingeye tracking studies in software engineering. Empirical Software
Engineering, 25(5):3128‚Äì3174, September 2020.
[56] Janet Siegmund, Norman Peitek, Andr ¬¥e Brechmann, Chris Parnin, and
Sven Apel. Studying programming in the neuroage: just a crazy idea?Communications of the ACM, 63(6):30‚Äì34, 2020.
[57] Ekta Sood, Simon Tannert, Diego Frassinelli, Andreas Bulling, and
Ngoc Thang Vu. Interpreting Attention Models with Human VisualAttention in Machine Reading Comprehension. In ACL SIGNLL Confer-
ence on Computational Natural Language Learning (CoNLL), October2020.[58] C. Spearman. The Proof and Measurement of Association between Two
Things. The American Journal of Psychology, 100(3/4):441‚Äì471, 1987.
[59] Michele Tufano, Jevgenija Pantiuchina, Cody Watson, Gabriele Bavota,
and Denys Poshyvanyk. On learning meaningful code changes via neuralmachine translation. In Proceedings of the 41st International Conference
on Software Engineering, ICSE ‚Äô19, pages 25‚Äì36, Montreal, Quebec,Canada, May 2019. IEEE Press.
[60] Ashish V aswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion
Jones, Aidan N. Gomez, ≈Åukasz Kaiser, and Illia Polosukhin. Attentionis All you Need. Advances in Neural Information Processing Systems,
30:5998‚Äì6008, 2017.
[61] Oriol Vinyals, Meire Fortunato, and Navdeep Jaitly. Pointer Networks.
Advances in Neural Information Processing Systems, 28, 2015.
[62] Yaza Wainakh, Moiz Rauf, and Michael Pradel. Idbench: Evaluating
semantic representations of identiÔ¨Åer names in source code. In 43rd
IEEE/ACM International Conference on Software Engineering, ICSE2021, Madrid, Spain, 22-30 May 2021, pages 562‚Äì573. IEEE, 2021.
[63] Yao Wan, Zhou Zhao, Min Yang, Guandong Xu, Haochao Ying, Jian Wu,
and Philip S. Y u. Improving automatic source code summarization viadeep reinforcement learning. In Proceedings of the 33rd ACM/IEEE In-
ternational Conference on Automated Software Engineering, ASE 2018,pages 397‚Äì407, New Y ork, NY , USA, September 2018. Association forComputing Machinery.
[64] Y u Wang, Ke Wang, Fengjuan Gao, and Linzhang Wang. Learning
semantic program embeddings with graph interval neural network. Pro-
ceedings of the ACM on Programming Languages, 4(OOPSLA):137:1‚Äì137:27, November 2020.
[65] Jiayi Wei, Maruth Goyal, Greg Durrett, and Isil Dillig. LambdaNet:
Probabilistic Type Inference using Graph Neural Networks. In Interna-
tional Conference on Learning Representations, September 2019.
[66] Sarah Wiegreffe and Y uval Pinter. Attention is not not Explanation.
arXiv:1908.04626 [cs], September 2019.
[67] Jian Zhang, Xu Wang, Hongyu Zhang, Hailong Sun, Yanjun Pu, and
Xudong Liu. Learning to handle exceptions. In IEEE/ACM International
Conference on Automated Software Engineering (ASE), 2020.
879