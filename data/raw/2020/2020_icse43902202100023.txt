SOAR: A Synthesis Approach
for Data Science API Refactoring
Ansong Ni‚àó
Yale University
New Haven, USA
ansong.ni@yale.eduDaniel Ramos‚àó
INESC-ID/IST, U. Lisboa, Portugal
Carnegie Mellon University, USA
danielrr@cmu.eduAidan Z.H. Yang
Queen‚Äôs University
Kingston, Canada
a.yang@queensu.caInÀÜes Lynce
INESC-ID/IST, U. Lisboa
Lisboa, Portugal
ines.lynce@tecnico.ulisboa.pt
Vasco Manquinho
INESC-ID/IST, U. Lisboa
Lisboa, Portugal
vasco.manquinho@tecnico.ulisboa.ptRuben Martins
School of Computer Science
Carnegie Mellon University
Pittsburgh, USA
rubenm@cs.cmu.eduClaire Le Goues
School of Computer Science
Carnegie Mellon University
Pittsburgh, USA
clegoues@cs.cmu.edu
Abstract ‚ÄîWith the growth of the open-source data science
community, both the number of data science libraries and the
number of versions for the same library are increasing rapidly.To match the evolving APIs from those libraries, open-sourceorganizations often have to exert manual effort to refactor theAPIs used in the code base. Moreover, due to the abundance ofsimilar open-source libraries, data scientists working on a certainapplication may have an abundance of libraries to choose, main-tain and migrate between. The manual refactoring between APIsis a tedious and error-prone task. Although recent research effortswere made on performing automatic API refactoring betweendifferent languages, previous work relies on statistical learningwith collected pairwise training data for the API matching andmigration. Using large statistical data for refactoring is not idealbecause such training data will not be available for a new libraryor a new version of the same library. We introduce Synthesis forOpen-Source API Refactoring (SOAR), a novel technique thatrequires no training data to achieve API migration and refac-toring. SOAR relies only on the documentation that is readilyavailable at the release of the library to learn API representationsand mapping between libraries. Using program synthesis, SOARautomatically computes the correct conÔ¨Åguration of argumentsto the APIs and any glue code required to invoke those APIs.SOAR also uses the interpreter‚Äôs error messages when runningrefactored code to generate logical constraints that can be usedto prune the search space. Our empirical evaluation showsthat SOAR can successfully refactor 80% of our benchmarkscorresponding to deep learning models with up to 44 layers withan average run time of 97.23 seconds, and 90% of the datawrangling benchmarks with an average run time of 17.31 seconds.
Index T erms‚Äîsoftware maintenance, program translation,
program synthesis
I. I NTRODUCTION
Modern software development makes heavy use of li-
braries, frameworks, and associated application programming
interfaces (APIs). Libraries provide modular functionality in-
tended for reuse, with prescribing a particular architecture [1],
and their widespread use has important productivity advan-tages [2]. The API for a library deÔ¨Ånes the interface, or
‚àóBoth authors contributed equally to this work.contract, between the (hidden) library implementation of apiece of library functionality, and its client component [3].Good API selection and maintenance is a key component ofmodern software engineering [4].
Although ideally API selection and usage could be stable
over the course of a software project‚Äôs lifetime, there are manypractical reasons that client code must update the way it usesa given API, or even which API/library it uses for a givenset of functionality. Broadly, software may evolve becauseof a change in the code, the documentation, its properties,or the customer-experienced functionality [5]. The APIs usedby the software can become invalid or inapplicable as thesoftware evolves. APIs themselves may become deprecatedor obsolete [6]. As a result, to maintain and optimize softwarethat depends on APIs, developers often have to refactorAPIs between different versions or to another API (i.e., APImigration) altogether.
API migration is a form of software refactoring, a critical
software engineering activity that is largely performed man-ually [7] and is tedious and often error-prone [8]. Migrationcan be difÔ¨Åcult even when migrating between two closely-related APIs that nominally provide the same functionality.For example, consider increasingly popular data science anddeep learning libraries, such as TensorFlow [9], PyTorch [10],and Numpy [11]. Moving between two such libraries oftenrequires signiÔ¨Åcant manual labor as well as domain- andlibrary-speciÔ¨Åc knowledge (we illustrate with an example inSection II); worse, APIs can change, and outdated historicalknowledge can exacerbate these challenges.
Fortunately, many popular APIs possess key properties that
can inform an automated approach to support migration orevolution. First, open-source APIs are often reasonably well-documented [12]. The quality, quantity, and structure of thatdocumentation can vary widely [13], but as code intendedto be called and reused by unrelated client applications,documentation is often key to successful API uptake [13].Second, unsuccessful API methods often raise exceptions
1122021 IEEE/ACM 43rd International Conference on Software Engineering (ICSE)
1558-1225/21/$31.00 ¬©2021 IEEE
DOI 10.1109/ICSE43902.2021.00023
with informative error messages, that developers can use to
access stack traces and information that can help them modify
a program [14]. We observe that data science API errormessages are particularly useful as these error messages oftenidentify how the input data relates to the raised exception. Takefor example ‚Äú Error in Ô¨Åt[5, 100]: subscript out of bounds ‚Äù,
which is an error message describing an index overÔ¨Çow. Fromthe example error message, we know that either 5 or 100 isout of bounds for the input matrix. Third, although multipleAPIs may vary in concrete implementation details, it is oftenpossible to discretely map between pieces of functionalitybetween source and target APIs (by virtue of solving the samegeneral sets of problems).
We propose SOAR (Synthesis for Open-source API Refac-
toring), a novel approach that combines natural languageprocessing (NLP) with program synthesis [15] to automaticallymigrate/refactor between APIs. We focus our approach and
evaluation on deep learning and data science APIs. Since there
are many APIs targeting these domains, changes and new
releases are introduced rapidly (as one example, TensorFlow
had 26 releases in 2019 alone), and switching between them iscommon and often tricky [16]. Moreover, data scientists andother users of such libraries have broad backgrounds and arenot always classically trained programmers, and thus could
particularly beneÔ¨Åt from tool support to assist them in thesetasks [8]. However, we believe the approach will generalize to
other APIs with similar properties (see detailed discussion in
Section IV).
Given a program that uses a given source API, SOAR‚Äôs
central proposition is to use NLP models learned over availableAPI documentation and error messages to inform programsynthesis to replace all source API calls with correspondingcalls taken from the target API. SOAR starts by using existingdocumentation for the source and target libraries to build anAPI matching model , which Ô¨Ånds likely replacement calls for
each API call in the source program.
However, simply Ô¨Ånding the right function in the corre-
sponding target API is not enough. The new function must becalled with the correct arguments, and function speciÔ¨Åcationsmay vary between libraries. SOAR uses program synthesis to
construct the full target method call in a way that replicatesthe original source behavior.
This synthesis step may be further informed by speciÔ¨Åca-
tions inferred, again, from the API documentation.
During the program synthesis enumeration procedure, a po-
tential migrated call may throw an error when tested. In thesesituations, SOAR uses an error message understanding model
that again uses NLP techniques to analyze error messages andgenerate logical constraints to prune the search space of the
synthesis task.
To the best of our knowledge, SOAR is the Ô¨Årst refactoring
tool that incorporates program synthesis and machine learningtools for refactoring, and is a signiÔ¨Åcant improvement over
the prior state of the art. SOAR maps programs between
different APIs using only readily-available documentation.It does not require manual migration mappings [17] or ahistory of previous migrations or refactorings in other software
projects [18]‚Äì[21]. Indeed, SOAR does not require trainingdata at all, and is thus applicable for migrations to a newlibrary or newer version of the same library shortly afterrelease. We demonstrate that SOAR is versatile in Section IV,
using it to migrate between two deep learning libraries ( i.e.,
TensorFlow to PyTorch) in the same programming language(i.e.,Python) and between two data manipulation libraries ( i.e.,
dplyr to pandas) in two different programming languages ( i.e.,
R and Python). Prior techniques either specialize exclusivelyin supporting cross-language migration ( e.g., StaMiner [19]),
or do not support it at all. Because SOAR uses synthesis, when
it succeeds the produced code is guaranteed to compile andpass existing test cases for the original source code.
In summary, our main contributions are:
1) We propose SOAR, a novel approach based on NLP and
synthesis for automatic API refactoring, focusing on (butnot limited to) deep learning and data science tasks.
2) SOAR requires no training data, and its output is guar-
anteed to compile and pass existing test cases. Instead of
using training data from prior programs, SOAR leveragesAPI documentation and program error messages to gener-ate logical constraints to prune the program enumerationsearch space.
3) We evaluate SOAR on two library migration tasks ( i.e.,
TensorFlow to PyTorch and dplyr to pandas) to demon-strate its effectiveness. Our results show that SOAR cansuccessfully migrate 80% of neural network programs
composed by 3 to 44 layers in with an average time of97.23 seconds. And for dplyr to pandas migration, 90%of benchmarks are solved on average in 17.31 seconds.
4) With ablation studies, we also evaluate how each part of
SOAR impacts its performance. We show that the use
of speciÔ¨Åcations from API documents and learning fromerror messages are largely helpful for the synthesis pro-
cess. We also show how different API matching methodsperform on the two migration tasks.
5) We release the SOAR implementation for the two migra-
tion tasks mentioned above. We also release the docu-mentation and benchmark tests we use in this work tofacilitate future research on this direction.
The remainder of this paper is organized as follows: Sec-
tion II presents a motivating example that illustrates the chal-lenges of manual API refactoring. In section III, we describeour approach to automatic API migration. Section IV presentsour empirical evaluation and analysis of results. Next, wediscuss our current approach and limitations in section V.Finally, we conclude with an overview of related work in
section VI and conclusions in section VII.
II. M
OTIV ATING EXAMPLE
We illustrate some of the difÔ¨Åculties of manual API refac-
toring via example. Consider the TensorFlow code snippet onthe left-hand-side of Figure 1. The program being refactoredshows an autoencoder program [22] written using the Tensor-Flow API; the goal is to migrate this code to use the PyTorch
113 !! 
 
"
" ' 
'! 
 '!
'  
"!
"%!(1!# ('$
"&!)1!# (,-/ $
#
##!
(1!)#
 1-+
" 1*
 1)$
#$
$!!-1!#$ 

 
!&
"'
 

 '! 

 '! 
 '!!
'  
	'
"
#!,1
!!# -+('$
# !-1
!!# ('(,-/ $
#!
,.!/1 	  !# '*()$
#%!01
!!)#
"
 1*)
"
 1-+
" 1#**$
 1#))$
 1#''$$
#&
$ !(*1
!!#$





#

#

Fig. 1: An example of how SOAR refactors a program written
with TensorFlow (left) to using PyTorch (right). Note that the whole
program consists of 15 APIs calls to TensorFlow, though we only
show four blocks of them ( i.e.,A, B, C and D) for brevity. SOAR
can migrate the full program in 161 seconds.
API. An autoencoder is a type of neural network that is trained
to copy its input to its output. SpeciÔ¨Åcally in this example, theautoencoder tries to compress an image with an encoder andthen the decoder will try to restore the original image.
The example in Figure 1 shows only a portion of the
program, for didactic purposes. To build the Ô¨Årst layer of
the encoder, function
Conv2D is called, which constructs a
convolution layer that can be applied to 2D images. Afterfurther (elided) activation and convolution layers, it calls
Dense to output a latent representation of the input image.
Decoding this output follows roughly the same procedure asthe encoding, but using
Conv2DTranspose instead of Conv2D .
The function ReLu appears in both the encoder (not shown)
and decoder, initializes a type of activation layer to ensurenon-linearity of the neural network.
The example of deep learning library code and transla-
tion in Figure 1 illustrates several of the core challengesin refactoring open-source APIs, as well as opportunities toinform an automated approach. First, the names of functioncalls implementing similar functionality may be very similaror even identical (such as those in blocks A, C, and D),or completely different ( e.g.,
Dense versus Linear in block
B). If a developer were performing this migration manually,they might reference the API documentation. For example,
the TensorFlow documentation describes the
Conv2D class
as a ‚Äú2D convolution layer ( e.g., spatial convolution over
images)‚Äù [23]; the corresponding PyTorch documentation forthe
Conv2d call describes it similarly, as a ‚Äú2D convolution
over an input signal composed of several input planes‚Äù [24].
Here, the function names map well, but when this does nothappen, it is more challenging to connect the documentation.
Even when we know which function to use, however, callsAlgorithm 1 SYNTHESIZER (I,S,T,C)
Input:I: existing program, S: source library, T: target li-
brary,C: test cases
Output: O: refactored program
1:/vectorr: API mapping = MAPAPI(T,S)
2:O=[]
3:for each l‚ààI do
4:O=O+[REFACTOR LINE(l,T,C,/vectorr)]
5:end for
that implement the same functionality can require different
types, parameters, parameter names, and even the parametervalues may be different between them. This is true for themajority of the calls in our example (see those in blocksA, B, and C). Note for example that the
Conv2D functions
take different parameters in each of the two libraries. Thereis overlap between them ‚Äî both include
kernel_size , and
stride and strides clearly correspond ‚Äî but even the in-
common parameters are not in the same argument positionbetween the two calls (
strides is the third parameter in
TensorFlow but stride is the fourth in PyTorch). Sometimes,
some or all of the arguments to a call in the source API can
be copied directly to the call in the target API (see the calls inblocks A, C); other times, correct arguments must be inferred
(such as the Ô¨Årst parameter to
Linear in block B). Finally,
in other situations, no single function in the target API canmatch the semantics of a call from the source API, requiring
instead a one-to-many mapping (as we see in converting the
Conv2DTranspose call in block C).
In the next section, we show how SOAR addresses these
challenges with each of its components.
III. R EFACTORING ALGORITHM
This section describes SOAR, our approach for automatic
API migration. We begin with a high-level overview of themethod (Section III-A) before providing more detail on indi-vidual components (Section III-B; III-C; III-D).
A. Overview
Figure 2 shows an overview of the SOAR architecture, while
Algorithm 1 provides an algorithmic view. SOAR takes asinput a program Iconsisting of a sequence of API calls from
a source library S, the source ( S) and target ( T) libraries and
their corresponding documentation, and a set of existing testcases (C). Since the user wants to refactor code from StoT,
we assume that the user already has test cases for Ithat can
be reused to check if the refactored code ( O) has the same
functional behavior has the original code ( I). Refactoring
proceeds one line at a time in I, Ô¨Ånding/constructing an
equivalent snippet of code (composed by one or more lines)that uses APIs of the target library T; the composition of all
these translated lines comprises the output O.
For each API call in the input program, the Ô¨Årst problem
either a developer or a tool must face is to identify methods
in the target API that implement the same functionality ( i.e.,
114	

	


	
 !$  	 	
 
 $
 $	

	
	

$

$	


 $#"
 $	  	 
    
	

		




	
Fig. 2: Overview of SOAR‚Äôs architecture.
Fig. 3: Description of the program parameters in torch.nn.Conv2d
documentation [24].
for a given set of input parameters, the target API call must
generate the same output). SOAR uses an API matching model
to identify target API calls. This model is built using NLP
techniques that analyze the provided API documentation for
each call, and provides a mapping ( /vectorrin Algorithm 1) that
computes the similarity between each target API function andeach potential source API function. SOAR uses this to Ô¨Ånd themost likely replacement methods in the target API for each
source API call in the input program. We provide additional
detail in Section III-B.
Given a potential match call in the target API, the next
step is to determine how to call it, in terms of providing
the correct parameters, in the correct order, of the correcttype. SOAR uses program synthesis to automatically writethe refactored API call, using the provided test cases to
deÔ¨Åne the expected behavior of the synthesized code and its
constituent parts. The synthesis process can be assisted with
additional automated analysis of API documentation, whichoften provides key information about each parameter, namely(1) whether it is required or optional, (2) its type, (3) its defaultvalue (if applicable), and (4) constraints between arguments,input and output ( e.g., input and output tensor shapes). Fig-
ure 3 shows a snippet of the descriptions of all parametersfor
torch.nn.Conv2d . For example, the parameter stride is
optional; it takes type intortuple , and its default value is 1.
Analysis of this documentation can produce a speciÔ¨Åcation
constraint for the stride parameter, assisting the program
synthesis task. Section III-C describes the synthesis step.
Given a potential rewrite in the target API, a natural step
for a developer would be to run the refactored code on testinputs. Unsuccessful runs can be quite informative, because
many APIs (especially in the deep learning and data sciencedomains) provide error messages that can be very helpful fordebugging. SOAR simulates the manual debugging processby Ô¨Årst adapting the input whole-program test cases to testpartially refactored code, and then extracting both syntacticand semantic information from any error messages observedwhen running them. SOAR uses this information to add newconstraints to the iterative synthesis process (Section III-D).
After migrating all calls in the source API to the target API
such that all input tests pass, SOAR outputs a fully refactored
program. Subsequent sections provide additional detail on thepreviously described steps.
B. API Representation Learning and Matching
The Ô¨Årst step in migrating a call in a source API is to
identify candidate replacement calls in the target API withsimilar semantics. The API matching model supports this task
by analyzing the prose documentation associated with eachcall in each API, and computing similarity scores between allAPI pairs. At a high level, this model embeds each API method
call in a source and target library into the same continuous
high-dimensional space, and then computes similarity betweentwo calls in terms of the distance between them in that space.We explored two ways on obtaining API representation: TF-IDF (term frequency ‚Äì inverse document frequency) [25] and
pretrained word embeddings [26].
TF-IDF . The intuition behind TF-IDF is to Ô¨Ånd the most
representative words rather than the most frequent words in
a sentence. Normalizing by the inverse-document-frequency
lowers the weights of common keywords that are less informa-tive, such as torch ,tensorÔ¨Çow and those stop words in natural
language such as theorthis.
SpeciÔ¨Åcally, we Ô¨Årst derive a bag-of-words representation x
i
from a description of an API call after some stemming of the
words with the Snowball Stemmer [27]. xi=[xi
1,xi2,...,xi
n]
wherexi
jdenotes the frequency with which word xjappeared
in the sentence xi, andnis the size of the vocabulary from
the descriptions of all APIs we are trying to embed. A TF-IDFrepresentation of the call is computed as Equation 1:
TF-IDF (x
i)=/bracketleftbiggxi
1/summationtextm
t=0xt
1,xi
2/summationtextm
t=0xt
2,...,xi
n/summationtextm
t=0xtn/bracketrightbigg
(1)
115However, the major downside of TF-IDF is that it does not
encode the similarities between words themselves. For exam-ple, consider two hypothetical call descriptions: (1) Remove
the last item of the collection , and (2) Delete one element
from the end of the list . They are semantically similar but since
they have minimal overlapping words, a TF-IDF representationmethod would not recognize these two API calls as similar.
TÔ¨Ådf-GloVe. We can extend the TF-IDF representation to rec-
ognize similar words by adding pretrained word embeddings.SpeciÔ¨Åcally, we propose to use the GloVe embedding [26],which is trained on a very large natural language corpus andlearns to embed similar words closer in the embedding space.
To obtain sentence embeddings from individual words, we
perform a weighted average of the word embeddings and usethe TF-IDF scores of individual words as weight factors. It is
a simple yet effective method to obtain sentence embedding
for downstream tasks, as noted by previous work [28], [29].This is shown in detail as Equation 2, where w
jis the vector
encoding the GloVe embedding of word xj:
Embedding (xi)=n/summationdisplay
i=jxi
j¬∑wj/summationtextm
t=0xt
j(2)
By including the GloVe embedding, word similarity is
preserved; by including the TF-IDF terms, the inÔ¨Çuence ofembeddings of common words is greatly reduced. However,GloVe is trained with Common Crawl [30] which contains rawwebpages, which is a mismatch from our domain of textual
data ( i.e.,data science and programming).
API matching. Given the representation of two APIs Rep (x
i),
Rep(xj)in the same space Rep (¬∑), we compute their similarity
with cosine distance:
sim(Rep(xi),Rep(xj)) =Rep(xi)¬∑Rep(xj)
|Rep(xi)||Rep(xj)|(3)
For computational efÔ¨Åciency, we pre-compute the similarity
matrix between the APIs across the source and target library.
So we will be able to query the most similar API for thesynthesizer to synthesize its parameters on the Ô¨Çy.
C. Program Synthesis
Given the input test cases and the API matching model
providing a ranked list /vectorrof APIs in the target library, the
synthesis model automatically constructs new, equivalent code,of one or more lines, that uses APIs of the target library T.
The refactored program Ohas the same functionality as input
programI, and passes the same set of tests C.
To refactor each line of the existing program I, we use
techniques of programming-by-example (PBE) synthesis [31].
PBE is a common approach for program synthesis, wherethe synthesizer takes as speciÔ¨Åcation a set of input-output
examples and automatically Ô¨Ånds a program that satisÔ¨Åes thoseexamples. In the context of program refactoring, our examples
correspond to the test cases for the existing code. In this paper,we restrict ourselves to straight-line code where each line
returns an object that can be tested. With these assumptions,Algorithm 2 REFACTOR LINE(l,T,C,/vectorr)
Input:l: line of code from I,T: target library, C: test cases,
/vectorr: ranked list of API matchings
Output: R: refactored snippet
1:for each a‚àà/vectorr[l]do ‚äøais a target API
2:/vectors=GENERATE SKETCHES (a,T)
3: for each s‚àà/vectorsdo
4: R=FILLSKETCH (s)
5: ifPASS TESTS (R,C)then
6: returnR
7: end if
8: end for
9:end for
we can automatically generate new test cases for each line k
of program I. This can be done by using the input of the
existing tests, running them, and using the output of line kas
a new test case for the program composed by lines 1tok.
Our program synthesizer for refactoring of APIs is presented
in Algorithm 2 and it is based on two main ideas: (i) programsketching, and (ii) program enumeration. For each line l
in program I, we start by enumerating a program sketch
(i.e.,program with holes) using APIs from the target library
T(line 2). For each program sketch, we perform program
enumeration on the possible completion of the API parameters(line 4). For each complete program, we run the test cases forthe program up to line l. If all test cases succeed, then we
found a correct mapping for line lbetween libraries Sand
T(line 5). Otherwise, we continue until we Ô¨Ånd a complete
program that passes all test cases.
Program Sketching. Program sketching is a well-known
technique for program synthesis [32] where the programmerprovides a sketch of a program and the program synthesizerautomatically Ô¨Ålls the holes in this sketch such that it satisÔ¨Åes agiven speciÔ¨Åcation. We refactor one line of program Iat each
time. Our Ô¨Årst step is to use the ranked list of APIs to create
a program sketch where the parameters are unknown. For
instance, consider the Ô¨Årst layer from the motivating examplethat shows the network for an autoencoder using TensorFlow:
tf.keras.layers.Conv2D
(filters=32, kernel_size=3,strides=(2, 2))
A possible sketch for this call using PyTorch is:
torch.nn.Conv2d(#1,#2,(#3,#4),stride=(#5,#6),padding=(#7,#8))
Where holes #ihave to be Ô¨Ålled with a speciÔ¨Åc value for
the APIs to be equivalent. This approach works for one-to-one
mappings but would not support common one-to-many map-
pings where the parameters often need to be transformed be-fore being used in the new API. This is the case of the previous
API where a reshaping operation must be performed before
calling the PyTorch API. To support this common behavior,we include in our program sketch one API from the target
libraryTand common reshaping APIs ( e.g., permute, long).
The sketch that corresponds to the refactoring solution of
116theConv2D API from TensorFlow uses a reshaping API before
calling the Conv2d API from PyTorch:
lambda x: x.permute(#9,#10,#11,#12)
torch.nn.Conv2d(#1,#2,(#3,#4), stride=(#5,#6),padding=(#7,#8))
Using Occam‚Äôs razor principle, our program synthesizer
enumerates program sketches of size 1and iteratively increases
the size of the synthesized program up to a speciÔ¨Åed limit.
Program Enumeration. For each program sketch P, our
program synthesizer enumerates all possible completions foreach hole. Since each hole has a given type, we only want toenumerate well-typed programs. We encode the enumerationof well-typed programs into a SatisÔ¨Åability Modulo Theories(SMT) problem using a combination of Boolean logic and
Linear Integer Arithmetic (LIA). This encoding is similar to
other approaches that use SMT-based enumeration for programsynthesis [33], [34] and encodes the following properties:
‚Ä¢Each hole contains exactly one parameter;
‚Ä¢Each hole only contains parameters of the correct type.
A satisfying assignment to the SMT formula can be trans-
lated into a complete program. The types for each hole can bedetermined by extracting this information from documentation,by performing static analysis, or by having this informationmanually annotated in the APIs. The available parameters andtheir respective types can be extracted automatically from theparameters used in the k-th line of program Iand by any
default parameters that can be used in the API from Tthat
appears in the program sketch P. For instance, for the
Conv2d
example presented in this section, we consider as possible
values for the holes, the values that appear in the existing code(32, 3, 2) and default values for integer parameters (-1, 0, 1,2, 3) that are automatically extracted from documentation.
Encoding the enumeration of well-typed programs in SMT
has the advantage of making it easier to add additional logicalconstraints that can prune the search space.
SpeciÔ¨Åcation Constraints. As we described in Section III-A,
API documentation often provides additional useful informa-tion about parameters to function calls, including type anddefault values. For each considered API call, we scrape/pro-cess the associated documentation to extract these propertiesand encode them as SMT constraints to further limit thesynthesizer search space.
Additionally, some APIs have complex relationships be-
tween parameters which if encoded into SMT may reduce thesearch space considerably. For instance, Figure 4 shows the
relationship between the different parameters for the
Conv2d
API described in PyTorch documentation. For APIs with these
kinds of shape constraints, we can encode these relationshipsinto SMT to further prune the number of feasible completions.When we use these relationships in our experiments, weencode them manually (a one-time cost for an actual SOARuser or API maintainer), but we observe that in many casesthey could be automatically extracted from documentation.
Besides these speciÔ¨Åcation constraints, we can also further
prune the search space by using the error messages provided
Fig. 4: Relationship between the parameters of Conv2d API de-
scribed in PyTorch documentation [24].
Trying to create tensor with negative dimension -2: [40, -2, 3, 3]
Hyponym 1torch.nn.Conv2d(-2,40,(3,3),stride=(1,1),padding=(0,0))
POS = JJ Target Param POS = NN
in_channels > 0['in_channels= -2', 'out_channels=40', 'kernel_size=(3,3)', 
'stride=(1,1)', 'padding=(0,0)']
If pass: generate SMT constraint Step 2. Match candidate faulty pa rameter with program parameter sCompile program and generate error message
Trying to create tensor with negative dimension -2: [40, -2, 3,  3]
Step 1. Collect candidate faulty parameters and fault causes
Step 3. Mutate program   
self.var5 = torch.nn.Conv2d(1,40, (3,3),stride=(1,1),padding=(0, 0))If fail
Fig. 5: Example error message to SMT constraint pipeline using
hyponym 1.
by the Python interpreter, as we discuss in the next section.
D. Error Message Understanding
We use a combination of extracting hyponymy relations
and Word2vec [35] to understand run-time error messages. Asoutlined in Figure 5, our SMT constraint generation methodconsists of three steps.
Step 1: Extract hyponymy relation candidates from error
messages. We perform an automatic extraction of customized
hyponyms on each error message. Hyponyms are speciÔ¨Åclexical relations that are expressed in well-known ways [36].In encoding a set of lexico-syntactic patterns that are easilyrecognizable ( i.e., hyponyms), we avoid the necessity for
semantic extraction of a wide-range of error message text. Wethen use the collected hyponyms to map the error message toa single faulty parameter, and output a SMT constraint basedon the faulty parameter.
Prior work on text parsing uses Tregex, which is a utility
developed by Levy and Andrew for matching patterns in
constituent trees [37]. For example, Evans et al. evaluated
the performance of Tregex on privacy policies [38]. However,Deep Learning (DL) API compilation error messages aredomain speciÔ¨Åc. Sumida et al. used the hierarchical layout
of Wikipedia articles to identify hyponymy relations [39].
Similarly to Wikipedia documents, DL API compilation error
117TABLE I: The four hyponyms in the error message understanding model
Type NP Example error messages IdentiÔ¨Åed hyponym
1 {/angbracketleftNoun/angbracketright‚àó/angbracketleftP reposition /angbracketright/angbracketleftAdjective /angbracketright?/angbracketleftNoun/angbracketright} ‚ÄòTrying to create tensor with negative
dimension -1: [-1, 100, -1, -1]‚Äôtensor with negative dimension
2 {/angbracketleftNoun/angbracketright/angbracketleftCardinal number/angbracketright} ‚Äòembedding(): argument weight (posi-
tion 1) must be Tensor, not int‚Äôposition 1
3 {/angbracketleftCoordinating conjunction /angbracketright/angbracketleftVe r b/angbracketright/angbracketleftAdjective /angbracketright/angbracketleftNoun/angbracketright} ‚ÄòExpected 3-dimensional input for 3-
dimensional weight [2, 2, 3], but got 4-dimensional input of size [100, 50, 40,1] instead‚Äôbut got 4-dimensional input
4 {/angbracketleftVe r b/angbracketright/angbracketleftAdverb/angbracketright/angbracketleftVe r b
past participle /angbracketright} ‚Äònon-positive stride is not supported‚Äô is not supported
messages are more consistent and organized than normal, nat-
ural language, documents. Therefore, we follow the approachof extracting hyponymy relations based on the hierarchicallayout of a string.
We propose a set of four lexico-syntatic patterns to identify
hyponyms using noun-phrases (NP) and regular expressions
frequently appearing in machine learning API error messages.Table I shows the four hyponyms. If we identify any of thefour lexico-syntatic patterns within an error message, we tagthe error message with a hyponym type. As shown in Figure
5, we identify hyponym 1 in error message ‚Äú Trying to create
tensor with negative dimension... ‚Äù.
Step 2: Identify candidate faulty parameters and con-
straints. Step 2 uses different keywords based on the result
of step 1 to identify the faulty parameter. As shown in Figure5, an error message with hyponym 1 is likely to have thePOS=JJ word as a parameter constraint ( i.e.,word ‚Äú negative ‚Äù).
Based on the fault cause candidate, we then store all negativenumbers as candidate faulty parameters ( e.g., [40, -2, 3, 3]
has -2 as the only faulty parameter). We then vectorize thecandidate faulty parameter name ( i.e.,-2) and Ô¨Ånd the program
parameter name with the closest vectorized distance. As shownin Figure 5, the parameter ‚Äú in
channels =‚àí2‚Äù has the
nearest vectorized distance to the candidate faulty parameter -
2. Based on the fault cause, we generate a candidate constraint.
The example error message in Figure 5 has only one candidateconstraint: ‚Äú in
channels > =0‚Äù.
Step 3: Mutate program. To validate the candidate faulty
parameters and constraints, we mutate each faulty parameteraccording to each faulty parameter and constraints pair. We
then re-compile the program for each mutation. If the errormessage remains the same, we discard the faulty parameterand constraint pair as a candidate. If the program passes, or
if the error message changes, we store the faulty parameter
and constraint pair as an SMT constraint. As shown in
Figure 5, the API call mutator mutates the second parame-
ter (‚Äúin
channels =‚àí2‚Äù) to a non-negative number. The
mutator Ô¨Årst attempts ‚Äú inchannels =0‚Äù and it encounters
a different error message. From the new error message, wemutate this parameter to ‚Äú in
channels =1‚Äù and observe no
further errors. Therefore, we reÔ¨Åne our previous constraint tobe ‚Äúin
channels > 0‚Äù, and store it as the Ô¨Ånal SMT constraint
for the program in Figure 5.IV . E V ALUATION
We selected two migrations tasks: TensorFlow to PyTorch
and dplyr to pandas. We believe that these two migration tasksare representative of the needs of the data-science community.Indeed, TensorFlow and PyTorch are the two most populardeep learning frameworks, and recent trends indicate that alarge portion of TensorFlow user-base is shifting to PyTorch
[40]. We thus chose it as an indicative, relevant task. Similarly,
dplyr is one of the top-5 most downloaded R libraries; pandas
is its python counterpart.
To evaluate our approach we answer the following research
questions:
Q1. How effective is SOAR at migrating neural network
programs between different libraries?
Q2. How does each component of SOAR impact its perfo-
mance?
Q3. Is SOAR generalizable to domains besides deep learning
library migration?
A. Benchmarks and experimental setup
We collected 20benchmarks for each of the two migration
tasks. In particular, for the TensorFlow to PyTorch task,
we gathered 20 neural network programs from tensorÔ¨Çow
tutorials [41], off-the-shelf models implemented with Tensor-Flow [42] or its model zoo [43]. This set of benchmarksincludes: Autoencoders for image and textual data, classicfeed-forward image classiÔ¨Åcation networks ( i.e., the VGG
family, AlexNet, LeNet, etc), convolutional network for text,
among others. The average number of layers in our benchmark
set is 11.80¬±11.52, whereas the median is 8. Our largest
benchmark is the VGG19 network which contains 44layers.
For the domain of table transformations, we collected 20
benchmarks from Kaggle [44], a popular website for data
science. The programs in the benchmark set have an averageof3.05¬±1.07lines of code, and a median of 3lines.
Although the programs considered for this task are relativelysmall compared to the deep learning benchmarks, they are
still relevant for data wrangling tasks as shown by previous
program synthesis approaches [45].
Each benchmark is also associated with a set of input-output
examples (i.e., test cases) used to decide migration success.For the deep learning task, the test cases are automaticallygenerated by running the original neural network on randominputs. Whereas the test cases for the dplyr to pandas task areuser provided.
118TABLE II: Execution time for the deep learning library
migration task in each of the 20benchmarks.
SOAR SOAR w/o Specs. SOAR w/o Err. Msg.
conv pool softmax(4L) 1.60 23.02 14.35
img classiÔ¨Åer(8L) 12.82 336.00 65.66
three linear(3L) 3.18 2.34 21.07
embed conv1d linear(5L) 5.27 123.85 16.90
word autoencoder(3L) 1.81 1.46 2.64
gan discriminator(8L) 12.80 timeout 252.20
two conv(4L) 16.69 timeout 15.09
img autoencoder(11L) 160.97 391.09 487.54
alexnet(20L) 425.22 timeout 66.13
gan generator(9L) 412.47 timeout timeout
lenet(13L) 280.91 timeout timeout
tutorial(10L) 6.04 timeout 58.29
conv fortext(11L) 9.04 timeout 32.29
vgg11(28L) 40.83 timeout 132.67
vgg16(38L) 82.05 timeout 139.27
vgg19(44L) 83.99 timeout 189.90
densenet main1(5L) timeout timeout timeout
densenet main2(3L) timeout timeout timeout
densenet conv block(6L) timeout timeout timeout
densenet trans block(3L) timeout timeout timeout
All results presented in this section were obtained using an
Intel(R) Xeon(R) CPU E5-2630 v2 @ 2.60GHz, with 64GBof RAM, running Debian GNU/Linux 10, and a time limit
of 3600 seconds. To evaluate the impact of each component
in SOAR, we run four versions of the tool. SOAR with TF-IDF (SOAR w/ TF-IDF) and SOAR with tÔ¨Ådf-GloVe (SOARw/ TÔ¨Ådf-GloVe) to evaluate the impact of API representationlearning methods. SOAR without speciÔ¨Åcation constraints(SOAR w/o Specs.) and SOAR without error message under-standing (SOAR w/o Err. Msg.) to evaluate the impact of thesecomponents on the performance of SOAR.
B. Implementation
The SOAR implementation integrates several technologies.
Scrapy [46], a Python web-scraping framework, is used tocollect documentation for the four libraries in our experiments.To enumerate programs in the synthesis step, we use the Z3SMT solver [47]. For each target program call parameter, weextract an answer for the four parameter questions in Section
III-A and generate corresponding SMT constraints. In both
API matching model and the error message understanding
model, the GloVe word embeddings [26] are used as an
off-the-shelf representation of words. For the four librariesappearing in our two evaluation migration tasks, we useTensorFlow 2.0.0, PyTorch 1.4.0, dplyr 1.0.1 (with R 4.0.0)
and pandas 1.0.1, though our proposed method and associated
implementation do not rely on speciÔ¨Åc versions. We providea replication package, including benchmarks, source code andvirtual environment to run SOAR.
1
C. Q1: SOAR effectiveness
Table II shows how long it takes to migrate each of the
deep learning models from TensorFlow to PyTorch, using
the various approaches. Our best approach (shown as SOAR)
successfully migrates 16of the 20DL models with a mean
1https://zenodo.org/record/4452730run-time of 97.23¬±141.58seconds, and a median of 14.76sec-
onds. The average number of lines in the 16benchmarks that
we successfully migrate is 13.6¬±12.14, whereas the average
number of lines in the output programs is 18.56¬±16.40. The
reason the number of synthesized lines is higher than those inthe original benchmarks is that we frequently do one-to-manymappings. In fact, 15out of the 16require at least one mapping
that is one-to-many. In the 16benchmarks, SOAR tests on
average 4414.18¬±5676 refactor candidates (i.e. program
fragments tested for each mapping), and it needs to test amedian 2111 candidates before migrating each benchmark.
The reason 4benchmarks timeout is that in each of these
benchmarks there is at least one API in the benchmark thathas a poor ranking ( i.e.,not in the top 200).
D. Q2: performance of each SOAR component
We perform an ablation study to understand the effective-
ness of several features in the SOAR design.
Embeddings. In Table III, we show the execution time and
average ranking for the correct API matchings for each bench-mark, using different API representation learning methods,
namely TF-IDF and tÔ¨Ådf-GloVe, as described in Section III.
We can see that for these tasks of TensorFlow to PyTorchmigration, using TF-IDF-based API matching model worksbetter than adding pretrained GloVe embeddings. We believethis is because similar APIs are often named with samewords( e.g.,
Conv2DTranspose vs.ConvTranspose2d )o re v e n
identical name ( e.g., the APIs of creating a RectiÔ¨Åed Linear
Unit are both named as ReLU(...) ), for TensorFlow and
PyTorch. Thus simple word matching method like TF-IDF
is sufÔ¨Åce for API matching purposes. However, things are
different for the second task we consider (see Section IV-E
for more details).
Another interesting result worth noticing is that although
the synthesis time differs for the two approaches, the averagerankings are quite similar for most of the benchmarks. Thereason is that despite the average rankings of correct targetAPIs being similar, the incorrect APIs ranked by the model
before the correct one is different, and the time it takes to rule
out those incorrect APIs varies greatly, determined largely bythe number of parameters required for that API.
Error Message Understanding. As shown in Table II, SOAR
performs signiÔ¨Åcantly better when using the error messageunderstanding model. We can observe that without this com-ponent, two of the benchmarks that SOAR could solve wouldtimeout at the 1 hour mark. For the 14benchmarks it still
manages to solve, the synthesis time increases on average4.66√ó.
The number of performed evaluations also increase substan-
tially for each benchmark. For the 16benchmarks that SOAR
successfully migrates, we evaluate an average of 43319.63¬±
61259.62refactor candidates without the error message under-
standing model. This corresponds to a 9.81√óincrease in the
number of necessary evaluations when compared to the full
119TABLE III: Execution time and average API ranking for each
of the 20benchmarks using TF-IDF and GloVe models.
SOAR w/ TF-IDF SOAR w/ TÔ¨Ådf-GloVe
Time(s) Avg. Ranking Time(s) Avg. Ranking
conv pool softmax(4L) 1.60 1.0 1.56 1.0
img classiÔ¨Åer(8L) 12.82 2.8 31.04 2.8
three linear(3L) 3.18 8.0 7.70 8.0
embed conv1d linear(5L) 5.27 2.4 7.75 2.4
word autoencoder(3L) 1.81 1.0 1.52 1.0
gan discriminator(8L) 12.80 3.5 37.01 2.8
two conv(4L) 16.69 1.0 13.75 1.0
img autoencoder(11L) 160.97 1.9 166.34 1.9
alexnet(20L) 425.22 2.0 428.42 2.0
gan generator(9L) 412.47 2.1 1892.86 2.6
lenet(13L) 280.91 4.3 timeout 89.1
tutorial(10L) 6.04 2.4 21.31 2.4
conv fortext(11L) 9.04 2.3 14.08 2.3
vgg11(28L) 40.83 1.8 73.92 1.8
vgg16(38L) 82.05 1.6 114.41 1.6
vgg19(44L) 83.99 1.5 114.98 1.5
densenet main1(5L) timeout 172.8 timeout 285.6
densenet main2(3L) timeout 10.0 timeout 285.5
densenet conv block(6L) timeout 293.3 timeout 634.0
densenet trans block(3L) timeout 291.0 timeout 662.7
SOAR method. In summary, we can signiÔ¨Åcantly reduce the
search space by interpreting error messages.
SpeciÔ¨Åcations Constraints. In Table II, we also show the
impact of speciÔ¨Åcation constraints that describe the rela-tionship between different parameters of a given API (seeSection III-C for details). Even though, we only have thesecomplex speciÔ¨Åcations for the 7most common APIs, the im-
pact on performance is signiÔ¨Åcant. Without these speciÔ¨Åcationwe can only solve 6out of 20benchmarks. Relating the
arguments of the APIs helps SOAR to signiÔ¨Åcantly reduce thenumber of argument combinations that it needs to enumerate.
E. Q3: SOAR generalizability.
Our experiments so far concern deep learning library mi-
gration in Python. To study the generality of our proposedmethod, we applied SOAR to another task of migrating from
dplyr, a data manipulation package for R, to pandas, a Python
library with similar functionality. Fig. 7 shows how the twoAPI matching methods perform in this domain. While withTÔ¨Ådf-GloVe, 30% of the correct APIs are ranked among thetop 5, saving lots of evaluations for the synthesizer, none of
the correct APIs are ranked by the TF-IDF-based matcher asits Ô¨Årst 5 choices. Worse, nearly half of those are ranked
above 100, making the synthesis time almost prohibitivelylong. We believe this is because the lexical overlap between the
names of similar APIs in those two libraries is much smallercompared to the deep learning migration task. For example,dplyr‚Äôs
arrange and panda‚Äôs sort_values provide the same
functionality (they both sort the rows by a given column), but
the function names are different. In this way, TÔ¨Ådf-GloVe cantake advantage of the pretrained embeddings to explore thesimilarities between APIs beyond simple TF-IDF matching.
In Figure 6, we show the time it takes to migrate each
of the 20benchmarks with a timeout of 3600 seconds when
using word embeddings. We solve 18out of 20collected
benchmarks in under 102.5seconds. The average run time1 5 10 15 20100102104
3600 seconds timeout
Instances solvedTime (s)
Fig. 6: Execution time for each benchmark of the dplyr-to-
pandas task with a timeout of 3600 seconds.
1-10 11-100 101+05101520
Average ranking# benchmarksTF-IDF
TÔ¨Ådf-GloVe
Fig. 7: Average ranking of the APIs for each of the 20 dplyr-
to-pandas benchmarks.
for18benchmarks is 17.31¬±22.59seconds and a median
of12.19seconds. Note that for this task we did not consider
error messages, nor speciÔ¨Åcations since we wanted to test howa basic version of SOAR would behave in a new domain.Moreover, for this domain, all the refactored benchmarksonly used one-to-one mappings since no additional reshaping
was needed before invoking pandas APIs. Even with theseconditions, we show that we are able to successfully refactorcode for a new domain across different languages.
V. L
IMITATIONS AND DISCUSSION
Here we discuss the main limitations of our method and
possible challenges for extending SOAR‚Äôs ability to refactornew APIs, even potentially beyond the domain of data science.
Benchmarks. Our evaluation of SOAR uses benchmarks from
well-known deep learning tutorials and architectures. However,they are all feed-forward networks, effectively sequences ofAPI calls where the output of the current layer is the input ofthe next layer. There may be more applications that share thisfeature, but support for more complex structure is likely nec-essary to adapt to other domains. Additionally, and naturally,the APIs in the benchmarks we collected may be biased andnot reÔ¨Çect the set of APIs developers actually use.
To assess this risk, we checked the degree to which the APIs
used in our benchmarks appear to be widely used on other
open-source repositories on GitHub. To do this, we collected
the top 1015 starred repositories that have TensorFlow as atopic tag, which contains over 8 million lines of code andover 500K TensorFlow API calls. We found that 76% of the1000+ repositories use API calls included in our benchmarks
at least once, which validates some representativeness of ourcollected benchmarks.
120Automatic testability. One beneÔ¨Åt of the data science/scien-
tiÔ¨Åc computing domain is that much of the input, output, and
underlying methods are typically well-deÔ¨Åned. As a result, itis particularly easy to test and verify the correctness of indi-vidually migrated calls, which can be processed in sequence.There may be other types of libraries that share these types ofcharacteristics, like string manipulation or image processinglibraries, whose intermediate outputs are strings/images. Wealso assume user-provided tests. Given the migration task, itis reasonable to assume the user has tests (the code must besufÔ¨Åciently mature to justify migrating, after all), but a moregeneral solution might beneÔ¨Åt from automatically generatingtests, which would both alleviate the input burden on the
user and, potentially, reduce the risks of overÔ¨Åtting. In our
current implementation, we moreover use the provided teststo construct smaller test cases for each mapping. This isparticularly easy in this domain, because data science anddeep learning API calls are often functional in their paradigm.
Adapting the technique to other paradigms would require more
complex test slicing or generation to support synthesis.
API Matching. Using the GloVe model for the API Matching
often results in out of vocabulary problems because of API
names and descriptions often use of data science speciÔ¨Åcterminologies, especially acronyms and abbreviations (e.g.
‚Äúconv2D‚Äù for 2-dimensional convolution, ‚ÄúLSTM‚Äù for long-short-term-memory). The GloVe model is not trained for thisdomain, and the out of vocabulary problems explain the
limited success of the TÔ¨Ådf-GloVe model when compared to a
plain TF-IDF. To address this problem, it would be necessaryto train a new set of embeddings with focus on data-sciencejargon, which is out of scope.
Error message understanding. The error message under-
standing model is built on four domain speciÔ¨Åc lexico-syntaticpatterns, which we identify as hyponyms when they appear inan error message. We propose the hyponyms based on thespeciÔ¨Åc syntax of DL API error messages, thus take non-
trivial human effort to make it generalize to error messages
that appear when calling APIs from libraries of other domains.However, we believe the idea of program mutation (Step 3 ofFig. 5) is still widely applicable for the purpose of generatingSMT constraints when dealing with error messages.
Synthesis. Our approach supports one-to-many mappings but
it restricts the mapping to oneAPI of the target library and
one or more reshaping APIs. However, this could be extended
to include many APIs of the target library at the cost of slower
synthesis times. An additional challenge is to support many-to-one or many-to-many mappings since this would requireextending our synthesis algorithm. However, even with thecurrent limitations, our experimental results show that the
current approach can solve a diverse number of benchmarks.
Generalizability. SOAR applies best to well-documented
APIs with easily decomposable tests (i.e., calls have well-
deÔ¨Åned semantics and limited side effects). Deep Learningand Data Science APIs have these properties, and are popular,rapidly evolving, and used by programmers with a variety
of backgrounds. We focus on them in the interest of im-pact. SOAR likely generalizes easily to domains that sharethese properties, like string or image manipulation libraries.Nonetheless, SOAR always requires a one-time effort to beinstantiated in any domain. SpeciÔ¨Åcally, SOAR needs: (1) acrawler and a parser to collect documentation used to build
the API matching model and speciÔ¨Åcation constraints; this step
can be facilitated with tools like python‚Äôs built-in function help
if API‚Äôs are well-documented; (2) an error message messageunderstanding model (which can be simply based on phrase
structure rules). We do not study the effort needed to provide
these two requirements; however, we believe it is signiÔ¨Åcantlylower than building a static migration tool from scratch.
Correctness. Since we evaluate our migration tasks using test
cases, it is always possible for our approach to overÔ¨Åt to
these tests. However, this threat can be mitigated if the user
provides a sufÔ¨Åciently robust test set that provides enough
coverage. Additionally, code written to different APIs may befunctionally equivalent, but demonstrate different performancecharacteristics, which we do not evaluate. However, this factis one reason users might Ô¨Ånd SOAR useful in the Ô¨Årst place:
a desire to migrate code from one library to another that is
more performant for the given use case.
Overall, we focus our design and evaluation on deep learn-
ing and data science libraries. These libraries have propertiesthat render them well-suited to our task in terms of commonprogramming paradigms, and norms, such as in the APIdocumentation. However, we believe this is also a particularly
useful domain to support, given the Ô¨Åeld‚Äôs popularity and how
quickly it moves, how often new libraries are released or
updated, as well as the wide variety of skill sets and back-grounds present in the developers who write data science ordeep learning code. Automation of migration and refactoringin this domain is very minimal, and we design SOAR as a steptowards better tool support for this diverse and highly active
developer population.
VI. R
ELATED WORK
A. Automatic Migration
Existing work on automatic API migration uses example-
based migration techniques. Lamothe et al. [48] proposed
an approach that automatically learns API migration patternsusing code examples and identiÔ¨Åed 83 API migration patternsout of 125 distinct Android APIs. Fazzini et al. [49] proposed
APIMigrator, which learns from how developers from existingapps migrate APIs and uses differential testing to checkvalidity of the migration. They were able to achieve 85% of theAPI usages in 15 apps, and validated 68% of those migrations.Meditor [50] mines open source repositories and extracts
migration related code changes to automatically migrate APIs.
Meditor was able to correctly migrate 218 out of 225 testcases. Unlike prior API migration tools, SOAR can migratecode without existing code examples.
121SOAR also relates to automatic migration on APIs be-
tween different programming languages. Zhong et al. proposed
MAM [51] and mined 25,805 unique API mapping relationsof APIs between Java and C# with 80% accuracy. Nguyen et
al.proposed StaMiner [19], which is a data-driven approach
that statistically learns the mappings of APIs between Javaand C#. Bui et al. [20] used a large sets of programs as
input and generated numeric vector representations of theprograms to adapt generative adversarial networks (GAN). Buiet al. then identiÔ¨Åed the cross-language API mappings via
nearest-neighbors queries in the aligned vector spaces. Againthese methods largely rely on existing training data, such asMAM and StaMiner [19], [51] mine mappings from parallelequivalent code from two languages (Java and C#), whereSOAR only leverages the documentation for migration.
B. Program Synthesis
Program synthesis has been used to automate tasks in many
different domains, such as, string manipulations [52], tabletransformations [45], SQL queries [53], and synthesis of Javafunctions [54]. However, its usage for program refactoring is
scarce. ReSynth [55] uses program synthesis for refactoring
of Java code by providing an interactive environment toprogrammers, where they indicate the desired transformationwith examples of changes. Our approach differs from ReSynthsince we do not require the user to provide a partiallyrefactored code. Since our problem domain is API migration,
it is unlikely that the user knows all the required APIs fromthe target library and can perform these edits.
NLP can be used to synthesize programs directly from nat-
ural language [52], [53] or to guide the search of the program
synthesizer [56], [57]. For instance, NLP has been used to
synthesize tasks related to repetitive text editing [52], SQL
queries [53], and synthesis of regular expressions [56]. Onecan also combine input-output examples with a user-providednatural description to have a stronger speciÔ¨Åcation and achieve
better performance [56], [57]. Our approach follows thistrend of work where we combine NLP to guide the program
synthesizer with input-output examples that provide strongerguarantees in the synthesized code. However, instead of usinga natural description provided by the user, our approach uses
documentation from libraries to guide the search.
Using error messages from the compiler or interpreter is not
common in program synthesis. The most relevant approach to
ours is the one from Guo et al. [58] where they use type error
information to reÔ¨Åne polymorphic types when synthesizing
Haskel code. In contrast, SOAR uses error messages from theinterpreter not to reÔ¨Åne the type information but to restrict thedomain of the parameters and to prune the search space.
Finally, our synthesis strategy is based on program sketching
and program enumeration. This approach has close parallels(e.g., [59], [60]) and is extremely common in modern synthe-sizers because it provides a simple way of splitting the searchspace. Our approach can also be seen a generate-and-validatestrategy using test-cases as an oracle to evaluate migrationsuccess, which is also widely used repair engines [61].VII. C
ONCLUSIONS
API selection and maintenance is an important and difÔ¨Åcult
task for software development. To match evolving software,
developers often have to manually refactor APIs, which is
a tedious and error-prone job. We proposed SOAR to takeadvantage of API documentation and error messages as a richsources of information intended for developers. It uses naturallanguage processing and program synthesis to automaticallywrite refactored API calls. It is particularly well-suited for data
science or deep learning library refactoring, a prevalent use
case in modern development where tool support is positionedto have particular impact. SOAR collects information fromboth API documentation and error messages to generate logi-cal constraints that can be used to limit the synthesizer searchspace. Unlike prior approaches to automatic API migration,
SOAR requires no training data, and its output is guaranteed
to compile and pass existing tests. Our empirical evaluationshows that SOAR can successfully refactor 16/20 of ourbenchmarks for the deep learning domain with an averagetime of 97.23 seconds, and 18/20 of the benchmark set fordata wrangling tasks with an average time of 17.31 seconds.
A
CKNOWLEDGMENTS
This work was partially supported under National Sci-
ence Foundation Grant Nos. CCF-1910067, CCF-1750116and CCF-1762363, and by Portuguese national funds throughFCT, Fundac ¬∏Àúao para a Ci ÀÜencia e a Tecnologia, under PhD
grant SFRH/BD/150688/2020 and projects UIDB/50021/2020,DSAIPA/AI/0044/2018, and project ANI 045917 funded byFEDER and FCT. All statements are those of the authors, anddo not necessarily reÔ¨Çect the views of any funding agency.
R
EFERENCES
[1] C. Jaspan and J. Aldrich, ‚ÄúChecking framework interactions with rela-
tionships,‚Äù in ECOOP , vol. 5653 of Lecture Notes in Computer Science ,
pp. 27‚Äì51, Springer, 2009.
[2] C. R. De Souza, D. Redmiles, L.-T. Cheng, D. Millen, and J. Patterson,
‚ÄúHow a good software practice thwarts collaboration: the multiple roles
of apis in software development,‚Äù ACM SIGSOFT Software Engineering
Notes , vol. 29, no. 6, pp. 221‚Äì230, 2004.
[3] W. Maalej and M. P. Robillard, ‚ÄúPatterns of knowledge in api reference
documentation,‚Äù IEEE Transactions on Software Engineering , vol. 39,
no. 9, pp. 1264‚Äì1282, 2013.
[4] C. R. de Souza and D. F. Redmiles, ‚ÄúOn the roles of apis in the co-
ordination of collaborative software development,‚Äù Computer Supported
Cooperative Work (CSCW) , vol. 18, no. 5-6, p. 445, 2009.
[5] N. Chapin, J. E. Hale, K. M. Khan, J. F. Ramil, and W.-G. Tan, ‚ÄúTypes
of software evolution and software maintenance,‚Äù Journal of Software
Maintenance and Evolution: Research and Practice , vol. 13, no. 1,
pp. 3‚Äì30, 2001.
[6] J. H. Perkins, ‚ÄúAutomatically generating refactorings to support API
evolution,‚Äù in Proc. Workshop on Program Analysis for Software Tools
and Engineering , pp. 111‚Äì114, ACM, 2005.
[7] M. Kim, T. Zimmermann, and N. Nagappan, ‚ÄúA Ô¨Åeld study of refactor-
ing challenges and beneÔ¨Åts,‚Äù in Proc. ACM SIGSOFT F oundations of
Software Engineering , p. 50, ACM, 2012.
[8] M. Kim, T. Zimmermann, R. DeLine, and A. Begel, ‚ÄúData scientists in
software teams: State of the art and challenges,‚Äù IEEE Transactions on
Software Engineering , vol. 44, no. 11, pp. 1024‚Äì1038, 2017.
[9] M. Abadi, A. Agarwal, P. Barham, E. Brevdo, Z. Chen, C. Citro, G. S.
Corrado, A. Davis, J. Dean, M. Devin, et al. , ‚ÄúTensorÔ¨Çow: Large-scale
machine learning on heterogeneous distributed systems,‚Äù arXiv preprint
arXiv:1603.04467 , 2016.
122[10] A. Paszke, S. Gross, S. Chintala, G. Chanan, E. Yang, Z. DeVito, Z. Lin,
A. Desmaison, L. Antiga, and A. Lerer, ‚ÄúAutomatic differentiation in
pytorch,‚Äù in Proc. Annual Conference on Neural Information Processing
Systems , 2017.
[11] S. v. d. Walt, S. C. Colbert, and G. Varoquaux, ‚ÄúThe numpy array: a
structure for efÔ¨Åcient numerical computation,‚Äù Computing in science &
engineering , vol. 13, no. 2, pp. 22‚Äì30, 2011.
[12] H. Zhong, L. Zhang, T. Xie, and H. Mei, ‚ÄúInferring resource speciÔ¨Åca-
tions from natural language API documentation,‚Äù in Proc. International
Conference on Automated Software Engineering , pp. 307‚Äì318, IEEE,
2009.
[13] G. Uddin and M. P. Robillard, ‚ÄúHow API documentation fails,‚Äù IEEE
Software , vol. 32, no. 4, pp. 68‚Äì75, 2015.
[14] B. Hartmann, D. MacDougall, J. Brandt, and S. R. Klemmer, ‚ÄúWhat
would other programmers do: suggesting solutions to error messages,‚Äù
inProc. Conference on Human Factors in Computing , pp. 1019‚Äì1028,
ACM, 2010.
[15] S. Gulwani, O. Polozov, R. Singh, et al. , ‚ÄúProgram synthesis,‚Äù F ounda-
tions and Trends¬Æ in Programming Languages , vol. 4, no. 1-2, pp. 1‚Äì
119, 2017.
[16] Q. Guo, S. Chen, X. Xie, L. Ma, Q. Hu, H. Liu, Y . Liu, J. Zhao, and
X. Li, ‚ÄúAn empirical study towards characterizing deep learning de-
velopment and deployment across different frameworks and platforms,‚Äù
inProc. International Conference on Automated Software Engineering ,
pp. 810‚Äì822, IEEE, 2019.
[17] O. Meqdadi and S. Aljawarneh, ‚ÄúBug types Ô¨Åxed by api-migration:
a case study,‚Äù in Proc. International Conference on Data Science,
Technology and Applications , pp. 2:1‚Äì2:7, ACM, 2018.
[18] I. Savga, M. Rudolf, and S. Goetz, ‚ÄúComeback!: a refactoring-based
tool for binary-compatible framework upgrade,‚Äù in Proc. International
Conference on Software Engineering , pp. 941‚Äì942, ACM, 2008.
[19] A. T. Nguyen, H. A. Nguyen, T. T. Nguyen, and T. N. Nguyen,
‚ÄúStatistical learning approach for mining API usage mappings for code
migration,‚Äù in Proc. International Conference on Automated Software
Engineering , pp. 457‚Äì468, ACM, 2014.
[20] N. D. Q. Bui, Y . Yu, and L. Jiang, ‚ÄúSAR: learning cross-language API
mappings with little knowledge,‚Äù in Proc. ACM SIGSOFT F oundations
of Software Engineering , pp. 796‚Äì806, ACM, 2019.
[21] X. Gu, H. Zhang, D. Zhang, and S. Kim, ‚ÄúDeepam: Migrate apis with
multi-modal sequence to sequence learning,‚Äù in Proc. International Joint
Conference on ArtiÔ¨Åcial Intelligence (C. Sierra, ed.), pp. 3675‚Äì3681,
ijcai.org, 2017.
[22] ‚ÄúIntro to autoencoders : TensorÔ¨Çow core.‚Äù https://www.tensorÔ¨Çow.org/
tutorials/generative/autoencoder, August 2020.
[23] ‚ÄúApi documentation : TensorÔ¨Çow core v2.2.0.‚Äù https://www.tensorÔ¨Çow.
org/api docs/index.html, August 2020.
[24] ‚ÄúPytorch conv2d api documentation.‚Äù https://pytorch.org/docs/stable/
generated/torch.nn.Conv2d.html, August 2020.
[25] G. Salton and C. Buckley, ‚ÄúTerm-weighting approaches in automatic
text retrieval,‚Äù Information processing & management , vol. 24, no. 5,
pp. 513‚Äì523, 1988.
[26] J. Pennington, R. Socher, and C. D. Manning, ‚ÄúGlove: Global vectors
for word representation,‚Äù in Proc. Conference on Empirical Methods in
Natural Language Processing , pp. 1532‚Äì1543, ACL, 2014.
[27] M. F. Porter, ‚ÄúSnowball: A language for stemming algorithms,‚Äù 2001.
[28] C. S. Perone, R. Silveira, and T. S. Paula, ‚ÄúEvaluation of sentence
embeddings in downstream and linguistic probing tasks,‚Äù arXiv preprint
arXiv:1806.06259 , 2018.
[29] S. Arora, Y . Liang, and T. Ma, ‚ÄúA simple but tough-to-beat baseline for
sentence embeddings,‚Äù in Proc. International Conference on Learning
Representations , OpenReview.net, 2017.
[30] ‚ÄúCommon crawl.‚Äù https://commoncrawl.org/, August 2020.
[31] S. Gulwani, O. Polozov, and R. Singh, ‚ÄúProgram synthesis,‚Äù F ound.
Trends Program. Lang. , vol. 4, no. 1-2, pp. 1‚Äì119, 2017.
[32] A. Solar-Lezama, ‚ÄúThe sketching approach to program synthesis,‚Äù in
APLAS , vol. 5904 of Lecture Notes in Computer Science , pp. 4‚Äì13,
Springer, 2009.
[33] P. Orvalho, M. Terra-Neves, M. Ventura, R. Martins, and V . M. Man-
quinho, ‚ÄúEncodings for enumeration-based program synthesis,‚Äù in Proc.
International Conference Principles and Practice of Constraint Pro-
gramming , vol. 11802 of Lecture Notes in Computer Science , pp. 583‚Äì
599, Springer, 2019.[34] R. Martins, J. Chen, Y . Chen, Y . Feng, and I. Dillig, ‚ÄúTrinity: An
extensible synthesis framework for data science,‚Äù Proc. VLDB Endow. ,
vol. 12, no. 12, pp. 1914‚Äì1917, 2019.
[35] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean,
‚ÄúDistributed representations of words and phrases and their composi-
tionality,‚Äù in Proc. Annual Conference on Neural Information Processing
Systems , pp. 3111‚Äì3119, 2013.
[36] M. A. Hearst, ‚ÄúAutomatic acquisition of hyponyms from large text cor-
pora,‚Äù in Proc. International Conference on Computational Linguistics ,
pp. 539‚Äì545, 1992.
[37] R. Levy and G. Andrew, ‚ÄúTregex and tsurgeon: tools for querying and
manipulating tree data structures.,‚Äù in Proc. International Conference on
Language Resources and Evaluation , pp. 2231‚Äì2234, Citeseer, 2006.
[38] M. C. Evans, J. Bhatia, S. Wadkar, and T. D. Breaux, ‚ÄúAn evaluation
of constituency-based hyponymy extraction from privacy policies,‚Äù in
Proc. International Requirements Engineering Conference , pp. 312‚Äì321,
IEEE, 2017.
[39] A. Sumida and K. Torisawa, ‚ÄúHacking wikipedia for hyponymy rela-
tion acquisition,‚Äù in Proc. International Joint Conference on Natural
Language Processing , pp. 883‚Äì888, The Association for Computer
Linguistics, 2008.
[40] H. He, ‚ÄúThe state of machine learning frameworks in 2019,‚Äù The
Gradient , 2019.
[41] ‚ÄúTensorÔ¨Çow tutorial.‚Äù https://www.tensorÔ¨Çow.org/tutorials, August 2020.
[42] ‚ÄúTensorÔ¨Çow applications.‚Äù https://www.tensorÔ¨Çow.org/apidocs/python/tf/
keras/applications, August 2020.
[43] ‚ÄúTensorÔ¨Çow models.‚Äù https://github.com/tensorÔ¨Çow/models, August
2020.
[44] ‚ÄúKaggle.‚Äù https://www.kaggle.com, August 2020.
[45] Y . Feng, R. Martins, J. V . Geffen, I. Dillig, and S. Chaudhuri,
‚ÄúComponent-based synthesis of table consolidation and transformation
tasks from examples,‚Äù in Proc. ACM SIGPLAN Conference on Program-
ming Language Design and Implementation , pp. 422‚Äì436, ACM, 2017.
[46] ‚ÄúScrapy: A fast and powerful scraping and web crawling framework.‚Äù
https://scrapy.org/, August 2020.
[47] L. M. de Moura and N. Bj√∏rner, ‚ÄúZ3: an efÔ¨Åcient SMT solver,‚Äù in Proc.
International Conference on Tools and Algorithms for the Construction
and Analysis of Systems , vol. 4963 of Lecture Notes in Computer
Science , pp. 337‚Äì340, Springer, 2008.
[48] M. Lamothe, W. Shang, and T.-H. Chen, ‚ÄúA4: Automatically as-
sisting android api migrations using code examples,‚Äù arXiv preprint
arXiv:1812.04894 , 2018.
[49] M. Fazzini, Q. Xin, and A. Orso, ‚ÄúAPIMigrator: An API-Usage Mi-
gration Tool for Android Apps,‚Äù in Proc. International Conference on
Software Engineering , IEEE / ACM, 2020.
[50] S. Xu, Z. Dong, and N. Meng, ‚ÄúMeditor: inference and application of
API migration edits,‚Äù in Proc. International Conference on Program
Comprehension , pp. 335‚Äì346, IEEE / ACM, 2019.
[51] H. Zhong, S. Thummalapenta, T. Xie, L. Zhang, and Q. Wang, ‚ÄúMining
API mapping for language migration,‚Äù in Proc. International Conference
on Software Engineering , pp. 195‚Äì204, ACM, 2010.
[52] A. Desai, S. Gulwani, V . Hingorani, N. Jain, A. Karkare, M. Marron,
S. R, and S. Roy, ‚ÄúProgram synthesis using natural language,‚Äù in Proc.
International Conference on Software Engineering , pp. 345‚Äì356, ACM,
2016.
[53] N. Yaghmazadeh, Y . Wang, I. Dillig, and T. Dillig, ‚ÄúSqlizer: query
synthesis from natural language,‚Äù Proc. ACM Programming Languages ,
vol. 1, pp. 63:1‚Äì63:26, 2017.
[54] K. Shi, J. Steinhardt, and P. Liang, ‚ÄúFrangel: component-based synthesis
with control structures,‚Äù Proc. ACM Programming Languages , vol. 3,
pp. 73:1‚Äì73:29, 2019.
[55] V . Raychev, M. Sch ¬®afer, M. Sridharan, and M. T. Vechev, ‚ÄúRefactoring
with synthesis,‚Äù in Proc. ACM SIGPLAN Object-Oriented Programming,
Systems, Languages & Applications , pp. 339‚Äì354, ACM, 2013.
[56] Q. Chen, X. Wang, X. Ye, G. Durrett, and I. Dillig, ‚ÄúMulti-modal
synthesis of regular expressions,‚Äù in Proc. ACM SIGPLAN Conference
on Programming Language Design and Implementation , pp. 487‚Äì502,
ACM, 2020.
[57] Y . Chen, R. Martins, and Y . Feng, ‚ÄúMaximal multi-layer speciÔ¨Åcation
synthesis,‚Äù in Proc. ACM SIGSOFT F oundations of Software Engineer-
ing, pp. 602‚Äì612, ACM, 2019.
[58] Z. Guo, M. James, D. Justo, J. Zhou, Z. Wang, R. Jhala, and N. Polikar-
pova, ‚ÄúProgram synthesis by type-guided abstraction reÔ¨Ånement,‚Äù Proc.
ACM Programming Languages , vol. 4, pp. 12:1‚Äì12:28, 2020.
123[59] Y . Feng, R. Martins, Y . Wang, I. Dillig, and T. W. Reps, ‚ÄúComponent-
based synthesis for complex apis,‚Äù in Proc. ACM SIGPLAN-SIGACT
Symposium on Principles of Programming Languages , pp. 599‚Äì612,
ACM, 2017.
[60] C. Wang, A. Cheung, and R. Bod ¬¥ƒ±k, ‚ÄúSynthesizing highly expressive
SQL queries from input-output examples,‚Äù in Proc. ACM SIGPLAN
Conference on Programming Language Design and Implementation
(A. Cohen and M. T. Vechev, eds.), pp. 452‚Äì466, ACM, 2017.
[61] C. Le Goues, M. Pradel, and A. Roychoudhury, ‚ÄúAutomated program
repair,‚Äù Commun. ACM , vol. 62, no. 12, pp. 56‚Äì65, 2019.
124