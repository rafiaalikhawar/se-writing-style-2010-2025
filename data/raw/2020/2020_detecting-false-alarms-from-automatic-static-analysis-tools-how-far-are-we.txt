Detecting False Alarms from Automatic Static Analysis Tools:
How Far are We?
Hong Jin Kang
Singapore Management University
Singapore, Singapore
hjkang.2018@phdcs.smu.edu.sgKhai Loong Aw
Singapore Management University
Singapore, Singapore
klaw.2020@scis.smu.edu.sgDavid Lo
Singapore Management University
Singapore, Singapore
davidlo@smu.edu.sg
ABSTRACT
Automaticstaticanalysistools(ASATs),suchasFindbugs,havea
highfalsealarmrate.Thelargenumberoffalsealarmsproduced
posesabarriertoadoption.Researchershaveproposedtheuseof
machinelearningtoprunefalsealarmsandpresentonlyactionable
warnings to developers. The state-of-the-art study has identified a
set of “Golden Features” based on metrics computed over the char-
acteristicsandhistoryofthefile,code,andwarning.Recentstudies
showthatmachinelearningusingthesefeaturesisextremelyeffec-
tive and that they achieve almost perfect performance.
Weperformadetailedanalysistobetterunderstandthestrong
performance of the “Golden Features”. We found that several stud-
ies used an experimental procedure that results in data leakage
anddataduplication,whicharesubtleissueswithsignificantimpli-
cations.Firstly,theground-truthlabelshaveleakedintofeatures
that measure the proportion of actionable warnings in a given con-
text. Secondly, many warnings in the testing dataset appear in the
trainingdataset.Next,wedemonstratelimitationsinthewarning
oracle that determines the ground-truth labels, a heuristic compar-
ingwarningsinagivenrevisiontoareferencerevisioninthefuture.
Weshowthechoiceofreferencerevisioninfluencesthewarning
distribution. Moreover, the heuristic produces labels that do not
agree with human oracles. Hence, the strong performance of these
techniques previously seen is overoptimistic of their true perfor-mance if adopted in practice. Our results convey several lessons
and provide guidelines for evaluating false alarm detectors.
CCS CONCEPTS
•Software and its engineering →Software defect analysis.
KEYWORDS
static analysis, false alarms, data leakage, data duplication
ACM Reference Format:
HongJinKang,KhaiLoongAw,andDavidLo.2022.DetectingFalseAlarms
from Automatic Static Analysis Tools: How Far are We?. In 44th Inter-
national Conference on Software Engineering (ICSE ’22), May 21–29, 2022,
Pittsburgh, PA, USA. ACM, New York, NY, USA, 13 pages. https://doi.org/10.
1145/3510003.3510214
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation
on the first page. Copyrights for components of this work owned by others than ACMmustbehonored.Abstractingwithcreditispermitted.Tocopyotherwise,orrepublish,topostonserversortoredistributetolists,requirespriorspecificpermissionand/ora
fee. Request permissions from permissions@acm.org.
ICSE ’22, May 21–29, 2022, Pittsburgh, PA, USA
© 2022 Association for Computing Machinery.
ACM ISBN 978-1-4503-9221-1/22/05...$15.00
https://doi.org/10.1145/3510003.35102141 INTRODUCTION
It has been 15 years since Findbugs [ 5] was introduced to detect
bugs in Java programs. Along with other automatic static analysis
tools(ASATs)[ 8,41,43],FindBugsaimstodetectincorrectcodeby
matchingcodeagainstbugpatterns[ 5,18],forexample,patternsof
codethatmaydereferenceanullpointer.Sincethen,manyprojects
have adopted these tools as they help in detecting bugs at low
cost. However, these tools do not guarantee that the warningsare real bugs. Many developers do not perceive the warnings by
ASATs to be relevant due to the high incidence of effective false
alarms [19,43,52]. Prior work has suggested that the false positive
ratemayrangeupto91%.Whiletheoverapproximationofstatic
analysismaycausefalsealarms,falsealarmsdonotonlyreferto
errorsfromanalysisoroverapproximation,butincludewarnings
that developers did not act on [ 19,43,44]. Developers may not act
on a warning if they do not think the warning represents a bug or
believe that a fix is too risky.
Toaddressthehighrateoffalsealarms,manyresearchers[ 15,53]
have proposed techniques to prune false alarms and identify ac-
tionablewarnings,whicharethewarningsthatdeveloperswould
fix. These approaches [ 12,13,23,25,27,42,45,55,59] consider
different aspects of a warning reported by Findbugs in a project,
includingfactorsaboutthesourcecode[ 12],repositoryhistory[ 55],
filecharacteristics[ 29,59],andhistoricaldataaboutfixestoFind-
bugs warnings [ 27] within the project. Wang et al. [ 53] completed
a systematic evaluation of the features that have been proposed in
the literature and identified 23 “Golden Features”, which are themost important features for detecting actionable Findbugs warn-ings. Using these features, subsequent studies [
56–58] show that
anymachinelearningtechnique,e.g.SVM,performseffectivelyand
that the use of a small number of training instances can train effec-
tive models. In these studies, performances of up to 96% Recall and
98% Precision, and 99.5% AUC can be achieved. A perfect predictor
has a Recall, Precision, and AUC of 100%, suggesting that machine
learning techniques using the Golden Features are almost perfect.
AlthoughtheGoldenFeatureshavebeenshowntoperformwell,
we do not know why they are effective. Therefore, in this work
we seek to get a deeper understanding of the Golden Features. We
find a few issues: First, the ground-truth label was leaked into the
featuresmeasuringtheproportionofactionablewarningsinagivencontext.Second,warningsinthetestdatawereusedfortraining.Tounderstandtheirimpact,weaddressedthetwoflawsandfoundthat
the performance of the Golden Features declines. Our results show
thattheuseoftheGoldenFeaturesdonotsubstantiallyoutperform
a strawman baseline that predicts all warnings are actionable.
6982022 IEEE/ACM 44th International Conference on Software Engineering (ICSE)
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 07:58:31 UTC from IEEE Xplore.  Restrictions apply. ICSE ’22, May 21–29, 2022, Pittsburgh, PA, USA Hong Jin Kang, Khai Loong Aw, and David Lo
Next, we investigate the warning oracle used to obtain ground-
truth labels when constructing the dataset. To evaluate any pro-
posedapproach,alargedatasetshouldbebuilt,whereeachwarn-
ing is accurately labeled as either an actionable warning or a false
alarm. Many studies [ 53,56,58] use a heuristic, which we term the
closed-warning heuristic, as the warning oracle to determine the
actionabilityofawarning,checkingifthesamewarningisreported
inareference revision,a revision chronologicallyafterthe testing
revision.Ifthefileisstillpresentandthewarningisnotreportedin
thereferencerevision,thenthewarningis closedandisassumed
to be fixed. It is, therefore, assumed to be actionable. Conversely, a
warningthatremained openisafalsealarm.A revisionmadea few
yearsafterthesimulatedtimeoftheexperimentalsettingisusedas
thereferencerevision.Prior studies[ 53,56,58]selectedreference
revisionsset2yearsafterthetestingrevision.However,noprior
work has investigated the robustness of the heuristic.
There are several desirable qualities of a warning oracle. Firstly,
it should allow the construction of a sufficiently large dataset. Sec-
ondly, it should be reliable; the labels should be robust to minor
changesintheoracle.Thirdly,itshouldgeneratelabelsthathuman
annotators and developers of projects using ASATs agree with. An
advantageoftheclosed-warningheuristicisthatitenablesthecon-
structionofalargedataset.However,ourexperimentsdemonstrate
the lack of consistency in the labels given changes in the choice of
thereferencerevision.Thismayallowdifferentconclusionstobe
reached from the experiments. Our experiments also uncover that
theoracledoesnotalwaysproducelabelsthathumanannotators
or developers agree with. These limitations show that alone, the
heuristicdonotalwaysproducetrustworthylabels.Afterremoving
unconfirmed actionable warnings, the effectiveness of the Golden
Features SVM improves, indicating the importance of clean data.
Finally, we highlight lessons learned from our experiments. Our
resultsshowtheneedtocarefullydesignanexperimentalprocedure
toassessfutureapproaches,comparingthemagainstappropriate
baselines. Ourwork points outopen challengesin thedesign ofa
warning oracle for the construction of a benchmark. Based on the
lessons learned, we outline several guidelines for future work.
We make the following contributions:
•We analyze the reasons for the strong performance fromtheuseofthe“GoldenFeatures”observedinpriorstudies.
Contrary to prior work, we find that machine learning tech-
niques are not almost perfect, and that there is still much
room for improvement for future work in this area.
•Westudythewarningoracle,the closed-warningheuristic,
that assigns labels to warnings used in previous studies. We
show that the heuristic may not be sufficiently robust.
•We discuss the lessons learned and their implications. Im-portantly, we highlight the need for community effort inbuilding an accurate benchmark and suggest that future
studies compare new approaches with strawman baselines.
Therestofthepaperisstructuredasfollows.Section2covers
thebackgroundofourwork.Section3presentsthedesignofthe
study.Section4analyzestheGoldenFeatures.Section5investigates
theclosed-warningheuristic.Section6discusseslessonslearned
fromourstudy.Section7presentsrelatedwork.Finally,Section8
concludes the paper.2 BACKGROUND
2.1 Automatic Static Analysis Tools
Manyresearchers haveproposedAutomaticStatic AnalysisTools
(ASATs),suchasFindbugs[ 5],todetectpossiblebugsduringthe
software development process. Research has shown these tools are
useful and succeed in detecting bugs that developers are interested
inatlowcost.Comparedtoprogramverificationorsoftwaretesting,
thesetoolsrelyonbugpatternswrittenbytheauthorsofthestaticanalysistools,matchingcodethatmaybebuggy.Findbugsincludes
over 400 bug patterns that match a range of possible bugs, suchas class casts that are impossible, null pointer dereferences, and
incorrect synchronization.
Studies have also shown that ASATs are able to detect real
bugs [11,49]. Indeed, static analysis tools are adopted by large
companies [ 4,8,43] and open source projects [ 7] to detect bugs.
Developersmayrunthemduringlocaldevelopment,usethemin
ContinuousIntegration[ 60]andduringcodereviewtodetectbuggy
code to catch bugs early [ 6,36,50,52]. Projects may configure the
tools[60],forexample,tosuppressfalsealarmsbyconfiguringa
filter file to exclude specific warnings [2].
DeveloperslargelyperceiveASATstoberelevant,andthema-
jority of practitioners have used or heard of ASATs [ 35,48,52].
Still, these tools are characterized by the large amounts of false
alarms that they produce, and among other reasons, this has led to
resistance in adopting them in many software projects [19].
2.2 Distinguishing between Actionable
Warnings and False Alarms
Tominimizetheoverheadofinspectingfalsealarms,researchers
haveproposedapproachesbasedonmachinelearningtorankor
classify the warnings. A large number of features have been de-
signed over the past 15 years; for example, based on software met-
rics(e.g.sizeof thefile, numberof commentsinthe code),source
codehistory(e.g.numberof linesofcoderecentlyaddedtoafile),
and characteristics and history of the warnings (e.g. the number of
revisions where the warning has been opened).
Researchershaveevaluatedtheirproposedtoolsthroughdatasets
of warnings produced by Findbugs [ 12–14,16,42,45,59]. Recently,
Wang et al. [ 53] performed a systematic analysis of the features
proposed in the literature. From 116 features, they identified 23
GoldenFeatures,whicharethefeaturesthatachieveeffectiveper-
formance. The features are listed in Table 1. These features include
metricssuchasthecode-to-commentsratio [ 29],andthenumber
of lines added in the past [ 14,42]. Of note are several features of
the “Warning combination” feature type. We will refer to three of
these features, warning context in method ,warning context
in file, andwarning context for warning type , as thewarn-
ingcontext features. We refer to another two features, the defect
likelihood for warning pattern , anddiscretization of defect
likelihood asthedefect likelihood features.Thesefeaturesare
variousmeasuresoftheproportionofactionablewarningswithina
populationofwarnings,buildingontopoftheinsightthatwarnings
withinthepopulationsharethesamelabel,e.g.ifawarningwas
previouslyfixedinafile,itismorelikelythattheotherwarnings
in the same file will be fixed too.
699
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 07:58:31 UTC from IEEE Xplore.  Restrictions apply. Detecting False Alarms from Automatic Static Analysis Tools: How Far are We? ICSE ’22, May 21–29, 2022, Pittsburgh, PA, USA
Table 1: The Golden Features studied in prior work [53,
56, 58]. A warning context is defined [53] as the difference
of the number of actionable warnings and false alarms di-
vided by the total number of warnings reported in a given
method/file, or for a warning pattern. We provide more de-
scriptions of each feature in our replication package [1].
Feature type Feature
warning context in method
warning context in file
Warning combination warning context for warning type
defect likelihood for warning pattern
discretization of defect likelihood
average lifetime for warning type
comment-code ratio
method depth
Code characteristics file depth
# methods in file
# classes in package
warning pattern
Warning characteristics warning type
warning priority
package
file age
File history file creation
developers
Code analysis parameter signature
method visibility
Code history LOC added in file (last 25 revisions)
LOC added in package (past 3 month)
Warning history warning lifetime by revision
Further research [ 56] on the Golden Features of Wang et al. [ 53]
showed the lack of influence of the choice of machine learningmodel on effectiveness. They suggested that a linear SVM was
optimal since it requires a lower cost of training. In contrast, while
a deep learning approach achieves similar levels of effectiveness,it has a longer training time. Their analysis [
56] suggested that
the detection of false alarms is an intrinsically easy problem. A
differentstudy[ 58]demonstratedthat,withtheGoldenFeatures,
only a small proportion of the dataset has to be labelled to train an
effective classifier. The Golden Features are a subject of our study.
In Section 4, we analyze them in detail.
Closed-warning heuristic. The procedure to construct and
label the ground-truth dataset can be visualized in Figure 1. To
assess an approach that detects false alarms, a dataset of Findbugs
warnings is collected. While some researchers [ 13,45] construct a
labelleddatasetthroughmanuallabellingofthewarningsinasingle
revision, other researchers collect a dataset through an automatic
ground-truth data collection process [ 12,13,53,56,58]. Data for a
testingrevision andatleastone trainingrevision,setchronologically
beforethetestingrevision,iscollected.Thissimulatesreal-world
use of the tool, in which training is done on the history of the
project, and then used at the time of the testing revision.
Usingthe closed-warningheuristic asthewarningoracle,each
warninginagivenrevisioniscomparedagainsta referencerevision
Figure1:Thedatasetcompriseswarningscreatedbeforethetraining and testing revisions. The labels of each warningaredeterminedbytheclosed-warningheuristic;ifawarningis closed at the reference revision and the file has not beendeleted, then it is actionable.
setinthefutureofthetestrevision.Priorstudiesselectedareference
revisionset2yearsafterthetestrevision.Ifaspecificwarningis
nolongerpresentinthereferencerevision(i.e.,a closedwarning ),
theheuristicassumesthatthewarningisactionable.Ifthewarning
is present in both the given and reference revision (i.e., an open
warning),thentheheuristicassumesthatitisafalsealarm.Ifthe
file that contains the code with the warning has been deleted, then
thewarningislabelled unknown andisremovedfromthedataset.
In other words, according to the the closed-warning heuristic, aclosed warning is always actionable as long as the file has not
beendeleted,andanopenwarningisalwaysunactionable.Other
thandetectingactionablewarnings,researchershaveappliedthe
heuristictoidentifybug-fixingcommitsforminingpatterns[ 32,33].
The heuristic is a subject of our study, and we assess its robustness
and its level of agreement with human oracles in Section 5.
3 STUDY DESIGN
3.1 Research Questions
RQ1. Why do the Golden Features work? This research ques-
tion seeks to understand the Golden Features. While previous stud-
ies have highlighted their strong results, there has not been an
in-depth analysis of their practicality. We study the Golden Fea-
turesandthedatasetusedintheexperimentsbyWangetal.[ 53]
and Yang et al. [ 56]. We investigate the aspects of the features
and dataset that allow accurate predictions by the best performing
machinelearningmodel,anSVMusingtheGoldenFeatures.We
replicate the results of the previous studies and validate the predic-
tivepoweroftheGoldenFeatures.Tounderstandtheimportance
ofdifferentfeatures,weuseLIME[ 39]tonarrowourfocusdownto
thefeaturesthatcontributethemosttothepredictions.Afterwards,
we switch to increasingly simpler classifiers and analyze the exper-
imental data to better understand why the choice of classifiers did
not influence the results in prior studies.
RQ2.Howsuitableistheclosed-warningheuristicasawarn-
ing oracle? This research question concerns the suitability of the
closed-warning heuristic as a warning oracle. A good oracle should
be robust, and its judgments should agree with the analysis of ahuman annotator. We investigate the robustness of the heuristic,checking the consistency of labels under different choices of the
referencerevision.Whilepreviousstudiesuseda2-yearsinterval
700
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 07:58:31 UTC from IEEE Xplore.  Restrictions apply. ICSE ’22, May 21–29, 2022, Pittsburgh, PA, USA Hong Jin Kang, Khai Loong Aw, and David Lo
between the test revision and reference revision, we investigate if
different conclusions can be reached with a different time interval.
Next,wecomputetheproportionofclosedwarningsthathuman
annotatorslabelledactionable,andtheproportionofopenwarnings
that project developers suppressed as false alarms.
3.2 Evaluation Setting
Toanalyzetheperformanceofmachinelearningapproachesthat
identify actionable Findbugs warnings, we use the same metrics
as prior studies [ 53,56,58]. A true positive (TP) is an actionable
Findbugswarningcorrectlypredictedtobeactionable.Afalseposi-tive(FP)isanunactionableFindbugswarningincorrectlypredicted
tobeactionable.Notethatweusetheterm falsealarm toreferto
unactionableFindbugswarning.A falsepositive,therefore,refers
toafalsealarmthatisincorrectlydeterminedtobeanactionable
warning.Afalsenegative(FN)isanactionablewarningincorrectly
predicted to be a false alarm. A true negative is an unactionable
warning correctly predicted to be a false alarm.
We compute Precision and Recall as follows:
Precision =TP
TP+FP
Recall=TP
TP+FN
Finally, we compute and present F1, the harmonic mean of Pre-
cision and Recall. F1 is known to capture the trade-off betweenPrecision and Recall, and is used in place of accuracy given an
imbalanced dataset. F1 is computed as follows:
F1=2×Precision ×Recall
Precision+Recall
The Area Under the receiver operator characteristics Curve
(AUC) is a measure of the predictive power of a machine learn-
ingapproachtodistinguishbetweentrueandfalsealarms.Ranging
between 0 (worst discrimination) and 1 (perfect discrimination),
AUC is the area under the curve of the true positive rate against
thefalsepositive rate,andrecommendedover accuracywhenthe
data is imbalanced. A strawman classifier that always outputs a
single label has an AUC of 0.5.
Our dataset consists of projects that were studied by Yang et
al.[56,58]andWangetal.[ 53].Similartopreviousstudies[ 56,58],
weuseonetrainingrevisionandonetestingrevision.Weusethe
same testing revision as previous studies [ 53,56,58]. We train one
model for each project.
4 ANALYSIS OF THE GOLDEN FEATURES
To answer the first research question, we investigate the perfor-
manceoftheGoldenFeaturesbyfirstusingthesamedatasetusedby
Yang et al. [ 56]. The dataset includes two revisions from 9 projects.
The testing revisions are the revisions of the projects on 1 January
2014, and the training revision is a revision of the projects up to6 months before the testing revision. In total, 31,058 warning in-
stanceswereobtained byrunningFindbugsoverthe trainingand
testingrevision.Onaverage,14.1%ofthewarningsinthedataset
were actionable. Table 2 shows the breakdown of the warnings.
Wesuccessfullyreplicatetheperformanceobservedintheexper-
imentsofYangetal.[ 56]andWangetal.[ 53],obtaininghighAUC
values of upto 0.99. An averageF1 of 0.88 wasobtained, with F1
ranging from 0.65 to 0.95. Table 3 shows our experimental results.Table 2: The number of training, testing instances, and the
percentage of actionable warnings (Act. %) in the dataset.
The testing revision is the last revision checked into the
main branch on 2014-01-01.
Project Training With duplicates W/o duplicates
testing Act. % testing Act. %
ant 1229 1115 5% 2171%
cassandra 2584 2601 14% 55170%
commons 725 786 5% 450%
derby 2479 2507 5% 49931%
jmeter 604 613 24% 5719%
lucene 3259 3425 34% 89359%
maven 813 818 3% 14914%
tomcat 1435 1441 23% 22741%
phoenix 2235 2389 14% 21422%
Yangetal.[ 56]foundthatthedatasetwasintrinsicallyeasyas
the data was inherently low dimensional. To further analyze their
findings,weusedtoolsfromthefieldofexplainableAI,inparticular
LIME [39], to identify the most important features contributing to
eachprediction.LIMEisanexplanationtechniquethatidentifiesthemostimportantfeaturesthatcontributedtoanindividualprediction.
Toidentifythemostimportantfeatures,wesampled50predictions
made by the Golden Features SVM, and used LIME to identifythe top features contributing to the predictions. We found thattwo features,
warning context of file andwarning context of
package, appeared in the top-3 features of every prediction.
Warning context and defect likelihood features. On ana-
lyzing the source code of the feature extractor developed by Wang
et al. [53], we found a subtle data leak in the implementation of
the warning context and defect likelihood features. These features
utilizefindingsfrompreviousstudies[ 27]thatfoundthatthewarn-
ings within a population (e.g. warnings in the same file) tend to be
homogenous; if one warning is a false alarm, then the other warn-
ings in the same population tend to be false alarms as well. Includ-
ingwarning context of file andwarning context of package ,
there are another 3 features computed similarly ( warning con-
text of warning type ,defect likelihood for warning pattern ,
Discretization of defect likelihood for warning pattern ). At
a high level, the warning context features are computed as follows:
|Wactionable
relevant|−|Wfalse alarm
relevant|
|Wrelevant|
Wrelevantreferstothesetofwarningsrelevanttothefeaturetype.
For example, Wrelevantofwarning context of file considers the
warnings that are reported in a given file, while Wrelevantofwarn-
ingcontextofwarningtype considersallwarningsforthegiven
category of patterns (e.g. STYLE,INTERNATIONALIZATION ). Note
that a warning pattern refers to a specific bug pattern in Findbugs
(e.g.“ES_COMPARING_STRINGS_WITH_EQ”),andawarningtype
isacategoryofpatterns.The defectlikelihoodforwarningpat-
tern[45]featurecomputestheproportionofwarningsthatwere
actionable out of all warnings with the given bug pattern, p:
D(p)=|Wactionable
relevant|
|Wrelevant|
701
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 07:58:31 UTC from IEEE Xplore.  Restrictions apply. Detecting False Alarms from Automatic Static Analysis Tools: How Far are We? ICSE ’22, May 21–29, 2022, Pittsburgh, PA, USA
Table 3: Effectiveness of an SVM using the Golden Features after removing the leaked features and removing the duplicate
warnings between the training and testing dataset. The numbers in parentheses are the F1 obtained by the baseline classifier
that predicts all warnings are actionable.
Project All Golden Features − leaked features − data duplication − leak, duplication
F1 AUC F1AUC F1AUC F1AUC
ant 0.94 (0.09) 1.000.11 (0.09) 0.67 - - - -
cassandra 0.92 (0.24) 1.000.45 (0.24) 0.860.9 (0.41) 0.990.29 (0.41) 0.54
commons 0.65 (0.10) 0.990.16 (0.10) 0.650.75 (0.25) 0.970.11 (0.25) 0.49
derby 0.95 (0.09) 1.000.39 (0.09) 0.930.97 (0.28) 0.970.30 (0.28) 0.59
jmeter 0.94 (0.38) 0.990.53 (0.38) 0.761.00 (0.14) 1.000.25 (0.14) 1.00
lucene-solr 0.87 (0.51) 0.970.59 (0.51) 0.740.87 (0.53) 0.980.23 (0.53) 0.62
maven 0.86 (0.07) 1.000.27 (0.07) 0.90.95 (0.24) 0.990.27 (0.24) 0.58
tomcat 0.93 (0.37) 1.000.48 (0.37) 0.730.95 (0.70) 1.000.65 (0.70) 0.39
phoenix 0.89 (0.25) 1.000.42 (0.25) 0.780.83 (0.37) 0.990.40 (0.37) 0.63
Average 0.88 (0.23) 1.000.38 (0.23) 0.760.90 (0.37) 0.990.31 (0.37) 0.59
Thediscretizationofdefectlikelihoodforwarningtype [45]
feature, computed for each type/category Tof bug patterns, is a
measureofthedifferenceindefectlikelihoodfromthedefectlikeli-
hoodof Tfor each bug pattern in the category:
1
|T|−1/summationtext.1
p∈T(D(p)−D(T))2
The five warning context and defect likelihood features require
information about the actionability of each warning in the pop-
ulation of warnings considered. A data leakage occurs when the
classifierutilizesinformationthatisunavailableatthetimeofits
predictions [ 22,51]. As shown in Figure 2, while the ratio of ac-
tionable warnings are computed over the warnings reported in the
past (the black line in Figure 2), the closed-warning heuristic to
determine the ground-truth label of a warning (the red lines in Fig-
ure 1 and Figure 2) is utilized to determine if these warnings were
actionable.Tocomputethewarningcontextofagivenwarning, Wt
in the testing revision, the labels of all warnings in the population
ofwarnings(e.g.allwarningsinthesamefile),including Wt,ar e
obtained based on comparison to the reference revision.
Sincetheground-truthlabelisalsoobtainedbasedoncompari-
sontothereferencerevision,theground-truthlabelis inadvertently
leakedintothecomputationofthewarningcontext.Thisisnota
realistic assumption in practice; at test time, the ground-truth label
of the warning context of Wtis the target of the prediction. While
checkingifawarningwillbeclosed2yearsinthefutureispossible
withinanexperiment,thereisnowaytocheckifthewarningswill
beclosed2yearsintothefutureinpractice.Table3showsthelargedropinF1,fromanaverageof0.88to0.38,whenthesefivefeatures
are dropped. We refer to these features as leaked features.
Baseline using data leakage. Data leakage leads to an experi-
mental setting that overestimates the effectiveness of the classifier
understudy[ 22,51].InTable4,weshowthatabaselineequivalent
to the Golden Features can be developed using only the five leaked
features. Using just the leaked features with an SVM, we construct
a baseline that achieves performance comparable to the use of the
GoldenFeatures.AnSVMusingtheleakedfeatureshasaPrecision
of0.79,about0.10lowerthantheGoldenFeaturesSVM,however,
they achieve identical Recall of 0.94, which results in an F1 of 0.83,
just 0.05 lower than the Golden Features. This indicates that the
Figure 2: The warning context and defect likelihood fea-tures use labels derived through the closed-warning heuris-tic, using information from the reference revision, chrono-logically in the future of the test revision. In a realistic set-ting, this information will not be present at test time.
strongperformanceof theGoldenFeaturesin theexperimentsde-
pends largely on the leaked features, and is an optimistic estimate
of their effectiveness.
Thecomputationofthe warningcontext anddefectlikelihood fea-
tures caused data leakage, as it used labels determined by compari-
son against the reference revision, chronologically in the future of
the testing time.
Data duplication. Next, we progressively selected simpler ma-
chine learning models and surprisingly, found that a k-Nearest
Neighbors(kNN)classifierperformseffectively.Inparticular,we
found asurprising trend where thelower values of kled tobetter
results.Theresultsoftheexperimentwhereweiterativelylowered
kto consider in the prediction are shown in Table 4.
Surprisingly,akNNclassifierwithk=1(i.e.,onlyoneneighboris
consideredto makea prediction)produces thebest result, obtained
a Precision of 0.87, a Recall of 0.90, with an F1 of 0.84. With k=1,the classifier was selecting a single most similar warning in the
trainingdataset.IntypicalusageofkNN,alowvalueof kmaycause
theclassifiertobeinfluencedbynoiseandoutliers,whichmakes
the strong results surprising. To analyze the results further, we
observed that the number of training (15,363) and testing instances
(15,695) were similar, and we investigated the data carefully. We
found that many testing instances appeared in both the training
and testing dataset.
702
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 07:58:31 UTC from IEEE Xplore.  Restrictions apply. ICSE ’22, May 21–29, 2022, Pittsburgh, PA, USA Hong Jin Kang, Khai Loong Aw, and David Lo
Table 4: Average Precision (Prec.), Recall, and F1 of various
approaches on the original dataset by Yang et al.
Technique Prec.Recall F1
Golden Features SVM 0.84 0.940.88
− leaked features 0.26 0.700.38
− data duplication 0.88 0.930.90
− data duplication and leaked features 0.27 0.570.31
+ reimplemented leaked features 0.32 0.570.38
Golden Features kNN (with k=10) 0.91 0.570.68
Golden Features kNN (with k=5) 0.86 0.720.78
Golden Features kNN (with k=3) 0.87 0.780.82
Golden Features kNN (with k=1) 0.87 0.900.84
Only leaked features SVM 0.79 0.940.83
Repeat label from training dataset 0.72 0.800.75
Thedataduplication wascausedbythedata collectionprocess,
inwhichallwarningsproducedbyFindbugsforboththetraining
and test revisions were included in the training and testing dataset.
Saywehaveawarningatthetrainingrevision,determinedtobe
open and, therefore, unactionable by the closed-warning heuristic.
Inotherwords,thewarningremainedopenintheperiodbeforethe
trainingrevisiontothereferencerevision.Then,thewarningwouldcertainlybeopenedatthetestingrevision,whichischronologically
beforethereferencerevisionbutafterthetrainingrevision.Like-
wise, if we have a warning only closed after the testing revision,
butwasopenduringthetestingrevision,thenthesamewarning
would be present at both the training and testing revision with the
same “actionable” label. Consequently, a large number of warnings
appear in both the training and testing dataset. This contributes to
an unrealistic experimental setting.
Baseline using duplicated data. Data duplication creates an
artificial experimental setting that inflates performance metrics [ 3].
To confirm that the data duplication contributes to the ease of the
task,weconstructaweakbaseline,adummyclassifier,thatlever-
ages the duplication of testing data in the training dataset. Given a
warning from the testing dataset, the classifier heuristically identi-
fies the same warning from the training dataset by searching for a
training warning based on the class name (e.g. “BooleanUtils”) and
bug patternname (e.g.“ES_COMPARING_STRINGS_WITH_EQ”).
Iftherearemultiplewarningswiththesameclassnameandbug
pattern name,a randomtraining instance isselected fromamong
them. The classifier then outputs the label of the training instance.
If there is no training instance with the same class and bug pattern
type, then the classifier defaults to predicting that the warning is a
false alarm, which is the majority class label.
Table 4 shows the comparison of various approaches, includ-
ingthebaseline approaches,onthedataset.Thedummy classifier
achievesstrongperformance,achievingaPrecisionof0.72,aRecallof0.80,andanF1of0.75.Whilethedummyclassifierunderperformsthemodelusingtheleakedfeatures,itoutperformstheGoldenFea-
tures SVM without the leaked features. This indicates that using
justtwoattributes,(1)theclassnameand(2)thebugpatternofthe
warning, is enough to obtain strong performance on a dataset with
data duplication. Therefore, we conclude that the data duplication
Figure 3: We reimplemented the leaked features. The reim-plemented features use only information (represented bythe blue, dashed lines) available at the present (i.e., eitherthe training or test revision) to determine if a warning (i.e.,created before the training or test revision) has been closed.Under this setting, no information from the reference revi-sion is used for making predictions.
between the training and testing dataset contributes to the strong
performance observed in previous studies.
The experimental results are summarized in Table 4. With both
the leaked features and duplicated data, the average F1 was 0.88.
Afterthedataleakagefeaturesareremoved,F1decreasedto0.38.
After removing the duplicated data, F1 decreases further to 0.31.
The average project’s AUC decreased from 1.00 to 0.59. In compari-
son, using a strawman baseline that predicts that every warning is
actionable produces an F1 of 0.52 (with an AUC of 0.5).
AllwarningsreportedbyFindBugsonboththetrainingandtesting
revisionswereincludedinthedatasets.Warningsreportedatthe
trainingrevisionmaystillbereportedatthetestingrevision,leading
to data duplication between the training and testing dataset.
Experimentsunderamorerealisticsetting. Tobetterunder-
standtheperformanceoftheGoldenFeaturesSVM,werananother
experiment where the two issues of data leakage and data dupli-cation have been fixed. First, we deduplicated the test data from
thetrainingdataset.Insteadofincludingallwarningsinthetest-
ing revision, we only consider new warnings introduced between
the time after the training revision and before the testing revision.
Figure3showsourprocedure.Ascomparedtothepreviousdataset
constructionprocessinFigure1,onlythewarningscreatedafter
the training revision and before the testing revision are used for
testing.Thisbetterreflectsreal-worldconditionswhereallwarn-
ings prior to usage are used for training, but none of the testing
datainvolveswarningsthathavealreadybeenclassified.Intotal,
thenumberofwarningsinthetesting revisionsdecreasedfroma
total of 15,695 to 2,615 after deduplication. Without the duplicated
dataandwithoutusingtheleakedfeatures,theaverageF1drops
from 0.88 to 0.31 as seen in Table 3.
Next,wereimplementedtheleakedfeaturestoinvestigatethe
effectiveness of Golden Features SVM. To prevent data leakage, we
modifiedthedefinitionoftheleakedfeatures.Figure3visualizesthe
computation of the warning context and defect likelihood features.
Instead of considering all warnings, we consider only warnings
thatwere introducedinthe 1yearduration beforethetraining or
testing revision. Instead of using the reference revision, we usethe given revision (i.e., either the training or testing revision) todetermine if the warning was closed. A warning is closed at a
703
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 07:58:31 UTC from IEEE Xplore.  Restrictions apply. Detecting False Alarms from Automatic Static Analysis Tools: How Far are We? ICSE ’22, May 21–29, 2022, Pittsburgh, PA, USA
givenrevisionifFindbugsdoesnotreportit.Inotherwords,forthe
trainingrevision,onlythewarningscreatedwithinthepastyear
before the training revision are considered. For testing, only the
warnings created within one year before the testing revision are
considered. A time interval of 1 year was selected in contrast to
the study by Wang et al. [ 53], which used time intervals of up to 6
months.UnlikeWangetal.[ 53],forthetestingrevision,weconsider
onlywarningscreatedafterthetrainingrevisiontopreventdata
duplication.Consequently,wefoundfewernewlycreatedwarnings
intheshorttimeintervalbetweenthetrainingandtestingrevisions.
Notethat afterreimplementing thewarning contextanddefect
likelihoodfeatures,wecouldnotruntheexperimentsfortheproject
Phoenix as we facedmany difficulties building old versions ofthe
project. Moreover, their revision history did not go back beyond3 years, required for computing the warning context and defect
likelihood features for the training revision. This limitation is not
presentforWangetal.[ 53],astheycomputethefeaturesbycheck-
ingifthegivenwarningisclosedinthereferencerevision,setin
thefutureofthetestrevision(causingdataleakage).Assuch,we
omit Phoenix for the rest of the experiments.
Table 4 shows the performance of the Golden Features SVM
using the reimplemented features. Without the leaked features,the Golden Features SVM achieves an F1 of 0.38. Even with the
reimplementation of the leaked features, the Golden Features SVM
underperforms the strawman baseline, which predicts all warnings
are actionable, with an F1 of 0.43. However, it has an AUC of 0.59,
greaterthan0.5,indicatingthattheGoldenFeaturesarebetterthan
random and have some predictive power.
Answer toRQ1: Afterremovingthedataleakageanddatadupli-
cation, our experimental results indicate that the Golden Features
SVMunderperformsthestrawmanbaseline,althoughitsAUC(>
0.5) suggests that the Golden Features have some predictive power.
5 ANALYSIS OF THE CLOSED-WARNING
HEURISTIC
Next,giventhatthequalityandrealismofthedatasetheavilyin-
fluencestheevaluationoftheGoldenFeaturesSVM,weperform
adeeperanalysisoftheconstructionoftheground-truthdataset.
In previous studies [ 53,56,58], the warning oracle is the closed-
warningheuristic ;awarningisheuristicallydeterminedtobeac-
tionableifitwasclosed(i.e.,reportedbyFindbugsinarevisionbut
wasnotreportedbyFindbugsinthereferencerevision,andthefilewasnotdeleted),andisafalsealarmifitwasopen(i.e.reportedby
Findbugs on both the training/test and reference revision).
Inthefirstpartofouranalysis,weinvestigatetheconsistencyin
the warning oracle given a change in the reference revision. Next,
wecheckifthewarningoracleproduceslabelsthathumanusers
wouldagreewith.Todoso,wefirstdetermineifhumanannotators
considerclosedwarningsasactionablewarnings.Inaddition,we
matchopenwarningsagainstFindbugsfilterfilesinprojectswhere
developers have configured the filters for suppressing false alarms.
Finally,weobserveifcleanerdataincreasestheeffectivenessofthe
Golden Features SVM.5.1 Choosing a different reference revision
We perform a series of experiments to determine how the time in-
terval between the test revision and the selected reference revision
influences the ground-truth label of the warnings. We hypothesize
that the longer the time interval between the test and reference
revision,thegreatertheproportionofclosedwarnings.Basedon
theclosed-warningheuristic,thiswouldcausemorewarningsto
belabelledactionable.Ifso,thelackofconsistencyinlabelsshould
call the robustness of the heuristic into question. If many bugs are
fixed only after many years, then an open warning at any given
time may, in fact, be actionable. Besides that, if changing the refer-
encerevisionleadsustoadifferentconclusionabouttheGolden
FeaturesSVM,thenitlimitsthelevelofconfidencethatresearchers
can have in the experimental results.
In our experiments, we use three reference revisions set two,
three, and four years after the test revision. By switching the refer-
encerevision,weobservechangesintheaverageactionabilityratio.
While the actionability ratio remained consistent for the 4 out of 8
projects, the actionability ratio increased by over 10% for the other
4projects,asseeninTable5.Overall,theaverageactionabilityratio
increased by 14% when varying the time interval between the test
andreferencerevisionfrom2to4years.Consideringallprojects,we
performed a Wilcoxon signed-rank test and found that the change
inactionabilityratioisstatisticallysignificant(p-value=0.03<0.05).
In terms of the effectiveness of the Golden Features SVM, its
average F1 increased from 0.39 to 0.57, as seen in Table 5. Con-sidering all projects, the Golden Features SVM underperformedthe strawman baseline. Our experiments showed some variation
oftheGoldenFeaturesSVM’seffectivenessgivenachangeinthe
referencerevision.Forinstance,theGoldenFeaturesSVMachieved
alowF1of0.06inDerbywhenthetimeintervalbetweenthetest
andreferencerevisionwas2years,buthadahighF1of0.72witha
time interval of 4 years.
By changing reference revisions, the problem exhibits different
characteristics. Using a reference revision 4 years after the test re-
vision,actionablewarningswouldbethemajorityclass,whilethey
weretheminorityclasswhenusingtheotherreferencerevisions.4
of8projectshaveanAUCthatflippedfromonesideof0.5tothe
other (e.g. theGolden Features SVM’sAUC is under0.5 on Derby
givena2-yearsinterval,buttheAUCincreasesabove0.5givena
4-years interval). In short, different conclusions about the task and
the effectiveness of the Golden Features may be reached.
Changing the reference revision may affect the distribution of the
actionablewarnings,whichmayimpacttheconclusionsreached
from experiments on the effectiveness of the Golden Features SVM.
5.2 Unconfirmed actionable warnings
Next,weinvestigateifclosedwarningsaretrulyactionablewarn-
ings.Awarningcouldbeclosedduetoseveralreasons.Codecon-
taining the warning could be deleted or modified while implement-inganewfeature,andthewarningmayonlybeclosedincidentally.
Tofurtherunderstandthecharacteristicsofclosedwarnings,and
to determine how likely is a closed warning an actionable warning,
we sampled 1,357 warnings (which is more than the statistically
representative sample size of 384 warnings) that were closed. Two
704
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 07:58:31 UTC from IEEE Xplore.  Restrictions apply. ICSE ’22, May 21–29, 2022, Pittsburgh, PA, USA Hong Jin Kang, Khai Loong Aw, and David Lo
Table 5: The number of training, testing instances, and the percentage of actionable warnings (Act. %) in the dataset when
varying the reference revision. The numbers in parentheses are the F1 obtained by the baseline classifier that predicts all
warnings are actionable. The testing revision is the last revision checked in to the main branch before 2014-01-01.
Project # testing 2 years 3 years 4 years
instances Act. % F1AUCAct. % F1AUCAct. % F1AUC
ant 21 240 (0.38) 0.43 430.13 (0.60) 0.48 430 (0.60) 0.32
cassandra 551 410.56 (0.59) 0.52 460.61 (0.63) 0.41 430.58 (0.60) 0.48
commons 4500.66 (1.00) 1.00 501.00 (0.67) 1.00 501.00 (0.67) 1.00
derby 489 100.06 (0.18) 0.33 590.58 (0.74) 0.46 660.72 (0.80) 0.52
jmeter 57 170.13 (0.16) 0.58 260.12 (0.27) 0.4 910.71 (0.95) 0.52
lucene 993 440.56 (0.62) 0.58 490.58 (0.66) 0.57 670.63 (0.8) 0.53
maven 149 170.25 (0.29) 0.41 160.27 (0.28) 0.44 160.27 (0.28) 0.44
tomcat 226 420.53 (0.59) 0.51 610.51 (0.57) 0.48 520.64 (0.69) 0.54
Average 311 400.39 (0.43) 0.54 420.48 (0.55) 0.53 540.57 (0.67) 0.54
Figure 4: Example of code that Findbugs reports a warning
on. Findbugs warns against using new Long, recommending
the more efficient Long.valueOf to instantiate a Longobject.
authorsofthisstudyindependentlyanalyzedeachwarningtode-
termine if they were removed for a bug fix. If the warning was
closedduetocodechangesunrelatedtothewarning,thenwedo
not consider the warning as actionable. If the code containing the
warning was modified such that it was not easily discernible if
thewarningwasclosedwiththeintentionoffixingthewarning,
then weconsider it “unknown”.If the originalversion of thecode
had any comments indicating that Findbugs reported a false alarm
(e.g. explaining the reason that a seemingly buggy behavior was
expected behavior), then we consider the warning a false alarm.
When the labels differed between the annotators, they discussed
the disagreements to reach a consensus. We computed Cohen’sKappa to measure the inter-annotator agreement and obtained a
value of 0.83, which is considered as strong agreement [28].
Finally,afterlabelling,176(13%)oftheheuristically-closedwarn-
ings were considered as false alarms. Another 520 warnings (38%)
werecategorizedas“unknown”.Lastly,660(49%)warningswere
still considered actionable after labelling.
Foranexampleofawarninglabelled“unknown”,Figure4shows
a fragment of code where Findbugs complains about the use of the
Longconstructor, indicating that Long.valueOf would be more
efficient. Even though the warning is removed in the reference
revision,theentirefunctionalityofthecodefragmentwaschanged
asshowninFigure5.Insuchcases,welabelthewarningas“un-known” instead of “actionable” or a “false alarm”, as there is no
evidencethatthewarningwasfixedorignored.Weconsiderthat
Figure 5: The warning from Figure 4 is removed through achangeinfunctionality,unrelatedtothewarningotherwise.
the warning was removed incidentally, and that the annotators are
unable to accurately label the warning.
Whiletheclosed-warningheuristicconsideredthatawarning
couldberemovedthroughthedeletionofafile,itdoesnotconsiderothercaseswhereawarningcouldbeincidentallyremovedthroughcodemodificationthatdoesnotfixthebugindicatedbythewarning,
Ourresultsindicatethatmoreinformationshouldbeconsidered,
and that the heuristic may not be sufficiently robust.
Only 47% of closed warnings were labelled actionable by human
annotators,implyingthatmanyclosedwarningsarenotactionable.
Many closed warnings were only closed incidentally.
5.3 Unconfirmed false alarms
Our findings from Section 5.1 indicate the possibility that some
actionablewarningswouldonlybeclosedgivenalongertimeinter-
val between thetest revision and the referencerevision. This may
reflect real-world conditions, where developers may not prioritize
reports from ASATs and may take a long time before inspectingthem.Thus,openwarningsmaybeactionablewarningsthatthedevelopers would fix with enough time. We run an experimentto understand this effect, focusing on projects that have shown
evidence of using Findbugs. In this experimental setup, we remove
open warnings that are not confirmed by the project developers to
be false alarms.
705
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 07:58:31 UTC from IEEE Xplore.  Restrictions apply. Detecting False Alarms from Automatic Static Analysis Tools: How Far are We? ICSE ’22, May 21–29, 2022, Pittsburgh, PA, USA
Table 6: Number of open warnings in each project matched
bytheirFindbugsfilterfile.Ifawarningwasfiltered,itindi-
cates that the project’s developers consider it a false alarm.
Project # open warnings # filtered % filtered
jmeter 710 6 1%
tomcat 1624 9 1%
commons-lang 106 19 18%
flink 4934 4754 96%
hadoop 3053 269 9%
jenkins 1212 178 15%
kudu 1873 464 25%
kafka 4668 2993 64%
morphia 65 0 0%
undertow 347 113 33%
xmlgraphics-fop 949 909 96%
Average (Mean) 1666 818 31%
Average (Median) 1212 178 18%
Someprojects,whichuseFindbugsintheirdevelopmentprocess,
configureaFindbugsfilterfile[ 2]forindicatingfalsealarms.The
filter file allows developers to suppress warnings of specific bugpatterns on the indicated files. Developers may add warnings to
theFindbugsfilterfileafterinspectingthe warningsandidentify-
ing false alarms. On projects that have created and maintained a
Findbugsfilterfile,weassumethatadeveloperwouldeitherfixthebuggycodeorupdatethefilterfileafterinspectingawarning.Ifso,thenanopenwarningthatisnotmatchedbytheFindbugsfilterfilemaynotbeafalsealarm,buthasnotbeeninspectedbyadeveloper.
Theseopenwarningscouldbefalsealarms,buttheymayalsobe
warnings that developers would act on after inspecting them. If an
open warning matches the filter, then it has been confirmed by the
developers to be a false alarm.
To investigate the proportion of open warnings that are con-
firmed to be false alarms by project developers, we identified 3
projects(JMeter,Tomcat,Commons-Lang)thathavealreadycon-
figured the Findbugs filter file from Wang et al.’s dataset [ 53], used
in the preceding experiments. Next, we searched GitHub for ma-ture projects that showed evidence of using Findbugs and have
configured aFindbugsfilter file.Using theGitHub SearchAPI, we
looked for XML files containing the term FindbugsFilter , which
is a keyword used in Findbugs filter files, in projects that were not
forks,filteringoutprojectswithlessthan100starsorhadlessthan
10 lines in the Findbugs filter file. We obtained 8 projects.
The statistics of the warnings reported by Findbugs on the
projects are displayed in Table 6. On average, 31% of the open
warnings(amedianof18%)arematchedbytheFindbugsfiltercon-
figured by the developers, although the proportion varies for each
project. Our results suggest that the majority of open warnings
remain uninspected by developers.
On average, only 31% of open warnings have been explicitly in-dicated by developers to be false alarms, suggesting that only a
minorityofopenwarningsarefalsealarms.Whiletherestofthe
open warnings could be false alarms, they could also be actionable
warnings that have not been inspected yet.Table 7: Effectiveness of the Golden Features SVM after re-
moving unconfirmed actionable warnings and false alarms.
Act. % refers to the proportion of actionable warnings. The
numbers in parentheses are the F1 of the dummy baseline,
which predicts that all warnings are actionable.
Dataset Act. % F1AUC
Original dataset [56, 58] 39.90.39 (0.43) 0.54
− unconfirmed actionable warnings 40.00.61 (0.57) 0.66
Projects using Findbugs 38.00.43 (0.44) 0.62
− unconfirmed false alarms 40.00.41 (0.46) 0.60
Next,weinvestigatetheimpactoftheunconfirmedactionable
warnings and false alarms on the Golden Features SVM. We hy-
pothesize that cleaning up the data will improve its effectiveness.
To study the impact of unconfirmed actionable warnings, we
used the dataset of warnings from the projects by Wang et al. [ 53]
and Yang et al. [ 56,58]. These projects were the same projects
studiedearlierinSection5.2.Weconstructadatasetofwarnings
with only the warnings confirmed by the human annotators to
be actionable warnings. We randomly sampled a subset of open
warnings to retain a similar actionability ratio.
Forevaluatingtheeffectofunconfirmedfalsealarms,weusedthe
warningsfromtheprojectsthatusedFindbugs(fromSection5.3).
However,weomit4projects(JMeter,Tomcat,Hadoop,Morphia)
where less than 10% of open warnings matched the filter file, as
thelowpercentagemayindicatethattheFindbugsfilterfilesare
not kept up to date in these projects. From the other projects, only
openwarningsthatmatchthefilterfileareincluded.Wesampleda
subset of closed warnings to retain a similar actionability ratio.
The outcome of our experiment is shown in Table 7. Removing
unconfirmed actionable warnings led to an increased AUC from
0.54to0.68,andanincreasedF1from0.39to0.64.Thisoutperforms
the strawman baseline which has an F1 of 0.57, suggesting that
cleaner data may increase the effectiveness of the Golden Features
SVM.However,removingunconfirmedfalsealarmsdidnothelp.
The results may indicate that cleaner data may help and removing
unconfirmedactionablewarnings,whichistheminorityclass,may
have a positive effect on the effectiveness of a classifier.
Answer to RQ2: The closed-warningheuristic may not bean ap-
propriatewarningoracle.Itlacksconsistencywithrespecttothe
choice of reference revision, which may affect the findings reached
from the experimental results. Moreover, the heuristic conflatesclosed warnings for actionable warnings and open warnings for
falsealarms.Wefindhavingcleanerdatabyremovingunconfirmed
actionable warnings can boost the performance of the Golden Fea-
tures SVM.
6 DISCUSSION
6.1 Lessons Learned
Todetectactionablewarnings,theGoldenFeaturesaloneare
not a silver bullet. Our results indicate that the performance of
the Golden Features SVM is not almost perfect, with only marginal
improvements over a strawman baseline that always predicts that
706
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 07:58:31 UTC from IEEE Xplore.  Restrictions apply. ICSE ’22, May 21–29, 2022, Pittsburgh, PA, USA Hong Jin Kang, Khai Loong Aw, and David Lo
a warning is actionable. Our study motivates the need for more
work.Futureworkshouldexploremorefeaturesandtechniques,
includingpre-processingmethods(e.g.SMOTE)andothermachine
learning methods (e.g. semi-supervised learning).
All that glitters is not gold; it is essential to qualitatively
analyze and understand the reasons for seemingly strong
performance. Despiteachievingexcellentperformance,theGolden
Features have subtle bugs related to data leakage and data dupli-cation. This emphasizes the importance of a deeper analysis of
experimentalresults,andbothquantitativeandqualitativeanalysis
areessential.Wecallfortheneedformorereplicationstudies,as
suchworkscanhighlightopportunitiesandchallengesforfuture
work.Ourworkreemphasizestheneedtocomparebothexisting
and newly proposed techniques to simple baselines [9].
Theclosed-warningheuristicforgeneratinglabelsallows
a large dataset to be built but is not enough for building abenchmark.
Ourworkshedslightonthelimitationsoftheclosed-
warningheuristic,suggestingthatitmaynotbesufficientlyaccu-
rate;warningsmaybeclosedincidentally,andactionablewarnings
may stay open for years before they are closed.
As a benchmark is essential for charting research direction [ 47],
theconstructionofarepresentativedatasetisimportant.Several
studies have proposed similar processes relying on the closed-
warning heuristic to build a ground-truth dataset [ 12,16,23,53],
while others have relied on manual labelling [ 13,14,25,42,45,59].
Heuristicsenablesautomation,allowingforadatasetofagreater
scale. However, heuristics may not be robust enough. On the other
hand, solely labelling warnings through manual analysis is not
scalableandmaybesubjecttoanannotator’sbias.Wesuggestthat
datasets proposed in the future should rely on bothheuristics and
manuallabelling;apartfromitsgreaterscale,theclosed-warning
heuristicenablesrichinformationtobegatheredfromtheactivities
ofthedeveloperstohelpthemanuallabellingprocess.Forexam-
ple, code commits provide richer information, such as the commit
message, simplifying the task for human annotators. In contrast,
prior studies [ 13,14,25,42,45,59] have relied on annotators who
inspected only the source code that warnings are reported on. Our
experimentssuggestusingtheclosed-warningheuristic,followed
by manual labelling is promising – the annotators had a strongagreement (Cohen’s Kappa > 0.8), while no strong agreement in
manual labelling has been demonstrated in prior work.
Agoodbenchmarkrequiresscaleandshouldbelabelledbymany
annotators. Fieldssuch ascode clone detectionhave createdlarge
benchmarks through community effort [ 40]. This motivates the
need for community effort to build a benchmark for actionable
warningdetectiontoo.Asaderivativeofthisempiricalstudy,we
have labelled 1,300 closed warnings, usable as a starting point.
6.2 Threats to Validity
Apossiblethreatto internalvalidity istheincorrectimplementa-
tion of our code. To mitigate this, we reused existing data and code
whenever possible, including the dataset by Wang et al. [ 53] and
Yang et al. [ 56,58], and the feature extractor by Wang et al. [ 53].
Our code and data are available [1].
Threatsto constructvalidity arerelatedtotheappropriateness
of the evaluation metrics. We considered the evaluation metricsusedinpriorstudies[ 53,56,58],andalsocomputedF1,whichhave
been used in many classification tasks [ 24,38,62]. F1 captures the
tradeoffbetweenPrecisionandRecall,andisamoreappropriate
measure on an imbalanced dataset.
Threatsto externalvalidity concernthegeneralizabilityofour
findings. There are several threats to external validity, including
the choice of projects and techniques used in our experiments.
Onethreattoexternalvalidityisthechoiceofprojectsstudiedin
thispaper.Westudiednineprojectsusedinpreviousstudies,and
we considered another set of projects that actively use Findbugs.
All considered projects were large, mature projects.
Another threat to external validity is the choice of the approach
used as an actionable warning detector. Our analysis focuses onthe use of the Golden Features SVM, which had the best median
performanceinexperimentsinpriorstudiesandwasthesuggested
model [56]. Other approaches using different features may achieve
stronger performance.
Our analysis regarding unconfirmed actionable warnings and
false alarms also relies on human oracles (configuration/filter files
written by developers, manual labelling by human annotators) that
may not be perfectly accurate. Moreover, these oracles will pro-
duce more accurate labels for warnings that are easier to label (e.g.
shorterandwell-documentedcode,warningtypesthatareeasierto
reasonabout).This mayskewthedistributionoflabels andwarn-
ingsinthedatasets.Tomitigatesomeoftheabovethreats,multiple
annotators labeled the warnings independently, and we report the
inter-rater reliability. We achieved a strong agreement (Cohen’s
Kappa>0.8).TomitigatethethreatofunmaintainedFindbugsfilter
files, we selected only popular projects that have filter files with at
least 10 lines.
AnotherthreatisthefocusonFindbugsand Javaprojects.Our
analysismaynotgeneralizetowarningsofotherASATs,suchas
Infer [8]. This threat is mitigated as Findbugs detects a wide range
of bug patterns, including bugs patterns shared by other ASATs,
andthefeaturesarenotlanguage-specific.Moreover,weusedthe
same dataset as prior studies [ 53,56,58]. Findbugs is among the
most commonly used ASATs [ 60], having been downloaded over a
million times.
7 RELATED WORK
In Section 2, we discussed the studies related to ASATs as well
astheapproachesthatusemachinelearningtodetectactionable
warnings. We discuss other related studies in this section.
Many studies have performed retrospectives of the state-of-the-
artforvariousSoftwareEngineeringtasks.Somepapers[ 10,17,30,
34,61]studythelimitationsofexistingtools,andothers[ 21,31,37]
assesstheapplicabilityofthetoolswhenappliedtosituationswith
a setting different from the original experiments. Our study not
only uncovers limitations of the Golden Features, but investigates
the performance of the Golden Features under different settings (a
different warning oracle in our study).
Other studies have shown the need to carefully consider data
usedinexperiments[ 3,20,26,51,63].SimilartoAllamanisetal.[ 3],
we show thatdata duplication may causeoverly optimistic exper-
imental results. Similar to Kalliamvakou et al. [ 20], we suggest
that researchers should be careful about interpreting automati-cally mined data. Kochhar et al. [
26] investigated multiple types
707
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 07:58:31 UTC from IEEE Xplore.  Restrictions apply. Detecting False Alarms from Automatic Static Analysis Tools: How Far are We? ICSE ’22, May 21–29, 2022, Pittsburgh, PA, USA
ofbiasthataffectdatasetsusedtoevaluatebuglocalizationtech-
niques [54,64], and, similar to our work, find that prior experi-
mentalresultswereimpactedbybiasinthedatasets.Ourworkis
similar to the work of Tu et al. [ 51] in highlighting the problem of
data leakage, where information from the future is used by aclassi-
fierandleadtooveroptimisticexperimentalresults.Ouranalysis
indicates that there may be delays before developers inspect staticanalysis warnings. Related to this, Zheng et al. [
63] found that the
statusofmanyissuesinBugzillamayonlybechangedafterlarge
delays. These delays have implications for heuristics that are used
toautomaticallyinferlabelsfromhistoricaldata(inourcase:ifa
warning is actionable).
Sheppardetal. [ 46]hadpreviouslydiscusseddataqualityina
commonlyuseddatasetfordefectprediction.Whilebothourstudy
as well as Sheppard et al. raise the problem of data duplication, the
duplicatedinstancesinthedatasetanalyzedinthispaperreferto
thesamewarningsandlabelsoccurringinbothtrainingandtesting
dataset.Incontrast,Sheppardetal.referstoduplicatedcasesthat
occurnaturally(similarfeaturesbelongingtodifferentinstances,
e.g. software modules).
8 CONCLUSION AND FUTURE WORK
In this study, we show that the problem of detecting actionable
warningsfromAutomaticStaticAnalysisToolsisfarfromsolved.In
prior work, the strong performance of the “Golden Features” were
contributedbydataleakageanddataduplicationissues,whichwere
subtle and difficult to detect.
Ourstudyhighlightsthe need fordeeperstudyofthewarning
oracle to determine ground-truth labels. By changing the reference
revision, different conclusions about performance of the Golden
Featurescanbereached.Furthermore,theoracleproducelabelsthat
human annotators and developers of projects using static analysis
toolsmaynotagreewith.OurexperimentsshowthattheGolden
Features SVM had improved performance on cleaner data.
Ourstudyindicatesopportunitiesandchallengesforfuturework.
It highlights the need for community effort to build a large andreliable benchmark and to compare newly proposed approaches
with strawman baselines. A replication package is provided at
https://github.com/soarsmu/SA_retrospective
ACKNOWLEDGMENTS
Thisresearch/projectissupportedbytheNationalResearchFounda-
tion,Singapore,underitsIndustryAlignmentFund-Pre-positioning
(IAF-PP)FundingInitiative.Anyopinions,findingsandconclusions
or recommendations expressed in this material are those of the au-
thor(s)anddonotreflecttheviewsofNationalResearchFoundation,
Singapore.
REFERENCES
[1] [n.d.]. Replication Package. https://github.com/soarsmu/SA_retrospective.
[2] 2021. Findbugs Filter file. http://findbugs.sourceforge.net/manual/filter.html.[3]
Miltiadis Allamanis. 2019. The adverse effects of code duplication in machine
learningmodelsofcode.In ACMSIGPLANInternationalSymposiumonNewIdeas,
New Paradigms, and Reflections on Programming and Software (Onward! 2019).
143–153. https://doi.org/10.1145/3359591.3359735
[4]Nathaniel Ayewah and William Pugh. 2010. The Google Findbugs Fixit. In 19th
International Symposium on Software Testing and Analysis (ISSTA 2010). 241–252.
https://doi.org/10.1145/1831708.1831738[5]Nathaniel Ayewah, William Pugh, David Hovemeyer, J David Morgenthaler, and
John Penix. 2008. Using Static Analysis to Find Bugs. IEEE Software 25, 5 (2008),
22–29. https://doi.org/10.1109/MS.2008.130
[6]Vipin Balachandran. 2013. Reducing human effort and improving quality in peer
code reviews using automatic static analysis and reviewer recommendation. In
35th International Conference on Software Engineering (ICSE 2013). IEEE, 931–940.
https://doi.org/10.1109/ICSE.2013.6606642
[7]Moritz Beller, Radjino Bholanath, Shane McIntosh, and Andy Zaidman. 2016.
Analyzing the State of Static Analysis: A Large-Scale Evaluation in Open Source
Software.In IEEE23rdInternationalConferenceonSoftwareAnalysis,Evolution,
andReengineering(SANER2016).IEEEComputerSociety,470–481. https://doi.
org/10.1109/SANER.2016.105
[8]DinoDistefano,ManuelFähndrich,FrancescoLogozzo,andPeterWO’Hearn.
2019. Scaling static analyses at Facebook. Commun. ACM 62, 8 (2019), 62–70.
https://doi.org/10.1145/3338112
[9]WeiFuandTimMenzies.2017. Easyoverhard:Acasestudyondeeplearning.
In11th Joint Meeting on Foundations of Software Engineering (ESEC/FSE 2017) .
49–60. https://doi.org/10.1145/3106237.3106256
[10]DavidGros,HariharanSezhiyan,PremDevanbu,andZhouYu.2020.CodetoCom-
ment“Translation”:Data,Metrics,Baselining&Evaluation.In 35thIEEE/ACM
International Conference on Automated Software Engineering (ASE 2020). IEEE,
746–757. https://doi.org/10.1145/3324884.3416546
[11]Andrew Habib and Michael Pradel. 2018. How many of all bugs do we find?a study of static bug detectors. In 33rd IEEE/ACM International Conference on
AutomatedSoftwareEngineering(ASE2018).IEEE,317–328. https://doi.org/10.
1145/3238147.3238213
[12]QuinnHanam,LinTan,ReidHolmes,andPatrickLam.2014. Findingpatterns
in static analysis alerts: improving actionable alert ranking. In 11th Working
Conference on mining software repositories (MSR 2014). 152–161. https://doi.org/
10.1145/2597073.2597100
[13]Sarah Heckman and Laurie Williams. 2008. On establishing a benchmark forevaluating static analysis alert prioritization and classification techniques. In
2nd ACM-IEEE International Symposiumon Empirical Software Engineering and
Measurement (ESEM 2008). 41–50. https://doi.org/10.1145/1414004.1414013
[14]Sarah Heckman and Laurie Williams. 2009. A Model Building Process forIdentifying Actionable Static Analysis Alerts. In International Conference on
Software Testing Verification and Validation (ICST 2009). IEEE, 161–170. https:
//doi.org/10.1109/ICST.2009.45
[15]Sarah Heckman and Laurie Williams. 2011. A systematic literature review ofactionable alert identification techniques for automated static code analysis.
Information and Software Technology (IST) 53, 4 (2011), 363–387. https://doi.org/
10.1016/j.infsof.2010.12.007
[16]SarahHeckmanandLaurieWilliams.2013. Acomparativeevaluationofstatic
analysisactionablealertidentificationtechniques.In 9thInternationalConference
onPredictiveModelsinSoftwareEngineering(PROMISE2013).1–10. https://doi.
org/10.1145/2499393.2499399
[17]VincentJHellendoornandPremkumarDevanbu.2017. Aredeepneuralnetworksthebestchoiceformodelingsourcecode?.In 11thJointMeetingonFoundationsof
Software Engineering (ESEC/FSE 2017). 763–773. https://doi.org/10.1145/3106237.
3106290
[18]David Hovemeyer and William Pugh. 2004. Finding bugs is easy. ACM SIGPLAN
notices39, 12 (2004), 92–106. https://doi.org/10.1145/1052883.1052895
[19]BrittanyJohnson,YoonkiSong,EmersonR.Murphy-Hill,andRobertW.Bow-didge. 2013. Why don’t software developers use static analysis tools to find
bugs?.In 35thInternationalConferenceonSoftwareEngineering,(ICSE2013) .IEEE
Computer Society, 672–681. https://doi.org/10.1109/ICSE.2013.6606613
[20]Eirini Kalliamvakou, Georgios Gousios, Kelly Blincoe, Leif Singer, Daniel M
German, andDaniela Damian.2014. Thepromises andperils ofmining Github.
In11thworkingconferenceonMiningSoftwareRepositories(MSR2014).92–101.
https://doi.org/10.1145/2597073.2597074
[21]Hong Jin Kang, Tegawendé F Bissyandé, and David Lo. 2019. Assessing the
Generalizability of code2vec Token Embeddings. In 34th IEEE/ACM International
Conference on Automated Software Engineering (ASE 2019). IEEE, 1–12. https:
//doi.org/10.1109/ASE.2019.00011
[22]Shachar Kaufman, Saharon Rosset, Claudia Perlich, and Ori Stitelman. 2012.
Leakageindatamining:Formulation,detection,andavoidance. ACMTransactions
on KnowledgeDiscovery fromData (TKDD) 6, 4(2012), 1–21. https://doi.org/10.
1145/2382577.2382579
[23]Sunghun Kim and Michael D Ernst. 2007. Prioritizing warning categories byanalyzing software history. In 4th International Workshop on Mining Software
Repositories(MSR07:ICSEWorkshops2007).IEEE,27–27. https://doi.org/10.1109/
MSR.2007.26
[24]Sunghun Kim, E James Whitehead, and Yi Zhang. 2008. Classifying software
changes: Clean or buggy? IEEE Transactions on Software Engineering (TSE) 34, 2
(2008), 181–196. https://doi.org/10.1109/TSE.2007.70773
[25]Ugur Koc, Shiyi Wei, Jeffrey S Foster, Marine Carpuat, and Adam A Porter. 2019.
An Empirical Assessment of Machine Learning Approaches for Triaging Reports
ofaJavaStaticAnalysisTool.In 12thIEEEconferenceonsoftwaretesting,validation
708
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 07:58:31 UTC from IEEE Xplore.  Restrictions apply. ICSE ’22, May 21–29, 2022, Pittsburgh, PA, USA Hong Jin Kang, Khai Loong Aw, and David Lo
andverification(ICST2019).IEEE,288–299. https://doi.org/10.1109/ICST.2019.
00036
[26]Pavneet SinghKochhar, Yuan Tian,and DavidLo. 2014. Potential biases inbug
localization: Do they matter?. In Proceedings of the 29th ACM/IEEE international
conference on Automated Software Engineering (ASE 2014). 803–814.
[27]TedKremenek,KenAshcraft,JunfengYang,andDawsonEngler.2004.Correlation
exploitation in error ranking. ACM SIGSOFT Software Engineering Notes 29, 6
(2004), 83–93. https://doi.org/10.1145/1029894.1029909
[28]JRichardLandisandGaryGKoch.1977.Themeasurementofobserveragreement
for categorical data. biometrics (1977), 159–174.
[29]GuangtaiLiang,LingWu,QianWu,QianxiangWang,TaoXie,andHongMei.
2010. Automaticconstructionofaneffectivetrainingsetforprioritizingstatic
analysis warnings. In IEEE/ACM international conference on Automated Software
Engineering (ASE 2010). 93–102. https://doi.org/10.1145/1858996.1859013
[30]Bo Lin, Shangwen Wang, Kui Liu, Xiaoguang Mao, and Tegawendé F Bissyandé.
2021. Automated Comment Update: How Far are We?. In IEEE/ACM 29th In-
ternational Conference on Program Comprehension (ICPC 2021). IEEE, 36–46.
https://doi.org/10.1109/ICPC52881.2021.00013
[31]Bin Lin, Fiorella Zampetti, Gabriele Bavota, Massimiliano Di Penta, Michele
Lanza, and Rocco Oliveto. 2018. Sentiment analysis for software engineering:How far can we go?. In 40th International Conference on Software Engineering
(ICSE 2018). 94–104. https://doi.org/10.1145/3180155.3180195
[32]Kui Liu, Dongsun Kim, Tegawendé F Bissyandé, Shin Yoo, and Yves Le Traon.
2018. MiningFixPatternsforFindbugsViolations. IEEETransactionsonSoftware
Engineering (2018). https://doi.org/10.1109/TSE.2018.2884955
[33]KuiLiu,AnilKoyuncu,DongsunKim,andTegawendéFBissyandé.2019.AVATAR:
Fixing Semantic Bugs with Fix Patterns of Static Analysis Violations. In 26th
IEEE International Conference on Software Analysis, Evolution and Reengineering
(SANER 2019). IEEE, 1–12. https://doi.org/10.1109/SANER.2019.8667970
[34]Zhongxin Liu, Xin Xia, Ahmed E Hassan, David Lo, Zhenchang Xing, and Xinyu
Wang.2018. Neural-machine-translation-basedcommitmessagegeneration:how
fararewe?.In 33rdACM/IEEEInternationalConferenceonAutomatedSoftware
Engineering (ASE 2018). 373–384. https://doi.org/10.1145/3238147.3238190
[35]DiegoMarcilio,RodrigoBonifácio,EduardoMonteiro,EdnaCanedo,WelderLuz,and Gustavo Pinto. 2019. Are static analysis violations really fixed? a closer look
atrealisticusageofSonarQube.In IEEE/ACM27thInternationalConferenceon
Program Comprehension (ICPC 2019). IEEE, 209–219. https://doi.org/10.1109/
ICPC.2019.00040
[36]Sebastiano Panichella, Venera Arnaoudova, Massimiliano Di Penta, and Giuliano
Antoniol.2015. Wouldstaticanalysistoolshelpdeveloperswithcodereviews?.In
22ndIEEEInternationalConferenceonSoftwareAnalysis,Evolution,andReengineer-
ing (SANER 2015). IEEE, 161–170. https://doi.org/10.1109/SANER.2015.7081826
[37]MdRafiqulIslamRabin,NghiDQBui,KeWang,YijunYu,LingxiaoJiang,and
MohammadAminAlipour.2021. OnthegeneralizabilityofNeuralProgramMod-
elswithrespecttosemantic-preservingprogramtransformations. Information
and Software Technology (IST) 135 (2021), 106552. https://doi.org/10.1016/j.infsof.
2021.106552
[38]FoyzurRahman,DarylPosnett,andPremkumarDevanbu.2012. Recallingthe
"imprecision"ofcross-projectdefectprediction.In ACMSIGSOFT20thInterna-
tional Symposium on the Foundations of Software Engineering (FSE 2012). 1–11.
https://doi.org/10.1145/2393596.2393669
[39]MarcoTulioRibeiro,SameerSingh,andCarlosGuestrin.2016. "WhyshouldItrust you?" Explaining the predictions of any classifier. In 22nd ACM SIGKDD
international conference on Knowledge Discovery and Data mining. 1135–1144.
https://doi.org/10.1145/2939672.2939778
[40]Chanchal K Roy and James R Cordy. 2018. Benchmarks for software clonedetection: A ten-year retrospective. In 25th IEEE International Conference on
SoftwareAnalysis,EvolutionandReengineering(SANER2018).IEEE,26–37. https:
//doi.org/10.1109/SANER.2018.8330194
[41]NickRutar,ChristianBAlmazan,andJeffreySFoster.2004. AComparisonofBug
FindingToolsforJava.In 15thInternationalSymposiumonSoftwareReliability
Engineering (ISSRE 2004). IEEE, 245–256. https://doi.org/10.1109/ISSRE.2004.1
[42]JosephRuthruff,JohnPenix,JMorgenthaler,SebastianElbaum,andGreggRother-
mel.2008. Predictingaccurateandactionablestaticanalysiswarnings.In 30th
ACM/IEEE International Conference on Software Engineering (ICSE 2008) . IEEE,
341–350. https://doi.org/10.1145/1368088.1368135
[43]CaitlinSadowski,EdwardAftandilian,AlexEagle,LiamMiller-Cushon,andCiera
Jaspan. 2018. Lessons from building static analysis tools at Google. Commun.
ACM61, 4 (2018), 58–66. https://doi.org/10.1145/3188720
[44]Caitlin Sadowski,Jeffrey VanGogh, CieraJaspan, EmmaSoderberg, andCollin
Winter. 2015. Tricorder: Buildinga Program AnalysisEcosystem. In IEEE/ACM
37th IEEE International Conference on Software Engineering (ICSE 2015) , Vol. 1.
IEEE, 598–608. https://doi.org/10.1109/ICSE.2015.76
[45]Haihao Shen, Jianhong Fang, and Jianjun Zhao. 2011. Efindbugs: Effective Error
RankingforFindbugs.In 4thIEEEInternationalConferenceonSoftwareTesting,
VerificationandValidation(ICST2011).IEEE,299–308. https://doi.org/10.1109/
ICST.2011.51[46]Martin Shepperd, Qinbao Song, Zhongbin Sun, and Carolyn Mair. 2013. Data
quality:SomecommentsontheNASAsoftwaredefectdatasets. IEEETransactions
on Software Engineering (TSE) 39, 9 (2013), 1208–1215.
[47]Susan Elliott Sim, Steve Easterbrook, and Richard C Holt. 2003. Using Bench-marking to Advance Research: A Challenge to Software Engineering. In 25th
International Conference on Software Engineering (ICSE 2003) . IEEE, 74–83. https:
//doi.org/10.1109/ICSE.2003.1201189
[48]Mohammad Tahaei, Kami Vaniea, Konstantin Beznosov, and Maria K Wolters.2021. Security Notifications in Static Analysis Tools: Developers’ Attitudes,
Comprehension, and Ability to Act on Them. In 2021 CHI Conference on Human
Factors in Computing Systems. 1–17. https://doi.org/10.1145/3411764.3445616
[49]FerdianThung,DavidLo,LingxiaoJiang,FoyzurRahman,PremkumarTDevanbu,
et al.2012. To what extent could we detect field defects? an empirical study
of false negatives in static bug finding tools. In 27th IEEE/ACM International
ConferenceonAutomatedSoftwareEngineering(ASE2012).IEEE,50–59. https:
//doi.org/10.1007/s10515-014-0169-8
[50]KristínFjólaTómasdóttir,MauricioAniche,andArievanDeursen.2017.WhyandhowJavaScriptdevelopersuselinters.In 32ndIEEE/ACMInternationalConference
onAutomatedSoftwareEngineering(ASE2017).IEEE,578–589. https://doi.org/
10.1109/ASE.2017.8115668
[51]Feifei Tu, Jiaxin Zhu, Qimu Zheng, and Minghui Zhou. 2018. Be careful of
when:anempiricalstudyontime-relatedmisuseofissuetrackingdata.In 26th
ACM Joint Meeting on European Software Engineering Conference and Symposium
on the Foundations of Software Engineering (ESEC/FSE 2018). 307–318. https:
//doi.org/10.1145/3236024.3236054
[52]Carmine Vassallo, Sebastiano Panichella, Fabio Palomba, Sebastian Proksch, Har-aldCGall,andAndyZaidman.2020. Howdevelopersengagewithstaticanalysis
toolsindifferentcontexts. EmpiricalSoftwareEngineering(EMSE) 25,2(2020),
1419–1457. https://doi.org/10.1007/s10664-019-09750-5
[53]JunjieWang,SongWang,andQingWang.2018. Istherea"golden"featureset
for static warning identification?: an experimental evaluation. In 12th ACM/IEEE
International Symposium on Empirical Software Engineering and Measurement,
(ESEM 2018). ACM, 17:1–17:10. https://doi.org/10.1145/3239235.3239523
[54]ShaoweiWangandDavidLo.2014. Versionhistory,similarreport,andstructure:
Putting them together for improved bug localization. In Proceedings of the 22nd
International Conference on Program Comprehension (ICPC 2014). 53–63.
[55]Chadd C Williams and Jeffrey K Hollingsworth. 2005. Automatic Mining of
SourceCodeRepositoriestoImproveBugFindingTechniques. IEEETransactions
on Software Engineering (TSE) 31, 6 (2005), 466–480. https://doi.org/10.1109/TSE.
2005.63
[56]XueqiYang,JianfengChen,RahulYedida,ZheYu,andTimMenzies.2021. Learn-
ing to recognize actionable static code warnings (is intrinsically easy). Empirical
SoftwareEngineering(EMSE) 26,3(2021),56. https://doi.org/10.1007/s10664-021-
09948-6
[57]Xueqi Yangand Tim Menzies.2021. Documentingevidence of areproductionof‘istherea“golden”featuresetforstaticwarningidentification?—anexperi-
mental evaluation’. In 29th ACM Joint Meeting on European Software Engineering
Conference and Symposium on the Foundations of Software Engineering (ESEC/FSE
2021). 1603–1603.
[58]Xueqi Yang, Zhe Yu, Junjie Wang, and Tim Menzies. 2021. Understanding static
codewarnings:AnincrementalAIapproach. ExpertSyst.Appl. 167(2021),114134.
https://doi.org/10.1016/j.eswa.2020.114134
[59]Ulas Yüksel and Hasan Sözer. 2013. Automated Classification of Static Code
Analysis Alerts: A Case Study. In IEEE International Conference on Software
Maintenance (ICSM 2013). IEEE, 532–535. https://doi.org/10.1109/ICSM.2013.89
[60]Fiorella Zampetti, Simone Scalabrino, Rocco Oliveto, Gerardo Canfora, and Mas-
similiano Di Penta. 2017. How open source projects use static code analy-sis tools in continuous integration pipelines. In IEEE/ACM 14th International
Conference on Mining Software Repositories (MSR 2017). IEEE, 334–344. https:
//doi.org/10.1109/MSR.2017.2
[61]ZhengranZeng,YuqunZhang,HaotianZhang,andLingmingZhang.2021. Deepjust-in-timedefectprediction:howfararewe?.In 30thACMSIGSOFTInternational
Symposiumon SoftwareTesting andAnalysis(ISSTA 2021).427–438. https://doi.
org/10.1145/3460319.3464819
[62]Ting Zhang, Bowen Xu, Ferdian Thung, Stefanus Agus Haryono, David Lo, and
Lingxiao Jiang. 2020. Sentiment Analysis for Software Engineering: How FarCan Pre-trained Transformer Models Go?. In IEEE International Conference on
Software Maintenance and Evolution (ICSME 2020). IEEE, 70–80. https://doi.org/
10.1109/ICSME46990.2020.00017
[63]QimuZheng,AudrisMockus,andMinghuiZhou.2015. Amethodtoidentifyand
correctproblematic softwareactivitydata:Exploitingcapacity constraintsand
data redundancies. In 10th Joint Meeting on Foundations of Software Engineering
(ESEC/FSE 2015). 637–648. https://doi.org/10.1145/2786805.2786866
[64]Jian Zhou, Hongyu Zhang, and David Lo. 2012. Where should the bugs be fixed?
more accurate information retrieval-based bug localization based on bug reports.
In201234th InternationalConferenceon SoftwareEngineering(ICSE 2012).IEEE,
14–24.
709
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 07:58:31 UTC from IEEE Xplore.  Restrictions apply. 