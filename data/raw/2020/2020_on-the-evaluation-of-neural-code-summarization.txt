On the Evaluation of Neural Code Summarization
Ensheng Shia,â€ , Yanlin Wangb,Â§, Lun Dub, Junjie Chenc
Shi Hanb, Hongyu Zhangd, Dongmei Zhangb, Hongbin Suna,Â§
aXiâ€™an Jiaotong UniversitybMicrosoft Research
cTianjin UniversitydThe University of Newcastle
s1530129650@stu.xjtu.edu.cn,hsun@mail.xjtu.edu.cn
{yanlwang,lun.du,shihan,dongmeiz}@microsoft.com
junjiechen@tju.edu.cn,hongyu.zhang@newcastle.edu.au
ABSTRACT
Source code summaries are important for program comprehension
andmaintenance.However,thereareplentyofprogramswithmiss-
ing,outdated,ormismatchedsummaries.Recently,deeplearning
techniques have been exploited to automatically generate sum-
maries for given code snippets. To achieve a profound understand-
ingofhowfarwearefromsolvingthisproblemandprovidesugges-
tions to future research, in this paper, we conduct a systematic and
in-depthanalysis of5 state-of-the-artneural code summarization
models on 6 widely used BLEU variants, 4 pre-processing oper-ations and their combinations, and 3 widely used datasets. The
evaluationresultsshowthatsomeimportantfactorshaveagreat
influenceonthemodelevaluation,especiallyontheperformanceof
models and the ranking among the models. However, these factors
mightbeeasilyoverlooked.Specifically,(1)theBLEUmetricwidely
used in existing work of evaluating code summarization models
hasmanyvariants.Ignoringthedifferencesamongthesevariants
could greatly affect the validity of the claimed results. Besides, we
discover and resolve an important and previously unknown bugin BLEU calculation in a commonly-used software package. Fur-
thermore, we conduct human evaluations and find that the metric
BLEU-DC is most correlated to human perception; (2) code pre-processing choices can have a large (from -18% to +25%) impacton the summarization performance and should not be neglected.
We also explore the aggregation of pre-processing combinations
and boost the performance of models; (3) some important char-acteristics of datasets (corpus sizes, data splitting methods, and
duplicationratios)haveasignificantimpactonmodelevaluation.
Based ontheexperimental results,we giveactionable suggestions
forevaluatingcodesummarizationandchoosingthebestmethodindifferentscenarios.Wealsobuildasharedcodesummarization
toolbox to facilitate future research.
CCS CONCEPTS
â€¢Softwareanditsengineering â†’Softwaremaintenancetools.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation
on the first page. Copyrights for components of this work owned by others than ACMmustbehonored.Abstractingwithcreditispermitted.Tocopyotherwise,orrepublish,topostonserversortoredistributetolists,requirespriorspecificpermissionand/ora
fee. Request permissions from permissions@acm.org.
ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
Â© 2022 Association for Computing Machinery.
ACM ISBN 978-1-4503-9221-1/22/05...$15.00
https://doi.org/10.1145/3510003.3510060KEYWORDS
Code summarization, Empirical study, Deep learning, Evaluation
ACM Reference Format:
Ensheng Shia,â€ , Yanlin Wangb,Â§, Lun Dub, Junjie Chenc, Shi Hanb, Hongyu
Zhangd, Dongmei Zhangb, Hongbin Suna,Â§. 2022. On the Evaluation of
NeuralCodeSummarization.In 44thInternationalConferenceonSoftware
Engineering (ICSE â€™22), May 21â€“29, 2022, Pittsburgh, PA, USA. ACM, New
York, NY, USA, 12 pages. https://doi.org/10.1145/3510003.3510060
1 INTRODUCTION
Source code summaries are important for program comprehen-
sionandmaintenancesincedeveloperscanquicklyunderstanda
piece of code by reading its natural language description. However,
documenting code with summaries remains a labor-intensive and
time-consumingtask.Asaresult,codesummariesareoftenmissing,
mismatched, or outdated in many projects [ 8,17,59]. Therefore,
automatic generation of code summaries is desirable and many
approaches have been proposed over the years [14, 20, 21, 51, 56].
Recently, deep learning (DL) based models are exploited to gen-
eratebetternaturallanguagesummariesforcodesnippets[ 1,25â€“
27,29,34,61,69]. These models usually adopt a neural machine
translation framework to learn the alignment between code and
summaries.SomestudiesalsoenhanceDL-basedmodelsbyincorpo-
ratinginformationretrievaltechniques[ 65,69].Generally,existing
neural source code summarization models show promising results
and claim their superiority over traditional approaches.
However,wenoticethatinthecurrentcodesummarizationwork,
therearemanyimportantdetailsthatcouldbeeasilyoverlooked
and important issues that have not received much attention. These
details and issues are associated with evaluation metrics, evaluated
datasetsandexperimentalsettings,andaffecttheevaluationand
comparisonofapproaches.Inthiswork,wewouldliketodivedeepinto the problem and answer: how to evaluate and compare code
summarization models more correctly and comprehensively?
To answer the above question, we conduct systematic exper-
iments of 5 representative code summarization approaches (in-
cludingCodeNN[ 29],Deepcom[ 25],Astattgru[ 34],Rencos[ 69]
andNCS[ 1])on6widelyusedBLEUvariants,4extensivelyused
code pre-processing operations (Table 4), and 3 commonly useddatasets (including TL-CodeSum [
27], Funcom [ 34], and Code-
SearchNet [ 28]). The 6 BLEU variants and 4 code pre-processing
operations cover most of the studies on code summarization since
2010. Each dataset is used in at least 5 previous studies.
â€ Work performed during internship at Microsoft Research Asia.
Â§Corresponding authors.
15972022 IEEE/ACM 44th International Conference on Software Engineering (ICSE)
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:30:33 UTC from IEEE Xplore.  Restrictions apply. ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA Shi, et al.
Our experiments can be divided into three major parts. First, we
conduct anin-depth analysis ofthe BLEU metric, whichis widely
used in previous code summarization work [1, 4, 15, 25â€“27, 29, 33,
34,61,64,65,69]andperformhumanevaluationstofindtheBLEU
variant that best correlates with human perception (Section 4.1).
Then, we study different code pre-processing operations in recent
codesummarizationworksandexploreanensemblelearningbasedtechnique to boost the performance of code summarization models
(Section4.2).Finally,weconductexperimentsonthethreedatasets
fromthree perspectives:corpussizes, datasplitting methods,and
duplicationratios(Section4.3).Throughextensiveexperiments,we
obtain the following major findings about the current neural code
summarization evaluation.
Thefirst major finding is that there is a wide variety of BLEU
metrics used in prior work and they produce rather differentresults for the same generated summaries. Some previous stud-ies [
4,15,25,26,29,34,37,64,65,69] accurately describe the
BLEU metric used and compare models under the same BLEU met-
ric[4,15,25,26,29,34,37,64,65,69].However,therearestillmany
works [1,16,27,61,66] cite or describe inconsistent BLEU metrics,
leading to confusion for subsequent research. Whatâ€™s worse, some
software packages used in [ 25,26,64] for calculating BLEU are
buggy:1/circlecopyrtthey may produce a BLEU score greater than 100% (or
even >700%), which extremely exaggerates the performance of
code summarization models, and 2/circlecopyrtthe results are also different
acrossdifferentpackageversions.Moreimportantly,BLEUscores
between papers cannot be directly compared [ 50]. However, some
studies [1,66] copy the BLEU scores reported in other papers and
directly compare with them under different BLEUmetrics. For ex-
ample,[1]copiedthescoresreportedin[ 64],and[66]copiedthe
scoresreportedin[ 1].TheBLEUimplementationsintheirreleased
code [1,64] are different. Furthermore, the study [ 66]d o e sn o t
release its sourcecode. Therefore, these studies mayoverestimate
theirmodelperformanceormayfailtoachievefaircomparisons,
even though they are evaluated on the same dataset with the same
experimental setting. Through human evaluation, we find that
BLEU-DC (Section 2.2) correlates with human perception the most.
We further give some actionable suggestions on the usage of BLEU
in Section 4.1.
Thesecond major finding is that different pre-processing combi-
nationscanaffecttheoverallperformancebyanoticeablemarginof
-18% to +25%. The results of the exploration experiment show that
asimpleensemblelearningtechniquecanboosttheperformance
of code summarization models. We also give actionable sugges-
tions on the choice and usage of code pre-processing operations in
Section 4.2.
Thethird major finding is that code summarization approaches
perform inconsistently on different datasets, i.e., one approachmay perform better than other approaches on one dataset andpoorly on another dataset. Furthermore, we experimentally find
thatthreedatasetattributes(corpussizes,datasplittingmethods,
and duplication ratios) have important impact on the performance
ofcode summarizationmodels. Wefurther givesomesuggestions
about evaluation datasets in Section 4.3.
In summary, our findings indicate that in order to evaluate and
comparecodesummarizationmodelsmorecorrectlyandcompre-
hensively,weneedtopaymuchattentiontotheimplementationof BLEU metrics, the way of code pre-processing, and the usage of
datasets. The major contributions of this work are as follows:
â€¢We conduct an extensive evaluation of five representative
neuralcodesummarizationmodelswithdifferentevaluation
metrics, code pre-processing operations, and datasets.
â€¢We conduct human evaluation and find that BLEU-DC ismost correlated to human perception for evaluating neu-
ralcodesummarizationmodelsamongthesixwidely-used
BLEU variants.
â€¢Weconcludethatmanyexistingcodesummarizationmodels
are not evaluated comprehensively and do not generalize
well in new experimental settings. Therefore, more research
is needed to further improve code summarization models.
â€¢Basedontheevaluationresults,wegiveactionablesugges-
tions for evaluating code summarization models from multi-
ple perspectives.
â€¢We build a shared code summarization toolbox1contain-
ing 6 BLEU variants implementation, 4 code pre-processing
operations and 16 of their combinations, 12 datasets, re-implementations of baseline approaches that do not have
publiclyavailablesourcecode,andallexperimentalresults
described in this paper.
2 BACKGROUND
2.1 Code Summarization
Intheearlystageofautomaticsourcecodesummarization,template-
basedapproaches [ 14,20,21,51,56]are widelyused.However,a
well-designed template requires expert domain knowledge. There-
fore, information retrieval (IR) based approaches [ 14,20,21,51]
are proposed. Thebasic ideais toretrieve termsfrom sourcecode
to generate term-based summaries or to retrieve similar source
code and use its summary as the target summary. However, the
retrieved summaries may not correctly describe the semantics and
behaviorofcodesnippets,leadingtothemismatchesbetweencode
and summaries.
Recently, Neural Machine Translation (NMT) based models are
exploited to generate summaries for code snippets [ 1,4,6,9,11,
15,16,22,25â€“27,29,33,34,37,61,63,64,67,68]. CodeNN [ 29]i s
an early attempt that uses only code token sequences, followedby various approaches that utilize AST [
4,25,26,33,34,37,54],
API knowledge [ 27], type information [ 9], global context [ 6,22,
63], reinforcement learning [ 61,62], multi-task learning [ 67], dual
learning[ 64,68],andpre-trainedlanguagemodels[ 15].Inaddition,
hybrid approaches [ 65,69] that combine the NMT-based and IR-
based methods are proposed and shown to be promising.
2.2 BLEU
Bilingual Evaluation Understudy (BLEU) [ 49] is commonly used
for evaluating the quality of the generated code summaries [ 1,
4,15,22,25â€“27,29,33,34,61,64,65,68,69]. In short, a BLEU
score is a percentage number between 0 and 100 that measures
the similarity between one sentence to a set of reference sentences
using constituent n-grams precision scores. BLEU typically uses
1https://github.com/DeepSoftwareAnalytics/CodeSumEvaluation
1598
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:30:33 UTC from IEEE Xplore.  Restrictions apply. On the Evaluation of Neural Code Summarization ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
BLEU-1, BLEU-2, BLEU-3, and BLEU-4 (calculated by 1-gram, 2-
gram, 3-gram, and 4-gram precisions) to measure the precision. A
valueof0meansthatthegeneratedsentencehasnooverlapwith
thereferencewhileavalueof100meansperfectoverlapwiththe
reference. Mathematically, the n-gram precision ğ‘ğ‘›is defined as:
ğ‘ğ‘›=/summationtext.1
ğ¶âˆˆ{Candidates }/summationtext.1
ğ‘›-gramâˆˆCCountclip(ğ‘›-gram)
/summationtext.1
ğ¶/primeâˆˆ{Candidates }/summationtext.1
ğ‘›-gramâˆˆC/primeCount(ğ‘›-gram/prime)(1)
BLEUcombinesalln-gramprecisionscoresusinggeometricmean:
ğµğ¿ğ¸ğ‘ˆ=ğµğ‘ƒÂ·exp/summationdisplay.1ğ‘
ğ‘›=1ğœ”ğ‘›logğ‘ğ‘› (2)
ğœ”ğ‘›is a uniform weight 1 /ğ‘(ğ‘=4). The straightforward calcu-
lation will result in high scores for short sentences or sentences
with repeated high-frequency n-grams. Therefore, Brevity Penalty
(BP)isusedtoscale thescoreandeachn-graminthe referenceis
limited to be used just once.
The original BLEU was designed for the corpus-level calcula-
tion[49].Forsentence-levelBLEU,sincethegeneratedsentences
and references are much shorter, ğ‘4is more likely to be zero when
the sentence has no 4-gram or 4-gram match. Then the geomet-
ric mean will be zero even if ğ‘1,ğ‘2, andğ‘3are large. In this case,
the BLEU score correlates poorly with human judgment. There-
fore, several smoothing methods are proposed [ 10] to mitigate this
problem.
As BLEU can be calculated at different levels and with differ-
ent smoothing methods, there are many BLEU variants used in
priorworkandtheycouldgeneratedifferentresultsforthesame
generated summary. Here, we use the names of BLEU variantsdefinedin[
19]andaddanotherBLEUvariant:BLEU-DM,which
is a Sentence BLEU without smoothing [ 10] and is based on the
implementation of NLTK 3.2.4. The meaning of these BLEU variants
are:
â€¢BLEU-CN:ThisisaSentenceBLEUmetricusedin[ 4,15,29].
ItappliesaLaplace-likesmoothingbyadding1toboththe
numerator and denominator of ğ‘ğ‘›forğ‘›â‰¥2.
â€¢BLEU-DM:ThisisaSentenceBLEUmetricusedin[ 25].It
uses smoothing method 0based on NLTK 3.2.4.
â€¢BLEU-DC: This is a Sentence BLEU metric based on
NLTK3.2.4smoothing method 4, used in [26, 64].
â€¢BLEU-FC:ThisisanunsmoothedCorpusBLEUmetricbased
on NLTK, used in [33, 34, 65].
â€¢BLEU-NCS: This is a Sentence BLEU metric used in [ 1]. It
applies a Laplace-like smoothing by adding 1 to both the
numerator and denominator of all ğ‘ğ‘›.
â€¢BLEU-RC: This is an unsmoothed Sentence BLEU metricused in [
69]. To avoid the divided-by-zero error, it adds a
tiny number 10âˆ’15in the numerator and a small number
10âˆ’9in the denominator of ğ‘ğ‘›.
ThereisaninterpretationofBLEUscoresbyGoogle[ 12],which
is shown in Table 1. We also show the original BLEU scores re-
portedbyexistingapproachesinTable2.Thesescoresvaryalot.
Specifically, 19.61 for Astattgru would be interpreted as â€œhard togetthegistâ€and38.17forDeepcomwouldbeinterpretedasâ€œun-
derstandabletogoodtranslationsâ€ accordingtoTable1.However,
thisinterpretationiscontrarytotheresultsshownin[ 34]where
Astattgru is relatively better than Deepcom. To study this issue,Table 1: Interpretation of BLEU scores [12].
Score Interpretation
<10 Almost useless
10-19 Hard to get the gist20-29 The gist is clear, but has significant grammatical errors30-40 Understandable to good translations40-50 High quality translations50-60 Very high quality, adequate, and fluent translations>60 Quality often better than human
we need to explore the difference and comparability of different
metrics and experimental settings used in different works.
Table 2: The best BLEU scores reported in their papers.
Model CodeNN [29] Deepcom [25] Astattgru [34] Rencos [69] NCS [1]
BLEU Score 20.50 38.17 19.61 20.70 44.14
3 EXPERIMENTAL DESIGN
3.1 Datasets
Weconductexperimentsonthreewidelyusedcodesummarization
datasets: TL-CodeSum [ 27], Funcom [ 34], and CodeSearchNet [ 28].
TL-CodeSum has87,136method-summarypairscrawledfrom
9,732 Java projects created from 2015 to 2016 with at least 20 stars.
Theratioofthetraining,validationandtestsetsis8:1:1.Sinceall
pairsareshuffled,therecanbemethodsfromthesameprojectin
thetraining,validation,and testsets.Inaddition,thereare exact
code duplicates among the three partitions.
CodeSearchNet is a well-formatted dataset containing 496,688
Javamethodsacrossthetraining,validation,andtestsets.Dupli-
cates are removed and the dataset is split into training, validation,
and test sets in proportion with 8:1:1 by project (80% of projects
intotraining,10%intovalidation,and10%intotesting)suchthat
code from the same repository can only exist in one partition.
Funcomis a collection of 2.1 million method-summary pairs
from 28,945 projects. Auto-generated code and exact duplicates
are removed. Then the dataset is split into three parts for training,
validation, and testing with the ratio of 9:0.5:0.5 by project.
InSection4.3,wefindthattheperformanceofthesamemodel
andtherankingamongthemodelsaredifferentondifferentdatasets.
Tostudywhichcharacteristic(suchascorpussize,deduplication,
etc) of datasets affects the performance and how they affect the
performance.wemodifysomecharacteristicsofthedatasetsand
obtain 9 new variants. In total, we experiment on 12 datasets, as
shown in Table 3 the statistics. In this paper, we use TLC, FCM,and CSN to denote TL-CodeSum, Funcom, and CodeSearchNet,respectively. TLC is the original TL-CodeSum. TLC
Dedupis a TL-
CodeSum variant, which removes the duplicated samples from the
testing set. CSN and FCM are CodeSearchNet and Funcom with
sourcecodethatcannotbeparsedbyjavalang2filteredout.Javalang
is used in many previous studies [ 26,37,47,48,69,70] to parse
sourcecode.Thethreemagnitudes(small,mediumandlarge)are
defined by the training set size of three widely used datasets we
2https://github.com/c2nes/javalang
1599
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:30:33 UTC from IEEE Xplore.  Restrictions apply. ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA Shi, et al.
investigated in this paper. Specifically, small: the training size of
TLC,medium:thetrainingsizeofCSN,large:thetrainingsizeof
FCM.Thesedatasetsaremainlydifferentfromeachotherincorpus
sizes, data splitting ways, and duplication ratios. Their detailed
descriptions can be found in Section 3.4.
3.2 Evaluated Approaches
We choose the five approaches with the consideration of represen-
tativeness and diversity.
CodeNN[29] is the first neural approach that learns to gener-
ate summaries of code snippets. It is a classical encoder-decoder
framework in NMT that encodes code to context vectors and then
generates summaries in the decoder with the attention mechanism.
Deepcom [25]isanSBT-based(Structure-basedTraversal)model,
which can capture the syntactic and structural information from
AST.ItisanattentionalLSTM-basedencoder-decoderneuralnet-
work that encodes the SBT sequence and generates summaries.
Astattgru [34]isamulti-encoderneuralmodelthatencodesboth
code and AST to learn lexical and syntactic information of Java
methods. It uses two GRUs to encode code and SBT sequences,
respectively.
NCS[1] is the first attempt to replace the previous RNN units
with the more advanced Transformer model, and it incorporates
the copying mechanism [ 53] in the Transformer to allow both
generating words from vocabulary and copying from the input
source code.
Rencos[69]isarepresentativemodelthatcombinesinformation
retrieval techniques with the generation model in the code summa-
rization task. Specifically, it enhances the neural model with the
most similar code snippets retrieved from the training set.
3.3 Experimental Settings
We use the default hyper-parameter settings provided by each
methodandadjusttheembeddingsize,hiddensize,learningrate,
andmaxepochempiricallytoensurethateachmodelperformswell
oneachdataset.Weadoptmaxepoch200forTLCandTLC Dedup
(others are 40) and early stopping with patience 20 to enable the
convergenceandgeneralization.Inaddition,weruneachexperi-
ment3timesanddisplaythemeanandstandarddeviationinthe
formofğ‘šğ‘’ğ‘ğ‘›Â±ğ‘ ğ‘¡ğ‘‘.Allexperimentsareconductedonamachine
with 252 GB main memory and 4 Tesla V100 32GB GPUs.
We use the provided implementations by each approach: Co-
deNN3, Astattgru4, NCS5and Rencos6. For Deepcom, we re-
implement the method7according to the paper description since it
isnotpubliclyavailable.Wehavecheckedthecorrectnessbyrepro-
ducing the scores in the original paper [ 25] and double confirmed
with the authors of Deepcom.
3.4 Research Questions
Weinvestigatethreeresearchquestionsfromthreeaspects:metrics,
pre-processing operations, and datasets.
3https://github.com/sriniiyer/codenn
4https://bit.ly/2MLSxFg
5https://github.com/wasiahmad/NeuralCodeSum
6https://github.com/zhangj111/rencos
7The code for our re-implementation is included in our toolbox.RQ1:HowdodifferentBLEUvariantsaffecttheevaluation
of code summarization?
There are several metrics commonly used for various NLP tasks
such as machine translation, text summarization, and caption-ing. These metrics include BLEU [
49], Meteor [ 5], Rouge-L [ 36],
Cider [60], etc. In RQ1, we only present BLEU as it is the most
commonlyusedmetricinthecodesummarizationtask.Tostudy
"how do different BLEU variants affect the evaluation of code sum-
marization?"andfind"whichvariantshouldweuseinpractice?",
weconductsomeextensiveexperimentsandthehumanevaluation.
We first trainand test the5 approaches onTLC and TLC Dedup, and
measure their generated summaries using different BLEU variants.
Then we will introduce the differences of the BLEU variants in
detail, and summarize the reasons for the differences from three
aspects:differentcalculationlevels(sentence-levelv.s.corpus-level),differentsmoothingmethodsused,andmanyproblematicsoftware
implementations. Finally, we analyze the impact of each aspect,
conducthumanevaluation,andprovideactionableguidelineson
the use of BLEU, such as how to choose a smoothing method, and
how to report BLEU scores more clearly and comprehensively.
Humanevaluation. TofindwhichBLEUcorrelateswiththehu-
manperceptionthemost,weconductahumanevaluation.First,werandomlysample300(100perdataset)generatedsummariespaired
withoriginal summaries.Then,we invite5annotators withexcel-
lentEnglishabilityandmorethan2yearsofsoftwaredevelopment
experience. Each annotator is asked to assign scores from 0 to 4 to
measurethe semanticsimilarity betweenreference andgenerated
summaries. The detailed meaning of these scores is given in Table
1 of our online Appendix8. To verify the agreement among the
annotators, we calculate the Krippendorffâ€™s alpha [ 23] and Kendall
rankcorrelationcoefficient(Kendallâ€™sTau)[ 31]values.Thevalue
ofKrippendorffâ€™salphais0.93,andthevaluesofpairwiseKendallâ€™s
Tau range from 0.87 to 0.99, which indicates that there is a high
degreeofagreementbetweenthe5annotatorsandthescoresare
reliable. Then, we average scores of 5 annotators as the human
scoreforeachgeneratedsummary.Finally,followingWeiatal.[ 58],
we use Kendallâ€™s rank correlation coefficient ğœ[31] and Spearman
correlationcoefficient ğœŒ[72]tomeasurethecorrelationbetween
the human evaluation and each BLEU variant.
Humanscoreforeachcorpus. Tostudythecorrelationbetween
BLEUvariantsandhumanevaluationatthecorpus-level,weshould
obtain the human score of a corpus. Following [ 41], we average
the human scores over all generated summaries as the final human
scoreforacorpus. Weusebotharithmeticand geometric average
in this paper.
Number of summaries in each corpus. To ensure the general-
ization and reliability of the conclusion, we randomly sample
ğ‘¥summaries from 300 scored samples as a corpus, where ğ‘¥âˆˆ
{1,20,40,60,80,100}, and we repeat this sampling process 5000
times.
RQ2: How do different pre-processing operations affect the
performance of code summarization?
Therearevariouscodepre-processingoperationsusedinrelated
work,suchastokensplitting,lowercase.Westudyrecentpapers
on code summarization since 2010 according to the pre-processing
8https://github.com/DeepSoftwareAnalytics/CodeSumEvaluation/tree/master/Appendix
1600
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:30:33 UTC from IEEE Xplore.  Restrictions apply. On the Evaluation of Neural Code Summarization ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
Table 3: The statistics of the 12 datasets used.
Name#Method#Class #Project Description
Training Validation Test All
TLC 69,708 8,714 8,714 87,136 â€“ 9,732 Original TL-CodeSum [27]
TLCDedup 69,708 8,714 6,449 84,871 â€“ â€“ Deduplicated TL-CodeSum
CSN 454,044 15,299 26,897 496,240 136,495 25,596 Filtered CodeSearchNet [28]
CSNProject-Medium 454,044 15,299 26,897 496,240 136,495 25,596 CSN split by project
CSNClass-Medium 448,368 19,707 28,165 496,240 136,495 25,596 CSN split by class
CSNMethod-Medium 446,607 19,855 29,778 496,240 136,495 25,596 CSN split by method
CSNMethod-Small 69,708 19,855 29,778 119,341 â€“ â€“ Subset of CSN Method-Medium
FCM 1,908,694 104,948 104,777 2,118,419 â€“ 28,790 Filtered Funcom [34]
FCMProject-Large 1,908,694 104,948 104,777 2,118,419 â€“ 28,790 Split FCM by project
FCMMethod-Large 1,908,694 104,948 104,777 2,118,419 â€“ 28,790 Split FCM by method
FCMMethod-Medium 454,044 104,948 104,777 663,769 â€“ â€“ Subset of FCM Method-Large
FCMMethod-Small 69,708 104,948 104,777 279,433 â€“ â€“ Subset of FCM Method-Large
Table 4: Code pre-processing operations used in previous
code summarization work.
Operation Studies Meaning
ğ‘…[25â€“27, 37, 64, 66] Replace string/number with
genericsymbols <STRING> /<NUM>
ğ‘†[1,3,4,6,16,20,22,
26,27,33,34,56,62,
64â€“66, 69]Split tokens using camelCase and
snake_case
ğ¹[6, 22, 33, 34, 65] Filter the punctuations in code
ğ¿[3,6,22,26,27,33,
34, 37, 62, 64â€“66]Lowercase all tokens
Others [11,15,29,45,51,61,
67]No pre-processing, BPE, etc
operations they have used and summarize the result in Table 4.
We select four operations ğ‘…,ğ‘†,ğ¹,ğ¿that are most widely used to in-
vestigate whether different pre-processing operations would affect
performance and find the dominated pre-processing choice.
We define a bit-wise notation ğ‘ƒğ‘…ğ‘†ğ¹ğ¿to denote different pre-
processing combinations. For example, ğ‘ƒ1010meansğ‘…=ğ‘‡ğ‘Ÿğ‘¢ğ‘’,ğ‘†=
ğ¹ğ‘ğ‘™ğ‘ ğ‘’,ğ¹=ğ‘‡ğ‘Ÿğ‘¢ğ‘’, andğ¿=ğ¹ğ‘ğ‘™ğ‘ ğ‘’, which stands for performing ğ‘…,
ğ¹, and preventing ğ‘†,ğ¿. Then, we evaluate different pre-processing
combinations on TLC Dedupdataset in Section 4.2.
RQ3:Howdodifferentcharacteristicsofdatasetsaffectthe
performance?
Many datasets have been used in source code summarization.
We first evaluate the performance of different methods on three
widelyuseddatasets,whicharedifferentinthreeattributes:corpus
sizes,datasplittingmethods,andduplicationratios.Then,westudy
theimpactofthethreeattributeswiththeextendeddatasetsshown
in Table 3. The three attributes we consider are as follows:
Data splitting methods : there are three data splitting ways we
investigate: 1/circlecopyrtbymethod:randomlysplitthedatasetaftershuffling
theallsamples[ 27],2/circlecopyrtbyclass:randomlydividetheclassesintothe
threepartitionssuchthatcodefromthesameclasscanonlyexistin one partition, and 3/circlecopyrtby project: randomly divide the projects
intothethreepartitionssuch thatcodefromthesameprojectcanonly exist in one partition [28, 34].
Corpus sizes
: there are three magnitudes of training set size we
investigate: 1/circlecopyrtsmall: the training size of TLC, 2/circlecopyrtmedium: the
training size of CSN, and 3/circlecopyrtlarge: the training size of FCM.
Duplication ratios : Code duplication is common insoftware de-
velopmentpractice.Thisisoftenbecausedeveloperscopyandpaste
codesnippetsand sourcefilesfromother projects[ 39].According
to a large-scale study [ 44], more than 50% of files were reused
in more than one open-source project. Normally, for evaluatingneural network models, the training set should not contain sam-
plesinthetestset.Thus,ignoringcodeduplicationmayresultin
model performance and generalization ability not being compre-hensively evaluated according to the actual practice. Among thethreedatasetsweexperimentedon,FuncomandCodeSearchNet
contain no duplicates because they have been deduplicated, but we
find the existence of 20% exact code duplication in TL-CodeSum.
Therefore, we conduct experiments on TL-CodeSum with different
duplication ratios to study this effect.
4 EXPERIMENTAL RESULTS
4.1 Analysis of Different Evaluation Metrics.
(RQ1)
Weexperimentonthefiveapproachesandmeasuretheirgenerated
summaries using different BLEU variants. The results are shown in
Table 5. We can find that:
â€¢The scores of different BLEU variants are different for the
samesummary.Forexample,theBLEUscoresofDeepcom
on TLC vary from 12.14 to 40.18. Astattgru is better than
Deepcom in all BLEU variants.
â€¢TherankingofmodelsisnotconsistentusingdifferentBLEU
variants.Forexample,thescoreofAstattgruishigherthan
that of CodeNN in terms of BLEU-FC but lower than that of
CodeNN in other BLEU variants on TLC.
â€¢Under the BLEU-FC measure, many existing models (except
Rencos) have scored lower than 20 on TLC Dedupdataset.
1601
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:30:33 UTC from IEEE Xplore.  Restrictions apply. ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA Shi, et al.
Table 5: Different metric scores in TLC and TLC Dedup. Underlined scoresrefer to the metric used in the corresponding papers.
ModelTLC TLCDedup
BLEU-DM BLEU-FC BLEU-DC BLEU-CN BLEU-NCS BLEU-RC BLEU-DM BLEU-FC BLEU-DC BLEU-CN BLEU-NCS BLEU-RC
ğ‘ ,ğ‘š0ğ‘,ğ‘š0ğ‘ ,ğ‘š4ğ‘ ,ğ‘š2ğ‘ ,ğ‘š ğ‘™ğ‘ ,ğ‘š0ğ‘ ,ğ‘š0ğ‘,ğ‘š0ğ‘ ,ğ‘š4ğ‘ ,ğ‘š2ğ‘ ,ğ‘š ğ‘™ğ‘ ,ğ‘š0
CodeNN 51.98 26.04 36.50 33.07 33.78 26.32 40.95 8.90 20.51 15.64 16.60 7.24
Deepcom 40.18 12.14 24.46 21.18 22.26 13.74 34.81 4.03 15.87 11.26 12.68 3.51
Astattgru 50.87 27.11 35.77 31.98 32.64 25.87 38.41 7.50 18.51 13.35 14.24 5.53
Rencos 58.64 41.01 47.78 46.75 47.17 40.39 45.69 22.98 31.22 29.81 30.37 21.39
NCS 57.08 36.89 45.97 45.19 45.51 38.37 43.91 18.37 29.07 27.99 28.42 18.94
ğ‘ andğ‘represent sentence BLEU and corpus BLEU, respectively. ğ‘šğ‘¥represents different smoothing methods,
ğ‘š0is without smoothing method, and ğ‘šğ‘™means using add-one Laplace smoothing which is similar to ğ‘š2.
According to the interpretations in Table 1, this means that
under this experimental setting, the generated summaries
are not gist-clear and understandable.
Next, we elaborate on the differences among the BLEU variants.
ThemathematicalequationofBLEUisshowninEquation (2),which
combines all n-gram precision scores using the geometric mean.
TheBP(BrevityPenalty)isusedtoscalethescorebecausetheshort
sentencesuchassinglewordoutputscouldpotentiallyhavehigh
precision.
BLEU[49]isfirstlydesignedformeasuringthegeneratedcorpus;
assuch,itrequiresnosmoothing,assomesentenceswouldhave
at least one n-gram match. For sentence-level BLEU, ğ‘4will be
zerowhentheexamplehasnota4-gram,andthusthegeometric
mean will be zero even if ğ‘ğ‘›(ğ‘›<4)is large. For sentence-level
measurement,itusuallycorrelatespoorlywithhumanjudgment.
Therefore, several smoothing methods have been proposed in [ 10].
NLTK9(the Natural Language Toolkit), which is a popular toolkit
with 9.7K stars, implements the corpus-level and sentence-level
measureswithdifferentsmoothingmethodsandarewidelyusedin
evaluatinggeneratedsummaries[ 25â€“27,33,34,57,64,65].However,
there are problematic implementations in different NLTK versions,
leading to some BLEU variants unusable. We further explain these
differences in detail.
4.1.1 Sentence v.s. corpus BLEU. The BLEU score calculated at the
sentence level and corpus level is different, which is mainly caused
by the different calculation strategies for merging all sentences.
Thecorpus-levelBLEUtreatsallsentencesasawhole,wherethe
numerator of ğ‘ğ‘›is the sum of the numerators of all sentencesâ€™
ğ‘ğ‘›,andthedenominatorof ğ‘ğ‘›isthesumofthedenominatorsof
all sentencesâ€™ ğ‘ğ‘›. Then the final BLEU score is calculated by the
geometric mean of ğ‘ğ‘›(ğ‘›=1,2,3,4). Different from corpus-level
BLEU,sentence-levelBLEUiscalculatedbyseparatelycalculating
theBLEUscoresforallsentences,andthenthearithmeticaverageofthemisusedassentence-levelBLEU.Inotherwords,sentence-level
BLEU aggregates the contributions of each sentence equally, while
for corpus-level, the contribution of each sentence is positively
correlated with the length of the sentence. Because of the different
calculation methods, the scores of the two are not comparable. We
thussuggestexplicitlyreportatwhichleveltheBLEUisbeingused.
9https://github.com/nltk/nltk4.1.2 Smoothing methods. Smoothing methods are applied when
deciding how to deal with cases if the number of matched n-grams
is0.SinceBLEUcombinesalln-gramprecisionscores( ğ‘ğ‘›)using
the geometric mean, BLEU will be zero as long as any n-gramprecision is zero. One may add a small number to
ğ‘ğ‘›, however,
it will result in the geometric mean being near zero. Thus, manysmoothing methods are proposed. Chen et al. [
10] summarized
7 smoothing methods. Smoothing methods 1-4 replace 0 with a
smallpositivevalue,whichcanbeaconstantorafunctionofthe
generated sentence length. Smoothing methods 5-7 average the
ğ‘›âˆ’1,ğ‘›,andğ‘›+1â€“grammatchedcountsindifferentwaystoobtain
the n-gram matched count. We plot the curve of ğ‘ğ‘›under different
smoothing methods applied to sentences of varying lengths inFigure 1 (upper). We can find that the values of
ğ‘ğ‘›calculated by
different smoothing methods can vary a lot, especially for short
sentences, which are often seen in code summaries.
4.1.3 Bugs in software packages. We measure the same summaries
generatedbyCodeNNinthreeBLEUvariants(BLEU-DM,BLEU-FC,
andBLEU-DC),whichareallbasedontheNLTKimplementation
(but with different versions). From Table 6, we can observe that
scoresofBLEU-DMandBLEU-DCareverydifferentunderdifferent
NLTK versions (from 3.2.ğ‘¥to3.5.ğ‘¥). This is because the buggy
implementations for method 0and method 4in different versions,
which can cause up to 97% performance difference for the same
metric.
Smoothingmethod 0bug.method 0(meansnosmoothingmethod)
of NLTK 3.2.xonly combines the non-zero precision values of all
n-grams using the geometric mean. For example, BLEU is the geo-
metric mean of ğ‘1,ğ‘2, andğ‘3whenğ‘4=0 andğ‘ğ‘›â‰ 0(ğ‘›=1,2,3).
Smoothing method 4bugs.method 4is implemented problem-
atically in different NLTK versions. We plot the curve of ğ‘ğ‘›of
differentsmoothingmethod 4implementationsinNLTKinFigure1
bottom, where the correct version is NLTK 3.6.x. In NLTK versions
3.2.2 to 3.4.x, ğ‘ğ‘›=1
ğ‘›âˆ’1+ğ¶/ğ‘™ğ‘›(ğ‘™â„), whereğ¶=5, which always in-
flatesthe score indifferentlength(Figure 1).Thecorrectmethod 4
proposedin[ 10]isğ‘ğ‘›=1/(ğ‘–ğ‘›ğ‘£ğ‘ğ‘›ğ‘¡âˆ—ğ¶
ln(ğ‘™â„)âˆ—ğ‘™â„),whereğ¶=5and
ğ‘–ğ‘›ğ‘£ğ‘ğ‘›ğ‘¡=1
2ğ‘˜isageometricsequencestartingfrom1/2ton-grams
with 0 matches. In NLTK 3.5.x,ğ‘ğ‘›=ğ‘›âˆ’1+5/ğ‘™ğ‘›(ğ‘™â„)
ğ‘™â„whereğ‘™â„is the
lengthofthegeneratedsentence,thus ğ‘ğ‘›canbeassignedwitha
percentagenumber thatismuch greaterthan 100%(even >700%)
1602
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:30:33 UTC from IEEE Xplore.  Restrictions apply. On the Evaluation of Neural Code Summarization ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
The length of generated sentenceDifferent Smoothing Method
The length of generated sentenceDifferent Implementation of Method 4
Figure 1: Comparison of different smoothing methods.
Table 6: BLEU scores in different NLTK versions.
MetricNLTK version
3.2.ğ‘¥123.3.ğ‘¥/3.4.ğ‘¥3.5.ğ‘¥3.6.ğ‘¥13
BLEU-DM (ğ‘ ,ğ‘š0)51.98 26.32 26.32 26.32
BLEU-FC (ğ‘,ğ‘š0)26.04 26.04 26.04 26.04
BLEU-DC (ğ‘ ,ğ‘š4)36.50 36.50 42.39 28.35
whenğ‘™â„<5 in n-gram. We have reported this issue10and filed a
pullrequest11toNLTKGitHubrepository,whichhasbeenaccepted
andmergedintotheofficialNLTKlibraryandreleasedinNLTK 3.6.x
(the revision is shown in Figure 2). Therefore, NLTK 3.6.xshould be
used when using smoothing method 4.
Fromtheaboveexperiments,wecanconcludethatBLEUvariants
used in prior work on code summarization are different from each
otherandthedifferencescancarrysomeriskssuchasthevalidityof
theirclaimedresults.Thus,itisunfairandriskytocomparedifferentmodelswithoutusingthesameBLEUimplementation.Forinstance,itisunacceptablethatresearchersignorethedifferencesamongthe
BLEU variants and directly compare their results with the BLEU
scoresreportedinotherpapers.Weusethecorrectimplementation
to calculate BLEU scores in the following experiments.
10https://github.com/nltk/nltk/issues/2676
11https://github.com/nltk/nltk/pull/2681
12Except for versions 3.2 and 3.2.1, as these versions are buggy with the
ZeroDivisionError exception. Please refer to https://github.com/nltk/nltk/issues/
1458 for more details.
13NLTK3.6.xare the versions with the BLEU calculation bug fixed by us.nltk/translate/bleu_score.py
571571def method4(self, p_n, references, hypothesis, 
hyp_len=None, *args, **kwargs):
â€¦...
579+    incvnt = 1
579580hyp_len = hyp_len if hyp_len else len(hypothesis)
580581 fori, p_i in enumerate (p_n):
581- if p_i.numerator == 0 and hyp_len != 0:
582- incvnt = i + 1 * self.k / math. log(
583- hyp_len
584- ) # Note that this K is ...
585- p_n[i] = incvnt /p_i.denominator
582+ if p_i.numerator == 0 and hyp_len >1:
583+ #            incvnt = i + 1 * self.k / math.log(
584+ #               hyp _len
585+ #            ) # Note that this K is ...
586+ #            p_n[i]  = incvnt / p_i.denominator\
587+            numerator = 1 / (2 ** incvnt * self.k
/ math.log(hyp_len))
588+            p _n[i] = numerator / p_i.denominator
589+            incvnt += 1
586590 returnp_n
Figure 2: Issue 267610about smoothing method 4in NLTK,
which is reported and fixed by us.
Table 7: The values of correlation coefficients. ğœŒis Spear-
man correlation coefficient and ğœis Kendall rank correla-
tion coefficient. Here we use arithmetic average to aggregate
summary-level human score as the corpus-level score. All
results are statistically significant ( ğ‘/lessmuch0.05).
Metric12 04 06 08 01 0 0
ğœğœŒğœğœŒğœğœŒğœğœŒğœğœŒğœğœŒ
BLEU-DM ğ‘ ,ğ‘š00.32 0.68 0.61 0.80 0.62 0.81 0.62 0.81 0.62 0.82 0.61 0.8
BLEU-FC ğ‘,ğ‘š00.32 0.68 0.41 0.58 0.39 0.56 0.38 0.55 0.38 0.55 0.37 0.54
BLEU-DC ğ‘ ,ğ‘š40.54 0.75 0.65 0.84 0.66 0.85 0.66 0.85 0.66 0.85 0.65 0.84
BLEU-CN ğ‘ ,ğ‘š20.47 0.66 0.60 0.79 0.61 0.81 0.62 0.81 0.62 0.81 0.61 0.81
BLEU-NCS ğ‘ ,ğ‘š ğ‘™0.37 0.53 0.57 0.76 0.58 0.78 0.59 0.78 0.59 0.79 0.58 0.78
BLEU-RC ğ‘ ,ğ‘š00.32 0.68 0.61 0.80 0.62 0.81 0.62 0.81 0.62 0.82 0.61 0.8
4.1.4 Human evaluation. To answer the question â€œwhich BLEU
correlateswithhumanperceptionthemostâ€,weconductthehumanevaluation.Table7showsthevaluesofcorrelationcoefficientunder
different corpus sizes when using arithmetic average14to aggre-
gate summary-level human scores as the corpus-level score. Table
7 shows that, in terms of correlation coefficient ğœ,1/circlecopyrtwhen the cor-
pussizeis1(one-sentencelevel),BLEUmetricswithoutsmoothingmethod(BLEU-DM,BLEU-FC,andBLEU-RC)correlatepoorlywith
human perception, and smoothing methods improve the correla-
tionovernosmoothing.Bothfindingsareconsistentwithprevious
studies[10,52].2/circlecopyrtBLEU-DC,BLEU-CN,andBLEU-NCSarecom-
parable and always have higher correlation coefficients than otherBLEU variants. Among them, the BLEU-DC performs significantly
better, which indicates that sentence-level BLEU with method 4
14We also conduct another experiment that uses a geometric average to aggregate
summary-levelhumanscoresasthecorpus-levelscore.Astheconclusionisconsistent
with the arithmetic average experiment, we put the results in the online Appendix
Table 2 to save space.
1603
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:30:33 UTC from IEEE Xplore.  Restrictions apply. ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA Shi, et al.
Table8:Theresultsoffourcodepre-processingoperations.
1 and 0 denotes use and non-use of a certain operation, re-
spectively. Stars * mean statistically significant.
Model ğ‘…0ğ‘…1ğ‘†0ğ‘†1ğ¹0ğ¹1ğ¿0ğ¿1
CodeNN 7.19 7.18 7.18 7.19 7.18 7.19 7.19 7.18
Astattgru 5.91 5.97 5.63 6.26 5.85 6.03 5.81 6.07
Rencos 21.85 21.55 20.91 22.5 21.79 21.62 21.43 21.98
NCS 12.20 12.08 11.65 12.63 12.04 12.24 11.82 12.45
Avg. 11.79 11.70 11.3412.15*11.72 11.77 11.56 11.92
is more relevant to human perception. This is because method 4
smoothszerovalueswithoutinflatingtheprecisioncomparedto
method 2and method 3(top of Figure 1).
Summary. Thedifferences amongtheBLEUvariants couldaf-
fect the validity of the experiment and conclusion. (1) The BLEU
measure should be implemented correctly and described pre-
cisely, including the calculation level (sentence or corpus) and
thesmoothingmethodbeingused.(2)Thecomparisonofmod-
els should be under the same BLEU metric. (3) BLEU-DC, thesentence-levelBLEUwithmethod
4,ismorerelevanttohuman
perception.
4.2 The Impact of Different Pre-processing
Operations (RQ2)
In order to evaluate the individual effect of four different codepre-processing operations and the effect of their combinations,
we train and test the four models (CodeNN, Astattgru, Rencos,
and NCS) under 16 different code pre-processing combinations.NotethatthemodelDeepcomisnotexperimentedasitdoesnotuse source code directly. In the following experiments, we have
performed calculations onall metrics. Due to spacelimitation, we
present the scores under BLEU-DC, which correlates more with
human perception.All findings inthe following sectionsstill hold
forothermetrics,andtheomittedresultscanbefoundintheonline
Appendix.
As shown in Table 8, for all models, performing ğ‘†(identifier
splitting)isalwaysbetterthannotperformingit,whileitisunclear
whether toperform the otherthree operations. Then,we conduct
thetwo-sided t-test[13]andWilcoxon-Mann-Whitneytest [42]to
statisticallyevaluatethedifferencebetweenusingordroppingeach
operation.Thesignificancesigns(*)labelledinTable8meanthat
thep-valuesofthestatisticaltestsat95%confidence levelareless
than0.05.Theresultsconfirmthattheimprovementachievedby
performing ğ‘†is statistically significant, while performing the other
threeoperationsdoesnotleadtostatisticallydifferentresults15.A s
pointedoutin[ 30],theOOV(outofvocabulary)ratioisreduced
aftersplittingcompoundwords,andusingsubtokensallowsamodeltosuggestneologisms,whichareunseeninthetrainingdata.Manystudies[
3,7,18,40,43]haveshownthattheperformanceofneural
languagemodelscanbeimprovedafterhandlingtheOOVproblem.
15The detailed statistical test scores can be found in the online Appendix Tables 11 to
19.Similarly, the performance of code summarization is also improved
after performing ğ‘†.
Next,weevaluatetheeffectofdifferentcombinationsofopera-
tions and show the result in Table 9. For each model, we mark the
bottom 5 in underline, the top 5 in bold. We can find that:
â€¢Different pre-processing operations can affect the overall
performance by a noticeable margin.
â€¢ğ‘ƒ1101isarecommendedcodepre-processingmethod,asitis
top 5 for all approaches. ğ‘ƒ0000is the not-recommended code
pre-processing method, as it is bottom 5 for all approaches.
â€¢Theranking ofperformance fordifferentmodels aregener-
ally consistent under different code pre-processing settings.
An exploration experiment From Table 9, we can see that
there is no dominated pre-processing combination across these
approaches. We conducta simple exploratoryexperiment thatag-
gregatesfourdifferentpre-processing: ğ‘ƒ1101,ğ‘ƒ0101,ğ‘ƒ0110,andğ‘ƒ0111,
which mostly perform better than other combinations on the four
approaches.Weusethestacking-basedtechnique[ 32](theonline
AppendixFigure1)toaggregatethecomponentmodels.Indetail,
ensemble components have the same network structure but the
inputdataisprocessedbydifferentpre-processingcombinations.
TheresultisshowninthelastcolumnofTable9.Wecanseethat
in general, the ensemble model performs better than the single
models, indicating that different pre-processing combinations may
contain complementary information that can improve the final
output through ensemble learning.
Summary. Code pre-processing has a large impact on perfor-
mance(-18%to+25%).And,thereisnodominatedpre-processing
combination for different approaches. In addition, a simple en-
semble modelon thedifferent pre-processingcan boost theper-
formance of the model. We share the implementations of 4 code
pre-processing operations and 16 combinations for the conve-
nience of follow-up research.
4.3 How Do Different Characteristics of
Datasets Affect the Performance?(RQ3)
To answer RQ3, we evaluate the five approaches on the three base
datasets: TLC, CSN, and FCM. From Table 10, we can find that:
â€¢The performance of the same model is different on different
datasets.
â€¢The ranking among the approaches does not preserve when
evaluating them on different datasets. For instance, Rencos
outperforms other approaches in TLC but is worse than
AstattgruandNCSinFCM.CodeNNperformsbetterthan
AstattgruonTLC,butAstattgruoutperformsCodeNNinthe
other two datasets.
â€¢TheaverageperformanceofallmodelsonTLCisbetterthantheothertwodatasets,althoughTLCismuchsmaller(about
96% less than FCM and 84% less than CSN).
â€¢The average performance of FCM is better than that of CSN.
Summary. To more comprehensively evaluate different models,
itisrecommendedtousemultipledatasets,astherankingamong
models can be inconsistent on different datasets.
1604
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:30:33 UTC from IEEE Xplore.  Restrictions apply. On the Evaluation of Neural Code Summarization ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
Table9:Performanceofdifferentcodepre-processingcombinations.Bottom5inunderline,top5inbold,andensemblemodels
in bold and with gray background.
Model ğ‘ƒ0000 ğ‘ƒ0001ğ‘ƒ0010ğ‘ƒ0011ğ‘ƒ0100ğ‘ƒ0101ğ‘ƒ0110ğ‘ƒ0111ğ‘ƒ1000ğ‘ƒ1001ğ‘ƒ1010ğ‘ƒ1011ğ‘ƒ1100 ğ‘ƒ1101 ğ‘ƒ1110ğ‘ƒ1111Ensemble
CodeNN 7.06 (6.37%â†“)7.10 6.98 7.25 7.54 7.017.437.06 7.22 7.19 7.24 7.407.067.34(5.16%â†‘)7.027.0510.64
Astattgru 5.67 (14.99%â†“)5.655.445.486.17 6.67 6.28 6.41 5.84 5.83 5.30 5.81 5.79 6.62(24.91%â†‘)6.03 6.09 11.28
Rencos 20.21 (16.52%â†“)20.3521.28 21.01 21.5223.37 22.25 22.45 20.9120.9621.20 21.33 21.42 24.21(19.79%â†‘)22.6222.1524.21
NCS 11.22 (17.92%â†“)11.95 11.12 12.07 12.06 13.3012.1212.8211.87 11.51 11.7811.6412.34 13.67 (22.93%â†‘)12.0912.67 19.90
Table 10: Performance in different datasets. Statistically sig-
nificant ( ğ‘/lessmuch0.05) results are marked with star *.
ModelDataset
TLC FCM CSN
CodeNN 28.24Â±0.19 12.64Â±0.13 3.32Â±0.09
Deepcom 15.65 Â±2.12 9.12 Â±0.03 1.98Â±0.30
Astattgru 25.90Â±0.79 15.58Â±0.11 5.01Â±0.27
Rencos 42.46Â±0.05* 15.47Â±0.00 6.65Â±0.05
NCS 39.50Â±0.23 18.07Â±0.46* 6.66Â±0.51
Avg 30.35Â±9.70 14.17Â±3.05 4.72Â±1.85
Since there are many factors that make the three datasets differ-
ent, in order to further explore the reasons for the above results
in-depth,weusethecontrolledvariablemethodtostudyfromthree
aspects: corpus sizes, data splitting ways, and duplication ratios.
4.3.1The impact of different corpus sizes .We evaluate all mod-
els on two groups (one group contains CSN Method-Medium
and CSN Method-Small , the other group contains FCM Method-Large ,
FCMMethod-Medium andFCM Method-Small ).Withineachgroup,the
test sets are the same, the only difference is in the corpus size.
The results are shown in Table 11. We can find that the ranking
between models can be generally preserved on different corpus
sizes.Also,asthesizeofthetrainingsetbecomeslarger,theper-
formanceofthefiveapproachesimprovesinbothgroups,which
is consistent with the findings of previous work [ 4]. We can also
find that, compared to other models, the performance of Deepcom
does not improve significantly when the size of the training set
increases. We suspect that this is due to the high OOV ratio, which
affectsthescalabilityoftheDeepcommodel[ 24,30],asshownin
the bottom of Table 11. Deepcom uses only SBT and represents an
ASTnodeasaconcatenationofthetypeandvalueoftheASTnode,
resulting in a sparse vocabulary. Therefore, even if the training set
becomes larger, the OOV ratio is still high. Therefore, Deepcom
could not fully leverage the larger datasets.
Summary. If additional data is available, one can enhance the
performance of models by training with more data since theperformance improves as the size of the training set becomes
larger. The ranking among models can be generally preserved on
different corpus sizes.
4.3.2The impact of different data splitting methods .In this exper-
iment,weevaluatethefiveapproachesontwogroups(onegroupcontains FCM Project-Large and FCM Method-Large and another con-
tainsCSN Project-Medium ,CSNClass-Medium ,CSNMethod-Medium ).Each
group only differs in data splitting ways. From Table 12, we canobserve that all approaches perform differently in different data
splittingways,andtheyallperformbetteronthedatasetsplitby
method than by project. This is because similar tokens and code
patternsareusedinthemethodsfromthesameproject[ 35,38,48].
Inaddition,whenthedatasplittingwaysaredifferent,therankings
between various approaches remain basically unchanged, which
indicatesthatitwouldnotimpactcomparisonfairnessacrossdiffer-
entapproacheswhetherornottoconsidermultipledatasplitting
ways.
Summary. Different data splitting methods can significantly
affecttheperformanceofallmodels.However,therankingofthemodelremainsbasicallyunchanged.Therefore,ifdataavailability
ortimeislimited,itisalsoreliabletoevaluatetheperformance
of different models under only one data splitting method.
4.3.3The impact of different duplication ratios .To simulate sce-
narioswithdifferentcodeduplicationratios,weconstructsynthetic
test sets from TLC Dedupby adding random samples from the train-
ing set to the test set. Then, we train the five models using the
sametrainingsetandtestthemusingthesynthetictestsetswith
different duplication ratios (i.e., the test sets with random samples).
From the results shown in Figure 3, we can find that:
â€¢TheBLEUscoresofallapproachesincreaseastheduplication
ratio increases.
â€¢The score of the model Rencos increases significantly when
the duplication ratio increases. We speculate that the reason
should be the duplicated samples being retrieved back bythe retrieval module in Rencos. Therefore, retrieval-based
models could benefit more from code duplication.
â€¢In addition, the ranking of the models is not preserved with
different duplication ratios. For instance, CodeNN outper-forms Astattgru without duplication and is no better than
Astattgru on other duplication ratios.
Summary. To evaluate theperformance of neural code summa-
rizationmodels,itisrecommendedtousededuplicateddatasetssothatthegeneralizationabilityofthemodelcanbetested.However,
in real scenarios, duplications are natural. Therefore, we suggest
evaluatingmodelsunderdifferentduplicationratios.Moreover,
it is recommended to consider incorporating retrieval techniques
toimprovetheperformanceespeciallywhencodeduplications
exist.
1605
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:30:33 UTC from IEEE Xplore.  Restrictions apply. ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA Shi, et al.
Table 11: The results of different corpus sizes. Statistically significant ( ğ‘/lessmuch0.05) results are marked with star *.
Model FCM Method-Small FCMMethod-Medium FCMMethod-Large CSNMethod-Small CSNMethod-Medium
CodeNN 10.37Â±0.17 14.76Â±0.17 18.68Â±0.26 5.20Â±0.01 12.71Â±0.23
Deepcom 8.99Â±0.06 10.87Â±0.20 11.65Â±0.36 7.57Â±0.74 7.85Â±1.07
Astattgru 12.86Â±0.64 18.15Â±0.05 21.73Â±0.11 5.89Â±0.12 15.83Â±0.17
Rencos 14.24Â±0.12 21.97Â±0.08 23.81Â±0.04 7.36Â±0.08 19.56Â±0.03
NCS 14.70Â±0.19 23.10Â±0.32* 29.03Â±0.32* 9.07Â±0.20* 25.17Â±0.39*
OOV Ratio of Deepcom 91.90% 88.94% 88.32% 91.49% 85.81%
OOV Ratio of Others 63.36% 53.09% 48.60% 60.99% 34.00%
Table 12: The results in different data splitting methods. Statistically significant ( ğ‘/lessmuch0.05) results are marked with star *.
Model CSNProject-Medium CSNClass-Medium CSNMethod-Medium FCMProject-Large FCMMethod-Large
CodeNN 3.32Â±0.09 9.57Â±0.15 12.71Â±0.23 12.64Â±0.13 18.68Â±0.26
Deepcom 1.98Â±0.30 6.14Â±0.12 7.85Â±1.07 9.12Â±0.03 11.65Â±0.36
Astattgru 6.86Â±3.07 11.72 Â±0.41 15.83Â±0.17 15.58Â±0.11 21.73Â±0.11
Rencos 6.65Â±0.05 14.37Â±0.03 19.56Â±0.03 15.47Â±0.00 23.81Â±0.04
NCS 6.66Â±0.51 17.96Â±0.23* 25.17Â±0.39* 18.07Â±0.46* 29.03Â±0.32*
OOV Ratio 48.74% 35.38% 34.00% 57.56% 48.60%
Figure 3: The results of different duplication ratios.
Weobservethatevenwhenwecontrolallthreefactors(splitting
methods, duplication ratios, and dataset sizes), the performance
of the same model still varies greatly between different datasets16.
This indicates that the differences in training data may also be a
factorthataffectstheperformanceofcodesummarization.Weleave
it to future work to study the impact of data differences.
5 THREATS TO VALIDITY
We have identified the following main threats to validity:
Programming languages. We only conduct experiments on
Javadatasets.Althoughinprinciple,themodelsandexperiments
are not specifically designed for Java, more evaluations are needed
whengeneralizingourfindingstootherlanguages.Inthefuture,
we will extend our study to other programming languages.
The quality of summaries. The summariesin alldatasets are
collected by extracting the first sentences of Javadoc. Although
this is a common practice to place a methodâ€™s summary at the first
16The results are given in the online Appendix Tables 61 to 69 due to space limitation.sentence according to the Javadoc guidelines17, there might still be
some incomplete or mismatched summaries in the datasets.
Models evaluated. Wecoveredallrepresentativemodelswith
differentcharacteristics,suchasTransformer-basedandRNN-based
models,single-channelandmulti-channelmodels,modelswithand
withoutretrievaltechniques. However,othermodelsthatweare
out of our study may still cause our findings to be untenable.
Human evaluation. Weusetwodifferentways(arithmeticand
geometricaverage)toaggregatethesentence-levelhumanscoresasacorpus-levelhumanscore.Theaggregationmethodmaythreaten
ourconclusion.Wewillexploreotherwaystoassesscorpus-level
quality in human evaluation.
6 RELATED WORK
Code summarization plays an important role in comprehension,
reusingandmaintenanceofprogram.Somesurveys[ 46,55,71]pro-
videdataxonomyofcodesummarizationmethodsanddiscussed
theadvantages,limitations,andchallengesofexistingmodelsfroma high-level perspective. Especially, Song et al. [
55] also provided a
discussionoftheevaluationtechniquesbeingusedinexistingmeth-
ods. Gros et al. [ 19] described an analysis of several machine learn-
ingapproachesoriginallydesignedforthetaskofnaturallanguage
translation for the code summarization task. They also observedthat different datasets were used in existing work and different
metricswereusedtoevaluatedifferentapproaches.Allamaniset
al.[2]exploredtheeffectofcodeduplicationandconcludedthat
theperformanceofthetechniqueissometimesoverestimatedwhenevaluatedontheduplicateddataset.LeClairetal.[
35]conductedthe
experimentofastandardNMTalgorithmfromtwoaspects:split-
ting strategies (splitting the dataset by project or by method) and a
cleanapproach,andproposedtheguidelinesforbuildingdatasets
based on experiment results. Some studies [ 52,57] conducted a
17http://www.oracle.com/technetwork/articles/java/index-137868.html
1606
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:30:33 UTC from IEEE Xplore.  Restrictions apply. On the Evaluation of Neural Code Summarization ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
human study and concluded that BLEU is not correlated to human
quality assessments when measuring one generated summary. Roy
et al. [52] also re-assessed and interpreted other automatic metrics
forcodesummarization.Ourworkdiffersfrompreviousworkin
that we not only observe the inconsistent usage of different BLEU
metrics but also conduct dozens of experiments on the five models
and explicitly confirm that the inconsistent usage can cause severe
problemsin evaluating/comparingmodels.Besides,we performa
human evaluation to provide additional findings, e.g., which BLEU
metricscorrelatewithhumanperceptionthemost.Moreover,we
explore factors affecting model evaluation, which have not been
systematically studied before, such as dataset size, dataset splitmethods,codepre-processingoperations,etc.Differentfromthe
surveys, we provide extensive experiments on various datasets for
various findings and corresponding discussions. Finally, we consol-
idate all findings and propose actionable guidelines for evaluating
code summarization models.
7 CONCLUSION
Inthispaper,weconductanin-depthanalysisofrecentneuralcode
summarization models. We have investigated several aspects ofmodel evaluation: evaluation metrics, code pre-processing oper-ations, and datasets. Our results point out that all these aspectshave large impact on evaluation results. Without a carefully andsystematically designed experiment, neural code summarizationmodels cannot be fairly evaluated and compared. Our work also
suggests some actionable guidelines including: (1) Reporting BLEU
metricsexplicitly(includingsentenceorcorpuslevel,smoothing
method,NLTKversion,etc).BLEU-DC,whichcorrelatesmorewith
human perception, can be selected as the evaluation metric. (2)
Usingproper(andmaybemultiple)codepre-processingoperations.
(3) Considering the dataset characteristics when evaluating and
choosing the best model. We build a shared code summarization
toolbox containing the implementation of BLEU variants, code pre-
processingoperations,datasets,theimplementationofbaselines,
and all experimental results. We believe the results and findings
weobtainedcan beofgreathelp forpractitionersandresearchers
working on this interesting area.
For future work, we will extend our study to programming lan-
guages other than Java. We will design an automatic evaluation
metric which is more correlated to human perception. We will also
exploremoreattributesofdatasets.Furthermore,weplantoextend
the study to other text generation tasks in software engineering
such as commit message generation.
Tofacilitatereproducibility,ourcodeanddataareavailableat
https://github.com/DeepSoftwareAnalytics/CodeSumEvaluation.
8 ACKNOWLEDGEMENT
Wethankreviewersfortheirvaluablecommentsonthiswork.This
researchwas supportedby NationalKeyR&DProgramofChina
(No.2017YFA0700800). We would like to thank Jiaqi Guo for his
valuable suggestions and feedback. We also thank the participants
of our human evaluation for their time.
REFERENCES
[1]Wasi Uddin Ahmad, Saikat Chakraborty, Baishakhi Ray, and Kai-Wei Chang.
2020. ATransformer-basedApproachforSourceCodeSummarization.In ACL.Association for Computational Linguistics, 4998â€“5007.
[2]Miltiadis Allamanis. 2019. The adverse effects of code duplication in machine
learning models of code. In Onward!ACM, 143â€“153.
[3]Miltiadis Allamanis, Hao Peng, and Charles Sutton. 2016. A Convolutional
Attention Network for Extreme Summarization of Source Code. In ICML (JMLR
Workshop and Conference Proceedings, Vol. 48). JMLR.org, 2091â€“2100.
[4]Uri Alon, Shaked Brody, Omer Levy, and Eran Yahav. 2019. code2seq: Gen-
erating Sequences from Structured Representations of Code. In ICLR (Poster).
OpenReview.net.
[5]Satanjeev Banerjee and Alon Lavie. 2005. METEOR: An Automatic Metric for
MT Evaluation with Improved Correlation with Human Judgments. In IEEvalua-
tion@ACL.
[6]Aakash Bansal, Sakib Haque, and Collin McMillan. 2021. Project-Level Encoding
for Neural Source Code Summarization of Subroutines. In ICPC.
[7]IssamBazzi.2002. Modellingout-of-vocabularywordsforrobustspeechrecognition.
Ph.D. Dissertation. Massachusetts Institute of Technology.
[8]Lionel C. Briand. 2003. Software Documentation: How Much Is Enough?. In
CSMR. IEEE Computer Society, 13.
[9]RuichuCai,ZhihaoLiang,BoyanXu,ZijianLi,YuexingHao,andYaoChen.2020.
TAG: Type Auxiliary Guiding for Code Comment Generation. In ACL.
[10]Boxing Chen and Colin Cherry. 2014. A Systematic Comparison of Smooth-ing Techniques for Sentence-Level BLEU. In WMT@ACL. The Association for
Computer Linguistics, 362â€“367.
[11]Qingying Chenand Minghui Zhou.2018. Aneural frameworkfor retrieval and
summarization of source code. In ASE. ACM, 826â€“831.
[12]Google Cloud. 2007. AutoML: Evaluating models. https://cloud.google.com/
translate/automl/docs/evaluate#bleu
[13]Shirley Dowdy, Stanley Wearden, and Daniel Chilko. 2011. Statistics for research.
Vol. 512. John Wiley & Sons.
[14]Brian P. Eddy, Jeffrey A. Robinson, Nicholas A. Kraft, and Jeffrey C. Carver. 2013.
Evaluatingsourcecodesummarizationtechniques:Replicationandexpansion.
InICPC. IEEE Computer Society, 13â€“22.
[15]Zhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xiaocheng Feng, Ming Gong,
LinjunShou,BingQin,TingLiu,DaxinJiang,andMingZhou.2020. CodeBERT:APre-TrainedModelforProgrammingandNaturalLanguages.In EMNLP(Findings)
(FindingsofACL,Vol.EMNLP2020).AssociationforComputationalLinguistics,
1536â€“1547.
[16]PatrickFernandes,MiltiadisAllamanis,andMarcBrockschmidt.2019. Structured
Neural Summarization. In ICLR.
[17]Andrew Forward and Timothy Lethbridge. 2002. The relevance of software
documentation,toolsandtechnologies:asurvey.In ACMSymposiumonDocument
Engineering. ACM, 26â€“33.
[18]Edouard Grave, Armand Joulin, and Nicolas Usunier. 2017. Improving Neural
Language Models with a Continuous Cache. In ICLR (Poster). OpenReview.net.
[19]David Gros, Hariharan Sezhiyan, Prem Devanbu, and Zhou Yu. 2020. Code to
Comment"Translation":Data,Metrics,Baselining&Evaluation.In ASE.IEEE,
746â€“757.
[20]Sonia Haiduc, Jairo Aponte, and Andrian Marcus. 2010. Supporting program
comprehensionwith sourcecode summarization.In ICSE,Vol.2. ACM,223â€“226.
[21]SoniaHaiduc,JairoAponte,LauraMoreno,andAndrianMarcus.2010. Onthe
UseofAutomatedTextSummarizationTechniquesforSummarizingSourceCode.
InWCRE. IEEE Computer Society, 35â€“44.
[22]SakibHaque,AlexanderLeClair,LingfeiWu,andCollinMcMillan.2020.Improved
automatic summarization of subroutines via attention to file context. In MSR.
[23]AndrewFHayesandKlausKrippendorff.2007. Answeringthecallforastandard
reliabilitymeasureforcodingdata. Communicationmethodsandmeasures 1,1
(2007), 77â€“89.
[24]Vincent J. Hellendoorn and Premkumar T. Devanbu. 2017. Are deep neural
networksthebestchoiceformodelingsourcecode?.In ESEC/SIGSOFTFSE.ACM,
763â€“773.
[25]Xing Hu, Ge Li, Xin Xia, David Lo, and Zhi Jin. 2018. Deep code comment
generation. In ICPC. ACM, 200â€“210.
[26]Xing Hu, Ge Li, Xin Xia, David Lo, and Zhi Jin. 2020. Deep code comment
generationwithhybridlexicalandsyntacticalinformation. Empir.Softw.Eng. 25,
3 (2020), 2179â€“2217.
[27]Xing Hu, Ge Li, Xin Xia, David Lo, Shuai Lu, and Zhi Jin. 2018. Summarizing
Source Code with Transferred API Knowledge. In IJCAI. ijcai.org, 2269â€“2275.
[28]Hamel Husain, Ho-Hsiang Wu, Tiferet Gazit, Miltiadis Allamanis, and Marc
Brockschmidt.2019. CodeSearchNetChallenge:EvaluatingtheStateofSemantic
Code Search. arXiv Preprint (2019). https://arxiv.org/abs/1909.09436
[29]Srinivasan Iyer, Ioannis Konstas, Alvin Cheung, and Luke Zettlemoyer. 2016.Summarizing Source Code using a Neural Attention Model. In ACL (1). The
Association for Computer Linguistics.
[30]Rafael-Michael Karampatsis, Hlib Babii, Romain Robbes, Charles Sutton, and
Andrea Janes. 2020. Big code != big vocabulary: open-vocabulary models for
sfBLEUzource code. In ICSE. ACM, 1073â€“1085.
[31]Maurice G Kendall. 1945. The treatment of ties in ranking problems. Biometrika
33, 3 (1945), 239â€“251.
1607
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:30:33 UTC from IEEE Xplore.  Restrictions apply. ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA Shi, et al.
[32]Alexander LeClair, Aakash Bansal, and Collin McMillan. 2021. Ensemble Models
for Neural Source Code Summarization of Subroutines. CoRRabs/2107.11423
(2021).
[33]AlexanderLeClair,SakibHaque,LingfeiWu,andCollinMcMillan.2020.Improved
Code Summarization via a Graph Neural Network. In ICPC. ACM, 184â€“195.
[34]Alexander LeClair, Siyuan Jiang, and Collin McMillan. 2019. A neural model for
generating natural language summaries of program subroutines. In ICSE. IEEE /
ACM, 795â€“806.
[35]AlexanderLeClairandCollinMcMillan.2019. Recommendationsfordatasetsfor
source code summarization. In NAACL.
[36]Chin-YewLin.2004. ROUGE:APackageforAutomaticEvaluationofSummaries.
InACL.
[37]ChenLin,ZhichaoOuyang,JunqingZhuang,JianqiangChen,HuiLi,andRongxin
Wu.2021. ImprovingCodeSummarizationwithBlock-wiseAbstractSyntaxTree
Splitting. In ICPC.
[38]Shangqing Liu, Cuiyun Gao, Sen Chen, Lun Yiu Nie, and Yang Liu. 2019. ATOM:
CommitMessageGenerationBasedonAbstractSyntaxTreeandHybridRanking.
arXiv(2019).
[39]Cristina VLopes, Petr Maj,Pedro Martins, Vaibhav Saini,Di Yang, Jakub Zitny,
HiteshSajnani,andJanVitek.2017. DÃ©jÃ Vu:amapofcodeduplicatesonGitHub.
InOOPSLA.
[40]Thang Luong, Richard Socher, and Christopher D. Manning. 2013. Better Word
Representations with Recursive Neural Networks for Morphology. In CoNLL.
ACL, 104â€“113.
[41]QingsongMa,JohnnyWei,OndrejBojar,andYvetteGraham.2019. Resultsof
theWMT19MetricsSharedTask:Segment-LevelandStrongMTSystemsPose
Big Challenges.In WMT (2). Associationfor ComputationalLinguistics, 62â€“90.
[42]H. B. Mann and D. R. Whitney. 1947. On a Test of Whether one of Two Random
Variables is Stochastically Larger than the Other. The Annals of Mathematical
Statistics 18, 1 (1947), 50 â€“ 60. https://doi.org/10.1214/aoms/1177730491
[43]Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. 2017.
Pointer Sentinel Mixture Models. In ICLR (Poster). OpenReview.net.
[44]AudrisMockus.2007. Large-scalecodereuseinopensourcesoftware.In First
International Workshop on Emerging Trends in FLOSS Research and Development
(FLOSSâ€™07: ICSE Workshops 2007). IEEE, 7â€“7.
[45]LauraMoreno,JairoAponte,GiriprasadSridhara,AndrianMarcus,LoriL.Pollock,
andK.Vijay-Shanker.2013. Automaticgenerationofnaturallanguagesummaries
for Java classes. In ICPC. IEEE Computer Society, 23â€“32.
[46]Najam Nazar, Yan Hu, and He Jiang. 2016. Summarizing software artifacts:
A literature review. Journal of Computer Science and Technology 31, 5 (2016),
883â€“909.
[47]SheenaPanthaplackel,JunyiJessyLi,MilosGligoric,andRaymondJ.Mooney.
2021. DeepJust-In-TimeInconsistencyDetectionBetweenCommentsandSource
Code. InAAAI. AAAI Press, 427â€“435.
[48]SheenaPanthaplackel,PengyuNie,MilosGligoric,JunyiJessyLi,andRaymondJ.
Mooney. 2020. Learning to Update Natural Language Comments Based on Code
Changes. In ACL. Association for Computational Linguistics, 1853â€“1868.
[49]Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. Bleu: a
MethodforAutomaticEvaluationofMachineTranslation.In ACL.ACL,311â€“318.
[50]MattPost.2018.ACallforClarityinReportingBLEUScores.In WMT.Association
for Computational Linguistics, 186â€“191.
[51]PaigeRodeghero,CollinMcMillan,PaulW.McBurney,NigelBosch,andSidneyK.
Dâ€™Mello. 2014. Improving automated source code summarization via an eye-
tracking study of programmers. In ICSE. ACM, 390â€“401.
[52]Devjeet Roy, Sarah Fakhoury, and Venera Arnaoudova. 2021. Reassessing auto-
matic evaluation metrics for code summarization tasks. In ESEC/SIGSOFT FSE.ACM, 1105â€“1116.
[53]AbigailSee,PeterJ.Liu,andChristopherD.Manning.2017. GetToThePoint:
Summarization with Pointer-Generator Networks. In ACL (1). Association for
Computational Linguistics, 1073â€“1083.
[54]EnshengShi,YanlinWang,LunDu,HongyuZhang,ShiHan,DongmeiZhang,
andHongbinSun.2021.CAST:EnhancingCodeSummarizationwithHierarchicalSplittingandReconstructionofAbstractSyntaxTrees.In EMNLP(1).Association
for Computational Linguistics, 4053â€“4062.
[55]XiaotaoSong,HailongSun,XuWang,andJiafeiYan.2019. Asurveyofautomatic
generation of source code comments: Algorithms and techniques. IEEE Access 7
(2019), 111411â€“111428.
[56]Giriprasad Sridhara, Emily Hill, Divya Muppaneni, Lori L. Pollock, and K. Vijay-
Shanker.2010. TowardsautomaticallygeneratingsummarycommentsforJava
methods. In ASE. ACM, 43â€“52.
[57]SeanStapleton,YashmeetGambhir,AlexanderLeClair,ZacharyEberhart,Westley
Weimer, Kevin Leach, and Yu Huang. 2020. A Human Study of Comprehension
and Code Summarization. In ICPC. ACM, 2â€“13.
[58]Wei Tao, Yanlin Wang, Ensheng Shi, Lun Du, Shi Han, Hongyu Zhang, Dongmei
Zhang, and Wenqiang Zhang. 2021. On the Evaluation of Commit Message
Generation Models: An Experimental Study. CoRRabs/2107.05373 (2021).
[59]Scott R. Tilley, Hausi A. MÃ¼ller, and Mehmet A. Orgun. 1992. Documenting
software systems with views. In SIGDOC. ACM, 211â€“219.
[60]Ramakrishna Vedantam, C. Lawrence Zitnick, and Devi Parikh. 2015. CIDEr:
Consensus-based image description evaluation. In CVPR.
[61]Yao Wan, Zhou Zhao, Min Yang, Guandong Xu, Haochao Ying, Jian Wu, andPhilip S. Yu. 2018. Improving automatic source code summarization via deep
reinforcement learning. In ASE. ACM, 397â€“407.
[62]WenhuaWang,YuqunZhang,YuleiSui,YaoWan,ZhouZhao,JianWu,Philip
Yu,andGuandongXu.2020. Reinforcement-learning-guidedsourcecodesum-
marization via hierarchical attention. IEEE Transactions on Software Engineering
(2020).
[63]Yanlin Wang, Ensheng Shi, Lun Du, Xiaodi Yang, Yuxuan Hu, Shi Han, HongyuZhang,and DongmeiZhang.2021. CoCoSum:ContextualCodeSummarization
with Multi-Relational Graph Neural Network. CoRRabs/2107.01933 (2021).
[64]BolinWei,GeLi,XinXia,ZhiyiFu,andZhiJin.2019. CodeGenerationasaDual
Task of Code Summarization. In NeurIPS. 6559â€“6569.
[65]Bolin Wei, Yongmin Li, Ge Li, Xin Xia, and Zhi Jin. 2020. Retrieve and Refine:
Exemplar-based Neural Comment Generation. In ASE. IEEE, 349â€“360.
[66]Hongqiu Wu, Hai Zhao, and Min Zhang. 2021. Code Summarization with
Structure-induced Transformer. In ACL/IJCNLP (Findings). Association for Com-
putational Linguistics, 1078â€“1090.
[67] Rui Xie,WeiYe,JinanSun,andShikunZhang.2021. ExploitingMethodNames
to Improve Code Summarization: A Deliberation Multi-Task Learning Approach.
InICPC.
[68]Wei Ye, Rui Xie, Jinglei Zhang, Tianxiang Hu, Xiaoyin Wang, and Shikun Zhang.
2020. Leveraging code generation to improve code retrieval and summarization
via dual learning. In The Web Conference.
[69]Jian Zhang, Xu Wang, Hongyu Zhang, Hailong Sun, and Xudong Liu. 2020.
Retrieval-based neural source code summarization. In ICSE. ACM, 1385â€“1397.
[70]JianZhang,XuWang,HongyuZhang,HailongSun,KaixuanWang,andXudong
Liu.2019. Anovelneuralsourcecoderepresentationbasedonabstractsyntax
tree. InICSE. IEEE / ACM, 783â€“794.
[71]Yuxiang Zhu and Minxue Pan. 2019. Automatic Code Summarization: A System-
atic Literature Review. arXiv preprint arXiv:1909.04352 (2019).
[72]Eric R Ziegel. 2001. Standard probability and statistics tables and formulae.
Technometrics 43, 2 (2001), 249.
1608
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:30:33 UTC from IEEE Xplore.  Restrictions apply. 