Testing of Autonomous Driving Systems: Where Are We and
Where Should We Go?
Guannan Lou
Macquarie University
Sydney, NSW, Australia
guannan.lou@mq.edu.auYao Deng
Macquarie University
Sydney, NSW, Australia
yao.deng@hdr.mq.edu.auXi Zheng∗
Macquarie University
Sydney, NSW, Australia
james.zheng@mq.edu.au
Mengshi Zhang∗
Meta
Menlo Park, CA, USA
mengshizhang@fb.comTianyi Zhang
Purdue University
West Lafayette, IN, USA
tianyi@purdue.edu
ABSTRACT
Autonomous driving has shown great potential to reform modern
transportation. Yet its reliability and safety have drawn a lot of
attention and concerns. Compared with traditional software sys-
tems, autonomous driving systems (ADSs) often use deep neural
networks in tandem with logic-based modules. This new paradigm
poses unique challenges for software testing. Despite the recent
development of new ADS testing techniques, it is not clear to what
extent those techniques have addressed the needs of ADS practi-
tioners. To fill this gap, we present the first comprehensive study
to identify the current practices and needs of ADS testing. We
conducted semi-structured interviews with developers from 10 au-
tonomous driving companies and surveyed 100 developers who
have worked on autonomous driving systems. A systematic analy-
sis of the interview and survey data revealed 7 common practices
and 4 emerging needs of autonomous driving testing. Through a
comprehensive literature review, we developed a taxonomy of ex-
isting ADS testing techniques and analyzed the gap between ADS
research and practitioners’ needs. Finally, we proposed several fu-
ture directions for SE researchers, such as developing test reduction
techniques to accelerate simulation-based ADS testing.
CCS CONCEPTS
•Software and its engineering →Software testing and de-
bugging .
KEYWORDS
Autonomous Driving, Software Testing, Empirical Study
ACM Reference Format:
Guannan Lou, Yao Deng, Xi Zheng, Mengshi Zhang, and Tianyi Zhang.
2022. Testing of Autonomous Driving Systems: Where Are We and Where
∗Corresponding authors: Xi Zheng, Mengshi Zhang.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
ESEC/FSE ’22, November 14–18, 2022, Singapore, Singapore
©2022 Association for Computing Machinery.
ACM ISBN 978-1-4503-9413-0/22/11. . . $15.00
https://doi.org/10.1145/3540250.3549111Should We Go?. In Proceedings of the 30th ACM Joint European Software
Engineering Conference and Symposium on the Foundations of Software Engi-
neering (ESEC/FSE ’22), November 14–18, 2022, Singapore, Singapore. ACM,
New York, NY, USA, 13 pages. https://doi.org/10.1145/3540250.3549111
1 INTRODUCTION
Autonomous driving has been making great strides towards re-
ality in recent years. In 2017, Waymo launched the trail of fully
autonomous ride-hailing services in California [ 40]. More recently,
Tesla released the beta version of its Full Self-Driving (FSD) soft-
ware, which has been installed on at least 60K Tesla vehicles [ 47].
However, given the traffic accidents caused by autonomous vehi-
cles [ 12,31,79] , there is still a long way to ensure the robustness
and reliability of autonomous driving systems (ADSs). Similar to
traditional software systems, autonomous driving companies also
adopt testing as the main quality assurance mechanism for ADSs.
Several leading companies such as Waymo and Tesla have their
own fleets to perform extensive on-road testing. Furthermore, sim-
ulation testing environments such as Carla [ 19] are widely adopted
to test various driving scenarios or extreme conditions that cannot
be easily replicated in reality.
In recent years, the Software Engineering (SE) community has
also proposed many testing techniques to improve the safety and re-
liability of ADSs [ 1,2,6,14,18,20,30,34–39,41,45,46,48,50,51,58–
62,64,66,67,69–71,73,77,86,92,93,95,96,101,106,108,110,111].
For example, DeepRoad and DeepTest [ 95,106] leverage meta-
morphic testing to test ADSs under extreme weather conditions.
AC3R [ 36] generates critical driving scenarios (e.g. collisions) in
a simulation environment based on crash reports. These meth-
ods have shown promising results for end-to-end driving models.
However, with the rapid evolution of autonomous driving technolo-
gies, industrial ADSs have become much more sophisticated these
days, using multiple perception models in tandem with logic-based
control and planning modules. Currently, there is a lack of compre-
hensive understanding of the emerging needs of ADS testing and
to what extent existing ADS testing techniques meet the needs of
ADS practitioners. To bridge this gap, we adopted a mixed methods
research design [ 52] with a combination of qualitative interview
study, large-scale survey, and literature review to investigate the
following three research questions:
•RQ1 .What are the industrial practices of ADS testing?
•RQ2. What are the emerging needs of ADS testing?arXiv:2106.12233v5  [cs.SE]  23 Sep 2022ESEC/FSE ’22, November 14–18, 2022, Singapore, Singapore Guannan Lou, Yao Deng, Xi Zheng, Mengshi Zhang, and Tianyi Zhang
•RQ3. To what extent existing ADS testing techniques address the
industrial needs?
We first interviewed developers from 10 autonomous driving
companies to collect qualitative responses on ADS testing practices
and needs. Based on the insights from the interview study, we
developed a survey and solicited quantitative responses from a
boarder audience. Specifically, we sent out 1978 surveys to ADS
developers and received 100 complete and valid responses. Through
comprehensive data analysis and triangulation, we summarized
seven common practices in ADS development and testing, some
of which have not been accounted by existing testing techniques.
For example, ADS practitioners adopt more system-level testing
metrics such as consistency and latency when testing an ADS,
rather than just using model accuracy. Furthermore, we identified
four common needs of ADS testing: (1) identifying possible corner
cases and unexpected driving scenarios , (2)speeding up ADS testing ,
(3)tool support for constructing complex driving scenarios , and (4)
tool support for data labeling .
We further conducted literature review on ADS testing methods
proposed by the SE community. We manually went through 117
papers that mentions autonomous driving or related keywords in
28 SE conferences and identified 42 papers specifically about ADS
testing. We categorized them and developed a taxonomy of different
kinds of research on ADS testing. We identified four gaps between
existing research and industrial needs. For instance, existing ADS
testing methods only consider simple image transformations. There
is a lack of support for identifying richer and more complex traffic
scenarios that ADS developers really need in practice. Based on
these gaps, we proposed several future directions. For example, SE
researchers can leverage test selection and prioritization techniques
to speed up ADS testing (Need 2). Specifically, to account for the
multi-module architecture of ADSs, test selection methods can be
formulated as a multi-objective optimization problem to maximize
multiple coverage metrics for different modules (e.g., neural cover-
age for a perception model and statement coverage for a logic-based
control module), rather than a single model.
Paper organization. Section 2 discusses related work in ADS
testing. Section 3 illustrates how we conduct interviews and sur-
veys. Section 4 provides current practice of ADS testing in industry.
Section 5 summarizes needs of industry in ADS testing. Section 6
discusses the existing SE solutions to these needs and future re-
search directions. Section 7 discusses the threats to validity in this
study. Section 8 concludes this work.
2 RELATED WORK
There are several literature reviews on testing and verification of
machine learning (ML) models [ 15,29,103,105]. The most compre-
hensive literature review is by Zhang et al. [ 105]. They analyzed
114 papers related to machine learning testing and provided an
overview of various testing properties, components, and workflows
of ML models. In addition, Zhang et al. identified several challenges
of testing ML models, such as how to generate natural test inputs.
Compared with these studies, our study focuses on ADS testing. In
addition to a literature review, we also interviewed and surveyed
ADS practitioners to understand the common practices and needs
of ADS testing. We found that the unique characteristics of ADSs(e.g., the multi-module architecture) along with the special testing
needs pose new challenges and opportunities compared with or-
dinary ML models, e.g., leveraging multi-objective search in test
selection for multi-module ADSs.
Two recent studies analyzed the codebase and bugs in autono-
mous driving systems [ 39,82]. Peng et al. [ 82] conducted a case
study of Baidu Apollo [ 5] and summarized the architecture and
individual modules in the Apollo system. They found that Apollo
lacked adequate testing at the system level. Garcia et al. [ 39] ana-
lyzed 16,851 commits and 499 issues in Apollo and Autoware [ 8].
They classified these issues and summarized their symptoms and
root causes. There are also several empirical studies on bugs in ML
applications [ 4,91,94,107,109]. For example, Zhang et al. [ 109]
analyzed 175 software bugs in ML applications built by TensorFlow.
The most related work to our study includes several literature re-
views on autonomous driving testing [ 49,63,73]. Huang et al. [ 49]
reviewed testing and verification methods from the Intelligent Ve-
hicle (IV) community. They summarized not only testing methods
for individual modules in the software stack, but also hardware-in-
the-loop testing methods for hardware components and integrated
testing for the entire vehicle. Compared with Huang et al., we focus
on testing methods from the software perspective. Our literature
review summarized recent advances in ADS testing from the SE
community. Please refer to Section 6 for a detailed summary and
discussion of these software testing techniques. Koopman and Wag-
ner proposed five challenges of testing autonomous vehicles based
on the V model for autonomous vehicles [ 63]. Masuda described the
software architecture of autonomous vehicle simulations and dis-
cussed several software testing challenges of such simulations [ 73].
Compared with them, our discussion is anchored upon common
practices and needs of ADS testing from interviews and online
surveys with ADS practitioners.
3 METHODOLOGY
Following [ 88], Our empirical study contains 3 steps, as shown
in Figure 1. First, we conducted semi-structured interviews with
10 developers from 10 different autonomous driving companies.
Based on the findings from these interviews, we further conducted
a large-scale survey with 100 ADS practitioners to quantitatively
validate our findings from the interviews. We then summarized and
triangulated the findings from the surveys and the interviews. Third,
we conducted an literature review of SE papers related to ADS
testing. We categorized those papers and developed a taxonomy
of ADS testing techniques. By comparing the taxonomy and the
emerging needs of ADS practitioners, we identified the research
gaps.
3.1 Interviews
Interview protocol. We designed an interview guide1for the semi-
structured interviews. The interview began with a short introduc-
tion of our study. Then, we asked high-level questions about: 1) the
background and expertise of interviewees, such as the current job
role and how long they have been working on ADSs, 2) the current
practices, methodologies, and tools they used for ADS testing, and
3) the challenges and difficulties they faced in ADS testing. We first
1The complete interview guide is publicly available here: https://bit.ly/3DKia0CTesting of Autonomous Driving Systems: Where Are We and Where Should We Go? ESEC/FSE ’22, November 14–18, 2022, Singapore, Singapore
Figure 1: Research methodology
Table 1: Interview participant background
Experience Responsibilities
P1 2 years Recognition algorithm design and testing
P2 2 years Simulation testing and real-world testing
P3 6 years Passenger car development
P4 1.5 years Perception system development
P5 10 years ADS testing
P6 3 years ADS testing
P7 1.5 years Perception system testing
P8 5 years ADS developing and testing
P9 1.5 years Recognition algorithm design and testing
P10 1.5 years ADS testing, Simulation testing
conducted 2 pilot interviews, based on which we further refined
the interview guide. Then, we interviewed 10 developers from 10
different autonomous driving companies. Each interview took be-
tween 30 minutes and an hour. The interviews were recorded with
permission of participants and then transcribed for analysis.
Participants. We recruited 10 software developers from 10 dif-
ferent autonomous driving companies based on our personal net-
work, industrial collaboration, and social media. As shown in Ta-
ble 1, these participants had at least one and a half years of expe-
rience (3.4 years on average) in ADS development and testing. In
addition, their responsibilities also covered every aspect of ADS
testing, including modular testing, simulation-based testing, and
real-world testing.
Analysis. We transcribed a total of 7 hours of interview record-
ings to text using an audio transcription tool called iflyrec2. We
manually corrected errors in the transcripts. The first two authors
conducted inductive thematic analysis [ 16]3using a qualitative
data analysis software called MAXQDA4. Specifically, they first
independently labeled the transcripts to extract relevant or insight-
ful responses from participants and summarize them into short
2https://www.iflyrec.com/
3The maxqda codebook is publicly available here: https://bit.ly/38K9Csk
4https://www.maxqda.com/descriptive texts, which are called codes. Then they met each other,
compared their codes, and discussed any inconsistencies. They
continuously refined the codes during the discussion. Then they
gro-uped related codes into themes. Results of the thematic analy-
sis were regularly reported and discussed with the whole research
team. The final inter-rater agreement ratio of the first two authors
is 0.85, measured by Cohen’s Kappa [97].
3.2 Survey
Since the findings in the interview study are only based on re-
sponses from 10 participants, we further conducted a large-scale
survey to validate the interview findings and solicit more feedback
from a broader population of ADS practitioners.
Survey design. We designed a survey5with 33 questions in 5
sections, including background ,autonomous driving system ,testing
practices and challenges in ADS ,testing needs in ADS , and follow
up. The background section asked about participants’ background,
expertise, and their current roles in the company or organization.
The autonomous driving system section asked about the ADSs par-
ticipants have worked on. The testing practices and challenges on
ADS section asked about three types of ADS testing identified from
the previous interviews, including unit testing, simulation testing,
and on-road testing. For each type of testing, we designed multiple-
choice questions based on the findings from the interview study.
The options in a multiple-choice question were derived from re-
sponses of the interview study. Participants can also select “Others”
to supplement alternative answers, or select “I don’t know” to indi-
cate they have no insights in the particular question. For each type
of testing, we also included an open-ended question in the end to
solicit additional feedback on what participants would like to have
or improve on. The testing needs in ADS section listed four common
needs identified in the interview study. We designed a linear scale
question for each need and asked participants to provide a numeric
response to indicate the importance of each need in a 7-point likert
scale, from “unimportant at all” (1) to “very important” (7). The
5The complete survey form is publicly available here: https://bit.ly/2WNVV93ESEC/FSE ’22, November 14–18, 2022, Singapore, Singapore Guannan Lou, Yao Deng, Xi Zheng, Mengshi Zhang, and Tianyi Zhang
follow up section asked participants their contact information and
whether they would like to participate in any follow-up study.
Participants. We recruited survey participants in three ways.
First, we sent out surveys to the autonomous driving companies
where our interview participants came from. Second, we used the
APIs provided by Twitter and LinkedIn to scrape the contact in-
formation (if any) of developers with profile associated with au-
tonomous driving. Finally, we searched for popular autonomous
driving software repositories on Github such as Apollo [ 5] and
DeepDrive [ 27]. Then, we manually identified the public email ad-
dress (if any) of those developers contributed to those repositories.
In total, we sent out 1978 surveys and received 114 responses,
with a response rate of 5%. We discarded 14 survey responses since
they are not complete. In the end, we collected 100 survey responses
that are complete for data analysis.690% of survey participants were
male, 8% were female, and 2% did not disclose their gender identity.
54% of them worked in research institutes, 36% worked in technol-
ogy companies, 7% of participants worked in traditional vehicle
manufactures, and 3% of participants were self-employed. Regard-
ing their job roles, 33% of participants were in R&D positions, 14%
were in management positions, and 24% were engineers, including
safety engineers, software engineers and perception engineers. 40%
of participants had two to three years of working experience in
ADS, 29% had worked in ADS for more than three years, and 31%
had less than two years of experience.
Analysis. For each multiple-choice question, we plotted the
choices made by survey participants in a histogram, such as Figure 2.
Specifically, if a survey participant supplemented an alternative an-
swer that was not identified in our interview study, we first checked
whether it was similar to an existing choice. If not, we considered it
as a unique answer and created a short, descriptive label for it. We
then merged all the alternative answers with the same label and
plotted their distributions in the histogram as well. For linear-scale
questions, we calculated the total number and percentage of each
option, and used the percentage to illustrate survey participants’
evaluation of the importance of industrial needs, such as Figure 6.
For open-ended questions, the first two authors used MAXQDA to
code each answers individually, classified these answers according
to sections in this work, and discussed with each other to refine
codes and classifications.
3.3 Literature Review
We created the literature review protocol including research ques-
tion, literature search strategy, literature selection criteria, literature
selection procedures, data extraction strategy, and synthesis of the
extracted data, following the methodology described in [ 57] to guide
the literature review process. Our research question of literature
review is to investigate whether existing research works related to
ADS testing are adequate for the challenges and needs identified
in our interviews and surveys. The literature search strategy is to
search research papers from 28SE conferences, journals and work-
shops including ICSE, ESEC/FSE, ASE, ISSTA, ISSRE, ICST, QRS,
TSE, JSS, and IST. Specifically, we used 35keywords to identify
research papers related to autonomous driving, e.g., “autonom-
ous driving”, “autonomous vehicle”, “traffic scene”, “Apollo”, etc.
6The survey result is publicly available here: https://bit.ly/38K9CskWe built a crawler to download papers from publisher websites
and found 117 papers that contain at least one of the keywords in
their title or abstract. The literature selection criterion is to include
technical papers that propose ADS testing methods and empirical
papers that discuss challenges and solutions related to ADS testing.
We manually reviewed all searched papers and found that 102 pa-
pers related to ADS, among which 42 are specifically about ADS
testing7. We categorized these 42 ADS testing papers into different
categories based on their research questions and solutions. The
result is summarized in Section 6.1.
4 COMMON PRACTICES OF ADS TESTING
This section summarizes the common practices of ADS testing.
Section 4.1 discusses the types of ADSs that our interview and
survey participants worked on. Section 4.2, Section 4.3 and Section
4.4 discuss three commonly used ADS testing methods— unit test-
ing,real-world testing andsimulation testing . We do not separately
present the results of the interview study and the survey for two
main reasons. First, since the purpose of the survey is to confirm
and supplement the qualitative findings from the interview, the
findings from the interview study and the survey study have a lot of
overlap. Therefore, if we report the findings separately, there will be
a lot of redundancy. Second, fusing quantitative evidence (such as
statistics from a large-scale survey) and qualitative evidence (such
as quotations from interview participants) is more convenient for
readers to read and understand the results of the survey.
4.1 Autonomous Driving Systems under Test
Our participants mainly work on two main types of ADSs: multi-
module driving systems andend-to-end driving models .
Multi-module driving systems. The majority of interviewees
(100%) and survey participants (69%) developed and tested multi-
module driving systems. Multi-module architectures are widely
used in industry-scale driving systems, e.g., Autoware [ 54] and
Apollo [ 5]. They contain several modules for perception, prediction,
planning, and control. The perception module takes a variety of
sensor data as input, such as road images, point clouds, and GPS
signals, to detect surrounding objects. The prediction module pre-
dicts the moving trajectories of these surrounding objects. Given
the perception and prediction results, the planning module then
decides on the route of the ego-vehicle. Finally, the control module
converts the planned route to vehicle control commands, including
braking force and steering angle. Four interview participants (P1,
P3, P7, P9) elaborated that the perception and prediction modules
in their systems heavily use deep neural networks, while the plan-
ning and control modules typically contain traditional logic-based
programs. This is consistent with a previous study on Apollo [ 98].
End-to-end driving models. 31% of survey participants said
they worked on end-to-end (E2E) driving models, while none of the
interviewees worked on E2E driving models. E2E driving models
such as PilotNet [ 13] and OpenPilot [ 24] treat the entire driving
pipeline as a single deep learning model, from processing sensor
data to generating vehicle controls. During the interview, partic-
ipants pointed out that E2E driving models are less preferred in
7The keyword list, venue list and paper list are publicly available at https://bit.ly/
3FLUwCzTesting of Autonomous Driving Systems: Where Are We and Where Should We Go? ESEC/FSE ’22, November 14–18, 2022, Singapore, Singapore
the industry, since they cannot handle complicated driving scenar-
ios and often suffer from generalizability issues. For example, P7
said, “Training an E2E driving model requires a large amount of data.
And E2E driving models can easily over-fit and perform worse than
multi-module systems. ”
Common Practice 1
The majority of ADS practitioners reported to work on multi-
module ADSs rather than end-to-end driving models. There-
fore, multi-module ADSs deserve more attention in future
research.
4.2 Unit testing
80% of interviewees and 52% of survey participants reported that
they conducted unit testing during ADS development.
Testing target. In addition to writing unit tests for control logic,
ADS developers also need to test DL models, especially those models
in the perception and prediction modules. Unlike program source
code, these models do not have clearly control logic or program
states. ADS developers need to manually label and clip driving
recordings collected from on-road or simulation testing to con-
struct small recording segments for testing these DL models, which
are referred to as unit tests in ADS development. There intervie-
wees (P1, P4, P9) reported that there are too many possible driving
scenarios to test, and it is time-consuming to manually process
driving recordings to construct test scenarios.
In addition, 70% of interviewees and over 68% of survey partici-
pants said their driving systems used at least three of four types of
sensors, e.g., cameras, LiDARs, radars, GPS. This multi-modality of
sensor data makes it more difficult to generate test cases. For exam-
ple, data from different sensors with different sample frequencies
need to be synchronized (e.g., by timestamp). Furthermore, when
transforming one type of sensor data, such as adding an object,
other types of sensor data must be updated consistently.
Common Practice 2
In addition to testing control logic, ADS developers also need
to construct segments of driving recordings to test DL models,
which take multi-modal sensor data as input, not just road
images.
Test metrics. When measuring the performance of a model,
ADS developers use metrics, including accuracy, precision, recall,
Receiver Operating Characteristic (ROC), and Intersection over
Union (IoU). Furthermore, 51% of survey participants said they
also use specific metrics tailored for different ADS modules. Two
interview participants (P1, P9) elaborated on this—consistency is
one of their metrics when testing the lane detection model. The
lane detection results between the front and back frames in a video
frame should be consistent.
Figure 2: Common driving scenarios tested in on-road test-
ing and simulation testing, respectively
Figure 3: Sources of tested driving scenarios
Common Practice 3
ADS practitioners not only use common model performance
metrics, such as accuracy and IoU, but also custom-tailored
metrics such as consistency when testing perception models
in ADS.
4.3 Real-world testing
All interview participants and 57% of survey participants reported
that they have done real-world testing. Two types of real-world
testing are mentioned: scenario-based testing andon-road testing .
Scenario-based testing. 4 interview participants (P1, P2, P8, P9)
and 89% survey participants mentioned they have tested specific
driving scenarios in closed automated vehicle proving grounds.
Figure 2 shows commonly tested driving scenarios, as reported in
our survey. It includes common driving scenarios such as changing
lanes (76%), following other cars (72%), and turning left or right
(69%), as well as special road sections such as intersections (33%)
and roundabouts (over 55%). Weathers such as rainy days (44%)ESEC/FSE ’22, November 14–18, 2022, Singapore, Singapore Guannan Lou, Yao Deng, Xi Zheng, Mengshi Zhang, and Tianyi Zhang
Figure 4: Length of on-road testing performed by industrial
survey participants
and snowy days (33%) are considered as well. Finally, dangerous
driving scenarios such as emergency braking and collision are also
conducted by 28% and 21% of survey participants, respectively.
In the intervew study, P1 and P2 said they have built their own
scenario database following traffic law and regulations. P8 and P9
said setting up those driving scenarios were time-consuming, which
often took 2 weeks to 1 month.
Figure 3 shows the sources where those driving scenarios were
from, as reported in the survey . 80% and 57% of survey participants
said those scenarios were based on common real-world driving
scenes and driving experiences. 54% of survey participants said
they referred to public benchmarks such as CityScapes [ 26], Apol-
loScape [ 98], and Waymo open dataset [ 90]. 40% of survey partic-
ipants said they referred to traffic laws and regulations. 23% said
they referred to traffic accident reports.
Common Practice 4
ADS practitioners design various driving scenarios based on
real-world driving scenes, public benchmarks, traffic laws and
regulations, and crash reports to test an ADS in the field.
On-road testing. In on-road testing, autonomous vehicles are
tested in public roads where various scenarios and unexpected
conditions could occur. In practice, ADS companies need to carry
out long-distance on-road testing to ensure the reliability of their
driving systems. The number of kilometers an ADS travels without
human intervention (i.e., km per disengagement) in on-road testing
is used to measure the reliability of their ADS. As shown in the
Disengagement from California Department of Motor Vehicles [ 80],
Waymo conducted over 1 million kilometers of on-road testing in
2020, and its distance for each disengagement was about 48,000 km.
During the interview, P8 said, “ the entire on-road testing required
50,000 to 100,000 kilometers, which must include different driving
scenes such as highways, country roads, and urban roads. ” Further-
more, P9 mentioned that testers had to sit in the car and record
driving data in on-road testing. The collected data would be either
saved in local storage or uploaded to cloud platform, some of which
would be cleaned and labelled for model training and testing. While
on-road testing is critical, the reality is that, due to technical and
financial constraints, many ADS practitioners can only conduct on-
road testing within a limited range of mileage. Figure 4 shows that
only 8% of participants conducted more than 100,000 km on-road
testing, and 11% of them conducted 10,000 to 50,000 km on-road
testing. As discussed in the next section, simulation testing is often
Figure 5: System-level metrics used in real-world testing
considered as a more affordable and safer testing option for the
majority of ADS practitioners.
Common Practice 5
On-road testing is considered as critical to ensure ADS re-
liability and robustness, while only a small portion of ADS
practitioners have done long-distance testing.
Test metrics. Compared with metrics used in unit testing, more
system-level metrics are used in real-world testing. As shown in
Figure 5, four system-level metrics are frequently mentioned by
survey participants, including generalizability (78%), passenger’s
experience (63%), robustness (53%), and system latency (49%). Gen-
eralizability focuses on whether an ADS can achieve similar perfor-
mance in unseen driving scenes. Passenger’s experience is another
important metric in real-world testing. P4 said, “ when there are
many vehicles in a driving scene, an ADS should not press the brake
too frequently, which may make passengers uncomfortable. ” Robust-
ness assesses whether an ADS can behave normally when noises,
external interference, or attacks exist. As an ADS is expected to
make quick, continuous responses to the change of driving scenes,
system latency is often used to measure the decision-making speed
of an ADS.
Common Practice 6
Test metrics used in real-world testing focus more on system-
level performance, including both functional and non-functional
properties, rather than only model accuracy.
4.4 Simulation testing
5 interviewees and 87% of survey participants said they conducted
ADS testing in a simulation platform such as Carla [ 19], AirSim [ 74]
and LGSVL [ 65]. When asked why simulation testing is so widely
practiced, participants mentioned two main reasons. First, 54% of
survey participants supported that modern simulation environme-
nts are powerful enough to test corner cases that are costly to set
up in the real world. In the interview study, P2 and P6 mentioned
that for dangerous driving scenarios such as collisions, simulation
testing is much more preferred due to safety concerns. As shown
in Figure 2, more participants did collision test in simulation than
in the real world. Second, P2 and P9 mentioned that simulators can
replay sensor data and vehicle control commands collected from
real-world testing, which can be used to test the performance of a
new release of an ADS. 55% of survey participants also support this
point of view. Simulation testing is an important part of regression
testing in ADS development. When developing an ADS, it is costlyTesting of Autonomous Driving Systems: Where Are We and Where Should We Go? ESEC/FSE ’22, November 14–18, 2022, Singapore, Singapore
Figure 6: The importance of four common needs voted by
survey participants. Details of each need are described in
Section 5.
and time-consuming if a developer tests every commit of ADS
in the real-world. Leveraging the simulation platform, industrial
practitioners do not need to deploy each commit of ADS on the
real vehicle and construct real-world test scenarios.
Common Practice 7
Simulation testing is widely adopted as a complement for real-
world testing. It is particularly adopted to test new commits
as part of regression testing.
Test metrics. Test metrics used in simulation testing is similar as
on-road testing, including accuracy-based metrics, generalizability,
passenger’s experience, and robustness. However, as the simulation
platform is built as an ideal environment without hardware delay,
system latency is less considered in simulation testing.
5 EMERGING NEEDS
This section summarizes four emerging needs of ADS testing identi-
fied in the interview and survey. Figure 6 shows survey participants’
agreement on the importance of these needs. 68%, 63%, 51% and 48%
of survey participants considered Need 4, Need 1, Need 3, and Need
2 important or very important. Especially for Need 4 and Need
1, they are regarded as very important by more than one-third of
survey participants.
5.1 Need 1: Identifying possible corner cases
and unexpected driving scenarios
As shown in Figure 6, 35% and 28% of survey participants rated
this need very important andimportant respectively. While mod-
ern driving systems are tested with diverse driving scenarios and
extensive on-road testing, new corner cases are still often found
during on-road testing. As P9 said, “During on-road testing on Cali-
fornia highways, we found it difficult to distinguish lane lines under
the sunset. This was a problem we did not expect when we perform
scenario-based testing. ” Based on our conversation with ADS practi-
tioners, the current industry practice to discover more corner cases
seems to be simply performing more and longer on-road testing.
However, unlike large automotive companies, small companiesoften lack resources (e.g., vehicle fleets) and on-road testing certifi-
cates to perform large-scale on-road testing. Therefore, identifying
corner cases efficiently is an urgent need for ADS practitioners.
5.2 Need 2: Speeding up ADS testing
As shown in Figure 6, 24% and 24% of survey participants rated this
need very important andimportant respectively. According to some
existing work [ 17,63], the catastrophic failure rate of an ADS should
be minimized to 10−7to10−9for 1 to 10 hours driving to achieve
the goal of high reliability. To verify that the failure rate falls within
one per 107hours, one must conduct at least 107-hour testing of an
ADS (about 1141 vehicle-years). It is unrealistic to achieve this goal
with on-road testing. Therefore, ADS practitioners often resort to
simulation testing. Widely-used simulators such as CarSim [ 11]
support acceleration that only takes one-tenth of the time compared
with real-world testing. However, given the large amount of time
required to achieve high reliability (i.e., 107-hour driving test), it is
still time consuming to conduct test in simulation. As P2 said, “Our
original goal was to do 20,000 kilometers of testing. Later, we found
that even if it accelerates the speed by 5 times, the speed of the test
is still slow, compared with our expectation. ” Therefore, simulation
acceleration is not a silver bullet to this challenge. Other methods
to speed up ADS testing are needed. For example, test selection
and prioritization approaches have been widely investigated in
tradition software systems, which can be leveraged to accelerate
ADS testing. We discuss this in detail in Section 6.3.
5.3 Need 3: Tool Support for Constructing
Complex Driving Scenarios
As shown in Figure 6, 25% and 26% of survey participants rated this
need very important andimportant respectively. ADS practitioners
design driving scenarios based on various sources such as real-
world driving scenes and traffic accident reports (Section 4.3). After
identifying driving scenarios worth testing, they need to construct
corresponding test cases. 65% of survey participants found it cum-
bersome to use toolkits (e.g., domain-specific languages, libraries,
etc.) provided by existing simulation platforms to construct a test
case of a driving scenario. Take OpenScenario [ 7] as an example. It
takes 258 lines of code to construct a simple driving scenario of lane
cutting, not even to mention complex scenarios. The main reason is
that existing simulation platforms only provide low-level APIs and
domain-specific languages (DSLs) to construct driving scenarios.
While such low-level API and language design ensures the flexibility
and expressiveness to precisely specify arbitrary driving scenarios,
it imposes significant coding effort. Similar to how Google releases
Keras as a higher-level abstraction of TensorFlow, it would be ben-
eficial to provide a higher-level abstraction of the APIs and DSLs
in the simulation platforms. In addition, as mentioned by P8, " We
already collected traffic accident video from internet, but it is still hard
to automatically transform them into test cases in simulators. " ADS
developers wish they can get more tool support that helps them
automatically or semi-automatically construct driving scenarios in
a simulation environment, such as translating a natural language
description of a traffic scene into the low-level code written in APIs
provided by a simulation environment.ESEC/FSE ’22, November 14–18, 2022, Singapore, Singapore Guannan Lou, Yao Deng, Xi Zheng, Mengshi Zhang, and Tianyi Zhang
5.4 Need 4: Tool support for data labeling
As shown in Figure 6, 38% and 30% of survey participants rated
this need very important and important respectively. Section 4.3
has already discussed that driving data collected in on-road testing,
such as point clouds and road images, are often used as test data.
However, as mentioned by P8, these driving data need to be manually
labelled and clipped first before they can be replayed in a simulation
environment or reused to test a DL model. Two typical labels are 2D
bounding boxes and semantic segmentation masks. A 2D bounding
box label includes the height and width of a detected object and
the type of the object. And a semantic segmentation mask requires
data labelers to manually segment all objects at pixel level. Given
the massive amount of driving data collected from on-road testing,
the labeling effort is enormous. While there have been some as-
sistive labeling tools in recent years [ 53,75,87], labeling driving
data still requires many manual effort. For example, Amazon Sage-
Maker [ 53] can automatically assign labels for object detection and
image segmentation tasks. But it requires data labelers to manually
go through those labels and make corrections. Several ADS com-
panies we have interviewed said they often outsourced the data
labeling task to data labeling companies or used crowdsourcing
platforms such as Amazon Mechanic Turks [ 81]. However, even
for manually labeled data, label quality is still a concern, especially
for safety-critical tasks like autonomous driving. According to a
recent study [ 78], several well-known datasets such as ImageNet
are riddled with manual labeling mistakes. Therefore, ADS prac-
titioners wish to get more tool support to analyze, recognize, and
repair labeling errors in their driving data.
6 LITERATURE REVIEW AND RESEARCH
GAPS
The previous section summarizes four emergent needs from indus-
trial practitioners in ADS testing based on interviews and surveys.
To understand to what extent state-of-the-art ADS testing tech-
niques have addressed these needs, we surveyed research papers
from software engineering conferences and journals and manually
assessed them. This section summarizes our findings and future
research opportunities.
6.1 Literature Taxonomy
Figure 7 describes our taxonomy of ADS testing techniques, based
on the 42 papers identified from Section 3.3.
Corner case generation. A variety of techniques have been pro-
posed to generate corner cases for ADSs. We categorized them into
two lines of research. One line of research is knowledge-based meth-
ods[20,35,36,45,60,67,93,95,96,106,111]. Knowledge-based
methods generate corner cases using domain-specific ontology or
metamorphic relations, which are designed based on human driving
experience, traffic accident reports, traffic law and regulations. For
example, DeepTest [ 95] and DeepRoad [ 106] generated corner cases
using metamorphic relations from common sense that weather
changes should not affect the steering angle prediction of an ADS.
AC3R [ 36] used domain-specific ontology and natural language
processing to extract information from police crash reports and
reconstruct corner cases of crash accidents. DeepBillboard [ 111]generated corner cases by adding adversarial perturbation [ 28]
patches on real-world billbo-ards to check whether the E2E driving
model keeps the same output of the steering angle.
Search-based method [1,2,6,14,18,37,38,41,50,61,66,70,71,
77,92] is another line of research in corner case generation for
ADSs. These methods aim to find a set of test parameters that
introduce behavior discrepancies in an ADS, such as collisions and
lane departures. To define a parameter search space, the majority
of search-based methods leverage driving patterns [1,6,14,18,37,
38,50,61,66,70,71,77,92]. Driving patterns are designed based on
the states of an autonomous vehicle, such as its position and speed,
and other vehicles and pedestrians on the road. The search process
is normally guided by manually defined fitness functions such as
time-to-collision and the minimal distance between the ego-vehicle
and pedestrians. Optimal solutions of such fitness functions are
found by applying genetic algorithms [ 1,37,38,71]. For example,
AsFault [ 38] used genetic algorithm to evaluate parameters of road,
like length and position, to make the ego-vehicle more error-prone
on lane keeping task on generated test cases. Abdessalem et al. [ 2]
proposed a search algorithm that combines the genetic algorithm
with a decision tree classifier to guide the test case generation
faster towards critical test scenarios. Gladisch et al. used Bayesian
optimization as an alternative for genetic algorithms [41].
Test selection and prioritization. We identified 5 papers [ 58,70,
71,77,101] on this research direction. These methods select or
prioritize test cases according to the similarity between the vector
representation of each test case. Kim et al. [ 58] encoded a test
case as an embedding vector using the output of a set of selected
neuron outputs, and used the likelihood-based surprise adequacy
and Mahalanobis-distance-based surprise adequacy to measure the
difference between a test case with others by estimating the density
of vector representations of each selected test case. Test cases with
high surprise adequacy would be selected as critical test cases.
Empirical Studies. We found 5 empirical studies on ADS testing.
Knauss et al. [ 62] and Zhang et al. [ 108] conducted empirical studies
including focus groups and interviews to identify challenges in ADS
testing. Jahangirova et al. [ 51] performed a systematic analysis on
metrics of driving qualities, and used a mutation analysis to select
26 metrics, such as the max lateral position and the standard error of
speed, as functional oracles for ADS testing. These selected oracles
can kill mutants of the E2E driving model with low false alarm
rate. Liu et al. [ 69] analyzed autonomous vehicle disengagement
and collision reports from the California Department of Motor
Vehicles [ 79] and found that the growth of on-road testing mileage is
not accompanied by the increase of the disengagement ratio. Garcia
et al. [ 39] investigated on commits and ADS bugs in Apollo [ 5] and
Autoware [ 8], and classified found bugs into 13 root causes, such
as incorrect algorithm implementations, incorrect condition logic
and concurrency.
Simulation Testing. 4 SE papers have discussed simulation test-
ing. Hu et al. [ 48] and Masuda et al. [ 73] discussed the potential
issues in simulation testing, including huge input space and preci-
sion issues in the simulator. Haq et al. [ 46] discussed the reliability
of driving data generated by simulators. Leudet et al. [ 64] designedTesting of Autonomous Driving Systems: Where Are We and Where Should We Go? ESEC/FSE ’22, November 14–18, 2022, Singapore, Singapore
Figure 7: A taxonomy of ADS testing research.
a simulator called AILiveSim, which is not only focused on au-
tonomous vehicles, but can also be used to simulate autonomous
ships and autonomous mining machines.
Formal Verification. Fritzsch et al. [ 34] reported their experience
of employing bounded model checking on a symbolic model checker
NuSMV to verify relevant properties for a vehicle control system.
Du et al. [ 30] used a DSL to specify relationships among driving
scenario elements and used Stochastic Hybrid Automata to specify
the dynamic behaviors, which can then be checked by an existing
model checker UPPAAL-SMC. The approach can be used to verify
safety-related properties of a specific driving scenario.
Scenario-based testing. King et al. [ 59] proposed a scenario-based
testing method to test the adaptive cruise control function in ADS.
The proposed method can parallel evaluate multiple systems in
multiple similar but unknown test scenarios. And the prototype is
Safety Analysis. Salay et al. [ 86] introduced a classification-specific
safety analysis tool based on failure mode effects analysis. The
method named CFMEA is able to identify failure modes and risk
levels in perception models of ADS. Similarly Zhao et al. [ 110] used
a variant of Conservative Baysian inference to avoid catastrophic
failure due to overconfidence for a few inference tools used in AV.
6.2 Gaps and Future Directions for Need 1
Among the four emerging needs identified in our study, the need of
identifying corner cases and unexpected driving scenarios is the one
that receives the most attention from the research community. As
discussed in Section 6.1, a large number of corner case generation
techniques have been proposed for ADSs. These techniques have
already constituted of good solutions that we believe ADS practi-
tioners should try out. Here we summarize several improvement
opportunities for the existing techniques. First, for ontology-based
or metamorphic relation-based methods, it is worth investigating
the generation of more complex driving scenarios (e.g., lane merg-
ing, overtaking) beyond, for instance, affine transformations and
weather conditions. Second, for search-based methods, it is always
challenging to define a tractable search space and fitness function
for realistic, complex driving scenarios. As a future direction, it is
worth investigating how to elicit new safety requirements from
existing crash reports, traffic rules and on-road driving recordings.
Such requirements can be used to generate new fitness functions or
refine existing ones to augment the search space for more mission-
critical testing scenarios.Furthermore, as discussed in Section 4.1, all interview partici-
pants and 69% of survey participants are now working on multi-
module ADSs. These multi-module ADSs take multiple types of
sensor data as input. Yet the majority of test generation techniques
only generate road image data. Therefore, it is worth investigating
how to generate multi-modal sensor data for new driving scenarios.
6.3 Gaps and Future Directions for Need 2
Five papers on test selection and prioritization can be applied to
address the need of accelerating ADS testing [ 58,70,71,77,101].
These techniques can be used to remove redundant testing scenar-
ios and prioritize driving recordings that are more likely to expose
errors in an ADS. For example, when driving on a highway, the
ego-vehicle often goes straight on a specific lane at a constant speed.
Therefore, there is no need to replay the entire highway driving
recording but only unique driving scenarios during the recording,
such as lane cutting. All these technical papers use similarity-based
test metrics. Researchers can also investigate other kinds of metrics,
such as confidence-based metrics [ 32,68,72,99], to select or priori-
tize test cases. For example, DeepGini [ 32] applied Gini impurity
on confidence to measure the likelihood of misclassification on an
input, and prioritized test cases with high Gini scores. Furthermore,
instead of using test metrics, we also suggest that researchers con-
sider dynamic HD maps, which is an intermediate representation
of driving scenes used by modern ADSs. HD maps incorporate
the outputs of perception models and are then fed as input for the
following prediction and control modules. They can be used as a
holistic representation for driving scenes.
Since current ADS test prioritization techniques focus on end-
to-end (E2E) driving models or only the path planning module in
an ADS, there is an opportunity to develop new techniques for
multi-module ADSs. To account for multi-module architectures,
researchers may want to investigate how to combine test metrics
for inner model behaviors, such as neuron coverage and surprise
adequacy, and logic-based test metrics, such as path coverage, to
capture the interaction among multiple models, and interaction
between models and logic-based control modules. One possible
solution is to formulate this as a multi-objective optimization prob-
lem. A technique should search for a minimal set of test cases that
collectively maximize the coverage of individual ML models as
well as the path or dependency coverage of an ADS. For exam-
ple, a driving scene at a busy traffic intersection is likely to have
higher collective coverage at the model and system levels compared
with a highway scene with little traffic, since it involves multipleESEC/FSE ’22, November 14–18, 2022, Singapore, Singapore Guannan Lou, Yao Deng, Xi Zheng, Mengshi Zhang, and Tianyi Zhang
types of objects (e.g., pedestrians, vehicles, and traffic lights) that
may trigger multiple execution paths in an ADS and also trigger
multiple perception models. Given the large number of possible
test case combinations, it is worth investigating how to leverage
multi-objective optimization algorithms to efficiently search for an
optimal test set.
6.4 Gaps and Future Directions for Need 3
We have only identified two relevant SE papers [ 30,36] that ad-
dressed the need of constructing complex driving scenarios. Gambi
et al. [ 36] proposed an automated approach called AC3R that parses
crash reports to driving scenarios in a simulation environment.
Specifically, AC3R leverages NLP techniques to extract key infor-
mation (e.g., traffic participants and driving actions) from crash
reports and transform crash information to a Lua script to control
traffic participants and generate corresponding crash scenarios.
AC3R is limited to some simple crash scenarios like rear-end colli-
sion of two vehicles. In addition, as crash reports often follow rigid
narrative standards, AC3R may have a difficult time translating
free-form driving scene descriptions to an executable driving scene.
As future work, it is worth investigating new techniques in NLP and
CV to support automated construction of driving scenes from text
or video data. Du et al. [ 30] proposed a new modeling language that
describes specifications in various driving scenarios for the purpose
of formal verification. While this new language is not directly used
to construct new test cases, it can be adapted as a domain-specific
language (DSL) for test generation.
We also searched more broadly online and found some effort
in addressing this need from other research communities. The In-
telligent Vehicle and Programming Language communities has
proposed several higher-level DSLs to reduce the effort of mod-
eling objects and motions in a simulation platform [ 3,10,33,83].
For instance, Scenic [ 33] is a probabilistic programming language
which enables developers to generate complex traffic jam scenarios
with just a few lines of code. These newly designed DSLs are more
compact and flexible. We believe SE researchers can also contribute
to this line of research.
6.5 Gaps and Future Directions for Need 4
Among all four emerging needs, the need of tool support for data
labeling receives the highest importance rating in the survey study.
68% of survey participants considered this need as very important
or important. We have only identified one SE paper [ 58] that at-
tempts to address this need. Kim et al. [ 58] proposed to surprise
adequacy, a simple metric to quantify how surprising an input is to
a DNN, to guide the selection of new road images for labeling. The
result is promising — 30 to 50% of manual labeling cost can be saved
with negligible impact on evaluation accuracy. However, reducing
the amount of labels does not help improve label quality. Despite
less effort in this direction so far, we have noticed that there is an
increasing attention to the general problem of data labeling in the
SE community. A case study by Amershi et al. [ 4] recognized data
collection, cleaning, and labeling as crucial steps in the develop-
ment pipeline of machine learning applications. Other communities
such as database and computer vision have proposed several su-
pervised or semi-supervised learning techniques to automaticallylabel data [ 9,21,55,56,102]. We suggest that readers refer to Roh
et al. [ 85] for a detailed survey of data collection and labeling tools
built by other research communities. In the future, we believe it is
worthwhile developing more automated or semi-automated tools
such as [58] to support reducing data labeling efforts in ADSs.
Furthermore, SE researchers can also contribute to data cleaning
by developing input validation or outlier detection techniques on
driving data. Traditional data cleaning approaches need users to
provide a set of specific rules to determine constraints [ 22,43,84].
Such rules are easy to define on tabular data or text data. However,
it is challenging to define such rules for sensor data such as videos
and point clouds that involve complex driving scenes. The SE com-
munity has proposed many techniques to identify failure-inducing
inputs in software testing and debugging [ 23,42,44,76,89,104].
For example, Gulzar et al. [44] leveraged data provenance to trace
error propagation in big data systems and identify fault-inducing
data. It would be beneficial to investigate how to adapt or renovate
these techniques to identify low-quality data that leads to abnormal
driving behavior in ADSs.
7 THREATS TO VALIDITY
We discuss the threats to validity following the standard proposed
by Wohlin et al. [100].
External Validity. The external validity concerns about the
generalizability of our findings. To mitigate the threat to external
validity, we improved the diversity of the interviewees as much as
possible. As shown in Section 2.1, these 10 interviewees came from
10 different companies and had different responsibilities. Further-
more, to expand the scale of our research, we also conducted 100
surveys, and these respondents were in a variety of positions, and
came from various types of companies and organizations, as shown
in Section 2.2. In addition, ADS developed and tested by these inter-
viewees and questionnaire respondents were from level 1 to level
5 [25], which covered all levels of industrial ADS. Therefore, the
interviewees and survey respondents in this study can represent
the industrial participants of ADS testing.
Construct Validity. The construct validity concerns about the
design of our empirical study and literature review. To mitigate
this threat, for interviews, we first conducted two pilot interviews.
Based on feedback from these two pilot interviews, we refined
the interview guide and interview questions. In addition, to not
bias participants’ responses during the interview, we conducted
semi-structured interviews and asked open-ended questions, which
allowed for new ideas and questions to be brought up during the
discussion. Though the survey questions were mainly multiple-
choice questions, we allowed respondents to provide alternative
answers. Also open-ended questions were added in the survey for
respondents to supplement questions and answers not included
in the survey. In the literature review, we used broad keywords
to include as many relevant papers as possible. Additionally, with
manual filtering, we removed papers that were not relevant to au-
tonomous driving testing. Though in the literature review process,
we did not apply literature quality assessment as described in [ 57],
the quality of searched papers can be assured because they are all
published on high quality SE conferences and journals. We did not
create detailed data collection forms for searched papers, whichTesting of Autonomous Driving Systems: Where Are We and Where Should We Go? ESEC/FSE ’22, November 14–18, 2022, Singapore, Singapore
might affect the categorization of reviewed papers. The process of
data collection will be refined in the future work.
Internal Validity. The internal validity concerns about how
accurate participants’ responses may reflect their real needs and
whether there are other compounding factors that may affect the
results. Specifically, since interview participants were developers
from technology companies, some of them may not faithfully re-
veal the exact and most emergent needs from their team due to
confidentiality concerns. When designing the interview, we tried
our best to avoid conversations that may involve technical details
or make them feel uncomfortable to answer. In addition, before
each interview, we shared the interview guide with the participant
and informed them that our goal was to solicit high-level feedback
rather than technical details. To avoid mistakes and biases in the
thematic analysis step, the first and second authors carried out this
step separately. Then two authors continuously adjusted the extrac-
tion result, until the inter-rater agreements between the two coders
reached 0.85, in Cohen’s Kappa [ 97]. Also, the collected information
was regularly reported and discussed in the whole research team.
8 CONCLUSION
This paper presents an empirical study on the current practices and
needs of testing autonomous driving systems. Through an inter-
view study and a large-scale survey, we identified seven common
practices and four common needs of ADS practitioners. These find-
ings provide a deep understanding about how ADS practitioners
test autonomous driving systems in practice and what kinds of
tool support they find helpful. Furthermore, we did a literature
review of related ADS testing research from the SE community and
assessed to what extent existing work can address those industrial
needs. We found that, though one need, identifying possible corner
cases and unexpected driving scenarios , is widely investigated by
the SE community, the other three needs are under-investigated.
Furthermore, most existing work is designed for end-to-end driving
models, while multi-module driving systems are widely adopted
nowadays. Based on these gaps, we proposed four future direc-
tions to build better tool support for testing autonomous driving
systems. We believe there are many research opportunities for SE
researchers and therefore call for more attention and effort towards
these future directions.
9 ACKNOWLEDGEMENTS
This work is in part supported by an Australian Research Council
(ARC) Discovery Project (DP210102447), an ARC Linkage Project
(LP190100676), and a DATA61 project (Data61 CRP C020996).
REFERENCES
[1]Raja Ben Abdessalem, Shiva Nejati, Lionel C. Briand, and Thomas Stifter. 2018.
Testing vision-based control systems using learnable evolutionary algorithms.
InProceedings of the 40th International Conference on Software Engineering,
ICSE 2018, Gothenburg, Sweden, May 27 - June 03, 2018 , Michel Chaudron, Ivica
Crnkovic, Marsha Chechik, and Mark Harman (Eds.). ACM, 1016–1026. https:
//doi.org/10.1145/3180155.3180160
[2]Raja Ben Abdessalem, Annibale Panichella, Shiva Nejati, Lionel C Briand, and
Thomas Stifter. 2018. Testing autonomous cars for feature interaction failures
using many-objective search. In 2018 33rd IEEE/ACM International Conference
on Automated Software Engineering (ASE) . IEEE, 143–154.
[3]M. Althoff, M. Koschi, and S. Manzinger. 2017. CommonRoad: Composable
benchmarks for motion planning on roads. In IV. IEEE, 719–726.[4]S. Amershi, A. Begel, C. Bird, R. DeLine, H. Gall, E. Kamar, N. Nagappan, B.
Nushi, and T. Zimmermann. 2019. Software engineering for machine learning:
A case study. In ICSE . IEEE, 291–300.
[5] ApolloAuto. 2021. Apollo. https://bit.ly/2E3vWyo.
[6]Paolo Arcaini, Xiao-Yi Zhang, and Fuyuki Ishikawa. 2021. Targeting patterns of
driving characteristics in testing autonomous driving systems. In ICST . IEEE,
295–305.
[7] ASAM. 2021. ASAM OpenSCENARIO. https://bit.ly/3ya34Rm.
[8] Autoware-AI. 2020. autoware.ai. https://bit.ly/3gZ1gBS.
[9]B. Benato. 2021. Semi-automatic data annotation guided by feature space
projection. Pattern Recognition 109 (2021), 107612.
[10] P. Bender, J. Ziegler, and C. Stiller. 2014. Lanelets: Efficient map representation
for autonomous driving. In IV. IEEE, 420–425.
[11] R. Benekohal and J. Treiterer. 1988. CARSIM: Car-following model for simulation
of traffic in normal and stop-and-go conditions. Transportation research record
1194 (1988), 99–111.
[12] The National Transportation Safety Board. 2019. Preliminary Report Highway
Hwy18mh010. https://bit.ly/2N0SHuj.
[13] M. Bojarski, P. Yeres, A. Choromanska, K. Choromanski, B. Firner, Lawrence D.
Jackel, and U. Muller. 2017. Explaining How a Deep Neural Network Trained
with End-to-End Learning Steers a Car. CoRR abs/1704.07911 (2017).
[14] Markus Borg, Raja Ben Abdessalem, Shiva Nejati, François-Xavier Jegeden, and
Donghwan Shin. 2021. Digital twins are not monozygotic–cross-replicating adas
testing in two industry-grade automotive simulators. In ICST . IEEE, 383–393.
[15] M. Borg, C. Englund, K. Wnuk, B. Duran, C. Levandowski, S. Gao, Y. Tan, H.
Kaijser, H. Lönn, and J. Törnqvist. 2018. Safely entering the deep: A review of
verification and validation for machine learning and a challenge elicitation in
the automotive industry. arXiv preprint arXiv:1812.05389 (2018).
[16] V. Braun and V. Clarke. 2006. Using thematic analysis in psychology. Qualitative
research in psychology 3, 2 (2006), 77–101.
[17] R. Butler and G. Finelli. 1991. The infeasibility of experimental quantification
of life-critical software reliability. In Conference on Software for Citical Systems .
66–76.
[18] Alessandro Calò, Paolo Arcaini, Shaukat Ali, Florian Hauer, and Fuyuki Ishikawa.
2020. Generating avoidable collision scenarios for testing autonomous driving
systems. In ICST . IEEE, 375–386.
[19] Carla. 2021. Carla: Open-Source Simulator for Autonomous Driving Research.
https://bit.ly/3qE26qA.
[20] Jaganmohan Chandrasekaran, Yu Lei, Raghu Kacker, and D Richard Kuhn. 2021.
A Combinatorial Approach to Testing Deep Neural Network-based Autonomous
Driving Systems. In 2021 IEEE International Conference on Software Testing,
Verification and Validation Workshops (ICSTW) . IEEE, 57–66.
[21] L. Chen, S. Fidler, A. Yuille, and R. Urtasun. 2014. Beat the mturkers: Automatic
image labeling from weak 3d supervision. In CVPR . 3198–3205.
[22] X. Chu, I. Ilyas, S. Krishnan, and J. Wang. 2016. Data cleaning: Overview
and emerging challenges. In International Conference on Management of Data .
2201–2206.
[23] J. Clause, A. Orso, and ro. 2009. Penumbra: Automatically identifying failure-
relevant inputs using dynamic tainting. In ISSTA . 249–260.
[24] comma. ai. 2021. OpenPilot. https://bit.ly/3w099fI.
[25] SAE On-Road Automated Vehicle Standards Committee et al .2014. Taxonomy
and definitions for terms related to on-road motor vehicle automated driving
systems. SAE Standard J 3016 (2014), 1–16.
[26] M. Cordts, M. Omran, S. Ramos, T. Rehfeld, M. Enzweiler, R. Benenson, U. Franke,
S. Roth, and B. Schiele. 2016. The Cityscapes Dataset for Semantic Urban Scene
Understanding. In CVPR .
[27] DeepDrive. 2020. DeepDrive. https://bit.ly/2OTsheJ.
[28] Yao Deng, Xi Zheng, Tianyi Zhang, Chen Chen, Guannan Lou, and Miryung
Kim. 2020. An analysis of adversarial attacks and defenses on autonomous
driving models. In 2020 IEEE international conference on pervasive computing
and communications (PerCom) . IEEE, 1–10.
[29] S. Dey and S. Lee. 2021. Multilayered review of safety approaches for machine
learning-based systems in the days of AI. Journal of Systems and Software 176
(jun 2021), 110941.
[30] Dehui Du, Jiena Chen, Mingzhuo Zhang, and Mingjun Ma. 2021. Towards
Verified Safety-critical Autonomous Driving Scenario with ADSML. In 2021
IEEE 45th Annual Computers, Software, and Applications Conference (COMPSAC) .
IEEE, 1333–1338.
[31] F. Favar, S. Eurich, and N. Nader. 2018. Autonomous vehicles’ disengagements:
Trends, triggers, and regulatory limitations. Accident Analysis & Prevention 110
(2018), 136–148.
[32] Y. Feng, Q. Shi, X. Gao, J. Wan, C. Fang, and Z. Chen. 2020. DeepGini: prioritizing
massive tests to enhance the robustness of deep neural networks. In ISSTA . ACM,
177–188.
[33] D. Fremont, T. Dreossi, S. Ghosh, X. Yue, A. Sangiovanni-Vincentelli, and S.
Seshia. 2019. Scenic: a language for scenario specification and scene generation.
InPLDI . 63–78.ESEC/FSE ’22, November 14–18, 2022, Singapore, Singapore Guannan Lou, Yao Deng, Xi Zheng, Mengshi Zhang, and Tianyi Zhang
[34] Jonas Fritzsch, Tobias Schmid, and Stefan Wagner. 2021. Experiences from
Large-Scale Model Checking: Verifying a Vehicle Control System with NuSMV.
InICST . IEEE, 372–382.
[35] Alessio Gambi, Tri Huynh, and Gordon Fraser. 2019. Automatically recon-
structing car crashes from police reports for testing self-driving cars. In 2019
IEEE/ACM 41st International Conference on Software Engineering: Companion
Proceedings (ICSE-Companion) . IEEE, 290–291.
[36] Alessio Gambi, Tri Huynh, and Gordon Fraser. 2019. Generating effective test
cases for self-driving cars from police reports. In Proceedings of the 2019 27th
ACM Joint Meeting on European Software Engineering Conference and Symposium
on the Foundations of Software Engineering . 257–267.
[37] Alessio Gambi, Marc Mueller, and Gordon Fraser. 2019. Automatically testing
self-driving cars with search-based procedural content generation. In Proceed-
ings of the 28th ACM SIGSOFT International Symposium on Software Testing and
Analysis . 318–328.
[38] Alessio Gambi, Marc Müller, and Gordon Fraser. 2019. Asfault: Testing self-
driving car software using search-based procedural content generation. In 2019
IEEE/ACM 41st International Conference on Software Engineering: Companion
Proceedings (ICSE-Companion) . IEEE, 27–30.
[39] J. Garcia, Y. Feng, J. Shen, S. Almanee, Y. Xia, Chen, and Q. Alfred. 2020. A
comprehensive study of autonomous vehicle bugs. In ICSE . 385–396.
[40] S. Gibbs. 2017. Google sibling waymo launches fully autonomous ride-hailing
service. The Guardian 7 (2017).
[41] Christoph Gladisch, Thomas Heinz, Christian Heinzemann, Jens Oehlerking,
Anne von Vietinghoff, and Tim Pfitzer. 2019. Experience paper: Search-based
testing in automated driving control applications. In 2019 34th IEEE/ACM Inter-
national Conference on Automated Software Engineering (ASE) . IEEE, 26–37.
[42] R. Gopinath, A. Kampmann, er, N. Havrikov, E. Soremekun, and A. Zeller. 2020.
Abstracting failure-inducing inputs. In ISSTA . 237–248.
[43] V. Gudivada, A. Apon, and J. Ding. 2017. Data quality considerations for big
data and machine learning: Going beyond data cleaning and transformations.
International Journal on Advances in Software 10, 1 (2017), 1–20.
[44] M. Gulzar, Interl, M. i, S. Yoo, S. Tetali, T. Condie, T. Millstein, and M. Kim. 2016.
Bigdebug: Debugging primitives for interactive big data processing in spark. In
ICSE . IEEE, 784–795.
[45] J. Han and Z. Zhou. 2020. Metamorphic Fuzz Testing of Autonomous Vehicles.
InICSE Workshop . ACM, 380–385.
[46] Fitash Ul Haq, Donghwan Shin, Shiva Nejati, and Lionel C Briand. 2020. Com-
paring offline and online testing of deep neural networks: An autonomous car
case study. In ICST . IEEE, 85–95.
[47] Arthur Frederick Hasler. 2022. 60,000 Drivers Now Have Tesla Full Self Driving
(FSD)—What It Is & How To Get It. https://bit.ly/3t1zIST.
[48] Zhisheng Hu, Shengjian Guo, Zhenyu Zhong, and Kang Li. 2021. Disclosing the
Fragility Problem of Virtual Safety Testing for Autonomous Driving Systems. In
2021 IEEE International Symposium on Software Reliability Engineering Workshops
(ISSREW) . IEEE, 387–392.
[49] W. Huang, K. Wang, Y. Lv, and F. Zhu. 2016. Autonomous vehicles testing
methods review. In ITSC . IEEE, 163–168.
[50] Fuyuki Ishikawa. 2020. Testing and Debugging Autonomous Driving: Expe-
riences with Path Planner and Future Challenges. In 2020 IEEE International
Symposium on Software Reliability Engineering Workshops (ISSREW) . IEEE, xxxiii–
xxxiv.
[51] Gunel Jahangirova, Andrea Stocco, and Paolo Tonella. 2021. Quality metrics
and oracles for autonomous vehicles testing. In ICST . IEEE, 194–204.
[52] R Burke Johnson and Anthony J Onwuegbuzie. 2004. Mixed methods research:
A research paradigm whose time has come. Educational researcher 33, 7 (2004),
14–26.
[53] A. Joshi. 2020. Amazon’s machine learning toolkit: Sagemaker. In Machine
Learning and Artificial Intelligence . Springer, 233–243.
[54] S. Kato, S. Tokunaga, Y. Maruyama, S. Maeda, M. Hirabayashi, Y. Kitsukawa, A.
Monrroy, T. Ando, Y. Fujii, and T. Azumi. 2018. Autoware on board: Enabling
autonomous vehicles with embedded systems. In ICCPS . IEEE, 287–296.
[55] X. Ke, M. Zhou, Y. Niu, and W. Guo. 2017. Data equilibrium based automatic
image annotation by fusing deep model and semantic propagation. Pattern
Recognition 71 (2017), 60–77.
[56] X. Ke, J. Zou, and Y. Niu. 2019. End-to-end automatic image annotation based on
deep CNN and multi-label data augmentation. IEEE Transactions on Multimedia
21, 8 (2019), 2093–2106.
[57] Staffs Keele et al .2007. Guidelines for performing systematic literature reviews in
software engineering . Technical Report. Technical report, ver. 2.3 ebse technical
report. ebse.
[58] Jinhan Kim, Jeongil Ju, Robert Feldt, and Shin Yoo. 2020. Reducing dnn labelling
cost using surprise adequacy: An industrial case study for autonomous driving.
InProceedings of the 28th ACM Joint Meeting on European Software Engineering
Conference and Symposium on the Foundations of Software Engineering . 1466–
1476.
[59] Christian King, Lennart Ries, Christopher Kober, Christoph Wohlfahrt, and Eric
Sax. 2019. Automated function assessment in driving scenarios. In ICST . IEEE,414–419.
[60] Florian Klück, Yihao Li, Mihai Nica, Jianbo Tao, and Franz Wotawa. 2018. Using
ontologies for test suites generation for automated and autonomous driving
functions. In 2018 IEEE International symposium on software reliability engineer-
ing workshops (ISSREW) . IEEE, 118–123.
[61] F. Kluck, M. Zimmermann, F. Wotawa, and M. Nica. 2019. Genetic Algorithm-
Based Test Parameter Optimization for ADAS System Testing. In QRS. IEEE.
[62] Alessia Knauss, Jan Schroder, Christian Berger, and Henrik Eriksson. 2017.
Software-related challenges of testing automated vehicles. In 2017 IEEE/ACM
39th International Conference on Software Engineering Companion (ICSE-C) . IEEE,
328–330.
[63] P. Koopman and M. Wagner. 2016. Challenges in autonomous vehicle testing
and validation. SAE International Journal of Transportation Safety 4, 1 (2016),
15–24.
[64] Jérôme Leudet, François Christophe, Tommi Mikkonen, and Tomi Männistö.
2019. Ailivesim: An extensible virtual environment for training autonomous
vehicles. In 2019 IEEE 43rd annual computer software and applications conference
(COMPSAC) , Vol. 1. IEEE, 479–488.
[65] lgsvl. 2020. simulator. https://bit.ly/3dBDif1.
[66] Guanpeng Li, Yiran Li, Saurabh Jha, Timothy Tsai, Michael Sullivan, Siva Ku-
mar Sastry Hari, Zbigniew Kalbarczyk, and Ravishankar Iyer. 2020. AV-FUZZER:
Finding safety violations in autonomous driving systems. In ISSRE . IEEE, 25–36.
[67] Yihao Li, Jianbo Tao, and Franz Wotawa. 2020. Ontology-based test generation
for automated and autonomous driving functions. Information and software
technology 117 (2020), 106200.
[68] Z. Li, L. Zhang, J. Yan, J. Zhang, Z. Zhang, and T. H. Tse. 2020. PEACEPACT:
Prioritizing Examples to Accelerate Perturbation-Based Adversary Generation
for DNN Classification Testing. In QRS. IEEE.
[69] Siyuan Liu and Luiz Fernando Capretz. 2021. An Analysis of Testing Scenar-
ios for Automated Driving Systems. In 2021 IEEE International Conference on
Software Analysis, Evolution and Reengineering (SANER) . IEEE, 622–629.
[70] Chengjie Lu, Huihui Zhang, Tao Yue, and Shaukat Ali. 2021. Search-Based
Selection and Prioritization of Test Scenarios for Autonomous Driving Systems.
InInternational Symposium on Search Based Software Engineering . Springer,
41–55.
[71] Yixing Luo, Xiao-Yi Zhang, Paolo Arcaini, Zhi Jin, Haiyan Zhao, Fuyuki Ishikawa,
Rongxin Wu, and Tao Xie. 2021. Targeting Requirements Violations of Au-
tonomous Driving Systems by Dynamic Evolutionary Search. In 2021 36th
IEEE/ACM International Conference on Automated Software Engineering (ASE) .
IEEE, 279–291.
[72] W. Ma, M. Papadakis, A. Tsakmalis, M. Cordy, and Y. Traon. 2021. Test selection
for deep learning systems. ACM Transactions on Software Engineering and
Methodology (TOSEM) 30, 2 (2021), 1–22.
[73] Satoshi Masuda. 2017. Software testing design techniques used in automated
vehicle simulations. In 2017 IEEE International Conference on Software Testing,
Verification and Validation Workshops (ICSTW) . IEEE, 300–303.
[74] microsoft. 2021. AirSim. https://bit.ly/3qREI8Q.
[75] microsoft. 2021. UVoTT. https://bit.ly/3rEWxJO.
[76] G. Misherghi and Z. Su. 2006. HDD: Hierarchical delta debugging. In ICSE .
142–151.
[77] Galen E Mullins, Paul G Stankiewicz, R Chad Hawthorne, and Satyandra K
Gupta. 2018. Adaptive generation of challenging scenarios for testing and
evaluation of autonomous vehicles. Journal of Systems and Software 137 (2018),
197–215.
[78] C. Northcutt, A. Athalye, and J. Mueller. 2021. Pervasive label errors in test
sets destabilize machine learning benchmarks. arXiv preprint arXiv:2103.14749
(2021).
[79] California Department of Motor Vehicles. 2019. Autonomous Vehicle Collision
Reports - California DMV. https://bit.ly/3cPUcGC.
[80] California Department of Motor Vehicles. 2021. Disengagement Report. https:
//bit.ly/2NmXA1c.
[81] G. Paolacci, Ch, and J. ler. 2014. Inside the Turk: Understanding Mechanical
Turk as a participant pool. Current directions in psychological science 23, 3 (2014),
184–188.
[82] Z. Peng, J. Yang, T. H. Chen, and L. Ma. 2020. A first look at the integration of
machine learning models in complex autonomous driving systems: a case study
on Apollo. In FSE. ACM, 1240–1250.
[83] R. Queiroz, T. Berger, and K. Czarnecki. 2019. GeoScenario: An open DSL for
autonomous driving scenario representation. In IV. IEEE, 287–294.
[84] E. Rahm and H. Do. 2000. Data cleaning: Problems and current approaches.
IEEE Data Eng. Bull. 23, 4 (2000), 3–13.
[85] Y. Roh, G. Heo, and S. Whang. 2019. A survey on data collection for machine
learning: a big data-ai integration perspective. IEEE Transactions on Knowledge
and Data Engineering (2019).
[86] Rick Salay, Matt Angus, and Krzysztof Czarnecki. 2019. A safety analysis method
for perceptual components in automated driving. In ISSRE . IEEE, 24–34.
[87] M. Sharma, D. Rasmuson, B. Rieger, D. Kjelkerud, et al .2019. Labelbox:
The best way to create and manage training data. software, LabelBox. Inc,Testing of Autonomous Driving Systems: Where Are We and Where Should We Go? ESEC/FSE ’22, November 14–18, 2022, Singapore, Singapore
https://bit.ly/2TBLzYW (2019).
[88] Forrest Shull, Janice Singer, and Dag I. K. Sjøberg (Eds.). 2008. Guide to Advanced
Empirical Software Engineering . Springer. https://doi.org/10.1007/978-1-84800-
044-5
[89] C. Sun, Y. Li, Q. Zhang, T. Gu, and Z. Su. 2018. Perses: Syntax-guided program
reduction. In ICSE . 361–371.
[90] P. Sun, H. Kretzschmar, X. Dotiwalla, A. Chouard, V. Patnaik, P. Tsui, J. Guo, Y.
Zhou, Y. Chai, B. Caine, et al .2020. Scalability in perception for autonomous
driving: Waymo open dataset. In CVPR . 2446–2454.
[91] X. Sun, T. Zhou, G. Li, J. Hu, H. Yang, and B. Li. 2017. An empirical study on
real bugs for machine learning programs. In Asia-Pacific Software Engineering
Conference . IEEE, 348–357.
[92] Yun Tang, Yuan Zhou, Tianwei Zhang, Fenghua Wu, Yang Liu, and Gang Wang.
2021. Systematic testing of autonomous driving systems using map topology-
based scenario classification. In 2021 36th IEEE/ACM International Conference on
Automated Software Engineering (ASE) . IEEE, 1342–1346.
[93] Jianbo Tao, Yihao Li, Franz Wotawa, Hermann Felbinger, and Mihai Nica. 2019.
On the industrial application of combinatorial testing for autonomous driving
functions. In 2019 IEEE International Conference on Software Testing, Verification
and Validation Workshops (ICSTW) . IEEE, 234–240.
[94] F. Thung, S. Wang, D. Lo, and L. Jiang. 2012. An empirical study of bugs in
machine learning systems. In ISSRE . IEEE, 271–280.
[95] Yuchi Tian, Kexin Pei, Suman Jana, and Baishakhi Ray. 2018. Deeptest: Auto-
mated testing of deep-neural-network-driven autonomous cars. In Proceedings
of the 40th international conference on software engineering . 303–314.
[96] Pablo Valle. 2021. Metamorphic testing of autonomous vehicles: a case study
on simulink. In 2021 IEEE/ACM 43rd International Conference on Software Engi-
neering: Companion Proceedings (ICSE-Companion) . IEEE, 105–107.
[97] A. Viera, J. Garrett, et al .2005. Understanding interobserver agreement: the
kappa statistic. Fam med 37, 5 (2005), 360–363.
[98] P. Wang, X. Huang, X. Cheng, D. Zhou, Q. Geng, and R. Yang. 2019. The
apolloscape open dataset for autonomous driving and its application. IEEE
transactions on pattern analysis and machine intelligence (2019).
[99] Z. Wang, H. You, J. Chen, Y. Zhang, X. Dong, and W. Zhang. 2021. Prioritizing
Test Inputs for Deep Neural Networks via Mutation Analysis. In ICSE . IEEE,
397–409.[100] C. Wohlin, P. Runeson, M. Hst, M. Ohlsson, B. Regnell, and A. Wessln. 2012.
Experimentation in Software Engineering . Springer Publishing Company, Incor-
porated.
[101] Christian Wolschke, Thomas Kuhn, Dieter Rombach, and Peter Liggesmeyer.
2017. Observation based creation of minimal test suites for autonomous vehi-
cles. In 2017 IEEE International symposium on software reliability engineering
workshops (ISSREW) . IEEE, 294–301.
[102] B. Wu, W. Chen, P. Sun, W. Liu, B. Ghanem, and S. Lyu. 2018. Tagging like
humans: Diverse and distinct image annotation. In CVPR . 7967–7975.
[103] W. Xiang, P. Musau, A. Wild, D. M. Lopez, N. Hamilton, X. Yang, J. Rosenfeld,
and T. Johnson. 2018. Verification for machine learning, autonomy, and neural
networks survey. arXiv preprint arXiv:1810.01989 (2018).
[104] A. Zeller, Hildebr, and R. t. 2002. Simplifying and isolating failure-inducing
input. IEEE Transactions on Software Engineering 28, 2 (2002), 183–200.
[105] J. Zhang, M. Harman, L. Ma, and Y. Liu. 2020. Machine learning testing: Survey,
landscapes and horizons. IEEE Transactions on Software Engineering (2020).
[106] Mengshi Zhang, Yuqun Zhang, Lingming Zhang, Cong Liu, and Sarfraz Khur-
shid. 2018. Deeproad: Gan-based metamorphic testing and input validation
framework for autonomous driving systems. In 2018 33rd IEEE/ACM Interna-
tional Conference on Automated Software Engineering (ASE) . IEEE, 132–142.
[107] T. Zhang, C. Gao, L. Ma, M. Lyu, and M. Kim. 2019. An empirical study of
common challenges in developing deep learning applications. In ISSRE . IEEE,
104–115.
[108] Xudong Zhang, Yan Cai, and Zijiang Yang. 2020. A Study on Testing Autonomous
Driving Systems. In 2020 IEEE 20th International Conference on Software Quality,
Reliability and Security Companion (QRS-C) . 241–244. https://doi.org/10.1109/
QRS-C51114.2020.00048
[109] Y. Zhang, Y. Chen, S. Cheung, Y. Xiong, and L. Zhang. 2018. An empirical study
on TensorFlow program bugs. In ISSTA . 129–140.
[110] Xingyu Zhao, Valentin Robu, David Flynn, Kizito Salako, and Lorenzo Strigini.
2019. Assessing the safety and reliability of autonomous vehicles from road
testing. In ISSRE . IEEE, 13–23.
[111] Husheng Zhou, Wei Li, Zelun Kong, Junfeng Guo, Yuqun Zhang, Bei Yu, Ling-
ming Zhang, and Cong Liu. 2020. Deepbillboard: Systematic physical-world
testing of autonomous driving systems. In ICSE . IEEE, 347–358.