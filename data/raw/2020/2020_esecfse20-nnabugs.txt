DetectingNumericalBugsin NeuralNetwork Architectures
YuhaoZhang
Key Laboratory of HighConfidence
SoftwareTechnologies, MoE
Department of Computer Science and
Technology, Peking University
Beijing, PR China
yuhaoz@cs.wisc.eduLuyao Ren
Key Laboratory of HighConfidence
SoftwareTechnologies, MoE
Department of Computer Science and
Technology, Peking University
Beijing, PR China
rly@pku.edu.cnLiqianChen
Key Laboratory of Software
EngineeringforComplexSystems,
Collegeof Computer, National
Universityof DefenseTechnology
Changsha,PR China
lqchen@nudt.edu.cn
YingfeiXiong∗
Key Laboratory of HighConfidence
SoftwareTechnologies, MoE
Department of Computer Science and
Technology, Peking University
Beijing, PR China
xiongyf@pku.edu.cnShing-Chi Cheung
Department of Computer Science and
Engineering,The Hong Kong
University ofScience and Technology
Hong Kong, PR China
scc@cse.ust.hkTaoXie
Key Laboratory of HighConfidence
SoftwareTechnologies, MoE
Department of Computer Science and
Technology, Peking University
Beijing, PR China
taoxie@pku.edu.cn
ABSTRACT
Detectingbugsindeeplearningsoftwareatthearchitecturelevel
provides additional benefits that detecting bugs at the model level
does not provide. This paper makes the first attempt to conduct
staticanalysisfordetectingnumericalbugsatthearchitecturelevel.
Weproposeastaticanalysisapproachfordetectingnumericalbugs
in neural architectures based on abstract interpretation. Our ap-
proach mainlycomprisestwokindsofabstractiontechniques,i.e.,
one for tensors and one for numerical values. Moreover, to scale
up while maintaining adequate detection precision, we propose
two abstraction techniques: tensor partitioning and (elementwise)
affine relation analysis to abstract tensors and numerical values,
respectively. We realize the combination scheme of tensor parti-
tioningandaffinerelationanalysis(togetherwithintervalanalysis)
as DEBAR, and evaluate it on two datasets: neural architectures
withknownbugs(collectedfromexistingstudies)andreal-world
neural architectures. The evaluation results show that DEBAR out-
performs other tensor and numerical abstraction techniques on
accuracy without losing scalability. DEBAR successfully detects
all known numerical bugs with no false positives within 1.7ś2.3
seconds per architecture. On the real-world architectures, DEBAR
reports 529 warnings within 2.6ś135.4 seconds per architecture,
where 299warnings are true positives.
∗Corresponding Author
Permissionto make digitalor hard copies of allorpart ofthis work for personalor
classroom use is granted without fee provided that copies are not made or distributed
forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation
on the first page. Copyrights for components of this work owned by others than ACM
mustbehonored.Abstractingwithcreditispermitted.Tocopyotherwise,orrepublish,
topostonserversortoredistributetolists,requirespriorspecificpermissionand/ora
fee. Request permissions from permissions@acm.org.
ESEC/FSE ’20, November 8ś13, 2020, Virtual Event, USA
©2020 Associationfor Computing Machinery.
ACM ISBN 978-1-4503-7043-1/20/11...$15.00
https://doi.org/10.1145/3368089.3409720CCS CONCEPTS
·Software and its engineering →Formal software verifica-
tion; ·Computingmethodologies →Neuralnetworks .
KEYWORDS
NeuralNetwork, Static Analysis,Numerical Bugs
ACMReference Format:
YuhaoZhang, LuyaoRen,LiqianChen,YingfeiXiong, Shing-ChiCheung,
and Tao Xie. 2020. Detecting Numerical Bugs in Neural Network Architec-
tures.InProceedingsofthe28thACMJointEuropeanSoftwareEngineering
ConferenceandSymposium on the Foundationsof SoftwareEngineering(ES-
EC/FSE ’20), November 8ś13, 2020, Virtual Event, USA. ACM, New York, NY,
USA,12pages.https://doi.org/10.1145/3368089.3409720
1 INTRODUCTION
The use of deep neural networks (DNNs) within software systems
(which are named as DL software systems) is increasingly pop-
ular, supporting critical classification tasks such as self-driving,
facialrecognition,andmedicaldiagnosis.Constructionofsuchsys-
temsrequirestrainingaDNNmodelbasedonaneuralarchitecture
scriptedbyadeeplearning(DL)program1.Toeasethedevelopment
of DL programs, the developers popularly adopt various DL frame-
works such as TensorFlow. A neural architecture, i.e., a network
oftensorswithasetofparameters,iscapturedbyacomputation
graphconfigured todoone learning task. When theseparameters
are concretely bound after training based on the given training
dataset, the architecture prescribes a DL model, which has been
trainedfor aclassification task.
To avoid unexpected or incorrect behaviors in DL software sys-
tems, it is necessary to detect bugs in their neural architectures.
Although various approaches [ 8ś10,14,17,19ś21,23ś25,27ś32]
have been proposed to test or verify DL models, these approaches
do not address the needs of two types of stakeholders: (1) archi-
tecture vendors who design and publish neural architectures to be
1A DL program may specify multiple neural architectures, each responsible for an
assigned learning task. To ease the presentation, we assume that each DL program
performsasingletaskwithaneuralarchitectureunlessotherwisestatedinthispaper.
826ESEC/FSE ’20, November8–13,2020,VirtualEvent, USA Y. Zhang,L.Ren, L.Chen, Y. Xiong, S.Cheung,T. Xie
used by other users, and (2) developers who use neural architec-
tures to train and deploy a model based on the developers’ own
training dataset.
•Architecture vendorsneed to provide qualityassurance for
their neural architecture. It is inadequate for the vendors
to verifythe architecture withspecific instantiated models,
whichare dataset-dependent.
•Bugs in a neural architecture may manifest themselves into
failuresafterdevelopershavetrainedamodelforhours,days,
orevenweeks,causinggreatlossintimeandcomputation
resources[ 34].Thelosscanbepreventedifthesebugscanbe
detectedearlyatthearchitecturelevelbeforemodeltraining.
•FailurescanalsooccurwhendevelopersofaDLmodelneed
toretraintheirmodelsuponupdatesontrainingdata.These
updatescanfrequentlyhappenduringsoftwaresystemde-
velopment and deployment, e.g., when the new feedback
data iscollectedfrom users[ 33].
•Failures in DL models can be caused by a bug in the DL
architecture, low-qualitytraining data, incorrect parameter
settings,orotherissues.Itisnoteasyforthedevelopersto
localizethe bug.
Inthispaper,wepresentthefirstattempttoconductstaticanaly-
sisforbugdetectionatthearchitecturelevel.Specifically,wetarget
numerical bugs, an important category of bugs known to have cat-
astrophic consequences. Numerical bugsare challenging to detect,
often caused by complex component interactions and difficult to
be spottedoutduringcode review.
A neural architecture can contain numerical bugs that cause
serious consequences. Numerical bugs in a neural architecture
manifest themselves as numerical errors in the form of łNaNž,
łINFž, or crashes during training or inference. For example, when a
non-zero number is divided by zero, the result is łINFž, indicating
thatitisaninfinitenumber;whenzeroisdividedbyzero,theresult
is łNaNž, indicating that it is not a number. When a numerical
error occurs during training, the model trained using the buggy
neuralarchitecturebecomesinvalid.Anumericalbugthatmanifests
only when making inference is even more devastating: it can crash
thesoftwaresystemorcauseunexpectedsystembehaviorswhen
certaininputsare encounteredduringreal systemusage [ 24].
Detecting numerical bugs via testing is either too challenging
atthearchitecturelevelortoolateatthemodellevelasrevealed
inpreviousempirical studies[ 15,24,34].Testing anarchitecture
ischallengingaswecannotexecutethearchitecture.Testingthe
trained models is too late to discover the bugs occurring at the
training time as statedearlier.
Todetect numerical bugsat the architecture level, in this paper,
we propose to use static analysis because static analysis is able
tocoverthelargecombinatorialspaceimposedbythenumerous
parameters and possible inputs of a neural architecture. We pro-
pose a static analysis approach for detecting numerical bugs in
neural architectures based on abstract interpretation [ 4], which
mainly comprises two kinds of abstraction techniques, i.e., one
for tensors and one for numerical values. We study three tensor
abstractiontechniques:arrayexpansion,arraysmashing,andten-
sorpartitioning,aswellastwonumericalabstractiontechniques:interval abstraction and affine relation analysis. Among these tech-
niques,arrayexpansion,arraysmashing,andintervalabstraction
are adapted from existing abstraction techniques for imperative
programs[ 1,5].Inaddition,toachievescalabilitywhilemaintain-
ing adequate precision, we propose tensor partitioning to partition
tensorsandinfernumerical informationoverpartitions,basedon
our insight: many elements of a tensor are subject to the same
operations.Inparticular,representing(concrete)tensorelements
inapartitionasoneabstractelementunderappropriateabstract
interpretation can reduce analysis effort by orders of magnitude.
Motivated bythisinsight, tensorpartitioning initiallyabstractsall
elementsinatensorasoneabstractelementanditerativelysplits
eachabstractelementintosmalleroneswhenitsconcreteelements
go through different operations. Each abstract element represents
onepartitionofthetensor,associatedwithanumericalintervalthat
indicates the range of its concrete elements. Moreover, for the sake
of precision, besides interval analysis, we conduct affine relation
analysistoinfertheelementwiseaffineequalityrelationsamong
abstract elements representing partitions.
We evaluate the scalability and accuracy of our approach on
twodatasets:asetof9architectureswithknownnumericalbugs
collectedbyexistingstudies[ 24,34],andasetof48largereal-world
neuralarchitectures. In our evaluation,we designcomparative ex-
periments to study three tensor abstraction techniques and two
numerical abstraction techniques. We specifically name the imple-
mentation of the combination scheme of tensor partitioning and
affine relation analysis together with interval abstraction as DE-
BAR,beingreleasedasopensourcecode2.Intermsofscalability,
the evaluation results show that array expansion is unscalable, and
ittimesoutin33architectureswithatimebudgetof30minutes,
while other techniques are scalable and can successfully analyze
allarchitecturesin3minutes.Intermsofaccuracy,DEBARcould
achieve 93.0% accuracy with almost the same time performance
compared to array smashing (87.1% accuracy) and (sole) interval
abstraction(80.6%accuracy).Theseresultsdemonstratetheeffec-
tiveness of tensor partitioning and affine relation analysis together
withintervalabstraction.
In summary,this paper makes three main contributions:
•Astudyofastaticanalysisapproachfornumericalbugde-
tection in neuralarchitectures, with three abstraction tech-
niques for abstracting tensors and two for abstracting nu-
mericalvalues.
•Twoabstractiontechniquesdesignedforanalyzingneural
architectures: tensor partitioning (for abstracting tensors)
and (elementwise) affine relation analysis (for inferring nu-
mericalrelations among tensorpartitions).
•Anevaluationon9buggyarchitecturesin48real-worldneu-
ralarchitectures,demonstratingtheeffectivenessofDEBAR.
2 OVERVIEW
In this section, we explain how our approach detects numerical
bugs with an example in Listing 1. The example is modified from a
2https://github.com/ForeverZyh/DEBAR
827DetectingNumerical Bugs in Neural Network Architectures ESEC/FSE ’20, November8–13,2020,VirtualEvent, USA
real-worldcodesnippetthatcreatesrectanglesandcalculatesthe
reciprocals oftheirareas3.
1# Input:
2# center: 2*100-shape tensor whose elements in [-1, 1]
3# offset: 2*100-shape tensor whose elements in [0, 2]
4
5# Create 100 rectangles.
6bottomLeft = center - offset
7topRight = center + offset
8rectangle = tf.concat([bottomLeft, topRight], axis=1)
9
10# Calculate the reciprocal of their areas.
11bottom, left, top, right = tf.split(rectangle,
num_or_size_splits=4, axis=1)
12width = right - left
13height = top - bottom
14area = width * height
15scale = tf.reciprocal(area)
Listing 1:ACodeSnippet ofaMotivatingExample
The program consists of two parts. The first part defines 100
rectangles, each by a central point and an offset vector. Input vari-
ablecenterrepresents 100 central points, where each point is a
2-elementvectorof32-bitfloats.Similarly, offsetrepresents100
offset vectors. Then from the central points and the offsets, the
bottom left points and the top right points are calculated (Lines
6ś7), and are concatenated into a 4*100 tensor to create rectangles
(Line 8). The second part calculates the reciprocals of the areas.
First, from rectangle ,the bottom, left, top,and right coordinates
are extracted (Line 11), each being a 1*100-shape tensor. The ar-
eas of rectangles are then calculated (Lines 12ś14) followed by the
calculation of their reciprocals (Line 15). This program contains
a numerical bug that when any offset vector has an element of
zero, the corresponding area becomes zero and the value of scale
becomes NaN.
To capture the bug, an ideal way is to statically consider all
possiblevaluesof centerandoffset,andcheckwhetheranyof
thevalueswouldresult inazero area.Abstract interpretation[ 4]
isaneffectivesolutiontostaticallyconsiderallpossiblevaluesof
variables. Itanalyzestheoriginalprogram viaanabstractdomain,
where eachabstractvaluerepresentsaset ofconcretevalues.To
apply abstract interpretation to our problem, the key is how to
abstractaneuralarchitecture.Givenaneuralarchitecture,weneed
to consider mainly two aspects. First, the numerical values and
arithmetic computations need to be abstracted. Second, the tensors
need to be abstracted. We first discuss three abstraction techniques
adaptedfromexistingworkforanalyzingimperativeprograms,and
thendescribetwonewtechniquesthatweproposeforanalyzing
neuralarchitectures.
2.1 Interval Abstraction
Interval abstraction [ 5] is a popular abstract interpretation tech-
nique for abstracting numerical values, where each scalar variable
𝑣is represented by an interval, indicating the lower bound and
theupperbound.Theseintervalsarethencalculatedbymapping
the standard operations into interval arithmetic. As a result, the
following showsthe calculationsperformedbythe analysis.
3https://github.com/tensorflow/models/blob/13e7c85d521d7bb7cba0bf7d743366f
7708b9df7/research/object_detection/box_coders/faster_rcnn_box_coder.py#L80center: allelements have [−1,1]
offset: allelements have [0,2]
bottomLeft : allelements have [−3,1]
topRight : allelements have [−1,3]
rectangle :the first two elements in each row have
[−3,1], and the last two elements in each
rowhave [−1,3]
bottom,left: allelements have [−3,1]
top,right: allelements have [−1,3]
width,height: allelements have [−2,6]
area: allelements have [−12,36]
To detect numerical bugs, one can predefine the safe conditions
for various operations, e.g., by restricting the argument not to take
a zero value when calling reciprocal . Since zero is included in
the interval of any element in area, a potential numerical bug is
detected.
Thefirsttypeofimprecision isintroducedbyintervalabstrac-
tion. In the preceding example, we can conclude from the interval
ofoffsetthat the elements in areaare within interval [0,16],
which is smaller than the inferred interval [−12,36]. Since both
bottomLeft andtopRight arecalculatedfrom center,theeffect
ofcenteris nullified when calculating widthandheight. How-
ever, such information is lost after the values have been abstracted
intointervals.The imprecision mayleadto false alarms. Consider
a situation that the elements in offsetcontain values within in-
terval[1,2], where no numerical error shouldbe triggered. When
analyzing using the interval abstraction, we would get [−18,36]
for all elements in area, leading to a false alarm of numerical bugs.
2.2 ArrayExpansion
Array expansion [ 1] is a basic technique for abstractingan array
in an imperative program to an abstract domain: the elements
in the array are one-to-one mapped to the abstract domain, and
noabstractionisperformed.Mappingarrayexpansionfortensor
abstraction with interval abstraction, we can also directly map the
elements inatensorone-to-oneto ranges inthe abstract domain.
Scalability is the main problem of array expansion. The reason
isthatweneedtorecordanintervalforeachelementinatensor,
and in the motivating example, we need to record 200 intervals
forcenterand 200 intervals for offset, substantially affecting
scalability.Asshownbyourevaluationlater,analyses using array
expansion time out for most real-world models with a time budget
of30 minutes.
2.3 ArraySmashing
Arraysmashing[ 1]isanalternativeabstractiontechniquethatuses
oneabstractelementtorepresentallelementsinatensor.Inthis
way,the number ofabstract elementsisgreatly reduced.Mapping
arraysmashingfortensorabstractionwithintervalabstraction,we
use one range to cover allelements inatensor.
rectangle : [−3,3]
bottom,left,top,right:[−3,3]
width,height: [−6,6]
area: [−36,36]
The second type of imprecision is introduced by array smash-
ing. In the preceding example, the intervals of center,offset,
bottomLeft ,andtopRight remainthesame.Nevertheless,array
828ESEC/FSE ’20, November8–13,2020,VirtualEvent, USA Y. Zhang,L.Ren, L.Chen, Y. Xiong, S.Cheung,T. Xie
smashing can get the interval of areaas[−36,36], which is less
precise than what array expansion gets. In fact,when using array
smashing, a warning would be reported for any input intervals
as the difference between bottomLeft andtopRight disappears
when they are concatenatedinto rectangle .
2.4 TensorPartitioningandAffine Relation
Analysis
To scale up while maintaining adequate precision, we propose two
techniques for abstracting tensors and numerical values, respec-
tively. The first technique is tensor partitioning , which allows a
tensortobesplitintomultiplepartitions,whereeachpartitionis
abstracted as a summary variable, and we maintain interval ranges
forsuchsummaryvariables.Thesecondtechniqueis affinerelation
analysis,whichmaintainsaffinerelationsbetweenpartitionsand
makesuse ofthis relation to achieve more precise analysis.
Specifically, to support tensor partitioning, we maintain the set
ofindexesforeachpartition.Weuse I𝐴todenotetheindexranges
of the partition 𝐴, and use ∗to denote all included indexes in a
dimension.Forexample,thefollowingshowsthecalculationpro-
cess of the preceding example in tensor partitioning, where the
names in uppercase represent partitions. Tensor centerhas one
partition 𝐶including all its concrete elements. To support affine
relationanalysis,weintroduceasymbolicsummaryvariable 𝑎to
denote each partition 𝐴(i.e.,𝛼(𝐴)=𝑎), whose name is in lower-
case. We use 𝜎(𝑎)to denote its corresponding interval. With these
summary variables, we maintain affine equality relations among
thesesummary variables.
Inthemotivatingexample,thepartition 𝐶correspondstoasym-
bolic summary variable 𝑐, whose interval range is 𝜎(𝑐)=[−1,1].
Similarly, offsetalso has one partition 𝑂corresponding to an
expression 𝑜, where the interval 𝑜is[0,2]. Next,bottomLeft is
calculatedfrom centerandoffset.Sinceboth centerandoffset
haveonepartition, bottomLeft alsohasonepartition 𝐵𝐿,which
correspondstoexpression 𝑐−𝑜thatiscalculatedfrom 𝛼(𝐶)−𝛼(𝑂).
Followingthisprocess,wecancalculatethepartitionsandmaintain
theiraffine equalityrelations.
1.center:I𝐶=∗×∗,𝛼(𝐶)=𝑐,
2. 𝜎(𝑐)=[−1,1]
3.offset:I𝑂=∗×∗,𝛼(𝑂)=𝑜,
4. 𝜎(𝑜)=[0,2]
5.bottomLeft :I𝐵𝐿=∗×∗,𝛼(𝐵𝐿)=𝛼(𝐶) −𝛼(𝑂)=𝑐−𝑜
6.topRight :I𝑇𝑅=∗×∗,𝛼(𝑇𝑅)=𝛼(𝐶) +𝛼(𝑂)=𝑐+𝑜
7.rectangle :I𝑅1=[0..1] ×∗,𝛼(𝑅1)=𝛼(𝐵𝐿)=𝑐−𝑜
8. I𝑅2=[2..3] ×∗,𝛼(𝑅2)=𝛼(𝑇𝑅)=𝑐+𝑜
9.bottom:I𝐵=∗×∗,𝛼(𝐵)=𝛼(𝑅1)=𝑐−𝑜
10.left: I𝐿=∗×∗, 𝛼(𝐿)=𝛼(𝑅1)=𝑐−𝑜
11.top: I𝑇=∗×∗,𝛼(𝑇)=𝛼(𝑅2)=𝑐+𝑜
12.right: I𝑅=∗×∗,𝛼(𝑅)=𝛼(𝑅2)=𝑐+𝑜
13.width: I𝑊=∗×∗,𝛼(𝑊)=𝛼(𝑅) −𝛼(𝐿)=2𝑜
14.height:I𝑅=∗×∗,𝛼(𝐻)=𝛼(𝑇) −𝛼(𝐵)=2𝑜
15.area: I𝐴=∗×∗,𝛼(𝐴)=𝑎,
16. 𝜎(𝑎)=2𝜎(𝑜) ×2𝜎(𝑜)=[0,16]
Thecalculationisdissimilartointervalabstractionwitharray
smashingintwowaysfortheexample.Thefirstdifferenceincal-
culation occurs at Lines 7 and 8. Since rectangle is concatenated
from two tensors, we keep rectangle as two partitions, 𝑅1and𝑅2, each corresponding to an argument. In this way, we overcome
thesecondtypeofimprecisionbroughtbyarraysmashingwhile
keeping the number of abstract elements small. When splitting
rectangle intobottom,left,top, andwidth, we can get the pre-
ciseintervalsforthesetensorsfromthecorrespondingpartitions
inrectangle .
The second difference in calculation occurs at Line 13. When
calculating 𝑤𝑖𝑑𝑡ℎ, we make use of the affine equality relations
amongpartitionsof 𝑅and𝐿,andthuswecanpreciselyinferthat
𝑤𝑖𝑑𝑡ℎ=2𝑜rather than onlyan imprecise interval for 𝑤𝑖𝑑𝑡ℎ. Simi-
larly,theintervalfor heightisalsoprecise.Inthisway,weover-
comethefirsttypeofimprecisionduetointervalabstraction.Finally,
wecalculate areabasedon widthandheight.Sincetheoperation
of 2𝑜×2𝑜is no longer linear, we cannot get any affine equality
relationsfor area.Hence,weintroduceasummaryvariable 𝑎for
the whole area, and compute its interval range. Then, for areawe
getan interval [0,16],which isthe ground-truth interval rangeof
area.
3 APPROACH
Inthissection,wefirstintroducethepreliminariesofabstractin-
terpretationandtwobasicnumericalabstractdomains,theinterval
abstract domain, and the affine equality abstractdomain. We then
describe our abstractionfor neural architectures using tensorpar-
titioning(forabstractingtensors)andnumericalabstractions,i.e.,
combining intervals with affine equalities (for abstracting numeri-
cal values). We then show how to abstract tensor operations under
two abstraction techniques, tensor partitioning and affine relation
analysis (as well as interval analysis), designed for neural archi-
tectures.Wealsodiscusstheinitialintervalsforinputrangesand
parameterranges.
3.1 Preliminaries
3.1.1 BasicsofAbstractInterpretation. Inabstractinterpretation[ 4],
concrete properties are described in the concrete domain Cwith a
partialorder ⊆,andabstractpropertiesaredescribedintheabstract
domainAwithapartialorder ⊑.Wesaythatthecorrespondence
betweenconcretepropertiesandabstractpropertiesisaGaloiscon-
nection⟨C,⊆⟩𝛾
⇆
𝛼⟨A,⊑⟩with an abstraction function 𝛼:C↦→A
andaconcretizationfunction 𝛾:A↦→Csatisfying
∀𝑐∈C,∀𝑎∈A.𝛼(𝑐) ⊑𝑎⇔𝑐⊆𝛾(𝑎).
To infer the value range for variables in a DL program, we need
tocomputethepossiblesetsofvaluesthateachvariablecantake.
Wedefinetheconcretedomain Cof𝑛variablesas P(R𝑛),wherean
elementis asetof 𝑛-elementvectors denotingthepossiblevalues
that𝑛variablescantake.Thepartialorderin Cisthesubsetrelation
⊆over sets.
3.1.2 AbstractDomainofIntervals. Theabstractdomainofinter-
vals [3]AIisdefinedas
AI≜{([𝑙1,𝑢1],...,[𝑙𝑛,𝑢𝑛]) |𝑙,𝑢∈R𝑛}.
An element inAIcan be seen as a pair of two vectors (𝑙,𝑢), where
[𝑙𝑖,𝑢𝑖]denotes the lower bound and upper bound of the values
that the𝑖-th variable may take. Given two elements 𝑎1,𝑎2∈AI,
829DetectingNumerical Bugs in Neural Network Architectures ESEC/FSE ’20, November8–13,2020,VirtualEvent, USA
we say𝑎1⊑𝑎2if both have 𝑛intervals and each interval in 𝑎1is a
sub-intervalofthe intervalinthe corresponding position in 𝑎2.
The abstraction function 𝛼Iof an element 𝑐∈Cis defined as
𝛼I(𝑐)=([𝑙𝑐
1,𝑢𝑐
1],...,[𝑙𝑐𝑛,𝑢𝑐𝑛]),where
𝑙𝑐
𝑖=min𝑥∈𝑐(𝑥𝑖), 𝑢𝑐
𝑖=max𝑥∈𝑐(𝑥𝑖),1≤𝑖≤𝑛.
Theconcretizationfunction 𝛾Iofanelement 𝑎∈AIisdefinedas
𝛾I(𝑎)={𝑥∈R𝑛| ∀𝑖∈ [1,𝑛].𝑥𝑖∈ [𝑙𝑎
𝑖,𝑢𝑎
𝑖]},
where[𝑙𝑎
𝑖,𝑢𝑎
𝑖]isthe intervalrange ofthe 𝑖-th variable of 𝑎.
Itiseasytoseethattheconcretedomain ⟨C,⊆⟩andtheinterval
abstractdomain ⟨AI,⊑I⟩formtheGalois connection.More details
can be foundinthe publication byCousotandCousot[ 3].
3.1.3 Abstract Domain of Affine Equalities. As discussed in Sec-
tion2, we alsomaintain affine relations among variables ina DL
program inthe form of
/summationdisplay.1
𝑖>0𝜔𝑖𝑥𝑖=𝜔0, (1)
where𝑥𝑖’s are variables and 𝜔𝑖’s are constant coefficients, inferred
automaticallyduringthe analysis.
The abstract domainofaffine equalities [ 16]AEisdefinedas
AE≜{(A,𝑏) |A∈R𝑚×𝑛,𝑏∈R𝑚,𝑚>0},
where amatrix Aand acolumn vector 𝑏define the affine space of
𝑛variables. An element in AEconstrains variables 𝑥∈R𝑛by an
equation A𝑥=𝑏describing the possible set of values that 𝑥can
take.Furthermore,tohaveacanonicalform,werequire (A,𝑏)to
be inthe reducedrowechelonform [ 16].
The abstractionfunction 𝛼Eofan element 𝑐∈Cisdefinedas
𝛼E(𝑐)= 
(A,𝑏),(A,𝑏)isinreducedrowechelonform,and
A𝑥=𝑏holdsfor all 𝑥in𝑐
⊤,if𝑐isthe wholespace
⊥,otherwise .
The concretization function 𝛾Eof an element 𝑎♯E=(A,𝑏) ∈AE
isdefinedas
𝛾E((A,𝑏))={𝑥∈R𝑛|A𝑥=𝑏}.
Theconcretedomain ⟨C,⊆⟩andtheaffine equalityabstractdo-
main⟨AE,⊑E⟩form the Galoisconnection
⟨C,⊆⟩𝛾E
⇆
𝛼E⟨AE,⊑E⟩.
Thedetailsaboutthedomainoperations(includingmeet,join,
inclusion test, etc.) of the affine equality abstract domain can be
foundinthepublication byKarr [16].Wedo notneedawidening
operation for thedomain of affine equalities becausethe lattice of
affineequalitieshasfiniteheight,andthenumberofaffineequal-
ities while analyzing a program is decreasing until reaching the
dimensionofthe affine spaceinthe program.
3.2 AbstractionforNeuralArchitectures
We use tensor partitioning and interval abstraction with affine
equalityrelation to abstract tensorsinneuralarchitectures.3.2.1 TensorPartitioning. AsdiscussedinSection 2,weintroducea
new granularity of array abstraction, named tensorpartitioning ,
which is a form of array partitioning (also named array segmen-
tation) [6,12,13] but tailored for tensor operations: we partition
atensor𝐴intoasetofdisjointpartitions {𝐴1,𝐴2,...,𝐴𝑛}where
each partition 𝐴𝑖is a sub-tensor of 𝐴. The number of partitions
of𝐴is denoted as N𝐴. The set of array indexes of the cells from
partition 𝐴𝑖iscontinuous in 𝐴anddefinedbyCartesianproducts
of index intervals for all dimensions, denoted as I𝐴𝑖. Note that the
indexes in I𝐴𝑖are indexes of the corresponding elements in ten-
sor𝐴,whilewesometimesuse I𝐴𝑖.𝑠ℎ𝑎𝑝𝑒torepresenttheindexes
of the corresponding elements in sub-tensor 𝐴𝑖, where𝐴𝑖.𝑠ℎ𝑎𝑝𝑒
denotes a tuple of integers giving the size of the sub-tensor 𝐴𝑖
along each dimension. In our motivating example, 𝑅2is a parti-
tionofrectangle ,andI𝑅2=[2..3] × [0..99],𝑅2.𝑠ℎ𝑎𝑝𝑒=(2,100),
I𝑅2.𝑠ℎ𝑎𝑝𝑒=[0..1]×[0..99].Forclarity,weintroduceanotionof par-
titioning positions for each dimension to denote the indexes where
we partition the tensor inthat dimension. Index 𝑖is a partitioning
position for a tensor 𝐴in dimension 𝑝iff the element 𝐴[𝑖]and the
element𝐴[𝑖+1]in dimension 𝑝belong to different partitions. It
is worth mentioning that the partitioning positions are easier to
infer for DNN implementations than for regular programs (e.g.,
C programs) [ 6,12,13], because the shapes of (sub-)tensors are
usually determined syntactically (often specified by parameters of
tensor operations) so that we know the exact boundary of each
partition,e.g., tf.concatandtf.splitinour motivatingexample.
Afterpartitioning,foreachpartition 𝐴𝑖,weintroduceanabstract
summary variable 𝑎𝑖to subsume all the elements in 𝐴𝑖, denoted
as𝛼(𝐴𝑖)=𝑎𝑖. Note that in this paper, we use lowercase letters
todenotethesummaryvariablesofpartitions(sub-tensors)while
uppercaseletters to denote tensors.Toperformstaticanalysis,we
maintainnumericalrelationsamongsummaryvariablesofparti-
tions,withdetails describedinthe nextsubsection.
3.2.2 IntervalAbstractionwithAffineEqualityRelation. Wecom-
binetheintervalabstractionandaffineequalityrelationabstraction
as our numerical abstraction to infer the value range for scalar
variables in the DL programs and also for those auxiliary abstract
summaryvariablesintroducedbytensorpartitioning.Wecoulduse
relational numericalabstractdomains (such as polyhedra) toinfer
(inequality) relations. However, because many tensor operations
induce affine equality relations, in this paper, we consider only the
affineequalityrelationsamongvariables.Inaddition,affineequality
relations are cheap to infer, and thus are fit to analyze large DNNs.
Furthermore, because ReLUoperations are widely used in DNN
implementations,foreachvariable 𝑎,weintroduce 𝑎ReLUtodenote
the resulting variable of ReLU(𝑎), i.e.,𝑎ReLU=𝑚𝑎𝑥(0,𝑎). Consider-
ing the way of using ReLUoperations in DNN implementations, in
this paper,we maintain only the affine equality relations between
avariable 𝑏andtheReLUresultofanothervariable 𝑎ofthesame
shape (while 𝑎,𝑏may be the summary variables of partitions from
differenttensors),inthe form of
𝑏−𝑎𝑅𝑒𝐿𝑈=0,
whichcanalsobeexpressedintheformofEq 1.Additionally,the
ReLUoperation has aproperty
ReLU(𝑎) −ReLU(−𝑎) −𝑎=0,∀𝑎∈R.
830ESEC/FSE ’20, November8–13,2020,VirtualEvent, USA Y. Zhang,L.Ren, L.Chen, Y. Xiong, S.Cheung,T. Xie
We can utilize this property for better analysis precision by (1)
addingan additionalequality
𝑎ReLU−𝑎−ReLU−𝑎=0,
where𝑎−ReLUdenotes the result of ReLU(−𝑎); (2) adding an addi-
tionalequality
𝑐ReLU−𝑎−ReLU=0
for every equalityinthe form of 𝑐=−𝑎.
3.2.3 AbstractDomain forNeuralArchitectures.
Definition 3.1. The abstract domain for Tensor partitioning and
Intervalabstraction withaffine EqualityrelationATIEis defined as
ATIE≜{(P,𝑎♯I,𝑎♯E) |𝑎♯I∈AI,𝑎♯E∈AE},
whereP={𝐴1,...,𝐴𝑛}is the set of the disjoint partitions of the
tensors and 𝑎♯I,𝑎♯Eare the numerical abstract elements over the 𝑛
summary variables corresponding to the partitions Pin the interval
domainAIand theaffineequalitydomain AE,respectively.
Definition3.2. Theconcretizationfunction 𝛾TIEofanelement
𝑎♯=(P,𝑎♯I,𝑎♯E) ∈ATIEis defined as
𝛾TIE(𝑎♯)=/braceleftbigg
AP={𝐴1,...,𝐴𝑛}∧∀(𝑗1,..., 𝑗𝑛) ∈𝐽.
(𝐴1[𝑗1],...,𝐴𝑛[𝑗𝑛]) ∈ (𝛾I(𝑎♯I) ∩𝛾E(𝑎♯E))/bracerightbigg
,
where𝐴isthe tensor constructedby its partitions P={𝐴1,...,𝐴𝑛}
and
𝐽={(𝑗1,..., 𝑗𝑛) | ∀𝑖∈ [1,𝑛].𝑗𝑖∈I𝐴𝑖.𝑠ℎ𝑎𝑝𝑒}.
3.3 AbstractingTensorOperations
Wenextshowhowtoconstructabstractoperationsbasedontensor
partitioning and affine relation analysis (together with interval
analysis)for analyzingthree commontensor operationsin neural
architectures. We provide the construction for other operations in
our DEBAR open sourcecode.
Toeasethepresentation,weillustrateourapproachusingvectors
(one-dimensional tensors) and matrices (two-dimensional tensors).
Our approach isgeneralizable to multi-dimensional tensors.
3.3.1 AdditionandSubtraction. Tensoradditionandsubtraction,
intheformof 𝐶=𝐴±𝐵,taketwoinputtensors 𝐴,𝐵withthesame
shape to calculatethe resultingtensor 𝐶.
Since the input tensors 𝐴and𝐵may not be partitioned in the
same way, weshould first align thepartitions of 𝐴and thoseof 𝐵,
such that ∀𝑖∈ [1,N𝐴].I𝐴𝑖=I𝐵𝑖whereN𝐴denotes the number of
partitions of 𝐴and𝐵after aligned. To align the partitions of 𝐴and
those of𝐵, for each dimension, we take the set union of the two
partitioningpositionsasthenewsetofpartitioningpositionsfor
𝐴and𝐵.Then,weputthealignedsetofpartitioningpositionsas
that for the resultingtensor 𝐶,such that 𝐶isalignedwith 𝐴,𝐵.
After that, for each partition 𝐶𝑖, we compute the interval range
for its summary variable by
𝜎(𝑐𝑖)=𝜎(𝑎𝑖) ±𝜎(𝑏𝑖).
Furthermore, for tensor addition and subtraction, we maintain the
elementwise affine equality relations among partitions of 𝐴,𝐵,𝐶.
For eachpartition 𝐶𝑖,we have
𝑐𝑖=𝑎𝑖±𝑏𝑖𝑖∈ {1,...,N𝐶},
Figure 1:Concatenating Tensors(Horizontally)
whichmeans
∀𝑗∈I(𝐶𝑖.𝑠ℎ𝑎𝑝𝑒).𝐶𝑖[𝑗]=𝐴𝑖[𝑗] ±𝐵𝑖[𝑗]𝑖∈ {1,...,N𝐶}.
Forexample,supposethatboththeone-dimensionaltensors 𝐴
and𝐵are partitionedintotwopartitions,and
I𝐴1=[0..2] ∧I𝐴2=[3..9] ∧I𝐵1=[0..5] ∧I𝐵2=[6..9]∧
𝛼(𝐴1)=𝑎1∧𝜎(𝑎1)=1∧𝛼(𝐴2)=𝑎2∧𝜎(𝑎2)=[2,3]∧
𝛼(𝐵1)=𝑏1∧𝜎(𝑏1)=4∧𝛼(𝐵2)=𝑏2∧𝜎(𝑏2)=[5,6].
After aligningpartitionsof 𝐴and𝐵,we have
I𝐴1=[0..2] ∧I𝐴2=[3..5] ∧I𝐴3=[6..9]∧
I𝐵1=[0..2] ∧I𝐵2=[3..5] ∧I𝐵3=[6..9]∧
𝛼(𝐴1)=𝑎1∧𝜎(𝑎1)=1∧𝛼(𝐴2)=𝑎2∧𝜎(𝑎2)=[2,3]∧
𝛼(𝐴3)=𝑎3∧𝜎(𝑎3)=[2,3] ∧𝛼(𝐵1)=𝑏1∧𝜎(𝑏1)=4∧
𝛼(𝐵2)=𝑏2∧𝜎(𝑏2)=4∧𝛼(𝐵3)=𝑏3∧𝜎(𝑏3)=[5,6].
After𝐶=𝐴±𝐵,for𝐶,we have:
I𝐶1=[0..2] ∧I𝐶2=[3..5] ∧I𝐶3=[6..9]∧
𝛼(𝐶1)=𝑐1∧𝜎(𝑐1)=1±4∧𝛼(𝐶2)=𝑐2∧𝜎(𝑐2)=[2,3] ±4
∧𝛼(𝐶3)=𝑐3∧𝜎(𝑐3)=[2,3] ± [5,6]∧
𝑐𝑖=𝑎𝑖±𝑏𝑖𝑖={1,2,3}.
3.3.2 Concatenate. InDNNimplementations,tensors canbecon-
catenated along certain dimension 𝑝. An assignment statement
𝐶=𝐶𝑜𝑛𝑐𝑎𝑡𝑒𝑛𝑎𝑡𝑒 (𝐴,𝐵,𝑝)denotes that two input tensors 𝐴and𝐵
areconcatenatedtoformanoutputtensor 𝐶alongdimension 𝑝.For
example,Figure 1showsthattwotwo-dimensionaltensors 𝐴and𝐵
areconcatenatedtoformatensor 𝐶alongdimension0(rows).To
handle the Concatenate operation, first, we need to align partitions
of𝐴and𝐵alongallotherdimensionsexceptdimension 𝑝.Toalign
the partitions, in each dimension (except dimension 𝑝), we use the
set union of partitioning positions of 𝐴and𝐵as the new set of
partitioningpositionsfor 𝐴,𝐵andalsothatfor 𝐶.Indimension 𝑝,
wedonotchangethepartitioningpositionsof 𝐴and𝐵,whilethe
set of partitioning positions of 𝐶consists of (1) 𝐴’s partitioning po-
sitions in dimension 𝑝; (2) the size of 𝐴in dimension 𝑝, denoted as
𝑛(representingtheboundarybetween 𝐴and𝐵);(3)𝐵’spartitioning
positions(indimension 𝑝)plus𝑛.LetN𝐴,N𝐵denotethenumber
ofpartitionsof 𝐴,𝐵after alignment,respectively.
Forsimplicityofpresentation,hereweassumethattwo-dimensional
tensors𝐴,𝐵areconcatenatedtoformatensor 𝐶alongdimension
0(i.e.,𝑝=0).Thenforeachpartition 𝐶𝑖,wecalculateitsinterval
range by
𝜎(𝑐𝑖)=/braceleftbigg𝜎(𝑎𝑖)if𝑖∈ {1,...,N𝐴}
𝜎(𝑏𝑖−N𝐴)if𝑖∈ {N𝐴+1,...,N𝐴+N𝐵}.
831DetectingNumerical Bugs in Neural Network Architectures ESEC/FSE ’20, November8–13,2020,VirtualEvent, USA
Figure 2:Splitting Tensors(Vertically)
Wealsomaintaintheelementwiseaffineequalityrelationsbetween
𝐶𝑖and𝐴𝑖(when𝑖≤ N𝐴), as well as relations between 𝐶𝑖and𝐵𝑖
(when𝑖>N𝐴), as
𝑐𝑖=𝑎𝑖𝑖∈ {1,...,N𝐴}
𝑐𝑖=𝑏𝑖−N𝐴𝑖∈ {N𝐴+1,...,N𝐴+N𝐵},
whichmeans
∀(𝑗,𝑘) ∈I(𝐶𝑖.𝑠ℎ𝑎𝑝𝑒).𝐶𝑖[𝑗][𝑘]=𝐴𝑖[𝑗][𝑘]1≤𝑖≤ N𝐴
∀(𝑗,𝑘) ∈I(𝐶𝑖.𝑠ℎ𝑎𝑝𝑒).𝐶𝑖[𝑗][𝑘]=𝐵𝑖−N𝐴[𝑗][𝑘] N𝐴<𝑖≤ N𝐴+N𝐵.
For the example showninFigure 1,suppose
I𝐴1=[0..0] × [0..1] ∧I𝐴2=[0..0] × [2..3]∧
I𝐵1=[0..1] × [0..3] ∧I𝐵2=[2..2] × [0..3]∧
𝛼(𝐴1)=𝑎1∧𝛼(𝐴2)=𝑎2∧𝛼(𝐵1)=𝑏·
1∧𝛼(𝐵2)=𝑏·
2,
where we temporarily use 𝑏·
𝑖to denote the summary variablesfor
𝐵𝑖here.Afteraligningthe partitionsof 𝐴and𝐵,we have
I𝐴1=[0..0] × [0..1] ∧I𝐴2=[0..0] × [2..3]∧
I𝐵1=[0..1] × [0..1] ∧I𝐵2=[0..1] × [2..3]∧
I𝐵3=[2..2] × [0..1] ∧I𝐵4=[2..2] × [2..3]∧
𝛼(𝐴1)=𝑎1∧𝛼(𝐴2)=𝑎2∧
𝛼(𝐵1)=𝑏1∧𝜎(𝑏1)=𝜎(𝑏·
1) ∧𝛼(𝐵2)=𝑏2∧𝜎(𝑏2)=𝜎(𝑏·
1)∧
𝛼(𝐵3)=𝑏3∧𝜎(𝑏3)=𝜎(𝑏·
2) ∧𝛼(𝐵4)=𝑏4∧𝜎(𝑏4)=𝜎(𝑏·
2).
Then, after 𝐶=𝐶𝑜𝑛𝑐𝑎𝑡𝑒𝑛𝑎𝑡𝑒 (𝐴,𝐵,0),for𝐶,we have:
I𝐶1=[0..0] × [0..1] ∧I𝐶2=[0..0] × [2..3] ∧I𝐶3=[1..2] × [0..1]∧
I𝐶4=[1..2] × [2..3] ∧I𝐶5=[3..3] × [0..1] ∧I𝐶6=[3..3] × [2..3]∧
𝛼(𝐶1)=𝑐1∧𝜎(𝑐1)=𝜎(𝑎1) ∧𝛼(𝐶2)=𝑐2∧𝜎(𝑐2)=𝜎(𝑎2)∧
𝛼(𝐶3)=𝑐3∧𝜎(𝑐3)=𝜎(𝑏1) ∧𝛼(𝐶4)=𝑐4∧𝜎(𝑐4)=𝜎(𝑏2)∧
𝛼(𝐶5)=𝑐5∧𝜎(𝑐5)=𝜎(𝑏3) ∧𝛼(𝐶6)=𝑐6∧𝜎(𝑐6)=𝜎(𝑏4)∧
𝑐𝑖=𝑎𝑖𝑖={1,2}∧𝑐𝑖=𝑏𝑖−2𝑖={3,4,5,6}.
3.3.3 Split. Atensorcanbesplitintosub-tensorsalongacertaindi-
mension.Moreclearly,astatement (𝐵(1),...,𝐵(𝑛))=𝑠𝑝𝑙𝑖𝑡(𝐴,𝑛,𝑝)
denotes that 𝐴is split along dimension 𝑝into𝑛smaller tensors,
which are stored in 𝐵(1),...,𝐵(𝑛). The statement requires that 𝑛
evenlydivides 𝐴.𝑠ℎ𝑎𝑝𝑒[𝑝](i.e.,thenumberofelementsindimen-
sion𝑝inA).Forexample,Figure 2showsthatatwo-dimensional
tensor𝐴is split along dimension 0 (rows) into 2 sub-tensors 𝐵1
and𝐵2.Tohandlethe 𝑆𝑝𝑙𝑖𝑡operation,first,weusethefollowing
set as the new set of partitioning positions of 𝐴in dimension 𝑝:
{𝐴.𝑠ℎ𝑎𝑝𝑒[𝑝]
𝑛−1,2∗𝐴.𝑠ℎ𝑎𝑝𝑒[𝑝]
𝑛−1,...,(𝑛−1)∗𝐴.𝑠ℎ𝑎𝑝𝑒[𝑝]
𝑛−1},and
align the partitions of 𝐴with respect to the new set of partitioning
positions. Let N𝐴denote the number of partitions of 𝐴after align-
ment.Then,wekeepthesetofpartitioningpositionsof 𝐴asthatof
𝐵(𝑗)for all dimensions except 𝑝. In dimension 𝑝, we use the emptysetasthesetofpartitioningpositionsof 𝐵(𝑗).Inotherwords,we
do not partition 𝐵(𝑗)indimension 𝑝.
Forsimplicityofpresentation,hereweassumethattwo-dimensional
tensors𝐴are split into sub-tensors (𝐵(1),...,𝐵(𝑛))along dimen-
sion0(i.e., 𝑝=0).Then,consideringanoutputsub-tensor 𝐵(𝑗),for
eachofits partitions 𝐵(𝑗)
𝑖,we calculateits intervalrange by
𝜎(𝑏(𝑗)
𝑖)=𝜎(𝑎𝑖′),
where𝑖′=(𝑗−1)∗N𝐴
𝑛+𝑖.Wealsomaintaintheelementwiseaffine
equalityrelationsbetween 𝐵(𝑗)
𝑖and𝐴𝑖′(where𝑖′=(𝑗−1)∗N𝐴
𝑛+𝑖):
𝑏(𝑗)
𝑖=𝑎𝑖′𝑖∈ {1,...,N𝐴
𝑛},
whichmeans
∀(𝑘,𝑚) ∈ (𝐵(𝑗)
𝑖.𝑠ℎ𝑎𝑝𝑒).𝐵(𝑗)
𝑖[𝑘][𝑚]=𝐴𝑖′[𝑘][𝑚]𝑖∈ {1,...,N𝐴
𝑛}.
For example, consider (𝐵(1),𝐵(2))=𝑠𝑝𝑙𝑖𝑡(𝐴,2,0)in Figure 2
and suppose that before this statement, 𝐴is partitioned into the
following twopartitions:
I𝐴1=[0..1]×[0..1]∧I𝐴2=[0..1]×[2..3]∧𝛼(𝐴1)=𝑎·
1∧𝛼(𝐴2)=𝑎·
2,
where we temporarily use 𝑎·
𝑖todenote thesummaryvariablesfor
𝐴𝑖here.After aligningthe partitionsof 𝐴,we have
I𝐴1=[0..0] × [0..1] ∧I𝐴2=[0..0] × [2..3]∧
I𝐴3=[1..1] × [0..1] ∧I𝐴4=[1..1] × [2..3]∧
𝛼(𝐴1)=𝑎1∧𝜎(𝑎1)=𝜎(𝑎·
1) ∧𝛼(𝐴2)=𝑎2∧𝜎(𝑎2)=𝜎(𝑎·
2)∧
𝛼(𝐴3)=𝑎3∧𝜎(𝑎3)=𝜎(𝑎·
1) ∧𝛼(𝐴4)=𝑎4∧𝜎(𝑎4)=𝜎(𝑎·
2).
Then, after (𝐵(1),𝐵(2))=𝑠𝑝𝑙𝑖𝑡(𝐴,2,0),for𝐵(1),𝐵(2),we have:
I𝐵(1)
1=[0..0] × [0..1] ∧I𝐵(1)
2=[0..0] × [2..3]∧
I𝐵(2)
1=[0..0] × [0..1] ∧I𝐵(2)
2=[0..0] × [2..3]∧
𝛼(𝐵(1)
1)=𝑏(1)
1∧𝜎(𝑏(1)
1)=𝜎(𝑎1) ∧𝛼(𝐵(1)
2)=𝑏(1)
2∧𝜎(𝑏(1)
2)=𝜎(𝑎2)∧
𝛼(𝐵(2)
1)=𝑏(2)
1∧𝜎(𝑏(2)
1)=𝜎(𝑎3) ∧𝛼(𝐵(2)
2)=𝑏(2)
2∧𝜎(𝑏(2)
2)=𝜎(𝑎4)∧
𝑏(𝑗)
𝑖=𝑎𝑖′𝑖, 𝑗={1,2},
where𝑖′=(𝑗−1) ∗2+𝑖.
3.4 Input Ranges andParameterRanges
In previous sections, we initialize the intervals as full ranges of the
respective types, e.g., [FLOAT_MIN, FLOAT_MAX] for floats. How-
ever,intherealworld,theinputmayfallintoonlyasmallrange.
Forexample,anRGBvalueofanimagefallsinto [0,255].Especially,
inmanyapplications,inputsarenormalizedintoasmallrange(e.g.,
[−1,1]) after the preprocessing step. Similarly, the parameters of a
neuralnetworkmayalsofallintoasmallrange.Forexample,many
neuralarchitecturesareinitializedwithaweightinitializationfunc-
tion, which reflects the desired upper bounds and lower bounds of
the parameters. Assuming full ranges for these inputs and parame-
tersmayleadtounnecessaryfalsepositives,andthusourapproach
also allows the usertospecifyinput rangesandparameterranges,
anduses the user-providedranges to initialize the intervals.
4 EVALUATION
Our evaluation aims to answer the following research questions
for assessing effectiveness of DEBAR (RQ1) and studying the tech-
niques inDEBAR (RQ2andRQ3):
832ESEC/FSE ’20, November8–13,2020,VirtualEvent, USA Y. Zhang,L.Ren, L.Chen, Y. Xiong, S.Cheung,T. Xie
•RQ1: Is DEBAR effective indetecting numerical bugs?
•RQ2: How effective are the three tensor abstraction tech-
niques?
•RQ3:Howeffectivearethe twonumericalabstractiontech-
niques?
4.1 Datasets
We collect two datasets for the evaluation. The first dataset is a set
of9buggyarchitecturescollectedbyexistingstudies.Thebuggy
architecturescomefromtwostudies:8architectureswerecollected
byapreviousempiricalstudy[ 34]onTensorFlowbugsand1archi-
tecturewasobtainedfromastudyconductedtoevaluateTensor-
Fuzz [24].
As most of the architectures in the first dataset are small, we
collect the second dataset, which contains 48 architectures from
alargecollectionofresearchprojectsintherepositoryofTensor-
Flow Research Models4. The whole collection contains 66 projects
implemented in TensorFlow by researchers and developers for dif-
ferenttasksinvariousdomains,includingcomputervision,natural
language processing, speech recognition, and adversarial machine
learning. We first filter out the projects that are not related to
specific neural architecturessuch as API frameworks andoptimiz-
ers. We further filter out the projects of which the computation
graph cannot be generated due to incomplete documentation or
complicated configuration. As a result, 32 projects remain after
filtering, and some of them contain more than one neural archi-
tecture. Overall, our second dataset contains a great diversity of
neural architectures such as Convolutional Neural Network (CNN),
RecurrentNeuralNetwork(RNN),GenerativeAdversarialNetwork
(GAN),andHiddenMarkovModel(HMM).Notethatwehaveno
knowledge about whether the architectures in this dataset contain
numerical bugswhen collecting the dataset.
Foreveryarchitectureinthetwodatasets,weextractthecom-
putation graph via a TensorFlow API. Each extracted computation
graph is represented by a Protocol Buffer file5, which provides the
operations (nodes) and the data flow relations (edges). We make 48
computationgraphs publiclyavailable6.
Columns1ś4inTable 1provideanoverviewofthetwodatasets.
Column2providesanestimationofthelinesofcodeinthecorre-
sponding DL programs. Column 3 shows the number of operations
in the computation graphs, and textsumhas the highest number
of operations (208,412). Moreover, Column 4 shows the number
of parameters (trainable weights) in the DNN architectures, and
lm_1bhas the largestnumber ofparameters(1.04G).
4.2 Setups ofInput Range andParameter
Range
In ourevaluation,we conservativelyprovidetheinputranges. As
described in Section 3.4, we get the initial input ranges from the
physical meaning of inputs, and derive input range information
from the preprocessing programs typically written for the training
4https://github.com/tensorflow/models/blob/13e7c85d521d7bb7cba0bf7d743366f
7708b9df7/research
5https://en.wikipedia.org/wiki/Protocol_Buffers
6https://doi.org/10.5281/zenodo.3843648data.Ifwefailtoprovideinputrangeswiththeprecedingtwosteps,
we setthe inputranges to [FLOAT_MIN, FLOAT_MAX] .
Wedeterminetheparameterrangeswiththeweightinitialization
functions. If the parameters are initialized to zero, we set their
rangesasdefault values [−1,1].Wealsoprovidesomeheuristics
foruninitializedparameters:setting varianceto[0, FLOAT_MAX] ,
andsetting countandstepto[1, FLOAT_MAX] .Otherwise,weset
the parameterranges to [FLOAT_MIN, FLOAT_MAX] .
We provide the setups for each architecture in our DEBAR open
sourcecode.
4.3 Unsafe Operationsto Check
By investigating the dataset in a previous empirical study [ 34], we
collect a list of unsafe operations shown in Table 2. These opera-
tionshavethemostfrequentoccurrencesandhaveahighpotential
to cause numerical errors. In this paper, we use our static analysis
approach based onabstract interpretationtocheckwhether these
operations can cause numerical errors. Specifically, after perform-
ingouranalysis,wecangettheintervalrangefortheparameter
𝑥of the operations, denoted as [𝑥,𝑥]. Then we check the unsafe
constraints listed in Table 2. If the unsafe constraints for an opera-
tion are satisfiable, our checker issues an alarm for indicating that
the operation may cause numerical errors. Otherwise, the opera-
tion is safe. In Table 2,𝑀𝑓and𝑚𝑓, respectively, denote the largest
non-infinity floating-point number and the smallest non-zero posi-
tive floating-point number that the used floating-point format (e.g.,
32-bit,64-bit) can represent exactly.
InDNNimplementations,therearemanyoperations(e.g.,the
multiplication) that may lead to numerical bugs. Our approach’s
current implementation checks onlythose operationslisted inTa-
ble2,but can be easily extendedto otheroperations.
4.4 Measurements
Our approach checks every operation that may lead to a numerical
error and determines whether a warning should be reported. To
measuretheeffectivenessofourapproach,wetreatitasaclassifier
thatclassifieswhethereachoperationisbuggy,andevaluatesits
effectiveness using the number of true/false positives/negatives
and accuracy. More concretely, true (false) positives refer to the
warningsthatare(not)indeedbugs,true(false)negativesreferto
thosecorrect(buggy)operationswherenowarningisreported,and
accuracyiscalculatedusingthefollowingformula,where 𝑇𝑃/𝐹𝑃
referstotrue/falsepositiveand 𝑇𝑁/𝐹𝑁referstotrue/falsenegative.
𝐴𝑐𝑐𝑢𝑟𝑎𝑐𝑦 =#𝑇𝑃+#𝑇𝑁
#𝑇𝑃+#𝑇𝑁+#𝐹𝑃+#𝐹𝑁
Forthefirstdataset,werefertouserpatchestodeterminewhether
awarning isabug.For the seconddataset,
•204truepositivesareconfirmedbyexecutingthearchitec-
tureunderanalysisusingthedesignedinputsandparameters
to trigger the numerical errors.
•52truepositivesareconfirmedbythedeveloper-provided
fixes (notmergedyet) inthe issuediscussion.
•43 true positives are confirmed when two authors of this
paperseparatelydoreasoningoneachcomputationgraph,
andbothauthorsconcludethateachwarningistruepositive.
833DetectingNumerical Bugs in Neural Network Architectures ESEC/FSE ’20, November8–13,2020,VirtualEvent, USA
Table 1:DatasetOverview andResults
Name LoC #Ops #Params TPDEBAR Array Smashing SoleIntervalAbstraction
TN FP Acc Time TN FP Acc Time TN FP Acc Time
TensorFuzz 77 225 178K 4 0 0 100.0% 1.9 0 0 100.0% 1.9 0 0 100.0% 1.7
Github-IPS-1 367 1,546 5.05M 1 4 0 100.0% 2.3 4 0 100.0% 2.2 4 0 100.0% 2.2
Github-IPS-6 2,377 167 23.6K 2 0 0 100.0% 1.7 0 0 100.0% 1.7 0 0 100.0% 1.7
Github-IPS-9 226 102 23.6K 1 0 0 100.0% 1.7 0 0 100.0% 1.7 0 0 100.0% 1.7
StackOverflow-IPS-1 102 329 9.28M 1 1 0 100.0% 1.8 1 0 100.0% 1.8 1 0 100.0% 1.8
StackOverflow-IPS-2 102 329 9.28M 1 1 0 100.0% 1.8 1 0 100.0% 1.7 1 0 100.0% 1.8
StackOverflow-IPS-6 102 329 9.28M 1 1 0 100.0% 1.8 1 0 100.0% 1.8 1 0 100.0% 1.8
StackOverflow-IPS-7 49 145 407K 2 0 0 100.0% 1.7 0 0 100.0% 1.7 0 0 100.0% 1.7
StackOverflow-IPS-14 48 74 7.85K 1 0 0 100.0% 1.7 0 0 100.0% 1.7 0 0 100.0% 1.6
ssd_mobile_net_v1 71,242 22,412 27.3M 26 233 48 84.4% 21.8 137 144 53.1% 21.5 136 145 52.8% 21.6
ssd_inception_v2 71,242 23,929 100M 3 49 2 96.3% 19.8 45 6 88.9% 19.8 44 7 87.0% 19.7
ssd_mobile_net_v2 71,242 28,724 24.3M 26 233 48 84.4% 25.5 137 144 53.1% 26.0 136 145 52.8% 25.9
faster_rcnn_resnet_50 71,242 12,485 73.4M 5 31 22 62.1% 11.4 31 22 62.1% 11.4 31 22 62.1% 11.5
deep_speech 659 7,318 0.131K 0 6 0 100.0% 6.9 6 0 100.0% 7.1 6 0 100.0% 7.1
deeplab 7,514 21,100 87.1M 0 2 0 100.0% 17.8 2 0 100.0% 17.7 2 0 100.0% 18.3
autoencoder_mnae 369 187 944K 0 1 0 100.0% 2.6 1 0 100.0% 2.7 1 0 100.0% 2.6
autoencoder_vae 369 370 1.41M 2 1 0 100.0% 2.7 1 0 100.0% 2.7 1 0 100.0% 2.7
attention_ocr 1,772 3,624 1.74M 1 4 2 71.4% 5.2 4 2 71.4% 5.1 4 2 71.4% 5.2
textsum 906 208,412 10.5M 0 94 0 100.0% 105.7 94 0 100.0% 103.1 94 0 100.0% 106.4
shake_shake_32 1,233 7,348 5.85M 0 55 0 100.0% 7.5 55 0 100.0% 7.6 55 0 100.0% 7.7
shake_shake_96 1,233 7,348 52.4M 0 55 0 100.0% 7.6 55 0 100.0% 7.7 55 0 100.0% 7.7
shake_shake_112 1,233 7,348 71.3M 0 55 0 100.0% 7.6 55 0 100.0% 7.6 55 0 100.0% 7.5
pyramid_net 1,233 43,142 52.6M 0 7 0 100.0% 37.9 7 0 100.0% 38.7 7 0 100.0% 39.2
sbn 1,108 11,262 2.21M 0 42 3 93.3% 4.1 42 3 93.3% 4.1 26 19 57.8% 3.7
sbnrebar 1,108 11,262 2.21M 0 187 2 98.9% 9.8 187 2 98.9% 9.8 107 82 56.6% 8.8
sbndynamicrebar 1,108 31,530 2.61M 0 194 2 99.0% 18.7 194 2 99.0% 19.2 114 82 58.2% 17.9
sbngumbel 1,108 2,070 1.98M 0 78 2 97.5% 4.6 78 2 97.5% 4.6 46 34 57.5% 4.0
audioset 405 699 216M 0 2 0 100.0% 2.9 2 0 100.0% 2.9 1 1 50.0% 2.9
learning_to_remember 702 1,027 4.30M 0 6 0 100.0% 3.1 6 0 100.0% 3.1 6 0 100.0% 3.1
neural_gpu1 2,401 5,080 2.68M 0 53 0 100.0% 5.5 53 0 100.0% 5.5 53 0 100.0% 5.6
neural_gpu2 2,401 2,327 1.35M 0 38 0 100.0% 4.2 38 0 100.0% 4.2 38 0 100.0% 4.2
ptn 1,713 23,636 145M 0 351 0 100.0% 14.8 351 0 100.0% 15.0 351 0 100.0% 14.8
namignizer 262 2,310 652K 0 3 0 100.0% 3.5 3 0 100.0% 3.5 3 0 100.0% 3.5
feelvos 2,955 83,558 83.0M 0 4 0 100.0% 135.4 4 0 100.0% 137.7 4 0 100.0% 132.6
fivo_srnn 5,661 3,514 357K 0 7 11 38.9% 4.2 7 11 38.9% 4.3 7 11 38.9% 4.3
fivo_vrnn 5,661 3,820 365M 0 7 11 38.9% 4.4 7 11 38.9% 4.5 7 11 38.9% 4.5
fivo_ghmm 5,661 2,759 60 0 9 23 28.1% 4.0 9 23 28.1% 4.1 9 23 28.1% 4.1
dcb_var_bnn 2,143 474 36.0K 0 22 0 100.0% 3.0 22 0 100.0% 3.0 22 0 100.0% 3.0
dcb_neural_ban 2,143 186 18.0K 0 4 0 100.0% 2.7 4 0 100.0% 2.7 4 0 100.0% 2.7
dcb_bb_alpha_nn 2,143 11,180 36.0K 0 163 2 98.8% 8.2 163 2 98.8% 8.2 163 2 98.8% 8.4
dcb_rms_bnn 2,143 186 18.0K 0 4 0 100.0% 2.7 4 0 100.0% 2.7 4 0 100.0% 2.7
adversarial_crypto 133 676 8.14K 0 6 0 100.0% 3.0 6 0 100.0% 3.0 6 0 100.0% 3.0
sentiment_analysis 130 334 4.39M 0 3 1 75.0% 2.7 3 1 75.0% 2.7 3 1 75.0% 2.7
next_frame_prediction 493 2,820 6.70M 1 6 0 100.0% 4.0 6 0 100.0% 4.1 6 0 100.0% 4.0
minigo 3,774 929 34.4K 1 0 0 100.0% 3.0 0 0 100.0% 3.0 0 0 100.0% 3.0
compression_entropy_coder 2,000 15,709 20.0K 0 13 0 100.0% 9.7 13 0 100.0% 10.0 13 0 100.0% 9.8
lfads 2,898 51,853 928K 202 213 3 99.3% 48.7 213 3 99.3% 49.5 213 3 99.3% 48.6
lm_1b 3,81 2,926 1.04G 0 1 0 100.0% 4.0 1 0 100.0% 4.0 1 0 100.0% 4.0
swivel 1,449 279 36.0K 0 1 0 100.0% 2.7 1 0 100.0% 2.7 1 0 100.0% 2.7
skip_thought 1,129 6,800 377M 0 15 0 100.0% 5.7 15 0 100.0% 5.8 15 0 100.0% 5.7
video_prediction 462 48,148 41.6M 32 288 0 100.0% 30.7 288 0 100.0% 30.6 288 0 100.0% 30.5
gan_mnist 806 2,664 39.7M 0 3 0 100.0% 3.7 3 0 100.0% 3.8 3 0 100.0% 3.7
gan_cifar 510 3,784 43.3M 0 17 0 100.0% 4.5 17 0 100.0% 4.5 17 0 100.0% 4.5
gan_image_compression 444 4,230 35.5M 0 17 0 100.0% 4.7 17 0 100.0% 4.7 17 0 100.0% 4.7
vid2depth 2,502 35,072 99.6M 0 132 48 73.3% 21.2 132 48 73.3% 21.9 132 48 73.3% 21.5
domain_adaptation 3,079 6,010 7.01M 0 28 0 100.0% 5.6 28 0 100.0% 5.6 25 3 89.3% 5.7
delf 3,683 2,712 9.10M 0 10 0 100.0% 5.1 10 0 100.0% 5.1 10 0 100.0% 5.0
Total Ð Ð Ð 313 2760 230 93.0% 691.1 2564 426 87.1% 694.9 2349 641 80.6% 688.7
Since our approach does not have false negatives by design, we
omit this column inreportingour evaluation results.
4.5 Implementation andHardwarePlatform
WehaveimplementedourDEBARtoolinPython.Allofourmea-
surementsare performedonaserverrunningUbuntu16.04.6LTSwith a GeForce GTX 1080 Ti GPU and i7-8700K CPU running at
3.70GHz.
4.6 RQ1: Effectiveness ofDEBAR
4.6.1 Setup. ToanswerRQ1,weinvokeDEBARonthetwodatasets
andcheckthenumberoftrue/falsepositives/negatives,accuracy,
andexecutiontime (inseconds).
834ESEC/FSE ’20, November8–13,2020,VirtualEvent, USA Y. Zhang,L.Ren, L.Chen, Y. Xiong, S.Cheung,T. Xie
Table 2:Operationsto Check
Operations UnsafeConstraints
Exp(x),Expm1(x) log (𝑀𝑓)<𝑥
Log(x) 𝑥<𝑚𝑓
Log1p(x) 𝑥+1<𝑚𝑓
RealDiv(y,x) 𝑥<𝑚𝑓∧𝑥>−𝑚𝑓
Reciprocal(x) 𝑥<𝑚𝑓∧𝑥>−𝑚𝑓
Sqrt(x) 𝑥≤ −𝑚𝑓
Rsqrt(x) 𝑥<𝑚𝑓
4.6.2 Results. Columns 5ś9 of Table 1show the results. We make
the following observations aboutDEBAR.
•It detects all knownnumerical errorson the 9 architectures
inthe firstdataset,withzerofalse positive.
•It detects 299 previously unknown operations that may lead
to numerical errors in the real-world architectures from the
second dataset. Note that a numerical bug can trigger multi-
plenumericalerrorsatdifferentoperations,e.g.,failingto
normalizeaninputtensorthatisusedinmultipleoperations.
•It correctly classifies 3,073 operations with only 230 false
positives, achievingaccuracyof93.0%.
•It is scalable to handle the real-world architectures, all of
which are analyzed in 3 minutes, and the average time is
12.1seconds.
TounderstandwhyDEBARgeneratessomefalsepositives,we
investigatethe false positives(FPs) and find the following reasons.
•Someoperationsdependonanargumenttoindexthetensor
elementsfortheoperations.Forexample,function gather
returnselementsinatensorbasedonanargumentthatspeci-
fiestheelements’indexes.Sincewedonotknowbeforehand
which indexes are subject to an operation, we merge the
intervals atallpossibleindexes, leadingtoimprecision. 116
FPsbelong to this category.
•Ouraffinerelationanalysisworksononlylinearexpressions.
When a non-linear operation is used, we create a new ab-
stract element, leading to imprecision. 15 FPs belong to this
category.
•48 FPsbelong to both ofthe preceding twocategories.
•Forwhileloopsin RNNs,wedonotusetensorpartitioning
and elementwise affine equality relations but use the classic
Kleene iteration together with the widening operator [ 4] in
the interval abstract domain, leading to imprecision. 50 FPs
belong to this category.
•The TensorFlow API used to extract computation graphs
fails to analyze the shapes ofsometensors,leadingto 1FP.
4.7 RQ2: Studyon TensorAbstraction
4.7.1 Setup. Tostudythreetensorabstractiontechniques,wecom-
pare array smashing and array expansion with tensor partitioning
by fixing the numerical abstraction as affine relation analysis (used
togetherwithintervalabstraction).
4.7.2 Results. Columns 10ś13 of Table 1show the results of array
smashing, andTable 3shows the resultsof array expansion.SinceTable 3:Results ofArrayExpansion
NameArray Expansion
TN FP Acc Time
TensorFuzz 0 0 100% 29.6
GitHub-IPS-6 0 0 100% 3.2
GitHub-IPS-9 0 0 100% 3.1
StackOverFlow-7 0 0 100% 72.4
StackOverFlow-14 0 0 100% 2.4
autoencoder_mnae 1 0 100.0% 105.4
autoencoder_vae 1 0 100.0% 232.6
sbn 42 3 93.3% 397.8
sbnrebar 187 2 98.9% 725.1
sbngumbel 78 2 97.5% 401.2
learning_to_remember 6 0 100.0% 913.5
neural_gpu1 53 0 100.0% 702.8
neural_gpu2 38 0 100.0% 336.8
namignizer 3 0 100.0% 629.9
fivo_srnn 7 11 38.9% 44.1
fivo_ghmm 9 23 28.1% 4.1
dcb_var_bnn 22 0 100.0% 12.3
dcb_neural_ban 4 0 100.0% 4.0
dcb_bb_alpha_nn 163 2 98.8% 155.5
dcb_rms_bnn 4 0 100.0% 4.0
adversarial_crypto 6 0 100.0% 3.6
sentiment_analysis 3 1 75.0% 693.8
mingo 0 0 100.0% 8.0
compression_entropy_coder 13 0 100.0% 340.7
arrayexpansiontimesouton33ofthesubjectswithatimebudgetof
30minutes,wereportonlytheresultsontheremaining24subjects.
From the tables,we make the following observations.
•Compared to array smashing, DEBAR even runs faster, indi-
catingthattheoverheadoftensorpartitioningissonegligible
that the overhead is dominated bythe random error of exe-
cution time, and DEBAR successfully eliminates 196 more
false positives, improving the (total) accuracy from 87.1% to
93.0%.
•Comparedtoarrayexpansion,theanalysisofDEBARruns
seconds to hundreds of seconds faster, and does not lose
any accuracy on all the 24 subjects that array expansion can
analyze within the time budget of 30 minutes.
These observations confirm that tensorpartitioning is more effec-
tive thanthe othertwotensorabstractiontechniques.
4.8 RQ3: Studyon NumericalAbstraction
4.8.1 Setup. Tostudytwonumericalabstractiontechniques,we
compare sole interval abstraction and affine relation analysis with
interval abstraction by fixing the tensor abstraction as tensor parti-
tioning.
4.8.2 Results. Columns14ś17ofTable 1showtheresultsofDE-
BARandsoleintervalabstraction.DEBARhasanegligibleoverhead
(0.3% on average) and eliminates 411 false positives, improving the
835DetectingNumerical Bugs in Neural Network Architectures ESEC/FSE ’20, November8–13,2020,VirtualEvent, USA
accuracy from 80.6% to 93.0% in total. These observations indi-
catethattheaffinerelationanalysisiseffectiveandsubstantially
contributesto the overalleffectiveness.
4.9 Threatsto Validity
The threat to internal validity mainly lies in the implementation of
our approachÐwhether our implementation correctly captures our
approach.Toalleviatethethreat,wehavemanuallycheckedallthe
warningsreportedbyourapproach,andanalyzedthereasonsfor
the false positives, validating the implementationto someextent.
Thethreattoexternalvalidityliesintherepresentativenessof
the subjects. In particular, the proportion of false warnings among
all warnings heavily depends on the number of numerical bugs in
the subjects and maynot be generalizable. On the otherhand, the
accuracyismoregeneralizableandthuswechooseaccuracyaspart
ofthe metrics inour evaluation.
Thethreattoconstructvalidityismainlythatwehavedefinedthe
input range and parameter range, and realusers may set different
ranges from us. To alleviate this threat, we take a very conserva-
tive approach such that real users are likely to set only smaller
ranges rather than larger. To further understand the effect of these
ranges, we conduct two additional experiments to understand the
effect of removing these ranges. We find that, after removing all
the input ranges, the accuracy drops 6.2 percentage points, and
after removing all the parameter ranges, the accuracy drops 6.4
percentage points. The results suggest that the ranges do affect the
accuracyof theDEBAR tool. However, the effect is relativelysmall,
and ourconclusionstill holds ingeneral,even ifdifferentranges
are specified.
5 RELATED WORK
StaticAnalysisforTensorFlowPrograms. Ariadne[ 7]can de-
tecterrorsatcodecreationtimeforTensorFlowprogramsinPython
byapplyingastaticanalysisframework(WALA).UnlikeDEBAR,
Ariadne cannot detect bugs at the architecture level. Moreover,
Ariadne targets to infer the shapes of tensors and builds a type
systemforanalyzingtensorshapes.SinceAriadnedoesnottrack
the tensorvalues,itcannotbe appliedto detectnumerical bugs.
Static Analysis of DL Models. Multiple approaches have been
proposed to statically analyze DL models. Reluplex [ 17] uses Satis-
fiability Modulo Theory (SMT) to verify properties of DNNs. Dutta
etal. [9]proposed an output range analysis forDNNsusing a lo-
cal gradient search and mixed-integer linear programming (MILP).
Lomuscio et al. [ 19] used linear programming to analyze the reach-
abilityofDNNs.Arecentstudy[ 10]showsthattheseapproaches
cannotscaleuptolargeDLmodelsduetothescalabilityissueof
existing constraintsolvers.
Otherworkbuiltstaticanalyzersbasedonabstractinterpretation.
Gehr et al. [ 10] proposed AI2, which deploys abstract interpreta-
tion(zonotope domain)to prove safety propertiesofDNNs.Singh
et al. [27] proposed DeepPoly for using floating-point polyhedra
and affine transformations to improve the scalability and precision
of analysis. Singh et al. [ 28] proposed RefineZero for using the
zonotope domain and MILP to further improve the precision of
analysis.Li etal.[ 18]usedthesymbolic propagationtechniqueto
improveprecision.UnlikeDEBAR,theseapproachesaimtoanalyzeneural network models precisely, so these approaches all adapt the
arrayexpansionstrategy,whereeachelementisinstantiatedasa
scalar variable. As shown by our evaluation, this strategy is not
efficient enough to identify numerical bugs before training. On the
other hand, DEBAR incorporates a novel tensor abstraction, maps
tensorpartitionstoabstractelements,anddiscoverslinearequality
relations between partitions.
Testing DL Models. Quite some previous work on testing DL
modelsfocuses ontestcoverage criteria for DL models[ 20,21,25,
29ś31].Forexample,basedoncoveragecriteria,Odenaetal.[ 24]
proposed TensorFuzz for using coverage-guided fuzzing to test DL
models. Such previous work focuses on DL models and does not
detectnumerical bugsbefore training.
Adversarial examples are often viewed as revealing vulnerabili-
ties of DL models, and many approaches focus on finding adversar-
ial examples. Popular adversarial attack approaches such as FGSM,
C&W,andPGD[ 2,11,22]usegradient-basedtechniquestogenerate
adversarialexamplesguidedbyobjectivefunctionswhoseinputs
are theparameters. These approachescannot be easilyadapted to
testDLarchitecturesastheobjectivefunctionscannotbecomputed
withoutparameters.
Testing DLLibraries. CRADLE [ 26] was proposed to detect and
locatebugsindeeplearninglibraries.Incontrast,ourDEBARap-
proach targets at DL architectures instead of DL libraries.
Array Analysis in Imperative Programs. Our approach is in-
spired by the existing approaches of abstract interpretation for
analyzingarrays,in particular,arraypartitioning[ 6,12,13].Com-
pared with the existing approachof array partitioning, we are the
first to generalize this approach from arrays to tensors, employ
an affine relation analysis for capturing affine equality relations
among partitions, and design abstract tensor operations for DL
architectures such as the abstract operation for ReLU.
6 CONCLUSION
Wehaveproposedastaticanalysisapproachtodetectnumerical
bugs in neural architectures. We specially designed tensor parti-
tioning and affine relation analysis (used together with interval
abstraction) over partitions for our approach, and implemented
them as DEBAR. We evaluated our approachon two datasets with
various settings on tensor abstraction and numerical abstraction
techniques, and the results show that (1) DEBAR is effective to
detect numerical bugs in real-world neural architectures; and (2)
two specially designed abstraction techniques are essential for im-
provingthe scalability andaccuracyof detecting numerical bugs.
ACKNOWLEDGMENTS
This work is supported by the National Key Research and Devel-
opment Program of China under Grant No. SQ2019YFE010068, the
National Natural Science Foundation of China under Grant Nos.
61922003, 61932021,61872445, and MSRACollaborativeResearch
Grant.
836ESEC/FSE ’20, November8–13,2020,VirtualEvent, USA Y. Zhang,L.Ren, L.Chen, Y. Xiong, S.Cheung,T. Xie
REFERENCES
[1]Bruno Blanchet, Patrick Cousot, Radhia Cousot, Jérôme Feret, Laurent
Mauborgne, Antoine Miné, David Monniaux, and Xavier Rival. 2003. A Static
Analyzer for Large Safety-critical Software. In Proceedings of the 2003 ACM SIG-
PLANConferenceonProgrammingLanguageDesignandImplementation,PLDI
2003, San Diego, California, USA, June 9-11, 2003 . 196ś207. https://doi.org/10.
1145/781131.781153
[2]NicholasCarliniandDavidA.Wagner.2017. TowardsEvaluatingtheRobustness
ofNeuralNetworks.In Proceedingsofthe2017IEEESymposiumonSecurityand
Privacy,SP2017,SanJose,CA,USA,May22-26,2017 .39ś57.https://doi.org/10.
1109/SP.2017.49
[3]Patrick Cousot and Radhia Cousot. 1976. Static Determination of Dynamic
Properties of Programs. In Proceedings of the 2nd International Symposium on
Programming . Dunod,Paris,France, 106ś130.
[4]Patrick Cousot and Radhia Cousot. 1977. Abstract Interpretation: A Unified
LatticeModelforStaticAnalysisofProgramsbyConstructionorApproximation
of Fixpoints. In Conference Record of the 4th ACM Symposium on Principles of
Programming Languages, POPL 1977, Los Angeles, California, USA, January 1977 .
238ś252. https://doi.org/10.1145/512950.512973
[5]Patrick Cousot and Radhia Cousot. 1977. Static Determination of Dynamic
Properties of Generalized Type Unions. In Proceedings of an ACM Conference on
LanguageDesignforReliableSoftware,Raleigh,NorthCarolina,USA .ACM,77ś94.
https://doi.org/10.1145/800022.808314
[6]Patrick Cousot, Radhia Cousot, and Francesco Logozzo. 2011. A Parametric
Segmentation Functor for Fully Automatic and Scalable Array Content Analysis.
InProceedings of the 38th ACM SIGPLAN-SIGACT Symposium on Principles of
Programming Languages, POPL 2011, Austin, TX, USA, January 26-28, 2011 . ACM,
105ś118.
[7]Julian Dolby, Avraham Shinnar, Allison Allain, and Jenna Reinen. 2018. Ari-
adne: Analysis for Machine Learning Program. In Proceedings of the 2nd ACM
SIGPLAN International Workshop on Machine Learning and Programming Lan-
guages,MAPL@PLDI2018,Philadelphia,PA,USA,June18-22,2018 .ACM,1ś10.
https://doi.org/10.1145/3211346.3211349
[8]SouradeepDutta,XinChen, andSriramSankaranarayanan.2019. Reachability
AnalysisforNeuralFeedbackSystemsusingRegressivePolynomialRuleInfer-
ence. InProceedings of the 22nd ACM International Conference on Hybrid Systems:
Computation and Control, HSCC 2019, Montreal, QC, Canada, April 16-18, 2019 .
ACM,157ś168. https://doi.org/10.1145/3302504.3311807
[9]SouradeepDutta,SusmitJha,SriramSankaranarayanan,andAshishTiwari.2018.
OutputRangeAnalysisforDeepFeedforwardNeuralNetworks.In Proceedings
ofthe10thNASAFormalMethodsSymposium,NFM2018,NewportNews,VA,USA,
April17-19, 2018 . 121ś138. https://doi.org/10.1007/978-3-319-77935-5_9
[10]TimonGehr, MatthewMirman,DanaDrachsler-Cohen,PetarTsankov, Swarat
Chaudhuri, andMartin T.Vechev. 2018. AI2: Safety and RobustnessCertification
of Neural Networks with Abstract Interpretation. In Proceedings of the 2018 IEEE
Symposium on Security and Privacy, SP 2018, San Francisco, California, USA, May
21-23, 2018 . 3ś18.https://doi.org/10.1109/SP.2018.00058
[11]Ian J. Goodfellow, Jonathon Shlens, and Christian Szegedy. 2015. Explaining
and Harnessing Adversarial Examples. In Proceedings of the 3rd International
Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9,
2015.http://arxiv.org/abs/1412.6572
[12]Denis Gopan, Thomas W. Reps, and Shmuel Sagiv. 2005. A Framework for
NumericAnalysisofArrayOperations.In Proceedingsofthe32ndACMSIGPLAN-
SIGACTSymposiumonPrinciplesofProgrammingLanguages,POPL2005,Long
Beach,California, USA, January12-14, 2005 . ACM,338ś350.
[13]Nicolas Halbwachs and Mathias Péron. 2008. Discovering Properties about
Arrays in Simple Programs. In Proceedings of the 2008 ACM SIGPLAN Conference
on Programming Language Design and Implementation, PLDI 2008, Tucson, AZ,
USA, June 7-13,2008 . 339ś348. https://doi.org/10.1145/1375581.1375623
[14]Xiaowei Huang, Marta Kwiatkowska, Sen Wang, and Min Wu. 2017. Safety
Verification of Deep Neural Networks. In Proceedings of the 29th International
Conference on Computer-Aided Verification, CAV 2017, Heidelberg, Germany, July
24-28, 2017 . 3ś29.https://doi.org/10.1007/978-3-319-63387-9_1
[15]Md Johirul Islam, Giang Nguyen, Rangeet Pan, and Hridesh Rajan. 2019. A
ComprehensiveStudyonDeepLearningBugCharacteristics.In Proceedingsof
the 2019 ACM Joint Meeting on European Software Engineering Conference and
Symposium on the Foundations of Software Engineering, ESEC/SIGSOFT FSE 2019,
Tallinn, Estonia, August 26-30, 2019 . 510ś520. https://doi.org/10.1145/3338906.
3338955
[16]Michael Karr.1976. Affine Relationshipsamong Variablesof a Program. ActaInf.
6,2 (June1976),133ś151. https://doi.org/10.1007/BF00268497
[17]GuyKatz,ClarkW.Barrett,DavidL.Dill,KyleJulian,andMykelJ.Kochenderfer.
2017. Reluplex: An Efficient SMT Solver for Verifying Deep Neural Networks. In
Proceedings of the29thInternationalConference onComputer-Aided Verification,
CAV2017,Heidelberg,Germany,July24-28,2017 .97ś117. https://doi.org/10.1007/
978-3-319-63387-9_5[18] Jianlin Li,JiangchaoLiu,PengfeiYang,Liqian Chen,Xiaowei Huang, and Lijun
Zhang. 2019. Analyzing Deep Neural Networks with Symbolic Propagation:
TowardsHigherPrecisionandFasterVerification.In Proceedingsofthe26thStatic
AnalysisSymposium,SAS 2019, Porto, Portugal,October 8-11,2019 (Lecture Notes
in Computer Science, Vol. 11822) . Springer, 296ś319. https://doi.org/10.1007/978-
3-030-32304-2_15
[19]AlessioLomuscioandLalitMaganti.2017. AnApproachtoReachabilityAnal-
ysis for Feed-forward ReLU Neural Networks. CoRRabs/1706.07351 (2017).
arXiv:1706.07351 http://arxiv.org/abs/1706.07351
[20]Lei Ma, Felix Juefei-Xu, Minhui Xue, Bo Li, Li Li, Yang Liu, and Jianjun Zhao.
2019. DeepCT: Tomographic Combinatorial Testing for Deep Learning Systems.
InProceedings of the 26th IEEE International Conference on Software Analysis,
Evolution and Reengineering, SANER 2019, Hangzhou, China, February 24-27, 2019 .
614ś618. https://doi.org/10.1109/SANER.2019.8668044
[21]LeiMa,FelixJuefei-Xu,FuyuanZhang,JiyuanSun,MinhuiXue,BoLi,Chunyang
Chen,TingSu,LiLi,YangLiu,JianjunZhao,andYadongWang.2018. DeepGauge:
Multi-granularity Testing Criteriafor Deep Learning Systems. In Proceedings of
the 33rd ACM/IEEE International Conference on Automated Software Engineering,
ASE2018,Montpellier,France,September3-7,2018 .120ś131. https://doi.org/10.
1145/3238147.3238202
[22]Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras,
and Adrian Vladu. 2018. Towards Deep Learning Models Resistant to Ad-
versarial Attacks. In Proceedings of the 6th International Conference on Learn-
ing Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018 .
https://openreview.net/forum?id=rJzIBfZAb
[23]Nina Narodytska, Shiva PrasadKasiviswanathan, LeonidRyzhyk, MoolySagiv,
and Toby Walsh. 2018. Verifying Properties of Binarized Deep Neural Networks.
InProceedingsofthe32rdAAAIConferenceonArtificialIntelligence,AAAI2018,
New Orleans, Louisiana, USA, February 2-7, 2018 . 6615ś6624. https://www.aaai.o
rg/ocs/index.php/AAAI/AAAI18/paper/view/16898
[24]AugustusOdena,CatherineOlsson,DavidAndersen,andIanJ.Goodfellow.2019.
TensorFuzz: Debugging Neural Networks with Coverage-Guided Fuzzing. In
Proceedings of the 36th International Conference on Machine Learning, ICML 2019,
Long Beach,California,USA, 9-15 June2019 . 4901ś4911. http://proceedings.mlr.
press/v97/odena19a.html
[25] KexinPei,YinzhiCao,JunfengYang,andSumanJana.2017. DeepXplore:Auto-
mated Whitebox Testing of Deep Learning Systems. In Proceedings of the26th
Symposium on Operating Systems Principles, SOSP 2017, Shanghai, China, October
28-31, 2017 . ACM,1ś18. https://doi.org/10.1145/3132747.3132785
[26]HungVietPham,ThibaudLutellier,WeizhenQi,andLinTan.2019. CRADLE:
Cross-BackendValidationtoDetectandLocalizeBugsinDeepLearningLibraries.
InProceedingsofthe41stInternationalConference onSoftware Engineering,ICSE
2019, Montreal, QC, Canada, May 25-31, 2019 . IEEE / ACM, 1027ś1038. https:
//doi.org/10.1109/ICSE.2019.00107
[27]GagandeepSingh,TimonGehr,MarkusPüschel,andMartinVechev.2019. An
AbstractDomainforCertifyingNeuralNetworks. Proc.ACMProgram.Lang. 3,
POPL, Article41(Jan. 2019),30pages. https://doi.org/10.1145/3290354
[28]Gagandeep Singh, Timon Gehr, Markus Püschel, and Martin T. Vechev. 2019.
Boosting Robustness Certification of Neural Networks. In Proceedings of the 7th
InternationalConferenceonLearningRepresentations,ICLR2019,NewOrleans,LA,
USA, May 6-9,2019 .https://openreview.net/forum?id=HJgeEh09KQ
[29]Youcheng Sun, XiaoweiHuang, and Daniel Kroening. 2018. Testing Deep Neural
Networks. CoRRabs/1803.04792(2018). arXiv: 1803.04792 http://arxiv.org/abs/
1803.04792
[30]Youcheng Sun, Min Wu, Wenjie Ruan, Xiaowei Huang, Marta Kwiatkowska,
and Daniel Kroening. 2018. Concolic Testing for Deep Neural Networks. In
Proceedingsofthe33rdACM/IEEEInternationalConferenceonAutomatedSoftware
Engineering,ASE2018,Montpellier,France,September3-7,2018 .109ś119. https:
//doi.org/10.1145/3238147.3238172
[31]YuchiTian,KexinPei,SumanJana,andBaishakhiRay.2018.DeepTest:Automated
Testingof Deep-neural-network-driven Autonomous Cars.In Proceedings of the
40th International Conference on Software Engineering, ICSE 2018, Gothenburg,
Sweden,May27-June03,2018 .303ś314. https://doi.org/10.1145/3180155.3180220
[32]Shiqi Wang, Kexin Pei, Justin Whitehouse, Junfeng Yang, and Suman Jana.
2018. Formal Security Analysis of Neural Networks using Symbolic Inter-
vals. InProceedings of the 27th USENIX Security Symposium, USENIX Security
2018,Baltimore,MD,USA,August15-17,2018 .USENIXAssociation,1599ś1614.
https://www.usenix.org/conference/usenixsecurity18/presentation/wang-shiqi
[33]Jie M. Zhang, Mark Harman, Lei Ma, and Yang Liu. 2019. Machine Learn-
ing Testing: Survey, Landscapes and Horizons. CoRRabs/1906.10742 (2019).
arXiv:1906.10742 http://arxiv.org/abs/1906.10742
[34]YuhaoZhang,YifanChen,Shing-ChiCheung,YingfeiXiong,andLuZhang.2018.
AnEmpiricalStudyonTensorFlowProgramBugs.In Proceedingsofthe27thACM
SIGSOFTInternationalSymposiumonSoftwareTestingandAnalysis,ISSTA2018,
Amsterdam, The Netherlands, July 16-21, 2018 . 129ś140. https://doi.org/10.1145/
3213846.3213866
837