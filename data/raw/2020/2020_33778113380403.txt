An InvestigationofCross-Project Learningin Online
Just-In-
TimeSo/f_twareDefect Prediction
SadiaTabassum
sxt901@cs.bham.ac.uk
Universityof Birmingham,UKLeandroL. Minku∗
L.L.Minku@cs.bham.ac.uk
Universityof Birmingham,UKDanyi Feng
danyi@ouchteam.com
XiliuTech,China
George G.Cabral
george.gcabral@ufrpe.br
Federal RuralUniversityof
Pernambuco,BrazilLiyanSong
L.Song.1@cs.bham.ac.uk
Universityof Birmingham,UK
ABSTRACT
Just-In-Time Software Defect Prediction (JIT-SDP) is concerned
withpredictingwhethersoftwarechangesaredefect-inducingor
cleanbasedonmachinelearningclassiﬁers.Buildingsuchclassiﬁers
requires a suﬃcient amount of training data that is not available
at the beginning of a software project. Cross-Project (CP) JIT-SDP
can overcome this issue by using data from other projects to build
the classiﬁer, achieving similar (not better) predictive performance
toclassiﬁerstrainedon Within-Project(WP) data.However,such
approacheshaveneverbeeninvestigatedinrealisticonlinelearning
scenarios, where WP software changes arrive continuously over
time and can be used to update the classiﬁers. It is unknown to
what extent CP data can be helpful in such situation. In particular,
it is unknown whether CP data are only useful during the very
initialphaseoftheprojectwhen thereislittleWP data,orwhether
they could be helpful for extended periods of time. This work thus
providestheﬁrstinvestigationofwhenandtowhatextentCPdata
are useful for JIT-SDP in a realistic online learning scenario. For
that, we develop three diﬀerent CP JIT-SDP approaches that can
operateinonlinemodeandbeupdatedwithbothincomingCPand
WPtrainingexamplesovertime.Wealsocollect2048commitsfrom
threesoftwarerepositoriesbeingdevelopedbyasoftwarecompany
overthecourseof9to10months,anduse19,8468commitsfrom
10 active open source GitHub projects being developed over the
course of 6 to 14 years. The study shows that training classiﬁers
withincomingCP+WPdatacanleadtoimprovementsinG-mean
ofupto53.90%comparedtoclassiﬁersusingonlyWPdataatthe
initial stage of the projects. For the open source projects, which
have been running for longer periods of time, using CP data to
supplementWPdataalsohelpedtheclassiﬁerstoreduceorprevent
large drops in predictive performance that may occur over time,
leading to up to around 40% better G-Mean during such periods.
∗Corresponding author.
Permissionto make digitalor hard copies of allorpart ofthis work for personalor
classroom use is granted without fee provided that copies are not made or distributed
forproﬁtorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation
ontheﬁrstpage.Copyrights forcomponentsofthisworkownedbyothersthanthe
author(s)mustbehonored.Abstractingwithcreditispermitted.Tocopyotherwise,or
republish,topostonserversortoredistributetolists,requirespriorspeciﬁcpermission
and/or afee. Request permissionsfrom permissions@acm.org.
ICSE’20, May 23–29,2020, Seoul, Republic ofKorea
©2020 Copyright heldby the owner/author(s). Publication rightslicensed to ACM.
ACM ISBN 978-1-4503-7121-6/20/05...$15.00
https://doi.org/10.1145/3377811.3380403SuchuseofCPdatawasshowntobebeneﬁcialevenafteralarge
numberofWPdatawerereceived,leadingtooverallG-meansup
to 18.5% betterthanthoseof WP classiﬁers.
KEYWORDS
Softwaredefectprediction,cross-projectlearning,transferlearning,
online learning,veriﬁcation latency, concept drift, class imbalance
ACMReference Format:
SadiaTabassum,LeandroL.Minku,DanyiFeng,GeorgeG.Cabral,andLiyan
Song.2020.AnInvestigationofCross-ProjectLearninginOnlineJust-In-
TimeSoftwareDefectPrediction.In 42ndInternationalConferenceonSoft-
wareEngineering(ICSE’20),May23–29,2020,Seoul,RepublicofKorea. ACM,
NewYork, NY, USA,12pages.https://doi.org/10.1145/3377811.3380403
1 INTRODUCTION
The primary objective of software quality assurance activities is
to reduce the number of defects in software products [ 20]. It is a
challenging problem considering the limitation of budget and time
allocationforsuchactivities.SoftwareDefectPrediction(SDP)helps
toreducethetimeandeﬀortrequiredfortestingsoftwareproducts.
DiﬀerentmachinelearningapproacheshavebeenproposedforSDP
[11]. Manystudieshave focusedon identifying defect prone com-
ponents(e.g.,modulesorﬁles)[ 10].Recentstudiesinthisareahave
beenincreasinglyfocusingonidentifyingdefect-inducingsoftware
changes. This is known as Just-In-Time Software Defect Prediction
(JIT-SDP)[ 16].AdvantagesofJIT-SDPovercomponentlevelSDP
include [16]: (1) prediction made at an early stage, facilitating code
inspection, (2) ﬁner granularity of the predictions, making it easier
to ﬁnd defects, and (3) straightforward allocation of developers to
inspectthe code.
SimilartoSDPatthemodule/ﬁlelevel,JIT-SDPclassiﬁersrequire
suﬃcientamountoftrainingdatawhichisnotavailableduringthe
initial phase of a project. To overcome this problem, previous work
has proposed Cross-Project (CP) JIT-SDP, where historical data
from other projects are used to train the classiﬁer [ 15]. Existing
CP JIT-SDP work [ 15] assumes an oﬄine learning scenario, where
classiﬁersarebuiltbasedonapre-existingtrainingsetandnever
updatedanymore.Thismeansthattheclassiﬁerisneverupdated
with Within-Project (WP) data. However, in practice, JIT-SDP is
anonline learning problem [ 2],where bothadditional CP and WP
training examples arrive over time.
TheroleofCPdatainsuchonlinelearningscenarioisunclear.
CP JIT-SDPhas never been investigatedin online mode before. In
5542020 IEEE/ACM 42nd International Conference on Software Engineering (ICSE)
ICSE ’20, May 23–29, 2020,Seoul,Republicof Korea S. Tabassum,L.L.Minku,D. Feng,G.G.Cabral,L.Song
particular,itisunknownwhetherCPdataisonlyhelpfulatthevery
early stages of the project when there is little WP data, or it brings
a prolonged beneﬁt to the predictive performance ofthe classiﬁer.
For instance, it may be that CP approaches using an augmented
training data stream formed by both WP and CP examples lead
to increased predictive performance even at later stages of the
project, given that classiﬁers are built using more data than WP
classiﬁers trained only with WP data. Or, it may be that CP data
causesuchapproachestoobtainworsepredictiveperformancethan
WPclassiﬁersonceenoughWPdataisusedfortraining.Besides,
predictionqualitycanﬂuctuateduetovariations(conceptdrifts)in
the underlying defect generating process [ 19]. Concept drift can
causethepredictiveperformanceofJIT-SDPclassiﬁerstodrop[ 19].
TheuseofCPtrainingdatacouldpotentiallyhelptohandleconcept
drift.ThisisspeciallythecaseconsideringthatJIT-SDPisaclass
imbalance problem, where the number of defect-inducing software
changes is typically much smaller than that of clean changes. In
such typeof problem,itwouldtake alotof timeto collect newWP
defect-inducing examples to recover well from concept drift. CP
trainingexamplescouldpotentiallyhelptorecoverfromconcept
drift more quickly.
Thispaperthusaimsatinvestigatingwhenandtowhatextent
CPJIT-SDPdatacanbehelpfulinarealisticonlinelearningscenario.
Itanswers the following ResearchQuestions (RQs):
RQ1Can CP data help to improve predictive performance in the
initial phase of a project, when there is no or little WP train-
ing data available? For howlongandto what extent?
RQ2Can CP data help to prevent sudden drops in predictive
performance, which may be caused by concept drift? To
what extent?
RQ3What is the overall eﬀect of using CP and WP data together
on the predictive performance throughout the development
ofsoftwareprojects?Inparticular,doclassiﬁerstrainedon
bothCP andWP data improve, deteriorateor retain similar
overallpredictive performance to WP classiﬁers?
It is worth noting that, in online scenarios, both WP and CP
training data arrive over time as their collection can be automated.
If incoming CP data is used for training a CP classiﬁer, there is
no reason to exclude incomingWP data from the training process
of this classiﬁer, as such WP data should not hurt its predictive
performance. Therefore, we use the term CP when referring to
onlinelearningapproachesthatmakeuseofbothincomingCPand
WPdatafortraining.Thismeansthat,whenwerefertothebeneﬁts
ofCPdata,wedonotmeanthebeneﬁtsofCPdatainisolation,but
the beneﬁts of CP data used along with incoming WP data. We use
the term WP when referring to approaches that only use WP data
for training.
To answer the above research questions, this paper investigates
threeCPonlinelearningapproaches:(1)asingleonlinelearning
classiﬁer trained on incoming WP and CP training examples, (2)
an online learning ensemble where each classiﬁer is trained on
incomingtrainingexamplesfromadiﬀerentproject,and(3)asingle
online learning classiﬁer that ﬁlters out CP trainingexamples that
are likely to be very diﬀerent from the recent WP examples. These
approachesareenhancementsoftheapproachesusedintheJIT-SDP
literature[ 15],sothattheycanoperateinonlinemode.TheyarecomparedagainstonlineWPapproaches.Ourexperimentsbased
on13softwarerepositoriesshowthattheﬁrstandthirdapproaches
arehelpfultoimprovepredictiveperformanceinJIT-SDPcompared
to WP classiﬁers, whilethe secondisnot.
The contributionsofthis work are the following:
•This paper provides the ﬁrst investigation of CP JIT-SDP in
arealistic onlinelearningscenario.
•We show how to adapt CP JIT-SDP approaches so that they
can be usedin an onlinelearningscenario.
•WerevealthattheuseofCPdatacombinedwithWPdata
canimproveoverallpredictiveperformance(ratherthanjust
achievingsimilarpredictiveperformance)comparedtoWP
learningin JIT-SDP.
•We show that CP data can be helpful for prolonged periods
of time, rather than only in the beginning of the learning
periodas assumedin previous work.
•We show that CP data can reduce the negative eﬀect of sud-
den predictive performance drops in the classiﬁer, resulting
in more reliable predictions over time.
•WeshowthatitisbettertousebothCPandWPdatatogether
tobuildasingleclassiﬁer,ratherthancreatingdiﬀerentclas-
siﬁers using disjointsubsetsof the data.
This paper is further organized as follows. Section 2 presents
related work.Section 3introducesour online JIT-SDPapproaches.
Section4presentsthedetailsoftheinvestigateddatasets.Section5
explainstheexperimentalsetupforansweringtheRQs.Section6
explainstheresultsoftheexperiments.Section7presentsthreats
tovalidity.Section8presentstheconclusionsandimplicationsof
this work.
2 RELATED WORK
TherearemanySDPstudies[ 11,18,32],includingrecentstudies
investigating class imbalance techniques [ 1], automated feature
engineering [ 17], ensemble learning [ 34], among others. In this
section,wediscussthreemainresearchareasofSDPthatareclosely
relatedtothiswork:CPSDPatthecomponentlevel(Section2.1),
CP JIT-SDP(Section 2.2) andOnline JIT-SDP(Section 2.3).
2.1 CP SDPat theComponent Level
There have been several studies on CP SDP at the component level.
An initial study provided guidelines for choosing training projects
[35]. They proposed an approach to identify factors that inﬂuence
CP prediction success, such as data and process factors. Another
study [13] showed that carefully selected CP training data may
provide better prediction results than training data from the same
project.Petersetal. [ 12]focusedon selecting suitableCP training
databasedonthesimilaritiesbetweenthedistributionofthetest
and potential training data. In particular, they used similarity mea-
suring and feature subset selection to remove irrelevant training
data. Canfora et al. [ 3] proposed a multi-objective approach for
CP defect prediction. They attempted to achieve a compromise
between amount of code to inspect and number of defect-prone ar-
tifacts.ThisapproachperformedbetterthanWPmodels.Panichella
et al. [24] analysed the equivalence of diﬀerent defect predictors
and proposed a combined approach CODEP (COmbined DEfect
Predictor) that uses machine learning to combine diﬀerent and
555AnInvestigation of Cross-ProjectLearning in Online Just-In-TimeSo/f_tware DefectPrediction ICSE ’20, May 23–29, 2020,Seoul,Republic of Korea
complementary classiﬁers. This combination performs better than
the stand-alone CP technique. Nam et al. [ 23] applied Transfer
Component Analysis (TCA) to CP SDP. TCA is a transfer learn-
ingapproachthatmapsthedatatoacommonlatentspacewhere
CP and WP data are similar to each other. They also proposed a
newapproachcalledTCA+,whichselectssuitablenormalisation
options for TCA. Other studies [ 14,26,27] consider class imbal-
ance learning for CP SDP. For instance, Ryu et al. [ 26] proposed
an approach that uses similarity weight drawn from distributional
characteristicsandtheasymmetricmisclassiﬁcationcosttobalance
imbalanceddistributions.
Overall, these studies demonstrate that data distributional char-
acteristicsareimportantforCPSDP.Inparticular,theyproposed
approachestoselectCPdatathataresimilartoWPdata,ortomap
CPandWPdataintoalatentspacewheretheyaresimilar.However,
none of these studies were in the context of JIT-SDP or online SDP.
2.2 CP JIT-SDP
The ﬁrst CP JIT-SDP study was done by Kamei et al. [ 15]. They
carried out an empirical evaluation of the JIT-SDP performance by
using data from 11 open source projects. They investigated ﬁve CP
JIT-SDP approaches based on project similarity, three variations of
data merging approaches, and ensemble approaches where each
model was trained on data from a diﬀerent project. All approaches
were based on random forests as base learners. They found that
simplemergingofallCPdataintoasingletrainingsetandensemble
approaches obtained similar predictive performance to that of WP
models. Diﬀerent from SDP at the component level, other more
complex approaches, including similarity-based approaches, did
not oﬀer any advantagecomparedto these.
Another study [ 4]investigated CP JIT-SDP in mobile platforms
using14appsand42,543commitsextractedfromtheCommit.Guru
platform [ 25]. They compared CP performance of four diﬀerent
well-known classiﬁers and fourensemble techniques. Naive Bayes
performedbestcomparedtootherclassiﬁersandsomeensemble
techniques.
Chen et al. [ 5] considered JIT-SDP as a multi-objective problem
tomaximisethenumberofidentiﬁeddefect-inducingchangeswhile
minimising the eﬀort required to ﬁx the defects. They proposed
a multi-objective optimization-based supervised method called
MULTI to build logistic regression-based JIT-SDP models. They
usedsixopensourceprojects.MULTIwasevaluatedonthreedif-
ferentmodelperformanceevaluationscenarios(cross-validation,
cross-project-validation, and timewise-cross-validation) against 43
state-of-the-art supervised and unsupervised methods. They found
that itcan perform signiﬁcantly better comparedto WP methods.
DespiteshowingthatCPJIT-SDPcanobtainpromisingresults
comparedtoWPJIT-SDP,noneofthestudiesaboveconsidereda
realistic onlinelearningscenario.
2.3 Online JIT-SDP
Fewstudies explored JIT-SDP in online mode. McIntosh et al.[ 19]
performed a longitudinal case study of 37,524 changes from the
rapidly evolving QT and OPENSTACK systems and found that
ﬂuctuationsinthepropertiesofﬁx-inducingchangescanimpactthe
performance of JIT models. They showed that JIT models typicallylose power after one year. Hence, the JIT model should be updated
withmore recent data.
Tan et al. [ 29] investigated JIT-SDP in a scenario where new
batchesoftrainingexamplesarrive overtimeand canbeused for
updatingthepredictivemodels,usingoneproprietaryandsixopen
source projects. They considered the fact that the labels of training
datamayarrivemuchlaterthanthecommittime.Thisisknown
asveriﬁcationlatency in the machine learningliterature [ 7].They
used resampling techniques to deal with the class imbalanced data
issueandupdatableclassiﬁcationtolearnovertime.However,their
approach assumes that there is no concept drift, i.e., that the defect
generatingprocessdoes not suﬀer variations over time.
Cabral et al. [ 2] proposed a method called Oversampling Online
Bagging (ORB) to tackle class imbalance evolution in an online
JIT-SDP scenario taking veriﬁcation latency into account. Class im-
balanceevolutionisatypeofconceptdriftwheretheproportionof
defect-inducing and clean examples ﬂuctuates over time. ORB has
anautomaticallyadjustableresamplingratetotackleclassimbal-
ance evolution, being able to improve predictive performance over
JIT-SDPapproachesthat assumeaﬁxedlevel of class imbalance.
None ofthe onlineJIT-SDPstudiesinvestigatedCP JIT-SDP.
3 ONLINECP JIT-SDPAPPROACHES
Inthissection,wemodifyandenhancethreeCPJIT-SDPapproaches
adopted in [ 15] to enable them to be applied to online JIT-SDP. All
ouralgorithmsfullyrespectchronology.Inparticular,theynever
usefutureCP/WPtrainingexamples,futureknowledgeaboutla-
bels (i.e., defect-inducing or clean), or test examples, for training a
modelusedfor testingthe present.
Training examples are generated using the online procedure
recommendedbyCabraletal.[ 2]totakeveriﬁcationlatencyinto
accountforallapproachesstudiedinthispaper.Asoftwarechange
becomes a training example either when a defect is found to be
associated to it, or once a pre-deﬁned waiting period whas passed,
whichever is earlier. This waiting period represents the amount of
timethatittakesforonetobeconﬁdentthatthechangeinquestion
isclean.Inotherwords,ifnodefectisfoundtobeassociatedtothe
softwarechangeduringthewaitingperiod,atrainingexampleof
the clean class is created to represent this software change. Oth-
erwise,atrainingexampleofthedefect-inducingclassiscreated
immediatelyafterthedefectisfound.If,afterthewaitingperiod,
adefectisfoundtobeassociatedtoachangethatwaspreviously
consideredclean,anewdefect-inducingtrainingexampleiscreated
forit.Trainingexamplesareusedtoupdatetheclassiﬁerassoon
as they are created.
It is worth noting that, for all our CP approaches, classiﬁers can
be trained not only with CP and WP training examples that are
madeavailableovertime afterthe ﬁrstWP commit,butalso with
CP training examples producedbefore the ﬁrstWP commit.
3.1 All-in-OneApproach
Existing oﬄine JIT-SDP work [ 15] assume that CP classiﬁers are
created only with CP data. Unlike oﬄine approaches, the All-in-
OneonlineapproachcanusebothCPandWPdatafortraining.All
incomingCPandWPtrainingdataareconsideredaspartofasingle
datastreamoftrainingexamples,whichareusedtotrainasingle
556ICSE ’20, May 23–29, 2020,Seoul,Republicof Korea S. Tabassum,L.L.Minku,D. Feng,G.G.Cabral,L.Song
Algorithm 1 All-in-One Approach
1:S=streamofincomingchangesfromseveralprojects, b=index
identifying the target project, w=waiting period
2:Initialise predictive model m
3:foreachincoming change xtp∈Sdo//xtpis a change arriving
from project pat timestamp t
4:ifp=bthen
5: ˆ/y.alt←pr
edict(m,xtp)
6:end if
7:storextpin aqueue WFL-Q //WFL-Qis aqueueof
incomingexamples waiting to be usedfor training
8:foreach itemqiinWFL-Qdo
9: ifadefectwaslinkedto qiat atimestamp≤tthen
10: create adefect-inducing trainin/afii10069.ital_exampleforqi
11: train(m,trainin/afii10069.ital_example)
12: removeqifromWFL-Q
13: else
14: ifqiis olderthan wthen
15: create aclean trainin/afii10069.ital_exampleforqi
16: train(m,trainin/afii10069.ital_example)
17: removeqifromWFL-Q
18: storetrainin/afii10069.ital_exampleinCL-H//CL-His a
hash of cleantraining examples
19: end if
20: end if
21:end for
22:ifadefectwaslinkedtoa trainin/afii10069.ital_exampleinCL-Hata
timestamp≤tthen
23: Swap the label of trainin/afii10069.ital_example to defect-inducing
24: train(m,trainin/afii10069.ital_example)
25: removetrainin/afii10069.ital_examplefromCL-H
26:end if
27:end for
onlineclassiﬁerassoonastheyareproduced.Thedatastreamis
in chronological order, i.e., the training examples are sorted based
on the unix timestamp of their creation. Algorithm 1 shows the
pseudocode for the All-in-One approach.
Thepredictivemodelisinitialisedasanemptymodelthatalways
predicts “clean”. When a new incoming change xtpis received at
timestep t(line3),thealgorithmﬁrstchecksifthischangebelongs
to the target project, i.e., to the project whose changes are being
predicted(line4).Ifitdoes,apredictionisprovidedforthischange.
After that, xtpis stored in a queue for a pre-deﬁned waiting period
(line 7). All changes in this queue are checked to see whether they
can be used for training (line 8 to 21). If a defect is found to be
associatedtoagivenchangeinthequeueduringthewaitingperiod
(lines 9 to 12), a defect-inducing training example is created to
represent thischange used for training.If a defectis not found by
the end of the waiting period of a given change (lines 13 to 20),
a clean training example is created for this change and used for
training. After that the change is removed from the queue. Thealgorithmalsocheckswhetherthereisanypastchangefoundto
bedefect-inducing,butthatwaspreviouslyconsideredasaclean
training example (lines 22 to 26). If there is, the classiﬁer is trained
using that changeas adefect-inducingtraining example.
The key diﬀerence between the proposed approach and the data
mergingapproachesusedbyKameietal.[ 15]isthat,in[ 15],the
learningwasoﬄine(withouttakingintoaccountincomingtraining
examplesand veriﬁcation latency)and theyused only CP data for
training. In the proposed approach, the learning is online, takes
veriﬁcation latency into account, and the classiﬁer is trained on
bothCPandWPdatawhoselabelsareproducedbeforethecurrent
timestamp.
3.2 Ensemble Approach
The Ensemble approach uses an ensemble of classiﬁers rather than
asingleclassiﬁer.Aseparateclassiﬁerisbuiltfromeachproject’s
separatetrainingdatastream(e.g.,for10projectstherewillbe10
diﬀerentclassiﬁers).ThisincludesbothCPandWPdatastreams.
Eachchangebelongingtothetargetprojectisthenpredictedbyall
theclassiﬁers,andthemeanofthepredictedprobabilitiesretrieved
bytheclassiﬁersiscalculated.Thismeanisusedtopredictwhether
the change is clean or defect-inducing. The pseudocode for the
EnsembleapproachissimilartothatoftheAll-in-Oneapproach,
and can be found in the supplementary material [ 28]. As with
the All-in-One approach, the chronological order of the training
examples is alwaysrespected.
Inthepreviousoﬄineensembleapproach[ 15],Kameietal.in-
vestigated both simple voting ensembles (where equal weight is
given to each classiﬁer) and weighted voting ensembles (where
moreweightisgiventoclassiﬁerstrainedonprojectsthataremore
similar to the target project). They showed that weighted voting
did not oﬀer any advantage over simple voting. Hence, our on-
line ensemble approach uses simple voting. The key diﬀerences
betweenourapproachandtheapproachusedbyKameietal.[15]
are that our approach is online, and our ensemble contains a classi-
ﬁer built from WP training examples that have been labelled up to
the currenttimestamp,ratherthanusing only CP data classiﬁers.
3.3 FilteringApproach
Eventhoughﬁlteringdidnotimprovepredictiveperformancein
oﬄine JIT-SDP [ 15], ﬁltering strategies have shown to be very
beneﬁcial in oﬄine SDP at the component level [ 30]. Therefore,
we investigate whether ﬁltering out software changes that are
dissimilarto the target changes could be useful in onlineJIT-SDP.
WeproposedthefollowingFilteringapproachforonlineJIT-SDP.
First, a ﬁxed-size window of most recent incoming WP training
examplesismaintained.WheneveraCPtrainingexamplearrives,it
iscomparedwiththetrainingexamplesintheWPwindow.Aswith
theAll-in-OneandEnsembleapproaches,thechronologicalorder
ofthetrainingexamplesisalwaysrespected.Euclideandistances
between the input features of the CP training example and each of
the WPtrainingexamplesin thewindowthat have the samelabel
as the CP training example are calculated to check how similar the
CPtrainingexampleistorecentWPexamples.Itisimportanttouse
onlytrainingexampleswiththesamelabeltocomputethedistance.
If the labels had been ignored, the approach would consider that a
557AnInvestigation of Cross-ProjectLearning in Online Just-In-TimeSo/f_tware DefectPrediction ICSE ’20, May 23–29, 2020,Seoul,Republic of Korea
Algorithm 2 Filtering Approach
1:S=streamofincomingchangesfromseveralprojects, b=index
identifying the test project, w= waiting period, windowSize =
size of the WP-Qsliding window, K= number of top short dis-
tances to be used, maxDist = distance threshold for similarity,
cpqSize= maximum size of the queue CP-Qof dissimilar CP
instancesto be re-checkedfor similarityinthe future
2:Initialisepredictive model m
3:foreachincoming change xtp∈Sdo//xtpis a change arriving
from project pat timestamp t
4:ifavgDist(trainin/afii10069.ital_example,WP-Q,K)≤maxDist forany
trainin/afii10069.ital_exampleinCP-Qthen
5: train(m,trainin/afii10069.ital_example)
6: removetrainin/afii10069.ital_examplefromCP-Q
7:end if
8:ifp=bthen
9: ˆ/y.alt=
predict(m,xtp)
10:end if
11:storextpin aqueue WFL-Q //WFL-Qis aqueueof
incomingexamples waiting to be usedfor training
12:foreach itemqiinWFL-Qdo
13: ifadefectwaslinkedto qiat atimestamp≤tthen
14: Create adefect-inducing trainin/afii10069.ital_exampleforqi
15: else
16: ifqiis olderthan wthen
17: Create aclean trainin/afii10069.ital_exampleforqi
18: storeqiinCL-H//CL-His ahash of clean
training examples
19: end if
20: end if
21: ifatrainin/afii10069.ital_examplewascreated for qithen
22: ifavgDist(trainin/afii10069.ital_example,WP-Q,K)≤maxDist
orthis isaWP change then
23: train(m,trainin/afii10069.ital_example)
24: else
25: Addtrainin/afii10069.ital_exampletoCP-Q
26: end if
27: RemoveqifromWFL-Q
28: SlideWP-Qiftrainin/afii10069.ital_exampleisWP
29: end if
30:end for
31:ifadefectwaslinkedtoa trainin/afii10069.ital_exampleinCL-Hbefore
timetthen
32: swaplabel of trainin/afii10069.ital_exampleto defect-inducing
33: removetrainin/afii10069.ital_examplefromCL-H
34: ifavgDist(trainin/afii10069.ital_example,WP-Q,K)≤maxDist
then
35: train(m,trainin/afii10069.ital_example)
36: else
37: addtrainin/afii10069.ital_exampletoCP-Q
38: end if
39:end if
40:end forCP clean trainingexample described by similar features as a WP
defect-inducing training example are similar training examples.
However,they are diﬀerentdueto the diﬀerentlabel.
The average of the smallest K distances is calculated. If this
averagedistanceisequaltoorlowerthanamaximumthreshold,
theCPtrainingexampleisallowedtotraintheclassiﬁer.Discarded
CPtrainingexamplesarekeptinaﬁxed-sizedqueue.Thisqueue
is checkedin every iterationto see whetherany old discardedCP
trainingexamplehasnowbecomesuitablefortraining.Thiscan
beusefulincaseconceptdriftcausessuchdiscardedexamplesto
become relevant.
Algorithm2showsthe pseudocodeforthe Filteringapproach.
The predictivemodel is initialisedasan empty modelthat always
predicts “clean”. When a new incoming change xtpis received at
time step t(line 3), the algorithm checks whether there are any old
CPtrainingexamplesthatwerepreviouslynotusedfortrainingdue
to their dissimilarity to WP examples, but that are now suitable for
trainingduetotheirsimilaritytothecurrentWPslidingwindow
(line 4 to 7). Then, a prediction is given if the change xtpbelongs
to target project (line 8 to 10). The change xtpis then stored in a
queue (line 11), waiting to be labelled. All changes in this queue
arecheckedtoseewhethertheycanbelabelled(lines12to30).If
theycan,correspondingtrainingexamplesarecreatedandusedfor
training only if they are similar enough to the WP sliding window
(lines21to23).IftheyarenotsimilarenoughandareCPexamples,
they are stored in the queue CP-Qof discarded CP examples for
possiblefutureuse(line25).Theslidingwindowisupdated(slided)
ifthetrainingexampleisWP(line28).Thealgorithmalsochecks
whetheranychangethatwaspreviouslyconsideredascleanhas
now been associated to a defect and uses it for training, but only if
itis similar enough to the WP slidingwindow(line31 to 39).
4 DATASETS
Wehaveextracteddatafromthreeproprietarysoftwaredevelop-
ment project repositories from a Chinese software development
companyforthepurposeofthisstudy.Wehavealsousedtenex-
isting datasets extracted from open source GitHub projects, which
weremadeavailablebyCabraletal.[ 2]athttps://zenodo.org/record/
2594681. Some information on the datasets is shown in Table 1. All
datasetswereextractedbasedonCommitGuru[ 25].Thechange
metrics include 14 metrics that can be grouped into ﬁve types of
metrics:i)diﬀusionofthechange,ii)sizeofthechange,iii)purpose
of the change, iv) history of the change, and v) experience of the
developerthatmade thechange.These changemetricshave been
shownto be adequate for JIT-SDPinprevious work [16].
Itisworthnotingthatthereisevidenceofconceptdriftthatcan
be attributed to software engineering in the datasets. For instance,
inTomcat,thenumberofdeveloperschangingthemodiﬁedﬁles
associated to a commit increases as the project matures during the
ﬁrst 16,000 commits, then drops. Upon dropping, new changes are
usually clean, diﬀerent from old ones with similar changemetrics.
To collect the proprietary data for this study, all commit mes-
sagesfromthethreerepositorieswereextractedusingthegitlog
command.AChineselanguagenativespeakerknowledgeableof
programmingwasaskedtoreadthecommitmessagestoidentify
keywordsthatcanbeusedtoidentify corrective commits.Keywords,
representativecommitmessagesofcorrectiveandnon-corrective
558ICSE ’20, May 23–29, 2020,Seoul,Republicof Korea S. Tabassum,L.L.Minku,D. Feng,G.G.Cabral,L.Song
Table 1: An overview ofthe projects
Project Total # Defect-inducing %Defect-inducing Median Defect Time Period MainLanguage ProjectType
Changes Changes Changes Discovery Delay(days)
Tomcat 18907 5207 27.54 200.5798 27-03-2006-06-12-2017 Java Opensource[2]
JGroups 18325 3153 17.21 116.1565 09-09-2003-05-12-2017 Java Opensource[2]
Spring-integration 8750 2333 26.66 415.1201 14-11-2007-16-01-2018 Java Opensource[2]
Camel 30575 6255 20.46 28.1947 19-03-2007-07-12-2017 Java Opensource[2]
Brackets 17364 4047 23.31 14.454 07-12-2011-07-12-2017 JavaScript Opensource[2]
Nova 48989 12430 25.37 88.5615 28-05-2010-28-01-2018 Python Opensource[2]
Fabric8 13106 2589 19.75 39.1833 13-04-2011-06-12-2017 Java Opensource[2]
Neutron 19522 4607 23.6 82.5097 01-01-2011-27-12-2017 Python Opensource[2]
Npm 7920 1407 17.77 111.514 29-09-2009-28-11-2017 JavaScript Opensource[2]
BroadleafCommerce 15010 2531 16.86 42.5818 19-12-2008-21-12-2017 Java Opensource[2]
C1 1030 conﬁdential conﬁdential 35.81 06-09-2018-17-07-2019 JavaScript Proprietary
C2 601 conﬁdential conﬁdential 1.98 16-10-2018-19-07-2019 JavaScript and 3DStudio Proprietary
C3 417 conﬁdential conﬁdential 2.97 05-09-2018-19-07-2019 Python Proprietary
cases,and commit messagesfor whichthe native speaker wasun-
certain about were stored in a separate ﬁle. This gave a total of
57 commit messages. The second author of this paper then went
throughtheﬁleprovidinghisindependentclassiﬁcationofthecom-
mits as corrective or non-corrective, by using Google Translate to
translatethecommitmessages.UnclearGoogletranslationswere
discussedwiththenativespeaker.Afterthat,thenativespeakerand
the second author met to discuss all 57 commit messages. Commit
messagesforwhichtherewasstillsomedoubtafterthisdiscussion
werefurtherdiscussedwiththe Company, whoconﬁrmedwhether
theywerecorrectiveornon-corrective.Thenativespeakerandsec-
ond author then agreed on a list of keywords to identify commits,
which was presented to the Company. The Company conﬁrmed
that the listof keywordswasadequate for their projects.
ThelistofkeywordswasusedasinputforCommitGurutoiden-
tify corrective commits, which were then used to identify which
changes are defect-inducingorclean [ 25],to generate thedata sets.
Asadataqualityassuranceprocedure,allcommitmessagesconsid-
ered by Commit Guru as corrective and a sample of non-corrective
commitmessagesweredouble checkedbythenativespeaker,giv-
ing a total of 1,230 commit messages. There was a disagreement
between Commit Guru’s classiﬁcation and the native speaker in
only 3 cases, and the native speaker was unsure of the correct clas-
siﬁcation in 7 cases. Therefore, Commit Guru’s classiﬁcation of
commits as corrective and non-corrective was deemed appropriate.
5 EXPERIMENTAL SETUP
TheRQsintroducedinSection1willbeansweredbycomparingthe
predictiveperformanceoftheAll-in-One,EnsembleandFiltering
approachesagainst WPlearning.Theanalysis doneforRQ1,RQ2
and RQ3 will concentrate on the predictive performance (1) in
the beginning of the projects, (2) during periods of time where
we can observe sudden drops in predictive performance of the
WPapproach,and(3)averageacrosstimesteps,respectively.We
deﬁneatimestepasasequentialnumberindicatingtheorderof
WP commits. Each WP commit requires a WP software changeto
be predicted as defect-inducing or clean. Due to the poor results
obtainedbytheEnsembleapproachontheopensourcedata,this
approach wasnot run for the proprietary data.
Given an open source project repository, all other 9 open reposi-
toriesareconsideredastheCPdata.Givenaproprietaryrepository,
two cases were considered: (1) the other 2 proprietary repositories
are the CP data,and(2) all other12 repositories are the CP data.JIT-SDPisaclassimbalanceproblem[ 2,15,16,29],andtheopen
sourcedatausedinthisstudyareknowntobeclassimbalanced[ 2].
Therefore,learningapproachesneedtouseonlinelearningclassi-
ﬁers that can deal with this issue. Two state-of-the-art approaches
foronlineclassimbalancedlearningareImprovedOversampling
OnlineBagging(OOB)andImprovedUndersamplingOnlineBag-
ging(UOB)[ 33].Also,Cabraletal.[ 2]proposedanewapproach
called Oversampling Rate Boosting (ORB), which improves the pre-
dictive performance further for JIT-SDP. The three approaches are
ensemblesofHoeﬀdingTrees[ 8].Theseareonlinelearningbase
classiﬁers,whichareupdatedincrementallywitheachnewtraining
example,preservingoldknowledgewithoutrequiringstorageofold
examples. Previously, OOB and UOB achieved similar performance
toJIT-SDP[ 2],andORBperformedthebest.Thus,onlyOOBand
ORB are selectedas the baseclassiﬁers in this study.
Toevaluatetheperformance,we adoptrecall onthe clean class
(Recall0), recall on the defect-inducing class (Recall1) and Geomet-
ric Mean of Recall0 and Recall1 (G-Mean). They were computed
prequentially and using a fading factor to enable tracking changes
inpredictiveperformanceovertime,asrecommendedforproblems
that may suﬀer concept drift [ 9]. If the current example belongs
to classi,Recall(t)
i=θRecall(t−1)
i+(1−θ)1ˆ/y.alt=i, whereiis zero
or one,tis the current time step, θis a fading factor set to 0.99
as in [2],ˆ/y.altis the predicted class, and 1ˆ/y.alt=iis the indicator func-
tion, whichevaluates toone if ˆ/y.alt=iand tozerootherwise.If the
current example does not belong to class i,Recall(t)
i=Recall(t−1)
i.
Also,G-Mean(t)=/radicalBig
Recall(t)
0×R ecall(t)
1. It is worth noting that
Recall0=1−FalseAlarmRate ,andsofalsealarmsaretakeninto
account through both Recall0 and G-Mean. These metrics were
chosenbecausetheyarethemostrecentlyrecommendedforonline
class imbalance learning[33].
Thepredictiveperformancesobtainedduringtheinitialphase
of the projects, and the overall predictive performances calculated
using all time steps will be compared across data sets using the
Scott-Knott procedure [ 22], which ranks the models and separates
them into clusters. This test is used to select the best subgroup
among diﬀerent models. Non-parametric bootstrap sampling is
usedtomakethetestnon-parametric,asrecommendedbyMenzies
et al. [21]. As explained by Demsar [ 6], non-parametric tests are
adequate for comparison across data sets. In addition, the Scott-
KnotttestadoptedinthispaperusesA12eﬀectsize[ 31]toruleout
anysmalldiﬀerencesinperformance.Speciﬁcally,Scott-Knottonly
559AnInvestigation of Cross-ProjectLearning in Online Just-In-TimeSo/f_tware DefectPrediction ICSE ’20, May 23–29, 2020,Seoul,Republic of Korea
performed statistical tests to check whether groups should be sepa-
rated if the A12 eﬀect sizewas mediumor large, as recommended
in[21].IftheA12eﬀectsizewasnotmediumorlarge,groupswere
notseparated.WewillrefertoScott-KnottbasedonBootstrapsam-
pling and A12 as Scott-Knott.BA12. We have also included the A12
eﬀectsizesfor eachdataset individuallyto support the analysis.
Theparametersforthe Filteringapproach werechosenbyper-
forming grid search on the initial portion (1000 commits) of the
datasetsusingthefollowingsetofvalues,whereboldvalueswerese-
lected: windowSize= { 500,600,700,1000}, K= { 50,100,200}, maxDist=
{0.6,0.7,0.8} and cpqSize= { 500, 1000}. Parameters of OOB and ORB
were keptto thesamevaluesasin[ 2]for open source datasets,as
theyhavealreadybeentunedforthesedatasets.Ensemblesizesand
decay factors were further tuned for the proprietary datasets. The
waiting period was 90 for the open source datasets as in [ 2] and
30 for the proprietary datasets, due to their lower defect discovery
delay(seeTable1).Thirtyexecutionsofeachapproachwitheach
of the baseclassiﬁers have been performedoneachdataset.
6 EXPERIMENTALRESULTS
6.1 RQ1: InitialPhaseoftheProject
Wedeﬁnetheinitialphaseoftheopensourceprojectsastheperiod
oftimerangingfromtheﬁrsttimestepuntilthetimestepwhere
theG-MeanoftheWPapproachreachesthevalueofitsaverage
G-Mean across time steps. It represents the time it takes for the G-
Meanofthisapproachtoreachitstypicalvaluesforagivenproject.
Fortheproprietarydata,thetotalnumberoftimestepsistoosmall
to use the average G-Mean across time steps for this purpose. In
particular,hadlongerperiodsoftimebeenobserved,theG-Mean
values would be likely to improve further, given the trends in G-
Meanattheendoftheperiodanalyzedfortheseprojects(seeFig.3,
whichwill be discussed laterin this section).Therefore,instead of
usingtheaverageG-Meanacrossalltimesteps,wehaveusedthe
averageG-Meanacrossthelast40timestepstodeterminetheinitial
phase. Table 2 shows the number of time stepsof theinitial phase
ofallprojects,aswellastheaverageG-Meanofeachapproach,the
eﬀect size A12 against the corresponding WP approach, and the
Scott-Knott.BA12results duringthis period.
The initial phase of the open source projects lasted from 461
to 6271 time steps with a median of 1418, and from 900 to 6271
time steps with a median of 1553 when using OOB and ORB, re-
spectively (Table 2). The average G-Mean of the All-in-One and
Filteringapproacheswasfrequentlyhigherduringtheinitialphase
of the open source projects than that of the WP approach (up to
53.90% higher, for Brackets using All-in-One-ORB). This is further
illustratedbyFigs.1and2,whichshowtheG-Meansacrosstime
steps. The G-Means of the WP classiﬁers were lower than those of
All-in-One and Filteringintheinitial phase ofall plots,except for
Figs. 1b, 1c and 2b, where the G-Means were similar. The superior-
ityofAll-in-OneandFilteringisconﬁrmedbyScott-Knott.BA12,
which shows that these approaches were better ranked than the
othersintermsofG-Mean.A12eﬀectsizesagainstWPlearningfor
individual datasets were typically large. The Ensemble approach
sometimes achieved better G-Means than the WP approach at the
verybeginningoftheprojects,butperformedworsethantheother
CP approaches during most of the initial phase (Table 2). These
(a)Tomcat
 (b) JGroups
(c) Spring-integration
 (d)Camel
(e)Brackets
 (f)Nova
(g)Fabric8
 (h) Neutron
(i) Npm
 (j) BroadleafCommerce
Figur
e 1: G-Mean for all datasets through time using OOB.
Theverticalredbarindicatesthelasttimestepoftheinitial
phase ofthe project, showninTable 2.
results are also supported Scott-Knott.BA12, which shows that the
Ensemble approach wasbetterrankedthanthe WPapproach, but
worse rankedthanAll-in-One andFiltering in terms of G-Mean.
GiventhepromisingresultsoftheAll-in-OneandFilteringap-
proaches,weinvestigatedthemfurtherinthecontextofthepro-
prietarydata.All-in-Onewas investigatedintwodiﬀerentways:
a)combiningbothopensourceandproprietarytrainingdataand
b)onlywithproprietarydata.Theinitialphaseoftheproprietary
projectswastypicallymuchsmallerthanthatoftheopensource
projects, lasting from 81 to 581 time steps and from 82 to 581
timestepsfor OOB and ORB, respectively(Table2). The G-Means
560ICSE ’20, May 23–29, 2020,Seoul,Republicof Korea S. Tabassum,L.L.Minku,D. Feng,G.G.Cabral,L.Song
Table 2: Number of initial time steps of the initial phase, and average G-Means, A12 eﬀect sizes and Scott-Knott.BA12 to
compare learning approacheson this initial phase
OOB ORB
OpenSourceData InitialTimeSteps WPAll-in-One Filtering Ensemble InitialTimeSteps WPAll-in-One Filtering Ensemble
Tomcat 2006 43.61 51.94[b] 52.49[ b] 33.66[-b] 926 40.44 45.82[b] 48.12[b] 35.32[-b]
JGroups 1268 38.638.28[-s] 39.01[ s] 16.64[-b] 1412 30.71 31.47[m] 33.47[b] 15.14[-b]
Spring-integration 461 27.324.27[-b] 38.69[b] 19.48[-b] 900 25.70 60.47[b] 63.43[ b] 42.17[b]
Camel 3112 46.81 57.74[b] 57.06[b] 39.42[-b] 5111 49.68 57.98[ b] 57.63[b] 41.38[-b]
Brackets 1569 21.36 64.88[b] 66.17[b] 46.83[b] 1721 13.49 67.39[ b] 67.20[b] 50.45[b]
Nova 6271 55.42 63.15[b] 64.39[b] 51.66[-b] 6271 52.80 66.43[ b] 65.0[b] 51.84[-s]
Fabric8 795 27.01 51.5[b] 58.63[b] 40.3[b] 1613 29.97 63.33[ b] 62.89[b] 45.19[b]
Neutron 917 44.83 73.55[b] 67.29[b] 55.06[b] 3304 71.78 75.09[ b] 74.84[s] 71.04[-b]
Npm 2536 26.91 48.36[ b] 45.75[b] 42.01[b] 1494 30.12 40.07[b] 43.48[b] 43.52[b]
BroadleafCommerce 677 26.36 50.31[b] 53.16[ b] 37.25[b] 950 27.68 51.81[b] 52.40[b] 41.68[b]
Ranking 4 2 1 3 4 1 1 3
ProprietaryData InitialTimeSteps WPAll-in-One All-in-One Filtering InitialTimeSteps WPAll-in-One All-in-One Filtering
(combined) (proprietary) (combined) (combined) (proprietary) (combined)
C1 347 18.93 38.31[b] 17.28[-b] 39.66[ b] 289 15.93 11.48[-b] 11.57[-b] 11.49[-b]
C2 581 25.03 40.75[b] 32.95[b] 40.23[b] 581 24.48 43.04[b] 36.03[b] 44.01[ b]
C3 81 7.96 13.67[b] 35.98[ b] 13.42[b] 82 8.48 0[-b] 0[-b] 0[-b]
Ranking 2 1 1 1 2 2 2 2
Top G-Means for each dataset are in bold. Symbols [*], [s], [m] and [b] represent insigniﬁcant, small, medium and large A12 eﬀect size against the
corresponding WP approach (WP-OOB or WP-ORB). Presence/absence of the sign “-” in the eﬀect size means that the corresponding approach was
worse/betterthanthecorrespondingWPapproach.Scott-Knott.BA12wasrunforallOOB-andORB-basedapproachestogether.Foreachperformance
metric, one test was run for the open source, and one test was run for the proprietary data results. The groups’ rankings retrieved by Scott-Knott.BA12 are
shown in the rankingrows,withsmaller numbers indicating better rankings.
were also much lower than for the (longer) initial phase of the
open source projects. CP approaches helped to improve average
G-Mean for OOB-based approaches, which is conﬁrmed by the
Scott-Knott.BA12 results and further illustrated in Fig. 3. For in-
stance,inFig.3a,theWPapproachobtainedverylowG-Meanof8%
around time step 290, while All-in-One (combined) and Filtering
obtained a higher G-Mean of around 40%. Interestingly, All-in-One
(combined)ledtobetterresultsthanAll-in-One(proprietary)for
C1andC2whenusingOOBandforC2whenusingORB,indicat-
ing that open source data can sometimes help to improve JIT-SDP
predictions onproprietary data duringthe initialperiod.
However,theuseofCPdatafortheproprietaryprojectswasless
helpful when using ORB-based approaches (see Table 2; plots in
supplementarymaterial[ 28]).Itispossiblethatthewholeperiod
of time analyzed for these projects belongs to the initial phase, and
that the last time step of the initial phase could not be precisely
identiﬁed due to the lack of information on the typical G-Means
thatwouldbeobtainedbytheWPapproachesinprolongedperiods
of time, as was done for the open source data. As shown in Section
6.3,theG-MeansobtainedforORB-basedcombinedCPapproaches
improve when considering the wholeperiodof the projects.
RQ1:CPdatawashelpfulintheinitialphaseoftheprojectwhenthere
was no or little WP training data available, in particular when using
All-in-One and Filtering approaches for the open source projects and
OOB-based approaches for the proprietary projects. This initial phase
lasted from 461 to 6271 and from 81 to 581 time steps for the open
sourceandproprietaryprojects,respectively.Improvementsinaverage
G-Mean were upto 53.90%,avoidingextremely low G-Means.
6.2 RQ2: Periods with Sudden Dropsin WP
Classiﬁer’s
PredictivePerformance
In some datasets, after the initial phase, the WP approach suﬀered
periodsoflargedropsinG-Means.Someclearcasescanbeobservedfrom time steps 1000 to 3000 in Fig. 1c, near time step 25,000 in
Fig. 1d, around time step 7000 in Fig. 1j, around time step 18,000 in
Fig.2a,fromtimestep1000to3000inFig.2c,andaroundtimestep
2100inFig.2i.TheCPapproachesfrequentlymanagedtoreduce
orsometimes even eliminatesuch drops.
For example, in Fig. 2c, we can see that from time steps 1000 to
3000,theWPclassiﬁerhadalargefallofperformancereducingthe
G-Meantoaround20%.Duringthisperiod,All-in-OneandFilter-
ing managed tomaintain a G-Mean ofaround60%. The Ensemble
approachalsomanagedtoavoidthedropinG-Mean,butdidnot
perform sowell as All-in-One andFiltering.
WP classiﬁers may suﬀer such drops in performance due to
changes in the characteristics of WP training data over time. How-
ever, in CP learning, training data comes from diﬀerent projects.
SomeoftheCPtrainingdatamayhavesimilardistributionasthe
targetprojectcurrentlyhas,helpingtoreducethenegativeeﬀectof
diﬀerences in the distribution over time. This is a potential reason
for CP data to be helpfulincaseof sudden performance drops.
Interestingly, the Filtering approach managed to achieve a more
stableG-MeanthanAll-in-OneforFabric8.Thissuggeststhateven
though CP data may prevent performance drops resulting from
changes in characteristics of the data, it might introduce other
performance drops due to the use of too dissimilar CP data. Experi-
ments withadditional projectsare neededto conﬁrmthat.
Fortheproprietarydata,theperiodoftimeanalysedwasnotlong
enoughtoidentifylargesuddendropsinpredictiveperformance
after the initial phase. Hence, to understand whether CP data is
helpfultopreventsuddendropsinperformanceforproprietarydata,
future work onotherproprietary projectsshould be performed.
RQ2: CP approaches frequently help to reduce or even prevent sudden
drops in performance compared to WP approaches. In particular, the
All-in-OneandFilteringapproachesobtaineduptoaround40%better
G-Mean thanthe WP approach during suchperiods.
561AnInvestigation of Cross-ProjectLearning in Online Just-In-TimeSo/f_tware DefectPrediction ICSE ’20, May 23–29, 2020,Seoul,Republic of Korea
(a)Tomcat
 (b) JGroups
(c) Spring-integration
 (d)Camel
(e)Brackets
 (f) Nova
(g)Fabric8
 (h) Neutron
(i)Npm
 (j) BroadleafCommerce
Figur
e 2: G-Mean for all datasets through time using ORB.
Theverticalredbarindicatesthelasttimestepoftheinitial
phase ofthe project, showninTable 2.
6.3 RQ3: OverallPredictivePerformance
According to the Scott-Knott.BA12 test to rank the overall G-Mean
ofallOOB-andORB-basedapproachesacrossopensourcedatasets
(Table 3), All-in-One-OOB, All-in-One-ORB, Filtering-OOB and
Filtering-ORBrankedbest,WP-ORBrankedsecond,WP-OOBranked
third, Ensemble-ORB ranked fourth and Ensemble-OOB ranked
worst.
Table3showsthatAll-in-One-OOB’sG-Meanswereupto13.43%
better (for Npm) and All-in-One-ORB’s were up to 16.04% better
(for Spring-integration). The improvements in average G-Mean
(a)C1
 (b) C2
(c)C3
Figur
e 3: G-Mean for proprietary datasets through time us-
ing OOB. The vertical red bar indicates the last time step of
the initial phase of the project, showninTable 2.
for the open source data when using All-in-One-OOB compared
with WP-OOB had large eﬀect size in 8 out of 10 datasets, and
the improvements when using All-in-One-ORB compared with
WP-ORB hadlarge eﬀectsize inall 10 datasets.
FilteringperformedsimilarlytoAll-in-One.Table 3showsthat
Filtering-OOB’sG-Meanswereupto13.97%betterthanWP-OOB’s
(forspring-integration),andFiltering-ORB’sG-Meanswereupto
16.88%betterthanWP-ORB’s(forspring-integration).Theimprove-
ments in average G-Means when using All-in-One-OOB compared
with WP-OOB had large eﬀect size in 8 out of 10 datasets, and the
improvements when using All-in-One-ORB compared with WP-
ORB had large eﬀect size in all 10 datasets. Therefore, even though
FilteringimprovedtheresultsovertheWPapproach,ﬁlteringin-
stancesdissimilartothetargetprojectdoesnothaveamajorimpact
on the performance of the classiﬁer compared to All-in-One. As
All-in-One merges all data together to build one classiﬁer, it is pos-
siblethatclassiﬁerperformancemainlydependsontheamountand
potentiallyvarietyoftrainingdataratherthanCPprojectsimilarity.
From that we can see that merging CP and WP data together to
train a classiﬁer (through All-In-One or Filtering) improved overall
predictive performance intermsof G-Mean,rather thanmaintain-
ing similar predictive performance as in oﬄine [15] scenarios.
Although Ensemble approaches performed well in oﬄine mode
[15],ourstudy showsthatensemblesof classiﬁerstrainedonsep-
arate projects did not perform well in a realistic online scenario.
Ensemble-OOB’s G-Means were up to 19.25% worse than WP’s
(for JGroups), and Ensemble-ORB’s were up to 19.85% worse than
WP’s (for JGroups). A further analysis of the results obtained by
each classiﬁer within the ensemble reveals that their individual
G-Means were not high, which resulted in the poor G-Mean of the
ensemble as awhole.This could be due to lack of enough training
datafortheindividualclassiﬁers,giventhateachclassiﬁerinthe
ensemble is trained with data from one project. This again sug-
gests thatthe larger amount of varieddata was crucialto improve
predictive performance in online JIT-SDP. It also explains why the
Ensemble approachworked inoﬄine modebut not in online mode.
562ICSE ’20, May 23–29, 2020,Seoul,Republicof Korea S. Tabassum,L.L.Minku,D. Feng,G.G.Cabral,L.Song
Table3:Overallpredictiveperformance,A12eﬀectsizesandScott-Knott.BA12statisticalteststocomparelearningapproaches
Dataset Approach Recall0 Recall1 G-Mean
TomcatWP-OOB 58.25(1.82) 63.59(1.53) 57.78(0.61)
All-in-One-OOB 61.07(1.32)[b] 63.76(1.13)[-*] 59.85(0.47)[b]
Filtering-OOB 66.99(1.22)[b] 60.05(0.97)[-b] 61.57(0.52)[b]
Ensemble-OOB 63.78(0.7)[b] 43.06(0.73)[-b] 49.12(0.2)[-b]
JGroupsWP-OOB 57.55(1.68) 59.01(1.51) 54.76(0.68)
All-in-One-OOB 64.54(1.58)[b] 52.85(1.46)[-b] 54.77(0.64)[*]
Filtering-OOB 65.94(1.32)[b] 51.59(1.22)[-b] 54.68(0.42)[-s]
Ensemble-OOB 84.03(0.4)[b] 18.6(0.36)[-b] 35.51(0.29)[-b]
Spring-integrationWP-OOB 61.13(1.64) 56.09(1.31) 48.41(0.7)
All-in-One-OOB 65.32(0.61)[b] 61.57(0.86)[b] 60.62(0.29)[b]
Filtering-OOB 69.82(0.75)[b] 59.96(1.19)[b] 62.38(0.68)[b]
Ensemble-OOB 73.09(0.57)[b] 37.6(0.63)[-b] 49.4(0.27)[-b]
CamelWP-OOB 56.04(1.23) 74.62(0.95) 62.45(0.57)
All-in-One-OOB 53.89(1.13)[-b] 75.29(0.71)[m] 62.16(0.61)[-m]
Filtering-OOB 55.52(0.81)[-s] 74.29(0.71)[-s] 62.66(0.55)[s]
Ensemble-OOB 55.25(0.87)[-m] 55.17(0.77)[-b] 52.63(0.24)[-b]
BracketsWP-OOB 49.16(0.27) 89.6(0.23) 64.03(0.11)
All-in-One-OOB 65.47(0.99)[b] 79.68(1.3)[-b] 70.85(0.47)[b]
Filtering-OOB 69.15(0.83)[b] 75.44(1.28)[-b] 70.91(0.39)[ b]
Ensemble-OOB 64.89(0.76)[b] 62.81(1.11)[-b] 62.68(0.47)[-b]
NovaWP-OOB 68.18(0.24) 86.96(0.52) 75.55(0.2)
All-in-One-OOB 70.04(0.32)[b] 88.9(0.53)[b] 78.25(0.2)[ b]
Filtering-OOB 70.91(0.3)[b] 87.24(0.37)[s] 78.01(0.17)[b]
Ensemble-OOB 75.88(0.78)[b] 57.23(1.59)[-b] 65.5(0.6)[-b]
Fabric8WP-OOB 50.56(2.68) 75.72(2.17) 59.75(0.99)
All-in-One-OOB 55.91(0.99)[b] 70.76(1.43)[-b] 60.92(0.47)[b]
Filtering-OOB 61.94(1.86)[b] 73.25(1.55)[-b] 66.55(0.62)[b]
Ensemble-OOB 48.84(1.2)[-m] 61.42(1.37)[-b] 53.27(0.35)[-b]
NeutronWP-OOB 70.03(0.69) 91.81(0.6) 79.43(0.36)
All-in-One-OOB 73.24(0.41)[b] 91.39(0.49)[-m] 81.48(0.23)[b]
Filtering-OOB 74.89(0.44)[b] 89.63(0.53)[-b] 81.51(0.25)[ b]
Ensemble-OOB 78.58(1.02)[b] 59.78(3.26)[-b] 68.28(1.39)[-b]
NpmWP-OOB 36.99(2.15) 75.74(1.5) 45.91(0.89)
All-in-One-OOB 54.8(1.58)[b] 68.69(2.0)[-b] 59.34(0.6)[b]
Filtering-OOB 55.39(1.58)[b] 68.59(1.46)[-b] 59.68(0.53)[b]
Ensemble-OOB 54.22(1.0)[b] 51.89(1.03)[-b] 50.42(0.28)[b]
BroadleafCommerceWP-OOB 58.44(1.55) 69.82(2.09) 60.41(0.82)
All-in-One-OOB 67.28(1.34)[b] 72.11(1.51)[b] 69.01(0.57)[ b]
Filtering-OOB 67.41(0.93)[b] 70.53(0.96)[s] 68.39(0.42)[b]
Ensemble-OOB 61.96(0.88)[b] 56.48(1.07)[-b] 57.31(0.17)[-b]
RankingWP-OOB 2 1 3
All-in-One-OOB 1 1 1
Filtering-OOB 1 1 1
Ensemble-OOB 1 4 5
C1WP-OOB 60.93(5.53) 38.32(4.22) 40.56(0.97)
All-in-One-OOB (combined) 30.42[-b](1.81) 67.12[b](1.91) 39.45[-b](1.45)
All-in-One-OOB (proprietary) 24.52[-b](3.5) 74.84[b](3.04) 35.76[-b](1.71)
Filtering-OOB 30.97(2.22) 67.77(1.96) 39.93[-m](1.35)
C2WP-OOB 16.49(0.17) 87.11(0.12) 26.36(0.46)
All-in-One-OOB (combined) 44.47[b](1.1) 58.6[-b](2.23) 41.3[b](1.01)
All-in-One-OOB (proprietary) 41.12[b](1.1) 54.58[-b](1.85) 33.77[b](0.83)
Filtering-OOB 45.88(2.71) 56.64(2.83) 40.79[b](0.97)
C3WP-OOB 52.05(4.13) 48.31(3.48) 40.28(1.05)
All-in-One-OOB (combined) 48.12[-b](1.52) 61.14[b](0.88) 43.53[ b](0.88)
All-in-One-OOB (proprietary) 23.51[-b](2.41) 78.5[b](1.87) 38.97[-b](1.75)
Filtering-OOB 47.36(1.33) 61.12(1.47) 43.05[b](0.63)
RankingWP-OOB 1 3 2
All-in-One-OOB (combined) 2 2 1
All-in-One-OOB (proprietary) 3 1 2
Filtering-OOB (combined) 2 2 1Dataset Approach Recall0 Recall1 G-Mean
TomcatWP-ORB 58.41(1.7) 64.49(1.19) 59.78(0.82)
All-in-One-ORB 61.98(1.17)[b] 63.38(0.73)[-b] 61.72(0.8)[ b]
Filtering-ORB 61.07(0.88)[b] 63.31(0.72)[-b] 61.25(0.47)[b]
Ensemble-ORB 61.75(1.21)[b] 47.67(1.06)[-b] 50.82(0.34)[-b]
JGroupsWP-ORB 61.18(0.7) 56.93(1.26) 57.2(0.79)
All-in-One-ORB 62.77(0.93)[b] 56.67(0.95)[*] 57.92(0.65)[b]
Filtering-ORB 63.05(1.06)[b] 57.21(1.24)[s] 58.29(0.67)[ b]
Ensemble-ORB 83.72(0.32)[b] 20.1(0.41)[-b] 37.35(0.32)[-b]
Spring-integrationWP-ORB 71.92(0.98) 45.83(1.29) 51.73(0.77)
All-in-One-ORB 67.51(0.84)[-b] 69.48(1.03)[b] 67.77(0.74)[b]
Filtering-ORB 68.57(0.82)[-b] 69.93(0.81)[b] 68.61(0.57)[ b]
Ensemble-ORB 74.61(0.47)[b] 39.98(0.66)[-b] 52.4(0.53)[b]
CamelWP-ORB 59.59(1.16) 70.11(1.13) 62.92(0.96)
All-in-One-ORB 59.12(0.51)[-m] 73.1(0.55)[b] 65.0(0.44)[ b]
Filtering-ORB 58.58(0.61)[-b] 72.7(0.52)[b] 64.48(0.42)[b]
Ensemble-ORB 61.46(0.86)[b] 52.33(1.11)[-b] 54.26(0.5)[-b]
BracketsWP-ORB 61.31(0.73) 76.66(1.41) 63.31(0.48)
All-in-One-ORB 66.9(1.01)[b] 74.94(1.68)[-b] 69.98(0.86)[b]
Filtering-ORB 66.34(0.9)[b] 75.35(1.31)[-b] 69.78(0.73)[b]
Ensemble-ORB 61.7(1.07)[m] 69.14(1.21)[-b] 64.71(0.74)[b]
NovaWP-ORB 74.25(1.47) 80.57(2.6) 75.42(0.64)
All-in-One-ORB 74.15(0.87)[-*] 81.95(1.25)[b] 77.28(0.32)[b]
Filtering-ORB 72.72(0.21)[-b] 82.85(0.53)[b] 76.83(0.34)[b]
Ensemble-ORB 76.76(0.74)[b] 68.42(1.77)[-b] 71.7(0.58)[-b]
Fabric8WP-ORB 61.32(2.13) 67.83(1.3) 61.42(1.02)
All-in-One-ORB 57.39(1.41)[-b] 73.78(1.4)[b] 63.7(1.02)[b]
Filtering-ORB 64.14(0.61)[b] 71.8(1.07)[b] 67.42(0.41)[ b]
Ensemble-ORB 49.61(1.22)[-b] 65.68(1.2)[-b] 55.91(0.58)[-b]
NeutronWP-ORB 80.47(1.68) 79.52(1.85) 79.52(0.57)
All-in-One-ORB 75.34(0.56)[-b] 87.53(0.88)[b] 80.72(0.39)[b]
Filtering-ORB 73.96(0.73)[-b] 88.94(1.02)[b] 80.62(0.46)[b]
Ensemble-ORB 77.01(1.52)[-b] 72.02(3.35)[-b] 74.28(1.08)[-b]
NpmWP-ORB 54.93(1.15) 63.67(1.3) 53.81(0.94)
All-in-One-ORB 50.95(1.54)[-b] 76.91(1.89)[b] 60.03(1.09)[ b]
Filtering-ORB 51.23(3.95)[-b] 73.26(3.88)[b] 59.15(1.91)[b]
Ensemble-ORB 52.48(1.23)[-b] 56.11(0.95)[-b] 52.41(0.57)[-b]
BroadleafCommerceWP-ORB 59.43(2.91) 65.93(2.96) 59.72(1.26)
All-in-One-ORB 66.78(0.84)[b] 70.12(1.0)[b] 67.85(0.57)[b]
Filtering-ORB 66.83(0.63)[b] 69.2(1.45)[b] 67.51(0.84)[b]
Ensemble-ORB 61.57(1.18)[b] 59.13(1.44)[-b] 58.94(0.44)[-m]
RankingWP-ORB 1 2 2
All-in-One-ORB 1 1 1
Filtering-ORB 1 1 1
Ensemble-ORB 1 3 4
c1WP-ORB 58.15(4.91) 40.56(4.66) 33.04(2.3)
All-in-One-ORB(combined) 46.46[-b](4.49) 47.98[b](3.89) 33.29[*](2.54)
All-in-One-ORB(proprietary) 34.44[-b](0.49) 59.36[b](0.47) 26.2[-b](0.71)
Filtering-ORB 43.63[-b](4.81) 51.16[b](4.16) 31.68[-s](2.81)
c2WP-ORB 16.3(0.11) 87.14(0.04) 25.81(0.32)
All-in-One-ORB(combined) 44.58[b](9.89) 56.03[-b](7.71) 43.53[b](3.37)
All-in-One-ORB(proprietary) 26.65[b](5.99) 76.45[-b](6.74) 36.86[b](3.7)
Filtering-ORB 52.54[b](7.32) 48.5[-b](8.29) 44.31[ b](3.71)
c3WP-ORB 49.64(4.43) 51.2(7.06) 39.79(1.14)
All-in-One-ORB(combined) 64.04[b](1.01) 48.98[-s](1.46) 42.85[b](0.79)
All-in-One-ORB(proprietary) 57.95[b](0.83) 50.52[-s](1.08) 39.78[*](0.61)
Filtering-ORB 62.97[b](1.01) 48.07[-m](1.54) 41.76[b](0.73)
RankingWP-ORB 2 2 3
All-in-One-ORB(combined) 2 3 1
All-in-One-ORB(proprietary) 2 2 2
Filtering-ORB (combined) 1 3 1
Top G-Means for each dataset are in bold. Standard deviations are shown in brackets. Symbols [*], [s], [m] and [b] represent insigniﬁcant, small, medium
and large A12 eﬀect size against the corresponding WP approach (WP-OOB or WP-ORB). Presence/absence of the sign “-” in the eﬀect size means that the
corresponding approach was worse/better than the corresponding WP approach. Scott-Knott.BA12 was run for all OOB- and ORB-based approaches
together. Foreach performancemetric, one testwas runfor the open source, andone test wasrun forthe proprietary data results. The groups’ rankings
retrievedby Scott-Knott.BA12 areshown in the rankingrows,withsmaller numbers indicating better rankings.
Speciﬁcally, studies in oﬄine scenarios ignore the chronology of
theprojects.Whenthetargetandotherprojectshaveanoverlap
intheirdevelopmentperiod,oﬄineCPapproachestraintheirin-
dividual classiﬁers witha considerably larger amount ofdatathat
would still not have been available for training in practice, leading
to overoptimistic estimates of predictive performance.
In terms of recalls for the open source datasets, WP-OOB per-
formed generally poorly in terms of Recall0, while Ensemble-OOB,
Ensemble-ORB and WP-ORB performed generally poorly in terms
of Recall1. As a result, these approaches ranked worse than the
othersontheseperformancemetrics.Therecallsoftheapproaches
acrossdatasetsareinﬂuencedbytrade-oﬀsbetweenRecall0andRe-
call1, resulting in several approaches obtaining the same best rankacrossdatasets.Thisisbecauseagivenapproachsometimesper-
forms better in terms of Recall0 and sometimes in terms of Recall1,
resulting in a the same rank among approaches across datasets.
However, given the G-Mean results, which combine Recall0 and
Recall1,the trade-oﬀsbetweenrecallsobtained byAll-In-One and
Filtering were better thanthoseobtainedby WP andEnsemble.
All-in-One(combined)andFiltering(combined)werethebest
rankedintermsofG-Meanfortheproprietaryprojects,accordingto
Scott-Knott.BA12 (Table 3). Eﬀect sizes varied from insigniﬁcant to
large. In particular, All-in-One-OOB (combined) obtained G-means
upto14.94%better(forC2)thanthoseofWP-OOB,andFiltering
(combined)obtainedG-meansupto18.5%better(forC2)thanthose
of WP-ORB.
563AnInvestigation of Cross-ProjectLearning in Online Just-In-TimeSo/f_tware DefectPrediction ICSE ’20, May 23–29, 2020,Seoul,Republic of Korea
Interestingly,bothAll-in-One-OOBandAll-in-One-ORBusing
only the proprietary CP data did not perform so well, and were
rankedsecondintermsofG-Mean,togetherwithWP-OOB.WP-
ORB was the worst ranked approach in terms of G-Mean. These
results again show that open source data can be helpful for propri-
etaryJIT-SDPpredictions.Theyalsosuggestagainthatthenumber
and variety of training examples used for training a classiﬁer is
a key factor for obtaining better G-Means, as the All-in-One ap-
proachusingonlythe proprietaryCPdatawastrainedon lessCP
data.However,theG-Meanvaluesobtainedwhenpredictingthe
proprietarydatawereingeneralmuchlowerthanwhenpredicting
the open source data, even when using all the open source data as
CPdata.ThissuggeststhathavingagoodamountofWPdataisalso
important.TheimportanceofthenumberofWPtrainingexamples
isalsosupportedbythe(lower)overallG-Meanduringtheinitial
phase of the open source projects (Table 2) against the (higher)
overallG-Meanacrossthewholeopensourceprojects(Table3).So,
both open source and proprietary projects need a good number of
WP training examples to perform well.
Intermsofrecalls,theresultsfortheproprietarydatawerevaried.
All-in-One-OOBtrainedonlywithproprietaryCPdatarankedbest
in terms of Recall1, but worst in terms of Recall0. WP-OOB and
Filter-ORBrankedbestintermsofRecall0,butwereintheworst
group interms of Recall1.
RQ3:MergingCPandWPdatatogether(All-in-OneorFiltering)to
trainaclassiﬁerachievedupto16.88%higheroverallG-Meanthan
WP classiﬁers for the open source, and up to 18.5% higher overall
G-Mean for the proprietary data in an online scenario. There was no
evidencethatﬁlteringouttrainingexamplesdissimilartotherecent
software changes of the target project is helpful to improve overall
predictiveperformance. Buildingseparate classiﬁers from individual
projects(Ensemble approach)wasdetrimental.
7 THREATS TO VALIDITY
Internal validity: each approach for each dataset with each clas-
siﬁers has been executed 30 times to mitigate threats to internal
validity. Also, results can be inﬂuenced by poor parameter choices.
To mitigate this threat, a grid search was performed on a set of
possiblevaluesforeachparameterbasedonaninitialportionofthe
datastream(seeSection5).Besides,allapproachestakeveriﬁcation
latency intoaccount andfully respectchronology.
Construct validity: The evaluation metrics used in this work are
G-Mean,Recall0,andRecall1.Thesearewidelyusedmetricsappro-
priate for class imbalance learning [ 33]. Predictive performance is
calculatedinaprequentialwaywithfadingfactortodiscountolder
information across time, so that plots of predictive performance
reﬂectthevariationsinpredictiveperformanceobservedovertime.
Statistical conclusion validity: Scott-Knott test was run with non-
parametric bootstrap sampling considering A12 eﬀect size to avoid
concluding that there is a diﬀerence in predictive performance
when this diﬀerence is likely to be irrelevant due to low eﬀect size.
Externalvalidity: Thisconcernswithgeneralisationof theﬁnd-
ings. This study used 10 open source projects and 3 proprietary
projectsofvariouscharacteristicssuchasprogramminglanguage,
starting date, number of commits per day, etc. The results may not
be generalisedfor othertypes of projects.8 CONCLUSION
ThisstudyinvestigatedCPlearningforJIT-SDPinarealisticonline
learning scenario, using both open source and proprietary data. In
oﬄinelearning,existingCPapproachesforJIT-SDPdidnotperform
betterthanWPapproaches[ 15].Inonlinelearning,weshowedthat
CP approaches trained with incoming CP and WP data can help
to improve predictive performance over WP approaches trained
only with WP data. The All-in-One and Filtering CP approaches
were particularly helpful during the initial phase of the project
whenthereisnotenoughWPdataavailable(RQ1),leadingtoupto
53.90% improvements in G-Mean. These approaches also helped to
reduce sudden drops in performance of the predictive model (RQ2)
after the initial phase of the project, achieving up to around 40%
better G-Mean during such periods of time. They also improved
overallpredictiveperformance(RQ3)comparedtotheWPapproach,
obtainingupto 18.5% higher overallG-Mean.
Eventhoughtheensembleapproachwasshowntoperformwell
inoﬄinelearning[ 15],itwastheworstapproachwhenconsidering
arealisticonlinelearningscenario,obtainingaverageG-Meansthat
were even lower than those of the WP approach. This indicates
thatsplittingdatafromdiﬀerentprojectsmaynotbeappropriate
in online scenario. On the other hand, training a single model
combiningCPandWPtogether(All-in-One)signiﬁcantlyimproved
performance, hence is more suitable in online JIT-SDP. Our results
indicate that both thenumber of CP andWP training examplesis
importantforachievinggoodpredictiveperformanceinJIT-SDP.
FilteringoutverydiﬀerentCPexamplesdidnotimprovethemodels
performance signiﬁcantly comparedto the All-in-One approach.
Our work has practical implications which are described below:
•In online JIT-SDP, if practitioners use CP data along with
WPdata,thiscanpreventtheperformanceofthemodelto
becomeverylowattheinitialphaseofaprojectwhichoften
occursduetolackofsuﬃcienttrainingdata.Thiswillenable
practitioners to use JIT-SDP earlier during the development
of aproject(RQ1).
•WP models can suﬀer performance drops which cause them
to be unsuitable during certain periods of time. These drops
mean that, at any given point in time, models may be per-
formingverywellorverypoorly,beingunreliableforpracti-
tioners.UsingCPdataalongwithWPdatacanovercomethis
issuebyhelpingtopreventorreducesuchdrops,enabling
practitioners to more continuously use JIT-SDP throughout
the lifetimeof the project(RQ2).
•ThecombineduseofbothWPandCPdatathroughAll-in-
OneandFilteringimproves overallpredictiveperformance
of JIT-SDP compared to WP classiﬁers (RQ3). Our study
indicates the importance of the amount of training data.
Practitioners should consider collecting large amounts of
both CP andWP training data when adopting JIT-SDP.
Futureworkincludesincorporatingadditionallongerrunningin-
dustrial projects and additional open source projects, investigation
of diﬀerent CP approaches for online JIT-SDP, and investigation of
methods for automatically adjusting the hyperparameters of the
approachesover time.
Acknowledgements: ThisworkwasfundedbyEPSRCGrant
No.EP/R006660/2.
564ICSE ’20, May 23–29, 2020,Seoul,Republicof Korea S. Tabassum,L.L.Minku,D. Feng,G.G.Cabral,L.Song
REFERENCES
[1]A.AgrawalandT.Menzies.2018.Is“betterdata”betterthan“betterdataminers”?:
on the beneﬁts of tuning SMOTE for defect prediction. In Proceedings of the 40th
InternationalConferenceonSoftwareEngineering. 1050–1061.
[2]George G Cabral, Leandro L Minku, Emad Shihab, and Suhaib Mujahid. 2019.
Class Imbalance Evolution and Veriﬁcation Latency in Just-in-Time Software
Defect Prediction. In Proceedings of the 41st International Conference on Software
Engineering (ICSE). 666–676.
[3]Gerardo Canfora, Andrea De Lucia, Massimiliano Di Penta, Rocco Oliveto, Anni-
balePanichella,andSebastianoPanichella.2013. Multi-objectivecross-project
defect prediction. In 2013 IEEE Sixth International Conference on Software Testing,
Veriﬁcation and Validation. IEEE,252–261.
[4]Gemma Catolino, Dario Di Nucci, and Filomena Ferrucci. 2019. Cross-Project
Just-in-Time BugPredictionfor MobileApps:An Empirical Assessment.In 6th
IEEE/ACM International Conference on Mobile Software Engineering and Systems.
[5]Xiang Chen, Yingquan Zhao, Qiuping Wang, and Zhidan Yuan. 2018. MULTI:
Multi-objective eﬀort-aware just-in-time software defect prediction. Information
and SoftwareTechnology 93(2018), 1–13.
[6]J.Demšar.2006. StatisticalComparisonsofClassiﬁersoverMultipleDataSets.
JMLR7 (2006), 1–30.
[7]GregoryDitzler,ManuelRoveri,CesareAlippi,andRobiPolikar.2015.Learningin
nonstationaryenvironments:Asurvey. IEEEComputationalIntelligenceMagazine
10,4 (2015), 12–25.
[8]PedroDomingosandGeoﬀHulten.2000. MiningHigh-speedDataStreams.In
Proceedings of the Sixth ACM SIGKDD International Conference on Knowledge
Discovery and Data Mining (KDD ’00). ACM, New York, NY, USA, 71–80. https:
//doi.org/10.1145/347090.347107
[9]João Gama, Raquel Sebastião, and Pedro Pereira Rodrigues. 2013. On evaluating
streamlearning algorithms. Machinelearning 90,3 (2013), 317–346.
[10]TiborGyimothy,RudolfFerenc,andIstvanSiket.2005. EmpiricalValidationof
Object-Oriented Metrics on Open Source Software for Fault Prediction. IEEE
Trans. Softw. Eng. 31, 10 (Oct. 2005), 897–910. https://doi.org/10.1109/TSE.2005.
112
[11]Tracy Hall, Sarah Beecham, David Bowes, David Gray, and Steve Counsell. 2012.
A systematic literature review on fault prediction performance in software engi-
neering.IEEE Transactions onSoftwareEngineering 38,6 (2012), 1276–1304.
[12]Zhimin He, Fayola Peters, Tim Menzies, and Ye Yang. 2013. Learning from
open-sourceprojects:Anempiricalstudyondefectprediction.In 2013ACM/IEEE
International Symposium on Empirical Software Engineering and Measurement.
IEEE,45–54.
[13]Zhimin He, Fengdi Shu, Ye Yang, Mingshu Li, and Qing Wang. 2012. An investi-
gationonthefeasibilityofcross-projectdefectprediction. AutomatedSoftware
Engineering 19,2 (2012), 167–199.
[14]Xiao-YuanJing,FeiWu,XiweiDong,andBaowenXu.2016. AnimprovedSDA
based defect prediction framework for both within-project and cross-project
class-imbalanceproblems. IEEETransactionsonSoftwareEngineering 43,4(2016),
321–339.
[15]Yasutaka Kamei, Takafumi Fukushima, Shane McIntosh, Kazuhiro Yamashita,
Naoyasu Ubayashi, and Ahmed E. Hassan. 2016. Studying just-in-time defect
predictionusingcross-projectmodels. EmpiricalSoftwareEngineering 21,5(2016),
2072–2106. https://doi.org/10.1007/s10664-015-9400-x
[16]Yasutaka Kamei, Emad Shihab, Bram Adams, Ahmed E. Hassan, Audris Mockus,
Anand Sinha, and Naoyasu Ubayashi. 2013. A large-scale empirical study of
just-in-time quality assurance. IEEE Transactions on Software Engineering (2013).
https://doi.org/10.1109/TSE.2012.70
[17]Jian Li, Pinjia He, Jieming Zhu, and Michael R Lyu. 2017. Software defect predic-
tion viaconvolutional neural network. In 2017 IEEE International Conference on
SoftwareQuality,Reliability and Security(QRS). IEEE,318–328.
[18]RuchikaMalhotra.2015. Asystematicreviewofmachinelearningtechniques
for softwarefaultprediction. AppliedSoftComputing 27(2015), 504–518.
[19]Shane McIntosh and Yasutaka Kamei. 2018. Are ﬁx-inducing changes a mov-
ing target? a longitudinal case study of just-in-time defect prediction. IEEE
TransactionsonSoftwareEngineering 44,5 (2018), 412–428.
[20]ThiloMendeandRainerKoschke.2010. Eﬀort-awaredefectpredictionmodels.
In2010 14th European Conference on Software Maintenance and Reengineering.
IEEE,107–116.
[21]Tim Menzies, Ye Yang, George Mathew, Barry Boehm, and Jairus Hihn. 2017.
Negativeresultsforsoftwareeﬀortestimation. EmpiricalSoftwareEngineering
22,5 (2017), 2658–2683.
[22]NikolaosMittasandLefterisAngelis.2012. Rankingandclusteringsoftwarecost
estimation models through a multiple comparisons algorithm. IEEE Transactions
onsoftwareengineering 39,4 (2012), 537–551.
[23]JaechangNam,SinnoJialinPan,andSunghunKim.2013. Transferdefectlearning.
In201335thInternationalConferenceonSoftwareEngineering(ICSE).IEEE,382–
391.[24]AnnibalePanichella,RoccoOliveto,andAndreaDeLucia.2014. Cross-project
defect prediction models: L’union fait la force. In 2014 Software Evolution Week-
IEEEConferenceonSoftwareMaintenance,Reengineering,andReverseEngineering
(CSMR-WCRE). IEEE,164–173.
[25]ChristoﬀerRosen,BenGrawi,andEmadShihab.2015. Commitguru:analytics
and risk prediction of software commits. In Proceedings of the 2015 10th Joint
MeetingonFoundationsofSoftwareEngineering. ACM,966–969.
[26]DuksanRyu,OkjooChoi,andJongmoonBaik.2016. Value-cognitiveboosting
with a support vector machine for cross-project defect prediction. Empirical
SoftwareEngineering 21,1 (2016), 43–71.
[27]DuksanRyu,Jong-InJang,andJongmoonBaik.2017. Atransfercost-sensitive
boosting approach for cross-project defect prediction. Software Quality Journal
25,1 (2017), 235–272.
[28]Sadia Tabassum, Leandro L. Minku, Danyi Feng, George G. Cabral, and Liyan
Song.2020. AnInvestigationofCross-ProjectLearninginOnlineJust-In-Time
Software Defect Prediction – Supplementary Material. http://www.cs.bham.ac.
uk/~minkull/publications/TabassumICSE2020-supplement.pdf
[29]MingTan,LinTan,SashankDara,andCalebMayeux.2015. Onlinedefectpredic-
tionforimbalanceddata.In 2015IEEE/ACM37thIEEEInternationalConference
onSoftwareEngineering, Vol. 2.IEEE,99–108.
[30]BurakTurhan,TimMenzies, AyşeBBener,andJustin DiStefano.2009. Onthe
relative value of cross-company and within-companydata for defect prediction.
EmpiricalSoftwareEngineering 14,5 (2009), 540–578.
[31]András Vargha and Harold D Delaney. 2000. A critique and improvement of
the CL common language eﬀect size statistics of McGraw and Wong. Journal of
Educationaland Behavioral Statistics 25,2 (2000), 101–132.
[32]RomiSatriaWahono.2015. Asystematicliteraturereviewofsoftwaredefectpre-
diction: research trends, datasets, methods and frameworks. Journal of Software
Engineering 1,1 (2015), 1–16.
[33]ShuoWang,LeandroLMinku,andXinYao.2018. Asystematicstudyofonline
class imbalance learning with concept drift. IEEE transactions on neural networks
and learningsystems 29,10(2018), 4802–4821.
[34]Tiejian Wang, Zhiwu Zhang, Xiaoyuan Jing, and Liqiang Zhang. 2016. Multiple
kernel ensemble learning for software defect prediction. Automated Software
Engineering 23,4 (2016), 569–590.
[35]Thomas Zimmermann, Nachiappan Nagappan, Harald Gall, Emanuel Giger, and
Brendan Murphy. 2009. Cross-project defect prediction: a large scale experiment
ondatavs.domainvs.process.In Proceedingsofthethe7thjointmeetingofthe
EuropeansoftwareengineeringconferenceandtheACMSIGSOFTsymposiumon
The foundations ofsoftwareengineering. ACM,91–100.
565