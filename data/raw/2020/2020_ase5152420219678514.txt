Mining Cross-Domain Apps for Software
Evolution: A Feature-based Approach
MD Kaﬁl Uddin, Qiang He*, Jun Han, Caslon Chua
Department of Computing Technologies
Swinburne University of Technology
Melbourne, Australia
E-mail:{mdkaﬁluddin, qhe, jhan, cchua}@swin.edu.au
Abstract —The skyrocketing growth of mobile apps and mobile
devices has signiﬁcantly fueled the competition among app
developers. They have leveraged the app store capabilities toanalyse app data and identify app improvement opportunities.Existing research has shown that app developers mostly relyon in-domain (i.e., same domain or same app) data to improvetheir apps. However, relying on in-domain data results in lowdiversity and lacks novelty in recommended features. In thiswork, we present an approach that automatically identiﬁes,classiﬁes and ranks relevant popular features from cross-domainapps for recommendation to any given target app. It includesthe following three steps: 1) identify cross-domain apps thatare relevant to the target app in terms of their features; 2)ﬁlter and group semantically the features of the relevant cross-domain apps that are complementary to the target app; 3)rank and prioritize the complementary cross-domain features(in terms of their domain, app, feature and popularity char-acteristics) for adoption by the target app’s developers. Wehave run extensive experiments on 100 target apps from 10categories over 15,200 cross-domain apps from 31 categories. Theexperimental results have shown that our approach to identifying,grouping and ranking complementary cross-domain features forrecommendation has achieved an accuracy level of over 89%.Our semantic feature grouping technique has also signiﬁcantlyoutperformed two existing baseline techniques. The empiricalevaluation validates the efﬁcacy of our approach in providingpersonalised feature recommendation and enhancing app’s userserendipity.
Index T erms—mobile apps; software evolution; app competi-
tion; app improvement; feature extraction; app store mining;
I. I NTRODUCTION
With the prevalence of mobile devices, the number of
mobile apps and their popularity in the market has hit new
heights. In 2020, apps became a life-saving tool in ﬁghtingand restricting the spread of the global COVID-19 pandemic[1]. By March 2021, smartphone users reached 3.8 billion [2],nearly half of the world population. Expectedly, the mobiledeveloper population has boomed and the competition amongthe developers intensiﬁed. In a recent report, Google Inc.announced that its app store has paid out $80 billion to itsdevelopers since 2008 whereas Apple Inc. has paid out $155billion to its iOS developers [3], indicating a large sharedeconomy by the developers in the app market. However, onlya relatively small number of developers with top rankingapps can survive in such ﬁerce competition. The economic0%5%10%15%20%25%30%35%40%45%50%
Facebook Gmail Vibers p p A  r a l i m i S  y l l a i t r a P   f o  %ProduĐƟvity
Business
Tools
EducaƟon
Books
Travel
Music
Social
Lifestyle
Entertainment
Personalisa Ɵon
Photography
Fig. 1. Percentage of Relevant Cross-Domain Apps
distribution of the market is found to be skewed rather than
hypothetically perfectly ﬂat [4], [5].
Survival of the ﬁttest [6], the Darwinian theory of evolution,
applies to the app market place. A large number of appsdropout soon after their birth in the app market. More than40% of app users stop using an app on the very ﬁrst day ofapp downloads [7]. One of the key drivers for app downloadsis the popularity of app features, and users’ liking or dislikingan app largely depends on the app features [8]. Hence, tosurvive market competition, apps must continuously improvetheir features and provide new features to retain and attractusers.
A major aspect of app improvement is to differentiate
the software value of a given app (hereafter referred to as
the target app ) from its competitors [9]. Researchers have
proposed several techniques to improve apps by analysingin-domain data without considering any relevance with thetarget app [10]–[13] or solely depending on the data relatedto the target app [11], [14]–[16]. However, a major drawbackof these studies is the “ﬁlter bubble” phenomenon in whichfeature recommendations made for the target app is trappedin a subspace of options that are too similar to the proﬁle ofthe target app [17]. For example, if the target app belongsto the Education domain (a.k.a Category in the app store),
they will only recommend features identiﬁed from the samedomain that are similar to the target app’s features. Moreover,popular features often migrate from one domain to another[18] offering novelty and feature diversity to cross-domainapps.
7432021 36th IEEE/ACM International Conference on Automated Software Engineering (ASE)
DOI 10.1109/ASE51524.2021.000712021 36th IEEE/ACM International Conference on Automated Software Engineering (ASE) | 978-1-6654-0337-5/21/$31.00 ©2021 IEEE | DOI: 10.1109/ASE51524.2021.9678514
978-1-6654-0337-5/21/$31.00  ©2021  IEEE
A report from the Australia Competition & Consumer
Commission (ACCC) [19] and App-Annie [20] found that due
to the competitive pressure many apps are picking up cross-domain features to gain popularity in the store. Drachen etal. have shown that incorporating simple social features inmobile games increases its software value [21] and extendssocial activity in the cohort [22]. Others reported that blend-ing elements from various domains such as Social, Videoand Entertainment is the key reason why TikTok
1became
the world’s most downloaded app in July 2020 [23]–[25].Nonetheless, existing studies have yet to exploit the analysis ofsuch cross-domain data available in the app store. To identifyimprovement opportunities for a target app, we propose togo beyond the in-domain apps and identify complementaryfeatures from relevant cross-domain apps for adoption by thetarget app.
Mining cross-domain features is, however, a highly chal-
lenging task that requires automatic analysis of a high volumeof apps from various categories. It becomes even more difﬁcultto identify app features that are relevant and complementary to
the target app due to the overlapping nature of apps and storecategories. Tackling this challenge requires the extraction offunctional features from the target app as well as the cross-domain apps. Then, the apps that are relevant to the target appin terms of feature similarity can be found, and their coexistingbut dissimilar features can be further identiﬁed and grouped ascomplimentary features for potential recommendation. Finally,the complementary features can be ranked for recommenda-tions to the target app. For example, Figure 1 shows ourinvestigation on three target apps (i.e., Facebook, Gmail, andViber) from the Communication domain over 15,200 cross-
domain apps in the dataset. We found that more than 30%of the apps from the Productivity and Tools domains partially
match all three target apps. This indicates a greater chanceof ﬁnding popular cross-domain features for recommendationand improving the diversity and novelty in the recommendedfeatures.
In a nutshell, we answer the following three research
questions in this paper:
1)Identify relevant cross-domain apps: How to extractapp features and effectively identify cross-domain appsthat are relevant to the target app in terms of their featuresimilarity?
2)Identify complementary cross-domain features:H o wto automatically identify and classify semantically thosefeatures of the relevant cross-domain apps that are com-plementary to the target app, in an effective way?
3)Rank complementary cross-domain features for per-sonalised recommendation: How to rank the identi-ﬁed complementary cross-domain features and prioritiseadoption recommendations speciﬁc to the target app?
The rest of the paper is organised as follows. Section II
discusses the related work and research gaps that motivatethis research. Section III presents the approach to identifying
1https://play.google.com/store/apps/details?id=com.zhiliaoapp.musicallyrelevant cross-domain apps, classifying and ranking com-plementary features from the relevant cross-domain apps tomake adoption recommendations to the target app. SectionIV describes the experiment settings. Section V presentsthe experimental results and discusses the limitations of ourapproach. Section VI concludes the paper and outlines somefuture work.
II. R
ELATED WORK
Existing studies use various data sources to elicit require-
ments for an app, but mostly from app reviews and tweets.These requirements are for identifying improvements for thenext app release or for supporting app developers and stake-holders [11], [14]–[16]. Many approaches have been proposedon review summarization, classiﬁcation, and requirementselicitation. In general, these studies focus on the followingthree key aspects.
A. Feature Extraction
Existing studies extract software aspects such as frequent
features, user opinions, sentiments, new features, bug reports,
etc., from app reviews [15], [16], [26], [27]. Other studiesextract app factors such as install size, code complexity, SDKversion, UI complexity, library, etc., from app resources andassets used by the app [27]. In [14], [28], authors extractfrequent features and sentiments from tweets on Twitter forsoftware evaluation and maintenance as well as elicit appimprovement requirements for the stakeholders. However,these existing studies use only the app’s own user reviewswhen eliciting requirements for software evolution. Theseapproaches neglect all the information relevant to apps fromother domains that might be a rich source of information forsoftware improvement and requirement elicitation. Moreover,the following comment from an app developer has also ex-pressed such concern and showed the importance of reviewsfor competitors: “I do not want to listen only to my users butalso the users of competitive apps!” [26].
B. Feature Grouping
Current approaches classify and categorise extracted topics
or aspects of apps into several predeﬁned categories such as
feature requests, praise, de-praise, etc. AR-miner [11] classi-ﬁes reviews into informative or non-informative reviews. Thestudy presented in [28] classiﬁes tweets into three categories,including technical, non-technical and general. In general,current studies classify user reviews and tweets into two [11],[14], three [26], [28], four [15] or ﬁve [29] broad categories.However, these studies limit themselves to high-level cate-gorisations only without considering speciﬁc app features forimproving the target app. This situation becomes even worsewhen an app has a limited amount of reviews or similar typesof reviews. We believe that there is a correlation between fea-tures of the target app and features of other cross-domain apps.This correlation of features would further enhance feature im-provement recommendations for the target app by identifyingrelevant popular features from other domains. The existing
744studies did not consider the feature correlation between the
target app and apps from other domains. As such, to deﬁnesuch a correlation of features, a new research question needsto be addressed, i.e., how to automatically identify and classifyrelevant popular app features across domains in the store? Tothe best of our knowledge, there has been no attempt to explorethe relationships between cross-domain apps and target apps,to date.
C. Feature Ranking
Existing studies group and rank apps based on sentiment
scores, users’ opinions, ratings, topic frequencies, social ranks,
etc., derived from app reviews. In [11], topics are grouped andranked with frequent item-set mining using LDA or ASUM.The authors of [16] group the review topics in terms ofsentiment scores extracted from users’ reviews. In their recentwork [14], authors use BTM topic modeling for groupingand social weight function for ranking. In [29], authors groupreview aspects using frequent item-set mining along with itssentiments and opinions. In [26], authors introduce CLAP(Crowd Listener for releAse Planning) to group reviews intoclusters and prioritize them using a rating function. However,these existing studies group and rank the extracted app reviewaspects, sentiment scores, user opinions, and rating scoresfor the target app only. This grouping and ranking do notnecessarily provide a comparative view of app features oraspects in high demand across other domains.
III. A
PPROACH
Our approach to mining and recommending features from
cross-domain apps for a given target app has the followingthree major components (corresponding to the three steps inFigure 2):
Feature 
Extraction Identify 
Relevant Cross-
Domain appsIdentify 
Complementary 
Cross-Domain 
FeaturesFiltering, 
Grouping 
Ranking of 
FeaturesApp 
Description
App 
DescriptionsCross-Domain 
FeaturesTarget App 
Features
Step-1 Step-2 Step-3
Target App
Cross-Domain 
Apps               Pre-processing Step
Fig. 2. Overview of Our Approach
1) A technique that allows us to accurately identify cross-
domain apps that are relevant to the target app in terms of
semantically similar features between the target app andcross-domain apps. This is done by extracting the appfeatures from their descriptions and analysing the featuresimilarity between the target app and each of the cross-domain apps (Section III-A).
2) A technique that identiﬁes features from the relevant
cross-domain apps that are different from but comple-
mentary to the target app’s features for potential adoption
by the target app. This is achieved by collecting thosefeatures of the relevant cross-domain apps that are dis-similar (i.e., novel and/or diverse) to the features of thetarget app, and partitioning these coexisting dissimilarfeatures into groups with each containing semanticallysimilar features and representing a complementary featureto the target app (Section III-B).
3) A technique that ranks the complementary features to
provide adoption recommendations to the target app’sdevelopers. Each complementary feature is scored ac-cording to the distribution of features, apps, domains,and app popularity associated with the features in thecoexisting dissimilar feature group represented by thecomplementary feature. Finally, complementary featureswith high scores are recommended for adoption by thetarget app (Section III-C).
We discuss these three components in the following sections.
A. Identiﬁcation of Relevant Cross-Domain Apps
Since the number of cross-domain apps is large, it is
important to identify those that are speciﬁcally relevant to
the target app. To do this, we ﬁrst run a pre-processingstep to clean up all unnecessary and irrelevant symbols andword tokens from the app descriptions. Then, we extract theapp features of the target app and each of the cross-domainapps from their descriptions. Finally, we use a feature-basedsimilarity assessment technique to identify the relevant cross-domain apps that have a set of features similar to the targetapp.
1) Pre-processing App Descriptions: In the pre-processing
step, we clean the apps’ text descriptions by removing allthe unnecessary symbols, numbers, hyphen, characters, tags,and spaces. Then, we discard the URLs, emails and all theEnglish STOPWORDS like “to”, “for”, “the”, etc., from theapp descriptions. STOPWORDS usually have little lexicalcontent, but are not easily distinguishable from other texts.We remove them by using the Python libraries NLTK [30].Finally, we manually built our own dictionary with respect tothe app domain for the removal of unnecessary words suchas1-letter , 2-letters, app names, connecting words, auxiliary
verbs, adverbs, praising words, etc. A portion of the dictionary
has been shown below:
[“inc”, “etc”, “able”, “typical”, “yet”,
“otherwise”, “welcome”, “none”,“done”, “ago”, “recently”,“still”,“wait”, “today”, “soon”, “always”, “app”, “also”,“even”, “ever”, “available”, “please”, “much”, “almost”,“many”, “2g”, “x”, “facebook”, “viber”, ...]
This pre-processing step makes sure that the apps’ features as
presented in the app descriptions are preserved without majorloss of information.
2) Extracting Functional App Features: In this step, we
extract features from both the target app and the cross-domainapps. Cross-domain apps are those that do not belong to thesame category as the target app. Take app Canvas for example.It belongs to the Education category. Cross-domain appsare the apps that belong to categories other than Education.Note that we focus on apps’ functional features because they
745determine the apps’ core functionalities and the popularity of
an app depends mainly on its functional features [31]–[33].Unlike app reviews which usually focus on non-functionalfeatures [34]–[37], app developers often describe their apps’functionalities in app descriptions [31], [38], [39]. Therefore,we extract the apps’ functional features from their descriptionscrawled from publicly available app store pages (see SectionIV-A for details).
We follow the approach proposed in [40] that uses 18 parts-
of-speech patterns and 5 sentence patterns to extract featuresfrom app descriptions. We found this approach efﬁcient in thatit does not require training and conﬁguration datasets to extractfeatures. Moreover, this method excels at extracting featuresfrom formal texts like app descriptions. To be more precisein feature extraction, however, we made some improvementsin our implementation concerning the following aspects: (1)feature mentioned in double-quotation, (2) feature starting withthe word “Feature”, and (3) feature mentioned in capitalizedwords. The pre-processed app descriptions are ﬁrst tokenisedusing a word-tokeniser, and then tagged with the Parts-of-Speech (POS) analysis done by using the Stanford POS-taggers library [41], the world’s richest POS-taggers libraryfor Natural Language Processing (NLP). Finally, the tokenisedand tagged app descriptions are fed into the feature extractionengine.
An example of the features extracted from Facebook Mes-
senger’s app description is shown below:
{video chat, share stories, group video chat, voice
message, share photos, share your location, send money,use speech text feature, find deals, ...}
In this step, we extract all the features from each of the target
and cross-domain apps and store them separately in a featuredatabase.
3) Feature-based Relevance of Cross-Domain Apps: Our
purpose is to identify the cross-domain apps that are relevant tothe target app. We do so by locating those cross-domain appsthat have a set of features similar to those of the target app.The remaining cross-domain apps are considered as irrelevantand are removed from further consideration.
To do this, we calculate the semantic relevance between the
features of the target app and the features of each cross-domainapp. To compute the feature-by-feature semantic similaritybetween apps, we adopt WordNet [42], one of the mostpopular and standard tools used by researchers for measuringsemantic relevance between words and phrases. Given twowords or phrases, a similarity score can be calculated basedon the shortest path between them in WordNet [43]. Since ourextracted features are already tokenised, WordNet provides fastsimilarity score without the need for a large training corpuslike word2vec [44].
First, We represent all the extracted features from a cross-
domain app with an app-feature matrix. Let us denote thefeatures extracted from an app in a given domain as F={f
1,
...fn}. Then, for mapps in a domain, there will be a mxn app-
feature matrix. Given kdomains, the ﬁnal feature matrix will
bemxnxk for each single target app tas shown in Figure 3.For each app in the cross-domains, the feature matrix containsitsfeature_set, app_id, domain_id.
App 1
App 2
App 3
App mCross-Domain Apps
feature 1
feature 2
feature 3
feature 4
feature nfeature 5Cross-Domain Features
Cross-Domain 1
Cross-Domain 2
Cross-Domain 3
Cross-Domain K
…
…
Fig. 3. Cross-Domain App-Feature Matrix
After the formation of the cross-domain app feature matrix,
we compute the relevance of each cross-domain app to thetarget app in terms of their feature similarity. We adopt the Wu-Palmer (WUP) path similarity between the two feature sets ofthe cross-domain app and target app for the WordNet model,as it provides a good semantic relatedness measure [45]. Thesimilarity score ranges between 0 and 1, where 0 means nomatch found and 1 means a perfect match with the target app.We consider those cross-domain apps with a similarity scoreabove a threshold δas relevant to the target app and ﬁlter
out all the apps below this threshold. In this study, we choosethe similarity score of 0.33 as the threshold (i.e., δ=0.33),
indicating that a cross-domain app with a similarity score of0.33or higher is relevant to the target app.
B. Identiﬁcation of Complementary Cross-Domain Features
In this step, our goal is to automatically identify those
unique features of the relevant cross-domain apps that donot exist in the target app in terms of semantic similarity,as complementary features for the target app. To do so, weneed to (1) identify those features of all the relevant cross-domain apps that are semantically different from the targetapp’s features (called coexisting dissimilar features or simply
coexisting features), and (2) put these coexisting featuresinto groups with each group being a maximum subset ofthese features that are similar to each other. Each of thecoexisting feature group represents a complementary cross-
domain feature for the target app.
1) Identifying Coexisting Features: In this phase, we make
sure to eliminate all the features of a relevant cross-domainapp that are similar to those of the target app and keep theother (coexisting) features. That is, the coexisting features arethe set or subset of features from the feature matrix that aredissimilar to those of the target app. For example, if a targetapp, T, which has features T
f={ft1,ft2,ft3, ...ftl}, has
similar features {f 11,f12,f13} from a cross-domain app, C1,
which has features C1f={f11,f12,f13,f14, ...f1n}, then
features {f 14, ...f1n} are considered as coexisting features.
We are interested in the coexisting features mainly becausethey are dissimilar, diverse and novel relative to the target app,
746f11 f12 f13 f14 f15 … f1n
f21 f22 f23 f24 f25 … f2n
f31 f32 f33 f34 f35 … f3n
… … … … … … …
fm1 fm2 fm3 fm4 fm5 … fmnApp 1
App 2
App 3
App m…Target App 
Features ft1               ft2              ft3              ft4              ft5              …               ftlCross-Domain Apps
Relevance matching (App similarity >= Ƒ)
Fig. 4. Coexisting Features in Relevant Cross-Domain Apps
and are potentially useful to the target app. Figure 4 illustrates
feature coexistence in relevant cross-domain apps for a giventarget app (with the features similar to the target app’s featuresbeing shown in dashed blocks).
2) Grouping Coexisting Features: After removing all those
features of the cross-domain apps that are similar to the targetapp’s features, all the remaining features are dissimilar to thetarget app’s features while also being part of the cross-domainapps, and therefore, are referred to as dissimilar coexisting
features. We put all these coexisting features together ina single set and partition them into semantically differentgroups with each representing a complementary cross-domainfeature for the target app. Algorithm 1 presents this semantics-
f11 f12 f13 f14 f15 … f1n
f21 f22 f23 f24 f25 … f2n
f31 f32 f33 f34 f35 … f3n
… … … … … … …
fm1 fm2 fm3 fm4 fm5 … fmnApp 1
App 2
App 3
App mFeature 
grouping 
by semantic relevance
…Cross-Domain Appsf1ff1ff1n
f2ff2ff2n
f3ff3ff3ff3n f3ff3ff3ff34
…f1ff1ff14 f1ff1ff15
f2ff2ff25
f3ff3ff3ff35Similarity
Fig. 5. Grouping Coexisting Features
based feature grouping process. For a feature fiin the initial
coexisting feature set F(Line 6), we compare it against all the
other features in the same set (Line 9). All the features thatare similar to f
iaccording to WordNet [42] with a threshold
θ(i.e., with a similarity score (S ij) equal to or higher than
the threshold θ(Lines 10-11)) form a coexisting feature group
(Lines 9-15) in the ﬁnal feature group set Flist(Line 12). We
do this because developers may describe the same functionalfeature in different ways. For example, share photos and share
images are considered as similar features. In our study, we set
the value of θ=0.6(Line 4), following an existing work [16].
When a match is found, we form a coexisting feature groupand remove this feature from the initial master set (Lines 11-13). Thus, each time a group is formed, the size of the masterset is reduced (Line 16). This process iterates until the masterlist is empty. When the algorithm terminates (Line 18), theﬁnal feature group set F
listcontains all the coexisting featureAlgorithm 1: Semantics-Based Feature Grouping
Input:F{...}, set of coexisting features
Result:Flist{{...},{...},{...}}, coexisting feature groups
1import package NLTK
2import package WORDNET
3Flist=∅, initialise output
4θ=0.6, initialise threshold
5k=0
6foreachfi∈Fdo
7Fn=F\{fi}
8k=k+1
9 foreachfj∈Fndo
10 Sij= WordNet.SemanticSimilarity(f i,fj)
11 ifSij>=θthen
12 Flistk{} = saveFeatureClass(f i,fj,Sij)
13 Fn=Fn\{fj}
14 end
15 end
16F=Fn
17end
18returnFlist
groups with each containing semantically similar coexistingfeatures.
Figure 5 illustrates some coexisting feature groups (in
dashed blocks with green background), with {f
14,f15,f25,
f35} being such a group containing semantically similar fea-
tures. Below are some example features in a coexisting featuregroup for target app Facebook Messenger :
Cross-domain Features (feature:app_id )
{transfer files: com.lenovo.anyshare.gps,
transfer large files: com.dewmobile.kuaiya.play
transfer pictures, transfer music: cn.xender
}
C. Feature Ranking
The ﬁnal step in recommending the complementary features
for adoption by the target app is to rank and prioritize them.Our feature grouping technique from the previous step pro-duces a set of complementary features (or coexisting featuresets), where each feature set contains several coexisting butsemantically similar features. Note that stored together witheach feature in the feature set are by its app_id (as shown in
the above example) and other relevant information such as appdownloads, app name, etc.
Although we consider coexisting features relevant to the
target app as complementary features, these features may ormay not be suitable for the target app. To identify personalised(or most suitable) features for the target app to adopt, weconsider 4-key aspects when ranking those features and derivean aggregated ranking score for each coexisting feature set.Only the top-ranked complementary features (or feature sets)are recommended to for adoption by the target app. The4-key ranking attributes are: i)domain distribution, ii)app
distribution, iii)popularity (i.e., download) distribution and
iv)feature distribution.
7471)Domain distribution (d s)- The distribution of the features
in a coexisting feature group over the app domains.
This is a ratio between the number of domains that thecoexisting features in the group belong to over the totalnumber of domains that all the relevant cross-domainapps belong to. It is essential to consider the rangeof domains involved because the use of a feature inmultiple domains indicates its popularity or reach acrossthose domains. This is conﬁrmed in [18] where theauthors mentioned that a strongly migratory feature hasno categories (i.e., domains) and can move across the appstore. On the other hand, a feature being only used in alimited number of domains indicates its limited ability incross-domain migration. Thus, we prioritise a coexistingfeature group with its features reaching a larger numberof domains. For any target app, if Dis the total number
of relevant cross-domains and dis the number of domains
that a coexisting feature group has features in, this group’sdomain distribution (d
s) is calculated as, d/D .
2)App distribution (a s)- The distribution of the features in
a coexisting feature group over the apps. This is a ratiobetween the number of apps that the coexisting featuresin the group belong to over the total number of all therelevant cross-domain apps. This is an important criterionbecause if a feature is shared by multiple apps, the featureis likely to be useful to the target app. Multiple featuresfrom the same app appearing in a coexisting featuregroup (see example in Section III-B) limits the range ofapps it reaches. Considering app distribution reduces thisconcentration effect, and prioritises a coexisting featuregroup that covers a wider range of apps. For any targetapp, ifNis the total number of relevant cross-domain
apps and nis the number of apps a coexisting feature
group has feature in, the group’s app distribution (a
s)i s
calculated as, n/N .
3)Popularity distribution (p s)- The distribution or weight-
ing of the popularity (i.e., app downloads) of appsinvolved in a coexisting feature group. This is a ratiobetween the accumulative popularity (downloads) of appshaving features in the coexisting feature group over theaccumulative popularity (downloads) of all the relevantcross-domain apps. Popularity distribution of an app isimportant because a popular app tends to have popularfeatures which contribute to its downloads [46]–[49].Therefore, if a feature is offered by apps with highdownloads, it is also likely to be more useful to thetarget app. For any target app, if Nis the total number
of relevant cross-domain apps and nis the number of
apps a coexisting feature group has feature in, the group’spopularity distribution (p
s) is calculated as,
ps=/summationtextn
j=1pgj/summationtextNk=1prk
wherepgis the downloads of each app involved in the
coexisting feature group and pris the downloads of each
relevant cross-domain app.4)Feature distribution (f s)- The distribution or weighting
of coexisting features of the apps involved in a coexisting
feature group. This is a ratio between the accumulativenumber of coexisting features of the apps having featuresin the coexisting feature group over the accumulativenumber of coexisting features of all the relevant cross-domain apps. Feature distribution in a group affects thepopularity of the group since the same feature fromdifferent apps may contribute to one group and viceversa. For any target app, if Nis the total number
of relevant cross-domain apps and a feature group thatcontains features from nnumber of apps, then the feature
distribution (f
s) is calculated as,
fs=/summationtextn
j=1cfgj/summationtextNk=1cfrk
wherecfg is the number of coexisting features of each
app involved in the coexisting feature group and cfr is
the number of coexisting features of each relevant cross-
domain app.
Finally, we compute a single ranking score for each of thecoexisting feature group (i.e., a complementary feature) bytaking the mean of the aggregated distribution scores of allthe attributes discussed above. We sort and rank the comple-mentary features for recommendation to the target app.
IV . E
XPERIMENT SETUP
We have run extensive experiments on a large number
of cross-domain apps. Section IV-A introduces the datasetused throughout the experiments, Section IV-B describes thevalidation methods, and Section IV-C presents the evaluationmetrics used.
A. Dataset
To facilitate the experiments, we have crawled the Google
Play store for the period of Nov 2019 to Jan 2020 and
produced a required dataset. It contains 21,050 apps in 41categories. We have removed those categories with 30 apps orfewer. Finally, our curated dataset contains 15,200 apps in 31categories, which also excludes the Games category becauseof the different structure in its app descriptions [16]. Then,we have randomly chosen 10 categories and 10 target appsfrom each category, forming a set of 100 target apps fromthe Communication, Education, Book and Ref., Health and
Fitness, Shopping, Finance, Lifestyle, Productivity, Maps andNavigation categories (or domains).
For each target app, we run our approach on all the
corresponding cross-domain apps in the dataset. Note that thefeature extraction step is run on all the apps in all categoriesonce only and the results are stored in the feature database foruse in all further steps of our approach (see Section III-A).
The dataset and experiment results can be found in a GitHub
repository at https://github.com/mycoderesearch/CDFRecommendation.
748B. V alidation Methods
We validate our techniques for (1) identiﬁcation of relevant
cross-domain apps for any given target app, (2) semantics-
based feature grouping of coexisting features from cross-
domain apps, and (3) ranking and prioritisation of comple-
mentary cross-domain features to make recommendations to
the target app’s developers. We have used a manually built
truth-set to validate the relevance of cross-domain apps to thetarget app, while we validate the feature grouping and rankingtechniques by conducting a user study. We also compare oursemantics-based feature grouping technique against two otherpopular techniques used for feature grouping. Further detailsof the validation methods can be found below in Sections V-A,V-B and V-C.
C. Evaluation Metrics
To evaluate the technique for identifying relevant cross-
domain apps, we have used 3 popular metrics: Precision,
Recall and F-score as deﬁned in [50]:
Precision =TP
TP+FP(1)
Recall =TP
TP+FN(2)
F−score =2∗Precision ∗Recall
Precision +Recall(3)
whereTP (true positive) is the number of features correctly
found,FP (false positive) is the number of features not
correctly found and FN (false negative) is the number of
features incorrectly excluded with respect to the truth set offeatures. The F-score is the harmonic mean of the precisionand recall.
To validate the ranking technique for the identiﬁed com-
plementary features (or coexisting feature groups), We adoptthe well-known Normalized Discounted Cumulative Gain(NDCG) [51] as a measure for evaluating the quality of top-kranking results:
NDCG @k=DCG @k
IDCG @k(4)
wherekis the number of top features to be considered,
DCG is the calculated ranking of the features and IDCG is
the ideal ranking of the same features in a truth set obtainedfrom the user study. NDCG@k is within the range of [0,1], and a higher value implies greater agreement between thecalculated rank order and the ideal rank order.
V. R
ESULTS ANALYSIS AND DISCUSSION
In this section, we discuss the experimental results of our
techniques for identifying relevant cross-domain apps, group-ing and ranking complementary cross-domain app features. Wealso make some observations from these results, and discussthe limitations of our approach and evaluation process.TABLE I
PRECISION (P), R ECALL (R) AND F-M EASURE (F) FOR IDENTIFYING
RELEV ANT CROSS -DOMAIN APPS
No Target Domain Precision (P) Recall (R) F-score (F)
1 Communication 0.93 0.86 0.89
2 Education 0.82 0.77 0.79
3 Book & Reference 0.71 0.84 0.76
4 Health & Fitness 0.87 0.89 0.88
5 Shopping 0.79 0.94 0.85
6 Finance 0.45 0.63 0.52
7 Lifestyle 0.74 0.78 0.75
8 Productivity 0.93 0.84 0.88
9 Business 0.94 0.96 0.94
10 Maps & Navigation 0.56 0.62 0.58
Average 0.77 0.81 0.78
A. Identiﬁcation of Relevant Cross-Domain Apps
We have run our relevant app identiﬁcation technique (see
Section III-A3) for each of the 100 target apps from 10target domains over the 15,200 apps in 31 categories of thecurated dataset (see Section IV-A). For example, for Facebook
Messenger as a target app from the Communication category,
we have identiﬁed more than 60 relevant cross-domain appsacross 8 different domains in our experimental dataset (i.e.,Tools, Social, Music & Audio, Photography, Productivity, En-tertainment, Video Players & Editors, and Lifestyle), with eachhaving feature similarity with Facebook Messenger higher thanthe set threshold δ=0.33.
To validate the relevant cross-domain apps identiﬁed by our
technique, we have manually created a truth set of 100 cross-domain apps corresponding to 10 target apps (one from eachcategory). Then, we run our relevant app identiﬁcation tech-nique on the apps in the truth set. The results are summarizedin Table I.
After running our experiment, we have found that our
technique produces mostly high precision, recall and F scores,with a large majority over 0.70. However, signiﬁcantly lowerP, R and F scores are observed for some target apps fromFinance (P=0.45) and Maps & Navigation (F= 0.58) domains.After further investigation, we have found that the target appsfrom the Finance and Maps & Navigation domains had a verypoor app description. For example, the DiDi app from the
Maps & Navigation domain had a very generic app descriptionwhich caused poor feature extraction and resulted in a poorrelevance score. However, for all the target apps, we haveachieved an average of 0.77, 0.81 and 0.78 as P, R, and Fscores, respectively. As such, our proposed technique has beenshown to be effective in ﬁnding relevant cross-domain apps fora given target app.
B. Semantics-based Feature Grouping
In these experiments, we compare our semantics-based
feature grouping technique with two other representative tech-
niques:
1)K-means Clustering is one of the most commonly used
grouping techniques in feature mining. The k-meansapproach starts off with a set of Kseeds and assign
documents (set of features in our case) to these seeds on
749TABLE II
COMPARISON BETWEEN FEATURE GROUPING TECHNIQUES (Facebook Messenger )
Video Editor File Transfer File upload Game Chat Save Files Share Translation Text to Speech Translation
'video editor'(10/10) 'transfer files'(10/10) 'app upload images' (9/10) 'face game'(3/10) 'save your effort'(10/10) share translati ons'(10/10) 'translate speech'(10/10)
'hd pro video editor'(10/10) 'transfer speed'(10/10) 'enables fil e upload'(10/10) 'strategy games'(5/10) 'save your effort space'( 7/10) 'share button'(3/10) 'speech recognition'(10/10)
'video editor maker'(9/10) 'transfer large files'(10/10) improves file upload'(10/10) chat sessions'(10/10) 'save videos'(10/10) 'share word definition'(8/10) 'speech text'(10/10)
editor transition effects'(10/10) 'transfer files friends'(10/10) 'file upload'(10/10) 'gamer chat'(10/10) 'status saver status'(4 /10) 'share text'(10/10) 'supported speech'(8/10)
'video editor emoji'(7/10) 'file transfer'(10/10) 'file uploads'(10/10) 'multiplayer games'(3/10) 'insta saver videos'(5/10) 'share texts'(10/10) 'include text speech'(8/10)
editor blur background'(10/10) 'transfer needs'(7/10) 'file upload easy'(10/10) 'fps games'(4/10) 'save high-quality photos'(10/10 )share  translations'(10/10) 'speech recognition'(10/10)
'video editor text'(8/10) 'transfer videos'(10/10) 'enable file upload'(10/10) 'watch gamers'(6/10) 'saved your gallery'(10/10) 'share your findings'(10/10) 'text speech'(8/10)
'video editing features'(10/10) 'transfer files friends'(10/10) 'file upload photos'(10/10) 'mmo rpg games(4/10) save favorite videos'(10/10) 'share word definition'(10/10) 'speech bubbles'(7/10)
'video editor'(10/10) 'transfer pictures'(10/10) 'upload videos'(10/10) 'online gamer chat'(10/10) 'save videos'(10/10) 'share photos'(10/10) 'text speech'(8/10)
'video editor'(10/10) 'transfer files'(10/10) 'upload video files'(10/10) 'online chat'(10/10) 'save images'(10/10) 'share texts'(10/10) speech translator'(10/10)
'video feature editor'(10/10) 'transfer music'(10/10) 'easy file upload'(10/10) 'game chat stream'(10/10) 'save favorite photos'(1 0/10) 'function share'(9/10) 'speech text feature'(10/10)
'feature editor'(10/10) 'transfer anything'(10/10) 'file upload s ) 0 1 / 4 ( ' e t i r o v a f  r u o y  e v a s ' ) 0 1 / 9 ( ' d e e p share translations'(10/10) translate  speech text'(10/10)
o v a f  e v a s ' ) 0 1 / 6 ( ' s p p a  r e f s n a r t ' ) 0 1 / 0 1 ( ' d n u o r g k c a b  r o t i d e  o e d i v ' rite file'(10/10) 'share translation'(10/10) 'use speech text feature'(9/10)
a s ' ) 0 1 / 0 1 ( ' a t a d  l a n o s r e p  r e f s n a r t ) 0 1 / 0 1 ( ' t i d e  d n u o r g k c a b  o e d i v ' ve fast image'(10/10) 'share option'(10/10)
'pro video editor'(10/10) 'transfer files'(10/10) allow share '(10/10)
'video editor effects'(10/10) 'transfer files quick'(10/10) share translations'(10/10)
'transfer files server'(10/10) 'share words'(9/10)
9 % 3 5 . 3 9 % 1 7 . 5 8 % 8 1 . 8 6 % 3 3 . 8 9 % 8 8 . 5 9 % 5 2 . 6 9     |     ) % (  y c a r u c c A 0.77%
Video Editor File Transfer File upload Game Chat Save Files Share Translation Text to Speech Translation
editor,(10/10) transfer(10/10) upload(10/10) chat(10/10) save(10/10) share(10/10) speech(7/10)
video,(10/10) files(10/10) file(10/10) gamer(6/10) favorite(6/10) translations(10/10) text(7/10)
background,(7/10) games(2/10) easy(10/10) online(10/10) videos(10/1 0) texts(8/10) recognition(10/10)
feature,(10/10) friends(10/10) uploads(10/10) sessions(10/10) effort(4/10) definition(5/10) translate(8/10)
effects,(7/10) status(2/10) videos(10/10) stream(3/10) photos(8/10) word(5/10) feature(5/10)
) 0 1 / 8 ( s e g a m i ) 0 1 / 0 1 ( e m a g ) 0 1 / 5 ( e l b a n e ) 0 1 / 3 ( s d e e n ) 0 1 / 5 ( , o r p words(5/10) bubbles(0/10)
emoji,(5/10) music(10/10) enables(5/10) function(8/10) space(4/10) button(0/10) supported(0/10)
maker,(10/10) apps(2/10) improves(10/10) features(8/10) insta(4/10) findings(3/10) include(0/10)
edit,(10/10) pictures(10/10) speed(10/10) file(2/10) image(10/10) function(4/10) translator(4/10)
editing,(10/10) speed(7/10) photos(10/10) files(2/10) fast(10/10) option(10/10) language(5/10)
features,(7/10) videos(7/10) app(10/10) findings(0/10) saver(10/10) translation(9/10) use(5/10)
) 0 1 / 4 ( h g i h ) 0 1 / 9 ( s e l i f ) 0 1 / 0 1 ( r e y a l p i t l u m ) 0 1 / 0 1 ( , r u l b photos(5/10) friends(8/10)
) 0 1 / 4 ( y t i l a u q ) 0 1 / 8 ( s e g a m i ) 0 1 / 6 ( s p f ) 0 1 / 0 1 ( , t x e t text(7/10) image(0/10)
) 0 1 / 4 ( e l i f ) 0 1 / 5 ( o e d i v ) 0 1 / 5 ( y g e t a r t s ) 0 1 / 6 ( n o i t i s n a r t allow(5/10)
) 0 1 / 0 1 ( e l i f ) 0 1 / 6 ( d h function(4/10) data(4/10) transfer(0/10)
5 4 % 3 3 . 7 5 % 7 5 . 8 6 % 3 7 . 2 6 % 4 1 . 7 8 % 3 3 . 9 6 % 0 0 . 2 8        |      ) % (  y c a r u c c A .38%
Video Editor File Transfer File upload Game Chat Save Files Share Translation Text to Speech Translation
video(10/10) speed(7/10) translate(0/10) file(2/10) save(10/10) share(10/10) speech(10/10)
editor(10/10) background(4/10) gallery(5/10) upload(1/10) video(9/10) translation(10/10) text(10/10)
) 0 1 / 4 ( e t i r o v a f ) 0 1 / 0 1 ( e m a g ) 0 1 / 7 ( d e v a s ) 0 1 / 2 ( r u l b ) 0 1 / 4 ( r e f s n a r t recognition(4/10) feature(5/10)
) 0 1 / 2 ( a t s n i ) 0 1 / 0 1 ( t a h c ) 0 1 / 4 ( n o i t p o ) 0 1 / 3 ( g n i t i d e ) 0 1 / 5 ( e l i f word(5/10) use(4/10)
friend(3/10) feature(5/10) personal(4/10) photo(3/10) fast(8/10) definition(5/10) translator(8/10)
) 0 1 / 8 ( y t i l a u q h g i h ) 0 1 / 4 ( r e f s n a r t ) 0 1 / 6 ( a t a d ) 0 1 / 6 ( o e d i v ) 0 1 / 2 ( o r p effect(3/10) language(5/10)
emoji(1/10) editor(10/10) anything(1/10) image(2/10) background(4/1 0) allow(2/10) bubble(0/10)
) 0 1 / 2 ( t i d e ) 0 1 / 8 ( r e m a g ) 0 1 / 2 ( d h ) 0 1 / 0 1 ( r e f s n a r t ) 0 1 / 2 ( s p p a button(0/10) include(0/10)
maker(3/10) upload(10/10) transfer(4/10) saver(2/10) picture(2/10) finding(2/10) supported(3/10)
) 0 1 / 0 1 ( r e v a s ) 0 1 / 2 ( s u t a t s ) 0 1 / 2 ( o r p ) 0 1 / 8 ( e l i f ) 0 1 / 1 ( d h function(4/10) recognition(3/10)
effect(8/10) gallery(7/10) speech(0/10) easy(0/10) photo(5/10) transition(3/10) translate(8/10)
) 0 1 / 2 ( e g a m i ) 0 1 / 7 ( e n i l n o ) 0 1 / 1 ( t x e t ) 0 1 / 4 ( y t i l a u q h g i h ) 0 1 / 4 ( d e e n need(5/10) editing(8/10)
transition(2/10) insta(2/10) share(8/10) effort(2/10) effort(6/10) maker(2/10) share(7/10)
picture(2/10) status(1/10) editor(2/10) enable(7/10) space(7/10) option(5/10) editor(3/10)
music(2/10) saver(7/10) video(6/10) improves(6/10) file(8/10) photo(1/10) video(0/10)
9 4 % 7 6 . 0 4 % 3 4 . 6 5 % 6 8 . 2 4 % 6 8 . 2 3 % 3 3 . 7 5 % 3 3 . 9 3       |       ) % (  y c a r u c c A .33%K-Means Feature Clustering
Overall Accuracy (%) 67.49%
Topic Modelling (LDA-based) Feature Grouping
Overall Accuracy (%) 45.54%Overall Accuracy (%) 89.81%6HPDQWLFVbased )HDWXUH Grouping
the basis of closest similarity. A new seed is deﬁned in
each next iteration so that it is a better central point for thegroup and this approach is continued until convergence. Anumber of recent studies employ k-means clustering forfeature categorisation with good performance [52], [53].
2)Latent Dirichlet Allocation (LDA) is a generative proba-
bilistic topic modelling technique used for topic identiﬁ-cation from documents containing a mixture of topics intextual corpora. LDA is a popular unsupervised machinelearning technique used by researchers to identify topicsfrom a group of terms [38], [54], [55].
In this work, we run our semantics-based feature grouping,
k-means clustering and topic modeling (LDA) techniques onthe identiﬁed coexisting cross-domain features for each of the100 target apps. To be consistent with our feature groupingtechnique, we set the value of k(number of clusters in k-
means or number of topics in LDA) to be the same as thenumber of groups generated by our approach. Each resultingcoexisting group is given a name or label. For example,Table II summarizes the grouping outcomes for Facebook
Messenger.
We validate the grouping outcomes of all three techniques
by 10 software practitioners (7 PhD students, 1 researcher,and 2 software developers) and asked them to score if theyﬁnd the feature grouping is appropriate in terms of semanticsimilarity. For example, the user scores 1 if they ﬁnd thatfeature video transition effects is relevant to the feature group
video editor. We ﬁnd that LDA performs the lowest whilek-means clustering performs slightly better than LDA. Ourtechnique outperforms the other two techniques in termsof individual groups and overall group averages, with ourtechnique’s overall accuracy being above 89%. This validatesthe effectiveness of our technique.
750C. Feature Ranking
We run the feature ranking experiments on all the comple-
mentary features (i.e., coexisting feature groups) for a target
app. For example, in our dataset, we have found severalcomplementary features for Facebook Messenger (as the target
app), including video editor , ﬁle transfer , save ﬁles, etc. Then,
we run our ranking technique (see Section III-C) on thesegroups rank them for prioritised recommendation to the targetapp. For example, the rankings for the coexisting featuregroups for Facebook Messenger (see Table II) is shown inTable III.
TABLE III
RANKING OF COMPLEMENTARY CROSS -DOMAIN FEATURES
No Feature Group Coexisting Features Ranking
1 {File Transfer} { transfer large ﬁles, ....., ...} +0.83
2 {Save File} { save videos, save images, ...} +0.77
3 {Video Editor} { video editor, feature editor, ...} +0.74
4 {File Upload} { improve ﬁle upload, ...., .....,} +0.71
5 {Game Chat} { online chat, face game,....} +0.62
We run feature ranking for all the target apps in our dataset
and evaluated our ranking results using NDCG@k. NDCGis normally used to rank documents retrieved from searchengines. DCG is the actual ranking of documents whereasIDCG is the ideal ranking of the same documents, usuallyfound from a truth set. In our case, we calculate IDCG from thetruth set with a user study involving 10 software practitioners(as mentioned in Section V-A). Since it is infeasible for ourusers to rank all complementary features for 100 target apps,we randomly selected 10 target apps (one per domain) andidentiﬁed a total of 74 complementary features. Then, weasked the users to rank the features recommended for eachof the target app based on the ranking attributes explained inSection III-C. In this way, an IDCG score can be calculatedfor each of the identiﬁed complementary feature.
Table IV summarises the overall NDCG@5 ranking evalu-
ation that considers the top 5 ranked complementary featuresextracted from the cross-domain apps. For clear understand-ing, we also include additional attributes such as number of
coexisting features, number of cross-domain apps and number
of cross-domains relating to each complementary feature for
a target app. As can be seen in Table IV, the number ofrelevant cross-domain apps varies depending on the targetapp. For example, 62 relevant apps were found for FacebookMessenger from 8 different domains, and 14 found for Zoomfrom 4 domains. However, GumTree has only one relevantcross-domain app CarSales from the LifeStyle domain, andhas only one complementary feature live ad stats. Since there
is only one complementary feature found for GumTree, itsNDCG@5 score is 1. No score is determined for the Canvasapp because no relevant app was found from other domains.
In summary, based on our empirical evaluation, the
NDCG@5 scores are higher than 0.63 for each of the targetapps. Though there is a slight difference in ranking per targetapp per domain, the overall average NDCG@5 score is higherthan 0.75 across all categories for all the target apps. Thisvalidates the effectiveness of our approach.
D. Discussion
From our experiments, we have drawn the following obser-
vations and actionable insights that might be helpful for the
target app developers.
Observation-1: A very popular app, such as Facebook
Messenger, standing at the top rank for a long period of timeusually offers multiple popular features as well as a well-written description of those features. Due to this, it matcheswith a large number of cross-domain apps (60+ apps from8 domains for Facebook Messenger ), meaning that this app
has more similarity with cross-domain apps. As such, moresimilar features are found from other domains than dissimilarcoexisting (i.e., complementary) features.
Observation-2: Apps with more speciﬁc but less diverse
features match very speciﬁc cross-domain apps with simi-lar features. For example, ‘ Fitbit ’ from the Health domain
matches with apps from 3 domains (Lifestyle, Tools, and
Personalisation). The same happens in the case of Gmail from
the Communication domain, which contains speciﬁc featuresrelevant to email communications only.
Observation-3: There is high correlation shown between
the target domain and the cross-domains related to that targetdomain. For example, apps from the Communication domainhave high relevance with the features from the Productivityand Tools domain. More than 65% of the complementaryfeatures for the Communications domain are found from theProductivity and the Tools domains, and more than 50%complementary features for the Books and Reference domainare found from the Education domain.
Observation-4: Unpopular or speciﬁc apps match only apps
from the same domain and not those from cross-domains.For example, the ‘DiDi’ app from the Maps & Navigationdomain and the ‘Canvas’ app from the Education domain didnot match any of the cross-domain apps. Although both appsare popular in their own domains, they offer features speciﬁcto only their own domains. Hence, no relevant apps or featuresare found from other domains.
Actionable Insight-1: Based on observation-1, the pop-
ular apps usually have a number of cross-domain features.Therefore, developers might consider that the only reason forapp popularity is “borrowing” features from cross-domains.However, after further investigation, we have found that thisis not necessarily true. First, some popular apps do not havecross-domain features at all. For example, the ‘DiDi’ app from
the Maps & Navigation domain and the ‘Canvas’ app from
the Education domain did not match any of the cross-domainapps although they are ranked top in their own domains.Second, some unpopular apps may have a number of cross-domain features. For example, ‘TickTick’ is an unpopular app
from the Productivity domain which matches 36 different appsfrom 5 cross-domains (including Lifestyle, Health & Fitness,Education, Business and Tools).
751TABLE IV
NDCG@5 V ALIDATION RESULTS FOR COMPLEMENTARY CROSS -DOMAIN FEATURE RANKING
Target App (Category) Complementary Feature #Coexisting Features #Cross-Domain Apps #Cross-Domains Rank Score
Facebook Messenger
(Communication)File Transfer 17
62 8 0.86Save File 14
Video Editor 16
File Upload 12
Game Chat 11
Wikipedia(Books_and_Reference)Share your knowledge 11
12 3 0.77 Search Results 8
V oice Translation 7
Canvas (Education) - - - - -
Gum Tree (Shopping) Live Ad Stats 1 1 1 1.0
Zoom(Business)Use Filters 6
14 4 0.72Message People 14
Share Location 4
Watch News 3
Use Custom Sticker 2
TickTick(Productivity)Increase Positive Energy 6
18 5 0.80Receive Daily Quotes 7
Reduce Your Anxiety 12
Refresh Morning Routine 15
Instarem(Finance)Payout Location 3
9 2 0.94 Exchange Rate Notiﬁcation 8
Video Support 4
My 7-Eleven(Maps & Navigation)Show Price 54 3 1.0Save Money 6
Earn Point 2
Notebooks(LifeStyle)Record Audio Note 921 4 0.64Attach Files 8
Format Your Notes 5
Fitbit(Health & Fitness)Achieve Your Goal 8
17 3 0.78Analyze Report 5
Show Distance Score 3
Evaluate User Data Routine 4
*Note: Each complementary feature corresponds to a set of similar coexisting features from cross-domain apps that are relevant to the target app
Actionable Insight-2: When the app description is very
short (usually, 3-7 sentences), vague or does not provide
clear information about the app features, the feature extractiontechnique is unable to identify the features as expected. Itmostly happens for low-rank apps or new apps or beta releasesof apps where developers did not explain their app featuresformally and properly. This may result in the extraction ofinaccurate features or no features at all, which ultimately leadsto ﬁnding no relevant features from cross-domains. Therefore,developers should pay attention to providing clear and speciﬁcfunctional feature descriptions on the app description page.
Actionable Insight-3: The fact that some apps have fea-
ture relevance with only one or two cross-domain apps mayindicate that there might be miscategorisation of them. Thiscan happen because developers are free to upload their appsin any category regardless of the capabilities of their apps.Further investigation shows that some miscategorised appsare not popular. It is possible that some developers uploadthe beta-version of their apps in different but closely-relatedcategories to test the users’ interests.
Actionable Insight-4: App developers should ﬁnd the
recommended features meaningful enough to understand asthe extracted features can have up to n-gram/word long (n= 4 in our case). For example, the use speech text feature
is a feature extracted from an app description, which is 4-grams/words. Further examples can be found in Section III-A2and Table II. In case that further details are needed forrecommended features to gain a fuller understanding, we alsokeep track of all the apps and app descriptions correspondingto the extracted and recommended features via their uniqueApp IDs. For example, transfer ﬁles is a popular feature
from app SHAREit, and the linking of cross-domain featuresand apps in Section III-B shows that transfer ﬁles is from
com.lenovo.anyshare.gps for app SHAREit. As such, we can
easily trace a recommended feature back to its source (i.e., thecorresponding app description) for more details when needed.
E. Threats to V alidity
We have extracted features from both the target app and
the cross-domain apps using an existing work [40]. We also
consider that all the features extracted from app descriptionsare functional features. But, not all the features extractedby this method are actual features. This technique may notextract proper features if the app description is not written inproper and formal English [40]. However, to the best of ourknowledge, this work produces the highest precision in featureextraction from text description written in formal English.
When creating the truth set for matching cross-domain app
features and ranking coexistent feature groups, we have solelyrelied on human judgment. Since feature relevance mapping is
752subjective, we have carried out a careful analysis to minimise
biases. We have also made sure that all disagreements wereresolved by debate and majority agreements among the au-thors. Moreover, 10 software practitioners with varying appdevelopment experiences are included in the user study toachieve better diversity.
A threat to external validity is the representativeness of
the domains/categories and the target apps selected in theexperiments. The experimental results are speciﬁc to thechosen target apps and may not be generalisable to otherapps. To minimize this threat, we have randomly selectedthe 100 target apps (from 15,200 apps) that belong to 10different app categories. For each target app, we have alsomade sure to run experiments across all the apps in allthe other 30 categories in the dataset. Furthermore, a usersurvey with excessive questions may discourage participantsfrom participating or completing the survey while a surveywith inadequate questions will make it difﬁcult to collectcomprehensive feedback. To ensure a proper user survey, wehave conducted a preliminary user study and found that a totalof 10 questions (one for each target app) are suitable for ouruser survey.
For recommendation purposes, we have intended to validate
our feature grouping and ranking techniques for all target apps.However, it is infeasible to do it all manually. To minimize thisthreat we took 100 features randomly from the feature groupsand validated by 10 annotators who are not authors. This isan standard approach followed by other researchers [31], [52],[53].
VI. C
ONCLUSION AND FUTURE WORK
In this work, we have revealed the importance of cross-
domain features for the evolution of software apps. We haveproposed a new approach that mines cross-domain featuresto make personalised feature recommendations for any targetapp. To the best of our knowledge, this is the ﬁrst work thatmines popular complementary cross-domain features from appstore repositories. Firstly, we identify all the apps across other
domains that are relevant to the target app, where we usefeature-based similarity between the target app and each ofthe cross-domain apps to calculate the functional relevance ofthe cross-domain app. Secondly, we perform semantic-based
feature grouping on the features of all the relevant cross-domain apps that are dissimilar to the target app’s featuresto partition them into feature groups with each having similarfeatures. Finally, we rank those feature groups to prioritise
them for recommendation to the target app’s developer. Wehave carried out comprehensive experiments on 100 targetapps from 10 categories over a 15,200 app population from31 categories. These experiments have generated encouragingresults in identifying, grouping and ranking cross-domain fea-tures for recommendation with an accuracy rate of over 89%.In addition, our semantics-based feature grouping techniquehas also been show to signiﬁcantly outperform two existingbaseline techniques. In general, the empirical evaluation vali-dates the usefulness of our approach.In the future, we plan to extract and incorporate the re-
quested features from the reviews of target apps when buildingfeature proﬁles for these apps, putting an emphasis on rankingcross-domain features requested in reviews. We also plan toincorporate sentiment analysis into the feature recommenda-tion process to improve recommendation accuracy.
753REFERENCES
[1] “Covid-19 apps,” https://en.wikipedia.org/wiki/COVID- 19\_apps,
accessed: 2021-03-07.
[2] “Smart phone users around the world,” h ttps://www.bankmycell.com/b
log/how-many-phones-are-in-the-world, accessed: 2021-03-08.
[3] “Google inc. and apple inc. paid to developers,” https://www.theverge.c
om/2020/2/4/21121558/google-80-billion-android-developers-apple-15
5, accessed: 2021-03-08.
[4] B. Carbunar and R. Potharaju, “A longitudinal study of the google
app market,” in Proceedings of the 2015 IEEE/ACM International
Conference on Advances in Social Networks Analysis and Mining 2015,2015, pp. 242–249.
[5] N. Zhong and F. Michahelles, “Google play is not a long tail market: An
empirical analysis of app adoption on the google play app market,” inProceedings of the 28th annual ACM symposium on applied computing,2013, pp. 499–504.
[6] G. J. Balady, “Survival of the ﬁttest—more evidence,” 2002.
[7] D. Lim, H. Lee, J. Yoo, and H. Zo, “Free to paid: Purchase and dropout
behavior of mobile application users.” in PACIS, 2013, p. 133.
[8] F. Sarro, M. Harman, Y . Jia, and Y . Zhang, “Customer rating reactions
can be predicted purely using app features,” in 2018 IEEE 26th Inter-
national Requirements Engineering Conference (RE). IEEE, 2018, pp.
76–87.
[9] M. Khurum, T. Gorschek, and M. Wilson, “The software value map—an
exhaustive collection of value aspects for the development of softwareintensive products,” Journal of software: Evolution and Process, vol. 25,
no. 7, pp. 711–741, 2013.
[10] B. Fu, J. Lin, L. Li, C. Faloutsos, J. Hong, and N. Sadeh, “Why people
hate your app: Making sense of user feedback in a mobile app store,”inProceedings of the 19th ACM SIGKDD international conference on
Knowledge discovery and data mining, 2013, pp. 1276–1284.
[11] N. Chen, J. Lin, S. C. Hoi, X. Xiao, and B. Zhang, “Ar-miner: mining
informative reviews for developers from mobile app marketplace,” inProceedings of the 36th International Conference on Software Engi-neering. ACM, 2014, pp. 767–778.
[12] J. Lin, K. Sugiyama, M.-Y . Kan, and T.-S. Chua, “New and improved:
modeling versions to improve app recommendation,” in Proceedings of
the 37th international ACM SIGIR conference on Research & develop-ment in information retrieval, 2014, pp. 647–656.
[13] C. Gao, J. Zeng, M. R. Lyu, and I. King, “Online app review analysis for
identifying emerging issues,” in Proceedings of the 40th International
Conference on Software Engineering, 2018, pp. 48–58.
[14] E. Guzman, M. Ibrahim, and M. Glinz, “A little bird told me: mining
tweets for requirements and software evolution,” in 2017 IEEE 25th
International Requirements Engineering Conference (RE) . IEEE, 2017,
pp. 11–20.
[15] S. Panichella, A. Di Sorbo, E. Guzman, C. A. Visaggio, G. Canfora,
and H. C. Gall, “How can i improve my app? classifying user reviewsfor software maintenance and evolution,” in 2015 IEEE International
Conference on Software Maintenance and Evolution (ICSME). IEEE,2015, pp. 281–290.
[16] E. Guzman and W. Maalej, “How do users like this feature? a ﬁne
grained sentiment analysis of app reviews,” in Requirements Engineering
Conference (RE). IEEE, 2014, pp. 153–162.
[17] L. Chen, Y . Yang, N. Wang, K. Yang, and Q. Yuan, “How serendipity
improves user satisfaction with recommendations? a large-scale userevaluation,” in The World Wide Web Conference, 2019, pp. 240–250.
[18] F. Sarro, A. A. Al-Subaihin, M. Harman, Y . Jia, W. Martin, and Y . Zhang,
“Feature lifecycles as they spread, migrate, remain, and die in appstores,” in Requirements Engineering Conference (RE), 2015 IEEE 23rd
International. IEEE, 2015, pp. 76–85.
[19] “Report: Australia competition & consumer commission,” https://www.
accc.gov.au/publications/serial-publications/digital-platform-services-inquiry-2020-2025/digital-platform-services-inquiry-september-2020-interim-report, accessed: 2021-01-18.
[20] “Report: App annie,” https://s3.amazonaws.com/files.appannie.com/repo
rts/App-Annie\_Mobile-App-Evolution-Report\_2020-07.pdf, accessed:2021-02-13.
[21] A. blogs, “Top mobile games that have social features incorporated,”
https://blog.getsocial.im/top-mobile-games-that-have-social-features-incorporated/, Jan. 2021, accessed: 2021-01-30.[22] A. Drachen, M. Pastor, A. Liu, D. J. Fontaine, Y . Chang, J. Runge,
R. Sifa, and D. Klabjan, “To be or not to be... social: Incorporatingsimple social features in mobile game customer lifetime value pre-dictions,” in Proceedings of the Australasian Computer Science Week
Multiconference, 2018, pp. 1–10.
[23] theconversation.com, “Tiktok is a unique blend of app,” https://thecon
versation.com/tiktok-is-a-unique-blend-of-social-media-platforms-her
es-why-kids-love-it-144541, Jan. 2021, accessed: 2021-01-30.
[24] techcrunch.com, “Tiktok consumer engagement,” https://techcrunch.c
om/2020/07/27/top-mobile-apps-see-declines-in-consumer-engagement-amid-increased-competition/, Jan. 2021, accessed: 2021-01-30.
[25] inﬂuencermarketinghub.com, “Tiktok growth,” https://inﬂuencermarketi
nghub.com/tiktok-growth/, Jan. 2021, accessed: 2021-01-30.
[26] L. Villarroel, G. Bavota, B. Russo, R. Oliveto, and M. Di Penta, “Release
planning of mobile apps based on user reviews,” in Proceedings of the
38th International Conference on Software Engineering . ACM, 2016,
pp. 14–24.
[27] Y . Tian, M. Nagappan, D. Lo, and A. E. Hassan, “What are the charac-
teristics of high-rated apps? a case study on free android applications,”in2015 IEEE Intl. Conf. on SW Maintenance and Evolution (ICSME),.
IEEE, 2015, pp. 301–310.
[28] E. Guzman, R. Alkadhi, and N. Seyff, “A needle in a haystack: What
do twitter users say about software?” in 2016 IEEE 24th International
Requirements Engineering Conference (RE). IEEE, 2016, pp. 96–105.
[29] X. Gu and S. Kim, “" what parts of your apps are loved by users?"(t),”
inAutomated Software Engineering (ASE), 2015 30th IEEE/ACM Inter-
national Conference on. IEEE, 2015, pp. 760–770.
[30] “Nltk, natural language toolkit,” http://www.nltk.org, accessed: 2020-
01-30.
[31] A. A. Al-Subaihin, F. Sarro, S. Black, L. Capra, M. Harman, Y . Jia, and
Y . Zhang, “Clustering mobile apps based on mined textual features,” inProc. of the 10th ACM Intl. Symposium on Empirical SW Engg. andMeasurement. ACM, 2016, p. 38.
[32] W. Martin, F. Sarro, Y . Jia, Y . Zhang, and M. Harman, “A survey of app
store analysis for software engineering,” IEEE transactions on software
engineering, vol. 43, no. 9, pp. 817–847, 2016.
[33] M. Harman, Y . Jia, and Y . Zhang, “App store mining and analysis: MSR
for app stores,” in Proceedings of the 9th IEEE Working Conference on
Mining Software Repositories. IEEE Press, 2012, pp. 108–111.
[34] A. Ahmad, C. Feng, K. Li, S. M. Asim, and T. Sun, “Toward empirically
investigating non-functional requirements of ios developers on stackoverﬂow,” IEEE Access, vol. 7, pp. 61 145–61 169, 2019.
[35] N. Jha and A. Mahmoud, “Mining non-functional requirements from
app store reviews,” Empirical Software Engineering, vol. 24, no. 6, pp.
3659–3695, 2019.
[36] M. Lu and P. Liang, “Automatic classiﬁcation of non-functional require-
ments from augmented app user reviews,” in Proceedings of the 21st
International Conference on Evaluation and Assessment in SoftwareEngineering, 2017, pp. 344–353.
[37] L. Hoon, R. Vasa, J.-G. Schneider, J. Grundy et al., “An analysis of
the mobile app review landscape: trends and implications,” Faculty of
Information and Communication Technologies, Swinburne University ofTechnology, Tech. Rep, 2013.
[38] Y . Liu, L. Liu, H. Liu, X. Wang, and H. Yang, “Mining domain
knowledge from app descriptions,” Journal of Systems and Software,
vol. 133, pp. 126–144, 2017.
[39] D. Lavid Ben Lulu and T. Kuﬂik, “Functionality-based clustering using
short textual description: helping users to ﬁnd apps installed on theirmobile device,” in Proceedings of the 2013 international conference on
Intelligent user interfaces. ACM, 2013, pp. 297–306.
[40] T. Johann, C. Stanik, W. Maalej et al., “SAFE: A simple approach for
feature extraction from app descriptions and app reviews,” in 2017 IEEE
25th International Requirements Engineering Conference (RE). IEEE,2017, pp. 21–30.
[41] Stanford, “Stanford pos-tagger library: The richest pos tagger library
online,” htt ps://nlp.stanford.edu/software/tagger.shtml/, Feb. 2019,
accessed: 2019-02-01.
[42] P. University, “W ordnet:lexical dictionary,” https://wordnet.princeton.ed
u, Jan. 2019, accessed: 2019-01-30.
[43] G. A. Miller, “Wordnet: a lexical database for english,” Communications
of the ACM , vol. 38, no. 11, pp. 39–41, 1995.
[44] Y . Goldberg and O. Levy, “word2vec explained: deriving mikolov et al.’snegative-sampling word-embedding method,” arXiv :1402.3722, 2014.
754[45] Z. Wu and M. Palmer, “Verbs semantics and lexical selection,” in Pro-
ceedings of the 32nd annual meeting on Association for Computational
Linguistics. Association for Computational Linguistics, 1994, pp. 133–138.
[46] Y . Ouyang, B. Guo, X. Lu, Q. Han, T. Guo, and Z. Yu, “Competitivebike:
Competitive analysis and popularity prediction of bike-sharing appsusing multi-source data,” IEEE Transactions on Mobile Computing,
vol. 18, no. 8, pp. 1760–1773, 2018.
[47] Y . Wang, N. J. Yuan, Y . Sun, C. Qin, and X. Xie, “App download
forecasting: An evolutionary hierarchical competition approach.” inIJCAI, 2017, pp. 2978–2984.
[48] E. Malmi, “Quality matters: Usage-based app popularity prediction,”
inProceedings of the 2014 ACM International Joint Conference on
Pervasive and Ubiquitous Computing: Adjunct Publication, 2014, pp.391–396.
[49] A. Finkelstein, M. Harman, Y . Jia, F. Sarro, and Y . Zhang, “Mining app
stores: Extracting technical, business and customer rating informationfor analysis and prediction,” RN, vol. 13, p. 21, 2013.
[50] J. Han, J. Pei, and M. Kamber, Data mining: concepts and techniques.Elsevier, 2011.
[51] Y . Wang, L. Wang, Y . Li, D. He, W. Chen, and T.-Y . Liu, “A theoretical
analysis of ndcg ranking measures,” in Proceedings of the 26th annual
conference on learning theory (COLT 2013), vol. 8, 2013, p. 6.
[52] A. Al-Subaihin, F. Sarro, S. Black, and L. Capra, “Empirical comparison
of text-based mobile apps similarity measurement techniques,” Empiri-
cal Software Engineering, vol. 24, no. 6, pp. 3290–3315, 2019.
[53] P. M. Vu, T. T. Nguyen, H. V . Pham, and T. T. Nguyen, “Mining user
opinions in mobile app reviews: A keyword-based approach (t),” in2015 30th IEEE/ACM International Conference on Automated SoftwareEngineering (ASE). IEEE, 2015, pp. 749–759.
[54] E. Noei, D. A. Da Costa, and Y . Zou, “Winning the app production
rally,” in Proceedings of the 2018 26th ACM Joint Meeting on European
Software Engineering Conference and Symposium on the F oundationsof Software Engineering, 2018, pp. 283–294.
[55] M. Nayebi, H. Cho, and G. Ruhe, “App store mining is not enough for
app improvement,” Empirical Software Engineering, vol. 23, no. 5, pp.
2764–2794, 2018.
755