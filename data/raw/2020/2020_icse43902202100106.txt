Shipwright: A Human-in-the-Loop System
for Dockerï¬le Repair
Jordan HenkelÂ§, Denini SilvayÂ§, Leopoldo Teixeiray, Marcelo dâ€™Amorimy, and Thomas Reps
University of Wisconsinâ€“Madison, Madison, WI, USA
yFederal University of Pernambuco, Recife, PE, Brazil
{jjhenkel,reps}@cs.wisc.edu {dgs,lmt,damorim}@cin.ufpe.br
Abstract â€”Docker is a tool for lightweight OS-level virtu-
alization. Docker images are created by performing a build,
controlled by a source-level artifact called a Dockerï¬le. We
studied Dockerï¬les on GitHub, andâ€”to our great surpriseâ€”
found that over a quarter of the examined Dockerï¬les failed to
build (and thus to produce images). To address this problem, we
propose S HIPWRIGHT , a human-in-the-loop system for ï¬nding
repairs to broken Dockerï¬les. S HIPWRIGHT uses a modiï¬ed
version of the BERT language model to embed build logs and
to cluster broken Dockerï¬les. Using these clusters and a search-
based procedure, we were able to design 13 rules for making
automated repairs to Dockerï¬les. With the aid of S HIPWRIGHT ,
we submitted 45 pull requests (with a 42.2% acceptance rate)
to GitHub projects with broken Dockerï¬les. Furthermore, in
a â€œtime-travelâ€ analysis of broken Dockerï¬les that were later
ï¬xed, we found that S HIPWRIGHT proposed repairs that were
equivalent to human-authored patches in 22.77% of the cases we
studied. Finally, we compared our work with recent, state-of-the-
art, static Dockerï¬le analyses, and found that, while static tools
detected possible build-failure-inducing issues in 20.6â€“33.8% of
the ï¬les we examined, S HIPWRIGHT was able to detect possible
issues in 73.25% of the ï¬les and, additionally, provide automated
repairs for 18.9% of the ï¬les.
Keywords-Docker, DevOps, Repair
I. I NTRODUCTION
Docker is one the most widely used tools for virtualization.
With79% of IT companies using it [1], Docker has made
an impact on developersâ€™ day-to-day work. Developers use
Docker to author images via an artifact called a Dockerï¬le.
Images can be based on a variety of operating systems, but
primarily, Docker images are Linux-based. Dockerï¬les are,
effectively, a linear sequence of â€œsetup instructionsâ€ that tell
the Docker engine how to prepare an image. The ï¬nal built
image is then used to run Docker containers. These containers
are similar to lightweight virtual machines. Each container
starts with the clean environment speciï¬ed by its originating
Docker image. Together, images and containers allow for
isolation, scaling, and reproducibility.
Nonetheless, we found that over 26% of the analyzed
samples of Dockerï¬les obtained â€œin the wildâ€ (sourced from
GitHub) failed to build successfully. This high a percentage
is surprising, because it runs counter to one of the core tenets
of Docker, namely, reproducibility. Furthermore, it is outside
of the scope of recent efforts to statically analyze Dockerï¬les
Â§Equal contributions.to detect such failures. For example, Hadolint [2] can detect
mistakes such as missing or incorrect ï¬‚agsâ€”e.g., forgetting
the use of -assume-yes/-y when invoking apt-get install
in a Dockerï¬le. This ï¬‚ag is required because a Dockerï¬le
build runs without interaction; therefore, forgetting this ï¬‚ag
may cause the build to hang. Unfortunately, while Hadolint
can detect such a mistake statically, many breakages occur
due to a change in the external environment andnot a change
in the source Dockerï¬le. These observations add to the mount-
ing evidence that external changes, such as dependencies
changing, can often lead to broken build-related artifacts [3],
[4]. As further evidence of this trend, our prototype tool,
SHIPWRIGHT , was used to guide 19 accepted pull requests
on GitHub and, for each of these patches, the underlying
issue was caused by a change in the external environment
(see Section II-A for an example of one such change).The Problem: Over a quarter of the GitHub repositories
with Dockerï¬les that we analyzed had a broken Dock-
erï¬le. Current state-of-the-art (static) analysis for Dock-
erï¬les is largely incapable of detecting and/or repairing
such broken Dockerï¬les.
Given this situation, our work seeks to meet the following
high-level goal:
Our Goal. Aid developers in automatically repairing
broken Dockerï¬les, with the hope of reducing the high
percentage of broken Dockerï¬les that we observed on
GitHub.
A. Contributions
Our work makes the following contributions: 1) Tech-
nique. We introduce a human-in-the-loop approach to ï¬xing
broken Dockerï¬les; 2) Tool. We made available a tool imple-
menting our technique; and 3) Data. We made available an
extension of the binnacle dataset [5], including build logs.
Technique. Unlike previous approaches that attempt to mine
patterns automatically and directly from Dockerï¬les, S HIP-
WRIGHT follows a human-in-the-loop approach to build a
repair database. We include human supervision to broaden
the effectiveness of S HIPWRIGHT : a fully automatic approach
would limit the scope of repairs that could be detected.
SHIPWRIGHT is designed to act as a â€œco-pilotâ€ that can side-
step the limitations of a completely automatic approach with
11482021 IEEE/ACM 43rd International Conference on Software Engineering (ICSE)
1558-1225/21/$31.00 Â©2021 IEEE
DOI 10.1109/ICSE43902.2021.00106
the goal of constructing a comprehensive database of repairs.
To build such a database, S HIPWRIGHT uses clustering, human
supervision, and a search-based recommendation system we
built to leverage vast community-knowledge bases, such as
StackOverï¬‚ow and Dockerâ€™s community forum. In general, we
require repairs to incorporate both some kind of pre-condition
(pattern) and a transformation function (patch).
During clustering, one challenge that S HIPWRIGHT needs
to address is the heterogeneity of the data, which is a mix-
ture of code and natural language. The mixing of code and
natural language makes it non-trivial to design a featurization
method for clustering. Therefore, to address this challenge,
SHIPWRIGHT uses a modiï¬ed version of Googleâ€™s BERT
model [6], [7] to embed Dockerï¬le build logs. By using a pre-
trained transformer-based neural model, we are able to side-
step tedious feature engineering, and beneï¬t from the diverse
corpora on which BERT-based models have been trained.
Using this embedding, we perform clustering with HDBSCAN
[8], in the vector space of embedded build logs, and use the
results to cluster failing builds. By using the vectors generated
through BERT, we leverage the â€œunderstandingâ€ encoded in
BERTâ€™s language model. To our surprise, we found that
recent off-the-shelf language models work well in this domain.
Using the generated clusters, we employ human supervision
to intuit, for each cluster, a likely root cause of failure and,
if possible, engineer one or more automated repairs to save
to S HIPWRIGHT â€™s database. Later, S HIPWRIGHT will use this
human-generated repair database to attempt automatic repair
of failing Dockerï¬les.
Tool. The scripts to automate each of the steps of S HIP-
WRIGHT (see Figure 3) are publicly available at https://github.
com/STAR-RG/shipwright.
Data. We have identiï¬ed a subset of Dockerï¬les from the
binnacle dataset [5] that are amenable to automated builds.
(The dataset and ï¬ltering criteria are described in Section III.)
We have built these ï¬les in-context (an expensive operation
that requires hundreds of hours of compute time), and captured
detailed data from the results, including logs from the builds.
These build logs represent a signiï¬cant expansion of the data
in the original binnacle dataset, and it is our hope that this
extended data will accelerate research on diagnostic tools for
Dockerï¬le analysis and repair. This expanded data is available
at https://github.com/STAR-RG/shipwright.
B. Evaluation
We evaluated several aspects of S HIPWRIGHT ; a summary
of our results is as follows: (i) Broken Dockerï¬les are preva-
lent: in the data we analyzed, 26.3% of Dockerï¬les failed to
build. (ii) Even using optimistic criteria, existing static tools
are capable of identifying the cause of a failure in only 20.6%â€“
33.8% of the broken Dockerï¬les. (iii) S HIPWRIGHT is capable
of clustering broken Dockerï¬les and offering actionable solu-
tions: for ï¬les that clustered, S HIPWRIGHT provides automated
repairs in 20.34% of the cases; for ï¬les that did not cluster,
SHIPWRIGHT is still able to provide automated repairs in18.18% of cases. (iv) In a â€œtime-travelâ€ analysis, we found that
SHIPWRIGHT would be able to provide actionable solutions
to 98.04% of the Dockerï¬les that we found initially broken
and then subsequently ï¬xed in their respective repositories.
Finally, we used the reports from S HIPWRIGHT to submit
45 Pull Requests to still-broken Dockerï¬les. Of these, 19
have been accepted. These results provide initial, yet strong,
evidence that S HIPWRIGHT is useful to help developers ï¬x
broken Dockerï¬les.
II. S OURCES OF BUILD FAILURES
This section describes some of the distinct sources of
problems that can lead to build failures in Dockerï¬les.
A. Breaking Changes in External Files
This kind of failure occurs when a Dockerï¬leâ€™s external
dependencies (such as the ï¬leâ€™s base image or URLs embedded
within the ï¬le) are changed; often these changes external to
the Dockerï¬le will require a change in the Dockerï¬le itself. To
illustrate this problem, consider the case where the developer
used latest to indicate the version of the base image of
her Dockerï¬le, as in FROM ubuntu:latest. These base images
are downloaded from Docker Hub [9], a distributed database
that is part of the Docker ecosystem. The problem with
using latest is that a change to the base image may require
changes to the Dockerï¬le. Unfortunately, there is no clear
way to incorporate those required changes automatically. For
instance, the python-pip package is part of Python 2, and
Python 2 is unavailable on Ubuntu images higher than 18.04.
Consequently, a build on a Dockerï¬le with the command
apt-get -y install python-pip will pass when the ï¬le is
based on Ubuntu images 18.04 and lower, but it will fail on
higher versions, including the latest LTS version of Ubuntu.
We used the S HIPWRIGHT toolset to analyze and cluster
hundreds of broken Dockerï¬les, looking for common er-
ror patterns in their build logs and associated Dockerï¬les.
Using a human-in-the-loop process, we then extracted pat-
terns and associated them with candidate repairs. For ex-
ample, when running the command docker build on the
Dockerï¬le FROM ubuntu:latest...RUN apt-get -y install
python-pip..., Docker reports the message â€œUnable to locate
package python-pipâ€ on output. When that message is present
in the logs, we found that the typical image in Dockerï¬les is
Ubuntu and the version is either undeï¬ned, latest, or 20.04.
We also noticed that this error message appears not only with
python-pip, but with other packages as well.
We expressed these patterns with regular expressions to be
checked against the Dockerï¬le (the static data) and error logs
(the dynamic data). For instance, we expressed the pattern for
the problem above with the regex â€œFROM ubuntu( | :latest |
:20.04)â€^â€œUnable to locate package (.*)â€2log. Note that
such an expression 1) deï¬nes a pattern over the Dockerï¬le,
2) deï¬nes a pattern over the output log, and 3) uses the groups
â€œ(...)â€ and â€œ(.*)â€ to bind data to variables for later use.
Figure 1 shows two possible solutions to the problem above.
The ï¬rst solution is to ï¬x the base image to the most recent
11491# s o l u t i o n 1 , use v e r s i o n 1 8 . 0 4
2FROM ubuntu : 1 8 . 0 4
3RUN apt g e t y i n s t a l l python  p i p
4 . . . # r e m a i n i n g code
5
6# s o l u t i o n 2 , manually i n s t a l l t h e package
7FROM ubuntu : l a t e s t
8ARG DEBIAN_FRONTEND= n o n i n t e r a c t i v e
9RUN apt g e t y i n s t a l l python2 c u r l s o f t w a r e  prop . . . \
10 && add  apt r e p o s i t o r y u n i v e r s e \
11 && c u r l h t t p s : / / . . . / get  p i p . py    o u t p u t get  p i p . py \
12 && python2 get  p i p . py
13 . . . # r e m a i n i n g code
Fig. 1: Solving python-pip unavailable on ubuntu:latest.
version with which the command can still be executed. The
following abstract operation characterizes the repair: replace
$0with :18.04. The symbol $0refers to the regex group
matching the Ubuntu image in the Dockerï¬le (i.e., â€œ(...)â€ )
that must be replaced with one speciï¬c Ubuntu version (e.g.,
:18.04). The second solution is to install Python 2 and its
toolset. The following operation characterizes the repair: add
ARG DEBIAN_FRONTEND=... after â€œFROM ubuntu( |:latest |
:20.04)â€. The symbol â€œ. . . â€ is a placeholder for the text
associated with the second solution from Figure 1.
SHIPWRIGHT records the association between a given pat-
tern (of build error) and possible solutions, such as the two
repairs above. From this information, S HIPWRIGHT is able to
repair broken Dockerï¬les whose build logs match some of the
pre-recorded patterns. Table I describes this example under
the row with â€œIdâ€ 5. Note that the repair operation consists
of multiple possible solutions, hence the comma and dots, as
we only show the ï¬rst solution. In this case, S HIPWRIGHT
produces two versions of the Dockerï¬le and the developer
should choose which one suits best their needs. We elaborate
on S HIPWRIGHT â€™s workï¬‚ow in the following sections.
It is worth noting that prior work has investigated the
impact of breaking changes in package-management systems
(e.g., in the Linux package manager, npm, maven, etc.) [10]â€“
[14]. S HIPWRIGHT is not restricted to this kind of issue, and
is distinct from prior work on the application context and
solution used. Section VII elaborates on related work.
B. Inconsistent Version Dependency Within Project
This kind of failure occurs when there is an inconsistency
between the versions of a Dockerï¬le and some of the ï¬les
it references within the project. Figure 2 shows a concrete
example that illustrates the problem. The Dockerï¬le requires
an image for Ruby version 2.6.3, whereas the application code
declares a dependency on a newer Ruby version (2.6.5) [15].
The execution of the command RUN bundle install triggers
an error, producing the following message on output â€œYour
Ruby version is 2.6.3, but your Gemï¬le speciï¬ed 2.6.5â€. In
this case, the solution was to replace line 1 of this Dockerï¬le
with FROM ruby:2.6.5. The pattern and corresponding repair
explained above are listed in Table I under row â€œIdâ€ 6. A
similar repair is â€œIdâ€ 1, which is also related to Ruby.1FROM ruby : 2 . 6 . 3
2RUN apt g e t u p d a t e  qq && apt  g e t i n s t a l l  y \
3 . . .
4RUN gem i n s t a l l b u n d l e r : 2 . 0 . 1
5RUN b u n d l e i n s t a l l # <   Gemfile depends on ruby 2 . 6 . 5 !
6ADD . / app
Fig. 2: Inconsistent Ruby version dependencies.
C. Missing Commands in the Dockerï¬le
This failure occurs when a given Dockerï¬le uses a command
that is unavailable on a given image. The solution in that case
is to install the command using the proper syntax, because it
depends on the version of the image. Table I lists one example
of this error pattern and the respective repair under row â€œIdâ€
8.
D. Project-Speciï¬c Failures and Suggestions
We observed that many broken Dockerï¬les require repairs
that are project-speciï¬c and cannot be generalized. In those
cases, S HIPWRIGHT is unable to produce a repair to the
broken ï¬le. Instead, in those cases, S HIPWRIGHT provides
suggestions. For instance, consider the case where a Dockerï¬le
includes a command to deploy a Node.js server, such as RUN
npm run build. The execution of that command fails because
there is an error in the Node.js project. There is nothing to ï¬x
within the Dockerï¬le. The developer needs to analyze what is
wrong in her Node.js project and ï¬x it. In that case, S HIP-
WRIGHT reports a suggestion, such as â€œNPM build error..â€.
As another example, consider a case in which a command
refers to a broken link, such as RUN wget <url>. S HIPWRIGHT
cannot guess how to ï¬x the broken link. Table II shows a
sample of suggestions provided by S HIPWRIGHT . In the future,
it would be interesting to examine how one might combine
SHIPWRIGHT with other tool-speciï¬c and language-speciï¬c
repair techniques. Tools that have a combined understanding
of DevOps artifacts and the programs these artifacts support
represent an intriguing area for future work.
III. D ATASET
We use an expanded version of the binnacle dataset [5]
as the source of Dockerï¬les to analyze. The binnacle
dataset consists of allDockerï¬les from GitHub repositories
with ten or more stars. These Dockerï¬les represent a broad
and largerly unï¬ltered picture of the state of Dockerï¬les
one might ï¬nd in popular GitHub repositories. Although
the original dataset was created in 2019, the binnacle
toolchain allows us to capture recent data using the same
methodology; thus, we populated our dataset with more recent
data (June 2020) extracted using the same tools. Unfortunately,
directly using the Dockerï¬les in this dataset is challenging
for two reasons: (i) many Dockerï¬les in the dataset come
from the same repository and, in such cases, the purpose of
the Dockerï¬les is obscured, making effortsâ€”like automated
buildsâ€”more difï¬cult; (ii) many Dockerï¬les are nested deep
within repositories (especially when repositories contain many
independent projects or services). In either case, automated
builds are challenging because the intent behind the Dockerï¬le
1150GitHub
DockerfilesDockerfile
Builds 
(in-context)Failing 
Builds
Build Logs
Dockerfile ASTs
MetadataStep 1: Data Collection
(of fline)Step 2: Fix Extraction
(of fline)BERT
HDBSCANBuild Logs
Full 
MetadataLog Embeddings
Fixes &
SuggestionsSearch-Based
Fix GenerationClustered
FailuresStep 3: Patch & 
Suggestion GenerationBroken
Dockerfile
shipwright
(online)Patches or
SuggestionsFig. 3: S HIPWRIGHT â€™s 3-step workï¬‚ow. In step (1), a database of Dockerï¬les and GitHub metadata is used to perform in-context builds. The results of
these builds are stored in a local database along with various forms of metadata. In step (2), S HIPWRIGHT uses BERT and HDBSCAN to cluster build
data [6]â€“[8]. The clusters are then fed to S HIPWRIGHT â€™s search-based repair-generation and suggestion-generation process. During this step, S HIPWRIGHT ,
with the assistance of a human, builds a database of repairs and suggestions. Finally, in step (3), online usage begins: new ï¬les are fed to S HIPWRIGHT and,
if matching database entries are found, S HIPWRIGHT provides relevant repairs or suggestions.
is difï¬cult to infer. In case (i), it is difï¬cult to infer which
of the (many) Dockerï¬les should be built. In case (ii), it is
difï¬cult to infer an appropriate context directory, which is a
pre-requisite to building a Docker image.
Dataset Filtering. To address these issues, we ï¬ltered the ï¬les
from the binnacle dataset using two criteria: (a) we only
considered repositories with a single Dockerï¬le, and (b) we re-
quired that the given Dockerï¬le resides within the repositoryâ€™s
root directory. For such a repository, it is not unreasonable to
assume two things: (i) the Dockerï¬le is intended to produce
an artifact corresponding to the given repository (because it is
theonly Dockerï¬le in that repository), and (ii) the Dockerï¬le
likely uses the repositoryâ€™s root directory as its build context :
the Dockerï¬le resides in the root directory, and the docker
build command assumes, by default, that the target Dock-
erï¬le resides within the given context directory. Performing
this ï¬ltering yielded 32,466 repositories and corresponding
Dockerï¬les that may be amenable to automated builds. It
is this subset of the original dataset (a refreshed version of
binnacleâ€™s dataset) that we used in our studies.
Building Dockerï¬les at Scale. SHIPWRIGHT performed in-
context builds on 20,526 of the 32,466 Dockerï¬les in our
ï¬ltered dataset (recall: we ï¬lter to ï¬nd Dockerï¬les from the
original dataset that are likely amenable to automated builds).
Although we would have liked to use all 32,466 Dockerï¬les,
we encountered some problems performing in-context builds
on that many ï¬les in a reasonable time frame, which forced us
to use a smaller set of 20,526 ï¬les. We tried various approaches
to scaling these in-context builds, but distributing this kind
of process would require Docker installations across a wide
variety of machinesâ€”in practice, this requirement was difï¬cult
to satisfy: none of the distributed-computing resources we had
access to would allow us to control a Docker daemon on the
distributed machines (while, for many distributed platforms,
running containers is easy). This requirement, while under-
standable from a security perspective, forced us to run builds
in a non-distributed way (on a single large server). Given this
constraint, some builds either time out (we set a limit of 30
minutes) or, due to contention from running multiple builds
on this single server and daemon instance, some builds fail to
complete due to errors internal to the Docker daemon; builds
that fail in this manner are marked as undetermined. Instead
of revisiting undetermined builds, we spent our resources on
building a larger portion of our ï¬ltered dataset. Through this
process, we captured the results from 20,526 Dockerï¬le builds,and saved these results to a database for further analysis.
Section IV-A describes this ofï¬‚ine data processing with the
SHIPWRIGHT toolset in further detail.
IV. S HIPWRIGHT
Figure 3 shows the workï¬‚ow of S HIPWRIGHT as a pipeline
of three steps, organized according to their respective goals.
The goal of the ï¬rst step is to analyze a corpus of broken
Dockerï¬lesâ€”mined from GitHubâ€”and to perform in-context
builds so that logs can be acquired (Section IV-A). The goal of
the second step is to cluster broken Dockerï¬les and ï¬nd repairs
(i.e., transformation functions on Dockerï¬les) (Section IV-B).
Given a cluster, S HIPWRIGHT automatically elaborates search
queries from log ï¬les of representative Dockerï¬les within the
cluster. A human then supervises the creation of repairs and
suggestions by (i) looking for error patterns as manifested
in existing QA forums resulting from the search query, and
(ii) creating plausible repairs (or, if no automated repairs are
possible, creating suggestions) which are saved in a database
for later, online, use. Finally, the third (interactive) stage of the
pipeline looks for actual repairs for a broken Dockerï¬le (Sec-
tion IV-C). This component takes as input the output produced
in the previous (ofï¬‚ine) stages and a broken Dockerï¬le, and
produces either (i) an automated repair, (ii) a suggestion (in
cases where repair cannot be automated), or (iii) an indication
that no existing repairs or suggestions apply. The following
sections describe each step in detail.
A. Data Collection
This component of S HIPWRIGHT builds and analyzes Dock-
erï¬les from a pre-existing corpus. We utilize the 32,466
Dockerï¬les described in Section III as our input corpus. Those
ï¬les were ï¬ltered to be amenable to automated builds. For
each ï¬le in this corpus, S HIPWRIGHT does the following:
1)Clones the originating repository for the given Dockerï¬le
into a unique /tmp/<repo-id> directory; 2) Runs docker
build -f <Dockerfile> /tmp/<repo-id>, which builds a
<Dockerfile> from our dataset using the root directory of
the cloned repository as the build context. Building in context
is crucial because the build may need to access ï¬les from the
originating repository to complete successfully. Although we
are interested in build failures, we want to avoid trivial failures;
3)Discards builds that still have trivial failures; and 4) Saves,
for each failing build, execution logs (standard output and
standard error streams), the AST for the given Dockerï¬le, and
various metadata (e.g., repository information, image history,
1151#1
#2
#3docker build
...'locale-gen fr_FR.UTF-8' 
returned a non-zero code: 127
'locale-gen en_US en_US.UTF-8'
returned a non-zero code: 127
'easy_install pip' 
returned a non-zero code: 127BERT< ... locale gen fr fr utf 8 ... >
< ... locale gen en us en us utf ... >
< ... easy install pip returned ... >2
31
123Fig. 4: Clustering Example. Starting with several broken Dockerï¬les, S HIPWRIGHT clusters by extracting standard error logs, applying aggressive token
splitting (we split on snake and camel case, as well as several operators that may occur in code snippets) and string normalization (we lowercase the input,
clear large blocks of repetitive characters, like extraneous white space, and we strip certain special characters/unicode), and passing the resulting sequences to
BERT. BERT takes the input sequences and produces vectors (shown as points in a high-dimensional space). S HIPWRIGHT uses that mapping, from failing
build logs to points in a vector space, along with a clustering algorithm (HDBSCAN), to produce its clusters, shown in the bottom right of the ï¬gure. The
ï¬nal clustering process has the advantage of being semantics aware: BERT understands some of the nuances of the English language, and our clustering
beneï¬ts from this capability.
and a log of the git clone procedure). An example would be
a broken build caused by an execution failure in a directive,
such as COPY orADD. Although builds are performed in context,
it is still possible that the Dockerï¬le is intended to be built as
part of a more complex workï¬‚ow with a context directory that
is different from the repository root directory. Unfortunately,
this information may exist in some third-party script, or may
be user-supplied.
Figure 3 illustrates this workï¬‚ow in the box named â€œStep 1:
Data Collection.â€ We note that data collection is quite costly:
we ran a 32-core CentOS workstation for several weeks and,
during that time, managed to build about two thirds (20,526) of
the 32,466 ï¬les in our dataset. (See Section III for a discussion
of the difï¬culties of building many Dockerï¬les at scale.)
Although builds can be parallelized, there is only one Docker
daemon per installation of Dockerâ€”this situation creates a
limit to the practical concurrency that can be achieved, along
with the network bandwidth to the workstation used for
analysis. We attempted to perform builds on a high-throughput
cluster but, unfortunately, the strict security requirements of
such clusters prevent deploying a workï¬‚ow involving Docker
images builds, which effectively execute untrusted code.
B. Repair & Suggestion Extraction
This step of S HIPWRIGHT works as follows. First, it uses
HDBSCAN, applied to embeddings,1to partition the Dock-
erï¬le data produced in the previous step. Second, it uses
those clusters to assist a human with the task of searching for
solutions and building a database of repairs. We now elaborate
on each of these steps.
Clustering: SHIPWRIGHT attempts to cluster failing Dock-
erï¬le builds using embeddings and HDBSCAN (a hierarchical
variant of DBSCAN, a classic clustering algorithm [16]). The
difï¬culty of clustering in this domain is two-fold: (i) the data
to cluster is heterogeneous, and is often a mix of code and
natural language (i.e., the build logs, which will often contain
a description of the failure in English and a reproduction of
the Bash or Dockerï¬le snippet that leads to the error), and
1Embeddings refer to high-dimensional vectors of numbers that are used
as a proxy for non-numeric artifacts (such as text or code). Embeddings are
often of use, because many operations can be performed in the resulting vector
space, and later mapped back to the originating artifacts.(ii) although we would like to cluster on the cause of build
failures, we do not have a way to deï¬nitively extract the cause
of a given build failure; therefore, we must use data that is, at
best, a proxy or symptomatic of the root cause of failure. In
particular, we use a tokenized version of the build logs for the
failing build (which may include a variety of things: debug
output, warnings, and errorsâ€”some of which may include
code snippets) as input to BERT to produce embeddings.
Despite these challenges, S HIPWRIGHT is able to perform
clustering by leveraging a key insight: recent off-the-shelf
language models, such as BERT, GPT-2 and, recently, GPT-3,
have reached impressive levels of sophistication [7], [17], [18].
Given the inputs these models are trained on (roughly, massive
crawls of the internet), it is highly likely that such models
have seen websites like StackOverï¬‚ow, which mix both natural
language and code. Therefore, to address challenge (i) (the
heterogeneous mix of code and natural language), S HIP-
WRIGHT leverages a sufï¬ciently sophisticated off-the-shelf
language model (BERT),2to obtain embeddings. In particular,
SHIPWRIGHT uses a variant of BERT suited to the task of
sentence embedding, in which similar sentences should end
up â€œcloseâ€ in the embedding space. S HIPWRIGHT applies this
BERT variant to the last few lines of the error logs to produce a
vector representation of each broken Dockerï¬le. These vectors
are then fed to HDBSCAN, which produces clusters. Figure 4
illustrates the S HIPWRIGHT clustering process.
Searching for Repairs or Suggestions: This component of
SHIPWRIGHT takes a set of clusters as input, and produces
alist of pairs as output. The ï¬rst element of the pair is a
signature that identiï¬es the issue (in the Dockerï¬le and its
logs) whereas the second element of the pair is either (i) a
repair, consisting of a pure transformation function that takes
a Dockerï¬le as input and produces another ï¬le as output, or
(ii) a suggestion (about what needs to be repaired and how)
for the cases where human knowledge is necessary to prepare
the repair. We elaborate on each of these two cases in the
following and Tables I and II provide examples of such pairs.
Case 1 (Searching for Repairs): SHIPWRIGHT uses a
2Ideally, one would try GPT-3 because it is the newest and largest such
language model; unfortunately, for now, access to GPT-3 is quite limited, and
we were unable to obtain access.
1152search-based recommendation system to assist a human in
locating repairs of broken Dockerï¬les. S HIPWRIGHT proceeds
as follows: it selects a cluster and a representative Dockerï¬le
from that cluster; it extracts keywords from the logs of that
ï¬le; it builds a search string from those keywords; it submits
the corresponding query to a search engine; it ï¬lters the
outputs from related community forums; and it reports a
list of the top-5 URLs as output for a human to inspect.
Human inspection consists of reading proposed solutions on
discussion forums, and then applying a given solution to the
representative Dockerï¬le from the cluster. If that solution
is plausible, i.e., if it allows the Dockerï¬le to build an
image successfully, the next step is to check if the error-
pattern/repair-function pair is applicable to other Dockerï¬les
in the cluster. While doing so, the human inspector looks for
opportunities to generalize the pattern and repair function to
avoid overï¬tting a solution to a particular case. For instance,
in the example given in Section II-A, the initial solution
was too narrow, focusing on ï¬xes of ï¬les containing the
exact message â€œUnable to locate package python-pipâ€ in
the output log. However, we observed similar error messages,
referring to different packages. In this case, the solution was
to replace â€œpython-pipâ€ in that string with a symbolic name
for a package. To sum up, S HIPWRIGHT leverages community
knowledge bases (e.g., StackOverï¬‚ow and Dockerâ€™s commu-
nity forums) to ï¬nd solutions to known issues, such as those
presented in Section II.
SHIPWRIGHT supports a total of 13 repair patterns. Table I
shows the pairs of (1) build-error signaturesâ€”referred to as
a patternâ€”and (2) a corresponding repair for 10 of them.
Column â€œIdâ€ shows the id of a pair. Column â€œPatternâ€ shows
the error pattern, which is a regular expression that matches
a string in the error logs (the dynamic part) and/or a string
in the Dockerï¬le (the static part). Column â€œRepairâ€ shows
a function, in natural language, describing how to transform
and ï¬x a broken Dockerï¬le. We use the keywords add,
remove , and replace to describe operations that need to be
performed on the Dockerï¬le. We informally described the
semantics of these operations with examples in Section II-A.
Although there is no fundamental reason preventing us from
creating these transformations automatically, we wrote the
code implementing these transformation functions because we
found empirically that creating these functions was not a time-
consuming error-prone task. Finally, column â€œSrc.â€ shows a
reference for the solution on the web.
Case 2 (Searching for Suggestions): There are cases
where S HIPWRIGHT cannot produce a repair. For example,
a Dockerï¬le whose build fails because of a compilation error
or broken URL requires a human to ï¬x the underlying error.
For those cases, we report a suggestion, i.e., generic advice on
what needs to be done. Table II shows a small sample from
the total of 50 suggestion patterns that S HIPWRIGHT supports
(in addition to 13 repair patterns). Column â€œIdâ€ shows the id
of the suggestions, column â€œPatternâ€ shows the signature, and
column â€œSuggestionsâ€ shows the suggestion message.C. Repair and Suggestion Generation
SHIPWRIGHT can be used to repair Dockerï¬les or provide
suggestions using the database generated in step 2 (Sec-
tion IV-B). Given a Dockerï¬le, S HIPWRIGHT iteratively ex-
amines the repairs and suggestions and, given a match, it
either (i) produces a patched ï¬le, by applying a repair, or (ii)
provides a suggestion message to the user. If neither a repair
nor a suggestion with a matching pre-condition exists within
the database, S HIPWRIGHT is still able to use its search-based
process to guide a human in producing ï¬xes and suggestions,
as we did in step 2 (Section IV-B). This search-based process
provides a user with a small set of (ï¬ltered) links to resources
likely to help in ï¬xing the given input ï¬le. In summary,
SHIPWRIGHT , during step 3, produces either: (i) a Dockerï¬le
repair, (ii) a suggestion on how to ï¬x the broken build, or (iii)
a curated set of results from a search-based process that may
provide solutions to the underlying build issue.
V. E VALUATION
The goal of S HIPWRIGHT is to help developers ï¬x broken
Dockerï¬les. It does that through a combination of (i) clustering
of broken Dockerï¬les (by likely root cause), and (ii) a search-
based method to ï¬nd repairs (and, if no automated repairs are
feasible, suggestions). To gain insights into the landscape of
broken Dockerï¬les used in GitHub projects and to understand
SHIPWRIGHT â€™s efï¬cacy, we pose the following research ques-
tions.
RQ1. How prevalent are Dockerï¬le build failures in projects
that use Docker on GitHub? Can existing (static) tools identify
the failure-inducing issues within these broken ï¬les?
Rationale: The purpose of this question is to evaluate the
potential impact of S HIPWRIGHT . If build failures are rare,
then impact is limited. Furthermore, reproducibility is a core
tenet of Dockerâ€”it would be surprising to ï¬nd many broken
Dockerï¬les. We also assess the ability of existing (static) tools
to identify issues that may lead to failing Dockerï¬le builds.
Metrics: We used the following metrics to answer RQ1: 1) the
fraction of Dockerï¬les in our dataset with builds that fail;
2) the relationship between failures and project popularity;
and 3) the success rate of existing (static) tools in predicting
Dockerï¬le build failures. The ï¬rst metric evaluates the fraction
of Dockerï¬les that we mined from GitHub that fail to build
because the Dockerï¬le is broken (for non-trivial and non-
toolchain-related reasons). The second metric examines the
relationship between the number of GitHub stars a given
repository has (a common proxy for popularity on GitHub)
and whether that repository contains a broken Dockerï¬le. This
measurement helps us ascertain whether popular repositories
suffer from broken Dockerï¬les at the same rate as less popular
repositories. Recall that we do not have any Dockerï¬les from
repositories with less than 10 GitHub stars (Section III).
Finally, the third metric ascertains the ability of pre-existing
tools, namely, Hadolint [2] and binnacleâ€™s rule checker
[29], to ï¬nd issues within broken Dockerï¬les.
1153TABLE I: Selected Repairs.
Id Pattern Repair Src.
1â€œERROR: Error installing bundler 2log^
â€œbundler requires Ruby version ([0-9]+.[0-9]+.[0-9]+)â€ 2logreplace FROM ruby:(.*) with FROM ruby:$0 [19]
2 â€œRpmdb checksum is invalid: dCDPTâ€ 2log addRUN yum install -y yum-plugin-ovl afterFROM(.*) [20]
3â€œE: Some index ï¬les failed to download.
They have been ignored, or old ones used instead â€2logreplace base image with latest release from hub.docker.com [21]
4 â€œE: Package â€˜libpng12-devâ€™ has no installation candidateâ€ 2log replace libpng-12dev withlibpng-dev [22]
5â€œFROM ubuntu(|:latest|:20.04)â€ ^
â€œUnable to locate package (.*)â€2 log2Dockerï¬lereplace $0with:18.04, ... [23]
6â€œbut your Gemï¬le speciï¬ed ([0-9\|\\.]+)â€2log
^â€œFROM ruby(.*)â€ 2Dockerï¬lereplace FROM ruby:(.*) with FROM ruby:$0 [24]
7â€œinvalid byte sequence in US-ASCII â€2log
^â€œFROM ruby(.*)â€ 2Dockerï¬leaddENV LANG C.UTF-8 afterFROM(.*) [25]
8 â€œsh: (.*): not found â€2logifâ€œFROM alpine(.*)â€ 2Dockerï¬le
then addRUN apk add -no-cache $0.
else addRUN apt-get -y update
&& apt-get -y install $0[26]
9â€œERROR: unsatisï¬able constraints: bzr (missing):â€ 2log
^â€œFROM alpine(.*)â€ 2Dockerï¬leremove bzr inâ€œapk addâ€ command [27]
10â€œconda: not found â€2log
^â€œRUN curl (https://repo.continuum.(.*))â€ 2Dockerï¬leadd-Lbefore $0 [28]
TABLE II: Selected Suggestions.
Id Pattern Suggestion
1 â€œmix â€2log^â€œCode .LoadError â€2log Problem running mix on the Elixir project...
2 â€œtscâ€2log^â€œ:error TS â€2log Error during TypeScript compilation with tsc. Please check your .ts ï¬les.
3 â€œwget: server returned error â€2log_â€œwget: unable to resolve host addressâ€ 2log wget error , you have a broken URL. Please check the log.
4 â€œnpm ERR!â€2log NPM Error, check your ï¬les, in particular, â€œnpm installâ€ commands.
5 â€œcurl:([0-9]+).*â€2log curl error , you have a broken URL. Please check the log.
RQ2. Can we use off-the-shelf language models, like BERT,
to easily cluster broken Dockerï¬les?
Rationale: Given the number of observed failures, it is reason-
able to ask whether many failures are unique. If many failures
are similar, one might hope that generalized repairs exist.
Furthermore, if failing Dockerï¬le builds can be clustered,
those clusters may be used to bootstrap ï¬nding repairs. Finally,
if we can leverage the level of understanding available in
large off-the-shelf language models (like BERT), then we
can design robust clustering routines with little specialized
engineering effort, and avoid techniques based on manually
designed heuristics.
Metrics: To answer RQ2, we examine the percentage of
clusters (generated using S HIPWRIGHT ), where all elements
share a single (likely) root cause. This metric provides insight
into S HIPWRIGHT â€™s ability to cluster broken Dockerï¬les and
the usefulness of those clustersâ€”good clustering allows for
ï¬nding multiple exemplars for a single failure which, in turn,
makes the task of generating automated repairs simpler.
RQ3. How effective is SHIPWRIGHT in producing repairs?
(i) To what extent do repairs cover the failures from our
dataset? (ii) For failures that can be clustered, is it possible
to generalize repairs? (iii) What can be done for failures from
non-clustered ï¬les?
Rationale: The purpose of this question is to evaluate S HIP-
WRIGHT â€™s effectiveness on our dataset. If proposed solutionsare unable to cover a variety of Dockerï¬les, then S HIP-
WRIGHT â€™s usefulness is questionable.
Metrics: 1) We measured the fraction of broken Dockerï¬les
(from our dataset) for which S HIPWRIGHT produces a repair.
2) For the set of clusters that S HIPWRIGHT produces, we
measured the extent to which repairs generalize. For that,
we measure â€œcoverageâ€ (i) in the cluster that originated that
pattern, and (ii) across different clusters. Coverage refers to
the portion of elements within a cluster that match the same
pre-condition for a repair. 3) For broken Dockerï¬les that did
not cluster, we measured how often S HIPWRIGHT provides a
repair. In all cases, we also evaluated S HIPWRIGHT â€™s ability
to provide suggestions if repairs were not feasible. Collec-
tively, these metrics measure how effective S HIPWRIGHT is in
proposing solutions to the broken ï¬les in our dataset.
RQ4. How effective is SHIPWRIGHT in reducing the number
of broken Dockerï¬les in public repositories?
Rationale: Although RQ3 seeks to evaluate S HIPWRIGHT â€™s
ability to ï¬x broken Dockerï¬les, there still remains a question
of S HIPWRIGHT â€™s usefulness in practice. RQ4 seeks an un-
derstanding of S HIPWRIGHT â€™s effectiveness to meet our over-
arching goal: ï¬xing broken Dockerï¬les in public repositories.
Metrics: To answer RQ4, we used two metrics: 1) What
proportion of Dockerï¬les that appear in our dataset as broken
Dockerï¬les, but have since been ï¬xed, would also have been
ï¬xed, had we applied SHIPWRIGHT ? 2) How often can we
1154use S HIPWRIGHT to produce pull-requests that are accepted
by external reviewers? The ï¬rst metric refers to a kind of
â€œtime-travelâ€ analysis because, using updates that took place
during the period in which we built S HIPWRIGHT , we can
attempt to measure how successful we would have been
had S HIPWRIGHT existed at an earlier date, and had we
applied it. Nevertheless, this metric is still a â€œsimulatedâ€ one.
Therefore, the second metric quantiï¬es S HIPWRIGHT â€™s â€œreal-
worldâ€ applicability by actually using it to produce repairs and
submitting them for (external) review.
A. Answering RQ1: How prevalent are Dockerï¬le build fail-
ures in projects that use Docker on GitHub? Can existing
(static) tools identify the failure-inducing issues within these
broken ï¬les?
26%55%
2%
17%
Failures
Successes
Timeouts
Undetermined
Fig. 6: Breakdown of the
20,526 ï¬les we attempted
to build.To answer RQ1, we used S HIP-
WRIGHT to build a random sam-
ple of Dockerï¬les from our (ï¬ltered)
dataset. In total, we tried to build
20,526 Dockerï¬les and found 5,405
broken Dockerï¬les. This gives us an
estimated 26.3% â€œbreakage rateâ€ for
Dockerï¬les in our overall dataset.
The large amount of broken Dock-
erï¬les on GitHub runs counter to
one of the core reasons for using
Docker: reproducibility. Aside from
broken Dockerï¬les, we encountered
393 Dockerï¬les with builds that time out (we use a threshold
of 30 minutes) and 3,514 Dockerï¬les with undetermined
results (which arise due to the pressure that multiple concur-
rent builds place on the Docker daemon). Neither timeouts
nor builds with undetermined results are counted as broken
Dockerï¬les. Instead, we count these results as successful
builds to give a conservative estimate (and lower bound) of
the â€œbreakage rateâ€ for Dockerï¬les in our dataset. Figure 6
provides a visual overview of these categories.
To put these results in context, we also examined the
distribution of stars for the 5,405 repositories in our dataset.
For these repositories, we ï¬nd that: (i) a third have 18 stars or
fewer, (ii) a third have greater than 18 stars, but fewer than 51
stars, and (iii) a third have 51 or more stars. This distribution
was surprising, especially because some repositories with
broken Dockerï¬les had many thousands of stars. We spot-
checked some of these cases and found that, indeed, even
quite popular repositories can have broken Dockerï¬les. For
example, the MEAN stack project [30] has over 12K stars,
yet it contains a Dockerï¬le that fails to build.
Finally, we also tested the capabilities of two existing
(static) tools: binnacle [29] and Hadolint [2]. For both
tools, we sought an estimate of the number of broken Dock-
erï¬les for which each tool identiï¬es a possible build-breaking
issue. Because we found it impractical to manually examine
the toolsâ€™ outputs on each of the 5,405 broken ï¬les, we instead
used a (generous) estimate based on how often each tool
reports a rule violation for an issue that might cause a buildto break. For example, Hadolint can identify when the version
of an image used for a base in a Dockerï¬le is un-pinned;
thus, if Hadolint reports a rule violation in this category, on
any ï¬le, we count it as Hadolint identifying a possible build-
breaking issue (and mark the ï¬le as â€œsolvedâ€ by Hadolint). In
total, Hadolint identiï¬es such issues in only 33.8% of ï¬les,
andbinnacle identiï¬es such issues in only 20.6% of ï¬les. Summary of RQ1: The presence of broken Dockerï¬les
on GitHub is common. Furthermore, even highly starred
repositories sometimes contain broken Dockerï¬les. Fi-
nally, existing static tools only identify plausible build-
breaking issues in 20.6â€“33.8% of cases (and, even when
issues are identiï¬ed, such tools do not provide repairs).
B. Answering RQ2: Can we use off-the-shelf language models,
like BERT, to easily cluster broken Dockerï¬les?
To generate the clusters we use throughout our evaluation,
we ï¬rst performed a grid search against S HIPWRIGHT â€™s clus-
tering algorithm. During this search, we focused on exploring
the space of hyperparameters used in HDBSCANâ€”the embed-
dings, although generated by a neural model, are not â€œtunableâ€
without investing in re-training the model, which is outside
the scope of S HIPWRIGHT . We searched approximately 200
conï¬gurations and found, on average, HDBSCAN was able to
cluster 34% (1,836) of the 5,405 broken Dockerï¬les identiï¬ed
by S HIPWRIGHT . In the clustering that we used, consisting of
144 clusters containing 1,814 ï¬les, we were able to conï¬rm
that 36.5% of the clusters consisted of Dockerï¬les that all had
the same root cause for their failures.
Summary of RQ2: SHIPWRIGHT â€™s approach to clus-
tering Dockerï¬les can, on average, cluster 34% of our
dataset, and, for over a third of the clusters generated,
we can conï¬rm that a single issue covers all failing
Dockerï¬les within a cluster.
The answer to RQ2 bodes well for using clusters to boot-
strap ï¬nding automated repairs. However, we note that the
clustered ï¬les only make up a portion of broken ï¬les: there-
fore, to assess generalizability, RQ3 examines S HIPWRIGHT â€™s
ability to use repairs learned from our clustered ï¬les and apply
them to non-clustered ï¬les.
C. Answering RQ3: How effective is SHIPWRIGHT in produc-
ing repairs? (i) To what extent do repairs cover the failures
from our dataset? (ii) For failures that can be clustered, is
it possible to generalize repairs? (iii) What can be done for
failures from non-clustered ï¬les?
This question evaluates S HIPWRIGHT â€™s effectiveness on our
dataset of broken Dockerï¬les (Section III).
RQ3.1 evaluates how much of the set of broken Dockerï¬les
can be addressed with the repairs that S HIPWRIGHT generates.
Figure 5 shows the effects of the repairs (and suggestions) that
we found across the 144 clusters produced by S HIPWRIGHT .
Each vertical bar denotes one cluster. These bars are divided
into three segments. The size of the segment at the bottom of
1155020406080100
repairs suggestions unknown
Fig. 5: Proportion of different kinds of solutions within each cluster (excluding singleton clusters).
the bar (in yellow) represents the percentage of failures in a
given cluster for which S HIPWRIGHT provided an automated
repair; the size of the segment in the middle of the bar (in blue)
represents the percentage of failures for which S HIPWRIGHT
provided suggestions (which are generated in cases where no
repairs apply); and the size of the segment at the top of the
bar (in gray) represents the percentage of failures for which
SHIPWRIGHT could notï¬nd a solution.
Summary of RQ3.1: The 13 repairs created with S HIP-
WRIGHT offered solutions to 20.34% of the 1,814 broken
and clustered Dockerï¬les. In cases where no repairs
were applicable, S HIPWRIGHT â€™s 50 suggestions applied
to an additional 69.63% of the broken and clustered
Dockerï¬les.
RQ3.2 evaluates the ability of the repairs to generalize to
a large number of cases. Table III shows the relative amount
of broken Dockerï¬les that each one of our 13 repair patterns
covered.
TABLE III: Repair Coverage.
Id # ClustersCoverage (%)
Parent Average
1 4 100 61
2 3 40 25.67
3 8 100 54
4 3 60 49.34
5 2 88 88
6 2 100 90.50
7 1 82.14 82.14
8 6 100 88.67
9 1 100 100
10 2 100 95.5
11 3 62 50
12 2 80 42
13 3 80 60
Column â€œIdâ€ refers to the id of the repair (most of which
listed in Table I), and column â€œ#Clustersâ€ shows the num-
ber of clusters where the corresponding repair could ï¬x at
least one of the broken Dockerï¬les in it. Error patterns are
extracted from a given cluster, which we refer to as â€œparentâ€.
Column â€œ(Coverage) Parentâ€ then shows the fraction of broken
Dockerï¬les within the parent cluster that were corrected using
the respective repair. Column â€œ(Coverage) Avg.â€ shows the
average fraction of repaired ï¬les across the different clusters
affected by a repair pattern.Summary of RQ3.2: The 13 repair patterns produced
with S HIPWRIGHT generalized well within the parent
cluster (avg. 84.01%) and across affected clusters (avg.
68.22%).
Recall that a total of 3,586 of the 5,400 broken Dock-
erï¬les (66.4%) were notclustered. For non-clustered ï¬les,
SHIPWRIGHT produced repairs to 18.18% of them. Overall,
SHIPWRIGHT produced an actionable solution to the developer
in 64.81% of the ï¬les that were not associated with any
cluster (18.18% from repairs and an additional 46.63% from
suggestions). Note that S HIPWRIGHT used the patterns pro-
duced by analyzing clustered ï¬les. That was possible because
the clustering step is conservative and clusters were based
on embeddings of largely syntactic information (logs). For
example, we observed that a ï¬le failing on the statement
apk add A && apk add B && ... && apk add bzr was not
clustered with other ï¬les failing on apk add bzrâ€”but, upon
further examination, we found that this ï¬le failed to cluster due
to its use of a conjunction of successive apk add commands
instead of the (more common) use of the multi-argument apk
add A B ... bzr variant. In practice, although conservative,
the generated clusters were suitable for creating useful and
generalizable repairs.
Finally, even when no repairs or suggestions apply, S HIP-
WRIGHT can still provide a list of URLs pointing to resources
that may provide a developer with a ï¬x for their broken ï¬le.
Summary of RQ3.3: Even in non-clustered broken
Dockerï¬les, S HIPWRIGHT was able to produce auto-
mated repairs in 18.18% of the ï¬les. Furthermore, when
no repairs applied, S HIPWRIGHT was able to provide
suggestions in 46.63% of the ï¬les.
D. Answering RQ4: How effective is SHIPWRIGHT in reduc-
ing the number of broken Dockerï¬les in public repositories?
This section reports on two experiments we conducted to as-
sess the practical usefulness of S HIPWRIGHT . The ï¬rst exper-
iment (Section V-D1) measures the fraction of initially-broken
but later-ï¬xed Dockerï¬les that could have been repaired
with S HIPWRIGHT . The second experiment (Section V-D2)
measures the acceptance ratio of Pull Requests (PRs) for
Dockerï¬les found to be still broken in their repositories.
11561) Repair Conï¬rmation: This experiment evaluates S HIP-
WRIGHT on real patches created by GitHub developers. The
metric we used was the fraction of the patches created by
developers that matched the repairs or suggestions of S HIP-
WRIGHT . To run this experiment, we searched for ï¬xed Dock-
erï¬les on GitHub. We used the same procedure as reported in
Section IV-A, but we re-cloned the repositories on Aug. 14,
2020 (8/14/20). Because we know that the Dockerï¬le build
on the ï¬rst version of the project failed, we only needed to
perform Dockerï¬le builds for the 8/14/20 versions of projects.
To avoid unnecessary builds, we looked for Dockerï¬les that
were changed in the repository, and found that 161 (=8.87%)
of the original 1,814 broken Dockerï¬les were changed in
their repositories from the day they were retrieved up to
8/14/20. We ran the command docker build in-context on
those 161 ï¬les, and discarded the cases where the build was
still unsuccessful. In the end, we obtained a set of 102 hx;yi
pairs to analyze, with xdenoting a broken Dockerï¬le from our
dataset and ydenoting its corresponding patch. The method
we used to measure effectiveness of S HIPWRIGHT was to
run S HIPWRIGHT onxand compare the generated repair or
suggestion, if found, with y.
Of the 102 cases of initially-broken then-ï¬xed Dockerï¬les,
SHIPWRIGHT produced an identical repair in 23 of the cases.
In 77 cases, S HIPWRIGHT provided suggestions that matched
the patch used by the developer. Although we found that
the ratio of suggestions to ï¬xes was higher compared to
results of RQ3.1, S HIPWRIGHT covered most of the cases
we analyzed (a total of 98.04% of the cases). Overall, we
believe that this result is encouraging because it provides a
strong (and relatively unbiased) indication that the repairs that
SHIPWRIGHT produces are (i) correct (they matched the ï¬xes
of developers) and (ii) useful (almost all cases were covered).
2) Pull Requests (PRs): This experiment evaluates S HIP-
WRIGHT on Pull Requests (PRs) issued to GitHub projects
with still-broken Dockerï¬les. The goal is to assess the feed-
back from developers to these PRs, which is a proxy for
their interest in S HIPWRIGHT â€™s results. For each of the 13
repair patterns, we randomly sampled 5 Dockerï¬les (from
our dataset) that remained broken until the date we ran this
experiment. Then, we manually prepared a PR that explained
the problem (including a link to a similar case) and proposed a
repair, as created by S HIPWRIGHT . To avoid violating double-
blind rules, we created and used a GitHub account under the
ï¬ctitious name â€œJoseph Pettâ€ to submit the PRs. Our artifact
(https://github.com/STAR-RG/shipwright) includes an up-to-
date tracker of the submitted, accepted, and rejected PRs.
Of the 45 PRs that we submitted, 19 were accepted by
developers (=42.2%); 4 PRs were rejected; and 22 PRs have
not yet been reviewed by developers. The number of submitted
PRs was lower than 65 (=13*5) because we could not ï¬nd ï¬ve
Dockerï¬les still broken for some of the patterns.
Three of the four rejected PRs were related to the same or-
ganization and the same problem, characterized by pattern #7
(Table I). The developer pointed out that using a new version
of the Docker Ruby image solved the encoding problem, andhe preferred to update the Ruby version. With that feedback,
we revised repair #7 to include a second solution, which is to
update the Ruby version to 2:5:8. We have conï¬rmed that this
repair also works for the Dockerï¬les repaired by the original
solution. The new version of the Ruby image was committed
on June 2020 [31], while this issue has been reported since
June 2015 [25]. This GitHub issue was the URL recommended
by S HIPWRIGHT to assist the human to produce a repair.Summary of RQ4: These results provide initial, yet
strong, evidence that S HIPWRIGHT is a useful aid to help
developers ï¬x broken Dockerï¬les.
VI. T HREATS TO VALIDITY
Although most of our analysis is based on samples of
(broken) Dockerï¬les from GitHub repositories with ten or
more stars, it is possible that this data is not representative
of Dockerï¬le use in general. Nonetheless, we found that real
developers accepted the patches that S HIPWRIGHT generated,
and thus we can be reasonably sure that the trends (and repairs)
we have identiï¬ed are applicable in practice. Additionally, our
repairs and suggestions require a human in the loop, which
is a source of bias. To side-step this source of bias, we ran
a â€œtime-travelâ€ analysis in which we were able to conï¬rm
retroactively that S HIPWRIGHT â€™s repairs and suggestions were
either identical (for repairs) or similar (for suggestions) to
patches that developers actually applied. This study is an
important counterpoint to our pull-request study, because even
pull-request acceptances could be biased, in the sense that it
is hard for a developer to â€œrejectâ€ an offered patch.
We also made efforts to bolster our results by using robust
methodology where possible: e.g., to understand clustering
behavior, we ran a grid search over 200 different conï¬gurations
of hyper-parameters; we also benchmarked two recent static
tools to give some context to S HIPWRIGHT results.
VII. R ELATED WORK
Empirical studies on Docker (and DevOps). A growing
number of studies have been carried out on Dockerï¬les, as
well as on the broader topic of DevOps [32] (also known as
infrastructure as code). For Docker, Cito et al. [33] examined
Dockerï¬le quality and, similar to us, found a high rate of
breakage in Dockerï¬le builds; they cite a 34% breakage rate
from a smaller sample of 560 projects. We found a comparable
breakage rate, but have also developed methods aimed at mak-
ingrepairs instead of just analyzing quality. More recently,
Wu et al. [34] conducted a comprehensive study of build
failures in Dockerï¬les. They analyzed a total of 3,828 GitHub
project containing Dockerï¬les, and a total of 857,086 Docker
builds. Overall, they found a failure rate of 17.8%. Despite
the differences in failures rates, these studies corroborate our
ï¬nding that build failures are prevalent. Lin et al. [35] analyzed
patterns (i.e., good and bad practices) in Dockerï¬les. Among
various observations, they found that many Dockerï¬les use
obsolete OS images, which can pose security risks (because
1157attackers could exploit documented vulnerabilities) and incor-
rectly use the latest tag. Xu and Marinov [36] investigated
characteristics of Docker images from DockerHub. Among
other ï¬ndings, they listed opportunities to improve Software
Engineering tasks based on how images are organized. For
example, they report that image variants could be used to
support combinatorial testing. Zerouali et al. [37] studied
version-related vulnerabilities (yet another category of issues
that may arise in Dockerï¬lesâ€”similar to some of the build-
breaking issues we observed, in which external changes in the
environment negatively effect a Dockerï¬le). Among various
ï¬ndings, they found that no release is devoid of vulnerabilities,
so deployers of Docker containers cannot avoid vulnerabilities
even if they deploy the most recent packages.
Analysis of Dockerï¬les Henkel et al. [29] created a static
checker for Dockerï¬les (similar to Hadolint [2]), called
binnacle, which is capable of learning rules from ex-
isting Dockerï¬les; however, unlike S HIPWRIGHT , neither
binnacle nor Hadolint attempts repairs. Xu et al. [38]
examined â€œTemporary File Smellsâ€, which are an image-
quality-related issue, not a build-breaking issue, such as the
ones we examined. Zhang et al. [39] studied the effect of
Dockerï¬le changes on build time and quality (and utilized
the static tool Hadolint). Hassan et al. [40] proposed RUD-
SEA a tool-supported technique that proposes updates in
Dockerï¬les. RUDSEA analyzes changes in software environ-
ment assumptionsâ€”obtained with static analysisâ€”and their
impacts. We consider RUDSEA and S HIPWRIGHT to be com-
plimentary approaches: RUDSEA focuses on changes within
a project and S HIPWRIGHT focuses on changes external to a
project. Other empirical studies on DevOps, but not Docker,
include an examination of smells in software-conï¬guration
ï¬les [41], and a study of the coupling between infrastructure-
as-code ï¬les and â€œtraditionalâ€ source-code ï¬les [42].
Automated Code Repairs. SHIPWRIGHT lies within the
growing body of work in automated repair. According to a
recent survey [43], our approach can be classiï¬ed as both
Generate-and-Validate and Fix Recommender. We use pre-
deï¬ned templates that are obtained (i) via the analysis of build
logs extracted from our clusters, and (ii) from examples found
in community websites. As such, we side-step the challenge of
a fully automatic repair process to produce acceptable ï¬xes. In
addition to automated repair of source code, there is a growing
effort to automate repair of build-related (DevOps) artifacts.
These DevOps artifacts are unique in that they are often tied to
both a source repository and the broader external environment
in which one wants to build, test, and/or run their code. In the
broader context of repair for build-related artifacts, both Lou
et al. [44] and Hassan and Wang [4] investigate repair in the
domain of Gradle builds (a kind of DevOps artifact used in
many Java projects) and Macho et al. [45] explore the related
problem of automated repair for Maven builds.
Broken Updates in Package Managers. Prior work in-
vestigated the impact of breaking changes in package man-
agers. Mancinelli et al. [14] formalized package dependencies
within a repository, and encoded the installability problemas a SAT problem. V ouillon and Cosmo [10] proposed an
algorithm to identify broken sets of packages that cannot be
upgraded together within a component repository. McCamant
and Ernst [11] proposed an approach for checking incom-
patibility of upgraded software components. They compute
operational abstractions based on input/output behaviour to test
whether a new component can replace an old one. MÃ¸ller and
Torp [12] proposed a model-based testing approach to identify
type-regression problems that result in breaking changes in
JavaScript libraries. These works deal with improvements and
repairs applied to a package repository or library, and thus
have a different focus compared to S HIPWRIGHT , which is
on repairing broken Dockerï¬les. More related to checking
inconsistenciesof client code, Tucker et al. [13] proposed the
OPIUM package-management tool. Given a set of installed
packages and information about dependencies and conï¬‚icts,
they used a variety of solvers to determine (i) if a new package
can be installed; (ii) the optimal way to install it; and (iii)
the minimal number of packages (possibly none) that must
be removed from the system. S HIPWRIGHT does not rely on
explicit information about dependencies (which might not be
available or feasible to obtain). Instead, it extracts information
from build logs, and leverages community knowledge bases to
ï¬nd solutions. This approach enables S HIPWRIGHT to address
problems that go beyond broken packages and conï¬‚icts.
VIII. C ONCLUSIONS
In an analysis of many open source repositories that use
Docker, we found a surprising number of broken Dockerï¬les,
most of which existing static analyzers cannot detect. For
the cases they can detect, they do not propose repairs. To
address this problem, we propose S HIPWRIGHT , a human-in-
the-loop approach for clustering, analyzing, and ï¬xing broken
Dockerï¬les. We conducted a comprehensive evaluation of
SHIPWRIGHT , which showed that it was a helpful aid to ï¬x
broken Dockerï¬les on GitHub. Using S HIPWRIGHT we were
able to submit 45 pull requests, of which 19 were accepted.
Furthermore, the tools and data we produced as result of this
study are publicly available online via our artifact:
https://github.com/STAR-RG/shipwright
.
ACKNOWLEDGMENTS
Supported, in part, by a gift from Rajiv and Ritu Ba-
tra; by Facebook under a Probability and Programming
Research Award; and by ONR under grants N00014-17-
1-2889 and N00014-19-1-2318; by INES 2.0, FACEPE
grants PRONEX APQ 0388-1.03/14 and APQ-0399-1.03/17,
CAPES grant 88887.136410/2017-00, FACEPE grant APQ-
0570-1.03/14 and CNPq (grants 465614/2014-0, 309032/2019-
9, 406308/2016-0, 409335/2016-9). Any opinions, ï¬ndings,
and conclusions or recommendations expressed in this pub-
lication are those of the authors, and do not necessarily reï¬‚ect
the views of the sponsoring entities.
1158REFERENCES
[1] Portworx, â€œAnnual Container Adoption Report,â€ Apr 2017, [Online;
accessed 21. Aug. 2019]. [Online]. Available: https://portworx.com/
2017-container-adoption-survey/
[2] â€œhadolint/hadolint,â€ Aug 2019, [Online; accessed 21. Aug. 2019].
[Online]. Available: https://github.com/hadolint/hadolint
[3] M. Tufano, F. Palomba, G. Bavota, M. Di Penta, R. Oliveto,
A. De Lucia, and D. Poshyvanyk, â€œThere and back again: Can you
compile that snapshot?â€ Journal of Software: Evolution and Process ,
vol. 29, no. 4, p. e1838, 2017, e1838 smr.1838. [Online]. Available:
https://onlinelibrary.wiley.com/doi/abs/10.1002/smr.1838
[4] F. Hassan and X. Wang, â€œHirebuild: An automatic approach to history-
driven repair of build scripts,â€ in Proceedings of the 40th International
Conference on Software Engineering, ser. ICSE â€™18. New York, NY ,
USA: Association for Computing Machinery, 2018, p. 1078â€“1089.
[Online]. Available: https://doi.org/10.1145/3180155.3180181
[5] J. Henkel, C. Bird, S. K. Lahiri, and T. Reps, â€œA dataset of dockerï¬les,â€
2020.
[6] N. Reimers and I. Gurevych, â€œSentence-bert: Sentence embeddings
using siamese bert-networks,â€ in Proceedings of the 2019 Conference
on Empirical Methods in Natural Language Processing. Association
for Computational Linguistics, 11 2019. [Online]. Available: http:
//arxiv.org/abs/1908.10084
[7] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, â€œBert: Pre-
training of deep bidirectional transformers for language understanding,â€
Proceedings of the 2019 Conference of the North, 2019. [Online].
Available: http://dx.doi.org/10.18653/v1/N19-1423
[8] R. J. G. B. Campello, D. Moulavi, and J. Sander, â€œDensity-based
clustering based on hierarchical density estimates,â€ in Advances in
Knowledge Discovery and Data Mining, J. Pei, V . S. Tseng, L. Cao,
H. Motoda, and G. Xu, Eds. Berlin, Heidelberg: Springer Berlin
Heidelberg, 2013, pp. 160â€“172.
[9] docker, â€œDocker hub: Database of container images,â€ 2015, hub.docker.
com.
[10] J. V ouillon and R. D. Cosmo, â€œBroken sets in software repository evo-
lution,â€ in 2013 35th International Conference on Software Engineering
(ICSE), May 2013, pp. 412â€“421.
[11] S. McCamant and M. D. Ernst, â€œEarly identiï¬cation of incompatibilities
in multi-component upgrades,â€ in ECOOP 2004 â€“ Object-Oriented
Programming, M. Odersky, Ed. Berlin, Heidelberg: Springer Berlin
Heidelberg, 2004, pp. 440â€“464.
[12] A. MÃ¸ller and M. T. Torp, â€œModel-based testing of breaking changes in
node.js libraries,â€ in Proceedings of the 2019 27th ACM Joint Meeting
on European Software Engineering Conference and Symposium on
the Foundations of Software Engineering, ser. ESEC/FSE 2019. New
York, NY , USA: Association for Computing Machinery, 2019, p.
409â€“419. [Online]. Available: https://doi.org/10.1145/3338906.3338940
[13] C. Tucker, D. Shuffelton, R. Jhala, and S. Lerner, â€œOpium: Optimal
package install/uninstall manager,â€ in 29th International Conference on
Software Engineering (ICSEâ€™07), May 2007, pp. 178â€“188.
[14] F. Mancinelli, J. Boender, R. D. Cosmo, J. V ouillon, B. Durak, X. Leroy,
and R. Treinen, â€œManaging the complexity of large free and open source
package-based software distributions,â€ in 21st IEEE/ACM International
Conference on Automated Software Engineering (ASEâ€™06), Sept 2006,
pp. 199â€“208.
[15] A. Barcelona, â€œDependendy on ruby version 2.6.5,â€ 2020,
https://github.com/AjuntamentdeBarcelona/decidim-barcelona/blob/
83ef28ee6af9d7ec2ac7914762c00db165592615/Gemï¬le#L5.
[16] M. Ester, H.-P. Kriegel, J. Sander, and X. Xu, â€œA density-based al-
gorithm for discovering clusters in large spatial databases with noise,â€
inProceedings of the Second International Conference on Knowledge
Discovery and Data Mining, ser. KDDâ€™96. AAAI Press, 1996, p.
226â€“231.
[17] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, and I. Sutskever,
â€œLanguage models are unsupervised multitask learners,â€ 2019.
[18] T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal,
A. Neelakantan, P. Shyam, G. Sastry, A. Askell, S. Agarwal, A. Herbert-
V oss, G. Krueger, T. Henighan, R. Child, A. Ramesh, D. M. Ziegler,
J. Wu, C. Winter, C. Hesse, M. Chen, E. Sigler, M. Litwin, S. Gray,
B. Chess, J. Clark, C. Berner, S. McCandlish, A. Radford, I. Sutskever,
and D. Amodei, â€œLanguage models are few-shot learners,â€ 2020.
[19] oshivwanshi, â€œRROR: Error installing bundler: bundler requires ruby
version,â€ 2019, https://github.com/rubygems/bundler/issues/6865.[20] r1williams, â€œRpmdb checksum is invalid: dcdpt(pkg checksums),â€ 2015,
https://github.com/CentOS/sig-cloud-instance-images/issues/15.
[21] D. Schulze, â€œapt-get update fails on 17.04 [closed],â€ 2018, https:
//askubuntu.com/questions/1059898/apt-get-update-fails-on-17-04.
[22] jahanzaib basharat, â€œE: Package â€™libpng12-devâ€™ has no installation
candidate,â€ 2018, https://github.com/docker-library/php/issues/662.
[23] Paciï¬cNW_Lover, â€œInstall python-pip using apt-get via ubuntuâ€™s apt-get
in dockerï¬le,â€ 2020, https://stackoverï¬‚ow.com/a/61564831.
[24] Tan, â€œHow to ï¬x your ruby version is 2.3.0, but your gemï¬le speciï¬ed
2.2.5 while server starting,â€ 2016, https://stackoverï¬‚ow.com/questions/
37914702.
[25] ubergesundheit, â€œChange locale to c.utf-8,â€ 2015, https://github.com/
docker-library/ruby/issues/45.
[26] rmNyro, â€œnpm not found on latest build,â€ 2017, https://github.com/
gliderlabs/docker-alpine/issues/327.
[27] yelizariev, â€œbzr is not available in alpline:,â€ 2020, https://github.com/
alpinelinux/docker-alpine/issues/87.
[28] Buddhi, â€œHow to download a ï¬le using curl,â€ 2019, https://stackoverï¬‚ow.
com/a/54735579.
[29] J. Henkel, C. Bird, S. K. Lahiri, and T. Reps, â€œLearning from, under-
standing, and supporting devops artifacts for docker,â€ 2020.
[30] Linnovate, â€œMean stack,â€ 2020, https://github.com/linnovate/mean.
[31] mtsmfm, â€œSet lang by default,â€ 2020, https://github.com/docker-library/
ruby/commit/8813cdda206acb36ea7797919bf8dadb84fc5ac7.
[32] A. Rahman, R. Mahdavi-Hezaveh, and L. Williams, â€œA systematic
mapping study of infrastructure as code research,â€ Information &
Software Technology, vol. 108, pp. 65â€“77, 2019. [Online]. Available:
https://doi.org/10.1016/j.infsof.2018.12.004
[33] J. Cito, G. Schermann, J. E. Wittern, P. Leitner, S. Zumberi, and H. C.
Gall, â€œAn empirical analysis of the docker container ecosystem on
github,â€ in 2017 IEEE/ACM 14th International Conference on Mining
Software Repositories (MSR), May 2017, pp. 323â€“333.
[34] Y . Wu, Y . Zhang, T. Wang, and H. Wang, â€œAn empirical study of build
failures in the docker context,â€ in Proceedings of the 17th International
Conference on Mining Software Repositories, ser. MSR â€™20. New
York, NY , USA: Association for Computing Machinery, 2020, p.
76â€“80. [Online]. Available: https://doi.org/10.1145/3379597.3387483
[35] C. Lin, S. Nadi, and H. Khazaei, â€œA large-scale data set and an
empirical study of docker images hosted on docker hub,â€ in 2020
IEEE International Conference on Software Maintenance and Evolution
(ICSME), 2020, pp. 371â€“381.
[36] T. Xu and D. Marinov, â€œMining container image repositories for software
conï¬guration and beyond,â€ in 2018 IEEE/ACM 40th International Con-
ference on Software Engineering: New Ideas and Emerging Technologies
Results (ICSE-NIER), May 2018, pp. 49â€“52.
[37] A. Zerouali, T. Mens, G. Robles, and J. M. Gonzalez-Barahona, â€œOn
the relation between outdated docker containers, severity vulnerabilities,
and bugs,â€ in 2019 IEEE 26th International Conference on Software
Analysis, Evolution and Reengineering (SANER), Feb 2019, pp. 491â€“
501.
[38] J. Xu, Y . Wu, Z. Lu, and T. Wang, â€œDockerï¬le tf smell detection based
on dynamic and static analysis methods,â€ in 2019 IEEE 43rd Annual
Computer Software and Applications Conference (COMPSAC), vol. 1,
July 2019, pp. 185â€“190.
[39] Y . Zhang, G. Yin, T. Wang, Y . Yu, and H. Wang, â€œAn insight into the
impact of dockerï¬le evolutionary trajectories on quality and latency,â€ in
2018 IEEE 42nd Annual Computer Software and Applications Confer-
ence (COMPSAC), vol. 1. IEEE, 2018, pp. 138â€“143.
[40] F. Hassan, R. Rodriguez, and X. Wang, â€œRudsea: Recommending
updates of dockerï¬les via software environment analysis,â€ in
Proceedings of the 33rd ACM/IEEE International Conference on
Automated Software Engineering, ser. ASE 2018. New York, NY ,
USA: Association for Computing Machinery, 2018, p. 796â€“801.
[Online]. Available: https://doi.org/10.1145/3238147.3240470
[41] T. Sharma, M. Fragkoulis, and D. Spinellis, â€œDoes your conï¬guration
code smell?â€ in 2016 IEEE/ACM 13th Working Conference on Mining
Software Repositories (MSR). IEEE, 2016, pp. 189â€“200.
[42] Y . Jiang and B. Adams, â€œCo-evolution of infrastructure and source code-
an empirical study,â€ in 2015 IEEE/ACM 12th Working Conference on
Mining Software Repositories. IEEE, 2015, pp. 45â€“55.
[43] L. Gazzola, D. Micucci, and L. Mariani, â€œAutomatic software repair: A
survey,â€ IEEE Transactions on Software Engineering , vol. 45, no. 1, pp.
34â€“67, 2019.
1159[44] Y . Lou, J. Chen, L. Zhang, D. Hao, and L. Zhang, â€œHistory-driven
build failure ï¬xing: How far are we?â€ in Proceedings of the 28th
ACM SIGSOFT International Symposium on Software Testing and
Analysis, ser. ISSTA 2019. New York, NY , USA: Association
for Computing Machinery, 2019, p. 43â€“54. [Online]. Available:https://doi.org/10.1145/3293882.3330578
[45] C. Macho, S. McIntosh, and M. Pinzger, â€œAutomatically repair-
ing dependency-related build breakage,â€ in 2018 IEEE 25th Interna-
tional Conference on Software Analysis, Evolution and Reengineering
(SANER), 2018, pp. 106â€“117.
1160