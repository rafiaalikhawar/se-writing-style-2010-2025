Establishing Multilevel Test-to-Code Traceability Links
Robert White
University College London
London, UKJens Krinke
University College London
London, UKRaymond Tan
University College London
London, UK
ABSTRACT
Test-to-codetraceabilitylinksmodeltherelationshipsbetweentest
artefactsandcodeartefacts.Whenutilisedduringthedevelopment
process, these links help developers to keep test code in sync with
tested code, reducing the rate of test failures and missed faults.
Test-to-code traceability links can also help developers to maintain
an accurate mental model of the system, reducing the risk of archi-
tectural degradation when making changes. However, establishing
and maintaining these links manually places an extra burden on
developersandiserror-prone.ThispaperpresentsTCtracer,an
approachandimplementationfortheautomaticestablishmentof
test-to-code traceability links. Unlike existing work, TCtracer op-
eratesatboththemethodlevelandtheclasslevel,allowingusto
establish links between tests and functions, as well as between test
classes and tested classes. We improve over existing techniques
by combining an ensemble of new and existing techniques and
exploitingasynergisticflowofinformationbetweenthemethod
and class levels. An evaluation of TCtracer using four large, well-
studied open source systems demonstrates that, on average, we
can establish test-to-function links with a mean average precision
(MAP) of 78% and test-class-to-class links with an MAP of 93%.
ACM Reference Format:
RobertWhite,JensKrinke,andRaymondTan.2020.EstablishingMultilevel
Test-to-CodeTraceabilityLinks.In 42ndInternationalConferenceonSoftware
Engineering(ICSEâ€™20),May23â€“29,2020,Seoul,RepublicofKorea. ACM,New
York, NY, USA, 12 pages. https://doi.org/10.1145/3377811.3380921
1 INTRODUCTION
Unittestingisan integralpartofsoftwaredevelopment,however,
tofullyrealisethebenefitsofunittesting,itisnecessarytomain-
tain an accurate picture of the relationships between the tests andthe tested code. Traceability links provide an intuitive mechanism
for modelling these relationships. Once established, test-to-codetraceability links can improve the software engineering processin several ways, including making changes to the system safer,
facilitating thereuse of artefacts,and aiding programcomprehen-
sion [2,9,34]. Changes to the system become safer as, when a
developermakesachangetoapieceoftestedcode,theycanusethe traceability links to easily discover which tests also need to
bechanged,andvice-versa.Thisreducestheriskofdesynchroni-
sation between the tests and code, an issue which can cause test
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation
onthe firstpage.Copyrights forcomponentsof thisworkowned byothersthan the
author(s)mustbehonored.Abstractingwithcreditispermitted.Tocopyotherwise,or
republish,topostonserversortoredistributetolists,requirespriorspecificpermission
and/or a fee. Request permissions from permissions@acm.org.
ICSE â€™20, May 23â€“29, 2020, Seoul, Republic of Korea
Â© 2020 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 978-1-4503-7121-6/20/05...$15.00
https://doi.org/10.1145/3377811.3380921failures and prevent the discovery of new faults. While developers
canusefaultlocalisationtechniquestodiscoverwhichfunctions
maybecausingtestfailures,traceabilitylinkshavethebenefitof
being bidirectional, so developers can start from a function and
find the corresponding tests. Industrial need for the automated
establishment of test-to-code traceability links is demonstrated by
StÃ¥hletal.[ 31]throughcasestudiesanddeveloperinterviews.The
developerinterviewswerefocusedonthemesandthethemethat
encompasses this work, â€™Test Results and Fault Tracingâ€™, attracted
the most number of relevant statements, with interviewees stating,
for example, that it was â€™particularly importantâ€™ and â€™super crucialâ€™.
Usingtrace linkstoâ€™drill downâ€™whentroubleshooting failedtests
was specifically mentioned. The developers also made clear that
automation is crucial as manual traceability handling is a major
blocker for more frequent deliveries of software.
While there has been an effort on some projects to have de-
velopersmanuallymaintaintraceabilitylinks,thispracticeisnot
commonasitcreatesextraworkfordevelopers.Instead,developersoftenemploy namingconventions,e.g.,matching thenames oftest
classeswiththenamesoftestedclasses,withâ€˜Testâ€™appended.In
most instances, where projects have attempted to manually main-
tain traceability links, these have been at the class level where the
numberoflinksismoremanageableandtherelationshipsbetween
test artefacts and tested artefacts are usually simple. Therefore,
toavoidcreatingextraworkforthedevelopersandtheerrorsas-
sociated with the manual maintenance of traceability links, the
research community has focused on developing approaches for the
automatic establishment of traceability links.
Most previous work on test-to-code traceability (see Parizi et
al.[27]for anoverview)has focused ontheclass level,wheretest
classes are linked to their tested classes [ 6,7,15,20,29,32]. Not
muchworkhasbeendoneonthemethodlevel[ 3,16,17],where
individual unit tests are linked to their tested functions, despite
beingshowntobehelpfulfordevelopers[ 17].Ourworkisthefirst
toaddressboththeclasslevelandthemethodlevelsimultaneously.
Thisallowsustoconstructbothtypesoflinksandutiliseacross-
level flow of information to improve overall performance. This
givesourapproachamoreaccurateandfine-grainedviewofthe
relationships between the artefacts. Our work also distinguishesitself from previous work by utilising dynamic information and
rankingpotentiallinks,insteadofthestaticinformationthathas
typically been used before to generate sets of (unranked) links.
The difficulty in establishing test-to-code links lies in the fact
that not all code executed by a test is part of the code that is being
tested. This is because many tests will call functions which are not
considered to be amongst the functions under test, such as helper
functions,gettersandsetters,orfunctionsthatinitialisethestate
of an object before the functions under test are invoked. Therefore,
simplyconsideringallexecutedcodeastestedcode[ 17]isnotan
accurate technique of establishing test-to-code traceability links.
8612020 IEEE/ACM 42nd International Conference on Software Engineering (ICSE)
Inthispaper,wepresentTCtracer,anapproachandimplemen-
tation which aims to overcome the weaknesses of existing test-to-
code traceability link establishment approaches by employing a
wide range of techniques that utilise information from dynamic
call traces, including two new call-based statistical techniques.
TCtraceralsocombinesthesetechniquestoproduceasingle
scorethatperformsbetteroverallthananyindividualtechnique.In
addition,TCtracerisappliedtoboththemethodlevelandtheclass
level which allows us to establish links between individual testsand their tested functions as well as whole test classes and their
tested classes. TCtracer uses its multilevel aspect to create a flow
of information between the levels that can improve effectiveness.
Our approach is evaluated using a manually curated ground
truth [33], at both the method and class levels, from four non-
trivialandwell-studiedsubjectprojects1.Ourfindingsshowthat,on
average,usingourcombinedtechnique,wecanachieveanincrease
ineffectivenessoverexistingtechniquesatboththemethodlevel
and class level. The main contributions of this paper are:
â€¢Anapproachtotest-to-codetraceabilitythatutilisesanen-
semble of techniques and a multilevel flow of information.
â€¢A comparative evaluation of each technique at both the
method and class levels.
â€¢Anevaluationofthebenefitgainedbyutilisingmultilevel
information.
â€¢A manually curated ground truth dataset [ 33] of test-to-
function and test-class-to-class links.
2 MOTIVATION
The development of a new approach to test-to-code traceability
establishment is motivated primarily by the fact that all existing
techniqueshavesomeweaknessesthatmakethemunsuitablefor
use as a general solution. One of the most common techniquesfor establishing traceability links, naming conventions (NC), is agood example of this. This approach relies on using the naming
conventions for test artefacts (unit tests or test classes) to identify
theirlinkstotestedartefacts(functionsorclasses).Thespecificcon-
ventionsusedmayvarybetweenprojects,however,thestandard
conventionisthatatestartefactshouldsharethesamenameasthe
artefact that it is testing, with testprepended or appended [ 23,32].
Forexample,afunctionnamed unionwillbeconsideredtobetested
by a test named testUnion. However, this technique is not effective
iftheprojectdoesnotadheretothenamingconventionsandcan
have poor recall even for projects that do. This is because it as-
sumes a one-to-one relationship between test artefacts and tested
artefactswhenthisisnotalwaysthecase.TheCommonsCollec-
tions project [ 11] provides a real-world example of this, where the
functiondisjunction istestedbythetests testDisjunctionAsUnionMi-
nusIntersection andtestDisjunctionAsSymmetricDifference.Asthis
isaone-to-tworelationship,thenamesdonotmatchthenaming
convention and NC would not be able to recover these links.
Last Call Before Assert (LCBA) is another existing technique
that has severe limitations. LCBA operates on the assumption that
the function which returned last before an assert is called is the
functionthattheassertistesting.However,thisassumptionisoften
incorrect. One common example of this is when the purpose of
1Evaluation artefacts available at github.com/RRGWhite/icse20-main-1069-data.a tested function is to change the state of an object. In this case,to check that the function has performed the correct operation,a getter must be called to get the changed state so that it can becompared to an oracle. This causes LCBA to incorrectly identifythegetterasthetestedfunction.Evenifthetestedfunctiondoes
directlyreturnthevaluethatneedstobechecked,thisvaluewill
oftennotbecheckedbyanassertimmediatelyafterbeingreturned.
This could be because the test needs to call helper functions before
the assert, possibly to establish the oracle.
Finally, textual similarity measures based on information re-
trieval techniques have also been used in an attempt to recover
test-to-codetraceabilitylinks,withvaryingdegreesofsuccess[ 2,6].
However,noneofthemhavebeenshowntobesufficientontheir
own as techniques designed for natural language do not directly
translate to code. This is due to the bimodality of code which leads
to the possibility that two code snippets may be closely related
semantically but completely different lexically, or vice-versa [1].
Given these inherent weaknesses in the individual existing tech-
niques, there is a strong motivation to design a new approach that,
while exploiting the strengths of the individual techniques, collec-
tively overcomes their weaknesses. This is the approach utilised in
TCtracer and presented in this paper.
A secondary motivation for the development of a new approach
totest-to-codetraceabilitystemsfromthefactthatexistingwork
hasonlyfocusedoneitherthemethodlevelortheclasslevel.As
both levels can provide useful information to a developer, we were
motivated to develop a single approach that worked at both levels
simultaneously.Thisresultedinthemultilevelaspectof TCtracer,
whichinturnfacilitatedtheuseofmultilevelinformationflowto
further increase the effectiveness of the approach.
3 APPROACH
Ourapproachobserveswhichartefactsareexecutedwhileatestruns, creating candidate links between test artefacts and tested
artefacts. It then assigns scores to the candidate links. These scores
are used to rank the candidates and predict which of them are true
test-to-codetraceabilitylinks.Thepredictedlinkscanthenbeused,
e.g., in an IDE, to navigate between test and the tested artefacts.
As we are establishing links on the method- as well as on the
class-level,weusethetermsfunctionormethod-under-testwhen
referring to a tested method and the terms tested class or class-
under-testfortheclass-level.Moreover,ontheclass-level,aclass-
under-testistestedbyoneormoretestclasses,andonthemethod-
level, a method-under-test is tested by one or more test methods.
Ourmultilevelapproachstartsbydynamicallycollectinginfor-
mation about each function call made by each test, specifically,
which function was called and the depth in the call stack of the
function call relative to the calling test. For each test, this informa-
tion is stored in an object, henceforth referred to as a hit spectrum.
Wethenapplyanensembleoftraceabilitytechniquestothemethod
level, using the information in the collected hit spectra. This re-
sults in a set of test-to-function scores for each technique, each of
which encodes the likelihood that a given function is the tested
functionforagiventestthatcallsit.Werefertothesescorescol-
lectivelyasthemethodlevelinformation.Thesameprocessisthen
applied at the class level, where sets of test-class-to-class scores
862areestablishedusingthesametechniques,providinguswiththe
classlevelinformation.Atthisstage,wecreateacross-levelflow
ofinformation byutilisingthe methodlevelinformation forclass
level predictions and the class level information to augment the
method level predictions.
To compute our scores we selected two existing test-to-code
traceability techniques and formulated six new techniques. Six
of the techniques produce a score in the interval [0 ,1] for every
possiblelink,indicatingthelikelihoodthatthelinkiscorrect,whiletheothertwoproducebinaryscores.Aninthscoreisalsocomputed
that combines the scores for all the individual techniques. Thesescores are used to rank the candidate links so that those ranked
highest are most likely to be true traceability links. Thresholds are
then applied to construct the sets of predicted links.
Wedescribeourtechniquesinthefollowingsectionwhere,for
simplicity,wewillpresentthematthemethodlevel.Toapplythem
on the class level, test classes are used instead of test methods and
tested classes instead of tested functions.
3.1 Techniques
As discussed in Section 2, existing test-to-code traceability tech-
niques have weaknesses which we try to overcome with new tech-
niques.Despitetheirweaknesses,weselectedtwoestablishedtech-niques, Naming Conventions (NC) [
32] and Last Call Before Assert
(LCBA)[32]becausetheyperformwellincertainsituations.The
newtechniquesformulatedforTCtracerincludefourstring-based
techniques: a variant of Naming Conventions (NCC), two variants
ofLongestCommonSubsequence(LCS-BandLCS-U),andusing
theLevenshteineditdistance[ 21],whichallutilisenamesimilarity.
Two statistical call-based techniques (SCTs) based on Tarantula
fault localisation [ 19] and Term Frequencyâ€“Inverse Document Fre-
quency (TFIDF) [24] are also included in the new techniques.
TheoriginalNCwasselectedforourtechniqueensembleasit
shouldhavehighprecision,especiallyinprojectswherethenam-
ing conventions are strictly followed and is a common methodbywhichdevelopersidentifytestsforagivenmethodduringde-
velopment [
17,23]. LCBA was selected as it can perform well in
certaincircumstances,specificallywhenthetestsconformtothe
styleofusinganasserttotestthereturnedvaluefromafunction
immediately after the function is called. As both NC and LCBA
arewell-established techniquesfortest-to-code traceabilityrecov-
ery [6,23,28,29], they also make good candidates to serve as
comparison points for our other techniques.
NCC requires that the name of the test contains the name of
the tested artefact. It was included in the technique ensemble as
itutilisesthestrengthsofNCbutshouldachievehigherrecallas
it can establish many-to-one relationships between functions and
tests, as opposed to the solely one-to-one relationships that are
discoverablewithtraditionalNC.Thishelpstoalleviatesomeoftheproblems with traditional NC, as discussed in Section 2. LCS-B andLCS-U compute the ratio of the name lengths and the length of thelongestcommonsubsequenceofthenamesofthetestandthetested
artefact. Theywere used asthey utilisethe same intuitionsas NC
andNCCrespectivelybutinsteadofproducingabinaryscore,they
produceareal-valuedscorethatindicateshowclosetosatisfying
NC/NCC the potential link is. This is useful as there are instanceswhereNC/NCCarenotsatisfiedbutareveryclosetobeingsatisfied,
for example, in the case of NC, if there are extra words before or
afterthenameofthefunctionor,inthecaseofNCC,ifthename
ofthefunctionisabbreviatedorhasgrammaticaldifferencesinthe
nameofthetestcase.Intheseinstances,thereal-valuedscoresof
LCS-B and LCS-U are more useful than the binary scores of NC
andNCCaswecanstilldetermineifatestandafunctionarelikely
related.Weinclude thenormalisedLevenshteindistancebetween
the names as a technique as it provides a different view of name
similaritytothelongestcommonsubsequencewhichisusedinthe
LCS-B and LCS-U techniques.
We include the Tarantula technique as, intuitively, the task of
recovering test-to-code traceability links is similar to the task of
fault localisation as, if a function is causing a test to fail, it is likely
that function and test should be linked. Therefore, our intuition is
thatbyadaptingawell-knownfaultlocalisationtechniquetotrace-
ability we may find an effective method of recovering test-to-code
trace links. The inclusion of the TFIDF technique is motivated in a
similarfashiontoTarantulainthatweviewthetaskofdetermining
the relevance of terms to a document as being analogous to the
taskofdeterminingwhichfunctionsaremostrelevanttoatestcase
andthereforewhichfunctionsaremostlikelytobethetargetsof
thattest.AsTFIDFisastandard,well-testedmethodofestablishingterm relevance, we adapted this method to test-to-code traceability.
Alloftheaboveeighttechniqueswillbeevaluatedtoidentifyin-
dividualstrengthsandweaknessesandcomparedtotheestablished
techniquesNCandLCBAtoestablishiftheirknownweaknesses
can be overcome. We also include all techniques in a combined
scoreaswebelievethateachtechniquehasthepotentialtoprovide
at least some information that cannot be wholly obtained using
anyothertechnique.However,thisiscurrentlymerelyanintuition
and will be tested in future work by measuring the contribution of
each technique to the effectiveness of the combined score.
Allofourtechniquesutilisedynamictraceinformationwhich
allowsustoavoidcommonproblemsassociatedwithstatictech-niques, such as over-approximation and the inability to reason
about references and dependencies that are resolved at run-time.
We have discarded a series of other techniques. Fixture Ele-
ment Types (FET) [ 32] and SCOTCH+ [ 29] cannot be applied on
method-levelandStaticCallGraph(SCG),LexicalAnalysis(LA),
andCo-Evolution(Co-Ev)havebeendiscardedbecauseoftheirlow
precision and recall [20, 27, 32].
3.1.1 Naming Conventions. As naming conventions can change
betweenprojects[ 32],wehaveselectedtwotechniquesfortrace-
abilityrecoveryusingnamingconventions: traditional andcontains.
TraditionalNamingConventions(NC). NCestablisheslinksby
considering a function to be linked to a test if the name of the test
is the same as the function after the word testhas been removed
fromthetestname.Forexample,afunctionnamed unionwillbe
considered to be tested by a test named testUnion.
score(t,f)=/braceleftBigg
1,ifntequalsnf
0,otherwise(1)
Wherentandnfare the names of tandfrespectively, after the
wordtesthas been removed from the name of test t.
863NamingConventionsâ€“Contains(NCC). NCCisaderivativeof
traditionalNCwhichreplacestherequirementthatthetestname
mustmatchthefunctionnameexactly,withthemorerelaxedre-
quirement that the test name only needs to contain the function
name. Therefore, NCC considers a function to be linked to a testif the name of the test contains the name of the function, after
removing testfromthetestname.ApositiveNCCresultiscounted
as a score of 1 while a negative NCC result is counted as 0:
score(t,f)=/braceleftBigg
1,ifnfsubstring of nt
0,otherwise(2)
3.1.2 Last Call Before Assert (LCBA). LCBA attempts to establish
traceability links by working on the assumption that the function
returnedlastbeforeanassertiscalledisthefunctionthattheassert
is testing. Therefore, LCBA will establish links between a test and
every function that is returned last before an assert that appears in
that test. In TCtracer, if an LCBA link is established between atest and a function it is counted as a traceability score of 1 while
no LCBA link is counted as a score of 0:
score(t,f)=/braceleftBigg
1,iffis last return before an assert in t
0,otherwise(3)
3.1.3 Name Similarity. Name similarity is a variation of the Nam-
ingConventionsapproach andisbasedon thepremisethatdevel-
opers, following established naming conventions, give unit tests
namesthataresimilartoormatchthenameofthefunction.Our
hypothesis is that name similarity measures have the potentialto perform better than the existing NC approach as they are lessstrict on exact matches and allow for slight variations in name,forexample,duetogrammaticalreasons.Forinstance,amethodnamedclonewould not be identified under NC for a test named
testCloning, whereas it would be possible under name similaritymeasures for cloneto be assigned a high traceability score with
testCloning.Weconsiderthenameforamethodtobesimplythe
name of the method in lower case without the class name and
with the string testremoved from test names when performing
comparisons. For example, for the fully qualified method name
com.example.ExampleClass.testComputeScore(boolean), we perform
name similarity comparisons on computescore. To compute the
namesimilarity,weusetwowell-establishedtechniques, Longest
Common Subsequence (LCS) andLevenshtein Distance.
To establish the LCS similarity, we compare the length of the
longestcommonsubsequencetothelengthofthefunctionandtest
name. The longest common subsequence techniques give function
namesthathavemorecharactersincommonwith(andinthesame
order as) a test name a higher score.
Longest Common Subsequence â€“ Both (LCS-B). In the first LCS
variant, we maximise the score at 1 when the method and function
names coincide exactly (aligned with the behaviour of the NCapproach),thatis,when
nt=nfandLCS(nt,nf)=nt.Wedivide
thelengthoftheLCSbythegreaterofthelengthofthetwostrings
as follows:
score(t,f)=|LCS(nt,nf)|
max(|nt|,|nf|)(4)LongestCommonSubsequenceâ€“Unit(LCS-U). Inthesecondvari-
ant,wedividethelengthoftheLCSbythelengthofthefunction
name only. This variant is more closely aligned with the behaviour
of the NCC approach, with the score maximised at 1 when the
function name is contained in the test name.
score(t,f)=|LCS(nt,nf)|
|nf|(5)
LevenshteinDistance. TheLevenshteindistance[ 21],oftenknown
aseditdistance,measuresthedistancebetweentwostringsbymea-
suring the minimum number of edits it takes to transform one
stringinto theother.Under thistechnique,the distancesbetween
the function names and test names are computed and links withthe lowest Levenshtein distance are awarded the highest scores.
WefirstnormalisetheLevenshteindistancebydividingitbythe
lengthofthelongeststringandthentakethecomplimentsothat
higher scores are given to closer strings:
score(t,f)=1âˆ’/parenleftBigg
Levenshtein( nt,nf)
max(|nt|,|nf|)/parenrightBigg
(6)
3.1.4 Tarantula. Tarantula[ 19]isanautomaticfaultlocalisation
technique that assigns a suspiciousness value to code, with higher
suspiciousness valuesindicating a higherprobability ofthe code in
question being responsible for the fault. The use of automatic fault
localisation is based on the idea that it would point to the most
relevantentityifthecurrenttestfails.Thesuspiciousnessofacode
entityeis defined as follows:
suspiciousness( e)=failed(e)
totalfailed
passed(e)
totalpassed+failed(e)
totalfailed(7)
Where failed( e) is the number of tests that executed eand failed,
totalfailed isthenumberofteststhatfailedintotal,passed( e)isthe
numberofteststhatexecuted eandpassed,and totalpassed isthe
number of tests that passed in total.
To obtain the traceability score for a given test-to-function pair,
wherethetestexecutesthefunction,wecomputethesuspiciousness
ofthefunctionwithrespecttothetest,assumingthatthetestunder
considerationfailsandallotherspass2.Usingthisassumptionwe
can derive our traceability score equation from Equation 8:
score(t,f)=1
|{t/primeâˆˆT:fâˆˆt/prime}|âˆ’1
|T|âˆ’1+1(8)
WhereTisthesetofalltestsinthetestsuiteand fâˆˆt/primeindicates
that function fis executed by test t/prime. For pairs where the test t
does not execute the function f, a score of 0 is assigned.
3.1.5 Term Frequencyâ€“Inverse Document Frequency (TFIDF). Term
frequencyâ€“inverse document frequency (TFIDF) is a measure tradi-
tionally used in information retrieval to determine how significant
atermistoadocument.TFIDFtakesintoaccounttheprevalence
of the term in the document and in the corpus as a whole, with the
intuitionbeingthatifatermisfrequentinaparticulardocument
butnotfrequentintherestofthecorpus,thattermmustcarrya
highsignificancetothedocumentandcarriesusefulinformation
2Amodelunderwhichalltestsexecutingthefunctionfailisnotsuitableasthe
Tarantula suspiciousness would then be 100%.
864about the semantics of the document. We apply this to the domain
of test-to-code traceability by having tests take the role of the doc-
uments and functions take the role of the terms. This expresses the
intuition that if a function is executed frequently by a particular
testandinfrequentlybyothertests,itislikelythatthetestistesting
the function. We define our traceability score using TFIDF as:
score(t,f) = tf(t,f)Â·idf(f) (9)
Theusualdefinitionofthetermfrequency(tf)functiondoesnot
match the test/function scenario. Thus, tf and idf are defined as:
tf(t,f)=l n/parenleftbigg
1+1
|{f/primeâˆˆF:f/primeâˆˆt}|/parenrightbigg
(10)
idf(f)=l n/parenleftbigg
1+|T|
|{t/primeâˆˆT:fâˆˆt/prime}|/parenrightbigg
(11)
WhereTis the set of all tests in the test suite and Fis the set
of all functions in the system. The tf function measures how the
information of a test is spread over the called functions and the idf
function measures how common the function is over all tests.
3.2 Score Scaling
Ourapproachutilisestwotechniquesforscalingtraceabilityscores
which can be applied independently as well as composed together.
3.2.1 Call Depth Discounting. Tests often do not invoke the tested
functions directly, for example when a public method delegates
the actual implementation to a private method. The TCtracer
approachutilisestheintuitionthattherelativedepthbetweenatest
andafunctioninthecallstackcanserveasanindicatorofifthefunctionistestedbythetest.Wehypothesisethatfunctionsthat
are closer to a test in the call stack are more likely to be the tested
functionsthanfunctionsthatarefaraway.Therefore,weutilisea
relative call depth discount factor Î³âˆˆ[0,1], which discounts the
traceability score for a test-to-function pair in proportion to the
distance between them in the call stack:
scored(t,f) = score( t,f)Â·Î³(dist(t,f)âˆ’1)(12)
Wherescoredis the discounted score, scoreis the non-discounted
score,and dist(t,f)isthedistancebetweenthetestandthefunction
inthecallstack.Wesubtractonefromthedistancesoastoapply
no discount to functions that are called directly by the test.
3.2.2 Normalisation. The computed scores can be used to rank
the possible links to called functions within a test directly, usingthe top-ranked link as the most likely link. However, the actual
distributionofscorescanvarybetweentechniquesandthedifferent
tests.Therefore,wenormalisethescoressothatthelargestscore
within a test is 1:
scoren(t,f)=scored(t,f)
max({scored(t,f/prime)|f/primeâˆˆt})(13)
Wherescorenisthenormalisedscore.Normalisationallowsusto
define a threshold around the top-ranked link.
In the end, we focus on nine individual techniques, shown in
Table 1. NC, NCC and LCBA are binary, i.e., they produce scores
ofeither1or0whichareuseddirectly.Thesixothernon-binary
techniques are normalised and use call depth discounting.Table1:Traceabilitytechniques,theirscorerange(Score),if
the technique is normalised (N), and the used threshold ( Ï„).
Technique Score N Ï„
Naming Conventions (NC) 0 or 1 â€“ â€“
Naming Conv. â€“ Contains (NCC) 0 or 1 â€“ â€“LCS â€“ Unit (LCS-U) [0,1] Yes 0.80
LCS â€“ Both (LCS-B) [0,1] Yes 0.45
Levenshtein (Leven) [0,1] Yes 0.35
Last Call Before Assert (LCBA) 0 or 1 â€“ â€“Tarantula [0,1] Yes 0.95
TFIDF [0,1] Yes 0.90
Combined [0,1] Yes 0.80
3.3 Link Prediction
Toconstructlinkpredictions,wefirstapplyourtraceabilitytech-
niques to the method level and class level individually. The tech-
niques can be directly applied to the class level by using the test
classes instead of test methods and tested classes instead of tested
methods. The information extracted from each level is then propa-
gated between levels to produce another set of links at each level.
3.3.1 Method-Level Prediction. The process starts by executing
eachofournineindividualtraceabilitytechniquesatthemethod
level, resulting in a matrix of scores for each technique:
MâˆˆR|T|Ã—|F|(14)
Where T is the set of all tests in the system and F is the set of all
functions.Eachelementof Misthetraceabilityscoreforagiven
test-to-function pair ( t,f)âˆˆ(TÃ—F).
Another matrix is then constructed for the combined technique
by averaging over all the individual technique matrices and nor-
malising the rows, using Equation 13.
Eachoftheseninematricesisusedtobuildsetsofpredictedtest-
to-functiontraceabilitylinks.Toconvertthereal-valuedscoresinto
booleanlink/no-linkpredictionsweintroduceasetofthresholds,
oneforeachtechnique(showninTable1),andconsiderscoresabove
the threshold as positive link predictions. Equation 15 defines how
each set of method level traceability links are constructed.
LM ={(t,f)âˆˆTÃ—F|Mtfâ‰¥Ï„} (15)
Where Mtfis the score for the given test-to-function pair and Ï„is
the threshold for the technique.
3.3.2 Class-Level Prediction. Wenowmovetotheclasslevelwhere,
inthesamewayasthemethodlevel,weapplyourindividualtrace-
ability techniques and combine them, resulting in nine matrices,
one for each technique:
CâˆˆR|TC|Ã—|FC|(16)
WhereTCisthesetofalltestclassesinthesystemand FCistheset
ofallnon-testclasses.Eachelementof Cisthetraceabilityscore
for a given test-class-to-class pair ( ct,cf)âˆˆ(TCÃ—FC).
In a similar fashion to the method level, Cis used to compute
sets of class level traceability links using Equation 17.
LC ={(ct,cf)âˆˆTCÃ—FC|Cctcfâ‰¥Ï„} (17)
8653.3.3 Method- to Class-Level Propagation. Given the method level
andclass levelscore matrices,wecan nowpropagateinformation
across levels. First, we elevate the method level information to
the class level by extracting scores from Mand organising them
into class level pairs. This allows us to use them for computing
classleveltraceabilityscores.Todothis,foreachtest-class-to-class
pair(ct,cf),weconstructamatrix EM(ct,cf)toholdtherelevant
method level information:
EM(ct,cf)âˆˆR|t(ct)|Ã—|f(cf)|(18)
Where t(ct) is the set of tests in test class ct,f (cf) is the set of
functions in class cf. Each element of EM(ct,cf) is the method
level traceability score for a given test-to-function pair ( t,f)âˆˆ
(t(ct)Ã—f(cf)).
To obtain the traceability score for the test-class-to-class pair,
the method-level scores in EM(ct,cf) are summed along both di-
mensions, resulting in a scalar score.
Thisprocessisexecutedforeachtest-class-to-classpairinthe
system and the produced scores are used to create a symmetric
matrix that holds the scores for all pairs:
EMâˆˆR|TC|Ã—|FC|(19)
Therefore,eachelementof EMisthescoreforagiventest-class-
to-class pair ( ct,cf)âˆˆ(TCÃ—FC) that is derived from method level
information. All rows in EMare normalised using Equation 13.
The scores in EMare then used to produce a set of class level
predicted links using Equation 20.
LEM ={(ct,cf)âˆˆTCÃ—FC|EMctcfâ‰¥Ï„} (20)
3.3.4 Class- to Method-Level Propagation. Topropagateinforma-
tionfromtheclassleveltothemethodlevel,wetakethemethod
level information in Mand augment it with the class level informa-
tion in C, creating a new matrix AMâˆˆR|T|Ã—|F|. For each test-to-
function pair ( t,f), the augmentation is performed by first finding
thetest-class-to-classpair( ct,cf)thatcorrespondstothetest-to-
functionpair,i.e.,thetestclass ctthatcontainsthetest tandthe
tested class cfthat contains the function f. We then take the score
forthemethodlevelpairfrom Mandthescorefortheclasslevel
pair from Cand multiply them to produce the augmented method
level score for AM, as shown in Equation 21.
AMtf=MtfÂ·Cc(t)c(f) (21)
Where c(m) returns the class containing method m.
From AM, the set of augmented method level traceability link
predictions are produced using Equation 22.
LAM ={(t,f)âˆˆTÃ—F|AMtfâ‰¥Ï„} (22)
4 IMPLEMENTATION
TCtraceriscompatiblewithanyJavasystemthatusestheJUnit
3, 4, or 5 test framework and is compatible with Java 8 or newer.Dynamic trace data is collected from JUnit test suite executions,
which is then used for computing the traceability links by the
techniques described in Section 3.
To collect the dynamic execution traces, TCtracer requires the
system-under-analysistobeinstrumented.TheJavaAgentAPIwas
used for this as it provides access to the bytecode of Java classesand allows for them to be transformed before being loaded by
theJVM.AsshowninFigure1,theinstructionsfortransforming
thebytecodeareprovidedbyaJavaprogram,TCagent,whichis
passedtotheJVMatruntimethroughthe -javaagent flag.TCagent
utilisestheByteBuddy[ 35]libraryandallowsustoeasilytransform
the bytecode of the running system to log the data that is used by
TCtracer to compute the traceability links.
The execution traces are parsed to build the hit spectrum for
each test and record the set of methods that were the last return
beforeanassertwascalled,asisneededforLCBA.Methodsthatarenotdefinedintheproject-under-analysis,suchasthosefrom
third-party APIs, are filtered out.
Inthefinalphase,TCtracercomputesthesetsofpredictedlinks
described in Section 3 using the hit spectra, the LCBA information,
and configuration parameters, such as threshold and call depth
discountfactor.Ifagroundtruthispresent,TCtracercomputes
the evaluation metrics for each set of predicted links.
5 EVALUATION
This section presents our research questions, the design of the
experiments carried out to answer these questions, the results, and
a discussion of the findings.
5.1 Experimental Setup
Theexperimentalsetupconsistsofrunning TCtraceronasetof
opensourcesubjectsandcomputingasetofevaluationmeasures
for each subject, using a manually established ground truth.
Subjects. For our subjects, we selected three well known open
source projects that are written in Java and utilise the JUnit testingframework:CommonsIO[
12],CommonsLang[ 13],andJFreeChart
[22].Thesesubjectswereselectedastheyarewellknown,widely
used, and sufficiently large to demonstrate the applicability of TC-
tracertoreal-worldsystems.Fortheevaluationof TCtracer,we
established a ground truth for these projects at both the method
levelandtheclasslevel.Toestablishthemethodlevelgroundtruth,
weusedateamofthreejudges,onePhDstudentandtwofinal-year
undergraduatestudents,whoeachindependentlyinspectedaset
oftestsselecteduniformlyatrandom fromthesubjectsandmade
determinations about which functions were tested by each test.
After thisprocess, thejudges collectively inspected any instances
where there were disagreements and were able to reach a final,
unanimousjudgement,resultinginfullinter-rateragreement.In
total, the method level ground truth contains 138 oracle links.
Theclasslevelgroundtruthwasprovidedmostlybythedevel-
opers as, in all three projects, a subset of the test classes contain a
commentatthestartoftheclassspecifyingwhichclassesittests.
Thesedeveloperprovidedlinkswereextractedandthenverifiedby
a judge. To boost the number of links for the project with the least
developerlinks,CommonsIO,arandomsamplewasdrawnfrom
the set of all test classes and the tested classes for this sample weredecidedbytwojudgesinthesamewayasthemethodlevelsample,
again resulting in full inter-rater agreement. Another class level
groundtruthhadpreviouslybeenestablishedbySCOTCH+[ 29],
which we also investigated for use. However, due to the age of the
projects, they were all no longer able to be built or were incom-
patible with our tracing agent, TCagent, which requires Java 8 or
866"# '(  

! 
	 
   

   )

   *

   )
 
	

   )
 
	
 ' % ( 	

 	"'
( 	"'(
" 
 
	"'
(! 
 
	"'
(

!
'&&$ (	#
 %
Figure 1: Integration of TCtracer into JUnit.
Table 2: Subject statistics.
Project VersionNum. of
FunctionsNum.
of TestsInstruction
Coverage
Apache Ant 1.9.5 10477 1830 50%
Commons IO 2.5 1246 994 89%
Commons Lang 3.7 3111 3061 95%
JFreeChart 1.0.19 9053 2244 52%
newer. The only ground truth links that we were able to use were
forApacheAnt[ 10]andtheresultscannotbecompareddirectly
as the oldest version of Apache Ant that was compatible with TCa-
gentwasnewerthantheversionusedbySCOTCH+.Thelinksthat
we used from SCOTCH+ were independently established by three
judgeswithanaverageinter-rateragreementof90%.Intotal,our
classlevelgroundtruthcontains608links.Otherpreviouswork[ 6]
hasusednamingconventionstoestablishagroundtruth.However,
as demonstrated by our work, this technique has low recall and
wouldintroducebias.Ultimately,whencreatinganewgroundtruth,
one cannot simply apply an existing traceability technique, as it
causesabiastowardsthattypeoftechnique.Informationaboutthe
subjects is given in Table 2.
EvaluationMeasures. Theevaluationmeasuresweselectedare:
precision, recall, F1 score, mean average precision (MAP), and area
under the precision-recall curve (AUC) [ 24]. We selected preci-
sion and recall as they are elementary measures for evaluating
the performance of a binary classifier and allow us to measure the
proportion of true positives out of all positive predictions and the
proportion of all positive examples that are retrieved. As precision
andrecallgenerallyrepresentatrade-offbetweeneachother,the
F1scoreisausefulmeasureasitevenlyweightsbothprecisionand
recall,allowingusto determinewhichtechniquesbesthandlethe
trade-off.Wealsousethemeanaverageprecision(MAP)asittakes
into account the rank of the true positives in our link prediction
lists.Thisisusefulinformationasitshowswhichtechniquesare
better at ranking true positives higher than false positives and will
also punish techniques that more often return no positives at all.
Finally,weusetheareaundertheprecision-recallcurve(AUC)
asitgivesusaviewoftheperformanceofeachtechniquethatis
thresholdindependent.Asmostofourtechniquesneedathreshold
tomakepredictions,theperformanceofthesetechniquescanbeverysensitivetothevaluesusedfortheirthresholds.Anincorrectly
chosenthresholdcangivetheincorrectimpressionoftheusefulness
of a technique and, therefore, while we have attempted to selectthe best threshold for each technique, AUC gives us a general
measureoftheperformanceofthesetechniquesthatisnotaffected
by threshold values. We selected a precision-recall (PR) curve over
areceiveroperatingcharacteristics(ROC)curvebecausetheclasses
in our domain are unbalanced, there are many more negative links
than positive links, and PR curves exhibit better characteristics in
thissituation[ 8].Allscoresarepresentedasintegerpercentages
for the sake of readability.
Rompaeyetal.[ 32]alsomeasureapplicability,i.e.,theratioof
testsforwhichatleastonelinkisretrieved.However,becauseof
the normalisation that we apply, all non-binary techniques will
always produce at least one link, resulting in 100% applicability.
5.2 RQ1 (Method level):
How effective are our techniques at the method level? This research
questioninvestigateshoweffectiveeachofthetechniquesarefor
establishingtest-to-functionlinksusingonlymethodlevelinforma-tion.Toanswerthisquestion,wecomputetheevaluationmeasures
over the link sets produced using Equation 15.
Findings. From the results for RQ1, shown in Table 3, we see
that, on average, the combined score is the most desirable as itperforms best for MAP and AUC. This means that it is good at
rankingpredictions,isconsistentwhenchangingthresholds,and
could benefit from a further optimised threshold selection. For
precision alone, NC is the best, while LCS-B is best for recall.
5.3 RQ2 (Class Level):
How effective are our techniques at the class level? This research
questioninvestigateshoweffectiveeachofthetechniquesarefor
establishing test-class-to-class links, using only class level informa-tion.Toanswerthisquestion,wecomputetheevaluationmeasures
over the link sets produced using Equation 17.
Findings. FromtheresultsforRQ2,showninTable4,weseethat
theresultsaresimilartoRQ1:thecombinedscoreagainperforms
best for MAP and AUC, but this time is also joint best for recall
with LCS-B. For pure precision, NC wins again.
867Table 3: RQ1 â€“ Method level traceability.
Technique Prec. Recall MAP F1 AUCCommons IONC 100 57 9â€“
NCC 86 43 44 57 â€“
LCS-U 64 81 71 72 63LCS-B 54 8669 66 54
Leven 67 57 62 62 59LCBA 40 33 30 36 â€“Tarantula 53 64 64 58 48TFIDF 55 64 64 59 55Combined 69 81 75 75 67Commons LangNC 100 10 17 19 â€“
NCC 90 49 54 63 â€“LCS-U 78 69 79 73 76LCS-B 70 7979 75 74
Leven 84 53 69 65 73LCBA 83 68 63 75 â€“Tarantula 74 7982 77 78
TFIDF 82 74 79 78 82Combined 86 76 85 80 87JFreeChartNC 100 19 21 32 â€“
NCC 100 30 32 46 â€“
LCS-U 20 68 73 31 17LCS-B 33 78 76 47 58Leven 74 62 71 6852
LCBA 53 73 74 61 â€“Tarantula 33 78 76 47 42
TFIDF 53 76 74 62 54Combined 23 70 74 35 60AverageNC 100 11 15 20 â€“
NCC 92 40 43 55 â€“LCS-U 54 73 74 59 52LCS-B 53 8175 63 62
Leven 75 57 67 65 61LCBA 59 58 55 57 â€“Tarantula 53 74 74 60 56TFIDF 63 71 73 6664
Combined 59 76 786371
5.4 RQ3 (Elevated Method Level):
What effectiveness is achieved by utilising method level information
for class level traceability? This research question investigates how
eachofthetechniquesperformforestablishingtest-class-to-class
links when we use method level information that has been ele-
vatedtotheclasslevel.Toanswerthisquestion,wecomputethe
evaluation measures over the link sets produced using Equation 20.
Findings. FromtheresultsshowninTable5weseethatTFIDF
slightly beats the combined score for MAP, F1 score, and AUC. NC
wins again on precision and this time LCS-B is best for recall.
5.5 RQ4 (Augmented Method Level):
Canweimprovemethodlevelpredictionsbyaugmentingwithclass
level information?
This research question investigates if the method level traceabil-
ity performance can be improved by augmenting the method level
information withclass level information.To answer thisquestion,Table 4: RQ2 â€“ Class level traceability.
Technique Prec. Recall MAP F1 AUCApache AntNC 100 89 92 94 â€“
NCC 88 88 89 88 â€“LCS-U 65 86 86 74 68LCS-B 51 88 81 65 77Leven 87 83 87 85 77LCBA 50 72 62 59 â€“Tarantula 49 56 58 52 40TFIDF 51 56 58 54 42Combined 83 86 89 85 82Commons IONC 100 69 72 81 â€“
NCC 98 94 96 96 â€“LCS-U 70 94 91 80 89LCS-B 55 96 83 70 92Leven 100 94 97 97 96LCBA 51 75 71 61 â€“Tarantula 74 81 83 77 67TFIDF 74 81 83 77 68Combined 96 96 98 96 96Commons LangNC 100 75 82 86 â€“
NCC 95 86 93 91 â€“
LCS-U 77 86 90 81 78LCS-B 63 8888 74 76
Leven 95 86 93 91 85
LCBA 51 73 70 60 â€“Tarantula 50 63 66 56 41TFIDF 34 60 59 44 34Combined 90 86 9388 84JFreeChartNC 100 85 91 92â€“
NCC 73 8684 79 â€“
LCS-U 56 8679 68 62
LCS-B 58 8681 69 86
Leven 99 86 92 92 86
LCBA 31 82 67 45 â€“Tarantula 69 77 77 73 66TFIDF 67 77 78 72 66Combined 99 86 92 92 86AverageNC 100 80 84 88 â€“
NCC 88 88 90 88 â€“LCS-U 67 88 86 76 74LCS-B 57 8983 69 83
Leven 95 87 92 9186
LCBA 46 75 67 56 â€“Tarantula 60 69 71 64 54TFIDF 57 69 69 62 53Combined 92 89 93 9087
weselectthebestoveralltechniquefromthemethodlevel,thecom-
binedtechnique,andcompareitsperformancetotheaugmented
combinedtechniquefromthelinksetsproducedusingEquation22.
Findings. TheresultsforRQ4,showninTable6,showthat,on
average, it is better to use augmented information as precision, F1
score, and AUC are significantly better using augmented informa-
tion, while recall is only slightly worse.
868Table 5: RQ3 â€“ Class level using method level information.
Technique Prec. Recall MAP F1 AUCApache AntNC 71 31 32 43 â€“
NCC 59 38 37 46 â€“
LCS-U 53 69 69 60 53LCS-B 48 73 71 58 52Leven 64 64 67 64 54LCBA 70 70 71 70 â€“Tarantula 73 70 74 71 59TFIDF 75 75 77 75 65
Combined 64 73 75 69 62Commons IONC 86 38 39 52 â€“
NCC 90 56 57 69 â€“
LCS-U 80 83 82 82 79LCS-B 78 88 86 82 79
Leven 84 79 81 82 79LCBA 73 63 66 67 â€“Tarantula 82 77 79 80 77TFIDF 81 79 80 80 78Combined 85 85 86 85 80Commons LangNC 93 58 64 71 â€“
NCC 93 68 74 79 â€“
LCS-U 82 79 84 81 75LCS-B 77 8184 79 73
Leven 85 78 84 81 75LCBA 83 71 78 76 â€“Tarantula 82 75 81 79 71TFIDF 87 79 86 83 77
Combined 83 79 84 81 75JFreeChartNC 76 63 69 69 â€“
NCC 77 65 70 71 â€“LCS-U 62 75 74 68 59LCS-B 47 7772 58 57
Leven 70 65 69 68 58LCBA 81 76 7978 â€“
Tarantula 73 63 67 68 58TFIDF 82 7679 79 72
Combined 70 75 75 72 65AverageNC 82 47 51 59 â€“
NCC 80 57 60 66 â€“LCS-U 69 77 77 72 67LCS-B 62 8078 69 65
Leven 76 72 75 74 67LCBA 77 70 73 73 â€“Tarantula 77 72 75 74 66TFIDF 81 77 81 79 73
Combined 76 78 80 77 71
5.6 Parameter Value Selection
Ourapproachincludestunableparameters;athresholdvaluefor
each technique and the call depth discount factor, all of which are
real numbers. The current values for the thresholds and the call
depth discount have been established in a pre-study with a smaller
ground truth and a smaller set of projects. Based on the pre-study,
we selected thresholds that generalised well. We also observed
that a discount factor <=0.5 usually gives the highest F-score and
varyingthefactorbetween0and0 .5doesnotchangetheresults.Table 6: RQ4 â€“ Method level using multilevel information.
Technique Prec. Recall MAP F1 AUC
Commons IO
Unaugmented 69 81757567
Augmented 72 7979 75 74
Commons Lang
Unaugmented 86 76 85 80 87
Augmented 85 73 82 79 88
JFreeChart
Unaugmented 23 70 74 35 60
Augmented 65 70 74 68 68
Average
Unaugmented 59 76 78 63 71
Augmented 74 7478 74 77
Increasingthefactorabove0 .5hasonlyasmalleffectonrecalland
alargernegativeeffectonprecision,loweringtheF-scoreoverall.
Given these results, we selected a final discount factor of 0.5.
Weconsiderthecurrentthresholdstobesufficientlygoodand
general. However, one can observe that the results for a technique
can vary a lot between projects and a developer may want to vary
the thresholds specific to a project or rely on the ranking of candi-
date links instead.
5.7 Discussion
The results reveal some insights that allow us to draw conclusions
abouttherelativeeffectivenessofthetechniquesandthedifferences
betweentheprojects.First,wecomparethenamingconventions
techniques, NC and NCC. For RQ1 and RQ2, NC has perfect preci-
sion. This is expected as it is unlikely that a test and function will
sharethesamename,aftertheword testhasbeenremoved,without
beinglinked.Ho wever,this strictnessresultsinlowrecallforNCin
RQ1 and RQ3, where NCC ends up performing significantly better
forF1scoreandMAP,asittrades-offasmallamountofprecision
for much more recall. For RQ2, NC and NCC end up being even in
F1scoreasitiseasierfordeveloperstomaintaintraditionalnaming
conventions at the class level, explaining the good recall for NC
atthislevel.However,theirlowrecallonthemethodlevel(RQ1)
make them unsuitable for that level â€“ an observation also made
byMadejaetal.[ 23].WhencomparingLCS-UandLCS-B,wesee
that LCS-U usually performs better for F1 score and MAP but is
generallyequivalentorworseforAUC.Thissuggeststhateither
LCS-U is more sensitive to the threshold or that it may be possible
to find a better-optimised threshold value for LCS-B.
LCBA performs poorly in general for RQ1 and RQ2 but is espe-
ciallybadforCommonsIOinRQ1.Thisisanartefactofthenature
ofCommonsIO,wheretheeffectofmanyfunctioncallsistochangesomestate,ratherthanreturnthevalueofacomputation.Therefore,
the results of method calls are not as frequently testable by simply
comparingthereturnvaluetoanoracle;instead,afurtherfunction
call is required to check that the state was changed correctly. This
causes many false positives for LCBA. Commons Lang is the op-posite of Commons IO in this regard, as tested functions usually
have their return values checked against oracles immediately after
returning, resulting in a relatively high LCBA score.
869Overall,thetwoLCStechniquesandLevenshteinarethemost
consistently well-performing from the set of individual techniques,
butwhichoneperformsbestisprojectdependent.Thecombined
score, however, is consistently strong and is the best on average
forMAPandAUC.WherethecombinedtechniquestrugglesforF1
score,mostnotablyforJFreeChartinRQ1,thepoorperformance
isduetoadifferenceinoptimalthresholdsbetweenprojects;the
combined threshold is best at approximately 0 .95 for JFreeChart
and 0.80 (the selected value) for the other projects. MAP and AUC
confirm this as the differential in those results is much smaller and
they are the least affected by the threshold. This is true for MAP as
ittakesintoaccounttherankingsofthetruepositives,whichthe
combinedscoreconsistentlyrankshighly,evenifitalsoproduces
false positives at lower ranks due to a low threshold. AUC is inde-
pendent of the threshold entirely. The results confirm our intuition
that the benefit gained from combining the individual strengths of
thetechniquesoutweighsthenegativeeffectsofcombiningtheir
weaknesses, thus giving a better result overall. This intuition holds
eveninsituationswherespecifictechniquesareexpectedtobevery
strong, such as naming conventions at the class level.
Intermsofmultilevelinformation,RQ4showsthatmethodlevel
traceabilityperformanceisimprovedwhenaugmentedwithclass
level information. The AUC results show that this improvement is
significant but may require threshold optimisation to fully realise.
Finally, we gain some additional insights into the differences
between subjects by utilising the two categories of techniques,
naming-basedandstatisticalcall-basedtechniques(SCTs),topro-
vide a new interpretation of the results: We use the naming-based
techniques as a proxy for how well organised the test suites are,
the SCTs at the method level as a proxy for how coherent the tests
are, and the SCTs at the class level as a proxy for how cohesive the
testclassesare.Thisinterpretationofthenaming-basedtechniques
flows from the intuition that the better test suites are organised,
suchasbymaintainingsimpleone-to-onerelationshipsbetween
tests and units-under-test, the better the naming techniques will
perform. For the SCTs, this interpretation comes from the fact that
theyaremeasuresofhowmanydifferentunits-under-testarecalled
by an individual test unit and, thus, serve as a proxy for method
level coherence and class level cohesiveness. Using this interpre-
tation, we see that Commons Lang is the best organised and most
coherent at the method level, while Commons IO is the best organ-
ised and most cohesive at the class level. Commons Lang scores
poorlyfortheSCTsattheclasslevelbecausesomeofitstestclassesare large and contain many tests. Therefore, these test classes have
lots of calls to non-tested classes, introducing noise.
We attempted to compare our results to results from previous
work.However,theonlytwopreviousworksonmethodlevel[ 3,17]
suggestallcalledmethodsinatest,leadingtoverylowprecision.
On the class level, we can compare our results as we have (in part)
reimplemented suggested approaches, namely NC and LCBA. Our
resultsaresimilartoRompaeyetal.â€™s[ 32],butdirectcomparisonis
not possible astheir ground truth is notavailable. Moreover, their
techniques do not provide any ranking over recommended links.
They also evaluate combined techniques, but as their ground truth
has 100% precision and recall for NC, all combinations result in
loweraccuracy.Incomparison,ourresultsshowthatacombination
of techniques outperforms individual techniques.Previous work that is based on similarity between tests and
units-under-test [ 6,7,20] use the NC results as a ground truth
andthereforecannotbedirectlycomparedtoourstudy,however,
their precision and recall values are lower than the ones from our
class-level combined approach.
As shown in Figure 1, our approach can be easily integrated
intothesoftwaredevelopmentprocess.TCagentisinjectedinto
the JUnit framework to collect the necessary data which is thenanalysed by TCtracer at the end of a JUnit run to generate the
test-to-codetraceabilitylinkswhicharereadytobeused.TCagent
and TCtracer can be used inside the IDE via a framework likeEzUnit [
3], allowing a developer to navigate between tests and
tested code quickly. TCtracer is also easy to integrate into a stan-
dard continuous integration process [ 30]. This integration is made
simplebythefactthatTCagentinstrumentstheJUnittestsuite
and,therefore,thegatheringofdynamictraceinformationhappens
automatically during the testing stage. All that remains is to add
anextrastepthatexecutesTCtracer.Theadditionofthisstepis
easyinmostmoderncontinuousintegrationframeworkssuchas
TravisCI [ 4]and Jenkins[ 18].The gatheredtraceability linkscan
then be used to backtrack from executed tests to the tested code
orviceversa.Moreover,thetraceabilitylinksareconstantlykept
up-to-dateaspartofthecontinuousintegrationpipelineandarereadily available. For example, a developer can change code and
corresponding tests at the same time, ensuring their co-evolution.
Inaddition,furtheranalysisoftheproducedlinkscanbeperformedaspartofthecontinuousintegrationprocess,suchasautomaticallyalertingdeveloperswhenafunctionhasnotestsevenifitiscovered
(executed) during testing. Therefore, using TCtracer to automate
test-to-codetraceabilitylinkcapturethroughcontinuousintegra-
tion can provide multiple benefits and could be especially useful
insafety-criticalsystemsthataresubjecttoregulationsrequiring
that traceability links are maintained [5].
5.8 Threats to Validity
Themainthreatstovaliditycomefromthesubjectsandtheground
truth.Firstly,therepresentativenessofthesubjectsisanexternal
threattovalidityaswehavenoclearevidenceastohowrepresenta-tivethesesubjectsareofthegeneralpopulationofsoftwareprojects.
However, the subjects that we have selected are widely used in re-
searchandbypractitionersandarelargeenoughtodemonstrate
theapplicabilityofourapproachtonon-trivialsystems.Thesec-
ond threat comes from the method by which the ground truth was
established. The use of manual investigation for establishing the
groundtruthposesaninternalthreattovalidityasthereisroom
forinterpretationwhendeterminingwhichfunctionsorclassesare
testedbyatest.However,alljudgementswerevalidatedbymore
than one judge. For the method level ground truth, three judges
wereusedandfullinter-rateragreementwasachieved.Attheclass
level, the majority of links were provided by the developers and
verifiedbyajudge,andasmallnumberoflinks(12)werecreated
by two judges, again with full inter-rater agreement. As we are
using somedeveloper created links,there is potentialfor a biasto
beintroducedduetotheselectionofclassesthatwereannotated
by the developers. While a manual inspection does not reveal anyobvious bias, the existence of one cannot be ruled out.
870As with any approach using a threshold, the results are based
on the set thresholds. While we attempted to choose good gen-
eralthresholds,differentthresholdsmayleadtodifferentresults,
observations, and conclusions.
Finally,thereisathreattogeneralisabilityasourexperiments
onlycoverJavaprojectsthatusetheJUnitframeworkandwedo
notknowhowrepresentativeourchosenprojectsare.Therefore,
wedonothavedirectevidencethatthisapproachwouldapplyto
otherlanguagesortestingframeworks.However,inourestimation,
thereisnothinginherentinourapproachthatwouldpreventthe
application of the TCtracer approach in other scenarios.
6 RELATED WORK
Establishingandmaintainingtraceabilitylinksbetweentestsand
theirtestedfunctionalityhasreceivedsignificantattentionastrace-abilitylinkshavemultipleapplicationsinthesoftwareengineering
process: determining which test cases need to be rerun after a
changehasbeenmade,maintainingconsistencyduringrefactoring,
andprovidingaformofdocumentation.Test-to-codetraceability
can,forexample,helptolocatethefaultthatcausesatestcasetofail.Qusefetal.[
29]describethesebenefitsindetailandPariziet
al.[27]presentsanoverviewoftheachievementsandchallenges
of test-to-code traceability. Prior research has investigated the use
of gamification to improve manual maintenance of traceability
links [25, 26] but this approach has not seen significant adoption.
Atthemethodlevel,EzUnit[ 3]isaframeworkthatallowsde-
veloperstoannotatetestswithlinkstothemethod-under-test.To
doso,itperformsastaticanalysisandidentifiesthemethodscalledby a test which are suggested for annotation. EzUnit highlights the
linkedmethodswhenanerrorinthetestoccurs.Asimilartoolis
TestNForce [ 17] which links tests to methods-under-test. Like our
approach,tracingisusedtoidentifythemethodsthatarecalledbyatest.Nofurtherfilteringisdoneandtheirapproachwillthusincludea large number ofutility methodsleading tolowprecision. Ghafari
et al. [16] also work at the method level where they break down
testcasesintosub-scenariosforwhichtheyattempttoestablishthe
tested function, termed the focal method. This is done using static
data flow analysis. The results for this technique are promising,
however, twoof thefour subjects usedfor the evaluationare very
small(130and43tests),whiletheothertwoarestillsmallerthan
our smallest subject. As it is easier to achieve higher precision and
recall on smaller projects, due to fewer candidate links, the results
cannot be directly compared to those presented in this paper.
SCOTCH+ (Sourcecode andConcept basedTestto Codetrace-
ability Hunter) is a traceability system introduced by Qusef et
al. [29] that achieves better accuracy and provides more benefit to
developers than LCBA or NC [ 28]. SCOTCH+ applies dynamic slic-
ing to identify a set of candidate tested classes which it then filters
using a textual coupling analysis called Close Coupling between
Classes (CCBC) and name similarity (NS) scores.
Other test-to-code traceability work is based on the assumption
thatatestshouldbesimilartoatestedunit.Kicsietal.[ 20]explore
the usage of Latent Semantic Indexing (LSI) over source code to
establishtraceabilitylinksbetweentestclassesandtestedclasses.
They extract a ground truth from five open source systems by
extracting only the links between test classes and tested classesthatfollow(exact)namingconventions.Theyreportthattheground
truth link is ranked top between 30% and 62% and is present in the
top5between57%and89%,suggestingalowrecall(precisionisnot
investigated). Csuvik et al. [ 7] replaced LSI with word embeddings
withinthesameapproachandreportbetterprecisionwhenusing
word embeddings(no investigationof recallhas beendone). They
also compare LSI, word embeddings and TF-IDF [ 6] in the same
way and report that word embeddings perform best in terms of
precision and recall.
While test-to-code traceability based on name similarity has
good accuracy on the class level as developers usually follow nam-
ing conventions for the test classes, on the method level thereexist various guidelines on how to name a test method. Madeja
etal.[23]investigated5popularAndroidprojectsandfoundthat
only49%oftestscontainthefullnameofthemethod-under-test
inthetestnameandthat76%oftestscontainapartialnameofa
method-under-test in the test name.
Closest to our work is the work by Rompaey and Demeyer [ 32]
who investigate six traceability techniques to link test classes to
classes-under-test over three projects from which they extracted a
groundtruthof59links.Theyreportperfectprecisionandrecall
fortheuseofnamingconventions,butreportverylowprecision
andrecallforusingsimilarity(LSI)betweentestclassesandclasses-
under-test.RompaeyandDemeyerinvestigatemostlystatictech-
niques and only use tracing to establish LCBA. While they only
investigateontheclasslevel,weinvestigatedynamictechniques
on the class and the methodlevel over much largerground truths.
Gergely et al. [ 14] do not extract links between units directly,
butinstead,useclustering.Theclusteringisdonewithstatic(pack-
agingstructure) anddynamic(coverage)analysis. Thetwosets of
traceability clusters are compared and the differences are manually
analysed for produce the final traceability links.
StÃ¥hl et al. [ 31] focuses on the deployment of traceability into
continuousintegrationanddeliverysystems.Aspartofthiswork
they present an investigation into existing needs and practices and
propose a unified framework for integrating traceability establish-
mentintocontinuousintegrationsystems.Theinvestigationinto
existingpracticesshowedthatthereisastrongdesireamongde-
velopers for the integration of automated traceability handing into
buildsystemswhichis,inlargepart,currentlynotbeingfulfilled.
This demonstrates the demand for tools such as TCtracer.
7 CONCLUSION
Inthispaper,wehavepresentedTCtracer,anapproachandim-
plementation for establishing test-to-code traceability links at both
themethodlevelandclasslevel.TCtracerutilisesawiderangeof new and existing test-to-code traceability link establishment
techniquesandenhancesthembycombiningthemandapplying
themtoboththemethodlevelandclasslevel,makingTCtracer
the first approach that establishes two types of links and utilises a
cross-levelinformationflow.Anempiricalevaluationof TCtracer
was conducted, at both the method level and class level, with four
real-world open source projects. The results show that, on average,
TCtracer is more effective at both the method level and the class
levelthananysingleexistingtechnique.ThismakesTCtracerthe
most effective approach for test-to-code traceability to date.
871REFERENCES
[1]Miltiadis Allamanis, Earl T Barr, Premkumar Devanbu, and Charles Sutton. 2018.
A survey of machine learning for big code and naturalness. ACM Computing
Surveys (CSUR) 51, 4 (2018), 81.
[2]Giuliano Antoniol, Gerardo Canfora, Gerardo Casazza, Andrea De Lucia, and Et-
toreMerlo.2002. Recoveringtraceabilitylinksbetweencodeanddocumentation.
IEEE Transactions on Software Engineering 28, 10 (2002), 970â€“983.
[3]PhilippBouillon,JensKrinke,NilsMeyer,andFriedrichSteimann.2007. EZUNIT:
A Framework for Associating Failed Unit Tests with Potential Programming
Errors. In Agile Processes in Software Engineering and Extreme Programming.
Springer Berlin / Heidelberg. https://doi.org/10.1007/978-3-540-73101-6_14
[4] Travis CI. [n.d.]. Travis CI. Retrieved 2020-01-13 from https://travis-ci.org/
[5]JaneCleland-Huang.2012. Traceabilityinagileprojects. In SoftwareandSystems
Traceability. Springer, 265â€“275.
[6]Viktor Csuvik, AndrÃ¡s Kicsi, and LÃ¡szlÃ³ VidÃ¡cs. 2019. Evaluation of Textual
Similarity Techniques in Code Level Traceability. In International Conference on
Computational Science and Its Applications. Springer, 529â€“543.
[7]ViktorCsuvik,AndrÃ¡sKicsi,andLÃ¡szlÃ³VidÃ¡cs.2019. SourceCodeLevelWord
Embeddings in Aiding Semantic Test-to-Code Traceability. In 10th International
Workshop on Software and Systems Traceability. 29â€”-36. https://doi.org/10.1109/
SST.2019.00016
[8]Jesse Davis and Mark Goadrich. 2006. The relationship between Precision-Recall
and ROC curves. In Proceedings of the 23rd International Conference on Machine
Learning. ACM, 233â€“240.
[9]Andrea De Lucia, Fausto Fasano, and Rocco Oliveto. 2008. Traceability man-
agement for impact analysis. In 2008 Frontiers of Software Maintenance. IEEE,
21â€“30.
[10]TheApacheSoftwareFoundation.[n.d.]. ApacheAnt. Retrieved2019-08-13from
https://ant.apache.org/
[11]The Apache Software Foundation. [n.d.]. Apache Commons Collections.
Retrieved Accessed: 2018-07-30 from https://commons.apache.org/proper/
commons-collections/
[12] The Apache SoftwareFoundation. [n.d.]. Apache CommonsIO. Retrieved2019-
08-18 from https://commons.apache.org/proper/commons-io/
[13]The Apache Software Foundation. [n.d.]. Apache Commons Lang. Retrieved
2018-07-30 from https://commons.apache.org/proper/commons-lang/
[14]TamÃ¡sGergely,GergÅ‘Balogh,FerencHorvÃ¡th,BÃ©laVancsics,ÃrpÃ¡dBeszÃ©des,
and Tibor GyimÃ³thy. 2019. Differences between a static and a dynamic test-to-code traceability recovery method. Software Quality Journal 27, 2 (2019),
797â€“822.
[15]Malcom Gethers, Rocco Oliveto, Denys Poshyvanyk, and Andrea De Lucia. 2011.
On integrating orthogonal information retrieval methods to improve traceability
recovery. In 27thIEEEInternationalConferenceonSoftwareMaintenance(ICSM).
IEEE, 133â€“142.
[16]MohammadGhafari,CarloGhezzi,andKonstantinRubinov.2015. Automatically
identifying focal methods under test in unit test cases. In 15th International
Working Conference on Source Code Analysis and Manipulation (SCAM). IEEE,
61â€“70. https://doi.org/10.1109/SCAM.2015.7335402
[17]Victor Hurdugaci and Andy Zaidman. 2012. Aiding software developers to
maintain developer tests. In 16th European Conference on Software Maintenance
and Reengineering. IEEE, 11â€“20.
[18] Jenkins. [n.d.]. Jenkins. Retrieved 2020-01-13 from https://jenkins.io/
[19]JamesAJones,MaryJeanHarrold,andJohnStasko.2002. Visualizationoftest
information to assist fault localization. In Proceedings of the 24th International
Conference on Software Engineering. ICSE 2002. IEEE, 467â€“477.
[20]AndrÃ¡s Kicsi, LÃ¡szlÃ³ TÃ³th, and LÃ¡szlÃ³ VidÃ¡cs. 2018. Exploring the benefits of
utilizing conceptual information in test-to-code traceability. In 6th International
Workshop on Realizing Artificial Intelligence Synergies in Software Engineering.
8â€“14. https://doi.org/10.1145/3194104.3194106
[21]Vladimir I Levenshtein. 1966. Binary codes capable of correcting deletions,
insertions, and reversals. In Soviet physics doklady, Vol. 10. 707â€“710.
[22]Object Refinery Limited. [n.d.]. JFreeChart. Retrieved 2018-07-30 from http:
//www.jfree.org/jfreechart/
[23]Matej Madeja and Jaroslav PorubÃ¤n. 2019. Tracing Naming Semantics in Unit
TestsofPopularGithubAndroidProjects.In 8thSymposiumonLanguages,Appli-
cationsandTechnologies(SLATE2019).SchlossDagstuhlâ€“Leibniz-Zentrumfuer
Informatik. https://doi.org/10.4230/OASIcs.SLATE.2019.3
[24]ChristopherManning,PrabhakarRaghavan,andHinrichSchÃ¼tze.2010. Intro-
duction to information retrieval. Natural Language Engineering 16, 1 (2010),
100â€“103.
[25]Reza Meimandi Parizi, Asem Kasem, and Azween Abdullah. 2015. TowardsGamification in Software Traceability: Between Test and Code Artifacts. InProceedings of the 10th International Conference on Software Engineering andApplications. SCITEPRESS - Science and Technology Publications, 393â€“400.
https://doi.org/10.5220/0005555503930400
[26]RezaMeimandiParizi.2016. Onthegamificationofhuman-centrictraceability
tasks in software testing and coding. In IEEE 14th International Conference onSoftware Engineering Research, Management and Applications (SERA). IEEE, 193â€“
200. https://doi.org/10.1109/SERA.2016.7516146
[27]RezaMeimandiParizi,SaiPeckLee,andMohammadDabbagh.2014. Achieve-
ments and Challenges in State-of-the-Art Software Traceability Between Testand Code Artifacts. IEEE Transactions on Reliability 63, 4 (dec 2014), 913â€“926.
https://doi.org/10.1109/TR.2014.2338254
[28]Abdallah Qusef, Gabriele Bavota, Rocco Oliveto, Andrea De Lucia, and DavidBinkley. 2013. Evaluating test-to-code traceability recovery methods throughcontrolled experiments. Journal of Software: Evolution and Process 25, 11 (Nov
2013), 1167â€“1191. https://doi.org/10.1002/smr.1573
[29]A.Qusef,G.Bavota,R.Oliveto,A.DeLucia,andD.Binkley.2014. Recoveringtest-to-code traceability using slicing and textual analysis. Journal of Systems
and Software 88 (2014), 147â€“168.
[30]Mojtaba Shahin, Muhammad Ali Babar, and Liming Zhu. 2017. Continuous
integration, delivery and deployment: a systematic review on approaches, tools,
challenges and practices. IEEE Access 5 (2017), 3909â€“3943.
[31]Daniel StÃ¥hl, Kristofer HallÃ©n, and Jan Bosch. 2017. Achieving traceability in
largescalecontinuousintegrationanddeliverydeployment,usageandvalidation
of the eiffel framework. Empirical Software Engineering 22, 3 (2017), 967â€“995.
[32]Bart Van Rompaey and Serge Demeyer. 2009. Establishing traceability linksbetween unit test cases and units under test. In 13th European Conference on
Software Maintenance and Reengineering. IEEE, 209â€“218.
[33]RobertWhiteandJensKrinke.2020. TCTracerEvaluationDataâ€“International
Conference on Software Engineering 2020 [Data set]. https://doi.org/10.5281/
zenodo.3637597
[34]StefanWinklerandJensvonPilgrim.2010. Asurveyoftraceabilityinrequire-
mentsengineeringandmodel-drivendevelopment. Software&SystemsModeling
9, 4 (2010), 529â€“565.
[35]Rafael Winterhalter. [n.d.]. Byte Buddy. Retrieved 2019-08-19 from https://
bytebuddy.net
872