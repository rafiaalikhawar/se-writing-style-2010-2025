Search-BasedAdversarial Testing and Improvementof
Constrained Credit Scoring Systems
Salah Ghamizi
Universityof Luxembourg
Luxembourg
salah.ghamizi@uni.luMaximeCordy
Universityof Luxembourg
Luxembourg
maxime.cordy@uni.luMartinGubri
Universityof Luxembourg
Luxembourg
martin.gubri@uni.lu
Mike Papadakis
Universityof Luxembourg
Luxembourg
michail.papadakis@uni.luAndreyBoystov
Universityof Luxembourg
Luxembourg
andrey.boystov@uni.luYves Le Traon
Universityof Luxembourg
Luxembourg
yves.letraon@uni.lu
Anne Goujon
BGLBNP Parisbas
Luxembourg
anne.goujon@bgl.lu
ABSTRACT
CreditscoringsystemsarecriticalFinTechapplicationsthatcon-
cerntheanalysisofthecreditworthinessofapersonororganization.
Whiledecisionswerepreviouslybasedonhumanexpertise,they
arenowincreasinglyrelyingondataanalysisandmachinelearning.
In this paper, we assess the ability of state-of-the-art adversarial
machine learning to craft attacks on a real-world credit scoring
system. Interestingly, we find that, while these techniques can gen-
eratelargenumbersofadversarialdata,thesearepracticallyuseless
as they all violate domain-specific constraints.In other words, the
generated examples are all false positives as they cannot occur
inpractice.Tocircumventthislimitation,weproposeCoEvA2,a
search-basedmethodthatgeneratesvalidadversarialexamples(sat-
isfying the domain constraints). CoEvA2 utilizes multi-objective
searchinorder to simultaneouslyhandle constraints, perform the
attack and maximize the overdraft amount requested. We evalu-
ateCoEvA2onamajorbankâ€™sreal-worldsystembycheckingits
abilityto craft valid attacks. CoEvA2 generates thousandsofvalid
adversarial examples, revealing a high risk for the banking system.
Fortunately, by improving the system through adversarial training
(basedontheproduced examples),we increaseitsrobustnessand
make our attackfail.
CCS CONCEPTS
Â·Appliedcomputing â†’Onlinebanking ;Â·Computingmethod-
ologiesâ†’Machine learning .
Permissionto make digitalor hard copies of allorpart ofthis work for personalor
classroom use is granted without fee provided that copies are not made or distributed
forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation
onthefirstpage.Copyrights forcomponentsofthisworkownedbyothersthanthe
author(s)mustbehonored.Abstractingwithcreditispermitted.Tocopyotherwise,or
republish,topostonserversortoredistributetolists,requirespriorspecificpermission
and/or a fee. Request permissions from permissions@acm.org.
ESEC/FSE â€™20, November 8Å›13, 2020, Virtual Event, USA
Â©2020 Copyright heldby the owner/author(s). Publicationrightslicensed to ACM.
ACM ISBN 978-1-4503-7043-1/20/11.
https://doi.org/10.1145/3368089.3409739KEYWORDS
Search-based, Adversarial attacks, FinTech, Random Forest, Credit
Scoring
ACMReference Format:
Salah Ghamizi, Maxime Cordy, Martin Gubri, Mike Papadakis, Andrey
Boystov,YvesLeTraon,andAnneGoujon.2020.Search-BasedAdversar-
ial Testing and Improvement of Constrained Credit Scoring Systems. In
Proceedings of the 28th ACM Joint European Software Engineering Confer-
ence and Symposium on the Foundations of Software Engineering (ESEC/FSE
â€™20), November 8Å›13, 2020, Virtual Event, USA. ACM, New York, NY, USA,
12pages.https://doi.org/10.1145/3368089.3409739
1 INTRODUCTION
The banking industry increasingly relies on machine learning to
support decision making based on customersâ€™ historical data. One
prominentapplicationcaseis creditscoring ,i.e.,Å‚asetofdecision
modelsandtheirunderlyingtechniquesthataidcreditlendersin
the grantingof creditÅ¾[ 25]. Bylearning fromhistory Å›credit cases
and their outcomes (whether the credit was returned in time) Å›
supervised models can automate the approval and rejection of new
creditrequests withlimitedhuman intervention.
Ourindustrialpartner,theDataScienceLabofBGLBNPParibas
Luxembourg (henceforth referred to as Å‚BGL BNP ParibasÅ¾) has
recently engineered such a credit scoring system. Their system
deals with the approval of overdraft requests, which occur when a
transactioncausesthebalanceoftheaccounttodropbelowzero.
Then, it is up to the bank employees to allow or reject this transac-
tion. BGL BNP Paribas implemented an automated system relying
on random forests. That is, the approval of overdraft requests is
seen as a binary classification problem (approved or rejected). The
system approves or rejects overdrafts automatically, based on data
abouttherequestedtransactionandthecustomersâ€™history.Ifthe
overdraft is rejected by the system, an expert re-analyzes the re-
questandmayoverrulethedecision.Ifitisaccepted,thesystem
latercheckswhether the overdrafthas been reimbursedintime.
1089
ESEC/FSE â€™20, November8â€“13,2020,VirtualEvent, USA S.Ghamizi, M. Cordy, M. Gubri, M. Papadakis,A.Boystov, Y. Le Traon,andA.Goujon
The first challenges faced by our partner were feature engineer-
ingandmodelselection.Asthesehavebeenwidelyresearched(see,
e.g.,[8,11,23]),theybenefitedfromtheavailablebodyofknowl-
edge and techniques tobuild a quality system that achieved a test
accuracyof80%.Asofnow,thissystemhasprocessedmorethan
400,000 overdraftrequests over aspan of30 months.
Yet,thestringentsecurityrequirementsforceduponthebanking
sector oblige them to protect their credit scoring system against
maliciousthirdparties.Inourpartnerâ€™scontext,thethreatliesin
the capability of the third-party to modify the requested credits
and the profile of customers to make the system accept overdrafts
that itshould have rejected.
Inmachinelearning,suchmaliciousinputsarecalled adversarial
examples andarecraftedbyalteringbenigninputsinsuchaway
thattheyfooltheclassificationsystem.Adversarialexamplesare
mainlystudiedinthecontextofcomputervisionanddeepneural
networks[ 1,6,24],whereelusivealterationstothepixelsofimages
cause misclassifications. Such research has shown that adversarial
examples can be crafted by a systematicprocedure Å› the adversar-
ialattack Å›whichtypicallyutilizesinformationabouttheneural
networkâ€™sgradientstofindtheslightestperturbationthatwould
changethe outputclass.
Interestingly,theapplicationofadversarialattackstoFinTech
andrandomforestsremainslargelyunexplored[ 20].Thisissurpris-
inggiventhewidespreaduseofthesetechniquesinindustrialappli-
cations.Toourknowledge,thestate-of-the-artattackforrandom
forest classification algorithms is the one designed by Papernot et
al.[21].Itconsistsofastochasticprocedurethatvisitsandattempts
tofliptheindividualdecisionnodesoftheforestâ€™streesuntilthe
classificationoutcomeischanged.Analternativeapproachcould
betobuiltaÅ‚surrogateÅ¾deepneuralnetwork(usingthetraining
data), based on which we could apply a prominent gradient-based
attack (with the hope that this attack will be transferable to the
random forestmodel).
Nevertheless, all adversarial attack techniques lean on the in-
ternal computationsof theclassification models anddisregardthe
factthatalteringtheoriginalinputmayproducefalsepositives,i.e.,
infeasible intherealworld,or invalidforthesoftwaresysteminputs
that are acceptable by the classification model. While this phenom-
enon is less likely to occur in image recognition, where slightly
altering an image can easily produce a valid image, application
domains such as FinTech are subject to hard domain constraints
delimiting the set of valid inputs. For instance, a credit scoring
systemrelies onfinancialinformation suchascustomersâ€™ account
balance,contractedcredits,monthlyincome,andindebtmentrate.
Such data are naturally constrained (e.g., income is positive), in-
terdependent (indebtment rate depends oncontracted creditsand
monthlyincome)orbounded(e.g.,themaximumoverdraftamount
authorizedbythebank).Thus,anysuccessfulattackshouldrespect
thesedomainconstraintsandproduce examples that satisfy them.
Moreover,weconductexperimentswiththecurrentstate-of-the-
art,i.e.,thePapernotattack,onourpartnerâ€™ssystem.1Interestingly,
we show that while the attack successfully generated adversar-
ial examples that flipped the classification results for 75% of the
cases (its grosssuccess rate), none of them satisfied the domain
1Wereport onthese experimentsin Section 5.constraints.Thismeansthattheattackhasan actualsuccessrateof
0%.Theseresultsindicatethatstate-of-the-artadversarialattacks
cannotgeneratedomain-constrainedtest inputs.
Dealing with domain constraints is a recurrent problem in soft-
wareengineering[ 2].Inthecaseofgeneratingadversarialexamples,
one cannot handle/satisfy the domain constraints independently
oftheattack technique.The issueis that ontopoftheconstraints
(manyofwhichareimposedbyothersystems/components),one
needsto craftthe attacks and fulfil some additional objectives(e.g.
causemisclassification,maximizetheoverdraftamount).Therefore,
reducing the problem to constraintsatisfactionisnot enough.
To dealwith this issue, we propose a search-based method that
generates constrained adversarial examples for banking applica-
tions.Weformulatethegenerationofadversarialexamplessatis-
fyingthedomainconstraintsasamulti-objectivesearchproblem
and show that search-based techniques offer suitable solutions.
Ourmethod,calledConstrainedEvolutionaryAdversarialAttack
(CoEvA2),operatesinagrey-boxway;itreliesonthefeaturerepre-
sentationoftheinputsbutisindependentoftheinternalparameters
ofthe classification model.
WeapplyCoEvA2toBGLBNPParibasâ€™screditscoringsystem
and show that it can generate thirteen thousand of valid adver-
sarialexamplesfrom8 .45%oftherealoverdrafts.Thisdrastically
improvesoverstate-of-the-artadversarialattacks,whichfailedcom-
pletely.Then,weshowthatwecanmakeourpartnerâ€™ssystemmore
robust by performing adversarial training (i.e., retrain the model
using the produced adversarial examples). After such training, the
systemresiststo our attack(appliedundersimilar conditions).
In summary,the contributionsof this paper are:
â€¢Wedemonstratetheneedfordomain-constrainedadversarial
attacktechniquesforindustrialfinancialsystems.Wealso
show that existing attacks are inapplicable to real-world
creditscoringsystems,such as the one of our partner.
â€¢WedevelopCoEvA2,anewadversarialattackmethod(for
random forest applications) based on multi-objective search.
Givenaclassificationmodelanddomainconstraints,CoEvA2
effectivelygeneratesvalid adversarialexamples.
â€¢WeevaluateCoEvA2onourpartnerâ€™ssystemandempirically
show that it can craft adversarial examples with an actual
successrateof8.45%, leading to thousandsof examples.
â€¢Wedemonstratethatourmethodhelpstoimprovethesys-
temâ€™s robustness (to adversarial attacks). Indeed, retraining
the system on adversarial examples results in improving its
robustnesssignificantly.
2 RELATED WORK
2.1 Credit Scoring
ThestudyofLouzadaetal.[ 15]presentsacomprehensivesurveyof
classification methods in the context of credit scoring automation.
Whilefocusingonclassificationmodels,Louzadaetal.alsoreported
thedifferentproblemstackledbythesurveyedpapers,withnone
ofthembeen relatedto modelrobustnessoradversarialattacks.
Much research has been conducted on feature engineering and
modelselectionforcreditscoring.Forexample,DeMeloandBanzhaf
[8]combinedKaizenprogrammingandlogisticregressiontofind
thebestnon-linearcombinationoffeatures.Saiaetal.[ 23]proposed
1090Search-BasedAdversarialTestingandImprovement of ConstrainedCreditScoringSystems ESEC/FSE â€™20, November8â€“13,2020,VirtualEvent, USA
a wavelet-based feature engineering method and evaluated its per-
formanceusingmultipletypesofmodels.Fengetal.[ 11]proposeda
featureselectionapproachbasedonfiltersandanovelindexnamed
new separation degree . These techniques are orthogonal to ours as
they donot target adversarialattacksorrobustness.
2.2 AdversarialExamples
Adversarialexamples were firstmentionedby the studies of Biggio
et al. [5] and Szegedy et al. [ 24] in the context of deep neural
networksforimageclassification.Theirintriguingpropertyresides
inthe small perturbations neededto changethe predictedlabel.
AccordingtoBiggioetal.[ 5]white-boxattacksassumeperfect
knowledge on the model, its parameters, training set, and features.
Grey-boxattacksusesomeknowledgeaboutthetargetedsystem
but assume another part to be unknown. Black-box attacks rely
on the (raw) input space and the systemâ€™s outputs to generate
adversarialexamples.
In the recent years, the ever-increasing literature has studied
adversarial examples, mainly for computer vision (e.g., [ 1,6]) with
applications to autonomous cars and facial recognition and, to a
lesserextent,naturallanguageprocessing[ 4]andsoftwaresecurity
[6,13].
Papernot et al. [ 20] mention the potential threats of adversar-
ial examples for financial fraud detection. Yet, to the best of our
knowledge, there exists no prior work applying adversarial attacks
to industrialsystemsfrom the financialdomain.
Inanotherpaper[ 21],Papernotetal.presentanattacktodeci-
siontrees.Whilethisattackisstraightforwardtoextendtorandom
forests,itdoesnotsupportdomainconstraints.Asweshowlater,
thismakesitincapableofgenerating adversarialexamplessatisfy-
ingthe constraints.
Kantchelianetal.[ 14]havealsoproposedanotherrandomforest
attack.Ittransformsthedecisionnodestoformula,formingthem
as misclassification objective, and uses a SAT solver to generate
solutions.Thus,anysolutioncorrespondstoanadversarialexample.
While this attack can theoretically solve the problem of generating
constrainedadversarialexamples(byaddingconstraintsintothe
formula), in practice,it faces scalability issuesdue to the inherent
problems andlimitationsofthe SATsolvers.
Indeed,weconductedanexploratoryexperimentbasedonthe
HELOCdataset2whichhashalfthenumberoffeaturescomparedto
our partnerâ€™s dataset and simple constraints involving at most two
features. After 20 hours, the Kantchelian attack could not generate
any adversarialexample satisfyingthe constraints.
Ourmethodovercomesthelimitationsofstate-of-the-artattacks
and designs a search-based (evolutionary) algorithm to generate
adversarial examples that cause misclassification and satisfy the
domainconstraintswhileminimizingtheperturbationandmaxi-
mizing the business impact (i.e.,the acceptedoverdraftamount).
Theideaofusingsearch-basedalgorithmstoperformadversarial
attackisnotnew.Alzantotetal.[ 3]haveproposedablack-boxattack
on image recognition models (viz. deep neural networks). Being
focused on images, the problem they tackle is different and does
not involve domainconstraints.
2https://community.fico.com/s/explainable-machine-learning-challenge2.3 Constrained TestGeneration
The problem of generating test inputs under domain constraints
is not new [ 18] and was tackled by several works in the context
oftraditional (code-based)software, aswitnessed by the survey of
McMinn [ 17]. More recently, Ali et al. [ 2] evaluate different search-
basedmethodsingeneratingtestinputssatisfyingOCLconstraints.
InthecontextofCombinatorialInteractionTesting(CIT),Garvin
et al [12] propose to reorganize the search space of metaheuristics
to reflect the structure of the CIT problems and their inherent
constraints.Comparedtosuchworks,thenoveltyofourresearchis
that it targets machine learning systems under adversarial settings.
Analternativetomulti-objectiveGAsearchwouldrequirethe
useofaSATsolvertofindallvalidconfigurationsthenchoosing
the optimal ones with regards to the other objectives. However,
thecombinationofbothsearchspacesandthecomplexityofthe
constraintsmake itcomputationallyexpensive.
3 INDUSTRIALCREDITSCORINGSYSTEM
3.1 Process andDatasets
When a customer initiates, through any channel, a transaction
whose amount exceeds the customerâ€™s account balance, the pay-
mentengineasksthecreditscoringsystem(CSS)forpermission.
TheCSSexaminesthecustomerâ€™sprofileandeitherapprovesthe
credit overdraft or it suggests the operator reject the request. In
thelattercase,theoperatorcanfollowthesuggestionoftheCSS
oroverrule itandacceptthe request.
To make informed decisions, the CSS pulls information from
adozensources.Inadditiontobasicfeatureslikethetransaction
amount and the customerâ€™s current balance, much information
aboutthecustomerâ€™shistoryisconsolidated.Intheend,anoverdraft
request isrepresentedas avector of 46 features.
Afterapproving an overdraft request, the bank expects the cus-
tomertoreturnthecreditinduetime.Incasethecustomerdoes
notdoso,thebankconsidersthatitwaswrongtoallowtheover-
draft; otherwise it considers that it was correct. Through this post-
analysis, we can associate each approved overdraft credit with a
binarylabel(trueorfalse).Suchlabelsformthe groundtruth and
are used to assess the accuracy of the CSS. A similar process is
usedforrejectedoverdraftandanalyzes,basedonthecustomerâ€™s
future transactions, whetherthe overdraft credit would have been
returnedintime should ithave been approved.
Overall,theCSSdatasetcomprises400,000overdraftcreditre-
questswiththeirassociatedlabel,outofwhich275,000areusedfor
training and125,000 for testing.
3.2 Model RequirementsandCharacteristics
Therationalebehindourpartnerâ€™sprojectistoreducehumanin-
terventioninoverdraftapprovalbyautomaticallyapprovingsafe
overdraft requests (sending only rejected overdraft to human ex-
perts) while minimizing the acceptance of risky overdrafts (e.g.
transactions of large amount). Our partner also expects the system
torunonline,inreal-time,andefficientlysothatitdoesnotcom-
promise the efficiency of the other services. Finally, the selected
modelshouldbe interpretable ,asexplaininghardly-interpretable
1091ESEC/FSE â€™20, November8â€“13,2020,VirtualEvent, USA S.Ghamizi, M. Cordy, M. Gubri, M. Papadakis,A.Boystov, Y. Le Traon,andA.Goujon
modelscanbeinefficientandevendangerousinhigh-stakedecision-
makingprocesses [ 22]such as overdraftapproval.
To satisfy those requirements, our partner performed feature
engineering in close collaboration with business experts. They
performed model selection (considering decision trees, random
forest and gradient boosted trees) and used grid search to find
optimal model parameters. AUC for ROC curve was used as an
optimizationcriterionforthegridsearch,whileF1scorewasthe
criteriontochoosetheoptimalclassificationthreshold.Thefinal
modelisarandom forestwith500estimators upto 8-level deep.
ThismodelisbuiltandintegratedwithinaDataikuDSSpipeline3.
Itachievesacceptableperformance:0.99AUCand0.99accuracyon
thetrainingset;0.88AUC,0.80accuracyand 0.70F1-scoreonthe
test set.
4 PROBLEM FORMULATION
4.1 UnconstrainedAdversarialAttack
Letğ‘“(.)be a binary classification model defined over a input space
ğ¼.Forsimplicity,assume ğ¼tobenormalizedsuchthat ğ¼=[0..1]ğ‘š
andğ‘“(ğ‘–)âˆˆ{1,0}for anyğ‘–âˆˆğ¼. Letx0âˆˆğ¼represents an original
example correctlyclassifiedby ğ‘“(.).
Adversarial attacks generate altered inputs that are close to
theiroriginalcounterparts,yetaremisclassifiedbythemodel.In
traditional, unconstrained adversarial attacks, the ideal adversarial
examplexâˆ—craftedfrom x0to foolğ‘“(.)isdefinedas:
xâˆ—=argmin
xâˆ¥xâˆ’x0âˆ¥ğ‘
such that
ğ‘“(x)=1âˆ’ğ‘“(x0)
{x,x0}âŠ‚ğ¼
andwhereâˆ¥.âˆ¥ğ‘istheğ¿ğ‘norm (e.g. ğ¿2).
Thep-normdistancebetweenaperturbedinputandaninitial
one is agood first indicationof the effort required to generatethe
adversarialexample.However,tobeacceptable,theperturbedinput
hastosatisfyinherentdomainconstraints.Thisisafundamental
differencewithimagerecognition,whereitisgenerallyadmitted
that a small distance between xandx0ensures that xhas a strong
perceptualsimilarityto x0and,thus, constitutes avalid image.
Therefore, the problem of generating adversarial attacks for ML-
basedFinTechsystemstakesadifferentform:boththeoriginaland
the adversarial examples must be part of the subspace of inputs
thatareconsideredvalid.Tocharacterizethissubspace,weproceed
byfirstelicitingthe differentdomainconstraints.
4.2 FormalizationoftheConstraints
A first validity criterion demands that the adversarial example still
represents an overdraft, that is, the transaction amount remains
above the current balance of the customerâ€™s account. Additionally,
weconsiderthisamountrelevantifitishigherthan1,000.00cur-
rencyunits.Features4canalsobeinterdependent.Forinstance,the
indebtmentratemustbepositiveandisobtainedbydividingthe
3https://www.dataiku.com/
4DuetoNDAwecannotrevealtheexactfeaturesused.Theexamplesoffeaturewe
provide are different from the ones used by our partner. However, their interrelations
areof the same level of complexity.monthly credit reimbursement by the monthly income. There also
exist categorical features that can only take values froma finite set.
Forexample,eachcustomercanbeassociatedwithapersonallevel
ofrisk(e.g.ona1Å›10scale)basedonitsprofileandpastinterviews
with the bank. The corresponding feature can only take as value
any integerbetween 1and10.
Thishighlights the types ofconstraints thatourmethodmust
support: features can be bounded, each by a different bound, some
may only take certain values, and there may exist numerical de-
pendencies between them. Accordingly, we define that a formula ğœ™
encoding such constraints (i.e. a constraint formula) over a set ğ¹of
features isformedaccording to the following grammar:
ğœ™:=ğœ™1âˆ§ğœ™2||ğ‘“âª°ğœ“|ğ‘“âˆˆ{ğ‘1...ğ‘ğ‘˜}
ğœ“:=ğ‘|ğ‘“|ğœ“1âŠ•ğœ“2
whereğ‘“âˆˆğ¹;ğ‘,ğ‘1,...,ğ‘ğ‘˜are constant values; ğœ™,ğœ™1,ğœ™2are con-
straintformulae;âª°âˆˆ{<,â‰¤,=,â‰ ,â‰¥,>};ğœ“,ğœ“1,ğœ“2arenumericalfor-
mulae andâŠ•âˆˆ{+,âˆ’,âˆ—,/}.
Inadditiontosatisfyingsuchformula,anadversarialattackmay
not be able tomodify some features. For instance, the level of risk
associatedtoacustomerisundercontrolofthebankandcannot
bechangedbythecustomerhimself.Thesameholdsforfeatures
resulting from the aggregation of data over time. Thus, we enforce
the requirement that the attack can only alter the subset FâŠ†ğ¹of
mutablefeatures.Theotherfeatures(whichtheattackcannotalter)
areimmutable . In BGL BNP Paribasâ€™s CSS, 16 features are mutable
and the other 30 are immutable. This means that the attackerâ€™s
capability to succeed strongly depends on the 30 features it cannot
change. We thus consider the features of different customers as
differentstarting pointsfor our search algorithm.
Wecannotdisclosethefeaturesofourpartnerâ€™smodel,oritsspe-
cificconstraints,butweprovideintheGITrepositoryareplication
example onthe Lending Club Load Datase5
4.3 Constrained AdversarialAttack
Letğ¼=[0..1]ğ‘šbe the feature vector space over the feature set
ğ¹={ğ‘“1...ğ‘“ğ‘š},FâŠ†ğ¹bethesetofmutablefeaturesand ğœ™bethe
formulaover ğ¹encodingthedomainconstraints.Furthermore,let ğ¼ğœ™
denote the subspace of valid feature vector, i.e. ğ¼ğœ™={ğ‘–âˆˆğ¼:ğ‘–|=ğœ™}.
Given a binary classification model ğ‘“(.)and an original input
x0={(x0)1,...(x0)ğ‘š}âˆˆğ¼ğœ™,theidealadversarialexample xâˆ—gen-
eratedfrom x0to foolğ‘“(.)isdefinedas
xâˆ—=argmin
xâˆ¥xâˆ’x0âˆ¥ğ‘
such that
ğ‘“(x)=1âˆ’ğ‘“(x0)
{x,x0}âŠ‚ğ¼ğœ™
ğ‘“ğ‘–âˆ‰Fâ‡’(x0)ğ‘–=(x)ğ‘–,âˆ€1â‰¤ğ‘–â‰¤ğ‘š.
Here,thedifficultyofperformingsuchattackliesinthatitcanonly
alterfeatures inFandinawaythat ğœ™remainssatisfied.
5https://www.kaggle.com/wendykan/lending-club-loan-data
1092Search-BasedAdversarialTestingandImprovement of ConstrainedCreditScoringSystems ESEC/FSE â€™20, November8â€“13,2020,VirtualEvent, USA
Table1:Successratesandaverageperturbationproducedby
existingadversarialattacksappliedonourpartnerâ€™ssystem.
Whileeverymethodmanagestogenerateadversarialexam-
ples,none ofthese satisfy thedomainconstrains.
Attack Grosssuccessrate Actual successrate Avgğ¿2
Papernot 74.86% 0.00% 10.64
PGD 17.30% 0.00% 0.10
CW2 80.00% 0.00% 0.37
5 MOTIVATION:HOWHELPFUL ARE
EXISTING ATTACK TECHNIQUES?
Westartourstudybyassessingthecapabilityofexisting(uncon-
strained) attacks to generate valid adversarial examples in our real-
world use case. We assess the gross success rate of these attacks
(percentage of times they manage to create an example misclas-
sifiedbythemodel),theiractualsuccessrate(afterremovingthe
examples that do not satisfy the domain constraints) and the aver-
ageamountofperturbationapplied(measuredasthe ğ¿2distanceto
theoriginalinput).Theamountofperturbationismeanttoserve
as ametric comparison between the attacks.
5.1 Random ForestAttack
First, we consider the attack proposed by Papernot et al. [ 21] Å›
henceforthnamedthe Papernotattack Å›whichwasoriginallyde-
signedtocausemisclassificationsindecisiontreesbyvisitingall
nodesinthetreeandmakingthemflipuntilthemisclassification
isachieved.
This is the only attack relevant to our case. So, we adapt it (to
randomforests)byiterativelyapplyingthePapernotattacktoevery
tree of the forest until the classification outcome of the random
forestchanges.We callthis method IterativePapernot .
We evaluated Iterative Papernot on all original test inputs of
our use case where the model makes correct classifications. The
results are recorded in Table 1and reveal that the attack seems
successful,asitmanagestogenerateadversarialexamples(causing
misclassification)in74 .86%ofourstartingpoints/inputs,withan
averageğ¿2distance (to the corresponding original inputs) of 10 .64.
However,it turned outthat noneof thegenerated inputs satisfied
the domainconstraints,leadingto an actual successrateof0%.
5.2 Gradient-BasedAttacks
Another popular family of adversarial attacks are the gradient-
based attacks. These attacks were designed to generate adversarial
examples onDeep NeuralNetworks (DNNs). Wenote that during
learning,aDNNiterativelyadjustsitsneuronsâ€™weightaccording
to the gradient of its cost function (which depends on the weights).
Gradient-based attacks exploit the same information to produce
a perturbation that changes the output of the last neuron layer,
thereby changing the classification outcome.
Being gradient-based, those methods can apply only on models
relyingondifferentiablecostfunctions.Thus,theydonotwork out
ofthebox onrandom forests.A common way to circumvent this limitation is to build a surro-
gatemodel(aDNN)thatmimicstherandomforest.Thatis,wetrain
this DNN on the same input set and use the outputs (classification
results)oftherandomforestasthegroundtruthfortheDNN.Then,
weperformthegradient-basedattackonthesurrogateDNNand
obtain an adversarialexample.The underlying assumption of this
method is that any adversarial example that fools the DNN also
foolsthe mimickedmodel.
For our experiments, we consider two gradient-based attacks:
Projected Gradient Descent (PGD) [ 16] and CW2 [ 7], which are
considered among the most effective attacks. We apply each attack
onalloriginaltestinputsthatthemodelcorrectlyclassifies.Weim-
plementaDNNmodelusingtheTensorflow/Kerasframeworksand
we use the implementation of the gradient-based attacks provided
bythe IBMrobustnesslibrary [ 19].
Results are shown in Table 1. PGD succeeds in generating ad-
versarial examples (causing misclassification) in only 17.30% of the
attempts,yetitdoessowiththesmallestaverageamountofpertur-
bationamongstalltechniques( ğ¿2distanceof0.10).Nevertheless,
noneofthegeneratedadversarialsatisfythedomainconstraints.
CW2 has a much higher gross success rate (80%) at the cost of
a higher perturbation than PGD (0.37), yet much lower than the
Papernotattack.Liketheothertwomethods,CW2failstogenerate
asingleexample satisfyingthe constraints.
Overall, our analysis shows that, by focusing on classification
method and outcome, while being unaware of the domain con-
straints,state-of-the-art attacks fail to generate valid adver-
sarialexamples .Thisfactdemonstratestheneedfornewconstraint-
aware attacks,i.e.,attacksthat satisfy the constraints bydesign.
6 RESEARCH QUESTIONS
Having shown that state-of-the-art adversarial attacks are not use-
fulinourcase,welookforwaystocircumventtheirlimitationsand
successfully generate valid adversarial examples. We focus more
particularly on theuse casewhere amalicious thirdpartyaimsat
foolingthesystem,i.e.,makingitapproveoverdraftsthatshould
berejected.Thisusecaseisdeemedrelevantbyourpartnerasit
inducesariskoffinanciallossfor the bank.
Tothisend,weinvestigatewhethersimplemethodssatisfying
the domain constraints can solve our problem. Thus, in our first
question,wecheckwhetheralteringtheinitialpointswhilekeeping
constraintssatisfiedissufficient. Hence,we ask:
RQ1Canwegeneratesuccessfuladversarialexamplesbyjust
satisfying thedomainconstraints?
Toanswerthisquestion,weinvestigatetwosolutions.Thefirstis
to extend the Iterative Papernot attack in order to make it consider
theconstraintsasitsearchesthroughthenodes.Thesecondisto
search for solutions (using single objective search) that satisfy the
constraints.Then,wecancheckwhethertheproducedexamples
are adversarial.
As our results shall show, these single-objective methods can
craft examples that either change the classification outcome or
satisfythedomainconstraints,butnotbothatthesametime.We
conjecturethat,ontheonehand,theiterativenatureofthePapernot
attack blocks it into a narrow part of the landscape and, on the
1093ESEC/FSE â€™20, November8â€“13,2020,VirtualEvent, USA S.Ghamizi, M. Cordy, M. Gubri, M. Papadakis,A.Boystov, Y. Le Traon,andA.Goujon
otherhand,therandomsearchdoesnotbenefitfromtheknowledge
oftheoriginalinput(causingarbitraryperturbation).Thismeans
that an effective searchshould not only be guided withadditional
criteria (e.g., minimize the perturbation) but also explore a diverse
space.
To achieve this, we experimented with evolutionary (genetic)
algorithms. Such techniques are directed by some feedback, aka
fitness function, that quantifies how close the current solutions are
tothesoughtones. Atthesametime,therandomalterationsthey
applytothecandidatesolutionscreatedisruptioninthesearchand,
doing so,avoids fallingintolocal optima.
Our definition of constrained adversarial attack (see Section 4.3)
hints that such a search algorithm needs to handle multiple ob-
jectives:minimizeperturbation,fliptheclassification,satisfythe
domain constraints (changing only mutable features). Addition-
ally, a malicious third party looks for optimizing a domain-specific
objective: maximizethe overdraftamount.
Thus,wedesignageneticalgorithmthathandlesallthesecon-
straintsand objectives. We assess itsperformance and investigate,
in particular, which fitness function (combination of objectives)
performs the best.Thus, we ask:
RQ2How effective is our fitness function at generating con-
strained adversarialexamples?
Weanswerthisquestionbypresentingouralgorithm,named Co-
EvA2,andempiricallyevaluatingitusingdifferentvariantsofthe
fitness function.
Havingshownthatourmethodconstitutesaneffectiveattack,
we aim to improve the defence mechanism of our partnerâ€™s sys-
tem, inorder to eliminate any risk that real-worldattacks succeed.
Therefore, we turn our attention toward improving the robustness
of the CSS. To achieve this, we used adversarial training , which
consists of re-training the model with generated (successful) ad-
versarial examples,together with their correctclassificationlabel.
Suchapracticeiswidelypopularandhasbeenshowntoimprove
therobustnessofmachinelearningmodels.We,therefore,usead-
versarialtrainingtoimproveourpartnerâ€™ssystemandcheckthe
scaleofthis improvement.Hence,we ask:
RQ3How much adversarial training based on CoEvA2 can
increase therobustness ofthesystem?
WeanswerthisquestionbycheckingthesuccessrateofCoEvA2
when appliedonvariousstarting points.
7 SEARCH-BASED GENERATION OF
CONSTRAINED ADVERSARIAL EXAMPLES
Figure1displaysanoverviewoftheCoEvA2process.Startingfrom
a set of samples (randomly selected from the test set), we iterate
over the elements of this set. At each iteration, CoEvA2 starts from
the sampled element (named the initial state ) and creates an initial
population of new examples. Then, it evolves this population with
the aim offindingvalid adversarialexamples.
7.1 Population
Since only a subset of the features are mutable (16 out of 46 in
our industrial case), an adversarial example can differ from the
initialstateonlybythevalueofitsmutablefeatures.Thus,givenaninitialstate ğ‘ andafeaturevectorspace ğ¼,thepopulation isa
subsetğ‘ƒâŠ‚ğ¼offeaturevectorssuchthatany individual ğ‘âˆˆğ‘ƒhas
thesamevalueas ğ‘ forallimmutablefeatures.Wecan,therefore,
reduce the genotype of an individual as a single chromosome ,
whichisthevectorofitsmutablefeatures.Any geneisanelement
ofthischromosomeandcontainsthevalueofthecorresponding
mutable features.
Notethatwedonotrequireanyindividualtosatisfythedomain
constraints ğœ™orto causea misclassification. Indeed,we allow the
algorithm to produce invalid and benign examples throughout the
evolutionprocess.Thisprovidesasmoothlandscapeforthesearch,
allowing it to explore efficiently this large search space. Constraint
satisfaction and misclassification are actually encoded into the
fitness/objective functions (see Section 7.2), in a way that valid
adversarial examples are considered better than invalid and benign
ones. Since misclassification is one of the objective, the evaluation
of the individuals makes use of the attacked model (in a black-box
way,using only the outputclass probabilities).
7.2 FitnessFunction
Weformulatethegenerationofconstrainedadversarialexamples
as an optimization problem with four objectives. Each objective
can be independently assessedthroughan objective function.
The first objective function ğ‘“1models the requirementsof caus-
ing misclassification,thatis,maximizingtheprobabilitythatthe
example is classified in the targeted class. It is defined as the dis-
tance between the example and the incorrect class targeted by the
adversarialattack.
Withoutlossofgeneralityweassumethetargetclassis0(the
correctclassis1).Whenprovidedwithaninput x,abinaryclassifi-
cation model outputs ğ‘(x), the prediction probability that xlies in
class 1. If ğ‘(x)is above the classification threshold (a hyperparam-
eter of the model), the model classifies it in class 1; otherwise, in
class0.Thus,wesee ğ‘(x)asthedistanceof xtoclass0.Byseeking
aninputxâˆ—thatminimizesthisdistance,weincreasethelikelihood
of misclassification regardless of the actual classification threshold.
Thus, we have:
ğ‘“1(x)=ğ‘(x).
The second objective is to minimize the amount of perturbation
measuredbetweentheinitialstateandtheadversarialexample,a
commonrequirementofadversarialattacks [ 5].We useaconven-
tionalmeasureofthisamount:thenormalized ğ¿2distancebetween
the two inputs. Thus, given aninitial state x0, the distance from an
examplexandx0isgiven by
ğ‘“2(x)=ğ¿2(x,x0).
The third objective is the actual domain objective, that is, maxi-
mizing the approved overdraft credit amount. By convenience, we
transform this objective into a (normalized) minimization problem.
Letxbe an example and (x)ğ‘¡be the value of the feature encoding
the requested overdraft amount. Then, the objective function ğ‘“3
can be definedas
ğ‘“3(x)=1
(x)ğ‘¡
Thus, this objective considers that the most successful solution
is the one that reaches the highest overdraft amount. In practice,
1094Search-BasedAdversarialTestingandImprovement of ConstrainedCreditScoringSystems ESEC/FSE â€™20, November8â€“13,2020,VirtualEvent, USA
Figure 1:Overview ofCoEvA2. Adversarial examples are generated frombenign inputs (sampled fromthe testset).
though,ourpartner(likemostbanks)specifiesamaximaloverdraft
amount above whichthe transactionisalwaysrejected.
The fourth and last objective concerns the satisfaction of the
domain constraints. As mentioned, we allow individuals to violate
the constraints as the evolution progresses. Yet, to converge to-
wards validadversarial examples, we transformthe satisfaction of
each (numerical) constraint into a (normalized) penaltyfunction to
minimize,representinghowfaranexample xisfromsatisfyingthe
constraint. More precisely, we transform each constraint into an
inequalityoftheformof ğ¶(ğ‘‹)â‰¥0(e.g.3ğ‘“â‰¥ğ‘”yields3ğ‘“âˆ’ğ‘”â‰¥0).
If the constraint is not satisfied, ğ¶(ğ‘‹)<0 and we use the abso-
lute value of C(X) as distance. The overall distance to constraint
satisfactionisthe mean ofthe normalizedindividualdistances.
Thus,assuming ğœ™=/logicalandtext.1
ğ‘–=1..ğ‘˜ğœ™ğ‘–,thefourthobjectivefunctionis
definedas:
ğ‘“4(x)=1
ğ‘˜/summationdisplay.1
ğœ™ğ‘–ğ‘ğ‘’ğ‘›ğ‘ğ‘™ğ‘¡ğ‘¦(x,ğœ™ğ‘–).
In our implementation, thetransformation ofthe constraints into
these penalty functions is automatically handled by the framework
weuse(seemoreinSection 8).Otherheuristicstocomputesuch
distance to satisfaction exist [ 17,18] and could be considered in
future work.
Overall, we consider thatthe success of an adversarialexample
canbemeasuredbythetrade-offbetweenthelikelihoodofflipping
theclassificationoutcome,theappliedperturbation,theoverdraft
amountandthesatisfactionoftheconstraints.Toobjectivelyquan-
tifythistrade-off,wedefineourfitnessfunctionasalinearequation
over the fourobjective functions, that is:
ğ‘“ğ‘–ğ‘¡ğ‘›ğ‘’ğ‘ ğ‘ (x)=ğ›¼Ã—ğ‘“1(x)+ğ›½Ã—ğ‘“2(x)+ğ›¾Ã—ğ‘“3(x)+ğ›¿Ã—ğ‘“4(x)
whereğ›¼,ğ›½,ğ›¾,ğ›¿>0 are meta-parameters that specify the relative
importance of the four objective. Overall the search process will
attempttogenerateexamplesthatminimizethisfitnessfunction
andsimultaneouslyfulfil the fourobjectives.
In practice, we set these meta-parameters according to our part-
nerâ€™s requirements and experience. The rationale was to reflect the
domainrequirements:
â€¢Constraints: These shape the valid input space, meaning
that any non-conforming input is invalid/infeasible. It isInput:x0,an initialstate;
ğ‘“ğ‘–ğ‘¡ğ‘›ğ‘’ğ‘ ğ‘ ,afitness function;
ğ‘ğ‘”ğ‘’ğ‘›,anumber ofgenerations;
ğ¿,apopulation size;
Output:A population ğ‘ƒof adversarialexamples
minimizing the ğ‘“ğ‘–ğ‘¡ğ‘›ğ‘’ğ‘ ğ‘ function;
1ğ‘ƒâ†ğ‘–ğ‘›ğ‘–ğ‘¡(x0,ğ¿);
2forğ‘—=1toğ‘ğ‘”ğ‘’ğ‘›do
3ğ‘ƒğ‘ ğ‘¢ğ‘Ÿğ‘£ğ‘–ğ‘£ğ‘’â†ğ‘ğ‘–ğ‘›ğ‘ğ‘Ÿğ‘¦_ğ‘¡ğ‘œğ‘¢ğ‘Ÿğ‘›ğ‘ğ‘šğ‘’ğ‘›ğ‘¡ _ğ‘ ğ‘’ğ‘™ğ‘’ğ‘ğ‘¡(ğ‘ƒ,ğ‘“ğ‘–ğ‘¡ğ‘›ğ‘’ğ‘ ğ‘ );
4ğ‘ƒğ‘œğ‘“ ğ‘“ğ‘ ğ‘ğ‘Ÿğ‘–ğ‘›ğ‘”â†ğ‘†ğµğ‘‹_ğ‘ğ‘Ÿğ‘œğ‘ ğ‘ ğ‘œğ‘£ğ‘’ğ‘Ÿ(ğ‘ƒğ‘ ğ‘¢ğ‘Ÿğ‘£ğ‘–ğ‘£ğ‘’);
5ğ‘ƒâ†ğ‘ƒğ‘ ğ‘¢ğ‘Ÿğ‘£ğ‘–ğ‘£ğ‘’âˆªğ‘ğ‘œğ‘™ğ‘¦ğ‘€ğ‘¢ğ‘¡ğ‘ğ‘¡ğ‘’(ğ‘ƒğ‘œğ‘“ ğ‘“ğ‘ ğ‘ğ‘Ÿğ‘–ğ‘›ğ‘”);
6end
7returnğ‘ƒ
Algorithm1: Generation processof CoEvA2
imperative to satisfy the constraints and hence, we make
themour mostimportantobjective ( ğ›¿=1,000).
â€¢Maximise overdraft: For a bank, minimising the potential
lossofmoneyisofutmostimportance.Indeed,theoverdraft
amount represents the potential gain for the attacker, which
forms the objective to maximize( ğ›¾=100).
â€¢Cause misclassification:wealsodeemedit moreimportant
tocausemisclassificationthantominimizingperturbation
(ğ›¼=2,ğ›½=1),such that theperturbation shouldonlyserve
to rank adversarialexamples that are successfulandvalid.
As revealed by our experiments, our fitness function provides
a feasible and practical solution to our problem. Alternatively,
we could have relied on search methods to automatically set the
weights. Another option is to define four fitness functions (one
per objective), thereby reducing our problem to multi-objective
optimization and search for Pareto fronts. While studying these
alternativesisofinterest,itisunlikelythattheywillmakemajordif-
ferences under such interdependent constraints. The github reposi-
toryproposesbothagrid-searchoptimisationoftheweightsand
a non-dominated multi-objective approach (NSGA-2) and shows
limitedperformanceimprovementsincomparisonwiththeweights
proposedbyour domain-expert.
1095ESEC/FSE â€™20, November8â€“13,2020,VirtualEvent, USA S.Ghamizi, M. Cordy, M. Gubri, M. Papadakis,A.Boystov, Y. Le Traon,andA.Goujon
7.3 Generation Process
Algorithm 1formalizes the generation process of our genetic algo-
rithm.Fromagiveninitialstate x0,wegenerateaninitialpopulation
ğ‘ƒincluding ğ¿individuals,byrandomlysettingthemutablefeatures
ofx0(Line 1). The only constraints we enforce are the categorical
constraints of the form ğ‘“âˆˆ{ğ‘,1...,ğ‘ğ‘˜}and the boundary con-
straintsoftheform ğ‘“âª°ğ‘whereğ‘“isafeature,âª°âˆˆ{<,â‰¤,=,â‰ ,â‰¥,>},
andğ‘,ğ‘1,...,ğ‘ğ‘˜are constant values. This allows reducing the num-
berofinvalidexampleswithoutbiasingthegeneration(sincethe
boundary constraintsinvolve only one feature each).
Then,wemakethepopulationevolveforapredefinednumber
ğ‘ğ‘”ğ‘’ğ‘›of generations (Lines 2Å›6). At each iteration (generation),
weevaluatethefitnessfunctionofeachindividualofthecurrent
population ğ‘ƒ. This is achieved by, first, combining the genotype of
each individual (its mutable features) with the immutable features
ofx0.Then,wecaninputanyresultingexample xintothefitness
function (as definedpreviously) andobtain the fitness valueof x.
What follows is the application of selectors and alterers to form
the next generation. Wefirst usetournamentselection thatkeeps
thebestindividuals(accordingtothefitnessfunction)outofsam-
ples oftwo(Line 3).Thus, halfofthe population disappear.
Asforalterers,werandomlyapplycrossoverandmutationoper-
ators. Forthe crossover(Line4), we randomly pick pairs of individ-
uals(thatsurvivedthetournamentselection)anduseasimulated
binary crossover [ 10] to create two new offsprings from the nu-
mericalandcategoricalfeaturesoftheparents.Weassignthesame
probabilisticimportancetoeachparent.Attheendofthecrossover,
weobtainanewapopulationofsize ğ¿(halfparents,halfoffsprings).
Next,weapplymixedpolynomialmutationtoalterrandomlythe
mutablefeaturesoftheoffspring(Line5).Eachfeaturehasaproba-
bilityğ‘ğ‘štobealtered(setto ğ‘ğ‘š=|F|âˆ’1inourexperiments).Like
theinitialisationprocessofthepopulation,theappliedmutation
operators take into account the nature (categorical/integeror real)
andboundariesofeachfeature.Attheend ofthemutationprocess,
we obtain anewpopulation ğ‘ƒğ‘—to proceedinthe nextgeneration.
After the specified number of generations passed, the algorithm
returns the examples of the last generation that satisfy the con-
straints. In addition to these individuals, the algorithm also returns
the associatedvaluesoffitness andobjective functions.
8 EMPIRICALEVALUATION
8.1 ExperimentalSetup
Toaddressourresearchquestions,weimplementedCoEvA2.The
tool was developed in Python on top of PyMoo, an established
frameworkformodellingandexecutinggeneticalgorithmsinPython.
Ourimplementationispubliclyavailable.6Allexperiments were
run onour partnerâ€™s internal server with about6 cores allocated
for our experiments.
We set the meta-parameters of the genetic algorithm as follows.
Populationsizewassetto40tomaintainanacceptablecomputa-
tiontime(CoEva2runon4,000initialstatestakesabout24days).
Exploratoryexperimentsshowedthatahigherpopulationsizedoes
notaffectourresults.Also,westopthealgorithmafter10,000gener-
ations.Thesenumberswerefoundexperimentallytobesufficientin
6https://github.com/UL-SnT-Serval/coeva2/tree/fsemaking our technique to craft successful adversarial examples. For
selection,mutationandcrossover,wekepttheirdefaultparameters
which worked well in our case. During our experimentation, we
performedexploratorytrialswithalternativesettingsandobserved
minor differences. This is in line with the study of Zamani and
Hemmati[ 26]onthesensitivityofsearch-basedtestingmethods
to theirhyper-parameters.
Allourexperimentsfocusonourpartnerâ€™scasestudy,i.e.gener-
atingfeasible,adversarialoverdraftrequestsapprovedbytheCSS.
To that end, we consider our partnerâ€™s real-world data comprising
400,000requests. The 275,000were usedby ourpartner to train the
CSSâ€™s random forest. Out of the 125,000 remaining (the test set),
we keep only those which are rejected overdraft requests correctly
classifiedbytheCSS.Therationaleisthatinrealisticsettings,an
attackercanonlymanipulatefuturetransactionsandaccountstatus
(whichareinherentlyoutsidethetrainingset)withtheaimtomake
previously-rejected requests accepted by the system and, doing so,
retrievingmoney illicitly.
This leaves us with 19,274 data points. We use two random sam-
plesofthisset,eachofwhichcontains4,000initialstates(customer
account and transaction history): the first sample is used in RQ1
and RQ2 while the second is used to assess the adversarial training
in RQ3. Thus, for each RQ we execute CoEvA2 4,000 times, once
oneachinitialstate.This issufficient to ruleoutrandom effects.
Here it must be noted, that the above settings are common to
all RQs we investigate. Still the related settings required to answer
eachspecificRQaregivenatthebeginningoftheresultSections,
i.e.,thosethat answer RQ1 andRQ2 (Sections 8.2,8.3,8.4).
8.2 RQ1: ConstrainedPapernotandRandom
Search
Our first series of experiments consider (1) the Papernot attack
extendedtoconsiderthedomainconstraintand(2)arandomsearch
thatonlyconsidersthesatisfactionoftheconstraintsasobjective
(aka CoEvA2 with the same meta-parameters but using only ğ‘“4
as the fitness function). We regard these two attacks as baseline
methodsthat we seekto improve.
Our extension of the Papernot attack differs from the original in
three ways. First, it avoids visiting thenodesrelated to immutable
features(thus,itneverchangesthesefeatures).Second,itchecks
the satisfaction of boundary constraints on the fly, each time a
feature is altered. Third, it attempts to satisfy the other constraints
byupdatingthe dependent features.
Toallowforfine-grainedanalysisoftheirresults,wedefinefour
objectiveindicators.Eachindicatorreportsthepercentageofinitial
states from which a given methodcan produce avalid adversarial
example.The objective corresponding to theseindicators are:
O1:satisfy the domainconstraints
O2:cause misclassification
O3:satisfy O1 andO2
O4:satisfyO3andcreatearelevantoverdraft(morethan1,000
currency units)
Weevaluatethetwobaselinemethodsonasampleof4,000initial
states(randomlypickedfrom19,274rejectedoverdrafts).Thatis,
we run eachmethod4,000 times (onceper sampledinitialstate).
1096Search-BasedAdversarialTestingandImprovement of ConstrainedCreditScoringSystems ESEC/FSE â€™20, November8â€“13,2020,VirtualEvent, USA
Table 2: Objective indicators of random search and con-
strained Papernotattacks
Success rate
Objective Random search Papernot
Constraints(O1) 0.00% 0.20%
Misclassification (O2) 57.15% 25.85%
O1 andO2 (O3) 0.00% 0.00%
O3 andoverdraftamount (O4) 0.00% 0.00%
ResultsareshowninTable 2.Interestingly,noneofthegenerated
adversarial examples (by any of the two attacks) are valid (none of
them satisfy O4). In the case of Papernot, a small number of the
generated examples satisfy the domain constraints and about one-
fourth overall cause misclassification. However,there is none that
fulfilbothobjectives.Thisshowsthatstraightforwardextensions
tounconstrainedattacks(tomakethemconsidertheconstraints)
remainineffective.
Inthecaseoftherandomsearch,weobservethatmorethanhalf
ofthereturnedexamplescausemisclassification.Interestingly,none
ofthemsatisfytheconstraintsalthoughthisistheonlyobjective
forceduponthesearch.Adetailedinvestigationofthegenerated
examples reveals that the perturbation amount ranges from 0.2
to more than 1,000. This is significantly more than the Papernot
attack and the aforementioned gradient-based methods (see our
preliminary study Section 5). From these observations, we hypo-
thesizethatminimizingtheperturbationwouldallowrestricting
the exploration within a reasonable area around the initial state.
Doing so, the search would increase the likelihood to find valid
adversarialexamplesaroundthisinitialstate(inparticular,when
initializingthe population andperformingmutation).
8.3 RQ2: Coeva2andItsFitnessFunction
Given thatthe baselinemethodsdonot generatevalidadversarial
examples, we implement and evaluate CoEvA2. We execute the
algorithmonthesamerandomly-pickedsetofthe4,000initialstates
that was used in RQ1. We also consider the same four objective
indicators as inRQ1 to allowfor fine-grainedanalysis.
To identify and form a good fitness function, we consider multi-
plevariantsofCoEvA2,eachofwhichusesadifferentsubsetofthe
objectivefunctions.Inadditiontotherandomsearchguidedonly
bytheconstraintsatisfaction(previouslystudiedinRQ1),wecon-
siderthreevariants:thefullCoEvA2,anothervariantwhereonly
theğ‘“2(perturbationminimization)partisremovedandanotherone
whereonlythe ğ‘“3(overdraftmaximization)partisremoved.Mis-
classificationandconstraintsatisfactionareminimummandatory
criteria in order to generate valid examples and thus, all the three
CoEvA2variants we examine include them.
Table3summarizes our results. It shows that the variant of
CoEvA2 with all parts of the objective function activated is the
only one capable of generating adversarial examples that cause
misclassification, satisfy the constraints and engender relevant
overdrafts.
CoEvA2 is successful for 8.45% of the initial states. Thus, on
average, only 12 initial states are needed to perform a successful
attack. An interesting observation here is that from one initial
(a)f1:Predictionprobability(lower is better)
(b) f2:Perturbation, ğ¿2distance (lower is better)
(c) f3:Overdraftamount (higheris better)
(d)f4:Constraintsviolation error (lower is better)
Figure2:Meanvalue(red)andboundaries(bluebetweenthe
maximumandtheminimumvalues)ofeachobjectivefunc-
tion over4,000 initial statesand for10,000 generations.
state we can generate more than one valid adversarial example.
This results in more than thirteen thousand of valid adversarial
examples bypassing the banking system. These results seem to
suggestthatthefitnessfunctionweformiseffectiveandthatallits
parts are important.
Thislastpointcanbeconfirmedbytherestoftherecordedre-
sults.Theseshowthatalltheobjectivefunctionpartsarenecessary
to generate successful adversarial examples. Without the pertur-
bationminimizationobjective(one-before-lastcolumn),CoEvA2
generatesslightlymoremisclassifiedexamples(for31.18%ofthe
1097ESEC/FSE â€™20, November8â€“13,2020,VirtualEvent, USA S.Ghamizi, M. Cordy, M. Gubri, M. Papadakis,A.Boystov, Y. Le Traon,andA.Goujon
Table 3:Objective indicatorsachieved by CoEvA2, usingdifferentfitnessfunctions.
Objective indicators Random search( ğ‘“4)CoEvA2(all) CoEvA2( ğ‘“1,ğ‘“3,ğ‘“4)CoEvA2( ğ‘“1,ğ‘“2,ğ‘“4)
Constraints(O1) 0.00% 58.9% 0.00% 100.00%
Misclassification (O2) 57.15% 27.1% 31.18% 18.79%
O1 andO2 (O3) 0.00% 17.2% 0.00% 18.79%
O3 andoverdraftamount (O4) 0.00% 8.45% 0.00% 0.00%
Figure 3:Adversarial training process.
initialstatesinsteadof27.10%)butnoneofthemsatisfiesthecon-
straints. This confirms our previous hypothesis that not restricting
the perturbation makes the algorithm create examples much differ-
ent from the original (valid) example. In highly constrained search
space, this increases thelikelihood of generating invalid examples.
Finally,withouttheobjectiveofmaximizingtheoverdraftamount
(lastcolumn),CoEvA2generatesexamplessatisfyingtheconstraints
in every case. However, only 18.79% of them cause misclassifica-
tion. None of these achieve a sufficient overdraft amount (1,000
currencyunits).Thisisbecausethealgorithmappliesonlysmall
variationsandonlytotheothermutablefeatures,whichreduces
the likelihoodofviolating the constraintsandofmisclassification.
TobetterunderstandhowCoEvA2handlesthetrade-offbetween
the four objective functions, we show in Figure 2how the value
ofeachofthem evolvesoverthegenerationswhenappliedtothe
initial states for which it managed to generate valid adversarial
examplesover 10,000generations.
At each generation, we average the objective function scores
obtainedbythecurrentpopulation.Thusweobtain,foreachinitial
stateandeachobjectivefunction,10,000values(onepergeneration).
Then, we show the minimum, mean and maximum values of each
(averaged) score over all the initial states. The red line is the mean,
whereas the blue area denotesthe minimal andmaximal scores.
The four plots confirm that constraint satisfaction is the first
objective fulfilled by the algorithm, and it does so always in the
early generations (after about 100). The figure also shows that
theoverdraftamountisthesecond-mostdominantobjectiveand
is always achieved within the first 200 generations, reaching 108
currencyunits,themaximumamountauthorizedbytheCSS.All
the individuals of the population in all the next generations inheritthismaximumvalueandkeepsatisfyingtheconstrains.Meanwhile,
theğ¿2distance fluctuates around 0.9, which is 10 times less than
thePapernotattack.Theaveragepredictionprobabilitystabilizes
around 0.35,whichisslightly belowthe prediction threshold.
Interestingly, taken together these results suggest that a careful
choiceoftheinitialstateallowsCoEvA2tofindvalidadversarial
examples after a limited number of generations (200). Moreover,
theseexamplesmakethesystemoverdraftofhighamount(close
to the strict maximum authorized by thebank). While frightening,
theseresultsalsomeanthatwecanfocusonspecificinitialstatesto
buildcountermeasures andincreasethe robustnessof the system.
Overall,ourresultscorroboratetheconclusionthatallfourparts
ofourfitnessfunctionplayacrucialroleincraftingvalidadversarial
examples.Atthesametime,ourapproachdemonstratesthatitis
indeed feasible to craft valid adversarial examples in real-world
criticalsystems.Thismotivatestheneedforappropriatedefence
mechanisms to reinforce the robustness of the system against such
attacks.We investigate this inthe nextresearch question.
8.4 RQ3: AdversarialTraining
Figure3shows the adversarial training process we designed to
improve the robustness of our partnerâ€™s system against our (previ-
ouslysuccessful)adversarialattack.First,wegenerate4,000valid
adversarialexamples(overdraftsacceptedbythemodelthatshould
be rejected) and re-train the model to classify them correctly. To
do so, we use the 4,000 initial states used in RQ2. After re-training
themodel,weexecuteagainCoEvA24,000timesusingeachtime
a new initial state that was not used to produce the adversarial
1098Search-BasedAdversarialTestingandImprovement of ConstrainedCreditScoringSystems ESEC/FSE â€™20, November8â€“13,2020,VirtualEvent, USA
trainingset.Wecheckwhethertheattackmanagedtogenerateany
adversarialexamples.
Itresultsthat the adversarial training makes CoEvA2 incapable
of generating valid adversarial examples. Thus, our adversarial
training method grants protection against the very same attack
thatwaspreviouslyeffective.Thisisapositiveoutcomethatcanbe
used by our partner in order to improve the robustness of the CSS.
As our system is still in testing phase, it is used in parallel to the
original model. Thus, an overdraft approved by the existing model
and rejected by ours is likely to be an adversarial example. In all
the othercases,one should followthe decision ofthe firstmodel.
8.5 Threatsto Validity
Validitythreatstoourresultsmayarisebytheimplementationswe
used. Thus, potential bugs either in our or the underlying frame-
works may influence our results. We do not consider this threat
as important since we thoroughly checked our code and many
of the adversarial examples we generated were verified by our
partner.Moreover,werelyonwidelyusedandrelativelyreliable
frameworks, Scikit-Learn and Tensorflow for the machine learning
algorithms, and reputable libraries like the Adversarial Toolbox
fromIBM[ 19]foradversarialattacksandPymoofromtheMichigan
StateUniversity7for multi-objective genetic algorithms.
Anotherpotential threatconcernsthespecificityofthedataset
and classification model we used. Both are from our partnerâ€™s real
production system and since our partner is a major player, its data
andpracticesshouldberepresentativeofothercompanies.More-
over,averificationofhistoricaldatarevealedremarkableresultsfor
thelastyear.Duetothespecificityofourindustrialcase,theresults
we obtained may not fully transfer to other industries (namely out-
side of credit scoring domain). Nevertheless, our endeavour shows
that the problem exists in the real world and formalises it to facili-
tate the design of similar solutions to other cases. Moreover, our
algorithm and approach have been designed to be generic enough
tobeadjustedtootherusecases,andweprovidethealgorithmand
allthe hyper-parametersofour approach for reproducibility.
Toreducetheimpactofrandomeffects,allourexperimentscon-
sider4,000differentinitialstates(customeraccountandtransaction
history)andrunthestudiedmethodsonceperstate.Sincewemake
4,000independentexecutions,multiplerunsperexecutioncanonly
make a difference in isolated cases and not in the overall perfor-
mance (expected case). This is because initial states can be seen as
independent repetitions.
Inourexperiment,weperformasinglerunperstatesincewe
focus on trends. Thus, we run our approach on 4,000 cases and
found adversarial cases in 341. These are sufficiently large num-
bers to ruleout randomeffects. Yet, multiple repetitionsand more
generallyadditionalsearchtime-budgetmayimprovetheresultsof
the search. We run the randommethod 4 ,000Ã—40,000 times(4,000
initial statesÃ—1,000 generations with 40 individuals), and found
0 adversarial cases, whichdemonstrates theineffectiveness of the
random method.
7https://pymoo.org9 CONCLUSION
Inthispaper,westudiedtheproblemoftestingamachine-learning-
basedindustrialcredit scoringsystemagainstmalicious inputs.In
particular,weconsideredthecasewhereanattackermanipulates
therelatedfeatures,withtheaimtocauseamisclassificationbya
binaryclassificationmodel.Tothisend,weevaluatedthecurrent
state-of-the-artadversarialattacks,bothinafull-knowledgecontext
andalimited-knowledgeusingourpartnerâ€™s datasetandsystem.
Basedonthisstudy,weshewthatapproachesproposedintheliter-
aturecanindeedgenerateadversarialexamplesbutthesearenot
usefulsincetheydoaccountfordomainconstraints.Thislimitation
of the methods results in generating implausible examples. To deal
withthissituation,weproposedasearch-basedmethodovercoming
theselimitations.Weshowedthatournewattackconstitutesareal
security threat to FinTech systems relying on machine learning. At
the same time, we exploit this threat to improve the defence mech-
anisms of our industrial system. In the end, the system becomes
immuneto our attack.
10 ARTIFACT
Our library is available on Github and can be extended to any
constrainedadversarialattacktask.The twomain branches are:
â€¢fse:thisbranchtacklestheimplementationpresentedinthis
paper and the experiments to reproduce our results. Our
datasetbeingproprietaryandprivate,weprovide asimilar
open-sourcedataset, LendingClubLoadDataset8toevaluate
our approach. The tool is built around configuration files
(locatedin /configurations folder) whereyoucandefinethe
constraintsofyour problem,your objective functions, etc...
The folder /srccontains the actual implementation of our
algorithm, whilethe folder /experiments provides scripts to
easily run eachofthe Research Questionsâ€™ experiments.
â€¢master: this branch is the ongoing iteration of the library.
It provides an extension of the approach using Grid and
Random search to optimize the weights of each objective,
andMoEva2anNSGA-2[ 9]extensionofourapproachthat
usesnon-dominated multi-objective evolution.Thisbranch
will contain the stable evolutions of the library, in particular
allelements mentionedas future orongoing work.
ACKNOWLEDGMENTS
ThisworkissupportedbytheLuxembourgNationalResearchFunds
(FNR) CORE project C18/IS/12669767/STELLAR/Le Traon .
This work is a collaborative work between the University of
Luxembourg andthe DataScienceLabof BGL BNPParisbas.
Special thanks to Thibault Simonetto from the University of
Luxembourgwhomadeourlibraryopentothepublicandextended
itto support publiclyavailable datasets for replication purposes.
8https://www.kaggle.com/wendykan/lending-club-loan-data
1099ESEC/FSE â€™20, November8â€“13,2020,VirtualEvent, USA S.Ghamizi, M. Cordy, M. Gubri, M. Papadakis,A.Boystov, Y. Le Traon,andA.Goujon
REFERENCES
[1]Naveed Akhtar and Ajmal Mian. 2018. Threat of adversarial attacks on deep
learning in computer vision: A survey. IEEE Access 6 (2018), 14410Å›14430.
[2]ShaukatAli,MuhammadZohaibZ.Iqbal,AndreaArcuri,andLionelC.Briand.
2013. Generating Test Data from OCL Constraintswith Search Techniques. IEEE
Trans.SoftwareEng. 39,10(2013),1376Å›1402. https://doi.org/10.1109/TSE.2013.17
[3]Moustafa Alzantot, Yash Sharma, Supriyo Chakraborty, Huan Zhang, Cho-Jui
Hsieh,andManiB.Srivastava.2019. GenAttack. ProceedingsoftheGeneticand
EvolutionaryComputationConference (Jul2019). https://doi.org/10.1145/3321707.
3321749
[4]MoustafaAlzantot,YashSharma,AhmedElgohary,Bo-JhangHo,ManiSrivastava,
andKai-WeiChang.2019. GeneratingNaturalLanguageAdversarialExamples.
Association for Computational Linguistics (ACL), 2890Å›2896. https://doi.org/10.
18653/v1/d18-1316 arXiv:1804.07998
[5]Battista Biggio, Igino Corona, Davide Maiorca, Blaine Nelson, Nedim Å rndiÄ‡,
Pavel Laskov, Giorgio Giacinto, and Fabio Roli. 2013. Evasion Attacks against
MachineLearningatTestTime. LectureNotesinComputerScience (2013),387Å›402.
https://doi.org/10.1007/978-3-642-40994-3_25
[6]Battista Biggio and Fabio Roli. 2018. Wild patterns: Ten years after the rise of
adversarial machine learning. Pattern Recognition 84 (December 2018), 317Å›331.
https://doi.org/10.1016/j.patcog.2018.07.023 arXiv:1712.03141
[7]NicholasCarliniandDavidWagner.2017. TowardsEvaluatingtheRobustnessof
Neural Networks. 2017 IEEE Symposium on Security and Privacy (SP) (May 2017).
https://doi.org/10.1109/sp.2017.49
[8]VinÃ­cius Veloso de Melo and Wolfgang Banzhaf. 2016. Improving Logistic Re-
gression Classification of Credit Approval with Features Constructed by Kaizen
Programming.In Proceedingsofthe2016onGeneticandEvolutionaryComputation
Conference Companion -GECCO â€™16 Companion . ACM Press,Denver, Colorado,
USA,61Å›62. https://doi.org/10.1145/2908961.2908963
[9]K.Deb,A.Pratap,S.Agarwal,andT.Meyarivan.2002.Afastandelitistmultiobjec-
tive genetic algorithm: NSGA-II. IEEE Transactions on Evolutionary Computation
6,2 (2002), 182Å›197.
[10]Kalyanmoy Deb, Karthik Sindhya, and Tatsuya Okabe. 2007. Self-Adaptive
Simulated Binary Crossover for Real-Parameter Optimization.In Proceedingsof
the 9th Annual Conference on Genetic and Evolutionary Computation (London,
England) (GECCO â€™07) . Association for Computing Machinery, New York, NY,
USA,1187Å›1194. https://doi.org/10.1145/1276958.1277190
[11]Hongwei Feng, Shuang Li, Dianyuan He, and Jun Feng. 2019. A novel feature
selection approach based on multiple filters and new separable degree index for
creditscoring.In ProceedingsoftheACMTuringCelebrationConference-China
on - ACM TURC â€™19 . ACM Press, Chengdu, China, 1Å›5. https://doi.org/10.1145/
3321408.3323928
[12]B.J.Garvin,M.B.Cohen,andM.B.Dwyer.2009. AnImprovedMeta-heuristic
SearchforConstrained Interaction Testing.In 20091stInternationalSymposium
onSearch BasedSoftwareEngineering . 13Å›22.
[13]Kathrin Grosse, Nicolas Papernot, Praveen Manoharan, Michael Backes, and
Patrick McDaniel. 2017. Adversarial examples formalware detection.In LectureNotesinComputerScience(includingsubseriesLectureNotesinArtificialIntelligence
and Lecture Notes in Bioinformatics) , Vol. 10493 LNCS. Springer Verlag, 62Å›79.
https://doi.org/10.1007/978-3-319-66399-9_4
[14]AlexKantchelian,J.D.Tygar,andAnthonyD.Joseph.2016. EvasionandHarden-
ingofTreeEnsembleClassifiers.In Proceedingsofthe33rdInternationalConference
on International Conference on Machine Learning - Volume 48 (New York, NY,
USA)(ICMLâ€™16) . JMLR.org,2387Å›2396.
[15]Francisco Louzada, Anderson Ara, and Guilherme B. Fernandes. 2016. Classifica-
tionmethodsappliedtocreditscoring:Systematicreviewandoverallcomparison.
SurveysinOperationsResearchandManagementScience 21,2(Dec.2016),117Å›134.
https://doi.org/10.1016/j.sorms.2016.10.001
[16]Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and
Adrian Vladu. 2017. Towards Deep Learning Models Resistant to Adversarial
Attacks. , 27pages. arXiv: 1706.06083 http://arxiv.org/abs/1706.06083
[17]Phil McMinn. 2004. Search-based software test data generation: a survey. Softw.
Test. Verification Reliab. 14,2 (2004), 105Å›156. https://doi.org/10.1002/stvr.294
[18]Zbigniew Michalewicz and Marc Schoenauer. 1996. Evolutionary Algorithms for
ConstrainedParameterOptimizationProblems. Evol.Comput. 4,1(1996),1Å›32.
https://doi.org/10.1162/evco.1996.4.1.1
[19]Maria-Irina Nicolae, Mathieu Sinn, Minh Ngoc Tran, Beat Buesser, Ambrish
Rawat,MartinWistuba,ValentinaZantedeschi,NathalieBaracaldo,BryantChen,
Heiko Ludwig, Ian Molloy, and Ben Edwards. 2018. Adversarial Robustness
Toolboxv1.2.0. CoRR1807.01069(2018). https://arxiv.org/pdf/1807.01069
[20]NicolasPapernot,PatrickMcDaniel,SomeshJha,MattFredrikson,ZBerkayCelik,
and Ananthram Swami. 2016. The limitations of deep learning in adversarial
settings. In 2016 IEEE European Symposium on Security and Privacy (EuroS&P) .
IEEE,372Å›387.
[21]Nicolas Papernot, Patrick D. McDaniel, and Ian J. Goodfellow. 2016. Trans-
ferability in Machine Learning: from Phenomena to Black-Box Attacks using
Adversarial Samples. CoRRabs/1605.07277 (2016). arXiv: 1605.07277 http:
//arxiv.org/abs/1605.07277
[22]Cynthia Rudin. 2019. Stop explaining black box machine learning models for
high stakes decisions and use interpretable models instead. Nature Machine
Intelligence 1, 5 (May 2019), 206Å›215. https://doi.org/10.1038/s42256-019-0048-x
[23]Roberto Saia, Salvatore Carta, and Gianni Fenu. 2018. A Wavelet-based Data
Analysis to Credit Scoring. In Proceedings of the 2nd International Conference
on Digital Signal Processing - ICDSP 2018 . ACM Press, Tokyo, Japan, 176Å›180.
https://doi.org/10.1145/3193025.3193039
[24]ChristianSzegedy,WojciechZaremba,IlyaSutskever,JoanBruna,DumitruErhan,
Ian Goodfellow, and Rob Fergus. 2013. Intriguing properties of neural networks.
arXiv preprint arXiv:1312.6199 (2013).
[25]Lyn C. Thomas, David B. Edelman, and Jonathan N. Crook. 2002. Credit Scoring
and Its Applications . Society for Industrial and Applied Mathematics. https:
//doi.org/10.1137/1.9780898718317
[26]ShayanZamaniandHadiHemmati.2019. RevisitingHyper-ParameterTuning
for Search-Based Test Data Generation. Lecture Notes in Computer Science (2019),
137Å›152. https://doi.org/10.1007/978-3-030-27455-9_10
1100