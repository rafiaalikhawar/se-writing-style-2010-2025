Log-based Anomaly Detection with Deep Learning:
How Far Are We?
Van-Hoang Le
The University of Newcastle
NSW, Australia
vanhoang.le@uon.edu.auHongyu Zhang‚àó
The University of Newcastle
NSW, Australia
hongyu.zhang@newcastle.edu.au
ABSTRACT
Software-intensivesystemsproducelogsfortroubleshootingpur-
poses.Recently,manydeeplearningmodelshavebeenproposed
to automatically detect system anomalies based on log data. These
modelstypicallyclaimveryhighdetectionaccuracy.Forexample,
mostmodelsreportanF-measuregreaterthan0.9onthecommonly-
usedHDFSdataset.Toachieveaprofoundunderstandingofhowfar
wearefromsolvingtheproblemoflog-basedanomalydetection,in
this paper, we conduct an in-depth analysis of five state-of-the-art
deeplearning-basedmodelsfordetectingsystemanomaliesonfour
public log datasets. Our experiments focus on several aspects of
model evaluation, including training data selection, data grouping,
class distribution, data noise, and early detection ability. Our re-
sults point out that all these aspects have significant impact on the
evaluation,andthatallthestudiedmodelsdonotalwaysworkwell.
Theproblemoflog-basedanomalydetectionhasnotbeensolved
yet. Based on our findings, we also suggest possible future work.
CCS CONCEPTS
‚Ä¢Software and its engineering ‚ÜíMaintaining software.
KEYWORDS
Anomaly Detection, Log Analysis, Log Parsing, Deep Learning
ACM Reference Format:
Van-Hoang Le and Hongyu Zhang. 2022. Log-based Anomaly Detection
with Deep Learning: How Far Are We?. In 44th International Conference on
Software Engineering (ICSE ‚Äô22), May 21‚Äì29, 2022, Pittsburgh, PA, USA. ACM,
New York, NY, USA, 12pages.https://doi.org/10.1145/3510003.3510155
1 INTRODUCTION
Highavailabilityandreliabilityareessentialforlarge-scalesoftware-
intensivesystems.Asthesesystemsprovidevariousservicestoa
large number of users, a small problem in the system could leadto user dissatisfaction and even significant financial loss. Anom-
alydetectionis,therefore,importantforthequalityassuranceof
complex software-intensive systems.
‚àóHongyu Zhang is the corresponding author.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation
on the first page. Copyrights for components of this work owned by others than ACMmustbehonored.Abstractingwithcreditispermitted.Tocopyotherwise,orrepublish,topostonserversortoredistributetolists,requirespriorspecificpermissionand/ora
fee. Request permissions from permissions@acm.org.
ICSE ‚Äô22, May 21‚Äì29, 2022, Pittsburgh, PA, USA
¬© 2022 Association for Computing Machinery.
ACM ISBN 978-1-4503-9221-1/22/05...$15.00
https://doi.org/10.1145/3510003.3510155Software-intensivesystemsoftenrecordruntimeinformationby
printingconsolelogs.Alargeandcomplexsystemcouldproduce
amassiveamountoflogs,whichcanbeusedfortroubleshooting
purposes.Forexample,thecloudcomputingsystemsofAlibabaInc.
produce about 30-50 gigabytes (around 120-200 million lines) of
tracinglogsperhour[ 37].Logdataisusuallyunstructuredtextmes-
sages, which can help engineers understand the system‚Äôs internal
statusandfacilitatemonitoring,administering,andtroubleshoot-
ing of the system [ 18]. Log messages can be parsed into log events,
whicharetemplates(constantpart)ofthemessages.Figure 1shows
an example of raw log messages and the corresponding log events
obtained after parsing.
Log Message
10.251.39.192:50010 Served block
blk_-5341992729755584578 to /10.251.39.192
10.250.5.237:50010 Served block
blk_3166960787499091856 to /10.251.43.147
10.251.30.85:50010: Got exception while serving
blk_-2918118818249673980 to /10.251.90.64:Log Event
* Served block * to *
* Served block * to *
* Got exception while serving * to *Parsing
Figure 1: An example of log messages and log events
Over the years, many data-driven methods have been proposed
to automatically detect system anomalies by analyzing log data [ 6,
7,12,29,47,50,54].Forexample,Heetal.[ 19]evaluatedsixpopular
machinelearning(ML)algorithmsforlog-basedanomalydetection.
TheseML-basedmethodssharesomelimitationsofinflexiblefea-
tures,inefficiency,andweakadaptability[ 54].Inordertoaddress
these issues, deep learning (DL) has been adopted and produced
promising results.Du etal. [ 12] proposedto useLong-Short Term
Memory (LSTM) to model the sequential patterns of normal ses-
sions, then identify anomalies as those violated the patterns. Meng
etal.[36]trainedanLSTMmodeltodetectsequentialandquantita-
tiveanomaliesusinglogcountvectorsasinputs.Theyalsoproposed
template2vectoconsiderthesynonymsandantonymsofthewords
inlogtemplates.Otherstudies[ 27,54]representlogtemplatesas
semanticvectorstohandletheinstabilityoflogdata.Generally,the
existing DL-based log anomaly detection methods show promis-
ing results on commonly used datasets and claim their superiority
over traditional ML-based approaches. For instance, DeepLog [ 12]
and LogAnomaly [ 36] all reported a very good performance on
commonly-usedHDFSandBGLdatasets(withF-measurevalues
greater than 0.9).
However, we notice that several important aspects are over-
looked by the existing work. These aspects are associated withexperimental datasets, evaluation metrics, and experimental set-
tings.Inthiswork,wewouldliketodivedeepintotheproblemandanswer:Arelog-basedanomalydetectionmethodswithdeeplearning
as good as they claimed? What are the major factors that could affect
their performance?
13562022 IEEE/ACM 44th International Conference on Software Engineering (ICSE)
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:28:46 UTC from IEEE Xplore.  Restrictions apply. ICSE ‚Äô22, May 21‚Äì29, 2022, Pittsburgh, PA, USA Van-Hoang Le and Hongyu Zhang
Toanswertheabovequestions,weconductasystematicevalu-
ation of five representative deep learning models for log-based
anomaly detection (including DeepLog [ 12], LogAnomaly [ 36],
PLELog[ 48],LogRobust[ 54],andCNN[ 33])onfourdatasets(in-
cluding HDFS, BGL, Spirit, and Thunderbird), under controlled
experimental settings. We first conduct an analysis of training data
selection and grouping techniques. Then we explore the impact of
different characteristics of datasets (including data noise and class
distribution) on model performance. Finally, we analyze the ability
ofthemodelsintheearlydetectionofanomalies.Throughexten-
siveexperiments,weobtainthefollowingmajorfindingsaboutthe
current deep learning models for log-based anomaly detection:
‚Ä¢The training data selection strategies (random or chronologi-cal)havesignificantimpactonthesemi-supervisedlog-basedanomaly detection models. Randomly selecting training datacould cause the data leakage problem and unreasonably high
detection accuracy.
‚Ä¢Different log data grouping methods have substantial influence
on the performance of the models. Models tend to lose their
accuracy when dealing with shorter log sequences.
‚Ä¢Theeffectivenessofthemodelsissignificantlyaffectedbythe
highlyimbalancedclassdistribution.Commonly-usedmetrics,
including Precision, Recall, and F-measure, are not comprehen-
siveenoughforevaluatingalog-basedanomalydetectionmodel
with highly imbalanced data.
‚Ä¢A small amount of data noise, including mislabeled logs and log
parsingerrorscandowngradeanomalydetectionperformance.
Comparedtosemi-supervisedmethods,supervisedmodelsare
moresensitivetomislabeledlogsinthetrainingdata.Models
capableofunderstandingthesemanticmeaningoflogdatacould
reduce the impact of log parsing errors.
‚Ä¢Differentmodelshavedifferentabilitiesintheearlydetection
of system anomalies. Some models can detect anomalies earlier
than others.
In summary, the major contributions of this work are as follows:
‚Ä¢We conduct an extensive evaluation of five representative deep
learning models for log-based anomaly detection.
‚Ä¢We conclude that the existing models are not evaluated compre-
hensivelyanddonotgeneralizewellindifferentexperimental
settings.
‚Ä¢Basedontheevaluationresults,wepointouttheadvantagesand
disadvantagesofexistingmodels,andsuggestfutureresearch
work for log-based anomaly detection.
2 LOG-BASED ANOMALY DETECTION WITH
DEEP LEARNING
2.1 Representative Models
In recentyears, manydeep learning-based modelshave beenpro-
posed to analyze log data and detect anomalies [ 6,12,27,39,48,
50,54].Someofthesemodelsusesupervisedlearningtechniques
(such as LogRobust [ 54] and CNN [ 33]), while others employ semi-
supervisedapproaches(suchasDeepLog[ 12],LogAnomaly[ 36])or
unsupervised approaches [ 13]. Some recent representative models
are as follows:DeepLog.Duetal.[ 12]proposedtoutilizeanLSTMmodelto
learn the system‚Äôs normal executions by predicting the next log
event given preceding events. It detects anomalies by determining
whetherornotanincominglogeventviolatesthepredictionresults
of the LSTM model. Their experimental results show that DeepLog
can achieve an F-measure of 0.96 on the HDFS dataset.
LogAnomaly.Mengetal.[ 36]proposedLogAnomaly,which
useslogcountvectorsasinputstotrainanLSTMmodel.Theyalso
proposed template2vec,asynonymsandantonymsbasedmethod,
to represent log templates as semantic vectors to match new log
events with existing templates. Like DeepLog, a forecasting-based
detectionmodelisdesignedtopredictthenextlogevent,andiftheexaminedlogeventviolatesthepredictionresults,itwillbemarked
as an anomaly. LogAnomaly can achieve F-measures of 0.95 and
0.96 on HDFS and BGL datasets, respectively.
PLELog.Yangetal.[ 48]addressedtheissueofinsufficientlabels
viaprobabilisticlabelestimationanddesignedanattention-based
GRUneuralnetworktodetectanomalies.TheGRU-baseddetection
model is built to classify log sequences into two classes, normal orabnormal. Their experimental results indicate that PLELog outper-
forms existing semi-supervised methods and achieves high perfor-
mance on HDFS and BGL datasets (i.e., 0.96 and 0.98, respectively).
LogRobust.Zhangetal.[ 54]incorporatedapre-trainedWord2vec
model,namelyFastText[ 23],andcombineditwithTF-IDFweights
to learn the representation vectors of log templates. Then, these
vectors were input to an Attention-based Bi-LSTM model to detect
anomalies.TheexperimentalresultsshowthatLogRobustcanad-
dresstheinstabilityoflogeventsandachieveF-measuresof0.99
on the original HDFS dataset and 0.89-0.96 on synthetic datasets.
CNN.L ue ta l .[ 33] applied a Convolutional Neural Network
(CNN) for log-based anomaly detection. Logs are grouped into
sessions,thentransformedintoatrainablematrix.ACNNmodel
istrainedusingthismatrixasinputstoclassifyalogsequenceinto
normal or abnormal. The CNN model can achieve an F-measure of
0.98 on the HDFS dataset.
In this study, we systematically evaluate the above five models.
DeepLog and LogAnomaly adopt a forecasting-based approach
(i.e., detecting anomalies by predicting the next log event given
preceding log sequences), while PLELog, LogRobust, and CNN are
classification-based models (i.e., detecting anomalies by classifying
log sequences). Apart from these models, there are some other
deeplearning-basedapproaches.Forexample, Logsy[39]utilizes
the Transformer network [ 46] to detect anomalies from log data.
AutoEncoder has been employedin [ 13] to detectlog anomalies in
an unsupervised manner along with Isolation Forest. SwissLog [27]
proposes to use a dictionary-based log parser and an Attention-
basedBi-LSTMnetworktodetectanomaliesfordiversefaults.As
theirsourcecodeisnotpubliclyavailable,wedonotexperimentally
evaluate these models in this study.
2.2 The Common Workflow
The common overall framework of DL models for log-based anom-
alydetectionisshowninFigure 2.Generally,theframeworkcon-
sists of four steps: (1) log parsing, (2) log grouping, (3) log repre-
sentation, (4) anomaly detection through DL models.
2.2.1 Log parsing. Logsare semi-structuredtexts, whichcontain
variousfieldssuchastimestampandseverity.Tofavordownstream
1357
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:28:46 UTC from IEEE Xplore.  Restrictions apply. Log-based Anomaly Detection with Deep Learning: How Far Are We? ICSE ‚Äô22, May 21‚Äì29, 2022, Pittsburgh, PA, USA
1. Log Parsing
...
i. 081109 205931 13 INFO dfs.DataBlockScanner:
Verification succeeded for blk_4980916519894289
......
i. Verification succeeded for *
...
2. Log Grouping
Log sequenceFixed windows
Sliding windows
Session windows3. Log Representation
Sequential Vectors
Quantitative Vectors
Semantic Vectors
4. Deep Learning Models
RNN Transformer CNNFeature Extraction
Anomaly?
Figure2:Log-basedAnomalyDetectionWorkflowwithDeep
Learning: The Common Workflow
tasks, log parsing is applied to automatically convert each log mes-
sage intoa specificevent template (constantpart) associatedwith
parameters (i.e., variable part). For example, the log template ‚ÄúVer-
ification succeeded for ‚àó"can be extracted from the log message
‚ÄúVerificationsucceededforblk_4980916519894289" inFigure 2. Here,
‚Äú‚àó"denotes the position of a parameter.
Therearemanylogparsingtechniques,basedonfrequentpat-
ternmining[ 10,38,45],clustering[ 16,42,44],andheuristics[ 17,
22,34]. The heuristics-based approaches make use of the charac-
teristicsoflogsandhavebeenfoundtoperform better thanother
techniques in terms of accuracy and time efficiency [55].
2.2.2 Log Grouping. Themainpurposeofthisstepistoseparate
logsintovariousgroups,whereeachgrouprepresentsafinitechunk
of logs [19]. These groups are called log sequences, from which fea-
tures are extracted and fed into anomaly detection models. As
introduced in [ 19], three types of windows are applied (see Figure
2) for log grouping, including: (1) Fixed window. Logs are grouped
byfixedwindowsaccordingtotheiroccurrences.Theoccurrenceis
defined by the timestamp of log messages or by the order of its ap-
pearance in the dataset. Each window has a fixed size (i.e., window
size), which means the time span or the number of logs. (2) Sliding
window.Slidingwindowsconsistoftwoattributes:windowsizeand
stepsize.Thewindowsizecanbethetimespanorthenumberof
logsinalogsequence,whilethestepsizeistheforwardingdistance.
(3)Sessionwindow. Differentfromfixed/slidingwindows,session
windows are based on the identifier of logs. Identifiers are used to
grouplogsinthesameexecutionpath.Forexample,HDFSlogsuse
block_idto record the execution path.
2.2.3 Log Representation. Afterloggrouping,logsarerepresented
in different formats required by DL models. Existing DL-based
anomalydetectionmodelsconvertlogsintothreemaintypes:(1)
sequentialvectors,(2)quantitativevectors,and(3)semanticvectors.
Sequentialvectorsreflecttheorderoflogeventsinawindow.Forexample, DeepLog [ 12] assigns each log event with an index, then
generates a sequential vector for each log window. Quantitative
vectors are similar to log count vectors, which are used to hold the
occurrence of each log event in a log window. LogAnomaly [ 36]
leverages both sequential and quantitative vectors to detect anom-
alies. Different from them, semantic vectors are acquired from
languagemodelstorepresent thesemanticmeaningoflogevents.
Eachlogwindowisconvertedintoasetofsemanticvectorsforthe
detectionmodels.Forinstance,LogRobust[ 54]adoptsapre-trained
FastText [ 23] model to compute the semantic vectors of log events.
2.2.4 Deep Learning Models. Afterthelogrepresentationphase,
theextractedfeaturesarefedtodeeplearningmodelsfortheanom-
alydetectiontask.AvarietyofDLtechniqueshavebeenappliedto log-based anomaly detection: 1) RNN. Recurrent Neural Net-
works (RNNs), including its variants Long Short-Term Memory
(LSTM) [
21] and Gated Recurrent Units (GRUs) [ 9], are neural
networksdesignedtohandlethesequentialinputswitharbitrary
length. Bi-directional RNN is used to represent the sequential text
inbothdirections(i.e.,forwardandbackward).RNNanditsvariantshavebeenappliedinmanystudiesonlog-basedanomalydetection.
Specifically, DeepLog [ 12] and LogAnomaly [ 36] use LSTM to pre-
dict the next log event. LogRobust [ 54] applies an Attention-based
Bi-LSTMnetworktohandletheinstabilityoflogs,whilePLELog
adoptsGRUstobuildaclassificationmodel.2) CNN.AConvolu-
tionalNeuralNetwork(CNN)usesconvolutionoperationtoextractmeaningful local patterns of input. In [
33], a CNN model originally
designed for sentence classification [ 25] is applied to log-based
anomalydetection.3) Transformer.TheTransformernetworkhas
madesignificantprogressinneuralmachinetranslationandrelated
pretraining tasks in recent years. It has been applied in [ 15,26,39]
for log-based anomaly detection.
3 STUDY DESIGN
3.1 Motivation
Recent studies [ 27,36] have shown that deep learning-based ap-
proachescanachievehighaccuracy(e.g.,F-Measurehigherthan
90%) on commonly used datasets (e.g., the HDFS dataset). These
results seem to suggest that the problem of anomaly detection can
besolvedalmostperfectlythroughdeeplearning.Toexplorethe
actualeffectivenessofexistingDLmodelsforlog-basedanomaly
detection ( ùëöùëúùëëùëíùëôùë†for short in the rest of the paper), we would like
to evaluate the models from the following aspects associated with
the common workflow of the models:
3.1.1 The selection of training data. The results of anomaly detec-
tion could be affected by the selection of training data. In somestudies(suchasDeepLog[
12],LogRobust[ 54],PLELog[ 48],and
CNN [33]), the training and test data are selected based on the
timestamp of logs. We call it ùëê‚Ñéùëüùëúùëõùëúùëôùëúùëîùëñùëêùëéùëô strategy. Other stud-
ies [8,15,36] apply fixed/sliding windows to group log events into
log sequences, then shuffle all logs sequences before splitting them
into training and testing sets. We call it ùëüùëéùëõùëëùëúùëöstrategy. The ran-
dom strategy allows models to see more log events and achieve
high accuracy (e.g.,higher than 88% F-measure for allmodels [8]).
However, this strategy may lead to the data leakage problem in the
trainingphase.Thatis,itispossiblethatthetrainingsetcontains
parts of future data, and the testing set contains parts of past data,
1358
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:28:46 UTC from IEEE Xplore.  Restrictions apply. ICSE ‚Äô22, May 21‚Äì29, 2022, Pittsburgh, PA, USA Van-Hoang Le and Hongyu Zhang
thus making it not suitable for real-world scenarios where we only
usehistorical logsto builda detectionmodel.The effectivenessof
themodelswithdifferenttrainingdataselectionstrategies(random
or chronological) should be investigated.
3.1.2 The grouping of log data. Thelogdatacanbegroupedinto
sequencesbysession,sliding,orfixedwindows.Choosingaproper
windowsizeischallenging.Forexample,ifthewindowsizeissmall,
themodelsfacedifficultyincapturingthoseanomaliesthatspan
multiple sequences. On the other hand, if the window size is large,
logsequencesmightincludemultipleanomaliesandconfusethe
detection scheme [ 31]. In this work, we evaluate many window
sizes,includingthewindowsizesfrom20to200logmessagesas
wellasthewindowsizesof0.5hourand1hour,andalsosession
window [19, 50].
3.1.3 The imbalanced class distribution. In literature, much re-
search work [ 14,51,52] has shown that in a large software system,
thedistributionoffaultsisskewed-thatasmallnumberofmodules
accounts fora large proportionof the faults. Inour work, wefind
thatthedistributionofanomaliesisalsoskewed.Theanomalous
logsequencesusuallyaccountfortheminorityofthedataset,which
can be only from 0.5% to 15% of a dataset, as shown in Section 3.3.
The highly imbalanced data imposes challenges for anomaly detec-
tion.Ingeneral,itisdifficultforamachinelearningtechniqueto
identify a small number of anomalies from a large amount of logs.
The imbalance between normal and abnormal classes could cause
the model to perform poorly.
Thehighlyimbalancedclassdistributionalsohasimplications
for evaluation metrics. The performance of log-based anomalydetectionmodelsisusuallymeasuredbyPrecision,Recall,andF-
Measure[ 12,15,19,36,54].However,previousstudies[ 53]pointed
out that prediction results may not always be satisfactory in the
presenceofimbalanceddatadistribution.Inthiswork,wewould
like to explore if the commonly-used metrics can effectively evalu-
atetheeffectivenessofalog-basedanomalydetectionmodelunder
the scenario of imbalanced class distribution.
3.1.4 The quality of data. Fortheevaluationoflog-basedanomaly
detection models, labeled data is required. The commonly-used
publicdatasets(suchasHDFSandBGL)aremanuallyinspectedand
labeled by engineers. Data noise (false positives/negatives) may be
introduced during the manual labeling process. Although the data
noises only occupy a small portion of logs, they could downgrade
the performance of existing models. The noise can be from theerrors in the preprocessing phase (i.e., log parsing). The logging
statementscouldalsofrequentlychangeduringsoftwareevolution,
resulting in new log events that were not seen in the training
phase[36,54].Zhangetal.[ 54]foundthat30.3%oflogsarechanged
in the latest version based on their empirical study on Microsoft
onlineservicesystems.Leetal.[ 26]foundthatlogparsingerrors
can lead to many incorrect log events, thus downgrading anomaly
detection performance. Therefore, we would like to investigate the
effectiveness of the models with different degrees of data noise.
3.1.5 Early detection ability. System anomalies can affect the nor-
mal operations of upper-layer software applications and signifi-
cantly affect users‚Äô experience. If no actions are taken, more severe
problems or even service interruptions may occur. Therefore, it
isimportantthattheanomaliesarecapturedearlier,sothatmoremitigation actions could be taken. An effective anomaly detection
model should be able to identify the early signals of system anom-
alies, detect the anomalies as early as possible, and meanwhile
achieve high detection accuracy. This is especially essential for the
online detection scenario, where anomalies are detected on the fly.
Because of the above concerns, we argue that the capabilities
of deep learning-based techniques for anomaly detection shouldbere-evaluated.Inthiswork,wedesignexperimentstomeasurethe impact of these factors on five representative DL models for
log-based anomaly detection.
3.2 Evaluated Models
In this study, we evaluate the five representative models described
in Section 2.1, namely DeepLog, LogAnomaly, PLELog, LogRobust,
and CNN. Thesemodels have their source codepublicly available,
andwecanconfirmthecorrectnessofsourcecodesbyreproduc-
ing results presented in their original paper. Specifically, we adopt
the public implementations [ 3,4] of DeepLog and LogAnomaly.
ForLogAnomaly,thetemplate2vecmodelistrainedwithdomain-
specificantonymsandsynonymsaddingbyoperators.Sincethis
informationisunavailable,weuseapre-trainedFastTextword2vec
mode[23]tocomputethesemanticvectorsoflogtemplates.The
templatevectoriscalculatedastheweightedaverageofthevectors
of the template‚Äôs words. For LogRobust, we adopt the implementa-
tion provided by its authors and convert it into a PyTorch-basedimplementation.ForPLELog,weleverageitspublicimplementa-
tion[1].Formodelswhosehyperparametersettingsarereported
in their paper, we use the same hyperparameter values. Otherwise,
we tune their hyperparameters empirically.
3.3 Datasets
Toevaluatethestudiedmodelsforlog-basedanomalydetection,we
selectfourpublicdatasets[ 2,20],namelyHDFS,BGL,Thunderbird,
and Spirit. The details of each dataset are as follows:
‚Ä¢HDFS (Hadoop Distributed File System) dataset is produced
from more than 200 Amazon EC2 nodes. In total, the HDFS
dataset consists of 11,175,629 log messages. These log messages
formdifferentlogwindowsaccordingtotheir block_id,reflecting
aprogramexecutionintheHDFSsystem.Thereare16,838blocks
of logs (2.93%) in this dataset indicating system anomalies.
‚Ä¢BGL (Blue Gene/L) dataset is a supercomputing system log
dataset collected byLawrence Livermore National Labs(LLNL)
[40].Thedatasetcontains4,747,963logmessages.Eachmessage
in the BGL dataset was manually labeled as either normal oranomalous. There are 348,460 log messages (7.34%) that were
labeled as anomalous.
‚Ä¢SpiritdatasetisanaggregationofsystemlogdatafromtheSpirit
supercomputing system at Sandia National Labs [ 40]. There are
more than 172 million log messages labeled as anomalous onthe Spirit dataset. In this paper, we use a small set containing
thefirst5millionloglinesoftheoriginalSpiritdataset,which
contains 764,500 abnormal log messages (15.29%).
‚Ä¢Thunderbird datasetisanopendatasetoflogscollectedfroma
Thunderbird supercomputer at Sandia National Labs (SNL) [ 40].
Thelogdatacontainsnormalandabnormalmessageswhichare
manuallyidentified.Thunderbirdisalargedatasetofmorethan
200 million log messages. We leverage 10 million continuous
1359
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:28:46 UTC from IEEE Xplore.  Restrictions apply. Log-based Anomaly Detection with Deep Learning: How Far Are We? ICSE ‚Äô22, May 21‚Äì29, 2022, Pittsburgh, PA, USA
log lines for computation-time purposes, which contain 4,934
abnormal log messages (0.49%).
Table1summarizes the statistics of datasets used in our experi-
ments.
3.4 Research Questions
Thegoalofthisstudyistoanalyzetheperformanceoftherepre-
sentative deep learning models for log-based anomaly detection
models. We design the following research questions in accordance
with the evaluation aspects described in Section 3.1.
RQ1: How do the existing approaches perform with dif-
ferent training data selection strategies? We want to evaluate
whetherornotthestudiedmodelsareabletoachievegoodaccu-
racywithdifferenttrainingdataselectionstrategies.Tothisend,
weconductexperimentswithtwodifferentstrategiesfortraining
dataselection: (1)Randomselection :Foreachdataset,wefirstsort
logs by timestamps, and then apply the fixed window grouping
technique to generate log sequences. Next, these log sequences are
shuffled, and split into training/testing sets with the ratio of 80:20.
(2) Chronological selection : For each dataset, we utilize the first 80%
of raw logs (that appear in chronological order) for training and
the remaining 20% for testing. Next, we apply the fixed window
groupingtechniquetogeneratelogsequences.Wedonotshuffle
the generated log sequences in this strategy. Therefore, we can
guaranteethatonlyhistoricallogsareusedinthetrainingphase,
and there are no future logs used in this phase.
In this RQ, we experiment on the BGL, Thunderbird, and Spirit
datasets. The window size for fixed window grouping is set to 1
hour.AstheHDFSdatasetdoesnotcontaintimestampinformation,
the chronological selection cannot be applied to HDFS, and thus it
is not used in this RQ.
RQ2: How do theexisting models perform with different
data grouping methods? To evaluate the impact of different data
groupingmethodsontheperformanceofanomalydetectionmodels,
wechoosethefollowingthreedatagroupingmethods,including:
(1)fixed-windowgroupingwiththewindowsizeof1hour(asused
in RQ1) and 0.5 hour, (2) fixed-window grouping with the window
sizevaryingfrom20to200logmessages,and(3)sessionwindow
grouping.Forthefirstcase,weuseBGL,Spirit,andThunderbird
datasets. For the case of session windows, we use block_idand
node_idto group logs on HDFS and BGL datasets, respectively.
RQ3: Can the existing approaches work with different
class distributions? As shown in Section 3.3, our subject datasets
representhighlyimbalancedclassdistributionswithanomalyra-
tios that can be only 0.1% (i.e., on the Thunderbird dataset). To
perform a more systematic evaluation, we simulate different imbal-
ancedscenariosbyrandomlyremovingthenormal/abnormallog
sequencesfromthesubjectdatasets.Forareal-worldproduction
system, the number of anomalies is much less than the number of
normal events [ 28,51,52]. Therefore, we vary the imbalance ratio
from0.1%to15%,whichindicatesthepercentageofanomaliesin
thedataset.Inthisway,wecreatesixsyntheticdatasetswiththe
imbalance ratio of 0.1%, 0.5%, 1%, 5%, 10%, and 15%.
RQ4:Canexistingapproachesworkwithdifferentdegrees
of data noise? To evaluate the impact of mislabeled logs on theperformance of the studied models, we randomly add some anom-
alies (from 1% to 10%) into the training data for semi-supervised
methods.Forsupervisedmethods,werandomlychangethelabelof
a specific portion (from 1% to 10%) to simulate the mislabeled logs.
Inthisway,wecreatefivesyntheticdatasetswiththemislabeled
proportion of 1%, 2%, 5%, 8%, and 10%. Moreover, to measure the
impact of noises from log parsing errors, we experiment with four
commonlyusedlogparsers,includingDrain[ 17],Spell[11],AEL
[22], and IPLoM [34].
RQ5: How early can the existing models detect anomalies
inonlinedetection? AsdescribedinSection 3.1.5,amodelshould
notonlydetectanomaliespreciselybutalsoshouldbeabletodetectanomaliesasearlyaspossiblesothatmoremitigationactionscould
be taken. Therefore, in this RQ, we evaluate the studied models on
four datasets to investigate their ability in early detection of anom-alies. To this end, we record the number of examined log messages
before each model raises an anomaly alert for a log sequence, in
the online detection setting.
3.5 Experimental Setup
In our experiments, we preprocess the log data and conduct DL-
based anomaly detection as follows:
(1) Log Parsing. To extract log templates from log data, we use
the log parser Drain [ 17] with the default parameter settings [ 5].
Thelogdataisdenotedby ùêø={ùëô1,ùëô2,...,ùëô ùëñ,...,ùëô ùëÅùêø}andcontains ùëÅùêø
entries (i.e., lines) of log messages. Each log message ùëôùëñis parsed
into a log template ùê∏(ùëôùëñ), which is denoted ùëíùëñfor short.
(2) Log Grouping. We apply session window to group logs
intheHDFSdatasetusing block_id.Eachsessionislabeledusing
groundtruth.Forotherdatasets(BGL,Spirit,andThunderbird),weusethefixedwindowstrategytogrouplogdatainto
ùëÅùëÜchunks(i.e.,
log sequences), denoted as ùëÜ={ùë†1,ùë†2,...,ùë† ùë¢,...,ùë† ùëÅùëÜ}, whereùë†ùë¢=
{ùëíùëñ,ùëíùëñ+1,...,ùëí ùëó}is a set of log templates. According to the ground
truth (labeled by domain engineers), a log sequence is abnormalifitcontainsananomalouslogmessageaccordingtotheground
truth (labeled by domain engineers). ùêπdenotes the size of each log
sequence.Inthisstudy,wevarythevalueof ùêπfrom20,100,200log
messages, to 0.5 and 1 hour depending on each research question.
(3)LogRepresentation. Logsequencesarenowconvertedinto
numerical vectors, which can be input to a DL model. DeepLog
transformslog sequences intosequential vectorsbyassigning each
log event with an index. LogAnomaly leverages both sequential
vectorsandquantitativevectorstotrainthemodel.Theoccurrence
of each log event is counted and forms the quantitative vectors,
whichrepresentthesystemexecutionbehaviors[ 36].PLELogex-
tracts the semantic vectors of log templates by using a pre-trained
Glovemodel[ 41].Similarly,LogRobustandCNNalsoconvertlogse-
quences into semantic vectors using a pre-trained word2vec model.
We adopt the pre-trained FastText [ 23] model to compute the se-
mantic vectors for LogRobust and CNN.
(4)DeepLearningModel. Inthisstep,dependingonthemethod,
a DL model is trained using the corresponding feature vectors gen-
eratedfromthepreviousphase.DeepLogandLogAnomalyhave
twoLSTMlayerswith128neurons.LogRobustcontainsatwo-layerBi-LSTMwith128neuronsandanattentionlayer.PLELogutilizesa
one-layer GRU network. CNN has three Convolutional layers with
different filters and a max-pooling layer for feature extraction.
1360
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:28:46 UTC from IEEE Xplore.  Restrictions apply. ICSE ‚Äô22, May 21‚Äì29, 2022, Pittsburgh, PA, USA Van-Hoang Le and Hongyu Zhang
Table 1: The statistics of datasets used in the experiments
Dataset#Log Events Grouping # Log sequences# Avg. seq.
lengthTraining Data Testing Data
#Log sequences # Anomaly # Log sequences # Anomaly
HDFS 48 session (random) 575,061 19.4 460,048 13,521 (2.9%) 115,013 3,317 (2.9%)
BGL 1,8471 hour (random) 3,606 1,307.1 2,884 536 (18.6%) 722 129 (17.9%)
1hour (chron.) 3,606 1,307.1 2,625 496 (18.9%) 981 171 (17.4%)
100logs (chron.) 47,135 100 37,708 4,009 (10.6%) 9,427 817 (8.7%)
session(random) 69,252 68.1 55,401 25,066 (45.2%) 13,851 6,309 (45.5%)
Spirit 2,8801 hour (random) 1,173 4,262.6 1,001 882 (88.11%) 172 71 (41.28%)
1hour (chron.) 1,173 4,279.0 938 760 (81.02%) 235 192 (81.7%)
100logs (chron.) 50,000 100 40,000 19,384 (48.5%) 10,000 346 (3.5%)
Thunderbir d 4,9921 hour (random) 209 47,651.5 167 42 (25.1%) 42 6 (14.29%)
1hour (chron.) 209 47,651.5 169 40 (23.7%) 40 8 (20.0%)
100logs (chron.) 99,593 100 79,674 816 (1.0%) 19,919 27 (0.1%)
Note:chron. denotes the chronological strategy.
Toavoidbiasfromrandomness,weperformeachexperimentfive
times and report the average results. We conduct our experiments
onaWindowsServer2012R2withIntelXeonE5-2609CPU,128GB
RAM, and an NVIDIA Tesla K40c.
3.6 Evaluation Metrics
Tomeasuretheeffectivenessofmodelsinanomalydetection,we
usethePrecision,Recall,Specificity,andF1-Scoremetrics,which
are defined as follows:
‚Ä¢Precision: the percentage of correctly detected abnormal log
sequences amongst all detected abnormal log sequences by the
model.ùëÉùëüùëíùëê=ùëáùëÉ
ùëáùëÉ+ùêπùëÉ.
‚Ä¢Recall:the percentage of log sequences that are correctly identi-
fied as anomalies over all real anomalies. ùëÖùëíùëê=ùëáùëÉ
ùëáùëÉ+ùêπùëÅ.
‚Ä¢Specificity : the percentage of log sequences that are correctly
identified as normal over all real normal sequences.
ùëÜùëùùëíùëê=ùëáùëÅ
ùëáùëÅ+ùêπùëÉ.
‚Ä¢F-Measure: the harmonic mean of Precision andRecall.
ùêπ1=2‚àóùëÉùëüùëíùëê‚àóùëÖùëíùëê
ùëÉùëüùëíùëê+ùëÖùëíùëê.
TP(TruePositive)isthenumberofabnormallogsequencestheare
correctlydetected bythemodel. FP(FalsePositive) isthenumber
ofnormallogsequencesthatarewronglyidentifiedasanomalies.
FN (False Negative) is the number of abnormal log sequences that
are not detected by the model.
4 RESULTS AND FINDINGS
4.1 RQ1: Performance with different training
data selection strategies?
ForRQ1,weapplyfixedwindowgroupingwiththesizeof1hour
togeneratelogsequencesonBGL,Spirit,andThunderbirddatasets
(seeTable 1).Thetrainingdataisselectedbyrandomorchronolog-
ical selection. The experimental results are shown in Table 2.
We find that, for semi-supervised models (i.e., LogAnomaly,
DeepLog,andPLELog),theresultswithrandomselectionaremuch
betterthanthosewithchronologicalselection.Forexample,DeepLog
achieves an F-measure of 0.927 with the random selection of train-
ing data on the BGL dataset. When training and testing sets areseparated by the time order (i.e., chronological selection), the F-
measuredropsto0.426.Thereasonisthatwithrandomselection,themodelscanseefuturelogeventsinthetrainingphase(i.e.,data
leakage),thereforetheycanmakemoreaccuratepredictions.Be-
sides, DeepLog and LogAnomaly train the models using the index
oflogevents(i.e.,sequentialandquantitativevectors)andignore
thesemanticmeaning oflogsduringthe trainingphase.DeepLog
marksanynewlogeventsasanomaliesandproducesmanyfalse
alarms. LogAnomaly can simply match some unseen log events
withthoseappearinginthetrainingphase,butitisnotadequate
compared to those models that are trained through the semantic
understanding of logs.
Thesupervisedmodels(i.e.,LogRobustandCNN)performmuch
betterthanthesemi-supervisedmodelsonbothstrategiessincethe
models are trained with a large amount of normal and abnormal
data.Forexample,thesetwomodelsachievearound0.94F-measure
ontheThunderbirddatasetwiththechronologicalsetting,while
othersperformpoorly.Anotherreasonfortheseresultsistheadvan-
tages of semantic vectors used by these models, which can identify
thesemanticallysimilarlogeventsandalsodistinguishdifferentlogevents[
54].Still,wecanseethatingeneraltheresultswithrandom
selection are better than those with chronological selection.
Our experimental results confirm that models perform better
withrandomselection.Thedataleakageproblemisareasonforthe
goodperformanceofsomeDL-basedloganomalydetectionmodels
(e.g., LogAnomaly [ 36], which uses the random strategy in their
evaluation). Due to this problem, we suggest that chronologicalselection should be applied to evaluate the effectiveness of the
modelsinreal-worldscenarios.Hence,forotherRQs,wewillapply
the chronological selection to group log messages.
Summary. Thetrainingdataselectionstrategieshavesignifi-
cantimpactonthesemi-supervisedlog-basedanomalydetection
models. Although the random selection strategy leads to better
results than the chronological selection strategy, it could cause
thedataleakageproblemandfailtoevaluatetheeffectiveness
of the models in real-world scenarios.
4.2 RQ2: How do the existing models perform
with different data grouping methods?
In RQ1, we use the fixed window size of 1-hour logs. In this RQ,
wetrainthemodelsonthreedatasetsusingchronologicalselection
1361
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:28:46 UTC from IEEE Xplore.  Restrictions apply. Log-based Anomaly Detection with Deep Learning: How Far Are We? ICSE ‚Äô22, May 21‚Äì29, 2022, Pittsburgh, PA, USA
Table 2: Comparison of model performance with random selection and chronological selection of training data
ModelBGL
(random/chronological selection)Spirit
(random/chronological selection)Thunderbird
(random/chronological selection)
Prec Rec Spec F1 Prec Rec Spec F1 Prec Rec Spec F1
DeepLog 0.952/0.270 0.903/0.988 0.990/0.437 0.927/0.426 0.867/0.438 1.0/1.0 0.386/0.090 0.929/0.609 0.232/0.200 1.0/1.0 0.007/0 0.369/0.333
LogAnomaly 0.961/0.313 0.903/0.798 0.992/0.551 0.931/0.483 0.882/0.438 1.0/1.0 0.456/0.090 0.937/0.609 0.234/0.229 1.0/1.0 0.014/0.156 0.371/0.371
PLELog 0.963/0.702 0.935/0.791 0.992/0.899 0.949/0.744 0.956/0.931 0.974/0.767 0.818/0.690 0.965/0.841 0.584/0.250 0.344/1.0 0.729/0.250 0.414/0.400
LogRobust 0.972/0.994 0.984/0.942 0.995/0.999 0.981/0.967 0.994/0.985 0.978/0.915 0.979/0.990 0.986/0.949 0.803/0.900 0.921/1.0 0.931/0.969 0.941/0.947
CNN 0.994/0.871 0.963/0.947 0.999/0.970 0.978/0.908 1.0/0.986 1.0/1.0 1.0/0.990 1.0/0.993 1.0/0.889 0.875/1.0 1.0/0.969 0.933/0.941
with fixed window grouping of various sizes (i.e., 20 log messages,
100 log messages, 200 log messages, and 0.5-hour logs). Table 3
shows the results.
Table 3: Results of models with fixed-window grouping of
different sizes
ModelBGL Spirit Thunderbird
20l 100l 200l 0.5h 20l 100l 200l 0.5h 20l 100l 200l 0.5h
DeepLogP 0.128 0.166 0.192 0.209 0.504 0.500 0.175 0.291 0.004 0.017 0.005 0.162
R 0.995 0.988 0.987 0.984 0.776 0.861 0.985 1.0 0.938 0.963 1.0 1.0
S 0.539 0.53 0.528 0.481 0.986 0.969 0.747 0.068 0.899 0.922 0.005 0.088
F1 0.227 0.285 0.322 0.345 0.611 0.633 0.298 0.450 0.008 0.033 0.010 0.279
LogAnomalyP0.136 0.176 0.203 0.276 0.498 0.508 0.198 0.330 0.004 0.025 0 0.154
P 0.970 0.985 0.985 0.973 0.773 0.870 0.981 1.0 0.938 0.963 1.0 1.0
S 0.581 0.562 0.559 0.643 0.986 0.970 0.783 0.225 0.891 0.950 0.005 0.029
F1 0.239 0.299 0.336 0.430 0.606 0.642 0.330 0.496 0.008 0.050 0.009 0.267
PLELogP0.592 0.595 0.862 0.760 0.375 0.371 0.141 0.516 0.429 0.826 0.692 1.0
R 0.882 0.880 0.844 0.785 0.286 0.824 0.552 0.663 0.688 0.704 0.360 0.500
S 0.958 0.968 0.985 0.965 0.991 0.950 0.816 0.763 1.0 1.0 1.0 1.0
F1 0.708 0.710 0.853 0.772 0.325 0.511 0.225 0.581 0.528 0.760 0.474 0.667
LogRobustP0.616 0.696 0.684 0.819 0.947 0.943 0.751 0.989 0.377 0.318 0.289 0.458
R 0.969 0.968 0.963 0.946 0.979 0.954 0.965 0.989 0.876 1.0 0.960 0.917
S 0.959 0.960 0.949 0.971 0.999 0.998 0.982 0.996 0.999 0.997 0.994 0.809
F1 0.753 0.810 0.800 0.878 0.963 0.947 0.845 0.989 0.531 0.482 0.444 0.611
CNNP0.634 0.698 0.744 0.837 0.954 0.959 0.961 0.989 0.907 0.900 0.720 1.0
R 0.969 0.965 0.965 0.935 0.966 0.948 0.950 0.989 0.813 0.670 0.720 0.667
S 0.962 0.96 0.949 0.974 0.999 0.999 0.998 0.996 1.0 1.0 0.999 1.0
F1 0.767 0.810 0.840 0.883 0.960 0.953 0.955 0.989 0.857 0.766 0.720 0.800
20l:20 logs, 100l: 100 logs, 200l: 200 logs, 0.5h: 0.5-hour logs.
The results show that different window sizes lead to different
performance of detection models. We can observe that, on the BGL
dataset,comparedtotheresultsof1-hour-logssettinginTable 2,the
performance of the models mostly drops. For example, on the BGLdataset, the decrease of F1 measure ranges from 4.84% (PLELog) to
50.31%(LogAnomaly)whenthe100-log-messagessettingisused.
OntheSpiritdataset,DeepLogandLogAnomalyachievethebest
results when using the window size of 100 log messages, while
others perform the best with 0.5-hour-logs setting. Similar results
can be found on the Thunderbird dataset, where the detection
performance is different across different window sizes.
Wenextevaluatetheperformanceofmodelsonsessionwindow
grouping.Logscanbegroupedbasedontheidentifiers(i.e., node_id
andblock_idforBGLandHDFSdatasets,respectively)torepresent
theexecutionpathofatask.Weperformanomalydetectionafter
each session ends [ 50,54]. We do not evaluate this RQ on other
datasets (i.e., Spirit and Thunderbird) because they do not have
theidentifierinformation,suchas block_id,intheirlogmessages.
Table4shows the results.
It is obvious that the results using session windows on the BGL
dataset are better than those using fixed windows. Moreover, com-
pared to the results on the BGL dataset in Table 2, we can findTable4:ResultsofmodelswithsessiongroupingonBGLand
HDFS datasets
ModelBGL HDFS
Prec Rec Spec F1 Prec Rec Spec F1
DeepLog 0.997 1.0 0.997 0.998 0.835 0.994 0.994 0.908
LogAnomaly 0.997 1.0 0.998 0.999 0.886 0.893 0.961 0.966
PLELog 0.995 0.992 0.996 0.994 0.893 0.979 0.996 0.934
LogRobust 1.0 1.0 1.0 1.0 0.961 1.0 0.989 0.980
CNN 1.0 1.0 1.0 1.0 0.966 1.0 0.991 0.982
thattheperformanceisimprovedusingthesessionwindow.The
reason could be that the log events in an execution path exhibitmany relations [
32,50], which can be captured and utilized for
anomalydetection.OntheHDFSdataset,allmodelsalsoachieve
good performance (all F-measure values are higher than 0.9).
Summary. Thedatagroupingmethodscouldhavesignificant
impact on the log-based anomaly detection models. With fixed
windowgrouping,modelstendtoperformunsteadilywhendeal-
ing with different window sizes. Grouping by session windows
could lead to better results.
4.3 RQ3: How do the existing approaches
perform with different class distributions?
Table2showstheresultsonthesubjectdatasets,whichhavedif-
ferent ratios of anomalies. For example, we can see that, on the
Spiritdataset,DeepLogandLog-AnomalyachievehighPrecision,
Recall, and F-measure (all higher than 0.86) with random selection.
However,theSpecificityresultsarelow(lessthan0.5),whichreveal
that the models actually perform poorly: they classify a lot of nor-
mallogsasanomaliesinthisscenario,causingmanyfalsealarms.
This also happens on the Thunderbird dataset when DeepLog and
LogAnomalymarkmostlyallofthenormallogsasanomalies(Speci-
ficity‚âà0). In this RQ, to perform a more systematic evaluation,
we simulate different imbalanced scenarios with the percentage of
anomalies increased from 0.1% to 15%. We use the results on HDFS
and BGL datasets to explain our findings, as shown in Table 5.
From Table 5, we find that when the percentage of anomalies
increases, the performance of the models is better, which is indi-
cated by the increase of all four metrics. For example, the scores of
LogRobust on BGL dataset are improved by 63.8%, 10.1%, 2.6%, and
38.4%in Precision,Recall, Specificity,and F-measure,respectively.
The result shows that it is more difficult to detect anomalies when
the dataset is highly imbalanced.
1362
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:28:46 UTC from IEEE Xplore.  Restrictions apply. ICSE ‚Äô22, May 21‚Äì29, 2022, Pittsburgh, PA, USA Van-Hoang Le and Hongyu Zhang
Table 5: Results with different class distributions
ModelHDFS BGL
0.1% 0.5% 1% 5% 10% 15% 0.1% 0.5% 1% 5% 10% 15%
DeepLogP 0.942 0.562 0.638 0.879 0.941 0.962 0.124 0.156 0.184 0.237 0.276 0.278
R 0.485 0.894 0.995 0.997 0.994 0.994 0.987 0.969 0.991 1.000 0.988 0.959
S 1.000 0.997 0.994 0.993 0.994 0.994 0.424 0.424 0.417 0.407 0.435 0.440
F1 0.640 0.690 0.777 0.935 0.967 0.978 0.221 0.268 0.311 0.383 0.431 0.432
LogAnomalyP0.975 0.656 0.732 0.896 0.950 0.972 0.146 0.183 0.213 0.274 0.321 0.338
R 0.485 0.905 0.946 0.993 0.994 0.993 0.987 0.979 0.957 0.961 0.971 0.924
S 1.000 0.998 0.996 0.994 0.995 0.995 0.523 0.521 0.530 0.534 0.552 0.593
F1 0.647 0.760 0.825 0.942 0.971 0.983 0.255 0.308 0.348 0.427 0.482 0.495
PLELogP0.947 0.910 0.934 0.972 0.974 0.974 0.583 0.564 0.617 0.765 0.786 0.895
R 0.972 0.690 0.794 0.956 1.000 1.000 0.691 0.587 0.706 0.872 0.883 0.817
S 1.000 0.999 0.999 0.999 0.996 0.996 0.915 0.950 0.919 0.942 0.946 0.987
F1 0.786 0.785 0.859 0.964 0.987 0.987 0.632 0.576 0.659 0.815 0.832 0.855
LogRobustP0.519 0.673 0.794 0.926 0.945 0.964 0.500 0.506 0.632 0.787 0.851 0.819
R 0.507 0.646 0.999 1.000 1.000 1.000 0.840 0.856 0.896 0.941 0.959 0.924
S 0.999 0.998 0.997 0.996 0.994 0.994 0.930 0.908 0.969 0.953 0.963 0.954
F1 0.513 0.659 0.885 0.962 0.972 0.982 0.627 0.636 0.741 0.857 0.902 0.868
CNNP0.945 0.675 0.793 0.920 0.953 0.967 0.911 0.974 0.901 0.818 0.860 0.865
R 0.388 0.879 0.947 1.000 0.999 0.999 0.680 0.784 0.791 0.922 0.930 0.901
S 1.000 0.998 0.997 0.996 0.995 0.995 0.994 0.998 0.988 0.963 0.967 0.968
F1 0.550 0.763 0.863 0.959 0.976 0.983 0.779 0.869 0.843 0.868 0.894 0.883
Previousstudiesonsoftwaredefectpredictionmodelsshowthat
predictionresultsmaynotalwaysbesatisfactoryinthepresenceof
imbalanceddatadistribution[ 53].Basically,ahighprobabilityofde-
tection (i.e., true-positive rate) and a low probability of false alarm
(i.e., false-positive rate) do not necessarily lead to high precision
due to the imbalanced class distributions. Our results confirm that
finding. As a consequence, the commonly used evaluation metrics
(Precision, Recall, and F-measure) are not capable of evaluating
models in some imbalanced data scenarios and may lead to impre-
cise evaluation. Therefore, we propose to use an additional metric,
Specificity,toevaluatelog-basedanomalydetectionmodelsmore
comprehensively. Specificity [ 43,49], which is the percentage of
log sequences that are correctly identified normal over all real nor-
malsequences,canmeasuretheprobabilityoffalsealarms.High
Specificitymeansthatmodelscanperformwithalowfalse-positive
(false alarms) rate.
Summary. Highly imbalanced data with a small percentage of
anomalies impedes model performance. Using different metrics
may lead to different conclusions about the model performance.
Some commonly-used metrics, including Precision, Recall, and
F-measure,arenotcomprehensiveenoughforevaluatinglog-
based anomaly detection with highly imbalanced data. More
evaluation metrics such as Specificity should be used for a thor-
ough evaluation.
4.4 RQ4: Can existing methods work with
different degrees of data noise?
4.4.1 The impact of mislabeled logs. We evaluate the studied mod-
els with different degrees of mislabeled logs. In this experiment,
we inject a specific portion of mislabeled logs into the training
data while the testingsets remain the same. Specifically, for semi-
supervisedmethods,whichonlyusenormallogsfortraining,we
put back some anomalies to the training sets. For supervised meth-
ods, we change the label of some anomalies in the training setsto normal. We experiment on all four datasets and find that theperformance of models can greatly decrease if training data con-
tainsmislabeledsamples.WeshowtheresultsonHDFSandBGL
datasets in Figure 3to demonstrate our finding.
Figure 3: Results with different ratios of mislabeled logs
We can find that, on the HDFS dataset, even with just 1% of
mislabeledlogs,theF-measuresofthestudiedmodelsdeclinesig-
nificantly. For example, LogRobust and CNN drop 37.2% and 36.2%
withonly1%ofnoise,respectively.Whenthenoiseratioreaches
10%, the F-measures of LogRobust and CNN drop to only 0.161 and
0.263.Theresultsconfirmthatevenwiththeadvantageofsemantic
understanding,theseDLmodelscanlosetheirperformancewith
onlyasmallproportionofmislabeledlogs.ItisalsotrueforPLELog,
whichleveragesthesemanticmeaningoflogsaswell.Anotherrea-
son for this remarkable reduction is that there are many duplicate
log sequences in the HDFS dataset, thus, labeling any duplicatenormal sequences as anomalies could have a large impact. Inter-estingly, the F-measure of LogAnomaly only drops slightly with
1%ofmislabeledlogs(from0.915to0.897).Whenthemislabeled
ratio is 5% and 10%, LogAnomaly can achieve better F-measurescompared to other models. The reason is that LogAnomaly uses
quantitativevectorstoextractquantitativerelationshipsholding
inlogsalongwithsequentialvectors,thus,allowingthemodelto
predict the possibilities of the next event more precisely.
Summary. Asmallamountofmislabeledlogscanquicklydown-
grade the performance of anomaly detection. Supervised mod-
elsaremoresensitivetomislabeledlogs.Modelsadoptingthe
forecasting-based approach (DeepLog and LogAnomaly) per-
form better with the presence of mislabeled logs.
4.4.2 The impact of log parsing errors. Wealsoevaluatetheimpact
ofdatanoiseintroducedbylogparsingerrors.Logparsingerrors
canleadtoextralogeventsandwronglogtemplates[ 26].Weex-
perimentwithfourcommonly-usedlogparsers,namelyDrain[ 17],
Spell[11],AEL[22],andIPLoM[ 34]onallfourdatasets.Wefind
that the performance of models is highly influenced by log parsers.
To demonstrate our findings, we show the performance of models
with different log parsers on BGL and Spirit datasets in Table 6.
We can observe that the performance of studied models varies a
lot with different log parsers. For example, DeepLog achieves an
F-measureof0.755andaSpecificityof0.545withtheIPLoM[ 34]
parseronSpiritdataset.WhenexperimentingwithDrain[ 17],these
1363
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:28:46 UTC from IEEE Xplore.  Restrictions apply. Log-based Anomaly Detection with Deep Learning: How Far Are We? ICSE ‚Äô22, May 21‚Äì29, 2022, Pittsburgh, PA, USA
Table 6: Results with different log parsers
ModelBGL Spirit
Drain Spell AEL IPLoM Drain Spell AEL IPLoM
DeepLogP 0.270 0.271 0.271 0.273 0.438 0.602 0.413 0.607
R 0.988 0.988 0.988 0.988 1.0 1.0 1.0 1.0
S 0.437 0.44 0.438 0.443 0.099 0.535 0 0.545
F1 0.426 0.426 0.425 0.427 0.609 0.751 0.584 0.755
LogAnomalyP0.313 0.339 0.539 0.548 0.438 0.612 0.413 0.607
R 0.798 0.977 0.965 0.971 1.0 1.0 1.0 1.0
S 0.551 0.599 0.826 0.831 0.099 0.554 0 0.545
F1 0.483 0.504 0.692 0.700 0.609 0.759 0.584 0.755
PLELogP0.702 0.560 0.661 0.655 0.931 0.859 0.863 0.500
R 0.791 0.404 0.854 0.433 0.767 0.859 0.887 0.662
S 0.899 0.938 0.907 0.952 0.690 0.901 0.901 0.535
F1 0.744 0.476 0.744 0.521 0.841 0.859 0.875 0.570
LogRobustP0.994 0.849 0.844 0.726 0.985 0.947 0.986 0.973
R 0.942 0.988 0.982 0.977 0.915 1.0 1.0 1.0
S 0.999 0.963 0.962 0.922 0.990 0.960 0.990 0.980
F1 0.967 0.914 0.908 0.833 0.949 0.973 0.993 0.986
CNNP0.871 0.994 0.942 0.937 0.986 0.959 0.986 0.986
R 0.947 0.965 0.947 0.959 1.0 1.0 1.0 1.0
S 0.97 0.999 0.988 0.986 0.990 0.970 0.990 0.990
F1 0.908 0.979 0.945 0.948 0.993 0.979 0.993 0.993
values drop to 0.609 and 0.099, respectively, although Drain is one
ofthemostaccuratelogparsersaccordingtoarecentbenchmark
study [55]. The results also show that LogRobust and CNN can
handle log parsing noise better than other models. This is prob-
ably because of their use of semantic vectors. Moreover, we findthat different log parsing errors have distinctive impact on thedetection models. The results on BGL and Spirit datasets show
that DeepLog and LogAnomaly perform better with Spell [ 11] and
IPLoM [34] than with Drain. This is because Drain often produces
many extra log events that hinder the performance of DeepLogand LogAnomaly (which use forecasting methods to predict the
nextlogevent).Incontrast,othermodelscanbetterhandleextra
log events but may fail when log parsers produce errors due to
semantic misunderstanding [26].
Summary. Thedatanoisefromlogparsingerrorshasimpacton
the performance of models. Extra log events can quickly down-
gradetheperformanceofforecasting-basedmodels.Methods
using semantic vectors can better handle log parsing errors.
4.5 RQ5: How early can the existing models
detect anomalies in online detection?
To answer RQ5,we evaluate the five studied modelsusing a fixed-
window grouping of different sizes (from 20 logs to 1-hour logs).
Werecordthenumberofexaminedlogmessagesbeforeeachmodel
can detect an anomaly in an online detection mode (i.e., detecting
anomaliesonthefly).Figure 4usestheresultswith100-logsand
0.5-hour-logs settings to explain our findings.
We find that DeepLog can detect anomalies the earliest, with
an average of 14.3 and 90.6 log messages with 100-logs and 0.5-
hour-logssetting,respectively.LogAnomaly,whichisalsobased
on predicting the next log events, detects anomaly a bit later since
itrequirestocapturebothsequentialandquantitativerelationships(a) Data grouping with 100 logs
(b) Data grouping with 0.5-hour logs
Figure 4: The number of examined log messages before each
model can detect an anomaly
inordertomarkalogsequenceasabnormal.Incontrast,theuse
of semantic vectors could make other models, including LogRo-bust, CNN, and PLELog, detect anomalies much later, as shownin Figure 4. These classification-based models require to capture
features of anonymous behavior to detect anomalies, thus tendto raise alarms closer to the time the anomaly happens. Figure 5
shows a case study on an anomalous log sequence (with the sizeof 100 logs) on BGL dataset. Five models are applied to identify
whetherthelogsequenceisabnormalornot.WefindthatDeepLog
raises analarm afterexamining 11logs, followed byLogAnomaly,
LogRobust,andCNN,whilePLELogisthelastmodelthatraisesan
alarm (after 64 log messages arrived).
As efficiency is critical in online anomaly detection, we also
evaluatethestudiedmodelsbyrecordingthetimespentonboth
thetrainingandtestingphases.OnBGLdataset,DeepLogspends
19.2 and 0.6 minutes on training and testing. LogRobust and CNN,
whichusehigher-dimensioninputsandmorecomplexnetworks,
consume31.7and28.3minutesontrainingandtesting,respectively.
In the testing phase, LogRobust and CNN only take 0.3 minutes.
LogAnomaly,whichusestwoLSTMnetworkstolearnsequential
and quantitative features, spends 56.7 and 1.6 minutes on train-
ingandtesting.PLELog,whichcontainsaclusteringmoduleand
a GRU module, is the most time-consuming model. Specifically,
PLELog spends 36.6 minutes and 69.1 minutes to train the cluster-
ing and GRU models, respectively. In the testing phase, PLELog
1364
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:28:46 UTC from IEEE Xplore.  Restrictions apply. ICSE ‚Äô22, May 21‚Äì29, 2022, Pittsburgh, PA, USA Van-Hoang Le and Hongyu Zhang
consumes 10.8 minutes to process the BGL dataset. The results
showthatPLELogmaybeinappropriateforonlinedetectiondue
to its incapability of early detection and heavy time cost.
Beginning EndDeepLogLogAnomaly LogRobustCNNPLELog
Figure 5: Timeline for detecting an anomaly in the BGL
dataset
Summary. Different models have different abilities in the
early detection of system anomalies. Forecasting-based models
(DeepLogandLogAnomaly) candetectanomaliesearlier than
classification-based models (PLELog, LogRobust, and CNN).
5 DISCUSSION
5.1 The Advantages and Disadvantages of the
Studied Methods
Based on our findings, we can conclude that all the studied models
donotalwaysworkaswellastheyclaimedintheirpapers.Different
scenarios have different impacts on the performance of anomaly
detection models.We pointout the advantagesand disadvantages
of each model as follows, which are also summarized in Table 7.
DeepLog. The main advantage of DeepLog is that it does not
require any abnormal logs to build a detection model using se-
quential vectors, thus reducing the effort for model construction. It
can also detect anomalies earlier than other models. However, due
to thischaracteristic, DeepLogperforms poorlyon more complex
datasets with a large number of log events (see Section 4.1). Be-
sides,DeepLogisgreatlyimpactedbythelogparsingerrorssinceit
only leverages the index of log templates and ignores the semantic
meaning of log templates (see Section 4.4.2).
LogAnomaly. LogAnomaly uses sequential and quantitative
vectors to train a detection model, which can help reduce the
impact of data noise caused by mislabeled logs. Like DeepLog,
LogAnomaly can detect anomalies early and deal with a largeamount of data since it only trains with normal logs. The main
benefit of LogAnomaly is in the phase of matching similar log tem-
plates using semantic vectors. This feature allows LogAnomaly to
improvetheaccuracybymatchingnewlogtemplateswithanexist-
ing one in the training logs instead of marking them as anomalies
as DeepLog does. However, like DeepLog, LogAnomaly trains the
model using the index of log event (sequential and quantitative
vectors)andcannotlearnthesemanticmeaningoflogtemplates,
so it is highly affected by the log parsing errors (see Section 4.4.2).
Moreover,LogAnomalycannotperformwellonlargedatasetswith
numerous log events (see Section 4.1and4.2).
PLELog. The main advantage of PLELog is that it can learn
knowledge about historical anomalies via probabilistic label esti-
mation. PLELog adopts a clusteringmethod (i.e., HDBSCAN [ 35])
toprobabilisticallyestimatethelabelsofunlabeledlogsequences.
ThisapproachallowsPLELogtoworkwithonlynormallogs.Be-
sides,theuseofsemanticvectorsandattention-basedGRUnetworkmakes PLELog perform more effectively. However, PLELog is time-
consuming since it requires time to train the clustering model (see
Section4.5). Moreover, PLELog cannot cope well with the noise in
training data (see Section 4.4.1), and it does not perform well on
the early detection task.
LogRobust.LogRobustleveragessemanticvectorsoflogtem-
platestogetherwithanattention-basedBi-LSTMmodel.LogRobust
canworkwellwiththenoisefromlogparsingerrorsbyutilizingtheattention-basedBi-LSTMmodel,whichhastheabilitytocap-
ture the contextual information of log sequences. However, as the
main characteristic of supervised models, LogRobust requires both
normal and abnormal data in the training phase, which would cost
much manual labeling effort. Another drawback of LogRobust is
thatitcanbegreatlyaffectedbythenoisefrommislabeledlogs(see
Section4.4.1).
CNN.TheConvolutionalNeuralNetworkcanminemorerela-
tionshipsinlog contextbyleveragingmultiple filters.Theconvo-
lution operation allows CNN to capture not only the correlation
between log templates but also the correlation inside the semantic
embeddingoflogtemplates[ 33].CNNcanachievehighaccuracyon
many datasets using a supervised approach. However, like LogRo-
bust, CNN requires a large amount of labeled data, which is mostly
unavailableinpractice.CNNalsolosesitsaccuracywhendealing
withhighlyimbalanceddataanddatanoise(seeSections 4.3and
4.4.1).
5.2 Future Research Work
Basedonourfindings,weidentifythefollowingresearchchallenges
and also suggest possible solutions:
A variety of datasets. Our findings suggest that more datasets
should be used for a more comprehensive evaluation of log-based
anomalydetectionmodels.Agoodresultononedatasetdoesnot
necessarily reflect good performance on other datasets due to a
variety of data characteristics (e.g., class distributions, noise, etc.).
Limited labeled data. Although supervised learning-based meth-
ods (i.e., LogRobust and CNN) can achieve higher accuracy than
theunsupervisedcounterparts,itistime-consumingandtedious
to manually label the anomalies due to the volume and velocity
of log data. Semi-supervised learning-based models (i.e., DeepLog,
LogAnomaly, and PLELog) can deal with a large amount of data as
they only require normal data. However, the accuracy achieved by
existing methods is rather low in practice, as shown in Section 4.1.
Improving the accuracy of semi-supervised models or designing
unsupervised models is a challenging but essential future work.
Early detection. Our findings show that different models have
differentabilitiesinearlydetectionofsystemanomalies.Theanom-
alies should be predicted as early as possible to allow enough time
for any preparatory or preventive actions in an online detection
scenario. More work is needed to build effective models that allow
sufficientleadtime(i.e.,reducingthenumberoflogmessagesbeing
examined) and meanwhile achieve high prediction accuracy.
Evolvingsystems. Ourfindingsshowthatlogparsingerrorshave
impact on log-based anomaly detection. Log parsing errors canbe introduced by the change of logging statements during soft-
ware evolution [ 26,54]. As real-world software systems constantly
evolve,newlogeventsalwaysappear[ 24,54].Therefore,existing
log-based anomaly detection models will either fail to work due to
1365
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:28:46 UTC from IEEE Xplore.  Restrictions apply. Log-based Anomaly Detection with Deep Learning: How Far Are We? ICSE ‚Äô22, May 21‚Äì29, 2022, Pittsburgh, PA, USA
Table 7: A Comparison of Different Log-based Anomaly Detection Approaches with Deep Learning
Model Pros Cons Data Requirements
DeepLog Relatively simple, only require normal data. Good at
detecting anomalies early detection.Perform poorly on complex datasets. Heavilyimpacted by the log parsing errors. Normal labeled andsimple data.
LogAnomaly Only require normal data. Can match new logtemplates with existing ones in the training data. Perform poorly on complex datasets. Heavilyimpacted by the log parsing errors. Normal labeled andsimple data.
PLELog Only require normal and unlabeled data. Goodperformance compared to semi-supervised methods.Can learn the semantic meaning of log templates.Cannot work well with the noise in training dataand in the early detection task. Complex designand time-consuming.Partially labeled data.
LogRobust Good performance on many datasets. Can capturethe contextual information of log sequences tohandle log parsing errors and the instability of logs.Require a large amount of labeled data. Heavilyimpacted by mislabeled logs. Fully labeled data.
CNN Good performance on many datasets. Can handle log
parsing errors.Require a large amount of labeled data. Heavilyimpacted by mislabeled logs. Fully labeled data.
theincompatibilitywithnewlogsorresultinlowperformancedue
to the incorrect classification. Therefore, models should be able to
learnthesemanticmeaningofthewholelogmessagestohandle
the instability of logs of evolving systems.
Relations among log events. As discussed in Section 4.2, ses-
sion window grouping gathers logs in a specific execution path,
which could possess more relations among log events than the
fixed-window grouping method. Existing methods convert logs
intosequences,whichcapturesequentialrelationshipsamonglog
messages. A possible research direction is to explore more relation-shipsbetweenlogs,suchasthelogicalrelationshipsandinteractive
relationships among logging components [ 30], to capture a variety
of anomalous behavior.
5.3 Threats to Validity
Duringourstudy,wehaveidentifiedthefollowingmajorthreats
to the validity.
Limitedmodels.Inthiswork,weonlyexperimentallyevaluate
five representative models that have publicly available source code.
Inthe future,wewill aimtore-implement theDL modelsthatdid
not release their source code based on the descriptions in their
papers and then perform a larger-scale evaluation.
Reimplementation. We mainly adopt the public implemen-
tations of studied models. For LogRobust, as its original imple-mentation is based on Keras, we convert it into a PyTorch-basedimplementation so that we have a unified framework for all log-based anomaly detection tools. In our version, we use the same
hyperparametersthatareprovidedbytheauthorsofLogRobust.For
LogAnomalyandCNN,weusetheFastTextmodeltoreplacethe
missing semantic embedding components from public implementa-
tions. To reduce this threat, we experiment on the same settings
anddatasetsfromtheoriginalpaperandconfirmthatourresults
are similar with the reported values.
Limited datasets. Our experiments are conducted on four pub-
liclogdatasets.Althoughtheyarewidelyusedinexistingstudies
on log-based anomaly detection, they may not represent all charac-
teristicsoflogdata.Toovercomethisthreat,wecreatesynthetic
datasetsforevaluatingmodelswithdifferentdatacharacteristics
(e.g.,differentclassdistributionsanddifferentlabelingnoise).Inthe
future,wewillexperimentonmoredatasets,includingindustrial
datasets, to cover more real-world scenarios.Data quality. Our experiments are conducted based on four
datasetsthataremanuallyinspectedandlabeledbyengineers.How-
ever, ourexperiment ondata noisesshows thatasmall portionof
mislabeledlogs coulddowngrade theperformanceof anomalyde-
tection models. To reduce this threat, we experiment with four
publicdatasets.Wealsocreatesyntheticdatasetsbyinjectingaspe-cificportionofmislabeledlogs.Inourfuturework,wewillexplore
methods for measuring and improving the quality of datasets.
6 CONCLUSION
We have conducted an in-depth analysis of recent deep learning
models for log-based anomaly detection. We have investigated sev-
eral aspects of model evaluation: training data selection strategies,
different characteristics of datasets, and early detection capability.
Ourresultspointoutthatalltheseaspectshavelargeimpactontheevaluationresultsandtheperformanceofthemodelsisoftennotasgood as expected. Our findings show that the problem of log-based
anomaly detection has not been solved yet. We also suggest some
possible future work. We hope that the results and findings of our
studycanbeofgreathelpforpractitionersandresearchersworking
on this interesting area.
Our source code and detailed experimental data are available
athttps://github.com/LogIntelligence/LogADEmpirical. The
datasets including the synthetic data that is used for evaluating
modelswithdifferentdatacharacteristics,canalsoserveasabench-
mark for evaluating future log-based anomaly detection models.
ACKNOWLEDGMENTS
This work is supported by Australian Research Council (ARC) Dis-
coveryProjects(DP200102940,DP220103044).Wealsothankanony-
mous reviewers for their insightful and constructive comments,
which significantly improve this paper.
REFERENCES
[1]2021. Implementation of PLELog. Retrieved August 27, 2021 from https:
//github.com/YangLin-George/PLELog
[2]2021. A large collection of system log datasets for AI-powered log analytics.
Retrieved August 31, 2021 from https://github.com/logpai/loghub
[3]2021. Log Anomaly Detection Toolkit. Retrieved August 27, 2021 from https:
//github.com/donglee-afar/logdeep
[4]2021. APytorchimplementationofDeepLog. RetrievedAugust21,2021from
https://github.com/wuyifan18/DeepLog
1366
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:28:46 UTC from IEEE Xplore.  Restrictions apply. ICSE ‚Äô22, May 21‚Äì29, 2022, Pittsburgh, PA, USA Van-Hoang Le and Hongyu Zhang
[5]2021. A toolkit for automated log parsing. Retrieved August 31, 2021 from
https://github.com/logpai/logparser
[6]JakubBreierandJanaBrani≈°ov√°.2015. Anomalydetectionfromlogfilesusing
dataminingtechniques. In InformationScienceandApplications.Springer,449‚Äì
457.
[7]MikeChen,AliceXZheng,JimLloyd,MichaelIJordan,andEricBrewer.2004.
Failure diagnosisusing decision trees.In International Conferenceon Autonomic
Computing, 2004. Proceedings. IEEE, 36‚Äì43.
[8]ZhuangbinChen,JinyangLiu,WenweiGu,YuxinSu,andMichaelRLyu.2021.
Experience Report: Deep Learning-based System Log Analysis for Anomaly
Detection. arXiv preprint arXiv:2107.05908 (2021).
[9]Kyunghyun Cho, Bart Van Merri√´nboer, Caglar Gulcehre, Dzmitry Bahdanau,
Fethi Bougares, Holger Schwenk, and Yoshua Bengio. 2014. Learning phrase
representations using RNN encoder-decoder for statistical machine translation.
arXiv preprint arXiv:1406.1078 (2014).
[10]HetongDai,HengLi,CheShaoChen,WeiyiShang,andTse-HsunChen.2020.
Logram: Efficient log parsing using n-gram dictionaries. IEEE Transactions on
Software Engineering (2020).
[11]MinDuandFeifeiLi.2016. Spell:Streamingparsingofsystemeventlogs.In 2016
IEEE 16th International Conference on Data Mining (ICDM). IEEE, 859‚Äì864.
[12]Min Du, Feifei Li, Guineng Zheng, and Vivek Srikumar. 2017. Deeplog: Anomaly
detection and diagnosis from system logs through deep learning. In Proceedings
ofthe2017ACMSIGSACConferenceonComputerandCommunicationsSecurity .
1285‚Äì1298.
[13]Amir Farzad and T Aaron Gulliver. 2020. Unsupervised log message anomaly
detection. ICT Express 6, 3 (2020), 229‚Äì237.
[14]Norman E. Fenton and Niclas Ohlsson. 2000. Quantitative analysis of faults and
failures in a complex software system. IEEE Transactions on Software engineering
26, 8 (2000), 797‚Äì814.
[15]Haixuan Guo, Shuhan Yuan, and Xintao Wu. 2021. LogBERT: Log Anomaly
Detection via BERT. arXiv preprint arXiv:2103.04475 (2021).
[16]Hossein Hamooni, Biplob Debnath, Jianwu Xu, Hui Zhang, Guofei Jiang, and
Abdullah Mueen. 2016. Logmine: Fast pattern recognition for log analytics.
InProceedingsofthe25thACMInternationalonConferenceonInformationand
Knowledge Management. 1573‚Äì1582.
[17]Pinjia He, Jieming Zhu, Zibin Zheng, and Michael R Lyu. 2017. Drain: An online
log parsing approach with fixed depth tree. In 2017 IEEE International Conference
on Web Services (ICWS). IEEE, 33‚Äì40.
[18]ShilinHe,PinjiaHe,ZhuangbinChen,TianyiYang,YuxinSu,andMichaelRLyu.
2020. ASurveyonAutomatedLogAnalysisforReliabilityEngineering. arXiv
preprint arXiv:2009.07237 (2020).
[19]ShilinHe,JiemingZhu,PinjiaHe,andMichaelRLyu.2016. Experiencereport:
System log analysis for anomaly detection. In ISSRE 2016. IEEE, 207‚Äì218.
[20]Shilin He, Jieming Zhu, Pinjia He, and Michael R Lyu. 2020. Loghub: a large
collectionofsystemlogdatasetstowardsautomatedloganalytics. arXivpreprint
arXiv:2008.06448 (2020).
[21]SeppHochreiterandJ√ºrgenSchmidhuber.1997. Longshort-termmemory. Neural
computation 9, 8 (1997), 1735‚Äì1780.
[22]Zhen Ming Jiang, Ahmed E Hassan, Parminder Flora, and Gilbert Hamann. 2008.
Abstracting execution logs to execution events for enterprise applications (short
paper). In 2008 The Eighth International Conference on Quality Software. IEEE,
181‚Äì186.
[23]Armand Joulin, Edouard Grave, Piotr Bojanowski, Matthijs Douze, H√©rve J√©gou,
and Tomas Mikolov. 2016. Fasttext. zip: Compressing text classification models.arXiv preprint arXiv:1612.03651 (2016).
[24]
Suhas Kabinna, Cor-Paul Bezemer, Weiyi Shang, Mark D Syer, and Ahmed E
Hassan. 2018. Examining the stability of logging statements. Empirical Software
Engineering 23, 1 (2018), 290‚Äì333.
[25]Yoon Kim. 2014. Convolutional Neural Networks for Sentence Classification.
InProceedings of the 2014 Conference on Empirical Methods in Natural Language
Processing (EMNLP). Association for Computational Linguistics, Doha, Qatar.
[26]Van-HoangLeandHongyuZhang.2021. Log-basedAnomalyDetectionWith-
out Log Parsing. In 2021 36th IEEE/ACM International Conference on Automated
Software Engineering (ASE). 492‚Äì504.
[27]XiaoyunLi,PengfeiChen,LinxiaoJing,ZilongHe,andGuangbaYu.2020. Swiss-Log:RobustandUnifiedDeepLearningBasedLogAnomalyDetectionforDiverse
Faults. In ISSRE 2020. IEEE, 92‚Äì103.
[28]Qingwei Lin, Ken Hsieh, Yingnong Dang, Hongyu Zhang, Kaixin Sui, Yong
Xu, Jian-Guang Lou, Chenggang Li, Youjiang Wu, Randolph Yao, et al .2018.
Predicting node failure in cloud service systems. In ESEC/FSE 2018. 480‚Äì490.
[29]QingweiLin,HongyuZhang,Jian-GuangLou,YuZhang,andXueweiChen.2016.
Log Clustering Based Problem Identification for Online Service Systems. In Pro-
ceedings of the 38th International Conference on Software Engineering Companion
(Austin, Texas) (ICSE ‚Äô16). 102‚Äì111.
[30]Fucheng Liu, Yu Wen, Dongxue Zhang, Xihe Jiang, Xinyu Xing, and Dan Meng.
2019. Log2vec: A heterogeneous graph embedding based approach for detecting
cyberthreatswithinenterprise.In Proceedingsofthe2019ACMSIGSACConference
on Computer and Communications Security. 1777‚Äì1794.[31]XuLiu,WeiyouLiu,XiaoqiangDi,JinqingLi,BinbinCai,WeiwuRen,andHuamin
Yang.2021. LogNADS:Networkanomalydetectionschemebasedonsemantic
representation. Future Generation Computer Systems (2021).
[32]Jian-Guang Lou, Qiang Fu, Shengqi Yang, Ye Xu, and Jiang Li. 2010. Mining
Invariants from Console Logs for System Problem Detection.. In USENIX Annual
Technical Conference. 1‚Äì14.
[33]Siyang Lu, Xiang Wei, Yandong Li, and Liqiang Wang. 2018. Detecting anomaly
inbigdatasystemlogsusingconvolutionalneuralnetwork.In 2018IEEE16thIntl
ConfonDependable,AutonomicandSecureComputing,16thIntlConfonPervasive
Intelligence and Computing, 4th Intl Conf on Big Data Intelligence and Computing
andCyberScienceandTechnologyCongress(DASC/PiCom/DataCom/CyberSciTech) .
IEEE, 151‚Äì158.
[34]AdetokunboAOMakanju,ANurZincir-Heywood,andEvangelosEMilios.2009.
Clustering event logs using iterative partitioning. In Proceedings of the 15th
ACM SIGKDDinternationalconference onKnowledgediscovery anddatamining.
1255‚Äì1264.
[35]LelandMcInnes,JohnHealy,andSteveAstels.2017.hdbscan:Hierarchicaldensity
based clustering. Journal of Open Source Software 2, 11 (2017), 205.
[36]WeibinMeng,YingLiu,YichenZhu,ShenglinZhang,DanPei,YuqingLiu,YihaoChen,RuizhiZhang,ShiminTao,PeiSun,etal
.2019. LogAnomaly:Unsupervised
DetectionofSequentialandQuantitativeAnomaliesinUnstructuredLogs..In
IJCAI, Vol. 7. 4739‚Äì4745.
[37]Haibo Mi, Huaimin Wang, Yangfan Zhou, Michael Rung-Tsong Lyu, and Hua
Cai.2013. Towardfine-grained,unsupervised,scalableperformancediagnosis
for production cloud computing systems. IEEE Transactions on Parallel and
Distributed Systems 24, 6 (2013), 1245‚Äì1255.
[38]Meiyappan Nagappan and Mladen A Vouk. 2010. Abstracting log lines to log
eventtypesforminingsoftwaresystemlogs.In 20107thIEEEWorkingConference
on Mining Software Repositories (MSR 2010). IEEE, 114‚Äì117.
[39]Sasho Nedelkoski, Jasmin Bogatinovski, Alexander Acker, Jorge Cardoso, and
Odej Kao. 2020. Self-attentive classification-based anomaly detection in unstruc-
tured logs. arXiv preprint arXiv:2008.09340 (2020).
[40] Adam Oliner and Jon Stearley. 2007. What supercomputers say: A study of five
system logs. In DSN 2007. IEEE, 575‚Äì584.
[41]JeffreyPennington,RichardSocher,andChristopherDManning.2014. Glove:
Globalvectorsforwordrepresentation.In Proceedingsofthe2014conferenceon
empirical methods in natural language processing (EMNLP). 1532‚Äì1543.
[42]Keiichi Shima. 2016. Length matters: Clustering system log messages using
length of words. arXiv preprint arXiv:1611.03213 (2016).
[43]Marina Sokolova,Nathalie Japkowicz, andStan Szpakowicz. 2006. Beyond ac-curacy, F-score and ROC: a family of discriminant measures for performanceevaluation. In Australasian joint conference on artificial intelligence. Springer,
1015‚Äì1021.
[44]Liang Tang, Tao Li, and Chang-Shing Perng. 2011. LogSig: Generating sys-
temeventsfromrawtextuallogs.In Proceedingsofthe20thACMinternational
conference on Information and knowledge management. 785‚Äì794.
[45]Risto Vaarandi and Mauno Pihelgas. 2015. Logcluster-a data clustering and
pattern mining algorithm for event logs. In 2015 11th International conference on
network and service management (CNSM). IEEE, 1‚Äì7.
[46]Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all
youneed. arXiv preprint arXiv:1706.03762 (2017).
[47]Wei Xu, Ling Huang, Armando Fox, David Patterson, and Michael I Jordan. 2009.
Detecting large-scale system problems by mining console logs. In Proceedings of
the ACM SIGOPS 22nd symposium on Operating systems principles. 117‚Äì132.
[48]Lin Yang, Junjie Chen, Zan Wang, Weijing Wang, Jiajun Jiang, Xuyuan Dong,and Wenbin Zhang. 2021. Semi-supervised log-based anomaly detection via
probabilistic label estimation. In ICSE 2021. IEEE, 1448‚Äì1460.
[49]Jacob Yerushalmy. 1947. Statistical problems in assessing methods of medicaldiagnosis, with special reference to X-ray techniques. Public Health Reports
(1896-1970) (1947), 1432‚Äì1449.
[50]Bo Zhang, Hongyu Zhang, Pablo Moscato, and Aozhong Zhang. 2020. Anom-aly Detection via Mining Numerical Workflow Relations from Logs. In 2020
International Symposium on Reliable Distributed Systems (SRDS). IEEE, 195‚Äì204.
[51]HongyuZhang.2008. Onthedistributionofsoftwarefaults. IEEETransactions
on Software Engineering 34, 2 (2008), 301‚Äì302.
[52]HongyuZhang.2009. Aninvestigationoftherelationshipsbetweenlinesofcodeand defects. In 2009 IEEE International Conference on Software Maintenance. IEEE,
274‚Äì283.
[53]HongyuZhangandXiuzhenZhang.2007. Commentson"dataminingstaticcode
attributes to learn defect predictors". IEEE Transactions on Software Engineering
33, 9 (2007), 635‚Äì637.
[54]Xu Zhang, Yong Xu, Qingwei Lin, Bo Qiao, Hongyu Zhang, Yingnong Dang,Chunyu Xie, Xinsheng Yang, Qian Cheng, Ze Li, et al
.2019. Robust log-based
anomaly detection on unstable log data. In ESEC/FSE 2019. 807‚Äì817.
[55]JiemingZhu,ShilinHe,JinyangLiu,PinjiaHe,QiXie,ZibinZheng,andMichaelR
Lyu. 2019. Tools and benchmarks for automated log parsing. In ICSE 2019. IEEE,
121‚Äì130.
1367
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:28:46 UTC from IEEE Xplore.  Restrictions apply. 