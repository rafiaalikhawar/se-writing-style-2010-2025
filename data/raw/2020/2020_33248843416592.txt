Towards Interpreting Recurrent Neural Networks through
Probabilistic Abstraction
Guoliang Dong
Zhejiang University
dgl-prc@zju.edu.cnJingyi Wangâˆ—
Zhejiang University
wangjyee@zju.edu.cnJun Sun
Singapore Management University
junsun@smu.edu.sg
Yang Zhang
Zhejiang University
leor@zju.edu.cnXinyu Wang
Zhejiang University
wangxinyu@zju.edu.cnTing Dai
Huawei International Pte Ltd
daiting2@huawei.com
Jin Song Dong
National University of Singapore
dongjs@comp.nus.edu.sgXingen Wang
Zhejiang University
newroot@zju.edu.cn
ABSTRACT
Neuralnetworksarebecomingapopulartoolforsolvingmanyreal-
worldproblemssuchasobjectrecognitionandmachinetranslation,
thanks to its exceptional performance as an end-to-end solution.
However,neuralnetworksarecomplexblack-boxmodels,which
hinders humans from interpreting and consequently trusting them
inmakingcriticaldecisions.Towardsinterpretingneuralnetworks,
several approaches have been proposed to extract simple determin-isticmodelsfromneuralnetworks.Theresultsarenotencouraging
(e.g., low accuracy and limited scalability), fundamentally due to
the limited expressiveness of such simple models.
In this work, we propose an approach to extract probabilistic
automataforinterpretinganimportantclassofneuralnetworks,
i.e., recurrent neural networks. Our work distinguishes itself from
existing approaches in two important ways. One is that probabilityisusedtocompensateforthelossofexpressiveness.Thisisinspired
by the observation that human reasoning is often â€˜probabilisticâ€™.
Theotheristhatweadaptivelyidentifytherightlevelofabstraction
so that a simple model is extracted in a request-specific way. We
conduct experiments on several real-world datasets using state-of-
the-artarchitecturesincludingGRUandLSTM.Theresultshows
that our approach significantly improves existing approaches in
terms of accuracy or scalability. Lastly, we demonstrate the useful-
ness of the extracted models through detecting adversarial texts.
CCS CONCEPTS
â€¢Theory of computation â†’Abstraction ;Machine learning
theory;Probabilistic computation.
âˆ—Corresponding authors: Jingyi Wang and Xinyu Wang.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation
on the first page. Copyrights for components of this work owned by others than ACMmustbehonored.Abstractingwithcreditispermitted.Tocopyotherwise,orrepublish,topostonserversortoredistributetolists,requirespriorspecificpermissionand/ora
fee. Request permissions from permissions@acm.org.
ASE â€™20, September 21â€“25, 2020, Virtual Event, Australia
Â© 2020 Association for Computing Machinery.
ACM ISBN 978-1-4503-6768-4/20/09...$15.00
https://doi.org/10.1145/3324884.3416592KEYWORDS
Abstraction, Interpretation, Probabilistic automata, Recurrent neu-
ral networks
ACM Reference Format:
Guoliang Dong, Jingyi Wang, Jun Sun, Yang Zhang, Xinyu Wang, Ting Dai,
Jin Song Dong, and Xingen Wang. 2020. Towards Interpreting Recurrent
NeuralNetworksthroughProbabilisticAbstraction.In 35thIEEE/ACMInter-
nationalConferenceonAutomatedSoftwareEngineering(ASEâ€™20),September
21â€“25,2020,VirtualEvent,Australia. ACM,NewYork,NY,USA,12pages.
https://doi.org/10.1145/3324884.3416592
1 INTRODUCTION
Neuralnetworkmodelsaregettingpopularduetotheirexceptional
performance in solving many real-world problems, such as self-
driving cars [ 7], malware detection [ 49], sentiment analysis [ 42]
and machine translation [ 6]. At the same time, neural networks
are shown to be vulnerable to issues such as adversarial attacks [ 8,
20,41] and embedded back-doors [ 10]. To be able to trust neural
networks, it is crucial to understand how neural networks make
decisions,evenbetter,toreasonaboutthembeforedeployingthem
in safety-critical applications.
Neural networks are, however, complex models that work in
ablack-boxmanner.Humaninterpretationofneuralnetworksis
often deemed infeasible [ 25,38]. Furthermore, the complexity also
hinders analysis through traditional software analysis techniques
suchastestingandverification.Recently,therehavebeennotice-
ableeffortsonportingestablishedsoftwaretestingandverificationtechniquestoneuralnetworkmodels.Forinstance,multipletesting
approacheslikedifferentialtesting[ 34],mutationtesting[ 28,43],
andconcolictesting[ 40]havebeenadaptedtotestneuralnetworks.
Furthermore, several verification techniques based on SMT solv-
ing [24], abstract interpretation [ 18] and reachability analysis [ 37]
havealsobeenexploredtoformallyverifyneuralnetworks.How-
ever,duetothecomplexityofneuralnetworks,existingapproaches
oftenhavehighcostand/oronlyworkforverylimitedclassesof
neural networks [22].
Recently, an alternative approach has been proposed. That is,
ratherthanunderstandingandreasoningaboutneuralnetworks
directly, researchers aim to extract simpler models from neural
4992020 35th IEEE/ACM International Conference on Automated Software Engineering (ASE)
ASE â€™20, September 21â€“25, 2020, Virtual Event, Australia Guoliang Dong, Jingyi Wang, Jun Sun, Yang Zhang, Xinyu Wang, Ting Dai, Jin Song Dong, and Xingen Wang
networks. Ideally, the simpler models would accurately approxi-
mate the neural networks and be simple enough such that theyare human-interpretable. Furthermore, such models can be sub-
ject to automated system analysis techniques such as model-based
testing[13],modelchecking[ 12]andruntimemonitoring[ 39].Sev-
eral attempts have been made on one particularly interesting class
of neural networks called recurrent neural networks (RNN), dueto their stateful nature as well as popularity in various domains.
In[32],Omlinetal.proposetoencodetheconcretehiddenstates
ofRNNintosymbolicrepresentationandthenextractsimpledeter-
ministic models from the symbolic data [ 23]. Followup approaches
have been proposed to extract different models like determinis-tic finite automata (DFA) from RNN [
21,46]. A recent empirical
study [45] shows that such approaches are useful for capturing the
structuralinformationoftheRNNandhencehelpfulformonitoring
its decision process.
Existingapproaches, however, haveeitherlimited accuracy (in
thecaseof[ 21,46]wheresimpledeterministicmodelsareextracted)
orscalability(inthecaseof[ 47]wheremoreexpressivemodelsare
extracted). For instance, the extracted models for the real-worldsentiment analysis tasks in [
21] have about 70% fidelity even on
the training data. This is not surprising since simple models like
DFAhavelimitedexpressivenesscomparedtoneuralnetworks.Forinstance,theworkin[
21]extractsdeterministictransitionsbetween
symbolicencodingofconcretehiddenstatesinRNN,whereas RNN
learnedfromreal-worlddataareintrinsicallyprobabilistic .Ifwewere
to improve accuracy by extracting more states and transitions, not
only the models are computationally expensive to extract but also
the extracted models become uninterpretable.
TowardsextractingaccurateinterpretablemodelsfromRNN,we
developatechniqueofextractingprobabilisticfinite-stateautomata
(PFA)fromRNNinthiswork.Ourworkdistinguishesitselffrom
existing approaches in two important ways. One is that we extract
probabilisticmodelstocompensateforthelimitedexpressivenessofsimple deterministic models (compared to that of neural networks).
Thisisinspiredby theobservationthathumanreasoningisoften
â€˜probabilisticâ€™ [ 31], i.e., humans often develop â€˜simpleâ€™ understand-
ingofcomplexsystemsbycuttingcorners(i.e.,low-probabilisticcases). The other is that we do not attempt to generate a single
modelthatapproximatesanRNNmodelasaccuratelyaspossible,
as it often leads to models with many states and transitions which
arehardtoextractorinterpret.Instead,wegeneratemodelsthat
are sufficiently accurate as per user-request. For instance, if theuser requires to get a model which achieves 90% accuracy in ap-
proximatingtheRNN,theextractedmodelwouldhavefewerstates
than a model extracted with 99% accuracy in doing that. This is
achieved by adaptively identifying the right level of abstraction
through clustering.
Our approach is based on a novel algorithm which combines
state-of-the-art probabilistic learning and abstraction through clus-
tering. Given an RNN model and a training set, we first encode
theconcrete numericalhiddenstatesof anRNNinto asetofclus-
ters. Then, we convert the samples in the training set into a set
of symbolic traces, each of which is the sequence of clusters vis-
itedbythesample.Afterwards,weapplyaprobabilisticlearning
algorithm [ 29] on the symbolic traces to learn a PFA. Furthermore,
given a specific requirement, we apply clustering in a greedy wayand automatically determine the right number of clusters (i.e., the
levelofabstraction),whichconsequentlydeterminesthenumber
of states in the learned PFA. That is, we optimize to balance the
complexity of the learned PFA (i.e., the fewer states the better) and
its accuracy in approximating the RNN (i.e., the higher the better).
We applied our approach to several RNN models with state-
of-the-art architectures for solving artificial and real-world (i.e.,
sentiment analysis) tasks. The results show that our approach sig-
nificantlyimprovesexistingapproachesintermsofeitheraccuracy
or scalability and is capable of extracting models which accurately
approximate the RNN models. Compared to [ 21], our approach
improves the fidelity of the extracted model from below 60% to
over 90% on average on the real-world datasets. Compared to [ 47]
whichislimitedtosmallartificialdatasets,ourapproachhandles
large real-world datasets effectively. Lastly, we demonstrate the
usefulnessoftheextractedmodelsthroughanimportantapplica-
tion, i.e., detecting adversarial texts that generated to attack RNN.
To the best of our knowledge, this is the first systematic approach
for detecting adversarial texts.
We organize the rest of the paper as follows. We provide prelim-
inaries in Section 2. We present our approach in detail in Section 3
and experiment results in Section 4. We review related work in
Section 5 and conclude in Section 6.
2 PRELIMINARIES
In this section, we review relevant background on recurrent neural
networks (RNN) and probabilisticfinite-state automata (PFA).
RecurrentNeuralNetwork Inthiswork,wefocusonstate-of-the-art
RNN architectures such as Gated Recurrent Unit (GRU) [ 11] and
Long Short-Term Memory (LSTM) [ 19]. We introduce RNN at a
conceptual level rather than introducing details of GRU and LSTM,
sinceourapproachappliestoRNNingeneral.Theconceptualmodel
ofRNNisshowninFigure1,whichtakesavariable-lengthsequence
/angbracketleftğ‘¥0,ğ‘¥1,Â·Â·Â·,ğ‘¥ğ‘š/angbracketrightas input and produces a sequence /angbracketleftğ‘œ0,ğ‘œ1,Â·Â·Â·,ğ‘œğ‘š/angbracketright
as output. In this work, we focus on RNN classifiers ğ‘…:ğ‘‹âˆ—â†’ğ¼,
whereğ‘‹is the set containing all the possible values of ğ‘¥;ğ‘‹âˆ—is the
setoffinitestringsover ğ‘‹;andğ¼isafinitesetoflabels(classification)
which only depend on the last output ğ‘œğ‘š.
RNNisstateful,i.e.,havingaâ€˜memoryâ€™ofprevioustimesteps
andwhathavebeencalculatedsofarthroughasetofhiddenstates
ğ». At each time step ğ‘¡, the hidden state ğ‘ ğ‘¡and the output ğ‘œğ‘¡are
calculated as follows.
ğ‘ ğ‘¡=ğ‘“(ğ‘ˆğ‘¥ğ‘¡+ğ‘Šğ‘ ğ‘¡âˆ’1), (1)
ğ‘œğ‘¡=argmax
ğ¼ğ‘ ğ‘œğ‘“ğ‘¡ğ‘šğ‘ğ‘¥(ğ‘‰ğ‘ ğ‘¡), (2)
whereğ‘“is usually a nonlinear function like tanhorReLU;ğ‘ˆ,ğ‘Š,
andğ‘‰are trainedparameters; and ğ‘ ğ‘œğ‘“ğ‘¡ğ‘šğ‘ğ‘¥ is anormalizing func-
tion which outputs a probability distribution. We remark that GRU
and LSTM networks have the same conceptual model shown in
Figure 1, except that more complex functions are used to compute
the hidden states. We refer the readers to [11, 19] for details.
Probabilistic Finite Automata Existing works on explaining RNN
in [21,46] focus on extracting models in the form of deterministic
finite-state automata (DFA).
500Towards Interpreting Recurrent Neural Networks through Probabilistic Abstraction ASE â€™20, September 21â€“25, 2020, Virtual Event, Australia
W
W W W. . .
. . .. . .
. . .V V V
U UU
xt+1 xt xt-1ot+1 ot ot-1
Figure 1: A conceptual RNN.
Definition 1. A DFA is a tuple Ağ·=/angbracketleftX,ğ‘„,ğ›¿,ğ‘„0,ğ‘„ğ‘“/angbracketright, where
Xisanalphabet; ğ‘„isafinitesetofallpossiblestates; ğ›¿:ğ‘„Ã—Xâ†’ğ‘„
isalabeledtransitionfunction; ğ‘„0âŠ†ğ‘„isasetofinitialstates;and
ğ‘„ğ‘“âŠ†ğ‘„is a set of accepting states.
Note that given a sequence of input symbols, the DFA quali-
tatively determines whether it is accepted (if the last state is an
accepting state) or not. This limits DFAâ€™s capability in approximat-
ing RNN. For instance, in the context of explaining RNN trained
for sentiment analysis, a DFA model produces a binary result (i.e.,
positive or negative) given any text, whereas RNN often produces
a probability of the text being positive or negative. To address this
limitation, we instead focus on extracting PFA in this work, which
associates probabilities with state transitions in the DFA.
Definition2. APFA isatuple A=/angbracketleftX,ğ‘„,ğ›¿,ğ‘„0,ğ‘„ğ‘“,ğœ‡0/angbracketright,where
Xisanalphabet; ğ‘„isafinitesetofstates; ğ›¿:ğ‘„Ã—XÃ—ğ‘„â†’[0,1]isala-
beled probabilistic transition function such that/summationtext.1
ğ‘’âˆˆX(ğ›¿(ğ‘ ğ‘–,ğ‘’,ğ‘  ğ‘—))=
1for allğ‘ ğ‘–,ğ‘ ğ‘—âˆˆğ‘„;ğ‘„0âŠ†ğ‘„is a set of initial states; ğœ‡0is the initial
probability distribution over ğ‘„0; andğ‘„ğ‘“is a set of accepting states.
An example PFA (extracted using our approach from an RNN
model)isshowninFigure4,whereacceptingstatesarerepresented
usingdouble-edgedcircles;theinitialstatesareindicatedwithanar-rowfromnowhere;andeachtransitionislabeledwithaprobability
ğ‘followed by a symbol ğ‘’in the form of ğ‘/ğ‘’.
3 OUR APPROACH
Inthissection,weintroduceourapproachstep-by-step.Anoverview
of the overall workflow is shown in Figure 2. The inputs are anRNN model and the associated training set. There are two main
parts,i.e.,anabstractionpart(ontheleft)andalearningpart(on
theright).Theabstractionpartabstractstheconcretehiddenstates
of a given RNN into an abstract alphabet. The goal is to systemati-
callygrouphiddenstatesofRNN(intheformofnumericalvectors)
that exhibit similar behaviors into clusters. The learning part then
takes the abstract alphabet and systematically learns a PFA. In the
following,weintroducethestepsindetailandillustratethemusing
the following running example.
Example3.1. ThesentenceshowninthefirstrowofTable1is
a review selected from the RTMR dataset [ 33], which is a widely
used movie review dataset for sentiment analysis. The first row in
Table1istheoriginalreview;thesecondrowisthecleanedinput
afterremovingthestopwords;andthethirdrowisthelabel,where
1 represents â€œpositiveâ€ (i.e., a positive review).3.1 Abstraction
Theobjectiveofthisstepistosystematicallyabstractthehidden
statesof theRNN.Note thatthestatesof theRNNare intheform
of numerical vectors which have numerous values and are hardtointerpret.Theideaisthatmanyofthehiddenstatesrepresent
similar behaviors and thus can be grouped together. There are two
existing techniques on abstracting the hidden states of RNN, i.e.,intervalpartition[
46]andclustering[ 21].Theformerarbitrarily
partitionstherangeofhiddenstatevaluesintomultipleintervals
and assumes that those hidden states in the same interval have
similarbehaviors,whereasthelatterassumesthatnearbyhidden
states exhibit similar behaviors. In this work, we choose the latter
for two reasons.Firstly,clustering has been shownto be intuitive
and effective in a recent empirical study [ 45]. In fact, it has been
shownthatthehiddenstatesnaturallyformclusters[ 21].Secondly,
existing clustering techniques allow us to flexibly control the level
of abstraction by controlling the number of clusters.
Thegoalofclusteringistogrouptheâ€˜infiniteâ€™hiddenstatespace
of RNN into a few clusters. There are many existing clustering
algorithms[ 48].Inthiswork,weadopttheK-Meansalgorithm.The
ideaofK-Meansistoidentifyanassignmentfunction ğ¶:ğ»â†’ğ¾
whereğ»is a set of concrete states and ğ¾is a set of clusters. In-
tuitively, ğ¶(â„)=ğ‘˜means that the concrete state â„is mapped to
clusterğ‘˜. Ideally, the hidden states that are assigned to the same
cluster should be close to each other in terms of certain distancemetrics (e.g., Euclidean distance). Assume that there are
ğ¾clus-
ters, the assignment ğ¶can be found by optimizing the following
objective.
ğ¶âˆ—=argmin
ğ¶ğ¾/summationdisplay.1
ğ‘˜=1ğ‘ğ‘˜/summationdisplay.1
â„âˆˆğ¶ğ‘˜||â„âˆ’Â¯â„ğ‘˜||2(3)
whereğ¶ğ‘˜={â„|â„âˆˆğ»,ğ¶(â„)=ğ‘˜}isthe setofhiddenstates which
areassignedtothecluster ğ‘˜;ğ‘ğ‘˜isthesizeof ğ¶ğ‘˜;andÂ¯â„ğ‘˜ismean
of all the hidden states in ğ¶ğ‘˜.
Abstracting traces Clustering is applied as follows in our work. We
first collect the RNN hidden state vectors of each sample in the
trainingset.Next,wetrainaclusteringfunctionusingtheK-means
algorithm on those vectors. Note that the number of clusters ğ¾
isaparameterwhichmustbefixedbeforeapplyingtheK-means
algorithm. We discuss how to set the value for ğ¾in Section 3.3.
Once we have a clustering function ğ¶, we obtain the abstract al-
phabet(i.e.,theclusters)andconstructasetofabstracttracesbased
onthetrainingsetasfollows.Wefeedeverysampleinthetraining
set into the RNN, and obtain the concrete hidden state at each step.
Theresultisasequenceofconcretehiddenstates /angbracketleftğ‘ 0,ğ‘ 1,ğ‘ 2,Â·Â·Â·,ğ‘ ğ‘›/angbracketright
whereğ‘ 0is a dummy initial state (i.e., the dummy initial mapping).
Next, we apply ğ¶to each concrete hidden state but the dummy ini-
tialstate ğ‘ 0andobtainanabstracttrace /angbracketleftğ‘ ,ğ¶(ğ‘ 1),ğ¶(ğ‘ 2),Â·Â·Â·,ğ¶(ğ‘ ğ‘›)/angbracketright
whereğ‘ issymboldenotingthedummyinitialstate.Givenasample
ğ‘¥, we write ğ›¼(ğ‘¥)to denote the abstract trace obtained as described
above. Afterward, we further concatenate ğ›¼(ğ‘¥)with the label of ğ‘¥
(whichformstheacceptingstatesoftheextractedPFAasexplained
later).Notethattheabstractalphabetisthus X={ğ‘ }âˆªğ¾âˆªğ¼where
ğ¾is the set of clusters and ğ¼is the set of labels. We denote the
above procedure as Î¨(ğ‘¥,ğ‘…,ğ¶ ğ¾)whereğ‘…is the given RNN, ğ¶ğ¾is
501ASE â€™20, September 21â€“25, 2020, Virtual Event, Australia Guoliang Dong, Jingyi Wang, Jun Sun, Yang Zhang, Xinyu Wang, Ting Dai, Jin Song Dong, and Xingen Wang
Abstraction Learn ing
trainingclustering/g2/g4 /g2/g5 /g2/g6 /g2/g3/g6/g2/g1/g1 /g6/g3 /g6/g4
/g38 /g38 /g38/g2/g7
/g38/g6/g5RNN
/g1/g2/g3/g2/g10/g1/g2/g3/g2/g9/g1/g2/g3/g2/g8/g1/g2/g3/g2/g7
/g1/g2/g3/g2/g14/g1/g2/g3/g2/g13/g1/g2/g3/g2/g12/g1/g2/g3/g2/g11
/g1/g2/g3/g2/g7/g8/g1/g2/g3/g2/g7/g7/g1/g2/g3/g2/g7/g6/g1/g2/g3/g2/g15/g3
/g4/g7/g8
/g5/g6
Abstract  
alphabet/g2/g4 /g2/g5 /g2/g6 /g2/g7
Training dataHidden states
/g3
/g4
 /g7
/g8
/g6
/g5
/g5
 /g7
 /g4
/g1/g8
/g1/g5
/g1/g7
/g1/g9
/g1/g4
/g1/g6PFA
Abstact 
trace
trainin g
clustering
/g2
/g4
 /g2
/g5
 /g2
/g6
 /g2
/g3
/g6
/g2
/g1
/g1
 /g6
/g3
 /g6
/g4
/g38
 /g38
 /g38
/g2
/g7
/g38
/g6
/g5
RNN
/g1/g2/g3/g2
/g10
/g1/g2/g3/g2
/g9
/g1/g2/g3/g2
/g8
/g1/g2/g3/g2
/g7
/g1/g2/g3/g2
/g14
/g1/g2/g3/g2
/g13
/g1/g2/g3/g2
/g12
/g1/g2/g3/g2
/g11
/g1/g2/g3/g2
/g7/g8
 /g1/g2/g3/g2
/g7/g7
/g1/g2/g3/g2
/g7/g6
/g1/g2/g3/g2
/g15
/g3
/g4
/g7
/g8
/g5
/g6
Abstract  
alphabet
/g2
/g5
 /g2
/g6
 /g2
 /g2
/g4
 /g2
/g7
Training data
Hidd en states
PFA learning
Figure 2: Overall framework
Table 1: An example input text
Original review not a film to rival to live, but a fine little amuse-bouche to keep your appetite whetted
Cleaned film rival live fine little amuse-bouche keep appetite whetted
Label 1
Concrete trace [0.0000, 0.0000, Â·Â·Â·, 0.0000], [-0.0119, -0.0059, Â·Â·Â·, 0.0281], Â·Â·Â·, [-0.0241, 0.1246, Â·Â·Â·, -0.1183]
Abstract trace ğ‘ ,1,1,1,0,0,0,0,0,0,ğ‘ƒ
the clustering function (parameterized by ğ¾) andğ‘¥is a sample.
Applyingtheaboveproceduretoeverysampleinthetrainingset
ğ‘‹, we obtain a bag of abstract traces, denoted as ğ›¼(ğ‘‹), as input for
the next phase of our approach.
Example 3.2. Given Example 3.1, let ğ¾be 2. The clustering func-
tion maps all hidden state vectors to two clusters, denoted as 0 and
1 for simplicity. The abstract alphabet is thus X={ğ‘ ,0,1,ğ‘ƒ,ğ‘}
whereğ‘ƒ,ğ‘arelabelsrepresentingâ€œpositiveâ€andâ€œnegativeâ€respec-
tively. The concrete trace obtained from the example text is shown
inthefourthrowofTable1.Withthetrainedclusteringfunction
ğ¶ğ¾, the abstract trace is shown in the fifth row.
3.2 Learning
The task of the learning part is to construct a PFA based on the
abstract traces. Ideally, the PFA should be simple (i.e., having a
smallnumber ofstatesand transitions)andshould havethemaxi-
mum likelihood of exhibiting the abstract traces (i.e., accurate with
respecttotheRNN).Ourapproachisbuiltontopofthe AAlergia
learning algorithm proposed in [ 29]. We choose AAlergia as it has
been proved to be useful for learning models suitable for system
reasoninglikeprobabilisticmodelchecking[ 29].Thekeyideaof
our learning part is to generate a PFA which generalizes the proba-
bilisticdistributionoftheabstracttracesoverthealphabet.Note
that this is fundamentally different from existing approaches such
as [14] which uses user-provided partitioned intervals as system
states directly.The details of the learning algorithm are shown in Algorithm 1.
The high-level idea is as follows. We first organize the abstract
tracesintoatreecalledfrequencyprefixtree(FPT),whichcanbe
considered as a huge model that exhibits exactly the set of abstract
traces. Afterwards, we repeatedly merge the nodes in the FPT suchthatthenumberofstatesisreducedgradually.Notethattwonodes
aremergedonlyiftheyexhibitsimilarbehaviors.Onceallnodes
with similar behaviors are merged, we transform the resultant FPT
into a PFA. In the following, we present each step in detail.
Frequencyprefixtree Thefirststepistotransformtheabstracttraces
into an FPT. Let ğ›¼(ğ‘‹)be the set of abstract traces and Xbe the
abstract alphabet. Let prefix(ğ›¼(ğ‘‹))be the set of all prefixes of any
ğ‘¥âˆˆğ›¼(ğ‘‹).AnFPTisatuple tree(ğ›¼(ğ‘‹))=/angbracketleftğ‘,ğ¸,ğ¹,root/angbracketright,whereğ‘
isprefix(ğ›¼(ğ‘‹));ğ¸âŠ†ğ‘Ã—ğ‘isthesetofedgessuchthat (ğ‘›,ğ‘›/prime)âˆˆğ¸
ifand onlyifthere exists ğœâˆˆXsuchthat ğ‘›Â·ğœ=ğ‘›/primewhereÂ·isthe
concatenation operator; ğ¹is a frequency function which records
the total number of occurrences of each prefix in ğ›¼(ğ‘‹); androotis
theemptystring /angbracketleft/angbracketrightwhichcorrespondstothedummyinitialstate
ğ‘ . For instance, given a bag of traces with 50 /angbracketleftğ‘,ğ‘/angbracketright,2 0/angbracketleftğ‘,ğ‘/angbracketright,1 0
/angbracketleftğ‘,ğ‘,ğ‘/angbracketright,1 0/angbracketleftğ‘,ğ‘/angbracketright,6/angbracketleftğ‘,ğ‘,ğ‘/angbracketrightand 4/angbracketleftğ‘,ğ‘,ğ‘/angbracketright, the FPT is shown on the
leftofFigure3.WeremarkthataleafnodeoftheFPTrepresents
a complete trace (which is associated with a label in ğ¼), whereas
an internal node of the FPT is a prefix associated with a certain
symbol in ğ¾(i.e., a cluster).
Note that an FPT can be regarded as a PFA. That is, the nodes in
the FPT can be regarded as states in the PFA and we can obtain the
one-stepprobabilityfromnode ğ‘›toğ‘›Â·ğœasğ‘ƒ(ğ‘›,ğ‘›Â·ğœ)=ğ¹(ğ‘›Â·ğœ)/ğ¹(ğ‘›)
502Towards Interpreting Recurrent Neural Networks through Probabilistic Abstraction ASE â€™20, September 21â€“25, 2020, Virtual Event, Australia
Algorithm 1: extract(ğ›¼(ğ‘‹),ğœ–)
1Organize ğ›¼(ğ‘‹)into a frequency prefix tree tree(ğ›¼(ğ‘‹));
2LetR=âˆ…be the set of nodes in thefinal PFA;
3Letğµ={root};
4whileğµâ‰ âˆ…do
5Select a node ğ‘fromğµ;
6Letğ‘šğ‘’ğ‘Ÿğ‘”ğ‘’ğ‘‘=ğ‘“ğ‘ğ‘™ğ‘ ğ‘’;
7foreachğ‘ŸâˆˆRdo
8 Test the compatibility between ğ‘andğ‘Ÿ;
9 ifcompatible then
10 ğ‘šğ‘’ğ‘Ÿğ‘”ğ‘’ğ‘‘=ğ‘¡ğ‘Ÿğ‘¢ğ‘’;
11 Mergeğ‘withğ‘Ÿ;
12 break;
13if!ğ‘šğ‘’ğ‘Ÿğ‘”ğ‘’ğ‘‘then
14 Addğ‘toR;
15Removeğ‘fromğµand add the children of ğ‘toğµ;
16Letğ›¿be a probabilistic transition function;
17foreachğ‘ŸâˆˆRdo
18foreachğœâˆˆXdo
19 ğ›¿(ğ‘Ÿ,ğœ,ğ‘ŸÂ·ğœ)=ğ¹(ğ‘ŸÂ·ğœ)
ğ¹(ğ‘Ÿ);
20ğ›¿(ğ‘Ÿ,/angbracketleft/angbracketright,ğ‘Ÿ)=1âˆ’/summationtext.1
ğœâˆˆXğ‘ƒ(ğ‘Ÿ,ğ‘ŸÂ·ğœ);
21Letğ‘„0={/angbracketleft/angbracketright}which only contains the root node;
22Letğœ‡0be the initial distribution which transits to the root
node (/angbracketleft/angbracketright) with probability 1;
23Letğ‘„ğ‘“be the set of leaf nodes in R;
24Construct the PFA as /angbracketleftX,R,ğ›¿,ğ‘„0,ğ‘„ğ‘“,ğœ‡0/angbracketright.
whereğ¹(ğ‘›)is the number of times ğ‘›appears in prefix(ğ›¼(ğ‘‹)).I n
addition, the probability that a node transits to itself is ğ‘ƒ(ğ‘›,ğ‘›)=
1âˆ’/summationtext.1
ğœâˆˆXğ‘ƒ(ğ‘›,ğ‘›Â·ğœ). However, the FPT is not a good model due to
itssize.In otherwords,althoughtheFPT representsthesetofab-
stractstatesprecisely,thereisnogeneralization(a.k.a.over-fitting).
To construct a concise PFA, we generalize the FPT by repeatedly
merging the nodes. Intuitively, two nodes are merged if and only if
they have similar future behaviors, which is determined through a
compatibility test.
Compatibility test Two nodes are considered compatible (and thus
to be merged) if they agree on the last letter, and their future prob-
ability distributions are sufficiently similar [29]. While the former
is easy to check, to check the latter, we compare the differences
between the probability of all paths from the two nodes in the FPT
and check if the difference is within a certain bound. Note that the
pathprobabilityistheproductoftheone-stepprobabilities.Thatis,
theprobabilityofapath ğœ‹=/angbracketleftğœ1ğœ2Â·Â·Â·ğœğ‘˜/angbracketrightfromanode ğ‘›isdefined
asğ‘ƒ(ğ‘›,ğœ‹)=ğ‘ƒ(ğ‘›,ğ‘›Â·ğœ1)Â·ğ‘ƒ(ğ‘›Â·ğœ1,ğœ2)Â·Â·Â·ğ‘ƒ(ğ‘›Â·ğœ1Â·Â·Â·ğœğ‘˜âˆ’1,ğœğ‘˜).
Formally,thefutureprobabilitydistributionsoftwonodes ğ‘›and
ğ‘›/primeare sufficiently similar if and only if for all path ğœ‹
âˆ€ğœ‹,|ğ‘ƒ(ğ‘›,ğ‘›Â·ğœ‹)âˆ’ğ‘ƒ(ğ‘›/prime,ğ‘›/primeÂ·ğœ‹)|</radicalbig
6ğœ–ğ‘™ğ‘œğ‘”(ğ¹(ğ‘›))/ğ¹(ğ‘›)+
/radicalbig
6ğœ–ğ‘™ğ‘œğ‘”(ğ¹(ğ‘›/prime))/ğ¹(ğ‘›/prime)(4)whereğ‘ƒ(ğ‘›,ğ‘›Â·ğœ‹)isthepathprobabilityasdefinedaboveand ğœ–is
a constant coefficient. Note that a larger ğœ–means that more nodes
would pass the compatibility test and consequently be merged.
In this work, we set ğœ–to be 64 following the empirical results
shown in [ 29]. For instance, the node marked ğ‘ğ‘and the node
markedğ‘ğ‘ğ‘shown in Figure 3 form a pair of compatible nodes as
theirlastclustersarethesame(i.e., ğ‘)andtheirfutureprobability
distributions are similar (i.e., both with no future paths).
Merging nodes In order to systematically identify and merge the
nodes in the FPT, we maintain two sets of nodes, i.e., a set of red
nodesR(see line 2 in Algorithm 1) which are to be transformed
into states in the learned PFA and a set of bluenodesB(see line
3)whicharenodespotentiallytobemergedintothoserednodes.
Initially,Ris empty and Bonly contains the root node.
Next,theloopfromline4to15systematicallycheckseverynode
inBtoseewhetheritiscompatiblewithanyrednode(atline8).If
there is one, the blue node is merged to the compatible red node at
line 11. Otherwise, the blue node is turned to a red one and added
intoRatline14.Afterthat,weaddthechildrenofthebluenode
to the blue set at line 15 (unless the blue node is a leaf node).
Atline11,abluenode ğ‘ismergedintoarednode ğ‘Ÿisasfollows.
We update the frequency function of both the ancestors and de-
scendant of the red node ğ‘Ÿ. In particular, for any of ğ‘Ÿâ€™s ancestor ğ‘Ÿğ‘,
we add its frequency ğ¹(ğ‘Ÿğ‘)byğ¹(ğ‘); and for any of ğ‘Ÿâ€™s descendants
ğ‘Ÿğ‘‘, letğœ‹ğ‘‘be the onwards path from ğ‘Ÿ, we add the frequency ğ¹(ğ‘Ÿğ‘‘)
byğ¹(ğ‘Â·ğœ‹ğ‘‘).Inaddition,weaddanedgefrom ğ‘â€™sparentto ğ‘Ÿ(since
ğ‘is now merged into ğ‘Ÿ).
For example, Figure 3 illustrates how two compatible nodes are
merged.OntheleftistheoriginalFPT,wherethenode ğ‘ğ‘andnode
ğ‘ğ‘are assumed to be compatible and thus to be merged. On the
right is the updated tree after merging, where the frequency of
nodeğ‘ğ‘and all its ancestors are updated.
PFA construction The loop from line 4 to 15 runs until there are
no more blue nodes to be merged. Afterwards, we construct thePFA from line 16 to line 24 as follows. The remaining nodes inthe FPT (which are all in the red set now) are turned into statesin the PFA. The transitions between states in the PFA are thenconstructed systematically based on the tree edges from line 17
to line 20. Take one red node
ğ‘Ÿas an example. For each ğœâˆˆX,
thetransitionprobabilityfrom ğ‘Ÿtoğ‘ŸÂ·ğœisdefinedas ğ‘ƒ(ğ‘Ÿ,ğ‘ŸÂ·ğœ)=
ğ¹(ğ‘ŸÂ·ğœ)/ğ¹(ğ‘Ÿ)(line 19). The probability of transition to itself is
1âˆ’/summationtext.1
ğœâˆˆXğ‘ƒ(ğ‘Ÿ,ğ‘ŸÂ·ğœ)(line20).Thesetofinitialstatesonlycontainsthe
rootnode,i.e.,theemptytrace /angbracketleft/angbracketright(line21).Theinitialdistribution
associates probability 1 with the root node (line 22). Note that the
setofacceptingstatesinthePFAareexactlythelabelset ğ¼sinceall
the leaf nodes with the same ending letter ğ‘ ğ‘“âˆˆğ¼will be merged as
onestateastheirfuturedistributionsarethesame(line23).Finally,
we construct the PFA at line 24.
Example 3.3. Central to the conversion of FPT is to build the
probabilistic transition function ğ›¿(from line 16 to 20). For the
sake of simplicity, we take the node ğ‘and its outgoing transi-
tions in the left FPT in Figure 3 as an example to illustrate the
conversion. In this FPT, Xcontains two symbols: [ğ‘,ğ‘], andRis
[<>,ğ‘,ğ‘,ğ‘ğ‘,ğ‘ğ‘,ğ‘ğ‘,ğ‘ğ‘ğ‘,ğ‘ğ‘ğ‘,ğ‘ğ‘ğ‘ ]. For node ğ‘âˆˆR,ğ›¿(ğ‘,ğ‘,ğ‘Â·ğ‘)=
ğ¹(ğ‘Â·ğ‘)/ğ¹(ğ‘)andğ›¿(ğ‘,ğ‘,ğ‘Â·ğ‘)=ğ¹(ğ‘Â·ğ‘)/ğ¹(ğ‘).Accordingtotheleft
FPT,ğ¹(ğ‘),ğ¹(ğ‘Â·ğ‘)andğ¹(ğ‘Â·ğ‘)is80,50and30respectively.Thus,
503ASE â€™20, September 21â€“25, 2020, Virtual Event, Australia Guoliang Dong, Jingyi Wang, Jun Sun, Yang Zhang, Xinyu Wang, Ting Dai, Jin Song Dong, and Xingen Wang

 
  
 





	 

  
 
 





	 


Figure 3: Merging nodes
Algorithm 2: Overall(ğ‘‹,ğ‘…,ğ›¾ ğ‘,ğœ–)
1Letğ¾be2;
2whilenot time out do
3Obtain the clustering function ğ¶ğ‘˜using K-means;
4ğ›¼(ğ‘‹)â†Î¨(ğ‘‹,ğ‘…,ğ¶ ğ‘˜);
5Aâ†ğ‘’ğ‘¥ğ‘¡ğ‘Ÿğ‘ğ‘ğ‘¡(ğ›¼(ğ‘‹),ğœ–);
6LetğœŒ=ğ‘ƒ(A(ğ‘¥)=ğ‘…(ğ‘¥)|A);
7ifğœŒâ‰¥ğ›¾ğ‘then
8 returnA;
9Increase ğ¾by 1;
10returnA
wecangetthatthetransitionprobabilityfrom ğ‘toğ‘ğ‘undersymbol
ğ‘is 0.625, the transition probability from ğ‘toğ‘ğ‘under symbol ğ‘is
0.375 and the transition probability from ğ‘to itself is 0.
3.3 Model Selection
Recall that we aim to extract a small model that approximates the
RNNaccurately(i.e.,bymakingthesamedecisiononasmanyinputs
as possible). The size of the extracted model matters for human
interpretation as well as potential tool-based analysis. The number
ofstatesinthelearnedPFAislargelydeterminedbythenumberof
clusters.Usually,themoreclustersweuse,themoreaccuratethe
extractedmodelwillbe(whichisevaluatedinSection4.1).Inthe
extreme case, if we consider each valuation of the numeric vectors
as a cluster, we would have a huge PFA which is perfectly accurate
but hardly interpretable. Thus, we do not attempt to generate a
modelthatapproximatesanRNNmodelasaccuratelyaspossible,asitoften leadstomodelswith manystates.Instead,wegenerate
models with a user-required level of consistency with the RNN.
Such consistency can be measured using fidelity [21, 51].
Givenauser-requestintheformofâ€œgeneratingamodelwhich
hasa90%fidelitycomparedtotheRNNâ€,thequestionisthenhowtodeterminetheâ€˜rightâ€™numberofclusters.Ouransweristogradually
increasethenumberofclustersuntilamodelsatisfyingtheuser-
requestisgenerated.OuroverallalgorithmisshowninAlgorithm2,
whereğ‘‹isthesetofconcretetraces(i.e.,thesequenceofvaluations
ofthehiddenfeaturevectorsoftheRNN)generatedbythesamples
inthetrainingset; ğ‘…istheRNNmodel; ğ›¾ğ‘istherequiredfidelity
of the extracted model; and ğœ–is the parameter for compatibility/g1
/g2/g3
/g4/g5/g6/g5/g1/g4/g1/g7/g5
/g8/g5/g6/g9/g2/g10/g1/g7/g3
/g5/g6/g5/g5/g8/g8/g7/g11
/g5/g6/g3/g3/g1/g1/g7/g12/g5/g6/g13/g2/g3/g8/g7/g5
/g5/g6/g5/g8/g9/g9/g7/g3/g5/g6/g5/g10/g5/g4/g7/g11
/g5/g6/g5/g3/g4/g8/g7/g12 /g5/g6/g5/g10/g3/g5/g7/g5/g5/g6/g13/g2/g14/g14/g7/g3
Figure 4: Example of a learned PFA
testing (refer to Algorithm 1). The output is an extracted PFA A
which satisfies the user-required fidelity.
Thealgorithmworksasfollows.Wefirstobtaintheclustering
functionğ¶ğ‘˜usingtheK-meansalgorithm[ 4]parameterizedby ğ¾at
line3.Notethat ğ¾isinitially2andisincreasedby1eachtime.Then
we apply the procedure Î¨(ğ‘‹,ğ‘…,ğ¶ ğ‘˜)(presented in Section 3.1) to
obtain a bag of abstract traces ğ›¼(ğ‘‹)at line 4. After that, we extract
aPF AAusingAlgorithm1atline5.Wemeasurethefidelityofthe
extractedmodelatline6.Ifthefidelitysatisfiestherequirement,i.e.,
the condition at line 7 is satisfied, the extracted model is returned
atline8.Otherwise,weincreasethenumberofclustersby1atline
9 and start over again.
To obtain a label of a given sample ğ‘¥using the extracted model,
we first generate the concrete trace of ğ‘¥given the RNN, i.e., the
sequence of valuations of the hidden feature vectors of the RNN.
Next,anabstracttraceisextractedusingtheapproachpresentedinSection3.1.Notethattheabstracttraceisintheformofasequence
of letters, each of which represents a cluster except the last one
which represents the label. Next, we remove the label from the
abstracttraceandsimulateitontheextractedPFA(i.e.,fromthe
initial state of the PFA, for each letter in the abstract trace, we takethecorrespondingtransitionofthePFA).Let
ğ‘ bethelaststatethat
is reached via the abstract trace. We then apply probabilistic model
checking techniques [ 12] to compute the probability of reaching
everylabelfrom ğ‘ .Wewrite ğ‘ƒ(ğ‘¥,ğ‘¦)whereğ‘¦isalabeltodenotethe
above-computedprobability forlabel ğ‘¦.The labelwiththelargest
probabilityisthengeneratedasthepredicatedlabelbytheextracted
PFA.
Example3.4. Figure4showsthePFAextractedfromaGRUmodel
trained on RTMR with 2 clusters. Recall that the abstract trace for
the sample text shown in Example 3.1 is /angbracketleftğ‘ ,1,1,1,0,0,0,0,0,0,ğ‘ƒ/angbracketright
(asdiscussedinExample3.2).Simulatingthetrace(excludingthe
labelğ‘ƒ) on the PFA shown in Figure 4, we end up with state 2. Ap-
plying probabilistic model checking, we obtain that the probability
of reaching state 5 (representing label ğ‘) is 0.1537, whereas the
probabilityofreachingstate4(representinglabel ğ‘ƒ)is0.8463.Thus,
the prediction is â€œpositiveâ€.
We remark that the extracted PFA predicts the label of a sample
basedonthetraceoftheRNN,notthesampleitself.ThisisbecausethePFAismeanttofacilitateinterpretationoftheRNNratherthan
being a predictive model itself.
4 EVALUATION
Our approach has been implemented as a self-contained prototype,
based on Pytorch [ 1] and scikit-learn with about 3800 lines of code.
504Towards Interpreting Recurrent Neural Networks through Probabilistic Abstraction ASE â€™20, September 21â€“25, 2020, Virtual Event, Australia
Our implementation (including the source code, the dataset, and
the trained models) is available at [2].
In the following, we evaluate our approach from two aspects.
First,weevaluateitseffectivenessintermsofextractingPFA,i.e.,is
it capable of generating small PFA which accurately approximates
the RNN? Second, we evaluate its usefulness, i.e., other than being
usefulforhumaninterpretation,canweusetheextractedPFAto
solve real-world problems?
4.1 Effectiveness
To evaluate the effectiveness of our approach, we design exper-
iments to evaluate how well the extracted PFA approximate the
RNN. Our test subjects are RNN trained on the following datasets.
â€¢TomitaGrammars isanartificialdatasetcontainingstrings
generated using different grammars. These grammars were
previouslyadoptedforresearchonRNNmodelextraction[ 45,
46]. They consist of 7 regular languages with different com-
plexityoveralphabet {0,1}.Thedetaileddefinitionsofthe
grammarsarelistedinTable2.Astringislabeledpositive
if it is valid according to the grammar. We apply the same
settingasin[ 46]togenerateatrainingsetandtestsetbased
on the grammars. That is, we craft the training set with var-
ious lengths for each grammar (i.e., 0-13 ,16,19,22 except
Tomita6whichhasadifferentlengthsetting),anduniformly
samplestringsoflength1 ,4,7,Â·Â·Â·,28asthetestsetforeach
grammar. The ratio between the training set and the test set
i s4t o1 .
â€¢BalancedParentheses(BP) isanartificialdatasetcontaining
strings generated with an alphabet with 28 letters (i.e., 26
lower-caselettersplusâ€˜(â€™andâ€˜)â€™).Astringinthedatasetis
labeledpositiveiftheparenthesesinthestringarebalanced,
i.e., each opening parenthesis is eventually followed by a
closing parenthesis. Following [ 46], we generate a set of
stringswithalengthof0-15 ,20,25,30andamaximumdepth
oftheparentheses11.Furthermore,thenumberofpositive
andnegativestringsforeachlengthisbalanced.Theratio
between the training set and the test set is 4 to 1.
â€¢Rotten Tomatoes Movie Review (RTMR) is a movie review
dataset collected from Rotten Tomatoes pages [ 33] for senti-
mentanalysis,which contains5331positiveand5331nega-
tivesentences.Theaveragelengthofthisdatasetisabout21.
We take all the samples inthe dataset and divide them into
two groups, i.e., 80% as a training set and 20% as a testing
set.
â€¢IMDBisawidelyusedbenchmarkforsentimentanalysisclas-
sification.Itcontains50 ,000moviereviewswhichareequally
splitintoatrainingsetandatestset.Intotal,thereare25k
positive reviews and 25k negative reviews. The dataset is
well collected and processed in a way that makes sure the
reviews are as independent as possible. Since the samples
in the dataset are much longer than those in RTMR and thesize of the dataset is much bigger, to reduce the experiment
time,wetakethosesampleswithalengthlessthan51.We
alsokeep80%oftheselecteddataasthetrainingsetand20%
as the test set.Table 2: Tomita grammars
Grammar Definition
Tomita1 1*
Tomita2 (10)*
Tomita3the complement of
((0|1)* 0)*1(11)*(0(0|1)*1)*0(00)*(1(0|1)* )*
Tomita4 words not containing 000
Tomita5the number of â€œ0â€ and the number of â€œ1â€ areboth even numbers for each string
Tomita6the difference between the number of â€œ0â€
and the number of â€œ1â€ is a multiple of 3
Tomita7 0*1*0*1*
Table 3: Size of the datasets
Task Dataset Training set Test set
Artificial DatasetTomita1 624 156
Tomita2 619 160
Tomita3 2898 724
Tomita4 2911 727
Tomita5 2311 577
Tomita6 3791 947
Tomita7 2878 719
BP 3978 995
Real-world DatasetRTMR 8528 2134
IMDB 3730 933
Table 4: Performance of target models
DatasetLSTM GRU
Training set Test set Training set Test set
RTMR 79.88% 77.46% 80.96% 77.32%
IMDB 88.07% 84.35% 88.36% 84.24%
BP 99.92% 100% 99.90% 99.90%
Tomita1 99.04% 98.72% 99.20% 98.72%
Tomita2 99.67% 99.38% 99.78% 99.38%
Tomita3 100% 99.86% 99.90% 99.86%
Tomita4 99.90% 100% 100% 100%
Tomita5 73.47% 74.52% 73.73% 75.39%
Tomita6 65.34% 63.99% 64.63% 66.42%
Tomita7 99.51% 100.00% 99.34% 100.00%
Table3summarizesthesizeofalldatasets.WetrainRNNmodels
to classify the strings in each dataset. We adopt two popular types
of RNNs, i.e., LSTM and GRU. We set the dimensions of hidden
states and the number of hidden layers for the two RNNs as 512
and 1 respectively as in [ 50]. When training the models, we use
one-hot encoding to encode each character of the artificial dataset,
and use word2vec [ 30] to transform each word of the real-world
dataset into a 300-dimensions numerical vector. Table 4 shows the
performance of the trained models, all of which is similar to the
state-of-the-art performance. In total, we have 20 models.
Weapplyourapproachtoall20modelstoextractmodels.We
evaluatewhetherthemodelspreciselyapproximatetheRNNsusingtwomeasurements,i.e.,accuracyandfidelity.Theformermeasures
thepercentageofthesamplesinthetestsetforwhichtheextracted
505ASE â€™20, September 21â€“25, 2020, Virtual Event, Australia Guoliang Dong, Jingyi Wang, Jun Sun, Yang Zhang, Xinyu Wang, Ting Dai, Jin Song Dong, and Xingen Wang
/g28/g22/g31/g28/g22/g32/g28/g22/g33/g28/g22/g34/g28/g22/g35/g28/g22/g36/g29
/g30/g31/g33/g35/g29 /g28/g1/g11/g11/g20/g17/g9/g11/g21
/g5/g20/g15/g10/g12/g17/g16/g13/g11 /g14/g20/g18/g19/g12/g17/g18/g6/g8/g4/g6/g23/g3/g7/g8/g4/g24/g34/g34/g22/g31/g33/g39/g25/g3/g2/g1
 /g2/g3/g29
/g29/g23/g32/g29/g23/g33/g29/g23/g34/g29/g23/g35/g29/g23/g36/g29/g23/g37/g30
/g31/g32/g34/g36 /g30 /g29/g1/g12/g12/g21/g18/g10/g12/g22
/g7/g21/g16/g11/g13/g18 /g17/g14 /g12/g15/g21/g19/g20/g13/g18/g19/g4/g6/g3/g2/g24/g5/g8/g9/g6/g25/g37/g29/g23/g37/g33/g40/g26/g3/g2/g1
 /g2/g5/g30
/g28/g22/g31/g28/g22/g32/g28/g22/g33/g28/g22/g34/g28/g22/g35/g28/g22/g36/g29
/g30/g31/g33/g35/g29 /g28/g1/g10/g10/g20/g17/g8/g10/g21
/g5/g20/g15/g9/g11/g17/g16/g12/g10 /g14/g20/g18/g19/g11/g17/g18/g7/g16/g15/g13/g19/g8/g23/g3/g6/g7/g4/g24/g36/g28/g22/g36/g30/g39/g25/g3/g2/g1
 /g2/g3/g29
/g28/g22/g31/g28/g22/g32/g28/g22/g33/g28/g22/g34/g28/g22/g35/g28/g22/g36/g29
/g30/g31/g33/g35/g29 /g28/g1/g11/g11/g20/g17/g9/g11/g21
/g5/g20/g15/g10/g12/g17/g16/g13/g11 /g14/g20/g18/g19/g12/g17/g18/g2/g6/g23/g3/g7/g8/g4/g24/g29/g28/g28/g39/g25/g3/g2/g1
 /g2/g3/g29
/g29/g23/g32/g29/g23/g33/g29/g23/g34/g29/g23/g35/g29/g23/g36/g29/g23/g37/g30
/g31/g32/g34/g36 /g30 /g29/g1/g12/g12/g21/g18/g10/g12/g22
/g6/g21/g16/g11/g13/g18 /g17/g14 /g12/g15/g21/g19/g20/g13/g18/g19/g3/g2/g1
 /g2/g4/g30/g7/g8/g5/g7/g24/g3/g7/g9/g25/g35/g35/g23/g32/g34/g40/g26
/g30/g24/g33/g30/g24/g34/g30/g24/g35/g30/g24/g36/g30/g24/g37/g30/g24/g38/g31
/g32 /g33 /g35 /g37/g31 /g30/g1/g13/g13/g22/g19/g11/g13 /g23
/g8/g22/g17/g12/g14/g19/g18/g15/g13/g16/g22/g20/g21/g14/g19/g20/g5/g7/g3/g2/g25/g4/g9/g10/g26/g37/g33/g24/g32/g33 /g41/g27 /g3/g2/g1
/g2/g6/g31
/g29/g23/g32/g29/g23/g33/g29/g23/g34/g29/g23/g35/g29/g23/g36/g29/g23/g37/g30
/g31 /g32 /g34/g36 /g30 /g29/g1/g11/g11/g21/g18 /g9/g11/g22
/g5/g21/g16/g10/g12/g18 /g17/g13 /g11/g15/g21/g19/g20/g12/g18/g19/g7/g17/g16/g14/g20 /g9/g24/g3/g6/g8/g25/g37/g30/g23/g29/g36/g40/g26/g3/g2/g1
/g2/g4/g30
/g28/g22/g31/g28/g22/g32/g28/g22/g33/g28/g22/g34/g28/g22/g35/g28/g22/g36/g29
/g30/g31/g33/g35/g29 /g28/g1/g11/g11/g20/g17/g9/g11/g21
/g5/g20/g15/g10/g12/g17/g16/g13/g11/g14/g20/g18/g19/g12/g17/g18/g2/g6/g23/g3/g7/g8/g24/g36/g36/g22/g36/g39/g25/g3/g2/g1
/g2/g4/g29
Figure 5: The accuracy of our approach vs. BL1
/g27/g23/g30/g27/g23/g31/g27/g23/g32/g27/g23/g33/g27/g23/g34/g27/g23/g35/g28
/g29 /g30/g32/g34 /g28/g27/g2/g14/g11/g12/g15/g14/g20/g22
/g5/g21/g16/g9/g12/g18 /g17/g13 /g10/g15/g21/g19/g20/g12/g18/g19/g6/g8/g4/g6/g24/g3/g7/g8/g4 /g3/g2/g1
/g1/g3/g28
/g28/g24/g31/g28/g24/g32/g28/g24/g33/g28/g24/g34/g28/g24/g35/g28/g24/g36/g29
/g30/g31/g33/g35/g29 /g28/g3/g15/g12/g13/g16/g15/g21/g23
/g7/g22/g17/g10/g13/g19/g18/g14/g11/g16/g22/g20/g21/g13/g19/g20/g4/g6/g2/g1/g25/g5/g8/g9/g6/g3/g2/g1
/g1/g5/g29
/g27/g23/g30/g27/g23/g31/g27/g23/g32/g27/g23/g33/g27/g23/g34/g27/g23/g35/g28
/g29/g30/g32/g34 /g28 /g27/g2/g14/g11/g12/g15/g14/g20/g22
/g5/g21/g16/g9/g12/g18/g17/g13/g10/g15/g21/g19/g20/g12/g18/g19/g7/g17/g16/g14/g20/g8/g24/g3/g6/g7/g4 /g3/g2/g1
/g1/g3/g28
/g27/g23/g30/g27/g23/g31/g27/g23/g32/g27/g23/g33/g27/g23/g34/g27/g23/g35/g28
/g29/g30/g32/g34/g28 /g27/g2/g14/g11/g12/g15/g14/g20/g22
/g5/g21/g16/g9/g12/g18/g17/g13/g10/g15/g21/g19/g20/g12/g18/g19/g1/g6/g24/g3/g7/g8/g4/g3/g2/g1
/g1/g3/g28
/g28/g24/g31/g28/g24/g32/g28/g24/g33/g28/g24/g34/g28/g24/g35/g28/g24/g36/g29
/g30/g31/g33/g35/g29 /g28/g2/g15/g12/g13/g16/g15/g21/g23
/g6/g22/g17/g10/g13/g19/g18/g14/g11/g16/g22/g20/g21/g13/g19/g20/g3/g2/g1
/g1/g4/g29/g7/g8/g5/g7/g25/g3/g7/g9
/g29/g25/g32/g29/g25/g33/g29/g25/g34/g29/g25/g35/g29/g25/g36/g29/g25/g37/g30
/g31/g32/g34/g36 /g30/g29/g3/g16/g13/g14/g17/g16/g22/g24
/g8/g23/g18/g11/g14/g20/g19/g15/g12/g17/g23/g21/g22/g14/g20/g21/g5/g7/g2/g1/g26/g4/g9/g10 /g3/g2/g1
/g1/g6/g30
/g28/g24/g31/g28/g24/g32/g28/g24/g33/g28/g24/g34/g28/g24/g35/g28/g24/g36/g29
/g30/g31/g33/g35/g29 /g28/g2/g15/g12/g13/g16/g15/g21/g23
/g5/g22/g17/g10/g13/g19/g18/g14/g11/g16/g22/g20/g21/g13/g19/g20/g7/g18/g17/g15/g21 /g9/g25/g3/g6/g8/g3/g2/g1
/g1/g4/g29
/g27/g23/g30/g27/g23/g31/g27/g23/g32/g27/g23/g33/g27/g23/g34/g27/g23/g35/g28
/g29/g30/g32/g34/g28 /g27/g2/g14/g11/g12/g15/g14/g20/g22
/g5/g21/g16/g9/g12/g18/g17/g13/g10/g15/g21/g19/g20/g12/g18/g19/g1/g6/g24/g3/g7/g8 /g3/g2/g1
/g1/g4/g28
Figure 6: The fidelity of our approach vs. BL1
model generatethe correctlabel. Thelatter measureshow consis-
tent the extracted model and the RNN model are, which is defined
as follows.
ğ¹ğ‘–ğ‘‘ğ‘’ğ‘™ğ‘–ğ‘¡ğ‘¦=/summationtext.1
ğ‘¥âˆˆğ‘‡ğ‘†ğ‘–ğ‘”ğ‘›(A(ğ‘¥)=ğ‘…(ğ‘¥))
|ğ‘‡|(5)
whereAistheextractedmodel; ğ‘…istheRNN; ğ‘‡isthetestset; ğ‘¥is
any sample in the test case; and ğ‘†ğ‘–ğ‘”ğ‘›(ğ‘¦)is a sign function which
equals 1 if ğ‘¦holds and 0 otherwise. In the following, we discuss
the experiment results via a comparison with existing approaches.
There are three approaches which we can potentially compare to.
Firstbaseline Thefirstoneistheapproachproposedin[ 21](referred
toasBL1),whichextractsDFAfromRNN.Theysimilarlyreduce
the hidden state space through clustering and then regard each
cluster as a state of the learned automaton. Next, they map the
hiddenstate traceof eachinput intoan abstracttrace.Finally,the
transitionsbetweenstatesintheabstractstatetracesthatoccurthe
most frequent are taken as transitions in the learned automaton.
ForasystematiccomparisonwithBL1,wevarythenumberof
clusters used for abstracting the hidden states for both approaches.
Figure 5 shows the accuracy of extracted models using BL1 andour approach respectively. Notice that the results on the Tomita
Grammars are the average of the 7 grammars for the sake of space.
Itcanbeobservedthatthemodelsextractedwithourapproachare
significantly more accurate than those generated by BL1 across all
20models.Whileourapproachconsistentlyachievesanaccuracyof70%tonearly90%,BL1â€™saccuracyrangesfrom50%toslightlyabove 60%.This isexpected asthe modelsthat BL1 extractsonly
containtransitionswithmaximumfrequencywhileourapproach
is able to preserve all transitions through a probability distribution.
Furthermore, it can be observed that in most cases the models
extractedusingourapproachhaveaperformancesimilartothatof
the original RNN models, i.e., most of the extracted models have
anaccuracywithin10%differencewiththeoriginalmodels.This
suggests that our approach is capable of extracting precise models
which have similar performance with the RNN.
In terms of fidelity, as shown in Figure 6, it can be observed that
ourapproachissignificantlybetterthanBL1aswell.Thefidelity
ofthemodelsextractedusingourapproachrangesfrom82%(BP-LSTM) to over 95%, whereas that of the models extracted using
BL1rangesfrom52%(BP-LSTM)toabout65%only.Specifically,thefidelitycomparisonforTomitagrammars,BP,RTMRandIMDBare
94% vs. 64%, 85% vs. 55%, 89% vs. 64% and 96% vs. 62% respectively.
Thedifferencesaremorenoticeableforreal-worldcomplextasks
like IMDB. One possible reason is that the real-world datasets are
complicated and the idea underling BL1 does not apply in the real-
worldsetting.Incomparison,ourprobabilisticabstractionapproach
is capable of taking into consideration the probability distribution
amongtheabstractstatesandthusextractaccuratemodels.Note
thatourapproachextractsmodelswithhighfidelity,i.e.,mostof
the models have fidelity larger than 90%, which shows that our
extracted models often precisely approximate the RNNs.
506Towards Interpreting Recurrent Neural Networks through Probabilistic Abstraction ASE â€™20, September 21â€“25, 2020, Virtual Event, Australia
Table 5: Accuracy comparison between our approach and BL2
ModelMethod Tomita1 Tomita2 Tomita3 Tomita4 Tomita5 Tomita6 Tomita7 BPRTMR IDMB
LSTMOurs 0.9872 0.9625 0.913 0.9697 0.7487 0.6167 0.9513 0.8642 0.7601 0.8264
BL2 0.9872 0.9812 1 10.7452 0.6420 10.997timeout timeout
GRUOurs 0.9872 0.9625 0.942 0.967 0.7227 0.6135 0.9875 0.9869 0.7629 0.7964
BL2 0.9872 0.9812 1 10.7522 0.6441 10.997timeout timeout
Table 6: Fidelity comparison between our approach and BL2
ModelMethod Tomita1 Tomita2 Tomita3 Tomita4 Tomita5 Tomita6 Tomita7 BPRTMR IDMB
LSTMOurs 10.9563 0.9144 0.9697 0.9931 0.8648 0.9513 0.8642 0.9349 0.9528
BL2 10.9938 0.9986 10.9757 0.9007 10.997timeout timeout
GRUOurs 10.9563 0.9434 0.967 0.9584 0.8807 0.9875 0.9879 0.9203 0.8553
BL2 10.9938 0.9986 10.9775 0.905 10.996timeout timeout
Second baseline The second approach we compare to is the one
in[46](referredtoasBL2),whichappliestheL*algorithm[ 3]to
extract a DFA from RNN. It first builds a DFA based on an observa-
tion table,and then buildsan abstract DFAfrom the RNN withan
intervalpartitionfunction(i.e.,aheuristic-basedabstraction).After
that,itcheckstheequivalencebetweenthetwoDFAsandrefines
oneofthemifaconflictoccurs.Thealgorithmrepeatstheabove
procedure until the two DFAs are equivalent and returns the DFA.
We remark that checking equivalence of two DFAs is expensive
and impractical when the alphabet is large which is the case for
real-world tasks.
Since there is no clustering in BL2, we apply Algorithm 2 with a
fidelityrequirementof ğ›¾ğ‘=0.99andatimeoutof400seconds.Note
that the same timeout is set for BL2, which is also the one adopted
in[46].TheresultsintermsofaccuracyareshowninTable5.First,
it can be observed that BL2 fails to work on either the RTMR or
IMDB dataset. This is expected as the alphabet of these datasets is
the vocabulary of the training set, which is 20995 for RTMR and
388441 for IMDB. They are thus way beyond the capability of BL2.
Second,ourapproachachievescompetitiveresultswith BL2onthe
two artificial datasets, i.e., on average, BL2 has an accuracy that is
3.32% more than our approach. The results of comparing fidelity
areshowninTable6.Wecanobservethatourapproachachieves
high fidelity, i.e., 94.29% on average, with the RNN model on all
datasetandBL2failstoreporttheresultsonbothRTMRandIMDB
forthesamereason.Onthetwoartificialdatasets,BL2hasafidelity
that is 3.39% more than our approach on average.
Thirdbaseline Thethirdapproachistherecentapproachreported
in [47]. It learns a probabilistic model for approximating RNN
throughanextendedversionoftheL*algorithm.Whileithasim-
pressiveperformanceontaskswithasmallalphabet(likeinthecase
of the two artificial datasets), the authors admittedly report [ 47]
that their approach does not apply when the alphabet is large (like
in the case of the two real-world datasets). This is confirmed in
ourexperimentsaswell,i.e.,theirimplementationfailedtowork
oneitherRTMR orIMDB.Weomitadetailedcomparison dueto
its limited applicability and the fact that it is not implemented for
classification tasks, which makes it hard to compare to.Based on the above experiment results, we thus conclude that our
approach is able to extract accurate models from RNN and is capable
of handling real-world RNN models.
4.2 Level of Abstraction
Our approach allows users to specify a target fidelity and aims
to extract a model based on the right level of abstraction. This is
donethroughcontrollingthenumberofclusters,whichdetermines
the size of the alphabet and consequently the size of the extracted
models.Algorithm2isdesignedbasedonthehypothesisthatthe
more clusters we use, the more fine-grained the abstraction is and
thus the more accurate the extracted model will be. Since the more
clustersweuse,themorecomplicated(i.e.,thelesscomprehensive)
the extracted model will be, it is important tofind a balance.
Toevaluatewhetherthishypothesisholdsandunderstandthe
relationshipbetweenthenumberofclustersandthefidelity/sizeof
theextractedmodels,weconductthefollowingexperiments.We
systematically extract models with clusters ranging from 2 to 10
and evaluate the size and accuracy/fidelity of the extracted models.
Table 7 summarizes how the size of extracted PFA changes with
different numbers of clusters.Note that these results are basedon
applyingourapproachtotheGRUmodels.Similarresultsareob-
tained on the LTSM models and are thus omitted. We observe that
asweincreasethenumberofclusters,thesizeoftheextractedmod-
els increases in most of the cases. However, it is not monotonically
so. For instance, for the BP dataset, the number of states decreases
whenthenumberofclustersincreasesfrom6to10.Thisisbecause
thenumberofstatesisdeterminedjointlybythenumberofclusters
andthedegreeofgeneralizationachievedbyAlgorithm1.Itisthus
possible that in some cases, the same cluster (of hidden feature
values) behaves differently in different contexts (e.g., the sequence
of feature values before reaching the cluster) and thus must be
differentiated into multiple states in the extracted PFA.
Figure 7 shows the relationship between the number of clusters
and the accuracy/fidelity of the extracted models. We observe that
as weincrease the numberof clusters,the accuracy/fidelity ofthe
extracted models improves overall. The improvement, however,
mayvaryacrossdifferentmodelsordifferentnumberofclusters.
For some models, the improvement is consistent and significant,
e.g.,inthecaseoftheBPdataset;forsomemodels,theimprovement
isconsistentbutminor,e.g.,inthecaseofTomitagrammarsand
507ASE â€™20, September 21â€“25, 2020, Virtual Event, Australia Guoliang Dong, Jingyi Wang, Jun Sun, Yang Zhang, Xinyu Wang, Ting Dai, Jin Song Dong, and Xingen Wang
/g27/g24/g31/g33/g27/g24/g32/g33/g27/g24/g33/g33/g27/g24/g34/g33
/g29/g30/g31 /g33 /g28/g27/g2/g14/g14/g22/g19/g12/g14/g23
/g4/g16/g22/g20/g21/g15/g19/g1/g18/g22/g17/g13/g15/g19/g20/g11/g8/g7/g6/g11/g2 /g3/g9 /g10/g11/g7/g10 /g6/g7/g5/g3
/g28/g25/g32/g34/g28/g25/g33/g34/g28/g25/g34/g34/g28/g25/g35/g34
/g30/g31/g32/g34 /g29/g28/g6/g16/g14/g15/g17/g16/g22/g24
/g4/g17/g23/g21/g22/g15/g20/g1/g19/g23/g18/g13/g15/g20/g21/g12/g9/g8/g7/g12/g2 /g3/g10 /g11/g12/g8/g11 /g7/g8/g5/g3
Figure 7: Effects of different number of clusters.
Table 7: The model size with different numbers of clusters
DatasetNumber of cluster
246810
Tomita 57131216
BP 510261513
RTMR 511181248
IMDB 546465671505
RTMR; and for some models, the accuracy/fidelity may drop along
the way, e.g., in the case of IMDB. For the last case, we suspect
that it is due to the fact the RNN model is very complicated and
Algorithm1failedtoconvergetoaconcise/accuratemodelaswe
noticethatthenumberofstatesincreasesdramaticallywhenwe
increase the number of clusters. This suggests a future research
direction on developing new learning algorithms for probabilistic
models that are effective with a large alphabet and complicated
probabilistic distribution. Note that existing work such as the one
in [46, 47] is limited to models with very small alphabets.
4.3 Usefulness
We have shownthat our approach is able to extract models which
approximateRNNaccurately.Thisfacilitatessomedegreeofhuman
interpretationandautomaticanalysisofRNN.Forinstance,given
anRNNtrainedforsentimentanalysis,withtheextractedmodel,wecansystematicallycomputetheprobabilityofgeneratingaâ€˜positiveâ€™
label (through manual computation if the model is very small or
probabilistic modelchecking otherwise) afterprovided witheach
word in a sentence. By monitoring how the probability changes
witheachword,wecandevelopsomeintuitiveunderstandingon
how the sentiment analysis result is derived. Such usefulness is,
however,subjective.Inthefollowing,wereportanapplicationof
the extracted PFA models for adversarial text detection.
GivenanRNNmodel,adversarialtextsaretextswhicharecrafted
specifically to induce mistake (i.e., so that the RNNâ€™s classificationresultiswrong).Ithasbeenshownin[ 16,26]thatadversarialtexts
canbesystematicallygeneratedbyapplyingasmallperturbationto
a benign text (which otherwise is correctly classified by the RNN).
Typicalwaysofgeneratingadversarialtextsincludeidentifyingand
replacing important words in a sentence with its synonyms [ 26]o r
applyingNeuralMachineTranslationtwice(e.g.,fromEnglishto
French and then back) to the given sentence. Detecting adversarial
textsishighlynontrivialandtothebestofourknowledge,there
have not been systematic methods proposed for that.
In the following, we show that the PFA models extracted using
our method can be used to detect adversarial texts effectively. The
intuition is that, given a benign text, our PFA would associate a
much higher probability with its label (predicted by the RNN) than
otherlabels;andgivenanadversarialtext,theprobabilityassociated
with each label would not be very different. This intuition is partly
basedonthefactthattheseadversarialtextsaretypicallygenerated
by perturbing a benign text just enough to across the classification
boundary. Based on this intuition, we design the following metric
todetectadversarialtexts.Givenatext ğ‘¥(whichcouldbebenign
or adversarial), let
ğ‘‡(ğ‘¥)=ğ‘ƒ(ğ‘¥,ğ‘¦)
ğ‘ƒ(ğ‘¥,ğ‘¦)(6)
whereğ‘¦isthelabelpredictedbytheRNN, ğ‘ƒ(ğ‘¥,ğ‘¦)istheprobability
ofreaching label ğ‘¦basedon ourextractedPFA(whichis obtained
as explained in Section 3.3 using probabilistic model checking) and
ğ‘ƒ(ğ‘¥,ğ‘¦)denotes the summed probability of reaching labels other
thanğ‘¦. We then distinguish adversarial texts from benign ones
using a threshold on ğ‘‡(ğ‘¥), i.e., a text is considered as adversarial if
it has ağ‘‡(ğ‘¥)smaller than the threshold.
Weevaluatetheeffectivenessoftheabovemethodforadversarial
text detection on the two real world datesets. Concretely, for each
benigntextinthetestsetofRTMRandIMDBdatasets,wegenerate
anadversarialtextusingTEXTBUGGER[ 26].Wethenrandomly
select 1000benign textsand 1000 correspondingadversarial texts
tocomposeatestsetforourdetectionmethod.Wecalculate ğ‘‡(ğ‘¥,ğ‘¦)
for all the texts in the test set and report the AUC (Area UnderCurve) score [
17] to measure the effectiveness of our detection
methodsinceAUCavertsthesupposedsubjectivitywhenselect-
ing the threshold for a classifier and measures how true positive
rateandfalsepositiveratetradeoff.Tofurtherstudytheeffectof
havingadifferentnumberofclusters,weapplythemethodwith
PFA extracted with different number of clusters. The results are
summarized in Table 8 where we vary the number of clusters from
2 to 10. We observe that our method effectively detects adversarial
texts,i.e.,achievinganaverageAUCof0.85and0.93forRTMRand
IMDB respectively. We do also notice that the AUC varies with the
numberofclustersinawaywithnoclearcorrespondencewiththe
PFAâ€™s accuracy/fidelity, which we will investigate in the future.
The above study suggests that our model extraction approach
notonlyoffersawayofsheddingsomelightonhowRNNworks
but alsopotentially opens thedoor for applyingsoftware analysis
techniques (like model-based testing, model checking, runtime
monitoring and verification) to real-world RNN models.
5 RELATED WORK
Wereview relatedworksin thissection.From abroader point of
view,this workis relevantto theexplanation ofmachine learning
508Towards Interpreting Recurrent Neural Networks through Probabilistic Abstraction ASE â€™20, September 21â€“25, 2020, Virtual Event, Australia
Table 8: AUC of adversarial sample detection.
Dataset ModelNumber of clusters
246 810
RTMRLSTM0.57450.81010.8328 0.8453 0.8479
GRU0.67940.78000.8183 0.8409 0.8459
IMDBLSTM0.76700.87710.89490.9274 0.9173
GRU0.70140.87560.91630.9323 0.9228
models which can be categorized into local and global explana-
tionsin general.Intuitively,local explanationtriesto explainwhy
thetargetmachinelearningmodelmakesadecisiononacertain
input. One example is the SHAP-like system [ 27], which uses a
linear function to mimic the complex models, e.g., convolutional
neural network (CNN), when producing a certain output on an
input.Globalexplanation,however,aimstounderstandtheinternal
decision process by using a more interpretable model to mimic the
behaviors of the original model on anyinputs. One example is the
work in [23]. Our work takes a global explanation perspective.
This work is related to work on RNN rule extraction. Rule ex-
tractionfromRNNistheprocessofconstructingdifferentcomputa-tionalmodelswhichmimictheRNN[
5,23].Thisworkisespecially
relatedtotheworkthatextractsadeterministicfiniteautomaton
(DFA)fromRNN.Theseapproachesusuallyrelyonencodingthe
hidden states intosymbolic representationsusing techniqueslike
clustering[ 21]orintervalpartitioning[ 46].Ourworkisdifferentby
learningaprobabilisticfiniteautomaton(PFA)fromthesymbolic
data. There is also some recent work aiming to extract a weighted
automaton (WA) [ 5] or discrete-time Markov Chain [ 14]. However,
neither of them provide generalization to capture the temporal
dependency over the symbolic representations. Our work encodes
the concrete states in a similar way but then uses probabilistic
abstraction to extract a probabilistic model.
ThestudyoflearningPFAisabranchofgrammarinference[ 15],
whichhasbeeninvestigatedunderdifferentsettingsusingmeth-
ods like state merging [ 9,44] or identifying the longest dependent
memory [ 35,36]. Recently, researchers have proposed to learn PFA
for system analysis tasks like model checking or runtime monitor-
ing[29].Thisworkfollowsastate-merginglearningparadigmto
learn a PFA from the symbolic data extracted from RNN.
6 CONCLUSION
In this work, we propose to extract probabilistic finite automata
fromstate-of-the-artrecurrentneuralnetworktotrace/mimicits
behaviorsforanalysisbyprobabilisticabstraction.Ourapproach
isbasedonsymbolicencodingofRNNhiddenstatevectorsanda
probabilistic learning algorithm which tries to recover the prob-ability distribution of the symbolic data. The experiment resultson real-world sentiment analysis tasks show that our approach
significantlyimprovesthequalityorscalability ofstate-of-the-art
modelextractionworks.Ourapproachprovidesonepromisingwaytobridgethegapforapplyingavarietyofsoftware/systemanalysis
techniques to real-world neural networks.ACKNOWLEDGEMENTS
This research is supported by the National Key R&D Program of
ChinaunderGrantNo.2019YFB1600700andProjectofScienceand
TechnologyResearchandDevelopmentProgramofChinaRailway-
Corporation (P2018X002) and NSFC Program (Grant No. 61972339).
ThisresearchisalsosupportedbytheNationalResearchFounda-
tion,SingaporeunderitsAISingaporeProgramme(AISGAwardNo: AISG-RP-2019-012). Any opinions, findings and conclusionsor recommendations expressed in this material are those of the
author(s)anddonotreflecttheviewsofNationalResearchFounda-
tion, Singapore. This research has also been supported by the Key-
Area Research and Development Program of Guangdong Province
(Grantno.2018B010107004),theFundamentalResearchFundsfor
theZhejiangUniversityNGICSPlatform,andtheNationalResearchFoundation,PrimeMinisterâ€™sOffice,SingaporeunderitsCorporate
Laboratory@University Scheme, National University of Singapore,
and Singapore Telecommunications Ltd.
REFERENCES
[1] https://pytorch.org/.
[2] https://github.com/dgl-prc/rnn2automata.[3]
DanaAngluin. Learningregularsetsfromqueriesandcounterexamples. Infor-
mation and computation, 75(2):87â€“106, 1987.
[4]David Arthur and Sergei Vassilvitskii. k-means++: The advantages of careful
seeding. In ProceedingsoftheeighteenthannualACM-SIAMsymposiumonDiscrete
algorithms, pages 1027â€“1035. Society for Industrial and Applied Mathematics,
2007.
[5]StÃ©phaneAyache,RÃ©miEyraud,andNoÃ©Goudian. Explainingblackboxeson
sequentialdatausingweightedautomata. arXivpreprintarXiv:1810.05741,2018.
[6]Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine trans-
lationbyjointlylearningtoalignandtranslate. arXivpreprintarXiv:1409.0473,
2014.
[7]Mariusz Bojarski, Davide Del Testa, Daniel Dworakowski, Bernhard Firner, Beat
Flepp, Prasoon Goyal, Lawrence D Jackel, Mathew Monfort, Urs Muller, Ji-
akai Zhang, et al. End to end learning for self-driving cars. arXiv preprint
arXiv:1604.07316, 2016.
[8]Nicholas Carlini and David A. Wagner. Towards evaluating the robustness of
neuralnetworks. In 2017IEEESymposiumonSecurityandPrivacy,SP2017,San
Jose, CA, USA, May 22-26, 2017, pages 39â€“57, 2017.
[9]Rafael C Carrasco and JosÃ© Oncina. Learning stochastic regular grammars by
means ofa state mergingmethod. In International Colloquiumon Grammatical
Inference, pages 139â€“152. Springer, 1994.
[10]XinyunChen,ChangLiu,BoLi,KimberlyLu,andDawnSong. Targetedback-
door attacks on deep learning systems using data poisoning. arXiv preprint
arXiv:1712.05526, 2017.
[11]Kyunghyun Cho, Bart Van MerriÃ«nboer, Caglar Gulcehre, Dzmitry Bahdanau,
Fethi Bougares, Holger Schwenk, and Yoshua Bengio. Learning phrase repre-
sentations using rnn encoder-decoder for statistical machine translation. arXiv
preprint arXiv:1406.1078, 2014.
[12]Edmund M Clarke, Orna Grumberg, and David E Long. Model checking and
abstraction. ACMtransactionsonProgrammingLanguagesandSystems(TOPLAS),
16(5):1512â€“1542, 1994.
[13]SiddharthaRDalal,AshishJain,NachimuthuKarunanithi,JMLeaton,Christo-
pher M Lott,Gardner C Patton, andBruce M Horowitz. Model-basedtesting in
practice. In Proceedingsofthe1999InternationalConferenceonSoftwareEngineer-
ing (IEEE Cat. No. 99CB37002), pages 285â€“294. IEEE, 1999.
[14]Xiaoning Du, Xiaofei Xie, Yi Li, Lei Ma, Yang Liu, and Jianjun Zhao. Deepstellar:
Model-based quantitative analysis of stateful deep learning systems. In Pro-
ceedingsofthe201927thACMJointMeetingonEuropeanSoftwareEngineering
ConferenceandSymposiumontheFoundationsofSoftwareEngineering,ESEC/FSE
2019, pages 477â€“487, New York, NY, USA, 2019. ACM.
[15]Arianna Dâ€™Ulizia, Fernando Ferri, and Patrizia Grifoni. A survey of grammatical
inferencemethodsfornaturallanguagelearning. ArtificialIntelligenceReview,
36(1):1â€“27, 2011.
[16]Javid Ebrahimi, Anyi Rao, Daniel Lowd, and Dejing Dou. Hotflip: White-box
adversarial examples for text classification. arXiv preprint arXiv:1712.06751, 2017.
[17]TomFawcett.Anintroductiontorocanalysis. Patternrecognitionletters,27(8):861â€“
874, 2006.
[18]Timon Gehr, MatthewMirman, DanaDrachsler-Cohen, PetarTsankov, Swarat
Chaudhuri, and Martin T. Vechev. AI2: safety and robustness certification of
509ASE â€™20, September 21â€“25, 2020, Virtual Event, Australia Guoliang Dong, Jingyi Wang, Jun Sun, Yang Zhang, Xinyu Wang, Ting Dai, Jin Song Dong, and Xingen Wang
neuralnetworkswithabstractinterpretation. In 2018IEEESymposiumonSecurity
and Privacy, SP 2018, Proceedings, 21-23 May 2018, San Francisco, California, USA,
pages 3â€“18, 2018.
[19]Felix A. Gers, JÃ¼rgen Schmidhuber, and Fred Cummins. Learning to forget:
Continual prediction with lstm. Neural Computation, 12(10):2451â€“2471, 2000.
[20]Ian J. Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and
harnessing adversarial examples. In 3rd International Conference on Learning
Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track
Proceedings, 2015.
[21]Bo-Jian Hou and Zhi-Hua Zhou. Learning with interpretable structure from
gated rnn. IEEE Transactions on Neural Networks and Learning Systems, 2020.
[22]Xiaowei Huang,Daniel Kroening, Marta Kwiatkowska, WenjieRuan,Youcheng
Sun,EmeseThamo,MinWu,andXinpingYi. Safetyandtrustworthinessofdeep
neural networks: A survey. arXiv preprint arXiv:1812.08342, 2018.
[23]Henrik Jacobsson. Rule extraction from recurrent neural networks: Ataxonomy
and review. Neural Computation, 17(6):1223â€“1263, 2005.
[24]GuyKatz,ClarkBarrett,DavidLDill,KyleJulian,andMykelJKochenderfer. Relu-
plex: An efficient smt solver for verifying deep neural networks. In International
Conference on Computer Aided Verification, pages 97â€“117. Springer, 2017.
[25]HimabinduLakkarajuandOsbertBastani. "howdoifoolyou?"manipulating
usertrustviamisleadingblackboxexplanations. In ProceedingsoftheAAAI/ACM
Conference on AI, Ethics, and Society, pages 79â€“85, 2020.
[26]JinfengLi,ShoulingJi,TianyuDu,BoLi,andTingWang. Textbugger:Generating
adversarial text against real-world applications. In 26th Annual Network and
Distributed System Security Symposium, NDSS, 2019.
[27]Scott M Lundberg and Su-In Lee. A unified approach to interpreting model
predictions. In AdvancesinNeuralInformationProcessingSystems,pages4765â€“
4774, 2017.
[28]Lei Ma, Fuyuan Zhang, Jiyuan Sun, Minhui Xue, Bo Li, Felix Juefei-Xu, Chao
Xie, Li Li, Yang Liu, Jianjun Zhao, et al. Deepmutation: Mutation testing of
deeplearningsystems. In 29thInternationalSymposiumonSoftwareReliability
Engineering, pages 100â€“111, 2018.
[29]Hua Mao, Yingke Chen, Manfred Jaeger, Thomas D. Nielsen, Kim G. Larsen,and Brian Nielsen. Learning probabilistic automata for model checking. In
Eighth International Conference on Quantitative Evaluation of Systems, QEST 2011,
Aachen, Germany, 5-8 September, 2011, pages 111â€“120, 2011.
[30]Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efficient estimation
of word representations in vector space. arXiv preprint arXiv:1301.3781, 2013.
[31]MikeOaksfordandNickChater. Theprobabilisticapproachtohumanreasoning.
Trends in cognitive sciences, 5(8):349â€“357, 2001.
[32]Christian W Omlin and C Lee Giles. Constructing deterministic finite-state
automatainrecurrent neuralnetworks. JournaloftheACM (JACM) ,43(6):937â€“
972, 1996.
[33]BoPangandLillianLee. Seeingstars:Exploitingclassrelationshipsforsentiment
categorization with respect to rating scales. In Proceedings of the 43rd annual
meetingonassociationforcomputationallinguistics,pages115â€“124.Association
for Computational Linguistics, 2005.
[34]KexinPei,YinzhiCao,JunfengYang,andSumanJana. Deepxplore:Automated
whitebox testing of deep learning systems. In Proceedings of the 26th Symposium
onOperatingSystemsPrinciples,Shanghai,China,October28-31,2017 ,pages1â€“18,
2017.
[35]DanaRon,YoramSinger,andNaftaliTishby. Thepowerofamnesia:Learning
probabilistic automata with variable memory length. Machine learning, 25(2-
3):117â€“149, 1996.
[36]DanaRon,YoramSinger,andNaftaliTishby. Onthelearnabilityandusageof
acyclicprobabilisticfiniteautomata. JournalofComputerandSystemSciences,
56(2):133â€“152, 1998.
[37]Wenjie Ruan, Xiaowei Huang, and Marta Kwiatkowska. Reachability analysis of
deepneuralnetworkswithprovableguarantees. In ProceedingsoftheTwenty-
SeventhInternationalJointConferenceonArtificialIntelligence,IJCAI2018,July
13-19, 2018, Stockholm, Sweden., pages 2651â€“2659, 2018.
[38]Cynthia Rudin. Stop explaining black box machine learning models for high
stakesdecisionsanduseinterpretablemodelsinstead. NatureMachineIntelligence,
1(5):206â€“215, 2019.
[39]USammapun,InsupLee,andOlegSokolsky. Rt-mac:Runtimemonitoringand
checking of quantitative and probabilistic properties. In 11thIEEEInternational
Conference on Embedded and Real-Time Computing Systems and Applications
(RTCSAâ€™05) , pages 147â€“153. IEEE, 2005.
[40]Youcheng Sun, Min Wu, Wenjie Ruan, Xiaowei Huang, Marta Kwiatkowska, and
DanielKroening. Concolictestingfordeepneuralnetworks. In Proceedingsof
the 33rd ACM/IEEE International Conference on Automated Software Engineering,
ASE 2018, Montpellier, France, September 3-7, 2018, pages 109â€“119, 2018.
[41]ChristianSzegedy,WojciechZaremba,IlyaSutskever,JoanBruna,DumitruErhan,
Ian J. Goodfellow, and Rob Fergus. Intriguing properties of neural networks. In
2ndInternationalConferenceonLearningRepresentations,ICLR2014,Banff,AB,
Canada, April 14-16, 2014, Conference Track Proceedings, 2014.
[42]DuyuTang,BingQin,andTingLiu. Documentmodelingwithgatedrecurrent
neural network for sentiment classification. In Proceedings of the 2015 conferenceon empirical methods in natural language processing, pages 1422â€“1432, 2015.
[43]JingyiWang,GuoliangDong,JunSun,XinyuWang,andPeixinZhang. Adver-
sarialsample detectionfor deepneural networkthroughmodelmutationtesting.
InProceedings of the 41th International Conference on Software Engineering, 2019.
[44]JingyiWang,JunSun,QixiaYuan,andJunPang. Shouldwelearnprobabilistic
models for model checking? a new approach and an empirical study. In Inter-
nationalConferenceonFundamentalApproachestoSoftwareEngineering,pages
3â€“21. Springer, 2017.
[45]QinglongWang,Kaixuan Zhang,Alexander GOrorbia II,Xinyu Xing,Xue Liu,
andCLeeGiles. Anempiricalevaluationofruleextractionfromrecurrentneural
networks. Neural computation, 30(9):2568â€“2591, 2018.
[46]Gail Weiss, Yoav Goldberg, and Eran Yahav. Extracting automata from recurrent
neuralnetworksusingqueriesandcounterexamples. In Proceedingsofthe35th
International Conference on Machine Learning, ICML 2018, StockholmsmÃ¤ssan,
Stockholm, Sweden, July 10-15, 2018, pages 5244â€“5253, 2018.
[47]Gail Weiss, Yoav Goldberg, and Eran Yahav. Learning deterministic weighted au-
tomatawithqueriesandcounterexamples.InHannaM.Wallach,HugoLarochelle,
Alina Beygelzimer, Florence dâ€™AlchÃ©-Buc, Emily B. Fox, and Roman Garnett, edi-
tors,Advances in Neural Information Processing Systems 32: Annual Conference on
Neural Information Processing Systems 2019, NeurIPS 2019, 8-14 December 2019,
Vancouver, BC, Canada, pages 8558â€“8569, 2019.
[48] Rui Xu and Donald C Wunsch. Survey of clustering algorithms. 2005.
[49]ZhenlongYuan,YongqiangLu,ZhaoguoWang,andYiboXue. Droid-sec:deep
learning in android malware detection. In ACM SIGCOMM Computer Communi-
cation Review, volume 44, pages 371â€“372. ACM, 2014.
[50]Xiang Zhang, Junbo Zhao, and Yann LeCun. Character-level convolutionalnetworks for text classification. In Advances in neural information processing
systems, pages 649â€“657, 2015.
[51]Zhi-Hua Zhou. Rule extraction: Using neural networks or for neural networks?
Journal of Computer Science and Technology, 19(2):249â€“253, 2004.
510