Fix Fairness,Don‚Äôt Ruin Accuracy: Performance AwareFairness
RepairusingAutoML
GiangNguyen
gnguyen@iastate.edu
Iowa StateUniversity
USASumonBiswas
sumonb@cs.cmu.edu
CarnegieMellonUniversity
USAHridesh Rajan
hridesh@iastate.edu
Iowa StateUniversity
USA
ABSTRACT
Machinelearning(ML)isincreasinglybeingusedincriticaldecision-
making software, but incidents have raised questions about the
fairness of ML predictions. To address this issue, new tools and
methodsareneededtomitigatebiasinML-basedsoftware.Previous
studieshaveproposedbiasmitigationalgorithmsthatonlywork
in speciÔ¨Åc situations and often result in a loss of accuracy. Our
proposed solutionis a novelapproach that utilizesautomatedma-
chinelearning(AutoML)techniquestomitigatebias.Ourapproach
includes two key innovations: a novel optimization function and a
fairness-awaresearchspace.Byimprovingthedefaultoptimization
function ofAutoMLand incorporating fairness objectives,we are
able to mitigate bias with little to no loss of accuracy. Additionally,
we propose a fairness-aware search space pruning method for Au-
toML to reduce computational cost and repair time. Our approach,
builtonthestate-of-the-art Auto-Sklearn tool,isdesignedtoreduce
bias in real-world scenarios. In ordertodemonstratethe eÔ¨Äective-
ness of our approach, we evaluated our approach on four fairness
problemsand16diÔ¨ÄerentMLmodels,andourresultsshowasignif-
icantimprovementoverthebaselineandexistingbiasmitigation
techniques.Ourapproach, Fair-AutoML ,successfullyrepaired60
out of 64 buggy cases, while existing bias mitigation techniques
only repairedup to44 outof64 cases.
CCSCONCEPTS
‚Ä¢Softwareanditsengineering ‚ÜíSearch-basedsoftwareen-
gineering ;‚Ä¢Computing methodologies ‚ÜíMachine learning .
KEYWORDS
Software fairness, bias mitigation, fairness-accuracy trade-oÔ¨Ä, ma-
chinelearningsoftware,automatedmachine learning
ACM Reference Format:
GiangNguyen,SumonBiswas,andHrideshRajan.2023.FixFairness,Don‚Äôt
Ruin Accuracy: Performance Aware Fairness Repair using AutoML. In Pro-
ceedings of the 31st ACM Joint European Software Engineering Conference
andSymposiumontheFoundationsofSoftwareEngineering(ESEC/FSE‚Äô23),
December 3‚Äì9, 2023, San Francisco, CA, USA. ACM, New York, NY, USA,
13pages.https://doi.org/10.1145/3611643.3616257
ESEC/FSE ‚Äô23, December 3‚Äì9, 2023, San Francisco, CA,USA
¬©2023 Copyright heldby theowner/author(s).
ACM ISBN 979-8-4007-0327-0/23/12.
https://doi.org/10.1145/3611643.36162571 INTRODUCTION
Recent advancementsinmachine learninghave ledto remarkable
success insolving complex decision-makingproblemssuch asjob
recommendations, hiring employees, social services, and educa-
tion [2,10,11,21,22,24,38,39,51,53,55,56,63]. However, ML
softwarecanexhibitdiscriminationduetounfairnessbugsinthe
models[3,6].Thesebugscanresultinskeweddecisionstowards
certain groups of people based on protected attributes such as race,
age,orsex [ 30,31].
To address this issue, the software engineering (SE) commu-
nityhasinvestedindevelopingtestingandveriÔ¨Åcationstrategies
to detect unfairness in software systems [ 1,8,30,31,62]. Addi-
tionally, the machine learning literature contains a wealth of re-
search on deÔ¨Åning diÔ¨Äerent fairness criteria for ML models and
mitigating bias [ 12,20,26,37,48,50,64,65]. Various bias mitiga-
tion methods have been proposed to build fairer models. Some ap-
proachesmitigatedatabiasbyadaptingthetrainingdata[ 15,16,52];
some modify ML models during the training process to mitigate
bias[19,32,41,58,60],andothersaimtoincreasefairnessbychang-
ingthe outcome ofpredictions [ 1,62,66].
DespitetheseeÔ¨Äorts,currentbiasmitigationtechniquesoften
come at the costofdecreasedaccuracy [ 6,42]. TheireÔ¨Äectiveness
variesbasedondatasets,fairnessmetrics,orthechoiceofprotected
attributes[ 18,25,26,37].Hortetal .proposedFairea[ 42],anovel
approachtoevaluatetheeÔ¨Äectivenessofbiasmitigationtechniques,
whichfoundthatnearlyhalfoftheevaluatedcasesreceivedpoor
eÔ¨Äectiveness. Moreover, evaluations by Chen et al.also showed
that in 25% of cases, bias mitigation methods reduced both ML
performance andfairness [ 18].
Recent works [ 34,42,60] have shown that parameter tuning
cansuccessfullyÔ¨ÅxfairnessbugswithoutsacriÔ¨Åcingaccuracy.By
Ô¨Ånding the best set of parameters, parameter tuning can minimize
theerrorbetweenthepredictedvaluesandthetruevaluestoreduce
bias. This helps to ensure that the model is not overly simpliÔ¨Åed or
toocomplex,whichcanleadtounderÔ¨Åtting(highbias)oroverÔ¨Åtting
(lowaccuracy),respectively. By tuning the parameters, we can Ô¨Ånd
therightbalancebetweenbiasandaccuracy,whichleadstoamodel
that generalizes well to diÔ¨Äerent dataor fairness metric. However,
itischallengingtoidentifywhichparametersettingachievesthe
best fairness-accuracytrade-oÔ¨Ä [ 34].
Recent advancements in AutoML technology [ 28,29,43] have
made it possible forbothexpertsand non-expertsto harnessthe
powerofmachinelearning.AutoMLprovestobeaneÔ¨Äectiveop-
tionfordiscoveringoptimalparametersettings;however,currently
thereisalackoffocusonreducingbiaswithintheAutoMLtech-
niques.Thus,weposethefollowingresearchquestions: Isitpossible
toutilizeAutoMLforthepurposeofreducingbias?IsAutoMLeÔ¨Äective
Thiswork islicensedunderaCreativeCommonsAttribution4.0Interna-
tional License.
502
ESEC/FSE ‚Äô23, December3‚Äì9, 2023,San Francisco, CA, USA GiangNguyen, SumonBiswas, andHrideshRajan
in mitigating bias? Does AutoML outperform existing bias reduction
methods? Is AutoML more adaptable than existing bias mitigation
techniques?
Weintroduce Fair-AutoML ,anoveltechniquethatutilizesAu-
toML to Ô¨Åx fairness bugs in machine learning models. Unlike exist-
ing bias mitigationtechniques, Fair-AutoML addresses their limita-
tionsbyenablingeÔ¨Écientandfairness-awareBayesiansearchto
repairunfairmodels,makingiteÔ¨Äectiveforawiderangeofdatasets,
models,andfairnessmetrics.Thekeyideabehind Fair-AutoML isto
use AutoML to explore as many conÔ¨Ågurations as possible in order
toÔ¨ÅndtheoptimalÔ¨Åxforabuggymodel.Particularly, Fair-AutoML
enhancesthepotentialofAutoMLforÔ¨Åxingfairnessbugsintwo
novel techniques: by generatinga new optimizationfunction that
guides AutoML to Ô¨Åx fairness bugs without sacriÔ¨Åcing accuracy,
and by deÔ¨Åning a new search space based on the speciÔ¨Åc input
to accelerate the bug-Ô¨Åxing process. Together, these contributions
enableFair-AutoML toeÔ¨ÄectivelyÔ¨Åxfairnessbugsacrossvarious
datasetsandfairnessmetrics.Wehaveimplemented Fair-AutoML
ontopof Auto-Sklearn [29],thestate-of-the-artAutoMLframework.
Fair-AutoML aims toeÔ¨Äectivelyaddressthe limitationsofexist-
ing bias mitigation techniques by utilizing AutoML to eÔ¨Éciently
repairunfairmodelsacrossvarious datasets, models,andfairness
metrics. We conduct an extensive evaluation of Fair-AutoML using
4widelyuseddatasetsinthefairnessliterature[ 1,31,62]and16
buggymodelscollectedfromarecentstudy[ 6].Theresultsdemon-
stratetheeÔ¨Äectivenessofourapproach,as Fair-AutoML successfully
repairs 60 out of 64 buggy cases, surpassing the performance of
existing bias mitigation techniques which were only able to Ô¨Åx up
to 44 outof64 bugsinthe same settings andtraining time.
Our main contributionsare the following:
‚Ä¢WehaveproposedanovelapproachtoÔ¨Åxunfairnessbugs
andretain accuracyat the same time.
‚Ä¢We have proposed methods to generate the optimization
functionautomaticallybasedonaninputtomakeAutoML
Ô¨Åxing fairnessbugsmore eÔ¨Éciently.
‚Ä¢We have pruned the search space automatically based on an
inputto Ô¨Åxfairnessbugsfasterusing AutoML.
‚Ä¢We have implemented our approach in a SOTA AutoML,
Auto-Sklearn [29].The artifactisavailablehere [ 33].
The paper is organized as follows: ¬ß 2describes the background,
¬ß3presents a motivation, ¬ß 4indicates the problem deÔ¨Ånition, ¬ß 5
shows the Fair-AutoML approaches, ¬ß 6presents the our evaluation,
¬ß7discussesthelimitationsandfuturedirectionsof Fair-AutoML ,
¬ß8discusses the threats to validity of Fair-AutoML , ¬ß9concludes,
and¬ß10describes the artifact.
2 BACKGROUND
We begin by providing an overview of the background and related
researchinthe Ô¨Åeld ofsoftware fairness.
2.1 Preliminaries
2.1.1 ML So/f_tware. Given an input dataset /u1D437split into a training
dataset/u1D437/u1D461/u1D45F/u1D44E/u1D456/u1D45Bandavalidationdataset /u1D437/u1D463/u1D44E/u1D459,aMLsoftwaresystem
canbeabstractlyviewingasmappingproblem /u1D440/u1D706,/u1D450:/u1D465‚Üí/u1D466.altfromin-
puts/u1D465tooutputs /u1D466.altbylearningfrom /u1D437/u1D461/u1D45F/u1D44E/u1D456/u1D45B.MLdevelopersaimsto
search for a hyperparameter conÔ¨Åguration /u1D706‚àóand complementarycomponents /u1D450‚àófor model /u1D440to obtain optimal fairness-accuracy
on/u1D437/u1D463/u1D44E/u1D459. The complementary components can be ML algorithms
combinedwithaclassiÔ¨Åer i.e.,pre-processing algorithms.
2.1.2 AutoML. Given the search spaces Œõand/u1D436for hyperparame-
ters and complementary components, AutoML aims to Ô¨Ånd /u1D706‚àóand
/u1D450‚àóto obtain the lowestvalueof the costfunction (Equation 1):
/u1D440=argmin
/u1D706‚ààŒõ,/u1D450‚àà/u1D436/u1D436/u1D45C/u1D460/u1D461(/u1D440/u1D706‚àó,/u1D450‚àó,/u1D437/u1D463/u1D44E/u1D459) (1)
(/u1D706‚àó,/u1D450‚àó)=argmin
(/u1D706,/u1D450)/u1D43F/u1D45C/u1D460/u1D460(/u1D440/u1D706,/u1D450,/u1D437/u1D461/u1D45F/u1D44E/u1D456/u1D45B) (2)
2.1.3 Measures. We consider a problem, where each individual in
the population has a true label in /u1D466.alt= {0, 1}. We assume a protected
attribute /u1D467= {0, 1}, such as race, sex, age, where one label is priv-
ileged (denoted 0) and the other is unprivileged (denoted 1). The
predictions are ÀÜ/u1D466.alt‚àà {0,1}that need to be not only accurate with
respectto /u1D466.altbut alsofair withrespectto the protectedattribute /u1D467.
Accuracy Measure. Accuracy is given by the ratio of the number
ofcorrectpredictions bythe total number of predictions.
Accuracy=(#Truepositive +#Truenegative)/ #Total
Fairness Measure. We use four ways to deÔ¨Åne group fairness
metrics, whichare widely usedinfairnessliterature [ 4,5,30]:
TheDisparate Impact (DI) is the proportion of the unprivileged
group with the favorable label divided by the proportion of the
privilegedgroup withthe favorable label [ 26,64].
/u1D437/u1D43C=/u1D443/u1D45F[ÀÜ/u1D466.alt=1|/u1D467=0]
/u1D443/u1D45F[ÀÜ/u1D466.alt=1|/u1D467=1]
TheStatistical Parity DiÔ¨Äerence (SPD) quantiÔ¨Åes the disparity be-
tweenthefavorablelabel‚Äôsprobabilityfortheunprivilegedgroup
andthe favorable label‚Äôs probability for the privilegedgroup [ 12].
/u1D446/u1D443/u1D437=/u1D443/u1D45F[ÀÜ/u1D466.alt=1|/u1D467=0] ‚àí/u1D443/u1D45F[ÀÜ/u1D466.alt=1|/u1D467=1]
TheEqual Opportunity DiÔ¨Äerence (EOD) measures the disparity
betweenthetrue-positiverateoftheunprivilegedgroupandthe
privilegedgroup.
/u1D447/u1D443/u1D445/u1D462=/u1D443/u1D45F[ÀÜ/u1D466.alt=1|/u1D466.alt=1,/u1D467=0];/u1D447/u1D443/u1D445/u1D45D=/u1D443/u1D45F[ÀÜ/u1D466.alt=1|/u1D466.alt=1,/u1D467=1]
/u1D438/u1D442/u1D437=/u1D447/u1D443/u1D445/u1D462‚àí/u1D447/u1D443/u1D445/u1D45D
The Average Absolute Odds DiÔ¨Äerence (AOD) is the mean of the
diÔ¨Äerenceoftrue-positive rate and false-positive rate among the
unprivilegedgroup andprivilegedgroup [ 37].
/u1D439/u1D443/u1D445/u1D462=/u1D443[ÀÜ/u1D466.alt=1|/u1D466.alt=0,/u1D467=0];/u1D439/u1D443/u1D445/u1D45D=/u1D443[ÀÜ/u1D466.alt=1|/u1D466.alt=0,/u1D467=1]
/u1D434/u1D442/u1D437=1
2‚àó |/u1D439/u1D443/u1D445/u1D462‚àí/u1D439/u1D443/u1D445/u1D45D| + |/u1D447/u1D443/u1D445/u1D462‚àí/u1D447/u1D443/u1D445/u1D45D|
To use all the metrics in the same setting, DI has been plotted in
the absolute value of the log scale, and SPD, EOD, AOD have been
plottedinabsolutevalue[ 16,42].Thus,thebiasscoreofamodelis
measuredfrom 0, withlower scores indicating more fairness.
2.2 Related Work
2.2.1 BiasMitigation. SEandMLresearchershasdevelopedvar-
iousbiasmitigationmethodstoincreasefairnessinMLsoftware
dividedintothree categories[ 30,40]:
Pre-processing approachesreducebiasbypre-processingthe
training data. For instance, Fair-SMOTE [15] addresses data bias by
removingbiasedlabelsandbalancingthedistributionofpositive
andnegativeexamplesforeachsensitiveattribute. Reweighing [48]
503Fix Fairness,Don‚Äôt RuinAccuracy: Performance Aware Fairness RepairusingAutoML ESEC/FSE ‚Äô23, December3‚Äì9, 2023,San Francisco, CA, USA
decreases bias by assigning diÔ¨Äerent weights to diÔ¨Äerent groups
based on the degree of favoritism of a group. Disparate Impact
Remover [26] is a pre-processing bias mitigation technique that
aims to reduce biasbyediting feature values.
In-processing approachesreducebiasbymodifyingMLmodels
during the training process i.e., Parfait-ML [60] present a search-
based solution to balance fairness and accuracy by tuning hyperpa-
rametertoapproximatethetwinedParetocurves. MAAT[19]isan
ensembleapproachaimedatimprovingthefairness-performance
trade-oÔ¨Ä in ML software. Instead of combining models with the
samelearningobjectivesastraditionalensemblemethods, MAAT
merges models that are optimizedfor diÔ¨Äerentgoals.
Post-processing approaches change the outcome of prediction
toreducebias.Thistechniqueunfavorsprivilegedgroups‚Äôinstances
and favors those of unprivileged groups lying around the decision
boundary.Forexample, EqualizedOdds [37]reducesthevalueof
EOD by modifying the output labels. Fax-AI[35] eliminates di-
rect discrimination in machine learning models by limiting the
use of certain features, thereby preventing them from serving as
surrogates for protected attributes. Reject Option ClassiÔ¨Åcation [49]
prioritizes instances from the privileged group over those from the
unprivilegedgroupthataresituatedonthedecisionboundarywith
high uncertainty.
PreviouseÔ¨ÄortshavemadesigniÔ¨Åcantprogressinreducingbias;
however, they come at the cost of decreased accuracy and their
resultscanvarydependingonthedatasetsandfairnessmetrics.Our
proposal, Fair-AutoML , aimsto strikeabalance betweenaccuracy
and bias reduction and demonstrate generalizability across various
datasets andmetrics.
2.2.2 Search Space Pruning. Search space pruning involves reduc-
ing the size or complexity of the search space in optimization or
machine learning tasks. Pruning techniques are employed to accel-
eratetheoptimizationprocessofAutoMLbyeliminatingunpromis-
ingorredundantoptions,thusfocusingcomputationalresources
on more promising areasofthesearch space. For example, Feurer
et al.[29] introduce Auto-Sklearn 2.0, a novel approach aimed at
enhancing the performance of Auto-Sklearn. This advancement
involvesconstrainingthesearchspacetoexclusivelycompriseit-
erativealgorithms,whileeliminatingfeaturepreprocessing.This
strategic adjustment streamlines the implementation of successive
halving,asitreducesthecomplexitytoasingleÔ¨Ådelitytype:the
numberofiterations.Otherwise,theincorporationofdatasetsub-
setsasanalternativeÔ¨Ådelitywouldrequireadditionalconsideration.
Another innovative contribution comes from Cambronero et al.,
whointroducesAMS[ 13].Thismethodcapitalizesonthewealthof
sourcecoderepositoriestostreamlinethesearchspaceforAutoML.
Notably, AMS harnesses the power of unspeciÔ¨Åed complementary
andfunctionallyrelatedAPIcomponents.Byleveragingthesecom-
ponents,thesearchspaceforAutoMLisprunedeÔ¨Äectively.Diverg-
ing from prior research eÔ¨Äorts, Fair-AutoML distinguishes itself by
leveraging data characteristics to eÔ¨Äectively trim down the search
space.Notably,existingtechniquesinsearchspacepruningprimar-
ilytargetaccuracyenhancementwithinAutoML.Incontrast,our
innovativepruningmethodologywithin Fair-AutoML isuniquely
directedtowardsrepairing unfairmodels.
Inverted trade-off 
regionsWin-win 
region
Good 
trade-off
region
Bad
trade-off
regionLose-lose 
region
Baseline
FairnessAccuracy Original Model
Model with 
mutation degree: 
100%Figure 1:Baselinefairness-accuracy trade-oÔ¨Ä [ 42]
2.2.3 AutoML Extension. AutoML aims to automate the process
of building a high-performing ML model, but it has limitations.
It can be costly, time-consuming to train, and produces complex
models that are diÔ¨Écult to understand. To address these limita-
tions,softwareengineeringresearchershavedevelopedmethodsto
enhanceAutoMLperformance,suchas AMS[13]andManas[54].
AMSutilizes source code repositories to create a new search space
forAutoML,while Manasmineshand-developedmodelstoÔ¨Ånda
betterstartingpointforAutoML.Thegoalofthesemethodsisto
improveAutoMLtomaximizetheaccuracy.DiÔ¨Äerentfromthese
methods, Fair-AutoML , built on top of Auto-Sklearn [29], is the Ô¨Årst
to focusonrepairing unfairmodels.
3 MOTIVATION
The widespread use of machine learning in software development
has brought attention to the issue of fairness in ML models. Al-
though various bias mitigation techniques have been developed
toaddressthisissue,theyhavelimitations.Thesetechniquessuf-
fer from a poor balance between fairness and accuracy [ 42], and
are not applicable to a wide range of datasets, metrics, and mod-
els[25,26,37].Togainadeeperunderstandingoftheselimitations,
we evaluate six diÔ¨Äerent bias mitigation techniques using four fair-
ness metrics, four datasets, and six model types. The evaluation
criteria are borrowed from Fairea [ 42] and are presented in Table 1.
Faireaisdesignedtoassessthetrade-oÔ¨Äbetweenfairnessand
accuracy of bias mitigation techniques. The methodology of Fairea
is demonstrated in Figure 1, where the fairness and accuracy of
a bias mitigation technique on a dataset are displayed in a two-
dimensional coordinate system. The baseline is established by con-
necting the fairness-accuracy points of the original model and the
mitigationmodelsonthedataset. Faireaevaluatestheperformance
ofthemitigationtechniquebyalteringtheoriginalmodelpredic-
tionsandreplacingarandomsubsetofthepredictionswithother
labels.Themutationdegreerangesfrom10%to100%withastep-
sizeof10%.ThebaselineclassiÔ¨Åesthefairness-accuracytrade-oÔ¨Ä
ofa biasmitigationtechniqueinto Ô¨Åveregions:lose-lose trade-oÔ¨Ä
(lose), bad trade-oÔ¨Ä ( bad), inverted trade-oÔ¨Ä ( inv), good trade-oÔ¨Ä
(good), and win-win trade-oÔ¨Ä ( win). A technique reducing both
accuracy and fairness would fall into the lose-lose trade-oÔ¨Ä region.
If the trade-oÔ¨Ä is worse than the baseline, it would fall into the bad
trade-oÔ¨Äregion.Ifthetrade-oÔ¨Äisbetterthanthebaseline,itwould
fallintothegoodtrade-oÔ¨Äregion.Ifabiasmitigationmethodsimul-
taneouslydecreasesbothbiasandaccuracy,itwouldfallintothe
504ESEC/FSE ‚Äô23, December3‚Äì9, 2023,San Francisco, CA, USA GiangNguyen, SumonBiswas, andHrideshRajan
Table 1:Mean proportions ofmitigationcasesthat that fall
into eachmitigationregion
Criteria LoseBadInvGoodWin
MetricDI 7%31%5%43%14%
SPD 4%36%6%40%14%
EOD 23%15%14%40%8%
AOD 9%30%5%40%16%
DatasetAdult[44]18%6%14%55%7%
Bank [45]9%44%7%23%17%
German [ 46]6%36%2%46%10%
Titanic [47]11%26%3%43%17%
Mean 11%28%7%41%13%
Bad:badtrade-oÔ¨Äregion, Lose: lose-losetrade-oÔ¨Äregion, Inv:inverted trade-oÔ¨Ä
region, Good:good trade-oÔ¨Äregion, Win:win-win trade-oÔ¨Äregion.
inverted trade-oÔ¨Ä region. If the technique improves both accuracy
andfairness,itwouldfall intothe win-wintrade-oÔ¨Äregion.
TheresultsoftheregionclassiÔ¨Åcationofsixbiasmitigationtech-
niques-Reweighing [48],DisparateImpactRemover [26],Parfait-
ML[60],Equalized Odds [37],FaX-AI[35],Reject Option ClassiÔ¨Å-
cation[49]-areshowninTable 1.Theevaluationwasconducted
on64buggycasesusingdiÔ¨Äerentcriteriasuchasfairnessmetrics
anddatasets.ThecaseisidentiÔ¨Åedasbuggywhenitfallsbelowthe
Faireabaseline.Themeanpercentageofeachtechniquefallinginto
thecorrespondingregionsislistedineachcell.Themeanresults
provideageneraloverviewofthecurrentstateofbiasmitigation
techniques.Furtherdetails ontheperformanceofeachindividual
biasmitigationtechniquecanbefoundinTable 3ofourevaluation.
Table1illustratesthatthemajorityofexistingbiasmitigation
techniques have a poor fairness-accuracy trade-oÔ¨Ä across diÔ¨Äerent
datasets, fairness metrics, and classiÔ¨Åcation models. SpeciÔ¨Åcally,
39%ofthe casesshowthatthesetechniquesperformworsethan
the original model, with 28% of the cases resulting in a poor trade-
oÔ¨Ä and 11% resulting in a decrease in accuracy and an increase
inbias. Additionally, Table 1shows thattheperformanceofthese
techniques varies depending on the input, as demonstrated by the
diÔ¨Äerent resultsobtainedwhen usingdiÔ¨Äerent datasetsorfainess
metrics [25,26,37]. For example, the bias mitigation techniques
had a high performance in62% of the cases using the Adult dataset
(55%forgoodtrade-oÔ¨Äregionand7%forwin-wintrade-oÔ¨Äregion),
but only achieved40%goodeÔ¨Äectiveness inthe Bank dataset.
Hortetal.[42]havedemonstratedthatthroughproperparameter
tuning, it is possible to address fairness issues in machine learning
models without sacriÔ¨Åcing accuracy. However, determining the
optimalfairness-accuracytrade-oÔ¨Äcanbeachallenge.Although
AutoMLcanbeeÔ¨ÄectiveinÔ¨Åndingthebestparametersettings,it
does not speciÔ¨Åcally address bias reduction. This motivates the de-
velopment of Fair-AutoML , a novel approach that utilizes Bayesian
optimizationto tuneparametersandaddressfairnessissueswith-
out hindering accuracy. Fair-AutoML is evaluated for its generality
across diÔ¨Äerent fairness metrics and datasets, and unlike other bias
mitigationmethods,itcan be appliedto any dataset ormetric.
Thisworkfocusesonimprovingfairnessquantitativelyofbuggy
models instead of targetinga speciÔ¨Åctype ofdatasets and models.
Our method is general since we utilize the power of AutoML to try
as many conÔ¨Ågurations as possible to obtain the optimal Ô¨Åx; there-
fore,ourmethodcanworkonvarioustypesofdatasetsandmetrics.
The rest of this work describes our approach, Fair-AutoML , thataddresses the limitations of both existing bias mitigation methods
and AutoML. As a demonstration, Fair-AutoML achieved good per-
formance in 100% of the 16 buggy cases in the Adult dataset, while
75% of the mitigation cases showed a good fairness-accuracy trade-
oÔ¨Ä,and theremaining 25%exhibited animprovement inaccuracy
withoutsacriÔ¨Åcingbiasreduction.
4 PROBLEM DEFINITION
ThisworkaimstoutilizeAutoMLtoaddressissuesofunfairnessin
MLsoftwarebyÔ¨ÅndinganewsetofconÔ¨Ågurationsforthemodel
that achieves optimal fairness-accuracy trade-oÔ¨Ä. Because fairness
is an additional consideration beyond accuracy, the problem be-
comesamulti-objectiveoptimizationproblem,requiringanewcost
functionthatcanoptimizebothfairnessandaccuracysimultane-
ously. To achieve this, we use a technique called weighted-sum
scalarization(Equation 3)[23],whichallowsustoweightheimpor-
tanceofdiÔ¨Äerentobjectivesandcreateasinglescalarcostfunction.
/u1D434=/u1D45B/summationdisplay.1
/u1D456=1/u1D450/u1D456‚àó/u1D6FD/u1D456 (3)
where,/u1D6FD/u1D456denotesthe relative weightof importanceof /u1D450/u1D456:
/u1D45B/summationdisplay.1
/u1D456=1/u1D6FD/u1D456=1 (4)
In this work, we use a cost function (or objective function) that
isaweighted-sumscalarizationoftwodecisioncriteria:biasand
accuracy.Thiscostfunction,asshowninEquation 5,assignweights
tobiasandaccuracyinthecostfunctionallowustoadjustthetrade-
oÔ¨Ä between the twocriteriaaccording to the speciÔ¨Åc problems:
/u1D436/u1D45C/u1D460/u1D461(/u1D440/u1D706,/u1D450,D(/u1D467))=/u1D6FD‚àó/u1D453+ (1‚àí/u1D6FD) ‚àó (1‚àí/u1D44E) (5)
We analyze the output of the buggy ML software (including bias
and accuracy) to create a suitable cost function for each input.
By analyzing the output, we are able to automatically estimate
the weights of the cost function in order to balance fairness and
accuracy for a speciÔ¨Åc problem. To the best of our knowledge, this
is the Ô¨Årst work that applies output analysis of the software to
AutoMLto repairunfairML models.
However, using AutoMLcan be costly and time-consuming. To
addressthisissue,weproposeanovelmethodthatautomatically
create new search spaces Œõ‚àóand/u1D436‚àóbased on diÔ¨Äerent inputs to
accelerate the bug-Ô¨Åxing process of AutoML. These new search
spacesaresmallerinsizecomparedtotheoriginalones, |Œõ‚àó|<|Œõ|
and|/u1D436‚àó |<|/u1D436|. Particularly, as shown in Equation 6,Fair-AutoML
takes as input a ML model and a dataset with a protected attribute
/u1D467,andaimstoÔ¨Ånd /u1D706‚àóand/u1D450‚àóinthesmallersearchspace,inorder
to minimizethe costvalue.
/u1D440=argmin
/u1D706‚àó‚ààŒõ‚àó,/u1D450‚àó‚àà/u1D436‚àó/u1D436/u1D45C/u1D460/u1D461(/u1D440/u1D706‚àó,/u1D450‚àó,/u1D437/u1D463/u1D44E/u1D459(/u1D467)) (6)
Thetechniqueofsearchspacepruningin Fair-AutoML utilizesdata
characteristicstoenhancebug-Ô¨ÅxingeÔ¨Éciency.Byshrinkingthe
search spaces based on input analysis, Fair-AutoML can Ô¨Ånd better
solutions more quickly. A set of predeÔ¨Åned modiÔ¨Åcations to the
ML model are pre-built and used as a new search space for new
input datasets, reducing the time needed to Ô¨Åx buggy models. Our
approach is based on previous works in AutoML [ 29], but updated
and modiÔ¨Åed to tackle bias issues. To the best of our knowledge,
505Fix Fairness,Don‚Äôt RuinAccuracy: Performance Aware Fairness RepairusingAutoML ESEC/FSE ‚Äô23, December3‚Äì9, 2023,San Francisco, CA, USA
Offline
Online
Input DatasetMatching InputGood fixesùõΩ‚àóùëì + (1‚àí ùõΩ)‚àó
(1‚àí ùëé)
Unfair ML models
Fixed ML 
models
Efficient Fair 
AutoML
Original 
Search SpaceFair AutoML
Fairness datasets ML models with 
default parameters
Modified optimization 
function
AutoML
Automatically 
estimate ùõΩ
New Search 
SpaceInput in offline 
phaseFair AutoML
Input in online 
phase1
The most 
similar inputThe most 
suitable fix
2
3
45
6
7
89
1110
1312
14
Figure 2:AnOverview of Fair-AutoML Approach
we are the Ô¨Årst to propose a search space pruning technique for
fairness-aware AutoML.
5 FAIR-AUTOML
This section describes a detailed description of key componentsof
Fair-AutoML (Figure2):thedynamicoptimizationfunction(steps
1-3)andthe searchspacepruning (steps 4-13).
5.1Dynamic Optimization for Bias Elimination
We strive to eliminate bias in unfair models by utilizing Equation 5
as the objective functionand determiningthe optimalvalue of /u1D6FDto
minimizethecostfunction.Inthissection,weproposeanapproach
toautomaticallyestimatetheoptimalvalueof /u1D6FDforaspeciÔ¨Åcdataset
andatargetedmodel.ThismethodensureseÔ¨Écientcorrectionof
fairnessissueswhilemaintaininghigh predictive accuracy.
5.1.1 UpperboundoftheCostFunction. Toestimatetheoptimal
valueof/u1D6FD,theÔ¨Årststepistodeterminetheupperboundofthecost
function. This can be done by using a "pseudo-model", which is
the 100% mutation degree model [ 42], as shown in the Figure 1. In
other words, the pseudo-model always achieves the accuracy on
any binary classiÔ¨Åcation problem as follows:
/u1D44E0=/u1D45A/u1D44E/u1D465(/u1D443(/u1D44C=1),/u1D443(/u1D44C=0)) (7)
Given an input, the pseudo-model achieves an accuracy of /u1D44E0
and a bias value of /u1D4530on that input. We deÔ¨Åne the cost function,
/u1D436/u1D45C/u1D460/u1D461,ofthebuggy ML modelwithaccuracy /u1D44Eand biasvalue /u1D453on
theinput.AsAutoMLtriesdiÔ¨ÄerenthyperparameterconÔ¨Ågurations
toÔ¨Åx themodel,thevalues of /u1D44Eand/u1D453may changeovertime.The
upper bound ofthe costfunctionisdeÔ¨Ånedas Equations 8and9:
/u1D436/u1D45C/u1D460/u1D461(/u1D440/u1D706,/u1D450,D(/u1D467))</u1D6FD‚àó/u1D4530+ (1‚àí/u1D6FD) ‚àó (1‚àí/u1D44E0)(8)
‚áî/u1D6FD‚àó/u1D453+ (1‚àí/u1D6FD) ‚àó (1‚àí/u1D44E)</u1D6FD‚àó/u1D4530+ (1‚àí/u1D6FD) ‚àó (1‚àí/u1D44E0)(9)
The upper bound of the cost function is deÔ¨Åned with the goal of
repairing a buggy model so that its performance falls within a
good/win-win trade-oÔ¨Ä region of fairness and accuracy. In other
words, the accuracy of the repaired model must be higher than the
accuracyofthe pseudo-model.The repairedmodelmustbe better
than the pseudo-model in terms of the cost function‚Äôs value. Since
the pseudo-model has zero bias ( /u1D4530=0), the upper bound of thecostfunction isdeÔ¨Ånedas follows(Equation 10):
/u1D6FD‚àó/u1D453+ (1‚àí/u1D6FD) ‚àó (1‚àí/u1D44E)<(1‚àí/u1D6FD) ‚àó (1‚àí/u1D44E0)(10)
5.1.2 Lower Bound of /u1D6FD.In this work, we desire to optimize the
valueof/u1D6FDinordertominimizebiasasmuchaspossible.Thecost
function used by Fair-AutoML is designed to balance accuracy and
fairness, and increasing /u1D6FDwill place more emphasis on reducing
bias. However, simply setting /u1D6FDto its highest possible value is
notaviableoption,asitmayleadtolowpredictiveaccuracyand
overÔ¨Åtting.Wecannotacceptmodelswithpoorpredictiveaccuracy
regardless of their low bias [ 36,57]. To overcome this challenge,
weaimtoÔ¨Åndthelowerboundof /u1D6FD,whichcanbedonebasedon
the upper bound ofthe costfunction. From Equation 10,we get:
/u1D6FD</u1D44E‚àí/u1D44E0
/u1D44E‚àí/u1D44E0+/u1D453(11)
However, if the value of /u1D6FDis smaller than/u1D44E‚àí/u1D44E0
/u1D44E‚àí/u1D44E0+/u1D453, the optimiza-
tionfunction /u1D436/u1D45C/u1D460/u1D461willalwaysmeetitsupperboundcondition.If
thevalueof /u1D6FDalwayssatisÔ¨Åestheupperboundconditionofthecost
functionregardlessofaccuracyandfairness,wecanobtainabetter
optimization function by either increasing accuracy or decreasing
bias. In this case, we cannot guide AutoML to produce a lower bias.
Therefore, to guide AutoML produces an output with improved
fairness,we setalower bound for /u1D6FDas Equation 12:
/u1D6FD‚â•/u1D44E‚àí/u1D44E0
/u1D44E‚àí/u1D44E0+/u1D453(12)
Theintuitionbeingthatourmethodaimstoincreasethechance
for AutoML to achieve better fairness. However, by setting /u1D6FD<
/u1D44E‚àí/u1D44E0
/u1D44E‚àí/u1D44E0+/u1D453and/u1D44E>/u1D44E0(we aim to Ô¨Ånd a model which has better
accuracythanthepseudo-model),anyvalueofbias(f)cansatisfy
upper bound condition of the cost function, which lower chance
to obtain fairer models of AutoML. To increase this chance, we set
/u1D6FD‚â•/u1D44E‚àí/u1D44E0
/u1D44E‚àí/u1D44E0+/u1D453and/u1D44E>/u1D44E0. In this case, AutoML need to Ô¨Ånd better
modelsthathaslowerbiastosatisfyEquation 10.Inotherwords,
this lower bound condition indirectly forces bayesian optimization
to searchfor lower biasmodels.
5.1.3/u1D6FDEstimation. The Ô¨Ånal step is estimating the value of /u1D6FD
based on its lower bound condition. Suppose that the buggy model
achievesanaccuracyof /u1D44E1andabiasvalueof /u1D4531onthatinput.From
the begining, we have: /u1D44E=/u1D44E1and/u1D453=/u1D4531. In that time, the lower
bound of /u1D6FDis/u1D43F=/u1D44E1‚àí/u1D44E0
/u1D44E1‚àí/u1D44E0+/u1D4531,sowe have:
/u1D6FD=/u1D43F+/u1D458,/u1D458‚àà [0,1‚àí/u1D43F] (13)
Wepresentagreedyalgorithmforestimatingthevalueof /u1D6FD,which
is detailed in Algorithm 1. Given a dataset /u1D437with a protected
attribute /u1D467and a buggy model /u1D440(Line 1), we start by measuring
thelowerboundof /u1D6FD.Next,werun Fair-AutoML ontheinputunder
time constraint twith a value of /u1D6FDset to/u1D44E1‚àí/u1D44E0
/u1D44E1‚àí/u1D44E0+/u1D4531(Line 2-8). As
the algorithm searches, whenever Fair-AutoML Ô¨Ånds a candidate
model that meets the condition /u1D436/u1D45C/u1D460/u1D461</u1D436/u1D45C/u1D460/u1D4610(Lines 10-12), the
value of/u1D6FDis slightly increased by /u1D6FC(Line 10-12). If after N tries,
Fair-AutoML cannotÔ¨ÅndamodelthatsatisÔ¨Åesthecondition,the
Ô¨Ånal value of /u1D6FDis set to/u1D6FD=/u1D6FD-/u1D6FCfor the remaining search time
to prevent overÔ¨Åtting from an excessively high value of /u1D6FD(Lines
13-15). The algorithm returns the bestmodelfound(Line 16).
506ESEC/FSE ‚Äô23, December3‚Äì9, 2023,San Francisco, CA, USA GiangNguyen, SumonBiswas, andHrideshRajan
Algorithm1 Greedy WeightIdentiÔ¨Åer
1:Input:a dataset /u1D437with protected attribute /u1D467, buggy model /u1D440
hyperparameteredby /u1D706,theincrementvalue /u1D6FC,thesearching
time/u1D461andthe threshold N
2:/u1D6FD=/u1D44E1‚àí/u1D44E0
/u1D44E1‚àí/u1D44E0+/u1D4531
3:/u1D436/u1D45C/u1D460/u1D461(/u1D440/u1D706,/u1D450,D(/u1D467))=/u1D6FD‚àó/u1D453+ (1‚àí/u1D6FD) ‚àó (1‚àí/u1D44E)
4:/u1D436/u1D45C/u1D460/u1D4610(/u1D440/u1D706,/u1D450,D(/u1D467))=(1‚àí/u1D6FD) ‚àó (1‚àí/u1D44E0)
5:count =0
6:checker =False
7:while/u1D461do
8:/u1D440/u1D706‚àó,/u1D450‚àó=argmin
/u1D706‚ààŒõ/u1D436/u1D45C/u1D460/u1D461(/u1D440/u1D706,/u1D450,D(/u1D467))
9:count =count +1
10:if/u1D436/u1D45C/u1D460/u1D461(/u1D440/u1D706,/u1D450,D(/u1D467))</u1D436/u1D45C/u1D460/u1D4610(/u1D440/u1D706,/u1D450,D(/u1D467))then
11: ifchecker =False then
12: /u1D6FD=/u1D6FD+/u1D6FC
13: count =0
14:if/u1D450/u1D45C/u1D462/u1D45B/u1D461‚â•/u1D441andchecker =False then
15: /u1D6FD=/u1D6FD-/u1D6FC
16: checker =True
17:return/u1D440/u1D706‚àó
5.2 Search SpacePruning forEÔ¨Écient Bias
Elimination
WeproposeasolutiontospeeduptheBayesianoptimizationpro-
cessinFair-AutoML byimplementingsearchspacepruning.This
technique takes advantageofdata characteristics toautomatically
reducethesizeofthesearchspaceinAutoML,thusimprovingits
eÔ¨Éciency.Ourapproachincludestwophases:theoÔ¨Ñinephaseand
theonlinephase.TheoÔ¨Ñinephasetrainsasetofinputsmultiple
timestogatheracollectionofhyperparametersandcomplementary
componentsforeachinput,formingapre-builtsearchspace.Inthe
onlinephase,whenanewinputisencountered,itismatchedagainst
theinputsstoredinourdatabasetoÔ¨Åndamatchingpre-builtsearch
space, which is then utilized to repair the buggy model. This ap-
proacheÔ¨Äectivelyreplacestheoriginalsearchspaceof Fair-AutoML ,
makingtheBayesianoptimizationprocessmuchfaster.Searchspace
pruning hasalready been successfully appliedbefore [ 13,28]; how-
ever, this is the Ô¨Årst application of data characteristics to prune the
searchspacefor fairness-aware AutoML.
5.2.1 OÔ¨ÄlinePhase. Thisphaseconstructsasetofsearchspacesfor
Fair-AutoML based on diÔ¨Äerent inputs. It is important to note that
the input format in the oÔ¨Ñine phase must match that of the online
phase, which includes a dataset with a protected attribute and a
ML model. This ensures that the pre-built search spaces created in
the oÔ¨Ñine phasecan be eÔ¨Äectivelyutilizedinthe onlinephase.
Input.In the oÔ¨Ñine phase, we collect a set of inputs to build
search spaces for Fair-AutoML . The inputs are obtained as follows.
Firstly,weminemachinelearningdatasetsfrom OpenML,consider-
ing onlythe 3425 active datasets thathave been veriÔ¨Åedto work
properly. Secondly, to ensure that themined datasets are relevant
to the fairness problem, we only collect datasets that contain at
least one of the following attributes: age,sex,race[17]. In total, we
collected 231 fairness datasets. Thirdly, for each mined dataset, we
use all available protected attributes. For example, when dealing
with datasets that contain multiple protected attributes, such asAlgorithm2 DatabaseBuilding
1:Input:a dataset /u1D437with protected attribute /u1D467, a model /u1D440with
defaulthyperparameters /u1D706.Running time t.
2:d=‚àÖ
3:dev =1
4:database={}
5:space={}
6:count =0
7:whilecount‚â§ndo
8:count =count +1
9:whiletdo
10: /u1D440/u1D706‚àó=argmin/u1D436/u1D45C/u1D460/u1D461(/u1D440/u1D706,D(/u1D467))
11: /u1D451=/u1D451‚à™/u1D440/u1D706‚àó
12:kBestPipelines =top_k(d)
13:mBestComponents=top_m(kBestPipelines)
14:formodel‚ààkBestPipelines do
15: forpara‚ààmodeldo
16: space[para] =space[para] ‚à™[para.val]
17:forpara‚ààspacedo
18:ifparaisnumerical then
19: no_outliers = ‚àÖ
20: fori‚ààspace[para] do
21: if|/u1D456‚àí/u1D460/u1D45D/u1D44E/u1D450/u1D452[/u1D45D/u1D44E/u1D45F/u1D44E]|</u1D451/u1D452/u1D463‚àó/u1D70E(/u1D460/u1D45D/u1D44E/u1D450/u1D452[/u1D45D/u1D44E/u1D45F/u1D44E])then
22: no_outliers =no_outliers ‚à™space[para][i]
23:space[para] =[min(no_outliers), max(no_outliers)]
24:database[input]=(space,mBestComponents)
25:returndatabase
theAdultdataset that includes sexandraceas protected attributes,
we treat them as distinct inputs for the dataset. Finally, we use the
defaultvaluesforthehyperparametersoftheinputMLmodelin
theoÔ¨Ñinephase,aswedonotknowthespeciÔ¨Åcvaluesthatwillbe
usedinthe onlinephase.
Database building. To build a pre-deÔ¨Åned search space database,
we use thealgorithm outlined in Algorithm 2to obtain a pre-built
searchspaceforeachcollectedinputinordertoÔ¨Åxthebuggymodel.
ThisprocessinvolvestrainingafairnessdatasetwithaspeciÔ¨Åcpro-
tectedattributeandMLmodelmultipletimesusing Fair-AutoML ,
collectingthetop /u1D458bestpipelinesfound,andextractingparameters
from these pipelines. In particular, we use Fair-AutoML to train the
fairness dataset with a speciÔ¨Åc protected attribute and a ML model
for/u1D45Biterations (Line 7-11). We then gatherthe top /u1D458best pipelines,
including a classiÔ¨Åer and complementary components, found by
Fair-AutoML accordingtotheoptimizationfunction‚Äôsvalue(Line
12).Thisresults in /u1D458‚àó/u1D45Btotalpipelines.Fromthesepipelines,we
extract and store the m most frequently used complementary com-
ponents in the database (Line 13). For each classiÔ¨Åer parameter, we
alsostoreitsvalue(Lines14-16).Thisresultsink ‚àónvaluesbeing
stored for each hyperparameter. If a hyperparameter is categorical
anditsvaluesaresampledfromasetofdiÔ¨Äerentvalues,westoreall
its unique values in the database. If a hyperparameter is numerical
and its values are sampled from a uniform distribution, we remove
anyoutliersandstoretherangeofvaluesfromtheminimumtothe
maximuminthedatabase(Lines17-23).Afterthisprocess,wehave
collected the pre-built search space for the input (Lines 24-25). We
believethattwosimilarinputsmayhavesimilarbuggymodelsand
507Fix Fairness,Don‚Äôt RuinAccuracy: Performance Aware Fairness RepairusingAutoML ESEC/FSE ‚Äô23, December3‚Äì9, 2023,San Francisco, CA, USA
Algorithm3 InputMatching
1:Input:a input dataset /u1D437with the protected attribute /u1D467, the
numberofdatapoints /u1D45D,thenumberoffeatures /u1D453,lowerbound
/u1D43F,abuggy model /u1D440,andadatabase.
2:dist={}
3:for/u1D451/u1D456indatabase do
4:dist[/u1D451/u1D456]=|/u1D453/u1D456‚àí/u1D453| + |/u1D45D/u1D456‚àí/u1D45D|
5:similarDataset=min(dist,key=dist.get)
6:dist={}
7:for/u1D467/u1D456insimilarDataset do
8:dist[/u1D451/u1D456]=|/u1D43F/u1D456‚àí/u1D43F|
9:similarAttribute =min(dist,key=dist.get)
10:similarModel=M withdefaultparameter
11:returnsimilarDataset, similarAttribute,similarModel
Ô¨Åxes,sothepre-builtsearchspaceisbuiltbasedonthebestmod-
elsfoundby Fair-AutoML fromsimilar inputs,makingitareliable
solution for Ô¨Åxing buggy models.
5.2.2 Online Phase. This phase utilizes a pre-built search space
from the database to Ô¨Åx a buggy model for a given dataset by
replacing the originalsearchspacewiththe pre-builtone.
Searchspacepruning. Ourapproachofsearchspacepruningin
Fair-AutoML improves the bug Ô¨Åxing performance by reducing the
size of the hyperparameter tuning space. Algorithm 3is used to
matchtheinputdataset,protectedattribute,andMLmodel tothe
mostsimilarinputinthedatabase.Firstly,datacharacteristicssuch
asthenumberofdatapointsandfeaturesareusedtomatchthenew
dataset with the most similar one in the database [ 28]. L1 distance
is computed between the new dataset and each mined dataset in
the space of data characteristics to determine the closest match.
We consider that the most similar dataset to the new dataset is
the nearest one (Line 2-5). Secondly, we compute the lower bound
/u1D43F=/u1D44E1‚àí/u1D44E0
/u1D44E1‚àí/u1D44E0+/u1D4531of/u1D6FDof the new input. We then estimate the lower
bound of /u1D6FDof all the protected attributes of the matched dataset
andselecttheattributewhoselowerboundisclosestto /u1D43F(Line6-9).
Lastly,twosimilarinputsmustusethesameMLalgorithm(Line10).
Thematchingprocessiscarriedoutintheorderofdatasetmatching,
protectedattributematching,andMLalgorithmmatching.Thepre-
builtsearchspaceofthesimilarinputisthenusedasthenewsearch
spacefor the newinput.
6 EVALUATION
In this section, we describe the design of the experiments to evalu-
atetheeÔ¨Écientof Fair-AutoML .WeÔ¨Årstposeresearchquestions
and discuss the experimental details. Then, we answer research
questions regarding the eÔ¨Éciency and adaptability of Fair-AutoML .
RQ1:IsFair-AutoML eÔ¨ÄectiveinÔ¨Åxingfairnessbugs? To
answer this question, we quantify the number of fairness bugs
thatFair-AutoML is able to repair compared to existing methods,
allowing us to assess the capability of an AutoML system in Ô¨Åxing
fairnessissues.
RQ2: IsFair-AutoML more adaptable than existing bias
mitigationtechniques? Theadaptabilityofabiasmitigationtech-
niqueindicatesitsperformanceacrossadiverserangeofdataset-
s/metrics. So, we analyze the eÔ¨Äectiveness of Fair-AutoML andexistingbiasmitigationtechniquesondiÔ¨Äerentdataset/metricsto
assessthe adaptability ofan AutoMLsystemonÔ¨Åxfairnessbugs.
RQ3:Aredynamicoptimizationfunctionandsearchspace
pruning eÔ¨Äective in Ô¨Åxing fairness bugs? To answer this ques-
tion, we assess the performance of Auto-Sklearn , both with and
without the dynamic optimization function and search space prun-
ing, to demonstratethe impact of eachproposedapproach.
6.1 Experiment
6.1.1 Benchmarks. Weevaluatedourmethodusingreal-worldfair-
nessbugssourcedfromarecentempiricalstudy[ 6],withourbench-
markconsistingof16modelscollectedfromKagglecoveringÔ¨Åve
distincttypes:XGBoost( XGB),RandomForest( RF),LogisticRe-
gression( LRG),GradientBoosting( GBC),SupportVectorMachine
(SVC). We use four popular datasets for our evaluation [ 10,61,62]:
TheAdult Census (race) [44] comprised of 32,561 observations
and12featuresthatcapturetheÔ¨Ånancialinformationofindividuals
fromthe1994U.S.census.Theobjectiveistopredictwhetheran
individualearns an annual incomegreater than50K.
TheBank Marketing (age) [45] has 41,188 data points with 20
featuresincludinginformationondirectmarketingcampaignsof
a Portuguese banking institution. The classiÔ¨Åcation task aims to
identifywhether the clientwillsubscribe to aterm deposit.
TheGerman Credit (sex) [46] has 1000 observations with 21
featurescontainingcreditinformationtopredictgoodorbadcredit.
TheTitanic(sex) [47] has 891 data points with 10 features con-
tainingindividualinformationofTitanicpassengers.Thedatasetis
usedto predict whosurvivedthe Titanic shipwreck.
6.1.2 Evaluated Learning Techniques. We examined the perfor-
manceof Fair-AutoML andothersupervised learning methodsad-
dressingdiscriminationinbinaryclassiÔ¨Åcationincludingallthree
types ofbiasmitigationtechniques andAuto-ML techniques.
Biasmitigationmethods. Weinvestigateallthreetypesofbias
mitigationmethods:pre-processing,in-processing,post-processing.
Weselectwidely-studiedbiasmitigationmethodsforeachcategory:
‚Ä¢Thepre-processing includesReweighing (R)[48],Disparate
Impact Remover (DIR) [26].
‚Ä¢Thein-processing includesParfait-ML (PML) [60].
‚Ä¢Thepost-processing includesEqualized Odds (EO) [37],
FaX-AI(FAX) [35],RejectOption ClassiÔ¨Åcation (ROC) [49].
Auto-Sklearn. WeexploretheeÔ¨Éciencyof Auto-Sklearn (AS)[29]
on mitigating bias in unfair model. Although, Auto-Sklearn does
notseektodecreasebias,wecompareitsperformancewith Fair-
AutoMLtodemonstratetheeÔ¨Écientofourtechniquesinguiding
Auto-ML to repairfairnessbugs.
Fair-AutoML. We create 4 versions of Fair-AutoML in this evalu-
ationrepresenting for Fair-AutoML withdiÔ¨Äerentcostfunctions:
‚Ä¢T1uses/u1D6FD‚àó/u1D437/u1D43C+ (1‚àí/u1D6FD) ‚àó (1‚àí/u1D44E/u1D450/u1D450/u1D462/u1D45F/u1D44E/u1D450/u1D466.alt)as a cost function.
‚Ä¢T2uses/u1D6FD‚àó/u1D446/u1D443/u1D437+(1‚àí/u1D6FD)‚àó(1‚àí/u1D44E/u1D450/u1D450/u1D462/u1D45F/u1D44E/u1D450/u1D466.alt)asacostfunction.
‚Ä¢T3uses/u1D6FD‚àó/u1D438/u1D442/u1D437+(1‚àí/u1D6FD)‚àó(1‚àí/u1D44E/u1D450/u1D450/u1D462/u1D45F/u1D44E/u1D450/u1D466.alt)asacostfunction.
‚Ä¢T4uses/u1D6FD‚àó/u1D434/u1D442/u1D437+(1‚àí/u1D6FD)‚àó(1‚àí/u1D44E/u1D450/u1D450/u1D462/u1D45F/u1D44E/u1D450/u1D466.alt)asacostfunction.
6.1.3 ExperimentalConfiguration. Experimentswereconducted
using Python 3.6 on Intel Skylake 6140 processors. Fair-AutoML
leverages the capabilities of Auto-Sklearn [29], taking advantage of
508ESEC/FSE ‚Äô23, December3‚Äì9, 2023,San Francisco, CA, USA GiangNguyen, SumonBiswas, andHrideshRajan
Table 2:Trade-oÔ¨Äassessmentresults of Fair-AutoML ,Auto-Sklearn ,andmitigationtechniques
ModelMetricT1T2T3T4ASRDIRPMLEOFAXROCModelMetricT1T2T3T4ASRDIRPMLEOFAXROC
Acc0.0100.0010.005-0.0030.0160.0090.0040.008-0.0060.002-0.047 Acc-0.013-0.022-0.009-0.032-0.001-0.056-0.055-0.054-0.057-0.057-0.061
DI0.0960.0110.1180.0950.0580.337invinv0.2920.0000.445 DI0.1900.4100.2120.1790.0400.5690.326lose0.375bad0.318
SPD0.0230.0240.0380.0480.0160.055invinv0.047inv0.055 SPD0.0540.0750.0290.0590.0050.0970.079bad0.0830.0760.072
EOD0.019inv0.0140.0200.008invinvinv0.041invlose EOD0.0480.0200.0440.049lose0.0570.0560.0600.0670.0520.059RF
AOD0.028inv0.0300.0350.0210.0050.001inv0.0510.007badLRG
AOD0.0850.0750.0790.0890.0390.0960.0940.0910.1030.0940.095
Acc-0.019-0.052-0.017-0.015-0.001-0.001-0.018-0.005-0.0350.001-0.056 Acc-0.023-0.046-0.014-0.0340.002-0.006-0.011-0.034-0.018-0.001-0.059
DI0.1830.1480.1430.1560.0040.378loselose0.330inv0.456 DI0.1240.1880.1040.147inv0.3870.027lose0.2270.0760.438
SPD0.0280.0740.0230.0300.0030.058loselose0.051inv0.054 SPDbad0.0550.0120.029inv0.058badlose0.0350.0110.047
EOD0.0360.0410.0370.030loseloselose0.0030.044invlose EODbad0.0250.0130.024invloseloselose0.0370.010loseAdultCensusXGB
AOD0.0550.0640.0470.0530.0170.009lose0.0100.0660.020badGBC
AOD0.0370.0550.0410.0500.0180.0310.031lose0.0630.041bad
Acc-0.012-0.023-0.001-0.0070.000-0.008-0.014-0.008-0.0340.001-0.082 Acc-0.001-0.0080.0000.0020.0000.001-0.080-0.005-0.0750.000-0.143
DI0.1030.2240.0310.0380.0000.210losebadbad0.129bad DI0.1580.2360.1660.097lose0.312badbadbad0.016bad
SPD0.035bad0.0070.027lose0.065bad0.031bad0.021bad SPD0.0320.0510.0280.021lose0.053badbadbad0.002bad
EODlosebadloseloseloseloselose0.029bad0.001bad EODloselose0.0030.012loseinvinv0.054badinvinvRF
AOD0.0330.0390.0320.0320.016bad0.0200.046bad0.031badXGB2
AOD0.0230.0180.0260.027loseinvinv0.040bad0.014bad
Acc-0.0020.0020.0070.000-0.001-0.007-0.019-0.005-0.0580.003-0.018 Acc-0.003-0.0070.0140.011-0.0020.000-0.066-0.034-0.0580.001-0.104
DI0.0980.3480.1140.092lose0.2220.174badbad0.0670.183 DI0.0100.0690.0110.014lose0.332badbadbadinvbad
SPD0.0230.0620.0230.020lose0.042bad0.029bad0.013bad SPDlose0.028invinvlose0.052badbadbadinvbad
EOD0.021inv0.0110.018loseloselose0.064badinvlose EODloseloseinvinvloseinvlosebadbadinvinvBank MarketingXGB1
AOD0.0430.0400.0430.046loselose0.0380.054bad0.0200.043GBC
AOD0.0140.0230.0270.025lose0.017losebadbad0.012bad
Acc-0.020-0.027-0.012-0.012-0.011-0.016-0.007-0.016-0.529-0.004-0.446 Acc-0.011-0.022-0.019-0.015-0.007-0.013-0.009-0.044-0.042-0.012-0.203
DIbad0.076lose0.060lose0.0660.0390.0760.095badbad DI0.1090.1300.1030.1080.035badbadbadbad0.111bad
SPDbad0.052lose0.039lose0.0440.0250.0520.068badbad SPD0.0770.0920.0700.0780.021badbadbadbad0.078bad
EOD0.0440.0620.0450.0590.0330.0540.0420.0790.0640.023bad EOD0.1010.1010.1010.0810.0680.0390.039bad0.1070.092badRF
AODlosebadloseloseloseloselose0.0150.044losebadSVC
AOD0.0190.034bad0.027loseloselosebad0.0590.016bad
Acc0.003-0.009-0.014-0.017-0.016-0.028-0.0260.005-0.043-0.006-0.443 Acc0.0010.0000.010-0.0050.0000.0050.009-0.010-0.002-0.049-0.420
DI0.0700.0910.1190.101badbadbad0.051bad0.035lose DI0.1120.1040.0650.1230.0380.0710.0460.1600.1500.135bad
SPD0.0500.0650.0820.069bad0.0600.0480.038bad0.025bad SPD0.0720.0750.0430.0900.0270.0470.0280.1200.1110.098bad
EOD0.0690.0740.0730.0830.0360.0640.0640.1030.0910.055bad EOD0.1040.0800.0850.0760.0660.0660.0660.1280.1150.002badGerman CreditXGB
AOD0.0200.0200.0410.037losebadbad0.0150.064badbadKNN
AOD0.0170.011inv0.034loseinvinv0.0720.0650.035bad
Acc-0.098-0.130-0.129-0.128-0.014-0.166-0.021-0.010-0.1790.005-0.178 Acc-0.035-0.076-0.136-0.1260.065-0.139-0.015-0.048-0.189-0.023-0.165
DI1.5491.8641.8481.849lose0.5360.160lose2.0240.4492.303 DI0.5010.8851.4111.3030.0920.3850.038bad1.769bad1.991
SPD0.3950.5710.5510.545losebadbadlosebad0.1530.651 SPD0.1210.2750.4620.447invbadbad0.285bad0.1870.641
EOD0.2740.4040.4450.446loseloseloselose0.4810.045bad EODbadbadbadbad0.058loselose0.280bad0.1160.426RF
AOD0.4770.5560.5340.6010.062bad0.1330.0970.6180.336badGBC
AOD0.1830.3050.4670.4450.176bad0.0810.3740.5680.306bad
Acc-0.021-0.084-0.0910.000-0.007-0.1590.0150.009-0.227-0.023-0.152 Acc-0.079-0.101-0.099-0.110-0.019-0.129-0.0060.008-0.1570.009-0.159
DI0.7431.6191.6730.1490.0860.5970.2140.643badbad2.557 DI1.3641.7011.4701.663lose0.6710.2030.5391.811Inv2.172
SPD0.1010.5520.5970.1130.063bad0.0070.115bad0.3120.785 SPD0.2800.5420.4060.491losebad0.0650.0510.567Inv0.642
EODbad0.4670.5570.021loselose0.0090.171bad0.2750.623 EODbad0.4000.2850.389loseloselose0.0580.473Inv0.423Titanic
LRG
AOD0.1400.5620.6320.1790.101bad0.0670.214bad0.420badXGB
AOD0.3000.5320.3560.524losebad0.1740.1810.5850.062bad
Eachcell shows the accuracy/bias diÔ¨Äerencebetween the originaland repaired models. Foraccuracy, accuracydiÔ¨Äerence=newaccuracy- old accuracy. Forbias(DI,SPD, EOD,
AOD), bias diÔ¨Äerence = old bias - new bias. Thus, a positive value indicates an improvement in bias/accuracy in the repaired model compared to the original and vice versa. For bias,
if a method falls into either the good region (regular numbers) or the win-win region ( bold numbers ), the bias diÔ¨Äerence value will be provided. If it falls into any other region, the
region type will be indicated. The values highlighted in bluedenote the most eÔ¨Äective bug Ô¨Åxing method. The data from this table is divided and analyzed in depth in Tables 3,4,6.
Table 3:Proportionof Fair-AutoML ,Auto-Sklearn ,andmitigationtechniques that fallinto eachmitigationregion
Fairness Metric Dataset
DI SPD EOD AOD AdultCensus Bank Marketing GermanCredit Titanic Method
Lose Bad Inv Good Win Lose Bad Inv Good Win Lose Bad Inv Good Win Lose Bad Inv Good Win Lose Bad Inv Good Win Lose Bad Inv Good Win Lose Bad Inv Good Win Lose Bad Inv Good Win
T10% 6% 0% 75%19% 6% 12% 0% 63%19% 18%25% 0% 38%19% 6% 0% 0% 75%19% 0% 12% 0% 63% 25% 25% 0% 0% 75% 0% 6% 13%0%31%50% 0% 19% 0% 81% 0%
T20% 0% 0% 81%19% 0% 0% 0% 81%19% 12%13%13%56% 6% 0% 6% 6% 75%13% 0% 0% 12% 75% 13% 12% 13% 6% 50%19% 0% 6% 0%69%25% 0% 6% 0% 94% 0%
T36% 0% 0% 63%31% 6% 0% 6% 63%25% 6% 6% 6% 56%26% 6% 6% 6% 56%26% 0% 0% 0% 75% 25% 0% 12% 19%13%56% 19% 6% 6%50%19% 0% 6% 0% 94% 0%
T40% 0% 0% 75%25% 0% 0% 6% 75%19% 6% 6% 6% 63%19% 6% 0% 0% 69%25% 0% 0% 0% 100% 0% 5% 0% 13%19%63% 6% 0% 0%94% 0% 0% 6% 0% 94% 0%
Avg2% 2% 0% 73%23% 3% 3% 3% 70%21% 10%13% 6% 53%18% 5% 3% 3% 69%20% 0% 3% 3% 78% 16% 11% 6% 9% 39%35% 8% 6% 2%61%23% 0% 9% 0% 91% 0%
AS38% 6% 6% 38%12% 44% 6% 13%31% 6% 56% 0% 6% 25%13% 50% 0% 0% 31%19% 13% 0% 19% 44% 24% 88% 0% 0% 12% 0% 38%12%0%50% 0% 50% 0% 6% 25%19%
R0% 13% 0% 62%25% 0% 31% 0% 44%25% 50% 0% 19%25% 6% 18%38%13%19%12% 13% 0% 6% 62% 19% 19% 6% 19%25%31% 13%25%6%38%18% 25%50% 0% 25% 0%
DIR13%25% 6% 44%12% 6% 50% 6% 25%13% 50% 0% 13%25%12% 25% 6% 13%44%12% 31% 6% 19% 38% 6% 31% 38% 13%18% 0% 13%25%6%38%18% 19%13% 0% 43%25%
PML25%38% 6% 12%19% 19%25% 6% 31%19% 13%13% 6% 49%19% 6% 13% 6% 56%19% 44% 6% 25% 25% 0% 0% 50% 0% 50% 0% 0% 25%0%50%25% 19% 6% 0% 25%50%
EO0% 44% 0% 56% 0% 0% 56% 0% 44% 0% 0% 38% 0% 62% 0% 0% 31% 0% 69% 0% 0% 0% 0% 100% 0% 0% 100% 0% 0% 0% 0% 25%0%75% 0% 0% 44% 0% 56% 0%
FAX0% 25%19%25%31% 0% 6% 25%44%25% 0% 0% 38%50%12% 6% 6% 0% 38%50% 0% 6% 31% 44% 19% 0% 0% 0% 31%69% 6% 19%0%75% 0% 0% 13%19%38%30%
ROC6% 38% 0% 56% 0% 0% 50% 0% 50% 0% 25%38%12%25% 0% 0% 88% 0% 12% 0% 19%19% 0% 62% 0% 6% 68% 13%13% 0% 6% 94%0% 0% 0% 0% 31% 0% 69% 0%
Avg7% 31% 5% 43%14% 4% 36% 6% 40%14% 23%15%14%40% 8% 9% 30% 5% 40%16% 18% 6% 14% 55% 7% 9% 44% 7% 23%17% 6% 36%2%46%10% 11%26% 3% 43%17%
The proportions in this table are determined based on the data presented in Table 2: proportion for fairness metric = # buggy cases of a metric fall into a region / # buggy cases of
that metric, proportionfor dataset=# buggy cases fall of adatasetintoaregion/ # buggy cases of that dataset.
itsautomaticoptimizationofthebestMLmodelforagivendataset.
Wetailored Auto-Sklearn tobetterÔ¨Åtourmethodintwoways:(1)its
search space was restricted to the type ofthe faulty classiÔ¨Åer - for
example, if the faulty classiÔ¨Åer is Random Forest, Auto-Sklearn willonly optimize the hyperparameters and identify complementary
components for that speciÔ¨Åc classiÔ¨Åer. (2) The faulty model was
setasthedefaultmodelfor Auto-Sklearn .ThesemodiÔ¨Åcationsare
features of Auto-Sklearn that we utilized.
509Fix Fairness,Don‚Äôt RuinAccuracy: Performance Aware Fairness RepairusingAutoML ESEC/FSE ‚Äô23, December3‚Äì9, 2023,San Francisco, CA, USA
MethodologyConÔ¨Åguration .Weselectedanincrementvalue
of/u1D6FCfor/u1D6FDof0.05 to balancethe timebetween /u1D6FDsearch andmodel
Ô¨Åxingprocesses.Theusercanoptforamoreaccuratevalueof /u1D6FD
by decreasing theincrement value and using a longersearch time.
To conduct search space pruning, we ran Fair-AutoML 10 times (n)
with a 1-hour search time (t) to gather the best ML pipelines [ 9].
From each run, we collected the top 10 pipelines (k), resulting in
100modelsperinput.Thispre-builtsearchspaceincludesasetof
hyperparametersandthetop3mostfrequentlyusedcomplemen-
tarycomponents(m).Wehaveexploredotherparametersettings,
but thesehave proven to provideoptimal results.
Evaluation ConÔ¨Åguration .We evaluate each tool on each
buggy scenario 10 times using a randomre-split ofthedatabased
on a 7:3 train-test split ratio [ 42]. The runtime for each run of Fair-
AutoMLandAuto-Sklearn isapproximatelyonehour[ 28,29].The
meanperformanceofeachmethodiscalculatedastheaverageof
the 10 runs, which is a commonly used practice in the fairness
literature [ 4,6,14]. Ourevaluation targets Ô¨Åxing 16buggy models
for 4fairnessmetrics, resultinginatotalof64 buggy cases.
6.2 EÔ¨Äectiveness (RQ1)
We evaluate the eÔ¨Äectiveness of Fair-AutoML by comparing it with
Auto-Sklearn and existing bias mitigation techniques based on
Faireabaseline. The comparisons are based on the following rules:
‚Ä¢Rule1:Amodelisconsideredsuccessfullyrepairedwhen
its post-mitigation mean accuracy and fairness falls into
win-win/goodtrade-oÔ¨Äregions.
‚Ä¢Rule2:Amodelthatfallsinthewin-winregionisalways
betterthanone fallingintoany otherregion.
‚Ä¢Rule3:Iftwomodelsareinthesametrade-oÔ¨Äregion,the
one withlower biasispreferred.
Our comparison rules for bug-Ô¨Åxing performance were estab-
lished based on Fairea and our evaluations. Firstly, we deÔ¨Åne a
successful bug Ô¨Åx as a Ô¨Åxed model that falls within the win-win
orgoodtrade-oÔ¨Äregions,astheseregionsdemonstrateimproved
fairness-accuracy trade-oÔ¨Äs compared to the baseline in Fairea.
Secondly,whencomparingsuccessfullyÔ¨ÅxedmodelsindiÔ¨Äerent
trade-oÔ¨Äregions(win-winversusgood), we consider thewin-win
modelstobesuperior as theyoÔ¨Äer improvedfairnessandaccuracy.
Lastly, for models that fall within the same trade-oÔ¨Ä region, the
one with lower bias is deemed to be better, as our goal is to Ô¨Åx
unfair models. Our evaluations then consider two aspects of the
bug-Ô¨Åxingperformance:thenumberofsuccessfulbugÔ¨Åxesandthe
number oftimes abiasmitigationmethodoutperforms others.
6.2.1 Is Fair-AutoML EÔ¨Äective in Fixing Fairness Bugs? The results
presented in Table 4show that Fair-AutoML was eÔ¨Äective in re-
solving 60 out of 64 (94%) fairness bugs, while Auto-Sklearn only
Ô¨Åxed 28 out of 64 (44%) and bias mitigation techniques resolved
up to 44 out of 64 (69%). This indicates that Auto-Sklearn alone
was not eÔ¨Äective in reducing bias, however, our methods were suc-
cessful in enhancing AutoML to repair fairness bugs. Moreover,
Fair-AutoML was able to repair more cases than other bias mitiga-
tiontechniques,whichoftenresultedinloweraccuracyforlowerTable 4:Fair-AutoML(FA) vs bias mitigationmethods in
Ô¨Åxing fairness bugs
FAASRDIRPMLEOFAXROC
# bugs Ô¨Åxed 6028353036374423
# best models 1949010993
TheresultsinthistablearederivedfromthedatapresentedinTable 2.Therow #bugs
Ô¨Åxedindicates the number of cases where the technique falls into either the win-win
orgood trade-oÔ¨Äregion. The row # bestmodels represents the number of instances
whereabiasmitigation technique outperforms allothermethods.
bias. This highlights the eÔ¨Äectiveness of our approaches in guid-
ing AutoML towards repairing models for better trade-oÔ¨Ä between
fairnessandaccuracycomparedto the Fairea baseline.
6.2.2 Does Fair-AutoML Outperform Bias Reduction Techniques?
Fair-AutoML demonstrated superior performance in Ô¨Åxing fairness
bugs compared to other bias mitigation techniques. The results
presented in Table 4indicate that 63 out of 64 buggy cases were
Ô¨Åxed byFair-AutoML ,Auto-Sklearn , or bias mitigation techniques.
Among the repaired buggy cases, Fair-AutoML outperformed other
techniques 19 times (30%). On the other hand, Auto-Sklearn outper-
formedFair-AutoML andbiasmitigationtechniquesonly4times
(6%),andbiasmitigationtechniquesoutperformedothertechniques
10 times at most (16%). This highlights that Fair-AutoML is often
more eÔ¨Äective inimproving fairness and accuracy simultaneously
orreducing more biasthanotherbiasmitigationtechniques.
6.3 Adaptability (RQ2)
Toassessthe adaptabilityof Fair-AutoML ,we measure the propor-
tionsofeachevaluatedtoolsthatfallintoeachfairness-accuracy
trade-oÔ¨Ä region in diÔ¨Äerent categories:fairness metric and dataset
(Table3).Tofurtherevaluatetheadaptabilityof Fair-AutoML ,in-
steadofusingourpreparedmodelsanddatasets,weusedthebench-
mark[59]ofParfait-MLtoevaluate Fair-AutoML .Particularly,we
evaluateFair-AutoML andParfait-MLonthreediÔ¨ÄerentMLmodels
(DecisionTree,LogisticRegression,RandomForest)ontwodatasets
(AdultCensus andCOMPAS)(Table 5andFigure 3).
6.3.1 IsFair-AutoMLMoreAdaptableThanExistingBiasMitigation
TechniquesandAuto-Sklearn? Table3showsFair-AutoML demon-
stratesexceptionalrepaircapabilitiesacrossvariousdatasetsand
fairnessmetrics,withahighrateofsuccessinÔ¨Åxingbuggymodels.
Forexample, inthe Adult Census, Bank Marketing, GermanCredit,
andTitanicdatasets, Fair-AutoML (T4)repaired100%,82%,94%,and
94%ofthemodels,respectively.Similarly,intheDI,SPD,EOD,and
AOD fairness metrics, Fair-AutoML (T4) achieved repair rates of
100%, 94%, 82%, and 94%. On the other hand, bias mitigation meth-
odsoftenshowinconsistentresults.Forinstance, EqualizedOdds
repairedallbuggycasesin AdultCensus butnonein BankMarketing .
In fact, our methods eÔ¨Äectively guides AutoML in hyperparameter
tuningtoreducebias,leadingtosuperiorrepairperformanceacross
diÔ¨Äerentdatasets andmetrics.
6.3.2 IsFair-AutoMLEÔ¨ÄectiveinFixingFairnessBugsonOtherBias
Mitigation Methods Benchmark? Based on evaluation of Parfait-
ML [60], we only use accuracy and EOD as evaluation metrics
for this evaluation. To make a fair comparison with Parfait-ML,
weutilizetheversionof Fair-AutoML thatincorporatesEODand
accuracy as its cost function (T3). The results are displayed in
Table5, showcasing the accuracy and bias (EOD) achieved by both
510ESEC/FSE ‚Äô23, December3‚Äì9, 2023,San Francisco, CA, USA GiangNguyen, SumonBiswas, andHrideshRajan
Table 5: Accuracy and fairness achieved by Fair-AutoML and
Pafait-ML on Pafait-ML‚Äôs benchmark
DataDecisionTree LogisticRegression RandomForest
T3 PML T3 PML T3 PML
AccEODAccEODAccEODAccEODAccEODAccEOD
Adult0.8470.0360.8170.0020.8180.0380.8030.0230.8510.0320.8430.039
Compas 0.9690.0000.9700.0000.9700.0000.9680.0000.9700.0000.9700.000
Figure 3:Accuracyandfairness achieved by Fair-AutoML
(green circle) andPafait-ML (orangecircle) with Decision
Tree (left)andlogisticregression(right)on Adult dataset
(Pafait-ML‚Äôsbenchmark). The blue line showstheFairea
baseline andred linesdeÔ¨Åne thetrade-oÔ¨Ä regions.
Fair-AutoML (T3) and Parfait-ML in Parfait-ML‚Äôsbenchmark. The
table showcases the actual results of the repaired models, rather
thanthediÔ¨Äerenceinaccuracy/fairnessbetweentheoriginaland
repaired models. Upon inspection, the results for the COMPAS
datasetfor both Fair-AutoML andParfait-ML are similar. However,
for the Adult dataset, some diÔ¨Äerences arise. For instance, with
the Random Forest classiÔ¨Åer, Fair-AutoML performs better than
Parfait-MLinbothaccuracyandEOD.WiththeLogisticRegression
classiÔ¨Åer, Fair-AutoML achievedahigheraccuracybuthigherbias
comparedtoParfait-ML.Nevertheless, Fair-AutoML fallsintothe
win-win trade-oÔ¨Ä region, while Parfait-ML only falls into good
trade-oÔ¨Äregion(Figure 3).WiththeDecisionTreeclassiÔ¨Åer,both
Fair-AutoML andParfait-MLfall into the win-wintrade-oÔ¨Ä region
(Figure3); however, Parfait-ML performed better since it has lower
bias. These results highlights the generalization capability of Fair-
AutoMLto repairvariousdatasets andML models.
6.4 Ablation Study(RQ3)
WecreateanablationstudytoobservetheeÔ¨Éciencyofthedynamic
optimizationfunctionandthesearchspacepruningseparately.The
ablationstudy compares the performance ofthe following tools:
‚Ä¢Auto-Sklearn (AS) represents AutoML.
‚Ä¢Fair-AutoML version 1 ( FAv1) represents AutoML + dynamic
optimization function.
‚Ä¢Fair-AutoML version 2 ( FAv2) represents AutoML + dynamic
optimization function+searchspacepruning.
To evaluate the eÔ¨Éciency of the dynamic optimization function,
wecomparethe performanceof FAv1withAuto-Sklearn .Wecom-
pareFAv1withFAv2toobservetheeÔ¨Éciencyofthesearchspace
pruning approach. The complete result is shown in Table 6. Notice
thatweuse Fair-AutoML tooptimizediÔ¨Äerentfairnessmetrics;thus,
we only consider the metric that each tool tries to optimize. For
instance,theresultsofRandomForestonAdultdatasetintheTable
6showsthatachievedscoresof0.096forDI,0.014forSPD,0.024for
EOD, and 0.035 for AOD. This result means that T1 achieves 0.096
for DI, T2 achieves 0.014 for SPD, T3 achieves 0.024 for EOD, T4
achieves0.035forAOD.TheevaluationonlyconsiderscaseswhereTable 6:Trade-oÔ¨Äassessmentresults of Auto-Sklearn ,FAv1,
andFAv2
Metric Model ASFAv1FAv2Model ASFAv1FAv2
DI 0.0580.1190.096 0.040.0940.19
SPD 0.0160.0530.024 0.0050.0410.075
EOD 0.0080.0150.014 lose0.0430.044
AODRF
0.0210.0250.035LRG
0.0390.0780.089
DI 0.0040.1360.183 invbad0.124
SPD 0.0030.0460.074 invbad0.055
EOD lose0.0150.037 inv0.020.013AdultCensus
AODXGB
0.0170.0490.053GBC
0.018 inv0.05
DI 0.0000.6630.103 loseinv0.158
SPD lose0.026 bad loseinv0.051
EOD loseinvlose loselose0.003
AODRF
0.0160.0040.032XGB2
lose0.0030.027
DI loseinv0.098 lose0.0140.01
SPD lose0.0180.062 loselose0.028
EOD loseinv0.011 loseloseinvBank Marketing
AODXGB1
loselose0.046GBC
lose0.0030.025
DI losebadbad 0.0350.1270.109
SPD losebad0.052 0.0210.0780.092
EOD 0.033bad0.045 0.0680.1120.101
AODRF
loseloseloseSVC
lose0.0320.027
DI badbad0.07 0.038 inv0.112
SPD badlose0.065 0.0270.0120.075
EOD 0.036lose0.073 0.0660.0500.085German Credit
AODXGB
loselose0.037KNN
loseinv0.034
DI lose1.041.549 0.0920.4470.501
SPD lose0.5250.571 inv0.2730.275
EOD lose0.3860.445 0.0580.184 bad
AODRF
0.0620.5770.601GBC
0.0580.4290.445
DI 0.0861.0620.743 lose1.2051.364
SPD 0.0630.5940.552 lose0.3140.542
EOD lose0.5560.557 lose0.2870.285Titanic
AODLRG
0.1010.6510.179XGB
lose0.4410.524
The data in Table 6iscreated in the same waysas Table 2. Foreachmethod in the
good trade-oÔ¨Äregionand win-win region( bold number ),a trade-oÔ¨Ämeasurement
valueisgiven;for otherregions the regiontypeis displayed.The values in blue,
orange, and blackindicate the top1,top2,top3 bugÔ¨Åxing tools, respectively.
thetoolssuccessfullyrepairthebug.Thesamerulesdescribedin
RQ1 isappliedinthis evaluation.
6.4.1 AreDynamicOptimizationFunctionandSearchSpacePruning
EÔ¨ÄectiveinFixingFairnessBugs? FromTable 6,ourresultsshowthat
the dynamic optimization function approach in Fair-AutoML helps
Ô¨Åx buggy models more eÔ¨Éciently. Comparing the performance
in Ô¨Åxing fairness bugs, FAv1outperforms Auto-Sklearn 39 times,
whileAuto-Sklearn outperforms FAv1only7times.Thesearchspace
pruningapproachin Fair-AutoML alsocontributestomoreeÔ¨Écient
bug Ô¨Åxing, as FAv2outperforms both FAv1andAuto-Sklearn 46 and
55timesrespectively,while FAv1andAuto-Sklearn onlyoutperform
FAv214 and4times respectively.
7 DISCUSSION
In this work, we bring particular attention to the fairness-accuracy
tradeoÔ¨Ä while mitigating bias in ML models. Many works in the
area only optimize fairness metrics by sacriÔ¨Åcing accuracy, and do
not consider the tradeoÔ¨Ä rigorously. However, as shown by recent
work[42],trivialmutationmethodscanalsoachievefairnessifaccu-
racy is compromised in diÔ¨Äerent magnitudes. Therefore, a rigorous
evaluation method is necessary to demonstrate that the tradeoÔ¨Ä is
beneÔ¨Åcial.Anotherlimitationofexistingtoolsisnotgeneralizing
over diÔ¨Äerent ML classiÔ¨Åers (e.g., LRG, GBC, RF, XGB), multiple
fairnessmetrics, anddatasetcharacteristics. Tothatend,welever-
aged the recent progress of AutoML in the context and achieved
better tradeoÔ¨Ä than SOTA methods. We believe that our approach
511Fix Fairness,Don‚Äôt RuinAccuracy: Performance Aware Fairness RepairusingAutoML ESEC/FSE ‚Äô23, December3‚Äì9, 2023,San Francisco, CA, USA
is versatile and can be applied to various ML problems. Particu-
larly,thedynamicoptimizationfunctionapproachremainsversatile
across various datasets and models. Furthermore, the search space
pruning approach is reÔ¨Åned through pre-constructed database and
amatchingmechanism,thatcapitalizesondiversedatasetsstored
inrepositoriessuch as OpenMLorKaggle.
We implemented Fair-AutoML on top of Auto-Sklearn to ensure
itswideapplicabilityonMLalgorithms.State-of-the-artbiasmit-
igationtechniquesalsoprimarilyuseclassicMLalgorithms[ 6,7,
15,16,19,60] that are supported by Auto-Sklearn . These models
are more suitable than the DL models since the fairness critical
tasks in prior works commonly use tabular datasets. Should one
desire to explore alternative model types not directly supported
byAuto-Sklearn , they can adopt the general ML model adoption of
Auto-Sklearn [27].
Our approach also outlines several opportunities towards lever-
aging AutoML and search-based software engineering to ensure
fairnessinnewMLmodelsthatarebecomingavailable.First,the
greedyweightidentiÔ¨Åeralgorithm‚ÄôsperformancemightsuÔ¨Äerfor
complex models due to computational costs (Algorithm 1). Sec-
ond,searchspacepruningquantitativelyestimatesthesimilarity
ofdatasetsbasedondatacharacteristics.Thus,ifwedonothave
a dataset similar enough to the input dataset, AutoML may not
perform well. To address this, we plan to regularly update our
database with new datasets. Lastly, constructing suitable search
spaces, particularly for resource-intensive methods like deep learn-
ing,couldentailsigniÔ¨Åcantcomputationalexpenses.Furtherworks
are needed to maximize the versatility and eÔ¨Äectiveness of our
approach over novel fairness-critical tasks. One key direction is to
combineFair-AutoML withotherbiasmitigationtechniques,such
as integrating Fair-AutoML‚Äôsmodel with pre-processingbiasmiti-
gationmethodsto enhanceoverallpipeline fairness. Additionally,
integrating Fair-AutoML with ensemble learning could improve
both performance and fairness by capturing a broader range of
biases and patterns. These directions could signiÔ¨Åcantly amplify
the impact of this work, making Fair-AutoML a potent tool for
promotingfairness and equityin machinelearning acrossvarious
domains.
8 THREATS TO VALIDITY
ConstructValidity. Thechoiceofevaluationmetricsandexisting
mitigationtechniquesmayposeathreattoourresults.Wemitigate
this threat by employing a diverse range of metrics and mitiga-
tionmethods.First,wehaveusedaccuracyandfourmostrecent
andwidely-usedfairnessmetricstoevaluate Fair-AutoML andthe
state-of-the-art. These metrics have been commonly applied in
the software engineering community [ 15,16,19,60]. Second, we
demonstratethesuperiorityof Fair-AutoML overstate-of-the-art
methods in diÔ¨Äerent categories: pre-processing, in-processing, and
post-processing, which are most advanced techniques from the SE
andMLcommunities.Forevaluatingfairnessandapplyingthese
mitigation algorithms except Parfait-ML [ 60], we have used AIF
360 toolkit. For evaluating Parfait-ML, we have used its original
implementation. We create a baseline using the original Fairea im-
plementation, enabling us to conduct a comprehensive comparisonbetweenourapproachandexistedmitigationmethods.Inthefu-
ture,weintendtoexploresupplementaryperformancemetricsand
extendouranalysistoincorporateadditionalmitigationtechniques
for amore comprehensive evaluation.
ExternalValidity. Toensureanequitablecomparisonwithcutting-
edge bias mitigation techniques, we leverage a diverse array of
real-world models, datasets, and evaluation scenarios. Particularly,
we utilize a practical benchmark comprising 16 real-world models
thoughtfullycuratedbypriorresearch[ 6].Then,thesemeticulously
chosenmodelsundergoevaluationusingfourextensivelystudied
datasets in the fairness literature [ 10,61,62]. We conducted ex-
perimentsunderidenticalsetupsandsubsequentlyvalidatedour
Ô¨Åndings [ 6]. In addition to assessing Fair-AutoML against alter-
native methods within our established settings and benchmarks,
we subject Fair-AutoML to evaluation using the Parfait-ML [ 60]
benchmark,aleading-edge biasmitigationframework.
Internal Validity. Implementing Fair-AutoML on top of Auto-
Sklearnmay introducea threat to itsactual bias mitigation perfor-
mance.In other words,the favorableoutcomesachievedby Fair-
AutoMLcould be attributed to its integration with Auto-Sklearn . To
address this threat, we evaluated Auto-Sklearn on various bench-
marks,comparingitsperformancewith( Fair-AutoML )andwithout
(Auto-Sklearn ) our proposed approaches, to gauge the eÔ¨Äectiveness
ofFair-AutoML .
9 CONCLUSION
We present Fair-AutoML , an innovative system that enhances exist-
ingAutoMLframeworkstoresolvefairnessbugs.Thecoreconcept
ofFair-AutoML istooptimizethehyperparametersoffaultymodels
to resolve fairness issues. This system oÔ¨Äers two novel technical
contributions: a dynamic optimization function and a search space
pruningapproach.Thedynamicoptimizationfunctiondynamically
generates an optimization function based on the input, enabling
AutoMLtosimultaneouslyoptimizebothfairnessandaccuracy.The
searchspacepruningapproachreducesthesizeofthesearchspace
basedontheinput,resultinginfasterandmoreeÔ¨Écientbugrepair.
Our experiments show that Fair-AutoML outperforms Auto-Sklearn
and conventional bias mitigationtechniques, witha higherrate of
bug repair and a better fairness-accuracy trade-oÔ¨Ä. In the future,
we plan to expandthe capabilitiesof Fair-AutoML to include deep
learningproblems,beyondthe scope of the currentstudy.
10 DATA AVAILABILITY
Toincreasetransparencyandencouragereproducibility,wehave
made our artifact publicly available. All the source code and evalu-
ationdata withdetaileddescriptionscan be foundhere [ 33].
ACKNOWLEDGMENTS
ThisworkwassupportedinpartbyNationalScienceFoundation
under Grant CCF-15-18897, CNS-15-13263, CNS-21-20448, CCF-
19-34884, and CCF-22-23812. Additionally, our sincere gratitude
extends to the reviewers for their invaluable insights, which signif-
icantly contributedto enhancingthe quality of the paper.
512ESEC/FSE ‚Äô23, December3‚Äì9, 2023,San Francisco, CA, USA GiangNguyen, SumonBiswas, andHrideshRajan
REFERENCES
[1]Aniya Aggarwal, Pranay Lohia, Seema Nagar, Kuntal Dey, and Diptikalyan Saha.
2019. Blackboxfairnesstestingofmachinelearningmodels.In Proceedingsof
the201927thACMJointMeetingonEuropeanSoftwareEngineeringConference
and Symposiumonthe FoundationsofSoftwareEngineering . 625‚Äì635.
[2]Shibbir Ahmed, Sayem Mohammad Imtiaz, Samantha Syeda Khairunnesa,
Breno Dantas Cruz, and Hridesh Rajan. 2023. Design by Contract for Deep
Learning APIs. In ESEC/FSE‚Äô2023: The 31st ACM Joint European Software Engi-
neeringConferenceandSymposiumontheFoundationsofSoftwareEngineering
(SanFrancisco, California).
[3]JuliaAngwin,JeÔ¨ÄLarson,SuryaMattu,andLaurenKirchner.2016. Machinebias
risk assessmentsin criminal sentencing. ProPublica, May 23(2016).
[4]Rachel KE Bellamy, Kuntal Dey, Michael Hind, Samuel C HoÔ¨Äman, Stephanie
Houde, Kalapriya Kannan, Pranay Lohia, Jacquelyn Martino, Sameep Mehta,
Aleksandra Mojsilovic, et al .2018. AI Fairness 360: An extensible toolkit for
detecting, understanding, and mitigating unwanted algorithmic bias. arXiv
preprint arXiv:1810.01943 (2018).
[5]ReubenBinns.2018. Fairnessinmachinelearning:Lessonsfrompoliticalphiloso-
phy.InConferenceonFairness,AccountabilityandTransparency .PMLR,149‚Äì159.
[6]Sumon Biswasand Hridesh Rajan. 2020. Dothe Machine Learning Models on a
CrowdSourcedPlatformExhibitBias?AnEmpiricalStudyonModelFairness.In
ESEC/FSE‚Äô2020:The28thACMJointEuropeanSoftwareEngineeringConferenceand
Symposium onthe Foundations of SoftwareEngineering (Sacramento, California,
United States).
[7]SumonBiswasandHrideshRajan.2021. Fairpreprocessing:towardsunderstand-
ing compositional fairness of data transformers in machine learning pipeline.
InProceedingsofthe29thACMJointMeetingonEuropeanSoftwareEngineering
Conference and Symposium on the Foundations of Software Engineering . 981‚Äì993.
[8]SumonBiswasand HrideshRajan. 2023. Fairify:Fairness VeriÔ¨Åcationof Neural
Networks.In ICSE‚Äô2023:The45thInternationalConferenceonSoftwareEngineering
(Melbourne, Australia).
[9]Sumon Biswas, Mohammad Wardat, and Hridesh Rajan. 2022. The Art and
Practice of Data Science Pipelines: A Comprehensive Study of Data Science
Pipelines In Theory, In-The-Small, and In-The-Large. In ICSE‚Äô2022: The 44th
InternationalConference onSoftwareEngineering (Pittsburgh,PA, USA).
[10]Miranda Bogen andAaron Rieke. 2018. Help wanted:An examination ofhiring
algorithms,equity, and bias. (2018).
[11]AjayByanjankar,MarkkuHeikkil√§,andJozsefMezei.2015. Predictingcreditrisk
in peer-to-peer lending: A neural network approach. In 2015 IEEE symposium
seriesoncomputational intelligence . IEEE,719‚Äì725.
[12]Toon Calders and Sicco Verwer. 2010. Three naive Bayes approaches for
discrimination-free classiÔ¨Åcation. Data mining and knowledge discovery 21, 2
(2010), 277‚Äì292.
[13]Jos√©PCambronero,J√ºrgenCito,andMartinCRinard.2020. Ams:Generating
automlsearchspacesfromweakspeciÔ¨Åcations.In Proceedingsofthe28thACM
JointMeetingonEuropeanSoftwareEngineeringConferenceandSymposiumon
the FoundationsofSoftwareEngineering . 763‚Äì774.
[14]L Elisa Celis, Lingxiao Huang, Vijay Keswani, and Nisheeth K Vishnoi. 2019.
ClassiÔ¨Åcation with fairness constraints: A meta-algorithm with provable guaran-
tees. InProceedings of the conference on fairness, accountability, and transparency .
319‚Äì328.
[15]Joymallya Chakraborty, Suvodeep Majumder, and Tim Menzies. 2021. Bias in
machinelearningsoftware:Why?how?whattodo?.In Proceedingsofthe29th
ACM Joint Meeting on European Software Engineering Conference and Symposium
onthe FoundationsofSoftwareEngineering . 429‚Äì440.
[16]Joymallya Chakraborty, Suvodeep Majumder, Zhe Yu, and Tim Menzies. 2020.
Fairway:awaytobuildfairMLsoftware.In Proceedingsofthe28thACMJoint
Meeting on European Software Engineering Conference and Symposium on the
FoundationsofSoftwareEngineering . 654‚Äì665.
[17]JoymallyaChakraborty,TianpeiXia,FahmidMFahid,andTimMenzies.2019.
Softwareengineeringforfairness:Acasestudywithhyperparameteroptimiza-
tion.arXiv preprint arXiv:1905.05786 (2019).
[18]ZhenpengChen,JieMZhang,FedericaSarro,andMarkHarman.2022. Acompre-
hensive empirical study of bias mitigation methods for software fairness. arXiv
preprint arXiv:2207.03277 (2022).
[19]ZhenpengChen, JieM Zhang, Federica Sarro, and Mark Harman. 2022. MAAT:
a novel ensemble approach to addressing fairness and performance bugs for
machinelearningsoftware.In Proceedingsofthe30thACMJointEuropeanSoftware
EngineeringConferenceandSymposiumontheFoundationsofSoftwareEngineering .
1122‚Äì1134.
[20]AlexandraChouldechova.2017. Fairpredictionwithdisparateimpact:Astudy
of biasin recidivism prediction instruments. Big data5,2 (2017), 153‚Äì163.
[21]Alexandra Chouldechova, Diana Benavides-Prado, Oleksandr Fialko, and Rhema
Vaithianathan.2018. Acase study ofalgorithm-assisted decisionmakinginchild
maltreatment hotline screening decisions. In Conference on Fairness, Accountabil-
ity and Transparency . PMLR, 134‚Äì148.
[22]JeÔ¨Ärey De Fauw, Joseph R Ledsam, Bernardino Romera-Paredes, Stanislav
Nikolov,NenadTomasev,SamBlackwell,HarryAskham,XavierGlorot,BrendanO‚ÄôDonoghue, Daniel Visentin, et al .2018. Clinically applicable deep learning for
diagnosisandreferralinretinaldisease. Naturemedicine 24,9(2018),1342‚Äì1350.
[23]MatthiasEhrgott.2005. Multicriteriaoptimization .Vol.491. SpringerScience&
BusinessMedia.
[24]VirginiaEubanks.2018. Automatinginequality:Howhigh-techtoolsproÔ¨Åle,police,
and punish the poor . St.Martin‚ÄôsPress.
[25]Michael FeÔ¨Äer, Martin Hirzel, Samuel C HoÔ¨Äman, Kiran Kate, Parikshit Ram,
and Avraham Shinnar. 2022. An Empirical Study of Modular Bias Mitigators and
Ensembles. arXiv preprint arXiv:2202.00751 (2022).
[26]MichaelFeldman,SorelleAFriedler,JohnMoeller,CarlosScheidegger,andSuresh
Venkatasubramanian.2015. Certifyingandremovingdisparateimpact.In pro-
ceedingsofthe21thACMSIGKDDinternationalconferenceonknowledgediscovery
and datamining . 259‚Äì268.
[27] Matthias Feurer. 2022. Auto-Sklearn Documentation. https://automl.github.io/
auto-sklearn/master/
[28]MatthiasFeurer,KatharinaEggensperger,StefanFalkner,MariusLindauer,and
Frank Hutter. 2020. Auto-sklearn 2.0: The next generation. arXiv preprint
arXiv:2007.04074 (2020).
[29]MatthiasFeurer,AaronKlein,KatharinaEggensperger,JostSpringenberg,Manuel
Blum, and Frank Hutter. 2015. EÔ¨Écient and robust automated machine learning.
InAdvancesinneuralinformation processingsystems . 2962‚Äì2970.
[30]Sorelle A Friedler, Carlos Scheidegger, Suresh Venkatasubramanian, Sonam
Choudhary, Evan P Hamilton, and Derek Roth. 2019. A comparative study
offairness-enhancinginterventionsinmachinelearning.In Proceedingsofthe
conference onfairness,accountability, and transparency . 329‚Äì338.
[31]Sainyam Galhotra, Yuriy Brun, and Alexandra Meliou. 2017. Fairness testing:
testing software for discrimination. In Proceedings of the 2017 11th Joint meeting
onfoundations ofsoftwareengineering . 498‚Äì510.
[32]XuanqiGao,JuanZhai,ShiqingMa,ChaoShen,YufeiChen,andQianWang.2022.
FairNeuron:improvingdeepneural networkfairness withadversary gameson
selective neurons. In Proceedings of the 44th International Conference on Software
Engineering . 921‚Äì933.
[33]GiangNguyen,SumonBiwas,andHrideshRajan.2023. ReplicationPackageof
the ESEC/FSE 2023 Paper Entitled "Fix Fairness, Don‚Äôt Ruin Accuracy: Performance
AwareFairnessRepairusing AutoML .https://doi.org/10.5281/zenodo.8280911
[34]Usman Gohar, Sumon Biswas, and Hridesh Rajan. 2023. Towards Understanding
FairnessanditsCompositioninEnsembleMachineLearning.In ICSE‚Äô2023:The
45thInternationalConference onSoftwareEngineering (Melbourne, Australia).
[35]PrzemyslawAGrabowicz,NicholasPerello,andAarsheeMishra.2022. Marrying
fairness and explainability in supervised learning. In 2022 ACM Conference on
Fairness,Accountability,and Transparency . 1905‚Äì1916.
[36]Jussi Hakanen and Joshua D Knowles. 2017. On using decision maker prefer-
enceswithParEGO.In InternationalConferenceonEvolutionaryMulti-Criterion
Optimization . Springer, 282‚Äì297.
[37]Moritz Hardt, Eric Price, and Nati Srebro. 2016. Equality of opportunity in
supervisedlearning. Advancesinneuralinformationprocessingsystems 29(2016).
[38]XinranHe,JunfengPan,OuJin,TianbingXu,BoLiu,TaoXu,YanxinShi,Antoine
Atallah,RalfHerbrich,StuartBowers,etal .2014.Practicallessonsfrompredicting
clicksonads atfacebook. In Proceedings oftheeighthinternational workshopon
dataminingfor onlineadvertising . 1‚Äì9.
[39] MitchellHoÔ¨Äman,LisaBKahn, andDanielleLi.2018. Discretioninhiring. The
Quarterly Journal ofEconomics 133, 2 (2018), 765‚Äì800.
[40]MaxHort,ZhenpengChen,JieMZhang,FedericaSarro,andMarkHarman.2022.
Bia mitigationformachinelearningclassiÔ¨Åers:A comprehensivesurvey. arXiv
preprint arXiv:2207.07068 (2022).
[41]Max Hort and Federica Sarro. 2021. Did you do your homework? Raising aware-
nessonsoftwarefairnessanddiscrimination.In 202136thIEEE/ACMInternational
Conference onAutomatedSoftwareEngineering (ASE) . IEEE,1322‚Äì1326.
[42]Max Hort, Jie M Zhang, Federica Sarro, and Mark Harman. 2021. Fairea: A
model behaviour mutation approach to benchmarking bias mitigation methods.
InProceedingsofthe29thACMJointMeetingonEuropeanSoftwareEngineering
ConferenceandSymposiumontheFoundationsofSoftwareEngineering .994‚Äì1006.
[43]Haifeng Jin, Qingquan Song, and Xia Hu. 2019. Auto-Keras: An EÔ¨Écient Neural
ArchitectureSearchSystem.In Proceedingsofthe25thACMSIGKDDInternational
Conference onKnowledgeDiscovery& Data Mining . ACM,1946‚Äì1956.
[44]Kaggle.2017. AdultCensusDataset. https://www.kaggle.com/datasets/uciml/
adult-census-income
[45]Kaggle. 2017. Bank Marketing Dataset. https://www.kaggle.com/c/bank-
marketing-uci
[46]Kaggle. 2017. German Credit Dataset. https://www.kaggle.com/datasets/uciml/
german-credit
[47] Kaggle. 2017. TitanicMLDataset. https://www.kaggle.com/c/titanic/data
[48]Faisal Kamiran and Toon Calders. 2012. Data preprocessing techniques for
classiÔ¨Åcationwithoutdiscrimination. Knowledgeandinformationsystems 33,1
(2012), 1‚Äì33.
[49]FaisalKamiran,AsimKarim,andXiangliangZhang.2012. Decisiontheoryfor
discrimination-awareclassiÔ¨Åcation.In 2012IEEE12thInternationalConferenceon
DataMining . IEEE,924‚Äì929.
513Fix Fairness,Don‚Äôt RuinAccuracy: Performance Aware Fairness RepairusingAutoML ESEC/FSE ‚Äô23, December3‚Äì9, 2023,San Francisco, CA, USA
[50]Toshihiro Kamishima, Shotaro Akaho, Hideki Asoh, and Jun Sakuma. 2012.
Fairness-awareclassiÔ¨Åerwithprejudiceremoverregularizer.In JointEuropean
conferenceonmachinelearningandknowledgediscoveryindatabases .Springer,
35‚Äì50.
[51]KonstantinaKourou,ThemisPExarchos,KonstantinosPExarchos,MichalisV
Karamouzis, and Dimitrios I Fotiadis. 2015. Machine learning applications in
cancer prognosis and prediction. Computational and structural biotechnology
journal13(2015), 8‚Äì17.
[52]YanhuiLi,LinghanMeng,LinChen,LiYu,DiWu,YumingZhou,andBaowen
Xu. 2022. Training data debugging for the fairness of machine learning software.
InProceedings of the 44th International Conference on Software Engineering . 2215‚Äì
2227.
[53]Milad Malekipirbazari and Vural Aksakalli. 2015. Risk assessment in social
lending via random forests. Expert Systems with Applications 42, 10 (2015), 4621‚Äì
4631.
[54]Giang Nguyen, Johir Islam, Rangeet Pan, and Hridesh Rajan. 2022. Manas:
MiningSoftwareRepositoriestoAssistAutoML.In ICSE‚Äô22:The44thInternational
Conference onSoftwareEngineering (Pittsburgh,PA, USA).
[55]Luca Oneto, Michele Donini, Andreas Maurer, and Massimiliano Pontil. 2019.
Learningfairandtransferablerepresentations. arXivpreprintarXiv:1906.10673
(2019).
[56]Claudia Perlich, Brian Dalessandro, Troy Raeder, Ori Stitelman, and Foster
Provost. 2014. Machine learning for targeted display advertising: Transfer learn-
ingin action. Machinelearning 95,1 (2014), 103‚Äì127.
[57]RalphESteuerandEng-UngChoo.1983. AninteractiveweightedTchebycheÔ¨Ä
procedureformultipleobjectiveprogramming. Mathematicalprogramming 26,3
(1983), 326‚Äì344.
[58]Guanhong Tao, Weisong Sun, Tingxu Han, Chunrong Fang, and Xiangyu Zhang.
2022. RULER:discriminativeanditerativeadversarialtrainingfordeepneuralnetworkfairness.In Proceedingsofthe30thACMJointEuropeanSoftwareEngi-
neeringConferenceandSymposiumontheFoundationsofSoftwareEngineering .
1173‚Äì1184.
[59]SaeidTizpaz-Niari,AshishKumar,GangTan,andAshutoshTrivedi.2022. https:
//github.com/Tizpaz/Parfait-ML
[60]Saeid Tizpaz-Niari, Ashish Kumar, Gang Tan, and Ashutosh Trivedi. 2022.
Fairness-aware ConÔ¨Åguration of Machine Learning Libraries. arXiv preprint
arXiv:2202.06196 (2022).
[61]FlorianTramer,VaggelisAtlidakis,RoxanaGeambasu,DanielHsu,Jean-Pierre
Hubaux, Mathias Humbert, Ari Juels, and Huang Lin. 2017. Fairtest: Discovering
unwarranted associations in data-driven applications. In 2017 IEEE European
SymposiumonSecurityand Privacy (EuroS&P) . IEEE,401‚Äì416.
[62]SakshiUdeshi,PryanshuArora,andSudiptaChattopadhyay.2018. Automated
directedfairnesstesting.In Proceedings ofthe33rd ACM/IEEE InternationalCon-
ference onAutomatedSoftwareEngineering . 98‚Äì108.
[63]RhemaVaithianathan,TimMaloney,EmilyPutnam-Hornstein,andNanJiang.
2013. ChildreninthepublicbeneÔ¨Åtsystematriskofmaltreatment:IdentiÔ¨Åcation
via predictive modeling. American journal of preventive medicine 45, 3 (2013),
354‚Äì359.
[64]Muhammad Bilal Zafar, Isabel Valera, Manuel Gomez Rogriguez, and Krishna P
Gummadi. 2017. Fairness constraints: Mechanisms for fair classiÔ¨Åcation. In
ArtiÔ¨Åcial Intelligenceand Statistics . PMLR, 962‚Äì970.
[65]Brian Hu Zhang, Blake Lemoine, and Margaret Mitchell. 2018. Mitigating un-
wantedbiaseswithadversariallearning.In Proceedingsofthe2018AAAI/ACM
Conference onAI, Ethics,and Society . 335‚Äì340.
[66]PeixinZhang,JingyiWang,JunSun,GuoliangDong,XinyuWang,XingenWang,
Jin Song Dong, and Ting Dai. 2020. White-box fairness testing through adversar-
ial sampling. In Proceedings of the ACM/IEEE 42nd International Conference on
SoftwareEngineering . 949‚Äì960.
514