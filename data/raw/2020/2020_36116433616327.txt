NaNofuzz: A Usable Tool for Automatic TestGeneration
MatthewC.Davis
CarnegieMellonUniversity
Pittsburgh,Pennsylvania, USA
mcd2@cs.cmu.eduSangheonChoi
Rose-HulmanInstituteofTechnology
TerreHaute,Indiana,USA
chois3@rose-hulman.eduSam Estep
CarnegieMellonUniversity
Pittsburgh,Pennsylvania, USA
estep@cmu.edu
Brad A.Myers
CarnegieMellonUniversity
Pittsburgh,Pennsylvania, USA
bam@cs.cmu.eduJoshuaSunshine
CarnegieMellonUniversity
Pittsburgh,Pennsylvania, USA
sunshine@cs.cmu.edu
1
2
3
5
7{
{
6
4
Figure1: TheNaNofuzzuserinterfaceintheVisualStudioCodeIDEprovidesone-clicktestgenerationforTypeScriptprograms.
Key UI elements: (1) AutoTest button above a function signature, (2) NaNofuzz testing window beside the program under test,
(3) Customizable input parameters with default values, (4) Test button to start NaNofuzz, (5) Advanced options, (6) Categorized
testing results with likely bugs prioritized inthedisplay,(7) Pinbuttonto add testcases to the testsuite inJest format.
ABSTRACT
In the United States alone, software testing labor is estimated to
cost $48 billion USD per year. Despite widespread test execution
automation and automation in other areas of software engineering,
testsuitescontinuetobecreatedmanuallybysoftwareengineers.
We have built a test generation tool, called NaNofuzz, that helps
ESEC/FSE ’23, December 3–9, 2023, San Francisco, CA,USA
©2023 Copyright heldby theowner/author(s).
ACM ISBN 979-8-4007-0327-0/23/12.
https://doi.org/10.1145/3611643.3616327users ﬁnd bugsin their code by suggesting tests where the output
islikelyindicativeofabug,e.g.,thatreturnNaN(not-a-number)
values.NaNofuzzisaninteractivetoolembeddedinadevelopment
environmenttoﬁtintotheprogrammer’sworkﬂow.NaNofuzztests
afunctionwithaslittleasonebuttonpress,analysestheprogramto
determineinputsitshouldevaluate,executestheprogramonthose
inputs, and categorizes outputs to prioritize likely bugs. We con-
ducted a randomized controlled trial with 28 professional software
engineersusingNaNofuzzasthe interventiontreatment and the
popular manual testing tool, Jest, as the control treatment. Partici-
pantsusingNaNofuzzonaverageidentiﬁedbugsmoreaccurately
(/u1D45D<.05, by 30%), were more conﬁdent in their tests ( /u1D45D<.03, by
20%),andﬁnishedtheirtasks more quickly ( /u1D45D<.007,by30%).
Thiswork islicensedunderaCreativeCommonsAttribution4.0Interna-
tional License.
1114
ESEC/FSE ’23, December3–9, 2023,San Francisco, CA, USA Ma/t_thewC.Davis,SangheonChoi, SamEstep, Brad A.Myers, andJoshua Sunshine
CCS CONCEPTS
•Softwareanditsengineering →Softwaretestinganddebug-
ging;•Human-centered computing →User studies .
KEYWORDS
Empiricalsoftwareengineering,userstudy,softwaretesting,human
subjects,experiments, usable testing,automatictest generation
ACMReference Format:
MatthewC.Davis,SangheonChoi,SamEstep,BradA.Myers,andJoshua
Sunshine.2023.NaNofuzz:AUsableToolforAutomaticTestGeneration.In
Proceedings of the 31st ACM Joint European Software Engineering Conference
andSymposiumontheFoundationsofSoftwareEngineering(ESEC/FSE’23),
December 3–9, 2023, San Francisco, CA, USA. ACM, New York, NY, USA,
13pages.https://doi.org/10.1145/3611643.3616327
1 INTRODUCTION
Software testing often intends to prevent bugs of various forms
fromaﬀectingusersandoperations[ 30,47,64].Theseeﬀortsare
estimated to represent 28% [ 6] to 50% [ 64] of the $174 billion USD
annual [45] 2021 labor cost of US software engineering profession-
als.Whileeach2%reductionintestingeﬀortmaysave$1billion
USD per year in labor1, engineers largely create test suites man-
ually [4,21,24,35]. An important problem is therefore how to
provideautomation support for engineers creating test suites.
AutomaticTestsUiteGeneration(ATUG) toolsattemptto
ﬁll this gap by generatingpersistent test cases using various tech-
niquesandobjectives(e.g.,testsuitesize,codecoverage,readability,
mutants killed, etc.). However, most existing tools lack evidence
thattheyimproveasoftware engineer’s abilitytocreate eﬀective
test suites [ 24,51], and these tools are not broadly adopted in in-
dustry [4,51]. For example, when the ATUG tool EvoSuite was
evaluatedwith humans, itshoweda penaltyfor using it [ 24]. Evo-
Suiteusesthe(buggy)softwareundertestasitstestoracle;asall
tests “pass,” the engineer must read the generated tests to ﬁnd and
ﬁx the “erroneous” tests. The negative human evaluation result
implies that the newtask of ﬁnding and ﬁxing invalid tests may be
morediﬃcultthanthe oldtaskEvoSuiteseekstoreplace:manually
creatingatestsuite.Subsequentresearchhastriedtoimprovethese
results by improving test suite readability [ 28,42,49,57,60], but
evidenceisunclearthatthesehelpengineersrelativetomanualtest
suite creation. We argue that a human-focused approach may be
moreappropriatefordesigningATUGtoolssothattheyimprove
engineers’ abilityto ﬁnd bugsandcreate test suites.
We present NaNofuzz , a human-focused ATUG tool that is
integrated into the Visual Studio Code integrated development en-
vironment (IDE). NaNofuzz allows a software engineer to test a
functionwithaslittleasoneclick.Itaccomplishesthisbyanalyz-
ing the function signature to determinewhattypes ofinputsthe
function accepts. It uses heuristics to propose default value ranges
for each input and allows the engineer to reﬁne those ranges, if
necessary.NaNofuzzrapidly generates testcasesbysampling the
input domain of the function. As NaNofuzz generates and executes
testcases,itautomaticallycategorizesandorganizesthetestresults
in the engineer’s IDE using a set of heuristics to identify likely
1$174 billion ·28% [6]·2%=$0.97billion; $174 billion ·50% [64]·2%=$1.74billionproblematicoutputvalues.Thetestcasesmorelikelytobeelicit-
ingbugsaremore-prominentlydisplayedtodrawtheengineer’s
attention. The engineer maythen add test casesto the test suite.
We conducted a randomized controlled human trial with 28
professionalsoftwareengineersusingNaNofuzzastheintervention
treatmentandthepopularmanualtestingtool,Jest[ 62],asthecon-
trol treatment.Participants using NaNofuzzonaverageidentiﬁed
bugs more accurately ( /u1D45D<.05, by 30%), were more conﬁdent in
their tests( /u1D45D<.03, by20%), andﬁnished theirtasks morequickly
(/u1D45D<.007,by30%).
Thispapercontributes:(i)NaNofuzz,ahuman-focusedautomatic
testgenerationtool;and(ii) an experimentalhuman evaluationof
NaNofuzzthatprovidesevidencethatNaNofuzzimprovessoftware
engineers’ abilityto generatetest casesfor atest suite.
2 DEFINITIONS
This paper uses a number of terms. A test oracle , proposed by
Howden[29], allows one to “check the correctness oftest output.”
“Program under test” is abbreviated as PUT, and “software engi-
neer”isoftenshortenedto engineer .Testsuitesize measuresthe
number of test cases or statements within the test suite. A mutant
is a modiﬁcation made to the original program through simple and
intentional syntax changes that aims to create faults [ 31]. Mutants
arekilledwhen a test case detects diﬀerences between the mu-
tant and the original code. Code coverage measures what code
is executed during a test [ 65]. Afuzzeris a tool that exercises a
PUTwithrandomly-determinedinputstoelicitbugs[ 38,40].An
opaque box fuzzer has no knowledge of the internals of the PUT
andgeneratestest casesbasedoninput/outputbehavior[ 38].
3 NANOFUZZ
NaNofuzzisintendedtohelpengineersﬁndincorrectnessduring
development or testing. Salient aspects of NaNofuzz are described
below.We reference the interfaceelements showninFigure 1.
(1) AutoTest button . NaNofuzz decorates exported TypeScript
functionsdisplayedintheIDE’seditorwithan“AutoTest”button.
Clickingthe buttonopens NaNofuzzinasidewindow (2).
(2)NaNofuzzwindow .Thetestingwindowopensbesidethefunc-
tionundertest sothat both the code andits tests are visible.
(3)Inputparameters .NaNofuzzanalyzesthefunctionsignatureto
determineitsinputs,types,anddefaultinputranges.Thehuman-in-
the-loop may adjust these ranges if desired. To minimize cognitive
load, inputsare displayedinaTypeScript-like format.
(4)Testbutton .Thisbuttonstartsthetest,whichusesastochastic,
opaque-box fuzzer with an implicit oracle that classiﬁes the follow-
ing as likely errors: runtime exceptions, non-termination within
a conﬁgurable time threshold, and outputs containing null, NaN,
inﬁnity,orundeﬁned.Whenthefuzzerterminates,testingresults
are displayedinthe results grid (6).
(5)Options .Thisbuttontogglesthedisplayofadvancedoptions,
which are shown in Figure 2and allow the engineer to, e.g., adjust
the fuzzer’s runtime. By default, the fuzzer returns results in 3
seconds, and runs at most 1,000 tests to ensure the tool provides
rapidfeedbackto maintaintheengineer’sattention.Longertesting
sessions maybe conﬁgured.
1115NaNofuzz: A Usable Tool forAutomatic TestGeneration ESEC/FSE ’23, December3–9, 2023,San Francisco, CA, USA
Figure 2: NaNofuzz with “More Options”Pane
(6)Results .Thispanedisplaysasetofrelevantcategorytabs,each
containingagridoftestresults.Thecategoriesare:non-termination,
runtimeexception,erroneousoutputs(e.g.,null,NaN,inﬁnity,unde-
ﬁned),andpassedtests.Tabscontainingnoresultarenotdisplayed.
Todirecttheengineer’sattentiontoteststhatarelikelytobeerrors,
thetabsareorganizedintheordershownabove.Weoptedtodis-
play tests and results in table form rather than as code to minimize
the engineer’s readingeﬀort.
(7) Pin button . After NaNofuzz displays test results, the engineer
may add any of the generated tests to the test suite by pressing the
Pin button next to the test result on the results grid (6). In Figure 1,
the engineer has pinned two tests. When pinned, NaNofuzz saves
the unit test in Jest format to the ﬁle system. When the test button
(4)is pressed again, all pinned tests are re-executed with their test
results displayed at the top of the grid above any newly-generated
tests. If a test is un-pinned, then the Jest unit test is removed from
the ﬁle system.
4 SCOPE AND LIMITATIONS
NaNofuzz has a number of important limitations, but it is a useful
vehicle for evaluating how a human-focused automatic testing tool
mightaﬀectanengineer’sabilitytogenerateatestsuite.Asmen-
tionedabove,theexperimentalversionofNaNofuzzinthisstudy
usesanimplicitoraclesuchthatitiscurrentlynotpossibletospecify
desired result values for a test case. This version of NaNofuzz also
only supports exported TypeScript functions that have parameters
of types: ﬁnitenumbers(integers andﬂoats),strings,booleans, lit-
eralobjecttypes,n-dimensionalarraysofanyoftheprevioustypes,
as well as optional and mandatory parameters. Type references are
not supported. In addition, default input ranges are determined us-
ingtype-basedheuristics.WeplantobroadensupportasNaNofuzz
matures.
5 EXPERIMENT
Weprovideanoverviewofthe randomizedcontrolledtrial in
Figure3andthedataandmethodsinTable 1.SadowskiandZim-
mermann [58]describe software engineer productivity in terms
of three dimensions—quality, satisfaction, and velocity. This paper
investigates the extent to which NaNofuzz impacts three testing
measuresinspiredbythesedimensions:bugidentiﬁcationaccuracy,conﬁdence,andtasktime.Thisstudyaimstoanswerthreevariants
of the question: “Relative to standard practice (e.g., Jest), to what
extentmay NaNofuzz aﬀect /u1D44B?”where the valuesof /u1D44Bare:
RQ1./u1D44B=the number of bugsan engineer accurately identiﬁes
RQ2./u1D44B=the engineer’s conﬁdence inthe test activity
RQ3./u1D44B=the engineer’s time onthe testingtask
Hypothesis : We hypothesize that NaNofuzz improves software
engineers’abilitytocreatetestsbyautomaticallysupportingtwo
tasks that are diﬃcult for engineers: identifying edge cases and
understanding howoutputs relate to inputs.
This experiment uses a between-subjects design . Human eval-
uations in software engineering are rare [ 34], and recruiting a
suﬃcient number of professional software engineers to achieve
statisticalpowerinahumanevaluationismorerare.Oneapproach
to reduce the number of participants required for statistical signiﬁ-
cance is to select a within-subjects design with repeated measures
such that participants use both treatments to complete the same or
matchedtasks.Fatigueandlearningeﬀectsareimportantproblems
thatmayoftenbeaddressedbycounter-balancingthetask/treat-
mentsequence.Whilesuchadesignwouldreducetherecruiting
burden, a within-subjects repeated-measures design is not suitable
for this study due to the task’s high learning eﬀect: once a partici-
pant discovers a bug, they do not quickly forget it. Consequently, a
between-subjectsdesignismoreappropriate,despiteitsoﬀering
less statisticalpower for the same number of participants.
In our design, participants use both treatments, but unlike a
within-subjects design they did so on diﬀerent tasks. We could
therefore ask participants to rate relative usability and provide
comparative feedback about both treatments to provide diagnostic
insightinto whyonetreatmentmighthaveameasuredeﬀect.Since
we collected timing data, a think-aloud protocol was not appropri-
ate.Instead,wecollectedqualitativedatainthepost-survey.Before
thestudybegan,wepilotedthestudywith9pilotparticipantsto
reﬁnethetasksandinstructions.Thisstudywasreviewedbyour
Institutional ReviewBoard.
5.1 Treatments
Jest[62] is an open-source TypeScript and JavaScript unit-testing
framework. As of October 2022, Jest had 50 million monthly down-
loads, was used in over 3.8 million public GitHub repositories, and
wasusedatMeta(Jest’screator),Twitter,Spotify,Airbnb,andmany
othercompanies[ 62].Thus,Jestrepresentsthestate-of-the-artin
manual test suite generation. Engineers largely create test suites
manually rather than using ATUG tools [ 4,21,24,35] such as Evo-
Suite, which one study showed had disappointing results when
compared against a manual testing tool [ 24]. Consequently, it is
important to compare ATUG tools with the tools that practitioners
areactivelyusinginpractice,evenifthosewidely-usedtoolslack
automation.
Tocontrolfordiﬀerencesthatwedidnotwanttomeasure,we
used Jest Runner [ 63], which allowed participants to run Jest via
a GUI button in the IDE, similar to the way they run NaNofuzz.
Similarly, our protocol instructed participant to open the Jest tests
“to the side” as shown in Figure 4so that bothcode and tests were
simultaneouslyvisible,similartoNaNofuzz.Tominimizetyping,
1116ESEC/FSE ’23, December3–9, 2023,San Francisco, CA, USA Ma/t_thewC.Davis,SangheonChoi, SamEstep, Brad A.Myers, andJoshua Sunshine
/gid00021/gid00028/gid00046/gid00038/gid00001/gid00427
 /gid00021/gid00028/gid00046/gid00038/gid00001/gid00426
/gid00017/gid00042/gid00046/gid00047/gid00496
/gid00020/gid00048/gid00045/gid00049/gid00032/gid00052
/gid00008/gid00045/gid00042/gid00048/gid00043/gid00001/gid00002
/gid00017/gid00028/gid00045/gid00047/gid00036/gid00030/gid00036/gid00043/gid00028/gid00041/gid00047/gid00046
/gid00506/gid00041/gid00715/gid00422/gid00425/gid00507
/gid00008/gid00045/gid00042/gid00048/gid00043/gid00001/gid00003
/gid00017/gid00028/gid00045/gid00047/gid00036/gid00030/gid00036/gid00043/gid00028/gid00041/gid00047/gid00046
/gid00506/gid00041/gid00715/gid00422/gid00425/gid00507
/gid00004/gid00042/gid00041/gid00046/gid00032/gid00041/gid00047
/gid00021/gid00048/gid00047/gid00042/gid00045/gid00036/gid00028/gid00039 /gid00021/gid00028/gid00046/gid00038/gid00001/gid00422 /gid00021/gid00048/gid00047/gid00042/gid00045/gid00036/gid00028/gid00039 /gid00021/gid00028/gid00046/gid00038/gid00001/gid00423 /gid00021/gid00028/gid00046/gid00038/gid00001/gid00424 /gid00021/gid00028/gid00046/gid00038/gid00001/gid00425
/gid00011/gid00032/gid00046/gid00047 /gid00011/gid00032/gid00046/gid00047 /gid00011/gid00032/gid00046/gid00047 /gid00011/gid00032/gid00046/gid00047 /gid00011/gid00032/gid00046/gid00047 /gid00011/gid00032/gid00046/gid00047 /gid00011/gid00032/gid00046/gid00047 /gid00011/gid00032/gid00046/gid00047
/gid00015/gid00028/gid00015/gid00042
/gid00014/gid00032/gid00028/gid00046/gid00048/gid00045/gid00032/gid00040/gid00032/gid00041/gid00047/gid00046/gid00001/gid00004/gid00042/gid00039/gid00039/gid00032/gid00030/gid00047/gid00032/gid00031/gid00001/gid00033/gid00045/gid00042/gid00040/gid00001/gid00021/gid00028/gid00046/gid00038/gid00046/gid00001/gid00422/gid00496/gid00427/gid00475
/gid00422/gid00473 /gid00004/gid00042/gid00048/gid00041/gid00047/gid00001/gid00042/gid00033/gid00001/gid00003/gid00048/gid00034/gid00046/gid00001/gid00006/gid00039/gid00036/gid00030/gid00036/gid00047/gid00032/gid00031/gid00001/gid00506/gid00019/gid00018/gid00422/gid00473/gid00422/gid00507
/gid00423/gid00473 /gid00003/gid00048/gid00034/gid00001/gid00005/gid00032/gid00046/gid00030/gid00045/gid00036/gid00043/gid00047/gid00036/gid00042/gid00041/gid00001/gid00002/gid00030/gid00030/gid00048/gid00045/gid00028/gid00030/gid00052/gid00001/gid00506/gid00019/gid00018/gid00422/gid00473/gid00423/gid00507
/gid00424/gid00473 /gid00017/gid00028/gid00045/gid00047/gid00036/gid00030/gid00036/gid00043/gid00028/gid00041/gid00047/gid00001/gid00004/gid00042/gid00041/gid00414/gid00031/gid00032/gid00041/gid00030/gid00032/gid00001/gid00506/gid00019/gid00018/gid00423/gid00507
/gid00425/gid00473 /gid00023/gid00032/gid00039/gid00042/gid00030/gid00036/gid00047/gid00052/gid00001/gid00506/gid00019/gid00018/gid00424/gid00507/gid00014/gid00032/gid00028/gid00046/gid00048/gid00045/gid00032/gid00040/gid00032/gid00041/gid00047/gid00046/gid00001/gid00004/gid00042/gid00039/gid00039/gid00032/gid00030/gid00047/gid00032/gid00031/gid00001/gid00033/gid00045/gid00042/gid00040/gid00001/gid00017/gid00042/gid00046/gid00047/gid00496/gid00020/gid00048/gid00045/gid00049/gid00032/gid00052/gid00475
/gid00426/gid00473/gid00001/gid00001/gid00001/gid00001/gid00021/gid00028/gid00046/gid00038/gid00001/gid00022/gid00046/gid00028/gid00029/gid00036/gid00039/gid00036/gid00047/gid00052/gid00001/gid00506/gid00005/gid00036/gid00028/gid00034/gid00041/gid00042/gid00046/gid00047/gid00036/gid00030/gid00507
/gid00427/gid00473/gid00001/gid00001/gid00001/gid00001/gid00021/gid00042/gid00042/gid00039/gid00001/gid00019/gid00032/gid00028/gid00039/gid00036/gid00046/gid00040/gid00001/gid00506/gid00005/gid00036/gid00028/gid00034/gid00041/gid00042/gid00046/gid00047/gid00036/gid00030/gid00507
/gid00428/gid00473/gid00001/gid00001/gid00001/gid00001/gid00021/gid00042/gid00042/gid00039/gid00001/gid00004/gid00042/gid00040/gid00043/gid00028/gid00045/gid00036/gid00046/gid00042/gid00041/gid00001/gid00506/gid00005/gid00036/gid00028/gid00034/gid00041/gid00042/gid00046/gid00047/gid00036/gid00030/gid00507/gid00015/gid00028/gid00015/gid00042 /gid00015/gid00028/gid00015/gid00042 /gid00015/gid00028/gid00015/gid00042 /gid00015/gid00028/gid00015/gid00042 /gid00015/gid00028/gid00015/gid00042 /gid00015/gid00028/gid00015/gid00042 /gid00015/gid00028/gid00015/gid00042
Figure3: Visualizationofstudytasksequence(seeSection 5).ParticipantsarerandomlyassignedbypairsintoGroupAor
Group B,which determines thetreatmentforeachtask. Shaded tasks(e.g., tutorials) are nottime-limited.
Table 1:Measurements,Instruments,andMethods
Collection Research Productivity
IDMeasurement Instrument Step Question Dimension † AnalysisMethod(s)
1Count ofBugsElicited Scoring Rubric Task RQ1.1 Quality ANOVA,Fisher’s exact test* [ 53]
2Bug Descr.Accuracy Scoring Rubric Task RQ1.2 Quality ANOVA,Fisher’s exact test* [ 53]
3Engineer Conﬁdence Likert Scale Task RQ2 Satisfaction ANOVA,Fisher’s exact test* [ 53]
4 Task Time ElapsedTime (clock) Task RQ3 Velocity ANOVA,paired /u1D461-test*[53]
5ToolUsability Sys.UsabilityScale [ 9]Post-Survey Diagnostic n/a Directcomparison
6 Task Realism Likert Scale Post-Survey Diagnostic n/a Directcomparison
7ToolComparison Free-formSurvey Post-Survey Diagnostic n/a Inductive Thematic Analysis[ 8]
†=As deﬁnedbySadowskiandZimmermann [ 58];*=two-tailed
Table 2:Programs UnderTest
Program
TaskName FoundOn LinesErrorClass Bugs
16.tsStackOverﬂow 3exception 1
23.tsStackOverﬂow 4NaNoutput 1
37.tsStackOverﬂow 12divide by0 1
411.tsRosetta Code 14exception 1
514.tsStackOverﬂow 15inﬁnite loop 1
610.ts GitHub 57NaNoutput 2
we provided Jest participants a single passing Jest test case and
instructedparticipants to copy andpaste from it.
NaNofuzz is the ATUG tool we created and described in Section 3.
To minimize inﬂuenceonparticipants, we didnot disclose during
thesessionthatwecreatedNaNofuzz;rather,wecharacterizedboth
tools as ones participants might have used previously. Participants
werenotallowedtousetheinternetduringthestudy,andwecalled
NaNofuzz“AutoTest”to obscure its origin.5.2 Tasks
AsshowninFigure 3,theexperimentincludedtwotutorials—one
foreachtreatment—andsixtestingtasks.Thesixtasksvaried:(a)
theprogramundertest(Table 2)accordingtoourstudyprotocoland
(b)thetreatment(Section 5.1)accordingtotheparticipant’srandom
group assignment and task number. In each of the six tasks, the
participant used Visual Studio Code and the treatment to generate
test casesandelicit bugsinthe program undertest.
Upon starting the task, we verbally instructed the participant to
openthePUTintheIDEandprovidedverbalinstructionsfroma
script explaining that the goal of the task was to ﬁnd inputs to the
program that cause any of the following results: null, undeﬁned,
NaN,inﬁnity,aruntimeexception,orapparentnon-termination.We
speciﬁedwhichtreatmenttouseand,ifJest,directedtheparticipant
to open the Jest test ﬁle “to the side” so that both the tests and the
code under test were simultaneously visible. We reminded each
participantthatthecommentatthetopoftheprogramspeciﬁed
theallowedinputvalues.Afterprovidingtheinstructionsandthe
participantindicatedtheywereready,wemanuallyrecordedthe
starttime.UsingZoom,wemonitoredeachparticipant’sscreenand
audiotoensureuseoftheintendedtreatmentandprogram.During
thetask,theparticipanttestedtheprogramusingthedesignated
1117NaNofuzz: A Usable Tool forAutomatic TestGeneration ESEC/FSE ’23, December3–9, 2023,San Francisco, CA, USA
treatment and created test cases. At the end of 15 minutes or when
theparticipantwasdonecreatingthetests,werecordedthestop
timeandverballyinstructedtheparticipanttocompleteapost-task
surveywheretheytypeduptheirunderstandingofthegeneralized
input domains where bugs occurred and their conﬁdence in the
testingactivityaccording to a5-pointLikert scale.
ProgramsUnderTest .WeutilizedKoetal.’s[ 34]suggestionto
use“found”tasks:wesearchedGitHub,StackOverﬂow,andRosetta
Codeand found14 faulty TypeScript programs that violate anim-
plicit oracle in some circumstances due to the presence of bugs. As
some programs were code fragments, we added or edited code nec-
essaryfortheprogramtorun.Weaddedtypeannotationsas-needed
to avoid distracting IDE warnings and provided a description of
each program, its allowed inputs, and its expected outputs. We ran
aseriesofpilotswithstudentsusingvariouscombinationsofthe14
buggy programs. These pilots allowed us to: (i) eliminate programs
that pilot users could not ﬁnish testing in 15 minutes regardless of
the treatment used, and (ii) determine the relative diﬃculty of the
remaining programs. Based on these observations and pilot partici-
pantfeedback,weselectedthesixprogramsundertestdescribed
inTable2andsequencedthemaccording to increasing diﬃculty.
Task Infrastructure . Deployment of experimental study software
to remote software engineers is diﬃcult [ 13]. To avoid some of
thesediﬃculties,wehostedthetasksonGitHubCodespaces[ 26],
which provides a web-based VS Code IDE and VM such that a
remotesoftwareengineermayedit,run,test,anddebugcodeusing
acomplete IDE inside awebbrowser.
Tutorials .WedesignedtheNaNofuzzandJesttutorialstobesimilar
suchthatboth havetwoshortexercises andareroughlythesame
length.Whileparticipantsspentamean93secondslongerinthe
NaNofuzztutorial,thismaybedueto68%(19/28)ofparticipants
enteringthe study sessionwithprevious Jest experience.
5.3 Participants
Finding and recruiting a large and/or representative sample of pro-
fessionalsoftware engineersisdiﬃcult [ 2,5,13,34].We recruited
professionalsoftwareengineersviaLinkedIn,Twitter,Mastodon,
ande-mail—bothviadirectmessagesandviapublicpostingsthat
described our study and included a link to the screener survey,
which screened for participants: (i) in the United States or Canada
(as required by our IRB), (ii) who were over 18, (iii) had at least
one year of professional programming experience, and (iv) had
programming experience with TypeScript. We oﬀered participants
a $30 Amazon gift card and did not oﬀer bonuses. When recruiting
onLinkedIn,weselectedparticipantslocatedintheUnitedStates
and Canada with TypeScript experience and more than one year of
professional software engineering experience in their proﬁles. We
askede-mailanddirectmessagerecipientstorecruitothersthey
thoughtmightbe open toparticipating;however, we didnot oﬀer
incentives to doso.
From November 4, 2022 to January 6, 2023, the screener survey
received552responsesandautomaticallyclassiﬁed99responses
as likely being eligible, of which 35 were humans that scheduled
sessionsand 28completedthestudy. Fourconsentedparticipants
are excluded from the data set: one was unable to access their
GitHub account and was unable to start the study, another did notfollowprotocol,andtwomorehadto leaveunexpectedlywithout
ﬁnishingthe session.
In the screener survey, potential participants self-reported: gen-
der; professional software engineering experience; hours of coding
perweek;andexperiencewithtestingtools,TypeScript,Jest,and
VS Code. The screener included timed questions recommended
byDanilova [12]toeliminatenon-programmers.Wealsocreated
timedTypeScriptandJestquestions,whichweusedtoidentifybots.
TheﬁnalpageofthescreenerincludedaGoogleCalendarlinkthat
allowedthe participant to choose atime slot.
Participantswereassignedtogroups using matchedpairran-
domassignment andaphysicalcoinﬂip.Weclassiﬁedparticipants
by self-reported professional coding experience: 1–5 years, 6–10
years,and11+years.Whenaparticipantscheduledasession,we
assigned a participant number and checked to see if a previous
participantwiththesameexperiencelevelwasawaitingamatch.
If no participant with the same experience level was awaiting a
match, we ﬂipped the coin to determine the participant’s group,
andtheparticipantwasﬂaggedasneedingamatch.Whenthenext
participant with the same experience level scheduled a time slot,
the new participant would be matched to the previous one such
thatoneparticipantwouldberandomlyassignedtogroup /u1D434and
the otherrandomly assignedto group /u1D435.
Participantdemographics were asfollows: 5participantsidenti-
ﬁedasfemale,21asmale,and2didnotdisclose;4participantshad
10+yearsofprofessionalexperience,4had6–9years,and20had
1–5 years; 11 participants reported spending 30+ hours coding per
week, 13reported spending 10–29 hours per week, and4 reported
exactly 5 hours per week. A table showing the demographics of
eachparticipant isprovidedinthe supplementary materials.
5.4 Measurements
The experiment includes seven measurements as shown in Table 1:
(1)Countofbugselicited .Priortothestudy,theﬁrstthreeauthors
created an unambiguous rubric that listed the bugs in each task
programandtheinputsetsthateliciteachbug.Theparticipants’
tests were evaluated by the ﬁrst or third author against this rubric
to determinehowmanybugswere elicited.
(2) Bug description accuracy . After generating the tests, the par-
ticipantwasaskedtotypeinthegeneralcircumstancesunderwhich
each bug may be elicited. For example, suppose we had a program
that throws a runtime exception for integers > 1. According to the
rubric, a full score was given for stating the entire set accurately
(integers>1).Ahalfscorewasgiveniftheparticipant’ssetincluded
only part of the rubric’s described subset (e.g., integers>2). Finally,
ahalfscore wasdeductedif theparticipant’s setincludedallowed
inputsthat didnot elicit the bug(e.g.,integers<1).
(3) Engineer conﬁdence . At the end of each task, the participant
reported their degree of conﬁdence in identifying the inputs under
whichthe bugsare elicitedusing a5-pointLikert scale.
(4) Task time . At the start of each task, the researcher recorded
the begin time manually. When the participant ﬁnished creating
tests or ran out of time, the researcher recorded the end time. Task
time wasmeasuredinseconds.
1118ESEC/FSE ’23, December3–9, 2023,San Francisco, CA, USA Ma/t_thewC.Davis,SangheonChoi, SamEstep, Brad A.Myers, andJoshua Sunshine
1
2
Figure 4: VSCodeEditor(1) with Jest Tests(2) Opened “tothe side.”
(5)Toolusability .TheSystemUsabilityScale(SUS)[ 9]isastan-
dardmeasureofsystemusability.Inthepost-survey,participants
rated both treatments using the standard SUS questions, which
were scoredaccording totheprocedure describedby Brookeetal .
[9].Theorderofthetreatmentsinthesurveywasrandomizedto
counter-balanceorderingeﬀects.
(6)Taskrealism .Participantsratedtheiragreementwiththestate-
ment, “I thought the testing tasks in this study resemble tasks I
mightencounteroutsidethe study,”using aLikert scale.
(7)Toolcomparison .Participantsansweredfree-formcompara-
tive survey questions regarding the treatments’: (i) eﬀectiveness in
testing, (ii) ease of use, (iii) ﬁt with the participant’s test and debug
workﬂow,and(iv) opportunitiesfor improvement.
5.5 Data Analysis
AsshowninTable 1,thismixed-methodsexperimentquantitatively
analyzedtaskdatatoanswertheresearchquestions.Adiagnostic
explanatoryanalysiswasalsodesignedtohelpexplainorexplore
theprimaryquantitativeresults.Wediscusstheanalysisforeach
measurement from Figure 3andTable 1below.
(1)Countofbugselicited .Weusedarandomizedmatchedpair
design(see Section 5.3)basedonyears ofprofessionalexperience.
From 28 total participants, this yielded 14 pairings. Within each
pairing,we created two pseudo-participants,one bytaking all the
data from the pairing using Jest, and another by taking all the
datafromthepairingusingNaNofuzz.Thesepseudo-participants
naturally partition into two groups, one exclusively using eachtreatment.Wethenperformeda two-tailedFisher’sexacttest on
the categorical data. There are a total of seven bugs per participant
forthismeasure,soforthismeasureweﬁrstallocated 7·14=98
possibletests,andthenwithinthetwogroups(JestandNaNofuzz),
counted up the number of bugs actually elicited by participants
through a test case. For each task and treatment, we performed
anANOVA usingthe presenceofthe interventionasafactorand
analyzed the eﬀect of other independent variables: years of pro-
fessionalexperience,experiencewithTypeScript,experiencewith
Jest,andexperiencewithVSCode.Forthismeasure,tasksixhas
twiceasmanybugsastasks1-5,sowedivideditsrawscoreby2
sothat scores acrosstaskshave the same range.
(2) Bug description accuracy . This analysis is the same as (1),
except we used categories corresponding to the recorded accuracy
values of −1,0,1,2, and3. We counted up the number of accuracy
scores ineachcategory for the twogroups, as showninTable 4.
(3)Engineerconﬁdence .Thisanalysisisthesameas(1),except
thatweusedcategoriescorrespondingtothepossibleconﬁdence
scores 1–5, as shown in Table 4,and we did not needto normalize
the range oftask6.
(4) Task time . This analysis is the same as (1), except that we
used atwo-tailed paired /u1D461-testdue to this measure containing
continuous time data rather than categorical data. See Table 5, and
we didnot needto normalizethe range of task6.
(5) Tool usability . We counted the number of participants who
gaveahigherSystemUsabilityScale[ 9]scoreforNaNofuzzvs.Jest
1119NaNofuzz: A Usable Tool forAutomatic TestGeneration ESEC/FSE ’23, December3–9, 2023,San Francisco, CA, USA
and divided it by the number of participants. We calculated the
mean and standard deviation of the SUS scores for each treatment.
(6) Taskrealism .We counted thenumber ofparticipants who re-
spondedinthepost-surveythatthetaskswererealistic(vs.neutral,
unrealistic) anddividedthe count bythe number ofparticipants.
(7) Tool comparison . We analyzed free-form post-survey data
qualitativelyusingtheinductivethematicanalysisprocedurede-
scribed by Braun and Clarke [ 8], which calls attention to Frith and
Gleeson[ 25]asa“particularlygoodexampleofaninductivethe-
maticanalysis.”WethereforeusedFrithandGleesonasamodelfor
our data analysis. Braun and Clarke [ 8] describes six phases and
emphasizes: (i) the phases are guidelines, not rules; (ii) thematic
analysisis“notlinear”andmovementamongphasesisexpected;
and(iii)theprocessshouldnotberushed.Goingbeyondtherec-
ommendations of Braun and Clarke [ 8] and Frith and Gleeson [ 25],
the third author established replicability of the second author’s
result by re-coding the data for ﬁve random participants using the
secondauthor’scodes.Thisresultedinasubstantial[ 36]levelof
inter-rater reliability( /u1D705=0.6962).
6 RESULTS
BelowwereporttheresultsofthedataanalysesdescribedinSection
5.5. Table3summarizes measures 1–4 of Figure 3. Figure5plots
results for measures 1–5. Table 4shows the two-tailed Fisher’s
exact tests for measures 1–3. Table 5shows the paired /u1D461-test for
measure 4.
(1) Count of Bugs Elicited . Table3shows that participants us-
ing NaNofuzz, on average, detected more bugs (92%) than when
usingJest(80%).Thetwo-tailedFisher’sexacttestshowninTable 4
(Measure1)indicatesthediﬀerencesinthismeasurearestatistically
signiﬁcant( /u1D45D<0.05).TheANOVAraisesnostatistically-signiﬁcant
alternative hypotheses.
(2) Bug description accuracy . Table3shows that participants us-
ingNaNofuzz,onaverage,describedbugsmoreaccurately(1.57/2.00)
than when using Jest (1.18/2.00). The two-tailed Fisher’s exact test
shown in Table 4(Measure 2) indicates the diﬀerences in this mea-
sure are statistically signiﬁcant ( /u1D45D<0.007). The ANOVA indicates
thatVSCodeexperiencewaspositivelycorrelatedwithtasktime
(/u1D45D<0.05) ontask2.
(3)Engineerconﬁdence .Table3andFigure 5showthatpartici-
pantsusingNaNofuzz,onaverage,reportedhigherconﬁdenceon
the Likert scale (3.70/5.00) than when using Jest (3.08/5.00). The
two-tailedFisher’sexacttestshowninTable 4(Measure3)indicates
themeasureddiﬀerencesarestatisticallysigniﬁcant( /u1D45D<0.03).The
ANOVA raisesnostatistically-signiﬁcant alternative hypotheses.
(4)Tasktime .Outof168tasks( 28participants ·6tasksperpartic-
ipant), 13 Jest tasks (15%) and 2 NaNofuzz tasks (2%) were stopped
due to running out of time. Table 3and Figure 5show that par-
ticipants usually completed tasks more quickly with NaNofuzz
than with Jest. The two-tailed paired /u1D461-test shown in Table 5in-
dicatesthediﬀerencesinthismeasurearestatisticallysigniﬁcant
(/u1D45D<0.0001).TheANOVAindicatesthatVSCodeexperiencewas
positivelycorrelatedwithtasktime ( /u1D45D<0.007) ontask4.(5) Tool usability . Figure5shows that 96% (27/28) of participants
ratedNaNofuzz (mean=87.86,SD=11.05)higheron theSystemUs-
abilityScale [ 9]thanJest (mean=75.98, SD=14.47).
(6) Task realism . 86% (24/28) of participants indicated the tasks in
this study were realistic.
(7)Toolcomparison .Ourinductivethematicanalysisidentiﬁed
sevenrepeatedthemesinparticipants’qualitativestatementsabout
Jest andNaNofuzz.The themes are:
T1:Automation can reduce human cognitive eﬀort required for
creating test cases.
T2:Automation can reduce manual laborfor creating tests.
T3:Flexibilityisvaluable—when I needit.
T4:I needto specifywhat correctness means.
T5:Building atest suite can require iteration andexploration.
T6:Understandingmanytestoutputshelpsmeunderstandthe
program behaviorandbe more conﬁdent.
T7:Intuitive tooldesigncan reduce barriers.
We discuss the implicationsof thesethemes inthe nextsection.
7 DISCUSSION
Our experiment investigated the three research questions intro-
ducedinSection 5—RQ1, RQ2,andRQ3. We now discusshow the
answers to thesequestionsare suggestedbyour results.
RQ1.Relativetostandardpractice,towhatextentmayNaNo-
fuzzaﬀectthenumberofbugsanengineeraccuratelyidenti-
ﬁes?NaNoFuzz improved the accuracy of bug identiﬁcation. Table
3showsthatparticipantsonaveragefound15%morebugs(Mea-
sure 1) and described bugs 30% more accurately (Measure 2) when
using NaNofuzzthanwhen using Jest.
However, task 6’s large input domain made it less likely for
NaNofuzz to randomly generate inputs that elicited task 6’s two
bugs. Of the participants who used NaNofuzz on task 6, 50% (7/14)
elicited both bugs in the task, 21% (3/14) elicited one bug, and 29%
(4/14) elicited no bugs at all. A successful strategy some partici-
pants adopted was narrowing NaNofuzz’ inputranges to generate
smallermatricesthatweremorelikelytoelicitthebugs.Still,three
participants(P33,P43,P44)ranNaNofuzzmultipletimes,didnot
observeobviousbugs,andincorrectlyconcludedthatnobugswere
present. Finding unlikely buggy inputs is a common problem with
fuzzers, and incorporating additional input generation guidance
(e.g.,codecoverage)intoNaNofuzzmaysupporthigheraccuracy
inthesesituations.Displayingorvisualizingintheuserinterface
how much of the input domain has been explored by NaNofuzz
mayhelpengineers betterdecidewhen to stop lookingfor bugs.
RQ2.Relativetostandardpractice,towhatextentmayNaNo-
fuzz aﬀect the engineer’s conﬁdence in the test activity?
NaNofuzz improved engineers conﬁdence. Table 3shows thatpar-
ticipantswereonaverage20%moreconﬁdent(Measure3)when
using NaNofuzzthanwhen using Jest.
ThehigherconﬁdencethatNaNofuzzinstillscouldnegatively
aﬀect accuracyinsomecases. Forinstance,P14 quicklyelicitedthe
bugintask1usingNaNofuzzandself-ratedahighconﬁdencelevel
butthendescribedthebugincompletely.Thispatternrepeatsfor
P34 task 2 and P31 task 3. One solution may be for NaNofuzz to
1120ESEC/FSE ’23, December3–9, 2023,San Francisco, CA, USA Ma/t_thewC.Davis,SangheonChoi, SamEstep, Brad A.Myers, andJoshua Sunshine
Table 3:Quantitative TaskResults Summary forMeasures1-4 fromTable 1(/u1D45B=28)
Task1 Task2 Task3 Task4 Task5 Task6 All Tasks
Tool Mean SD Mean SD Mean SD Mean SD Mean SD Mean SD Mean SD
Measure 1:Bugselicited (1.00=allbugsfound);higher isbetter
Jest 0.93 0.27 1.000.001.00 0.00 1.00 0.00 0.36 0.50 0.50 0.39 0.80 0.38
NaNofuzz 1.000.000.930.271.000.001.000.001.000.000.610.450.920.25
Measure 2:Bug descriptionaccuracy(2.00=allbugsdescribed accurately);higher is better
Jest 1.29 0.99 1.71 0.47 1.71 0.61 1.64 0.63 0.21 0.43 0.50 0.44 1.18 0.43
NaNofuzz 1.860.361.930.271.930.271.930.271.210.800.540.541.570.69
Measure 3:Engineerconﬁdence(Likert Scale);higher isbetter
Jest 3.14 1.35 3.43 1.22 3.29 0.99 4.36 0.74 2.29 1.07 2.00 1.04 3.08 1.31
NaNofuzz 4.210.704.070.834.140.664.360.843.071.142.361.153.701.15
Measure 4:Time oftestsuite creation (elapsed seconds);loweris better
Jest 585 224 526 199 622 214 297 113 585 255 628 200 540 230
NaNofuzz 28413942115031811823377402199622222380199
Bold =Superiormean score
Table 4:Two-tailed Fisher’s Exact Test forMeasures1-3 fromTable 1
Measure 1 Jest NaNo Σ
bugelicited 74 86 160
bugnot elicited 24 12 36
Σ98 98 196Measure 2 Jest NaNo Σ
accuracy =−11 1 2
accuracy =020 6 26
accuracy =119 15 34
accuracy =244 61 105
accuracy =30 1 1
Σ84 84 168Measure 3 Jest NaNo Σ
conﬁdence =113 6 19
conﬁdence =217 7 24
conﬁdence =316 14 30
conﬁdence =426 36 62
conﬁdence =512 21 33
Σ84 84 168
Table 5:Two-tailed Paired /u1D461-test forMeasure 4 fromTable 1
Measure 4 Jest NaNo
P12 / P15 time 59m52s 42m38s
P14 / P16 time 55m32s 35m26s
P18 / P19 time 61m46s 47m49s
P20 / P21 time 49m23s 28m58s
P22 / P24 time 42m13s 33m30s
P23 / P33 time 58m37s 42m39s
P25 / P27 time 58m09s 43m20s
P26 / P44 time 61m57s 29m44s
P30 / P45 time 48m25s 38m15s
P31 / P46 time 48m44s 32m45s
P32 / P34 time 46m21s 42m53s
P35 / P36 time 42m40s 28m52s
P37 / P47 time 59m58s 46m12s
P42 / P43 time 66m53s 38m42s
mean time 54m19s 37m59slookforadditional,adjacentexamplesofalikelybugonceoneis
found—and then to summarize theseﬁndingsfor the engineer.
RQ3.Relativetostandardpractice,towhatextentmayNaNo-
fuzz aﬀectthe engineer’stime onthe testingtask? NaNofuzz
spedup testingtasks.Table 6shows participantsonaveragecom-
pletedtasks30%fasterwithNaNofuzzthanwithJest.
WhydidNaNofuzzshowpositiveresults?Whendidparticipants
encounterproblemsusingNaNofuzz?Ourparticipantscompared
bothtreatmentsinthepost-survey,andwestructuretheremainder
ofthissectionusingtheseventhemespresentedinSection 6and
then we discuss someour ownthoughts.
T1: Automation can reduce human cognitive eﬀort required
for creating test cases. The process of testing involves a number
oftasksthatarecognitivelydiﬃcult:tocreateatestcaseinJest,the
engineer must think of inputs to test, and then determine what the
appropriateoutputmightbe.Byautomaticallygeneratinginputs
and providing a categorized list of test results for the engineer
to choose from, NaNofuzz may beneﬁt from the advantages of
recognitionover recall[ 44]to improve task time(RQ3) .
“[NaNofuzz] is really useful for surfacing edge cases without me
needing to think ofthem.”(P45.Q15)
1121NaNofuzz: A Usable Tool forAutomatic TestGeneration ESEC/FSE ’23, December3–9, 2023,San Francisco, CA, USA
/gid00506/gid00422/gid00507/gid00001/gid00003/gid00048/gid00034/gid00046/gid00001/gid00006/gid00039/gid00036/gid00030/gid00036/gid00047/gid00032/gid00031 /gid00506/gid00423/gid00507/gid00001/gid00002/gid00030/gid00030/gid00048/gid00045/gid00028/gid00030/gid00052 /gid00506/gid00424/gid00507/gid00001/gid00006/gid00041/gid00034/gid00036/gid00041/gid00032/gid00032/gid00045/gid00001/gid00004/gid00042/gid00041/gid00414/gid00031/gid00032/gid00041/gid00030/gid00032 /gid00506/gid00425/gid00507/gid00001/gid00021/gid00028/gid00046/gid00038/gid00001/gid00021/gid00036/gid00040/gid00032 /gid00506/gid00426/gid00507/gid00001/gid00022/gid00046/gid00028/gid00029/gid00036/gid00039/gid00036/gid00047/gid00052/gid00011/gid00032/gid00046/gid00047/gid00001/gid00001/gid00001/gid00001/gid00001/gid00001/gid00001/gid00001/gid00015/gid00028/gid00015/gid00042 /gid00011/gid00032/gid00046/gid00047/gid00001/gid00001/gid00001/gid00001/gid00001/gid00001/gid00001/gid00001/gid00015/gid00028/gid00015/gid00042 /gid00011/gid00032/gid00046/gid00047/gid00001/gid00001/gid00001/gid00001/gid00001/gid00001/gid00001/gid00001/gid00015/gid00028/gid00015/gid00042 /gid00011/gid00032/gid00046/gid00047/gid00001/gid00001/gid00001/gid00001/gid00001/gid00001/gid00001/gid00001/gid00015/gid00028/gid00015/gid00042 /gid00011/gid00032/gid00046/gid00047/gid00001/gid00001/gid00001/gid00001/gid00001/gid00001/gid00001/gid00001/gid00015/gid00028/gid00015/gid00042/gid00003/gid00048/gid00034/gid00046/gid00001/gid00006/gid00039/gid00036/gid00030/gid00036/gid00047/gid00032/gid00031
/gid00003/gid00048/gid00034/gid00001/gid00005/gid00032/gid00046/gid00030/gid00045/gid00036/gid00043/gid00047/gid00036/gid00042/gid00041/gid00001/gid00002/gid00030/gid00030/gid00048/gid00045/gid00028/gid00030/gid00052
/gid00004/gid00042/gid00041/gid00414/gid00031/gid00032/gid00041/gid00030/gid00032/gid00001/gid00506/gid00020/gid00048/gid00040/gid00001/gid00042/gid00033/gid00001/gid00013/gid00036/gid00038/gid00032/gid00045/gid00047/gid00001/gid00020/gid00030/gid00028/gid00039/gid00032/gid00046/gid00507
/gid00006/gid00039/gid00028/gid00043/gid00046/gid00032/gid00031/gid00001/gid00014/gid00036/gid00041/gid00048/gid00047/gid00032/gid00046/gid00001/gid00506/gid00046/gid00048/gid00040/gid00001/gid00042/gid00033/gid00001/gid00047/gid00028/gid00046/gid00038/gid00046/gid00507
/gid00020/gid00052/gid00046/gid00047/gid00032/gid00040/gid00001/gid00022/gid00046/gid00028/gid00029/gid00036/gid00039/gid00036/gid00047/gid00052/gid00001/gid00020/gid00030/gid00028/gid00039/gid00032/gid00001/gid00020/gid00030/gid00042/gid00045/gid00032/gid00428
/gid00427
/gid00426
/gid00425
/gid00424 /gid00425/gid00427/gid00429/gid00422/gid00421
/gid00423/gid00423
/gid00422/gid00429
/gid00422/gid00425/gid00423/gid00427/gid00422/gid00423
/gid00427/gid00421
/gid00426/gid00421
/gid00425/gid00421
/gid00424/gid00421/gid00425/gid00421/gid00427/gid00421/gid00429/gid00421/gid00422/gid00421/gid00421
Figure5: ViolinplotsshowingthedistributionofresultsbytreatmentforMeasures1-5.Y-axisforMeasures1-4showsthesum
of six tasks for paired participants. Y-axis for Measure 5 is the SUS [ 9] score range. The mean is indicated by a black dot. Grey
dotsindicateobservations. Widerareasoftheplot indicatealargernumberofobservations.
“[With Jest] if I didn’t know what was wrong with the program I
hadtositandstareatthesourcecodeormaybetryafewrandom
guess inputsto get unstuck.”(P25.Q11)
T2:Automationcanreducemanuallaborforcreatingtests.
ManualtestgenerationtoolssuchasJestoftenrequiresigniﬁcant
manual labor to type up each test case, including repetitive boil-
erplate testing code that can result in a low signal-to-noise ratio.
Forexample, Figure 4shows aten lineJesttest case,butonly half
the lines contain any meaningful information for testing. While
theseburdensmightseemsmall,participantsnoticedthisdiﬀerence,
whichmaycontributeto improved task time(RQ3) .
“[Jest] can be most cumbersome to use when there are a lot of
similar edge cases that need to be tested, providing those edge
casescantakealotofeﬀorttosetupvs[NaNofuzz]makingthem
foryou.”(P25.Q16)
“[NaNofuzz] helped me enumerate a large set of cases quickly. It
wouldbeusefulinscenarioswherestatespaceexplosionisrelevant
andthebehavioroftheprogramiscomplexandunpredictable.”
(P34.Q11)
T3: Flexibility is valuable—when I need it. No tool is always
best.Someparticipantssaidtheywouldusebothtoolswhentesting.
“Woulduse both in combination” (P22.Q11.3)
“I like [NaNofuzz] more, but still think that both are better situa-
tionally.”(P30.Q11)
T4: I need to specify what correctness means. NaNofuzz in the
formweevaluateddoesnotallowtheengineertospecifyanexplicit
oracle such that the PUT’s correctness relative to a speciﬁcationmay be evaluated. Some participants noticed this limitation and
suggesteditbe addressedinfuture versionsof NaNofuzz.
“Theoptiontoenforceagroundtruthvaluehelpstocheckboth
theundeﬁned, NaN, etc errorsas well as logicerrors.”(P35.Q11)
“Jest makes way more sense to test exact semantics one cares
about.”(P14.Q15)
T5:Buildingatestsuitecanrequireiterationandexploration.
TraditionaltoolslikeJestthatrequiresigniﬁcantmanualeﬀortto
operate can make it diﬃcult to explore a PUT’s behavior during
testing, especially with a new or unfamiliar program. NaNofuzz’
automationsupportsexplorationoftherelationshipamongaPUT’s
inputs andoutputs, whichmaycontributeto NaNofuzz’ improved
performance on bugsaccuratelyidentiﬁed (RQ1) .
“[NaNofuzz]wasusefulindynamicallyverifyingmyassumptions
and exploringoptions I wasn’t aware of”(P22.Q11.2)
“[W]henIwantedto testawide rangeof valuesorwanted to do
somequick overview [NaNofuzz] wasmore useful” (P21.Q13.2)
T6:Understandingmanytestoutputshelpsmeunderstand
theprogrambehaviorandbemoreconﬁdent. Simplyhaving
more test results available made many participants feel more conﬁ-
dentintheirtesting,eitherbecausetheybetter-understoodthefunc-
tion’sbehaviororbecausethehighernumberoftestcaseswasmore
likelytoﬁndunexpectederrors.Theseaspectsmaycontributeto
NaNofuzz’improvedperformanceon engineerconﬁdence(RQ2) .
“I was notcertain Ihad generated enough inputsmanually using
Jest.”(P16.Q11)
1122ESEC/FSE ’23, December3–9, 2023,San Francisco, CA, USA Ma/t_thewC.Davis,SangheonChoi, SamEstep, Brad A.Myers, andJoshua Sunshine
“[NaNofuzz gave me] conﬁdence that I had exercised lots of
possibleinputsforthefunctionsundertest.WithJestIfeltlikeI
wasﬁshing around trying to ﬁnddegeneratecases.”(P26.Q11)
T7:Intuitivetooldesigncanreducebarriers. WesawinSection
6thatparticipantsratedNaNofuzzasmoreusableontheSystem
Usability Scale, and the free-form feedback provides insight into
some reasons that NaNofuzz might be considered more usable. For
instance, NaNofuzz outputs have a higher signal-to-noise ratio
than Jest, and NaNofuzz only requires minimal information to test
whileJest expectsengineersto writetestingcode, much ofwhich
isboilerplate.
“[NaNofuzz] had areallysimple UI whileJest displayed alotof
informationthatwasn’tnecessaryformetounderstandhowtogo
about ﬁxingthe outputor ﬁguringout whatinputs were wrong.”
(P08.Q13)
“Itwaseasiertoclick[NaNofuzz’]testbuttonandthenjustchange
theinputparameters.”(P20.Q15)
8 THREATS TO VALIDITYOFTHE STUDY
This study has a number of limitations that might be addressed in
future studies.
InternalValidity .Toensurethatthestudydurationﬁtwithina
lengthoftimethatmightbeacceptabletoprofessionalengineers,
we limited the maximum time for each task to ﬁfteen minutes.
We found that this limit stopped participants in 15 tasks (2 with
NaNofuzz,13withJest),or9%ofthetotaltasksencountered.Ofthe
stoppedJesttasks,4receivedaperfectscore,soitisnotreasonable
toexpect that the time limit reducedthese scores. Given thislimit
aﬀected Jest tasks more often than NaNofuzz tasks, it is possible
that accuracy for 9 Jest tasks might be higher had no time limit
been imposed. Despite using an unambiguous rubric, it is possible
that human error reduced the reliability of RQ1’s scoring. To ad-
dress this risk, the third author randomly sampled four sessions
andindependentlyscoredthembyre-watchingtherecordedses-
sionand agreed with the original scoring.Whileour study design
randomizedtreatmentsequence,theindividualtaskprogramswere
presented in a ﬁxed order, which did not control for fatigue eﬀects.
Asourstudydesignrequiredparticipantstousebothtools,itispos-
sible thateﬀects fromthis combinationoftools arenot controlled.
Forinstance,engineersmayhavebeenmoreconﬁdentonJesttasks
if they exclusivelyusedJest withoutseeing NaNofuzz.
External Validity . While we adoptedKoetal.’s[ 34] recommenda-
tion to use “found” tasks to improve task realism, it is possible that
the tasks we selected from GitHub, Rosetta Code, and Stack Over-
ﬂowareneitherrealisticnorrepresentative.Duringthepost-survey,
we askedparticipants to what extent the tasks they encountered
were realistic, and 86% (24/28) indicated the study tasks were simi-
lartotaskstheymightencounterwhileprogrammingoutsidethe
study.Oursampleofprofessionalsoftwareengineersmaybedis-
similartotheoverallpopulationofsoftwareengineersinimportant
ways that are not quantiﬁed. Due to procedural diﬃculties with se-
curing approval to recruit participants outside of the United States
and Canada, our sample does not include software engineers from
other important geographies. The NaNofuzz prototype used in this
studypossessescapabilitynecessarytoevaluatethehypothesisbutlackssomeimportantfeaturesthatwediscussinSection 4.Tools
with more functionality may be harderto use[ 41], andNaNofuzz’
presentsimplicitymayprovideadiﬀerenteﬀectthanthatofamore-
comprehensive tool.The bugs inour tasks were elicited using the
same implicit oracle, which is not the case with many testing tasks.
Duetothis limitation,we propose inSection 10humanstudiesof
gradualoraclespeciﬁcationusingafutureandmorefeature-rich
versionofNaNofuzz.Duringtheparticipantsessions,wedidnot
model some aspects of industrial software engineering such as
interruptions, since that might introduce variance that would be
diﬃculttocontrolandthatmayobscuretheeﬀectweintendedto
measure withalimitednumber of participants.
Construct Validity .Quality (RQ1) investigates the number of
bugs an engineer accurately identiﬁes while generating the test
suite, and is measured in two parts. RQ1.1measures how many
known bugs the test suite elicits, which is an important quality
component: test suites that ﬁnd actual bugs may be considered
higherquality. RQ1.2measuresthequalityofthetestingactivity
by assessing the extent to which an engineer can accurately de-
scribethebugselicited.Othermeasuresofqualityexistthatwedid
notassess,suchascodecoverage,testsuitesize,andmutantskilled.
However, it was not feasible to test all measures of quality in study
sessionsoflimitedduration. Satisfaction(RQ2) investigatesthe
participant’s conﬁdence in the testing activity. Sadowski and Zim-
mermann [58]explain that a software engineer’s “satisfaction may
be impacted by the real or perceived eﬀectiveness of their personal
work.” We measure self-reported conﬁdence as a proxy variable for
satisfaction on each given task, but it is not clear to what extent
an engineer’s conﬁdence may relate to overall satisfaction with
thetestingactivity. Tasktime (RQ3)investigateshowquicklya
software engineer may generate a test suite. This was measured as
time elapsedfrom the beginningto the end of the task.
9 RELATED WORK
Anandetal .[3]statedthat“testcasegenerationisamongthemost
labour-intensive tasks in software testing.” Yet, test suites are often
created manually [ 4,21,24,35] despite decades of prior work to
automate aspectsoftest suite generation.
The Randoop [ 48] and IntelliTest [ 39] fuzzers are similar to
NaNofuzzinthattheyuseanimplicitoracleandgeneratepersistent
testcasesforatestsuite.LikeNaNofuzz,IntelliTestisintegrated
into an IDE. NaNofuzz diﬀers by prioritizing speed and ease of use
over code coverage; further, NaNofuzz was evaluated with humans
inarandomizedcontrolledtrial.
Mutation testingoriginated in the1970s [ 10,18,19,32,46]. De-
Millo and Oﬀutt [20]described adding mutation testing to the
MothraandGodzillatestingsystemsandreportedpromisinglab
results. Subsequent improvements were reported (e.g., [ 31,50]);
yet,mutationtestingoftenusesthePUTasitsoracle,sothetests
these tools generate neither detect bugs that presently exist nor
assert correctness. NaNofuzz diﬀers from mutation testing tools by
not mutatingthePUTand byusing animplicitoraclethatdetects
common types of bugs. Like mutation testing tools, NaNofuzz is
unable to evaluate aPUT’s correctness relative to aspeciﬁcation.
Ahlgren et al .[1]and Bornholtet al .[7]provided evidencethat
metamorphictestgenerationtoolscontinuetoﬁndimportantuse
1123NaNofuzz: A Usable Tool forAutomatic TestGeneration ESEC/FSE ’23, December3–9, 2023,San Francisco, CA, USA
cases,particularlyinproblemswhereanoraclemaybeunknowable
or complex. Segura et al . [59]surveyed the metamorphic testing
literature, but excludes related work on property-based testing
(e.g., QuickCheck [ 11] and its progeny). However, many engineers
ﬁnd such testing tools diﬃcult to use, and an important theme
of Bornholt et al . [7]was that removing usability barriers was
important for engineer adoption. Goldstein et al . [27]provided
further evidence that practitioners ﬁnd that property-based testing
tools have signiﬁcant usability barriers, which may be limiting the
application of metamorphic testing tools to an artiﬁcially-small set
of use cases. NaNofuzz diﬀers from these tools in that it uses an
implicitoracleanddoesnotcurrentlyallowanengineertoassert
additionalrelationships between inputsandoutputs.
Rothermel et al . [54,55,56]performed foundational work in
testing spreadsheets and evaluated “What You See Is What You
Test” (WYSIWYT), a usable testing interface for end-users that did
notrequirewritingcode.Fisheretal .[22,23]expandedthisworkby
creating“HelpMeTest”(HMT),anATUGtoolforspreadsheetsthat
used random and search-based techniques. Similar to NaNofuzz,
these tools display tests and results as elements within a graphical
userinterface,butdosowithinthecontextofspreadsheetsandnot
within an IDE.
Fraser et al . [24]evaluated EvoSuite, a state-of-the-art search-
basedATUGtool,withhumansoftwareengineersandreportedthat
humansoftwareengineersfoundfewerbugswithEvoSuite,despite
the tool generating test suites that have higher code coverage. Like
mutationtestingtools,EvoSuiteusesthePUTastheoracle,soits
tests neither detect current bugs nor assert correctness. The study
usedqualitativedatatosuggestthatpoorreadabilityoftestcases
represented as code may be partially at fault for its negative result.
Thisﬁndinggavefocustorecentresearchthataimedtoimprovetest
casereadability(e.g.,[ 28,42,49,57,60]).UnlikeEvoSuite,NaNofuzz
detects bugs in current programs and has shown positive results
relativetomanualtestgenerationinarandomizedhumantrialwith
professional software engineers.
Ng et al. [43]observed that one of the top barriers reported
for automatic testing tools is their diﬃculty of use. Li et al . [37]
called for improved usability of random testing tools. Prado and
Vincenzi[51]andArcuri [4]observedthatATUGtoolsarerelatively
unusedinindustryandthattooldesignersoftenprioritizetechnical
orsecondarymeasuresovertheanengineer’sproductivity.Rojas
etal.[52]observedusersandfoundtheneedforimprovedEvoSuite
usabilityandforittobeintegratedintotheIDE.Inarecentindustry
blog post [ 61], James Sowers questioned many interaction aspects
of current testing tools such as Jest. We designed NaNofuzz in
reactionto the ﬁndingsofthe prior work.
10 IMPLICATIONSAND FUTUREWORK
This study’s ﬁndings imply that empirical software engineering
researchers may achieve more impactful results by prioritizing
usabilityaspectssuchasvelocityandsatisfaction inadditionto test
suite qualitywhen designing orevaluating ATUG tools.
Thisstudyprovidesimportantevidencethatfuzzingcanprovide
productivity beneﬁts to software engineers, even with a simple
stochasticopaque-boxalgorithmsuchastheoneusedbyNaNofuzz.Moresophisticatedfuzzersmayprovide furtherbeneﬁtsifprovided
to engineers inausable waywithin adevelopmentenvironment.
Reducingthenumberofteststhatneedtobecreatedmanually
might also allow engineers to re-direct eﬀorts towards testing soft-
ware more rigorously than may be practical today. Additionally,
end-userdevelopersareoftenignoredbyATUGtooldesigners,but
these developers represent a large user base that is particularly
sensitivetothebeneﬁtreceived(e.g.,bugsfoundrelativetotime
invested)[ 33].Duetothisaspect,usableATUGtoolsmayalsohave
agreatimpact onthe quality of end-userdevelopedsoftware.
WehavereleasedNaNofuzztotheVisualStudioCodeMarket-
place [14] and plan to use it in real-world situations outside our
study. As these situations may require testing with explicit oracles,
weplantoinvestigatehowNaNofuzzmightadoptaspectsoftest-
ing tools that allow specifying an explicit oracle. This extension
of NaNofuzz was also suggested by participants (Section 6, T4).
Usabilitymightvarywithadditionalfeaturecomplexity[ 41],and
it is important to explore how additional complexity may aﬀect
engineerproductivity[ 58]whenbuildingatestsuitewhilesimul-
taneously reﬁningan explicitoracle. Additional formative human
studies on engineers using metamorphic testing tools may pro-
videinsightsintothebarrierstheseengineersencounterandhelp
researchers identify human-centered solutions to these barriers.
FutureversionsofNaNofuzzmayalsosupport otherIDEsand other
languagesbeyondVisualStudioCode andTypeScript.
11 CONCLUSIONS
Thispaper presentsNaNofuzz,a usableautomatictest generation
toolthatrunswithinanengineer’sIDEandoﬀersasimplesetof
interactions for generating a test suite. NaNofuzz provides automa-
tion support forﬁnding edge cases, generating test cases for a test
suite, and categorizing test results. We evaluated NaNofuzz in a
randomizedcontrolledhumantrial with28professionalsoftware
engineers using Jest as the control treatment. Participants using
NaNofuzz on average identiﬁed bugs more accurately ( /u1D45D<.05,
by30%),weremoreconﬁdentintheirtests( /u1D45D<.03,by20%),and
ﬁnished their tasks more quickly ( /u1D45D<.007, by 30%). Given the esti-
mated$47billionUSDannualcostoftestingintheUnitedStates,
theseﬁndingssuggestthatprioritizingtestingtoolusabilitymay
leadtosigniﬁcantproductivitygainsforsoftwareengineers,aswell
asallowformore-rigoroussoftwaretesting.Wehopethatthisstudy
motivatesfurtherresearchintousabletestsuitegenerationtools
that help engineers eﬃciently and conﬁdently generate eﬀective
test suites,alongwithappropriate evaluations of their success.
12 DATA AVAILABILITY
The study data, analysis, tasks, participant demographics, and pro-
tocol are submitted with this paper as supplementary material [ 17]
andviaourstudyrepository[ 16].NaNofuzzisMIT-licensedand
available viaGitHub[ 15]andthe VSCode Marketplace [ 14].
ACKNOWLEDGMENTS
ThisworkwassupportedinpartbyaCyLabseedfundingaward
andbyNSFgrants2150217and1910264.Anyopinions,ﬁndings,or
conclusions expressed in this material are those of the authors and
do not necessarily reﬂectthoseof any of the sponsors.
1124ESEC/FSE ’23, December3–9, 2023,San Francisco, CA, USA Ma/t_thewC.Davis,SangheonChoi, SamEstep, Brad A.Myers, andJoshua Sunshine
REFERENCES
[1]John Ahlgren, Maria Berezin, Kinga Bojarczuk, Elena Dulskyte, Inna Dvortsova,
Johann George, Natalija Gucevska, Mark Harman, Maria Lomeli, Erik Meijer,
Silvia Sapora, and Justin Spahr-Summers. 2021. Testing Web Enabled Simula-
tionatScaleUsingMetamorphicTesting.In 2021IEEE/ACM43rdInternational
Conference on Software Engineering: Software Engineering in Practice (ICSE-SEIP) .
140–149. https://doi.org/10.1109/ICSE-SEIP52600.2021.00023
[2]Bilal Amir and Paul Ralph. 2018. There is no random sampling in software engi-
neering research. In Proceedings of the 40th International Conference on Software
Engineering: companion proceeedings . 344–345. https://doi.org/10.1145/3183440.
3195001
[3]Saswat Anand, Edmund K. Burke, Tsong Yueh Chen, John Clark, Myra B. Cohen,
Wolfgang Grieskamp, Mark Harman, Mary Jean Harrold, Phil McMinn, Antonia
Bertolino,J.JennyLi,andHongZhu.2013. Anorchestratedsurveyofmethodolo-
giesforautomatedsoftwaretestcasegeneration. JournalofSystemsandSoftware
86,8 (2013), 1978–2001. https://doi.org/10.1016/j.jss.2013.02.061
[4]AndreaArcuri.2018. Anexperiencereportonapplyingsoftwaretestingacademic
resultsinindustry:weneedusableautomatedtestgeneration. EmpiricalSoftware
Engineering 23,4 (2018),1959–1981. https://doi.org/10.1007/s10664-017-9570-9
[5]SebastianBaltesandPaulRalph.2022. Samplinginsoftwareengineeringresearch:
acriticalreviewandguidelines. EmpiricalSoftwareEngineering 27,4(April2022),
94.https://doi.org/10.1007/s10664-021-10072-8
[6]MoritzBeller,GeorgiosGousios,AnnibalePanichella,SebastianProksch,Sven
Amann, and Andy Zaidman. 2017. Developer testing in the ide: Patterns, beliefs,
andbehavior. IEEETransactionsonSoftwareEngineering 45,3(2017),261–284.
https://doi.org/10.1109/TSE.2017.2776152
[7]James Bornholt, Rajeev Joshi, Vytautas Astrauskas, Brendan Cully, Bernhard
Kragl,SethMarkle,KyleSauri,DrewSchleit,GrantSlatton,SerdarTasiran,Jacob
Van Geﬀen, and Andrew Warﬁeld. 2021. Using Lightweight Formal Methods
toValidateaKey-ValueStorageNodeinAmazonS3.In ProceedingsoftheACM
SIGOPS28thSymposiumonOperatingSystemsPrinciples (VirtualEvent,Germany)
(SOSP ’21) . Association for Computing Machinery, New York, NY, USA, 836–850.
https://doi.org/10.1145/3477132.3483540
[8]VirginiaBraun andVictoria Clarke.2006. Usingthematic analysisinpsychology.
Qualitativeresearchinpsychology 3,2(2006),77–101. https://doi.org/10.1191/
1478088706qp063oa
[9]JohnBrookeetal .1996. SUS-Aquickanddirtyusabilityscale. Usabilityevaluation
inindustry 189, 194(1996), 4–7. https://doi.org/10.1201/9781498710411
[10]TimothyABudd,RichardJLipton,RichardDeMillo,andFrederickSayward.1978.
The design of a prototype mutation system for program testing. In Managing
Requirements Knowledge, International Workshop on . IEEE Computer Society,
623–623. https://doi.org/10.1109/AFIPS.1978.195
[11]Koen Claessen and John Hughes. 2000. QuickCheck: a lightweight tool for
randomtestingofHaskellprograms.In ProceedingsoftheﬁfthACMSIGPLAN
international conference on Functional programming . 268–279. https://doi.org/10.
1145/351240.351266
[12]Anastasia Danilova. 2022. How to Conduct Security Studies with Software De-
velopers. Ph.D. Dissertation. Universitäts-und Landesbibliothek Bonn. https:
//hdl.handle.net/20.500.11811/10063
[13]Matthew C. Davis, Emad Aghayi, Thomas D. Latoza, Xiaoyin Wang, Brad A.
Myers, and JoshuaSunshine. 2023. What’s (Not)Workingin Programmer User
Studies?ACM Trans. Softw. Eng. Methodol. 32, 5, Article 120 (jul 2023), 32 pages.
https://doi.org/10.1145/3587157
[14]MatthewCDavis,SangheonChoi,andSamEstep.2022. NaNofuzz-VisualStudio
Marketplace. https://marketplace.visualstudio.com/items?itemName=penrose.
nanofuzz. [Online;accessed 2022-11-20].
[15]MatthewCDavis,SangheonChoi,andSamEstep.2022. NaNofuzz:afastand
easy-to-use automatic test suite generator for Typescript that runs inside VS
Code.https://github.com/nanofuzz/nanofuzz . [Online;accessed 2022-11-20].
[16]MatthewCDavis, Sangheon Choi,andSam Estep. 2022. nanofuzz-study. https:
//github.com/nanofuzz/nanofuzz-study .
[17]Matthew CDavis, SangheonChoi,and SamEstep.2023. Reproduction Package
for Article “NaNofuzz: A Usable Test Suite Generation Tool”. https://doi.org/10.
1145/3580413
[18] R.A.DeMillo,R.J.Lipton,andF.G.Sayward.1978. HintsonTestDataSelection:
Help for the Practicing Programmer. Computer 11, 4 (April 1978), 34–41. https:
//doi.org/10.1109/C-M.1978.218136 ConferenceName: Computer.
[19]Richard A DeMillo. 1989. Completely validated software: Test adequacy and pro-
grammutation(panelsession).In Proceedingsofthe11thInternationalConference
onSoftwareengineering . 355–356. https://doi.org/10.1145/74587.74634
[20]RichardADeMilloandAJeﬀersonOﬀutt.1993. Experimentalresultsfroman
automatic test case generator. ACM Transactions on Software Engineering and
Methodology(TOSEM) 2,2(1993),109–127. https://doi.org/10.1145/151257.151258
[21]Eduard Enoiu and Robert Feldt. 2021. Towards Human-Like Automated Test
Generation:PerspectivesfromCognitionandProblemSolving.In 2021IEEE/ACM
13th International Workshop on Cooperative and Human Aspects of Software Engi-
neering (CHASE) . 123–124. https://doi.org/10.1109/CHASE52884.2021.00026[22]MarcFisher,MingmingCao,GreggRothermel,CurtisRCook,andMargaretM
Burnett.2002. Automatedtestcasegenerationforspreadsheets.In Proceedingsof
the24thInternationalConferenceonSoftwareEngineering(ICSE) .IEEE,141–151.
https://doi.org/10.1145/581339.581359
[23]Marc Fisher, Gregg Rothermel, Darren Brown, Mingming Cao, Curtis Cook,
and Margaret Burnett. 2006. Integrating automated test generation into the
WYSIWYT spreadsheet testing methodology. ACM Transactions on Software
Engineering and Methodology (TOSEM) 15, 2 (2006), 150–194. https://doi.org/10.
1145/1131421.1131423
[24]GordonFraser,MattStaats,PhilMcMinn,AndreaArcuri,andFrankPadberg.2015.
Doesautomatedunittestgenerationreallyhelpsoftwaretesters?acontrolled
empirical study. ACM Transactions on Software Engineering and Methodology
(TOSEM) 24,4 (2015), 1–49. https://doi.org/10.1145/2699688
[25]HannahFrithandKateGleeson.2004. Clothingandembodiment:Menmanaging
body image and appearance. Psychology of men & masculinity 5, 1 (2004), 40.
https://doi.org/10.1037/1524-9220.5.1.40
[26]GitHub. 2022. GitHub Codespaces. https://github.com/features/codespaces .
[Online;accessed 2022-11-21].
[27]HarrisonGoldstein,JosephWCutler,AdamStein,BenjaminCPierce,andAndrew
Head. 2022. Some Problems with Properties. In Proc. Workshop on the Human
Aspects ofTypesand ReasoningAssistants (HATRA) .
[28]Giovanni Grano, Simone Scalabrino, Harald C Gall, and Rocco Oliveto. 2018. An
empirical investigation on the readability of manual and generated test cases. In
2018 IEEE/ACM 26th International Conference on Program Comprehension (ICPC) .
IEEE,348–3483. https://doi.org/10.1145/3196321.3196363
[29]WilliamEHowden.1978. Theoreticaland empiricalstudiesofprogramtesting.
IEEE Transactions on Software Engineering 4 (1978), 293–298. https://doi.org/10.
1109/TSE.1978.231514
[30]Pankaj Jalote. 2008. A concise introduction to software engineering . Springer
Science & BusinessMedia. https://doi.org/10.1007/978-1-84800-302-6
[31] Yue Jia and Mark Harman. 2010. Ananalysis and survey of the development of
mutation testing. IEEE transactions on software engineering 37, 5 (2010), 649–678.
https://doi.org/10.1109/TSE.2010.62
[32]Yue Jia and Mark Harman. 2011. An Analysis and Survey of the Development of
MutationTesting. IEEETransactionsonSoftwareEngineering 37,5(Sept.2011),
649–678. https://doi.org/10.1109/TSE.2010.62
[33]Amy J. Ko, Robin Abraham, Laura Beckwith, Alan Blackwell, Margaret Burnett,
Martin Erwig, Chris Scaﬃdi, Joseph Lawrance, Henry Lieberman, Brad A Myers,
Mary Beth Rosson, Gregg Rothermel, Mary Shaw, and Susan Wiedenbeck. 2011.
TheStateofthe ArtinEnd-User SoftwareEngineering. ACM Comput.Surv. 43,
3,Article21(apr2011),44pages. https://doi.org/10.1145/1922649.1922658
[34]AmyJKo,ThomasDLaToza,andMargaretMBurnett.2015. Apracticalguideto
controlledexperimentsofsoftwareengineeringtoolswithhumanparticipants.
Empirical Software Engineering 20, 1 (2015), 110–141. https://doi.org/10.1007/
s10664-013-9279-3
[35]JeshuaSKracht,JacobZPetrovic,andKristenRWalcott-Justice.2014.Empirically
evaluating the quality of automatically generated and manually written test
suites. In 2014 14thInternational Conference on Quality Software . IEEE,256–265.
https://doi.org/10.1109/QSIC.2014.33
[36]J. Richard Landis and Gary G. Koch. 1977. The Measurement of Observer
Agreement for Categorical Data. Biometrics 33, 1 (1977), 159–174. https:
//doi.org/10.2307/2529310 Publisher: [Wiley, International Biometric Society].
[37]YuweiLi,ShoulingJi,YuanChen,SizhuangLiang,Wei-HanLee,YueyaoChen,
ChenyangLyu,ChunmingWu,RaheemBeyah,PengCheng,etal .2021.UNIFUZZ:
A Holistic and Pragmatic Metrics-Driven Platform for Evaluating Fuzzers.. In
USENIXSecuritySymposium . USENIXAssociation, 2777–2794.
[38]ValentinJMManès,HyungSeokHan,ChoongwooHan,SangKilCha,Manuel
Egele, Edward J Schwartz, and Maverick Woo. 2021. The Art, Science, and
Engineering of Fuzzing: A Survey. IEEE Transactions on Software Engineering 47,
11(2021), 2312–2331. https://doi.org/10.1109/TSE.2019.2946563
[39]Microsoft.2023. OverviewofMicrosoftIntelliTester. https://learn.microsoft.com/
en-us/visualstudio/test/intellitest-manual/ . [Online;accessed 2023-01-27].
[40]Barton P Miller, Lars Fredriksen, and Bryan So. 1990. An empirical study of
the reliability of UNIX utilities. Commun. ACM 33, 12 (1990), 32–44. https:
//doi.org/10.1145/96267.96279
[41]Brad A Myers. 1994. Challenges of HCI Design andImplementation. Interactions
1,1 (jan1994),73–83. https://doi.org/10.1145/174800.174808
[42]MathieuNassif,AlexaHernandez,AshvithaSridharan,andMartinPRobillard.
2022. Generating unit tests for documentation. IEEE Transactions on Software
Engineering 48,9 (2022), 3268–3279. https://doi.org/10.1109/TSE.2021.3087087
[43]Sebastian P Ng, Taﬂine Murnane, Karl Reed, D Grant, and Tsong Yueh Chen.
2004. A preliminary survey on software testing practices in Australia. In 2004
AustralianSoftwareEngineeringConference.Proceedings. IEEE,116–125. https:
//doi.org/10.1109/ASWEC.2004.1290464
[44]JakobNielsen.1994. EnhancingtheExplanatoryPowerofUsabilityHeuristics.
InProceedings of the SIGCHI Conference on Human Factors in Computing Systems
(Boston, Massachusetts,USA) (CHI’94). AssociationforComputingMachinery,
NewYork, NY, USA,152–158. https://doi.org/10.1145/191666.191729
1125NaNofuzz: A Usable Tool forAutomatic TestGeneration ESEC/FSE ’23, December3–9, 2023,San Francisco, CA, USA
[45]Bureau of Labor Statistics. 2022. Occupational Outlook Handbook, Software
Developers, Quality Assurance Analysts, and Testers. https://www.bls.gov/
ooh/computer-and-information-technology/software-developers.htm . [Online;
accessed 2022-10-06].
[46]Jeﬀ Oﬀutt. 2011. A mutation carol: Past, present and future. Information and
Software Technology 53, 10 (Oct. 2011), 1098–1107. https://doi.org/10.1016/j.
infsof.2011.03.007
[47]Gerard O’Regan. 2019. Fundamentals of Software Testing . Springer International
Publishing,59–78. https://doi.org/10.1007/978-3-030-28494-7_3
[48]Carlos Pacheco, Shuvendu K. Lahiri, Michael D. Ernst, and Thomas Ball. 2007.
Feedback-Directed Random Test Generation. In 29th International Conference
onSoftwareEngineering(ICSE’07) .IEEE,Minneapolis,MN,USA,75–84. https:
//doi.org/10.1109/ICSE.2007.37 ISSN:0270-5257.
[49]SebastianoPanichella,AnnibalePanichella,MoritzBeller,AndyZaidman,and
Harald C. Gall. 2016. The Impact of Test Case Summaries on Bug Fixing Per-
formance: An Empirical Investigation. In Proceedings of the 38th International
Conference on Software Engineering (Austin, Texas) (ICSE ’16) . Association for
ComputingMachinery,NewYork,NY,USA,547–558. https://doi.org/10.1145/
2884781.2884847
[50]AlessandroViolaPizzoleto,FabianoCutigiFerrari,JeﬀOﬀutt,LeoFernandes,and
MárcioRibeiro.2019. Asystematicliteraturereviewoftechniquesandmetricsto
reducethecost of mutationtesting. JournalofSystems and Software 157 (2019),
110388. https://doi.org/10.1016/j.jss.2019.07.100
[51]Marllos Paiva Prado andAuri Marcelo RizzoVincenzi.2018. Towards cognitive
supportforunittesting:Aqualitativestudywithpractitioners. JournalofSystems
and Software 141(2018), 66–84. https://doi.org/10.1016/j.jss.2018.03.052
[52]José Miguel Rojas, Gordon Fraser, and Andrea Arcuri. 2015. Automated Unit
TestGeneration duringSoftwareDevelopment:AControlledExperimentand
Think-AloudObservations.In Proceedingsofthe2015InternationalSymposiumon
SoftwareTestingandAnalysis (Baltimore,MD,USA) (ISSTA2015) .Associationfor
ComputingMachinery,NewYork,NY,USA,338–349. https://doi.org/10.1145/
2771783.2771801
[53]Robert Rosenthal and Ralph L Rosnow. 2008. Essentials of behavioral research:
Methodsand dataanalysis .
[54]G. Rothermel, L. Li, and M. Burnett. 1997. Testing strategies for form-based
visual programs. In Proceedings TheEighth International Symposium on Software
Reliability Engineering . 96–107. https://doi.org/10.1109/ISSRE.1997.630851[55]G. Rothermel, L. Li, C. DuPuis, and M. Burnett. 1998. What you see is what
you test:a methodologyfor testing form-based visual programs. In Proceedings
of the 20th International Conference on Software Engineering . 198–207. https:
//doi.org/10.1109/ICSE.1998.671118 ISSN:0270-5257.
[56]KarenJ.Rothermel,CurtisR.Cook,MargaretM.Burnett,JustinSchonfeld,T.R.G.
Green, and Gregg Rothermel. 2000. WYSIWYT Testing in the Spreadsheet Para-
digm:AnEmpiricalEvaluation.In Proceedingsofthe22ndInternationalConference
onSoftwareEngineering (Limerick,Ireland) (ICSE’00) .AssociationforComputing
Machinery,NewYork,NY,USA,230–239. https://doi.org/10.1145/337180.337206
[57]Devjeet Roy, Ziyi Zhang, MaggieMa, VeneraArnaoudova,AnnibalePanichella,
Sebastiano Panichella, Danielle Gonzalez, and Mehdi Mirakhorli. 2021. DeepTC-
Enhancer:ImprovingtheReadabilityofAutomaticallyGeneratedTests.In Pro-
ceedingsofthe35thIEEE/ACMInternationalConferenceonAutomatedSoftware
Engineering (Virtual Event, Australia) (ASE ’20). Association for Computing Ma-
chinery, New York, NY, USA, 287–298. https://doi.org/10.1145/3324884.3416622
[58]Caitlin Sadowski and Thomas Zimmermann. 2019. Rethinking productivity in
softwareengineering .SpringerNature. https://doi.org/10.1007/978-1-4842-4221-6
[59]Sergio Segura, Gordon Fraser, Ana B Sanchez, and Antonio Ruiz-Cortés. 2016. A
Surveyon Metamorphic Testing. IEEETransactionson Software Engineering 42,9
(2016), 805–824. https://doi.org/10.1109/TSE.2016.2532875
[60]NoviSetiani,RidiFerdiana,andRudyHartanto.2022. UnderstandableAutomatic
Generated Unit Tests using Semantic and Format Improvement. In 2022 6th
InternationalConferenceonInformaticsandComputationalSciences(ICICoS) .122–
127.https://doi.org/10.1109/ICICoS56336.2022.9930600
[61]James Somers. 2023. What if writing tests was a joyful experience? https:
//blog.janestreet.com/the-joy-of-expect-tests/
[62]FacebookOpenSource.2022. Jest-DelightfulJavascriptTesting. https://jestjs.io/ .
[Online;accessed 2022-11-08].
[63]TristanTeufelandcontributors.2022. JestRunner. https://github.com/ﬁrsttris/
vscode-jest-runner . [Online;accessed 2022-11-10].
[64]PriyadarshiTripathyandKshirasagarNaik.2011. Softwaretestingandquality
assurance: theory and practice . John Wiley & Sons. https://doi.org/10.1002/
9780470382844
[65]Hong Zhu, Patrick A. V. Hall, and John H. R. May. 1997. Software Unit Test
CoverageandAdequacy. ACMComput.Surv. 29,4(dec1997),366–427. https:
//doi.org/10.1145/267580.267590
Received 2023-02-02; accepted 2023-07-27
1126